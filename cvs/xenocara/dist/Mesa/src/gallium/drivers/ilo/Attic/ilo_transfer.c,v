head	1.2;
access;
symbols
	OPENBSD_5_8:1.1.1.4.0.4
	OPENBSD_5_8_BASE:1.1.1.4
	OPENBSD_5_7:1.1.1.4.0.2
	OPENBSD_5_7_BASE:1.1.1.4
	v10_2_9:1.1.1.4
	v10_4_3:1.1.1.3
	v10_2_7:1.1.1.2
	OPENBSD_5_6:1.1.1.2.0.2
	OPENBSD_5_6_BASE:1.1.1.2
	v10_2_3:1.1.1.2
	OPENBSD_5_5:1.1.1.1.0.2
	OPENBSD_5_5_BASE:1.1.1.1
	v9_2_5:1.1.1.1
	v9_2_3:1.1.1.1
	v9_2_2:1.1.1.1
	v9_2_1:1.1.1.1
	v9_2_0:1.1.1.1
	mesa:1.1.1;
locks; strict;
comment	@ * @;
expand	@o@;


1.2
date	2015.12.23.05.17.30;	author jsg;	state dead;
branches;
next	1.1;
commitid	TnlogFl9nOv2eaRf;

1.1
date	2013.09.05.13.11.18;	author jsg;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2013.09.05.13.11.18;	author jsg;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2014.07.09.20.33.53;	author jsg;	state Exp;
branches;
next	1.1.1.3;
commitid	3JhLfwcuBALP0ZR7;

1.1.1.3
date	2015.01.25.14.07.18;	author jsg;	state Exp;
branches;
next	1.1.1.4;
commitid	ce2W5rH5aF7VS9gi;

1.1.1.4
date	2015.02.20.22.44.26;	author jsg;	state Exp;
branches;
next	;
commitid	F54a1i0WXHMxq7kE;


desc
@@


1.2
log
@remove the now unused Mesa 10.2.9 code
@
text
@/*
 * Mesa 3-D graphics library
 *
 * Copyright (C) 2012-2013 LunarG, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
 * DEALINGS IN THE SOFTWARE.
 *
 * Authors:
 *    Chia-I Wu <olv@@lunarg.com>
 */

#include "util/u_surface.h"
#include "util/u_transfer.h"
#include "util/u_format_etc.h"

#include "ilo_cp.h"
#include "ilo_context.h"
#include "ilo_resource.h"
#include "ilo_state.h"
#include "ilo_transfer.h"

static bool
is_bo_busy(struct ilo_context *ilo, struct intel_bo *bo, bool *need_flush)
{
   const bool referenced = intel_bo_references(ilo->cp->bo, bo);

   if (need_flush)
      *need_flush = referenced;

   if (referenced)
      return true;

   return intel_bo_is_busy(bo);
}

static bool
map_bo_for_transfer(struct ilo_context *ilo, struct intel_bo *bo,
                    const struct ilo_transfer *xfer)
{
   int err;

   switch (xfer->method) {
   case ILO_TRANSFER_MAP_CPU:
      err = intel_bo_map(bo, (xfer->base.usage & PIPE_TRANSFER_WRITE));
      break;
   case ILO_TRANSFER_MAP_GTT:
      err = intel_bo_map_gtt(bo);
      break;
   case ILO_TRANSFER_MAP_UNSYNC:
      err = intel_bo_map_unsynchronized(bo);
      break;
   default:
      assert(!"unknown mapping method");
      err = -1;
      break;
   }

   return !err;
}

/**
 * Choose the best mapping method, depending on the transfer usage and whether
 * the bo is busy.
 */
static bool
choose_transfer_method(struct ilo_context *ilo, struct ilo_transfer *xfer)
{
   struct pipe_resource *res = xfer->base.resource;
   const unsigned usage = xfer->base.usage;
   /* prefer map() when there is the last-level cache */
   const bool prefer_cpu =
      (ilo->dev->has_llc || (usage & PIPE_TRANSFER_READ));
   struct ilo_texture *tex;
   struct ilo_buffer *buf;
   struct intel_bo *bo;
   bool tiled, need_flush;

   if (res->target == PIPE_BUFFER) {
      tex = NULL;

      buf = ilo_buffer(res);
      bo = buf->bo;
      tiled = false;
   }
   else {
      buf = NULL;

      tex = ilo_texture(res);
      bo = tex->bo;
      tiled = (tex->tiling != INTEL_TILING_NONE);
   }

   /* choose between mapping through CPU or GTT */
   if (usage & PIPE_TRANSFER_MAP_DIRECTLY) {
      /* we do not want fencing */
      if (tiled || prefer_cpu)
         xfer->method = ILO_TRANSFER_MAP_CPU;
      else
         xfer->method = ILO_TRANSFER_MAP_GTT;
   }
   else {
      if (!tiled && prefer_cpu)
         xfer->method = ILO_TRANSFER_MAP_CPU;
      else
         xfer->method = ILO_TRANSFER_MAP_GTT;
   }

   /* see if we can avoid stalling */
   if (is_bo_busy(ilo, bo, &need_flush)) {
      bool will_stall = true;

      if (usage & PIPE_TRANSFER_MAP_DIRECTLY) {
         /* nothing we can do */
      }
      else if (usage & PIPE_TRANSFER_UNSYNCHRONIZED) {
         /* unsynchronized gtt mapping does not stall */
         xfer->method = ILO_TRANSFER_MAP_UNSYNC;
         will_stall = false;
      }
      else if (usage & PIPE_TRANSFER_DISCARD_WHOLE_RESOURCE) {
         /* discard old bo and allocate a new one for mapping */
         if ((tex && ilo_texture_alloc_bo(tex)) ||
             (buf && ilo_buffer_alloc_bo(buf))) {
            ilo_mark_states_with_resource_dirty(ilo, res);
            will_stall = false;
         }
      }
      else if (usage & PIPE_TRANSFER_FLUSH_EXPLICIT) {
         /*
          * We could allocate and return a system buffer here.  When a region of
          * the buffer is explicitly flushed, we pwrite() the region to a
          * temporary bo and emit pipelined copy blit.
          *
          * For now, do nothing.
          */
      }
      else if (usage & PIPE_TRANSFER_DISCARD_RANGE) {
         /*
          * We could allocate a temporary bo for mapping, and emit pipelined copy
          * blit upon unmapping.
          *
          * For now, do nothing.
          */
      }

      if (will_stall) {
         if (usage & PIPE_TRANSFER_DONTBLOCK)
            return false;

         /* flush to make bo busy (so that map() stalls as it should be) */
         if (need_flush)
            ilo_cp_flush(ilo->cp);
      }
   }

   if (tex && !(usage & PIPE_TRANSFER_MAP_DIRECTLY)) {
      if (tex->separate_s8 || tex->bo_format == PIPE_FORMAT_S8_UINT)
         xfer->method = ILO_TRANSFER_MAP_SW_ZS;
      /* need to convert on-the-fly */
      else if (tex->bo_format != tex->base.format)
         xfer->method = ILO_TRANSFER_MAP_SW_CONVERT;
   }

   return true;
}

static void
tex_get_box_origin(const struct ilo_texture *tex,
                   unsigned level, unsigned slice,
                   const struct pipe_box *box,
                   unsigned *mem_x, unsigned *mem_y)
{
   unsigned x, y;

   x = tex->slice_offsets[level][slice + box->z].x + box->x;
   y = tex->slice_offsets[level][slice + box->z].y + box->y;

   assert(x % tex->block_width == 0 && y % tex->block_height == 0);

   *mem_x = x / tex->block_width * tex->bo_cpp;
   *mem_y = y / tex->block_height;
}

static unsigned
tex_get_box_offset(const struct ilo_texture *tex, unsigned level,
                   const struct pipe_box *box)
{
   unsigned mem_x, mem_y;

   tex_get_box_origin(tex, level, 0, box, &mem_x, &mem_y);

   return mem_y * tex->bo_stride + mem_x;
}

static unsigned
tex_get_slice_stride(const struct ilo_texture *tex, unsigned level)
{
   unsigned qpitch;

   /* there is no 3D array texture */
   assert(tex->base.array_size == 1 || tex->base.depth0 == 1);

   if (tex->base.array_size == 1) {
      /* non-array, non-3D */
      if (tex->base.depth0 == 1)
         return 0;

      /* only the first level has a fixed slice stride */
      if (level > 0) {
         assert(!"no slice stride for 3D texture with level > 0");
         return 0;
      }
   }

   qpitch = tex->slice_offsets[level][1].y - tex->slice_offsets[level][0].y;
   assert(qpitch % tex->block_height == 0);

   return (qpitch / tex->block_height) * tex->bo_stride;
}

static unsigned
tex_tile_x_swizzle(unsigned addr)
{
   /*
    * From the Ivy Bridge PRM, volume 1 part 2, page 24:
    *
    *     "As shown in the tiling algorithm, the new address bit[6] should be:
    *
    *        Address bit[6] <= TiledAddr bit[6] XOR
    *                          TiledAddr bit[9] XOR
    *                          TiledAddr bit[10]"
    */
   return addr ^ (((addr >> 3) ^ (addr >> 4)) & 0x40);
}

static unsigned
tex_tile_y_swizzle(unsigned addr)
{
   /*
    * From the Ivy Bridge PRM, volume 1 part 2, page 24:
    *
    *     "As shown in the tiling algorithm, The new address bit[6] becomes:
    *
    *        Address bit[6] <= TiledAddr bit[6] XOR
    *                          TiledAddr bit[9]"
    */
   return addr ^ ((addr >> 3) & 0x40);
}

static unsigned
tex_tile_x_offset(unsigned mem_x, unsigned mem_y,
                  unsigned tiles_per_row, bool swizzle)
{
   /*
    * From the Sandy Bridge PRM, volume 1 part 2, page 21, we know that a
    * X-major tile has 8 rows and 32 OWord columns (512 bytes).  Tiles in the
    * tiled region are numbered in row-major order, starting from zero.  The
    * tile number can thus be calculated as follows:
    *
    *    tile = (mem_y / 8) * tiles_per_row + (mem_x / 512)
    *
    * OWords in that tile are also numbered in row-major order, starting from
    * zero.  The OWord number can thus be calculated as follows:
    *
    *    oword = (mem_y % 8) * 32 + ((mem_x % 512) / 16)
    *
    * and the tiled offset is
    *
    *    offset = tile * 4096 + oword * 16 + (mem_x % 16)
    *           = tile * 4096 + (mem_y % 8) * 512 + (mem_x % 512)
    */
   unsigned tile, offset;

   tile = (mem_y >> 3) * tiles_per_row + (mem_x >> 9);
   offset = tile << 12 | (mem_y & 0x7) << 9 | (mem_x & 0x1ff);

   return (swizzle) ? tex_tile_x_swizzle(offset) : offset;
}

static unsigned
tex_tile_y_offset(unsigned mem_x, unsigned mem_y,
                  unsigned tiles_per_row, bool swizzle)
{
   /*
    * From the Sandy Bridge PRM, volume 1 part 2, page 22, we know that a
    * Y-major tile has 32 rows and 8 OWord columns (128 bytes).  Tiles in the
    * tiled region are numbered in row-major order, starting from zero.  The
    * tile number can thus be calculated as follows:
    *
    *    tile = (mem_y / 32) * tiles_per_row + (mem_x / 128)
    *
    * OWords in that tile are numbered in column-major order, starting from
    * zero.  The OWord number can thus be calculated as follows:
    *
    *    oword = ((mem_x % 128) / 16) * 32 + (mem_y % 32)
    *
    * and the tiled offset is
    *
    *    offset = tile * 4096 + oword * 16 + (mem_x % 16)
    */
   unsigned tile, oword, offset;

   tile = (mem_y >> 5) * tiles_per_row + (mem_x >> 7);
   oword = (mem_x & 0x70) << 1 | (mem_y & 0x1f);
   offset = tile << 12 | oword << 4 | (mem_x & 0xf);

   return (swizzle) ? tex_tile_y_swizzle(offset) : offset;
}

static unsigned
tex_tile_w_offset(unsigned mem_x, unsigned mem_y,
                  unsigned tiles_per_row, bool swizzle)
{
   /*
    * From the Sandy Bridge PRM, volume 1 part 2, page 23, we know that a
    * W-major tile has 8 8x8-block rows and 8 8x8-block columns.  Tiles in the
    * tiled region are numbered in row-major order, starting from zero.  The
    * tile number can thus be calculated as follows:
    *
    *    tile = (mem_y / 64) * tiles_per_row + (mem_x / 64)
    *
    * 8x8-blocks in that tile are numbered in column-major order, starting
    * from zero.  The 8x8-block number can thus be calculated as follows:
    *
    *    blk8 = ((mem_x % 64) / 8) * 8 + ((mem_y % 64) / 8)
    *
    * Each 8x8-block is divided into 4 4x4-blocks, in row-major order.  Each
    * 4x4-block is further divided into 4 2x2-blocks, also in row-major order.
    * We have
    *
    *    blk4 = (((mem_y % 64) / 4) & 1) * 2 + (((mem_x % 64) / 4) & 1)
    *    blk2 = (((mem_y % 64) / 2) & 1) * 2 + (((mem_x % 64) / 2) & 1)
    *    blk1 = (((mem_y % 64)    ) & 1) * 2 + (((mem_x % 64)    ) & 1)
    *
    * and the tiled offset is
    *
    *    offset = tile * 4096 + blk8 * 64 + blk4 * 16 + blk2 * 4 + blk1
    */
   unsigned tile, blk8, blk4, blk2, blk1, offset;

   tile = (mem_y >> 6) * tiles_per_row + (mem_x >> 6);
   blk8 = ((mem_x >> 3) & 0x7) << 3 | ((mem_y >> 3) & 0x7);
   blk4 = ((mem_y >> 2) & 0x1) << 1 | ((mem_x >> 2) & 0x1);
   blk2 = ((mem_y >> 1) & 0x1) << 1 | ((mem_x >> 1) & 0x1);
   blk1 = ((mem_y     ) & 0x1) << 1 | ((mem_x     ) & 0x1);
   offset = tile << 12 | blk8 << 6 | blk4 << 4 | blk2 << 2 | blk1;

   return (swizzle) ? tex_tile_y_swizzle(offset) : offset;
}

static unsigned
tex_tile_none_offset(unsigned mem_x, unsigned mem_y,
                     unsigned tiles_per_row, bool swizzle)
{
   return mem_y * tiles_per_row + mem_x;
}

typedef unsigned (*tex_tile_offset_func)(unsigned mem_x, unsigned mem_y,
                                         unsigned tiles_per_row,
                                         bool swizzle);

static tex_tile_offset_func
tex_tile_choose_offset_func(const struct ilo_texture *tex,
                            unsigned *tiles_per_row)
{
   switch (tex->tiling) {
   case INTEL_TILING_X:
      *tiles_per_row = tex->bo_stride / 512;
      return tex_tile_x_offset;
   case INTEL_TILING_Y:
      *tiles_per_row = tex->bo_stride / 128;
      return tex_tile_y_offset;
   case INTEL_TILING_NONE:
   default:
      /* W-tiling */
      if (tex->bo_format == PIPE_FORMAT_S8_UINT) {
         *tiles_per_row = tex->bo_stride / 64;
         return tex_tile_w_offset;
      }
      else {
         *tiles_per_row = tex->bo_stride;
         return tex_tile_none_offset;
      }
   }
}

static void
tex_staging_sys_zs_read(struct ilo_context *ilo,
                        struct ilo_texture *tex,
                        const struct ilo_transfer *xfer)
{
   const bool swizzle = ilo->dev->has_address_swizzling;
   const struct pipe_box *box = &xfer->base.box;
   const uint8_t *src = intel_bo_get_virtual(tex->bo);
   tex_tile_offset_func tile_offset;
   unsigned tiles_per_row;
   int slice;

   tile_offset = tex_tile_choose_offset_func(tex, &tiles_per_row);

   assert(tex->block_width == 1 && tex->block_height == 1);

   if (tex->separate_s8) {
      struct ilo_texture *s8_tex = tex->separate_s8;
      const uint8_t *s8_src = intel_bo_get_virtual(s8_tex->bo);
      tex_tile_offset_func s8_tile_offset;
      unsigned s8_tiles_per_row;
      int dst_cpp, dst_s8_pos, src_cpp_used;

      s8_tile_offset = tex_tile_choose_offset_func(s8_tex, &s8_tiles_per_row);

      if (tex->base.format == PIPE_FORMAT_Z24_UNORM_S8_UINT) {
         assert(tex->bo_format == PIPE_FORMAT_Z24X8_UNORM);

         dst_cpp = 4;
         dst_s8_pos = 3;
         src_cpp_used = 3;
      }
      else {
         assert(tex->base.format == PIPE_FORMAT_Z32_FLOAT_S8X24_UINT);
         assert(tex->bo_format == PIPE_FORMAT_Z32_FLOAT);

         dst_cpp = 8;
         dst_s8_pos = 4;
         src_cpp_used = 4;
      }

      for (slice = 0; slice < box->depth; slice++) {
         unsigned mem_x, mem_y, s8_mem_x, s8_mem_y;
         uint8_t *dst;
         int i, j;

         tex_get_box_origin(tex, xfer->base.level, slice,
                            box, &mem_x, &mem_y);
         tex_get_box_origin(s8_tex, xfer->base.level, slice,
                            box, &s8_mem_x, &s8_mem_y);

         dst = xfer->staging_sys + xfer->base.layer_stride * slice;

         for (i = 0; i < box->height; i++) {
            unsigned x = mem_x, s8_x = s8_mem_x;
            uint8_t *d = dst;

            for (j = 0; j < box->width; j++) {
               const unsigned offset =
                  tile_offset(x, mem_y, tiles_per_row, swizzle);
               const unsigned s8_offset =
                  s8_tile_offset(s8_x, s8_mem_y, s8_tiles_per_row, swizzle);

               memcpy(d, src + offset, src_cpp_used);
               d[dst_s8_pos] = s8_src[s8_offset];

               d += dst_cpp;
               x += tex->bo_cpp;
               s8_x++;
            }

            dst += xfer->base.stride;
            mem_y++;
            s8_mem_y++;
         }
      }
   }
   else {
      assert(tex->bo_format == PIPE_FORMAT_S8_UINT);

      for (slice = 0; slice < box->depth; slice++) {
         unsigned mem_x, mem_y;
         uint8_t *dst;
         int i, j;

         tex_get_box_origin(tex, xfer->base.level, slice,
                            box, &mem_x, &mem_y);

         dst = xfer->staging_sys + xfer->base.layer_stride * slice;

         for (i = 0; i < box->height; i++) {
            unsigned x = mem_x;
            uint8_t *d = dst;

            for (j = 0; j < box->width; j++) {
               const unsigned offset =
                  tile_offset(x, mem_y, tiles_per_row, swizzle);

               *d = src[offset];

               d++;
               x++;
            }

            dst += xfer->base.stride;
            mem_y++;
         }
      }
   }
}

static void
tex_staging_sys_zs_write(struct ilo_context *ilo,
                         struct ilo_texture *tex,
                         const struct ilo_transfer *xfer)
{
   const bool swizzle = ilo->dev->has_address_swizzling;
   const struct pipe_box *box = &xfer->base.box;
   uint8_t *dst = intel_bo_get_virtual(tex->bo);
   tex_tile_offset_func tile_offset;
   unsigned tiles_per_row;
   int slice;

   tile_offset = tex_tile_choose_offset_func(tex, &tiles_per_row);

   assert(tex->block_width == 1 && tex->block_height == 1);

   if (tex->separate_s8) {
      struct ilo_texture *s8_tex = tex->separate_s8;
      uint8_t *s8_dst = intel_bo_get_virtual(s8_tex->bo);
      tex_tile_offset_func s8_tile_offset;
      unsigned s8_tiles_per_row;
      int src_cpp, src_s8_pos, dst_cpp_used;

      s8_tile_offset = tex_tile_choose_offset_func(s8_tex, &s8_tiles_per_row);

      if (tex->base.format == PIPE_FORMAT_Z24_UNORM_S8_UINT) {
         assert(tex->bo_format == PIPE_FORMAT_Z24X8_UNORM);

         src_cpp = 4;
         src_s8_pos = 3;
         dst_cpp_used = 3;
      }
      else {
         assert(tex->base.format == PIPE_FORMAT_Z32_FLOAT_S8X24_UINT);
         assert(tex->bo_format == PIPE_FORMAT_Z32_FLOAT);

         src_cpp = 8;
         src_s8_pos = 4;
         dst_cpp_used = 4;
      }

      for (slice = 0; slice < box->depth; slice++) {
         unsigned mem_x, mem_y, s8_mem_x, s8_mem_y;
         const uint8_t *src;
         int i, j;

         tex_get_box_origin(tex, xfer->base.level, slice,
                            box, &mem_x, &mem_y);
         tex_get_box_origin(s8_tex, xfer->base.level, slice,
                            box, &s8_mem_x, &s8_mem_y);

         src = xfer->staging_sys + xfer->base.layer_stride * slice;

         for (i = 0; i < box->height; i++) {
            unsigned x = mem_x, s8_x = s8_mem_x;
            const uint8_t *s = src;

            for (j = 0; j < box->width; j++) {
               const unsigned offset =
                  tile_offset(x, mem_y, tiles_per_row, swizzle);
               const unsigned s8_offset =
                  s8_tile_offset(s8_x, s8_mem_y, s8_tiles_per_row, swizzle);

               memcpy(dst + offset, s, dst_cpp_used);
               s8_dst[s8_offset] = s[src_s8_pos];

               s += src_cpp;
               x += tex->bo_cpp;
               s8_x++;
            }

            src += xfer->base.stride;
            mem_y++;
            s8_mem_y++;
         }
      }
   }
   else {
      assert(tex->bo_format == PIPE_FORMAT_S8_UINT);

      for (slice = 0; slice < box->depth; slice++) {
         unsigned mem_x, mem_y;
         const uint8_t *src;
         int i, j;

         tex_get_box_origin(tex, xfer->base.level, slice,
                            box, &mem_x, &mem_y);

         src = xfer->staging_sys + xfer->base.layer_stride * slice;

         for (i = 0; i < box->height; i++) {
            unsigned x = mem_x;
            const uint8_t *s = src;

            for (j = 0; j < box->width; j++) {
               const unsigned offset =
                  tile_offset(x, mem_y, tiles_per_row, swizzle);

               dst[offset] = *s;

               s++;
               x++;
            }

            src += xfer->base.stride;
            mem_y++;
         }
      }
   }
}

static void
tex_staging_sys_convert_write(struct ilo_context *ilo,
                              struct ilo_texture *tex,
                              const struct ilo_transfer *xfer)
{
   const struct pipe_box *box = &xfer->base.box;
   unsigned dst_slice_stride;
   void *dst;
   int slice;

   dst = intel_bo_get_virtual(tex->bo);
   dst += tex_get_box_offset(tex, xfer->base.level, box);

   /* slice stride is not always available */
   if (box->depth > 1)
      dst_slice_stride = tex_get_slice_stride(tex, xfer->base.level);
   else
      dst_slice_stride = 0;

   if (unlikely(tex->bo_format == tex->base.format)) {
      util_copy_box(dst, tex->bo_format, tex->bo_stride, dst_slice_stride,
            0, 0, 0, box->width, box->height, box->depth,
            xfer->staging_sys, xfer->base.stride, xfer->base.layer_stride,
            0, 0, 0);
      return;
   }

   switch (tex->base.format) {
   case PIPE_FORMAT_ETC1_RGB8:
      assert(tex->bo_format == PIPE_FORMAT_R8G8B8X8_UNORM);

      for (slice = 0; slice < box->depth; slice++) {
         const void *src =
            xfer->staging_sys + xfer->base.layer_stride * slice;

         util_format_etc1_rgb8_unpack_rgba_8unorm(dst,
               tex->bo_stride, src, xfer->base.stride,
               box->width, box->height);

         dst += dst_slice_stride;
      }
      break;
   default:
      assert(!"unable to convert the staging data");
      break;
   }
}

static bool
tex_staging_sys_map_bo(const struct ilo_context *ilo,
                       const struct ilo_texture *tex,
                       bool for_read_back, bool linear_view)
{
   const bool prefer_cpu = (ilo->dev->has_llc || for_read_back);
   int err;

   if (prefer_cpu && (tex->tiling == INTEL_TILING_NONE || !linear_view))
      err = intel_bo_map(tex->bo, !for_read_back);
   else
      err = intel_bo_map_gtt(tex->bo);

   if (!tex->separate_s8)
      return !err;

   err = intel_bo_map(tex->separate_s8->bo, !for_read_back);
   if (err)
      intel_bo_unmap(tex->bo);

   return !err;
}

static void
tex_staging_sys_unmap_bo(const struct ilo_context *ilo,
                         const struct ilo_texture *tex)
{
   if (tex->separate_s8)
      intel_bo_unmap(tex->separate_s8->bo);

   intel_bo_unmap(tex->bo);
}

static void
tex_staging_sys_unmap(struct ilo_context *ilo,
                      struct ilo_texture *tex,
                      struct ilo_transfer *xfer)
{
   bool success;

   if (!(xfer->base.usage & PIPE_TRANSFER_WRITE)) {
      FREE(xfer->staging_sys);
      return;
   }

   switch (xfer->method) {
   case ILO_TRANSFER_MAP_SW_CONVERT:
      success = tex_staging_sys_map_bo(ilo, tex, false, true);
      if (success) {
         tex_staging_sys_convert_write(ilo, tex, xfer);
         tex_staging_sys_unmap_bo(ilo, tex);
      }
      break;
   case ILO_TRANSFER_MAP_SW_ZS:
      success = tex_staging_sys_map_bo(ilo, tex, false, false);
      if (success) {
         tex_staging_sys_zs_write(ilo, tex, xfer);
         tex_staging_sys_unmap_bo(ilo, tex);
      }
      break;
   default:
      assert(!"unknown mapping method");
      success = false;
      break;
   }

   if (!success)
      ilo_err("failed to map resource for moving staging data\n");

   FREE(xfer->staging_sys);
}

static bool
tex_staging_sys_map(struct ilo_context *ilo,
                    struct ilo_texture *tex,
                    struct ilo_transfer *xfer)
{
   const struct pipe_box *box = &xfer->base.box;
   const size_t stride = util_format_get_stride(tex->base.format, box->width);
   const size_t size =
      util_format_get_2d_size(tex->base.format, stride, box->height);
   bool read_back = false, success;

   xfer->staging_sys = MALLOC(size * box->depth);
   if (!xfer->staging_sys)
      return false;

   xfer->base.stride = stride;
   xfer->base.layer_stride = size;
   xfer->ptr = xfer->staging_sys;

   /* see if we need to read the resource back */
   if (xfer->base.usage & PIPE_TRANSFER_READ) {
      read_back = true;
   }
   else if (xfer->base.usage & PIPE_TRANSFER_WRITE) {
      const unsigned discard_flags =
         (PIPE_TRANSFER_DISCARD_RANGE | PIPE_TRANSFER_DISCARD_WHOLE_RESOURCE);

      if (!(xfer->base.usage & discard_flags))
         read_back = true;
   }

   if (!read_back)
      return true;

   switch (xfer->method) {
   case ILO_TRANSFER_MAP_SW_CONVERT:
      assert(!"no on-the-fly format conversion for mapping");
      success = false;
      break;
   case ILO_TRANSFER_MAP_SW_ZS:
      success = tex_staging_sys_map_bo(ilo, tex, true, false);
      if (success) {
         tex_staging_sys_zs_read(ilo, tex, xfer);
         tex_staging_sys_unmap_bo(ilo, tex);
      }
      break;
   default:
      assert(!"unknown mapping method");
      success = false;
      break;
   }

   return success;
}

static void
tex_direct_unmap(struct ilo_context *ilo,
                 struct ilo_texture *tex,
                 struct ilo_transfer *xfer)
{
   intel_bo_unmap(tex->bo);
}

static bool
tex_direct_map(struct ilo_context *ilo,
               struct ilo_texture *tex,
               struct ilo_transfer *xfer)
{
   if (!map_bo_for_transfer(ilo, tex->bo, xfer))
      return false;

   /* note that stride is for a block row, not a texel row */
   xfer->base.stride = tex->bo_stride;

   /* slice stride is not always available */
   if (xfer->base.box.depth > 1)
      xfer->base.layer_stride = tex_get_slice_stride(tex, xfer->base.level);
   else
      xfer->base.layer_stride = 0;

   xfer->ptr = intel_bo_get_virtual(tex->bo);
   xfer->ptr += tex_get_box_offset(tex, xfer->base.level, &xfer->base.box);

   return true;
}

static bool
tex_map(struct ilo_context *ilo, struct ilo_transfer *xfer)
{
   struct ilo_texture *tex = ilo_texture(xfer->base.resource);
   bool success;

   if (!choose_transfer_method(ilo, xfer))
      return false;

   switch (xfer->method) {
   case ILO_TRANSFER_MAP_CPU:
   case ILO_TRANSFER_MAP_GTT:
   case ILO_TRANSFER_MAP_UNSYNC:
      success = tex_direct_map(ilo, tex, xfer);
      break;
   case ILO_TRANSFER_MAP_SW_CONVERT:
   case ILO_TRANSFER_MAP_SW_ZS:
      success = tex_staging_sys_map(ilo, tex, xfer);
      break;
   default:
      assert(!"unknown mapping method");
      success = false;
      break;
   }

   return success;
}

static void
tex_unmap(struct ilo_context *ilo, struct ilo_transfer *xfer)
{
   struct ilo_texture *tex = ilo_texture(xfer->base.resource);

   switch (xfer->method) {
   case ILO_TRANSFER_MAP_CPU:
   case ILO_TRANSFER_MAP_GTT:
   case ILO_TRANSFER_MAP_UNSYNC:
      tex_direct_unmap(ilo, tex, xfer);
      break;
   case ILO_TRANSFER_MAP_SW_CONVERT:
   case ILO_TRANSFER_MAP_SW_ZS:
      tex_staging_sys_unmap(ilo, tex, xfer);
      break;
   default:
      assert(!"unknown mapping method");
      break;
   }
}

static bool
buf_map(struct ilo_context *ilo, struct ilo_transfer *xfer)
{
   struct ilo_buffer *buf = ilo_buffer(xfer->base.resource);

   if (!choose_transfer_method(ilo, xfer))
      return false;

   if (!map_bo_for_transfer(ilo, buf->bo, xfer))
      return false;

   assert(xfer->base.level == 0);
   assert(xfer->base.box.y == 0);
   assert(xfer->base.box.z == 0);
   assert(xfer->base.box.height == 1);
   assert(xfer->base.box.depth == 1);

   xfer->base.stride = 0;
   xfer->base.layer_stride = 0;

   xfer->ptr = intel_bo_get_virtual(buf->bo);
   xfer->ptr += xfer->base.box.x;

   return true;
}

static void
buf_unmap(struct ilo_context *ilo, struct ilo_transfer *xfer)
{
   struct ilo_buffer *buf = ilo_buffer(xfer->base.resource);

   intel_bo_unmap(buf->bo);
}

static void
buf_pwrite(struct ilo_context *ilo, struct ilo_buffer *buf,
           unsigned usage, int offset, int size, const void *data)
{
   bool need_flush;

   /* see if we can avoid stalling */
   if (is_bo_busy(ilo, buf->bo, &need_flush)) {
      bool will_stall = true;

      if (usage & PIPE_TRANSFER_DISCARD_WHOLE_RESOURCE) {
         /* old data not needed so discard the old bo to avoid stalling */
         if (ilo_buffer_alloc_bo(buf)) {
            ilo_mark_states_with_resource_dirty(ilo, &buf->base);
            will_stall = false;
         }
      }
      else {
         /*
          * We could allocate a temporary bo to hold the data and emit
          * pipelined copy blit to move them to buf->bo.  But for now, do
          * nothing.
          */
      }

      /* flush to make bo busy (so that pwrite() stalls as it should be) */
      if (will_stall && need_flush)
         ilo_cp_flush(ilo->cp);
   }

   intel_bo_pwrite(buf->bo, offset, size, data);
}

static void
ilo_transfer_flush_region(struct pipe_context *pipe,
                          struct pipe_transfer *transfer,
                          const struct pipe_box *box)
{
}

static void
ilo_transfer_unmap(struct pipe_context *pipe,
                   struct pipe_transfer *transfer)
{
   struct ilo_context *ilo = ilo_context(pipe);
   struct ilo_transfer *xfer = ilo_transfer(transfer);

   if (xfer->base.resource->target == PIPE_BUFFER)
      buf_unmap(ilo, xfer);
   else
      tex_unmap(ilo, xfer);

   pipe_resource_reference(&xfer->base.resource, NULL);

   util_slab_free(&ilo->transfer_mempool, xfer);
}

static void *
ilo_transfer_map(struct pipe_context *pipe,
                 struct pipe_resource *res,
                 unsigned level,
                 unsigned usage,
                 const struct pipe_box *box,
                 struct pipe_transfer **transfer)
{
   struct ilo_context *ilo = ilo_context(pipe);
   struct ilo_transfer *xfer;
   bool success;

   xfer = util_slab_alloc(&ilo->transfer_mempool);
   if (!xfer) {
      *transfer = NULL;
      return NULL;
   }

   xfer->base.resource = NULL;
   pipe_resource_reference(&xfer->base.resource, res);
   xfer->base.level = level;
   xfer->base.usage = usage;
   xfer->base.box = *box;

   if (res->target == PIPE_BUFFER)
      success = buf_map(ilo, xfer);
   else
      success = tex_map(ilo, xfer);

   if (!success) {
      pipe_resource_reference(&xfer->base.resource, NULL);
      FREE(xfer);
      *transfer = NULL;
      return NULL;
   }

   *transfer = &xfer->base;

   return xfer->ptr;
}

static void
ilo_transfer_inline_write(struct pipe_context *pipe,
                          struct pipe_resource *res,
                          unsigned level,
                          unsigned usage,
                          const struct pipe_box *box,
                          const void *data,
                          unsigned stride,
                          unsigned layer_stride)
{
   if (likely(res->target == PIPE_BUFFER) &&
       !(usage & PIPE_TRANSFER_UNSYNCHRONIZED)) {
      /* they should specify just an offset and a size */
      assert(level == 0);
      assert(box->y == 0);
      assert(box->z == 0);
      assert(box->height == 1);
      assert(box->depth == 1);

      buf_pwrite(ilo_context(pipe), ilo_buffer(res),
            usage, box->x, box->width, data);
   }
   else {
      u_default_transfer_inline_write(pipe, res,
            level, usage, box, data, stride, layer_stride);
   }
}

/**
 * Initialize transfer-related functions.
 */
void
ilo_init_transfer_functions(struct ilo_context *ilo)
{
   ilo->base.transfer_map = ilo_transfer_map;
   ilo->base.transfer_flush_region = ilo_transfer_flush_region;
   ilo->base.transfer_unmap = ilo_transfer_unmap;
   ilo->base.transfer_inline_write = ilo_transfer_inline_write;
}
@


1.1
log
@Initial revision
@
text
@@


1.1.1.1
log
@Import Mesa 9.2.0
@
text
@@


1.1.1.2
log
@Import Mesa 10.2.3
@
text
@a31 1
#include "ilo_blit.h"
d41 1
a41 1
   const bool referenced = intel_bo_has_reloc(ilo->cp->bo, bo);
d52 1
a52 1
static void *
d56 1
a56 1
   void *ptr;
d60 1
a60 1
      ptr = intel_bo_map(bo, (xfer->base.usage & PIPE_TRANSFER_WRITE));
d63 1
a63 1
      ptr = intel_bo_map_gtt(bo);
d66 1
a66 1
      ptr = intel_bo_map_unsynchronized(bo);
d70 1
a70 1
      ptr = NULL;
d74 1
a74 1
   return ptr;
d168 1
a168 1
            ilo_cp_flush(ilo->cp, "syncing for transfers");
a188 2
   const struct ilo_texture_slice *s =
      ilo_texture_get_slice(tex, level, slice + box->z);
d191 2
a192 2
   x = s->x + box->x;
   y = s->y + box->y;
a213 1
   const struct ilo_texture_slice *s0, *s1;
d231 1
a231 3
   s0 = ilo_texture_get_slice(tex, level, 0);
   s1 = ilo_texture_get_slice(tex, level, 1);
   qpitch = s1->y - s0->y;
a402 16
static void *
tex_staging_sys_map_bo(const struct ilo_context *ilo,
                       struct ilo_texture *tex,
                       bool for_read_back, bool linear_view)
{
   const bool prefer_cpu = (ilo->dev->has_llc || for_read_back);
   void *ptr;

   if (prefer_cpu && (tex->tiling == INTEL_TILING_NONE || !linear_view))
      ptr = intel_bo_map(tex->bo, !for_read_back);
   else
      ptr = intel_bo_map_gtt(tex->bo);

   return ptr;
}

a403 7
tex_staging_sys_unmap_bo(const struct ilo_context *ilo,
                         const struct ilo_texture *tex)
{
   intel_bo_unmap(tex->bo);
}

static bool
d410 1
a410 1
   const uint8_t *src;
a414 4
   src = tex_staging_sys_map_bo(ilo, tex, true, false);
   if (!src)
      return false;

d421 1
a421 1
      const uint8_t *s8_src;
a425 6
      s8_src = tex_staging_sys_map_bo(ilo, s8_tex, true, false);
      if (!s8_src) {
         tex_staging_sys_unmap_bo(ilo, tex);
         return false;
      }

a478 2

      tex_staging_sys_unmap_bo(ilo, s8_tex);
a511 4

   tex_staging_sys_unmap_bo(ilo, tex);

   return true;
d514 1
a514 1
static bool
d521 1
a521 1
   uint8_t *dst;
a525 4
   dst = tex_staging_sys_map_bo(ilo, tex, false, false);
   if (!dst)
      return false;

d532 1
a532 1
      uint8_t *s8_dst;
a536 6
      s8_dst = tex_staging_sys_map_bo(ilo, s8_tex, false, false);
      if (!s8_dst) {
         tex_staging_sys_unmap_bo(ilo, s8_tex);
         return false;
      }

a589 2

      tex_staging_sys_unmap_bo(ilo, s8_tex);
a622 4

   tex_staging_sys_unmap_bo(ilo, tex);

   return true;
d625 1
a625 1
static bool
d635 1
a635 4
   dst = tex_staging_sys_map_bo(ilo, tex, false, true);
   if (!dst)
      return false;

d649 1
a649 4

      tex_staging_sys_unmap_bo(ilo, tex);

      return true;
d671 24
d696 6
a701 1
   tex_staging_sys_unmap_bo(ilo, tex);
d703 1
a703 1
   return true;
d720 5
a724 1
      success = tex_staging_sys_convert_write(ilo, tex, xfer);
d727 5
a731 1
      success = tex_staging_sys_zs_write(ilo, tex, xfer);
d785 5
a789 1
      success = tex_staging_sys_zs_read(ilo, tex, xfer);
d813 1
a813 2
   xfer->ptr = map_bo_for_transfer(ilo, tex->bo, xfer);
   if (!xfer->ptr)
a815 2
   xfer->ptr += tex_get_box_offset(tex, xfer->base.level, &xfer->base.box);

d825 3
d888 1
a888 2
   xfer->ptr = map_bo_for_transfer(ilo, buf->bo, xfer);
   if (!xfer->ptr)
a896 1
   xfer->ptr += xfer->base.box.x;
d900 3
d941 1
a941 1
         ilo_cp_flush(ilo->cp, "syncing for pwrites");
a993 2

   ilo_blit_resolve_transfer(ilo, &xfer->base);
@


1.1.1.3
log
@Import Mesa 10.4.3
@
text
@a32 1
#include "ilo_blitter.h"
a38 111
/*
 * For buffers that are not busy, we want to map/unmap them directly.  For
 * those that are busy, we have to worry about synchronization.  We could wait
 * for GPU to finish, but there are cases where we could avoid waiting.
 *
 *  - When PIPE_TRANSFER_DISCARD_WHOLE_RESOURCE is set, the contents of the
 *    buffer can be discarded.  We can replace the backing bo by a new one of
 *    the same size (renaming).
 *  - When PIPE_TRANSFER_DISCARD_RANGE is set, the contents of the mapped
 *    range can be discarded.  We can allocate and map a staging bo on
 *    mapping, and (pipelined-)copy it over to the real bo on unmapping.
 *  - When PIPE_TRANSFER_FLUSH_EXPLICIT is set, there is no reading and only
 *    flushed regions need to be written.  We can still allocate and map a
 *    staging bo, but should copy only the flushed regions over.
 *
 * However, there are other flags to consider.
 *
 *  - When PIPE_TRANSFER_UNSYNCHRONIZED is set, we do not need to worry about
 *    synchronization at all on mapping.
 *  - When PIPE_TRANSFER_MAP_DIRECTLY is set, no staging area is allowed.
 *  - When PIPE_TRANSFER_DONTBLOCK is set, we should fail if we have to block.
 *  - When PIPE_TRANSFER_PERSISTENT is set, GPU may access the buffer while it
 *    is mapped.  Synchronization is done by defining memory barriers,
 *    explicitly via memory_barrier() or implicitly via
 *    transfer_flush_region(), as well as GPU fences.
 *  - When PIPE_TRANSFER_COHERENT is set, updates by either CPU or GPU should
 *    be made visible to the other side immediately.  Since the kernel flushes
 *    GPU caches at the end of each batch buffer, CPU always sees GPU updates.
 *    We could use a coherent mapping to make all persistent mappings
 *    coherent.
 *
 * These also apply to textures, except that we may additionally need to do
 * format conversion or tiling/untiling.
 */

/**
 * Return a transfer method suitable for the usage.  The returned method will
 * correctly block when the resource is busy.
 */
static bool
resource_get_transfer_method(struct pipe_resource *res,
                             const struct pipe_transfer *transfer,
                             enum ilo_transfer_map_method *method)
{
   const struct ilo_screen *is = ilo_screen(res->screen);
   const unsigned usage = transfer->usage;
   enum ilo_transfer_map_method m;
   bool tiled;

   if (res->target == PIPE_BUFFER) {
      tiled = false;
   }
   else {
      struct ilo_texture *tex = ilo_texture(res);
      bool need_convert = false;

      /* we may need to convert on the fly */
      if (tex->separate_s8 || tex->layout.format == PIPE_FORMAT_S8_UINT) {
         /* on GEN6, separate stencil is enabled only when HiZ is */
         if (ilo_dev_gen(&is->dev) >= ILO_GEN(7) ||
             ilo_texture_can_enable_hiz(tex, transfer->level,
                transfer->box.z, transfer->box.depth)) {
            m = ILO_TRANSFER_MAP_SW_ZS;
            need_convert = true;
         }
      } else if (tex->layout.format != tex->base.format) {
         m = ILO_TRANSFER_MAP_SW_CONVERT;
         need_convert = true;
      }

      if (need_convert) {
         if (usage & (PIPE_TRANSFER_MAP_DIRECTLY | PIPE_TRANSFER_PERSISTENT))
            return false;

         *method = m;
         return true;
      }

      tiled = (tex->layout.tiling != INTEL_TILING_NONE);
   }

   if (tiled)
      m = ILO_TRANSFER_MAP_GTT; /* to have a linear view */
   else if (is->dev.has_llc)
      m = ILO_TRANSFER_MAP_CPU; /* fast and mostly coherent */
   else if (usage & PIPE_TRANSFER_PERSISTENT)
      m = ILO_TRANSFER_MAP_GTT; /* for coherency */
   else if (usage & PIPE_TRANSFER_READ)
      m = ILO_TRANSFER_MAP_CPU; /* gtt read is too slow */
   else
      m = ILO_TRANSFER_MAP_GTT;

   *method = m;

   return true;
}

/**
 * Rename the bo of the resource.
 */
static bool
resource_rename_bo(struct pipe_resource *res)
{
   return (res->target == PIPE_BUFFER) ?
      ilo_buffer_rename_bo(ilo_buffer(res)) :
      ilo_texture_rename_bo(ilo_texture(res));
}

/**
 * Return true if usage allows the use of staging bo to avoid blocking.
 */
d40 1
a40 1
usage_allows_staging_bo(unsigned usage)
d42 1
a42 7
   /* do we know how to write the data back to the resource? */
   const unsigned can_writeback = (PIPE_TRANSFER_DISCARD_WHOLE_RESOURCE |
                                   PIPE_TRANSFER_DISCARD_RANGE |
                                   PIPE_TRANSFER_FLUSH_EXPLICIT);
   const unsigned reasons_against = (PIPE_TRANSFER_READ |
                                     PIPE_TRANSFER_MAP_DIRECTLY |
                                     PIPE_TRANSFER_PERSISTENT);
d44 2
a45 2
   return (usage & can_writeback) && !(usage & reasons_against);
}
d47 2
a48 10
/**
 * Allocate the staging resource.  It is always linear and its size matches
 * the transfer box, with proper paddings.
 */
static bool
xfer_alloc_staging_res(struct ilo_transfer *xfer)
{
   const struct pipe_resource *res = xfer->base.resource;
   const struct pipe_box *box = &xfer->base.box;
   struct pipe_resource templ;
d50 1
a50 35
   memset(&templ, 0, sizeof(templ));

   templ.format = res->format;

   if (res->target == PIPE_BUFFER) {
      templ.target = PIPE_BUFFER;
      templ.width0 =
         (box->x % ILO_TRANSFER_MAP_BUFFER_ALIGNMENT) + box->width;
   }
   else {
      /* use 2D array for any texture target */
      templ.target = PIPE_TEXTURE_2D_ARRAY;
      templ.width0 = box->width;
   }

   templ.height0 = box->height;
   templ.depth0 = 1;
   templ.array_size = box->depth;
   templ.nr_samples = 1;
   templ.usage = PIPE_USAGE_STAGING;
   templ.bind = PIPE_BIND_TRANSFER_WRITE;

   if (xfer->base.usage & PIPE_TRANSFER_FLUSH_EXPLICIT) {
      templ.flags = PIPE_RESOURCE_FLAG_MAP_PERSISTENT |
                    PIPE_RESOURCE_FLAG_MAP_COHERENT;
   }

   xfer->staging.res = res->screen->resource_create(res->screen, &templ);

   if (xfer->staging.res && xfer->staging.res->target != PIPE_BUFFER) {
      assert(ilo_texture(xfer->staging.res)->layout.tiling ==
            INTEL_TILING_NONE);
   }

   return (xfer->staging.res != NULL);
d53 3
a55 6
/**
 * Use an alternative transfer method or rename the resource to unblock an
 * otherwise blocking transfer.
 */
static bool
xfer_unblock(struct ilo_transfer *xfer, bool *resource_renamed)
d57 1
a57 2
   struct pipe_resource *res = xfer->base.resource;
   bool unblocked = false, renamed = false;
d61 2
d64 1
a64 14
      if (xfer->base.usage & PIPE_TRANSFER_UNSYNCHRONIZED) {
         xfer->method = ILO_TRANSFER_MAP_GTT_ASYNC;
         unblocked = true;
      }
      else if ((xfer->base.usage & PIPE_TRANSFER_DISCARD_WHOLE_RESOURCE) &&
               resource_rename_bo(res)) {
         renamed = true;
         unblocked = true;
      }
      else if (usage_allows_staging_bo(xfer->base.usage) &&
               xfer_alloc_staging_res(xfer)) {
         xfer->method = ILO_TRANSFER_MAP_STAGING;
         unblocked = true;
      }
d66 2
a67 3
   case ILO_TRANSFER_MAP_GTT_ASYNC:
   case ILO_TRANSFER_MAP_STAGING:
      unblocked = true;
d70 2
d75 1
a75 3
   *resource_renamed = renamed;

   return unblocked;
d79 2
a80 2
 * Allocate the staging system buffer based on the resource format and the
 * transfer box.
d83 1
a83 1
xfer_alloc_staging_sys(struct ilo_transfer *xfer)
d85 9
a93 3
   const enum pipe_format format = xfer->base.resource->format;
   const struct pipe_box *box = &xfer->base.box;
   const unsigned alignment = 64;
d95 2
a96 5
   /* need to tell the world the layout */
   xfer->base.stride =
      align(util_format_get_stride(format, box->width), alignment);
   xfer->base.layer_stride =
      util_format_get_2d_size(format, xfer->base.stride, box->height);
d98 6
a103 2
   xfer->staging.sys =
      align_malloc(xfer->base.layer_stride * box->depth, alignment);
d105 4
a108 2
   return (xfer->staging.sys != NULL);
}
d110 14
a123 24
/**
 * Map according to the method.  The staging system buffer should have been
 * allocated if the method requires it.
 */
static void *
xfer_map(struct ilo_transfer *xfer)
{
   void *ptr;

   switch (xfer->method) {
   case ILO_TRANSFER_MAP_CPU:
      ptr = intel_bo_map(ilo_resource_get_bo(xfer->base.resource),
            xfer->base.usage & PIPE_TRANSFER_WRITE);
      break;
   case ILO_TRANSFER_MAP_GTT:
      ptr = intel_bo_map_gtt(ilo_resource_get_bo(xfer->base.resource));
      break;
   case ILO_TRANSFER_MAP_GTT_ASYNC:
      ptr = intel_bo_map_gtt_async(ilo_resource_get_bo(xfer->base.resource));
      break;
   case ILO_TRANSFER_MAP_STAGING:
      {
         const struct ilo_screen *is = ilo_screen(xfer->staging.res->screen);
         struct intel_bo *bo = ilo_resource_get_bo(xfer->staging.res);
d125 30
d156 4
a159 3
          * We want a writable, optionally persistent and coherent, mapping
          * for a linear bo.  We can call resource_get_transfer_method(), but
          * this turns out to be fairly simple.
d161 1
a161 4
         if (is->dev.has_llc)
            ptr = intel_bo_map(bo, true);
         else
            ptr = intel_bo_map_gtt(bo);
d163 3
a165 2
         if (ptr && xfer->staging.res->target == PIPE_BUFFER)
            ptr += (xfer->base.box.x % ILO_TRANSFER_MAP_BUFFER_ALIGNMENT);
d167 3
a170 9
      break;
   case ILO_TRANSFER_MAP_SW_CONVERT:
   case ILO_TRANSFER_MAP_SW_ZS:
      ptr = xfer->staging.sys;
      break;
   default:
      assert(!"unknown mapping method");
      ptr = NULL;
      break;
d173 7
a179 2
   return ptr;
}
d181 1
a181 18
/**
 * Unmap a transfer.
 */
static void
xfer_unmap(struct ilo_transfer *xfer)
{
   switch (xfer->method) {
   case ILO_TRANSFER_MAP_CPU:
   case ILO_TRANSFER_MAP_GTT:
   case ILO_TRANSFER_MAP_GTT_ASYNC:
      intel_bo_unmap(ilo_resource_get_bo(xfer->base.resource));
      break;
   case ILO_TRANSFER_MAP_STAGING:
      intel_bo_unmap(ilo_resource_get_bo(xfer->staging.res));
      break;
   default:
      break;
   }
d190 2
d194 2
a195 3
   ilo_layout_get_slice_pos(&tex->layout, level, box->z + slice, &x, &y);
   x += box->x;
   y += box->y;
d197 4
a200 1
   ilo_layout_pos_to_mem(&tex->layout, x, y, mem_x, mem_y);
d211 1
a211 1
   return ilo_layout_mem_to_linear(&tex->layout, mem_x, mem_y);
d217 24
a240 1
   return ilo_layout_get_slice_stride(&tex->layout, level);
d388 1
a388 1
   switch (tex->layout.tiling) {
d390 1
a390 1
      *tiles_per_row = tex->layout.bo_stride / 512;
d393 1
a393 1
      *tiles_per_row = tex->layout.bo_stride / 128;
d398 2
a399 2
      if (tex->layout.format == PIPE_FORMAT_S8_UINT) {
         *tiles_per_row = tex->layout.bo_stride / 64;
d403 1
a403 1
         *tiles_per_row = tex->layout.bo_stride;
d410 3
a412 3
tex_staging_sys_map_bo(struct ilo_texture *tex,
                       bool for_read_back,
                       bool linear_view)
d414 1
a414 2
   const struct ilo_screen *is = ilo_screen(tex->base.screen);
   const bool prefer_cpu = (is->dev.has_llc || for_read_back);
d417 1
a417 2
   if (prefer_cpu && (tex->layout.tiling == INTEL_TILING_NONE ||
                      !linear_view))
d426 2
a427 1
tex_staging_sys_unmap_bo(struct ilo_texture *tex)
d433 2
a434 1
tex_staging_sys_zs_read(struct ilo_texture *tex,
d437 1
a437 2
   const struct ilo_screen *is = ilo_screen(tex->base.screen);
   const bool swizzle = is->dev.has_address_swizzling;
d444 1
a444 1
   src = tex_staging_sys_map_bo(tex, true, false);
d450 1
a450 1
   assert(tex->layout.block_width == 1 && tex->layout.block_height == 1);
d459 1
a459 1
      s8_src = tex_staging_sys_map_bo(s8_tex, true, false);
d461 1
a461 1
         tex_staging_sys_unmap_bo(tex);
d468 1
a468 1
         assert(tex->layout.format == PIPE_FORMAT_Z24X8_UNORM);
d476 1
a476 1
         assert(tex->layout.format == PIPE_FORMAT_Z32_FLOAT);
d493 1
a493 1
         dst = xfer->staging.sys + xfer->base.layer_stride * slice;
d509 1
a509 1
               x += tex->layout.block_size;
d519 1
a519 1
      tex_staging_sys_unmap_bo(s8_tex);
d522 1
a522 1
      assert(tex->layout.format == PIPE_FORMAT_S8_UINT);
d532 1
a532 1
         dst = xfer->staging.sys + xfer->base.layer_stride * slice;
d554 1
a554 1
   tex_staging_sys_unmap_bo(tex);
d560 2
a561 1
tex_staging_sys_zs_write(struct ilo_texture *tex,
d564 1
a564 2
   const struct ilo_screen *is = ilo_screen(tex->base.screen);
   const bool swizzle = is->dev.has_address_swizzling;
d571 1
a571 1
   dst = tex_staging_sys_map_bo(tex, false, false);
d577 1
a577 1
   assert(tex->layout.block_width == 1 && tex->layout.block_height == 1);
d586 1
a586 1
      s8_dst = tex_staging_sys_map_bo(s8_tex, false, false);
d588 1
a588 1
         tex_staging_sys_unmap_bo(s8_tex);
d595 1
a595 1
         assert(tex->layout.format == PIPE_FORMAT_Z24X8_UNORM);
d603 1
a603 1
         assert(tex->layout.format == PIPE_FORMAT_Z32_FLOAT);
d620 1
a620 1
         src = xfer->staging.sys + xfer->base.layer_stride * slice;
d636 1
a636 1
               x += tex->layout.block_size;
d646 1
a646 1
      tex_staging_sys_unmap_bo(s8_tex);
d649 1
a649 1
      assert(tex->layout.format == PIPE_FORMAT_S8_UINT);
d659 1
a659 1
         src = xfer->staging.sys + xfer->base.layer_stride * slice;
d681 1
a681 1
   tex_staging_sys_unmap_bo(tex);
d687 2
a688 1
tex_staging_sys_convert_write(struct ilo_texture *tex,
d696 1
a696 1
   dst = tex_staging_sys_map_bo(tex, false, true);
d708 4
a711 4
   if (unlikely(tex->layout.format == tex->base.format)) {
      util_copy_box(dst, tex->layout.format, tex->layout.bo_stride,
            dst_slice_stride, 0, 0, 0, box->width, box->height, box->depth,
            xfer->staging.sys, xfer->base.stride, xfer->base.layer_stride,
d714 1
a714 1
      tex_staging_sys_unmap_bo(tex);
d721 1
a721 1
      assert(tex->layout.format == PIPE_FORMAT_R8G8B8X8_UNORM);
d725 1
a725 1
            xfer->staging.sys + xfer->base.layer_stride * slice;
d728 1
a728 1
               tex->layout.bo_stride, src, xfer->base.stride,
d739 1
a739 1
   tex_staging_sys_unmap_bo(tex);
d745 3
a747 1
tex_staging_sys_writeback(struct ilo_transfer *xfer)
a748 1
   struct ilo_texture *tex = ilo_texture(xfer->base.resource);
d751 2
a752 1
   if (!(xfer->base.usage & PIPE_TRANSFER_WRITE))
d754 1
d758 1
a758 1
      success = tex_staging_sys_convert_write(tex, xfer);
d761 1
a761 1
      success = tex_staging_sys_zs_write(tex, xfer);
d771 2
d776 3
a778 1
tex_staging_sys_readback(struct ilo_transfer *xfer)
d780 4
a783 1
   struct ilo_texture *tex = ilo_texture(xfer->base.resource);
d786 8
d815 1
a815 1
      success = tex_staging_sys_zs_read(tex, xfer);
d826 33
a858 2
static void *
tex_map(struct ilo_transfer *xfer)
d860 5
a864 1
   void *ptr;
d869 2
a870 21
   case ILO_TRANSFER_MAP_GTT_ASYNC:
      ptr = xfer_map(xfer);
      if (ptr) {
         const struct ilo_texture *tex = ilo_texture(xfer->base.resource);

         ptr += tex_get_box_offset(tex, xfer->base.level, &xfer->base.box);

         /* stride is for a block row, not a texel row */
         xfer->base.stride = tex->layout.bo_stride;
         /* note that slice stride is not always available */
         xfer->base.layer_stride = (xfer->base.box.depth > 1) ?
            tex_get_slice_stride(tex, xfer->base.level) : 0;
      }
      break;
   case ILO_TRANSFER_MAP_STAGING:
      ptr = xfer_map(xfer);
      if (ptr) {
         const struct ilo_texture *staging = ilo_texture(xfer->staging.res);
         xfer->base.stride = staging->layout.bo_stride;
         xfer->base.layer_stride = tex_get_slice_stride(staging, 0);
      }
d874 1
a874 4
      if (xfer_alloc_staging_sys(xfer) && tex_staging_sys_readback(xfer))
         ptr = xfer_map(xfer);
      else
         ptr = NULL;
d878 1
a878 1
      ptr = NULL;
d882 1
a882 1
   return ptr;
d885 2
a886 2
static void *
buf_map(struct ilo_transfer *xfer)
d888 17
a904 1
   void *ptr;
d906 4
a909 3
   ptr = xfer_map(xfer);
   if (!ptr)
      return NULL;
d911 2
a912 2
   if (xfer->method != ILO_TRANSFER_MAP_STAGING)
      ptr += xfer->base.box.x;
d914 3
a916 2
   xfer->base.stride = 0;
   xfer->base.layer_stride = 0;
d924 5
a928 1
   return ptr;
d932 1
a932 29
copy_staging_resource(struct ilo_context *ilo,
                      struct ilo_transfer *xfer,
                      const struct pipe_box *box)
{
   const unsigned pad_x = (xfer->staging.res->target == PIPE_BUFFER) ?
      xfer->base.box.x % ILO_TRANSFER_MAP_BUFFER_ALIGNMENT : 0;
   struct pipe_box modified_box;

   assert(xfer->method == ILO_TRANSFER_MAP_STAGING && xfer->staging.res);

   if (!box) {
      u_box_3d(pad_x, 0, 0, xfer->base.box.width, xfer->base.box.height,
            xfer->base.box.depth, &modified_box);
      box = &modified_box;
   }
   else if (pad_x) {
      modified_box = *box;
      modified_box.x += pad_x;
      box = &modified_box;
   }

   ilo_blitter_blt_copy_resource(ilo->blitter,
         xfer->base.resource, xfer->base.level,
         xfer->base.box.x, xfer->base.box.y, xfer->base.box.z,
         xfer->staging.res, 0, box);
}

static bool
is_bo_busy(struct ilo_context *ilo, struct intel_bo *bo, bool *need_submit)
d934 1
a934 7
   const bool referenced = ilo_builder_has_reloc(&ilo->cp->builder, bo);

   if (need_submit)
      *need_submit = referenced;

   if (referenced)
      return true;
d936 1
a936 34
   return intel_bo_is_busy(bo);
}

/**
 * Choose the best mapping method, depending on the transfer usage and whether
 * the bo is busy.
 */
static bool
choose_transfer_method(struct ilo_context *ilo, struct ilo_transfer *xfer)
{
   struct pipe_resource *res = xfer->base.resource;
   bool need_submit;

   if (!resource_get_transfer_method(res, &xfer->base, &xfer->method))
      return false;

   /* see if we can avoid blocking */
   if (is_bo_busy(ilo, ilo_resource_get_bo(res), &need_submit)) {
      bool resource_renamed;

      if (!xfer_unblock(xfer, &resource_renamed)) {
         if (xfer->base.usage & PIPE_TRANSFER_DONTBLOCK)
            return false;

         /* submit to make bo really busy and map() correctly blocks */
         if (need_submit)
            ilo_cp_submit(ilo->cp, "syncing for transfers");
      }

      if (resource_renamed)
         ilo_state_vector_resource_renamed(&ilo->state_vector, res);
   }

   return true;
d943 1
a943 1
   bool need_submit;
d945 10
a954 8
   /* see if we can avoid blocking */
   if (is_bo_busy(ilo, buf->bo, &need_submit)) {
      bool unblocked = false;

      if ((usage & PIPE_TRANSFER_DISCARD_WHOLE_RESOURCE) &&
          ilo_buffer_rename_bo(buf)) {
         ilo_state_vector_resource_renamed(&ilo->state_vector, &buf->base);
         unblocked = true;
a956 2
         struct pipe_resource templ, *staging;

d958 3
a960 2
          * allocate a staging buffer to hold the data and pipelined copy it
          * over
a961 19
         templ = buf->base;
         templ.width0 = size;
         templ.usage = PIPE_USAGE_STAGING;
         templ.bind = PIPE_BIND_TRANSFER_WRITE;
         staging = ilo->base.screen->resource_create(ilo->base.screen, &templ);
         if (staging) {
            struct pipe_box staging_box;

            intel_bo_pwrite(ilo_buffer(staging)->bo, 0, size, data);

            u_box_1d(0, size, &staging_box);
            ilo_blitter_blt_copy_resource(ilo->blitter,
                  &buf->base, 0, offset, 0, 0,
                  staging, 0, &staging_box);

            pipe_resource_reference(&staging, NULL);

            return;
         }
d964 3
a966 3
      /* submit to make bo really busy and pwrite() correctly blocks */
      if (!unblocked && need_submit)
         ilo_cp_submit(ilo->cp, "syncing for pwrites");
a976 10
   struct ilo_context *ilo = ilo_context(pipe);
   struct ilo_transfer *xfer = ilo_transfer(transfer);

   /*
    * The staging resource is mapped persistently and coherently.  We can copy
    * without unmapping.
    */
   if (xfer->method == ILO_TRANSFER_MAP_STAGING &&
       (xfer->base.usage & PIPE_TRANSFER_FLUSH_EXPLICIT))
      copy_staging_resource(ilo, xfer, box);
d986 4
a989 16
   xfer_unmap(xfer);

   switch (xfer->method) {
   case ILO_TRANSFER_MAP_STAGING:
      if (!(xfer->base.usage & PIPE_TRANSFER_FLUSH_EXPLICIT))
         copy_staging_resource(ilo, xfer, NULL);
      pipe_resource_reference(&xfer->staging.res, NULL);
      break;
   case ILO_TRANSFER_MAP_SW_CONVERT:
   case ILO_TRANSFER_MAP_SW_ZS:
      tex_staging_sys_writeback(xfer);
      align_free(xfer->staging.sys);
      break;
   default:
      break;
   }
d1006 1
a1006 1
   void *ptr;
a1007 1
   /* note that xfer is not zero'd */
d1022 4
a1025 9
   if (choose_transfer_method(ilo, xfer)) {
      if (res->target == PIPE_BUFFER)
         ptr = buf_map(xfer);
      else
         ptr = tex_map(xfer);
   }
   else {
      ptr = NULL;
   }
d1027 1
a1027 1
   if (!ptr) {
d1029 1
a1029 1
      util_slab_free(&ilo->transfer_mempool, xfer);
d1036 1
a1036 1
   return ptr;
@


1.1.1.4
log
@Import Mesa 10.2.9
@
text
@d33 1
d40 111
d152 1
a152 1
is_bo_busy(struct ilo_context *ilo, struct intel_bo *bo, bool *need_flush)
d154 21
a174 1
   const bool referenced = intel_bo_has_reloc(ilo->cp->bo, bo);
d176 1
a176 2
   if (need_flush)
      *need_flush = referenced;
d178 31
a208 2
   if (referenced)
      return true;
d210 1
a210 1
   return intel_bo_is_busy(bo);
d213 6
a218 3
static void *
map_bo_for_transfer(struct ilo_context *ilo, struct intel_bo *bo,
                    const struct ilo_transfer *xfer)
d220 2
a221 1
   void *ptr;
a224 2
      ptr = intel_bo_map(bo, (xfer->base.usage & PIPE_TRANSFER_WRITE));
      break;
d226 14
a239 1
      ptr = intel_bo_map_gtt(bo);
d241 3
a243 2
   case ILO_TRANSFER_MAP_UNSYNC:
      ptr = intel_bo_map_unsynchronized(bo);
a245 2
      assert(!"unknown mapping method");
      ptr = NULL;
d249 3
a251 1
   return ptr;
d255 2
a256 2
 * Choose the best mapping method, depending on the transfer usage and whether
 * the bo is busy.
d259 1
a259 1
choose_transfer_method(struct ilo_context *ilo, struct ilo_transfer *xfer)
d261 9
a269 9
   struct pipe_resource *res = xfer->base.resource;
   const unsigned usage = xfer->base.usage;
   /* prefer map() when there is the last-level cache */
   const bool prefer_cpu =
      (ilo->dev->has_llc || (usage & PIPE_TRANSFER_READ));
   struct ilo_texture *tex;
   struct ilo_buffer *buf;
   struct intel_bo *bo;
   bool tiled, need_flush;
d271 2
a272 2
   if (res->target == PIPE_BUFFER) {
      tex = NULL;
d274 2
a275 6
      buf = ilo_buffer(res);
      bo = buf->bo;
      tiled = false;
   }
   else {
      buf = NULL;
d277 8
a284 4
      tex = ilo_texture(res);
      bo = tex->bo;
      tiled = (tex->tiling != INTEL_TILING_NONE);
   }
d286 15
a300 14
   /* choose between mapping through CPU or GTT */
   if (usage & PIPE_TRANSFER_MAP_DIRECTLY) {
      /* we do not want fencing */
      if (tiled || prefer_cpu)
         xfer->method = ILO_TRANSFER_MAP_CPU;
      else
         xfer->method = ILO_TRANSFER_MAP_GTT;
   }
   else {
      if (!tiled && prefer_cpu)
         xfer->method = ILO_TRANSFER_MAP_CPU;
      else
         xfer->method = ILO_TRANSFER_MAP_GTT;
   }
a301 21
   /* see if we can avoid stalling */
   if (is_bo_busy(ilo, bo, &need_flush)) {
      bool will_stall = true;

      if (usage & PIPE_TRANSFER_MAP_DIRECTLY) {
         /* nothing we can do */
      }
      else if (usage & PIPE_TRANSFER_UNSYNCHRONIZED) {
         /* unsynchronized gtt mapping does not stall */
         xfer->method = ILO_TRANSFER_MAP_UNSYNC;
         will_stall = false;
      }
      else if (usage & PIPE_TRANSFER_DISCARD_WHOLE_RESOURCE) {
         /* discard old bo and allocate a new one for mapping */
         if ((tex && ilo_texture_alloc_bo(tex)) ||
             (buf && ilo_buffer_alloc_bo(buf))) {
            ilo_mark_states_with_resource_dirty(ilo, res);
            will_stall = false;
         }
      }
      else if (usage & PIPE_TRANSFER_FLUSH_EXPLICIT) {
d303 3
a305 5
          * We could allocate and return a system buffer here.  When a region of
          * the buffer is explicitly flushed, we pwrite() the region to a
          * temporary bo and emit pipelined copy blit.
          *
          * For now, do nothing.
d307 4
a310 9
      }
      else if (usage & PIPE_TRANSFER_DISCARD_RANGE) {
         /*
          * We could allocate a temporary bo for mapping, and emit pipelined copy
          * blit upon unmapping.
          *
          * For now, do nothing.
          */
      }
d312 2
a313 3
      if (will_stall) {
         if (usage & PIPE_TRANSFER_DONTBLOCK)
            return false;
a314 3
         /* flush to make bo busy (so that map() stalls as it should be) */
         if (need_flush)
            ilo_cp_flush(ilo->cp, "syncing for transfers");
d316 9
d327 20
a346 6
   if (tex && !(usage & PIPE_TRANSFER_MAP_DIRECTLY)) {
      if (tex->separate_s8 || tex->bo_format == PIPE_FORMAT_S8_UINT)
         xfer->method = ILO_TRANSFER_MAP_SW_ZS;
      /* need to convert on-the-fly */
      else if (tex->bo_format != tex->base.format)
         xfer->method = ILO_TRANSFER_MAP_SW_CONVERT;
a347 2

   return true;
a355 2
   const struct ilo_texture_slice *s =
      ilo_texture_get_slice(tex, level, slice + box->z);
d358 3
a360 2
   x = s->x + box->x;
   y = s->y + box->y;
d362 1
a362 4
   assert(x % tex->block_width == 0 && y % tex->block_height == 0);

   *mem_x = x / tex->block_width * tex->bo_cpp;
   *mem_y = y / tex->block_height;
d373 1
a373 1
   return mem_y * tex->bo_stride + mem_x;
d379 1
a379 24
   const struct ilo_texture_slice *s0, *s1;
   unsigned qpitch;

   /* there is no 3D array texture */
   assert(tex->base.array_size == 1 || tex->base.depth0 == 1);

   if (tex->base.array_size == 1) {
      /* non-array, non-3D */
      if (tex->base.depth0 == 1)
         return 0;

      /* only the first level has a fixed slice stride */
      if (level > 0) {
         assert(!"no slice stride for 3D texture with level > 0");
         return 0;
      }
   }

   s0 = ilo_texture_get_slice(tex, level, 0);
   s1 = ilo_texture_get_slice(tex, level, 1);
   qpitch = s1->y - s0->y;
   assert(qpitch % tex->block_height == 0);

   return (qpitch / tex->block_height) * tex->bo_stride;
d527 1
a527 1
   switch (tex->tiling) {
d529 1
a529 1
      *tiles_per_row = tex->bo_stride / 512;
d532 1
a532 1
      *tiles_per_row = tex->bo_stride / 128;
d537 2
a538 2
      if (tex->bo_format == PIPE_FORMAT_S8_UINT) {
         *tiles_per_row = tex->bo_stride / 64;
d542 1
a542 1
         *tiles_per_row = tex->bo_stride;
d549 3
a551 3
tex_staging_sys_map_bo(const struct ilo_context *ilo,
                       struct ilo_texture *tex,
                       bool for_read_back, bool linear_view)
d553 2
a554 1
   const bool prefer_cpu = (ilo->dev->has_llc || for_read_back);
d557 2
a558 1
   if (prefer_cpu && (tex->tiling == INTEL_TILING_NONE || !linear_view))
d567 1
a567 2
tex_staging_sys_unmap_bo(const struct ilo_context *ilo,
                         const struct ilo_texture *tex)
d573 1
a573 2
tex_staging_sys_zs_read(struct ilo_context *ilo,
                        struct ilo_texture *tex,
d576 2
a577 1
   const bool swizzle = ilo->dev->has_address_swizzling;
d584 1
a584 1
   src = tex_staging_sys_map_bo(ilo, tex, true, false);
d590 1
a590 1
   assert(tex->block_width == 1 && tex->block_height == 1);
d599 1
a599 1
      s8_src = tex_staging_sys_map_bo(ilo, s8_tex, true, false);
d601 1
a601 1
         tex_staging_sys_unmap_bo(ilo, tex);
d608 1
a608 1
         assert(tex->bo_format == PIPE_FORMAT_Z24X8_UNORM);
d616 1
a616 1
         assert(tex->bo_format == PIPE_FORMAT_Z32_FLOAT);
d633 1
a633 1
         dst = xfer->staging_sys + xfer->base.layer_stride * slice;
d649 1
a649 1
               x += tex->bo_cpp;
d659 1
a659 1
      tex_staging_sys_unmap_bo(ilo, s8_tex);
d662 1
a662 1
      assert(tex->bo_format == PIPE_FORMAT_S8_UINT);
d672 1
a672 1
         dst = xfer->staging_sys + xfer->base.layer_stride * slice;
d694 1
a694 1
   tex_staging_sys_unmap_bo(ilo, tex);
d700 1
a700 2
tex_staging_sys_zs_write(struct ilo_context *ilo,
                         struct ilo_texture *tex,
d703 2
a704 1
   const bool swizzle = ilo->dev->has_address_swizzling;
d711 1
a711 1
   dst = tex_staging_sys_map_bo(ilo, tex, false, false);
d717 1
a717 1
   assert(tex->block_width == 1 && tex->block_height == 1);
d726 1
a726 1
      s8_dst = tex_staging_sys_map_bo(ilo, s8_tex, false, false);
d728 1
a728 1
         tex_staging_sys_unmap_bo(ilo, s8_tex);
d735 1
a735 1
         assert(tex->bo_format == PIPE_FORMAT_Z24X8_UNORM);
d743 1
a743 1
         assert(tex->bo_format == PIPE_FORMAT_Z32_FLOAT);
d760 1
a760 1
         src = xfer->staging_sys + xfer->base.layer_stride * slice;
d776 1
a776 1
               x += tex->bo_cpp;
d786 1
a786 1
      tex_staging_sys_unmap_bo(ilo, s8_tex);
d789 1
a789 1
      assert(tex->bo_format == PIPE_FORMAT_S8_UINT);
d799 1
a799 1
         src = xfer->staging_sys + xfer->base.layer_stride * slice;
d821 1
a821 1
   tex_staging_sys_unmap_bo(ilo, tex);
d827 1
a827 2
tex_staging_sys_convert_write(struct ilo_context *ilo,
                              struct ilo_texture *tex,
d835 1
a835 1
   dst = tex_staging_sys_map_bo(ilo, tex, false, true);
d847 4
a850 4
   if (unlikely(tex->bo_format == tex->base.format)) {
      util_copy_box(dst, tex->bo_format, tex->bo_stride, dst_slice_stride,
            0, 0, 0, box->width, box->height, box->depth,
            xfer->staging_sys, xfer->base.stride, xfer->base.layer_stride,
d853 1
a853 1
      tex_staging_sys_unmap_bo(ilo, tex);
d860 1
a860 1
      assert(tex->bo_format == PIPE_FORMAT_R8G8B8X8_UNORM);
d864 1
a864 1
            xfer->staging_sys + xfer->base.layer_stride * slice;
d867 1
a867 1
               tex->bo_stride, src, xfer->base.stride,
d878 1
a878 1
   tex_staging_sys_unmap_bo(ilo, tex);
d884 1
a884 3
tex_staging_sys_unmap(struct ilo_context *ilo,
                      struct ilo_texture *tex,
                      struct ilo_transfer *xfer)
d886 1
d889 1
a889 2
   if (!(xfer->base.usage & PIPE_TRANSFER_WRITE)) {
      FREE(xfer->staging_sys);
a890 1
   }
d894 1
a894 1
      success = tex_staging_sys_convert_write(ilo, tex, xfer);
d897 1
a897 1
      success = tex_staging_sys_zs_write(ilo, tex, xfer);
a906 2

   FREE(xfer->staging_sys);
d910 1
a910 3
tex_staging_sys_map(struct ilo_context *ilo,
                    struct ilo_texture *tex,
                    struct ilo_transfer *xfer)
d912 1
a912 4
   const struct pipe_box *box = &xfer->base.box;
   const size_t stride = util_format_get_stride(tex->base.format, box->width);
   const size_t size =
      util_format_get_2d_size(tex->base.format, stride, box->height);
a914 8
   xfer->staging_sys = MALLOC(size * box->depth);
   if (!xfer->staging_sys)
      return false;

   xfer->base.stride = stride;
   xfer->base.layer_stride = size;
   xfer->ptr = xfer->staging_sys;

d936 1
a936 1
      success = tex_staging_sys_zs_read(ilo, tex, xfer);
d947 2
a948 4
static void
tex_direct_unmap(struct ilo_context *ilo,
                 struct ilo_texture *tex,
                 struct ilo_transfer *xfer)
d950 1
a950 34
   intel_bo_unmap(tex->bo);
}

static bool
tex_direct_map(struct ilo_context *ilo,
               struct ilo_texture *tex,
               struct ilo_transfer *xfer)
{
   xfer->ptr = map_bo_for_transfer(ilo, tex->bo, xfer);
   if (!xfer->ptr)
      return false;

   xfer->ptr += tex_get_box_offset(tex, xfer->base.level, &xfer->base.box);

   /* note that stride is for a block row, not a texel row */
   xfer->base.stride = tex->bo_stride;

   /* slice stride is not always available */
   if (xfer->base.box.depth > 1)
      xfer->base.layer_stride = tex_get_slice_stride(tex, xfer->base.level);
   else
      xfer->base.layer_stride = 0;

   return true;
}

static bool
tex_map(struct ilo_context *ilo, struct ilo_transfer *xfer)
{
   struct ilo_texture *tex = ilo_texture(xfer->base.resource);
   bool success;

   if (!choose_transfer_method(ilo, xfer))
      return false;
d955 21
a975 2
   case ILO_TRANSFER_MAP_UNSYNC:
      success = tex_direct_map(ilo, tex, xfer);
d979 4
a982 1
      success = tex_staging_sys_map(ilo, tex, xfer);
d986 1
a986 1
      success = false;
d990 1
a990 1
   return success;
d993 2
a994 2
static void
tex_unmap(struct ilo_context *ilo, struct ilo_transfer *xfer)
d996 1
a996 1
   struct ilo_texture *tex = ilo_texture(xfer->base.resource);
d998 3
a1000 15
   switch (xfer->method) {
   case ILO_TRANSFER_MAP_CPU:
   case ILO_TRANSFER_MAP_GTT:
   case ILO_TRANSFER_MAP_UNSYNC:
      tex_direct_unmap(ilo, tex, xfer);
      break;
   case ILO_TRANSFER_MAP_SW_CONVERT:
   case ILO_TRANSFER_MAP_SW_ZS:
      tex_staging_sys_unmap(ilo, tex, xfer);
      break;
   default:
      assert(!"unknown mapping method");
      break;
   }
}
d1002 2
a1003 4
static bool
buf_map(struct ilo_context *ilo, struct ilo_transfer *xfer)
{
   struct ilo_buffer *buf = ilo_buffer(xfer->base.resource);
d1005 2
a1006 6
   if (!choose_transfer_method(ilo, xfer))
      return false;

   xfer->ptr = map_bo_for_transfer(ilo, buf->bo, xfer);
   if (!xfer->ptr)
      return false;
d1014 2
a1015 3
   xfer->ptr += xfer->base.box.x;
   xfer->base.stride = 0;
   xfer->base.layer_stride = 0;
d1017 26
a1042 1
   return true;
d1045 2
a1046 2
static void
buf_unmap(struct ilo_context *ilo, struct ilo_transfer *xfer)
d1048 7
a1054 1
   struct ilo_buffer *buf = ilo_buffer(xfer->base.resource);
d1056 34
a1089 1
   intel_bo_unmap(buf->bo);
d1096 1
a1096 1
   bool need_flush;
d1098 8
a1105 10
   /* see if we can avoid stalling */
   if (is_bo_busy(ilo, buf->bo, &need_flush)) {
      bool will_stall = true;

      if (usage & PIPE_TRANSFER_DISCARD_WHOLE_RESOURCE) {
         /* old data not needed so discard the old bo to avoid stalling */
         if (ilo_buffer_alloc_bo(buf)) {
            ilo_mark_states_with_resource_dirty(ilo, &buf->base);
            will_stall = false;
         }
d1108 2
d1111 2
a1112 3
          * We could allocate a temporary bo to hold the data and emit
          * pipelined copy blit to move them to buf->bo.  But for now, do
          * nothing.
d1114 19
d1135 3
a1137 3
      /* flush to make bo busy (so that pwrite() stalls as it should be) */
      if (will_stall && need_flush)
         ilo_cp_flush(ilo->cp, "syncing for pwrites");
d1148 10
d1167 16
a1182 4
   if (xfer->base.resource->target == PIPE_BUFFER)
      buf_unmap(ilo, xfer);
   else
      tex_unmap(ilo, xfer);
d1199 1
a1199 1
   bool success;
d1201 1
d1216 9
a1224 4
   if (res->target == PIPE_BUFFER)
      success = buf_map(ilo, xfer);
   else
      success = tex_map(ilo, xfer);
d1226 1
a1226 1
   if (!success) {
d1228 1
a1228 1
      FREE(xfer);
d1235 1
a1235 1
   return xfer->ptr;
@


