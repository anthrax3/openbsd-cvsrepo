head	1.2;
access;
symbols
	OPENBSD_5_8:1.1.1.3.0.4
	OPENBSD_5_8_BASE:1.1.1.3
	OPENBSD_5_7:1.1.1.3.0.2
	OPENBSD_5_7_BASE:1.1.1.3
	v10_2_9:1.1.1.3
	v10_4_3:1.1.1.2
	v10_2_7:1.1.1.1
	OPENBSD_5_6:1.1.1.1.0.4
	OPENBSD_5_6_BASE:1.1.1.1
	v10_2_3:1.1.1.1
	OPENBSD_5_5:1.1.1.1.0.2
	OPENBSD_5_5_BASE:1.1.1.1
	v9_2_5:1.1.1.1
	v9_2_3:1.1.1.1
	v9_2_2:1.1.1.1
	v9_2_1:1.1.1.1
	v9_2_0:1.1.1.1
	mesa:1.1.1;
locks; strict;
comment	@ * @;
expand	@o@;


1.2
date	2015.12.23.05.17.33;	author jsg;	state dead;
branches;
next	1.1;
commitid	TnlogFl9nOv2eaRf;

1.1
date	2013.09.05.13.12.14;	author jsg;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2013.09.05.13.12.14;	author jsg;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2015.01.25.14.08.12;	author jsg;	state Exp;
branches;
next	1.1.1.3;
commitid	ce2W5rH5aF7VS9gi;

1.1.1.3
date	2015.02.20.22.45.22;	author jsg;	state Exp;
branches;
next	;
commitid	F54a1i0WXHMxq7kE;


desc
@@


1.2
log
@remove the now unused Mesa 10.2.9 code
@
text
@/*
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * on the rights to use, copy, modify, merge, publish, distribute, sub
 * license, and/or sell copies of the Software, and to permit persons to whom
 * the Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice (including the next
 * paragraph) shall be included in all copies or substantial portions of the
 * Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
 * THE AUTHOR(S) AND/OR THEIR SUPPLIERS BE LIABLE FOR ANY CLAIM,
 * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
 * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
 * USE OR OTHER DEALINGS IN THE SOFTWARE.
 *
 * Authors:
 *      Adam Rak <adam.rak@@streamnovation.com>
 */

#ifndef COMPUTE_MEMORY_POOL
#define COMPUTE_MEMORY_POOL

#include <stdlib.h>

struct compute_memory_pool;

struct compute_memory_item
{
	int64_t id; ///ID of the memory chunk

	int untouched; ///True if the memory contains only junk, no need to save it for defrag

	int64_t start_in_dw; ///Start pointer in dwords relative in the pool bo
	int64_t size_in_dw; ///Size of the chunk in dwords

	struct compute_memory_pool* pool;

	struct compute_memory_item* prev;
	struct compute_memory_item* next;
};

struct compute_memory_pool
{
	int64_t next_id; ///For generating unique IDs for memory chunks
	int64_t size_in_dw; ///Size of the pool in dwords

	struct r600_resource *bo; ///The pool buffer object resource
	struct compute_memory_item* item_list; ///Allocated memory chunks in the buffer,they must be ordered by "start_in_dw"
	struct r600_screen *screen;

	uint32_t *shadow; ///host copy of the pool, used for defragmentation
};


struct compute_memory_pool* compute_memory_pool_new(struct r600_screen *rscreen); ///Creates a new pool
void compute_memory_pool_delete(struct compute_memory_pool* pool); ///Frees all stuff in the pool and the pool struct itself too

int64_t compute_memory_prealloc_chunk(struct compute_memory_pool* pool, int64_t size_in_dw); ///searches for an empty space in the pool, return with the pointer to the allocatable space in the pool, returns -1 on failure

struct compute_memory_item* compute_memory_postalloc_chunk(struct compute_memory_pool* pool, int64_t start_in_dw); ///search for the chunk where we can link our new chunk after it

/** 
 * reallocates pool, conserves data
 */
void compute_memory_grow_pool(struct compute_memory_pool* pool, struct pipe_context * pipe,
	int new_size_in_dw);

/**
 * Copy pool from device to host, or host to device
 */
void compute_memory_shadow(struct compute_memory_pool* pool,
	struct pipe_context * pipe, int device_to_host);

/**
 * Allocates pending allocations in the pool
 */
void compute_memory_finalize_pending(struct compute_memory_pool* pool,
	struct pipe_context * pipe);
void compute_memory_defrag(struct compute_memory_pool* pool); ///Defragment the memory pool, always heavy memory usage
void compute_memory_free(struct compute_memory_pool* pool, int64_t id);
struct compute_memory_item* compute_memory_alloc(struct compute_memory_pool* pool, int64_t size_in_dw); ///Creates pending allocations

/**
 * Transfer data host<->device, offset and size is in bytes
 */
void compute_memory_transfer(struct compute_memory_pool* pool,
	struct pipe_context * pipe, int device_to_host,
	struct compute_memory_item* chunk, void* data,
	int offset_in_chunk, int size);

void compute_memory_transfer_direct(struct compute_memory_pool* pool, int chunk_to_data, struct compute_memory_item* chunk, struct r600_resource* data, int offset_in_chunk, int offset_in_data, int size); ///Transfer data between chunk<->data, it is for VRAM<->GART transfers

#endif
@


1.1
log
@Initial revision
@
text
@@


1.1.1.1
log
@Import Mesa 9.2.0
@
text
@@


1.1.1.2
log
@Import Mesa 10.4.3
@
text
@a29 7
#define ITEM_MAPPED_FOR_READING (1<<0)
#define ITEM_MAPPED_FOR_WRITING (1<<1)
#define ITEM_FOR_PROMOTING      (1<<2)
#define ITEM_FOR_DEMOTING       (1<<3)

#define POOL_FRAGMENTED (1<<0)

d34 1
a34 1
	int64_t id;		/**< ID of the memory chunk */
d36 1
a36 1
	uint32_t status;	/**< Will track the status of the item */
d38 2
a39 8
	/** Start pointer in dwords relative in the pool bo. If an item
	 * is unallocated, then this value must be -1 to indicate this. */
	int64_t start_in_dw;
	int64_t size_in_dw;	/**< Size of the chunk in dwords */

	/** Intermediate buffer asociated with an item. It is used mainly for mapping
	 * items against it. They are listed in the pool's unallocated list */
	struct r600_resource *real_buffer;
d43 2
a44 1
	struct list_head link;
d49 2
a50 2
	int64_t next_id;	/**< For generating unique IDs for memory chunks */
	int64_t size_in_dw;	/**< Size of the pool in dwords */
d52 2
a53 1
	struct r600_resource *bo;	/**< The pool buffer object resource */
d56 1
a56 10
	uint32_t *shadow;	/**< host copy of the pool, used for growing the pool */

	uint32_t status;	/**< Status of the pool */

	/** Allocated memory items in the pool, they must be ordered by "start_in_dw" */
	struct list_head *item_list;

	/** Unallocated memory items, this list contains all the items that aren't
	 * yet in the pool */
	struct list_head *unallocated_list;
d60 2
a61 11
static inline int is_item_in_pool(struct compute_memory_item *item)
{
	return item->start_in_dw != -1;
}

struct compute_memory_pool* compute_memory_pool_new(struct r600_screen *rscreen);

void compute_memory_pool_delete(struct compute_memory_pool* pool);

int64_t compute_memory_prealloc_chunk(struct compute_memory_pool* pool,
	int64_t size_in_dw);
d63 1
a63 2
struct list_head *compute_memory_postalloc_chunk(struct compute_memory_pool* pool,
	int64_t start_in_dw);
d65 1
a65 2
int compute_memory_grow_defrag_pool(struct compute_memory_pool* pool,
	struct pipe_context *pipe, int new_size_in_dw);
d67 9
d77 1
a77 1
	struct pipe_context *pipe, int device_to_host);
d79 4
a82 1
int compute_memory_finalize_pending(struct compute_memory_pool* pool,
d84 1
a84 17

void compute_memory_defrag(struct compute_memory_pool *pool,
	struct pipe_resource *src, struct pipe_resource *dst,
	struct pipe_context *pipe);

int compute_memory_promote_item(struct compute_memory_pool *pool,
	struct compute_memory_item *item, struct pipe_context *pipe,
	int64_t allocated);

void compute_memory_demote_item(struct compute_memory_pool *pool,
	struct compute_memory_item *item, struct pipe_context *pipe);

void compute_memory_move_item(struct compute_memory_pool *pool,
	struct pipe_resource *src, struct pipe_resource *dst,
	struct compute_memory_item *item, uint64_t new_start_in_dw,
	struct pipe_context *pipe);

d86 1
d88 3
a90 3
struct compute_memory_item* compute_memory_alloc(struct compute_memory_pool* pool,
	int64_t size_in_dw);

d96 1
a96 4
void compute_memory_transfer_direct(struct compute_memory_pool* pool,
	int chunk_to_data, struct compute_memory_item* chunk,
	struct r600_resource* data, int offset_in_chunk,
	int offset_in_data, int size);
@


1.1.1.3
log
@Import Mesa 10.2.9
@
text
@d30 7
d41 1
a41 1
	int64_t id; ///ID of the memory chunk
d43 1
a43 1
	int untouched; ///True if the memory contains only junk, no need to save it for defrag
d45 8
a52 2
	int64_t start_in_dw; ///Start pointer in dwords relative in the pool bo
	int64_t size_in_dw; ///Size of the chunk in dwords
d56 1
a56 2
	struct compute_memory_item* prev;
	struct compute_memory_item* next;
d61 2
a62 2
	int64_t next_id; ///For generating unique IDs for memory chunks
	int64_t size_in_dw; ///Size of the pool in dwords
d64 1
a64 2
	struct r600_resource *bo; ///The pool buffer object resource
	struct compute_memory_item* item_list; ///Allocated memory chunks in the buffer,they must be ordered by "start_in_dw"
d67 10
a76 1
	uint32_t *shadow; ///host copy of the pool, used for defragmentation
d80 11
a90 2
struct compute_memory_pool* compute_memory_pool_new(struct r600_screen *rscreen); ///Creates a new pool
void compute_memory_pool_delete(struct compute_memory_pool* pool); ///Frees all stuff in the pool and the pool struct itself too
d92 2
a93 1
int64_t compute_memory_prealloc_chunk(struct compute_memory_pool* pool, int64_t size_in_dw); ///searches for an empty space in the pool, return with the pointer to the allocatable space in the pool, returns -1 on failure
d95 2
a96 1
struct compute_memory_item* compute_memory_postalloc_chunk(struct compute_memory_pool* pool, int64_t start_in_dw); ///search for the chunk where we can link our new chunk after it
a97 9
/** 
 * reallocates pool, conserves data
 */
void compute_memory_grow_pool(struct compute_memory_pool* pool, struct pipe_context * pipe,
	int new_size_in_dw);

/**
 * Copy pool from device to host, or host to device
 */
d99 1
a99 1
	struct pipe_context * pipe, int device_to_host);
d101 1
a101 4
/**
 * Allocates pending allocations in the pool
 */
void compute_memory_finalize_pending(struct compute_memory_pool* pool,
d103 17
a119 1
void compute_memory_defrag(struct compute_memory_pool* pool); ///Defragment the memory pool, always heavy memory usage
a120 1
struct compute_memory_item* compute_memory_alloc(struct compute_memory_pool* pool, int64_t size_in_dw); ///Creates pending allocations
d122 3
a124 3
/**
 * Transfer data host<->device, offset and size is in bytes
 */
d130 4
a133 1
void compute_memory_transfer_direct(struct compute_memory_pool* pool, int chunk_to_data, struct compute_memory_item* chunk, struct r600_resource* data, int offset_in_chunk, int offset_in_data, int size); ///Transfer data between chunk<->data, it is for VRAM<->GART transfers
@


