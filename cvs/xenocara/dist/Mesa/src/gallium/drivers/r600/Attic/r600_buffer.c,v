head	1.5;
access;
symbols
	OPENBSD_5_5:1.4.0.2
	OPENBSD_5_5_BASE:1.4
	v9_2_5:1.1.1.2
	v9_2_3:1.1.1.2
	v9_2_2:1.1.1.2
	v9_2_1:1.1.1.2
	v9_2_0:1.1.1.2
	OPENBSD_5_4:1.3.0.2
	OPENBSD_5_4_BASE:1.3
	OPENBSD_5_3:1.2.0.2
	OPENBSD_5_3_BASE:1.2
	OPENBSD_5_2:1.1.1.1.0.4
	OPENBSD_5_2_BASE:1.1.1.1
	OPENBSD_5_1_BASE:1.1.1.1
	OPENBSD_5_1:1.1.1.1.0.2
	v7_10_3:1.1.1.1
	mesa:1.1.1;
locks; strict;
comment	@ * @;


1.5
date	2014.07.09.21.08.54;	author jsg;	state dead;
branches;
next	1.4;
commitid	WPD6rgPryPkvXOr9;

1.4
date	2013.09.05.14.01.00;	author jsg;	state Exp;
branches;
next	1.3;

1.3
date	2013.06.17.23.21.23;	author jsg;	state Exp;
branches;
next	1.2;

1.2
date	2012.08.17.13.58.06;	author mpi;	state Exp;
branches;
next	1.1;

1.1
date	2011.10.23.13.29.28;	author matthieu;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2011.10.23.13.29.28;	author matthieu;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2013.09.05.13.12.19;	author jsg;	state Exp;
branches;
next	;


desc
@@


1.5
log
@Merge Mesa 10.2.3
tested by matthieu@@ kettenis@@ mpi@@ brett@@ and myself across a
diverse range of hardware
@
text
@/*
 * Copyright 2010 Jerome Glisse <glisse@@freedesktop.org>
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * on the rights to use, copy, modify, merge, publish, distribute, sub
 * license, and/or sell copies of the Software, and to permit persons to whom
 * the Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice (including the next
 * paragraph) shall be included in all copies or substantial portions of the
 * Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
 * THE AUTHOR(S) AND/OR THEIR SUPPLIERS BE LIABLE FOR ANY CLAIM,
 * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
 * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
 * USE OR OTHER DEALINGS IN THE SOFTWARE.
 *
 * Authors:
 *      Jerome Glisse
 *      Corbin Simpson <MostAwesomeDude@@gmail.com>
 */
#include "r600_pipe.h"
#include "util/u_upload_mgr.h"
#include "util/u_memory.h"
#include "util/u_surface.h"

static void r600_buffer_destroy(struct pipe_screen *screen,
				struct pipe_resource *buf)
{
	struct r600_resource *rbuffer = r600_resource(buf);

	util_range_destroy(&rbuffer->valid_buffer_range);
	pb_reference(&rbuffer->buf, NULL);
	FREE(rbuffer);
}

static void r600_set_constants_dirty_if_bound(struct r600_context *rctx,
					      struct r600_resource *rbuffer)
{
	unsigned shader;

	for (shader = 0; shader < PIPE_SHADER_TYPES; shader++) {
		struct r600_constbuf_state *state = &rctx->constbuf_state[shader];
		bool found = false;
		uint32_t mask = state->enabled_mask;

		while (mask) {
			unsigned i = u_bit_scan(&mask);
			if (state->cb[i].buffer == &rbuffer->b.b) {
				found = true;
				state->dirty_mask |= 1 << i;
			}
		}
		if (found) {
			r600_constant_buffers_dirty(rctx, state);
		}
	}
}

static void *r600_buffer_get_transfer(struct pipe_context *ctx,
				      struct pipe_resource *resource,
                                      unsigned level,
                                      unsigned usage,
                                      const struct pipe_box *box,
				      struct pipe_transfer **ptransfer,
				      void *data, struct r600_resource *staging,
				      unsigned offset)
{
	struct r600_context *rctx = (struct r600_context*)ctx;
	struct r600_transfer *transfer = util_slab_alloc(&rctx->pool_transfers);

	transfer->transfer.resource = resource;
	transfer->transfer.level = level;
	transfer->transfer.usage = usage;
	transfer->transfer.box = *box;
	transfer->transfer.stride = 0;
	transfer->transfer.layer_stride = 0;
	transfer->offset = offset;
	transfer->staging = staging;
	*ptransfer = &transfer->transfer;
	return data;
}

static void *r600_buffer_transfer_map(struct pipe_context *ctx,
					struct pipe_resource *resource,
					unsigned level,
					unsigned usage,
					const struct pipe_box *box,
					struct pipe_transfer **ptransfer)
{
	struct r600_context *rctx = (struct r600_context*)ctx;
	struct r600_resource *rbuffer = r600_resource(resource);
	uint8_t *data;

	assert(box->x + box->width <= resource->width0);

	/* See if the buffer range being mapped has never been initialized,
	 * in which case it can be mapped unsynchronized. */
	if (!(usage & PIPE_TRANSFER_UNSYNCHRONIZED) &&
	    usage & PIPE_TRANSFER_WRITE &&
	    !util_ranges_intersect(&rbuffer->valid_buffer_range, box->x, box->x + box->width)) {
		usage |= PIPE_TRANSFER_UNSYNCHRONIZED;
	}

	if (usage & PIPE_TRANSFER_DISCARD_WHOLE_RESOURCE &&
	    !(usage & PIPE_TRANSFER_UNSYNCHRONIZED)) {
		assert(usage & PIPE_TRANSFER_WRITE);

		/* Check if mapping this buffer would cause waiting for the GPU. */
		if (r600_rings_is_buffer_referenced(rctx, rbuffer->cs_buf, RADEON_USAGE_READWRITE) ||
		    rctx->ws->buffer_is_busy(rbuffer->buf, RADEON_USAGE_READWRITE)) {
			unsigned i, mask;

			/* Discard the buffer. */
			pb_reference(&rbuffer->buf, NULL);

			/* Create a new one in the same pipe_resource. */
			/* XXX We probably want a different alignment for buffers and textures. */
			r600_init_resource(rctx->screen, rbuffer, rbuffer->b.b.width0, 4096,
					   TRUE, rbuffer->b.b.usage);

			/* We changed the buffer, now we need to bind it where the old one was bound. */
			/* Vertex buffers. */
			mask = rctx->vertex_buffer_state.enabled_mask;
			while (mask) {
				i = u_bit_scan(&mask);
				if (rctx->vertex_buffer_state.vb[i].buffer == &rbuffer->b.b) {
					rctx->vertex_buffer_state.dirty_mask |= 1 << i;
					r600_vertex_buffers_dirty(rctx);
				}
			}
			/* Streamout buffers. */
			for (i = 0; i < rctx->streamout.num_targets; i++) {
				if (rctx->streamout.targets[i]->b.buffer == &rbuffer->b.b) {
					if (rctx->streamout.begin_emitted) {
						r600_emit_streamout_end(rctx);
					}
					rctx->streamout.append_bitmask = rctx->streamout.enabled_mask;
					r600_streamout_buffers_dirty(rctx);
				}
			}
			/* Constant buffers. */
			r600_set_constants_dirty_if_bound(rctx, rbuffer);
		}
	}
	else if ((usage & PIPE_TRANSFER_DISCARD_RANGE) &&
		 !(usage & PIPE_TRANSFER_UNSYNCHRONIZED) &&
		 !(rctx->screen->debug_flags & DBG_NO_DISCARD_RANGE) &&
		 (rctx->screen->has_cp_dma ||
		  (rctx->screen->has_streamout &&
		   /* The buffer range must be aligned to 4 with streamout. */
		   box->x % 4 == 0 && box->width % 4 == 0))) {
		assert(usage & PIPE_TRANSFER_WRITE);

		/* Check if mapping this buffer would cause waiting for the GPU. */
		if (r600_rings_is_buffer_referenced(rctx, rbuffer->cs_buf, RADEON_USAGE_READWRITE) ||
		    rctx->ws->buffer_is_busy(rbuffer->buf, RADEON_USAGE_READWRITE)) {
			/* Do a wait-free write-only transfer using a temporary buffer. */
			unsigned offset;
			struct r600_resource *staging = NULL;

			u_upload_alloc(rctx->uploader, 0, box->width + (box->x % R600_MAP_BUFFER_ALIGNMENT),
				       &offset, (struct pipe_resource**)&staging, (void**)&data);

			if (staging) {
				data += box->x % R600_MAP_BUFFER_ALIGNMENT;
				return r600_buffer_get_transfer(ctx, resource, level, usage, box,
								ptransfer, data, staging, offset);
			}
		}
	}

	/* mmap and synchronize with rings */
	data = r600_buffer_mmap_sync_with_rings(rctx, rbuffer, usage);
	if (!data) {
		return NULL;
	}
	data += box->x;

	return r600_buffer_get_transfer(ctx, resource, level, usage, box,
					ptransfer, data, NULL, 0);
}

static void r600_buffer_transfer_unmap(struct pipe_context *pipe,
					struct pipe_transfer *transfer)
{
	struct r600_context *rctx = (struct r600_context*)pipe;
	struct r600_transfer *rtransfer = (struct r600_transfer*)transfer;
	struct r600_resource *rbuffer = r600_resource(transfer->resource);

	if (rtransfer->staging) {
		struct pipe_resource *dst, *src;
		unsigned soffset, doffset, size;

		dst = transfer->resource;
		src = &rtransfer->staging->b.b;
		size = transfer->box.width;
		doffset = transfer->box.x;
		soffset = rtransfer->offset + transfer->box.x % R600_MAP_BUFFER_ALIGNMENT;
		/* Copy the staging buffer into the original one. */
		if (rctx->rings.dma.cs && !(size % 4) && !(doffset % 4) && !(soffset % 4)) {
			if (rctx->screen->chip_class >= EVERGREEN) {
				evergreen_dma_copy(rctx, dst, src, doffset, soffset, size);
			} else {
				r600_dma_copy(rctx, dst, src, doffset, soffset, size);
			}
		} else {
			struct pipe_box box;

			u_box_1d(soffset, size, &box);
			r600_copy_buffer(pipe, dst, doffset, src, &box);
		}
		pipe_resource_reference((struct pipe_resource**)&rtransfer->staging, NULL);
	}

	if (transfer->usage & PIPE_TRANSFER_WRITE) {
		util_range_add(&rbuffer->valid_buffer_range, transfer->box.x,
			       transfer->box.x + transfer->box.width);
	}
	util_slab_free(&rctx->pool_transfers, transfer);
}

static const struct u_resource_vtbl r600_buffer_vtbl =
{
	u_default_resource_get_handle,		/* get_handle */
	r600_buffer_destroy,			/* resource_destroy */
	r600_buffer_transfer_map,		/* transfer_map */
	NULL,					/* transfer_flush_region */
	r600_buffer_transfer_unmap,		/* transfer_unmap */
	NULL					/* transfer_inline_write */
};

bool r600_init_resource(struct r600_screen *rscreen,
			struct r600_resource *res,
			unsigned size, unsigned alignment,
			bool use_reusable_pool, unsigned usage)
{
	uint32_t initial_domain, domains;

	switch(usage) {
	case PIPE_USAGE_STAGING:
		/* Staging resources participate in transfers, i.e. are used
		 * for uploads and downloads from regular resources.
		 * We generate them internally for some transfers.
		 */
		initial_domain = RADEON_DOMAIN_GTT;
		domains = RADEON_DOMAIN_GTT;
		break;
	case PIPE_USAGE_DYNAMIC:
	case PIPE_USAGE_STREAM:
		/* Default to GTT, but allow the memory manager to move it to VRAM. */
		initial_domain = RADEON_DOMAIN_GTT;
		domains = RADEON_DOMAIN_GTT | RADEON_DOMAIN_VRAM;
		break;
	case PIPE_USAGE_DEFAULT:
	case PIPE_USAGE_STATIC:
	case PIPE_USAGE_IMMUTABLE:
	default:
		/* Don't list GTT here, because the memory manager would put some
		 * resources to GTT no matter what the initial domain is.
		 * Not listing GTT in the domains improves performance a lot. */
		initial_domain = RADEON_DOMAIN_VRAM;
		domains = RADEON_DOMAIN_VRAM;
		break;
	}

	res->buf = rscreen->ws->buffer_create(rscreen->ws, size, alignment,
                                              use_reusable_pool,
                                              initial_domain);
	if (!res->buf) {
		return false;
	}

	res->cs_buf = rscreen->ws->buffer_get_cs_handle(res->buf);
	res->domains = domains;
	util_range_set_empty(&res->valid_buffer_range);

	if (rscreen->debug_flags & DBG_VM && res->b.b.target == PIPE_BUFFER) {
		fprintf(stderr, "VM start=0x%llX  end=0x%llX | Buffer %u bytes\n",
			r600_resource_va(&rscreen->screen, &res->b.b),
			r600_resource_va(&rscreen->screen, &res->b.b) + res->buf->size,
			res->buf->size);
	}
	return true;
}

struct pipe_resource *r600_buffer_create(struct pipe_screen *screen,
					 const struct pipe_resource *templ,
					 unsigned alignment)
{
	struct r600_screen *rscreen = (struct r600_screen*)screen;
	struct r600_resource *rbuffer;

	rbuffer = MALLOC_STRUCT(r600_resource);

	rbuffer->b.b = *templ;
	pipe_reference_init(&rbuffer->b.b.reference, 1);
	rbuffer->b.b.screen = screen;
	rbuffer->b.vtbl = &r600_buffer_vtbl;
	util_range_init(&rbuffer->valid_buffer_range);

	if (!r600_init_resource(rscreen, rbuffer, templ->width0, alignment, TRUE, templ->usage)) {
		FREE(rbuffer);
		return NULL;
	}
	return &rbuffer->b.b;
}
@


1.4
log
@Merge Mesa 9.2.0
@
text
@@


1.3
log
@byteswap.h and bswap_32 aren't portable, replace them with calls to
gallium's util_bswap32 as suggested by kettenis.

already merged upstream
ok kettenis@@
@
text
@d27 1
a27 5
#include <pipe/p_screen.h>
#include <util/u_format.h>
#include <util/u_math.h>
#include <util/u_inlines.h>
#include <util/u_memory.h>
d29 2
a30 8

#include "state_tracker/drm_driver.h"

#include <xf86drm.h>
#include "radeon_drm.h"

#include "r600.h"
#include "r600_pipe.h"
d35 1
a35 2
	struct r600_screen *rscreen = (struct r600_screen*)screen;
	struct r600_resource_buffer *rbuffer = r600_buffer(buf);
d37 3
a39 5
	if (rbuffer->r.bo) {
		r600_bo_reference((struct radeon*)screen->winsys, &rbuffer->r.bo, NULL);
	}
	rbuffer->r.bo = NULL;
	util_slab_free(&rscreen->pool_buffers, rbuffer);
d42 2
a43 5
static struct pipe_transfer *r600_get_transfer(struct pipe_context *ctx,
					       struct pipe_resource *resource,
					       unsigned level,
					       unsigned usage,
					       const struct pipe_box *box)
d45 6
a50 2
	struct r600_pipe_context *rctx = (struct r600_pipe_context*)ctx;
	struct pipe_transfer *transfer = util_slab_alloc(&rctx->pool_transfers);
d52 11
a62 12
	transfer->resource = resource;
	transfer->level = level;
	transfer->usage = usage;
	transfer->box = *box;
	transfer->stride = 0;
	transfer->layer_stride = 0;
	transfer->data = NULL;

	/* Note strides are zero, this is ok for buffers, but not for
	 * textures 2d & higher at least.
	 */
	return transfer;
d65 30
a94 2
static void *r600_buffer_transfer_map(struct pipe_context *pipe,
				      struct pipe_transfer *transfer)
d96 2
a97 1
	struct r600_resource_buffer *rbuffer = r600_buffer(transfer->resource);
d100 1
a100 2
	if (rbuffer->r.b.user_ptr)
		return (uint8_t*)rbuffer->r.b.user_ptr + transfer->box.x;
d102 79
a180 2
	data = r600_bo_map((struct radeon*)pipe->winsys, rbuffer->r.bo, transfer->usage, pipe);
	if (!data)
d182 2
d185 2
a186 1
	return (uint8_t*)data + transfer->box.x;
d192 22
a213 1
	struct r600_resource_buffer *rbuffer = r600_buffer(transfer->resource);
d215 5
a219 2
	if (rbuffer->r.b.user_ptr)
		return;
d221 4
a224 14
	if (rbuffer->r.bo)
		r600_bo_unmap((struct radeon*)pipe->winsys, rbuffer->r.bo);
}

static void r600_buffer_transfer_flush_region(struct pipe_context *pipe,
						struct pipe_transfer *transfer,
						const struct pipe_box *box)
{
}

static void r600_transfer_destroy(struct pipe_context *ctx,
				  struct pipe_transfer *transfer)
{
	struct r600_pipe_context *rctx = (struct r600_pipe_context*)ctx;
a227 25
static void r600_buffer_transfer_inline_write(struct pipe_context *pipe,
						struct pipe_resource *resource,
						unsigned level,
						unsigned usage,
						const struct pipe_box *box,
						const void *data,
						unsigned stride,
						unsigned layer_stride)
{
	struct radeon *ws = (struct radeon*)pipe->winsys;
	struct r600_resource_buffer *rbuffer = r600_buffer(resource);
	uint8_t *map = NULL;

	assert(rbuffer->r.b.user_ptr == NULL);

	map = r600_bo_map(ws, rbuffer->r.bo,
			  PIPE_TRANSFER_WRITE | PIPE_TRANSFER_DISCARD | usage,
			  pipe);

	memcpy(map + box->x, data, box->width);

	if (rbuffer->r.bo)
		r600_bo_unmap(ws, rbuffer->r.bo);
}

a231 2
	r600_get_transfer,			/* get_transfer */
	r600_transfer_destroy,			/* transfer_destroy */
d233 1
a233 1
	r600_buffer_transfer_flush_region,	/* transfer_flush_region */
d235 1
a235 1
	r600_buffer_transfer_inline_write	/* transfer_inline_write */
d238 32
a269 28
struct pipe_resource *r600_buffer_create(struct pipe_screen *screen,
					 const struct pipe_resource *templ)
{
	struct r600_screen *rscreen = (struct r600_screen*)screen;
	struct r600_resource_buffer *rbuffer;
	struct r600_bo *bo;
	/* XXX We probably want a different alignment for buffers and textures. */
	unsigned alignment = 4096;

	rbuffer = util_slab_alloc(&rscreen->pool_buffers);

	rbuffer->magic = R600_BUFFER_MAGIC;
	rbuffer->r.b.b.b = *templ;
	pipe_reference_init(&rbuffer->r.b.b.b.reference, 1);
	rbuffer->r.b.b.b.screen = screen;
	rbuffer->r.b.b.vtbl = &r600_buffer_vtbl;
	rbuffer->r.b.user_ptr = NULL;
	rbuffer->r.size = rbuffer->r.b.b.b.width0;
	rbuffer->r.bo_size = rbuffer->r.size;

	bo = r600_bo((struct radeon*)screen->winsys,
		     rbuffer->r.b.b.b.width0,
		     alignment, rbuffer->r.b.b.b.bind,
		     rbuffer->r.b.b.b.usage);

	if (bo == NULL) {
		FREE(rbuffer);
		return NULL;
a270 38
	rbuffer->r.bo = bo;
	return &rbuffer->r.b.b.b;
}

struct pipe_resource *r600_user_buffer_create(struct pipe_screen *screen,
					      void *ptr, unsigned bytes,
					      unsigned bind)
{
	struct r600_screen *rscreen = (struct r600_screen*)screen;
	struct r600_resource_buffer *rbuffer;

	rbuffer = util_slab_alloc(&rscreen->pool_buffers);

	rbuffer->magic = R600_BUFFER_MAGIC;
	pipe_reference_init(&rbuffer->r.b.b.b.reference, 1);
	rbuffer->r.b.b.vtbl = &r600_buffer_vtbl;
	rbuffer->r.b.b.b.screen = screen;
	rbuffer->r.b.b.b.target = PIPE_BUFFER;
	rbuffer->r.b.b.b.format = PIPE_FORMAT_R8_UNORM;
	rbuffer->r.b.b.b.usage = PIPE_USAGE_IMMUTABLE;
	rbuffer->r.b.b.b.bind = bind;
	rbuffer->r.b.b.b.width0 = bytes;
	rbuffer->r.b.b.b.height0 = 1;
	rbuffer->r.b.b.b.depth0 = 1;
	rbuffer->r.b.b.b.array_size = 1;
	rbuffer->r.b.b.b.flags = 0;
	rbuffer->r.b.user_ptr = ptr;
	rbuffer->r.bo = NULL;
	rbuffer->r.bo_size = 0;
	return &rbuffer->r.b.b.b;
}

struct pipe_resource *r600_buffer_from_handle(struct pipe_screen *screen,
					      struct winsys_handle *whandle)
{
	struct radeon *rw = (struct radeon*)screen->winsys;
	struct r600_resource *rbuffer;
	struct r600_bo *bo = NULL;
d272 5
a276 3
	bo = r600_bo_handle(rw, whandle->handle, NULL);
	if (bo == NULL) {
		return NULL;
d279 9
a287 4
	rbuffer = CALLOC_STRUCT(r600_resource);
	if (rbuffer == NULL) {
		r600_bo_reference(rw, &bo, NULL);
		return NULL;
d289 1
a289 7

	pipe_reference_init(&rbuffer->b.b.b.reference, 1);
	rbuffer->b.b.b.target = PIPE_BUFFER;
	rbuffer->b.b.b.screen = screen;
	rbuffer->b.b.vtbl = &r600_buffer_vtbl;
	rbuffer->bo = bo;
	return &rbuffer->b.b.b;
d292 3
a294 1
void r600_upload_index_buffer(struct r600_pipe_context *rctx, struct r600_drawl *draw)
d296 2
a297 9
	struct r600_resource_buffer *rbuffer = r600_buffer(draw->index_buffer);
	boolean flushed;

	u_upload_data(rctx->vbuf_mgr->uploader, 0,
		      draw->info.count * draw->index_size,
		      rbuffer->r.b.user_ptr,
		      &draw->index_buffer_offset,
		      &draw->index_buffer, &flushed);
}
d299 1
a299 18
void r600_upload_const_buffer(struct r600_pipe_context *rctx, struct r600_resource_buffer **rbuffer,
			     uint32_t *const_offset)
{
	if ((*rbuffer)->r.b.user_ptr) {
		uint8_t *ptr = (*rbuffer)->r.b.user_ptr;
		unsigned size = (*rbuffer)->r.b.b.b.width0;
		boolean flushed;

		*rbuffer = NULL;

		if (R600_BIG_ENDIAN) {
			uint32_t *tmpPtr;
			unsigned i;

			if (!(tmpPtr = malloc(size))) {
				R600_ERR("Failed to allocate BE swap buffer.\n");
				return;
			}
d301 5
a305 3
			for (i = 0; i < size / 4; ++i) {
				tmpPtr[i] = util_bswap32(((uint32_t *)ptr)[i]);
			}
d307 3
a309 10
			u_upload_data(rctx->vbuf_mgr->uploader, 0, size, tmpPtr, const_offset,
				      (struct pipe_resource**)rbuffer, &flushed);

			free(tmpPtr);
		} else {
			u_upload_data(rctx->vbuf_mgr->uploader, 0, size, ptr, const_offset,
				      (struct pipe_resource**)rbuffer, &flushed);
		}
	} else {
		*const_offset = 0;
d311 1
@


1.2
log
@Upate to libGL 7.11.2

Tested by jsg@@, matthieu@@ and ajacoutot@@, ok mattieu@@
@
text
@a26 2
#include <byteswap.h>

d275 1
a275 1
				tmpPtr[i] = bswap_32(((uint32_t *)ptr)[i]);
@


1.1
log
@Initial revision
@
text
@d27 2
d34 2
d37 1
d40 1
a43 63
extern struct u_resource_vtbl r600_buffer_vtbl;


struct pipe_resource *r600_buffer_create(struct pipe_screen *screen,
					 const struct pipe_resource *templ)
{
	struct r600_resource_buffer *rbuffer;
	struct r600_bo *bo;
	/* XXX We probably want a different alignment for buffers and textures. */
	unsigned alignment = 4096;

	rbuffer = CALLOC_STRUCT(r600_resource_buffer);
	if (rbuffer == NULL)
		return NULL;

	rbuffer->magic = R600_BUFFER_MAGIC;
	rbuffer->user_buffer = NULL;
	rbuffer->r.base.b = *templ;
	pipe_reference_init(&rbuffer->r.base.b.reference, 1);
	rbuffer->r.base.b.screen = screen;
	rbuffer->r.base.vtbl = &r600_buffer_vtbl;
	rbuffer->r.size = rbuffer->r.base.b.width0;
	rbuffer->r.bo_size = rbuffer->r.size;
	rbuffer->uploaded = FALSE;
	bo = r600_bo((struct radeon*)screen->winsys, rbuffer->r.base.b.width0, alignment, rbuffer->r.base.b.bind, rbuffer->r.base.b.usage);
	if (bo == NULL) {
		FREE(rbuffer);
		return NULL;
	}
	rbuffer->r.bo = bo;
	return &rbuffer->r.base.b;
}

struct pipe_resource *r600_user_buffer_create(struct pipe_screen *screen,
					      void *ptr, unsigned bytes,
					      unsigned bind)
{
	struct r600_resource_buffer *rbuffer;

	rbuffer = CALLOC_STRUCT(r600_resource_buffer);
	if (rbuffer == NULL)
		return NULL;

	rbuffer->magic = R600_BUFFER_MAGIC;
	pipe_reference_init(&rbuffer->r.base.b.reference, 1);
	rbuffer->r.base.vtbl = &r600_buffer_vtbl;
	rbuffer->r.base.b.screen = screen;
	rbuffer->r.base.b.target = PIPE_BUFFER;
	rbuffer->r.base.b.format = PIPE_FORMAT_R8_UNORM;
	rbuffer->r.base.b.usage = PIPE_USAGE_IMMUTABLE;
	rbuffer->r.base.b.bind = bind;
	rbuffer->r.base.b.width0 = bytes;
	rbuffer->r.base.b.height0 = 1;
	rbuffer->r.base.b.depth0 = 1;
	rbuffer->r.base.b.array_size = 1;
	rbuffer->r.base.b.flags = 0;
	rbuffer->r.bo = NULL;
	rbuffer->r.bo_size = 0;
	rbuffer->user_buffer = ptr;
	rbuffer->uploaded = FALSE;
	return &rbuffer->r.base.b;
}

d47 1
d54 24
a77 1
	FREE(rbuffer);
a83 1
	int write = 0;
d86 2
a87 2
	if (rbuffer->user_buffer)
		return (uint8_t*)rbuffer->user_buffer + transfer->box.x;
a88 6
	if (transfer->usage & PIPE_TRANSFER_DONTBLOCK) {
		/* FIXME */
	}
	if (transfer->usage & PIPE_TRANSFER_WRITE) {
		write = 1;
	}
d101 1
a101 1
	if (rbuffer->user_buffer)
d114 2
a115 3
unsigned r600_buffer_is_referenced_by_cs(struct pipe_context *context,
					 struct pipe_resource *buf,
					 unsigned level, int layer)
d117 100
a216 2
	/* FIXME */
	return PIPE_REFERENCED_FOR_READ | PIPE_REFERENCED_FOR_WRITE;
d237 4
a240 4
	pipe_reference_init(&rbuffer->base.b.reference, 1);
	rbuffer->base.b.target = PIPE_BUFFER;
	rbuffer->base.b.screen = screen;
	rbuffer->base.vtbl = &r600_buffer_vtbl;
d242 1
a242 1
	return &rbuffer->base.b;
d245 1
a245 14
struct u_resource_vtbl r600_buffer_vtbl =
{
	u_default_resource_get_handle,		/* get_handle */
	r600_buffer_destroy,			/* resource_destroy */
	r600_buffer_is_referenced_by_cs,	/* is_buffer_referenced */
	u_default_get_transfer,			/* get_transfer */
	u_default_transfer_destroy,		/* transfer_destroy */
	r600_buffer_transfer_map,		/* transfer_map */
	r600_buffer_transfer_flush_region,	/* transfer_flush_region */
	r600_buffer_transfer_unmap,		/* transfer_unmap */
	u_default_transfer_inline_write		/* transfer_inline_write */
};

int r600_upload_index_buffer(struct r600_pipe_context *rctx, struct r600_drawl *draw)
d247 2
a248 17
	if (r600_buffer_is_user_buffer(draw->index_buffer)) {
		struct r600_resource_buffer *rbuffer = r600_buffer(draw->index_buffer);
		unsigned upload_offset;
		int ret = 0;

		ret = r600_upload_buffer(rctx->rupload_vb,
					draw->index_buffer_offset,
					draw->count * draw->index_size,
					rbuffer,
					&upload_offset,
					&rbuffer->r.bo_size,
					&rbuffer->r.bo);
		if (ret)
			return ret;
		rbuffer->uploaded = TRUE;
		draw->index_buffer_offset = upload_offset;
	}
d250 5
a254 1
	return 0;
d257 2
a258 1
int r600_upload_user_buffers(struct r600_pipe_context *rctx)
d260 27
a286 23
	enum pipe_error ret = PIPE_OK;
	int i, nr;

	nr = rctx->vertex_elements->count;
	nr = rctx->nvertex_buffer;

	for (i = 0; i < nr; i++) {
		struct pipe_vertex_buffer *vb = &rctx->vertex_buffer[i];

		if (r600_buffer_is_user_buffer(vb->buffer)) {
			struct r600_resource_buffer *rbuffer = r600_buffer(vb->buffer);
			unsigned upload_offset;

			ret = r600_upload_buffer(rctx->rupload_vb,
						0, vb->buffer->width0,
						rbuffer,
						&upload_offset,
						&rbuffer->r.bo_size,
						&rbuffer->r.bo);
			if (ret)
				return ret;
			rbuffer->uploaded = TRUE;
			vb->buffer_offset = upload_offset;
d288 2
a290 1
	return ret;
@


1.1.1.1
log
@Import Mesa 7.10.3
@
text
@@


1.1.1.2
log
@Import Mesa 9.2.0
@
text
@d27 9
a36 3
#include "util/u_upload_mgr.h"
#include "util/u_memory.h"
#include "util/u_surface.h"
d38 5
a42 2
static void r600_buffer_destroy(struct pipe_screen *screen,
				struct pipe_resource *buf)
d44 8
a51 1
	struct r600_resource *rbuffer = r600_resource(buf);
d53 16
a68 3
	util_range_destroy(&rbuffer->valid_buffer_range);
	pb_reference(&rbuffer->buf, NULL);
	FREE(rbuffer);
d71 3
a73 2
static void r600_set_constants_dirty_if_bound(struct r600_context *rctx,
					      struct r600_resource *rbuffer)
d75 5
a79 1
	unsigned shader;
d81 18
a98 16
	for (shader = 0; shader < PIPE_SHADER_TYPES; shader++) {
		struct r600_constbuf_state *state = &rctx->constbuf_state[shader];
		bool found = false;
		uint32_t mask = state->enabled_mask;

		while (mask) {
			unsigned i = u_bit_scan(&mask);
			if (state->cb[i].buffer == &rbuffer->b.b) {
				found = true;
				state->dirty_mask |= 1 << i;
			}
		}
		if (found) {
			r600_constant_buffers_dirty(rctx, state);
		}
	}
d101 10
a110 22
static void *r600_buffer_get_transfer(struct pipe_context *ctx,
				      struct pipe_resource *resource,
                                      unsigned level,
                                      unsigned usage,
                                      const struct pipe_box *box,
				      struct pipe_transfer **ptransfer,
				      void *data, struct r600_resource *staging,
				      unsigned offset)
{
	struct r600_context *rctx = (struct r600_context*)ctx;
	struct r600_transfer *transfer = util_slab_alloc(&rctx->pool_transfers);

	transfer->transfer.resource = resource;
	transfer->transfer.level = level;
	transfer->transfer.usage = usage;
	transfer->transfer.box = *box;
	transfer->transfer.stride = 0;
	transfer->transfer.layer_stride = 0;
	transfer->offset = offset;
	transfer->staging = staging;
	*ptransfer = &transfer->transfer;
	return data;
d113 2
a114 6
static void *r600_buffer_transfer_map(struct pipe_context *ctx,
					struct pipe_resource *resource,
					unsigned level,
					unsigned usage,
					const struct pipe_box *box,
					struct pipe_transfer **ptransfer)
d116 2
a117 2
	struct r600_context *rctx = (struct r600_context*)ctx;
	struct r600_resource *rbuffer = r600_resource(resource);
d120 2
a121 1
	assert(box->x + box->width <= resource->width0);
d123 2
a124 6
	/* See if the buffer range being mapped has never been initialized,
	 * in which case it can be mapped unsynchronized. */
	if (!(usage & PIPE_TRANSFER_UNSYNCHRONIZED) &&
	    usage & PIPE_TRANSFER_WRITE &&
	    !util_ranges_intersect(&rbuffer->valid_buffer_range, box->x, box->x + box->width)) {
		usage |= PIPE_TRANSFER_UNSYNCHRONIZED;
d126 2
a127 67

	if (usage & PIPE_TRANSFER_DISCARD_WHOLE_RESOURCE &&
	    !(usage & PIPE_TRANSFER_UNSYNCHRONIZED)) {
		assert(usage & PIPE_TRANSFER_WRITE);

		/* Check if mapping this buffer would cause waiting for the GPU. */
		if (r600_rings_is_buffer_referenced(rctx, rbuffer->cs_buf, RADEON_USAGE_READWRITE) ||
		    rctx->ws->buffer_is_busy(rbuffer->buf, RADEON_USAGE_READWRITE)) {
			unsigned i, mask;

			/* Discard the buffer. */
			pb_reference(&rbuffer->buf, NULL);

			/* Create a new one in the same pipe_resource. */
			/* XXX We probably want a different alignment for buffers and textures. */
			r600_init_resource(rctx->screen, rbuffer, rbuffer->b.b.width0, 4096,
					   TRUE, rbuffer->b.b.usage);

			/* We changed the buffer, now we need to bind it where the old one was bound. */
			/* Vertex buffers. */
			mask = rctx->vertex_buffer_state.enabled_mask;
			while (mask) {
				i = u_bit_scan(&mask);
				if (rctx->vertex_buffer_state.vb[i].buffer == &rbuffer->b.b) {
					rctx->vertex_buffer_state.dirty_mask |= 1 << i;
					r600_vertex_buffers_dirty(rctx);
				}
			}
			/* Streamout buffers. */
			for (i = 0; i < rctx->streamout.num_targets; i++) {
				if (rctx->streamout.targets[i]->b.buffer == &rbuffer->b.b) {
					if (rctx->streamout.begin_emitted) {
						r600_emit_streamout_end(rctx);
					}
					rctx->streamout.append_bitmask = rctx->streamout.enabled_mask;
					r600_streamout_buffers_dirty(rctx);
				}
			}
			/* Constant buffers. */
			r600_set_constants_dirty_if_bound(rctx, rbuffer);
		}
	}
	else if ((usage & PIPE_TRANSFER_DISCARD_RANGE) &&
		 !(usage & PIPE_TRANSFER_UNSYNCHRONIZED) &&
		 !(rctx->screen->debug_flags & DBG_NO_DISCARD_RANGE) &&
		 (rctx->screen->has_cp_dma ||
		  (rctx->screen->has_streamout &&
		   /* The buffer range must be aligned to 4 with streamout. */
		   box->x % 4 == 0 && box->width % 4 == 0))) {
		assert(usage & PIPE_TRANSFER_WRITE);

		/* Check if mapping this buffer would cause waiting for the GPU. */
		if (r600_rings_is_buffer_referenced(rctx, rbuffer->cs_buf, RADEON_USAGE_READWRITE) ||
		    rctx->ws->buffer_is_busy(rbuffer->buf, RADEON_USAGE_READWRITE)) {
			/* Do a wait-free write-only transfer using a temporary buffer. */
			unsigned offset;
			struct r600_resource *staging = NULL;

			u_upload_alloc(rctx->uploader, 0, box->width + (box->x % R600_MAP_BUFFER_ALIGNMENT),
				       &offset, (struct pipe_resource**)&staging, (void**)&data);

			if (staging) {
				data += box->x % R600_MAP_BUFFER_ALIGNMENT;
				return r600_buffer_get_transfer(ctx, resource, level, usage, box,
								ptransfer, data, staging, offset);
			}
		}
d129 2
a130 4

	/* mmap and synchronize with rings */
	data = r600_buffer_mmap_sync_with_rings(rctx, rbuffer, usage);
	if (!data) {
a131 2
	}
	data += box->x;
d133 1
a133 2
	return r600_buffer_get_transfer(ctx, resource, level, usage, box,
					ptransfer, data, NULL, 0);
d139 4
a142 22
	struct r600_context *rctx = (struct r600_context*)pipe;
	struct r600_transfer *rtransfer = (struct r600_transfer*)transfer;
	struct r600_resource *rbuffer = r600_resource(transfer->resource);

	if (rtransfer->staging) {
		struct pipe_resource *dst, *src;
		unsigned soffset, doffset, size;

		dst = transfer->resource;
		src = &rtransfer->staging->b.b;
		size = transfer->box.width;
		doffset = transfer->box.x;
		soffset = rtransfer->offset + transfer->box.x % R600_MAP_BUFFER_ALIGNMENT;
		/* Copy the staging buffer into the original one. */
		if (rctx->rings.dma.cs && !(size % 4) && !(doffset % 4) && !(soffset % 4)) {
			if (rctx->screen->chip_class >= EVERGREEN) {
				evergreen_dma_copy(rctx, dst, src, doffset, soffset, size);
			} else {
				r600_dma_copy(rctx, dst, src, doffset, soffset, size);
			}
		} else {
			struct pipe_box box;
d144 28
a171 4
			u_box_1d(soffset, size, &box);
			r600_copy_buffer(pipe, dst, doffset, src, &box);
		}
		pipe_resource_reference((struct pipe_resource**)&rtransfer->staging, NULL);
d174 4
a177 3
	if (transfer->usage & PIPE_TRANSFER_WRITE) {
		util_range_add(&rbuffer->valid_buffer_range, transfer->box.x,
			       transfer->box.x + transfer->box.width);
d179 7
a185 1
	util_slab_free(&rctx->pool_transfers, transfer);
d188 1
a188 1
static const struct u_resource_vtbl r600_buffer_vtbl =
d192 3
d196 1
a196 1
	NULL,					/* transfer_flush_region */
d198 1
a198 1
	NULL					/* transfer_inline_write */
d201 1
a201 57
bool r600_init_resource(struct r600_screen *rscreen,
			struct r600_resource *res,
			unsigned size, unsigned alignment,
			bool use_reusable_pool, unsigned usage)
{
	uint32_t initial_domain, domains;

	switch(usage) {
	case PIPE_USAGE_STAGING:
		/* Staging resources participate in transfers, i.e. are used
		 * for uploads and downloads from regular resources.
		 * We generate them internally for some transfers.
		 */
		initial_domain = RADEON_DOMAIN_GTT;
		domains = RADEON_DOMAIN_GTT;
		break;
	case PIPE_USAGE_DYNAMIC:
	case PIPE_USAGE_STREAM:
		/* Default to GTT, but allow the memory manager to move it to VRAM. */
		initial_domain = RADEON_DOMAIN_GTT;
		domains = RADEON_DOMAIN_GTT | RADEON_DOMAIN_VRAM;
		break;
	case PIPE_USAGE_DEFAULT:
	case PIPE_USAGE_STATIC:
	case PIPE_USAGE_IMMUTABLE:
	default:
		/* Don't list GTT here, because the memory manager would put some
		 * resources to GTT no matter what the initial domain is.
		 * Not listing GTT in the domains improves performance a lot. */
		initial_domain = RADEON_DOMAIN_VRAM;
		domains = RADEON_DOMAIN_VRAM;
		break;
	}

	res->buf = rscreen->ws->buffer_create(rscreen->ws, size, alignment,
                                              use_reusable_pool,
                                              initial_domain);
	if (!res->buf) {
		return false;
	}

	res->cs_buf = rscreen->ws->buffer_get_cs_handle(res->buf);
	res->domains = domains;
	util_range_set_empty(&res->valid_buffer_range);

	if (rscreen->debug_flags & DBG_VM && res->b.b.target == PIPE_BUFFER) {
		fprintf(stderr, "VM start=0x%llX  end=0x%llX | Buffer %u bytes\n",
			r600_resource_va(&rscreen->screen, &res->b.b),
			r600_resource_va(&rscreen->screen, &res->b.b) + res->buf->size,
			res->buf->size);
	}
	return true;
}

struct pipe_resource *r600_buffer_create(struct pipe_screen *screen,
					 const struct pipe_resource *templ,
					 unsigned alignment)
d203 47
a249 14
	struct r600_screen *rscreen = (struct r600_screen*)screen;
	struct r600_resource *rbuffer;

	rbuffer = MALLOC_STRUCT(r600_resource);

	rbuffer->b.b = *templ;
	pipe_reference_init(&rbuffer->b.b.reference, 1);
	rbuffer->b.b.screen = screen;
	rbuffer->b.vtbl = &r600_buffer_vtbl;
	util_range_init(&rbuffer->valid_buffer_range);

	if (!r600_init_resource(rscreen, rbuffer, templ->width0, alignment, TRUE, templ->usage)) {
		FREE(rbuffer);
		return NULL;
d251 1
a251 1
	return &rbuffer->b.b;
@

