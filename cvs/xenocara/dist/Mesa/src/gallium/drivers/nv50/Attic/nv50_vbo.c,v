head	1.6;
access;
symbols
	OPENBSD_5_5:1.5.0.2
	OPENBSD_5_5_BASE:1.5
	v9_2_5:1.1.1.3
	v9_2_3:1.1.1.2
	v9_2_2:1.1.1.2
	v9_2_1:1.1.1.2
	v9_2_0:1.1.1.2
	OPENBSD_5_4:1.3.0.4
	OPENBSD_5_4_BASE:1.3
	OPENBSD_5_3:1.3.0.2
	OPENBSD_5_3_BASE:1.3
	OPENBSD_5_2:1.2.0.4
	OPENBSD_5_2_BASE:1.2
	OPENBSD_5_1_BASE:1.2
	OPENBSD_5_1:1.2.0.2
	v7_10_3:1.1.1.1
	mesa:1.1.1
	OPENBSD_5_0:1.1.0.6
	OPENBSD_5_0_BASE:1.1
	OPENBSD_4_9:1.1.0.2
	OPENBSD_4_9_BASE:1.1
	OPENBSD_4_8:1.1.0.4
	OPENBSD_4_8_BASE:1.1;
locks; strict;
comment	@ * @;


1.6
date	2014.07.09.21.08.54;	author jsg;	state dead;
branches;
next	1.5;
commitid	WPD6rgPryPkvXOr9;

1.5
date	2014.01.19.03.13.15;	author jsg;	state Exp;
branches;
next	1.4;

1.4
date	2013.09.05.14.00.45;	author jsg;	state Exp;
branches;
next	1.3;

1.3
date	2012.08.17.13.58.05;	author mpi;	state Exp;
branches;
next	1.2;

1.2
date	2011.10.23.13.37.33;	author matthieu;	state Exp;
branches;
next	1.1;

1.1
date	2010.05.22.20.06.07;	author matthieu;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2011.10.23.13.29.28;	author matthieu;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2013.09.05.13.11.43;	author jsg;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2014.01.19.03.03.45;	author jsg;	state Exp;
branches;
next	;


desc
@@


1.6
log
@Merge Mesa 10.2.3
tested by matthieu@@ kettenis@@ mpi@@ brett@@ and myself across a
diverse range of hardware
@
text
@/*
 * Copyright 2010 Christoph Bumiller
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
 * OTHER DEALINGS IN THE SOFTWARE.
 */

#include "pipe/p_context.h"
#include "pipe/p_state.h"
#include "util/u_inlines.h"
#include "util/u_format.h"
#include "translate/translate.h"

#include "nv50_context.h"
#include "nv50_resource.h"

#include "nv50_3d.xml.h"

void
nv50_vertex_state_delete(struct pipe_context *pipe,
                         void *hwcso)
{
   struct nv50_vertex_stateobj *so = hwcso;

   if (so->translate)
      so->translate->release(so->translate);
   FREE(hwcso);
}

void *
nv50_vertex_state_create(struct pipe_context *pipe,
                         unsigned num_elements,
                         const struct pipe_vertex_element *elements)
{
    struct nv50_vertex_stateobj *so;
    struct translate_key transkey;
    unsigned i;

    so = MALLOC(sizeof(*so) +
                num_elements * sizeof(struct nv50_vertex_element));
    if (!so)
        return NULL;
    so->num_elements = num_elements;
    so->instance_elts = 0;
    so->instance_bufs = 0;
    so->need_conversion = FALSE;

    memset(so->vb_access_size, 0, sizeof(so->vb_access_size));

    for (i = 0; i < PIPE_MAX_ATTRIBS; ++i)
       so->min_instance_div[i] = 0xffffffff;

    transkey.nr_elements = 0;
    transkey.output_stride = 0;

    for (i = 0; i < num_elements; ++i) {
        const struct pipe_vertex_element *ve = &elements[i];
        const unsigned vbi = ve->vertex_buffer_index;
        unsigned size;
        enum pipe_format fmt = ve->src_format;

        so->element[i].pipe = elements[i];
        so->element[i].state = nv50_format_table[fmt].vtx;

        if (!so->element[i].state) {
            switch (util_format_get_nr_components(fmt)) {
            case 1: fmt = PIPE_FORMAT_R32_FLOAT; break;
            case 2: fmt = PIPE_FORMAT_R32G32_FLOAT; break;
            case 3: fmt = PIPE_FORMAT_R32G32B32_FLOAT; break;
            case 4: fmt = PIPE_FORMAT_R32G32B32A32_FLOAT; break;
            default:
                assert(0);
                FREE(so);
                return NULL;
            }
            so->element[i].state = nv50_format_table[fmt].vtx;
            so->need_conversion = TRUE;
        }
        so->element[i].state |= i;

        size = util_format_get_blocksize(fmt);
        if (so->vb_access_size[vbi] < (ve->src_offset + size))
           so->vb_access_size[vbi] = ve->src_offset + size;

        if (1) {
            unsigned j = transkey.nr_elements++;

            transkey.element[j].type = TRANSLATE_ELEMENT_NORMAL;
            transkey.element[j].input_format = ve->src_format;
            transkey.element[j].input_buffer = vbi;
            transkey.element[j].input_offset = ve->src_offset;
            transkey.element[j].instance_divisor = ve->instance_divisor;

            transkey.element[j].output_format = fmt;
            transkey.element[j].output_offset = transkey.output_stride;
            transkey.output_stride += (util_format_get_stride(fmt, 1) + 3) & ~3;

            if (unlikely(ve->instance_divisor)) {
               so->instance_elts |= 1 << i;
               so->instance_bufs |= 1 << vbi;
               if (ve->instance_divisor < so->min_instance_div[vbi])
                  so->min_instance_div[vbi] = ve->instance_divisor;
            }
        }
    }

    so->translate = translate_create(&transkey);
    so->vertex_size = transkey.output_stride / 4;
    so->packet_vertex_limit = NV04_PFIFO_MAX_PACKET_LEN /
       MAX2(so->vertex_size, 1);

    return so;
}

#define NV50_3D_VERTEX_ATTRIB_INACTIVE              \
   NV50_3D_VERTEX_ARRAY_ATTRIB_TYPE_FLOAT |         \
   NV50_3D_VERTEX_ARRAY_ATTRIB_FORMAT_32_32_32_32 | \
   NV50_3D_VERTEX_ARRAY_ATTRIB_CONST

static void
nv50_emit_vtxattr(struct nv50_context *nv50, struct pipe_vertex_buffer *vb,
                  struct pipe_vertex_element *ve, unsigned attr)
{
   struct nouveau_pushbuf *push = nv50->base.pushbuf;
   const void *data = (const uint8_t *)vb->user_buffer + ve->src_offset;
   float v[4];
   const unsigned nc = util_format_get_nr_components(ve->src_format);
   const struct util_format_description *desc =
      util_format_description(ve->src_format);

   assert(vb->user_buffer);

   if (desc->channel[0].pure_integer) {
      if (desc->channel[0].type == UTIL_FORMAT_TYPE_SIGNED) {
         desc->unpack_rgba_sint((int32_t *)v, 0, data, 0, 1, 1);
      } else {
         desc->unpack_rgba_uint((uint32_t *)v, 0, data, 0, 1, 1);
      }
   } else {
      desc->unpack_rgba_float(v, 0, data, 0, 1, 1);
   }

   switch (nc) {
   case 4:
      BEGIN_NV04(push, NV50_3D(VTX_ATTR_4F_X(attr)), 4);
      PUSH_DATAf(push, v[0]);
      PUSH_DATAf(push, v[1]);
      PUSH_DATAf(push, v[2]);
      PUSH_DATAf(push, v[3]);
      break;
   case 3:
      BEGIN_NV04(push, NV50_3D(VTX_ATTR_3F_X(attr)), 3);
      PUSH_DATAf(push, v[0]);
      PUSH_DATAf(push, v[1]);
      PUSH_DATAf(push, v[2]);
      break;
   case 2:
      BEGIN_NV04(push, NV50_3D(VTX_ATTR_2F_X(attr)), 2);
      PUSH_DATAf(push, v[0]);
      PUSH_DATAf(push, v[1]);
      break;
   case 1:
      if (attr == nv50->vertprog->vp.edgeflag) {
         BEGIN_NV04(push, NV50_3D(EDGEFLAG), 1);
         PUSH_DATA (push, v[0] ? 1 : 0);
      }
      BEGIN_NV04(push, NV50_3D(VTX_ATTR_1F(attr)), 1);
      PUSH_DATAf(push, v[0]);
      break;
   default:
      assert(0);
      break;
   }
}

static INLINE void
nv50_user_vbuf_range(struct nv50_context *nv50, int vbi,
                     uint32_t *base, uint32_t *size)
{
   if (unlikely(nv50->vertex->instance_bufs & (1 << vbi))) {
      /* TODO: use min and max instance divisor to get a proper range */
      *base = 0;
      *size = nv50->vtxbuf[vbi].buffer->width0;
   } else {
      /* NOTE: if there are user buffers, we *must* have index bounds */
      assert(nv50->vb_elt_limit != ~0);
      *base = nv50->vb_elt_first * nv50->vtxbuf[vbi].stride;
      *size = nv50->vb_elt_limit * nv50->vtxbuf[vbi].stride +
         nv50->vertex->vb_access_size[vbi];
   }
}

static void
nv50_upload_user_buffers(struct nv50_context *nv50,
                         uint64_t addrs[], uint32_t limits[])
{
   unsigned b;

   for (b = 0; b < nv50->num_vtxbufs; ++b) {
      struct nouveau_bo *bo;
      const struct pipe_vertex_buffer *vb = &nv50->vtxbuf[b];
      uint32_t base, size;

      if (!(nv50->vbo_user & (1 << b)) || !vb->stride)
         continue;
      nv50_user_vbuf_range(nv50, b, &base, &size);

      limits[b] = base + size - 1;
      addrs[b] = nouveau_scratch_data(&nv50->base, vb->user_buffer, base, size,
                                      &bo);
      if (addrs[b])
         BCTX_REFN_bo(nv50->bufctx_3d, VERTEX_TMP, NOUVEAU_BO_GART |
                      NOUVEAU_BO_RD, bo);
   }
   nv50->base.vbo_dirty = TRUE;
}

static void
nv50_update_user_vbufs(struct nv50_context *nv50)
{
   uint64_t address[PIPE_MAX_ATTRIBS];
   struct nouveau_pushbuf *push = nv50->base.pushbuf;
   unsigned i;
   uint32_t written = 0;

   for (i = 0; i < nv50->vertex->num_elements; ++i) {
      struct pipe_vertex_element *ve = &nv50->vertex->element[i].pipe;
      const unsigned b = ve->vertex_buffer_index;
      struct pipe_vertex_buffer *vb = &nv50->vtxbuf[b];
      uint32_t base, size;

      if (!(nv50->vbo_user & (1 << b)))
         continue;

      if (!vb->stride) {
         nv50_emit_vtxattr(nv50, vb, ve, i);
         continue;
      }
      nv50_user_vbuf_range(nv50, b, &base, &size);

      if (!(written & (1 << b))) {
         struct nouveau_bo *bo;
         const uint32_t bo_flags = NOUVEAU_BO_GART | NOUVEAU_BO_RD;
         written |= 1 << b;
         address[b] = nouveau_scratch_data(&nv50->base, vb->user_buffer,
                                           base, size, &bo);
         if (address[b])
            BCTX_REFN_bo(nv50->bufctx_3d, VERTEX_TMP, bo_flags, bo);
      }

      BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_LIMIT_HIGH(i)), 2);
      PUSH_DATAh(push, address[b] + base + size - 1);
      PUSH_DATA (push, address[b] + base + size - 1);
      BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_START_HIGH(i)), 2);
      PUSH_DATAh(push, address[b] + ve->src_offset);
      PUSH_DATA (push, address[b] + ve->src_offset);
   }
   nv50->base.vbo_dirty = TRUE;
}

static INLINE void
nv50_release_user_vbufs(struct nv50_context *nv50)
{
   if (nv50->vbo_user) {
      nouveau_bufctx_reset(nv50->bufctx_3d, NV50_BIND_VERTEX_TMP);
      nouveau_scratch_done(&nv50->base);
   }
}

void
nv50_vertex_arrays_validate(struct nv50_context *nv50)
{
   uint64_t addrs[PIPE_MAX_ATTRIBS];
   uint32_t limits[PIPE_MAX_ATTRIBS];
   struct nouveau_pushbuf *push = nv50->base.pushbuf;
   struct nv50_vertex_stateobj *vertex = nv50->vertex;
   struct pipe_vertex_buffer *vb;
   struct nv50_vertex_element *ve;
   uint32_t mask;
   uint32_t refd = 0;
   unsigned i;
   const unsigned n = MAX2(vertex->num_elements, nv50->state.num_vtxelts);

   if (unlikely(vertex->need_conversion))
      nv50->vbo_fifo = ~0;
   else
   if (nv50->vbo_user & ~nv50->vbo_constant)
      nv50->vbo_fifo = nv50->vbo_push_hint ? ~0 : 0;
   else
      nv50->vbo_fifo = 0;

   if (!nv50->vbo_fifo) {
      /* if vertex buffer was written by GPU - flush VBO cache */
      for (i = 0; i < nv50->num_vtxbufs; ++i) {
         struct nv04_resource *buf = nv04_resource(nv50->vtxbuf[i].buffer);
         if (buf && buf->status & NOUVEAU_BUFFER_STATUS_GPU_WRITING) {
            buf->status &= ~NOUVEAU_BUFFER_STATUS_GPU_WRITING;
            nv50->base.vbo_dirty = TRUE;
            break;
         }
      }
   }

   /* update vertex format state */
   BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_ATTRIB(0)), n);
   if (nv50->vbo_fifo) {
      nv50->state.num_vtxelts = vertex->num_elements;
      for (i = 0; i < vertex->num_elements; ++i)
         PUSH_DATA (push, vertex->element[i].state);
      for (; i < n; ++i)
         PUSH_DATA (push, NV50_3D_VERTEX_ATTRIB_INACTIVE);
      for (i = 0; i < n; ++i) {
         BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_FETCH(i)), 1);
         PUSH_DATA (push, 0);
      }
      return;
   }
   for (i = 0; i < vertex->num_elements; ++i) {
      const unsigned b = vertex->element[i].pipe.vertex_buffer_index;
      ve = &vertex->element[i];
      vb = &nv50->vtxbuf[b];

      if (likely(vb->stride) || !(nv50->vbo_user & (1 << b)))
         PUSH_DATA(push, ve->state);
      else
         PUSH_DATA(push, ve->state | NV50_3D_VERTEX_ARRAY_ATTRIB_CONST);
   }
   for (; i < n; ++i)
      PUSH_DATA(push, NV50_3D_VERTEX_ATTRIB_INACTIVE);

   /* update per-instance enables */
   mask = vertex->instance_elts ^ nv50->state.instance_elts;
   while (mask) {
      const int i = ffs(mask) - 1;
      mask &= ~(1 << i);
      BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_PER_INSTANCE(i)), 1);
      PUSH_DATA (push, (vertex->instance_elts >> i) & 1);
   }
   nv50->state.instance_elts = vertex->instance_elts;

   if (nv50->vbo_user & ~nv50->vbo_constant)
      nv50_upload_user_buffers(nv50, addrs, limits);

   /* update buffers and set constant attributes */
   for (i = 0; i < vertex->num_elements; ++i) {
      uint64_t address, limit;
      const unsigned b = vertex->element[i].pipe.vertex_buffer_index;
      ve = &vertex->element[i];
      vb = &nv50->vtxbuf[b];

      if (unlikely(nv50->vbo_constant & (1 << b))) {
         BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_FETCH(i)), 1);
         PUSH_DATA (push, 0);
         nv50_emit_vtxattr(nv50, vb, &ve->pipe, i);
         continue;
      } else
      if (nv50->vbo_user & (1 << b)) {
         address = addrs[b] + ve->pipe.src_offset;
         limit = addrs[b] + limits[b];
      } else {
         struct nv04_resource *buf = nv04_resource(vb->buffer);
         if (!(refd & (1 << b))) {
            refd |= 1 << b;
            BCTX_REFN(nv50->bufctx_3d, VERTEX, buf, RD);
         }
         address = buf->address + vb->buffer_offset + ve->pipe.src_offset;
         limit = buf->address + buf->base.width0 - 1;
      }

      if (unlikely(ve->pipe.instance_divisor)) {
         BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_FETCH(i)), 4);
         PUSH_DATA (push, NV50_3D_VERTEX_ARRAY_FETCH_ENABLE | vb->stride);
         PUSH_DATAh(push, address);
         PUSH_DATA (push, address);
         PUSH_DATA (push, ve->pipe.instance_divisor);
      } else {
         BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_FETCH(i)), 3);
         PUSH_DATA (push, NV50_3D_VERTEX_ARRAY_FETCH_ENABLE | vb->stride);
         PUSH_DATAh(push, address);
         PUSH_DATA (push, address);
      }
      BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_LIMIT_HIGH(i)), 2);
      PUSH_DATAh(push, limit);
      PUSH_DATA (push, limit);
   }
   for (; i < nv50->state.num_vtxelts; ++i) {
      BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_FETCH(i)), 1);
      PUSH_DATA (push, 0);
   }
   nv50->state.num_vtxelts = vertex->num_elements;
}

#define NV50_PRIM_GL_CASE(n) \
   case PIPE_PRIM_##n: return NV50_3D_VERTEX_BEGIN_GL_PRIMITIVE_##n

static INLINE unsigned
nv50_prim_gl(unsigned prim)
{
   switch (prim) {
   NV50_PRIM_GL_CASE(POINTS);
   NV50_PRIM_GL_CASE(LINES);
   NV50_PRIM_GL_CASE(LINE_LOOP);
   NV50_PRIM_GL_CASE(LINE_STRIP);
   NV50_PRIM_GL_CASE(TRIANGLES);
   NV50_PRIM_GL_CASE(TRIANGLE_STRIP);
   NV50_PRIM_GL_CASE(TRIANGLE_FAN);
   NV50_PRIM_GL_CASE(QUADS);
   NV50_PRIM_GL_CASE(QUAD_STRIP);
   NV50_PRIM_GL_CASE(POLYGON);
   NV50_PRIM_GL_CASE(LINES_ADJACENCY);
   NV50_PRIM_GL_CASE(LINE_STRIP_ADJACENCY);
   NV50_PRIM_GL_CASE(TRIANGLES_ADJACENCY);
   NV50_PRIM_GL_CASE(TRIANGLE_STRIP_ADJACENCY);
   default:
      return NV50_3D_VERTEX_BEGIN_GL_PRIMITIVE_POINTS;
      break;
   }
}

/* For pre-nva0 transform feedback. */
static const uint8_t nv50_pipe_prim_to_prim_size[PIPE_PRIM_MAX + 1] =
{
   [PIPE_PRIM_POINTS] = 1,
   [PIPE_PRIM_LINES] = 2,
   [PIPE_PRIM_LINE_LOOP] = 2,
   [PIPE_PRIM_LINE_STRIP] = 2,
   [PIPE_PRIM_TRIANGLES] = 3,
   [PIPE_PRIM_TRIANGLE_STRIP] = 3,
   [PIPE_PRIM_TRIANGLE_FAN] = 3,
   [PIPE_PRIM_QUADS] = 3,
   [PIPE_PRIM_QUAD_STRIP] = 3,
   [PIPE_PRIM_POLYGON] = 3,
   [PIPE_PRIM_LINES_ADJACENCY] = 2,
   [PIPE_PRIM_LINE_STRIP_ADJACENCY] = 2,
   [PIPE_PRIM_TRIANGLES_ADJACENCY] = 3,
   [PIPE_PRIM_TRIANGLE_STRIP_ADJACENCY] = 3
};

static void
nv50_draw_arrays(struct nv50_context *nv50,
                 unsigned mode, unsigned start, unsigned count,
                 unsigned instance_count)
{
   struct nouveau_pushbuf *push = nv50->base.pushbuf;
   unsigned prim;

   if (nv50->state.index_bias) {
      BEGIN_NV04(push, NV50_3D(VB_ELEMENT_BASE), 1);
      PUSH_DATA (push, 0);
      nv50->state.index_bias = 0;
   }

   prim = nv50_prim_gl(mode);

   while (instance_count--) {
      BEGIN_NV04(push, NV50_3D(VERTEX_BEGIN_GL), 1);
      PUSH_DATA (push, prim);
      BEGIN_NV04(push, NV50_3D(VERTEX_BUFFER_FIRST), 2);
      PUSH_DATA (push, start);
      PUSH_DATA (push, count);
      BEGIN_NV04(push, NV50_3D(VERTEX_END_GL), 1);
      PUSH_DATA (push, 0);

      prim |= NV50_3D_VERTEX_BEGIN_GL_INSTANCE_NEXT;
   }
}

static void
nv50_draw_elements_inline_u08(struct nouveau_pushbuf *push, const uint8_t *map,
                              unsigned start, unsigned count)
{
   map += start;

   if (count & 3) {
      unsigned i;
      BEGIN_NI04(push, NV50_3D(VB_ELEMENT_U32), count & 3);
      for (i = 0; i < (count & 3); ++i)
         PUSH_DATA(push, *map++);
      count &= ~3;
   }
   while (count) {
      unsigned i, nr = MIN2(count, NV04_PFIFO_MAX_PACKET_LEN * 4) / 4;

      BEGIN_NI04(push, NV50_3D(VB_ELEMENT_U8), nr);
      for (i = 0; i < nr; ++i) {
         PUSH_DATA(push,
                   (map[3] << 24) | (map[2] << 16) | (map[1] << 8) | map[0]);
         map += 4;
      }
      count -= nr * 4;
   }
}

static void
nv50_draw_elements_inline_u16(struct nouveau_pushbuf *push, const uint16_t *map,
                              unsigned start, unsigned count)
{
   map += start;

   if (count & 1) {
      count &= ~1;
      BEGIN_NV04(push, NV50_3D(VB_ELEMENT_U32), 1);
      PUSH_DATA (push, *map++);
   }
   while (count) {
      unsigned i, nr = MIN2(count, NV04_PFIFO_MAX_PACKET_LEN * 2) / 2;

      BEGIN_NI04(push, NV50_3D(VB_ELEMENT_U16), nr);
      for (i = 0; i < nr; ++i) {
         PUSH_DATA(push, (map[1] << 16) | map[0]);
         map += 2;
      }
      count -= nr * 2;
   }
}

static void
nv50_draw_elements_inline_u32(struct nouveau_pushbuf *push, const uint32_t *map,
                              unsigned start, unsigned count)
{
   map += start;

   while (count) {
      const unsigned nr = MIN2(count, NV04_PFIFO_MAX_PACKET_LEN);

      BEGIN_NI04(push, NV50_3D(VB_ELEMENT_U32), nr);
      PUSH_DATAp(push, map, nr);

      map += nr;
      count -= nr;
   }
}

static void
nv50_draw_elements_inline_u32_short(struct nouveau_pushbuf *push,
                                    const uint32_t *map,
                                    unsigned start, unsigned count)
{
   map += start;

   if (count & 1) {
      count--;
      BEGIN_NV04(push, NV50_3D(VB_ELEMENT_U32), 1);
      PUSH_DATA (push, *map++);
   }
   while (count) {
      unsigned i, nr = MIN2(count, NV04_PFIFO_MAX_PACKET_LEN * 2) / 2;

      BEGIN_NI04(push, NV50_3D(VB_ELEMENT_U16), nr);
      for (i = 0; i < nr; ++i) {
         PUSH_DATA(push, (map[1] << 16) | map[0]);
         map += 2;
      }
      count -= nr * 2;
   }
}

static void
nv50_draw_elements(struct nv50_context *nv50, boolean shorten,
                   unsigned mode, unsigned start, unsigned count,
                   unsigned instance_count, int32_t index_bias)
{
   struct nouveau_pushbuf *push = nv50->base.pushbuf;
   unsigned prim;
   const unsigned index_size = nv50->idxbuf.index_size;

   prim = nv50_prim_gl(mode);

   if (index_bias != nv50->state.index_bias) {
      BEGIN_NV04(push, NV50_3D(VB_ELEMENT_BASE), 1);
      PUSH_DATA (push, index_bias);
      nv50->state.index_bias = index_bias;
   }

   if (nv50->idxbuf.buffer) {
      struct nv04_resource *buf = nv04_resource(nv50->idxbuf.buffer);
      unsigned pb_start;
      unsigned pb_bytes;
      const unsigned base = (buf->offset + nv50->idxbuf.offset) & ~3;

      start += ((buf->offset + nv50->idxbuf.offset) & 3) >> (index_size >> 1);

      assert(nouveau_resource_mapped_by_gpu(nv50->idxbuf.buffer));

      /* This shouldn't have to be here. The going theory is that the buffer
       * is being filled in by PGRAPH, and it's not done yet by the time it
       * gets submitted to PFIFO, which in turn starts immediately prefetching
       * the not-yet-written data. Ideally this wait would only happen on
       * pushbuf submit, but it's probably not a big performance difference.
       */
      if (buf->fence_wr && !nouveau_fence_signalled(buf->fence_wr))
         nouveau_fence_wait(buf->fence_wr);

      while (instance_count--) {
         BEGIN_NV04(push, NV50_3D(VERTEX_BEGIN_GL), 1);
         PUSH_DATA (push, prim);

         nouveau_pushbuf_space(push, 8, 0, 1);

         switch (index_size) {
         case 4:
            BEGIN_NL50(push, NV50_3D(VB_ELEMENT_U32), count);
            nouveau_pushbuf_data(push, buf->bo, base + start * 4, count * 4);
            break;
         case 2:
            pb_start = (start & ~1) * 2;
            pb_bytes = ((start + count + 1) & ~1) * 2 - pb_start;

            BEGIN_NV04(push, NV50_3D(VB_ELEMENT_U16_SETUP), 1);
            PUSH_DATA (push, (start << 31) | count);
            BEGIN_NL50(push, NV50_3D(VB_ELEMENT_U16), pb_bytes / 4);
            nouveau_pushbuf_data(push, buf->bo, base + pb_start, pb_bytes);
            BEGIN_NV04(push, NV50_3D(VB_ELEMENT_U16_SETUP), 1);
            PUSH_DATA (push, 0);
            break;
         default:
            assert(index_size == 1);
            pb_start = start & ~3;
            pb_bytes = ((start + count + 3) & ~3) - pb_start;

            BEGIN_NV04(push, NV50_3D(VB_ELEMENT_U8_SETUP), 1);
            PUSH_DATA (push, (start << 30) | count);
            BEGIN_NL50(push, NV50_3D(VB_ELEMENT_U8), pb_bytes / 4);
            nouveau_pushbuf_data(push, buf->bo, base + pb_start, pb_bytes);
            BEGIN_NV04(push, NV50_3D(VB_ELEMENT_U8_SETUP), 1);
            PUSH_DATA (push, 0);
            break;
         }
         BEGIN_NV04(push, NV50_3D(VERTEX_END_GL), 1);
         PUSH_DATA (push, 0);

         prim |= NV50_3D_VERTEX_BEGIN_GL_INSTANCE_NEXT;
      }
   } else {
      const void *data = nv50->idxbuf.user_buffer;

      while (instance_count--) {
         BEGIN_NV04(push, NV50_3D(VERTEX_BEGIN_GL), 1);
         PUSH_DATA (push, prim);
         switch (index_size) {
         case 1:
            nv50_draw_elements_inline_u08(push, data, start, count);
            break;
         case 2:
            nv50_draw_elements_inline_u16(push, data, start, count);
            break;
         case 4:
            if (shorten)
               nv50_draw_elements_inline_u32_short(push, data, start, count);
            else
               nv50_draw_elements_inline_u32(push, data, start, count);
            break;
         default:
            assert(0);
            return;
         }
         BEGIN_NV04(push, NV50_3D(VERTEX_END_GL), 1);
         PUSH_DATA (push, 0);

         prim |= NV50_3D_VERTEX_BEGIN_GL_INSTANCE_NEXT;
      }
   }
}

static void
nva0_draw_stream_output(struct nv50_context *nv50,
                        const struct pipe_draw_info *info)
{
   struct nouveau_pushbuf *push = nv50->base.pushbuf;
   struct nv50_so_target *so = nv50_so_target(info->count_from_stream_output);
   struct nv04_resource *res = nv04_resource(so->pipe.buffer);
   unsigned num_instances = info->instance_count;
   unsigned mode = nv50_prim_gl(info->mode);

   if (unlikely(nv50->screen->base.class_3d < NVA0_3D_CLASS)) {
      /* A proper implementation without waiting doesn't seem possible,
       * so don't bother.
       */
      NOUVEAU_ERR("draw_stream_output not supported on pre-NVA0 cards\n");
      return;
   }

   if (res->status & NOUVEAU_BUFFER_STATUS_GPU_WRITING) {
      res->status &= ~NOUVEAU_BUFFER_STATUS_GPU_WRITING;
      PUSH_SPACE(push, 4);
      BEGIN_NV04(push, SUBC_3D(NV50_GRAPH_SERIALIZE), 1);
      PUSH_DATA (push, 0);
      BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_FLUSH), 1);
      PUSH_DATA (push, 0);
   }

   assert(num_instances);
   do {
      PUSH_SPACE(push, 8);
      BEGIN_NV04(push, NV50_3D(VERTEX_BEGIN_GL), 1);
      PUSH_DATA (push, mode);
      BEGIN_NV04(push, NVA0_3D(DRAW_TFB_BASE), 1);
      PUSH_DATA (push, 0);
      BEGIN_NV04(push, NVA0_3D(DRAW_TFB_STRIDE), 1);
      PUSH_DATA (push, 0);
      BEGIN_NV04(push, NVA0_3D(DRAW_TFB_BYTES), 1);
      nv50_query_pushbuf_submit(push, so->pq, 0x4);
      BEGIN_NV04(push, NV50_3D(VERTEX_END_GL), 1);
      PUSH_DATA (push, 0);

      mode |= NV50_3D_VERTEX_BEGIN_GL_INSTANCE_NEXT;
   } while (--num_instances);
}

static void
nv50_draw_vbo_kick_notify(struct nouveau_pushbuf *chan)
{
   struct nv50_screen *screen = chan->user_priv;

   nouveau_fence_update(&screen->base, TRUE);

   nv50_bufctx_fence(screen->cur_ctx->bufctx_3d, TRUE);
}

void
nv50_draw_vbo(struct pipe_context *pipe, const struct pipe_draw_info *info)
{
   struct nv50_context *nv50 = nv50_context(pipe);
   struct nouveau_pushbuf *push = nv50->base.pushbuf;

   /* NOTE: caller must ensure that (min_index + index_bias) is >= 0 */
   nv50->vb_elt_first = info->min_index + info->index_bias;
   nv50->vb_elt_limit = info->max_index - info->min_index;
   nv50->instance_off = info->start_instance;
   nv50->instance_max = info->instance_count - 1;

   /* For picking only a few vertices from a large user buffer, push is better,
    * if index count is larger and we expect repeated vertices, suggest upload.
    */
   nv50->vbo_push_hint = /* the 64 is heuristic */
      !(info->indexed && ((nv50->vb_elt_limit + 64) < info->count));

   if (nv50->vbo_user && !(nv50->dirty & (NV50_NEW_ARRAYS | NV50_NEW_VERTEX))) {
      if (!!nv50->vbo_fifo != nv50->vbo_push_hint)
         nv50->dirty |= NV50_NEW_ARRAYS;
      else
      if (!nv50->vbo_fifo)
         nv50_update_user_vbufs(nv50);
   }

   if (unlikely(nv50->num_so_targets && !nv50->gmtyprog))
      nv50->state.prim_size = nv50_pipe_prim_to_prim_size[info->mode];

   nv50_state_validate(nv50, ~0, 8); /* 8 as minimum, we use flush_notify */

   push->kick_notify = nv50_draw_vbo_kick_notify;

   if (nv50->vbo_fifo) {
      nv50_push_vbo(nv50, info);
      push->kick_notify = nv50_default_kick_notify;
      nouveau_pushbuf_bufctx(push, NULL);
      return;
   }

   if (nv50->state.instance_base != info->start_instance) {
      nv50->state.instance_base = info->start_instance;
      /* NOTE: this does not affect the shader input, should it ? */
      BEGIN_NV04(push, NV50_3D(VB_INSTANCE_BASE), 1);
      PUSH_DATA (push, info->start_instance);
   }

   if (nv50->base.vbo_dirty) {
      BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_FLUSH), 1);
      PUSH_DATA (push, 0);
      nv50->base.vbo_dirty = FALSE;
   }

   if (info->indexed) {
      boolean shorten = info->max_index <= 65535;

      if (info->primitive_restart != nv50->state.prim_restart) {
         if (info->primitive_restart) {
            BEGIN_NV04(push, NV50_3D(PRIM_RESTART_ENABLE), 2);
            PUSH_DATA (push, 1);
            PUSH_DATA (push, info->restart_index);

            if (info->restart_index > 65535)
               shorten = FALSE;
         } else {
            BEGIN_NV04(push, NV50_3D(PRIM_RESTART_ENABLE), 1);
            PUSH_DATA (push, 0);
         }
         nv50->state.prim_restart = info->primitive_restart;
      } else
      if (info->primitive_restart) {
         BEGIN_NV04(push, NV50_3D(PRIM_RESTART_INDEX), 1);
         PUSH_DATA (push, info->restart_index);

         if (info->restart_index > 65535)
            shorten = FALSE;
      }

      nv50_draw_elements(nv50, shorten,
                         info->mode, info->start, info->count,
                         info->instance_count, info->index_bias);
   } else
   if (unlikely(info->count_from_stream_output)) {
      nva0_draw_stream_output(nv50, info);
   } else {
      nv50_draw_arrays(nv50,
                       info->mode, info->start, info->count,
                       info->instance_count);
   }
   push->kick_notify = nv50_default_kick_notify;

   nv50_release_user_vbufs(nv50);

   nouveau_pushbuf_bufctx(push, NULL);
}
@


1.5
log
@Merge Mesa 9.2.5
@
text
@@


1.4
log
@Merge Mesa 9.2.0
@
text
@d600 9
@


1.3
log
@Upate to libGL 7.11.2

Tested by jsg@@, matthieu@@ and ajacoutot@@, ok mattieu@@
@
text
@d17 4
a20 4
 * THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
 * WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF
 * OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
d63 5
d74 1
d88 1
d96 4
d116 2
d139 2
a140 3
   const void *data;
   struct nouveau_channel *chan = nv50->screen->base.channel;
   struct nv04_resource *res = nv04_resource(vb->buffer);
d143 2
d146 1
a146 2
   data = nouveau_resource_map_offset(&nv50->base, res, vb->buffer_offset +
                                      ve->src_offset, NOUVEAU_BO_RD);
d148 9
a156 1
   util_format_read_4f(ve->src_format, v, 0, data, 0, 0, 0, 1, 1);
d160 5
a164 5
      BEGIN_RING(chan, RING_3D(VTX_ATTR_4F_X(attr)), 4);
      OUT_RINGf (chan, v[0]);
      OUT_RINGf (chan, v[1]);
      OUT_RINGf (chan, v[2]);
      OUT_RINGf (chan, v[3]);
d167 4
a170 4
      BEGIN_RING(chan, RING_3D(VTX_ATTR_3F_X(attr)), 3);
      OUT_RINGf (chan, v[0]);
      OUT_RINGf (chan, v[1]);
      OUT_RINGf (chan, v[2]);
d173 3
a175 3
      BEGIN_RING(chan, RING_3D(VTX_ATTR_2F_X(attr)), 2);
      OUT_RINGf (chan, v[0]);
      OUT_RINGf (chan, v[1]);
d179 2
a180 2
         BEGIN_RING(chan, RING_3D(EDGEFLAG_ENABLE), 1);
         OUT_RING  (chan, v[0] ? 1 : 0);
d182 2
a183 2
      BEGIN_RING(chan, RING_3D(VTX_ATTR_1F(attr)), 1);
      OUT_RINGf (chan, v[0]);
d192 2
a193 2
nv50_vbuf_range(struct nv50_context *nv50, int vbi,
                uint32_t *base, uint32_t *size)
d200 5
a204 4
      assert(nv50->vbo_max_index != ~0);
      *base = nv50->vbo_min_index * nv50->vtxbuf[vbi].stride;
      *size = (nv50->vbo_max_index -
               nv50->vbo_min_index + 1) * nv50->vtxbuf[vbi].stride;
d209 2
a210 1
nv50_prevalidate_vbufs(struct nv50_context *nv50)
d212 1
a212 6
   struct pipe_vertex_buffer *vb;
   struct nv04_resource *buf;
   int i;
   uint32_t base, size;

   nv50->vbo_fifo = nv50->vbo_user = 0;
d214 4
a217 1
   nv50_bufctx_reset(nv50, NV50_BUFCTX_VERTEX);
d219 1
a219 3
   for (i = 0; i < nv50->num_vtxbufs; ++i) {
      vb = &nv50->vtxbuf[i];
      if (!vb->stride)
d221 1
a221 1
      buf = nv04_resource(vb->buffer);
d223 6
a228 19
      /* NOTE: user buffers with temporary storage count as mapped by GPU */
      if (!nouveau_resource_mapped_by_gpu(vb->buffer)) {
         if (nv50->vbo_push_hint) {
            nv50->vbo_fifo = ~0;
            continue;
         } else {
            if (buf->status & NOUVEAU_BUFFER_STATUS_USER_MEMORY) {
               nv50->vbo_user |= 1 << i;
               assert(vb->stride > vb->buffer_offset);
               nv50_vbuf_range(nv50, i, &base, &size);
               nouveau_user_buffer_upload(buf, base, size);
            } else {
               nouveau_buffer_migrate(&nv50->base, buf, NOUVEAU_BO_GART);
            }
            nv50->base.vbo_dirty = TRUE;
         }
      }
      nv50_bufctx_add_resident(nv50, NV50_BUFCTX_VERTEX, buf, NOUVEAU_BO_RD);
      nouveau_buffer_adjust_score(&nv50->base, buf, 1);
d230 1
d236 3
a238 3
   struct nouveau_channel *chan = nv50->screen->base.channel;
   uint32_t base, offset, size;
   int i;
d243 1
a243 1
      const int b = ve->vertex_buffer_index;
d245 1
a245 1
      struct nv04_resource *buf = nv04_resource(vb->buffer);
d254 1
a254 1
      nv50_vbuf_range(nv50, b, &base, &size);
d257 2
d260 4
a263 1
         nouveau_user_buffer_upload(buf, base, size);
a264 1
      offset = vb->buffer_offset + ve->src_offset;
d266 6
a271 7
      MARK_RING (chan, 6, 4);
      BEGIN_RING(chan, RING_3D(VERTEX_ARRAY_LIMIT_HIGH(i)), 2);
      OUT_RESRCh(chan, buf, base + size - 1, NOUVEAU_BO_RD);
      OUT_RESRCl(chan, buf, base + size - 1, NOUVEAU_BO_RD);
      BEGIN_RING(chan, RING_3D(VERTEX_ARRAY_START_HIGH(i)), 2);
      OUT_RESRCh(chan, buf, offset, NOUVEAU_BO_RD);
      OUT_RESRCl(chan, buf, offset, NOUVEAU_BO_RD);
d279 3
a281 7
   uint32_t vbo_user = nv50->vbo_user;

   while (vbo_user) {
      int i = ffs(vbo_user) - 1;
      vbo_user &= ~(1 << i);

      nouveau_buffer_release_gpu_storage(nv04_resource(nv50->vtxbuf[i].buffer));
d288 3
a290 1
   struct nouveau_channel *chan = nv50->screen->base.channel;
d294 2
d297 1
d299 1
a299 1
   if (unlikely(vertex->need_conversion)) {
d301 16
a316 3
      nv50->vbo_user = 0;
   } else {
      nv50_prevalidate_vbufs(nv50);
d319 14
a332 1
   BEGIN_RING(chan, RING_3D(VERTEX_ARRAY_ATTRIB(0)), vertex->num_elements);
d334 1
d336 1
a336 1
      vb = &nv50->vtxbuf[ve->pipe.vertex_buffer_index];
d338 15
a352 6
      if (likely(vb->stride) || nv50->vbo_fifo) {
         OUT_RING(chan, ve->state);
      } else {
         OUT_RING(chan, ve->state | NV50_3D_VERTEX_ARRAY_ATTRIB_CONST);
         nv50->vbo_fifo &= ~(1 << i);
      }
d354 4
d359 1
d361 2
a362 3
      struct nv04_resource *res;
      unsigned size, offset;
      
d364 1
a364 1
      vb = &nv50->vtxbuf[ve->pipe.vertex_buffer_index];
d366 14
a379 4
      if (unlikely(ve->pipe.instance_divisor)) {
         if (!(nv50->state.instance_elts & (1 << i))) {
            BEGIN_RING(chan, RING_3D(VERTEX_ARRAY_PER_INSTANCE(i)), 1);
            OUT_RING  (chan, 1);
d381 2
a382 6
         BEGIN_RING(chan, RING_3D(VERTEX_ARRAY_DIVISOR(i)), 1);
         OUT_RING  (chan, ve->pipe.instance_divisor);
      } else
      if (unlikely(nv50->state.instance_elts & (1 << i))) {
         BEGIN_RING(chan, RING_3D(VERTEX_ARRAY_PER_INSTANCE(i)), 1);
         OUT_RING  (chan, 0);
d385 15
a399 22
      res = nv04_resource(vb->buffer);

      if (nv50->vbo_fifo || unlikely(vb->stride == 0)) {
         if (!nv50->vbo_fifo)
            nv50_emit_vtxattr(nv50, vb, &ve->pipe, i);
         BEGIN_RING(chan, RING_3D(VERTEX_ARRAY_FETCH(i)), 1);
         OUT_RING  (chan, 0);
         continue;
      }

      size = vb->buffer->width0;
      offset = ve->pipe.src_offset + vb->buffer_offset;

      MARK_RING (chan, 8, 4);
      BEGIN_RING(chan, RING_3D(VERTEX_ARRAY_FETCH(i)), 1);
      OUT_RING  (chan, NV50_3D_VERTEX_ARRAY_FETCH_ENABLE | vb->stride);
      BEGIN_RING(chan, RING_3D(VERTEX_ARRAY_LIMIT_HIGH(i)), 2);
      OUT_RESRCh(chan, res, size - 1, NOUVEAU_BO_RD);
      OUT_RESRCl(chan, res, size - 1, NOUVEAU_BO_RD);
      BEGIN_RING(chan, RING_3D(VERTEX_ARRAY_START_HIGH(i)), 2);
      OUT_RESRCh(chan, res, offset, NOUVEAU_BO_RD);
      OUT_RESRCl(chan, res, offset, NOUVEAU_BO_RD);
d402 2
a403 4
      BEGIN_RING(chan, RING_3D(VERTEX_ARRAY_ATTRIB(i)), 1);
      OUT_RING  (chan, NV50_3D_VERTEX_ATTRIB_INACTIVE);
      BEGIN_RING(chan, RING_3D(VERTEX_ARRAY_FETCH(i)), 1);
      OUT_RING  (chan, 0);
a404 1

a405 1
   nv50->state.instance_elts = vertex->instance_elts;
d435 2
a436 2
static void
nv50_draw_vbo_flush_notify(struct nouveau_channel *chan)
d438 15
a452 6
   struct nv50_screen *screen = chan->user_private;

   nouveau_fence_update(&screen->base, TRUE);

   nv50_bufctx_emit_relocs(screen->cur_ctx);
}
d459 1
a459 1
   struct nouveau_channel *chan = nv50->screen->base.channel;
d462 6
d471 7
a477 7
      BEGIN_RING(chan, RING_3D(VERTEX_BEGIN_GL), 1);
      OUT_RING  (chan, prim);
      BEGIN_RING(chan, RING_3D(VERTEX_BUFFER_FIRST), 2);
      OUT_RING  (chan, start);
      OUT_RING  (chan, count);
      BEGIN_RING(chan, RING_3D(VERTEX_END_GL), 1);
      OUT_RING  (chan, 0);
d484 1
a484 1
nv50_draw_elements_inline_u08(struct nouveau_channel *chan, uint8_t *map,
d491 1
a491 1
      BEGIN_RING_NI(chan, RING_3D(VB_ELEMENT_U32), count & 3);
d493 1
a493 1
         OUT_RING(chan, *map++);
d499 1
a499 1
      BEGIN_RING_NI(chan, RING_3D(VB_ELEMENT_U8), nr);
d501 2
a502 2
         OUT_RING(chan,
                  (map[3] << 24) | (map[2] << 16) | (map[1] << 8) | map[0]);
d510 1
a510 1
nv50_draw_elements_inline_u16(struct nouveau_channel *chan, uint16_t *map,
d517 2
a518 2
      BEGIN_RING(chan, RING_3D(VB_ELEMENT_U32), 1);
      OUT_RING  (chan, *map++);
d523 1
a523 1
      BEGIN_RING_NI(chan, RING_3D(VB_ELEMENT_U16), nr);
d525 1
a525 1
         OUT_RING(chan, (map[1] << 16) | map[0]);
d533 1
a533 1
nv50_draw_elements_inline_u32(struct nouveau_channel *chan, uint32_t *map,
d541 2
a542 2
      BEGIN_RING_NI(chan, RING_3D(VB_ELEMENT_U32), nr);
      OUT_RINGp    (chan, map, nr);
d550 2
a551 1
nv50_draw_elements_inline_u32_short(struct nouveau_channel *chan, uint32_t *map,
d558 2
a559 2
      BEGIN_RING(chan, RING_3D(VB_ELEMENT_U32), 1);
      OUT_RING  (chan, *map++);
d564 1
a564 1
      BEGIN_RING_NI(chan, RING_3D(VB_ELEMENT_U16), nr);
d566 1
a566 1
         OUT_RING(chan, (map[1] << 16) | map[0]);
d578 1
a578 2
   struct nouveau_channel *chan = nv50->screen->base.channel;
   void *data;
d585 2
a586 2
      BEGIN_RING(chan, RING_3D(VB_ELEMENT_BASE), 1);
      OUT_RING  (chan, index_bias);
d590 5
a594 2
   if (nouveau_resource_mapped_by_gpu(nv50->idxbuf.buffer)) {
      struct nv04_resource *res = nv04_resource(nv50->idxbuf.buffer);
d596 1
a596 1
      start += nv50->idxbuf.offset >> (index_size >> 1);
d598 1
a598 1
      nouveau_buffer_adjust_score(&nv50->base, res, 1);
d601 4
a604 2
         BEGIN_RING(chan, RING_3D(VERTEX_BEGIN_GL), 1);
         OUT_RING  (chan, mode);
d608 2
a609 7
         {
            WAIT_RING (chan, 2);
            BEGIN_RING(chan, RING_3D(VB_ELEMENT_U32) | 0x30000, 0);
            OUT_RING  (chan, count);
            nouveau_pushbuf_submit(chan, res->bo, res->offset + start * 4,
                                   count * 4);
         }
d612 9
a620 13
         {
            unsigned pb_start = (start & ~1);
            unsigned pb_words = (((start + count + 1) & ~1) - pb_start) >> 1;

            BEGIN_RING(chan, RING_3D(VB_ELEMENT_U16_SETUP), 1);
            OUT_RING  (chan, (start << 31) | count);
            WAIT_RING (chan, 2);
            BEGIN_RING(chan, RING_3D(VB_ELEMENT_U16) | 0x30000, 0);
            OUT_RING  (chan, pb_words);
            nouveau_pushbuf_submit(chan, res->bo, res->offset + pb_start * 2,
                                   pb_words * 4);
            BEGIN_RING(chan, RING_3D(VB_ELEMENT_U16_SETUP), 1);
            OUT_RING  (chan, 0);
d622 11
a632 15
         }
         case 1:
         {
            unsigned pb_start = (start & ~3);
            unsigned pb_words = (((start + count + 3) & ~3) - pb_start) >> 1;

            BEGIN_RING(chan, RING_3D(VB_ELEMENT_U8_SETUP), 1);
            OUT_RING  (chan, (start << 30) | count);
            WAIT_RING (chan, 2);
            BEGIN_RING(chan, RING_3D(VB_ELEMENT_U8) | 0x30000, 0);
            OUT_RING  (chan, pb_words);
            nouveau_pushbuf_submit(chan, res->bo, res->offset + pb_start,
                                   pb_words * 4);
            BEGIN_RING(chan, RING_3D(VB_ELEMENT_U8_SETUP), 1);
            OUT_RING  (chan, 0);
d635 2
a636 6
         default:
            assert(0);
            return;
         }
         BEGIN_RING(chan, RING_3D(VERTEX_END_GL), 1);
         OUT_RING  (chan, 0);
d638 1
a638 3
         nv50_resource_fence(res, NOUVEAU_BO_RD);

         mode |= NV50_3D_VERTEX_BEGIN_GL_INSTANCE_NEXT;
d641 1
a641 5
      data = nouveau_resource_map_offset(&nv50->base,
                                         nv04_resource(nv50->idxbuf.buffer),
                                         nv50->idxbuf.offset, NOUVEAU_BO_RD);
      if (!data)
         return;
d644 2
a645 2
         BEGIN_RING(chan, RING_3D(VERTEX_BEGIN_GL), 1);
         OUT_RING  (chan, prim);
d648 1
a648 1
            nv50_draw_elements_inline_u08(chan, data, start, count);
d651 1
a651 1
            nv50_draw_elements_inline_u16(chan, data, start, count);
d655 1
a655 1
               nv50_draw_elements_inline_u32_short(chan, data, start, count);
d657 1
a657 1
               nv50_draw_elements_inline_u32(chan, data, start, count);
d663 2
a664 2
         BEGIN_RING(chan, RING_3D(VERTEX_END_GL), 1);
         OUT_RING  (chan, 0);
d671 55
d730 7
a736 1
   struct nouveau_channel *chan = nv50->screen->base.channel;
d742 1
a742 2
      !(info->indexed &&
        ((info->max_index - info->min_index + 64) < info->count));
d744 7
a750 2
   nv50->vbo_min_index = info->min_index;
   nv50->vbo_max_index = info->max_index;
d752 2
a753 2
   if (nv50->vbo_push_hint != !!nv50->vbo_fifo)
      nv50->dirty |= NV50_NEW_ARRAYS;
d755 1
a755 2
   if (nv50->vbo_user && !(nv50->dirty & (NV50_NEW_VERTEX | NV50_NEW_ARRAYS)))
      nv50_update_user_vbufs(nv50);
d757 1
a757 3
   nv50_state_validate(nv50);

   chan->flush_notify = nv50_draw_vbo_flush_notify;
d761 2
a762 1
      chan->flush_notify = nv50_default_flush_notify;
d769 2
a770 2
      BEGIN_RING(chan, RING_3D(VB_INSTANCE_BASE), 1);
      OUT_RING  (chan, info->start_instance);
d774 2
a775 2
      BEGIN_RING(chan, RING_3D(VERTEX_ARRAY_FLUSH), 1);
      OUT_RING  (chan, 0);
d779 1
a779 5
   if (!info->indexed) {
      nv50_draw_arrays(nv50,
                       info->mode, info->start, info->count,
                       info->instance_count);
   } else {
a781 2
      assert(nv50->idxbuf.buffer);

d784 3
a786 3
            BEGIN_RING(chan, RING_3D(PRIM_RESTART_ENABLE), 2);
            OUT_RING  (chan, 1);
            OUT_RING  (chan, info->restart_index);
d791 2
a792 2
            BEGIN_RING(chan, RING_3D(PRIM_RESTART_ENABLE), 1);
            OUT_RING  (chan, 0);
d797 2
a798 2
         BEGIN_RING(chan, RING_3D(PRIM_RESTART_INDEX), 1);
         OUT_RING  (chan, info->restart_index);
d807 7
d815 1
a815 1
   chan->flush_notify = nv50_default_flush_notify;
d818 2
@


1.2
log
@Merge Mesa 7.10.3
@
text
@d2 1
a2 1
 * Copyright 2008 Ben Skeggs
d27 1
a27 1
#include "util/u_split_prim.h"
d32 1
a32 7
struct instance {
	struct nouveau_bo *bo;
	unsigned delta;
	unsigned stride;
	unsigned step;
	unsigned divisor;
};
d34 3
a36 2
static void
instance_init(struct nv50_context *nv50, struct instance *a, unsigned first)
d38 1
a38 1
	int i;
d40 81
a120 16
	for (i = 0; i < nv50->vtxelt->num_elements; i++) {
		struct pipe_vertex_element *ve = &nv50->vtxelt->pipe[i];
		struct pipe_vertex_buffer *vb;

		a[i].divisor = ve->instance_divisor;
		if (a[i].divisor) {
			vb = &nv50->vtxbuf[ve->vertex_buffer_index];

			a[i].bo = nv50_resource(vb->buffer)->bo;
			a[i].stride = vb->stride;
			a[i].step = first % a[i].divisor;
			a[i].delta = vb->buffer_offset + ve->src_offset +
				     (first * a[i].stride);
		}
	}
}
d123 2
a124 1
instance_step(struct nv50_context *nv50, struct instance *a)
d126 58
a183 19
	struct nouveau_channel *chan = nv50->screen->tesla->channel;
	struct nouveau_grobj *tesla = nv50->screen->tesla;
	int i;

	for (i = 0; i < nv50->vtxelt->num_elements; i++) {
		if (!a[i].divisor)
			continue;

		BEGIN_RING(chan, tesla,
			   NV50TCL_VERTEX_ARRAY_START_HIGH(i), 2);
		OUT_RELOCh(chan, a[i].bo, a[i].delta, NOUVEAU_BO_RD |
			   NOUVEAU_BO_VRAM | NOUVEAU_BO_GART);
		OUT_RELOCl(chan, a[i].bo, a[i].delta, NOUVEAU_BO_RD |
			   NOUVEAU_BO_VRAM | NOUVEAU_BO_GART);
		if (++a[i].step == a[i].divisor) {
			a[i].step = 0;
			a[i].delta += a[i].stride;
		}
	}
d187 38
a224 50
nv50_draw_arrays_instanced(struct pipe_context *pipe,
			   unsigned mode, unsigned start, unsigned count,
			   unsigned startInstance, unsigned instanceCount)
{
	struct nv50_context *nv50 = nv50_context(pipe);
	struct nouveau_channel *chan = nv50->screen->tesla->channel;
	struct nouveau_grobj *tesla = nv50->screen->tesla;
	struct instance a[16];
	unsigned prim = nv50_prim(mode);

	instance_init(nv50, a, startInstance);
	if (!nv50_state_validate(nv50, 10 + 16*3))
		return;

	if (nv50->vbo_fifo) {
		nv50_push_elements_instanced(pipe, NULL, 0, 0, mode, start,
					     count, startInstance,
					     instanceCount);
		return;
	}

	BEGIN_RING(chan, tesla, NV50TCL_CB_ADDR, 2);
	OUT_RING  (chan, NV50_CB_AUX | (24 << 8));
	OUT_RING  (chan, startInstance);
	while (instanceCount--) {
		if (AVAIL_RING(chan) < (7 + 16*3)) {
			FIRE_RING(chan);
			if (!nv50_state_validate(nv50, 7 + 16*3)) {
				assert(0);
				return;
			}
		}
		instance_step(nv50, a);

		BEGIN_RING(chan, tesla, NV50TCL_VERTEX_BEGIN, 1);
		OUT_RING  (chan, prim);
		BEGIN_RING(chan, tesla, NV50TCL_VERTEX_BUFFER_FIRST, 2);
		OUT_RING  (chan, start);
		OUT_RING  (chan, count);
		BEGIN_RING(chan, tesla, NV50TCL_VERTEX_END, 1);
		OUT_RING  (chan, 0);

		prim |= (1 << 28);
	}
}

struct inline_ctx {
	struct nv50_context *nv50;
	void *map;
};
d227 1
a227 1
inline_elt08(void *priv, unsigned start, unsigned count)
d229 48
a276 21
	struct inline_ctx *ctx = priv;
	struct nouveau_grobj *tesla = ctx->nv50->screen->tesla;
	struct nouveau_channel *chan = tesla->channel;
	uint8_t *map = (uint8_t *)ctx->map + start;

	if (count & 1) {
		BEGIN_RING(chan, tesla, NV50TCL_VB_ELEMENT_U32, 1);
		OUT_RING  (chan, map[0]);
		map++;
		count &= ~1;
	}

	count >>= 1;
	if (!count)
		return;

	BEGIN_RING_NI(chan, tesla, NV50TCL_VB_ELEMENT_U16, count);
	while (count--) {
		OUT_RING(chan, (map[1] << 16) | map[0]);
		map += 2;
	}
d279 2
a280 2
static void
inline_elt16(void *priv, unsigned start, unsigned count)
d282 105
a386 21
	struct inline_ctx *ctx = priv;
	struct nouveau_grobj *tesla = ctx->nv50->screen->tesla;
	struct nouveau_channel *chan = tesla->channel;
	uint16_t *map = (uint16_t *)ctx->map + start;

	if (count & 1) {
		BEGIN_RING(chan, tesla, NV50TCL_VB_ELEMENT_U32, 1);
		OUT_RING  (chan, map[0]);
		count &= ~1;
		map++;
	}

	count >>= 1;
	if (!count)
		return;

	BEGIN_RING_NI(chan, tesla, NV50TCL_VB_ELEMENT_U16, count);
	while (count--) {
		OUT_RING(chan, (map[1] << 16) | map[0]);
		map += 2;
	}
d390 1
a390 1
inline_elt32(void *priv, unsigned start, unsigned count)
d392 3
a394 3
	struct inline_ctx *ctx = priv;
	struct nouveau_grobj *tesla = ctx->nv50->screen->tesla;
	struct nouveau_channel *chan = tesla->channel;
d396 1
a396 2
	BEGIN_RING_NI(chan, tesla, NV50TCL_VB_ELEMENT_U32, count);
	OUT_RINGp    (chan, (uint32_t *)ctx->map + start, count);
d400 17
a416 5
inline_edgeflag(void *priv, boolean enabled)
{
	struct inline_ctx *ctx = priv;
	struct nouveau_grobj *tesla = ctx->nv50->screen->tesla;
	struct nouveau_channel *chan = tesla->channel;
d418 2
a419 2
	BEGIN_RING(chan, tesla, NV50TCL_EDGEFLAG_ENABLE, 1);
	OUT_RING  (chan, enabled ? 1 : 0);
d423 4
a426 73
nv50_draw_elements_inline(struct pipe_context *pipe,
			  struct pipe_resource *indexBuffer, unsigned indexSize,
			  unsigned mode, unsigned start, unsigned count,
			  unsigned startInstance, unsigned instanceCount)
{
	struct nv50_context *nv50 = nv50_context(pipe);
	struct nouveau_channel *chan = nv50->screen->tesla->channel;
	struct nouveau_grobj *tesla = nv50->screen->tesla;
	struct pipe_transfer *transfer;
	struct instance a[16];
	struct inline_ctx ctx;
	struct util_split_prim s;
	boolean nzi = FALSE;
	unsigned overhead;

	overhead = 16*3; /* potential instance adjustments */
	overhead += 4; /* Begin()/End() */
	overhead += 4; /* potential edgeflag disable/reenable */
	overhead += 3; /* potentially 3 VTX_ELT_U16/U32 packet headers */

	s.priv = &ctx;
	if (indexSize == 1)
		s.emit = inline_elt08;
	else
	if (indexSize == 2)
		s.emit = inline_elt16;
	else
		s.emit = inline_elt32;
	s.edge = inline_edgeflag;

	ctx.nv50 = nv50;
	ctx.map = pipe_buffer_map(pipe, indexBuffer, PIPE_TRANSFER_READ, &transfer);
	assert(ctx.map);
	if (!ctx.map)
		return;

	instance_init(nv50, a, startInstance);
	if (!nv50_state_validate(nv50, overhead + 6 + 3))
		return;

	BEGIN_RING(chan, tesla, NV50TCL_CB_ADDR, 2);
	OUT_RING  (chan, NV50_CB_AUX | (24 << 8));
	OUT_RING  (chan, startInstance);
	while (instanceCount--) {
		unsigned max_verts;
		boolean done;

		util_split_prim_init(&s, mode, start, count);
		do {
			if (AVAIL_RING(chan) < (overhead + 6)) {
				FIRE_RING(chan);
				if (!nv50_state_validate(nv50, (overhead + 6))) {
					assert(0);
					return;
				}
			}

			max_verts = AVAIL_RING(chan) - overhead;
			if (max_verts > 2047)
				max_verts = 2047;
			if (indexSize != 4)
				max_verts <<= 1;
			instance_step(nv50, a);

			BEGIN_RING(chan, tesla, NV50TCL_VERTEX_BEGIN, 1);
			OUT_RING  (chan, nv50_prim(s.mode) | (nzi ? (1<<28) : 0));
			done = util_split_prim_next(&s, max_verts);
			BEGIN_RING(chan, tesla, NV50TCL_VERTEX_END, 1);
			OUT_RING  (chan, 0);
		} while (!done);

		nzi = TRUE;
	}
d428 18
a445 1
	pipe_buffer_unmap(pipe, indexBuffer, transfer);
d449 4
a452 73
nv50_draw_elements_instanced(struct pipe_context *pipe,
			     struct pipe_resource *indexBuffer,
			     unsigned indexSize, int indexBias,
			     unsigned mode, unsigned start, unsigned count,
			     unsigned startInstance, unsigned instanceCount)
{
	struct nv50_context *nv50 = nv50_context(pipe);
	struct nouveau_channel *chan = nv50->screen->tesla->channel;
	struct nouveau_grobj *tesla = nv50->screen->tesla;
	struct instance a[16];
	unsigned prim = nv50_prim(mode);

	instance_init(nv50, a, startInstance);
	if (!nv50_state_validate(nv50, 13 + 16*3))
		return;

	if (nv50->vbo_fifo) {
		nv50_push_elements_instanced(pipe, indexBuffer, indexSize,
					     indexBias, mode, start, count,
					     startInstance, instanceCount);
		return;
	}

	/* indices are uint32 internally, so large indexBias means negative */
	BEGIN_RING(chan, tesla, NV50TCL_VB_ELEMENT_BASE, 1);
	OUT_RING  (chan, indexBias);

	if (!nv50_resource_mapped_by_gpu(indexBuffer) || indexSize == 1) {
		nv50_draw_elements_inline(pipe, indexBuffer, indexSize,
					  mode, start, count, startInstance,
					  instanceCount);
		return;
	}

	BEGIN_RING(chan, tesla, NV50TCL_CB_ADDR, 2);
	OUT_RING  (chan, NV50_CB_AUX | (24 << 8));
	OUT_RING  (chan, startInstance);
	while (instanceCount--) {
		if (AVAIL_RING(chan) < (7 + 16*3)) {
			FIRE_RING(chan);
			if (!nv50_state_validate(nv50, 10 + 16*3)) {
				assert(0);
				return;
			}
		}
		instance_step(nv50, a);

		BEGIN_RING(chan, tesla, NV50TCL_VERTEX_BEGIN, 1);
		OUT_RING  (chan, prim);
		if (indexSize == 4) {
			BEGIN_RING(chan, tesla, NV50TCL_VB_ELEMENT_U32 | 0x30000, 0);
			OUT_RING  (chan, count);
			nouveau_pushbuf_submit(chan, 
					       nv50_resource(indexBuffer)->bo,
					       start << 2, count << 2);
		} else
		if (indexSize == 2) {
			unsigned vb_start = (start & ~1);
			unsigned vb_end = (start + count + 1) & ~1;
			unsigned dwords = (vb_end - vb_start) >> 1;

			BEGIN_RING(chan, tesla, NV50TCL_VB_ELEMENT_U16_SETUP, 1);
			OUT_RING  (chan, ((start & 1) << 31) | count);
			BEGIN_RING(chan, tesla, NV50TCL_VB_ELEMENT_U16 | 0x30000, 0);
			OUT_RING  (chan, dwords);
			nouveau_pushbuf_submit(chan,
					       nv50_resource(indexBuffer)->bo,
					       vb_start << 1, dwords << 2);
			BEGIN_RING(chan, tesla, NV50TCL_VB_ELEMENT_U16_SETUP, 1);
			OUT_RING  (chan, 0);
		}
		BEGIN_RING(chan, tesla, NV50TCL_VERTEX_END, 1);
		OUT_RING  (chan, 0);
d454 15
a468 2
		prim |= (1 << 28);
	}
d471 3
a473 2
void
nv50_draw_vbo(struct pipe_context *pipe, const struct pipe_draw_info *info)
d475 1
a475 1
	struct nv50_context *nv50 = nv50_context(pipe);
d477 2
a478 2
	if (info->indexed && nv50->idxbuf.buffer) {
		unsigned offset;
d480 2
a481 2
		assert(nv50->idxbuf.offset % nv50->idxbuf.index_size == 0);
		offset = nv50->idxbuf.offset / nv50->idxbuf.index_size;
d483 3
a485 80
		nv50_draw_elements_instanced(pipe,
					     nv50->idxbuf.buffer,
					     nv50->idxbuf.index_size,
					     info->index_bias,
					     info->mode,
					     info->start + offset,
					     info->count,
					     info->start_instance,
					     info->instance_count);
	}
	else {
		nv50_draw_arrays_instanced(pipe,
					   info->mode,
					   info->start,
					   info->count,
					   info->start_instance,
					   info->instance_count);
	}
}

static INLINE boolean
nv50_vbo_static_attrib(struct nv50_context *nv50, unsigned attrib,
		       struct nouveau_stateobj **pso,
		       struct pipe_vertex_element *ve,
		       struct pipe_vertex_buffer *vb)

{
	struct nouveau_stateobj *so;
	struct nouveau_grobj *tesla = nv50->screen->tesla;
	struct nouveau_bo *bo = nv50_resource(vb->buffer)->bo;
	float v[4];
	int ret;
	unsigned nr_components = util_format_get_nr_components(ve->src_format);

	ret = nouveau_bo_map(bo, NOUVEAU_BO_RD);
	if (ret)
		return FALSE;

	util_format_read_4f(ve->src_format, v, 0, (uint8_t *)bo->map +
			    (vb->buffer_offset + ve->src_offset), 0,
			    0, 0, 1, 1);
	so = *pso;
	if (!so)
		*pso = so = so_new(nv50->vtxelt->num_elements,
				   nv50->vtxelt->num_elements * 4, 0);

	switch (nr_components) {
	case 4:
		so_method(so, tesla, NV50TCL_VTX_ATTR_4F_X(attrib), 4);
		so_data  (so, fui(v[0]));
		so_data  (so, fui(v[1]));
		so_data  (so, fui(v[2]));
		so_data  (so, fui(v[3]));
		break;
	case 3:
		so_method(so, tesla, NV50TCL_VTX_ATTR_3F_X(attrib), 3);
		so_data  (so, fui(v[0]));
		so_data  (so, fui(v[1]));
		so_data  (so, fui(v[2]));
		break;
	case 2:
		so_method(so, tesla, NV50TCL_VTX_ATTR_2F_X(attrib), 2);
		so_data  (so, fui(v[0]));
		so_data  (so, fui(v[1]));
		break;
	case 1:
		if (attrib == nv50->vertprog->vp.edgeflag) {
			so_method(so, tesla, NV50TCL_EDGEFLAG_ENABLE, 1);
			so_data  (so, v[0] ? 1 : 0);
		}
		so_method(so, tesla, NV50TCL_VTX_ATTR_1F(attrib), 1);
		so_data  (so, fui(v[0]));
		break;
	default:
		nouveau_bo_unmap(bo);
		return FALSE;
	}

	nouveau_bo_unmap(bo);
	return TRUE;
d488 3
a490 2
void
nv50_vtxelt_construct(struct nv50_vtxelt_stateobj *cso)
d492 1
a492 1
	unsigned i;
d494 15
a508 2
	for (i = 0; i < cso->num_elements; ++i)
		cso->hw[i] = nv50_format_table[cso->pipe[i].src_format].vtx;
d511 115
a625 90
struct nouveau_stateobj *
nv50_vbo_validate(struct nv50_context *nv50)
{
	struct nouveau_grobj *tesla = nv50->screen->tesla;
	struct nouveau_stateobj *vtxbuf, *vtxfmt, *vtxattr;
	unsigned i, n_ve;

	/* don't validate if Gallium took away our buffers */
	if (nv50->vtxbuf_nr == 0)
		return NULL;

	nv50->vbo_fifo = 0;
	if (nv50->screen->force_push ||
	    nv50->vertprog->vp.edgeflag < 16)
		nv50->vbo_fifo = 0xffff;

	for (i = 0; i < nv50->vtxbuf_nr; i++) {
		if (nv50->vtxbuf[i].stride &&
		    !nv50_resource_mapped_by_gpu(nv50->vtxbuf[i].buffer))
			nv50->vbo_fifo = 0xffff;
	}

	n_ve = MAX2(nv50->vtxelt->num_elements, nv50->state.vtxelt_nr);

	vtxattr = NULL;
	vtxbuf = so_new(n_ve * 2, n_ve * 5, nv50->vtxelt->num_elements * 4);
	vtxfmt = so_new(1, n_ve, 0);
	so_method(vtxfmt, tesla, NV50TCL_VERTEX_ARRAY_ATTRIB(0), n_ve);

	for (i = 0; i < nv50->vtxelt->num_elements; i++) {
		struct pipe_vertex_element *ve = &nv50->vtxelt->pipe[i];
		struct pipe_vertex_buffer *vb =
			&nv50->vtxbuf[ve->vertex_buffer_index];
		struct nouveau_bo *bo = nv50_resource(vb->buffer)->bo;
		uint32_t hw = nv50->vtxelt->hw[i];

		if (!vb->stride &&
		    nv50_vbo_static_attrib(nv50, i, &vtxattr, ve, vb)) {
			so_data(vtxfmt, hw | (1 << 4));

			so_method(vtxbuf, tesla,
				  NV50TCL_VERTEX_ARRAY_FORMAT(i), 1);
			so_data  (vtxbuf, 0);

			nv50->vbo_fifo &= ~(1 << i);
			continue;
		}

		if (nv50->vbo_fifo) {
			so_data  (vtxfmt, hw | (ve->instance_divisor ? (1 << 4) : i));
			so_method(vtxbuf, tesla,
				  NV50TCL_VERTEX_ARRAY_FORMAT(i), 1);
			so_data  (vtxbuf, 0);
			continue;
		}

		so_data(vtxfmt, hw | i);

		so_method(vtxbuf, tesla, NV50TCL_VERTEX_ARRAY_FORMAT(i), 3);
		so_data  (vtxbuf, 0x20000000 |
			  (ve->instance_divisor ? 0 : vb->stride));
		so_reloc (vtxbuf, bo, vb->buffer_offset +
			  ve->src_offset, NOUVEAU_BO_VRAM | NOUVEAU_BO_GART |
			  NOUVEAU_BO_RD | NOUVEAU_BO_HIGH, 0, 0);
		so_reloc (vtxbuf, bo, vb->buffer_offset +
			  ve->src_offset, NOUVEAU_BO_VRAM | NOUVEAU_BO_GART |
			  NOUVEAU_BO_RD | NOUVEAU_BO_LOW, 0, 0);

		/* vertex array limits */
		so_method(vtxbuf, tesla, NV50TCL_VERTEX_ARRAY_LIMIT_HIGH(i), 2);
		so_reloc (vtxbuf, bo, vb->buffer->width0 - 1,
			  NOUVEAU_BO_VRAM | NOUVEAU_BO_GART | NOUVEAU_BO_RD |
			  NOUVEAU_BO_HIGH, 0, 0);
		so_reloc (vtxbuf, bo, vb->buffer->width0 - 1,
			  NOUVEAU_BO_VRAM | NOUVEAU_BO_GART | NOUVEAU_BO_RD |
			  NOUVEAU_BO_LOW, 0, 0);
	}
	for (; i < n_ve; ++i) {
		so_data  (vtxfmt, 0x7e080010);

		so_method(vtxbuf, tesla, NV50TCL_VERTEX_ARRAY_FORMAT(i), 1);
		so_data  (vtxbuf, 0);
	}
	nv50->state.vtxelt_nr = nv50->vtxelt->num_elements;

	so_ref (vtxbuf, &nv50->state.vtxbuf);
	so_ref (vtxattr, &nv50->state.vtxattr);
	so_ref (NULL, &vtxbuf);
	so_ref (NULL, &vtxattr);
	return vtxfmt;
d628 81
d710 2
@


1.1
log
@Update to Mesa 7.8.1. Tested on a bulk ports build by naddy@@, ok oga@@.
@
text
@d27 1
d30 1
d32 7
a38 2
static boolean
nv50_push_elements_u08(struct nv50_context *, uint8_t *, unsigned);
d40 2
a41 13
static boolean
nv50_push_elements_u16(struct nv50_context *, uint16_t *, unsigned);

static boolean
nv50_push_elements_u32(struct nv50_context *, uint32_t *, unsigned);

static boolean
nv50_push_arrays(struct nv50_context *, unsigned, unsigned);

#define NV50_USING_LOATHED_EDGEFLAG(ctx) ((ctx)->vertprog->cfg.edgeflag_in < 16)

static INLINE unsigned
nv50_prim(unsigned mode)
d43 1
a43 23
	switch (mode) {
	case PIPE_PRIM_POINTS: return NV50TCL_VERTEX_BEGIN_POINTS;
	case PIPE_PRIM_LINES: return NV50TCL_VERTEX_BEGIN_LINES;
	case PIPE_PRIM_LINE_LOOP: return NV50TCL_VERTEX_BEGIN_LINE_LOOP;
	case PIPE_PRIM_LINE_STRIP: return NV50TCL_VERTEX_BEGIN_LINE_STRIP;
	case PIPE_PRIM_TRIANGLES: return NV50TCL_VERTEX_BEGIN_TRIANGLES;
	case PIPE_PRIM_TRIANGLE_STRIP:
		return NV50TCL_VERTEX_BEGIN_TRIANGLE_STRIP;
	case PIPE_PRIM_TRIANGLE_FAN: return NV50TCL_VERTEX_BEGIN_TRIANGLE_FAN;
	case PIPE_PRIM_QUADS: return NV50TCL_VERTEX_BEGIN_QUADS;
	case PIPE_PRIM_QUAD_STRIP: return NV50TCL_VERTEX_BEGIN_QUAD_STRIP;
	case PIPE_PRIM_POLYGON: return NV50TCL_VERTEX_BEGIN_POLYGON;
	case PIPE_PRIM_LINES_ADJACENCY:
		return NV50TCL_VERTEX_BEGIN_LINES_ADJACENCY;
	case PIPE_PRIM_LINE_STRIP_ADJACENCY:
		return NV50TCL_VERTEX_BEGIN_LINE_STRIP_ADJACENCY;
	case PIPE_PRIM_TRIANGLES_ADJACENCY:
		return NV50TCL_VERTEX_BEGIN_TRIANGLES_ADJACENCY;
	case PIPE_PRIM_TRIANGLE_STRIP_ADJACENCY:
		return NV50TCL_VERTEX_BEGIN_TRIANGLE_STRIP_ADJACENCY;
	default:
		break;
	}
d45 3
a47 3
	NOUVEAU_ERR("invalid primitive type %d\n", mode);
	return NV50TCL_VERTEX_BEGIN_POINTS;
}
d49 9
a57 19
static INLINE uint32_t
nv50_vbo_type_to_hw(enum pipe_format format)
{
	const struct util_format_description *desc;

	desc = util_format_description(format);
	assert(desc);

	switch (desc->channel[0].type) {
	case UTIL_FORMAT_TYPE_FLOAT:
		return NV50TCL_VERTEX_ARRAY_ATTRIB_TYPE_FLOAT;
	case UTIL_FORMAT_TYPE_UNSIGNED:
		if (desc->channel[0].normalized) {
			return NV50TCL_VERTEX_ARRAY_ATTRIB_TYPE_UNORM;
		}
		return NV50TCL_VERTEX_ARRAY_ATTRIB_TYPE_USCALED;
	case UTIL_FORMAT_TYPE_SIGNED:
		if (desc->channel[0].normalized) {
			return NV50TCL_VERTEX_ARRAY_ATTRIB_TYPE_SNORM;
a58 8
		return NV50TCL_VERTEX_ARRAY_ATTRIB_TYPE_SSCALED;
	/*
	case PIPE_FORMAT_TYPE_UINT:
		return NV50TCL_VERTEX_ARRAY_ATTRIB_TYPE_UINT;
	case PIPE_FORMAT_TYPE_SINT:
		return NV50TCL_VERTEX_ARRAY_ATTRIB_TYPE_SINT; */
	default:
		return 0;
a61 96
static INLINE uint32_t
nv50_vbo_size_to_hw(unsigned size, unsigned nr_c)
{
	static const uint32_t hw_values[] = {
		0, 0, 0, 0,
		NV50TCL_VERTEX_ARRAY_ATTRIB_FORMAT_8,
		NV50TCL_VERTEX_ARRAY_ATTRIB_FORMAT_8_8,
		NV50TCL_VERTEX_ARRAY_ATTRIB_FORMAT_8_8_8,
		NV50TCL_VERTEX_ARRAY_ATTRIB_FORMAT_8_8_8_8,
		NV50TCL_VERTEX_ARRAY_ATTRIB_FORMAT_16,
		NV50TCL_VERTEX_ARRAY_ATTRIB_FORMAT_16_16,
		NV50TCL_VERTEX_ARRAY_ATTRIB_FORMAT_16_16_16,
		NV50TCL_VERTEX_ARRAY_ATTRIB_FORMAT_16_16_16_16,
		0, 0, 0, 0,
		NV50TCL_VERTEX_ARRAY_ATTRIB_FORMAT_32,
		NV50TCL_VERTEX_ARRAY_ATTRIB_FORMAT_32_32,
		NV50TCL_VERTEX_ARRAY_ATTRIB_FORMAT_32_32_32,
		NV50TCL_VERTEX_ARRAY_ATTRIB_FORMAT_32_32_32_32 };

	/* we'd also have R11G11B10 and R10G10B10A2 */

	assert(nr_c > 0 && nr_c <= 4);

	if (size > 32)
		return 0;
	size >>= (3 - 2);

	return hw_values[size + (nr_c - 1)];
}

static INLINE uint32_t
nv50_vbo_vtxelt_to_hw(struct pipe_vertex_element *ve)
{
	uint32_t hw_type, hw_size;
	enum pipe_format pf = ve->src_format;
	const struct util_format_description *desc;
	unsigned size;

	desc = util_format_description(pf);
	assert(desc);

	size = util_format_get_component_bits(pf, UTIL_FORMAT_COLORSPACE_RGB, 0);

	hw_type = nv50_vbo_type_to_hw(pf);
	hw_size = nv50_vbo_size_to_hw(size, ve->nr_components);

	if (!hw_type || !hw_size) {
		NOUVEAU_ERR("unsupported vbo format: %s\n", util_format_name(pf));
		abort();
		return 0x24e80000;
	}

	if (desc->swizzle[0] == UTIL_FORMAT_SWIZZLE_Z) /* BGRA */
		hw_size |= (1 << 31); /* no real swizzle bits :-( */

	return (hw_type | hw_size);
}

/* For instanced drawing from user buffers, hitting the FIFO repeatedly
 * with the same vertex data is probably worse than uploading all data.
 */
static boolean
nv50_upload_vtxbuf(struct nv50_context *nv50, unsigned i)
{
	struct nv50_screen *nscreen = nv50->screen;
	struct pipe_screen *pscreen = &nscreen->base.base;
	struct pipe_buffer *buf = nscreen->strm_vbuf[i];
	struct pipe_vertex_buffer *vb = &nv50->vtxbuf[i];
	uint8_t *src;
	unsigned size = align(vb->buffer->size, 4096);

	if (buf && buf->size < size)
		pipe_buffer_reference(&nscreen->strm_vbuf[i], NULL);

	if (!nscreen->strm_vbuf[i]) {
		nscreen->strm_vbuf[i] = pipe_buffer_create(
			pscreen, 0, PIPE_BUFFER_USAGE_VERTEX, size);
		buf = nscreen->strm_vbuf[i];
	}

	src = pipe_buffer_map(pscreen, vb->buffer, PIPE_BUFFER_USAGE_CPU_READ);
	if (!src)
		return FALSE;
	src += vb->buffer_offset;

	size = (vb->max_index + 1) * vb->stride + 16; /* + 16 is for stride 0 */
	if (vb->buffer_offset + size > vb->buffer->size)
		size = vb->buffer->size - vb->buffer_offset;

	pipe_buffer_write(pscreen, buf, vb->buffer_offset, size, src);
	pipe_buffer_unmap(pscreen, vb->buffer);

	vb->buffer = buf; /* don't pipe_reference, this is a private copy */
	return TRUE;
}

d63 1
a63 18
nv50_upload_user_vbufs(struct nv50_context *nv50)
{
	unsigned i;

	if (nv50->vbo_fifo)
		nv50->dirty |= NV50_NEW_ARRAYS;
	if (!(nv50->dirty & NV50_NEW_ARRAYS))
		return;

	for (i = 0; i < nv50->vtxbuf_nr; ++i) {
		if (nv50->vtxbuf[i].buffer->usage & PIPE_BUFFER_USAGE_VERTEX)
			continue;
		nv50_upload_vtxbuf(nv50, i);
	}
}

static void
nv50_set_static_vtxattr(struct nv50_context *nv50, unsigned i, void *data)
d65 1
d67 1
a67 63
	struct nouveau_channel *chan = tesla->channel;
	float v[4];

	util_format_read_4f(nv50->vtxelt[i].src_format,
			    v, 0, data, 0, 0, 0, 1, 1);

	switch (nv50->vtxelt[i].nr_components) {
	case 4:
		BEGIN_RING(chan, tesla, NV50TCL_VTX_ATTR_4F_X(i), 4);
		OUT_RINGf (chan, v[0]);
		OUT_RINGf (chan, v[1]);
		OUT_RINGf (chan, v[2]);
		OUT_RINGf (chan, v[3]);
		break;
	case 3:
		BEGIN_RING(chan, tesla, NV50TCL_VTX_ATTR_3F_X(i), 3);
		OUT_RINGf (chan, v[0]);
		OUT_RINGf (chan, v[1]);
		OUT_RINGf (chan, v[2]);
		break;
	case 2:
		BEGIN_RING(chan, tesla, NV50TCL_VTX_ATTR_2F_X(i), 2);
		OUT_RINGf (chan, v[0]);
		OUT_RINGf (chan, v[1]);
		break;
	case 1:
		BEGIN_RING(chan, tesla, NV50TCL_VTX_ATTR_1F(i), 1);
		OUT_RINGf (chan, v[0]);
		break;
	default:
		assert(0);
		break;
	}
}

static unsigned
init_per_instance_arrays_immd(struct nv50_context *nv50,
			      unsigned startInstance,
			      unsigned pos[16], unsigned step[16])
{
	struct nouveau_bo *bo;
	unsigned i, b, count = 0;

	for (i = 0; i < nv50->vtxelt_nr; ++i) {
		if (!nv50->vtxelt[i].instance_divisor)
			continue;
		++count;
		b = nv50->vtxelt[i].vertex_buffer_index;

		pos[i] = nv50->vtxelt[i].src_offset +
			nv50->vtxbuf[b].buffer_offset +
			startInstance * nv50->vtxbuf[b].stride;
		step[i] = startInstance % nv50->vtxelt[i].instance_divisor;

		bo = nouveau_bo(nv50->vtxbuf[b].buffer);
		if (!bo->map)
			nouveau_bo_map(bo, NOUVEAU_BO_RD);

		nv50_set_static_vtxattr(nv50, i, (uint8_t *)bo->map + pos[i]);
	}

	return count;
}
d69 2
a70 20
static unsigned
init_per_instance_arrays(struct nv50_context *nv50,
			 unsigned startInstance,
			 unsigned pos[16], unsigned step[16])
{
	struct nouveau_grobj *tesla = nv50->screen->tesla;
	struct nouveau_channel *chan = tesla->channel;
	struct nouveau_bo *bo;
	struct nouveau_stateobj *so;
	unsigned i, b, count = 0;
	const uint32_t rl = NOUVEAU_BO_VRAM | NOUVEAU_BO_GART | NOUVEAU_BO_RD;

	if (nv50->vbo_fifo)
		return init_per_instance_arrays_immd(nv50, startInstance,
						     pos, step);

	so = so_new(nv50->vtxelt_nr, nv50->vtxelt_nr * 2, nv50->vtxelt_nr * 2);

	for (i = 0; i < nv50->vtxelt_nr; ++i) {
		if (!nv50->vtxelt[i].instance_divisor)
a71 6
		++count;
		b = nv50->vtxelt[i].vertex_buffer_index;

		pos[i] = nv50->vtxelt[i].src_offset +
			nv50->vtxbuf[b].buffer_offset +
			startInstance * nv50->vtxbuf[b].stride;
d73 9
a81 3
		if (!startInstance) {
			step[i] = 0;
			continue;
a82 37
		step[i] = startInstance % nv50->vtxelt[i].instance_divisor;

		bo = nouveau_bo(nv50->vtxbuf[b].buffer);

		so_method(so, tesla, NV50TCL_VERTEX_ARRAY_START_HIGH(i), 2);
		so_reloc (so, bo, pos[i], rl | NOUVEAU_BO_HIGH, 0, 0);
		so_reloc (so, bo, pos[i], rl | NOUVEAU_BO_LOW, 0, 0);
	}

	if (count && startInstance) {
		so_ref (so, &nv50->state.instbuf); /* for flush notify */
		so_emit(chan, nv50->state.instbuf);
	}
	so_ref (NULL, &so);

	return count;
}

static void
step_per_instance_arrays_immd(struct nv50_context *nv50,
			      unsigned pos[16], unsigned step[16])
{
	struct nouveau_bo *bo;
	unsigned i, b;

	for (i = 0; i < nv50->vtxelt_nr; ++i) {
		if (!nv50->vtxelt[i].instance_divisor)
			continue;
		if (++step[i] != nv50->vtxelt[i].instance_divisor)
			continue;
		b = nv50->vtxelt[i].vertex_buffer_index;
		bo = nouveau_bo(nv50->vtxbuf[b].buffer);

		step[i] = 0;
		pos[i] += nv50->vtxbuf[b].stride;

		nv50_set_static_vtxattr(nv50, i, (uint8_t *)bo->map + pos[i]);
a86 51
step_per_instance_arrays(struct nv50_context *nv50,
			 unsigned pos[16], unsigned step[16])
{
	struct nouveau_grobj *tesla = nv50->screen->tesla;
	struct nouveau_channel *chan = tesla->channel;
	struct nouveau_bo *bo;
	struct nouveau_stateobj *so;
	unsigned i, b;
	const uint32_t rl = NOUVEAU_BO_VRAM | NOUVEAU_BO_GART | NOUVEAU_BO_RD;

	if (nv50->vbo_fifo) {
		step_per_instance_arrays_immd(nv50, pos, step);
		return;
	}

	so = so_new(nv50->vtxelt_nr, nv50->vtxelt_nr * 2, nv50->vtxelt_nr * 2);

	for (i = 0; i < nv50->vtxelt_nr; ++i) {
		if (!nv50->vtxelt[i].instance_divisor)
			continue;
		b = nv50->vtxelt[i].vertex_buffer_index;

		if (++step[i] == nv50->vtxelt[i].instance_divisor) {
			step[i] = 0;
			pos[i] += nv50->vtxbuf[b].stride;
		}

		bo = nouveau_bo(nv50->vtxbuf[b].buffer);

		so_method(so, tesla, NV50TCL_VERTEX_ARRAY_START_HIGH(i), 2);
		so_reloc (so, bo, pos[i], rl | NOUVEAU_BO_HIGH, 0, 0);
		so_reloc (so, bo, pos[i], rl | NOUVEAU_BO_LOW, 0, 0);
	}

	so_ref (so, &nv50->state.instbuf); /* for flush notify */
	so_ref (NULL, &so);

	so_emit(chan, nv50->state.instbuf);
}

static INLINE void
nv50_unmap_vbufs(struct nv50_context *nv50)
{
        unsigned i;

        for (i = 0; i < nv50->vtxbuf_nr; ++i)
                if (nouveau_bo(nv50->vtxbuf[i].buffer)->map)
                        nouveau_bo_unmap(nouveau_bo(nv50->vtxbuf[i].buffer));
}

void
d94 2
a95 2
	unsigned i, nz_divisors;
	unsigned step[16], pos[16];
d97 3
a99 2
	if (!NV50_USING_LOATHED_EDGEFLAG(nv50))
		nv50_upload_user_vbufs(nv50);
d101 6
a106 3
	nv50_state_validate(nv50);

	nz_divisors = init_per_instance_arrays(nv50, startInstance, pos, step);
d111 9
d121 2
a122 6
	BEGIN_RING(chan, tesla, NV50TCL_VERTEX_BEGIN, 1);
	OUT_RING  (chan, nv50_prim(mode));

	if (nv50->vbo_fifo)
		nv50_push_arrays(nv50, start, count);
	else {
a125 18
	}
	BEGIN_RING(chan, tesla, NV50TCL_VERTEX_END, 1);
	OUT_RING  (chan, 0);

	for (i = 1; i < instanceCount; i++) {
		if (nz_divisors) /* any non-zero array divisors ? */
			step_per_instance_arrays(nv50, pos, step);

		BEGIN_RING(chan, tesla, NV50TCL_VERTEX_BEGIN, 1);
		OUT_RING  (chan, nv50_prim(mode) | (1 << 28));

		if (nv50->vbo_fifo)
			nv50_push_arrays(nv50, start, count);
		else {
			BEGIN_RING(chan, tesla, NV50TCL_VERTEX_BUFFER_FIRST, 2);
			OUT_RING  (chan, start);
			OUT_RING  (chan, count);
		}
d128 2
d131 1
a131 1
	nv50_unmap_vbufs(nv50);
d133 4
a136 2
	so_ref(NULL, &nv50->state.instbuf);
}
d138 2
a139 3
void
nv50_draw_arrays(struct pipe_context *pipe, unsigned mode, unsigned start,
		 unsigned count)
d141 4
a144 4
	struct nv50_context *nv50 = nv50_context(pipe);
	struct nouveau_channel *chan = nv50->screen->tesla->channel;
	struct nouveau_grobj *tesla = nv50->screen->tesla;
	boolean ret;
d146 6
a151 1
	nv50_state_validate(nv50);
d153 3
a155 4
	BEGIN_RING(chan, tesla, 0x142c, 1);
	OUT_RING  (chan, 0);
	BEGIN_RING(chan, tesla, 0x142c, 1);
	OUT_RING  (chan, 0);
d157 4
a160 10
	BEGIN_RING(chan, tesla, NV50TCL_VERTEX_BEGIN, 1);
	OUT_RING  (chan, nv50_prim(mode));

	if (nv50->vbo_fifo)
		ret = nv50_push_arrays(nv50, start, count);
	else {
		BEGIN_RING(chan, tesla, NV50TCL_VERTEX_BUFFER_FIRST, 2);
		OUT_RING  (chan, start);
		OUT_RING  (chan, count);
		ret = TRUE;
a161 8
	BEGIN_RING(chan, tesla, NV50TCL_VERTEX_END, 1);
	OUT_RING  (chan, 0);

	nv50_unmap_vbufs(nv50);

        /* XXX: not sure what to do if ret != TRUE: flush and retry?
         */
        assert(ret);
d164 2
a165 3
static INLINE boolean
nv50_draw_elements_inline_u08(struct nv50_context *nv50, uint8_t *map,
			      unsigned start, unsigned count)
d167 4
a170 7
	struct nouveau_channel *chan = nv50->screen->tesla->channel;
	struct nouveau_grobj *tesla = nv50->screen->tesla;

	map += start;

	if (nv50->vbo_fifo)
		return nv50_push_elements_u08(nv50, map, count);
d175 1
a176 1
		count--;
d179 3
a181 7
	while (count) {
		unsigned nr = count > 2046 ? 2046 : count;
		int i;

		BEGIN_RING_NI(chan, tesla, NV50TCL_VB_ELEMENT_U16, nr >> 1);
		for (i = 0; i < nr; i += 2)
			OUT_RING  (chan, (map[i + 1] << 16) | map[i]);
d183 4
a186 2
		count -= nr;
		map += nr;
a187 1
	return TRUE;
d190 2
a191 3
static INLINE boolean
nv50_draw_elements_inline_u16(struct nv50_context *nv50, uint16_t *map,
			      unsigned start, unsigned count)
d193 3
a195 2
	struct nouveau_channel *chan = nv50->screen->tesla->channel;
	struct nouveau_grobj *tesla = nv50->screen->tesla;
d197 3
a199 1
	map += start;
d201 6
a206 2
	if (nv50->vbo_fifo)
		return nv50_push_elements_u16(nv50, map, count);
d208 2
a209 19
	if (count & 1) {
		BEGIN_RING(chan, tesla, NV50TCL_VB_ELEMENT_U32, 1);
		OUT_RING  (chan, map[0]);
		map++;
		count--;
	}

	while (count) {
		unsigned nr = count > 2046 ? 2046 : count;
		int i;

		BEGIN_RING_NI(chan, tesla, NV50TCL_VB_ELEMENT_U16, nr >> 1);
		for (i = 0; i < nr; i += 2)
			OUT_RING  (chan, (map[i + 1] << 16) | map[i]);

		count -= nr;
		map += nr;
	}
	return TRUE;
d212 5
a216 3
static INLINE boolean
nv50_draw_elements_inline_u32(struct nv50_context *nv50, uint32_t *map,
			      unsigned start, unsigned count)
d218 1
d221 27
d249 3
a251 1
	map += start;
d253 16
a268 5
	if (nv50->vbo_fifo)
		return nv50_push_elements_u32(nv50, map, count);

	while (count) {
		unsigned nr = count > 2047 ? 2047 : count;
d270 13
a282 2
		BEGIN_RING_NI(chan, tesla, NV50TCL_VB_ELEMENT_U32, nr);
		OUT_RINGp (chan, map, nr);
d284 1
a284 2
		count -= nr;
		map += nr;
a285 2
	return TRUE;
}
d287 1
a287 16
static INLINE void
nv50_draw_elements_inline(struct nv50_context *nv50,
			  void *map, unsigned indexSize,
			  unsigned start, unsigned count)
{
	switch (indexSize) {
	case 1:
		nv50_draw_elements_inline_u08(nv50, map, start, count);
		break;
	case 2:
		nv50_draw_elements_inline_u16(nv50, map, start, count);
		break;
	case 4:
		nv50_draw_elements_inline_u32(nv50, map, start, count);
		break;
	}
d290 1
a290 1
void
d292 2
a293 2
			     struct pipe_buffer *indexBuffer,
			     unsigned indexSize,
d298 1
d300 2
a301 5
	struct nouveau_channel *chan = tesla->channel;
	struct pipe_screen *pscreen = pipe->screen;
	void *map;
	unsigned i, nz_divisors;
	unsigned step[16], pos[16];
d303 3
a305 1
	map = pipe_buffer_map(pscreen, indexBuffer, PIPE_BUFFER_USAGE_CPU_READ);
d307 6
a312 2
	if (!NV50_USING_LOATHED_EDGEFLAG(nv50))
		nv50_upload_user_vbufs(nv50);
d314 10
a323 3
	nv50_state_validate(nv50);

	nz_divisors = init_per_instance_arrays(nv50, startInstance, pos, step);
d328 9
a336 12

	BEGIN_RING(chan, tesla, NV50TCL_VERTEX_BEGIN, 1);
	OUT_RING  (chan, nv50_prim(mode));

	nv50_draw_elements_inline(nv50, map, indexSize, start, count);

	BEGIN_RING(chan, tesla, NV50TCL_VERTEX_END, 1);
	OUT_RING  (chan, 0);

	for (i = 1; i < instanceCount; ++i) {
		if (nz_divisors) /* any non-zero array divisors ? */
			step_per_instance_arrays(nv50, pos, step);
d339 23
a361 4
		OUT_RING  (chan, nv50_prim(mode) | (1 << 28));

		nv50_draw_elements_inline(nv50, map, indexSize, start, count);

d364 2
a366 3
	nv50_unmap_vbufs(nv50);

	so_ref(NULL, &nv50->state.instbuf);
d370 1
a370 3
nv50_draw_elements(struct pipe_context *pipe,
		   struct pipe_buffer *indexBuffer, unsigned indexSize,
		   unsigned mode, unsigned start, unsigned count)
a372 6
	struct nouveau_channel *chan = nv50->screen->tesla->channel;
	struct nouveau_grobj *tesla = nv50->screen->tesla;
	struct pipe_screen *pscreen = pipe->screen;
	void *map;
	
	nv50_state_validate(nv50);
d374 2
a375 4
	BEGIN_RING(chan, tesla, 0x142c, 1);
	OUT_RING  (chan, 0);
	BEGIN_RING(chan, tesla, 0x142c, 1);
	OUT_RING  (chan, 0);
d377 2
a378 2
	BEGIN_RING(chan, tesla, NV50TCL_VERTEX_BEGIN, 1);
	OUT_RING  (chan, nv50_prim(mode));
d380 17
a396 25
	if (!nv50->vbo_fifo && indexSize == 4) {
		BEGIN_RING(chan, tesla, NV50TCL_VB_ELEMENT_U32 | 0x30000, 0);
		OUT_RING  (chan, count);
		nouveau_pushbuf_submit(chan, nouveau_bo(indexBuffer),
				       start << 2, count << 2);
	} else
	if (!nv50->vbo_fifo && indexSize == 2) {
		unsigned vb_start = (start & ~1);
		unsigned vb_end = (start + count + 1) & ~1;
		unsigned dwords = (vb_end - vb_start) >> 1;

		BEGIN_RING(chan, tesla, NV50TCL_VB_ELEMENT_U16_SETUP, 1);
		OUT_RING  (chan, ((start & 1) << 31) | count);
		BEGIN_RING(chan, tesla, NV50TCL_VB_ELEMENT_U16 | 0x30000, 0);
		OUT_RING  (chan, dwords);
		nouveau_pushbuf_submit(chan, nouveau_bo(indexBuffer),
				       vb_start << 1, dwords << 2);
		BEGIN_RING(chan, tesla, NV50TCL_VB_ELEMENT_U16_SETUP, 1);
		OUT_RING  (chan, 0);
	} else {
		map = pipe_buffer_map(pscreen, indexBuffer,
				      PIPE_BUFFER_USAGE_CPU_READ);
		nv50_draw_elements_inline(nv50, map, indexSize, start, count);
		nv50_unmap_vbufs(nv50);
		pipe_buffer_unmap(pscreen, indexBuffer);
a397 3

	BEGIN_RING(chan, tesla, NV50TCL_VERTEX_END, 1);
	OUT_RING  (chan, 0);
d409 1
a409 1
	struct nouveau_bo *bo = nouveau_bo(vb->buffer);
d412 1
d423 2
a424 1
		*pso = so = so_new(nv50->vtxelt_nr, nv50->vtxelt_nr * 4, 0);
d426 1
a426 1
	switch (ve->nr_components) {
d446 1
a446 1
		if (attrib == nv50->vertprog->cfg.edgeflag_in) {
d463 9
d480 2
a481 1
		return;
d483 3
d487 1
a487 1
	for (i = 0; i < nv50->vtxbuf_nr; ++i)
d489 1
a489 1
		    !(nv50->vtxbuf[i].buffer->usage & PIPE_BUFFER_USAGE_VERTEX))
d491 1
d493 1
a493 4
	if (NV50_USING_LOATHED_EDGEFLAG(nv50))
		nv50->vbo_fifo = 0xffff; /* vertprog can't set edgeflag */

	n_ve = MAX2(nv50->vtxelt_nr, nv50->state.vtxelt_nr);
d496 1
a496 1
	vtxbuf = so_new(n_ve * 2, n_ve * 5, nv50->vtxelt_nr * 4);
d500 2
a501 2
	for (i = 0; i < nv50->vtxelt_nr; i++) {
		struct pipe_vertex_element *ve = &nv50->vtxelt[i];
d504 2
a505 2
		struct nouveau_bo *bo = nouveau_bo(vb->buffer);
		uint32_t hw = nv50_vbo_vtxelt_to_hw(ve);
d520 1
a520 2
			so_data  (vtxfmt, hw |
				  (ve->instance_divisor ? (1 << 4) : i));
d526 1
d541 1
a541 1
		so_reloc (vtxbuf, bo, vb->buffer->size - 1,
d544 1
a544 1
		so_reloc (vtxbuf, bo, vb->buffer->size - 1,
d554 1
a554 1
	nv50->state.vtxelt_nr = nv50->vtxelt_nr;
a555 1
	so_ref (vtxfmt, &nv50->state.vtxfmt);
a558 1
	so_ref (NULL, &vtxfmt);
d560 1
a562 144
typedef void (*pfn_push)(struct nouveau_channel *, void *);

struct nv50_vbo_emitctx
{
	pfn_push push[16];
	uint8_t *map[16];
	unsigned stride[16];
	unsigned nr_ve;
	unsigned vtx_dwords;
	unsigned vtx_max;

	float edgeflag;
	unsigned ve_edgeflag;
};

static INLINE void
emit_vtx_next(struct nouveau_channel *chan, struct nv50_vbo_emitctx *emit)
{
	unsigned i;

	for (i = 0; i < emit->nr_ve; ++i) {
		emit->push[i](chan, emit->map[i]);
		emit->map[i] += emit->stride[i];
	}
}

static INLINE void
emit_vtx(struct nouveau_channel *chan, struct nv50_vbo_emitctx *emit,
	 uint32_t vi)
{
	unsigned i;

	for (i = 0; i < emit->nr_ve; ++i)
		emit->push[i](chan, emit->map[i] + emit->stride[i] * vi);
}

static INLINE boolean
nv50_map_vbufs(struct nv50_context *nv50)
{
	int i;

	for (i = 0; i < nv50->vtxbuf_nr; ++i) {
		struct pipe_vertex_buffer *vb = &nv50->vtxbuf[i];
		unsigned size = vb->stride * (vb->max_index + 1) + 16;

		if (nouveau_bo(vb->buffer)->map)
			continue;

		size = vb->stride * (vb->max_index + 1) + 16;
		size = MIN2(size, vb->buffer->size);
		if (!size)
			size = vb->buffer->size;

		if (nouveau_bo_map_range(nouveau_bo(vb->buffer),
					 0, size, NOUVEAU_BO_RD))
			break;
	}

	if (i == nv50->vtxbuf_nr)
		return TRUE;
	for (; i >= 0; --i)
		nouveau_bo_unmap(nouveau_bo(nv50->vtxbuf[i].buffer));
	return FALSE;
}

static void
emit_b32_1(struct nouveau_channel *chan, void *data)
{
	uint32_t *v = data;

	OUT_RING(chan, v[0]);
}

static void
emit_b32_2(struct nouveau_channel *chan, void *data)
{
	uint32_t *v = data;

	OUT_RING(chan, v[0]);
	OUT_RING(chan, v[1]);
}

static void
emit_b32_3(struct nouveau_channel *chan, void *data)
{
	uint32_t *v = data;

	OUT_RING(chan, v[0]);
	OUT_RING(chan, v[1]);
	OUT_RING(chan, v[2]);
}

static void
emit_b32_4(struct nouveau_channel *chan, void *data)
{
	uint32_t *v = data;

	OUT_RING(chan, v[0]);
	OUT_RING(chan, v[1]);
	OUT_RING(chan, v[2]);
	OUT_RING(chan, v[3]);
}

static void
emit_b16_1(struct nouveau_channel *chan, void *data)
{
	uint16_t *v = data;

	OUT_RING(chan, v[0]);
}

static void
emit_b16_3(struct nouveau_channel *chan, void *data)
{
	uint16_t *v = data;

	OUT_RING(chan, (v[1] << 16) | v[0]);
	OUT_RING(chan, v[2]);
}

static void
emit_b08_1(struct nouveau_channel *chan, void *data)
{
	uint8_t *v = data;

	OUT_RING(chan, v[0]);
}

static void
emit_b08_3(struct nouveau_channel *chan, void *data)
{
	uint8_t *v = data;

	OUT_RING(chan, (v[2] << 16) | (v[1] << 8) | v[0]);
}

static boolean
emit_prepare(struct nv50_context *nv50, struct nv50_vbo_emitctx *emit,
	     unsigned start)
{
	unsigned i;

	if (nv50_map_vbufs(nv50) == FALSE)
		return FALSE;
a563 197
	emit->ve_edgeflag = nv50->vertprog->cfg.edgeflag_in;

	emit->edgeflag = 0.5f;
	emit->nr_ve = 0;
	emit->vtx_dwords = 0;

	for (i = 0; i < nv50->vtxelt_nr; ++i) {
		struct pipe_vertex_element *ve;
		struct pipe_vertex_buffer *vb;
		unsigned n, size;
		const struct util_format_description *desc;

		ve = &nv50->vtxelt[i];
		vb = &nv50->vtxbuf[ve->vertex_buffer_index];
		if (!(nv50->vbo_fifo & (1 << i)) || ve->instance_divisor)
			continue;
		n = emit->nr_ve++;

		emit->stride[n] = vb->stride;
		emit->map[n] = (uint8_t *)nouveau_bo(vb->buffer)->map +
			vb->buffer_offset +
			(start * vb->stride + ve->src_offset);

		desc = util_format_description(ve->src_format);
		assert(desc);

		size = util_format_get_component_bits(
			ve->src_format, UTIL_FORMAT_COLORSPACE_RGB, 0);

		assert(ve->nr_components > 0 && ve->nr_components <= 4);

		/* It shouldn't be necessary to push the implicit 1s
		 * for case 3 and size 8 cases 1, 2, 3.
		 */
		switch (size) {
		default:
			NOUVEAU_ERR("unsupported vtxelt size: %u\n", size);
			return FALSE;
		case 32:
			switch (ve->nr_components) {
			case 1: emit->push[n] = emit_b32_1; break;
			case 2: emit->push[n] = emit_b32_2; break;
			case 3: emit->push[n] = emit_b32_3; break;
			case 4: emit->push[n] = emit_b32_4; break;
			}
			emit->vtx_dwords += ve->nr_components;
			break;
		case 16:
			switch (ve->nr_components) {
			case 1: emit->push[n] = emit_b16_1; break;
			case 2: emit->push[n] = emit_b32_1; break;
			case 3: emit->push[n] = emit_b16_3; break;
			case 4: emit->push[n] = emit_b32_2; break;
			}
			emit->vtx_dwords += (ve->nr_components + 1) >> 1;
			break;
		case 8:
			switch (ve->nr_components) {
			case 1: emit->push[n] = emit_b08_1; break;
			case 2: emit->push[n] = emit_b16_1; break;
			case 3: emit->push[n] = emit_b08_3; break;
			case 4: emit->push[n] = emit_b32_1; break;
			}
			emit->vtx_dwords += 1;
			break;
		}
	}

	emit->vtx_max = 512 / emit->vtx_dwords;
	if (emit->ve_edgeflag < 16)
		emit->vtx_max = 1;

	return TRUE;
}

static INLINE void
set_edgeflag(struct nouveau_channel *chan,
	     struct nouveau_grobj *tesla,
	     struct nv50_vbo_emitctx *emit, uint32_t index)
{
	unsigned i = emit->ve_edgeflag;

	if (i < 16) {
		float f = *((float *)(emit->map[i] + index * emit->stride[i]));

		if (emit->edgeflag != f) {
			emit->edgeflag = f;

			BEGIN_RING(chan, tesla, 0x15e4, 1);
			OUT_RING  (chan, f ? 1 : 0);
		}
	}
}

static boolean
nv50_push_arrays(struct nv50_context *nv50, unsigned start, unsigned count)
{
	struct nouveau_channel *chan = nv50->screen->base.channel;
	struct nouveau_grobj *tesla = nv50->screen->tesla;
	struct nv50_vbo_emitctx emit;

	if (emit_prepare(nv50, &emit, start) == FALSE)
		return FALSE;

	while (count) {
		unsigned i, dw, nr = MIN2(count, emit.vtx_max);
	        dw = nr * emit.vtx_dwords;

		set_edgeflag(chan, tesla, &emit, 0); /* nr will be 1 */

		BEGIN_RING_NI(chan, tesla, NV50TCL_VERTEX_DATA, dw);
		for (i = 0; i < nr; ++i)
			emit_vtx_next(chan, &emit);

		count -= nr;
	}

	return TRUE;
}

static boolean
nv50_push_elements_u32(struct nv50_context *nv50, uint32_t *map, unsigned count)
{
	struct nouveau_channel *chan = nv50->screen->base.channel;
	struct nouveau_grobj *tesla = nv50->screen->tesla;
	struct nv50_vbo_emitctx emit;

	if (emit_prepare(nv50, &emit, 0) == FALSE)
		return FALSE;

	while (count) {
		unsigned i, dw, nr = MIN2(count, emit.vtx_max);
	        dw = nr * emit.vtx_dwords;

		set_edgeflag(chan, tesla, &emit, *map);

		BEGIN_RING_NI(chan, tesla, NV50TCL_VERTEX_DATA, dw);
		for (i = 0; i < nr; ++i)
			emit_vtx(chan, &emit, *map++);

		count -= nr;
	}

	return TRUE;
}

static boolean
nv50_push_elements_u16(struct nv50_context *nv50, uint16_t *map, unsigned count)
{
	struct nouveau_channel *chan = nv50->screen->base.channel;
	struct nouveau_grobj *tesla = nv50->screen->tesla;
	struct nv50_vbo_emitctx emit;

	if (emit_prepare(nv50, &emit, 0) == FALSE)
		return FALSE;

	while (count) {
		unsigned i, dw, nr = MIN2(count, emit.vtx_max);
	        dw = nr * emit.vtx_dwords;

		set_edgeflag(chan, tesla, &emit, *map);

		BEGIN_RING_NI(chan, tesla, NV50TCL_VERTEX_DATA, dw);
		for (i = 0; i < nr; ++i)
			emit_vtx(chan, &emit, *map++);

		count -= nr;
	}

	return TRUE;
}

static boolean
nv50_push_elements_u08(struct nv50_context *nv50, uint8_t *map, unsigned count)
{
	struct nouveau_channel *chan = nv50->screen->base.channel;
	struct nouveau_grobj *tesla = nv50->screen->tesla;
	struct nv50_vbo_emitctx emit;

	if (emit_prepare(nv50, &emit, 0) == FALSE)
		return FALSE;

	while (count) {
		unsigned i, dw, nr = MIN2(count, emit.vtx_max);
	        dw = nr * emit.vtx_dwords;

		set_edgeflag(chan, tesla, &emit, *map);

		BEGIN_RING_NI(chan, tesla, NV50TCL_VERTEX_DATA, dw);
		for (i = 0; i < nr; ++i)
			emit_vtx(chan, &emit, *map++);

		count -= nr;
	}

	return TRUE;
}
@


1.1.1.1
log
@Import Mesa 7.10.3
@
text
@a26 1
#include "util/u_split_prim.h"
a28 1
#include "nv50_resource.h"
d30 264
a293 1
struct instance {
d295 41
a335 5
	unsigned delta;
	unsigned stride;
	unsigned step;
	unsigned divisor;
};
d338 2
a339 1
instance_init(struct nv50_context *nv50, struct instance *a, unsigned first)
d341 10
a350 1
	int i;
d352 2
a353 3
	for (i = 0; i < nv50->vtxelt->num_elements; i++) {
		struct pipe_vertex_element *ve = &nv50->vtxelt->pipe[i];
		struct pipe_vertex_buffer *vb;
d355 1
a355 10
		a[i].divisor = ve->instance_divisor;
		if (a[i].divisor) {
			vb = &nv50->vtxbuf[ve->vertex_buffer_index];

			a[i].bo = nv50_resource(vb->buffer)->bo;
			a[i].stride = vb->stride;
			a[i].step = first % a[i].divisor;
			a[i].delta = vb->buffer_offset + ve->src_offset +
				     (first * a[i].stride);
		}
d360 2
a361 1
instance_step(struct nv50_context *nv50, struct instance *a)
a362 1
	struct nouveau_channel *chan = nv50->screen->tesla->channel;
d364 12
a375 1
	int i;
d377 2
a378 2
	for (i = 0; i < nv50->vtxelt->num_elements; i++) {
		if (!a[i].divisor)
d380 1
d382 3
a384 9
		BEGIN_RING(chan, tesla,
			   NV50TCL_VERTEX_ARRAY_START_HIGH(i), 2);
		OUT_RELOCh(chan, a[i].bo, a[i].delta, NOUVEAU_BO_RD |
			   NOUVEAU_BO_VRAM | NOUVEAU_BO_GART);
		OUT_RELOCl(chan, a[i].bo, a[i].delta, NOUVEAU_BO_RD |
			   NOUVEAU_BO_VRAM | NOUVEAU_BO_GART);
		if (++a[i].step == a[i].divisor) {
			a[i].step = 0;
			a[i].delta += a[i].stride;
d386 6
d393 15
d410 1
a410 1
static void
d418 5
a422 2
	struct instance a[16];
	unsigned prim = nv50_prim(mode);
d424 1
a424 3
	instance_init(nv50, a, startInstance);
	if (!nv50_state_validate(nv50, 10 + 16*3))
		return;
d426 1
a426 6
	if (nv50->vbo_fifo) {
		nv50_push_elements_instanced(pipe, NULL, 0, 0, mode, start,
					     count, startInstance,
					     instanceCount);
		return;
	}
a430 9
	while (instanceCount--) {
		if (AVAIL_RING(chan) < (7 + 16*3)) {
			FIRE_RING(chan);
			if (!nv50_state_validate(nv50, 7 + 16*3)) {
				assert(0);
				return;
			}
		}
		instance_step(nv50, a);
d432 6
a437 2
		BEGIN_RING(chan, tesla, NV50TCL_VERTEX_BEGIN, 1);
		OUT_RING  (chan, prim);
d441 18
d461 2
d464 29
a492 1
		prim |= (1 << 28);
d494 8
d504 8
a511 4
struct inline_ctx {
	struct nv50_context *nv50;
	void *map;
};
d513 2
a514 7
static void
inline_elt08(void *priv, unsigned start, unsigned count)
{
	struct inline_ctx *ctx = priv;
	struct nouveau_grobj *tesla = ctx->nv50->screen->tesla;
	struct nouveau_channel *chan = tesla->channel;
	uint8_t *map = (uint8_t *)ctx->map + start;
d520 1
a520 1
		count &= ~1;
d523 7
a529 3
	count >>= 1;
	if (!count)
		return;
d531 2
a532 4
	BEGIN_RING_NI(chan, tesla, NV50TCL_VB_ELEMENT_U16, count);
	while (count--) {
		OUT_RING(chan, (map[1] << 16) | map[0]);
		map += 2;
d534 1
d537 3
a539 2
static void
inline_elt16(void *priv, unsigned start, unsigned count)
d541 7
a547 4
	struct inline_ctx *ctx = priv;
	struct nouveau_grobj *tesla = ctx->nv50->screen->tesla;
	struct nouveau_channel *chan = tesla->channel;
	uint16_t *map = (uint16_t *)ctx->map + start;
a551 1
		count &= ~1;
d553 1
d556 7
a562 3
	count >>= 1;
	if (!count)
		return;
d564 2
a565 4
	BEGIN_RING_NI(chan, tesla, NV50TCL_VB_ELEMENT_U16, count);
	while (count--) {
		OUT_RING(chan, (map[1] << 16) | map[0]);
		map += 2;
d567 1
d570 3
a572 2
static void
inline_elt32(void *priv, unsigned start, unsigned count)
a573 26
	struct inline_ctx *ctx = priv;
	struct nouveau_grobj *tesla = ctx->nv50->screen->tesla;
	struct nouveau_channel *chan = tesla->channel;

	BEGIN_RING_NI(chan, tesla, NV50TCL_VB_ELEMENT_U32, count);
	OUT_RINGp    (chan, (uint32_t *)ctx->map + start, count);
}

static void
inline_edgeflag(void *priv, boolean enabled)
{
	struct inline_ctx *ctx = priv;
	struct nouveau_grobj *tesla = ctx->nv50->screen->tesla;
	struct nouveau_channel *chan = tesla->channel;

	BEGIN_RING(chan, tesla, NV50TCL_EDGEFLAG_ENABLE, 1);
	OUT_RING  (chan, enabled ? 1 : 0);
}

static void
nv50_draw_elements_inline(struct pipe_context *pipe,
			  struct pipe_resource *indexBuffer, unsigned indexSize,
			  unsigned mode, unsigned start, unsigned count,
			  unsigned startInstance, unsigned instanceCount)
{
	struct nv50_context *nv50 = nv50_context(pipe);
a575 27
	struct pipe_transfer *transfer;
	struct instance a[16];
	struct inline_ctx ctx;
	struct util_split_prim s;
	boolean nzi = FALSE;
	unsigned overhead;

	overhead = 16*3; /* potential instance adjustments */
	overhead += 4; /* Begin()/End() */
	overhead += 4; /* potential edgeflag disable/reenable */
	overhead += 3; /* potentially 3 VTX_ELT_U16/U32 packet headers */

	s.priv = &ctx;
	if (indexSize == 1)
		s.emit = inline_elt08;
	else
	if (indexSize == 2)
		s.emit = inline_elt16;
	else
		s.emit = inline_elt32;
	s.edge = inline_edgeflag;

	ctx.nv50 = nv50;
	ctx.map = pipe_buffer_map(pipe, indexBuffer, PIPE_TRANSFER_READ, &transfer);
	assert(ctx.map);
	if (!ctx.map)
		return;
d577 4
a580 3
	instance_init(nv50, a, startInstance);
	if (!nv50_state_validate(nv50, overhead + 6 + 3))
		return;
d582 2
a583 16
	BEGIN_RING(chan, tesla, NV50TCL_CB_ADDR, 2);
	OUT_RING  (chan, NV50_CB_AUX | (24 << 8));
	OUT_RING  (chan, startInstance);
	while (instanceCount--) {
		unsigned max_verts;
		boolean done;

		util_split_prim_init(&s, mode, start, count);
		do {
			if (AVAIL_RING(chan) < (overhead + 6)) {
				FIRE_RING(chan);
				if (!nv50_state_validate(nv50, (overhead + 6))) {
					assert(0);
					return;
				}
			}
d585 2
a586 13
			max_verts = AVAIL_RING(chan) - overhead;
			if (max_verts > 2047)
				max_verts = 2047;
			if (indexSize != 4)
				max_verts <<= 1;
			instance_step(nv50, a);

			BEGIN_RING(chan, tesla, NV50TCL_VERTEX_BEGIN, 1);
			OUT_RING  (chan, nv50_prim(s.mode) | (nzi ? (1<<28) : 0));
			done = util_split_prim_next(&s, max_verts);
			BEGIN_RING(chan, tesla, NV50TCL_VERTEX_END, 1);
			OUT_RING  (chan, 0);
		} while (!done);
d588 2
a589 1
		nzi = TRUE;
d591 2
d594 16
a609 1
	pipe_buffer_unmap(pipe, indexBuffer, transfer);
d612 1
a612 1
static void
d614 2
a615 2
			     struct pipe_resource *indexBuffer,
			     unsigned indexSize, int indexBias,
a619 1
	struct nouveau_channel *chan = nv50->screen->tesla->channel;
d621 7
a627 2
	struct instance a[16];
	unsigned prim = nv50_prim(mode);
d629 2
a630 3
	instance_init(nv50, a, startInstance);
	if (!nv50_state_validate(nv50, 13 + 16*3))
		return;
d632 1
a632 6
	if (nv50->vbo_fifo) {
		nv50_push_elements_instanced(pipe, indexBuffer, indexSize,
					     indexBias, mode, start, count,
					     startInstance, instanceCount);
		return;
	}
d634 1
a634 10
	/* indices are uint32 internally, so large indexBias means negative */
	BEGIN_RING(chan, tesla, NV50TCL_VB_ELEMENT_BASE, 1);
	OUT_RING  (chan, indexBias);

	if (!nv50_resource_mapped_by_gpu(indexBuffer) || indexSize == 1) {
		nv50_draw_elements_inline(pipe, indexBuffer, indexSize,
					  mode, start, count, startInstance,
					  instanceCount);
		return;
	}
d639 12
a650 9
	while (instanceCount--) {
		if (AVAIL_RING(chan) < (7 + 16*3)) {
			FIRE_RING(chan);
			if (!nv50_state_validate(nv50, 10 + 16*3)) {
				assert(0);
				return;
			}
		}
		instance_step(nv50, a);
d653 4
a656 23
		OUT_RING  (chan, prim);
		if (indexSize == 4) {
			BEGIN_RING(chan, tesla, NV50TCL_VB_ELEMENT_U32 | 0x30000, 0);
			OUT_RING  (chan, count);
			nouveau_pushbuf_submit(chan, 
					       nv50_resource(indexBuffer)->bo,
					       start << 2, count << 2);
		} else
		if (indexSize == 2) {
			unsigned vb_start = (start & ~1);
			unsigned vb_end = (start + count + 1) & ~1;
			unsigned dwords = (vb_end - vb_start) >> 1;

			BEGIN_RING(chan, tesla, NV50TCL_VB_ELEMENT_U16_SETUP, 1);
			OUT_RING  (chan, ((start & 1) << 31) | count);
			BEGIN_RING(chan, tesla, NV50TCL_VB_ELEMENT_U16 | 0x30000, 0);
			OUT_RING  (chan, dwords);
			nouveau_pushbuf_submit(chan,
					       nv50_resource(indexBuffer)->bo,
					       vb_start << 1, dwords << 2);
			BEGIN_RING(chan, tesla, NV50TCL_VB_ELEMENT_U16_SETUP, 1);
			OUT_RING  (chan, 0);
		}
d659 2
d662 1
a662 2
		prim |= (1 << 28);
	}
d666 3
a668 1
nv50_draw_vbo(struct pipe_context *pipe, const struct pipe_draw_info *info)
d671 6
d678 4
a681 2
	if (info->indexed && nv50->idxbuf.buffer) {
		unsigned offset;
d683 2
a684 2
		assert(nv50->idxbuf.offset % nv50->idxbuf.index_size == 0);
		offset = nv50->idxbuf.offset / nv50->idxbuf.index_size;
d686 25
a710 17
		nv50_draw_elements_instanced(pipe,
					     nv50->idxbuf.buffer,
					     nv50->idxbuf.index_size,
					     info->index_bias,
					     info->mode,
					     info->start + offset,
					     info->count,
					     info->start_instance,
					     info->instance_count);
	}
	else {
		nv50_draw_arrays_instanced(pipe,
					   info->mode,
					   info->start,
					   info->count,
					   info->start_instance,
					   info->instance_count);
d712 3
d726 1
a726 1
	struct nouveau_bo *bo = nv50_resource(vb->buffer)->bo;
a728 1
	unsigned nr_components = util_format_get_nr_components(ve->src_format);
d739 1
a739 2
		*pso = so = so_new(nv50->vtxelt->num_elements,
				   nv50->vtxelt->num_elements * 4, 0);
d741 1
a741 1
	switch (nr_components) {
d761 1
a761 1
		if (attrib == nv50->vertprog->vp.edgeflag) {
a777 9
nv50_vtxelt_construct(struct nv50_vtxelt_stateobj *cso)
{
	unsigned i;

	for (i = 0; i < cso->num_elements; ++i)
		cso->hw[i] = nv50_format_table[cso->pipe[i].src_format].vtx;
}

struct nouveau_stateobj *
d786 1
a786 2
		return NULL;

a787 3
	if (nv50->screen->force_push ||
	    nv50->vertprog->vp.edgeflag < 16)
		nv50->vbo_fifo = 0xffff;
d789 1
a789 1
	for (i = 0; i < nv50->vtxbuf_nr; i++) {
d791 1
a791 1
		    !nv50_resource_mapped_by_gpu(nv50->vtxbuf[i].buffer))
a792 1
	}
d794 4
a797 1
	n_ve = MAX2(nv50->vtxelt->num_elements, nv50->state.vtxelt_nr);
d800 1
a800 1
	vtxbuf = so_new(n_ve * 2, n_ve * 5, nv50->vtxelt->num_elements * 4);
d804 2
a805 2
	for (i = 0; i < nv50->vtxelt->num_elements; i++) {
		struct pipe_vertex_element *ve = &nv50->vtxelt->pipe[i];
d808 2
a809 2
		struct nouveau_bo *bo = nv50_resource(vb->buffer)->bo;
		uint32_t hw = nv50->vtxelt->hw[i];
d824 2
a825 1
			so_data  (vtxfmt, hw | (ve->instance_divisor ? (1 << 4) : i));
a830 1

d845 1
a845 1
		so_reloc (vtxbuf, bo, vb->buffer->width0 - 1,
d848 1
a848 1
		so_reloc (vtxbuf, bo, vb->buffer->width0 - 1,
d858 1
a858 1
	nv50->state.vtxelt_nr = nv50->vtxelt->num_elements;
d860 1
d864 1
a865 1
	return vtxfmt;
d868 144
d1013 197
@


1.1.1.2
log
@Import Mesa 9.2.0
@
text
@d2 1
a2 1
 * Copyright 2010 Christoph Bumiller
d17 4
a20 4
 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
 * OTHER DEALINGS IN THE SOFTWARE.
d27 1
a27 1
#include "translate/translate.h"
d32 7
a38 175
#include "nv50_3d.xml.h"

void
nv50_vertex_state_delete(struct pipe_context *pipe,
                         void *hwcso)
{
   struct nv50_vertex_stateobj *so = hwcso;

   if (so->translate)
      so->translate->release(so->translate);
   FREE(hwcso);
}

void *
nv50_vertex_state_create(struct pipe_context *pipe,
                         unsigned num_elements,
                         const struct pipe_vertex_element *elements)
{
    struct nv50_vertex_stateobj *so;
    struct translate_key transkey;
    unsigned i;

    so = MALLOC(sizeof(*so) +
                num_elements * sizeof(struct nv50_vertex_element));
    if (!so)
        return NULL;
    so->num_elements = num_elements;
    so->instance_elts = 0;
    so->instance_bufs = 0;
    so->need_conversion = FALSE;

    memset(so->vb_access_size, 0, sizeof(so->vb_access_size));

    for (i = 0; i < PIPE_MAX_ATTRIBS; ++i)
       so->min_instance_div[i] = 0xffffffff;

    transkey.nr_elements = 0;
    transkey.output_stride = 0;

    for (i = 0; i < num_elements; ++i) {
        const struct pipe_vertex_element *ve = &elements[i];
        const unsigned vbi = ve->vertex_buffer_index;
        unsigned size;
        enum pipe_format fmt = ve->src_format;

        so->element[i].pipe = elements[i];
        so->element[i].state = nv50_format_table[fmt].vtx;

        if (!so->element[i].state) {
            switch (util_format_get_nr_components(fmt)) {
            case 1: fmt = PIPE_FORMAT_R32_FLOAT; break;
            case 2: fmt = PIPE_FORMAT_R32G32_FLOAT; break;
            case 3: fmt = PIPE_FORMAT_R32G32B32_FLOAT; break;
            case 4: fmt = PIPE_FORMAT_R32G32B32A32_FLOAT; break;
            default:
                assert(0);
                FREE(so);
                return NULL;
            }
            so->element[i].state = nv50_format_table[fmt].vtx;
            so->need_conversion = TRUE;
        }
        so->element[i].state |= i;

        size = util_format_get_blocksize(fmt);
        if (so->vb_access_size[vbi] < (ve->src_offset + size))
           so->vb_access_size[vbi] = ve->src_offset + size;

        if (1) {
            unsigned j = transkey.nr_elements++;

            transkey.element[j].type = TRANSLATE_ELEMENT_NORMAL;
            transkey.element[j].input_format = ve->src_format;
            transkey.element[j].input_buffer = vbi;
            transkey.element[j].input_offset = ve->src_offset;
            transkey.element[j].instance_divisor = ve->instance_divisor;

            transkey.element[j].output_format = fmt;
            transkey.element[j].output_offset = transkey.output_stride;
            transkey.output_stride += (util_format_get_stride(fmt, 1) + 3) & ~3;

            if (unlikely(ve->instance_divisor)) {
               so->instance_elts |= 1 << i;
               so->instance_bufs |= 1 << vbi;
               if (ve->instance_divisor < so->min_instance_div[vbi])
                  so->min_instance_div[vbi] = ve->instance_divisor;
            }
        }
    }

    so->translate = translate_create(&transkey);
    so->vertex_size = transkey.output_stride / 4;
    so->packet_vertex_limit = NV04_PFIFO_MAX_PACKET_LEN /
       MAX2(so->vertex_size, 1);

    return so;
}

#define NV50_3D_VERTEX_ATTRIB_INACTIVE              \
   NV50_3D_VERTEX_ARRAY_ATTRIB_TYPE_FLOAT |         \
   NV50_3D_VERTEX_ARRAY_ATTRIB_FORMAT_32_32_32_32 | \
   NV50_3D_VERTEX_ARRAY_ATTRIB_CONST

static void
nv50_emit_vtxattr(struct nv50_context *nv50, struct pipe_vertex_buffer *vb,
                  struct pipe_vertex_element *ve, unsigned attr)
{
   struct nouveau_pushbuf *push = nv50->base.pushbuf;
   const void *data = (const uint8_t *)vb->user_buffer + ve->src_offset;
   float v[4];
   const unsigned nc = util_format_get_nr_components(ve->src_format);
   const struct util_format_description *desc =
      util_format_description(ve->src_format);

   assert(vb->user_buffer);

   if (desc->channel[0].pure_integer) {
      if (desc->channel[0].type == UTIL_FORMAT_TYPE_SIGNED) {
         desc->unpack_rgba_sint((int32_t *)v, 0, data, 0, 1, 1);
      } else {
         desc->unpack_rgba_uint((uint32_t *)v, 0, data, 0, 1, 1);
      }
   } else {
      desc->unpack_rgba_float(v, 0, data, 0, 1, 1);
   }

   switch (nc) {
   case 4:
      BEGIN_NV04(push, NV50_3D(VTX_ATTR_4F_X(attr)), 4);
      PUSH_DATAf(push, v[0]);
      PUSH_DATAf(push, v[1]);
      PUSH_DATAf(push, v[2]);
      PUSH_DATAf(push, v[3]);
      break;
   case 3:
      BEGIN_NV04(push, NV50_3D(VTX_ATTR_3F_X(attr)), 3);
      PUSH_DATAf(push, v[0]);
      PUSH_DATAf(push, v[1]);
      PUSH_DATAf(push, v[2]);
      break;
   case 2:
      BEGIN_NV04(push, NV50_3D(VTX_ATTR_2F_X(attr)), 2);
      PUSH_DATAf(push, v[0]);
      PUSH_DATAf(push, v[1]);
      break;
   case 1:
      if (attr == nv50->vertprog->vp.edgeflag) {
         BEGIN_NV04(push, NV50_3D(EDGEFLAG), 1);
         PUSH_DATA (push, v[0] ? 1 : 0);
      }
      BEGIN_NV04(push, NV50_3D(VTX_ATTR_1F(attr)), 1);
      PUSH_DATAf(push, v[0]);
      break;
   default:
      assert(0);
      break;
   }
}

static INLINE void
nv50_user_vbuf_range(struct nv50_context *nv50, int vbi,
                     uint32_t *base, uint32_t *size)
{
   if (unlikely(nv50->vertex->instance_bufs & (1 << vbi))) {
      /* TODO: use min and max instance divisor to get a proper range */
      *base = 0;
      *size = nv50->vtxbuf[vbi].buffer->width0;
   } else {
      /* NOTE: if there are user buffers, we *must* have index bounds */
      assert(nv50->vb_elt_limit != ~0);
      *base = nv50->vb_elt_first * nv50->vtxbuf[vbi].stride;
      *size = nv50->vb_elt_limit * nv50->vtxbuf[vbi].stride +
         nv50->vertex->vb_access_size[vbi];
   }
}
d41 1
a41 2
nv50_upload_user_buffers(struct nv50_context *nv50,
                         uint64_t addrs[], uint32_t limits[])
d43 1
a43 1
   unsigned b;
d45 91
a135 238
   for (b = 0; b < nv50->num_vtxbufs; ++b) {
      struct nouveau_bo *bo;
      const struct pipe_vertex_buffer *vb = &nv50->vtxbuf[b];
      uint32_t base, size;

      if (!(nv50->vbo_user & (1 << b)) || !vb->stride)
         continue;
      nv50_user_vbuf_range(nv50, b, &base, &size);

      limits[b] = base + size - 1;
      addrs[b] = nouveau_scratch_data(&nv50->base, vb->user_buffer, base, size,
                                      &bo);
      if (addrs[b])
         BCTX_REFN_bo(nv50->bufctx_3d, VERTEX_TMP, NOUVEAU_BO_GART |
                      NOUVEAU_BO_RD, bo);
   }
   nv50->base.vbo_dirty = TRUE;
}

static void
nv50_update_user_vbufs(struct nv50_context *nv50)
{
   uint64_t address[PIPE_MAX_ATTRIBS];
   struct nouveau_pushbuf *push = nv50->base.pushbuf;
   unsigned i;
   uint32_t written = 0;

   for (i = 0; i < nv50->vertex->num_elements; ++i) {
      struct pipe_vertex_element *ve = &nv50->vertex->element[i].pipe;
      const unsigned b = ve->vertex_buffer_index;
      struct pipe_vertex_buffer *vb = &nv50->vtxbuf[b];
      uint32_t base, size;

      if (!(nv50->vbo_user & (1 << b)))
         continue;

      if (!vb->stride) {
         nv50_emit_vtxattr(nv50, vb, ve, i);
         continue;
      }
      nv50_user_vbuf_range(nv50, b, &base, &size);

      if (!(written & (1 << b))) {
         struct nouveau_bo *bo;
         const uint32_t bo_flags = NOUVEAU_BO_GART | NOUVEAU_BO_RD;
         written |= 1 << b;
         address[b] = nouveau_scratch_data(&nv50->base, vb->user_buffer,
                                           base, size, &bo);
         if (address[b])
            BCTX_REFN_bo(nv50->bufctx_3d, VERTEX_TMP, bo_flags, bo);
      }

      BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_LIMIT_HIGH(i)), 2);
      PUSH_DATAh(push, address[b] + base + size - 1);
      PUSH_DATA (push, address[b] + base + size - 1);
      BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_START_HIGH(i)), 2);
      PUSH_DATAh(push, address[b] + ve->src_offset);
      PUSH_DATA (push, address[b] + ve->src_offset);
   }
   nv50->base.vbo_dirty = TRUE;
}

static INLINE void
nv50_release_user_vbufs(struct nv50_context *nv50)
{
   if (nv50->vbo_user) {
      nouveau_bufctx_reset(nv50->bufctx_3d, NV50_BIND_VERTEX_TMP);
      nouveau_scratch_done(&nv50->base);
   }
}

void
nv50_vertex_arrays_validate(struct nv50_context *nv50)
{
   uint64_t addrs[PIPE_MAX_ATTRIBS];
   uint32_t limits[PIPE_MAX_ATTRIBS];
   struct nouveau_pushbuf *push = nv50->base.pushbuf;
   struct nv50_vertex_stateobj *vertex = nv50->vertex;
   struct pipe_vertex_buffer *vb;
   struct nv50_vertex_element *ve;
   uint32_t mask;
   uint32_t refd = 0;
   unsigned i;
   const unsigned n = MAX2(vertex->num_elements, nv50->state.num_vtxelts);

   if (unlikely(vertex->need_conversion))
      nv50->vbo_fifo = ~0;
   else
   if (nv50->vbo_user & ~nv50->vbo_constant)
      nv50->vbo_fifo = nv50->vbo_push_hint ? ~0 : 0;
   else
      nv50->vbo_fifo = 0;

   if (!nv50->vbo_fifo) {
      /* if vertex buffer was written by GPU - flush VBO cache */
      for (i = 0; i < nv50->num_vtxbufs; ++i) {
         struct nv04_resource *buf = nv04_resource(nv50->vtxbuf[i].buffer);
         if (buf && buf->status & NOUVEAU_BUFFER_STATUS_GPU_WRITING) {
            buf->status &= ~NOUVEAU_BUFFER_STATUS_GPU_WRITING;
            nv50->base.vbo_dirty = TRUE;
            break;
         }
      }
   }

   /* update vertex format state */
   BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_ATTRIB(0)), n);
   if (nv50->vbo_fifo) {
      nv50->state.num_vtxelts = vertex->num_elements;
      for (i = 0; i < vertex->num_elements; ++i)
         PUSH_DATA (push, vertex->element[i].state);
      for (; i < n; ++i)
         PUSH_DATA (push, NV50_3D_VERTEX_ATTRIB_INACTIVE);
      for (i = 0; i < n; ++i) {
         BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_FETCH(i)), 1);
         PUSH_DATA (push, 0);
      }
      return;
   }
   for (i = 0; i < vertex->num_elements; ++i) {
      const unsigned b = vertex->element[i].pipe.vertex_buffer_index;
      ve = &vertex->element[i];
      vb = &nv50->vtxbuf[b];

      if (likely(vb->stride) || !(nv50->vbo_user & (1 << b)))
         PUSH_DATA(push, ve->state);
      else
         PUSH_DATA(push, ve->state | NV50_3D_VERTEX_ARRAY_ATTRIB_CONST);
   }
   for (; i < n; ++i)
      PUSH_DATA(push, NV50_3D_VERTEX_ATTRIB_INACTIVE);

   /* update per-instance enables */
   mask = vertex->instance_elts ^ nv50->state.instance_elts;
   while (mask) {
      const int i = ffs(mask) - 1;
      mask &= ~(1 << i);
      BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_PER_INSTANCE(i)), 1);
      PUSH_DATA (push, (vertex->instance_elts >> i) & 1);
   }
   nv50->state.instance_elts = vertex->instance_elts;

   if (nv50->vbo_user & ~nv50->vbo_constant)
      nv50_upload_user_buffers(nv50, addrs, limits);

   /* update buffers and set constant attributes */
   for (i = 0; i < vertex->num_elements; ++i) {
      uint64_t address, limit;
      const unsigned b = vertex->element[i].pipe.vertex_buffer_index;
      ve = &vertex->element[i];
      vb = &nv50->vtxbuf[b];

      if (unlikely(nv50->vbo_constant & (1 << b))) {
         BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_FETCH(i)), 1);
         PUSH_DATA (push, 0);
         nv50_emit_vtxattr(nv50, vb, &ve->pipe, i);
         continue;
      } else
      if (nv50->vbo_user & (1 << b)) {
         address = addrs[b] + ve->pipe.src_offset;
         limit = addrs[b] + limits[b];
      } else {
         struct nv04_resource *buf = nv04_resource(vb->buffer);
         if (!(refd & (1 << b))) {
            refd |= 1 << b;
            BCTX_REFN(nv50->bufctx_3d, VERTEX, buf, RD);
         }
         address = buf->address + vb->buffer_offset + ve->pipe.src_offset;
         limit = buf->address + buf->base.width0 - 1;
      }

      if (unlikely(ve->pipe.instance_divisor)) {
         BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_FETCH(i)), 4);
         PUSH_DATA (push, NV50_3D_VERTEX_ARRAY_FETCH_ENABLE | vb->stride);
         PUSH_DATAh(push, address);
         PUSH_DATA (push, address);
         PUSH_DATA (push, ve->pipe.instance_divisor);
      } else {
         BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_FETCH(i)), 3);
         PUSH_DATA (push, NV50_3D_VERTEX_ARRAY_FETCH_ENABLE | vb->stride);
         PUSH_DATAh(push, address);
         PUSH_DATA (push, address);
      }
      BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_LIMIT_HIGH(i)), 2);
      PUSH_DATAh(push, limit);
      PUSH_DATA (push, limit);
   }
   for (; i < nv50->state.num_vtxelts; ++i) {
      BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_FETCH(i)), 1);
      PUSH_DATA (push, 0);
   }
   nv50->state.num_vtxelts = vertex->num_elements;
}

#define NV50_PRIM_GL_CASE(n) \
   case PIPE_PRIM_##n: return NV50_3D_VERTEX_BEGIN_GL_PRIMITIVE_##n

static INLINE unsigned
nv50_prim_gl(unsigned prim)
{
   switch (prim) {
   NV50_PRIM_GL_CASE(POINTS);
   NV50_PRIM_GL_CASE(LINES);
   NV50_PRIM_GL_CASE(LINE_LOOP);
   NV50_PRIM_GL_CASE(LINE_STRIP);
   NV50_PRIM_GL_CASE(TRIANGLES);
   NV50_PRIM_GL_CASE(TRIANGLE_STRIP);
   NV50_PRIM_GL_CASE(TRIANGLE_FAN);
   NV50_PRIM_GL_CASE(QUADS);
   NV50_PRIM_GL_CASE(QUAD_STRIP);
   NV50_PRIM_GL_CASE(POLYGON);
   NV50_PRIM_GL_CASE(LINES_ADJACENCY);
   NV50_PRIM_GL_CASE(LINE_STRIP_ADJACENCY);
   NV50_PRIM_GL_CASE(TRIANGLES_ADJACENCY);
   NV50_PRIM_GL_CASE(TRIANGLE_STRIP_ADJACENCY);
   default:
      return NV50_3D_VERTEX_BEGIN_GL_PRIMITIVE_POINTS;
      break;
   }
}

/* For pre-nva0 transform feedback. */
static const uint8_t nv50_pipe_prim_to_prim_size[PIPE_PRIM_MAX + 1] =
{
   [PIPE_PRIM_POINTS] = 1,
   [PIPE_PRIM_LINES] = 2,
   [PIPE_PRIM_LINE_LOOP] = 2,
   [PIPE_PRIM_LINE_STRIP] = 2,
   [PIPE_PRIM_TRIANGLES] = 3,
   [PIPE_PRIM_TRIANGLE_STRIP] = 3,
   [PIPE_PRIM_TRIANGLE_FAN] = 3,
   [PIPE_PRIM_QUADS] = 3,
   [PIPE_PRIM_QUAD_STRIP] = 3,
   [PIPE_PRIM_POLYGON] = 3,
   [PIPE_PRIM_LINES_ADJACENCY] = 2,
   [PIPE_PRIM_LINE_STRIP_ADJACENCY] = 2,
   [PIPE_PRIM_TRIANGLES_ADJACENCY] = 3,
   [PIPE_PRIM_TRIANGLE_STRIP_ADJACENCY] = 3
d139 1
a139 31
nv50_draw_arrays(struct nv50_context *nv50,
                 unsigned mode, unsigned start, unsigned count,
                 unsigned instance_count)
{
   struct nouveau_pushbuf *push = nv50->base.pushbuf;
   unsigned prim;

   if (nv50->state.index_bias) {
      BEGIN_NV04(push, NV50_3D(VB_ELEMENT_BASE), 1);
      PUSH_DATA (push, 0);
      nv50->state.index_bias = 0;
   }

   prim = nv50_prim_gl(mode);

   while (instance_count--) {
      BEGIN_NV04(push, NV50_3D(VERTEX_BEGIN_GL), 1);
      PUSH_DATA (push, prim);
      BEGIN_NV04(push, NV50_3D(VERTEX_BUFFER_FIRST), 2);
      PUSH_DATA (push, start);
      PUSH_DATA (push, count);
      BEGIN_NV04(push, NV50_3D(VERTEX_END_GL), 1);
      PUSH_DATA (push, 0);

      prim |= NV50_3D_VERTEX_BEGIN_GL_INSTANCE_NEXT;
   }
}

static void
nv50_draw_elements_inline_u08(struct nouveau_pushbuf *push, const uint8_t *map,
                              unsigned start, unsigned count)
d141 223
a363 1
   map += start;
d365 2
a366 18
   if (count & 3) {
      unsigned i;
      BEGIN_NI04(push, NV50_3D(VB_ELEMENT_U32), count & 3);
      for (i = 0; i < (count & 3); ++i)
         PUSH_DATA(push, *map++);
      count &= ~3;
   }
   while (count) {
      unsigned i, nr = MIN2(count, NV04_PFIFO_MAX_PACKET_LEN * 4) / 4;

      BEGIN_NI04(push, NV50_3D(VB_ELEMENT_U8), nr);
      for (i = 0; i < nr; ++i) {
         PUSH_DATA(push,
                   (map[3] << 24) | (map[2] << 16) | (map[1] << 8) | map[0]);
         map += 4;
      }
      count -= nr * 4;
   }
d369 2
a370 3
static void
nv50_draw_elements_inline_u16(struct nouveau_pushbuf *push, const uint16_t *map,
                              unsigned start, unsigned count)
d372 1
a372 18
   map += start;

   if (count & 1) {
      count &= ~1;
      BEGIN_NV04(push, NV50_3D(VB_ELEMENT_U32), 1);
      PUSH_DATA (push, *map++);
   }
   while (count) {
      unsigned i, nr = MIN2(count, NV04_PFIFO_MAX_PACKET_LEN * 2) / 2;

      BEGIN_NI04(push, NV50_3D(VB_ELEMENT_U16), nr);
      for (i = 0; i < nr; ++i) {
         PUSH_DATA(push, (map[1] << 16) | map[0]);
         map += 2;
      }
      count -= nr * 2;
   }
}
d374 2
a375 5
static void
nv50_draw_elements_inline_u32(struct nouveau_pushbuf *push, const uint32_t *map,
                              unsigned start, unsigned count)
{
   map += start;
d377 2
a378 2
   while (count) {
      const unsigned nr = MIN2(count, NV04_PFIFO_MAX_PACKET_LEN);
d380 77
a456 2
      BEGIN_NI04(push, NV50_3D(VB_ELEMENT_U32), nr);
      PUSH_DATAp(push, map, nr);
d458 2
a459 3
      map += nr;
      count -= nr;
   }
d462 2
a463 125
static void
nv50_draw_elements_inline_u32_short(struct nouveau_pushbuf *push,
                                    const uint32_t *map,
                                    unsigned start, unsigned count)
{
   map += start;

   if (count & 1) {
      count--;
      BEGIN_NV04(push, NV50_3D(VB_ELEMENT_U32), 1);
      PUSH_DATA (push, *map++);
   }
   while (count) {
      unsigned i, nr = MIN2(count, NV04_PFIFO_MAX_PACKET_LEN * 2) / 2;

      BEGIN_NI04(push, NV50_3D(VB_ELEMENT_U16), nr);
      for (i = 0; i < nr; ++i) {
         PUSH_DATA(push, (map[1] << 16) | map[0]);
         map += 2;
      }
      count -= nr * 2;
   }
}

static void
nv50_draw_elements(struct nv50_context *nv50, boolean shorten,
                   unsigned mode, unsigned start, unsigned count,
                   unsigned instance_count, int32_t index_bias)
{
   struct nouveau_pushbuf *push = nv50->base.pushbuf;
   unsigned prim;
   const unsigned index_size = nv50->idxbuf.index_size;

   prim = nv50_prim_gl(mode);

   if (index_bias != nv50->state.index_bias) {
      BEGIN_NV04(push, NV50_3D(VB_ELEMENT_BASE), 1);
      PUSH_DATA (push, index_bias);
      nv50->state.index_bias = index_bias;
   }

   if (nv50->idxbuf.buffer) {
      struct nv04_resource *buf = nv04_resource(nv50->idxbuf.buffer);
      unsigned pb_start;
      unsigned pb_bytes;
      const unsigned base = (buf->offset + nv50->idxbuf.offset) & ~3;

      start += ((buf->offset + nv50->idxbuf.offset) & 3) >> (index_size >> 1);

      assert(nouveau_resource_mapped_by_gpu(nv50->idxbuf.buffer));

      while (instance_count--) {
         BEGIN_NV04(push, NV50_3D(VERTEX_BEGIN_GL), 1);
         PUSH_DATA (push, prim);

         nouveau_pushbuf_space(push, 8, 0, 1);

         switch (index_size) {
         case 4:
            BEGIN_NL50(push, NV50_3D(VB_ELEMENT_U32), count);
            nouveau_pushbuf_data(push, buf->bo, base + start * 4, count * 4);
            break;
         case 2:
            pb_start = (start & ~1) * 2;
            pb_bytes = ((start + count + 1) & ~1) * 2 - pb_start;

            BEGIN_NV04(push, NV50_3D(VB_ELEMENT_U16_SETUP), 1);
            PUSH_DATA (push, (start << 31) | count);
            BEGIN_NL50(push, NV50_3D(VB_ELEMENT_U16), pb_bytes / 4);
            nouveau_pushbuf_data(push, buf->bo, base + pb_start, pb_bytes);
            BEGIN_NV04(push, NV50_3D(VB_ELEMENT_U16_SETUP), 1);
            PUSH_DATA (push, 0);
            break;
         default:
            assert(index_size == 1);
            pb_start = start & ~3;
            pb_bytes = ((start + count + 3) & ~3) - pb_start;

            BEGIN_NV04(push, NV50_3D(VB_ELEMENT_U8_SETUP), 1);
            PUSH_DATA (push, (start << 30) | count);
            BEGIN_NL50(push, NV50_3D(VB_ELEMENT_U8), pb_bytes / 4);
            nouveau_pushbuf_data(push, buf->bo, base + pb_start, pb_bytes);
            BEGIN_NV04(push, NV50_3D(VB_ELEMENT_U8_SETUP), 1);
            PUSH_DATA (push, 0);
            break;
         }
         BEGIN_NV04(push, NV50_3D(VERTEX_END_GL), 1);
         PUSH_DATA (push, 0);

         prim |= NV50_3D_VERTEX_BEGIN_GL_INSTANCE_NEXT;
      }
   } else {
      const void *data = nv50->idxbuf.user_buffer;

      while (instance_count--) {
         BEGIN_NV04(push, NV50_3D(VERTEX_BEGIN_GL), 1);
         PUSH_DATA (push, prim);
         switch (index_size) {
         case 1:
            nv50_draw_elements_inline_u08(push, data, start, count);
            break;
         case 2:
            nv50_draw_elements_inline_u16(push, data, start, count);
            break;
         case 4:
            if (shorten)
               nv50_draw_elements_inline_u32_short(push, data, start, count);
            else
               nv50_draw_elements_inline_u32(push, data, start, count);
            break;
         default:
            assert(0);
            return;
         }
         BEGIN_NV04(push, NV50_3D(VERTEX_END_GL), 1);
         PUSH_DATA (push, 0);

         prim |= NV50_3D_VERTEX_BEGIN_GL_INSTANCE_NEXT;
      }
   }
}

static void
nva0_draw_stream_output(struct nv50_context *nv50,
                        const struct pipe_draw_info *info)
d465 1
a465 36
   struct nouveau_pushbuf *push = nv50->base.pushbuf;
   struct nv50_so_target *so = nv50_so_target(info->count_from_stream_output);
   struct nv04_resource *res = nv04_resource(so->pipe.buffer);
   unsigned num_instances = info->instance_count;
   unsigned mode = nv50_prim_gl(info->mode);

   if (unlikely(nv50->screen->base.class_3d < NVA0_3D_CLASS)) {
      /* A proper implementation without waiting doesn't seem possible,
       * so don't bother.
       */
      NOUVEAU_ERR("draw_stream_output not supported on pre-NVA0 cards\n");
      return;
   }

   if (res->status & NOUVEAU_BUFFER_STATUS_GPU_WRITING) {
      res->status &= ~NOUVEAU_BUFFER_STATUS_GPU_WRITING;
      PUSH_SPACE(push, 4);
      BEGIN_NV04(push, SUBC_3D(NV50_GRAPH_SERIALIZE), 1);
      PUSH_DATA (push, 0);
      BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_FLUSH), 1);
      PUSH_DATA (push, 0);
   }

   assert(num_instances);
   do {
      PUSH_SPACE(push, 8);
      BEGIN_NV04(push, NV50_3D(VERTEX_BEGIN_GL), 1);
      PUSH_DATA (push, mode);
      BEGIN_NV04(push, NVA0_3D(DRAW_TFB_BASE), 1);
      PUSH_DATA (push, 0);
      BEGIN_NV04(push, NVA0_3D(DRAW_TFB_STRIDE), 1);
      PUSH_DATA (push, 0);
      BEGIN_NV04(push, NVA0_3D(DRAW_TFB_BYTES), 1);
      nv50_query_pushbuf_submit(push, so->pq, 0x4);
      BEGIN_NV04(push, NV50_3D(VERTEX_END_GL), 1);
      PUSH_DATA (push, 0);
d467 2
a468 2
      mode |= NV50_3D_VERTEX_BEGIN_GL_INSTANCE_NEXT;
   } while (--num_instances);
d471 90
a560 8
static void
nv50_draw_vbo_kick_notify(struct nouveau_pushbuf *chan)
{
   struct nv50_screen *screen = chan->user_priv;

   nouveau_fence_update(&screen->base, TRUE);

   nv50_bufctx_fence(screen->cur_ctx->bufctx_3d, TRUE);
a562 90
void
nv50_draw_vbo(struct pipe_context *pipe, const struct pipe_draw_info *info)
{
   struct nv50_context *nv50 = nv50_context(pipe);
   struct nouveau_pushbuf *push = nv50->base.pushbuf;

   /* NOTE: caller must ensure that (min_index + index_bias) is >= 0 */
   nv50->vb_elt_first = info->min_index + info->index_bias;
   nv50->vb_elt_limit = info->max_index - info->min_index;
   nv50->instance_off = info->start_instance;
   nv50->instance_max = info->instance_count - 1;

   /* For picking only a few vertices from a large user buffer, push is better,
    * if index count is larger and we expect repeated vertices, suggest upload.
    */
   nv50->vbo_push_hint = /* the 64 is heuristic */
      !(info->indexed && ((nv50->vb_elt_limit + 64) < info->count));

   if (nv50->vbo_user && !(nv50->dirty & (NV50_NEW_ARRAYS | NV50_NEW_VERTEX))) {
      if (!!nv50->vbo_fifo != nv50->vbo_push_hint)
         nv50->dirty |= NV50_NEW_ARRAYS;
      else
      if (!nv50->vbo_fifo)
         nv50_update_user_vbufs(nv50);
   }

   if (unlikely(nv50->num_so_targets && !nv50->gmtyprog))
      nv50->state.prim_size = nv50_pipe_prim_to_prim_size[info->mode];

   nv50_state_validate(nv50, ~0, 8); /* 8 as minimum, we use flush_notify */

   push->kick_notify = nv50_draw_vbo_kick_notify;

   if (nv50->vbo_fifo) {
      nv50_push_vbo(nv50, info);
      push->kick_notify = nv50_default_kick_notify;
      nouveau_pushbuf_bufctx(push, NULL);
      return;
   }

   if (nv50->state.instance_base != info->start_instance) {
      nv50->state.instance_base = info->start_instance;
      /* NOTE: this does not affect the shader input, should it ? */
      BEGIN_NV04(push, NV50_3D(VB_INSTANCE_BASE), 1);
      PUSH_DATA (push, info->start_instance);
   }

   if (nv50->base.vbo_dirty) {
      BEGIN_NV04(push, NV50_3D(VERTEX_ARRAY_FLUSH), 1);
      PUSH_DATA (push, 0);
      nv50->base.vbo_dirty = FALSE;
   }

   if (info->indexed) {
      boolean shorten = info->max_index <= 65535;

      if (info->primitive_restart != nv50->state.prim_restart) {
         if (info->primitive_restart) {
            BEGIN_NV04(push, NV50_3D(PRIM_RESTART_ENABLE), 2);
            PUSH_DATA (push, 1);
            PUSH_DATA (push, info->restart_index);

            if (info->restart_index > 65535)
               shorten = FALSE;
         } else {
            BEGIN_NV04(push, NV50_3D(PRIM_RESTART_ENABLE), 1);
            PUSH_DATA (push, 0);
         }
         nv50->state.prim_restart = info->primitive_restart;
      } else
      if (info->primitive_restart) {
         BEGIN_NV04(push, NV50_3D(PRIM_RESTART_INDEX), 1);
         PUSH_DATA (push, info->restart_index);

         if (info->restart_index > 65535)
            shorten = FALSE;
      }

      nv50_draw_elements(nv50, shorten,
                         info->mode, info->start, info->count,
                         info->instance_count, info->index_bias);
   } else
   if (unlikely(info->count_from_stream_output)) {
      nva0_draw_stream_output(nv50, info);
   } else {
      nv50_draw_arrays(nv50,
                       info->mode, info->start, info->count,
                       info->instance_count);
   }
   push->kick_notify = nv50_default_kick_notify;
a563 4
   nv50_release_user_vbufs(nv50);

   nouveau_pushbuf_bufctx(push, NULL);
}
@


1.1.1.3
log
@Import Mesa 9.2.5
@
text
@a599 9
      /* This shouldn't have to be here. The going theory is that the buffer
       * is being filled in by PGRAPH, and it's not done yet by the time it
       * gets submitted to PFIFO, which in turn starts immediately prefetching
       * the not-yet-written data. Ideally this wait would only happen on
       * pushbuf submit, but it's probably not a big performance difference.
       */
      if (buf->fence_wr && !nouveau_fence_signalled(buf->fence_wr))
         nouveau_fence_wait(buf->fence_wr);

@


