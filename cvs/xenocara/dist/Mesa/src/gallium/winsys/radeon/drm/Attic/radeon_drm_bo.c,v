head	1.7;
access;
symbols
	OPENBSD_5_8:1.6.0.4
	OPENBSD_5_8_BASE:1.6
	OPENBSD_5_7:1.6.0.2
	OPENBSD_5_7_BASE:1.6
	v10_2_9:1.1.1.4
	v10_4_3:1.1.1.3
	v10_2_7:1.1.1.2
	OPENBSD_5_6:1.4.0.2
	OPENBSD_5_6_BASE:1.4
	v10_2_3:1.1.1.2
	OPENBSD_5_5:1.3.0.2
	OPENBSD_5_5_BASE:1.3
	v9_2_5:1.1.1.1
	v9_2_3:1.1.1.1
	v9_2_2:1.1.1.1
	v9_2_1:1.1.1.1
	v9_2_0:1.1.1.1
	mesa:1.1.1
	OPENBSD_5_4:1.2.0.2
	OPENBSD_5_4_BASE:1.2
	OPENBSD_5_3:1.1.0.2
	OPENBSD_5_3_BASE:1.1;
locks; strict;
comment	@ * @;


1.7
date	2015.12.23.05.17.42;	author jsg;	state dead;
branches;
next	1.6;
commitid	TnlogFl9nOv2eaRf;

1.6
date	2015.02.20.23.09.56;	author jsg;	state Exp;
branches;
next	1.5;
commitid	4ry2gvZGMXkCUD2n;

1.5
date	2015.01.25.14.41.19;	author jsg;	state Exp;
branches;
next	1.4;
commitid	mcxB0JvoI9gTDYXU;

1.4
date	2014.07.09.21.08.57;	author jsg;	state Exp;
branches;
next	1.3;
commitid	WPD6rgPryPkvXOr9;

1.3
date	2013.09.05.14.02.16;	author jsg;	state Exp;
branches;
next	1.2;

1.2
date	2013.06.09.13.57.19;	author jsg;	state Exp;
branches;
next	1.1;

1.1
date	2012.08.17.13.58.09;	author mpi;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2013.09.05.13.13.54;	author jsg;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2014.07.09.20.34.23;	author jsg;	state Exp;
branches;
next	1.1.1.3;
commitid	3JhLfwcuBALP0ZR7;

1.1.1.3
date	2015.01.25.14.09.54;	author jsg;	state Exp;
branches;
next	1.1.1.4;
commitid	ce2W5rH5aF7VS9gi;

1.1.1.4
date	2015.02.20.22.47.01;	author jsg;	state Exp;
branches;
next	;
commitid	F54a1i0WXHMxq7kE;


desc
@@


1.7
log
@remove the now unused Mesa 10.2.9 code
@
text
@/*
 * Copyright © 2011 Marek Olšák <maraeo@@gmail.com>
 * All Rights Reserved.
 *
 * Permission is hereby granted, free of charge, to any person obtaining
 * a copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sub license, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
 * OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
 * NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS, AUTHORS
 * AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
 * USE OR OTHER DEALINGS IN THE SOFTWARE.
 *
 * The above copyright notice and this permission notice (including the
 * next paragraph) shall be included in all copies or substantial portions
 * of the Software.
 */

#define _FILE_OFFSET_BITS 64
#include "radeon_drm_cs.h"

#include "util/u_hash_table.h"
#include "util/u_memory.h"
#include "util/u_simple_list.h"
#include "util/u_double_list.h"
#include "os/os_thread.h"
#include "os/os_mman.h"
#include "os/os_time.h"

#include "state_tracker/drm_driver.h"

#include <sys/ioctl.h>
#include <xf86drm.h>
#include <errno.h>
#include <fcntl.h>
#include <stdio.h>

extern const struct pb_vtbl radeon_bo_vtbl;

static INLINE struct radeon_bo *radeon_bo(struct pb_buffer *bo)
{
    assert(bo->vtbl == &radeon_bo_vtbl);
    return (struct radeon_bo *)bo;
}

struct radeon_bo_va_hole {
    struct list_head list;
    uint64_t         offset;
    uint64_t         size;
};

struct radeon_bomgr {
    /* Base class. */
    struct pb_manager base;

    /* Winsys. */
    struct radeon_drm_winsys *rws;

    /* List of buffer GEM names. Protected by bo_handles_mutex. */
    struct util_hash_table *bo_names;
    /* List of buffer handles. Protectded by bo_handles_mutex. */
    struct util_hash_table *bo_handles;
    /* List of buffer virtual memory ranges. Protectded by bo_handles_mutex. */
    struct util_hash_table *bo_vas;
    pipe_mutex bo_handles_mutex;
    pipe_mutex bo_va_mutex;

    /* is virtual address supported */
    bool va;
    uint64_t va_offset;
    struct list_head va_holes;
};

static INLINE struct radeon_bomgr *radeon_bomgr(struct pb_manager *mgr)
{
    return (struct radeon_bomgr *)mgr;
}

static struct radeon_bo *get_radeon_bo(struct pb_buffer *_buf)
{
    struct radeon_bo *bo = NULL;

    if (_buf->vtbl == &radeon_bo_vtbl) {
        bo = radeon_bo(_buf);
    } else {
        struct pb_buffer *base_buf;
        pb_size offset;
        pb_get_base_buffer(_buf, &base_buf, &offset);

        if (base_buf->vtbl == &radeon_bo_vtbl)
            bo = radeon_bo(base_buf);
    }

    return bo;
}

static void radeon_bo_wait(struct pb_buffer *_buf, enum radeon_bo_usage usage)
{
    struct radeon_bo *bo = get_radeon_bo(_buf);
    struct drm_radeon_gem_wait_idle args = {0};

    while (p_atomic_read(&bo->num_active_ioctls)) {
        sched_yield();
    }

    args.handle = bo->handle;
    while (drmCommandWrite(bo->rws->fd, DRM_RADEON_GEM_WAIT_IDLE,
                           &args, sizeof(args)) == -EBUSY);
}

static boolean radeon_bo_is_busy(struct pb_buffer *_buf,
                                 enum radeon_bo_usage usage)
{
    struct radeon_bo *bo = get_radeon_bo(_buf);
    struct drm_radeon_gem_busy args = {0};

    if (p_atomic_read(&bo->num_active_ioctls)) {
        return TRUE;
    }

    args.handle = bo->handle;
    return drmCommandWriteRead(bo->rws->fd, DRM_RADEON_GEM_BUSY,
                               &args, sizeof(args)) != 0;
}

static enum radeon_bo_domain get_valid_domain(enum radeon_bo_domain domain)
{
    /* Zero domains the driver doesn't understand. */
    domain &= RADEON_DOMAIN_VRAM_GTT;

    /* If no domain is set, we must set something... */
    if (!domain)
        domain = RADEON_DOMAIN_VRAM_GTT;

    return domain;
}

static enum radeon_bo_domain radeon_bo_get_initial_domain(
		struct radeon_winsys_cs_handle *buf)
{
    struct radeon_bo *bo = (struct radeon_bo*)buf;
    struct drm_radeon_gem_op args;

    if (bo->rws->info.drm_minor < 38)
        return RADEON_DOMAIN_VRAM_GTT;

    memset(&args, 0, sizeof(args));
    args.handle = bo->handle;
    args.op = RADEON_GEM_OP_GET_INITIAL_DOMAIN;

    drmCommandWriteRead(bo->rws->fd, DRM_RADEON_GEM_OP,
                        &args, sizeof(args));

    /* GEM domains and winsys domains are defined the same. */
    return get_valid_domain(args.value);
}

static uint64_t radeon_bomgr_find_va(struct radeon_bomgr *mgr, uint64_t size, uint64_t alignment)
{
    struct radeon_bo_va_hole *hole, *n;
    uint64_t offset = 0, waste = 0;

    alignment = MAX2(alignment, 4096);
    size = align(size, 4096);

    pipe_mutex_lock(mgr->bo_va_mutex);
    /* first look for a hole */
    LIST_FOR_EACH_ENTRY_SAFE(hole, n, &mgr->va_holes, list) {
        offset = hole->offset;
        waste = offset % alignment;
        waste = waste ? alignment - waste : 0;
        offset += waste;
        if (offset >= (hole->offset + hole->size)) {
            continue;
        }
        if (!waste && hole->size == size) {
            offset = hole->offset;
            list_del(&hole->list);
            FREE(hole);
            pipe_mutex_unlock(mgr->bo_va_mutex);
            return offset;
        }
        if ((hole->size - waste) > size) {
            if (waste) {
                n = CALLOC_STRUCT(radeon_bo_va_hole);
                n->size = waste;
                n->offset = hole->offset;
                list_add(&n->list, &hole->list);
            }
            hole->size -= (size + waste);
            hole->offset += size + waste;
            pipe_mutex_unlock(mgr->bo_va_mutex);
            return offset;
        }
        if ((hole->size - waste) == size) {
            hole->size = waste;
            pipe_mutex_unlock(mgr->bo_va_mutex);
            return offset;
        }
    }

    offset = mgr->va_offset;
    waste = offset % alignment;
    waste = waste ? alignment - waste : 0;
    if (waste) {
        n = CALLOC_STRUCT(radeon_bo_va_hole);
        n->size = waste;
        n->offset = offset;
        list_add(&n->list, &mgr->va_holes);
    }
    offset += waste;
    mgr->va_offset += size + waste;
    pipe_mutex_unlock(mgr->bo_va_mutex);
    return offset;
}

static void radeon_bomgr_free_va(struct radeon_bomgr *mgr, uint64_t va, uint64_t size)
{
    struct radeon_bo_va_hole *hole;

    size = align(size, 4096);

    pipe_mutex_lock(mgr->bo_va_mutex);
    if ((va + size) == mgr->va_offset) {
        mgr->va_offset = va;
        /* Delete uppermost hole if it reaches the new top */
        if (!LIST_IS_EMPTY(&mgr->va_holes)) {
            hole = container_of(mgr->va_holes.next, hole, list);
            if ((hole->offset + hole->size) == va) {
                mgr->va_offset = hole->offset;
                list_del(&hole->list);
                FREE(hole);
            }
        }
    } else {
        struct radeon_bo_va_hole *next;

        hole = container_of(&mgr->va_holes, hole, list);
        LIST_FOR_EACH_ENTRY(next, &mgr->va_holes, list) {
	    if (next->offset < va)
	        break;
            hole = next;
        }

        if (&hole->list != &mgr->va_holes) {
            /* Grow upper hole if it's adjacent */
            if (hole->offset == (va + size)) {
                hole->offset = va;
                hole->size += size;
                /* Merge lower hole if it's adjacent */
                if (next != hole && &next->list != &mgr->va_holes &&
                    (next->offset + next->size) == va) {
                    next->size += hole->size;
                    list_del(&hole->list);
                    FREE(hole);
                }
                goto out;
            }
        }

        /* Grow lower hole if it's adjacent */
        if (next != hole && &next->list != &mgr->va_holes &&
            (next->offset + next->size) == va) {
            next->size += size;
            goto out;
        }

        /* FIXME on allocation failure we just lose virtual address space
         * maybe print a warning
         */
        next = CALLOC_STRUCT(radeon_bo_va_hole);
        if (next) {
            next->size = size;
            next->offset = va;
            list_add(&next->list, &hole->list);
        }
    }
out:
    pipe_mutex_unlock(mgr->bo_va_mutex);
}

static void radeon_bo_destroy(struct pb_buffer *_buf)
{
    struct radeon_bo *bo = radeon_bo(_buf);
    struct radeon_bomgr *mgr = bo->mgr;
    struct drm_gem_close args;

    memset(&args, 0, sizeof(args));

    pipe_mutex_lock(bo->mgr->bo_handles_mutex);
    util_hash_table_remove(bo->mgr->bo_handles, (void*)(uintptr_t)bo->handle);
    if (bo->flink_name) {
        util_hash_table_remove(bo->mgr->bo_names,
                               (void*)(uintptr_t)bo->flink_name);
    }
    pipe_mutex_unlock(bo->mgr->bo_handles_mutex);

    if (bo->ptr)
        os_munmap(bo->ptr, bo->base.size);

    /* Close object. */
    args.handle = bo->handle;
    drmIoctl(bo->rws->fd, DRM_IOCTL_GEM_CLOSE, &args);

    if (mgr->va) {
        radeon_bomgr_free_va(mgr, bo->va, bo->base.size);
    }

    pipe_mutex_destroy(bo->map_mutex);

    if (bo->initial_domain & RADEON_DOMAIN_VRAM)
        bo->rws->allocated_vram -= align(bo->base.size, 4096);
    else if (bo->initial_domain & RADEON_DOMAIN_GTT)
        bo->rws->allocated_gtt -= align(bo->base.size, 4096);
    FREE(bo);
}

void *radeon_bo_do_map(struct radeon_bo *bo)
{
    struct drm_radeon_gem_mmap args = {0};
    void *ptr;

    /* Return the pointer if it's already mapped. */
    if (bo->ptr)
        return bo->ptr;

    /* Map the buffer. */
    pipe_mutex_lock(bo->map_mutex);
    /* Return the pointer if it's already mapped (in case of a race). */
    if (bo->ptr) {
        pipe_mutex_unlock(bo->map_mutex);
        return bo->ptr;
    }
    args.handle = bo->handle;
    args.offset = 0;
    args.size = (uint64_t)bo->base.size;
    if (drmCommandWriteRead(bo->rws->fd,
                            DRM_RADEON_GEM_MMAP,
                            &args,
                            sizeof(args))) {
        pipe_mutex_unlock(bo->map_mutex);
        fprintf(stderr, "radeon: gem_mmap failed: %p 0x%08X\n",
                bo, bo->handle);
        return NULL;
    }

    ptr = os_mmap(0, args.size, PROT_READ|PROT_WRITE, MAP_SHARED,
               bo->rws->fd, args.addr_ptr);
    if (ptr == MAP_FAILED) {
        pipe_mutex_unlock(bo->map_mutex);
        fprintf(stderr, "radeon: mmap failed, errno: %i\n", errno);
        return NULL;
    }
    bo->ptr = ptr;
    pipe_mutex_unlock(bo->map_mutex);

    return bo->ptr;
}

static void *radeon_bo_map(struct radeon_winsys_cs_handle *buf,
                           struct radeon_winsys_cs *rcs,
                           enum pipe_transfer_usage usage)
{
    struct radeon_bo *bo = (struct radeon_bo*)buf;
    struct radeon_drm_cs *cs = (struct radeon_drm_cs*)rcs;

    /* If it's not unsynchronized bo_map, flush CS if needed and then wait. */
    if (!(usage & PIPE_TRANSFER_UNSYNCHRONIZED)) {
        /* DONTBLOCK doesn't make sense with UNSYNCHRONIZED. */
        if (usage & PIPE_TRANSFER_DONTBLOCK) {
            if (!(usage & PIPE_TRANSFER_WRITE)) {
                /* Mapping for read.
                 *
                 * Since we are mapping for read, we don't need to wait
                 * if the GPU is using the buffer for read too
                 * (neither one is changing it).
                 *
                 * Only check whether the buffer is being used for write. */
                if (cs && radeon_bo_is_referenced_by_cs_for_write(cs, bo)) {
                    cs->flush_cs(cs->flush_data, RADEON_FLUSH_ASYNC, NULL);
                    return NULL;
                }

                if (radeon_bo_is_busy((struct pb_buffer*)bo,
                                      RADEON_USAGE_WRITE)) {
                    return NULL;
                }
            } else {
                if (cs && radeon_bo_is_referenced_by_cs(cs, bo)) {
                    cs->flush_cs(cs->flush_data, RADEON_FLUSH_ASYNC, NULL);
                    return NULL;
                }

                if (radeon_bo_is_busy((struct pb_buffer*)bo,
                                      RADEON_USAGE_READWRITE)) {
                    return NULL;
                }
            }
        } else {
            uint64_t time = os_time_get_nano();

            if (!(usage & PIPE_TRANSFER_WRITE)) {
                /* Mapping for read.
                 *
                 * Since we are mapping for read, we don't need to wait
                 * if the GPU is using the buffer for read too
                 * (neither one is changing it).
                 *
                 * Only check whether the buffer is being used for write. */
                if (cs && radeon_bo_is_referenced_by_cs_for_write(cs, bo)) {
                    cs->flush_cs(cs->flush_data, 0, NULL);
                }
                radeon_bo_wait((struct pb_buffer*)bo,
                               RADEON_USAGE_WRITE);
            } else {
                /* Mapping for write. */
                if (cs) {
                    if (radeon_bo_is_referenced_by_cs(cs, bo)) {
                        cs->flush_cs(cs->flush_data, 0, NULL);
                    } else {
                        /* Try to avoid busy-waiting in radeon_bo_wait. */
                        if (p_atomic_read(&bo->num_active_ioctls))
                            radeon_drm_cs_sync_flush(rcs);
                    }
                }

                radeon_bo_wait((struct pb_buffer*)bo, RADEON_USAGE_READWRITE);
            }

            bo->mgr->rws->buffer_wait_time += os_time_get_nano() - time;
        }
    }

    return radeon_bo_do_map(bo);
}

static void radeon_bo_unmap(struct radeon_winsys_cs_handle *_buf)
{
    /* NOP */
}

static void radeon_bo_get_base_buffer(struct pb_buffer *buf,
                                      struct pb_buffer **base_buf,
                                      unsigned *offset)
{
    *base_buf = buf;
    *offset = 0;
}

static enum pipe_error radeon_bo_validate(struct pb_buffer *_buf,
                                          struct pb_validate *vl,
                                          unsigned flags)
{
    /* Always pinned */
    return PIPE_OK;
}

static void radeon_bo_fence(struct pb_buffer *buf,
                            struct pipe_fence_handle *fence)
{
}

const struct pb_vtbl radeon_bo_vtbl = {
    radeon_bo_destroy,
    NULL, /* never called */
    NULL, /* never called */
    radeon_bo_validate,
    radeon_bo_fence,
    radeon_bo_get_base_buffer,
};

static struct pb_buffer *radeon_bomgr_create_bo(struct pb_manager *_mgr,
                                                pb_size size,
                                                const struct pb_desc *desc)
{
    struct radeon_bomgr *mgr = radeon_bomgr(_mgr);
    struct radeon_drm_winsys *rws = mgr->rws;
    struct radeon_bo *bo;
    struct drm_radeon_gem_create args;
    struct radeon_bo_desc *rdesc = (struct radeon_bo_desc*)desc;
    int r;

    memset(&args, 0, sizeof(args));

    assert(rdesc->initial_domains);
    assert((rdesc->initial_domains &
            ~(RADEON_GEM_DOMAIN_GTT | RADEON_GEM_DOMAIN_VRAM)) == 0);

    args.size = size;
    args.alignment = desc->alignment;
    args.initial_domain = rdesc->initial_domains;

    if (drmCommandWriteRead(rws->fd, DRM_RADEON_GEM_CREATE,
                            &args, sizeof(args))) {
        fprintf(stderr, "radeon: Failed to allocate a buffer:\n");
        fprintf(stderr, "radeon:    size      : %d bytes\n", size);
        fprintf(stderr, "radeon:    alignment : %d bytes\n", desc->alignment);
        fprintf(stderr, "radeon:    domains   : %d\n", args.initial_domain);
        return NULL;
    }

    bo = CALLOC_STRUCT(radeon_bo);
    if (!bo)
        return NULL;

    pipe_reference_init(&bo->base.reference, 1);
    bo->base.alignment = desc->alignment;
    bo->base.usage = desc->usage;
    bo->base.size = size;
    bo->base.vtbl = &radeon_bo_vtbl;
    bo->mgr = mgr;
    bo->rws = mgr->rws;
    bo->handle = args.handle;
    bo->va = 0;
    bo->initial_domain = rdesc->initial_domains;
    pipe_mutex_init(bo->map_mutex);

    if (mgr->va) {
        struct drm_radeon_gem_va va;

        bo->va = radeon_bomgr_find_va(mgr, size, desc->alignment);

        va.handle = bo->handle;
        va.vm_id = 0;
        va.operation = RADEON_VA_MAP;
        va.flags = RADEON_VM_PAGE_READABLE |
                   RADEON_VM_PAGE_WRITEABLE |
                   RADEON_VM_PAGE_SNOOPED;
        va.offset = bo->va;
        r = drmCommandWriteRead(rws->fd, DRM_RADEON_GEM_VA, &va, sizeof(va));
        if (r && va.operation == RADEON_VA_RESULT_ERROR) {
            fprintf(stderr, "radeon: Failed to allocate virtual address for buffer:\n");
            fprintf(stderr, "radeon:    size      : %d bytes\n", size);
            fprintf(stderr, "radeon:    alignment : %d bytes\n", desc->alignment);
            fprintf(stderr, "radeon:    domains   : %d\n", args.initial_domain);
            fprintf(stderr, "radeon:    va        : 0x%016llx\n", (unsigned long long)bo->va);
            radeon_bo_destroy(&bo->base);
            return NULL;
        }
        pipe_mutex_lock(mgr->bo_handles_mutex);
        if (va.operation == RADEON_VA_RESULT_VA_EXIST) {
            struct pb_buffer *b = &bo->base;
            struct radeon_bo *old_bo =
                util_hash_table_get(mgr->bo_vas, (void*)(uintptr_t)va.offset);

            pipe_mutex_unlock(mgr->bo_handles_mutex);
            pb_reference(&b, &old_bo->base);
            return b;
        }

        util_hash_table_set(mgr->bo_vas, (void*)(uintptr_t)bo->va, bo);
        pipe_mutex_unlock(mgr->bo_handles_mutex);
    }

    if (rdesc->initial_domains & RADEON_DOMAIN_VRAM)
        rws->allocated_vram += align(size, 4096);
    else if (rdesc->initial_domains & RADEON_DOMAIN_GTT)
        rws->allocated_gtt += align(size, 4096);

    return &bo->base;
}

static void radeon_bomgr_flush(struct pb_manager *mgr)
{
    /* NOP */
}

/* This is for the cache bufmgr. */
static boolean radeon_bomgr_is_buffer_busy(struct pb_manager *_mgr,
                                           struct pb_buffer *_buf)
{
   struct radeon_bo *bo = radeon_bo(_buf);

   if (radeon_bo_is_referenced_by_any_cs(bo)) {
       return TRUE;
   }

   if (radeon_bo_is_busy((struct pb_buffer*)bo, RADEON_USAGE_READWRITE)) {
       return TRUE;
   }

   return FALSE;
}

static void radeon_bomgr_destroy(struct pb_manager *_mgr)
{
    struct radeon_bomgr *mgr = radeon_bomgr(_mgr);
    util_hash_table_destroy(mgr->bo_names);
    util_hash_table_destroy(mgr->bo_handles);
    util_hash_table_destroy(mgr->bo_vas);
    pipe_mutex_destroy(mgr->bo_handles_mutex);
    pipe_mutex_destroy(mgr->bo_va_mutex);
    FREE(mgr);
}

#define PTR_TO_UINT(x) ((unsigned)((intptr_t)(x)))

static unsigned handle_hash(void *key)
{
    return PTR_TO_UINT(key);
}

static int handle_compare(void *key1, void *key2)
{
    return PTR_TO_UINT(key1) != PTR_TO_UINT(key2);
}

struct pb_manager *radeon_bomgr_create(struct radeon_drm_winsys *rws)
{
    struct radeon_bomgr *mgr;

    mgr = CALLOC_STRUCT(radeon_bomgr);
    if (!mgr)
        return NULL;

    mgr->base.destroy = radeon_bomgr_destroy;
    mgr->base.create_buffer = radeon_bomgr_create_bo;
    mgr->base.flush = radeon_bomgr_flush;
    mgr->base.is_buffer_busy = radeon_bomgr_is_buffer_busy;

    mgr->rws = rws;
    mgr->bo_names = util_hash_table_create(handle_hash, handle_compare);
    mgr->bo_handles = util_hash_table_create(handle_hash, handle_compare);
    mgr->bo_vas = util_hash_table_create(handle_hash, handle_compare);
    pipe_mutex_init(mgr->bo_handles_mutex);
    pipe_mutex_init(mgr->bo_va_mutex);

    mgr->va = rws->info.r600_virtual_address;
    mgr->va_offset = rws->va_start;
    list_inithead(&mgr->va_holes);

    return &mgr->base;
}

static unsigned eg_tile_split(unsigned tile_split)
{
    switch (tile_split) {
    case 0:     tile_split = 64;    break;
    case 1:     tile_split = 128;   break;
    case 2:     tile_split = 256;   break;
    case 3:     tile_split = 512;   break;
    default:
    case 4:     tile_split = 1024;  break;
    case 5:     tile_split = 2048;  break;
    case 6:     tile_split = 4096;  break;
    }
    return tile_split;
}

static unsigned eg_tile_split_rev(unsigned eg_tile_split)
{
    switch (eg_tile_split) {
    case 64:    return 0;
    case 128:   return 1;
    case 256:   return 2;
    case 512:   return 3;
    default:
    case 1024:  return 4;
    case 2048:  return 5;
    case 4096:  return 6;
    }
}

static void radeon_bo_get_tiling(struct pb_buffer *_buf,
                                 enum radeon_bo_layout *microtiled,
                                 enum radeon_bo_layout *macrotiled,
                                 unsigned *bankw, unsigned *bankh,
                                 unsigned *tile_split,
                                 unsigned *stencil_tile_split,
                                 unsigned *mtilea,
                                 bool *scanout)
{
    struct radeon_bo *bo = get_radeon_bo(_buf);
    struct drm_radeon_gem_set_tiling args;

    memset(&args, 0, sizeof(args));

    args.handle = bo->handle;

    drmCommandWriteRead(bo->rws->fd,
                        DRM_RADEON_GEM_GET_TILING,
                        &args,
                        sizeof(args));

    *microtiled = RADEON_LAYOUT_LINEAR;
    *macrotiled = RADEON_LAYOUT_LINEAR;
    if (args.tiling_flags & RADEON_TILING_MICRO)
        *microtiled = RADEON_LAYOUT_TILED;
    else if (args.tiling_flags & RADEON_TILING_MICRO_SQUARE)
        *microtiled = RADEON_LAYOUT_SQUARETILED;

    if (args.tiling_flags & RADEON_TILING_MACRO)
        *macrotiled = RADEON_LAYOUT_TILED;
    if (bankw && tile_split && stencil_tile_split && mtilea && tile_split) {
        *bankw = (args.tiling_flags >> RADEON_TILING_EG_BANKW_SHIFT) & RADEON_TILING_EG_BANKW_MASK;
        *bankh = (args.tiling_flags >> RADEON_TILING_EG_BANKH_SHIFT) & RADEON_TILING_EG_BANKH_MASK;
        *tile_split = (args.tiling_flags >> RADEON_TILING_EG_TILE_SPLIT_SHIFT) & RADEON_TILING_EG_TILE_SPLIT_MASK;
        *stencil_tile_split = (args.tiling_flags >> RADEON_TILING_EG_STENCIL_TILE_SPLIT_SHIFT) & RADEON_TILING_EG_STENCIL_TILE_SPLIT_MASK;
        *mtilea = (args.tiling_flags >> RADEON_TILING_EG_MACRO_TILE_ASPECT_SHIFT) & RADEON_TILING_EG_MACRO_TILE_ASPECT_MASK;
        *tile_split = eg_tile_split(*tile_split);
    }
    if (scanout)
        *scanout = bo->rws->gen >= DRV_SI && !(args.tiling_flags & RADEON_TILING_R600_NO_SCANOUT);
}

static void radeon_bo_set_tiling(struct pb_buffer *_buf,
                                 struct radeon_winsys_cs *rcs,
                                 enum radeon_bo_layout microtiled,
                                 enum radeon_bo_layout macrotiled,
                                 unsigned bankw, unsigned bankh,
                                 unsigned tile_split,
                                 unsigned stencil_tile_split,
                                 unsigned mtilea,
                                 uint32_t pitch,
                                 bool scanout)
{
    struct radeon_bo *bo = get_radeon_bo(_buf);
    struct radeon_drm_cs *cs = radeon_drm_cs(rcs);
    struct drm_radeon_gem_set_tiling args;

    memset(&args, 0, sizeof(args));

    /* Tiling determines how DRM treats the buffer data.
     * We must flush CS when changing it if the buffer is referenced. */
    if (cs && radeon_bo_is_referenced_by_cs(cs, bo)) {
        cs->flush_cs(cs->flush_data, 0, NULL);
    }

    while (p_atomic_read(&bo->num_active_ioctls)) {
        sched_yield();
    }

    if (microtiled == RADEON_LAYOUT_TILED)
        args.tiling_flags |= RADEON_TILING_MICRO;
    else if (microtiled == RADEON_LAYOUT_SQUARETILED)
        args.tiling_flags |= RADEON_TILING_MICRO_SQUARE;

    if (macrotiled == RADEON_LAYOUT_TILED)
        args.tiling_flags |= RADEON_TILING_MACRO;

    args.tiling_flags |= (bankw & RADEON_TILING_EG_BANKW_MASK) <<
        RADEON_TILING_EG_BANKW_SHIFT;
    args.tiling_flags |= (bankh & RADEON_TILING_EG_BANKH_MASK) <<
        RADEON_TILING_EG_BANKH_SHIFT;
    if (tile_split) {
	args.tiling_flags |= (eg_tile_split_rev(tile_split) &
			      RADEON_TILING_EG_TILE_SPLIT_MASK) <<
	    RADEON_TILING_EG_TILE_SPLIT_SHIFT;
    }
    args.tiling_flags |= (stencil_tile_split &
			  RADEON_TILING_EG_STENCIL_TILE_SPLIT_MASK) <<
        RADEON_TILING_EG_STENCIL_TILE_SPLIT_SHIFT;
    args.tiling_flags |= (mtilea & RADEON_TILING_EG_MACRO_TILE_ASPECT_MASK) <<
        RADEON_TILING_EG_MACRO_TILE_ASPECT_SHIFT;

    if (bo->rws->gen >= DRV_SI && !scanout)
        args.tiling_flags |= RADEON_TILING_R600_NO_SCANOUT;

    args.handle = bo->handle;
    args.pitch = pitch;

    drmCommandWriteRead(bo->rws->fd,
                        DRM_RADEON_GEM_SET_TILING,
                        &args,
                        sizeof(args));
}

static struct radeon_winsys_cs_handle *radeon_drm_get_cs_handle(struct pb_buffer *_buf)
{
    /* return radeon_bo. */
    return (struct radeon_winsys_cs_handle*)get_radeon_bo(_buf);
}

static struct pb_buffer *
radeon_winsys_bo_create(struct radeon_winsys *rws,
                        unsigned size,
                        unsigned alignment,
                        boolean use_reusable_pool,
                        enum radeon_bo_domain domain)
{
    struct radeon_drm_winsys *ws = radeon_drm_winsys(rws);
    struct radeon_bomgr *mgr = radeon_bomgr(ws->kman);
    struct radeon_bo_desc desc;
    struct pb_manager *provider;
    struct pb_buffer *buffer;

    memset(&desc, 0, sizeof(desc));
    desc.base.alignment = alignment;

    /* Additional criteria for the cache manager. */
    desc.base.usage = domain;
    desc.initial_domains = domain;

    /* Assign a buffer manager. */
    if (use_reusable_pool)
        provider = ws->cman;
    else
        provider = ws->kman;

    buffer = provider->create_buffer(provider, size, &desc.base);
    if (!buffer)
        return NULL;

    pipe_mutex_lock(mgr->bo_handles_mutex);
    util_hash_table_set(mgr->bo_handles, (void*)(uintptr_t)get_radeon_bo(buffer)->handle, buffer);
    pipe_mutex_unlock(mgr->bo_handles_mutex);

    return (struct pb_buffer*)buffer;
}

static struct pb_buffer *radeon_winsys_bo_from_handle(struct radeon_winsys *rws,
                                                      struct winsys_handle *whandle,
                                                      unsigned *stride)
{
    struct radeon_drm_winsys *ws = radeon_drm_winsys(rws);
    struct radeon_bo *bo;
    struct radeon_bomgr *mgr = radeon_bomgr(ws->kman);
    int r;
    unsigned handle;
    uint64_t size = 0;

    /* We must maintain a list of pairs <handle, bo>, so that we always return
     * the same BO for one particular handle. If we didn't do that and created
     * more than one BO for the same handle and then relocated them in a CS,
     * we would hit a deadlock in the kernel.
     *
     * The list of pairs is guarded by a mutex, of course. */
    pipe_mutex_lock(mgr->bo_handles_mutex);

    if (whandle->type == DRM_API_HANDLE_TYPE_SHARED) {
        /* First check if there already is an existing bo for the handle. */
        bo = util_hash_table_get(mgr->bo_names, (void*)(uintptr_t)whandle->handle);
    } else if (whandle->type == DRM_API_HANDLE_TYPE_FD) {
        /* We must first get the GEM handle, as fds are unreliable keys */
        r = drmPrimeFDToHandle(ws->fd, whandle->handle, &handle);
        if (r)
            goto fail;
        bo = util_hash_table_get(mgr->bo_handles, (void*)(uintptr_t)handle);
    } else {
        /* Unknown handle type */
        goto fail;
    }

    if (bo) {
        /* Increase the refcount. */
        struct pb_buffer *b = NULL;
        pb_reference(&b, &bo->base);
        goto done;
    }

    /* There isn't, create a new one. */
    bo = CALLOC_STRUCT(radeon_bo);
    if (!bo) {
        goto fail;
    }

    if (whandle->type == DRM_API_HANDLE_TYPE_SHARED) {
        struct drm_gem_open open_arg = {};
        memset(&open_arg, 0, sizeof(open_arg));
        /* Open the BO. */
        open_arg.name = whandle->handle;
        if (drmIoctl(ws->fd, DRM_IOCTL_GEM_OPEN, &open_arg)) {
            FREE(bo);
            goto fail;
        }
        handle = open_arg.handle;
        size = open_arg.size;
        bo->flink_name = whandle->handle;
    } else if (whandle->type == DRM_API_HANDLE_TYPE_FD) {
        size = lseek(whandle->handle, 0, SEEK_END);
        /* 
         * Could check errno to determine whether the kernel is new enough, but
         * it doesn't really matter why this failed, just that it failed.
         */
        if (size == (off_t)-1) {
            FREE(bo);
            goto fail;
        }
        lseek(whandle->handle, 0, SEEK_SET);
    }

    bo->handle = handle;

    /* Initialize it. */
    pipe_reference_init(&bo->base.reference, 1);
    bo->base.alignment = 0;
    bo->base.usage = PB_USAGE_GPU_WRITE | PB_USAGE_GPU_READ;
    bo->base.size = (unsigned) size;
    bo->base.vtbl = &radeon_bo_vtbl;
    bo->mgr = mgr;
    bo->rws = mgr->rws;
    bo->va = 0;
    pipe_mutex_init(bo->map_mutex);

    if (bo->flink_name)
        util_hash_table_set(mgr->bo_names, (void*)(uintptr_t)bo->flink_name, bo);

    util_hash_table_set(mgr->bo_handles, (void*)(uintptr_t)bo->handle, bo);

done:
    pipe_mutex_unlock(mgr->bo_handles_mutex);

    if (stride)
        *stride = whandle->stride;

    if (mgr->va && !bo->va) {
        struct drm_radeon_gem_va va;

        bo->va = radeon_bomgr_find_va(mgr, bo->base.size, 1 << 20);

        va.handle = bo->handle;
        va.operation = RADEON_VA_MAP;
        va.vm_id = 0;
        va.offset = bo->va;
        va.flags = RADEON_VM_PAGE_READABLE |
                   RADEON_VM_PAGE_WRITEABLE |
                   RADEON_VM_PAGE_SNOOPED;
        va.offset = bo->va;
        r = drmCommandWriteRead(ws->fd, DRM_RADEON_GEM_VA, &va, sizeof(va));
        if (r && va.operation == RADEON_VA_RESULT_ERROR) {
            fprintf(stderr, "radeon: Failed to assign virtual address space\n");
            radeon_bo_destroy(&bo->base);
            return NULL;
        }
        pipe_mutex_lock(mgr->bo_handles_mutex);
        if (va.operation == RADEON_VA_RESULT_VA_EXIST) {
            struct pb_buffer *b = &bo->base;
            struct radeon_bo *old_bo =
                util_hash_table_get(mgr->bo_vas, (void*)(uintptr_t)va.offset);

            pipe_mutex_unlock(mgr->bo_handles_mutex);
            pb_reference(&b, &old_bo->base);
            return b;
        }

        util_hash_table_set(mgr->bo_vas, (void*)(uintptr_t)bo->va, bo);
        pipe_mutex_unlock(mgr->bo_handles_mutex);
    }

    bo->initial_domain = radeon_bo_get_initial_domain((void*)bo);

    if (bo->initial_domain & RADEON_DOMAIN_VRAM)
        ws->allocated_vram += align(bo->base.size, 4096);
    else if (bo->initial_domain & RADEON_DOMAIN_GTT)
        ws->allocated_gtt += align(bo->base.size, 4096);

    return (struct pb_buffer*)bo;

fail:
    pipe_mutex_unlock(mgr->bo_handles_mutex);
    return NULL;
}

static boolean radeon_winsys_bo_get_handle(struct pb_buffer *buffer,
                                           unsigned stride,
                                           struct winsys_handle *whandle)
{
    struct drm_gem_flink flink;
    struct radeon_bo *bo = get_radeon_bo(buffer);

    memset(&flink, 0, sizeof(flink));

    if (whandle->type == DRM_API_HANDLE_TYPE_SHARED) {
        if (!bo->flink_name) {
            flink.handle = bo->handle;

            if (ioctl(bo->rws->fd, DRM_IOCTL_GEM_FLINK, &flink)) {
                return FALSE;
            }

            bo->flink_name = flink.name;

            pipe_mutex_lock(bo->mgr->bo_handles_mutex);
            util_hash_table_set(bo->mgr->bo_names, (void*)(uintptr_t)bo->flink_name, bo);
            pipe_mutex_unlock(bo->mgr->bo_handles_mutex);
        }
        whandle->handle = bo->flink_name;
    } else if (whandle->type == DRM_API_HANDLE_TYPE_KMS) {
        whandle->handle = bo->handle;
    } else if (whandle->type == DRM_API_HANDLE_TYPE_FD) {
        if (drmPrimeHandleToFD(bo->rws->fd, bo->handle, DRM_CLOEXEC, (int*)&whandle->handle))
            return FALSE;
    }

    whandle->stride = stride;
    return TRUE;
}

static uint64_t radeon_winsys_bo_va(struct radeon_winsys_cs_handle *buf)
{
    return ((struct radeon_bo*)buf)->va;
}

void radeon_bomgr_init_functions(struct radeon_drm_winsys *ws)
{
    ws->base.buffer_get_cs_handle = radeon_drm_get_cs_handle;
    ws->base.buffer_set_tiling = radeon_bo_set_tiling;
    ws->base.buffer_get_tiling = radeon_bo_get_tiling;
    ws->base.buffer_map = radeon_bo_map;
    ws->base.buffer_unmap = radeon_bo_unmap;
    ws->base.buffer_wait = radeon_bo_wait;
    ws->base.buffer_is_busy = radeon_bo_is_busy;
    ws->base.buffer_create = radeon_winsys_bo_create;
    ws->base.buffer_from_handle = radeon_winsys_bo_from_handle;
    ws->base.buffer_get_handle = radeon_winsys_bo_get_handle;
    ws->base.buffer_get_virtual_address = radeon_winsys_bo_va;
    ws->base.buffer_get_initial_domain = radeon_bo_get_initial_domain;
}
@


1.6
log
@Merge Mesa 10.2.9
@
text
@@


1.5
log
@Merge Mesa 10.4.3
Tested by matthieu@@ mpi@@ and myself.  landry@@ ran a ports bulk build.
kettenis@@ tracked down the cause of an alignment fault on archs
that require strict eight byte pointer alignment.
@
text
@d27 1
a479 12
#ifndef RADEON_GEM_GTT_WC
#define RADEON_GEM_GTT_WC		(1 << 2)
#endif
#ifndef RADEON_GEM_CPU_ACCESS
/* BO is expected to be accessed by the CPU */
#define RADEON_GEM_CPU_ACCESS		(1 << 3)
#endif
#ifndef RADEON_GEM_NO_CPU_ACCESS
/* CPU access is not expected to work for this BO */
#define RADEON_GEM_NO_CPU_ACCESS	(1 << 4)
#endif

a499 8
    args.flags = 0;

    if (rdesc->flags & RADEON_FLAG_GTT_WC)
        args.flags |= RADEON_GEM_GTT_WC;
    if (rdesc->flags & RADEON_FLAG_CPU_ACCESS)
        args.flags |= RADEON_GEM_CPU_ACCESS;
    if (rdesc->flags & RADEON_FLAG_NO_CPU_ACCESS)
        args.flags |= RADEON_GEM_NO_CPU_ACCESS;
a506 1
        fprintf(stderr, "radeon:    flags     : %d\n", args.flags);
d787 1
a787 2
                        enum radeon_bo_domain domain,
                        enum radeon_bo_flag flags)
d798 2
a799 10
    /* Only set one usage bit each for domains and flags, or the cache manager
     * might consider different sets of domains / flags compatible
     */
    if (domain == RADEON_DOMAIN_VRAM_GTT)
        desc.base.usage = 1 << 2;
    else
        desc.base.usage = domain >> 1;
    assert(flags < sizeof(desc.base.usage) * 8 - 3);
    desc.base.usage |= 1 << (flags + 3);

a800 1
    desc.flags = flags;
@


1.4
log
@Merge Mesa 10.2.3
tested by matthieu@@ kettenis@@ mpi@@ brett@@ and myself across a
diverse range of hardware
@
text
@a26 1
#define _FILE_OFFSET_BITS 64
d479 12
d511 8
d526 1
d807 2
a808 1
                        enum radeon_bo_domain domain)
d819 10
a828 2
    /* Additional criteria for the cache manager. */
    desc.base.usage = domain;
d830 1
@


1.3
log
@Merge Mesa 9.2.0
@
text
@d43 2
a44 51

/*
 * this are copy from radeon_drm, once an updated libdrm is released
 * we should bump configure.ac requirement for it and remove the following
 * field
 */
#define RADEON_BO_FLAGS_MACRO_TILE  1
#define RADEON_BO_FLAGS_MICRO_TILE  2
#define RADEON_BO_FLAGS_MICRO_TILE_SQUARE 0x20

#ifndef DRM_RADEON_GEM_WAIT
#define DRM_RADEON_GEM_WAIT     0x2b

#define RADEON_GEM_NO_WAIT      0x1
#define RADEON_GEM_USAGE_READ   0x2
#define RADEON_GEM_USAGE_WRITE  0x4

struct drm_radeon_gem_wait {
    uint32_t    handle;
    uint32_t    flags;  /* one of RADEON_GEM_* */
};

#endif

#ifndef RADEON_VA_MAP

#define RADEON_VA_MAP               1
#define RADEON_VA_UNMAP             2

#define RADEON_VA_RESULT_OK         0
#define RADEON_VA_RESULT_ERROR      1
#define RADEON_VA_RESULT_VA_EXIST   2

#define RADEON_VM_PAGE_VALID        (1 << 0)
#define RADEON_VM_PAGE_READABLE     (1 << 1)
#define RADEON_VM_PAGE_WRITEABLE    (1 << 2)
#define RADEON_VM_PAGE_SYSTEM       (1 << 3)
#define RADEON_VM_PAGE_SNOOPED      (1 << 4)

struct drm_radeon_gem_va {
    uint32_t    handle;
    uint32_t    operation;
    uint32_t    vm_id;
    uint32_t    flags;
    uint64_t    offset;
};

#define DRM_RADEON_GEM_VA   0x2b
#endif


a47 1

d67 3
a69 1
    /* List of buffer handles and its mutex. */
d71 2
d108 1
d114 3
a116 14
    /* XXX use this when it's ready */
    /*if (bo->rws->info.drm_minor >= 12) {
        struct drm_radeon_gem_wait args = {};
        args.handle = bo->handle;
        args.flags = usage;
        while (drmCommandWriteRead(bo->rws->fd, DRM_RADEON_GEM_WAIT,
                                   &args, sizeof(args)) == -EBUSY);
    } else*/ {
        struct drm_radeon_gem_wait_idle args;
        memset(&args, 0, sizeof(args));
        args.handle = bo->handle;
        while (drmCommandWrite(bo->rws->fd, DRM_RADEON_GEM_WAIT_IDLE,
                                   &args, sizeof(args)) == -EBUSY);
    }
d123 1
d129 35
a163 14
    /* XXX use this when it's ready */
    /*if (bo->rws->info.drm_minor >= 12) {
        struct drm_radeon_gem_wait args = {};
        args.handle = bo->handle;
        args.flags = usage | RADEON_GEM_NO_WAIT;
        return drmCommandWriteRead(bo->rws->fd, DRM_RADEON_GEM_WAIT,
                                   &args, sizeof(args)) != 0;
    } else*/ {
        struct drm_radeon_gem_busy args;
        memset(&args, 0, sizeof(args));
        args.handle = bo->handle;
        return drmCommandWriteRead(bo->rws->fd, DRM_RADEON_GEM_BUSY,
                                   &args, sizeof(args)) != 0;
    }
d171 3
d178 2
a179 5
        waste = 0;
        if (alignment) {
            waste = offset % alignment;
            waste = waste ? alignment - waste : 0;
        }
d211 2
a212 5
    waste = 0;
    if (alignment) {
        waste = offset % alignment;
        waste = waste ? alignment - waste : 0;
    }
a224 40
static void radeon_bomgr_force_va(struct radeon_bomgr *mgr, uint64_t va, uint64_t size)
{
    pipe_mutex_lock(mgr->bo_va_mutex);
    if (va >= mgr->va_offset) {
        if (va > mgr->va_offset) {
            struct radeon_bo_va_hole *hole;
            hole = CALLOC_STRUCT(radeon_bo_va_hole);
            if (hole) {
                hole->size = va - mgr->va_offset;
                hole->offset = mgr->va_offset;
                list_add(&hole->list, &mgr->va_holes);
            }
        }
        mgr->va_offset = va + size;
    } else {
        struct radeon_bo_va_hole *hole, *n;
        uint64_t hole_end, va_end;

        /* Prune/free all holes that fall into the range
         */
        LIST_FOR_EACH_ENTRY_SAFE(hole, n, &mgr->va_holes, list) {
            hole_end = hole->offset + hole->size;
            va_end = va + size;
            if (hole->offset >= va_end || hole_end <= va)
                continue;
            if (hole->offset >= va && hole_end <= va_end) {
                list_del(&hole->list);
                FREE(hole);
                continue;
            }
            if (hole->offset >= va)
                hole->offset = va_end;
            else
                hole_end = va;
            hole->size = hole_end - hole->offset;
        }
    }
    pipe_mutex_unlock(mgr->bo_va_mutex);
}

d229 2
d298 5
a302 5
    if (bo->name) {
        pipe_mutex_lock(bo->mgr->bo_handles_mutex);
        util_hash_table_remove(bo->mgr->bo_handles,
                               (void*)(uintptr_t)bo->name);
        pipe_mutex_unlock(bo->mgr->bo_handles_mutex);
d304 1
d314 1
a314 1
        radeon_bomgr_free_va(mgr, bo->va, bo->va_size);
d388 1
a388 1
                    cs->flush_cs(cs->flush_data, RADEON_FLUSH_ASYNC);
d398 1
a398 1
                    cs->flush_cs(cs->flush_data, RADEON_FLUSH_ASYNC);
d419 1
a419 1
                    cs->flush_cs(cs->flush_data, 0);
d427 1
a427 1
                        cs->flush_cs(cs->flush_data, 0);
d529 1
a529 2
        bo->va_size = align(size,  4096);
        bo->va = radeon_bomgr_find_va(mgr, bo->va_size, desc->alignment);
d548 1
d550 7
a556 3
            radeon_bomgr_free_va(mgr, bo->va, bo->va_size);
            bo->va = va.offset;
            radeon_bomgr_force_va(mgr, bo->va, bo->va_size);
d558 3
d596 1
d598 1
d630 1
d632 1
d637 1
a637 1
    mgr->va_offset = rws->info.r600_va_start;
d678 2
a679 1
                                 unsigned *mtilea)
d695 1
a695 1
    if (args.tiling_flags & RADEON_BO_FLAGS_MICRO_TILE)
d697 2
d700 1
a700 1
    if (args.tiling_flags & RADEON_BO_FLAGS_MACRO_TILE)
d710 2
d722 2
a723 1
                                 uint32_t pitch)
d734 1
a734 1
        cs->flush_cs(cs->flush_data, 0);
d742 1
a742 1
        args.tiling_flags |= RADEON_BO_FLAGS_MICRO_TILE;
d744 1
a744 1
        args.tiling_flags |= RADEON_BO_FLAGS_MICRO_TILE_SQUARE;
d747 1
a747 1
        args.tiling_flags |= RADEON_BO_FLAGS_MACRO_TILE;
d764 3
d790 1
d812 4
a825 1
    struct drm_gem_open open_arg = {};
d827 2
a828 2

    memset(&open_arg, 0, sizeof(open_arg));
d838 14
a851 2
    /* First check if there already is an existing bo for the handle. */
    bo = util_hash_table_get(mgr->bo_handles, (void*)(uintptr_t)whandle->handle);
d865 23
a887 5
    /* Open the BO. */
    open_arg.name = whandle->handle;
    if (drmIoctl(ws->fd, DRM_IOCTL_GEM_OPEN, &open_arg)) {
        FREE(bo);
        goto fail;
d889 2
a890 2
    bo->handle = open_arg.handle;
    bo->name = whandle->handle;
d896 1
a896 1
    bo->base.size = open_arg.size;
d903 4
a906 1
    util_hash_table_set(mgr->bo_handles, (void*)(uintptr_t)whandle->handle, bo);
d917 1
a917 2
        bo->va_size = ((bo->base.size + 4095) & ~4095);
        bo->va = radeon_bomgr_find_va(mgr, bo->va_size, 1 << 20);
d933 1
d935 7
a941 3
            radeon_bomgr_free_va(mgr, bo->va, bo->va_size);
            bo->va = va.offset;
            radeon_bomgr_force_va(mgr, bo->va, bo->va_size);
d943 3
d948 6
a953 2
    ws->allocated_vram += align(open_arg.size, 4096);
    bo->initial_domain = RADEON_DOMAIN_VRAM;
d972 1
a972 1
        if (!bo->flinked) {
d979 1
a979 2
            bo->flinked = TRUE;
            bo->flink = flink.name;
d982 1
a982 1
            util_hash_table_set(bo->mgr->bo_handles, (void*)(uintptr_t)bo->flink, bo);
d985 1
a985 1
        whandle->handle = bo->flink;
d988 3
d1015 1
@


1.2
log
@correct RADEON_GEM_WAIT_IDLE use.

RADEON_GEM_WAIT_IDLE is declared DRM_IOW but mesa
uses it with drmCommandWriteRead instead of drmCommandWrite
which leads to the ioctl being unmatched and returning an
error on at least OpenBSD.

Problem originally found and fixed in libdrm by kettenis@@
Dave Airlie pointed out that mesa has the same issue.

This change has already been merged in upstream mesa.

ok matthieu@@ kettenis@@
@
text
@d33 1
d35 2
a40 1
#include <sys/mman.h>
d44 5
d53 42
d104 6
d120 6
d140 3
a142 3
	struct pb_buffer *base_buf;
	pb_size offset;
	pb_get_base_buffer(_buf, &base_buf, &offset);
d151 1
a151 1
static void radeon_bo_wait(struct pb_buffer *_buf)
d153 1
a153 2
    struct radeon_bo *bo = get_radeon_bo(pb_buffer(_buf));
    struct drm_radeon_gem_wait_idle args = {};
d159 14
a172 5
    args.handle = bo->handle;
    while (drmCommandWrite(bo->rws->fd, DRM_RADEON_GEM_WAIT_IDLE,
                               &args, sizeof(args)) == -EBUSY);

    bo->busy_for_write = FALSE;
d175 2
a176 1
static boolean radeon_bo_is_busy(struct pb_buffer *_buf)
d178 1
a178 3
    struct radeon_bo *bo = get_radeon_bo(pb_buffer(_buf));
    struct drm_radeon_gem_busy args = {};
    boolean busy;
d184 166
a349 3
    args.handle = bo->handle;
    busy = drmCommandWriteRead(bo->rws->fd, DRM_RADEON_GEM_BUSY,
                               &args, sizeof(args)) != 0;
d351 12
a362 3
    if (!busy)
        bo->busy_for_write = FALSE;
    return busy;
d368 4
a371 1
    struct drm_gem_close args = {};
d376 1
a376 1
			       (void*)(uintptr_t)bo->name);
d381 1
a381 1
        munmap(bo->ptr, bo->size);
a385 3
    pipe_mutex_destroy(bo->map_mutex);
    FREE(bo);
}
d387 3
a389 3
static unsigned get_pb_usage_from_transfer_flags(enum pipe_transfer_usage usage)
{
    unsigned res = 0;
d391 1
a391 2
    if (usage & PIPE_TRANSFER_WRITE)
        res |= PB_USAGE_CPU_WRITE;
d393 5
a397 7
    if (usage & PIPE_TRANSFER_DONTBLOCK)
        res |= PB_USAGE_DONTBLOCK;

    if (usage & PIPE_TRANSFER_UNSYNCHRONIZED)
        res |= PB_USAGE_UNSYNCHRONIZED;

    return res;
d400 1
a400 2
static void *radeon_bo_map_internal(struct pb_buffer *_buf,
                                    unsigned flags, void *flush_ctx)
d402 1
a402 3
    struct radeon_bo *bo = radeon_bo(_buf);
    struct radeon_drm_cs *cs = flush_ctx;
    struct drm_radeon_gem_mmap args = {};
a404 46
    /* If it's not unsynchronized bo_map, flush CS if needed and then wait. */
    if (!(flags & PB_USAGE_UNSYNCHRONIZED)) {
        /* DONTBLOCK doesn't make sense with UNSYNCHRONIZED. */
        if (flags & PB_USAGE_DONTBLOCK) {
            if (radeon_bo_is_referenced_by_cs(cs, bo)) {
                cs->flush_cs(cs->flush_data, RADEON_FLUSH_ASYNC);
                return NULL;
            }

            if (radeon_bo_is_busy((struct pb_buffer*)bo)) {
                return NULL;
            }
        } else {
            if (!(flags & PB_USAGE_CPU_WRITE)) {
                /* Mapping for read.
                 *
                 * Since we are mapping for read, we don't need to wait
                 * if the GPU is using the buffer for read too
                 * (neither one is changing it).
                 *
                 * Only check whether the buffer is being used for write. */
                if (radeon_bo_is_referenced_by_cs_for_write(cs, bo)) {
                    cs->flush_cs(cs->flush_data, 0);
                    radeon_bo_wait((struct pb_buffer*)bo);
                } else if (bo->busy_for_write) {
                    /* Update the busy_for_write field (done by radeon_bo_is_busy)
                     * and wait if needed. */
                    if (radeon_bo_is_busy((struct pb_buffer*)bo)) {
                        radeon_bo_wait((struct pb_buffer*)bo);
                    }
                }
            } else {
                /* Mapping for write. */
                if (radeon_bo_is_referenced_by_cs(cs, bo)) {
                    cs->flush_cs(cs->flush_data, 0);
                } else {
                    /* Try to avoid busy-waiting in radeon_bo_wait. */
                    if (p_atomic_read(&bo->num_active_ioctls))
                        radeon_drm_cs_sync_flush(cs);
                }

                radeon_bo_wait((struct pb_buffer*)bo);
            }
        }
    }

d418 1
a418 1
    args.size = (uint64_t)bo->size;
d429 1
a429 1
    ptr = mmap(0, args.size, PROT_READ|PROT_WRITE, MAP_SHARED,
d442 78
a519 1
static void radeon_bo_unmap_internal(struct pb_buffer *_buf)
d525 2
a526 2
				      struct pb_buffer **base_buf,
				      unsigned *offset)
d533 2
a534 2
					  struct pb_validate *vl,
					  unsigned flags)
d547 2
a548 2
    radeon_bo_map_internal,
    radeon_bo_unmap_internal,
d555 2
a556 2
						pb_size size,
						const struct pb_desc *desc)
d561 9
a569 1
    struct drm_radeon_gem_create args = {};
d573 1
a573 5
    args.initial_domain =
        (desc->usage & RADEON_PB_USAGE_DOMAIN_GTT  ?
         RADEON_GEM_DOMAIN_GTT  : 0) |
        (desc->usage & RADEON_PB_USAGE_DOMAIN_VRAM ?
         RADEON_GEM_DOMAIN_VRAM : 0);
d586 1
a586 1
	return NULL;
d588 4
a591 4
    pipe_reference_init(&bo->base.base.reference, 1);
    bo->base.base.alignment = desc->alignment;
    bo->base.base.usage = desc->usage;
    bo->base.base.size = size;
d596 2
a597 1
    bo->size = size;
d600 35
d653 1
a653 1
   if (radeon_bo_is_busy((struct pb_buffer*)bo)) {
d665 1
d687 1
a687 1
	return NULL;
d697 6
d706 1
a706 3
static void *radeon_bo_map(struct pb_buffer *buf,
                           struct radeon_winsys_cs *cs,
                           enum pipe_transfer_usage usage)
d708 25
a732 3
    struct pb_buffer *_buf = pb_buffer(buf);

    return pb_map(_buf, get_pb_usage_from_transfer_flags(usage), cs);
d737 5
a741 1
                                 enum radeon_bo_layout *macrotiled)
d743 4
a746 2
    struct radeon_bo *bo = get_radeon_bo(pb_buffer(_buf));
    struct drm_radeon_gem_set_tiling args = {};
d758 1
a758 1
	*microtiled = RADEON_LAYOUT_TILED;
d761 9
a769 1
	*macrotiled = RADEON_LAYOUT_TILED;
d776 4
d782 1
a782 1
    struct radeon_bo *bo = get_radeon_bo(pb_buffer(_buf));
d784 3
a786 1
    struct drm_radeon_gem_set_tiling args = {};
d806 15
d830 1
a830 2
static struct radeon_winsys_cs_handle *radeon_drm_get_cs_handle(
        struct pb_buffer *_buf)
d833 1
a833 16
    return (struct radeon_winsys_cs_handle*)
            get_radeon_bo(pb_buffer(_buf));
}

static unsigned get_pb_usage_from_create_flags(unsigned bind, unsigned usage,
                                               enum radeon_bo_domain domain)
{
    unsigned res = 0;

    if (domain & RADEON_DOMAIN_GTT)
        res |= RADEON_PB_USAGE_DOMAIN_GTT;

    if (domain & RADEON_DOMAIN_VRAM)
        res |= RADEON_PB_USAGE_DOMAIN_VRAM;

    return res;
d840 1
a840 2
                        unsigned bind,
                        unsigned usage,
d844 1
a844 1
    struct pb_desc desc;
d849 5
a853 2
    desc.alignment = alignment;
    desc.usage = get_pb_usage_from_create_flags(bind, usage, domain);
d856 2
a857 2
    if (bind & (PIPE_BIND_VERTEX_BUFFER | PIPE_BIND_INDEX_BUFFER))
	provider = ws->cman;
d861 1
a861 1
    buffer = provider->create_buffer(provider, size, &desc);
d863 1
a863 1
	return NULL;
d869 2
a870 3
                                                           struct winsys_handle *whandle,
                                                           unsigned *stride,
                                                           unsigned *size)
d876 3
a909 1
    bo->size = open_arg.size;
d913 4
a916 4
    pipe_reference_init(&bo->base.base.reference, 1);
    bo->base.base.alignment = 0;
    bo->base.base.usage = PB_USAGE_GPU_WRITE | PB_USAGE_GPU_READ;
    bo->base.base.size = bo->size;
d920 1
d930 30
a959 2
    if (size)
        *size = bo->base.base.size;
d972 4
a975 2
    struct drm_gem_flink flink = {};
    struct radeon_bo *bo = get_radeon_bo(pb_buffer(buffer));
d987 4
d1001 5
d1012 1
a1012 1
    ws->base.buffer_unmap = pb_unmap;
d1018 1
@


1.1
log
@Upate to libGL 7.11.2

Tested by jsg@@, matthieu@@ and ajacoutot@@, ok mattieu@@
@
text
@d100 1
a100 1
    while (drmCommandWriteRead(bo->rws->fd, DRM_RADEON_GEM_WAIT_IDLE,
@


1.1.1.1
log
@Import Mesa 9.2.0
@
text
@a32 1
#include "util/u_double_list.h"
a33 2
#include "os/os_mman.h"
#include "os/os_time.h"
d38 1
a41 5
/*
 * this are copy from radeon_drm, once an updated libdrm is released
 * we should bump configure.ac requirement for it and remove the following
 * field
 */
a45 42
#ifndef DRM_RADEON_GEM_WAIT
#define DRM_RADEON_GEM_WAIT     0x2b

#define RADEON_GEM_NO_WAIT      0x1
#define RADEON_GEM_USAGE_READ   0x2
#define RADEON_GEM_USAGE_WRITE  0x4

struct drm_radeon_gem_wait {
    uint32_t    handle;
    uint32_t    flags;  /* one of RADEON_GEM_* */
};

#endif

#ifndef RADEON_VA_MAP

#define RADEON_VA_MAP               1
#define RADEON_VA_UNMAP             2

#define RADEON_VA_RESULT_OK         0
#define RADEON_VA_RESULT_ERROR      1
#define RADEON_VA_RESULT_VA_EXIST   2

#define RADEON_VM_PAGE_VALID        (1 << 0)
#define RADEON_VM_PAGE_READABLE     (1 << 1)
#define RADEON_VM_PAGE_WRITEABLE    (1 << 2)
#define RADEON_VM_PAGE_SYSTEM       (1 << 3)
#define RADEON_VM_PAGE_SNOOPED      (1 << 4)

struct drm_radeon_gem_va {
    uint32_t    handle;
    uint32_t    operation;
    uint32_t    vm_id;
    uint32_t    flags;
    uint64_t    offset;
};

#define DRM_RADEON_GEM_VA   0x2b
#endif



a54 6
struct radeon_bo_va_hole {
    struct list_head list;
    uint64_t         offset;
    uint64_t         size;
};

a64 6
    pipe_mutex bo_va_mutex;

    /* is virtual address supported */
    bool va;
    uint64_t va_offset;
    struct list_head va_holes;
d79 3
a81 3
        struct pb_buffer *base_buf;
        pb_size offset;
        pb_get_base_buffer(_buf, &base_buf, &offset);
d90 1
a90 1
static void radeon_bo_wait(struct pb_buffer *_buf, enum radeon_bo_usage usage)
d92 2
a93 1
    struct radeon_bo *bo = get_radeon_bo(_buf);
d99 5
a103 14
    /* XXX use this when it's ready */
    /*if (bo->rws->info.drm_minor >= 12) {
        struct drm_radeon_gem_wait args = {};
        args.handle = bo->handle;
        args.flags = usage;
        while (drmCommandWriteRead(bo->rws->fd, DRM_RADEON_GEM_WAIT,
                                   &args, sizeof(args)) == -EBUSY);
    } else*/ {
        struct drm_radeon_gem_wait_idle args;
        memset(&args, 0, sizeof(args));
        args.handle = bo->handle;
        while (drmCommandWrite(bo->rws->fd, DRM_RADEON_GEM_WAIT_IDLE,
                                   &args, sizeof(args)) == -EBUSY);
    }
d106 1
a106 2
static boolean radeon_bo_is_busy(struct pb_buffer *_buf,
                                 enum radeon_bo_usage usage)
d108 3
a110 1
    struct radeon_bo *bo = get_radeon_bo(_buf);
d116 3
a118 136
    /* XXX use this when it's ready */
    /*if (bo->rws->info.drm_minor >= 12) {
        struct drm_radeon_gem_wait args = {};
        args.handle = bo->handle;
        args.flags = usage | RADEON_GEM_NO_WAIT;
        return drmCommandWriteRead(bo->rws->fd, DRM_RADEON_GEM_WAIT,
                                   &args, sizeof(args)) != 0;
    } else*/ {
        struct drm_radeon_gem_busy args;
        memset(&args, 0, sizeof(args));
        args.handle = bo->handle;
        return drmCommandWriteRead(bo->rws->fd, DRM_RADEON_GEM_BUSY,
                                   &args, sizeof(args)) != 0;
    }
}

static uint64_t radeon_bomgr_find_va(struct radeon_bomgr *mgr, uint64_t size, uint64_t alignment)
{
    struct radeon_bo_va_hole *hole, *n;
    uint64_t offset = 0, waste = 0;

    pipe_mutex_lock(mgr->bo_va_mutex);
    /* first look for a hole */
    LIST_FOR_EACH_ENTRY_SAFE(hole, n, &mgr->va_holes, list) {
        offset = hole->offset;
        waste = 0;
        if (alignment) {
            waste = offset % alignment;
            waste = waste ? alignment - waste : 0;
        }
        offset += waste;
        if (offset >= (hole->offset + hole->size)) {
            continue;
        }
        if (!waste && hole->size == size) {
            offset = hole->offset;
            list_del(&hole->list);
            FREE(hole);
            pipe_mutex_unlock(mgr->bo_va_mutex);
            return offset;
        }
        if ((hole->size - waste) > size) {
            if (waste) {
                n = CALLOC_STRUCT(radeon_bo_va_hole);
                n->size = waste;
                n->offset = hole->offset;
                list_add(&n->list, &hole->list);
            }
            hole->size -= (size + waste);
            hole->offset += size + waste;
            pipe_mutex_unlock(mgr->bo_va_mutex);
            return offset;
        }
        if ((hole->size - waste) == size) {
            hole->size = waste;
            pipe_mutex_unlock(mgr->bo_va_mutex);
            return offset;
        }
    }

    offset = mgr->va_offset;
    waste = 0;
    if (alignment) {
        waste = offset % alignment;
        waste = waste ? alignment - waste : 0;
    }
    if (waste) {
        n = CALLOC_STRUCT(radeon_bo_va_hole);
        n->size = waste;
        n->offset = offset;
        list_add(&n->list, &mgr->va_holes);
    }
    offset += waste;
    mgr->va_offset += size + waste;
    pipe_mutex_unlock(mgr->bo_va_mutex);
    return offset;
}

static void radeon_bomgr_force_va(struct radeon_bomgr *mgr, uint64_t va, uint64_t size)
{
    pipe_mutex_lock(mgr->bo_va_mutex);
    if (va >= mgr->va_offset) {
        if (va > mgr->va_offset) {
            struct radeon_bo_va_hole *hole;
            hole = CALLOC_STRUCT(radeon_bo_va_hole);
            if (hole) {
                hole->size = va - mgr->va_offset;
                hole->offset = mgr->va_offset;
                list_add(&hole->list, &mgr->va_holes);
            }
        }
        mgr->va_offset = va + size;
    } else {
        struct radeon_bo_va_hole *hole, *n;
        uint64_t hole_end, va_end;

        /* Prune/free all holes that fall into the range
         */
        LIST_FOR_EACH_ENTRY_SAFE(hole, n, &mgr->va_holes, list) {
            hole_end = hole->offset + hole->size;
            va_end = va + size;
            if (hole->offset >= va_end || hole_end <= va)
                continue;
            if (hole->offset >= va && hole_end <= va_end) {
                list_del(&hole->list);
                FREE(hole);
                continue;
            }
            if (hole->offset >= va)
                hole->offset = va_end;
            else
                hole_end = va;
            hole->size = hole_end - hole->offset;
        }
    }
    pipe_mutex_unlock(mgr->bo_va_mutex);
}

static void radeon_bomgr_free_va(struct radeon_bomgr *mgr, uint64_t va, uint64_t size)
{
    struct radeon_bo_va_hole *hole;

    pipe_mutex_lock(mgr->bo_va_mutex);
    if ((va + size) == mgr->va_offset) {
        mgr->va_offset = va;
        /* Delete uppermost hole if it reaches the new top */
        if (!LIST_IS_EMPTY(&mgr->va_holes)) {
            hole = container_of(mgr->va_holes.next, hole, list);
            if ((hole->offset + hole->size) == va) {
                mgr->va_offset = hole->offset;
                list_del(&hole->list);
                FREE(hole);
            }
        }
    } else {
        struct radeon_bo_va_hole *next;
d120 3
a122 42
        hole = container_of(&mgr->va_holes, hole, list);
        LIST_FOR_EACH_ENTRY(next, &mgr->va_holes, list) {
	    if (next->offset < va)
	        break;
            hole = next;
        }

        if (&hole->list != &mgr->va_holes) {
            /* Grow upper hole if it's adjacent */
            if (hole->offset == (va + size)) {
                hole->offset = va;
                hole->size += size;
                /* Merge lower hole if it's adjacent */
                if (next != hole && &next->list != &mgr->va_holes &&
                    (next->offset + next->size) == va) {
                    next->size += hole->size;
                    list_del(&hole->list);
                    FREE(hole);
                }
                goto out;
            }
        }

        /* Grow lower hole if it's adjacent */
        if (next != hole && &next->list != &mgr->va_holes &&
            (next->offset + next->size) == va) {
            next->size += size;
            goto out;
        }

        /* FIXME on allocation failure we just lose virtual address space
         * maybe print a warning
         */
        next = CALLOC_STRUCT(radeon_bo_va_hole);
        if (next) {
            next->size = size;
            next->offset = va;
            list_add(&next->list, &hole->list);
        }
    }
out:
    pipe_mutex_unlock(mgr->bo_va_mutex);
d128 1
a128 4
    struct radeon_bomgr *mgr = bo->mgr;
    struct drm_gem_close args;

    memset(&args, 0, sizeof(args));
d133 1
a133 1
                               (void*)(uintptr_t)bo->name);
d138 1
a138 1
        os_munmap(bo->ptr, bo->base.size);
d143 10
d154 2
a155 3
    if (mgr->va) {
        radeon_bomgr_free_va(mgr, bo->va, bo->va_size);
    }
d157 2
a158 1
    pipe_mutex_destroy(bo->map_mutex);
d160 1
a160 5
    if (bo->initial_domain & RADEON_DOMAIN_VRAM)
        bo->rws->allocated_vram -= align(bo->base.size, 4096);
    else if (bo->initial_domain & RADEON_DOMAIN_GTT)
        bo->rws->allocated_gtt -= align(bo->base.size, 4096);
    FREE(bo);
d163 2
a164 1
void *radeon_bo_do_map(struct radeon_bo *bo)
d166 3
a168 1
    struct drm_radeon_gem_mmap args = {0};
d171 46
d230 1
a230 1
    args.size = (uint64_t)bo->base.size;
d241 1
a241 1
    ptr = os_mmap(0, args.size, PROT_READ|PROT_WRITE, MAP_SHARED,
d254 1
a254 78
static void *radeon_bo_map(struct radeon_winsys_cs_handle *buf,
                           struct radeon_winsys_cs *rcs,
                           enum pipe_transfer_usage usage)
{
    struct radeon_bo *bo = (struct radeon_bo*)buf;
    struct radeon_drm_cs *cs = (struct radeon_drm_cs*)rcs;

    /* If it's not unsynchronized bo_map, flush CS if needed and then wait. */
    if (!(usage & PIPE_TRANSFER_UNSYNCHRONIZED)) {
        /* DONTBLOCK doesn't make sense with UNSYNCHRONIZED. */
        if (usage & PIPE_TRANSFER_DONTBLOCK) {
            if (!(usage & PIPE_TRANSFER_WRITE)) {
                /* Mapping for read.
                 *
                 * Since we are mapping for read, we don't need to wait
                 * if the GPU is using the buffer for read too
                 * (neither one is changing it).
                 *
                 * Only check whether the buffer is being used for write. */
                if (cs && radeon_bo_is_referenced_by_cs_for_write(cs, bo)) {
                    cs->flush_cs(cs->flush_data, RADEON_FLUSH_ASYNC);
                    return NULL;
                }

                if (radeon_bo_is_busy((struct pb_buffer*)bo,
                                      RADEON_USAGE_WRITE)) {
                    return NULL;
                }
            } else {
                if (cs && radeon_bo_is_referenced_by_cs(cs, bo)) {
                    cs->flush_cs(cs->flush_data, RADEON_FLUSH_ASYNC);
                    return NULL;
                }

                if (radeon_bo_is_busy((struct pb_buffer*)bo,
                                      RADEON_USAGE_READWRITE)) {
                    return NULL;
                }
            }
        } else {
            uint64_t time = os_time_get_nano();

            if (!(usage & PIPE_TRANSFER_WRITE)) {
                /* Mapping for read.
                 *
                 * Since we are mapping for read, we don't need to wait
                 * if the GPU is using the buffer for read too
                 * (neither one is changing it).
                 *
                 * Only check whether the buffer is being used for write. */
                if (cs && radeon_bo_is_referenced_by_cs_for_write(cs, bo)) {
                    cs->flush_cs(cs->flush_data, 0);
                }
                radeon_bo_wait((struct pb_buffer*)bo,
                               RADEON_USAGE_WRITE);
            } else {
                /* Mapping for write. */
                if (cs) {
                    if (radeon_bo_is_referenced_by_cs(cs, bo)) {
                        cs->flush_cs(cs->flush_data, 0);
                    } else {
                        /* Try to avoid busy-waiting in radeon_bo_wait. */
                        if (p_atomic_read(&bo->num_active_ioctls))
                            radeon_drm_cs_sync_flush(rcs);
                    }
                }

                radeon_bo_wait((struct pb_buffer*)bo, RADEON_USAGE_READWRITE);
            }

            bo->mgr->rws->buffer_wait_time += os_time_get_nano() - time;
        }
    }

    return radeon_bo_do_map(bo);
}

static void radeon_bo_unmap(struct radeon_winsys_cs_handle *_buf)
d260 2
a261 2
                                      struct pb_buffer **base_buf,
                                      unsigned *offset)
d268 2
a269 2
                                          struct pb_validate *vl,
                                          unsigned flags)
d282 2
a283 2
    NULL, /* never called */
    NULL, /* never called */
d290 2
a291 2
                                                pb_size size,
                                                const struct pb_desc *desc)
d296 1
a296 9
    struct drm_radeon_gem_create args;
    struct radeon_bo_desc *rdesc = (struct radeon_bo_desc*)desc;
    int r;

    memset(&args, 0, sizeof(args));

    assert(rdesc->initial_domains);
    assert((rdesc->initial_domains &
            ~(RADEON_GEM_DOMAIN_GTT | RADEON_GEM_DOMAIN_VRAM)) == 0);
d300 5
a304 1
    args.initial_domain = rdesc->initial_domains;
d317 1
a317 1
        return NULL;
d319 4
a322 4
    pipe_reference_init(&bo->base.reference, 1);
    bo->base.alignment = desc->alignment;
    bo->base.usage = desc->usage;
    bo->base.size = size;
d327 1
a327 2
    bo->va = 0;
    bo->initial_domain = rdesc->initial_domains;
a329 35
    if (mgr->va) {
        struct drm_radeon_gem_va va;

        bo->va_size = align(size,  4096);
        bo->va = radeon_bomgr_find_va(mgr, bo->va_size, desc->alignment);

        va.handle = bo->handle;
        va.vm_id = 0;
        va.operation = RADEON_VA_MAP;
        va.flags = RADEON_VM_PAGE_READABLE |
                   RADEON_VM_PAGE_WRITEABLE |
                   RADEON_VM_PAGE_SNOOPED;
        va.offset = bo->va;
        r = drmCommandWriteRead(rws->fd, DRM_RADEON_GEM_VA, &va, sizeof(va));
        if (r && va.operation == RADEON_VA_RESULT_ERROR) {
            fprintf(stderr, "radeon: Failed to allocate virtual address for buffer:\n");
            fprintf(stderr, "radeon:    size      : %d bytes\n", size);
            fprintf(stderr, "radeon:    alignment : %d bytes\n", desc->alignment);
            fprintf(stderr, "radeon:    domains   : %d\n", args.initial_domain);
            fprintf(stderr, "radeon:    va        : 0x%016llx\n", (unsigned long long)bo->va);
            radeon_bo_destroy(&bo->base);
            return NULL;
        }
        if (va.operation == RADEON_VA_RESULT_VA_EXIST) {
            radeon_bomgr_free_va(mgr, bo->va, bo->va_size);
            bo->va = va.offset;
            radeon_bomgr_force_va(mgr, bo->va, bo->va_size);
        }
    }

    if (rdesc->initial_domains & RADEON_DOMAIN_VRAM)
        rws->allocated_vram += align(size, 4096);
    else if (rdesc->initial_domains & RADEON_DOMAIN_GTT)
        rws->allocated_gtt += align(size, 4096);

d348 1
a348 1
   if (radeon_bo_is_busy((struct pb_buffer*)bo, RADEON_USAGE_READWRITE)) {
a359 1
    pipe_mutex_destroy(mgr->bo_va_mutex);
d381 1
a381 1
        return NULL;
a390 6
    pipe_mutex_init(mgr->bo_va_mutex);

    mgr->va = rws->info.r600_virtual_address;
    mgr->va_offset = rws->info.r600_va_start;
    list_inithead(&mgr->va_holes);

d394 3
a396 1
static unsigned eg_tile_split(unsigned tile_split)
d398 3
a400 25
    switch (tile_split) {
    case 0:     tile_split = 64;    break;
    case 1:     tile_split = 128;   break;
    case 2:     tile_split = 256;   break;
    case 3:     tile_split = 512;   break;
    default:
    case 4:     tile_split = 1024;  break;
    case 5:     tile_split = 2048;  break;
    case 6:     tile_split = 4096;  break;
    }
    return tile_split;
}

static unsigned eg_tile_split_rev(unsigned eg_tile_split)
{
    switch (eg_tile_split) {
    case 64:    return 0;
    case 128:   return 1;
    case 256:   return 2;
    case 512:   return 3;
    default:
    case 1024:  return 4;
    case 2048:  return 5;
    case 4096:  return 6;
    }
d405 1
a405 5
                                 enum radeon_bo_layout *macrotiled,
                                 unsigned *bankw, unsigned *bankh,
                                 unsigned *tile_split,
                                 unsigned *stencil_tile_split,
                                 unsigned *mtilea)
d407 2
a408 4
    struct radeon_bo *bo = get_radeon_bo(_buf);
    struct drm_radeon_gem_set_tiling args;

    memset(&args, 0, sizeof(args));
d420 1
a420 1
        *microtiled = RADEON_LAYOUT_TILED;
d423 1
a423 9
        *macrotiled = RADEON_LAYOUT_TILED;
    if (bankw && tile_split && stencil_tile_split && mtilea && tile_split) {
        *bankw = (args.tiling_flags >> RADEON_TILING_EG_BANKW_SHIFT) & RADEON_TILING_EG_BANKW_MASK;
        *bankh = (args.tiling_flags >> RADEON_TILING_EG_BANKH_SHIFT) & RADEON_TILING_EG_BANKH_MASK;
        *tile_split = (args.tiling_flags >> RADEON_TILING_EG_TILE_SPLIT_SHIFT) & RADEON_TILING_EG_TILE_SPLIT_MASK;
        *stencil_tile_split = (args.tiling_flags >> RADEON_TILING_EG_STENCIL_TILE_SPLIT_SHIFT) & RADEON_TILING_EG_STENCIL_TILE_SPLIT_MASK;
        *mtilea = (args.tiling_flags >> RADEON_TILING_EG_MACRO_TILE_ASPECT_SHIFT) & RADEON_TILING_EG_MACRO_TILE_ASPECT_MASK;
        *tile_split = eg_tile_split(*tile_split);
    }
a429 4
                                 unsigned bankw, unsigned bankh,
                                 unsigned tile_split,
                                 unsigned stencil_tile_split,
                                 unsigned mtilea,
d432 1
a432 1
    struct radeon_bo *bo = get_radeon_bo(_buf);
d434 1
a434 3
    struct drm_radeon_gem_set_tiling args;

    memset(&args, 0, sizeof(args));
a453 15
    args.tiling_flags |= (bankw & RADEON_TILING_EG_BANKW_MASK) <<
        RADEON_TILING_EG_BANKW_SHIFT;
    args.tiling_flags |= (bankh & RADEON_TILING_EG_BANKH_MASK) <<
        RADEON_TILING_EG_BANKH_SHIFT;
    if (tile_split) {
	args.tiling_flags |= (eg_tile_split_rev(tile_split) &
			      RADEON_TILING_EG_TILE_SPLIT_MASK) <<
	    RADEON_TILING_EG_TILE_SPLIT_SHIFT;
    }
    args.tiling_flags |= (stencil_tile_split &
			  RADEON_TILING_EG_STENCIL_TILE_SPLIT_MASK) <<
        RADEON_TILING_EG_STENCIL_TILE_SPLIT_SHIFT;
    args.tiling_flags |= (mtilea & RADEON_TILING_EG_MACRO_TILE_ASPECT_MASK) <<
        RADEON_TILING_EG_MACRO_TILE_ASPECT_SHIFT;

d463 2
a464 1
static struct radeon_winsys_cs_handle *radeon_drm_get_cs_handle(struct pb_buffer *_buf)
d467 16
a482 1
    return (struct radeon_winsys_cs_handle*)get_radeon_bo(_buf);
d489 2
a490 1
                        boolean use_reusable_pool,
d494 1
a494 1
    struct radeon_bo_desc desc;
d499 2
a500 5
    desc.base.alignment = alignment;

    /* Additional criteria for the cache manager. */
    desc.base.usage = domain;
    desc.initial_domains = domain;
d503 2
a504 2
    if (use_reusable_pool)
        provider = ws->cman;
d508 1
a508 1
    buffer = provider->create_buffer(provider, size, &desc.base);
d510 1
a510 1
        return NULL;
d516 3
a518 2
                                                      struct winsys_handle *whandle,
                                                      unsigned *stride)
a523 3
    int r;

    memset(&open_arg, 0, sizeof(open_arg));
d555 1
d559 4
a562 4
    pipe_reference_init(&bo->base.reference, 1);
    bo->base.alignment = 0;
    bo->base.usage = PB_USAGE_GPU_WRITE | PB_USAGE_GPU_READ;
    bo->base.size = open_arg.size;
a565 1
    bo->va = 0;
d575 2
a576 30

    if (mgr->va && !bo->va) {
        struct drm_radeon_gem_va va;

        bo->va_size = ((bo->base.size + 4095) & ~4095);
        bo->va = radeon_bomgr_find_va(mgr, bo->va_size, 1 << 20);

        va.handle = bo->handle;
        va.operation = RADEON_VA_MAP;
        va.vm_id = 0;
        va.offset = bo->va;
        va.flags = RADEON_VM_PAGE_READABLE |
                   RADEON_VM_PAGE_WRITEABLE |
                   RADEON_VM_PAGE_SNOOPED;
        va.offset = bo->va;
        r = drmCommandWriteRead(ws->fd, DRM_RADEON_GEM_VA, &va, sizeof(va));
        if (r && va.operation == RADEON_VA_RESULT_ERROR) {
            fprintf(stderr, "radeon: Failed to assign virtual address space\n");
            radeon_bo_destroy(&bo->base);
            return NULL;
        }
        if (va.operation == RADEON_VA_RESULT_VA_EXIST) {
            radeon_bomgr_free_va(mgr, bo->va, bo->va_size);
            bo->va = va.offset;
            radeon_bomgr_force_va(mgr, bo->va, bo->va_size);
        }
    }

    ws->allocated_vram += align(open_arg.size, 4096);
    bo->initial_domain = RADEON_DOMAIN_VRAM;
d589 2
a590 4
    struct drm_gem_flink flink;
    struct radeon_bo *bo = get_radeon_bo(buffer);

    memset(&flink, 0, sizeof(flink));
a601 4

            pipe_mutex_lock(bo->mgr->bo_handles_mutex);
            util_hash_table_set(bo->mgr->bo_handles, (void*)(uintptr_t)bo->flink, bo);
            pipe_mutex_unlock(bo->mgr->bo_handles_mutex);
a611 5
static uint64_t radeon_winsys_bo_va(struct radeon_winsys_cs_handle *buf)
{
    return ((struct radeon_bo*)buf)->va;
}

d618 1
a618 1
    ws->base.buffer_unmap = radeon_bo_unmap;
a623 1
    ws->base.buffer_get_virtual_address = radeon_winsys_bo_va;
@


1.1.1.2
log
@Import Mesa 10.2.3
@
text
@d43 51
a93 2
#include <fcntl.h>
#include <stdio.h>
d97 1
d117 1
a117 3
    /* List of buffer GEM names. Protected by bo_handles_mutex. */
    struct util_hash_table *bo_names;
    /* List of buffer handles. Protectded by bo_handles_mutex. */
a118 2
    /* List of buffer virtual memory ranges. Protectded by bo_handles_mutex. */
    struct util_hash_table *bo_vas;
a153 1
    struct drm_radeon_gem_wait_idle args = {0};
d159 14
a172 3
    args.handle = bo->handle;
    while (drmCommandWrite(bo->rws->fd, DRM_RADEON_GEM_WAIT_IDLE,
                           &args, sizeof(args)) == -EBUSY);
a178 1
    struct drm_radeon_gem_busy args = {0};
d184 14
a197 35
    args.handle = bo->handle;
    return drmCommandWriteRead(bo->rws->fd, DRM_RADEON_GEM_BUSY,
                               &args, sizeof(args)) != 0;
}

static enum radeon_bo_domain get_valid_domain(enum radeon_bo_domain domain)
{
    /* Zero domains the driver doesn't understand. */
    domain &= RADEON_DOMAIN_VRAM_GTT;

    /* If no domain is set, we must set something... */
    if (!domain)
        domain = RADEON_DOMAIN_VRAM_GTT;

    return domain;
}

static enum radeon_bo_domain radeon_bo_get_initial_domain(
		struct radeon_winsys_cs_handle *buf)
{
    struct radeon_bo *bo = (struct radeon_bo*)buf;
    struct drm_radeon_gem_op args;

    if (bo->rws->info.drm_minor < 38)
        return RADEON_DOMAIN_VRAM_GTT;

    memset(&args, 0, sizeof(args));
    args.handle = bo->handle;
    args.op = RADEON_GEM_OP_GET_INITIAL_DOMAIN;

    drmCommandWriteRead(bo->rws->fd, DRM_RADEON_GEM_OP,
                        &args, sizeof(args));

    /* GEM domains and winsys domains are defined the same. */
    return get_valid_domain(args.value);
a204 3
    alignment = MAX2(alignment, 4096);
    size = align(size, 4096);

d209 5
a213 2
        waste = offset % alignment;
        waste = waste ? alignment - waste : 0;
d245 5
a249 2
    waste = offset % alignment;
    waste = waste ? alignment - waste : 0;
d262 40
a305 2
    size = align(size, 4096);

d373 5
a377 5
    pipe_mutex_lock(bo->mgr->bo_handles_mutex);
    util_hash_table_remove(bo->mgr->bo_handles, (void*)(uintptr_t)bo->handle);
    if (bo->flink_name) {
        util_hash_table_remove(bo->mgr->bo_names,
                               (void*)(uintptr_t)bo->flink_name);
a378 1
    pipe_mutex_unlock(bo->mgr->bo_handles_mutex);
d388 1
a388 1
        radeon_bomgr_free_va(mgr, bo->va, bo->base.size);
d462 1
a462 1
                    cs->flush_cs(cs->flush_data, RADEON_FLUSH_ASYNC, NULL);
d472 1
a472 1
                    cs->flush_cs(cs->flush_data, RADEON_FLUSH_ASYNC, NULL);
d493 1
a493 1
                    cs->flush_cs(cs->flush_data, 0, NULL);
d501 1
a501 1
                        cs->flush_cs(cs->flush_data, 0, NULL);
d603 2
a604 1
        bo->va = radeon_bomgr_find_va(mgr, size, desc->alignment);
a622 1
        pipe_mutex_lock(mgr->bo_handles_mutex);
d624 3
a626 7
            struct pb_buffer *b = &bo->base;
            struct radeon_bo *old_bo =
                util_hash_table_get(mgr->bo_vas, (void*)(uintptr_t)va.offset);

            pipe_mutex_unlock(mgr->bo_handles_mutex);
            pb_reference(&b, &old_bo->base);
            return b;
a627 3

        util_hash_table_set(mgr->bo_vas, (void*)(uintptr_t)bo->va, bo);
        pipe_mutex_unlock(mgr->bo_handles_mutex);
a662 1
    util_hash_table_destroy(mgr->bo_names);
a663 1
    util_hash_table_destroy(mgr->bo_vas);
a694 1
    mgr->bo_names = util_hash_table_create(handle_hash, handle_compare);
a695 1
    mgr->bo_vas = util_hash_table_create(handle_hash, handle_compare);
d700 1
a700 1
    mgr->va_offset = rws->va_start;
d741 1
a741 2
                                 unsigned *mtilea,
                                 bool *scanout)
d757 1
a757 1
    if (args.tiling_flags & RADEON_TILING_MICRO)
a758 2
    else if (args.tiling_flags & RADEON_TILING_MICRO_SQUARE)
        *microtiled = RADEON_LAYOUT_SQUARETILED;
d760 1
a760 1
    if (args.tiling_flags & RADEON_TILING_MACRO)
a769 2
    if (scanout)
        *scanout = bo->rws->gen >= DRV_SI && !(args.tiling_flags & RADEON_TILING_R600_NO_SCANOUT);
d780 1
a780 2
                                 uint32_t pitch,
                                 bool scanout)
d791 1
a791 1
        cs->flush_cs(cs->flush_data, 0, NULL);
d799 1
a799 1
        args.tiling_flags |= RADEON_TILING_MICRO;
d801 1
a801 1
        args.tiling_flags |= RADEON_TILING_MICRO_SQUARE;
d804 1
a804 1
        args.tiling_flags |= RADEON_TILING_MACRO;
a820 3
    if (bo->rws->gen >= DRV_SI && !scanout)
        args.tiling_flags |= RADEON_TILING_R600_NO_SCANOUT;

a843 1
    struct radeon_bomgr *mgr = radeon_bomgr(ws->kman);
a864 4
    pipe_mutex_lock(mgr->bo_handles_mutex);
    util_hash_table_set(mgr->bo_handles, (void*)(uintptr_t)get_radeon_bo(buffer)->handle, buffer);
    pipe_mutex_unlock(mgr->bo_handles_mutex);

d875 1
d877 2
a878 2
    unsigned handle;
    uint64_t size = 0;
d888 2
a889 14
    if (whandle->type == DRM_API_HANDLE_TYPE_SHARED) {
        /* First check if there already is an existing bo for the handle. */
        bo = util_hash_table_get(mgr->bo_names, (void*)(uintptr_t)whandle->handle);
    } else if (whandle->type == DRM_API_HANDLE_TYPE_FD) {
        /* We must first get the GEM handle, as fds are unreliable keys */
        r = drmPrimeFDToHandle(ws->fd, whandle->handle, &handle);
        if (r)
            goto fail;
        bo = util_hash_table_get(mgr->bo_handles, (void*)(uintptr_t)handle);
    } else {
        /* Unknown handle type */
        goto fail;
    }

d903 5
a907 23
    if (whandle->type == DRM_API_HANDLE_TYPE_SHARED) {
        struct drm_gem_open open_arg = {};
        memset(&open_arg, 0, sizeof(open_arg));
        /* Open the BO. */
        open_arg.name = whandle->handle;
        if (drmIoctl(ws->fd, DRM_IOCTL_GEM_OPEN, &open_arg)) {
            FREE(bo);
            goto fail;
        }
        handle = open_arg.handle;
        size = open_arg.size;
        bo->flink_name = whandle->handle;
    } else if (whandle->type == DRM_API_HANDLE_TYPE_FD) {
        size = lseek(whandle->handle, 0, SEEK_END);
        /* 
         * Could check errno to determine whether the kernel is new enough, but
         * it doesn't really matter why this failed, just that it failed.
         */
        if (size == (off_t)-1) {
            FREE(bo);
            goto fail;
        }
        lseek(whandle->handle, 0, SEEK_SET);
d909 2
a910 2

    bo->handle = handle;
d916 1
a916 1
    bo->base.size = (unsigned) size;
d923 1
a923 4
    if (bo->flink_name)
        util_hash_table_set(mgr->bo_names, (void*)(uintptr_t)bo->flink_name, bo);

    util_hash_table_set(mgr->bo_handles, (void*)(uintptr_t)bo->handle, bo);
d934 2
a935 1
        bo->va = radeon_bomgr_find_va(mgr, bo->base.size, 1 << 20);
a950 1
        pipe_mutex_lock(mgr->bo_handles_mutex);
d952 3
a954 7
            struct pb_buffer *b = &bo->base;
            struct radeon_bo *old_bo =
                util_hash_table_get(mgr->bo_vas, (void*)(uintptr_t)va.offset);

            pipe_mutex_unlock(mgr->bo_handles_mutex);
            pb_reference(&b, &old_bo->base);
            return b;
a955 3

        util_hash_table_set(mgr->bo_vas, (void*)(uintptr_t)bo->va, bo);
        pipe_mutex_unlock(mgr->bo_handles_mutex);
d958 2
a959 6
    bo->initial_domain = radeon_bo_get_initial_domain((void*)bo);

    if (bo->initial_domain & RADEON_DOMAIN_VRAM)
        ws->allocated_vram += align(bo->base.size, 4096);
    else if (bo->initial_domain & RADEON_DOMAIN_GTT)
        ws->allocated_gtt += align(bo->base.size, 4096);
d978 1
a978 1
        if (!bo->flink_name) {
d985 2
a986 1
            bo->flink_name = flink.name;
d989 1
a989 1
            util_hash_table_set(bo->mgr->bo_names, (void*)(uintptr_t)bo->flink_name, bo);
d992 1
a992 1
        whandle->handle = bo->flink_name;
a994 3
    } else if (whandle->type == DRM_API_HANDLE_TYPE_FD) {
        if (drmPrimeHandleToFD(bo->rws->fd, bo->handle, DRM_CLOEXEC, (int*)&whandle->handle))
            return FALSE;
a1018 1
    ws->base.buffer_get_initial_domain = radeon_bo_get_initial_domain;
@


1.1.1.3
log
@Import Mesa 10.4.3
@
text
@d27 1
a479 12
#ifndef RADEON_GEM_GTT_WC
#define RADEON_GEM_GTT_WC		(1 << 2)
#endif
#ifndef RADEON_GEM_CPU_ACCESS
/* BO is expected to be accessed by the CPU */
#define RADEON_GEM_CPU_ACCESS		(1 << 3)
#endif
#ifndef RADEON_GEM_NO_CPU_ACCESS
/* CPU access is not expected to work for this BO */
#define RADEON_GEM_NO_CPU_ACCESS	(1 << 4)
#endif

a499 8
    args.flags = 0;

    if (rdesc->flags & RADEON_FLAG_GTT_WC)
        args.flags |= RADEON_GEM_GTT_WC;
    if (rdesc->flags & RADEON_FLAG_CPU_ACCESS)
        args.flags |= RADEON_GEM_CPU_ACCESS;
    if (rdesc->flags & RADEON_FLAG_NO_CPU_ACCESS)
        args.flags |= RADEON_GEM_NO_CPU_ACCESS;
a506 1
        fprintf(stderr, "radeon:    flags     : %d\n", args.flags);
d787 1
a787 2
                        enum radeon_bo_domain domain,
                        enum radeon_bo_flag flags)
d798 2
a799 10
    /* Only set one usage bit each for domains and flags, or the cache manager
     * might consider different sets of domains / flags compatible
     */
    if (domain == RADEON_DOMAIN_VRAM_GTT)
        desc.base.usage = 1 << 2;
    else
        desc.base.usage = domain >> 1;
    assert(flags < sizeof(desc.base.usage) * 8 - 3);
    desc.base.usage |= 1 << (flags + 3);

a800 1
    desc.flags = flags;
@


1.1.1.4
log
@Import Mesa 10.2.9
@
text
@a26 1
#define _FILE_OFFSET_BITS 64
d479 12
d511 8
d526 1
d807 2
a808 1
                        enum radeon_bo_domain domain)
d819 10
a828 2
    /* Additional criteria for the cache manager. */
    desc.base.usage = domain;
d830 1
@


