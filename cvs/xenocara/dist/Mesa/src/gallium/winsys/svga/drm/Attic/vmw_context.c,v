head	1.7;
access;
symbols
	OPENBSD_5_8:1.6.0.4
	OPENBSD_5_8_BASE:1.6
	OPENBSD_5_7:1.6.0.2
	OPENBSD_5_7_BASE:1.6
	v10_2_9:1.1.1.5
	v10_4_3:1.1.1.4
	v10_2_7:1.1.1.3
	OPENBSD_5_6:1.4.0.2
	OPENBSD_5_6_BASE:1.4
	v10_2_3:1.1.1.3
	OPENBSD_5_5:1.3.0.2
	OPENBSD_5_5_BASE:1.3
	v9_2_5:1.1.1.2
	v9_2_3:1.1.1.2
	v9_2_2:1.1.1.2
	v9_2_1:1.1.1.2
	v9_2_0:1.1.1.2
	OPENBSD_5_4:1.2.0.4
	OPENBSD_5_4_BASE:1.2
	OPENBSD_5_3:1.2.0.2
	OPENBSD_5_3_BASE:1.2
	OPENBSD_5_2:1.1.1.1.0.4
	OPENBSD_5_2_BASE:1.1.1.1
	OPENBSD_5_1_BASE:1.1.1.1
	OPENBSD_5_1:1.1.1.1.0.2
	v7_10_3:1.1.1.1
	mesa:1.1.1;
locks; strict;
comment	@ * @;


1.7
date	2015.12.23.05.17.42;	author jsg;	state dead;
branches;
next	1.6;
commitid	TnlogFl9nOv2eaRf;

1.6
date	2015.02.20.23.09.56;	author jsg;	state Exp;
branches;
next	1.5;
commitid	4ry2gvZGMXkCUD2n;

1.5
date	2015.01.25.14.41.19;	author jsg;	state Exp;
branches;
next	1.4;
commitid	mcxB0JvoI9gTDYXU;

1.4
date	2014.07.09.21.08.57;	author jsg;	state Exp;
branches;
next	1.3;
commitid	WPD6rgPryPkvXOr9;

1.3
date	2013.09.05.14.02.17;	author jsg;	state Exp;
branches;
next	1.2;

1.2
date	2012.08.17.13.58.09;	author mpi;	state Exp;
branches;
next	1.1;

1.1
date	2011.10.23.13.29.30;	author matthieu;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2011.10.23.13.29.30;	author matthieu;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2013.09.05.13.13.55;	author jsg;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2014.07.09.20.34.24;	author jsg;	state Exp;
branches;
next	1.1.1.4;
commitid	3JhLfwcuBALP0ZR7;

1.1.1.4
date	2015.01.25.14.09.55;	author jsg;	state Exp;
branches;
next	1.1.1.5;
commitid	ce2W5rH5aF7VS9gi;

1.1.1.5
date	2015.02.20.22.47.03;	author jsg;	state Exp;
branches;
next	;
commitid	F54a1i0WXHMxq7kE;


desc
@@


1.7
log
@remove the now unused Mesa 10.2.9 code
@
text
@/**********************************************************
 * Copyright 2009 VMware, Inc.  All rights reserved.
 *
 * Permission is hereby granted, free of charge, to any person
 * obtaining a copy of this software and associated documentation
 * files (the "Software"), to deal in the Software without
 * restriction, including without limitation the rights to use, copy,
 * modify, merge, publish, distribute, sublicense, and/or sell copies
 * of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be
 * included in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 *
 **********************************************************/


#include "svga_cmd.h"

#include "util/u_debug.h"
#include "util/u_memory.h"
#include "util/u_debug_stack.h"
#include "util/u_debug_flush.h"
#include "util/u_hash_table.h"
#include "pipebuffer/pb_buffer.h"
#include "pipebuffer/pb_validate.h"

#include "svga_winsys.h"
#include "vmw_context.h"
#include "vmw_screen.h"
#include "vmw_buffer.h"
#include "vmw_surface.h"
#include "vmw_fence.h"
#include "vmw_shader.h"

#define VMW_COMMAND_SIZE (64*1024)
#define VMW_SURFACE_RELOCS (1024)
#define VMW_SHADER_RELOCS (1024)
#define VMW_REGION_RELOCS (512)

#define VMW_MUST_FLUSH_STACK 8

struct vmw_buffer_relocation
{
   struct pb_buffer *buffer;
   boolean is_mob;
   uint32 offset;

   union {
      struct {
	 struct SVGAGuestPtr *where;
      } region;
      struct {
	 SVGAMobId *id;
	 uint32 *offset_into_mob;
      } mob;
   };
};

struct vmw_ctx_validate_item {
   union {
      struct vmw_svga_winsys_surface *vsurf;
      struct vmw_svga_winsys_shader *vshader;
   };
   boolean referenced;
};

struct vmw_svga_winsys_context
{
   struct svga_winsys_context base;

   struct vmw_winsys_screen *vws;
   struct util_hash_table *hash;

#ifdef DEBUG
   boolean must_flush;
   struct debug_stack_frame must_flush_stack[VMW_MUST_FLUSH_STACK];
   struct debug_flush_ctx *fctx;
#endif

   struct {
      uint8_t buffer[VMW_COMMAND_SIZE];
      uint32_t size;
      uint32_t used;
      uint32_t reserved;
   } command;

   struct {
      struct vmw_ctx_validate_item items[VMW_SURFACE_RELOCS];
      uint32_t size;
      uint32_t used;
      uint32_t staged;
      uint32_t reserved;
   } surface;
   
   struct {
      struct vmw_buffer_relocation relocs[VMW_REGION_RELOCS];
      uint32_t size;
      uint32_t used;
      uint32_t staged;
      uint32_t reserved;
   } region;

   struct {
      struct vmw_ctx_validate_item items[VMW_SHADER_RELOCS];
      uint32_t size;
      uint32_t used;
      uint32_t staged;
      uint32_t reserved;
   } shader;

   struct pb_validate *validate;

   /**
    * The amount of surface, GMR or MOB memory that is referred by the commands
    * currently batched in the context command buffer.
    */
   uint64_t seen_surfaces;
   uint64_t seen_regions;
   uint64_t seen_mobs;

   /**
    * Whether this context should fail to reserve more commands, not because it
    * ran out of command space, but because a substantial ammount of GMR was
    * referred.
    */
   boolean preemptive_flush;
};


static INLINE struct vmw_svga_winsys_context *
vmw_svga_winsys_context(struct svga_winsys_context *swc)
{
   assert(swc);
   return (struct vmw_svga_winsys_context *)swc;
}


static INLINE unsigned
vmw_translate_to_pb_flags(unsigned flags)
{
   unsigned f = 0;
   if (flags & SVGA_RELOC_READ)
      f |= PB_USAGE_GPU_READ;

   if (flags & SVGA_RELOC_WRITE)
      f |= PB_USAGE_GPU_WRITE;

   return f;
}

static enum pipe_error
vmw_swc_flush(struct svga_winsys_context *swc,
              struct pipe_fence_handle **pfence)
{
   struct vmw_svga_winsys_context *vswc = vmw_svga_winsys_context(swc);
   struct pipe_fence_handle *fence = NULL;
   unsigned i;
   enum pipe_error ret;

   ret = pb_validate_validate(vswc->validate);
   assert(ret == PIPE_OK);
   if(ret == PIPE_OK) {
   
      /* Apply relocations */
      for(i = 0; i < vswc->region.used; ++i) {
         struct vmw_buffer_relocation *reloc = &vswc->region.relocs[i];
         struct SVGAGuestPtr ptr;

         if(!vmw_gmr_bufmgr_region_ptr(reloc->buffer, &ptr))
            assert(0);

         ptr.offset += reloc->offset;

	 if (reloc->is_mob) {
	    if (reloc->mob.id)
	       *reloc->mob.id = ptr.gmrId;
	    if (reloc->mob.offset_into_mob)
	       *reloc->mob.offset_into_mob = ptr.offset;
	    else {
	       assert(ptr.offset == 0);
	    }
	 } else
	    *reloc->region.where = ptr;
      }

      if (vswc->command.used || pfence != NULL)
         vmw_ioctl_command(vswc->vws,
			   vswc->base.cid,
			   0,
                           vswc->command.buffer,
                           vswc->command.used,
                           &fence);

      pb_validate_fence(vswc->validate, fence);
   }

   vswc->command.used = 0;
   vswc->command.reserved = 0;

   for(i = 0; i < vswc->surface.used + vswc->surface.staged; ++i) {
      struct vmw_ctx_validate_item *isurf = &vswc->surface.items[i];
      if (isurf->referenced)
         p_atomic_dec(&isurf->vsurf->validated);
      vmw_svga_winsys_surface_reference(&isurf->vsurf, NULL);
   }

   util_hash_table_clear(vswc->hash);
   vswc->surface.used = 0;
   vswc->surface.reserved = 0;

   for(i = 0; i < vswc->shader.used + vswc->shader.staged; ++i) {
      struct vmw_ctx_validate_item *ishader = &vswc->shader.items[i];
      if (ishader->referenced)
         p_atomic_dec(&ishader->vshader->validated);
      vmw_svga_winsys_shader_reference(&ishader->vshader, NULL);
   }

   vswc->shader.used = 0;
   vswc->shader.reserved = 0;

   vswc->region.used = 0;
   vswc->region.reserved = 0;

#ifdef DEBUG
   vswc->must_flush = FALSE;
   debug_flush_flush(vswc->fctx);
#endif
   vswc->preemptive_flush = FALSE;
   vswc->seen_surfaces = 0;
   vswc->seen_regions = 0;
   vswc->seen_mobs = 0;

   if(pfence)
      vmw_fence_reference(vswc->vws, pfence, fence);

   vmw_fence_reference(vswc->vws, &fence, NULL);

   return ret;
}


static void *
vmw_swc_reserve(struct svga_winsys_context *swc,
                uint32_t nr_bytes, uint32_t nr_relocs )
{
   struct vmw_svga_winsys_context *vswc = vmw_svga_winsys_context(swc);

#ifdef DEBUG
   /* Check if somebody forgot to check the previous failure */
   if(vswc->must_flush) {
      debug_printf("Forgot to flush:\n");
      debug_backtrace_dump(vswc->must_flush_stack, VMW_MUST_FLUSH_STACK);
      assert(!vswc->must_flush);
   }
   debug_flush_might_flush(vswc->fctx);
#endif

   assert(nr_bytes <= vswc->command.size);
   if(nr_bytes > vswc->command.size)
      return NULL;

   if(vswc->preemptive_flush ||
      vswc->command.used + nr_bytes > vswc->command.size ||
      vswc->surface.used + nr_relocs > vswc->surface.size ||
      vswc->shader.used + nr_relocs > vswc->shader.size ||
      vswc->region.used + nr_relocs > vswc->region.size) {
#ifdef DEBUG
      vswc->must_flush = TRUE;
      debug_backtrace_capture(vswc->must_flush_stack, 1,
                              VMW_MUST_FLUSH_STACK);
#endif
      return NULL;
   }

   assert(vswc->command.used + nr_bytes <= vswc->command.size);
   assert(vswc->surface.used + nr_relocs <= vswc->surface.size);
   assert(vswc->shader.used + nr_relocs <= vswc->shader.size);
   assert(vswc->region.used + nr_relocs <= vswc->region.size);
   
   vswc->command.reserved = nr_bytes;
   vswc->surface.reserved = nr_relocs;
   vswc->surface.staged = 0;
   vswc->shader.reserved = nr_relocs;
   vswc->shader.staged = 0;
   vswc->region.reserved = nr_relocs;
   vswc->region.staged = 0;
   
   return vswc->command.buffer + vswc->command.used;
}

static void
vmw_swc_context_relocation(struct svga_winsys_context *swc,
			   uint32 *cid)
{
   *cid = swc->cid;
}

static boolean
vmw_swc_add_validate_buffer(struct vmw_svga_winsys_context *vswc,
			    struct pb_buffer *pb_buf,
			    unsigned flags)
{
   enum pipe_error ret;
   unsigned translated_flags;

   /*
    * TODO: Update pb_validate to provide a similar functionality
    * (Check buffer already present before adding)
    */
   if (util_hash_table_get(vswc->hash, pb_buf) != pb_buf) {
      translated_flags = vmw_translate_to_pb_flags(flags);
      ret = pb_validate_add_buffer(vswc->validate, pb_buf, translated_flags);
      /* TODO: Update pipebuffer to reserve buffers and not fail here */
      assert(ret == PIPE_OK);
      (void)ret;
      (void)util_hash_table_set(vswc->hash, pb_buf, pb_buf);
      return TRUE;
   }

   return FALSE;
}

static void
vmw_swc_region_relocation(struct svga_winsys_context *swc,
                          struct SVGAGuestPtr *where,
                          struct svga_winsys_buffer *buffer,
                          uint32 offset,
                          unsigned flags)
{
   struct vmw_svga_winsys_context *vswc = vmw_svga_winsys_context(swc);
   struct vmw_buffer_relocation *reloc;

   assert(vswc->region.staged < vswc->region.reserved);

   reloc = &vswc->region.relocs[vswc->region.used + vswc->region.staged];
   reloc->region.where = where;

   /*
    * pb_validate holds a refcount to the buffer, so no need to
    * refcount it again in the relocation.
    */
   reloc->buffer = vmw_pb_buffer(buffer);
   reloc->offset = offset;
   reloc->is_mob = FALSE;
   ++vswc->region.staged;

   if (vmw_swc_add_validate_buffer(vswc, reloc->buffer, flags)) {
      vswc->seen_regions += reloc->buffer->size;
      if(vswc->seen_regions >= VMW_GMR_POOL_SIZE/5)
         vswc->preemptive_flush = TRUE;
   }

#ifdef DEBUG
   if (!(flags & SVGA_RELOC_INTERNAL))
      debug_flush_cb_reference(vswc->fctx, vmw_debug_flush_buf(buffer));
#endif
}

static void
vmw_swc_mob_relocation(struct svga_winsys_context *swc,
		       SVGAMobId *id,
		       uint32 *offset_into_mob,
		       struct svga_winsys_buffer *buffer,
		       uint32 offset,
		       unsigned flags)
{
   struct vmw_svga_winsys_context *vswc = vmw_svga_winsys_context(swc);
   struct vmw_buffer_relocation *reloc;

   assert(vswc->region.staged < vswc->region.reserved);

   reloc = &vswc->region.relocs[vswc->region.used + vswc->region.staged];
   reloc->mob.id = id;
   reloc->mob.offset_into_mob = offset_into_mob;

   /*
    * pb_validate holds a refcount to the buffer, so no need to
    * refcount it again in the relocation.
    */
   reloc->buffer = vmw_pb_buffer(buffer);
   reloc->offset = offset;
   reloc->is_mob = TRUE;
   ++vswc->region.staged;

   if (vmw_swc_add_validate_buffer(vswc, reloc->buffer, flags)) {
      vswc->seen_mobs += reloc->buffer->size;
      /* divide by 5, tested for best performance */
      if (vswc->seen_mobs >= vswc->vws->ioctl.max_mob_memory / 5)
         vswc->preemptive_flush = TRUE;
   }

#ifdef DEBUG
   if (!(flags & SVGA_RELOC_INTERNAL))
      debug_flush_cb_reference(vswc->fctx, vmw_debug_flush_buf(buffer));
#endif
}


/**
 * vmw_swc_surface_clear_reference - Clear referenced info for a surface
 *
 * @@swc:   Pointer to an svga_winsys_context
 * @@vsurf: Pointer to a vmw_svga_winsys_surface, the referenced info of which
 *         we want to clear
 *
 * This is primarily used by a discard surface map to indicate that the
 * surface data is no longer referenced by a draw call, and mapping it
 * should therefore no longer cause a flush.
 */
void
vmw_swc_surface_clear_reference(struct svga_winsys_context *swc,
                                struct vmw_svga_winsys_surface *vsurf)
{
   struct vmw_svga_winsys_context *vswc = vmw_svga_winsys_context(swc);
   struct vmw_ctx_validate_item *isrf =
      util_hash_table_get(vswc->hash, vsurf);

   if (isrf && isrf->referenced) {
      isrf->referenced = FALSE;
      p_atomic_dec(&vsurf->validated);
   }
}

static void
vmw_swc_surface_only_relocation(struct svga_winsys_context *swc,
				uint32 *where,
				struct vmw_svga_winsys_surface *vsurf,
				unsigned flags)
{
   struct vmw_svga_winsys_context *vswc = vmw_svga_winsys_context(swc);
   struct vmw_ctx_validate_item *isrf;

   assert(vswc->surface.staged < vswc->surface.reserved);
   isrf = util_hash_table_get(vswc->hash, vsurf);

   if (isrf == NULL) {
      isrf = &vswc->surface.items[vswc->surface.used + vswc->surface.staged];
      vmw_svga_winsys_surface_reference(&isrf->vsurf, vsurf);
      isrf->referenced = FALSE;
      /*
       * Note that a failure here may just fall back to unhashed behavior
       * and potentially cause unnecessary flushing, so ignore the
       * return code.
       */
      (void) util_hash_table_set(vswc->hash, vsurf, isrf);
      ++vswc->surface.staged;

      vswc->seen_surfaces += vsurf->size;
      /* divide by 5 not well tuned for performance */
      if (vswc->seen_surfaces >= vswc->vws->ioctl.max_surface_memory / 5)
         vswc->preemptive_flush = TRUE;
   }

   if (!(flags & SVGA_RELOC_INTERNAL) && !isrf->referenced) {
      isrf->referenced = TRUE;
      p_atomic_inc(&vsurf->validated);
   }

   *where = vsurf->sid;
}

static void
vmw_swc_surface_relocation(struct svga_winsys_context *swc,
                           uint32 *where,
                           uint32 *mobid,
                           struct svga_winsys_surface *surface,
                           unsigned flags)
{
   struct vmw_svga_winsys_surface *vsurf;

   assert(swc->have_gb_objects || mobid == NULL);

   if(!surface) {
      *where = SVGA3D_INVALID_ID;
      if (mobid)
         *mobid = SVGA3D_INVALID_ID;
      return;
   }

   vsurf = vmw_svga_winsys_surface(surface);
   vmw_swc_surface_only_relocation(swc, where, vsurf, flags);

   if (swc->have_gb_objects && vsurf->buf != NULL) {

      /*
       * Make sure backup buffer ends up fenced.
       */

      pipe_mutex_lock(vsurf->mutex);
      assert(vsurf->buf != NULL);
      
      vmw_swc_mob_relocation(swc, mobid, NULL, (struct svga_winsys_buffer *)
                             vsurf->buf, 0, flags);
      pipe_mutex_unlock(vsurf->mutex);
   }
}

static void
vmw_swc_shader_relocation(struct svga_winsys_context *swc,
			  uint32 *shid,
			  uint32 *mobid,
			  uint32 *offset,
			  struct svga_winsys_gb_shader *shader)
{
   struct vmw_svga_winsys_context *vswc = vmw_svga_winsys_context(swc);
   struct vmw_svga_winsys_shader *vshader;
   struct vmw_ctx_validate_item *ishader;
   if(!shader) {
      *shid = SVGA3D_INVALID_ID;
      return;
   }

   assert(vswc->shader.staged < vswc->shader.reserved);
   vshader = vmw_svga_winsys_shader(shader);
   ishader = util_hash_table_get(vswc->hash, vshader);

   if (ishader == NULL) {
      ishader = &vswc->shader.items[vswc->shader.used + vswc->shader.staged];
      vmw_svga_winsys_shader_reference(&ishader->vshader, vshader);
      ishader->referenced = FALSE;
      /*
       * Note that a failure here may just fall back to unhashed behavior
       * and potentially cause unnecessary flushing, so ignore the
       * return code.
       */
      (void) util_hash_table_set(vswc->hash, vshader, ishader);
      ++vswc->shader.staged;
   }

   if (!ishader->referenced) {
      ishader->referenced = TRUE;
      p_atomic_inc(&vshader->validated);
   }

   *shid = vshader->shid;

   if (mobid != NULL && vshader->buf)
      vmw_swc_mob_relocation(swc, mobid, offset, vshader->buf,
			     0, SVGA_RELOC_READ);
}

static void
vmw_swc_commit(struct svga_winsys_context *swc)
{
   struct vmw_svga_winsys_context *vswc = vmw_svga_winsys_context(swc);

   assert(vswc->command.reserved);
   assert(vswc->command.used + vswc->command.reserved <= vswc->command.size);
   vswc->command.used += vswc->command.reserved;
   vswc->command.reserved = 0;

   assert(vswc->surface.staged <= vswc->surface.reserved);
   assert(vswc->surface.used + vswc->surface.staged <= vswc->surface.size);
   vswc->surface.used += vswc->surface.staged;
   vswc->surface.staged = 0;
   vswc->surface.reserved = 0;

   assert(vswc->shader.staged <= vswc->shader.reserved);
   assert(vswc->shader.used + vswc->shader.staged <= vswc->shader.size);
   vswc->shader.used += vswc->shader.staged;
   vswc->shader.staged = 0;
   vswc->shader.reserved = 0;

   assert(vswc->region.staged <= vswc->region.reserved);
   assert(vswc->region.used + vswc->region.staged <= vswc->region.size);
   vswc->region.used += vswc->region.staged;
   vswc->region.staged = 0;
   vswc->region.reserved = 0;
}


static void
vmw_swc_destroy(struct svga_winsys_context *swc)
{
   struct vmw_svga_winsys_context *vswc = vmw_svga_winsys_context(swc);
   unsigned i;

   for(i = 0; i < vswc->surface.used; ++i) {
      struct vmw_ctx_validate_item *isurf = &vswc->surface.items[i];
      if (isurf->referenced)
         p_atomic_dec(&isurf->vsurf->validated);
      vmw_svga_winsys_surface_reference(&isurf->vsurf, NULL);
   }

   for(i = 0; i < vswc->shader.used; ++i) {
      struct vmw_ctx_validate_item *ishader = &vswc->shader.items[i];
      if (ishader->referenced)
         p_atomic_dec(&ishader->vshader->validated);
      vmw_svga_winsys_shader_reference(&ishader->vshader, NULL);
   }

   util_hash_table_destroy(vswc->hash);
   pb_validate_destroy(vswc->validate);
   vmw_ioctl_context_destroy(vswc->vws, swc->cid);
#ifdef DEBUG
   debug_flush_ctx_destroy(vswc->fctx);
#endif
   FREE(vswc);
}

static unsigned vmw_hash_ptr(void *p)
{
   return (unsigned)(unsigned long)p;
}

static int vmw_ptr_compare(void *key1, void *key2)
{
   return (key1 == key2) ? 0 : 1;
}

struct svga_winsys_context *
vmw_svga_winsys_context_create(struct svga_winsys_screen *sws)
{
   struct vmw_winsys_screen *vws = vmw_winsys_screen(sws);
   struct vmw_svga_winsys_context *vswc;

   vswc = CALLOC_STRUCT(vmw_svga_winsys_context);
   if(!vswc)
      return NULL;

   vswc->base.destroy = vmw_swc_destroy;
   vswc->base.reserve = vmw_swc_reserve;
   vswc->base.surface_relocation = vmw_swc_surface_relocation;
   vswc->base.region_relocation = vmw_swc_region_relocation;
   vswc->base.mob_relocation = vmw_swc_mob_relocation;
   vswc->base.context_relocation = vmw_swc_context_relocation;
   vswc->base.shader_relocation = vmw_swc_shader_relocation;
   vswc->base.commit = vmw_swc_commit;
   vswc->base.flush = vmw_swc_flush;
   vswc->base.surface_map = vmw_svga_winsys_surface_map;
   vswc->base.surface_unmap = vmw_svga_winsys_surface_unmap;

   vswc->base.cid = vmw_ioctl_context_create(vws);
   vswc->base.have_gb_objects = sws->have_gb_objects;

   vswc->vws = vws;

   vswc->command.size = VMW_COMMAND_SIZE;
   vswc->surface.size = VMW_SURFACE_RELOCS;
   vswc->shader.size = VMW_SHADER_RELOCS;
   vswc->region.size = VMW_REGION_RELOCS;

   vswc->validate = pb_validate_create();
   if(!vswc->validate)
      goto out_no_validate;

   vswc->hash = util_hash_table_create(vmw_hash_ptr, vmw_ptr_compare);
   if (!vswc->hash)
      goto out_no_hash;

#ifdef DEBUG
   vswc->fctx = debug_flush_ctx_create(TRUE, VMW_DEBUG_FLUSH_STACK);
#endif

   return &vswc->base;

out_no_hash:
   pb_validate_destroy(vswc->validate);
out_no_validate:
   FREE(vswc);
   return NULL;
}
@


1.6
log
@Merge Mesa 10.2.9
@
text
@@


1.5
log
@Merge Mesa 10.4.3
Tested by matthieu@@ mpi@@ and myself.  landry@@ ran a ports bulk build.
kettenis@@ tracked down the cause of an alignment fault on archs
that require strict eight byte pointer alignment.
@
text
@a51 15
/*
 * A factor applied to the maximum mob memory size to determine
 * the optimial time to preemptively flush the command buffer.
 * The constant is based on some performance trials with SpecViewperf.
 */
#define VMW_MAX_MOB_MEM_FACTOR  2

/*
 * A factor applied to the maximum surface memory size to determine
 * the optimial time to preemptively flush the command buffer.
 * The constant is based on some performance trials with SpecViewperf.
 */
#define VMW_MAX_SURF_MEM_FACTOR 2


d398 1
a398 1
      if (vswc->seen_mobs >= vswc->vws->ioctl.max_mob_memory / VMW_MAX_MOB_MEM_FACTOR)
d460 1
a460 1
      if (vswc->seen_surfaces >= vswc->vws->ioctl.max_surface_memory / VMW_MAX_SURF_MEM_FACTOR)
@


1.4
log
@Merge Mesa 10.2.3
tested by matthieu@@ kettenis@@ mpi@@ brett@@ and myself across a
diverse range of hardware
@
text
@d52 15
d413 1
a413 1
      if (vswc->seen_mobs >= vswc->vws->ioctl.max_mob_memory / 5)
d475 1
a475 1
      if (vswc->seen_surfaces >= vswc->vws->ioctl.max_surface_memory / 5)
@


1.3
log
@Merge Mesa 9.2.0
@
text
@d32 2
d43 1
d47 1
d52 1
a52 1
struct vmw_region_relocation
a53 1
   struct SVGAGuestPtr *where;
d55 1
a55 1
   /* TODO: put offset info inside where */
d57 18
d82 1
d87 1
d98 1
a98 1
      struct vmw_svga_winsys_surface *handles[VMW_SURFACE_RELOCS];
d106 1
a106 1
      struct vmw_region_relocation relocs[VMW_REGION_RELOCS];
d113 8
d124 2
a125 2
    * The amount of GMR that is referred by the commands currently batched
    * in the context.
d127 3
a129 1
   uint32_t seen_regions;
d176 1
a176 1
         struct vmw_region_relocation *reloc = &vswc->region.relocs[i];
d184 10
a193 1
         *reloc->where = ptr;
d211 4
a214 4
      struct vmw_svga_winsys_surface *vsurf =
	 vswc->surface.handles[i];
      p_atomic_dec(&vsurf->validated);
      vmw_svga_winsys_surface_reference(&vswc->surface.handles[i], NULL);
d217 1
d221 5
a225 2
   for(i = 0; i < vswc->region.used + vswc->region.staged; ++i) {
      pb_reference(&vswc->region.relocs[i].buffer, NULL);
d228 3
d236 1
d239 1
d241 1
d265 1
d275 1
d287 1
d293 2
d301 170
d475 1
a478 1
   struct vmw_svga_winsys_context *vswc = vmw_svga_winsys_context(swc);
d481 2
d485 2
a489 2
   assert(vswc->surface.staged < vswc->surface.reserved);

d491 1
d493 1
a493 1
   *where = vsurf->sid;
d495 11
a505 3
   vmw_svga_winsys_surface_reference(&vswc->surface.handles[vswc->surface.used + vswc->surface.staged], vsurf);
   p_atomic_inc(&vsurf->validated);
   ++vswc->surface.staged;
a507 1

d509 5
a513 5
vmw_swc_region_relocation(struct svga_winsys_context *swc,
                          struct SVGAGuestPtr *where,
                          struct svga_winsys_buffer *buffer,
                          uint32 offset,
                          unsigned flags)
d516 6
a521 5
   struct vmw_region_relocation *reloc;
   unsigned translated_flags;
   enum pipe_error ret;
   
   assert(vswc->region.staged < vswc->region.reserved);
d523 16
a538 4
   reloc = &vswc->region.relocs[vswc->region.used + vswc->region.staged];
   reloc->where = where;
   pb_reference(&reloc->buffer, vmw_pb_buffer(buffer));
   reloc->offset = offset;
d540 4
a543 1
   ++vswc->region.staged;
d545 1
a545 5
   translated_flags = vmw_translate_to_pb_flags(flags);
   ret = pb_validate_add_buffer(vswc->validate, reloc->buffer, translated_flags);
   /* TODO: Update pipebuffer to reserve buffers and not fail here */
   assert(ret == PIPE_OK);
   (void)ret;
d547 3
a549 19
   /*
    * Flush preemptively the FIFO commands to keep the GMR working set within
    * the GMR pool size.
    *
    * This is necessary for applications like SPECviewperf that generate huge
    * amounts of immediate vertex data, so that we don't pile up too much of
    * that vertex data neither in the guest nor in the host.
    *
    * Note that in the current implementation if a region is referred twice in
    * a command stream, it will be accounted twice. We could detect repeated
    * regions and count only once, but there is no incentive to do that, since
    * regions are typically short-lived; always referred in a single command;
    * and at the worst we just flush the commands a bit sooner, which for the
    * SVGA virtual device it's not a performance issue since flushing commands
    * to the FIFO won't cause flushing in the host.
    */
   vswc->seen_regions += reloc->buffer->size;
   if(vswc->seen_regions >= VMW_GMR_POOL_SIZE/3)
      vswc->preemptive_flush = TRUE;
a551 1

d568 6
d588 5
a592 2
   for(i = 0; i < vswc->region.used; ++i) {
      pb_reference(&vswc->region.relocs[i].buffer, NULL);
d595 5
a599 3
   for(i = 0; i < vswc->surface.used; ++i) {
      p_atomic_dec(&vswc->surface.handles[i]->validated);
      vmw_svga_winsys_surface_reference(&vswc->surface.handles[i], NULL);
d601 2
d605 3
d611 9
d635 3
d640 2
d644 1
d650 1
d654 10
a663 4
   if(!vswc->validate) {
      FREE(vswc);
      return NULL;
   }
d666 6
@


1.2
log
@Upate to libGL 7.11.2

Tested by jsg@@, matthieu@@ and ajacoutot@@, ok mattieu@@
@
text
@a91 2
   uint32_t last_fence;

a103 3

   boolean throttle_set;
   uint32_t throttle_us;
a135 1
   uint32_t throttle_us;
d154 1
a154 4
      throttle_us = vswc->throttle_set ?
	 vswc->throttle_us : vswc->vws->default_throttle_us;

      if (vswc->command.used)
d157 1
a157 1
			   throttle_us,
d160 1
a160 3
                           &vswc->last_fence);

      fence = vmw_pipe_fence(vswc->last_fence);
d192 3
a194 1
      *pfence = fence;
d296 1
d314 1
a314 1
   vswc->seen_regions += reloc->buffer->base.size;
a395 12
}


void
vmw_svga_context_set_throttling(struct pipe_context *pipe,
				uint32_t throttle_us)
{
   struct svga_winsys_context *swc = svga_winsys_context(pipe);
   struct vmw_svga_winsys_context *vswc = vmw_svga_winsys_context(swc);

   vswc->throttle_us = throttle_us;
   vswc->throttle_set = TRUE;
@


1.1
log
@Initial revision
@
text
@d323 1
a323 1
   if(vswc->seen_regions >= VMW_GMR_POOL_SIZE/2)
@


1.1.1.1
log
@Import Mesa 7.10.3
@
text
@@


1.1.1.2
log
@Import Mesa 9.2.0
@
text
@d92 2
d106 3
d141 1
d160 4
a163 1
      if (vswc->command.used || pfence != NULL)
d166 1
a166 1
			   0,
d169 3
a171 1
                           &fence);
d203 1
a203 3
      vmw_fence_reference(vswc->vws, pfence, fence);

   vmw_fence_reference(vswc->vws, &fence, NULL);
a304 1
   (void)ret;
d322 2
a323 2
   vswc->seen_regions += reloc->buffer->size;
   if(vswc->seen_regions >= VMW_GMR_POOL_SIZE/3)
d404 12
@


1.1.1.3
log
@Import Mesa 10.2.3
@
text
@a31 2
#include "util/u_debug_flush.h"
#include "util/u_hash_table.h"
a40 1
#include "vmw_shader.h"
a43 1
#define VMW_SHADER_RELOCS (1024)
d48 1
a48 1
struct vmw_buffer_relocation
d50 1
d52 1
a52 1
   boolean is_mob;
a53 18

   union {
      struct {
	 struct SVGAGuestPtr *where;
      } region;
      struct {
	 SVGAMobId *id;
	 uint32 *offset_into_mob;
      } mob;
   };
};

struct vmw_ctx_validate_item {
   union {
      struct vmw_svga_winsys_surface *vsurf;
      struct vmw_svga_winsys_shader *vshader;
   };
   boolean referenced;
a60 1
   struct util_hash_table *hash;
a64 1
   struct debug_flush_ctx *fctx;
d75 1
a75 1
      struct vmw_ctx_validate_item items[VMW_SURFACE_RELOCS];
d83 1
a83 1
      struct vmw_buffer_relocation relocs[VMW_REGION_RELOCS];
a89 8
   struct {
      struct vmw_ctx_validate_item items[VMW_SHADER_RELOCS];
      uint32_t size;
      uint32_t used;
      uint32_t staged;
      uint32_t reserved;
   } shader;

d93 2
a94 2
    * The amount of surface, GMR or MOB memory that is referred by the commands
    * currently batched in the context command buffer.
d96 1
a96 3
   uint64_t seen_surfaces;
   uint64_t seen_regions;
   uint64_t seen_mobs;
d143 1
a143 1
         struct vmw_buffer_relocation *reloc = &vswc->region.relocs[i];
d151 1
a151 10
	 if (reloc->is_mob) {
	    if (reloc->mob.id)
	       *reloc->mob.id = ptr.gmrId;
	    if (reloc->mob.offset_into_mob)
	       *reloc->mob.offset_into_mob = ptr.offset;
	    else {
	       assert(ptr.offset == 0);
	    }
	 } else
	    *reloc->region.where = ptr;
d169 4
a172 4
      struct vmw_ctx_validate_item *isurf = &vswc->surface.items[i];
      if (isurf->referenced)
         p_atomic_dec(&isurf->vsurf->validated);
      vmw_svga_winsys_surface_reference(&isurf->vsurf, NULL);
a174 1
   util_hash_table_clear(vswc->hash);
d178 2
a179 5
   for(i = 0; i < vswc->shader.used + vswc->shader.staged; ++i) {
      struct vmw_ctx_validate_item *ishader = &vswc->shader.items[i];
      if (ishader->referenced)
         p_atomic_dec(&ishader->vshader->validated);
      vmw_svga_winsys_shader_reference(&ishader->vshader, NULL);
a181 3
   vswc->shader.used = 0;
   vswc->shader.reserved = 0;

a186 1
   debug_flush_flush(vswc->fctx);
a188 1
   vswc->seen_surfaces = 0;
a189 1
   vswc->seen_mobs = 0;
a212 1
   debug_flush_might_flush(vswc->fctx);
a221 1
      vswc->shader.used + nr_relocs > vswc->shader.size ||
a232 1
   assert(vswc->shader.used + nr_relocs <= vswc->shader.size);
a237 2
   vswc->shader.reserved = nr_relocs;
   vswc->shader.staged = 0;
a243 170
static void
vmw_swc_context_relocation(struct svga_winsys_context *swc,
			   uint32 *cid)
{
   *cid = swc->cid;
}

static boolean
vmw_swc_add_validate_buffer(struct vmw_svga_winsys_context *vswc,
			    struct pb_buffer *pb_buf,
			    unsigned flags)
{
   enum pipe_error ret;
   unsigned translated_flags;

   /*
    * TODO: Update pb_validate to provide a similar functionality
    * (Check buffer already present before adding)
    */
   if (util_hash_table_get(vswc->hash, pb_buf) != pb_buf) {
      translated_flags = vmw_translate_to_pb_flags(flags);
      ret = pb_validate_add_buffer(vswc->validate, pb_buf, translated_flags);
      /* TODO: Update pipebuffer to reserve buffers and not fail here */
      assert(ret == PIPE_OK);
      (void)ret;
      (void)util_hash_table_set(vswc->hash, pb_buf, pb_buf);
      return TRUE;
   }

   return FALSE;
}

static void
vmw_swc_region_relocation(struct svga_winsys_context *swc,
                          struct SVGAGuestPtr *where,
                          struct svga_winsys_buffer *buffer,
                          uint32 offset,
                          unsigned flags)
{
   struct vmw_svga_winsys_context *vswc = vmw_svga_winsys_context(swc);
   struct vmw_buffer_relocation *reloc;

   assert(vswc->region.staged < vswc->region.reserved);

   reloc = &vswc->region.relocs[vswc->region.used + vswc->region.staged];
   reloc->region.where = where;

   /*
    * pb_validate holds a refcount to the buffer, so no need to
    * refcount it again in the relocation.
    */
   reloc->buffer = vmw_pb_buffer(buffer);
   reloc->offset = offset;
   reloc->is_mob = FALSE;
   ++vswc->region.staged;

   if (vmw_swc_add_validate_buffer(vswc, reloc->buffer, flags)) {
      vswc->seen_regions += reloc->buffer->size;
      if(vswc->seen_regions >= VMW_GMR_POOL_SIZE/5)
         vswc->preemptive_flush = TRUE;
   }

#ifdef DEBUG
   if (!(flags & SVGA_RELOC_INTERNAL))
      debug_flush_cb_reference(vswc->fctx, vmw_debug_flush_buf(buffer));
#endif
}

static void
vmw_swc_mob_relocation(struct svga_winsys_context *swc,
		       SVGAMobId *id,
		       uint32 *offset_into_mob,
		       struct svga_winsys_buffer *buffer,
		       uint32 offset,
		       unsigned flags)
{
   struct vmw_svga_winsys_context *vswc = vmw_svga_winsys_context(swc);
   struct vmw_buffer_relocation *reloc;

   assert(vswc->region.staged < vswc->region.reserved);

   reloc = &vswc->region.relocs[vswc->region.used + vswc->region.staged];
   reloc->mob.id = id;
   reloc->mob.offset_into_mob = offset_into_mob;

   /*
    * pb_validate holds a refcount to the buffer, so no need to
    * refcount it again in the relocation.
    */
   reloc->buffer = vmw_pb_buffer(buffer);
   reloc->offset = offset;
   reloc->is_mob = TRUE;
   ++vswc->region.staged;

   if (vmw_swc_add_validate_buffer(vswc, reloc->buffer, flags)) {
      vswc->seen_mobs += reloc->buffer->size;
      /* divide by 5, tested for best performance */
      if (vswc->seen_mobs >= vswc->vws->ioctl.max_mob_memory / 5)
         vswc->preemptive_flush = TRUE;
   }

#ifdef DEBUG
   if (!(flags & SVGA_RELOC_INTERNAL))
      debug_flush_cb_reference(vswc->fctx, vmw_debug_flush_buf(buffer));
#endif
}


/**
 * vmw_swc_surface_clear_reference - Clear referenced info for a surface
 *
 * @@swc:   Pointer to an svga_winsys_context
 * @@vsurf: Pointer to a vmw_svga_winsys_surface, the referenced info of which
 *         we want to clear
 *
 * This is primarily used by a discard surface map to indicate that the
 * surface data is no longer referenced by a draw call, and mapping it
 * should therefore no longer cause a flush.
 */
void
vmw_swc_surface_clear_reference(struct svga_winsys_context *swc,
                                struct vmw_svga_winsys_surface *vsurf)
{
   struct vmw_svga_winsys_context *vswc = vmw_svga_winsys_context(swc);
   struct vmw_ctx_validate_item *isrf =
      util_hash_table_get(vswc->hash, vsurf);

   if (isrf && isrf->referenced) {
      isrf->referenced = FALSE;
      p_atomic_dec(&vsurf->validated);
   }
}

static void
vmw_swc_surface_only_relocation(struct svga_winsys_context *swc,
				uint32 *where,
				struct vmw_svga_winsys_surface *vsurf,
				unsigned flags)
{
   struct vmw_svga_winsys_context *vswc = vmw_svga_winsys_context(swc);
   struct vmw_ctx_validate_item *isrf;

   assert(vswc->surface.staged < vswc->surface.reserved);
   isrf = util_hash_table_get(vswc->hash, vsurf);

   if (isrf == NULL) {
      isrf = &vswc->surface.items[vswc->surface.used + vswc->surface.staged];
      vmw_svga_winsys_surface_reference(&isrf->vsurf, vsurf);
      isrf->referenced = FALSE;
      /*
       * Note that a failure here may just fall back to unhashed behavior
       * and potentially cause unnecessary flushing, so ignore the
       * return code.
       */
      (void) util_hash_table_set(vswc->hash, vsurf, isrf);
      ++vswc->surface.staged;

      vswc->seen_surfaces += vsurf->size;
      /* divide by 5 not well tuned for performance */
      if (vswc->seen_surfaces >= vswc->vws->ioctl.max_surface_memory / 5)
         vswc->preemptive_flush = TRUE;
   }

   if (!(flags & SVGA_RELOC_INTERNAL) && !isrf->referenced) {
      isrf->referenced = TRUE;
      p_atomic_inc(&vsurf->validated);
   }

   *where = vsurf->sid;
}
a247 1
                           uint32 *mobid,
d251 1
a253 2
   assert(swc->have_gb_objects || mobid == NULL);

a255 2
      if (mobid)
         *mobid = SVGA3D_INVALID_ID;
d259 2
a261 1
   vmw_swc_surface_only_relocation(swc, where, vsurf, flags);
d263 1
a263 1
   if (swc->have_gb_objects && vsurf->buf != NULL) {
d265 3
a267 11
      /*
       * Make sure backup buffer ends up fenced.
       */

      pipe_mutex_lock(vsurf->mutex);
      assert(vsurf->buf != NULL);
      
      vmw_swc_mob_relocation(swc, mobid, NULL, (struct svga_winsys_buffer *)
                             vsurf->buf, 0, flags);
      pipe_mutex_unlock(vsurf->mutex);
   }
d270 1
d272 5
a276 5
vmw_swc_shader_relocation(struct svga_winsys_context *swc,
			  uint32 *shid,
			  uint32 *mobid,
			  uint32 *offset,
			  struct svga_winsys_gb_shader *shader)
d279 5
a283 6
   struct vmw_svga_winsys_shader *vshader;
   struct vmw_ctx_validate_item *ishader;
   if(!shader) {
      *shid = SVGA3D_INVALID_ID;
      return;
   }
d285 4
a288 16
   assert(vswc->shader.staged < vswc->shader.reserved);
   vshader = vmw_svga_winsys_shader(shader);
   ishader = util_hash_table_get(vswc->hash, vshader);

   if (ishader == NULL) {
      ishader = &vswc->shader.items[vswc->shader.used + vswc->shader.staged];
      vmw_svga_winsys_shader_reference(&ishader->vshader, vshader);
      ishader->referenced = FALSE;
      /*
       * Note that a failure here may just fall back to unhashed behavior
       * and potentially cause unnecessary flushing, so ignore the
       * return code.
       */
      (void) util_hash_table_set(vswc->hash, vshader, ishader);
      ++vswc->shader.staged;
   }
d290 1
a290 4
   if (!ishader->referenced) {
      ishader->referenced = TRUE;
      p_atomic_inc(&vshader->validated);
   }
d292 5
a296 1
   *shid = vshader->shid;
d298 19
a316 3
   if (mobid != NULL && vshader->buf)
      vmw_swc_mob_relocation(swc, mobid, offset, vshader->buf,
			     0, SVGA_RELOC_READ);
d319 1
a335 6
   assert(vswc->shader.staged <= vswc->shader.reserved);
   assert(vswc->shader.used + vswc->shader.staged <= vswc->shader.size);
   vswc->shader.used += vswc->shader.staged;
   vswc->shader.staged = 0;
   vswc->shader.reserved = 0;

d350 2
a351 5
   for(i = 0; i < vswc->surface.used; ++i) {
      struct vmw_ctx_validate_item *isurf = &vswc->surface.items[i];
      if (isurf->referenced)
         p_atomic_dec(&isurf->vsurf->validated);
      vmw_svga_winsys_surface_reference(&isurf->vsurf, NULL);
d354 3
a356 5
   for(i = 0; i < vswc->shader.used; ++i) {
      struct vmw_ctx_validate_item *ishader = &vswc->shader.items[i];
      if (ishader->referenced)
         p_atomic_dec(&ishader->vshader->validated);
      vmw_svga_winsys_shader_reference(&ishader->vshader, NULL);
a357 2

   util_hash_table_destroy(vswc->hash);
a359 3
#ifdef DEBUG
   debug_flush_ctx_destroy(vswc->fctx);
#endif
a362 9
static unsigned vmw_hash_ptr(void *p)
{
   return (unsigned)(unsigned long)p;
}

static int vmw_ptr_compare(void *key1, void *key2)
{
   return (key1 == key2) ? 0 : 1;
}
a377 3
   vswc->base.mob_relocation = vmw_swc_mob_relocation;
   vswc->base.context_relocation = vmw_swc_context_relocation;
   vswc->base.shader_relocation = vmw_swc_shader_relocation;
a379 2
   vswc->base.surface_map = vmw_svga_winsys_surface_map;
   vswc->base.surface_unmap = vmw_svga_winsys_surface_unmap;
a381 1
   vswc->base.have_gb_objects = sws->have_gb_objects;
a386 1
   vswc->shader.size = VMW_SHADER_RELOCS;
d390 4
a393 10
   if(!vswc->validate)
      goto out_no_validate;

   vswc->hash = util_hash_table_create(vmw_hash_ptr, vmw_ptr_compare);
   if (!vswc->hash)
      goto out_no_hash;

#ifdef DEBUG
   vswc->fctx = debug_flush_ctx_create(TRUE, VMW_DEBUG_FLUSH_STACK);
#endif
a395 6

out_no_hash:
   pb_validate_destroy(vswc->validate);
out_no_validate:
   FREE(vswc);
   return NULL;
@


1.1.1.4
log
@Import Mesa 10.4.3
@
text
@a51 15
/*
 * A factor applied to the maximum mob memory size to determine
 * the optimial time to preemptively flush the command buffer.
 * The constant is based on some performance trials with SpecViewperf.
 */
#define VMW_MAX_MOB_MEM_FACTOR  2

/*
 * A factor applied to the maximum surface memory size to determine
 * the optimial time to preemptively flush the command buffer.
 * The constant is based on some performance trials with SpecViewperf.
 */
#define VMW_MAX_SURF_MEM_FACTOR 2


d398 1
a398 1
      if (vswc->seen_mobs >= vswc->vws->ioctl.max_mob_memory / VMW_MAX_MOB_MEM_FACTOR)
d460 1
a460 1
      if (vswc->seen_surfaces >= vswc->vws->ioctl.max_surface_memory / VMW_MAX_SURF_MEM_FACTOR)
@


1.1.1.5
log
@Import Mesa 10.2.9
@
text
@d52 15
d413 1
a413 1
      if (vswc->seen_mobs >= vswc->vws->ioctl.max_mob_memory / 5)
d475 1
a475 1
      if (vswc->seen_surfaces >= vswc->vws->ioctl.max_surface_memory / 5)
@


