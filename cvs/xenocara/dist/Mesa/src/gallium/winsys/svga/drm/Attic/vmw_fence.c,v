head	1.2;
access;
symbols
	OPENBSD_5_8:1.1.1.3.0.6
	OPENBSD_5_8_BASE:1.1.1.3
	OPENBSD_5_7:1.1.1.3.0.4
	OPENBSD_5_7_BASE:1.1.1.3
	v10_2_9:1.1.1.3
	v10_4_3:1.1.1.3
	v10_2_7:1.1.1.3
	OPENBSD_5_6:1.1.1.3.0.2
	OPENBSD_5_6_BASE:1.1.1.3
	v10_2_3:1.1.1.3
	OPENBSD_5_5:1.1.1.2.0.2
	OPENBSD_5_5_BASE:1.1.1.2
	v9_2_5:1.1.1.2
	v9_2_3:1.1.1.2
	v9_2_2:1.1.1.2
	v9_2_1:1.1.1.2
	v9_2_0:1.1.1.2
	OPENBSD_5_4:1.1.1.1.0.8
	OPENBSD_5_4_BASE:1.1.1.1
	OPENBSD_5_3:1.1.1.1.0.6
	OPENBSD_5_3_BASE:1.1.1.1
	OPENBSD_5_2:1.1.1.1.0.4
	OPENBSD_5_2_BASE:1.1.1.1
	OPENBSD_5_1_BASE:1.1.1.1
	OPENBSD_5_1:1.1.1.1.0.2
	v7_10_3:1.1.1.1
	mesa:1.1.1;
locks; strict;
comment	@ * @;


1.2
date	2015.12.23.05.17.42;	author jsg;	state dead;
branches;
next	1.1;
commitid	TnlogFl9nOv2eaRf;

1.1
date	2011.10.23.13.29.30;	author matthieu;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2011.10.23.13.29.30;	author matthieu;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2013.09.05.13.13.55;	author jsg;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2014.07.09.20.34.24;	author jsg;	state Exp;
branches;
next	;
commitid	3JhLfwcuBALP0ZR7;


desc
@@


1.2
log
@remove the now unused Mesa 10.2.9 code
@
text
@/**********************************************************
 * Copyright 2009 VMware, Inc.  All rights reserved.
 *
 * Permission is hereby granted, free of charge, to any person
 * obtaining a copy of this software and associated documentation
 * files (the "Software"), to deal in the Software without
 * restriction, including without limitation the rights to use, copy,
 * modify, merge, publish, distribute, sublicense, and/or sell copies
 * of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be
 * included in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 *
 **********************************************************/


#include "util/u_memory.h"
#include "pipebuffer/pb_buffer_fenced.h"

#include "vmw_screen.h"
#include "vmw_fence.h"



struct vmw_fence_ops 
{
   struct pb_fence_ops base;

   struct vmw_winsys_screen *vws;
};


static INLINE struct vmw_fence_ops *
vmw_fence_ops(struct pb_fence_ops *ops)
{
   assert(ops);
   return (struct vmw_fence_ops *)ops;
}


static void
vmw_fence_ops_fence_reference(struct pb_fence_ops *ops,
                              struct pipe_fence_handle **ptr,
                              struct pipe_fence_handle *fence)
{
   *ptr = fence;
}


static int
vmw_fence_ops_fence_signalled(struct pb_fence_ops *ops,
                              struct pipe_fence_handle *fence,
                              unsigned flag)
{
   struct vmw_winsys_screen *vws = vmw_fence_ops(ops)->vws;
   (void)flag;
   return vmw_ioctl_fence_signalled(vws, vmw_fence(fence));
}


static int
vmw_fence_ops_fence_finish(struct pb_fence_ops *ops,
                           struct pipe_fence_handle *fence,
                           unsigned flag)
{
   struct vmw_winsys_screen *vws = vmw_fence_ops(ops)->vws;
   (void)flag;
   return vmw_ioctl_fence_finish(vws, vmw_fence(fence));
}


static void
vmw_fence_ops_destroy(struct pb_fence_ops *ops)
{
   FREE(ops);
}


struct pb_fence_ops *
vmw_fence_ops_create(struct vmw_winsys_screen *vws) 
{
   struct vmw_fence_ops *ops;

   ops = CALLOC_STRUCT(vmw_fence_ops);
   if(!ops)
      return NULL;

   ops->base.destroy = &vmw_fence_ops_destroy;
   ops->base.fence_reference = &vmw_fence_ops_fence_reference;
   ops->base.fence_signalled = &vmw_fence_ops_fence_signalled;
   ops->base.fence_finish = &vmw_fence_ops_fence_finish;

   ops->vws = vws;

   return &ops->base;
}


@


1.1
log
@Initial revision
@
text
@@


1.1.1.1
log
@Import Mesa 7.10.3
@
text
@@


1.1.1.2
log
@Import Mesa 9.2.0
@
text
@d2 1
a2 1
 * Copyright 2009-2011 VMware, Inc.  All rights reserved.
d25 1
a25 11
/*
 * TODO:
 *
 * Fencing is currently a bit inefficient, since we need to call the
 * kernel do determine a fence object signaled status if the fence is not
 * signaled. This can be greatly improved upon by using the fact that the
 * execbuf ioctl returns the last signaled fence seqno, as does the
 * fence signaled ioctl. We should set up a ring of fence objects and
 * walk through them checking for signaled status each time we receive a
 * new passed fence seqno.
 */
a27 2
#include "util/u_atomic.h"

d33 2
a41 43
struct vmw_fence
{
   int32_t refcount;
   uint32_t handle;
   uint32_t mask;
   int32_t signalled;
};

/**
 * vmw_fence - return the vmw_fence object identified by a
 * struct pipe_fence_handle *
 *
 * @@fence: The opaque pipe fence handle.
 */
static INLINE struct vmw_fence *
vmw_fence(struct pipe_fence_handle *fence)
{
   return (struct vmw_fence *) fence;
}

/**
 * vmw_fence_create - Create a user-space fence object.
 *
 * @@handle: Handle identifying the kernel fence object.
 * @@mask: Mask of flags that this fence object may signal.
 *
 * Returns NULL on failure.
 */
struct pipe_fence_handle *
vmw_fence_create(uint32_t handle, uint32_t mask)
{
   struct vmw_fence *fence = CALLOC_STRUCT(vmw_fence);

   if (!fence)
      return NULL;

   p_atomic_set(&fence->refcount, 1);
   fence->handle = handle;
   fence->mask = mask;
   p_atomic_set(&fence->signalled, 0);

   return (struct pipe_fence_handle *) fence;
}
a42 7
/**
 * vmw_fence_ops - Return the vmw_fence_ops structure backing a
 * struct pb_fence_ops pointer.
 *
 * @@ops: Pointer to a struct pb_fence_ops.
 *
 */
a50 127

/**
 * vmw_fence_reference - Reference / unreference a vmw fence object.
 *
 * @@vws: Pointer to the winsys screen.
 * @@ptr: Pointer to reference transfer destination.
 * @@fence: Pointer to object to reference. May be NULL.
 */
void
vmw_fence_reference(struct vmw_winsys_screen *vws,
		    struct pipe_fence_handle **ptr,
		    struct pipe_fence_handle *fence)
{
   if (*ptr) {
      struct vmw_fence *vfence = vmw_fence(*ptr);

      if (p_atomic_dec_zero(&vfence->refcount)) {
	 vmw_ioctl_fence_unref(vws, vfence->handle);
	 FREE(vfence);
      }
   }

   if (fence) {
      struct vmw_fence *vfence = vmw_fence(fence);

      p_atomic_inc(&vfence->refcount);
   }

   *ptr = fence;
}


/**
 * vmw_fence_signalled - Check whether a fence object is signalled.
 *
 * @@vws: Pointer to the winsys screen.
 * @@fence: Handle to the fence object.
 * @@flag: Fence flags to check. If the fence object can't signal
 * a flag, it is assumed to be already signaled.
 *
 * Returns 0 if the fence object was signaled, nonzero otherwise.
 */
int
vmw_fence_signalled(struct vmw_winsys_screen *vws,
		   struct pipe_fence_handle *fence,
		   unsigned flag)
{
   struct vmw_fence *vfence;
   int32_t vflags = SVGA_FENCE_FLAG_EXEC;
   int ret;
   uint32_t old;

   if (!fence)
      return 0;

   vfence = vmw_fence(fence);
   old = p_atomic_read(&vfence->signalled);

   vflags &= ~vfence->mask;

   if ((old & vflags) == vflags)
      return 0;

   ret = vmw_ioctl_fence_signalled(vws, vfence->handle, vflags);

   if (ret == 0) {
      int32_t prev = old;

      do {
	 old = prev;
	 prev = p_atomic_cmpxchg(&vfence->signalled, old, old | vflags);
      } while (prev != old);
   }

   return ret;
}

/**
 * vmw_fence_finish - Wait for a fence object to signal.
 *
 * @@vws: Pointer to the winsys screen.
 * @@fence: Handle to the fence object.
 * @@flag: Fence flags to wait for. If the fence object can't signal
 * a flag, it is assumed to be already signaled.
 *
 * Returns 0 if the wait succeeded. Nonzero otherwise.
 */
int
vmw_fence_finish(struct vmw_winsys_screen *vws,
		 struct pipe_fence_handle *fence,
		 unsigned flag)
{
   struct vmw_fence *vfence;
   int32_t vflags = SVGA_FENCE_FLAG_EXEC;
   int ret;
   uint32_t old;

   if (!fence)
      return 0;

   vfence = vmw_fence(fence);
   old = p_atomic_read(&vfence->signalled);
   vflags &= ~vfence->mask;

   if ((old & vflags) == vflags)
      return 0;

   ret = vmw_ioctl_fence_finish(vws, vfence->handle, vflags);

   if (ret == 0) {
      int32_t prev = old;

      do {
	 old = prev;
	 prev = p_atomic_cmpxchg(&vfence->signalled, old, old | vflags);
      } while (prev != old);
   }

   return ret;
}


/**
 * vmw_fence_ops_fence_reference - wrapper for the pb_fence_ops api.
 *
 * wrapper around vmw_fence_reference.
 */
d56 2
a57 1
   struct vmw_winsys_screen *vws = vmw_fence_ops(ops)->vws;
a58 2
   vmw_fence_reference(vws, ptr, fence);
}
a59 5
/**
 * vmw_fence_ops_fence_signalled - wrapper for the pb_fence_ops api.
 *
 * wrapper around vmw_fence_signalled.
 */
d66 2
a67 2

   return vmw_fence_signalled(vws, fence, flag);
a70 5
/**
 * vmw_fence_ops_fence_finish - wrapper for the pb_fence_ops api.
 *
 * wrapper around vmw_fence_finish.
 */
d77 2
a78 2

   return vmw_fence_finish(vws, fence, flag);
a81 7
/**
 * vmw_fence_ops_destroy - Destroy a pb_fence_ops function table.
 *
 * @@ops: The function table to destroy.
 *
 * Part of the pb_fence_ops api.
 */
a88 10
/**
 * vmw_fence_ops_create - Create a pb_fence_ops function table.
 *
 * @@vws: Pointer to a struct vmw_winsys_screen.
 *
 * Returns a pointer to a pb_fence_ops function table to interface
 * with pipe_buffer. This function is typically called on driver setup.
 *
 * Returns NULL on failure.
 */
@


1.1.1.3
log
@Import Mesa 10.2.3
@
text
@d25 12
a38 2
#include "util/u_double_list.h"
#include "os/os_thread.h"
a46 3
   /*
    * Immutable members.
    */
d48 1
a49 9

   pipe_mutex mutex;

   /*
    * Protected by mutex;
    */
   struct list_head not_signaled;
   uint32_t last_signaled;
   uint32_t last_emitted;
a53 1
   struct list_head ops_list;
a57 1
   uint32_t seqno;
a60 96
 * vmw_fence_seq_is_signaled - Check whether a fence seqno is
 * signaled.
 *
 * @@ops: Pointer to a struct pb_fence_ops.
 *
 */
static INLINE boolean
vmw_fence_seq_is_signaled(uint32_t seq, uint32_t last, uint32_t cur)
{
   return (cur - last <= cur - seq);
}


/**
 * vmw_fence_ops - Return the vmw_fence_ops structure backing a
 * struct pb_fence_ops pointer.
 *
 * @@ops: Pointer to a struct pb_fence_ops.
 *
 */
static INLINE struct vmw_fence_ops *
vmw_fence_ops(struct pb_fence_ops *ops)
{
   assert(ops);
   return (struct vmw_fence_ops *)ops;
}


/**
 * vmw_fences_release - Release all fences from the not_signaled
 * list.
 *
 * @@ops: Pointer to a struct vmw_fence_ops.
 *
 */
static void
vmw_fences_release(struct vmw_fence_ops *ops)
{
   struct vmw_fence *fence, *n;

   pipe_mutex_lock(ops->mutex);
   LIST_FOR_EACH_ENTRY_SAFE(fence, n, &ops->not_signaled, ops_list)
      LIST_DELINIT(&fence->ops_list);
   pipe_mutex_unlock(ops->mutex);
}

/**
 * vmw_fences_signal - Traverse the not_signaled list and try to
 * signal unsignaled fences.
 *
 * @@ops: Pointer to a struct pb_fence_ops.
 * @@signaled: Seqno that has signaled.
 * @@emitted: Last seqno emitted by the kernel.
 * @@has_emitted: Whether we provide the emitted value.
 *
 */
void
vmw_fences_signal(struct pb_fence_ops *fence_ops,
                  uint32_t signaled,
                  uint32_t emitted,
                  boolean has_emitted)
{
   struct vmw_fence_ops *ops = NULL;
   struct vmw_fence *fence, *n;

   if (fence_ops == NULL)
      return;

   ops = vmw_fence_ops(fence_ops);
   pipe_mutex_lock(ops->mutex);

   if (!has_emitted) {
      emitted = ops->last_emitted;
      if (emitted - signaled > (1 << 30))
	emitted = signaled;
   }

   if (signaled == ops->last_signaled && emitted == ops->last_emitted)
      goto out_unlock;

   LIST_FOR_EACH_ENTRY_SAFE(fence, n, &ops->not_signaled, ops_list) {
      if (!vmw_fence_seq_is_signaled(fence->seqno, signaled, emitted))
         break;

      p_atomic_set(&fence->signalled, 1);
      LIST_DELINIT(&fence->ops_list);
   }
   ops->last_signaled = signaled;
   ops->last_emitted = emitted;

out_unlock:
   pipe_mutex_unlock(ops->mutex);
}


/**
a71 1

a74 1
 * @@fence_ops: The fence_ops manager to register with.
d81 1
a81 2
vmw_fence_create(struct pb_fence_ops *fence_ops, uint32_t handle,
                 uint32_t seqno, uint32_t mask)
a83 1
   struct vmw_fence_ops *ops = vmw_fence_ops(fence_ops);
a90 1
   fence->seqno = seqno;
a91 1
   pipe_mutex_lock(ops->mutex);
d93 2
a94 7
   if (vmw_fence_seq_is_signaled(seqno, ops->last_signaled, seqno)) {
      p_atomic_set(&fence->signalled, 1);
      LIST_INITHEAD(&fence->ops_list);
   } else {
      p_atomic_set(&fence->signalled, 0);
      LIST_ADDTAIL(&fence->ops_list, &ops->not_signaled);
   }
d96 13
a108 1
   pipe_mutex_unlock(ops->mutex);
a109 2
   return (struct pipe_fence_handle *) fence;
}
a127 2
         struct vmw_fence_ops *ops = vmw_fence_ops(vws->fence_ops);

a128 5

         pipe_mutex_lock(ops->mutex);
         LIST_DELINIT(&vfence->ops_list);
         pipe_mutex_unlock(ops->mutex);

a173 6
   /*
    * Currently we update signaled fences on each execbuf call.
    * That should really be sufficient, and we can avoid
    * a lot of kernel calls this way.
    */
#if 1
d176 9
a184 2
   if (ret == 0)
      p_atomic_set(&vfence->signalled, 1);
a185 4
#else
   (void) ret;
   return -1;
#endif
a289 1
   vmw_fences_release(vmw_fence_ops(ops));
a312 2
   pipe_mutex_init(ops->mutex);
   LIST_INITHEAD(&ops->not_signaled);
d322 2
@


