head	1.8;
access;
symbols
	OPENBSD_5_8:1.7.0.4
	OPENBSD_5_8_BASE:1.7
	OPENBSD_5_7:1.7.0.2
	OPENBSD_5_7_BASE:1.7
	v10_2_9:1.1.1.5
	v10_4_3:1.1.1.4
	v10_2_7:1.1.1.3
	OPENBSD_5_6:1.5.0.2
	OPENBSD_5_6_BASE:1.5
	v10_2_3:1.1.1.3
	OPENBSD_5_5:1.4.0.2
	OPENBSD_5_5_BASE:1.4
	v9_2_5:1.1.1.2
	v9_2_3:1.1.1.2
	v9_2_2:1.1.1.2
	v9_2_1:1.1.1.2
	v9_2_0:1.1.1.2
	OPENBSD_5_4:1.3.0.4
	OPENBSD_5_4_BASE:1.3
	OPENBSD_5_3:1.3.0.2
	OPENBSD_5_3_BASE:1.3
	OPENBSD_5_2:1.2.0.4
	OPENBSD_5_2_BASE:1.2
	OPENBSD_5_1_BASE:1.2
	OPENBSD_5_1:1.2.0.2
	v7_10_3:1.1.1.1
	mesa:1.1.1
	OPENBSD_5_0:1.1.0.6
	OPENBSD_5_0_BASE:1.1
	OPENBSD_4_9:1.1.0.2
	OPENBSD_4_9_BASE:1.1
	OPENBSD_4_8:1.1.0.4
	OPENBSD_4_8_BASE:1.1;
locks; strict;
comment	@ * @;


1.8
date	2015.12.23.05.17.27;	author jsg;	state dead;
branches;
next	1.7;
commitid	TnlogFl9nOv2eaRf;

1.7
date	2015.02.20.23.09.51;	author jsg;	state Exp;
branches;
next	1.6;
commitid	4ry2gvZGMXkCUD2n;

1.6
date	2015.01.25.14.41.15;	author jsg;	state Exp;
branches;
next	1.5;
commitid	mcxB0JvoI9gTDYXU;

1.5
date	2014.07.09.21.08.52;	author jsg;	state Exp;
branches;
next	1.4;
commitid	WPD6rgPryPkvXOr9;

1.4
date	2013.09.05.13.59.40;	author jsg;	state Exp;
branches;
next	1.3;

1.3
date	2012.08.17.13.58.03;	author mpi;	state Exp;
branches;
next	1.2;

1.2
date	2011.10.23.13.37.32;	author matthieu;	state Exp;
branches;
next	1.1;

1.1
date	2010.05.22.20.06.04;	author matthieu;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2011.10.23.13.29.25;	author matthieu;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2013.09.05.13.10.20;	author jsg;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2014.07.09.20.33.44;	author jsg;	state Exp;
branches;
next	1.1.1.4;
commitid	3JhLfwcuBALP0ZR7;

1.1.1.4
date	2015.01.25.14.06.25;	author jsg;	state Exp;
branches;
next	1.1.1.5;
commitid	ce2W5rH5aF7VS9gi;

1.1.1.5
date	2015.02.20.22.43.33;	author jsg;	state Exp;
branches;
next	;
commitid	F54a1i0WXHMxq7kE;


desc
@@


1.8
log
@remove the now unused Mesa 10.2.9 code
@
text
@/**************************************************************************
 *
 * Copyright 2007-2008 VMware, Inc.
 * All Rights Reserved.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sub license, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice (including the
 * next paragraph) shall be included in all copies or substantial portions
 * of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
 * IN NO EVENT SHALL VMWARE AND/OR ITS SUPPLIERS BE LIABLE FOR
 * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *
 **************************************************************************/

/**
 * \file
 * Buffer cache.
 * 
 * \author Jose Fonseca <jfonseca-at-vmware-dot-com>
 * \author Thomas Hellstr√∂m <thellstom-at-vmware-dot-com>
 */


#include "pipe/p_compiler.h"
#include "util/u_debug.h"
#include "os/os_thread.h"
#include "util/u_memory.h"
#include "util/u_double_list.h"
#include "util/u_time.h"

#include "pb_buffer.h"
#include "pb_bufmgr.h"


/**
 * Convenience macro (type safe).
 */
#define SUPER(__derived) (&(__derived)->base)


struct pb_cache_manager;


/**
 * Wrapper around a pipe buffer which adds delayed destruction.
 */
struct pb_cache_buffer
{
   struct pb_buffer base;
   
   struct pb_buffer *buffer;
   struct pb_cache_manager *mgr;

   /** Caching time interval */
   int64_t start, end;

   struct list_head head;
};


struct pb_cache_manager
{
   struct pb_manager base;

   struct pb_manager *provider;
   unsigned usecs;
   
   pipe_mutex mutex;
   
   struct list_head delayed;
   pb_size numDelayed;
   float size_factor;
   unsigned bypass_usage;
};


static INLINE struct pb_cache_buffer *
pb_cache_buffer(struct pb_buffer *buf)
{
   assert(buf);
   return (struct pb_cache_buffer *)buf;
}


static INLINE struct pb_cache_manager *
pb_cache_manager(struct pb_manager *mgr)
{
   assert(mgr);
   return (struct pb_cache_manager *)mgr;
}


/**
 * Actually destroy the buffer.
 */
static INLINE void
_pb_cache_buffer_destroy(struct pb_cache_buffer *buf)
{
   struct pb_cache_manager *mgr = buf->mgr;

   LIST_DEL(&buf->head);
   assert(mgr->numDelayed);
   --mgr->numDelayed;
   assert(!pipe_is_referenced(&buf->base.reference));
   pb_reference(&buf->buffer, NULL);
   FREE(buf);
}


/**
 * Free as many cache buffers from the list head as possible. 
 */
static void
_pb_cache_buffer_list_check_free(struct pb_cache_manager *mgr)
{
   struct list_head *curr, *next;
   struct pb_cache_buffer *buf;
   int64_t now;
   
   now = os_time_get();
   
   curr = mgr->delayed.next;
   next = curr->next;
   while(curr != &mgr->delayed) {
      buf = LIST_ENTRY(struct pb_cache_buffer, curr, head);

      if(!os_time_timeout(buf->start, buf->end, now))
	 break;
	 
      _pb_cache_buffer_destroy(buf);

      curr = next; 
      next = curr->next;
   }
}


static void
pb_cache_buffer_destroy(struct pb_buffer *_buf)
{
   struct pb_cache_buffer *buf = pb_cache_buffer(_buf);   
   struct pb_cache_manager *mgr = buf->mgr;

   pipe_mutex_lock(mgr->mutex);
   assert(!pipe_is_referenced(&buf->base.reference));
   
   _pb_cache_buffer_list_check_free(mgr);
   
   buf->start = os_time_get();
   buf->end = buf->start + mgr->usecs;
   LIST_ADDTAIL(&buf->head, &mgr->delayed);
   ++mgr->numDelayed;
   pipe_mutex_unlock(mgr->mutex);
}


static void *
pb_cache_buffer_map(struct pb_buffer *_buf, 
		    unsigned flags, void *flush_ctx)
{
   struct pb_cache_buffer *buf = pb_cache_buffer(_buf);   
   return pb_map(buf->buffer, flags, flush_ctx);
}


static void
pb_cache_buffer_unmap(struct pb_buffer *_buf)
{
   struct pb_cache_buffer *buf = pb_cache_buffer(_buf);   
   pb_unmap(buf->buffer);
}


static enum pipe_error 
pb_cache_buffer_validate(struct pb_buffer *_buf, 
                         struct pb_validate *vl,
                         unsigned flags)
{
   struct pb_cache_buffer *buf = pb_cache_buffer(_buf);
   return pb_validate(buf->buffer, vl, flags);
}


static void
pb_cache_buffer_fence(struct pb_buffer *_buf, 
                      struct pipe_fence_handle *fence)
{
   struct pb_cache_buffer *buf = pb_cache_buffer(_buf);
   pb_fence(buf->buffer, fence);
}


static void
pb_cache_buffer_get_base_buffer(struct pb_buffer *_buf,
                              struct pb_buffer **base_buf,
                              pb_size *offset)
{
   struct pb_cache_buffer *buf = pb_cache_buffer(_buf);
   pb_get_base_buffer(buf->buffer, base_buf, offset);
}


const struct pb_vtbl 
pb_cache_buffer_vtbl = {
      pb_cache_buffer_destroy,
      pb_cache_buffer_map,
      pb_cache_buffer_unmap,
      pb_cache_buffer_validate,
      pb_cache_buffer_fence,
      pb_cache_buffer_get_base_buffer
};


static INLINE int
pb_cache_is_buffer_compat(struct pb_cache_buffer *buf,  
                          pb_size size,
                          const struct pb_desc *desc)
{
   if (desc->usage & buf->mgr->bypass_usage)
      return 0;

   if(buf->base.size < size)
      return 0;

   /* be lenient with size */
   if(buf->base.size > (unsigned) (buf->mgr->size_factor * size))
      return 0;
   
   if(!pb_check_alignment(desc->alignment, buf->base.alignment))
      return 0;
   
   if(!pb_check_usage(desc->usage, buf->base.usage))
      return 0;

   if (buf->mgr->provider->is_buffer_busy) {
      if (buf->mgr->provider->is_buffer_busy(buf->mgr->provider, buf->buffer))
         return -1;
   } else {
      void *ptr = pb_map(buf->buffer, PB_USAGE_DONTBLOCK, NULL);

      if (!ptr)
         return -1;

      pb_unmap(buf->buffer);
   }

   return 1;
}


static struct pb_buffer *
pb_cache_manager_create_buffer(struct pb_manager *_mgr, 
                               pb_size size,
                               const struct pb_desc *desc)
{
   struct pb_cache_manager *mgr = pb_cache_manager(_mgr);
   struct pb_cache_buffer *buf;
   struct pb_cache_buffer *curr_buf;
   struct list_head *curr, *next;
   int64_t now;
   int ret = 0;

   pipe_mutex_lock(mgr->mutex);

   buf = NULL;
   curr = mgr->delayed.next;
   next = curr->next;
   
   /* search in the expired buffers, freeing them in the process */
   now = os_time_get();
   while(curr != &mgr->delayed) {
      curr_buf = LIST_ENTRY(struct pb_cache_buffer, curr, head);
      if(!buf && (ret = pb_cache_is_buffer_compat(curr_buf, size, desc) > 0))
         buf = curr_buf;
      else if(os_time_timeout(curr_buf->start, curr_buf->end, now))
         _pb_cache_buffer_destroy(curr_buf);
      else
         /* This buffer (and all hereafter) are still hot in cache */
         break;
      if (ret == -1)
         break;
      curr = next; 
      next = curr->next;
   }

   /* keep searching in the hot buffers */
   if(!buf && ret != -1) {
      while(curr != &mgr->delayed) {
         curr_buf = LIST_ENTRY(struct pb_cache_buffer, curr, head);
         ret = pb_cache_is_buffer_compat(curr_buf, size, desc);
         if (ret > 0) {
            buf = curr_buf;
            break;
         }
         if (ret == -1)
            break;
         /* no need to check the timeout here */
         curr = next;
         next = curr->next;
      }
   }
   
   if(buf) {
      LIST_DEL(&buf->head);
      --mgr->numDelayed;
      pipe_mutex_unlock(mgr->mutex);
      /* Increase refcount */
      pipe_reference_init(&buf->base.reference, 1);
      return &buf->base;
   }
   
   pipe_mutex_unlock(mgr->mutex);

   buf = CALLOC_STRUCT(pb_cache_buffer);
   if(!buf)
      return NULL;
   
   buf->buffer = mgr->provider->create_buffer(mgr->provider, size, desc);

   /* Empty the cache and try again. */
   if (!buf->buffer) {
      mgr->base.flush(&mgr->base);
      buf->buffer = mgr->provider->create_buffer(mgr->provider, size, desc);
   }

   if(!buf->buffer) {
      FREE(buf);
      return NULL;
   }
   
   assert(pipe_is_referenced(&buf->buffer->reference));
   assert(pb_check_alignment(desc->alignment, buf->buffer->alignment));
   assert(pb_check_usage(desc->usage & ~mgr->bypass_usage, buf->buffer->usage));
   assert(buf->buffer->size >= size);
   
   pipe_reference_init(&buf->base.reference, 1);
   buf->base.alignment = buf->buffer->alignment;
   buf->base.usage = buf->buffer->usage;
   buf->base.size = buf->buffer->size;
   
   buf->base.vtbl = &pb_cache_buffer_vtbl;
   buf->mgr = mgr;
   
   return &buf->base;
}


static void
pb_cache_manager_flush(struct pb_manager *_mgr)
{
   struct pb_cache_manager *mgr = pb_cache_manager(_mgr);
   struct list_head *curr, *next;
   struct pb_cache_buffer *buf;

   pipe_mutex_lock(mgr->mutex);
   curr = mgr->delayed.next;
   next = curr->next;
   while(curr != &mgr->delayed) {
      buf = LIST_ENTRY(struct pb_cache_buffer, curr, head);
      _pb_cache_buffer_destroy(buf);
      curr = next; 
      next = curr->next;
   }
   pipe_mutex_unlock(mgr->mutex);
   
   assert(mgr->provider->flush);
   if(mgr->provider->flush)
      mgr->provider->flush(mgr->provider);
}


static void
pb_cache_manager_destroy(struct pb_manager *mgr)
{
   pb_cache_manager_flush(mgr);
   FREE(mgr);
}

/**
 * Create a caching buffer manager
 *
 * @@param provider The buffer manager to which cache miss buffer requests
 * should be redirected.
 * @@param usecs Unused buffers may be released from the cache after this
 * time
 * @@param size_factor Declare buffers that are size_factor times bigger than
 * the requested size as cache hits.
 * @@param bypass_usage Bitmask. If (requested usage & bypass_usage) != 0,
 * buffer allocation requests are redirected to the provider.
 */
struct pb_manager *
pb_cache_manager_create(struct pb_manager *provider, 
                        unsigned usecs,
                        float size_factor,
                        unsigned bypass_usage)
{
   struct pb_cache_manager *mgr;

   if(!provider)
      return NULL;
   
   mgr = CALLOC_STRUCT(pb_cache_manager);
   if (!mgr)
      return NULL;

   mgr->base.destroy = pb_cache_manager_destroy;
   mgr->base.create_buffer = pb_cache_manager_create_buffer;
   mgr->base.flush = pb_cache_manager_flush;
   mgr->provider = provider;
   mgr->usecs = usecs;
   mgr->size_factor = size_factor;
   mgr->bypass_usage = bypass_usage;
   LIST_INITHEAD(&mgr->delayed);
   mgr->numDelayed = 0;
   pipe_mutex_init(mgr->mutex);
      
   return &mgr->base;
}
@


1.7
log
@Merge Mesa 10.2.9
@
text
@@


1.6
log
@Merge Mesa 10.4.3
Tested by matthieu@@ mpi@@ and myself.  landry@@ ran a ports bulk build.
kettenis@@ tracked down the cause of an alignment fault on archs
that require strict eight byte pointer alignment.
@
text
@a86 1
   uint64_t cache_size, max_cache_size;
a116 1
   mgr->cache_size -= buf->base.size;
d161 1
a161 9

   /* Directly release any buffer that exceeds the limit. */
   if (mgr->cache_size + buf->base.size > mgr->max_cache_size) {
      pb_reference(&buf->buffer, NULL);
      FREE(buf);
      pipe_mutex_unlock(mgr->mutex);
      return;
   }

a165 1
   mgr->cache_size += buf->base.size;
a316 1
      mgr->cache_size -= buf->base.size;
a402 2
 * @@param maximum_cache_size  Maximum size of all unused buffers the cache can
 * hold.
d408 1
a408 2
                        unsigned bypass_usage,
                        uint64_t maximum_cache_size)
a427 1
   mgr->max_cache_size = maximum_cache_size;
@


1.5
log
@Merge Mesa 10.2.3
tested by matthieu@@ kettenis@@ mpi@@ brett@@ and myself across a
diverse range of hardware
@
text
@d87 1
d118 1
d163 9
a171 1
   
d176 1
d328 1
d415 2
d422 2
a423 1
                        unsigned bypass_usage)
d443 1
@


1.4
log
@Merge Mesa 9.2.0
@
text
@d3 1
a3 1
 * Copyright 2007-2008 Tungsten Graphics, Inc., Cedar Park, Texas.
d21 1
a21 1
 * IN NO EVENT SHALL TUNGSTEN GRAPHICS AND/OR ITS SUPPLIERS BE LIABLE FOR
d32 2
a33 2
 * \author Jose Fonseca <jrfonseca-at-tungstengraphics-dot-com>
 * \author Thomas Hellstr√∂m <thomas-at-tungstengraphics-dot-com>
d85 2
d232 3
d239 1
a239 1
   if(buf->base.size >= 2*size)
d346 1
a346 1
   assert(pb_check_usage(desc->usage, buf->buffer->usage));
d392 12
a403 1

d406 3
a408 1
                     	unsigned usecs) 
d424 2
@


1.3
log
@Upate to libGL 7.11.2

Tested by jsg@@, matthieu@@ and ajacoutot@@, ok mattieu@@
@
text
@d115 1
a115 1
   assert(!pipe_is_referenced(&buf->base.base.reference));
d156 1
a156 1
   assert(!pipe_is_referenced(&buf->base.base.reference));
d230 1
a230 1
   if(buf->base.base.size < size)
d234 1
a234 1
   if(buf->base.base.size >= 2*size)
d237 1
a237 1
   if(!pb_check_alignment(desc->alignment, buf->base.base.alignment))
d240 1
a240 1
   if(!pb_check_usage(desc->usage, buf->base.base.usage))
d316 1
a316 1
      pipe_reference_init(&buf->base.base.reference, 1);
d339 9
a347 9
   assert(pipe_is_referenced(&buf->buffer->base.reference));
   assert(pb_check_alignment(desc->alignment, buf->buffer->base.alignment));
   assert(pb_check_usage(desc->usage, buf->buffer->base.usage));
   assert(buf->buffer->base.size >= size);
   
   pipe_reference_init(&buf->base.base.reference, 1);
   buf->base.base.alignment = buf->buffer->base.alignment;
   buf->base.base.usage = buf->buffer->base.usage;
   buf->base.base.size = buf->buffer->base.size;
@


1.2
log
@Merge Mesa 7.10.3
@
text
@a229 2
   void *map;

d243 10
a252 3
   map = pb_map(buf->buffer, PB_USAGE_DONTBLOCK, NULL);
   if (!map) {
      return -1;
a254 2
   pb_unmap(buf->buffer);
   
d327 7
@


1.1
log
@Update to Mesa 7.8.1. Tested on a bulk ports build by naddy@@, ok oga@@.
@
text
@d170 1
a170 1
                  unsigned flags)
d173 1
a173 1
   return pb_map(buf->buffer, flags);
d225 1
a225 1
static INLINE boolean
d230 2
d233 1
a233 1
      return FALSE;
d237 1
a237 1
      return FALSE;
d240 1
a240 1
      return FALSE;
d243 8
a250 1
      return FALSE;
d252 1
a252 1
   return TRUE;
d266 2
a267 1
   
d278 2
a279 2
      if(!buf && pb_cache_is_buffer_compat(curr_buf, size, desc))
	 buf = curr_buf;
d281 1
a281 1
	 _pb_cache_buffer_destroy(curr_buf);
d285 2
d292 1
a292 1
   if(!buf) {
d295 2
a296 1
         if(pb_cache_is_buffer_compat(curr_buf, size, desc)) {
d300 2
d310 1
@


1.1.1.1
log
@Import Mesa 7.10.3
@
text
@d170 1
a170 1
		    unsigned flags, void *flush_ctx)
d173 1
a173 1
   return pb_map(buf->buffer, flags, flush_ctx);
d225 1
a225 1
static INLINE int
a229 2
   void *map;

d231 1
a231 1
      return 0;
d235 1
a235 1
      return 0;
d238 1
a238 1
      return 0;
d241 1
a241 8
      return 0;

   map = pb_map(buf->buffer, PB_USAGE_DONTBLOCK, NULL);
   if (!map) {
      return -1;
   }

   pb_unmap(buf->buffer);
d243 1
a243 1
   return 1;
d257 1
a257 2
   int ret = 0;

d268 2
a269 2
      if(!buf && (ret = pb_cache_is_buffer_compat(curr_buf, size, desc) > 0))
         buf = curr_buf;
d271 1
a271 1
         _pb_cache_buffer_destroy(curr_buf);
a274 2
      if (ret == -1)
         break;
d280 1
a280 1
   if(!buf && ret != -1) {
d283 1
a283 2
         ret = pb_cache_is_buffer_compat(curr_buf, size, desc);
         if (ret > 0) {
a286 2
         if (ret == -1)
            break;
a294 1
      --mgr->numDelayed;
@


1.1.1.2
log
@Import Mesa 9.2.0
@
text
@d115 1
a115 1
   assert(!pipe_is_referenced(&buf->base.reference));
d156 1
a156 1
   assert(!pipe_is_referenced(&buf->base.reference));
d230 3
a232 1
   if(buf->base.size < size)
d236 1
a236 1
   if(buf->base.size >= 2*size)
d239 1
a239 1
   if(!pb_check_alignment(desc->alignment, buf->base.alignment))
d242 1
a242 1
   if(!pb_check_usage(desc->usage, buf->base.usage))
d245 3
a247 10
   if (buf->mgr->provider->is_buffer_busy) {
      if (buf->mgr->provider->is_buffer_busy(buf->mgr->provider, buf->buffer))
         return -1;
   } else {
      void *ptr = pb_map(buf->buffer, PB_USAGE_DONTBLOCK, NULL);

      if (!ptr)
         return -1;

      pb_unmap(buf->buffer);
d250 2
d313 1
a313 1
      pipe_reference_init(&buf->base.reference, 1);
a323 7

   /* Empty the cache and try again. */
   if (!buf->buffer) {
      mgr->base.flush(&mgr->base);
      buf->buffer = mgr->provider->create_buffer(mgr->provider, size, desc);
   }

d329 9
a337 9
   assert(pipe_is_referenced(&buf->buffer->reference));
   assert(pb_check_alignment(desc->alignment, buf->buffer->alignment));
   assert(pb_check_usage(desc->usage, buf->buffer->usage));
   assert(buf->buffer->size >= size);
   
   pipe_reference_init(&buf->base.reference, 1);
   buf->base.alignment = buf->buffer->alignment;
   buf->base.usage = buf->buffer->usage;
   buf->base.size = buf->buffer->size;
@


1.1.1.3
log
@Import Mesa 10.2.3
@
text
@d3 1
a3 1
 * Copyright 2007-2008 VMware, Inc.
d21 1
a21 1
 * IN NO EVENT SHALL VMWARE AND/OR ITS SUPPLIERS BE LIABLE FOR
d32 2
a33 2
 * \author Jose Fonseca <jfonseca-at-vmware-dot-com>
 * \author Thomas Hellstr√∂m <thellstom-at-vmware-dot-com>
a84 2
   float size_factor;
   unsigned bypass_usage;
a229 3
   if (desc->usage & buf->mgr->bypass_usage)
      return 0;

d234 1
a234 1
   if(buf->base.size > (unsigned) (buf->mgr->size_factor * size))
d341 1
a341 1
   assert(pb_check_usage(desc->usage & ~mgr->bypass_usage, buf->buffer->usage));
d387 1
a387 12
/**
 * Create a caching buffer manager
 *
 * @@param provider The buffer manager to which cache miss buffer requests
 * should be redirected.
 * @@param usecs Unused buffers may be released from the cache after this
 * time
 * @@param size_factor Declare buffers that are size_factor times bigger than
 * the requested size as cache hits.
 * @@param bypass_usage Bitmask. If (requested usage & bypass_usage) != 0,
 * buffer allocation requests are redirected to the provider.
 */
d390 1
a390 3
                        unsigned usecs,
                        float size_factor,
                        unsigned bypass_usage)
a405 2
   mgr->size_factor = size_factor;
   mgr->bypass_usage = bypass_usage;
@


1.1.1.4
log
@Import Mesa 10.4.3
@
text
@a86 1
   uint64_t cache_size, max_cache_size;
a116 1
   mgr->cache_size -= buf->base.size;
d161 1
a161 9

   /* Directly release any buffer that exceeds the limit. */
   if (mgr->cache_size + buf->base.size > mgr->max_cache_size) {
      pb_reference(&buf->buffer, NULL);
      FREE(buf);
      pipe_mutex_unlock(mgr->mutex);
      return;
   }

a165 1
   mgr->cache_size += buf->base.size;
a316 1
      mgr->cache_size -= buf->base.size;
a402 2
 * @@param maximum_cache_size  Maximum size of all unused buffers the cache can
 * hold.
d408 1
a408 2
                        unsigned bypass_usage,
                        uint64_t maximum_cache_size)
a427 1
   mgr->max_cache_size = maximum_cache_size;
@


1.1.1.5
log
@Import Mesa 10.2.9
@
text
@d87 1
d118 1
d163 9
a171 1
   
d176 1
d328 1
d415 2
d422 2
a423 1
                        unsigned bypass_usage)
d443 1
@


