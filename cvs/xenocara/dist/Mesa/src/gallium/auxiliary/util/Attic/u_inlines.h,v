head	1.8;
access;
symbols
	OPENBSD_5_8:1.7.0.4
	OPENBSD_5_8_BASE:1.7
	OPENBSD_5_7:1.7.0.2
	OPENBSD_5_7_BASE:1.7
	v10_2_9:1.1.1.5
	v10_4_3:1.1.1.4
	v10_2_7:1.1.1.3
	OPENBSD_5_6:1.5.0.2
	OPENBSD_5_6_BASE:1.5
	v10_2_3:1.1.1.3
	OPENBSD_5_5:1.4.0.2
	OPENBSD_5_5_BASE:1.4
	v9_2_5:1.1.1.2
	v9_2_3:1.1.1.2
	v9_2_2:1.1.1.2
	v9_2_1:1.1.1.2
	v9_2_0:1.1.1.2
	OPENBSD_5_4:1.3.0.4
	OPENBSD_5_4_BASE:1.3
	OPENBSD_5_3:1.3.0.2
	OPENBSD_5_3_BASE:1.3
	OPENBSD_5_2:1.2.0.4
	OPENBSD_5_2_BASE:1.2
	OPENBSD_5_1_BASE:1.2
	OPENBSD_5_1:1.2.0.2
	v7_10_3:1.1.1.1
	mesa:1.1.1
	OPENBSD_5_0:1.1.0.6
	OPENBSD_5_0_BASE:1.1
	OPENBSD_4_9:1.1.0.2
	OPENBSD_4_9_BASE:1.1
	OPENBSD_4_8:1.1.0.4
	OPENBSD_4_8_BASE:1.1;
locks; strict;
comment	@ * @;


1.8
date	2015.12.23.05.17.28;	author jsg;	state dead;
branches;
next	1.7;
commitid	TnlogFl9nOv2eaRf;

1.7
date	2015.02.20.23.09.51;	author jsg;	state Exp;
branches;
next	1.6;
commitid	4ry2gvZGMXkCUD2n;

1.6
date	2015.01.25.14.41.15;	author jsg;	state Exp;
branches;
next	1.5;
commitid	mcxB0JvoI9gTDYXU;

1.5
date	2014.07.09.21.08.52;	author jsg;	state Exp;
branches;
next	1.4;
commitid	WPD6rgPryPkvXOr9;

1.4
date	2013.09.05.13.59.50;	author jsg;	state Exp;
branches;
next	1.3;

1.3
date	2012.08.17.13.58.04;	author mpi;	state Exp;
branches;
next	1.2;

1.2
date	2011.10.23.13.37.32;	author matthieu;	state Exp;
branches;
next	1.1;

1.1
date	2010.05.22.20.06.05;	author matthieu;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2011.10.23.13.29.25;	author matthieu;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2013.09.05.13.10.52;	author jsg;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2014.07.09.20.33.48;	author jsg;	state Exp;
branches;
next	1.1.1.4;
commitid	3JhLfwcuBALP0ZR7;

1.1.1.4
date	2015.01.25.14.06.46;	author jsg;	state Exp;
branches;
next	1.1.1.5;
commitid	ce2W5rH5aF7VS9gi;

1.1.1.5
date	2015.02.20.22.43.53;	author jsg;	state Exp;
branches;
next	;
commitid	F54a1i0WXHMxq7kE;


desc
@@


1.8
log
@remove the now unused Mesa 10.2.9 code
@
text
@/**************************************************************************
 * 
 * Copyright 2007 VMware, Inc.
 * All Rights Reserved.
 * 
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sub license, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 * 
 * The above copyright notice and this permission notice (including the
 * next paragraph) shall be included in all copies or substantial portions
 * of the Software.
 * 
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
 * IN NO EVENT SHALL VMWARE AND/OR ITS SUPPLIERS BE LIABLE FOR
 * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 * 
 **************************************************************************/

#ifndef U_INLINES_H
#define U_INLINES_H

#include "pipe/p_context.h"
#include "pipe/p_defines.h"
#include "pipe/p_shader_tokens.h"
#include "pipe/p_state.h"
#include "pipe/p_screen.h"
#include "util/u_debug.h"
#include "util/u_debug_describe.h"
#include "util/u_debug_refcnt.h"
#include "util/u_atomic.h"
#include "util/u_box.h"
#include "util/u_math.h"


#ifdef __cplusplus
extern "C" {
#endif


/*
 * Reference counting helper functions.
 */


static INLINE void
pipe_reference_init(struct pipe_reference *reference, unsigned count)
{
   p_atomic_set(&reference->count, count);
}

static INLINE boolean
pipe_is_referenced(struct pipe_reference *reference)
{
   return p_atomic_read(&reference->count) != 0;
}

/**
 * Update reference counting.
 * The old thing pointed to, if any, will be unreferenced.
 * Both 'ptr' and 'reference' may be NULL.
 * \return TRUE if the object's refcount hits zero and should be destroyed.
 */
static INLINE boolean
pipe_reference_described(struct pipe_reference *ptr, 
                         struct pipe_reference *reference, 
                         debug_reference_descriptor get_desc)
{
   boolean destroy = FALSE;

   if(ptr != reference) {
      /* bump the reference.count first */
      if (reference) {
         assert(pipe_is_referenced(reference));
         p_atomic_inc(&reference->count);
         debug_reference(reference, get_desc, 1);
      }

      if (ptr) {
         assert(pipe_is_referenced(ptr));
         if (p_atomic_dec_zero(&ptr->count)) {
            destroy = TRUE;
         }
         debug_reference(ptr, get_desc, -1);
      }
   }

   return destroy;
}

static INLINE boolean
pipe_reference(struct pipe_reference *ptr, struct pipe_reference *reference)
{
   return pipe_reference_described(ptr, reference, 
                                   (debug_reference_descriptor)debug_describe_reference);
}

static INLINE void
pipe_surface_reference(struct pipe_surface **ptr, struct pipe_surface *surf)
{
   struct pipe_surface *old_surf = *ptr;

   if (pipe_reference_described(&(*ptr)->reference, &surf->reference, 
                                (debug_reference_descriptor)debug_describe_surface))
      old_surf->context->surface_destroy(old_surf->context, old_surf);
   *ptr = surf;
}

/**
 * Similar to pipe_surface_reference() but always set the pointer to NULL
 * and pass in an explicit context.  The explicit context avoids the problem
 * of using a deleted context's surface_destroy() method when freeing a surface
 * that's shared by multiple contexts.
 */
static INLINE void
pipe_surface_release(struct pipe_context *pipe, struct pipe_surface **ptr)
{
   if (pipe_reference_described(&(*ptr)->reference, NULL,
                                (debug_reference_descriptor)debug_describe_surface))
      pipe->surface_destroy(pipe, *ptr);
   *ptr = NULL;
}


static INLINE void
pipe_resource_reference(struct pipe_resource **ptr, struct pipe_resource *tex)
{
   struct pipe_resource *old_tex = *ptr;

   if (pipe_reference_described(&(*ptr)->reference, &tex->reference, 
                                (debug_reference_descriptor)debug_describe_resource))
      old_tex->screen->resource_destroy(old_tex->screen, old_tex);
   *ptr = tex;
}

static INLINE void
pipe_sampler_view_reference(struct pipe_sampler_view **ptr, struct pipe_sampler_view *view)
{
   struct pipe_sampler_view *old_view = *ptr;

   if (pipe_reference_described(&(*ptr)->reference, &view->reference,
                                (debug_reference_descriptor)debug_describe_sampler_view))
      old_view->context->sampler_view_destroy(old_view->context, old_view);
   *ptr = view;
}

/**
 * Similar to pipe_sampler_view_reference() but always set the pointer to
 * NULL and pass in an explicit context.  Passing an explicit context is a
 * work-around for fixing a dangling context pointer problem when textures
 * are shared by multiple contexts.  XXX fix this someday.
 */
static INLINE void
pipe_sampler_view_release(struct pipe_context *ctx,
                          struct pipe_sampler_view **ptr)
{
   struct pipe_sampler_view *old_view = *ptr;
   if (*ptr && (*ptr)->context != ctx) {
      debug_printf_once(("context mis-match in pipe_sampler_view_release()\n"));
   }
   if (pipe_reference_described(&(*ptr)->reference, NULL,
                    (debug_reference_descriptor)debug_describe_sampler_view)) {
      ctx->sampler_view_destroy(ctx, old_view);
   }
   *ptr = NULL;
}


static INLINE void
pipe_so_target_reference(struct pipe_stream_output_target **ptr,
                         struct pipe_stream_output_target *target)
{
   struct pipe_stream_output_target *old = *ptr;

   if (pipe_reference_described(&(*ptr)->reference, &target->reference,
                     (debug_reference_descriptor)debug_describe_so_target))
      old->context->stream_output_target_destroy(old->context, old);
   *ptr = target;
}

static INLINE void
pipe_surface_reset(struct pipe_context *ctx, struct pipe_surface* ps,
                   struct pipe_resource *pt, unsigned level, unsigned layer)
{
   pipe_resource_reference(&ps->texture, pt);
   ps->format = pt->format;
   ps->width = u_minify(pt->width0, level);
   ps->height = u_minify(pt->height0, level);
   ps->u.tex.level = level;
   ps->u.tex.first_layer = ps->u.tex.last_layer = layer;
   ps->context = ctx;
}

static INLINE void
pipe_surface_init(struct pipe_context *ctx, struct pipe_surface* ps,
                  struct pipe_resource *pt, unsigned level, unsigned layer)
{
   ps->texture = 0;
   pipe_reference_init(&ps->reference, 1);
   pipe_surface_reset(ctx, ps, pt, level, layer);
}

/* Return true if the surfaces are equal. */
static INLINE boolean
pipe_surface_equal(struct pipe_surface *s1, struct pipe_surface *s2)
{
   return s1->texture == s2->texture &&
          s1->format == s2->format &&
          (s1->texture->target != PIPE_BUFFER ||
           (s1->u.buf.first_element == s2->u.buf.first_element &&
            s1->u.buf.last_element == s2->u.buf.last_element)) &&
          (s1->texture->target == PIPE_BUFFER ||
           (s1->u.tex.level == s2->u.tex.level &&
            s1->u.tex.first_layer == s2->u.tex.first_layer &&
            s1->u.tex.last_layer == s2->u.tex.last_layer));
}

/*
 * Convenience wrappers for screen buffer functions.
 */


/**
 * Create a new resource.
 * \param bind  bitmask of PIPE_BIND_x flags
 * \param usage  bitmask of PIPE_USAGE_x flags
 */
static INLINE struct pipe_resource *
pipe_buffer_create( struct pipe_screen *screen,
		    unsigned bind,
		    unsigned usage,
		    unsigned size )
{
   struct pipe_resource buffer;
   memset(&buffer, 0, sizeof buffer);
   buffer.target = PIPE_BUFFER;
   buffer.format = PIPE_FORMAT_R8_UNORM; /* want TYPELESS or similar */
   buffer.bind = bind;
   buffer.usage = usage;
   buffer.flags = 0;
   buffer.width0 = size;
   buffer.height0 = 1;
   buffer.depth0 = 1;
   buffer.array_size = 1;
   return screen->resource_create(screen, &buffer);
}


/**
 * Map a range of a resource.
 * \param offset  start of region, in bytes 
 * \param length  size of region, in bytes 
 * \param access  bitmask of PIPE_TRANSFER_x flags
 * \param transfer  returns a transfer object
 */
static INLINE void *
pipe_buffer_map_range(struct pipe_context *pipe,
		      struct pipe_resource *buffer,
		      unsigned offset,
		      unsigned length,
		      unsigned access,
		      struct pipe_transfer **transfer)
{
   struct pipe_box box;
   void *map;

   assert(offset < buffer->width0);
   assert(offset + length <= buffer->width0);
   assert(length);

   u_box_1d(offset, length, &box);

   map = pipe->transfer_map(pipe, buffer, 0, access, &box, transfer);
   if (map == NULL) {
      return NULL;
   }

   return map;
}


/**
 * Map whole resource.
 * \param access  bitmask of PIPE_TRANSFER_x flags
 * \param transfer  returns a transfer object
 */
static INLINE void *
pipe_buffer_map(struct pipe_context *pipe,
                struct pipe_resource *buffer,
                unsigned access,
                struct pipe_transfer **transfer)
{
   return pipe_buffer_map_range(pipe, buffer, 0, buffer->width0, access, transfer);
}


static INLINE void
pipe_buffer_unmap(struct pipe_context *pipe,
                  struct pipe_transfer *transfer)
{
   pipe->transfer_unmap(pipe, transfer);
}

static INLINE void
pipe_buffer_flush_mapped_range(struct pipe_context *pipe,
                               struct pipe_transfer *transfer,
                               unsigned offset,
                               unsigned length)
{
   struct pipe_box box;
   int transfer_offset;

   assert(length);
   assert(transfer->box.x <= (int) offset);
   assert((int) (offset + length) <= transfer->box.x + transfer->box.width);

   /* Match old screen->buffer_flush_mapped_range() behaviour, where
    * offset parameter is relative to the start of the buffer, not the
    * mapped range.
    */
   transfer_offset = offset - transfer->box.x;

   u_box_1d(transfer_offset, length, &box);

   pipe->transfer_flush_region(pipe, transfer, &box);
}

static INLINE void
pipe_buffer_write(struct pipe_context *pipe,
                  struct pipe_resource *buf,
                  unsigned offset,
                  unsigned size,
                  const void *data)
{
   struct pipe_box box;
   unsigned access = PIPE_TRANSFER_WRITE;

   if (offset == 0 && size == buf->width0) {
      access |= PIPE_TRANSFER_DISCARD_WHOLE_RESOURCE;
   } else {
      access |= PIPE_TRANSFER_DISCARD_RANGE;
   }

   u_box_1d(offset, size, &box);

   pipe->transfer_inline_write( pipe,
                                buf,
                                0,
                                access,
                                &box,
                                data,
                                size,
                                0);
}

/**
 * Special case for writing non-overlapping ranges.
 *
 * We can avoid GPU/CPU synchronization when writing range that has never
 * been written before.
 */
static INLINE void
pipe_buffer_write_nooverlap(struct pipe_context *pipe,
                            struct pipe_resource *buf,
                            unsigned offset, unsigned size,
                            const void *data)
{
   struct pipe_box box;

   u_box_1d(offset, size, &box);

   pipe->transfer_inline_write(pipe,
                               buf,
                               0,
                               (PIPE_TRANSFER_WRITE |
                                PIPE_TRANSFER_UNSYNCHRONIZED),
                               &box,
                               data,
                               0, 0);
}


/**
 * Create a new resource and immediately put data into it
 * \param bind  bitmask of PIPE_BIND_x flags
 * \param usage  bitmask of PIPE_USAGE_x flags
 */
static INLINE struct pipe_resource *
pipe_buffer_create_with_data(struct pipe_context *pipe,
                             unsigned bind,
                             unsigned usage,
                             unsigned size,
                             const void *ptr)
{
   struct pipe_resource *res = pipe_buffer_create(pipe->screen,
                                                  bind, usage, size);
   pipe_buffer_write_nooverlap(pipe, res, 0, size, ptr);
   return res;
}

static INLINE void
pipe_buffer_read(struct pipe_context *pipe,
                 struct pipe_resource *buf,
                 unsigned offset,
                 unsigned size,
                 void *data)
{
   struct pipe_transfer *src_transfer;
   ubyte *map;

   map = (ubyte *) pipe_buffer_map_range(pipe,
					 buf,
					 offset, size,
					 PIPE_TRANSFER_READ,
					 &src_transfer);
   if (!map)
      return;

   memcpy(data, map, size);
   pipe_buffer_unmap(pipe, src_transfer);
}


/**
 * Map a resource for reading/writing.
 * \param access  bitmask of PIPE_TRANSFER_x flags
 */
static INLINE void *
pipe_transfer_map(struct pipe_context *context,
                  struct pipe_resource *resource,
                  unsigned level, unsigned layer,
                  unsigned access,
                  unsigned x, unsigned y,
                  unsigned w, unsigned h,
                  struct pipe_transfer **transfer)
{
   struct pipe_box box;
   u_box_2d_zslice(x, y, layer, w, h, &box);
   return context->transfer_map(context,
                                resource,
                                level,
                                access,
                                &box, transfer);
}


/**
 * Map a 3D (texture) resource for reading/writing.
 * \param access  bitmask of PIPE_TRANSFER_x flags
 */
static INLINE void *
pipe_transfer_map_3d(struct pipe_context *context,
                     struct pipe_resource *resource,
                     unsigned level,
                     unsigned access,
                     unsigned x, unsigned y, unsigned z,
                     unsigned w, unsigned h, unsigned d,
                     struct pipe_transfer **transfer)
{
   struct pipe_box box;
   u_box_3d(x, y, z, w, h, d, &box);
   return context->transfer_map(context,
                                resource,
                                level,
                                access,
                                &box, transfer);
}

static INLINE void
pipe_transfer_unmap( struct pipe_context *context,
                     struct pipe_transfer *transfer )
{
   context->transfer_unmap( context, transfer );
}

static INLINE void
pipe_set_constant_buffer(struct pipe_context *pipe, uint shader, uint index,
                         struct pipe_resource *buf)
{
   if (buf) {
      struct pipe_constant_buffer cb;
      cb.buffer = buf;
      cb.buffer_offset = 0;
      cb.buffer_size = buf->width0;
      cb.user_buffer = NULL;
      pipe->set_constant_buffer(pipe, shader, index, &cb);
   } else {
      pipe->set_constant_buffer(pipe, shader, index, NULL);
   }
}


/**
 * Get the polygon offset enable/disable flag for the given polygon fill mode.
 * \param fill_mode  one of PIPE_POLYGON_MODE_POINT/LINE/FILL
 */
static INLINE boolean
util_get_offset(const struct pipe_rasterizer_state *templ,
                unsigned fill_mode)
{
   switch(fill_mode) {
   case PIPE_POLYGON_MODE_POINT:
      return templ->offset_point;
   case PIPE_POLYGON_MODE_LINE:
      return templ->offset_line;
   case PIPE_POLYGON_MODE_FILL:
      return templ->offset_tri;
   default:
      assert(0);
      return FALSE;
   }
}

static INLINE float
util_get_min_point_size(const struct pipe_rasterizer_state *state)
{
   /* The point size should be clamped to this value at the rasterizer stage.
    */
   return !state->point_quad_rasterization &&
          !state->point_smooth &&
          !state->multisample ? 1.0f : 0.0f;
}

static INLINE void
util_query_clear_result(union pipe_query_result *result, unsigned type)
{
   switch (type) {
   case PIPE_QUERY_OCCLUSION_PREDICATE:
   case PIPE_QUERY_SO_OVERFLOW_PREDICATE:
   case PIPE_QUERY_GPU_FINISHED:
      result->b = FALSE;
      break;
   case PIPE_QUERY_OCCLUSION_COUNTER:
   case PIPE_QUERY_TIMESTAMP:
   case PIPE_QUERY_TIME_ELAPSED:
   case PIPE_QUERY_PRIMITIVES_GENERATED:
   case PIPE_QUERY_PRIMITIVES_EMITTED:
      result->u64 = 0;
      break;
   case PIPE_QUERY_SO_STATISTICS:
      memset(&result->so_statistics, 0, sizeof(result->so_statistics));
      break;
   case PIPE_QUERY_TIMESTAMP_DISJOINT:
      memset(&result->timestamp_disjoint, 0, sizeof(result->timestamp_disjoint));
      break;
   case PIPE_QUERY_PIPELINE_STATISTICS:
      memset(&result->pipeline_statistics, 0, sizeof(result->pipeline_statistics));
      break;
   default:
      memset(result, 0, sizeof(*result));
   }
}

/** Convert PIPE_TEXTURE_x to TGSI_TEXTURE_x */
static INLINE unsigned
util_pipe_tex_to_tgsi_tex(enum pipe_texture_target pipe_tex_target,
                          unsigned nr_samples)
{
   switch (pipe_tex_target) {
   case PIPE_TEXTURE_1D:
      assert(nr_samples <= 1);
      return TGSI_TEXTURE_1D;

   case PIPE_TEXTURE_2D:
      return nr_samples > 1 ? TGSI_TEXTURE_2D_MSAA : TGSI_TEXTURE_2D;

   case PIPE_TEXTURE_RECT:
      assert(nr_samples <= 1);
      return TGSI_TEXTURE_RECT;

   case PIPE_TEXTURE_3D:
      assert(nr_samples <= 1);
      return TGSI_TEXTURE_3D;

   case PIPE_TEXTURE_CUBE:
      assert(nr_samples <= 1);
      return TGSI_TEXTURE_CUBE;

   case PIPE_TEXTURE_1D_ARRAY:
      assert(nr_samples <= 1);
      return TGSI_TEXTURE_1D_ARRAY;

   case PIPE_TEXTURE_2D_ARRAY:
      return nr_samples > 1 ? TGSI_TEXTURE_2D_ARRAY_MSAA :
                              TGSI_TEXTURE_2D_ARRAY;

   case PIPE_TEXTURE_CUBE_ARRAY:
      return TGSI_TEXTURE_CUBE_ARRAY;

   default:
      assert(0 && "unexpected texture target");
      return TGSI_TEXTURE_UNKNOWN;
   }
}


static INLINE void
util_copy_constant_buffer(struct pipe_constant_buffer *dst,
                          const struct pipe_constant_buffer *src)
{
   if (src) {
      pipe_resource_reference(&dst->buffer, src->buffer);
      dst->buffer_offset = src->buffer_offset;
      dst->buffer_size = src->buffer_size;
      dst->user_buffer = src->user_buffer;
   }
   else {
      pipe_resource_reference(&dst->buffer, NULL);
      dst->buffer_offset = 0;
      dst->buffer_size = 0;
      dst->user_buffer = NULL;
   }
}

static INLINE unsigned
util_max_layer(const struct pipe_resource *r, unsigned level)
{
   switch (r->target) {
   case PIPE_TEXTURE_CUBE:
      return 6 - 1;
   case PIPE_TEXTURE_3D:
      return u_minify(r->depth0, level) - 1;
   case PIPE_TEXTURE_1D_ARRAY:
   case PIPE_TEXTURE_2D_ARRAY:
   case PIPE_TEXTURE_CUBE_ARRAY:
      return r->array_size - 1;
   default:
      return 0;
   }
}

#ifdef __cplusplus
}
#endif

#endif /* U_INLINES_H */
@


1.7
log
@Merge Mesa 10.2.9
@
text
@@


1.6
log
@Merge Mesa 10.4.3
Tested by matthieu@@ mpi@@ and myself.  landry@@ ran a ports bulk build.
kettenis@@ tracked down the cause of an alignment fault on archs
that require strict eight byte pointer alignment.
@
text
@a567 3
   case PIPE_BUFFER:
      return TGSI_TEXTURE_BUFFER;

d627 2
a630 3
   case PIPE_TEXTURE_CUBE:
      assert(r->array_size == 6);
      /* fall-through */
@


1.5
log
@Merge Mesa 10.2.3
tested by matthieu@@ kettenis@@ mpi@@ brett@@ and myself across a
diverse range of hardware
@
text
@d568 3
a629 2
   case PIPE_TEXTURE_CUBE:
      return 6 - 1;
d632 3
@


1.4
log
@Merge Mesa 9.2.0
@
text
@d3 1
a3 1
 * Copyright 2007 Tungsten Graphics, Inc., Cedar Park, Texas.
d21 1
a21 1
 * IN NO EVENT SHALL TUNGSTEN GRAPHICS AND/OR ITS SUPPLIERS BE LIABLE FOR
d230 6
d256 8
d269 1
a269 1
		      unsigned usage,
d281 1
a281 1
   map = pipe->transfer_map(pipe, buffer, 0, usage, &box, transfer);
d290 5
d298 1
a298 1
                unsigned usage,
d301 1
a301 1
   return pipe_buffer_map_range(pipe, buffer, 0, buffer->width0, usage, transfer);
d344 1
a344 1
   unsigned usage = PIPE_TRANSFER_WRITE;
d347 1
a347 1
      usage |= PIPE_TRANSFER_DISCARD_WHOLE_RESOURCE;
d349 1
a349 1
      usage |= PIPE_TRANSFER_DISCARD_RANGE;
d357 1
a357 1
                                usage,
d390 6
d401 1
a401 1
                             void *ptr)
d431 5
d440 1
a440 1
                  enum pipe_transfer_usage usage,
d450 1
a450 1
                                usage,
d454 5
d463 1
a463 1
                     enum pipe_transfer_usage usage,
d473 1
a473 1
                                usage,
d501 7
a507 3
static INLINE boolean util_get_offset( 
   const struct pipe_rasterizer_state *templ,
   unsigned fill_mode)
@


1.3
log
@Upate to libGL 7.11.2

Tested by jsg@@, matthieu@@ and ajacoutot@@, ok mattieu@@
@
text
@d33 1
d117 16
d155 34
d191 1
a191 2
                   struct pipe_resource *pt, unsigned level, unsigned layer,
                   unsigned flags)
a196 1
   ps->usage = flags;
d204 1
a204 2
                  struct pipe_resource *pt, unsigned level, unsigned layer,
                  unsigned flags)
d208 1
a208 1
   pipe_surface_reset(ctx, ps, pt, level, layer, flags);
a249 8

static INLINE struct pipe_resource *
pipe_user_buffer_create( struct pipe_screen *screen, void *ptr, unsigned size,
			 unsigned usage )
{
   return screen->user_buffer_create(screen, ptr, size, usage);
}

d267 1
a267 10
   *transfer = pipe->get_transfer( pipe,
                                   buffer,
                                   0,
                                   usage,
                                   &box);

   if (*transfer == NULL)
      return NULL;

   map = pipe->transfer_map( pipe, *transfer );
a268 2
      pipe->transfer_destroy( pipe, *transfer );
      *transfer = NULL;
d272 1
a272 4
   /* Match old screen->buffer_map_range() behaviour, return pointer
    * to where the beginning of the buffer would be:
    */
   return (void *)((char *)map - offset);
d290 1
a290 4
   if (transfer) {
      pipe->transfer_unmap(pipe, transfer);
      pipe->transfer_destroy(pipe, transfer);
   }
d303 2
a304 2
   assert(transfer->box.x <= offset);
   assert(offset + length <= transfer->box.x + transfer->box.width);
d365 1
a365 1
                                PIPE_TRANSFER_NOOVERWRITE),
d371 13
d399 2
d402 1
a402 3
   if (map)
      memcpy(data, map + offset, size);

d406 8
a413 7
static INLINE struct pipe_transfer *
pipe_get_transfer( struct pipe_context *context,
                   struct pipe_resource *resource,
                   unsigned level, unsigned layer,
                   enum pipe_transfer_usage usage,
                   unsigned x, unsigned y,
                   unsigned w, unsigned h)
d416 6
a421 6
   u_box_2d_zslice( x, y, layer, w, h, &box );
   return context->get_transfer( context,
                                 resource,
                                 level,
                                 usage,
                                 &box );
d425 7
a431 2
pipe_transfer_map( struct pipe_context *context,
                   struct pipe_transfer *transfer )
d433 7
a439 1
   return context->transfer_map( context, transfer );
a448 1

d450 2
a451 2
pipe_transfer_destroy( struct pipe_context *context, 
                       struct pipe_transfer *transfer )
d453 10
a462 1
   context->transfer_destroy(context, transfer);
d483 116
a598 26
/**
 * This function is used to copy an array of pipe_vertex_buffer structures,
 * while properly referencing the pipe_vertex_buffer::buffer member.
 *
 * \sa util_copy_framebuffer_state
 */
static INLINE void util_copy_vertex_buffers(struct pipe_vertex_buffer *dst,
                                            unsigned *dst_count,
                                            const struct pipe_vertex_buffer *src,
                                            unsigned src_count)
{
   unsigned i;

   /* Reference the buffers of 'src' in 'dst'. */
   for (i = 0; i < src_count; i++) {
      pipe_resource_reference(&dst[i].buffer, src[i].buffer);
   }
   /* Unreference the rest of the buffers in 'dst'. */
   for (; i < *dst_count; i++) {
      pipe_resource_reference(&dst[i].buffer, NULL);
   }

   /* Update the size of 'dst' and copy over the other members
    * of pipe_vertex_buffer. */
   *dst_count = src_count;
   memcpy(dst, src, src_count * sizeof(struct pipe_vertex_buffer));
@


1.2
log
@Merge Mesa 7.10.3
@
text
@d163 15
d185 1
d193 1
a193 1
   buffer.usage = PIPE_USAGE_DEFAULT;
d239 1
a261 1
                  struct pipe_resource *buf,
d302 7
d315 1
a315 1
                                PIPE_TRANSFER_WRITE,
d367 1
a367 1
   pipe_buffer_unmap(pipe, buf, src_transfer);
d425 28
@


1.1
log
@Update to Mesa 7.8.1. Tested on a bulk ports build by naddy@@, ok oga@@.
@
text
@d36 2
d39 2
d72 3
a74 1
pipe_reference(struct pipe_reference *ptr, struct pipe_reference *reference)
d83 1
d91 1
d98 2
a99 2
static INLINE void
pipe_buffer_reference(struct pipe_buffer **ptr, struct pipe_buffer *buf)
d101 2
a102 8
   struct pipe_buffer *old_buf;

   assert(ptr);
   old_buf = *ptr;

   if (pipe_reference(&(*ptr)->reference, &buf->reference))
      old_buf->screen->buffer_destroy(old_buf);
   *ptr = buf;
d110 3
a112 2
   if (pipe_reference(&(*ptr)->reference, &surf->reference))
      old_surf->texture->screen->tex_surface_destroy(old_surf);
d117 1
a117 1
pipe_texture_reference(struct pipe_texture **ptr, struct pipe_texture *tex)
d119 1
a119 1
   struct pipe_texture *old_tex = *ptr;
d121 3
a123 2
   if (pipe_reference(&(*ptr)->reference, &tex->reference))
      old_tex->screen->texture_destroy(old_tex);
d127 35
d167 1
a167 1
static INLINE struct pipe_buffer *
d169 2
a170 1
                    unsigned alignment, unsigned usage, unsigned size )
d172 12
a183 1
   return screen->buffer_create(screen, alignment, usage, size);
d186 4
a189 2
static INLINE struct pipe_buffer *
pipe_user_buffer_create( struct pipe_screen *screen, void *ptr, unsigned size )
d191 1
a191 1
   return screen->user_buffer_create(screen, ptr, size);
d195 29
a223 8
pipe_buffer_map(struct pipe_screen *screen,
                struct pipe_buffer *buf,
                unsigned usage)
{
   if(screen->buffer_map_range) {
      unsigned offset = 0;
      unsigned length = buf->size;
      return screen->buffer_map_range(screen, buf, offset, length, usage);
d225 5
a229 2
   else
      return screen->buffer_map(screen, buf, usage);
d232 6
a237 3
static INLINE void
pipe_buffer_unmap(struct pipe_screen *screen,
                  struct pipe_buffer *buf)
d239 1
a239 1
   screen->buffer_unmap(screen, buf);
d242 5
a246 6
static INLINE void *
pipe_buffer_map_range(struct pipe_screen *screen,
                struct pipe_buffer *buf,
                unsigned offset,
                unsigned length,
                unsigned usage)
d248 4
a251 7
   assert(offset < buf->size);
   assert(offset + length <= buf->size);
   assert(length);
   if(screen->buffer_map_range)
      return screen->buffer_map_range(screen, buf, offset, length, usage);
   else
      return screen->buffer_map(screen, buf, usage);
d255 2
a256 2
pipe_buffer_flush_mapped_range(struct pipe_screen *screen,
                               struct pipe_buffer *buf,
d260 3
a262 2
   assert(offset < buf->size);
   assert(offset + length <= buf->size);
d264 12
a275 2
   if(screen->buffer_flush_mapped_range)
      screen->buffer_flush_mapped_range(screen, buf, offset, length);
d279 4
a282 3
pipe_buffer_write(struct pipe_screen *screen,
                  struct pipe_buffer *buf,
                  unsigned offset, unsigned size,
d285 12
a296 16
   void *map;
   
   assert(offset < buf->size);
   assert(offset + size <= buf->size);
   assert(size);

   map = pipe_buffer_map_range(screen, buf, offset, size, 
                               PIPE_BUFFER_USAGE_CPU_WRITE | 
                               PIPE_BUFFER_USAGE_FLUSH_EXPLICIT |
                               PIPE_BUFFER_USAGE_DISCARD);
   assert(map);
   if(map) {
      memcpy((uint8_t *)map + offset, data, size);
      pipe_buffer_flush_mapped_range(screen, buf, offset, size);
      pipe_buffer_unmap(screen, buf);
   }
d306 2
a307 2
pipe_buffer_write_nooverlap(struct pipe_screen *screen,
                            struct pipe_buffer *buf,
d311 3
a313 1
   void *map;
d315 8
a322 15
   assert(offset < buf->size);
   assert(offset + size <= buf->size);
   assert(size);

   map = pipe_buffer_map_range(screen, buf, offset, size,
                               PIPE_BUFFER_USAGE_CPU_WRITE |
                               PIPE_BUFFER_USAGE_FLUSH_EXPLICIT |
                               PIPE_BUFFER_USAGE_DISCARD |
                               PIPE_BUFFER_USAGE_UNSYNCHRONIZED);
   assert(map);
   if(map) {
      memcpy((uint8_t *)map + offset, data, size);
      pipe_buffer_flush_mapped_range(screen, buf, offset, size);
      pipe_buffer_unmap(screen, buf);
   }
d326 4
a329 3
pipe_buffer_read(struct pipe_screen *screen,
                 struct pipe_buffer *buf,
                 unsigned offset, unsigned size,
d332 30
a361 12
   void *map;
   
   assert(offset < buf->size);
   assert(offset + size <= buf->size);
   assert(size);

   map = pipe_buffer_map_range(screen, buf, offset, size, PIPE_BUFFER_USAGE_CPU_READ);
   assert(map);
   if(map) {
      memcpy(data, (const uint8_t *)map + offset, size);
      pipe_buffer_unmap(screen, buf);
   }
d365 2
a366 1
pipe_transfer_map( struct pipe_transfer *transf )
d368 1
a368 2
   struct pipe_screen *screen = transf->texture->screen;
   return screen->transfer_map(screen, transf);
d372 2
a373 1
pipe_transfer_unmap( struct pipe_transfer *transf )
d375 1
a375 2
   struct pipe_screen *screen = transf->texture->screen;
   screen->transfer_unmap(screen, transf);
d378 1
d380 2
a381 1
pipe_transfer_destroy( struct pipe_transfer *transf )
d383 1
a383 2
   struct pipe_screen *screen = transf->texture->screen;
   screen->tex_transfer_destroy(transf);
d386 4
a389 2
static INLINE unsigned
pipe_transfer_buffer_flags( struct pipe_transfer *transf )
d391 7
a397 7
   switch (transf->usage & PIPE_TRANSFER_READ_WRITE) {
   case PIPE_TRANSFER_READ_WRITE:
      return PIPE_BUFFER_USAGE_CPU_READ | PIPE_BUFFER_USAGE_CPU_WRITE;
   case PIPE_TRANSFER_READ:
      return PIPE_BUFFER_USAGE_CPU_READ;
   case PIPE_TRANSFER_WRITE:
      return PIPE_BUFFER_USAGE_CPU_WRITE;
d399 2
a400 2
      debug_assert(0);
      return 0;
@


1.1.1.1
log
@Import Mesa 7.10.3
@
text
@a35 2
#include "util/u_debug_describe.h"
#include "util/u_debug_refcnt.h"
a36 2
#include "util/u_box.h"
#include "util/u_math.h"
d68 1
a68 3
pipe_reference_described(struct pipe_reference *ptr, 
                         struct pipe_reference *reference, 
                         debug_reference_descriptor get_desc)
a76 1
         debug_reference(reference, get_desc, 1);
a83 1
         debug_reference(ptr, get_desc, -1);
d90 2
a91 2
static INLINE boolean
pipe_reference(struct pipe_reference *ptr, struct pipe_reference *reference)
d93 8
a100 2
   return pipe_reference_described(ptr, reference, 
                                   (debug_reference_descriptor)debug_describe_reference);
d108 2
a109 3
   if (pipe_reference_described(&(*ptr)->reference, &surf->reference, 
                                (debug_reference_descriptor)debug_describe_surface))
      old_surf->context->surface_destroy(old_surf->context, old_surf);
d114 1
a114 1
pipe_resource_reference(struct pipe_resource **ptr, struct pipe_resource *tex)
d116 1
a116 1
   struct pipe_resource *old_tex = *ptr;
d118 2
a119 3
   if (pipe_reference_described(&(*ptr)->reference, &tex->reference, 
                                (debug_reference_descriptor)debug_describe_resource))
      old_tex->screen->resource_destroy(old_tex->screen, old_tex);
a122 35
static INLINE void
pipe_sampler_view_reference(struct pipe_sampler_view **ptr, struct pipe_sampler_view *view)
{
   struct pipe_sampler_view *old_view = *ptr;

   if (pipe_reference_described(&(*ptr)->reference, &view->reference,
                                (debug_reference_descriptor)debug_describe_sampler_view))
      old_view->context->sampler_view_destroy(old_view->context, old_view);
   *ptr = view;
}

static INLINE void
pipe_surface_reset(struct pipe_context *ctx, struct pipe_surface* ps,
                   struct pipe_resource *pt, unsigned level, unsigned layer,
                   unsigned flags)
{
   pipe_resource_reference(&ps->texture, pt);
   ps->format = pt->format;
   ps->width = u_minify(pt->width0, level);
   ps->height = u_minify(pt->height0, level);
   ps->usage = flags;
   ps->u.tex.level = level;
   ps->u.tex.first_layer = ps->u.tex.last_layer = layer;
   ps->context = ctx;
}

static INLINE void
pipe_surface_init(struct pipe_context *ctx, struct pipe_surface* ps,
                  struct pipe_resource *pt, unsigned level, unsigned layer,
                  unsigned flags)
{
   ps->texture = 0;
   pipe_reference_init(&ps->reference, 1);
   pipe_surface_reset(ctx, ps, pt, level, layer, flags);
}
d128 1
a128 1
static INLINE struct pipe_resource *
d130 1
a130 2
		    unsigned bind,
		    unsigned size )
d132 1
a132 12
   struct pipe_resource buffer;
   memset(&buffer, 0, sizeof buffer);
   buffer.target = PIPE_BUFFER;
   buffer.format = PIPE_FORMAT_R8_UNORM; /* want TYPELESS or similar */
   buffer.bind = bind;
   buffer.usage = PIPE_USAGE_DEFAULT;
   buffer.flags = 0;
   buffer.width0 = size;
   buffer.height0 = 1;
   buffer.depth0 = 1;
   buffer.array_size = 1;
   return screen->resource_create(screen, &buffer);
d135 2
a136 4

static INLINE struct pipe_resource *
pipe_user_buffer_create( struct pipe_screen *screen, void *ptr, unsigned size,
			 unsigned usage )
d138 1
a138 1
   return screen->user_buffer_create(screen, ptr, size, usage);
d142 8
a149 29
pipe_buffer_map_range(struct pipe_context *pipe,
		      struct pipe_resource *buffer,
		      unsigned offset,
		      unsigned length,
		      unsigned usage,
		      struct pipe_transfer **transfer)
{
   struct pipe_box box;
   void *map;

   assert(offset < buffer->width0);
   assert(offset + length <= buffer->width0);
   assert(length);

   u_box_1d(offset, length, &box);

   *transfer = pipe->get_transfer( pipe,
                                   buffer,
                                   0,
                                   usage,
                                   &box);

   if (*transfer == NULL)
      return NULL;

   map = pipe->transfer_map( pipe, *transfer );
   if (map == NULL) {
      pipe->transfer_destroy( pipe, *transfer );
      return NULL;
d151 2
a152 5

   /* Match old screen->buffer_map_range() behaviour, return pointer
    * to where the beginning of the buffer would be:
    */
   return (void *)((char *)map - offset);
d155 3
a157 6

static INLINE void *
pipe_buffer_map(struct pipe_context *pipe,
                struct pipe_resource *buffer,
                unsigned usage,
                struct pipe_transfer **transfer)
d159 1
a159 1
   return pipe_buffer_map_range(pipe, buffer, 0, buffer->width0, usage, transfer);
d162 6
a167 5

static INLINE void
pipe_buffer_unmap(struct pipe_context *pipe,
                  struct pipe_resource *buf,
                  struct pipe_transfer *transfer)
d169 7
a175 4
   if (transfer) {
      pipe->transfer_unmap(pipe, transfer);
      pipe->transfer_destroy(pipe, transfer);
   }
d179 2
a180 2
pipe_buffer_flush_mapped_range(struct pipe_context *pipe,
                               struct pipe_transfer *transfer,
d184 2
a185 3
   struct pipe_box box;
   int transfer_offset;

d187 2
a188 12
   assert(transfer->box.x <= offset);
   assert(offset + length <= transfer->box.x + transfer->box.width);

   /* Match old screen->buffer_flush_mapped_range() behaviour, where
    * offset parameter is relative to the start of the buffer, not the
    * mapped range.
    */
   transfer_offset = offset - transfer->box.x;

   u_box_1d(transfer_offset, length, &box);

   pipe->transfer_flush_region(pipe, transfer, &box);
d192 3
a194 4
pipe_buffer_write(struct pipe_context *pipe,
                  struct pipe_resource *buf,
                  unsigned offset,
                  unsigned size,
d197 16
a212 12
   struct pipe_box box;

   u_box_1d(offset, size, &box);

   pipe->transfer_inline_write( pipe,
                                buf,
                                0,
                                PIPE_TRANSFER_WRITE,
                                &box,
                                data,
                                size,
                                0);
d222 2
a223 2
pipe_buffer_write_nooverlap(struct pipe_context *pipe,
                            struct pipe_resource *buf,
d227 1
a227 1
   struct pipe_box box;
d229 15
a243 10
   u_box_1d(offset, size, &box);

   pipe->transfer_inline_write(pipe,
                               buf,
                               0,
                               (PIPE_TRANSFER_WRITE |
                                PIPE_TRANSFER_NOOVERWRITE),
                               &box,
                               data,
                               0, 0);
d247 3
a249 4
pipe_buffer_read(struct pipe_context *pipe,
                 struct pipe_resource *buf,
                 unsigned offset,
                 unsigned size,
d252 12
a263 30
   struct pipe_transfer *src_transfer;
   ubyte *map;

   map = (ubyte *) pipe_buffer_map_range(pipe,
					 buf,
					 offset, size,
					 PIPE_TRANSFER_READ,
					 &src_transfer);

   if (map)
      memcpy(data, map + offset, size);

   pipe_buffer_unmap(pipe, buf, src_transfer);
}

static INLINE struct pipe_transfer *
pipe_get_transfer( struct pipe_context *context,
                   struct pipe_resource *resource,
                   unsigned level, unsigned layer,
                   enum pipe_transfer_usage usage,
                   unsigned x, unsigned y,
                   unsigned w, unsigned h)
{
   struct pipe_box box;
   u_box_2d_zslice( x, y, layer, w, h, &box );
   return context->get_transfer( context,
                                 resource,
                                 level,
                                 usage,
                                 &box );
d267 1
a267 2
pipe_transfer_map( struct pipe_context *context,
                   struct pipe_transfer *transfer )
d269 2
a270 1
   return context->transfer_map( context, transfer );
d274 1
a274 2
pipe_transfer_unmap( struct pipe_context *context,
                     struct pipe_transfer *transfer )
d276 2
a277 1
   context->transfer_unmap( context, transfer );
a279 1

d281 1
a281 2
pipe_transfer_destroy( struct pipe_context *context, 
                       struct pipe_transfer *transfer )
d283 2
a284 1
   context->transfer_destroy(context, transfer);
d287 2
a288 4

static INLINE boolean util_get_offset( 
   const struct pipe_rasterizer_state *templ,
   unsigned fill_mode)
d290 7
a296 7
   switch(fill_mode) {
   case PIPE_POLYGON_MODE_POINT:
      return templ->offset_point;
   case PIPE_POLYGON_MODE_LINE:
      return templ->offset_line;
   case PIPE_POLYGON_MODE_FILL:
      return templ->offset_tri;
d298 2
a299 2
      assert(0);
      return FALSE;
@


1.1.1.2
log
@Import Mesa 9.2.0
@
text
@a32 1
#include "pipe/p_shader_tokens.h"
a115 16
/**
 * Similar to pipe_surface_reference() but always set the pointer to NULL
 * and pass in an explicit context.  The explicit context avoids the problem
 * of using a deleted context's surface_destroy() method when freeing a surface
 * that's shared by multiple contexts.
 */
static INLINE void
pipe_surface_release(struct pipe_context *pipe, struct pipe_surface **ptr)
{
   if (pipe_reference_described(&(*ptr)->reference, NULL,
                                (debug_reference_descriptor)debug_describe_surface))
      pipe->surface_destroy(pipe, *ptr);
   *ptr = NULL;
}


a137 34
/**
 * Similar to pipe_sampler_view_reference() but always set the pointer to
 * NULL and pass in an explicit context.  Passing an explicit context is a
 * work-around for fixing a dangling context pointer problem when textures
 * are shared by multiple contexts.  XXX fix this someday.
 */
static INLINE void
pipe_sampler_view_release(struct pipe_context *ctx,
                          struct pipe_sampler_view **ptr)
{
   struct pipe_sampler_view *old_view = *ptr;
   if (*ptr && (*ptr)->context != ctx) {
      debug_printf_once(("context mis-match in pipe_sampler_view_release()\n"));
   }
   if (pipe_reference_described(&(*ptr)->reference, NULL,
                    (debug_reference_descriptor)debug_describe_sampler_view)) {
      ctx->sampler_view_destroy(ctx, old_view);
   }
   *ptr = NULL;
}


static INLINE void
pipe_so_target_reference(struct pipe_stream_output_target **ptr,
                         struct pipe_stream_output_target *target)
{
   struct pipe_stream_output_target *old = *ptr;

   if (pipe_reference_described(&(*ptr)->reference, &target->reference,
                     (debug_reference_descriptor)debug_describe_so_target))
      old->context->stream_output_target_destroy(old->context, old);
   *ptr = target;
}

d140 2
a141 1
                   struct pipe_resource *pt, unsigned level, unsigned layer)
d147 1
d155 2
a156 1
                  struct pipe_resource *pt, unsigned level, unsigned layer)
d160 1
a160 16
   pipe_surface_reset(ctx, ps, pt, level, layer);
}

/* Return true if the surfaces are equal. */
static INLINE boolean
pipe_surface_equal(struct pipe_surface *s1, struct pipe_surface *s2)
{
   return s1->texture == s2->texture &&
          s1->format == s2->format &&
          (s1->texture->target != PIPE_BUFFER ||
           (s1->u.buf.first_element == s2->u.buf.first_element &&
            s1->u.buf.last_element == s2->u.buf.last_element)) &&
          (s1->texture->target == PIPE_BUFFER ||
           (s1->u.tex.level == s2->u.tex.level &&
            s1->u.tex.first_layer == s2->u.tex.first_layer &&
            s1->u.tex.last_layer == s2->u.tex.last_layer));
a169 1
		    unsigned usage,
d177 1
a177 1
   buffer.usage = usage;
d186 8
d211 10
a220 1
   map = pipe->transfer_map(pipe, buffer, 0, usage, &box, transfer);
d222 1
d226 4
a229 1
   return map;
d245 1
d248 4
a251 1
   pipe->transfer_unmap(pipe, transfer);
d264 2
a265 2
   assert(transfer->box.x <= (int) offset);
   assert((int) (offset + length) <= transfer->box.x + transfer->box.width);
a285 7
   unsigned usage = PIPE_TRANSFER_WRITE;

   if (offset == 0 && size == buf->width0) {
      usage |= PIPE_TRANSFER_DISCARD_WHOLE_RESOURCE;
   } else {
      usage |= PIPE_TRANSFER_DISCARD_RANGE;
   }
d292 1
a292 1
                                usage,
d319 1
a319 1
                                PIPE_TRANSFER_UNSYNCHRONIZED),
a324 13
static INLINE struct pipe_resource *
pipe_buffer_create_with_data(struct pipe_context *pipe,
                             unsigned bind,
                             unsigned usage,
                             unsigned size,
                             void *ptr)
{
   struct pipe_resource *res = pipe_buffer_create(pipe->screen,
                                                  bind, usage, size);
   pipe_buffer_write_nooverlap(pipe, res, 0, size, ptr);
   return res;
}

a339 2
   if (!map)
      return;
d341 4
a344 2
   memcpy(data, map, size);
   pipe_buffer_unmap(pipe, src_transfer);
d347 7
a353 8
static INLINE void *
pipe_transfer_map(struct pipe_context *context,
                  struct pipe_resource *resource,
                  unsigned level, unsigned layer,
                  enum pipe_transfer_usage usage,
                  unsigned x, unsigned y,
                  unsigned w, unsigned h,
                  struct pipe_transfer **transfer)
d356 6
a361 6
   u_box_2d_zslice(x, y, layer, w, h, &box);
   return context->transfer_map(context,
                                resource,
                                level,
                                usage,
                                &box, transfer);
d365 2
a366 7
pipe_transfer_map_3d(struct pipe_context *context,
                     struct pipe_resource *resource,
                     unsigned level,
                     enum pipe_transfer_usage usage,
                     unsigned x, unsigned y, unsigned z,
                     unsigned w, unsigned h, unsigned d,
                     struct pipe_transfer **transfer)
d368 1
a368 7
   struct pipe_box box;
   u_box_3d(x, y, z, w, h, d, &box);
   return context->transfer_map(context,
                                resource,
                                level,
                                usage,
                                &box, transfer);
d378 1
d380 2
a381 2
pipe_set_constant_buffer(struct pipe_context *pipe, uint shader, uint index,
                         struct pipe_resource *buf)
d383 1
a383 10
   if (buf) {
      struct pipe_constant_buffer cb;
      cb.buffer = buf;
      cb.buffer_offset = 0;
      cb.buffer_size = buf->width0;
      cb.user_buffer = NULL;
      pipe->set_constant_buffer(pipe, shader, index, &cb);
   } else {
      pipe->set_constant_buffer(pipe, shader, index, NULL);
   }
a400 118
   }
}

static INLINE float
util_get_min_point_size(const struct pipe_rasterizer_state *state)
{
   /* The point size should be clamped to this value at the rasterizer stage.
    */
   return !state->point_quad_rasterization &&
          !state->point_smooth &&
          !state->multisample ? 1.0f : 0.0f;
}

static INLINE void
util_query_clear_result(union pipe_query_result *result, unsigned type)
{
   switch (type) {
   case PIPE_QUERY_OCCLUSION_PREDICATE:
   case PIPE_QUERY_SO_OVERFLOW_PREDICATE:
   case PIPE_QUERY_GPU_FINISHED:
      result->b = FALSE;
      break;
   case PIPE_QUERY_OCCLUSION_COUNTER:
   case PIPE_QUERY_TIMESTAMP:
   case PIPE_QUERY_TIME_ELAPSED:
   case PIPE_QUERY_PRIMITIVES_GENERATED:
   case PIPE_QUERY_PRIMITIVES_EMITTED:
      result->u64 = 0;
      break;
   case PIPE_QUERY_SO_STATISTICS:
      memset(&result->so_statistics, 0, sizeof(result->so_statistics));
      break;
   case PIPE_QUERY_TIMESTAMP_DISJOINT:
      memset(&result->timestamp_disjoint, 0, sizeof(result->timestamp_disjoint));
      break;
   case PIPE_QUERY_PIPELINE_STATISTICS:
      memset(&result->pipeline_statistics, 0, sizeof(result->pipeline_statistics));
      break;
   default:
      memset(result, 0, sizeof(*result));
   }
}

/** Convert PIPE_TEXTURE_x to TGSI_TEXTURE_x */
static INLINE unsigned
util_pipe_tex_to_tgsi_tex(enum pipe_texture_target pipe_tex_target,
                          unsigned nr_samples)
{
   switch (pipe_tex_target) {
   case PIPE_TEXTURE_1D:
      assert(nr_samples <= 1);
      return TGSI_TEXTURE_1D;

   case PIPE_TEXTURE_2D:
      return nr_samples > 1 ? TGSI_TEXTURE_2D_MSAA : TGSI_TEXTURE_2D;

   case PIPE_TEXTURE_RECT:
      assert(nr_samples <= 1);
      return TGSI_TEXTURE_RECT;

   case PIPE_TEXTURE_3D:
      assert(nr_samples <= 1);
      return TGSI_TEXTURE_3D;

   case PIPE_TEXTURE_CUBE:
      assert(nr_samples <= 1);
      return TGSI_TEXTURE_CUBE;

   case PIPE_TEXTURE_1D_ARRAY:
      assert(nr_samples <= 1);
      return TGSI_TEXTURE_1D_ARRAY;

   case PIPE_TEXTURE_2D_ARRAY:
      return nr_samples > 1 ? TGSI_TEXTURE_2D_ARRAY_MSAA :
                              TGSI_TEXTURE_2D_ARRAY;

   case PIPE_TEXTURE_CUBE_ARRAY:
      return TGSI_TEXTURE_CUBE_ARRAY;

   default:
      assert(0 && "unexpected texture target");
      return TGSI_TEXTURE_UNKNOWN;
   }
}


static INLINE void
util_copy_constant_buffer(struct pipe_constant_buffer *dst,
                          const struct pipe_constant_buffer *src)
{
   if (src) {
      pipe_resource_reference(&dst->buffer, src->buffer);
      dst->buffer_offset = src->buffer_offset;
      dst->buffer_size = src->buffer_size;
      dst->user_buffer = src->user_buffer;
   }
   else {
      pipe_resource_reference(&dst->buffer, NULL);
      dst->buffer_offset = 0;
      dst->buffer_size = 0;
      dst->user_buffer = NULL;
   }
}

static INLINE unsigned
util_max_layer(const struct pipe_resource *r, unsigned level)
{
   switch (r->target) {
   case PIPE_TEXTURE_CUBE:
      return 6 - 1;
   case PIPE_TEXTURE_3D:
      return u_minify(r->depth0, level) - 1;
   case PIPE_TEXTURE_1D_ARRAY:
   case PIPE_TEXTURE_2D_ARRAY:
   case PIPE_TEXTURE_CUBE_ARRAY:
      return r->array_size - 1;
   default:
      return 0;
@


1.1.1.3
log
@Import Mesa 10.2.3
@
text
@d3 1
a3 1
 * Copyright 2007 VMware, Inc.
d21 1
a21 1
 * IN NO EVENT SHALL VMWARE AND/OR ITS SUPPLIERS BE LIABLE FOR
a229 6

/**
 * Create a new resource.
 * \param bind  bitmask of PIPE_BIND_x flags
 * \param usage  bitmask of PIPE_USAGE_x flags
 */
a249 8

/**
 * Map a range of a resource.
 * \param offset  start of region, in bytes 
 * \param length  size of region, in bytes 
 * \param access  bitmask of PIPE_TRANSFER_x flags
 * \param transfer  returns a transfer object
 */
d255 1
a255 1
		      unsigned access,
d267 1
a267 1
   map = pipe->transfer_map(pipe, buffer, 0, access, &box, transfer);
a275 5
/**
 * Map whole resource.
 * \param access  bitmask of PIPE_TRANSFER_x flags
 * \param transfer  returns a transfer object
 */
d279 1
a279 1
                unsigned access,
d282 1
a282 1
   return pipe_buffer_map_range(pipe, buffer, 0, buffer->width0, access, transfer);
d325 1
a325 1
   unsigned access = PIPE_TRANSFER_WRITE;
d328 1
a328 1
      access |= PIPE_TRANSFER_DISCARD_WHOLE_RESOURCE;
d330 1
a330 1
      access |= PIPE_TRANSFER_DISCARD_RANGE;
d338 1
a338 1
                                access,
a370 6

/**
 * Create a new resource and immediately put data into it
 * \param bind  bitmask of PIPE_BIND_x flags
 * \param usage  bitmask of PIPE_USAGE_x flags
 */
d376 1
a376 1
                             const void *ptr)
a405 5

/**
 * Map a resource for reading/writing.
 * \param access  bitmask of PIPE_TRANSFER_x flags
 */
d410 1
a410 1
                  unsigned access,
d420 1
a420 1
                                access,
a423 5

/**
 * Map a 3D (texture) resource for reading/writing.
 * \param access  bitmask of PIPE_TRANSFER_x flags
 */
d428 1
a428 1
                     unsigned access,
d438 1
a438 1
                                access,
d466 3
a468 7
/**
 * Get the polygon offset enable/disable flag for the given polygon fill mode.
 * \param fill_mode  one of PIPE_POLYGON_MODE_POINT/LINE/FILL
 */
static INLINE boolean
util_get_offset(const struct pipe_rasterizer_state *templ,
                unsigned fill_mode)
@


1.1.1.4
log
@Import Mesa 10.4.3
@
text
@a567 3
   case PIPE_BUFFER:
      return TGSI_TEXTURE_BUFFER;

d627 2
a630 3
   case PIPE_TEXTURE_CUBE:
      assert(r->array_size == 6);
      /* fall-through */
@


1.1.1.5
log
@Import Mesa 10.2.9
@
text
@d568 3
a629 2
   case PIPE_TEXTURE_CUBE:
      return 6 - 1;
d632 3
@


