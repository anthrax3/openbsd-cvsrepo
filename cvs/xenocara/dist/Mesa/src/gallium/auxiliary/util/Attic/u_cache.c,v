head	1.5;
access;
symbols
	OPENBSD_5_8:1.4.0.8
	OPENBSD_5_8_BASE:1.4
	OPENBSD_5_7:1.4.0.6
	OPENBSD_5_7_BASE:1.4
	v10_2_9:1.1.1.2
	v10_4_3:1.1.1.2
	v10_2_7:1.1.1.2
	OPENBSD_5_6:1.4.0.4
	OPENBSD_5_6_BASE:1.4
	v10_2_3:1.1.1.2
	OPENBSD_5_5:1.4.0.2
	OPENBSD_5_5_BASE:1.4
	v9_2_5:1.1.1.2
	v9_2_3:1.1.1.2
	v9_2_2:1.1.1.2
	v9_2_1:1.1.1.2
	v9_2_0:1.1.1.2
	OPENBSD_5_4:1.3.0.4
	OPENBSD_5_4_BASE:1.3
	OPENBSD_5_3:1.3.0.2
	OPENBSD_5_3_BASE:1.3
	OPENBSD_5_2:1.2.0.4
	OPENBSD_5_2_BASE:1.2
	OPENBSD_5_1_BASE:1.2
	OPENBSD_5_1:1.2.0.2
	v7_10_3:1.1.1.1
	mesa:1.1.1
	OPENBSD_5_0:1.1.0.6
	OPENBSD_5_0_BASE:1.1
	OPENBSD_4_9:1.1.0.2
	OPENBSD_4_9_BASE:1.1
	OPENBSD_4_8:1.1.0.4
	OPENBSD_4_8_BASE:1.1;
locks; strict;
comment	@ * @;


1.5
date	2015.12.23.05.17.28;	author jsg;	state dead;
branches;
next	1.4;
commitid	TnlogFl9nOv2eaRf;

1.4
date	2013.09.05.13.59.47;	author jsg;	state Exp;
branches;
next	1.3;

1.3
date	2012.08.17.13.58.04;	author mpi;	state Exp;
branches;
next	1.2;

1.2
date	2011.10.23.13.37.32;	author matthieu;	state Exp;
branches;
next	1.1;

1.1
date	2010.05.22.20.06.04;	author matthieu;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2011.10.23.13.29.25;	author matthieu;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2013.09.05.13.10.45;	author jsg;	state Exp;
branches;
next	;


desc
@@


1.5
log
@remove the now unused Mesa 10.2.9 code
@
text
@/**************************************************************************
 *
 * Copyright 2008 VMware, Inc.
 * All Rights Reserved.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sub license, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice (including the
 * next paragraph) shall be included in all copies or substantial portions
 * of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
 * IN NO EVENT SHALL VMWARE AND/OR ITS SUPPLIERS BE LIABLE FOR
 * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *
 **************************************************************************/

/**
 * @@file
 * Improved cache implementation.
 *
 * Fixed size array with linear probing on collision and LRU eviction
 * on full.
 * 
 * @@author Jose Fonseca <jfonseca@@vmware.com>
 */


#include "pipe/p_compiler.h"
#include "util/u_debug.h"

#include "util/u_math.h"
#include "util/u_memory.h"
#include "util/u_cache.h"
#include "util/u_simple_list.h"


struct util_cache_entry
{
   enum { EMPTY = 0, FILLED, DELETED } state;
   uint32_t hash;

   struct util_cache_entry *next;
   struct util_cache_entry *prev;

   void *key;
   void *value;
   
#ifdef DEBUG
   unsigned count;
#endif
};


struct util_cache
{
   /** Hash function */
   uint32_t (*hash)(const void *key);
   
   /** Compare two keys */
   int (*compare)(const void *key1, const void *key2);

   /** Destroy a (key, value) pair */
   void (*destroy)(void *key, void *value);

   uint32_t size;
   
   struct util_cache_entry *entries;
   
   unsigned count;
   struct util_cache_entry lru;
};

static void
ensure_sanity(const struct util_cache *cache);

#define CACHE_DEFAULT_ALPHA 2

struct util_cache *
util_cache_create(uint32_t (*hash)(const void *key),
                  int (*compare)(const void *key1, const void *key2),
                  void (*destroy)(void *key, void *value),
                  uint32_t size)
{
   struct util_cache *cache;
   
   cache = CALLOC_STRUCT(util_cache);
   if(!cache)
      return NULL;
   
   cache->hash = hash;
   cache->compare = compare;
   cache->destroy = destroy;

   make_empty_list(&cache->lru);

   size *= CACHE_DEFAULT_ALPHA;
   cache->size = size;
   
   cache->entries = CALLOC(size, sizeof(struct util_cache_entry));
   if(!cache->entries) {
      FREE(cache);
      return NULL;
   }
   
   ensure_sanity(cache);
   return cache;
}


static struct util_cache_entry *
util_cache_entry_get(struct util_cache *cache,
                     uint32_t hash,
                     const void *key)
{
   struct util_cache_entry *first_unfilled = NULL;
   uint32_t index = hash % cache->size;
   uint32_t probe;

   /* Probe until we find either a matching FILLED entry or an EMPTY
    * slot (which has never been occupied).
    *
    * Deleted or non-matching slots are not indicative of completion
    * as a previous linear probe for the same key could have continued
    * past this point.
    */
   for (probe = 0; probe < cache->size; probe++) {
      uint32_t i = (index + probe) % cache->size;
      struct util_cache_entry *current = &cache->entries[i];

      if (current->state == FILLED) {
         if (current->hash == hash &&
             cache->compare(key, current->key) == 0)
            return current;
      }
      else {
         if (!first_unfilled)
            first_unfilled = current;

         if (current->state == EMPTY)
            return first_unfilled;
      }
   }

   return NULL;
}

static INLINE void
util_cache_entry_destroy(struct util_cache *cache,
                         struct util_cache_entry *entry)
{
   void *key = entry->key;
   void *value = entry->value;
   
   entry->key = NULL;
   entry->value = NULL;
   
   if (entry->state == FILLED) {
      remove_from_list(entry);
      cache->count--;

      if(cache->destroy)
         cache->destroy(key, value);

      entry->state = DELETED;
   }
}


void
util_cache_set(struct util_cache *cache,
               void *key,
               void *value)
{
   struct util_cache_entry *entry;
   uint32_t hash;

   assert(cache);
   if (!cache)
      return;
   hash = cache->hash(key);
   entry = util_cache_entry_get(cache, hash, key);
   if (!entry)
      entry = cache->lru.prev;

   if (cache->count >= cache->size / CACHE_DEFAULT_ALPHA)
      util_cache_entry_destroy(cache, cache->lru.prev);

   util_cache_entry_destroy(cache, entry);
   
#ifdef DEBUG
   ++entry->count;
#endif
   
   entry->key = key;
   entry->hash = hash;
   entry->value = value;
   entry->state = FILLED;
   insert_at_head(&cache->lru, entry);
   cache->count++;

   ensure_sanity(cache);
}


void *
util_cache_get(struct util_cache *cache, 
               const void *key)
{
   struct util_cache_entry *entry;
   uint32_t hash;

   assert(cache);
   if (!cache)
      return NULL;
   hash = cache->hash(key);
   entry = util_cache_entry_get(cache, hash, key);
   if (!entry)
      return NULL;

   if (entry->state == FILLED)
      move_to_head(&cache->lru, entry);
   
   return entry->value;
}


void 
util_cache_clear(struct util_cache *cache)
{
   uint32_t i;

   assert(cache);
   if (!cache)
      return;

   for(i = 0; i < cache->size; ++i) {
      util_cache_entry_destroy(cache, &cache->entries[i]);
      cache->entries[i].state = EMPTY;
   }

   assert(cache->count == 0);
   assert(is_empty_list(&cache->lru));
   ensure_sanity(cache);
}


void
util_cache_destroy(struct util_cache *cache)
{
   assert(cache);
   if (!cache)
      return;

#ifdef DEBUG
   if(cache->count >= 20*cache->size) {
      /* Normal approximation of the Poisson distribution */
      double mean = (double)cache->count/(double)cache->size;
      double stddev = sqrt(mean);
      unsigned i;
      for(i = 0; i < cache->size; ++i) {
         double z = fabs(cache->entries[i].count - mean)/stddev;
         /* This assert should not fail 99.9999998027% of the times, unless 
          * the hash function is a poor one */
         assert(z <= 6.0);
      }
   }
#endif
      
   util_cache_clear(cache);
   
   FREE(cache->entries);
   FREE(cache);
}


void
util_cache_remove(struct util_cache *cache,
                  const void *key)
{
   struct util_cache_entry *entry;
   uint32_t hash;

   assert(cache);
   if (!cache)
      return;

   hash = cache->hash(key);

   entry = util_cache_entry_get(cache, hash, key);
   if (!entry)
      return;

   if (entry->state == FILLED)
      util_cache_entry_destroy(cache, entry);

   ensure_sanity(cache);
}


static void
ensure_sanity(const struct util_cache *cache)
{
#ifdef DEBUG
   unsigned i, cnt = 0;

   assert(cache);
   for (i = 0; i < cache->size; i++) {
      struct util_cache_entry *header = &cache->entries[i];

      assert(header);
      assert(header->state == FILLED ||
             header->state == EMPTY ||
             header->state == DELETED);
      if (header->state == FILLED) {
         cnt++;
         assert(header->hash == cache->hash(header->key));
      }
   }

   assert(cnt == cache->count);
   assert(cache->size >= cnt);

   if (cache->count == 0) {
      assert (is_empty_list(&cache->lru));
   }
   else {
      struct util_cache_entry *header = cache->lru.next;

      assert (header);
      assert (!is_empty_list(&cache->lru));

      for (i = 0; i < cache->count; i++)
         header = header->next;

      assert(header == &cache->lru);
   }
#endif

   (void)cache;
}
@


1.4
log
@Merge Mesa 9.2.0
@
text
@@


1.3
log
@Upate to libGL 7.11.2

Tested by jsg@@, matthieu@@ and ajacoutot@@, ok mattieu@@
@
text
@d186 1
a186 1
   uint32_t hash = cache->hash(key);
d191 1
a191 1

d221 1
a221 1
   uint32_t hash = cache->hash(key);
d226 1
a226 1

@


1.2
log
@Merge Mesa 7.10.3
@
text
@d30 1
a30 1
 * Simple cache implementation.
d32 2
a33 1
 * We simply have fixed size array, and destroy previous values on collision. 
d45 1
d50 6
a79 1
#ifdef DEBUG
d81 1
a81 1
#endif
d84 4
d104 4
d116 1
d121 1
a121 1
static INLINE struct util_cache_entry *
d123 1
d126 28
a153 3
   uint32_t hash;
   
   hash = cache->hash(key);
d155 1
a155 1
   return &cache->entries[hash % cache->size];
d168 4
a171 1
   if(key || value)
d174 3
d186 1
d192 7
a198 1
   entry = util_cache_entry_get(cache, key);
a202 1
   ++cache->count;
d206 1
d208 5
d221 1
d227 2
a228 5
   entry = util_cache_entry_get(cache, key);
   if(!entry->key && !entry->value)
      return NULL;
   
   if(cache->compare(key, entry->key) != 0)
d230 3
d247 1
a247 1
   for(i = 0; i < cache->size; ++i)
d249 6
d284 67
@


1.1
log
@Update to Mesa 7.8.1. Tested on a bulk ports build by naddy@@, ok oga@@.
@
text
@d205 1
a205 1
         double z = fabs(cache->count - mean)/stddev;
@


1.1.1.1
log
@Import Mesa 7.10.3
@
text
@d205 1
a205 1
         double z = fabs(cache->entries[i].count - mean)/stddev;
@


1.1.1.2
log
@Import Mesa 9.2.0
@
text
@d30 1
a30 1
 * Improved cache implementation.
d32 1
a32 2
 * Fixed size array with linear probing on collision and LRU eviction
 * on full.
a43 1
#include "util/u_simple_list.h"
a47 6
   enum { EMPTY = 0, FILLED, DELETED } state;
   uint32_t hash;

   struct util_cache_entry *next;
   struct util_cache_entry *prev;

d72 1
d74 1
a74 1
   struct util_cache_entry lru;
a76 4
static void
ensure_sanity(const struct util_cache *cache);

#define CACHE_DEFAULT_ALPHA 2
a92 4

   make_empty_list(&cache->lru);

   size *= CACHE_DEFAULT_ALPHA;
a100 1
   ensure_sanity(cache);
d105 1
a105 1
static struct util_cache_entry *
a106 1
                     uint32_t hash,
d109 3
a111 28
   struct util_cache_entry *first_unfilled = NULL;
   uint32_t index = hash % cache->size;
   uint32_t probe;

   /* Probe until we find either a matching FILLED entry or an EMPTY
    * slot (which has never been occupied).
    *
    * Deleted or non-matching slots are not indicative of completion
    * as a previous linear probe for the same key could have continued
    * past this point.
    */
   for (probe = 0; probe < cache->size; probe++) {
      uint32_t i = (index + probe) % cache->size;
      struct util_cache_entry *current = &cache->entries[i];

      if (current->state == FILLED) {
         if (current->hash == hash &&
             cache->compare(key, current->key) == 0)
            return current;
      }
      else {
         if (!first_unfilled)
            first_unfilled = current;

         if (current->state == EMPTY)
            return first_unfilled;
      }
   }
d113 1
a113 1
   return NULL;
d126 1
a126 4
   if (entry->state == FILLED) {
      remove_from_list(entry);
      cache->count--;

a128 3

      entry->state = DELETED;
   }
a137 1
   uint32_t hash;
a141 7
   hash = cache->hash(key);
   entry = util_cache_entry_get(cache, hash, key);
   if (!entry)
      entry = cache->lru.prev;

   if (cache->count >= cache->size / CACHE_DEFAULT_ALPHA)
      util_cache_entry_destroy(cache, cache->lru.prev);
d143 1
d148 1
a151 1
   entry->hash = hash;
a152 5
   entry->state = FILLED;
   insert_at_head(&cache->lru, entry);
   cache->count++;

   ensure_sanity(cache);
a160 1
   uint32_t hash;
d165 6
a170 3
   hash = cache->hash(key);
   entry = util_cache_entry_get(cache, hash, key);
   if (!entry)
a171 3

   if (entry->state == FILLED)
      move_to_head(&cache->lru, entry);
d186 1
a186 1
   for(i = 0; i < cache->size; ++i) {
a187 6
      cache->entries[i].state = EMPTY;
   }

   assert(cache->count == 0);
   assert(is_empty_list(&cache->lru));
   ensure_sanity(cache);
a216 67
}


void
util_cache_remove(struct util_cache *cache,
                  const void *key)
{
   struct util_cache_entry *entry;
   uint32_t hash;

   assert(cache);
   if (!cache)
      return;

   hash = cache->hash(key);

   entry = util_cache_entry_get(cache, hash, key);
   if (!entry)
      return;

   if (entry->state == FILLED)
      util_cache_entry_destroy(cache, entry);

   ensure_sanity(cache);
}


static void
ensure_sanity(const struct util_cache *cache)
{
#ifdef DEBUG
   unsigned i, cnt = 0;

   assert(cache);
   for (i = 0; i < cache->size; i++) {
      struct util_cache_entry *header = &cache->entries[i];

      assert(header);
      assert(header->state == FILLED ||
             header->state == EMPTY ||
             header->state == DELETED);
      if (header->state == FILLED) {
         cnt++;
         assert(header->hash == cache->hash(header->key));
      }
   }

   assert(cnt == cache->count);
   assert(cache->size >= cnt);

   if (cache->count == 0) {
      assert (is_empty_list(&cache->lru));
   }
   else {
      struct util_cache_entry *header = cache->lru.next;

      assert (header);
      assert (!is_empty_list(&cache->lru));

      for (i = 0; i < cache->count; i++)
         header = header->next;

      assert(header == &cache->lru);
   }
#endif

   (void)cache;
@


