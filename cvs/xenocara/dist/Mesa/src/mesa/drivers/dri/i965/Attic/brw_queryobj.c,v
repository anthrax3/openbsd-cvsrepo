head	1.9;
access;
symbols
	OPENBSD_5_8:1.8.0.4
	OPENBSD_5_8_BASE:1.8
	OPENBSD_5_7:1.8.0.2
	OPENBSD_5_7_BASE:1.8
	v10_2_9:1.1.1.5
	v10_4_3:1.1.1.4
	v10_2_7:1.1.1.3
	OPENBSD_5_6:1.6.0.2
	OPENBSD_5_6_BASE:1.6
	v10_2_3:1.1.1.3
	OPENBSD_5_5:1.5.0.2
	OPENBSD_5_5_BASE:1.5
	v9_2_5:1.1.1.2
	v9_2_3:1.1.1.2
	v9_2_2:1.1.1.2
	v9_2_1:1.1.1.2
	v9_2_0:1.1.1.2
	OPENBSD_5_4:1.4.0.4
	OPENBSD_5_4_BASE:1.4
	OPENBSD_5_3:1.4.0.2
	OPENBSD_5_3_BASE:1.4
	OPENBSD_5_2:1.3.0.4
	OPENBSD_5_2_BASE:1.3
	OPENBSD_5_1_BASE:1.3
	OPENBSD_5_1:1.3.0.2
	v7_10_3:1.1.1.1
	mesa:1.1.1
	OPENBSD_5_0:1.2.0.6
	OPENBSD_5_0_BASE:1.2
	OPENBSD_4_9:1.2.0.2
	OPENBSD_4_9_BASE:1.2
	OPENBSD_4_8:1.2.0.4
	OPENBSD_4_8_BASE:1.2
	OPENBSD_4_7:1.1.0.4
	OPENBSD_4_7_BASE:1.1
	OPENBSD_4_6:1.1.0.2
	OPENBSD_4_6_BASE:1.1;
locks; strict;
comment	@ * @;


1.9
date	2015.12.23.05.17.49;	author jsg;	state dead;
branches;
next	1.8;
commitid	TnlogFl9nOv2eaRf;

1.8
date	2015.02.20.23.09.58;	author jsg;	state Exp;
branches;
next	1.7;
commitid	4ry2gvZGMXkCUD2n;

1.7
date	2015.01.25.14.41.21;	author jsg;	state Exp;
branches;
next	1.6;
commitid	mcxB0JvoI9gTDYXU;

1.6
date	2014.07.09.21.08.59;	author jsg;	state Exp;
branches;
next	1.5;
commitid	WPD6rgPryPkvXOr9;

1.5
date	2013.09.05.14.04.19;	author jsg;	state Exp;
branches;
next	1.4;

1.4
date	2012.08.17.13.58.15;	author mpi;	state Exp;
branches;
next	1.3;

1.3
date	2011.10.23.13.37.39;	author matthieu;	state Exp;
branches;
next	1.2;

1.2
date	2010.05.22.20.06.18;	author matthieu;	state Exp;
branches;
next	1.1;

1.1
date	2009.05.17.20.26.39;	author matthieu;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2011.10.23.13.29.36;	author matthieu;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2013.09.05.13.15.36;	author jsg;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2014.07.09.20.34.50;	author jsg;	state Exp;
branches;
next	1.1.1.4;
commitid	3JhLfwcuBALP0ZR7;

1.1.1.4
date	2015.01.25.14.11.43;	author jsg;	state Exp;
branches;
next	1.1.1.5;
commitid	ce2W5rH5aF7VS9gi;

1.1.1.5
date	2015.02.20.22.48.48;	author jsg;	state Exp;
branches;
next	;
commitid	F54a1i0WXHMxq7kE;


desc
@@


1.9
log
@remove the now unused Mesa 10.2.9 code
@
text
@/*
 * Copyright Â© 2008 Intel Corporation
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice (including the next
 * paragraph) shall be included in all copies or substantial portions of the
 * Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
 * IN THE SOFTWARE.
 *
 * Authors:
 *    Eric Anholt <eric@@anholt.net>
 *
 */

/** @@file brw_queryobj.c
 *
 * Support for query objects (GL_ARB_occlusion_query, GL_ARB_timer_query,
 * GL_EXT_transform_feedback, and friends).
 *
 * The hardware provides a PIPE_CONTROL command that can report the number of
 * fragments that passed the depth test, or the hardware timer.  They are
 * appropriately synced with the stage of the pipeline for our extensions'
 * needs.
 */
#include "main/imports.h"

#include "brw_context.h"
#include "brw_defines.h"
#include "brw_state.h"
#include "intel_batchbuffer.h"
#include "intel_reg.h"

/**
 * Emit PIPE_CONTROLs to write the current GPU timestamp into a buffer.
 */
void
brw_write_timestamp(struct brw_context *brw, drm_intel_bo *query_bo, int idx)
{
   if (brw->gen == 6) {
      /* Emit Sandybridge workaround flush: */
      brw_emit_pipe_control_flush(brw,
                                  PIPE_CONTROL_CS_STALL |
                                  PIPE_CONTROL_STALL_AT_SCOREBOARD);
   }

   brw_emit_pipe_control_write(brw, PIPE_CONTROL_WRITE_TIMESTAMP,
                               query_bo, idx * sizeof(uint64_t), 0, 0);
}

/**
 * Emit PIPE_CONTROLs to write the PS_DEPTH_COUNT register into a buffer.
 */
void
brw_write_depth_count(struct brw_context *brw, drm_intel_bo *query_bo, int idx)
{
   /* Emit Sandybridge workaround flush: */
   if (brw->gen == 6)
      intel_emit_post_sync_nonzero_flush(brw);

   brw_emit_pipe_control_write(brw,
                               PIPE_CONTROL_WRITE_DEPTH_COUNT
                               | PIPE_CONTROL_DEPTH_STALL,
                               query_bo, idx * sizeof(uint64_t), 0, 0);
}

/**
 * Wait on the query object's BO and calculate the final result.
 */
static void
brw_queryobj_get_results(struct gl_context *ctx,
			 struct brw_query_object *query)
{
   struct brw_context *brw = brw_context(ctx);

   int i;
   uint64_t *results;

   assert(brw->gen < 6);

   if (query->bo == NULL)
      return;

   /* If the application has requested the query result, but this batch is
    * still contributing to it, flush it now so the results will be present
    * when mapped.
    */
   if (drm_intel_bo_references(brw->batch.bo, query->bo))
      intel_batchbuffer_flush(brw);

   if (unlikely(brw->perf_debug)) {
      if (drm_intel_bo_busy(query->bo)) {
         perf_debug("Stalling on the GPU waiting for a query object.\n");
      }
   }

   drm_intel_bo_map(query->bo, false);
   results = query->bo->virtual;
   switch (query->Base.Target) {
   case GL_TIME_ELAPSED_EXT:
      /* The query BO contains the starting and ending timestamps.
       * Subtract the two and convert to nanoseconds.
       */
      query->Base.Result += 1000 * ((results[1] >> 32) - (results[0] >> 32));
      break;

   case GL_TIMESTAMP:
      /* The query BO contains a single timestamp value in results[0]. */
      query->Base.Result = 1000 * (results[0] >> 32);
      break;

   case GL_SAMPLES_PASSED_ARB:
      /* Loop over pairs of values from the BO, which are the PS_DEPTH_COUNT
       * value at the start and end of the batchbuffer.  Subtract them to
       * get the number of fragments which passed the depth test in each
       * individual batch, and add those differences up to get the number
       * of fragments for the entire query.
       *
       * Note that query->Base.Result may already be non-zero.  We may have
       * run out of space in the query's BO and allocated a new one.  If so,
       * this function was already called to accumulate the results so far.
       */
      for (i = 0; i < query->last_index; i++) {
	 query->Base.Result += results[i * 2 + 1] - results[i * 2];
      }
      break;

   case GL_ANY_SAMPLES_PASSED:
   case GL_ANY_SAMPLES_PASSED_CONSERVATIVE:
      /* If the starting and ending PS_DEPTH_COUNT from any of the batches
       * differ, then some fragments passed the depth test.
       */
      for (i = 0; i < query->last_index; i++) {
	 if (results[i * 2 + 1] != results[i * 2]) {
            query->Base.Result = GL_TRUE;
            break;
         }
      }
      break;

   default:
      assert(!"Unrecognized query target in brw_queryobj_get_results()");
      break;
   }
   drm_intel_bo_unmap(query->bo);

   /* Now that we've processed the data stored in the query's buffer object,
    * we can release it.
    */
   drm_intel_bo_unreference(query->bo);
   query->bo = NULL;
}

/**
 * The NewQueryObject() driver hook.
 *
 * Allocates and initializes a new query object.
 */
static struct gl_query_object *
brw_new_query_object(struct gl_context *ctx, GLuint id)
{
   struct brw_query_object *query;

   query = calloc(1, sizeof(struct brw_query_object));

   query->Base.Id = id;
   query->Base.Result = 0;
   query->Base.Active = false;
   query->Base.Ready = true;

   return &query->Base;
}

/**
 * The DeleteQuery() driver hook.
 */
static void
brw_delete_query(struct gl_context *ctx, struct gl_query_object *q)
{
   struct brw_query_object *query = (struct brw_query_object *)q;

   drm_intel_bo_unreference(query->bo);
   free(query);
}

/**
 * Gen4-5 driver hook for glBeginQuery().
 *
 * Initializes driver structures and emits any GPU commands required to begin
 * recording data for the query.
 */
static void
brw_begin_query(struct gl_context *ctx, struct gl_query_object *q)
{
   struct brw_context *brw = brw_context(ctx);
   struct brw_query_object *query = (struct brw_query_object *)q;

   assert(brw->gen < 6);

   switch (query->Base.Target) {
   case GL_TIME_ELAPSED_EXT:
      /* For timestamp queries, we record the starting time right away so that
       * we measure the full time between BeginQuery and EndQuery.  There's
       * some debate about whether this is the right thing to do.  Our decision
       * is based on the following text from the ARB_timer_query extension:
       *
       * "(5) Should the extension measure total time elapsed between the full
       *      completion of the BeginQuery and EndQuery commands, or just time
       *      spent in the graphics library?
       *
       *  RESOLVED:  This extension will measure the total time elapsed
       *  between the full completion of these commands.  Future extensions
       *  may implement a query to determine time elapsed at different stages
       *  of the graphics pipeline."
       *
       * We write a starting timestamp now (at index 0).  At EndQuery() time,
       * we'll write a second timestamp (at index 1), and subtract the two to
       * obtain the time elapsed.  Notably, this includes time elapsed while
       * the system was doing other work, such as running other applications.
       */
      drm_intel_bo_unreference(query->bo);
      query->bo = drm_intel_bo_alloc(brw->bufmgr, "timer query", 4096, 4096);
      brw_write_timestamp(brw, query->bo, 0);
      break;

   case GL_ANY_SAMPLES_PASSED:
   case GL_ANY_SAMPLES_PASSED_CONSERVATIVE:
   case GL_SAMPLES_PASSED_ARB:
      /* For occlusion queries, we delay taking an initial sample until the
       * first drawing occurs in this batch.  See the reasoning in the comments
       * for brw_emit_query_begin() below.
       *
       * Since we're starting a new query, we need to be sure to throw away
       * any previous occlusion query results.
       */
      drm_intel_bo_unreference(query->bo);
      query->bo = NULL;
      query->last_index = -1;

      brw->query.obj = query;

      /* Depth statistics on Gen4 require strange workarounds, so we try to
       * avoid them when necessary.  They're required for occlusion queries,
       * so turn them on now.
       */
      brw->stats_wm++;
      brw->state.dirty.brw |= BRW_NEW_STATS_WM;
      break;

   default:
      assert(!"Unrecognized query target in brw_begin_query()");
      break;
   }
}

/**
 * Gen4-5 driver hook for glEndQuery().
 *
 * Emits GPU commands to record a final query value, ending any data capturing.
 * However, the final result isn't necessarily available until the GPU processes
 * those commands.  brw_queryobj_get_results() processes the captured data to
 * produce the final result.
 */
static void
brw_end_query(struct gl_context *ctx, struct gl_query_object *q)
{
   struct brw_context *brw = brw_context(ctx);
   struct brw_query_object *query = (struct brw_query_object *)q;

   assert(brw->gen < 6);

   switch (query->Base.Target) {
   case GL_TIME_ELAPSED_EXT:
      /* Write the final timestamp. */
      brw_write_timestamp(brw, query->bo, 1);
      break;

   case GL_ANY_SAMPLES_PASSED:
   case GL_ANY_SAMPLES_PASSED_CONSERVATIVE:
   case GL_SAMPLES_PASSED_ARB:

      /* No query->bo means that EndQuery was called after BeginQuery with no
       * intervening drawing. Rather than doing nothing at all here in this
       * case, we emit the query_begin and query_end state to the
       * hardware. This is to guarantee that waiting on the result of this
       * empty state will cause all previous queries to complete at all, as
       * required by the specification:
       *
       * 	It must always be true that if any query object
       *	returns a result available of TRUE, all queries of the
       *	same type issued prior to that query must also return
       *	TRUE. [Open GL 4.3 (Core Profile) Section 4.2.1]
       */
      if (!query->bo) {
         brw_emit_query_begin(brw);
      }

      assert(query->bo);

      brw_emit_query_end(brw);

      brw->query.obj = NULL;

      brw->stats_wm--;
      brw->state.dirty.brw |= BRW_NEW_STATS_WM;
      break;

   default:
      assert(!"Unrecognized query target in brw_end_query()");
      break;
   }
}

/**
 * The Gen4-5 WaitQuery() driver hook.
 *
 * Wait for a query result to become available and return it.  This is the
 * backing for glGetQueryObjectiv() with the GL_QUERY_RESULT pname.
 */
static void brw_wait_query(struct gl_context *ctx, struct gl_query_object *q)
{
   struct brw_query_object *query = (struct brw_query_object *)q;

   assert(brw_context(ctx)->gen < 6);

   brw_queryobj_get_results(ctx, query);
   query->Base.Ready = true;
}

/**
 * The Gen4-5 CheckQuery() driver hook.
 *
 * Checks whether a query result is ready yet.  If not, flushes.
 * This is the backing for glGetQueryObjectiv()'s QUERY_RESULT_AVAILABLE pname.
 */
static void brw_check_query(struct gl_context *ctx, struct gl_query_object *q)
{
   struct brw_context *brw = brw_context(ctx);
   struct brw_query_object *query = (struct brw_query_object *)q;

   assert(brw->gen < 6);

   /* From the GL_ARB_occlusion_query spec:
    *
    *     "Instead of allowing for an infinite loop, performing a
    *      QUERY_RESULT_AVAILABLE_ARB will perform a flush if the result is
    *      not ready yet on the first time it is queried.  This ensures that
    *      the async query will return true in finite time.
    */
   if (query->bo && drm_intel_bo_references(brw->batch.bo, query->bo))
      intel_batchbuffer_flush(brw);

   if (query->bo == NULL || !drm_intel_bo_busy(query->bo)) {
      brw_queryobj_get_results(ctx, query);
      query->Base.Ready = true;
   }
}

/**
 * Ensure there query's BO has enough space to store a new pair of values.
 *
 * If not, gather the existing BO's results and create a new buffer of the
 * same size.
 */
static void
ensure_bo_has_space(struct gl_context *ctx, struct brw_query_object *query)
{
   struct brw_context *brw = brw_context(ctx);

   assert(brw->gen < 6);

   if (!query->bo || query->last_index * 2 + 1 >= 4096 / sizeof(uint64_t)) {

      if (query->bo != NULL) {
         /* The old query BO did not have enough space, so we allocated a new
          * one.  Gather the results so far (adding up the differences) and
          * release the old BO.
          */
         brw_queryobj_get_results(ctx, query);
      }

      query->bo = drm_intel_bo_alloc(brw->bufmgr, "query", 4096, 1);
      query->last_index = 0;
   }
}

/**
 * Record the PS_DEPTH_COUNT value (for occlusion queries) just before
 * primitive drawing.
 *
 * In a pre-hardware context world, the single PS_DEPTH_COUNT register is
 * shared among all applications using the GPU.  However, our query value
 * needs to only include fragments generated by our application/GL context.
 *
 * To accommodate this, we record PS_DEPTH_COUNT at the start and end of
 * each batchbuffer (technically, the first primitive drawn and flush time).
 * Subtracting each pair of values calculates the change in PS_DEPTH_COUNT
 * caused by a batchbuffer.  Since there is no preemption inside batches,
 * this is guaranteed to only measure the effects of our current application.
 *
 * Adding each of these differences (in case drawing is done over many batches)
 * produces the final expected value.
 *
 * In a world with hardware contexts, PS_DEPTH_COUNT is saved and restored
 * as part of the context state, so this is unnecessary, and skipped.
 */
void
brw_emit_query_begin(struct brw_context *brw)
{
   struct gl_context *ctx = &brw->ctx;
   struct brw_query_object *query = brw->query.obj;

   if (brw->hw_ctx)
      return;

   /* Skip if we're not doing any queries, or we've already recorded the
    * initial query value for this batchbuffer.
    */
   if (!query || brw->query.begin_emitted)
      return;

   ensure_bo_has_space(ctx, query);

   brw_write_depth_count(brw, query->bo, query->last_index * 2);

   brw->query.begin_emitted = true;
}

/**
 * Called at batchbuffer flush to get an ending PS_DEPTH_COUNT
 * (for non-hardware context platforms).
 *
 * See the explanation in brw_emit_query_begin().
 */
void
brw_emit_query_end(struct brw_context *brw)
{
   struct brw_query_object *query = brw->query.obj;

   if (brw->hw_ctx)
      return;

   if (!brw->query.begin_emitted)
      return;

   brw_write_depth_count(brw, query->bo, query->last_index * 2 + 1);

   brw->query.begin_emitted = false;
   query->last_index++;
}

/**
 * Driver hook for glQueryCounter().
 *
 * This handles GL_TIMESTAMP queries, which perform a pipelined read of the
 * current GPU time.  This is unlike GL_TIME_ELAPSED, which measures the
 * time while the query is active.
 */
static void
brw_query_counter(struct gl_context *ctx, struct gl_query_object *q)
{
   struct brw_context *brw = brw_context(ctx);
   struct brw_query_object *query = (struct brw_query_object *) q;

   assert(q->Target == GL_TIMESTAMP);

   drm_intel_bo_unreference(query->bo);
   query->bo = drm_intel_bo_alloc(brw->bufmgr, "timestamp query", 4096, 4096);
   brw_write_timestamp(brw, query->bo, 0);
}

/**
 * Read the TIMESTAMP register immediately (in a non-pipelined fashion).
 *
 * This is used to implement the GetTimestamp() driver hook.
 */
static uint64_t
brw_get_timestamp(struct gl_context *ctx)
{
   struct brw_context *brw = brw_context(ctx);
   uint64_t result = 0;

   drm_intel_reg_read(brw->bufmgr, TIMESTAMP, &result);

   /* See logic in brw_queryobj_get_results() */
   result = result >> 32;
   result *= 80;
   result &= (1ull << 36) - 1;

   return result;
}

/* Initialize query object functions used on all generations. */
void brw_init_common_queryobj_functions(struct dd_function_table *functions)
{
   functions->NewQueryObject = brw_new_query_object;
   functions->DeleteQuery = brw_delete_query;
   functions->QueryCounter = brw_query_counter;
   functions->GetTimestamp = brw_get_timestamp;
}

/* Initialize Gen4/5-specific query object functions. */
void gen4_init_queryobj_functions(struct dd_function_table *functions)
{
   functions->BeginQuery = brw_begin_query;
   functions->EndQuery = brw_end_query;
   functions->CheckQuery = brw_check_query;
   functions->WaitQuery = brw_wait_query;
}
@


1.8
log
@Merge Mesa 10.2.9
@
text
@@


1.7
log
@Merge Mesa 10.4.3
Tested by matthieu@@ mpi@@ and myself.  landry@@ ran a ports bulk build.
kettenis@@ tracked down the cause of an alignment fault on archs
that require strict eight byte pointer alignment.
@
text
@d154 2
a155 1
      unreachable("Unrecognized query target in brw_queryobj_get_results()");
d263 2
a264 1
      unreachable("Unrecognized query target in brw_begin_query()");
d321 2
a322 1
      unreachable("Unrecognized query target in brw_end_query()");
@


1.6
log
@Merge Mesa 10.2.3
tested by matthieu@@ kettenis@@ mpi@@ brett@@ and myself across a
diverse range of hardware
@
text
@d154 1
a154 2
      assert(!"Unrecognized query target in brw_queryobj_get_results()");
      break;
d262 1
a262 2
      assert(!"Unrecognized query target in brw_begin_query()");
      break;
d319 1
a319 2
      assert(!"Unrecognized query target in brw_end_query()");
      break;
@


1.5
log
@Merge Mesa 9.2.0
@
text
@d49 2
a50 2
static void
write_timestamp(struct brw_context *brw, drm_intel_bo *query_bo, int idx)
d52 6
a57 14
   if (brw->gen >= 6) {
      /* Emit workaround flushes: */
      if (brw->gen == 6) {
         /* The timestamp write below is a non-zero post-sync op, which on
          * Gen6 necessitates a CS stall.  CS stalls need stall at scoreboard
          * set.  See the comments for intel_emit_post_sync_nonzero_flush().
          */
         BEGIN_BATCH(4);
         OUT_BATCH(_3DSTATE_PIPE_CONTROL | (4 - 2));
         OUT_BATCH(PIPE_CONTROL_CS_STALL | PIPE_CONTROL_STALL_AT_SCOREBOARD);
         OUT_BATCH(0);
         OUT_BATCH(0);
         ADVANCE_BATCH();
      }
d59 2
a60 22
      BEGIN_BATCH(5);
      OUT_BATCH(_3DSTATE_PIPE_CONTROL | (5 - 2));
      OUT_BATCH(PIPE_CONTROL_WRITE_TIMESTAMP);
      OUT_RELOC(query_bo,
                I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
                PIPE_CONTROL_GLOBAL_GTT_WRITE |
                idx * sizeof(uint64_t));
      OUT_BATCH(0);
      OUT_BATCH(0);
      ADVANCE_BATCH();
   } else {
      BEGIN_BATCH(4);
      OUT_BATCH(_3DSTATE_PIPE_CONTROL | (4 - 2) |
                PIPE_CONTROL_WRITE_TIMESTAMP);
      OUT_RELOC(query_bo,
                I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
                PIPE_CONTROL_GLOBAL_GTT_WRITE |
                idx * sizeof(uint64_t));
      OUT_BATCH(0);
      OUT_BATCH(0);
      ADVANCE_BATCH();
   }
d66 2
a67 2
static void
write_depth_count(struct brw_context *brw, drm_intel_bo *query_bo, int idx)
d69 8
a76 17
   assert(brw->gen < 6);

   BEGIN_BATCH(4);
   OUT_BATCH(_3DSTATE_PIPE_CONTROL | (4 - 2) |
             PIPE_CONTROL_DEPTH_STALL | PIPE_CONTROL_WRITE_DEPTH_COUNT);
   /* This object could be mapped cacheable, but we don't have an exposed
    * mechanism to support that.  Since it's going uncached, tell GEM that
    * we're writing to it.  The usual clflush should be all that's required
    * to pick up the results.
    */
   OUT_RELOC(query_bo,
             I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
             PIPE_CONTROL_GLOBAL_GTT_WRITE |
             (idx * sizeof(uint64_t)));
   OUT_BATCH(0);
   OUT_BATCH(0);
   ADVANCE_BATCH();
d235 1
a235 1
      write_timestamp(brw, query->bo, 0);
d287 1
a287 1
      write_timestamp(brw, query->bo, 1);
d436 1
a436 1
   write_depth_count(brw, query->bo, query->last_index * 2);
d458 1
a458 1
   write_depth_count(brw, query->bo, query->last_index * 2 + 1);
d481 1
a481 1
   write_timestamp(brw, query->bo, 0);
@


1.4
log
@Upate to libGL 7.11.2

Tested by jsg@@, matthieu@@ and ajacoutot@@, ok mattieu@@
@
text
@d28 1
a28 1
/** @@file support for ARB_query_object
d30 7
a36 10
 * ARB_query_object is implemented by using the PIPE_CONTROL command to stall
 * execution on the completion of previous depth tests, and write the
 * current PS_DEPTH_COUNT to a buffer object.
 *
 * We use before and after counts when drawing during a query so that
 * we don't pick up other clients' query data in ours.  To reduce overhead,
 * a single BO is used to record the query data for all active queries at
 * once.  This also gives us a simple bound on how much batchbuffer space is
 * required for handling queries, so that we can be sure that we won't
 * have to emit a batchbuffer without getting the ending PS_DEPTH_COUNT.
d41 1
d46 73
a118 1
/** Waits on the query object's BO and totals the results for this query */
d120 2
a121 1
brw_queryobj_get_results(struct brw_query_object *query)
d123 2
d128 2
d133 14
a146 1
   drm_intel_bo_map(query->bo, GL_FALSE);
d148 5
a152 1
   if (query->Base.Target == GL_TIME_ELAPSED_EXT) {
d154 19
a172 3
   } else {
      /* Map and count the pixels from the current query BO */
      for (i = query->first_index; i <= query->last_index; i++) {
d175 18
d196 3
d203 5
d217 2
a218 2
   query->Base.Active = GL_FALSE;
   query->Base.Ready = GL_TRUE;
d223 3
d235 6
a244 1
   struct intel_context *intel = intel_context(ctx);
d247 23
a269 1
   if (query->Base.Target == GL_TIME_ELAPSED_EXT) {
d271 14
a284 28
      query->bo = drm_intel_bo_alloc(intel->bufmgr, "timer query",
				     4096, 4096);

      if (intel->gen >= 6) {
	  BEGIN_BATCH(4);
	  OUT_BATCH(_3DSTATE_PIPE_CONTROL);
	  OUT_BATCH(PIPE_CONTROL_WRITE_TIMESTAMP);
	  OUT_RELOC(query->bo,
		  I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
		  PIPE_CONTROL_GLOBAL_GTT_WRITE |
		  0);
	  OUT_BATCH(0);
	  ADVANCE_BATCH();
      
      } else {
	  BEGIN_BATCH(4);
	  OUT_BATCH(_3DSTATE_PIPE_CONTROL |
		  PIPE_CONTROL_WRITE_TIMESTAMP);
	  OUT_RELOC(query->bo,
		  I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
		  PIPE_CONTROL_GLOBAL_GTT_WRITE |
		  0);
	  OUT_BATCH(0);
	  OUT_BATCH(0);
	  ADVANCE_BATCH();
      }
   } else {
      /* Reset our driver's tracking of query state. */
a286 1
      query->first_index = -1;
d290 12
a301 1
      intel->stats_wm++;
d306 6
a311 1
 * Begin the ARB_occlusion_query query on a query object.
a316 1
   struct intel_context *intel = intel_context(ctx);
d319 26
a344 23
   if (query->Base.Target == GL_TIME_ELAPSED_EXT) {
      if (intel->gen >= 6) {
	  BEGIN_BATCH(4);
	  OUT_BATCH(_3DSTATE_PIPE_CONTROL);
	  OUT_BATCH(PIPE_CONTROL_WRITE_TIMESTAMP);
	  OUT_RELOC(query->bo,
		  I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
		  PIPE_CONTROL_GLOBAL_GTT_WRITE |
		  8);
	  OUT_BATCH(0);
	  ADVANCE_BATCH();
      
      } else {
	  BEGIN_BATCH(4);
	  OUT_BATCH(_3DSTATE_PIPE_CONTROL |
		  PIPE_CONTROL_WRITE_TIMESTAMP);
	  OUT_RELOC(query->bo,
		  I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
		  PIPE_CONTROL_GLOBAL_GTT_WRITE |
		  8);
	  OUT_BATCH(0);
	  OUT_BATCH(0);
	  ADVANCE_BATCH();
d347 1
a347 9
      intel_batchbuffer_flush(intel);
   } else {
      /* Flush the batchbuffer in case it has writes to our query BO.
       * Have later queries write to a new query BO so that further rendering
       * doesn't delay the collection of our results.
       */
      if (query->bo) {
	 brw_emit_query_end(brw);
	 intel_batchbuffer_flush(intel);
d349 1
a349 3
	 drm_intel_bo_unreference(brw->query.bo);
	 brw->query.bo = NULL;
      }
d353 7
a359 1
      intel->stats_wm--;
d363 6
d373 4
a376 2
   brw_queryobj_get_results(query);
   query->Base.Ready = GL_TRUE;
d379 6
d387 1
d390 12
d403 2
a404 2
      brw_queryobj_get_results(query);
      query->Base.Ready = GL_TRUE;
d408 8
a415 3
/** Called to set up the query BO and account for its aperture space */
void
brw_prepare_query_begin(struct brw_context *brw)
d417 1
a417 1
   struct intel_context *intel = &brw->intel;
d419 3
a421 3
   /* Skip if we're not doing any queries. */
   if (!brw->query.obj)
      return;
d423 7
a429 12
   /* Get a new query BO if we're going to need it. */
   if (brw->query.bo == NULL ||
       brw->query.index * 2 + 1 >= 4096 / sizeof(uint64_t)) {
      drm_intel_bo_unreference(brw->query.bo);
      brw->query.bo = NULL;

      brw->query.bo = drm_intel_bo_alloc(intel->bufmgr, "query", 4096, 1);

      /* clear target buffer */
      drm_intel_bo_map(brw->query.bo, GL_TRUE);
      memset((char *)brw->query.bo->virtual, 0, 4096);
      drm_intel_bo_unmap(brw->query.bo);
d431 2
a432 1
      brw->query.index = 0;
a433 2

   brw_add_validated_bo(brw, brw->query.bo);
d436 20
a455 1
/** Called just before primitive drawing to get a beginning PS_DEPTH_COUNT. */
d459 1
a459 1
   struct intel_context *intel = &brw->intel;
d462 1
a462 2
   /* Skip if we're not doing any queries, or we've emitted the start. */
   if (!query || brw->query.active)
d465 5
a469 2
   if (intel->gen >= 6) {
       BEGIN_BATCH(8);
d471 3
a473 34
       /* workaround: CS stall required before depth stall. */
       OUT_BATCH(_3DSTATE_PIPE_CONTROL);
       OUT_BATCH(PIPE_CONTROL_CS_STALL);
       OUT_BATCH(0); /* write address */
       OUT_BATCH(0); /* write data */

       OUT_BATCH(_3DSTATE_PIPE_CONTROL);
       OUT_BATCH(PIPE_CONTROL_DEPTH_STALL |
	         PIPE_CONTROL_WRITE_DEPTH_COUNT);
       OUT_RELOC(brw->query.bo,
	         I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
		 PIPE_CONTROL_GLOBAL_GTT_WRITE |
		 ((brw->query.index * 2) * sizeof(uint64_t)));
       OUT_BATCH(0);
       ADVANCE_BATCH();
       
   } else {
       BEGIN_BATCH(4);
       OUT_BATCH(_3DSTATE_PIPE_CONTROL |
	       PIPE_CONTROL_DEPTH_STALL |
	       PIPE_CONTROL_WRITE_DEPTH_COUNT);
       /* This object could be mapped cacheable, but we don't have an exposed
	* mechanism to support that.  Since it's going uncached, tell GEM that
	* we're writing to it.  The usual clflush should be all that's required
	* to pick up the results.
	*/
       OUT_RELOC(brw->query.bo,
	       I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
	       PIPE_CONTROL_GLOBAL_GTT_WRITE |
	       ((brw->query.index * 2) * sizeof(uint64_t)));
       OUT_BATCH(0);
       OUT_BATCH(0);
       ADVANCE_BATCH();
   }
d475 1
a475 9
   if (query->bo != brw->query.bo) {
      if (query->bo != NULL)
	 brw_queryobj_get_results(query);
      drm_intel_bo_reference(brw->query.bo);
      query->bo = brw->query.bo;
      query->first_index = brw->query.index;
   }
   query->last_index = brw->query.index;
   brw->query.active = GL_TRUE;
d478 6
a483 1
/** Called at batchbuffer flush to get an ending PS_DEPTH_COUNT */
d487 4
a490 1
   struct intel_context *intel = &brw->intel;
d492 1
a492 1
   if (!brw->query.active)
d495 43
a537 31
   if (intel->gen >= 6) {
       BEGIN_BATCH(8);
       /* workaround: CS stall required before depth stall. */
       OUT_BATCH(_3DSTATE_PIPE_CONTROL);
       OUT_BATCH(PIPE_CONTROL_CS_STALL);
       OUT_BATCH(0); /* write address */
       OUT_BATCH(0); /* write data */

       OUT_BATCH(_3DSTATE_PIPE_CONTROL);
       OUT_BATCH(PIPE_CONTROL_DEPTH_STALL |
	         PIPE_CONTROL_WRITE_DEPTH_COUNT);
       OUT_RELOC(brw->query.bo,
	         I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
		 PIPE_CONTROL_GLOBAL_GTT_WRITE |
		 ((brw->query.index * 2 + 1) * sizeof(uint64_t)));
       OUT_BATCH(0);
       ADVANCE_BATCH();
   
   } else {
       BEGIN_BATCH(4);
       OUT_BATCH(_3DSTATE_PIPE_CONTROL |
	       PIPE_CONTROL_DEPTH_STALL |
	       PIPE_CONTROL_WRITE_DEPTH_COUNT);
       OUT_RELOC(brw->query.bo,
	       I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
	       PIPE_CONTROL_GLOBAL_GTT_WRITE |
	       ((brw->query.index * 2 + 1) * sizeof(uint64_t)));
       OUT_BATCH(0);
       OUT_BATCH(0);
       ADVANCE_BATCH();
   }
d539 1
a539 2
   brw->query.active = GL_FALSE;
   brw->query.index++;
d542 2
a543 1
void brw_init_queryobj_functions(struct dd_function_table *functions)
d547 7
@


1.3
log
@Merge Mesa 7.10.3
@
text
@d180 1
a180 1
      intel_batchbuffer_flush(intel->batch);
d188 1
a188 1
	 intel_batchbuffer_flush(intel->batch);
@


1.2
log
@Update to Mesa 7.8.1. Tested on a bulk ports build by naddy@@, ok oga@@.
@
text
@a40 1
#include "main/simple_list.h"
d58 1
a58 2
   /* Map and count the pixels from the current query BO */
   dri_bo_map(query->bo, GL_FALSE);
d60 7
a66 2
   for (i = query->first_index; i <= query->last_index; i++) {
      query->Base.Result += results[i * 2 + 1] - results[i * 2];
d68 1
a68 1
   dri_bo_unmap(query->bo);
d70 1
a70 1
   dri_bo_unreference(query->bo);
d75 1
a75 1
brw_new_query_object(GLcontext *ctx, GLuint id)
d90 1
a90 1
brw_delete_query(GLcontext *ctx, struct gl_query_object *q)
d94 1
a94 1
   dri_bo_unreference(query->bo);
d99 1
a99 1
brw_begin_query(GLcontext *ctx, struct gl_query_object *q)
d105 34
a138 5
   /* Reset our driver's tracking of query state. */
   dri_bo_unreference(query->bo);
   query->bo = NULL;
   query->first_index = -1;
   query->last_index = -1;
d140 3
a142 2
   insert_at_head(&brw->query.active_head, query);
   intel->stats_wm++;
d149 1
a149 1
brw_end_query(GLcontext *ctx, struct gl_query_object *q)
d155 25
a179 6
   /* Flush the batchbuffer in case it has writes to our query BO.
    * Have later queries write to a new query BO so that further rendering
    * doesn't delay the collection of our results.
    */
   if (query->bo) {
      brw_emit_query_end(brw);
d181 8
d190 3
a192 3
      dri_bo_unreference(brw->query.bo);
      brw->query.bo = NULL;
   }
d194 1
a194 1
   remove_from_list(query);
d196 2
a197 1
   intel->stats_wm--;
d200 1
a200 1
static void brw_wait_query(GLcontext *ctx, struct gl_query_object *q)
d208 1
a208 1
static void brw_check_query(GLcontext *ctx, struct gl_query_object *q)
d225 1
a225 1
   if (is_empty_list(&brw->query.active_head))
d231 1
a231 1
      dri_bo_unreference(brw->query.bo);
d234 7
a240 1
      brw->query.bo = dri_bo_alloc(intel->bufmgr, "query", 4096, 1);
d252 1
a252 1
   struct brw_query_object *query;
d255 1
a255 1
   if (brw->query.active || is_empty_list(&brw->query.active_head))
d258 44
a301 26
   BEGIN_BATCH(4);
   OUT_BATCH(_3DSTATE_PIPE_CONTROL |
	     PIPE_CONTROL_DEPTH_STALL |
	     PIPE_CONTROL_WRITE_DEPTH_COUNT);
   /* This object could be mapped cacheable, but we don't have an exposed
    * mechanism to support that.  Since it's going uncached, tell GEM that
    * we're writing to it.  The usual clflush should be all that's required
    * to pick up the results.
    */
   OUT_RELOC(brw->query.bo,
	     I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
	     PIPE_CONTROL_GLOBAL_GTT_WRITE |
	     ((brw->query.index * 2) * sizeof(uint64_t)));
   OUT_BATCH(0);
   OUT_BATCH(0);
   ADVANCE_BATCH();

   foreach(query, &brw->query.active_head) {
      if (query->bo != brw->query.bo) {
	 if (query->bo != NULL)
	    brw_queryobj_get_results(query);
	 dri_bo_reference(brw->query.bo);
	 query->bo = brw->query.bo;
	 query->first_index = brw->query.index;
      }
      query->last_index = brw->query.index;
d303 1
d316 31
a346 11
   BEGIN_BATCH(4);
   OUT_BATCH(_3DSTATE_PIPE_CONTROL |
	     PIPE_CONTROL_DEPTH_STALL |
	     PIPE_CONTROL_WRITE_DEPTH_COUNT);
   OUT_RELOC(brw->query.bo,
	     I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
	     PIPE_CONTROL_GLOBAL_GTT_WRITE |
	     ((brw->query.index * 2 + 1) * sizeof(uint64_t)));
   OUT_BATCH(0);
   OUT_BATCH(0);
   ADVANCE_BATCH();
@


1.1
log
@Update to Mesa 7.4.2. Tested by oga@@, ckuethe@@ and naddy@@.
@
text
@d76 1
a76 1
   query = _mesa_calloc(sizeof(struct brw_query_object));
d92 1
a92 1
   _mesa_free(query);
a148 2
   /* XXX: Need to expose dri_bo_is_idle from bufmgr. */
#if 0
d151 1
a151 1
   if (dri_bo_is_idle(query->bo)) {
a154 3
#else
   brw_wait_query(ctx, q);
#endif
d191 1
a191 1
   BEGIN_BATCH(4, IGNORE_CLIPRECTS);
d230 1
a230 1
   BEGIN_BATCH(4, IGNORE_CLIPRECTS);
@


1.1.1.1
log
@Import Mesa 7.10.3
@
text
@d41 1
d59 2
a60 1
   drm_intel_bo_map(query->bo, GL_FALSE);
d62 2
a63 7
   if (query->Base.Target == GL_TIME_ELAPSED_EXT) {
      query->Base.Result += 1000 * ((results[1] >> 32) - (results[0] >> 32));
   } else {
      /* Map and count the pixels from the current query BO */
      for (i = query->first_index; i <= query->last_index; i++) {
	 query->Base.Result += results[i * 2 + 1] - results[i * 2];
      }
d65 1
a65 1
   drm_intel_bo_unmap(query->bo);
d67 1
a67 1
   drm_intel_bo_unreference(query->bo);
d72 1
a72 1
brw_new_query_object(struct gl_context *ctx, GLuint id)
d76 1
a76 1
   query = calloc(1, sizeof(struct brw_query_object));
d87 1
a87 1
brw_delete_query(struct gl_context *ctx, struct gl_query_object *q)
d91 2
a92 2
   drm_intel_bo_unreference(query->bo);
   free(query);
d96 1
a96 1
brw_begin_query(struct gl_context *ctx, struct gl_query_object *q)
d102 5
a106 34
   if (query->Base.Target == GL_TIME_ELAPSED_EXT) {
      drm_intel_bo_unreference(query->bo);
      query->bo = drm_intel_bo_alloc(intel->bufmgr, "timer query",
				     4096, 4096);

      if (intel->gen >= 6) {
	  BEGIN_BATCH(4);
	  OUT_BATCH(_3DSTATE_PIPE_CONTROL);
	  OUT_BATCH(PIPE_CONTROL_WRITE_TIMESTAMP);
	  OUT_RELOC(query->bo,
		  I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
		  PIPE_CONTROL_GLOBAL_GTT_WRITE |
		  0);
	  OUT_BATCH(0);
	  ADVANCE_BATCH();
      
      } else {
	  BEGIN_BATCH(4);
	  OUT_BATCH(_3DSTATE_PIPE_CONTROL |
		  PIPE_CONTROL_WRITE_TIMESTAMP);
	  OUT_RELOC(query->bo,
		  I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
		  PIPE_CONTROL_GLOBAL_GTT_WRITE |
		  0);
	  OUT_BATCH(0);
	  OUT_BATCH(0);
	  ADVANCE_BATCH();
      }
   } else {
      /* Reset our driver's tracking of query state. */
      drm_intel_bo_unreference(query->bo);
      query->bo = NULL;
      query->first_index = -1;
      query->last_index = -1;
d108 2
a109 3
      brw->query.obj = query;
      intel->stats_wm++;
   }
d116 1
a116 1
brw_end_query(struct gl_context *ctx, struct gl_query_object *q)
d122 6
a127 25
   if (query->Base.Target == GL_TIME_ELAPSED_EXT) {
      if (intel->gen >= 6) {
	  BEGIN_BATCH(4);
	  OUT_BATCH(_3DSTATE_PIPE_CONTROL);
	  OUT_BATCH(PIPE_CONTROL_WRITE_TIMESTAMP);
	  OUT_RELOC(query->bo,
		  I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
		  PIPE_CONTROL_GLOBAL_GTT_WRITE |
		  8);
	  OUT_BATCH(0);
	  ADVANCE_BATCH();
      
      } else {
	  BEGIN_BATCH(4);
	  OUT_BATCH(_3DSTATE_PIPE_CONTROL |
		  PIPE_CONTROL_WRITE_TIMESTAMP);
	  OUT_RELOC(query->bo,
		  I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
		  PIPE_CONTROL_GLOBAL_GTT_WRITE |
		  8);
	  OUT_BATCH(0);
	  OUT_BATCH(0);
	  ADVANCE_BATCH();
      }

a128 8
   } else {
      /* Flush the batchbuffer in case it has writes to our query BO.
       * Have later queries write to a new query BO so that further rendering
       * doesn't delay the collection of our results.
       */
      if (query->bo) {
	 brw_emit_query_end(brw);
	 intel_batchbuffer_flush(intel->batch);
d130 3
a132 3
	 drm_intel_bo_unreference(brw->query.bo);
	 brw->query.bo = NULL;
      }
d134 1
a134 1
      brw->query.obj = NULL;
d136 1
a136 2
      intel->stats_wm--;
   }
d139 1
a139 1
static void brw_wait_query(struct gl_context *ctx, struct gl_query_object *q)
d147 1
a147 1
static void brw_check_query(struct gl_context *ctx, struct gl_query_object *q)
d149 2
d153 1
a153 1
   if (query->bo == NULL || !drm_intel_bo_busy(query->bo)) {
d157 3
d169 1
a169 1
   if (!brw->query.obj)
d175 1
a175 1
      drm_intel_bo_unreference(brw->query.bo);
d178 1
a178 7
      brw->query.bo = drm_intel_bo_alloc(intel->bufmgr, "query", 4096, 1);

      /* clear target buffer */
      drm_intel_bo_map(brw->query.bo, GL_TRUE);
      memset((char *)brw->query.bo->virtual, 0, 4096);
      drm_intel_bo_unmap(brw->query.bo);

d190 1
a190 1
   struct brw_query_object *query = brw->query.obj;
d193 1
a193 1
   if (!query || brw->query.active)
d196 26
a221 44
   if (intel->gen >= 6) {
       BEGIN_BATCH(8);

       /* workaround: CS stall required before depth stall. */
       OUT_BATCH(_3DSTATE_PIPE_CONTROL);
       OUT_BATCH(PIPE_CONTROL_CS_STALL);
       OUT_BATCH(0); /* write address */
       OUT_BATCH(0); /* write data */

       OUT_BATCH(_3DSTATE_PIPE_CONTROL);
       OUT_BATCH(PIPE_CONTROL_DEPTH_STALL |
	         PIPE_CONTROL_WRITE_DEPTH_COUNT);
       OUT_RELOC(brw->query.bo,
	         I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
		 PIPE_CONTROL_GLOBAL_GTT_WRITE |
		 ((brw->query.index * 2) * sizeof(uint64_t)));
       OUT_BATCH(0);
       ADVANCE_BATCH();
       
   } else {
       BEGIN_BATCH(4);
       OUT_BATCH(_3DSTATE_PIPE_CONTROL |
	       PIPE_CONTROL_DEPTH_STALL |
	       PIPE_CONTROL_WRITE_DEPTH_COUNT);
       /* This object could be mapped cacheable, but we don't have an exposed
	* mechanism to support that.  Since it's going uncached, tell GEM that
	* we're writing to it.  The usual clflush should be all that's required
	* to pick up the results.
	*/
       OUT_RELOC(brw->query.bo,
	       I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
	       PIPE_CONTROL_GLOBAL_GTT_WRITE |
	       ((brw->query.index * 2) * sizeof(uint64_t)));
       OUT_BATCH(0);
       OUT_BATCH(0);
       ADVANCE_BATCH();
   }

   if (query->bo != brw->query.bo) {
      if (query->bo != NULL)
	 brw_queryobj_get_results(query);
      drm_intel_bo_reference(brw->query.bo);
      query->bo = brw->query.bo;
      query->first_index = brw->query.index;
a222 1
   query->last_index = brw->query.index;
d235 11
a245 31
   if (intel->gen >= 6) {
       BEGIN_BATCH(8);
       /* workaround: CS stall required before depth stall. */
       OUT_BATCH(_3DSTATE_PIPE_CONTROL);
       OUT_BATCH(PIPE_CONTROL_CS_STALL);
       OUT_BATCH(0); /* write address */
       OUT_BATCH(0); /* write data */

       OUT_BATCH(_3DSTATE_PIPE_CONTROL);
       OUT_BATCH(PIPE_CONTROL_DEPTH_STALL |
	         PIPE_CONTROL_WRITE_DEPTH_COUNT);
       OUT_RELOC(brw->query.bo,
	         I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
		 PIPE_CONTROL_GLOBAL_GTT_WRITE |
		 ((brw->query.index * 2 + 1) * sizeof(uint64_t)));
       OUT_BATCH(0);
       ADVANCE_BATCH();
   
   } else {
       BEGIN_BATCH(4);
       OUT_BATCH(_3DSTATE_PIPE_CONTROL |
	       PIPE_CONTROL_DEPTH_STALL |
	       PIPE_CONTROL_WRITE_DEPTH_COUNT);
       OUT_RELOC(brw->query.bo,
	       I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
	       PIPE_CONTROL_GLOBAL_GTT_WRITE |
	       ((brw->query.index * 2 + 1) * sizeof(uint64_t)));
       OUT_BATCH(0);
       OUT_BATCH(0);
       ADVANCE_BATCH();
   }
@


1.1.1.2
log
@Import Mesa 9.2.0
@
text
@d28 1
a28 1
/** @@file brw_queryobj.c
d30 10
a39 7
 * Support for query objects (GL_ARB_occlusion_query, GL_ARB_timer_query,
 * GL_EXT_transform_feedback, and friends).
 *
 * The hardware provides a PIPE_CONTROL command that can report the number of
 * fragments that passed the depth test, or the hardware timer.  They are
 * appropriately synced with the stage of the pipeline for our extensions'
 * needs.
a43 1
#include "brw_defines.h"
d48 1
a48 73
/**
 * Emit PIPE_CONTROLs to write the current GPU timestamp into a buffer.
 */
static void
write_timestamp(struct brw_context *brw, drm_intel_bo *query_bo, int idx)
{
   if (brw->gen >= 6) {
      /* Emit workaround flushes: */
      if (brw->gen == 6) {
         /* The timestamp write below is a non-zero post-sync op, which on
          * Gen6 necessitates a CS stall.  CS stalls need stall at scoreboard
          * set.  See the comments for intel_emit_post_sync_nonzero_flush().
          */
         BEGIN_BATCH(4);
         OUT_BATCH(_3DSTATE_PIPE_CONTROL | (4 - 2));
         OUT_BATCH(PIPE_CONTROL_CS_STALL | PIPE_CONTROL_STALL_AT_SCOREBOARD);
         OUT_BATCH(0);
         OUT_BATCH(0);
         ADVANCE_BATCH();
      }

      BEGIN_BATCH(5);
      OUT_BATCH(_3DSTATE_PIPE_CONTROL | (5 - 2));
      OUT_BATCH(PIPE_CONTROL_WRITE_TIMESTAMP);
      OUT_RELOC(query_bo,
                I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
                PIPE_CONTROL_GLOBAL_GTT_WRITE |
                idx * sizeof(uint64_t));
      OUT_BATCH(0);
      OUT_BATCH(0);
      ADVANCE_BATCH();
   } else {
      BEGIN_BATCH(4);
      OUT_BATCH(_3DSTATE_PIPE_CONTROL | (4 - 2) |
                PIPE_CONTROL_WRITE_TIMESTAMP);
      OUT_RELOC(query_bo,
                I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
                PIPE_CONTROL_GLOBAL_GTT_WRITE |
                idx * sizeof(uint64_t));
      OUT_BATCH(0);
      OUT_BATCH(0);
      ADVANCE_BATCH();
   }
}

/**
 * Emit PIPE_CONTROLs to write the PS_DEPTH_COUNT register into a buffer.
 */
static void
write_depth_count(struct brw_context *brw, drm_intel_bo *query_bo, int idx)
{
   assert(brw->gen < 6);

   BEGIN_BATCH(4);
   OUT_BATCH(_3DSTATE_PIPE_CONTROL | (4 - 2) |
             PIPE_CONTROL_DEPTH_STALL | PIPE_CONTROL_WRITE_DEPTH_COUNT);
   /* This object could be mapped cacheable, but we don't have an exposed
    * mechanism to support that.  Since it's going uncached, tell GEM that
    * we're writing to it.  The usual clflush should be all that's required
    * to pick up the results.
    */
   OUT_RELOC(query_bo,
             I915_GEM_DOMAIN_INSTRUCTION, I915_GEM_DOMAIN_INSTRUCTION,
             PIPE_CONTROL_GLOBAL_GTT_WRITE |
             (idx * sizeof(uint64_t)));
   OUT_BATCH(0);
   OUT_BATCH(0);
   ADVANCE_BATCH();
}

/**
 * Wait on the query object's BO and calculate the final result.
 */
d50 1
a50 2
brw_queryobj_get_results(struct gl_context *ctx,
			 struct brw_query_object *query)
a51 2
   struct brw_context *brw = brw_context(ctx);

a54 2
   assert(brw->gen < 6);

d58 1
a58 14
   /* If the application has requested the query result, but this batch is
    * still contributing to it, flush it now so the results will be present
    * when mapped.
    */
   if (drm_intel_bo_references(brw->batch.bo, query->bo))
      intel_batchbuffer_flush(brw);

   if (unlikely(brw->perf_debug)) {
      if (drm_intel_bo_busy(query->bo)) {
         perf_debug("Stalling on the GPU waiting for a query object.\n");
      }
   }

   drm_intel_bo_map(query->bo, false);
d60 1
a60 5
   switch (query->Base.Target) {
   case GL_TIME_ELAPSED_EXT:
      /* The query BO contains the starting and ending timestamps.
       * Subtract the two and convert to nanoseconds.
       */
d62 3
a64 19
      break;

   case GL_TIMESTAMP:
      /* The query BO contains a single timestamp value in results[0]. */
      query->Base.Result = 1000 * (results[0] >> 32);
      break;

   case GL_SAMPLES_PASSED_ARB:
      /* Loop over pairs of values from the BO, which are the PS_DEPTH_COUNT
       * value at the start and end of the batchbuffer.  Subtract them to
       * get the number of fragments which passed the depth test in each
       * individual batch, and add those differences up to get the number
       * of fragments for the entire query.
       *
       * Note that query->Base.Result may already be non-zero.  We may have
       * run out of space in the query's BO and allocated a new one.  If so,
       * this function was already called to accumulate the results so far.
       */
      for (i = 0; i < query->last_index; i++) {
a66 18
      break;

   case GL_ANY_SAMPLES_PASSED:
   case GL_ANY_SAMPLES_PASSED_CONSERVATIVE:
      /* If the starting and ending PS_DEPTH_COUNT from any of the batches
       * differ, then some fragments passed the depth test.
       */
      for (i = 0; i < query->last_index; i++) {
	 if (results[i * 2 + 1] != results[i * 2]) {
            query->Base.Result = GL_TRUE;
            break;
         }
      }
      break;

   default:
      assert(!"Unrecognized query target in brw_queryobj_get_results()");
      break;
a69 3
   /* Now that we've processed the data stored in the query's buffer object,
    * we can release it.
    */
a73 5
/**
 * The NewQueryObject() driver hook.
 *
 * Allocates and initializes a new query object.
 */
d83 2
a84 2
   query->Base.Active = false;
   query->Base.Ready = true;
a88 3
/**
 * The DeleteQuery() driver hook.
 */
a97 6
/**
 * Gen4-5 driver hook for glBeginQuery().
 *
 * Initializes driver structures and emits any GPU commands required to begin
 * recording data for the query.
 */
d102 1
d105 4
a108 1
   assert(brw->gen < 6);
d110 25
a134 36
   switch (query->Base.Target) {
   case GL_TIME_ELAPSED_EXT:
      /* For timestamp queries, we record the starting time right away so that
       * we measure the full time between BeginQuery and EndQuery.  There's
       * some debate about whether this is the right thing to do.  Our decision
       * is based on the following text from the ARB_timer_query extension:
       *
       * "(5) Should the extension measure total time elapsed between the full
       *      completion of the BeginQuery and EndQuery commands, or just time
       *      spent in the graphics library?
       *
       *  RESOLVED:  This extension will measure the total time elapsed
       *  between the full completion of these commands.  Future extensions
       *  may implement a query to determine time elapsed at different stages
       *  of the graphics pipeline."
       *
       * We write a starting timestamp now (at index 0).  At EndQuery() time,
       * we'll write a second timestamp (at index 1), and subtract the two to
       * obtain the time elapsed.  Notably, this includes time elapsed while
       * the system was doing other work, such as running other applications.
       */
      drm_intel_bo_unreference(query->bo);
      query->bo = drm_intel_bo_alloc(brw->bufmgr, "timer query", 4096, 4096);
      write_timestamp(brw, query->bo, 0);
      break;

   case GL_ANY_SAMPLES_PASSED:
   case GL_ANY_SAMPLES_PASSED_CONSERVATIVE:
   case GL_SAMPLES_PASSED_ARB:
      /* For occlusion queries, we delay taking an initial sample until the
       * first drawing occurs in this batch.  See the reasoning in the comments
       * for brw_emit_query_begin() below.
       *
       * Since we're starting a new query, we need to be sure to throw away
       * any previous occlusion query results.
       */
d137 1
d141 1
a141 12

      /* Depth statistics on Gen4 require strange workarounds, so we try to
       * avoid them when necessary.  They're required for occlusion queries,
       * so turn them on now.
       */
      brw->stats_wm++;
      brw->state.dirty.brw |= BRW_NEW_STATS_WM;
      break;

   default:
      assert(!"Unrecognized query target in brw_begin_query()");
      break;
d146 1
a146 6
 * Gen4-5 driver hook for glEndQuery().
 *
 * Emits GPU commands to record a final query value, ending any data capturing.
 * However, the final result isn't necessarily available until the GPU processes
 * those commands.  brw_queryobj_get_results() processes the captured data to
 * produce the final result.
d152 1
d155 24
a178 1
   assert(brw->gen < 6);
d180 5
a184 21
   switch (query->Base.Target) {
   case GL_TIME_ELAPSED_EXT:
      /* Write the final timestamp. */
      write_timestamp(brw, query->bo, 1);
      break;

   case GL_ANY_SAMPLES_PASSED:
   case GL_ANY_SAMPLES_PASSED_CONSERVATIVE:
   case GL_SAMPLES_PASSED_ARB:

      /* No query->bo means that EndQuery was called after BeginQuery with no
       * intervening drawing. Rather than doing nothing at all here in this
       * case, we emit the query_begin and query_end state to the
       * hardware. This is to guarantee that waiting on the result of this
       * empty state will cause all previous queries to complete at all, as
       * required by the specification:
       *
       * 	It must always be true that if any query object
       *	returns a result available of TRUE, all queries of the
       *	same type issued prior to that query must also return
       *	TRUE. [Open GL 4.3 (Core Profile) Section 4.2.1]
d186 6
a191 2
      if (!query->bo) {
         brw_emit_query_begin(brw);
a193 4
      assert(query->bo);

      brw_emit_query_end(brw);

d196 1
a196 7
      brw->stats_wm--;
      brw->state.dirty.brw |= BRW_NEW_STATS_WM;
      break;

   default:
      assert(!"Unrecognized query target in brw_end_query()");
      break;
a199 6
/**
 * The Gen4-5 WaitQuery() driver hook.
 *
 * Wait for a query result to become available and return it.  This is the
 * backing for glGetQueryObjectiv() with the GL_QUERY_RESULT pname.
 */
d204 2
a205 4
   assert(brw_context(ctx)->gen < 6);

   brw_queryobj_get_results(ctx, query);
   query->Base.Ready = true;
a207 6
/**
 * The Gen4-5 CheckQuery() driver hook.
 *
 * Checks whether a query result is ready yet.  If not, flushes.
 * This is the backing for glGetQueryObjectiv()'s QUERY_RESULT_AVAILABLE pname.
 */
a209 1
   struct brw_context *brw = brw_context(ctx);
a211 12
   assert(brw->gen < 6);

   /* From the GL_ARB_occlusion_query spec:
    *
    *     "Instead of allowing for an infinite loop, performing a
    *      QUERY_RESULT_AVAILABLE_ARB will perform a flush if the result is
    *      not ready yet on the first time it is queried.  This ensures that
    *      the async query will return true in finite time.
    */
   if (query->bo && drm_intel_bo_references(brw->batch.bo, query->bo))
      intel_batchbuffer_flush(brw);

d213 2
a214 2
      brw_queryobj_get_results(ctx, query);
      query->Base.Ready = true;
d218 3
a220 8
/**
 * Ensure there query's BO has enough space to store a new pair of values.
 *
 * If not, gather the existing BO's results and create a new buffer of the
 * same size.
 */
static void
ensure_bo_has_space(struct gl_context *ctx, struct brw_query_object *query)
d222 1
a222 1
   struct brw_context *brw = brw_context(ctx);
d224 3
a226 1
   assert(brw->gen < 6);
d228 12
a239 1
   if (!query->bo || query->last_index * 2 + 1 >= 4096 / sizeof(uint64_t)) {
d241 2
a242 7
      if (query->bo != NULL) {
         /* The old query BO did not have enough space, so we allocated a new
          * one.  Gather the results so far (adding up the differences) and
          * release the old BO.
          */
         brw_queryobj_get_results(ctx, query);
      }
d244 1
a244 3
      query->bo = drm_intel_bo_alloc(brw->bufmgr, "query", 4096, 1);
      query->last_index = 0;
   }
d247 1
a247 20
/**
 * Record the PS_DEPTH_COUNT value (for occlusion queries) just before
 * primitive drawing.
 *
 * In a pre-hardware context world, the single PS_DEPTH_COUNT register is
 * shared among all applications using the GPU.  However, our query value
 * needs to only include fragments generated by our application/GL context.
 *
 * To accommodate this, we record PS_DEPTH_COUNT at the start and end of
 * each batchbuffer (technically, the first primitive drawn and flush time).
 * Subtracting each pair of values calculates the change in PS_DEPTH_COUNT
 * caused by a batchbuffer.  Since there is no preemption inside batches,
 * this is guaranteed to only measure the effects of our current application.
 *
 * Adding each of these differences (in case drawing is done over many batches)
 * produces the final expected value.
 *
 * In a world with hardware contexts, PS_DEPTH_COUNT is saved and restored
 * as part of the context state, so this is unnecessary, and skipped.
 */
d251 1
a251 1
   struct gl_context *ctx = &brw->ctx;
d254 2
a255 1
   if (brw->hw_ctx)
d258 2
a259 5
   /* Skip if we're not doing any queries, or we've already recorded the
    * initial query value for this batchbuffer.
    */
   if (!query || brw->query.begin_emitted)
      return;
d261 34
a294 1
   ensure_bo_has_space(ctx, query);
d296 9
a304 3
   write_depth_count(brw, query->bo, query->last_index * 2);

   brw->query.begin_emitted = true;
d307 1
a307 6
/**
 * Called at batchbuffer flush to get an ending PS_DEPTH_COUNT
 * (for non-hardware context platforms).
 *
 * See the explanation in brw_emit_query_begin().
 */
d311 1
a311 1
   struct brw_query_object *query = brw->query.obj;
d313 1
a313 1
   if (brw->hw_ctx)
d316 31
a346 2
   if (!brw->query.begin_emitted)
      return;
d348 2
a349 4
   write_depth_count(brw, query->bo, query->last_index * 2 + 1);

   brw->query.begin_emitted = false;
   query->last_index++;
d352 1
a352 43
/**
 * Driver hook for glQueryCounter().
 *
 * This handles GL_TIMESTAMP queries, which perform a pipelined read of the
 * current GPU time.  This is unlike GL_TIME_ELAPSED, which measures the
 * time while the query is active.
 */
static void
brw_query_counter(struct gl_context *ctx, struct gl_query_object *q)
{
   struct brw_context *brw = brw_context(ctx);
   struct brw_query_object *query = (struct brw_query_object *) q;

   assert(q->Target == GL_TIMESTAMP);

   drm_intel_bo_unreference(query->bo);
   query->bo = drm_intel_bo_alloc(brw->bufmgr, "timestamp query", 4096, 4096);
   write_timestamp(brw, query->bo, 0);
}

/**
 * Read the TIMESTAMP register immediately (in a non-pipelined fashion).
 *
 * This is used to implement the GetTimestamp() driver hook.
 */
static uint64_t
brw_get_timestamp(struct gl_context *ctx)
{
   struct brw_context *brw = brw_context(ctx);
   uint64_t result = 0;

   drm_intel_reg_read(brw->bufmgr, TIMESTAMP, &result);

   /* See logic in brw_queryobj_get_results() */
   result = result >> 32;
   result *= 80;
   result &= (1ull << 36) - 1;

   return result;
}

/* Initialize query object functions used on all generations. */
void brw_init_common_queryobj_functions(struct dd_function_table *functions)
a355 7
   functions->QueryCounter = brw_query_counter;
   functions->GetTimestamp = brw_get_timestamp;
}

/* Initialize Gen4/5-specific query object functions. */
void gen4_init_queryobj_functions(struct dd_function_table *functions)
{
@


1.1.1.3
log
@Import Mesa 10.2.3
@
text
@d49 2
a50 2
void
brw_write_timestamp(struct brw_context *brw, drm_intel_bo *query_bo, int idx)
d52 36
a87 5
   if (brw->gen == 6) {
      /* Emit Sandybridge workaround flush: */
      brw_emit_pipe_control_flush(brw,
                                  PIPE_CONTROL_CS_STALL |
                                  PIPE_CONTROL_STALL_AT_SCOREBOARD);
a88 3

   brw_emit_pipe_control_write(brw, PIPE_CONTROL_WRITE_TIMESTAMP,
                               query_bo, idx * sizeof(uint64_t), 0, 0);
d94 2
a95 2
void
brw_write_depth_count(struct brw_context *brw, drm_intel_bo *query_bo, int idx)
d97 17
a113 8
   /* Emit Sandybridge workaround flush: */
   if (brw->gen == 6)
      intel_emit_post_sync_nonzero_flush(brw);

   brw_emit_pipe_control_write(brw,
                               PIPE_CONTROL_WRITE_DEPTH_COUNT
                               | PIPE_CONTROL_DEPTH_STALL,
                               query_bo, idx * sizeof(uint64_t), 0, 0);
d272 1
a272 1
      brw_write_timestamp(brw, query->bo, 0);
d324 1
a324 1
      brw_write_timestamp(brw, query->bo, 1);
d473 1
a473 1
   brw_write_depth_count(brw, query->bo, query->last_index * 2);
d495 1
a495 1
   brw_write_depth_count(brw, query->bo, query->last_index * 2 + 1);
d518 1
a518 1
   brw_write_timestamp(brw, query->bo, 0);
@


1.1.1.4
log
@Import Mesa 10.4.3
@
text
@d154 2
a155 1
      unreachable("Unrecognized query target in brw_queryobj_get_results()");
d263 2
a264 1
      unreachable("Unrecognized query target in brw_begin_query()");
d321 2
a322 1
      unreachable("Unrecognized query target in brw_end_query()");
@


1.1.1.5
log
@Import Mesa 10.2.9
@
text
@d154 1
a154 2
      assert(!"Unrecognized query target in brw_queryobj_get_results()");
      break;
d262 1
a262 2
      assert(!"Unrecognized query target in brw_begin_query()");
      break;
d319 1
a319 2
      assert(!"Unrecognized query target in brw_end_query()");
      break;
@


