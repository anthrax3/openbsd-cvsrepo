head	1.4;
access;
symbols
	OPENBSD_5_8:1.3.0.8
	OPENBSD_5_8_BASE:1.3
	OPENBSD_5_7:1.3.0.6
	OPENBSD_5_7_BASE:1.3
	v10_2_9:1.1.1.2
	v10_4_3:1.1.1.2
	v10_2_7:1.1.1.2
	OPENBSD_5_6:1.3.0.4
	OPENBSD_5_6_BASE:1.3
	v10_2_3:1.1.1.2
	OPENBSD_5_5:1.3.0.2
	OPENBSD_5_5_BASE:1.3
	v9_2_5:1.1.1.2
	v9_2_3:1.1.1.2
	v9_2_2:1.1.1.2
	v9_2_1:1.1.1.2
	v9_2_0:1.1.1.2
	OPENBSD_4_4:1.1.1.1.0.6
	OPENBSD_4_4_BASE:1.1.1.1
	OPENBSD_4_3_BASE:1.1.1.1
	OPENBSD_4_3:1.1.1.1.0.4
	v7_0_1:1.1.1.1
	OPENBSD_4_2:1.1.1.1.0.2
	OPENBSD_4_2_BASE:1.1.1.1
	v6_5_2:1.1.1.1
	v6_5_1:1.1.1.1
	mesa:1.1.1;
locks; strict;
comment	@ * @;


1.4
date	2015.12.23.05.17.48;	author jsg;	state dead;
branches;
next	1.3;
commitid	TnlogFl9nOv2eaRf;

1.3
date	2013.09.05.14.04.14;	author jsg;	state Exp;
branches;
next	1.2;

1.2
date	2008.11.02.14.58.14;	author matthieu;	state dead;
branches;
next	1.1;

1.1
date	2006.11.25.18.52.35;	author matthieu;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2006.11.25.18.52.35;	author matthieu;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2013.09.05.13.15.22;	author jsg;	state Exp;
branches;
next	;


desc
@@


1.4
log
@remove the now unused Mesa 10.2.9 code
@
text
@#ifndef INTEL_BATCHBUFFER_H
#define INTEL_BATCHBUFFER_H

#include "main/mtypes.h"

#include "intel_context.h"
#include "intel_bufmgr.h"
#include "intel_reg.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * Number of bytes to reserve for commands necessary to complete a batch.
 *
 * This includes:
 * - MI_BATCHBUFFER_END (4 bytes)
 * - Optional MI_NOOP for ensuring the batch length is qword aligned (4 bytes)
 * - Any state emitted by vtbl->finish_batch():
 *   - Gen4-5 record ending occlusion query values (4 * 4 = 16 bytes)
 */
#define BATCH_RESERVED 24

struct intel_batchbuffer;

void intel_batchbuffer_init(struct intel_context *intel);
void intel_batchbuffer_free(struct intel_context *intel);

int _intel_batchbuffer_flush(struct intel_context *intel,
			     const char *file, int line);

#define intel_batchbuffer_flush(intel) \
	_intel_batchbuffer_flush(intel, __FILE__, __LINE__)



/* Unlike bmBufferData, this currently requires the buffer be mapped.
 * Consider it a convenience function wrapping multple
 * intel_buffer_dword() calls.
 */
void intel_batchbuffer_data(struct intel_context *intel,
                            const void *data, GLuint bytes);

bool intel_batchbuffer_emit_reloc(struct intel_context *intel,
                                       drm_intel_bo *buffer,
				       uint32_t read_domains,
				       uint32_t write_domain,
				       uint32_t offset);
bool intel_batchbuffer_emit_reloc_fenced(struct intel_context *intel,
					      drm_intel_bo *buffer,
					      uint32_t read_domains,
					      uint32_t write_domain,
					      uint32_t offset);
void intel_batchbuffer_emit_mi_flush(struct intel_context *intel);

static INLINE uint32_t float_as_int(float f)
{
   union {
      float f;
      uint32_t d;
   } fi;

   fi.f = f;
   return fi.d;
}

/* Inline functions - might actually be better off with these
 * non-inlined.  Certainly better off switching all command packets to
 * be passed as structs rather than dwords, but that's a little bit of
 * work...
 */
static INLINE unsigned
intel_batchbuffer_space(struct intel_context *intel)
{
   return (intel->batch.bo->size - intel->batch.reserved_space)
      - intel->batch.used*4;
}


static INLINE void
intel_batchbuffer_emit_dword(struct intel_context *intel, GLuint dword)
{
#ifdef DEBUG
   assert(intel_batchbuffer_space(intel) >= 4);
#endif
   intel->batch.map[intel->batch.used++] = dword;
}

static INLINE void
intel_batchbuffer_emit_float(struct intel_context *intel, float f)
{
   intel_batchbuffer_emit_dword(intel, float_as_int(f));
}

static INLINE void
intel_batchbuffer_require_space(struct intel_context *intel,
                                GLuint sz)
{
#ifdef DEBUG
   assert(sz < intel->maxBatchSize - BATCH_RESERVED);
#endif
   if (intel_batchbuffer_space(intel) < sz)
      intel_batchbuffer_flush(intel);
}

static INLINE void
intel_batchbuffer_begin(struct intel_context *intel, int n)
{
   intel_batchbuffer_require_space(intel, n * 4);

   intel->batch.emit = intel->batch.used;
#ifdef DEBUG
   intel->batch.total = n;
#endif
}

static INLINE void
intel_batchbuffer_advance(struct intel_context *intel)
{
#ifdef DEBUG
   struct intel_batchbuffer *batch = &intel->batch;
   unsigned int _n = batch->used - batch->emit;
   assert(batch->total != 0);
   if (_n != batch->total) {
      fprintf(stderr, "ADVANCE_BATCH: %d of %d dwords emitted\n",
	      _n, batch->total);
      abort();
   }
   batch->total = 0;
#endif
}

/* Here are the crusty old macros, to be removed:
 */
#define BATCH_LOCALS

#define BEGIN_BATCH(n) intel_batchbuffer_begin(intel, n)
#define OUT_BATCH(d) intel_batchbuffer_emit_dword(intel, d)
#define OUT_BATCH_F(f) intel_batchbuffer_emit_float(intel,f)
#define OUT_RELOC(buf, read_domains, write_domain, delta) do {		\
   intel_batchbuffer_emit_reloc(intel, buf,			\
				read_domains, write_domain, delta);	\
} while (0)
#define OUT_RELOC_FENCED(buf, read_domains, write_domain, delta) do {	\
   intel_batchbuffer_emit_reloc_fenced(intel, buf,		\
				       read_domains, write_domain, delta); \
} while (0)

#define ADVANCE_BATCH() intel_batchbuffer_advance(intel);
#define CACHED_BATCH() intel_batchbuffer_cached_advance(intel);

#ifdef __cplusplus
}
#endif

#endif
@


1.3
log
@Merge Mesa 9.2.0
@
text
@@


1.2
log
@Mesa 7.2, Tested by ckuethe@@, naddy@@, oga@@, and others.
@
text
@a0 27
/**************************************************************************
 * 
 * Copyright 2003 Tungsten Graphics, Inc., Cedar Park, Texas.
 * All Rights Reserved.
 * 
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sub license, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 * 
 * The above copyright notice and this permission notice (including the
 * next paragraph) shall be included in all copies or substantial portions
 * of the Software.
 * 
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
 * IN NO EVENT SHALL TUNGSTEN GRAPHICS AND/OR ITS SUPPLIERS BE LIABLE FOR
 * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 * 
 **************************************************************************/

d4 2
d7 6
a12 1
#include "intel_ioctl.h"
d14 21
a35 1
#define BATCH_LOCALS	GLubyte *batch_ptr;
a36 4
/* #define VERBOSE 0 */
#ifndef VERBOSE
extern int VERBOSE;
#endif
d38 25
d64 3
a66 11
#define BEGIN_BATCH(n)							\
do {									\
   if (VERBOSE) fprintf(stderr, 					\
			"BEGIN_BATCH(%ld) in %s, %d dwords free\n",	\
			((unsigned long)n), __FUNCTION__,		\
			intel->batch.space/4);				\
   if (intel->batch.space < (n)*4)					\
      intelFlushBatch(intel, GL_TRUE);					\
   if (intel->batch.space == intel->batch.size)	intel->batch.func = __FUNCTION__;			\
   batch_ptr = intel->batch.ptr;					\
} while (0)
d68 11
a78 6
#define OUT_BATCH(n)					\
do {							\
   *(GLuint *)batch_ptr = (n);				\
   if (VERBOSE) fprintf(stderr, " -- %08x at %s/%d\n", (n), __FILE__, __LINE__);	\
   batch_ptr += 4;					\
} while (0)
a79 39
#define ADVANCE_BATCH()						\
do {								\
   if (VERBOSE) fprintf(stderr, "ADVANCE_BATCH()\n");		\
   intel->batch.space -= (batch_ptr - intel->batch.ptr);	\
   intel->batch.ptr = batch_ptr;				\
   assert(intel->batch.space >= 0);				\
} while(0)

extern void intelInitBatchBuffer( GLcontext *ctx );
extern void intelDestroyBatchBuffer( GLcontext *ctx );

extern void intelStartInlinePrimitive( intelContextPtr intel, GLuint prim );
extern void intelWrapInlinePrimitive( intelContextPtr intel );
extern void intelRestartInlinePrimitive( intelContextPtr intel );
extern GLuint *intelEmitInlinePrimitiveLocked(intelContextPtr intel, 
					      int primitive, int dwords,
					      int vertex_size);
extern void intelCopyBuffer( const __DRIdrawablePrivate *dpriv,
			     const drm_clip_rect_t	*rect);
extern void intelClearWithBlit(GLcontext *ctx, GLbitfield mask, GLboolean all,
			     GLint cx1, GLint cy1, GLint cw, GLint ch);

extern void intelEmitCopyBlitLocked( intelContextPtr intel,
				     GLuint cpp,
				     GLshort src_pitch,
				     GLuint  src_offset,
				     GLshort dst_pitch,
				     GLuint  dst_offset,
				     GLshort srcx, GLshort srcy,
				     GLshort dstx, GLshort dsty,
				     GLshort w, GLshort h );

extern void intelEmitFillBlitLocked( intelContextPtr intel,
				     GLuint cpp,
				     GLshort dst_pitch,
				     GLuint dst_offset,
				     GLshort x, GLshort y, 
				     GLshort w, GLshort h,
				     GLuint color );
d81 8
d90 5
d96 10
d107 4
a110 10
static __inline GLuint *intelExtendInlinePrimitive( intelContextPtr intel, 
						GLuint dwords )
{
   GLuint sz = dwords * sizeof(GLuint);
   GLuint *ptr;

   if (intel->batch.space < sz) {
      intelWrapInlinePrimitive( intel );
/*       assert(intel->batch.space >= sz); */
   }
d112 5
a116 4
/*    assert(intel->prim.primitive != ~0); */
   ptr = (GLuint *)intel->batch.ptr;
   intel->batch.ptr += sz;
   intel->batch.space -= sz;
d118 14
a131 1
   return ptr;
d134 18
d153 3
@


1.1
log
@Initial revision
@
text
@@


1.1.1.1
log
@Import MesaLibs 6.5.1. (in dist/ since its code is shared between lib 
and xserver)...
@
text
@@


1.1.1.2
log
@Import Mesa 9.2.0
@
text
@d1 27
a30 2
#include "main/mtypes.h"

d32 1
a32 2
#include "intel_bufmgr.h"
#include "intel_reg.h"
a33 3
#ifdef __cplusplus
extern "C" {
#endif
d35 1
a35 21
/**
 * Number of bytes to reserve for commands necessary to complete a batch.
 *
 * This includes:
 * - MI_BATCHBUFFER_END (4 bytes)
 * - Optional MI_NOOP for ensuring the batch length is qword aligned (4 bytes)
 * - Any state emitted by vtbl->finish_batch():
 *   - Gen4-5 record ending occlusion query values (4 * 4 = 16 bytes)
 */
#define BATCH_RESERVED 24

struct intel_batchbuffer;

void intel_batchbuffer_init(struct intel_context *intel);
void intel_batchbuffer_free(struct intel_context *intel);

int _intel_batchbuffer_flush(struct intel_context *intel,
			     const char *file, int line);

#define intel_batchbuffer_flush(intel) \
	_intel_batchbuffer_flush(intel, __FILE__, __LINE__)
d37 4
d43 11
a53 18
/* Unlike bmBufferData, this currently requires the buffer be mapped.
 * Consider it a convenience function wrapping multple
 * intel_buffer_dword() calls.
 */
void intel_batchbuffer_data(struct intel_context *intel,
                            const void *data, GLuint bytes);

bool intel_batchbuffer_emit_reloc(struct intel_context *intel,
                                       drm_intel_bo *buffer,
				       uint32_t read_domains,
				       uint32_t write_domain,
				       uint32_t offset);
bool intel_batchbuffer_emit_reloc_fenced(struct intel_context *intel,
					      drm_intel_bo *buffer,
					      uint32_t read_domains,
					      uint32_t write_domain,
					      uint32_t offset);
void intel_batchbuffer_emit_mi_flush(struct intel_context *intel);
d55 6
a60 6
static INLINE uint32_t float_as_int(float f)
{
   union {
      float f;
      uint32_t d;
   } fi;
d62 39
a100 3
   fi.f = f;
   return fi.d;
}
a101 11
/* Inline functions - might actually be better off with these
 * non-inlined.  Certainly better off switching all command packets to
 * be passed as structs rather than dwords, but that's a little bit of
 * work...
 */
static INLINE unsigned
intel_batchbuffer_space(struct intel_context *intel)
{
   return (intel->batch.bo->size - intel->batch.reserved_space)
      - intel->batch.used*4;
}
a103 8
static INLINE void
intel_batchbuffer_emit_dword(struct intel_context *intel, GLuint dword)
{
#ifdef DEBUG
   assert(intel_batchbuffer_space(intel) >= 4);
#endif
   intel->batch.map[intel->batch.used++] = dword;
}
d105 10
a114 5
static INLINE void
intel_batchbuffer_emit_float(struct intel_context *intel, float f)
{
   intel_batchbuffer_emit_dword(intel, float_as_int(f));
}
d116 4
a119 21
static INLINE void
intel_batchbuffer_require_space(struct intel_context *intel,
                                GLuint sz)
{
#ifdef DEBUG
   assert(sz < intel->maxBatchSize - BATCH_RESERVED);
#endif
   if (intel_batchbuffer_space(intel) < sz)
      intel_batchbuffer_flush(intel);
}

static INLINE void
intel_batchbuffer_begin(struct intel_context *intel, int n)
{
   intel_batchbuffer_require_space(intel, n * 4);

   intel->batch.emit = intel->batch.used;
#ifdef DEBUG
   intel->batch.total = n;
#endif
}
d121 1
a121 14
static INLINE void
intel_batchbuffer_advance(struct intel_context *intel)
{
#ifdef DEBUG
   struct intel_batchbuffer *batch = &intel->batch;
   unsigned int _n = batch->used - batch->emit;
   assert(batch->total != 0);
   if (_n != batch->total) {
      fprintf(stderr, "ADVANCE_BATCH: %d of %d dwords emitted\n",
	      _n, batch->total);
      abort();
   }
   batch->total = 0;
#endif
a123 18
/* Here are the crusty old macros, to be removed:
 */
#define BATCH_LOCALS

#define BEGIN_BATCH(n) intel_batchbuffer_begin(intel, n)
#define OUT_BATCH(d) intel_batchbuffer_emit_dword(intel, d)
#define OUT_BATCH_F(f) intel_batchbuffer_emit_float(intel,f)
#define OUT_RELOC(buf, read_domains, write_domain, delta) do {		\
   intel_batchbuffer_emit_reloc(intel, buf,			\
				read_domains, write_domain, delta);	\
} while (0)
#define OUT_RELOC_FENCED(buf, read_domains, write_domain, delta) do {	\
   intel_batchbuffer_emit_reloc_fenced(intel, buf,		\
				       read_domains, write_domain, delta); \
} while (0)

#define ADVANCE_BATCH() intel_batchbuffer_advance(intel);
#define CACHED_BATCH() intel_batchbuffer_cached_advance(intel);
a124 3
#ifdef __cplusplus
}
#endif
@

