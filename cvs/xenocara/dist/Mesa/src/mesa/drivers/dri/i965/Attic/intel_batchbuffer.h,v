head	1.5;
access;
symbols
	OPENBSD_5_8:1.4.0.6
	OPENBSD_5_8_BASE:1.4
	OPENBSD_5_7:1.4.0.4
	OPENBSD_5_7_BASE:1.4
	v10_2_9:1.1.1.4
	v10_4_3:1.1.1.4
	v10_2_7:1.1.1.4
	OPENBSD_5_6:1.4.0.2
	OPENBSD_5_6_BASE:1.4
	v10_2_3:1.1.1.4
	OPENBSD_5_5:1.3.0.2
	OPENBSD_5_5_BASE:1.3
	v9_2_5:1.1.1.3
	v9_2_3:1.1.1.3
	v9_2_2:1.1.1.3
	v9_2_1:1.1.1.3
	v9_2_0:1.1.1.3
	OPENBSD_4_4:1.1.1.2.0.6
	OPENBSD_4_4_BASE:1.1.1.2
	OPENBSD_4_3_BASE:1.1.1.2
	OPENBSD_4_3:1.1.1.2.0.4
	v7_0_1:1.1.1.2
	OPENBSD_4_2:1.1.1.2.0.2
	OPENBSD_4_2_BASE:1.1.1.2
	v6_5_2:1.1.1.2
	v6_5_1:1.1.1.1
	mesa:1.1.1;
locks; strict;
comment	@ * @;


1.5
date	2015.12.23.05.17.50;	author jsg;	state dead;
branches;
next	1.4;
commitid	TnlogFl9nOv2eaRf;

1.4
date	2014.07.09.21.08.59;	author jsg;	state Exp;
branches;
next	1.3;
commitid	WPD6rgPryPkvXOr9;

1.3
date	2013.09.05.14.04.26;	author jsg;	state Exp;
branches;
next	1.2;

1.2
date	2008.11.02.14.58.15;	author matthieu;	state dead;
branches;
next	1.1;

1.1
date	2006.11.25.18.52.50;	author matthieu;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2006.11.25.18.52.50;	author matthieu;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2007.03.03.11.57.18;	author matthieu;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2013.09.05.13.15.45;	author jsg;	state Exp;
branches;
next	1.1.1.4;

1.1.1.4
date	2014.07.09.20.34.53;	author jsg;	state Exp;
branches;
next	;
commitid	3JhLfwcuBALP0ZR7;


desc
@@


1.5
log
@remove the now unused Mesa 10.2.9 code
@
text
@#ifndef INTEL_BATCHBUFFER_H
#define INTEL_BATCHBUFFER_H

#include "main/mtypes.h"

#include "brw_context.h"
#include "intel_bufmgr.h"
#include "intel_reg.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * Number of bytes to reserve for commands necessary to complete a batch.
 *
 * This includes:
 * - MI_BATCHBUFFER_END (4 bytes)
 * - Optional MI_NOOP for ensuring the batch length is qword aligned (4 bytes)
 * - Any state emitted by vtbl->finish_batch():
 *   - Gen4-5 record ending occlusion query values (4 * 4 = 16 bytes)
 *   - Disabling OA counters on Gen6+ (3 DWords = 12 bytes)
 *   - Ending MI_REPORT_PERF_COUNT on Gen5+, plus associated PIPE_CONTROLs:
 *     - Two sets of PIPE_CONTROLs, which become 3 PIPE_CONTROLs each on SNB,
 *       which are 4 DWords each ==> 2 * 3 * 4 * 4 = 96 bytes
 *     - 3 DWords for MI_REPORT_PERF_COUNT itself on Gen6+.  ==> 12 bytes.
 *       On Ironlake, it's 6 DWords, but we have some slack due to the lack of
 *       Sandybridge PIPE_CONTROL madness.
 */
#define BATCH_RESERVED 146

struct intel_batchbuffer;

void intel_batchbuffer_emit_render_ring_prelude(struct brw_context *brw);
void intel_batchbuffer_init(struct brw_context *brw);
void intel_batchbuffer_free(struct brw_context *brw);
void intel_batchbuffer_save_state(struct brw_context *brw);
void intel_batchbuffer_reset_to_saved(struct brw_context *brw);

int _intel_batchbuffer_flush(struct brw_context *brw,
			     const char *file, int line);

#define intel_batchbuffer_flush(intel) \
	_intel_batchbuffer_flush(intel, __FILE__, __LINE__)



/* Unlike bmBufferData, this currently requires the buffer be mapped.
 * Consider it a convenience function wrapping multple
 * intel_buffer_dword() calls.
 */
void intel_batchbuffer_data(struct brw_context *brw,
                            const void *data, GLuint bytes,
                            enum brw_gpu_ring ring);

bool intel_batchbuffer_emit_reloc(struct brw_context *brw,
                                       drm_intel_bo *buffer,
				       uint32_t read_domains,
				       uint32_t write_domain,
				       uint32_t offset);
bool intel_batchbuffer_emit_reloc64(struct brw_context *brw,
                                    drm_intel_bo *buffer,
                                    uint32_t read_domains,
                                    uint32_t write_domain,
                                    uint32_t offset);
void brw_emit_pipe_control_flush(struct brw_context *brw, uint32_t flags);
void brw_emit_pipe_control_write(struct brw_context *brw, uint32_t flags,
                                 drm_intel_bo *bo, uint32_t offset,
                                 uint32_t imm_lower, uint32_t imm_upper);
void intel_batchbuffer_emit_mi_flush(struct brw_context *brw);
void intel_emit_post_sync_nonzero_flush(struct brw_context *brw);
void intel_emit_depth_stall_flushes(struct brw_context *brw);
void gen7_emit_vs_workaround_flush(struct brw_context *brw);
void gen7_emit_cs_stall_flush(struct brw_context *brw);

static inline uint32_t float_as_int(float f)
{
   union {
      float f;
      uint32_t d;
   } fi;

   fi.f = f;
   return fi.d;
}

/* Inline functions - might actually be better off with these
 * non-inlined.  Certainly better off switching all command packets to
 * be passed as structs rather than dwords, but that's a little bit of
 * work...
 */
static inline unsigned
intel_batchbuffer_space(struct brw_context *brw)
{
   return (brw->batch.state_batch_offset - brw->batch.reserved_space)
      - brw->batch.used*4;
}


static inline void
intel_batchbuffer_emit_dword(struct brw_context *brw, GLuint dword)
{
#ifdef DEBUG
   assert(intel_batchbuffer_space(brw) >= 4);
#endif
   brw->batch.map[brw->batch.used++] = dword;
   assert(brw->batch.ring != UNKNOWN_RING);
}

static inline void
intel_batchbuffer_emit_float(struct brw_context *brw, float f)
{
   intel_batchbuffer_emit_dword(brw, float_as_int(f));
}

static inline void
intel_batchbuffer_require_space(struct brw_context *brw, GLuint sz,
                                enum brw_gpu_ring ring)
{
   /* If we're switching rings, implicitly flush the batch. */
   if (unlikely(ring != brw->batch.ring) && brw->batch.ring != UNKNOWN_RING &&
       brw->gen >= 6) {
      intel_batchbuffer_flush(brw);
   }

#ifdef DEBUG
   assert(sz < BATCH_SZ - BATCH_RESERVED);
#endif
   if (intel_batchbuffer_space(brw) < sz)
      intel_batchbuffer_flush(brw);

   enum brw_gpu_ring prev_ring = brw->batch.ring;
   /* The intel_batchbuffer_flush() calls above might have changed
    * brw->batch.ring to UNKNOWN_RING, so we need to set it here at the end.
    */
   brw->batch.ring = ring;

   if (unlikely(prev_ring == UNKNOWN_RING && ring == RENDER_RING))
      intel_batchbuffer_emit_render_ring_prelude(brw);
}

static inline void
intel_batchbuffer_begin(struct brw_context *brw, int n, enum brw_gpu_ring ring)
{
   intel_batchbuffer_require_space(brw, n * 4, ring);

   brw->batch.emit = brw->batch.used;
#ifdef DEBUG
   brw->batch.total = n;
#endif
}

static inline void
intel_batchbuffer_advance(struct brw_context *brw)
{
#ifdef DEBUG
   struct intel_batchbuffer *batch = &brw->batch;
   unsigned int _n = batch->used - batch->emit;
   assert(batch->total != 0);
   if (_n != batch->total) {
      fprintf(stderr, "ADVANCE_BATCH: %d of %d dwords emitted\n",
	      _n, batch->total);
      abort();
   }
   batch->total = 0;
#endif
}

#define BEGIN_BATCH(n) intel_batchbuffer_begin(brw, n, RENDER_RING)
#define BEGIN_BATCH_BLT(n) intel_batchbuffer_begin(brw, n, BLT_RING)
#define OUT_BATCH(d) intel_batchbuffer_emit_dword(brw, d)
#define OUT_BATCH_F(f) intel_batchbuffer_emit_float(brw, f)
#define OUT_RELOC(buf, read_domains, write_domain, delta) do {		\
   intel_batchbuffer_emit_reloc(brw, buf,			\
				read_domains, write_domain, delta);	\
} while (0)

/* Handle 48-bit address relocations for Gen8+ */
#define OUT_RELOC64(buf, read_domains, write_domain, delta) do { \
   intel_batchbuffer_emit_reloc64(brw, buf, read_domains, write_domain, delta);	\
} while (0)

#define ADVANCE_BATCH() intel_batchbuffer_advance(brw);

#ifdef __cplusplus
}
#endif

#endif
@


1.4
log
@Merge Mesa 10.2.3
tested by matthieu@@ kettenis@@ mpi@@ brett@@ and myself across a
diverse range of hardware
@
text
@@


1.3
log
@Merge Mesa 9.2.0
@
text
@d22 7
d30 1
a30 1
#define BATCH_RESERVED 24
d34 1
d53 2
a54 1
                            const void *data, GLuint bytes, bool is_blit);
d61 9
a69 5
bool intel_batchbuffer_emit_reloc_fenced(struct brw_context *brw,
					      drm_intel_bo *buffer,
					      uint32_t read_domains,
					      uint32_t write_domain,
					      uint32_t offset);
d74 1
d76 1
a76 1
static INLINE uint32_t float_as_int(float f)
d92 1
a92 1
static INLINE unsigned
d100 1
a100 1
static INLINE void
d107 1
d110 1
a110 1
static INLINE void
d116 7
a122 5
static INLINE void
intel_batchbuffer_require_space(struct brw_context *brw, GLuint sz, int is_blit)
{
   if (brw->gen >= 6 &&
       brw->batch.is_blit != is_blit && brw->batch.used) {
a125 2
   brw->batch.is_blit = is_blit;

d131 9
d142 2
a143 2
static INLINE void
intel_batchbuffer_begin(struct brw_context *brw, int n, bool is_blit)
d145 1
a145 1
   intel_batchbuffer_require_space(brw, n * 4, is_blit);
d153 1
a153 1
static INLINE void
d169 2
a170 8
void intel_batchbuffer_cached_advance(struct brw_context *brw);

/* Here are the crusty old macros, to be removed:
 */
#define BATCH_LOCALS

#define BEGIN_BATCH(n) intel_batchbuffer_begin(brw, n, false)
#define BEGIN_BATCH_BLT(n) intel_batchbuffer_begin(brw, n, true)
d177 4
a180 3
#define OUT_RELOC_FENCED(buf, read_domains, write_domain, delta) do {	\
   intel_batchbuffer_emit_reloc_fenced(brw, buf,		\
				       read_domains, write_domain, delta); \
a183 1
#define CACHED_BATCH() intel_batchbuffer_cached_advance(brw);
@


1.2
log
@Mesa 7.2, Tested by ckuethe@@, naddy@@, oga@@, and others.
@
text
@a0 27
/**************************************************************************
 * 
 * Copyright 2006 Tungsten Graphics, Inc., Cedar Park, Texas.
 * All Rights Reserved.
 * 
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sub license, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 * 
 * The above copyright notice and this permission notice (including the
 * next paragraph) shall be included in all copies or substantial portions
 * of the Software.
 * 
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
 * IN NO EVENT SHALL TUNGSTEN GRAPHICS AND/OR ITS SUPPLIERS BE LIABLE FOR
 * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 * 
 **************************************************************************/

d4 1
a4 2
#include "mtypes.h"
#include "bufmgr.h"
d6 18
a23 13
struct intel_context;

#define BATCH_SZ (16 * 1024)
#define BATCH_REFILL 4096
#define BATCH_RESERVED 16

#define INTEL_BATCH_NO_CLIPRECTS 0x1
#define INTEL_BATCH_CLIPRECTS    0x2

struct intel_batchbuffer {
   struct intel_context *intel;

   struct buffer *buffer;
d25 1
a25 2
   GLuint flags;
   GLuint offset;
d27 4
a30 3
   GLubyte *map;
   GLubyte *ptr; 
};
d32 2
a33 1
struct intel_batchbuffer *intel_batchbuffer_alloc( struct intel_context *intel );
d35 2
a36 1
void intel_batchbuffer_free( struct intel_batchbuffer *batch );
a38 5
GLboolean intel_batchbuffer_flush( struct intel_batchbuffer *batch );

void intel_batchbuffer_unmap( struct intel_batchbuffer *batch );
GLubyte *intel_batchbuffer_map( struct intel_batchbuffer *batch );

d44 17
a60 4
void intel_batchbuffer_data(struct intel_batchbuffer *batch,
			    const void *data,
			    GLuint bytes,
			    GLuint flags);
d62 6
a67 2
void intel_batchbuffer_release_space(struct intel_batchbuffer *batch,
				   GLuint bytes);
d69 3
d78 2
a79 2
static inline GLuint 
intel_batchbuffer_space( struct intel_batchbuffer *batch )
d81 2
a82 1
   return (BATCH_SZ - BATCH_RESERVED) - (batch->ptr - (batch->map + batch->offset));
d86 2
a87 3
static inline void 
intel_batchbuffer_emit_dword(struct intel_batchbuffer *batch,
			     GLuint dword)
d89 4
a92 4
   assert(batch->map);
   assert(intel_batchbuffer_space(batch) >= 4);
   *(GLuint *)(batch->ptr) = dword;
   batch->ptr += 4;
d95 2
a96 4
static inline void 
intel_batchbuffer_require_space(struct intel_batchbuffer *batch,
				GLuint sz,
				GLuint flags)
d98 1
a98 6
   assert(sz < BATCH_SZ - 8);
   if (intel_batchbuffer_space(batch) < sz ||
       (batch->flags != 0 && flags != 0 && batch->flags != flags))
      intel_batchbuffer_flush(batch);
   
   batch->flags |= flags;
d101 21
a121 3
void intel_batchbuffer_align( struct intel_batchbuffer *batch,
			      GLuint align,
			      GLuint sz );
d123 23
d149 14
a162 4
#define BATCH_LOCALS 
#define BEGIN_BATCH(n, flags) intel_batchbuffer_require_space(intel->batch, n*4, flags)
#define OUT_BATCH(d)  intel_batchbuffer_emit_dword(intel->batch, d)
#define ADVANCE_BATCH() do { } while(0)
d164 6
@


1.1
log
@Initial revision
@
text
@@


1.1.1.1
log
@Import MesaLibs 6.5.1. (in dist/ since its code is shared between lib 
and xserver)...
@
text
@@


1.1.1.2
log
@import MesaLibs version 6.5.2
@
text
@d49 1
a49 1
   unsigned long offset;
@


1.1.1.3
log
@Import Mesa 9.2.0
@
text
@d1 27
d31 16
a46 1
#include "main/mtypes.h"
d48 6
a53 18
#include "brw_context.h"
#include "intel_bufmgr.h"
#include "intel_reg.h"

#ifdef __cplusplus
extern "C" {
#endif

/**
 * Number of bytes to reserve for commands necessary to complete a batch.
 *
 * This includes:
 * - MI_BATCHBUFFER_END (4 bytes)
 * - Optional MI_NOOP for ensuring the batch length is qword aligned (4 bytes)
 * - Any state emitted by vtbl->finish_batch():
 *   - Gen4-5 record ending occlusion query values (4 * 4 = 16 bytes)
 */
#define BATCH_RESERVED 24
d55 1
a55 1
struct intel_batchbuffer;
d57 1
a57 4
void intel_batchbuffer_init(struct brw_context *brw);
void intel_batchbuffer_free(struct brw_context *brw);
void intel_batchbuffer_save_state(struct brw_context *brw);
void intel_batchbuffer_reset_to_saved(struct brw_context *brw);
a58 2
int _intel_batchbuffer_flush(struct brw_context *brw,
			     const char *file, int line);
d60 1
a60 2
#define intel_batchbuffer_flush(intel) \
	_intel_batchbuffer_flush(intel, __FILE__, __LINE__)
d62 2
d70 4
a73 17
void intel_batchbuffer_data(struct brw_context *brw,
                            const void *data, GLuint bytes, bool is_blit);

bool intel_batchbuffer_emit_reloc(struct brw_context *brw,
                                       drm_intel_bo *buffer,
				       uint32_t read_domains,
				       uint32_t write_domain,
				       uint32_t offset);
bool intel_batchbuffer_emit_reloc_fenced(struct brw_context *brw,
					      drm_intel_bo *buffer,
					      uint32_t read_domains,
					      uint32_t write_domain,
					      uint32_t offset);
void intel_batchbuffer_emit_mi_flush(struct brw_context *brw);
void intel_emit_post_sync_nonzero_flush(struct brw_context *brw);
void intel_emit_depth_stall_flushes(struct brw_context *brw);
void gen7_emit_vs_workaround_flush(struct brw_context *brw);
d75 2
a76 6
static INLINE uint32_t float_as_int(float f)
{
   union {
      float f;
      uint32_t d;
   } fi;
a77 3
   fi.f = f;
   return fi.d;
}
d84 2
a85 2
static INLINE unsigned
intel_batchbuffer_space(struct brw_context *brw)
d87 1
a87 2
   return (brw->batch.state_batch_offset - brw->batch.reserved_space)
      - brw->batch.used*4;
d91 3
a93 2
static INLINE void
intel_batchbuffer_emit_dword(struct brw_context *brw, GLuint dword)
d95 4
a98 4
#ifdef DEBUG
   assert(intel_batchbuffer_space(brw) >= 4);
#endif
   brw->batch.map[brw->batch.used++] = dword;
d101 4
a104 2
static INLINE void
intel_batchbuffer_emit_float(struct brw_context *brw, float f)
d106 6
a111 1
   intel_batchbuffer_emit_dword(brw, float_as_int(f));
d114 3
a116 21
static INLINE void
intel_batchbuffer_require_space(struct brw_context *brw, GLuint sz, int is_blit)
{
   if (brw->gen >= 6 &&
       brw->batch.is_blit != is_blit && brw->batch.used) {
      intel_batchbuffer_flush(brw);
   }

   brw->batch.is_blit = is_blit;

#ifdef DEBUG
   assert(sz < BATCH_SZ - BATCH_RESERVED);
#endif
   if (intel_batchbuffer_space(brw) < sz)
      intel_batchbuffer_flush(brw);
}

static INLINE void
intel_batchbuffer_begin(struct brw_context *brw, int n, bool is_blit)
{
   intel_batchbuffer_require_space(brw, n * 4, is_blit);
a117 23
   brw->batch.emit = brw->batch.used;
#ifdef DEBUG
   brw->batch.total = n;
#endif
}

static INLINE void
intel_batchbuffer_advance(struct brw_context *brw)
{
#ifdef DEBUG
   struct intel_batchbuffer *batch = &brw->batch;
   unsigned int _n = batch->used - batch->emit;
   assert(batch->total != 0);
   if (_n != batch->total) {
      fprintf(stderr, "ADVANCE_BATCH: %d of %d dwords emitted\n",
	      _n, batch->total);
      abort();
   }
   batch->total = 0;
#endif
}

void intel_batchbuffer_cached_advance(struct brw_context *brw);
d121 4
a124 14
#define BATCH_LOCALS

#define BEGIN_BATCH(n) intel_batchbuffer_begin(brw, n, false)
#define BEGIN_BATCH_BLT(n) intel_batchbuffer_begin(brw, n, true)
#define OUT_BATCH(d) intel_batchbuffer_emit_dword(brw, d)
#define OUT_BATCH_F(f) intel_batchbuffer_emit_float(brw, f)
#define OUT_RELOC(buf, read_domains, write_domain, delta) do {		\
   intel_batchbuffer_emit_reloc(brw, buf,			\
				read_domains, write_domain, delta);	\
} while (0)
#define OUT_RELOC_FENCED(buf, read_domains, write_domain, delta) do {	\
   intel_batchbuffer_emit_reloc_fenced(brw, buf,		\
				       read_domains, write_domain, delta); \
} while (0)
a125 6
#define ADVANCE_BATCH() intel_batchbuffer_advance(brw);
#define CACHED_BATCH() intel_batchbuffer_cached_advance(brw);

#ifdef __cplusplus
}
#endif
@


1.1.1.4
log
@Import Mesa 10.2.3
@
text
@a21 7
 *   - Disabling OA counters on Gen6+ (3 DWords = 12 bytes)
 *   - Ending MI_REPORT_PERF_COUNT on Gen5+, plus associated PIPE_CONTROLs:
 *     - Two sets of PIPE_CONTROLs, which become 3 PIPE_CONTROLs each on SNB,
 *       which are 4 DWords each ==> 2 * 3 * 4 * 4 = 96 bytes
 *     - 3 DWords for MI_REPORT_PERF_COUNT itself on Gen6+.  ==> 12 bytes.
 *       On Ironlake, it's 6 DWords, but we have some slack due to the lack of
 *       Sandybridge PIPE_CONTROL madness.
d23 1
a23 1
#define BATCH_RESERVED 146
a26 1
void intel_batchbuffer_emit_render_ring_prelude(struct brw_context *brw);
d45 1
a45 2
                            const void *data, GLuint bytes,
                            enum brw_gpu_ring ring);
d52 5
a56 9
bool intel_batchbuffer_emit_reloc64(struct brw_context *brw,
                                    drm_intel_bo *buffer,
                                    uint32_t read_domains,
                                    uint32_t write_domain,
                                    uint32_t offset);
void brw_emit_pipe_control_flush(struct brw_context *brw, uint32_t flags);
void brw_emit_pipe_control_write(struct brw_context *brw, uint32_t flags,
                                 drm_intel_bo *bo, uint32_t offset,
                                 uint32_t imm_lower, uint32_t imm_upper);
a60 1
void gen7_emit_cs_stall_flush(struct brw_context *brw);
d62 1
a62 1
static inline uint32_t float_as_int(float f)
d78 1
a78 1
static inline unsigned
d86 1
a86 1
static inline void
a92 1
   assert(brw->batch.ring != UNKNOWN_RING);
d95 1
a95 1
static inline void
d101 5
a105 7
static inline void
intel_batchbuffer_require_space(struct brw_context *brw, GLuint sz,
                                enum brw_gpu_ring ring)
{
   /* If we're switching rings, implicitly flush the batch. */
   if (unlikely(ring != brw->batch.ring) && brw->batch.ring != UNKNOWN_RING &&
       brw->gen >= 6) {
d109 2
a115 9

   enum brw_gpu_ring prev_ring = brw->batch.ring;
   /* The intel_batchbuffer_flush() calls above might have changed
    * brw->batch.ring to UNKNOWN_RING, so we need to set it here at the end.
    */
   brw->batch.ring = ring;

   if (unlikely(prev_ring == UNKNOWN_RING && ring == RENDER_RING))
      intel_batchbuffer_emit_render_ring_prelude(brw);
d118 2
a119 2
static inline void
intel_batchbuffer_begin(struct brw_context *brw, int n, enum brw_gpu_ring ring)
d121 1
a121 1
   intel_batchbuffer_require_space(brw, n * 4, ring);
d129 1
a129 1
static inline void
d145 8
a152 2
#define BEGIN_BATCH(n) intel_batchbuffer_begin(brw, n, RENDER_RING)
#define BEGIN_BATCH_BLT(n) intel_batchbuffer_begin(brw, n, BLT_RING)
d159 3
a161 4

/* Handle 48-bit address relocations for Gen8+ */
#define OUT_RELOC64(buf, read_domains, write_domain, delta) do { \
   intel_batchbuffer_emit_reloc64(brw, buf, read_domains, write_domain, delta);	\
d165 1
@


