head	1.10;
access;
symbols
	OPENBSD_5_8:1.9.0.4
	OPENBSD_5_8_BASE:1.9
	OPENBSD_5_7:1.9.0.2
	OPENBSD_5_7_BASE:1.9
	v10_2_9:1.1.1.8
	v10_4_3:1.1.1.7
	v10_2_7:1.1.1.6
	OPENBSD_5_6:1.6.0.2
	OPENBSD_5_6_BASE:1.6
	v10_2_3:1.1.1.5
	OPENBSD_5_5:1.5.0.2
	OPENBSD_5_5_BASE:1.5
	v9_2_5:1.1.1.4
	v9_2_3:1.1.1.3
	v9_2_2:1.1.1.3
	v9_2_1:1.1.1.3
	v9_2_0:1.1.1.2
	OPENBSD_5_4:1.2.0.4
	OPENBSD_5_4_BASE:1.2
	OPENBSD_5_3:1.2.0.2
	OPENBSD_5_3_BASE:1.2
	OPENBSD_5_2:1.1.1.1.0.4
	OPENBSD_5_2_BASE:1.1.1.1
	OPENBSD_5_1_BASE:1.1.1.1
	OPENBSD_5_1:1.1.1.1.0.2
	v7_10_3:1.1.1.1
	mesa:1.1.1;
locks; strict;
comment	@// @;


1.10
date	2015.12.23.05.17.49;	author jsg;	state dead;
branches;
next	1.9;
commitid	TnlogFl9nOv2eaRf;

1.9
date	2015.02.20.23.09.58;	author jsg;	state Exp;
branches;
next	1.8;
commitid	4ry2gvZGMXkCUD2n;

1.8
date	2015.01.25.14.41.20;	author jsg;	state Exp;
branches;
next	1.7;
commitid	mcxB0JvoI9gTDYXU;

1.7
date	2014.09.07.15.20.07;	author jsg;	state Exp;
branches;
next	1.6;
commitid	7kimTMT4YlQauAIU;

1.6
date	2014.07.09.21.08.59;	author jsg;	state Exp;
branches;
next	1.5;
commitid	WPD6rgPryPkvXOr9;

1.5
date	2014.01.19.03.13.15;	author jsg;	state Exp;
branches;
next	1.4;

1.4
date	2013.10.05.09.53.58;	author jsg;	state Exp;
branches;
next	1.3;

1.3
date	2013.09.05.14.04.18;	author jsg;	state Exp;
branches;
next	1.2;

1.2
date	2012.08.17.13.58.15;	author mpi;	state Exp;
branches;
next	1.1;

1.1
date	2011.10.23.13.29.36;	author matthieu;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2011.10.23.13.29.36;	author matthieu;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2013.09.05.13.15.32;	author jsg;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2013.10.05.09.25.16;	author jsg;	state Exp;
branches;
next	1.1.1.4;

1.1.1.4
date	2014.01.19.03.04.26;	author jsg;	state Exp;
branches;
next	1.1.1.5;

1.1.1.5
date	2014.07.09.20.34.50;	author jsg;	state Exp;
branches;
next	1.1.1.6;
commitid	3JhLfwcuBALP0ZR7;

1.1.1.6
date	2014.09.07.15.06.06;	author jsg;	state Exp;
branches;
next	1.1.1.7;
commitid	dm8VnQHhowGHmemJ;

1.1.1.7
date	2015.01.25.14.11.36;	author jsg;	state Exp;
branches;
next	1.1.1.8;
commitid	ce2W5rH5aF7VS9gi;

1.1.1.8
date	2015.02.20.22.48.43;	author jsg;	state Exp;
branches;
next	;
commitid	F54a1i0WXHMxq7kE;


desc
@@


1.10
log
@remove the now unused Mesa 10.2.9 code
@
text
@/*
 * Copyright Â© 2010 Intel Corporation
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice (including the next
 * paragraph) shall be included in all copies or substantial portions of the
 * Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
 * IN THE SOFTWARE.
 */

/** @@file brw_fs.cpp
 *
 * This file drives the GLSL IR -> LIR translation, contains the
 * optimizations on the LIR, and drives the generation of native code
 * from the LIR.
 */

extern "C" {

#include <sys/types.h>

#include "main/hash_table.h"
#include "main/macros.h"
#include "main/shaderobj.h"
#include "main/fbobject.h"
#include "program/prog_parameter.h"
#include "program/prog_print.h"
#include "program/register_allocate.h"
#include "program/sampler.h"
#include "program/hash_table.h"
#include "brw_context.h"
#include "brw_eu.h"
#include "brw_wm.h"
}
#include "brw_fs.h"
#include "brw_dead_control_flow.h"
#include "main/uniforms.h"
#include "brw_fs_live_variables.h"
#include "glsl/glsl_types.h"

void
fs_inst::init()
{
   memset(this, 0, sizeof(*this));
   this->conditional_mod = BRW_CONDITIONAL_NONE;

   this->dst = reg_undef;
   this->src[0] = reg_undef;
   this->src[1] = reg_undef;
   this->src[2] = reg_undef;

   /* This will be the case for almost all instructions. */
   this->regs_written = 1;

   this->writes_accumulator = false;
}

fs_inst::fs_inst()
{
   init();
   this->opcode = BRW_OPCODE_NOP;
}

fs_inst::fs_inst(enum opcode opcode)
{
   init();
   this->opcode = opcode;
}

fs_inst::fs_inst(enum opcode opcode, fs_reg dst)
{
   init();
   this->opcode = opcode;
   this->dst = dst;

   if (dst.file == GRF)
      assert(dst.reg_offset >= 0);
}

fs_inst::fs_inst(enum opcode opcode, fs_reg dst, fs_reg src0)
{
   init();
   this->opcode = opcode;
   this->dst = dst;
   this->src[0] = src0;

   if (dst.file == GRF)
      assert(dst.reg_offset >= 0);
   if (src[0].file == GRF)
      assert(src[0].reg_offset >= 0);
}

fs_inst::fs_inst(enum opcode opcode, fs_reg dst, fs_reg src0, fs_reg src1)
{
   init();
   this->opcode = opcode;
   this->dst = dst;
   this->src[0] = src0;
   this->src[1] = src1;

   if (dst.file == GRF)
      assert(dst.reg_offset >= 0);
   if (src[0].file == GRF)
      assert(src[0].reg_offset >= 0);
   if (src[1].file == GRF)
      assert(src[1].reg_offset >= 0);
}

fs_inst::fs_inst(enum opcode opcode, fs_reg dst,
		 fs_reg src0, fs_reg src1, fs_reg src2)
{
   init();
   this->opcode = opcode;
   this->dst = dst;
   this->src[0] = src0;
   this->src[1] = src1;
   this->src[2] = src2;

   if (dst.file == GRF)
      assert(dst.reg_offset >= 0);
   if (src[0].file == GRF)
      assert(src[0].reg_offset >= 0);
   if (src[1].file == GRF)
      assert(src[1].reg_offset >= 0);
   if (src[2].file == GRF)
      assert(src[2].reg_offset >= 0);
}

#define ALU1(op)                                                        \
   fs_inst *                                                            \
   fs_visitor::op(fs_reg dst, fs_reg src0)                              \
   {                                                                    \
      return new(mem_ctx) fs_inst(BRW_OPCODE_##op, dst, src0);          \
   }

#define ALU2(op)                                                        \
   fs_inst *                                                            \
   fs_visitor::op(fs_reg dst, fs_reg src0, fs_reg src1)                 \
   {                                                                    \
      return new(mem_ctx) fs_inst(BRW_OPCODE_##op, dst, src0, src1);    \
   }

#define ALU2_ACC(op)                                                    \
   fs_inst *                                                            \
   fs_visitor::op(fs_reg dst, fs_reg src0, fs_reg src1)                 \
   {                                                                    \
      fs_inst *inst = new(mem_ctx) fs_inst(BRW_OPCODE_##op, dst, src0, src1);\
      inst->writes_accumulator = true;                                  \
      return inst;                                                      \
   }

#define ALU3(op)                                                        \
   fs_inst *                                                            \
   fs_visitor::op(fs_reg dst, fs_reg src0, fs_reg src1, fs_reg src2)    \
   {                                                                    \
      return new(mem_ctx) fs_inst(BRW_OPCODE_##op, dst, src0, src1, src2);\
   }

ALU1(NOT)
ALU1(MOV)
ALU1(FRC)
ALU1(RNDD)
ALU1(RNDE)
ALU1(RNDZ)
ALU2(ADD)
ALU2(MUL)
ALU2_ACC(MACH)
ALU2(AND)
ALU2(OR)
ALU2(XOR)
ALU2(SHL)
ALU2(SHR)
ALU2(ASR)
ALU3(LRP)
ALU1(BFREV)
ALU3(BFE)
ALU2(BFI1)
ALU3(BFI2)
ALU1(FBH)
ALU1(FBL)
ALU1(CBIT)
ALU3(MAD)
ALU2_ACC(ADDC)
ALU2_ACC(SUBB)
ALU2(SEL)
ALU2(MAC)

/** Gen4 predicated IF. */
fs_inst *
fs_visitor::IF(uint32_t predicate)
{
   fs_inst *inst = new(mem_ctx) fs_inst(BRW_OPCODE_IF);
   inst->predicate = predicate;
   return inst;
}

/** Gen6 IF with embedded comparison. */
fs_inst *
fs_visitor::IF(fs_reg src0, fs_reg src1, uint32_t condition)
{
   assert(brw->gen == 6);
   fs_inst *inst = new(mem_ctx) fs_inst(BRW_OPCODE_IF,
                                        reg_null_d, src0, src1);
   inst->conditional_mod = condition;
   return inst;
}

/**
 * CMP: Sets the low bit of the destination channels with the result
 * of the comparison, while the upper bits are undefined, and updates
 * the flag register with the packed 16 bits of the result.
 */
fs_inst *
fs_visitor::CMP(fs_reg dst, fs_reg src0, fs_reg src1, uint32_t condition)
{
   fs_inst *inst;

   /* Take the instruction:
    *
    * CMP null<d> src0<f> src1<f>
    *
    * Original gen4 does type conversion to the destination type before
    * comparison, producing garbage results for floating point comparisons.
    * gen5 does the comparison on the execution type (resolved source types),
    * so dst type doesn't matter.  gen6 does comparison and then uses the
    * result as if it was the dst type with no conversion, which happens to
    * mostly work out for float-interpreted-as-int since our comparisons are
    * for >0, =0, <0.
    */
   if (brw->gen == 4) {
      dst.type = src0.type;
      if (dst.file == HW_REG)
	 dst.fixed_hw_reg.type = dst.type;
   }

   resolve_ud_negate(&src0);
   resolve_ud_negate(&src1);

   inst = new(mem_ctx) fs_inst(BRW_OPCODE_CMP, dst, src0, src1);
   inst->conditional_mod = condition;

   return inst;
}

exec_list
fs_visitor::VARYING_PULL_CONSTANT_LOAD(const fs_reg &dst,
                                       const fs_reg &surf_index,
                                       const fs_reg &varying_offset,
                                       uint32_t const_offset)
{
   exec_list instructions;
   fs_inst *inst;

   /* We have our constant surface use a pitch of 4 bytes, so our index can
    * be any component of a vector, and then we load 4 contiguous
    * components starting from that.
    *
    * We break down the const_offset to a portion added to the variable
    * offset and a portion done using reg_offset, which means that if you
    * have GLSL using something like "uniform vec4 a[20]; gl_FragColor =
    * a[i]", we'll temporarily generate 4 vec4 loads from offset i * 4, and
    * CSE can later notice that those loads are all the same and eliminate
    * the redundant ones.
    */
   fs_reg vec4_offset = fs_reg(this, glsl_type::int_type);
   instructions.push_tail(ADD(vec4_offset,
                              varying_offset, const_offset & ~3));

   int scale = 1;
   if (brw->gen == 4 && dispatch_width == 8) {
      /* Pre-gen5, we can either use a SIMD8 message that requires (header,
       * u, v, r) as parameters, or we can just use the SIMD16 message
       * consisting of (header, u).  We choose the second, at the cost of a
       * longer return length.
       */
      scale = 2;
   }

   enum opcode op;
   if (brw->gen >= 7)
      op = FS_OPCODE_VARYING_PULL_CONSTANT_LOAD_GEN7;
   else
      op = FS_OPCODE_VARYING_PULL_CONSTANT_LOAD;
   fs_reg vec4_result = fs_reg(GRF, virtual_grf_alloc(4 * scale), dst.type);
   inst = new(mem_ctx) fs_inst(op, vec4_result, surf_index, vec4_offset);
   inst->regs_written = 4 * scale;
   instructions.push_tail(inst);

   if (brw->gen < 7) {
      inst->base_mrf = 13;
      inst->header_present = true;
      if (brw->gen == 4)
         inst->mlen = 3;
      else
         inst->mlen = 1 + dispatch_width / 8;
   }

   vec4_result.reg_offset += (const_offset & 3) * scale;
   instructions.push_tail(MOV(dst, vec4_result));

   return instructions;
}

/**
 * A helper for MOV generation for fixing up broken hardware SEND dependency
 * handling.
 */
fs_inst *
fs_visitor::DEP_RESOLVE_MOV(int grf)
{
   fs_inst *inst = MOV(brw_null_reg(), fs_reg(GRF, grf, BRW_REGISTER_TYPE_F));

   inst->ir = NULL;
   inst->annotation = "send dependency resolve";

   /* The caller always wants uncompressed to emit the minimal extra
    * dependencies, and to avoid having to deal with aligning its regs to 2.
    */
   inst->force_uncompressed = true;

   return inst;
}

bool
fs_inst::equals(fs_inst *inst) const
{
   return (opcode == inst->opcode &&
           dst.equals(inst->dst) &&
           src[0].equals(inst->src[0]) &&
           src[1].equals(inst->src[1]) &&
           src[2].equals(inst->src[2]) &&
           saturate == inst->saturate &&
           predicate == inst->predicate &&
           conditional_mod == inst->conditional_mod &&
           mlen == inst->mlen &&
           base_mrf == inst->base_mrf &&
           sampler == inst->sampler &&
           target == inst->target &&
           eot == inst->eot &&
           header_present == inst->header_present &&
           shadow_compare == inst->shadow_compare &&
           offset == inst->offset);
}

bool
fs_inst::overwrites_reg(const fs_reg &reg) const
{
   return (reg.file == dst.file &&
           reg.reg == dst.reg &&
           reg.reg_offset >= dst.reg_offset  &&
           reg.reg_offset < dst.reg_offset + regs_written);
}

bool
fs_inst::is_send_from_grf() const
{
   return (opcode == FS_OPCODE_VARYING_PULL_CONSTANT_LOAD_GEN7 ||
           opcode == SHADER_OPCODE_SHADER_TIME_ADD ||
           (opcode == FS_OPCODE_UNIFORM_PULL_CONSTANT_LOAD &&
            src[1].file == GRF) ||
           (is_tex() && src[0].file == GRF));
}

bool
fs_visitor::can_do_source_mods(fs_inst *inst)
{
   if (brw->gen == 6 && inst->is_math())
      return false;

   if (inst->is_send_from_grf())
      return false;

   if (!inst->can_do_source_mods())
      return false;

   return true;
}

void
fs_reg::init()
{
   memset(this, 0, sizeof(*this));
   stride = 1;
}

/** Generic unset register constructor. */
fs_reg::fs_reg()
{
   init();
   this->file = BAD_FILE;
}

/** Immediate value constructor. */
fs_reg::fs_reg(float f)
{
   init();
   this->file = IMM;
   this->type = BRW_REGISTER_TYPE_F;
   this->imm.f = f;
}

/** Immediate value constructor. */
fs_reg::fs_reg(int32_t i)
{
   init();
   this->file = IMM;
   this->type = BRW_REGISTER_TYPE_D;
   this->imm.i = i;
}

/** Immediate value constructor. */
fs_reg::fs_reg(uint32_t u)
{
   init();
   this->file = IMM;
   this->type = BRW_REGISTER_TYPE_UD;
   this->imm.u = u;
}

/** Fixed brw_reg. */
fs_reg::fs_reg(struct brw_reg fixed_hw_reg)
{
   init();
   this->file = HW_REG;
   this->fixed_hw_reg = fixed_hw_reg;
   this->type = fixed_hw_reg.type;
}

bool
fs_reg::equals(const fs_reg &r) const
{
   return (file == r.file &&
           reg == r.reg &&
           reg_offset == r.reg_offset &&
           subreg_offset == r.subreg_offset &&
           type == r.type &&
           negate == r.negate &&
           abs == r.abs &&
           !reladdr && !r.reladdr &&
           memcmp(&fixed_hw_reg, &r.fixed_hw_reg,
                  sizeof(fixed_hw_reg)) == 0 &&
           stride == r.stride &&
           imm.u == r.imm.u);
}

fs_reg &
fs_reg::apply_stride(unsigned stride)
{
   assert((this->stride * stride) <= 4 &&
          (is_power_of_two(stride) || stride == 0) &&
          file != HW_REG && file != IMM);
   this->stride *= stride;
   return *this;
}

fs_reg &
fs_reg::set_smear(unsigned subreg)
{
   assert(file != HW_REG && file != IMM);
   subreg_offset = subreg * type_sz(type);
   stride = 0;
   return *this;
}

bool
fs_reg::is_contiguous() const
{
   return stride == 1;
}

bool
fs_reg::is_zero() const
{
   if (file != IMM)
      return false;

   return type == BRW_REGISTER_TYPE_F ? imm.f == 0.0 : imm.i == 0;
}

bool
fs_reg::is_one() const
{
   if (file != IMM)
      return false;

   return type == BRW_REGISTER_TYPE_F ? imm.f == 1.0 : imm.i == 1;
}

bool
fs_reg::is_null() const
{
   return file == HW_REG &&
          fixed_hw_reg.file == BRW_ARCHITECTURE_REGISTER_FILE &&
          fixed_hw_reg.nr == BRW_ARF_NULL;
}

bool
fs_reg::is_valid_3src() const
{
   return file == GRF || file == UNIFORM;
}

bool
fs_reg::is_accumulator() const
{
   return file == HW_REG &&
          fixed_hw_reg.file == BRW_ARCHITECTURE_REGISTER_FILE &&
          fixed_hw_reg.nr == BRW_ARF_ACCUMULATOR;
}

int
fs_visitor::type_size(const struct glsl_type *type)
{
   unsigned int size, i;

   switch (type->base_type) {
   case GLSL_TYPE_UINT:
   case GLSL_TYPE_INT:
   case GLSL_TYPE_FLOAT:
   case GLSL_TYPE_BOOL:
      return type->components();
   case GLSL_TYPE_ARRAY:
      return type_size(type->fields.array) * type->length;
   case GLSL_TYPE_STRUCT:
      size = 0;
      for (i = 0; i < type->length; i++) {
	 size += type_size(type->fields.structure[i].type);
      }
      return size;
   case GLSL_TYPE_SAMPLER:
      /* Samplers take up no register space, since they're baked in at
       * link time.
       */
      return 0;
   case GLSL_TYPE_ATOMIC_UINT:
      return 0;
   case GLSL_TYPE_IMAGE:
   case GLSL_TYPE_VOID:
   case GLSL_TYPE_ERROR:
   case GLSL_TYPE_INTERFACE:
      assert(!"not reached");
      break;
   }

   return 0;
}

fs_reg
fs_visitor::get_timestamp()
{
   assert(brw->gen >= 7);

   fs_reg ts = fs_reg(retype(brw_vec1_reg(BRW_ARCHITECTURE_REGISTER_FILE,
                                          BRW_ARF_TIMESTAMP,
                                          0),
                             BRW_REGISTER_TYPE_UD));

   fs_reg dst = fs_reg(this, glsl_type::uint_type);

   fs_inst *mov = emit(MOV(dst, ts));
   /* We want to read the 3 fields we care about (mostly field 0, but also 2)
    * even if it's not enabled in the dispatch.
    */
   mov->force_writemask_all = true;
   mov->force_uncompressed = true;

   /* The caller wants the low 32 bits of the timestamp.  Since it's running
    * at the GPU clock rate of ~1.2ghz, it will roll over every ~3 seconds,
    * which is plenty of time for our purposes.  It is identical across the
    * EUs, but since it's tracking GPU core speed it will increment at a
    * varying rate as render P-states change.
    *
    * The caller could also check if render P-states have changed (or anything
    * else that might disrupt timing) by setting smear to 2 and checking if
    * that field is != 0.
    */
   dst.set_smear(0);

   return dst;
}

void
fs_visitor::emit_shader_time_begin()
{
   current_annotation = "shader time start";
   shader_start_time = get_timestamp();
}

void
fs_visitor::emit_shader_time_end()
{
   current_annotation = "shader time end";

   enum shader_time_shader_type type, written_type, reset_type;
   if (dispatch_width == 8) {
      type = ST_FS8;
      written_type = ST_FS8_WRITTEN;
      reset_type = ST_FS8_RESET;
   } else {
      assert(dispatch_width == 16);
      type = ST_FS16;
      written_type = ST_FS16_WRITTEN;
      reset_type = ST_FS16_RESET;
   }

   fs_reg shader_end_time = get_timestamp();

   /* Check that there weren't any timestamp reset events (assuming these
    * were the only two timestamp reads that happened).
    */
   fs_reg reset = shader_end_time;
   reset.set_smear(2);
   fs_inst *test = emit(AND(reg_null_d, reset, fs_reg(1u)));
   test->conditional_mod = BRW_CONDITIONAL_Z;
   emit(IF(BRW_PREDICATE_NORMAL));

   push_force_uncompressed();
   fs_reg start = shader_start_time;
   start.negate = true;
   fs_reg diff = fs_reg(this, glsl_type::uint_type);
   emit(ADD(diff, start, shader_end_time));

   /* If there were no instructions between the two timestamp gets, the diff
    * is 2 cycles.  Remove that overhead, so I can forget about that when
    * trying to determine the time taken for single instructions.
    */
   emit(ADD(diff, diff, fs_reg(-2u)));

   emit_shader_time_write(type, diff);
   emit_shader_time_write(written_type, fs_reg(1u));
   emit(BRW_OPCODE_ELSE);
   emit_shader_time_write(reset_type, fs_reg(1u));
   emit(BRW_OPCODE_ENDIF);

   pop_force_uncompressed();
}

void
fs_visitor::emit_shader_time_write(enum shader_time_shader_type type,
                                   fs_reg value)
{
   int shader_time_index =
      brw_get_shader_time_index(brw, shader_prog, &fp->Base, type);
   fs_reg offset = fs_reg(shader_time_index * SHADER_TIME_STRIDE);

   fs_reg payload;
   if (dispatch_width == 8)
      payload = fs_reg(this, glsl_type::uvec2_type);
   else
      payload = fs_reg(this, glsl_type::uint_type);

   emit(new(mem_ctx) fs_inst(SHADER_OPCODE_SHADER_TIME_ADD,
                             fs_reg(), payload, offset, value));
}

void
fs_visitor::vfail(const char *format, va_list va)
{
   char *msg;

   if (failed)
      return;

   failed = true;

   msg = ralloc_vasprintf(mem_ctx, format, va);
   msg = ralloc_asprintf(mem_ctx, "FS compile failed: %s\n", msg);

   this->fail_msg = msg;

   if (INTEL_DEBUG & DEBUG_WM) {
      fprintf(stderr, "%s",  msg);
   }
}

void
fs_visitor::fail(const char *format, ...)
{
   va_list va;

   va_start(va, format);
   vfail(format, va);
   va_end(va);
}

/**
 * Mark this program as impossible to compile in SIMD16 mode.
 *
 * During the SIMD8 compile (which happens first), we can detect and flag
 * things that are unsupported in SIMD16 mode, so the compiler can skip
 * the SIMD16 compile altogether.
 *
 * During a SIMD16 compile (if one happens anyway), this just calls fail().
 */
void
fs_visitor::no16(const char *format, ...)
{
   va_list va;

   va_start(va, format);

   if (dispatch_width == 16) {
      vfail(format, va);
   } else {
      simd16_unsupported = true;

      if (brw->perf_debug) {
         if (no16_msg)
            ralloc_vasprintf_append(&no16_msg, format, va);
         else
            no16_msg = ralloc_vasprintf(mem_ctx, format, va);
      }
   }

   va_end(va);
}

fs_inst *
fs_visitor::emit(enum opcode opcode)
{
   return emit(new(mem_ctx) fs_inst(opcode));
}

fs_inst *
fs_visitor::emit(enum opcode opcode, fs_reg dst)
{
   return emit(new(mem_ctx) fs_inst(opcode, dst));
}

fs_inst *
fs_visitor::emit(enum opcode opcode, fs_reg dst, fs_reg src0)
{
   return emit(new(mem_ctx) fs_inst(opcode, dst, src0));
}

fs_inst *
fs_visitor::emit(enum opcode opcode, fs_reg dst, fs_reg src0, fs_reg src1)
{
   return emit(new(mem_ctx) fs_inst(opcode, dst, src0, src1));
}

fs_inst *
fs_visitor::emit(enum opcode opcode, fs_reg dst,
                 fs_reg src0, fs_reg src1, fs_reg src2)
{
   return emit(new(mem_ctx) fs_inst(opcode, dst, src0, src1, src2));
}

void
fs_visitor::push_force_uncompressed()
{
   force_uncompressed_stack++;
}

void
fs_visitor::pop_force_uncompressed()
{
   force_uncompressed_stack--;
   assert(force_uncompressed_stack >= 0);
}

/**
 * Returns true if the instruction has a flag that means it won't
 * update an entire destination register.
 *
 * For example, dead code elimination and live variable analysis want to know
 * when a write to a variable screens off any preceding values that were in
 * it.
 */
bool
fs_inst::is_partial_write() const
{
   return ((this->predicate && this->opcode != BRW_OPCODE_SEL) ||
           this->force_uncompressed ||
           this->force_sechalf || !this->dst.is_contiguous());
}

int
fs_inst::regs_read(fs_visitor *v, int arg) const
{
   if (is_tex() && arg == 0 && src[0].file == GRF) {
      if (v->dispatch_width == 16)
	 return (mlen + 1) / 2;
      else
	 return mlen;
   }
   return 1;
}

bool
fs_inst::reads_flag() const
{
   return predicate;
}

bool
fs_inst::writes_flag() const
{
   return (conditional_mod && opcode != BRW_OPCODE_SEL) ||
          opcode == FS_OPCODE_MOV_DISPATCH_TO_FLAGS;
}

/**
 * Returns how many MRFs an FS opcode will write over.
 *
 * Note that this is not the 0 or 1 implied writes in an actual gen
 * instruction -- the FS opcodes often generate MOVs in addition.
 */
int
fs_visitor::implied_mrf_writes(fs_inst *inst)
{
   if (inst->mlen == 0)
      return 0;

   if (inst->base_mrf == -1)
      return 0;

   switch (inst->opcode) {
   case SHADER_OPCODE_RCP:
   case SHADER_OPCODE_RSQ:
   case SHADER_OPCODE_SQRT:
   case SHADER_OPCODE_EXP2:
   case SHADER_OPCODE_LOG2:
   case SHADER_OPCODE_SIN:
   case SHADER_OPCODE_COS:
      return 1 * dispatch_width / 8;
   case SHADER_OPCODE_POW:
   case SHADER_OPCODE_INT_QUOTIENT:
   case SHADER_OPCODE_INT_REMAINDER:
      return 2 * dispatch_width / 8;
   case SHADER_OPCODE_TEX:
   case FS_OPCODE_TXB:
   case SHADER_OPCODE_TXD:
   case SHADER_OPCODE_TXF:
   case SHADER_OPCODE_TXF_CMS:
   case SHADER_OPCODE_TXF_MCS:
   case SHADER_OPCODE_TG4:
   case SHADER_OPCODE_TG4_OFFSET:
   case SHADER_OPCODE_TXL:
   case SHADER_OPCODE_TXS:
   case SHADER_OPCODE_LOD:
      return 1;
   case FS_OPCODE_FB_WRITE:
      return 2;
   case FS_OPCODE_UNIFORM_PULL_CONSTANT_LOAD:
   case SHADER_OPCODE_GEN4_SCRATCH_READ:
      return 1;
   case FS_OPCODE_VARYING_PULL_CONSTANT_LOAD:
      return inst->mlen;
   case SHADER_OPCODE_GEN4_SCRATCH_WRITE:
      return 2;
   case SHADER_OPCODE_UNTYPED_ATOMIC:
   case SHADER_OPCODE_UNTYPED_SURFACE_READ:
      return 0;
   default:
      assert(!"not reached");
      return inst->mlen;
   }
}

int
fs_visitor::virtual_grf_alloc(int size)
{
   if (virtual_grf_array_size <= virtual_grf_count) {
      if (virtual_grf_array_size == 0)
	 virtual_grf_array_size = 16;
      else
	 virtual_grf_array_size *= 2;
      virtual_grf_sizes = reralloc(mem_ctx, virtual_grf_sizes, int,
				   virtual_grf_array_size);
   }
   virtual_grf_sizes[virtual_grf_count] = size;
   return virtual_grf_count++;
}

/** Fixed HW reg constructor. */
fs_reg::fs_reg(enum register_file file, int reg)
{
   init();
   this->file = file;
   this->reg = reg;
   this->type = BRW_REGISTER_TYPE_F;
}

/** Fixed HW reg constructor. */
fs_reg::fs_reg(enum register_file file, int reg, uint32_t type)
{
   init();
   this->file = file;
   this->reg = reg;
   this->type = type;
}

/** Automatic reg constructor. */
fs_reg::fs_reg(class fs_visitor *v, const struct glsl_type *type)
{
   init();

   this->file = GRF;
   this->reg = v->virtual_grf_alloc(v->type_size(type));
   this->reg_offset = 0;
   this->type = brw_type_for_base_type(type);
}

fs_reg *
fs_visitor::variable_storage(ir_variable *var)
{
   return (fs_reg *)hash_table_find(this->variable_ht, var);
}

void
import_uniforms_callback(const void *key,
			 void *data,
			 void *closure)
{
   struct hash_table *dst_ht = (struct hash_table *)closure;
   const fs_reg *reg = (const fs_reg *)data;

   if (reg->file != UNIFORM)
      return;

   hash_table_insert(dst_ht, data, key);
}

/* For SIMD16, we need to follow from the uniform setup of SIMD8 dispatch.
 * This brings in those uniform definitions
 */
void
fs_visitor::import_uniforms(fs_visitor *v)
{
   hash_table_call_foreach(v->variable_ht,
			   import_uniforms_callback,
			   variable_ht);
   this->push_constant_loc = v->push_constant_loc;
   this->pull_constant_loc = v->pull_constant_loc;
   this->uniforms = v->uniforms;
   this->param_size = v->param_size;
}

/* Our support for uniforms is piggy-backed on the struct
 * gl_fragment_program, because that's where the values actually
 * get stored, rather than in some global gl_shader_program uniform
 * store.
 */
void
fs_visitor::setup_uniform_values(ir_variable *ir)
{
   int namelen = strlen(ir->name);

   /* The data for our (non-builtin) uniforms is stored in a series of
    * gl_uniform_driver_storage structs for each subcomponent that
    * glGetUniformLocation() could name.  We know it's been set up in the same
    * order we'd walk the type, so walk the list of storage and find anything
    * with our name, or the prefix of a component that starts with our name.
    */
   unsigned params_before = uniforms;
   for (unsigned u = 0; u < shader_prog->NumUserUniformStorage; u++) {
      struct gl_uniform_storage *storage = &shader_prog->UniformStorage[u];

      if (strncmp(ir->name, storage->name, namelen) != 0 ||
          (storage->name[namelen] != 0 &&
           storage->name[namelen] != '.' &&
           storage->name[namelen] != '[')) {
         continue;
      }

      unsigned slots = storage->type->component_slots();
      if (storage->array_elements)
         slots *= storage->array_elements;

      for (unsigned i = 0; i < slots; i++) {
         stage_prog_data->param[uniforms++] = &storage->storage[i].f;
      }
   }

   /* Make sure we actually initialized the right amount of stuff here. */
   assert(params_before + ir->type->component_slots() == uniforms);
   (void)params_before;
}


/* Our support for builtin uniforms is even scarier than non-builtin.
 * It sits on top of the PROG_STATE_VAR parameters that are
 * automatically updated from GL context state.
 */
void
fs_visitor::setup_builtin_uniform_values(ir_variable *ir)
{
   const ir_state_slot *const slots = ir->state_slots;
   assert(ir->state_slots != NULL);

   for (unsigned int i = 0; i < ir->num_state_slots; i++) {
      /* This state reference has already been setup by ir_to_mesa, but we'll
       * get the same index back here.
       */
      int index = _mesa_add_state_reference(this->fp->Base.Parameters,
					    (gl_state_index *)slots[i].tokens);

      /* Add each of the unique swizzles of the element as a parameter.
       * This'll end up matching the expected layout of the
       * array/matrix/structure we're trying to fill in.
       */
      int last_swiz = -1;
      for (unsigned int j = 0; j < 4; j++) {
	 int swiz = GET_SWZ(slots[i].swizzle, j);
	 if (swiz == last_swiz)
	    break;
	 last_swiz = swiz;

         stage_prog_data->param[uniforms++] =
            &fp->Base.Parameters->ParameterValues[index][swiz].f;
      }
   }
}

fs_reg *
fs_visitor::emit_fragcoord_interpolation(ir_variable *ir)
{
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, ir->type);
   fs_reg wpos = *reg;
   bool flip = !ir->data.origin_upper_left ^ c->key.render_to_fbo;

   /* gl_FragCoord.x */
   if (ir->data.pixel_center_integer) {
      emit(MOV(wpos, this->pixel_x));
   } else {
      emit(ADD(wpos, this->pixel_x, fs_reg(0.5f)));
   }
   wpos.reg_offset++;

   /* gl_FragCoord.y */
   if (!flip && ir->data.pixel_center_integer) {
      emit(MOV(wpos, this->pixel_y));
   } else {
      fs_reg pixel_y = this->pixel_y;
      float offset = (ir->data.pixel_center_integer ? 0.0 : 0.5);

      if (flip) {
	 pixel_y.negate = true;
	 offset += c->key.drawable_height - 1.0;
      }

      emit(ADD(wpos, pixel_y, fs_reg(offset)));
   }
   wpos.reg_offset++;

   /* gl_FragCoord.z */
   if (brw->gen >= 6) {
      emit(MOV(wpos, fs_reg(brw_vec8_grf(c->source_depth_reg, 0))));
   } else {
      emit(FS_OPCODE_LINTERP, wpos,
           this->delta_x[BRW_WM_PERSPECTIVE_PIXEL_BARYCENTRIC],
           this->delta_y[BRW_WM_PERSPECTIVE_PIXEL_BARYCENTRIC],
           interp_reg(VARYING_SLOT_POS, 2));
   }
   wpos.reg_offset++;

   /* gl_FragCoord.w: Already set up in emit_interpolation */
   emit(BRW_OPCODE_MOV, wpos, this->wpos_w);

   return reg;
}

fs_inst *
fs_visitor::emit_linterp(const fs_reg &attr, const fs_reg &interp,
                         glsl_interp_qualifier interpolation_mode,
                         bool is_centroid, bool is_sample)
{
   brw_wm_barycentric_interp_mode barycoord_mode;
   if (brw->gen >= 6) {
      if (is_centroid) {
         if (interpolation_mode == INTERP_QUALIFIER_SMOOTH)
            barycoord_mode = BRW_WM_PERSPECTIVE_CENTROID_BARYCENTRIC;
         else
            barycoord_mode = BRW_WM_NONPERSPECTIVE_CENTROID_BARYCENTRIC;
      } else if (is_sample) {
          if (interpolation_mode == INTERP_QUALIFIER_SMOOTH)
            barycoord_mode = BRW_WM_PERSPECTIVE_SAMPLE_BARYCENTRIC;
         else
            barycoord_mode = BRW_WM_NONPERSPECTIVE_SAMPLE_BARYCENTRIC;
      } else {
         if (interpolation_mode == INTERP_QUALIFIER_SMOOTH)
            barycoord_mode = BRW_WM_PERSPECTIVE_PIXEL_BARYCENTRIC;
         else
            barycoord_mode = BRW_WM_NONPERSPECTIVE_PIXEL_BARYCENTRIC;
      }
   } else {
      /* On Ironlake and below, there is only one interpolation mode.
       * Centroid interpolation doesn't mean anything on this hardware --
       * there is no multisampling.
       */
      barycoord_mode = BRW_WM_PERSPECTIVE_PIXEL_BARYCENTRIC;
   }
   return emit(FS_OPCODE_LINTERP, attr,
               this->delta_x[barycoord_mode],
               this->delta_y[barycoord_mode], interp);
}

fs_reg *
fs_visitor::emit_general_interpolation(ir_variable *ir)
{
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, ir->type);
   reg->type = brw_type_for_base_type(ir->type->get_scalar_type());
   fs_reg attr = *reg;

   unsigned int array_elements;
   const glsl_type *type;

   if (ir->type->is_array()) {
      array_elements = ir->type->length;
      if (array_elements == 0) {
	 fail("dereferenced array '%s' has length 0\n", ir->name);
      }
      type = ir->type->fields.array;
   } else {
      array_elements = 1;
      type = ir->type;
   }

   glsl_interp_qualifier interpolation_mode =
      ir->determine_interpolation_mode(c->key.flat_shade);

   int location = ir->data.location;
   for (unsigned int i = 0; i < array_elements; i++) {
      for (unsigned int j = 0; j < type->matrix_columns; j++) {
	 if (c->prog_data.urb_setup[location] == -1) {
	    /* If there's no incoming setup data for this slot, don't
	     * emit interpolation for it.
	     */
	    attr.reg_offset += type->vector_elements;
	    location++;
	    continue;
	 }

	 if (interpolation_mode == INTERP_QUALIFIER_FLAT) {
	    /* Constant interpolation (flat shading) case. The SF has
	     * handed us defined values in only the constant offset
	     * field of the setup reg.
	     */
	    for (unsigned int k = 0; k < type->vector_elements; k++) {
	       struct brw_reg interp = interp_reg(location, k);
	       interp = suboffset(interp, 3);
               interp.type = reg->type;
	       emit(FS_OPCODE_CINTERP, attr, fs_reg(interp));
	       attr.reg_offset++;
	    }
	 } else {
	    /* Smooth/noperspective interpolation case. */
	    for (unsigned int k = 0; k < type->vector_elements; k++) {
               struct brw_reg interp = interp_reg(location, k);
               emit_linterp(attr, fs_reg(interp), interpolation_mode,
                            ir->data.centroid && !c->key.persample_shading,
                            ir->data.sample || c->key.persample_shading);
               if (brw->needs_unlit_centroid_workaround && ir->data.centroid) {
                  /* Get the pixel/sample mask into f0 so that we know
                   * which pixels are lit.  Then, for each channel that is
                   * unlit, replace the centroid data with non-centroid
                   * data.
                   */
                  emit(FS_OPCODE_MOV_DISPATCH_TO_FLAGS);
                  fs_inst *inst = emit_linterp(attr, fs_reg(interp),
                                               interpolation_mode,
                                               false, false);
                  inst->predicate = BRW_PREDICATE_NORMAL;
                  inst->predicate_inverse = true;
               }
               if (brw->gen < 6 && interpolation_mode == INTERP_QUALIFIER_SMOOTH) {
                  emit(BRW_OPCODE_MUL, attr, attr, this->pixel_w);
               }
	       attr.reg_offset++;
	    }

	 }
	 location++;
      }
   }

   return reg;
}

fs_reg *
fs_visitor::emit_frontfacing_interpolation(ir_variable *ir)
{
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, ir->type);

   /* The frontfacing comes in as a bit in the thread payload. */
   if (brw->gen >= 6) {
      emit(BRW_OPCODE_ASR, *reg,
	   fs_reg(retype(brw_vec1_grf(0, 0), BRW_REGISTER_TYPE_D)),
	   fs_reg(15));
      emit(BRW_OPCODE_NOT, *reg, *reg);
      emit(BRW_OPCODE_AND, *reg, *reg, fs_reg(1));
   } else {
      struct brw_reg r1_6ud = retype(brw_vec1_grf(1, 6), BRW_REGISTER_TYPE_UD);
      /* bit 31 is "primitive is back face", so checking < (1 << 31) gives
       * us front face
       */
      emit(CMP(*reg, fs_reg(r1_6ud), fs_reg(1u << 31), BRW_CONDITIONAL_L));
      emit(BRW_OPCODE_AND, *reg, *reg, fs_reg(1u));
   }

   return reg;
}

void
fs_visitor::compute_sample_position(fs_reg dst, fs_reg int_sample_pos)
{
   assert(dst.type == BRW_REGISTER_TYPE_F);

   if (c->key.compute_pos_offset) {
      /* Convert int_sample_pos to floating point */
      emit(MOV(dst, int_sample_pos));
      /* Scale to the range [0, 1] */
      emit(MUL(dst, dst, fs_reg(1 / 16.0f)));
   }
   else {
      /* From ARB_sample_shading specification:
       * "When rendering to a non-multisample buffer, or if multisample
       *  rasterization is disabled, gl_SamplePosition will always be
       *  (0.5, 0.5).
       */
      emit(MOV(dst, fs_reg(0.5f)));
   }
}

fs_reg *
fs_visitor::emit_samplepos_setup(ir_variable *ir)
{
   assert(brw->gen >= 6);
   assert(ir->type == glsl_type::vec2_type);

   this->current_annotation = "compute sample position";
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, ir->type);
   fs_reg pos = *reg;
   fs_reg int_sample_x = fs_reg(this, glsl_type::int_type);
   fs_reg int_sample_y = fs_reg(this, glsl_type::int_type);

   /* WM will be run in MSDISPMODE_PERSAMPLE. So, only one of SIMD8 or SIMD16
    * mode will be enabled.
    *
    * From the Ivy Bridge PRM, volume 2 part 1, page 344:
    * R31.1:0         Position Offset X/Y for Slot[3:0]
    * R31.3:2         Position Offset X/Y for Slot[7:4]
    * .....
    *
    * The X, Y sample positions come in as bytes in  thread payload. So, read
    * the positions using vstride=16, width=8, hstride=2.
    */
   struct brw_reg sample_pos_reg =
      stride(retype(brw_vec1_grf(c->sample_pos_reg, 0),
                    BRW_REGISTER_TYPE_B), 16, 8, 2);

   fs_inst *inst = emit(MOV(int_sample_x, fs_reg(sample_pos_reg)));
   if (dispatch_width == 16) {
      inst->force_uncompressed = true;
      inst = emit(MOV(half(int_sample_x, 1),
                      fs_reg(suboffset(sample_pos_reg, 16))));
      inst->force_sechalf = true;
   }
   /* Compute gl_SamplePosition.x */
   compute_sample_position(pos, int_sample_x);
   pos.reg_offset++;
   inst = emit(MOV(int_sample_y, fs_reg(suboffset(sample_pos_reg, 1))));
   if (dispatch_width == 16) {
      inst->force_uncompressed = true;
      inst = emit(MOV(half(int_sample_y, 1),
                      fs_reg(suboffset(sample_pos_reg, 17))));
      inst->force_sechalf = true;
   }
   /* Compute gl_SamplePosition.y */
   compute_sample_position(pos, int_sample_y);
   return reg;
}

fs_reg *
fs_visitor::emit_sampleid_setup(ir_variable *ir)
{
   assert(brw->gen >= 6);

   this->current_annotation = "compute sample id";
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, ir->type);

   if (c->key.compute_sample_id) {
      fs_reg t1 = fs_reg(this, glsl_type::int_type);
      fs_reg t2 = fs_reg(this, glsl_type::int_type);
      t2.type = BRW_REGISTER_TYPE_UW;

      /* The PS will be run in MSDISPMODE_PERSAMPLE. For example with
       * 8x multisampling, subspan 0 will represent sample N (where N
       * is 0, 2, 4 or 6), subspan 1 will represent sample 1, 3, 5 or
       * 7. We can find the value of N by looking at R0.0 bits 7:6
       * ("Starting Sample Pair Index (SSPI)") and multiplying by two
       * (since samples are always delivered in pairs). That is, we
       * compute 2*((R0.0 & 0xc0) >> 6) == (R0.0 & 0xc0) >> 5. Then
       * we need to add N to the sequence (0, 0, 0, 0, 1, 1, 1, 1) in
       * case of SIMD8 and sequence (0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2,
       * 2, 3, 3, 3, 3) in case of SIMD16. We compute this sequence by
       * populating a temporary variable with the sequence (0, 1, 2, 3),
       * and then reading from it using vstride=1, width=4, hstride=0.
       * These computations hold good for 4x multisampling as well.
       *
       * For 2x MSAA and SIMD16, we want to use the sequence (0, 1, 0, 1):
       * the first four slots are sample 0 of subspan 0; the next four
       * are sample 1 of subspan 0; the third group is sample 0 of
       * subspan 1, and finally sample 1 of subspan 1.
       */
      fs_inst *inst;
      inst = emit(BRW_OPCODE_AND, t1,
                  fs_reg(retype(brw_vec1_grf(0, 0), BRW_REGISTER_TYPE_UD)),
                  fs_reg(0xc0));
      inst->force_writemask_all = true;
      inst = emit(BRW_OPCODE_SHR, t1, t1, fs_reg(5));
      inst->force_writemask_all = true;
      /* This works for both SIMD8 and SIMD16 */
      inst = emit(MOV(t2, brw_imm_v(c->key.persample_2x ? 0x1010 : 0x3210)));
      inst->force_writemask_all = true;
      /* This special instruction takes care of setting vstride=1,
       * width=4, hstride=0 of t2 during an ADD instruction.
       */
      emit(FS_OPCODE_SET_SAMPLE_ID, *reg, t1, t2);
   } else {
      /* As per GL_ARB_sample_shading specification:
       * "When rendering to a non-multisample buffer, or if multisample
       *  rasterization is disabled, gl_SampleID will always be zero."
       */
      emit(BRW_OPCODE_MOV, *reg, fs_reg(0));
   }

   return reg;
}

fs_reg *
fs_visitor::emit_samplemaskin_setup(ir_variable *ir)
{
   assert(brw->gen >= 7);
   this->current_annotation = "compute gl_SampleMaskIn";
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, ir->type);
   emit(MOV(*reg, fs_reg(retype(brw_vec8_grf(c->sample_mask_reg, 0), BRW_REGISTER_TYPE_D))));
   return reg;
}

fs_reg
fs_visitor::fix_math_operand(fs_reg src)
{
   /* Can't do hstride == 0 args on gen6 math, so expand it out. We
    * might be able to do better by doing execsize = 1 math and then
    * expanding that result out, but we would need to be careful with
    * masking.
    *
    * The hardware ignores source modifiers (negate and abs) on math
    * instructions, so we also move to a temp to set those up.
    */
   if (brw->gen == 6 && src.file != UNIFORM && src.file != IMM &&
       !src.abs && !src.negate)
      return src;

   /* Gen7 relaxes most of the above restrictions, but still can't use IMM
    * operands to math
    */
   if (brw->gen >= 7 && src.file != IMM)
      return src;

   fs_reg expanded = fs_reg(this, glsl_type::float_type);
   expanded.type = src.type;
   emit(BRW_OPCODE_MOV, expanded, src);
   return expanded;
}

fs_inst *
fs_visitor::emit_math(enum opcode opcode, fs_reg dst, fs_reg src)
{
   switch (opcode) {
   case SHADER_OPCODE_RCP:
   case SHADER_OPCODE_RSQ:
   case SHADER_OPCODE_SQRT:
   case SHADER_OPCODE_EXP2:
   case SHADER_OPCODE_LOG2:
   case SHADER_OPCODE_SIN:
   case SHADER_OPCODE_COS:
      break;
   default:
      assert(!"not reached: bad math opcode");
      return NULL;
   }

   /* Can't do hstride == 0 args to gen6 math, so expand it out.  We
    * might be able to do better by doing execsize = 1 math and then
    * expanding that result out, but we would need to be careful with
    * masking.
    *
    * Gen 6 hardware ignores source modifiers (negate and abs) on math
    * instructions, so we also move to a temp to set those up.
    */
   if (brw->gen == 6 || brw->gen == 7)
      src = fix_math_operand(src);

   fs_inst *inst = emit(opcode, dst, src);

   if (brw->gen < 6) {
      inst->base_mrf = 2;
      inst->mlen = dispatch_width / 8;
   }

   return inst;
}

fs_inst *
fs_visitor::emit_math(enum opcode opcode, fs_reg dst, fs_reg src0, fs_reg src1)
{
   int base_mrf = 2;
   fs_inst *inst;

   switch (opcode) {
   case SHADER_OPCODE_INT_QUOTIENT:
   case SHADER_OPCODE_INT_REMAINDER:
      if (brw->gen >= 7)
	 no16("SIMD16 INTDIV unsupported\n");
      break;
   case SHADER_OPCODE_POW:
      break;
   default:
      assert(!"not reached: unsupported binary math opcode.");
      return NULL;
   }

   if (brw->gen >= 8) {
      inst = emit(opcode, dst, src0, src1);
   } else if (brw->gen >= 6) {
      src0 = fix_math_operand(src0);
      src1 = fix_math_operand(src1);

      inst = emit(opcode, dst, src0, src1);
   } else {
      /* From the Ironlake PRM, Volume 4, Part 1, Section 6.1.13
       * "Message Payload":
       *
       * "Operand0[7].  For the INT DIV functions, this operand is the
       *  denominator."
       *  ...
       * "Operand1[7].  For the INT DIV functions, this operand is the
       *  numerator."
       */
      bool is_int_div = opcode != SHADER_OPCODE_POW;
      fs_reg &op0 = is_int_div ? src1 : src0;
      fs_reg &op1 = is_int_div ? src0 : src1;

      emit(BRW_OPCODE_MOV, fs_reg(MRF, base_mrf + 1, op1.type), op1);
      inst = emit(opcode, dst, op0, reg_null_f);

      inst->base_mrf = base_mrf;
      inst->mlen = 2 * dispatch_width / 8;
   }
   return inst;
}

void
fs_visitor::assign_curb_setup()
{
   if (dispatch_width == 8) {
      c->prog_data.first_curbe_grf = c->nr_payload_regs;
   } else {
      c->prog_data.first_curbe_grf_16 = c->nr_payload_regs;
   }

   c->prog_data.curb_read_length = ALIGN(stage_prog_data->nr_params, 8) / 8;

   /* Map the offsets in the UNIFORM file to fixed HW regs. */
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      for (unsigned int i = 0; i < 3; i++) {
	 if (inst->src[i].file == UNIFORM) {
            int uniform_nr = inst->src[i].reg + inst->src[i].reg_offset;
            int constant_nr;
            if (uniform_nr >= 0 && uniform_nr < (int) uniforms) {
               constant_nr = push_constant_loc[uniform_nr];
            } else {
               /* Section 5.11 of the OpenGL 4.1 spec says:
                * "Out-of-bounds reads return undefined values, which include
                *  values from other variables of the active program or zero."
                * Just return the first push constant.
                */
               constant_nr = 0;
            }

	    struct brw_reg brw_reg = brw_vec1_grf(c->nr_payload_regs +
						  constant_nr / 8,
						  constant_nr % 8);

	    inst->src[i].file = HW_REG;
	    inst->src[i].fixed_hw_reg = byte_offset(
               retype(brw_reg, inst->src[i].type),
               inst->src[i].subreg_offset);
	 }
      }
   }
}

void
fs_visitor::calculate_urb_setup()
{
   for (unsigned int i = 0; i < VARYING_SLOT_MAX; i++) {
      c->prog_data.urb_setup[i] = -1;
   }

   int urb_next = 0;
   /* Figure out where each of the incoming setup attributes lands. */
   if (brw->gen >= 6) {
      if (_mesa_bitcount_64(fp->Base.InputsRead &
                            BRW_FS_VARYING_INPUT_MASK) <= 16) {
         /* The SF/SBE pipeline stage can do arbitrary rearrangement of the
          * first 16 varying inputs, so we can put them wherever we want.
          * Just put them in order.
          *
          * This is useful because it means that (a) inputs not used by the
          * fragment shader won't take up valuable register space, and (b) we
          * won't have to recompile the fragment shader if it gets paired with
          * a different vertex (or geometry) shader.
          */
         for (unsigned int i = 0; i < VARYING_SLOT_MAX; i++) {
            if (fp->Base.InputsRead & BRW_FS_VARYING_INPUT_MASK &
                BITFIELD64_BIT(i)) {
               c->prog_data.urb_setup[i] = urb_next++;
            }
         }
      } else {
         /* We have enough input varyings that the SF/SBE pipeline stage can't
          * arbitrarily rearrange them to suit our whim; we have to put them
          * in an order that matches the output of the previous pipeline stage
          * (geometry or vertex shader).
          */
         struct brw_vue_map prev_stage_vue_map;
         brw_compute_vue_map(brw, &prev_stage_vue_map,
                             c->key.input_slots_valid);
         int first_slot = 2 * BRW_SF_URB_ENTRY_READ_OFFSET;
         assert(prev_stage_vue_map.num_slots <= first_slot + 32);
         for (int slot = first_slot; slot < prev_stage_vue_map.num_slots;
              slot++) {
            int varying = prev_stage_vue_map.slot_to_varying[slot];
            /* Note that varying == BRW_VARYING_SLOT_COUNT when a slot is
             * unused.
             */
            if (varying != BRW_VARYING_SLOT_COUNT &&
                (fp->Base.InputsRead & BRW_FS_VARYING_INPUT_MASK &
                 BITFIELD64_BIT(varying))) {
               c->prog_data.urb_setup[varying] = slot - first_slot;
            }
         }
         urb_next = prev_stage_vue_map.num_slots - first_slot;
      }
   } else {
      /* FINISHME: The sf doesn't map VS->FS inputs for us very well. */
      for (unsigned int i = 0; i < VARYING_SLOT_MAX; i++) {
         /* Point size is packed into the header, not as a general attribute */
         if (i == VARYING_SLOT_PSIZ)
            continue;

	 if (c->key.input_slots_valid & BITFIELD64_BIT(i)) {
	    /* The back color slot is skipped when the front color is
	     * also written to.  In addition, some slots can be
	     * written in the vertex shader and not read in the
	     * fragment shader.  So the register number must always be
	     * incremented, mapped or not.
	     */
	    if (_mesa_varying_slot_in_fs((gl_varying_slot) i))
	       c->prog_data.urb_setup[i] = urb_next;
            urb_next++;
	 }
      }

      /*
       * It's a FS only attribute, and we did interpolation for this attribute
       * in SF thread. So, count it here, too.
       *
       * See compile_sf_prog() for more info.
       */
      if (fp->Base.InputsRead & BITFIELD64_BIT(VARYING_SLOT_PNTC))
         c->prog_data.urb_setup[VARYING_SLOT_PNTC] = urb_next++;
   }

   c->prog_data.num_varying_inputs = urb_next;
}

void
fs_visitor::assign_urb_setup()
{
   int urb_start = c->nr_payload_regs + c->prog_data.curb_read_length;

   /* Offset all the urb_setup[] index by the actual position of the
    * setup regs, now that the location of the constants has been chosen.
    */
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      if (inst->opcode == FS_OPCODE_LINTERP) {
	 assert(inst->src[2].file == HW_REG);
	 inst->src[2].fixed_hw_reg.nr += urb_start;
      }

      if (inst->opcode == FS_OPCODE_CINTERP) {
	 assert(inst->src[0].file == HW_REG);
	 inst->src[0].fixed_hw_reg.nr += urb_start;
      }
   }

   /* Each attribute is 4 setup channels, each of which is half a reg. */
   this->first_non_payload_grf =
      urb_start + c->prog_data.num_varying_inputs * 2;
}

/**
 * Split large virtual GRFs into separate components if we can.
 *
 * This is mostly duplicated with what brw_fs_vector_splitting does,
 * but that's really conservative because it's afraid of doing
 * splitting that doesn't result in real progress after the rest of
 * the optimization phases, which would cause infinite looping in
 * optimization.  We can do it once here, safely.  This also has the
 * opportunity to split interpolated values, or maybe even uniforms,
 * which we don't have at the IR level.
 *
 * We want to split, because virtual GRFs are what we register
 * allocate and spill (due to contiguousness requirements for some
 * instructions), and they're what we naturally generate in the
 * codegen process, but most virtual GRFs don't actually need to be
 * contiguous sets of GRFs.  If we split, we'll end up with reduced
 * live intervals and better dead code elimination and coalescing.
 */
void
fs_visitor::split_virtual_grfs()
{
   int num_vars = this->virtual_grf_count;
   bool split_grf[num_vars];
   int new_virtual_grf[num_vars];

   /* Try to split anything > 0 sized. */
   for (int i = 0; i < num_vars; i++) {
      if (this->virtual_grf_sizes[i] != 1)
	 split_grf[i] = true;
      else
	 split_grf[i] = false;
   }

   if (brw->has_pln &&
       this->delta_x[BRW_WM_PERSPECTIVE_PIXEL_BARYCENTRIC].file == GRF) {
      /* PLN opcodes rely on the delta_xy being contiguous.  We only have to
       * check this for BRW_WM_PERSPECTIVE_PIXEL_BARYCENTRIC, because prior to
       * Gen6, that was the only supported interpolation mode, and since Gen6,
       * delta_x and delta_y are in fixed hardware registers.
       */
      split_grf[this->delta_x[BRW_WM_PERSPECTIVE_PIXEL_BARYCENTRIC].reg] =
         false;
   }

   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      /* If there's a SEND message that requires contiguous destination
       * registers, no splitting is allowed.
       */
      if (inst->regs_written > 1) {
	 split_grf[inst->dst.reg] = false;
      }

      /* If we're sending from a GRF, don't split it, on the assumption that
       * the send is reading the whole thing.
       */
      if (inst->is_send_from_grf()) {
         for (int i = 0; i < 3; i++) {
            if (inst->src[i].file == GRF) {
               split_grf[inst->src[i].reg] = false;
            }
         }
      }
   }

   /* Allocate new space for split regs.  Note that the virtual
    * numbers will be contiguous.
    */
   for (int i = 0; i < num_vars; i++) {
      if (split_grf[i]) {
	 new_virtual_grf[i] = virtual_grf_alloc(1);
	 for (int j = 2; j < this->virtual_grf_sizes[i]; j++) {
	    int reg = virtual_grf_alloc(1);
	    assert(reg == new_virtual_grf[i] + j - 1);
	    (void) reg;
	 }
	 this->virtual_grf_sizes[i] = 1;
      }
   }

   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      if (inst->dst.file == GRF &&
	  split_grf[inst->dst.reg] &&
	  inst->dst.reg_offset != 0) {
	 inst->dst.reg = (new_virtual_grf[inst->dst.reg] +
			  inst->dst.reg_offset - 1);
	 inst->dst.reg_offset = 0;
      }
      for (int i = 0; i < 3; i++) {
	 if (inst->src[i].file == GRF &&
	     split_grf[inst->src[i].reg] &&
	     inst->src[i].reg_offset != 0) {
	    inst->src[i].reg = (new_virtual_grf[inst->src[i].reg] +
				inst->src[i].reg_offset - 1);
	    inst->src[i].reg_offset = 0;
	 }
      }
   }
   invalidate_live_intervals();
}

/**
 * Remove unused virtual GRFs and compact the virtual_grf_* arrays.
 *
 * During code generation, we create tons of temporary variables, many of
 * which get immediately killed and are never used again.  Yet, in later
 * optimization and analysis passes, such as compute_live_intervals, we need
 * to loop over all the virtual GRFs.  Compacting them can save a lot of
 * overhead.
 */
void
fs_visitor::compact_virtual_grfs()
{
   /* Mark which virtual GRFs are used, and count how many. */
   int remap_table[this->virtual_grf_count];
   memset(remap_table, -1, sizeof(remap_table));

   foreach_list(node, &this->instructions) {
      const fs_inst *inst = (const fs_inst *) node;

      if (inst->dst.file == GRF)
         remap_table[inst->dst.reg] = 0;

      for (int i = 0; i < 3; i++) {
         if (inst->src[i].file == GRF)
            remap_table[inst->src[i].reg] = 0;
      }
   }

   /* In addition to registers used in instructions, fs_visitor keeps
    * direct references to certain special values which must be patched:
    */
   struct {
      fs_reg *reg;
      unsigned count;
   } special[] = {
      { &frag_depth, 1 },
      { &pixel_x, 1 },
      { &pixel_y, 1 },
      { &pixel_w, 1 },
      { &wpos_w, 1 },
      { &dual_src_output, 1 },
      { outputs, ARRAY_SIZE(outputs) },
      { delta_x, ARRAY_SIZE(delta_x) },
      { delta_y, ARRAY_SIZE(delta_y) },
      { &sample_mask, 1 },
      { &shader_start_time, 1 },
   };

   /* Treat all special values as used, to be conservative */
   for (unsigned i = 0; i < ARRAY_SIZE(special); i++) {
      for (unsigned j = 0; j < special[i].count; j++) {
         if (special[i].reg[j].file == GRF)
            remap_table[special[i].reg[j].reg] = 0;
      }
   }

   /* Compact the GRF arrays. */
   int new_index = 0;
   for (int i = 0; i < this->virtual_grf_count; i++) {
      if (remap_table[i] != -1) {
         remap_table[i] = new_index;
         virtual_grf_sizes[new_index] = virtual_grf_sizes[i];
         invalidate_live_intervals();
         ++new_index;
      }
   }

   this->virtual_grf_count = new_index;

   /* Patch all the instructions to use the newly renumbered registers */
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *) node;

      if (inst->dst.file == GRF)
         inst->dst.reg = remap_table[inst->dst.reg];

      for (int i = 0; i < 3; i++) {
         if (inst->src[i].file == GRF)
            inst->src[i].reg = remap_table[inst->src[i].reg];
      }
   }

   /* Patch all the references to special values */
   for (unsigned i = 0; i < ARRAY_SIZE(special); i++) {
      for (unsigned j = 0; j < special[i].count; j++) {
         fs_reg *reg = &special[i].reg[j];
         if (reg->file == GRF && remap_table[reg->reg] != -1)
            reg->reg = remap_table[reg->reg];
      }
   }
}

/*
 * Implements array access of uniforms by inserting a
 * PULL_CONSTANT_LOAD instruction.
 *
 * Unlike temporary GRF array access (where we don't support it due to
 * the difficulty of doing relative addressing on instruction
 * destinations), we could potentially do array access of uniforms
 * that were loaded in GRF space as push constants.  In real-world
 * usage we've seen, though, the arrays being used are always larger
 * than we could load as push constants, so just always move all
 * uniform array access out to a pull constant buffer.
 */
void
fs_visitor::move_uniform_array_access_to_pull_constants()
{
   if (dispatch_width != 8)
      return;

   pull_constant_loc = ralloc_array(mem_ctx, int, uniforms);

   for (unsigned int i = 0; i < uniforms; i++) {
      pull_constant_loc[i] = -1;
   }

   /* Walk through and find array access of uniforms.  Put a copy of that
    * uniform in the pull constant buffer.
    *
    * Note that we don't move constant-indexed accesses to arrays.  No
    * testing has been done of the performance impact of this choice.
    */
   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      for (int i = 0 ; i < 3; i++) {
         if (inst->src[i].file != UNIFORM || !inst->src[i].reladdr)
            continue;

         int uniform = inst->src[i].reg;

         /* If this array isn't already present in the pull constant buffer,
          * add it.
          */
         if (pull_constant_loc[uniform] == -1) {
            const float **values = &stage_prog_data->param[uniform];

            assert(param_size[uniform]);

            for (int j = 0; j < param_size[uniform]; j++) {
               pull_constant_loc[uniform + j] = stage_prog_data->nr_pull_params;

               stage_prog_data->pull_param[stage_prog_data->nr_pull_params++] =
                  values[j];
            }
         }
      }
   }
}

/**
 * Assign UNIFORM file registers to either push constants or pull constants.
 *
 * We allow a fragment shader to have more than the specified minimum
 * maximum number of fragment shader uniform components (64).  If
 * there are too many of these, they'd fill up all of register space.
 * So, this will push some of them out to the pull constant buffer and
 * update the program to load them.
 */
void
fs_visitor::assign_constant_locations()
{
   /* Only the first compile (SIMD8 mode) gets to decide on locations. */
   if (dispatch_width != 8)
      return;

   /* Find which UNIFORM registers are still in use. */
   bool is_live[uniforms];
   for (unsigned int i = 0; i < uniforms; i++) {
      is_live[i] = false;
   }

   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *) node;

      for (int i = 0; i < 3; i++) {
         if (inst->src[i].file != UNIFORM)
            continue;

         int constant_nr = inst->src[i].reg + inst->src[i].reg_offset;
         if (constant_nr >= 0 && constant_nr < (int) uniforms)
            is_live[constant_nr] = true;
      }
   }

   /* Only allow 16 registers (128 uniform components) as push constants.
    *
    * Just demote the end of the list.  We could probably do better
    * here, demoting things that are rarely used in the program first.
    */
   unsigned int max_push_components = 16 * 8;
   unsigned int num_push_constants = 0;

   push_constant_loc = ralloc_array(mem_ctx, int, uniforms);

   for (unsigned int i = 0; i < uniforms; i++) {
      if (!is_live[i] || pull_constant_loc[i] != -1) {
         /* This UNIFORM register is either dead, or has already been demoted
          * to a pull const.  Mark it as no longer living in the param[] array.
          */
         push_constant_loc[i] = -1;
         continue;
      }

      if (num_push_constants < max_push_components) {
         /* Retain as a push constant.  Record the location in the params[]
          * array.
          */
         push_constant_loc[i] = num_push_constants++;
      } else {
         /* Demote to a pull constant. */
         push_constant_loc[i] = -1;

         int pull_index = stage_prog_data->nr_pull_params++;
         stage_prog_data->pull_param[pull_index] = stage_prog_data->param[i];
         pull_constant_loc[i] = pull_index;
      }
   }

   stage_prog_data->nr_params = num_push_constants;

   /* Up until now, the param[] array has been indexed by reg + reg_offset
    * of UNIFORM registers.  Condense it to only contain the uniforms we
    * chose to upload as push constants.
    */
   for (unsigned int i = 0; i < uniforms; i++) {
      int remapped = push_constant_loc[i];

      if (remapped == -1)
         continue;

      assert(remapped <= (int)i);
      stage_prog_data->param[remapped] = stage_prog_data->param[i];
   }
}

/**
 * Replace UNIFORM register file access with either UNIFORM_PULL_CONSTANT_LOAD
 * or VARYING_PULL_CONSTANT_LOAD instructions which load values into VGRFs.
 */
void
fs_visitor::demote_pull_constants()
{
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      for (int i = 0; i < 3; i++) {
	 if (inst->src[i].file != UNIFORM)
	    continue;

         int pull_index = pull_constant_loc[inst->src[i].reg +
                                            inst->src[i].reg_offset];
         if (pull_index == -1)
	    continue;

         /* Set up the annotation tracking for new generated instructions. */
         base_ir = inst->ir;
         current_annotation = inst->annotation;

         fs_reg surf_index(stage_prog_data->binding_table.pull_constants_start);
         fs_reg dst = fs_reg(this, glsl_type::float_type);

         /* Generate a pull load into dst. */
         if (inst->src[i].reladdr) {
            exec_list list = VARYING_PULL_CONSTANT_LOAD(dst,
                                                        surf_index,
                                                        *inst->src[i].reladdr,
                                                        pull_index);
            inst->insert_before(&list);
            inst->src[i].reladdr = NULL;
         } else {
            fs_reg offset = fs_reg((unsigned)(pull_index * 4) & ~15);
            fs_inst *pull =
               new(mem_ctx) fs_inst(FS_OPCODE_UNIFORM_PULL_CONSTANT_LOAD,
                                    dst, surf_index, offset);
            inst->insert_before(pull);
            inst->src[i].set_smear(pull_index & 3);
         }

         /* Rewrite the instruction to use the temporary VGRF. */
         inst->src[i].file = GRF;
         inst->src[i].reg = dst.reg;
         inst->src[i].reg_offset = 0;
      }
   }
   invalidate_live_intervals();
}

bool
fs_visitor::opt_algebraic()
{
   bool progress = false;

   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      switch (inst->opcode) {
      case BRW_OPCODE_MUL:
	 if (inst->src[1].file != IMM)
	    continue;

	 /* a * 1.0 = a */
	 if (inst->src[1].is_one()) {
	    inst->opcode = BRW_OPCODE_MOV;
	    inst->src[1] = reg_undef;
	    progress = true;
	    break;
	 }

         /* a * 0.0 = 0.0 */
         if (inst->src[1].is_zero()) {
            inst->opcode = BRW_OPCODE_MOV;
            inst->src[0] = inst->src[1];
            inst->src[1] = reg_undef;
            progress = true;
            break;
         }

	 break;
      case BRW_OPCODE_ADD:
         if (inst->src[1].file != IMM)
            continue;

         /* a + 0.0 = a */
         if (inst->src[1].is_zero()) {
            inst->opcode = BRW_OPCODE_MOV;
            inst->src[1] = reg_undef;
            progress = true;
            break;
         }
         break;
      case BRW_OPCODE_OR:
         if (inst->src[0].equals(inst->src[1])) {
            inst->opcode = BRW_OPCODE_MOV;
            inst->src[1] = reg_undef;
            progress = true;
            break;
         }
         break;
      case BRW_OPCODE_LRP:
         if (inst->src[1].equals(inst->src[2])) {
            inst->opcode = BRW_OPCODE_MOV;
            inst->src[0] = inst->src[1];
            inst->src[1] = reg_undef;
            inst->src[2] = reg_undef;
            progress = true;
            break;
         }
         break;
      case BRW_OPCODE_SEL:
         if (inst->saturate && inst->src[1].file == IMM) {
            switch (inst->conditional_mod) {
            case BRW_CONDITIONAL_LE:
            case BRW_CONDITIONAL_L:
               switch (inst->src[1].type) {
               case BRW_REGISTER_TYPE_F:
                  if (inst->src[1].imm.f >= 1.0f) {
                     inst->opcode = BRW_OPCODE_MOV;
                     inst->src[1] = reg_undef;
                     progress = true;
                  }
                  break;
               default:
                  break;
               }
               break;
            case BRW_CONDITIONAL_GE:
            case BRW_CONDITIONAL_G:
               switch (inst->src[1].type) {
               case BRW_REGISTER_TYPE_F:
                  if (inst->src[1].imm.f <= 0.0f) {
                     inst->opcode = BRW_OPCODE_MOV;
                     inst->src[1] = reg_undef;
                     inst->conditional_mod = BRW_CONDITIONAL_NONE;
                     progress = true;
                  }
                  break;
               default:
                  break;
               }
            default:
               break;
            }
         }
         break;
      default:
	 break;
      }
   }

   return progress;
}

bool
fs_visitor::compute_to_mrf()
{
   bool progress = false;
   int next_ip = 0;

   calculate_live_intervals();

   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      int ip = next_ip;
      next_ip++;

      if (inst->opcode != BRW_OPCODE_MOV ||
	  inst->is_partial_write() ||
	  inst->dst.file != MRF || inst->src[0].file != GRF ||
	  inst->dst.type != inst->src[0].type ||
	  inst->src[0].abs || inst->src[0].negate ||
          !inst->src[0].is_contiguous() ||
          inst->src[0].subreg_offset)
	 continue;

      /* Work out which hardware MRF registers are written by this
       * instruction.
       */
      int mrf_low = inst->dst.reg & ~BRW_MRF_COMPR4;
      int mrf_high;
      if (inst->dst.reg & BRW_MRF_COMPR4) {
	 mrf_high = mrf_low + 4;
      } else if (dispatch_width == 16 &&
		 (!inst->force_uncompressed && !inst->force_sechalf)) {
	 mrf_high = mrf_low + 1;
      } else {
	 mrf_high = mrf_low;
      }

      /* Can't compute-to-MRF this GRF if someone else was going to
       * read it later.
       */
      if (this->virtual_grf_end[inst->src[0].reg] > ip)
	 continue;

      /* Found a move of a GRF to a MRF.  Let's see if we can go
       * rewrite the thing that made this GRF to write into the MRF.
       */
      fs_inst *scan_inst;
      for (scan_inst = (fs_inst *)inst->prev;
	   scan_inst->prev != NULL;
	   scan_inst = (fs_inst *)scan_inst->prev) {
	 if (scan_inst->dst.file == GRF &&
	     scan_inst->dst.reg == inst->src[0].reg) {
	    /* Found the last thing to write our reg we want to turn
	     * into a compute-to-MRF.
	     */

	    /* If this one instruction didn't populate all the
	     * channels, bail.  We might be able to rewrite everything
	     * that writes that reg, but it would require smarter
	     * tracking to delay the rewriting until complete success.
	     */
	    if (scan_inst->is_partial_write())
	       break;

            /* Things returning more than one register would need us to
             * understand coalescing out more than one MOV at a time.
             */
            if (scan_inst->regs_written > 1)
               break;

	    /* SEND instructions can't have MRF as a destination. */
	    if (scan_inst->mlen)
	       break;

	    if (brw->gen == 6) {
	       /* gen6 math instructions must have the destination be
		* GRF, so no compute-to-MRF for them.
		*/
	       if (scan_inst->is_math()) {
		  break;
	       }
	    }

	    if (scan_inst->dst.reg_offset == inst->src[0].reg_offset) {
	       /* Found the creator of our MRF's source value. */
	       scan_inst->dst.file = MRF;
	       scan_inst->dst.reg = inst->dst.reg;
	       scan_inst->saturate |= inst->saturate;
	       inst->remove();
	       progress = true;
	    }
	    break;
	 }

	 /* We don't handle control flow here.  Most computation of
	  * values that end up in MRFs are shortly before the MRF
	  * write anyway.
	  */
	 if (scan_inst->is_control_flow() && scan_inst->opcode != BRW_OPCODE_IF)
	    break;

	 /* You can't read from an MRF, so if someone else reads our
	  * MRF's source GRF that we wanted to rewrite, that stops us.
	  */
	 bool interfered = false;
	 for (int i = 0; i < 3; i++) {
	    if (scan_inst->src[i].file == GRF &&
		scan_inst->src[i].reg == inst->src[0].reg &&
		scan_inst->src[i].reg_offset == inst->src[0].reg_offset) {
	       interfered = true;
	    }
	 }
	 if (interfered)
	    break;

	 if (scan_inst->dst.file == MRF) {
	    /* If somebody else writes our MRF here, we can't
	     * compute-to-MRF before that.
	     */
	    int scan_mrf_low = scan_inst->dst.reg & ~BRW_MRF_COMPR4;
	    int scan_mrf_high;

	    if (scan_inst->dst.reg & BRW_MRF_COMPR4) {
	       scan_mrf_high = scan_mrf_low + 4;
	    } else if (dispatch_width == 16 &&
		       (!scan_inst->force_uncompressed &&
			!scan_inst->force_sechalf)) {
	       scan_mrf_high = scan_mrf_low + 1;
	    } else {
	       scan_mrf_high = scan_mrf_low;
	    }

	    if (mrf_low == scan_mrf_low ||
		mrf_low == scan_mrf_high ||
		mrf_high == scan_mrf_low ||
		mrf_high == scan_mrf_high) {
	       break;
	    }
	 }

	 if (scan_inst->mlen > 0 && scan_inst->base_mrf != -1) {
	    /* Found a SEND instruction, which means that there are
	     * live values in MRFs from base_mrf to base_mrf +
	     * scan_inst->mlen - 1.  Don't go pushing our MRF write up
	     * above it.
	     */
	    if (mrf_low >= scan_inst->base_mrf &&
		mrf_low < scan_inst->base_mrf + scan_inst->mlen) {
	       break;
	    }
	    if (mrf_high >= scan_inst->base_mrf &&
		mrf_high < scan_inst->base_mrf + scan_inst->mlen) {
	       break;
	    }
	 }
      }
   }

   if (progress)
      invalidate_live_intervals();

   return progress;
}

/**
 * Walks through basic blocks, looking for repeated MRF writes and
 * removing the later ones.
 */
bool
fs_visitor::remove_duplicate_mrf_writes()
{
   fs_inst *last_mrf_move[16];
   bool progress = false;

   /* Need to update the MRF tracking for compressed instructions. */
   if (dispatch_width == 16)
      return false;

   memset(last_mrf_move, 0, sizeof(last_mrf_move));

   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      if (inst->is_control_flow()) {
	 memset(last_mrf_move, 0, sizeof(last_mrf_move));
      }

      if (inst->opcode == BRW_OPCODE_MOV &&
	  inst->dst.file == MRF) {
	 fs_inst *prev_inst = last_mrf_move[inst->dst.reg];
	 if (prev_inst && inst->equals(prev_inst)) {
	    inst->remove();
	    progress = true;
	    continue;
	 }
      }

      /* Clear out the last-write records for MRFs that were overwritten. */
      if (inst->dst.file == MRF) {
	 last_mrf_move[inst->dst.reg] = NULL;
      }

      if (inst->mlen > 0 && inst->base_mrf != -1) {
	 /* Found a SEND instruction, which will include two or fewer
	  * implied MRF writes.  We could do better here.
	  */
	 for (int i = 0; i < implied_mrf_writes(inst); i++) {
	    last_mrf_move[inst->base_mrf + i] = NULL;
	 }
      }

      /* Clear out any MRF move records whose sources got overwritten. */
      if (inst->dst.file == GRF) {
	 for (unsigned int i = 0; i < Elements(last_mrf_move); i++) {
	    if (last_mrf_move[i] &&
		last_mrf_move[i]->src[0].reg == inst->dst.reg) {
	       last_mrf_move[i] = NULL;
	    }
	 }
      }

      if (inst->opcode == BRW_OPCODE_MOV &&
	  inst->dst.file == MRF &&
	  inst->src[0].file == GRF &&
	  !inst->is_partial_write()) {
	 last_mrf_move[inst->dst.reg] = inst;
      }
   }

   if (progress)
      invalidate_live_intervals();

   return progress;
}

static void
clear_deps_for_inst_src(fs_inst *inst, int dispatch_width, bool *deps,
                        int first_grf, int grf_len)
{
   bool inst_simd16 = (dispatch_width > 8 &&
                       !inst->force_uncompressed &&
                       !inst->force_sechalf);

   /* Clear the flag for registers that actually got read (as expected). */
   for (int i = 0; i < 3; i++) {
      int grf;
      if (inst->src[i].file == GRF) {
         grf = inst->src[i].reg;
      } else if (inst->src[i].file == HW_REG &&
                 inst->src[i].fixed_hw_reg.file == BRW_GENERAL_REGISTER_FILE) {
         grf = inst->src[i].fixed_hw_reg.nr;
      } else {
         continue;
      }

      if (grf >= first_grf &&
          grf < first_grf + grf_len) {
         deps[grf - first_grf] = false;
         if (inst_simd16)
            deps[grf - first_grf + 1] = false;
      }
   }
}

/**
 * Implements this workaround for the original 965:
 *
 *     "[DevBW, DevCL] Implementation Restrictions: As the hardware does not
 *      check for post destination dependencies on this instruction, software
 *      must ensure that there is no destination hazard for the case of âwrite
 *      followed by a posted writeâ shown in the following example.
 *
 *      1. mov r3 0
 *      2. send r3.xy <rest of send instruction>
 *      3. mov r2 r3
 *
 *      Due to no post-destination dependency check on the âsendâ, the above
 *      code sequence could have two instructions (1 and 2) in flight at the
 *      same time that both consider âr3â as the target of their final writes.
 */
void
fs_visitor::insert_gen4_pre_send_dependency_workarounds(fs_inst *inst)
{
   int reg_size = dispatch_width / 8;
   int write_len = inst->regs_written * reg_size;
   int first_write_grf = inst->dst.reg;
   bool needs_dep[BRW_MAX_MRF];
   assert(write_len < (int)sizeof(needs_dep) - 1);

   memset(needs_dep, false, sizeof(needs_dep));
   memset(needs_dep, true, write_len);

   clear_deps_for_inst_src(inst, dispatch_width,
                           needs_dep, first_write_grf, write_len);

   /* Walk backwards looking for writes to registers we're writing which
    * aren't read since being written.  If we hit the start of the program,
    * we assume that there are no outstanding dependencies on entry to the
    * program.
    */
   for (fs_inst *scan_inst = (fs_inst *)inst->prev;
        !scan_inst->is_head_sentinel();
        scan_inst = (fs_inst *)scan_inst->prev) {

      /* If we hit control flow, assume that there *are* outstanding
       * dependencies, and force their cleanup before our instruction.
       */
      if (scan_inst->is_control_flow()) {
         for (int i = 0; i < write_len; i++) {
            if (needs_dep[i]) {
               inst->insert_before(DEP_RESOLVE_MOV(first_write_grf + i));
            }
         }
         return;
      }

      bool scan_inst_simd16 = (dispatch_width > 8 &&
                               !scan_inst->force_uncompressed &&
                               !scan_inst->force_sechalf);

      /* We insert our reads as late as possible on the assumption that any
       * instruction but a MOV that might have left us an outstanding
       * dependency has more latency than a MOV.
       */
      if (scan_inst->dst.file == GRF) {
         for (int i = 0; i < scan_inst->regs_written; i++) {
            int reg = scan_inst->dst.reg + i * reg_size;

            if (reg >= first_write_grf &&
                reg < first_write_grf + write_len &&
                needs_dep[reg - first_write_grf]) {
               inst->insert_before(DEP_RESOLVE_MOV(reg));
               needs_dep[reg - first_write_grf] = false;
               if (scan_inst_simd16)
                  needs_dep[reg - first_write_grf + 1] = false;
            }
         }
      }

      /* Clear the flag for registers that actually got read (as expected). */
      clear_deps_for_inst_src(scan_inst, dispatch_width,
                              needs_dep, first_write_grf, write_len);

      /* Continue the loop only if we haven't resolved all the dependencies */
      int i;
      for (i = 0; i < write_len; i++) {
         if (needs_dep[i])
            break;
      }
      if (i == write_len)
         return;
   }
}

/**
 * Implements this workaround for the original 965:
 *
 *     "[DevBW, DevCL] Errata: A destination register from a send can not be
 *      used as a destination register until after it has been sourced by an
 *      instruction with a different destination register.
 */
void
fs_visitor::insert_gen4_post_send_dependency_workarounds(fs_inst *inst)
{
   int write_len = inst->regs_written * dispatch_width / 8;
   int first_write_grf = inst->dst.reg;
   bool needs_dep[BRW_MAX_MRF];
   assert(write_len < (int)sizeof(needs_dep) - 1);

   memset(needs_dep, false, sizeof(needs_dep));
   memset(needs_dep, true, write_len);
   /* Walk forwards looking for writes to registers we're writing which aren't
    * read before being written.
    */
   for (fs_inst *scan_inst = (fs_inst *)inst->next;
        !scan_inst->is_tail_sentinel();
        scan_inst = (fs_inst *)scan_inst->next) {
      /* If we hit control flow, force resolve all remaining dependencies. */
      if (scan_inst->is_control_flow()) {
         for (int i = 0; i < write_len; i++) {
            if (needs_dep[i])
               scan_inst->insert_before(DEP_RESOLVE_MOV(first_write_grf + i));
         }
         return;
      }

      /* Clear the flag for registers that actually got read (as expected). */
      clear_deps_for_inst_src(scan_inst, dispatch_width,
                              needs_dep, first_write_grf, write_len);

      /* We insert our reads as late as possible since they're reading the
       * result of a SEND, which has massive latency.
       */
      if (scan_inst->dst.file == GRF &&
          scan_inst->dst.reg >= first_write_grf &&
          scan_inst->dst.reg < first_write_grf + write_len &&
          needs_dep[scan_inst->dst.reg - first_write_grf]) {
         scan_inst->insert_before(DEP_RESOLVE_MOV(scan_inst->dst.reg));
         needs_dep[scan_inst->dst.reg - first_write_grf] = false;
      }

      /* Continue the loop only if we haven't resolved all the dependencies */
      int i;
      for (i = 0; i < write_len; i++) {
         if (needs_dep[i])
            break;
      }
      if (i == write_len)
         return;
   }

   /* If we hit the end of the program, resolve all remaining dependencies out
    * of paranoia.
    */
   fs_inst *last_inst = (fs_inst *)this->instructions.get_tail();
   assert(last_inst->eot);
   for (int i = 0; i < write_len; i++) {
      if (needs_dep[i])
         last_inst->insert_before(DEP_RESOLVE_MOV(first_write_grf + i));
   }
}

void
fs_visitor::insert_gen4_send_dependency_workarounds()
{
   if (brw->gen != 4 || brw->is_g4x)
      return;

   bool progress = false;

   /* Note that we're done with register allocation, so GRF fs_regs always
    * have a .reg_offset of 0.
    */

   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      if (inst->mlen != 0 && inst->dst.file == GRF) {
         insert_gen4_pre_send_dependency_workarounds(inst);
         insert_gen4_post_send_dependency_workarounds(inst);
         progress = true;
      }
   }

   if (progress)
      invalidate_live_intervals();
}

/**
 * Turns the generic expression-style uniform pull constant load instruction
 * into a hardware-specific series of instructions for loading a pull
 * constant.
 *
 * The expression style allows the CSE pass before this to optimize out
 * repeated loads from the same offset, and gives the pre-register-allocation
 * scheduling full flexibility, while the conversion to native instructions
 * allows the post-register-allocation scheduler the best information
 * possible.
 *
 * Note that execution masking for setting up pull constant loads is special:
 * the channels that need to be written are unrelated to the current execution
 * mask, since a later instruction will use one of the result channels as a
 * source operand for all 8 or 16 of its channels.
 */
void
fs_visitor::lower_uniform_pull_constant_loads()
{
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      if (inst->opcode != FS_OPCODE_UNIFORM_PULL_CONSTANT_LOAD)
         continue;

      if (brw->gen >= 7) {
         /* The offset arg before was a vec4-aligned byte offset.  We need to
          * turn it into a dword offset.
          */
         fs_reg const_offset_reg = inst->src[1];
         assert(const_offset_reg.file == IMM &&
                const_offset_reg.type == BRW_REGISTER_TYPE_UD);
         const_offset_reg.imm.u /= 4;
         fs_reg payload = fs_reg(this, glsl_type::uint_type);

         /* This is actually going to be a MOV, but since only the first dword
          * is accessed, we have a special opcode to do just that one.  Note
          * that this needs to be an operation that will be considered a def
          * by live variable analysis, or register allocation will explode.
          */
         fs_inst *setup = new(mem_ctx) fs_inst(FS_OPCODE_SET_SIMD4X2_OFFSET,
                                               payload, const_offset_reg);
         setup->force_writemask_all = true;

         setup->ir = inst->ir;
         setup->annotation = inst->annotation;
         inst->insert_before(setup);

         /* Similarly, this will only populate the first 4 channels of the
          * result register (since we only use smear values from 0-3), but we
          * don't tell the optimizer.
          */
         inst->opcode = FS_OPCODE_UNIFORM_PULL_CONSTANT_LOAD_GEN7;
         inst->src[1] = payload;

         invalidate_live_intervals();
      } else {
         /* Before register allocation, we didn't tell the scheduler about the
          * MRF we use.  We know it's safe to use this MRF because nothing
          * else does except for register spill/unspill, which generates and
          * uses its MRF within a single IR instruction.
          */
         inst->base_mrf = 14;
         inst->mlen = 1;
      }
   }
}

void
fs_visitor::dump_instructions()
{
   calculate_register_pressure();

   int ip = 0, max_pressure = 0;
   foreach_list(node, &this->instructions) {
      backend_instruction *inst = (backend_instruction *)node;
      max_pressure = MAX2(max_pressure, regs_live_at_ip[ip]);
      fprintf(stderr, "{%3d} %4d: ", regs_live_at_ip[ip], ip);
      dump_instruction(inst);
      ++ip;
   }
   fprintf(stderr, "Maximum %3d registers live at once.\n", max_pressure);
}

void
fs_visitor::dump_instruction(backend_instruction *be_inst)
{
   fs_inst *inst = (fs_inst *)be_inst;

   if (inst->predicate) {
      fprintf(stderr, "(%cf0.%d) ",
             inst->predicate_inverse ? '-' : '+',
             inst->flag_subreg);
   }

   fprintf(stderr, "%s", brw_instruction_name(inst->opcode));
   if (inst->saturate)
      fprintf(stderr, ".sat");
   if (inst->conditional_mod) {
      fprintf(stderr, "%s", conditional_modifier[inst->conditional_mod]);
      if (!inst->predicate &&
          (brw->gen < 5 || (inst->opcode != BRW_OPCODE_SEL &&
                              inst->opcode != BRW_OPCODE_IF &&
                              inst->opcode != BRW_OPCODE_WHILE))) {
         fprintf(stderr, ".f0.%d", inst->flag_subreg);
      }
   }
   fprintf(stderr, " ");


   switch (inst->dst.file) {
   case GRF:
      fprintf(stderr, "vgrf%d", inst->dst.reg);
      if (virtual_grf_sizes[inst->dst.reg] != 1 ||
          inst->dst.subreg_offset)
         fprintf(stderr, "+%d.%d",
                 inst->dst.reg_offset, inst->dst.subreg_offset);
      break;
   case MRF:
      fprintf(stderr, "m%d", inst->dst.reg);
      break;
   case BAD_FILE:
      fprintf(stderr, "(null)");
      break;
   case UNIFORM:
      fprintf(stderr, "***u%d***", inst->dst.reg + inst->dst.reg_offset);
      break;
   case HW_REG:
      if (inst->dst.fixed_hw_reg.file == BRW_ARCHITECTURE_REGISTER_FILE) {
         switch (inst->dst.fixed_hw_reg.nr) {
         case BRW_ARF_NULL:
            fprintf(stderr, "null");
            break;
         case BRW_ARF_ADDRESS:
            fprintf(stderr, "a0.%d", inst->dst.fixed_hw_reg.subnr);
            break;
         case BRW_ARF_ACCUMULATOR:
            fprintf(stderr, "acc%d", inst->dst.fixed_hw_reg.subnr);
            break;
         case BRW_ARF_FLAG:
            fprintf(stderr, "f%d.%d", inst->dst.fixed_hw_reg.nr & 0xf,
                             inst->dst.fixed_hw_reg.subnr);
            break;
         default:
            fprintf(stderr, "arf%d.%d", inst->dst.fixed_hw_reg.nr & 0xf,
                               inst->dst.fixed_hw_reg.subnr);
            break;
         }
      } else {
         fprintf(stderr, "hw_reg%d", inst->dst.fixed_hw_reg.nr);
      }
      if (inst->dst.fixed_hw_reg.subnr)
         fprintf(stderr, "+%d", inst->dst.fixed_hw_reg.subnr);
      break;
   default:
      fprintf(stderr, "???");
      break;
   }
   fprintf(stderr, ":%s, ", brw_reg_type_letters(inst->dst.type));

   for (int i = 0; i < 3 && inst->src[i].file != BAD_FILE; i++) {
      if (inst->src[i].negate)
         fprintf(stderr, "-");
      if (inst->src[i].abs)
         fprintf(stderr, "|");
      switch (inst->src[i].file) {
      case GRF:
         fprintf(stderr, "vgrf%d", inst->src[i].reg);
         if (virtual_grf_sizes[inst->src[i].reg] != 1 ||
             inst->src[i].subreg_offset)
            fprintf(stderr, "+%d.%d", inst->src[i].reg_offset,
                    inst->src[i].subreg_offset);
         break;
      case MRF:
         fprintf(stderr, "***m%d***", inst->src[i].reg);
         break;
      case UNIFORM:
         fprintf(stderr, "u%d", inst->src[i].reg + inst->src[i].reg_offset);
         if (inst->src[i].reladdr) {
            fprintf(stderr, "+reladdr");
         } else if (virtual_grf_sizes[inst->src[i].reg] != 1 ||
             inst->src[i].subreg_offset) {
            fprintf(stderr, "+%d.%d", inst->src[i].reg_offset,
                    inst->src[i].subreg_offset);
         }
         break;
      case BAD_FILE:
         fprintf(stderr, "(null)");
         break;
      case IMM:
         switch (inst->src[i].type) {
         case BRW_REGISTER_TYPE_F:
            fprintf(stderr, "%ff", inst->src[i].imm.f);
            break;
         case BRW_REGISTER_TYPE_D:
            fprintf(stderr, "%dd", inst->src[i].imm.i);
            break;
         case BRW_REGISTER_TYPE_UD:
            fprintf(stderr, "%uu", inst->src[i].imm.u);
            break;
         default:
            fprintf(stderr, "???");
            break;
         }
         break;
      case HW_REG:
         if (inst->src[i].fixed_hw_reg.negate)
            fprintf(stderr, "-");
         if (inst->src[i].fixed_hw_reg.abs)
            fprintf(stderr, "|");
         if (inst->src[i].fixed_hw_reg.file == BRW_ARCHITECTURE_REGISTER_FILE) {
            switch (inst->src[i].fixed_hw_reg.nr) {
            case BRW_ARF_NULL:
               fprintf(stderr, "null");
               break;
            case BRW_ARF_ADDRESS:
               fprintf(stderr, "a0.%d", inst->src[i].fixed_hw_reg.subnr);
               break;
            case BRW_ARF_ACCUMULATOR:
               fprintf(stderr, "acc%d", inst->src[i].fixed_hw_reg.subnr);
               break;
            case BRW_ARF_FLAG:
               fprintf(stderr, "f%d.%d", inst->src[i].fixed_hw_reg.nr & 0xf,
                                inst->src[i].fixed_hw_reg.subnr);
               break;
            default:
               fprintf(stderr, "arf%d.%d", inst->src[i].fixed_hw_reg.nr & 0xf,
                                  inst->src[i].fixed_hw_reg.subnr);
               break;
            }
         } else {
            fprintf(stderr, "hw_reg%d", inst->src[i].fixed_hw_reg.nr);
         }
         if (inst->src[i].fixed_hw_reg.subnr)
            fprintf(stderr, "+%d", inst->src[i].fixed_hw_reg.subnr);
         if (inst->src[i].fixed_hw_reg.abs)
            fprintf(stderr, "|");
         break;
      default:
         fprintf(stderr, "???");
         break;
      }
      if (inst->src[i].abs)
         fprintf(stderr, "|");

      if (inst->src[i].file != IMM) {
         fprintf(stderr, ":%s", brw_reg_type_letters(inst->src[i].type));
      }

      if (i < 2 && inst->src[i + 1].file != BAD_FILE)
         fprintf(stderr, ", ");
   }

   fprintf(stderr, " ");

   if (inst->force_uncompressed)
      fprintf(stderr, "1sthalf ");

   if (inst->force_sechalf)
      fprintf(stderr, "2ndhalf ");

   fprintf(stderr, "\n");
}

/**
 * Possibly returns an instruction that set up @@param reg.
 *
 * Sometimes we want to take the result of some expression/variable
 * dereference tree and rewrite the instruction generating the result
 * of the tree.  When processing the tree, we know that the
 * instructions generated are all writing temporaries that are dead
 * outside of this tree.  So, if we have some instructions that write
 * a temporary, we're free to point that temp write somewhere else.
 *
 * Note that this doesn't guarantee that the instruction generated
 * only reg -- it might be the size=4 destination of a texture instruction.
 */
fs_inst *
fs_visitor::get_instruction_generating_reg(fs_inst *start,
					   fs_inst *end,
					   const fs_reg &reg)
{
   if (end == start ||
       end->is_partial_write() ||
       reg.reladdr ||
       !reg.equals(end->dst)) {
      return NULL;
   } else {
      return end;
   }
}

void
fs_visitor::setup_payload_gen6()
{
   bool uses_depth =
      (fp->Base.InputsRead & (1 << VARYING_SLOT_POS)) != 0;
   unsigned barycentric_interp_modes = c->prog_data.barycentric_interp_modes;

   assert(brw->gen >= 6);

   /* R0-1: masks, pixel X/Y coordinates. */
   c->nr_payload_regs = 2;
   /* R2: only for 32-pixel dispatch.*/

   /* R3-26: barycentric interpolation coordinates.  These appear in the
    * same order that they appear in the brw_wm_barycentric_interp_mode
    * enum.  Each set of coordinates occupies 2 registers if dispatch width
    * == 8 and 4 registers if dispatch width == 16.  Coordinates only
    * appear if they were enabled using the "Barycentric Interpolation
    * Mode" bits in WM_STATE.
    */
   for (int i = 0; i < BRW_WM_BARYCENTRIC_INTERP_MODE_COUNT; ++i) {
      if (barycentric_interp_modes & (1 << i)) {
         c->barycentric_coord_reg[i] = c->nr_payload_regs;
         c->nr_payload_regs += 2;
         if (dispatch_width == 16) {
            c->nr_payload_regs += 2;
         }
      }
   }

   /* R27: interpolated depth if uses source depth */
   if (uses_depth) {
      c->source_depth_reg = c->nr_payload_regs;
      c->nr_payload_regs++;
      if (dispatch_width == 16) {
         /* R28: interpolated depth if not SIMD8. */
         c->nr_payload_regs++;
      }
   }
   /* R29: interpolated W set if GEN6_WM_USES_SOURCE_W. */
   if (uses_depth) {
      c->source_w_reg = c->nr_payload_regs;
      c->nr_payload_regs++;
      if (dispatch_width == 16) {
         /* R30: interpolated W if not SIMD8. */
         c->nr_payload_regs++;
      }
   }

   c->prog_data.uses_pos_offset = c->key.compute_pos_offset;
   /* R31: MSAA position offsets. */
   if (c->prog_data.uses_pos_offset) {
      c->sample_pos_reg = c->nr_payload_regs;
      c->nr_payload_regs++;
   }

   /* R32: MSAA input coverage mask */
   if (fp->Base.SystemValuesRead & SYSTEM_BIT_SAMPLE_MASK_IN) {
      assert(brw->gen >= 7);
      c->sample_mask_reg = c->nr_payload_regs;
      c->nr_payload_regs++;
      if (dispatch_width == 16) {
         /* R33: input coverage mask if not SIMD8. */
         c->nr_payload_regs++;
      }
   }

   /* R34-: bary for 32-pixel. */
   /* R58-59: interp W for 32-pixel. */

   if (fp->Base.OutputsWritten & BITFIELD64_BIT(FRAG_RESULT_DEPTH)) {
      c->source_depth_to_render_target = true;
   }
}

void
fs_visitor::assign_binding_table_offsets()
{
   uint32_t next_binding_table_offset = 0;

   /* If there are no color regions, we still perform an FB write to a null
    * renderbuffer, which we place at surface index 0.
    */
   c->prog_data.binding_table.render_target_start = next_binding_table_offset;
   next_binding_table_offset += MAX2(c->key.nr_color_regions, 1);

   assign_common_binding_table_offsets(next_binding_table_offset);
}

void
fs_visitor::calculate_register_pressure()
{
   invalidate_live_intervals();
   calculate_live_intervals();

   int num_instructions = 0;
   foreach_list(node, &this->instructions) {
      ++num_instructions;
   }

   regs_live_at_ip = rzalloc_array(mem_ctx, int, num_instructions);

   for (int reg = 0; reg < virtual_grf_count; reg++) {
      for (int ip = virtual_grf_start[reg]; ip <= virtual_grf_end[reg]; ip++)
         regs_live_at_ip[ip] += virtual_grf_sizes[reg];
   }
}

/**
 * Look for repeated FS_OPCODE_MOV_DISPATCH_TO_FLAGS and drop the later ones.
 *
 * The needs_unlit_centroid_workaround ends up producing one of these per
 * channel of centroid input, so it's good to clean them up.
 *
 * An assumption here is that nothing ever modifies the dispatched pixels
 * value that FS_OPCODE_MOV_DISPATCH_TO_FLAGS reads from, but the hardware
 * dictates that anyway.
 */
void
fs_visitor::opt_drop_redundant_mov_to_flags()
{
   bool flag_mov_found[2] = {false};

   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      if (inst->is_control_flow()) {
         memset(flag_mov_found, 0, sizeof(flag_mov_found));
      } else if (inst->opcode == FS_OPCODE_MOV_DISPATCH_TO_FLAGS) {
         if (!flag_mov_found[inst->flag_subreg])
            flag_mov_found[inst->flag_subreg] = true;
         else
            inst->remove();
      } else if (inst->writes_flag()) {
         flag_mov_found[inst->flag_subreg] = false;
      }
   }
}

bool
fs_visitor::run()
{
   sanity_param_count = fp->Base.Parameters->NumParameters;
   bool allocated_without_spills;

   assign_binding_table_offsets();

   if (brw->gen >= 6)
      setup_payload_gen6();
   else
      setup_payload_gen4();

   if (0) {
      emit_dummy_fs();
   } else {
      if (INTEL_DEBUG & DEBUG_SHADER_TIME)
         emit_shader_time_begin();

      calculate_urb_setup();
      if (fp->Base.InputsRead > 0) {
         if (brw->gen < 6)
            emit_interpolation_setup_gen4();
         else
            emit_interpolation_setup_gen6();
      }

      /* We handle discards by keeping track of the still-live pixels in f0.1.
       * Initialize it with the dispatched pixels.
       */
      if (fp->UsesKill || c->key.alpha_test_func) {
         fs_inst *discard_init = emit(FS_OPCODE_MOV_DISPATCH_TO_FLAGS);
         discard_init->flag_subreg = 1;
      }

      /* Generate FS IR for main().  (the visitor only descends into
       * functions called "main").
       */
      if (shader) {
         foreach_list(node, &*shader->base.ir) {
            ir_instruction *ir = (ir_instruction *)node;
            base_ir = ir;
            this->result = reg_undef;
            ir->accept(this);
         }
      } else {
         emit_fragment_program_code();
      }
      base_ir = NULL;
      if (failed)
	 return false;

      emit(FS_OPCODE_PLACEHOLDER_HALT);

      if (c->key.alpha_test_func)
         emit_alpha_test();

      emit_fb_writes();

      split_virtual_grfs();

      move_uniform_array_access_to_pull_constants();
      assign_constant_locations();
      demote_pull_constants();

      opt_drop_redundant_mov_to_flags();

      bool progress;
      do {
	 progress = false;

         compact_virtual_grfs();

	 progress = remove_duplicate_mrf_writes() || progress;

	 progress = opt_algebraic() || progress;
	 progress = opt_cse() || progress;
	 progress = opt_copy_propagate() || progress;
         progress = opt_peephole_predicated_break() || progress;
         progress = dead_code_eliminate() || progress;
         progress = opt_peephole_sel() || progress;
         progress = dead_control_flow_eliminate(this) || progress;
         progress = opt_saturate_propagation() || progress;
         progress = register_coalesce() || progress;
	 progress = compute_to_mrf() || progress;
      } while (progress);

      lower_uniform_pull_constant_loads();

      assign_curb_setup();
      assign_urb_setup();

      static enum instruction_scheduler_mode pre_modes[] = {
         SCHEDULE_PRE,
         SCHEDULE_PRE_NON_LIFO,
         SCHEDULE_PRE_LIFO,
      };

      /* Try each scheduling heuristic to see if it can successfully register
       * allocate without spilling.  They should be ordered by decreasing
       * performance but increasing likelihood of allocating.
       */
      for (unsigned i = 0; i < ARRAY_SIZE(pre_modes); i++) {
         schedule_instructions(pre_modes[i]);

         if (0) {
            assign_regs_trivial();
            allocated_without_spills = true;
         } else {
            allocated_without_spills = assign_regs(false);
         }
         if (allocated_without_spills)
            break;
      }

      if (!allocated_without_spills) {
         /* We assume that any spilling is worse than just dropping back to
          * SIMD8.  There's probably actually some intermediate point where
          * SIMD16 with a couple of spills is still better.
          */
         if (dispatch_width == 16) {
            fail("Failure to register allocate.  Reduce number of "
                 "live scalar values to avoid this.");
         }

         /* Since we're out of heuristics, just go spill registers until we
          * get an allocation.
          */
         while (!assign_regs(true)) {
            if (failed)
               break;
         }
      }
   }
   assert(force_uncompressed_stack == 0);

   /* This must come after all optimization and register allocation, since
    * it inserts dead code that happens to have side effects, and it does
    * so based on the actual physical registers in use.
    */
   insert_gen4_send_dependency_workarounds();

   if (failed)
      return false;

   if (!allocated_without_spills)
      schedule_instructions(SCHEDULE_POST);

   if (dispatch_width == 8)
      c->prog_data.reg_blocks = brw_register_blocks(grf_used);
   else
      c->prog_data.reg_blocks_16 = brw_register_blocks(grf_used);

   /* If any state parameters were appended, then ParameterValues could have
    * been realloced, in which case the driver uniform storage set up by
    * _mesa_associate_uniform_storage() would point to freed memory.  Make
    * sure that didn't happen.
    */
   assert(sanity_param_count == fp->Base.Parameters->NumParameters);

   return !failed;
}

const unsigned *
brw_wm_fs_emit(struct brw_context *brw, struct brw_wm_compile *c,
               struct gl_fragment_program *fp,
               struct gl_shader_program *prog,
               unsigned *final_assembly_size)
{
   bool start_busy = false;
   double start_time = 0;

   if (unlikely(brw->perf_debug)) {
      start_busy = (brw->batch.last_bo &&
                    drm_intel_bo_busy(brw->batch.last_bo));
      start_time = get_time();
   }

   struct brw_shader *shader = NULL;
   if (prog)
      shader = (brw_shader *) prog->_LinkedShaders[MESA_SHADER_FRAGMENT];

   if (unlikely(INTEL_DEBUG & DEBUG_WM))
      brw_dump_ir(brw, "fragment", prog, &shader->base, &fp->Base);

   /* Now the main event: Visit the shader IR and generate our FS IR for it.
    */
   fs_visitor v(brw, c, prog, fp, 8);
   if (!v.run()) {
      if (prog) {
         prog->LinkStatus = false;
         ralloc_strcat(&prog->InfoLog, v.fail_msg);
      }

      _mesa_problem(NULL, "Failed to compile fragment shader: %s\n",
                    v.fail_msg);

      return NULL;
   }

   exec_list *simd16_instructions = NULL;
   fs_visitor v2(brw, c, prog, fp, 16);
   if (brw->gen >= 5 && likely(!(INTEL_DEBUG & DEBUG_NO16))) {
      if (!v.simd16_unsupported) {
         /* Try a SIMD16 compile */
         v2.import_uniforms(&v);
         if (!v2.run()) {
            perf_debug("SIMD16 shader failed to compile, falling back to "
                       "SIMD8 at a 10-20%% performance cost: %s", v2.fail_msg);
         } else {
            simd16_instructions = &v2.instructions;
         }
      } else {
         perf_debug("SIMD16 shader unsupported, falling back to "
                    "SIMD8 at a 10-20%% performance cost: %s", v.no16_msg);
      }
   }

   const unsigned *assembly = NULL;
   if (brw->gen >= 8) {
      gen8_fs_generator g(brw, c, prog, fp, v.do_dual_src);
      assembly = g.generate_assembly(&v.instructions, simd16_instructions,
                                     final_assembly_size);
   } else {
      fs_generator g(brw, c, prog, fp, v.do_dual_src);
      assembly = g.generate_assembly(&v.instructions, simd16_instructions,
                                     final_assembly_size);
   }

   if (unlikely(brw->perf_debug) && shader) {
      if (shader->compiled_once)
         brw_wm_debug_recompile(brw, prog, &c->key);
      shader->compiled_once = true;

      if (start_busy && !drm_intel_bo_busy(brw->batch.last_bo)) {
         perf_debug("FS compile took %.03f ms and stalled the GPU\n",
                    (get_time() - start_time) * 1000);
      }
   }

   return assembly;
}

bool
brw_fs_precompile(struct gl_context *ctx, struct gl_shader_program *prog)
{
   struct brw_context *brw = brw_context(ctx);
   struct brw_wm_prog_key key;

   if (!prog->_LinkedShaders[MESA_SHADER_FRAGMENT])
      return true;

   struct gl_fragment_program *fp = (struct gl_fragment_program *)
      prog->_LinkedShaders[MESA_SHADER_FRAGMENT]->Program;
   struct brw_fragment_program *bfp = brw_fragment_program(fp);
   bool program_uses_dfdy = fp->UsesDFdy;

   memset(&key, 0, sizeof(key));

   if (brw->gen < 6) {
      if (fp->UsesKill)
         key.iz_lookup |= IZ_PS_KILL_ALPHATEST_BIT;

      if (fp->Base.OutputsWritten & BITFIELD64_BIT(FRAG_RESULT_DEPTH))
         key.iz_lookup |= IZ_PS_COMPUTES_DEPTH_BIT;

      /* Just assume depth testing. */
      key.iz_lookup |= IZ_DEPTH_TEST_ENABLE_BIT;
      key.iz_lookup |= IZ_DEPTH_WRITE_ENABLE_BIT;
   }

   if (brw->gen < 6 || _mesa_bitcount_64(fp->Base.InputsRead &
                                         BRW_FS_VARYING_INPUT_MASK) > 16)
      key.input_slots_valid = fp->Base.InputsRead | VARYING_BIT_POS;

   key.clamp_fragment_color = ctx->API == API_OPENGL_COMPAT;

   unsigned sampler_count = _mesa_fls(fp->Base.SamplersUsed);
   for (unsigned i = 0; i < sampler_count; i++) {
      if (fp->Base.ShadowSamplers & (1 << i)) {
         /* Assume DEPTH_TEXTURE_MODE is the default: X, X, X, 1 */
         key.tex.swizzles[i] =
            MAKE_SWIZZLE4(SWIZZLE_X, SWIZZLE_X, SWIZZLE_X, SWIZZLE_ONE);
      } else {
         /* Color sampler: assume no swizzling. */
         key.tex.swizzles[i] = SWIZZLE_XYZW;
      }
   }

   if (fp->Base.InputsRead & VARYING_BIT_POS) {
      key.drawable_height = ctx->DrawBuffer->Height;
   }

   key.nr_color_regions = _mesa_bitcount_64(fp->Base.OutputsWritten &
         ~(BITFIELD64_BIT(FRAG_RESULT_DEPTH) |
         BITFIELD64_BIT(FRAG_RESULT_SAMPLE_MASK)));

   if ((fp->Base.InputsRead & VARYING_BIT_POS) || program_uses_dfdy) {
      key.render_to_fbo = _mesa_is_user_fbo(ctx->DrawBuffer) ||
                          key.nr_color_regions > 1;
   }

   /* GL_FRAGMENT_SHADER_DERIVATIVE_HINT is almost always GL_DONT_CARE.  The
    * quality of the derivatives is likely to be determined by the driconf
    * option.
    */
   key.high_quality_derivatives = brw->disable_derivative_optimization;

   key.program_string_id = bfp->id;

   uint32_t old_prog_offset = brw->wm.base.prog_offset;
   struct brw_wm_prog_data *old_prog_data = brw->wm.prog_data;

   bool success = do_wm_prog(brw, prog, bfp, &key);

   brw->wm.base.prog_offset = old_prog_offset;
   brw->wm.prog_data = old_prog_data;

   return success;
}
@


1.9
log
@Merge Mesa 10.2.9
@
text
@@


1.8
log
@Merge Mesa 10.4.3
Tested by matthieu@@ mpi@@ and myself.  landry@@ ran a ports bulk build.
kettenis@@ tracked down the cause of an alignment fault on archs
that require strict eight byte pointer alignment.
@
text
@d35 1
a35 1
#include "util/hash_table.h"
d41 1
a41 1
#include "util/register_allocate.h"
a48 1
#include "brw_cfg.h"
d55 1
a55 2
fs_inst::init(enum opcode opcode, uint8_t exec_size, const fs_reg &dst,
              fs_reg *src, int sources)
d58 1
d60 4
a63 57
   this->opcode = opcode;
   this->dst = dst;
   this->src = src;
   this->sources = sources;
   this->exec_size = exec_size;

   assert(dst.file != IMM && dst.file != UNIFORM);

   /* If exec_size == 0, try to guess it from the registers.  Since all
    * manner of things may use hardware registers, we first try to guess
    * based on GRF registers.  If this fails, we will go ahead and take the
    * width from the destination register.
    */
   if (this->exec_size == 0) {
      if (dst.file == GRF) {
         this->exec_size = dst.width;
      } else {
         for (int i = 0; i < sources; ++i) {
            if (src[i].file != GRF)
               continue;

            if (this->exec_size <= 1)
               this->exec_size = src[i].width;
            assert(src[i].width == 1 || src[i].width == this->exec_size);
         }
      }

      if (this->exec_size == 0 && dst.file != BAD_FILE)
         this->exec_size = dst.width;
   }
   assert(this->exec_size != 0);

   for (int i = 0; i < sources; ++i) {
      switch (this->src[i].file) {
      case BAD_FILE:
         this->src[i].effective_width = 8;
         break;
      case GRF:
      case HW_REG:
         assert(this->src[i].width > 0);
         if (this->src[i].width == 1) {
            this->src[i].effective_width = this->exec_size;
         } else {
            this->src[i].effective_width = this->src[i].width;
         }
         break;
      case IMM:
      case UNIFORM:
         this->src[i].effective_width = this->exec_size;
         break;
      default:
         unreachable("Invalid source register file");
      }
   }
   this->dst.effective_width = this->exec_size;

   this->conditional_mod = BRW_CONDITIONAL_NONE;
d66 1
a66 15
   switch (dst.file) {
   case GRF:
   case HW_REG:
   case MRF:
      this->regs_written = (dst.width * dst.stride * type_sz(dst.type) + 31) / 32;
      break;
   case BAD_FILE:
      this->regs_written = 0;
      break;
   case IMM:
   case UNIFORM:
      unreachable("Invalid destination register file");
   default:
      unreachable("Invalid register file");
   }
d73 2
a74 2
   fs_reg *src = ralloc_array(this, fs_reg, 3);
   init(BRW_OPCODE_NOP, 8, dst, src, 0);
d77 1
a77 1
fs_inst::fs_inst(enum opcode opcode, uint8_t exec_size)
d79 2
a80 2
   fs_reg *src = ralloc_array(this, fs_reg, 3);
   init(opcode, exec_size, reg_undef, src, 0);
d83 1
a83 1
fs_inst::fs_inst(enum opcode opcode, const fs_reg &dst)
d85 3
a87 3
   fs_reg *src = ralloc_array(this, fs_reg, 3);
   init(opcode, 0, dst, src, 0);
}
d89 2
a90 6
fs_inst::fs_inst(enum opcode opcode, uint8_t exec_size, const fs_reg &dst,
                 const fs_reg &src0)
{
   fs_reg *src = ralloc_array(this, fs_reg, 3);
   src[0] = src0;
   init(opcode, exec_size, dst, src, 1);
d93 1
a93 1
fs_inst::fs_inst(enum opcode opcode, const fs_reg &dst, const fs_reg &src0)
d95 4
a98 4
   fs_reg *src = ralloc_array(this, fs_reg, 3);
   src[0] = src0;
   init(opcode, 0, dst, src, 1);
}
d100 4
a103 7
fs_inst::fs_inst(enum opcode opcode, uint8_t exec_size, const fs_reg &dst,
                 const fs_reg &src0, const fs_reg &src1)
{
   fs_reg *src = ralloc_array(this, fs_reg, 3);
   src[0] = src0;
   src[1] = src1;
   init(opcode, exec_size, dst, src, 2);
d106 1
a106 2
fs_inst::fs_inst(enum opcode opcode, const fs_reg &dst, const fs_reg &src0,
                 const fs_reg &src1)
d108 5
a112 5
   fs_reg *src = ralloc_array(this, fs_reg, 3);
   src[0] = src0;
   src[1] = src1;
   init(opcode, 0, dst, src, 2);
}
d114 6
a119 8
fs_inst::fs_inst(enum opcode opcode, uint8_t exec_size, const fs_reg &dst,
                 const fs_reg &src0, const fs_reg &src1, const fs_reg &src2)
{
   fs_reg *src = ralloc_array(this, fs_reg, 3);
   src[0] = src0;
   src[1] = src1;
   src[2] = src2;
   init(opcode, exec_size, dst, src, 3);
d122 2
a123 2
fs_inst::fs_inst(enum opcode opcode, const fs_reg &dst, const fs_reg &src0,
                 const fs_reg &src1, const fs_reg &src2)
d125 15
a139 35
   fs_reg *src = ralloc_array(this, fs_reg, 3);
   src[0] = src0;
   src[1] = src1;
   src[2] = src2;
   init(opcode, 0, dst, src, 3);
}

fs_inst::fs_inst(enum opcode opcode, const fs_reg &dst, fs_reg src[], int sources)
{
   init(opcode, 0, dst, src, sources);
}

fs_inst::fs_inst(enum opcode opcode, uint8_t exec_width, const fs_reg &dst,
                 fs_reg src[], int sources)
{
   init(opcode, exec_width, dst, src, sources);
}

fs_inst::fs_inst(const fs_inst &that)
{
   memcpy(this, &that, sizeof(that));

   this->src = ralloc_array(this, fs_reg, that.sources);

   for (int i = 0; i < that.sources; i++)
      this->src[i] = that.src[i];
}

void
fs_inst::resize_sources(uint8_t num_sources)
{
   if (this->sources != num_sources) {
      this->src = reralloc(this, this->src, fs_reg, num_sources);
      this->sources = num_sources;
   }
d144 1
a144 1
   fs_visitor::op(const fs_reg &dst, const fs_reg &src0)                \
d151 1
a151 2
   fs_visitor::op(const fs_reg &dst, const fs_reg &src0,                \
                  const fs_reg &src1)                                   \
d158 1
a158 2
   fs_visitor::op(const fs_reg &dst, const fs_reg &src0,                \
                  const fs_reg &src1)                                   \
d167 1
a167 2
   fs_visitor::op(const fs_reg &dst, const fs_reg &src0,                \
                  const fs_reg &src1, const fs_reg &src2)               \
d203 1
a203 1
fs_visitor::IF(enum brw_predicate predicate)
d205 1
a205 1
   fs_inst *inst = new(mem_ctx) fs_inst(BRW_OPCODE_IF, dispatch_width);
d212 1
a212 2
fs_visitor::IF(const fs_reg &src0, const fs_reg &src1,
               enum brw_conditional_mod condition)
d215 1
a215 1
   fs_inst *inst = new(mem_ctx) fs_inst(BRW_OPCODE_IF, dispatch_width,
d227 1
a227 2
fs_visitor::CMP(fs_reg dst, fs_reg src0, fs_reg src1,
                enum brw_conditional_mod condition)
a257 26
fs_inst *
fs_visitor::LOAD_PAYLOAD(const fs_reg &dst, fs_reg *src, int sources)
{
   uint8_t exec_size = dst.width;
   for (int i = 0; i < sources; ++i) {
      assert(src[i].width % dst.width == 0);
      if (src[i].width > exec_size)
         exec_size = src[i].width;
   }

   fs_inst *inst = new(mem_ctx) fs_inst(SHADER_OPCODE_LOAD_PAYLOAD, exec_size,
                                        dst, src, sources);
   inst->regs_written = 0;
   for (int i = 0; i < sources; ++i) {
      /* The LOAD_PAYLOAD instruction only really makes sense if we are
       * dealing with whole registers.  If this ever changes, we can deal
       * with it later.
       */
      int size = src[i].effective_width * type_sz(src[i].type);
      assert(size % 32 == 0);
      inst->regs_written += (size + 31) / 32;
   }

   return inst;
}

d280 1
a280 1
                              varying_offset, fs_reg(const_offset & ~3)));
d283 1
a283 1
   if (brw->gen == 4 && dst.width == 8) {
d297 1
a297 5

   assert(dst.width % 8 == 0);
   int regs_written = 4 * (dst.width / 8) * scale;
   fs_reg vec4_result = fs_reg(GRF, virtual_grf_alloc(regs_written),
                               dst.type, dst.width);
d299 1
a299 1
   inst->regs_written = regs_written;
d311 2
a312 2
   fs_reg result = offset(vec4_result, (const_offset & 3) * scale);
   instructions.push_tail(MOV(dst, result));
d332 1
a332 1
   inst->exec_size = 8;
d350 1
a354 1
           exec_size == inst->exec_size &&
d370 5
a374 20
   switch (opcode) {
   case FS_OPCODE_VARYING_PULL_CONSTANT_LOAD_GEN7:
   case SHADER_OPCODE_SHADER_TIME_ADD:
   case FS_OPCODE_INTERPOLATE_AT_CENTROID:
   case FS_OPCODE_INTERPOLATE_AT_SAMPLE:
   case FS_OPCODE_INTERPOLATE_AT_SHARED_OFFSET:
   case FS_OPCODE_INTERPOLATE_AT_PER_SLOT_OFFSET:
   case SHADER_OPCODE_UNTYPED_ATOMIC:
   case SHADER_OPCODE_UNTYPED_SURFACE_READ:
      return true;
   case FS_OPCODE_UNIFORM_PULL_CONSTANT_LOAD:
      return src[1].file == GRF;
   case FS_OPCODE_FB_WRITE:
      return src[0].file == GRF;
   default:
      if (is_tex())
         return src[0].file == GRF;

      return false;
   }
d378 1
a378 1
fs_inst::can_do_source_mods(struct brw_context *brw)
d380 1
a380 1
   if (brw->gen == 6 && is_math())
d383 1
a383 1
   if (is_send_from_grf())
d386 1
a386 1
   if (!backend_instruction::can_do_source_mods())
d412 1
a412 2
   this->fixed_hw_reg.dw1.f = f;
   this->width = 1;
d421 1
a421 2
   this->fixed_hw_reg.dw1.d = i;
   this->width = 1;
d430 1
a430 2
   this->fixed_hw_reg.dw1.ud = u;
   this->width = 1;
a439 1
   this->width = 1 << fixed_hw_reg.width;
d453 4
a456 3
           memcmp(&fixed_hw_reg, &r.fixed_hw_reg, sizeof(fixed_hw_reg)) == 0 &&
           width == r.width &&
           stride == r.stride);
d485 26
d516 8
d554 2
a555 1
      unreachable("not reached");
d566 1
a566 1
   fs_reg ts = fs_reg(retype(brw_vec4_reg(BRW_ARCHITECTURE_REGISTER_FILE,
d571 1
a571 1
   fs_reg dst = fs_reg(GRF, virtual_grf_alloc(1), BRW_REGISTER_TYPE_UD, 4);
d574 2
a575 2
   /* We want to read the 3 fields we care about even if it's not enabled in
    * the dispatch.
d578 1
d630 1
d633 1
a633 1
   fs_reg diff = fs_reg(GRF, virtual_grf_alloc(1), BRW_REGISTER_TYPE_UD, 1);
d647 2
d656 1
a656 1
      brw_get_shader_time_index(brw, shader_prog, prog, type);
d734 1
a734 1
   return emit(new(mem_ctx) fs_inst(opcode, dispatch_width));
d738 1
a738 1
fs_visitor::emit(enum opcode opcode, const fs_reg &dst)
d744 1
a744 1
fs_visitor::emit(enum opcode opcode, const fs_reg &dst, const fs_reg &src0)
d750 1
a750 2
fs_visitor::emit(enum opcode opcode, const fs_reg &dst, const fs_reg &src0,
                 const fs_reg &src1)
d756 2
a757 2
fs_visitor::emit(enum opcode opcode, const fs_reg &dst, const fs_reg &src0,
                 const fs_reg &src1, const fs_reg &src2)
d762 8
a769 3
fs_inst *
fs_visitor::emit(enum opcode opcode, const fs_reg &dst,
                 fs_reg src[], int sources)
d771 2
a772 1
   return emit(new(mem_ctx) fs_inst(opcode, dst, src, sources));
d787 2
a788 2
           (this->dst.width * type_sz(this->dst.type)) < 32 ||
           !this->dst.is_contiguous());
d795 4
a798 26
      return mlen;
   } else if (opcode == FS_OPCODE_FB_WRITE && arg == 0) {
      return mlen;
   } else if (opcode == SHADER_OPCODE_UNTYPED_ATOMIC && arg == 0) {
      return mlen;
   } else if (opcode == SHADER_OPCODE_UNTYPED_SURFACE_READ && arg == 0) {
      return mlen;
   }

   switch (src[arg].file) {
   case BAD_FILE:
   case UNIFORM:
   case IMM:
      return 1;
   case GRF:
   case HW_REG:
      if (src[arg].stride == 0) {
         return 1;
      } else {
         int size = src[arg].width * src[arg].stride * type_sz(src[arg].type);
         return (size + 31) / 32;
      }
   case MRF:
      unreachable("MRF registers are not allowed as sources");
   default:
      unreachable("Invalid register file");
d800 1
a866 4
   case FS_OPCODE_INTERPOLATE_AT_CENTROID:
   case FS_OPCODE_INTERPOLATE_AT_SAMPLE:
   case FS_OPCODE_INTERPOLATE_AT_SHARED_OFFSET:
   case FS_OPCODE_INTERPOLATE_AT_PER_SLOT_OFFSET:
d869 2
a870 1
      unreachable("not reached");
a895 8

   switch (file) {
   case UNIFORM:
      this->width = 1;
      break;
   default:
      this->width = 8;
   }
d899 1
a899 1
fs_reg::fs_reg(enum register_file file, int reg, enum brw_reg_type type)
a904 19

   switch (file) {
   case UNIFORM:
      this->width = 1;
      break;
   default:
      this->width = 8;
   }
}

/** Fixed HW reg constructor. */
fs_reg::fs_reg(enum register_file file, int reg, enum brw_reg_type type,
               uint8_t width)
{
   init();
   this->file = file;
   this->reg = reg;
   this->type = type;
   this->width = width;
d908 1
a908 1
fs_reg::fs_reg(fs_visitor *v, const struct glsl_type *type)
a910 1
   int reg_width = v->dispatch_width / 8;
d913 1
a913 1
   this->reg = v->virtual_grf_alloc(v->type_size(type) * reg_width);
a915 2
   this->width = v->dispatch_width;
   assert(this->width == 8 || this->width == 16);
d985 1
a985 1
         stage_prog_data->param[uniforms++] = &storage->storage[i];
d1002 2
a1003 2
   const ir_state_slot *const slots = ir->get_state_slots();
   assert(slots != NULL);
d1005 1
a1005 1
   for (unsigned int i = 0; i < ir->get_num_state_slots(); i++) {
d1009 1
a1009 1
      int index = _mesa_add_state_reference(this->prog->Parameters,
d1024 1
a1024 1
            &prog->Parameters->ParameterValues[index][swiz];
a1031 2
   assert(stage == MESA_SHADER_FRAGMENT);
   brw_wm_prog_key *key = (brw_wm_prog_key*) this->key;
d1034 1
a1034 1
   bool flip = !ir->data.origin_upper_left ^ key->render_to_fbo;
d1042 1
a1042 1
   wpos = offset(wpos, 1);
d1053 1
a1053 1
	 offset += key->drawable_height - 1.0;
d1058 1
a1058 1
   wpos = offset(wpos, 1);
d1062 1
a1062 1
      emit(MOV(wpos, fs_reg(brw_vec8_grf(payload.source_depth_reg, 0))));
d1069 1
a1069 1
   wpos = offset(wpos, 1);
a1118 4
   assert(stage == MESA_SHADER_FRAGMENT);
   brw_wm_prog_data *prog_data = (brw_wm_prog_data*) this->prog_data;
   brw_wm_prog_key *key = (brw_wm_prog_key*) this->key;

d1134 1
a1134 1
      ir->determine_interpolation_mode(key->flat_shade);
d1139 1
a1139 1
	 if (prog_data->urb_setup[location] == -1) {
d1143 1
a1143 1
	    attr = offset(attr, type->vector_elements);
d1158 1
a1158 1
	       attr = offset(attr, 1);
d1164 3
d1174 3
a1176 4

                  fs_inst *inst;
                  inst = emit_linterp(attr, fs_reg(interp), interpolation_mode,
                                      false, false);
a1178 15
                  if (brw->has_pln)
                     inst->no_dd_clear = true;

                  inst = emit_linterp(attr, fs_reg(interp), interpolation_mode,
                                      ir->data.centroid && !key->persample_shading,
                                      ir->data.sample || key->persample_shading);
                  inst->predicate = BRW_PREDICATE_NORMAL;
                  inst->predicate_inverse = false;
                  if (brw->has_pln)
                     inst->no_dd_check = true;

               } else {
                  emit_linterp(attr, fs_reg(interp), interpolation_mode,
                               ir->data.centroid && !key->persample_shading,
                               ir->data.sample || key->persample_shading);
d1183 1
a1183 1
	       attr = offset(attr, 1);
d1195 1
a1195 1
fs_visitor::emit_frontfacing_interpolation()
d1197 1
a1197 1
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, glsl_type::bool_type);
d1199 1
d1201 5
a1205 15
      /* Bit 15 of g0.0 is 0 if the polygon is front facing. We want to create
       * a boolean result from this (~0/true or 0/false).
       *
       * We can use the fact that bit 15 is the MSB of g0.0:W to accomplish
       * this task in only one instruction:
       *    - a negation source modifier will flip the bit; and
       *    - a W -> D type conversion will sign extend the bit into the high
       *      word of the destination.
       *
       * An ASR 15 fills the low word of the destination.
       */
      fs_reg g0 = fs_reg(retype(brw_vec1_grf(0, 0), BRW_REGISTER_TYPE_W));
      g0.negate = true;

      emit(ASR(*reg, g0, fs_reg(15)));
d1207 3
a1209 10
      /* Bit 31 of g1.6 is 0 if the polygon is front facing. We want to create
       * a boolean result from this (1/true or 0/false).
       *
       * Like in the above case, since the bit is the MSB of g1.6:UD we can use
       * the negation source modifier to flip it. Unfortunately the SHR
       * instruction only operates on UD (or D with an abs source modifier)
       * sources without negation.
       *
       * Instead, use ASR (which will give ~0/true or 0/false) followed by an
       * AND 1.
d1211 2
a1212 6
      fs_reg asr = fs_reg(this, glsl_type::bool_type);
      fs_reg g1_6 = fs_reg(retype(brw_vec1_grf(1, 6), BRW_REGISTER_TYPE_D));
      g1_6.negate = true;

      emit(ASR(asr, g1_6, fs_reg(31)));
      emit(AND(*reg, asr, fs_reg(1)));
a1220 2
   assert(stage == MESA_SHADER_FRAGMENT);
   brw_wm_prog_key *key = (brw_wm_prog_key*) this->key;
d1223 1
a1223 1
   if (key->compute_pos_offset) {
d1240 1
a1240 1
fs_visitor::emit_samplepos_setup()
d1243 1
d1246 1
a1246 1
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, glsl_type::vec2_type);
d1263 1
a1263 1
      stride(retype(brw_vec1_grf(payload.sample_pos_reg, 0),
d1266 6
a1271 6
   if (dispatch_width == 8) {
      emit(MOV(int_sample_x, fs_reg(sample_pos_reg)));
   } else {
      emit(MOV(half(int_sample_x, 0), fs_reg(sample_pos_reg)));
      emit(MOV(half(int_sample_x, 1), fs_reg(suboffset(sample_pos_reg, 16))))
         ->force_sechalf = true;
d1275 7
a1281 8
   pos = offset(pos, 1);
   if (dispatch_width == 8) {
      emit(MOV(int_sample_y, fs_reg(suboffset(sample_pos_reg, 1))));
   } else {
      emit(MOV(half(int_sample_y, 0),
               fs_reg(suboffset(sample_pos_reg, 1))));
      emit(MOV(half(int_sample_y, 1), fs_reg(suboffset(sample_pos_reg, 17))))
         ->force_sechalf = true;
d1289 1
a1289 1
fs_visitor::emit_sampleid_setup()
a1290 2
   assert(stage == MESA_SHADER_FRAGMENT);
   brw_wm_prog_key *key = (brw_wm_prog_key*) this->key;
d1294 1
a1294 1
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, glsl_type::int_type);
d1296 1
a1296 1
   if (key->compute_sample_id) {
d1328 1
a1328 1
      inst = emit(MOV(t2, brw_imm_v(key->persample_2x ? 0x1010 : 0x3210)));
d1345 10
d1395 2
a1396 1
      unreachable("not reached: bad math opcode");
d1426 13
d1460 1
a1460 1
      emit(MOV(fs_reg(MRF, base_mrf + 1, op1.type, dispatch_width), op1));
d1473 1
a1473 1
      prog_data->dispatch_grf_start_reg = payload.num_regs;
d1475 1
a1475 3
      assert(stage == MESA_SHADER_FRAGMENT);
      brw_wm_prog_data *prog_data = (brw_wm_prog_data*) this->prog_data;
      prog_data->dispatch_grf_start_reg_16 = payload.num_regs;
d1478 1
a1478 1
   prog_data->curb_read_length = ALIGN(stage_prog_data->nr_params, 8) / 8;
d1481 4
a1484 2
   foreach_block_and_inst(block, fs_inst, inst, cfg) {
      for (unsigned int i = 0; i < inst->sources; i++) {
d1499 1
a1499 1
	    struct brw_reg brw_reg = brw_vec1_grf(payload.num_regs +
d1515 3
a1517 6
   assert(stage == MESA_SHADER_FRAGMENT);
   brw_wm_prog_data *prog_data = (brw_wm_prog_data*) this->prog_data;
   brw_wm_prog_key *key = (brw_wm_prog_key*) this->key;

   memset(prog_data->urb_setup, -1,
          sizeof(prog_data->urb_setup[0]) * VARYING_SLOT_MAX);
d1522 1
a1522 1
      if (_mesa_bitcount_64(prog->InputsRead &
d1534 1
a1534 1
            if (prog->InputsRead & BRW_FS_VARYING_INPUT_MASK &
d1536 1
a1536 1
               prog_data->urb_setup[i] = urb_next++;
d1547 1
a1547 1
                             key->input_slots_valid);
d1557 1
a1557 1
                (prog->InputsRead & BRW_FS_VARYING_INPUT_MASK &
d1559 1
a1559 1
               prog_data->urb_setup[varying] = slot - first_slot;
d1571 1
a1571 1
	 if (key->input_slots_valid & BITFIELD64_BIT(i)) {
d1579 1
a1579 1
	       prog_data->urb_setup[i] = urb_next;
d1590 2
a1591 2
      if (prog->InputsRead & BITFIELD64_BIT(VARYING_SLOT_PNTC))
         prog_data->urb_setup[VARYING_SLOT_PNTC] = urb_next++;
d1594 1
a1594 1
   prog_data->num_varying_inputs = urb_next;
d1600 1
a1600 4
   assert(stage == MESA_SHADER_FRAGMENT);
   brw_wm_prog_data *prog_data = (brw_wm_prog_data*) this->prog_data;

   int urb_start = payload.num_regs + prog_data->base.curb_read_length;
d1605 3
a1607 1
   foreach_block_and_inst(block, fs_inst, inst, cfg) {
d1621 1
a1621 1
      urb_start + prog_data->num_varying_inputs * 2;
d1646 2
d1649 1
a1649 3
   /* Count the total number of registers */
   int reg_count = 0;
   int vgrf_to_reg[num_vars];
d1651 4
a1654 28
      vgrf_to_reg[i] = reg_count;
      reg_count += virtual_grf_sizes[i];
   }

   /* An array of "split points".  For each register slot, this indicates
    * if this slot can be separated from the previous slot.  Every time an
    * instruction uses multiple elements of a register (as a source or
    * destination), we mark the used slots as inseparable.  Then we go
    * through and split the registers into the smallest pieces we can.
    */
   bool split_points[reg_count];
   memset(split_points, 0, sizeof(split_points));

   /* Mark all used registers as fully splittable */
   foreach_block_and_inst(block, fs_inst, inst, cfg) {
      if (inst->dst.file == GRF) {
         int reg = vgrf_to_reg[inst->dst.reg];
         for (int j = 1; j < this->virtual_grf_sizes[inst->dst.reg]; j++)
            split_points[reg + j] = true;
      }

      for (int i = 0; i < inst->sources; i++) {
         if (inst->src[i].file == GRF) {
            int reg = vgrf_to_reg[inst->src[i].reg];
            for (int j = 1; j < this->virtual_grf_sizes[inst->src[i].reg]; j++)
               split_points[reg + j] = true;
         }
      }
d1664 2
a1665 2
      int vgrf = this->delta_x[BRW_WM_PERSPECTIVE_PIXEL_BARYCENTRIC].reg;
      split_points[vgrf_to_reg[vgrf] + 1] = false;
d1668 8
a1675 5
   foreach_block_and_inst(block, fs_inst, inst, cfg) {
      if (inst->dst.file == GRF) {
         int reg = vgrf_to_reg[inst->dst.reg] + inst->dst.reg_offset;
         for (int j = 1; j < inst->regs_written; j++)
            split_points[reg + j] = false;
d1677 9
a1685 5
      for (int i = 0; i < inst->sources; i++) {
         if (inst->src[i].file == GRF) {
            int reg = vgrf_to_reg[inst->src[i].reg] + inst->src[i].reg_offset;
            for (int j = 1; j < inst->regs_read(this, i); j++)
               split_points[reg + j] = false;
d1690 3
a1692 4
   int new_virtual_grf[reg_count];
   int new_reg_offset[reg_count];

   int reg = 0;
d1694 10
a1703 2
      /* The first one should always be 0 as a quick sanity check. */
      assert(split_points[reg] == false);
d1705 2
a1706 29
      /* j = 0 case */
      new_reg_offset[reg] = 0;
      reg++;
      int offset = 1;

      /* j > 0 case */
      for (int j = 1; j < virtual_grf_sizes[i]; j++) {
         /* If this is a split point, reset the offset to 0 and allocate a
          * new virtual GRF for the previous offset many registers
          */
         if (split_points[reg]) {
            assert(offset <= MAX_VGRF_SIZE);
            int grf = virtual_grf_alloc(offset);
            for (int k = reg - offset; k < reg; k++)
               new_virtual_grf[k] = grf;
            offset = 0;
         }
         new_reg_offset[reg] = offset;
         offset++;
         reg++;
      }

      /* The last one gets the original register number */
      assert(offset <= MAX_VGRF_SIZE);
      virtual_grf_sizes[i] = offset;
      for (int k = reg - offset; k < reg; k++)
         new_virtual_grf[k] = i;
   }
   assert(reg == reg_count);
d1708 15
a1722 14
   foreach_block_and_inst(block, fs_inst, inst, cfg) {
      if (inst->dst.file == GRF) {
         reg = vgrf_to_reg[inst->dst.reg] + inst->dst.reg_offset;
         inst->dst.reg = new_virtual_grf[reg];
         inst->dst.reg_offset = new_reg_offset[reg];
         assert(new_reg_offset[reg] < virtual_grf_sizes[new_virtual_grf[reg]]);
      }
      for (int i = 0; i < inst->sources; i++) {
	 if (inst->src[i].file == GRF) {
            reg = vgrf_to_reg[inst->src[i].reg] + inst->src[i].reg_offset;
            inst->src[i].reg = new_virtual_grf[reg];
            inst->src[i].reg_offset = new_reg_offset[reg];
            assert(new_reg_offset[reg] < virtual_grf_sizes[new_virtual_grf[reg]]);
         }
d1737 1
a1737 1
bool
d1740 1
a1740 1
   bool progress = false;
d1744 3
a1746 2
   /* Mark which virtual GRFs are used. */
   foreach_block_and_inst(block, const fs_inst, inst, cfg) {
d1750 1
a1750 1
      for (int i = 0; i < inst->sources; i++) {
d1756 28
d1787 1
a1787 6
      if (remap_table[i] == -1) {
         /* We just found an unused register.  This means that we are
          * actually going to compact something.
          */
         progress = true;
      } else {
d1798 3
a1800 1
   foreach_block_and_inst(block, fs_inst, inst, cfg) {
d1804 1
a1804 1
      for (int i = 0; i < inst->sources; i++) {
d1810 6
a1815 11
   /* Patch all the references to delta_x/delta_y, since they're used in
    * register allocation.  If they're unused, switch them to BAD_FILE so
    * we don't think some random VGRF is delta_x/delta_y.
    */
   for (unsigned i = 0; i < ARRAY_SIZE(delta_x); i++) {
      if (delta_x[i].file == GRF) {
         if (remap_table[delta_x[i].reg] != -1) {
            delta_x[i].reg = remap_table[delta_x[i].reg];
         } else {
            delta_x[i].file = BAD_FILE;
         }
a1817 11
   for (unsigned i = 0; i < ARRAY_SIZE(delta_y); i++) {
      if (delta_y[i].file == GRF) {
         if (remap_table[delta_y[i].reg] != -1) {
            delta_y[i].reg = remap_table[delta_y[i].reg];
         } else {
            delta_y[i].file = BAD_FILE;
         }
      }
   }

   return progress;
d1839 4
a1842 1
   memset(pull_constant_loc, -1, sizeof(pull_constant_loc[0]) * uniforms);
d1850 4
a1853 2
   foreach_block_and_inst_safe(block, fs_inst, inst, cfg) {
      for (int i = 0 ; i < inst->sources; i++) {
d1863 1
a1863 1
            const gl_constant_value **values = &stage_prog_data->param[uniform];
d1900 4
a1903 2
   foreach_block_and_inst(block, fs_inst, inst, cfg) {
      for (int i = 0; i < inst->sources; i++) {
a1916 3
    *
    * If changing this value, note the limitation about total_regs in
    * brw_curbe.c.
d1971 4
a1974 2
   foreach_block_and_inst (block, fs_inst, inst, cfg) {
      for (int i = 0; i < inst->sources; i++) {
d1996 1
a1996 1
            inst->insert_before(block, &list);
d2001 1
a2001 1
               new(mem_ctx) fs_inst(FS_OPCODE_UNIFORM_PULL_CONSTANT_LOAD, 8,
d2003 1
a2003 1
            inst->insert_before(block, pull);
a2010 1
         inst->src[i].width = dispatch_width;
d2021 3
a2023 1
   foreach_block_and_inst(block, fs_inst, inst, cfg) {
d2078 1
a2078 7
         if (inst->src[0].equals(inst->src[1])) {
            inst->opcode = BRW_OPCODE_MOV;
            inst->src[1] = reg_undef;
            inst->predicate = BRW_PREDICATE_NONE;
            inst->predicate_inverse = false;
            progress = true;
         } else if (inst->saturate && inst->src[1].file == IMM) {
d2084 1
a2084 1
                  if (inst->src[1].fixed_hw_reg.dw1.f >= 1.0f) {
d2098 1
a2098 1
                  if (inst->src[1].fixed_hw_reg.dw1.f <= 0.0f) {
a2112 11
      case SHADER_OPCODE_RCP: {
         fs_inst *prev = (fs_inst *)inst->prev;
         if (prev->opcode == SHADER_OPCODE_SQRT) {
            if (inst->src[0].equals(prev->dst)) {
               inst->opcode = SHADER_OPCODE_RSQ;
               inst->src[0] = prev->src[0];
               progress = true;
            }
         }
         break;
      }
a2121 66
fs_visitor::opt_register_renaming()
{
   bool progress = false;
   int depth = 0;

   int remap[virtual_grf_count];
   memset(remap, -1, sizeof(int) * virtual_grf_count);

   foreach_block_and_inst(block, fs_inst, inst, cfg) {
      if (inst->opcode == BRW_OPCODE_IF || inst->opcode == BRW_OPCODE_DO) {
         depth++;
      } else if (inst->opcode == BRW_OPCODE_ENDIF ||
                 inst->opcode == BRW_OPCODE_WHILE) {
         depth--;
      }

      /* Rewrite instruction sources. */
      for (int i = 0; i < inst->sources; i++) {
         if (inst->src[i].file == GRF &&
             remap[inst->src[i].reg] != -1 &&
             remap[inst->src[i].reg] != inst->src[i].reg) {
            inst->src[i].reg = remap[inst->src[i].reg];
            progress = true;
         }
      }

      const int dst = inst->dst.reg;

      if (depth == 0 &&
          inst->dst.file == GRF &&
          virtual_grf_sizes[inst->dst.reg] == inst->dst.width / 8 &&
          !inst->is_partial_write()) {
         if (remap[dst] == -1) {
            remap[dst] = dst;
         } else {
            remap[dst] = virtual_grf_alloc(inst->dst.width / 8);
            inst->dst.reg = remap[dst];
            progress = true;
         }
      } else if (inst->dst.file == GRF &&
                 remap[dst] != -1 &&
                 remap[dst] != dst) {
         inst->dst.reg = remap[dst];
         progress = true;
      }
   }

   if (progress) {
      invalidate_live_intervals();

      for (unsigned i = 0; i < ARRAY_SIZE(delta_x); i++) {
         if (delta_x[i].file == GRF && remap[delta_x[i].reg] != -1) {
            delta_x[i].reg = remap[delta_x[i].reg];
         }
      }
      for (unsigned i = 0; i < ARRAY_SIZE(delta_y); i++) {
         if (delta_y[i].file == GRF && remap[delta_y[i].reg] != -1) {
            delta_y[i].reg = remap[delta_y[i].reg];
         }
      }
   }

   return progress;
}

bool
d2127 1
a2127 3
   /* No MRFs on Gen >= 7. */
   if (brw->gen >= 7)
      return false;
d2129 2
a2130 1
   calculate_live_intervals();
a2131 1
   foreach_block_and_inst_safe(block, fs_inst, inst, cfg) {
d2151 2
a2152 1
      } else if (inst->exec_size == 16) {
d2167 4
a2170 1
      foreach_inst_in_block_reverse_starting_from(fs_inst, scan_inst, inst, block) {
d2188 1
a2188 1
            if (scan_inst->regs_written > scan_inst->dst.width / 8)
d2209 1
a2209 1
	       inst->remove(block);
d2219 1
a2219 1
	 if (block->start() == scan_inst)
d2226 1
a2226 1
	 for (int i = 0; i < scan_inst->sources; i++) {
d2245 3
a2247 1
	    } else if (scan_inst->exec_size == 16) {
a2285 46
 * Once we've generated code, try to convert normal FS_OPCODE_FB_WRITE
 * instructions to FS_OPCODE_REP_FB_WRITE.
 */
void
fs_visitor::emit_repclear_shader()
{
   brw_wm_prog_key *key = (brw_wm_prog_key*) this->key;
   int base_mrf = 1;
   int color_mrf = base_mrf + 2;

   fs_inst *mov = emit(MOV(vec4(brw_message_reg(color_mrf)),
                           fs_reg(UNIFORM, 0, BRW_REGISTER_TYPE_F)));
   mov->force_writemask_all = true;

   fs_inst *write;
   if (key->nr_color_regions == 1) {
      write = emit(FS_OPCODE_REP_FB_WRITE);
      write->saturate = key->clamp_fragment_color;
      write->base_mrf = color_mrf;
      write->target = 0;
      write->header_present = false;
      write->mlen = 1;
   } else {
      assume(key->nr_color_regions > 0);
      for (int i = 0; i < key->nr_color_regions; ++i) {
         write = emit(FS_OPCODE_REP_FB_WRITE);
         write->saturate = key->clamp_fragment_color;
         write->base_mrf = base_mrf;
         write->target = i;
         write->header_present = true;
         write->mlen = 3;
      }
   }
   write->eot = true;

   calculate_cfg();

   assign_constant_locations();
   assign_curb_setup();

   /* Now that we have the uniform assigned, go ahead and force it to a vec4. */
   assert(mov->src[0].file == HW_REG);
   mov->src[0] = brw_vec4_grf(mov->src[0].fixed_hw_reg.nr, 0);
}

/**
d2301 3
a2303 1
   foreach_block_and_inst_safe (block, fs_inst, inst, cfg) {
d2312 1
a2312 1
	    inst->remove(block);
d2360 4
d2365 1
a2365 1
   for (int i = 0; i < inst->sources; i++) {
d2379 1
a2379 1
         if (inst->exec_size == 16)
d2402 1
a2402 2
fs_visitor::insert_gen4_pre_send_dependency_workarounds(bblock_t *block,
                                                        fs_inst *inst)
d2404 2
a2405 1
   int write_len = inst->regs_written;
d2421 4
a2424 1
   foreach_inst_in_block_reverse_starting_from(fs_inst, scan_inst, inst, block) {
d2428 1
a2428 1
      if (block->start() == scan_inst) {
d2431 1
a2431 1
               inst->insert_before(block, DEP_RESOLVE_MOV(first_write_grf + i));
d2437 4
d2447 1
a2447 1
            int reg = scan_inst->dst.reg + i;
d2452 1
a2452 1
               inst->insert_before(block, DEP_RESOLVE_MOV(reg));
d2454 1
a2454 1
               if (scan_inst->exec_size == 16)
d2483 1
a2483 1
fs_visitor::insert_gen4_post_send_dependency_workarounds(bblock_t *block, fs_inst *inst)
d2485 1
a2485 1
   int write_len = inst->regs_written;
d2495 3
a2497 1
   foreach_inst_in_block_starting_from(fs_inst, scan_inst, inst, block) {
d2499 1
a2499 1
      if (block->end() == scan_inst) {
d2502 1
a2502 2
               scan_inst->insert_before(block,
                                        DEP_RESOLVE_MOV(first_write_grf + i));
d2518 1
a2518 1
         scan_inst->insert_before(block, DEP_RESOLVE_MOV(scan_inst->dst.reg));
d2539 1
a2539 1
         last_inst->insert_before(block, DEP_RESOLVE_MOV(first_write_grf + i));
d2555 3
a2557 1
   foreach_block_and_inst(block, fs_inst, inst, cfg) {
d2559 2
a2560 2
         insert_gen4_pre_send_dependency_workarounds(block, inst);
         insert_gen4_post_send_dependency_workarounds(block, inst);
d2588 3
a2590 1
   foreach_block_and_inst (block, fs_inst, inst, cfg) {
d2601 1
a2601 1
         const_offset_reg.fixed_hw_reg.dw1.ud /= 4;
d2610 1
a2610 1
                                               8, payload, const_offset_reg);
d2615 1
a2615 1
         inst->insert_before(block, setup);
a2636 105
bool
fs_visitor::lower_load_payload()
{
   bool progress = false;

   int vgrf_to_reg[virtual_grf_count];
   int reg_count = 16; /* Leave room for MRF */
   for (int i = 0; i < virtual_grf_count; ++i) {
      vgrf_to_reg[i] = reg_count;
      reg_count += virtual_grf_sizes[i];
   }

   struct {
      bool written:1; /* Whether this register has ever been written */
      bool force_writemask_all:1;
      bool force_sechalf:1;
   } metadata[reg_count];
   memset(metadata, 0, sizeof(metadata));

   foreach_block_and_inst_safe (block, fs_inst, inst, cfg) {
      int dst_reg;
      if (inst->dst.file == GRF) {
         dst_reg = vgrf_to_reg[inst->dst.reg];
      } else {
         /* MRF */
         dst_reg = inst->dst.reg;
      }

      if (inst->dst.file == MRF || inst->dst.file == GRF) {
         bool force_sechalf = inst->force_sechalf;
         bool toggle_sechalf = inst->dst.width == 16 &&
                               type_sz(inst->dst.type) == 4;
         for (int i = 0; i < inst->regs_written; ++i) {
            metadata[dst_reg + i].written = true;
            metadata[dst_reg + i].force_sechalf = force_sechalf;
            metadata[dst_reg + i].force_writemask_all = inst->force_writemask_all;
            force_sechalf = (toggle_sechalf != force_sechalf);
         }
      }

      if (inst->opcode == SHADER_OPCODE_LOAD_PAYLOAD) {
         assert(inst->dst.file == MRF || inst->dst.file == GRF);
         fs_reg dst = inst->dst;

         for (int i = 0; i < inst->sources; i++) {
            dst.width = inst->src[i].effective_width;
            dst.type = inst->src[i].type;

            if (inst->src[i].file == BAD_FILE) {
               /* Do nothing but otherwise increment as normal */
            } else if (dst.file == MRF &&
                       dst.width == 8 &&
                       brw->has_compr4 &&
                       i + 4 < inst->sources &&
                       inst->src[i + 4].equals(horiz_offset(inst->src[i], 8))) {
               fs_reg compr4_dst = dst;
               compr4_dst.reg += BRW_MRF_COMPR4;
               compr4_dst.width = 16;
               fs_reg compr4_src = inst->src[i];
               compr4_src.width = 16;
               fs_inst *mov = MOV(compr4_dst, compr4_src);
               mov->force_writemask_all = true;
               inst->insert_before(block, mov);
               /* Mark i+4 as BAD_FILE so we don't emit a MOV for it */
               inst->src[i + 4].file = BAD_FILE;
            } else {
               fs_inst *mov = MOV(dst, inst->src[i]);
               if (inst->src[i].file == GRF) {
                  int src_reg = vgrf_to_reg[inst->src[i].reg] +
                                inst->src[i].reg_offset;
                  mov->force_sechalf = metadata[src_reg].force_sechalf;
                  mov->force_writemask_all = metadata[src_reg].force_writemask_all;
                  metadata[dst_reg] = metadata[src_reg];
                  if (dst.width * type_sz(dst.type) > 32) {
                     assert((!metadata[src_reg].written ||
                             !metadata[src_reg].force_sechalf) &&
                            (!metadata[src_reg + 1].written ||
                             metadata[src_reg + 1].force_sechalf));
                     metadata[dst_reg + 1] = metadata[src_reg + 1];
                  }
               } else {
                  metadata[dst_reg].force_writemask_all = false;
                  metadata[dst_reg].force_sechalf = false;
                  if (dst.width == 16) {
                     metadata[dst_reg + 1].force_writemask_all = false;
                     metadata[dst_reg + 1].force_sechalf = true;
                  }
               }
               inst->insert_before(block, mov);
            }

            dst = offset(dst, 1);
         }

         inst->remove(block);
         progress = true;
      }
   }

   if (progress)
      invalidate_live_intervals();

   return progress;
}

a2639 6
   dump_instructions(NULL);
}

void
fs_visitor::dump_instructions(const char *name)
{
a2640 6
   FILE *file = stderr;
   if (name && geteuid() != 0) {
      file = fopen(name, "w");
      if (!file)
         file = stderr;
   }
d2643 2
a2644 1
   foreach_block_and_inst(block, backend_instruction, inst, cfg) {
d2646 2
a2647 2
      fprintf(file, "{%3d} %4d: ", regs_live_at_ip[ip], ip);
      dump_instruction(inst, file);
d2650 1
a2650 5
   fprintf(file, "Maximum %3d registers live at once.\n", max_pressure);

   if (file != stderr) {
      fclose(file);
   }
a2655 6
   dump_instruction(be_inst, stderr);
}

void
fs_visitor::dump_instruction(backend_instruction *be_inst, FILE *file)
{
d2659 1
a2659 1
      fprintf(file, "(%cf0.%d) ",
d2664 1
a2664 1
   fprintf(file, "%s", brw_instruction_name(inst->opcode));
d2666 1
a2666 1
      fprintf(file, ".sat");
d2668 1
a2668 1
      fprintf(file, "%s", conditional_modifier[inst->conditional_mod]);
d2673 1
a2673 1
         fprintf(file, ".f0.%d", inst->flag_subreg);
d2676 1
a2676 1
   fprintf(file, "(%d) ", inst->exec_size);
d2681 2
a2682 4
      fprintf(file, "vgrf%d", inst->dst.reg);
      if (inst->dst.width != dispatch_width)
         fprintf(file, "@@%d", inst->dst.width);
      if (virtual_grf_sizes[inst->dst.reg] != inst->dst.width / 8 ||
d2684 1
a2684 1
         fprintf(file, "+%d.%d",
d2688 1
a2688 1
      fprintf(file, "m%d", inst->dst.reg);
d2691 1
a2691 1
      fprintf(file, "(null)");
d2694 1
a2694 1
      fprintf(file, "***u%d***", inst->dst.reg + inst->dst.reg_offset);
d2700 1
a2700 1
            fprintf(file, "null");
d2703 1
a2703 1
            fprintf(file, "a0.%d", inst->dst.fixed_hw_reg.subnr);
d2706 1
a2706 1
            fprintf(file, "acc%d", inst->dst.fixed_hw_reg.subnr);
d2709 1
a2709 1
            fprintf(file, "f%d.%d", inst->dst.fixed_hw_reg.nr & 0xf,
d2713 1
a2713 1
            fprintf(file, "arf%d.%d", inst->dst.fixed_hw_reg.nr & 0xf,
d2718 1
a2718 1
         fprintf(file, "hw_reg%d", inst->dst.fixed_hw_reg.nr);
d2721 1
a2721 1
         fprintf(file, "+%d", inst->dst.fixed_hw_reg.subnr);
d2724 1
a2724 1
      fprintf(file, "???");
d2727 1
a2727 1
   fprintf(file, ":%s, ", brw_reg_type_letters(inst->dst.type));
d2729 1
a2729 1
   for (int i = 0; i < inst->sources; i++) {
d2731 1
a2731 1
         fprintf(file, "-");
d2733 1
a2733 1
         fprintf(file, "|");
d2736 2
a2737 4
         fprintf(file, "vgrf%d", inst->src[i].reg);
         if (inst->src[i].width != dispatch_width)
            fprintf(file, "@@%d", inst->src[i].width);
         if (virtual_grf_sizes[inst->src[i].reg] != inst->src[i].width / 8 ||
d2739 1
a2739 1
            fprintf(file, "+%d.%d", inst->src[i].reg_offset,
d2743 1
a2743 1
         fprintf(file, "***m%d***", inst->src[i].reg);
d2746 1
a2746 1
         fprintf(file, "u%d", inst->src[i].reg + inst->src[i].reg_offset);
d2748 4
a2751 3
            fprintf(file, "+reladdr");
         } else if (inst->src[i].subreg_offset) {
            fprintf(file, "+%d.%d", inst->src[i].reg_offset,
d2756 1
a2756 1
         fprintf(file, "(null)");
d2761 1
a2761 1
            fprintf(file, "%ff", inst->src[i].fixed_hw_reg.dw1.f);
d2764 1
a2764 1
            fprintf(file, "%dd", inst->src[i].fixed_hw_reg.dw1.d);
d2767 1
a2767 1
            fprintf(file, "%uu", inst->src[i].fixed_hw_reg.dw1.ud);
d2770 1
a2770 1
            fprintf(file, "???");
d2776 1
a2776 1
            fprintf(file, "-");
d2778 1
a2778 1
            fprintf(file, "|");
d2782 1
a2782 1
               fprintf(file, "null");
d2785 1
a2785 1
               fprintf(file, "a0.%d", inst->src[i].fixed_hw_reg.subnr);
d2788 1
a2788 1
               fprintf(file, "acc%d", inst->src[i].fixed_hw_reg.subnr);
d2791 1
a2791 1
               fprintf(file, "f%d.%d", inst->src[i].fixed_hw_reg.nr & 0xf,
d2795 1
a2795 1
               fprintf(file, "arf%d.%d", inst->src[i].fixed_hw_reg.nr & 0xf,
d2800 1
a2800 1
            fprintf(file, "hw_reg%d", inst->src[i].fixed_hw_reg.nr);
d2803 1
a2803 1
            fprintf(file, "+%d", inst->src[i].fixed_hw_reg.subnr);
d2805 1
a2805 1
            fprintf(file, "|");
d2808 1
a2808 1
         fprintf(file, "???");
d2812 1
a2812 1
         fprintf(file, "|");
d2815 1
a2815 1
         fprintf(file, ":%s", brw_reg_type_letters(inst->src[i].type));
d2818 2
a2819 2
      if (i < inst->sources - 1 && inst->src[i + 1].file != BAD_FILE)
         fprintf(file, ", ");
d2822 1
a2822 1
   fprintf(file, " ");
d2824 5
a2828 6
   if (dispatch_width == 16 && inst->exec_size == 8) {
      if (inst->force_sechalf)
         fprintf(file, "2ndhalf ");
      else
         fprintf(file, "1sthalf ");
   }
d2830 1
a2830 1
   fprintf(file, "\n");
d2865 2
a2866 4
      (prog->InputsRead & (1 << VARYING_SLOT_POS)) != 0;
   unsigned barycentric_interp_modes =
      (stage == MESA_SHADER_FRAGMENT) ?
      ((brw_wm_prog_data*) this->prog_data)->barycentric_interp_modes : 0;
d2871 1
a2871 1
   payload.num_regs = 2;
d2883 2
a2884 2
         payload.barycentric_coord_reg[i] = payload.num_regs;
         payload.num_regs += 2;
d2886 1
a2886 1
            payload.num_regs += 2;
d2893 2
a2894 2
      payload.source_depth_reg = payload.num_regs;
      payload.num_regs++;
d2897 1
a2897 1
         payload.num_regs++;
d2902 2
a2903 2
      payload.source_w_reg = payload.num_regs;
      payload.num_regs++;
d2906 1
a2906 1
         payload.num_regs++;
d2910 5
a2914 9
   if (stage == MESA_SHADER_FRAGMENT) {
      brw_wm_prog_data *prog_data = (brw_wm_prog_data*) this->prog_data;
      brw_wm_prog_key *key = (brw_wm_prog_key*) this->key;
      prog_data->uses_pos_offset = key->compute_pos_offset;
      /* R31: MSAA position offsets. */
      if (prog_data->uses_pos_offset) {
         payload.sample_pos_reg = payload.num_regs;
         payload.num_regs++;
      }
d2918 1
a2918 1
   if (prog->SystemValuesRead & SYSTEM_BIT_SAMPLE_MASK_IN) {
d2920 2
a2921 2
      payload.sample_mask_in_reg = payload.num_regs;
      payload.num_regs++;
d2924 1
a2924 1
         payload.num_regs++;
d2931 2
a2932 2
   if (prog->OutputsWritten & BITFIELD64_BIT(FRAG_RESULT_DEPTH)) {
      source_depth_to_render_target = true;
a2938 3
   assert(stage == MESA_SHADER_FRAGMENT);
   brw_wm_prog_data *prog_data = (brw_wm_prog_data*) this->prog_data;
   brw_wm_prog_key *key = (brw_wm_prog_key*) this->key;
d2944 2
a2945 2
   prog_data->binding_table.render_target_start = next_binding_table_offset;
   next_binding_table_offset += MAX2(key->nr_color_regions, 1);
d2956 4
a2959 3
   unsigned num_instructions = 0;
   foreach_block(block, cfg)
      num_instructions += block->instructions.length();
d2984 3
a2986 1
   foreach_block_and_inst_safe(block, fs_inst, inst, cfg) {
d2993 1
a2993 1
            inst->remove(block);
a2999 137
void
fs_visitor::optimize()
{
   calculate_cfg();

   split_virtual_grfs();

   move_uniform_array_access_to_pull_constants();
   assign_constant_locations();
   demote_pull_constants();

   opt_drop_redundant_mov_to_flags();

#define OPT(pass, args...) do {                                         \
      pass_num++;                                                       \
      bool this_progress = pass(args);                                  \
                                                                        \
      if (unlikely(INTEL_DEBUG & DEBUG_OPTIMIZER) && this_progress) {   \
         char filename[64];                                             \
         snprintf(filename, 64, "fs%d-%04d-%02d-%02d-" #pass,           \
                  dispatch_width, shader_prog ? shader_prog->Name : 0, iteration, pass_num); \
                                                                        \
         backend_visitor::dump_instructions(filename);                  \
      }                                                                 \
                                                                        \
      progress = progress || this_progress;                             \
   } while (false)

   if (unlikely(INTEL_DEBUG & DEBUG_OPTIMIZER)) {
      char filename[64];
      snprintf(filename, 64, "fs%d-%04d-00-start",
               dispatch_width, shader_prog ? shader_prog->Name : 0);

      backend_visitor::dump_instructions(filename);
   }

   bool progress;
   int iteration = 0;
   do {
      progress = false;
      iteration++;
      int pass_num = 0;

      OPT(remove_duplicate_mrf_writes);

      OPT(opt_algebraic);
      OPT(opt_cse);
      OPT(opt_copy_propagate);
      OPT(opt_peephole_predicated_break);
      OPT(dead_code_eliminate);
      OPT(opt_peephole_sel);
      OPT(dead_control_flow_eliminate, this);
      OPT(opt_register_renaming);
      OPT(opt_saturate_propagation);
      OPT(register_coalesce);
      OPT(compute_to_mrf);

      OPT(compact_virtual_grfs);
   } while (progress);

   if (lower_load_payload()) {
      split_virtual_grfs();
      register_coalesce();
      compute_to_mrf();
      dead_code_eliminate();
   }

   lower_uniform_pull_constant_loads();
}

void
fs_visitor::allocate_registers()
{
   bool allocated_without_spills;

   static enum instruction_scheduler_mode pre_modes[] = {
      SCHEDULE_PRE,
      SCHEDULE_PRE_NON_LIFO,
      SCHEDULE_PRE_LIFO,
   };

   /* Try each scheduling heuristic to see if it can successfully register
    * allocate without spilling.  They should be ordered by decreasing
    * performance but increasing likelihood of allocating.
    */
   for (unsigned i = 0; i < ARRAY_SIZE(pre_modes); i++) {
      schedule_instructions(pre_modes[i]);

      if (0) {
         assign_regs_trivial();
         allocated_without_spills = true;
      } else {
         allocated_without_spills = assign_regs(false);
      }
      if (allocated_without_spills)
         break;
   }

   if (!allocated_without_spills) {
      /* We assume that any spilling is worse than just dropping back to
       * SIMD8.  There's probably actually some intermediate point where
       * SIMD16 with a couple of spills is still better.
       */
      if (dispatch_width == 16) {
         fail("Failure to register allocate.  Reduce number of "
              "live scalar values to avoid this.");
      } else {
         perf_debug("Fragment shader triggered register spilling.  "
                    "Try reducing the number of live scalar values to "
                    "improve performance.\n");
      }

      /* Since we're out of heuristics, just go spill registers until we
       * get an allocation.
       */
      while (!assign_regs(true)) {
         if (failed)
            break;
      }
   }

   /* This must come after all optimization and register allocation, since
    * it inserts dead code that happens to have side effects, and it does
    * so based on the actual physical registers in use.
    */
   insert_gen4_send_dependency_workarounds();

   if (failed)
      return;

   if (!allocated_without_spills)
      schedule_instructions(SCHEDULE_POST);

   if (last_scratch > 0)
      prog_data->total_scratch = brw_get_scratch_size(last_scratch);
}

d3003 2
a3004 1
   sanity_param_count = prog->Parameters->NumParameters;
a3014 2
   } else if (brw->use_rep_send && dispatch_width == 16) {
      emit_repclear_shader();
d3020 1
a3020 1
      if (prog->InputsRead > 0) {
d3030 1
a3030 7
      bool uses_kill =
         (stage == MESA_SHADER_FRAGMENT) &&
         ((brw_wm_prog_data*) this->prog_data)->uses_kill;
      bool alpha_test_func =
         (stage == MESA_SHADER_FRAGMENT) &&
         ((brw_wm_prog_key*) this->key)->alpha_test_func;
      if (uses_kill || alpha_test_func) {
d3039 2
a3040 1
         foreach_in_list(ir_instruction, ir, shader->base.ir) {
d3054 1
a3054 1
      if (alpha_test_func)
d3059 29
a3087 1
      optimize();
d3092 32
a3123 1
      allocate_registers();
d3125 8
a3132 2
      if (failed)
         return false;
d3134 7
d3142 10
a3151 7
   if (stage == MESA_SHADER_FRAGMENT) {
      brw_wm_prog_data *prog_data = (brw_wm_prog_data*) this->prog_data;
      if (dispatch_width == 8)
         prog_data->reg_blocks = brw_register_blocks(grf_used);
      else
         prog_data->reg_blocks_16 = brw_register_blocks(grf_used);
   }
d3158 1
a3158 1
   assert(sanity_param_count == prog->Parameters->NumParameters);
d3164 1
a3164 4
brw_wm_fs_emit(struct brw_context *brw,
               void *mem_ctx,
               const struct brw_wm_prog_key *key,
               struct brw_wm_prog_data *prog_data,
d3183 1
a3183 1
      brw_dump_ir("fragment", prog, &shader->base, &fp->Base);
d3187 1
a3187 1
   fs_visitor v(brw, mem_ctx, key, prog_data, prog, fp, 8);
d3200 3
a3202 4
   cfg_t *simd16_cfg = NULL;
   fs_visitor v2(brw, mem_ctx, key, prog_data, prog, fp, 16);
   if (brw->gen >= 5 && likely(!(INTEL_DEBUG & DEBUG_NO16) ||
                               brw->use_rep_send)) {
d3210 1
a3210 1
            simd16_cfg = v2.cfg;
d3218 5
a3222 5
   cfg_t *simd8_cfg;
   int no_simd8 = (INTEL_DEBUG & DEBUG_NO8) || brw->no_simd8;
   if (no_simd8 && simd16_cfg) {
      simd8_cfg = NULL;
      prog_data->no_8 = true;
d3224 3
a3226 2
      simd8_cfg = v.cfg;
      prog_data->no_8 = false;
a3228 7
   fs_generator g(brw, mem_ctx, key, prog_data, prog, fp,
                  v.runtime_check_aads_emit, INTEL_DEBUG & DEBUG_WM);
   if (simd8_cfg)
      g.generate_code(simd8_cfg, 8);
   if (simd16_cfg)
      prog_data->prog_offset_16 = g.generate_code(simd16_cfg, 16);

d3231 1
a3231 1
         brw_wm_debug_recompile(brw, prog, key);
d3240 1
a3240 1
   return g.get_assembly(final_assembly_size);
d3274 2
@


1.7
log
@Merge Mesa 10.2.7
@
text
@d35 1
a35 1
#include "main/hash_table.h"
d41 1
a41 1
#include "program/register_allocate.h"
d49 1
d56 2
a57 1
fs_inst::init()
d60 57
a118 5
   this->dst = reg_undef;
   this->src[0] = reg_undef;
   this->src[1] = reg_undef;
   this->src[2] = reg_undef;

d120 15
a134 1
   this->regs_written = 1;
d141 38
a178 2
   init();
   this->opcode = BRW_OPCODE_NOP;
d181 2
a182 1
fs_inst::fs_inst(enum opcode opcode)
d184 4
a187 2
   init();
   this->opcode = opcode;
d190 2
a191 1
fs_inst::fs_inst(enum opcode opcode, fs_reg dst)
d193 6
a198 3
   init();
   this->opcode = opcode;
   this->dst = dst;
d200 8
a207 2
   if (dst.file == GRF)
      assert(dst.reg_offset >= 0);
d210 1
a210 1
fs_inst::fs_inst(enum opcode opcode, fs_reg dst, fs_reg src0)
d212 2
a213 4
   init();
   this->opcode = opcode;
   this->dst = dst;
   this->src[0] = src0;
d215 4
a218 4
   if (dst.file == GRF)
      assert(dst.reg_offset >= 0);
   if (src[0].file == GRF)
      assert(src[0].reg_offset >= 0);
d221 1
a221 1
fs_inst::fs_inst(enum opcode opcode, fs_reg dst, fs_reg src0, fs_reg src1)
d223 3
a225 5
   init();
   this->opcode = opcode;
   this->dst = dst;
   this->src[0] = src0;
   this->src[1] = src1;
d227 2
a228 6
   if (dst.file == GRF)
      assert(dst.reg_offset >= 0);
   if (src[0].file == GRF)
      assert(src[0].reg_offset >= 0);
   if (src[1].file == GRF)
      assert(src[1].reg_offset >= 0);
d231 2
a232 2
fs_inst::fs_inst(enum opcode opcode, fs_reg dst,
		 fs_reg src0, fs_reg src1, fs_reg src2)
d234 4
a237 15
   init();
   this->opcode = opcode;
   this->dst = dst;
   this->src[0] = src0;
   this->src[1] = src1;
   this->src[2] = src2;

   if (dst.file == GRF)
      assert(dst.reg_offset >= 0);
   if (src[0].file == GRF)
      assert(src[0].reg_offset >= 0);
   if (src[1].file == GRF)
      assert(src[1].reg_offset >= 0);
   if (src[2].file == GRF)
      assert(src[2].reg_offset >= 0);
d242 1
a242 1
   fs_visitor::op(fs_reg dst, fs_reg src0)                              \
d249 2
a250 1
   fs_visitor::op(fs_reg dst, fs_reg src0, fs_reg src1)                 \
d257 2
a258 1
   fs_visitor::op(fs_reg dst, fs_reg src0, fs_reg src1)                 \
d267 2
a268 1
   fs_visitor::op(fs_reg dst, fs_reg src0, fs_reg src1, fs_reg src2)    \
d304 1
a304 1
fs_visitor::IF(uint32_t predicate)
d306 1
a306 1
   fs_inst *inst = new(mem_ctx) fs_inst(BRW_OPCODE_IF);
d313 2
a314 1
fs_visitor::IF(fs_reg src0, fs_reg src1, uint32_t condition)
d317 1
a317 1
   fs_inst *inst = new(mem_ctx) fs_inst(BRW_OPCODE_IF,
d329 2
a330 1
fs_visitor::CMP(fs_reg dst, fs_reg src0, fs_reg src1, uint32_t condition)
d361 26
d409 1
a409 1
                              varying_offset, const_offset & ~3));
d412 1
a412 1
   if (brw->gen == 4 && dispatch_width == 8) {
d426 5
a430 1
   fs_reg vec4_result = fs_reg(GRF, virtual_grf_alloc(4 * scale), dst.type);
d432 1
a432 1
   inst->regs_written = 4 * scale;
d444 2
a445 2
   vec4_result.reg_offset += (const_offset & 3) * scale;
   instructions.push_tail(MOV(dst, vec4_result));
d465 1
a465 1
   inst->force_uncompressed = true;
a482 1
           sampler == inst->sampler &&
d487 1
d503 20
a522 5
   return (opcode == FS_OPCODE_VARYING_PULL_CONSTANT_LOAD_GEN7 ||
           opcode == SHADER_OPCODE_SHADER_TIME_ADD ||
           (opcode == FS_OPCODE_UNIFORM_PULL_CONSTANT_LOAD &&
            src[1].file == GRF) ||
           (is_tex() && src[0].file == GRF));
d526 1
a526 1
fs_visitor::can_do_source_mods(fs_inst *inst)
d528 1
a528 1
   if (brw->gen == 6 && inst->is_math())
d531 1
a531 1
   if (inst->is_send_from_grf())
d534 1
a534 1
   if (!inst->can_do_source_mods())
d560 2
a561 1
   this->imm.f = f;
d570 2
a571 1
   this->imm.i = i;
d580 2
a581 1
   this->imm.u = u;
d591 1
d605 3
a607 4
           memcmp(&fixed_hw_reg, &r.fixed_hw_reg,
                  sizeof(fixed_hw_reg)) == 0 &&
           stride == r.stride &&
           imm.u == r.imm.u);
a635 26
fs_reg::is_zero() const
{
   if (file != IMM)
      return false;

   return type == BRW_REGISTER_TYPE_F ? imm.f == 0.0 : imm.i == 0;
}

bool
fs_reg::is_one() const
{
   if (file != IMM)
      return false;

   return type == BRW_REGISTER_TYPE_F ? imm.f == 1.0 : imm.i == 1;
}

bool
fs_reg::is_null() const
{
   return file == HW_REG &&
          fixed_hw_reg.file == BRW_ARCHITECTURE_REGISTER_FILE &&
          fixed_hw_reg.nr == BRW_ARF_NULL;
}

bool
a640 8
bool
fs_reg::is_accumulator() const
{
   return file == HW_REG &&
          fixed_hw_reg.file == BRW_ARCHITECTURE_REGISTER_FILE &&
          fixed_hw_reg.nr == BRW_ARF_ACCUMULATOR;
}

d671 1
a671 2
      assert(!"not reached");
      break;
d682 1
a682 1
   fs_reg ts = fs_reg(retype(brw_vec1_reg(BRW_ARCHITECTURE_REGISTER_FILE,
d687 1
a687 1
   fs_reg dst = fs_reg(this, glsl_type::uint_type);
d690 2
a691 2
   /* We want to read the 3 fields we care about (mostly field 0, but also 2)
    * even if it's not enabled in the dispatch.
a693 1
   mov->force_uncompressed = true;
a744 1
   push_force_uncompressed();
d747 1
a747 1
   fs_reg diff = fs_reg(this, glsl_type::uint_type);
a760 2

   pop_force_uncompressed();
d768 1
a768 1
      brw_get_shader_time_index(brw, shader_prog, &fp->Base, type);
d846 1
a846 1
   return emit(new(mem_ctx) fs_inst(opcode));
d850 1
a850 1
fs_visitor::emit(enum opcode opcode, fs_reg dst)
d856 1
a856 1
fs_visitor::emit(enum opcode opcode, fs_reg dst, fs_reg src0)
d862 2
a863 1
fs_visitor::emit(enum opcode opcode, fs_reg dst, fs_reg src0, fs_reg src1)
d869 2
a870 2
fs_visitor::emit(enum opcode opcode, fs_reg dst,
                 fs_reg src0, fs_reg src1, fs_reg src2)
d875 3
a877 8
void
fs_visitor::push_force_uncompressed()
{
   force_uncompressed_stack++;
}

void
fs_visitor::pop_force_uncompressed()
d879 1
a879 2
   force_uncompressed_stack--;
   assert(force_uncompressed_stack >= 0);
d894 2
a895 2
           this->force_uncompressed ||
           this->force_sechalf || !this->dst.is_contiguous());
d902 26
a927 4
      if (v->dispatch_width == 16)
	 return (mlen + 1) / 2;
      else
	 return mlen;
a928 1
   return 1;
d995 4
d1001 1
a1001 2
      assert(!"not reached");
      return inst->mlen;
d1027 8
d1038 1
a1038 1
fs_reg::fs_reg(enum register_file file, int reg, uint32_t type)
d1044 19
d1066 1
a1066 1
fs_reg::fs_reg(class fs_visitor *v, const struct glsl_type *type)
d1069 1
d1072 1
a1072 1
   this->reg = v->virtual_grf_alloc(v->type_size(type));
d1075 2
d1146 1
a1146 1
         stage_prog_data->param[uniforms++] = &storage->storage[i].f;
d1163 2
a1164 2
   const ir_state_slot *const slots = ir->state_slots;
   assert(ir->state_slots != NULL);
d1166 1
a1166 1
   for (unsigned int i = 0; i < ir->num_state_slots; i++) {
d1170 1
a1170 1
      int index = _mesa_add_state_reference(this->fp->Base.Parameters,
d1185 1
a1185 1
            &fp->Base.Parameters->ParameterValues[index][swiz].f;
d1193 2
d1197 1
a1197 1
   bool flip = !ir->data.origin_upper_left ^ c->key.render_to_fbo;
d1205 1
a1205 1
   wpos.reg_offset++;
d1216 1
a1216 1
	 offset += c->key.drawable_height - 1.0;
d1221 1
a1221 1
   wpos.reg_offset++;
d1225 1
a1225 1
      emit(MOV(wpos, fs_reg(brw_vec8_grf(c->source_depth_reg, 0))));
d1232 1
a1232 1
   wpos.reg_offset++;
d1282 4
d1301 1
a1301 1
      ir->determine_interpolation_mode(c->key.flat_shade);
d1306 1
a1306 1
	 if (c->prog_data.urb_setup[location] == -1) {
d1310 1
a1310 1
	    attr.reg_offset += type->vector_elements;
d1325 1
a1325 1
	       attr.reg_offset++;
a1330 3
               emit_linterp(attr, fs_reg(interp), interpolation_mode,
                            ir->data.centroid && !c->key.persample_shading,
                            ir->data.sample || c->key.persample_shading);
d1338 4
a1341 3
                  fs_inst *inst = emit_linterp(attr, fs_reg(interp),
                                               interpolation_mode,
                                               false, false);
d1344 15
d1363 1
a1363 1
	       attr.reg_offset++;
d1375 1
a1375 1
fs_visitor::emit_frontfacing_interpolation(ir_variable *ir)
d1377 1
a1377 1
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, ir->type);
a1378 1
   /* The frontfacing comes in as a bit in the thread payload. */
d1380 15
a1394 5
      emit(BRW_OPCODE_ASR, *reg,
	   fs_reg(retype(brw_vec1_grf(0, 0), BRW_REGISTER_TYPE_D)),
	   fs_reg(15));
      emit(BRW_OPCODE_NOT, *reg, *reg);
      emit(BRW_OPCODE_AND, *reg, *reg, fs_reg(1));
d1396 10
a1405 3
      struct brw_reg r1_6ud = retype(brw_vec1_grf(1, 6), BRW_REGISTER_TYPE_UD);
      /* bit 31 is "primitive is back face", so checking < (1 << 31) gives
       * us front face
d1407 6
a1412 2
      emit(CMP(*reg, fs_reg(r1_6ud), fs_reg(1u << 31), BRW_CONDITIONAL_L));
      emit(BRW_OPCODE_AND, *reg, *reg, fs_reg(1u));
d1421 2
d1425 1
a1425 1
   if (c->key.compute_pos_offset) {
d1442 1
a1442 1
fs_visitor::emit_samplepos_setup(ir_variable *ir)
a1444 1
   assert(ir->type == glsl_type::vec2_type);
d1447 1
a1447 1
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, ir->type);
d1464 1
a1464 1
      stride(retype(brw_vec1_grf(c->sample_pos_reg, 0),
d1467 6
a1472 6
   fs_inst *inst = emit(MOV(int_sample_x, fs_reg(sample_pos_reg)));
   if (dispatch_width == 16) {
      inst->force_uncompressed = true;
      inst = emit(MOV(half(int_sample_x, 1),
                      fs_reg(suboffset(sample_pos_reg, 16))));
      inst->force_sechalf = true;
d1476 8
a1483 7
   pos.reg_offset++;
   inst = emit(MOV(int_sample_y, fs_reg(suboffset(sample_pos_reg, 1))));
   if (dispatch_width == 16) {
      inst->force_uncompressed = true;
      inst = emit(MOV(half(int_sample_y, 1),
                      fs_reg(suboffset(sample_pos_reg, 17))));
      inst->force_sechalf = true;
d1491 1
a1491 1
fs_visitor::emit_sampleid_setup(ir_variable *ir)
d1493 2
d1498 1
a1498 1
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, ir->type);
d1500 1
a1500 1
   if (c->key.compute_sample_id) {
d1532 1
a1532 1
      inst = emit(MOV(t2, brw_imm_v(c->key.persample_2x ? 0x1010 : 0x3210)));
a1548 10
fs_reg *
fs_visitor::emit_samplemaskin_setup(ir_variable *ir)
{
   assert(brw->gen >= 7);
   this->current_annotation = "compute gl_SampleMaskIn";
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, ir->type);
   emit(MOV(*reg, fs_reg(retype(brw_vec8_grf(c->sample_mask_reg, 0), BRW_REGISTER_TYPE_D))));
   return reg;
}

d1589 1
a1589 2
      assert(!"not reached: bad math opcode");
      return NULL;
a1618 13
   switch (opcode) {
   case SHADER_OPCODE_INT_QUOTIENT:
   case SHADER_OPCODE_INT_REMAINDER:
      if (brw->gen >= 7)
	 no16("SIMD16 INTDIV unsupported\n");
      break;
   case SHADER_OPCODE_POW:
      break;
   default:
      assert(!"not reached: unsupported binary math opcode.");
      return NULL;
   }

d1640 1
a1640 1
      emit(BRW_OPCODE_MOV, fs_reg(MRF, base_mrf + 1, op1.type), op1);
d1653 1
a1653 1
      c->prog_data.first_curbe_grf = c->nr_payload_regs;
d1655 3
a1657 1
      c->prog_data.first_curbe_grf_16 = c->nr_payload_regs;
d1660 1
a1660 1
   c->prog_data.curb_read_length = ALIGN(stage_prog_data->nr_params, 8) / 8;
d1663 2
a1664 4
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      for (unsigned int i = 0; i < 3; i++) {
d1679 1
a1679 1
	    struct brw_reg brw_reg = brw_vec1_grf(c->nr_payload_regs +
d1695 6
a1700 3
   for (unsigned int i = 0; i < VARYING_SLOT_MAX; i++) {
      c->prog_data.urb_setup[i] = -1;
   }
d1705 1
a1705 1
      if (_mesa_bitcount_64(fp->Base.InputsRead &
d1717 1
a1717 1
            if (fp->Base.InputsRead & BRW_FS_VARYING_INPUT_MASK &
d1719 1
a1719 1
               c->prog_data.urb_setup[i] = urb_next++;
d1730 1
a1730 1
                             c->key.input_slots_valid);
d1740 1
a1740 1
                (fp->Base.InputsRead & BRW_FS_VARYING_INPUT_MASK &
d1742 1
a1742 1
               c->prog_data.urb_setup[varying] = slot - first_slot;
d1754 1
a1754 1
	 if (c->key.input_slots_valid & BITFIELD64_BIT(i)) {
d1762 1
a1762 1
	       c->prog_data.urb_setup[i] = urb_next;
d1773 2
a1774 2
      if (fp->Base.InputsRead & BITFIELD64_BIT(VARYING_SLOT_PNTC))
         c->prog_data.urb_setup[VARYING_SLOT_PNTC] = urb_next++;
d1777 1
a1777 1
   c->prog_data.num_varying_inputs = urb_next;
d1783 4
a1786 1
   int urb_start = c->nr_payload_regs + c->prog_data.curb_read_length;
d1791 1
a1791 3
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

d1805 1
a1805 1
      urb_start + c->prog_data.num_varying_inputs * 2;
a1829 2
   bool split_grf[num_vars];
   int new_virtual_grf[num_vars];
d1831 3
a1833 1
   /* Try to split anything > 0 sized. */
d1835 28
a1862 4
      if (this->virtual_grf_sizes[i] != 1)
	 split_grf[i] = true;
      else
	 split_grf[i] = false;
d1872 2
a1873 2
      split_grf[this->delta_x[BRW_WM_PERSPECTIVE_PIXEL_BARYCENTRIC].reg] =
         false;
d1876 5
a1880 8
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      /* If there's a SEND message that requires contiguous destination
       * registers, no splitting is allowed.
       */
      if (inst->regs_written > 1) {
	 split_grf[inst->dst.reg] = false;
d1882 5
a1886 9

      /* If we're sending from a GRF, don't split it, on the assumption that
       * the send is reading the whole thing.
       */
      if (inst->is_send_from_grf()) {
         for (int i = 0; i < 3; i++) {
            if (inst->src[i].file == GRF) {
               split_grf[inst->src[i].reg] = false;
            }
d1891 4
a1894 3
   /* Allocate new space for split regs.  Note that the virtual
    * numbers will be contiguous.
    */
d1896 30
a1925 9
      if (split_grf[i]) {
	 new_virtual_grf[i] = virtual_grf_alloc(1);
	 for (int j = 2; j < this->virtual_grf_sizes[i]; j++) {
	    int reg = virtual_grf_alloc(1);
	    assert(reg == new_virtual_grf[i] + j - 1);
	    (void) reg;
	 }
	 this->virtual_grf_sizes[i] = 1;
      }
d1927 1
d1929 14
a1942 18
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      if (inst->dst.file == GRF &&
	  split_grf[inst->dst.reg] &&
	  inst->dst.reg_offset != 0) {
	 inst->dst.reg = (new_virtual_grf[inst->dst.reg] +
			  inst->dst.reg_offset - 1);
	 inst->dst.reg_offset = 0;
      }
      for (int i = 0; i < 3; i++) {
	 if (inst->src[i].file == GRF &&
	     split_grf[inst->src[i].reg] &&
	     inst->src[i].reg_offset != 0) {
	    inst->src[i].reg = (new_virtual_grf[inst->src[i].reg] +
				inst->src[i].reg_offset - 1);
	    inst->src[i].reg_offset = 0;
	 }
d1957 1
a1957 1
void
d1960 1
a1960 1
   /* Mark which virtual GRFs are used, and count how many. */
d1964 2
a1965 3
   foreach_list(node, &this->instructions) {
      const fs_inst *inst = (const fs_inst *) node;

d1969 1
a1969 1
      for (int i = 0; i < 3; i++) {
a1974 28
   /* In addition to registers used in instructions, fs_visitor keeps
    * direct references to certain special values which must be patched:
    */
   struct {
      fs_reg *reg;
      unsigned count;
   } special[] = {
      { &frag_depth, 1 },
      { &pixel_x, 1 },
      { &pixel_y, 1 },
      { &pixel_w, 1 },
      { &wpos_w, 1 },
      { &dual_src_output, 1 },
      { outputs, ARRAY_SIZE(outputs) },
      { delta_x, ARRAY_SIZE(delta_x) },
      { delta_y, ARRAY_SIZE(delta_y) },
      { &sample_mask, 1 },
      { &shader_start_time, 1 },
   };

   /* Treat all special values as used, to be conservative */
   for (unsigned i = 0; i < ARRAY_SIZE(special); i++) {
      for (unsigned j = 0; j < special[i].count; j++) {
         if (special[i].reg[j].file == GRF)
            remap_table[special[i].reg[j].reg] = 0;
      }
   }

d1978 6
a1983 1
      if (remap_table[i] != -1) {
d1994 1
a1994 3
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *) node;

d1998 1
a1998 1
      for (int i = 0; i < 3; i++) {
d2004 11
a2014 6
   /* Patch all the references to special values */
   for (unsigned i = 0; i < ARRAY_SIZE(special); i++) {
      for (unsigned j = 0; j < special[i].count; j++) {
         fs_reg *reg = &special[i].reg[j];
         if (reg->file == GRF && remap_table[reg->reg] != -1)
            reg->reg = remap_table[reg->reg];
d2017 11
d2049 1
a2049 4

   for (unsigned int i = 0; i < uniforms; i++) {
      pull_constant_loc[i] = -1;
   }
d2057 2
a2058 4
   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      for (int i = 0 ; i < 3; i++) {
d2068 1
a2068 1
            const float **values = &stage_prog_data->param[uniform];
d2105 2
a2106 4
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *) node;

      for (int i = 0; i < 3; i++) {
d2120 3
d2177 2
a2178 4
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      for (int i = 0; i < 3; i++) {
d2200 1
a2200 1
            inst->insert_before(&list);
d2205 1
a2205 1
               new(mem_ctx) fs_inst(FS_OPCODE_UNIFORM_PULL_CONSTANT_LOAD,
d2207 1
a2207 1
            inst->insert_before(pull);
d2215 1
d2226 1
a2226 3
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

d2281 7
a2287 1
         if (inst->saturate && inst->src[1].file == IMM) {
d2293 1
a2293 1
                  if (inst->src[1].imm.f >= 1.0f) {
d2307 1
a2307 1
                  if (inst->src[1].imm.f <= 0.0f) {
d2322 11
d2342 66
d2413 4
d2419 1
a2419 3
   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

d2439 1
a2439 2
      } else if (dispatch_width == 16 &&
		 (!inst->force_uncompressed && !inst->force_sechalf)) {
d2454 1
a2454 4
      fs_inst *scan_inst;
      for (scan_inst = (fs_inst *)inst->prev;
	   scan_inst->prev != NULL;
	   scan_inst = (fs_inst *)scan_inst->prev) {
d2472 1
a2472 1
            if (scan_inst->regs_written > 1)
d2493 1
a2493 1
	       inst->remove();
d2503 1
a2503 1
	 if (scan_inst->is_control_flow() && scan_inst->opcode != BRW_OPCODE_IF)
d2510 1
a2510 1
	 for (int i = 0; i < 3; i++) {
d2529 1
a2529 3
	    } else if (dispatch_width == 16 &&
		       (!scan_inst->force_uncompressed &&
			!scan_inst->force_sechalf)) {
d2568 46
d2629 1
a2629 3
   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

d2638 1
a2638 1
	    inst->remove();
a2685 4
   bool inst_simd16 = (dispatch_width > 8 &&
                       !inst->force_uncompressed &&
                       !inst->force_sechalf);

d2687 1
a2687 1
   for (int i = 0; i < 3; i++) {
d2701 1
a2701 1
         if (inst_simd16)
d2724 2
a2725 1
fs_visitor::insert_gen4_pre_send_dependency_workarounds(fs_inst *inst)
d2727 1
a2727 2
   int reg_size = dispatch_width / 8;
   int write_len = inst->regs_written * reg_size;
d2743 1
a2743 4
   for (fs_inst *scan_inst = (fs_inst *)inst->prev;
        !scan_inst->is_head_sentinel();
        scan_inst = (fs_inst *)scan_inst->prev) {

d2747 1
a2747 1
      if (scan_inst->is_control_flow()) {
d2750 1
a2750 1
               inst->insert_before(DEP_RESOLVE_MOV(first_write_grf + i));
a2755 4
      bool scan_inst_simd16 = (dispatch_width > 8 &&
                               !scan_inst->force_uncompressed &&
                               !scan_inst->force_sechalf);

d2762 1
a2762 1
            int reg = scan_inst->dst.reg + i * reg_size;
d2767 1
a2767 1
               inst->insert_before(DEP_RESOLVE_MOV(reg));
d2769 1
a2769 1
               if (scan_inst_simd16)
d2798 1
a2798 1
fs_visitor::insert_gen4_post_send_dependency_workarounds(fs_inst *inst)
d2800 1
a2800 1
   int write_len = inst->regs_written * dispatch_width / 8;
d2810 1
a2810 3
   for (fs_inst *scan_inst = (fs_inst *)inst->next;
        !scan_inst->is_tail_sentinel();
        scan_inst = (fs_inst *)scan_inst->next) {
d2812 1
a2812 1
      if (scan_inst->is_control_flow()) {
d2815 2
a2816 1
               scan_inst->insert_before(DEP_RESOLVE_MOV(first_write_grf + i));
d2832 1
a2832 1
         scan_inst->insert_before(DEP_RESOLVE_MOV(scan_inst->dst.reg));
d2853 1
a2853 1
         last_inst->insert_before(DEP_RESOLVE_MOV(first_write_grf + i));
d2869 1
a2869 3
   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

d2871 2
a2872 2
         insert_gen4_pre_send_dependency_workarounds(inst);
         insert_gen4_post_send_dependency_workarounds(inst);
d2900 1
a2900 3
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

d2911 1
a2911 1
         const_offset_reg.imm.u /= 4;
d2920 1
a2920 1
                                               payload, const_offset_reg);
d2925 1
a2925 1
         inst->insert_before(setup);
d2947 105
d3055 6
d3062 6
d3070 1
a3070 2
   foreach_list(node, &this->instructions) {
      backend_instruction *inst = (backend_instruction *)node;
d3072 2
a3073 2
      fprintf(stderr, "{%3d} %4d: ", regs_live_at_ip[ip], ip);
      dump_instruction(inst);
d3076 5
a3080 1
   fprintf(stderr, "Maximum %3d registers live at once.\n", max_pressure);
d3086 6
d3095 1
a3095 1
      fprintf(stderr, "(%cf0.%d) ",
d3100 1
a3100 1
   fprintf(stderr, "%s", brw_instruction_name(inst->opcode));
d3102 1
a3102 1
      fprintf(stderr, ".sat");
d3104 1
a3104 1
      fprintf(stderr, "%s", conditional_modifier[inst->conditional_mod]);
d3109 1
a3109 1
         fprintf(stderr, ".f0.%d", inst->flag_subreg);
d3112 1
a3112 1
   fprintf(stderr, " ");
d3117 4
a3120 2
      fprintf(stderr, "vgrf%d", inst->dst.reg);
      if (virtual_grf_sizes[inst->dst.reg] != 1 ||
d3122 1
a3122 1
         fprintf(stderr, "+%d.%d",
d3126 1
a3126 1
      fprintf(stderr, "m%d", inst->dst.reg);
d3129 1
a3129 1
      fprintf(stderr, "(null)");
d3132 1
a3132 1
      fprintf(stderr, "***u%d***", inst->dst.reg + inst->dst.reg_offset);
d3138 1
a3138 1
            fprintf(stderr, "null");
d3141 1
a3141 1
            fprintf(stderr, "a0.%d", inst->dst.fixed_hw_reg.subnr);
d3144 1
a3144 1
            fprintf(stderr, "acc%d", inst->dst.fixed_hw_reg.subnr);
d3147 1
a3147 1
            fprintf(stderr, "f%d.%d", inst->dst.fixed_hw_reg.nr & 0xf,
d3151 1
a3151 1
            fprintf(stderr, "arf%d.%d", inst->dst.fixed_hw_reg.nr & 0xf,
d3156 1
a3156 1
         fprintf(stderr, "hw_reg%d", inst->dst.fixed_hw_reg.nr);
d3159 1
a3159 1
         fprintf(stderr, "+%d", inst->dst.fixed_hw_reg.subnr);
d3162 1
a3162 1
      fprintf(stderr, "???");
d3165 1
a3165 1
   fprintf(stderr, ":%s, ", brw_reg_type_letters(inst->dst.type));
d3167 1
a3167 1
   for (int i = 0; i < 3 && inst->src[i].file != BAD_FILE; i++) {
d3169 1
a3169 1
         fprintf(stderr, "-");
d3171 1
a3171 1
         fprintf(stderr, "|");
d3174 4
a3177 2
         fprintf(stderr, "vgrf%d", inst->src[i].reg);
         if (virtual_grf_sizes[inst->src[i].reg] != 1 ||
d3179 1
a3179 1
            fprintf(stderr, "+%d.%d", inst->src[i].reg_offset,
d3183 1
a3183 1
         fprintf(stderr, "***m%d***", inst->src[i].reg);
d3186 1
a3186 1
         fprintf(stderr, "u%d", inst->src[i].reg + inst->src[i].reg_offset);
d3188 3
a3190 4
            fprintf(stderr, "+reladdr");
         } else if (virtual_grf_sizes[inst->src[i].reg] != 1 ||
             inst->src[i].subreg_offset) {
            fprintf(stderr, "+%d.%d", inst->src[i].reg_offset,
d3195 1
a3195 1
         fprintf(stderr, "(null)");
d3200 1
a3200 1
            fprintf(stderr, "%ff", inst->src[i].imm.f);
d3203 1
a3203 1
            fprintf(stderr, "%dd", inst->src[i].imm.i);
d3206 1
a3206 1
            fprintf(stderr, "%uu", inst->src[i].imm.u);
d3209 1
a3209 1
            fprintf(stderr, "???");
d3215 1
a3215 1
            fprintf(stderr, "-");
d3217 1
a3217 1
            fprintf(stderr, "|");
d3221 1
a3221 1
               fprintf(stderr, "null");
d3224 1
a3224 1
               fprintf(stderr, "a0.%d", inst->src[i].fixed_hw_reg.subnr);
d3227 1
a3227 1
               fprintf(stderr, "acc%d", inst->src[i].fixed_hw_reg.subnr);
d3230 1
a3230 1
               fprintf(stderr, "f%d.%d", inst->src[i].fixed_hw_reg.nr & 0xf,
d3234 1
a3234 1
               fprintf(stderr, "arf%d.%d", inst->src[i].fixed_hw_reg.nr & 0xf,
d3239 1
a3239 1
            fprintf(stderr, "hw_reg%d", inst->src[i].fixed_hw_reg.nr);
d3242 1
a3242 1
            fprintf(stderr, "+%d", inst->src[i].fixed_hw_reg.subnr);
d3244 1
a3244 1
            fprintf(stderr, "|");
d3247 1
a3247 1
         fprintf(stderr, "???");
d3251 1
a3251 1
         fprintf(stderr, "|");
d3254 1
a3254 1
         fprintf(stderr, ":%s", brw_reg_type_letters(inst->src[i].type));
d3257 2
a3258 2
      if (i < 2 && inst->src[i + 1].file != BAD_FILE)
         fprintf(stderr, ", ");
d3261 1
a3261 1
   fprintf(stderr, " ");
d3263 6
a3268 2
   if (inst->force_uncompressed)
      fprintf(stderr, "1sthalf ");
d3270 1
a3270 4
   if (inst->force_sechalf)
      fprintf(stderr, "2ndhalf ");

   fprintf(stderr, "\n");
d3305 4
a3308 2
      (fp->Base.InputsRead & (1 << VARYING_SLOT_POS)) != 0;
   unsigned barycentric_interp_modes = c->prog_data.barycentric_interp_modes;
d3313 1
a3313 1
   c->nr_payload_regs = 2;
d3325 2
a3326 2
         c->barycentric_coord_reg[i] = c->nr_payload_regs;
         c->nr_payload_regs += 2;
d3328 1
a3328 1
            c->nr_payload_regs += 2;
d3335 2
a3336 2
      c->source_depth_reg = c->nr_payload_regs;
      c->nr_payload_regs++;
d3339 1
a3339 1
         c->nr_payload_regs++;
d3344 2
a3345 2
      c->source_w_reg = c->nr_payload_regs;
      c->nr_payload_regs++;
d3348 1
a3348 1
         c->nr_payload_regs++;
d3352 9
a3360 5
   c->prog_data.uses_pos_offset = c->key.compute_pos_offset;
   /* R31: MSAA position offsets. */
   if (c->prog_data.uses_pos_offset) {
      c->sample_pos_reg = c->nr_payload_regs;
      c->nr_payload_regs++;
d3364 1
a3364 1
   if (fp->Base.SystemValuesRead & SYSTEM_BIT_SAMPLE_MASK_IN) {
d3366 2
a3367 2
      c->sample_mask_reg = c->nr_payload_regs;
      c->nr_payload_regs++;
d3370 1
a3370 1
         c->nr_payload_regs++;
d3377 2
a3378 2
   if (fp->Base.OutputsWritten & BITFIELD64_BIT(FRAG_RESULT_DEPTH)) {
      c->source_depth_to_render_target = true;
d3385 3
d3393 2
a3394 2
   c->prog_data.binding_table.render_target_start = next_binding_table_offset;
   next_binding_table_offset += MAX2(c->key.nr_color_regions, 1);
d3405 3
a3407 4
   int num_instructions = 0;
   foreach_list(node, &this->instructions) {
      ++num_instructions;
   }
d3432 1
a3432 3
   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

d3439 1
a3439 1
            inst->remove();
d3446 137
d3586 1
a3586 2
   sanity_param_count = fp->Base.Parameters->NumParameters;
   bool allocated_without_spills;
d3597 2
d3604 1
a3604 1
      if (fp->Base.InputsRead > 0) {
d3614 7
a3620 1
      if (fp->UsesKill || c->key.alpha_test_func) {
d3629 1
a3629 2
         foreach_list(node, &*shader->base.ir) {
            ir_instruction *ir = (ir_instruction *)node;
d3643 1
a3643 1
      if (c->key.alpha_test_func)
d3648 1
a3648 29
      split_virtual_grfs();

      move_uniform_array_access_to_pull_constants();
      assign_constant_locations();
      demote_pull_constants();

      opt_drop_redundant_mov_to_flags();

      bool progress;
      do {
	 progress = false;

         compact_virtual_grfs();

	 progress = remove_duplicate_mrf_writes() || progress;

	 progress = opt_algebraic() || progress;
	 progress = opt_cse() || progress;
	 progress = opt_copy_propagate() || progress;
         progress = opt_peephole_predicated_break() || progress;
         progress = dead_code_eliminate() || progress;
         progress = opt_peephole_sel() || progress;
         progress = dead_control_flow_eliminate(this) || progress;
         progress = opt_saturate_propagation() || progress;
         progress = register_coalesce() || progress;
	 progress = compute_to_mrf() || progress;
      } while (progress);

      lower_uniform_pull_constant_loads();
d3653 1
a3653 22
      static enum instruction_scheduler_mode pre_modes[] = {
         SCHEDULE_PRE,
         SCHEDULE_PRE_NON_LIFO,
         SCHEDULE_PRE_LIFO,
      };

      /* Try each scheduling heuristic to see if it can successfully register
       * allocate without spilling.  They should be ordered by decreasing
       * performance but increasing likelihood of allocating.
       */
      for (unsigned i = 0; i < ARRAY_SIZE(pre_modes); i++) {
         schedule_instructions(pre_modes[i]);

         if (0) {
            assign_regs_trivial();
            allocated_without_spills = true;
         } else {
            allocated_without_spills = assign_regs(false);
         }
         if (allocated_without_spills)
            break;
      }
d3655 3
a3657 9
      if (!allocated_without_spills) {
         /* We assume that any spilling is worse than just dropping back to
          * SIMD8.  There's probably actually some intermediate point where
          * SIMD16 with a couple of spills is still better.
          */
         if (dispatch_width == 16) {
            fail("Failure to register allocate.  Reduce number of "
                 "live scalar values to avoid this.");
         }
d3659 6
a3664 8
         /* Since we're out of heuristics, just go spill registers until we
          * get an allocation.
          */
         while (!assign_regs(true)) {
            if (failed)
               break;
         }
      }
a3665 18
   assert(force_uncompressed_stack == 0);

   /* This must come after all optimization and register allocation, since
    * it inserts dead code that happens to have side effects, and it does
    * so based on the actual physical registers in use.
    */
   insert_gen4_send_dependency_workarounds();

   if (failed)
      return false;

   if (!allocated_without_spills)
      schedule_instructions(SCHEDULE_POST);

   if (dispatch_width == 8)
      c->prog_data.reg_blocks = brw_register_blocks(grf_used);
   else
      c->prog_data.reg_blocks_16 = brw_register_blocks(grf_used);
d3672 1
a3672 1
   assert(sanity_param_count == fp->Base.Parameters->NumParameters);
d3678 4
a3681 1
brw_wm_fs_emit(struct brw_context *brw, struct brw_wm_compile *c,
d3700 1
a3700 1
      brw_dump_ir(brw, "fragment", prog, &shader->base, &fp->Base);
d3704 1
a3704 1
   fs_visitor v(brw, c, prog, fp, 8);
d3717 4
a3720 3
   exec_list *simd16_instructions = NULL;
   fs_visitor v2(brw, c, prog, fp, 16);
   if (brw->gen >= 5 && likely(!(INTEL_DEBUG & DEBUG_NO16))) {
d3728 1
a3728 1
            simd16_instructions = &v2.instructions;
d3736 5
a3740 5
   const unsigned *assembly = NULL;
   if (brw->gen >= 8) {
      gen8_fs_generator g(brw, c, prog, fp, v.do_dual_src);
      assembly = g.generate_assembly(&v.instructions, simd16_instructions,
                                     final_assembly_size);
d3742 2
a3743 3
      fs_generator g(brw, c, prog, fp, v.do_dual_src);
      assembly = g.generate_assembly(&v.instructions, simd16_instructions,
                                     final_assembly_size);
d3746 7
d3755 1
a3755 1
         brw_wm_debug_recompile(brw, prog, &c->key);
d3764 1
a3764 1
   return assembly;
a3797 2

   key.clamp_fragment_color = ctx->API == API_OPENGL_COMPAT;
@


1.6
log
@Merge Mesa 10.2.3
tested by matthieu@@ kettenis@@ mpi@@ brett@@ and myself across a
diverse range of hardware
@
text
@d1266 1
a1266 1
   emit(MOV(int_sample_x, fs_reg(sample_pos_reg)));
d1268 3
a1270 2
      fs_inst *inst = emit(MOV(half(int_sample_x, 1),
                               fs_reg(suboffset(sample_pos_reg, 16))));
d1276 1
a1276 1
   emit(MOV(int_sample_y, fs_reg(suboffset(sample_pos_reg, 1))));
d1278 3
a1280 2
      fs_inst *inst = emit(MOV(half(int_sample_y, 1),
                               fs_reg(suboffset(sample_pos_reg, 17))));
d1314 5
d1320 7
a1326 4
      emit(BRW_OPCODE_AND, t1,
           fs_reg(retype(brw_vec1_grf(0, 0), BRW_REGISTER_TYPE_D)),
           fs_reg(brw_imm_d(0xc0)));
      emit(BRW_OPCODE_SHR, t1, t1, fs_reg(5));
d1328 2
a1329 1
      emit(MOV(t2, brw_imm_v(0x3210)));
d1407 1
a1407 1
   if (brw->gen >= 6)
d1439 3
a1441 1
   if (brw->gen >= 6) {
@


1.5
log
@Merge Mesa 9.2.5
@
text
@a37 1
#include "main/uniforms.h"
d49 3
a57 1
   this->opcode = BRW_OPCODE_NOP;
d67 2
d74 1
d156 9
d180 1
a180 1
ALU2(MACH)
d195 5
d210 1
a210 1
/** Gen6+ IF with embedded comparison. */
d214 1
a214 1
   assert(brw->gen >= 6);
d259 3
a261 2
fs_visitor::VARYING_PULL_CONSTANT_LOAD(fs_reg dst, fs_reg surf_index,
                                       fs_reg varying_offset,
d338 1
a338 1
fs_inst::equals(fs_inst *inst)
d359 1
a359 1
fs_inst::overwrites_reg(const fs_reg &reg)
d368 1
a368 1
fs_inst::is_send_from_grf()
d373 2
a374 1
            src[1].file == GRF));
d386 3
d396 1
a396 1
   this->smear = -1;
d433 1
a433 1
/** Fixed brw_reg Immediate value constructor. */
d448 1
d455 1
a455 1
           smear == r.smear &&
d459 25
d503 8
d516 8
d548 3
d590 1
a590 1
   dst.smear = 0;
d625 1
a625 1
   reset.smear = 2;
d665 2
a666 2
   emit(fs_inst(SHADER_OPCODE_SHADER_TIME_ADD,
                fs_reg(), payload, offset, value));
d670 1
a670 1
fs_visitor::fail(const char *format, ...)
a671 1
   va_list va;
a678 1
   va_start(va, format);
a679 1
   va_end(va);
d689 42
d734 1
a734 1
   return emit(fs_inst(opcode));
d740 1
a740 1
   return emit(fs_inst(opcode, dst));
d746 1
a746 1
   return emit(fs_inst(opcode, dst, src0));
d752 1
a752 1
   return emit(fs_inst(opcode, dst, src0, src1));
d759 1
a759 1
   return emit(fs_inst(opcode, dst, src0, src1, src2));
a774 13
void
fs_visitor::push_force_sechalf()
{
   force_sechalf_stack++;
}

void
fs_visitor::pop_force_sechalf()
{
   force_sechalf_stack--;
   assert(force_sechalf_stack >= 0);
}

d784 1
a784 1
fs_inst::is_partial_write()
d786 1
a786 1
   return (this->predicate ||
d788 26
a813 1
           this->force_sechalf);
d828 3
d848 4
a851 1
   case SHADER_OPCODE_TXF_MS:
d859 1
a859 1
   case FS_OPCODE_UNSPILL:
d863 1
a863 1
   case FS_OPCODE_SPILL:
d865 3
d938 1
a938 1
/* For 16-wide, we need to follow from the uniform setup of 8-wide dispatch.
d947 4
a950 2
   this->params_remap = v->params_remap;
   this->nr_params_remap = v->nr_params_remap;
d969 1
a969 1
   unsigned params_before = c->prog_data.nr_params;
d985 1
a985 2
         c->prog_data.param[c->prog_data.nr_params++] =
            &storage->storage[i].f;
d990 1
a990 2
   assert(params_before + ir->type->component_slots() ==
          c->prog_data.nr_params);
d1023 1
a1023 1
	 c->prog_data.param[c->prog_data.nr_params++] =
d1034 1
a1034 1
   bool flip = !ir->origin_upper_left ^ c->key.render_to_fbo;
d1037 1
a1037 1
   if (ir->pixel_center_integer) {
d1045 1
a1045 1
   if (!flip && ir->pixel_center_integer) {
d1049 1
a1049 1
      float offset = (ir->pixel_center_integer ? 0.0 : 0.5);
d1080 1
a1080 1
                         bool is_centroid)
d1089 5
d1136 1
a1136 1
   int location = ir->location;
d1139 1
a1139 1
	 if (urb_setup[location] == -1) {
a1162 6
	       /* FINISHME: At some point we probably want to push
		* this farther by giving similar treatment to the
		* other potentially constant components of the
		* attribute, as well as making brw_vs_constval.c
		* handle varyings other than gl_TexCoord.
		*/
d1165 3
a1167 2
                            ir->centroid);
               if (brw->needs_unlit_centroid_workaround && ir->centroid) {
d1175 2
a1176 1
                                               interpolation_mode, false);
d1180 1
a1180 1
               if (brw->gen < 6) {
d1218 126
d1418 2
a1419 2
      if (brw->gen >= 7 && dispatch_width == 16)
	 fail("16-wide INTDIV unsupported\n");
a1458 1
   c->prog_data.curb_read_length = ALIGN(c->prog_data.nr_params, 8) / 8;
d1465 2
d1473 13
a1485 1
	    int constant_nr = inst->src[i].reg + inst->src[i].reg_offset;
d1491 3
a1493 1
	    inst->src[i].fixed_hw_reg = retype(brw_reg, inst->src[i].type);
d1503 1
a1503 1
      urb_setup[i] = -1;
d1509 41
a1549 4
      for (unsigned int i = 0; i < VARYING_SLOT_MAX; i++) {
	 if (fp->Base.InputsRead & BITFIELD64_BIT(i)) {
	    urb_setup[i] = urb_next++;
	 }
d1566 1
a1566 1
	       urb_setup[i] = urb_next;
d1578 1
a1578 1
         urb_setup[VARYING_SLOT_PNTC] = urb_next++;
d1581 1
a1581 2
   /* Each attribute is 4 setup channels, each of which is half a reg. */
   c->prog_data.urb_read_length = urb_next * 2;
d1606 3
a1608 1
   this->first_non_payload_grf = urb_start + c->prog_data.urb_read_length;
d1712 1
a1712 1
   this->live_intervals_valid = false;
d1746 15
a1760 8
   fs_reg *special[] = {
      &frag_depth, &pixel_x, &pixel_y, &pixel_w, &wpos_w, &dual_src_output,
      &outputs[0], &outputs[1], &outputs[2], &outputs[3],
      &outputs[4], &outputs[5], &outputs[6], &outputs[7],
      &delta_x[0], &delta_x[1], &delta_x[2],
      &delta_x[3], &delta_x[4], &delta_x[5],
      &delta_y[0], &delta_y[1], &delta_y[2],
      &delta_y[3], &delta_y[4], &delta_y[5],
a1761 2
   STATIC_ASSERT(BRW_WM_BARYCENTRIC_INTERP_MODE_COUNT == 6);
   STATIC_ASSERT(BRW_MAX_DRAW_BUFFERS == 8);
d1765 4
a1768 2
      if (special[i]->file == GRF)
	 remap_table[special[i]->reg] = 0;
d1777 1
a1777 4
         if (live_intervals_valid) {
            virtual_grf_start[new_index] = virtual_grf_start[i];
            virtual_grf_end[new_index] = virtual_grf_end[i];
         }
d1799 4
a1802 87
      if (special[i]->file == GRF && remap_table[special[i]->reg] != -1)
	 special[i]->reg = remap_table[special[i]->reg];
   }
}

bool
fs_visitor::remove_dead_constants()
{
   if (dispatch_width == 8) {
      this->params_remap = ralloc_array(mem_ctx, int, c->prog_data.nr_params);
      this->nr_params_remap = c->prog_data.nr_params;

      for (unsigned int i = 0; i < c->prog_data.nr_params; i++)
	 this->params_remap[i] = -1;

      /* Find which params are still in use. */
      foreach_list(node, &this->instructions) {
	 fs_inst *inst = (fs_inst *)node;

	 for (int i = 0; i < 3; i++) {
	    int constant_nr = inst->src[i].reg + inst->src[i].reg_offset;

	    if (inst->src[i].file != UNIFORM)
	       continue;

	    /* Section 5.11 of the OpenGL 4.3 spec says:
	     *
	     *     "Out-of-bounds reads return undefined values, which include
	     *     values from other variables of the active program or zero."
	     */
	    if (constant_nr < 0 || constant_nr >= (int)c->prog_data.nr_params) {
	       constant_nr = 0;
	    }

	    /* For now, set this to non-negative.  We'll give it the
	     * actual new number in a moment, in order to keep the
	     * register numbers nicely ordered.
	     */
	    this->params_remap[constant_nr] = 0;
	 }
      }

      /* Figure out what the new numbers for the params will be.  At some
       * point when we're doing uniform array access, we're going to want
       * to keep the distinction between .reg and .reg_offset, but for
       * now we don't care.
       */
      unsigned int new_nr_params = 0;
      for (unsigned int i = 0; i < c->prog_data.nr_params; i++) {
	 if (this->params_remap[i] != -1) {
	    this->params_remap[i] = new_nr_params++;
	 }
      }

      /* Update the list of params to be uploaded to match our new numbering. */
      for (unsigned int i = 0; i < c->prog_data.nr_params; i++) {
	 int remapped = this->params_remap[i];

	 if (remapped == -1)
	    continue;

	 c->prog_data.param[remapped] = c->prog_data.param[i];
      }

      c->prog_data.nr_params = new_nr_params;
   } else {
      /* This should have been generated in the 8-wide pass already. */
      assert(this->params_remap);
   }

   /* Now do the renumbering of the shader to remove unused params. */
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      for (int i = 0; i < 3; i++) {
	 int constant_nr = inst->src[i].reg + inst->src[i].reg_offset;

	 if (inst->src[i].file != UNIFORM)
	    continue;

	 /* as above alias to 0 */
	 if (constant_nr < 0 || constant_nr >= (int)this->nr_params_remap) {
	    constant_nr = 0;
	 }
	 assert(this->params_remap[constant_nr] != -1);
	 inst->src[i].reg = this->params_remap[constant_nr];
	 inst->src[i].reg_offset = 0;
a1804 2

   return true;
d1822 2
a1823 1
   int pull_constant_loc[c->prog_data.nr_params];
d1825 3
a1827 1
   for (unsigned int i = 0; i < c->prog_data.nr_params; i++) {
d1850 1
a1850 3
            const float **values = &c->prog_data.param[uniform];

            pull_constant_loc[uniform] = c->prog_data.nr_pull_params;
d1855 3
a1857 1
               c->prog_data.pull_param[c->prog_data.nr_pull_params++] =
a1860 18

         /* Set up the annotation tracking for new generated instructions. */
         base_ir = inst->ir;
         current_annotation = inst->annotation;

         fs_reg surf_index = fs_reg((unsigned)SURF_INDEX_FRAG_CONST_BUFFER);
         fs_reg temp = fs_reg(this, glsl_type::float_type);
         exec_list list = VARYING_PULL_CONSTANT_LOAD(temp,
                                                     surf_index,
                                                     *inst->src[i].reladdr,
                                                     pull_constant_loc[uniform] +
                                                     inst->src[i].reg_offset);
         inst->insert_before(&list);

         inst->src[i].file = temp.file;
         inst->src[i].reg = temp.reg;
         inst->src[i].reg_offset = temp.reg_offset;
         inst->src[i].reladdr = NULL;
d1866 1
a1866 2
 * Choose accesses from the UNIFORM file to demote to using the pull
 * constant buffer.
d1875 1
a1875 1
fs_visitor::setup_pull_constants()
d1877 2
a1878 3
   /* Only allow 16 registers (128 uniform components) as push constants. */
   unsigned int max_uniform_components = 16 * 8;
   if (c->prog_data.nr_params <= max_uniform_components)
d1881 17
a1897 3
   if (dispatch_width == 16) {
      fail("Pull constants not supported in 16-wide\n");
      return;
d1900 3
a1902 1
   /* Just demote the end of the list.  We could probably do better
d1905 13
a1917 1
   unsigned int pull_uniform_base = max_uniform_components;
d1919 5
a1923 4
   int pull_constant_loc[c->prog_data.nr_params];
   for (unsigned int i = 0; i < c->prog_data.nr_params; i++) {
      if (i < pull_uniform_base) {
         pull_constant_loc[i] = -1;
d1925 6
a1930 15
         pull_constant_loc[i] = -1;
         /* If our constant is already being uploaded for reladdr purposes,
          * reuse it.
          */
         for (unsigned int j = 0; j < c->prog_data.nr_pull_params; j++) {
            if (c->prog_data.pull_param[j] == c->prog_data.param[i]) {
               pull_constant_loc[i] = j;
               break;
            }
         }
         if (pull_constant_loc[i] == -1) {
            int pull_index = c->prog_data.nr_pull_params++;
            c->prog_data.pull_param[pull_index] = c->prog_data.param[i];
            pull_constant_loc[i] = pull_index;;
         }
a1932 1
   c->prog_data.nr_params = pull_uniform_base;
d1934 24
d1970 23
a1992 1
         assert(!inst->src[i].reladdr);
d1994 4
a1997 15
	 fs_reg dst = fs_reg(this, glsl_type::float_type);
	 fs_reg index = fs_reg((unsigned)SURF_INDEX_FRAG_CONST_BUFFER);
	 fs_reg offset = fs_reg((unsigned)(pull_index * 4) & ~15);
	 fs_inst *pull =
            new(mem_ctx) fs_inst(FS_OPCODE_UNIFORM_PULL_CONSTANT_LOAD,
                                 dst, index, offset);
	 pull->ir = inst->ir;
	 pull->annotation = inst->annotation;

	 inst->insert_before(pull);

	 inst->src[i].file = GRF;
	 inst->src[i].reg = dst.reg;
	 inst->src[i].reg_offset = 0;
	 inst->src[i].smear = pull_index & 3;
d2000 1
d2046 16
a2061 28
      default:
	 break;
      }
   }

   return progress;
}

/**
 * Removes any instructions writing a VGRF where that VGRF is not used by any
 * later instruction.
 */
bool
fs_visitor::dead_code_eliminate()
{
   bool progress = false;
   int pc = 0;

   calculate_live_intervals();

   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      if (inst->dst.file == GRF) {
         assert(this->virtual_grf_end[inst->dst.reg] >= pc);
         if (this->virtual_grf_end[inst->dst.reg] == pc) {
            inst->remove();
            progress = true;
d2063 16
a2078 142
      }

      pc++;
   }

   if (progress)
      live_intervals_valid = false;

   return progress;
}

struct dead_code_hash_key
{
   int vgrf;
   int reg_offset;
};

static bool
dead_code_hash_compare(const void *a, const void *b)
{
   return memcmp(a, b, sizeof(struct dead_code_hash_key)) == 0;
}

static void
clear_dead_code_hash(struct hash_table *ht)
{
   struct hash_entry *entry;

   hash_table_foreach(ht, entry) {
      _mesa_hash_table_remove(ht, entry);
   }
}

static void
insert_dead_code_hash(struct hash_table *ht,
                      int vgrf, int reg_offset, fs_inst *inst)
{
   /* We don't bother freeing keys, because they'll be GCed with the ht. */
   struct dead_code_hash_key *key = ralloc(ht, struct dead_code_hash_key);

   key->vgrf = vgrf;
   key->reg_offset = reg_offset;

   _mesa_hash_table_insert(ht, _mesa_hash_data(key, sizeof(*key)), key, inst);
}

static struct hash_entry *
get_dead_code_hash_entry(struct hash_table *ht, int vgrf, int reg_offset)
{
   struct dead_code_hash_key key;

   key.vgrf = vgrf;
   key.reg_offset = reg_offset;

   return _mesa_hash_table_search(ht, _mesa_hash_data(&key, sizeof(key)), &key);
}

static void
remove_dead_code_hash(struct hash_table *ht,
                      int vgrf, int reg_offset)
{
   struct hash_entry *entry = get_dead_code_hash_entry(ht, vgrf, reg_offset);
   if (!entry)
      return;

   _mesa_hash_table_remove(ht, entry);
}

/**
 * Walks basic blocks, removing any regs that are written but not read before
 * being redefined.
 *
 * The dead_code_eliminate() function implements a global dead code
 * elimination, but it only handles the removing the last write to a register
 * if it's never read.  This one can handle intermediate writes, but only
 * within a basic block.
 */
bool
fs_visitor::dead_code_eliminate_local()
{
   struct hash_table *ht;
   bool progress = false;

   ht = _mesa_hash_table_create(mem_ctx, dead_code_hash_compare);

   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      /* At a basic block, empty the HT since we don't understand dataflow
       * here.
       */
      if (inst->is_control_flow()) {
         clear_dead_code_hash(ht);
         continue;
      }

      /* Clear the HT of any instructions that got read. */
      for (int i = 0; i < 3; i++) {
         fs_reg src = inst->src[i];
         if (src.file != GRF)
            continue;

         int read = 1;
         if (inst->is_send_from_grf())
            read = virtual_grf_sizes[src.reg] - src.reg_offset;

         for (int reg_offset = src.reg_offset;
              reg_offset < src.reg_offset + read;
              reg_offset++) {
            remove_dead_code_hash(ht, src.reg, reg_offset);
         }
      }

      /* Add any update of a GRF to the HT, removing a previous write if it
       * wasn't read.
       */
      if (inst->dst.file == GRF) {
         if (inst->regs_written > 1) {
            /* We don't know how to trim channels from an instruction's
             * writes, so we can't incrementally remove unread channels from
             * it.  Just remove whatever it overwrites from the table
             */
            for (int i = 0; i < inst->regs_written; i++) {
               remove_dead_code_hash(ht,
                                     inst->dst.reg,
                                     inst->dst.reg_offset + i);
            }
         } else {
            struct hash_entry *entry =
               get_dead_code_hash_entry(ht, inst->dst.reg,
                                        inst->dst.reg_offset);

            if (inst->is_partial_write()) {
               /* For a partial write, we can't remove any previous dead code
                * candidate, since we're just modifying their result, but we can
                * be dead code eliminiated ourselves.
                */
               if (entry) {
                  entry->data = inst;
               } else {
                  insert_dead_code_hash(ht, inst->dst.reg, inst->dst.reg_offset,
                                        inst);
d2080 14
a2093 9
            } else {
               if (entry) {
                  /* We're completely updating a channel, and there was a
                   * previous write to the channel that wasn't read.  Kill it!
                   */
                  fs_inst *inst = (fs_inst *)entry->data;
                  inst->remove();
                  progress = true;
                  _mesa_hash_table_remove(ht, entry);
d2095 2
a2096 3

               insert_dead_code_hash(ht, inst->dst.reg, inst->dst.reg_offset,
                                     inst);
d2099 1
a2099 121
      }
   }

   _mesa_hash_table_destroy(ht, NULL);

   if (progress)
      live_intervals_valid = false;

   return progress;
}

/**
 * Implements a second type of register coalescing: This one checks if
 * the two regs involved in a raw move don't interfere, in which case
 * they can both by stored in the same place and the MOV removed.
 */
bool
fs_visitor::register_coalesce_2()
{
   bool progress = false;

   calculate_live_intervals();

   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      if (inst->opcode != BRW_OPCODE_MOV ||
	  inst->is_partial_write() ||
	  inst->saturate ||
	  inst->src[0].file != GRF ||
	  inst->src[0].negate ||
	  inst->src[0].abs ||
	  inst->src[0].smear != -1 ||
	  inst->dst.file != GRF ||
	  inst->dst.type != inst->src[0].type ||
	  virtual_grf_sizes[inst->src[0].reg] != 1 ||
	  virtual_grf_interferes(inst->dst.reg, inst->src[0].reg)) {
	 continue;
      }

      int reg_from = inst->src[0].reg;
      assert(inst->src[0].reg_offset == 0);
      int reg_to = inst->dst.reg;
      int reg_to_offset = inst->dst.reg_offset;

      foreach_list(node, &this->instructions) {
	 fs_inst *scan_inst = (fs_inst *)node;

	 if (scan_inst->dst.file == GRF &&
	     scan_inst->dst.reg == reg_from) {
	    scan_inst->dst.reg = reg_to;
	    scan_inst->dst.reg_offset = reg_to_offset;
	 }
	 for (int i = 0; i < 3; i++) {
	    if (scan_inst->src[i].file == GRF &&
		scan_inst->src[i].reg == reg_from) {
	       scan_inst->src[i].reg = reg_to;
	       scan_inst->src[i].reg_offset = reg_to_offset;
	    }
	 }
      }

      inst->remove();

      /* We don't need to recalculate live intervals inside the loop despite
       * flagging live_intervals_valid because we only use live intervals for
       * the interferes test, and we must have had a situation where the
       * intervals were:
       *
       *  from  to
       *  ^
       *  |
       *  v
       *        ^
       *        |
       *        v
       *
       * Some register R that might get coalesced with one of these two could
       * only be referencing "to", otherwise "from"'s range would have been
       * longer.  R's range could also only start at the end of "to" or later,
       * otherwise it will conflict with "to" when we try to coalesce "to"
       * into Rw anyway.
       */
      live_intervals_valid = false;

      progress = true;
      continue;
   }

   return progress;
}

bool
fs_visitor::register_coalesce()
{
   bool progress = false;
   int if_depth = 0;
   int loop_depth = 0;

   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      /* Make sure that we dominate the instructions we're going to
       * scan for interfering with our coalescing, or we won't have
       * scanned enough to see if anything interferes with our
       * coalescing.  We don't dominate the following instructions if
       * we're in a loop or an if block.
       */
      switch (inst->opcode) {
      case BRW_OPCODE_DO:
	 loop_depth++;
	 break;
      case BRW_OPCODE_WHILE:
	 loop_depth--;
	 break;
      case BRW_OPCODE_IF:
	 if_depth++;
	 break;
      case BRW_OPCODE_ENDIF:
	 if_depth--;
	 break;
a2102 96
      if (loop_depth || if_depth)
	 continue;

      if (inst->opcode != BRW_OPCODE_MOV ||
	  inst->is_partial_write() ||
	  inst->saturate ||
	  inst->dst.file != GRF || (inst->src[0].file != GRF &&
				    inst->src[0].file != UNIFORM)||
	  inst->dst.type != inst->src[0].type)
	 continue;

      bool has_source_modifiers = (inst->src[0].abs ||
                                   inst->src[0].negate ||
                                   inst->src[0].smear != -1 ||
                                   inst->src[0].file == UNIFORM);

      /* Found a move of a GRF to a GRF.  Let's see if we can coalesce
       * them: check for no writes to either one until the exit of the
       * program.
       */
      bool interfered = false;

      for (fs_inst *scan_inst = (fs_inst *)inst->next;
	   !scan_inst->is_tail_sentinel();
	   scan_inst = (fs_inst *)scan_inst->next) {
	 if (scan_inst->dst.file == GRF) {
	    if (scan_inst->overwrites_reg(inst->dst) ||
                scan_inst->overwrites_reg(inst->src[0])) {
	       interfered = true;
	       break;
	    }
	 }

         if (has_source_modifiers) {
            for (int i = 0; i < 3; i++) {
               if (scan_inst->src[i].file == GRF &&
                   scan_inst->src[i].reg == inst->dst.reg &&
                   scan_inst->src[i].reg_offset == inst->dst.reg_offset &&
                   inst->dst.type != scan_inst->src[i].type)
               {
                 interfered = true;
                 break;
               }
            }
         }


	 /* The gen6 MATH instruction can't handle source modifiers or
	  * unusual register regions, so avoid coalescing those for
	  * now.  We should do something more specific.
	  */
	 if (has_source_modifiers && !can_do_source_mods(scan_inst)) {
            interfered = true;
	    break;
	 }

	 /* The accumulator result appears to get used for the
	  * conditional modifier generation.  When negating a UD
	  * value, there is a 33rd bit generated for the sign in the
	  * accumulator value, so now you can't check, for example,
	  * equality with a 32-bit value.  See piglit fs-op-neg-uint.
	  */
	 if (scan_inst->conditional_mod &&
	     inst->src[0].negate &&
	     inst->src[0].type == BRW_REGISTER_TYPE_UD) {
	    interfered = true;
	    break;
	 }
      }
      if (interfered) {
	 continue;
      }

      /* Rewrite the later usage to point at the source of the move to
       * be removed.
       */
      for (fs_inst *scan_inst = inst;
	   !scan_inst->is_tail_sentinel();
	   scan_inst = (fs_inst *)scan_inst->next) {
	 for (int i = 0; i < 3; i++) {
	    if (scan_inst->src[i].file == GRF &&
		scan_inst->src[i].reg == inst->dst.reg &&
		scan_inst->src[i].reg_offset == inst->dst.reg_offset) {
	       fs_reg new_src = inst->src[0];
               if (scan_inst->src[i].abs) {
                  new_src.negate = 0;
                  new_src.abs = 1;
               }
	       new_src.negate ^= scan_inst->src[i].negate;
	       scan_inst->src[i] = new_src;
	    }
	 }
      }

      inst->remove();
      progress = true;
a2104 3
   if (progress)
      live_intervals_valid = false;

a2107 1

d2126 3
a2128 1
	  inst->src[0].abs || inst->src[0].negate || inst->src[0].smear != -1)
d2248 1
a2248 1
	 if (scan_inst->mlen > 0) {
d2267 1
a2267 1
      live_intervals_valid = false;
d2310 1
a2310 1
      if (inst->mlen > 0) {
d2338 1
a2338 1
      live_intervals_valid = false;
d2347 1
a2347 1
   bool inst_16wide = (dispatch_width > 8 &&
d2366 1
a2366 1
         if (inst_16wide)
d2409 1
a2409 1
        scan_inst != NULL;
d2424 1
a2424 1
      bool scan_inst_16wide = (dispatch_width > 8 &&
d2441 1
a2441 1
               if (scan_inst_16wide)
d2536 2
d2548 1
d2551 3
d2611 1
a2611 1
         this->live_intervals_valid = false;
d2625 16
d2646 1
a2646 1
      printf("(%cf0.%d) ",
d2651 1
a2651 1
   printf("%s", brw_instruction_name(inst->opcode));
d2653 1
a2653 1
      printf(".sat");
d2655 1
a2655 1
      printf(".cmod");
d2660 1
a2660 1
         printf(".f0.%d\n", inst->flag_subreg);
d2663 1
a2663 1
   printf(" ");
d2668 5
a2672 3
      printf("vgrf%d", inst->dst.reg);
      if (inst->dst.reg_offset)
         printf("+%d", inst->dst.reg_offset);
d2675 1
a2675 1
      printf("m%d", inst->dst.reg);
d2678 1
a2678 1
      printf("(null)");
d2681 28
a2708 1
      printf("***u%d***", inst->dst.reg);
d2711 1
a2711 1
      printf("???");
d2714 1
a2714 1
   printf(", ");
d2716 1
a2716 1
   for (int i = 0; i < 3; i++) {
d2718 1
a2718 1
         printf("-");
d2720 1
a2720 1
         printf("|");
d2723 5
a2727 3
         printf("vgrf%d", inst->src[i].reg);
         if (inst->src[i].reg_offset)
            printf("+%d", inst->src[i].reg_offset);
d2730 1
a2730 1
         printf("***m%d***", inst->src[i].reg);
d2733 8
a2740 3
         printf("u%d", inst->src[i].reg);
         if (inst->src[i].reg_offset)
            printf(".%d", inst->src[i].reg_offset);
d2743 1
a2743 1
         printf("(null)");
d2748 1
a2748 1
            printf("%ff", inst->src[i].imm.f);
d2751 1
a2751 1
            printf("%dd", inst->src[i].imm.i);
d2754 1
a2754 1
            printf("%uu", inst->src[i].imm.u);
d2757 1
a2757 1
            printf("???");
d2761 33
d2795 1
a2795 1
         printf("???");
d2799 5
a2803 1
         printf("|");
d2805 2
a2806 2
      if (i < 3)
         printf(", ");
d2809 1
a2809 1
   printf(" ");
d2812 1
a2812 1
      printf("1sthalf ");
d2815 1
a2815 1
      printf("2ndhalf ");
d2817 1
a2817 1
   printf("\n");
d2836 1
a2836 1
					   fs_reg reg)
d2883 1
a2883 1
         /* R28: interpolated depth if not 8-wide. */
d2892 1
a2892 1
         /* R30: interpolated W if not 8-wide. */
d2896 2
d2899 17
a2915 1
   /* R32-: bary for 32-pixel. */
d2923 64
d2991 3
a2993 1
   uint32_t orig_nr_params = c->prog_data.nr_params;
d3007 6
a3012 4
      if (brw->gen < 6)
	 emit_interpolation_setup_gen4();
      else
	 emit_interpolation_setup_gen6();
d3026 1
a3026 1
         foreach_list(node, &*shader->ir) {
d3049 4
a3052 1
      setup_pull_constants();
d3065 6
a3070 4
	 progress = dead_code_eliminate() || progress;
	 progress = dead_code_eliminate_local() || progress;
	 progress = register_coalesce() || progress;
	 progress = register_coalesce_2() || progress;
a3073 4
      remove_dead_constants();

      schedule_instructions(false);

d3079 21
a3099 5
      if (0) {
	 /* Debug of register spilling: Go spill everything. */
	 for (int i = 0; i < virtual_grf_count; i++) {
	    spill_reg(i);
	 }
d3102 17
a3118 7
      if (0)
	 assign_regs_trivial();
      else {
	 while (!assign_regs()) {
	    if (failed)
	       break;
	 }
a3121 1
   assert(force_sechalf_stack == 0);
d3132 2
a3133 1
   schedule_instructions(true);
d3135 1
a3135 1
   if (dispatch_width == 8) {
d3137 1
a3137 1
   } else {
a3139 5
      /* Make sure we didn't try to sneak in an extra uniform */
      assert(orig_nr_params == c->prog_data.nr_params);
      (void) orig_nr_params;
   }

d3157 1
a3157 1
   float start_time = 0;
d3169 2
a3170 11
   if (unlikely(INTEL_DEBUG & DEBUG_WM)) {
      if (prog) {
         printf("GLSL IR for native fragment shader %d:\n", prog->Name);
         _mesa_print_ir(shader->ir, NULL);
         printf("\n\n");
      } else {
         printf("ARB_fragment_program %d ir for native fragment shader\n",
                fp->Base.Id);
         _mesa_print_program(&fp->Base);
      }
   }
d3189 10
a3198 6
   bool no16 = INTEL_DEBUG & DEBUG_NO16;
   if (brw->gen >= 5 && c->prog_data.nr_pull_params == 0 && likely(!no16)) {
      v2.import_uniforms(&v);
      if (!v2.run()) {
         perf_debug("16-wide shader failed to compile, falling back to "
                    "8-wide at a 10-20%% performance cost: %s", v2.fail_msg);
d3200 2
a3201 1
         simd16_instructions = &v2.instructions;
d3205 10
a3214 6
   c->prog_data.dispatch_width = 8;

   fs_generator g(brw, c, prog, fp, v.dual_src_output.file != BAD_FILE);
   const unsigned *generated = g.generate_assembly(&v.instructions,
                                                   simd16_instructions,
                                                   final_assembly_size);
d3227 1
a3227 1
   return generated;
d3258 3
a3260 12
   if (brw->gen < 6)
      key.input_slots_valid |= BITFIELD64_BIT(VARYING_SLOT_POS);

   for (int i = 0; i < VARYING_SLOT_MAX; i++) {
      if (!(fp->Base.InputsRead & BITFIELD64_BIT(i)))
	 continue;

      if (brw->gen < 6) {
         if (_mesa_varying_slot_in_fs((gl_varying_slot) i))
            key.input_slots_valid |= BITFIELD64_BIT(i);
      }
   }
d3264 2
a3265 1
   for (int i = 0; i < MAX_SAMPLERS; i++) {
d3280 4
d3285 2
a3286 1
      key.render_to_fbo = _mesa_is_user_fbo(ctx->DrawBuffer);
d3289 5
a3293 1
   key.nr_color_regions = 1;
d3297 1
a3297 1
   uint32_t old_prog_offset = brw->wm.prog_offset;
d3302 1
a3302 1
   brw->wm.prog_offset = old_prog_offset;
@


1.4
log
@Merge Mesa 9.2.1
@
text
@d2923 1
a2923 1
      if (fp->UsesKill) {
d2946 3
@


1.3
log
@Merge Mesa 9.2.0
@
text
@d1361 5
a1365 1
         split_grf[inst->src[0].reg] = false;
@


1.2
log
@Upate to libGL 7.11.2

Tested by jsg@@, matthieu@@ and ajacoutot@@, ok mattieu@@
@
text
@d35 1
d39 1
a48 1
#include "brw_shader.h"
d50 1
a50 2
#include "../glsl/glsl_types.h"
#include "../glsl/ir_print_visitor.h"
d52 406
a457 1
#define MAX_INSTRUCTION (1 << 30)
d483 3
a485 1
   default:
d487 62
a548 1
      return 0;
d550 49
d624 31
d682 16
d710 13
a722 11
   case FS_OPCODE_RCP:
   case FS_OPCODE_RSQ:
   case FS_OPCODE_SQRT:
   case FS_OPCODE_EXP2:
   case FS_OPCODE_LOG2:
   case FS_OPCODE_SIN:
   case FS_OPCODE_COS:
      return 1 * c->dispatch_width / 8;
   case FS_OPCODE_POW:
      return 2 * c->dispatch_width / 8;
   case FS_OPCODE_TEX:
d724 6
a729 2
   case FS_OPCODE_TXD:
   case FS_OPCODE_TXL:
d733 1
a733 1
   case FS_OPCODE_PULL_CONSTANT_LOAD:
d736 2
d749 1
a749 1
   if (virtual_grf_array_size <= virtual_grf_next) {
a755 3

      /* This slot is always unused. */
      virtual_grf_sizes[0] = 0;
d757 2
a758 2
   virtual_grf_sizes[virtual_grf_next] = size;
   return virtual_grf_next++;
d762 1
a762 1
fs_reg::fs_reg(enum register_file file, int hw_reg)
d766 1
a766 1
   this->hw_reg = hw_reg;
d771 1
a771 1
fs_reg::fs_reg(enum register_file file, int hw_reg, uint32_t type)
d775 1
a775 1
   this->hw_reg = hw_reg;
d814 1
a814 1
fs_visitor::import_uniforms(struct hash_table *src_variable_ht)
d816 1
a816 1
   hash_table_call_foreach(src_variable_ht,
d819 2
d828 2
a829 2
int
fs_visitor::setup_uniform_values(int loc, const glsl_type *type)
d831 1
a831 1
   unsigned int offset = 0;
d833 9
a841 4
   if (type->is_matrix()) {
      const glsl_type *column = glsl_type::get_instance(GLSL_TYPE_FLOAT,
							type->vector_elements,
							1);
d843 5
a847 2
      for (unsigned int i = 0; i < type->matrix_columns; i++) {
	 offset += setup_uniform_values(loc + offset, column);
d850 3
a852 2
      return offset;
   }
d854 3
a856 30
   switch (type->base_type) {
   case GLSL_TYPE_FLOAT:
   case GLSL_TYPE_UINT:
   case GLSL_TYPE_INT:
   case GLSL_TYPE_BOOL:
      for (unsigned int i = 0; i < type->vector_elements; i++) {
	 unsigned int param = c->prog_data.nr_params++;

	 assert(param < ARRAY_SIZE(c->prog_data.param));

	 switch (type->base_type) {
	 case GLSL_TYPE_FLOAT:
	    c->prog_data.param_convert[param] = PARAM_NO_CONVERT;
	    break;
	 case GLSL_TYPE_UINT:
	    c->prog_data.param_convert[param] = PARAM_CONVERT_F2U;
	    break;
	 case GLSL_TYPE_INT:
	    c->prog_data.param_convert[param] = PARAM_CONVERT_F2I;
	    break;
	 case GLSL_TYPE_BOOL:
	    c->prog_data.param_convert[param] = PARAM_CONVERT_F2B;
	    break;
	 default:
	    assert(!"not reached");
	    c->prog_data.param_convert[param] = PARAM_NO_CONVERT;
	    break;
	 }
	 this->param_index[param] = loc;
	 this->param_offset[param] = i;
d858 1
a858 1
      return 1;
d860 4
a863 21
   case GLSL_TYPE_STRUCT:
      for (unsigned int i = 0; i < type->length; i++) {
	 offset += setup_uniform_values(loc + offset,
					type->fields.structure[i].type);
      }
      return offset;

   case GLSL_TYPE_ARRAY:
      for (unsigned int i = 0; i < type->length; i++) {
	 offset += setup_uniform_values(loc + offset, type->fields.array);
      }
      return offset;

   case GLSL_TYPE_SAMPLER:
      /* The sampler takes up a slot, but we don't use any values from it. */
      return 1;

   default:
      assert(!"not reached");
      return 0;
   }
d895 2
a896 5
	 c->prog_data.param_convert[c->prog_data.nr_params] =
	    PARAM_NO_CONVERT;
	 this->param_index[c->prog_data.nr_params] = index;
	 this->param_offset[c->prog_data.nr_params] = swiz;
	 c->prog_data.nr_params++;
d910 1
a910 1
      emit(BRW_OPCODE_MOV, wpos, this->pixel_x);
d912 1
a912 1
      emit(BRW_OPCODE_ADD, wpos, this->pixel_x, fs_reg(0.5f));
d918 1
a918 1
      emit(BRW_OPCODE_MOV, wpos, this->pixel_y);
d928 1
a928 1
      emit(BRW_OPCODE_ADD, wpos, pixel_y, fs_reg(offset));
d933 2
a934 3
   if (intel->gen >= 6) {
      emit(BRW_OPCODE_MOV, wpos,
	   fs_reg(brw_vec8_grf(c->source_depth_reg, 0)));
d936 4
a939 2
      emit(FS_OPCODE_LINTERP, wpos, this->delta_x, this->delta_y,
	   interp_reg(FRAG_ATTRIB_WPOS, 2));
d949 30
d983 1
a983 2
   /* Interpolation is always in floating point regs. */
   reg->type = BRW_REGISTER_TYPE_F;
d1000 3
d1015 1
a1015 4
	 bool is_gl_Color =
	    location == FRAG_ATTRIB_COL0 || location == FRAG_ATTRIB_COL1;

	 if (c->key.flat_shade && is_gl_Color) {
d1023 1
d1028 1
a1028 1
	    /* Perspective interpolation case. */
d1030 24
a1053 3
	       struct brw_reg interp = interp_reg(location, k);
	       emit(FS_OPCODE_LINTERP, attr,
		    this->delta_x, this->delta_y, fs_reg(interp));
a1056 7
	    if (intel->gen < 6) {
	       attr.reg_offset -= type->vector_elements;
	       for (unsigned int k = 0; k < type->vector_elements; k++) {
		  emit(BRW_OPCODE_MUL, attr, attr, this->pixel_w);
		  attr.reg_offset++;
	       }
	    }
d1071 1
a1071 1
   if (intel->gen >= 6) {
d1082 1
a1082 4
      fs_inst *inst = emit(BRW_OPCODE_CMP, *reg,
			   fs_reg(r1_6ud),
			   fs_reg(1u << 31));
      inst->conditional_mod = BRW_CONDITIONAL_L;
d1089 27
d1117 1
a1117 1
fs_visitor::emit_math(fs_opcodes opcode, fs_reg dst, fs_reg src)
d1120 7
a1126 7
   case FS_OPCODE_RCP:
   case FS_OPCODE_RSQ:
   case FS_OPCODE_SQRT:
   case FS_OPCODE_EXP2:
   case FS_OPCODE_LOG2:
   case FS_OPCODE_SIN:
   case FS_OPCODE_COS:
d1138 1
a1138 1
    * The hardware ignores source modifiers (negate and abs) on math
d1141 2
a1142 7
   if (intel->gen >= 6 && (src.file == UNIFORM ||
			   src.abs ||
			   src.negate)) {
      fs_reg expanded = fs_reg(this, glsl_type::float_type);
      emit(BRW_OPCODE_MOV, expanded, src);
      src = expanded;
   }
d1146 1
a1146 1
   if (intel->gen < 6) {
d1148 1
a1148 1
      inst->mlen = c->dispatch_width / 8;
d1155 1
a1155 1
fs_visitor::emit_math(fs_opcodes opcode, fs_reg dst, fs_reg src0, fs_reg src1)
d1160 12
a1171 1
   assert(opcode == FS_OPCODE_POW);
d1173 8
a1180 2
   if (intel->gen >= 6) {
      /* Can't do hstride == 0 args to gen6 math, so expand it out.
d1182 5
a1186 2
       * The hardware ignores source modifiers (negate and abs) on math
       * instructions, so we also move to a temp to set those up.
d1188 3
a1190 5
      if (src0.file == UNIFORM || src0.abs || src0.negate) {
	 fs_reg expanded = fs_reg(this, glsl_type::float_type);
	 emit(BRW_OPCODE_MOV, expanded, src0);
	 src0 = expanded;
      }
d1192 2
a1193 10
      if (src1.file == UNIFORM || src1.abs || src1.negate) {
	 fs_reg expanded = fs_reg(this, glsl_type::float_type);
	 emit(BRW_OPCODE_MOV, expanded, src1);
	 src1 = expanded;
      }

      inst = emit(opcode, dst, src0, src1);
   } else {
      emit(BRW_OPCODE_MOV, fs_reg(MRF, base_mrf + 1), src1);
      inst = emit(opcode, dst, src0, reg_null_f);
d1196 1
a1196 1
      inst->mlen = 2 * c->dispatch_width / 8;
a1200 19
/**
 * To be called after the last _mesa_add_state_reference() call, to
 * set up prog_data.param[] for assign_curb_setup() and
 * setup_pull_constants().
 */
void
fs_visitor::setup_paramvalues_refs()
{
   if (c->dispatch_width != 8)
      return;

   /* Set up the pointers to ParamValues now that that array is finalized. */
   for (unsigned int i = 0; i < c->prog_data.nr_params; i++) {
      c->prog_data.param[i] =
	 fp->Base.Parameters->ParameterValues[this->param_index[i]] +
	 this->param_offset[i];
   }
}

d1205 1
a1205 1
   if (c->dispatch_width == 8) {
d1212 2
a1213 2
   foreach_iter(exec_list_iterator, iter, this->instructions) {
      fs_inst *inst = (fs_inst *)iter.get();
d1217 1
a1217 1
	    int constant_nr = inst->src[i].hw_reg + inst->src[i].reg_offset;
d1222 1
a1222 1
	    inst->src[i].file = FIXED_HW_REG;
d1232 1
a1232 1
   for (unsigned int i = 0; i < FRAG_ATTRIB_MAX; i++) {
d1238 2
a1239 2
   if (intel->gen >= 6) {
      for (unsigned int i = 0; i < FRAG_ATTRIB_MAX; i++) {
d1246 15
a1260 13
      for (unsigned int i = 0; i < VERT_RESULT_MAX; i++) {
	 if (c->key.vp_outputs_written & BITFIELD64_BIT(i)) {
	    int fp_index;

	    if (i >= VERT_RESULT_VAR0)
	       fp_index = i - (VERT_RESULT_VAR0 - FRAG_ATTRIB_VAR0);
	    else if (i <= VERT_RESULT_TEX7)
	       fp_index = i;
	    else
	       fp_index = -1;

	    if (fp_index >= 0)
	       urb_setup[fp_index] = urb_next++;
d1263 9
d1286 2
a1287 2
   foreach_iter(exec_list_iterator, iter, this->instructions) {
      fs_inst *inst = (fs_inst *)iter.get();
d1290 1
a1290 1
	 assert(inst->src[2].file == FIXED_HW_REG);
d1295 1
a1295 1
	 assert(inst->src[0].file == FIXED_HW_REG);
d1324 1
a1324 1
   int num_vars = this->virtual_grf_next;
d1336 9
a1344 3
   if (brw->has_pln) {
      /* PLN opcodes rely on the delta_xy being contiguous. */
      split_grf[this->delta_x.reg] = false;
d1347 2
a1348 2
   foreach_iter(exec_list_iterator, iter, this->instructions) {
      fs_inst *inst = (fs_inst *)iter.get();
d1350 4
a1353 2
      /* Texturing produces 4 contiguous registers, so no splitting. */
      if (inst->is_tex()) {
d1356 7
d1380 2
a1381 2
   foreach_iter(exec_list_iterator, iter, this->instructions) {
      fs_inst *inst = (fs_inst *)iter.get();
d1404 246
d1667 1
a1667 1
   if (c->dispatch_width == 16) {
d1675 1
a1675 2
   int pull_uniform_base = max_uniform_components;
   int pull_uniform_count = c->prog_data.nr_params - pull_uniform_base;
d1677 26
a1702 2
   foreach_iter(exec_list_iterator, iter, this->instructions) {
      fs_inst *inst = (fs_inst *)iter.get();
d1708 3
a1710 2
	 int uniform_nr = inst->src[i].hw_reg + inst->src[i].reg_offset;
	 if (uniform_nr < pull_uniform_base)
d1713 2
d1716 5
a1720 3
	 fs_inst *pull = new(mem_ctx) fs_inst(FS_OPCODE_PULL_CONSTANT_LOAD,
					      dst);
	 pull->offset = ((uniform_nr - pull_uniform_base) * 4) & ~15;
a1722 2
	 pull->base_mrf = 14;
	 pull->mlen = 1;
d1729 1
a1729 1
	 inst->src[i].smear = (uniform_nr - pull_uniform_base) & 3;
d1732 31
d1764 16
a1779 4
   for (int i = 0; i < pull_uniform_count; i++) {
      c->prog_data.pull_param[i] = c->prog_data.param[pull_uniform_base + i];
      c->prog_data.pull_param_convert[i] =
	 c->prog_data.param_convert[pull_uniform_base + i];
d1781 2
a1782 2
   c->prog_data.nr_params -= pull_uniform_count;
   c->prog_data.nr_pull_params = pull_uniform_count;
d1785 6
a1790 2
void
fs_visitor::calculate_live_intervals()
d1792 7
a1798 5
   int num_vars = this->virtual_grf_next;
   int *def = ralloc_array(mem_ctx, int, num_vars);
   int *use = ralloc_array(mem_ctx, int, num_vars);
   int loop_depth = 0;
   int loop_start = 0;
d1800 7
a1806 2
   if (this->live_intervals_valid)
      return;
d1808 1
a1808 3
   for (int i = 0; i < num_vars; i++) {
      def[i] = MAX_INSTRUCTION;
      use[i] = -1;
d1811 11
a1821 9
   int ip = 0;
   foreach_iter(exec_list_iterator, iter, this->instructions) {
      fs_inst *inst = (fs_inst *)iter.get();

      if (inst->opcode == BRW_OPCODE_DO) {
	 if (loop_depth++ == 0)
	    loop_start = ip;
      } else if (inst->opcode == BRW_OPCODE_WHILE) {
	 loop_depth--;
d1823 5
a1827 30
	 if (loop_depth == 0) {
	    /* Patches up the use of vars marked for being live across
	     * the whole loop.
	     */
	    for (int i = 0; i < num_vars; i++) {
	       if (use[i] == loop_start) {
		  use[i] = ip;
	       }
	    }
	 }
      } else {
	 for (unsigned int i = 0; i < 3; i++) {
	    if (inst->src[i].file == GRF && inst->src[i].reg != 0) {
	       int reg = inst->src[i].reg;

	       if (!loop_depth) {
		  use[reg] = ip;
	       } else {
		  def[reg] = MIN2(loop_start, def[reg]);
		  use[reg] = loop_start;

		  /* Nobody else is going to go smash our start to
		   * later in the loop now, because def[reg] now
		   * points before the bb header.
		   */
	       }
	    }
	 }
	 if (inst->dst.file == GRF && inst->dst.reg != 0) {
	    int reg = inst->dst.reg;
d1829 4
a1832 7
	    if (!loop_depth) {
	       def[reg] = MIN2(def[reg], ip);
	    } else {
	       def[reg] = MIN2(def[reg], loop_start);
	    }
	 }
      }
d1834 2
a1835 1
      ip++;
d1837 25
d1863 7
a1869 4
   ralloc_free(this->virtual_grf_def);
   ralloc_free(this->virtual_grf_use);
   this->virtual_grf_def = def;
   this->virtual_grf_use = use;
d1871 1
a1871 1
   this->live_intervals_valid = true;
d1875 2
a1876 2
 * Attempts to move immediate constants into the immediate
 * constant slot of following instructions.
d1878 4
a1881 2
 * Immediate constants are a bit tricky -- they have to be in the last
 * operand slot, you can't do abs/negate on them,
a1882 1

d1884 1
a1884 1
fs_visitor::propagate_constants()
d1886 1
d1889 1
a1889 1
   calculate_live_intervals();
d1891 2
a1892 2
   foreach_iter(exec_list_iterator, iter, this->instructions) {
      fs_inst *inst = (fs_inst *)iter.get();
d1894 7
a1900 7
      if (inst->opcode != BRW_OPCODE_MOV ||
	  inst->predicated ||
	  inst->dst.file != GRF || inst->src[0].file != IMM ||
	  inst->dst.type != inst->src[0].type ||
	  (c->dispatch_width == 16 &&
	   (inst->force_uncompressed || inst->force_sechalf)))
	 continue;
d1902 16
a1917 5
      /* Don't bother with cases where we should have had the
       * operation on the constant folded in GLSL already.
       */
      if (inst->saturate)
	 continue;
d1919 2
a1920 2
      /* Found a move of a constant to a GRF.  Find anything else using the GRF
       * before it's written, and replace it with the constant if we can.
d1922 42
a1963 90
      exec_list_iterator scan_iter = iter;
      scan_iter.next();
      for (; scan_iter.has_next(); scan_iter.next()) {
	 fs_inst *scan_inst = (fs_inst *)scan_iter.get();

	 if (scan_inst->opcode == BRW_OPCODE_DO ||
	     scan_inst->opcode == BRW_OPCODE_WHILE ||
	     scan_inst->opcode == BRW_OPCODE_ELSE ||
	     scan_inst->opcode == BRW_OPCODE_ENDIF) {
	    break;
	 }

	 for (int i = 2; i >= 0; i--) {
	    if (scan_inst->src[i].file != GRF ||
		scan_inst->src[i].reg != inst->dst.reg ||
		scan_inst->src[i].reg_offset != inst->dst.reg_offset)
	       continue;

	    /* Don't bother with cases where we should have had the
	     * operation on the constant folded in GLSL already.
	     */
	    if (scan_inst->src[i].negate || scan_inst->src[i].abs)
	       continue;

	    switch (scan_inst->opcode) {
	    case BRW_OPCODE_MOV:
	       scan_inst->src[i] = inst->src[0];
	       progress = true;
	       break;

	    case BRW_OPCODE_MUL:
	    case BRW_OPCODE_ADD:
	       if (i == 1) {
		  scan_inst->src[i] = inst->src[0];
		  progress = true;
	       } else if (i == 0 && scan_inst->src[1].file != IMM) {
		  /* Fit this constant in by commuting the operands */
		  scan_inst->src[0] = scan_inst->src[1];
		  scan_inst->src[1] = inst->src[0];
		  progress = true;
	       }
	       break;

	    case BRW_OPCODE_CMP:
	       if (i == 1) {
		  scan_inst->src[i] = inst->src[0];
		  progress = true;
	       } else if (i == 0 && scan_inst->src[1].file != IMM) {
		  uint32_t new_cmod;

		  new_cmod = brw_swap_cmod(scan_inst->conditional_mod);
		  if (new_cmod != ~0u) {
		     /* Fit this constant in by swapping the operands and
		      * flipping the test
		      */
		     scan_inst->src[0] = scan_inst->src[1];
		     scan_inst->src[1] = inst->src[0];
		     scan_inst->conditional_mod = new_cmod;
		     progress = true;
		  }
	       }
	       break;

	    case BRW_OPCODE_SEL:
	       if (i == 1) {
		  scan_inst->src[i] = inst->src[0];
		  progress = true;
	       } else if (i == 0 && scan_inst->src[1].file != IMM) {
		  scan_inst->src[0] = scan_inst->src[1];
		  scan_inst->src[1] = inst->src[0];

		  /* If this was predicated, flipping operands means
		   * we also need to flip the predicate.
		   */
		  if (scan_inst->conditional_mod == BRW_CONDITIONAL_NONE) {
		     scan_inst->predicate_inverse =
			!scan_inst->predicate_inverse;
		  }
		  progress = true;
	       }
	       break;
	    }
	 }

	 if (scan_inst->dst.file == GRF &&
	     scan_inst->dst.reg == inst->dst.reg &&
	     (scan_inst->dst.reg_offset == inst->dst.reg_offset ||
	      scan_inst->is_tex())) {
	    break;
	 }
d1967 2
d1970 1
a1970 1
       this->live_intervals_valid = false;
d1974 1
d1976 3
a1978 4
 * Must be called after calculate_live_intervales() to remove unused
 * writes to registers -- register allocation will fail otherwise
 * because something deffed but not used won't be considered to
 * interfere with other regs.
d1981 1
a1981 1
fs_visitor::dead_code_eliminate()
a1983 1
   int pc = 0;
d1987 24
a2010 2
   foreach_iter(exec_list_iterator, iter, this->instructions) {
      fs_inst *inst = (fs_inst *)iter.get();
d2012 12
a2023 3
      if (inst->dst.file == GRF && this->virtual_grf_use[inst->dst.reg] <= pc) {
	 inst->remove();
	 progress = true;
d2026 1
a2026 2
      pc++;
   }
d2028 19
a2046 1
   if (progress)
d2049 4
d2063 2
a2064 2
   foreach_iter(exec_list_iterator, iter, this->instructions) {
      fs_inst *inst = (fs_inst *)iter.get();
d2085 2
d2092 1
a2092 1
	  inst->predicated ||
d2094 2
a2095 1
	  inst->dst.file != GRF || inst->src[0].file != GRF ||
d2099 4
a2102 1
      bool has_source_modifiers = inst->src[0].abs || inst->src[0].negate;
a2108 4
      exec_list_iterator scan_iter = iter;
      scan_iter.next();
      for (; scan_iter.has_next(); scan_iter.next()) {
	 fs_inst *scan_inst = (fs_inst *)scan_iter.get();
d2110 3
d2114 2
a2115 9
	    if (scan_inst->dst.reg == inst->dst.reg &&
		(scan_inst->dst.reg_offset == inst->dst.reg_offset ||
		 scan_inst->is_tex())) {
	       interfered = true;
	       break;
	    }
	    if (scan_inst->dst.reg == inst->src[0].reg &&
		(scan_inst->dst.reg_offset == inst->src[0].reg_offset ||
		 scan_inst->is_tex())) {
d2121 28
a2148 2
	 /* The gen6 MATH instruction can't handle source modifiers, so avoid
	  * coalescing those for now.  We should do something more specific.
d2150 3
a2152 1
	 if (intel->gen >= 6 && scan_inst->is_math() && has_source_modifiers) {
d2164 3
a2166 4
      for (exec_list_iterator scan_iter = iter; scan_iter.has_next();
	   scan_iter.next()) {
	 fs_inst *scan_inst = (fs_inst *)scan_iter.get();

d2171 7
a2177 5
	       scan_inst->src[i].reg = inst->src[0].reg;
	       scan_inst->src[i].reg_offset = inst->src[0].reg_offset;
	       scan_inst->src[i].abs |= inst->src[0].abs;
	       scan_inst->src[i].negate ^= inst->src[0].negate;
	       scan_inst->src[i].smear = inst->src[0].smear;
d2201 2
a2202 2
   foreach_iter(exec_list_iterator, iter, this->instructions) {
      fs_inst *inst = (fs_inst *)iter.get();
d2208 1
a2208 1
	  inst->predicated ||
d2217 1
a2217 1
      int mrf_low = inst->dst.hw_reg & ~BRW_MRF_COMPR4;
d2219 1
a2219 1
      if (inst->dst.hw_reg & BRW_MRF_COMPR4) {
d2221 1
a2221 1
      } else if (c->dispatch_width == 16 &&
d2231 1
a2231 1
      if (this->virtual_grf_use[inst->src[0].reg] > ip)
d2247 2
a2248 9
	    if (scan_inst->is_tex()) {
	       /* texturing writes several continuous regs, so we can't
		* compute-to-mrf that.
		*/
	       break;
	    }

	    /* If it's predicated, it (probably) didn't populate all
	     * the channels.  We might be able to rewrite everything
d2252 1
a2252 1
	    if (scan_inst->predicated)
d2255 5
a2259 7
	    /* If it's half of register setup and not the same half as
	     * our MOV we're trying to remove, bail for now.
	     */
	    if (scan_inst->force_uncompressed != inst->force_uncompressed ||
		scan_inst->force_sechalf != inst->force_sechalf) {
	       break;
	    }
d2265 1
a2265 1
	    if (intel->gen >= 6) {
d2277 1
a2277 1
	       scan_inst->dst.hw_reg = inst->dst.hw_reg;
d2285 1
a2285 1
	 /* We don't handle flow control here.  Most computation of
d2289 1
a2289 4
	 if (scan_inst->opcode == BRW_OPCODE_DO ||
	     scan_inst->opcode == BRW_OPCODE_WHILE ||
	     scan_inst->opcode == BRW_OPCODE_ELSE ||
	     scan_inst->opcode == BRW_OPCODE_ENDIF) {
a2290 1
	 }
d2310 1
a2310 1
	    int scan_mrf_low = scan_inst->dst.hw_reg & ~BRW_MRF_COMPR4;
d2313 1
a2313 1
	    if (scan_inst->dst.hw_reg & BRW_MRF_COMPR4) {
d2315 1
a2315 1
	    } else if (c->dispatch_width == 16 &&
d2349 3
d2356 1
a2356 1
 * Walks through basic blocks, locking for repeated MRF writes and
d2366 1
a2366 1
   if (c->dispatch_width == 16)
d2371 2
a2372 2
   foreach_iter(exec_list_iterator, iter, this->instructions) {
      fs_inst *inst = (fs_inst *)iter.get();
d2374 1
a2374 6
      switch (inst->opcode) {
      case BRW_OPCODE_DO:
      case BRW_OPCODE_WHILE:
      case BRW_OPCODE_IF:
      case BRW_OPCODE_ELSE:
      case BRW_OPCODE_ENDIF:
a2375 3
	 continue;
      default:
	 break;
d2380 1
a2380 1
	 fs_inst *prev_inst = last_mrf_move[inst->dst.hw_reg];
d2390 1
a2390 1
	 last_mrf_move[inst->dst.hw_reg] = NULL;
d2415 2
a2416 2
	  !inst->predicated) {
	 last_mrf_move[inst->dst.hw_reg] = inst;
d2420 3
d2426 189
a2614 2
bool
fs_visitor::virtual_grf_interferes(int a, int b)
d2616 2
a2617 2
   int start = MAX2(this->virtual_grf_def[a], this->virtual_grf_def[b]);
   int end = MIN2(this->virtual_grf_use[a], this->virtual_grf_use[b]);
d2619 2
a2620 26
   /* We can't handle dead register writes here, without iterating
    * over the whole instruction stream to find every single dead
    * write to that register to compare to the live interval of the
    * other register.  Just assert that dead_code_eliminate() has been
    * called.
    */
   assert((this->virtual_grf_use[a] != -1 ||
	   this->virtual_grf_def[a] == MAX_INSTRUCTION) &&
	  (this->virtual_grf_use[b] != -1 ||
	   this->virtual_grf_def[b] == MAX_INSTRUCTION));

   /* If the register is used to store 16 values of less than float
    * size (only the case for pixel_[xy]), then we can't allocate
    * another dword-sized thing to that register that would be used in
    * the same instruction.  This is because when the GPU decodes (for
    * example):
    *
    * (declare (in ) vec4 gl_FragCoord@@0x97766a0)
    * add(16)         g6<1>F          g6<8,8,1>UW     0.5F { align1 compr };
    *
    * it's actually processed as:
    * add(8)         g6<1>F          g6<8,8,1>UW     0.5F { align1 };
    * add(8)         g7<1>F          g6.8<8,8,1>UW   0.5F { align1 sechalf };
    *
    * so our second half values in g6 got overwritten in the first
    * half.
d2622 8
a2629 5
   if (c->dispatch_width == 16 && (this->pixel_x.reg == a ||
				   this->pixel_x.reg == b ||
				   this->pixel_y.reg == a ||
				   this->pixel_y.reg == b)) {
      return start <= end;
d2631 56
d2688 11
a2698 1
   return start < end;
d2701 2
a2702 2
bool
fs_visitor::run()
d2704 103
a2806 2
   uint32_t prog_offset_16 = 0;
   uint32_t orig_nr_params = c->prog_data.nr_params;
d2808 27
a2834 1
   brw_wm_payload_setup(brw, c);
d2836 27
a2862 4
   if (c->dispatch_width == 16) {
      /* align to 64 byte boundary. */
      while ((c->func.nr_insn * sizeof(struct brw_instruction)) % 64) {
	 brw_NOP(p);
d2864 1
d2866 21
a2886 2
      /* Save off the start of this 16-wide program in case we succeed. */
      prog_offset_16 = c->func.nr_insn * sizeof(struct brw_instruction);
d2888 2
a2889 1
      brw_set_compression_control(p, BRW_COMPRESSION_COMPRESSED);
d2891 12
d2907 3
d2911 1
a2911 1
      if (intel->gen < 6)
d2916 8
d2927 9
a2935 5
      foreach_iter(exec_list_iterator, iter, *shader->ir) {
	 ir_instruction *ir = (ir_instruction *)iter.get();
	 base_ir = ir;
	 this->result = reg_undef;
	 ir->accept(this);
d2937 1
d2941 2
d2947 1
a2947 1
      setup_paramvalues_refs();
d2954 2
d2958 5
a2962 1
	 progress = propagate_constants() || progress;
d2964 1
a2965 1
	 progress = dead_code_eliminate() || progress;
d2968 5
a2972 1
      schedule_instructions();
d2979 1
a2979 2
	 int virtual_grf_count = virtual_grf_next;
	 for (int i = 1; i < virtual_grf_count; i++) {
d2996 6
d3005 1
a3005 1
   generate_code();
d3007 1
a3007 1
   if (c->dispatch_width == 8) {
a3010 1
      c->prog_data.prog_offset_16 = prog_offset_16;
d3014 1
d3017 7
d3027 1
a3027 1
bool
d3029 16
a3044 11
	       struct gl_shader_program *prog)
{
   struct intel_context *intel = &brw->intel;

   if (!prog)
      return false;

   struct brw_shader *shader =
     (brw_shader *) prog->_LinkedShaders[MESA_SHADER_FRAGMENT];
   if (!shader)
      return false;
d3047 9
a3055 3
      printf("GLSL IR for native fragment shader %d:\n", prog->Name);
      _mesa_print_ir(shader->ir, NULL);
      printf("\n\n");
d3060 6
a3065 1
   c->dispatch_width = 8;
d3067 2
a3068 4
   fs_visitor v(c, prog, shader);
   if (!v.run()) {
      prog->LinkStatus = GL_FALSE;
      prog->InfoLog = ralloc_strdup(prog, v.fail_msg);
d3070 1
a3070 1
      return false;
d3073 11
a3083 5
   if (intel->gen >= 5 && c->prog_data.nr_pull_params == 0) {
      c->dispatch_width = 16;
      fs_visitor v2(c, prog, shader);
      v2.import_uniforms(v.variable_ht);
      v2.run();
d3088 17
a3104 1
   return true;
a3111 2
   struct gl_fragment_program *fp = prog->FragmentProgram;
   struct brw_fragment_program *bfp = brw_fragment_program(fp);
d3113 1
a3113 1
   if (!fp)
d3116 5
d3123 3
a3125 2
   if (fp->UsesKill)
      key.iz_lookup |= IZ_PS_KILL_ALPHATEST_BIT;
d3127 7
a3133 2
   if (fp->Base.OutputsWritten & BITFIELD64_BIT(FRAG_RESULT_DEPTH))
      key.iz_lookup |= IZ_PS_COMPUTES_DEPTH_BIT;
d3135 2
a3136 7
   /* Just assume depth testing. */
   key.iz_lookup |= IZ_DEPTH_TEST_ENABLE_BIT;
   key.iz_lookup |= IZ_DEPTH_WRITE_ENABLE_BIT;

   key.vp_outputs_written |= BITFIELD64_BIT(FRAG_ATTRIB_WPOS);
   for (int i = 0; i < FRAG_ATTRIB_MAX; i++) {
      int vp_index = -1;
d3138 1
d3142 5
a3146 1
      key.proj_attrib_mask |= 1 << i;
d3148 1
a3148 4
      if (i <= FRAG_ATTRIB_TEX7)
	 vp_index = i;
      else if (i >= FRAG_ATTRIB_VAR0)
	 vp_index = i - FRAG_ATTRIB_VAR0 + VERT_RESULT_VAR0;
d3150 9
a3158 2
      if (vp_index >= 0)
	 key.vp_outputs_written |= BITFIELD64_BIT(vp_index);
d3161 2
a3162 8
   key.clamp_fragment_color = true;

   for (int i = 0; i < BRW_MAX_TEX_UNIT; i++) {
      if (fp->Base.ShadowSamplers & (1 << i))
	 key.compare_funcs[i] = GL_LESS;

      /* FINISHME: depth compares might use (0,0,0,W) for example */
      key.tex_swizzles[i] = SWIZZLE_XYZW;
d3165 2
a3166 3
   if (fp->Base.InputsRead & FRAG_BIT_WPOS) {
      key.drawable_height = ctx->DrawBuffer->Height;
      key.render_to_fbo = ctx->DrawBuffer->Name != 0;
@


1.1
log
@Initial revision
@
text
@d22 3
d26 3
a28 3
 * Authors:
 *    Eric Anholt <eric@@anholt.net>
 *
a39 1
#include "program/prog_optimize.h"
d47 1
a49 1
#include "../glsl/ir_optimization.h"
d52 1
a52 1
static struct brw_reg brw_reg_from_fs_reg(class fs_reg *reg);
d54 2
a55 110
struct gl_shader *
brw_new_shader(struct gl_context *ctx, GLuint name, GLuint type)
{
   struct brw_shader *shader;

   shader = rzalloc(NULL, struct brw_shader);
   if (shader) {
      shader->base.Type = type;
      shader->base.Name = name;
      _mesa_init_shader(ctx, &shader->base);
   }

   return &shader->base;
}

struct gl_shader_program *
brw_new_shader_program(struct gl_context *ctx, GLuint name)
{
   struct brw_shader_program *prog;
   prog = rzalloc(NULL, struct brw_shader_program);
   if (prog) {
      prog->base.Name = name;
      _mesa_init_shader_program(ctx, &prog->base);
   }
   return &prog->base;
}

GLboolean
brw_compile_shader(struct gl_context *ctx, struct gl_shader *shader)
{
   if (!_mesa_ir_compile_shader(ctx, shader))
      return GL_FALSE;

   return GL_TRUE;
}

GLboolean
brw_link_shader(struct gl_context *ctx, struct gl_shader_program *prog)
{
   struct brw_context *brw = brw_context(ctx);
   struct intel_context *intel = &brw->intel;

   struct brw_shader *shader =
      (struct brw_shader *)prog->_LinkedShaders[MESA_SHADER_FRAGMENT];
   if (shader != NULL) {
      void *mem_ctx = ralloc_context(NULL);
      bool progress;

      if (shader->ir)
	 ralloc_free(shader->ir);
      shader->ir = new(shader) exec_list;
      clone_ir_list(mem_ctx, shader->ir, shader->base.ir);

      do_mat_op_to_vec(shader->ir);
      lower_instructions(shader->ir,
			 MOD_TO_FRACT |
			 DIV_TO_MUL_RCP |
			 SUB_TO_ADD_NEG |
			 EXP_TO_EXP2 |
			 LOG_TO_LOG2);

      /* Pre-gen6 HW can only nest if-statements 16 deep.  Beyond this,
       * if-statements need to be flattened.
       */
      if (intel->gen < 6)
	 lower_if_to_cond_assign(shader->ir, 16);

      do_lower_texture_projection(shader->ir);
      do_vec_index_to_cond_assign(shader->ir);
      brw_do_cubemap_normalize(shader->ir);

      do {
	 progress = false;

	 brw_do_channel_expressions(shader->ir);
	 brw_do_vector_splitting(shader->ir);

	 progress = do_lower_jumps(shader->ir, true, true,
				   true, /* main return */
				   false, /* continue */
				   false /* loops */
				   ) || progress;

	 progress = do_common_optimization(shader->ir, true, 32) || progress;

	 progress = lower_noise(shader->ir) || progress;
	 progress =
	    lower_variable_index_to_cond_assign(shader->ir,
						GL_TRUE, /* input */
						GL_TRUE, /* output */
						GL_TRUE, /* temp */
						GL_TRUE /* uniform */
						) || progress;
	 progress = lower_quadop_vector(shader->ir, false) || progress;
      } while (progress);

      validate_ir_tree(shader->ir);

      reparent_ir(shader->ir, shader->ir);
      ralloc_free(mem_ctx);
   }

   if (!_mesa_ir_link_shader(ctx, prog))
      return GL_FALSE;

   return GL_TRUE;
}

static int
type_size(const struct glsl_type *type)
d84 49
d153 1
a153 1
      return 1;
d155 1
a155 1
      return 2;
d158 1
a209 25
int
brw_type_for_base_type(const struct glsl_type *type)
{
   switch (type->base_type) {
   case GLSL_TYPE_FLOAT:
      return BRW_REGISTER_TYPE_F;
   case GLSL_TYPE_INT:
   case GLSL_TYPE_BOOL:
      return BRW_REGISTER_TYPE_D;
   case GLSL_TYPE_UINT:
      return BRW_REGISTER_TYPE_UD;
   case GLSL_TYPE_ARRAY:
   case GLSL_TYPE_STRUCT:
   case GLSL_TYPE_SAMPLER:
      /* These should be overridden with the type of the member when
       * dereferenced into.  BRW_REGISTER_TYPE_UD seems like a likely
       * way to trip up if we don't.
       */
      return BRW_REGISTER_TYPE_UD;
   default:
      assert(!"not reached");
      return BRW_REGISTER_TYPE_F;
   }
}

d216 1
a216 1
   this->reg = v->virtual_grf_alloc(type_size(type));
d227 25
d338 2
a339 1
   const struct gl_builtin_uniform_desc *statevar = NULL;
d341 17
a357 28
   for (unsigned int i = 0; _mesa_builtin_uniform_desc[i].name; i++) {
      statevar = &_mesa_builtin_uniform_desc[i];
      if (strcmp(ir->name, _mesa_builtin_uniform_desc[i].name) == 0)
	 break;
   }

   if (!statevar->name) {
      this->fail = true;
      printf("Failed to find builtin uniform `%s'\n", ir->name);
      return;
   }

   int array_count;
   if (ir->type->is_array()) {
      array_count = ir->type->length;
   } else {
      array_count = 1;
   }

   for (int a = 0; a < array_count; a++) {
      for (unsigned int i = 0; i < statevar->num_elements; i++) {
	 struct gl_builtin_uniform_element *element = &statevar->elements[i];
	 int tokens[STATE_LENGTH];

	 memcpy(tokens, element->tokens, sizeof(element->tokens));
	 if (ir->type->is_array()) {
	    tokens[1] = a;
	 }
d359 5
a363 23
	 /* This state reference has already been setup by ir_to_mesa,
	  * but we'll get the same index back here.
	  */
	 int index = _mesa_add_state_reference(this->fp->Base.Parameters,
					       (gl_state_index *)tokens);

	 /* Add each of the unique swizzles of the element as a
	  * parameter.  This'll end up matching the expected layout of
	  * the array/matrix/structure we're trying to fill in.
	  */
	 int last_swiz = -1;
	 for (unsigned int i = 0; i < 4; i++) {
	    int swiz = GET_SWZ(element->swizzle, i);
	    if (swiz == last_swiz)
	       break;
	    last_swiz = swiz;

	    c->prog_data.param_convert[c->prog_data.nr_params] =
	       PARAM_NO_CONVERT;
	    this->param_index[c->prog_data.nr_params] = index;
	    this->param_offset[c->prog_data.nr_params] = swiz;
	    c->prog_data.nr_params++;
	 }
a372 2
   fs_reg neg_y = this->pixel_y;
   neg_y.negate = true;
d377 1
a377 1
      emit(fs_inst(BRW_OPCODE_MOV, wpos, this->pixel_x));
d379 1
a379 1
      emit(fs_inst(BRW_OPCODE_ADD, wpos, this->pixel_x, fs_reg(0.5f)));
d385 1
a385 1
      emit(fs_inst(BRW_OPCODE_MOV, wpos, this->pixel_y));
d395 1
a395 1
      emit(fs_inst(BRW_OPCODE_ADD, wpos, pixel_y, fs_reg(offset)));
d401 2
a402 2
      emit(fs_inst(BRW_OPCODE_MOV, wpos,
		   fs_reg(brw_vec8_grf(c->source_depth_reg, 0))));
d404 2
a405 2
      emit(fs_inst(FS_OPCODE_LINTERP, wpos, this->delta_x, this->delta_y,
		   interp_reg(FRAG_ATTRIB_WPOS, 2)));
d410 1
a410 1
   emit(fs_inst(BRW_OPCODE_MOV, wpos, this->wpos_w));
d429 1
a429 1
	 this->fail = true;
d457 2
a458 2
	    for (unsigned int c = 0; c < type->vector_elements; c++) {
	       struct brw_reg interp = interp_reg(location, c);
d460 1
a460 1
	       emit(fs_inst(FS_OPCODE_CINTERP, attr, fs_reg(interp)));
d465 4
a468 7
	    for (unsigned int c = 0; c < type->vector_elements; c++) {
	       struct brw_reg interp = interp_reg(location, c);
	       emit(fs_inst(FS_OPCODE_LINTERP,
			    attr,
			    this->delta_x,
			    this->delta_y,
			    fs_reg(interp)));
d472 1
a472 1
	    if (intel->gen < 6 && !(is_gl_Color && c->key.linear_color)) {
d474 2
a475 5
	       for (unsigned int c = 0; c < type->vector_elements; c++) {
		  emit(fs_inst(BRW_OPCODE_MUL,
			       attr,
			       attr,
			       this->pixel_w));
d494 5
a498 11
      emit(fs_inst(BRW_OPCODE_ASR,
		   *reg,
		   fs_reg(retype(brw_vec1_grf(0, 0), BRW_REGISTER_TYPE_D)),
		   fs_reg(15)));
      emit(fs_inst(BRW_OPCODE_NOT,
		   *reg,
		   *reg));
      emit(fs_inst(BRW_OPCODE_AND,
		   *reg,
		   *reg,
		   fs_reg(1)));
d504 3
a506 4
      fs_inst *inst = emit(fs_inst(BRW_OPCODE_CMP,
				   *reg,
				   fs_reg(r1_6ud),
				   fs_reg(1u << 31)));
d508 1
a508 1
      emit(fs_inst(BRW_OPCODE_AND, *reg, *reg, fs_reg(1u)));
d543 1
a543 1
      emit(fs_inst(BRW_OPCODE_MOV, expanded, src));
d547 1
a547 1
   fs_inst *inst = emit(fs_inst(opcode, dst, src));
d551 1
a551 1
      inst->mlen = 1;
d573 1
a573 1
	 emit(fs_inst(BRW_OPCODE_MOV, expanded, src0));
d579 1
a579 1
	 emit(fs_inst(BRW_OPCODE_MOV, expanded, src1));
d583 1
a583 1
      inst = emit(fs_inst(opcode, dst, src0, src1));
d585 2
a586 2
      emit(fs_inst(BRW_OPCODE_MOV, fs_reg(MRF, base_mrf + 1), src1));
      inst = emit(fs_inst(opcode, dst, src0, reg_null_f));
d589 1
a589 1
      inst->mlen = 2;
d594 4
a597 96
void
fs_visitor::visit(ir_variable *ir)
{
   fs_reg *reg = NULL;

   if (variable_storage(ir))
      return;

   if (strcmp(ir->name, "gl_FragColor") == 0) {
      this->frag_color = ir;
   } else if (strcmp(ir->name, "gl_FragData") == 0) {
      this->frag_data = ir;
   } else if (strcmp(ir->name, "gl_FragDepth") == 0) {
      this->frag_depth = ir;
   }

   if (ir->mode == ir_var_in) {
      if (!strcmp(ir->name, "gl_FragCoord")) {
	 reg = emit_fragcoord_interpolation(ir);
      } else if (!strcmp(ir->name, "gl_FrontFacing")) {
	 reg = emit_frontfacing_interpolation(ir);
      } else {
	 reg = emit_general_interpolation(ir);
      }
      assert(reg);
      hash_table_insert(this->variable_ht, reg, ir);
      return;
   }

   if (ir->mode == ir_var_uniform) {
      int param_index = c->prog_data.nr_params;

      if (!strncmp(ir->name, "gl_", 3)) {
	 setup_builtin_uniform_values(ir);
      } else {
	 setup_uniform_values(ir->location, ir->type);
      }

      reg = new(this->mem_ctx) fs_reg(UNIFORM, param_index);
      reg->type = brw_type_for_base_type(ir->type);
   }

   if (!reg)
      reg = new(this->mem_ctx) fs_reg(this, ir->type);

   hash_table_insert(this->variable_ht, reg, ir);
}

void
fs_visitor::visit(ir_dereference_variable *ir)
{
   fs_reg *reg = variable_storage(ir->var);
   this->result = *reg;
}

void
fs_visitor::visit(ir_dereference_record *ir)
{
   const glsl_type *struct_type = ir->record->type;

   ir->record->accept(this);

   unsigned int offset = 0;
   for (unsigned int i = 0; i < struct_type->length; i++) {
      if (strcmp(struct_type->fields.structure[i].name, ir->field) == 0)
	 break;
      offset += type_size(struct_type->fields.structure[i].type);
   }
   this->result.reg_offset += offset;
   this->result.type = brw_type_for_base_type(ir->type);
}

void
fs_visitor::visit(ir_dereference_array *ir)
{
   ir_constant *index;
   int element_size;

   ir->array->accept(this);
   index = ir->array_index->as_constant();

   element_size = type_size(ir->type);
   this->result.type = brw_type_for_base_type(ir->type);

   if (index) {
      assert(this->result.file == UNIFORM ||
	     (this->result.file == GRF &&
	      this->result.reg != 0));
      this->result.reg_offset += index->value.i[0] * element_size;
   } else {
      assert(!"FINISHME: non-constant array element");
   }
}

/* Instruction selection: Produce a MOV.sat instead of
 * MIN(MAX(val, 0), 1) when possible.
a598 42
bool
fs_visitor::try_emit_saturate(ir_expression *ir)
{
   ir_rvalue *sat_val = ir->as_rvalue_to_saturate();

   if (!sat_val)
      return false;

   sat_val->accept(this);
   fs_reg src = this->result;

   this->result = fs_reg(this, ir->type);
   fs_inst *inst = emit(fs_inst(BRW_OPCODE_MOV, this->result, src));
   inst->saturate = true;

   return true;
}

static uint32_t
brw_conditional_for_comparison(unsigned int op)
{
   switch (op) {
   case ir_binop_less:
      return BRW_CONDITIONAL_L;
   case ir_binop_greater:
      return BRW_CONDITIONAL_G;
   case ir_binop_lequal:
      return BRW_CONDITIONAL_LE;
   case ir_binop_gequal:
      return BRW_CONDITIONAL_GE;
   case ir_binop_equal:
   case ir_binop_all_equal: /* same as equal for scalars */
      return BRW_CONDITIONAL_Z;
   case ir_binop_nequal:
   case ir_binop_any_nequal: /* same as nequal for scalars */
      return BRW_CONDITIONAL_NZ;
   default:
      assert(!"not reached: bad operation for comparison");
      return BRW_CONDITIONAL_NZ;
   }
}

d600 1
a600 1
fs_visitor::visit(ir_expression *ir)
d602 1
a602 7
   unsigned int operand;
   fs_reg op[2], temp;
   fs_inst *inst;

   assert(ir->get_num_operands() <= 2);

   if (try_emit_saturate(ir))
a604 1684
   for (operand = 0; operand < ir->get_num_operands(); operand++) {
      ir->operands[operand]->accept(this);
      if (this->result.file == BAD_FILE) {
	 ir_print_visitor v;
	 printf("Failed to get tree for expression operand:\n");
	 ir->operands[operand]->accept(&v);
	 this->fail = true;
      }
      op[operand] = this->result;

      /* Matrix expression operands should have been broken down to vector
       * operations already.
       */
      assert(!ir->operands[operand]->type->is_matrix());
      /* And then those vector operands should have been broken down to scalar.
       */
      assert(!ir->operands[operand]->type->is_vector());
   }

   /* Storage for our result.  If our result goes into an assignment, it will
    * just get copy-propagated out, so no worries.
    */
   this->result = fs_reg(this, ir->type);

   switch (ir->operation) {
   case ir_unop_logic_not:
      /* Note that BRW_OPCODE_NOT is not appropriate here, since it is
       * ones complement of the whole register, not just bit 0.
       */
      emit(fs_inst(BRW_OPCODE_XOR, this->result, op[0], fs_reg(1)));
      break;
   case ir_unop_neg:
      op[0].negate = !op[0].negate;
      this->result = op[0];
      break;
   case ir_unop_abs:
      op[0].abs = true;
      op[0].negate = false;
      this->result = op[0];
      break;
   case ir_unop_sign:
      temp = fs_reg(this, ir->type);

      emit(fs_inst(BRW_OPCODE_MOV, this->result, fs_reg(0.0f)));

      inst = emit(fs_inst(BRW_OPCODE_CMP, reg_null_f, op[0], fs_reg(0.0f)));
      inst->conditional_mod = BRW_CONDITIONAL_G;
      inst = emit(fs_inst(BRW_OPCODE_MOV, this->result, fs_reg(1.0f)));
      inst->predicated = true;

      inst = emit(fs_inst(BRW_OPCODE_CMP, reg_null_f, op[0], fs_reg(0.0f)));
      inst->conditional_mod = BRW_CONDITIONAL_L;
      inst = emit(fs_inst(BRW_OPCODE_MOV, this->result, fs_reg(-1.0f)));
      inst->predicated = true;

      break;
   case ir_unop_rcp:
      emit_math(FS_OPCODE_RCP, this->result, op[0]);
      break;

   case ir_unop_exp2:
      emit_math(FS_OPCODE_EXP2, this->result, op[0]);
      break;
   case ir_unop_log2:
      emit_math(FS_OPCODE_LOG2, this->result, op[0]);
      break;
   case ir_unop_exp:
   case ir_unop_log:
      assert(!"not reached: should be handled by ir_explog_to_explog2");
      break;
   case ir_unop_sin:
   case ir_unop_sin_reduced:
      emit_math(FS_OPCODE_SIN, this->result, op[0]);
      break;
   case ir_unop_cos:
   case ir_unop_cos_reduced:
      emit_math(FS_OPCODE_COS, this->result, op[0]);
      break;

   case ir_unop_dFdx:
      emit(fs_inst(FS_OPCODE_DDX, this->result, op[0]));
      break;
   case ir_unop_dFdy:
      emit(fs_inst(FS_OPCODE_DDY, this->result, op[0]));
      break;

   case ir_binop_add:
      emit(fs_inst(BRW_OPCODE_ADD, this->result, op[0], op[1]));
      break;
   case ir_binop_sub:
      assert(!"not reached: should be handled by ir_sub_to_add_neg");
      break;

   case ir_binop_mul:
      emit(fs_inst(BRW_OPCODE_MUL, this->result, op[0], op[1]));
      break;
   case ir_binop_div:
      assert(!"not reached: should be handled by ir_div_to_mul_rcp");
      break;
   case ir_binop_mod:
      assert(!"ir_binop_mod should have been converted to b * fract(a/b)");
      break;

   case ir_binop_less:
   case ir_binop_greater:
   case ir_binop_lequal:
   case ir_binop_gequal:
   case ir_binop_equal:
   case ir_binop_all_equal:
   case ir_binop_nequal:
   case ir_binop_any_nequal:
      temp = this->result;
      /* original gen4 does implicit conversion before comparison. */
      if (intel->gen < 5)
	 temp.type = op[0].type;

      inst = emit(fs_inst(BRW_OPCODE_CMP, temp, op[0], op[1]));
      inst->conditional_mod = brw_conditional_for_comparison(ir->operation);
      emit(fs_inst(BRW_OPCODE_AND, this->result, this->result, fs_reg(0x1)));
      break;

   case ir_binop_logic_xor:
      emit(fs_inst(BRW_OPCODE_XOR, this->result, op[0], op[1]));
      break;

   case ir_binop_logic_or:
      emit(fs_inst(BRW_OPCODE_OR, this->result, op[0], op[1]));
      break;

   case ir_binop_logic_and:
      emit(fs_inst(BRW_OPCODE_AND, this->result, op[0], op[1]));
      break;

   case ir_binop_dot:
   case ir_unop_any:
      assert(!"not reached: should be handled by brw_fs_channel_expressions");
      break;

   case ir_unop_noise:
      assert(!"not reached: should be handled by lower_noise");
      break;

   case ir_quadop_vector:
      assert(!"not reached: should be handled by lower_quadop_vector");
      break;

   case ir_unop_sqrt:
      emit_math(FS_OPCODE_SQRT, this->result, op[0]);
      break;

   case ir_unop_rsq:
      emit_math(FS_OPCODE_RSQ, this->result, op[0]);
      break;

   case ir_unop_i2f:
   case ir_unop_b2f:
   case ir_unop_b2i:
   case ir_unop_f2i:
      emit(fs_inst(BRW_OPCODE_MOV, this->result, op[0]));
      break;
   case ir_unop_f2b:
   case ir_unop_i2b:
      temp = this->result;
      /* original gen4 does implicit conversion before comparison. */
      if (intel->gen < 5)
	 temp.type = op[0].type;

      inst = emit(fs_inst(BRW_OPCODE_CMP, temp, op[0], fs_reg(0.0f)));
      inst->conditional_mod = BRW_CONDITIONAL_NZ;
      inst = emit(fs_inst(BRW_OPCODE_AND, this->result,
			  this->result, fs_reg(1)));
      break;

   case ir_unop_trunc:
      emit(fs_inst(BRW_OPCODE_RNDZ, this->result, op[0]));
      break;
   case ir_unop_ceil:
      op[0].negate = !op[0].negate;
      inst = emit(fs_inst(BRW_OPCODE_RNDD, this->result, op[0]));
      this->result.negate = true;
      break;
   case ir_unop_floor:
      inst = emit(fs_inst(BRW_OPCODE_RNDD, this->result, op[0]));
      break;
   case ir_unop_fract:
      inst = emit(fs_inst(BRW_OPCODE_FRC, this->result, op[0]));
      break;
   case ir_unop_round_even:
      emit(fs_inst(BRW_OPCODE_RNDE, this->result, op[0]));
      break;

   case ir_binop_min:
      inst = emit(fs_inst(BRW_OPCODE_CMP, this->result, op[0], op[1]));
      inst->conditional_mod = BRW_CONDITIONAL_L;

      inst = emit(fs_inst(BRW_OPCODE_SEL, this->result, op[0], op[1]));
      inst->predicated = true;
      break;
   case ir_binop_max:
      inst = emit(fs_inst(BRW_OPCODE_CMP, this->result, op[0], op[1]));
      inst->conditional_mod = BRW_CONDITIONAL_G;

      inst = emit(fs_inst(BRW_OPCODE_SEL, this->result, op[0], op[1]));
      inst->predicated = true;
      break;

   case ir_binop_pow:
      emit_math(FS_OPCODE_POW, this->result, op[0], op[1]);
      break;

   case ir_unop_bit_not:
      inst = emit(fs_inst(BRW_OPCODE_NOT, this->result, op[0]));
      break;
   case ir_binop_bit_and:
      inst = emit(fs_inst(BRW_OPCODE_AND, this->result, op[0], op[1]));
      break;
   case ir_binop_bit_xor:
      inst = emit(fs_inst(BRW_OPCODE_XOR, this->result, op[0], op[1]));
      break;
   case ir_binop_bit_or:
      inst = emit(fs_inst(BRW_OPCODE_OR, this->result, op[0], op[1]));
      break;

   case ir_unop_u2f:
   case ir_binop_lshift:
   case ir_binop_rshift:
      assert(!"GLSL 1.30 features unsupported");
      break;
   }
}

void
fs_visitor::emit_assignment_writes(fs_reg &l, fs_reg &r,
				   const glsl_type *type, bool predicated)
{
   switch (type->base_type) {
   case GLSL_TYPE_FLOAT:
   case GLSL_TYPE_UINT:
   case GLSL_TYPE_INT:
   case GLSL_TYPE_BOOL:
      for (unsigned int i = 0; i < type->components(); i++) {
	 l.type = brw_type_for_base_type(type);
	 r.type = brw_type_for_base_type(type);

	 fs_inst *inst = emit(fs_inst(BRW_OPCODE_MOV, l, r));
	 inst->predicated = predicated;

	 l.reg_offset++;
	 r.reg_offset++;
      }
      break;
   case GLSL_TYPE_ARRAY:
      for (unsigned int i = 0; i < type->length; i++) {
	 emit_assignment_writes(l, r, type->fields.array, predicated);
      }
      break;

   case GLSL_TYPE_STRUCT:
      for (unsigned int i = 0; i < type->length; i++) {
	 emit_assignment_writes(l, r, type->fields.structure[i].type,
				predicated);
      }
      break;

   case GLSL_TYPE_SAMPLER:
      break;

   default:
      assert(!"not reached");
      break;
   }
}

void
fs_visitor::visit(ir_assignment *ir)
{
   struct fs_reg l, r;
   fs_inst *inst;

   /* FINISHME: arrays on the lhs */
   ir->lhs->accept(this);
   l = this->result;

   ir->rhs->accept(this);
   r = this->result;

   assert(l.file != BAD_FILE);
   assert(r.file != BAD_FILE);

   if (ir->condition) {
      emit_bool_to_cond_code(ir->condition);
   }

   if (ir->lhs->type->is_scalar() ||
       ir->lhs->type->is_vector()) {
      for (int i = 0; i < ir->lhs->type->vector_elements; i++) {
	 if (ir->write_mask & (1 << i)) {
	    inst = emit(fs_inst(BRW_OPCODE_MOV, l, r));
	    if (ir->condition)
	       inst->predicated = true;
	    r.reg_offset++;
	 }
	 l.reg_offset++;
      }
   } else {
      emit_assignment_writes(l, r, ir->lhs->type, ir->condition != NULL);
   }
}

fs_inst *
fs_visitor::emit_texture_gen4(ir_texture *ir, fs_reg dst, fs_reg coordinate)
{
   int mlen;
   int base_mrf = 1;
   bool simd16 = false;
   fs_reg orig_dst;

   /* g0 header. */
   mlen = 1;

   if (ir->shadow_comparitor) {
      for (int i = 0; i < ir->coordinate->type->vector_elements; i++) {
	 emit(fs_inst(BRW_OPCODE_MOV, fs_reg(MRF, base_mrf + mlen + i),
		      coordinate));
	 coordinate.reg_offset++;
      }
      /* gen4's SIMD8 sampler always has the slots for u,v,r present. */
      mlen += 3;

      if (ir->op == ir_tex) {
	 /* There's no plain shadow compare message, so we use shadow
	  * compare with a bias of 0.0.
	  */
	 emit(fs_inst(BRW_OPCODE_MOV, fs_reg(MRF, base_mrf + mlen),
		      fs_reg(0.0f)));
	 mlen++;
      } else if (ir->op == ir_txb) {
	 ir->lod_info.bias->accept(this);
	 emit(fs_inst(BRW_OPCODE_MOV, fs_reg(MRF, base_mrf + mlen),
		      this->result));
	 mlen++;
      } else {
	 assert(ir->op == ir_txl);
	 ir->lod_info.lod->accept(this);
	 emit(fs_inst(BRW_OPCODE_MOV, fs_reg(MRF, base_mrf + mlen),
		      this->result));
	 mlen++;
      }

      ir->shadow_comparitor->accept(this);
      emit(fs_inst(BRW_OPCODE_MOV, fs_reg(MRF, base_mrf + mlen), this->result));
      mlen++;
   } else if (ir->op == ir_tex) {
      for (int i = 0; i < ir->coordinate->type->vector_elements; i++) {
	 emit(fs_inst(BRW_OPCODE_MOV, fs_reg(MRF, base_mrf + mlen + i),
		      coordinate));
	 coordinate.reg_offset++;
      }
      /* gen4's SIMD8 sampler always has the slots for u,v,r present. */
      mlen += 3;
   } else {
      /* Oh joy.  gen4 doesn't have SIMD8 non-shadow-compare bias/lod
       * instructions.  We'll need to do SIMD16 here.
       */
      assert(ir->op == ir_txb || ir->op == ir_txl);

      for (int i = 0; i < ir->coordinate->type->vector_elements; i++) {
	 emit(fs_inst(BRW_OPCODE_MOV, fs_reg(MRF, base_mrf + mlen + i * 2),
		      coordinate));
	 coordinate.reg_offset++;
      }

      /* lod/bias appears after u/v/r. */
      mlen += 6;

      if (ir->op == ir_txb) {
	 ir->lod_info.bias->accept(this);
	 emit(fs_inst(BRW_OPCODE_MOV, fs_reg(MRF, base_mrf + mlen),
		      this->result));
	 mlen++;
      } else {
	 ir->lod_info.lod->accept(this);
	 emit(fs_inst(BRW_OPCODE_MOV, fs_reg(MRF, base_mrf + mlen),
		      this->result));
	 mlen++;
      }

      /* The unused upper half. */
      mlen++;

      /* Now, since we're doing simd16, the return is 2 interleaved
       * vec4s where the odd-indexed ones are junk. We'll need to move
       * this weirdness around to the expected layout.
       */
      simd16 = true;
      orig_dst = dst;
      dst = fs_reg(this, glsl_type::get_array_instance(glsl_type::vec4_type,
						       2));
      dst.type = BRW_REGISTER_TYPE_F;
   }

   fs_inst *inst = NULL;
   switch (ir->op) {
   case ir_tex:
      inst = emit(fs_inst(FS_OPCODE_TEX, dst));
      break;
   case ir_txb:
      inst = emit(fs_inst(FS_OPCODE_TXB, dst));
      break;
   case ir_txl:
      inst = emit(fs_inst(FS_OPCODE_TXL, dst));
      break;
   case ir_txd:
   case ir_txf:
      assert(!"GLSL 1.30 features unsupported");
      break;
   }
   inst->base_mrf = base_mrf;
   inst->mlen = mlen;

   if (simd16) {
      for (int i = 0; i < 4; i++) {
	 emit(fs_inst(BRW_OPCODE_MOV, orig_dst, dst));
	 orig_dst.reg_offset++;
	 dst.reg_offset += 2;
      }
   }

   return inst;
}

fs_inst *
fs_visitor::emit_texture_gen5(ir_texture *ir, fs_reg dst, fs_reg coordinate)
{
   /* gen5's SIMD8 sampler has slots for u, v, r, array index, then
    * optional parameters like shadow comparitor or LOD bias.  If
    * optional parameters aren't present, those base slots are
    * optional and don't need to be included in the message.
    *
    * We don't fill in the unnecessary slots regardless, which may
    * look surprising in the disassembly.
    */
   int mlen = 1; /* g0 header always present. */
   int base_mrf = 1;

   for (int i = 0; i < ir->coordinate->type->vector_elements; i++) {
      emit(fs_inst(BRW_OPCODE_MOV, fs_reg(MRF, base_mrf + mlen + i),
		   coordinate));
      coordinate.reg_offset++;
   }
   mlen += ir->coordinate->type->vector_elements;

   if (ir->shadow_comparitor) {
      mlen = MAX2(mlen, 5);

      ir->shadow_comparitor->accept(this);
      emit(fs_inst(BRW_OPCODE_MOV, fs_reg(MRF, base_mrf + mlen), this->result));
      mlen++;
   }

   fs_inst *inst = NULL;
   switch (ir->op) {
   case ir_tex:
      inst = emit(fs_inst(FS_OPCODE_TEX, dst));
      break;
   case ir_txb:
      ir->lod_info.bias->accept(this);
      mlen = MAX2(mlen, 5);
      emit(fs_inst(BRW_OPCODE_MOV, fs_reg(MRF, base_mrf + mlen), this->result));
      mlen++;

      inst = emit(fs_inst(FS_OPCODE_TXB, dst));
      break;
   case ir_txl:
      ir->lod_info.lod->accept(this);
      mlen = MAX2(mlen, 5);
      emit(fs_inst(BRW_OPCODE_MOV, fs_reg(MRF, base_mrf + mlen), this->result));
      mlen++;

      inst = emit(fs_inst(FS_OPCODE_TXL, dst));
      break;
   case ir_txd:
   case ir_txf:
      assert(!"GLSL 1.30 features unsupported");
      break;
   }
   inst->base_mrf = base_mrf;
   inst->mlen = mlen;

   return inst;
}

void
fs_visitor::visit(ir_texture *ir)
{
   int sampler;
   fs_inst *inst = NULL;

   ir->coordinate->accept(this);
   fs_reg coordinate = this->result;

   /* Should be lowered by do_lower_texture_projection */
   assert(!ir->projector);

   sampler = _mesa_get_sampler_uniform_value(ir->sampler,
					     ctx->Shader.CurrentFragmentProgram,
					     &brw->fragment_program->Base);
   sampler = c->fp->program.Base.SamplerUnits[sampler];

   /* The 965 requires the EU to do the normalization of GL rectangle
    * texture coordinates.  We use the program parameter state
    * tracking to get the scaling factor.
    */
   if (ir->sampler->type->sampler_dimensionality == GLSL_SAMPLER_DIM_RECT) {
      struct gl_program_parameter_list *params = c->fp->program.Base.Parameters;
      int tokens[STATE_LENGTH] = {
	 STATE_INTERNAL,
	 STATE_TEXRECT_SCALE,
	 sampler,
	 0,
	 0
      };

      c->prog_data.param_convert[c->prog_data.nr_params] =
	 PARAM_NO_CONVERT;
      c->prog_data.param_convert[c->prog_data.nr_params + 1] =
	 PARAM_NO_CONVERT;

      fs_reg scale_x = fs_reg(UNIFORM, c->prog_data.nr_params);
      fs_reg scale_y = fs_reg(UNIFORM, c->prog_data.nr_params + 1);
      GLuint index = _mesa_add_state_reference(params,
					       (gl_state_index *)tokens);

      this->param_index[c->prog_data.nr_params] = index;
      this->param_offset[c->prog_data.nr_params] = 0;
      c->prog_data.nr_params++;
      this->param_index[c->prog_data.nr_params] = index;
      this->param_offset[c->prog_data.nr_params] = 1;
      c->prog_data.nr_params++;

      fs_reg dst = fs_reg(this, ir->coordinate->type);
      fs_reg src = coordinate;
      coordinate = dst;

      emit(fs_inst(BRW_OPCODE_MUL, dst, src, scale_x));
      dst.reg_offset++;
      src.reg_offset++;
      emit(fs_inst(BRW_OPCODE_MUL, dst, src, scale_y));
   }

   /* Writemasking doesn't eliminate channels on SIMD8 texture
    * samples, so don't worry about them.
    */
   fs_reg dst = fs_reg(this, glsl_type::vec4_type);

   if (intel->gen < 5) {
      inst = emit_texture_gen4(ir, dst, coordinate);
   } else {
      inst = emit_texture_gen5(ir, dst, coordinate);
   }

   inst->sampler = sampler;

   this->result = dst;

   if (ir->shadow_comparitor)
      inst->shadow_compare = true;

   if (c->key.tex_swizzles[inst->sampler] != SWIZZLE_NOOP) {
      fs_reg swizzle_dst = fs_reg(this, glsl_type::vec4_type);

      for (int i = 0; i < 4; i++) {
	 int swiz = GET_SWZ(c->key.tex_swizzles[inst->sampler], i);
	 fs_reg l = swizzle_dst;
	 l.reg_offset += i;

	 if (swiz == SWIZZLE_ZERO) {
	    emit(fs_inst(BRW_OPCODE_MOV, l, fs_reg(0.0f)));
	 } else if (swiz == SWIZZLE_ONE) {
	    emit(fs_inst(BRW_OPCODE_MOV, l, fs_reg(1.0f)));
	 } else {
	    fs_reg r = dst;
	    r.reg_offset += GET_SWZ(c->key.tex_swizzles[inst->sampler], i);
	    emit(fs_inst(BRW_OPCODE_MOV, l, r));
	 }
      }
      this->result = swizzle_dst;
   }
}

void
fs_visitor::visit(ir_swizzle *ir)
{
   ir->val->accept(this);
   fs_reg val = this->result;

   if (ir->type->vector_elements == 1) {
      this->result.reg_offset += ir->mask.x;
      return;
   }

   fs_reg result = fs_reg(this, ir->type);
   this->result = result;

   for (unsigned int i = 0; i < ir->type->vector_elements; i++) {
      fs_reg channel = val;
      int swiz = 0;

      switch (i) {
      case 0:
	 swiz = ir->mask.x;
	 break;
      case 1:
	 swiz = ir->mask.y;
	 break;
      case 2:
	 swiz = ir->mask.z;
	 break;
      case 3:
	 swiz = ir->mask.w;
	 break;
      }

      channel.reg_offset += swiz;
      emit(fs_inst(BRW_OPCODE_MOV, result, channel));
      result.reg_offset++;
   }
}

void
fs_visitor::visit(ir_discard *ir)
{
   fs_reg temp = fs_reg(this, glsl_type::uint_type);

   assert(ir->condition == NULL); /* FINISHME */

   emit(fs_inst(FS_OPCODE_DISCARD_NOT, temp, reg_null_d));
   emit(fs_inst(FS_OPCODE_DISCARD_AND, reg_null_d, temp));
   kill_emitted = true;
}

void
fs_visitor::visit(ir_constant *ir)
{
   /* Set this->result to reg at the bottom of the function because some code
    * paths will cause this visitor to be applied to other fields.  This will
    * cause the value stored in this->result to be modified.
    *
    * Make reg constant so that it doesn't get accidentally modified along the
    * way.  Yes, I actually had this problem. :(
    */
   const fs_reg reg(this, ir->type);
   fs_reg dst_reg = reg;

   if (ir->type->is_array()) {
      const unsigned size = type_size(ir->type->fields.array);

      for (unsigned i = 0; i < ir->type->length; i++) {
	 ir->array_elements[i]->accept(this);
	 fs_reg src_reg = this->result;

	 dst_reg.type = src_reg.type;
	 for (unsigned j = 0; j < size; j++) {
	    emit(fs_inst(BRW_OPCODE_MOV, dst_reg, src_reg));
	    src_reg.reg_offset++;
	    dst_reg.reg_offset++;
	 }
      }
   } else if (ir->type->is_record()) {
      foreach_list(node, &ir->components) {
	 ir_instruction *const field = (ir_instruction *) node;
	 const unsigned size = type_size(field->type);

	 field->accept(this);
	 fs_reg src_reg = this->result;

	 dst_reg.type = src_reg.type;
	 for (unsigned j = 0; j < size; j++) {
	    emit(fs_inst(BRW_OPCODE_MOV, dst_reg, src_reg));
	    src_reg.reg_offset++;
	    dst_reg.reg_offset++;
	 }
      }
   } else {
      const unsigned size = type_size(ir->type);

      for (unsigned i = 0; i < size; i++) {
	 switch (ir->type->base_type) {
	 case GLSL_TYPE_FLOAT:
	    emit(fs_inst(BRW_OPCODE_MOV, dst_reg, fs_reg(ir->value.f[i])));
	    break;
	 case GLSL_TYPE_UINT:
	    emit(fs_inst(BRW_OPCODE_MOV, dst_reg, fs_reg(ir->value.u[i])));
	    break;
	 case GLSL_TYPE_INT:
	    emit(fs_inst(BRW_OPCODE_MOV, dst_reg, fs_reg(ir->value.i[i])));
	    break;
	 case GLSL_TYPE_BOOL:
	    emit(fs_inst(BRW_OPCODE_MOV, dst_reg, fs_reg((int)ir->value.b[i])));
	    break;
	 default:
	    assert(!"Non-float/uint/int/bool constant");
	 }
	 dst_reg.reg_offset++;
      }
   }

   this->result = reg;
}

void
fs_visitor::emit_bool_to_cond_code(ir_rvalue *ir)
{
   ir_expression *expr = ir->as_expression();

   if (expr) {
      fs_reg op[2];
      fs_inst *inst;

      assert(expr->get_num_operands() <= 2);
      for (unsigned int i = 0; i < expr->get_num_operands(); i++) {
	 assert(expr->operands[i]->type->is_scalar());

	 expr->operands[i]->accept(this);
	 op[i] = this->result;
      }

      switch (expr->operation) {
      case ir_unop_logic_not:
	 inst = emit(fs_inst(BRW_OPCODE_AND, reg_null_d, op[0], fs_reg(1)));
	 inst->conditional_mod = BRW_CONDITIONAL_Z;
	 break;

      case ir_binop_logic_xor:
	 inst = emit(fs_inst(BRW_OPCODE_XOR, reg_null_d, op[0], op[1]));
	 inst->conditional_mod = BRW_CONDITIONAL_NZ;
	 break;

      case ir_binop_logic_or:
	 inst = emit(fs_inst(BRW_OPCODE_OR, reg_null_d, op[0], op[1]));
	 inst->conditional_mod = BRW_CONDITIONAL_NZ;
	 break;

      case ir_binop_logic_and:
	 inst = emit(fs_inst(BRW_OPCODE_AND, reg_null_d, op[0], op[1]));
	 inst->conditional_mod = BRW_CONDITIONAL_NZ;
	 break;

      case ir_unop_f2b:
	 if (intel->gen >= 6) {
	    inst = emit(fs_inst(BRW_OPCODE_CMP, reg_null_d,
				op[0], fs_reg(0.0f)));
	 } else {
	    inst = emit(fs_inst(BRW_OPCODE_MOV, reg_null_f, op[0]));
	 }
	 inst->conditional_mod = BRW_CONDITIONAL_NZ;
	 break;

      case ir_unop_i2b:
	 if (intel->gen >= 6) {
	    inst = emit(fs_inst(BRW_OPCODE_CMP, reg_null_d, op[0], fs_reg(0)));
	 } else {
	    inst = emit(fs_inst(BRW_OPCODE_MOV, reg_null_d, op[0]));
	 }
	 inst->conditional_mod = BRW_CONDITIONAL_NZ;
	 break;

      case ir_binop_greater:
      case ir_binop_gequal:
      case ir_binop_less:
      case ir_binop_lequal:
      case ir_binop_equal:
      case ir_binop_all_equal:
      case ir_binop_nequal:
      case ir_binop_any_nequal:
	 inst = emit(fs_inst(BRW_OPCODE_CMP, reg_null_cmp, op[0], op[1]));
	 inst->conditional_mod =
	    brw_conditional_for_comparison(expr->operation);
	 break;

      default:
	 assert(!"not reached");
	 this->fail = true;
	 break;
      }
      return;
   }

   ir->accept(this);

   if (intel->gen >= 6) {
      fs_inst *inst = emit(fs_inst(BRW_OPCODE_AND, reg_null_d,
				   this->result, fs_reg(1)));
      inst->conditional_mod = BRW_CONDITIONAL_NZ;
   } else {
      fs_inst *inst = emit(fs_inst(BRW_OPCODE_MOV, reg_null_d, this->result));
      inst->conditional_mod = BRW_CONDITIONAL_NZ;
   }
}

/**
 * Emit a gen6 IF statement with the comparison folded into the IF
 * instruction.
 */
void
fs_visitor::emit_if_gen6(ir_if *ir)
{
   ir_expression *expr = ir->condition->as_expression();

   if (expr) {
      fs_reg op[2];
      fs_inst *inst;
      fs_reg temp;

      assert(expr->get_num_operands() <= 2);
      for (unsigned int i = 0; i < expr->get_num_operands(); i++) {
	 assert(expr->operands[i]->type->is_scalar());

	 expr->operands[i]->accept(this);
	 op[i] = this->result;
      }

      switch (expr->operation) {
      case ir_unop_logic_not:
	 inst = emit(fs_inst(BRW_OPCODE_IF, temp, op[0], fs_reg(0)));
	 inst->conditional_mod = BRW_CONDITIONAL_Z;
	 return;

      case ir_binop_logic_xor:
	 inst = emit(fs_inst(BRW_OPCODE_IF, reg_null_d, op[0], op[1]));
	 inst->conditional_mod = BRW_CONDITIONAL_NZ;
	 return;

      case ir_binop_logic_or:
	 temp = fs_reg(this, glsl_type::bool_type);
	 emit(fs_inst(BRW_OPCODE_OR, temp, op[0], op[1]));
	 inst = emit(fs_inst(BRW_OPCODE_IF, reg_null_d, temp, fs_reg(0)));
	 inst->conditional_mod = BRW_CONDITIONAL_NZ;
	 return;

      case ir_binop_logic_and:
	 temp = fs_reg(this, glsl_type::bool_type);
	 emit(fs_inst(BRW_OPCODE_AND, temp, op[0], op[1]));
	 inst = emit(fs_inst(BRW_OPCODE_IF, reg_null_d, temp, fs_reg(0)));
	 inst->conditional_mod = BRW_CONDITIONAL_NZ;
	 return;

      case ir_unop_f2b:
	 inst = emit(fs_inst(BRW_OPCODE_IF, reg_null_f, op[0], fs_reg(0)));
	 inst->conditional_mod = BRW_CONDITIONAL_NZ;
	 return;

      case ir_unop_i2b:
	 inst = emit(fs_inst(BRW_OPCODE_IF, reg_null_d, op[0], fs_reg(0)));
	 inst->conditional_mod = BRW_CONDITIONAL_NZ;
	 return;

      case ir_binop_greater:
      case ir_binop_gequal:
      case ir_binop_less:
      case ir_binop_lequal:
      case ir_binop_equal:
      case ir_binop_all_equal:
      case ir_binop_nequal:
      case ir_binop_any_nequal:
	 inst = emit(fs_inst(BRW_OPCODE_IF, reg_null_d, op[0], op[1]));
	 inst->conditional_mod =
	    brw_conditional_for_comparison(expr->operation);
	 return;
      default:
	 assert(!"not reached");
	 inst = emit(fs_inst(BRW_OPCODE_IF, reg_null_d, op[0], fs_reg(0)));
	 inst->conditional_mod = BRW_CONDITIONAL_NZ;
	 this->fail = true;
	 return;
      }
      return;
   }

   ir->condition->accept(this);

   fs_inst *inst = emit(fs_inst(BRW_OPCODE_IF, reg_null_d, this->result, fs_reg(0)));
   inst->conditional_mod = BRW_CONDITIONAL_NZ;
}

void
fs_visitor::visit(ir_if *ir)
{
   fs_inst *inst;

   /* Don't point the annotation at the if statement, because then it plus
    * the then and else blocks get printed.
    */
   this->base_ir = ir->condition;

   if (intel->gen >= 6) {
      emit_if_gen6(ir);
   } else {
      emit_bool_to_cond_code(ir->condition);

      inst = emit(fs_inst(BRW_OPCODE_IF));
      inst->predicated = true;
   }

   foreach_iter(exec_list_iterator, iter, ir->then_instructions) {
      ir_instruction *ir = (ir_instruction *)iter.get();
      this->base_ir = ir;

      ir->accept(this);
   }

   if (!ir->else_instructions.is_empty()) {
      emit(fs_inst(BRW_OPCODE_ELSE));

      foreach_iter(exec_list_iterator, iter, ir->else_instructions) {
	 ir_instruction *ir = (ir_instruction *)iter.get();
	 this->base_ir = ir;

	 ir->accept(this);
      }
   }

   emit(fs_inst(BRW_OPCODE_ENDIF));
}

void
fs_visitor::visit(ir_loop *ir)
{
   fs_reg counter = reg_undef;

   if (ir->counter) {
      this->base_ir = ir->counter;
      ir->counter->accept(this);
      counter = *(variable_storage(ir->counter));

      if (ir->from) {
	 this->base_ir = ir->from;
	 ir->from->accept(this);

	 emit(fs_inst(BRW_OPCODE_MOV, counter, this->result));
      }
   }

   emit(fs_inst(BRW_OPCODE_DO));

   if (ir->to) {
      this->base_ir = ir->to;
      ir->to->accept(this);

      fs_inst *inst = emit(fs_inst(BRW_OPCODE_CMP, reg_null_cmp,
				   counter, this->result));
      inst->conditional_mod = brw_conditional_for_comparison(ir->cmp);

      inst = emit(fs_inst(BRW_OPCODE_BREAK));
      inst->predicated = true;
   }

   foreach_iter(exec_list_iterator, iter, ir->body_instructions) {
      ir_instruction *ir = (ir_instruction *)iter.get();

      this->base_ir = ir;
      ir->accept(this);
   }

   if (ir->increment) {
      this->base_ir = ir->increment;
      ir->increment->accept(this);
      emit(fs_inst(BRW_OPCODE_ADD, counter, counter, this->result));
   }

   emit(fs_inst(BRW_OPCODE_WHILE));
}

void
fs_visitor::visit(ir_loop_jump *ir)
{
   switch (ir->mode) {
   case ir_loop_jump::jump_break:
      emit(fs_inst(BRW_OPCODE_BREAK));
      break;
   case ir_loop_jump::jump_continue:
      emit(fs_inst(BRW_OPCODE_CONTINUE));
      break;
   }
}

void
fs_visitor::visit(ir_call *ir)
{
   assert(!"FINISHME");
}

void
fs_visitor::visit(ir_return *ir)
{
   assert(!"FINISHME");
}

void
fs_visitor::visit(ir_function *ir)
{
   /* Ignore function bodies other than main() -- we shouldn't see calls to
    * them since they should all be inlined before we get to ir_to_mesa.
    */
   if (strcmp(ir->name, "main") == 0) {
      const ir_function_signature *sig;
      exec_list empty;

      sig = ir->matching_signature(&empty);

      assert(sig);

      foreach_iter(exec_list_iterator, iter, sig->body) {
	 ir_instruction *ir = (ir_instruction *)iter.get();
	 this->base_ir = ir;

	 ir->accept(this);
      }
   }
}

void
fs_visitor::visit(ir_function_signature *ir)
{
   assert(!"not reached");
   (void)ir;
}

fs_inst *
fs_visitor::emit(fs_inst inst)
{
   fs_inst *list_inst = new(mem_ctx) fs_inst;
   *list_inst = inst;

   list_inst->annotation = this->current_annotation;
   list_inst->ir = this->base_ir;

   this->instructions.push_tail(list_inst);

   return list_inst;
}

/** Emits a dummy fragment shader consisting of magenta for bringup purposes. */
void
fs_visitor::emit_dummy_fs()
{
   /* Everyone's favorite color. */
   emit(fs_inst(BRW_OPCODE_MOV,
		fs_reg(MRF, 2),
		fs_reg(1.0f)));
   emit(fs_inst(BRW_OPCODE_MOV,
		fs_reg(MRF, 3),
		fs_reg(0.0f)));
   emit(fs_inst(BRW_OPCODE_MOV,
		fs_reg(MRF, 4),
		fs_reg(1.0f)));
   emit(fs_inst(BRW_OPCODE_MOV,
		fs_reg(MRF, 5),
		fs_reg(0.0f)));

   fs_inst *write;
   write = emit(fs_inst(FS_OPCODE_FB_WRITE,
			fs_reg(0),
			fs_reg(0)));
   write->base_mrf = 0;
}

/* The register location here is relative to the start of the URB
 * data.  It will get adjusted to be a real location before
 * generate_code() time.
 */
struct brw_reg
fs_visitor::interp_reg(int location, int channel)
{
   int regnr = urb_setup[location] * 2 + channel / 2;
   int stride = (channel & 1) * 4;

   assert(urb_setup[location] != -1);

   return brw_vec1_grf(regnr, stride);
}

/** Emits the interpolation for the varying inputs. */
void
fs_visitor::emit_interpolation_setup_gen4()
{
   struct brw_reg g1_uw = retype(brw_vec1_grf(1, 0), BRW_REGISTER_TYPE_UW);

   this->current_annotation = "compute pixel centers";
   this->pixel_x = fs_reg(this, glsl_type::uint_type);
   this->pixel_y = fs_reg(this, glsl_type::uint_type);
   this->pixel_x.type = BRW_REGISTER_TYPE_UW;
   this->pixel_y.type = BRW_REGISTER_TYPE_UW;
   emit(fs_inst(BRW_OPCODE_ADD,
		this->pixel_x,
		fs_reg(stride(suboffset(g1_uw, 4), 2, 4, 0)),
		fs_reg(brw_imm_v(0x10101010))));
   emit(fs_inst(BRW_OPCODE_ADD,
		this->pixel_y,
		fs_reg(stride(suboffset(g1_uw, 5), 2, 4, 0)),
		fs_reg(brw_imm_v(0x11001100))));

   this->current_annotation = "compute pixel deltas from v0";
   if (brw->has_pln) {
      this->delta_x = fs_reg(this, glsl_type::vec2_type);
      this->delta_y = this->delta_x;
      this->delta_y.reg_offset++;
   } else {
      this->delta_x = fs_reg(this, glsl_type::float_type);
      this->delta_y = fs_reg(this, glsl_type::float_type);
   }
   emit(fs_inst(BRW_OPCODE_ADD,
		this->delta_x,
		this->pixel_x,
		fs_reg(negate(brw_vec1_grf(1, 0)))));
   emit(fs_inst(BRW_OPCODE_ADD,
		this->delta_y,
		this->pixel_y,
		fs_reg(negate(brw_vec1_grf(1, 1)))));

   this->current_annotation = "compute pos.w and 1/pos.w";
   /* Compute wpos.w.  It's always in our setup, since it's needed to
    * interpolate the other attributes.
    */
   this->wpos_w = fs_reg(this, glsl_type::float_type);
   emit(fs_inst(FS_OPCODE_LINTERP, wpos_w, this->delta_x, this->delta_y,
		interp_reg(FRAG_ATTRIB_WPOS, 3)));
   /* Compute the pixel 1/W value from wpos.w. */
   this->pixel_w = fs_reg(this, glsl_type::float_type);
   emit_math(FS_OPCODE_RCP, this->pixel_w, wpos_w);
   this->current_annotation = NULL;
}

/** Emits the interpolation for the varying inputs. */
void
fs_visitor::emit_interpolation_setup_gen6()
{
   struct brw_reg g1_uw = retype(brw_vec1_grf(1, 0), BRW_REGISTER_TYPE_UW);

   /* If the pixel centers end up used, the setup is the same as for gen4. */
   this->current_annotation = "compute pixel centers";
   fs_reg int_pixel_x = fs_reg(this, glsl_type::uint_type);
   fs_reg int_pixel_y = fs_reg(this, glsl_type::uint_type);
   int_pixel_x.type = BRW_REGISTER_TYPE_UW;
   int_pixel_y.type = BRW_REGISTER_TYPE_UW;
   emit(fs_inst(BRW_OPCODE_ADD,
		int_pixel_x,
		fs_reg(stride(suboffset(g1_uw, 4), 2, 4, 0)),
		fs_reg(brw_imm_v(0x10101010))));
   emit(fs_inst(BRW_OPCODE_ADD,
		int_pixel_y,
		fs_reg(stride(suboffset(g1_uw, 5), 2, 4, 0)),
		fs_reg(brw_imm_v(0x11001100))));

   /* As of gen6, we can no longer mix float and int sources.  We have
    * to turn the integer pixel centers into floats for their actual
    * use.
    */
   this->pixel_x = fs_reg(this, glsl_type::float_type);
   this->pixel_y = fs_reg(this, glsl_type::float_type);
   emit(fs_inst(BRW_OPCODE_MOV, this->pixel_x, int_pixel_x));
   emit(fs_inst(BRW_OPCODE_MOV, this->pixel_y, int_pixel_y));

   this->current_annotation = "compute pos.w";
   this->pixel_w = fs_reg(brw_vec8_grf(c->source_w_reg, 0));
   this->wpos_w = fs_reg(this, glsl_type::float_type);
   emit_math(FS_OPCODE_RCP, this->wpos_w, this->pixel_w);

   this->delta_x = fs_reg(brw_vec8_grf(2, 0));
   this->delta_y = fs_reg(brw_vec8_grf(3, 0));

   this->current_annotation = NULL;
}

void
fs_visitor::emit_fb_writes()
{
   this->current_annotation = "FB write header";
   GLboolean header_present = GL_TRUE;
   int nr = 0;

   if (intel->gen >= 6 &&
       !this->kill_emitted &&
       c->key.nr_color_regions == 1) {
      header_present = false;
   }

   if (header_present) {
      /* m0, m1 header */
      nr += 2;
   }

   if (c->aa_dest_stencil_reg) {
      emit(fs_inst(BRW_OPCODE_MOV, fs_reg(MRF, nr++),
		   fs_reg(brw_vec8_grf(c->aa_dest_stencil_reg, 0))));
   }

   /* Reserve space for color. It'll be filled in per MRT below. */
   int color_mrf = nr;
   nr += 4;

   if (c->source_depth_to_render_target) {
      if (c->computes_depth) {
	 /* Hand over gl_FragDepth. */
	 assert(this->frag_depth);
	 fs_reg depth = *(variable_storage(this->frag_depth));

	 emit(fs_inst(BRW_OPCODE_MOV, fs_reg(MRF, nr++), depth));
      } else {
	 /* Pass through the payload depth. */
	 emit(fs_inst(BRW_OPCODE_MOV, fs_reg(MRF, nr++),
		      fs_reg(brw_vec8_grf(c->source_depth_reg, 0))));
      }
   }

   if (c->dest_depth_reg) {
      emit(fs_inst(BRW_OPCODE_MOV, fs_reg(MRF, nr++),
		   fs_reg(brw_vec8_grf(c->dest_depth_reg, 0))));
   }

   fs_reg color = reg_undef;
   if (this->frag_color)
      color = *(variable_storage(this->frag_color));
   else if (this->frag_data) {
      color = *(variable_storage(this->frag_data));
      color.type = BRW_REGISTER_TYPE_F;
   }

   for (int target = 0; target < c->key.nr_color_regions; target++) {
      this->current_annotation = ralloc_asprintf(this->mem_ctx,
						 "FB write target %d",
						 target);
      if (this->frag_color || this->frag_data) {
	 for (int i = 0; i < 4; i++) {
	    emit(fs_inst(BRW_OPCODE_MOV,
			 fs_reg(MRF, color_mrf + i),
			 color));
	    color.reg_offset++;
	 }
      }

      if (this->frag_color)
	 color.reg_offset -= 4;

      fs_inst *inst = emit(fs_inst(FS_OPCODE_FB_WRITE,
				   reg_undef, reg_undef));
      inst->target = target;
      inst->base_mrf = 0;
      inst->mlen = nr;
      if (target == c->key.nr_color_regions - 1)
	 inst->eot = true;
      inst->header_present = header_present;
   }

   if (c->key.nr_color_regions == 0) {
      if (c->key.alpha_test && (this->frag_color || this->frag_data)) {
	 /* If the alpha test is enabled but there's no color buffer,
	  * we still need to send alpha out the pipeline to our null
	  * renderbuffer.
	  */
	 color.reg_offset += 3;
	 emit(fs_inst(BRW_OPCODE_MOV,
		      fs_reg(MRF, color_mrf + 3),
		      color));
      }

      fs_inst *inst = emit(fs_inst(FS_OPCODE_FB_WRITE,
				   reg_undef, reg_undef));
      inst->base_mrf = 0;
      inst->mlen = nr;
      inst->eot = true;
      inst->header_present = header_present;
   }

   this->current_annotation = NULL;
}

void
fs_visitor::generate_fb_write(fs_inst *inst)
{
   GLboolean eot = inst->eot;
   struct brw_reg implied_header;

   /* Header is 2 regs, g0 and g1 are the contents. g0 will be implied
    * move, here's g1.
    */
   brw_push_insn_state(p);
   brw_set_mask_control(p, BRW_MASK_DISABLE);
   brw_set_compression_control(p, BRW_COMPRESSION_NONE);

   if (inst->header_present) {
      if (intel->gen >= 6) {
	 brw_MOV(p,
		 brw_message_reg(inst->base_mrf),
		 brw_vec8_grf(0, 0));

	 if (inst->target > 0) {
	    /* Set the render target index for choosing BLEND_STATE. */
	    brw_MOV(p, retype(brw_vec1_reg(BRW_MESSAGE_REGISTER_FILE, 0, 2),
			      BRW_REGISTER_TYPE_UD),
		    brw_imm_ud(inst->target));
	 }

	 /* Clear viewport index, render target array index. */
	 brw_AND(p, retype(brw_vec1_reg(BRW_MESSAGE_REGISTER_FILE, 0, 0),
			   BRW_REGISTER_TYPE_UD),
		 retype(brw_vec1_grf(0, 0), BRW_REGISTER_TYPE_UD),
		 brw_imm_ud(0xf7ff));

	 implied_header = brw_null_reg();
      } else {
	 implied_header = retype(brw_vec8_grf(0, 0), BRW_REGISTER_TYPE_UW);
      }

      brw_MOV(p,
	      brw_message_reg(inst->base_mrf + 1),
	      brw_vec8_grf(1, 0));
   } else {
      implied_header = brw_null_reg();
   }

   brw_pop_insn_state(p);

   brw_fb_WRITE(p,
		8, /* dispatch_width */
		retype(vec8(brw_null_reg()), BRW_REGISTER_TYPE_UW),
		inst->base_mrf,
		implied_header,
		inst->target,
		inst->mlen,
		0,
		eot,
		inst->header_present);
}

void
fs_visitor::generate_linterp(fs_inst *inst,
			     struct brw_reg dst, struct brw_reg *src)
{
   struct brw_reg delta_x = src[0];
   struct brw_reg delta_y = src[1];
   struct brw_reg interp = src[2];

   if (brw->has_pln &&
       delta_y.nr == delta_x.nr + 1 &&
       (intel->gen >= 6 || (delta_x.nr & 1) == 0)) {
      brw_PLN(p, dst, interp, delta_x);
   } else {
      brw_LINE(p, brw_null_reg(), interp, delta_x);
      brw_MAC(p, dst, suboffset(interp, 1), delta_y);
   }
}

void
fs_visitor::generate_math(fs_inst *inst,
			  struct brw_reg dst, struct brw_reg *src)
{
   int op;

   switch (inst->opcode) {
   case FS_OPCODE_RCP:
      op = BRW_MATH_FUNCTION_INV;
      break;
   case FS_OPCODE_RSQ:
      op = BRW_MATH_FUNCTION_RSQ;
      break;
   case FS_OPCODE_SQRT:
      op = BRW_MATH_FUNCTION_SQRT;
      break;
   case FS_OPCODE_EXP2:
      op = BRW_MATH_FUNCTION_EXP;
      break;
   case FS_OPCODE_LOG2:
      op = BRW_MATH_FUNCTION_LOG;
      break;
   case FS_OPCODE_POW:
      op = BRW_MATH_FUNCTION_POW;
      break;
   case FS_OPCODE_SIN:
      op = BRW_MATH_FUNCTION_SIN;
      break;
   case FS_OPCODE_COS:
      op = BRW_MATH_FUNCTION_COS;
      break;
   default:
      assert(!"not reached: unknown math function");
      op = 0;
      break;
   }

   if (intel->gen >= 6) {
      assert(inst->mlen == 0);

      if (inst->opcode == FS_OPCODE_POW) {
	 brw_math2(p, dst, op, src[0], src[1]);
      } else {
	 brw_math(p, dst,
		  op,
		  inst->saturate ? BRW_MATH_SATURATE_SATURATE :
		  BRW_MATH_SATURATE_NONE,
		  0, src[0],
		  BRW_MATH_DATA_VECTOR,
		  BRW_MATH_PRECISION_FULL);
      }
   } else {
      assert(inst->mlen >= 1);

      brw_math(p, dst,
	       op,
	       inst->saturate ? BRW_MATH_SATURATE_SATURATE :
	       BRW_MATH_SATURATE_NONE,
	       inst->base_mrf, src[0],
	       BRW_MATH_DATA_VECTOR,
	       BRW_MATH_PRECISION_FULL);
   }
}

void
fs_visitor::generate_tex(fs_inst *inst, struct brw_reg dst)
{
   int msg_type = -1;
   int rlen = 4;
   uint32_t simd_mode = BRW_SAMPLER_SIMD_MODE_SIMD8;

   if (intel->gen >= 5) {
      switch (inst->opcode) {
      case FS_OPCODE_TEX:
	 if (inst->shadow_compare) {
	    msg_type = BRW_SAMPLER_MESSAGE_SAMPLE_COMPARE_GEN5;
	 } else {
	    msg_type = BRW_SAMPLER_MESSAGE_SAMPLE_GEN5;
	 }
	 break;
      case FS_OPCODE_TXB:
	 if (inst->shadow_compare) {
	    msg_type = BRW_SAMPLER_MESSAGE_SAMPLE_BIAS_COMPARE_GEN5;
	 } else {
	    msg_type = BRW_SAMPLER_MESSAGE_SAMPLE_BIAS_GEN5;
	 }
	 break;
      }
   } else {
      switch (inst->opcode) {
      case FS_OPCODE_TEX:
	 /* Note that G45 and older determines shadow compare and dispatch width
	  * from message length for most messages.
	  */
	 msg_type = BRW_SAMPLER_MESSAGE_SIMD8_SAMPLE;
	 if (inst->shadow_compare) {
	    assert(inst->mlen == 6);
	 } else {
	    assert(inst->mlen <= 4);
	 }
	 break;
      case FS_OPCODE_TXB:
	 if (inst->shadow_compare) {
	    assert(inst->mlen == 6);
	    msg_type = BRW_SAMPLER_MESSAGE_SIMD8_SAMPLE;
	 } else {
	    assert(inst->mlen == 9);
	    msg_type = BRW_SAMPLER_MESSAGE_SIMD16_SAMPLE_BIAS;
	    simd_mode = BRW_SAMPLER_SIMD_MODE_SIMD16;
	 }
	 break;
      }
   }
   assert(msg_type != -1);

   if (simd_mode == BRW_SAMPLER_SIMD_MODE_SIMD16) {
      rlen = 8;
      dst = vec16(dst);
   }

   brw_SAMPLE(p,
	      retype(dst, BRW_REGISTER_TYPE_UW),
	      inst->base_mrf,
	      retype(brw_vec8_grf(0, 0), BRW_REGISTER_TYPE_UW),
              SURF_INDEX_TEXTURE(inst->sampler),
	      inst->sampler,
	      WRITEMASK_XYZW,
	      msg_type,
	      rlen,
	      inst->mlen,
	      0,
	      1,
	      simd_mode);
}


/* For OPCODE_DDX and OPCODE_DDY, per channel of output we've got input
 * looking like:
 *
 * arg0: ss0.tl ss0.tr ss0.bl ss0.br ss1.tl ss1.tr ss1.bl ss1.br
 *
 * and we're trying to produce:
 *
 *           DDX                     DDY
 * dst: (ss0.tr - ss0.tl)     (ss0.tl - ss0.bl)
 *      (ss0.tr - ss0.tl)     (ss0.tr - ss0.br)
 *      (ss0.br - ss0.bl)     (ss0.tl - ss0.bl)
 *      (ss0.br - ss0.bl)     (ss0.tr - ss0.br)
 *      (ss1.tr - ss1.tl)     (ss1.tl - ss1.bl)
 *      (ss1.tr - ss1.tl)     (ss1.tr - ss1.br)
 *      (ss1.br - ss1.bl)     (ss1.tl - ss1.bl)
 *      (ss1.br - ss1.bl)     (ss1.tr - ss1.br)
 *
 * and add another set of two more subspans if in 16-pixel dispatch mode.
 *
 * For DDX, it ends up being easy: width = 2, horiz=0 gets us the same result
 * for each pair, and vertstride = 2 jumps us 2 elements after processing a
 * pair. But for DDY, it's harder, as we want to produce the pairs swizzled
 * between each other.  We could probably do it like ddx and swizzle the right
 * order later, but bail for now and just produce
 * ((ss0.tl - ss0.bl)x4 (ss1.tl - ss1.bl)x4)
 */
void
fs_visitor::generate_ddx(fs_inst *inst, struct brw_reg dst, struct brw_reg src)
{
   struct brw_reg src0 = brw_reg(src.file, src.nr, 1,
				 BRW_REGISTER_TYPE_F,
				 BRW_VERTICAL_STRIDE_2,
				 BRW_WIDTH_2,
				 BRW_HORIZONTAL_STRIDE_0,
				 BRW_SWIZZLE_XYZW, WRITEMASK_XYZW);
   struct brw_reg src1 = brw_reg(src.file, src.nr, 0,
				 BRW_REGISTER_TYPE_F,
				 BRW_VERTICAL_STRIDE_2,
				 BRW_WIDTH_2,
				 BRW_HORIZONTAL_STRIDE_0,
				 BRW_SWIZZLE_XYZW, WRITEMASK_XYZW);
   brw_ADD(p, dst, src0, negate(src1));
}

void
fs_visitor::generate_ddy(fs_inst *inst, struct brw_reg dst, struct brw_reg src)
{
   struct brw_reg src0 = brw_reg(src.file, src.nr, 0,
				 BRW_REGISTER_TYPE_F,
				 BRW_VERTICAL_STRIDE_4,
				 BRW_WIDTH_4,
				 BRW_HORIZONTAL_STRIDE_0,
				 BRW_SWIZZLE_XYZW, WRITEMASK_XYZW);
   struct brw_reg src1 = brw_reg(src.file, src.nr, 2,
				 BRW_REGISTER_TYPE_F,
				 BRW_VERTICAL_STRIDE_4,
				 BRW_WIDTH_4,
				 BRW_HORIZONTAL_STRIDE_0,
				 BRW_SWIZZLE_XYZW, WRITEMASK_XYZW);
   brw_ADD(p, dst, src0, negate(src1));
}

void
fs_visitor::generate_discard_not(fs_inst *inst, struct brw_reg mask)
{
   if (intel->gen >= 6) {
      /* Gen6 no longer has the mask reg for us to just read the
       * active channels from.  However, cmp updates just the channels
       * of the flag reg that are enabled, so we can get at the
       * channel enables that way.  In this step, make a reg of ones
       * we'll compare to.
       */
      brw_MOV(p, mask, brw_imm_ud(1));
   } else {
      brw_push_insn_state(p);
      brw_set_mask_control(p, BRW_MASK_DISABLE);
      brw_NOT(p, mask, brw_mask_reg(1)); /* IMASK */
      brw_pop_insn_state(p);
   }
}

void
fs_visitor::generate_discard_and(fs_inst *inst, struct brw_reg mask)
{
   if (intel->gen >= 6) {
      struct brw_reg f0 = brw_flag_reg();
      struct brw_reg g1 = retype(brw_vec1_grf(1, 7), BRW_REGISTER_TYPE_UW);

      brw_push_insn_state(p);
      brw_set_mask_control(p, BRW_MASK_DISABLE);
      brw_MOV(p, f0, brw_imm_uw(0xffff)); /* inactive channels undiscarded */
      brw_pop_insn_state(p);

      brw_CMP(p, retype(brw_null_reg(), BRW_REGISTER_TYPE_UD),
	      BRW_CONDITIONAL_Z, mask, brw_imm_ud(0)); /* active channels fail test */
      /* Undo CMP's whacking of predication*/
      brw_set_predicate_control(p, BRW_PREDICATE_NONE);

      brw_push_insn_state(p);
      brw_set_mask_control(p, BRW_MASK_DISABLE);
      brw_AND(p, g1, f0, g1);
      brw_pop_insn_state(p);
   } else {
      struct brw_reg g0 = retype(brw_vec1_grf(0, 0), BRW_REGISTER_TYPE_UW);

      mask = brw_uw1_reg(mask.file, mask.nr, 0);

      brw_push_insn_state(p);
      brw_set_mask_control(p, BRW_MASK_DISABLE);
      brw_AND(p, g0, mask, g0);
      brw_pop_insn_state(p);
   }
}

void
fs_visitor::generate_spill(fs_inst *inst, struct brw_reg src)
{
   assert(inst->mlen != 0);

   brw_MOV(p,
	   retype(brw_message_reg(inst->base_mrf + 1), BRW_REGISTER_TYPE_UD),
	   retype(src, BRW_REGISTER_TYPE_UD));
   brw_oword_block_write_scratch(p, brw_message_reg(inst->base_mrf), 1,
				 inst->offset);
}

void
fs_visitor::generate_unspill(fs_inst *inst, struct brw_reg dst)
{
   assert(inst->mlen != 0);

   /* Clear any post destination dependencies that would be ignored by
    * the block read.  See the B-Spec for pre-gen5 send instruction.
    *
    * This could use a better solution, since texture sampling and
    * math reads could potentially run into it as well -- anywhere
    * that we have a SEND with a destination that is a register that
    * was written but not read within the last N instructions (what's
    * N?  unsure).  This is rare because of dead code elimination, but
    * not impossible.
    */
   if (intel->gen == 4 && !intel->is_g4x)
      brw_MOV(p, brw_null_reg(), dst);

   brw_oword_block_read_scratch(p, dst, brw_message_reg(inst->base_mrf), 1,
				inst->offset);

   if (intel->gen == 4 && !intel->is_g4x) {
      /* gen4 errata: destination from a send can't be used as a
       * destination until it's been read.  Just read it so we don't
       * have to worry.
       */
      brw_MOV(p, brw_null_reg(), dst);
   }
}


void
fs_visitor::generate_pull_constant_load(fs_inst *inst, struct brw_reg dst)
{
   assert(inst->mlen != 0);

   /* Clear any post destination dependencies that would be ignored by
    * the block read.  See the B-Spec for pre-gen5 send instruction.
    *
    * This could use a better solution, since texture sampling and
    * math reads could potentially run into it as well -- anywhere
    * that we have a SEND with a destination that is a register that
    * was written but not read within the last N instructions (what's
    * N?  unsure).  This is rare because of dead code elimination, but
    * not impossible.
    */
   if (intel->gen == 4 && !intel->is_g4x)
      brw_MOV(p, brw_null_reg(), dst);

   brw_oword_block_read(p, dst, brw_message_reg(inst->base_mrf),
			inst->offset, SURF_INDEX_FRAG_CONST_BUFFER);

   if (intel->gen == 4 && !intel->is_g4x) {
      /* gen4 errata: destination from a send can't be used as a
       * destination until it's been read.  Just read it so we don't
       * have to worry.
       */
      brw_MOV(p, brw_null_reg(), dst);
   }
}

/**
 * To be called after the last _mesa_add_state_reference() call, to
 * set up prog_data.param[] for assign_curb_setup() and
 * setup_pull_constants().
 */
void
fs_visitor::setup_paramvalues_refs()
{
a615 1
   c->prog_data.first_curbe_grf = c->nr_payload_regs;
d617 5
d630 1
a630 1
	    struct brw_reg brw_reg = brw_vec1_grf(c->prog_data.first_curbe_grf +
d652 1
a652 1
	 if (brw->fragment_program->Base.InputsRead & BITFIELD64_BIT(i)) {
d682 1
a682 1
   int urb_start = c->prog_data.first_curbe_grf + c->prog_data.curb_read_length;
d746 1
a746 4
      if ((inst->opcode == FS_OPCODE_TEX ||
	   inst->opcode == FS_OPCODE_TXB ||
	   inst->opcode == FS_OPCODE_TXL) &&
	  inst->dst.file == GRF) {
d786 1
d807 5
d864 3
a866 1
   int bb_header_ip = 0;
d869 1
a869 1
      def[i] = 1 << 30;
d898 1
a898 2
	       if (!loop_depth || (this->virtual_grf_sizes[reg] == 1 &&
				   def[reg] >= bb_header_ip)) {
d914 1
a914 2
	    if (!loop_depth || (this->virtual_grf_sizes[reg] == 1 &&
				!inst->predicated)) {
a922 16

      /* Set the basic block header IP.  This is used for determining
       * if a complete def of single-register virtual GRF in a loop
       * dominates a use in the same basic block.  It's a quick way to
       * reduce the live interval range of most register used in a
       * loop.
       */
      if (inst->opcode == BRW_OPCODE_IF ||
	  inst->opcode == BRW_OPCODE_ELSE ||
	  inst->opcode == BRW_OPCODE_ENDIF ||
	  inst->opcode == BRW_OPCODE_DO ||
	  inst->opcode == BRW_OPCODE_WHILE ||
	  inst->opcode == BRW_OPCODE_BREAK ||
	  inst->opcode == BRW_OPCODE_CONTINUE) {
	 bb_header_ip = ip;
      }
d929 2
d946 2
d954 3
a956 1
	  inst->dst.type != inst->src[0].type)
d1007 1
d1010 1
d1012 19
d1035 12
d1048 1
d1055 1
a1055 1
	      scan_inst->opcode == FS_OPCODE_TEX)) {
d1061 3
d1078 2
d1091 3
d1101 2
d1107 23
a1148 8
	 if (scan_inst->opcode == BRW_OPCODE_DO ||
	     scan_inst->opcode == BRW_OPCODE_WHILE ||
	     scan_inst->opcode == BRW_OPCODE_ENDIF) {
	    interfered = true;
	    iter = scan_iter;
	    break;
	 }

d1152 1
a1152 1
		 scan_inst->opcode == FS_OPCODE_TEX)) {
d1158 1
a1158 1
		 scan_inst->opcode == FS_OPCODE_TEX)) {
d1167 1
a1167 1
	 if (intel->gen == 6 && scan_inst->is_math() && has_source_modifiers) {
a1175 4
      /* Update live interval so we don't have to recalculate. */
      this->virtual_grf_use[inst->src[0].reg] = MAX2(virtual_grf_use[inst->src[0].reg],
						     virtual_grf_use[inst->dst.reg]);

d1200 3
d1213 2
d1228 14
d1261 1
a1261 1
	    if (scan_inst->opcode == FS_OPCODE_TEX) {
d1269 3
a1271 1
	     * the channels.
d1276 8
d1333 2
a1334 3
	 if (scan_inst->dst.file == MRF &&
	     scan_inst->dst.hw_reg == inst->dst.hw_reg) {
	    /* Somebody else wrote our MRF here, so we can't can't
d1337 19
a1355 1
	    break;
d1364 6
a1369 2
	    if (inst->dst.hw_reg >= scan_inst->base_mrf &&
		inst->dst.hw_reg < scan_inst->base_mrf + scan_inst->mlen) {
d1389 4
d1426 1
a1426 1
	 /* Found a SEND instruction, which will include two of fewer
d1461 32
a1492 8
   /* For dead code, just check if the def interferes with the other range. */
   if (this->virtual_grf_use[a] == -1) {
      return (this->virtual_grf_def[a] >= this->virtual_grf_def[b] &&
	      this->virtual_grf_def[a] < this->virtual_grf_use[b]);
   }
   if (this->virtual_grf_use[b] == -1) {
      return (this->virtual_grf_def[b] >= this->virtual_grf_def[a] &&
	      this->virtual_grf_def[b] < this->virtual_grf_use[a]);
d1498 2
a1499 1
static struct brw_reg brw_reg_from_fs_reg(fs_reg *reg)
d1501 4
a1504 1
   struct brw_reg brw_reg;
d1506 4
a1509 28
   switch (reg->file) {
   case GRF:
   case ARF:
   case MRF:
      if (reg->smear == -1) {
	 brw_reg = brw_vec8_reg(reg->file,
				reg->hw_reg, 0);
      } else {
	 brw_reg = brw_vec1_reg(reg->file,
				reg->hw_reg, reg->smear);
      }
      brw_reg = retype(brw_reg, reg->type);
      break;
   case IMM:
      switch (reg->type) {
      case BRW_REGISTER_TYPE_F:
	 brw_reg = brw_imm_f(reg->imm.f);
	 break;
      case BRW_REGISTER_TYPE_D:
	 brw_reg = brw_imm_d(reg->imm.i);
	 break;
      case BRW_REGISTER_TYPE_UD:
	 brw_reg = brw_imm_ud(reg->imm.u);
	 break;
      default:
	 assert(!"not reached");
	 brw_reg = brw_null_reg();
	 break;
a1510 41
      break;
   case FIXED_HW_REG:
      brw_reg = reg->fixed_hw_reg;
      break;
   case BAD_FILE:
      /* Probably unused. */
      brw_reg = brw_null_reg();
      break;
   case UNIFORM:
      assert(!"not reached");
      brw_reg = brw_null_reg();
      break;
   default:
      assert(!"not reached");
      brw_reg = brw_null_reg();
      break;
   }
   if (reg->abs)
      brw_reg = brw_abs(brw_reg);
   if (reg->negate)
      brw_reg = negate(brw_reg);

   return brw_reg;
}

void
fs_visitor::generate_code()
{
   int last_native_inst = 0;
   const char *last_annotation_string = NULL;
   ir_instruction *last_annotation_ir = NULL;

   int if_stack_array_size = 16;
   int loop_stack_array_size = 16;
   int if_stack_depth = 0, loop_stack_depth = 0;
   brw_instruction **if_stack =
      rzalloc_array(this->mem_ctx, brw_instruction *, if_stack_array_size);
   brw_instruction **loop_stack =
      rzalloc_array(this->mem_ctx, brw_instruction *, loop_stack_array_size);
   int *if_depth_in_loop =
      rzalloc_array(this->mem_ctx, int, loop_stack_array_size);
d1512 2
d1515 1
a1515 3
   if (unlikely(INTEL_DEBUG & DEBUG_WM)) {
      printf("Native code for fragment shader %d:\n",
	     ctx->Shader.CurrentFragmentProgram->Name);
d1518 8
a1525 3
   foreach_iter(exec_list_iterator, iter, this->instructions) {
      fs_inst *inst = (fs_inst *)iter.get();
      struct brw_reg src[3], dst;
d1527 8
a1534 14
      if (unlikely(INTEL_DEBUG & DEBUG_WM)) {
	 if (last_annotation_ir != inst->ir) {
	    last_annotation_ir = inst->ir;
	    if (last_annotation_ir) {
	       printf("   ");
	       last_annotation_ir->print();
	       printf("\n");
	    }
	 }
	 if (last_annotation_string != inst->annotation) {
	    last_annotation_string = inst->annotation;
	    if (last_annotation_string)
	       printf("   %s\n", last_annotation_string);
	 }
d1536 2
d1539 1
a1539 19
      for (unsigned int i = 0; i < 3; i++) {
	 src[i] = brw_reg_from_fs_reg(&inst->src[i]);
      }
      dst = brw_reg_from_fs_reg(&inst->dst);

      brw_set_conditionalmod(p, inst->conditional_mod);
      brw_set_predicate_control(p, inst->predicated);
      brw_set_saturate(p, inst->saturate);

      switch (inst->opcode) {
      case BRW_OPCODE_MOV:
	 brw_MOV(p, dst, src[0]);
	 break;
      case BRW_OPCODE_ADD:
	 brw_ADD(p, dst, src[0], src[1]);
	 break;
      case BRW_OPCODE_MUL:
	 brw_MUL(p, dst, src[0], src[1]);
	 break;
d1541 1
a1541 12
      case BRW_OPCODE_FRC:
	 brw_FRC(p, dst, src[0]);
	 break;
      case BRW_OPCODE_RNDD:
	 brw_RNDD(p, dst, src[0]);
	 break;
      case BRW_OPCODE_RNDE:
	 brw_RNDE(p, dst, src[0]);
	 break;
      case BRW_OPCODE_RNDZ:
	 brw_RNDZ(p, dst, src[0]);
	 break;
d1543 2
a1544 21
      case BRW_OPCODE_AND:
	 brw_AND(p, dst, src[0], src[1]);
	 break;
      case BRW_OPCODE_OR:
	 brw_OR(p, dst, src[0], src[1]);
	 break;
      case BRW_OPCODE_XOR:
	 brw_XOR(p, dst, src[0], src[1]);
	 break;
      case BRW_OPCODE_NOT:
	 brw_NOT(p, dst, src[0]);
	 break;
      case BRW_OPCODE_ASR:
	 brw_ASR(p, dst, src[0], src[1]);
	 break;
      case BRW_OPCODE_SHR:
	 brw_SHR(p, dst, src[0], src[1]);
	 break;
      case BRW_OPCODE_SHL:
	 brw_SHL(p, dst, src[0], src[1]);
	 break;
d1546 3
a1548 6
      case BRW_OPCODE_CMP:
	 brw_CMP(p, dst, inst->conditional_mod, src[0], src[1]);
	 break;
      case BRW_OPCODE_SEL:
	 brw_SEL(p, dst, src[0], src[1]);
	 break;
d1550 1
a1550 15
      case BRW_OPCODE_IF:
	 if (inst->src[0].file != BAD_FILE) {
	    assert(intel->gen >= 6);
	    if_stack[if_stack_depth] = brw_IF_gen6(p, inst->conditional_mod, src[0], src[1]);
	 } else {
	    if_stack[if_stack_depth] = brw_IF(p, BRW_EXECUTE_8);
	 }
	 if_depth_in_loop[loop_stack_depth]++;
	 if_stack_depth++;
	 if (if_stack_array_size <= if_stack_depth) {
	    if_stack_array_size *= 2;
	    if_stack = reralloc(this->mem_ctx, if_stack, brw_instruction *,
			        if_stack_array_size);
	 }
	 break;
d1552 5
a1556 9
      case BRW_OPCODE_ELSE:
	 if_stack[if_stack_depth - 1] =
	    brw_ELSE(p, if_stack[if_stack_depth - 1]);
	 break;
      case BRW_OPCODE_ENDIF:
	 if_stack_depth--;
	 brw_ENDIF(p , if_stack[if_stack_depth]);
	 if_depth_in_loop[loop_stack_depth]--;
	 break;
d1558 1
a1558 11
      case BRW_OPCODE_DO:
	 loop_stack[loop_stack_depth++] = brw_DO(p, BRW_EXECUTE_8);
	 if (loop_stack_array_size <= loop_stack_depth) {
	    loop_stack_array_size *= 2;
	    loop_stack = reralloc(this->mem_ctx, loop_stack, brw_instruction *,
				  loop_stack_array_size);
	    if_depth_in_loop = reralloc(this->mem_ctx, if_depth_in_loop, int,
				        loop_stack_array_size);
	 }
	 if_depth_in_loop[loop_stack_depth] = 0;
	 break;
d1560 2
a1561 12
      case BRW_OPCODE_BREAK:
	 brw_BREAK(p, if_depth_in_loop[loop_stack_depth]);
	 brw_set_predicate_control(p, BRW_PREDICATE_NONE);
	 break;
      case BRW_OPCODE_CONTINUE:
	 /* FINISHME: We need to write the loop instruction support still. */
	 if (intel->gen >= 6)
	    brw_CONT_gen6(p, loop_stack[loop_stack_depth - 1]);
	 else
	    brw_CONT(p, if_depth_in_loop[loop_stack_depth]);
	 brw_set_predicate_control(p, BRW_PREDICATE_NONE);
	 break;
d1563 5
a1567 23
      case BRW_OPCODE_WHILE: {
	 struct brw_instruction *inst0, *inst1;
	 GLuint br = 1;

	 if (intel->gen >= 5)
	    br = 2;

	 assert(loop_stack_depth > 0);
	 loop_stack_depth--;
	 inst0 = inst1 = brw_WHILE(p, loop_stack[loop_stack_depth]);
	 if (intel->gen < 6) {
	    /* patch all the BREAK/CONT instructions from last BGNLOOP */
	    while (inst0 > loop_stack[loop_stack_depth]) {
	       inst0--;
	       if (inst0->header.opcode == BRW_OPCODE_BREAK &&
		   inst0->bits3.if_else.jump_count == 0) {
		  inst0->bits3.if_else.jump_count = br * (inst1 - inst0 + 1);
	    }
	       else if (inst0->header.opcode == BRW_OPCODE_CONTINUE &&
			inst0->bits3.if_else.jump_count == 0) {
		  inst0->bits3.if_else.jump_count = br * (inst1 - inst0);
	       }
	    }
a1569 1
	 break;
d1571 6
a1576 55
      case FS_OPCODE_RCP:
      case FS_OPCODE_RSQ:
      case FS_OPCODE_SQRT:
      case FS_OPCODE_EXP2:
      case FS_OPCODE_LOG2:
      case FS_OPCODE_POW:
      case FS_OPCODE_SIN:
      case FS_OPCODE_COS:
	 generate_math(inst, dst, src);
	 break;
      case FS_OPCODE_CINTERP:
	 brw_MOV(p, dst, src[0]);
	 break;
      case FS_OPCODE_LINTERP:
	 generate_linterp(inst, dst, src);
	 break;
      case FS_OPCODE_TEX:
      case FS_OPCODE_TXB:
      case FS_OPCODE_TXL:
	 generate_tex(inst, dst);
	 break;
      case FS_OPCODE_DISCARD_NOT:
	 generate_discard_not(inst, dst);
	 break;
      case FS_OPCODE_DISCARD_AND:
	 generate_discard_and(inst, src[0]);
	 break;
      case FS_OPCODE_DDX:
	 generate_ddx(inst, dst, src[0]);
	 break;
      case FS_OPCODE_DDY:
	 generate_ddy(inst, dst, src[0]);
	 break;

      case FS_OPCODE_SPILL:
	 generate_spill(inst, src[0]);
	 break;

      case FS_OPCODE_UNSPILL:
	 generate_unspill(inst, dst);
	 break;

      case FS_OPCODE_PULL_CONSTANT_LOAD:
	 generate_pull_constant_load(inst, dst);
	 break;

      case FS_OPCODE_FB_WRITE:
	 generate_fb_write(inst);
	 break;
      default:
	 if (inst->opcode < (int)ARRAY_SIZE(brw_opcodes)) {
	    _mesa_problem(ctx, "Unsupported opcode `%s' in FS",
			  brw_opcodes[inst->opcode].name);
	 } else {
	    _mesa_problem(ctx, "Unsupported opcode %d in FS", inst->opcode);
a1577 1
	 this->fail = true;
d1579 3
d1583 2
a1584 12
      if (unlikely(INTEL_DEBUG & DEBUG_WM)) {
	 for (unsigned int i = last_native_inst; i < p->nr_insn; i++) {
	    if (0) {
	       printf("0x%08x 0x%08x 0x%08x 0x%08x ",
		      ((uint32_t *)&p->store[i])[3],
		      ((uint32_t *)&p->store[i])[2],
		      ((uint32_t *)&p->store[i])[1],
		      ((uint32_t *)&p->store[i])[0]);
	    }
	    brw_disasm(stdout, &p->store[i], intel->gen);
	 }
      }
d1586 1
a1586 2
      last_native_inst = p->nr_insn;
   }
d1588 5
a1592 3
   ralloc_free(if_stack);
   ralloc_free(loop_stack);
   ralloc_free(if_depth_in_loop);
d1594 3
a1596 1
   brw_set_uip_jip(p);
d1598 1
a1598 17
   /* OK, while the INTEL_DEBUG=wm above is very nice for debugging FS
    * emit issues, it doesn't get the jump distances into the output,
    * which is often something we want to debug.  So this is here in
    * case you're doing that.
    */
   if (0) {
      if (unlikely(INTEL_DEBUG & DEBUG_WM)) {
	 for (unsigned int i = 0; i < p->nr_insn; i++) {
	    printf("0x%08x 0x%08x 0x%08x 0x%08x ",
		   ((uint32_t *)&p->store[i])[3],
		   ((uint32_t *)&p->store[i])[2],
		   ((uint32_t *)&p->store[i])[1],
		   ((uint32_t *)&p->store[i])[0]);
	    brw_disasm(stdout, &p->store[i], intel->gen);
	 }
      }
   }
d1601 3
a1603 2
GLboolean
brw_wm_fs_emit(struct brw_context *brw, struct brw_wm_compile *c)
a1605 2
   struct gl_context *ctx = &intel->ctx;
   struct gl_shader_program *prog = ctx->Shader.CurrentFragmentProgram;
d1608 1
a1608 1
      return GL_FALSE;
d1613 1
a1613 9
      return GL_FALSE;

   /* We always use 8-wide mode, at least for now.  For one, flow
    * control only works in 8-wide.  Also, when we're fragment shader
    * bound, we're almost always under register pressure as well, so
    * 8-wide would save us from the performance cliff of spilling
    * regs.
    */
   c->dispatch_width = 8;
d1618 1
a1618 1
      printf("\n");
d1623 37
a1659 1
   fs_visitor v(c, shader);
d1661 2
a1662 8
   if (0) {
      v.emit_dummy_fs();
   } else {
      v.calculate_urb_setup();
      if (intel->gen < 6)
	 v.emit_interpolation_setup_gen4();
      else
	 v.emit_interpolation_setup_gen6();
d1664 3
a1666 8
      /* Generate FS IR for main().  (the visitor only descends into
       * functions called "main").
       */
      foreach_iter(exec_list_iterator, iter, *shader->ir) {
	 ir_instruction *ir = (ir_instruction *)iter.get();
	 v.base_ir = ir;
	 ir->accept(&v);
      }
d1668 3
a1670 1
      v.emit_fb_writes();
d1672 2
a1673 1
      v.split_virtual_grfs();
d1675 1
a1675 4
      v.setup_paramvalues_refs();
      v.setup_pull_constants();
      v.assign_curb_setup();
      v.assign_urb_setup();
d1677 4
a1680 3
      bool progress;
      do {
	 progress = false;
d1682 3
a1684 1
	 progress = v.remove_duplicate_mrf_writes() || progress;
d1686 1
a1686 6
	 v.calculate_live_intervals();
	 progress = v.propagate_constants() || progress;
	 progress = v.register_coalesce() || progress;
	 progress = v.compute_to_mrf() || progress;
	 progress = v.dead_code_eliminate() || progress;
      } while (progress);
d1688 3
a1690 8
      if (0) {
	 /* Debug of register spilling: Go spill everything. */
	 int virtual_grf_count = v.virtual_grf_next;
	 for (int i = 1; i < virtual_grf_count; i++) {
	    v.spill_reg(i);
	 }
	 v.calculate_live_intervals();
      }
d1692 3
a1694 6
      if (0)
	 v.assign_regs_trivial();
      else {
	 while (!v.assign_regs()) {
	    if (v.fail)
	       break;
d1696 3
a1698 3
	    v.calculate_live_intervals();
	 }
      }
d1701 3
a1703 2
   if (!v.fail)
      v.generate_code();
d1705 2
a1706 1
   assert(!v.fail); /* FINISHME: Cleanly fail, tested at link time, etc. */
d1708 1
a1708 2
   if (v.fail)
      return GL_FALSE;
d1710 2
a1711 1
   c->prog_data.total_grf = v.grf_used;
d1713 1
a1713 1
   return GL_TRUE;
@


1.1.1.1
log
@Import Mesa 7.10.3
@
text
@@


1.1.1.2
log
@Import Mesa 9.2.0
@
text
@a21 3
 */

/** @@file brw_fs.cpp
d23 3
a25 3
 * This file drives the GLSL IR -> LIR translation, contains the
 * optimizations on the LIR, and drives the generation of native code
 * from the LIR.
a31 1
#include "main/hash_table.h"
a34 1
#include "main/fbobject.h"
d37 1
d46 3
a48 141
#include "glsl/glsl_types.h"

void
fs_inst::init()
{
   memset(this, 0, sizeof(*this));
   this->opcode = BRW_OPCODE_NOP;
   this->conditional_mod = BRW_CONDITIONAL_NONE;

   this->dst = reg_undef;
   this->src[0] = reg_undef;
   this->src[1] = reg_undef;
   this->src[2] = reg_undef;

   /* This will be the case for almost all instructions. */
   this->regs_written = 1;
}

fs_inst::fs_inst()
{
   init();
}

fs_inst::fs_inst(enum opcode opcode)
{
   init();
   this->opcode = opcode;
}

fs_inst::fs_inst(enum opcode opcode, fs_reg dst)
{
   init();
   this->opcode = opcode;
   this->dst = dst;

   if (dst.file == GRF)
      assert(dst.reg_offset >= 0);
}

fs_inst::fs_inst(enum opcode opcode, fs_reg dst, fs_reg src0)
{
   init();
   this->opcode = opcode;
   this->dst = dst;
   this->src[0] = src0;

   if (dst.file == GRF)
      assert(dst.reg_offset >= 0);
   if (src[0].file == GRF)
      assert(src[0].reg_offset >= 0);
}

fs_inst::fs_inst(enum opcode opcode, fs_reg dst, fs_reg src0, fs_reg src1)
{
   init();
   this->opcode = opcode;
   this->dst = dst;
   this->src[0] = src0;
   this->src[1] = src1;

   if (dst.file == GRF)
      assert(dst.reg_offset >= 0);
   if (src[0].file == GRF)
      assert(src[0].reg_offset >= 0);
   if (src[1].file == GRF)
      assert(src[1].reg_offset >= 0);
}

fs_inst::fs_inst(enum opcode opcode, fs_reg dst,
		 fs_reg src0, fs_reg src1, fs_reg src2)
{
   init();
   this->opcode = opcode;
   this->dst = dst;
   this->src[0] = src0;
   this->src[1] = src1;
   this->src[2] = src2;

   if (dst.file == GRF)
      assert(dst.reg_offset >= 0);
   if (src[0].file == GRF)
      assert(src[0].reg_offset >= 0);
   if (src[1].file == GRF)
      assert(src[1].reg_offset >= 0);
   if (src[2].file == GRF)
      assert(src[2].reg_offset >= 0);
}

#define ALU1(op)                                                        \
   fs_inst *                                                            \
   fs_visitor::op(fs_reg dst, fs_reg src0)                              \
   {                                                                    \
      return new(mem_ctx) fs_inst(BRW_OPCODE_##op, dst, src0);          \
   }

#define ALU2(op)                                                        \
   fs_inst *                                                            \
   fs_visitor::op(fs_reg dst, fs_reg src0, fs_reg src1)                 \
   {                                                                    \
      return new(mem_ctx) fs_inst(BRW_OPCODE_##op, dst, src0, src1);    \
   }

#define ALU3(op)                                                        \
   fs_inst *                                                            \
   fs_visitor::op(fs_reg dst, fs_reg src0, fs_reg src1, fs_reg src2)    \
   {                                                                    \
      return new(mem_ctx) fs_inst(BRW_OPCODE_##op, dst, src0, src1, src2);\
   }

ALU1(NOT)
ALU1(MOV)
ALU1(FRC)
ALU1(RNDD)
ALU1(RNDE)
ALU1(RNDZ)
ALU2(ADD)
ALU2(MUL)
ALU2(MACH)
ALU2(AND)
ALU2(OR)
ALU2(XOR)
ALU2(SHL)
ALU2(SHR)
ALU2(ASR)
ALU3(LRP)
ALU1(BFREV)
ALU3(BFE)
ALU2(BFI1)
ALU3(BFI2)
ALU1(FBH)
ALU1(FBL)
ALU1(CBIT)

/** Gen4 predicated IF. */
fs_inst *
fs_visitor::IF(uint32_t predicate)
{
   fs_inst *inst = new(mem_ctx) fs_inst(BRW_OPCODE_IF);
   inst->predicate = predicate;
   return inst;
}
d50 1
a50 10
/** Gen6+ IF with embedded comparison. */
fs_inst *
fs_visitor::IF(fs_reg src0, fs_reg src1, uint32_t condition)
{
   assert(brw->gen >= 6);
   fs_inst *inst = new(mem_ctx) fs_inst(BRW_OPCODE_IF,
                                        reg_null_d, src0, src1);
   inst->conditional_mod = condition;
   return inst;
}
d52 2
a53 7
/**
 * CMP: Sets the low bit of the destination channels with the result
 * of the comparison, while the upper bits are undefined, and updates
 * the flag register with the packed 16 bits of the result.
 */
fs_inst *
fs_visitor::CMP(fs_reg dst, fs_reg src0, fs_reg src1, uint32_t condition)
d55 1
a55 1
   fs_inst *inst;
d57 5
a61 16
   /* Take the instruction:
    *
    * CMP null<d> src0<f> src1<f>
    *
    * Original gen4 does type conversion to the destination type before
    * comparison, producing garbage results for floating point comparisons.
    * gen5 does the comparison on the execution type (resolved source types),
    * so dst type doesn't matter.  gen6 does comparison and then uses the
    * result as if it was the dst type with no conversion, which happens to
    * mostly work out for float-interpreted-as-int since our comparisons are
    * for >0, =0, <0.
    */
   if (brw->gen == 4) {
      dst.type = src0.type;
      if (dst.file == HW_REG)
	 dst.fixed_hw_reg.type = dst.type;
d64 1
a64 7
   resolve_ud_negate(&src0);
   resolve_ud_negate(&src1);

   inst = new(mem_ctx) fs_inst(BRW_OPCODE_CMP, dst, src0, src1);
   inst->conditional_mod = condition;

   return inst;
d67 2
a68 4
exec_list
fs_visitor::VARYING_PULL_CONSTANT_LOAD(fs_reg dst, fs_reg surf_index,
                                       fs_reg varying_offset,
                                       uint32_t const_offset)
d70 5
a74 45
   exec_list instructions;
   fs_inst *inst;

   /* We have our constant surface use a pitch of 4 bytes, so our index can
    * be any component of a vector, and then we load 4 contiguous
    * components starting from that.
    *
    * We break down the const_offset to a portion added to the variable
    * offset and a portion done using reg_offset, which means that if you
    * have GLSL using something like "uniform vec4 a[20]; gl_FragColor =
    * a[i]", we'll temporarily generate 4 vec4 loads from offset i * 4, and
    * CSE can later notice that those loads are all the same and eliminate
    * the redundant ones.
    */
   fs_reg vec4_offset = fs_reg(this, glsl_type::int_type);
   instructions.push_tail(ADD(vec4_offset,
                              varying_offset, const_offset & ~3));

   int scale = 1;
   if (brw->gen == 4 && dispatch_width == 8) {
      /* Pre-gen5, we can either use a SIMD8 message that requires (header,
       * u, v, r) as parameters, or we can just use the SIMD16 message
       * consisting of (header, u).  We choose the second, at the cost of a
       * longer return length.
       */
      scale = 2;
   }

   enum opcode op;
   if (brw->gen >= 7)
      op = FS_OPCODE_VARYING_PULL_CONSTANT_LOAD_GEN7;
   else
      op = FS_OPCODE_VARYING_PULL_CONSTANT_LOAD;
   fs_reg vec4_result = fs_reg(GRF, virtual_grf_alloc(4 * scale), dst.type);
   inst = new(mem_ctx) fs_inst(op, vec4_result, surf_index, vec4_offset);
   inst->regs_written = 4 * scale;
   instructions.push_tail(inst);

   if (brw->gen < 7) {
      inst->base_mrf = 13;
      inst->header_present = true;
      if (brw->gen == 4)
         inst->mlen = 3;
      else
         inst->mlen = 1 + dispatch_width / 8;
d76 1
a76 25

   vec4_result.reg_offset += (const_offset & 3) * scale;
   instructions.push_tail(MOV(dst, vec4_result));

   return instructions;
}

/**
 * A helper for MOV generation for fixing up broken hardware SEND dependency
 * handling.
 */
fs_inst *
fs_visitor::DEP_RESOLVE_MOV(int grf)
{
   fs_inst *inst = MOV(brw_null_reg(), fs_reg(GRF, grf, BRW_REGISTER_TYPE_F));

   inst->ir = NULL;
   inst->annotation = "send dependency resolve";

   /* The caller always wants uncompressed to emit the minimal extra
    * dependencies, and to avoid having to deal with aligning its regs to 2.
    */
   inst->force_uncompressed = true;

   return inst;
d79 2
a80 2
bool
fs_inst::equals(fs_inst *inst)
d82 2
a83 17
   return (opcode == inst->opcode &&
           dst.equals(inst->dst) &&
           src[0].equals(inst->src[0]) &&
           src[1].equals(inst->src[1]) &&
           src[2].equals(inst->src[2]) &&
           saturate == inst->saturate &&
           predicate == inst->predicate &&
           conditional_mod == inst->conditional_mod &&
           mlen == inst->mlen &&
           base_mrf == inst->base_mrf &&
           sampler == inst->sampler &&
           target == inst->target &&
           eot == inst->eot &&
           header_present == inst->header_present &&
           shadow_compare == inst->shadow_compare &&
           offset == inst->offset);
}
d85 1
a85 7
bool
fs_inst::overwrites_reg(const fs_reg &reg)
{
   return (reg.file == dst.file &&
           reg.reg == dst.reg &&
           reg.reg_offset >= dst.reg_offset  &&
           reg.reg_offset < dst.reg_offset + regs_written);
d88 2
a89 2
bool
fs_inst::is_send_from_grf()
d91 2
a92 5
   return (opcode == FS_OPCODE_VARYING_PULL_CONSTANT_LOAD_GEN7 ||
           opcode == SHADER_OPCODE_SHADER_TIME_ADD ||
           (opcode == FS_OPCODE_UNIFORM_PULL_CONSTANT_LOAD &&
            src[1].file == GRF));
}
d94 5
a98 5
bool
fs_visitor::can_do_source_mods(fs_inst *inst)
{
   if (brw->gen == 6 && inst->is_math())
      return false;
d100 22
a121 2
   if (inst->is_send_from_grf())
      return false;
d123 2
a124 9
   return true;
}

void
fs_reg::init()
{
   memset(this, 0, sizeof(*this));
   this->smear = -1;
}
d126 2
a127 6
/** Generic unset register constructor. */
fs_reg::fs_reg()
{
   init();
   this->file = BAD_FILE;
}
d129 18
a146 8
/** Immediate value constructor. */
fs_reg::fs_reg(float f)
{
   init();
   this->file = IMM;
   this->type = BRW_REGISTER_TYPE_F;
   this->imm.f = f;
}
d148 1
a148 8
/** Immediate value constructor. */
fs_reg::fs_reg(int32_t i)
{
   init();
   this->file = IMM;
   this->type = BRW_REGISTER_TYPE_D;
   this->imm.i = i;
}
d150 3
a152 42
/** Immediate value constructor. */
fs_reg::fs_reg(uint32_t u)
{
   init();
   this->file = IMM;
   this->type = BRW_REGISTER_TYPE_UD;
   this->imm.u = u;
}

/** Fixed brw_reg Immediate value constructor. */
fs_reg::fs_reg(struct brw_reg fixed_hw_reg)
{
   init();
   this->file = HW_REG;
   this->fixed_hw_reg = fixed_hw_reg;
   this->type = fixed_hw_reg.type;
}

bool
fs_reg::equals(const fs_reg &r) const
{
   return (file == r.file &&
           reg == r.reg &&
           reg_offset == r.reg_offset &&
           type == r.type &&
           negate == r.negate &&
           abs == r.abs &&
           !reladdr && !r.reladdr &&
           memcmp(&fixed_hw_reg, &r.fixed_hw_reg,
                  sizeof(fixed_hw_reg)) == 0 &&
           smear == r.smear &&
           imm.u == r.imm.u);
}

bool
fs_reg::is_zero() const
{
   if (file != IMM)
      return false;

   return type == BRW_REGISTER_TYPE_F ? imm.f == 0.0 : imm.i == 0;
}
d154 2
a155 8
bool
fs_reg::is_one() const
{
   if (file != IMM)
      return false;

   return type == BRW_REGISTER_TYPE_F ? imm.f == 1.0 : imm.i == 1;
}
d157 1
a157 4
bool
fs_reg::is_valid_3src() const
{
   return file == GRF || file == UNIFORM;
d160 2
a161 2
int
fs_visitor::type_size(const struct glsl_type *type)
d184 1
a184 3
   case GLSL_TYPE_VOID:
   case GLSL_TYPE_ERROR:
   case GLSL_TYPE_INTERFACE:
d186 1
a186 134
      break;
   }

   return 0;
}

fs_reg
fs_visitor::get_timestamp()
{
   assert(brw->gen >= 7);

   fs_reg ts = fs_reg(retype(brw_vec1_reg(BRW_ARCHITECTURE_REGISTER_FILE,
                                          BRW_ARF_TIMESTAMP,
                                          0),
                             BRW_REGISTER_TYPE_UD));

   fs_reg dst = fs_reg(this, glsl_type::uint_type);

   fs_inst *mov = emit(MOV(dst, ts));
   /* We want to read the 3 fields we care about (mostly field 0, but also 2)
    * even if it's not enabled in the dispatch.
    */
   mov->force_writemask_all = true;
   mov->force_uncompressed = true;

   /* The caller wants the low 32 bits of the timestamp.  Since it's running
    * at the GPU clock rate of ~1.2ghz, it will roll over every ~3 seconds,
    * which is plenty of time for our purposes.  It is identical across the
    * EUs, but since it's tracking GPU core speed it will increment at a
    * varying rate as render P-states change.
    *
    * The caller could also check if render P-states have changed (or anything
    * else that might disrupt timing) by setting smear to 2 and checking if
    * that field is != 0.
    */
   dst.smear = 0;

   return dst;
}

void
fs_visitor::emit_shader_time_begin()
{
   current_annotation = "shader time start";
   shader_start_time = get_timestamp();
}

void
fs_visitor::emit_shader_time_end()
{
   current_annotation = "shader time end";

   enum shader_time_shader_type type, written_type, reset_type;
   if (dispatch_width == 8) {
      type = ST_FS8;
      written_type = ST_FS8_WRITTEN;
      reset_type = ST_FS8_RESET;
   } else {
      assert(dispatch_width == 16);
      type = ST_FS16;
      written_type = ST_FS16_WRITTEN;
      reset_type = ST_FS16_RESET;
   }

   fs_reg shader_end_time = get_timestamp();

   /* Check that there weren't any timestamp reset events (assuming these
    * were the only two timestamp reads that happened).
    */
   fs_reg reset = shader_end_time;
   reset.smear = 2;
   fs_inst *test = emit(AND(reg_null_d, reset, fs_reg(1u)));
   test->conditional_mod = BRW_CONDITIONAL_Z;
   emit(IF(BRW_PREDICATE_NORMAL));

   push_force_uncompressed();
   fs_reg start = shader_start_time;
   start.negate = true;
   fs_reg diff = fs_reg(this, glsl_type::uint_type);
   emit(ADD(diff, start, shader_end_time));

   /* If there were no instructions between the two timestamp gets, the diff
    * is 2 cycles.  Remove that overhead, so I can forget about that when
    * trying to determine the time taken for single instructions.
    */
   emit(ADD(diff, diff, fs_reg(-2u)));

   emit_shader_time_write(type, diff);
   emit_shader_time_write(written_type, fs_reg(1u));
   emit(BRW_OPCODE_ELSE);
   emit_shader_time_write(reset_type, fs_reg(1u));
   emit(BRW_OPCODE_ENDIF);

   pop_force_uncompressed();
}

void
fs_visitor::emit_shader_time_write(enum shader_time_shader_type type,
                                   fs_reg value)
{
   int shader_time_index =
      brw_get_shader_time_index(brw, shader_prog, &fp->Base, type);
   fs_reg offset = fs_reg(shader_time_index * SHADER_TIME_STRIDE);

   fs_reg payload;
   if (dispatch_width == 8)
      payload = fs_reg(this, glsl_type::uvec2_type);
   else
      payload = fs_reg(this, glsl_type::uint_type);

   emit(fs_inst(SHADER_OPCODE_SHADER_TIME_ADD,
                fs_reg(), payload, offset, value));
}

void
fs_visitor::fail(const char *format, ...)
{
   va_list va;
   char *msg;

   if (failed)
      return;

   failed = true;

   va_start(va, format);
   msg = ralloc_vasprintf(mem_ctx, format, va);
   va_end(va);
   msg = ralloc_asprintf(mem_ctx, "FS compile failed: %s\n", msg);

   this->fail_msg = msg;

   if (INTEL_DEBUG & DEBUG_WM) {
      fprintf(stderr, "%s",  msg);
a189 73
fs_inst *
fs_visitor::emit(enum opcode opcode)
{
   return emit(fs_inst(opcode));
}

fs_inst *
fs_visitor::emit(enum opcode opcode, fs_reg dst)
{
   return emit(fs_inst(opcode, dst));
}

fs_inst *
fs_visitor::emit(enum opcode opcode, fs_reg dst, fs_reg src0)
{
   return emit(fs_inst(opcode, dst, src0));
}

fs_inst *
fs_visitor::emit(enum opcode opcode, fs_reg dst, fs_reg src0, fs_reg src1)
{
   return emit(fs_inst(opcode, dst, src0, src1));
}

fs_inst *
fs_visitor::emit(enum opcode opcode, fs_reg dst,
                 fs_reg src0, fs_reg src1, fs_reg src2)
{
   return emit(fs_inst(opcode, dst, src0, src1, src2));
}

void
fs_visitor::push_force_uncompressed()
{
   force_uncompressed_stack++;
}

void
fs_visitor::pop_force_uncompressed()
{
   force_uncompressed_stack--;
   assert(force_uncompressed_stack >= 0);
}

void
fs_visitor::push_force_sechalf()
{
   force_sechalf_stack++;
}

void
fs_visitor::pop_force_sechalf()
{
   force_sechalf_stack--;
   assert(force_sechalf_stack >= 0);
}

/**
 * Returns true if the instruction has a flag that means it won't
 * update an entire destination register.
 *
 * For example, dead code elimination and live variable analysis want to know
 * when a write to a variable screens off any preceding values that were in
 * it.
 */
bool
fs_inst::is_partial_write()
{
   return (this->predicate ||
           this->force_uncompressed ||
           this->force_sechalf);
}

d203 11
a213 13
   case SHADER_OPCODE_RCP:
   case SHADER_OPCODE_RSQ:
   case SHADER_OPCODE_SQRT:
   case SHADER_OPCODE_EXP2:
   case SHADER_OPCODE_LOG2:
   case SHADER_OPCODE_SIN:
   case SHADER_OPCODE_COS:
      return 1 * dispatch_width / 8;
   case SHADER_OPCODE_POW:
   case SHADER_OPCODE_INT_QUOTIENT:
   case SHADER_OPCODE_INT_REMAINDER:
      return 2 * dispatch_width / 8;
   case SHADER_OPCODE_TEX:
d215 1
a215 6
   case SHADER_OPCODE_TXD:
   case SHADER_OPCODE_TXF:
   case SHADER_OPCODE_TXF_MS:
   case SHADER_OPCODE_TXL:
   case SHADER_OPCODE_TXS:
   case SHADER_OPCODE_LOD:
d219 1
a219 1
   case FS_OPCODE_UNIFORM_PULL_CONSTANT_LOAD:
a221 2
   case FS_OPCODE_VARYING_PULL_CONSTANT_LOAD:
      return inst->mlen;
d233 1
a233 1
   if (virtual_grf_array_size <= virtual_grf_count) {
d240 3
d244 2
a245 2
   virtual_grf_sizes[virtual_grf_count] = size;
   return virtual_grf_count++;
d249 1
a249 1
fs_reg::fs_reg(enum register_file file, int reg)
d253 1
a253 1
   this->reg = reg;
d258 1
a258 1
fs_reg::fs_reg(enum register_file file, int reg, uint32_t type)
d262 1
a262 1
   this->reg = reg;
d266 25
d297 1
a297 1
   this->reg = v->virtual_grf_alloc(v->type_size(type));
a307 27
void
import_uniforms_callback(const void *key,
			 void *data,
			 void *closure)
{
   struct hash_table *dst_ht = (struct hash_table *)closure;
   const fs_reg *reg = (const fs_reg *)data;

   if (reg->file != UNIFORM)
      return;

   hash_table_insert(dst_ht, data, key);
}

/* For 16-wide, we need to follow from the uniform setup of 8-wide dispatch.
 * This brings in those uniform definitions
 */
void
fs_visitor::import_uniforms(fs_visitor *v)
{
   hash_table_call_foreach(v->variable_ht,
			   import_uniforms_callback,
			   variable_ht);
   this->params_remap = v->params_remap;
   this->nr_params_remap = v->nr_params_remap;
}

d313 2
a314 2
void
fs_visitor::setup_uniform_values(ir_variable *ir)
d316 1
a316 1
   int namelen = strlen(ir->name);
d318 4
a321 9
   /* The data for our (non-builtin) uniforms is stored in a series of
    * gl_uniform_driver_storage structs for each subcomponent that
    * glGetUniformLocation() could name.  We know it's been set up in the same
    * order we'd walk the type, so walk the list of storage and find anything
    * with our name, or the prefix of a component that starts with our name.
    */
   unsigned params_before = c->prog_data.nr_params;
   for (unsigned u = 0; u < shader_prog->NumUserUniformStorage; u++) {
      struct gl_uniform_storage *storage = &shader_prog->UniformStorage[u];
d323 2
a324 5
      if (strncmp(ir->name, storage->name, namelen) != 0 ||
          (storage->name[namelen] != 0 &&
           storage->name[namelen] != '.' &&
           storage->name[namelen] != '[')) {
         continue;
d327 2
a328 3
      unsigned slots = storage->type->component_slots();
      if (storage->array_elements)
         slots *= storage->array_elements;
d330 43
a372 3
      for (unsigned i = 0; i < slots; i++) {
         c->prog_data.param[c->prog_data.nr_params++] =
            &storage->storage[i].f;
d374 9
a383 5

   /* Make sure we actually initialized the right amount of stuff here. */
   assert(params_before + ir->type->component_slots() ==
          c->prog_data.nr_params);
   (void)params_before;
d394 36
a429 2
   const ir_state_slot *const slots = ir->state_slots;
   assert(ir->state_slots != NULL);
d431 10
a440 17
   for (unsigned int i = 0; i < ir->num_state_slots; i++) {
      /* This state reference has already been setup by ir_to_mesa, but we'll
       * get the same index back here.
       */
      int index = _mesa_add_state_reference(this->fp->Base.Parameters,
					    (gl_state_index *)slots[i].tokens);

      /* Add each of the unique swizzles of the element as a parameter.
       * This'll end up matching the expected layout of the
       * array/matrix/structure we're trying to fill in.
       */
      int last_swiz = -1;
      for (unsigned int j = 0; j < 4; j++) {
	 int swiz = GET_SWZ(slots[i].swizzle, j);
	 if (swiz == last_swiz)
	    break;
	 last_swiz = swiz;
d442 6
a447 2
	 c->prog_data.param[c->prog_data.nr_params++] =
            &fp->Base.Parameters->ParameterValues[index][swiz].f;
d457 2
d463 1
a463 1
      emit(MOV(wpos, this->pixel_x));
d465 1
a465 1
      emit(ADD(wpos, this->pixel_x, fs_reg(0.5f)));
d471 1
a471 1
      emit(MOV(wpos, this->pixel_y));
d481 1
a481 1
      emit(ADD(wpos, pixel_y, fs_reg(offset)));
d486 3
a488 2
   if (brw->gen >= 6) {
      emit(MOV(wpos, fs_reg(brw_vec8_grf(c->source_depth_reg, 0))));
d490 2
a491 4
      emit(FS_OPCODE_LINTERP, wpos,
           this->delta_x[BRW_WM_PERSPECTIVE_PIXEL_BARYCENTRIC],
           this->delta_y[BRW_WM_PERSPECTIVE_PIXEL_BARYCENTRIC],
           interp_reg(VARYING_SLOT_POS, 2));
d496 1
a496 1
   emit(BRW_OPCODE_MOV, wpos, this->wpos_w);
a500 30
fs_inst *
fs_visitor::emit_linterp(const fs_reg &attr, const fs_reg &interp,
                         glsl_interp_qualifier interpolation_mode,
                         bool is_centroid)
{
   brw_wm_barycentric_interp_mode barycoord_mode;
   if (brw->gen >= 6) {
      if (is_centroid) {
         if (interpolation_mode == INTERP_QUALIFIER_SMOOTH)
            barycoord_mode = BRW_WM_PERSPECTIVE_CENTROID_BARYCENTRIC;
         else
            barycoord_mode = BRW_WM_NONPERSPECTIVE_CENTROID_BARYCENTRIC;
      } else {
         if (interpolation_mode == INTERP_QUALIFIER_SMOOTH)
            barycoord_mode = BRW_WM_PERSPECTIVE_PIXEL_BARYCENTRIC;
         else
            barycoord_mode = BRW_WM_NONPERSPECTIVE_PIXEL_BARYCENTRIC;
      }
   } else {
      /* On Ironlake and below, there is only one interpolation mode.
       * Centroid interpolation doesn't mean anything on this hardware --
       * there is no multisampling.
       */
      barycoord_mode = BRW_WM_PERSPECTIVE_PIXEL_BARYCENTRIC;
   }
   return emit(FS_OPCODE_LINTERP, attr,
               this->delta_x[barycoord_mode],
               this->delta_y[barycoord_mode], interp);
}

d505 2
a506 1
   reg->type = brw_type_for_base_type(ir->type->get_scalar_type());
d515 1
a515 1
	 fail("dereferenced array '%s' has length 0\n", ir->name);
a522 3
   glsl_interp_qualifier interpolation_mode =
      ir->determine_interpolation_mode(c->key.flat_shade);

d535 4
a538 1
	 if (interpolation_mode == INTERP_QUALIFIER_FLAT) {
d543 2
a544 2
	    for (unsigned int k = 0; k < type->vector_elements; k++) {
	       struct brw_reg interp = interp_reg(location, k);
d546 1
a546 2
               interp.type = reg->type;
	       emit(FS_OPCODE_CINTERP, attr, fs_reg(interp));
d550 8
a557 26
	    /* Smooth/noperspective interpolation case. */
	    for (unsigned int k = 0; k < type->vector_elements; k++) {
	       /* FINISHME: At some point we probably want to push
		* this farther by giving similar treatment to the
		* other potentially constant components of the
		* attribute, as well as making brw_vs_constval.c
		* handle varyings other than gl_TexCoord.
		*/
               struct brw_reg interp = interp_reg(location, k);
               emit_linterp(attr, fs_reg(interp), interpolation_mode,
                            ir->centroid);
               if (brw->needs_unlit_centroid_workaround && ir->centroid) {
                  /* Get the pixel/sample mask into f0 so that we know
                   * which pixels are lit.  Then, for each channel that is
                   * unlit, replace the centroid data with non-centroid
                   * data.
                   */
                  emit(FS_OPCODE_MOV_DISPATCH_TO_FLAGS);
                  fs_inst *inst = emit_linterp(attr, fs_reg(interp),
                                               interpolation_mode, false);
                  inst->predicate = BRW_PREDICATE_NORMAL;
                  inst->predicate_inverse = true;
               }
               if (brw->gen < 6) {
                  emit(BRW_OPCODE_MUL, attr, attr, this->pixel_w);
               }
d561 10
d585 12
a596 6
   if (brw->gen >= 6) {
      emit(BRW_OPCODE_ASR, *reg,
	   fs_reg(retype(brw_vec1_grf(0, 0), BRW_REGISTER_TYPE_D)),
	   fs_reg(15));
      emit(BRW_OPCODE_NOT, *reg, *reg);
      emit(BRW_OPCODE_AND, *reg, *reg, fs_reg(1));
d602 6
a607 2
      emit(CMP(*reg, fs_reg(r1_6ud), fs_reg(1u << 31), BRW_CONDITIONAL_L));
      emit(BRW_OPCODE_AND, *reg, *reg, fs_reg(1u));
a612 27
fs_reg
fs_visitor::fix_math_operand(fs_reg src)
{
   /* Can't do hstride == 0 args on gen6 math, so expand it out. We
    * might be able to do better by doing execsize = 1 math and then
    * expanding that result out, but we would need to be careful with
    * masking.
    *
    * The hardware ignores source modifiers (negate and abs) on math
    * instructions, so we also move to a temp to set those up.
    */
   if (brw->gen == 6 && src.file != UNIFORM && src.file != IMM &&
       !src.abs && !src.negate)
      return src;

   /* Gen7 relaxes most of the above restrictions, but still can't use IMM
    * operands to math
    */
   if (brw->gen >= 7 && src.file != IMM)
      return src;

   fs_reg expanded = fs_reg(this, glsl_type::float_type);
   expanded.type = src.type;
   emit(BRW_OPCODE_MOV, expanded, src);
   return expanded;
}

d614 1
a614 1
fs_visitor::emit_math(enum opcode opcode, fs_reg dst, fs_reg src)
d617 7
a623 7
   case SHADER_OPCODE_RCP:
   case SHADER_OPCODE_RSQ:
   case SHADER_OPCODE_SQRT:
   case SHADER_OPCODE_EXP2:
   case SHADER_OPCODE_LOG2:
   case SHADER_OPCODE_SIN:
   case SHADER_OPCODE_COS:
d635 1
a635 1
    * Gen 6 hardware ignores source modifiers (negate and abs) on math
d638 7
a644 2
   if (brw->gen >= 6)
      src = fix_math_operand(src);
d646 1
a646 1
   fs_inst *inst = emit(opcode, dst, src);
d648 1
a648 1
   if (brw->gen < 6) {
d650 1
a650 1
      inst->mlen = dispatch_width / 8;
d657 1
a657 1
fs_visitor::emit_math(enum opcode opcode, fs_reg dst, fs_reg src0, fs_reg src1)
d662 13
a674 12
   switch (opcode) {
   case SHADER_OPCODE_INT_QUOTIENT:
   case SHADER_OPCODE_INT_REMAINDER:
      if (brw->gen >= 7 && dispatch_width == 16)
	 fail("16-wide INTDIV unsupported\n");
      break;
   case SHADER_OPCODE_POW:
      break;
   default:
      assert(!"not reached: unsupported binary math opcode.");
      return NULL;
   }
d676 5
a680 3
   if (brw->gen >= 6) {
      src0 = fix_math_operand(src0);
      src1 = fix_math_operand(src1);
d682 1
a682 1
      inst = emit(opcode, dst, src0, src1);
d684 2
a685 15
      /* From the Ironlake PRM, Volume 4, Part 1, Section 6.1.13
       * "Message Payload":
       *
       * "Operand0[7].  For the INT DIV functions, this operand is the
       *  denominator."
       *  ...
       * "Operand1[7].  For the INT DIV functions, this operand is the
       *  numerator."
       */
      bool is_int_div = opcode != SHADER_OPCODE_POW;
      fs_reg &op0 = is_int_div ? src1 : src0;
      fs_reg &op1 = is_int_div ? src0 : src1;

      emit(BRW_OPCODE_MOV, fs_reg(MRF, base_mrf + 1, op1.type), op1);
      inst = emit(opcode, dst, op0, reg_null_f);
d688 1
a688 1
      inst->mlen = 2 * dispatch_width / 8;
d694 1
a694 1
fs_visitor::assign_curb_setup()
d696 1
a696 6
   c->prog_data.curb_read_length = ALIGN(c->prog_data.nr_params, 8) / 8;
   if (dispatch_width == 8) {
      c->prog_data.first_curbe_grf = c->nr_payload_regs;
   } else {
      c->prog_data.first_curbe_grf_16 = c->nr_payload_regs;
   }
d698 2
a699 3
   /* Map the offsets in the UNIFORM file to fixed HW regs. */
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;
d701 7
a707 6
      for (unsigned int i = 0; i < 3; i++) {
	 if (inst->src[i].file == UNIFORM) {
	    int constant_nr = inst->src[i].reg + inst->src[i].reg_offset;
	    struct brw_reg brw_reg = brw_vec1_grf(c->nr_payload_regs +
						  constant_nr / 8,
						  constant_nr % 8);
d709 7
a715 3
	    inst->src[i].file = HW_REG;
	    inst->src[i].fixed_hw_reg = retype(brw_reg, inst->src[i].type);
	 }
d717 3
a720 1
}
d722 2
a723 6
void
fs_visitor::calculate_urb_setup()
{
   for (unsigned int i = 0; i < VARYING_SLOT_MAX; i++) {
      urb_setup[i] = -1;
   }
d725 4
a728 26
   int urb_next = 0;
   /* Figure out where each of the incoming setup attributes lands. */
   if (brw->gen >= 6) {
      for (unsigned int i = 0; i < VARYING_SLOT_MAX; i++) {
	 if (fp->Base.InputsRead & BITFIELD64_BIT(i)) {
	    urb_setup[i] = urb_next++;
	 }
      }
   } else {
      /* FINISHME: The sf doesn't map VS->FS inputs for us very well. */
      for (unsigned int i = 0; i < VARYING_SLOT_MAX; i++) {
         /* Point size is packed into the header, not as a general attribute */
         if (i == VARYING_SLOT_PSIZ)
            continue;

	 if (c->key.input_slots_valid & BITFIELD64_BIT(i)) {
	    /* The back color slot is skipped when the front color is
	     * also written to.  In addition, some slots can be
	     * written in the vertex shader and not read in the
	     * fragment shader.  So the register number must always be
	     * incremented, mapped or not.
	     */
	    if (_mesa_varying_slot_in_fs((gl_varying_slot) i))
	       urb_setup[i] = urb_next;
            urb_next++;
	 }
d731 2
a732 8
      /*
       * It's a FS only attribute, and we did interpolation for this attribute
       * in SF thread. So, count it here, too.
       *
       * See compile_sf_prog() for more info.
       */
      if (fp->Base.InputsRead & BITFIELD64_BIT(VARYING_SLOT_PNTC))
         urb_setup[VARYING_SLOT_PNTC] = urb_next++;
d735 4
a738 2
   /* Each attribute is 4 setup channels, each of which is half a reg. */
   c->prog_data.urb_read_length = urb_next * 2;
d742 1
a742 1
fs_visitor::assign_urb_setup()
d744 3
a746 1
   int urb_start = c->nr_payload_regs + c->prog_data.curb_read_length;
d748 4
a751 5
   /* Offset all the urb_setup[] index by the actual position of the
    * setup regs, now that the location of the constants has been chosen.
    */
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;
d753 1
a753 4
      if (inst->opcode == FS_OPCODE_LINTERP) {
	 assert(inst->src[2].file == HW_REG);
	 inst->src[2].fixed_hw_reg.nr += urb_start;
      }
d755 5
a759 4
      if (inst->opcode == FS_OPCODE_CINTERP) {
	 assert(inst->src[0].file == HW_REG);
	 inst->src[0].fixed_hw_reg.nr += urb_start;
      }
d761 2
a762 2

   this->first_non_payload_grf = urb_start + c->prog_data.urb_read_length;
a764 18
/**
 * Split large virtual GRFs into separate components if we can.
 *
 * This is mostly duplicated with what brw_fs_vector_splitting does,
 * but that's really conservative because it's afraid of doing
 * splitting that doesn't result in real progress after the rest of
 * the optimization phases, which would cause infinite looping in
 * optimization.  We can do it once here, safely.  This also has the
 * opportunity to split interpolated values, or maybe even uniforms,
 * which we don't have at the IR level.
 *
 * We want to split, because virtual GRFs are what we register
 * allocate and spill (due to contiguousness requirements for some
 * instructions), and they're what we naturally generate in the
 * codegen process, but most virtual GRFs don't actually need to be
 * contiguous sets of GRFs.  If we split, we'll end up with reduced
 * live intervals and better dead code elimination and coalescing.
 */
d766 1
a766 1
fs_visitor::split_virtual_grfs()
d768 5
a772 3
   int num_vars = this->virtual_grf_count;
   bool split_grf[num_vars];
   int new_virtual_grf[num_vars];
d774 2
a775 7
   /* Try to split anything > 0 sized. */
   for (int i = 0; i < num_vars; i++) {
      if (this->virtual_grf_sizes[i] != 1)
	 split_grf[i] = true;
      else
	 split_grf[i] = false;
   }
d777 7
a783 9
   if (brw->has_pln &&
       this->delta_x[BRW_WM_PERSPECTIVE_PIXEL_BARYCENTRIC].file == GRF) {
      /* PLN opcodes rely on the delta_xy being contiguous.  We only have to
       * check this for BRW_WM_PERSPECTIVE_PIXEL_BARYCENTRIC, because prior to
       * Gen6, that was the only supported interpolation mode, and since Gen6,
       * delta_x and delta_y are in fixed hardware registers.
       */
      split_grf[this->delta_x[BRW_WM_PERSPECTIVE_PIXEL_BARYCENTRIC].reg] =
         false;
d785 1
d787 7
a793 2
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;
d795 2
a796 6
      /* If there's a SEND message that requires contiguous destination
       * registers, no splitting is allowed.
       */
      if (inst->regs_written > 1) {
	 split_grf[inst->dst.reg] = false;
      }
d798 2
a799 7
      /* If we're sending from a GRF, don't split it, on the assumption that
       * the send is reading the whole thing.
       */
      if (inst->is_send_from_grf()) {
         split_grf[inst->src[0].reg] = false;
      }
   }
d801 3
a803 14
   /* Allocate new space for split regs.  Note that the virtual
    * numbers will be contiguous.
    */
   for (int i = 0; i < num_vars; i++) {
      if (split_grf[i]) {
	 new_virtual_grf[i] = virtual_grf_alloc(1);
	 for (int j = 2; j < this->virtual_grf_sizes[i]; j++) {
	    int reg = virtual_grf_alloc(1);
	    assert(reg == new_virtual_grf[i] + j - 1);
	    (void) reg;
	 }
	 this->virtual_grf_sizes[i] = 1;
      }
   }
d805 2
a806 2
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;
d808 21
a828 16
      if (inst->dst.file == GRF &&
	  split_grf[inst->dst.reg] &&
	  inst->dst.reg_offset != 0) {
	 inst->dst.reg = (new_virtual_grf[inst->dst.reg] +
			  inst->dst.reg_offset - 1);
	 inst->dst.reg_offset = 0;
      }
      for (int i = 0; i < 3; i++) {
	 if (inst->src[i].file == GRF &&
	     split_grf[inst->src[i].reg] &&
	     inst->src[i].reg_offset != 0) {
	    inst->src[i].reg = (new_virtual_grf[inst->src[i].reg] +
				inst->src[i].reg_offset - 1);
	    inst->src[i].reg_offset = 0;
	 }
      }
a829 1
   this->live_intervals_valid = false;
a831 9
/**
 * Remove unused virtual GRFs and compact the virtual_grf_* arrays.
 *
 * During code generation, we create tons of temporary variables, many of
 * which get immediately killed and are never used again.  Yet, in later
 * optimization and analysis passes, such as compute_live_intervals, we need
 * to loop over all the virtual GRFs.  Compacting them can save a lot of
 * overhead.
 */
d833 1
a833 1
fs_visitor::compact_virtual_grfs()
d835 3
a837 3
   /* Mark which virtual GRFs are used, and count how many. */
   int remap_table[this->virtual_grf_count];
   memset(remap_table, -1, sizeof(remap_table));
d839 1
a839 2
   foreach_list(node, &this->instructions) {
      const fs_inst *inst = (const fs_inst *) node;
d841 2
a842 2
      if (inst->dst.file == GRF)
         remap_table[inst->dst.reg] = 0;
d844 7
a850 3
      for (int i = 0; i < 3; i++) {
         if (inst->src[i].file == GRF)
            remap_table[inst->src[i].reg] = 0;
d852 9
d863 2
a864 2
   /* In addition to registers used in instructions, fs_visitor keeps
    * direct references to certain special values which must be patched:
d866 20
a885 31
   fs_reg *special[] = {
      &frag_depth, &pixel_x, &pixel_y, &pixel_w, &wpos_w, &dual_src_output,
      &outputs[0], &outputs[1], &outputs[2], &outputs[3],
      &outputs[4], &outputs[5], &outputs[6], &outputs[7],
      &delta_x[0], &delta_x[1], &delta_x[2],
      &delta_x[3], &delta_x[4], &delta_x[5],
      &delta_y[0], &delta_y[1], &delta_y[2],
      &delta_y[3], &delta_y[4], &delta_y[5],
   };
   STATIC_ASSERT(BRW_WM_BARYCENTRIC_INTERP_MODE_COUNT == 6);
   STATIC_ASSERT(BRW_MAX_DRAW_BUFFERS == 8);

   /* Treat all special values as used, to be conservative */
   for (unsigned i = 0; i < ARRAY_SIZE(special); i++) {
      if (special[i]->file == GRF)
	 remap_table[special[i]->reg] = 0;
   }

   /* Compact the GRF arrays. */
   int new_index = 0;
   for (int i = 0; i < this->virtual_grf_count; i++) {
      if (remap_table[i] != -1) {
         remap_table[i] = new_index;
         virtual_grf_sizes[new_index] = virtual_grf_sizes[i];
         if (live_intervals_valid) {
            virtual_grf_start[new_index] = virtual_grf_start[i];
            virtual_grf_end[new_index] = virtual_grf_end[i];
         }
         ++new_index;
      }
   }
d887 1
a887 5
   this->virtual_grf_count = new_index;

   /* Patch all the instructions to use the newly renumbered registers */
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *) node;
d889 9
a897 2
      if (inst->dst.file == GRF)
         inst->dst.reg = remap_table[inst->dst.reg];
d899 4
a902 5
      for (int i = 0; i < 3; i++) {
         if (inst->src[i].file == GRF)
            inst->src[i].reg = remap_table[inst->src[i].reg];
      }
   }
d904 18
a921 6
   /* Patch all the references to special values */
   for (unsigned i = 0; i < ARRAY_SIZE(special); i++) {
      if (special[i]->file == GRF && remap_table[special[i]->reg] != -1)
	 special[i]->reg = remap_table[special[i]->reg];
   }
}
d923 6
a928 13
bool
fs_visitor::remove_dead_constants()
{
   if (dispatch_width == 8) {
      this->params_remap = ralloc_array(mem_ctx, int, c->prog_data.nr_params);
      this->nr_params_remap = c->prog_data.nr_params;

      for (unsigned int i = 0; i < c->prog_data.nr_params; i++)
	 this->params_remap[i] = -1;

      /* Find which params are still in use. */
      foreach_list(node, &this->instructions) {
	 fs_inst *inst = (fs_inst *)node;
d930 6
a935 2
	 for (int i = 0; i < 3; i++) {
	    int constant_nr = inst->src[i].reg + inst->src[i].reg_offset;
d937 9
a945 2
	    if (inst->src[i].file != UNIFORM)
	       continue;
d947 175
a1121 8
	    /* Section 5.11 of the OpenGL 4.3 spec says:
	     *
	     *     "Out-of-bounds reads return undefined values, which include
	     *     values from other variables of the active program or zero."
	     */
	    if (constant_nr < 0 || constant_nr >= (int)c->prog_data.nr_params) {
	       constant_nr = 0;
	    }
d1123 22
a1144 5
	    /* For now, set this to non-negative.  We'll give it the
	     * actual new number in a moment, in order to keep the
	     * register numbers nicely ordered.
	     */
	    this->params_remap[constant_nr] = 0;
d1146 1
d1148 4
d1153 54
a1206 4
      /* Figure out what the new numbers for the params will be.  At some
       * point when we're doing uniform array access, we're going to want
       * to keep the distinction between .reg and .reg_offset, but for
       * now we don't care.
d1208 399
a1606 4
      unsigned int new_nr_params = 0;
      for (unsigned int i = 0; i < c->prog_data.nr_params; i++) {
	 if (this->params_remap[i] != -1) {
	    this->params_remap[i] = new_nr_params++;
d1608 20
d1629 4
d1634 9
a1642 3
      /* Update the list of params to be uploaded to match our new numbering. */
      for (unsigned int i = 0; i < c->prog_data.nr_params; i++) {
	 int remapped = this->params_remap[i];
d1644 8
a1651 2
	 if (remapped == -1)
	    continue;
d1653 66
a1718 1
	 c->prog_data.param[remapped] = c->prog_data.param[i];
d1720 8
d1729 12
a1740 1
      c->prog_data.nr_params = new_nr_params;
d1742 22
a1763 2
      /* This should have been generated in the 8-wide pass already. */
      assert(this->params_remap);
d1766 20
a1785 3
   /* Now do the renumbering of the shader to remove unused params. */
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;
d1787 826
a2612 2
      for (int i = 0; i < 3; i++) {
	 int constant_nr = inst->src[i].reg + inst->src[i].reg_offset;
d2614 3
a2616 10
	 if (inst->src[i].file != UNIFORM)
	    continue;

	 /* as above alias to 0 */
	 if (constant_nr < 0 || constant_nr >= (int)this->nr_params_remap) {
	    constant_nr = 0;
	 }
	 assert(this->params_remap[constant_nr] != -1);
	 inst->src[i].reg = this->params_remap[constant_nr];
	 inst->src[i].reg_offset = 0;
d2620 1
a2620 1
   return true;
d2623 10
a2632 3
/*
 * Implements array access of uniforms by inserting a
 * PULL_CONSTANT_LOAD instruction.
d2634 6
a2639 7
 * Unlike temporary GRF array access (where we don't support it due to
 * the difficulty of doing relative addressing on instruction
 * destinations), we could potentially do array access of uniforms
 * that were loaded in GRF space as push constants.  In real-world
 * usage we've seen, though, the arrays being used are always larger
 * than we could load as push constants, so just always move all
 * uniform array access out to a pull constant buffer.
d2642 1
a2642 1
fs_visitor::move_uniform_array_access_to_pull_constants()
d2644 11
a2654 1
   int pull_constant_loc[c->prog_data.nr_params];
d2656 15
a2670 2
   for (unsigned int i = 0; i < c->prog_data.nr_params; i++) {
      pull_constant_loc[i] = -1;
d2673 2
a2674 5
   /* Walk through and find array access of uniforms.  Put a copy of that
    * uniform in the pull constant buffer.
    *
    * Note that we don't move constant-indexed accesses to arrays.  No
    * testing has been done of the performance impact of this choice.
d2676 14
a2689 2
   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;
d2691 15
a2705 39
      for (int i = 0 ; i < 3; i++) {
         if (inst->src[i].file != UNIFORM || !inst->src[i].reladdr)
            continue;

         int uniform = inst->src[i].reg;

         /* If this array isn't already present in the pull constant buffer,
          * add it.
          */
         if (pull_constant_loc[uniform] == -1) {
            const float **values = &c->prog_data.param[uniform];

            pull_constant_loc[uniform] = c->prog_data.nr_pull_params;

            assert(param_size[uniform]);

            for (int j = 0; j < param_size[uniform]; j++) {
               c->prog_data.pull_param[c->prog_data.nr_pull_params++] =
                  values[j];
            }
         }

         /* Set up the annotation tracking for new generated instructions. */
         base_ir = inst->ir;
         current_annotation = inst->annotation;

         fs_reg surf_index = fs_reg((unsigned)SURF_INDEX_FRAG_CONST_BUFFER);
         fs_reg temp = fs_reg(this, glsl_type::float_type);
         exec_list list = VARYING_PULL_CONSTANT_LOAD(temp,
                                                     surf_index,
                                                     *inst->src[i].reladdr,
                                                     pull_constant_loc[uniform] +
                                                     inst->src[i].reg_offset);
         inst->insert_before(&list);

         inst->src[i].file = temp.file;
         inst->src[i].reg = temp.reg;
         inst->src[i].reg_offset = temp.reg_offset;
         inst->src[i].reladdr = NULL;
a2727 5
   if (dispatch_width == 16) {
      fail("Pull constants not supported in 16-wide\n");
      return;
   }

d2731 2
a2732 25
   unsigned int pull_uniform_base = max_uniform_components;

   int pull_constant_loc[c->prog_data.nr_params];
   for (unsigned int i = 0; i < c->prog_data.nr_params; i++) {
      if (i < pull_uniform_base) {
         pull_constant_loc[i] = -1;
      } else {
         pull_constant_loc[i] = -1;
         /* If our constant is already being uploaded for reladdr purposes,
          * reuse it.
          */
         for (unsigned int j = 0; j < c->prog_data.nr_pull_params; j++) {
            if (c->prog_data.pull_param[j] == c->prog_data.param[i]) {
               pull_constant_loc[i] = j;
               break;
            }
         }
         if (pull_constant_loc[i] == -1) {
            int pull_index = c->prog_data.nr_pull_params++;
            c->prog_data.pull_param[pull_index] = c->prog_data.param[i];
            pull_constant_loc[i] = pull_index;;
         }
      }
   }
   c->prog_data.nr_params = pull_uniform_base;
d2734 2
a2735 2
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;
d2741 2
a2742 3
         int pull_index = pull_constant_loc[inst->src[i].reg +
                                            inst->src[i].reg_offset];
         if (pull_index == -1)
a2744 2
         assert(!inst->src[i].reladdr);

d2746 3
a2748 5
	 fs_reg index = fs_reg((unsigned)SURF_INDEX_FRAG_CONST_BUFFER);
	 fs_reg offset = fs_reg((unsigned)(pull_index * 4) & ~15);
	 fs_inst *pull =
            new(mem_ctx) fs_inst(FS_OPCODE_UNIFORM_PULL_CONSTANT_LOAD,
                                 dst, index, offset);
d2751 2
d2759 1
a2759 1
	 inst->src[i].smear = pull_index & 3;
d2762 8
d2772 2
a2773 2
bool
fs_visitor::opt_algebraic()
d2775 6
a2780 1
   bool progress = false;
d2782 4
a2785 2
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;
d2787 9
a2795 4
      switch (inst->opcode) {
      case BRW_OPCODE_MUL:
	 if (inst->src[1].file != IMM)
	    continue;
d2797 28
a2824 6
	 /* a * 1.0 = a */
	 if (inst->src[1].is_one()) {
	    inst->opcode = BRW_OPCODE_MOV;
	    inst->src[1] = reg_undef;
	    progress = true;
	    break;
d2826 2
d2829 8
a2836 8
         /* a * 0.0 = 0.0 */
         if (inst->src[1].is_zero()) {
            inst->opcode = BRW_OPCODE_MOV;
            inst->src[0] = inst->src[1];
            inst->src[1] = reg_undef;
            progress = true;
            break;
         }
d2838 1
a2838 4
	 break;
      case BRW_OPCODE_ADD:
         if (inst->src[1].file != IMM)
            continue;
d2840 14
a2853 10
         /* a + 0.0 = a */
         if (inst->src[1].is_zero()) {
            inst->opcode = BRW_OPCODE_MOV;
            inst->src[1] = reg_undef;
            progress = true;
            break;
         }
         break;
      default:
	 break;
d2857 4
a2860 1
   return progress;
d2864 5
a2868 2
 * Removes any instructions writing a VGRF where that VGRF is not used by any
 * later instruction.
d2870 1
d2872 1
a2872 1
fs_visitor::dead_code_eliminate()
a2874 1
   int pc = 0;
d2876 2
a2877 1
   calculate_live_intervals();
d2879 5
a2883 2
   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;
d2885 5
a2889 7
      if (inst->dst.file == GRF) {
         assert(this->virtual_grf_end[inst->dst.reg] >= pc);
         if (this->virtual_grf_end[inst->dst.reg] == pc) {
            inst->remove();
            progress = true;
         }
      }
d2891 14
a2904 2
      pc++;
   }
d2906 5
a2910 2
   if (progress)
      live_intervals_valid = false;
d2912 5
a2916 2
   return progress;
}
d2918 5
a2922 5
struct dead_code_hash_key
{
   int vgrf;
   int reg_offset;
};
d2924 19
a2942 5
static bool
dead_code_hash_compare(const void *a, const void *b)
{
   return memcmp(a, b, sizeof(struct dead_code_hash_key)) == 0;
}
d2944 6
a2949 135
static void
clear_dead_code_hash(struct hash_table *ht)
{
   struct hash_entry *entry;

   hash_table_foreach(ht, entry) {
      _mesa_hash_table_remove(ht, entry);
   }
}

static void
insert_dead_code_hash(struct hash_table *ht,
                      int vgrf, int reg_offset, fs_inst *inst)
{
   /* We don't bother freeing keys, because they'll be GCed with the ht. */
   struct dead_code_hash_key *key = ralloc(ht, struct dead_code_hash_key);

   key->vgrf = vgrf;
   key->reg_offset = reg_offset;

   _mesa_hash_table_insert(ht, _mesa_hash_data(key, sizeof(*key)), key, inst);
}

static struct hash_entry *
get_dead_code_hash_entry(struct hash_table *ht, int vgrf, int reg_offset)
{
   struct dead_code_hash_key key;

   key.vgrf = vgrf;
   key.reg_offset = reg_offset;

   return _mesa_hash_table_search(ht, _mesa_hash_data(&key, sizeof(key)), &key);
}

static void
remove_dead_code_hash(struct hash_table *ht,
                      int vgrf, int reg_offset)
{
   struct hash_entry *entry = get_dead_code_hash_entry(ht, vgrf, reg_offset);
   if (!entry)
      return;

   _mesa_hash_table_remove(ht, entry);
}

/**
 * Walks basic blocks, removing any regs that are written but not read before
 * being redefined.
 *
 * The dead_code_eliminate() function implements a global dead code
 * elimination, but it only handles the removing the last write to a register
 * if it's never read.  This one can handle intermediate writes, but only
 * within a basic block.
 */
bool
fs_visitor::dead_code_eliminate_local()
{
   struct hash_table *ht;
   bool progress = false;

   ht = _mesa_hash_table_create(mem_ctx, dead_code_hash_compare);

   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      /* At a basic block, empty the HT since we don't understand dataflow
       * here.
       */
      if (inst->is_control_flow()) {
         clear_dead_code_hash(ht);
         continue;
      }

      /* Clear the HT of any instructions that got read. */
      for (int i = 0; i < 3; i++) {
         fs_reg src = inst->src[i];
         if (src.file != GRF)
            continue;

         int read = 1;
         if (inst->is_send_from_grf())
            read = virtual_grf_sizes[src.reg] - src.reg_offset;

         for (int reg_offset = src.reg_offset;
              reg_offset < src.reg_offset + read;
              reg_offset++) {
            remove_dead_code_hash(ht, src.reg, reg_offset);
         }
      }

      /* Add any update of a GRF to the HT, removing a previous write if it
       * wasn't read.
       */
      if (inst->dst.file == GRF) {
         if (inst->regs_written > 1) {
            /* We don't know how to trim channels from an instruction's
             * writes, so we can't incrementally remove unread channels from
             * it.  Just remove whatever it overwrites from the table
             */
            for (int i = 0; i < inst->regs_written; i++) {
               remove_dead_code_hash(ht,
                                     inst->dst.reg,
                                     inst->dst.reg_offset + i);
            }
         } else {
            struct hash_entry *entry =
               get_dead_code_hash_entry(ht, inst->dst.reg,
                                        inst->dst.reg_offset);

            if (inst->is_partial_write()) {
               /* For a partial write, we can't remove any previous dead code
                * candidate, since we're just modifying their result, but we can
                * be dead code eliminiated ourselves.
                */
               if (entry) {
                  entry->data = inst;
               } else {
                  insert_dead_code_hash(ht, inst->dst.reg, inst->dst.reg_offset,
                                        inst);
               }
            } else {
               if (entry) {
                  /* We're completely updating a channel, and there was a
                   * previous write to the channel that wasn't read.  Kill it!
                   */
                  fs_inst *inst = (fs_inst *)entry->data;
                  inst->remove();
                  progress = true;
                  _mesa_hash_table_remove(ht, entry);
               }

               insert_dead_code_hash(ht, inst->dst.reg, inst->dst.reg_offset,
                                     inst);
            }
         }
a2952 5
   _mesa_hash_table_destroy(ht, NULL);

   if (progress)
      live_intervals_valid = false;

a2954 1

d2956 4
a2959 3
 * Implements a second type of register coalescing: This one checks if
 * the two regs involved in a raw move don't interfere, in which case
 * they can both by stored in the same place and the MOV removed.
d2962 1
a2962 1
fs_visitor::register_coalesce_2()
d2965 1
d2967 2
a2968 1
   calculate_live_intervals();
d2970 3
a2972 15
   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      if (inst->opcode != BRW_OPCODE_MOV ||
	  inst->is_partial_write() ||
	  inst->saturate ||
	  inst->src[0].file != GRF ||
	  inst->src[0].negate ||
	  inst->src[0].abs ||
	  inst->src[0].smear != -1 ||
	  inst->dst.file != GRF ||
	  inst->dst.type != inst->src[0].type ||
	  virtual_grf_sizes[inst->src[0].reg] != 1 ||
	  virtual_grf_interferes(inst->dst.reg, inst->src[0].reg)) {
	 continue;
d2975 1
a2975 47
      int reg_from = inst->src[0].reg;
      assert(inst->src[0].reg_offset == 0);
      int reg_to = inst->dst.reg;
      int reg_to_offset = inst->dst.reg_offset;

      foreach_list(node, &this->instructions) {
	 fs_inst *scan_inst = (fs_inst *)node;

	 if (scan_inst->dst.file == GRF &&
	     scan_inst->dst.reg == reg_from) {
	    scan_inst->dst.reg = reg_to;
	    scan_inst->dst.reg_offset = reg_to_offset;
	 }
	 for (int i = 0; i < 3; i++) {
	    if (scan_inst->src[i].file == GRF &&
		scan_inst->src[i].reg == reg_from) {
	       scan_inst->src[i].reg = reg_to;
	       scan_inst->src[i].reg_offset = reg_to_offset;
	    }
	 }
      }

      inst->remove();

      /* We don't need to recalculate live intervals inside the loop despite
       * flagging live_intervals_valid because we only use live intervals for
       * the interferes test, and we must have had a situation where the
       * intervals were:
       *
       *  from  to
       *  ^
       *  |
       *  v
       *        ^
       *        |
       *        v
       *
       * Some register R that might get coalesced with one of these two could
       * only be referencing "to", otherwise "from"'s range would have been
       * longer.  R's range could also only start at the end of "to" or later,
       * otherwise it will conflict with "to" when we try to coalesce "to"
       * into Rw anyway.
       */
      live_intervals_valid = false;

      progress = true;
      continue;
a2984 2
   int if_depth = 0;
   int loop_depth = 0;
d2986 2
a2987 27
   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      /* Make sure that we dominate the instructions we're going to
       * scan for interfering with our coalescing, or we won't have
       * scanned enough to see if anything interferes with our
       * coalescing.  We don't dominate the following instructions if
       * we're in a loop or an if block.
       */
      switch (inst->opcode) {
      case BRW_OPCODE_DO:
	 loop_depth++;
	 break;
      case BRW_OPCODE_WHILE:
	 loop_depth--;
	 break;
      case BRW_OPCODE_IF:
	 if_depth++;
	 break;
      case BRW_OPCODE_ENDIF:
	 if_depth--;
	 break;
      default:
	 break;
      }
      if (loop_depth || if_depth)
	 continue;
d2990 1
a2990 1
	  inst->is_partial_write() ||
d2992 1
a2992 2
	  inst->dst.file != GRF || (inst->src[0].file != GRF &&
				    inst->src[0].file != UNIFORM)||
d2996 1
a2996 4
      bool has_source_modifiers = (inst->src[0].abs ||
                                   inst->src[0].negate ||
                                   inst->src[0].smear != -1 ||
                                   inst->src[0].file == UNIFORM);
d3003 12
a3015 3
      for (fs_inst *scan_inst = (fs_inst *)inst->next;
	   !scan_inst->is_tail_sentinel();
	   scan_inst = (fs_inst *)scan_inst->next) {
d3017 9
a3025 2
	    if (scan_inst->overwrites_reg(inst->dst) ||
                scan_inst->overwrites_reg(inst->src[0])) {
d3031 2
a3032 17
         if (has_source_modifiers) {
            for (int i = 0; i < 3; i++) {
               if (scan_inst->src[i].file == GRF &&
                   scan_inst->src[i].reg == inst->dst.reg &&
                   scan_inst->src[i].reg_offset == inst->dst.reg_offset &&
                   inst->dst.type != scan_inst->src[i].type)
               {
                 interfered = true;
                 break;
               }
            }
         }


	 /* The gen6 MATH instruction can't handle source modifiers or
	  * unusual register regions, so avoid coalescing those for
	  * now.  We should do something more specific.
d3034 1
a3034 14
	 if (has_source_modifiers && !can_do_source_mods(scan_inst)) {
            interfered = true;
	    break;
	 }

	 /* The accumulator result appears to get used for the
	  * conditional modifier generation.  When negating a UD
	  * value, there is a 33rd bit generated for the sign in the
	  * accumulator value, so now you can't check, for example,
	  * equality with a 32-bit value.  See piglit fs-op-neg-uint.
	  */
	 if (scan_inst->conditional_mod &&
	     inst->src[0].negate &&
	     inst->src[0].type == BRW_REGISTER_TYPE_UD) {
d3043 4
d3050 4
a3053 3
      for (fs_inst *scan_inst = inst;
	   !scan_inst->is_tail_sentinel();
	   scan_inst = (fs_inst *)scan_inst->next) {
d3058 5
a3062 7
	       fs_reg new_src = inst->src[0];
               if (scan_inst->src[i].abs) {
                  new_src.negate = 0;
                  new_src.abs = 1;
               }
	       new_src.negate ^= scan_inst->src[i].negate;
	       scan_inst->src[i] = new_src;
a3070 3
   if (progress)
      live_intervals_valid = false;

d3081 2
a3082 4
   calculate_live_intervals();

   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;
d3088 1
a3088 1
	  inst->is_partial_write() ||
a3093 14
      /* Work out which hardware MRF registers are written by this
       * instruction.
       */
      int mrf_low = inst->dst.reg & ~BRW_MRF_COMPR4;
      int mrf_high;
      if (inst->dst.reg & BRW_MRF_COMPR4) {
	 mrf_high = mrf_low + 4;
      } else if (dispatch_width == 16 &&
		 (!inst->force_uncompressed && !inst->force_sechalf)) {
	 mrf_high = mrf_low + 1;
      } else {
	 mrf_high = mrf_low;
      }

d3097 1
a3097 1
      if (this->virtual_grf_end[inst->src[0].reg] > ip)
d3113 9
a3121 4
	    /* If this one instruction didn't populate all the
	     * channels, bail.  We might be able to rewrite everything
	     * that writes that reg, but it would require smarter
	     * tracking to delay the rewriting until complete success.
d3123 1
a3123 1
	    if (scan_inst->is_partial_write())
a3125 6
            /* Things returning more than one register would need us to
             * understand coalescing out more than one MOV at a time.
             */
            if (scan_inst->regs_written > 1)
               break;

d3130 1
a3130 1
	    if (brw->gen == 6) {
d3142 1
a3142 1
	       scan_inst->dst.reg = inst->dst.reg;
d3150 1
a3150 1
	 /* We don't handle control flow here.  Most computation of
d3154 4
a3157 1
	 if (scan_inst->is_control_flow() && scan_inst->opcode != BRW_OPCODE_IF)
d3159 1
d3175 3
a3177 2
	 if (scan_inst->dst.file == MRF) {
	    /* If somebody else writes our MRF here, we can't
d3180 1
a3180 19
	    int scan_mrf_low = scan_inst->dst.reg & ~BRW_MRF_COMPR4;
	    int scan_mrf_high;

	    if (scan_inst->dst.reg & BRW_MRF_COMPR4) {
	       scan_mrf_high = scan_mrf_low + 4;
	    } else if (dispatch_width == 16 &&
		       (!scan_inst->force_uncompressed &&
			!scan_inst->force_sechalf)) {
	       scan_mrf_high = scan_mrf_low + 1;
	    } else {
	       scan_mrf_high = scan_mrf_low;
	    }

	    if (mrf_low == scan_mrf_low ||
		mrf_low == scan_mrf_high ||
		mrf_high == scan_mrf_low ||
		mrf_high == scan_mrf_high) {
	       break;
	    }
d3189 2
a3190 6
	    if (mrf_low >= scan_inst->base_mrf &&
		mrf_low < scan_inst->base_mrf + scan_inst->mlen) {
	       break;
	    }
	    if (mrf_high >= scan_inst->base_mrf &&
		mrf_high < scan_inst->base_mrf + scan_inst->mlen) {
a3196 3
   if (progress)
      live_intervals_valid = false;

d3201 1
a3201 1
 * Walks through basic blocks, looking for repeated MRF writes and
a3209 4
   /* Need to update the MRF tracking for compressed instructions. */
   if (dispatch_width == 16)
      return false;

d3212 2
a3213 2
   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;
d3215 6
a3220 1
      if (inst->is_control_flow()) {
d3222 3
d3229 1
a3229 1
	 fs_inst *prev_inst = last_mrf_move[inst->dst.reg];
d3239 1
a3239 1
	 last_mrf_move[inst->dst.reg] = NULL;
d3243 1
a3243 1
	 /* Found a SEND instruction, which will include two or fewer
d3264 2
a3265 2
	  !inst->is_partial_write()) {
	 last_mrf_move[inst->dst.reg] = inst;
a3268 3
   if (progress)
      live_intervals_valid = false;

d3272 2
a3273 47
static void
clear_deps_for_inst_src(fs_inst *inst, int dispatch_width, bool *deps,
                        int first_grf, int grf_len)
{
   bool inst_16wide = (dispatch_width > 8 &&
                       !inst->force_uncompressed &&
                       !inst->force_sechalf);

   /* Clear the flag for registers that actually got read (as expected). */
   for (int i = 0; i < 3; i++) {
      int grf;
      if (inst->src[i].file == GRF) {
         grf = inst->src[i].reg;
      } else if (inst->src[i].file == HW_REG &&
                 inst->src[i].fixed_hw_reg.file == BRW_GENERAL_REGISTER_FILE) {
         grf = inst->src[i].fixed_hw_reg.nr;
      } else {
         continue;
      }

      if (grf >= first_grf &&
          grf < first_grf + grf_len) {
         deps[grf - first_grf] = false;
         if (inst_16wide)
            deps[grf - first_grf + 1] = false;
      }
   }
}

/**
 * Implements this workaround for the original 965:
 *
 *     "[DevBW, DevCL] Implementation Restrictions: As the hardware does not
 *      check for post destination dependencies on this instruction, software
 *      must ensure that there is no destination hazard for the case of âwrite
 *      followed by a posted writeâ shown in the following example.
 *
 *      1. mov r3 0
 *      2. send r3.xy <rest of send instruction>
 *      3. mov r2 r3
 *
 *      Due to no post-destination dependency check on the âsendâ, the above
 *      code sequence could have two instructions (1 and 2) in flight at the
 *      same time that both consider âr3â as the target of their final writes.
 */
void
fs_visitor::insert_gen4_pre_send_dependency_workarounds(fs_inst *inst)
d3275 2
a3276 70
   int reg_size = dispatch_width / 8;
   int write_len = inst->regs_written * reg_size;
   int first_write_grf = inst->dst.reg;
   bool needs_dep[BRW_MAX_MRF];
   assert(write_len < (int)sizeof(needs_dep) - 1);

   memset(needs_dep, false, sizeof(needs_dep));
   memset(needs_dep, true, write_len);

   clear_deps_for_inst_src(inst, dispatch_width,
                           needs_dep, first_write_grf, write_len);

   /* Walk backwards looking for writes to registers we're writing which
    * aren't read since being written.  If we hit the start of the program,
    * we assume that there are no outstanding dependencies on entry to the
    * program.
    */
   for (fs_inst *scan_inst = (fs_inst *)inst->prev;
        scan_inst != NULL;
        scan_inst = (fs_inst *)scan_inst->prev) {

      /* If we hit control flow, assume that there *are* outstanding
       * dependencies, and force their cleanup before our instruction.
       */
      if (scan_inst->is_control_flow()) {
         for (int i = 0; i < write_len; i++) {
            if (needs_dep[i]) {
               inst->insert_before(DEP_RESOLVE_MOV(first_write_grf + i));
            }
         }
         return;
      }

      bool scan_inst_16wide = (dispatch_width > 8 &&
                               !scan_inst->force_uncompressed &&
                               !scan_inst->force_sechalf);

      /* We insert our reads as late as possible on the assumption that any
       * instruction but a MOV that might have left us an outstanding
       * dependency has more latency than a MOV.
       */
      if (scan_inst->dst.file == GRF) {
         for (int i = 0; i < scan_inst->regs_written; i++) {
            int reg = scan_inst->dst.reg + i * reg_size;

            if (reg >= first_write_grf &&
                reg < first_write_grf + write_len &&
                needs_dep[reg - first_write_grf]) {
               inst->insert_before(DEP_RESOLVE_MOV(reg));
               needs_dep[reg - first_write_grf] = false;
               if (scan_inst_16wide)
                  needs_dep[reg - first_write_grf + 1] = false;
            }
         }
      }

      /* Clear the flag for registers that actually got read (as expected). */
      clear_deps_for_inst_src(scan_inst, dispatch_width,
                              needs_dep, first_write_grf, write_len);

      /* Continue the loop only if we haven't resolved all the dependencies */
      int i;
      for (i = 0; i < write_len; i++) {
         if (needs_dep[i])
            break;
      }
      if (i == write_len)
         return;
   }
}
d3278 4
a3281 55
/**
 * Implements this workaround for the original 965:
 *
 *     "[DevBW, DevCL] Errata: A destination register from a send can not be
 *      used as a destination register until after it has been sourced by an
 *      instruction with a different destination register.
 */
void
fs_visitor::insert_gen4_post_send_dependency_workarounds(fs_inst *inst)
{
   int write_len = inst->regs_written * dispatch_width / 8;
   int first_write_grf = inst->dst.reg;
   bool needs_dep[BRW_MAX_MRF];
   assert(write_len < (int)sizeof(needs_dep) - 1);

   memset(needs_dep, false, sizeof(needs_dep));
   memset(needs_dep, true, write_len);
   /* Walk forwards looking for writes to registers we're writing which aren't
    * read before being written.
    */
   for (fs_inst *scan_inst = (fs_inst *)inst->next;
        !scan_inst->is_tail_sentinel();
        scan_inst = (fs_inst *)scan_inst->next) {
      /* If we hit control flow, force resolve all remaining dependencies. */
      if (scan_inst->is_control_flow()) {
         for (int i = 0; i < write_len; i++) {
            if (needs_dep[i])
               scan_inst->insert_before(DEP_RESOLVE_MOV(first_write_grf + i));
         }
         return;
      }

      /* Clear the flag for registers that actually got read (as expected). */
      clear_deps_for_inst_src(scan_inst, dispatch_width,
                              needs_dep, first_write_grf, write_len);

      /* We insert our reads as late as possible since they're reading the
       * result of a SEND, which has massive latency.
       */
      if (scan_inst->dst.file == GRF &&
          scan_inst->dst.reg >= first_write_grf &&
          scan_inst->dst.reg < first_write_grf + write_len &&
          needs_dep[scan_inst->dst.reg - first_write_grf]) {
         scan_inst->insert_before(DEP_RESOLVE_MOV(scan_inst->dst.reg));
         needs_dep[scan_inst->dst.reg - first_write_grf] = false;
      }

      /* Continue the loop only if we haven't resolved all the dependencies */
      int i;
      for (i = 0; i < write_len; i++) {
         if (needs_dep[i])
            break;
      }
      if (i == write_len)
         return;
d3283 3
a3285 9

   /* If we hit the end of the program, resolve all remaining dependencies out
    * of paranoia.
    */
   fs_inst *last_inst = (fs_inst *)this->instructions.get_tail();
   assert(last_inst->eot);
   for (int i = 0; i < write_len; i++) {
      if (needs_dep[i])
         last_inst->insert_before(DEP_RESOLVE_MOV(first_write_grf + i));
a3286 14
}

void
fs_visitor::insert_gen4_send_dependency_workarounds()
{
   if (brw->gen != 4 || brw->is_g4x)
      return;

   /* Note that we're done with register allocation, so GRF fs_regs always
    * have a .reg_offset of 0.
    */

   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;
d3288 1
a3288 5
      if (inst->mlen != 0 && inst->dst.file == GRF) {
         insert_gen4_pre_send_dependency_workarounds(inst);
         insert_gen4_post_send_dependency_workarounds(inst);
      }
   }
d3291 1
a3291 18
/**
 * Turns the generic expression-style uniform pull constant load instruction
 * into a hardware-specific series of instructions for loading a pull
 * constant.
 *
 * The expression style allows the CSE pass before this to optimize out
 * repeated loads from the same offset, and gives the pre-register-allocation
 * scheduling full flexibility, while the conversion to native instructions
 * allows the post-register-allocation scheduler the best information
 * possible.
 *
 * Note that execution masking for setting up pull constant loads is special:
 * the channels that need to be written are unrelated to the current execution
 * mask, since a later instruction will use one of the result channels as a
 * source operand for all 8 or 16 of its channels.
 */
void
fs_visitor::lower_uniform_pull_constant_loads()
d3293 1
a3293 2
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;
d3295 7
a3301 34
      if (inst->opcode != FS_OPCODE_UNIFORM_PULL_CONSTANT_LOAD)
         continue;

      if (brw->gen >= 7) {
         /* The offset arg before was a vec4-aligned byte offset.  We need to
          * turn it into a dword offset.
          */
         fs_reg const_offset_reg = inst->src[1];
         assert(const_offset_reg.file == IMM &&
                const_offset_reg.type == BRW_REGISTER_TYPE_UD);
         const_offset_reg.imm.u /= 4;
         fs_reg payload = fs_reg(this, glsl_type::uint_type);

         /* This is actually going to be a MOV, but since only the first dword
          * is accessed, we have a special opcode to do just that one.  Note
          * that this needs to be an operation that will be considered a def
          * by live variable analysis, or register allocation will explode.
          */
         fs_inst *setup = new(mem_ctx) fs_inst(FS_OPCODE_SET_SIMD4X2_OFFSET,
                                               payload, const_offset_reg);
         setup->force_writemask_all = true;

         setup->ir = inst->ir;
         setup->annotation = inst->annotation;
         inst->insert_before(setup);

         /* Similarly, this will only populate the first 4 channels of the
          * result register (since we only use smear values from 0-3), but we
          * don't tell the optimizer.
          */
         inst->opcode = FS_OPCODE_UNIFORM_PULL_CONSTANT_LOAD_GEN7;
         inst->src[1] = payload;

         this->live_intervals_valid = false;
d3303 2
a3304 7
         /* Before register allocation, we didn't tell the scheduler about the
          * MRF we use.  We know it's safe to use this MRF because nothing
          * else does except for register spill/unspill, which generates and
          * uses its MRF within a single IR instruction.
          */
         inst->base_mrf = 14;
         inst->mlen = 1;
d3306 17
a3322 24
   }
}

void
fs_visitor::dump_instruction(backend_instruction *be_inst)
{
   fs_inst *inst = (fs_inst *)be_inst;

   if (inst->predicate) {
      printf("(%cf0.%d) ",
             inst->predicate_inverse ? '-' : '+',
             inst->flag_subreg);
   }

   printf("%s", brw_instruction_name(inst->opcode));
   if (inst->saturate)
      printf(".sat");
   if (inst->conditional_mod) {
      printf(".cmod");
      if (!inst->predicate &&
          (brw->gen < 5 || (inst->opcode != BRW_OPCODE_SEL &&
                              inst->opcode != BRW_OPCODE_IF &&
                              inst->opcode != BRW_OPCODE_WHILE))) {
         printf(".f0.%d\n", inst->flag_subreg);
a3323 9
   }
   printf(" ");


   switch (inst->dst.file) {
   case GRF:
      printf("vgrf%d", inst->dst.reg);
      if (inst->dst.reg_offset)
         printf("+%d", inst->dst.reg_offset);
d3325 2
a3326 2
   case MRF:
      printf("m%d", inst->dst.reg);
d3329 2
a3330 1
      printf("(null)");
d3333 2
a3334 1
      printf("***u%d***", inst->dst.reg);
d3337 2
a3338 1
      printf("???");
d3341 6
a3346 88
   printf(", ");

   for (int i = 0; i < 3; i++) {
      if (inst->src[i].negate)
         printf("-");
      if (inst->src[i].abs)
         printf("|");
      switch (inst->src[i].file) {
      case GRF:
         printf("vgrf%d", inst->src[i].reg);
         if (inst->src[i].reg_offset)
            printf("+%d", inst->src[i].reg_offset);
         break;
      case MRF:
         printf("***m%d***", inst->src[i].reg);
         break;
      case UNIFORM:
         printf("u%d", inst->src[i].reg);
         if (inst->src[i].reg_offset)
            printf(".%d", inst->src[i].reg_offset);
         break;
      case BAD_FILE:
         printf("(null)");
         break;
      case IMM:
         switch (inst->src[i].type) {
         case BRW_REGISTER_TYPE_F:
            printf("%ff", inst->src[i].imm.f);
            break;
         case BRW_REGISTER_TYPE_D:
            printf("%dd", inst->src[i].imm.i);
            break;
         case BRW_REGISTER_TYPE_UD:
            printf("%uu", inst->src[i].imm.u);
            break;
         default:
            printf("???");
            break;
         }
         break;
      default:
         printf("???");
         break;
      }
      if (inst->src[i].abs)
         printf("|");

      if (i < 3)
         printf(", ");
   }

   printf(" ");

   if (inst->force_uncompressed)
      printf("1sthalf ");

   if (inst->force_sechalf)
      printf("2ndhalf ");

   printf("\n");
}

/**
 * Possibly returns an instruction that set up @@param reg.
 *
 * Sometimes we want to take the result of some expression/variable
 * dereference tree and rewrite the instruction generating the result
 * of the tree.  When processing the tree, we know that the
 * instructions generated are all writing temporaries that are dead
 * outside of this tree.  So, if we have some instructions that write
 * a temporary, we're free to point that temp write somewhere else.
 *
 * Note that this doesn't guarantee that the instruction generated
 * only reg -- it might be the size=4 destination of a texture instruction.
 */
fs_inst *
fs_visitor::get_instruction_generating_reg(fs_inst *start,
					   fs_inst *end,
					   fs_reg reg)
{
   if (end == start ||
       end->is_partial_write() ||
       reg.reladdr ||
       !reg.equals(end->dst)) {
      return NULL;
   } else {
      return end;
   }
d3350 15
a3364 5
fs_visitor::setup_payload_gen6()
{
   bool uses_depth =
      (fp->Base.InputsRead & (1 << VARYING_SLOT_POS)) != 0;
   unsigned barycentric_interp_modes = c->prog_data.barycentric_interp_modes;
a3365 1
   assert(brw->gen >= 6);
d3367 4
a3370 3
   /* R0-1: masks, pixel X/Y coordinates. */
   c->nr_payload_regs = 2;
   /* R2: only for 32-pixel dispatch.*/
d3372 18
a3389 14
   /* R3-26: barycentric interpolation coordinates.  These appear in the
    * same order that they appear in the brw_wm_barycentric_interp_mode
    * enum.  Each set of coordinates occupies 2 registers if dispatch width
    * == 8 and 4 registers if dispatch width == 16.  Coordinates only
    * appear if they were enabled using the "Barycentric Interpolation
    * Mode" bits in WM_STATE.
    */
   for (int i = 0; i < BRW_WM_BARYCENTRIC_INTERP_MODE_COUNT; ++i) {
      if (barycentric_interp_modes & (1 << i)) {
         c->barycentric_coord_reg[i] = c->nr_payload_regs;
         c->nr_payload_regs += 2;
         if (dispatch_width == 16) {
            c->nr_payload_regs += 2;
         }
a3390 1
   }
d3392 4
a3395 21
   /* R27: interpolated depth if uses source depth */
   if (uses_depth) {
      c->source_depth_reg = c->nr_payload_regs;
      c->nr_payload_regs++;
      if (dispatch_width == 16) {
         /* R28: interpolated depth if not 8-wide. */
         c->nr_payload_regs++;
      }
   }
   /* R29: interpolated W set if GEN6_WM_USES_SOURCE_W. */
   if (uses_depth) {
      c->source_w_reg = c->nr_payload_regs;
      c->nr_payload_regs++;
      if (dispatch_width == 16) {
         /* R30: interpolated W if not 8-wide. */
         c->nr_payload_regs++;
      }
   }
   /* R31: MSAA position offsets. */
   /* R32-: bary for 32-pixel. */
   /* R58-59: interp W for 32-pixel. */
d3397 3
a3399 4
   if (fp->Base.OutputsWritten & BITFIELD64_BIT(FRAG_RESULT_DEPTH)) {
      c->source_depth_to_render_target = true;
   }
}
d3401 10
a3410 5
bool
fs_visitor::run()
{
   sanity_param_count = fp->Base.Parameters->NumParameters;
   uint32_t orig_nr_params = c->prog_data.nr_params;
d3412 12
a3423 4
   if (brw->gen >= 6)
      setup_payload_gen6();
   else
      setup_payload_gen4();
d3425 21
a3445 5
   if (0) {
      emit_dummy_fs();
   } else {
      if (INTEL_DEBUG & DEBUG_SHADER_TIME)
         emit_shader_time_begin();
d3447 6
a3452 5
      calculate_urb_setup();
      if (brw->gen < 6)
	 emit_interpolation_setup_gen4();
      else
	 emit_interpolation_setup_gen6();
d3454 15
a3468 7
      /* We handle discards by keeping track of the still-live pixels in f0.1.
       * Initialize it with the dispatched pixels.
       */
      if (fp->UsesKill) {
         fs_inst *discard_init = emit(FS_OPCODE_MOV_DISPATCH_TO_FLAGS);
         discard_init->flag_subreg = 1;
      }
d3470 9
a3478 16
      /* Generate FS IR for main().  (the visitor only descends into
       * functions called "main").
       */
      if (shader) {
         foreach_list(node, &*shader->ir) {
            ir_instruction *ir = (ir_instruction *)node;
            base_ir = ir;
            this->result = reg_undef;
            ir->accept(this);
         }
      } else {
         emit_fragment_program_code();
      }
      base_ir = NULL;
      if (failed)
	 return false;
d3480 11
a3490 1
      emit(FS_OPCODE_PLACEHOLDER_HALT);
d3492 12
a3503 1
      emit_fb_writes();
d3505 26
a3530 1
      split_virtual_grfs();
d3532 33
a3564 2
      move_uniform_array_access_to_pull_constants();
      setup_pull_constants();
d3566 3
a3568 3
      bool progress;
      do {
	 progress = false;
d3570 3
a3572 1
         compact_virtual_grfs();
d3574 3
a3576 1
	 progress = remove_duplicate_mrf_writes() || progress;
d3578 9
a3586 23
	 progress = opt_algebraic() || progress;
	 progress = opt_cse() || progress;
	 progress = opt_copy_propagate() || progress;
	 progress = dead_code_eliminate() || progress;
	 progress = dead_code_eliminate_local() || progress;
	 progress = register_coalesce() || progress;
	 progress = register_coalesce_2() || progress;
	 progress = compute_to_mrf() || progress;
      } while (progress);

      remove_dead_constants();

      schedule_instructions(false);

      lower_uniform_pull_constant_loads();

      assign_curb_setup();
      assign_urb_setup();

      if (0) {
	 /* Debug of register spilling: Go spill everything. */
	 for (int i = 0; i < virtual_grf_count; i++) {
	    spill_reg(i);
d3588 1
d3591 10
a3600 6
      if (0)
	 assign_regs_trivial();
      else {
	 while (!assign_regs()) {
	    if (failed)
	       break;
d3603 2
a3605 2
   assert(force_uncompressed_stack == 0);
   assert(force_sechalf_stack == 0);
d3607 3
a3609 5
   /* This must come after all optimization and register allocation, since
    * it inserts dead code that happens to have side effects, and it does
    * so based on the actual physical registers in use.
    */
   insert_gen4_send_dependency_workarounds();
d3611 1
a3611 2
   if (failed)
      return false;
d3613 16
a3628 10
   schedule_instructions(true);

   if (dispatch_width == 8) {
      c->prog_data.reg_blocks = brw_register_blocks(grf_used);
   } else {
      c->prog_data.reg_blocks_16 = brw_register_blocks(grf_used);

      /* Make sure we didn't try to sneak in an extra uniform */
      assert(orig_nr_params == c->prog_data.nr_params);
      (void) orig_nr_params;
a3629 9

   /* If any state parameters were appended, then ParameterValues could have
    * been realloced, in which case the driver uniform storage set up by
    * _mesa_associate_uniform_storage() would point to freed memory.  Make
    * sure that didn't happen.
    */
   assert(sanity_param_count == fp->Base.Parameters->NumParameters);

   return !failed;
d3632 2
a3633 5
const unsigned *
brw_wm_fs_emit(struct brw_context *brw, struct brw_wm_compile *c,
               struct gl_fragment_program *fp,
               struct gl_shader_program *prog,
               unsigned *final_assembly_size)
d3635 6
a3640 2
   bool start_busy = false;
   float start_time = 0;
d3642 4
a3645 5
   if (unlikely(brw->perf_debug)) {
      start_busy = (brw->batch.last_bo &&
                    drm_intel_bo_busy(brw->batch.last_bo));
      start_time = get_time();
   }
d3647 7
a3653 3
   struct brw_shader *shader = NULL;
   if (prog)
      shader = (brw_shader *) prog->_LinkedShaders[MESA_SHADER_FRAGMENT];
d3656 3
a3658 9
      if (prog) {
         printf("GLSL IR for native fragment shader %d:\n", prog->Name);
         _mesa_print_ir(shader->ir, NULL);
         printf("\n\n");
      } else {
         printf("ARB_fragment_program %d ir for native fragment shader\n",
                fp->Base.Id);
         _mesa_print_program(&fp->Base);
      }
d3663 1
a3663 6
   fs_visitor v(brw, c, prog, fp, 8);
   if (!v.run()) {
      if (prog) {
         prog->LinkStatus = false;
         ralloc_strcat(&prog->InfoLog, v.fail_msg);
      }
d3665 8
a3672 2
      _mesa_problem(NULL, "Failed to compile fragment shader: %s\n",
                    v.fail_msg);
d3674 7
a3680 13
      return NULL;
   }

   exec_list *simd16_instructions = NULL;
   fs_visitor v2(brw, c, prog, fp, 16);
   bool no16 = INTEL_DEBUG & DEBUG_NO16;
   if (brw->gen >= 5 && c->prog_data.nr_pull_params == 0 && likely(!no16)) {
      v2.import_uniforms(&v);
      if (!v2.run()) {
         perf_debug("16-wide shader failed to compile, falling back to "
                    "8-wide at a 10-20%% performance cost: %s", v2.fail_msg);
      } else {
         simd16_instructions = &v2.instructions;
a3681 1
   }
d3683 1
a3683 1
   c->prog_data.dispatch_width = 8;
d3685 1
a3685 4
   fs_generator g(brw, c, prog, fp, v.dual_src_output.file != BAD_FILE);
   const unsigned *generated = g.generate_assembly(&v.instructions,
                                                   simd16_instructions,
                                                   final_assembly_size);
d3687 4
a3690 4
   if (unlikely(brw->perf_debug) && shader) {
      if (shader->compiled_once)
         brw_wm_debug_recompile(brw, prog, &c->key);
      shader->compiled_once = true;
d3692 3
a3694 5
      if (start_busy && !drm_intel_bo_busy(brw->batch.last_bo)) {
         perf_debug("FS compile took %.03f ms and stalled the GPU\n",
                    (get_time() - start_time) * 1000);
      }
   }
d3696 1
a3696 2
   return generated;
}
d3698 6
a3703 5
bool
brw_fs_precompile(struct gl_context *ctx, struct gl_shader_program *prog)
{
   struct brw_context *brw = brw_context(ctx);
   struct brw_wm_prog_key key;
d3705 7
a3711 32
   if (!prog->_LinkedShaders[MESA_SHADER_FRAGMENT])
      return true;

   struct gl_fragment_program *fp = (struct gl_fragment_program *)
      prog->_LinkedShaders[MESA_SHADER_FRAGMENT]->Program;
   struct brw_fragment_program *bfp = brw_fragment_program(fp);
   bool program_uses_dfdy = fp->UsesDFdy;

   memset(&key, 0, sizeof(key));

   if (brw->gen < 6) {
      if (fp->UsesKill)
         key.iz_lookup |= IZ_PS_KILL_ALPHATEST_BIT;

      if (fp->Base.OutputsWritten & BITFIELD64_BIT(FRAG_RESULT_DEPTH))
         key.iz_lookup |= IZ_PS_COMPUTES_DEPTH_BIT;

      /* Just assume depth testing. */
      key.iz_lookup |= IZ_DEPTH_TEST_ENABLE_BIT;
      key.iz_lookup |= IZ_DEPTH_WRITE_ENABLE_BIT;
   }

   if (brw->gen < 6)
      key.input_slots_valid |= BITFIELD64_BIT(VARYING_SLOT_POS);

   for (int i = 0; i < VARYING_SLOT_MAX; i++) {
      if (!(fp->Base.InputsRead & BITFIELD64_BIT(i)))
	 continue;

      if (brw->gen < 6) {
         if (_mesa_varying_slot_in_fs((gl_varying_slot) i))
            key.input_slots_valid |= BITFIELD64_BIT(i);
a3712 1
   }
d3714 6
a3719 1
   key.clamp_fragment_color = ctx->API == API_OPENGL_COMPAT;
d3721 2
a3722 8
   for (int i = 0; i < MAX_SAMPLERS; i++) {
      if (fp->Base.ShadowSamplers & (1 << i)) {
         /* Assume DEPTH_TEXTURE_MODE is the default: X, X, X, 1 */
         key.tex.swizzles[i] =
            MAKE_SWIZZLE4(SWIZZLE_X, SWIZZLE_X, SWIZZLE_X, SWIZZLE_ONE);
      } else {
         /* Color sampler: assume no swizzling. */
         key.tex.swizzles[i] = SWIZZLE_XYZW;
d3726 2
a3727 11
   if (fp->Base.InputsRead & VARYING_BIT_POS) {
      key.drawable_height = ctx->DrawBuffer->Height;
   }

   if ((fp->Base.InputsRead & VARYING_BIT_POS) || program_uses_dfdy) {
      key.render_to_fbo = _mesa_is_user_fbo(ctx->DrawBuffer);
   }

   key.nr_color_regions = 1;

   key.program_string_id = bfp->id;
d3729 1
a3729 2
   uint32_t old_prog_offset = brw->wm.prog_offset;
   struct brw_wm_prog_data *old_prog_data = brw->wm.prog_data;
d3731 2
a3732 1
   bool success = do_wm_prog(brw, prog, bfp, &key);
d3734 1
a3734 2
   brw->wm.prog_offset = old_prog_offset;
   brw->wm.prog_data = old_prog_data;
d3736 1
a3736 1
   return success;
@


1.1.1.3
log
@Import Mesa 9.2.1
@
text
@d1361 1
a1361 5
         for (int i = 0; i < 3; i++) {
            if (inst->src[i].file == GRF) {
               split_grf[inst->src[i].reg] = false;
            }
         }
@


1.1.1.4
log
@Import Mesa 9.2.5
@
text
@d2923 1
a2923 1
      if (fp->UsesKill || c->key.alpha_test_func) {
a2945 3

      if (c->key.alpha_test_func)
         emit_alpha_test();
@


1.1.1.5
log
@Import Mesa 10.2.3
@
text
@d38 1
a49 3
#include "brw_dead_control_flow.h"
#include "main/uniforms.h"
#include "brw_fs_live_variables.h"
d56 1
a65 2

   this->writes_accumulator = false;
a70 1
   this->opcode = BRW_OPCODE_NOP;
a151 9
#define ALU2_ACC(op)                                                    \
   fs_inst *                                                            \
   fs_visitor::op(fs_reg dst, fs_reg src0, fs_reg src1)                 \
   {                                                                    \
      fs_inst *inst = new(mem_ctx) fs_inst(BRW_OPCODE_##op, dst, src0, src1);\
      inst->writes_accumulator = true;                                  \
      return inst;                                                      \
   }

d167 1
a167 1
ALU2_ACC(MACH)
a181 5
ALU3(MAD)
ALU2_ACC(ADDC)
ALU2_ACC(SUBB)
ALU2(SEL)
ALU2(MAC)
d192 1
a192 1
/** Gen6 IF with embedded comparison. */
d196 1
a196 1
   assert(brw->gen == 6);
d241 2
a242 3
fs_visitor::VARYING_PULL_CONSTANT_LOAD(const fs_reg &dst,
                                       const fs_reg &surf_index,
                                       const fs_reg &varying_offset,
d319 1
a319 1
fs_inst::equals(fs_inst *inst) const
d340 1
a340 1
fs_inst::overwrites_reg(const fs_reg &reg) const
d349 1
a349 1
fs_inst::is_send_from_grf() const
d354 1
a354 2
            src[1].file == GRF) ||
           (is_tex() && src[0].file == GRF));
a365 3
   if (!inst->can_do_source_mods())
      return false;

d373 1
a373 1
   stride = 1;
d410 1
a410 1
/** Fixed brw_reg. */
a424 1
           subreg_offset == r.subreg_offset &&
d431 1
a431 1
           stride == r.stride &&
a434 25
fs_reg &
fs_reg::apply_stride(unsigned stride)
{
   assert((this->stride * stride) <= 4 &&
          (is_power_of_two(stride) || stride == 0) &&
          file != HW_REG && file != IMM);
   this->stride *= stride;
   return *this;
}

fs_reg &
fs_reg::set_smear(unsigned subreg)
{
   assert(file != HW_REG && file != IMM);
   subreg_offset = subreg * type_sz(type);
   stride = 0;
   return *this;
}

bool
fs_reg::is_contiguous() const
{
   return stride == 1;
}

a453 8
fs_reg::is_null() const
{
   return file == HW_REG &&
          fixed_hw_reg.file == BRW_ARCHITECTURE_REGISTER_FILE &&
          fixed_hw_reg.nr == BRW_ARF_NULL;
}

bool
a458 8
bool
fs_reg::is_accumulator() const
{
   return file == HW_REG &&
          fixed_hw_reg.file == BRW_ARCHITECTURE_REGISTER_FILE &&
          fixed_hw_reg.nr == BRW_ARF_ACCUMULATOR;
}

a482 3
   case GLSL_TYPE_ATOMIC_UINT:
      return 0;
   case GLSL_TYPE_IMAGE:
d522 1
a522 1
   dst.set_smear(0);
d557 1
a557 1
   reset.set_smear(2);
d597 2
a598 2
   emit(new(mem_ctx) fs_inst(SHADER_OPCODE_SHADER_TIME_ADD,
                             fs_reg(), payload, offset, value));
d602 1
a602 1
fs_visitor::vfail(const char *format, va_list va)
d604 1
d612 1
d614 1
a623 42
void
fs_visitor::fail(const char *format, ...)
{
   va_list va;

   va_start(va, format);
   vfail(format, va);
   va_end(va);
}

/**
 * Mark this program as impossible to compile in SIMD16 mode.
 *
 * During the SIMD8 compile (which happens first), we can detect and flag
 * things that are unsupported in SIMD16 mode, so the compiler can skip
 * the SIMD16 compile altogether.
 *
 * During a SIMD16 compile (if one happens anyway), this just calls fail().
 */
void
fs_visitor::no16(const char *format, ...)
{
   va_list va;

   va_start(va, format);

   if (dispatch_width == 16) {
      vfail(format, va);
   } else {
      simd16_unsupported = true;

      if (brw->perf_debug) {
         if (no16_msg)
            ralloc_vasprintf_append(&no16_msg, format, va);
         else
            no16_msg = ralloc_vasprintf(mem_ctx, format, va);
      }
   }

   va_end(va);
}

d627 1
a627 1
   return emit(new(mem_ctx) fs_inst(opcode));
d633 1
a633 1
   return emit(new(mem_ctx) fs_inst(opcode, dst));
d639 1
a639 1
   return emit(new(mem_ctx) fs_inst(opcode, dst, src0));
d645 1
a645 1
   return emit(new(mem_ctx) fs_inst(opcode, dst, src0, src1));
d652 1
a652 1
   return emit(new(mem_ctx) fs_inst(opcode, dst, src0, src1, src2));
d668 13
d690 1
a690 1
fs_inst::is_partial_write() const
d692 1
a692 1
   return ((this->predicate && this->opcode != BRW_OPCODE_SEL) ||
d694 1
a694 26
           this->force_sechalf || !this->dst.is_contiguous());
}

int
fs_inst::regs_read(fs_visitor *v, int arg) const
{
   if (is_tex() && arg == 0 && src[0].file == GRF) {
      if (v->dispatch_width == 16)
	 return (mlen + 1) / 2;
      else
	 return mlen;
   }
   return 1;
}

bool
fs_inst::reads_flag() const
{
   return predicate;
}

bool
fs_inst::writes_flag() const
{
   return (conditional_mod && opcode != BRW_OPCODE_SEL) ||
          opcode == FS_OPCODE_MOV_DISPATCH_TO_FLAGS;
a708 3
   if (inst->base_mrf == -1)
      return 0;

d726 1
a726 4
   case SHADER_OPCODE_TXF_CMS:
   case SHADER_OPCODE_TXF_MCS:
   case SHADER_OPCODE_TG4:
   case SHADER_OPCODE_TG4_OFFSET:
d734 1
a734 1
   case SHADER_OPCODE_GEN4_SCRATCH_READ:
d738 1
a738 1
   case SHADER_OPCODE_GEN4_SCRATCH_WRITE:
a739 3
   case SHADER_OPCODE_UNTYPED_ATOMIC:
   case SHADER_OPCODE_UNTYPED_SURFACE_READ:
      return 0;
d810 1
a810 1
/* For SIMD16, we need to follow from the uniform setup of SIMD8 dispatch.
d819 2
a820 4
   this->push_constant_loc = v->push_constant_loc;
   this->pull_constant_loc = v->pull_constant_loc;
   this->uniforms = v->uniforms;
   this->param_size = v->param_size;
d839 1
a839 1
   unsigned params_before = uniforms;
d855 2
a856 1
         stage_prog_data->param[uniforms++] = &storage->storage[i].f;
d861 2
a862 1
   assert(params_before + ir->type->component_slots() == uniforms);
d895 1
a895 1
         stage_prog_data->param[uniforms++] =
d906 1
a906 1
   bool flip = !ir->data.origin_upper_left ^ c->key.render_to_fbo;
d909 1
a909 1
   if (ir->data.pixel_center_integer) {
d917 1
a917 1
   if (!flip && ir->data.pixel_center_integer) {
d921 1
a921 1
      float offset = (ir->data.pixel_center_integer ? 0.0 : 0.5);
d952 1
a952 1
                         bool is_centroid, bool is_sample)
a960 5
      } else if (is_sample) {
          if (interpolation_mode == INTERP_QUALIFIER_SMOOTH)
            barycoord_mode = BRW_WM_PERSPECTIVE_SAMPLE_BARYCENTRIC;
         else
            barycoord_mode = BRW_WM_NONPERSPECTIVE_SAMPLE_BARYCENTRIC;
d1003 1
a1003 1
   int location = ir->data.location;
d1006 1
a1006 1
	 if (c->prog_data.urb_setup[location] == -1) {
d1030 6
d1038 2
a1039 3
                            ir->data.centroid && !c->key.persample_shading,
                            ir->data.sample || c->key.persample_shading);
               if (brw->needs_unlit_centroid_workaround && ir->data.centroid) {
d1047 1
a1047 2
                                               interpolation_mode,
                                               false, false);
d1051 1
a1051 1
               if (brw->gen < 6 && interpolation_mode == INTERP_QUALIFIER_SMOOTH) {
a1088 126
void
fs_visitor::compute_sample_position(fs_reg dst, fs_reg int_sample_pos)
{
   assert(dst.type == BRW_REGISTER_TYPE_F);

   if (c->key.compute_pos_offset) {
      /* Convert int_sample_pos to floating point */
      emit(MOV(dst, int_sample_pos));
      /* Scale to the range [0, 1] */
      emit(MUL(dst, dst, fs_reg(1 / 16.0f)));
   }
   else {
      /* From ARB_sample_shading specification:
       * "When rendering to a non-multisample buffer, or if multisample
       *  rasterization is disabled, gl_SamplePosition will always be
       *  (0.5, 0.5).
       */
      emit(MOV(dst, fs_reg(0.5f)));
   }
}

fs_reg *
fs_visitor::emit_samplepos_setup(ir_variable *ir)
{
   assert(brw->gen >= 6);
   assert(ir->type == glsl_type::vec2_type);

   this->current_annotation = "compute sample position";
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, ir->type);
   fs_reg pos = *reg;
   fs_reg int_sample_x = fs_reg(this, glsl_type::int_type);
   fs_reg int_sample_y = fs_reg(this, glsl_type::int_type);

   /* WM will be run in MSDISPMODE_PERSAMPLE. So, only one of SIMD8 or SIMD16
    * mode will be enabled.
    *
    * From the Ivy Bridge PRM, volume 2 part 1, page 344:
    * R31.1:0         Position Offset X/Y for Slot[3:0]
    * R31.3:2         Position Offset X/Y for Slot[7:4]
    * .....
    *
    * The X, Y sample positions come in as bytes in  thread payload. So, read
    * the positions using vstride=16, width=8, hstride=2.
    */
   struct brw_reg sample_pos_reg =
      stride(retype(brw_vec1_grf(c->sample_pos_reg, 0),
                    BRW_REGISTER_TYPE_B), 16, 8, 2);

   emit(MOV(int_sample_x, fs_reg(sample_pos_reg)));
   if (dispatch_width == 16) {
      fs_inst *inst = emit(MOV(half(int_sample_x, 1),
                               fs_reg(suboffset(sample_pos_reg, 16))));
      inst->force_sechalf = true;
   }
   /* Compute gl_SamplePosition.x */
   compute_sample_position(pos, int_sample_x);
   pos.reg_offset++;
   emit(MOV(int_sample_y, fs_reg(suboffset(sample_pos_reg, 1))));
   if (dispatch_width == 16) {
      fs_inst *inst = emit(MOV(half(int_sample_y, 1),
                               fs_reg(suboffset(sample_pos_reg, 17))));
      inst->force_sechalf = true;
   }
   /* Compute gl_SamplePosition.y */
   compute_sample_position(pos, int_sample_y);
   return reg;
}

fs_reg *
fs_visitor::emit_sampleid_setup(ir_variable *ir)
{
   assert(brw->gen >= 6);

   this->current_annotation = "compute sample id";
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, ir->type);

   if (c->key.compute_sample_id) {
      fs_reg t1 = fs_reg(this, glsl_type::int_type);
      fs_reg t2 = fs_reg(this, glsl_type::int_type);
      t2.type = BRW_REGISTER_TYPE_UW;

      /* The PS will be run in MSDISPMODE_PERSAMPLE. For example with
       * 8x multisampling, subspan 0 will represent sample N (where N
       * is 0, 2, 4 or 6), subspan 1 will represent sample 1, 3, 5 or
       * 7. We can find the value of N by looking at R0.0 bits 7:6
       * ("Starting Sample Pair Index (SSPI)") and multiplying by two
       * (since samples are always delivered in pairs). That is, we
       * compute 2*((R0.0 & 0xc0) >> 6) == (R0.0 & 0xc0) >> 5. Then
       * we need to add N to the sequence (0, 0, 0, 0, 1, 1, 1, 1) in
       * case of SIMD8 and sequence (0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2,
       * 2, 3, 3, 3, 3) in case of SIMD16. We compute this sequence by
       * populating a temporary variable with the sequence (0, 1, 2, 3),
       * and then reading from it using vstride=1, width=4, hstride=0.
       * These computations hold good for 4x multisampling as well.
       */
      emit(BRW_OPCODE_AND, t1,
           fs_reg(retype(brw_vec1_grf(0, 0), BRW_REGISTER_TYPE_D)),
           fs_reg(brw_imm_d(0xc0)));
      emit(BRW_OPCODE_SHR, t1, t1, fs_reg(5));
      /* This works for both SIMD8 and SIMD16 */
      emit(MOV(t2, brw_imm_v(0x3210)));
      /* This special instruction takes care of setting vstride=1,
       * width=4, hstride=0 of t2 during an ADD instruction.
       */
      emit(FS_OPCODE_SET_SAMPLE_ID, *reg, t1, t2);
   } else {
      /* As per GL_ARB_sample_shading specification:
       * "When rendering to a non-multisample buffer, or if multisample
       *  rasterization is disabled, gl_SampleID will always be zero."
       */
      emit(BRW_OPCODE_MOV, *reg, fs_reg(0));
   }

   return reg;
}

fs_reg *
fs_visitor::emit_samplemaskin_setup(ir_variable *ir)
{
   assert(brw->gen >= 7);
   this->current_annotation = "compute gl_SampleMaskIn";
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, ir->type);
   emit(MOV(*reg, fs_reg(retype(brw_vec8_grf(c->sample_mask_reg, 0), BRW_REGISTER_TYPE_D))));
   return reg;
}

d1163 2
a1164 2
      if (brw->gen >= 7)
	 no16("SIMD16 INTDIV unsupported\n");
d1204 1
a1210 2
   c->prog_data.curb_read_length = ALIGN(stage_prog_data->nr_params, 8) / 8;

d1217 1
a1217 13
            int uniform_nr = inst->src[i].reg + inst->src[i].reg_offset;
            int constant_nr;
            if (uniform_nr >= 0 && uniform_nr < (int) uniforms) {
               constant_nr = push_constant_loc[uniform_nr];
            } else {
               /* Section 5.11 of the OpenGL 4.1 spec says:
                * "Out-of-bounds reads return undefined values, which include
                *  values from other variables of the active program or zero."
                * Just return the first push constant.
                */
               constant_nr = 0;
            }

d1223 1
a1223 3
	    inst->src[i].fixed_hw_reg = byte_offset(
               retype(brw_reg, inst->src[i].type),
               inst->src[i].subreg_offset);
d1233 1
a1233 1
      c->prog_data.urb_setup[i] = -1;
d1239 4
a1242 41
      if (_mesa_bitcount_64(fp->Base.InputsRead &
                            BRW_FS_VARYING_INPUT_MASK) <= 16) {
         /* The SF/SBE pipeline stage can do arbitrary rearrangement of the
          * first 16 varying inputs, so we can put them wherever we want.
          * Just put them in order.
          *
          * This is useful because it means that (a) inputs not used by the
          * fragment shader won't take up valuable register space, and (b) we
          * won't have to recompile the fragment shader if it gets paired with
          * a different vertex (or geometry) shader.
          */
         for (unsigned int i = 0; i < VARYING_SLOT_MAX; i++) {
            if (fp->Base.InputsRead & BRW_FS_VARYING_INPUT_MASK &
                BITFIELD64_BIT(i)) {
               c->prog_data.urb_setup[i] = urb_next++;
            }
         }
      } else {
         /* We have enough input varyings that the SF/SBE pipeline stage can't
          * arbitrarily rearrange them to suit our whim; we have to put them
          * in an order that matches the output of the previous pipeline stage
          * (geometry or vertex shader).
          */
         struct brw_vue_map prev_stage_vue_map;
         brw_compute_vue_map(brw, &prev_stage_vue_map,
                             c->key.input_slots_valid);
         int first_slot = 2 * BRW_SF_URB_ENTRY_READ_OFFSET;
         assert(prev_stage_vue_map.num_slots <= first_slot + 32);
         for (int slot = first_slot; slot < prev_stage_vue_map.num_slots;
              slot++) {
            int varying = prev_stage_vue_map.slot_to_varying[slot];
            /* Note that varying == BRW_VARYING_SLOT_COUNT when a slot is
             * unused.
             */
            if (varying != BRW_VARYING_SLOT_COUNT &&
                (fp->Base.InputsRead & BRW_FS_VARYING_INPUT_MASK &
                 BITFIELD64_BIT(varying))) {
               c->prog_data.urb_setup[varying] = slot - first_slot;
            }
         }
         urb_next = prev_stage_vue_map.num_slots - first_slot;
d1259 1
a1259 1
	       c->prog_data.urb_setup[i] = urb_next;
d1271 1
a1271 1
         c->prog_data.urb_setup[VARYING_SLOT_PNTC] = urb_next++;
d1274 2
a1275 1
   c->prog_data.num_varying_inputs = urb_next;
d1300 1
a1300 3
   /* Each attribute is 4 setup channels, each of which is half a reg. */
   this->first_non_payload_grf =
      urb_start + c->prog_data.num_varying_inputs * 2;
d1404 1
a1404 1
   invalidate_live_intervals();
d1438 8
a1445 15
   struct {
      fs_reg *reg;
      unsigned count;
   } special[] = {
      { &frag_depth, 1 },
      { &pixel_x, 1 },
      { &pixel_y, 1 },
      { &pixel_w, 1 },
      { &wpos_w, 1 },
      { &dual_src_output, 1 },
      { outputs, ARRAY_SIZE(outputs) },
      { delta_x, ARRAY_SIZE(delta_x) },
      { delta_y, ARRAY_SIZE(delta_y) },
      { &sample_mask, 1 },
      { &shader_start_time, 1 },
d1447 2
d1452 2
a1453 4
      for (unsigned j = 0; j < special[i].count; j++) {
         if (special[i].reg[j].file == GRF)
            remap_table[special[i].reg[j].reg] = 0;
      }
d1462 4
a1465 1
         invalidate_live_intervals();
d1487 87
a1573 4
      for (unsigned j = 0; j < special[i].count; j++) {
         fs_reg *reg = &special[i].reg[j];
         if (reg->file == GRF && remap_table[reg->reg] != -1)
            reg->reg = remap_table[reg->reg];
d1576 2
d1595 1
a1595 2
   if (dispatch_width != 8)
      return;
d1597 1
a1597 3
   pull_constant_loc = ralloc_array(mem_ctx, int, uniforms);

   for (unsigned int i = 0; i < uniforms; i++) {
d1620 3
a1622 1
            const float **values = &stage_prog_data->param[uniform];
d1627 1
a1627 3
               pull_constant_loc[uniform + j] = stage_prog_data->nr_pull_params;

               stage_prog_data->pull_param[stage_prog_data->nr_pull_params++] =
d1631 18
d1654 2
a1655 1
 * Assign UNIFORM file registers to either push constants or pull constants.
d1664 1
a1664 1
fs_visitor::assign_constant_locations()
d1666 3
a1668 2
   /* Only the first compile (SIMD8 mode) gets to decide on locations. */
   if (dispatch_width != 8)
d1671 3
a1673 17
   /* Find which UNIFORM registers are still in use. */
   bool is_live[uniforms];
   for (unsigned int i = 0; i < uniforms; i++) {
      is_live[i] = false;
   }

   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *) node;

      for (int i = 0; i < 3; i++) {
         if (inst->src[i].file != UNIFORM)
            continue;

         int constant_nr = inst->src[i].reg + inst->src[i].reg_offset;
         if (constant_nr >= 0 && constant_nr < (int) uniforms)
            is_live[constant_nr] = true;
      }
d1676 1
a1676 3
   /* Only allow 16 registers (128 uniform components) as push constants.
    *
    * Just demote the end of the list.  We could probably do better
d1679 1
a1679 2
   unsigned int max_push_components = 16 * 8;
   unsigned int num_push_constants = 0;
d1681 8
a1688 6
   push_constant_loc = ralloc_array(mem_ctx, int, uniforms);

   for (unsigned int i = 0; i < uniforms; i++) {
      if (!is_live[i] || pull_constant_loc[i] != -1) {
         /* This UNIFORM register is either dead, or has already been demoted
          * to a pull const.  Mark it as no longer living in the param[] array.
d1690 11
a1700 16
         push_constant_loc[i] = -1;
         continue;
      }

      if (num_push_constants < max_push_components) {
         /* Retain as a push constant.  Record the location in the params[]
          * array.
          */
         push_constant_loc[i] = num_push_constants++;
      } else {
         /* Demote to a pull constant. */
         push_constant_loc[i] = -1;

         int pull_index = stage_prog_data->nr_pull_params++;
         stage_prog_data->pull_param[pull_index] = stage_prog_data->param[i];
         pull_constant_loc[i] = pull_index;
d1703 1
a1704 24
   stage_prog_data->nr_params = num_push_constants;

   /* Up until now, the param[] array has been indexed by reg + reg_offset
    * of UNIFORM registers.  Condense it to only contain the uniforms we
    * chose to upload as push constants.
    */
   for (unsigned int i = 0; i < uniforms; i++) {
      int remapped = push_constant_loc[i];

      if (remapped == -1)
         continue;

      assert(remapped <= (int)i);
      stage_prog_data->param[remapped] = stage_prog_data->param[i];
   }
}

/**
 * Replace UNIFORM register file access with either UNIFORM_PULL_CONSTANT_LOAD
 * or VARYING_PULL_CONSTANT_LOAD instructions which load values into VGRFs.
 */
void
fs_visitor::demote_pull_constants()
{
d1717 1
a1717 23
         /* Set up the annotation tracking for new generated instructions. */
         base_ir = inst->ir;
         current_annotation = inst->annotation;

         fs_reg surf_index(stage_prog_data->binding_table.pull_constants_start);
         fs_reg dst = fs_reg(this, glsl_type::float_type);

         /* Generate a pull load into dst. */
         if (inst->src[i].reladdr) {
            exec_list list = VARYING_PULL_CONSTANT_LOAD(dst,
                                                        surf_index,
                                                        *inst->src[i].reladdr,
                                                        pull_index);
            inst->insert_before(&list);
            inst->src[i].reladdr = NULL;
         } else {
            fs_reg offset = fs_reg((unsigned)(pull_index * 4) & ~15);
            fs_inst *pull =
               new(mem_ctx) fs_inst(FS_OPCODE_UNIFORM_PULL_CONSTANT_LOAD,
                                    dst, surf_index, offset);
            inst->insert_before(pull);
            inst->src[i].set_smear(pull_index & 3);
         }
d1719 15
a1733 4
         /* Rewrite the instruction to use the temporary VGRF. */
         inst->src[i].file = GRF;
         inst->src[i].reg = dst.reg;
         inst->src[i].reg_offset = 0;
a1735 1
   invalidate_live_intervals();
d1781 27
a1807 4
      case BRW_OPCODE_OR:
         if (inst->src[0].equals(inst->src[1])) {
            inst->opcode = BRW_OPCODE_MOV;
            inst->src[1] = reg_undef;
a1808 1
            break;
d1810 110
a1919 9
         break;
      case BRW_OPCODE_LRP:
         if (inst->src[1].equals(inst->src[2])) {
            inst->opcode = BRW_OPCODE_MOV;
            inst->src[0] = inst->src[1];
            inst->src[1] = reg_undef;
            inst->src[2] = reg_undef;
            progress = true;
            break;
d1921 31
a1951 16
         break;
      case BRW_OPCODE_SEL:
         if (inst->saturate && inst->src[1].file == IMM) {
            switch (inst->conditional_mod) {
            case BRW_CONDITIONAL_LE:
            case BRW_CONDITIONAL_L:
               switch (inst->src[1].type) {
               case BRW_REGISTER_TYPE_F:
                  if (inst->src[1].imm.f >= 1.0f) {
                     inst->opcode = BRW_OPCODE_MOV;
                     inst->src[1] = reg_undef;
                     progress = true;
                  }
                  break;
               default:
                  break;
d1953 9
a1961 14
               break;
            case BRW_CONDITIONAL_GE:
            case BRW_CONDITIONAL_G:
               switch (inst->src[1].type) {
               case BRW_REGISTER_TYPE_F:
                  if (inst->src[1].imm.f <= 0.0f) {
                     inst->opcode = BRW_OPCODE_MOV;
                     inst->src[1] = reg_undef;
                     inst->conditional_mod = BRW_CONDITIONAL_NONE;
                     progress = true;
                  }
                  break;
               default:
                  break;
d1963 3
a1965 2
            default:
               break;
d1968 121
a2088 1
         break;
d2092 96
d2190 3
d2196 1
d2215 1
a2215 3
	  inst->src[0].abs || inst->src[0].negate ||
          !inst->src[0].is_contiguous() ||
          inst->src[0].subreg_offset)
d2335 1
a2335 1
	 if (scan_inst->mlen > 0 && scan_inst->base_mrf != -1) {
d2354 1
a2354 1
      invalidate_live_intervals();
d2397 1
a2397 1
      if (inst->mlen > 0 && inst->base_mrf != -1) {
d2425 1
a2425 1
      invalidate_live_intervals();
d2434 1
a2434 1
   bool inst_simd16 = (dispatch_width > 8 &&
d2453 1
a2453 1
         if (inst_simd16)
d2496 1
a2496 1
        !scan_inst->is_head_sentinel();
d2511 1
a2511 1
      bool scan_inst_simd16 = (dispatch_width > 8 &&
d2528 1
a2528 1
               if (scan_inst_simd16)
a2622 2
   bool progress = false;

a2632 1
         progress = true;
a2634 3

   if (progress)
      invalidate_live_intervals();
d2692 1
a2692 1
         invalidate_live_intervals();
a2705 16
fs_visitor::dump_instructions()
{
   calculate_register_pressure();

   int ip = 0, max_pressure = 0;
   foreach_list(node, &this->instructions) {
      backend_instruction *inst = (backend_instruction *)node;
      max_pressure = MAX2(max_pressure, regs_live_at_ip[ip]);
      fprintf(stderr, "{%3d} %4d: ", regs_live_at_ip[ip], ip);
      dump_instruction(inst);
      ++ip;
   }
   fprintf(stderr, "Maximum %3d registers live at once.\n", max_pressure);
}

void
d2711 1
a2711 1
      fprintf(stderr, "(%cf0.%d) ",
d2716 1
a2716 1
   fprintf(stderr, "%s", brw_instruction_name(inst->opcode));
d2718 1
a2718 1
      fprintf(stderr, ".sat");
d2720 1
a2720 1
      fprintf(stderr, "%s", conditional_modifier[inst->conditional_mod]);
d2725 1
a2725 1
         fprintf(stderr, ".f0.%d", inst->flag_subreg);
d2728 1
a2728 1
   fprintf(stderr, " ");
d2733 3
a2735 5
      fprintf(stderr, "vgrf%d", inst->dst.reg);
      if (virtual_grf_sizes[inst->dst.reg] != 1 ||
          inst->dst.subreg_offset)
         fprintf(stderr, "+%d.%d",
                 inst->dst.reg_offset, inst->dst.subreg_offset);
d2738 1
a2738 1
      fprintf(stderr, "m%d", inst->dst.reg);
d2741 1
a2741 1
      fprintf(stderr, "(null)");
d2744 1
a2744 28
      fprintf(stderr, "***u%d***", inst->dst.reg + inst->dst.reg_offset);
      break;
   case HW_REG:
      if (inst->dst.fixed_hw_reg.file == BRW_ARCHITECTURE_REGISTER_FILE) {
         switch (inst->dst.fixed_hw_reg.nr) {
         case BRW_ARF_NULL:
            fprintf(stderr, "null");
            break;
         case BRW_ARF_ADDRESS:
            fprintf(stderr, "a0.%d", inst->dst.fixed_hw_reg.subnr);
            break;
         case BRW_ARF_ACCUMULATOR:
            fprintf(stderr, "acc%d", inst->dst.fixed_hw_reg.subnr);
            break;
         case BRW_ARF_FLAG:
            fprintf(stderr, "f%d.%d", inst->dst.fixed_hw_reg.nr & 0xf,
                             inst->dst.fixed_hw_reg.subnr);
            break;
         default:
            fprintf(stderr, "arf%d.%d", inst->dst.fixed_hw_reg.nr & 0xf,
                               inst->dst.fixed_hw_reg.subnr);
            break;
         }
      } else {
         fprintf(stderr, "hw_reg%d", inst->dst.fixed_hw_reg.nr);
      }
      if (inst->dst.fixed_hw_reg.subnr)
         fprintf(stderr, "+%d", inst->dst.fixed_hw_reg.subnr);
d2747 1
a2747 1
      fprintf(stderr, "???");
d2750 1
a2750 1
   fprintf(stderr, ":%s, ", brw_reg_type_letters(inst->dst.type));
d2752 1
a2752 1
   for (int i = 0; i < 3 && inst->src[i].file != BAD_FILE; i++) {
d2754 1
a2754 1
         fprintf(stderr, "-");
d2756 1
a2756 1
         fprintf(stderr, "|");
d2759 3
a2761 5
         fprintf(stderr, "vgrf%d", inst->src[i].reg);
         if (virtual_grf_sizes[inst->src[i].reg] != 1 ||
             inst->src[i].subreg_offset)
            fprintf(stderr, "+%d.%d", inst->src[i].reg_offset,
                    inst->src[i].subreg_offset);
d2764 1
a2764 1
         fprintf(stderr, "***m%d***", inst->src[i].reg);
d2767 3
a2769 8
         fprintf(stderr, "u%d", inst->src[i].reg + inst->src[i].reg_offset);
         if (inst->src[i].reladdr) {
            fprintf(stderr, "+reladdr");
         } else if (virtual_grf_sizes[inst->src[i].reg] != 1 ||
             inst->src[i].subreg_offset) {
            fprintf(stderr, "+%d.%d", inst->src[i].reg_offset,
                    inst->src[i].subreg_offset);
         }
d2772 1
a2772 1
         fprintf(stderr, "(null)");
d2777 1
a2777 1
            fprintf(stderr, "%ff", inst->src[i].imm.f);
d2780 1
a2780 1
            fprintf(stderr, "%dd", inst->src[i].imm.i);
d2783 1
a2783 1
            fprintf(stderr, "%uu", inst->src[i].imm.u);
d2786 1
a2786 1
            fprintf(stderr, "???");
a2789 33
      case HW_REG:
         if (inst->src[i].fixed_hw_reg.negate)
            fprintf(stderr, "-");
         if (inst->src[i].fixed_hw_reg.abs)
            fprintf(stderr, "|");
         if (inst->src[i].fixed_hw_reg.file == BRW_ARCHITECTURE_REGISTER_FILE) {
            switch (inst->src[i].fixed_hw_reg.nr) {
            case BRW_ARF_NULL:
               fprintf(stderr, "null");
               break;
            case BRW_ARF_ADDRESS:
               fprintf(stderr, "a0.%d", inst->src[i].fixed_hw_reg.subnr);
               break;
            case BRW_ARF_ACCUMULATOR:
               fprintf(stderr, "acc%d", inst->src[i].fixed_hw_reg.subnr);
               break;
            case BRW_ARF_FLAG:
               fprintf(stderr, "f%d.%d", inst->src[i].fixed_hw_reg.nr & 0xf,
                                inst->src[i].fixed_hw_reg.subnr);
               break;
            default:
               fprintf(stderr, "arf%d.%d", inst->src[i].fixed_hw_reg.nr & 0xf,
                                  inst->src[i].fixed_hw_reg.subnr);
               break;
            }
         } else {
            fprintf(stderr, "hw_reg%d", inst->src[i].fixed_hw_reg.nr);
         }
         if (inst->src[i].fixed_hw_reg.subnr)
            fprintf(stderr, "+%d", inst->src[i].fixed_hw_reg.subnr);
         if (inst->src[i].fixed_hw_reg.abs)
            fprintf(stderr, "|");
         break;
d2791 1
a2791 1
         fprintf(stderr, "???");
d2795 1
a2795 1
         fprintf(stderr, "|");
d2797 2
a2798 6
      if (inst->src[i].file != IMM) {
         fprintf(stderr, ":%s", brw_reg_type_letters(inst->src[i].type));
      }

      if (i < 2 && inst->src[i + 1].file != BAD_FILE)
         fprintf(stderr, ", ");
d2801 1
a2801 1
   fprintf(stderr, " ");
d2804 1
a2804 1
      fprintf(stderr, "1sthalf ");
d2807 1
a2807 1
      fprintf(stderr, "2ndhalf ");
d2809 1
a2809 1
   fprintf(stderr, "\n");
d2828 1
a2828 1
					   const fs_reg &reg)
d2875 1
a2875 1
         /* R28: interpolated depth if not SIMD8. */
d2884 1
a2884 1
         /* R30: interpolated W if not SIMD8. */
a2887 2

   c->prog_data.uses_pos_offset = c->key.compute_pos_offset;
d2889 1
a2889 17
   if (c->prog_data.uses_pos_offset) {
      c->sample_pos_reg = c->nr_payload_regs;
      c->nr_payload_regs++;
   }

   /* R32: MSAA input coverage mask */
   if (fp->Base.SystemValuesRead & SYSTEM_BIT_SAMPLE_MASK_IN) {
      assert(brw->gen >= 7);
      c->sample_mask_reg = c->nr_payload_regs;
      c->nr_payload_regs++;
      if (dispatch_width == 16) {
         /* R33: input coverage mask if not SIMD8. */
         c->nr_payload_regs++;
      }
   }

   /* R34-: bary for 32-pixel. */
a2896 64
void
fs_visitor::assign_binding_table_offsets()
{
   uint32_t next_binding_table_offset = 0;

   /* If there are no color regions, we still perform an FB write to a null
    * renderbuffer, which we place at surface index 0.
    */
   c->prog_data.binding_table.render_target_start = next_binding_table_offset;
   next_binding_table_offset += MAX2(c->key.nr_color_regions, 1);

   assign_common_binding_table_offsets(next_binding_table_offset);
}

void
fs_visitor::calculate_register_pressure()
{
   invalidate_live_intervals();
   calculate_live_intervals();

   int num_instructions = 0;
   foreach_list(node, &this->instructions) {
      ++num_instructions;
   }

   regs_live_at_ip = rzalloc_array(mem_ctx, int, num_instructions);

   for (int reg = 0; reg < virtual_grf_count; reg++) {
      for (int ip = virtual_grf_start[reg]; ip <= virtual_grf_end[reg]; ip++)
         regs_live_at_ip[ip] += virtual_grf_sizes[reg];
   }
}

/**
 * Look for repeated FS_OPCODE_MOV_DISPATCH_TO_FLAGS and drop the later ones.
 *
 * The needs_unlit_centroid_workaround ends up producing one of these per
 * channel of centroid input, so it's good to clean them up.
 *
 * An assumption here is that nothing ever modifies the dispatched pixels
 * value that FS_OPCODE_MOV_DISPATCH_TO_FLAGS reads from, but the hardware
 * dictates that anyway.
 */
void
fs_visitor::opt_drop_redundant_mov_to_flags()
{
   bool flag_mov_found[2] = {false};

   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      if (inst->is_control_flow()) {
         memset(flag_mov_found, 0, sizeof(flag_mov_found));
      } else if (inst->opcode == FS_OPCODE_MOV_DISPATCH_TO_FLAGS) {
         if (!flag_mov_found[inst->flag_subreg])
            flag_mov_found[inst->flag_subreg] = true;
         else
            inst->remove();
      } else if (inst->writes_flag()) {
         flag_mov_found[inst->flag_subreg] = false;
      }
   }
}

d2901 1
a2901 3
   bool allocated_without_spills;

   assign_binding_table_offsets();
d2915 4
a2918 6
      if (fp->Base.InputsRead > 0) {
         if (brw->gen < 6)
            emit_interpolation_setup_gen4();
         else
            emit_interpolation_setup_gen6();
      }
d2932 1
a2932 1
         foreach_list(node, &*shader->base.ir) {
d2955 1
a2955 4
      assign_constant_locations();
      demote_pull_constants();

      opt_drop_redundant_mov_to_flags();
d2968 4
a2971 6
         progress = opt_peephole_predicated_break() || progress;
         progress = dead_code_eliminate() || progress;
         progress = opt_peephole_sel() || progress;
         progress = dead_control_flow_eliminate(this) || progress;
         progress = opt_saturate_propagation() || progress;
         progress = register_coalesce() || progress;
d2975 4
d2984 5
a2988 21
      static enum instruction_scheduler_mode pre_modes[] = {
         SCHEDULE_PRE,
         SCHEDULE_PRE_NON_LIFO,
         SCHEDULE_PRE_LIFO,
      };

      /* Try each scheduling heuristic to see if it can successfully register
       * allocate without spilling.  They should be ordered by decreasing
       * performance but increasing likelihood of allocating.
       */
      for (unsigned i = 0; i < ARRAY_SIZE(pre_modes); i++) {
         schedule_instructions(pre_modes[i]);

         if (0) {
            assign_regs_trivial();
            allocated_without_spills = true;
         } else {
            allocated_without_spills = assign_regs(false);
         }
         if (allocated_without_spills)
            break;
d2991 7
a2997 17
      if (!allocated_without_spills) {
         /* We assume that any spilling is worse than just dropping back to
          * SIMD8.  There's probably actually some intermediate point where
          * SIMD16 with a couple of spills is still better.
          */
         if (dispatch_width == 16) {
            fail("Failure to register allocate.  Reduce number of "
                 "live scalar values to avoid this.");
         }

         /* Since we're out of heuristics, just go spill registers until we
          * get an allocation.
          */
         while (!assign_regs(true)) {
            if (failed)
               break;
         }
d3001 1
d3012 1
a3012 2
   if (!allocated_without_spills)
      schedule_instructions(SCHEDULE_POST);
d3014 1
a3014 1
   if (dispatch_width == 8)
d3016 1
a3016 1
   else
d3019 5
d3041 1
a3041 1
   double start_time = 0;
d3053 11
a3063 2
   if (unlikely(INTEL_DEBUG & DEBUG_WM))
      brw_dump_ir(brw, "fragment", prog, &shader->base, &fp->Base);
d3082 6
a3087 10
   if (brw->gen >= 5 && likely(!(INTEL_DEBUG & DEBUG_NO16))) {
      if (!v.simd16_unsupported) {
         /* Try a SIMD16 compile */
         v2.import_uniforms(&v);
         if (!v2.run()) {
            perf_debug("SIMD16 shader failed to compile, falling back to "
                       "SIMD8 at a 10-20%% performance cost: %s", v2.fail_msg);
         } else {
            simd16_instructions = &v2.instructions;
         }
d3089 1
a3089 2
         perf_debug("SIMD16 shader unsupported, falling back to "
                    "SIMD8 at a 10-20%% performance cost: %s", v.no16_msg);
d3093 6
a3098 10
   const unsigned *assembly = NULL;
   if (brw->gen >= 8) {
      gen8_fs_generator g(brw, c, prog, fp, v.do_dual_src);
      assembly = g.generate_assembly(&v.instructions, simd16_instructions,
                                     final_assembly_size);
   } else {
      fs_generator g(brw, c, prog, fp, v.do_dual_src);
      assembly = g.generate_assembly(&v.instructions, simd16_instructions,
                                     final_assembly_size);
   }
d3111 1
a3111 1
   return assembly;
d3142 12
a3153 3
   if (brw->gen < 6 || _mesa_bitcount_64(fp->Base.InputsRead &
                                         BRW_FS_VARYING_INPUT_MASK) > 16)
      key.input_slots_valid = fp->Base.InputsRead | VARYING_BIT_POS;
d3157 1
a3157 2
   unsigned sampler_count = _mesa_fls(fp->Base.SamplersUsed);
   for (unsigned i = 0; i < sampler_count; i++) {
a3171 4
   key.nr_color_regions = _mesa_bitcount_64(fp->Base.OutputsWritten &
         ~(BITFIELD64_BIT(FRAG_RESULT_DEPTH) |
         BITFIELD64_BIT(FRAG_RESULT_SAMPLE_MASK)));

d3173 1
a3173 2
      key.render_to_fbo = _mesa_is_user_fbo(ctx->DrawBuffer) ||
                          key.nr_color_regions > 1;
d3176 1
a3176 5
   /* GL_FRAGMENT_SHADER_DERIVATIVE_HINT is almost always GL_DONT_CARE.  The
    * quality of the derivatives is likely to be determined by the driconf
    * option.
    */
   key.high_quality_derivatives = brw->disable_derivative_optimization;
d3180 1
a3180 1
   uint32_t old_prog_offset = brw->wm.base.prog_offset;
d3185 1
a3185 1
   brw->wm.base.prog_offset = old_prog_offset;
@


1.1.1.6
log
@Import Mesa 10.2.7
@
text
@d1266 1
a1266 1
   fs_inst *inst = emit(MOV(int_sample_x, fs_reg(sample_pos_reg)));
d1268 2
a1269 3
      inst->force_uncompressed = true;
      inst = emit(MOV(half(int_sample_x, 1),
                      fs_reg(suboffset(sample_pos_reg, 16))));
d1275 1
a1275 1
   inst = emit(MOV(int_sample_y, fs_reg(suboffset(sample_pos_reg, 1))));
d1277 2
a1278 3
      inst->force_uncompressed = true;
      inst = emit(MOV(half(int_sample_y, 1),
                      fs_reg(suboffset(sample_pos_reg, 17))));
a1311 5
       *
       * For 2x MSAA and SIMD16, we want to use the sequence (0, 1, 0, 1):
       * the first four slots are sample 0 of subspan 0; the next four
       * are sample 1 of subspan 0; the third group is sample 0 of
       * subspan 1, and finally sample 1 of subspan 1.
d1313 4
a1316 7
      fs_inst *inst;
      inst = emit(BRW_OPCODE_AND, t1,
                  fs_reg(retype(brw_vec1_grf(0, 0), BRW_REGISTER_TYPE_UD)),
                  fs_reg(0xc0));
      inst->force_writemask_all = true;
      inst = emit(BRW_OPCODE_SHR, t1, t1, fs_reg(5));
      inst->force_writemask_all = true;
d1318 1
a1318 2
      inst = emit(MOV(t2, brw_imm_v(c->key.persample_2x ? 0x1010 : 0x3210)));
      inst->force_writemask_all = true;
d1396 1
a1396 1
   if (brw->gen == 6 || brw->gen == 7)
d1428 1
a1428 3
   if (brw->gen >= 8) {
      inst = emit(opcode, dst, src0, src1);
   } else if (brw->gen >= 6) {
@


1.1.1.7
log
@Import Mesa 10.4.3
@
text
@d35 1
a35 1
#include "util/hash_table.h"
d41 1
a41 1
#include "util/register_allocate.h"
a48 1
#include "brw_cfg.h"
d55 1
a55 2
fs_inst::init(enum opcode opcode, uint8_t exec_size, const fs_reg &dst,
              fs_reg *src, int sources)
d58 1
d60 4
a63 57
   this->opcode = opcode;
   this->dst = dst;
   this->src = src;
   this->sources = sources;
   this->exec_size = exec_size;

   assert(dst.file != IMM && dst.file != UNIFORM);

   /* If exec_size == 0, try to guess it from the registers.  Since all
    * manner of things may use hardware registers, we first try to guess
    * based on GRF registers.  If this fails, we will go ahead and take the
    * width from the destination register.
    */
   if (this->exec_size == 0) {
      if (dst.file == GRF) {
         this->exec_size = dst.width;
      } else {
         for (int i = 0; i < sources; ++i) {
            if (src[i].file != GRF)
               continue;

            if (this->exec_size <= 1)
               this->exec_size = src[i].width;
            assert(src[i].width == 1 || src[i].width == this->exec_size);
         }
      }

      if (this->exec_size == 0 && dst.file != BAD_FILE)
         this->exec_size = dst.width;
   }
   assert(this->exec_size != 0);

   for (int i = 0; i < sources; ++i) {
      switch (this->src[i].file) {
      case BAD_FILE:
         this->src[i].effective_width = 8;
         break;
      case GRF:
      case HW_REG:
         assert(this->src[i].width > 0);
         if (this->src[i].width == 1) {
            this->src[i].effective_width = this->exec_size;
         } else {
            this->src[i].effective_width = this->src[i].width;
         }
         break;
      case IMM:
      case UNIFORM:
         this->src[i].effective_width = this->exec_size;
         break;
      default:
         unreachable("Invalid source register file");
      }
   }
   this->dst.effective_width = this->exec_size;

   this->conditional_mod = BRW_CONDITIONAL_NONE;
d66 1
a66 15
   switch (dst.file) {
   case GRF:
   case HW_REG:
   case MRF:
      this->regs_written = (dst.width * dst.stride * type_sz(dst.type) + 31) / 32;
      break;
   case BAD_FILE:
      this->regs_written = 0;
      break;
   case IMM:
   case UNIFORM:
      unreachable("Invalid destination register file");
   default:
      unreachable("Invalid register file");
   }
d73 2
a74 2
   fs_reg *src = ralloc_array(this, fs_reg, 3);
   init(BRW_OPCODE_NOP, 8, dst, src, 0);
d77 1
a77 1
fs_inst::fs_inst(enum opcode opcode, uint8_t exec_size)
d79 2
a80 2
   fs_reg *src = ralloc_array(this, fs_reg, 3);
   init(opcode, exec_size, reg_undef, src, 0);
d83 1
a83 1
fs_inst::fs_inst(enum opcode opcode, const fs_reg &dst)
d85 3
a87 3
   fs_reg *src = ralloc_array(this, fs_reg, 3);
   init(opcode, 0, dst, src, 0);
}
d89 2
a90 6
fs_inst::fs_inst(enum opcode opcode, uint8_t exec_size, const fs_reg &dst,
                 const fs_reg &src0)
{
   fs_reg *src = ralloc_array(this, fs_reg, 3);
   src[0] = src0;
   init(opcode, exec_size, dst, src, 1);
d93 1
a93 1
fs_inst::fs_inst(enum opcode opcode, const fs_reg &dst, const fs_reg &src0)
d95 4
a98 4
   fs_reg *src = ralloc_array(this, fs_reg, 3);
   src[0] = src0;
   init(opcode, 0, dst, src, 1);
}
d100 4
a103 7
fs_inst::fs_inst(enum opcode opcode, uint8_t exec_size, const fs_reg &dst,
                 const fs_reg &src0, const fs_reg &src1)
{
   fs_reg *src = ralloc_array(this, fs_reg, 3);
   src[0] = src0;
   src[1] = src1;
   init(opcode, exec_size, dst, src, 2);
d106 1
a106 2
fs_inst::fs_inst(enum opcode opcode, const fs_reg &dst, const fs_reg &src0,
                 const fs_reg &src1)
d108 5
a112 5
   fs_reg *src = ralloc_array(this, fs_reg, 3);
   src[0] = src0;
   src[1] = src1;
   init(opcode, 0, dst, src, 2);
}
d114 6
a119 8
fs_inst::fs_inst(enum opcode opcode, uint8_t exec_size, const fs_reg &dst,
                 const fs_reg &src0, const fs_reg &src1, const fs_reg &src2)
{
   fs_reg *src = ralloc_array(this, fs_reg, 3);
   src[0] = src0;
   src[1] = src1;
   src[2] = src2;
   init(opcode, exec_size, dst, src, 3);
d122 2
a123 2
fs_inst::fs_inst(enum opcode opcode, const fs_reg &dst, const fs_reg &src0,
                 const fs_reg &src1, const fs_reg &src2)
d125 15
a139 35
   fs_reg *src = ralloc_array(this, fs_reg, 3);
   src[0] = src0;
   src[1] = src1;
   src[2] = src2;
   init(opcode, 0, dst, src, 3);
}

fs_inst::fs_inst(enum opcode opcode, const fs_reg &dst, fs_reg src[], int sources)
{
   init(opcode, 0, dst, src, sources);
}

fs_inst::fs_inst(enum opcode opcode, uint8_t exec_width, const fs_reg &dst,
                 fs_reg src[], int sources)
{
   init(opcode, exec_width, dst, src, sources);
}

fs_inst::fs_inst(const fs_inst &that)
{
   memcpy(this, &that, sizeof(that));

   this->src = ralloc_array(this, fs_reg, that.sources);

   for (int i = 0; i < that.sources; i++)
      this->src[i] = that.src[i];
}

void
fs_inst::resize_sources(uint8_t num_sources)
{
   if (this->sources != num_sources) {
      this->src = reralloc(this, this->src, fs_reg, num_sources);
      this->sources = num_sources;
   }
d144 1
a144 1
   fs_visitor::op(const fs_reg &dst, const fs_reg &src0)                \
d151 1
a151 2
   fs_visitor::op(const fs_reg &dst, const fs_reg &src0,                \
                  const fs_reg &src1)                                   \
d158 1
a158 2
   fs_visitor::op(const fs_reg &dst, const fs_reg &src0,                \
                  const fs_reg &src1)                                   \
d167 1
a167 2
   fs_visitor::op(const fs_reg &dst, const fs_reg &src0,                \
                  const fs_reg &src1, const fs_reg &src2)               \
d203 1
a203 1
fs_visitor::IF(enum brw_predicate predicate)
d205 1
a205 1
   fs_inst *inst = new(mem_ctx) fs_inst(BRW_OPCODE_IF, dispatch_width);
d212 1
a212 2
fs_visitor::IF(const fs_reg &src0, const fs_reg &src1,
               enum brw_conditional_mod condition)
d215 1
a215 1
   fs_inst *inst = new(mem_ctx) fs_inst(BRW_OPCODE_IF, dispatch_width,
d227 1
a227 2
fs_visitor::CMP(fs_reg dst, fs_reg src0, fs_reg src1,
                enum brw_conditional_mod condition)
a257 26
fs_inst *
fs_visitor::LOAD_PAYLOAD(const fs_reg &dst, fs_reg *src, int sources)
{
   uint8_t exec_size = dst.width;
   for (int i = 0; i < sources; ++i) {
      assert(src[i].width % dst.width == 0);
      if (src[i].width > exec_size)
         exec_size = src[i].width;
   }

   fs_inst *inst = new(mem_ctx) fs_inst(SHADER_OPCODE_LOAD_PAYLOAD, exec_size,
                                        dst, src, sources);
   inst->regs_written = 0;
   for (int i = 0; i < sources; ++i) {
      /* The LOAD_PAYLOAD instruction only really makes sense if we are
       * dealing with whole registers.  If this ever changes, we can deal
       * with it later.
       */
      int size = src[i].effective_width * type_sz(src[i].type);
      assert(size % 32 == 0);
      inst->regs_written += (size + 31) / 32;
   }

   return inst;
}

d280 1
a280 1
                              varying_offset, fs_reg(const_offset & ~3)));
d283 1
a283 1
   if (brw->gen == 4 && dst.width == 8) {
d297 1
a297 5

   assert(dst.width % 8 == 0);
   int regs_written = 4 * (dst.width / 8) * scale;
   fs_reg vec4_result = fs_reg(GRF, virtual_grf_alloc(regs_written),
                               dst.type, dst.width);
d299 1
a299 1
   inst->regs_written = regs_written;
d311 2
a312 2
   fs_reg result = offset(vec4_result, (const_offset & 3) * scale);
   instructions.push_tail(MOV(dst, result));
d332 1
a332 1
   inst->exec_size = 8;
d350 1
a354 1
           exec_size == inst->exec_size &&
d370 5
a374 20
   switch (opcode) {
   case FS_OPCODE_VARYING_PULL_CONSTANT_LOAD_GEN7:
   case SHADER_OPCODE_SHADER_TIME_ADD:
   case FS_OPCODE_INTERPOLATE_AT_CENTROID:
   case FS_OPCODE_INTERPOLATE_AT_SAMPLE:
   case FS_OPCODE_INTERPOLATE_AT_SHARED_OFFSET:
   case FS_OPCODE_INTERPOLATE_AT_PER_SLOT_OFFSET:
   case SHADER_OPCODE_UNTYPED_ATOMIC:
   case SHADER_OPCODE_UNTYPED_SURFACE_READ:
      return true;
   case FS_OPCODE_UNIFORM_PULL_CONSTANT_LOAD:
      return src[1].file == GRF;
   case FS_OPCODE_FB_WRITE:
      return src[0].file == GRF;
   default:
      if (is_tex())
         return src[0].file == GRF;

      return false;
   }
d378 1
a378 1
fs_inst::can_do_source_mods(struct brw_context *brw)
d380 1
a380 1
   if (brw->gen == 6 && is_math())
d383 1
a383 1
   if (is_send_from_grf())
d386 1
a386 1
   if (!backend_instruction::can_do_source_mods())
d412 1
a412 2
   this->fixed_hw_reg.dw1.f = f;
   this->width = 1;
d421 1
a421 2
   this->fixed_hw_reg.dw1.d = i;
   this->width = 1;
d430 1
a430 2
   this->fixed_hw_reg.dw1.ud = u;
   this->width = 1;
a439 1
   this->width = 1 << fixed_hw_reg.width;
d453 4
a456 3
           memcmp(&fixed_hw_reg, &r.fixed_hw_reg, sizeof(fixed_hw_reg)) == 0 &&
           width == r.width &&
           stride == r.stride);
d485 26
d516 8
d554 2
a555 1
      unreachable("not reached");
d566 1
a566 1
   fs_reg ts = fs_reg(retype(brw_vec4_reg(BRW_ARCHITECTURE_REGISTER_FILE,
d571 1
a571 1
   fs_reg dst = fs_reg(GRF, virtual_grf_alloc(1), BRW_REGISTER_TYPE_UD, 4);
d574 2
a575 2
   /* We want to read the 3 fields we care about even if it's not enabled in
    * the dispatch.
d578 1
d630 1
d633 1
a633 1
   fs_reg diff = fs_reg(GRF, virtual_grf_alloc(1), BRW_REGISTER_TYPE_UD, 1);
d647 2
d656 1
a656 1
      brw_get_shader_time_index(brw, shader_prog, prog, type);
d734 1
a734 1
   return emit(new(mem_ctx) fs_inst(opcode, dispatch_width));
d738 1
a738 1
fs_visitor::emit(enum opcode opcode, const fs_reg &dst)
d744 1
a744 1
fs_visitor::emit(enum opcode opcode, const fs_reg &dst, const fs_reg &src0)
d750 1
a750 2
fs_visitor::emit(enum opcode opcode, const fs_reg &dst, const fs_reg &src0,
                 const fs_reg &src1)
d756 2
a757 2
fs_visitor::emit(enum opcode opcode, const fs_reg &dst, const fs_reg &src0,
                 const fs_reg &src1, const fs_reg &src2)
d762 8
a769 3
fs_inst *
fs_visitor::emit(enum opcode opcode, const fs_reg &dst,
                 fs_reg src[], int sources)
d771 2
a772 1
   return emit(new(mem_ctx) fs_inst(opcode, dst, src, sources));
d787 2
a788 2
           (this->dst.width * type_sz(this->dst.type)) < 32 ||
           !this->dst.is_contiguous());
d795 4
a798 26
      return mlen;
   } else if (opcode == FS_OPCODE_FB_WRITE && arg == 0) {
      return mlen;
   } else if (opcode == SHADER_OPCODE_UNTYPED_ATOMIC && arg == 0) {
      return mlen;
   } else if (opcode == SHADER_OPCODE_UNTYPED_SURFACE_READ && arg == 0) {
      return mlen;
   }

   switch (src[arg].file) {
   case BAD_FILE:
   case UNIFORM:
   case IMM:
      return 1;
   case GRF:
   case HW_REG:
      if (src[arg].stride == 0) {
         return 1;
      } else {
         int size = src[arg].width * src[arg].stride * type_sz(src[arg].type);
         return (size + 31) / 32;
      }
   case MRF:
      unreachable("MRF registers are not allowed as sources");
   default:
      unreachable("Invalid register file");
d800 1
a866 4
   case FS_OPCODE_INTERPOLATE_AT_CENTROID:
   case FS_OPCODE_INTERPOLATE_AT_SAMPLE:
   case FS_OPCODE_INTERPOLATE_AT_SHARED_OFFSET:
   case FS_OPCODE_INTERPOLATE_AT_PER_SLOT_OFFSET:
d869 2
a870 1
      unreachable("not reached");
a895 8

   switch (file) {
   case UNIFORM:
      this->width = 1;
      break;
   default:
      this->width = 8;
   }
d899 1
a899 1
fs_reg::fs_reg(enum register_file file, int reg, enum brw_reg_type type)
a904 19

   switch (file) {
   case UNIFORM:
      this->width = 1;
      break;
   default:
      this->width = 8;
   }
}

/** Fixed HW reg constructor. */
fs_reg::fs_reg(enum register_file file, int reg, enum brw_reg_type type,
               uint8_t width)
{
   init();
   this->file = file;
   this->reg = reg;
   this->type = type;
   this->width = width;
d908 1
a908 1
fs_reg::fs_reg(fs_visitor *v, const struct glsl_type *type)
a910 1
   int reg_width = v->dispatch_width / 8;
d913 1
a913 1
   this->reg = v->virtual_grf_alloc(v->type_size(type) * reg_width);
a915 2
   this->width = v->dispatch_width;
   assert(this->width == 8 || this->width == 16);
d985 1
a985 1
         stage_prog_data->param[uniforms++] = &storage->storage[i];
d1002 2
a1003 2
   const ir_state_slot *const slots = ir->get_state_slots();
   assert(slots != NULL);
d1005 1
a1005 1
   for (unsigned int i = 0; i < ir->get_num_state_slots(); i++) {
d1009 1
a1009 1
      int index = _mesa_add_state_reference(this->prog->Parameters,
d1024 1
a1024 1
            &prog->Parameters->ParameterValues[index][swiz];
a1031 2
   assert(stage == MESA_SHADER_FRAGMENT);
   brw_wm_prog_key *key = (brw_wm_prog_key*) this->key;
d1034 1
a1034 1
   bool flip = !ir->data.origin_upper_left ^ key->render_to_fbo;
d1042 1
a1042 1
   wpos = offset(wpos, 1);
d1053 1
a1053 1
	 offset += key->drawable_height - 1.0;
d1058 1
a1058 1
   wpos = offset(wpos, 1);
d1062 1
a1062 1
      emit(MOV(wpos, fs_reg(brw_vec8_grf(payload.source_depth_reg, 0))));
d1069 1
a1069 1
   wpos = offset(wpos, 1);
a1118 4
   assert(stage == MESA_SHADER_FRAGMENT);
   brw_wm_prog_data *prog_data = (brw_wm_prog_data*) this->prog_data;
   brw_wm_prog_key *key = (brw_wm_prog_key*) this->key;

d1134 1
a1134 1
      ir->determine_interpolation_mode(key->flat_shade);
d1139 1
a1139 1
	 if (prog_data->urb_setup[location] == -1) {
d1143 1
a1143 1
	    attr = offset(attr, type->vector_elements);
d1158 1
a1158 1
	       attr = offset(attr, 1);
d1164 3
d1174 3
a1176 4

                  fs_inst *inst;
                  inst = emit_linterp(attr, fs_reg(interp), interpolation_mode,
                                      false, false);
a1178 15
                  if (brw->has_pln)
                     inst->no_dd_clear = true;

                  inst = emit_linterp(attr, fs_reg(interp), interpolation_mode,
                                      ir->data.centroid && !key->persample_shading,
                                      ir->data.sample || key->persample_shading);
                  inst->predicate = BRW_PREDICATE_NORMAL;
                  inst->predicate_inverse = false;
                  if (brw->has_pln)
                     inst->no_dd_check = true;

               } else {
                  emit_linterp(attr, fs_reg(interp), interpolation_mode,
                               ir->data.centroid && !key->persample_shading,
                               ir->data.sample || key->persample_shading);
d1183 1
a1183 1
	       attr = offset(attr, 1);
d1195 1
a1195 1
fs_visitor::emit_frontfacing_interpolation()
d1197 1
a1197 1
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, glsl_type::bool_type);
d1199 1
d1201 5
a1205 15
      /* Bit 15 of g0.0 is 0 if the polygon is front facing. We want to create
       * a boolean result from this (~0/true or 0/false).
       *
       * We can use the fact that bit 15 is the MSB of g0.0:W to accomplish
       * this task in only one instruction:
       *    - a negation source modifier will flip the bit; and
       *    - a W -> D type conversion will sign extend the bit into the high
       *      word of the destination.
       *
       * An ASR 15 fills the low word of the destination.
       */
      fs_reg g0 = fs_reg(retype(brw_vec1_grf(0, 0), BRW_REGISTER_TYPE_W));
      g0.negate = true;

      emit(ASR(*reg, g0, fs_reg(15)));
d1207 3
a1209 10
      /* Bit 31 of g1.6 is 0 if the polygon is front facing. We want to create
       * a boolean result from this (1/true or 0/false).
       *
       * Like in the above case, since the bit is the MSB of g1.6:UD we can use
       * the negation source modifier to flip it. Unfortunately the SHR
       * instruction only operates on UD (or D with an abs source modifier)
       * sources without negation.
       *
       * Instead, use ASR (which will give ~0/true or 0/false) followed by an
       * AND 1.
d1211 2
a1212 6
      fs_reg asr = fs_reg(this, glsl_type::bool_type);
      fs_reg g1_6 = fs_reg(retype(brw_vec1_grf(1, 6), BRW_REGISTER_TYPE_D));
      g1_6.negate = true;

      emit(ASR(asr, g1_6, fs_reg(31)));
      emit(AND(*reg, asr, fs_reg(1)));
a1220 2
   assert(stage == MESA_SHADER_FRAGMENT);
   brw_wm_prog_key *key = (brw_wm_prog_key*) this->key;
d1223 1
a1223 1
   if (key->compute_pos_offset) {
d1240 1
a1240 1
fs_visitor::emit_samplepos_setup()
d1243 1
d1246 1
a1246 1
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, glsl_type::vec2_type);
d1263 1
a1263 1
      stride(retype(brw_vec1_grf(payload.sample_pos_reg, 0),
d1266 6
a1271 6
   if (dispatch_width == 8) {
      emit(MOV(int_sample_x, fs_reg(sample_pos_reg)));
   } else {
      emit(MOV(half(int_sample_x, 0), fs_reg(sample_pos_reg)));
      emit(MOV(half(int_sample_x, 1), fs_reg(suboffset(sample_pos_reg, 16))))
         ->force_sechalf = true;
d1275 7
a1281 8
   pos = offset(pos, 1);
   if (dispatch_width == 8) {
      emit(MOV(int_sample_y, fs_reg(suboffset(sample_pos_reg, 1))));
   } else {
      emit(MOV(half(int_sample_y, 0),
               fs_reg(suboffset(sample_pos_reg, 1))));
      emit(MOV(half(int_sample_y, 1), fs_reg(suboffset(sample_pos_reg, 17))))
         ->force_sechalf = true;
d1289 1
a1289 1
fs_visitor::emit_sampleid_setup()
a1290 2
   assert(stage == MESA_SHADER_FRAGMENT);
   brw_wm_prog_key *key = (brw_wm_prog_key*) this->key;
d1294 1
a1294 1
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, glsl_type::int_type);
d1296 1
a1296 1
   if (key->compute_sample_id) {
d1328 1
a1328 1
      inst = emit(MOV(t2, brw_imm_v(key->persample_2x ? 0x1010 : 0x3210)));
d1345 10
d1395 2
a1396 1
      unreachable("not reached: bad math opcode");
d1426 13
d1460 1
a1460 1
      emit(MOV(fs_reg(MRF, base_mrf + 1, op1.type, dispatch_width), op1));
d1473 1
a1473 1
      prog_data->dispatch_grf_start_reg = payload.num_regs;
d1475 1
a1475 3
      assert(stage == MESA_SHADER_FRAGMENT);
      brw_wm_prog_data *prog_data = (brw_wm_prog_data*) this->prog_data;
      prog_data->dispatch_grf_start_reg_16 = payload.num_regs;
d1478 1
a1478 1
   prog_data->curb_read_length = ALIGN(stage_prog_data->nr_params, 8) / 8;
d1481 4
a1484 2
   foreach_block_and_inst(block, fs_inst, inst, cfg) {
      for (unsigned int i = 0; i < inst->sources; i++) {
d1499 1
a1499 1
	    struct brw_reg brw_reg = brw_vec1_grf(payload.num_regs +
d1515 3
a1517 6
   assert(stage == MESA_SHADER_FRAGMENT);
   brw_wm_prog_data *prog_data = (brw_wm_prog_data*) this->prog_data;
   brw_wm_prog_key *key = (brw_wm_prog_key*) this->key;

   memset(prog_data->urb_setup, -1,
          sizeof(prog_data->urb_setup[0]) * VARYING_SLOT_MAX);
d1522 1
a1522 1
      if (_mesa_bitcount_64(prog->InputsRead &
d1534 1
a1534 1
            if (prog->InputsRead & BRW_FS_VARYING_INPUT_MASK &
d1536 1
a1536 1
               prog_data->urb_setup[i] = urb_next++;
d1547 1
a1547 1
                             key->input_slots_valid);
d1557 1
a1557 1
                (prog->InputsRead & BRW_FS_VARYING_INPUT_MASK &
d1559 1
a1559 1
               prog_data->urb_setup[varying] = slot - first_slot;
d1571 1
a1571 1
	 if (key->input_slots_valid & BITFIELD64_BIT(i)) {
d1579 1
a1579 1
	       prog_data->urb_setup[i] = urb_next;
d1590 2
a1591 2
      if (prog->InputsRead & BITFIELD64_BIT(VARYING_SLOT_PNTC))
         prog_data->urb_setup[VARYING_SLOT_PNTC] = urb_next++;
d1594 1
a1594 1
   prog_data->num_varying_inputs = urb_next;
d1600 1
a1600 4
   assert(stage == MESA_SHADER_FRAGMENT);
   brw_wm_prog_data *prog_data = (brw_wm_prog_data*) this->prog_data;

   int urb_start = payload.num_regs + prog_data->base.curb_read_length;
d1605 3
a1607 1
   foreach_block_and_inst(block, fs_inst, inst, cfg) {
d1621 1
a1621 1
      urb_start + prog_data->num_varying_inputs * 2;
d1646 2
d1649 1
a1649 3
   /* Count the total number of registers */
   int reg_count = 0;
   int vgrf_to_reg[num_vars];
d1651 4
a1654 28
      vgrf_to_reg[i] = reg_count;
      reg_count += virtual_grf_sizes[i];
   }

   /* An array of "split points".  For each register slot, this indicates
    * if this slot can be separated from the previous slot.  Every time an
    * instruction uses multiple elements of a register (as a source or
    * destination), we mark the used slots as inseparable.  Then we go
    * through and split the registers into the smallest pieces we can.
    */
   bool split_points[reg_count];
   memset(split_points, 0, sizeof(split_points));

   /* Mark all used registers as fully splittable */
   foreach_block_and_inst(block, fs_inst, inst, cfg) {
      if (inst->dst.file == GRF) {
         int reg = vgrf_to_reg[inst->dst.reg];
         for (int j = 1; j < this->virtual_grf_sizes[inst->dst.reg]; j++)
            split_points[reg + j] = true;
      }

      for (int i = 0; i < inst->sources; i++) {
         if (inst->src[i].file == GRF) {
            int reg = vgrf_to_reg[inst->src[i].reg];
            for (int j = 1; j < this->virtual_grf_sizes[inst->src[i].reg]; j++)
               split_points[reg + j] = true;
         }
      }
d1664 2
a1665 2
      int vgrf = this->delta_x[BRW_WM_PERSPECTIVE_PIXEL_BARYCENTRIC].reg;
      split_points[vgrf_to_reg[vgrf] + 1] = false;
d1668 8
a1675 5
   foreach_block_and_inst(block, fs_inst, inst, cfg) {
      if (inst->dst.file == GRF) {
         int reg = vgrf_to_reg[inst->dst.reg] + inst->dst.reg_offset;
         for (int j = 1; j < inst->regs_written; j++)
            split_points[reg + j] = false;
d1677 9
a1685 5
      for (int i = 0; i < inst->sources; i++) {
         if (inst->src[i].file == GRF) {
            int reg = vgrf_to_reg[inst->src[i].reg] + inst->src[i].reg_offset;
            for (int j = 1; j < inst->regs_read(this, i); j++)
               split_points[reg + j] = false;
d1690 3
a1692 4
   int new_virtual_grf[reg_count];
   int new_reg_offset[reg_count];

   int reg = 0;
d1694 10
a1703 2
      /* The first one should always be 0 as a quick sanity check. */
      assert(split_points[reg] == false);
d1705 2
a1706 29
      /* j = 0 case */
      new_reg_offset[reg] = 0;
      reg++;
      int offset = 1;

      /* j > 0 case */
      for (int j = 1; j < virtual_grf_sizes[i]; j++) {
         /* If this is a split point, reset the offset to 0 and allocate a
          * new virtual GRF for the previous offset many registers
          */
         if (split_points[reg]) {
            assert(offset <= MAX_VGRF_SIZE);
            int grf = virtual_grf_alloc(offset);
            for (int k = reg - offset; k < reg; k++)
               new_virtual_grf[k] = grf;
            offset = 0;
         }
         new_reg_offset[reg] = offset;
         offset++;
         reg++;
      }

      /* The last one gets the original register number */
      assert(offset <= MAX_VGRF_SIZE);
      virtual_grf_sizes[i] = offset;
      for (int k = reg - offset; k < reg; k++)
         new_virtual_grf[k] = i;
   }
   assert(reg == reg_count);
d1708 15
a1722 14
   foreach_block_and_inst(block, fs_inst, inst, cfg) {
      if (inst->dst.file == GRF) {
         reg = vgrf_to_reg[inst->dst.reg] + inst->dst.reg_offset;
         inst->dst.reg = new_virtual_grf[reg];
         inst->dst.reg_offset = new_reg_offset[reg];
         assert(new_reg_offset[reg] < virtual_grf_sizes[new_virtual_grf[reg]]);
      }
      for (int i = 0; i < inst->sources; i++) {
	 if (inst->src[i].file == GRF) {
            reg = vgrf_to_reg[inst->src[i].reg] + inst->src[i].reg_offset;
            inst->src[i].reg = new_virtual_grf[reg];
            inst->src[i].reg_offset = new_reg_offset[reg];
            assert(new_reg_offset[reg] < virtual_grf_sizes[new_virtual_grf[reg]]);
         }
d1737 1
a1737 1
bool
d1740 1
a1740 1
   bool progress = false;
d1744 3
a1746 2
   /* Mark which virtual GRFs are used. */
   foreach_block_and_inst(block, const fs_inst, inst, cfg) {
d1750 1
a1750 1
      for (int i = 0; i < inst->sources; i++) {
d1756 28
d1787 1
a1787 6
      if (remap_table[i] == -1) {
         /* We just found an unused register.  This means that we are
          * actually going to compact something.
          */
         progress = true;
      } else {
d1798 3
a1800 1
   foreach_block_and_inst(block, fs_inst, inst, cfg) {
d1804 1
a1804 1
      for (int i = 0; i < inst->sources; i++) {
d1810 6
a1815 11
   /* Patch all the references to delta_x/delta_y, since they're used in
    * register allocation.  If they're unused, switch them to BAD_FILE so
    * we don't think some random VGRF is delta_x/delta_y.
    */
   for (unsigned i = 0; i < ARRAY_SIZE(delta_x); i++) {
      if (delta_x[i].file == GRF) {
         if (remap_table[delta_x[i].reg] != -1) {
            delta_x[i].reg = remap_table[delta_x[i].reg];
         } else {
            delta_x[i].file = BAD_FILE;
         }
a1817 11
   for (unsigned i = 0; i < ARRAY_SIZE(delta_y); i++) {
      if (delta_y[i].file == GRF) {
         if (remap_table[delta_y[i].reg] != -1) {
            delta_y[i].reg = remap_table[delta_y[i].reg];
         } else {
            delta_y[i].file = BAD_FILE;
         }
      }
   }

   return progress;
d1839 4
a1842 1
   memset(pull_constant_loc, -1, sizeof(pull_constant_loc[0]) * uniforms);
d1850 4
a1853 2
   foreach_block_and_inst_safe(block, fs_inst, inst, cfg) {
      for (int i = 0 ; i < inst->sources; i++) {
d1863 1
a1863 1
            const gl_constant_value **values = &stage_prog_data->param[uniform];
d1900 4
a1903 2
   foreach_block_and_inst(block, fs_inst, inst, cfg) {
      for (int i = 0; i < inst->sources; i++) {
a1916 3
    *
    * If changing this value, note the limitation about total_regs in
    * brw_curbe.c.
d1971 4
a1974 2
   foreach_block_and_inst (block, fs_inst, inst, cfg) {
      for (int i = 0; i < inst->sources; i++) {
d1996 1
a1996 1
            inst->insert_before(block, &list);
d2001 1
a2001 1
               new(mem_ctx) fs_inst(FS_OPCODE_UNIFORM_PULL_CONSTANT_LOAD, 8,
d2003 1
a2003 1
            inst->insert_before(block, pull);
a2010 1
         inst->src[i].width = dispatch_width;
d2021 3
a2023 1
   foreach_block_and_inst(block, fs_inst, inst, cfg) {
d2078 1
a2078 7
         if (inst->src[0].equals(inst->src[1])) {
            inst->opcode = BRW_OPCODE_MOV;
            inst->src[1] = reg_undef;
            inst->predicate = BRW_PREDICATE_NONE;
            inst->predicate_inverse = false;
            progress = true;
         } else if (inst->saturate && inst->src[1].file == IMM) {
d2084 1
a2084 1
                  if (inst->src[1].fixed_hw_reg.dw1.f >= 1.0f) {
d2098 1
a2098 1
                  if (inst->src[1].fixed_hw_reg.dw1.f <= 0.0f) {
a2112 11
      case SHADER_OPCODE_RCP: {
         fs_inst *prev = (fs_inst *)inst->prev;
         if (prev->opcode == SHADER_OPCODE_SQRT) {
            if (inst->src[0].equals(prev->dst)) {
               inst->opcode = SHADER_OPCODE_RSQ;
               inst->src[0] = prev->src[0];
               progress = true;
            }
         }
         break;
      }
a2121 66
fs_visitor::opt_register_renaming()
{
   bool progress = false;
   int depth = 0;

   int remap[virtual_grf_count];
   memset(remap, -1, sizeof(int) * virtual_grf_count);

   foreach_block_and_inst(block, fs_inst, inst, cfg) {
      if (inst->opcode == BRW_OPCODE_IF || inst->opcode == BRW_OPCODE_DO) {
         depth++;
      } else if (inst->opcode == BRW_OPCODE_ENDIF ||
                 inst->opcode == BRW_OPCODE_WHILE) {
         depth--;
      }

      /* Rewrite instruction sources. */
      for (int i = 0; i < inst->sources; i++) {
         if (inst->src[i].file == GRF &&
             remap[inst->src[i].reg] != -1 &&
             remap[inst->src[i].reg] != inst->src[i].reg) {
            inst->src[i].reg = remap[inst->src[i].reg];
            progress = true;
         }
      }

      const int dst = inst->dst.reg;

      if (depth == 0 &&
          inst->dst.file == GRF &&
          virtual_grf_sizes[inst->dst.reg] == inst->dst.width / 8 &&
          !inst->is_partial_write()) {
         if (remap[dst] == -1) {
            remap[dst] = dst;
         } else {
            remap[dst] = virtual_grf_alloc(inst->dst.width / 8);
            inst->dst.reg = remap[dst];
            progress = true;
         }
      } else if (inst->dst.file == GRF &&
                 remap[dst] != -1 &&
                 remap[dst] != dst) {
         inst->dst.reg = remap[dst];
         progress = true;
      }
   }

   if (progress) {
      invalidate_live_intervals();

      for (unsigned i = 0; i < ARRAY_SIZE(delta_x); i++) {
         if (delta_x[i].file == GRF && remap[delta_x[i].reg] != -1) {
            delta_x[i].reg = remap[delta_x[i].reg];
         }
      }
      for (unsigned i = 0; i < ARRAY_SIZE(delta_y); i++) {
         if (delta_y[i].file == GRF && remap[delta_y[i].reg] != -1) {
            delta_y[i].reg = remap[delta_y[i].reg];
         }
      }
   }

   return progress;
}

bool
d2127 1
a2127 3
   /* No MRFs on Gen >= 7. */
   if (brw->gen >= 7)
      return false;
d2129 2
a2130 1
   calculate_live_intervals();
a2131 1
   foreach_block_and_inst_safe(block, fs_inst, inst, cfg) {
d2151 2
a2152 1
      } else if (inst->exec_size == 16) {
d2167 4
a2170 1
      foreach_inst_in_block_reverse_starting_from(fs_inst, scan_inst, inst, block) {
d2188 1
a2188 1
            if (scan_inst->regs_written > scan_inst->dst.width / 8)
d2209 1
a2209 1
	       inst->remove(block);
d2219 1
a2219 1
	 if (block->start() == scan_inst)
d2226 1
a2226 1
	 for (int i = 0; i < scan_inst->sources; i++) {
d2245 3
a2247 1
	    } else if (scan_inst->exec_size == 16) {
a2285 46
 * Once we've generated code, try to convert normal FS_OPCODE_FB_WRITE
 * instructions to FS_OPCODE_REP_FB_WRITE.
 */
void
fs_visitor::emit_repclear_shader()
{
   brw_wm_prog_key *key = (brw_wm_prog_key*) this->key;
   int base_mrf = 1;
   int color_mrf = base_mrf + 2;

   fs_inst *mov = emit(MOV(vec4(brw_message_reg(color_mrf)),
                           fs_reg(UNIFORM, 0, BRW_REGISTER_TYPE_F)));
   mov->force_writemask_all = true;

   fs_inst *write;
   if (key->nr_color_regions == 1) {
      write = emit(FS_OPCODE_REP_FB_WRITE);
      write->saturate = key->clamp_fragment_color;
      write->base_mrf = color_mrf;
      write->target = 0;
      write->header_present = false;
      write->mlen = 1;
   } else {
      assume(key->nr_color_regions > 0);
      for (int i = 0; i < key->nr_color_regions; ++i) {
         write = emit(FS_OPCODE_REP_FB_WRITE);
         write->saturate = key->clamp_fragment_color;
         write->base_mrf = base_mrf;
         write->target = i;
         write->header_present = true;
         write->mlen = 3;
      }
   }
   write->eot = true;

   calculate_cfg();

   assign_constant_locations();
   assign_curb_setup();

   /* Now that we have the uniform assigned, go ahead and force it to a vec4. */
   assert(mov->src[0].file == HW_REG);
   mov->src[0] = brw_vec4_grf(mov->src[0].fixed_hw_reg.nr, 0);
}

/**
d2301 3
a2303 1
   foreach_block_and_inst_safe (block, fs_inst, inst, cfg) {
d2312 1
a2312 1
	    inst->remove(block);
d2360 4
d2365 1
a2365 1
   for (int i = 0; i < inst->sources; i++) {
d2379 1
a2379 1
         if (inst->exec_size == 16)
d2402 1
a2402 2
fs_visitor::insert_gen4_pre_send_dependency_workarounds(bblock_t *block,
                                                        fs_inst *inst)
d2404 2
a2405 1
   int write_len = inst->regs_written;
d2421 4
a2424 1
   foreach_inst_in_block_reverse_starting_from(fs_inst, scan_inst, inst, block) {
d2428 1
a2428 1
      if (block->start() == scan_inst) {
d2431 1
a2431 1
               inst->insert_before(block, DEP_RESOLVE_MOV(first_write_grf + i));
d2437 4
d2447 1
a2447 1
            int reg = scan_inst->dst.reg + i;
d2452 1
a2452 1
               inst->insert_before(block, DEP_RESOLVE_MOV(reg));
d2454 1
a2454 1
               if (scan_inst->exec_size == 16)
d2483 1
a2483 1
fs_visitor::insert_gen4_post_send_dependency_workarounds(bblock_t *block, fs_inst *inst)
d2485 1
a2485 1
   int write_len = inst->regs_written;
d2495 3
a2497 1
   foreach_inst_in_block_starting_from(fs_inst, scan_inst, inst, block) {
d2499 1
a2499 1
      if (block->end() == scan_inst) {
d2502 1
a2502 2
               scan_inst->insert_before(block,
                                        DEP_RESOLVE_MOV(first_write_grf + i));
d2518 1
a2518 1
         scan_inst->insert_before(block, DEP_RESOLVE_MOV(scan_inst->dst.reg));
d2539 1
a2539 1
         last_inst->insert_before(block, DEP_RESOLVE_MOV(first_write_grf + i));
d2555 3
a2557 1
   foreach_block_and_inst(block, fs_inst, inst, cfg) {
d2559 2
a2560 2
         insert_gen4_pre_send_dependency_workarounds(block, inst);
         insert_gen4_post_send_dependency_workarounds(block, inst);
d2588 3
a2590 1
   foreach_block_and_inst (block, fs_inst, inst, cfg) {
d2601 1
a2601 1
         const_offset_reg.fixed_hw_reg.dw1.ud /= 4;
d2610 1
a2610 1
                                               8, payload, const_offset_reg);
d2615 1
a2615 1
         inst->insert_before(block, setup);
a2636 105
bool
fs_visitor::lower_load_payload()
{
   bool progress = false;

   int vgrf_to_reg[virtual_grf_count];
   int reg_count = 16; /* Leave room for MRF */
   for (int i = 0; i < virtual_grf_count; ++i) {
      vgrf_to_reg[i] = reg_count;
      reg_count += virtual_grf_sizes[i];
   }

   struct {
      bool written:1; /* Whether this register has ever been written */
      bool force_writemask_all:1;
      bool force_sechalf:1;
   } metadata[reg_count];
   memset(metadata, 0, sizeof(metadata));

   foreach_block_and_inst_safe (block, fs_inst, inst, cfg) {
      int dst_reg;
      if (inst->dst.file == GRF) {
         dst_reg = vgrf_to_reg[inst->dst.reg];
      } else {
         /* MRF */
         dst_reg = inst->dst.reg;
      }

      if (inst->dst.file == MRF || inst->dst.file == GRF) {
         bool force_sechalf = inst->force_sechalf;
         bool toggle_sechalf = inst->dst.width == 16 &&
                               type_sz(inst->dst.type) == 4;
         for (int i = 0; i < inst->regs_written; ++i) {
            metadata[dst_reg + i].written = true;
            metadata[dst_reg + i].force_sechalf = force_sechalf;
            metadata[dst_reg + i].force_writemask_all = inst->force_writemask_all;
            force_sechalf = (toggle_sechalf != force_sechalf);
         }
      }

      if (inst->opcode == SHADER_OPCODE_LOAD_PAYLOAD) {
         assert(inst->dst.file == MRF || inst->dst.file == GRF);
         fs_reg dst = inst->dst;

         for (int i = 0; i < inst->sources; i++) {
            dst.width = inst->src[i].effective_width;
            dst.type = inst->src[i].type;

            if (inst->src[i].file == BAD_FILE) {
               /* Do nothing but otherwise increment as normal */
            } else if (dst.file == MRF &&
                       dst.width == 8 &&
                       brw->has_compr4 &&
                       i + 4 < inst->sources &&
                       inst->src[i + 4].equals(horiz_offset(inst->src[i], 8))) {
               fs_reg compr4_dst = dst;
               compr4_dst.reg += BRW_MRF_COMPR4;
               compr4_dst.width = 16;
               fs_reg compr4_src = inst->src[i];
               compr4_src.width = 16;
               fs_inst *mov = MOV(compr4_dst, compr4_src);
               mov->force_writemask_all = true;
               inst->insert_before(block, mov);
               /* Mark i+4 as BAD_FILE so we don't emit a MOV for it */
               inst->src[i + 4].file = BAD_FILE;
            } else {
               fs_inst *mov = MOV(dst, inst->src[i]);
               if (inst->src[i].file == GRF) {
                  int src_reg = vgrf_to_reg[inst->src[i].reg] +
                                inst->src[i].reg_offset;
                  mov->force_sechalf = metadata[src_reg].force_sechalf;
                  mov->force_writemask_all = metadata[src_reg].force_writemask_all;
                  metadata[dst_reg] = metadata[src_reg];
                  if (dst.width * type_sz(dst.type) > 32) {
                     assert((!metadata[src_reg].written ||
                             !metadata[src_reg].force_sechalf) &&
                            (!metadata[src_reg + 1].written ||
                             metadata[src_reg + 1].force_sechalf));
                     metadata[dst_reg + 1] = metadata[src_reg + 1];
                  }
               } else {
                  metadata[dst_reg].force_writemask_all = false;
                  metadata[dst_reg].force_sechalf = false;
                  if (dst.width == 16) {
                     metadata[dst_reg + 1].force_writemask_all = false;
                     metadata[dst_reg + 1].force_sechalf = true;
                  }
               }
               inst->insert_before(block, mov);
            }

            dst = offset(dst, 1);
         }

         inst->remove(block);
         progress = true;
      }
   }

   if (progress)
      invalidate_live_intervals();

   return progress;
}

a2639 6
   dump_instructions(NULL);
}

void
fs_visitor::dump_instructions(const char *name)
{
a2640 6
   FILE *file = stderr;
   if (name && geteuid() != 0) {
      file = fopen(name, "w");
      if (!file)
         file = stderr;
   }
d2643 2
a2644 1
   foreach_block_and_inst(block, backend_instruction, inst, cfg) {
d2646 2
a2647 2
      fprintf(file, "{%3d} %4d: ", regs_live_at_ip[ip], ip);
      dump_instruction(inst, file);
d2650 1
a2650 5
   fprintf(file, "Maximum %3d registers live at once.\n", max_pressure);

   if (file != stderr) {
      fclose(file);
   }
a2655 6
   dump_instruction(be_inst, stderr);
}

void
fs_visitor::dump_instruction(backend_instruction *be_inst, FILE *file)
{
d2659 1
a2659 1
      fprintf(file, "(%cf0.%d) ",
d2664 1
a2664 1
   fprintf(file, "%s", brw_instruction_name(inst->opcode));
d2666 1
a2666 1
      fprintf(file, ".sat");
d2668 1
a2668 1
      fprintf(file, "%s", conditional_modifier[inst->conditional_mod]);
d2673 1
a2673 1
         fprintf(file, ".f0.%d", inst->flag_subreg);
d2676 1
a2676 1
   fprintf(file, "(%d) ", inst->exec_size);
d2681 2
a2682 4
      fprintf(file, "vgrf%d", inst->dst.reg);
      if (inst->dst.width != dispatch_width)
         fprintf(file, "@@%d", inst->dst.width);
      if (virtual_grf_sizes[inst->dst.reg] != inst->dst.width / 8 ||
d2684 1
a2684 1
         fprintf(file, "+%d.%d",
d2688 1
a2688 1
      fprintf(file, "m%d", inst->dst.reg);
d2691 1
a2691 1
      fprintf(file, "(null)");
d2694 1
a2694 1
      fprintf(file, "***u%d***", inst->dst.reg + inst->dst.reg_offset);
d2700 1
a2700 1
            fprintf(file, "null");
d2703 1
a2703 1
            fprintf(file, "a0.%d", inst->dst.fixed_hw_reg.subnr);
d2706 1
a2706 1
            fprintf(file, "acc%d", inst->dst.fixed_hw_reg.subnr);
d2709 1
a2709 1
            fprintf(file, "f%d.%d", inst->dst.fixed_hw_reg.nr & 0xf,
d2713 1
a2713 1
            fprintf(file, "arf%d.%d", inst->dst.fixed_hw_reg.nr & 0xf,
d2718 1
a2718 1
         fprintf(file, "hw_reg%d", inst->dst.fixed_hw_reg.nr);
d2721 1
a2721 1
         fprintf(file, "+%d", inst->dst.fixed_hw_reg.subnr);
d2724 1
a2724 1
      fprintf(file, "???");
d2727 1
a2727 1
   fprintf(file, ":%s, ", brw_reg_type_letters(inst->dst.type));
d2729 1
a2729 1
   for (int i = 0; i < inst->sources; i++) {
d2731 1
a2731 1
         fprintf(file, "-");
d2733 1
a2733 1
         fprintf(file, "|");
d2736 2
a2737 4
         fprintf(file, "vgrf%d", inst->src[i].reg);
         if (inst->src[i].width != dispatch_width)
            fprintf(file, "@@%d", inst->src[i].width);
         if (virtual_grf_sizes[inst->src[i].reg] != inst->src[i].width / 8 ||
d2739 1
a2739 1
            fprintf(file, "+%d.%d", inst->src[i].reg_offset,
d2743 1
a2743 1
         fprintf(file, "***m%d***", inst->src[i].reg);
d2746 1
a2746 1
         fprintf(file, "u%d", inst->src[i].reg + inst->src[i].reg_offset);
d2748 4
a2751 3
            fprintf(file, "+reladdr");
         } else if (inst->src[i].subreg_offset) {
            fprintf(file, "+%d.%d", inst->src[i].reg_offset,
d2756 1
a2756 1
         fprintf(file, "(null)");
d2761 1
a2761 1
            fprintf(file, "%ff", inst->src[i].fixed_hw_reg.dw1.f);
d2764 1
a2764 1
            fprintf(file, "%dd", inst->src[i].fixed_hw_reg.dw1.d);
d2767 1
a2767 1
            fprintf(file, "%uu", inst->src[i].fixed_hw_reg.dw1.ud);
d2770 1
a2770 1
            fprintf(file, "???");
d2776 1
a2776 1
            fprintf(file, "-");
d2778 1
a2778 1
            fprintf(file, "|");
d2782 1
a2782 1
               fprintf(file, "null");
d2785 1
a2785 1
               fprintf(file, "a0.%d", inst->src[i].fixed_hw_reg.subnr);
d2788 1
a2788 1
               fprintf(file, "acc%d", inst->src[i].fixed_hw_reg.subnr);
d2791 1
a2791 1
               fprintf(file, "f%d.%d", inst->src[i].fixed_hw_reg.nr & 0xf,
d2795 1
a2795 1
               fprintf(file, "arf%d.%d", inst->src[i].fixed_hw_reg.nr & 0xf,
d2800 1
a2800 1
            fprintf(file, "hw_reg%d", inst->src[i].fixed_hw_reg.nr);
d2803 1
a2803 1
            fprintf(file, "+%d", inst->src[i].fixed_hw_reg.subnr);
d2805 1
a2805 1
            fprintf(file, "|");
d2808 1
a2808 1
         fprintf(file, "???");
d2812 1
a2812 1
         fprintf(file, "|");
d2815 1
a2815 1
         fprintf(file, ":%s", brw_reg_type_letters(inst->src[i].type));
d2818 2
a2819 2
      if (i < inst->sources - 1 && inst->src[i + 1].file != BAD_FILE)
         fprintf(file, ", ");
d2822 1
a2822 1
   fprintf(file, " ");
d2824 5
a2828 6
   if (dispatch_width == 16 && inst->exec_size == 8) {
      if (inst->force_sechalf)
         fprintf(file, "2ndhalf ");
      else
         fprintf(file, "1sthalf ");
   }
d2830 1
a2830 1
   fprintf(file, "\n");
d2865 2
a2866 4
      (prog->InputsRead & (1 << VARYING_SLOT_POS)) != 0;
   unsigned barycentric_interp_modes =
      (stage == MESA_SHADER_FRAGMENT) ?
      ((brw_wm_prog_data*) this->prog_data)->barycentric_interp_modes : 0;
d2871 1
a2871 1
   payload.num_regs = 2;
d2883 2
a2884 2
         payload.barycentric_coord_reg[i] = payload.num_regs;
         payload.num_regs += 2;
d2886 1
a2886 1
            payload.num_regs += 2;
d2893 2
a2894 2
      payload.source_depth_reg = payload.num_regs;
      payload.num_regs++;
d2897 1
a2897 1
         payload.num_regs++;
d2902 2
a2903 2
      payload.source_w_reg = payload.num_regs;
      payload.num_regs++;
d2906 1
a2906 1
         payload.num_regs++;
d2910 5
a2914 9
   if (stage == MESA_SHADER_FRAGMENT) {
      brw_wm_prog_data *prog_data = (brw_wm_prog_data*) this->prog_data;
      brw_wm_prog_key *key = (brw_wm_prog_key*) this->key;
      prog_data->uses_pos_offset = key->compute_pos_offset;
      /* R31: MSAA position offsets. */
      if (prog_data->uses_pos_offset) {
         payload.sample_pos_reg = payload.num_regs;
         payload.num_regs++;
      }
d2918 1
a2918 1
   if (prog->SystemValuesRead & SYSTEM_BIT_SAMPLE_MASK_IN) {
d2920 2
a2921 2
      payload.sample_mask_in_reg = payload.num_regs;
      payload.num_regs++;
d2924 1
a2924 1
         payload.num_regs++;
d2931 2
a2932 2
   if (prog->OutputsWritten & BITFIELD64_BIT(FRAG_RESULT_DEPTH)) {
      source_depth_to_render_target = true;
a2938 3
   assert(stage == MESA_SHADER_FRAGMENT);
   brw_wm_prog_data *prog_data = (brw_wm_prog_data*) this->prog_data;
   brw_wm_prog_key *key = (brw_wm_prog_key*) this->key;
d2944 2
a2945 2
   prog_data->binding_table.render_target_start = next_binding_table_offset;
   next_binding_table_offset += MAX2(key->nr_color_regions, 1);
d2956 4
a2959 3
   unsigned num_instructions = 0;
   foreach_block(block, cfg)
      num_instructions += block->instructions.length();
d2984 3
a2986 1
   foreach_block_and_inst_safe(block, fs_inst, inst, cfg) {
d2993 1
a2993 1
            inst->remove(block);
a2999 137
void
fs_visitor::optimize()
{
   calculate_cfg();

   split_virtual_grfs();

   move_uniform_array_access_to_pull_constants();
   assign_constant_locations();
   demote_pull_constants();

   opt_drop_redundant_mov_to_flags();

#define OPT(pass, args...) do {                                         \
      pass_num++;                                                       \
      bool this_progress = pass(args);                                  \
                                                                        \
      if (unlikely(INTEL_DEBUG & DEBUG_OPTIMIZER) && this_progress) {   \
         char filename[64];                                             \
         snprintf(filename, 64, "fs%d-%04d-%02d-%02d-" #pass,           \
                  dispatch_width, shader_prog ? shader_prog->Name : 0, iteration, pass_num); \
                                                                        \
         backend_visitor::dump_instructions(filename);                  \
      }                                                                 \
                                                                        \
      progress = progress || this_progress;                             \
   } while (false)

   if (unlikely(INTEL_DEBUG & DEBUG_OPTIMIZER)) {
      char filename[64];
      snprintf(filename, 64, "fs%d-%04d-00-start",
               dispatch_width, shader_prog ? shader_prog->Name : 0);

      backend_visitor::dump_instructions(filename);
   }

   bool progress;
   int iteration = 0;
   do {
      progress = false;
      iteration++;
      int pass_num = 0;

      OPT(remove_duplicate_mrf_writes);

      OPT(opt_algebraic);
      OPT(opt_cse);
      OPT(opt_copy_propagate);
      OPT(opt_peephole_predicated_break);
      OPT(dead_code_eliminate);
      OPT(opt_peephole_sel);
      OPT(dead_control_flow_eliminate, this);
      OPT(opt_register_renaming);
      OPT(opt_saturate_propagation);
      OPT(register_coalesce);
      OPT(compute_to_mrf);

      OPT(compact_virtual_grfs);
   } while (progress);

   if (lower_load_payload()) {
      split_virtual_grfs();
      register_coalesce();
      compute_to_mrf();
      dead_code_eliminate();
   }

   lower_uniform_pull_constant_loads();
}

void
fs_visitor::allocate_registers()
{
   bool allocated_without_spills;

   static enum instruction_scheduler_mode pre_modes[] = {
      SCHEDULE_PRE,
      SCHEDULE_PRE_NON_LIFO,
      SCHEDULE_PRE_LIFO,
   };

   /* Try each scheduling heuristic to see if it can successfully register
    * allocate without spilling.  They should be ordered by decreasing
    * performance but increasing likelihood of allocating.
    */
   for (unsigned i = 0; i < ARRAY_SIZE(pre_modes); i++) {
      schedule_instructions(pre_modes[i]);

      if (0) {
         assign_regs_trivial();
         allocated_without_spills = true;
      } else {
         allocated_without_spills = assign_regs(false);
      }
      if (allocated_without_spills)
         break;
   }

   if (!allocated_without_spills) {
      /* We assume that any spilling is worse than just dropping back to
       * SIMD8.  There's probably actually some intermediate point where
       * SIMD16 with a couple of spills is still better.
       */
      if (dispatch_width == 16) {
         fail("Failure to register allocate.  Reduce number of "
              "live scalar values to avoid this.");
      } else {
         perf_debug("Fragment shader triggered register spilling.  "
                    "Try reducing the number of live scalar values to "
                    "improve performance.\n");
      }

      /* Since we're out of heuristics, just go spill registers until we
       * get an allocation.
       */
      while (!assign_regs(true)) {
         if (failed)
            break;
      }
   }

   /* This must come after all optimization and register allocation, since
    * it inserts dead code that happens to have side effects, and it does
    * so based on the actual physical registers in use.
    */
   insert_gen4_send_dependency_workarounds();

   if (failed)
      return;

   if (!allocated_without_spills)
      schedule_instructions(SCHEDULE_POST);

   if (last_scratch > 0)
      prog_data->total_scratch = brw_get_scratch_size(last_scratch);
}

d3003 2
a3004 1
   sanity_param_count = prog->Parameters->NumParameters;
a3014 2
   } else if (brw->use_rep_send && dispatch_width == 16) {
      emit_repclear_shader();
d3020 1
a3020 1
      if (prog->InputsRead > 0) {
d3030 1
a3030 7
      bool uses_kill =
         (stage == MESA_SHADER_FRAGMENT) &&
         ((brw_wm_prog_data*) this->prog_data)->uses_kill;
      bool alpha_test_func =
         (stage == MESA_SHADER_FRAGMENT) &&
         ((brw_wm_prog_key*) this->key)->alpha_test_func;
      if (uses_kill || alpha_test_func) {
d3039 2
a3040 1
         foreach_in_list(ir_instruction, ir, shader->base.ir) {
d3054 1
a3054 1
      if (alpha_test_func)
d3059 29
a3087 1
      optimize();
d3092 32
a3123 1
      allocate_registers();
d3125 8
a3132 2
      if (failed)
         return false;
d3134 7
d3142 10
a3151 7
   if (stage == MESA_SHADER_FRAGMENT) {
      brw_wm_prog_data *prog_data = (brw_wm_prog_data*) this->prog_data;
      if (dispatch_width == 8)
         prog_data->reg_blocks = brw_register_blocks(grf_used);
      else
         prog_data->reg_blocks_16 = brw_register_blocks(grf_used);
   }
d3158 1
a3158 1
   assert(sanity_param_count == prog->Parameters->NumParameters);
d3164 1
a3164 4
brw_wm_fs_emit(struct brw_context *brw,
               void *mem_ctx,
               const struct brw_wm_prog_key *key,
               struct brw_wm_prog_data *prog_data,
d3183 1
a3183 1
      brw_dump_ir("fragment", prog, &shader->base, &fp->Base);
d3187 1
a3187 1
   fs_visitor v(brw, mem_ctx, key, prog_data, prog, fp, 8);
d3200 3
a3202 4
   cfg_t *simd16_cfg = NULL;
   fs_visitor v2(brw, mem_ctx, key, prog_data, prog, fp, 16);
   if (brw->gen >= 5 && likely(!(INTEL_DEBUG & DEBUG_NO16) ||
                               brw->use_rep_send)) {
d3210 1
a3210 1
            simd16_cfg = v2.cfg;
d3218 5
a3222 5
   cfg_t *simd8_cfg;
   int no_simd8 = (INTEL_DEBUG & DEBUG_NO8) || brw->no_simd8;
   if (no_simd8 && simd16_cfg) {
      simd8_cfg = NULL;
      prog_data->no_8 = true;
d3224 3
a3226 2
      simd8_cfg = v.cfg;
      prog_data->no_8 = false;
a3228 7
   fs_generator g(brw, mem_ctx, key, prog_data, prog, fp,
                  v.runtime_check_aads_emit, INTEL_DEBUG & DEBUG_WM);
   if (simd8_cfg)
      g.generate_code(simd8_cfg, 8);
   if (simd16_cfg)
      prog_data->prog_offset_16 = g.generate_code(simd16_cfg, 16);

d3231 1
a3231 1
         brw_wm_debug_recompile(brw, prog, key);
d3240 1
a3240 1
   return g.get_assembly(final_assembly_size);
d3274 2
@


1.1.1.8
log
@Import Mesa 10.2.9
@
text
@d35 1
a35 1
#include "main/hash_table.h"
d41 1
a41 1
#include "program/register_allocate.h"
d49 1
d56 2
a57 1
fs_inst::init()
d60 57
a118 5
   this->dst = reg_undef;
   this->src[0] = reg_undef;
   this->src[1] = reg_undef;
   this->src[2] = reg_undef;

d120 15
a134 1
   this->regs_written = 1;
d141 38
a178 2
   init();
   this->opcode = BRW_OPCODE_NOP;
d181 2
a182 1
fs_inst::fs_inst(enum opcode opcode)
d184 4
a187 2
   init();
   this->opcode = opcode;
d190 2
a191 1
fs_inst::fs_inst(enum opcode opcode, fs_reg dst)
d193 6
a198 3
   init();
   this->opcode = opcode;
   this->dst = dst;
d200 8
a207 2
   if (dst.file == GRF)
      assert(dst.reg_offset >= 0);
d210 1
a210 1
fs_inst::fs_inst(enum opcode opcode, fs_reg dst, fs_reg src0)
d212 2
a213 4
   init();
   this->opcode = opcode;
   this->dst = dst;
   this->src[0] = src0;
d215 4
a218 4
   if (dst.file == GRF)
      assert(dst.reg_offset >= 0);
   if (src[0].file == GRF)
      assert(src[0].reg_offset >= 0);
d221 1
a221 1
fs_inst::fs_inst(enum opcode opcode, fs_reg dst, fs_reg src0, fs_reg src1)
d223 3
a225 5
   init();
   this->opcode = opcode;
   this->dst = dst;
   this->src[0] = src0;
   this->src[1] = src1;
d227 2
a228 6
   if (dst.file == GRF)
      assert(dst.reg_offset >= 0);
   if (src[0].file == GRF)
      assert(src[0].reg_offset >= 0);
   if (src[1].file == GRF)
      assert(src[1].reg_offset >= 0);
d231 2
a232 2
fs_inst::fs_inst(enum opcode opcode, fs_reg dst,
		 fs_reg src0, fs_reg src1, fs_reg src2)
d234 4
a237 15
   init();
   this->opcode = opcode;
   this->dst = dst;
   this->src[0] = src0;
   this->src[1] = src1;
   this->src[2] = src2;

   if (dst.file == GRF)
      assert(dst.reg_offset >= 0);
   if (src[0].file == GRF)
      assert(src[0].reg_offset >= 0);
   if (src[1].file == GRF)
      assert(src[1].reg_offset >= 0);
   if (src[2].file == GRF)
      assert(src[2].reg_offset >= 0);
d242 1
a242 1
   fs_visitor::op(fs_reg dst, fs_reg src0)                              \
d249 2
a250 1
   fs_visitor::op(fs_reg dst, fs_reg src0, fs_reg src1)                 \
d257 2
a258 1
   fs_visitor::op(fs_reg dst, fs_reg src0, fs_reg src1)                 \
d267 2
a268 1
   fs_visitor::op(fs_reg dst, fs_reg src0, fs_reg src1, fs_reg src2)    \
d304 1
a304 1
fs_visitor::IF(uint32_t predicate)
d306 1
a306 1
   fs_inst *inst = new(mem_ctx) fs_inst(BRW_OPCODE_IF);
d313 2
a314 1
fs_visitor::IF(fs_reg src0, fs_reg src1, uint32_t condition)
d317 1
a317 1
   fs_inst *inst = new(mem_ctx) fs_inst(BRW_OPCODE_IF,
d329 2
a330 1
fs_visitor::CMP(fs_reg dst, fs_reg src0, fs_reg src1, uint32_t condition)
d361 26
d409 1
a409 1
                              varying_offset, const_offset & ~3));
d412 1
a412 1
   if (brw->gen == 4 && dispatch_width == 8) {
d426 5
a430 1
   fs_reg vec4_result = fs_reg(GRF, virtual_grf_alloc(4 * scale), dst.type);
d432 1
a432 1
   inst->regs_written = 4 * scale;
d444 2
a445 2
   vec4_result.reg_offset += (const_offset & 3) * scale;
   instructions.push_tail(MOV(dst, vec4_result));
d465 1
a465 1
   inst->force_uncompressed = true;
a482 1
           sampler == inst->sampler &&
d487 1
d503 20
a522 5
   return (opcode == FS_OPCODE_VARYING_PULL_CONSTANT_LOAD_GEN7 ||
           opcode == SHADER_OPCODE_SHADER_TIME_ADD ||
           (opcode == FS_OPCODE_UNIFORM_PULL_CONSTANT_LOAD &&
            src[1].file == GRF) ||
           (is_tex() && src[0].file == GRF));
d526 1
a526 1
fs_visitor::can_do_source_mods(fs_inst *inst)
d528 1
a528 1
   if (brw->gen == 6 && inst->is_math())
d531 1
a531 1
   if (inst->is_send_from_grf())
d534 1
a534 1
   if (!inst->can_do_source_mods())
d560 2
a561 1
   this->imm.f = f;
d570 2
a571 1
   this->imm.i = i;
d580 2
a581 1
   this->imm.u = u;
d591 1
d605 3
a607 4
           memcmp(&fixed_hw_reg, &r.fixed_hw_reg,
                  sizeof(fixed_hw_reg)) == 0 &&
           stride == r.stride &&
           imm.u == r.imm.u);
a635 26
fs_reg::is_zero() const
{
   if (file != IMM)
      return false;

   return type == BRW_REGISTER_TYPE_F ? imm.f == 0.0 : imm.i == 0;
}

bool
fs_reg::is_one() const
{
   if (file != IMM)
      return false;

   return type == BRW_REGISTER_TYPE_F ? imm.f == 1.0 : imm.i == 1;
}

bool
fs_reg::is_null() const
{
   return file == HW_REG &&
          fixed_hw_reg.file == BRW_ARCHITECTURE_REGISTER_FILE &&
          fixed_hw_reg.nr == BRW_ARF_NULL;
}

bool
a640 8
bool
fs_reg::is_accumulator() const
{
   return file == HW_REG &&
          fixed_hw_reg.file == BRW_ARCHITECTURE_REGISTER_FILE &&
          fixed_hw_reg.nr == BRW_ARF_ACCUMULATOR;
}

d671 1
a671 2
      assert(!"not reached");
      break;
d682 1
a682 1
   fs_reg ts = fs_reg(retype(brw_vec1_reg(BRW_ARCHITECTURE_REGISTER_FILE,
d687 1
a687 1
   fs_reg dst = fs_reg(this, glsl_type::uint_type);
d690 2
a691 2
   /* We want to read the 3 fields we care about (mostly field 0, but also 2)
    * even if it's not enabled in the dispatch.
a693 1
   mov->force_uncompressed = true;
a744 1
   push_force_uncompressed();
d747 1
a747 1
   fs_reg diff = fs_reg(this, glsl_type::uint_type);
a760 2

   pop_force_uncompressed();
d768 1
a768 1
      brw_get_shader_time_index(brw, shader_prog, &fp->Base, type);
d846 1
a846 1
   return emit(new(mem_ctx) fs_inst(opcode));
d850 1
a850 1
fs_visitor::emit(enum opcode opcode, fs_reg dst)
d856 1
a856 1
fs_visitor::emit(enum opcode opcode, fs_reg dst, fs_reg src0)
d862 2
a863 1
fs_visitor::emit(enum opcode opcode, fs_reg dst, fs_reg src0, fs_reg src1)
d869 2
a870 2
fs_visitor::emit(enum opcode opcode, fs_reg dst,
                 fs_reg src0, fs_reg src1, fs_reg src2)
d875 3
a877 8
void
fs_visitor::push_force_uncompressed()
{
   force_uncompressed_stack++;
}

void
fs_visitor::pop_force_uncompressed()
d879 1
a879 2
   force_uncompressed_stack--;
   assert(force_uncompressed_stack >= 0);
d894 2
a895 2
           this->force_uncompressed ||
           this->force_sechalf || !this->dst.is_contiguous());
d902 26
a927 4
      if (v->dispatch_width == 16)
	 return (mlen + 1) / 2;
      else
	 return mlen;
a928 1
   return 1;
d995 4
d1001 1
a1001 2
      assert(!"not reached");
      return inst->mlen;
d1027 8
d1038 1
a1038 1
fs_reg::fs_reg(enum register_file file, int reg, uint32_t type)
d1044 19
d1066 1
a1066 1
fs_reg::fs_reg(class fs_visitor *v, const struct glsl_type *type)
d1069 1
d1072 1
a1072 1
   this->reg = v->virtual_grf_alloc(v->type_size(type));
d1075 2
d1146 1
a1146 1
         stage_prog_data->param[uniforms++] = &storage->storage[i].f;
d1163 2
a1164 2
   const ir_state_slot *const slots = ir->state_slots;
   assert(ir->state_slots != NULL);
d1166 1
a1166 1
   for (unsigned int i = 0; i < ir->num_state_slots; i++) {
d1170 1
a1170 1
      int index = _mesa_add_state_reference(this->fp->Base.Parameters,
d1185 1
a1185 1
            &fp->Base.Parameters->ParameterValues[index][swiz].f;
d1193 2
d1197 1
a1197 1
   bool flip = !ir->data.origin_upper_left ^ c->key.render_to_fbo;
d1205 1
a1205 1
   wpos.reg_offset++;
d1216 1
a1216 1
	 offset += c->key.drawable_height - 1.0;
d1221 1
a1221 1
   wpos.reg_offset++;
d1225 1
a1225 1
      emit(MOV(wpos, fs_reg(brw_vec8_grf(c->source_depth_reg, 0))));
d1232 1
a1232 1
   wpos.reg_offset++;
d1282 4
d1301 1
a1301 1
      ir->determine_interpolation_mode(c->key.flat_shade);
d1306 1
a1306 1
	 if (c->prog_data.urb_setup[location] == -1) {
d1310 1
a1310 1
	    attr.reg_offset += type->vector_elements;
d1325 1
a1325 1
	       attr.reg_offset++;
a1330 3
               emit_linterp(attr, fs_reg(interp), interpolation_mode,
                            ir->data.centroid && !c->key.persample_shading,
                            ir->data.sample || c->key.persample_shading);
d1338 4
a1341 3
                  fs_inst *inst = emit_linterp(attr, fs_reg(interp),
                                               interpolation_mode,
                                               false, false);
d1344 15
d1363 1
a1363 1
	       attr.reg_offset++;
d1375 1
a1375 1
fs_visitor::emit_frontfacing_interpolation(ir_variable *ir)
d1377 1
a1377 1
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, ir->type);
a1378 1
   /* The frontfacing comes in as a bit in the thread payload. */
d1380 15
a1394 5
      emit(BRW_OPCODE_ASR, *reg,
	   fs_reg(retype(brw_vec1_grf(0, 0), BRW_REGISTER_TYPE_D)),
	   fs_reg(15));
      emit(BRW_OPCODE_NOT, *reg, *reg);
      emit(BRW_OPCODE_AND, *reg, *reg, fs_reg(1));
d1396 10
a1405 3
      struct brw_reg r1_6ud = retype(brw_vec1_grf(1, 6), BRW_REGISTER_TYPE_UD);
      /* bit 31 is "primitive is back face", so checking < (1 << 31) gives
       * us front face
d1407 6
a1412 2
      emit(CMP(*reg, fs_reg(r1_6ud), fs_reg(1u << 31), BRW_CONDITIONAL_L));
      emit(BRW_OPCODE_AND, *reg, *reg, fs_reg(1u));
d1421 2
d1425 1
a1425 1
   if (c->key.compute_pos_offset) {
d1442 1
a1442 1
fs_visitor::emit_samplepos_setup(ir_variable *ir)
a1444 1
   assert(ir->type == glsl_type::vec2_type);
d1447 1
a1447 1
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, ir->type);
d1464 1
a1464 1
      stride(retype(brw_vec1_grf(c->sample_pos_reg, 0),
d1467 6
a1472 6
   fs_inst *inst = emit(MOV(int_sample_x, fs_reg(sample_pos_reg)));
   if (dispatch_width == 16) {
      inst->force_uncompressed = true;
      inst = emit(MOV(half(int_sample_x, 1),
                      fs_reg(suboffset(sample_pos_reg, 16))));
      inst->force_sechalf = true;
d1476 8
a1483 7
   pos.reg_offset++;
   inst = emit(MOV(int_sample_y, fs_reg(suboffset(sample_pos_reg, 1))));
   if (dispatch_width == 16) {
      inst->force_uncompressed = true;
      inst = emit(MOV(half(int_sample_y, 1),
                      fs_reg(suboffset(sample_pos_reg, 17))));
      inst->force_sechalf = true;
d1491 1
a1491 1
fs_visitor::emit_sampleid_setup(ir_variable *ir)
d1493 2
d1498 1
a1498 1
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, ir->type);
d1500 1
a1500 1
   if (c->key.compute_sample_id) {
d1532 1
a1532 1
      inst = emit(MOV(t2, brw_imm_v(c->key.persample_2x ? 0x1010 : 0x3210)));
a1548 10
fs_reg *
fs_visitor::emit_samplemaskin_setup(ir_variable *ir)
{
   assert(brw->gen >= 7);
   this->current_annotation = "compute gl_SampleMaskIn";
   fs_reg *reg = new(this->mem_ctx) fs_reg(this, ir->type);
   emit(MOV(*reg, fs_reg(retype(brw_vec8_grf(c->sample_mask_reg, 0), BRW_REGISTER_TYPE_D))));
   return reg;
}

d1589 1
a1589 2
      assert(!"not reached: bad math opcode");
      return NULL;
a1618 13
   switch (opcode) {
   case SHADER_OPCODE_INT_QUOTIENT:
   case SHADER_OPCODE_INT_REMAINDER:
      if (brw->gen >= 7)
	 no16("SIMD16 INTDIV unsupported\n");
      break;
   case SHADER_OPCODE_POW:
      break;
   default:
      assert(!"not reached: unsupported binary math opcode.");
      return NULL;
   }

d1640 1
a1640 1
      emit(BRW_OPCODE_MOV, fs_reg(MRF, base_mrf + 1, op1.type), op1);
d1653 1
a1653 1
      c->prog_data.first_curbe_grf = c->nr_payload_regs;
d1655 3
a1657 1
      c->prog_data.first_curbe_grf_16 = c->nr_payload_regs;
d1660 1
a1660 1
   c->prog_data.curb_read_length = ALIGN(stage_prog_data->nr_params, 8) / 8;
d1663 2
a1664 4
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      for (unsigned int i = 0; i < 3; i++) {
d1679 1
a1679 1
	    struct brw_reg brw_reg = brw_vec1_grf(c->nr_payload_regs +
d1695 6
a1700 3
   for (unsigned int i = 0; i < VARYING_SLOT_MAX; i++) {
      c->prog_data.urb_setup[i] = -1;
   }
d1705 1
a1705 1
      if (_mesa_bitcount_64(fp->Base.InputsRead &
d1717 1
a1717 1
            if (fp->Base.InputsRead & BRW_FS_VARYING_INPUT_MASK &
d1719 1
a1719 1
               c->prog_data.urb_setup[i] = urb_next++;
d1730 1
a1730 1
                             c->key.input_slots_valid);
d1740 1
a1740 1
                (fp->Base.InputsRead & BRW_FS_VARYING_INPUT_MASK &
d1742 1
a1742 1
               c->prog_data.urb_setup[varying] = slot - first_slot;
d1754 1
a1754 1
	 if (c->key.input_slots_valid & BITFIELD64_BIT(i)) {
d1762 1
a1762 1
	       c->prog_data.urb_setup[i] = urb_next;
d1773 2
a1774 2
      if (fp->Base.InputsRead & BITFIELD64_BIT(VARYING_SLOT_PNTC))
         c->prog_data.urb_setup[VARYING_SLOT_PNTC] = urb_next++;
d1777 1
a1777 1
   c->prog_data.num_varying_inputs = urb_next;
d1783 4
a1786 1
   int urb_start = c->nr_payload_regs + c->prog_data.curb_read_length;
d1791 1
a1791 3
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

d1805 1
a1805 1
      urb_start + c->prog_data.num_varying_inputs * 2;
a1829 2
   bool split_grf[num_vars];
   int new_virtual_grf[num_vars];
d1831 3
a1833 1
   /* Try to split anything > 0 sized. */
d1835 28
a1862 4
      if (this->virtual_grf_sizes[i] != 1)
	 split_grf[i] = true;
      else
	 split_grf[i] = false;
d1872 2
a1873 2
      split_grf[this->delta_x[BRW_WM_PERSPECTIVE_PIXEL_BARYCENTRIC].reg] =
         false;
d1876 5
a1880 8
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      /* If there's a SEND message that requires contiguous destination
       * registers, no splitting is allowed.
       */
      if (inst->regs_written > 1) {
	 split_grf[inst->dst.reg] = false;
d1882 5
a1886 9

      /* If we're sending from a GRF, don't split it, on the assumption that
       * the send is reading the whole thing.
       */
      if (inst->is_send_from_grf()) {
         for (int i = 0; i < 3; i++) {
            if (inst->src[i].file == GRF) {
               split_grf[inst->src[i].reg] = false;
            }
d1891 4
a1894 3
   /* Allocate new space for split regs.  Note that the virtual
    * numbers will be contiguous.
    */
d1896 30
a1925 9
      if (split_grf[i]) {
	 new_virtual_grf[i] = virtual_grf_alloc(1);
	 for (int j = 2; j < this->virtual_grf_sizes[i]; j++) {
	    int reg = virtual_grf_alloc(1);
	    assert(reg == new_virtual_grf[i] + j - 1);
	    (void) reg;
	 }
	 this->virtual_grf_sizes[i] = 1;
      }
d1927 1
d1929 14
a1942 18
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      if (inst->dst.file == GRF &&
	  split_grf[inst->dst.reg] &&
	  inst->dst.reg_offset != 0) {
	 inst->dst.reg = (new_virtual_grf[inst->dst.reg] +
			  inst->dst.reg_offset - 1);
	 inst->dst.reg_offset = 0;
      }
      for (int i = 0; i < 3; i++) {
	 if (inst->src[i].file == GRF &&
	     split_grf[inst->src[i].reg] &&
	     inst->src[i].reg_offset != 0) {
	    inst->src[i].reg = (new_virtual_grf[inst->src[i].reg] +
				inst->src[i].reg_offset - 1);
	    inst->src[i].reg_offset = 0;
	 }
d1957 1
a1957 1
void
d1960 1
a1960 1
   /* Mark which virtual GRFs are used, and count how many. */
d1964 2
a1965 3
   foreach_list(node, &this->instructions) {
      const fs_inst *inst = (const fs_inst *) node;

d1969 1
a1969 1
      for (int i = 0; i < 3; i++) {
a1974 28
   /* In addition to registers used in instructions, fs_visitor keeps
    * direct references to certain special values which must be patched:
    */
   struct {
      fs_reg *reg;
      unsigned count;
   } special[] = {
      { &frag_depth, 1 },
      { &pixel_x, 1 },
      { &pixel_y, 1 },
      { &pixel_w, 1 },
      { &wpos_w, 1 },
      { &dual_src_output, 1 },
      { outputs, ARRAY_SIZE(outputs) },
      { delta_x, ARRAY_SIZE(delta_x) },
      { delta_y, ARRAY_SIZE(delta_y) },
      { &sample_mask, 1 },
      { &shader_start_time, 1 },
   };

   /* Treat all special values as used, to be conservative */
   for (unsigned i = 0; i < ARRAY_SIZE(special); i++) {
      for (unsigned j = 0; j < special[i].count; j++) {
         if (special[i].reg[j].file == GRF)
            remap_table[special[i].reg[j].reg] = 0;
      }
   }

d1978 6
a1983 1
      if (remap_table[i] != -1) {
d1994 1
a1994 3
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *) node;

d1998 1
a1998 1
      for (int i = 0; i < 3; i++) {
d2004 11
a2014 6
   /* Patch all the references to special values */
   for (unsigned i = 0; i < ARRAY_SIZE(special); i++) {
      for (unsigned j = 0; j < special[i].count; j++) {
         fs_reg *reg = &special[i].reg[j];
         if (reg->file == GRF && remap_table[reg->reg] != -1)
            reg->reg = remap_table[reg->reg];
d2017 11
d2049 1
a2049 4

   for (unsigned int i = 0; i < uniforms; i++) {
      pull_constant_loc[i] = -1;
   }
d2057 2
a2058 4
   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      for (int i = 0 ; i < 3; i++) {
d2068 1
a2068 1
            const float **values = &stage_prog_data->param[uniform];
d2105 2
a2106 4
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *) node;

      for (int i = 0; i < 3; i++) {
d2120 3
d2177 2
a2178 4
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

      for (int i = 0; i < 3; i++) {
d2200 1
a2200 1
            inst->insert_before(&list);
d2205 1
a2205 1
               new(mem_ctx) fs_inst(FS_OPCODE_UNIFORM_PULL_CONSTANT_LOAD,
d2207 1
a2207 1
            inst->insert_before(pull);
d2215 1
d2226 1
a2226 3
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

d2281 7
a2287 1
         if (inst->saturate && inst->src[1].file == IMM) {
d2293 1
a2293 1
                  if (inst->src[1].imm.f >= 1.0f) {
d2307 1
a2307 1
                  if (inst->src[1].imm.f <= 0.0f) {
d2322 11
d2342 66
d2413 4
d2419 1
a2419 3
   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

d2439 1
a2439 2
      } else if (dispatch_width == 16 &&
		 (!inst->force_uncompressed && !inst->force_sechalf)) {
d2454 1
a2454 4
      fs_inst *scan_inst;
      for (scan_inst = (fs_inst *)inst->prev;
	   scan_inst->prev != NULL;
	   scan_inst = (fs_inst *)scan_inst->prev) {
d2472 1
a2472 1
            if (scan_inst->regs_written > 1)
d2493 1
a2493 1
	       inst->remove();
d2503 1
a2503 1
	 if (scan_inst->is_control_flow() && scan_inst->opcode != BRW_OPCODE_IF)
d2510 1
a2510 1
	 for (int i = 0; i < 3; i++) {
d2529 1
a2529 3
	    } else if (dispatch_width == 16 &&
		       (!scan_inst->force_uncompressed &&
			!scan_inst->force_sechalf)) {
d2568 46
d2629 1
a2629 3
   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

d2638 1
a2638 1
	    inst->remove();
a2685 4
   bool inst_simd16 = (dispatch_width > 8 &&
                       !inst->force_uncompressed &&
                       !inst->force_sechalf);

d2687 1
a2687 1
   for (int i = 0; i < 3; i++) {
d2701 1
a2701 1
         if (inst_simd16)
d2724 2
a2725 1
fs_visitor::insert_gen4_pre_send_dependency_workarounds(fs_inst *inst)
d2727 1
a2727 2
   int reg_size = dispatch_width / 8;
   int write_len = inst->regs_written * reg_size;
d2743 1
a2743 4
   for (fs_inst *scan_inst = (fs_inst *)inst->prev;
        !scan_inst->is_head_sentinel();
        scan_inst = (fs_inst *)scan_inst->prev) {

d2747 1
a2747 1
      if (scan_inst->is_control_flow()) {
d2750 1
a2750 1
               inst->insert_before(DEP_RESOLVE_MOV(first_write_grf + i));
a2755 4
      bool scan_inst_simd16 = (dispatch_width > 8 &&
                               !scan_inst->force_uncompressed &&
                               !scan_inst->force_sechalf);

d2762 1
a2762 1
            int reg = scan_inst->dst.reg + i * reg_size;
d2767 1
a2767 1
               inst->insert_before(DEP_RESOLVE_MOV(reg));
d2769 1
a2769 1
               if (scan_inst_simd16)
d2798 1
a2798 1
fs_visitor::insert_gen4_post_send_dependency_workarounds(fs_inst *inst)
d2800 1
a2800 1
   int write_len = inst->regs_written * dispatch_width / 8;
d2810 1
a2810 3
   for (fs_inst *scan_inst = (fs_inst *)inst->next;
        !scan_inst->is_tail_sentinel();
        scan_inst = (fs_inst *)scan_inst->next) {
d2812 1
a2812 1
      if (scan_inst->is_control_flow()) {
d2815 2
a2816 1
               scan_inst->insert_before(DEP_RESOLVE_MOV(first_write_grf + i));
d2832 1
a2832 1
         scan_inst->insert_before(DEP_RESOLVE_MOV(scan_inst->dst.reg));
d2853 1
a2853 1
         last_inst->insert_before(DEP_RESOLVE_MOV(first_write_grf + i));
d2869 1
a2869 3
   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

d2871 2
a2872 2
         insert_gen4_pre_send_dependency_workarounds(inst);
         insert_gen4_post_send_dependency_workarounds(inst);
d2900 1
a2900 3
   foreach_list(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

d2911 1
a2911 1
         const_offset_reg.imm.u /= 4;
d2920 1
a2920 1
                                               payload, const_offset_reg);
d2925 1
a2925 1
         inst->insert_before(setup);
d2947 105
d3055 6
d3062 6
d3070 1
a3070 2
   foreach_list(node, &this->instructions) {
      backend_instruction *inst = (backend_instruction *)node;
d3072 2
a3073 2
      fprintf(stderr, "{%3d} %4d: ", regs_live_at_ip[ip], ip);
      dump_instruction(inst);
d3076 5
a3080 1
   fprintf(stderr, "Maximum %3d registers live at once.\n", max_pressure);
d3086 6
d3095 1
a3095 1
      fprintf(stderr, "(%cf0.%d) ",
d3100 1
a3100 1
   fprintf(stderr, "%s", brw_instruction_name(inst->opcode));
d3102 1
a3102 1
      fprintf(stderr, ".sat");
d3104 1
a3104 1
      fprintf(stderr, "%s", conditional_modifier[inst->conditional_mod]);
d3109 1
a3109 1
         fprintf(stderr, ".f0.%d", inst->flag_subreg);
d3112 1
a3112 1
   fprintf(stderr, " ");
d3117 4
a3120 2
      fprintf(stderr, "vgrf%d", inst->dst.reg);
      if (virtual_grf_sizes[inst->dst.reg] != 1 ||
d3122 1
a3122 1
         fprintf(stderr, "+%d.%d",
d3126 1
a3126 1
      fprintf(stderr, "m%d", inst->dst.reg);
d3129 1
a3129 1
      fprintf(stderr, "(null)");
d3132 1
a3132 1
      fprintf(stderr, "***u%d***", inst->dst.reg + inst->dst.reg_offset);
d3138 1
a3138 1
            fprintf(stderr, "null");
d3141 1
a3141 1
            fprintf(stderr, "a0.%d", inst->dst.fixed_hw_reg.subnr);
d3144 1
a3144 1
            fprintf(stderr, "acc%d", inst->dst.fixed_hw_reg.subnr);
d3147 1
a3147 1
            fprintf(stderr, "f%d.%d", inst->dst.fixed_hw_reg.nr & 0xf,
d3151 1
a3151 1
            fprintf(stderr, "arf%d.%d", inst->dst.fixed_hw_reg.nr & 0xf,
d3156 1
a3156 1
         fprintf(stderr, "hw_reg%d", inst->dst.fixed_hw_reg.nr);
d3159 1
a3159 1
         fprintf(stderr, "+%d", inst->dst.fixed_hw_reg.subnr);
d3162 1
a3162 1
      fprintf(stderr, "???");
d3165 1
a3165 1
   fprintf(stderr, ":%s, ", brw_reg_type_letters(inst->dst.type));
d3167 1
a3167 1
   for (int i = 0; i < 3 && inst->src[i].file != BAD_FILE; i++) {
d3169 1
a3169 1
         fprintf(stderr, "-");
d3171 1
a3171 1
         fprintf(stderr, "|");
d3174 4
a3177 2
         fprintf(stderr, "vgrf%d", inst->src[i].reg);
         if (virtual_grf_sizes[inst->src[i].reg] != 1 ||
d3179 1
a3179 1
            fprintf(stderr, "+%d.%d", inst->src[i].reg_offset,
d3183 1
a3183 1
         fprintf(stderr, "***m%d***", inst->src[i].reg);
d3186 1
a3186 1
         fprintf(stderr, "u%d", inst->src[i].reg + inst->src[i].reg_offset);
d3188 3
a3190 4
            fprintf(stderr, "+reladdr");
         } else if (virtual_grf_sizes[inst->src[i].reg] != 1 ||
             inst->src[i].subreg_offset) {
            fprintf(stderr, "+%d.%d", inst->src[i].reg_offset,
d3195 1
a3195 1
         fprintf(stderr, "(null)");
d3200 1
a3200 1
            fprintf(stderr, "%ff", inst->src[i].imm.f);
d3203 1
a3203 1
            fprintf(stderr, "%dd", inst->src[i].imm.i);
d3206 1
a3206 1
            fprintf(stderr, "%uu", inst->src[i].imm.u);
d3209 1
a3209 1
            fprintf(stderr, "???");
d3215 1
a3215 1
            fprintf(stderr, "-");
d3217 1
a3217 1
            fprintf(stderr, "|");
d3221 1
a3221 1
               fprintf(stderr, "null");
d3224 1
a3224 1
               fprintf(stderr, "a0.%d", inst->src[i].fixed_hw_reg.subnr);
d3227 1
a3227 1
               fprintf(stderr, "acc%d", inst->src[i].fixed_hw_reg.subnr);
d3230 1
a3230 1
               fprintf(stderr, "f%d.%d", inst->src[i].fixed_hw_reg.nr & 0xf,
d3234 1
a3234 1
               fprintf(stderr, "arf%d.%d", inst->src[i].fixed_hw_reg.nr & 0xf,
d3239 1
a3239 1
            fprintf(stderr, "hw_reg%d", inst->src[i].fixed_hw_reg.nr);
d3242 1
a3242 1
            fprintf(stderr, "+%d", inst->src[i].fixed_hw_reg.subnr);
d3244 1
a3244 1
            fprintf(stderr, "|");
d3247 1
a3247 1
         fprintf(stderr, "???");
d3251 1
a3251 1
         fprintf(stderr, "|");
d3254 1
a3254 1
         fprintf(stderr, ":%s", brw_reg_type_letters(inst->src[i].type));
d3257 2
a3258 2
      if (i < 2 && inst->src[i + 1].file != BAD_FILE)
         fprintf(stderr, ", ");
d3261 1
a3261 1
   fprintf(stderr, " ");
d3263 6
a3268 2
   if (inst->force_uncompressed)
      fprintf(stderr, "1sthalf ");
d3270 1
a3270 4
   if (inst->force_sechalf)
      fprintf(stderr, "2ndhalf ");

   fprintf(stderr, "\n");
d3305 4
a3308 2
      (fp->Base.InputsRead & (1 << VARYING_SLOT_POS)) != 0;
   unsigned barycentric_interp_modes = c->prog_data.barycentric_interp_modes;
d3313 1
a3313 1
   c->nr_payload_regs = 2;
d3325 2
a3326 2
         c->barycentric_coord_reg[i] = c->nr_payload_regs;
         c->nr_payload_regs += 2;
d3328 1
a3328 1
            c->nr_payload_regs += 2;
d3335 2
a3336 2
      c->source_depth_reg = c->nr_payload_regs;
      c->nr_payload_regs++;
d3339 1
a3339 1
         c->nr_payload_regs++;
d3344 2
a3345 2
      c->source_w_reg = c->nr_payload_regs;
      c->nr_payload_regs++;
d3348 1
a3348 1
         c->nr_payload_regs++;
d3352 9
a3360 5
   c->prog_data.uses_pos_offset = c->key.compute_pos_offset;
   /* R31: MSAA position offsets. */
   if (c->prog_data.uses_pos_offset) {
      c->sample_pos_reg = c->nr_payload_regs;
      c->nr_payload_regs++;
d3364 1
a3364 1
   if (fp->Base.SystemValuesRead & SYSTEM_BIT_SAMPLE_MASK_IN) {
d3366 2
a3367 2
      c->sample_mask_reg = c->nr_payload_regs;
      c->nr_payload_regs++;
d3370 1
a3370 1
         c->nr_payload_regs++;
d3377 2
a3378 2
   if (fp->Base.OutputsWritten & BITFIELD64_BIT(FRAG_RESULT_DEPTH)) {
      c->source_depth_to_render_target = true;
d3385 3
d3393 2
a3394 2
   c->prog_data.binding_table.render_target_start = next_binding_table_offset;
   next_binding_table_offset += MAX2(c->key.nr_color_regions, 1);
d3405 3
a3407 4
   int num_instructions = 0;
   foreach_list(node, &this->instructions) {
      ++num_instructions;
   }
d3432 1
a3432 3
   foreach_list_safe(node, &this->instructions) {
      fs_inst *inst = (fs_inst *)node;

d3439 1
a3439 1
            inst->remove();
d3446 137
d3586 1
a3586 2
   sanity_param_count = fp->Base.Parameters->NumParameters;
   bool allocated_without_spills;
d3597 2
d3604 1
a3604 1
      if (fp->Base.InputsRead > 0) {
d3614 7
a3620 1
      if (fp->UsesKill || c->key.alpha_test_func) {
d3629 1
a3629 2
         foreach_list(node, &*shader->base.ir) {
            ir_instruction *ir = (ir_instruction *)node;
d3643 1
a3643 1
      if (c->key.alpha_test_func)
d3648 1
a3648 29
      split_virtual_grfs();

      move_uniform_array_access_to_pull_constants();
      assign_constant_locations();
      demote_pull_constants();

      opt_drop_redundant_mov_to_flags();

      bool progress;
      do {
	 progress = false;

         compact_virtual_grfs();

	 progress = remove_duplicate_mrf_writes() || progress;

	 progress = opt_algebraic() || progress;
	 progress = opt_cse() || progress;
	 progress = opt_copy_propagate() || progress;
         progress = opt_peephole_predicated_break() || progress;
         progress = dead_code_eliminate() || progress;
         progress = opt_peephole_sel() || progress;
         progress = dead_control_flow_eliminate(this) || progress;
         progress = opt_saturate_propagation() || progress;
         progress = register_coalesce() || progress;
	 progress = compute_to_mrf() || progress;
      } while (progress);

      lower_uniform_pull_constant_loads();
d3653 1
a3653 22
      static enum instruction_scheduler_mode pre_modes[] = {
         SCHEDULE_PRE,
         SCHEDULE_PRE_NON_LIFO,
         SCHEDULE_PRE_LIFO,
      };

      /* Try each scheduling heuristic to see if it can successfully register
       * allocate without spilling.  They should be ordered by decreasing
       * performance but increasing likelihood of allocating.
       */
      for (unsigned i = 0; i < ARRAY_SIZE(pre_modes); i++) {
         schedule_instructions(pre_modes[i]);

         if (0) {
            assign_regs_trivial();
            allocated_without_spills = true;
         } else {
            allocated_without_spills = assign_regs(false);
         }
         if (allocated_without_spills)
            break;
      }
d3655 3
a3657 9
      if (!allocated_without_spills) {
         /* We assume that any spilling is worse than just dropping back to
          * SIMD8.  There's probably actually some intermediate point where
          * SIMD16 with a couple of spills is still better.
          */
         if (dispatch_width == 16) {
            fail("Failure to register allocate.  Reduce number of "
                 "live scalar values to avoid this.");
         }
d3659 6
a3664 8
         /* Since we're out of heuristics, just go spill registers until we
          * get an allocation.
          */
         while (!assign_regs(true)) {
            if (failed)
               break;
         }
      }
a3665 18
   assert(force_uncompressed_stack == 0);

   /* This must come after all optimization and register allocation, since
    * it inserts dead code that happens to have side effects, and it does
    * so based on the actual physical registers in use.
    */
   insert_gen4_send_dependency_workarounds();

   if (failed)
      return false;

   if (!allocated_without_spills)
      schedule_instructions(SCHEDULE_POST);

   if (dispatch_width == 8)
      c->prog_data.reg_blocks = brw_register_blocks(grf_used);
   else
      c->prog_data.reg_blocks_16 = brw_register_blocks(grf_used);
d3672 1
a3672 1
   assert(sanity_param_count == fp->Base.Parameters->NumParameters);
d3678 4
a3681 1
brw_wm_fs_emit(struct brw_context *brw, struct brw_wm_compile *c,
d3700 1
a3700 1
      brw_dump_ir(brw, "fragment", prog, &shader->base, &fp->Base);
d3704 1
a3704 1
   fs_visitor v(brw, c, prog, fp, 8);
d3717 4
a3720 3
   exec_list *simd16_instructions = NULL;
   fs_visitor v2(brw, c, prog, fp, 16);
   if (brw->gen >= 5 && likely(!(INTEL_DEBUG & DEBUG_NO16))) {
d3728 1
a3728 1
            simd16_instructions = &v2.instructions;
d3736 5
a3740 5
   const unsigned *assembly = NULL;
   if (brw->gen >= 8) {
      gen8_fs_generator g(brw, c, prog, fp, v.do_dual_src);
      assembly = g.generate_assembly(&v.instructions, simd16_instructions,
                                     final_assembly_size);
d3742 2
a3743 3
      fs_generator g(brw, c, prog, fp, v.do_dual_src);
      assembly = g.generate_assembly(&v.instructions, simd16_instructions,
                                     final_assembly_size);
d3746 7
d3755 1
a3755 1
         brw_wm_debug_recompile(brw, prog, &c->key);
d3764 1
a3764 1
   return assembly;
a3797 2

   key.clamp_fragment_color = ctx->API == API_OPENGL_COMPAT;
@


