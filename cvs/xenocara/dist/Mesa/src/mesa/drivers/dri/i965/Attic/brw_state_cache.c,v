head	1.11;
access;
symbols
	OPENBSD_5_8:1.10.0.4
	OPENBSD_5_8_BASE:1.10
	OPENBSD_5_7:1.10.0.2
	OPENBSD_5_7_BASE:1.10
	v10_2_9:1.1.1.6
	v10_4_3:1.1.1.5
	v10_2_7:1.1.1.4
	OPENBSD_5_6:1.8.0.2
	OPENBSD_5_6_BASE:1.8
	v10_2_3:1.1.1.4
	OPENBSD_5_5:1.7.0.2
	OPENBSD_5_5_BASE:1.7
	v9_2_5:1.1.1.3
	v9_2_3:1.1.1.3
	v9_2_2:1.1.1.3
	v9_2_1:1.1.1.3
	v9_2_0:1.1.1.3
	OPENBSD_5_4:1.6.0.4
	OPENBSD_5_4_BASE:1.6
	OPENBSD_5_3:1.6.0.2
	OPENBSD_5_3_BASE:1.6
	OPENBSD_5_2:1.5.0.4
	OPENBSD_5_2_BASE:1.5
	OPENBSD_5_1_BASE:1.5
	OPENBSD_5_1:1.5.0.2
	v7_10_3:1.1.1.2
	OPENBSD_5_0:1.4.0.6
	OPENBSD_5_0_BASE:1.4
	OPENBSD_4_9:1.4.0.2
	OPENBSD_4_9_BASE:1.4
	OPENBSD_4_8:1.4.0.4
	OPENBSD_4_8_BASE:1.4
	OPENBSD_4_7:1.3.0.4
	OPENBSD_4_7_BASE:1.3
	OPENBSD_4_6:1.3.0.2
	OPENBSD_4_6_BASE:1.3
	OPENBSD_4_5:1.2.0.2
	OPENBSD_4_5_BASE:1.2
	OPENBSD_4_4:1.1.1.1.0.6
	OPENBSD_4_4_BASE:1.1.1.1
	OPENBSD_4_3_BASE:1.1.1.1
	OPENBSD_4_3:1.1.1.1.0.4
	v7_0_1:1.1.1.1
	OPENBSD_4_2:1.1.1.1.0.2
	OPENBSD_4_2_BASE:1.1.1.1
	v6_5_2:1.1.1.1
	v6_5_1:1.1.1.1
	mesa:1.1.1;
locks; strict;
comment	@ * @;


1.11
date	2015.12.23.05.17.49;	author jsg;	state dead;
branches;
next	1.10;
commitid	TnlogFl9nOv2eaRf;

1.10
date	2015.02.20.23.09.58;	author jsg;	state Exp;
branches;
next	1.9;
commitid	4ry2gvZGMXkCUD2n;

1.9
date	2015.01.25.14.41.21;	author jsg;	state Exp;
branches;
next	1.8;
commitid	mcxB0JvoI9gTDYXU;

1.8
date	2014.07.09.21.08.59;	author jsg;	state Exp;
branches;
next	1.7;
commitid	WPD6rgPryPkvXOr9;

1.7
date	2013.09.05.14.04.20;	author jsg;	state Exp;
branches;
next	1.6;

1.6
date	2012.08.17.13.58.15;	author mpi;	state Exp;
branches;
next	1.5;

1.5
date	2011.10.23.13.37.39;	author matthieu;	state Exp;
branches;
next	1.4;

1.4
date	2010.05.22.20.06.18;	author matthieu;	state Exp;
branches;
next	1.3;

1.3
date	2009.05.17.20.26.39;	author matthieu;	state Exp;
branches;
next	1.2;

1.2
date	2008.11.02.14.58.15;	author matthieu;	state Exp;
branches;
next	1.1;

1.1
date	2006.11.25.18.52.45;	author matthieu;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2006.11.25.18.52.45;	author matthieu;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2011.10.23.13.29.36;	author matthieu;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2013.09.05.13.15.37;	author jsg;	state Exp;
branches;
next	1.1.1.4;

1.1.1.4
date	2014.07.09.20.34.51;	author jsg;	state Exp;
branches;
next	1.1.1.5;
commitid	3JhLfwcuBALP0ZR7;

1.1.1.5
date	2015.01.25.14.11.44;	author jsg;	state Exp;
branches;
next	1.1.1.6;
commitid	ce2W5rH5aF7VS9gi;

1.1.1.6
date	2015.02.20.22.48.49;	author jsg;	state Exp;
branches;
next	;
commitid	F54a1i0WXHMxq7kE;


desc
@@


1.11
log
@remove the now unused Mesa 10.2.9 code
@
text
@/*
 Copyright (C) Intel Corp.  2006.  All Rights Reserved.
 Intel funded Tungsten Graphics to
 develop this 3D driver.

 Permission is hereby granted, free of charge, to any person obtaining
 a copy of this software and associated documentation files (the
 "Software"), to deal in the Software without restriction, including
 without limitation the rights to use, copy, modify, merge, publish,
 distribute, sublicense, and/or sell copies of the Software, and to
 permit persons to whom the Software is furnished to do so, subject to
 the following conditions:

 The above copyright notice and this permission notice (including the
 next paragraph) shall be included in all copies or substantial
 portions of the Software.

 THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
 EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 IN NO EVENT SHALL THE COPYRIGHT OWNER(S) AND/OR ITS SUPPLIERS BE
 LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
 OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
 WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

 **********************************************************************/
 /*
  * Authors:
  *   Keith Whitwell <keithw@@vmware.com>
  */

/** @@file brw_state_cache.c
 *
 * This file implements a simple static state cache for 965.  The
 * consumers can query the hash table of state using a cache_id,
 * opaque key data, and receive the corresponding state buffer object
 * of state (plus associated auxiliary data) in return.  Objects in
 * the cache may not have relocations (pointers to other BOs) in them.
 *
 * The inner workings are a simple hash table based on a CRC of the
 * key data.
 *
 * Replacement is not implemented.  Instead, when the cache gets too
 * big we throw out all of the cache data and let it get regenerated.
 */

#include "main/imports.h"
#include "intel_batchbuffer.h"
#include "brw_state.h"
#include "brw_vs.h"
#include "brw_wm.h"
#include "brw_vs.h"
#include "brw_vec4_gs.h"

#define FILE_DEBUG_FLAG DEBUG_STATE

static GLuint
hash_key(struct brw_cache_item *item)
{
   GLuint *ikey = (GLuint *)item->key;
   GLuint hash = item->cache_id, i;

   assert(item->key_size % 4 == 0);

   /* I'm sure this can be improved on:
    */
   for (i = 0; i < item->key_size/4; i++) {
      hash ^= ikey[i];
      hash = (hash << 5) | (hash >> 27);
   }

   return hash;
}

static int
brw_cache_item_equals(const struct brw_cache_item *a,
		      const struct brw_cache_item *b)
{
   return a->cache_id == b->cache_id &&
      a->hash == b->hash &&
      a->key_size == b->key_size &&
      (memcmp(a->key, b->key, a->key_size) == 0);
}

static struct brw_cache_item *
search_cache(struct brw_cache *cache, GLuint hash,
	     struct brw_cache_item *lookup)
{
   struct brw_cache_item *c;

#if 0
   int bucketcount = 0;

   for (c = cache->items[hash % cache->size]; c; c = c->next)
      bucketcount++;

   fprintf(stderr, "bucket %d/%d = %d/%d items\n", hash % cache->size,
	   cache->size, bucketcount, cache->n_items);
#endif

   for (c = cache->items[hash % cache->size]; c; c = c->next) {
      if (brw_cache_item_equals(lookup, c))
	 return c;
   }

   return NULL;
}


static void
rehash(struct brw_cache *cache)
{
   struct brw_cache_item **items;
   struct brw_cache_item *c, *next;
   GLuint size, i;

   size = cache->size * 3;
   items = calloc(1, size * sizeof(*items));

   for (i = 0; i < cache->size; i++)
      for (c = cache->items[i]; c; c = next) {
	 next = c->next;
	 c->next = items[c->hash % size];
	 items[c->hash % size] = c;
      }

   free(cache->items);
   cache->items = items;
   cache->size = size;
}


/**
 * Returns the buffer object matching cache_id and key, or NULL.
 */
bool
brw_search_cache(struct brw_cache *cache,
                 enum brw_cache_id cache_id,
                 const void *key, GLuint key_size,
                 uint32_t *inout_offset, void *out_aux)
{
   struct brw_context *brw = cache->brw;
   struct brw_cache_item *item;
   struct brw_cache_item lookup;
   GLuint hash;

   lookup.cache_id = cache_id;
   lookup.key = key;
   lookup.key_size = key_size;
   hash = hash_key(&lookup);
   lookup.hash = hash;

   item = search_cache(cache, hash, &lookup);

   if (item == NULL)
      return false;

   *(void **)out_aux = ((char *)item->key + item->key_size);

   if (item->offset != *inout_offset) {
      brw->state.dirty.cache |= (1 << cache_id);
      *inout_offset = item->offset;
   }

   return true;
}

static void
brw_cache_new_bo(struct brw_cache *cache, uint32_t new_size)
{
   struct brw_context *brw = cache->brw;
   drm_intel_bo *new_bo;

   new_bo = drm_intel_bo_alloc(brw->bufmgr, "program cache", new_size, 64);

   /* Copy any existing data that needs to be saved. */
   if (cache->next_offset != 0) {
      drm_intel_bo_map(cache->bo, false);
      drm_intel_bo_subdata(new_bo, 0, cache->next_offset, cache->bo->virtual);
      drm_intel_bo_unmap(cache->bo);
   }

   drm_intel_bo_unreference(cache->bo);
   cache->bo = new_bo;
   cache->bo_used_by_gpu = false;

   /* Since we have a new BO in place, we need to signal the units
    * that depend on it (state base address on gen5+, or unit state before).
    */
   brw->state.dirty.brw |= BRW_NEW_PROGRAM_CACHE;
}

/**
 * Attempts to find an item in the cache with identical data and aux
 * data to use
 */
static bool
brw_try_upload_using_copy(struct brw_cache *cache,
			  struct brw_cache_item *result_item,
			  const void *data,
			  const void *aux)
{
   int i;
   struct brw_cache_item *item;

   for (i = 0; i < cache->size; i++) {
      for (item = cache->items[i]; item; item = item->next) {
	 const void *item_aux = item->key + item->key_size;
	 int ret;

	 if (item->cache_id != result_item->cache_id ||
	     item->size != result_item->size ||
	     item->aux_size != result_item->aux_size) {
	    continue;
	 }

         if (cache->aux_compare[result_item->cache_id]) {
            if (!cache->aux_compare[result_item->cache_id](item_aux, aux))
               continue;
         } else if (memcmp(item_aux, aux, item->aux_size) != 0) {
	    continue;
	 }

	 drm_intel_bo_map(cache->bo, false);
	 ret = memcmp(cache->bo->virtual + item->offset, data, item->size);
	 drm_intel_bo_unmap(cache->bo);
	 if (ret)
	    continue;

	 result_item->offset = item->offset;

	 return true;
      }
   }

   return false;
}

static void
brw_upload_item_data(struct brw_cache *cache,
		     struct brw_cache_item *item,
		     const void *data)
{
   /* Allocate space in the cache BO for our new program. */
   if (cache->next_offset + item->size > cache->bo->size) {
      uint32_t new_size = cache->bo->size * 2;

      while (cache->next_offset + item->size > new_size)
	 new_size *= 2;

      brw_cache_new_bo(cache, new_size);
   }

   /* If we would block on writing to an in-use program BO, just
    * recreate it.
    */
   if (cache->bo_used_by_gpu) {
      brw_cache_new_bo(cache, cache->bo->size);
   }

   item->offset = cache->next_offset;

   /* Programs are always 64-byte aligned, so set up the next one now */
   cache->next_offset = ALIGN(item->offset + item->size, 64);
}

void
brw_upload_cache(struct brw_cache *cache,
		 enum brw_cache_id cache_id,
		 const void *key,
		 GLuint key_size,
		 const void *data,
		 GLuint data_size,
		 const void *aux,
		 GLuint aux_size,
		 uint32_t *out_offset,
		 void *out_aux)
{
   struct brw_cache_item *item = CALLOC_STRUCT(brw_cache_item);
   GLuint hash;
   void *tmp;

   item->cache_id = cache_id;
   item->size = data_size;
   item->key = key;
   item->key_size = key_size;
   item->aux_size = aux_size;
   hash = hash_key(item);
   item->hash = hash;

   /* If we can find a matching prog/prog_data combo in the cache
    * already, then reuse the existing stuff.  This will mean not
    * flagging CACHE_NEW_* when transitioning between the two
    * equivalent hash keys.  This is notably useful for programs
    * generating shaders at runtime, where multiple shaders may
    * compile to the thing in our backend.
    */
   if (!brw_try_upload_using_copy(cache, item, data, aux)) {
      brw_upload_item_data(cache, item, data);
   }

   /* Set up the memory containing the key and aux_data */
   tmp = malloc(key_size + aux_size);

   memcpy(tmp, key, key_size);
   memcpy(tmp + key_size, aux, aux_size);

   item->key = tmp;

   if (cache->n_items > cache->size * 1.5)
      rehash(cache);

   hash %= cache->size;
   item->next = cache->items[hash];
   cache->items[hash] = item;
   cache->n_items++;

   /* Copy data to the buffer */
   drm_intel_bo_subdata(cache->bo, item->offset, data_size, data);

   *out_offset = item->offset;
   *(void **)out_aux = (void *)((char *)item->key + item->key_size);
   cache->brw->state.dirty.cache |= 1 << cache_id;
}

void
brw_init_caches(struct brw_context *brw)
{
   struct brw_cache *cache = &brw->cache;

   cache->brw = brw;

   cache->size = 7;
   cache->n_items = 0;
   cache->items =
      calloc(1, cache->size * sizeof(struct brw_cache_item *));

   cache->bo = drm_intel_bo_alloc(brw->bufmgr,
				  "program cache",
				  4096, 64);

   cache->aux_compare[BRW_VS_PROG] = brw_vs_prog_data_compare;
   cache->aux_compare[BRW_GS_PROG] = brw_gs_prog_data_compare;
   cache->aux_compare[BRW_WM_PROG] = brw_wm_prog_data_compare;
   cache->aux_free[BRW_VS_PROG] = brw_stage_prog_data_free;
   cache->aux_free[BRW_GS_PROG] = brw_stage_prog_data_free;
   cache->aux_free[BRW_WM_PROG] = brw_stage_prog_data_free;
}

static void
brw_clear_cache(struct brw_context *brw, struct brw_cache *cache)
{
   struct brw_cache_item *c, *next;
   GLuint i;

   DBG("%s\n", __FUNCTION__);

   for (i = 0; i < cache->size; i++) {
      for (c = cache->items[i]; c; c = next) {
	 next = c->next;
         if (cache->aux_free[c->cache_id]) {
            const void *item_aux = c->key + c->key_size;
            cache->aux_free[c->cache_id](item_aux);
         }
	 free((void *)c->key);
	 free(c);
      }
      cache->items[i] = NULL;
   }

   cache->n_items = 0;

   /* Start putting programs into the start of the BO again, since
    * we'll never find the old results.
    */
   cache->next_offset = 0;

   /* We need to make sure that the programs get regenerated, since
    * any offsets leftover in brw_context will no longer be valid.
    */
   brw->state.dirty.mesa |= ~0;
   brw->state.dirty.brw |= ~0;
   brw->state.dirty.cache |= ~0;
   intel_batchbuffer_flush(brw);
}

void
brw_state_cache_check_size(struct brw_context *brw)
{
   /* un-tuned guess.  Each object is generally a page, so 2000 of them is 8 MB of
    * state cache.
    */
   if (brw->cache.n_items > 2000) {
      perf_debug("Exceeded state cache size limit.  Clearing the set "
                 "of compiled programs, which will trigger recompiles\n");
      brw_clear_cache(brw, &brw->cache);
   }
}


static void
brw_destroy_cache(struct brw_context *brw, struct brw_cache *cache)
{

   DBG("%s\n", __FUNCTION__);

   drm_intel_bo_unreference(cache->bo);
   cache->bo = NULL;
   brw_clear_cache(brw, cache);
   free(cache->items);
   cache->items = NULL;
   cache->size = 0;
}


void
brw_destroy_caches(struct brw_context *brw)
{
   brw_destroy_cache(brw, &brw->cache);
}
@


1.10
log
@Merge Mesa 10.2.9
@
text
@@


1.9
log
@Merge Mesa 10.4.3
Tested by matthieu@@ mpi@@ and myself.  landry@@ ran a ports bulk build.
kettenis@@ tracked down the cause of an alignment fault on archs
that require strict eight byte pointer alignment.
@
text
@d52 2
a53 1
#include "brw_gs.h"
d118 1
a118 1
   items = calloc(size, sizeof(*items));
a174 2
   if (brw->has_llc)
      drm_intel_gem_bo_map_unsynchronized(new_bo);
d178 3
a180 8
      if (brw->has_llc) {
         memcpy(new_bo->virtual, cache->bo->virtual, cache->next_offset);
      } else {
         drm_intel_bo_map(cache->bo, false);
         drm_intel_bo_subdata(new_bo, 0, cache->next_offset,
                              cache->bo->virtual);
         drm_intel_bo_unmap(cache->bo);
      }
a182 2
   if (brw->has_llc)
      drm_intel_bo_unmap(cache->bo);
a202 1
   struct brw_context *brw = cache->brw;
d224 1
a224 2
         if (!brw->has_llc)
            drm_intel_bo_map(cache->bo, false);
d226 1
a226 2
         if (!brw->has_llc)
            drm_intel_bo_unmap(cache->bo);
a243 2
   struct brw_context *brw = cache->brw;

d257 1
a257 2
   if (!brw->has_llc && cache->bo_used_by_gpu) {
      perf_debug("Copying busy program cache buffer.\n");
a278 1
   struct brw_context *brw = cache->brw;
d319 1
a319 5
   if (brw->has_llc) {
      memcpy((char *) cache->bo->virtual + item->offset, data, data_size);
   } else {
      drm_intel_bo_subdata(cache->bo, item->offset, data_size, data);
   }
d336 1
a336 1
      calloc(cache->size, sizeof(struct brw_cache_item *));
a340 2
   if (brw->has_llc)
      drm_intel_gem_bo_map_unsynchronized(cache->bo);
d382 1
a382 1
   brw->state.dirty.brw |= ~0ull;
a406 2
   if (brw->has_llc)
      drm_intel_bo_unmap(cache->bo);
@


1.8
log
@Merge Mesa 10.2.3
tested by matthieu@@ kettenis@@ mpi@@ brett@@ and myself across a
diverse range of hardware
@
text
@d52 1
a52 2
#include "brw_vs.h"
#include "brw_vec4_gs.h"
d117 1
a117 1
   items = calloc(1, size * sizeof(*items));
d174 2
d179 8
a186 3
      drm_intel_bo_map(cache->bo, false);
      drm_intel_bo_subdata(new_bo, 0, cache->next_offset, cache->bo->virtual);
      drm_intel_bo_unmap(cache->bo);
d189 2
d211 1
d233 2
a234 1
	 drm_intel_bo_map(cache->bo, false);
d236 2
a237 1
	 drm_intel_bo_unmap(cache->bo);
d255 2
d270 2
a271 1
   if (cache->bo_used_by_gpu) {
d293 1
d334 5
a338 1
   drm_intel_bo_subdata(cache->bo, item->offset, data_size, data);
d355 1
a355 1
      calloc(1, cache->size * sizeof(struct brw_cache_item *));
d360 2
d403 1
a403 1
   brw->state.dirty.brw |= ~0;
d428 2
@


1.7
log
@Merge Mesa 9.2.0
@
text
@d3 1
a3 1
 Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to
d5 1
a5 1
 
d13 1
a13 1
 
d17 1
a17 1
 
d25 1
a25 1
 
d29 1
a29 1
  *   Keith Whitwell <keith@@tungstengraphics.com>
d53 1
d218 1
a218 3
            if (!cache->aux_compare[result_item->cache_id](item_aux, aux,
                                                           item->aux_size,
                                                           item->key))
d343 1
d345 3
a347 2
   cache->aux_free[BRW_VS_PROG] = brw_vs_prog_data_free;
   cache->aux_free[BRW_WM_PROG] = brw_wm_prog_data_free;
@


1.6
log
@Upate to libGL 7.11.2

Tested by jsg@@, matthieu@@ and ajacoutot@@, ok mattieu@@
@
text
@d50 3
d117 1
a117 1
   items = (struct brw_cache_item**) calloc(1, size * sizeof(*items));
d126 1
a126 1
   FREE(cache->items);
a170 1
   struct intel_context *intel = &brw->intel;
d173 1
a173 1
   new_bo = drm_intel_bo_alloc(intel->bufmgr, "program cache", new_size, 64);
d216 6
a221 1
	 if (memcmp(item_aux, aux, item->aux_size) != 0) {
a329 1
   struct intel_context *intel = &brw->intel;
d336 2
a337 2
   cache->items = (struct brw_cache_item **)
      calloc(1, cache->size * sizeof(struct brw_cache_item));
d339 1
a339 1
   cache->bo = drm_intel_bo_alloc(intel->bufmgr,
d342 5
a351 1
   struct intel_context *intel = &brw->intel;
d360 4
d383 1
a383 1
   intel_batchbuffer_flush(intel);
d389 1
a389 1
   /* un-tuned guess.  Each object is generally a page, so 1000 of them is 4 MB of
d392 3
a394 1
   if (brw->cache.n_items > 1000)
d396 1
d406 2
@


1.5
log
@Merge Mesa 7.10.3
@
text
@d34 5
a38 5
 * This file implements a simple static state cache for 965.  The consumers
 * can query the hash table of state using a cache_id, opaque key data,
 * and list of buffers that will be used in relocations, and receive the
 * corresponding state buffer object of state (plus associated auxiliary
 * data) in return.
d40 2
a41 5
 * The inner workings are a simple hash table based on a CRC of the key data.
 * The cache_id and relocation target buffers associated with the state
 * buffer are included as auxiliary key data, but are not part of the hash
 * value (this should be fixed, but will likely be fixed instead by making
 * consumers use structured keys).
d43 2
a44 11
 * Replacement is not implemented.  Instead, when the cache gets too big, at
 * a safe point (unlock) we throw out all of the cache data and let it
 * regenerate for the next rendering operation.
 *
 * The reloc_buf pointers need to be included as key data, otherwise the
 * non-unique values stuffed in the offset in key data through
 * brw_cache_data() may result in successful probe for state buffers
 * even when the buffer being referenced doesn't match.  The result would be
 * that the same state cache entry is used twice for different buffers,
 * only one of the two buffers referenced gets put into the offset, and the
 * incorrect program is run for the other instance.
d48 1
a49 2
#include "intel_batchbuffer.h"
#include "brw_wm.h"
a67 7
   /* Include the BO pointers as key data as well */
   ikey = (GLuint *)item->reloc_bufs;
   for (i = 0; i < item->nr_reloc_bufs * sizeof(drm_intel_bo *) / 4; i++) {
      hash ^= ikey[i];
      hash = (hash << 5) | (hash >> 27);
   }

a70 17

/**
 * Marks a new buffer as being chosen for the given cache id.
 */
static void
update_cache_last(struct brw_cache *cache, enum brw_cache_id cache_id,
		  drm_intel_bo *bo)
{
   if (bo == cache->last_bo[cache_id])
      return; /* no change */

   drm_intel_bo_unreference(cache->last_bo[cache_id]);
   cache->last_bo[cache_id] = bo;
   drm_intel_bo_reference(cache->last_bo[cache_id]);
   cache->brw->state.dirty.cache |= 1 << cache_id;
}

d78 1
a78 4
      (memcmp(a->key, b->key, a->key_size) == 0) &&
      a->nr_reloc_bufs == b->nr_reloc_bufs &&
      (memcmp(a->reloc_bufs, b->reloc_bufs,
	      a->nr_reloc_bufs * sizeof(drm_intel_bo *)) == 0);
d132 1
a132 1
drm_intel_bo *
d135 2
a136 4
                 const void *key,
                 GLuint key_size,
                 drm_intel_bo **reloc_bufs, GLuint nr_reloc_bufs,
                 void *aux_return)
d138 1
a145 2
   lookup.reloc_bufs = reloc_bufs;
   lookup.nr_reloc_bufs = nr_reloc_bufs;
d152 1
a152 1
      return NULL;
d154 1
a154 2
   if (aux_return)
      *(void **)aux_return = (void *)((char *)item->key + item->key_size);
d156 4
a159 1
   update_cache_last(cache, cache_id, item->bo);
d161 1
a161 2
   drm_intel_bo_reference(item->bo);
   return item->bo;
d164 19
d184 15
a198 12
drm_intel_bo *
brw_upload_cache_with_auxdata(struct brw_cache *cache,
			      enum brw_cache_id cache_id,
			      const void *key,
			      GLuint key_size,
			      drm_intel_bo **reloc_bufs,
			      GLuint nr_reloc_bufs,
			      const void *data,
			      GLuint data_size,
			      const void *aux,
			      GLuint aux_size,
			      void *aux_return)
a199 5
   struct brw_cache_item *item = CALLOC_STRUCT(brw_cache_item);
   GLuint hash;
   GLuint relocs_size = nr_reloc_bufs * sizeof(drm_intel_bo *);
   void *tmp;
   drm_intel_bo *bo;
d201 1
d203 20
a222 7
   item->cache_id = cache_id;
   item->key = key;
   item->key_size = key_size;
   item->reloc_bufs = reloc_bufs;
   item->nr_reloc_bufs = nr_reloc_bufs;
   hash = hash_key(item);
   item->hash = hash;
d224 1
a224 3
   /* Create the buffer object to contain the data */
   bo = drm_intel_bo_alloc(cache->brw->intel.bufmgr,
			   cache->name[cache_id], data_size, 1 << 6);
d226 2
a227 10

   /* Set up the memory containing the key, aux_data, and reloc_bufs */
   tmp = malloc(key_size + aux_size + relocs_size);

   memcpy(tmp, key, key_size);
   memcpy(tmp + key_size, aux, aux_size);
   memcpy(tmp + key_size + aux_size, reloc_bufs, relocs_size);
   for (i = 0; i < nr_reloc_bufs; i++) {
      if (reloc_bufs[i] != NULL)
	 drm_intel_bo_reference(reloc_bufs[i]);
d230 2
a231 2
   item->key = tmp;
   item->reloc_bufs = tmp + key_size + aux_size;
d233 8
a240 2
   item->bo = bo;
   drm_intel_bo_reference(bo);
d242 2
a243 2
   if (cache->n_items > cache->size * 1.5)
      rehash(cache);
d245 2
a246 4
   hash %= cache->size;
   item->next = cache->items[hash];
   cache->items[hash] = item;
   cache->n_items++;
d248 5
a252 2
   if (aux_return) {
      *(void **)aux_return = (void *)((char *)item->key + item->key_size);
d255 1
a255 3
   DBG("upload %s: %d bytes to cache id %d\n",
       cache->name[cache_id],
       data_size, cache_id);
d257 2
a258 6
   /* Copy data to the buffer */
   drm_intel_bo_subdata(bo, 0, data_size, data);

   update_cache_last(cache, cache_id, bo);

   return bo;
d261 1
a261 1
drm_intel_bo *
a265 2
		 drm_intel_bo **reloc_bufs,
		 GLuint nr_reloc_bufs,
d267 5
a271 1
		 GLuint data_size)
d273 11
a283 7
   return brw_upload_cache_with_auxdata(cache, cache_id,
					key, key_size,
					reloc_bufs, nr_reloc_bufs,
					data, data_size,
					NULL, 0,
					NULL);
}
d285 10
a294 8
/**
 * Wrapper around brw_cache_data_sz using the cache_id's canonical key size.
 *
 * If nr_reloc_bufs is nonzero, brw_search_cache()/brw_upload_cache() would be
 * better to use, as the potentially changing offsets in the data-used-as-key
 * will result in excessive cache misses.
 *
 * If aux data is involved, use search/upload instead.
d296 2
a297 10
 */
drm_intel_bo *
brw_cache_data(struct brw_cache *cache,
	       enum brw_cache_id cache_id,
	       const void *data,
	       GLuint data_size)
{
   drm_intel_bo *bo;
   struct brw_cache_item *item, lookup;
   GLuint hash;
d299 2
a300 7
   lookup.cache_id = cache_id;
   lookup.key = data;
   lookup.key_size = data_size;
   lookup.reloc_bufs = NULL;
   lookup.nr_reloc_bufs = 0;
   hash = hash_key(&lookup);
   lookup.hash = hash;
d302 1
a302 6
   item = search_cache(cache, hash, &lookup);
   if (item) {
      update_cache_last(cache, cache_id, item->bo);
      drm_intel_bo_reference(item->bo);
      return item->bo;
   }
d304 2
a305 7
   bo = brw_upload_cache(cache, cache_id,
			 data, data_size,
			 NULL, 0,
			 data, data_size);

   return bo;
}
d307 4
a310 4
enum pool_type {
   DW_SURFACE_STATE,
   DW_GENERAL_STATE
};
d312 2
d315 3
a317 6
static void
brw_init_cache_id(struct brw_cache *cache,
                  const char *name,
                  enum brw_cache_id id)
{
   cache->name[id] = strdup(name);
d320 2
a321 3

static void
brw_init_non_surface_cache(struct brw_context *brw)
d323 1
d333 3
a335 26
   brw_init_cache_id(cache, "CC_VP", BRW_CC_VP);
   brw_init_cache_id(cache, "CC_UNIT", BRW_CC_UNIT);
   brw_init_cache_id(cache, "WM_PROG", BRW_WM_PROG);
   brw_init_cache_id(cache, "SAMPLER_DEFAULT_COLOR", BRW_SAMPLER_DEFAULT_COLOR);
   brw_init_cache_id(cache, "SAMPLER", BRW_SAMPLER);
   brw_init_cache_id(cache, "WM_UNIT", BRW_WM_UNIT);
   brw_init_cache_id(cache, "SF_PROG", BRW_SF_PROG);
   brw_init_cache_id(cache, "SF_VP", BRW_SF_VP);

   brw_init_cache_id(cache, "SF_UNIT", BRW_SF_UNIT);

   brw_init_cache_id(cache, "VS_UNIT", BRW_VS_UNIT);

   brw_init_cache_id(cache, "VS_PROG", BRW_VS_PROG);

   brw_init_cache_id(cache, "CLIP_UNIT", BRW_CLIP_UNIT);

   brw_init_cache_id(cache, "CLIP_PROG", BRW_CLIP_PROG);
   brw_init_cache_id(cache, "CLIP_VP", BRW_CLIP_VP);

   brw_init_cache_id(cache, "GS_UNIT", BRW_GS_UNIT);

   brw_init_cache_id(cache, "GS_PROG", BRW_GS_PROG);
   brw_init_cache_id(cache, "BLEND_STATE", BRW_BLEND_STATE);
   brw_init_cache_id(cache, "COLOR_CALC_STATE", BRW_COLOR_CALC_STATE);
   brw_init_cache_id(cache, "DEPTH_STENCIL_STATE", BRW_DEPTH_STENCIL_STATE);
a337 7
void
brw_init_caches(struct brw_context *brw)
{
   brw_init_non_surface_cache(brw);
}


d341 1
a348 2
	 int j;

a349 3
	 for (j = 0; j < c->nr_reloc_bufs; j++)
	    drm_intel_bo_unreference(c->reloc_bufs[j]);
	 drm_intel_bo_unreference(c->bo);
d358 8
d369 1
a374 2
   DBG("%s (n_items=%d)\n", __FUNCTION__, brw->cache.n_items);

a385 1
   GLuint i;
a389 4
   for (i = 0; i < BRW_MAX_CACHE; i++) {
      drm_intel_bo_unreference(cache->last_bo[i]);
      free(cache->name[i]);
   }
@


1.4
log
@Update to Mesa 7.8.1. Tested on a bulk ports build by naddy@@, ok oga@@.
@
text
@d64 1
d97 1
a97 1
		  dri_bo *bo)
d102 1
a102 1
   dri_bo_unreference(cache->last_bo[cache_id]);
d104 1
a104 1
   dri_bo_reference(cache->last_bo[cache_id]);
d118 1
a118 1
	      a->nr_reloc_bufs * sizeof(dri_bo *)) == 0);
d172 1
a172 1
dri_bo *
d177 1
a177 1
                 dri_bo **reloc_bufs, GLuint nr_reloc_bufs,
d202 1
a202 1
   dri_bo_reference(item->bo);
d212 1
a212 1
			      dri_bo **reloc_bufs,
d222 1
a222 1
   GLuint relocs_size = nr_reloc_bufs * sizeof(dri_bo *);
d224 1
a224 1
   dri_bo *bo;
d236 2
a237 2
   bo = dri_bo_alloc(cache->brw->intel.bufmgr,
		     cache->name[cache_id], data_size, 1 << 6);
d248 1
a248 1
	 dri_bo_reference(reloc_bufs[i]);
d255 1
a255 1
   dri_bo_reference(bo);
d269 3
a271 4
   if (INTEL_DEBUG & DEBUG_STATE)
      printf("upload %s: %d bytes to cache id %d\n",
		   cache->name[cache_id],
		   data_size, cache_id);
d274 1
a274 1
   dri_bo_subdata(bo, 0, data_size, data);
d286 1
a286 1
		 dri_bo **reloc_bufs,
d309 1
a309 1
dri_bo *
d313 1
a313 3
	       GLuint data_size,
	       dri_bo **reloc_bufs,
	       GLuint nr_reloc_bufs)
d315 1
a315 1
   dri_bo *bo;
d322 2
a323 2
   lookup.reloc_bufs = reloc_bufs;
   lookup.nr_reloc_bufs = nr_reloc_bufs;
d330 1
a330 1
      dri_bo_reference(item->bo);
d336 1
a336 1
			 reloc_bufs, nr_reloc_bufs,
d387 1
d393 2
a396 18

static void
brw_init_surface_cache(struct brw_context *brw)
{
   struct brw_cache *cache = &brw->surface_cache;

   cache->brw = brw;

   cache->size = 7;
   cache->n_items = 0;
   cache->items = (struct brw_cache_item **)
      calloc(1, cache->size * sizeof(struct brw_cache_item));

   brw_init_cache_id(cache, "SS_SURFACE", BRW_SS_SURFACE);
   brw_init_cache_id(cache, "SS_SURF_BIND", BRW_SS_SURF_BIND);
}


a400 1
   brw_init_surface_cache(brw);
d410 1
a410 2
   if (INTEL_DEBUG & DEBUG_STATE)
      printf("%s\n", __FUNCTION__);
d418 2
a419 2
	    dri_bo_unreference(c->reloc_bufs[j]);
	 dri_bo_unreference(c->bo);
a427 5
   if (brw->curbe.last_buf) {
      free(brw->curbe.last_buf);
      brw->curbe.last_buf = NULL;
   }

a432 36
/* Clear all entries from the cache that point to the given bo.
 *
 * This lets us release memory for reuse earlier for known-dead buffers,
 * at the cost of walking the entire hash table.
 */
void
brw_state_cache_bo_delete(struct brw_cache *cache, dri_bo *bo)
{
   struct brw_cache_item **prev;
   GLuint i;

   if (INTEL_DEBUG & DEBUG_STATE)
      printf("%s\n", __FUNCTION__);

   for (i = 0; i < cache->size; i++) {
      for (prev = &cache->items[i]; *prev;) {
	 struct brw_cache_item *c = *prev;

	 if (drm_intel_bo_references(c->bo, bo)) {
	    int j;

	    *prev = c->next;

	    for (j = 0; j < c->nr_reloc_bufs; j++)
	       dri_bo_unreference(c->reloc_bufs[j]);
	    dri_bo_unreference(c->bo);
	    free((void *)c->key);
	    free(c);
	    cache->n_items--;
	 } else {
	    prev = &c->next;
	 }
      }
   }
}

d436 1
a436 2
   if (INTEL_DEBUG & DEBUG_STATE)
      printf("%s (n_items=%d)\n", __FUNCTION__, brw->cache.n_items);
d438 2
a439 2
   /* un-tuned guess.  We've got around 20 state objects for a total of around
    * 32k, so 1000 of them is around 1.5MB.
a442 3

   if (brw->surface_cache.n_items > 1000)
      brw_clear_cache(brw, &brw->surface_cache);
d451 1
a451 2
   if (INTEL_DEBUG & DEBUG_STATE)
      printf("%s\n", __FUNCTION__);
d455 1
a455 1
      dri_bo_unreference(cache->last_bo[i]);
a467 1
   brw_destroy_cache(brw, &brw->surface_cache);
@


1.3
log
@Update to Mesa 7.4.2. Tested by oga@@, ckuethe@@ and naddy@@.
@
text
@d59 1
d62 1
a62 1
#include "main/imports.h"
a63 8
/* XXX: Fixme - have to include these to get the sizes of the prog_key
 * structs:
 */
#include "brw_wm.h"
#include "brw_vs.h"
#include "brw_clip.h"
#include "brw_sf.h"
#include "brw_gs.h"
d65 2
a66 2
static GLuint hash_key( const void *key, GLuint key_size,
			dri_bo **reloc_bufs, GLuint nr_reloc_bufs)
d68 2
a69 2
   GLuint *ikey = (GLuint *)key;
   GLuint hash = 0, i;
d71 1
a71 1
   assert(key_size % 4 == 0);
d75 1
a75 1
   for (i = 0; i < key_size/4; i++) {
d81 2
a82 3
   ikey = (GLuint *)reloc_bufs;
   key_size = nr_reloc_bufs * sizeof(dri_bo *);
   for (i = 0; i < key_size/4; i++) {
d90 1
d107 13
d121 2
a122 3
search_cache(struct brw_cache *cache, enum brw_cache_id cache_id,
	     GLuint hash, const void *key, GLuint key_size,
	     dri_bo **reloc_bufs, GLuint nr_reloc_bufs)
d137 1
a137 7
      if (c->cache_id == cache_id &&
	  c->hash == hash &&
	  c->key_size == key_size &&
	  memcmp(c->key, key, key_size) == 0 &&
	  c->nr_reloc_bufs == nr_reloc_bufs &&
	  memcmp(c->reloc_bufs, reloc_bufs,
		 nr_reloc_bufs * sizeof(dri_bo *)) == 0)
d145 2
a146 1
static void rehash( struct brw_cache *cache )
d153 1
a153 1
   items = (struct brw_cache_item**) _mesa_calloc(size * sizeof(*items));
d167 1
d171 7
a177 6
dri_bo *brw_search_cache( struct brw_cache *cache,
			  enum brw_cache_id cache_id,
			  const void *key,
			  GLuint key_size,
			  dri_bo **reloc_bufs, GLuint nr_reloc_bufs,
			  void *aux_return )
d180 2
a181 1
   GLuint hash = hash_key(key, key_size, reloc_bufs, nr_reloc_bufs);
d183 9
a191 2
   item = search_cache(cache, cache_id, hash, key, key_size,
		       reloc_bufs, nr_reloc_bufs);
d205 13
a217 11
dri_bo *
brw_upload_cache( struct brw_cache *cache,
		  enum brw_cache_id cache_id,
		  const void *key,
		  GLuint key_size,
		  dri_bo **reloc_bufs,
		  GLuint nr_reloc_bufs,
		  const void *data,
		  GLuint data_size,
		  const void *aux,
		  void *aux_return )
d220 1
a220 1
   GLuint hash = hash_key(key, key_size, reloc_bufs, nr_reloc_bufs);
a221 1
   GLuint aux_size = cache->aux_size[cache_id];
d226 8
d240 1
a240 1
   tmp = _mesa_malloc(key_size + aux_size + relocs_size);
d243 1
a243 1
   memcpy(tmp + key_size, aux, cache->aux_size[cache_id]);
a249 1
   item->cache_id = cache_id;
a250 2
   item->hash = hash;
   item->key_size = key_size;
a251 1
   item->nr_reloc_bufs = nr_reloc_bufs;
a254 1
   item->data_size = data_size;
a264 1
      assert(cache->aux_size[cache_id]);
d269 1
a269 1
      _mesa_printf("upload %s: %d bytes to cache id %d\n",
d281 27
a307 1
/* This doesn't really work with aux data.  Use search/upload instead
d310 6
a315 6
brw_cache_data_sz(struct brw_cache *cache,
		  enum brw_cache_id cache_id,
		  const void *data,
		  GLuint data_size,
		  dri_bo **reloc_bufs,
		  GLuint nr_reloc_bufs)
d318 10
a327 2
   struct brw_cache_item *item;
   GLuint hash = hash_key(data, data_size, reloc_bufs, nr_reloc_bufs);
d329 1
a329 2
   item = search_cache(cache, cache_id, hash, data, data_size,
		       reloc_bufs, nr_reloc_bufs);
d339 1
a339 2
			 data, data_size,
			 NULL, NULL);
a343 18
/**
 * Wrapper around brw_cache_data_sz using the cache_id's canonical key size.
 *
 * If nr_reloc_bufs is nonzero, brw_search_cache()/brw_upload_cache() would be
 * better to use, as the potentially changing offsets in the data-used-as-key
 * will result in excessive cache misses.
 */
dri_bo *
brw_cache_data(struct brw_cache *cache,
	       enum brw_cache_id cache_id,
	       const void *data,
	       dri_bo **reloc_bufs,
	       GLuint nr_reloc_bufs)
{
   return brw_cache_data_sz(cache, cache_id, data, cache->key_size[cache_id],
			    reloc_bufs, nr_reloc_bufs);
}

d349 1
d351 3
a353 5
brw_init_cache_id( struct brw_context *brw,
		const char *name,
		enum brw_cache_id id,
		GLuint key_size,
		GLuint aux_size)
a354 2
   struct brw_cache *cache = &brw->cache;

a355 2
   cache->key_size[id] = key_size;
   cache->aux_size[id] = aux_size;
d358 3
a360 1
void brw_init_cache( struct brw_context *brw )
d369 22
a390 2
      _mesa_calloc(cache->size * 
		   sizeof(struct brw_cache_item));
d392 2
a393 101
   brw_init_cache_id(brw,
		     "CC_VP",
		     BRW_CC_VP,
		     sizeof(struct brw_cc_viewport),
		     0);

   brw_init_cache_id(brw,
		     "CC_UNIT",
		     BRW_CC_UNIT,
		     sizeof(struct brw_cc_unit_state),
		     0);

   brw_init_cache_id(brw,
		     "WM_PROG",
		     BRW_WM_PROG,
		     sizeof(struct brw_wm_prog_key),
		     sizeof(struct brw_wm_prog_data));

   brw_init_cache_id(brw,
		     "SAMPLER_DEFAULT_COLOR",
		     BRW_SAMPLER_DEFAULT_COLOR,
		     sizeof(struct brw_sampler_default_color),
		     0);

   brw_init_cache_id(brw,
		     "SAMPLER",
		     BRW_SAMPLER,
		     0,		/* variable key/data size */
		     0);

   brw_init_cache_id(brw,
		     "WM_UNIT",
		     BRW_WM_UNIT,
		     sizeof(struct brw_wm_unit_state),
		     0);

   brw_init_cache_id(brw,
		     "SF_PROG",
		     BRW_SF_PROG,
		     sizeof(struct brw_sf_prog_key),
		     sizeof(struct brw_sf_prog_data));

   brw_init_cache_id(brw,
		     "SF_VP",
		     BRW_SF_VP,
		     sizeof(struct brw_sf_viewport),
		     0);

   brw_init_cache_id(brw,
		     "SF_UNIT",
		     BRW_SF_UNIT,
		     sizeof(struct brw_sf_unit_state),
		     0);

   brw_init_cache_id(brw,
		     "VS_UNIT",
		     BRW_VS_UNIT,
		     sizeof(struct brw_vs_unit_state),
		     0);

   brw_init_cache_id(brw,
		     "VS_PROG",
		     BRW_VS_PROG,
		     sizeof(struct brw_vs_prog_key),
		     sizeof(struct brw_vs_prog_data));

   brw_init_cache_id(brw,
		     "CLIP_UNIT",
		     BRW_CLIP_UNIT,
		     sizeof(struct brw_clip_unit_state),
		     0);

   brw_init_cache_id(brw,
		     "CLIP_PROG",
		     BRW_CLIP_PROG,
		     sizeof(struct brw_clip_prog_key),
		     sizeof(struct brw_clip_prog_data));

   brw_init_cache_id(brw,
		     "GS_UNIT",
		     BRW_GS_UNIT,
		     sizeof(struct brw_gs_unit_state),
		     0);

   brw_init_cache_id(brw,
		     "GS_PROG",
		     BRW_GS_PROG,
		     sizeof(struct brw_gs_prog_key),
		     sizeof(struct brw_gs_prog_data));

   brw_init_cache_id(brw,
		     "SS_SURFACE",
		     BRW_SS_SURFACE,
		     sizeof(struct brw_surface_state),
		     0);

   brw_init_cache_id(brw,
		     "SS_SURF_BIND",
		     BRW_SS_SURF_BIND,
		     0,
		     0);
d396 1
d398 26
a423 1
brw_clear_cache( struct brw_context *brw )
d429 1
a429 1
      _mesa_printf("%s\n", __FUNCTION__);
d431 2
a432 2
   for (i = 0; i < brw->cache.size; i++) {
      for (c = brw->cache.items[i]; c; c = next) {
d442 1
a442 1
      brw->cache.items[i] = NULL;
d445 1
a445 1
   brw->cache.n_items = 0;
d448 1
a448 1
      _mesa_free(brw->curbe.last_buf);
d457 7
a463 1
void brw_state_cache_check_size( struct brw_context *brw )
d465 34
d503 4
a506 1
      brw_clear_cache(brw);
d509 3
a511 1
void brw_destroy_cache( struct brw_context *brw )
d515 4
a518 1
   brw_clear_cache(brw);
d520 2
a521 2
      dri_bo_unreference(brw->cache.last_bo[i]);
      free(brw->cache.name[i]);
d523 11
a533 3
   free(brw->cache.items);
   brw->cache.items = NULL;
   brw->cache.size = 0;
@


1.2
log
@Mesa 7.2, Tested by ckuethe@@, naddy@@, oga@@, and others.
@
text
@d61 1
a61 1
#include "imports.h"
d217 1
a217 4
		     cache->name[cache_id], data_size, 1 << 6,
		     DRM_BO_FLAG_MEM_LOCAL |
		     DRM_BO_FLAG_CACHED |
		     DRM_BO_FLAG_CACHED_MAPPED);
d500 2
a501 1
   for (i = 0; i < BRW_MAX_CACHE; i++)
d503 1
a503 1

@


1.1
log
@Initial revision
@
text
@d31 27
a57 1
      
a59 1
#include "brw_aub.h"
d72 2
a73 10

/***********************************************************************
 * Check cache for uploaded version of struct, else upload new one.
 * Fail when memory is exhausted.
 *
 * XXX: FIXME: Currently search is so slow it would be quicker to
 * regenerate the data every time...
 */

static GLuint hash_key( const void *key, GLuint key_size )
d82 1
a82 1
   for (i = 0; i < key_size/4; i++)
d84 10
d98 20
a117 4
static struct brw_cache_item *search_cache( struct brw_cache *cache,
					     GLuint hash,
					     const void *key,
					     GLuint key_size)
d121 10
d132 2
a133 1
      if (c->hash == hash && 
d135 4
a138 1
	  memcmp(c->key, key, key_size) == 0)
d153 1
a153 2
   items = (struct brw_cache_item**) _mesa_malloc(size * sizeof(*items));
   _mesa_memset(items, 0, size * sizeof(*items));
d167 9
a175 6

GLboolean brw_search_cache( struct brw_cache *cache,
			    const void *key,
			    GLuint key_size,
			    void *aux_return,
			    GLuint *offset_return)
d178 10
a187 2
   GLuint addr = 0;
   GLuint hash = hash_key(key, key_size);
d189 1
a189 1
   item = search_cache(cache, hash, key, key_size);
d191 2
a192 13
   if (item) {
      if (aux_return) 
	 *(void **)aux_return = (void *)((char *)item->key + item->key_size);
      
      *offset_return = addr = item->offset;
   }    
    
   if (item == NULL || addr != cache->last_addr) {
      cache->brw->state.dirty.cache |= 1<<cache->id;
      cache->last_addr = addr;
   }
   
   return item != NULL;
d195 12
a206 9
GLuint brw_upload_cache( struct brw_cache *cache,
			 const void *key,
			 GLuint key_size,
			 const void *data,
			 GLuint data_size,
			 const void *aux,
			 void *aux_return )
{   
   GLuint offset;
d208 17
a224 9
   GLuint hash = hash_key(key, key_size);
   void *tmp = _mesa_malloc(key_size + cache->aux_size);
   
   if (!brw_pool_alloc(cache->pool, data_size, 6, &offset)) {
      /* Should not be possible: 
       */
      _mesa_printf("brw_pool_alloc failed\n");
      exit(1);
   }
d227 6
d234 1
a234 3
   if (cache->aux_size)
      memcpy(tmp+key_size, aux, cache->aux_size);
	 
d238 5
a242 1
   item->offset = offset;
d245 1
a245 1
   if (++cache->n_items > cache->size * 1.5)
d247 1
a247 1
   
d251 2
a252 1
      
d254 1
a254 1
      assert(cache->aux_size);
d259 3
a261 5
      _mesa_printf("upload %s: %d bytes to pool buffer %d offset %x\n",
		   cache->name,
		   data_size, 
		   cache->pool->buffer,
		   offset);
d263 2
a264 10
   /* Copy data to the buffer:
    */
   bmBufferSubDataAUB(&cache->brw->intel,
		      cache->pool->buffer,
		      offset, 
		      data_size, 
		      data,
		      cache->aub_type,
		      cache->aub_sub_type);
   
d266 1
a266 2
   cache->brw->state.dirty.cache |= 1<<cache->id;
   cache->last_addr = offset;
d268 1
a268 1
   return offset;
d273 18
a290 11
GLuint brw_cache_data_sz(struct brw_cache *cache,
			 const void *data,
			 GLuint data_size)
{
   GLuint addr;

   if (!brw_search_cache(cache, data, data_size, NULL, &addr)) {
      addr = brw_upload_cache(cache, 
			      data, data_size, 
			      data, data_size, 
			      NULL, NULL);
d293 7
a299 1
   return addr;
d302 13
a314 2
GLuint brw_cache_data(struct brw_cache *cache,
		      const void *data)
d316 2
a317 1
   return brw_cache_data_sz(cache, data, cache->key_size);
d320 4
d325 8
d334 4
d339 3
a342 9
static void brw_init_cache( struct brw_context *brw, 
			    const char *name,
			    GLuint id,
			    GLuint key_size,
			    GLuint aux_size,
			    GLuint aub_type,
			    GLuint aub_sub_type )
{
   struct brw_cache *cache = &brw->cache[id];
a343 3
   cache->id = id;
   cache->name = name;
   cache->items = NULL;
d351 101
a451 10

   cache->key_size = key_size;
   cache->aux_size = aux_size;
   cache->aub_type = aub_type;
   cache->aub_sub_type = aub_sub_type;
   switch (aub_type) {
   case DW_GENERAL_STATE: cache->pool = &brw->pool[BRW_GS_POOL]; break;
   case DW_SURFACE_STATE: cache->pool = &brw->pool[BRW_SS_POOL]; break;
   default: assert(0); break;
   }
d454 2
a455 157
void brw_init_caches( struct brw_context *brw )
{

   brw_init_cache(brw,
		  "CC_VP",
		  BRW_CC_VP,
		  sizeof(struct brw_cc_viewport),
		  0,
		  DW_GENERAL_STATE,
		  DWGS_COLOR_CALC_VIEWPORT_STATE);

   brw_init_cache(brw,
		  "CC_UNIT",
		  BRW_CC_UNIT,
		  sizeof(struct brw_cc_unit_state),
		  0,
		  DW_GENERAL_STATE,
		  DWGS_COLOR_CALC_STATE);

   brw_init_cache(brw,
		  "WM_PROG",
		  BRW_WM_PROG,
		  sizeof(struct brw_wm_prog_key),
		  sizeof(struct brw_wm_prog_data),
		  DW_GENERAL_STATE,
		  DWGS_KERNEL_INSTRUCTIONS);

   brw_init_cache(brw,
		  "SAMPLER_DEFAULT_COLOR",
		  BRW_SAMPLER_DEFAULT_COLOR,
		  sizeof(struct brw_sampler_default_color),
		  0,
		  DW_GENERAL_STATE,
		  DWGS_SAMPLER_DEFAULT_COLOR);

   brw_init_cache(brw,
		  "SAMPLER",
		  BRW_SAMPLER,
		  0,		/* variable key/data size */
		  0,
		  DW_GENERAL_STATE,
		  DWGS_SAMPLER_STATE);

   brw_init_cache(brw,
		  "WM_UNIT",
		  BRW_WM_UNIT,
		  sizeof(struct brw_wm_unit_state),
		  0,
		  DW_GENERAL_STATE,
		  DWGS_WINDOWER_IZ_STATE);

   brw_init_cache(brw,
		  "SF_PROG",
		  BRW_SF_PROG,
		  sizeof(struct brw_sf_prog_key),
		  sizeof(struct brw_sf_prog_data),
		  DW_GENERAL_STATE,
		  DWGS_KERNEL_INSTRUCTIONS);

   brw_init_cache(brw,
		  "SF_VP",
		  BRW_SF_VP,
		  sizeof(struct brw_sf_viewport),
		  0,
		  DW_GENERAL_STATE,
		  DWGS_STRIPS_FANS_VIEWPORT_STATE);

   brw_init_cache(brw,
		  "SF_UNIT",
		  BRW_SF_UNIT,
		  sizeof(struct brw_sf_unit_state),
		  0,
		  DW_GENERAL_STATE,
		  DWGS_STRIPS_FANS_STATE);

   brw_init_cache(brw,
		  "VS_UNIT",
		  BRW_VS_UNIT,
		  sizeof(struct brw_vs_unit_state),
		  0,
		  DW_GENERAL_STATE,
		  DWGS_VERTEX_SHADER_STATE);

   brw_init_cache(brw,
		  "VS_PROG",
		  BRW_VS_PROG,
		  sizeof(struct brw_vs_prog_key),
		  sizeof(struct brw_vs_prog_data),
		  DW_GENERAL_STATE,
		  DWGS_KERNEL_INSTRUCTIONS);

   brw_init_cache(brw,
		  "CLIP_UNIT",
		  BRW_CLIP_UNIT,
		  sizeof(struct brw_clip_unit_state),
		  0,
		  DW_GENERAL_STATE,
		  DWGS_CLIPPER_STATE);

   brw_init_cache(brw,
		  "CLIP_PROG",
		  BRW_CLIP_PROG,
		  sizeof(struct brw_clip_prog_key),
		  sizeof(struct brw_clip_prog_data),
		  DW_GENERAL_STATE,
		  DWGS_KERNEL_INSTRUCTIONS);

   brw_init_cache(brw,
		  "GS_UNIT",
		  BRW_GS_UNIT,
		  sizeof(struct brw_gs_unit_state),
		  0,
		  DW_GENERAL_STATE,
		  DWGS_GEOMETRY_SHADER_STATE);

   brw_init_cache(brw,
		  "GS_PROG",
		  BRW_GS_PROG,
		  sizeof(struct brw_gs_prog_key),
		  sizeof(struct brw_gs_prog_data),
		  DW_GENERAL_STATE,
		  DWGS_KERNEL_INSTRUCTIONS);

   brw_init_cache(brw,
		  "SS_SURFACE",
		  BRW_SS_SURFACE,
		  sizeof(struct brw_surface_state),
		  0,
		  DW_SURFACE_STATE,
		  DWSS_SURFACE_STATE);

   brw_init_cache(brw,
		  "SS_SURF_BIND",
		  BRW_SS_SURF_BIND,
		  sizeof(struct brw_surface_binding_table),
		  0,
		  DW_SURFACE_STATE,
		  DWSS_BINDING_TABLE_STATE);
}


/* When we lose hardware context, need to invalidate the surface cache
 * as these structs must be explicitly re-uploaded.  They are subject
 * to fixup by the memory manager as they contain absolute agp
 * offsets, so we need to ensure there is a fresh version of the
 * struct available to receive the fixup.
 *
 * XXX: Need to ensure that there aren't two versions of a surface or
 * bufferobj with different backing data active in the same buffer at
 * once?  Otherwise the cache could confuse them.  Maybe better not to
 * cache at all?
 * 
 * --> Isn't this the same as saying need to ensure batch is flushed
 *         before new data is uploaded to an existing buffer?  We
 *         already try to make sure of that.
 */
static void clear_cache( struct brw_cache *cache )
d460 7
a466 2
   for (i = 0; i < cache->size; i++) {
      for (c = cache->items[i]; c; c = next) {
d468 3
d474 1
a474 1
      cache->items[i] = NULL;
d477 1
a477 12
   cache->n_items = 0;
}

void brw_clear_all_caches( struct brw_context *brw )
{
   GLint i;

   if (INTEL_DEBUG & DEBUG_STATE)
      _mesa_printf("%s\n", __FUNCTION__);

   for (i = 0; i < BRW_MAX_CACHE; i++)
      clear_cache(&brw->cache[i]);      
d489 8
d498 1
a498 4



void brw_destroy_caches( struct brw_context *brw )
d502 1
d504 5
a508 1
      clear_cache(&brw->cache[i]);      
@


1.1.1.1
log
@Import MesaLibs 6.5.1. (in dist/ since its code is shared between lib 
and xserver)...
@
text
@@


1.1.1.2
log
@Import Mesa 7.10.3
@
text
@d31 1
a32 28
/** @@file brw_state_cache.c
 *
 * This file implements a simple static state cache for 965.  The consumers
 * can query the hash table of state using a cache_id, opaque key data,
 * and list of buffers that will be used in relocations, and receive the
 * corresponding state buffer object of state (plus associated auxiliary
 * data) in return.
 *
 * The inner workings are a simple hash table based on a CRC of the key data.
 * The cache_id and relocation target buffers associated with the state
 * buffer are included as auxiliary key data, but are not part of the hash
 * value (this should be fixed, but will likely be fixed instead by making
 * consumers use structured keys).
 *
 * Replacement is not implemented.  Instead, when the cache gets too big, at
 * a safe point (unlock) we throw out all of the cache data and let it
 * regenerate for the next rendering operation.
 *
 * The reloc_buf pointers need to be included as key data, otherwise the
 * non-unique values stuffed in the offset in key data through
 * brw_cache_data() may result in successful probe for state buffers
 * even when the buffer being referenced doesn't match.  The result would be
 * that the same state cache entry is used twice for different buffers,
 * only one of the two buffers referenced gets put into the offset, and the
 * incorrect program is run for the other instance.
 */

#include "main/imports.h"
d34 1
d36 5
d42 5
d48 7
a54 1
#define FILE_DEBUG_FLAG DEBUG_STATE
d56 1
a56 2
static GLuint
hash_key(struct brw_cache_item *item)
d58 2
a59 2
   GLuint *ikey = (GLuint *)item->key;
   GLuint hash = item->cache_id, i;
d61 1
a61 1
   assert(item->key_size % 4 == 0);
d65 1
a65 1
   for (i = 0; i < item->key_size/4; i++) {
a66 9
      hash = (hash << 5) | (hash >> 27);
   }

   /* Include the BO pointers as key data as well */
   ikey = (GLuint *)item->reloc_bufs;
   for (i = 0; i < item->nr_reloc_bufs * sizeof(drm_intel_bo *) / 4; i++) {
      hash ^= ikey[i];
      hash = (hash << 5) | (hash >> 27);
   }
d71 4
a74 33

/**
 * Marks a new buffer as being chosen for the given cache id.
 */
static void
update_cache_last(struct brw_cache *cache, enum brw_cache_id cache_id,
		  drm_intel_bo *bo)
{
   if (bo == cache->last_bo[cache_id])
      return; /* no change */

   drm_intel_bo_unreference(cache->last_bo[cache_id]);
   cache->last_bo[cache_id] = bo;
   drm_intel_bo_reference(cache->last_bo[cache_id]);
   cache->brw->state.dirty.cache |= 1 << cache_id;
}

static int
brw_cache_item_equals(const struct brw_cache_item *a,
		      const struct brw_cache_item *b)
{
   return a->cache_id == b->cache_id &&
      a->hash == b->hash &&
      a->key_size == b->key_size &&
      (memcmp(a->key, b->key, a->key_size) == 0) &&
      a->nr_reloc_bufs == b->nr_reloc_bufs &&
      (memcmp(a->reloc_bufs, b->reloc_bufs,
	      a->nr_reloc_bufs * sizeof(drm_intel_bo *)) == 0);
}

static struct brw_cache_item *
search_cache(struct brw_cache *cache, GLuint hash,
	     struct brw_cache_item *lookup)
a77 10
#if 0
   int bucketcount = 0;

   for (c = cache->items[hash % cache->size]; c; c = c->next)
      bucketcount++;

   fprintf(stderr, "bucket %d/%d = %d/%d items\n", hash % cache->size,
	   cache->size, bucketcount, cache->n_items);
#endif

d79 3
a81 1
      if (brw_cache_item_equals(lookup, c))
d89 1
a89 2
static void
rehash(struct brw_cache *cache)
d96 2
a97 1
   items = (struct brw_cache_item**) calloc(1, size * sizeof(*items));
d112 5
a116 10
/**
 * Returns the buffer object matching cache_id and key, or NULL.
 */
drm_intel_bo *
brw_search_cache(struct brw_cache *cache,
                 enum brw_cache_id cache_id,
                 const void *key,
                 GLuint key_size,
                 drm_intel_bo **reloc_bufs, GLuint nr_reloc_bufs,
                 void *aux_return)
d119 2
a120 2
   struct brw_cache_item lookup;
   GLuint hash;
d122 1
a122 7
   lookup.cache_id = cache_id;
   lookup.key = key;
   lookup.key_size = key_size;
   lookup.reloc_bufs = reloc_bufs;
   lookup.nr_reloc_bufs = nr_reloc_bufs;
   hash = hash_key(&lookup);
   lookup.hash = hash;
d124 13
a136 12
   item = search_cache(cache, hash, &lookup);

   if (item == NULL)
      return NULL;

   if (aux_return)
      *(void **)aux_return = (void *)((char *)item->key + item->key_size);

   update_cache_last(cache, cache_id, item->bo);

   drm_intel_bo_reference(item->bo);
   return item->bo;
d139 9
a147 14

drm_intel_bo *
brw_upload_cache_with_auxdata(struct brw_cache *cache,
			      enum brw_cache_id cache_id,
			      const void *key,
			      GLuint key_size,
			      drm_intel_bo **reloc_bufs,
			      GLuint nr_reloc_bufs,
			      const void *data,
			      GLuint data_size,
			      const void *aux,
			      GLuint aux_size,
			      void *aux_return)
{
d149 9
a157 21
   GLuint hash;
   GLuint relocs_size = nr_reloc_bufs * sizeof(drm_intel_bo *);
   void *tmp;
   drm_intel_bo *bo;
   int i;

   item->cache_id = cache_id;
   item->key = key;
   item->key_size = key_size;
   item->reloc_bufs = reloc_bufs;
   item->nr_reloc_bufs = nr_reloc_bufs;
   hash = hash_key(item);
   item->hash = hash;

   /* Create the buffer object to contain the data */
   bo = drm_intel_bo_alloc(cache->brw->intel.bufmgr,
			   cache->name[cache_id], data_size, 1 << 6);


   /* Set up the memory containing the key, aux_data, and reloc_bufs */
   tmp = malloc(key_size + aux_size + relocs_size);
a159 6
   memcpy(tmp + key_size, aux, aux_size);
   memcpy(tmp + key_size + aux_size, reloc_bufs, relocs_size);
   for (i = 0; i < nr_reloc_bufs; i++) {
      if (reloc_bufs[i] != NULL)
	 drm_intel_bo_reference(reloc_bufs[i]);
   }
d161 3
d165 4
a168 1
   item->reloc_bufs = tmp + key_size + aux_size;
d170 1
a170 4
   item->bo = bo;
   drm_intel_bo_reference(bo);

   if (cache->n_items > cache->size * 1.5)
d172 1
a172 1

d176 1
a176 2
   cache->n_items++;

d178 1
d182 22
a203 28
   DBG("upload %s: %d bytes to cache id %d\n",
       cache->name[cache_id],
       data_size, cache_id);

   /* Copy data to the buffer */
   drm_intel_bo_subdata(bo, 0, data_size, data);

   update_cache_last(cache, cache_id, bo);

   return bo;
}

drm_intel_bo *
brw_upload_cache(struct brw_cache *cache,
		 enum brw_cache_id cache_id,
		 const void *key,
		 GLuint key_size,
		 drm_intel_bo **reloc_bufs,
		 GLuint nr_reloc_bufs,
		 const void *data,
		 GLuint data_size)
{
   return brw_upload_cache_with_auxdata(cache, cache_id,
					key, key_size,
					reloc_bufs, nr_reloc_bufs,
					data, data_size,
					NULL, 0,
					NULL);
d206 1
a206 9
/**
 * Wrapper around brw_cache_data_sz using the cache_id's canonical key size.
 *
 * If nr_reloc_bufs is nonzero, brw_search_cache()/brw_upload_cache() would be
 * better to use, as the potentially changing offsets in the data-used-as-key
 * will result in excessive cache misses.
 *
 * If aux data is involved, use search/upload instead.

d208 11
a218 23
drm_intel_bo *
brw_cache_data(struct brw_cache *cache,
	       enum brw_cache_id cache_id,
	       const void *data,
	       GLuint data_size)
{
   drm_intel_bo *bo;
   struct brw_cache_item *item, lookup;
   GLuint hash;

   lookup.cache_id = cache_id;
   lookup.key = data;
   lookup.key_size = data_size;
   lookup.reloc_bufs = NULL;
   lookup.nr_reloc_bufs = 0;
   hash = hash_key(&lookup);
   lookup.hash = hash;

   item = search_cache(cache, hash, &lookup);
   if (item) {
      update_cache_last(cache, cache_id, item->bo);
      drm_intel_bo_reference(item->bo);
      return item->bo;
d221 2
a222 4
   bo = brw_upload_cache(cache, cache_id,
			 data, data_size,
			 NULL, 0,
			 data, data_size);
d224 4
a227 1
   return bo;
a229 4
enum pool_type {
   DW_SURFACE_STATE,
   DW_GENERAL_STATE
};
a231 7
static void
brw_init_cache_id(struct brw_cache *cache,
                  const char *name,
                  enum brw_cache_id id)
{
   cache->name[id] = strdup(name);
}
d234 7
a240 2
static void
brw_init_non_surface_cache(struct brw_context *brw)
d242 1
a242 2
   struct brw_cache *cache = &brw->cache;

d244 3
d251 2
a252 1
      calloc(1, cache->size * sizeof(struct brw_cache_item));
a253 8
   brw_init_cache_id(cache, "CC_VP", BRW_CC_VP);
   brw_init_cache_id(cache, "CC_UNIT", BRW_CC_UNIT);
   brw_init_cache_id(cache, "WM_PROG", BRW_WM_PROG);
   brw_init_cache_id(cache, "SAMPLER_DEFAULT_COLOR", BRW_SAMPLER_DEFAULT_COLOR);
   brw_init_cache_id(cache, "SAMPLER", BRW_SAMPLER);
   brw_init_cache_id(cache, "WM_UNIT", BRW_WM_UNIT);
   brw_init_cache_id(cache, "SF_PROG", BRW_SF_PROG);
   brw_init_cache_id(cache, "SF_VP", BRW_SF_VP);
d255 9
a263 17
   brw_init_cache_id(cache, "SF_UNIT", BRW_SF_UNIT);

   brw_init_cache_id(cache, "VS_UNIT", BRW_VS_UNIT);

   brw_init_cache_id(cache, "VS_PROG", BRW_VS_PROG);

   brw_init_cache_id(cache, "CLIP_UNIT", BRW_CLIP_UNIT);

   brw_init_cache_id(cache, "CLIP_PROG", BRW_CLIP_PROG);
   brw_init_cache_id(cache, "CLIP_VP", BRW_CLIP_VP);

   brw_init_cache_id(cache, "GS_UNIT", BRW_GS_UNIT);

   brw_init_cache_id(cache, "GS_PROG", BRW_GS_PROG);
   brw_init_cache_id(cache, "BLEND_STATE", BRW_BLEND_STATE);
   brw_init_cache_id(cache, "COLOR_CALC_STATE", BRW_COLOR_CALC_STATE);
   brw_init_cache_id(cache, "DEPTH_STENCIL_STATE", BRW_DEPTH_STENCIL_STATE);
d266 1
a266 2
void
brw_init_caches(struct brw_context *brw)
a267 2
   brw_init_non_surface_cache(brw);
}
d269 154
a422 3

static void
brw_clear_cache(struct brw_context *brw, struct brw_cache *cache)
a426 2
   DBG("%s\n", __FUNCTION__);

a428 2
	 int j;

a429 3
	 for (j = 0; j < c->nr_reloc_bufs; j++)
	    drm_intel_bo_unreference(c->reloc_bufs[j]);
	 drm_intel_bo_unreference(c->bo);
d437 16
a458 4
void
brw_state_cache_check_size(struct brw_context *brw)
{
   DBG("%s (n_items=%d)\n", __FUNCTION__, brw->cache.n_items);
a459 6
   /* un-tuned guess.  Each object is generally a page, so 1000 of them is 4 MB of
    * state cache.
    */
   if (brw->cache.n_items > 1000)
      brw_clear_cache(brw, &brw->cache);
}
d462 2
a463 2
static void
brw_destroy_cache(struct brw_context *brw, struct brw_cache *cache)
d467 2
a468 17
   DBG("%s\n", __FUNCTION__);

   brw_clear_cache(brw, cache);
   for (i = 0; i < BRW_MAX_CACHE; i++) {
      drm_intel_bo_unreference(cache->last_bo[i]);
      free(cache->name[i]);
   }
   free(cache->items);
   cache->items = NULL;
   cache->size = 0;
}


void
brw_destroy_caches(struct brw_context *brw)
{
   brw_destroy_cache(brw, &brw->cache);
@


1.1.1.3
log
@Import Mesa 9.2.0
@
text
@d34 5
a38 5
 * This file implements a simple static state cache for 965.  The
 * consumers can query the hash table of state using a cache_id,
 * opaque key data, and receive the corresponding state buffer object
 * of state (plus associated auxiliary data) in return.  Objects in
 * the cache may not have relocations (pointers to other BOs) in them.
d40 5
a44 2
 * The inner workings are a simple hash table based on a CRC of the
 * key data.
d46 11
a56 2
 * Replacement is not implemented.  Instead, when the cache gets too
 * big we throw out all of the cache data and let it get regenerated.
d60 1
a61 2
#include "brw_state.h"
#include "brw_vs.h"
a62 1
#include "brw_vs.h"
d81 7
d91 17
d115 4
a118 1
      (memcmp(a->key, b->key, a->key_size) == 0);
d154 1
a154 1
   items = calloc(1, size * sizeof(*items));
d163 1
a163 1
   free(cache->items);
d172 1
a172 1
bool
d175 4
a178 2
                 const void *key, GLuint key_size,
                 uint32_t *inout_offset, void *out_aux)
a179 1
   struct brw_context *brw = cache->brw;
d187 2
d195 1
a195 1
      return false;
d197 2
a198 1
   *(void **)out_aux = ((char *)item->key + item->key_size);
d200 1
a200 4
   if (item->offset != *inout_offset) {
      brw->state.dirty.cache |= (1 << cache_id);
      *inout_offset = item->offset;
   }
d202 2
a203 1
   return true;
d206 13
a218 2
static void
brw_cache_new_bo(struct brw_cache *cache, uint32_t new_size)
d220 6
a225 2
   struct brw_context *brw = cache->brw;
   drm_intel_bo *new_bo;
d227 7
a233 8
   new_bo = drm_intel_bo_alloc(brw->bufmgr, "program cache", new_size, 64);

   /* Copy any existing data that needs to be saved. */
   if (cache->next_offset != 0) {
      drm_intel_bo_map(cache->bo, false);
      drm_intel_bo_subdata(new_bo, 0, cache->next_offset, cache->bo->virtual);
      drm_intel_bo_unmap(cache->bo);
   }
d235 3
a237 3
   drm_intel_bo_unreference(cache->bo);
   cache->bo = new_bo;
   cache->bo_used_by_gpu = false;
a238 5
   /* Since we have a new BO in place, we need to signal the units
    * that depend on it (state base address on gen5+, or unit state before).
    */
   brw->state.dirty.brw |= BRW_NEW_PROGRAM_CACHE;
}
d240 2
a241 12
/**
 * Attempts to find an item in the cache with identical data and aux
 * data to use
 */
static bool
brw_try_upload_using_copy(struct brw_cache *cache,
			  struct brw_cache_item *result_item,
			  const void *data,
			  const void *aux)
{
   int i;
   struct brw_cache_item *item;
d243 7
a249 25
   for (i = 0; i < cache->size; i++) {
      for (item = cache->items[i]; item; item = item->next) {
	 const void *item_aux = item->key + item->key_size;
	 int ret;

	 if (item->cache_id != result_item->cache_id ||
	     item->size != result_item->size ||
	     item->aux_size != result_item->aux_size) {
	    continue;
	 }

         if (cache->aux_compare[result_item->cache_id]) {
            if (!cache->aux_compare[result_item->cache_id](item_aux, aux,
                                                           item->aux_size,
                                                           item->key))
               continue;
         } else if (memcmp(item_aux, aux, item->aux_size) != 0) {
	    continue;
	 }

	 drm_intel_bo_map(cache->bo, false);
	 ret = memcmp(cache->bo->virtual + item->offset, data, item->size);
	 drm_intel_bo_unmap(cache->bo);
	 if (ret)
	    continue;
d251 2
a252 1
	 result_item->offset = item->offset;
d254 2
a255 3
	 return true;
      }
   }
d257 2
a258 2
   return false;
}
d260 4
a263 8
static void
brw_upload_item_data(struct brw_cache *cache,
		     struct brw_cache_item *item,
		     const void *data)
{
   /* Allocate space in the cache BO for our new program. */
   if (cache->next_offset + item->size > cache->bo->size) {
      uint32_t new_size = cache->bo->size * 2;
d265 3
a267 2
      while (cache->next_offset + item->size > new_size)
	 new_size *= 2;
d269 3
a271 2
      brw_cache_new_bo(cache, new_size);
   }
d273 2
a274 6
   /* If we would block on writing to an in-use program BO, just
    * recreate it.
    */
   if (cache->bo_used_by_gpu) {
      brw_cache_new_bo(cache, cache->bo->size);
   }
d276 1
a276 1
   item->offset = cache->next_offset;
d278 1
a278 2
   /* Programs are always 64-byte aligned, so set up the next one now */
   cache->next_offset = ALIGN(item->offset + item->size, 64);
d281 1
a281 1
void
d286 2
d289 25
a313 5
		 GLuint data_size,
		 const void *aux,
		 GLuint aux_size,
		 uint32_t *out_offset,
		 void *out_aux)
d315 2
a316 1
   struct brw_cache_item *item = CALLOC_STRUCT(brw_cache_item);
a317 1
   void *tmp;
d319 7
a325 7
   item->cache_id = cache_id;
   item->size = data_size;
   item->key = key;
   item->key_size = key_size;
   item->aux_size = aux_size;
   hash = hash_key(item);
   item->hash = hash;
d327 5
a331 9
   /* If we can find a matching prog/prog_data combo in the cache
    * already, then reuse the existing stuff.  This will mean not
    * flagging CACHE_NEW_* when transitioning between the two
    * equivalent hash keys.  This is notably useful for programs
    * generating shaders at runtime, where multiple shaders may
    * compile to the thing in our backend.
    */
   if (!brw_try_upload_using_copy(cache, item, data, aux)) {
      brw_upload_item_data(cache, item, data);
d334 7
a340 2
   /* Set up the memory containing the key and aux_data */
   tmp = malloc(key_size + aux_size);
d342 4
a345 2
   memcpy(tmp, key, key_size);
   memcpy(tmp + key_size, aux, aux_size);
a346 1
   item->key = tmp;
d348 7
a354 7
   if (cache->n_items > cache->size * 1.5)
      rehash(cache);

   hash %= cache->size;
   item->next = cache->items[hash];
   cache->items[hash] = item;
   cache->n_items++;
a355 2
   /* Copy data to the buffer */
   drm_intel_bo_subdata(cache->bo, item->offset, data_size, data);
d357 2
a358 7
   *out_offset = item->offset;
   *(void **)out_aux = (void *)((char *)item->key + item->key_size);
   cache->brw->state.dirty.cache |= 1 << cache_id;
}

void
brw_init_caches(struct brw_context *brw)
d366 22
a387 2
   cache->items =
      calloc(1, cache->size * sizeof(struct brw_cache_item *));
d389 12
a400 8
   cache->bo = drm_intel_bo_alloc(brw->bufmgr,
				  "program cache",
				  4096, 64);

   cache->aux_compare[BRW_VS_PROG] = brw_vs_prog_data_compare;
   cache->aux_compare[BRW_WM_PROG] = brw_wm_prog_data_compare;
   cache->aux_free[BRW_VS_PROG] = brw_vs_prog_data_free;
   cache->aux_free[BRW_WM_PROG] = brw_wm_prog_data_free;
d403 1
d414 2
d417 3
a419 4
         if (cache->aux_free[c->cache_id]) {
            const void *item_aux = c->key + c->key_size;
            cache->aux_free[c->cache_id](item_aux);
         }
a427 8
   /* Start putting programs into the start of the BO again, since
    * we'll never find the old results.
    */
   cache->next_offset = 0;

   /* We need to make sure that the programs get regenerated, since
    * any offsets leftover in brw_context will no longer be valid.
    */
a430 1
   intel_batchbuffer_flush(brw);
d436 3
a438 1
   /* un-tuned guess.  Each object is generally a page, so 2000 of them is 8 MB of
d441 1
a441 3
   if (brw->cache.n_items > 2000) {
      perf_debug("Exceeded state cache size limit.  Clearing the set "
                 "of compiled programs, which will trigger recompiles\n");
a442 1
   }
d449 1
a452 2
   drm_intel_bo_unreference(cache->bo);
   cache->bo = NULL;
d454 4
@


1.1.1.4
log
@Import Mesa 10.2.3
@
text
@d3 1
a3 1
 Intel funded Tungsten Graphics to
d5 1
a5 1

d13 1
a13 1

d17 1
a17 1

d25 1
a25 1

d29 1
a29 1
  *   Keith Whitwell <keithw@@vmware.com>
a52 1
#include "brw_vec4_gs.h"
d217 3
a219 1
            if (!cache->aux_compare[result_item->cache_id](item_aux, aux))
a343 1
   cache->aux_compare[BRW_GS_PROG] = brw_gs_prog_data_compare;
d345 2
a346 3
   cache->aux_free[BRW_VS_PROG] = brw_stage_prog_data_free;
   cache->aux_free[BRW_GS_PROG] = brw_stage_prog_data_free;
   cache->aux_free[BRW_WM_PROG] = brw_stage_prog_data_free;
@


1.1.1.5
log
@Import Mesa 10.4.3
@
text
@d52 2
a53 1
#include "brw_gs.h"
d118 1
a118 1
   items = calloc(size, sizeof(*items));
a174 2
   if (brw->has_llc)
      drm_intel_gem_bo_map_unsynchronized(new_bo);
d178 3
a180 8
      if (brw->has_llc) {
         memcpy(new_bo->virtual, cache->bo->virtual, cache->next_offset);
      } else {
         drm_intel_bo_map(cache->bo, false);
         drm_intel_bo_subdata(new_bo, 0, cache->next_offset,
                              cache->bo->virtual);
         drm_intel_bo_unmap(cache->bo);
      }
a182 2
   if (brw->has_llc)
      drm_intel_bo_unmap(cache->bo);
a202 1
   struct brw_context *brw = cache->brw;
d224 1
a224 2
         if (!brw->has_llc)
            drm_intel_bo_map(cache->bo, false);
d226 1
a226 2
         if (!brw->has_llc)
            drm_intel_bo_unmap(cache->bo);
a243 2
   struct brw_context *brw = cache->brw;

d257 1
a257 2
   if (!brw->has_llc && cache->bo_used_by_gpu) {
      perf_debug("Copying busy program cache buffer.\n");
a278 1
   struct brw_context *brw = cache->brw;
d319 1
a319 5
   if (brw->has_llc) {
      memcpy((char *) cache->bo->virtual + item->offset, data, data_size);
   } else {
      drm_intel_bo_subdata(cache->bo, item->offset, data_size, data);
   }
d336 1
a336 1
      calloc(cache->size, sizeof(struct brw_cache_item *));
a340 2
   if (brw->has_llc)
      drm_intel_gem_bo_map_unsynchronized(cache->bo);
d382 1
a382 1
   brw->state.dirty.brw |= ~0ull;
a406 2
   if (brw->has_llc)
      drm_intel_bo_unmap(cache->bo);
@


1.1.1.6
log
@Import Mesa 10.2.9
@
text
@d52 1
a52 2
#include "brw_vs.h"
#include "brw_vec4_gs.h"
d117 1
a117 1
   items = calloc(1, size * sizeof(*items));
d174 2
d179 8
a186 3
      drm_intel_bo_map(cache->bo, false);
      drm_intel_bo_subdata(new_bo, 0, cache->next_offset, cache->bo->virtual);
      drm_intel_bo_unmap(cache->bo);
d189 2
d211 1
d233 2
a234 1
	 drm_intel_bo_map(cache->bo, false);
d236 2
a237 1
	 drm_intel_bo_unmap(cache->bo);
d255 2
d270 2
a271 1
   if (cache->bo_used_by_gpu) {
d293 1
d334 5
a338 1
   drm_intel_bo_subdata(cache->bo, item->offset, data_size, data);
d355 1
a355 1
      calloc(1, cache->size * sizeof(struct brw_cache_item *));
d360 2
d403 1
a403 1
   brw->state.dirty.brw |= ~0;
d428 2
@


