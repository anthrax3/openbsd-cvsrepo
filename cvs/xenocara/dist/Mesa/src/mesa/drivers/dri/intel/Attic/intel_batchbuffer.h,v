head	1.6;
access;
symbols
	OPENBSD_5_4:1.5.0.4
	OPENBSD_5_4_BASE:1.5
	OPENBSD_5_3:1.5.0.2
	OPENBSD_5_3_BASE:1.5
	OPENBSD_5_2:1.4.0.4
	OPENBSD_5_2_BASE:1.4
	OPENBSD_5_1_BASE:1.4
	OPENBSD_5_1:1.4.0.2
	v7_10_3:1.1.1.1
	mesa:1.1.1
	OPENBSD_5_0:1.3.0.6
	OPENBSD_5_0_BASE:1.3
	OPENBSD_4_9:1.3.0.2
	OPENBSD_4_9_BASE:1.3
	OPENBSD_4_8:1.3.0.4
	OPENBSD_4_8_BASE:1.3
	OPENBSD_4_7:1.2.0.4
	OPENBSD_4_7_BASE:1.2
	OPENBSD_4_6:1.2.0.2
	OPENBSD_4_6_BASE:1.2
	OPENBSD_4_5:1.1.0.2
	OPENBSD_4_5_BASE:1.1;
locks; strict;
comment	@ * @;


1.6
date	2013.09.05.14.04.29;	author jsg;	state dead;
branches;
next	1.5;

1.5
date	2012.08.17.13.58.15;	author mpi;	state Exp;
branches;
next	1.4;

1.4
date	2011.10.23.13.37.39;	author matthieu;	state Exp;
branches;
next	1.3;

1.3
date	2010.05.22.20.06.19;	author matthieu;	state Exp;
branches;
next	1.2;

1.2
date	2009.05.17.20.26.39;	author matthieu;	state Exp;
branches;
next	1.1;

1.1
date	2008.11.02.14.58.16;	author matthieu;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2011.10.23.13.29.38;	author matthieu;	state Exp;
branches;
next	;


desc
@@


1.6
log
@Merge Mesa 9.2.0
@
text
@#ifndef INTEL_BATCHBUFFER_H
#define INTEL_BATCHBUFFER_H

#include "main/mtypes.h"

#include "intel_context.h"
#include "intel_bufmgr.h"
#include "intel_reg.h"

#define BATCH_RESERVED 16

void intel_batchbuffer_init(struct intel_context *intel);
void intel_batchbuffer_reset(struct intel_context *intel);
void intel_batchbuffer_free(struct intel_context *intel);

void _intel_batchbuffer_flush(struct intel_context *intel,
			      const char *file, int line);

#define intel_batchbuffer_flush(intel) \
	_intel_batchbuffer_flush(intel, __FILE__, __LINE__)



/* Unlike bmBufferData, this currently requires the buffer be mapped.
 * Consider it a convenience function wrapping multple
 * intel_buffer_dword() calls.
 */
void intel_batchbuffer_data(struct intel_context *intel,
                            const void *data, GLuint bytes, bool is_blit);

GLboolean intel_batchbuffer_emit_reloc(struct intel_context *intel,
                                       drm_intel_bo *buffer,
				       uint32_t read_domains,
				       uint32_t write_domain,
				       uint32_t offset);
GLboolean intel_batchbuffer_emit_reloc_fenced(struct intel_context *intel,
					      drm_intel_bo *buffer,
					      uint32_t read_domains,
					      uint32_t write_domain,
					      uint32_t offset);
void intel_batchbuffer_emit_mi_flush(struct intel_context *intel);
void intel_emit_post_sync_nonzero_flush(struct intel_context *intel);

static INLINE uint32_t float_as_int(float f)
{
   union {
      float f;
      uint32_t d;
   } fi;

   fi.f = f;
   return fi.d;
}

/* Inline functions - might actually be better off with these
 * non-inlined.  Certainly better off switching all command packets to
 * be passed as structs rather than dwords, but that's a little bit of
 * work...
 */
static INLINE GLint
intel_batchbuffer_space(struct intel_context *intel)
{
   return (intel->batch.state_batch_offset - intel->batch.reserved_space) - intel->batch.used*4;
}


static INLINE void
intel_batchbuffer_emit_dword(struct intel_context *intel, GLuint dword)
{
#ifdef DEBUG
   assert(intel_batchbuffer_space(intel) >= 4);
#endif
   intel->batch.map[intel->batch.used++] = dword;
}

static INLINE void
intel_batchbuffer_emit_float(struct intel_context *intel, float f)
{
   intel_batchbuffer_emit_dword(intel, float_as_int(f));
}

static INLINE void
intel_batchbuffer_require_space(struct intel_context *intel,
                                GLuint sz, int is_blit)
{

   if (intel->gen >= 6 &&
       intel->batch.is_blit != is_blit && intel->batch.used) {
      intel_batchbuffer_flush(intel);
   }

   intel->batch.is_blit = is_blit;

#ifdef DEBUG
   assert(sz < sizeof(intel->batch.map) - BATCH_RESERVED);
#endif
   if (intel_batchbuffer_space(intel) < sz)
      intel_batchbuffer_flush(intel);
}

static INLINE void
intel_batchbuffer_begin(struct intel_context *intel, int n, bool is_blit)
{
   intel_batchbuffer_require_space(intel, n * 4, is_blit);

   intel->batch.emit = intel->batch.used;
#ifdef DEBUG
   intel->batch.total = n;
#endif
}

static INLINE void
intel_batchbuffer_advance(struct intel_context *intel)
{
#ifdef DEBUG
   struct intel_batchbuffer *batch = &intel->batch;
   unsigned int _n = batch->used - batch->emit;
   assert(batch->total != 0);
   if (_n != batch->total) {
      fprintf(stderr, "ADVANCE_BATCH: %d of %d dwords emitted\n",
	      _n, batch->total);
      abort();
   }
   batch->total = 0;
#endif
}

void intel_batchbuffer_cached_advance(struct intel_context *intel);

/* Here are the crusty old macros, to be removed:
 */
#define BATCH_LOCALS

#define BEGIN_BATCH(n) intel_batchbuffer_begin(intel, n, false)
#define BEGIN_BATCH_BLT(n) intel_batchbuffer_begin(intel, n, true)
#define OUT_BATCH(d) intel_batchbuffer_emit_dword(intel, d)
#define OUT_BATCH_F(f) intel_batchbuffer_emit_float(intel,f)
#define OUT_RELOC(buf, read_domains, write_domain, delta) do {		\
   intel_batchbuffer_emit_reloc(intel, buf,			\
				read_domains, write_domain, delta);	\
} while (0)
#define OUT_RELOC_FENCED(buf, read_domains, write_domain, delta) do {	\
   intel_batchbuffer_emit_reloc_fenced(intel, buf,		\
				       read_domains, write_domain, delta); \
} while (0)

#define ADVANCE_BATCH() intel_batchbuffer_advance(intel);
#define CACHED_BATCH() intel_batchbuffer_cached_advance(intel);

#endif
@


1.5
log
@Upate to libGL 7.11.2

Tested by jsg@@, matthieu@@ and ajacoutot@@, ok mattieu@@
@
text
@@


1.4
log
@Merge Mesa 7.10.3
@
text
@a9 1
#define BATCH_SZ 16384
d12 3
d16 1
a16 32
struct intel_batchbuffer
{
   struct intel_context *intel;

   drm_intel_bo *buf;

   GLubyte *map;
   GLubyte *ptr;

   GLuint size;
   uint32_t state_batch_offset;

#ifdef DEBUG
   /** Tracking of BEGIN_BATCH()/OUT_BATCH()/ADVANCE_BATCH() debugging */
   struct {
      GLuint total;
      GLubyte *start_ptr;
   } emit;
#endif

   bool is_blit;
   GLuint dirty_state;
   GLuint reserved_space;
};

struct intel_batchbuffer *intel_batchbuffer_alloc(struct intel_context
                                                  *intel);

void intel_batchbuffer_free(struct intel_batchbuffer *batch);


void _intel_batchbuffer_flush(struct intel_batchbuffer *batch,
d19 2
a20 2
#define intel_batchbuffer_flush(batch) \
	_intel_batchbuffer_flush(batch, __FILE__, __LINE__)
a21 1
void intel_batchbuffer_reset(struct intel_batchbuffer *batch);
d28 1
a28 1
void intel_batchbuffer_data(struct intel_batchbuffer *batch,
d31 1
a31 4
void intel_batchbuffer_release_space(struct intel_batchbuffer *batch,
                                     GLuint bytes);

GLboolean intel_batchbuffer_emit_reloc(struct intel_batchbuffer *batch,
d36 1
a36 1
GLboolean intel_batchbuffer_emit_reloc_fenced(struct intel_batchbuffer *batch,
d41 2
a42 1
void intel_batchbuffer_emit_mi_flush(struct intel_batchbuffer *batch);
d61 1
a61 1
intel_batchbuffer_space(struct intel_batchbuffer *batch)
d63 1
a63 2
   return (batch->state_batch_offset - batch->reserved_space) -
      (batch->ptr - batch->map);
d68 1
a68 1
intel_batchbuffer_emit_dword(struct intel_batchbuffer *batch, GLuint dword)
d71 1
a71 1
   assert(intel_batchbuffer_space(batch) >= 4);
d73 1
a73 2
   *(GLuint *) (batch->ptr) = dword;
   batch->ptr += 4;
d77 1
a77 1
intel_batchbuffer_emit_float(struct intel_batchbuffer *batch, float f)
d79 1
a79 1
   intel_batchbuffer_emit_dword(batch, float_as_int(f));
d83 1
a83 1
intel_batchbuffer_require_space(struct intel_batchbuffer *batch,
d87 3
a89 3
   if (batch->intel->gen >= 6 && batch->is_blit != is_blit &&
       batch->ptr != batch->map) {
      intel_batchbuffer_flush(batch);
d92 1
a92 1
   batch->is_blit = is_blit;
d95 1
a95 1
   assert(sz < batch->size - 8);
d97 2
a98 2
   if (intel_batchbuffer_space(batch) < sz)
      intel_batchbuffer_flush(batch);
d102 1
a102 1
intel_batchbuffer_begin(struct intel_batchbuffer *batch, int n, bool is_blit)
d104 1
a104 1
   intel_batchbuffer_require_space(batch, n * 4, is_blit);
d106 1
d108 1
a108 4
   assert(batch->map);
   assert(batch->emit.start_ptr == NULL);
   batch->emit.total = n * 4;
   batch->emit.start_ptr = batch->ptr;
d113 1
a113 1
intel_batchbuffer_advance(struct intel_batchbuffer *batch)
d116 4
a119 3
   unsigned int _n = batch->ptr - batch->emit.start_ptr;
   assert(batch->emit.start_ptr != NULL);
   if (_n != batch->emit.total) {
d121 1
a121 1
	      _n, batch->emit.total);
d124 1
a124 1
   batch->emit.start_ptr = NULL;
d128 2
d134 4
a137 4
#define BEGIN_BATCH(n) intel_batchbuffer_begin(intel->batch, n, false)
#define BEGIN_BATCH_BLT(n) intel_batchbuffer_begin(intel->batch, n, true)
#define OUT_BATCH(d) intel_batchbuffer_emit_dword(intel->batch, d)
#define OUT_BATCH_F(f) intel_batchbuffer_emit_float(intel->batch,f)
d139 1
a139 1
   intel_batchbuffer_emit_reloc(intel->batch, buf,			\
d143 1
a143 1
   intel_batchbuffer_emit_reloc_fenced(intel->batch, buf,		\
d147 2
a148 1
#define ADVANCE_BATCH() intel_batchbuffer_advance(intel->batch);
@


1.3
log
@Update to Mesa 7.8.1. Tested on a bulk ports build by naddy@@, ok oga@@.
@
text
@d18 1
a18 3
   dri_bo *buf;

   GLubyte *buffer;
d24 1
d34 1
d59 1
a59 1
                            const void *data, GLuint bytes);
d65 1
a65 1
                                       dri_bo *buffer,
d95 2
a96 1
   return (batch->size - batch->reserved_space) - (batch->ptr - batch->map);
d118 1
a118 1
                                GLuint sz)
d120 8
d136 1
a136 1
intel_batchbuffer_begin(struct intel_batchbuffer *batch, int n)
d138 2
a139 1
   intel_batchbuffer_require_space(batch, n * 4);
d167 2
a168 1
#define BEGIN_BATCH(n) intel_batchbuffer_begin(intel->batch, n)
@


1.2
log
@Update to Mesa 7.4.2. Tested by oga@@, ckuethe@@ and naddy@@.
@
text
@a12 29
enum cliprect_mode {
   /**
    * Batchbuffer contents may be looped over per cliprect, but do not
    * require it.
    */
   IGNORE_CLIPRECTS,
   /**
    * Batchbuffer contents require looping over per cliprect at batch submit
    * time.
    *
    * This will be upgraded to NO_LOOP_CLIPRECTS when there's a single
    * constant cliprect, as in DRI2 or FBO rendering.
    */
   LOOP_CLIPRECTS,
   /**
    * Batchbuffer contents contain drawing that should not be executed multiple
    * times.
    */
   NO_LOOP_CLIPRECTS,
   /**
    * Batchbuffer contents contain drawing that already handles cliprects, such
    * as 2D drawing to front/back/depth that doesn't respect DRAWING_RECTANGLE.
    *
    * Equivalent behavior to NO_LOOP_CLIPRECTS, but may not persist in batch
    * outside of LOCK/UNLOCK.  This is upgraded to just NO_LOOP_CLIPRECTS when
    * there's a constant cliprect, as in DRI2 or FBO rendering.
    */
   REFERENCES_CLIPRECTS
};
a24 2
   enum cliprect_mode cliprect_mode;

d27 1
d33 1
d36 1
d59 1
a59 2
                            const void *data, GLuint bytes,
			    enum cliprect_mode cliprect_mode);
d69 17
d95 1
a95 1
   return (batch->size - BATCH_RESERVED) - (batch->ptr - batch->map);
d102 1
a102 1
   assert(batch->map);
d104 1
d110 6
d117 1
a117 2
                                GLuint sz,
				enum cliprect_mode cliprect_mode)
d119 1
d121 1
d124 13
d138 10
a147 14
   if ((cliprect_mode == LOOP_CLIPRECTS ||
	cliprect_mode == REFERENCES_CLIPRECTS) &&
       batch->intel->constant_cliprect)
      cliprect_mode = NO_LOOP_CLIPRECTS;

   if (cliprect_mode != IGNORE_CLIPRECTS) {
      if (batch->cliprect_mode == IGNORE_CLIPRECTS) {
	 batch->cliprect_mode = cliprect_mode;
      } else {
	 if (batch->cliprect_mode != cliprect_mode) {
	    intel_batchbuffer_flush(batch);
	    batch->cliprect_mode = cliprect_mode;
	 }
      }
d149 2
d157 1
a157 7
#define BEGIN_BATCH(n, cliprect_mode) do {				\
   intel_batchbuffer_require_space(intel->batch, (n)*4, cliprect_mode); \
   assert(intel->batch->emit.start_ptr == NULL);			\
   intel->batch->emit.total = (n) * 4;					\
   intel->batch->emit.start_ptr = intel->batch->ptr;			\
} while (0)

d159 1
a159 1

a160 1
   assert((delta) >= 0);						\
d164 4
d169 1
a169 18
#define ADVANCE_BATCH() do {						\
   unsigned int _n = intel->batch->ptr - intel->batch->emit.start_ptr;	\
   assert(intel->batch->emit.start_ptr != NULL);			\
   if (_n != intel->batch->emit.total) {				\
      fprintf(stderr, "ADVANCE_BATCH: %d of %d dwords emitted\n",	\
	      _n, intel->batch->emit.total);				\
      abort();								\
   }									\
   intel->batch->emit.start_ptr = NULL;					\
} while(0)


static INLINE void
intel_batchbuffer_emit_mi_flush(struct intel_batchbuffer *batch)
{
   intel_batchbuffer_require_space(batch, 4, IGNORE_CLIPRECTS);
   intel_batchbuffer_emit_dword(batch, MI_FLUSH);
}
@


1.1
log
@Mesa 7.2, Tested by ckuethe@@, naddy@@, oga@@, and others.
@
text
@d4 1
a4 1
#include "mtypes.h"
d6 3
a8 3
#include "dri_bufmgr.h"

struct intel_context;
d22 3
d35 1
d37 2
a38 1
    * outside of LOCK/UNLOCK.
d48 2
a49 1
   dri_fence *last_fence;
d58 6
a72 2
void intel_batchbuffer_finish(struct intel_batchbuffer *batch);

d95 3
a97 1
                                       GLuint flags, GLuint offset);
d104 1
a104 1
static INLINE GLuint
d129 5
d152 3
d157 1
a157 1
#define OUT_BATCH(d)  intel_batchbuffer_emit_dword(intel->batch, d)
d159 1
a159 1
#define OUT_RELOC(buf, cliprect_mode, delta) do { 			\
d161 2
a162 1
   intel_batchbuffer_emit_reloc(intel->batch, buf, cliprect_mode, delta); \
d165 10
a174 1
#define ADVANCE_BATCH() do { } while(0)
d176 7
@


1.1.1.1
log
@Import Mesa 7.10.3
@
text
@d4 1
a4 1
#include "main/mtypes.h"
d6 3
a8 3
#include "intel_context.h"
#include "intel_bufmgr.h"
#include "intel_reg.h"
d13 24
d42 2
a43 1
   drm_intel_bo *buf;
d48 2
a50 9
   uint32_t state_batch_offset;

#ifdef DEBUG
   /** Tracking of BEGIN_BATCH()/OUT_BATCH()/ADVANCE_BATCH() debugging */
   struct {
      GLuint total;
      GLubyte *start_ptr;
   } emit;
#endif
a51 1
   bool is_blit;
a52 1
   GLuint reserved_space;
d61 2
d77 2
a78 1
                            const void *data, GLuint bytes, bool is_blit);
d84 2
a85 21
                                       drm_intel_bo *buffer,
				       uint32_t read_domains,
				       uint32_t write_domain,
				       uint32_t offset);
GLboolean intel_batchbuffer_emit_reloc_fenced(struct intel_batchbuffer *batch,
					      drm_intel_bo *buffer,
					      uint32_t read_domains,
					      uint32_t write_domain,
					      uint32_t offset);
void intel_batchbuffer_emit_mi_flush(struct intel_batchbuffer *batch);

static INLINE uint32_t float_as_int(float f)
{
   union {
      float f;
      uint32_t d;
   } fi;

   fi.f = f;
   return fi.d;
}
d92 1
a92 1
static INLINE GLint
d95 1
a95 2
   return (batch->state_batch_offset - batch->reserved_space) -
      (batch->ptr - batch->map);
d102 1
a102 1
#ifdef DEBUG
a103 1
#endif
a108 6
intel_batchbuffer_emit_float(struct intel_batchbuffer *batch, float f)
{
   intel_batchbuffer_emit_dword(batch, float_as_int(f));
}

static INLINE void
d110 2
a111 1
                                GLuint sz, int is_blit)
a112 9

   if (batch->intel->gen >= 6 && batch->is_blit != is_blit &&
       batch->ptr != batch->map) {
      intel_batchbuffer_flush(batch);
   }

   batch->is_blit = is_blit;

#ifdef DEBUG
a113 1
#endif
a115 1
}
d117 9
a125 23
static INLINE void
intel_batchbuffer_begin(struct intel_batchbuffer *batch, int n, bool is_blit)
{
   intel_batchbuffer_require_space(batch, n * 4, is_blit);

#ifdef DEBUG
   assert(batch->map);
   assert(batch->emit.start_ptr == NULL);
   batch->emit.total = n * 4;
   batch->emit.start_ptr = batch->ptr;
#endif
}

static INLINE void
intel_batchbuffer_advance(struct intel_batchbuffer *batch)
{
#ifdef DEBUG
   unsigned int _n = batch->ptr - batch->emit.start_ptr;
   assert(batch->emit.start_ptr != NULL);
   if (_n != batch->emit.total) {
      fprintf(stderr, "ADVANCE_BATCH: %d of %d dwords emitted\n",
	      _n, batch->emit.total);
      abort();
a126 2
   batch->emit.start_ptr = NULL;
#endif
d133 2
a134 7
#define BEGIN_BATCH(n) intel_batchbuffer_begin(intel->batch, n, false)
#define BEGIN_BATCH_BLT(n) intel_batchbuffer_begin(intel->batch, n, true)
#define OUT_BATCH(d) intel_batchbuffer_emit_dword(intel->batch, d)
#define OUT_BATCH_F(f) intel_batchbuffer_emit_float(intel->batch,f)
#define OUT_RELOC(buf, read_domains, write_domain, delta) do {		\
   intel_batchbuffer_emit_reloc(intel->batch, buf,			\
				read_domains, write_domain, delta);	\
d136 6
a141 3
#define OUT_RELOC_FENCED(buf, read_domains, write_domain, delta) do {	\
   intel_batchbuffer_emit_reloc_fenced(intel->batch, buf,		\
				       read_domains, write_domain, delta); \
d144 2
a145 1
#define ADVANCE_BATCH() intel_batchbuffer_advance(intel->batch);
@


