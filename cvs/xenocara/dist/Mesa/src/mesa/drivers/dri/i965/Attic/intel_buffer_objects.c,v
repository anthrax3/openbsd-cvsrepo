head	1.7;
access;
symbols
	OPENBSD_5_8:1.6.0.4
	OPENBSD_5_8_BASE:1.6
	OPENBSD_5_7:1.6.0.2
	OPENBSD_5_7_BASE:1.6
	v10_2_9:1.1.1.5
	v10_4_3:1.1.1.4
	v10_2_7:1.1.1.3
	OPENBSD_5_6:1.4.0.2
	OPENBSD_5_6_BASE:1.4
	v10_2_3:1.1.1.3
	OPENBSD_5_5:1.3.0.2
	OPENBSD_5_5_BASE:1.3
	v9_2_5:1.1.1.2
	v9_2_3:1.1.1.2
	v9_2_2:1.1.1.2
	v9_2_1:1.1.1.2
	v9_2_0:1.1.1.2
	OPENBSD_4_4:1.1.1.1.0.6
	OPENBSD_4_4_BASE:1.1.1.1
	OPENBSD_4_3_BASE:1.1.1.1
	OPENBSD_4_3:1.1.1.1.0.4
	v7_0_1:1.1.1.1
	OPENBSD_4_2:1.1.1.1.0.2
	OPENBSD_4_2_BASE:1.1.1.1
	v6_5_2:1.1.1.1
	v6_5_1:1.1.1.1
	mesa:1.1.1;
locks; strict;
comment	@ * @;


1.7
date	2015.12.23.05.17.50;	author jsg;	state dead;
branches;
next	1.6;
commitid	TnlogFl9nOv2eaRf;

1.6
date	2015.02.20.23.09.58;	author jsg;	state Exp;
branches;
next	1.5;
commitid	4ry2gvZGMXkCUD2n;

1.5
date	2015.01.25.14.41.21;	author jsg;	state Exp;
branches;
next	1.4;
commitid	mcxB0JvoI9gTDYXU;

1.4
date	2014.07.09.21.08.59;	author jsg;	state Exp;
branches;
next	1.3;
commitid	WPD6rgPryPkvXOr9;

1.3
date	2013.09.05.14.04.26;	author jsg;	state Exp;
branches;
next	1.2;

1.2
date	2008.11.02.14.58.15;	author matthieu;	state dead;
branches;
next	1.1;

1.1
date	2006.11.25.18.52.51;	author matthieu;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2006.11.25.18.52.51;	author matthieu;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2013.09.05.13.15.45;	author jsg;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2014.07.09.20.34.53;	author jsg;	state Exp;
branches;
next	1.1.1.4;
commitid	3JhLfwcuBALP0ZR7;

1.1.1.4
date	2015.01.25.14.11.54;	author jsg;	state Exp;
branches;
next	1.1.1.5;
commitid	ce2W5rH5aF7VS9gi;

1.1.1.5
date	2015.02.20.22.49.01;	author jsg;	state Exp;
branches;
next	;
commitid	F54a1i0WXHMxq7kE;


desc
@@


1.7
log
@remove the now unused Mesa 10.2.9 code
@
text
@/**************************************************************************
 *
 * Copyright 2003 VMware, Inc.
 * All Rights Reserved.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sub license, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice (including the
 * next paragraph) shall be included in all copies or substantial portions
 * of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
 * IN NO EVENT SHALL VMWARE AND/OR ITS SUPPLIERS BE LIABLE FOR
 * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *
 **************************************************************************/

/**
 * @@file intel_buffer_objects.c
 *
 * This provides core GL buffer object functionality.
 */

#include "main/imports.h"
#include "main/mtypes.h"
#include "main/macros.h"
#include "main/bufferobj.h"

#include "brw_context.h"
#include "intel_blit.h"
#include "intel_buffer_objects.h"
#include "intel_batchbuffer.h"

/**
 * Map a buffer object; issue performance warnings if mapping causes stalls.
 *
 * This matches the drm_intel_bo_map API, but takes an additional human-readable
 * name for the buffer object to use in the performance debug message.
 */
int
brw_bo_map(struct brw_context *brw,
           drm_intel_bo *bo, int write_enable,
           const char *bo_name)
{
   if (likely(!brw->perf_debug) || !drm_intel_bo_busy(bo))
      return drm_intel_bo_map(bo, write_enable);

   double start_time = get_time();

   int ret = drm_intel_bo_map(bo, write_enable);

   perf_debug("CPU mapping a busy %s BO stalled and took %.03f ms.\n",
              bo_name, (get_time() - start_time) * 1000);

   return ret;
}

int
brw_bo_map_gtt(struct brw_context *brw, drm_intel_bo *bo, const char *bo_name)
{
   if (likely(!brw->perf_debug) || !drm_intel_bo_busy(bo))
      return drm_intel_gem_bo_map_gtt(bo);

   double start_time = get_time();

   int ret = drm_intel_gem_bo_map_gtt(bo);

   perf_debug("GTT mapping a busy %s BO stalled and took %.03f ms.\n",
              bo_name, (get_time() - start_time) * 1000);

   return ret;
}

static GLboolean
intel_bufferobj_unmap(struct gl_context * ctx, struct gl_buffer_object *obj,
                      gl_map_buffer_index index);

static void
intel_bufferobj_mark_gpu_usage(struct intel_buffer_object *intel_obj,
                               uint32_t offset, uint32_t size)
{
   intel_obj->gpu_active_start = MIN2(intel_obj->gpu_active_start, offset);
   intel_obj->gpu_active_end = MAX2(intel_obj->gpu_active_end, offset + size);
}

static void
intel_bufferobj_mark_inactive(struct intel_buffer_object *intel_obj)
{
   intel_obj->gpu_active_start = ~0;
   intel_obj->gpu_active_end = 0;
}

/** Allocates a new drm_intel_bo to store the data for the buffer object. */
static void
intel_bufferobj_alloc_buffer(struct brw_context *brw,
			     struct intel_buffer_object *intel_obj)
{
   intel_obj->buffer = drm_intel_bo_alloc(brw->bufmgr, "bufferobj",
					  intel_obj->Base.Size, 64);

   /* the buffer might be bound as a uniform buffer, need to update it
    */
   brw->state.dirty.brw |= BRW_NEW_UNIFORM_BUFFER;

   intel_bufferobj_mark_inactive(intel_obj);
}

static void
release_buffer(struct intel_buffer_object *intel_obj)
{
   drm_intel_bo_unreference(intel_obj->buffer);
   intel_obj->buffer = NULL;
}

/**
 * The NewBufferObject() driver hook.
 *
 * Allocates a new intel_buffer_object structure and initializes it.
 *
 * There is some duplication between mesa's bufferobjects and our
 * bufmgr buffers.  Both have an integer handle and a hashtable to
 * lookup an opaque structure.  It would be nice if the handles and
 * internal structure where somehow shared.
 */
static struct gl_buffer_object *
intel_bufferobj_alloc(struct gl_context * ctx, GLuint name, GLenum target)
{
   struct intel_buffer_object *obj = CALLOC_STRUCT(intel_buffer_object);

   _mesa_initialize_buffer_object(ctx, &obj->Base, name, target);

   obj->buffer = NULL;

   return &obj->Base;
}

/**
 * The DeleteBuffer() driver hook.
 *
 * Deletes a single OpenGL buffer object.  Used by glDeleteBuffers().
 */
static void
intel_bufferobj_free(struct gl_context * ctx, struct gl_buffer_object *obj)
{
   struct intel_buffer_object *intel_obj = intel_buffer_object(obj);

   assert(intel_obj);

   /* Buffer objects are automatically unmapped when deleting according
    * to the spec, but Mesa doesn't do UnmapBuffer for us at context destroy
    * (though it does if you call glDeleteBuffers)
    */
   _mesa_buffer_unmap_all_mappings(ctx, obj);

   drm_intel_bo_unreference(intel_obj->buffer);
   free(intel_obj);
}


/**
 * The BufferData() driver hook.
 *
 * Implements glBufferData(), which recreates a buffer object's data store
 * and populates it with the given data, if present.
 *
 * Any data that was previously stored in the buffer object is lost.
 *
 * \return true for success, false if out of memory
 */
static GLboolean
intel_bufferobj_data(struct gl_context * ctx,
                     GLenum target,
                     GLsizeiptrARB size,
                     const GLvoid * data,
                     GLenum usage,
                     GLbitfield storageFlags,
                     struct gl_buffer_object *obj)
{
   struct brw_context *brw = brw_context(ctx);
   struct intel_buffer_object *intel_obj = intel_buffer_object(obj);

   /* Part of the ABI, but this function doesn't use it.
    */
   (void) target;

   intel_obj->Base.Size = size;
   intel_obj->Base.Usage = usage;
   intel_obj->Base.StorageFlags = storageFlags;

   assert(!obj->Mappings[MAP_USER].Pointer); /* Mesa should have unmapped it */
   assert(!obj->Mappings[MAP_INTERNAL].Pointer);

   if (intel_obj->buffer != NULL)
      release_buffer(intel_obj);

   if (size != 0) {
      intel_bufferobj_alloc_buffer(brw, intel_obj);
      if (!intel_obj->buffer)
         return false;

      if (data != NULL)
	 drm_intel_bo_subdata(intel_obj->buffer, 0, size, data);
   }

   return true;
}


/**
 * The BufferSubData() driver hook.
 *
 * Implements glBufferSubData(), which replaces a portion of the data in a
 * buffer object.
 *
 * If the data range specified by (size + offset) extends beyond the end of
 * the buffer or if data is NULL, no copy is performed.
 */
static void
intel_bufferobj_subdata(struct gl_context * ctx,
                        GLintptrARB offset,
                        GLsizeiptrARB size,
                        const GLvoid * data, struct gl_buffer_object *obj)
{
   struct brw_context *brw = brw_context(ctx);
   struct intel_buffer_object *intel_obj = intel_buffer_object(obj);
   bool busy;

   if (size == 0)
      return;

   assert(intel_obj);

   /* See if we can unsynchronized write the data into the user's BO. This
    * avoids GPU stalls in unfortunately common user patterns (uploading
    * sequentially into a BO, with draw calls in between each upload).
    *
    * Once we've hit this path, we mark this GL BO as preferring stalling to
    * blits, so that we can hopefully hit this path again in the future
    * (otherwise, an app that might occasionally stall but mostly not will end
    * up with blitting all the time, at the cost of bandwidth)
    */
   if (brw->has_llc) {
      if (offset + size <= intel_obj->gpu_active_start ||
          intel_obj->gpu_active_end <= offset) {
         drm_intel_gem_bo_map_unsynchronized(intel_obj->buffer);
         memcpy(intel_obj->buffer->virtual + offset, data, size);
         drm_intel_bo_unmap(intel_obj->buffer);

         if (intel_obj->gpu_active_end > intel_obj->gpu_active_start)
            intel_obj->prefer_stall_to_blit = true;
         return;
      }
   }

   busy =
      drm_intel_bo_busy(intel_obj->buffer) ||
      drm_intel_bo_references(brw->batch.bo, intel_obj->buffer);

   if (busy) {
      if (size == intel_obj->Base.Size) {
	 /* Replace the current busy bo so the subdata doesn't stall. */
	 drm_intel_bo_unreference(intel_obj->buffer);
	 intel_bufferobj_alloc_buffer(brw, intel_obj);
      } else if (!intel_obj->prefer_stall_to_blit) {
         perf_debug("Using a blit copy to avoid stalling on "
                    "glBufferSubData(%ld, %ld) (%ldkb) to a busy "
                    "(%d-%d) buffer object.\n",
                    (long)offset, (long)offset + size, (long)(size/1024),
                    intel_obj->gpu_active_start,
                    intel_obj->gpu_active_end);
	 drm_intel_bo *temp_bo =
	    drm_intel_bo_alloc(brw->bufmgr, "subdata temp", size, 64);

	 drm_intel_bo_subdata(temp_bo, 0, size, data);

	 intel_emit_linear_blit(brw,
				intel_obj->buffer, offset,
				temp_bo, 0,
				size);

	 drm_intel_bo_unreference(temp_bo);
         return;
      } else {
         perf_debug("Stalling on glBufferSubData(%ld, %ld) (%ldkb) to a busy "
                    "(%d-%d) buffer object.  Use glMapBufferRange() to "
                    "avoid this.\n",
                    (long)offset, (long)offset + size, (long)(size/1024),
                    intel_obj->gpu_active_start,
                    intel_obj->gpu_active_end);
         intel_batchbuffer_flush(brw);
      }
   }

   drm_intel_bo_subdata(intel_obj->buffer, offset, size, data);
   intel_bufferobj_mark_inactive(intel_obj);
}


/**
 * The GetBufferSubData() driver hook.
 *
 * Implements glGetBufferSubData(), which copies a subrange of a buffer
 * object into user memory.
 */
static void
intel_bufferobj_get_subdata(struct gl_context * ctx,
                            GLintptrARB offset,
                            GLsizeiptrARB size,
                            GLvoid * data, struct gl_buffer_object *obj)
{
   struct intel_buffer_object *intel_obj = intel_buffer_object(obj);
   struct brw_context *brw = brw_context(ctx);

   assert(intel_obj);
   if (drm_intel_bo_references(brw->batch.bo, intel_obj->buffer)) {
      intel_batchbuffer_flush(brw);
   }
   drm_intel_bo_get_subdata(intel_obj->buffer, offset, size, data);

   intel_bufferobj_mark_inactive(intel_obj);
}


/**
 * The MapBufferRange() driver hook.
 *
 * This implements both glMapBufferRange() and glMapBuffer().
 *
 * The goal of this extension is to allow apps to accumulate their rendering
 * at the same time as they accumulate their buffer object.  Without it,
 * you'd end up blocking on execution of rendering every time you mapped
 * the buffer to put new data in.
 *
 * We support it in 3 ways: If unsynchronized, then don't bother
 * flushing the batchbuffer before mapping the buffer, which can save blocking
 * in many cases.  If we would still block, and they allow the whole buffer
 * to be invalidated, then just allocate a new buffer to replace the old one.
 * If not, and we'd block, and they allow the subrange of the buffer to be
 * invalidated, then we can make a new little BO, let them write into that,
 * and blit it into the real BO at unmap time.
 */
static void *
intel_bufferobj_map_range(struct gl_context * ctx,
			  GLintptr offset, GLsizeiptr length,
			  GLbitfield access, struct gl_buffer_object *obj,
                          gl_map_buffer_index index)
{
   struct brw_context *brw = brw_context(ctx);
   struct intel_buffer_object *intel_obj = intel_buffer_object(obj);

   assert(intel_obj);

   /* _mesa_MapBufferRange (GL entrypoint) sets these, but the vbo module also
    * internally uses our functions directly.
    */
   obj->Mappings[index].Offset = offset;
   obj->Mappings[index].Length = length;
   obj->Mappings[index].AccessFlags = access;

   if (intel_obj->buffer == NULL) {
      obj->Mappings[index].Pointer = NULL;
      return NULL;
   }

   /* If the access is synchronized (like a normal buffer mapping), then get
    * things flushed out so the later mapping syncs appropriately through GEM.
    * If the user doesn't care about existing buffer contents and mapping would
    * cause us to block, then throw out the old buffer.
    *
    * If they set INVALIDATE_BUFFER, we can pitch the current contents to
    * achieve the required synchronization.
    */
   if (!(access & GL_MAP_UNSYNCHRONIZED_BIT)) {
      if (drm_intel_bo_references(brw->batch.bo, intel_obj->buffer)) {
	 if (access & GL_MAP_INVALIDATE_BUFFER_BIT) {
	    drm_intel_bo_unreference(intel_obj->buffer);
	    intel_bufferobj_alloc_buffer(brw, intel_obj);
	 } else {
            perf_debug("Stalling on the GPU for mapping a busy buffer "
                       "object\n");
	    intel_batchbuffer_flush(brw);
	 }
      } else if (drm_intel_bo_busy(intel_obj->buffer) &&
		 (access & GL_MAP_INVALIDATE_BUFFER_BIT)) {
	 drm_intel_bo_unreference(intel_obj->buffer);
	 intel_bufferobj_alloc_buffer(brw, intel_obj);
      }
   }

   /* If the user is mapping a range of an active buffer object but
    * doesn't require the current contents of that range, make a new
    * BO, and we'll copy what they put in there out at unmap or
    * FlushRange time.
    *
    * That is, unless they're looking for a persistent mapping -- we would
    * need to do blits in the MemoryBarrier call, and it's easier to just do a
    * GPU stall and do a mapping.
    */
   if (!(access & (GL_MAP_UNSYNCHRONIZED_BIT | GL_MAP_PERSISTENT_BIT)) &&
       (access & GL_MAP_INVALIDATE_RANGE_BIT) &&
       drm_intel_bo_busy(intel_obj->buffer)) {
      /* Ensure that the base alignment of the allocation meets the alignment
       * guarantees the driver has advertised to the application.
       */
      const unsigned alignment = ctx->Const.MinMapBufferAlignment;

      intel_obj->map_extra[index] = (uintptr_t) offset % alignment;
      intel_obj->range_map_bo[index] = drm_intel_bo_alloc(brw->bufmgr,
                                                          "BO blit temp",
                                                          length +
                                                          intel_obj->map_extra[index],
                                                          alignment);
      if (brw->has_llc) {
         drm_intel_bo_map(intel_obj->range_map_bo[index],
                          (access & GL_MAP_WRITE_BIT) != 0);
      } else {
         drm_intel_gem_bo_map_gtt(intel_obj->range_map_bo[index]);
      }
      obj->Mappings[index].Pointer =
         intel_obj->range_map_bo[index]->virtual + intel_obj->map_extra[index];
      return obj->Mappings[index].Pointer;
   }

   if (access & GL_MAP_UNSYNCHRONIZED_BIT)
      drm_intel_gem_bo_map_unsynchronized(intel_obj->buffer);
   else if (!brw->has_llc && (!(access & GL_MAP_READ_BIT) ||
                              (access & GL_MAP_PERSISTENT_BIT))) {
      drm_intel_gem_bo_map_gtt(intel_obj->buffer);
      intel_bufferobj_mark_inactive(intel_obj);
   } else {
      drm_intel_bo_map(intel_obj->buffer, (access & GL_MAP_WRITE_BIT) != 0);
      intel_bufferobj_mark_inactive(intel_obj);
   }

   obj->Mappings[index].Pointer = intel_obj->buffer->virtual + offset;
   return obj->Mappings[index].Pointer;
}

/**
 * The FlushMappedBufferRange() driver hook.
 *
 * Implements glFlushMappedBufferRange(), which signifies that modifications
 * have been made to a range of a mapped buffer, and it should be flushed.
 *
 * This is only used for buffers mapped with GL_MAP_FLUSH_EXPLICIT_BIT.
 *
 * Ideally we'd use a BO to avoid taking up cache space for the temporary
 * data, but FlushMappedBufferRange may be followed by further writes to
 * the pointer, so we would have to re-map after emitting our blit, which
 * would defeat the point.
 */
static void
intel_bufferobj_flush_mapped_range(struct gl_context *ctx,
				   GLintptr offset, GLsizeiptr length,
				   struct gl_buffer_object *obj,
                                   gl_map_buffer_index index)
{
   struct brw_context *brw = brw_context(ctx);
   struct intel_buffer_object *intel_obj = intel_buffer_object(obj);
   GLbitfield access = obj->Mappings[index].AccessFlags;

   assert(access & GL_MAP_FLUSH_EXPLICIT_BIT);

   /* If we gave a direct mapping of the buffer instead of using a temporary,
    * then there's nothing to do.
    */
   if (intel_obj->range_map_bo[index] == NULL)
      return;

   if (length == 0)
      return;

   /* Note that we're not unmapping our buffer while executing the blit.  We
    * need to have a mapping still at the end of this call, since the user
    * gets to make further modifications and glFlushMappedBufferRange() calls.
    * This is safe, because:
    *
    * - On LLC platforms, we're using a CPU mapping that's coherent with the
    *   GPU (except for the render caches), so the kernel doesn't need to do
    *   any flushing work for us except for what happens at batch exec time
    *   anyway.
    *
    * - On non-LLC platforms, we're using a GTT mapping that writes directly
    *   to system memory (except for the chipset cache that gets flushed at
    *   batch exec time).
    *
    * In both cases we don't need to stall for the previous blit to complete
    * so we can re-map (and we definitely don't want to, since that would be
    * slow): If the user edits a part of their buffer that's previously been
    * blitted, then our lack of synchoronization is fine, because either
    * they'll get some too-new data in the first blit and not do another blit
    * of that area (but in that case the results are undefined), or they'll do
    * another blit of that area and the complete newer data will land the
    * second time.
    */
   intel_emit_linear_blit(brw,
			  intel_obj->buffer,
                          obj->Mappings[index].Offset + offset,
			  intel_obj->range_map_bo[index],
                          intel_obj->map_extra[index] + offset,
			  length);
   intel_bufferobj_mark_gpu_usage(intel_obj,
                                  obj->Mappings[index].Offset + offset,
                                  length);
}


/**
 * The UnmapBuffer() driver hook.
 *
 * Implements glUnmapBuffer().
 */
static GLboolean
intel_bufferobj_unmap(struct gl_context * ctx, struct gl_buffer_object *obj,
                      gl_map_buffer_index index)
{
   struct brw_context *brw = brw_context(ctx);
   struct intel_buffer_object *intel_obj = intel_buffer_object(obj);

   assert(intel_obj);
   assert(obj->Mappings[index].Pointer);
   if (intel_obj->range_map_bo[index] != NULL) {
      drm_intel_bo_unmap(intel_obj->range_map_bo[index]);

      if (!(obj->Mappings[index].AccessFlags & GL_MAP_FLUSH_EXPLICIT_BIT)) {
         intel_emit_linear_blit(brw,
                                intel_obj->buffer, obj->Mappings[index].Offset,
                                intel_obj->range_map_bo[index],
                                intel_obj->map_extra[index],
                                obj->Mappings[index].Length);
         intel_bufferobj_mark_gpu_usage(intel_obj, obj->Mappings[index].Offset,
                                        obj->Mappings[index].Length);
      }

      /* Since we've emitted some blits to buffers that will (likely) be used
       * in rendering operations in other cache domains in this batch, emit a
       * flush.  Once again, we wish for a domain tracker in libdrm to cover
       * usage inside of a batchbuffer.
       */
      intel_batchbuffer_emit_mi_flush(brw);

      drm_intel_bo_unreference(intel_obj->range_map_bo[index]);
      intel_obj->range_map_bo[index] = NULL;
   } else if (intel_obj->buffer != NULL) {
      drm_intel_bo_unmap(intel_obj->buffer);
   }
   obj->Mappings[index].Pointer = NULL;
   obj->Mappings[index].Offset = 0;
   obj->Mappings[index].Length = 0;

   return true;
}

/**
 * Gets a pointer to the object's BO, and marks the given range as being used
 * on the GPU.
 *
 * Anywhere that uses buffer objects in the pipeline should be using this to
 * mark the range of the buffer that is being accessed by the pipeline.
 */
drm_intel_bo *
intel_bufferobj_buffer(struct brw_context *brw,
                       struct intel_buffer_object *intel_obj,
                       uint32_t offset, uint32_t size)
{
   /* This is needed so that things like transform feedback and texture buffer
    * objects that need a BO but don't want to check that they exist for
    * draw-time validation can just always get a BO from a GL buffer object.
    */
   if (intel_obj->buffer == NULL)
      intel_bufferobj_alloc_buffer(brw, intel_obj);

   intel_bufferobj_mark_gpu_usage(intel_obj, offset, size);

   return intel_obj->buffer;
}

/**
 * The CopyBufferSubData() driver hook.
 *
 * Implements glCopyBufferSubData(), which copies a portion of one buffer
 * object's data to another.  Independent source and destination offsets
 * are allowed.
 */
static void
intel_bufferobj_copy_subdata(struct gl_context *ctx,
			     struct gl_buffer_object *src,
			     struct gl_buffer_object *dst,
			     GLintptr read_offset, GLintptr write_offset,
			     GLsizeiptr size)
{
   struct brw_context *brw = brw_context(ctx);
   struct intel_buffer_object *intel_src = intel_buffer_object(src);
   struct intel_buffer_object *intel_dst = intel_buffer_object(dst);
   drm_intel_bo *src_bo, *dst_bo;

   if (size == 0)
      return;

   dst_bo = intel_bufferobj_buffer(brw, intel_dst, write_offset, size);
   src_bo = intel_bufferobj_buffer(brw, intel_src, read_offset, size);

   intel_emit_linear_blit(brw,
			  dst_bo, write_offset,
			  src_bo, read_offset, size);

   /* Since we've emitted some blits to buffers that will (likely) be used
    * in rendering operations in other cache domains in this batch, emit a
    * flush.  Once again, we wish for a domain tracker in libdrm to cover
    * usage inside of a batchbuffer.
    */
   intel_batchbuffer_emit_mi_flush(brw);
}

void
intelInitBufferObjectFuncs(struct dd_function_table *functions)
{
   functions->NewBufferObject = intel_bufferobj_alloc;
   functions->DeleteBuffer = intel_bufferobj_free;
   functions->BufferData = intel_bufferobj_data;
   functions->BufferSubData = intel_bufferobj_subdata;
   functions->GetBufferSubData = intel_bufferobj_get_subdata;
   functions->MapBufferRange = intel_bufferobj_map_range;
   functions->FlushMappedBufferRange = intel_bufferobj_flush_mapped_range;
   functions->UnmapBuffer = intel_bufferobj_unmap;
   functions->CopyBufferSubData = intel_bufferobj_copy_subdata;
}
@


1.6
log
@Merge Mesa 10.2.9
@
text
@@


1.5
log
@Merge Mesa 10.4.3
Tested by matthieu@@ mpi@@ and myself.  landry@@ ran a ports bulk build.
kettenis@@ tracked down the cause of an alignment fault on archs
that require strict eight byte pointer alignment.
@
text
@d84 4
d89 1
a89 1
mark_buffer_gpu_usage(struct intel_buffer_object *intel_obj,
d97 1
a97 1
mark_buffer_inactive(struct intel_buffer_object *intel_obj)
d105 2
a106 2
alloc_buffer_object(struct brw_context *brw,
                    struct intel_buffer_object *intel_obj)
d113 1
a113 6
   if (intel_obj->Base.UsageHistory & USAGE_UNIFORM_BUFFER)
      brw->state.dirty.brw |= BRW_NEW_UNIFORM_BUFFER;
   if (intel_obj->Base.UsageHistory & USAGE_TEXTURE_BUFFER)
      brw->state.dirty.brw |= BRW_NEW_TEXTURE_BUFFER;
   if (intel_obj->Base.UsageHistory & USAGE_ATOMIC_COUNTER_BUFFER)
      brw->state.dirty.brw |= BRW_NEW_ATOMIC_BUFFER;
d115 1
a115 1
   mark_buffer_inactive(intel_obj);
d136 1
a136 1
brw_new_buffer_object(struct gl_context * ctx, GLuint name)
a138 3
   if (!obj) {
      _mesa_error_no_memory(__func__);
   }
d140 1
a140 1
   _mesa_initialize_buffer_object(ctx, &obj->Base, name);
d153 1
a153 1
brw_delete_buffer(struct gl_context * ctx, struct gl_buffer_object *obj)
d181 7
a187 7
brw_buffer_data(struct gl_context *ctx,
                GLenum target,
                GLsizeiptrARB size,
                const GLvoid *data,
                GLenum usage,
                GLbitfield storageFlags,
                struct gl_buffer_object *obj)
d207 1
a207 1
      alloc_buffer_object(brw, intel_obj);
d229 4
a232 5
brw_buffer_subdata(struct gl_context *ctx,
                   GLintptrARB offset,
                   GLsizeiptrARB size,
                   const GLvoid *data,
                   struct gl_buffer_object *obj)
d273 1
a273 1
	 alloc_buffer_object(brw, intel_obj);
d305 1
a305 1
   mark_buffer_inactive(intel_obj);
d316 4
a319 5
brw_get_buffer_subdata(struct gl_context *ctx,
                       GLintptrARB offset,
                       GLsizeiptrARB size,
                       GLvoid *data,
                       struct gl_buffer_object *obj)
d330 1
a330 1
   mark_buffer_inactive(intel_obj);
d353 4
a356 4
brw_map_buffer_range(struct gl_context *ctx,
                     GLintptr offset, GLsizeiptr length,
                     GLbitfield access, struct gl_buffer_object *obj,
                     gl_map_buffer_index index)
d387 1
a387 1
	    alloc_buffer_object(brw, intel_obj);
d396 1
a396 1
	 alloc_buffer_object(brw, intel_obj);
d424 2
a425 2
         brw_bo_map(brw, intel_obj->range_map_bo[index],
                    (access & GL_MAP_WRITE_BIT) != 0, "range-map");
d439 1
a439 1
      mark_buffer_inactive(intel_obj);
d441 2
a442 3
      brw_bo_map(brw, intel_obj->buffer, (access & GL_MAP_WRITE_BIT) != 0,
                 "MapBufferRange");
      mark_buffer_inactive(intel_obj);
d463 4
a466 4
brw_flush_mapped_buffer_range(struct gl_context *ctx,
                              GLintptr offset, GLsizeiptr length,
                              struct gl_buffer_object *obj,
                              gl_map_buffer_index index)
d470 1
d472 1
a472 1
   assert(obj->Mappings[index].AccessFlags & GL_MAP_FLUSH_EXPLICIT_BIT);
d512 3
a514 3
   mark_buffer_gpu_usage(intel_obj,
                         obj->Mappings[index].Offset + offset,
                         length);
d524 2
a525 3
brw_unmap_buffer(struct gl_context *ctx,
                 struct gl_buffer_object *obj,
                 gl_map_buffer_index index)
d541 2
a542 2
         mark_buffer_gpu_usage(intel_obj, obj->Mappings[index].Offset,
                               obj->Mappings[index].Length);
d581 1
a581 1
      alloc_buffer_object(brw, intel_obj);
d583 1
a583 1
   mark_buffer_gpu_usage(intel_obj, offset, size);
d596 5
a600 5
brw_copy_buffer_subdata(struct gl_context *ctx,
                        struct gl_buffer_object *src,
                        struct gl_buffer_object *dst,
                        GLintptr read_offset, GLintptr write_offset,
                        GLsizeiptr size)
d628 9
a636 9
   functions->NewBufferObject = brw_new_buffer_object;
   functions->DeleteBuffer = brw_delete_buffer;
   functions->BufferData = brw_buffer_data;
   functions->BufferSubData = brw_buffer_subdata;
   functions->GetBufferSubData = brw_get_buffer_subdata;
   functions->MapBufferRange = brw_map_buffer_range;
   functions->FlushMappedBufferRange = brw_flush_mapped_buffer_range;
   functions->UnmapBuffer = brw_unmap_buffer;
   functions->CopyBufferSubData = brw_copy_buffer_subdata;
@


1.4
log
@Merge Mesa 10.2.3
tested by matthieu@@ kettenis@@ mpi@@ brett@@ and myself across a
diverse range of hardware
@
text
@a83 4
static GLboolean
intel_bufferobj_unmap(struct gl_context * ctx, struct gl_buffer_object *obj,
                      gl_map_buffer_index index);

d85 1
a85 1
intel_bufferobj_mark_gpu_usage(struct intel_buffer_object *intel_obj,
d93 1
a93 1
intel_bufferobj_mark_inactive(struct intel_buffer_object *intel_obj)
d101 2
a102 2
intel_bufferobj_alloc_buffer(struct brw_context *brw,
			     struct intel_buffer_object *intel_obj)
d109 6
a114 1
   brw->state.dirty.brw |= BRW_NEW_UNIFORM_BUFFER;
d116 1
a116 1
   intel_bufferobj_mark_inactive(intel_obj);
d137 1
a137 1
intel_bufferobj_alloc(struct gl_context * ctx, GLuint name, GLenum target)
d140 3
d144 1
a144 1
   _mesa_initialize_buffer_object(ctx, &obj->Base, name, target);
d157 1
a157 1
intel_bufferobj_free(struct gl_context * ctx, struct gl_buffer_object *obj)
d185 7
a191 7
intel_bufferobj_data(struct gl_context * ctx,
                     GLenum target,
                     GLsizeiptrARB size,
                     const GLvoid * data,
                     GLenum usage,
                     GLbitfield storageFlags,
                     struct gl_buffer_object *obj)
d211 1
a211 1
      intel_bufferobj_alloc_buffer(brw, intel_obj);
d233 5
a237 4
intel_bufferobj_subdata(struct gl_context * ctx,
                        GLintptrARB offset,
                        GLsizeiptrARB size,
                        const GLvoid * data, struct gl_buffer_object *obj)
d278 1
a278 1
	 intel_bufferobj_alloc_buffer(brw, intel_obj);
d310 1
a310 1
   intel_bufferobj_mark_inactive(intel_obj);
d321 5
a325 4
intel_bufferobj_get_subdata(struct gl_context * ctx,
                            GLintptrARB offset,
                            GLsizeiptrARB size,
                            GLvoid * data, struct gl_buffer_object *obj)
d336 1
a336 1
   intel_bufferobj_mark_inactive(intel_obj);
d359 4
a362 4
intel_bufferobj_map_range(struct gl_context * ctx,
			  GLintptr offset, GLsizeiptr length,
			  GLbitfield access, struct gl_buffer_object *obj,
                          gl_map_buffer_index index)
d393 1
a393 1
	    intel_bufferobj_alloc_buffer(brw, intel_obj);
d402 1
a402 1
	 intel_bufferobj_alloc_buffer(brw, intel_obj);
d430 2
a431 2
         drm_intel_bo_map(intel_obj->range_map_bo[index],
                          (access & GL_MAP_WRITE_BIT) != 0);
d445 1
a445 1
      intel_bufferobj_mark_inactive(intel_obj);
d447 3
a449 2
      drm_intel_bo_map(intel_obj->buffer, (access & GL_MAP_WRITE_BIT) != 0);
      intel_bufferobj_mark_inactive(intel_obj);
d470 4
a473 4
intel_bufferobj_flush_mapped_range(struct gl_context *ctx,
				   GLintptr offset, GLsizeiptr length,
				   struct gl_buffer_object *obj,
                                   gl_map_buffer_index index)
a476 1
   GLbitfield access = obj->Mappings[index].AccessFlags;
d478 1
a478 1
   assert(access & GL_MAP_FLUSH_EXPLICIT_BIT);
d518 3
a520 3
   intel_bufferobj_mark_gpu_usage(intel_obj,
                                  obj->Mappings[index].Offset + offset,
                                  length);
d530 3
a532 2
intel_bufferobj_unmap(struct gl_context * ctx, struct gl_buffer_object *obj,
                      gl_map_buffer_index index)
d548 2
a549 2
         intel_bufferobj_mark_gpu_usage(intel_obj, obj->Mappings[index].Offset,
                                        obj->Mappings[index].Length);
d588 1
a588 1
      intel_bufferobj_alloc_buffer(brw, intel_obj);
d590 1
a590 1
   intel_bufferobj_mark_gpu_usage(intel_obj, offset, size);
d603 5
a607 5
intel_bufferobj_copy_subdata(struct gl_context *ctx,
			     struct gl_buffer_object *src,
			     struct gl_buffer_object *dst,
			     GLintptr read_offset, GLintptr write_offset,
			     GLsizeiptr size)
d635 9
a643 9
   functions->NewBufferObject = intel_bufferobj_alloc;
   functions->DeleteBuffer = intel_bufferobj_free;
   functions->BufferData = intel_bufferobj_data;
   functions->BufferSubData = intel_bufferobj_subdata;
   functions->GetBufferSubData = intel_bufferobj_get_subdata;
   functions->MapBufferRange = intel_bufferobj_map_range;
   functions->FlushMappedBufferRange = intel_bufferobj_flush_mapped_range;
   functions->UnmapBuffer = intel_bufferobj_unmap;
   functions->CopyBufferSubData = intel_bufferobj_copy_subdata;
@


1.3
log
@Merge Mesa 9.2.0
@
text
@d2 2
a3 2
 * 
 * Copyright 2003 Tungsten Graphics, Inc., Cedar Park, Texas.
d5 1
a5 1
 * 
d13 1
a13 1
 * 
d17 1
a17 1
 * 
d21 1
a21 1
 * IN NO EVENT SHALL TUNGSTEN GRAPHICS AND/OR ITS SUPPLIERS BE LIABLE FOR
d25 1
a25 1
 * 
d28 5
a42 3
#include "intel_fbo.h"
#include "intel_mipmap_tree.h"
#include "intel_regions.h"
d44 39
a82 1
#include "brw_context.h"
d85 17
a101 1
intel_bufferobj_unmap(struct gl_context * ctx, struct gl_buffer_object *obj);
d114 2
a122 1
   intel_obj->offset = 0;
d126 4
d148 3
a150 2
 * Deallocate/free a vertex/pixel buffer object.
 * Called via glDeleteBuffersARB().
d163 1
a163 2
   if (obj->Pointer)
      intel_bufferobj_unmap(ctx, obj);
a169 1

d171 7
a177 4
 * Allocate space for and store data in a buffer object.  Any data that was
 * previously stored in the buffer object is lost.  If data is NULL,
 * memory will be allocated, but no copy will occur.
 * Called via ctx->Driver.BufferData().
d185 3
a187 1
                     GLenum usage, struct gl_buffer_object *obj)
d198 1
d200 2
a201 1
   assert(!obj->Pointer); /* Mesa should have unmapped it */
d220 7
a226 4
 * Replace data in a subrange of buffer object.  If the data range
 * specified by size + offset extends beyond the end of the buffer or
 * if data is NULL, no copy is performed.
 * Called via glBufferSubDataARB().
d243 22
d271 1
a271 1
	 /* Replace the current busy bo with fresh data. */
d274 7
a280 5
	 drm_intel_bo_subdata(intel_obj->buffer, 0, size, data);
      } else {
         perf_debug("Using a blit copy to avoid stalling on %ldb "
                    "glBufferSubData() to a busy buffer object.\n",
                    (long)size);
d292 9
a301 2
   } else {
      drm_intel_bo_subdata(intel_obj->buffer, offset, size, data);
d303 3
d310 4
a313 1
 * Called via glGetBufferSubDataARB().
d329 2
a333 1

d335 3
a337 1
 * Called via glMapBufferRange and glMapBuffer
d355 2
a356 1
			  GLbitfield access, struct gl_buffer_object *obj)
d366 3
a368 3
   obj->Offset = offset;
   obj->Length = length;
   obj->AccessFlags = access;
d371 1
a371 1
      obj->Pointer = NULL;
d391 1
a391 1
	    intel_flush(ctx);
d404 4
d409 2
a410 1
   if ((access & GL_MAP_INVALIDATE_RANGE_BIT) &&
d412 14
a425 3
      if (access & GL_MAP_FLUSH_EXPLICIT_BIT) {
	 intel_obj->range_map_buffer = malloc(length);
	 obj->Pointer = intel_obj->range_map_buffer;
d427 1
a427 10
	 intel_obj->range_map_bo = drm_intel_bo_alloc(brw->bufmgr,
						      "range map",
						      length, 64);
	 if (!(access & GL_MAP_READ_BIT)) {
	    drm_intel_gem_bo_map_gtt(intel_obj->range_map_bo);
	 } else {
	    drm_intel_bo_map(intel_obj->range_map_bo,
			     (access & GL_MAP_WRITE_BIT) != 0);
	 }
	 obj->Pointer = intel_obj->range_map_bo->virtual;
d429 3
a431 1
      return obj->Pointer;
d436 2
a437 1
   else if (!(access & GL_MAP_READ_BIT)) {
d439 1
d442 1
d445 2
a446 2
   obj->Pointer = intel_obj->buffer->virtual + offset;
   return obj->Pointer;
d449 9
a457 1
/* Ideally we'd use a BO to avoid taking up cache space for the temporary
d465 2
a466 1
				   struct gl_buffer_object *obj)
d470 3
a472 1
   drm_intel_bo *temp_bo;
d474 2
a475 2
   /* Unless we're in the range map using a temporary system buffer,
    * there's no work to do.
d477 1
a477 1
   if (intel_obj->range_map_buffer == NULL)
d483 23
a505 4
   temp_bo = drm_intel_bo_alloc(brw->bufmgr, "range map flush", length, 64);

   drm_intel_bo_subdata(temp_bo, 0, length, intel_obj->range_map_buffer);

d507 4
a510 2
			  intel_obj->buffer, obj->Offset + offset,
			  temp_bo, 0,
d512 3
a514 2

   drm_intel_bo_unreference(temp_bo);
d519 3
a521 1
 * Called via glUnmapBuffer().
d524 2
a525 1
intel_bufferobj_unmap(struct gl_context * ctx, struct gl_buffer_object *obj)
d531 13
a543 17
   assert(obj->Pointer);
   if (intel_obj->range_map_buffer != NULL) {
      /* Since we've emitted some blits to buffers that will (likely) be used
       * in rendering operations in other cache domains in this batch, emit a
       * flush.  Once again, we wish for a domain tracker in libdrm to cover
       * usage inside of a batchbuffer.
       */
      intel_batchbuffer_emit_mi_flush(brw);
      free(intel_obj->range_map_buffer);
      intel_obj->range_map_buffer = NULL;
   } else if (intel_obj->range_map_bo != NULL) {
      drm_intel_bo_unmap(intel_obj->range_map_bo);

      intel_emit_linear_blit(brw,
			     intel_obj->buffer, obj->Offset,
			     intel_obj->range_map_bo, 0,
			     obj->Length);
d552 2
a553 2
      drm_intel_bo_unreference(intel_obj->range_map_bo);
      intel_obj->range_map_bo = NULL;
d557 3
a559 3
   obj->Pointer = NULL;
   obj->Offset = 0;
   obj->Length = 0;
d564 7
d574 1
a574 1
		       GLuint flag)
d576 4
d583 1
a583 4
   return intel_obj->buffer;
}

#define INTEL_UPLOAD_SIZE (64*1024)
a584 137
void
intel_upload_finish(struct brw_context *brw)
{
   if (!brw->upload.bo)
	   return;

   if (brw->upload.buffer_len) {
	   drm_intel_bo_subdata(brw->upload.bo,
				brw->upload.buffer_offset,
				brw->upload.buffer_len,
				brw->upload.buffer);
	   brw->upload.buffer_len = 0;
   }

   drm_intel_bo_unreference(brw->upload.bo);
   brw->upload.bo = NULL;
}

static void wrap_buffers(struct brw_context *brw, GLuint size)
{
   intel_upload_finish(brw);

   if (size < INTEL_UPLOAD_SIZE)
      size = INTEL_UPLOAD_SIZE;

   brw->upload.bo = drm_intel_bo_alloc(brw->bufmgr, "upload", size, 0);
   brw->upload.offset = 0;
}

void intel_upload_data(struct brw_context *brw,
		       const void *ptr, GLuint size, GLuint align,
		       drm_intel_bo **return_bo,
		       GLuint *return_offset)
{
   GLuint base, delta;

   base = (brw->upload.offset + align - 1) / align * align;
   if (brw->upload.bo == NULL || base + size > brw->upload.bo->size) {
      wrap_buffers(brw, size);
      base = 0;
   }

   drm_intel_bo_reference(brw->upload.bo);
   *return_bo = brw->upload.bo;
   *return_offset = base;

   delta = base - brw->upload.offset;
   if (brw->upload.buffer_len &&
       brw->upload.buffer_len + delta + size > sizeof(brw->upload.buffer))
   {
      drm_intel_bo_subdata(brw->upload.bo,
			   brw->upload.buffer_offset,
			   brw->upload.buffer_len,
			   brw->upload.buffer);
      brw->upload.buffer_len = 0;
   }

   if (size < sizeof(brw->upload.buffer))
   {
      if (brw->upload.buffer_len == 0)
	 brw->upload.buffer_offset = base;
      else
	 brw->upload.buffer_len += delta;

      memcpy(brw->upload.buffer + brw->upload.buffer_len, ptr, size);
      brw->upload.buffer_len += size;
   }
   else
   {
      drm_intel_bo_subdata(brw->upload.bo, base, size, ptr);
   }

   brw->upload.offset = base + size;
}

void *intel_upload_map(struct brw_context *brw, GLuint size, GLuint align)
{
   GLuint base, delta;
   char *ptr;

   base = (brw->upload.offset + align - 1) / align * align;
   if (brw->upload.bo == NULL || base + size > brw->upload.bo->size) {
      wrap_buffers(brw, size);
      base = 0;
   }

   delta = base - brw->upload.offset;
   if (brw->upload.buffer_len &&
       brw->upload.buffer_len + delta + size > sizeof(brw->upload.buffer))
   {
      drm_intel_bo_subdata(brw->upload.bo,
			   brw->upload.buffer_offset,
			   brw->upload.buffer_len,
			   brw->upload.buffer);
      brw->upload.buffer_len = 0;
   }

   if (size <= sizeof(brw->upload.buffer)) {
      if (brw->upload.buffer_len == 0)
	 brw->upload.buffer_offset = base;
      else
	 brw->upload.buffer_len += delta;

      ptr = brw->upload.buffer + brw->upload.buffer_len;
      brw->upload.buffer_len += size;
   } else
      ptr = malloc(size);

   return ptr;
}

void intel_upload_unmap(struct brw_context *brw,
			const void *ptr, GLuint size, GLuint align,
			drm_intel_bo **return_bo,
			GLuint *return_offset)
{
   GLuint base;

   base = (brw->upload.offset + align - 1) / align * align;
   if (size > sizeof(brw->upload.buffer)) {
      drm_intel_bo_subdata(brw->upload.bo, base, size, ptr);
      free((void*)ptr);
   }

   drm_intel_bo_reference(brw->upload.bo);
   *return_bo = brw->upload.bo;
   *return_offset = base;

   brw->upload.offset = base + size;
}

drm_intel_bo *
intel_bufferobj_source(struct brw_context *brw,
                       struct intel_buffer_object *intel_obj,
		       GLuint align, GLuint *offset)
{
   *offset = intel_obj->offset;
d588 7
a605 1
   GLuint src_offset;
d610 2
a611 2
   dst_bo = intel_bufferobj_buffer(brw, intel_dst, INTEL_WRITE_PART);
   src_bo = intel_bufferobj_source(brw, intel_src, 64, &src_offset);
d615 1
a615 1
			  src_bo, read_offset + src_offset, size);
a624 123
static GLenum
intel_buffer_purgeable(drm_intel_bo *buffer)
{
   int retained = 0;

   if (buffer != NULL)
      retained = drm_intel_bo_madvise (buffer, I915_MADV_DONTNEED);

   return retained ? GL_VOLATILE_APPLE : GL_RELEASED_APPLE;
}

static GLenum
intel_buffer_object_purgeable(struct gl_context * ctx,
                              struct gl_buffer_object *obj,
                              GLenum option)
{
   struct intel_buffer_object *intel_obj = intel_buffer_object (obj);

   if (intel_obj->buffer != NULL)
      return intel_buffer_purgeable(intel_obj->buffer);

   if (option == GL_RELEASED_APPLE) {
      return GL_RELEASED_APPLE;
   } else {
      /* XXX Create the buffer and madvise(MADV_DONTNEED)? */
      struct brw_context *brw = brw_context(ctx);
      drm_intel_bo *bo = intel_bufferobj_buffer(brw, intel_obj, INTEL_READ);

      return intel_buffer_purgeable(bo);
   }
}

static GLenum
intel_texture_object_purgeable(struct gl_context * ctx,
                               struct gl_texture_object *obj,
                               GLenum option)
{
   struct intel_texture_object *intel;

   (void) ctx;
   (void) option;

   intel = intel_texture_object(obj);
   if (intel->mt == NULL || intel->mt->region == NULL)
      return GL_RELEASED_APPLE;

   return intel_buffer_purgeable(intel->mt->region->bo);
}

static GLenum
intel_render_object_purgeable(struct gl_context * ctx,
                              struct gl_renderbuffer *obj,
                              GLenum option)
{
   struct intel_renderbuffer *intel;

   (void) ctx;
   (void) option;

   intel = intel_renderbuffer(obj);
   if (intel->mt == NULL)
      return GL_RELEASED_APPLE;

   return intel_buffer_purgeable(intel->mt->region->bo);
}

static GLenum
intel_buffer_unpurgeable(drm_intel_bo *buffer)
{
   int retained;

   retained = 0;
   if (buffer != NULL)
      retained = drm_intel_bo_madvise (buffer, I915_MADV_WILLNEED);

   return retained ? GL_RETAINED_APPLE : GL_UNDEFINED_APPLE;
}

static GLenum
intel_buffer_object_unpurgeable(struct gl_context * ctx,
                                struct gl_buffer_object *obj,
                                GLenum option)
{
   (void) ctx;
   (void) option;

   return intel_buffer_unpurgeable(intel_buffer_object (obj)->buffer);
}

static GLenum
intel_texture_object_unpurgeable(struct gl_context * ctx,
                                 struct gl_texture_object *obj,
                                 GLenum option)
{
   struct intel_texture_object *intel;

   (void) ctx;
   (void) option;

   intel = intel_texture_object(obj);
   if (intel->mt == NULL || intel->mt->region == NULL)
      return GL_UNDEFINED_APPLE;

   return intel_buffer_unpurgeable(intel->mt->region->bo);
}

static GLenum
intel_render_object_unpurgeable(struct gl_context * ctx,
                                struct gl_renderbuffer *obj,
                                GLenum option)
{
   struct intel_renderbuffer *intel;

   (void) ctx;
   (void) option;

   intel = intel_renderbuffer(obj);
   if (intel->mt == NULL)
      return GL_UNDEFINED_APPLE;

   return intel_buffer_unpurgeable(intel->mt->region->bo);
}

a636 8

   functions->BufferObjectPurgeable = intel_buffer_object_purgeable;
   functions->TextureObjectPurgeable = intel_texture_object_purgeable;
   functions->RenderObjectPurgeable = intel_render_object_purgeable;

   functions->BufferObjectUnpurgeable = intel_buffer_object_unpurgeable;
   functions->TextureObjectUnpurgeable = intel_texture_object_unpurgeable;
   functions->RenderObjectUnpurgeable = intel_render_object_unpurgeable;
@


1.2
log
@Mesa 7.2, Tested by ckuethe@@, naddy@@, oga@@, and others.
@
text
@d3 1
a3 1
 * Copyright 2006 Tungsten Graphics, Inc., Cedar Park, Texas.
d29 4
a32 3
#include "imports.h"
#include "mtypes.h"
#include "bufferobj.h"
d34 2
a35 1
#include "intel_context.h"
d37 22
a58 1
#include "bufmgr.h"
d60 7
d74 2
a75 3
static struct gl_buffer_object *intel_bufferobj_alloc( GLcontext *ctx, 
						       GLuint name, 
						       GLenum target )
d77 1
a77 2
   struct intel_context *intel = intel_context(ctx);
   struct intel_buffer_object *obj = MALLOC_STRUCT(intel_buffer_object);
d79 1
a79 1
   _mesa_initialize_buffer_object(&obj->Base, name, target);
d81 1
a81 4
   /* XXX:  We generate our own handle, which is different to 'name' above.
    */
   bmGenBuffers(intel, "bufferobj", 1, &obj->buffer, 6);
   assert(obj->buffer);
a85 1

d90 3
a92 4
static void intel_bufferobj_free( GLcontext *ctx, 
				  struct gl_buffer_object *obj )
{ 
   struct intel_context *intel = intel_context(ctx);
d97 9
a105 4
   if (intel_obj->buffer) 
      bmDeleteBuffers( intel, 1, &intel_obj->buffer );
  
   _mesa_free(intel_obj);
d114 2
a115 1
 * Called via glBufferDataARB().
d117 6
a122 6
static void intel_bufferobj_data( GLcontext *ctx, 
				  GLenum target, 
				  GLsizeiptrARB size,
				  const GLvoid *data, 
				  GLenum usage,
				  struct gl_buffer_object *obj )
d124 1
a124 1
   struct intel_context *intel = intel_context(ctx);
d127 1
a127 2
   /* XXX: do something useful with 'usage' (eg. populate flags
    * argument below)
d129 14
a142 1
   assert(intel_obj);
d144 3
a146 2
   obj->Size = size;
   obj->Usage = usage;
d148 1
a148 2
   bmBufferDataAUB(intel, intel_obj->buffer, size, data, 0,
		   0, 0);
d158 5
a162 6
static void intel_bufferobj_subdata( GLcontext *ctx, 
				     GLenum target, 
				     GLintptrARB offset,
				     GLsizeiptrARB size, 
				     const GLvoid * data,
				     struct gl_buffer_object * obj )
d164 1
a164 1
   struct intel_context *intel = intel_context(ctx);
d166 4
d172 30
a201 1
   bmBufferSubDataAUB(intel, intel_obj->buffer, offset, size, data, 0, 0);
d208 5
a212 6
static void intel_bufferobj_get_subdata( GLcontext *ctx, 
					 GLenum target, 
					 GLintptrARB offset,
					 GLsizeiptrARB size, 
					 GLvoid * data,
					 struct gl_buffer_object * obj )
a213 1
   struct intel_context *intel = intel_context(ctx);
d215 1
d218 4
a221 1
   bmBufferGetSubData(intel, intel_obj->buffer, offset, size, data);
d227 14
a240 1
 * Called via glMapBufferARB().
d242 4
a245 4
static void *intel_bufferobj_map( GLcontext *ctx, 
				  GLenum target, 
				  GLenum access,
				  struct gl_buffer_object *obj )
d247 1
a247 1
   struct intel_context *intel = intel_context(ctx);
d250 43
a292 1
   /* XXX: Translate access to flags arg below:
d294 29
a322 3
   assert(intel_obj);
   assert(intel_obj->buffer);
   obj->Pointer = bmMapBuffer(intel, intel_obj->buffer, 0);
d326 35
d363 1
a363 1
 * Called via glMapBufferARB().
d365 2
a366 3
static GLboolean intel_bufferobj_unmap( GLcontext *ctx,
					GLenum target,
					struct gl_buffer_object *obj )
d368 1
a368 1
   struct intel_context *intel = intel_context(ctx);
a371 1
   assert(intel_obj->buffer);
d373 29
a401 1
   bmUnmapBufferAUB(intel, intel_obj->buffer, 0, 0);
d403 92
a494 1
   return GL_TRUE;
d497 1
a497 1
struct buffer *intel_bufferobj_buffer( const struct intel_buffer_object *intel_obj )
d499 60
a558 2
   assert(intel_obj->Base.Name);
   assert(intel_obj->buffer);
d560 74
a633 1
}  
d635 11
a645 1
void intel_bufferobj_init( struct intel_context *intel )
d647 4
a650 1
   GLcontext *ctx = &intel->ctx;
d652 84
a735 7
   ctx->Driver.NewBufferObject = intel_bufferobj_alloc;
   ctx->Driver.DeleteBuffer = intel_bufferobj_free;
   ctx->Driver.BufferData = intel_bufferobj_data;
   ctx->Driver.BufferSubData = intel_bufferobj_subdata;
   ctx->Driver.GetBufferSubData = intel_bufferobj_get_subdata;
   ctx->Driver.MapBuffer = intel_bufferobj_map;
   ctx->Driver.UnmapBuffer = intel_bufferobj_unmap;
@


1.1
log
@Initial revision
@
text
@@


1.1.1.1
log
@Import MesaLibs 6.5.1. (in dist/ since its code is shared between lib 
and xserver)...
@
text
@@


1.1.1.2
log
@Import Mesa 9.2.0
@
text
@d3 1
a3 1
 * Copyright 2003 Tungsten Graphics, Inc., Cedar Park, Texas.
d29 3
a31 4
#include "main/imports.h"
#include "main/mtypes.h"
#include "main/macros.h"
#include "main/bufferobj.h"
d33 1
a33 2
#include "brw_context.h"
#include "intel_blit.h"
d35 1
a35 22
#include "intel_batchbuffer.h"
#include "intel_fbo.h"
#include "intel_mipmap_tree.h"
#include "intel_regions.h"

#include "brw_context.h"

static GLboolean
intel_bufferobj_unmap(struct gl_context * ctx, struct gl_buffer_object *obj);

/** Allocates a new drm_intel_bo to store the data for the buffer object. */
static void
intel_bufferobj_alloc_buffer(struct brw_context *brw,
			     struct intel_buffer_object *intel_obj)
{
   intel_obj->buffer = drm_intel_bo_alloc(brw->bufmgr, "bufferobj",
					  intel_obj->Base.Size, 64);

   /* the buffer might be bound as a uniform buffer, need to update it
    */
   brw->state.dirty.brw |= BRW_NEW_UNIFORM_BUFFER;
}
a36 7
static void
release_buffer(struct intel_buffer_object *intel_obj)
{
   drm_intel_bo_unreference(intel_obj->buffer);
   intel_obj->buffer = NULL;
   intel_obj->offset = 0;
}
d44 3
a46 2
static struct gl_buffer_object *
intel_bufferobj_alloc(struct gl_context * ctx, GLuint name, GLenum target)
d48 2
a49 1
   struct intel_buffer_object *obj = CALLOC_STRUCT(intel_buffer_object);
d51 1
a51 1
   _mesa_initialize_buffer_object(ctx, &obj->Base, name, target);
d53 4
a56 1
   obj->buffer = NULL;
d61 1
d66 4
a69 3
static void
intel_bufferobj_free(struct gl_context * ctx, struct gl_buffer_object *obj)
{
d74 4
a77 9
   /* Buffer objects are automatically unmapped when deleting according
    * to the spec, but Mesa doesn't do UnmapBuffer for us at context destroy
    * (though it does if you call glDeleteBuffers)
    */
   if (obj->Pointer)
      intel_bufferobj_unmap(ctx, obj);

   drm_intel_bo_unreference(intel_obj->buffer);
   free(intel_obj);
d86 1
a86 2
 * Called via ctx->Driver.BufferData().
 * \return true for success, false if out of memory
d88 6
a93 6
static GLboolean
intel_bufferobj_data(struct gl_context * ctx,
                     GLenum target,
                     GLsizeiptrARB size,
                     const GLvoid * data,
                     GLenum usage, struct gl_buffer_object *obj)
d95 1
a95 1
   struct brw_context *brw = brw_context(ctx);
d98 2
a99 1
   /* Part of the ABI, but this function doesn't use it.
d101 1
a101 9
   (void) target;

   intel_obj->Base.Size = size;
   intel_obj->Base.Usage = usage;

   assert(!obj->Pointer); /* Mesa should have unmapped it */

   if (intel_obj->buffer != NULL)
      release_buffer(intel_obj);
d103 2
a104 4
   if (size != 0) {
      intel_bufferobj_alloc_buffer(brw, intel_obj);
      if (!intel_obj->buffer)
         return false;
d106 2
a107 5
      if (data != NULL)
	 drm_intel_bo_subdata(intel_obj->buffer, 0, size, data);
   }

   return true;
d117 6
a122 5
static void
intel_bufferobj_subdata(struct gl_context * ctx,
                        GLintptrARB offset,
                        GLsizeiptrARB size,
                        const GLvoid * data, struct gl_buffer_object *obj)
d124 1
a124 1
   struct brw_context *brw = brw_context(ctx);
a125 4
   bool busy;

   if (size == 0)
      return;
d128 1
a128 30

   busy =
      drm_intel_bo_busy(intel_obj->buffer) ||
      drm_intel_bo_references(brw->batch.bo, intel_obj->buffer);

   if (busy) {
      if (size == intel_obj->Base.Size) {
	 /* Replace the current busy bo with fresh data. */
	 drm_intel_bo_unreference(intel_obj->buffer);
	 intel_bufferobj_alloc_buffer(brw, intel_obj);
	 drm_intel_bo_subdata(intel_obj->buffer, 0, size, data);
      } else {
         perf_debug("Using a blit copy to avoid stalling on %ldb "
                    "glBufferSubData() to a busy buffer object.\n",
                    (long)size);
	 drm_intel_bo *temp_bo =
	    drm_intel_bo_alloc(brw->bufmgr, "subdata temp", size, 64);

	 drm_intel_bo_subdata(temp_bo, 0, size, data);

	 intel_emit_linear_blit(brw,
				intel_obj->buffer, offset,
				temp_bo, 0,
				size);

	 drm_intel_bo_unreference(temp_bo);
      }
   } else {
      drm_intel_bo_subdata(intel_obj->buffer, offset, size, data);
   }
d135 6
a140 5
static void
intel_bufferobj_get_subdata(struct gl_context * ctx,
                            GLintptrARB offset,
                            GLsizeiptrARB size,
                            GLvoid * data, struct gl_buffer_object *obj)
d142 1
a143 1
   struct brw_context *brw = brw_context(ctx);
d146 1
a146 4
   if (drm_intel_bo_references(brw->batch.bo, intel_obj->buffer)) {
      intel_batchbuffer_flush(brw);
   }
   drm_intel_bo_get_subdata(intel_obj->buffer, offset, size, data);
d152 1
a152 14
 * Called via glMapBufferRange and glMapBuffer
 *
 * The goal of this extension is to allow apps to accumulate their rendering
 * at the same time as they accumulate their buffer object.  Without it,
 * you'd end up blocking on execution of rendering every time you mapped
 * the buffer to put new data in.
 *
 * We support it in 3 ways: If unsynchronized, then don't bother
 * flushing the batchbuffer before mapping the buffer, which can save blocking
 * in many cases.  If we would still block, and they allow the whole buffer
 * to be invalidated, then just allocate a new buffer to replace the old one.
 * If not, and we'd block, and they allow the subrange of the buffer to be
 * invalidated, then we can make a new little BO, let them write into that,
 * and blit it into the real BO at unmap time.
d154 4
a157 4
static void *
intel_bufferobj_map_range(struct gl_context * ctx,
			  GLintptr offset, GLsizeiptr length,
			  GLbitfield access, struct gl_buffer_object *obj)
d159 1
a159 1
   struct brw_context *brw = brw_context(ctx);
d162 2
d165 2
a166 72

   /* _mesa_MapBufferRange (GL entrypoint) sets these, but the vbo module also
    * internally uses our functions directly.
    */
   obj->Offset = offset;
   obj->Length = length;
   obj->AccessFlags = access;

   if (intel_obj->buffer == NULL) {
      obj->Pointer = NULL;
      return NULL;
   }

   /* If the access is synchronized (like a normal buffer mapping), then get
    * things flushed out so the later mapping syncs appropriately through GEM.
    * If the user doesn't care about existing buffer contents and mapping would
    * cause us to block, then throw out the old buffer.
    *
    * If they set INVALIDATE_BUFFER, we can pitch the current contents to
    * achieve the required synchronization.
    */
   if (!(access & GL_MAP_UNSYNCHRONIZED_BIT)) {
      if (drm_intel_bo_references(brw->batch.bo, intel_obj->buffer)) {
	 if (access & GL_MAP_INVALIDATE_BUFFER_BIT) {
	    drm_intel_bo_unreference(intel_obj->buffer);
	    intel_bufferobj_alloc_buffer(brw, intel_obj);
	 } else {
            perf_debug("Stalling on the GPU for mapping a busy buffer "
                       "object\n");
	    intel_flush(ctx);
	 }
      } else if (drm_intel_bo_busy(intel_obj->buffer) &&
		 (access & GL_MAP_INVALIDATE_BUFFER_BIT)) {
	 drm_intel_bo_unreference(intel_obj->buffer);
	 intel_bufferobj_alloc_buffer(brw, intel_obj);
      }
   }

   /* If the user is mapping a range of an active buffer object but
    * doesn't require the current contents of that range, make a new
    * BO, and we'll copy what they put in there out at unmap or
    * FlushRange time.
    */
   if ((access & GL_MAP_INVALIDATE_RANGE_BIT) &&
       drm_intel_bo_busy(intel_obj->buffer)) {
      if (access & GL_MAP_FLUSH_EXPLICIT_BIT) {
	 intel_obj->range_map_buffer = malloc(length);
	 obj->Pointer = intel_obj->range_map_buffer;
      } else {
	 intel_obj->range_map_bo = drm_intel_bo_alloc(brw->bufmgr,
						      "range map",
						      length, 64);
	 if (!(access & GL_MAP_READ_BIT)) {
	    drm_intel_gem_bo_map_gtt(intel_obj->range_map_bo);
	 } else {
	    drm_intel_bo_map(intel_obj->range_map_bo,
			     (access & GL_MAP_WRITE_BIT) != 0);
	 }
	 obj->Pointer = intel_obj->range_map_bo->virtual;
      }
      return obj->Pointer;
   }

   if (access & GL_MAP_UNSYNCHRONIZED_BIT)
      drm_intel_gem_bo_map_unsynchronized(intel_obj->buffer);
   else if (!(access & GL_MAP_READ_BIT)) {
      drm_intel_gem_bo_map_gtt(intel_obj->buffer);
   } else {
      drm_intel_bo_map(intel_obj->buffer, (access & GL_MAP_WRITE_BIT) != 0);
   }

   obj->Pointer = intel_obj->buffer->virtual + offset;
a169 35
/* Ideally we'd use a BO to avoid taking up cache space for the temporary
 * data, but FlushMappedBufferRange may be followed by further writes to
 * the pointer, so we would have to re-map after emitting our blit, which
 * would defeat the point.
 */
static void
intel_bufferobj_flush_mapped_range(struct gl_context *ctx,
				   GLintptr offset, GLsizeiptr length,
				   struct gl_buffer_object *obj)
{
   struct brw_context *brw = brw_context(ctx);
   struct intel_buffer_object *intel_obj = intel_buffer_object(obj);
   drm_intel_bo *temp_bo;

   /* Unless we're in the range map using a temporary system buffer,
    * there's no work to do.
    */
   if (intel_obj->range_map_buffer == NULL)
      return;

   if (length == 0)
      return;

   temp_bo = drm_intel_bo_alloc(brw->bufmgr, "range map flush", length, 64);

   drm_intel_bo_subdata(temp_bo, 0, length, intel_obj->range_map_buffer);

   intel_emit_linear_blit(brw,
			  intel_obj->buffer, obj->Offset + offset,
			  temp_bo, 0,
			  length);

   drm_intel_bo_unreference(temp_bo);
}

d172 1
a172 1
 * Called via glUnmapBuffer().
d174 3
a176 2
static GLboolean
intel_bufferobj_unmap(struct gl_context * ctx, struct gl_buffer_object *obj)
d178 1
a178 1
   struct brw_context *brw = brw_context(ctx);
d182 1
d184 1
a184 29
   if (intel_obj->range_map_buffer != NULL) {
      /* Since we've emitted some blits to buffers that will (likely) be used
       * in rendering operations in other cache domains in this batch, emit a
       * flush.  Once again, we wish for a domain tracker in libdrm to cover
       * usage inside of a batchbuffer.
       */
      intel_batchbuffer_emit_mi_flush(brw);
      free(intel_obj->range_map_buffer);
      intel_obj->range_map_buffer = NULL;
   } else if (intel_obj->range_map_bo != NULL) {
      drm_intel_bo_unmap(intel_obj->range_map_bo);

      intel_emit_linear_blit(brw,
			     intel_obj->buffer, obj->Offset,
			     intel_obj->range_map_bo, 0,
			     obj->Length);

      /* Since we've emitted some blits to buffers that will (likely) be used
       * in rendering operations in other cache domains in this batch, emit a
       * flush.  Once again, we wish for a domain tracker in libdrm to cover
       * usage inside of a batchbuffer.
       */
      intel_batchbuffer_emit_mi_flush(brw);

      drm_intel_bo_unreference(intel_obj->range_map_bo);
      intel_obj->range_map_bo = NULL;
   } else if (intel_obj->buffer != NULL) {
      drm_intel_bo_unmap(intel_obj->buffer);
   }
d186 1
a186 92
   obj->Offset = 0;
   obj->Length = 0;

   return true;
}

drm_intel_bo *
intel_bufferobj_buffer(struct brw_context *brw,
                       struct intel_buffer_object *intel_obj,
		       GLuint flag)
{
   if (intel_obj->buffer == NULL)
      intel_bufferobj_alloc_buffer(brw, intel_obj);

   return intel_obj->buffer;
}

#define INTEL_UPLOAD_SIZE (64*1024)

void
intel_upload_finish(struct brw_context *brw)
{
   if (!brw->upload.bo)
	   return;

   if (brw->upload.buffer_len) {
	   drm_intel_bo_subdata(brw->upload.bo,
				brw->upload.buffer_offset,
				brw->upload.buffer_len,
				brw->upload.buffer);
	   brw->upload.buffer_len = 0;
   }

   drm_intel_bo_unreference(brw->upload.bo);
   brw->upload.bo = NULL;
}

static void wrap_buffers(struct brw_context *brw, GLuint size)
{
   intel_upload_finish(brw);

   if (size < INTEL_UPLOAD_SIZE)
      size = INTEL_UPLOAD_SIZE;

   brw->upload.bo = drm_intel_bo_alloc(brw->bufmgr, "upload", size, 0);
   brw->upload.offset = 0;
}

void intel_upload_data(struct brw_context *brw,
		       const void *ptr, GLuint size, GLuint align,
		       drm_intel_bo **return_bo,
		       GLuint *return_offset)
{
   GLuint base, delta;

   base = (brw->upload.offset + align - 1) / align * align;
   if (brw->upload.bo == NULL || base + size > brw->upload.bo->size) {
      wrap_buffers(brw, size);
      base = 0;
   }

   drm_intel_bo_reference(brw->upload.bo);
   *return_bo = brw->upload.bo;
   *return_offset = base;

   delta = base - brw->upload.offset;
   if (brw->upload.buffer_len &&
       brw->upload.buffer_len + delta + size > sizeof(brw->upload.buffer))
   {
      drm_intel_bo_subdata(brw->upload.bo,
			   brw->upload.buffer_offset,
			   brw->upload.buffer_len,
			   brw->upload.buffer);
      brw->upload.buffer_len = 0;
   }

   if (size < sizeof(brw->upload.buffer))
   {
      if (brw->upload.buffer_len == 0)
	 brw->upload.buffer_offset = base;
      else
	 brw->upload.buffer_len += delta;

      memcpy(brw->upload.buffer + brw->upload.buffer_len, ptr, size);
      brw->upload.buffer_len += size;
   }
   else
   {
      drm_intel_bo_subdata(brw->upload.bo, base, size, ptr);
   }

   brw->upload.offset = base + size;
d189 1
a189 1
void *intel_upload_map(struct brw_context *brw, GLuint size, GLuint align)
d191 2
a192 60
   GLuint base, delta;
   char *ptr;

   base = (brw->upload.offset + align - 1) / align * align;
   if (brw->upload.bo == NULL || base + size > brw->upload.bo->size) {
      wrap_buffers(brw, size);
      base = 0;
   }

   delta = base - brw->upload.offset;
   if (brw->upload.buffer_len &&
       brw->upload.buffer_len + delta + size > sizeof(brw->upload.buffer))
   {
      drm_intel_bo_subdata(brw->upload.bo,
			   brw->upload.buffer_offset,
			   brw->upload.buffer_len,
			   brw->upload.buffer);
      brw->upload.buffer_len = 0;
   }

   if (size <= sizeof(brw->upload.buffer)) {
      if (brw->upload.buffer_len == 0)
	 brw->upload.buffer_offset = base;
      else
	 brw->upload.buffer_len += delta;

      ptr = brw->upload.buffer + brw->upload.buffer_len;
      brw->upload.buffer_len += size;
   } else
      ptr = malloc(size);

   return ptr;
}

void intel_upload_unmap(struct brw_context *brw,
			const void *ptr, GLuint size, GLuint align,
			drm_intel_bo **return_bo,
			GLuint *return_offset)
{
   GLuint base;

   base = (brw->upload.offset + align - 1) / align * align;
   if (size > sizeof(brw->upload.buffer)) {
      drm_intel_bo_subdata(brw->upload.bo, base, size, ptr);
      free((void*)ptr);
   }

   drm_intel_bo_reference(brw->upload.bo);
   *return_bo = brw->upload.bo;
   *return_offset = base;

   brw->upload.offset = base + size;
}

drm_intel_bo *
intel_bufferobj_source(struct brw_context *brw,
                       struct intel_buffer_object *intel_obj,
		       GLuint align, GLuint *offset)
{
   *offset = intel_obj->offset;
d194 1
a194 74
}

static void
intel_bufferobj_copy_subdata(struct gl_context *ctx,
			     struct gl_buffer_object *src,
			     struct gl_buffer_object *dst,
			     GLintptr read_offset, GLintptr write_offset,
			     GLsizeiptr size)
{
   struct brw_context *brw = brw_context(ctx);
   struct intel_buffer_object *intel_src = intel_buffer_object(src);
   struct intel_buffer_object *intel_dst = intel_buffer_object(dst);
   drm_intel_bo *src_bo, *dst_bo;
   GLuint src_offset;

   if (size == 0)
      return;

   dst_bo = intel_bufferobj_buffer(brw, intel_dst, INTEL_WRITE_PART);
   src_bo = intel_bufferobj_source(brw, intel_src, 64, &src_offset);

   intel_emit_linear_blit(brw,
			  dst_bo, write_offset,
			  src_bo, read_offset + src_offset, size);

   /* Since we've emitted some blits to buffers that will (likely) be used
    * in rendering operations in other cache domains in this batch, emit a
    * flush.  Once again, we wish for a domain tracker in libdrm to cover
    * usage inside of a batchbuffer.
    */
   intel_batchbuffer_emit_mi_flush(brw);
}

static GLenum
intel_buffer_purgeable(drm_intel_bo *buffer)
{
   int retained = 0;

   if (buffer != NULL)
      retained = drm_intel_bo_madvise (buffer, I915_MADV_DONTNEED);

   return retained ? GL_VOLATILE_APPLE : GL_RELEASED_APPLE;
}

static GLenum
intel_buffer_object_purgeable(struct gl_context * ctx,
                              struct gl_buffer_object *obj,
                              GLenum option)
{
   struct intel_buffer_object *intel_obj = intel_buffer_object (obj);

   if (intel_obj->buffer != NULL)
      return intel_buffer_purgeable(intel_obj->buffer);

   if (option == GL_RELEASED_APPLE) {
      return GL_RELEASED_APPLE;
   } else {
      /* XXX Create the buffer and madvise(MADV_DONTNEED)? */
      struct brw_context *brw = brw_context(ctx);
      drm_intel_bo *bo = intel_bufferobj_buffer(brw, intel_obj, INTEL_READ);

      return intel_buffer_purgeable(bo);
   }
}

static GLenum
intel_texture_object_purgeable(struct gl_context * ctx,
                               struct gl_texture_object *obj,
                               GLenum option)
{
   struct intel_texture_object *intel;

   (void) ctx;
   (void) option;
d196 1
a196 11
   intel = intel_texture_object(obj);
   if (intel->mt == NULL || intel->mt->region == NULL)
      return GL_RELEASED_APPLE;

   return intel_buffer_purgeable(intel->mt->region->bo);
}

static GLenum
intel_render_object_purgeable(struct gl_context * ctx,
                              struct gl_renderbuffer *obj,
                              GLenum option)
d198 1
a198 4
   struct intel_renderbuffer *intel;

   (void) ctx;
   (void) option;
d200 7
a206 84
   intel = intel_renderbuffer(obj);
   if (intel->mt == NULL)
      return GL_RELEASED_APPLE;

   return intel_buffer_purgeable(intel->mt->region->bo);
}

static GLenum
intel_buffer_unpurgeable(drm_intel_bo *buffer)
{
   int retained;

   retained = 0;
   if (buffer != NULL)
      retained = drm_intel_bo_madvise (buffer, I915_MADV_WILLNEED);

   return retained ? GL_RETAINED_APPLE : GL_UNDEFINED_APPLE;
}

static GLenum
intel_buffer_object_unpurgeable(struct gl_context * ctx,
                                struct gl_buffer_object *obj,
                                GLenum option)
{
   (void) ctx;
   (void) option;

   return intel_buffer_unpurgeable(intel_buffer_object (obj)->buffer);
}

static GLenum
intel_texture_object_unpurgeable(struct gl_context * ctx,
                                 struct gl_texture_object *obj,
                                 GLenum option)
{
   struct intel_texture_object *intel;

   (void) ctx;
   (void) option;

   intel = intel_texture_object(obj);
   if (intel->mt == NULL || intel->mt->region == NULL)
      return GL_UNDEFINED_APPLE;

   return intel_buffer_unpurgeable(intel->mt->region->bo);
}

static GLenum
intel_render_object_unpurgeable(struct gl_context * ctx,
                                struct gl_renderbuffer *obj,
                                GLenum option)
{
   struct intel_renderbuffer *intel;

   (void) ctx;
   (void) option;

   intel = intel_renderbuffer(obj);
   if (intel->mt == NULL)
      return GL_UNDEFINED_APPLE;

   return intel_buffer_unpurgeable(intel->mt->region->bo);
}

void
intelInitBufferObjectFuncs(struct dd_function_table *functions)
{
   functions->NewBufferObject = intel_bufferobj_alloc;
   functions->DeleteBuffer = intel_bufferobj_free;
   functions->BufferData = intel_bufferobj_data;
   functions->BufferSubData = intel_bufferobj_subdata;
   functions->GetBufferSubData = intel_bufferobj_get_subdata;
   functions->MapBufferRange = intel_bufferobj_map_range;
   functions->FlushMappedBufferRange = intel_bufferobj_flush_mapped_range;
   functions->UnmapBuffer = intel_bufferobj_unmap;
   functions->CopyBufferSubData = intel_bufferobj_copy_subdata;

   functions->BufferObjectPurgeable = intel_buffer_object_purgeable;
   functions->TextureObjectPurgeable = intel_texture_object_purgeable;
   functions->RenderObjectPurgeable = intel_render_object_purgeable;

   functions->BufferObjectUnpurgeable = intel_buffer_object_unpurgeable;
   functions->TextureObjectUnpurgeable = intel_texture_object_unpurgeable;
   functions->RenderObjectUnpurgeable = intel_render_object_unpurgeable;
@


1.1.1.3
log
@Import Mesa 10.2.3
@
text
@d2 2
a3 2
 *
 * Copyright 2003 VMware, Inc.
d5 1
a5 1
 *
d13 1
a13 1
 *
d17 1
a17 1
 *
d21 1
a21 1
 * IN NO EVENT SHALL VMWARE AND/OR ITS SUPPLIERS BE LIABLE FOR
d25 1
a25 1
 *
a27 5
/**
 * @@file intel_buffer_objects.c
 *
 * This provides core GL buffer object functionality.
 */
d38 3
d42 1
a42 39
/**
 * Map a buffer object; issue performance warnings if mapping causes stalls.
 *
 * This matches the drm_intel_bo_map API, but takes an additional human-readable
 * name for the buffer object to use in the performance debug message.
 */
int
brw_bo_map(struct brw_context *brw,
           drm_intel_bo *bo, int write_enable,
           const char *bo_name)
{
   if (likely(!brw->perf_debug) || !drm_intel_bo_busy(bo))
      return drm_intel_bo_map(bo, write_enable);

   double start_time = get_time();

   int ret = drm_intel_bo_map(bo, write_enable);

   perf_debug("CPU mapping a busy %s BO stalled and took %.03f ms.\n",
              bo_name, (get_time() - start_time) * 1000);

   return ret;
}

int
brw_bo_map_gtt(struct brw_context *brw, drm_intel_bo *bo, const char *bo_name)
{
   if (likely(!brw->perf_debug) || !drm_intel_bo_busy(bo))
      return drm_intel_gem_bo_map_gtt(bo);

   double start_time = get_time();

   int ret = drm_intel_gem_bo_map_gtt(bo);

   perf_debug("GTT mapping a busy %s BO stalled and took %.03f ms.\n",
              bo_name, (get_time() - start_time) * 1000);

   return ret;
}
d45 1
a45 17
intel_bufferobj_unmap(struct gl_context * ctx, struct gl_buffer_object *obj,
                      gl_map_buffer_index index);

static void
intel_bufferobj_mark_gpu_usage(struct intel_buffer_object *intel_obj,
                               uint32_t offset, uint32_t size)
{
   intel_obj->gpu_active_start = MIN2(intel_obj->gpu_active_start, offset);
   intel_obj->gpu_active_end = MAX2(intel_obj->gpu_active_end, offset + size);
}

static void
intel_bufferobj_mark_inactive(struct intel_buffer_object *intel_obj)
{
   intel_obj->gpu_active_start = ~0;
   intel_obj->gpu_active_end = 0;
}
a57 2

   intel_bufferobj_mark_inactive(intel_obj);
d65 1
a68 4
 * The NewBufferObject() driver hook.
 *
 * Allocates a new intel_buffer_object structure and initializes it.
 *
d87 2
a88 3
 * The DeleteBuffer() driver hook.
 *
 * Deletes a single OpenGL buffer object.  Used by glDeleteBuffers().
d101 2
a102 1
   _mesa_buffer_unmap_all_mappings(ctx, obj);
d109 1
d111 4
a114 7
 * The BufferData() driver hook.
 *
 * Implements glBufferData(), which recreates a buffer object's data store
 * and populates it with the given data, if present.
 *
 * Any data that was previously stored in the buffer object is lost.
 *
d122 1
a122 3
                     GLenum usage,
                     GLbitfield storageFlags,
                     struct gl_buffer_object *obj)
a132 1
   intel_obj->Base.StorageFlags = storageFlags;
d134 1
a134 2
   assert(!obj->Mappings[MAP_USER].Pointer); /* Mesa should have unmapped it */
   assert(!obj->Mappings[MAP_INTERNAL].Pointer);
d153 4
a156 7
 * The BufferSubData() driver hook.
 *
 * Implements glBufferSubData(), which replaces a portion of the data in a
 * buffer object.
 *
 * If the data range specified by (size + offset) extends beyond the end of
 * the buffer or if data is NULL, no copy is performed.
a172 22
   /* See if we can unsynchronized write the data into the user's BO. This
    * avoids GPU stalls in unfortunately common user patterns (uploading
    * sequentially into a BO, with draw calls in between each upload).
    *
    * Once we've hit this path, we mark this GL BO as preferring stalling to
    * blits, so that we can hopefully hit this path again in the future
    * (otherwise, an app that might occasionally stall but mostly not will end
    * up with blitting all the time, at the cost of bandwidth)
    */
   if (brw->has_llc) {
      if (offset + size <= intel_obj->gpu_active_start ||
          intel_obj->gpu_active_end <= offset) {
         drm_intel_gem_bo_map_unsynchronized(intel_obj->buffer);
         memcpy(intel_obj->buffer->virtual + offset, data, size);
         drm_intel_bo_unmap(intel_obj->buffer);

         if (intel_obj->gpu_active_end > intel_obj->gpu_active_start)
            intel_obj->prefer_stall_to_blit = true;
         return;
      }
   }

d179 1
a179 1
	 /* Replace the current busy bo so the subdata doesn't stall. */
d182 5
a186 7
      } else if (!intel_obj->prefer_stall_to_blit) {
         perf_debug("Using a blit copy to avoid stalling on "
                    "glBufferSubData(%ld, %ld) (%ldkb) to a busy "
                    "(%d-%d) buffer object.\n",
                    (long)offset, (long)offset + size, (long)(size/1024),
                    intel_obj->gpu_active_start,
                    intel_obj->gpu_active_end);
a197 9
         return;
      } else {
         perf_debug("Stalling on glBufferSubData(%ld, %ld) (%ldkb) to a busy "
                    "(%d-%d) buffer object.  Use glMapBufferRange() to "
                    "avoid this.\n",
                    (long)offset, (long)offset + size, (long)(size/1024),
                    intel_obj->gpu_active_start,
                    intel_obj->gpu_active_end);
         intel_batchbuffer_flush(brw);
d199 2
a201 3

   drm_intel_bo_subdata(intel_obj->buffer, offset, size, data);
   intel_bufferobj_mark_inactive(intel_obj);
d206 1
a206 4
 * The GetBufferSubData() driver hook.
 *
 * Implements glGetBufferSubData(), which copies a subrange of a buffer
 * object into user memory.
d222 1
a223 2
   intel_bufferobj_mark_inactive(intel_obj);
}
d227 1
a227 3
 * The MapBufferRange() driver hook.
 *
 * This implements both glMapBufferRange() and glMapBuffer().
d245 1
a245 2
			  GLbitfield access, struct gl_buffer_object *obj,
                          gl_map_buffer_index index)
d255 3
a257 3
   obj->Mappings[index].Offset = offset;
   obj->Mappings[index].Length = length;
   obj->Mappings[index].AccessFlags = access;
d260 1
a260 1
      obj->Mappings[index].Pointer = NULL;
d280 1
a280 1
	    intel_batchbuffer_flush(brw);
a292 4
    *
    * That is, unless they're looking for a persistent mapping -- we would
    * need to do blits in the MemoryBarrier call, and it's easier to just do a
    * GPU stall and do a mapping.
d294 1
a294 2
   if (!(access & (GL_MAP_UNSYNCHRONIZED_BIT | GL_MAP_PERSISTENT_BIT)) &&
       (access & GL_MAP_INVALIDATE_RANGE_BIT) &&
d296 3
a298 14
      /* Ensure that the base alignment of the allocation meets the alignment
       * guarantees the driver has advertised to the application.
       */
      const unsigned alignment = ctx->Const.MinMapBufferAlignment;

      intel_obj->map_extra[index] = (uintptr_t) offset % alignment;
      intel_obj->range_map_bo[index] = drm_intel_bo_alloc(brw->bufmgr,
                                                          "BO blit temp",
                                                          length +
                                                          intel_obj->map_extra[index],
                                                          alignment);
      if (brw->has_llc) {
         drm_intel_bo_map(intel_obj->range_map_bo[index],
                          (access & GL_MAP_WRITE_BIT) != 0);
d300 10
a309 1
         drm_intel_gem_bo_map_gtt(intel_obj->range_map_bo[index]);
d311 1
a311 3
      obj->Mappings[index].Pointer =
         intel_obj->range_map_bo[index]->virtual + intel_obj->map_extra[index];
      return obj->Mappings[index].Pointer;
d316 1
a316 2
   else if (!brw->has_llc && (!(access & GL_MAP_READ_BIT) ||
                              (access & GL_MAP_PERSISTENT_BIT))) {
a317 1
      intel_bufferobj_mark_inactive(intel_obj);
a319 1
      intel_bufferobj_mark_inactive(intel_obj);
d322 2
a323 2
   obj->Mappings[index].Pointer = intel_obj->buffer->virtual + offset;
   return obj->Mappings[index].Pointer;
d326 1
a326 9
/**
 * The FlushMappedBufferRange() driver hook.
 *
 * Implements glFlushMappedBufferRange(), which signifies that modifications
 * have been made to a range of a mapped buffer, and it should be flushed.
 *
 * This is only used for buffers mapped with GL_MAP_FLUSH_EXPLICIT_BIT.
 *
 * Ideally we'd use a BO to avoid taking up cache space for the temporary
d334 1
a334 2
				   struct gl_buffer_object *obj,
                                   gl_map_buffer_index index)
d338 1
a338 3
   GLbitfield access = obj->Mappings[index].AccessFlags;

   assert(access & GL_MAP_FLUSH_EXPLICIT_BIT);
d340 2
a341 2
   /* If we gave a direct mapping of the buffer instead of using a temporary,
    * then there's nothing to do.
d343 1
a343 1
   if (intel_obj->range_map_bo[index] == NULL)
d349 4
a352 23
   /* Note that we're not unmapping our buffer while executing the blit.  We
    * need to have a mapping still at the end of this call, since the user
    * gets to make further modifications and glFlushMappedBufferRange() calls.
    * This is safe, because:
    *
    * - On LLC platforms, we're using a CPU mapping that's coherent with the
    *   GPU (except for the render caches), so the kernel doesn't need to do
    *   any flushing work for us except for what happens at batch exec time
    *   anyway.
    *
    * - On non-LLC platforms, we're using a GTT mapping that writes directly
    *   to system memory (except for the chipset cache that gets flushed at
    *   batch exec time).
    *
    * In both cases we don't need to stall for the previous blit to complete
    * so we can re-map (and we definitely don't want to, since that would be
    * slow): If the user edits a part of their buffer that's previously been
    * blitted, then our lack of synchoronization is fine, because either
    * they'll get some too-new data in the first blit and not do another blit
    * of that area (but in that case the results are undefined), or they'll do
    * another blit of that area and the complete newer data will land the
    * second time.
    */
d354 2
a355 4
			  intel_obj->buffer,
                          obj->Mappings[index].Offset + offset,
			  intel_obj->range_map_bo[index],
                          intel_obj->map_extra[index] + offset,
d357 2
a358 3
   intel_bufferobj_mark_gpu_usage(intel_obj,
                                  obj->Mappings[index].Offset + offset,
                                  length);
d363 1
a363 3
 * The UnmapBuffer() driver hook.
 *
 * Implements glUnmapBuffer().
d366 1
a366 2
intel_bufferobj_unmap(struct gl_context * ctx, struct gl_buffer_object *obj,
                      gl_map_buffer_index index)
d372 17
a388 13
   assert(obj->Mappings[index].Pointer);
   if (intel_obj->range_map_bo[index] != NULL) {
      drm_intel_bo_unmap(intel_obj->range_map_bo[index]);

      if (!(obj->Mappings[index].AccessFlags & GL_MAP_FLUSH_EXPLICIT_BIT)) {
         intel_emit_linear_blit(brw,
                                intel_obj->buffer, obj->Mappings[index].Offset,
                                intel_obj->range_map_bo[index],
                                intel_obj->map_extra[index],
                                obj->Mappings[index].Length);
         intel_bufferobj_mark_gpu_usage(intel_obj, obj->Mappings[index].Offset,
                                        obj->Mappings[index].Length);
      }
d397 2
a398 2
      drm_intel_bo_unreference(intel_obj->range_map_bo[index]);
      intel_obj->range_map_bo[index] = NULL;
d402 3
a404 3
   obj->Mappings[index].Pointer = NULL;
   obj->Mappings[index].Offset = 0;
   obj->Mappings[index].Length = 0;
a408 7
/**
 * Gets a pointer to the object's BO, and marks the given range as being used
 * on the GPU.
 *
 * Anywhere that uses buffer objects in the pipeline should be using this to
 * mark the range of the buffer that is being accessed by the pipeline.
 */
d412 1
a412 1
                       uint32_t offset, uint32_t size)
a413 4
   /* This is needed so that things like transform feedback and texture buffer
    * objects that need a BO but don't want to check that they exist for
    * draw-time validation can just always get a BO from a GL buffer object.
    */
d417 4
a420 1
   intel_bufferobj_mark_gpu_usage(intel_obj, offset, size);
d422 137
a561 7
/**
 * The CopyBufferSubData() driver hook.
 *
 * Implements glCopyBufferSubData(), which copies a portion of one buffer
 * object's data to another.  Independent source and destination offsets
 * are allowed.
 */
d573 1
d578 2
a579 2
   dst_bo = intel_bufferobj_buffer(brw, intel_dst, write_offset, size);
   src_bo = intel_bufferobj_buffer(brw, intel_src, read_offset, size);
d583 1
a583 1
			  src_bo, read_offset, size);
d593 123
d728 8
@


1.1.1.4
log
@Import Mesa 10.4.3
@
text
@d84 4
d89 1
a89 1
mark_buffer_gpu_usage(struct intel_buffer_object *intel_obj,
d97 1
a97 1
mark_buffer_inactive(struct intel_buffer_object *intel_obj)
d105 2
a106 2
alloc_buffer_object(struct brw_context *brw,
                    struct intel_buffer_object *intel_obj)
d113 1
a113 6
   if (intel_obj->Base.UsageHistory & USAGE_UNIFORM_BUFFER)
      brw->state.dirty.brw |= BRW_NEW_UNIFORM_BUFFER;
   if (intel_obj->Base.UsageHistory & USAGE_TEXTURE_BUFFER)
      brw->state.dirty.brw |= BRW_NEW_TEXTURE_BUFFER;
   if (intel_obj->Base.UsageHistory & USAGE_ATOMIC_COUNTER_BUFFER)
      brw->state.dirty.brw |= BRW_NEW_ATOMIC_BUFFER;
d115 1
a115 1
   mark_buffer_inactive(intel_obj);
d136 1
a136 1
brw_new_buffer_object(struct gl_context * ctx, GLuint name)
a138 3
   if (!obj) {
      _mesa_error_no_memory(__func__);
   }
d140 1
a140 1
   _mesa_initialize_buffer_object(ctx, &obj->Base, name);
d153 1
a153 1
brw_delete_buffer(struct gl_context * ctx, struct gl_buffer_object *obj)
d181 7
a187 7
brw_buffer_data(struct gl_context *ctx,
                GLenum target,
                GLsizeiptrARB size,
                const GLvoid *data,
                GLenum usage,
                GLbitfield storageFlags,
                struct gl_buffer_object *obj)
d207 1
a207 1
      alloc_buffer_object(brw, intel_obj);
d229 4
a232 5
brw_buffer_subdata(struct gl_context *ctx,
                   GLintptrARB offset,
                   GLsizeiptrARB size,
                   const GLvoid *data,
                   struct gl_buffer_object *obj)
d273 1
a273 1
	 alloc_buffer_object(brw, intel_obj);
d305 1
a305 1
   mark_buffer_inactive(intel_obj);
d316 4
a319 5
brw_get_buffer_subdata(struct gl_context *ctx,
                       GLintptrARB offset,
                       GLsizeiptrARB size,
                       GLvoid *data,
                       struct gl_buffer_object *obj)
d330 1
a330 1
   mark_buffer_inactive(intel_obj);
d353 4
a356 4
brw_map_buffer_range(struct gl_context *ctx,
                     GLintptr offset, GLsizeiptr length,
                     GLbitfield access, struct gl_buffer_object *obj,
                     gl_map_buffer_index index)
d387 1
a387 1
	    alloc_buffer_object(brw, intel_obj);
d396 1
a396 1
	 alloc_buffer_object(brw, intel_obj);
d424 2
a425 2
         brw_bo_map(brw, intel_obj->range_map_bo[index],
                    (access & GL_MAP_WRITE_BIT) != 0, "range-map");
d439 1
a439 1
      mark_buffer_inactive(intel_obj);
d441 2
a442 3
      brw_bo_map(brw, intel_obj->buffer, (access & GL_MAP_WRITE_BIT) != 0,
                 "MapBufferRange");
      mark_buffer_inactive(intel_obj);
d463 4
a466 4
brw_flush_mapped_buffer_range(struct gl_context *ctx,
                              GLintptr offset, GLsizeiptr length,
                              struct gl_buffer_object *obj,
                              gl_map_buffer_index index)
d470 1
d472 1
a472 1
   assert(obj->Mappings[index].AccessFlags & GL_MAP_FLUSH_EXPLICIT_BIT);
d512 3
a514 3
   mark_buffer_gpu_usage(intel_obj,
                         obj->Mappings[index].Offset + offset,
                         length);
d524 2
a525 3
brw_unmap_buffer(struct gl_context *ctx,
                 struct gl_buffer_object *obj,
                 gl_map_buffer_index index)
d541 2
a542 2
         mark_buffer_gpu_usage(intel_obj, obj->Mappings[index].Offset,
                               obj->Mappings[index].Length);
d581 1
a581 1
      alloc_buffer_object(brw, intel_obj);
d583 1
a583 1
   mark_buffer_gpu_usage(intel_obj, offset, size);
d596 5
a600 5
brw_copy_buffer_subdata(struct gl_context *ctx,
                        struct gl_buffer_object *src,
                        struct gl_buffer_object *dst,
                        GLintptr read_offset, GLintptr write_offset,
                        GLsizeiptr size)
d628 9
a636 9
   functions->NewBufferObject = brw_new_buffer_object;
   functions->DeleteBuffer = brw_delete_buffer;
   functions->BufferData = brw_buffer_data;
   functions->BufferSubData = brw_buffer_subdata;
   functions->GetBufferSubData = brw_get_buffer_subdata;
   functions->MapBufferRange = brw_map_buffer_range;
   functions->FlushMappedBufferRange = brw_flush_mapped_buffer_range;
   functions->UnmapBuffer = brw_unmap_buffer;
   functions->CopyBufferSubData = brw_copy_buffer_subdata;
@


1.1.1.5
log
@Import Mesa 10.2.9
@
text
@a83 4
static GLboolean
intel_bufferobj_unmap(struct gl_context * ctx, struct gl_buffer_object *obj,
                      gl_map_buffer_index index);

d85 1
a85 1
intel_bufferobj_mark_gpu_usage(struct intel_buffer_object *intel_obj,
d93 1
a93 1
intel_bufferobj_mark_inactive(struct intel_buffer_object *intel_obj)
d101 2
a102 2
intel_bufferobj_alloc_buffer(struct brw_context *brw,
			     struct intel_buffer_object *intel_obj)
d109 6
a114 1
   brw->state.dirty.brw |= BRW_NEW_UNIFORM_BUFFER;
d116 1
a116 1
   intel_bufferobj_mark_inactive(intel_obj);
d137 1
a137 1
intel_bufferobj_alloc(struct gl_context * ctx, GLuint name, GLenum target)
d140 3
d144 1
a144 1
   _mesa_initialize_buffer_object(ctx, &obj->Base, name, target);
d157 1
a157 1
intel_bufferobj_free(struct gl_context * ctx, struct gl_buffer_object *obj)
d185 7
a191 7
intel_bufferobj_data(struct gl_context * ctx,
                     GLenum target,
                     GLsizeiptrARB size,
                     const GLvoid * data,
                     GLenum usage,
                     GLbitfield storageFlags,
                     struct gl_buffer_object *obj)
d211 1
a211 1
      intel_bufferobj_alloc_buffer(brw, intel_obj);
d233 5
a237 4
intel_bufferobj_subdata(struct gl_context * ctx,
                        GLintptrARB offset,
                        GLsizeiptrARB size,
                        const GLvoid * data, struct gl_buffer_object *obj)
d278 1
a278 1
	 intel_bufferobj_alloc_buffer(brw, intel_obj);
d310 1
a310 1
   intel_bufferobj_mark_inactive(intel_obj);
d321 5
a325 4
intel_bufferobj_get_subdata(struct gl_context * ctx,
                            GLintptrARB offset,
                            GLsizeiptrARB size,
                            GLvoid * data, struct gl_buffer_object *obj)
d336 1
a336 1
   intel_bufferobj_mark_inactive(intel_obj);
d359 4
a362 4
intel_bufferobj_map_range(struct gl_context * ctx,
			  GLintptr offset, GLsizeiptr length,
			  GLbitfield access, struct gl_buffer_object *obj,
                          gl_map_buffer_index index)
d393 1
a393 1
	    intel_bufferobj_alloc_buffer(brw, intel_obj);
d402 1
a402 1
	 intel_bufferobj_alloc_buffer(brw, intel_obj);
d430 2
a431 2
         drm_intel_bo_map(intel_obj->range_map_bo[index],
                          (access & GL_MAP_WRITE_BIT) != 0);
d445 1
a445 1
      intel_bufferobj_mark_inactive(intel_obj);
d447 3
a449 2
      drm_intel_bo_map(intel_obj->buffer, (access & GL_MAP_WRITE_BIT) != 0);
      intel_bufferobj_mark_inactive(intel_obj);
d470 4
a473 4
intel_bufferobj_flush_mapped_range(struct gl_context *ctx,
				   GLintptr offset, GLsizeiptr length,
				   struct gl_buffer_object *obj,
                                   gl_map_buffer_index index)
a476 1
   GLbitfield access = obj->Mappings[index].AccessFlags;
d478 1
a478 1
   assert(access & GL_MAP_FLUSH_EXPLICIT_BIT);
d518 3
a520 3
   intel_bufferobj_mark_gpu_usage(intel_obj,
                                  obj->Mappings[index].Offset + offset,
                                  length);
d530 3
a532 2
intel_bufferobj_unmap(struct gl_context * ctx, struct gl_buffer_object *obj,
                      gl_map_buffer_index index)
d548 2
a549 2
         intel_bufferobj_mark_gpu_usage(intel_obj, obj->Mappings[index].Offset,
                                        obj->Mappings[index].Length);
d588 1
a588 1
      intel_bufferobj_alloc_buffer(brw, intel_obj);
d590 1
a590 1
   intel_bufferobj_mark_gpu_usage(intel_obj, offset, size);
d603 5
a607 5
intel_bufferobj_copy_subdata(struct gl_context *ctx,
			     struct gl_buffer_object *src,
			     struct gl_buffer_object *dst,
			     GLintptr read_offset, GLintptr write_offset,
			     GLsizeiptr size)
d635 9
a643 9
   functions->NewBufferObject = intel_bufferobj_alloc;
   functions->DeleteBuffer = intel_bufferobj_free;
   functions->BufferData = intel_bufferobj_data;
   functions->BufferSubData = intel_bufferobj_subdata;
   functions->GetBufferSubData = intel_bufferobj_get_subdata;
   functions->MapBufferRange = intel_bufferobj_map_range;
   functions->FlushMappedBufferRange = intel_bufferobj_flush_mapped_range;
   functions->UnmapBuffer = intel_bufferobj_unmap;
   functions->CopyBufferSubData = intel_bufferobj_copy_subdata;
@


