head	1.8;
access;
symbols
	OPENBSD_5_4:1.7.0.4
	OPENBSD_5_4_BASE:1.7
	OPENBSD_5_3:1.7.0.2
	OPENBSD_5_3_BASE:1.7
	OPENBSD_5_2:1.6.0.4
	OPENBSD_5_2_BASE:1.6
	OPENBSD_5_1_BASE:1.6
	OPENBSD_5_1:1.6.0.2
	v7_10_3:1.1.1.4
	OPENBSD_5_0:1.5.0.6
	OPENBSD_5_0_BASE:1.5
	OPENBSD_4_9:1.5.0.2
	OPENBSD_4_9_BASE:1.5
	OPENBSD_4_8:1.5.0.4
	OPENBSD_4_8_BASE:1.5
	OPENBSD_4_7:1.4.0.4
	OPENBSD_4_7_BASE:1.4
	OPENBSD_4_6:1.4.0.2
	OPENBSD_4_6_BASE:1.4
	OPENBSD_4_5:1.3.0.2
	OPENBSD_4_5_BASE:1.3
	OPENBSD_4_4:1.2.0.2
	OPENBSD_4_4_BASE:1.2
	OPENBSD_4_3_BASE:1.1.1.3
	OPENBSD_4_3:1.1.1.3.0.2
	v7_0_1:1.1.1.3
	OPENBSD_4_2:1.1.1.2.0.2
	OPENBSD_4_2_BASE:1.1.1.2
	v6_5_2:1.1.1.2
	v6_5_1:1.1.1.1
	mesa:1.1.1;
locks; strict;
comment	@ * @;


1.8
date	2013.09.05.14.04.21;	author jsg;	state dead;
branches;
next	1.7;

1.7
date	2012.08.17.13.58.15;	author mpi;	state Exp;
branches;
next	1.6;

1.6
date	2011.10.23.13.37.39;	author matthieu;	state Exp;
branches;
next	1.5;

1.5
date	2010.05.22.20.06.18;	author matthieu;	state Exp;
branches;
next	1.4;

1.4
date	2009.05.17.20.26.39;	author matthieu;	state Exp;
branches;
next	1.3;

1.3
date	2008.11.02.14.58.15;	author matthieu;	state Exp;
branches;
next	1.2;

1.2
date	2008.05.31.16.36.48;	author matthieu;	state Exp;
branches;
next	1.1;

1.1
date	2006.11.25.18.52.47;	author matthieu;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2006.11.25.18.52.47;	author matthieu;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2007.03.03.11.57.17;	author matthieu;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2007.11.24.17.28.37;	author matthieu;	state Exp;
branches;
next	1.1.1.4;

1.1.1.4
date	2011.10.23.13.29.36;	author matthieu;	state Exp;
branches;
next	;


desc
@@


1.8
log
@Merge Mesa 9.2.0
@
text
@/*
 Copyright (C) Intel Corp.  2006.  All Rights Reserved.
 Intel funded Tungsten Graphics (http://www.tungstengraphics.com) to
 develop this 3D driver.
 
 Permission is hereby granted, free of charge, to any person obtaining
 a copy of this software and associated documentation files (the
 "Software"), to deal in the Software without restriction, including
 without limitation the rights to use, copy, modify, merge, publish,
 distribute, sublicense, and/or sell copies of the Software, and to
 permit persons to whom the Software is furnished to do so, subject to
 the following conditions:
 
 The above copyright notice and this permission notice (including the
 next paragraph) shall be included in all copies or substantial
 portions of the Software.
 
 THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
 EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 IN NO EVENT SHALL THE COPYRIGHT OWNER(S) AND/OR ITS SUPPLIERS BE
 LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
 OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
 WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 
 **********************************************************************/
 /*
  * Authors:
  *   Keith Whitwell <keith@@tungstengraphics.com>
  */
            

#include "main/macros.h"
#include "program/program.h"
#include "program/prog_parameter.h"
#include "program/prog_print.h"
#include "brw_context.h"
#include "brw_vs.h"

/* Return the SrcReg index of the channels that can be immediate float operands
 * instead of usage of PROGRAM_CONSTANT values through push/pull.
 */
static GLboolean
brw_vs_arg_can_be_immediate(enum prog_opcode opcode, int arg)
{
   int opcode_array[] = {
      [OPCODE_MOV] = 1,
      [OPCODE_ADD] = 2,
      [OPCODE_CMP] = 3,
      [OPCODE_DP2] = 2,
      [OPCODE_DP3] = 2,
      [OPCODE_DP4] = 2,
      [OPCODE_DPH] = 2,
      [OPCODE_MAX] = 2,
      [OPCODE_MIN] = 2,
      [OPCODE_MUL] = 2,
      [OPCODE_SEQ] = 2,
      [OPCODE_SGE] = 2,
      [OPCODE_SGT] = 2,
      [OPCODE_SLE] = 2,
      [OPCODE_SLT] = 2,
      [OPCODE_SNE] = 2,
      [OPCODE_XPD] = 2,
   };

   /* These opcodes get broken down in a way that allow two
    * args to be immediates.
    */
   if (opcode == OPCODE_MAD || opcode == OPCODE_LRP) {
      if (arg == 1 || arg == 2)
	 return GL_TRUE;
   }

   if (opcode > ARRAY_SIZE(opcode_array))
      return GL_FALSE;

   return arg == opcode_array[opcode] - 1;
}

static struct brw_reg get_tmp( struct brw_vs_compile *c )
{
   struct brw_reg tmp = brw_vec8_grf(c->last_tmp, 0);

   if (++c->last_tmp > c->prog_data.total_grf)
      c->prog_data.total_grf = c->last_tmp;

   return tmp;
}

static void release_tmp( struct brw_vs_compile *c, struct brw_reg tmp )
{
   if (tmp.nr == c->last_tmp-1)
      c->last_tmp--;
}
			       
static void release_tmps( struct brw_vs_compile *c )
{
   c->last_tmp = c->first_tmp;
}

static int
get_first_reladdr_output(struct gl_vertex_program *vp)
{
   int i;
   int first_reladdr_output = VERT_RESULT_MAX;

   for (i = 0; i < vp->Base.NumInstructions; i++) {
      struct prog_instruction *inst = vp->Base.Instructions + i;

      if (inst->DstReg.File == PROGRAM_OUTPUT &&
	  inst->DstReg.RelAddr &&
	  inst->DstReg.Index < first_reladdr_output)
	 first_reladdr_output = inst->DstReg.Index;
   }

   return first_reladdr_output;
}

/* Clears the record of which vp_const_buffer elements have been
 * loaded into our constant buffer registers, for the starts of new
 * blocks after control flow.
 */
static void
clear_current_const(struct brw_vs_compile *c)
{
   unsigned int i;

   if (c->vp->use_const_buffer) {
      for (i = 0; i < 3; i++) {
         c->current_const[i].index = -1;
      }
   }
}

/**
 * Preallocate GRF register before code emit.
 * Do things as simply as possible.  Allocate and populate all regs
 * ahead of time.
 */
static void brw_vs_alloc_regs( struct brw_vs_compile *c )
{
   struct intel_context *intel = &c->func.brw->intel;
   GLuint i, reg = 0, mrf, j;
   int attributes_in_vue;
   int first_reladdr_output;
   int max_constant;
   int constant = 0;
   int vert_result_reoder[VERT_RESULT_MAX];
   int bfc = 0;

   /* Determine whether to use a real constant buffer or use a block
    * of GRF registers for constants.  The later is faster but only
    * works if everything fits in the GRF.
    * XXX this heuristic/check may need some fine tuning...
    */
   if (c->vp->program.Base.Parameters->NumParameters +
       c->vp->program.Base.NumTemporaries + 20 > BRW_MAX_GRF)
      c->vp->use_const_buffer = GL_TRUE;
   else
      c->vp->use_const_buffer = GL_FALSE;

   /*printf("use_const_buffer = %d\n", c->vp->use_const_buffer);*/

   /* r0 -- reserved as usual
    */
   c->r0 = brw_vec8_grf(reg, 0);
   reg++;

   /* User clip planes from curbe: 
    */
   if (c->key.nr_userclip) {
      if (intel->gen >= 6) {
	 for (i = 0; i < c->key.nr_userclip; i++) {
	    c->userplane[i] = stride(brw_vec4_grf(reg + i / 2,
						  (i % 2) * 4), 0, 4, 1);
	 }
	 reg += ALIGN(c->key.nr_userclip, 2) / 2;
      } else {
	 for (i = 0; i < c->key.nr_userclip; i++) {
	    c->userplane[i] = stride(brw_vec4_grf(reg + (6 + i) / 2,
						  (i % 2) * 4), 0, 4, 1);
	 }
	 reg += (ALIGN(6 + c->key.nr_userclip, 4) / 4) * 2;
      }

   }

   /* Assign some (probably all) of the vertex program constants to
    * the push constant buffer/CURBE.
    *
    * There's an obvious limit to the numer of push constants equal to
    * the number of register available, and that number is smaller
    * than the minimum maximum number of vertex program parameters, so
    * support for pull constants is required if we overflow.
    * Additionally, on gen6 the number of push constants is even
    * lower.
    *
    * When there's relative addressing, we don't know what range of
    * Mesa IR registers can be accessed.  And generally, when relative
    * addressing is used we also have too many constants to load them
    * all as push constants.  So, we'll just support relative
    * addressing out of the pull constant buffers, and try to load as
    * many statically-accessed constants into the push constant buffer
    * as we can.
    */
   if (intel->gen >= 6) {
      /* We can only load 32 regs of push constants. */
      max_constant = 32 * 2 - c->key.nr_userclip;
   } else {
      max_constant = BRW_MAX_GRF - 20 - c->vp->program.Base.NumTemporaries;
   }

   /* constant_map maps from ParameterValues[] index to index in the
    * push constant buffer, or -1 if it's only in the pull constant
    * buffer.
    */
   memset(c->constant_map, -1, c->vp->program.Base.Parameters->NumParameters);
   for (i = 0;
	i < c->vp->program.Base.NumInstructions && constant < max_constant;
	i++) {
      struct prog_instruction *inst = &c->vp->program.Base.Instructions[i];
      int arg;

      for (arg = 0; arg < 3 && constant < max_constant; arg++) {
	 if (inst->SrcReg[arg].File != PROGRAM_STATE_VAR &&
	     inst->SrcReg[arg].File != PROGRAM_CONSTANT &&
	     inst->SrcReg[arg].File != PROGRAM_UNIFORM &&
	     inst->SrcReg[arg].File != PROGRAM_ENV_PARAM &&
	     inst->SrcReg[arg].File != PROGRAM_LOCAL_PARAM) {
	    continue;
	 }

	 if (inst->SrcReg[arg].RelAddr) {
	    c->vp->use_const_buffer = GL_TRUE;
	    continue;
	 }

	 if (c->constant_map[inst->SrcReg[arg].Index] == -1) {
	    c->constant_map[inst->SrcReg[arg].Index] = constant++;
	 }
      }
   }

   /* If we ran out of push constant space, then we'll also upload all
    * constants through the pull constant buffer so that they can be
    * accessed no matter what.  For relative addressing (the common
    * case) we need them all in place anyway.
    */
   if (constant == max_constant)
      c->vp->use_const_buffer = GL_TRUE;

   for (i = 0; i < constant; i++) {
      c->regs[PROGRAM_STATE_VAR][i] = stride(brw_vec4_grf(reg + i / 2,
							  (i % 2) * 4),
					     0, 4, 1);
   }
   reg += (constant + 1) / 2;
   c->prog_data.curb_read_length = reg - 1;
   c->prog_data.nr_params = constant * 4;
   /* XXX 0 causes a bug elsewhere... */
   if (intel->gen < 6 && c->prog_data.nr_params == 0)
      c->prog_data.nr_params = 4;

   /* Allocate input regs:  
    */
   c->nr_inputs = 0;
   for (i = 0; i < VERT_ATTRIB_MAX; i++) {
      if (c->prog_data.inputs_read & (1 << i)) {
	 c->nr_inputs++;
	 c->regs[PROGRAM_INPUT][i] = brw_vec8_grf(reg, 0);
	 reg++;
      }
   }
   /* If there are no inputs, we'll still be reading one attribute's worth
    * because it's required -- see urb_read_length setting.
    */
   if (c->nr_inputs == 0)
      reg++;

   /* Allocate outputs.  The non-position outputs go straight into message regs.
    */
   c->nr_outputs = 0;
   c->first_output = reg;
   c->first_overflow_output = 0;

   if (intel->gen >= 6) {
      mrf = 3;
      if (c->key.nr_userclip)
	 mrf += 2;
   } else if (intel->gen == 5)
      mrf = 8;
   else
      mrf = 4;

   first_reladdr_output = get_first_reladdr_output(&c->vp->program);

   for (i = 0; i < VERT_RESULT_MAX; i++)
       vert_result_reoder[i] = i;

   /* adjust attribute order in VUE for BFC0/BFC1 on Gen6+ */
   if (intel->gen >= 6 && c->key.two_side_color) {
       if ((c->prog_data.outputs_written & BITFIELD64_BIT(VERT_RESULT_COL1)) &&
           (c->prog_data.outputs_written & BITFIELD64_BIT(VERT_RESULT_BFC1))) {
           assert(c->prog_data.outputs_written & BITFIELD64_BIT(VERT_RESULT_COL0));
           assert(c->prog_data.outputs_written & BITFIELD64_BIT(VERT_RESULT_BFC0));
           bfc = 2;
       } else if ((c->prog_data.outputs_written & BITFIELD64_BIT(VERT_RESULT_COL0)) &&
           (c->prog_data.outputs_written & BITFIELD64_BIT(VERT_RESULT_BFC0)))
           bfc = 1;

       if (bfc) {
           for (i = 0; i < bfc; i++) {
               vert_result_reoder[VERT_RESULT_COL0 + i * 2 + 0] = VERT_RESULT_COL0 + i;
               vert_result_reoder[VERT_RESULT_COL0 + i * 2 + 1] = VERT_RESULT_BFC0 + i;
           }

           for (i = VERT_RESULT_COL0 + bfc * 2; i < VERT_RESULT_BFC0 + bfc; i++) {
               vert_result_reoder[i] = i - bfc;
           }
       }
   }

   for (j = 0; j < VERT_RESULT_MAX; j++) {
      i = vert_result_reoder[j];

      if (c->prog_data.outputs_written & BITFIELD64_BIT(i)) {
	 c->nr_outputs++;
         assert(i < Elements(c->regs[PROGRAM_OUTPUT]));
	 if (i == VERT_RESULT_HPOS) {
	    c->regs[PROGRAM_OUTPUT][i] = brw_vec8_grf(reg, 0);
	    reg++;
	 }
	 else if (i == VERT_RESULT_PSIZ) {
	    c->regs[PROGRAM_OUTPUT][i] = brw_vec8_grf(reg, 0);
	    reg++;
	 }
	 else {
	    /* Two restrictions on our compute-to-MRF here.  The
	     * message length for all SEND messages is restricted to
	     * [1,15], so we can't use mrf 15, as that means a length
	     * of 16.
	     *
	     * Additionally, URB writes are aligned to URB rows, so we
	     * need to put an even number of registers of URB data in
	     * each URB write so that the later write is aligned.  A
	     * message length of 15 means 1 message header reg plus 14
	     * regs of URB data.
	     *
	     * For attributes beyond the compute-to-MRF, we compute to
	     * GRFs and they will be written in the second URB_WRITE.
	     */
            if (first_reladdr_output > i && mrf < 15) {
               c->regs[PROGRAM_OUTPUT][i] = brw_message_reg(mrf);
               mrf++;
            }
            else {
               if (mrf >= 15 && !c->first_overflow_output)
                  c->first_overflow_output = i;
               c->regs[PROGRAM_OUTPUT][i] = brw_vec8_grf(reg, 0);
               reg++;
	       mrf++;
            }
	 }
      }
   }     

   /* Allocate program temporaries:
    */
   for (i = 0; i < c->vp->program.Base.NumTemporaries; i++) {
      c->regs[PROGRAM_TEMPORARY][i] = brw_vec8_grf(reg, 0);
      reg++;
   }

   /* Address reg(s).  Don't try to use the internal address reg until
    * deref time.
    */
   for (i = 0; i < c->vp->program.Base.NumAddressRegs; i++) {
      c->regs[PROGRAM_ADDRESS][i] =  brw_reg(BRW_GENERAL_REGISTER_FILE,
					     reg,
					     0,
					     BRW_REGISTER_TYPE_D,
					     BRW_VERTICAL_STRIDE_8,
					     BRW_WIDTH_8,
					     BRW_HORIZONTAL_STRIDE_1,
					     BRW_SWIZZLE_XXXX,
					     WRITEMASK_X);
      reg++;
   }

   if (c->vp->use_const_buffer) {
      for (i = 0; i < 3; i++) {
         c->current_const[i].reg = brw_vec8_grf(reg, 0);
         reg++;
      }
      clear_current_const(c);
   }

   for (i = 0; i < 128; i++) {
      if (c->output_regs[i].used_in_src) {
         c->output_regs[i].reg = brw_vec8_grf(reg, 0);
         reg++;
      }
   }

   if (c->needs_stack) {
      c->stack =  brw_uw16_reg(BRW_GENERAL_REGISTER_FILE, reg, 0);
      reg += 2;
   }

   /* Some opcodes need an internal temporary:
    */
   c->first_tmp = reg;
   c->last_tmp = reg;		/* for allocation purposes */

   /* Each input reg holds data from two vertices.  The
    * urb_read_length is the number of registers read from *each*
    * vertex urb, so is half the amount:
    */
   c->prog_data.urb_read_length = (c->nr_inputs + 1) / 2;
   /* Setting this field to 0 leads to undefined behavior according to the
    * the VS_STATE docs.  Our VUEs will always have at least one attribute
    * sitting in them, even if it's padding.
    */
   if (c->prog_data.urb_read_length == 0)
      c->prog_data.urb_read_length = 1;

   /* The VS VUEs are shared by VF (outputting our inputs) and VS, so size
    * them to fit the biggest thing they need to.
    */
   attributes_in_vue = MAX2(c->nr_outputs, c->nr_inputs);

   /* See emit_vertex_write() for where the VUE's overhead on top of the
    * attributes comes from.
    */
   if (intel->gen >= 7) {
      int header_regs = 2;
      if (c->key.nr_userclip)
	 header_regs += 2;

      /* Each attribute is 16 bytes (1 vec4), so dividing by 4 gives us the
       * number of 64-byte (512-bit) units.
       */
      c->prog_data.urb_entry_size = (attributes_in_vue + header_regs + 3) / 4;
   } else if (intel->gen == 6) {
      int header_regs = 2;
      if (c->key.nr_userclip)
	 header_regs += 2;

      /* Each attribute is 16 bytes (1 vec4), so dividing by 8 gives us the
       * number of 128-byte (1024-bit) units.
       */
      c->prog_data.urb_entry_size = (attributes_in_vue + header_regs + 7) / 8;
   } else if (intel->gen == 5)
      /* Each attribute is 16 bytes (1 vec4), so dividing by 4 gives us the
       * number of 64-byte (512-bit) units.
       */
      c->prog_data.urb_entry_size = (attributes_in_vue + 6 + 3) / 4;
   else
      c->prog_data.urb_entry_size = (attributes_in_vue + 2 + 3) / 4;

   c->prog_data.total_grf = reg;

   if (unlikely(INTEL_DEBUG & DEBUG_VS)) {
      printf("%s NumAddrRegs %d\n", __FUNCTION__, c->vp->program.Base.NumAddressRegs);
      printf("%s NumTemps %d\n", __FUNCTION__, c->vp->program.Base.NumTemporaries);
      printf("%s reg = %d\n", __FUNCTION__, reg);
   }
}


/**
 * If an instruction uses a temp reg both as a src and the dest, we
 * sometimes need to allocate an intermediate temporary.
 */
static void unalias1( struct brw_vs_compile *c,
		      struct brw_reg dst,
		      struct brw_reg arg0,
		      void (*func)( struct brw_vs_compile *,
				    struct brw_reg,
				    struct brw_reg ))
{
   if (dst.file == arg0.file && dst.nr == arg0.nr) {
      struct brw_compile *p = &c->func;
      struct brw_reg tmp = brw_writemask(get_tmp(c), dst.dw1.bits.writemask);
      func(c, tmp, arg0);
      brw_MOV(p, dst, tmp);
      release_tmp(c, tmp);
   }
   else {
      func(c, dst, arg0);
   }
}

/**
 * \sa unalias2
 * Checkes if 2-operand instruction needs an intermediate temporary.
 */
static void unalias2( struct brw_vs_compile *c,
		      struct brw_reg dst,
		      struct brw_reg arg0,
		      struct brw_reg arg1,
		      void (*func)( struct brw_vs_compile *,
				    struct brw_reg,
				    struct brw_reg,
				    struct brw_reg ))
{
   if ((dst.file == arg0.file && dst.nr == arg0.nr) ||
       (dst.file == arg1.file && dst.nr == arg1.nr)) {
      struct brw_compile *p = &c->func;
      struct brw_reg tmp = brw_writemask(get_tmp(c), dst.dw1.bits.writemask);
      func(c, tmp, arg0, arg1);
      brw_MOV(p, dst, tmp);
      release_tmp(c, tmp);
   }
   else {
      func(c, dst, arg0, arg1);
   }
}

/**
 * \sa unalias2
 * Checkes if 3-operand instruction needs an intermediate temporary.
 */
static void unalias3( struct brw_vs_compile *c,
		      struct brw_reg dst,
		      struct brw_reg arg0,
		      struct brw_reg arg1,
		      struct brw_reg arg2,
		      void (*func)( struct brw_vs_compile *,
				    struct brw_reg,
				    struct brw_reg,
				    struct brw_reg,
				    struct brw_reg ))
{
   if ((dst.file == arg0.file && dst.nr == arg0.nr) ||
       (dst.file == arg1.file && dst.nr == arg1.nr) ||
       (dst.file == arg2.file && dst.nr == arg2.nr)) {
      struct brw_compile *p = &c->func;
      struct brw_reg tmp = brw_writemask(get_tmp(c), dst.dw1.bits.writemask);
      func(c, tmp, arg0, arg1, arg2);
      brw_MOV(p, dst, tmp);
      release_tmp(c, tmp);
   }
   else {
      func(c, dst, arg0, arg1, arg2);
   }
}

static void emit_sop( struct brw_vs_compile *c,
                      struct brw_reg dst,
                      struct brw_reg arg0,
                      struct brw_reg arg1, 
		      GLuint cond)
{
   struct brw_compile *p = &c->func;

   brw_MOV(p, dst, brw_imm_f(0.0f));
   brw_CMP(p, brw_null_reg(), cond, arg0, arg1);
   brw_MOV(p, dst, brw_imm_f(1.0f));
   brw_set_predicate_control_flag_value(p, 0xff);
}

static void emit_seq( struct brw_vs_compile *c,
                      struct brw_reg dst,
                      struct brw_reg arg0,
                      struct brw_reg arg1 )
{
   emit_sop(c, dst, arg0, arg1, BRW_CONDITIONAL_EQ);
}

static void emit_sne( struct brw_vs_compile *c,
                      struct brw_reg dst,
                      struct brw_reg arg0,
                      struct brw_reg arg1 )
{
   emit_sop(c, dst, arg0, arg1, BRW_CONDITIONAL_NEQ);
}
static void emit_slt( struct brw_vs_compile *c,
		      struct brw_reg dst,
		      struct brw_reg arg0,
		      struct brw_reg arg1 )
{
   emit_sop(c, dst, arg0, arg1, BRW_CONDITIONAL_L);
}

static void emit_sle( struct brw_vs_compile *c,
		      struct brw_reg dst,
		      struct brw_reg arg0,
		      struct brw_reg arg1 )
{
   emit_sop(c, dst, arg0, arg1, BRW_CONDITIONAL_LE);
}

static void emit_sgt( struct brw_vs_compile *c,
		      struct brw_reg dst,
		      struct brw_reg arg0,
		      struct brw_reg arg1 )
{
   emit_sop(c, dst, arg0, arg1, BRW_CONDITIONAL_G);
}

static void emit_sge( struct brw_vs_compile *c,
		      struct brw_reg dst,
		      struct brw_reg arg0,
		      struct brw_reg arg1 )
{
  emit_sop(c, dst, arg0, arg1, BRW_CONDITIONAL_GE);
}

static void emit_cmp( struct brw_compile *p,
		      struct brw_reg dst,
		      struct brw_reg arg0,
		      struct brw_reg arg1,
		      struct brw_reg arg2 )
{
   brw_CMP(p, brw_null_reg(), BRW_CONDITIONAL_L, arg0, brw_imm_f(0));
   brw_SEL(p, dst, arg1, arg2);
   brw_set_predicate_control(p, BRW_PREDICATE_NONE);
}

static void emit_sign(struct brw_vs_compile *c,
		      struct brw_reg dst,
		      struct brw_reg arg0)
{
   struct brw_compile *p = &c->func;

   brw_MOV(p, dst, brw_imm_f(0));

   brw_CMP(p, brw_null_reg(), BRW_CONDITIONAL_L, arg0, brw_imm_f(0));
   brw_MOV(p, dst, brw_imm_f(-1.0));
   brw_set_predicate_control(p, BRW_PREDICATE_NONE);

   brw_CMP(p, brw_null_reg(), BRW_CONDITIONAL_G, arg0, brw_imm_f(0));
   brw_MOV(p, dst, brw_imm_f(1.0));
   brw_set_predicate_control(p, BRW_PREDICATE_NONE);
}

static void emit_max( struct brw_compile *p, 
		      struct brw_reg dst,
		      struct brw_reg arg0,
		      struct brw_reg arg1 )
{
   struct intel_context *intel = &p->brw->intel;

   if (intel->gen >= 6) {
      brw_set_conditionalmod(p, BRW_CONDITIONAL_GE);
      brw_SEL(p, dst, arg0, arg1);
      brw_set_conditionalmod(p, BRW_CONDITIONAL_NONE);
      brw_set_predicate_control(p, BRW_PREDICATE_NONE);
   } else {
      brw_CMP(p, brw_null_reg(), BRW_CONDITIONAL_GE, arg0, arg1);
      brw_SEL(p, dst, arg0, arg1);
      brw_set_predicate_control(p, BRW_PREDICATE_NONE);
   }
}

static void emit_min( struct brw_compile *p, 
		      struct brw_reg dst,
		      struct brw_reg arg0,
		      struct brw_reg arg1 )
{
   struct intel_context *intel = &p->brw->intel;

   if (intel->gen >= 6) {
      brw_set_conditionalmod(p, BRW_CONDITIONAL_L);
      brw_SEL(p, dst, arg0, arg1);
      brw_set_conditionalmod(p, BRW_CONDITIONAL_NONE);
      brw_set_predicate_control(p, BRW_PREDICATE_NONE);
   } else {
      brw_CMP(p, brw_null_reg(), BRW_CONDITIONAL_L, arg0, arg1);
      brw_SEL(p, dst, arg0, arg1);
      brw_set_predicate_control(p, BRW_PREDICATE_NONE);
   }
}

static void emit_arl(struct brw_compile *p,
		     struct brw_reg dst,
		     struct brw_reg src)
{
   struct intel_context *intel = &p->brw->intel;

   if (intel->gen >= 6) {
      struct brw_reg dst_f = retype(dst, BRW_REGISTER_TYPE_F);

      brw_RNDD(p, dst_f, src);
      brw_MOV(p, dst, dst_f);
   } else {
      brw_RNDD(p, dst, src);
   }
}

static void emit_math1_gen4(struct brw_vs_compile *c,
			    GLuint function,
			    struct brw_reg dst,
			    struct brw_reg arg0,
			    GLuint precision)
{
   /* There are various odd behaviours with SEND on the simulator.  In
    * addition there are documented issues with the fact that the GEN4
    * processor doesn't do dependency control properly on SEND
    * results.  So, on balance, this kludge to get around failures
    * with writemasked math results looks like it might be necessary
    * whether that turns out to be a simulator bug or not:
    */
   struct brw_compile *p = &c->func;
   struct brw_reg tmp = dst;
   GLboolean need_tmp = GL_FALSE;

   if (dst.file != BRW_GENERAL_REGISTER_FILE ||
       dst.dw1.bits.writemask != 0xf)
      need_tmp = GL_TRUE;

   if (need_tmp)
      tmp = get_tmp(c);

   brw_math(p, 
	    tmp,
	    function,
	    BRW_MATH_SATURATE_NONE,
	    2,
	    arg0,
	    BRW_MATH_DATA_SCALAR,
	    precision);

   if (need_tmp) {
      brw_MOV(p, dst, tmp);
      release_tmp(c, tmp);
   }
}

static void
emit_math1_gen6(struct brw_vs_compile *c,
		GLuint function,
		struct brw_reg dst,
		struct brw_reg arg0,
		GLuint precision)
{
   struct brw_compile *p = &c->func;
   struct brw_reg tmp_src, tmp_dst;

   /* Something is strange on gen6 math in 16-wide mode, though the
    * docs say it's supposed to work.  Punt to using align1 mode,
    * which doesn't do writemasking and swizzles.
    */
   tmp_src = get_tmp(c);
   tmp_dst = get_tmp(c);

   brw_MOV(p, tmp_src, arg0);

   brw_set_access_mode(p, BRW_ALIGN_1);
   brw_math(p,
	    tmp_dst,
	    function,
	    BRW_MATH_SATURATE_NONE,
	    2,
	    tmp_src,
	    BRW_MATH_DATA_SCALAR,
	    precision);
   brw_set_access_mode(p, BRW_ALIGN_16);

   brw_MOV(p, dst, tmp_dst);

   release_tmp(c, tmp_src);
   release_tmp(c, tmp_dst);
}

static void
emit_math1(struct brw_vs_compile *c,
	   GLuint function,
	   struct brw_reg dst,
	   struct brw_reg arg0,
	   GLuint precision)
{
   struct brw_compile *p = &c->func;
   struct intel_context *intel = &p->brw->intel;

   if (intel->gen >= 6)
      emit_math1_gen6(c, function, dst, arg0, precision);
   else
      emit_math1_gen4(c, function, dst, arg0, precision);
}

static void emit_math2_gen4( struct brw_vs_compile *c, 
			GLuint function,
			struct brw_reg dst,
			struct brw_reg arg0,
			struct brw_reg arg1,
			GLuint precision)
{
   struct brw_compile *p = &c->func;
   struct brw_reg tmp = dst;
   GLboolean need_tmp = GL_FALSE;

   if (dst.file != BRW_GENERAL_REGISTER_FILE ||
       dst.dw1.bits.writemask != 0xf)
      need_tmp = GL_TRUE;

   if (need_tmp) 
      tmp = get_tmp(c);

   brw_MOV(p, brw_message_reg(3), arg1);
   
   brw_math(p, 
	    tmp,
	    function,
	    BRW_MATH_SATURATE_NONE,
	    2,
 	    arg0,
	    BRW_MATH_DATA_SCALAR,
	    precision);

   if (need_tmp) {
      brw_MOV(p, dst, tmp);
      release_tmp(c, tmp);
   }
}

static void emit_math2_gen6( struct brw_vs_compile *c, 
			GLuint function,
			struct brw_reg dst,
			struct brw_reg arg0,
			struct brw_reg arg1,
			GLuint precision)
{
   struct brw_compile *p = &c->func;
   struct brw_reg tmp_src0, tmp_src1, tmp_dst;

   tmp_src0 = get_tmp(c);
   tmp_src1 = get_tmp(c);
   tmp_dst = get_tmp(c);

   brw_MOV(p, tmp_src0, arg0);
   brw_MOV(p, tmp_src1, arg1);
   
   brw_set_access_mode(p, BRW_ALIGN_1);
   brw_math2(p,
	    tmp_dst,
	    function,
	    tmp_src0,
	    tmp_src1);
   brw_set_access_mode(p, BRW_ALIGN_16);

   brw_MOV(p, dst, tmp_dst);

   release_tmp(c, tmp_src0);
   release_tmp(c, tmp_src1);
   release_tmp(c, tmp_dst);
}

static void emit_math2( struct brw_vs_compile *c, 
			GLuint function,
			struct brw_reg dst,
			struct brw_reg arg0,
			struct brw_reg arg1,
			GLuint precision)
{
   struct brw_compile *p = &c->func;
   struct intel_context *intel = &p->brw->intel;

   if (intel->gen >= 6)
      emit_math2_gen6(c, function, dst, arg0, arg1, precision);
   else
      emit_math2_gen4(c, function, dst, arg0, arg1, precision);
}

static void emit_exp_noalias( struct brw_vs_compile *c,
			      struct brw_reg dst,
			      struct brw_reg arg0 )
{
   struct brw_compile *p = &c->func;
   

   if (dst.dw1.bits.writemask & WRITEMASK_X) {
      struct brw_reg tmp = get_tmp(c);
      struct brw_reg tmp_d = retype(tmp, BRW_REGISTER_TYPE_D);

      /* tmp_d = floor(arg0.x) */
      brw_RNDD(p, tmp_d, brw_swizzle1(arg0, 0));

      /* result[0] = 2.0 ^ tmp */

      /* Adjust exponent for floating point: 
       * exp += 127 
       */
      brw_ADD(p, brw_writemask(tmp_d, WRITEMASK_X), tmp_d, brw_imm_d(127));

      /* Install exponent and sign.  
       * Excess drops off the edge: 
       */
      brw_SHL(p, brw_writemask(retype(dst, BRW_REGISTER_TYPE_D), WRITEMASK_X), 
	      tmp_d, brw_imm_d(23));

      release_tmp(c, tmp);
   }

   if (dst.dw1.bits.writemask & WRITEMASK_Y) {
      /* result[1] = arg0.x - floor(arg0.x) */
      brw_FRC(p, brw_writemask(dst, WRITEMASK_Y), brw_swizzle1(arg0, 0));
   }
   
   if (dst.dw1.bits.writemask & WRITEMASK_Z) {
      /* As with the LOG instruction, we might be better off just
       * doing a taylor expansion here, seeing as we have to do all
       * the prep work.
       *
       * If mathbox partial precision is too low, consider also:
       * result[3] = result[0] * EXP(result[1])
       */
      emit_math1(c, 
		 BRW_MATH_FUNCTION_EXP, 
		 brw_writemask(dst, WRITEMASK_Z),
		 brw_swizzle1(arg0, 0), 
		 BRW_MATH_PRECISION_FULL);
   }  

   if (dst.dw1.bits.writemask & WRITEMASK_W) {
      /* result[3] = 1.0; */
      brw_MOV(p, brw_writemask(dst, WRITEMASK_W), brw_imm_f(1));
   }
}


static void emit_log_noalias( struct brw_vs_compile *c,
			      struct brw_reg dst,
			      struct brw_reg arg0 )
{
   struct brw_compile *p = &c->func;
   struct brw_reg tmp = dst;
   struct brw_reg tmp_ud = retype(tmp, BRW_REGISTER_TYPE_UD);
   struct brw_reg arg0_ud = retype(arg0, BRW_REGISTER_TYPE_UD);
   GLboolean need_tmp = (dst.dw1.bits.writemask != 0xf ||
			 dst.file != BRW_GENERAL_REGISTER_FILE);

   if (need_tmp) {
      tmp = get_tmp(c);
      tmp_ud = retype(tmp, BRW_REGISTER_TYPE_UD);
   }
   
   /* Perform mant = frexpf(fabsf(x), &exp), adjust exp and mnt
    * according to spec:
    *
    * These almost look likey they could be joined up, but not really
    * practical:
    *
    * result[0].f = (x.i & ((1<<31)-1) >> 23) - 127
    * result[1].i = (x.i & ((1<<23)-1)        + (127<<23)
    */
   if (dst.dw1.bits.writemask & WRITEMASK_XZ) {
      brw_AND(p, 
	      brw_writemask(tmp_ud, WRITEMASK_X),
	      brw_swizzle1(arg0_ud, 0),
	      brw_imm_ud((1U<<31)-1));

      brw_SHR(p, 
	      brw_writemask(tmp_ud, WRITEMASK_X), 
	      tmp_ud,
	      brw_imm_ud(23));

      brw_ADD(p, 
	      brw_writemask(tmp, WRITEMASK_X), 
	      retype(tmp_ud, BRW_REGISTER_TYPE_D),	/* does it matter? */
	      brw_imm_d(-127));
   }

   if (dst.dw1.bits.writemask & WRITEMASK_YZ) {
      brw_AND(p, 
	      brw_writemask(tmp_ud, WRITEMASK_Y),
	      brw_swizzle1(arg0_ud, 0),
	      brw_imm_ud((1<<23)-1));

      brw_OR(p, 
	     brw_writemask(tmp_ud, WRITEMASK_Y), 
	     tmp_ud,
	     brw_imm_ud(127<<23));
   }
   
   if (dst.dw1.bits.writemask & WRITEMASK_Z) {
      /* result[2] = result[0] + LOG2(result[1]); */

      /* Why bother?  The above is just a hint how to do this with a
       * taylor series.  Maybe we *should* use a taylor series as by
       * the time all the above has been done it's almost certainly
       * quicker than calling the mathbox, even with low precision.
       * 
       * Options are:
       *    - result[0] + mathbox.LOG2(result[1])
       *    - mathbox.LOG2(arg0.x)
       *    - result[0] + inline_taylor_approx(result[1])
       */
      emit_math1(c, 
		 BRW_MATH_FUNCTION_LOG, 
		 brw_writemask(tmp, WRITEMASK_Z), 
		 brw_swizzle1(tmp, 1), 
		 BRW_MATH_PRECISION_FULL);
      
      brw_ADD(p, 
	      brw_writemask(tmp, WRITEMASK_Z), 
	      brw_swizzle1(tmp, 2), 
	      brw_swizzle1(tmp, 0));
   }  

   if (dst.dw1.bits.writemask & WRITEMASK_W) {
      /* result[3] = 1.0; */
      brw_MOV(p, brw_writemask(tmp, WRITEMASK_W), brw_imm_f(1));
   }

   if (need_tmp) {
      brw_MOV(p, dst, tmp);
      release_tmp(c, tmp);
   }
}


/* Need to unalias - consider swizzles:   r0 = DST r0.xxxx r1
 */
static void emit_dst_noalias( struct brw_vs_compile *c, 
			      struct brw_reg dst,
			      struct brw_reg arg0,
			      struct brw_reg arg1)
{
   struct brw_compile *p = &c->func;

   /* There must be a better way to do this: 
    */
   if (dst.dw1.bits.writemask & WRITEMASK_X)
      brw_MOV(p, brw_writemask(dst, WRITEMASK_X), brw_imm_f(1.0));
   if (dst.dw1.bits.writemask & WRITEMASK_Y)
      brw_MUL(p, brw_writemask(dst, WRITEMASK_Y), arg0, arg1);
   if (dst.dw1.bits.writemask & WRITEMASK_Z)
      brw_MOV(p, brw_writemask(dst, WRITEMASK_Z), arg0);
   if (dst.dw1.bits.writemask & WRITEMASK_W)
      brw_MOV(p, brw_writemask(dst, WRITEMASK_W), arg1);
}


static void emit_xpd( struct brw_compile *p,
		      struct brw_reg dst,
		      struct brw_reg t,
		      struct brw_reg u)
{
   brw_MUL(p, brw_null_reg(), brw_swizzle(t, 1,2,0,3),  brw_swizzle(u,2,0,1,3));
   brw_MAC(p, dst,     negate(brw_swizzle(t, 2,0,1,3)), brw_swizzle(u,1,2,0,3));
}


static void emit_lit_noalias( struct brw_vs_compile *c, 
			      struct brw_reg dst,
			      struct brw_reg arg0 )
{
   struct brw_compile *p = &c->func;
   struct brw_reg tmp = dst;
   GLboolean need_tmp = (dst.file != BRW_GENERAL_REGISTER_FILE);

   if (need_tmp) 
      tmp = get_tmp(c);
   
   brw_MOV(p, brw_writemask(dst, WRITEMASK_YZ), brw_imm_f(0)); 
   brw_MOV(p, brw_writemask(dst, WRITEMASK_XW), brw_imm_f(1)); 

   /* Need to use BRW_EXECUTE_8 and also do an 8-wide compare in order
    * to get all channels active inside the IF.  In the clipping code
    * we run with NoMask, so it's not an option and we can use
    * BRW_EXECUTE_1 for all comparisions.
    */
   brw_CMP(p, brw_null_reg(), BRW_CONDITIONAL_G, brw_swizzle1(arg0,0), brw_imm_f(0));
   brw_IF(p, BRW_EXECUTE_8);
   {
      brw_MOV(p, brw_writemask(dst, WRITEMASK_Y), brw_swizzle1(arg0,0));

      brw_CMP(p, brw_null_reg(), BRW_CONDITIONAL_G, brw_swizzle1(arg0,1), brw_imm_f(0));
      brw_MOV(p, brw_writemask(tmp, WRITEMASK_Z),  brw_swizzle1(arg0,1));
      brw_set_predicate_control(p, BRW_PREDICATE_NONE);

      emit_math2(c, 
		 BRW_MATH_FUNCTION_POW, 
		 brw_writemask(dst, WRITEMASK_Z),
		 brw_swizzle1(tmp, 2),
		 brw_swizzle1(arg0, 3),
		 BRW_MATH_PRECISION_PARTIAL);      
   }
   brw_ENDIF(p);

   release_tmp(c, tmp);
}

static void emit_lrp_noalias(struct brw_vs_compile *c,
			     struct brw_reg dst,
			     struct brw_reg arg0,
			     struct brw_reg arg1,
			     struct brw_reg arg2)
{
   struct brw_compile *p = &c->func;

   brw_ADD(p, dst, negate(arg0), brw_imm_f(1.0));
   brw_MUL(p, brw_null_reg(), dst, arg2);
   brw_MAC(p, dst, arg0, arg1);
}

/** 3 or 4-component vector normalization */
static void emit_nrm( struct brw_vs_compile *c, 
                      struct brw_reg dst,
                      struct brw_reg arg0,
                      int num_comps)
{
   struct brw_compile *p = &c->func;
   struct brw_reg tmp = get_tmp(c);

   /* tmp = dot(arg0, arg0) */
   if (num_comps == 3)
      brw_DP3(p, tmp, arg0, arg0);
   else
      brw_DP4(p, tmp, arg0, arg0);

   /* tmp = 1 / sqrt(tmp) */
   emit_math1(c, BRW_MATH_FUNCTION_RSQ, tmp, tmp, BRW_MATH_PRECISION_FULL);

   /* dst = arg0 * tmp */
   brw_MUL(p, dst, arg0, tmp);

   release_tmp(c, tmp);
}


static struct brw_reg
get_constant(struct brw_vs_compile *c,
             const struct prog_instruction *inst,
             GLuint argIndex)
{
   const struct prog_src_register *src = &inst->SrcReg[argIndex];
   struct brw_compile *p = &c->func;
   struct brw_reg const_reg = c->current_const[argIndex].reg;

   assert(argIndex < 3);

   if (c->current_const[argIndex].index != src->Index) {
      /* Keep track of the last constant loaded in this slot, for reuse. */
      c->current_const[argIndex].index = src->Index;

#if 0
      printf("  fetch const[%d] for arg %d into reg %d\n",
             src->Index, argIndex, c->current_const[argIndex].reg.nr);
#endif
      /* need to fetch the constant now */
      brw_dp_READ_4_vs(p,
                       const_reg,                     /* writeback dest */
                       16 * src->Index,               /* byte offset */
                       SURF_INDEX_VERT_CONST_BUFFER   /* binding table index */
                       );
   }

   /* replicate lower four floats into upper half (to get XYZWXYZW) */
   const_reg = stride(const_reg, 0, 4, 1);
   const_reg.subnr = 0;

   return const_reg;
}

static struct brw_reg
get_reladdr_constant(struct brw_vs_compile *c,
		     const struct prog_instruction *inst,
		     GLuint argIndex)
{
   const struct prog_src_register *src = &inst->SrcReg[argIndex];
   struct brw_compile *p = &c->func;
   struct brw_context *brw = p->brw;
   struct intel_context *intel = &brw->intel;
   struct brw_reg const_reg = c->current_const[argIndex].reg;
   struct brw_reg addr_reg = c->regs[PROGRAM_ADDRESS][0];
   uint32_t offset;

   assert(argIndex < 3);

   /* Can't reuse a reladdr constant load. */
   c->current_const[argIndex].index = -1;

 #if 0
   printf("  fetch const[a0.x+%d] for arg %d into reg %d\n",
	  src->Index, argIndex, c->current_const[argIndex].reg.nr);
#endif

   if (intel->gen >= 6) {
      offset = src->Index;
   } else {
      struct brw_reg byte_addr_reg = retype(get_tmp(c), BRW_REGISTER_TYPE_D);
      brw_MUL(p, byte_addr_reg, addr_reg, brw_imm_d(16));
      addr_reg = byte_addr_reg;
      offset = 16 * src->Index;
   }

   /* fetch the first vec4 */
   brw_dp_READ_4_vs_relative(p,
			     const_reg,
			     addr_reg,
			     offset,
			     SURF_INDEX_VERT_CONST_BUFFER);

   return const_reg;
}



/* TODO: relative addressing!
 */
static struct brw_reg get_reg( struct brw_vs_compile *c,
			       gl_register_file file,
			       GLuint index )
{
   switch (file) {
   case PROGRAM_TEMPORARY:
   case PROGRAM_INPUT:
   case PROGRAM_OUTPUT:
      assert(c->regs[file][index].nr != 0);
      return c->regs[file][index];
   case PROGRAM_STATE_VAR:
   case PROGRAM_CONSTANT:
   case PROGRAM_UNIFORM:
      assert(c->regs[PROGRAM_STATE_VAR][index].nr != 0);
      return c->regs[PROGRAM_STATE_VAR][index];
   case PROGRAM_ADDRESS:
      assert(index == 0);
      return c->regs[file][index];

   case PROGRAM_UNDEFINED:			/* undef values */
      return brw_null_reg();

   case PROGRAM_LOCAL_PARAM: 
   case PROGRAM_ENV_PARAM: 
   case PROGRAM_WRITE_ONLY:
   default:
      assert(0);
      return brw_null_reg();
   }
}


/**
 * Indirect addressing:  get reg[[arg] + offset].
 */
static struct brw_reg deref( struct brw_vs_compile *c,
			     struct brw_reg arg,
			     GLint offset,
			     GLuint reg_size )
{
   struct brw_compile *p = &c->func;
   struct brw_reg tmp = get_tmp(c);
   struct brw_reg addr_reg = c->regs[PROGRAM_ADDRESS][0];
   struct brw_reg vp_address = retype(vec1(addr_reg), BRW_REGISTER_TYPE_D);
   GLuint byte_offset = arg.nr * 32 + arg.subnr + offset * reg_size;
   struct brw_reg indirect = brw_vec4_indirect(0,0);
   struct brw_reg acc = retype(vec1(get_tmp(c)), BRW_REGISTER_TYPE_UW);

   /* Set the vertical stride on the register access so that the first
    * 4 components come from a0.0 and the second 4 from a0.1.
    */
   indirect.vstride = BRW_VERTICAL_STRIDE_ONE_DIMENSIONAL;

   {
      brw_push_insn_state(p);
      brw_set_access_mode(p, BRW_ALIGN_1);

      brw_MUL(p, acc, vp_address, brw_imm_uw(reg_size));
      brw_ADD(p, brw_address_reg(0), acc, brw_imm_uw(byte_offset));

      brw_MUL(p, acc, suboffset(vp_address, 4), brw_imm_uw(reg_size));
      brw_ADD(p, brw_address_reg(1), acc, brw_imm_uw(byte_offset));

      brw_MOV(p, tmp, indirect);

      brw_pop_insn_state(p);
   }

   /* NOTE: tmp not released */
   return tmp;
}

static void
move_to_reladdr_dst(struct brw_vs_compile *c,
		    const struct prog_instruction *inst,
		    struct brw_reg val)
{
   struct brw_compile *p = &c->func;
   int reg_size = 32;
   struct brw_reg addr_reg = c->regs[PROGRAM_ADDRESS][0];
   struct brw_reg vp_address = retype(vec1(addr_reg), BRW_REGISTER_TYPE_D);
   struct brw_reg base = c->regs[inst->DstReg.File][inst->DstReg.Index];
   GLuint byte_offset = base.nr * 32 + base.subnr;
   struct brw_reg indirect = brw_vec4_indirect(0,0);
   struct brw_reg acc = retype(vec1(get_tmp(c)), BRW_REGISTER_TYPE_UW);

   /* Because destination register indirect addressing can only use
    * one index, we'll write each vertex's vec4 value separately.
    */
   val.width = BRW_WIDTH_4;
   val.vstride = BRW_VERTICAL_STRIDE_4;

   brw_push_insn_state(p);
   brw_set_access_mode(p, BRW_ALIGN_1);

   brw_MUL(p, acc, vp_address, brw_imm_uw(reg_size));
   brw_ADD(p, brw_address_reg(0), acc, brw_imm_uw(byte_offset));
   brw_MOV(p, indirect, val);

   brw_MUL(p, acc, suboffset(vp_address, 4), brw_imm_uw(reg_size));
   brw_ADD(p, brw_address_reg(0), acc,
	   brw_imm_uw(byte_offset + reg_size / 2));
   brw_MOV(p, indirect, suboffset(val, 4));

   brw_pop_insn_state(p);
}

/**
 * Get brw reg corresponding to the instruction's [argIndex] src reg.
 * TODO: relative addressing!
 */
static struct brw_reg
get_src_reg( struct brw_vs_compile *c,
             const struct prog_instruction *inst,
             GLuint argIndex )
{
   const GLuint file = inst->SrcReg[argIndex].File;
   const GLint index = inst->SrcReg[argIndex].Index;
   const GLboolean relAddr = inst->SrcReg[argIndex].RelAddr;

   if (brw_vs_arg_can_be_immediate(inst->Opcode, argIndex)) {
      const struct prog_src_register *src = &inst->SrcReg[argIndex];

      if (src->Swizzle == MAKE_SWIZZLE4(SWIZZLE_ZERO,
					SWIZZLE_ZERO,
					SWIZZLE_ZERO,
					SWIZZLE_ZERO)) {
	  return brw_imm_f(0.0f);
      } else if (src->Swizzle == MAKE_SWIZZLE4(SWIZZLE_ONE,
					       SWIZZLE_ONE,
					       SWIZZLE_ONE,
					       SWIZZLE_ONE)) {
	 if (src->Negate)
	    return brw_imm_f(-1.0F);
	 else
	    return brw_imm_f(1.0F);
      } else if (src->File == PROGRAM_CONSTANT) {
	 const struct gl_program_parameter_list *params;
	 float f;
	 int component = -1;

	 switch (src->Swizzle) {
	 case SWIZZLE_XXXX:
	    component = 0;
	    break;
	 case SWIZZLE_YYYY:
	    component = 1;
	    break;
	 case SWIZZLE_ZZZZ:
	    component = 2;
	    break;
	 case SWIZZLE_WWWW:
	    component = 3;
	    break;
	 }

	 if (component >= 0) {
	    params = c->vp->program.Base.Parameters;
	    f = params->ParameterValues[src->Index][component];

	    if (src->Abs)
	       f = fabs(f);
	    if (src->Negate)
	       f = -f;
	    return brw_imm_f(f);
	 }
      }
   }

   switch (file) {
   case PROGRAM_TEMPORARY:
   case PROGRAM_INPUT:
   case PROGRAM_OUTPUT:
      if (relAddr) {
         return deref(c, c->regs[file][0], index, 32);
      }
      else {
         assert(c->regs[file][index].nr != 0);
         return c->regs[file][index];
      }

   case PROGRAM_STATE_VAR:
   case PROGRAM_CONSTANT:
   case PROGRAM_UNIFORM:
   case PROGRAM_ENV_PARAM:
   case PROGRAM_LOCAL_PARAM:
      if (!relAddr && c->constant_map[index] != -1) {
	 /* Take from the push constant buffer if possible. */
	 assert(c->regs[PROGRAM_STATE_VAR][c->constant_map[index]].nr != 0);
	 return c->regs[PROGRAM_STATE_VAR][c->constant_map[index]];
      } else {
	 /* Must be in the pull constant buffer then .*/
	 assert(c->vp->use_const_buffer);
	 if (relAddr)
	    return get_reladdr_constant(c, inst, argIndex);
	 else
	    return get_constant(c, inst, argIndex);
      }
   case PROGRAM_ADDRESS:
      assert(index == 0);
      return c->regs[file][index];

   case PROGRAM_UNDEFINED:
      /* this is a normal case since we loop over all three src args */
      return brw_null_reg();

   case PROGRAM_WRITE_ONLY:
   default:
      assert(0);
      return brw_null_reg();
   }
}

/**
 * Return the brw reg for the given instruction's src argument.
 * Will return mangled results for SWZ op.  The emit_swz() function
 * ignores this result and recalculates taking extended swizzles into
 * account.
 */
static struct brw_reg get_arg( struct brw_vs_compile *c,
                               const struct prog_instruction *inst,
                               GLuint argIndex )
{
   const struct prog_src_register *src = &inst->SrcReg[argIndex];
   struct brw_reg reg;

   if (src->File == PROGRAM_UNDEFINED)
      return brw_null_reg();

   reg = get_src_reg(c, inst, argIndex);

   /* Convert 3-bit swizzle to 2-bit.  
    */
   if (reg.file != BRW_IMMEDIATE_VALUE) {
      reg.dw1.bits.swizzle = BRW_SWIZZLE4(GET_SWZ(src->Swizzle, 0),
					  GET_SWZ(src->Swizzle, 1),
					  GET_SWZ(src->Swizzle, 2),
					  GET_SWZ(src->Swizzle, 3));

      /* Note this is ok for non-swizzle ARB_vp instructions */
      reg.negate = src->Negate ? 1 : 0;
   }

   return reg;
}


/**
 * Get brw register for the given program dest register.
 */
static struct brw_reg get_dst( struct brw_vs_compile *c,
			       struct prog_dst_register dst )
{
   struct brw_reg reg;

   switch (dst.File) {
   case PROGRAM_TEMPORARY:
   case PROGRAM_OUTPUT:
      /* register-indirect addressing is only 1x1, not VxH, for
       * destination regs.  So, for RelAddr we'll return a temporary
       * for the dest and do a move of the result to the RelAddr
       * register after the instruction emit.
       */
      if (dst.RelAddr) {
	 reg = get_tmp(c);
      } else {
	 assert(c->regs[dst.File][dst.Index].nr != 0);
	 reg = c->regs[dst.File][dst.Index];
      }
      break;
   case PROGRAM_ADDRESS:
      assert(dst.Index == 0);
      reg = c->regs[dst.File][dst.Index];
      break;
   case PROGRAM_UNDEFINED:
      /* we may hit this for OPCODE_END, OPCODE_KIL, etc */
      reg = brw_null_reg();
      break;
   default:
      assert(0);
      reg = brw_null_reg();
   }

   assert(reg.type != BRW_IMMEDIATE_VALUE);
   reg.dw1.bits.writemask = dst.WriteMask;

   return reg;
}


static void emit_swz( struct brw_vs_compile *c, 
		      struct brw_reg dst,
                      const struct prog_instruction *inst)
{
   const GLuint argIndex = 0;
   const struct prog_src_register src = inst->SrcReg[argIndex];
   struct brw_compile *p = &c->func;
   GLuint zeros_mask = 0;
   GLuint ones_mask = 0;
   GLuint src_mask = 0;
   GLubyte src_swz[4];
   GLboolean need_tmp = (src.Negate &&
			 dst.file != BRW_GENERAL_REGISTER_FILE);
   struct brw_reg tmp = dst;
   GLuint i;

   if (need_tmp)
      tmp = get_tmp(c);

   for (i = 0; i < 4; i++) {
      if (dst.dw1.bits.writemask & (1<<i)) {
	 GLubyte s = GET_SWZ(src.Swizzle, i);
	 switch (s) {
	 case SWIZZLE_X:
	 case SWIZZLE_Y:
	 case SWIZZLE_Z:
	 case SWIZZLE_W:
	    src_mask |= 1<<i;
	    src_swz[i] = s;
	    break;
	 case SWIZZLE_ZERO:
	    zeros_mask |= 1<<i;
	    break;
	 case SWIZZLE_ONE:
	    ones_mask |= 1<<i;
	    break;
	 }
      }
   }
   
   /* Do src first, in case dst aliases src:
    */
   if (src_mask) {
      struct brw_reg arg0;

      arg0 = get_src_reg(c, inst, argIndex);

      arg0 = brw_swizzle(arg0, 
			 src_swz[0], src_swz[1], 
			 src_swz[2], src_swz[3]);

      brw_MOV(p, brw_writemask(tmp, src_mask), arg0);
   } 
   
   if (zeros_mask) 
      brw_MOV(p, brw_writemask(tmp, zeros_mask), brw_imm_f(0));

   if (ones_mask) 
      brw_MOV(p, brw_writemask(tmp, ones_mask), brw_imm_f(1));

   if (src.Negate)
      brw_MOV(p, brw_writemask(tmp, src.Negate), negate(tmp));
   
   if (need_tmp) {
      brw_MOV(p, dst, tmp);
      release_tmp(c, tmp);
   }
}

static int
align_interleaved_urb_mlen(struct brw_context *brw, int mlen)
{
   struct intel_context *intel = &brw->intel;

   if (intel->gen >= 6) {
      /* URB data written (does not include the message header reg) must
       * be a multiple of 256 bits, or 2 VS registers.  See vol5c.5,
       * section 5.4.3.2.2: URB_INTERLEAVED.
       *
       * URB entries are allocated on a multiple of 1024 bits, so an
       * extra 128 bits written here to make the end align to 256 is
       * no problem.
       */
      if ((mlen % 2) != 1)
	 mlen++;
   }

   return mlen;
}

/**
 * Post-vertex-program processing.  Send the results to the URB.
 */
static void emit_vertex_write( struct brw_vs_compile *c)
{
   struct brw_compile *p = &c->func;
   struct brw_context *brw = p->brw;
   struct intel_context *intel = &brw->intel;
   struct brw_reg pos = c->regs[PROGRAM_OUTPUT][VERT_RESULT_HPOS];
   struct brw_reg ndc;
   int eot;
   GLuint len_vertex_header = 2;
   int next_mrf, i;
   int msg_len;

   if (c->key.copy_edgeflag) {
      brw_MOV(p, 
	      get_reg(c, PROGRAM_OUTPUT, VERT_RESULT_EDGE),
	      get_reg(c, PROGRAM_INPUT, VERT_ATTRIB_EDGEFLAG));
   }

   if (intel->gen < 6) {
      /* Build ndc coords */
      ndc = get_tmp(c);
      /* ndc = 1.0 / pos.w */
      emit_math1(c, BRW_MATH_FUNCTION_INV, ndc, brw_swizzle1(pos, 3), BRW_MATH_PRECISION_FULL);
      /* ndc.xyz = pos * ndc */
      brw_MUL(p, brw_writemask(ndc, WRITEMASK_XYZ), pos, ndc);
   }

   /* Update the header for point size, user clipping flags, and -ve rhw
    * workaround.
    */
   if (intel->gen >= 6) {
      struct brw_reg m1 = brw_message_reg(1);

      /* On gen6, m1 has each value in a separate dword, so we never
       * need to mess with a temporary for computing the m1 value.
       */
      brw_MOV(p, retype(m1, BRW_REGISTER_TYPE_UD), brw_imm_ud(0));
      if (c->prog_data.outputs_written & BITFIELD64_BIT(VERT_RESULT_PSIZ)) {
	 brw_MOV(p, brw_writemask(m1, WRITEMASK_W),
		 brw_swizzle1(c->regs[PROGRAM_OUTPUT][VERT_RESULT_PSIZ], 0));
      }

      /* Set the user clip distances in dword 8-15. (m3-4)*/
      if (c->key.nr_userclip) {
	 for (i = 0; i < c->key.nr_userclip; i++) {
	    struct brw_reg m;
	    if (i < 4)
	       m = brw_message_reg(3);
	    else
	       m = brw_message_reg(4);

	    brw_DP4(p, brw_writemask(m, (1 << (i & 3))),pos, c->userplane[i]);
	 }
      }
   } else if ((c->prog_data.outputs_written &
	       BITFIELD64_BIT(VERT_RESULT_PSIZ)) ||
	      c->key.nr_userclip || brw->has_negative_rhw_bug) {
      struct brw_reg header1 = retype(get_tmp(c), BRW_REGISTER_TYPE_UD);
      GLuint i;

      brw_MOV(p, header1, brw_imm_ud(0));

      brw_set_access_mode(p, BRW_ALIGN_16);	

      if (c->prog_data.outputs_written & BITFIELD64_BIT(VERT_RESULT_PSIZ)) {
	 struct brw_reg psiz = c->regs[PROGRAM_OUTPUT][VERT_RESULT_PSIZ];
	 brw_MUL(p, brw_writemask(header1, WRITEMASK_W),
		 brw_swizzle1(psiz, 0), brw_imm_f(1<<11));
	 brw_AND(p, brw_writemask(header1, WRITEMASK_W),
		 header1, brw_imm_ud(0x7ff<<8));
      }

      for (i = 0; i < c->key.nr_userclip; i++) {
	 brw_set_conditionalmod(p, BRW_CONDITIONAL_L);
	 brw_DP4(p, brw_null_reg(), pos, c->userplane[i]);
	 brw_OR(p, brw_writemask(header1, WRITEMASK_W), header1, brw_imm_ud(1<<i));
	 brw_set_predicate_control(p, BRW_PREDICATE_NONE);
      }

      /* i965 clipping workaround: 
       * 1) Test for -ve rhw
       * 2) If set, 
       *      set ndc = (0,0,0,0)
       *      set ucp[6] = 1
       *
       * Later, clipping will detect ucp[6] and ensure the primitive is
       * clipped against all fixed planes.
       */
      if (brw->has_negative_rhw_bug) {
	 brw_CMP(p,
		 vec8(brw_null_reg()),
		 BRW_CONDITIONAL_L,
		 brw_swizzle1(ndc, 3),
		 brw_imm_f(0));
   
	 brw_OR(p, brw_writemask(header1, WRITEMASK_W), header1, brw_imm_ud(1<<6));
	 brw_MOV(p, ndc, brw_imm_f(0));
	 brw_set_predicate_control(p, BRW_PREDICATE_NONE);
      }

      brw_set_access_mode(p, BRW_ALIGN_1);	/* why? */
      brw_MOV(p, retype(brw_message_reg(1), BRW_REGISTER_TYPE_UD), header1);
      brw_set_access_mode(p, BRW_ALIGN_16);

      release_tmp(c, header1);
   }
   else {
      brw_MOV(p, retype(brw_message_reg(1), BRW_REGISTER_TYPE_UD), brw_imm_ud(0));
   }

   /* Emit the (interleaved) headers for the two vertices - an 8-reg
    * of zeros followed by two sets of NDC coordinates:
    */
   brw_set_access_mode(p, BRW_ALIGN_1);
   brw_set_acc_write_control(p, 0);

   /* The VUE layout is documented in Volume 2a. */
   if (intel->gen >= 6) {
      /* There are 8 or 16 DWs (D0-D15) in VUE header on Sandybridge:
       * dword 0-3 (m1) of the header is indices, point width, clip flags.
       * dword 4-7 (m2) is the 4D space position
       * dword 8-15 (m3,m4) of the vertex header is the user clip distance if
       * enabled.
       * m3 or 5 is the first vertex element data we fill, which is
       * the vertex position.
       */
      brw_MOV(p, brw_message_reg(2), pos);
      len_vertex_header = 1;
      if (c->key.nr_userclip > 0)
	 len_vertex_header += 2;
   } else if (intel->gen == 5) {
      /* There are 20 DWs (D0-D19) in VUE header on Ironlake:
       * dword 0-3 (m1) of the header is indices, point width, clip flags.
       * dword 4-7 (m2) is the ndc position (set above)
       * dword 8-11 (m3) of the vertex header is the 4D space position
       * dword 12-19 (m4,m5) of the vertex header is the user clip distance.
       * m6 is a pad so that the vertex element data is aligned
       * m7 is the first vertex data we fill, which is the vertex position.
       */
      brw_MOV(p, brw_message_reg(2), ndc);
      brw_MOV(p, brw_message_reg(3), pos);
      brw_MOV(p, brw_message_reg(7), pos);
      len_vertex_header = 6;
   } else {
      /* There are 8 dwords in VUE header pre-Ironlake:
       * dword 0-3 (m1) is indices, point width, clip flags.
       * dword 4-7 (m2) is ndc position (set above)
       *
       * dword 8-11 (m3) is the first vertex data, which we always have be the
       * vertex position.
       */
      brw_MOV(p, brw_message_reg(2), ndc);
      brw_MOV(p, brw_message_reg(3), pos);
      len_vertex_header = 2;
   }

   /* Move variable-addressed, non-overflow outputs to their MRFs. */
   next_mrf = 2 + len_vertex_header;
   for (i = 0; i < VERT_RESULT_MAX; i++) {
      if (c->first_overflow_output > 0 && i >= c->first_overflow_output)
	 break;
      if (!(c->prog_data.outputs_written & BITFIELD64_BIT(i)))
	 continue;
      if (i == VERT_RESULT_PSIZ)
	 continue;

      if (i >= VERT_RESULT_TEX0 &&
	  c->regs[PROGRAM_OUTPUT][i].file == BRW_GENERAL_REGISTER_FILE) {
	 brw_MOV(p, brw_message_reg(next_mrf), c->regs[PROGRAM_OUTPUT][i]);
	 next_mrf++;
      } else if (c->regs[PROGRAM_OUTPUT][i].file == BRW_MESSAGE_REGISTER_FILE) {
	 next_mrf = c->regs[PROGRAM_OUTPUT][i].nr + 1;
      }
   }

   eot = (c->first_overflow_output == 0);

   /* Message header, plus VUE header, plus the (first set of) outputs. */
   msg_len = 1 + len_vertex_header + c->nr_outputs;
   msg_len = align_interleaved_urb_mlen(brw, msg_len);
   /* Any outputs beyond BRW_MAX_MRF should be past first_overflow_output */
   msg_len = MIN2(msg_len, (BRW_MAX_MRF - 1)),

   brw_urb_WRITE(p, 
		 brw_null_reg(), /* dest */
		 0,		/* starting mrf reg nr */
		 c->r0,		/* src */
		 0,		/* allocate */
		 1,		/* used */
		 msg_len,
		 0,		/* response len */
		 eot, 		/* eot */
		 eot, 		/* writes complete */
		 0, 		/* urb destination offset */
		 BRW_URB_SWIZZLE_INTERLEAVE);

   if (c->first_overflow_output > 0) {
      /* Not all of the vertex outputs/results fit into the MRF.
       * Move the overflowed attributes from the GRF to the MRF and
       * issue another brw_urb_WRITE().
       */
      GLuint i, mrf = 1;
      for (i = c->first_overflow_output; i < VERT_RESULT_MAX; i++) {
         if (c->prog_data.outputs_written & BITFIELD64_BIT(i)) {
            /* move from GRF to MRF */
            brw_MOV(p, brw_message_reg(mrf), c->regs[PROGRAM_OUTPUT][i]);
            mrf++;
         }
      }

      brw_urb_WRITE(p,
                    brw_null_reg(), /* dest */
                    0,              /* starting mrf reg nr */
                    c->r0,          /* src */
                    0,              /* allocate */
                    1,              /* used */
                    align_interleaved_urb_mlen(brw, mrf),
                    0,              /* response len */
                    1,              /* eot */
                    1,              /* writes complete */
                    14 / 2,  /* urb destination offset */
                    BRW_URB_SWIZZLE_INTERLEAVE);
   }
}

static GLboolean
accumulator_contains(struct brw_vs_compile *c, struct brw_reg val)
{
   struct brw_compile *p = &c->func;
   struct brw_instruction *prev_insn = &p->store[p->nr_insn - 1];

   if (p->nr_insn == 0)
      return GL_FALSE;

   if (val.address_mode != BRW_ADDRESS_DIRECT)
      return GL_FALSE;

   if (val.negate || val.abs)
      return GL_FALSE;

   switch (prev_insn->header.opcode) {
   case BRW_OPCODE_MOV:
   case BRW_OPCODE_MAC:
   case BRW_OPCODE_MUL:
      if (prev_insn->header.access_mode == BRW_ALIGN_16 &&
	  prev_insn->header.execution_size == val.width &&
	  prev_insn->bits1.da1.dest_reg_file == val.file &&
	  prev_insn->bits1.da1.dest_reg_type == val.type &&
	  prev_insn->bits1.da1.dest_address_mode == val.address_mode &&
	  prev_insn->bits1.da1.dest_reg_nr == val.nr &&
	  prev_insn->bits1.da16.dest_subreg_nr == val.subnr / 16 &&
	  prev_insn->bits1.da16.dest_writemask == 0xf)
	 return GL_TRUE;
      else
	 return GL_FALSE;
   default:
      return GL_FALSE;
   }
}

static uint32_t
get_predicate(const struct prog_instruction *inst)
{
   if (inst->DstReg.CondMask == COND_TR)
      return BRW_PREDICATE_NONE;

   /* All of GLSL only produces predicates for COND_NE and one channel per
    * vector.  Fail badly if someone starts doing something else, as it might
    * mean infinite looping or something.
    *
    * We'd like to support all the condition codes, but our hardware doesn't
    * quite match the Mesa IR, which is modeled after the NV extensions.  For
    * those, the instruction may update the condition codes or not, then any
    * later instruction may use one of those condition codes.  For gen4, the
    * instruction may update the flags register based on one of the condition
    * codes output by the instruction, and then further instructions may
    * predicate on that.  We can probably support this, but it won't
    * necessarily be easy.
    */
   assert(inst->DstReg.CondMask == COND_NE);

   switch (inst->DstReg.CondSwizzle) {
   case SWIZZLE_XXXX:
      return BRW_PREDICATE_ALIGN16_REPLICATE_X;
   case SWIZZLE_YYYY:
      return BRW_PREDICATE_ALIGN16_REPLICATE_Y;
   case SWIZZLE_ZZZZ:
      return BRW_PREDICATE_ALIGN16_REPLICATE_Z;
   case SWIZZLE_WWWW:
      return BRW_PREDICATE_ALIGN16_REPLICATE_W;
   default:
      _mesa_problem(NULL, "Unexpected predicate: 0x%08x\n",
		    inst->DstReg.CondMask);
      return BRW_PREDICATE_NORMAL;
   }
}

static void
brw_vs_rescale_gl_fixed(struct brw_vs_compile *c)
{
   struct brw_compile *p = &c->func;
   int i;

   for (i = 0; i < VERT_ATTRIB_MAX; i++) {
      if (!(c->prog_data.inputs_read & (1 << i)))
	 continue;

      if (c->key.gl_fixed_input_size[i] != 0) {
	 struct brw_reg reg = c->regs[PROGRAM_INPUT][i];

	 brw_MUL(p,
		 brw_writemask(reg, (1 << c->key.gl_fixed_input_size[i]) - 1),
		 reg, brw_imm_f(1.0 / 65536.0));
      }
   }
}

/* Emit the vertex program instructions here.
 */
void brw_vs_emit(struct brw_vs_compile *c )
{
#define MAX_IF_DEPTH 32
#define MAX_LOOP_DEPTH 32
   struct brw_compile *p = &c->func;
   struct brw_context *brw = p->brw;
   struct intel_context *intel = &brw->intel;
   const GLuint nr_insns = c->vp->program.Base.NumInstructions;
   GLuint insn, loop_depth = 0;
   struct brw_instruction *loop_inst[MAX_LOOP_DEPTH] = { 0 };
   int if_depth_in_loop[MAX_LOOP_DEPTH];
   const struct brw_indirect stack_index = brw_indirect(0, 0);   
   GLuint index;
   GLuint file;

   if (unlikely(INTEL_DEBUG & DEBUG_VS)) {
      printf("vs-mesa:\n");
      _mesa_fprint_program_opt(stdout, &c->vp->program.Base, PROG_PRINT_DEBUG,
			       GL_TRUE);
      printf("\n");
   }

   brw_set_compression_control(p, BRW_COMPRESSION_NONE);
   brw_set_access_mode(p, BRW_ALIGN_16);
   if_depth_in_loop[loop_depth] = 0;

   brw_set_acc_write_control(p, 1);

   for (insn = 0; insn < nr_insns; insn++) {
       GLuint i;
       struct prog_instruction *inst = &c->vp->program.Base.Instructions[insn];

       /* Message registers can't be read, so copy the output into GRF
	* register if they are used in source registers
	*/
       for (i = 0; i < 3; i++) {
	   struct prog_src_register *src = &inst->SrcReg[i];
	   GLuint index = src->Index;
	   GLuint file = src->File;	
	   if (file == PROGRAM_OUTPUT && index != VERT_RESULT_HPOS)
	       c->output_regs[index].used_in_src = GL_TRUE;
       }

       switch (inst->Opcode) {
       case OPCODE_CAL:
       case OPCODE_RET:
	  c->needs_stack = GL_TRUE;
	  break;
       default:
	  break;
       }
   }

   /* Static register allocation
    */
   brw_vs_alloc_regs(c);

   brw_vs_rescale_gl_fixed(c);

   if (c->needs_stack)
      brw_MOV(p, get_addr_reg(stack_index), brw_address(c->stack));

   for (insn = 0; insn < nr_insns; insn++) {

      const struct prog_instruction *inst = &c->vp->program.Base.Instructions[insn];
      struct brw_reg args[3], dst;
      GLuint i;

#if 0
      printf("%d: ", insn);
      _mesa_print_instruction(inst);
#endif

      /* Get argument regs.  SWZ is special and does this itself.
       */
      if (inst->Opcode != OPCODE_SWZ)
	  for (i = 0; i < 3; i++) {
	      const struct prog_src_register *src = &inst->SrcReg[i];
	      index = src->Index;
	      file = src->File;	
	      if (file == PROGRAM_OUTPUT && c->output_regs[index].used_in_src) {
		 /* Can't just make get_arg "do the right thing" here because
		  * other callers of get_arg and get_src_reg don't expect any
		  * special behavior for the c->output_regs[index].used_in_src
		  * case.
		  */
		 args[i] = c->output_regs[index].reg;
		 args[i].dw1.bits.swizzle =
		    BRW_SWIZZLE4(GET_SWZ(src->Swizzle, 0),
				 GET_SWZ(src->Swizzle, 1),
				 GET_SWZ(src->Swizzle, 2),
				 GET_SWZ(src->Swizzle, 3));

		 /* Note this is ok for non-swizzle ARB_vp instructions */
		 args[i].negate = src->Negate ? 1 : 0;
	      } else
                  args[i] = get_arg(c, inst, i);
	  }

      /* Get dest regs.  Note that it is possible for a reg to be both
       * dst and arg, given the static allocation of registers.  So
       * care needs to be taken emitting multi-operation instructions.
       */ 
      index = inst->DstReg.Index;
      file = inst->DstReg.File;
      if (file == PROGRAM_OUTPUT && c->output_regs[index].used_in_src)
	 /* Can't just make get_dst "do the right thing" here because other
	  * callers of get_dst don't expect any special behavior for the
	  * c->output_regs[index].used_in_src case.
	  */
	 dst = brw_writemask(c->output_regs[index].reg, inst->DstReg.WriteMask);
      else
	  dst = get_dst(c, inst->DstReg);

      if (inst->SaturateMode != SATURATE_OFF) {
	 _mesa_problem(NULL, "Unsupported saturate %d in vertex shader",
                       inst->SaturateMode);
      }

      switch (inst->Opcode) {
      case OPCODE_ABS:
	 args[0].negate = false;
	 brw_MOV(p, dst, brw_abs(args[0]));
	 break;
      case OPCODE_ADD:
	 brw_ADD(p, dst, args[0], args[1]);
	 break;
      case OPCODE_COS:
	 emit_math1(c, BRW_MATH_FUNCTION_COS, dst, args[0], BRW_MATH_PRECISION_FULL);
	 break;
      case OPCODE_DP2:
	 brw_DP2(p, dst, args[0], args[1]);
	 break;
      case OPCODE_DP3:
	 brw_DP3(p, dst, args[0], args[1]);
	 break;
      case OPCODE_DP4:
	 brw_DP4(p, dst, args[0], args[1]);
	 break;
      case OPCODE_DPH:
	 brw_DPH(p, dst, args[0], args[1]);
	 break;
      case OPCODE_NRM3:
	 emit_nrm(c, dst, args[0], 3);
	 break;
      case OPCODE_NRM4:
	 emit_nrm(c, dst, args[0], 4);
	 break;
      case OPCODE_DST:
	 unalias2(c, dst, args[0], args[1], emit_dst_noalias); 
	 break;
      case OPCODE_EXP:
	 unalias1(c, dst, args[0], emit_exp_noalias);
	 break;
      case OPCODE_EX2:
	 emit_math1(c, BRW_MATH_FUNCTION_EXP, dst, args[0], BRW_MATH_PRECISION_FULL);
	 break;
      case OPCODE_ARL:
	 emit_arl(p, dst, args[0]);
	 break;
      case OPCODE_FLR:
	 brw_RNDD(p, dst, args[0]);
	 break;
      case OPCODE_FRC:
	 brw_FRC(p, dst, args[0]);
	 break;
      case OPCODE_LOG:
	 unalias1(c, dst, args[0], emit_log_noalias);
	 break;
      case OPCODE_LG2:
	 emit_math1(c, BRW_MATH_FUNCTION_LOG, dst, args[0], BRW_MATH_PRECISION_FULL);
	 break;
      case OPCODE_LIT:
	 unalias1(c, dst, args[0], emit_lit_noalias);
	 break;
      case OPCODE_LRP:
	 unalias3(c, dst, args[0], args[1], args[2], emit_lrp_noalias);
	 break;
      case OPCODE_MAD:
	 if (!accumulator_contains(c, args[2]))
	    brw_MOV(p, brw_acc_reg(), args[2]);
	 brw_MAC(p, dst, args[0], args[1]);
	 break;
      case OPCODE_CMP:
	 emit_cmp(p, dst, args[0], args[1], args[2]);
	 break;
      case OPCODE_MAX:
	 emit_max(p, dst, args[0], args[1]);
	 break;
      case OPCODE_MIN:
	 emit_min(p, dst, args[0], args[1]);
	 break;
      case OPCODE_MOV:
	 brw_MOV(p, dst, args[0]);
	 break;
      case OPCODE_MUL:
	 brw_MUL(p, dst, args[0], args[1]);
	 break;
      case OPCODE_POW:
	 emit_math2(c, BRW_MATH_FUNCTION_POW, dst, args[0], args[1], BRW_MATH_PRECISION_FULL); 
	 break;
      case OPCODE_RCP:
	 emit_math1(c, BRW_MATH_FUNCTION_INV, dst, args[0], BRW_MATH_PRECISION_FULL);
	 break;
      case OPCODE_RSQ:
	 emit_math1(c, BRW_MATH_FUNCTION_RSQ, dst, brw_abs(args[0]), BRW_MATH_PRECISION_FULL);
	 break;

      case OPCODE_SEQ:
         unalias2(c, dst, args[0], args[1], emit_seq);
         break;
      case OPCODE_SIN:
	 emit_math1(c, BRW_MATH_FUNCTION_SIN, dst, args[0], BRW_MATH_PRECISION_FULL);
	 break;
      case OPCODE_SNE:
         unalias2(c, dst, args[0], args[1], emit_sne);
         break;
      case OPCODE_SGE:
         unalias2(c, dst, args[0], args[1], emit_sge);
	 break;
      case OPCODE_SGT:
         unalias2(c, dst, args[0], args[1], emit_sgt);
         break;
      case OPCODE_SLT:
         unalias2(c, dst, args[0], args[1], emit_slt);
	 break;
      case OPCODE_SLE:
         unalias2(c, dst, args[0], args[1], emit_sle);
         break;
      case OPCODE_SSG:
         unalias1(c, dst, args[0], emit_sign);
         break;
      case OPCODE_SUB:
	 brw_ADD(p, dst, args[0], negate(args[1]));
	 break;
      case OPCODE_SWZ:
	 /* The args[0] value can't be used here as it won't have
	  * correctly encoded the full swizzle:
	  */
	 emit_swz(c, dst, inst);
	 break;
      case OPCODE_TRUNC:
         /* round toward zero */
	 brw_RNDZ(p, dst, args[0]);
	 break;
      case OPCODE_XPD:
	 emit_xpd(p, dst, args[0], args[1]);
	 break;
      case OPCODE_IF: {
	 struct brw_instruction *if_inst = brw_IF(p, BRW_EXECUTE_8);
	 /* Note that brw_IF smashes the predicate_control field. */
	 if_inst->header.predicate_control = get_predicate(inst);
	 if_depth_in_loop[loop_depth]++;
	 break;
      }
      case OPCODE_ELSE:
	 clear_current_const(c);
	 brw_ELSE(p);
	 break;
      case OPCODE_ENDIF:
	 clear_current_const(c);
	 brw_ENDIF(p);
	 if_depth_in_loop[loop_depth]--;
	 break;			
      case OPCODE_BGNLOOP:
	 clear_current_const(c);
         loop_inst[loop_depth++] = brw_DO(p, BRW_EXECUTE_8);
	 if_depth_in_loop[loop_depth] = 0;
         break;
      case OPCODE_BRK:
	 brw_set_predicate_control(p, get_predicate(inst));
	 brw_BREAK(p, if_depth_in_loop[loop_depth]);
	 brw_set_predicate_control(p, BRW_PREDICATE_NONE);
         break;
      case OPCODE_CONT:
	 brw_set_predicate_control(p, get_predicate(inst));
	 if (intel->gen >= 6) {
	    gen6_CONT(p, loop_inst[loop_depth - 1]);
	 } else {
	    brw_CONT(p, if_depth_in_loop[loop_depth]);
	 }
         brw_set_predicate_control(p, BRW_PREDICATE_NONE);
         break;

      case OPCODE_ENDLOOP: {
	 clear_current_const(c);
	 struct brw_instruction *inst0, *inst1;
	 GLuint br = 1;

	 loop_depth--;

	 if (intel->gen == 5)
	    br = 2;

	 inst0 = inst1 = brw_WHILE(p, loop_inst[loop_depth]);

	 if (intel->gen < 6) {
	    /* patch all the BREAK/CONT instructions from last BEGINLOOP */
	    while (inst0 > loop_inst[loop_depth]) {
	       inst0--;
	       if (inst0->header.opcode == BRW_OPCODE_BREAK &&
		   inst0->bits3.if_else.jump_count == 0) {
		  inst0->bits3.if_else.jump_count = br * (inst1 - inst0 + 1);
	       } else if (inst0->header.opcode == BRW_OPCODE_CONTINUE &&
			  inst0->bits3.if_else.jump_count == 0) {
		  inst0->bits3.if_else.jump_count = br * (inst1 - inst0);
	       }
	    }
	 }
      }
         break;

      case OPCODE_BRA:
	 brw_set_predicate_control(p, get_predicate(inst));
         brw_ADD(p, brw_ip_reg(), brw_ip_reg(), brw_imm_d(1*16));
	 brw_set_predicate_control(p, BRW_PREDICATE_NONE);
         break;
      case OPCODE_CAL:
	 brw_set_access_mode(p, BRW_ALIGN_1);
	 brw_ADD(p, deref_1d(stack_index, 0), brw_ip_reg(), brw_imm_d(3*16));
	 brw_set_access_mode(p, BRW_ALIGN_16);
	 brw_ADD(p, get_addr_reg(stack_index),
			 get_addr_reg(stack_index), brw_imm_d(4));
         brw_save_call(p, inst->Comment, p->nr_insn);
	 brw_ADD(p, brw_ip_reg(), brw_ip_reg(), brw_imm_d(1*16));
         break;
      case OPCODE_RET:
	 brw_ADD(p, get_addr_reg(stack_index),
			 get_addr_reg(stack_index), brw_imm_d(-4));
	 brw_set_access_mode(p, BRW_ALIGN_1);
         brw_MOV(p, brw_ip_reg(), deref_1d(stack_index, 0));
	 brw_set_access_mode(p, BRW_ALIGN_16);
	 break;
      case OPCODE_END:
	 emit_vertex_write(c);
         break;
      case OPCODE_PRINT:
         /* no-op */
         break;
      case OPCODE_BGNSUB:
         brw_save_label(p, inst->Comment, p->nr_insn);
         break;
      case OPCODE_ENDSUB:
         /* no-op */
         break;
      default:
	 _mesa_problem(NULL, "Unsupported opcode %i (%s) in vertex shader",
                       inst->Opcode, inst->Opcode < MAX_OPCODE ?
				    _mesa_opcode_string(inst->Opcode) :
				    "unknown");
      }

      /* Set the predication update on the last instruction of the native
       * instruction sequence.
       *
       * This would be problematic if it was set on a math instruction,
       * but that shouldn't be the case with the current GLSL compiler.
       */
      if (inst->CondUpdate) {
	 struct brw_instruction *hw_insn = &p->store[p->nr_insn - 1];

	 assert(hw_insn->header.destreg__conditionalmod == 0);
	 hw_insn->header.destreg__conditionalmod = BRW_CONDITIONAL_NZ;
      }

      if ((inst->DstReg.File == PROGRAM_OUTPUT)
          && (inst->DstReg.Index != VERT_RESULT_HPOS)
          && c->output_regs[inst->DstReg.Index].used_in_src) {
         brw_MOV(p, get_dst(c, inst->DstReg), dst);
      }

      /* Result color clamping.
       *
       * When destination register is an output register and
       * it's primary/secondary front/back color, we have to clamp
       * the result to [0,1]. This is done by enabling the
       * saturation bit for the last instruction.
       *
       * We don't use brw_set_saturate() as it modifies
       * p->current->header.saturate, which affects all the subsequent
       * instructions. Instead, we directly modify the header
       * of the last (already stored) instruction.
       */
      if (inst->DstReg.File == PROGRAM_OUTPUT &&
	  c->key.clamp_vertex_color) {
         if ((inst->DstReg.Index == VERT_RESULT_COL0)
             || (inst->DstReg.Index == VERT_RESULT_COL1)
             || (inst->DstReg.Index == VERT_RESULT_BFC0)
             || (inst->DstReg.Index == VERT_RESULT_BFC1)) {
            p->store[p->nr_insn-1].header.saturate = 1;
         }
      }

      if (inst->DstReg.RelAddr) {
	 assert(inst->DstReg.File == PROGRAM_TEMPORARY||
		inst->DstReg.File == PROGRAM_OUTPUT);
	 move_to_reladdr_dst(c, inst, dst);
      }

      release_tmps(c);
   }

   brw_resolve_cals(p);
   brw_set_uip_jip(p);

   brw_optimize(p);

   if (unlikely(INTEL_DEBUG & DEBUG_VS)) {
      int i;

      printf("vs-native:\n");
      for (i = 0; i < p->nr_insn; i++)
	 brw_disasm(stdout, &p->store[i], intel->gen);
      printf("\n");
   }
}
@


1.7
log
@Upate to libGL 7.11.2

Tested by jsg@@, matthieu@@ and ajacoutot@@, ok mattieu@@
@
text
@@


1.6
log
@Merge Mesa 7.10.3
@
text
@d435 10
a444 1
   if (intel->gen >= 6) {
d449 3
d454 3
a1050 1
   struct brw_instruction *if_insn;
d1066 1
a1066 1
   if_insn = brw_IF(p, BRW_EXECUTE_8);
d1081 1
a1081 2

   brw_ENDIF(p, if_insn);
d1563 20
d1638 1
a1638 1
	    brw_DP4(p, brw_writemask(m, (1 << (i & 7))),pos, c->userplane[i]);
d1764 5
a1768 6
   msg_len = c->nr_outputs + 2 + len_vertex_header; 
   if (intel->gen >= 6) {
	   /* interleaved urb write message length for gen6 should be multiple of 2 */
	   if ((msg_len % 2) != 0)
		msg_len++;
   }
d1776 1
a1776 1
		 MIN2(msg_len - 1, (BRW_MAX_MRF - 1)), /* msg len */
d1803 1
a1803 1
                    mrf,            /* msg len */
d1824 3
d1884 20
d1914 2
a1915 2
   GLuint insn, if_depth = 0, loop_depth = 0;
   struct brw_instruction *if_inst[MAX_IF_DEPTH], *loop_inst[MAX_LOOP_DEPTH] = { 0 };
d1963 2
d1986 16
a2001 3
	      if (file == PROGRAM_OUTPUT && c->output_regs[index].used_in_src)
		  args[i] = c->output_regs[index].reg;
	      else
d2012 5
a2016 1
	  dst = c->output_regs[index].reg;
d2154 2
a2155 3
      case OPCODE_IF:
	 assert(if_depth < MAX_IF_DEPTH);
	 if_inst[if_depth] = brw_IF(p, BRW_EXECUTE_8);
d2157 1
a2157 1
	 if_inst[if_depth]->header.predicate_control = get_predicate(inst);
a2158 1
	 if_depth++;
d2160 1
d2163 1
a2163 2
	 assert(if_depth > 0);
	 if_inst[if_depth-1] = brw_ELSE(p, if_inst[if_depth-1]);
d2167 1
a2167 2
         assert(if_depth > 0);
	 brw_ENDIF(p, if_inst[--if_depth]);
d2183 1
a2183 1
	    brw_CONT_gen6(p, loop_inst[loop_depth - 1]);
d2289 2
a2290 1
      if (inst->DstReg.File == PROGRAM_OUTPUT) {
@


1.5
log
@Update to Mesa 7.8.1. Tested on a bulk ports build by naddy@@, ok oga@@.
@
text
@d34 3
a36 3
#include "shader/program.h"
#include "shader/prog_parameter.h"
#include "shader/prog_print.h"
d40 39
d101 33
d143 1
a143 1
   GLuint i, reg = 0, mrf;
d145 5
d172 13
a184 3
      for (i = 0; i < c->key.nr_userclip; i++) {
	 c->userplane[i] = stride( brw_vec4_grf(reg+3+i/2, (i%2) * 4), 0, 4, 1);
      }     
a185 3
      /* Deal with curbe alignment:
       */
      reg += ((6 + c->key.nr_userclip + 3) / 4) * 2;
d188 17
a204 1
   /* Vertex program parameters from curbe:
d206 26
a231 3
   if (c->vp->use_const_buffer) {
      int max_constant = BRW_MAX_GRF - 20 - c->vp->program.Base.NumTemporaries;
      int constant = 0;
d233 4
a236 22
      /* We've got more constants than we can load with the push
       * mechanism.  This is often correlated with reladdr loads where
       * we should probably be using a pull mechanism anyway to avoid
       * excessive reading.  However, the pull mechanism is slow in
       * general.  So, we try to allocate as many non-reladdr-loaded
       * constants through the push buffer as we can before giving up.
       */
      memset(c->constant_map, -1, c->vp->program.Base.Parameters->NumParameters);
      for (i = 0;
	   i < c->vp->program.Base.NumInstructions && constant < max_constant;
	   i++) {
	 struct prog_instruction *inst = &c->vp->program.Base.Instructions[i];
	 int arg;

	 for (arg = 0; arg < 3 && constant < max_constant; arg++) {
	    if ((inst->SrcReg[arg].File != PROGRAM_STATE_VAR &&
		 inst->SrcReg[arg].File != PROGRAM_CONSTANT &&
		 inst->SrcReg[arg].File != PROGRAM_UNIFORM &&
		 inst->SrcReg[arg].File != PROGRAM_ENV_PARAM &&
		 inst->SrcReg[arg].File != PROGRAM_LOCAL_PARAM) ||
		inst->SrcReg[arg].RelAddr)
	       continue;
d238 2
a239 3
	    if (c->constant_map[inst->SrcReg[arg].Index] == -1) {
	       c->constant_map[inst->SrcReg[arg].Index] = constant++;
	    }
d242 1
d244 7
a250 18
      for (i = 0; i < constant; i++) {
         c->regs[PROGRAM_STATE_VAR][i] = stride( brw_vec4_grf(reg+i/2,
							      (i%2) * 4),
						 0, 4, 1);
      }
      reg += (constant + 1) / 2;
      c->prog_data.curb_read_length = reg - 1;
      /* XXX 0 causes a bug elsewhere... */
      c->prog_data.nr_params = MAX2(constant * 4, 4);
   }
   else {
      /* use a section of the GRF for constants */
      GLuint nr_params = c->vp->program.Base.Parameters->NumParameters;
      for (i = 0; i < nr_params; i++) {
         c->regs[PROGRAM_STATE_VAR][i] = stride( brw_vec4_grf(reg+i/2, (i%2) * 4), 0, 4, 1);
      }
      reg += (nr_params + 1) / 2;
      c->prog_data.curb_read_length = reg - 1;
d252 11
a262 2
      c->prog_data.nr_params = nr_params * 4;
   }
d286 5
a290 3
   if (intel->gen >= 6)
      mrf = 6;
   else if (intel->gen == 5)
d295 31
a325 1
   for (i = 0; i < VERT_RESULT_MAX; i++) {
a335 1
	    mrf++;		/* just a placeholder?  XXX fix later stages & remove this */
d338 15
a352 1
            if (mrf < 16) {
d357 1
a357 2
               /* too many vertex results to fit in MRF, use GRF for overflow */
               if (!c->first_overflow_output)
d361 1
a391 1
         c->current_const[i].index = -1;
d395 1
d432 10
a441 3
   if (intel->gen >= 6)
      c->prog_data.urb_entry_size = (attributes_in_vue + 4 + 7) / 8;
   else if (intel->gen == 5)
d448 1
a448 1
   if (INTEL_DEBUG & DEBUG_VS) {
d606 17
d628 12
a639 3
   brw_CMP(p, brw_null_reg(), BRW_CONDITIONAL_L, arg0, arg1);
   brw_SEL(p, dst, arg1, arg0);
   brw_set_predicate_control(p, BRW_PREDICATE_NONE);
d647 12
a658 3
   brw_CMP(p, brw_null_reg(), BRW_CONDITIONAL_L, arg0, arg1);
   brw_SEL(p, dst, arg0, arg1);
   brw_set_predicate_control(p, BRW_PREDICATE_NONE);
d661 5
d667 15
a681 5
static void emit_math1( struct brw_vs_compile *c,
			GLuint function,
			struct brw_reg dst,
			struct brw_reg arg0,
			GLuint precision)
a690 1
   struct intel_context *intel = &p->brw->intel;
d692 5
a696 3
   GLboolean need_tmp = (intel->gen < 6 &&
			 (dst.dw1.bits.writemask != 0xf ||
			  dst.file != BRW_GENERAL_REGISTER_FILE));
d698 1
a698 1
   if (need_tmp) 
d716 45
d762 7
a768 1
static void emit_math2( struct brw_vs_compile *c, 
a775 1
   struct intel_context *intel = &p->brw->intel;
d777 5
a781 3
   GLboolean need_tmp = (intel->gen < 6 &&
			 (dst.dw1.bits.writemask != 0xf ||
			  dst.file != BRW_GENERAL_REGISTER_FILE));
d803 47
a1122 2
      struct brw_reg addrReg = c->regs[PROGRAM_ADDRESS][0];

a1132 3
                       0,                             /* oword */
                       0,                             /* relative indexing? */
                       addrReg,                       /* address register */
d1139 1
a1139 1
   const_reg = stride(const_reg, 0, 4, 0);
d1152 2
d1155 2
a1156 2
   struct brw_reg const2_reg;
   struct brw_reg addrReg = c->regs[PROGRAM_ADDRESS][0];
d1168 9
d1178 5
a1182 30
   brw_dp_READ_4_vs(p,
		    const_reg,                     /* writeback dest */
		    0,                             /* oword */
		    1,                             /* relative indexing? */
		    addrReg,                       /* address register */
		    16 * src->Index,               /* byte offset */
		    SURF_INDEX_VERT_CONST_BUFFER   /* binding table index */
		    );
   /* second vec4 */
   const2_reg = get_tmp(c);

   /* use upper half of address reg for second read */
   addrReg = stride(addrReg, 0, 4, 0);
   addrReg.subnr = 16;

   brw_dp_READ_4_vs(p,
		    const2_reg,              /* writeback dest */
		    1,                       /* oword */
		    1,                       /* relative indexing? */
		    addrReg,                 /* address register */
		    16 * src->Index,         /* byte offset */
		    SURF_INDEX_VERT_CONST_BUFFER
		    );

   /* merge the two Owords into the constant register */
   /* const_reg[7..4] = const2_reg[7..4] */
   brw_MOV(p,
	   suboffset(stride(const_reg, 0, 4, 1), 4),
	   suboffset(stride(const2_reg, 0, 4, 1), 4));
   release_tmp(c, const2_reg);
d1228 2
a1229 1
			     GLint offset)
d1232 1
a1232 1
   struct brw_reg tmp = vec4(get_tmp(c));
d1234 2
a1235 2
   struct brw_reg vp_address = retype(vec1(addr_reg), BRW_REGISTER_TYPE_UW);
   GLuint byte_offset = arg.nr * 32 + arg.subnr + offset * 16;
d1237 6
d1248 6
a1253 5
      /* This is pretty clunky - load the address register twice and
       * fetch each 4-dword value in turn.  There must be a way to do
       * this in a single pass, but I couldn't get it to work.
       */
      brw_ADD(p, brw_address_reg(0), vp_address, brw_imm_d(byte_offset));
a1255 3
      brw_ADD(p, brw_address_reg(0), suboffset(vp_address, 8), brw_imm_d(byte_offset));
      brw_MOV(p, suboffset(tmp, 4), indirect);

d1258 1
a1258 1
   
d1260 1
a1260 1
   return vec8(tmp);
d1263 34
d1311 49
d1365 1
a1365 1
         return deref(c, c->regs[file][0], index);
d1377 8
a1384 5
      if (c->vp->use_const_buffer) {
	 if (!relAddr && c->constant_map[index] != -1) {
	    assert(c->regs[PROGRAM_STATE_VAR][c->constant_map[index]].nr != 0);
	    return c->regs[PROGRAM_STATE_VAR][c->constant_map[index]];
	 } else if (relAddr)
a1388 7
      else if (relAddr) {
         return deref(c, c->regs[PROGRAM_STATE_VAR][0], index);
      }
      else {
         assert(c->regs[PROGRAM_STATE_VAR][index].nr != 0);
         return c->regs[PROGRAM_STATE_VAR][index];
      }
a1403 20

static void emit_arl( struct brw_vs_compile *c,
		      struct brw_reg dst,
		      struct brw_reg arg0 )
{
   struct brw_compile *p = &c->func;
   struct brw_reg tmp = dst;
   GLboolean need_tmp = (dst.file != BRW_GENERAL_REGISTER_FILE);
   
   if (need_tmp) 
      tmp = get_tmp(c);

   brw_RNDD(p, tmp, arg0);               /* tmp = round(arg0) */
   brw_MUL(p, dst, tmp, brw_imm_d(16));  /* dst = tmp * 16 */

   if (need_tmp)
      release_tmp(c, tmp);
}


d1424 5
a1428 4
   reg.dw1.bits.swizzle = BRW_SWIZZLE4(GET_SWZ(src->Swizzle, 0),
				       GET_SWZ(src->Swizzle, 1),
				       GET_SWZ(src->Swizzle, 2),
				       GET_SWZ(src->Swizzle, 3));
d1430 3
a1432 3
   /* Note this is ok for non-swizzle instructions: 
    */
   reg.negate = src->Negate ? 1 : 0;   
d1449 11
a1459 2
      assert(c->regs[dst.File][dst.Index].nr != 0);
      reg = c->regs[dst.File][dst.Index];
d1474 1
a1558 1
   struct brw_reg m0 = brw_message_reg(0);
d1563 2
d1584 27
a1610 3
   if ((c->prog_data.outputs_written & BITFIELD64_BIT(VERT_RESULT_PSIZ)) ||
       c->key.nr_userclip || brw->has_negative_rhw_bug)
   {
d1620 4
a1623 2
	 brw_MUL(p, brw_writemask(header1, WRITEMASK_W), brw_swizzle1(psiz, 0), brw_imm_f(1<<11));
	 brw_AND(p, brw_writemask(header1, WRITEMASK_W), header1, brw_imm_ud(0x7ff<<8));
d1668 1
d1670 1
d1672 1
a1672 1
      /* There are 16 DWs (D0-D15) in VUE header on Sandybridge:
d1675 4
a1678 2
       * dword 8-15 (m3,m4) of the vertex header is the user clip distance.
       * m5 is the first vertex data we fill, which is the vertex position.
d1680 4
a1683 3
      brw_MOV(p, offset(m0, 2), pos);
      brw_MOV(p, offset(m0, 5), pos);
      len_vertex_header = 4;
d1693 3
a1695 3
      brw_MOV(p, offset(m0, 2), ndc);
      brw_MOV(p, offset(m0, 3), pos);
      brw_MOV(p, offset(m0, 7), pos);
d1705 2
a1706 2
      brw_MOV(p, offset(m0, 2), ndc);
      brw_MOV(p, offset(m0, 3), pos);
d1710 19
d1731 7
d1744 1
a1744 1
		 MIN2(c->nr_outputs + 1 + len_vertex_header, (BRW_MAX_MRF-1)), /* msg len */
d1756 1
a1756 4
      /* XXX I'm not 100% sure about which MRF regs to use here.  Starting
       * at mrf[4] atm...
       */
      GLuint i, mrf = 0;
d1760 1
a1760 1
            brw_MOV(p, brw_message_reg(4+mrf), c->regs[PROGRAM_OUTPUT][i]);
d1767 1
a1767 1
                    4,              /* starting mrf reg nr */
d1771 1
a1771 1
                    mrf+1,          /* msg len */
d1775 1
a1775 1
                    BRW_MAX_MRF-1,  /* urb destination offset */
a1779 25

/**
 * Called after code generation to resolve subroutine calls and the
 * END instruction.
 * \param end_inst  points to brw code for END instruction
 * \param last_inst  points to last instruction emitted before vertex write
 */
static void 
post_vs_emit( struct brw_vs_compile *c,
              struct brw_instruction *end_inst,
              struct brw_instruction *last_inst )
{
   GLint offset;

   brw_resolve_cals(&c->func);

   /* patch up the END code to jump past subroutines, etc */
   offset = last_inst - end_inst;
   if (offset > 1) {
      brw_set_src1(end_inst, brw_imm_d(offset * 16));
   } else {
      end_inst->header.opcode = BRW_OPCODE_NOP;
   }
}

a1859 2
   GLuint end_offset = 0;
   struct brw_instruction *end_inst, *last_inst;
d1861 1
d1866 1
a1866 1
   if (INTEL_DEBUG & DEBUG_VS) {
d1868 2
a1869 1
      _mesa_print_program(&c->vp->program.Base); 
d1875 3
d1916 1
a1916 1
      
d1953 1
d1962 3
d1990 1
a1990 1
	 emit_arl(c, dst, args[0]);
d2037 1
a2037 1
	 emit_math1(c, BRW_MATH_FUNCTION_RSQ, dst, args[0], BRW_MATH_PRECISION_FULL);
d2061 3
d2085 1
d2089 2
d2094 1
d2097 1
d2100 1
d2102 1
d2106 1
a2106 1
         brw_BREAK(p);
d2111 5
a2115 1
         brw_CONT(p);
d2118 18
a2135 15
      case OPCODE_ENDLOOP: 
         {
            struct brw_instruction *inst0, *inst1;
	    GLuint br = 1;

            loop_depth--;

	    if (intel->gen == 5)
	       br = 2;

            inst0 = inst1 = brw_WHILE(p, loop_inst[loop_depth]);
            /* patch all the BREAK/CONT instructions from last BEGINLOOP */
            while (inst0 > loop_inst[loop_depth]) {
               inst0--;
               if (inst0->header.opcode == BRW_OPCODE_BREAK &&
d2137 8
a2144 10
                  inst0->bits3.if_else.jump_count = br * (inst1 - inst0 + 1);
                  inst0->bits3.if_else.pop_count = 0;
               }
               else if (inst0->header.opcode == BRW_OPCODE_CONTINUE &&
			inst0->bits3.if_else.jump_count == 0) {
                  inst0->bits3.if_else.jump_count = br * (inst1 - inst0);
                  inst0->bits3.if_else.pop_count = 0;
               }
            }
         }
d2146 1
d2168 2
a2169 6
      case OPCODE_END:	
         end_offset = p->nr_insn;
         /* this instruction will get patched later to jump past subroutine
          * code, etc.
          */
         brw_ADD(p, brw_ip_reg(), brw_ip_reg(), brw_imm_d(1*16));
d2227 6
d2236 2
a2237 5
   end_inst = &p->store[end_offset];
   last_inst = &p->store[p->nr_insn];

   /* The END instruction will be patched to jump to this code */
   emit_vertex_write(c);
d2239 1
a2239 1
   post_vs_emit(c, end_inst, last_inst);
d2241 1
a2241 1
   if (INTEL_DEBUG & DEBUG_VS) {
d2246 1
a2246 1
	 brw_disasm(stderr, &p->store[i]);
@


1.4
log
@Update to Mesa 7.4.2. Tested by oga@@, ckuethe@@ and naddy@@.
@
text
@d41 21
d63 3
a65 1
/* Do things as simply as possible.  Allocate and populate all regs
d70 1
d72 14
a85 1
   GLuint nr_params;
d89 2
a90 1
   c->r0 = brw_vec8_grf(reg, 0); reg++;
d101 1
a101 1
      reg += ((6+c->key.nr_userclip+3)/4)*2;
d106 51
a156 5
   nr_params = c->vp->program.Base.Parameters->NumParameters;
   for (i = 0; i < nr_params; i++) {
      c->regs[PROGRAM_STATE_VAR][i] = stride( brw_vec4_grf(reg+i/2, (i%2) * 4), 0, 4, 1);
   }     
   reg += (nr_params+1)/2;
d158 2
a159 1
   c->prog_data.curb_read_length = reg - 1;
d165 1
a165 1
      if (c->prog_data.inputs_read & (1<<i)) {
d171 5
d177 1
a177 2
   /* Allocate outputs: TODO: could organize the non-position outputs
    * to go straight into message regs.
d181 9
a189 1
   mrf = 4;
d191 1
a191 1
      if (c->prog_data.outputs_written & (1<<i)) {
d193 1
d204 11
a214 2
	    c->regs[PROGRAM_OUTPUT][i] = brw_message_reg(mrf);
	    mrf++;
d242 8
d251 9
a259 4
       if (c->output_regs[i].used_in_src) {
            c->output_regs[i].reg = brw_vec8_grf(reg, 0);
            reg++;
        }
a261 4
   c->stack =  brw_uw16_reg(BRW_GENERAL_REGISTER_FILE, reg, 0);
   reg += 2;
 
   
d271 7
a277 1
   c->prog_data.urb_read_length = (c->nr_inputs+1)/2;
d279 4
a282 3
   c->prog_data.urb_entry_size = (c->nr_outputs+2+3)/4;
   c->prog_data.total_grf = reg;
}
d284 6
d291 1
a291 3
static struct brw_reg get_tmp( struct brw_vs_compile *c )
{
   struct brw_reg tmp = brw_vec8_grf(c->last_tmp, 0);
d293 5
a297 15
   if (++c->last_tmp > c->prog_data.total_grf)
      c->prog_data.total_grf = c->last_tmp;

   return tmp;
}

static void release_tmp( struct brw_vs_compile *c, struct brw_reg tmp )
{
   if (tmp.nr == c->last_tmp-1)
      c->last_tmp--;
}
			       
static void release_tmps( struct brw_vs_compile *c )
{
   c->last_tmp = c->first_tmp;
d301 4
d324 4
d350 4
d379 1
a379 1
static void emit_sop( struct brw_compile *p,
d385 2
d393 1
a393 1
static void emit_seq( struct brw_compile *p,
d398 1
a398 1
   emit_sop(p, dst, arg0, arg1, BRW_CONDITIONAL_EQ);
d401 1
a401 1
static void emit_sne( struct brw_compile *p,
d406 1
a406 1
   emit_sop(p, dst, arg0, arg1, BRW_CONDITIONAL_NEQ);
d408 1
a408 1
static void emit_slt( struct brw_compile *p, 
d413 1
a413 1
   emit_sop(p, dst, arg0, arg1, BRW_CONDITIONAL_L);
d416 1
a416 1
static void emit_sle( struct brw_compile *p, 
d421 1
a421 1
   emit_sop(p, dst, arg0, arg1, BRW_CONDITIONAL_LE);
d424 1
a424 1
static void emit_sgt( struct brw_compile *p, 
d429 1
a429 1
   emit_sop(p, dst, arg0, arg1, BRW_CONDITIONAL_G);
d432 1
a432 1
static void emit_sge( struct brw_compile *p, 
d437 12
a448 1
  emit_sop(p, dst, arg0, arg1, BRW_CONDITIONAL_GE);
d486 1
d488 3
a490 2
   GLboolean need_tmp = (dst.dw1.bits.writemask != 0xf ||
			 dst.file != BRW_GENERAL_REGISTER_FILE);
d519 1
d521 3
a523 2
   GLboolean need_tmp = (dst.dw1.bits.writemask != 0xf ||
			 dst.file != BRW_GENERAL_REGISTER_FILE);
d764 2
d806 97
d906 1
a906 1
			       GLuint file,
a908 1

d937 3
d946 2
a947 1
   struct brw_reg vp_address = retype(vec1(get_reg(c, PROGRAM_ADDRESS, 0)), BRW_REGISTER_TYPE_UW);
d968 1
d973 62
d1046 2
a1047 2
   brw_RNDD(p, tmp, arg0);
   brw_MUL(p, dst, tmp, brw_imm_d(16));
d1054 3
a1056 1
/* Will return mangled results for SWZ op.  The emit_swz() function
d1061 2
a1062 1
			       struct prog_src_register *src )
d1064 1
d1070 1
a1070 4
   if (src->RelAddr) 
      reg = deref(c, c->regs[PROGRAM_STATE_VAR][0], src->Index);
   else
      reg = get_reg(c, src->File, src->Index);
d1081 1
a1081 1
   reg.negate = src->NegateBase ? 1 : 0;   
d1087 3
d1093 20
a1112 1
   struct brw_reg reg = get_reg(c, dst.File, dst.Index);
d1122 1
a1122 1
		      struct prog_src_register src )
d1124 2
d1131 1
a1131 1
   GLboolean need_tmp = (src.NegateBase &&
d1165 1
a1165 4
      if (src.RelAddr) 
	 arg0 = deref(c, c->regs[PROGRAM_STATE_VAR][0], src.Index);
      else
	 arg0 = get_reg(c, src.File, src.Index);
d1180 2
a1181 2
   if (src.NegateBase)
      brw_MOV(p, brw_writemask(tmp, src.NegateBase), negate(tmp));
d1196 2
d1201 2
d1210 8
a1217 4
   /* Build ndc coords */
   ndc = get_tmp(c);
   emit_math1(c, BRW_MATH_FUNCTION_INV, ndc, brw_swizzle1(pos, 3), BRW_MATH_PRECISION_FULL);
   brw_MUL(p, brw_writemask(ndc, WRITEMASK_XYZ), pos, ndc);
d1222 2
a1223 2
   if ((c->prog_data.outputs_written & (1<<VERT_RESULT_PSIZ)) ||
       c->key.nr_userclip || !BRW_IS_G4X(p->brw))
d1232 1
a1232 1
      if (c->prog_data.outputs_written & (1<<VERT_RESULT_PSIZ)) {
d1254 1
a1254 1
      if (!BRW_IS_G4X(p->brw)) {
d1280 38
a1317 2
   brw_MOV(p, offset(m0, 2), ndc);
   brw_MOV(p, offset(m0, 3), pos);
d1325 1
a1325 1
		 c->nr_outputs + 3, /* msg len */
d1327 2
a1328 2
		 1, 		/* eot */
		 1, 		/* writes complete */
d1331 31
d1382 5
a1386 1
   brw_set_src1(end_inst, brw_imm_d(offset * 16));
d1389 68
d1458 1
a1458 1
/* Emit the fragment program instructions here.
d1462 2
a1463 1
#define MAX_IFSN 32
d1465 4
a1468 2
   GLuint nr_insns = c->vp->program.Base.NumInstructions;
   GLuint insn, if_insn = 0;
d1471 2
a1472 3
   struct brw_instruction *if_inst[MAX_IFSN];
   struct brw_indirect stack_index = brw_indirect(0, 0);   

d1477 1
a1477 1
      _mesa_printf("vs-emit:\n");
d1479 1
a1479 1
      _mesa_printf("\n");
d1484 1
a1484 3
   
   /* Message registers can't be read, so copy the output into GRF register
      if they are used in source registers */
d1488 4
d1499 9
d1513 3
a1515 1
   brw_MOV(p, get_addr_reg(stack_index), brw_address(c->stack));
d1519 1
a1519 1
      struct prog_instruction *inst = &c->vp->program.Base.Instructions[insn];
d1523 5
d1532 1
a1532 1
	      struct prog_src_register *src = &inst->SrcReg[i];
d1535 1
a1535 1
	      if (file == PROGRAM_OUTPUT&&c->output_regs[index].used_in_src)
d1538 1
a1538 1
		  args[i] = get_arg(c, src);
d1613 2
a1614 1
	 brw_MOV(p, brw_acc_reg(), args[2]);
d1617 3
d1643 1
a1643 1
         emit_seq(p, dst, args[0], args[1]);
d1649 1
a1649 1
         emit_sne(p, dst, args[0], args[1]);
d1652 1
a1652 1
	 emit_sge(p, dst, args[0], args[1]);
d1655 1
a1655 1
         emit_sgt(p, dst, args[0], args[1]);
d1658 1
a1658 1
	 emit_slt(p, dst, args[0], args[1]);
d1661 1
a1661 1
         emit_sle(p, dst, args[0], args[1]);
d1670 1
a1670 1
	 emit_swz(c, dst, inst->SrcReg[0] );
d1680 5
a1684 2
	 assert(if_insn < MAX_IFSN);
         if_inst[if_insn++] = brw_IF(p, BRW_EXECUTE_8);
d1687 1
a1687 1
	 if_inst[if_insn-1] = brw_ELSE(p, if_inst[if_insn-1]);
d1690 2
a1691 2
         assert(if_insn > 0);
	 brw_ENDIF(p, if_inst[--if_insn]);
d1693 40
d1734 1
a1734 1
         brw_set_predicate_control(p, BRW_PREDICATE_NORMAL);
d1736 1
a1736 1
         brw_set_predicate_control_flag_value(p, 0xff);
d1777 13
d1827 9
@


1.3
log
@Mesa 7.2, Tested by ckuethe@@, naddy@@, oga@@, and others.
@
text
@a75 2


d85 1
a85 2
   }     

d196 1
d218 1
d225 25
d366 1
a397 1
		     
d447 1
a447 1
		 BRW_MATH_PRECISION_PARTIAL);
a547 2
      

d569 1
a579 1

d620 30
d651 2
d654 2
a692 1

a785 2


d857 2
a858 2

/* Post-vertex-program processing.  Send the results to the URB.
d873 4
d878 2
a879 13
   /* Build ndc coords?   TODO: Shortcircuit when w is known to be one.
    */
   if (!c->key.know_w_is_one) {
      ndc = get_tmp(c);
      emit_math1(c, BRW_MATH_FUNCTION_INV, ndc, brw_swizzle1(pos, 3), BRW_MATH_PRECISION_FULL);
      brw_MUL(p, brw_writemask(ndc, WRITEMASK_XYZ), pos, ndc);
   }
   else {
      ndc = pos;
   }

   /* This includes the workaround for -ve rhw, so is no longer an
    * optional step:
d882 1
a882 2
       c->key.nr_userclip ||
       !c->key.know_w_is_one)
a896 1

a903 1

d913 1
a913 1
      if (!(BRW_IS_GM45(p->brw) || BRW_IS_G4X(p->brw)) && !c->key.know_w_is_one) {
a934 1

a940 1
   
d954 1
a955 1
}
d957 6
d964 3
a966 1
post_vs_emit( struct brw_vs_compile *c, struct brw_instruction *end_inst )
d968 7
a974 25
   GLuint nr_insns = c->vp->program.Base.NumInstructions;
   GLuint insn, target_insn;
   struct prog_instruction *inst1, *inst2;
   struct brw_instruction *brw_inst1, *brw_inst2;
   int offset;
   for (insn = 0; insn < nr_insns; insn++) {
       inst1 = &c->vp->program.Base.Instructions[insn];
       brw_inst1 = inst1->Data;
       switch (inst1->Opcode) {
	   case OPCODE_CAL:
	   case OPCODE_BRA:
	       target_insn = inst1->BranchTarget;
	       inst2 = &c->vp->program.Base.Instructions[target_insn];
	       brw_inst2 = inst2->Data;
	       offset = brw_inst2 - brw_inst1;
	       brw_set_src1(brw_inst1, brw_imm_d(offset*16));
	       break;
	   case OPCODE_END:
	       offset = end_inst - brw_inst1;
	       brw_set_src1(brw_inst1, brw_imm_d(offset*16));
	       break;
	   default:
	       break;
       }
   }
d977 1
d986 2
a987 1
   struct brw_instruction *end_inst;
d995 1
a995 1
      _mesa_printf("\n\n\nvs-emit:\n");
a1029 1
      inst->Data = &p->store[p->nr_insn];
d1052 5
d1064 3
d1076 6
d1109 3
d1141 3
d1152 1
a1152 1
        break;
d1168 4
d1190 1
a1190 1
        break;
d1197 1
a1197 1
	 inst->Data = &p->store[p->nr_insn];
d1199 1
a1199 1
        break;
d1206 1
d1208 4
d1213 1
a1213 1
        break;
d1215 2
d1218 2
d1221 2
a1222 1
	 break;
d1224 4
a1227 2
	 _mesa_printf("Unsupport opcode %d in vertex shader\n", inst->Opcode);
	 break;
d1260 4
a1263 1
   end_inst = &p->store[p->nr_insn];
d1265 2
a1266 3
   post_vs_emit(c, end_inst);
   for (insn = 0; insn < nr_insns; insn++)
       c->vp->program.Base.Instructions[insn].Data = NULL;
@


1.2
log
@Update to Mesa 7.0.3. tested my oga@@ and johan@@
@
text
@d33 2
a34 2
#include "program.h"
#include "macros.h"
d137 10
d214 1
a214 1
   if ((dst.file == arg0.file && dst.nr == arg0.nr) &&
d226 11
d238 7
d246 7
a252 1

d258 2
a259 8
   /* Could be done with an if/else/endif, but this method uses half
    * the instructions.  Note that we are careful to reference the
    * arguments before writing the dest.  That means we emit the
    * instructions in an odd order and have to play with the flag
    * values.
    */
   brw_push_insn_state(p);
   brw_CMP(p, brw_null_reg(), BRW_CONDITIONAL_GE, arg0, arg1);
d261 7
a267 4
   /* Write all values to 1:
    */
   brw_set_predicate_control(p, BRW_PREDICATE_NONE);
   brw_MOV(p, dst, brw_imm_f(1.0));
d269 6
a274 5
   /* Where the test succeeded, overwite with zero:
    */
   brw_set_predicate_control(p, BRW_PREDICATE_NORMAL);
   brw_MOV(p, dst, brw_imm_f(0.0));
   brw_pop_insn_state(p);
a276 1

d282 1
a282 13
   brw_push_insn_state(p);
   brw_CMP(p, brw_null_reg(), BRW_CONDITIONAL_GE, arg0, arg1);

   /* Write all values to zero:
    */
   brw_set_predicate_control(p, BRW_PREDICATE_NONE);
   brw_MOV(p, dst, brw_imm_f(0));

   /* Where the test succeeded, overwite with 1:
    */
   brw_set_predicate_control(p, BRW_PREDICATE_NORMAL);
   brw_MOV(p, dst, brw_imm_f(1.0));
   brw_pop_insn_state(p);
a284 1

a612 1
   case PROGRAM_STATE_VAR:
d615 5
d693 1
a693 1
			       struct prog_src_register src )
d697 1
a697 1
   if (src.File == PROGRAM_UNDEFINED)
d700 2
a701 2
   if (src.RelAddr) 
      reg = deref(c, c->regs[PROGRAM_STATE_VAR][0], src.Index);
d703 1
a703 1
      reg = get_reg(c, src.File, src.Index);
d707 4
a710 4
   reg.dw1.bits.swizzle = BRW_SWIZZLE4(GET_SWZ(src.Swizzle, 0),
				       GET_SWZ(src.Swizzle, 1),
				       GET_SWZ(src.Swizzle, 2),
				       GET_SWZ(src.Swizzle, 3));
d714 1
a714 1
   reg.negate = src.NegateBase ? 1 : 0;   
d870 1
a870 1
      if (!BRW_IS_IGD(p->brw) && !c->key.know_w_is_one) {
d916 29
a944 2


d948 1
a948 1
void brw_vs_emit( struct brw_vs_compile *c )
d950 1
d953 4
a956 1
   GLuint insn;
d958 2
d970 14
d987 1
d997 1
d999 9
a1007 2
	 for (i = 0; i < 3; i++) 
	    args[i] = get_arg(c, inst->SrcReg[i]);
d1012 7
a1018 2
       */
      dst = get_dst(c, inst->DstReg);
a1019 1
      
d1088 7
d1098 3
d1104 3
d1119 31
d1151 2
d1154 2
d1158 1
d1162 27
d1192 1
d1194 3
a1196 1

a1197 5





@


1.1
log
@Initial revision
@
text
@a32 1
#include "brw_context.h"
a33 1
#include "program_instruction.h"
d35 3
d81 1
a81 1
   for (i = 0; i < BRW_ATTRIB_MAX; i++) {
d795 1
a795 1
	      get_reg(c, PROGRAM_INPUT, BRW_ATTRIB_EDGEFLAG));
d801 8
a808 3
   ndc = get_tmp(c);
   emit_math1(c, BRW_MATH_FUNCTION_INV, ndc, brw_swizzle1(pos, 3), BRW_MATH_PRECISION_FULL);
   brw_MUL(p, brw_writemask(ndc, WRITEMASK_XYZ), pos, ndc);
d813 3
d848 6
a853 5
      brw_CMP(p,
	      vec8(brw_null_reg()),
	      BRW_CONDITIONAL_L,
	      brw_swizzle1(ndc, 3),
	      brw_imm_f(0));
d855 4
a858 8
      brw_OR(p, brw_writemask(header1, WRITEMASK_W), header1, brw_imm_ud(1<<6));
      brw_MOV(p, ndc, brw_imm_f(0));
      brw_set_predicate_control(p, BRW_PREDICATE_NONE);





d865 3
@


1.1.1.1
log
@Import MesaLibs 6.5.1. (in dist/ since its code is shared between lib 
and xserver)...
@
text
@@


1.1.1.2
log
@import MesaLibs version 6.5.2
@
text
@d81 1
a81 1
      if (c->prog_data.inputs_read & ((GLuint64EXT)1<<i)) {
d800 3
a802 8
   if (!c->key.know_w_is_one) {
      ndc = get_tmp(c);
      emit_math1(c, BRW_MATH_FUNCTION_INV, ndc, brw_swizzle1(pos, 3), BRW_MATH_PRECISION_FULL);
      brw_MUL(p, brw_writemask(ndc, WRITEMASK_XYZ), pos, ndc);
   }
   else {
      ndc = pos;
   }
a806 3
   if ((c->prog_data.outputs_written & (1<<VERT_RESULT_PSIZ)) ||
       c->key.nr_userclip ||
       !c->key.know_w_is_one)
d839 5
a843 6
      if (!c->key.know_w_is_one) {
	 brw_CMP(p,
		 vec8(brw_null_reg()),
		 BRW_CONDITIONAL_L,
		 brw_swizzle1(ndc, 3),
		 brw_imm_f(0));
d845 8
a852 4
	 brw_OR(p, brw_writemask(header1, WRITEMASK_W), header1, brw_imm_ud(1<<6));
	 brw_MOV(p, ndc, brw_imm_f(0));
	 brw_set_predicate_control(p, BRW_PREDICATE_NONE);
      }
a858 3
   }
   else {
      brw_MOV(p, retype(brw_message_reg(1), BRW_REGISTER_TYPE_UD), brw_imm_ud(0));
@


1.1.1.3
log
@Mesa 7.0.1
@
text
@d33 1
d35 1
a36 3
#include "shader/prog_parameter.h"
#include "shader/prog_print.h"
#include "brw_context.h"
d80 2
a81 2
   for (i = 0; i < VERT_ATTRIB_MAX; i++) {
      if (c->prog_data.inputs_read & (1<<i)) {
d794 1
a794 1
	      get_reg(c, PROGRAM_INPUT, VERT_ATTRIB_EDGEFLAG));
@


1.1.1.4
log
@Import Mesa 7.10.3
@
text
@d33 4
a36 4
#include "main/macros.h"
#include "program/program.h"
#include "program/prog_parameter.h"
#include "program/prog_print.h"
a39 49
/* Return the SrcReg index of the channels that can be immediate float operands
 * instead of usage of PROGRAM_CONSTANT values through push/pull.
 */
static GLboolean
brw_vs_arg_can_be_immediate(enum prog_opcode opcode, int arg)
{
   int opcode_array[] = {
      [OPCODE_MOV] = 1,
      [OPCODE_ADD] = 2,
      [OPCODE_CMP] = 3,
      [OPCODE_DP2] = 2,
      [OPCODE_DP3] = 2,
      [OPCODE_DP4] = 2,
      [OPCODE_DPH] = 2,
      [OPCODE_MAX] = 2,
      [OPCODE_MIN] = 2,
      [OPCODE_MUL] = 2,
      [OPCODE_SEQ] = 2,
      [OPCODE_SGE] = 2,
      [OPCODE_SGT] = 2,
      [OPCODE_SLE] = 2,
      [OPCODE_SLT] = 2,
      [OPCODE_SNE] = 2,
      [OPCODE_XPD] = 2,
   };

   /* These opcodes get broken down in a way that allow two
    * args to be immediates.
    */
   if (opcode == OPCODE_MAD || opcode == OPCODE_LRP) {
      if (arg == 1 || arg == 2)
	 return GL_TRUE;
   }

   if (opcode > ARRAY_SIZE(opcode_array))
      return GL_FALSE;

   return arg == opcode_array[opcode] - 1;
}

static struct brw_reg get_tmp( struct brw_vs_compile *c )
{
   struct brw_reg tmp = brw_vec8_grf(c->last_tmp, 0);

   if (++c->last_tmp > c->prog_data.total_grf)
      c->prog_data.total_grf = c->last_tmp;

   return tmp;
}
a40 10
static void release_tmp( struct brw_vs_compile *c, struct brw_reg tmp )
{
   if (tmp.nr == c->last_tmp-1)
      c->last_tmp--;
}
			       
static void release_tmps( struct brw_vs_compile *c )
{
   c->last_tmp = c->first_tmp;
}
d42 1
a42 37
static int
get_first_reladdr_output(struct gl_vertex_program *vp)
{
   int i;
   int first_reladdr_output = VERT_RESULT_MAX;

   for (i = 0; i < vp->Base.NumInstructions; i++) {
      struct prog_instruction *inst = vp->Base.Instructions + i;

      if (inst->DstReg.File == PROGRAM_OUTPUT &&
	  inst->DstReg.RelAddr &&
	  inst->DstReg.Index < first_reladdr_output)
	 first_reladdr_output = inst->DstReg.Index;
   }

   return first_reladdr_output;
}

/* Clears the record of which vp_const_buffer elements have been
 * loaded into our constant buffer registers, for the starts of new
 * blocks after control flow.
 */
static void
clear_current_const(struct brw_vs_compile *c)
{
   unsigned int i;

   if (c->vp->use_const_buffer) {
      for (i = 0; i < 3; i++) {
         c->current_const[i].index = -1;
      }
   }
}

/**
 * Preallocate GRF register before code emit.
 * Do things as simply as possible.  Allocate and populate all regs
d47 2
a48 21
   struct intel_context *intel = &c->func.brw->intel;
   GLuint i, reg = 0, mrf, j;
   int attributes_in_vue;
   int first_reladdr_output;
   int max_constant;
   int constant = 0;
   int vert_result_reoder[VERT_RESULT_MAX];
   int bfc = 0;

   /* Determine whether to use a real constant buffer or use a block
    * of GRF registers for constants.  The later is faster but only
    * works if everything fits in the GRF.
    * XXX this heuristic/check may need some fine tuning...
    */
   if (c->vp->program.Base.Parameters->NumParameters +
       c->vp->program.Base.NumTemporaries + 20 > BRW_MAX_GRF)
      c->vp->use_const_buffer = GL_TRUE;
   else
      c->vp->use_const_buffer = GL_FALSE;

   /*printf("use_const_buffer = %d\n", c->vp->use_const_buffer);*/
d52 1
a52 2
   c->r0 = brw_vec8_grf(reg, 0);
   reg++;
d57 3
a59 13
      if (intel->gen >= 6) {
	 for (i = 0; i < c->key.nr_userclip; i++) {
	    c->userplane[i] = stride(brw_vec4_grf(reg + i / 2,
						  (i % 2) * 4), 0, 4, 1);
	 }
	 reg += ALIGN(c->key.nr_userclip, 2) / 2;
      } else {
	 for (i = 0; i < c->key.nr_userclip; i++) {
	    c->userplane[i] = stride(brw_vec4_grf(reg + (6 + i) / 2,
						  (i % 2) * 4), 0, 4, 1);
	 }
	 reg += (ALIGN(6 + c->key.nr_userclip, 4) / 4) * 2;
      }
d61 3
d66 7
a72 44
   /* Assign some (probably all) of the vertex program constants to
    * the push constant buffer/CURBE.
    *
    * There's an obvious limit to the numer of push constants equal to
    * the number of register available, and that number is smaller
    * than the minimum maximum number of vertex program parameters, so
    * support for pull constants is required if we overflow.
    * Additionally, on gen6 the number of push constants is even
    * lower.
    *
    * When there's relative addressing, we don't know what range of
    * Mesa IR registers can be accessed.  And generally, when relative
    * addressing is used we also have too many constants to load them
    * all as push constants.  So, we'll just support relative
    * addressing out of the pull constant buffers, and try to load as
    * many statically-accessed constants into the push constant buffer
    * as we can.
    */
   if (intel->gen >= 6) {
      /* We can only load 32 regs of push constants. */
      max_constant = 32 * 2 - c->key.nr_userclip;
   } else {
      max_constant = BRW_MAX_GRF - 20 - c->vp->program.Base.NumTemporaries;
   }

   /* constant_map maps from ParameterValues[] index to index in the
    * push constant buffer, or -1 if it's only in the pull constant
    * buffer.
    */
   memset(c->constant_map, -1, c->vp->program.Base.Parameters->NumParameters);
   for (i = 0;
	i < c->vp->program.Base.NumInstructions && constant < max_constant;
	i++) {
      struct prog_instruction *inst = &c->vp->program.Base.Instructions[i];
      int arg;

      for (arg = 0; arg < 3 && constant < max_constant; arg++) {
	 if (inst->SrcReg[arg].File != PROGRAM_STATE_VAR &&
	     inst->SrcReg[arg].File != PROGRAM_CONSTANT &&
	     inst->SrcReg[arg].File != PROGRAM_UNIFORM &&
	     inst->SrcReg[arg].File != PROGRAM_ENV_PARAM &&
	     inst->SrcReg[arg].File != PROGRAM_LOCAL_PARAM) {
	    continue;
	 }
d74 1
a74 4
	 if (inst->SrcReg[arg].RelAddr) {
	    c->vp->use_const_buffer = GL_TRUE;
	    continue;
	 }
a75 5
	 if (c->constant_map[inst->SrcReg[arg].Index] == -1) {
	    c->constant_map[inst->SrcReg[arg].Index] = constant++;
	 }
      }
   }
a76 19
   /* If we ran out of push constant space, then we'll also upload all
    * constants through the pull constant buffer so that they can be
    * accessed no matter what.  For relative addressing (the common
    * case) we need them all in place anyway.
    */
   if (constant == max_constant)
      c->vp->use_const_buffer = GL_TRUE;

   for (i = 0; i < constant; i++) {
      c->regs[PROGRAM_STATE_VAR][i] = stride(brw_vec4_grf(reg + i / 2,
							  (i % 2) * 4),
					     0, 4, 1);
   }
   reg += (constant + 1) / 2;
   c->prog_data.curb_read_length = reg - 1;
   c->prog_data.nr_params = constant * 4;
   /* XXX 0 causes a bug elsewhere... */
   if (intel->gen < 6 && c->prog_data.nr_params == 0)
      c->prog_data.nr_params = 4;
d82 1
a82 1
      if (c->prog_data.inputs_read & (1 << i)) {
d87 2
a88 6
   }
   /* If there are no inputs, we'll still be reading one attribute's worth
    * because it's required -- see urb_read_length setting.
    */
   if (c->nr_inputs == 0)
      reg++;
d90 2
a91 1
   /* Allocate outputs.  The non-position outputs go straight into message regs.
d95 3
a97 43
   c->first_overflow_output = 0;

   if (intel->gen >= 6) {
      mrf = 3;
      if (c->key.nr_userclip)
	 mrf += 2;
   } else if (intel->gen == 5)
      mrf = 8;
   else
      mrf = 4;

   first_reladdr_output = get_first_reladdr_output(&c->vp->program);

   for (i = 0; i < VERT_RESULT_MAX; i++)
       vert_result_reoder[i] = i;

   /* adjust attribute order in VUE for BFC0/BFC1 on Gen6+ */
   if (intel->gen >= 6 && c->key.two_side_color) {
       if ((c->prog_data.outputs_written & BITFIELD64_BIT(VERT_RESULT_COL1)) &&
           (c->prog_data.outputs_written & BITFIELD64_BIT(VERT_RESULT_BFC1))) {
           assert(c->prog_data.outputs_written & BITFIELD64_BIT(VERT_RESULT_COL0));
           assert(c->prog_data.outputs_written & BITFIELD64_BIT(VERT_RESULT_BFC0));
           bfc = 2;
       } else if ((c->prog_data.outputs_written & BITFIELD64_BIT(VERT_RESULT_COL0)) &&
           (c->prog_data.outputs_written & BITFIELD64_BIT(VERT_RESULT_BFC0)))
           bfc = 1;

       if (bfc) {
           for (i = 0; i < bfc; i++) {
               vert_result_reoder[VERT_RESULT_COL0 + i * 2 + 0] = VERT_RESULT_COL0 + i;
               vert_result_reoder[VERT_RESULT_COL0 + i * 2 + 1] = VERT_RESULT_BFC0 + i;
           }

           for (i = VERT_RESULT_COL0 + bfc * 2; i < VERT_RESULT_BFC0 + bfc; i++) {
               vert_result_reoder[i] = i - bfc;
           }
       }
   }

   for (j = 0; j < VERT_RESULT_MAX; j++) {
      i = vert_result_reoder[j];

      if (c->prog_data.outputs_written & BITFIELD64_BIT(i)) {
a98 1
         assert(i < Elements(c->regs[PROGRAM_OUTPUT]));
d106 1
d109 2
a110 25
	    /* Two restrictions on our compute-to-MRF here.  The
	     * message length for all SEND messages is restricted to
	     * [1,15], so we can't use mrf 15, as that means a length
	     * of 16.
	     *
	     * Additionally, URB writes are aligned to URB rows, so we
	     * need to put an even number of registers of URB data in
	     * each URB write so that the later write is aligned.  A
	     * message length of 15 means 1 message header reg plus 14
	     * regs of URB data.
	     *
	     * For attributes beyond the compute-to-MRF, we compute to
	     * GRFs and they will be written in the second URB_WRITE.
	     */
            if (first_reladdr_output > i && mrf < 15) {
               c->regs[PROGRAM_OUTPUT][i] = brw_message_reg(mrf);
               mrf++;
            }
            else {
               if (mrf >= 15 && !c->first_overflow_output)
                  c->first_overflow_output = i;
               c->regs[PROGRAM_OUTPUT][i] = brw_vec8_grf(reg, 0);
               reg++;
	       mrf++;
            }
d137 2
a138 21

   if (c->vp->use_const_buffer) {
      for (i = 0; i < 3; i++) {
         c->current_const[i].reg = brw_vec8_grf(reg, 0);
         reg++;
      }
      clear_current_const(c);
   }

   for (i = 0; i < 128; i++) {
      if (c->output_regs[i].used_in_src) {
         c->output_regs[i].reg = brw_vec8_grf(reg, 0);
         reg++;
      }
   }

   if (c->needs_stack) {
      c->stack =  brw_uw16_reg(BRW_GENERAL_REGISTER_FILE, reg, 0);
      reg += 2;
   }

d148 1
a148 26
   c->prog_data.urb_read_length = (c->nr_inputs + 1) / 2;
   /* Setting this field to 0 leads to undefined behavior according to the
    * the VS_STATE docs.  Our VUEs will always have at least one attribute
    * sitting in them, even if it's padding.
    */
   if (c->prog_data.urb_read_length == 0)
      c->prog_data.urb_read_length = 1;

   /* The VS VUEs are shared by VF (outputting our inputs) and VS, so size
    * them to fit the biggest thing they need to.
    */
   attributes_in_vue = MAX2(c->nr_outputs, c->nr_inputs);

   /* See emit_vertex_write() for where the VUE's overhead on top of the
    * attributes comes from.
    */
   if (intel->gen >= 6) {
      int header_regs = 2;
      if (c->key.nr_userclip)
	 header_regs += 2;

      c->prog_data.urb_entry_size = (attributes_in_vue + header_regs + 7) / 8;
   } else if (intel->gen == 5)
      c->prog_data.urb_entry_size = (attributes_in_vue + 6 + 3) / 4;
   else
      c->prog_data.urb_entry_size = (attributes_in_vue + 2 + 3) / 4;
d150 1
d152 12
d165 9
a173 5
   if (unlikely(INTEL_DEBUG & DEBUG_VS)) {
      printf("%s NumAddrRegs %d\n", __FUNCTION__, c->vp->program.Base.NumAddressRegs);
      printf("%s NumTemps %d\n", __FUNCTION__, c->vp->program.Base.NumTemporaries);
      printf("%s reg = %d\n", __FUNCTION__, reg);
   }
a176 4
/**
 * If an instruction uses a temp reg both as a src and the dest, we
 * sometimes need to allocate an intermediate temporary.
 */
a188 1
      release_tmp(c, tmp);
a194 4
/**
 * \sa unalias2
 * Checkes if 2-operand instruction needs an intermediate temporary.
 */
d204 1
a204 1
   if ((dst.file == arg0.file && dst.nr == arg0.nr) ||
a209 1
      release_tmp(c, tmp);
a215 36
/**
 * \sa unalias2
 * Checkes if 3-operand instruction needs an intermediate temporary.
 */
static void unalias3( struct brw_vs_compile *c,
		      struct brw_reg dst,
		      struct brw_reg arg0,
		      struct brw_reg arg1,
		      struct brw_reg arg2,
		      void (*func)( struct brw_vs_compile *,
				    struct brw_reg,
				    struct brw_reg,
				    struct brw_reg,
				    struct brw_reg ))
{
   if ((dst.file == arg0.file && dst.nr == arg0.nr) ||
       (dst.file == arg1.file && dst.nr == arg1.nr) ||
       (dst.file == arg2.file && dst.nr == arg2.nr)) {
      struct brw_compile *p = &c->func;
      struct brw_reg tmp = brw_writemask(get_tmp(c), dst.dw1.bits.writemask);
      func(c, tmp, arg0, arg1, arg2);
      brw_MOV(p, dst, tmp);
      release_tmp(c, tmp);
   }
   else {
      func(c, dst, arg0, arg1, arg2);
   }
}

static void emit_sop( struct brw_vs_compile *c,
                      struct brw_reg dst,
                      struct brw_reg arg0,
                      struct brw_reg arg1, 
		      GLuint cond)
{
   struct brw_compile *p = &c->func;
a216 5
   brw_MOV(p, dst, brw_imm_f(0.0f));
   brw_CMP(p, brw_null_reg(), cond, arg0, arg1);
   brw_MOV(p, dst, brw_imm_f(1.0f));
   brw_set_predicate_control_flag_value(p, 0xff);
}
a217 7
static void emit_seq( struct brw_vs_compile *c,
                      struct brw_reg dst,
                      struct brw_reg arg0,
                      struct brw_reg arg1 )
{
   emit_sop(c, dst, arg0, arg1, BRW_CONDITIONAL_EQ);
}
d219 1
a219 8
static void emit_sne( struct brw_vs_compile *c,
                      struct brw_reg dst,
                      struct brw_reg arg0,
                      struct brw_reg arg1 )
{
   emit_sop(c, dst, arg0, arg1, BRW_CONDITIONAL_NEQ);
}
static void emit_slt( struct brw_vs_compile *c,
d224 13
a236 2
   emit_sop(c, dst, arg0, arg1, BRW_CONDITIONAL_L);
}
d238 5
a242 6
static void emit_sle( struct brw_vs_compile *c,
		      struct brw_reg dst,
		      struct brw_reg arg0,
		      struct brw_reg arg1 )
{
   emit_sop(c, dst, arg0, arg1, BRW_CONDITIONAL_LE);
a244 7
static void emit_sgt( struct brw_vs_compile *c,
		      struct brw_reg dst,
		      struct brw_reg arg0,
		      struct brw_reg arg1 )
{
   emit_sop(c, dst, arg0, arg1, BRW_CONDITIONAL_G);
}
d246 1
a246 1
static void emit_sge( struct brw_vs_compile *c,
d251 2
a252 2
  emit_sop(c, dst, arg0, arg1, BRW_CONDITIONAL_GE);
}
d254 2
a255 8
static void emit_cmp( struct brw_compile *p,
		      struct brw_reg dst,
		      struct brw_reg arg0,
		      struct brw_reg arg1,
		      struct brw_reg arg2 )
{
   brw_CMP(p, brw_null_reg(), BRW_CONDITIONAL_L, arg0, brw_imm_f(0));
   brw_SEL(p, dst, arg1, arg2);
a256 8
}

static void emit_sign(struct brw_vs_compile *c,
		      struct brw_reg dst,
		      struct brw_reg arg0)
{
   struct brw_compile *p = &c->func;

d259 3
a261 5
   brw_CMP(p, brw_null_reg(), BRW_CONDITIONAL_L, arg0, brw_imm_f(0));
   brw_MOV(p, dst, brw_imm_f(-1.0));
   brw_set_predicate_control(p, BRW_PREDICATE_NONE);

   brw_CMP(p, brw_null_reg(), BRW_CONDITIONAL_G, arg0, brw_imm_f(0));
d263 1
a263 1
   brw_set_predicate_control(p, BRW_PREDICATE_NONE);
d266 1
d272 3
a274 12
   struct intel_context *intel = &p->brw->intel;

   if (intel->gen >= 6) {
      brw_set_conditionalmod(p, BRW_CONDITIONAL_GE);
      brw_SEL(p, dst, arg0, arg1);
      brw_set_conditionalmod(p, BRW_CONDITIONAL_NONE);
      brw_set_predicate_control(p, BRW_PREDICATE_NONE);
   } else {
      brw_CMP(p, brw_null_reg(), BRW_CONDITIONAL_GE, arg0, arg1);
      brw_SEL(p, dst, arg0, arg1);
      brw_set_predicate_control(p, BRW_PREDICATE_NONE);
   }
d282 3
a284 12
   struct intel_context *intel = &p->brw->intel;

   if (intel->gen >= 6) {
      brw_set_conditionalmod(p, BRW_CONDITIONAL_L);
      brw_SEL(p, dst, arg0, arg1);
      brw_set_conditionalmod(p, BRW_CONDITIONAL_NONE);
      brw_set_predicate_control(p, BRW_PREDICATE_NONE);
   } else {
      brw_CMP(p, brw_null_reg(), BRW_CONDITIONAL_L, arg0, arg1);
      brw_SEL(p, dst, arg0, arg1);
      brw_set_predicate_control(p, BRW_PREDICATE_NONE);
   }
a286 5
static void emit_arl(struct brw_compile *p,
		     struct brw_reg dst,
		     struct brw_reg src)
{
   struct intel_context *intel = &p->brw->intel;
d288 5
a292 15
   if (intel->gen >= 6) {
      struct brw_reg dst_f = retype(dst, BRW_REGISTER_TYPE_F);

      brw_RNDD(p, dst_f, src);
      brw_MOV(p, dst, dst_f);
   } else {
      brw_RNDD(p, dst, src);
   }
}

static void emit_math1_gen4(struct brw_vs_compile *c,
			    GLuint function,
			    struct brw_reg dst,
			    struct brw_reg arg0,
			    GLuint precision)
d303 2
a304 1
   GLboolean need_tmp = GL_FALSE;
d306 1
a306 5
   if (dst.file != BRW_GENERAL_REGISTER_FILE ||
       dst.dw1.bits.writemask != 0xf)
      need_tmp = GL_TRUE;

   if (need_tmp)
d324 1
a324 53
static void
emit_math1_gen6(struct brw_vs_compile *c,
		GLuint function,
		struct brw_reg dst,
		struct brw_reg arg0,
		GLuint precision)
{
   struct brw_compile *p = &c->func;
   struct brw_reg tmp_src, tmp_dst;

   /* Something is strange on gen6 math in 16-wide mode, though the
    * docs say it's supposed to work.  Punt to using align1 mode,
    * which doesn't do writemasking and swizzles.
    */
   tmp_src = get_tmp(c);
   tmp_dst = get_tmp(c);

   brw_MOV(p, tmp_src, arg0);

   brw_set_access_mode(p, BRW_ALIGN_1);
   brw_math(p,
	    tmp_dst,
	    function,
	    BRW_MATH_SATURATE_NONE,
	    2,
	    tmp_src,
	    BRW_MATH_DATA_SCALAR,
	    precision);
   brw_set_access_mode(p, BRW_ALIGN_16);

   brw_MOV(p, dst, tmp_dst);

   release_tmp(c, tmp_src);
   release_tmp(c, tmp_dst);
}

static void
emit_math1(struct brw_vs_compile *c,
	   GLuint function,
	   struct brw_reg dst,
	   struct brw_reg arg0,
	   GLuint precision)
{
   struct brw_compile *p = &c->func;
   struct intel_context *intel = &p->brw->intel;

   if (intel->gen >= 6)
      emit_math1_gen6(c, function, dst, arg0, precision);
   else
      emit_math1_gen4(c, function, dst, arg0, precision);
}

static void emit_math2_gen4( struct brw_vs_compile *c, 
d333 2
a334 5
   GLboolean need_tmp = GL_FALSE;

   if (dst.file != BRW_GENERAL_REGISTER_FILE ||
       dst.dw1.bits.writemask != 0xf)
      need_tmp = GL_TRUE;
d355 1
a356 47
static void emit_math2_gen6( struct brw_vs_compile *c, 
			GLuint function,
			struct brw_reg dst,
			struct brw_reg arg0,
			struct brw_reg arg1,
			GLuint precision)
{
   struct brw_compile *p = &c->func;
   struct brw_reg tmp_src0, tmp_src1, tmp_dst;

   tmp_src0 = get_tmp(c);
   tmp_src1 = get_tmp(c);
   tmp_dst = get_tmp(c);

   brw_MOV(p, tmp_src0, arg0);
   brw_MOV(p, tmp_src1, arg1);
   
   brw_set_access_mode(p, BRW_ALIGN_1);
   brw_math2(p,
	    tmp_dst,
	    function,
	    tmp_src0,
	    tmp_src1);
   brw_set_access_mode(p, BRW_ALIGN_16);

   brw_MOV(p, dst, tmp_dst);

   release_tmp(c, tmp_src0);
   release_tmp(c, tmp_src1);
   release_tmp(c, tmp_dst);
}

static void emit_math2( struct brw_vs_compile *c, 
			GLuint function,
			struct brw_reg dst,
			struct brw_reg arg0,
			struct brw_reg arg1,
			GLuint precision)
{
   struct brw_compile *p = &c->func;
   struct intel_context *intel = &p->brw->intel;

   if (intel->gen >= 6)
      emit_math2_gen6(c, function, dst, arg0, arg1, precision);
   else
      emit_math2_gen4(c, function, dst, arg0, arg1, precision);
}
d405 1
a405 1
		 BRW_MATH_PRECISION_FULL);
d506 2
a528 1

d539 1
a577 2

   release_tmp(c, tmp);
a579 7
static void emit_lrp_noalias(struct brw_vs_compile *c,
			     struct brw_reg dst,
			     struct brw_reg arg0,
			     struct brw_reg arg1,
			     struct brw_reg arg2)
{
   struct brw_compile *p = &c->func;
a580 105
   brw_ADD(p, dst, negate(arg0), brw_imm_f(1.0));
   brw_MUL(p, brw_null_reg(), dst, arg2);
   brw_MAC(p, dst, arg0, arg1);
}

/** 3 or 4-component vector normalization */
static void emit_nrm( struct brw_vs_compile *c, 
                      struct brw_reg dst,
                      struct brw_reg arg0,
                      int num_comps)
{
   struct brw_compile *p = &c->func;
   struct brw_reg tmp = get_tmp(c);

   /* tmp = dot(arg0, arg0) */
   if (num_comps == 3)
      brw_DP3(p, tmp, arg0, arg0);
   else
      brw_DP4(p, tmp, arg0, arg0);

   /* tmp = 1 / sqrt(tmp) */
   emit_math1(c, BRW_MATH_FUNCTION_RSQ, tmp, tmp, BRW_MATH_PRECISION_FULL);

   /* dst = arg0 * tmp */
   brw_MUL(p, dst, arg0, tmp);

   release_tmp(c, tmp);
}


static struct brw_reg
get_constant(struct brw_vs_compile *c,
             const struct prog_instruction *inst,
             GLuint argIndex)
{
   const struct prog_src_register *src = &inst->SrcReg[argIndex];
   struct brw_compile *p = &c->func;
   struct brw_reg const_reg = c->current_const[argIndex].reg;

   assert(argIndex < 3);

   if (c->current_const[argIndex].index != src->Index) {
      /* Keep track of the last constant loaded in this slot, for reuse. */
      c->current_const[argIndex].index = src->Index;

#if 0
      printf("  fetch const[%d] for arg %d into reg %d\n",
             src->Index, argIndex, c->current_const[argIndex].reg.nr);
#endif
      /* need to fetch the constant now */
      brw_dp_READ_4_vs(p,
                       const_reg,                     /* writeback dest */
                       16 * src->Index,               /* byte offset */
                       SURF_INDEX_VERT_CONST_BUFFER   /* binding table index */
                       );
   }

   /* replicate lower four floats into upper half (to get XYZWXYZW) */
   const_reg = stride(const_reg, 0, 4, 1);
   const_reg.subnr = 0;

   return const_reg;
}

static struct brw_reg
get_reladdr_constant(struct brw_vs_compile *c,
		     const struct prog_instruction *inst,
		     GLuint argIndex)
{
   const struct prog_src_register *src = &inst->SrcReg[argIndex];
   struct brw_compile *p = &c->func;
   struct brw_context *brw = p->brw;
   struct intel_context *intel = &brw->intel;
   struct brw_reg const_reg = c->current_const[argIndex].reg;
   struct brw_reg addr_reg = c->regs[PROGRAM_ADDRESS][0];
   uint32_t offset;

   assert(argIndex < 3);

   /* Can't reuse a reladdr constant load. */
   c->current_const[argIndex].index = -1;

 #if 0
   printf("  fetch const[a0.x+%d] for arg %d into reg %d\n",
	  src->Index, argIndex, c->current_const[argIndex].reg.nr);
#endif

   if (intel->gen >= 6) {
      offset = src->Index;
   } else {
      struct brw_reg byte_addr_reg = retype(get_tmp(c), BRW_REGISTER_TYPE_D);
      brw_MUL(p, byte_addr_reg, addr_reg, brw_imm_d(16));
      addr_reg = byte_addr_reg;
      offset = 16 * src->Index;
   }

   /* fetch the first vec4 */
   brw_dp_READ_4_vs_relative(p,
			     const_reg,
			     addr_reg,
			     offset,
			     SURF_INDEX_VERT_CONST_BUFFER);

   return const_reg;
}
d587 1
a587 1
			       gl_register_file file,
d590 1
d595 1
a597 5
   case PROGRAM_STATE_VAR:
   case PROGRAM_CONSTANT:
   case PROGRAM_UNIFORM:
      assert(c->regs[PROGRAM_STATE_VAR][index].nr != 0);
      return c->regs[PROGRAM_STATE_VAR][index];
d615 1
a615 3
/**
 * Indirect addressing:  get reg[[arg] + offset].
 */
d618 1
a618 2
			     GLint offset,
			     GLuint reg_size )
d621 3
a623 4
   struct brw_reg tmp = get_tmp(c);
   struct brw_reg addr_reg = c->regs[PROGRAM_ADDRESS][0];
   struct brw_reg vp_address = retype(vec1(addr_reg), BRW_REGISTER_TYPE_D);
   GLuint byte_offset = arg.nr * 32 + arg.subnr + offset * reg_size;
a624 6
   struct brw_reg acc = retype(vec1(get_tmp(c)), BRW_REGISTER_TYPE_UW);

   /* Set the vertical stride on the register access so that the first
    * 4 components come from a0.0 and the second 4 from a0.1.
    */
   indirect.vstride = BRW_VERTICAL_STRIDE_ONE_DIMENSIONAL;
d630 6
a635 2
      brw_MUL(p, acc, vp_address, brw_imm_uw(reg_size));
      brw_ADD(p, brw_address_reg(0), acc, brw_imm_uw(byte_offset));
d637 2
a638 4
      brw_MUL(p, acc, suboffset(vp_address, 4), brw_imm_uw(reg_size));
      brw_ADD(p, brw_address_reg(1), acc, brw_imm_uw(byte_offset));

      brw_MOV(p, tmp, indirect);
d642 3
a645 3
   /* NOTE: tmp not released */
   return tmp;
}
d647 3
a649 4
static void
move_to_reladdr_dst(struct brw_vs_compile *c,
		    const struct prog_instruction *inst,
		    struct brw_reg val)
d652 5
a656 7
   int reg_size = 32;
   struct brw_reg addr_reg = c->regs[PROGRAM_ADDRESS][0];
   struct brw_reg vp_address = retype(vec1(addr_reg), BRW_REGISTER_TYPE_D);
   struct brw_reg base = c->regs[inst->DstReg.File][inst->DstReg.Index];
   GLuint byte_offset = base.nr * 32 + base.subnr;
   struct brw_reg indirect = brw_vec4_indirect(0,0);
   struct brw_reg acc = retype(vec1(get_tmp(c)), BRW_REGISTER_TYPE_UW);
d658 2
a659 5
   /* Because destination register indirect addressing can only use
    * one index, we'll write each vertex's vec4 value separately.
    */
   val.width = BRW_WIDTH_4;
   val.vstride = BRW_VERTICAL_STRIDE_4;
d661 2
a662 13
   brw_push_insn_state(p);
   brw_set_access_mode(p, BRW_ALIGN_1);

   brw_MUL(p, acc, vp_address, brw_imm_uw(reg_size));
   brw_ADD(p, brw_address_reg(0), acc, brw_imm_uw(byte_offset));
   brw_MOV(p, indirect, val);

   brw_MUL(p, acc, suboffset(vp_address, 4), brw_imm_uw(reg_size));
   brw_ADD(p, brw_address_reg(0), acc,
	   brw_imm_uw(byte_offset + reg_size / 2));
   brw_MOV(p, indirect, suboffset(val, 4));

   brw_pop_insn_state(p);
a664 61
/**
 * Get brw reg corresponding to the instruction's [argIndex] src reg.
 * TODO: relative addressing!
 */
static struct brw_reg
get_src_reg( struct brw_vs_compile *c,
             const struct prog_instruction *inst,
             GLuint argIndex )
{
   const GLuint file = inst->SrcReg[argIndex].File;
   const GLint index = inst->SrcReg[argIndex].Index;
   const GLboolean relAddr = inst->SrcReg[argIndex].RelAddr;

   if (brw_vs_arg_can_be_immediate(inst->Opcode, argIndex)) {
      const struct prog_src_register *src = &inst->SrcReg[argIndex];

      if (src->Swizzle == MAKE_SWIZZLE4(SWIZZLE_ZERO,
					SWIZZLE_ZERO,
					SWIZZLE_ZERO,
					SWIZZLE_ZERO)) {
	  return brw_imm_f(0.0f);
      } else if (src->Swizzle == MAKE_SWIZZLE4(SWIZZLE_ONE,
					       SWIZZLE_ONE,
					       SWIZZLE_ONE,
					       SWIZZLE_ONE)) {
	 if (src->Negate)
	    return brw_imm_f(-1.0F);
	 else
	    return brw_imm_f(1.0F);
      } else if (src->File == PROGRAM_CONSTANT) {
	 const struct gl_program_parameter_list *params;
	 float f;
	 int component = -1;

	 switch (src->Swizzle) {
	 case SWIZZLE_XXXX:
	    component = 0;
	    break;
	 case SWIZZLE_YYYY:
	    component = 1;
	    break;
	 case SWIZZLE_ZZZZ:
	    component = 2;
	    break;
	 case SWIZZLE_WWWW:
	    component = 3;
	    break;
	 }

	 if (component >= 0) {
	    params = c->vp->program.Base.Parameters;
	    f = params->ParameterValues[src->Index][component];

	    if (src->Abs)
	       f = fabs(f);
	    if (src->Negate)
	       f = -f;
	    return brw_imm_f(f);
	 }
      }
   }
d666 1
a666 47
   switch (file) {
   case PROGRAM_TEMPORARY:
   case PROGRAM_INPUT:
   case PROGRAM_OUTPUT:
      if (relAddr) {
         return deref(c, c->regs[file][0], index, 32);
      }
      else {
         assert(c->regs[file][index].nr != 0);
         return c->regs[file][index];
      }

   case PROGRAM_STATE_VAR:
   case PROGRAM_CONSTANT:
   case PROGRAM_UNIFORM:
   case PROGRAM_ENV_PARAM:
   case PROGRAM_LOCAL_PARAM:
      if (!relAddr && c->constant_map[index] != -1) {
	 /* Take from the push constant buffer if possible. */
	 assert(c->regs[PROGRAM_STATE_VAR][c->constant_map[index]].nr != 0);
	 return c->regs[PROGRAM_STATE_VAR][c->constant_map[index]];
      } else {
	 /* Must be in the pull constant buffer then .*/
	 assert(c->vp->use_const_buffer);
	 if (relAddr)
	    return get_reladdr_constant(c, inst, argIndex);
	 else
	    return get_constant(c, inst, argIndex);
      }
   case PROGRAM_ADDRESS:
      assert(index == 0);
      return c->regs[file][index];

   case PROGRAM_UNDEFINED:
      /* this is a normal case since we loop over all three src args */
      return brw_null_reg();

   case PROGRAM_WRITE_ONLY:
   default:
      assert(0);
      return brw_null_reg();
   }
}

/**
 * Return the brw reg for the given instruction's src argument.
 * Will return mangled results for SWZ op.  The emit_swz() function
d671 1
a671 2
                               const struct prog_instruction *inst,
                               GLuint argIndex )
a672 1
   const struct prog_src_register *src = &inst->SrcReg[argIndex];
d675 1
a675 1
   if (src->File == PROGRAM_UNDEFINED)
d678 4
a681 1
   reg = get_src_reg(c, inst, argIndex);
d685 4
a688 5
   if (reg.file != BRW_IMMEDIATE_VALUE) {
      reg.dw1.bits.swizzle = BRW_SWIZZLE4(GET_SWZ(src->Swizzle, 0),
					  GET_SWZ(src->Swizzle, 1),
					  GET_SWZ(src->Swizzle, 2),
					  GET_SWZ(src->Swizzle, 3));
d690 3
a692 3
      /* Note this is ok for non-swizzle ARB_vp instructions */
      reg.negate = src->Negate ? 1 : 0;
   }
a697 3
/**
 * Get brw register for the given program dest register.
 */
d701 1
a701 1
   struct brw_reg reg;
a702 29
   switch (dst.File) {
   case PROGRAM_TEMPORARY:
   case PROGRAM_OUTPUT:
      /* register-indirect addressing is only 1x1, not VxH, for
       * destination regs.  So, for RelAddr we'll return a temporary
       * for the dest and do a move of the result to the RelAddr
       * register after the instruction emit.
       */
      if (dst.RelAddr) {
	 reg = get_tmp(c);
      } else {
	 assert(c->regs[dst.File][dst.Index].nr != 0);
	 reg = c->regs[dst.File][dst.Index];
      }
      break;
   case PROGRAM_ADDRESS:
      assert(dst.Index == 0);
      reg = c->regs[dst.File][dst.Index];
      break;
   case PROGRAM_UNDEFINED:
      /* we may hit this for OPCODE_END, OPCODE_KIL, etc */
      reg = brw_null_reg();
      break;
   default:
      assert(0);
      reg = brw_null_reg();
   }

   assert(reg.type != BRW_IMMEDIATE_VALUE);
d709 2
d713 1
a713 1
                      const struct prog_instruction *inst)
a714 2
   const GLuint argIndex = 0;
   const struct prog_src_register src = inst->SrcReg[argIndex];
d720 1
a720 1
   GLboolean need_tmp = (src.Negate &&
d754 4
a757 1
      arg0 = get_src_reg(c, inst, argIndex);
d772 2
a773 2
   if (src.Negate)
      brw_MOV(p, brw_writemask(tmp, src.Negate), negate(tmp));
d782 2
a783 2
/**
 * Post-vertex-program processing.  Send the results to the URB.
d788 1
a788 2
   struct brw_context *brw = p->brw;
   struct intel_context *intel = &brw->intel;
a790 4
   int eot;
   GLuint len_vertex_header = 2;
   int next_mrf, i;
   int msg_len;
d798 4
a801 2
   if (intel->gen < 6) {
      /* Build ndc coords */
a802 1
      /* ndc = 1.0 / pos.w */
a803 1
      /* ndc.xyz = pos * ndc */
d806 3
d810 2
a811 2
   /* Update the header for point size, user clipping flags, and -ve rhw
    * workaround.
d813 4
a816 27
   if (intel->gen >= 6) {
      struct brw_reg m1 = brw_message_reg(1);

      /* On gen6, m1 has each value in a separate dword, so we never
       * need to mess with a temporary for computing the m1 value.
       */
      brw_MOV(p, retype(m1, BRW_REGISTER_TYPE_UD), brw_imm_ud(0));
      if (c->prog_data.outputs_written & BITFIELD64_BIT(VERT_RESULT_PSIZ)) {
	 brw_MOV(p, brw_writemask(m1, WRITEMASK_W),
		 brw_swizzle1(c->regs[PROGRAM_OUTPUT][VERT_RESULT_PSIZ], 0));
      }

      /* Set the user clip distances in dword 8-15. (m3-4)*/
      if (c->key.nr_userclip) {
	 for (i = 0; i < c->key.nr_userclip; i++) {
	    struct brw_reg m;
	    if (i < 4)
	       m = brw_message_reg(3);
	    else
	       m = brw_message_reg(4);

	    brw_DP4(p, brw_writemask(m, (1 << (i & 7))),pos, c->userplane[i]);
	 }
      }
   } else if ((c->prog_data.outputs_written &
	       BITFIELD64_BIT(VERT_RESULT_PSIZ)) ||
	      c->key.nr_userclip || brw->has_negative_rhw_bug) {
d824 1
a824 1
      if (c->prog_data.outputs_written & BITFIELD64_BIT(VERT_RESULT_PSIZ)) {
d826 2
a827 4
	 brw_MUL(p, brw_writemask(header1, WRITEMASK_W),
		 brw_swizzle1(psiz, 0), brw_imm_f(1<<11));
	 brw_AND(p, brw_writemask(header1, WRITEMASK_W),
		 header1, brw_imm_ud(0x7ff<<8));
d830 1
d838 1
d848 1
a848 1
      if (brw->has_negative_rhw_bug) {
d870 1
d875 3
a877 69
   brw_set_acc_write_control(p, 0);

   /* The VUE layout is documented in Volume 2a. */
   if (intel->gen >= 6) {
      /* There are 8 or 16 DWs (D0-D15) in VUE header on Sandybridge:
       * dword 0-3 (m1) of the header is indices, point width, clip flags.
       * dword 4-7 (m2) is the 4D space position
       * dword 8-15 (m3,m4) of the vertex header is the user clip distance if
       * enabled.
       * m3 or 5 is the first vertex element data we fill, which is
       * the vertex position.
       */
      brw_MOV(p, brw_message_reg(2), pos);
      len_vertex_header = 1;
      if (c->key.nr_userclip > 0)
	 len_vertex_header += 2;
   } else if (intel->gen == 5) {
      /* There are 20 DWs (D0-D19) in VUE header on Ironlake:
       * dword 0-3 (m1) of the header is indices, point width, clip flags.
       * dword 4-7 (m2) is the ndc position (set above)
       * dword 8-11 (m3) of the vertex header is the 4D space position
       * dword 12-19 (m4,m5) of the vertex header is the user clip distance.
       * m6 is a pad so that the vertex element data is aligned
       * m7 is the first vertex data we fill, which is the vertex position.
       */
      brw_MOV(p, brw_message_reg(2), ndc);
      brw_MOV(p, brw_message_reg(3), pos);
      brw_MOV(p, brw_message_reg(7), pos);
      len_vertex_header = 6;
   } else {
      /* There are 8 dwords in VUE header pre-Ironlake:
       * dword 0-3 (m1) is indices, point width, clip flags.
       * dword 4-7 (m2) is ndc position (set above)
       *
       * dword 8-11 (m3) is the first vertex data, which we always have be the
       * vertex position.
       */
      brw_MOV(p, brw_message_reg(2), ndc);
      brw_MOV(p, brw_message_reg(3), pos);
      len_vertex_header = 2;
   }

   /* Move variable-addressed, non-overflow outputs to their MRFs. */
   next_mrf = 2 + len_vertex_header;
   for (i = 0; i < VERT_RESULT_MAX; i++) {
      if (c->first_overflow_output > 0 && i >= c->first_overflow_output)
	 break;
      if (!(c->prog_data.outputs_written & BITFIELD64_BIT(i)))
	 continue;
      if (i == VERT_RESULT_PSIZ)
	 continue;

      if (i >= VERT_RESULT_TEX0 &&
	  c->regs[PROGRAM_OUTPUT][i].file == BRW_GENERAL_REGISTER_FILE) {
	 brw_MOV(p, brw_message_reg(next_mrf), c->regs[PROGRAM_OUTPUT][i]);
	 next_mrf++;
      } else if (c->regs[PROGRAM_OUTPUT][i].file == BRW_MESSAGE_REGISTER_FILE) {
	 next_mrf = c->regs[PROGRAM_OUTPUT][i].nr + 1;
      }
   }

   eot = (c->first_overflow_output == 0);

   msg_len = c->nr_outputs + 2 + len_vertex_header; 
   if (intel->gen >= 6) {
	   /* interleaved urb write message length for gen6 should be multiple of 2 */
	   if ((msg_len % 2) != 0)
		msg_len++;
   }
d885 1
a885 1
		 MIN2(msg_len - 1, (BRW_MAX_MRF - 1)), /* msg len */
d887 2
a888 2
		 eot, 		/* eot */
		 eot, 		/* writes complete */
a891 27
   if (c->first_overflow_output > 0) {
      /* Not all of the vertex outputs/results fit into the MRF.
       * Move the overflowed attributes from the GRF to the MRF and
       * issue another brw_urb_WRITE().
       */
      GLuint i, mrf = 1;
      for (i = c->first_overflow_output; i < VERT_RESULT_MAX; i++) {
         if (c->prog_data.outputs_written & BITFIELD64_BIT(i)) {
            /* move from GRF to MRF */
            brw_MOV(p, brw_message_reg(mrf), c->regs[PROGRAM_OUTPUT][i]);
            mrf++;
         }
      }

      brw_urb_WRITE(p,
                    brw_null_reg(), /* dest */
                    0,              /* starting mrf reg nr */
                    c->r0,          /* src */
                    0,              /* allocate */
                    1,              /* used */
                    mrf,            /* msg len */
                    0,              /* response len */
                    1,              /* eot */
                    1,              /* writes complete */
                    14 / 2,  /* urb destination offset */
                    BRW_URB_SWIZZLE_INTERLEAVE);
   }
a893 8
static GLboolean
accumulator_contains(struct brw_vs_compile *c, struct brw_reg val)
{
   struct brw_compile *p = &c->func;
   struct brw_instruction *prev_insn = &p->store[p->nr_insn - 1];

   if (p->nr_insn == 0)
      return GL_FALSE;
a894 2
   if (val.address_mode != BRW_ADDRESS_DIRECT)
      return GL_FALSE;
a895 19
   switch (prev_insn->header.opcode) {
   case BRW_OPCODE_MOV:
   case BRW_OPCODE_MAC:
   case BRW_OPCODE_MUL:
      if (prev_insn->header.access_mode == BRW_ALIGN_16 &&
	  prev_insn->header.execution_size == val.width &&
	  prev_insn->bits1.da1.dest_reg_file == val.file &&
	  prev_insn->bits1.da1.dest_reg_type == val.type &&
	  prev_insn->bits1.da1.dest_address_mode == val.address_mode &&
	  prev_insn->bits1.da1.dest_reg_nr == val.nr &&
	  prev_insn->bits1.da16.dest_subreg_nr == val.subnr / 16 &&
	  prev_insn->bits1.da16.dest_writemask == 0xf)
	 return GL_TRUE;
      else
	 return GL_FALSE;
   default:
      return GL_FALSE;
   }
}
d897 3
a899 2
static uint32_t
get_predicate(const struct prog_instruction *inst)
d901 3
a903 2
   if (inst->DstReg.CondMask == COND_TR)
      return BRW_PREDICATE_NONE;
a904 30
   /* All of GLSL only produces predicates for COND_NE and one channel per
    * vector.  Fail badly if someone starts doing something else, as it might
    * mean infinite looping or something.
    *
    * We'd like to support all the condition codes, but our hardware doesn't
    * quite match the Mesa IR, which is modeled after the NV extensions.  For
    * those, the instruction may update the condition codes or not, then any
    * later instruction may use one of those condition codes.  For gen4, the
    * instruction may update the flags register based on one of the condition
    * codes output by the instruction, and then further instructions may
    * predicate on that.  We can probably support this, but it won't
    * necessarily be easy.
    */
   assert(inst->DstReg.CondMask == COND_NE);

   switch (inst->DstReg.CondSwizzle) {
   case SWIZZLE_XXXX:
      return BRW_PREDICATE_ALIGN16_REPLICATE_X;
   case SWIZZLE_YYYY:
      return BRW_PREDICATE_ALIGN16_REPLICATE_Y;
   case SWIZZLE_ZZZZ:
      return BRW_PREDICATE_ALIGN16_REPLICATE_Z;
   case SWIZZLE_WWWW:
      return BRW_PREDICATE_ALIGN16_REPLICATE_W;
   default:
      _mesa_problem(NULL, "Unexpected predicate: 0x%08x\n",
		    inst->DstReg.CondMask);
      return BRW_PREDICATE_NORMAL;
   }
}
d906 4
a909 22
/* Emit the vertex program instructions here.
 */
void brw_vs_emit(struct brw_vs_compile *c )
{
#define MAX_IF_DEPTH 32
#define MAX_LOOP_DEPTH 32
   struct brw_compile *p = &c->func;
   struct brw_context *brw = p->brw;
   struct intel_context *intel = &brw->intel;
   const GLuint nr_insns = c->vp->program.Base.NumInstructions;
   GLuint insn, if_depth = 0, loop_depth = 0;
   struct brw_instruction *if_inst[MAX_IF_DEPTH], *loop_inst[MAX_LOOP_DEPTH] = { 0 };
   int if_depth_in_loop[MAX_LOOP_DEPTH];
   const struct brw_indirect stack_index = brw_indirect(0, 0);   
   GLuint index;
   GLuint file;

   if (unlikely(INTEL_DEBUG & DEBUG_VS)) {
      printf("vs-mesa:\n");
      _mesa_fprint_program_opt(stdout, &c->vp->program.Base, PROG_PRINT_DEBUG,
			       GL_TRUE);
      printf("\n");
d914 1
a914 29
   if_depth_in_loop[loop_depth] = 0;

   brw_set_acc_write_control(p, 1);

   for (insn = 0; insn < nr_insns; insn++) {
       GLuint i;
       struct prog_instruction *inst = &c->vp->program.Base.Instructions[insn];

       /* Message registers can't be read, so copy the output into GRF
	* register if they are used in source registers
	*/
       for (i = 0; i < 3; i++) {
	   struct prog_src_register *src = &inst->SrcReg[i];
	   GLuint index = src->Index;
	   GLuint file = src->File;	
	   if (file == PROGRAM_OUTPUT && index != VERT_RESULT_HPOS)
	       c->output_regs[index].used_in_src = GL_TRUE;
       }

       switch (inst->Opcode) {
       case OPCODE_CAL:
       case OPCODE_RET:
	  c->needs_stack = GL_TRUE;
	  break;
       default:
	  break;
       }
   }

a918 3
   if (c->needs_stack)
      brw_MOV(p, get_addr_reg(stack_index), brw_address(c->stack));

d921 1
a921 1
      const struct prog_instruction *inst = &c->vp->program.Base.Instructions[insn];
d924 1
a924 6

#if 0
      printf("%d: ", insn);
      _mesa_print_instruction(inst);
#endif

d928 2
a929 9
	  for (i = 0; i < 3; i++) {
	      const struct prog_src_register *src = &inst->SrcReg[i];
	      index = src->Index;
	      file = src->File;	
	      if (file == PROGRAM_OUTPUT && c->output_regs[index].used_in_src)
		  args[i] = c->output_regs[index].reg;
	      else
                  args[i] = get_arg(c, inst, i);
	  }
d934 2
a935 12
       */ 
      index = inst->DstReg.Index;
      file = inst->DstReg.File;
      if (file == PROGRAM_OUTPUT && c->output_regs[index].used_in_src)
	  dst = c->output_regs[index].reg;
      else
	  dst = get_dst(c, inst->DstReg);

      if (inst->SaturateMode != SATURATE_OFF) {
	 _mesa_problem(NULL, "Unsupported saturate %d in vertex shader",
                       inst->SaturateMode);
      }
d937 1
a939 1
	 args[0].negate = false;
a944 6
      case OPCODE_COS:
	 emit_math1(c, BRW_MATH_FUNCTION_COS, dst, args[0], BRW_MATH_PRECISION_FULL);
	 break;
      case OPCODE_DP2:
	 brw_DP2(p, dst, args[0], args[1]);
	 break;
a953 6
      case OPCODE_NRM3:
	 emit_nrm(c, dst, args[0], 3);
	 break;
      case OPCODE_NRM4:
	 emit_nrm(c, dst, args[0], 4);
	 break;
d964 1
a964 1
	 emit_arl(p, dst, args[0]);
a980 3
      case OPCODE_LRP:
	 unalias3(c, dst, args[0], args[1], args[2], emit_lrp_noalias);
	 break;
d982 1
a982 2
	 if (!accumulator_contains(c, args[2]))
	    brw_MOV(p, brw_acc_reg(), args[2]);
a984 3
      case OPCODE_CMP:
	 emit_cmp(p, dst, args[0], args[1], args[2]);
	 break;
d1004 1
a1004 1
	 emit_math1(c, BRW_MATH_FUNCTION_RSQ, dst, brw_abs(args[0]), BRW_MATH_PRECISION_FULL);
a1005 10

      case OPCODE_SEQ:
         unalias2(c, dst, args[0], args[1], emit_seq);
         break;
      case OPCODE_SIN:
	 emit_math1(c, BRW_MATH_FUNCTION_SIN, dst, args[0], BRW_MATH_PRECISION_FULL);
	 break;
      case OPCODE_SNE:
         unalias2(c, dst, args[0], args[1], emit_sne);
         break;
d1007 1
a1007 1
         unalias2(c, dst, args[0], args[1], emit_sge);
a1008 3
      case OPCODE_SGT:
         unalias2(c, dst, args[0], args[1], emit_sgt);
         break;
d1010 1
a1010 1
         unalias2(c, dst, args[0], args[1], emit_slt);
a1011 6
      case OPCODE_SLE:
         unalias2(c, dst, args[0], args[1], emit_sle);
         break;
      case OPCODE_SSG:
         unalias1(c, dst, args[0], emit_sign);
         break;
d1019 1
a1019 5
	 emit_swz(c, dst, inst);
	 break;
      case OPCODE_TRUNC:
         /* round toward zero */
	 brw_RNDZ(p, dst, args[0]);
d1024 1
a1024 91
      case OPCODE_IF:
	 assert(if_depth < MAX_IF_DEPTH);
	 if_inst[if_depth] = brw_IF(p, BRW_EXECUTE_8);
	 /* Note that brw_IF smashes the predicate_control field. */
	 if_inst[if_depth]->header.predicate_control = get_predicate(inst);
	 if_depth_in_loop[loop_depth]++;
	 if_depth++;
	 break;
      case OPCODE_ELSE:
	 clear_current_const(c);
	 assert(if_depth > 0);
	 if_inst[if_depth-1] = brw_ELSE(p, if_inst[if_depth-1]);
	 break;
      case OPCODE_ENDIF:
	 clear_current_const(c);
         assert(if_depth > 0);
	 brw_ENDIF(p, if_inst[--if_depth]);
	 if_depth_in_loop[loop_depth]--;
	 break;			
      case OPCODE_BGNLOOP:
	 clear_current_const(c);
         loop_inst[loop_depth++] = brw_DO(p, BRW_EXECUTE_8);
	 if_depth_in_loop[loop_depth] = 0;
         break;
      case OPCODE_BRK:
	 brw_set_predicate_control(p, get_predicate(inst));
	 brw_BREAK(p, if_depth_in_loop[loop_depth]);
	 brw_set_predicate_control(p, BRW_PREDICATE_NONE);
         break;
      case OPCODE_CONT:
	 brw_set_predicate_control(p, get_predicate(inst));
	 if (intel->gen >= 6) {
	    brw_CONT_gen6(p, loop_inst[loop_depth - 1]);
	 } else {
	    brw_CONT(p, if_depth_in_loop[loop_depth]);
	 }
         brw_set_predicate_control(p, BRW_PREDICATE_NONE);
         break;

      case OPCODE_ENDLOOP: {
	 clear_current_const(c);
	 struct brw_instruction *inst0, *inst1;
	 GLuint br = 1;

	 loop_depth--;

	 if (intel->gen == 5)
	    br = 2;

	 inst0 = inst1 = brw_WHILE(p, loop_inst[loop_depth]);

	 if (intel->gen < 6) {
	    /* patch all the BREAK/CONT instructions from last BEGINLOOP */
	    while (inst0 > loop_inst[loop_depth]) {
	       inst0--;
	       if (inst0->header.opcode == BRW_OPCODE_BREAK &&
		   inst0->bits3.if_else.jump_count == 0) {
		  inst0->bits3.if_else.jump_count = br * (inst1 - inst0 + 1);
	       } else if (inst0->header.opcode == BRW_OPCODE_CONTINUE &&
			  inst0->bits3.if_else.jump_count == 0) {
		  inst0->bits3.if_else.jump_count = br * (inst1 - inst0);
	       }
	    }
	 }
      }
         break;

      case OPCODE_BRA:
	 brw_set_predicate_control(p, get_predicate(inst));
         brw_ADD(p, brw_ip_reg(), brw_ip_reg(), brw_imm_d(1*16));
	 brw_set_predicate_control(p, BRW_PREDICATE_NONE);
         break;
      case OPCODE_CAL:
	 brw_set_access_mode(p, BRW_ALIGN_1);
	 brw_ADD(p, deref_1d(stack_index, 0), brw_ip_reg(), brw_imm_d(3*16));
	 brw_set_access_mode(p, BRW_ALIGN_16);
	 brw_ADD(p, get_addr_reg(stack_index),
			 get_addr_reg(stack_index), brw_imm_d(4));
         brw_save_call(p, inst->Comment, p->nr_insn);
	 brw_ADD(p, brw_ip_reg(), brw_ip_reg(), brw_imm_d(1*16));
         break;
      case OPCODE_RET:
	 brw_ADD(p, get_addr_reg(stack_index),
			 get_addr_reg(stack_index), brw_imm_d(-4));
	 brw_set_access_mode(p, BRW_ALIGN_1);
         brw_MOV(p, brw_ip_reg(), deref_1d(stack_index, 0));
	 brw_set_access_mode(p, BRW_ALIGN_16);
	 break;
      case OPCODE_END:
	 emit_vertex_write(c);
         break;
d1026 1
a1026 8
         /* no-op */
         break;
      case OPCODE_BGNSUB:
         brw_save_label(p, inst->Comment, p->nr_insn);
         break;
      case OPCODE_ENDSUB:
         /* no-op */
         break;
d1028 1
a1028 4
	 _mesa_problem(NULL, "Unsupported opcode %i (%s) in vertex shader",
                       inst->Opcode, inst->Opcode < MAX_OPCODE ?
				    _mesa_opcode_string(inst->Opcode) :
				    "unknown");
d1031 2
a1032 8
      /* Set the predication update on the last instruction of the native
       * instruction sequence.
       *
       * This would be problematic if it was set on a math instruction,
       * but that shouldn't be the case with the current GLSL compiler.
       */
      if (inst->CondUpdate) {
	 struct brw_instruction *hw_insn = &p->store[p->nr_insn - 1];
d1034 1
a1034 3
	 assert(hw_insn->header.destreg__conditionalmod == 0);
	 hw_insn->header.destreg__conditionalmod = BRW_CONDITIONAL_NZ;
      }
d1036 1
a1036 26
      if ((inst->DstReg.File == PROGRAM_OUTPUT)
          && (inst->DstReg.Index != VERT_RESULT_HPOS)
          && c->output_regs[inst->DstReg.Index].used_in_src) {
         brw_MOV(p, get_dst(c, inst->DstReg), dst);
      }

      /* Result color clamping.
       *
       * When destination register is an output register and
       * it's primary/secondary front/back color, we have to clamp
       * the result to [0,1]. This is done by enabling the
       * saturation bit for the last instruction.
       *
       * We don't use brw_set_saturate() as it modifies
       * p->current->header.saturate, which affects all the subsequent
       * instructions. Instead, we directly modify the header
       * of the last (already stored) instruction.
       */
      if (inst->DstReg.File == PROGRAM_OUTPUT) {
         if ((inst->DstReg.Index == VERT_RESULT_COL0)
             || (inst->DstReg.Index == VERT_RESULT_COL1)
             || (inst->DstReg.Index == VERT_RESULT_BFC0)
             || (inst->DstReg.Index == VERT_RESULT_BFC1)) {
            p->store[p->nr_insn-1].header.saturate = 1;
         }
      }
a1037 5
      if (inst->DstReg.RelAddr) {
	 assert(inst->DstReg.File == PROGRAM_TEMPORARY||
		inst->DstReg.File == PROGRAM_OUTPUT);
	 move_to_reladdr_dst(c, inst, dst);
      }
a1038 2
      release_tmps(c);
   }
a1039 2
   brw_resolve_cals(p);
   brw_set_uip_jip(p);
a1040 1
   brw_optimize(p);
a1041 9
   if (unlikely(INTEL_DEBUG & DEBUG_VS)) {
      int i;

      printf("vs-native:\n");
      for (i = 0; i < p->nr_insn; i++)
	 brw_disasm(stdout, &p->store[i], intel->gen);
      printf("\n");
   }
}
@


