head	1.2;
access;
symbols
	OPENBSD_5_8:1.1.1.6.0.4
	OPENBSD_5_8_BASE:1.1.1.6
	OPENBSD_5_7:1.1.1.6.0.2
	OPENBSD_5_7_BASE:1.1.1.6
	v10_2_9:1.1.1.6
	v10_4_3:1.1.1.5
	v10_2_7:1.1.1.4
	OPENBSD_5_6:1.1.1.3.0.2
	OPENBSD_5_6_BASE:1.1.1.3
	v10_2_3:1.1.1.3
	OPENBSD_5_5:1.1.1.2.0.2
	OPENBSD_5_5_BASE:1.1.1.2
	v9_2_5:1.1.1.2
	v9_2_3:1.1.1.2
	v9_2_2:1.1.1.2
	v9_2_1:1.1.1.2
	v9_2_0:1.1.1.1
	mesa:1.1.1;
locks; strict;
comment	@// @;
expand	@o@;


1.2
date	2015.12.23.05.17.49;	author jsg;	state dead;
branches;
next	1.1;
commitid	TnlogFl9nOv2eaRf;

1.1
date	2013.09.05.13.15.41;	author jsg;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2013.09.05.13.15.41;	author jsg;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2013.10.05.09.25.25;	author jsg;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2014.07.09.20.34.52;	author jsg;	state Exp;
branches;
next	1.1.1.4;
commitid	3JhLfwcuBALP0ZR7;

1.1.1.4
date	2014.09.07.15.06.16;	author jsg;	state Exp;
branches;
next	1.1.1.5;
commitid	dm8VnQHhowGHmemJ;

1.1.1.5
date	2015.01.25.14.11.49;	author jsg;	state Exp;
branches;
next	1.1.1.6;
commitid	ce2W5rH5aF7VS9gi;

1.1.1.6
date	2015.02.20.22.48.53;	author jsg;	state Exp;
branches;
next	;
commitid	F54a1i0WXHMxq7kE;


desc
@@


1.2
log
@remove the now unused Mesa 10.2.9 code
@
text
@/*
 * Copyright Â© 2011 Intel Corporation
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice (including the next
 * paragraph) shall be included in all copies or substantial portions of the
 * Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
 * IN THE SOFTWARE.
 */

#include "brw_vec4.h"
#include "glsl/ir_uniform.h"
extern "C" {
#include "main/context.h"
#include "main/macros.h"
#include "program/prog_parameter.h"
#include "program/sampler.h"
}

namespace brw {

vec4_instruction::vec4_instruction(vec4_visitor *v,
				   enum opcode opcode, dst_reg dst,
				   src_reg src0, src_reg src1, src_reg src2)
{
   this->opcode = opcode;
   this->dst = dst;
   this->src[0] = src0;
   this->src[1] = src1;
   this->src[2] = src2;
   this->ir = v->base_ir;
   this->annotation = v->current_annotation;
}

vec4_instruction *
vec4_visitor::emit(vec4_instruction *inst)
{
   this->instructions.push_tail(inst);

   return inst;
}

vec4_instruction *
vec4_visitor::emit_before(vec4_instruction *inst, vec4_instruction *new_inst)
{
   new_inst->ir = inst->ir;
   new_inst->annotation = inst->annotation;

   inst->insert_before(new_inst);

   return inst;
}

vec4_instruction *
vec4_visitor::emit(enum opcode opcode, dst_reg dst,
		   src_reg src0, src_reg src1, src_reg src2)
{
   return emit(new(mem_ctx) vec4_instruction(this, opcode, dst,
					     src0, src1, src2));
}


vec4_instruction *
vec4_visitor::emit(enum opcode opcode, dst_reg dst, src_reg src0, src_reg src1)
{
   return emit(new(mem_ctx) vec4_instruction(this, opcode, dst, src0, src1));
}

vec4_instruction *
vec4_visitor::emit(enum opcode opcode, dst_reg dst, src_reg src0)
{
   return emit(new(mem_ctx) vec4_instruction(this, opcode, dst, src0));
}

vec4_instruction *
vec4_visitor::emit(enum opcode opcode)
{
   return emit(new(mem_ctx) vec4_instruction(this, opcode, dst_reg()));
}

#define ALU1(op)							\
   vec4_instruction *							\
   vec4_visitor::op(dst_reg dst, src_reg src0)				\
   {									\
      return new(mem_ctx) vec4_instruction(this, BRW_OPCODE_##op, dst,	\
					   src0);			\
   }

#define ALU2(op)							\
   vec4_instruction *							\
   vec4_visitor::op(dst_reg dst, src_reg src0, src_reg src1)		\
   {									\
      return new(mem_ctx) vec4_instruction(this, BRW_OPCODE_##op, dst,	\
					   src0, src1);			\
   }

#define ALU3(op)							\
   vec4_instruction *							\
   vec4_visitor::op(dst_reg dst, src_reg src0, src_reg src1, src_reg src2)\
   {									\
      return new(mem_ctx) vec4_instruction(this, BRW_OPCODE_##op, dst,	\
					   src0, src1, src2);		\
   }

ALU1(NOT)
ALU1(MOV)
ALU1(FRC)
ALU1(RNDD)
ALU1(RNDE)
ALU1(RNDZ)
ALU1(F32TO16)
ALU1(F16TO32)
ALU2(ADD)
ALU2(MUL)
ALU2(MACH)
ALU2(AND)
ALU2(OR)
ALU2(XOR)
ALU2(DP3)
ALU2(DP4)
ALU2(DPH)
ALU2(SHL)
ALU2(SHR)
ALU2(ASR)
ALU3(LRP)
ALU1(BFREV)
ALU3(BFE)
ALU2(BFI1)
ALU3(BFI2)
ALU1(FBH)
ALU1(FBL)
ALU1(CBIT)

/** Gen4 predicated IF. */
vec4_instruction *
vec4_visitor::IF(uint32_t predicate)
{
   vec4_instruction *inst;

   inst = new(mem_ctx) vec4_instruction(this, BRW_OPCODE_IF);
   inst->predicate = predicate;

   return inst;
}

/** Gen6+ IF with embedded comparison. */
vec4_instruction *
vec4_visitor::IF(src_reg src0, src_reg src1, uint32_t condition)
{
   assert(brw->gen >= 6);

   vec4_instruction *inst;

   resolve_ud_negate(&src0);
   resolve_ud_negate(&src1);

   inst = new(mem_ctx) vec4_instruction(this, BRW_OPCODE_IF, dst_null_d(),
					src0, src1);
   inst->conditional_mod = condition;

   return inst;
}

/**
 * CMP: Sets the low bit of the destination channels with the result
 * of the comparison, while the upper bits are undefined, and updates
 * the flag register with the packed 16 bits of the result.
 */
vec4_instruction *
vec4_visitor::CMP(dst_reg dst, src_reg src0, src_reg src1, uint32_t condition)
{
   vec4_instruction *inst;

   /* original gen4 does type conversion to the destination type
    * before before comparison, producing garbage results for floating
    * point comparisons.
    */
   if (brw->gen == 4) {
      dst.type = src0.type;
      if (dst.file == HW_REG)
	 dst.fixed_hw_reg.type = dst.type;
   }

   resolve_ud_negate(&src0);
   resolve_ud_negate(&src1);

   inst = new(mem_ctx) vec4_instruction(this, BRW_OPCODE_CMP, dst, src0, src1);
   inst->conditional_mod = condition;

   return inst;
}

vec4_instruction *
vec4_visitor::SCRATCH_READ(dst_reg dst, src_reg index)
{
   vec4_instruction *inst;

   inst = new(mem_ctx) vec4_instruction(this, VS_OPCODE_SCRATCH_READ,
					dst, index);
   inst->base_mrf = 14;
   inst->mlen = 2;

   return inst;
}

vec4_instruction *
vec4_visitor::SCRATCH_WRITE(dst_reg dst, src_reg src, src_reg index)
{
   vec4_instruction *inst;

   inst = new(mem_ctx) vec4_instruction(this, VS_OPCODE_SCRATCH_WRITE,
					dst, src, index);
   inst->base_mrf = 13;
   inst->mlen = 3;

   return inst;
}

void
vec4_visitor::emit_dp(dst_reg dst, src_reg src0, src_reg src1, unsigned elements)
{
   static enum opcode dot_opcodes[] = {
      BRW_OPCODE_DP2, BRW_OPCODE_DP3, BRW_OPCODE_DP4
   };

   emit(dot_opcodes[elements - 2], dst, src0, src1);
}

src_reg
vec4_visitor::fix_3src_operand(src_reg src)
{
   /* Using vec4 uniforms in SIMD4x2 programs is difficult. You'd like to be
    * able to use vertical stride of zero to replicate the vec4 uniform, like
    *
    *    g3<0;4,1>:f - [0, 4][1, 5][2, 6][3, 7]
    *
    * But you can't, since vertical stride is always four in three-source
    * instructions. Instead, insert a MOV instruction to do the replication so
    * that the three-source instruction can consume it.
    */

   /* The MOV is only needed if the source is a uniform or immediate. */
   if (src.file != UNIFORM && src.file != IMM)
      return src;

   dst_reg expanded = dst_reg(this, glsl_type::vec4_type);
   expanded.type = src.type;
   emit(MOV(expanded, src));
   return src_reg(expanded);
}

src_reg
vec4_visitor::fix_math_operand(src_reg src)
{
   /* The gen6 math instruction ignores the source modifiers --
    * swizzle, abs, negate, and at least some parts of the register
    * region description.
    *
    * Rather than trying to enumerate all these cases, *always* expand the
    * operand to a temp GRF for gen6.
    *
    * For gen7, keep the operand as-is, except if immediate, which gen7 still
    * can't use.
    */

   if (brw->gen == 7 && src.file != IMM)
      return src;

   dst_reg expanded = dst_reg(this, glsl_type::vec4_type);
   expanded.type = src.type;
   emit(MOV(expanded, src));
   return src_reg(expanded);
}

void
vec4_visitor::emit_math1_gen6(enum opcode opcode, dst_reg dst, src_reg src)
{
   src = fix_math_operand(src);

   if (dst.writemask != WRITEMASK_XYZW) {
      /* The gen6 math instruction must be align1, so we can't do
       * writemasks.
       */
      dst_reg temp_dst = dst_reg(this, glsl_type::vec4_type);

      emit(opcode, temp_dst, src);

      emit(MOV(dst, src_reg(temp_dst)));
   } else {
      emit(opcode, dst, src);
   }
}

void
vec4_visitor::emit_math1_gen4(enum opcode opcode, dst_reg dst, src_reg src)
{
   vec4_instruction *inst = emit(opcode, dst, src);
   inst->base_mrf = 1;
   inst->mlen = 1;
}

void
vec4_visitor::emit_math(opcode opcode, dst_reg dst, src_reg src)
{
   switch (opcode) {
   case SHADER_OPCODE_RCP:
   case SHADER_OPCODE_RSQ:
   case SHADER_OPCODE_SQRT:
   case SHADER_OPCODE_EXP2:
   case SHADER_OPCODE_LOG2:
   case SHADER_OPCODE_SIN:
   case SHADER_OPCODE_COS:
      break;
   default:
      assert(!"not reached: bad math opcode");
      return;
   }

   if (brw->gen >= 6) {
      return emit_math1_gen6(opcode, dst, src);
   } else {
      return emit_math1_gen4(opcode, dst, src);
   }
}

void
vec4_visitor::emit_math2_gen6(enum opcode opcode,
			      dst_reg dst, src_reg src0, src_reg src1)
{
   src0 = fix_math_operand(src0);
   src1 = fix_math_operand(src1);

   if (dst.writemask != WRITEMASK_XYZW) {
      /* The gen6 math instruction must be align1, so we can't do
       * writemasks.
       */
      dst_reg temp_dst = dst_reg(this, glsl_type::vec4_type);
      temp_dst.type = dst.type;

      emit(opcode, temp_dst, src0, src1);

      emit(MOV(dst, src_reg(temp_dst)));
   } else {
      emit(opcode, dst, src0, src1);
   }
}

void
vec4_visitor::emit_math2_gen4(enum opcode opcode,
			      dst_reg dst, src_reg src0, src_reg src1)
{
   vec4_instruction *inst = emit(opcode, dst, src0, src1);
   inst->base_mrf = 1;
   inst->mlen = 2;
}

void
vec4_visitor::emit_math(enum opcode opcode,
			dst_reg dst, src_reg src0, src_reg src1)
{
   switch (opcode) {
   case SHADER_OPCODE_POW:
   case SHADER_OPCODE_INT_QUOTIENT:
   case SHADER_OPCODE_INT_REMAINDER:
      break;
   default:
      assert(!"not reached: unsupported binary math opcode");
      return;
   }

   if (brw->gen >= 6) {
      return emit_math2_gen6(opcode, dst, src0, src1);
   } else {
      return emit_math2_gen4(opcode, dst, src0, src1);
   }
}

void
vec4_visitor::emit_pack_half_2x16(dst_reg dst, src_reg src0)
{
   if (brw->gen < 7)
      assert(!"ir_unop_pack_half_2x16 should be lowered");

   assert(dst.type == BRW_REGISTER_TYPE_UD);
   assert(src0.type == BRW_REGISTER_TYPE_F);

   /* From the Ivybridge PRM, Vol4, Part3, Section 6.27 f32to16:
    *
    *   Because this instruction does not have a 16-bit floating-point type,
    *   the destination data type must be Word (W).
    *
    *   The destination must be DWord-aligned and specify a horizontal stride
    *   (HorzStride) of 2. The 16-bit result is stored in the lower word of
    *   each destination channel and the upper word is not modified.
    *
    * The above restriction implies that the f32to16 instruction must use
    * align1 mode, because only in align1 mode is it possible to specify
    * horizontal stride.  We choose here to defy the hardware docs and emit
    * align16 instructions.
    *
    * (I [chadv] did attempt to emit align1 instructions for VS f32to16
    * instructions. I was partially successful in that the code passed all
    * tests.  However, the code was dubiously correct and fragile, and the
    * tests were not harsh enough to probe that frailty. Not trusting the
    * code, I chose instead to remain in align16 mode in defiance of the hw
    * docs).
    *
    * I've [chadv] experimentally confirmed that, on gen7 hardware and the
    * simulator, emitting a f32to16 in align16 mode with UD as destination
    * data type is safe. The behavior differs from that specified in the PRM
    * in that the upper word of each destination channel is cleared to 0.
    */

   dst_reg tmp_dst(this, glsl_type::uvec2_type);
   src_reg tmp_src(tmp_dst);

#if 0
   /* Verify the undocumented behavior on which the following instructions
    * rely.  If f32to16 fails to clear the upper word of the X and Y channels,
    * then the result of the bit-or instruction below will be incorrect.
    *
    * You should inspect the disasm output in order to verify that the MOV is
    * not optimized away.
    */
   emit(MOV(tmp_dst, src_reg(0x12345678u)));
#endif

   /* Give tmp the form below, where "." means untouched.
    *
    *     w z          y          x w z          y          x
    *   |.|.|0x0000hhhh|0x0000llll|.|.|0x0000hhhh|0x0000llll|
    *
    * That the upper word of each write-channel be 0 is required for the
    * following bit-shift and bit-or instructions to work. Note that this
    * relies on the undocumented hardware behavior mentioned above.
    */
   tmp_dst.writemask = WRITEMASK_XY;
   emit(F32TO16(tmp_dst, src0));

   /* Give the write-channels of dst the form:
    *   0xhhhh0000
    */
   tmp_src.swizzle = SWIZZLE_Y;
   emit(SHL(dst, tmp_src, src_reg(16u)));

   /* Finally, give the write-channels of dst the form of packHalf2x16's
    * output:
    *   0xhhhhllll
    */
   tmp_src.swizzle = SWIZZLE_X;
   emit(OR(dst, src_reg(dst), tmp_src));
}

void
vec4_visitor::emit_unpack_half_2x16(dst_reg dst, src_reg src0)
{
   if (brw->gen < 7)
      assert(!"ir_unop_unpack_half_2x16 should be lowered");

   assert(dst.type == BRW_REGISTER_TYPE_F);
   assert(src0.type == BRW_REGISTER_TYPE_UD);

   /* From the Ivybridge PRM, Vol4, Part3, Section 6.26 f16to32:
    *
    *   Because this instruction does not have a 16-bit floating-point type,
    *   the source data type must be Word (W). The destination type must be
    *   F (Float).
    *
    * To use W as the source data type, we must adjust horizontal strides,
    * which is only possible in align1 mode. All my [chadv] attempts at
    * emitting align1 instructions for unpackHalf2x16 failed to pass the
    * Piglit tests, so I gave up.
    *
    * I've verified that, on gen7 hardware and the simulator, it is safe to
    * emit f16to32 in align16 mode with UD as source data type.
    */

   dst_reg tmp_dst(this, glsl_type::uvec2_type);
   src_reg tmp_src(tmp_dst);

   tmp_dst.writemask = WRITEMASK_X;
   emit(AND(tmp_dst, src0, src_reg(0xffffu)));

   tmp_dst.writemask = WRITEMASK_Y;
   emit(SHR(tmp_dst, src0, src_reg(16u)));

   dst.writemask = WRITEMASK_XY;
   emit(F16TO32(dst, tmp_src));
}

void
vec4_visitor::visit_instructions(const exec_list *list)
{
   foreach_list(node, list) {
      ir_instruction *ir = (ir_instruction *)node;

      base_ir = ir;
      ir->accept(this);
   }
}


static int
type_size(const struct glsl_type *type)
{
   unsigned int i;
   int size;

   switch (type->base_type) {
   case GLSL_TYPE_UINT:
   case GLSL_TYPE_INT:
   case GLSL_TYPE_FLOAT:
   case GLSL_TYPE_BOOL:
      if (type->is_matrix()) {
	 return type->matrix_columns;
      } else {
	 /* Regardless of size of vector, it gets a vec4. This is bad
	  * packing for things like floats, but otherwise arrays become a
	  * mess.  Hopefully a later pass over the code can pack scalars
	  * down if appropriate.
	  */
	 return 1;
      }
   case GLSL_TYPE_ARRAY:
      assert(type->length > 0);
      return type_size(type->fields.array) * type->length;
   case GLSL_TYPE_STRUCT:
      size = 0;
      for (i = 0; i < type->length; i++) {
	 size += type_size(type->fields.structure[i].type);
      }
      return size;
   case GLSL_TYPE_SAMPLER:
      /* Samplers take up one slot in UNIFORMS[], but they're baked in
       * at link time.
       */
      return 1;
   case GLSL_TYPE_VOID:
   case GLSL_TYPE_ERROR:
   case GLSL_TYPE_INTERFACE:
      assert(0);
      break;
   }

   return 0;
}

int
vec4_visitor::virtual_grf_alloc(int size)
{
   if (virtual_grf_array_size <= virtual_grf_count) {
      if (virtual_grf_array_size == 0)
	 virtual_grf_array_size = 16;
      else
	 virtual_grf_array_size *= 2;
      virtual_grf_sizes = reralloc(mem_ctx, virtual_grf_sizes, int,
				   virtual_grf_array_size);
      virtual_grf_reg_map = reralloc(mem_ctx, virtual_grf_reg_map, int,
				     virtual_grf_array_size);
   }
   virtual_grf_reg_map[virtual_grf_count] = virtual_grf_reg_count;
   virtual_grf_reg_count += size;
   virtual_grf_sizes[virtual_grf_count] = size;
   return virtual_grf_count++;
}

src_reg::src_reg(class vec4_visitor *v, const struct glsl_type *type)
{
   init();

   this->file = GRF;
   this->reg = v->virtual_grf_alloc(type_size(type));

   if (type->is_array() || type->is_record()) {
      this->swizzle = BRW_SWIZZLE_NOOP;
   } else {
      this->swizzle = swizzle_for_size(type->vector_elements);
   }

   this->type = brw_type_for_base_type(type);
}

dst_reg::dst_reg(class vec4_visitor *v, const struct glsl_type *type)
{
   init();

   this->file = GRF;
   this->reg = v->virtual_grf_alloc(type_size(type));

   if (type->is_array() || type->is_record()) {
      this->writemask = WRITEMASK_XYZW;
   } else {
      this->writemask = (1 << type->vector_elements) - 1;
   }

   this->type = brw_type_for_base_type(type);
}

/* Our support for uniforms is piggy-backed on the struct
 * gl_fragment_program, because that's where the values actually
 * get stored, rather than in some global gl_shader_program uniform
 * store.
 */
void
vec4_visitor::setup_uniform_values(ir_variable *ir)
{
   int namelen = strlen(ir->name);

   /* The data for our (non-builtin) uniforms is stored in a series of
    * gl_uniform_driver_storage structs for each subcomponent that
    * glGetUniformLocation() could name.  We know it's been set up in the same
    * order we'd walk the type, so walk the list of storage and find anything
    * with our name, or the prefix of a component that starts with our name.
    */
   for (unsigned u = 0; u < shader_prog->NumUserUniformStorage; u++) {
      struct gl_uniform_storage *storage = &shader_prog->UniformStorage[u];

      if (strncmp(ir->name, storage->name, namelen) != 0 ||
          (storage->name[namelen] != 0 &&
           storage->name[namelen] != '.' &&
           storage->name[namelen] != '[')) {
         continue;
      }

      gl_constant_value *components = storage->storage;
      unsigned vector_count = (MAX2(storage->array_elements, 1) *
                               storage->type->matrix_columns);

      for (unsigned s = 0; s < vector_count; s++) {
         uniform_vector_size[uniforms] = storage->type->vector_elements;

         int i;
         for (i = 0; i < uniform_vector_size[uniforms]; i++) {
            prog_data->param[uniforms * 4 + i] = &components->f;
            components++;
         }
         for (; i < 4; i++) {
            static float zero = 0;
            prog_data->param[uniforms * 4 + i] = &zero;
         }

         uniforms++;
      }
   }
}

void
vec4_visitor::setup_uniform_clipplane_values()
{
   gl_clip_plane *clip_planes = brw_select_clip_planes(ctx);

   if (brw->gen < 6) {
      /* Pre-Gen6, we compact clip planes.  For example, if the user
       * enables just clip planes 0, 1, and 3, we will enable clip planes
       * 0, 1, and 2 in the hardware, and we'll move clip plane 3 to clip
       * plane 2.  This simplifies the implementation of the Gen6 clip
       * thread.
       */
      int compacted_clipplane_index = 0;
      for (int i = 0; i < MAX_CLIP_PLANES; ++i) {
	 if (!(key->userclip_planes_enabled_gen_4_5 & (1 << i)))
	    continue;

	 this->uniform_vector_size[this->uniforms] = 4;
	 this->userplane[compacted_clipplane_index] = dst_reg(UNIFORM, this->uniforms);
	 this->userplane[compacted_clipplane_index].type = BRW_REGISTER_TYPE_F;
	 for (int j = 0; j < 4; ++j) {
	    prog_data->param[this->uniforms * 4 + j] = &clip_planes[i][j];
	 }
	 ++compacted_clipplane_index;
	 ++this->uniforms;
      }
   } else {
      /* In Gen6 and later, we don't compact clip planes, because this
       * simplifies the implementation of gl_ClipDistance.
       */
      for (int i = 0; i < key->nr_userclip_plane_consts; ++i) {
	 this->uniform_vector_size[this->uniforms] = 4;
	 this->userplane[i] = dst_reg(UNIFORM, this->uniforms);
	 this->userplane[i].type = BRW_REGISTER_TYPE_F;
	 for (int j = 0; j < 4; ++j) {
	    prog_data->param[this->uniforms * 4 + j] = &clip_planes[i][j];
	 }
	 ++this->uniforms;
      }
   }
}

/* Our support for builtin uniforms is even scarier than non-builtin.
 * It sits on top of the PROG_STATE_VAR parameters that are
 * automatically updated from GL context state.
 */
void
vec4_visitor::setup_builtin_uniform_values(ir_variable *ir)
{
   const ir_state_slot *const slots = ir->state_slots;
   assert(ir->state_slots != NULL);

   for (unsigned int i = 0; i < ir->num_state_slots; i++) {
      /* This state reference has already been setup by ir_to_mesa,
       * but we'll get the same index back here.  We can reference
       * ParameterValues directly, since unlike brw_fs.cpp, we never
       * add new state references during compile.
       */
      int index = _mesa_add_state_reference(this->prog->Parameters,
					    (gl_state_index *)slots[i].tokens);
      float *values = &this->prog->Parameters->ParameterValues[index][0].f;

      this->uniform_vector_size[this->uniforms] = 0;
      /* Add each of the unique swizzled channels of the element.
       * This will end up matching the size of the glsl_type of this field.
       */
      int last_swiz = -1;
      for (unsigned int j = 0; j < 4; j++) {
	 int swiz = GET_SWZ(slots[i].swizzle, j);
	 last_swiz = swiz;

	 prog_data->param[this->uniforms * 4 + j] = &values[swiz];
	 if (swiz <= last_swiz)
	    this->uniform_vector_size[this->uniforms]++;
      }
      this->uniforms++;
   }
}

dst_reg *
vec4_visitor::variable_storage(ir_variable *var)
{
   return (dst_reg *)hash_table_find(this->variable_ht, var);
}

void
vec4_visitor::emit_bool_to_cond_code(ir_rvalue *ir, uint32_t *predicate)
{
   ir_expression *expr = ir->as_expression();

   *predicate = BRW_PREDICATE_NORMAL;

   if (expr) {
      src_reg op[2];
      vec4_instruction *inst;

      assert(expr->get_num_operands() <= 2);
      for (unsigned int i = 0; i < expr->get_num_operands(); i++) {
	 expr->operands[i]->accept(this);
	 op[i] = this->result;

	 resolve_ud_negate(&op[i]);
      }

      switch (expr->operation) {
      case ir_unop_logic_not:
	 inst = emit(AND(dst_null_d(), op[0], src_reg(1)));
	 inst->conditional_mod = BRW_CONDITIONAL_Z;
	 break;

      case ir_binop_logic_xor:
	 inst = emit(XOR(dst_null_d(), op[0], op[1]));
	 inst->conditional_mod = BRW_CONDITIONAL_NZ;
	 break;

      case ir_binop_logic_or:
	 inst = emit(OR(dst_null_d(), op[0], op[1]));
	 inst->conditional_mod = BRW_CONDITIONAL_NZ;
	 break;

      case ir_binop_logic_and:
	 inst = emit(AND(dst_null_d(), op[0], op[1]));
	 inst->conditional_mod = BRW_CONDITIONAL_NZ;
	 break;

      case ir_unop_f2b:
	 if (brw->gen >= 6) {
	    emit(CMP(dst_null_d(), op[0], src_reg(0.0f), BRW_CONDITIONAL_NZ));
	 } else {
	    inst = emit(MOV(dst_null_f(), op[0]));
	    inst->conditional_mod = BRW_CONDITIONAL_NZ;
	 }
	 break;

      case ir_unop_i2b:
	 if (brw->gen >= 6) {
	    emit(CMP(dst_null_d(), op[0], src_reg(0), BRW_CONDITIONAL_NZ));
	 } else {
	    inst = emit(MOV(dst_null_d(), op[0]));
	    inst->conditional_mod = BRW_CONDITIONAL_NZ;
	 }
	 break;

      case ir_binop_all_equal:
	 inst = emit(CMP(dst_null_d(), op[0], op[1], BRW_CONDITIONAL_Z));
	 *predicate = BRW_PREDICATE_ALIGN16_ALL4H;
	 break;

      case ir_binop_any_nequal:
	 inst = emit(CMP(dst_null_d(), op[0], op[1], BRW_CONDITIONAL_NZ));
	 *predicate = BRW_PREDICATE_ALIGN16_ANY4H;
	 break;

      case ir_unop_any:
	 inst = emit(CMP(dst_null_d(), op[0], src_reg(0), BRW_CONDITIONAL_NZ));
	 *predicate = BRW_PREDICATE_ALIGN16_ANY4H;
	 break;

      case ir_binop_greater:
      case ir_binop_gequal:
      case ir_binop_less:
      case ir_binop_lequal:
      case ir_binop_equal:
      case ir_binop_nequal:
	 emit(CMP(dst_null_d(), op[0], op[1],
		  brw_conditional_for_comparison(expr->operation)));
	 break;

      default:
	 assert(!"not reached");
	 break;
      }
      return;
   }

   ir->accept(this);

   resolve_ud_negate(&this->result);

   if (brw->gen >= 6) {
      vec4_instruction *inst = emit(AND(dst_null_d(),
					this->result, src_reg(1)));
      inst->conditional_mod = BRW_CONDITIONAL_NZ;
   } else {
      vec4_instruction *inst = emit(MOV(dst_null_d(), this->result));
      inst->conditional_mod = BRW_CONDITIONAL_NZ;
   }
}

/**
 * Emit a gen6 IF statement with the comparison folded into the IF
 * instruction.
 */
void
vec4_visitor::emit_if_gen6(ir_if *ir)
{
   ir_expression *expr = ir->condition->as_expression();

   if (expr) {
      src_reg op[2];
      dst_reg temp;

      assert(expr->get_num_operands() <= 2);
      for (unsigned int i = 0; i < expr->get_num_operands(); i++) {
	 expr->operands[i]->accept(this);
	 op[i] = this->result;
      }

      switch (expr->operation) {
      case ir_unop_logic_not:
	 emit(IF(op[0], src_reg(0), BRW_CONDITIONAL_Z));
	 return;

      case ir_binop_logic_xor:
	 emit(IF(op[0], op[1], BRW_CONDITIONAL_NZ));
	 return;

      case ir_binop_logic_or:
	 temp = dst_reg(this, glsl_type::bool_type);
	 emit(OR(temp, op[0], op[1]));
	 emit(IF(src_reg(temp), src_reg(0), BRW_CONDITIONAL_NZ));
	 return;

      case ir_binop_logic_and:
	 temp = dst_reg(this, glsl_type::bool_type);
	 emit(AND(temp, op[0], op[1]));
	 emit(IF(src_reg(temp), src_reg(0), BRW_CONDITIONAL_NZ));
	 return;

      case ir_unop_f2b:
	 emit(IF(op[0], src_reg(0), BRW_CONDITIONAL_NZ));
	 return;

      case ir_unop_i2b:
	 emit(IF(op[0], src_reg(0), BRW_CONDITIONAL_NZ));
	 return;

      case ir_binop_greater:
      case ir_binop_gequal:
      case ir_binop_less:
      case ir_binop_lequal:
      case ir_binop_equal:
      case ir_binop_nequal:
	 emit(IF(op[0], op[1],
		 brw_conditional_for_comparison(expr->operation)));
	 return;

      case ir_binop_all_equal:
	 emit(CMP(dst_null_d(), op[0], op[1], BRW_CONDITIONAL_Z));
	 emit(IF(BRW_PREDICATE_ALIGN16_ALL4H));
	 return;

      case ir_binop_any_nequal:
	 emit(CMP(dst_null_d(), op[0], op[1], BRW_CONDITIONAL_NZ));
	 emit(IF(BRW_PREDICATE_ALIGN16_ANY4H));
	 return;

      case ir_unop_any:
	 emit(CMP(dst_null_d(), op[0], src_reg(0), BRW_CONDITIONAL_NZ));
	 emit(IF(BRW_PREDICATE_ALIGN16_ANY4H));
	 return;

      default:
	 assert(!"not reached");
	 emit(IF(op[0], src_reg(0), BRW_CONDITIONAL_NZ));
	 return;
      }
      return;
   }

   ir->condition->accept(this);

   emit(IF(this->result, src_reg(0), BRW_CONDITIONAL_NZ));
}

static dst_reg
with_writemask(dst_reg const & r, int mask)
{
   dst_reg result = r;
   result.writemask = mask;
   return result;
}

void
vec4_vs_visitor::emit_prolog()
{
   dst_reg sign_recovery_shift;
   dst_reg normalize_factor;
   dst_reg es3_normalize_factor;

   for (int i = 0; i < VERT_ATTRIB_MAX; i++) {
      if (vs_prog_data->inputs_read & BITFIELD64_BIT(i)) {
         uint8_t wa_flags = vs_compile->key.gl_attrib_wa_flags[i];
         dst_reg reg(ATTR, i);
         dst_reg reg_d = reg;
         reg_d.type = BRW_REGISTER_TYPE_D;
         dst_reg reg_ud = reg;
         reg_ud.type = BRW_REGISTER_TYPE_UD;

         /* Do GL_FIXED rescaling for GLES2.0.  Our GL_FIXED attributes
          * come in as floating point conversions of the integer values.
          */
         if (wa_flags & BRW_ATTRIB_WA_COMPONENT_MASK) {
            dst_reg dst = reg;
            dst.type = brw_type_for_base_type(glsl_type::vec4_type);
            dst.writemask = (1 << (wa_flags & BRW_ATTRIB_WA_COMPONENT_MASK)) - 1;
            emit(MUL(dst, src_reg(dst), src_reg(1.0f / 65536.0f)));
         }

         /* Do sign recovery for 2101010 formats if required. */
         if (wa_flags & BRW_ATTRIB_WA_SIGN) {
            if (sign_recovery_shift.file == BAD_FILE) {
               /* shift constant: <22,22,22,30> */
               sign_recovery_shift = dst_reg(this, glsl_type::uvec4_type);
               emit(MOV(with_writemask(sign_recovery_shift, WRITEMASK_XYZ), src_reg(22u)));
               emit(MOV(with_writemask(sign_recovery_shift, WRITEMASK_W), src_reg(30u)));
            }

            emit(SHL(reg_ud, src_reg(reg_ud), src_reg(sign_recovery_shift)));
            emit(ASR(reg_d, src_reg(reg_d), src_reg(sign_recovery_shift)));
         }

         /* Apply BGRA swizzle if required. */
         if (wa_flags & BRW_ATTRIB_WA_BGRA) {
            src_reg temp = src_reg(reg);
            temp.swizzle = BRW_SWIZZLE4(2,1,0,3);
            emit(MOV(reg, temp));
         }

         if (wa_flags & BRW_ATTRIB_WA_NORMALIZE) {
            /* ES 3.0 has different rules for converting signed normalized
             * fixed-point numbers than desktop GL.
             */
            if (_mesa_is_gles3(ctx) && (wa_flags & BRW_ATTRIB_WA_SIGN)) {
               /* According to equation 2.2 of the ES 3.0 specification,
                * signed normalization conversion is done by:
                *
                * f = c / (2^(b-1)-1)
                */
               if (es3_normalize_factor.file == BAD_FILE) {
                  /* mul constant: 1 / (2^(b-1) - 1) */
                  es3_normalize_factor = dst_reg(this, glsl_type::vec4_type);
                  emit(MOV(with_writemask(es3_normalize_factor, WRITEMASK_XYZ),
                           src_reg(1.0f / ((1<<9) - 1))));
                  emit(MOV(with_writemask(es3_normalize_factor, WRITEMASK_W),
                           src_reg(1.0f / ((1<<1) - 1))));
               }

               dst_reg dst = reg;
               dst.type = brw_type_for_base_type(glsl_type::vec4_type);
               emit(MOV(dst, src_reg(reg_d)));
               emit(MUL(dst, src_reg(dst), src_reg(es3_normalize_factor)));
               emit_minmax(BRW_CONDITIONAL_G, dst, src_reg(dst), src_reg(-1.0f));
            } else {
               /* The following equations are from the OpenGL 3.2 specification:
                *
                * 2.1 unsigned normalization
                * f = c/(2^n-1)
                *
                * 2.2 signed normalization
                * f = (2c+1)/(2^n-1)
                *
                * Both of these share a common divisor, which is represented by
                * "normalize_factor" in the code below.
                */
               if (normalize_factor.file == BAD_FILE) {
                  /* 1 / (2^b - 1) for b=<10,10,10,2> */
                  normalize_factor = dst_reg(this, glsl_type::vec4_type);
                  emit(MOV(with_writemask(normalize_factor, WRITEMASK_XYZ),
                           src_reg(1.0f / ((1<<10) - 1))));
                  emit(MOV(with_writemask(normalize_factor, WRITEMASK_W),
                           src_reg(1.0f / ((1<<2) - 1))));
               }

               dst_reg dst = reg;
               dst.type = brw_type_for_base_type(glsl_type::vec4_type);
               emit(MOV(dst, src_reg((wa_flags & BRW_ATTRIB_WA_SIGN) ? reg_d : reg_ud)));

               /* For signed normalization, we want the numerator to be 2c+1. */
               if (wa_flags & BRW_ATTRIB_WA_SIGN) {
                  emit(MUL(dst, src_reg(dst), src_reg(2.0f)));
                  emit(ADD(dst, src_reg(dst), src_reg(1.0f)));
               }

               emit(MUL(dst, src_reg(dst), src_reg(normalize_factor)));
            }
         }

         if (wa_flags & BRW_ATTRIB_WA_SCALE) {
            dst_reg dst = reg;
            dst.type = brw_type_for_base_type(glsl_type::vec4_type);
            emit(MOV(dst, src_reg((wa_flags & BRW_ATTRIB_WA_SIGN) ? reg_d : reg_ud)));
         }
      }
   }
}


dst_reg *
vec4_vs_visitor::make_reg_for_system_value(ir_variable *ir)
{
   /* VertexID is stored by the VF as the last vertex element, but
    * we don't represent it with a flag in inputs_read, so we call
    * it VERT_ATTRIB_MAX, which setup_attributes() picks up on.
    */
   dst_reg *reg = new(mem_ctx) dst_reg(ATTR, VERT_ATTRIB_MAX);
   vs_prog_data->uses_vertexid = true;

   switch (ir->location) {
   case SYSTEM_VALUE_VERTEX_ID:
      reg->writemask = WRITEMASK_X;
      break;
   case SYSTEM_VALUE_INSTANCE_ID:
      reg->writemask = WRITEMASK_Y;
      break;
   default:
      assert(!"not reached");
      break;
   }

   return reg;
}


void
vec4_visitor::visit(ir_variable *ir)
{
   dst_reg *reg = NULL;

   if (variable_storage(ir))
      return;

   switch (ir->mode) {
   case ir_var_shader_in:
      reg = new(mem_ctx) dst_reg(ATTR, ir->location);
      break;

   case ir_var_shader_out:
      reg = new(mem_ctx) dst_reg(this, ir->type);

      for (int i = 0; i < type_size(ir->type); i++) {
	 output_reg[ir->location + i] = *reg;
	 output_reg[ir->location + i].reg_offset = i;
	 output_reg[ir->location + i].type =
            brw_type_for_base_type(ir->type->get_scalar_type());
	 output_reg_annotation[ir->location + i] = ir->name;
      }
      break;

   case ir_var_auto:
   case ir_var_temporary:
      reg = new(mem_ctx) dst_reg(this, ir->type);
      break;

   case ir_var_uniform:
      reg = new(this->mem_ctx) dst_reg(UNIFORM, this->uniforms);

      /* Thanks to the lower_ubo_reference pass, we will see only
       * ir_binop_ubo_load expressions and not ir_dereference_variable for UBO
       * variables, so no need for them to be in variable_ht.
       */
      if (ir->is_in_uniform_block())
         return;

      /* Track how big the whole uniform variable is, in case we need to put a
       * copy of its data into pull constants for array access.
       */
      this->uniform_size[this->uniforms] = type_size(ir->type);

      if (!strncmp(ir->name, "gl_", 3)) {
	 setup_builtin_uniform_values(ir);
      } else {
	 setup_uniform_values(ir);
      }
      break;

   case ir_var_system_value:
      reg = make_reg_for_system_value(ir);
      break;

   default:
      assert(!"not reached");
   }

   reg->type = brw_type_for_base_type(ir->type);
   hash_table_insert(this->variable_ht, reg, ir);
}

void
vec4_visitor::visit(ir_loop *ir)
{
   dst_reg counter;

   /* We don't want debugging output to print the whole body of the
    * loop as the annotation.
    */
   this->base_ir = NULL;

   if (ir->counter != NULL) {
      this->base_ir = ir->counter;
      ir->counter->accept(this);
      counter = *(variable_storage(ir->counter));

      if (ir->from != NULL) {
	 this->base_ir = ir->from;
	 ir->from->accept(this);

	 emit(MOV(counter, this->result));
      }
   }

   emit(BRW_OPCODE_DO);

   if (ir->to) {
      this->base_ir = ir->to;
      ir->to->accept(this);

      emit(CMP(dst_null_d(), src_reg(counter), this->result,
	       brw_conditional_for_comparison(ir->cmp)));

      vec4_instruction *inst = emit(BRW_OPCODE_BREAK);
      inst->predicate = BRW_PREDICATE_NORMAL;
   }

   visit_instructions(&ir->body_instructions);


   if (ir->increment) {
      this->base_ir = ir->increment;
      ir->increment->accept(this);
      emit(ADD(counter, src_reg(counter), this->result));
   }

   emit(BRW_OPCODE_WHILE);
}

void
vec4_visitor::visit(ir_loop_jump *ir)
{
   switch (ir->mode) {
   case ir_loop_jump::jump_break:
      emit(BRW_OPCODE_BREAK);
      break;
   case ir_loop_jump::jump_continue:
      emit(BRW_OPCODE_CONTINUE);
      break;
   }
}


void
vec4_visitor::visit(ir_function_signature *ir)
{
   assert(0);
   (void)ir;
}

void
vec4_visitor::visit(ir_function *ir)
{
   /* Ignore function bodies other than main() -- we shouldn't see calls to
    * them since they should all be inlined.
    */
   if (strcmp(ir->name, "main") == 0) {
      const ir_function_signature *sig;
      exec_list empty;

      sig = ir->matching_signature(&empty);

      assert(sig);

      visit_instructions(&sig->body);
   }
}

bool
vec4_visitor::try_emit_sat(ir_expression *ir)
{
   ir_rvalue *sat_src = ir->as_rvalue_to_saturate();
   if (!sat_src)
      return false;

   sat_src->accept(this);
   src_reg src = this->result;

   this->result = src_reg(this, ir->type);
   vec4_instruction *inst;
   inst = emit(MOV(dst_reg(this->result), src));
   inst->saturate = true;

   return true;
}

bool
vec4_visitor::try_emit_mad(ir_expression *ir, int mul_arg)
{
   /* 3-src instructions were introduced in gen6. */
   if (brw->gen < 6)
      return false;

   /* MAD can only handle floating-point data. */
   if (ir->type->base_type != GLSL_TYPE_FLOAT)
      return false;

   ir_rvalue *nonmul = ir->operands[1 - mul_arg];
   ir_expression *mul = ir->operands[mul_arg]->as_expression();

   if (!mul || mul->operation != ir_binop_mul)
      return false;

   nonmul->accept(this);
   src_reg src0 = fix_3src_operand(this->result);

   mul->operands[0]->accept(this);
   src_reg src1 = fix_3src_operand(this->result);

   mul->operands[1]->accept(this);
   src_reg src2 = fix_3src_operand(this->result);

   this->result = src_reg(this, ir->type);
   emit(BRW_OPCODE_MAD, dst_reg(this->result), src0, src1, src2);

   return true;
}

void
vec4_visitor::emit_bool_comparison(unsigned int op,
				 dst_reg dst, src_reg src0, src_reg src1)
{
   /* original gen4 does destination conversion before comparison. */
   if (brw->gen < 5)
      dst.type = src0.type;

   emit(CMP(dst, src0, src1, brw_conditional_for_comparison(op)));

   dst.type = BRW_REGISTER_TYPE_D;
   emit(AND(dst, src_reg(dst), src_reg(0x1)));
}

void
vec4_visitor::emit_minmax(uint32_t conditionalmod, dst_reg dst,
                          src_reg src0, src_reg src1)
{
   vec4_instruction *inst;

   if (brw->gen >= 6) {
      inst = emit(BRW_OPCODE_SEL, dst, src0, src1);
      inst->conditional_mod = conditionalmod;
   } else {
      emit(CMP(dst, src0, src1, conditionalmod));

      inst = emit(BRW_OPCODE_SEL, dst, src0, src1);
      inst->predicate = BRW_PREDICATE_NORMAL;
   }
}

static bool
is_16bit_constant(ir_rvalue *rvalue)
{
   ir_constant *constant = rvalue->as_constant();
   if (!constant)
      return false;

   if (constant->type != glsl_type::int_type &&
       constant->type != glsl_type::uint_type)
      return false;

   return constant->value.u[0] < (1 << 16);
}

void
vec4_visitor::visit(ir_expression *ir)
{
   unsigned int operand;
   src_reg op[Elements(ir->operands)];
   src_reg result_src;
   dst_reg result_dst;
   vec4_instruction *inst;

   if (try_emit_sat(ir))
      return;

   if (ir->operation == ir_binop_add) {
      if (try_emit_mad(ir, 0) || try_emit_mad(ir, 1))
	 return;
   }

   for (operand = 0; operand < ir->get_num_operands(); operand++) {
      this->result.file = BAD_FILE;
      ir->operands[operand]->accept(this);
      if (this->result.file == BAD_FILE) {
	 printf("Failed to get tree for expression operand:\n");
	 ir->operands[operand]->print();
	 exit(1);
      }
      op[operand] = this->result;

      /* Matrix expression operands should have been broken down to vector
       * operations already.
       */
      assert(!ir->operands[operand]->type->is_matrix());
   }

   int vector_elements = ir->operands[0]->type->vector_elements;
   if (ir->operands[1]) {
      vector_elements = MAX2(vector_elements,
			     ir->operands[1]->type->vector_elements);
   }

   this->result.file = BAD_FILE;

   /* Storage for our result.  Ideally for an assignment we'd be using
    * the actual storage for the result here, instead.
    */
   result_src = src_reg(this, ir->type);
   /* convenience for the emit functions below. */
   result_dst = dst_reg(result_src);
   /* If nothing special happens, this is the result. */
   this->result = result_src;
   /* Limit writes to the channels that will be used by result_src later.
    * This does limit this temp's use as a temporary for multi-instruction
    * sequences.
    */
   result_dst.writemask = (1 << ir->type->vector_elements) - 1;

   switch (ir->operation) {
   case ir_unop_logic_not:
      /* Note that BRW_OPCODE_NOT is not appropriate here, since it is
       * ones complement of the whole register, not just bit 0.
       */
      emit(XOR(result_dst, op[0], src_reg(1)));
      break;
   case ir_unop_neg:
      op[0].negate = !op[0].negate;
      emit(MOV(result_dst, op[0]));
      break;
   case ir_unop_abs:
      op[0].abs = true;
      op[0].negate = false;
      emit(MOV(result_dst, op[0]));
      break;

   case ir_unop_sign:
      emit(MOV(result_dst, src_reg(0.0f)));

      emit(CMP(dst_null_d(), op[0], src_reg(0.0f), BRW_CONDITIONAL_G));
      inst = emit(MOV(result_dst, src_reg(1.0f)));
      inst->predicate = BRW_PREDICATE_NORMAL;

      emit(CMP(dst_null_d(), op[0], src_reg(0.0f), BRW_CONDITIONAL_L));
      inst = emit(MOV(result_dst, src_reg(-1.0f)));
      inst->predicate = BRW_PREDICATE_NORMAL;

      break;

   case ir_unop_rcp:
      emit_math(SHADER_OPCODE_RCP, result_dst, op[0]);
      break;

   case ir_unop_exp2:
      emit_math(SHADER_OPCODE_EXP2, result_dst, op[0]);
      break;
   case ir_unop_log2:
      emit_math(SHADER_OPCODE_LOG2, result_dst, op[0]);
      break;
   case ir_unop_exp:
   case ir_unop_log:
      assert(!"not reached: should be handled by ir_explog_to_explog2");
      break;
   case ir_unop_sin:
   case ir_unop_sin_reduced:
      emit_math(SHADER_OPCODE_SIN, result_dst, op[0]);
      break;
   case ir_unop_cos:
   case ir_unop_cos_reduced:
      emit_math(SHADER_OPCODE_COS, result_dst, op[0]);
      break;

   case ir_unop_dFdx:
   case ir_unop_dFdy:
      assert(!"derivatives not valid in vertex shader");
      break;

   case ir_unop_bitfield_reverse:
      emit(BFREV(result_dst, op[0]));
      break;
   case ir_unop_bit_count:
      emit(CBIT(result_dst, op[0]));
      break;
   case ir_unop_find_msb: {
      src_reg temp = src_reg(this, glsl_type::uint_type);

      inst = emit(FBH(dst_reg(temp), op[0]));
      inst->dst.writemask = WRITEMASK_XYZW;

      /* FBH counts from the MSB side, while GLSL's findMSB() wants the count
       * from the LSB side. If FBH didn't return an error (0xFFFFFFFF), then
       * subtract the result from 31 to convert the MSB count into an LSB count.
       */

      /* FBH only supports UD type for dst, so use a MOV to convert UD to D. */
      temp.swizzle = BRW_SWIZZLE_NOOP;
      emit(MOV(result_dst, temp));

      src_reg src_tmp = src_reg(result_dst);
      emit(CMP(dst_null_d(), src_tmp, src_reg(-1), BRW_CONDITIONAL_NZ));

      src_tmp.negate = true;
      inst = emit(ADD(result_dst, src_tmp, src_reg(31)));
      inst->predicate = BRW_PREDICATE_NORMAL;
      break;
   }
   case ir_unop_find_lsb:
      emit(FBL(result_dst, op[0]));
      break;

   case ir_unop_noise:
      assert(!"not reached: should be handled by lower_noise");
      break;

   case ir_binop_add:
      emit(ADD(result_dst, op[0], op[1]));
      break;
   case ir_binop_sub:
      assert(!"not reached: should be handled by ir_sub_to_add_neg");
      break;

   case ir_binop_mul:
      if (ir->type->is_integer()) {
	 /* For integer multiplication, the MUL uses the low 16 bits of one of
	  * the operands (src0 through SNB, src1 on IVB and later).  The MACH
	  * accumulates in the contribution of the upper 16 bits of that
	  * operand.  If we can determine that one of the args is in the low
	  * 16 bits, though, we can just emit a single MUL.
          */
         if (is_16bit_constant(ir->operands[0])) {
            if (brw->gen < 7)
               emit(MUL(result_dst, op[0], op[1]));
            else
               emit(MUL(result_dst, op[1], op[0]));
         } else if (is_16bit_constant(ir->operands[1])) {
            if (brw->gen < 7)
               emit(MUL(result_dst, op[1], op[0]));
            else
               emit(MUL(result_dst, op[0], op[1]));
         } else {
            struct brw_reg acc = retype(brw_acc_reg(), BRW_REGISTER_TYPE_D);

            emit(MUL(acc, op[0], op[1]));
            emit(MACH(dst_null_d(), op[0], op[1]));
            emit(MOV(result_dst, src_reg(acc)));
         }
      } else {
	 emit(MUL(result_dst, op[0], op[1]));
      }
      break;
   case ir_binop_div:
      /* Floating point should be lowered by DIV_TO_MUL_RCP in the compiler. */
      assert(ir->type->is_integer());
      emit_math(SHADER_OPCODE_INT_QUOTIENT, result_dst, op[0], op[1]);
      break;
   case ir_binop_mod:
      /* Floating point should be lowered by MOD_TO_FRACT in the compiler. */
      assert(ir->type->is_integer());
      emit_math(SHADER_OPCODE_INT_REMAINDER, result_dst, op[0], op[1]);
      break;

   case ir_binop_less:
   case ir_binop_greater:
   case ir_binop_lequal:
   case ir_binop_gequal:
   case ir_binop_equal:
   case ir_binop_nequal: {
      emit(CMP(result_dst, op[0], op[1],
	       brw_conditional_for_comparison(ir->operation)));
      emit(AND(result_dst, result_src, src_reg(0x1)));
      break;
   }

   case ir_binop_all_equal:
      /* "==" operator producing a scalar boolean. */
      if (ir->operands[0]->type->is_vector() ||
	  ir->operands[1]->type->is_vector()) {
	 emit(CMP(dst_null_d(), op[0], op[1], BRW_CONDITIONAL_Z));
	 emit(MOV(result_dst, src_reg(0)));
	 inst = emit(MOV(result_dst, src_reg(1)));
	 inst->predicate = BRW_PREDICATE_ALIGN16_ALL4H;
      } else {
	 emit(CMP(result_dst, op[0], op[1], BRW_CONDITIONAL_Z));
	 emit(AND(result_dst, result_src, src_reg(0x1)));
      }
      break;
   case ir_binop_any_nequal:
      /* "!=" operator producing a scalar boolean. */
      if (ir->operands[0]->type->is_vector() ||
	  ir->operands[1]->type->is_vector()) {
	 emit(CMP(dst_null_d(), op[0], op[1], BRW_CONDITIONAL_NZ));

	 emit(MOV(result_dst, src_reg(0)));
	 inst = emit(MOV(result_dst, src_reg(1)));
	 inst->predicate = BRW_PREDICATE_ALIGN16_ANY4H;
      } else {
	 emit(CMP(result_dst, op[0], op[1], BRW_CONDITIONAL_NZ));
	 emit(AND(result_dst, result_src, src_reg(0x1)));
      }
      break;

   case ir_unop_any:
      emit(CMP(dst_null_d(), op[0], src_reg(0), BRW_CONDITIONAL_NZ));
      emit(MOV(result_dst, src_reg(0)));

      inst = emit(MOV(result_dst, src_reg(1)));
      inst->predicate = BRW_PREDICATE_ALIGN16_ANY4H;
      break;

   case ir_binop_logic_xor:
      emit(XOR(result_dst, op[0], op[1]));
      break;

   case ir_binop_logic_or:
      emit(OR(result_dst, op[0], op[1]));
      break;

   case ir_binop_logic_and:
      emit(AND(result_dst, op[0], op[1]));
      break;

   case ir_binop_dot:
      assert(ir->operands[0]->type->is_vector());
      assert(ir->operands[0]->type == ir->operands[1]->type);
      emit_dp(result_dst, op[0], op[1], ir->operands[0]->type->vector_elements);
      break;

   case ir_unop_sqrt:
      emit_math(SHADER_OPCODE_SQRT, result_dst, op[0]);
      break;
   case ir_unop_rsq:
      emit_math(SHADER_OPCODE_RSQ, result_dst, op[0]);
      break;

   case ir_unop_bitcast_i2f:
   case ir_unop_bitcast_u2f:
      this->result = op[0];
      this->result.type = BRW_REGISTER_TYPE_F;
      break;

   case ir_unop_bitcast_f2i:
      this->result = op[0];
      this->result.type = BRW_REGISTER_TYPE_D;
      break;

   case ir_unop_bitcast_f2u:
      this->result = op[0];
      this->result.type = BRW_REGISTER_TYPE_UD;
      break;

   case ir_unop_i2f:
   case ir_unop_i2u:
   case ir_unop_u2i:
   case ir_unop_u2f:
   case ir_unop_b2f:
   case ir_unop_b2i:
   case ir_unop_f2i:
   case ir_unop_f2u:
      emit(MOV(result_dst, op[0]));
      break;
   case ir_unop_f2b:
   case ir_unop_i2b: {
      emit(CMP(result_dst, op[0], src_reg(0.0f), BRW_CONDITIONAL_NZ));
      emit(AND(result_dst, result_src, src_reg(1)));
      break;
   }

   case ir_unop_trunc:
      emit(RNDZ(result_dst, op[0]));
      break;
   case ir_unop_ceil:
      op[0].negate = !op[0].negate;
      inst = emit(RNDD(result_dst, op[0]));
      this->result.negate = true;
      break;
   case ir_unop_floor:
      inst = emit(RNDD(result_dst, op[0]));
      break;
   case ir_unop_fract:
      inst = emit(FRC(result_dst, op[0]));
      break;
   case ir_unop_round_even:
      emit(RNDE(result_dst, op[0]));
      break;

   case ir_binop_min:
      emit_minmax(BRW_CONDITIONAL_L, result_dst, op[0], op[1]);
      break;
   case ir_binop_max:
      emit_minmax(BRW_CONDITIONAL_G, result_dst, op[0], op[1]);
      break;

   case ir_binop_pow:
      emit_math(SHADER_OPCODE_POW, result_dst, op[0], op[1]);
      break;

   case ir_unop_bit_not:
      inst = emit(NOT(result_dst, op[0]));
      break;
   case ir_binop_bit_and:
      inst = emit(AND(result_dst, op[0], op[1]));
      break;
   case ir_binop_bit_xor:
      inst = emit(XOR(result_dst, op[0], op[1]));
      break;
   case ir_binop_bit_or:
      inst = emit(OR(result_dst, op[0], op[1]));
      break;

   case ir_binop_lshift:
      inst = emit(SHL(result_dst, op[0], op[1]));
      break;

   case ir_binop_rshift:
      if (ir->type->base_type == GLSL_TYPE_INT)
         inst = emit(ASR(result_dst, op[0], op[1]));
      else
         inst = emit(SHR(result_dst, op[0], op[1]));
      break;

   case ir_binop_bfm:
      emit(BFI1(result_dst, op[0], op[1]));
      break;

   case ir_binop_ubo_load: {
      ir_constant *uniform_block = ir->operands[0]->as_constant();
      ir_constant *const_offset_ir = ir->operands[1]->as_constant();
      unsigned const_offset = const_offset_ir ? const_offset_ir->value.u[0] : 0;
      src_reg offset = op[1];

      /* Now, load the vector from that offset. */
      assert(ir->type->is_vector() || ir->type->is_scalar());

      src_reg packed_consts = src_reg(this, glsl_type::vec4_type);
      packed_consts.type = result.type;
      src_reg surf_index =
         src_reg(SURF_INDEX_VS_UBO(uniform_block->value.u[0]));
      if (const_offset_ir) {
         offset = src_reg(const_offset / 16);
      } else {
         emit(SHR(dst_reg(offset), offset, src_reg(4)));
      }

      vec4_instruction *pull =
         emit(new(mem_ctx) vec4_instruction(this,
                                            VS_OPCODE_PULL_CONSTANT_LOAD,
                                            dst_reg(packed_consts),
                                            surf_index,
                                            offset));
      pull->base_mrf = 14;
      pull->mlen = 1;

      packed_consts.swizzle = swizzle_for_size(ir->type->vector_elements);
      packed_consts.swizzle += BRW_SWIZZLE4(const_offset % 16 / 4,
                                            const_offset % 16 / 4,
                                            const_offset % 16 / 4,
                                            const_offset % 16 / 4);

      /* UBO bools are any nonzero int.  We store bools as either 0 or 1. */
      if (ir->type->base_type == GLSL_TYPE_BOOL) {
         emit(CMP(result_dst, packed_consts, src_reg(0u),
                  BRW_CONDITIONAL_NZ));
         emit(AND(result_dst, result, src_reg(0x1)));
      } else {
         emit(MOV(result_dst, packed_consts));
      }
      break;
   }

   case ir_binop_vector_extract:
      assert(!"should have been lowered by vec_index_to_cond_assign");
      break;

   case ir_triop_lrp:
      op[0] = fix_3src_operand(op[0]);
      op[1] = fix_3src_operand(op[1]);
      op[2] = fix_3src_operand(op[2]);
      /* Note that the instruction's argument order is reversed from GLSL
       * and the IR.
       */
      emit(LRP(result_dst, op[2], op[1], op[0]));
      break;

   case ir_triop_bfi:
      op[0] = fix_3src_operand(op[0]);
      op[1] = fix_3src_operand(op[1]);
      op[2] = fix_3src_operand(op[2]);
      emit(BFI2(result_dst, op[0], op[1], op[2]));
      break;

   case ir_triop_bitfield_extract:
      op[0] = fix_3src_operand(op[0]);
      op[1] = fix_3src_operand(op[1]);
      op[2] = fix_3src_operand(op[2]);
      /* Note that the instruction's argument order is reversed from GLSL
       * and the IR.
       */
      emit(BFE(result_dst, op[2], op[1], op[0]));
      break;

   case ir_triop_vector_insert:
      assert(!"should have been lowered by lower_vector_insert");
      break;

   case ir_quadop_bitfield_insert:
      assert(!"not reached: should be handled by "
              "bitfield_insert_to_bfm_bfi\n");
      break;

   case ir_quadop_vector:
      assert(!"not reached: should be handled by lower_quadop_vector");
      break;

   case ir_unop_pack_half_2x16:
      emit_pack_half_2x16(result_dst, op[0]);
      break;
   case ir_unop_unpack_half_2x16:
      emit_unpack_half_2x16(result_dst, op[0]);
      break;
   case ir_unop_pack_snorm_2x16:
   case ir_unop_pack_snorm_4x8:
   case ir_unop_pack_unorm_2x16:
   case ir_unop_pack_unorm_4x8:
   case ir_unop_unpack_snorm_2x16:
   case ir_unop_unpack_snorm_4x8:
   case ir_unop_unpack_unorm_2x16:
   case ir_unop_unpack_unorm_4x8:
      assert(!"not reached: should be handled by lower_packing_builtins");
      break;
   case ir_unop_unpack_half_2x16_split_x:
   case ir_unop_unpack_half_2x16_split_y:
   case ir_binop_pack_half_2x16_split:
      assert(!"not reached: should not occur in vertex shader");
      break;
   }
}


void
vec4_visitor::visit(ir_swizzle *ir)
{
   src_reg src;
   int i = 0;
   int swizzle[4];

   /* Note that this is only swizzles in expressions, not those on the left
    * hand side of an assignment, which do write masking.  See ir_assignment
    * for that.
    */

   ir->val->accept(this);
   src = this->result;
   assert(src.file != BAD_FILE);

   for (i = 0; i < ir->type->vector_elements; i++) {
      switch (i) {
      case 0:
	 swizzle[i] = BRW_GET_SWZ(src.swizzle, ir->mask.x);
	 break;
      case 1:
	 swizzle[i] = BRW_GET_SWZ(src.swizzle, ir->mask.y);
	 break;
      case 2:
	 swizzle[i] = BRW_GET_SWZ(src.swizzle, ir->mask.z);
	 break;
      case 3:
	 swizzle[i] = BRW_GET_SWZ(src.swizzle, ir->mask.w);
	    break;
      }
   }
   for (; i < 4; i++) {
      /* Replicate the last channel out. */
      swizzle[i] = swizzle[ir->type->vector_elements - 1];
   }

   src.swizzle = BRW_SWIZZLE4(swizzle[0], swizzle[1], swizzle[2], swizzle[3]);

   this->result = src;
}

void
vec4_visitor::visit(ir_dereference_variable *ir)
{
   const struct glsl_type *type = ir->type;
   dst_reg *reg = variable_storage(ir->var);

   if (!reg) {
      fail("Failed to find variable storage for %s\n", ir->var->name);
      this->result = src_reg(brw_null_reg());
      return;
   }

   this->result = src_reg(*reg);

   /* System values get their swizzle from the dst_reg writemask */
   if (ir->var->mode == ir_var_system_value)
      return;

   if (type->is_scalar() || type->is_vector() || type->is_matrix())
      this->result.swizzle = swizzle_for_size(type->vector_elements);
}


int
vec4_visitor::compute_array_stride(ir_dereference_array *ir)
{
   /* Under normal circumstances array elements are stored consecutively, so
    * the stride is equal to the size of the array element.
    */
   return type_size(ir->type);
}


void
vec4_visitor::visit(ir_dereference_array *ir)
{
   ir_constant *constant_index;
   src_reg src;
   int array_stride = compute_array_stride(ir);

   constant_index = ir->array_index->constant_expression_value();

   ir->array->accept(this);
   src = this->result;

   if (constant_index) {
      src.reg_offset += constant_index->value.i[0] * array_stride;
   } else {
      /* Variable index array dereference.  It eats the "vec4" of the
       * base of the array and an index that offsets the Mesa register
       * index.
       */
      ir->array_index->accept(this);

      src_reg index_reg;

      if (array_stride == 1) {
	 index_reg = this->result;
      } else {
	 index_reg = src_reg(this, glsl_type::int_type);

	 emit(MUL(dst_reg(index_reg), this->result, src_reg(array_stride)));
      }

      if (src.reladdr) {
	 src_reg temp = src_reg(this, glsl_type::int_type);

	 emit(ADD(dst_reg(temp), *src.reladdr, index_reg));

	 index_reg = temp;
      }

      src.reladdr = ralloc(mem_ctx, src_reg);
      memcpy(src.reladdr, &index_reg, sizeof(index_reg));
   }

   /* If the type is smaller than a vec4, replicate the last channel out. */
   if (ir->type->is_scalar() || ir->type->is_vector() || ir->type->is_matrix())
      src.swizzle = swizzle_for_size(ir->type->vector_elements);
   else
      src.swizzle = BRW_SWIZZLE_NOOP;
   src.type = brw_type_for_base_type(ir->type);

   this->result = src;
}

void
vec4_visitor::visit(ir_dereference_record *ir)
{
   unsigned int i;
   const glsl_type *struct_type = ir->record->type;
   int offset = 0;

   ir->record->accept(this);

   for (i = 0; i < struct_type->length; i++) {
      if (strcmp(struct_type->fields.structure[i].name, ir->field) == 0)
	 break;
      offset += type_size(struct_type->fields.structure[i].type);
   }

   /* If the type is smaller than a vec4, replicate the last channel out. */
   if (ir->type->is_scalar() || ir->type->is_vector() || ir->type->is_matrix())
      this->result.swizzle = swizzle_for_size(ir->type->vector_elements);
   else
      this->result.swizzle = BRW_SWIZZLE_NOOP;
   this->result.type = brw_type_for_base_type(ir->type);

   this->result.reg_offset += offset;
}

/**
 * We want to be careful in assignment setup to hit the actual storage
 * instead of potentially using a temporary like we might with the
 * ir_dereference handler.
 */
static dst_reg
get_assignment_lhs(ir_dereference *ir, vec4_visitor *v)
{
   /* The LHS must be a dereference.  If the LHS is a variable indexed array
    * access of a vector, it must be separated into a series conditional moves
    * before reaching this point (see ir_vec_index_to_cond_assign).
    */
   assert(ir->as_dereference());
   ir_dereference_array *deref_array = ir->as_dereference_array();
   if (deref_array) {
      assert(!deref_array->array->type->is_vector());
   }

   /* Use the rvalue deref handler for the most part.  We'll ignore
    * swizzles in it and write swizzles using writemask, though.
    */
   ir->accept(v);
   return dst_reg(v->result);
}

void
vec4_visitor::emit_block_move(dst_reg *dst, src_reg *src,
			      const struct glsl_type *type, uint32_t predicate)
{
   if (type->base_type == GLSL_TYPE_STRUCT) {
      for (unsigned int i = 0; i < type->length; i++) {
	 emit_block_move(dst, src, type->fields.structure[i].type, predicate);
      }
      return;
   }

   if (type->is_array()) {
      for (unsigned int i = 0; i < type->length; i++) {
	 emit_block_move(dst, src, type->fields.array, predicate);
      }
      return;
   }

   if (type->is_matrix()) {
      const struct glsl_type *vec_type;

      vec_type = glsl_type::get_instance(GLSL_TYPE_FLOAT,
					 type->vector_elements, 1);

      for (int i = 0; i < type->matrix_columns; i++) {
	 emit_block_move(dst, src, vec_type, predicate);
      }
      return;
   }

   assert(type->is_scalar() || type->is_vector());

   dst->type = brw_type_for_base_type(type);
   src->type = dst->type;

   dst->writemask = (1 << type->vector_elements) - 1;

   src->swizzle = swizzle_for_size(type->vector_elements);

   vec4_instruction *inst = emit(MOV(*dst, *src));
   inst->predicate = predicate;

   dst->reg_offset++;
   src->reg_offset++;
}


/* If the RHS processing resulted in an instruction generating a
 * temporary value, and it would be easy to rewrite the instruction to
 * generate its result right into the LHS instead, do so.  This ends
 * up reliably removing instructions where it can be tricky to do so
 * later without real UD chain information.
 */
bool
vec4_visitor::try_rewrite_rhs_to_dst(ir_assignment *ir,
				     dst_reg dst,
				     src_reg src,
				     vec4_instruction *pre_rhs_inst,
				     vec4_instruction *last_rhs_inst)
{
   /* This could be supported, but it would take more smarts. */
   if (ir->condition)
      return false;

   if (pre_rhs_inst == last_rhs_inst)
      return false; /* No instructions generated to work with. */

   /* Make sure the last instruction generated our source reg. */
   if (src.file != GRF ||
       src.file != last_rhs_inst->dst.file ||
       src.reg != last_rhs_inst->dst.reg ||
       src.reg_offset != last_rhs_inst->dst.reg_offset ||
       src.reladdr ||
       src.abs ||
       src.negate ||
       last_rhs_inst->predicate != BRW_PREDICATE_NONE)
      return false;

   /* Check that that last instruction fully initialized the channels
    * we want to use, in the order we want to use them.  We could
    * potentially reswizzle the operands of many instructions so that
    * we could handle out of order channels, but don't yet.
    */

   for (unsigned i = 0; i < 4; i++) {
      if (dst.writemask & (1 << i)) {
	 if (!(last_rhs_inst->dst.writemask & (1 << i)))
	    return false;

	 if (BRW_GET_SWZ(src.swizzle, i) != i)
	    return false;
      }
   }

   /* Success!  Rewrite the instruction. */
   last_rhs_inst->dst.file = dst.file;
   last_rhs_inst->dst.reg = dst.reg;
   last_rhs_inst->dst.reg_offset = dst.reg_offset;
   last_rhs_inst->dst.reladdr = dst.reladdr;
   last_rhs_inst->dst.writemask &= dst.writemask;

   return true;
}

void
vec4_visitor::visit(ir_assignment *ir)
{
   dst_reg dst = get_assignment_lhs(ir->lhs, this);
   uint32_t predicate = BRW_PREDICATE_NONE;

   if (!ir->lhs->type->is_scalar() &&
       !ir->lhs->type->is_vector()) {
      ir->rhs->accept(this);
      src_reg src = this->result;

      if (ir->condition) {
	 emit_bool_to_cond_code(ir->condition, &predicate);
      }

      /* emit_block_move doesn't account for swizzles in the source register.
       * This should be ok, since the source register is a structure or an
       * array, and those can't be swizzled.  But double-check to be sure.
       */
      assert(src.swizzle ==
             (ir->rhs->type->is_matrix()
              ? swizzle_for_size(ir->rhs->type->vector_elements)
              : BRW_SWIZZLE_NOOP));

      emit_block_move(&dst, &src, ir->rhs->type, predicate);
      return;
   }

   /* Now we're down to just a scalar/vector with writemasks. */
   int i;

   vec4_instruction *pre_rhs_inst, *last_rhs_inst;
   pre_rhs_inst = (vec4_instruction *)this->instructions.get_tail();

   ir->rhs->accept(this);

   last_rhs_inst = (vec4_instruction *)this->instructions.get_tail();

   src_reg src = this->result;

   int swizzles[4];
   int first_enabled_chan = 0;
   int src_chan = 0;

   assert(ir->lhs->type->is_vector() ||
	  ir->lhs->type->is_scalar());
   dst.writemask = ir->write_mask;

   for (int i = 0; i < 4; i++) {
      if (dst.writemask & (1 << i)) {
	 first_enabled_chan = BRW_GET_SWZ(src.swizzle, i);
	 break;
      }
   }

   /* Swizzle a small RHS vector into the channels being written.
    *
    * glsl ir treats write_mask as dictating how many channels are
    * present on the RHS while in our instructions we need to make
    * those channels appear in the slots of the vec4 they're written to.
    */
   for (int i = 0; i < 4; i++) {
      if (dst.writemask & (1 << i))
	 swizzles[i] = BRW_GET_SWZ(src.swizzle, src_chan++);
      else
	 swizzles[i] = first_enabled_chan;
   }
   src.swizzle = BRW_SWIZZLE4(swizzles[0], swizzles[1],
			      swizzles[2], swizzles[3]);

   if (try_rewrite_rhs_to_dst(ir, dst, src, pre_rhs_inst, last_rhs_inst)) {
      return;
   }

   if (ir->condition) {
      emit_bool_to_cond_code(ir->condition, &predicate);
   }

   for (i = 0; i < type_size(ir->lhs->type); i++) {
      vec4_instruction *inst = emit(MOV(dst, src));
      inst->predicate = predicate;

      dst.reg_offset++;
      src.reg_offset++;
   }
}

void
vec4_visitor::emit_constant_values(dst_reg *dst, ir_constant *ir)
{
   if (ir->type->base_type == GLSL_TYPE_STRUCT) {
      foreach_list(node, &ir->components) {
	 ir_constant *field_value = (ir_constant *)node;

	 emit_constant_values(dst, field_value);
      }
      return;
   }

   if (ir->type->is_array()) {
      for (unsigned int i = 0; i < ir->type->length; i++) {
	 emit_constant_values(dst, ir->array_elements[i]);
      }
      return;
   }

   if (ir->type->is_matrix()) {
      for (int i = 0; i < ir->type->matrix_columns; i++) {
	 float *vec = &ir->value.f[i * ir->type->vector_elements];

	 for (int j = 0; j < ir->type->vector_elements; j++) {
	    dst->writemask = 1 << j;
	    dst->type = BRW_REGISTER_TYPE_F;

	    emit(MOV(*dst, src_reg(vec[j])));
	 }
	 dst->reg_offset++;
      }
      return;
   }

   int remaining_writemask = (1 << ir->type->vector_elements) - 1;

   for (int i = 0; i < ir->type->vector_elements; i++) {
      if (!(remaining_writemask & (1 << i)))
	 continue;

      dst->writemask = 1 << i;
      dst->type = brw_type_for_base_type(ir->type);

      /* Find other components that match the one we're about to
       * write.  Emits fewer instructions for things like vec4(0.5,
       * 1.5, 1.5, 1.5).
       */
      for (int j = i + 1; j < ir->type->vector_elements; j++) {
	 if (ir->type->base_type == GLSL_TYPE_BOOL) {
	    if (ir->value.b[i] == ir->value.b[j])
	       dst->writemask |= (1 << j);
	 } else {
	    /* u, i, and f storage all line up, so no need for a
	     * switch case for comparing each type.
	     */
	    if (ir->value.u[i] == ir->value.u[j])
	       dst->writemask |= (1 << j);
	 }
      }

      switch (ir->type->base_type) {
      case GLSL_TYPE_FLOAT:
	 emit(MOV(*dst, src_reg(ir->value.f[i])));
	 break;
      case GLSL_TYPE_INT:
	 emit(MOV(*dst, src_reg(ir->value.i[i])));
	 break;
      case GLSL_TYPE_UINT:
	 emit(MOV(*dst, src_reg(ir->value.u[i])));
	 break;
      case GLSL_TYPE_BOOL:
	 emit(MOV(*dst, src_reg(ir->value.b[i])));
	 break;
      default:
	 assert(!"Non-float/uint/int/bool constant");
	 break;
      }

      remaining_writemask &= ~dst->writemask;
   }
   dst->reg_offset++;
}

void
vec4_visitor::visit(ir_constant *ir)
{
   dst_reg dst = dst_reg(this, ir->type);
   this->result = src_reg(dst);

   emit_constant_values(&dst, ir);
}

void
vec4_visitor::visit(ir_call *ir)
{
   assert(!"not reached");
}

void
vec4_visitor::visit(ir_texture *ir)
{
   int sampler =
      _mesa_get_sampler_uniform_value(ir->sampler, shader_prog, prog);

   /* Should be lowered by do_lower_texture_projection */
   assert(!ir->projector);

   /* Generate code to compute all the subexpression trees.  This has to be
    * done before loading any values into MRFs for the sampler message since
    * generating these values may involve SEND messages that need the MRFs.
    */
   src_reg coordinate;
   if (ir->coordinate) {
      ir->coordinate->accept(this);
      coordinate = this->result;
   }

   src_reg shadow_comparitor;
   if (ir->shadow_comparitor) {
      ir->shadow_comparitor->accept(this);
      shadow_comparitor = this->result;
   }

   const glsl_type *lod_type = NULL, *sample_index_type = NULL;
   src_reg lod, dPdx, dPdy, sample_index;
   switch (ir->op) {
   case ir_tex:
      lod = src_reg(0.0f);
      lod_type = glsl_type::float_type;
      break;
   case ir_txf:
   case ir_txl:
   case ir_txs:
      ir->lod_info.lod->accept(this);
      lod = this->result;
      lod_type = ir->lod_info.lod->type;
      break;
   case ir_txf_ms:
      ir->lod_info.sample_index->accept(this);
      sample_index = this->result;
      sample_index_type = ir->lod_info.sample_index->type;
      break;
   case ir_txd:
      ir->lod_info.grad.dPdx->accept(this);
      dPdx = this->result;

      ir->lod_info.grad.dPdy->accept(this);
      dPdy = this->result;

      lod_type = ir->lod_info.grad.dPdx->type;
      break;
   case ir_txb:
   case ir_lod:
      break;
   }

   vec4_instruction *inst = NULL;
   switch (ir->op) {
   case ir_tex:
   case ir_txl:
      inst = new(mem_ctx) vec4_instruction(this, SHADER_OPCODE_TXL);
      break;
   case ir_txd:
      inst = new(mem_ctx) vec4_instruction(this, SHADER_OPCODE_TXD);
      break;
   case ir_txf:
      inst = new(mem_ctx) vec4_instruction(this, SHADER_OPCODE_TXF);
      break;
   case ir_txf_ms:
      inst = new(mem_ctx) vec4_instruction(this, SHADER_OPCODE_TXF_MS);
      break;
   case ir_txs:
      inst = new(mem_ctx) vec4_instruction(this, SHADER_OPCODE_TXS);
      break;
   case ir_txb:
      assert(!"TXB is not valid for vertex shaders.");
      break;
   case ir_lod:
      assert(!"LOD is not valid for vertex shaders.");
      break;
   }

   bool use_texture_offset = ir->offset != NULL && ir->op != ir_txf;

   /* Texel offsets go in the message header; Gen4 also requires headers. */
   inst->header_present = use_texture_offset || brw->gen < 5;
   inst->base_mrf = 2;
   inst->mlen = inst->header_present + 1; /* always at least one */
   inst->sampler = sampler;
   inst->dst = dst_reg(this, ir->type);
   inst->dst.writemask = WRITEMASK_XYZW;
   inst->shadow_compare = ir->shadow_comparitor != NULL;

   if (use_texture_offset)
      inst->texture_offset = brw_texture_offset(ir->offset->as_constant());

   /* MRF for the first parameter */
   int param_base = inst->base_mrf + inst->header_present;

   if (ir->op == ir_txs) {
      int writemask = brw->gen == 4 ? WRITEMASK_W : WRITEMASK_X;
      emit(MOV(dst_reg(MRF, param_base, lod_type, writemask), lod));
   } else {
      int i, coord_mask = 0, zero_mask = 0;
      /* Load the coordinate */
      /* FINISHME: gl_clamp_mask and saturate */
      for (i = 0; i < ir->coordinate->type->vector_elements; i++)
	 coord_mask |= (1 << i);
      for (; i < 4; i++)
	 zero_mask |= (1 << i);

      if (ir->offset && ir->op == ir_txf) {
	 /* It appears that the ld instruction used for txf does its
	  * address bounds check before adding in the offset.  To work
	  * around this, just add the integer offset to the integer
	  * texel coordinate, and don't put the offset in the header.
	  */
	 ir_constant *offset = ir->offset->as_constant();
	 assert(offset);

	 for (int j = 0; j < ir->coordinate->type->vector_elements; j++) {
	    src_reg src = coordinate;
	    src.swizzle = BRW_SWIZZLE4(BRW_GET_SWZ(src.swizzle, j),
				       BRW_GET_SWZ(src.swizzle, j),
				       BRW_GET_SWZ(src.swizzle, j),
				       BRW_GET_SWZ(src.swizzle, j));
	    emit(ADD(dst_reg(MRF, param_base, ir->coordinate->type, 1 << j),
		     src, offset->value.i[j]));
	 }
      } else {
	 emit(MOV(dst_reg(MRF, param_base, ir->coordinate->type, coord_mask),
		  coordinate));
      }
      emit(MOV(dst_reg(MRF, param_base, ir->coordinate->type, zero_mask),
	       src_reg(0)));
      /* Load the shadow comparitor */
      if (ir->shadow_comparitor && ir->op != ir_txd) {
	 emit(MOV(dst_reg(MRF, param_base + 1, ir->shadow_comparitor->type,
			  WRITEMASK_X),
		  shadow_comparitor));
	 inst->mlen++;
      }

      /* Load the LOD info */
      if (ir->op == ir_tex || ir->op == ir_txl) {
	 int mrf, writemask;
	 if (brw->gen >= 5) {
	    mrf = param_base + 1;
	    if (ir->shadow_comparitor) {
	       writemask = WRITEMASK_Y;
	       /* mlen already incremented */
	    } else {
	       writemask = WRITEMASK_X;
	       inst->mlen++;
	    }
	 } else /* brw->gen == 4 */ {
	    mrf = param_base;
	    writemask = WRITEMASK_W;
	 }
	 emit(MOV(dst_reg(MRF, mrf, lod_type, writemask), lod));
      } else if (ir->op == ir_txf) {
         emit(MOV(dst_reg(MRF, param_base, lod_type, WRITEMASK_W), lod));
      } else if (ir->op == ir_txf_ms) {
         emit(MOV(dst_reg(MRF, param_base + 1, sample_index_type, WRITEMASK_X),
                  sample_index));
         inst->mlen++;

         /* on Gen7, there is an additional MCS parameter here after SI,
          * but we don't bother to emit it since it's always zero. If
          * we start supporting texturing from CMS surfaces, this will have
          * to change
          */
      } else if (ir->op == ir_txd) {
	 const glsl_type *type = lod_type;

	 if (brw->gen >= 5) {
	    dPdx.swizzle = BRW_SWIZZLE4(SWIZZLE_X,SWIZZLE_X,SWIZZLE_Y,SWIZZLE_Y);
	    dPdy.swizzle = BRW_SWIZZLE4(SWIZZLE_X,SWIZZLE_X,SWIZZLE_Y,SWIZZLE_Y);
	    emit(MOV(dst_reg(MRF, param_base + 1, type, WRITEMASK_XZ), dPdx));
	    emit(MOV(dst_reg(MRF, param_base + 1, type, WRITEMASK_YW), dPdy));
	    inst->mlen++;

	    if (ir->type->vector_elements == 3 || ir->shadow_comparitor) {
	       dPdx.swizzle = BRW_SWIZZLE_ZZZZ;
	       dPdy.swizzle = BRW_SWIZZLE_ZZZZ;
	       emit(MOV(dst_reg(MRF, param_base + 2, type, WRITEMASK_X), dPdx));
	       emit(MOV(dst_reg(MRF, param_base + 2, type, WRITEMASK_Y), dPdy));
	       inst->mlen++;

               if (ir->shadow_comparitor) {
                  emit(MOV(dst_reg(MRF, param_base + 2,
                                   ir->shadow_comparitor->type, WRITEMASK_Z),
                           shadow_comparitor));
               }
	    }
	 } else /* brw->gen == 4 */ {
	    emit(MOV(dst_reg(MRF, param_base + 1, type, WRITEMASK_XYZ), dPdx));
	    emit(MOV(dst_reg(MRF, param_base + 2, type, WRITEMASK_XYZ), dPdy));
	    inst->mlen += 2;
	 }
      }
   }

   emit(inst);

   /* fixup num layers (z) for cube arrays: hardware returns faces * layers;
    * spec requires layers.
    */
   if (ir->op == ir_txs) {
      glsl_type const *type = ir->sampler->type;
      if (type->sampler_dimensionality == GLSL_SAMPLER_DIM_CUBE &&
          type->sampler_array) {
         emit_math(SHADER_OPCODE_INT_QUOTIENT,
                   with_writemask(inst->dst, WRITEMASK_Z),
                   src_reg(inst->dst), src_reg(6));
      }
   }

   swizzle_result(ir, src_reg(inst->dst), sampler);
}

void
vec4_visitor::swizzle_result(ir_texture *ir, src_reg orig_val, int sampler)
{
   int s = key->tex.swizzles[sampler];

   this->result = src_reg(this, ir->type);
   dst_reg swizzled_result(this->result);

   if (ir->op == ir_txs || ir->type == glsl_type::float_type
			|| s == SWIZZLE_NOOP) {
      emit(MOV(swizzled_result, orig_val));
      return;
   }

   int zero_mask = 0, one_mask = 0, copy_mask = 0;
   int swizzle[4] = {0};

   for (int i = 0; i < 4; i++) {
      switch (GET_SWZ(s, i)) {
      case SWIZZLE_ZERO:
	 zero_mask |= (1 << i);
	 break;
      case SWIZZLE_ONE:
	 one_mask |= (1 << i);
	 break;
      default:
	 copy_mask |= (1 << i);
	 swizzle[i] = GET_SWZ(s, i);
	 break;
      }
   }

   if (copy_mask) {
      orig_val.swizzle = BRW_SWIZZLE4(swizzle[0], swizzle[1], swizzle[2], swizzle[3]);
      swizzled_result.writemask = copy_mask;
      emit(MOV(swizzled_result, orig_val));
   }

   if (zero_mask) {
      swizzled_result.writemask = zero_mask;
      emit(MOV(swizzled_result, src_reg(0.0f)));
   }

   if (one_mask) {
      swizzled_result.writemask = one_mask;
      emit(MOV(swizzled_result, src_reg(1.0f)));
   }
}

void
vec4_visitor::visit(ir_return *ir)
{
   assert(!"not reached");
}

void
vec4_visitor::visit(ir_discard *ir)
{
   assert(!"not reached");
}

void
vec4_visitor::visit(ir_if *ir)
{
   /* Don't point the annotation at the if statement, because then it plus
    * the then and else blocks get printed.
    */
   this->base_ir = ir->condition;

   if (brw->gen == 6) {
      emit_if_gen6(ir);
   } else {
      uint32_t predicate;
      emit_bool_to_cond_code(ir->condition, &predicate);
      emit(IF(predicate));
   }

   visit_instructions(&ir->then_instructions);

   if (!ir->else_instructions.is_empty()) {
      this->base_ir = ir->condition;
      emit(BRW_OPCODE_ELSE);

      visit_instructions(&ir->else_instructions);
   }

   this->base_ir = ir->condition;
   emit(BRW_OPCODE_ENDIF);
}

void
vec4_visitor::emit_ndc_computation()
{
   /* Get the position */
   src_reg pos = src_reg(output_reg[VARYING_SLOT_POS]);

   /* Build ndc coords, which are (x/w, y/w, z/w, 1/w) */
   dst_reg ndc = dst_reg(this, glsl_type::vec4_type);
   output_reg[BRW_VARYING_SLOT_NDC] = ndc;

   current_annotation = "NDC";
   dst_reg ndc_w = ndc;
   ndc_w.writemask = WRITEMASK_W;
   src_reg pos_w = pos;
   pos_w.swizzle = BRW_SWIZZLE4(SWIZZLE_W, SWIZZLE_W, SWIZZLE_W, SWIZZLE_W);
   emit_math(SHADER_OPCODE_RCP, ndc_w, pos_w);

   dst_reg ndc_xyz = ndc;
   ndc_xyz.writemask = WRITEMASK_XYZ;

   emit(MUL(ndc_xyz, pos, src_reg(ndc_w)));
}

void
vec4_visitor::emit_psiz_and_flags(struct brw_reg reg)
{
   if (brw->gen < 6 &&
       ((prog_data->vue_map.slots_valid & VARYING_BIT_PSIZ) ||
        key->userclip_active || brw->has_negative_rhw_bug)) {
      dst_reg header1 = dst_reg(this, glsl_type::uvec4_type);
      dst_reg header1_w = header1;
      header1_w.writemask = WRITEMASK_W;
      GLuint i;

      emit(MOV(header1, 0u));

      if (prog_data->vue_map.slots_valid & VARYING_BIT_PSIZ) {
	 src_reg psiz = src_reg(output_reg[VARYING_SLOT_PSIZ]);

	 current_annotation = "Point size";
	 emit(MUL(header1_w, psiz, src_reg((float)(1 << 11))));
	 emit(AND(header1_w, src_reg(header1_w), 0x7ff << 8));
      }

      current_annotation = "Clipping flags";
      for (i = 0; i < key->nr_userclip_plane_consts; i++) {
	 vec4_instruction *inst;
         gl_varying_slot slot = (prog_data->vue_map.slots_valid & VARYING_BIT_CLIP_VERTEX)
            ? VARYING_SLOT_CLIP_VERTEX : VARYING_SLOT_POS;

	 inst = emit(DP4(dst_null_f(), src_reg(output_reg[slot]),
                         src_reg(this->userplane[i])));
	 inst->conditional_mod = BRW_CONDITIONAL_L;

	 inst = emit(OR(header1_w, src_reg(header1_w), 1u << i));
	 inst->predicate = BRW_PREDICATE_NORMAL;
      }

      /* i965 clipping workaround:
       * 1) Test for -ve rhw
       * 2) If set,
       *      set ndc = (0,0,0,0)
       *      set ucp[6] = 1
       *
       * Later, clipping will detect ucp[6] and ensure the primitive is
       * clipped against all fixed planes.
       */
      if (brw->has_negative_rhw_bug) {
         src_reg ndc_w = src_reg(output_reg[BRW_VARYING_SLOT_NDC]);
         ndc_w.swizzle = BRW_SWIZZLE_WWWW;
         emit(CMP(dst_null_f(), ndc_w, src_reg(0.0f), BRW_CONDITIONAL_L));
         vec4_instruction *inst;
         inst = emit(OR(header1_w, src_reg(header1_w), src_reg(1u << 6)));
         inst->predicate = BRW_PREDICATE_NORMAL;
         inst = emit(MOV(output_reg[BRW_VARYING_SLOT_NDC], src_reg(0.0f)));
         inst->predicate = BRW_PREDICATE_NORMAL;
      }

      emit(MOV(retype(reg, BRW_REGISTER_TYPE_UD), src_reg(header1)));
   } else if (brw->gen < 6) {
      emit(MOV(retype(reg, BRW_REGISTER_TYPE_UD), 0u));
   } else {
      emit(MOV(retype(reg, BRW_REGISTER_TYPE_D), src_reg(0)));
      if (prog_data->vue_map.slots_valid & VARYING_BIT_PSIZ) {
         emit(MOV(brw_writemask(reg, WRITEMASK_W),
                  src_reg(output_reg[VARYING_SLOT_PSIZ])));
      }
      if (prog_data->vue_map.slots_valid & VARYING_BIT_LAYER) {
         emit(MOV(retype(brw_writemask(reg, WRITEMASK_Y), BRW_REGISTER_TYPE_D),
                  src_reg(output_reg[VARYING_SLOT_LAYER])));
      }
   }
}

void
vec4_visitor::emit_clip_distances(struct brw_reg reg, int offset)
{
   if (brw->gen < 6) {
      /* Clip distance slots are set aside in gen5, but they are not used.  It
       * is not clear whether we actually need to set aside space for them,
       * but the performance cost is negligible.
       */
      return;
   }

   /* From the GLSL 1.30 spec, section 7.1 (Vertex Shader Special Variables):
    *
    *     "If a linked set of shaders forming the vertex stage contains no
    *     static write to gl_ClipVertex or gl_ClipDistance, but the
    *     application has requested clipping against user clip planes through
    *     the API, then the coordinate written to gl_Position is used for
    *     comparison against the user clip planes."
    *
    * This function is only called if the shader didn't write to
    * gl_ClipDistance.  Accordingly, we use gl_ClipVertex to perform clipping
    * if the user wrote to it; otherwise we use gl_Position.
    */
   gl_varying_slot clip_vertex = VARYING_SLOT_CLIP_VERTEX;
   if (!(prog_data->vue_map.slots_valid & VARYING_BIT_CLIP_VERTEX)) {
      clip_vertex = VARYING_SLOT_POS;
   }

   for (int i = 0; i + offset < key->nr_userclip_plane_consts && i < 4;
        ++i) {
      emit(DP4(dst_reg(brw_writemask(reg, 1 << i)),
               src_reg(output_reg[clip_vertex]),
               src_reg(this->userplane[i + offset])));
   }
}

void
vec4_visitor::emit_generic_urb_slot(dst_reg reg, int varying)
{
   assert (varying < VARYING_SLOT_MAX);
   reg.type = output_reg[varying].type;
   current_annotation = output_reg_annotation[varying];
   /* Copy the register, saturating if necessary */
   vec4_instruction *inst = emit(MOV(reg,
                                     src_reg(output_reg[varying])));
   if ((varying == VARYING_SLOT_COL0 ||
        varying == VARYING_SLOT_COL1 ||
        varying == VARYING_SLOT_BFC0 ||
        varying == VARYING_SLOT_BFC1) &&
       key->clamp_vertex_color) {
      inst->saturate = true;
   }
}

void
vec4_visitor::emit_urb_slot(int mrf, int varying)
{
   struct brw_reg hw_reg = brw_message_reg(mrf);
   dst_reg reg = dst_reg(MRF, mrf);
   reg.type = BRW_REGISTER_TYPE_F;

   switch (varying) {
   case VARYING_SLOT_PSIZ:
      /* PSIZ is always in slot 0, and is coupled with other flags. */
      current_annotation = "indices, point width, clip flags";
      emit_psiz_and_flags(hw_reg);
      break;
   case BRW_VARYING_SLOT_NDC:
      current_annotation = "NDC";
      emit(MOV(reg, src_reg(output_reg[BRW_VARYING_SLOT_NDC])));
      break;
   case VARYING_SLOT_POS:
      current_annotation = "gl_Position";
      emit(MOV(reg, src_reg(output_reg[VARYING_SLOT_POS])));
      break;
   case VARYING_SLOT_CLIP_DIST0:
   case VARYING_SLOT_CLIP_DIST1:
      if (this->key->uses_clip_distance) {
         emit_generic_urb_slot(reg, varying);
      } else {
         current_annotation = "user clip distances";
         emit_clip_distances(hw_reg, (varying - VARYING_SLOT_CLIP_DIST0) * 4);
      }
      break;
   case VARYING_SLOT_EDGE:
      /* This is present when doing unfilled polygons.  We're supposed to copy
       * the edge flag from the user-provided vertex array
       * (glEdgeFlagPointer), or otherwise we'll copy from the current value
       * of that attribute (starts as 1.0f).  This is then used in clipping to
       * determine which edges should be drawn as wireframe.
       */
      current_annotation = "edge flag";
      emit(MOV(reg, src_reg(dst_reg(ATTR, VERT_ATTRIB_EDGEFLAG,
                                    glsl_type::float_type, WRITEMASK_XYZW))));
      break;
   case BRW_VARYING_SLOT_PAD:
      /* No need to write to this slot */
      break;
   default:
      emit_generic_urb_slot(reg, varying);
      break;
   }
}

static int
align_interleaved_urb_mlen(struct brw_context *brw, int mlen)
{
   if (brw->gen >= 6) {
      /* URB data written (does not include the message header reg) must
       * be a multiple of 256 bits, or 2 VS registers.  See vol5c.5,
       * section 5.4.3.2.2: URB_INTERLEAVED.
       *
       * URB entries are allocated on a multiple of 1024 bits, so an
       * extra 128 bits written here to make the end align to 256 is
       * no problem.
       */
      if ((mlen % 2) != 1)
	 mlen++;
   }

   return mlen;
}

void
vec4_vs_visitor::emit_urb_write_header(int mrf)
{
   /* No need to do anything for VS; an implied write to this MRF will be
    * performed by VS_OPCODE_URB_WRITE.
    */
   (void) mrf;
}

vec4_instruction *
vec4_vs_visitor::emit_urb_write_opcode(bool complete)
{
   /* For VS, the URB writes end the thread. */
   if (complete) {
      if (INTEL_DEBUG & DEBUG_SHADER_TIME)
         emit_shader_time_end();
   }

   vec4_instruction *inst = emit(VS_OPCODE_URB_WRITE);
   inst->eot = complete;

   return inst;
}

/**
 * Generates the VUE payload plus the necessary URB write instructions to
 * output it.
 *
 * The VUE layout is documented in Volume 2a.
 */
void
vec4_visitor::emit_vertex()
{
   /* MRF 0 is reserved for the debugger, so start with message header
    * in MRF 1.
    */
   int base_mrf = 1;
   int mrf = base_mrf;
   /* In the process of generating our URB write message contents, we
    * may need to unspill a register or load from an array.  Those
    * reads would use MRFs 14-15.
    */
   int max_usable_mrf = 13;

   /* The following assertion verifies that max_usable_mrf causes an
    * even-numbered amount of URB write data, which will meet gen6's
    * requirements for length alignment.
    */
   assert ((max_usable_mrf - base_mrf) % 2 == 0);

   /* First mrf is the g0-based message header containing URB handles and
    * such.
    */
   emit_urb_write_header(mrf++);

   if (brw->gen < 6) {
      emit_ndc_computation();
   }

   /* Set up the VUE data for the first URB write */
   int slot;
   for (slot = 0; slot < prog_data->vue_map.num_slots; ++slot) {
      emit_urb_slot(mrf++, prog_data->vue_map.slot_to_varying[slot]);

      /* If this was max_usable_mrf, we can't fit anything more into this URB
       * WRITE.
       */
      if (mrf > max_usable_mrf) {
	 slot++;
	 break;
      }
   }

   bool complete = slot >= prog_data->vue_map.num_slots;
   current_annotation = "URB write";
   vec4_instruction *inst = emit_urb_write_opcode(complete);
   inst->base_mrf = base_mrf;
   inst->mlen = align_interleaved_urb_mlen(brw, mrf - base_mrf);

   /* Optional second URB write */
   if (!complete) {
      mrf = base_mrf + 1;

      for (; slot < prog_data->vue_map.num_slots; ++slot) {
	 assert(mrf < max_usable_mrf);

         emit_urb_slot(mrf++, prog_data->vue_map.slot_to_varying[slot]);
      }

      current_annotation = "URB write";
      inst = emit_urb_write_opcode(true /* complete */);
      inst->base_mrf = base_mrf;
      inst->mlen = align_interleaved_urb_mlen(brw, mrf - base_mrf);
      /* URB destination offset.  In the previous write, we got MRFs
       * 2-13 minus the one header MRF, so 12 regs.  URB offset is in
       * URB row increments, and each of our MRFs is half of one of
       * those, since we're doing interleaved writes.
       */
      inst->offset = (max_usable_mrf - base_mrf) / 2;
   }
}

void
vec4_vs_visitor::emit_thread_end()
{
   /* For VS, we always end the thread by emitting a single vertex.
    * emit_urb_write_opcode() will take care of setting the eot flag on the
    * SEND instruction.
    */
   emit_vertex();
}

src_reg
vec4_visitor::get_scratch_offset(vec4_instruction *inst,
				 src_reg *reladdr, int reg_offset)
{
   /* Because we store the values to scratch interleaved like our
    * vertex data, we need to scale the vec4 index by 2.
    */
   int message_header_scale = 2;

   /* Pre-gen6, the message header uses byte offsets instead of vec4
    * (16-byte) offset units.
    */
   if (brw->gen < 6)
      message_header_scale *= 16;

   if (reladdr) {
      src_reg index = src_reg(this, glsl_type::int_type);

      emit_before(inst, ADD(dst_reg(index), *reladdr, src_reg(reg_offset)));
      emit_before(inst, MUL(dst_reg(index),
			    index, src_reg(message_header_scale)));

      return index;
   } else {
      return src_reg(reg_offset * message_header_scale);
   }
}

src_reg
vec4_visitor::get_pull_constant_offset(vec4_instruction *inst,
				       src_reg *reladdr, int reg_offset)
{
   if (reladdr) {
      src_reg index = src_reg(this, glsl_type::int_type);

      emit_before(inst, ADD(dst_reg(index), *reladdr, src_reg(reg_offset)));

      /* Pre-gen6, the message header uses byte offsets instead of vec4
       * (16-byte) offset units.
       */
      if (brw->gen < 6) {
	 emit_before(inst, MUL(dst_reg(index), index, src_reg(16)));
      }

      return index;
   } else {
      int message_header_scale = brw->gen < 6 ? 16 : 1;
      return src_reg(reg_offset * message_header_scale);
   }
}

/**
 * Emits an instruction before @@inst to load the value named by @@orig_src
 * from scratch space at @@base_offset to @@temp.
 *
 * @@base_offset is measured in 32-byte units (the size of a register).
 */
void
vec4_visitor::emit_scratch_read(vec4_instruction *inst,
				dst_reg temp, src_reg orig_src,
				int base_offset)
{
   int reg_offset = base_offset + orig_src.reg_offset;
   src_reg index = get_scratch_offset(inst, orig_src.reladdr, reg_offset);

   emit_before(inst, SCRATCH_READ(temp, index));
}

/**
 * Emits an instruction after @@inst to store the value to be written
 * to @@orig_dst to scratch space at @@base_offset, from @@temp.
 *
 * @@base_offset is measured in 32-byte units (the size of a register).
 */
void
vec4_visitor::emit_scratch_write(vec4_instruction *inst, int base_offset)
{
   int reg_offset = base_offset + inst->dst.reg_offset;
   src_reg index = get_scratch_offset(inst, inst->dst.reladdr, reg_offset);

   /* Create a temporary register to store *inst's result in.
    *
    * We have to be careful in MOVing from our temporary result register in
    * the scratch write.  If we swizzle from channels of the temporary that
    * weren't initialized, it will confuse live interval analysis, which will
    * make spilling fail to make progress.
    */
   src_reg temp = src_reg(this, glsl_type::vec4_type);
   temp.type = inst->dst.type;
   int first_writemask_chan = ffs(inst->dst.writemask) - 1;
   int swizzles[4];
   for (int i = 0; i < 4; i++)
      if (inst->dst.writemask & (1 << i))
         swizzles[i] = i;
      else
         swizzles[i] = first_writemask_chan;
   temp.swizzle = BRW_SWIZZLE4(swizzles[0], swizzles[1],
                               swizzles[2], swizzles[3]);

   dst_reg dst = dst_reg(brw_writemask(brw_vec8_grf(0, 0),
				       inst->dst.writemask));
   vec4_instruction *write = SCRATCH_WRITE(dst, temp, index);
   write->predicate = inst->predicate;
   write->ir = inst->ir;
   write->annotation = inst->annotation;
   inst->insert_after(write);

   inst->dst.file = temp.file;
   inst->dst.reg = temp.reg;
   inst->dst.reg_offset = temp.reg_offset;
   inst->dst.reladdr = NULL;
}

/**
 * We can't generally support array access in GRF space, because a
 * single instruction's destination can only span 2 contiguous
 * registers.  So, we send all GRF arrays that get variable index
 * access to scratch space.
 */
void
vec4_visitor::move_grf_array_access_to_scratch()
{
   int scratch_loc[this->virtual_grf_count];

   for (int i = 0; i < this->virtual_grf_count; i++) {
      scratch_loc[i] = -1;
   }

   /* First, calculate the set of virtual GRFs that need to be punted
    * to scratch due to having any array access on them, and where in
    * scratch.
    */
   foreach_list(node, &this->instructions) {
      vec4_instruction *inst = (vec4_instruction *)node;

      if (inst->dst.file == GRF && inst->dst.reladdr &&
	  scratch_loc[inst->dst.reg] == -1) {
	 scratch_loc[inst->dst.reg] = c->last_scratch;
	 c->last_scratch += this->virtual_grf_sizes[inst->dst.reg];
      }

      for (int i = 0 ; i < 3; i++) {
	 src_reg *src = &inst->src[i];

	 if (src->file == GRF && src->reladdr &&
	     scratch_loc[src->reg] == -1) {
	    scratch_loc[src->reg] = c->last_scratch;
	    c->last_scratch += this->virtual_grf_sizes[src->reg];
	 }
      }
   }

   /* Now, for anything that will be accessed through scratch, rewrite
    * it to load/store.  Note that this is a _safe list walk, because
    * we may generate a new scratch_write instruction after the one
    * we're processing.
    */
   foreach_list_safe(node, &this->instructions) {
      vec4_instruction *inst = (vec4_instruction *)node;

      /* Set up the annotation tracking for new generated instructions. */
      base_ir = inst->ir;
      current_annotation = inst->annotation;

      if (inst->dst.file == GRF && scratch_loc[inst->dst.reg] != -1) {
	 emit_scratch_write(inst, scratch_loc[inst->dst.reg]);
      }

      for (int i = 0 ; i < 3; i++) {
	 if (inst->src[i].file != GRF || scratch_loc[inst->src[i].reg] == -1)
	    continue;

	 dst_reg temp = dst_reg(this, glsl_type::vec4_type);

	 emit_scratch_read(inst, temp, inst->src[i],
			   scratch_loc[inst->src[i].reg]);

	 inst->src[i].file = temp.file;
	 inst->src[i].reg = temp.reg;
	 inst->src[i].reg_offset = temp.reg_offset;
	 inst->src[i].reladdr = NULL;
      }
   }
}

/**
 * Emits an instruction before @@inst to load the value named by @@orig_src
 * from the pull constant buffer (surface) at @@base_offset to @@temp.
 */
void
vec4_visitor::emit_pull_constant_load(vec4_instruction *inst,
				      dst_reg temp, src_reg orig_src,
				      int base_offset)
{
   int reg_offset = base_offset + orig_src.reg_offset;
   src_reg index = src_reg((unsigned)SURF_INDEX_VERT_CONST_BUFFER);
   src_reg offset = get_pull_constant_offset(inst, orig_src.reladdr, reg_offset);
   vec4_instruction *load;

   if (brw->gen >= 7) {
      dst_reg grf_offset = dst_reg(this, glsl_type::int_type);
      grf_offset.type = offset.type;
      emit_before(inst, MOV(grf_offset, offset));

      load = new(mem_ctx) vec4_instruction(this,
                                           VS_OPCODE_PULL_CONSTANT_LOAD_GEN7,
                                           temp, index, src_reg(grf_offset));
   } else {
      load = new(mem_ctx) vec4_instruction(this, VS_OPCODE_PULL_CONSTANT_LOAD,
                                           temp, index, offset);
      load->base_mrf = 14;
      load->mlen = 1;
   }
   emit_before(inst, load);
}

/**
 * Implements array access of uniforms by inserting a
 * PULL_CONSTANT_LOAD instruction.
 *
 * Unlike temporary GRF array access (where we don't support it due to
 * the difficulty of doing relative addressing on instruction
 * destinations), we could potentially do array access of uniforms
 * that were loaded in GRF space as push constants.  In real-world
 * usage we've seen, though, the arrays being used are always larger
 * than we could load as push constants, so just always move all
 * uniform array access out to a pull constant buffer.
 */
void
vec4_visitor::move_uniform_array_access_to_pull_constants()
{
   int pull_constant_loc[this->uniforms];

   for (int i = 0; i < this->uniforms; i++) {
      pull_constant_loc[i] = -1;
   }

   /* Walk through and find array access of uniforms.  Put a copy of that
    * uniform in the pull constant buffer.
    *
    * Note that we don't move constant-indexed accesses to arrays.  No
    * testing has been done of the performance impact of this choice.
    */
   foreach_list_safe(node, &this->instructions) {
      vec4_instruction *inst = (vec4_instruction *)node;

      for (int i = 0 ; i < 3; i++) {
	 if (inst->src[i].file != UNIFORM || !inst->src[i].reladdr)
	    continue;

	 int uniform = inst->src[i].reg;

	 /* If this array isn't already present in the pull constant buffer,
	  * add it.
	  */
	 if (pull_constant_loc[uniform] == -1) {
	    const float **values = &prog_data->param[uniform * 4];

	    pull_constant_loc[uniform] = prog_data->nr_pull_params / 4;

	    for (int j = 0; j < uniform_size[uniform] * 4; j++) {
	       prog_data->pull_param[prog_data->nr_pull_params++]
                  = values[j];
	    }
	 }

	 /* Set up the annotation tracking for new generated instructions. */
	 base_ir = inst->ir;
	 current_annotation = inst->annotation;

	 dst_reg temp = dst_reg(this, glsl_type::vec4_type);

	 emit_pull_constant_load(inst, temp, inst->src[i],
				 pull_constant_loc[uniform]);

	 inst->src[i].file = temp.file;
	 inst->src[i].reg = temp.reg;
	 inst->src[i].reg_offset = temp.reg_offset;
	 inst->src[i].reladdr = NULL;
      }
   }

   /* Now there are no accesses of the UNIFORM file with a reladdr, so
    * no need to track them as larger-than-vec4 objects.  This will be
    * relied on in cutting out unused uniform vectors from push
    * constants.
    */
   split_uniform_registers();
}

void
vec4_visitor::resolve_ud_negate(src_reg *reg)
{
   if (reg->type != BRW_REGISTER_TYPE_UD ||
       !reg->negate)
      return;

   src_reg temp = src_reg(this, glsl_type::uvec4_type);
   emit(BRW_OPCODE_MOV, dst_reg(temp), *reg);
   *reg = temp;
}

vec4_visitor::vec4_visitor(struct brw_context *brw,
                           struct brw_vec4_compile *c,
                           struct gl_program *prog,
                           const struct brw_vec4_prog_key *key,
                           struct brw_vec4_prog_data *prog_data,
			   struct gl_shader_program *shader_prog,
			   struct brw_shader *shader,
			   void *mem_ctx,
                           bool debug_flag)
   : debug_flag(debug_flag)
{
   this->brw = brw;
   this->ctx = &brw->ctx;
   this->shader_prog = shader_prog;
   this->shader = shader;

   this->mem_ctx = mem_ctx;
   this->failed = false;

   this->base_ir = NULL;
   this->current_annotation = NULL;
   memset(this->output_reg_annotation, 0, sizeof(this->output_reg_annotation));

   this->c = c;
   this->prog = prog;
   this->key = key;
   this->prog_data = prog_data;

   this->variable_ht = hash_table_ctor(0,
				       hash_table_pointer_hash,
				       hash_table_pointer_compare);

   this->virtual_grf_start = NULL;
   this->virtual_grf_end = NULL;
   this->virtual_grf_sizes = NULL;
   this->virtual_grf_count = 0;
   this->virtual_grf_reg_map = NULL;
   this->virtual_grf_reg_count = 0;
   this->virtual_grf_array_size = 0;
   this->live_intervals_valid = false;

   this->max_grf = brw->gen >= 7 ? GEN7_MRF_HACK_START : BRW_MAX_GRF;

   this->uniforms = 0;
}

vec4_visitor::~vec4_visitor()
{
   hash_table_dtor(this->variable_ht);
}


vec4_vs_visitor::vec4_vs_visitor(struct brw_context *brw,
                                 struct brw_vs_compile *vs_compile,
                                 struct brw_vs_prog_data *vs_prog_data,
                                 struct gl_shader_program *prog,
                                 struct brw_shader *shader,
                                 void *mem_ctx)
   : vec4_visitor(brw, &vs_compile->base, &vs_compile->vp->program.Base,
                  &vs_compile->key.base, &vs_prog_data->base, prog, shader,
                  mem_ctx, INTEL_DEBUG & DEBUG_VS),
     vs_compile(vs_compile),
     vs_prog_data(vs_prog_data)
{
}


void
vec4_visitor::fail(const char *format, ...)
{
   va_list va;
   char *msg;

   if (failed)
      return;

   failed = true;

   va_start(va, format);
   msg = ralloc_vasprintf(mem_ctx, format, va);
   va_end(va);
   msg = ralloc_asprintf(mem_ctx, "VS compile failed: %s\n", msg);

   this->fail_msg = msg;

   if (debug_flag) {
      fprintf(stderr, "%s",  msg);
   }
}

} /* namespace brw */
@


1.1
log
@Initial revision
@
text
@@


1.1.1.1
log
@Import Mesa 9.2.0
@
text
@@


1.1.1.2
log
@Import Mesa 9.2.1
@
text
@d2406 2
a2407 4
      if (zero_mask != 0) {
         emit(MOV(dst_reg(MRF, param_base, ir->coordinate->type, zero_mask),
                  src_reg(0)));
      }
@


1.1.1.3
log
@Import Mesa 10.2.3
@
text
@d27 3
a43 10
   this->saturate = false;
   this->force_writemask_all = false;
   this->no_dd_clear = false;
   this->no_dd_check = false;
   this->writes_accumulator = false;
   this->conditional_mod = BRW_CONDITIONAL_NONE;
   this->sampler = 0;
   this->texture_offset = 0;
   this->target = 0;
   this->shadow_compare = false;
a44 5
   this->urb_write_flags = BRW_URB_WRITE_NO_FLAGS;
   this->header_present = false;
   this->mlen = 0;
   this->base_mrf = 0;
   this->offset = 0;
a88 6
vec4_visitor::emit(enum opcode opcode, dst_reg dst)
{
   return emit(new(mem_ctx) vec4_instruction(this, opcode, dst));
}

vec4_instruction *
a109 10
#define ALU2_ACC(op)							\
   vec4_instruction *							\
   vec4_visitor::op(dst_reg dst, src_reg src0, src_reg src1)		\
   {									\
      vec4_instruction *inst = new(mem_ctx) vec4_instruction(this,     \
                       BRW_OPCODE_##op, dst, src0, src1);		\
      inst->writes_accumulator = true;                                 \
      return inst;                                                     \
   }

a113 1
      assert(brw->gen >= 6);						\
d128 1
a128 1
ALU2_ACC(MACH)
a145 4
ALU3(MAD)
ALU2_ACC(ADDC)
ALU2_ACC(SUBB)
ALU2(MAC)
d159 1
a159 1
/** Gen6 IF with embedded comparison. */
d163 1
a163 1
   assert(brw->gen == 6);
d211 1
a211 1
   inst = new(mem_ctx) vec4_instruction(this, SHADER_OPCODE_GEN4_SCRATCH_READ,
d224 1
a224 1
   inst = new(mem_ctx) vec4_instruction(this, SHADER_OPCODE_GEN4_SCRATCH_WRITE,
a258 3
   if (src.file == UNIFORM && brw_is_single_value_swizzle(src.swizzle))
      return src;

d456 1
a456 1
   tmp_src.swizzle = BRW_SWIZZLE_YYYY;
d463 1
a463 1
   tmp_src.swizzle = BRW_SWIZZLE_XXXX;
a550 3
   case GLSL_TYPE_ATOMIC_UINT:
      return 0;
   case GLSL_TYPE_IMAGE:
a642 1
         assert(uniforms < uniform_array_size);
d647 1
a647 1
            stage_prog_data->param[uniforms * 4 + i] = &components->f;
d652 1
a652 1
            stage_prog_data->param[uniforms * 4 + i] = &zero;
d665 33
a697 7
   for (int i = 0; i < key->nr_userclip_plane_consts; ++i) {
      assert(this->uniforms < uniform_array_size);
      this->uniform_vector_size[this->uniforms] = 4;
      this->userplane[i] = dst_reg(UNIFORM, this->uniforms);
      this->userplane[i].type = BRW_REGISTER_TYPE_F;
      for (int j = 0; j < 4; ++j) {
         stage_prog_data->param[this->uniforms * 4 + j] = &clip_planes[i][j];
a698 1
      ++this->uniforms;
a721 1
      assert(this->uniforms < uniform_array_size);
d731 1
a731 2
	 stage_prog_data->param[this->uniforms * 4 + j] = &values[swiz];
	 assert(this->uniforms < uniform_array_size);
d935 149
d1092 1
a1092 1
   switch (ir->data.mode) {
d1094 1
a1094 1
      reg = new(mem_ctx) dst_reg(ATTR, ir->data.location);
d1101 3
a1103 3
	 output_reg[ir->data.location + i] = *reg;
	 output_reg[ir->data.location + i].reg_offset = i;
	 output_reg[ir->data.location + i].type =
d1105 1
a1105 1
	 output_reg_annotation[ir->data.location + i] = ir->name;
a1119 3
       *
       * Atomic counters take no uniform storage, no need to do
       * anything here.
d1121 1
a1121 1
      if (ir->is_in_uniform_block() || ir->type->contains_atomic())
a1126 1
      assert(this->uniforms < uniform_array_size);
d1151 2
d1158 13
d1173 11
d1186 7
d1227 1
a1227 1
      sig = ir->matching_signature(NULL, &empty);
d1254 1
a1254 1
vec4_visitor::try_emit_mad(ir_expression *ir)
d1264 2
a1265 2
   ir_rvalue *nonmul = ir->operands[1];
   ir_expression *mul = ir->operands[0]->as_expression();
d1267 2
a1268 7
   if (!mul || mul->operation != ir_binop_mul) {
      nonmul = ir->operands[0];
      mul = ir->operands[1]->as_expression();

      if (!mul || mul->operation != ir_binop_mul)
         return false;
   }
d1316 2
a1317 3
void
vec4_visitor::emit_lrp(const dst_reg &dst,
                       const src_reg &x, const src_reg &y, const src_reg &a)
d1319 9
a1327 22
   if (brw->gen >= 6) {
      /* Note that the instruction's argument order is reversed from GLSL
       * and the IR.
       */
      emit(LRP(dst,
               fix_3src_operand(a), fix_3src_operand(y), fix_3src_operand(x)));
   } else {
      /* Earlier generations don't support three source operations, so we
       * need to emit x*(1-a) + y*a.
       */
      dst_reg y_times_a           = dst_reg(this, glsl_type::vec4_type);
      dst_reg one_minus_a         = dst_reg(this, glsl_type::vec4_type);
      dst_reg x_times_one_minus_a = dst_reg(this, glsl_type::vec4_type);
      y_times_a.writemask           = dst.writemask;
      one_minus_a.writemask         = dst.writemask;
      x_times_one_minus_a.writemask = dst.writemask;

      emit(MUL(y_times_a, y, a));
      emit(ADD(one_minus_a, negate(a), src_reg(1.0f)));
      emit(MUL(x_times_one_minus_a, x, src_reg(one_minus_a)));
      emit(ADD(dst, src_reg(x_times_one_minus_a), src_reg(y_times_a)));
   }
d1343 1
a1343 1
      if (try_emit_mad(ir))
d1351 2
a1352 2
	 fprintf(stderr, "Failed to get tree for expression operand:\n");
	 ir->operands[operand]->fprint(stderr);
d1403 1
a1403 7
      if (ir->type->is_float()) {
         /* AND(val, 0x80000000) gives the sign bit.
          *
          * Predicated OR ORs 1.0 (0x3f800000) with the sign bit if val is not
          * zero.
          */
         emit(CMP(dst_null_f(), op[0], src_reg(0.0f), BRW_CONDITIONAL_NZ));
d1405 3
a1407 3
         op[0].type = BRW_REGISTER_TYPE_UD;
         result_dst.type = BRW_REGISTER_TYPE_UD;
         emit(AND(result_dst, op[0], src_reg(0x80000000u)));
d1409 3
a1411 10
         inst = emit(OR(result_dst, src_reg(result_dst), src_reg(0x3f800000u)));
         inst->predicate = BRW_PREDICATE_NORMAL;

         this->result.type = BRW_REGISTER_TYPE_F;
      } else {
         /*  ASR(val, 31) -> negative val generates 0xffffffff (signed -1).
          *               -> non-negative val generates 0x00000000.
          *  Predicated OR sets 1 if val is positive.
          */
         emit(CMP(dst_null_d(), op[0], src_reg(0), BRW_CONDITIONAL_G));
a1412 5
         emit(ASR(result_dst, op[0], src_reg(31)));

         inst = emit(OR(result_dst, src_reg(result_dst), src_reg(1)));
         inst->predicate = BRW_PREDICATE_NORMAL;
      }
d1488 1
a1488 1
      if (brw->gen < 8 && ir->type->is_integer()) {
d1495 1
a1495 1
         if (ir->operands[0]->is_uint16_constant()) {
d1500 1
a1500 1
         } else if (ir->operands[1]->is_uint16_constant()) {
d1506 1
a1506 1
            struct brw_reg acc = retype(brw_acc_reg(), result_dst.type);
a1515 7
   case ir_binop_imul_high: {
      struct brw_reg acc = retype(brw_acc_reg(), result_dst.type);

      emit(MUL(acc, op[0], op[1]));
      emit(MACH(result_dst, op[0], op[1]));
      break;
   }
a1520 14
   case ir_binop_carry: {
      struct brw_reg acc = retype(brw_acc_reg(), BRW_REGISTER_TYPE_UD);

      emit(ADDC(dst_null_ud(), op[0], op[1]));
      emit(MOV(result_dst, src_reg(acc)));
      break;
   }
   case ir_binop_borrow: {
      struct brw_reg acc = retype(brw_acc_reg(), BRW_REGISTER_TYPE_UD);

      emit(SUBB(dst_null_ud(), op[0], op[1]));
      emit(MOV(result_dst, src_reg(acc)));
      break;
   }
d1694 1
a1694 1
      src_reg offset;
d1702 1
a1702 1
         src_reg(prog_data->base.binding_table.ubo_start + uniform_block->value.u[0]);
d1704 1
a1704 10
         if (brw->gen >= 8) {
            /* Store the offset in a GRF so we can send-from-GRF. */
            offset = src_reg(this, glsl_type::int_type);
            emit(MOV(dst_reg(offset), src_reg(const_offset / 16)));
         } else {
            /* Immediates are fine on older generations since they'll be moved
             * to a (potentially fake) MRF at the generator level.
             */
            offset = src_reg(const_offset / 16);
         }
d1706 1
a1706 2
         offset = src_reg(this, glsl_type::uint_type);
         emit(SHR(dst_reg(offset), op[1], src_reg(4)));
d1709 1
a1709 6
      if (brw->gen >= 7) {
         dst_reg grf_offset = dst_reg(this, glsl_type::int_type);
         grf_offset.type = offset.type;

         emit(MOV(grf_offset, offset));

d1711 1
a1711 1
                                            VS_OPCODE_PULL_CONSTANT_LOAD_GEN7,
d1714 3
a1716 11
                                            src_reg(grf_offset)));
      } else {
         vec4_instruction *pull =
            emit(new(mem_ctx) vec4_instruction(this,
                                               VS_OPCODE_PULL_CONSTANT_LOAD,
                                               dst_reg(packed_consts),
                                               surf_index,
                                               offset));
         pull->base_mrf = 14;
         pull->mlen = 1;
      }
d1739 1
a1739 1
   case ir_triop_fma:
d1746 1
a1746 11
      emit(MAD(result_dst, op[2], op[1], op[0]));
      break;

   case ir_triop_lrp:
      emit_lrp(result_dst, op[0], op[1], op[2]);
      break;

   case ir_triop_csel:
      emit(CMP(dst_null_d(), op[0], src_reg(0), BRW_CONDITIONAL_NZ));
      inst = emit(BRW_OPCODE_SEL, result_dst, op[1], op[2]);
      inst->predicate = BRW_PREDICATE_NORMAL;
a1799 3
   case ir_binop_ldexp:
      assert(!"not reached: should be handled by ldexp_to_arith()");
      break;
d1861 1
a1861 1
   if (ir->var->data.mode == ir_var_system_value)
a2265 39
vec4_visitor::visit_atomic_counter_intrinsic(ir_call *ir)
{
   ir_dereference *deref = static_cast<ir_dereference *>(
      ir->actual_parameters.get_head());
   ir_variable *location = deref->variable_referenced();
   unsigned surf_index = (prog_data->base.binding_table.abo_start +
                          location->data.atomic.buffer_index);

   /* Calculate the surface offset */
   src_reg offset(this, glsl_type::uint_type);
   ir_dereference_array *deref_array = deref->as_dereference_array();
   if (deref_array) {
      deref_array->array_index->accept(this);

      src_reg tmp(this, glsl_type::uint_type);
      emit(MUL(dst_reg(tmp), this->result, ATOMIC_COUNTER_SIZE));
      emit(ADD(dst_reg(offset), tmp, location->data.atomic.offset));
   } else {
      offset = location->data.atomic.offset;
   }

   /* Emit the appropriate machine instruction */
   const char *callee = ir->callee->function_name();
   dst_reg dst = get_assignment_lhs(ir->return_deref, this);

   if (!strcmp("__intrinsic_atomic_read", callee)) {
      emit_untyped_surface_read(surf_index, dst, offset);

   } else if (!strcmp("__intrinsic_atomic_increment", callee)) {
      emit_untyped_atomic(BRW_AOP_INC, surf_index, dst, offset,
                          src_reg(), src_reg());

   } else if (!strcmp("__intrinsic_atomic_predecrement", callee)) {
      emit_untyped_atomic(BRW_AOP_PREDEC, surf_index, dst, offset,
                          src_reg(), src_reg());
   }
}

void
d2268 1
a2268 34
   const char *callee = ir->callee->function_name();

   if (!strcmp("__intrinsic_atomic_read", callee) ||
       !strcmp("__intrinsic_atomic_increment", callee) ||
       !strcmp("__intrinsic_atomic_predecrement", callee)) {
      visit_atomic_counter_intrinsic(ir);
   } else {
      assert(!"Unsupported intrinsic.");
   }
}

src_reg
vec4_visitor::emit_mcs_fetch(ir_texture *ir, src_reg coordinate, int sampler)
{
   vec4_instruction *inst = new(mem_ctx) vec4_instruction(this, SHADER_OPCODE_TXF_MCS);
   inst->base_mrf = 2;
   inst->mlen = 1;
   inst->sampler = sampler;
   inst->dst = dst_reg(this, glsl_type::uvec4_type);
   inst->dst.writemask = WRITEMASK_XYZW;

   /* parameters are: u, v, r, lod; lod will always be zero due to api restrictions */
   int param_base = inst->base_mrf;
   int coord_mask = (1 << ir->coordinate->type->vector_elements) - 1;
   int zero_mask = 0xf & ~coord_mask;

   emit(MOV(dst_reg(MRF, param_base, ir->coordinate->type, coord_mask),
            coordinate));

   emit(MOV(dst_reg(MRF, param_base, ir->coordinate->type, zero_mask),
            src_reg(0)));

   emit(inst);
   return src_reg(inst->dst);
a2276 14
   /* When tg4 is used with the degenerate ZERO/ONE swizzles, don't bother
    * emitting anything other than setting up the constant result.
    */
   if (ir->op == ir_tg4) {
      ir_constant *chan = ir->lod_info.component->as_constant();
      int swiz = GET_SWZ(key->tex.swizzles[sampler], chan->value.i[0]);
      if (swiz == SWIZZLE_ZERO || swiz == SWIZZLE_ONE) {
         dst_reg result(this, ir->type);
         this->result = src_reg(result);
         emit(MOV(result, src_reg(swiz == SWIZZLE_ONE ? 1.0f : 0.0f)));
         return;
      }
   }

a2279 3
   /* Should be lowered */
   assert(!ir->offset || !ir->offset->type->is_array());

a2295 7
   bool has_nonconstant_offset = ir->offset && !ir->offset->as_constant();
   src_reg offset_value;
   if (has_nonconstant_offset) {
      ir->offset->accept(this);
      offset_value = src_reg(this->result);
   }

d2297 1
a2297 1
   src_reg lod, dPdx, dPdy, sample_index, mcs;
a2309 4
   case ir_query_levels:
      lod = src_reg(0);
      lod_type = glsl_type::int_type;
      break;
a2313 5

      if (brw->gen >= 7 && key->tex.compressed_multisample_layout_mask & (1<<sampler))
         mcs = emit_mcs_fetch(ir, coordinate, sampler);
      else
         mcs = src_reg(0u);
a2325 1
   case ir_tg4:
d2342 1
a2342 1
      inst = new(mem_ctx) vec4_instruction(this, SHADER_OPCODE_TXF_CMS);
a2346 9
   case ir_tg4:
      if (has_nonconstant_offset)
         inst = new(mem_ctx) vec4_instruction(this, SHADER_OPCODE_TG4_OFFSET);
      else
         inst = new(mem_ctx) vec4_instruction(this, SHADER_OPCODE_TG4);
      break;
   case ir_query_levels:
      inst = new(mem_ctx) vec4_instruction(this, SHADER_OPCODE_TXS);
      break;
a2352 2
   default:
      assert(!"Unrecognized tex op");
d2355 1
a2355 6
   if (ir->offset != NULL && ir->op != ir_txf)
      inst->texture_offset = brw_texture_offset(ctx, ir->offset->as_constant());

   /* Stuff the channel select bits in the top of the texture offset */
   if (ir->op == ir_tg4)
      inst->texture_offset |= gather_channel(ir, sampler) << 16;
d2357 2
a2358 9
   /* The message header is necessary for:
    * - Gen4 (always)
    * - Texel offsets
    * - Gather channel selection
    * - Sampler indices too large to fit in a 4-bit value.
    */
   inst->header_present =
      brw->gen < 5 || inst->texture_offset != 0 || ir->op == ir_tg4 ||
      sampler >= 16;
d2366 3
d2372 1
a2372 1
   if (ir->op == ir_txs || ir->op == ir_query_levels) {
d2376 1
d2379 4
a2382 2
      int coord_mask = (1 << ir->coordinate->type->vector_elements) - 1;
      int zero_mask = 0xf & ~coord_mask;
d2384 8
a2391 2
      emit(MOV(dst_reg(MRF, param_base, ir->coordinate->type, coord_mask),
               coordinate));
d2393 13
d2411 1
a2411 1
      if (ir->shadow_comparitor && ir->op != ir_txd && (ir->op != ir_tg4 || !has_nonconstant_offset)) {
a2439 8
         if (brw->gen >= 7)
            /* MCS data is in the first channel of `mcs`, but we need to get it into
             * the .y channel of the second vec4 of params, so replicate .x across
             * the whole vec4 and then mask off everything except .y
             */
            mcs.swizzle = BRW_SWIZZLE_XXXX;
            emit(MOV(dst_reg(MRF, param_base + 1, glsl_type::uint_type, WRITEMASK_Y),
                     mcs));
d2441 6
a2474 9
      } else if (ir->op == ir_tg4 && has_nonconstant_offset) {
         if (ir->shadow_comparitor) {
            emit(MOV(dst_reg(MRF, param_base, ir->shadow_comparitor->type, WRITEMASK_W),
                     shadow_comparitor));
         }

         emit(MOV(dst_reg(MRF, param_base + 1, glsl_type::ivec2_type, WRITEMASK_XY),
                  offset_value));
         inst->mlen++;
d2488 1
a2488 1
                   writemask(inst->dst, WRITEMASK_Z),
a2492 4
   if (brw->gen == 6 && ir->op == ir_tg4) {
      emit_gen6_gather_wa(key->tex.gen6_gather_wa[sampler], inst->dst);
   }

a2495 52
/**
 * Apply workarounds for Gen6 gather with UINT/SINT
 */
void
vec4_visitor::emit_gen6_gather_wa(uint8_t wa, dst_reg dst)
{
   if (!wa)
      return;

   int width = (wa & WA_8BIT) ? 8 : 16;
   dst_reg dst_f = dst;
   dst_f.type = BRW_REGISTER_TYPE_F;

   /* Convert from UNORM to UINT */
   emit(MUL(dst_f, src_reg(dst_f), src_reg((float)((1 << width) - 1))));
   emit(MOV(dst, src_reg(dst_f)));

   if (wa & WA_SIGN) {
      /* Reinterpret the UINT value as a signed INT value by
       * shifting the sign bit into place, then shifting back
       * preserving sign.
       */
      emit(SHL(dst, src_reg(dst), src_reg(32 - width)));
      emit(ASR(dst, src_reg(dst), src_reg(32 - width)));
   }
}

/**
 * Set up the gather channel based on the swizzle, for gather4.
 */
uint32_t
vec4_visitor::gather_channel(ir_texture *ir, int sampler)
{
   ir_constant *chan = ir->lod_info.component->as_constant();
   int swiz = GET_SWZ(key->tex.swizzles[sampler], chan->value.i[0]);
   switch (swiz) {
      case SWIZZLE_X: return 0;
      case SWIZZLE_Y:
         /* gather4 sampler is broken for green channel on RG32F --
          * we must ask for blue instead.
          */
         if (key->tex.gather_channel_quirk_mask & (1<<sampler))
            return 2;
         return 1;
      case SWIZZLE_Z: return 2;
      case SWIZZLE_W: return 3;
      default:
         assert(!"Not reached"); /* zero, one swizzles handled already */
         return 0;
   }
}

a2503 7
   if (ir->op == ir_query_levels) {
      /* # levels is in .w */
      orig_val.swizzle = BRW_SWIZZLE4(SWIZZLE_W, SWIZZLE_W, SWIZZLE_W, SWIZZLE_W);
      emit(MOV(swizzled_result, orig_val));
      return;
   }

d2505 1
a2505 1
			|| s == SWIZZLE_NOOP || ir->op == ir_tg4) {
a2509 1

a2586 61
vec4_visitor::visit(ir_emit_vertex *)
{
   assert(!"not reached");
}

void
vec4_visitor::visit(ir_end_primitive *)
{
   assert(!"not reached");
}

void
vec4_visitor::emit_untyped_atomic(unsigned atomic_op, unsigned surf_index,
                                  dst_reg dst, src_reg offset,
                                  src_reg src0, src_reg src1)
{
   unsigned mlen = 0;

   /* Set the atomic operation offset. */
   emit(MOV(brw_writemask(brw_uvec_mrf(8, mlen, 0), WRITEMASK_X), offset));
   mlen++;

   /* Set the atomic operation arguments. */
   if (src0.file != BAD_FILE) {
      emit(MOV(brw_writemask(brw_uvec_mrf(8, mlen, 0), WRITEMASK_X), src0));
      mlen++;
   }

   if (src1.file != BAD_FILE) {
      emit(MOV(brw_writemask(brw_uvec_mrf(8, mlen, 0), WRITEMASK_X), src1));
      mlen++;
   }

   /* Emit the instruction.  Note that this maps to the normal SIMD8
    * untyped atomic message on Ivy Bridge, but that's OK because
    * unused channels will be masked out.
    */
   vec4_instruction *inst = emit(SHADER_OPCODE_UNTYPED_ATOMIC, dst,
                                 src_reg(atomic_op), src_reg(surf_index));
   inst->base_mrf = 0;
   inst->mlen = mlen;
}

void
vec4_visitor::emit_untyped_surface_read(unsigned surf_index, dst_reg dst,
                                        src_reg offset)
{
   /* Set the surface read offset. */
   emit(MOV(brw_writemask(brw_uvec_mrf(8, 0, 0), WRITEMASK_X), offset));

   /* Emit the instruction.  Note that this maps to the normal SIMD8
    * untyped surface read message, but that's OK because unused
    * channels will be masked out.
    */
   vec4_instruction *inst = emit(SHADER_OPCODE_UNTYPED_SURFACE_READ,
                                 dst, src_reg(surf_index));
   inst->base_mrf = 0;
   inst->mlen = 1;
}

void
d2618 1
d2630 12
a2641 13
      if (key->userclip_active) {
         current_annotation = "Clipping flags";
         dst_reg flags0 = dst_reg(this, glsl_type::uint_type);
         dst_reg flags1 = dst_reg(this, glsl_type::uint_type);

         emit(CMP(dst_null_f(), src_reg(output_reg[VARYING_SLOT_CLIP_DIST0]), src_reg(0.0f), BRW_CONDITIONAL_L));
         emit(VS_OPCODE_UNPACK_FLAGS_SIMD4X2, flags0, src_reg(0));
         emit(OR(header1_w, src_reg(header1_w), src_reg(flags0)));

         emit(CMP(dst_null_f(), src_reg(output_reg[VARYING_SLOT_CLIP_DIST1]), src_reg(0.0f), BRW_CONDITIONAL_L));
         emit(VS_OPCODE_UNPACK_FLAGS_SIMD4X2, flags1, src_reg(0));
         emit(SHL(flags1, src_reg(flags1), src_reg(4)));
         emit(OR(header1_w, src_reg(header1_w), src_reg(flags1)));
a2676 4
      if (prog_data->vue_map.slots_valid & VARYING_BIT_VIEWPORT) {
         emit(MOV(retype(brw_writemask(reg, WRITEMASK_Z), BRW_REGISTER_TYPE_D),
                  src_reg(output_reg[VARYING_SLOT_VIEWPORT])));
      }
d2681 1
a2681 1
vec4_visitor::emit_clip_distances(dst_reg reg, int offset)
d2683 8
d2710 1
a2710 2
      reg.writemask = 1 << i;
      emit(DP4(reg,
d2755 9
d2803 23
d2862 4
a2865 3
   /* Lower legacy ff and ClipVertex clipping to clip distances */
   if (key->userclip_active && !prog->UsesClipDistanceOut) {
      current_annotation = "user clip distances";
d2867 7
a2873 5
      output_reg[VARYING_SLOT_CLIP_DIST0] = dst_reg(this, glsl_type::vec4_type);
      output_reg[VARYING_SLOT_CLIP_DIST1] = dst_reg(this, glsl_type::vec4_type);

      emit_clip_distances(output_reg[VARYING_SLOT_CLIP_DIST0], 0);
      emit_clip_distances(output_reg[VARYING_SLOT_CLIP_DIST1], 4);
d2876 5
a2880 10
   /* We may need to split this up into several URB writes, so do them in a
    * loop.
    */
   int slot = 0;
   bool complete = false;
   do {
      /* URB offset is in URB row increments, and each of our MRFs is half of
       * one of those, since we're doing interleaved writes.
       */
      int offset = slot / 2;
d2882 2
d2885 1
d2887 2
a2889 8

         /* If this was max_usable_mrf, we can't fit anything more into this
          * URB WRITE.
          */
         if (mrf > max_usable_mrf) {
            slot++;
            break;
         }
a2891 1
      complete = slot >= prog_data->vue_map.num_slots;
d2893 1
a2893 1
      vec4_instruction *inst = emit_urb_write_opcode(complete);
d2896 7
a2902 2
      inst->offset += offset;
   } while(!complete);
d2905 9
a2959 5
   } else if (brw->gen >= 8) {
      /* Store the offset in a GRF so we can send-from-GRF. */
      src_reg offset = src_reg(this, glsl_type::int_type);
      emit_before(inst, MOV(dst_reg(offset), src_reg(reg_offset)));
      return offset;
d3110 1
a3110 1
   src_reg index = src_reg(prog_data->base.binding_table.pull_constants_start);
d3171 1
a3171 1
	    const float **values = &stage_prog_data->param[uniform * 4];
d3173 1
a3173 1
	    pull_constant_loc[uniform] = stage_prog_data->nr_pull_params / 4;
a3174 1
	    assert(uniform < uniform_array_size);
d3176 1
a3176 1
	       stage_prog_data->pull_param[stage_prog_data->nr_pull_params++]
d3223 1
a3223 1
                           gl_shader_stage stage,
d3225 2
a3226 18
                           bool debug_flag,
                           bool no_spills,
                           shader_time_shader_type st_base,
                           shader_time_shader_type st_written,
                           shader_time_shader_type st_reset)
   : backend_visitor(brw, shader_prog, prog, &prog_data->base, stage),
     c(c),
     key(key),
     prog_data(prog_data),
     sanity_param_count(0),
     fail_msg(NULL),
     first_non_payload_grf(0),
     need_all_constants_in_pull_buffer(false),
     debug_flag(debug_flag),
     no_spills(no_spills),
     st_base(st_base),
     st_written(st_written),
     st_reset(st_reset)
d3228 5
d3240 5
a3260 11

   /* Initialize uniform_array_size to at least 1 because pre-gen6 VS requires
    * at least one. See setup_uniforms() in brw_vec4.cpp.
    */
   this->uniform_array_size = 1;
   if (prog_data) {
      this->uniform_array_size = MAX2(stage_prog_data->nr_params, 1);
   }

   this->uniform_size = rzalloc_array(mem_ctx, int, this->uniform_array_size);
   this->uniform_vector_size = rzalloc_array(mem_ctx, int, this->uniform_array_size);
d3269 15
d3298 1
a3298 1
   msg = ralloc_asprintf(mem_ctx, "vec4 compile failed: %s\n", msg);
@


1.1.1.4
log
@Import Mesa 10.2.7
@
text
@d368 2
a369 4
   if (brw->gen >= 8) {
      emit(opcode, dst, src);
   } else if (brw->gen >= 6) {
      emit_math1_gen6(opcode, dst, src);
d371 1
a371 1
      emit_math1_gen4(opcode, dst, src);
d420 2
a421 4
   if (brw->gen >= 8) {
      emit(opcode, dst, src0, src1);
   } else if (brw->gen >= 6) {
      emit_math2_gen6(opcode, dst, src0, src1);
d423 1
a423 1
      emit_math2_gen4(opcode, dst, src0, src1);
d2477 1
a2477 1
         if (brw->gen >= 7) {
a2484 1
         }
@


1.1.1.5
log
@Import Mesa 10.4.3
@
text
@a24 1
#include "brw_cfg.h"
d33 2
a34 3
                                   enum opcode opcode, const dst_reg &dst,
                                   const src_reg &src0, const src_reg &src1,
                                   const src_reg &src2)
d47 1
d69 1
a69 2
vec4_visitor::emit_before(bblock_t *block, vec4_instruction *inst,
                          vec4_instruction *new_inst)
d74 1
a74 1
   inst->insert_before(block, new_inst);
d80 2
a81 2
vec4_visitor::emit(enum opcode opcode, const dst_reg &dst, const src_reg &src0,
                   const src_reg &src1, const src_reg &src2)
d89 1
a89 2
vec4_visitor::emit(enum opcode opcode, const dst_reg &dst, const src_reg &src0,
                   const src_reg &src1)
d95 1
a95 1
vec4_visitor::emit(enum opcode opcode, const dst_reg &dst, const src_reg &src0)
d101 1
a101 1
vec4_visitor::emit(enum opcode opcode, const dst_reg &dst)
d114 1
a114 1
   vec4_visitor::op(const dst_reg &dst, const src_reg &src0)		\
d122 1
a122 2
   vec4_visitor::op(const dst_reg &dst, const src_reg &src0,		\
                    const src_reg &src1)				\
d130 1
a130 2
   vec4_visitor::op(const dst_reg &dst, const src_reg &src0,		\
                    const src_reg &src1)				\
d140 1
a140 2
   vec4_visitor::op(const dst_reg &dst, const src_reg &src0,		\
                    const src_reg &src1, const src_reg &src2)		\
d182 1
a182 1
vec4_visitor::IF(enum brw_predicate predicate)
d194 1
a194 2
vec4_visitor::IF(src_reg src0, src_reg src1,
                 enum brw_conditional_mod condition)
d216 1
a216 2
vec4_visitor::CMP(dst_reg dst, src_reg src0, src_reg src1,
                  enum brw_conditional_mod condition)
d240 1
a240 1
vec4_visitor::SCRATCH_READ(const dst_reg &dst, const src_reg &index)
d253 1
a253 2
vec4_visitor::SCRATCH_WRITE(const dst_reg &dst, const src_reg &src,
                            const src_reg &index)
a303 3
   if (brw->gen < 6 || brw->gen >= 8 || src.file == BAD_FILE)
      return src;

d325 84
d410 1
a410 2
                        const dst_reg &dst,
                        const src_reg &src0, const src_reg &src1)
d412 9
a420 2
   vec4_instruction *math =
      emit(opcode, dst, fix_math_operand(src0), fix_math_operand(src1));
d422 6
a427 8
   if (brw->gen == 6 && dst.writemask != WRITEMASK_XYZW) {
      /* MATH on Gen6 must be align1, so we can't do writemasks. */
      math->dst = dst_reg(this, glsl_type::vec4_type);
      math->dst.type = dst.type;
      emit(MOV(dst, src_reg(math->dst)));
   } else if (brw->gen < 6) {
      math->base_mrf = 1;
      math->mlen = src1.file == BAD_FILE ? 1 : 2;
d434 2
a435 3
   if (brw->gen < 7) {
      unreachable("ir_unop_pack_half_2x16 should be lowered");
   }
d510 2
a511 3
   if (brw->gen < 7) {
      unreachable("ir_unop_unpack_half_2x16 should be lowered");
   }
d547 3
a549 1
   foreach_in_list(ir_instruction, ir, list) {
d587 2
a588 2
      /* Samplers take up no register space, since they're baked in at
       * link time.
d590 1
a590 1
      return 0;
d597 2
a598 1
      unreachable("not reached");
a638 14
src_reg::src_reg(class vec4_visitor *v, const struct glsl_type *type, int size)
{
   assert(size > 0);

   init();

   this->file = GRF;
   this->reg = v->virtual_grf_alloc(type_size(type) * size);

   this->swizzle = BRW_SWIZZLE_NOOP;

   this->type = brw_type_for_base_type(type);
}

d691 1
a691 1
            stage_prog_data->param[uniforms * 4 + i] = components;
d695 1
a695 1
            static gl_constant_value zero = { 0.0 };
d715 1
a715 2
         stage_prog_data->param[this->uniforms * 4 + j] =
            (gl_constant_value *) &clip_planes[i][j];
d728 2
a729 2
   const ir_state_slot *const slots = ir->get_state_slots();
   assert(slots != NULL);
d731 1
a731 1
   for (unsigned int i = 0; i < ir->get_num_state_slots(); i++) {
d739 1
a739 2
      gl_constant_value *values =
         &this->prog->Parameters->ParameterValues[index][0];
d767 1
a767 2
vec4_visitor::emit_bool_to_cond_code(ir_rvalue *ir,
                                     enum brw_predicate *predicate)
d773 2
a774 2
   if (expr && expr->operation != ir_binop_ubo_load) {
      src_reg op[3];
d777 1
a777 1
      assert(expr->get_num_operands() <= 3);
a848 16
      case ir_triop_csel: {
         /* Expand the boolean condition into the flag register. */
         inst = emit(MOV(dst_null_d(), op[0]));
         inst->conditional_mod = BRW_CONDITIONAL_NZ;

         /* Select which boolean to return. */
         dst_reg temp(this, expr->operands[1]->type);
         inst = emit(BRW_OPCODE_SEL, temp, op[1], op[2]);
         inst->predicate = BRW_PREDICATE_NORMAL;

         /* Expand the result to a condition code. */
         inst = emit(MOV(dst_null_d(), src_reg(temp)));
         inst->conditional_mod = BRW_CONDITIONAL_NZ;
         break;
      }

d850 2
a851 1
	 unreachable("not reached");
d879 2
a880 2
   if (expr && expr->operation != ir_binop_ubo_load) {
      src_reg op[3];
d883 1
a883 1
      assert(expr->get_num_operands() <= 3);
a942 14
      case ir_triop_csel: {
         /* Expand the boolean condition into the flag register. */
         vec4_instruction *inst = emit(MOV(dst_null_d(), op[0]));
         inst->conditional_mod = BRW_CONDITIONAL_NZ;

         /* Select which boolean to return. */
         dst_reg temp(this, expr->operands[1]->type);
         inst = emit(BRW_OPCODE_SEL, temp, op[1], op[2]);
         inst->predicate = BRW_PREDICATE_NORMAL;

         emit(IF(src_reg(temp), src_reg(0), BRW_CONDITIONAL_NZ));
         return;
      }

d944 3
a946 1
	 unreachable("not reached");
d993 2
a994 2
       * Some uniforms, such as samplers and atomic counters, have no actual
       * storage, so we should ignore them.
d996 1
a996 1
      if (ir->is_in_uniform_block() || type_size(ir->type) == 0)
d1017 1
a1017 1
      unreachable("not reached");
d1054 1
a1054 1
vec4_visitor::visit(ir_function_signature *)
d1056 2
a1057 1
   unreachable("not reached");
d1070 1
a1070 1
      sig = ir->matching_signature(NULL, &empty, false);
d1079 18
d1133 3
a1135 2
bool
vec4_visitor::try_emit_b2f_of_compare(ir_expression *ir)
d1137 3
a1139 35
   /* This optimization relies on CMP setting the destination to 0 when
    * false.  Early hardware only sets the least significant bit, and
    * leaves the other bits undefined.  So we can't use it.
    */
   if (brw->gen < 6)
      return false;

   ir_expression *const cmp = ir->operands[0]->as_expression();

   if (cmp == NULL)
      return false;

   switch (cmp->operation) {
   case ir_binop_less:
   case ir_binop_greater:
   case ir_binop_lequal:
   case ir_binop_gequal:
   case ir_binop_equal:
   case ir_binop_nequal:
      break;

   default:
      return false;
   }

   cmp->operands[0]->accept(this);
   const src_reg cmp_src0 = this->result;

   cmp->operands[1]->accept(this);
   const src_reg cmp_src1 = this->result;

   this->result = src_reg(this, ir->type);

   emit(CMP(dst_reg(this->result), cmp_src0, cmp_src1,
            brw_conditional_for_comparison(cmp->operation)));
d1141 1
a1141 6
   /* If the comparison is false, this->result will just happen to be zero.
    */
   vec4_instruction *const inst = emit(BRW_OPCODE_SEL, dst_reg(this->result),
                                       this->result, src_reg(1.0f));
   inst->predicate = BRW_PREDICATE_NORMAL;
   inst->predicate_inverse = true;
d1143 2
a1144 1
   return true;
d1148 1
a1148 1
vec4_visitor::emit_minmax(enum brw_conditional_mod conditionalmod, dst_reg dst,
d1197 2
d1201 3
a1208 25
   if (ir->operation == ir_unop_b2f) {
      if (try_emit_b2f_of_compare(ir))
	 return;
   }

   /* Storage for our result.  Ideally for an assignment we'd be using
    * the actual storage for the result here, instead.
    */
   dst_reg result_dst(this, ir->type);
   src_reg result_src(result_dst);

   if (ir->operation == ir_triop_csel) {
      ir->operands[1]->accept(this);
      op[1] = this->result;
      ir->operands[2]->accept(this);
      op[2] = this->result;

      enum brw_predicate predicate;
      emit_bool_to_cond_code(ir->operands[0], &predicate);
      inst = emit(BRW_OPCODE_SEL, result_dst, op[1], op[2]);
      inst->predicate = predicate;
      this->result = result_src;
      return;
   }

d1225 14
d1241 5
d1249 4
a1252 5
      if (ctx->Const.UniformBooleanTrue != 1) {
         emit(NOT(result_dst, op[0]));
      } else {
         emit(XOR(result_dst, op[0], src_reg(1u)));
      }
d1307 2
a1308 1
      unreachable("not reached: should be handled by ir_explog_to_explog2");
a1318 2
   case ir_unop_dFdx_coarse:
   case ir_unop_dFdx_fine:
d1320 2
a1321 3
   case ir_unop_dFdy_coarse:
   case ir_unop_dFdy_fine:
      unreachable("derivatives not valid in vertex shader");
a1354 4
   case ir_unop_saturate:
      inst = emit(MOV(result_dst, op[0]));
      inst->saturate = true;
      break;
d1357 2
a1358 1
      unreachable("not reached: should be handled by lower_noise");
d1364 2
a1365 1
      unreachable("not reached: should be handled by ir_sub_to_add_neg");
d1386 1
a1386 1
            struct brw_reg acc = retype(brw_acc_reg(8), result_dst.type);
d1397 1
a1397 1
      struct brw_reg acc = retype(brw_acc_reg(8), result_dst.type);
d1409 1
a1409 1
      struct brw_reg acc = retype(brw_acc_reg(8), BRW_REGISTER_TYPE_UD);
d1416 1
a1416 1
      struct brw_reg acc = retype(brw_acc_reg(8), BRW_REGISTER_TYPE_UD);
d1436 1
a1436 3
      if (ctx->Const.UniformBooleanTrue == 1) {
         emit(AND(result_dst, result_src, src_reg(1u)));
      }
d1446 1
a1446 1
         inst = emit(MOV(result_dst, src_reg(ctx->Const.UniformBooleanTrue)));
d1450 1
a1450 3
         if (ctx->Const.UniformBooleanTrue == 1) {
            emit(AND(result_dst, result_src, src_reg(1u)));
         }
d1460 1
a1460 1
         inst = emit(MOV(result_dst, src_reg(ctx->Const.UniformBooleanTrue)));
d1464 1
a1464 3
         if (ctx->Const.UniformBooleanTrue == 1) {
            emit(AND(result_dst, result_src, src_reg(1u)));
         }
d1472 1
a1472 1
      inst = emit(MOV(result_dst, src_reg(ctx->Const.UniformBooleanTrue)));
d1521 2
a1526 17
   case ir_unop_b2i:
      if (ctx->Const.UniformBooleanTrue != 1) {
         emit(AND(result_dst, op[0], src_reg(1u)));
      } else {
         emit(MOV(result_dst, op[0]));
      }
      break;
   case ir_unop_b2f:
      if (ctx->Const.UniformBooleanTrue != 1) {
         op[0].type = BRW_REGISTER_TYPE_UD;
         result_dst.type = BRW_REGISTER_TYPE_UD;
         emit(AND(result_dst, op[0], src_reg(0x3f800000u)));
         result_dst.type = BRW_REGISTER_TYPE_F;
      } else {
         emit(MOV(result_dst, op[0]));
      }
      break;
d1528 1
a1528 1
   case ir_unop_i2b:
d1530 1
a1530 3
      if (ctx->Const.UniformBooleanTrue == 1) {
         emit(AND(result_dst, result_src, src_reg(1u)));
      }
d1532 1
d1592 1
a1592 1
      ir_constant *const_uniform_block = ir->operands[0]->as_constant();
d1602 2
a1603 25
      src_reg surf_index;

      if (const_uniform_block) {
         /* The block index is a constant, so just emit the binding table entry
          * as an immediate.
          */
         surf_index = src_reg(prog_data->base.binding_table.ubo_start +
                              const_uniform_block->value.u[0]);
      } else {
         /* The block index is not a constant. Evaluate the index expression
          * per-channel and add the base UBO index; the generator will select
          * a value from any live channel.
          */
         surf_index = src_reg(this, glsl_type::uint_type);
         emit(ADD(dst_reg(surf_index), op[0],
                  src_reg(prog_data->base.binding_table.ubo_start)));

         /* Assume this may touch any UBO. It would be nice to provide
          * a tighter bound, but the array information is already lowered away.
          */
         brw_mark_surface_used(&prog_data->base,
                               prog_data->base.binding_table.ubo_start +
                               shader_prog->NumUniformBlocks - 1);
      }

d1648 1
a1648 3
      /* UBO bools are any nonzero int.  We need to convert them to use the
       * value of true stored in ctx->Const.UniformBooleanTrue.
       */
d1652 1
a1652 3
         if (ctx->Const.UniformBooleanTrue == 1) {
            emit(AND(result_dst, result, src_reg(1u)));
         }
d1660 2
a1661 1
      unreachable("should have been lowered by vec_index_to_cond_assign");
d1678 3
a1680 1
      unreachable("already handled above");
d1701 2
a1702 1
      unreachable("should have been lowered by lower_vector_insert");
d1705 1
a1705 1
      unreachable("not reached: should be handled by "
d1707 1
d1710 2
a1711 1
      unreachable("not reached: should be handled by lower_quadop_vector");
d1727 2
a1728 1
      unreachable("not reached: should be handled by lower_packing_builtins");
d1732 2
a1733 4
   case ir_unop_interpolate_at_centroid:
   case ir_binop_interpolate_at_sample:
   case ir_binop_interpolate_at_offset:
      unreachable("not reached: should not occur in vertex shader");
d1735 2
a1736 1
      unreachable("not reached: should be handled by ldexp_to_arith()");
d1921 1
a1921 2
                              const struct glsl_type *type,
                              enum brw_predicate predicate)
d2027 1
a2027 1
   enum brw_predicate predicate = BRW_PREDICATE_NONE;
d2114 3
a2116 1
      foreach_in_list(ir_constant, field_value, &ir->components) {
d2181 1
a2181 3
         emit(MOV(*dst,
                  src_reg(ir->value.b[i] != 0 ? ctx->Const.UniformBooleanTrue
                                              : 0u)));
d2184 2
a2185 1
	 unreachable("Non-float/uint/int/bool constant");
d2209 1
a2209 1
                          location->data.binding);
d2251 1
a2251 1
      unreachable("Unsupported intrinsic.");
d2256 1
a2256 1
vec4_visitor::emit_mcs_fetch(ir_texture *ir, src_reg coordinate, src_reg sampler)
d2261 1
a2264 2
   inst->src[1] = sampler;

a2279 9
static bool
is_high_sampler(struct brw_context *brw, src_reg sampler)
{
   if (brw->gen < 8 && !brw->is_haswell)
      return false;

   return sampler.file != IMM || sampler.fixed_hw_reg.dw1.ud >= 16;
}

d2283 1
a2283 1
   uint32_t sampler =
a2285 35
   ir_rvalue *nonconst_sampler_index =
      _mesa_get_sampler_array_nonconst_index(ir->sampler);

   /* Handle non-constant sampler array indexing */
   src_reg sampler_reg;
   if (nonconst_sampler_index) {
      /* The highest sampler which may be used by this operation is
       * the last element of the array. Mark it here, because the generator
       * doesn't have enough information to determine the bound.
       */
      uint32_t array_size = ir->sampler->as_dereference_array()
         ->array->type->array_size();

      uint32_t max_used = sampler + array_size - 1;
      if (ir->op == ir_tg4 && brw->gen < 8) {
         max_used += prog_data->base.binding_table.gather_texture_start;
      } else {
         max_used += prog_data->base.binding_table.texture_start;
      }

      brw_mark_surface_used(&prog_data->base, max_used);

      /* Emit code to evaluate the actual indexing expression */
      nonconst_sampler_index->accept(this);
      dst_reg temp(this, glsl_type::uint_type);
      emit(ADD(temp, this->result, src_reg(sampler)))
         ->force_writemask_all = true;
      sampler_reg = src_reg(temp);
   } else {
      /* Single sampler, or constant array index; the indexing expression
       * is just an immediate.
       */
      sampler_reg = src_reg(sampler);
   }

d2353 1
a2353 1
         mcs = emit_mcs_fetch(ir, coordinate, sampler_reg);
d2372 1
a2372 1
   enum opcode opcode;
d2374 25
a2398 9
   case ir_tex: opcode = SHADER_OPCODE_TXL; break;
   case ir_txl: opcode = SHADER_OPCODE_TXL; break;
   case ir_txd: opcode = SHADER_OPCODE_TXD; break;
   case ir_txf: opcode = SHADER_OPCODE_TXF; break;
   case ir_txf_ms: opcode = SHADER_OPCODE_TXF_CMS; break;
   case ir_txs: opcode = SHADER_OPCODE_TXS; break;
   case ir_tg4: opcode = has_nonconstant_offset
                         ? SHADER_OPCODE_TG4_OFFSET : SHADER_OPCODE_TG4; break;
   case ir_query_levels: opcode = SHADER_OPCODE_TXS; break;
d2400 2
a2401 1
      unreachable("TXB is not valid for vertex shaders.");
d2403 2
a2404 1
      unreachable("LOD is not valid for vertex shaders.");
d2406 1
a2406 1
      unreachable("Unrecognized tex op");
d2409 2
a2410 7
   vec4_instruction *inst = new(mem_ctx) vec4_instruction(this, opcode);

   if (ir->offset != NULL && !has_nonconstant_offset) {
      inst->texture_offset =
         brw_texture_offset(ctx, ir->offset->as_constant()->value.i,
                            ir->offset->type->vector_elements);
   }
d2424 1
a2424 1
      is_high_sampler(brw, sampler_reg);
d2427 1
a2431 2
   inst->src[1] = sampler_reg;

d2584 1
a2584 1
vec4_visitor::gather_channel(ir_texture *ir, uint32_t sampler)
d2600 2
a2601 1
         unreachable("Not reached"); /* zero, one swizzles handled already */
d2606 1
a2606 1
vec4_visitor::swizzle_result(ir_texture *ir, src_reg orig_val, uint32_t sampler)
d2663 1
a2663 1
vec4_visitor::visit(ir_return *)
d2665 1
a2665 1
   unreachable("not reached");
d2669 1
a2669 1
vec4_visitor::visit(ir_discard *)
d2671 1
a2671 1
   unreachable("not reached");
d2685 1
a2685 1
      enum brw_predicate predicate;
d2706 1
a2706 1
   unreachable("not reached");
d2712 1
a2712 1
   unreachable("not reached");
d2788 1
a2788 1
vec4_visitor::emit_psiz_and_flags(dst_reg reg)
d2848 2
a2849 3
         dst_reg reg_w = reg;
         reg_w.writemask = WRITEMASK_W;
         emit(MOV(reg_w, src_reg(output_reg[VARYING_SLOT_PSIZ])));
d2852 2
a2853 4
         dst_reg reg_y = reg;
         reg_y.writemask = WRITEMASK_Y;
         reg_y.type = BRW_REGISTER_TYPE_D;
         emit(MOV(reg_y, src_reg(output_reg[VARYING_SLOT_LAYER])));
d2856 2
a2857 4
         dst_reg reg_z = reg;
         reg_z.writemask = WRITEMASK_Z;
         reg_z.type = BRW_REGISTER_TYPE_D;
         emit(MOV(reg_z, src_reg(output_reg[VARYING_SLOT_VIEWPORT])));
d2910 1
a2910 1
vec4_visitor::emit_urb_slot(dst_reg reg, int varying)
d2912 2
a2917 1
   {
d2920 1
a2920 1
      emit_psiz_and_flags(reg);
a2921 1
   }
d3029 1
a3029 2
         emit_urb_slot(dst_reg(MRF, mrf++),
                       prog_data->vue_map.slot_to_varying[slot]);
d3051 1
a3051 1
vec4_visitor::get_scratch_offset(bblock_t *block, vec4_instruction *inst,
d3068 3
a3070 4
      emit_before(block, inst, ADD(dst_reg(index), *reladdr,
                                   src_reg(reg_offset)));
      emit_before(block, inst, MUL(dst_reg(index), index,
                                   src_reg(message_header_scale)));
d3079 1
a3079 1
vec4_visitor::get_pull_constant_offset(bblock_t * block, vec4_instruction *inst,
d3085 1
a3085 2
      emit_before(block, inst, ADD(dst_reg(index), *reladdr,
                                   src_reg(reg_offset)));
d3091 1
a3091 1
         emit_before(block, inst, MUL(dst_reg(index), index, src_reg(16)));
d3098 1
a3098 1
      emit_before(block, inst, MOV(dst_reg(offset), src_reg(reg_offset)));
d3113 1
a3113 1
vec4_visitor::emit_scratch_read(bblock_t *block, vec4_instruction *inst,
d3118 1
a3118 2
   src_reg index = get_scratch_offset(block, inst, orig_src.reladdr,
                                      reg_offset);
d3120 1
a3120 1
   emit_before(block, inst, SCRATCH_READ(temp, index));
d3130 1
a3130 2
vec4_visitor::emit_scratch_write(bblock_t *block, vec4_instruction *inst,
                                 int base_offset)
d3133 1
a3133 2
   src_reg index = get_scratch_offset(block, inst, inst->dst.reladdr,
                                      reg_offset);
d3160 1
a3160 1
   inst->insert_after(block, write);
d3178 4
a3181 1
   memset(scratch_loc, -1, sizeof(scratch_loc));
d3187 3
a3189 1
   foreach_block_and_inst(block, vec4_instruction, inst, cfg) {
d3212 3
a3214 1
   foreach_block_and_inst_safe(block, vec4_instruction, inst, cfg) {
d3220 1
a3220 1
	 emit_scratch_write(block, inst, scratch_loc[inst->dst.reg]);
d3229 1
a3229 1
	 emit_scratch_read(block, inst, temp, inst->src[i],
d3245 1
a3245 1
vec4_visitor::emit_pull_constant_load(bblock_t *block, vec4_instruction *inst,
d3251 1
a3251 2
   src_reg offset = get_pull_constant_offset(block, inst, orig_src.reladdr,
                                             reg_offset);
d3257 1
a3257 1
      emit_before(block, inst, MOV(grf_offset, offset));
d3268 1
a3268 1
   emit_before(block, inst, load);
d3287 4
a3290 2
   memset(pull_constant_loc, -1, sizeof(pull_constant_loc));
   bool nested_reladdr;
d3298 6
a3303 2
   do {
      nested_reladdr = false;
d3305 1
a3305 4
      foreach_block_and_inst_safe(block, vec4_instruction, inst, cfg) {
         for (int i = 0 ; i < 3; i++) {
            if (inst->src[i].file != UNIFORM || !inst->src[i].reladdr)
               continue;
d3307 5
a3311 1
            int uniform = inst->src[i].reg;
d3313 1
a3313 2
            if (inst->src[i].reladdr->reladdr)
               nested_reladdr = true;  /* will need another pass */
d3315 6
a3320 15
            /* If this array isn't already present in the pull constant buffer,
             * add it.
             */
            if (pull_constant_loc[uniform] == -1) {
               const gl_constant_value **values =
                  &stage_prog_data->param[uniform * 4];

               pull_constant_loc[uniform] = stage_prog_data->nr_pull_params / 4;

               assert(uniform < uniform_array_size);
               for (int j = 0; j < uniform_size[uniform] * 4; j++) {
                  stage_prog_data->pull_param[stage_prog_data->nr_pull_params++]
                     = values[j];
               }
            }
d3322 3
a3324 3
            /* Set up the annotation tracking for new generated instructions. */
            base_ir = inst->ir;
            current_annotation = inst->annotation;
d3326 1
a3326 1
            dst_reg temp = dst_reg(this, glsl_type::vec4_type);
d3328 2
a3329 2
            emit_pull_constant_load(block, inst, temp, inst->src[i],
                                    pull_constant_loc[uniform]);
d3331 4
a3334 5
            inst->src[i].file = temp.file;
            inst->src[i].reg = temp.reg;
            inst->src[i].reg_offset = temp.reg_offset;
            inst->src[i].reladdr = NULL;
         }
d3336 1
a3336 1
   } while (nested_reladdr);
@


1.1.1.6
log
@Import Mesa 10.2.9
@
text
@d25 1
d34 3
a36 2
				   enum opcode opcode, dst_reg dst,
				   src_reg src0, src_reg src1, src_reg src2)
a48 1
   this->sampler = 0;
d70 2
a71 1
vec4_visitor::emit_before(vec4_instruction *inst, vec4_instruction *new_inst)
d76 1
a76 1
   inst->insert_before(new_inst);
d82 2
a83 2
vec4_visitor::emit(enum opcode opcode, dst_reg dst,
		   src_reg src0, src_reg src1, src_reg src2)
d91 2
a92 1
vec4_visitor::emit(enum opcode opcode, dst_reg dst, src_reg src0, src_reg src1)
d98 1
a98 1
vec4_visitor::emit(enum opcode opcode, dst_reg dst, src_reg src0)
d104 1
a104 1
vec4_visitor::emit(enum opcode opcode, dst_reg dst)
d117 1
a117 1
   vec4_visitor::op(dst_reg dst, src_reg src0)				\
d125 2
a126 1
   vec4_visitor::op(dst_reg dst, src_reg src0, src_reg src1)		\
d134 2
a135 1
   vec4_visitor::op(dst_reg dst, src_reg src0, src_reg src1)		\
d145 2
a146 1
   vec4_visitor::op(dst_reg dst, src_reg src0, src_reg src1, src_reg src2)\
d188 1
a188 1
vec4_visitor::IF(uint32_t predicate)
d200 2
a201 1
vec4_visitor::IF(src_reg src0, src_reg src1, uint32_t condition)
d223 2
a224 1
vec4_visitor::CMP(dst_reg dst, src_reg src0, src_reg src1, uint32_t condition)
d248 1
a248 1
vec4_visitor::SCRATCH_READ(dst_reg dst, src_reg index)
d261 2
a262 1
vec4_visitor::SCRATCH_WRITE(dst_reg dst, src_reg src, src_reg index)
d313 3
a336 84
vec4_visitor::emit_math1_gen6(enum opcode opcode, dst_reg dst, src_reg src)
{
   src = fix_math_operand(src);

   if (dst.writemask != WRITEMASK_XYZW) {
      /* The gen6 math instruction must be align1, so we can't do
       * writemasks.
       */
      dst_reg temp_dst = dst_reg(this, glsl_type::vec4_type);

      emit(opcode, temp_dst, src);

      emit(MOV(dst, src_reg(temp_dst)));
   } else {
      emit(opcode, dst, src);
   }
}

void
vec4_visitor::emit_math1_gen4(enum opcode opcode, dst_reg dst, src_reg src)
{
   vec4_instruction *inst = emit(opcode, dst, src);
   inst->base_mrf = 1;
   inst->mlen = 1;
}

void
vec4_visitor::emit_math(opcode opcode, dst_reg dst, src_reg src)
{
   switch (opcode) {
   case SHADER_OPCODE_RCP:
   case SHADER_OPCODE_RSQ:
   case SHADER_OPCODE_SQRT:
   case SHADER_OPCODE_EXP2:
   case SHADER_OPCODE_LOG2:
   case SHADER_OPCODE_SIN:
   case SHADER_OPCODE_COS:
      break;
   default:
      assert(!"not reached: bad math opcode");
      return;
   }

   if (brw->gen >= 8) {
      emit(opcode, dst, src);
   } else if (brw->gen >= 6) {
      emit_math1_gen6(opcode, dst, src);
   } else {
      emit_math1_gen4(opcode, dst, src);
   }
}

void
vec4_visitor::emit_math2_gen6(enum opcode opcode,
			      dst_reg dst, src_reg src0, src_reg src1)
{
   src0 = fix_math_operand(src0);
   src1 = fix_math_operand(src1);

   if (dst.writemask != WRITEMASK_XYZW) {
      /* The gen6 math instruction must be align1, so we can't do
       * writemasks.
       */
      dst_reg temp_dst = dst_reg(this, glsl_type::vec4_type);
      temp_dst.type = dst.type;

      emit(opcode, temp_dst, src0, src1);

      emit(MOV(dst, src_reg(temp_dst)));
   } else {
      emit(opcode, dst, src0, src1);
   }
}

void
vec4_visitor::emit_math2_gen4(enum opcode opcode,
			      dst_reg dst, src_reg src0, src_reg src1)
{
   vec4_instruction *inst = emit(opcode, dst, src0, src1);
   inst->base_mrf = 1;
   inst->mlen = 2;
}

void
d338 2
a339 1
			dst_reg dst, src_reg src0, src_reg src1)
d341 2
a342 9
   switch (opcode) {
   case SHADER_OPCODE_POW:
   case SHADER_OPCODE_INT_QUOTIENT:
   case SHADER_OPCODE_INT_REMAINDER:
      break;
   default:
      assert(!"not reached: unsupported binary math opcode");
      return;
   }
d344 8
a351 6
   if (brw->gen >= 8) {
      emit(opcode, dst, src0, src1);
   } else if (brw->gen >= 6) {
      emit_math2_gen6(opcode, dst, src0, src1);
   } else {
      emit_math2_gen4(opcode, dst, src0, src1);
d358 3
a360 2
   if (brw->gen < 7)
      assert(!"ir_unop_pack_half_2x16 should be lowered");
d435 3
a437 2
   if (brw->gen < 7)
      assert(!"ir_unop_unpack_half_2x16 should be lowered");
d473 1
a473 3
   foreach_list(node, list) {
      ir_instruction *ir = (ir_instruction *)node;

d511 2
a512 2
      /* Samplers take up one slot in UNIFORMS[], but they're baked in
       * at link time.
d514 1
a514 1
      return 1;
d521 1
a521 2
      assert(0);
      break;
d562 14
d628 1
a628 1
            stage_prog_data->param[uniforms * 4 + i] = &components->f;
d632 1
a632 1
            static float zero = 0;
d652 2
a653 1
         stage_prog_data->param[this->uniforms * 4 + j] = &clip_planes[i][j];
d666 2
a667 2
   const ir_state_slot *const slots = ir->state_slots;
   assert(ir->state_slots != NULL);
d669 1
a669 1
   for (unsigned int i = 0; i < ir->num_state_slots; i++) {
d677 2
a678 1
      float *values = &this->prog->Parameters->ParameterValues[index][0].f;
d706 2
a707 1
vec4_visitor::emit_bool_to_cond_code(ir_rvalue *ir, uint32_t *predicate)
d713 2
a714 2
   if (expr) {
      src_reg op[2];
d717 1
a717 1
      assert(expr->get_num_operands() <= 2);
d789 16
d806 1
a806 2
	 assert(!"not reached");
	 break;
d834 2
a835 2
   if (expr) {
      src_reg op[2];
d838 1
a838 1
      assert(expr->get_num_operands() <= 2);
d898 14
d913 1
a913 3
	 assert(!"not reached");
	 emit(IF(op[0], src_reg(0), BRW_CONDITIONAL_NZ));
	 return;
d960 2
a961 2
       * Atomic counters take no uniform storage, no need to do
       * anything here.
d963 1
a963 1
      if (ir->is_in_uniform_block() || ir->type->contains_atomic())
d984 1
a984 1
      assert(!"not reached");
d1021 1
a1021 1
vec4_visitor::visit(ir_function_signature *ir)
d1023 1
a1023 2
   assert(0);
   (void)ir;
d1036 1
a1036 1
      sig = ir->matching_signature(NULL, &empty);
a1044 18
vec4_visitor::try_emit_sat(ir_expression *ir)
{
   ir_rvalue *sat_src = ir->as_rvalue_to_saturate();
   if (!sat_src)
      return false;

   sat_src->accept(this);
   src_reg src = this->result;

   this->result = src_reg(this, ir->type);
   vec4_instruction *inst;
   inst = emit(MOV(dst_reg(this->result), src));
   inst->saturate = true;

   return true;
}

bool
d1081 2
a1082 3
void
vec4_visitor::emit_bool_comparison(unsigned int op,
				 dst_reg dst, src_reg src0, src_reg src1)
d1084 35
a1118 3
   /* original gen4 does destination conversion before comparison. */
   if (brw->gen < 5)
      dst.type = src0.type;
d1120 6
a1125 1
   emit(CMP(dst, src0, src1, brw_conditional_for_comparison(op)));
d1127 1
a1127 2
   dst.type = BRW_REGISTER_TYPE_D;
   emit(AND(dst, src_reg(dst), src_reg(0x1)));
d1131 1
a1131 1
vec4_visitor::emit_minmax(uint32_t conditionalmod, dst_reg dst,
a1179 2
   src_reg result_src;
   dst_reg result_dst;
a1181 3
   if (try_emit_sat(ir))
      return;

d1187 25
a1227 14
   int vector_elements = ir->operands[0]->type->vector_elements;
   if (ir->operands[1]) {
      vector_elements = MAX2(vector_elements,
			     ir->operands[1]->type->vector_elements);
   }

   this->result.file = BAD_FILE;

   /* Storage for our result.  Ideally for an assignment we'd be using
    * the actual storage for the result here, instead.
    */
   result_src = src_reg(this, ir->type);
   /* convenience for the emit functions below. */
   result_dst = dst_reg(result_src);
a1229 5
   /* Limit writes to the channels that will be used by result_src later.
    * This does limit this temp's use as a temporary for multi-instruction
    * sequences.
    */
   result_dst.writemask = (1 << ir->type->vector_elements) - 1;
d1233 5
a1237 4
      /* Note that BRW_OPCODE_NOT is not appropriate here, since it is
       * ones complement of the whole register, not just bit 0.
       */
      emit(XOR(result_dst, op[0], src_reg(1)));
d1292 1
a1292 2
      assert(!"not reached: should be handled by ir_explog_to_explog2");
      break;
d1303 2
d1306 3
a1308 2
      assert(!"derivatives not valid in vertex shader");
      break;
d1342 4
d1348 1
a1348 2
      assert(!"not reached: should be handled by lower_noise");
      break;
d1354 1
a1354 2
      assert(!"not reached: should be handled by ir_sub_to_add_neg");
      break;
d1375 1
a1375 1
            struct brw_reg acc = retype(brw_acc_reg(), result_dst.type);
d1386 1
a1386 1
      struct brw_reg acc = retype(brw_acc_reg(), result_dst.type);
d1398 1
a1398 1
      struct brw_reg acc = retype(brw_acc_reg(), BRW_REGISTER_TYPE_UD);
d1405 1
a1405 1
      struct brw_reg acc = retype(brw_acc_reg(), BRW_REGISTER_TYPE_UD);
d1425 3
a1427 1
      emit(AND(result_dst, result_src, src_reg(0x1)));
d1437 1
a1437 1
	 inst = emit(MOV(result_dst, src_reg(1)));
d1441 3
a1443 1
	 emit(AND(result_dst, result_src, src_reg(0x1)));
d1453 1
a1453 1
	 inst = emit(MOV(result_dst, src_reg(1)));
d1457 3
a1459 1
	 emit(AND(result_dst, result_src, src_reg(0x1)));
d1467 1
a1467 1
      inst = emit(MOV(result_dst, src_reg(1)));
a1515 2
   case ir_unop_b2f:
   case ir_unop_b2i:
d1520 17
d1538 1
a1538 1
   case ir_unop_i2b: {
d1540 3
a1542 1
      emit(AND(result_dst, result_src, src_reg(1)));
a1543 1
   }
d1603 1
a1603 1
      ir_constant *uniform_block = ir->operands[0]->as_constant();
d1613 25
a1637 2
      src_reg surf_index =
         src_reg(prog_data->base.binding_table.ubo_start + uniform_block->value.u[0]);
d1682 3
a1684 1
      /* UBO bools are any nonzero int.  We store bools as either 0 or 1. */
d1688 3
a1690 1
         emit(AND(result_dst, result, src_reg(0x1)));
d1698 1
a1698 2
      assert(!"should have been lowered by vec_index_to_cond_assign");
      break;
d1715 1
a1715 3
      emit(CMP(dst_null_d(), op[0], src_reg(0), BRW_CONDITIONAL_NZ));
      inst = emit(BRW_OPCODE_SEL, result_dst, op[1], op[2]);
      inst->predicate = BRW_PREDICATE_NORMAL;
d1736 1
a1736 2
      assert(!"should have been lowered by lower_vector_insert");
      break;
d1739 1
a1739 1
      assert(!"not reached: should be handled by "
a1740 1
      break;
d1743 1
a1743 2
      assert(!"not reached: should be handled by lower_quadop_vector");
      break;
d1759 1
a1759 2
      assert(!"not reached: should be handled by lower_packing_builtins");
      break;
d1763 4
a1766 2
      assert(!"not reached: should not occur in vertex shader");
      break;
d1768 1
a1768 2
      assert(!"not reached: should be handled by ldexp_to_arith()");
      break;
d1953 2
a1954 1
			      const struct glsl_type *type, uint32_t predicate)
d2060 1
a2060 1
   uint32_t predicate = BRW_PREDICATE_NONE;
d2147 1
a2147 3
      foreach_list(node, &ir->components) {
	 ir_constant *field_value = (ir_constant *)node;

d2212 3
a2214 1
	 emit(MOV(*dst, src_reg(ir->value.b[i])));
d2217 1
a2217 2
	 assert(!"Non-float/uint/int/bool constant");
	 break;
d2241 1
a2241 1
                          location->data.atomic.buffer_index);
d2283 1
a2283 1
      assert(!"Unsupported intrinsic.");
d2288 1
a2288 1
vec4_visitor::emit_mcs_fetch(ir_texture *ir, src_reg coordinate, int sampler)
a2292 1
   inst->sampler = sampler;
d2296 2
d2313 9
d2325 1
a2325 1
   int sampler =
d2328 35
d2430 1
a2430 1
         mcs = emit_mcs_fetch(ir, coordinate, sampler);
d2449 1
a2449 1
   vec4_instruction *inst = NULL;
d2451 9
a2459 25
   case ir_tex:
   case ir_txl:
      inst = new(mem_ctx) vec4_instruction(this, SHADER_OPCODE_TXL);
      break;
   case ir_txd:
      inst = new(mem_ctx) vec4_instruction(this, SHADER_OPCODE_TXD);
      break;
   case ir_txf:
      inst = new(mem_ctx) vec4_instruction(this, SHADER_OPCODE_TXF);
      break;
   case ir_txf_ms:
      inst = new(mem_ctx) vec4_instruction(this, SHADER_OPCODE_TXF_CMS);
      break;
   case ir_txs:
      inst = new(mem_ctx) vec4_instruction(this, SHADER_OPCODE_TXS);
      break;
   case ir_tg4:
      if (has_nonconstant_offset)
         inst = new(mem_ctx) vec4_instruction(this, SHADER_OPCODE_TG4_OFFSET);
      else
         inst = new(mem_ctx) vec4_instruction(this, SHADER_OPCODE_TG4);
      break;
   case ir_query_levels:
      inst = new(mem_ctx) vec4_instruction(this, SHADER_OPCODE_TXS);
      break;
d2461 1
a2461 2
      assert(!"TXB is not valid for vertex shaders.");
      break;
d2463 1
a2463 2
      assert(!"LOD is not valid for vertex shaders.");
      break;
d2465 1
a2465 1
      assert(!"Unrecognized tex op");
d2468 7
a2474 2
   if (ir->offset != NULL && ir->op != ir_txf)
      inst->texture_offset = brw_texture_offset(ctx, ir->offset->as_constant());
d2488 1
a2488 1
      sampler >= 16;
a2490 1
   inst->sampler = sampler;
d2495 2
d2649 1
a2649 1
vec4_visitor::gather_channel(ir_texture *ir, int sampler)
d2665 1
a2665 2
         assert(!"Not reached"); /* zero, one swizzles handled already */
         return 0;
d2670 1
a2670 1
vec4_visitor::swizzle_result(ir_texture *ir, src_reg orig_val, int sampler)
d2727 1
a2727 1
vec4_visitor::visit(ir_return *ir)
d2729 1
a2729 1
   assert(!"not reached");
d2733 1
a2733 1
vec4_visitor::visit(ir_discard *ir)
d2735 1
a2735 1
   assert(!"not reached");
d2749 1
a2749 1
      uint32_t predicate;
d2770 1
a2770 1
   assert(!"not reached");
d2776 1
a2776 1
   assert(!"not reached");
d2852 1
a2852 1
vec4_visitor::emit_psiz_and_flags(struct brw_reg reg)
d2912 3
a2914 2
         emit(MOV(brw_writemask(reg, WRITEMASK_W),
                  src_reg(output_reg[VARYING_SLOT_PSIZ])));
d2917 4
a2920 2
         emit(MOV(retype(brw_writemask(reg, WRITEMASK_Y), BRW_REGISTER_TYPE_D),
                  src_reg(output_reg[VARYING_SLOT_LAYER])));
d2923 4
a2926 2
         emit(MOV(retype(brw_writemask(reg, WRITEMASK_Z), BRW_REGISTER_TYPE_D),
                  src_reg(output_reg[VARYING_SLOT_VIEWPORT])));
d2979 1
a2979 1
vec4_visitor::emit_urb_slot(int mrf, int varying)
a2980 2
   struct brw_reg hw_reg = brw_message_reg(mrf);
   dst_reg reg = dst_reg(MRF, mrf);
d2985 1
d2988 1
a2988 1
      emit_psiz_and_flags(hw_reg);
d2990 1
d3098 2
a3099 1
         emit_urb_slot(mrf++, prog_data->vue_map.slot_to_varying[slot]);
d3121 1
a3121 1
vec4_visitor::get_scratch_offset(vec4_instruction *inst,
d3138 4
a3141 3
      emit_before(inst, ADD(dst_reg(index), *reladdr, src_reg(reg_offset)));
      emit_before(inst, MUL(dst_reg(index),
			    index, src_reg(message_header_scale)));
d3150 1
a3150 1
vec4_visitor::get_pull_constant_offset(vec4_instruction *inst,
d3156 2
a3157 1
      emit_before(inst, ADD(dst_reg(index), *reladdr, src_reg(reg_offset)));
d3163 1
a3163 1
	 emit_before(inst, MUL(dst_reg(index), index, src_reg(16)));
d3170 1
a3170 1
      emit_before(inst, MOV(dst_reg(offset), src_reg(reg_offset)));
d3185 1
a3185 1
vec4_visitor::emit_scratch_read(vec4_instruction *inst,
d3190 2
a3191 1
   src_reg index = get_scratch_offset(inst, orig_src.reladdr, reg_offset);
d3193 1
a3193 1
   emit_before(inst, SCRATCH_READ(temp, index));
d3203 2
a3204 1
vec4_visitor::emit_scratch_write(vec4_instruction *inst, int base_offset)
d3207 2
a3208 1
   src_reg index = get_scratch_offset(inst, inst->dst.reladdr, reg_offset);
d3235 1
a3235 1
   inst->insert_after(write);
d3253 1
a3253 4

   for (int i = 0; i < this->virtual_grf_count; i++) {
      scratch_loc[i] = -1;
   }
d3259 1
a3259 3
   foreach_list(node, &this->instructions) {
      vec4_instruction *inst = (vec4_instruction *)node;

d3282 1
a3282 3
   foreach_list_safe(node, &this->instructions) {
      vec4_instruction *inst = (vec4_instruction *)node;

d3288 1
a3288 1
	 emit_scratch_write(inst, scratch_loc[inst->dst.reg]);
d3297 1
a3297 1
	 emit_scratch_read(inst, temp, inst->src[i],
d3313 1
a3313 1
vec4_visitor::emit_pull_constant_load(vec4_instruction *inst,
d3319 2
a3320 1
   src_reg offset = get_pull_constant_offset(inst, orig_src.reladdr, reg_offset);
d3326 1
a3326 1
      emit_before(inst, MOV(grf_offset, offset));
d3337 1
a3337 1
   emit_before(inst, load);
d3356 2
a3357 4

   for (int i = 0; i < this->uniforms; i++) {
      pull_constant_loc[i] = -1;
   }
d3365 2
a3366 2
   foreach_list_safe(node, &this->instructions) {
      vec4_instruction *inst = (vec4_instruction *)node;
d3368 4
a3371 3
      for (int i = 0 ; i < 3; i++) {
	 if (inst->src[i].file != UNIFORM || !inst->src[i].reladdr)
	    continue;
d3373 1
a3373 1
	 int uniform = inst->src[i].reg;
d3375 2
a3376 5
	 /* If this array isn't already present in the pull constant buffer,
	  * add it.
	  */
	 if (pull_constant_loc[uniform] == -1) {
	    const float **values = &stage_prog_data->param[uniform * 4];
d3378 15
a3392 1
	    pull_constant_loc[uniform] = stage_prog_data->nr_pull_params / 4;
d3394 3
a3396 6
	    assert(uniform < uniform_array_size);
	    for (int j = 0; j < uniform_size[uniform] * 4; j++) {
	       stage_prog_data->pull_param[stage_prog_data->nr_pull_params++]
                  = values[j];
	    }
	 }
d3398 1
a3398 3
	 /* Set up the annotation tracking for new generated instructions. */
	 base_ir = inst->ir;
	 current_annotation = inst->annotation;
d3400 2
a3401 1
	 dst_reg temp = dst_reg(this, glsl_type::vec4_type);
d3403 5
a3407 7
	 emit_pull_constant_load(inst, temp, inst->src[i],
				 pull_constant_loc[uniform]);

	 inst->src[i].file = temp.file;
	 inst->src[i].reg = temp.reg;
	 inst->src[i].reg_offset = temp.reg_offset;
	 inst->src[i].reladdr = NULL;
d3409 1
a3409 1
   }
@


