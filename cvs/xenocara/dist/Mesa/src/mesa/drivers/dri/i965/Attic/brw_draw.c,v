head	1.12;
access;
symbols
	OPENBSD_5_8:1.11.0.4
	OPENBSD_5_8_BASE:1.11
	OPENBSD_5_7:1.11.0.2
	OPENBSD_5_7_BASE:1.11
	v10_2_9:1.1.1.8
	v10_4_3:1.1.1.7
	v10_2_7:1.1.1.6
	OPENBSD_5_6:1.9.0.2
	OPENBSD_5_6_BASE:1.9
	v10_2_3:1.1.1.6
	OPENBSD_5_5:1.8.0.2
	OPENBSD_5_5_BASE:1.8
	v9_2_5:1.1.1.5
	v9_2_3:1.1.1.5
	v9_2_2:1.1.1.5
	v9_2_1:1.1.1.5
	v9_2_0:1.1.1.5
	OPENBSD_5_4:1.7.0.4
	OPENBSD_5_4_BASE:1.7
	OPENBSD_5_3:1.7.0.2
	OPENBSD_5_3_BASE:1.7
	OPENBSD_5_2:1.6.0.4
	OPENBSD_5_2_BASE:1.6
	OPENBSD_5_1_BASE:1.6
	OPENBSD_5_1:1.6.0.2
	v7_10_3:1.1.1.4
	OPENBSD_5_0:1.5.0.6
	OPENBSD_5_0_BASE:1.5
	OPENBSD_4_9:1.5.0.2
	OPENBSD_4_9_BASE:1.5
	OPENBSD_4_8:1.5.0.4
	OPENBSD_4_8_BASE:1.5
	OPENBSD_4_7:1.4.0.4
	OPENBSD_4_7_BASE:1.4
	OPENBSD_4_6:1.4.0.2
	OPENBSD_4_6_BASE:1.4
	OPENBSD_4_5:1.3.0.2
	OPENBSD_4_5_BASE:1.3
	OPENBSD_4_4:1.2.0.4
	OPENBSD_4_4_BASE:1.2
	OPENBSD_4_3_BASE:1.2
	OPENBSD_4_3:1.2.0.2
	v7_0_1:1.1.1.3
	OPENBSD_4_2:1.1.1.2.0.2
	OPENBSD_4_2_BASE:1.1.1.2
	v6_5_2:1.1.1.2
	v6_5_1:1.1.1.1
	mesa:1.1.1;
locks; strict;
comment	@ * @;


1.12
date	2015.12.23.05.17.49;	author jsg;	state dead;
branches;
next	1.11;
commitid	TnlogFl9nOv2eaRf;

1.11
date	2015.02.20.23.09.58;	author jsg;	state Exp;
branches;
next	1.10;
commitid	4ry2gvZGMXkCUD2n;

1.10
date	2015.01.25.14.41.20;	author jsg;	state Exp;
branches;
next	1.9;
commitid	mcxB0JvoI9gTDYXU;

1.9
date	2014.07.09.21.08.59;	author jsg;	state Exp;
branches;
next	1.8;
commitid	WPD6rgPryPkvXOr9;

1.8
date	2013.09.05.14.04.17;	author jsg;	state Exp;
branches;
next	1.7;

1.7
date	2012.08.17.13.58.15;	author mpi;	state Exp;
branches;
next	1.6;

1.6
date	2011.10.23.13.37.39;	author matthieu;	state Exp;
branches;
next	1.5;

1.5
date	2010.05.22.20.06.18;	author matthieu;	state Exp;
branches;
next	1.4;

1.4
date	2009.05.17.20.26.39;	author matthieu;	state Exp;
branches;
next	1.3;

1.3
date	2008.11.02.14.58.15;	author matthieu;	state Exp;
branches;
next	1.2;

1.2
date	2008.02.12.21.09.26;	author matthieu;	state Exp;
branches;
next	1.1;

1.1
date	2006.11.25.18.52.41;	author matthieu;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2006.11.25.18.52.41;	author matthieu;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2007.03.03.11.57.15;	author matthieu;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2007.11.24.17.28.33;	author matthieu;	state Exp;
branches;
next	1.1.1.4;

1.1.1.4
date	2011.10.23.13.29.36;	author matthieu;	state Exp;
branches;
next	1.1.1.5;

1.1.1.5
date	2013.09.05.13.15.30;	author jsg;	state Exp;
branches;
next	1.1.1.6;

1.1.1.6
date	2014.07.09.20.34.49;	author jsg;	state Exp;
branches;
next	1.1.1.7;
commitid	3JhLfwcuBALP0ZR7;

1.1.1.7
date	2015.01.25.14.11.33;	author jsg;	state Exp;
branches;
next	1.1.1.8;
commitid	ce2W5rH5aF7VS9gi;

1.1.1.8
date	2015.02.20.22.48.40;	author jsg;	state Exp;
branches;
next	;
commitid	F54a1i0WXHMxq7kE;


desc
@@


1.12
log
@remove the now unused Mesa 10.2.9 code
@
text
@/**************************************************************************
 *
 * Copyright 2003 VMware, Inc.
 * All Rights Reserved.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sub license, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice (including the
 * next paragraph) shall be included in all copies or substantial portions
 * of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
 * IN NO EVENT SHALL VMWARE AND/OR ITS SUPPLIERS BE LIABLE FOR
 * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *
 **************************************************************************/

#include <sys/errno.h>

#include "main/glheader.h"
#include "main/context.h"
#include "main/condrender.h"
#include "main/samplerobj.h"
#include "main/state.h"
#include "main/enums.h"
#include "main/macros.h"
#include "main/transformfeedback.h"
#include "tnl/tnl.h"
#include "vbo/vbo_context.h"
#include "swrast/swrast.h"
#include "swrast_setup/swrast_setup.h"
#include "drivers/common/meta.h"

#include "brw_blorp.h"
#include "brw_draw.h"
#include "brw_defines.h"
#include "brw_context.h"
#include "brw_state.h"

#include "intel_batchbuffer.h"
#include "intel_buffers.h"
#include "intel_fbo.h"
#include "intel_mipmap_tree.h"
#include "intel_buffer_objects.h"

#define FILE_DEBUG_FLAG DEBUG_PRIMS

const GLuint prim_to_hw_prim[GL_TRIANGLE_STRIP_ADJACENCY+1] = {
   _3DPRIM_POINTLIST,
   _3DPRIM_LINELIST,
   _3DPRIM_LINELOOP,
   _3DPRIM_LINESTRIP,
   _3DPRIM_TRILIST,
   _3DPRIM_TRISTRIP,
   _3DPRIM_TRIFAN,
   _3DPRIM_QUADLIST,
   _3DPRIM_QUADSTRIP,
   _3DPRIM_POLYGON,
   _3DPRIM_LINELIST_ADJ,
   _3DPRIM_LINESTRIP_ADJ,
   _3DPRIM_TRILIST_ADJ,
   _3DPRIM_TRISTRIP_ADJ,
};


static const GLenum reduced_prim[GL_POLYGON+1] = {
   GL_POINTS,
   GL_LINES,
   GL_LINES,
   GL_LINES,
   GL_TRIANGLES,
   GL_TRIANGLES,
   GL_TRIANGLES,
   GL_TRIANGLES,
   GL_TRIANGLES,
   GL_TRIANGLES
};


/* When the primitive changes, set a state bit and re-validate.  Not
 * the nicest and would rather deal with this by having all the
 * programs be immune to the active primitive (ie. cope with all
 * possibilities).  That may not be realistic however.
 */
static void brw_set_prim(struct brw_context *brw,
                         const struct _mesa_prim *prim)
{
   struct gl_context *ctx = &brw->ctx;
   uint32_t hw_prim = prim_to_hw_prim[prim->mode];

   DBG("PRIM: %s\n", _mesa_lookup_enum_by_nr(prim->mode));

   /* Slight optimization to avoid the GS program when not needed:
    */
   if (prim->mode == GL_QUAD_STRIP &&
       ctx->Light.ShadeModel != GL_FLAT &&
       ctx->Polygon.FrontMode == GL_FILL &&
       ctx->Polygon.BackMode == GL_FILL)
      hw_prim = _3DPRIM_TRISTRIP;

   if (prim->mode == GL_QUADS && prim->count == 4 &&
       ctx->Light.ShadeModel != GL_FLAT &&
       ctx->Polygon.FrontMode == GL_FILL &&
       ctx->Polygon.BackMode == GL_FILL) {
      hw_prim = _3DPRIM_TRIFAN;
   }

   if (hw_prim != brw->primitive) {
      brw->primitive = hw_prim;
      brw->state.dirty.brw |= BRW_NEW_PRIMITIVE;

      if (reduced_prim[prim->mode] != brw->reduced_primitive) {
	 brw->reduced_primitive = reduced_prim[prim->mode];
	 brw->state.dirty.brw |= BRW_NEW_REDUCED_PRIMITIVE;
      }
   }
}

static void gen6_set_prim(struct brw_context *brw,
                          const struct _mesa_prim *prim)
{
   uint32_t hw_prim;

   DBG("PRIM: %s\n", _mesa_lookup_enum_by_nr(prim->mode));

   hw_prim = prim_to_hw_prim[prim->mode];

   if (hw_prim != brw->primitive) {
      brw->primitive = hw_prim;
      brw->state.dirty.brw |= BRW_NEW_PRIMITIVE;
   }
}


/**
 * The hardware is capable of removing dangling vertices on its own; however,
 * prior to Gen6, we sometimes convert quads into trifans (and quad strips
 * into tristrips), since pre-Gen6 hardware requires a GS to render quads.
 * This function manually trims dangling vertices from a draw call involving
 * quads so that those dangling vertices won't get drawn when we convert to
 * trifans/tristrips.
 */
static GLuint trim(GLenum prim, GLuint length)
{
   if (prim == GL_QUAD_STRIP)
      return length > 3 ? (length - length % 2) : 0;
   else if (prim == GL_QUADS)
      return length - length % 4;
   else
      return length;
}


static void brw_emit_prim(struct brw_context *brw,
			  const struct _mesa_prim *prim,
			  uint32_t hw_prim)
{
   int verts_per_instance;
   int vertex_access_type;
   int start_vertex_location;
   int base_vertex_location;
   int indirect_flag;

   DBG("PRIM: %s %d %d\n", _mesa_lookup_enum_by_nr(prim->mode),
       prim->start, prim->count);

   start_vertex_location = prim->start;
   base_vertex_location = prim->basevertex;
   if (prim->indexed) {
      vertex_access_type = brw->gen >= 7 ?
         GEN7_3DPRIM_VERTEXBUFFER_ACCESS_RANDOM :
         GEN4_3DPRIM_VERTEXBUFFER_ACCESS_RANDOM;
      start_vertex_location += brw->ib.start_vertex_offset;
      base_vertex_location += brw->vb.start_vertex_bias;
   } else {
      vertex_access_type = brw->gen >= 7 ?
         GEN7_3DPRIM_VERTEXBUFFER_ACCESS_SEQUENTIAL :
         GEN4_3DPRIM_VERTEXBUFFER_ACCESS_SEQUENTIAL;
      start_vertex_location += brw->vb.start_vertex_bias;
   }

   /* We only need to trim the primitive count on pre-Gen6. */
   if (brw->gen < 6)
      verts_per_instance = trim(prim->mode, prim->count);
   else
      verts_per_instance = prim->count;

   /* If nothing to emit, just return. */
   if (verts_per_instance == 0 && !prim->is_indirect)
      return;

   /* If we're set to always flush, do it before and after the primitive emit.
    * We want to catch both missed flushes that hurt instruction/state cache
    * and missed flushes of the render cache as it heads to other parts of
    * the besides the draw code.
    */
   if (brw->always_flush_cache) {
      intel_batchbuffer_emit_mi_flush(brw);
   }

   /* If indirect, emit a bunch of loads from the indirect BO. */
   if (prim->is_indirect) {
      struct gl_buffer_object *indirect_buffer = brw->ctx.DrawIndirectBuffer;
      drm_intel_bo *bo = intel_bufferobj_buffer(brw,
            intel_buffer_object(indirect_buffer),
            prim->indirect_offset, 5 * sizeof(GLuint));

      indirect_flag = GEN7_3DPRIM_INDIRECT_PARAMETER_ENABLE;

      brw_load_register_mem(brw, GEN7_3DPRIM_VERTEX_COUNT, bo,
                            I915_GEM_DOMAIN_VERTEX, 0,
                            prim->indirect_offset + 0);
      brw_load_register_mem(brw, GEN7_3DPRIM_INSTANCE_COUNT, bo,
                            I915_GEM_DOMAIN_VERTEX, 0,
                            prim->indirect_offset + 4);

      brw_load_register_mem(brw, GEN7_3DPRIM_START_VERTEX, bo,
                            I915_GEM_DOMAIN_VERTEX, 0,
                            prim->indirect_offset + 8);
      if (prim->indexed) {
         brw_load_register_mem(brw, GEN7_3DPRIM_BASE_VERTEX, bo,
                               I915_GEM_DOMAIN_VERTEX, 0,
                               prim->indirect_offset + 12);
         brw_load_register_mem(brw, GEN7_3DPRIM_START_INSTANCE, bo,
                               I915_GEM_DOMAIN_VERTEX, 0,
                               prim->indirect_offset + 16);
      } else {
         brw_load_register_mem(brw, GEN7_3DPRIM_START_INSTANCE, bo,
                               I915_GEM_DOMAIN_VERTEX, 0,
                               prim->indirect_offset + 12);
         BEGIN_BATCH(3);
         OUT_BATCH(MI_LOAD_REGISTER_IMM | (3 - 2));
         OUT_BATCH(GEN7_3DPRIM_BASE_VERTEX);
         OUT_BATCH(0);
         ADVANCE_BATCH();
      }
   }
   else {
      indirect_flag = 0;
   }


   if (brw->gen >= 7) {
      BEGIN_BATCH(7);
      OUT_BATCH(CMD_3D_PRIM << 16 | (7 - 2) | indirect_flag);
      OUT_BATCH(hw_prim | vertex_access_type);
   } else {
      BEGIN_BATCH(6);
      OUT_BATCH(CMD_3D_PRIM << 16 | (6 - 2) |
                hw_prim << GEN4_3DPRIM_TOPOLOGY_TYPE_SHIFT |
                vertex_access_type);
   }
   OUT_BATCH(verts_per_instance);
   OUT_BATCH(start_vertex_location);
   OUT_BATCH(prim->num_instances);
   OUT_BATCH(prim->base_instance);
   OUT_BATCH(base_vertex_location);
   ADVANCE_BATCH();

   /* Only used on Sandybridge; harmless to set elsewhere. */
   brw->batch.need_workaround_flush = true;

   if (brw->always_flush_cache) {
      intel_batchbuffer_emit_mi_flush(brw);
   }
}


static void brw_merge_inputs( struct brw_context *brw,
		       const struct gl_client_array *arrays[])
{
   GLuint i;

   for (i = 0; i < brw->vb.nr_buffers; i++) {
      drm_intel_bo_unreference(brw->vb.buffers[i].bo);
      brw->vb.buffers[i].bo = NULL;
   }
   brw->vb.nr_buffers = 0;

   for (i = 0; i < VERT_ATTRIB_MAX; i++) {
      brw->vb.inputs[i].buffer = -1;
      brw->vb.inputs[i].glarray = arrays[i];
   }
}

/*
 * \brief Resolve buffers before drawing.
 *
 * Resolve the depth buffer's HiZ buffer, resolve the depth buffer of each
 * enabled depth texture, and flush the render cache for any dirty textures.
 *
 * (In the future, this will also perform MSAA resolves).
 */
static void
brw_predraw_resolve_buffers(struct brw_context *brw)
{
   struct gl_context *ctx = &brw->ctx;
   struct intel_renderbuffer *depth_irb;
   struct intel_texture_object *tex_obj;

   /* Resolve the depth buffer's HiZ buffer. */
   depth_irb = intel_get_renderbuffer(ctx->DrawBuffer, BUFFER_DEPTH);
   if (depth_irb)
      intel_renderbuffer_resolve_hiz(brw, depth_irb);

   /* Resolve depth buffer and render cache of each enabled texture. */
   int maxEnabledUnit = ctx->Texture._MaxEnabledTexImageUnit;
   for (int i = 0; i <= maxEnabledUnit; i++) {
      if (!ctx->Texture.Unit[i]._Current)
	 continue;
      tex_obj = intel_texture_object(ctx->Texture.Unit[i]._Current);
      if (!tex_obj || !tex_obj->mt)
	 continue;
      intel_miptree_all_slices_resolve_depth(brw, tex_obj->mt);
      intel_miptree_resolve_color(brw, tex_obj->mt);
      brw_render_cache_set_check_flush(brw, tex_obj->mt->bo);
   }
}

/**
 * \brief Call this after drawing to mark which buffers need resolving
 *
 * If the depth buffer was written to and if it has an accompanying HiZ
 * buffer, then mark that it needs a depth resolve.
 *
 * If the color buffer is a multisample window system buffer, then
 * mark that it needs a downsample.
 *
 * Also mark any render targets which will be textured as needing a render
 * cache flush.
 */
static void brw_postdraw_set_buffers_need_resolve(struct brw_context *brw)
{
   struct gl_context *ctx = &brw->ctx;
   struct gl_framebuffer *fb = ctx->DrawBuffer;

   struct intel_renderbuffer *front_irb = NULL;
   struct intel_renderbuffer *back_irb = intel_get_renderbuffer(fb, BUFFER_BACK_LEFT);
   struct intel_renderbuffer *depth_irb = intel_get_renderbuffer(fb, BUFFER_DEPTH);
   struct intel_renderbuffer *stencil_irb = intel_get_renderbuffer(fb, BUFFER_STENCIL);
   struct gl_renderbuffer_attachment *depth_att = &fb->Attachment[BUFFER_DEPTH];

   if (brw_is_front_buffer_drawing(fb))
      front_irb = intel_get_renderbuffer(fb, BUFFER_FRONT_LEFT);

   if (front_irb)
      front_irb->need_downsample = true;
   if (back_irb)
      back_irb->need_downsample = true;
   if (depth_irb && ctx->Depth.Mask) {
      intel_renderbuffer_att_set_needs_depth_resolve(depth_att);
      brw_render_cache_set_add_bo(brw, depth_irb->mt->bo);
   }

   if (ctx->Extensions.ARB_stencil_texturing &&
       stencil_irb && ctx->Stencil._WriteEnabled) {
      brw_render_cache_set_add_bo(brw, stencil_irb->mt->bo);
   }

   for (int i = 0; i < fb->_NumColorDrawBuffers; i++) {
      struct intel_renderbuffer *irb =
         intel_renderbuffer(fb->_ColorDrawBuffers[i]);

      if (irb)
         brw_render_cache_set_add_bo(brw, irb->mt->bo);
   }
}

/* May fail if out of video memory for texture or vbo upload, or on
 * fallback conditions.
 */
static bool brw_try_draw_prims( struct gl_context *ctx,
				     const struct gl_client_array *arrays[],
				     const struct _mesa_prim *prims,
				     GLuint nr_prims,
				     const struct _mesa_index_buffer *ib,
				     GLuint min_index,
				     GLuint max_index,
				     struct gl_buffer_object *indirect)
{
   struct brw_context *brw = brw_context(ctx);
   bool retval = true;
   GLuint i;
   bool fail_next = false;

   if (ctx->NewState)
      _mesa_update_state( ctx );

   /* Find the highest sampler unit used by each shader program.  A bit-count
    * won't work since ARB programs use the texture unit number as the sampler
    * index.
    */
   brw->wm.base.sampler_count =
      _mesa_fls(ctx->FragmentProgram._Current->Base.SamplersUsed);
   brw->gs.base.sampler_count = ctx->GeometryProgram._Current ?
      _mesa_fls(ctx->GeometryProgram._Current->Base.SamplersUsed) : 0;
   brw->vs.base.sampler_count =
      _mesa_fls(ctx->VertexProgram._Current->Base.SamplersUsed);

   /* We have to validate the textures *before* checking for fallbacks;
    * otherwise, the software fallback won't be able to rely on the
    * texture state, the firstLevel and lastLevel fields won't be
    * set in the intel texture object (they'll both be 0), and the
    * software fallback will segfault if it attempts to access any
    * texture level other than level 0.
    */
   brw_validate_textures( brw );

   intel_prepare_render(brw);

   /* This workaround has to happen outside of brw_upload_state() because it
    * may flush the batchbuffer for a blit, affecting the state flags.
    */
   brw_workaround_depthstencil_alignment(brw, 0);

   /* Resolves must occur after updating renderbuffers, updating context state,
    * and finalizing textures but before setting up any hardware state for
    * this draw call.
    */
   brw_predraw_resolve_buffers(brw);

   /* Bind all inputs, derive varying and size information:
    */
   brw_merge_inputs( brw, arrays );

   brw->ib.ib = ib;
   brw->state.dirty.brw |= BRW_NEW_INDICES;

   brw->vb.min_index = min_index;
   brw->vb.max_index = max_index;
   brw->state.dirty.brw |= BRW_NEW_VERTICES;

   for (i = 0; i < nr_prims; i++) {
      int estimated_max_prim_size;

      estimated_max_prim_size = 512; /* batchbuffer commands */
      estimated_max_prim_size += (BRW_MAX_TEX_UNIT *
				  (sizeof(struct brw_sampler_state) +
				   sizeof(struct gen5_sampler_default_color)));
      estimated_max_prim_size += 1024; /* gen6 VS push constants */
      estimated_max_prim_size += 1024; /* gen6 WM push constants */
      estimated_max_prim_size += 512; /* misc. pad */

      /* Flush the batch if it's approaching full, so that we don't wrap while
       * we've got validated state that needs to be in the same batch as the
       * primitives.
       */
      intel_batchbuffer_require_space(brw, estimated_max_prim_size, RENDER_RING);
      intel_batchbuffer_save_state(brw);

      if (brw->num_instances != prims[i].num_instances) {
         brw->num_instances = prims[i].num_instances;
         brw->state.dirty.brw |= BRW_NEW_VERTICES;
         brw_merge_inputs(brw, arrays);
      }
      if (brw->basevertex != prims[i].basevertex) {
         brw->basevertex = prims[i].basevertex;
         brw->state.dirty.brw |= BRW_NEW_VERTICES;
         brw_merge_inputs(brw, arrays);
      }
      if (brw->gen < 6)
	 brw_set_prim(brw, &prims[i]);
      else
	 gen6_set_prim(brw, &prims[i]);

retry:
      /* Note that before the loop, brw->state.dirty.brw was set to != 0, and
       * that the state updated in the loop outside of this block is that in
       * *_set_prim or intel_batchbuffer_flush(), which only impacts
       * brw->state.dirty.brw.
       */
      if (brw->state.dirty.brw) {
	 brw->no_batch_wrap = true;
	 brw_upload_state(brw);
      }

      brw_emit_prim(brw, &prims[i], brw->primitive);

      brw->no_batch_wrap = false;

      if (dri_bufmgr_check_aperture_space(&brw->batch.bo, 1)) {
	 if (!fail_next) {
	    intel_batchbuffer_reset_to_saved(brw);
	    intel_batchbuffer_flush(brw);
	    fail_next = true;
	    goto retry;
	 } else {
	    if (intel_batchbuffer_flush(brw) == -ENOSPC) {
	       static bool warned = false;

	       if (!warned) {
		  fprintf(stderr, "i965: Single primitive emit exceeded"
			  "available aperture space\n");
		  warned = true;
	       }

	       retval = false;
	    }
	 }
      }

      /* Now that we know we haven't run out of aperture space, we can safely
       * reset the dirty bits.
       */
      if (brw->state.dirty.brw)
         brw_clear_dirty_bits(brw);
   }

   if (brw->always_flush_batch)
      intel_batchbuffer_flush(brw);

   brw_state_cache_check_size(brw);
   brw_postdraw_set_buffers_need_resolve(brw);

   return retval;
}

void brw_draw_prims( struct gl_context *ctx,
		     const struct _mesa_prim *prims,
		     GLuint nr_prims,
		     const struct _mesa_index_buffer *ib,
		     GLboolean index_bounds_valid,
		     GLuint min_index,
		     GLuint max_index,
		     struct gl_transform_feedback_object *unused_tfb_object,
		     struct gl_buffer_object *indirect )
{
   struct brw_context *brw = brw_context(ctx);
   const struct gl_client_array **arrays = ctx->Array._DrawArrays;

   assert(unused_tfb_object == NULL);

   if (!_mesa_check_conditional_render(ctx))
      return;

   /* Handle primitive restart if needed */
   if (brw_handle_primitive_restart(ctx, prims, nr_prims, ib, indirect)) {
      /* The draw was handled, so we can exit now */
      return;
   }

   /* Do GL_SELECT and GL_FEEDBACK rendering using swrast, even though it
    * won't support all the extensions we support.
    */
   if (ctx->RenderMode != GL_RENDER) {
      perf_debug("%s render mode not supported in hardware\n",
                 _mesa_lookup_enum_by_nr(ctx->RenderMode));
      _swsetup_Wakeup(ctx);
      _tnl_wakeup(ctx);
      _tnl_draw_prims(ctx, prims, nr_prims, ib,
                      index_bounds_valid, min_index, max_index, NULL, NULL);
      return;
   }

   /* If we're going to have to upload any of the user's vertex arrays, then
    * get the minimum and maximum of their index buffer so we know what range
    * to upload.
    */
   if (!index_bounds_valid && !vbo_all_varyings_in_vbos(arrays)) {
      perf_debug("Scanning index buffer to compute index buffer bounds.  "
                 "Use glDrawRangeElements() to avoid this.\n");
      vbo_get_minmax_indices(ctx, prims, ib, &min_index, &max_index, nr_prims);
   }

   /* Try drawing with the hardware, but don't do anything else if we can't
    * manage it.  swrast doesn't support our featureset, so we can't fall back
    * to it.
    */
   brw_try_draw_prims(ctx, arrays, prims, nr_prims, ib, min_index, max_index, indirect);
}

void brw_draw_init( struct brw_context *brw )
{
   struct gl_context *ctx = &brw->ctx;
   struct vbo_context *vbo = vbo_context(ctx);
   int i;

   /* Register our drawing function:
    */
   vbo->draw_prims = brw_draw_prims;

   for (i = 0; i < VERT_ATTRIB_MAX; i++)
      brw->vb.inputs[i].buffer = -1;
   brw->vb.nr_buffers = 0;
   brw->vb.nr_enabled = 0;
}

void brw_draw_destroy( struct brw_context *brw )
{
   int i;

   for (i = 0; i < brw->vb.nr_buffers; i++) {
      drm_intel_bo_unreference(brw->vb.buffers[i].bo);
      brw->vb.buffers[i].bo = NULL;
   }
   brw->vb.nr_buffers = 0;

   for (i = 0; i < brw->vb.nr_enabled; i++) {
      brw->vb.enabled[i]->buffer = -1;
   }
   brw->vb.nr_enabled = 0;

   drm_intel_bo_unreference(brw->ib.bo);
   brw->ib.bo = NULL;
}
@


1.11
log
@Merge Mesa 10.2.9
@
text
@@


1.10
log
@Merge Mesa 10.4.3
Tested by matthieu@@ mpi@@ and myself.  landry@@ ran a ports bulk build.
kettenis@@ tracked down the cause of an alignment fault on archs
that require strict eight byte pointer alignment.
@
text
@d58 1
a58 1
static const GLuint prim_to_hw_prim[GL_TRIANGLE_STRIP_ADJACENCY+1] = {
a88 9
uint32_t
get_hw_prim_for_gl_prim(int mode)
{
   if (mode >= BRW_PRIM_OFFSET)
      return mode - BRW_PRIM_OFFSET;
   else
      return prim_to_hw_prim[mode];
}

d99 1
a99 1
   uint32_t hw_prim = get_hw_prim_for_gl_prim(prim->mode);
d136 1
a136 1
   hw_prim = get_hw_prim_for_gl_prim(prim->mode);
d170 2
d177 2
a178 3
   int start_vertex_location = prim->start;
   int base_vertex_location = prim->basevertex;

d296 34
d426 6
a444 1
      const int sampler_state_size = 16;
d447 3
a449 2
      estimated_max_prim_size += BRW_MAX_TEX_UNIT *
         (sampler_state_size + sizeof(struct gen5_sampler_default_color));
d461 1
a461 2
      if (brw->num_instances != prims[i].num_instances ||
          brw->basevertex != prims[i].basevertex) {
d463 4
d468 2
a469 4
         if (i > 0) { /* For i == 0 we just did this before the loop */
            brw->state.dirty.brw |= BRW_NEW_VERTICES;
            brw_merge_inputs(brw, arrays);
         }
a470 21

      brw->draw.gl_basevertex =
         prims[i].indexed ? prims[i].basevertex : prims[i].start;

      drm_intel_bo_unreference(brw->draw.draw_params_bo);

      if (prims[i].is_indirect) {
         /* Point draw_params_bo at the indirect buffer. */
         brw->draw.draw_params_bo =
            intel_buffer_object(ctx->DrawIndirectBuffer)->buffer;
         drm_intel_bo_reference(brw->draw.draw_params_bo);
         brw->draw.draw_params_offset =
            prims[i].indirect_offset + (prims[i].indexed ? 12 : 8);
      } else {
         /* Set draw_params_bo to NULL so brw_prepare_vertices knows it
          * has to upload gl_BaseVertex and such if they're needed.
          */
         brw->draw.draw_params_bo = NULL;
         brw->draw.draw_params_offset = 0;
      }

a476 1

a541 5

   if (ctx->Query.CondRenderQuery) {
      perf_debug("Conditional rendering is implemented in software and may "
                 "stall.  This should be fixed in the driver.\n");
   }
@


1.9
log
@Merge Mesa 10.2.3
tested by matthieu@@ kettenis@@ mpi@@ brett@@ and myself across a
diverse range of hardware
@
text
@d58 1
a58 1
const GLuint prim_to_hw_prim[GL_TRIANGLE_STRIP_ADJACENCY+1] = {
d89 9
d108 1
a108 1
   uint32_t hw_prim = prim_to_hw_prim[prim->mode];
d145 1
a145 1
   hw_prim = prim_to_hw_prim[prim->mode];
a178 2
   int start_vertex_location;
   int base_vertex_location;
d184 3
a186 2
   start_vertex_location = prim->start;
   base_vertex_location = prim->basevertex;
a303 34
/*
 * \brief Resolve buffers before drawing.
 *
 * Resolve the depth buffer's HiZ buffer, resolve the depth buffer of each
 * enabled depth texture, and flush the render cache for any dirty textures.
 *
 * (In the future, this will also perform MSAA resolves).
 */
static void
brw_predraw_resolve_buffers(struct brw_context *brw)
{
   struct gl_context *ctx = &brw->ctx;
   struct intel_renderbuffer *depth_irb;
   struct intel_texture_object *tex_obj;

   /* Resolve the depth buffer's HiZ buffer. */
   depth_irb = intel_get_renderbuffer(ctx->DrawBuffer, BUFFER_DEPTH);
   if (depth_irb)
      intel_renderbuffer_resolve_hiz(brw, depth_irb);

   /* Resolve depth buffer and render cache of each enabled texture. */
   int maxEnabledUnit = ctx->Texture._MaxEnabledTexImageUnit;
   for (int i = 0; i <= maxEnabledUnit; i++) {
      if (!ctx->Texture.Unit[i]._Current)
	 continue;
      tex_obj = intel_texture_object(ctx->Texture.Unit[i]._Current);
      if (!tex_obj || !tex_obj->mt)
	 continue;
      intel_miptree_all_slices_resolve_depth(brw, tex_obj->mt);
      intel_miptree_resolve_color(brw, tex_obj->mt);
      brw_render_cache_set_check_flush(brw, tex_obj->mt->bo);
   }
}

a399 6
   /* Resolves must occur after updating renderbuffers, updating context state,
    * and finalizing textures but before setting up any hardware state for
    * this draw call.
    */
   brw_predraw_resolve_buffers(brw);

d413 1
d416 2
a417 3
      estimated_max_prim_size += (BRW_MAX_TEX_UNIT *
				  (sizeof(struct brw_sampler_state) +
				   sizeof(struct gen5_sampler_default_color)));
d429 2
a430 1
      if (brw->num_instances != prims[i].num_instances) {
d432 5
a436 2
         brw->state.dirty.brw |= BRW_NEW_VERTICES;
         brw_merge_inputs(brw, arrays);
d438 19
a456 4
      if (brw->basevertex != prims[i].basevertex) {
         brw->basevertex = prims[i].basevertex;
         brw->state.dirty.brw |= BRW_NEW_VERTICES;
         brw_merge_inputs(brw, arrays);
d458 1
d465 1
d531 5
@


1.8
log
@Merge Mesa 9.2.0
@
text
@d2 2
a3 2
 * 
 * Copyright 2003 Tungsten Graphics, Inc., Cedar Park, Texas.
d5 1
a5 1
 * 
d13 1
a13 1
 * 
d17 1
a17 1
 * 
d21 1
a21 1
 * IN NO EVENT SHALL TUNGSTEN GRAPHICS AND/OR ITS SUPPLIERS BE LIABLE FOR
d25 1
a25 1
 * 
d51 1
d54 1
a54 1
#include "intel_regions.h"
d58 1
a58 1
static GLuint prim_to_hw_prim[GL_POLYGON+1] = {
d68 5
a72 1
   _3DPRIM_POLYGON
d76 1
a76 1
static const GLenum reduced_prim[GL_POLYGON+1] = {  
d159 1
a159 1
   else 
d172 1
d180 3
a182 1
      vertex_access_type = GEN4_3DPRIM_VERTEXBUFFER_ACCESS_RANDOM;
d186 3
a188 1
      vertex_access_type = GEN4_3DPRIM_VERTEXBUFFER_ACCESS_SEQUENTIAL;
d199 1
a199 1
   if (verts_per_instance == 0)
d211 39
a249 15
   BEGIN_BATCH(6);
   OUT_BATCH(CMD_3D_PRIM << 16 | (6 - 2) |
	     hw_prim << GEN4_3DPRIM_TOPOLOGY_TYPE_SHIFT |
	     vertex_access_type);
   OUT_BATCH(verts_per_instance);
   OUT_BATCH(start_vertex_location);
   OUT_BATCH(prim->num_instances);
   OUT_BATCH(prim->base_instance);
   OUT_BATCH(base_vertex_location);
   ADVANCE_BATCH();

   brw->batch.need_workaround_flush = true;

   if (brw->always_flush_cache) {
      intel_batchbuffer_emit_mi_flush(brw);
a250 1
}
a251 11
static void gen7_emit_prim(struct brw_context *brw,
			   const struct _mesa_prim *prim,
			   uint32_t hw_prim)
{
   int verts_per_instance;
   int vertex_access_type;
   int start_vertex_location;
   int base_vertex_location;

   DBG("PRIM: %s %d %d\n", _mesa_lookup_enum_by_nr(prim->mode),
       prim->start, prim->count);
d253 4
a256 6
   start_vertex_location = prim->start;
   base_vertex_location = prim->basevertex;
   if (prim->indexed) {
      vertex_access_type = GEN7_3DPRIM_VERTEXBUFFER_ACCESS_RANDOM;
      start_vertex_location += brw->ib.start_vertex_offset;
      base_vertex_location += brw->vb.start_vertex_bias;
d258 4
a261 17
      vertex_access_type = GEN7_3DPRIM_VERTEXBUFFER_ACCESS_SEQUENTIAL;
      start_vertex_location += brw->vb.start_vertex_bias;
   }

   verts_per_instance = prim->count;

   /* If nothing to emit, just return. */
   if (verts_per_instance == 0)
      return;

   /* If we're set to always flush, do it before and after the primitive emit.
    * We want to catch both missed flushes that hurt instruction/state cache
    * and missed flushes of the render cache as it heads to other parts of
    * the besides the draw code.
    */
   if (brw->always_flush_cache) {
      intel_batchbuffer_emit_mi_flush(brw);
a262 4

   BEGIN_BATCH(7);
   OUT_BATCH(CMD_3D_PRIM << 16 | (7 - 2));
   OUT_BATCH(hw_prim | vertex_access_type);
d270 3
a292 1
      brw->vb.inputs[i].attrib = (gl_vert_attrib) i;
d299 2
a300 2
 * Resolve the depth buffer's HiZ buffer and resolve the depth buffer of each
 * enabled depth texture.
d316 4
a319 5
   /* Resolve depth buffer of each enabled depth texture, and color buffer of
    * each fast-clear-enabled color texture.
    */
   for (int i = 0; i < BRW_MAX_TEX_UNIT; i++) {
      if (!ctx->Texture.Unit[i]._ReallyEnabled)
d326 1
d338 3
d350 2
d353 1
a353 1
   if (brw->is_front_buffer_rendering)
d357 1
a357 1
      intel_renderbuffer_set_needs_downsample(front_irb);
d359 18
a376 3
      intel_renderbuffer_set_needs_downsample(back_irb);
   if (depth_irb && ctx->Depth.Mask)
      intel_renderbuffer_set_needs_depth_resolve(depth_irb);
d384 1
a384 1
				     const struct _mesa_prim *prim,
d388 2
a389 1
				     GLuint max_index )
d399 11
d413 1
a413 1
    * set in the intel texture object (they'll both be 0), and the 
d458 1
a458 1
      intel_batchbuffer_require_space(brw, estimated_max_prim_size, false);
d461 2
a462 2
      if (brw->num_instances != prim->num_instances) {
         brw->num_instances = prim->num_instances;
d464 1
d466 2
a467 2
      if (brw->basevertex != prim->basevertex) {
         brw->basevertex = prim->basevertex;
d469 1
d472 1
a472 1
	 brw_set_prim(brw, &prim[i]);
d474 1
a474 1
	 gen6_set_prim(brw, &prim[i]);
d487 1
a487 4
      if (brw->gen >= 7)
	 gen7_emit_prim(brw, &prim[i], brw->primitive);
      else
	 brw_emit_prim(brw, &prim[i], brw->primitive);
d511 6
d529 1
a529 1
		     const struct _mesa_prim *prim,
d535 2
a536 1
		     struct gl_transform_feedback_object *tfb_vertcount )
d541 2
d547 1
a547 1
   if (brw_handle_primitive_restart(ctx, prim, nr_prims, ib)) {
a551 7
   /* If we're going to have to upload any of the user's vertex arrays, then
    * get the minimum and maximum of their index buffer so we know what range
    * to upload.
    */
   if (!vbo_all_varyings_in_vbos(arrays) && !index_bounds_valid)
      vbo_get_minmax_indices(ctx, prim, ib, &min_index, &max_index, nr_prims);

d560 2
a561 1
      _tnl_draw_prims(ctx, arrays, prim, nr_prims, ib, min_index, max_index);
d565 10
d579 1
a579 1
   brw_try_draw_prims(ctx, arrays, prim, nr_prims, ib, min_index, max_index);
d588 1
a588 1
   /* Register our drawing function: 
@


1.7
log
@Upate to libGL 7.11.2

Tested by jsg@@, matthieu@@ and ajacoutot@@, ok mattieu@@
@
text
@d28 1
d36 2
d42 1
d44 1
d51 3
d90 2
a91 2
static GLuint brw_set_prim(struct brw_context *brw,
			   const struct _mesa_prim *prim)
d93 2
a94 2
   struct gl_context *ctx = &brw->intel.ctx;
   GLenum mode = prim->mode;
d100 1
a100 1
   if (mode == GL_QUAD_STRIP &&
d104 1
a104 1
      mode = GL_TRIANGLE_STRIP;
d110 1
a110 1
      mode = GL_TRIANGLE_FAN;
d113 2
a114 2
   if (mode != brw->primitive) {
      brw->primitive = mode;
d117 2
a118 2
      if (reduced_prim[mode] != brw->intel.reduced_primitive) {
	 brw->intel.reduced_primitive = reduced_prim[mode];
d122 6
d129 8
a136 1
   return prim_to_hw_prim[mode];
d140 8
a162 1
   struct intel_context *intel = &brw->intel;
d182 5
a186 1
   verts_per_instance = trim(prim->mode, prim->count);
d197 2
a198 2
   if (intel->always_flush_cache) {
      intel_batchbuffer_emit_mi_flush(intel);
d207 2
a208 2
   OUT_BATCH(1); // instance count
   OUT_BATCH(0); // start instance location
d212 1
a212 1
   intel->batch.need_workaround_flush = true;
d214 2
a215 2
   if (intel->always_flush_cache) {
      intel_batchbuffer_emit_mi_flush(intel);
a222 1
   struct intel_context *intel = &brw->intel;
d242 1
a242 1
   verts_per_instance = trim(prim->mode, prim->count);
d253 2
a254 2
   if (intel->always_flush_cache) {
      intel_batchbuffer_emit_mi_flush(intel);
d262 2
a263 2
   OUT_BATCH(1); // instance count
   OUT_BATCH(0); // start instance location
d267 2
a268 2
   if (intel->always_flush_cache) {
      intel_batchbuffer_emit_mi_flush(intel);
a275 1
   struct brw_vertex_info old = brw->vb.info;
a283 2
   memset(&brw->vb.info, 0, sizeof(brw->vb.info));

d288 2
d291 31
a321 3
      if (arrays[i]->StrideB != 0)
	 brw->vb.info.sizes[i/16] |= (brw->vb.inputs[i].glarray->Size - 1) <<
	    ((i%16) * 2);
d323 15
d339 13
a351 3
   /* Raise statechanges if input sizes have changed. */
   if (memcmp(brw->vb.info.sizes, old.sizes, sizeof(old.sizes)) != 0)
      brw->state.dirty.brw |= BRW_NEW_INPUT_DIMENSIONS;
d357 1
a357 1
static GLboolean brw_try_draw_prims( struct gl_context *ctx,
a364 1
   struct intel_context *intel = intel_context(ctx);
d366 1
a366 2
   GLboolean retval = GL_FALSE;
   GLboolean warn = GL_FALSE;
d368 1
d382 13
a405 9
   /* Have to validate state quite late.  Will rebuild tnl_program,
    * which depends on varying information.  
    * 
    * Note this is where brw->vs->prog_data.inputs_read is calculated,
    * so can't access it earlier.
    */

   intel_prepare_render(intel);

a406 1
      uint32_t hw_prim;
d421 15
a435 1
      intel_batchbuffer_require_space(intel, estimated_max_prim_size, false);
d437 6
a442 1
      hw_prim = brw_set_prim(brw, &prim[i]);
d444 1
a444 30
	 brw_validate_state(brw);

	 /* Various fallback checks:  */
	 if (brw->intel.Fallback)
	    goto out;

	 /* Check that we can fit our state in with our existing batchbuffer, or
	  * flush otherwise.
	  */
	 if (dri_bufmgr_check_aperture_space(brw->state.validated_bos,
					     brw->state.validated_bo_count)) {
	    static GLboolean warned;
	    intel_batchbuffer_flush(intel);

	    /* Validate the state after we flushed the batch (which would have
	     * changed the set of dirty state).  If we still fail to
	     * check_aperture, warn of what's happening, but attempt to continue
	     * on since it may succeed anyway, and the user would probably rather
	     * see a failure and a warning than a fallback.
	     */
	    brw_validate_state(brw);
	    if (!warned &&
		dri_bufmgr_check_aperture_space(brw->state.validated_bos,
						brw->state.validated_bo_count)) {
	       warn = GL_TRUE;
	       warned = GL_TRUE;
	    }
	 }

	 intel->no_batch_wrap = GL_TRUE;
d448 2
a449 2
      if (intel->gen >= 7)
	 gen7_emit_prim(brw, &prim[i], hw_prim);
d451 3
a453 1
	 brw_emit_prim(brw, &prim[i], hw_prim);
d455 15
a469 1
      intel->no_batch_wrap = GL_FALSE;
d471 4
a474 1
      retval = GL_TRUE;
d477 2
a478 3
   if (intel->always_flush_batch)
      intel_batchbuffer_flush(intel);
 out:
d481 1
a481 7

   if (warn)
      fprintf(stderr, "i965: Single primitive emit potentially exceeded "
	      "available aperture space\n");

   if (!retval)
      DBG("%s failed\n", __FUNCTION__);
a486 1
		     const struct gl_client_array *arrays[],
d492 2
a493 1
		     GLuint max_index )
d495 2
a496 1
   GLboolean retval;
d501 4
a504 14
   if (!vbo_all_varyings_in_vbos(arrays)) {
      if (!index_bounds_valid)
	 vbo_get_minmax_index(ctx, prim, ib, &min_index, &max_index);

      /* Decide if we want to rebase.  If so we end up recursing once
       * only into this function.
       */
      if (min_index != 0 && !vbo_any_varyings_in_vbos(arrays)) {
	 vbo_rebase_prims(ctx, arrays,
			  prim, nr_prims,
			  ib, min_index, max_index,
			  brw_draw_prims );
	 return;
      }
d507 3
a509 1
   /* Make a first attempt at drawing:
d511 2
a512 1
   retval = brw_try_draw_prims(ctx, arrays, prim, nr_prims, ib, min_index, max_index);
d514 2
a515 3
   /* Otherwise, we really are out of memory.  Pass the drawing
    * command to the software tnl module and which will in turn call
    * swrast to do the drawing.
d517 5
a521 3
   if (!retval) {
       _swsetup_Wakeup(ctx);
       _tnl_wakeup(ctx);
d523 1
d526 5
d535 1
a535 1
   struct gl_context *ctx = &brw->intel.ctx;
@


1.6
log
@Merge Mesa 7.10.3
@
text
@d31 2
a133 1
   struct brw_3d_primitive prim_packet;
d135 4
d143 16
a158 13
   prim_packet.header.opcode = CMD_3D_PRIM;
   prim_packet.header.length = sizeof(prim_packet)/4 - 2;
   prim_packet.header.pad = 0;
   prim_packet.header.topology = hw_prim;
   prim_packet.header.indexed = prim->indexed;

   prim_packet.verts_per_instance = trim(prim->mode, prim->count);
   prim_packet.start_vert_location = prim->start;
   if (prim->indexed)
      prim_packet.start_vert_location += brw->ib.start_vertex_offset;
   prim_packet.instance_count = 1;
   prim_packet.start_instance_location = 0;
   prim_packet.base_vert_location = prim->basevertex;
d166 1
a166 1
      intel_batchbuffer_emit_mi_flush(intel->batch);
d168 56
a223 3
   if (prim_packet.verts_per_instance) {
      intel_batchbuffer_data( brw->intel.batch, &prim_packet,
			      sizeof(prim_packet), false);
d225 11
d237 1
a237 1
      intel_batchbuffer_emit_mi_flush(intel->batch);
d241 1
d248 5
a252 2
   for (i = 0; i < VERT_ATTRIB_MAX; i++)
      drm_intel_bo_unreference(brw->vb.inputs[i].bo);
a253 1
   memset(&brw->vb.inputs, 0, sizeof(brw->vb.inputs));
d257 1
a270 96
/* XXX: could split the primitive list to fallback only on the
 * non-conformant primitives.
 */
static GLboolean check_fallbacks( struct brw_context *brw,
				  const struct _mesa_prim *prim,
				  GLuint nr_prims )
{
   struct gl_context *ctx = &brw->intel.ctx;
   GLuint i;

   /* If we don't require strict OpenGL conformance, never 
    * use fallbacks.  If we're forcing fallbacks, always
    * use fallfacks.
    */
   if (brw->intel.conformance_mode == 0)
      return GL_FALSE;

   if (brw->intel.conformance_mode == 2)
      return GL_TRUE;

   if (ctx->Polygon.SmoothFlag) {
      for (i = 0; i < nr_prims; i++)
	 if (reduced_prim[prim[i].mode] == GL_TRIANGLES) 
	    return GL_TRUE;
   }

   /* BRW hardware will do AA lines, but they are non-conformant it
    * seems.  TBD whether we keep this fallback:
    */
   if (ctx->Line.SmoothFlag) {
      for (i = 0; i < nr_prims; i++)
	 if (reduced_prim[prim[i].mode] == GL_LINES) 
	    return GL_TRUE;
   }

   /* Stipple -- these fallbacks could be resolved with a little
    * bit of work?
    */
   if (ctx->Line.StippleFlag) {
      for (i = 0; i < nr_prims; i++) {
	 /* GS doesn't get enough information to know when to reset
	  * the stipple counter?!?
	  */
	 if (prim[i].mode == GL_LINE_LOOP || prim[i].mode == GL_LINE_STRIP) 
	    return GL_TRUE;
	    
	 if (prim[i].mode == GL_POLYGON &&
	     (ctx->Polygon.FrontMode == GL_LINE ||
	      ctx->Polygon.BackMode == GL_LINE))
	    return GL_TRUE;
      }
   }

   if (ctx->Point.SmoothFlag) {
      for (i = 0; i < nr_prims; i++)
	 if (prim[i].mode == GL_POINTS) 
	    return GL_TRUE;
   }

   /* BRW hardware doesn't handle GL_CLAMP texturing correctly;
    * brw_wm_sampler_state:translate_wrap_mode() treats GL_CLAMP
    * as GL_CLAMP_TO_EDGE instead.  If we're using GL_CLAMP, and
    * we want strict conformance, force the fallback.
    * Right now, we only do this for 2D textures.
    */
   {
      int u;
      for (u = 0; u < ctx->Const.MaxTextureCoordUnits; u++) {
         struct gl_texture_unit *texUnit = &ctx->Texture.Unit[u];
         if (texUnit->Enabled) {
            if (texUnit->Enabled & TEXTURE_1D_BIT) {
               if (texUnit->CurrentTex[TEXTURE_1D_INDEX]->WrapS == GL_CLAMP) {
                   return GL_TRUE;
               }
            }
            if (texUnit->Enabled & TEXTURE_2D_BIT) {
               if (texUnit->CurrentTex[TEXTURE_2D_INDEX]->WrapS == GL_CLAMP ||
                   texUnit->CurrentTex[TEXTURE_2D_INDEX]->WrapT == GL_CLAMP) {
                   return GL_TRUE;
               }
            }
            if (texUnit->Enabled & TEXTURE_3D_BIT) {
               if (texUnit->CurrentTex[TEXTURE_3D_INDEX]->WrapS == GL_CLAMP ||
                   texUnit->CurrentTex[TEXTURE_3D_INDEX]->WrapT == GL_CLAMP ||
                   texUnit->CurrentTex[TEXTURE_3D_INDEX]->WrapR == GL_CLAMP) {
                   return GL_TRUE;
               }
            }
         }
      }
   }
      
   /* Nothing stopping us from the fast path now */
   return GL_FALSE;
}

a285 1
   GLboolean first_time = GL_TRUE;
a299 3
   if (check_fallbacks(brw, prim, nr_prims))
      return GL_FALSE;

d322 9
d334 1
a334 4
       * primitives.  This fraction is just a guess (minimal full state plus
       * a primitive is around 512 bytes), and would be better if we had
       * an upper bound of how much we might emit in a single
       * brw_try_draw_prims().
d336 1
a336 2
      intel_batchbuffer_require_space(intel->batch, intel->batch->size / 4,
				      false);
d339 1
a339 4

      if (first_time || (brw->state.dirty.brw & BRW_NEW_PRIMITIVE)) {
	 first_time = GL_FALSE;

d352 1
a352 1
	    intel_batchbuffer_flush(intel->batch);
d373 4
a376 1
      brw_emit_prim(brw, &prim[i], hw_prim);
d384 1
a384 1
      intel_batchbuffer_flush(intel->batch);
d410 3
d439 1
d449 1
d454 5
d465 3
a467 3
   if (brw->vb.upload.bo != NULL) {
      drm_intel_bo_unreference(brw->vb.upload.bo);
      brw->vb.upload.bo = NULL;
d469 1
d471 2
a472 3
   for (i = 0; i < VERT_ATTRIB_MAX; i++) {
      drm_intel_bo_unreference(brw->vb.inputs[i].bo);
      brw->vb.inputs[i].bo = NULL;
d474 1
@


1.5
log
@Update to Mesa 7.8.1. Tested on a bulk ports build by naddy@@, ok oga@@.
@
text
@d45 1
a45 1
#define FILE_DEBUG_FLAG DEBUG_BATCH
d80 2
a81 1
static GLuint brw_set_prim(struct brw_context *brw, GLenum prim)
d83 4
a86 1
   GLcontext *ctx = &brw->intel.ctx;
a87 3
   if (INTEL_DEBUG & DEBUG_PRIMS)
      printf("PRIM: %s\n", _mesa_lookup_enum_by_nr(prim));
   
d90 1
a90 1
   if (prim == GL_QUAD_STRIP &&
d94 8
a101 1
      prim = GL_TRIANGLE_STRIP;
d103 2
a104 2
   if (prim != brw->primitive) {
      brw->primitive = prim;
d107 2
a108 2
      if (reduced_prim[prim] != brw->intel.reduced_primitive) {
	 brw->intel.reduced_primitive = reduced_prim[prim];
d113 1
a113 1
   return prim_to_hw_prim[prim];
d135 2
a136 3
   if (INTEL_DEBUG & DEBUG_PRIMS)
      printf("PRIM: %s %d %d\n", _mesa_lookup_enum_by_nr(prim->mode), 
		   prim->start, prim->count);
a151 3
   /* Can't wrap here, since we rely on the validated state. */
   intel->no_batch_wrap = GL_TRUE;

d162 1
a162 1
			      sizeof(prim_packet));
a166 2

   intel->no_batch_wrap = GL_FALSE;
d176 1
a176 1
      dri_bo_unreference(brw->vb.inputs[i].bo);
d202 1
a202 1
   GLcontext *ctx = &brw->intel.ctx;
d294 1
a294 1
static GLboolean brw_try_draw_prims( GLcontext *ctx,
d354 2
a355 1
      intel_batchbuffer_require_space(intel->batch, intel->batch->size / 4);
d357 1
a357 1
      hw_prim = brw_set_prim(brw, prim[i].mode);
d391 1
d397 2
d418 1
a418 1
void brw_draw_prims( GLcontext *ctx,
d436 1
a436 1
      if (min_index != 0) {
d462 1
a462 1
   GLcontext *ctx = &brw->intel.ctx;
d475 1
a475 1
      dri_bo_unreference(brw->vb.upload.bo);
d480 1
a480 1
      dri_bo_unreference(brw->vb.inputs[i].bo);
d484 1
a484 1
   dri_bo_unreference(brw->ib.bo);
@


1.4
log
@Update to Mesa 7.4.2. Tested by oga@@, ckuethe@@ and naddy@@.
@
text
@a27 1
#include <stdlib.h>
a31 1
#include "main/api_validate.h"
d33 4
a41 1
#include "brw_fallback.h"
a43 6
#include "intel_buffer_objects.h"

#include "tnl/tnl.h"
#include "vbo/vbo_context.h"
#include "swrast/swrast.h"
#include "swrast_setup/swrast_setup.h"
d85 1
a85 1
      _mesa_printf("PRIM: %s\n", _mesa_lookup_enum_by_nr(prim));
d125 1
d128 1
a128 1
      _mesa_printf("PRIM: %s %d %d\n", _mesa_lookup_enum_by_nr(prim->mode), 
d139 2
d143 1
a143 1
   prim_packet.base_vert_location = 0;
d146 10
a155 1
   brw->no_batch_wrap = GL_TRUE;
d158 4
a161 1
			      sizeof(prim_packet), LOOP_CLIPRECTS);
d163 2
a164 1
   brw->no_batch_wrap = GL_FALSE;
d181 1
a183 2
	 brw->vb.info.varying |= 1 << i;

d188 1
a188 2
   /* Raise statechanges if input sizes and varying have changed: 
    */
a190 3

   if (brw->vb.info.varying != old.varying)
      brw->state.dirty.brw |= BRW_NEW_INPUT_VARYING;
d340 1
a340 6
   LOCK_HARDWARE(intel);

   if (!intel->constant_cliprect && intel->driDrawable->numClipRects == 0) {
      UNLOCK_HARDWARE(intel);
      return GL_TRUE;
   }
d352 1
a352 2
      intel_batchbuffer_require_space(intel->batch, intel->batch->size / 4,
				      LOOP_CLIPRECTS);
d396 2
d399 2
a400 1
   UNLOCK_HARDWARE(intel);
a411 28
static GLboolean brw_need_rebase( GLcontext *ctx,
				  const struct gl_client_array *arrays[],
				  const struct _mesa_index_buffer *ib,
				  GLuint min_index )
{
   if (min_index == 0) 
      return GL_FALSE;

   if (ib) {
      if (!vbo_all_varyings_in_vbos(arrays))
	 return GL_TRUE;
      else
	 return GL_FALSE;
   }
   else {
      /* Hmm.  This isn't quite what I wanted.  BRW can actually
       * handle the mixed case well enough that we shouldn't need to
       * rebase.  However, it's probably not very common, nor hugely
       * expensive to do it this way:
       */
      if (!vbo_all_varyings_in_vbos(arrays))
	 return GL_TRUE;
      else
	 return GL_FALSE;
   }
}
				  

d417 1
d423 14
a436 10
   /* Decide if we want to rebase.  If so we end up recursing once
    * only into this function.
    */
   if (brw_need_rebase( ctx, arrays, ib, min_index )) {
      vbo_rebase_prims( ctx, arrays, 
			prim, nr_prims, 
			ib, min_index, max_index, 
			brw_draw_prims );
      
      return;
@


1.3
log
@Mesa 7.2, Tested by ckuethe@@, naddy@@, oga@@, and others.
@
text
@d30 5
a34 5
#include "glheader.h"
#include "context.h"
#include "state.h"
#include "api_validate.h"
#include "enums.h"
a41 1
#include "intel_ioctl.h"
d52 1
a52 1
static GLuint hw_prim[GL_POLYGON+1] = {
d85 1
a85 1
static GLuint brw_set_prim(struct brw_context *brw, GLenum prim, GLboolean *need_flush)
d87 2
a88 1
   int ret;
d95 3
a97 3
       brw->attribs.Light->ShadeModel != GL_FLAT &&
       brw->attribs.Polygon->FrontMode == GL_FILL &&
       brw->attribs.Polygon->BackMode == GL_FILL)
a107 4

      ret = brw_validate_state(brw);
      if (ret)
         *need_flush = GL_TRUE;
d110 1
a110 1
   return hw_prim[prim];
d125 3
a127 3
static void brw_emit_prim( struct brw_context *brw, 
			   const struct _mesa_prim *prim )

a129 1
   GLboolean need_flush = GL_FALSE;
d138 1
a138 1
   prim_packet.header.topology = brw_set_prim(brw, prim->mode, &need_flush);
d147 2
d153 1
a153 2

   assert(need_flush == GL_FALSE);
a158 1
   struct brw_vertex_element *inputs = brw->vb.inputs;
d162 4
a165 1
   memset(inputs, 0, sizeof(*inputs));
d171 2
a172 4
      /* XXX: metaops passes null arrays */
      if (arrays[i]) {
	 if (arrays[i]->StrideB != 0)
	    brw->vb.info.varying |= 1 << i;
d174 2
a175 2
	 brw->vb.info.sizes[i/16] |= (inputs[i].glarray->Size - 1) << ((i%16) * 2);
      }
d194 1
d197 5
a201 1
   if (!brw->intel.strict_conformance)
d204 4
a207 1
   if (brw->attribs.Polygon->SmoothFlag) {
d216 1
a216 1
   if (brw->attribs.Line->SmoothFlag) {
d225 1
a225 1
   if (brw->attribs.Line->StippleFlag) {
d230 1
a230 1
	 if (prim[i].mode == GL_LINE_LOOP) 
d234 2
a235 2
	     (brw->attribs.Polygon->FrontMode == GL_LINE ||
	      brw->attribs.Polygon->BackMode == GL_LINE))
d240 1
a240 2

   if (brw->attribs.Point->SmoothFlag) {
d245 33
d279 1
d297 2
a299 4
   GLuint ib_offset;
   dri_bo *ib_bo;
   GLboolean force_flush = GL_FALSE;
   int ret;
d304 7
d313 3
d319 8
a326 1
      
d336 1
a336 1
   if (brw->intel.numClipRects == 0) {
d341 3
a343 1
   {
d351 2
a352 8
   flush:
      if (force_flush)
         brw->no_batch_wrap = GL_FALSE;

      if (intel->batch->ptr - intel->batch->map > intel->batch->size * 3 / 4
	/* brw_emit_prim may change the cliprect_mode to LOOP_CLIPRECTS */
	  || intel->batch->cliprect_mode != LOOP_CLIPRECTS || (force_flush == GL_TRUE))
	      intel_batchbuffer_flush(intel->batch);
d354 1
a354 2
      force_flush = GL_FALSE;
      brw->no_batch_wrap = GL_TRUE;
d356 2
a357 3
      /* Set the first primitive early, ahead of validate_state:
       */
      brw_set_prim(brw, prim[0].mode, &force_flush);
d359 1
a359 7
      /* XXX:  Need to separate validate and upload of state.  
       */
      ret = brw_validate_state( brw );
      if (ret) {
         force_flush = GL_TRUE;
         goto flush;
      }
d361 3
a363 4
      /* Various fallback checks:
       */
      if (brw->intel.Fallback) 
	 goto out;
d365 22
a386 2
      if (check_fallbacks( brw, prim, nr_prims ))
	 goto out;
d388 1
a388 16
      /* need to account for index buffer and vertex buffer */
      if (ib) {
         ret = brw_prepare_indices( brw, ib , &ib_bo, &ib_offset);
         if (ret) {
            force_flush = GL_TRUE;
            goto flush;
         }
      }

      ret = brw_prepare_vertices( brw, min_index, max_index);
      if (ret < 0)
         goto out;

      if (ret > 0) {
         force_flush = GL_TRUE;
         goto flush;
a389 5
	  
      /* Upload index, vertex data: 
       */
      if (ib)
	brw_emit_indices( brw, ib, ib_bo, ib_offset);
d391 1
a391 5
      brw_emit_vertices( brw, min_index, max_index);

      for (i = 0; i < nr_prims; i++) {
	 brw_emit_prim(brw, &prim[i]);
      }
d397 1
d399 3
a401 3
   brw->no_batch_wrap = GL_FALSE;

   UNLOCK_HARDWARE(intel);
a458 1

d471 1
d486 2
d492 8
@


1.2
log
@Update to Mesa 7.0.2. Tested by naddy@@ (full ports build), simon@@
and oga@@ (with dri enabled).
@
text
@a38 1
#include "brw_aub.h"
d49 1
d51 1
a51 1

d86 1
a86 1
static GLuint brw_set_prim(struct brw_context *brw, GLenum prim)
d88 1
d109 3
a111 1
      brw_validate_state(brw);
a128 19
static void brw_emit_cliprect( struct brw_context *brw, 
			       const drm_clip_rect_t *rect )
{
   struct brw_drawrect bdr;

   bdr.header.opcode = CMD_DRAW_RECT;
   bdr.header.length = sizeof(bdr)/4 - 2;
   bdr.xmin = rect->x1;
   bdr.xmax = rect->x2 - 1;
   bdr.ymin = rect->y1;
   bdr.ymax = rect->y2 - 1;
   bdr.xorg = brw->intel.drawX;
   bdr.yorg = brw->intel.drawY;

   intel_batchbuffer_data( brw->intel.batch, &bdr, sizeof(bdr), 
			   INTEL_BATCH_NO_CLIPRECTS);
}


d134 1
d143 1
a143 1
   prim_packet.header.topology = brw_set_prim(brw, prim->mode);
d153 2
a154 2
      intel_batchbuffer_data( brw->intel.batch, &prim_packet, sizeof(prim_packet), 
			      INTEL_BATCH_NO_CLIPRECTS);
d156 2
d260 5
a264 1
   GLuint i, j;
d269 2
a284 1
      assert(intel->batch->ptr == intel->batch->map + intel->batch->offset);
d290 19
d311 1
a311 1
      brw_set_prim(brw, prim[0].mode);
d315 5
a319 1
      brw_validate_state( brw );
d328 18
d350 3
a352 1
	 brw_upload_indices( brw, ib );
d354 2
a355 2
      if (!brw_upload_vertices( brw, min_index, max_index)) {
	 goto out;
a357 28
      /* For single cliprect, state is already emitted: 
       */
      if (brw->intel.numClipRects == 1) {
	 for (i = 0; i < nr_prims; i++) {
	    brw_emit_prim(brw, &prim[i]);   
	 }
      }
      else {
	 /* Otherwise, explicitly do the cliprects at this point:
	  */
          GLuint nprims = 0;
	 for (j = 0; j < brw->intel.numClipRects; j++) {
	    brw_emit_cliprect(brw, &brw->intel.pClipRects[j]);

	    /* Emit prims to batchbuffer: 
	     */
	    for (i = 0; i < nr_prims; i++) {
	       brw_emit_prim(brw, &prim[i]);   

          if (++nprims == VBO_MAX_PRIM) {
              intel_batchbuffer_flush(brw->intel.batch);
              nprims = 0;
          }
	    }
	 }
      }
      
      intel->need_flush = GL_TRUE;
d363 1
a363 27
   /* Currently have to do this to synchronize with the map/unmap of
    * the vertex buffer in brw_exec_api.c.  Not sure if there is any
    * way around this, as not every flush is due to a buffer filling
    * up.
    */
   if (!intel_batchbuffer_flush( brw->intel.batch )) {
      DBG("%s intel_batchbuffer_flush failed\n", __FUNCTION__);
      retval = GL_FALSE;
   }

   if (retval && intel->thrashing) {
      bmSetFence(intel);
   }

   /* Free any old data so it doesn't clog up texture memory - we
    * won't be referencing it again.
    */
   while (brw->vb.upload.wrap != brw->vb.upload.buf) {
      ctx->Driver.BufferData(ctx,
			     GL_ARRAY_BUFFER_ARB,
			     BRW_UPLOAD_INIT_SIZE,
			     NULL,
			     GL_DYNAMIC_DRAW_ARB,
			     brw->vb.upload.vbo[brw->vb.upload.wrap]);
      brw->vb.upload.wrap++;
      brw->vb.upload.wrap %= BRW_NR_UPLOAD_BUFS;
   }
a408 1
   struct intel_context *intel = intel_context(ctx);
a427 14
   
   /* This looks like out-of-memory but potentially we have
    * situation where there is enough memory but it has become
    * fragmented.  Clear out all heaps and start from scratch by
    * faking a contended lock event:  (done elsewhere)
    */
   if (!retval && !intel->Fallback && bmError(intel)) {
      DBG("retrying\n");
      /* Then try a second time only to upload textures and draw the
       * primitives:
       */
      retval = brw_try_draw_prims(ctx, arrays, prim, nr_prims, ib, min_index, max_index);
   }

a435 11

   if (intel->aub_file && (INTEL_DEBUG & DEBUG_SYNC)) {
      intelFinish( &intel->ctx );
      intel->aub_wrap = 1;
   }
}


static void brw_invalidate_vbo_cb( struct intel_context *intel, void *ptr )
{
   /* nothing to do, we don't rely on the contents being preserved */
a437 1

d442 1
a442 2
   GLuint i;
   
a445 21

   brw->vb.upload.size = BRW_UPLOAD_INIT_SIZE;

   for (i = 0; i < BRW_NR_UPLOAD_BUFS; i++) {
      brw->vb.upload.vbo[i] = ctx->Driver.NewBufferObject(ctx, 1, GL_ARRAY_BUFFER_ARB);
      
      /* NOTE:  These are set to no-backing-store.
       */
      bmBufferSetInvalidateCB(&brw->intel,
			      intel_bufferobj_buffer(intel_buffer_object(brw->vb.upload.vbo[i])),
			      brw_invalidate_vbo_cb,
			      &brw->intel,
			      GL_TRUE);
   }

   ctx->Driver.BufferData( ctx, 
			   GL_ARRAY_BUFFER_ARB, 
			   BRW_UPLOAD_INIT_SIZE,
			   NULL,
			   GL_DYNAMIC_DRAW_ARB,
			   brw->vb.upload.vbo[0] );
d450 4
a453 5
   GLcontext *ctx = &brw->intel.ctx;
   GLuint i;
   
   for (i = 0; i < BRW_NR_UPLOAD_BUFS; i++)
      ctx->Driver.DeleteBuffer(ctx, brw->vb.upload.vbo[i]);
@


1.1
log
@Initial revision
@
text
@a37 1
#include "brw_attrib.h"
d47 3
a49 2


d126 17
d146 1
a146 1
			   const struct brw_draw_prim *prim )
d169 1
a169 1
			      INTEL_BATCH_CLIPRECTS);
a172 24


static void update_current_size( struct gl_client_array *array)
{
   const GLfloat *ptr = (const GLfloat *)array->Ptr;

   assert(array->StrideB == 0);
   assert(array->Type == GL_FLOAT || array->Type == GL_UNSIGNED_BYTE);

   if (ptr[3] != 1.0) 
      array->Size = 4;
   else if (ptr[2] != 0.0) 
      array->Size = 3;
   else if (ptr[1] != 0.0) 
      array->Size = 2;
   else
      array->Size = 1;
}



/* Fill in any gaps in passed arrays with pointers to current
 * attributes:
 */
a175 1
   struct gl_client_array *current_values = brw->vb.current_values;
d183 9
a191 5
   for (i = 0; i < BRW_ATTRIB_MAX; i++) {
      if (arrays[i] && arrays[i]->Enabled)
      {
	 brw->vb.inputs[i].glarray = arrays[i];
	 brw->vb.info.varying |= 1 << i;
a192 7
      else 
      {
	 brw->vb.inputs[i].glarray = &current_values[i];
	 update_current_size(&current_values[i]);
      }

      brw->vb.info.sizes[i/16] |= (inputs[i].glarray->Size - 1) << ((i%16) * 2);
d204 3
d208 1
a208 1
				  const struct brw_draw_prim *prim,
d259 3
a261 1

d264 1
a264 1
				     const struct brw_draw_prim *prim,
d266 1
a266 1
				     const struct brw_draw_index_buffer *ib,
d268 1
a268 2
				     GLuint max_index,
				     GLuint flags )
d273 1
a273 1
   GLuint i;
d277 1
a277 1
      
d281 1
a281 1

d290 7
a297 2
      assert(intel->locked);

d323 1
a323 1
      /* Emit prims to batchbuffer: 
d325 23
a347 2
      for (i = 0; i < nr_prims; i++) {
	 brw_emit_prim(brw, &prim[i]);   
d350 1
d392 7
d400 27
a426 8
GLboolean brw_draw_prims( GLcontext *ctx,
			  const struct gl_client_array *arrays[],
			  const struct brw_draw_prim *prim,
			  GLuint nr_prims,
			  const struct brw_draw_index_buffer *ib,
			  GLuint min_index,
			  GLuint max_index,
			  GLuint flags )
d431 16
a446 1
   retval = brw_try_draw_prims(ctx, arrays, prim, nr_prims, ib, min_index, max_index, flags);
d449 6
a454 2
   if (!retval && bmError(intel)) {

a455 6
      /* This looks like out-of-memory but potentially we have
       * situation where there is enough memory but it has become
       * fragmented.  Clear out all heaps and start from scratch by
       * faking a contended lock event:  (done elsewhere)
       */

d459 10
a468 1
      retval = brw_try_draw_prims(ctx, arrays, prim, nr_prims, ib, min_index, max_index, flags);
d471 1
a471 1
   if (intel->aub_file) {
a474 3


   return retval;
d487 1
d490 4
d499 1
a499 1
      /* XXX: Set these to no-backing-store
a505 1

a513 3
						     
   
   brw_init_current_values(ctx, brw->vb.current_values);
@


1.1.1.1
log
@Import MesaLibs 6.5.1. (in dist/ since its code is shared between lib 
and xserver)...
@
text
@@


1.1.1.2
log
@import MesaLibs version 6.5.2
@
text
@a125 17
static void brw_emit_cliprect( struct brw_context *brw, 
			       const drm_clip_rect_t *rect )
{
   struct brw_drawrect bdr;

   bdr.header.opcode = CMD_DRAW_RECT;
   bdr.header.length = sizeof(bdr)/4 - 2;
   bdr.xmin = rect->x1;
   bdr.xmax = rect->x2 - 1;
   bdr.ymin = rect->y1;
   bdr.ymax = rect->y2 - 1;
   bdr.xorg = brw->intel.drawX;
   bdr.yorg = brw->intel.drawY;

   intel_batchbuffer_data( brw->intel.batch, &bdr, sizeof(bdr), 
			   INTEL_BATCH_NO_CLIPRECTS);
}
d152 1
a152 1
			      INTEL_BATCH_NO_CLIPRECTS);
d195 1
a195 1
	 brw->vb.info.varying |= (GLuint64EXT) 1 << i;
d280 1
a280 1
   GLuint i, j;
a296 7

   if (brw->intel.numClipRects == 0) {
      assert(intel->batch->ptr == intel->batch->map + intel->batch->offset);
      UNLOCK_HARDWARE(intel);
      return GL_TRUE;
   }

a298 2
      

d325 1
a325 1
      /* For single cliprect, state is already emitted: 
d327 2
a328 17
      if (brw->intel.numClipRects == 1) {
	 for (i = 0; i < nr_prims; i++) {
	    brw_emit_prim(brw, &prim[i]);   
	 }
      }
      else {
	 /* Otherwise, explicitly do the cliprects at this point:
	  */
	 for (j = 0; j < brw->intel.numClipRects; j++) {
	    brw_emit_cliprect(brw, &brw->intel.pClipRects[j]);

	    /* Emit prims to batchbuffer: 
	     */
	    for (i = 0; i < nr_prims; i++) {
	       brw_emit_prim(brw, &prim[i]);   
	    }
	 }
a330 1
      intel->need_flush = GL_TRUE;
d403 1
a403 1
   if (intel->aub_file && (INTEL_DEBUG & DEBUG_SYNC)) {
@


1.1.1.3
log
@Mesa 7.0.1
@
text
@d38 1
d48 2
a49 3
#include "tnl/tnl.h"
#include "vbo/vbo_context.h"
#include "swrast/swrast.h"
d146 1
a146 1
			   const struct _mesa_prim *prim )
d173 24
d200 1
d208 11
a218 7
   for (i = 0; i < VERT_ATTRIB_MAX; i++) {
      brw->vb.inputs[i].glarray = arrays[i];

      /* XXX: metaops passes null arrays */
      if (arrays[i]) {
	 if (arrays[i]->StrideB != 0)
	    brw->vb.info.varying |= 1 << i;
d220 1
a220 2
	 brw->vb.info.sizes[i/16] |= (inputs[i].glarray->Size - 1) << ((i%16) * 2);
      }
a231 3
/* XXX: could split the primitive list to fallback only on the
 * non-conformant primitives.
 */
d233 1
a233 1
				  const struct _mesa_prim *prim,
d284 1
a284 3
/* May fail if out of video memory for texture or vbo upload, or on
 * fallback conditions.
 */
d287 1
a287 1
				     const struct _mesa_prim *prim,
d289 1
a289 1
				     const struct _mesa_index_buffer *ib,
d291 2
a292 1
				     GLuint max_index )
d301 1
a301 1

d305 1
a305 1
      
d322 4
a413 7
static GLboolean brw_need_rebase( GLcontext *ctx,
				  const struct gl_client_array *arrays[],
				  const struct _mesa_index_buffer *ib,
				  GLuint min_index )
{
   if (min_index == 0) 
      return GL_FALSE;
d415 8
a422 27
   if (ib) {
      if (!vbo_all_varyings_in_vbos(arrays))
	 return GL_TRUE;
      else
	 return GL_FALSE;
   }
   else {
      /* Hmm.  This isn't quite what I wanted.  BRW can actually
       * handle the mixed case well enough that we shouldn't need to
       * rebase.  However, it's probably not very common, nor hugely
       * expensive to do it this way:
       */
      if (!vbo_all_varyings_in_vbos(arrays))
	 return GL_TRUE;
      else
	 return GL_FALSE;
   }
}
				  

void brw_draw_prims( GLcontext *ctx,
		     const struct gl_client_array *arrays[],
		     const struct _mesa_prim *prim,
		     GLuint nr_prims,
		     const struct _mesa_index_buffer *ib,
		     GLuint min_index,
		     GLuint max_index )
d427 1
a427 11
   /* Decide if we want to rebase.  If so we end up recursing once
    * only into this function.
    */
   if (brw_need_rebase( ctx, arrays, ib, min_index )) {
      vbo_rebase_prims( ctx, arrays, 
			prim, nr_prims, 
			ib, min_index, max_index, 
			brw_draw_prims );
      
      return;
   }
d429 2
d432 6
a437 3
   /* Make a first attempt at drawing:
    */
   retval = brw_try_draw_prims(ctx, arrays, prim, nr_prims, ib, min_index, max_index);
a438 8
   
   /* This looks like out-of-memory but potentially we have
    * situation where there is enough memory but it has become
    * fragmented.  Clear out all heaps and start from scratch by
    * faking a contended lock event:  (done elsewhere)
    */
   if (!retval && !intel->Fallback && bmError(intel)) {
      DBG("retrying\n");
d442 1
a442 10
      retval = brw_try_draw_prims(ctx, arrays, prim, nr_prims, ib, min_index, max_index);
   }

   /* Otherwise, we really are out of memory.  Pass the drawing
    * command to the software tnl module and which will in turn call
    * swrast to do the drawing.
    */
   if (!retval) {
       _swsetup_Wakeup(ctx);
      _tnl_draw_prims(ctx, arrays, prim, nr_prims, ib, min_index, max_index);
d449 3
a463 1
   struct vbo_context *vbo = vbo_context(ctx);
a465 4
   /* Register our drawing function: 
    */
   vbo->draw_prims = brw_draw_prims;

d471 1
a471 1
      /* NOTE:  These are set to no-backing-store.
d478 1
d487 3
@


1.1.1.4
log
@Import Mesa 7.10.3
@
text
@d28 1
d30 5
a34 8
#include "main/glheader.h"
#include "main/context.h"
#include "main/state.h"
#include "main/enums.h"
#include "tnl/tnl.h"
#include "vbo/vbo_context.h"
#include "swrast/swrast.h"
#include "swrast_setup/swrast_setup.h"
d39 1
d41 1
d43 1
d45 5
a50 1
#define FILE_DEBUG_FLAG DEBUG_PRIMS
d52 2
a53 1
static GLuint prim_to_hw_prim[GL_POLYGON+1] = {
d86 1
a86 2
static GLuint brw_set_prim(struct brw_context *brw,
			   const struct _mesa_prim *prim)
d88 3
a90 5
   struct gl_context *ctx = &brw->intel.ctx;
   GLenum mode = prim->mode;

   DBG("PRIM: %s\n", _mesa_lookup_enum_by_nr(prim->mode));

d93 5
a97 12
   if (mode == GL_QUAD_STRIP &&
       ctx->Light.ShadeModel != GL_FLAT &&
       ctx->Polygon.FrontMode == GL_FILL &&
       ctx->Polygon.BackMode == GL_FILL)
      mode = GL_TRIANGLE_STRIP;

   if (prim->mode == GL_QUADS && prim->count == 4 &&
       ctx->Light.ShadeModel != GL_FLAT &&
       ctx->Polygon.FrontMode == GL_FILL &&
       ctx->Polygon.BackMode == GL_FILL) {
      mode = GL_TRIANGLE_FAN;
   }
d99 2
a100 2
   if (mode != brw->primitive) {
      brw->primitive = mode;
d103 2
a104 2
      if (reduced_prim[mode] != brw->intel.reduced_primitive) {
	 brw->intel.reduced_primitive = reduced_prim[mode];
d107 2
d111 1
a111 1
   return prim_to_hw_prim[mode];
d126 22
a147 3
static void brw_emit_prim(struct brw_context *brw,
			  const struct _mesa_prim *prim,
			  uint32_t hw_prim)
a149 1
   struct intel_context *intel = &brw->intel;
d151 3
a153 2
   DBG("PRIM: %s %d %d\n", _mesa_lookup_enum_by_nr(prim->mode),
       prim->start, prim->count);
d158 1
a158 1
   prim_packet.header.topology = hw_prim;
a162 2
   if (prim->indexed)
      prim_packet.start_vert_location += brw->ib.start_vertex_offset;
d165 1
a165 1
   prim_packet.base_vert_location = prim->basevertex;
a166 8
   /* If we're set to always flush, do it before and after the primitive emit.
    * We want to catch both missed flushes that hurt instruction/state cache
    * and missed flushes of the render cache as it heads to other parts of
    * the besides the draw code.
    */
   if (intel->always_flush_cache) {
      intel_batchbuffer_emit_mi_flush(intel->batch);
   }
d168 2
a169 5
      intel_batchbuffer_data( brw->intel.batch, &prim_packet,
			      sizeof(prim_packet), false);
   }
   if (intel->always_flush_cache) {
      intel_batchbuffer_emit_mi_flush(intel->batch);
d176 1
d180 1
a180 4
   for (i = 0; i < VERT_ATTRIB_MAX; i++)
      drm_intel_bo_unreference(brw->vb.inputs[i].bo);

   memset(&brw->vb.inputs, 0, sizeof(brw->vb.inputs));
a184 1
      brw->vb.inputs[i].attrib = (gl_vert_attrib) i;
d186 7
a192 3
      if (arrays[i]->StrideB != 0)
	 brw->vb.info.sizes[i/16] |= (brw->vb.inputs[i].glarray->Size - 1) <<
	    ((i%16) * 2);
d195 2
a196 1
   /* Raise statechanges if input sizes have changed. */
d199 3
a210 1
   struct gl_context *ctx = &brw->intel.ctx;
d213 1
a213 5
   /* If we don't require strict OpenGL conformance, never 
    * use fallbacks.  If we're forcing fallbacks, always
    * use fallfacks.
    */
   if (brw->intel.conformance_mode == 0)
d216 1
a216 4
   if (brw->intel.conformance_mode == 2)
      return GL_TRUE;

   if (ctx->Polygon.SmoothFlag) {
d225 1
a225 1
   if (ctx->Line.SmoothFlag) {
d234 1
a234 1
   if (ctx->Line.StippleFlag) {
d239 1
a239 1
	 if (prim[i].mode == GL_LINE_LOOP || prim[i].mode == GL_LINE_STRIP) 
d243 2
a244 2
	     (ctx->Polygon.FrontMode == GL_LINE ||
	      ctx->Polygon.BackMode == GL_LINE))
d249 2
a250 1
   if (ctx->Point.SmoothFlag) {
a254 33

   /* BRW hardware doesn't handle GL_CLAMP texturing correctly;
    * brw_wm_sampler_state:translate_wrap_mode() treats GL_CLAMP
    * as GL_CLAMP_TO_EDGE instead.  If we're using GL_CLAMP, and
    * we want strict conformance, force the fallback.
    * Right now, we only do this for 2D textures.
    */
   {
      int u;
      for (u = 0; u < ctx->Const.MaxTextureCoordUnits; u++) {
         struct gl_texture_unit *texUnit = &ctx->Texture.Unit[u];
         if (texUnit->Enabled) {
            if (texUnit->Enabled & TEXTURE_1D_BIT) {
               if (texUnit->CurrentTex[TEXTURE_1D_INDEX]->WrapS == GL_CLAMP) {
                   return GL_TRUE;
               }
            }
            if (texUnit->Enabled & TEXTURE_2D_BIT) {
               if (texUnit->CurrentTex[TEXTURE_2D_INDEX]->WrapS == GL_CLAMP ||
                   texUnit->CurrentTex[TEXTURE_2D_INDEX]->WrapT == GL_CLAMP) {
                   return GL_TRUE;
               }
            }
            if (texUnit->Enabled & TEXTURE_3D_BIT) {
               if (texUnit->CurrentTex[TEXTURE_3D_INDEX]->WrapS == GL_CLAMP ||
                   texUnit->CurrentTex[TEXTURE_3D_INDEX]->WrapT == GL_CLAMP ||
                   texUnit->CurrentTex[TEXTURE_3D_INDEX]->WrapR == GL_CLAMP) {
                   return GL_TRUE;
               }
            }
         }
      }
   }
a255 1
   /* Nothing stopping us from the fast path now */
d262 1
a262 1
static GLboolean brw_try_draw_prims( struct gl_context *ctx,
d273 1
a273 3
   GLboolean warn = GL_FALSE;
   GLboolean first_time = GL_TRUE;
   GLuint i;
a277 12
   /* We have to validate the textures *before* checking for fallbacks;
    * otherwise, the software fallback won't be able to rely on the
    * texture state, the firstLevel and lastLevel fields won't be
    * set in the intel texture object (they'll both be 0), and the 
    * software fallback will segfault if it attempts to access any
    * texture level other than level 0.
    */
   brw_validate_textures( brw );

   if (check_fallbacks(brw, prim, nr_prims))
      return GL_FALSE;

d281 1
a281 8

   brw->ib.ib = ib;
   brw->state.dirty.brw |= BRW_NEW_INDICES;

   brw->vb.min_index = min_index;
   brw->vb.max_index = max_index;
   brw->state.dirty.brw |= BRW_NEW_VERTICES;

d289 1
a289 1
   intel_prepare_render(intel);
d291 5
a295 2
   for (i = 0; i < nr_prims; i++) {
      uint32_t hw_prim;
d297 2
a298 6
      /* Flush the batch if it's approaching full, so that we don't wrap while
       * we've got validated state that needs to be in the same batch as the
       * primitives.  This fraction is just a guess (minimal full state plus
       * a primitive is around 512 bytes), and would be better if we had
       * an upper bound of how much we might emit in a single
       * brw_try_draw_prims().
d300 1
a300 2
      intel_batchbuffer_require_space(intel->batch, intel->batch->size / 4,
				      false);
d302 3
a304 1
      hw_prim = brw_set_prim(brw, &prim[i]);
d306 4
a309 2
      if (first_time || (brw->state.dirty.brw & BRW_NEW_PRIMITIVE)) {
	 first_time = GL_FALSE;
d311 7
a317 1
	 brw_validate_state(brw);
d319 3
a321 3
	 /* Various fallback checks:  */
	 if (brw->intel.Fallback)
	    goto out;
d323 9
a331 2
	 /* Check that we can fit our state in with our existing batchbuffer, or
	  * flush otherwise.
d333 4
a336 10
	 if (dri_bufmgr_check_aperture_space(brw->state.validated_bos,
					     brw->state.validated_bo_count)) {
	    static GLboolean warned;
	    intel_batchbuffer_flush(intel->batch);

	    /* Validate the state after we flushed the batch (which would have
	     * changed the set of dirty state).  If we still fail to
	     * check_aperture, warn of what's happening, but attempt to continue
	     * on since it may succeed anyway, and the user would probably rather
	     * see a failure and a warning than a fallback.
d338 2
a339 6
	    brw_validate_state(brw);
	    if (!warned &&
		dri_bufmgr_check_aperture_space(brw->state.validated_bos,
						brw->state.validated_bo_count)) {
	       warn = GL_TRUE;
	       warned = GL_TRUE;
a341 3

	 intel->no_batch_wrap = GL_TRUE;
	 brw_upload_state(brw);
d343 4
d348 1
a348 1
      brw_emit_prim(brw, &prim[i], hw_prim);
d350 9
a358 1
      intel->no_batch_wrap = GL_FALSE;
d360 2
a361 1
      retval = GL_TRUE;
d364 13
a376 3
   if (intel->always_flush_batch)
      intel_batchbuffer_flush(intel->batch);
 out:
d378 1
a378 5
   brw_state_cache_check_size(brw);

   if (warn)
      fprintf(stderr, "i965: Single primitive emit potentially exceeded "
	      "available aperture space\n");
d386 29
a414 1
void brw_draw_prims( struct gl_context *ctx,
a418 1
		     GLboolean index_bounds_valid,
d422 1
d425 11
a435 3
   if (!vbo_all_varyings_in_vbos(arrays)) {
      if (!index_bounds_valid)
	 vbo_get_minmax_index(ctx, prim, ib, &min_index, &max_index);
a436 11
      /* Decide if we want to rebase.  If so we end up recursing once
       * only into this function.
       */
      if (min_index != 0 && !vbo_any_varyings_in_vbos(arrays)) {
	 vbo_rebase_prims(ctx, arrays,
			  prim, nr_prims,
			  ib, min_index, max_index,
			  brw_draw_prims );
	 return;
      }
   }
d442 14
d465 10
d477 1
d480 1
a480 1
   struct gl_context *ctx = &brw->intel.ctx;
d482 2
a483 1

a486 1
}
d488 1
a488 3
void brw_draw_destroy( struct brw_context *brw )
{
   int i;
d490 10
a499 3
   if (brw->vb.upload.bo != NULL) {
      drm_intel_bo_unreference(brw->vb.upload.bo);
      brw->vb.upload.bo = NULL;
d502 7
a508 4
   for (i = 0; i < VERT_ATTRIB_MAX; i++) {
      drm_intel_bo_unreference(brw->vb.inputs[i].bo);
      brw->vb.inputs[i].bo = NULL;
   }
d510 7
a516 2
   drm_intel_bo_unreference(brw->ib.bo);
   brw->ib.bo = NULL;
@


1.1.1.5
log
@Import Mesa 9.2.0
@
text
@a27 1
#include <sys/errno.h>
a30 2
#include "main/condrender.h"
#include "main/samplerobj.h"
a32 2
#include "main/macros.h"
#include "main/transformfeedback.h"
a36 1
#include "drivers/common/meta.h"
a37 1
#include "brw_blorp.h"
a43 3
#include "intel_fbo.h"
#include "intel_mipmap_tree.h"
#include "intel_regions.h"
d80 2
a81 2
static void brw_set_prim(struct brw_context *brw,
                         const struct _mesa_prim *prim)
d83 2
a84 2
   struct gl_context *ctx = &brw->ctx;
   uint32_t hw_prim = prim_to_hw_prim[prim->mode];
d90 1
a90 1
   if (prim->mode == GL_QUAD_STRIP &&
d94 1
a94 1
      hw_prim = _3DPRIM_TRISTRIP;
d100 1
a100 1
      hw_prim = _3DPRIM_TRIFAN;
d103 2
a104 2
   if (hw_prim != brw->primitive) {
      brw->primitive = hw_prim;
d107 2
a108 2
      if (reduced_prim[prim->mode] != brw->reduced_primitive) {
	 brw->reduced_primitive = reduced_prim[prim->mode];
a111 8
}

static void gen6_set_prim(struct brw_context *brw,
                          const struct _mesa_prim *prim)
{
   uint32_t hw_prim;

   DBG("PRIM: %s\n", _mesa_lookup_enum_by_nr(prim->mode));
d113 1
a113 6
   hw_prim = prim_to_hw_prim[prim->mode];

   if (hw_prim != brw->primitive) {
      brw->primitive = hw_prim;
      brw->state.dirty.brw |= BRW_NEW_PRIMITIVE;
   }
a116 8
/**
 * The hardware is capable of removing dangling vertices on its own; however,
 * prior to Gen6, we sometimes convert quads into trifans (and quad strips
 * into tristrips), since pre-Gen6 hardware requires a GS to render quads.
 * This function manually trims dangling vertices from a draw call involving
 * quads so that those dangling vertices won't get drawn when we convert to
 * trifans/tristrips.
 */
d132 2
a133 4
   int verts_per_instance;
   int vertex_access_type;
   int start_vertex_location;
   int base_vertex_location;
d138 13
a150 20
   start_vertex_location = prim->start;
   base_vertex_location = prim->basevertex;
   if (prim->indexed) {
      vertex_access_type = GEN4_3DPRIM_VERTEXBUFFER_ACCESS_RANDOM;
      start_vertex_location += brw->ib.start_vertex_offset;
      base_vertex_location += brw->vb.start_vertex_bias;
   } else {
      vertex_access_type = GEN4_3DPRIM_VERTEXBUFFER_ACCESS_SEQUENTIAL;
      start_vertex_location += brw->vb.start_vertex_bias;
   }

   /* We only need to trim the primitive count on pre-Gen6. */
   if (brw->gen < 6)
      verts_per_instance = trim(prim->mode, prim->count);
   else
      verts_per_instance = prim->count;

   /* If nothing to emit, just return. */
   if (verts_per_instance == 0)
      return;
d157 2
a158 2
   if (brw->always_flush_cache) {
      intel_batchbuffer_emit_mi_flush(brw);
d160 3
a162 55

   BEGIN_BATCH(6);
   OUT_BATCH(CMD_3D_PRIM << 16 | (6 - 2) |
	     hw_prim << GEN4_3DPRIM_TOPOLOGY_TYPE_SHIFT |
	     vertex_access_type);
   OUT_BATCH(verts_per_instance);
   OUT_BATCH(start_vertex_location);
   OUT_BATCH(prim->num_instances);
   OUT_BATCH(prim->base_instance);
   OUT_BATCH(base_vertex_location);
   ADVANCE_BATCH();

   brw->batch.need_workaround_flush = true;

   if (brw->always_flush_cache) {
      intel_batchbuffer_emit_mi_flush(brw);
   }
}

static void gen7_emit_prim(struct brw_context *brw,
			   const struct _mesa_prim *prim,
			   uint32_t hw_prim)
{
   int verts_per_instance;
   int vertex_access_type;
   int start_vertex_location;
   int base_vertex_location;

   DBG("PRIM: %s %d %d\n", _mesa_lookup_enum_by_nr(prim->mode),
       prim->start, prim->count);

   start_vertex_location = prim->start;
   base_vertex_location = prim->basevertex;
   if (prim->indexed) {
      vertex_access_type = GEN7_3DPRIM_VERTEXBUFFER_ACCESS_RANDOM;
      start_vertex_location += brw->ib.start_vertex_offset;
      base_vertex_location += brw->vb.start_vertex_bias;
   } else {
      vertex_access_type = GEN7_3DPRIM_VERTEXBUFFER_ACCESS_SEQUENTIAL;
      start_vertex_location += brw->vb.start_vertex_bias;
   }

   verts_per_instance = prim->count;

   /* If nothing to emit, just return. */
   if (verts_per_instance == 0)
      return;

   /* If we're set to always flush, do it before and after the primitive emit.
    * We want to catch both missed flushes that hurt instruction/state cache
    * and missed flushes of the render cache as it heads to other parts of
    * the besides the draw code.
    */
   if (brw->always_flush_cache) {
      intel_batchbuffer_emit_mi_flush(brw);
d164 2
a165 13

   BEGIN_BATCH(7);
   OUT_BATCH(CMD_3D_PRIM << 16 | (7 - 2));
   OUT_BATCH(hw_prim | vertex_access_type);
   OUT_BATCH(verts_per_instance);
   OUT_BATCH(start_vertex_location);
   OUT_BATCH(prim->num_instances);
   OUT_BATCH(prim->base_instance);
   OUT_BATCH(base_vertex_location);
   ADVANCE_BATCH();

   if (brw->always_flush_cache) {
      intel_batchbuffer_emit_mi_flush(brw);
a168 1

d172 1
d175 5
a179 5
   for (i = 0; i < brw->vb.nr_buffers; i++) {
      drm_intel_bo_unreference(brw->vb.buffers[i].bo);
      brw->vb.buffers[i].bo = NULL;
   }
   brw->vb.nr_buffers = 0;
a181 1
      brw->vb.inputs[i].buffer = -1;
d184 4
d189 4
d195 2
a196 7
/*
 * \brief Resolve buffers before drawing.
 *
 * Resolve the depth buffer's HiZ buffer and resolve the depth buffer of each
 * enabled depth texture.
 *
 * (In the future, this will also perform MSAA resolves).
d198 3
a200 2
static void
brw_predraw_resolve_buffers(struct brw_context *brw)
d202 44
a245 20
   struct gl_context *ctx = &brw->ctx;
   struct intel_renderbuffer *depth_irb;
   struct intel_texture_object *tex_obj;

   /* Resolve the depth buffer's HiZ buffer. */
   depth_irb = intel_get_renderbuffer(ctx->DrawBuffer, BUFFER_DEPTH);
   if (depth_irb)
      intel_renderbuffer_resolve_hiz(brw, depth_irb);

   /* Resolve depth buffer of each enabled depth texture, and color buffer of
    * each fast-clear-enabled color texture.
    */
   for (int i = 0; i < BRW_MAX_TEX_UNIT; i++) {
      if (!ctx->Texture.Unit[i]._ReallyEnabled)
	 continue;
      tex_obj = intel_texture_object(ctx->Texture.Unit[i]._Current);
      if (!tex_obj || !tex_obj->mt)
	 continue;
      intel_miptree_all_slices_resolve_depth(brw, tex_obj->mt);
      intel_miptree_resolve_color(brw, tex_obj->mt);
a246 1
}
d248 41
a288 27
/**
 * \brief Call this after drawing to mark which buffers need resolving
 *
 * If the depth buffer was written to and if it has an accompanying HiZ
 * buffer, then mark that it needs a depth resolve.
 *
 * If the color buffer is a multisample window system buffer, then
 * mark that it needs a downsample.
 */
static void brw_postdraw_set_buffers_need_resolve(struct brw_context *brw)
{
   struct gl_context *ctx = &brw->ctx;
   struct gl_framebuffer *fb = ctx->DrawBuffer;

   struct intel_renderbuffer *front_irb = NULL;
   struct intel_renderbuffer *back_irb = intel_get_renderbuffer(fb, BUFFER_BACK_LEFT);
   struct intel_renderbuffer *depth_irb = intel_get_renderbuffer(fb, BUFFER_DEPTH);

   if (brw->is_front_buffer_rendering)
      front_irb = intel_get_renderbuffer(fb, BUFFER_FRONT_LEFT);

   if (front_irb)
      intel_renderbuffer_set_needs_downsample(front_irb);
   if (back_irb)
      intel_renderbuffer_set_needs_downsample(back_irb);
   if (depth_irb && ctx->Depth.Mask)
      intel_renderbuffer_set_needs_depth_resolve(depth_irb);
d294 1
a294 1
static bool brw_try_draw_prims( struct gl_context *ctx,
d302 1
d304 3
a306 1
   bool retval = true;
a307 1
   bool fail_next = false;
d321 2
a322 12
   intel_prepare_render(brw);

   /* This workaround has to happen outside of brw_upload_state() because it
    * may flush the batchbuffer for a blit, affecting the state flags.
    */
   brw_workaround_depthstencil_alignment(brw, 0);

   /* Resolves must occur after updating renderbuffers, updating context state,
    * and finalizing textures but before setting up any hardware state for
    * this draw call.
    */
   brw_predraw_resolve_buffers(brw);
d335 9
d345 1
a345 9
      int estimated_max_prim_size;

      estimated_max_prim_size = 512; /* batchbuffer commands */
      estimated_max_prim_size += (BRW_MAX_TEX_UNIT *
				  (sizeof(struct brw_sampler_state) +
				   sizeof(struct gen5_sampler_default_color)));
      estimated_max_prim_size += 1024; /* gen6 VS push constants */
      estimated_max_prim_size += 1024; /* gen6 WM push constants */
      estimated_max_prim_size += 512; /* misc. pad */
d349 4
a352 1
       * primitives.
d354 2
a355 2
      intel_batchbuffer_require_space(brw, estimated_max_prim_size, false);
      intel_batchbuffer_save_state(brw);
d357 1
a357 23
      if (brw->num_instances != prim->num_instances) {
         brw->num_instances = prim->num_instances;
         brw->state.dirty.brw |= BRW_NEW_VERTICES;
      }
      if (brw->basevertex != prim->basevertex) {
         brw->basevertex = prim->basevertex;
         brw->state.dirty.brw |= BRW_NEW_VERTICES;
      }
      if (brw->gen < 6)
	 brw_set_prim(brw, &prim[i]);
      else
	 gen6_set_prim(brw, &prim[i]);

retry:
      /* Note that before the loop, brw->state.dirty.brw was set to != 0, and
       * that the state updated in the loop outside of this block is that in
       * *_set_prim or intel_batchbuffer_flush(), which only impacts
       * brw->state.dirty.brw.
       */
      if (brw->state.dirty.brw) {
	 brw->no_batch_wrap = true;
	 brw_upload_state(brw);
      }
d359 2
a360 22
      if (brw->gen >= 7)
	 gen7_emit_prim(brw, &prim[i], brw->primitive);
      else
	 brw_emit_prim(brw, &prim[i], brw->primitive);

      brw->no_batch_wrap = false;

      if (dri_bufmgr_check_aperture_space(&brw->batch.bo, 1)) {
	 if (!fail_next) {
	    intel_batchbuffer_reset_to_saved(brw);
	    intel_batchbuffer_flush(brw);
	    fail_next = true;
	    goto retry;
	 } else {
	    if (intel_batchbuffer_flush(brw) == -ENOSPC) {
	       static bool warned = false;

	       if (!warned) {
		  fprintf(stderr, "i965: Single primitive emit exceeded"
			  "available aperture space\n");
		  warned = true;
	       }
d362 26
a387 1
	       retval = false;
d390 3
d394 6
d402 3
a404 2
   if (brw->always_flush_batch)
      intel_batchbuffer_flush(brw);
d407 7
a413 1
   brw_postdraw_set_buffers_need_resolve(brw);
d419 1
d425 1
a425 2
		     GLuint max_index,
		     struct gl_transform_feedback_object *tfb_vertcount )
d427 1
a427 2
   struct brw_context *brw = brw_context(ctx);
   const struct gl_client_array **arrays = ctx->Array._DrawArrays;
d429 3
a431 2
   if (!_mesa_check_conditional_render(ctx))
      return;
d433 10
a442 4
   /* Handle primitive restart if needed */
   if (brw_handle_primitive_restart(ctx, prim, nr_prims, ib)) {
      /* The draw was handled, so we can exit now */
      return;
d445 1
a445 3
   /* If we're going to have to upload any of the user's vertex arrays, then
    * get the minimum and maximum of their index buffer so we know what range
    * to upload.
d447 1
a447 2
   if (!vbo_all_varyings_in_vbos(arrays) && !index_bounds_valid)
      vbo_get_minmax_indices(ctx, prim, ib, &min_index, &max_index, nr_prims);
d449 3
a451 2
   /* Do GL_SELECT and GL_FEEDBACK rendering using swrast, even though it
    * won't support all the extensions we support.
d453 2
a454 5
   if (ctx->RenderMode != GL_RENDER) {
      perf_debug("%s render mode not supported in hardware\n",
                 _mesa_lookup_enum_by_nr(ctx->RenderMode));
      _swsetup_Wakeup(ctx);
      _tnl_wakeup(ctx);
a455 1
      return;
a457 5
   /* Try drawing with the hardware, but don't do anything else if we can't
    * manage it.  swrast doesn't support our featureset, so we can't fall back
    * to it.
    */
   brw_try_draw_prims(ctx, arrays, prim, nr_prims, ib, min_index, max_index);
d462 1
a462 1
   struct gl_context *ctx = &brw->ctx;
a463 1
   int i;
a467 5

   for (i = 0; i < VERT_ATTRIB_MAX; i++)
      brw->vb.inputs[i].buffer = -1;
   brw->vb.nr_buffers = 0;
   brw->vb.nr_enabled = 0;
d474 3
a476 3
   for (i = 0; i < brw->vb.nr_buffers; i++) {
      drm_intel_bo_unreference(brw->vb.buffers[i].bo);
      brw->vb.buffers[i].bo = NULL;
a477 1
   brw->vb.nr_buffers = 0;
d479 3
a481 2
   for (i = 0; i < brw->vb.nr_enabled; i++) {
      brw->vb.enabled[i]->buffer = -1;
a482 1
   brw->vb.nr_enabled = 0;
@


1.1.1.6
log
@Import Mesa 10.2.3
@
text
@d2 2
a3 2
 *
 * Copyright 2003 VMware, Inc.
d5 1
a5 1
 *
d13 1
a13 1
 *
d17 1
a17 1
 *
d21 1
a21 1
 * IN NO EVENT SHALL VMWARE AND/OR ITS SUPPLIERS BE LIABLE FOR
d25 1
a25 1
 *
a50 1
#include "intel_buffers.h"
d53 1
a53 1
#include "intel_buffer_objects.h"
d57 1
a57 1
const GLuint prim_to_hw_prim[GL_TRIANGLE_STRIP_ADJACENCY+1] = {
d67 1
a67 5
   _3DPRIM_POLYGON,
   _3DPRIM_LINELIST_ADJ,
   _3DPRIM_LINESTRIP_ADJ,
   _3DPRIM_TRILIST_ADJ,
   _3DPRIM_TRISTRIP_ADJ,
d71 1
a71 1
static const GLenum reduced_prim[GL_POLYGON+1] = {
d154 1
a154 1
   else
a166 1
   int indirect_flag;
d174 1
a174 3
      vertex_access_type = brw->gen >= 7 ?
         GEN7_3DPRIM_VERTEXBUFFER_ACCESS_RANDOM :
         GEN4_3DPRIM_VERTEXBUFFER_ACCESS_RANDOM;
d178 1
a178 3
      vertex_access_type = brw->gen >= 7 ?
         GEN7_3DPRIM_VERTEXBUFFER_ACCESS_SEQUENTIAL :
         GEN4_3DPRIM_VERTEXBUFFER_ACCESS_SEQUENTIAL;
d189 1
a189 1
   if (verts_per_instance == 0 && !prim->is_indirect)
d201 15
a215 36
   /* If indirect, emit a bunch of loads from the indirect BO. */
   if (prim->is_indirect) {
      struct gl_buffer_object *indirect_buffer = brw->ctx.DrawIndirectBuffer;
      drm_intel_bo *bo = intel_bufferobj_buffer(brw,
            intel_buffer_object(indirect_buffer),
            prim->indirect_offset, 5 * sizeof(GLuint));

      indirect_flag = GEN7_3DPRIM_INDIRECT_PARAMETER_ENABLE;

      brw_load_register_mem(brw, GEN7_3DPRIM_VERTEX_COUNT, bo,
                            I915_GEM_DOMAIN_VERTEX, 0,
                            prim->indirect_offset + 0);
      brw_load_register_mem(brw, GEN7_3DPRIM_INSTANCE_COUNT, bo,
                            I915_GEM_DOMAIN_VERTEX, 0,
                            prim->indirect_offset + 4);

      brw_load_register_mem(brw, GEN7_3DPRIM_START_VERTEX, bo,
                            I915_GEM_DOMAIN_VERTEX, 0,
                            prim->indirect_offset + 8);
      if (prim->indexed) {
         brw_load_register_mem(brw, GEN7_3DPRIM_BASE_VERTEX, bo,
                               I915_GEM_DOMAIN_VERTEX, 0,
                               prim->indirect_offset + 12);
         brw_load_register_mem(brw, GEN7_3DPRIM_START_INSTANCE, bo,
                               I915_GEM_DOMAIN_VERTEX, 0,
                               prim->indirect_offset + 16);
      } else {
         brw_load_register_mem(brw, GEN7_3DPRIM_START_INSTANCE, bo,
                               I915_GEM_DOMAIN_VERTEX, 0,
                               prim->indirect_offset + 12);
         BEGIN_BATCH(3);
         OUT_BATCH(MI_LOAD_REGISTER_IMM | (3 - 2));
         OUT_BATCH(GEN7_3DPRIM_BASE_VERTEX);
         OUT_BATCH(0);
         ADVANCE_BATCH();
      }
d217 23
a239 2
   else {
      indirect_flag = 0;
d242 5
d248 7
a254 9
   if (brw->gen >= 7) {
      BEGIN_BATCH(7);
      OUT_BATCH(CMD_3D_PRIM << 16 | (7 - 2) | indirect_flag);
      OUT_BATCH(hw_prim | vertex_access_type);
   } else {
      BEGIN_BATCH(6);
      OUT_BATCH(CMD_3D_PRIM << 16 | (6 - 2) |
                hw_prim << GEN4_3DPRIM_TOPOLOGY_TYPE_SHIFT |
                vertex_access_type);
d256 4
a266 3
   /* Only used on Sandybridge; harmless to set elsewhere. */
   brw->batch.need_workaround_flush = true;

d287 1
d294 2
a295 2
 * Resolve the depth buffer's HiZ buffer, resolve the depth buffer of each
 * enabled depth texture, and flush the render cache for any dirty textures.
d311 5
a315 4
   /* Resolve depth buffer and render cache of each enabled texture. */
   int maxEnabledUnit = ctx->Texture._MaxEnabledTexImageUnit;
   for (int i = 0; i <= maxEnabledUnit; i++) {
      if (!ctx->Texture.Unit[i]._Current)
a321 1
      brw_render_cache_set_check_flush(brw, tex_obj->mt->bo);
a332 3
 *
 * Also mark any render targets which will be textured as needing a render
 * cache flush.
a341 2
   struct intel_renderbuffer *stencil_irb = intel_get_renderbuffer(fb, BUFFER_STENCIL);
   struct gl_renderbuffer_attachment *depth_att = &fb->Attachment[BUFFER_DEPTH];
d343 1
a343 1
   if (brw_is_front_buffer_drawing(fb))
d347 1
a347 1
      front_irb->need_downsample = true;
d349 3
a351 18
      back_irb->need_downsample = true;
   if (depth_irb && ctx->Depth.Mask) {
      intel_renderbuffer_att_set_needs_depth_resolve(depth_att);
      brw_render_cache_set_add_bo(brw, depth_irb->mt->bo);
   }

   if (ctx->Extensions.ARB_stencil_texturing &&
       stencil_irb && ctx->Stencil._WriteEnabled) {
      brw_render_cache_set_add_bo(brw, stencil_irb->mt->bo);
   }

   for (int i = 0; i < fb->_NumColorDrawBuffers; i++) {
      struct intel_renderbuffer *irb =
         intel_renderbuffer(fb->_ColorDrawBuffers[i]);

      if (irb)
         brw_render_cache_set_add_bo(brw, irb->mt->bo);
   }
d359 1
a359 1
				     const struct _mesa_prim *prims,
d363 1
a363 2
				     GLuint max_index,
				     struct gl_buffer_object *indirect)
a372 11
   /* Find the highest sampler unit used by each shader program.  A bit-count
    * won't work since ARB programs use the texture unit number as the sampler
    * index.
    */
   brw->wm.base.sampler_count =
      _mesa_fls(ctx->FragmentProgram._Current->Base.SamplersUsed);
   brw->gs.base.sampler_count = ctx->GeometryProgram._Current ?
      _mesa_fls(ctx->GeometryProgram._Current->Base.SamplersUsed) : 0;
   brw->vs.base.sampler_count =
      _mesa_fls(ctx->VertexProgram._Current->Base.SamplersUsed);

d376 1
a376 1
    * set in the intel texture object (they'll both be 0), and the
d421 1
a421 1
      intel_batchbuffer_require_space(brw, estimated_max_prim_size, RENDER_RING);
d424 2
a425 2
      if (brw->num_instances != prims[i].num_instances) {
         brw->num_instances = prims[i].num_instances;
a426 1
         brw_merge_inputs(brw, arrays);
d428 2
a429 2
      if (brw->basevertex != prims[i].basevertex) {
         brw->basevertex = prims[i].basevertex;
a430 1
         brw_merge_inputs(brw, arrays);
d433 1
a433 1
	 brw_set_prim(brw, &prims[i]);
d435 1
a435 1
	 gen6_set_prim(brw, &prims[i]);
d448 4
a451 1
      brw_emit_prim(brw, &prims[i], brw->primitive);
a474 6

      /* Now that we know we haven't run out of aperture space, we can safely
       * reset the dirty bits.
       */
      if (brw->state.dirty.brw)
         brw_clear_dirty_bits(brw);
d487 1
a487 1
		     const struct _mesa_prim *prims,
d493 1
a493 2
		     struct gl_transform_feedback_object *unused_tfb_object,
		     struct gl_buffer_object *indirect )
a497 2
   assert(unused_tfb_object == NULL);

d502 1
a502 1
   if (brw_handle_primitive_restart(ctx, prims, nr_prims, ib, indirect)) {
d507 7
d522 1
a522 2
      _tnl_draw_prims(ctx, prims, nr_prims, ib,
                      index_bounds_valid, min_index, max_index, NULL, NULL);
a525 10
   /* If we're going to have to upload any of the user's vertex arrays, then
    * get the minimum and maximum of their index buffer so we know what range
    * to upload.
    */
   if (!index_bounds_valid && !vbo_all_varyings_in_vbos(arrays)) {
      perf_debug("Scanning index buffer to compute index buffer bounds.  "
                 "Use glDrawRangeElements() to avoid this.\n");
      vbo_get_minmax_indices(ctx, prims, ib, &min_index, &max_index, nr_prims);
   }

d530 1
a530 1
   brw_try_draw_prims(ctx, arrays, prims, nr_prims, ib, min_index, max_index, indirect);
d539 1
a539 1
   /* Register our drawing function:
@


1.1.1.7
log
@Import Mesa 10.4.3
@
text
@d58 1
a58 1
static const GLuint prim_to_hw_prim[GL_TRIANGLE_STRIP_ADJACENCY+1] = {
a88 9
uint32_t
get_hw_prim_for_gl_prim(int mode)
{
   if (mode >= BRW_PRIM_OFFSET)
      return mode - BRW_PRIM_OFFSET;
   else
      return prim_to_hw_prim[mode];
}

d99 1
a99 1
   uint32_t hw_prim = get_hw_prim_for_gl_prim(prim->mode);
d136 1
a136 1
   hw_prim = get_hw_prim_for_gl_prim(prim->mode);
d170 2
d177 2
a178 3
   int start_vertex_location = prim->start;
   int base_vertex_location = prim->basevertex;

d296 34
d426 6
a444 1
      const int sampler_state_size = 16;
d447 3
a449 2
      estimated_max_prim_size += BRW_MAX_TEX_UNIT *
         (sampler_state_size + sizeof(struct gen5_sampler_default_color));
d461 1
a461 2
      if (brw->num_instances != prims[i].num_instances ||
          brw->basevertex != prims[i].basevertex) {
d463 4
d468 2
a469 4
         if (i > 0) { /* For i == 0 we just did this before the loop */
            brw->state.dirty.brw |= BRW_NEW_VERTICES;
            brw_merge_inputs(brw, arrays);
         }
a470 21

      brw->draw.gl_basevertex =
         prims[i].indexed ? prims[i].basevertex : prims[i].start;

      drm_intel_bo_unreference(brw->draw.draw_params_bo);

      if (prims[i].is_indirect) {
         /* Point draw_params_bo at the indirect buffer. */
         brw->draw.draw_params_bo =
            intel_buffer_object(ctx->DrawIndirectBuffer)->buffer;
         drm_intel_bo_reference(brw->draw.draw_params_bo);
         brw->draw.draw_params_offset =
            prims[i].indirect_offset + (prims[i].indexed ? 12 : 8);
      } else {
         /* Set draw_params_bo to NULL so brw_prepare_vertices knows it
          * has to upload gl_BaseVertex and such if they're needed.
          */
         brw->draw.draw_params_bo = NULL;
         brw->draw.draw_params_offset = 0;
      }

a476 1

a541 5

   if (ctx->Query.CondRenderQuery) {
      perf_debug("Conditional rendering is implemented in software and may "
                 "stall.  This should be fixed in the driver.\n");
   }
@


1.1.1.8
log
@Import Mesa 10.2.9
@
text
@d58 1
a58 1
const GLuint prim_to_hw_prim[GL_TRIANGLE_STRIP_ADJACENCY+1] = {
d89 9
d108 1
a108 1
   uint32_t hw_prim = prim_to_hw_prim[prim->mode];
d145 1
a145 1
   hw_prim = prim_to_hw_prim[prim->mode];
a178 2
   int start_vertex_location;
   int base_vertex_location;
d184 3
a186 2
   start_vertex_location = prim->start;
   base_vertex_location = prim->basevertex;
a303 34
/*
 * \brief Resolve buffers before drawing.
 *
 * Resolve the depth buffer's HiZ buffer, resolve the depth buffer of each
 * enabled depth texture, and flush the render cache for any dirty textures.
 *
 * (In the future, this will also perform MSAA resolves).
 */
static void
brw_predraw_resolve_buffers(struct brw_context *brw)
{
   struct gl_context *ctx = &brw->ctx;
   struct intel_renderbuffer *depth_irb;
   struct intel_texture_object *tex_obj;

   /* Resolve the depth buffer's HiZ buffer. */
   depth_irb = intel_get_renderbuffer(ctx->DrawBuffer, BUFFER_DEPTH);
   if (depth_irb)
      intel_renderbuffer_resolve_hiz(brw, depth_irb);

   /* Resolve depth buffer and render cache of each enabled texture. */
   int maxEnabledUnit = ctx->Texture._MaxEnabledTexImageUnit;
   for (int i = 0; i <= maxEnabledUnit; i++) {
      if (!ctx->Texture.Unit[i]._Current)
	 continue;
      tex_obj = intel_texture_object(ctx->Texture.Unit[i]._Current);
      if (!tex_obj || !tex_obj->mt)
	 continue;
      intel_miptree_all_slices_resolve_depth(brw, tex_obj->mt);
      intel_miptree_resolve_color(brw, tex_obj->mt);
      brw_render_cache_set_check_flush(brw, tex_obj->mt->bo);
   }
}

a399 6
   /* Resolves must occur after updating renderbuffers, updating context state,
    * and finalizing textures but before setting up any hardware state for
    * this draw call.
    */
   brw_predraw_resolve_buffers(brw);

d413 1
d416 2
a417 3
      estimated_max_prim_size += (BRW_MAX_TEX_UNIT *
				  (sizeof(struct brw_sampler_state) +
				   sizeof(struct gen5_sampler_default_color)));
d429 2
a430 1
      if (brw->num_instances != prims[i].num_instances) {
d432 5
a436 2
         brw->state.dirty.brw |= BRW_NEW_VERTICES;
         brw_merge_inputs(brw, arrays);
d438 19
a456 4
      if (brw->basevertex != prims[i].basevertex) {
         brw->basevertex = prims[i].basevertex;
         brw->state.dirty.brw |= BRW_NEW_VERTICES;
         brw_merge_inputs(brw, arrays);
d458 1
d465 1
d531 5
@


