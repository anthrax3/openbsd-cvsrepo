head	1.7;
access;
symbols
	OPENBSD_5_4:1.6.0.2
	OPENBSD_5_4_BASE:1.6
	OPENBSD_5_3:1.5.0.6
	OPENBSD_5_3_BASE:1.5
	OPENBSD_5_2:1.5.0.4
	OPENBSD_5_2_BASE:1.5
	OPENBSD_5_1_BASE:1.5
	OPENBSD_5_1:1.5.0.2
	OPENBSD_5_0:1.4.0.6
	OPENBSD_5_0_BASE:1.4
	OPENBSD_4_9:1.4.0.2
	OPENBSD_4_9_BASE:1.4
	OPENBSD_4_8:1.4.0.4
	OPENBSD_4_8_BASE:1.4
	OPENBSD_4_7:1.1.0.4
	OPENBSD_4_7_BASE:1.1
	OPENBSD_4_6:1.1.0.2
	OPENBSD_4_6_BASE:1.1;
locks; strict;
comment	@ * @;


1.7
date	2014.02.03.15.54.53;	author matthieu;	state dead;
branches;
next	1.6;

1.6
date	2013.03.18.18.38.21;	author matthieu;	state Exp;
branches;
next	1.5;

1.5
date	2011.11.29.12.39.03;	author oga;	state Exp;
branches;
next	1.4;

1.4
date	2010.07.18.14.47.48;	author oga;	state Exp;
branches;
next	1.3;

1.3
date	2010.05.23.21.28.35;	author oga;	state Exp;
branches;
next	1.2;

1.2
date	2010.05.10.22.32.30;	author oga;	state Exp;
branches;
next	1.1;

1.1
date	2009.06.25.20.16.44;	author matthieu;	state Exp;
branches;
next	;


desc
@@


1.7
log
@Update to xf86-video-intel 2.99.909
Tested by jsg@@, kettenis@@ and myself on a wide range of intel cards.
@
text
@/*
 * Copyright Â© 2008 Intel Corporation
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice (including the next
 * paragraph) shall be included in all copies or substantial portions of the
 * Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 *
 * Author:
 *    Zou Nan hai <nanhai.zou@@intel.com>
 *
 */
#include "intel_xvmc.h"
#include "i830_reg.h"
#include "i965_reg.h"
#include "brw_defines.h"
#include "brw_structs.h"
#include "intel_batchbuffer.h"
#include "intel_hwmc.h"
#define BATCH_STRUCT(x) intelBatchbufferData(&x, sizeof(x), 0)
#define URB_SIZE     256	/* XXX */

#define ARRAY_SIZE(x) (sizeof(x) / sizeof(x[0]))

enum interface {
	INTRA_INTERFACE = 0,	/* non field intra */
	NULL_INTERFACE,		/* fill with white, do nothing, for debug */
	FORWARD_INTERFACE,	/* non field forward predict */
	BACKWARD_INTERFACE,	/* non field backward predict */
	F_B_INTERFACE,		/* non field forward and backward predict */
	FIELD_FORWARD_INTERFACE,	/* field forward predict */
	FIELD_BACKWARD_INTERFACE,	/* field backward predict */
	FIELD_F_B_INTERFACE,	/* field forward and backward predict */
	DUAL_PRIME_INTERFACE
};

static const uint32_t ipicture_kernel_static[][4] = {
#include "shader/mc/ipicture.g4b"
};

static const uint32_t null_kernel_static[][4] = {
#include "shader/mc/null.g4b"
};

static const uint32_t frame_forward_kernel_static[][4] = {
#include "shader/mc/frame_forward.g4b"
};

static const uint32_t frame_backward_kernel_static[][4] = {
#include "shader/mc/frame_backward.g4b"
};

static const uint32_t frame_f_b_kernel_static[][4] = {
#include "shader/mc/frame_f_b.g4b"
};

static const uint32_t field_forward_kernel_static[][4] = {
#include "shader/mc/field_forward.g4b"
};

static const uint32_t field_backward_kernel_static[][4] = {
#include "shader/mc/field_backward.g4b"
};

static const uint32_t field_f_b_kernel_static[][4] = {
#include "shader/mc/field_f_b.g4b"
};

static const uint32_t dual_prime_kernel_static[][4] = {
#include "shader/mc/dual_prime.g4b"
};

static const uint32_t frame_forward_igd_kernel_static[][4] = {
#include "shader/mc/frame_forward_igd.g4b"
};

static const uint32_t frame_backward_igd_kernel_static[][4] = {
#include "shader/mc/frame_backward_igd.g4b"
};

static const uint32_t frame_f_b_igd_kernel_static[][4] = {
#include "shader/mc/frame_f_b_igd.g4b"
};

static const uint32_t field_forward_igd_kernel_static[][4] = {
#include "shader/mc/field_forward_igd.g4b"
};

static const uint32_t field_backward_igd_kernel_static[][4] = {
#include "shader/mc/field_backward_igd.g4b"
};

static const uint32_t field_f_b_igd_kernel_static[][4] = {
#include "shader/mc/field_f_b_igd.g4b"
};

static const uint32_t dual_prime_igd_kernel_static[][4] = {
#include "shader/mc/dual_prime_igd.g4b"
};

struct kernel_struct {
	const uint32_t(*bin)[4];
	uint32_t size;
};

struct kernel_struct kernels_igd[] = {
	{ipicture_kernel_static, sizeof(ipicture_kernel_static)}
	,
	{null_kernel_static, sizeof(null_kernel_static)}
	,
	{frame_forward_igd_kernel_static,
	 sizeof(frame_forward_igd_kernel_static)}
	,
	{frame_backward_igd_kernel_static,
	 sizeof(frame_backward_igd_kernel_static)}
	,
	{frame_f_b_igd_kernel_static, sizeof(frame_f_b_igd_kernel_static)}
	,
	{field_forward_igd_kernel_static,
	 sizeof(field_forward_igd_kernel_static)}
	,
	{field_backward_igd_kernel_static,
	 sizeof(field_backward_igd_kernel_static)}
	,
	{field_f_b_igd_kernel_static, sizeof(field_f_b_igd_kernel_static)}
	,
	{dual_prime_igd_kernel_static, sizeof(dual_prime_igd_kernel_static)}
};

struct kernel_struct kernels_965[] = {
	{ipicture_kernel_static, sizeof(ipicture_kernel_static)}
	,
	{null_kernel_static, sizeof(null_kernel_static)}
	,
	{frame_forward_kernel_static, sizeof(frame_forward_kernel_static)}
	,
	{frame_backward_kernel_static, sizeof(frame_backward_kernel_static)}
	,
	{frame_f_b_kernel_static, sizeof(frame_f_b_kernel_static)}
	,
	{field_forward_kernel_static, sizeof(field_forward_kernel_static)}
	,
	{field_backward_kernel_static, sizeof(field_backward_kernel_static)}
	,
	{field_f_b_kernel_static, sizeof(field_f_b_kernel_static)}
	,
	{dual_prime_kernel_static, sizeof(dual_prime_kernel_static)}
};

#define ALIGN(i,m)    (((i) + (m) - 1) & ~((m) - 1))

#define MAX_SURFACE_NUM	10
#define DESCRIPTOR_NUM 12

struct media_kernel_obj {
	dri_bo *bo;
};

struct interface_descriptor_obj {
	dri_bo *bo;
	struct media_kernel_obj kernels[DESCRIPTOR_NUM];
};

struct vfe_state_obj {
	dri_bo *bo;
	struct interface_descriptor_obj interface;
};

struct surface_obj {
	dri_bo *bo;
};

struct surface_state_obj {
	struct surface_obj surface;
	dri_bo *bo;
};

struct binding_table_obj {
	dri_bo *bo;
	struct surface_state_obj surface_states[MAX_SURFACE_NUM];
};

struct indirect_data_obj {
	dri_bo *bo;
};

struct media_state {
	unsigned int is_g4x:1;
	unsigned int is_965_q:1;

	struct vfe_state_obj vfe_state;
	struct binding_table_obj binding_table;
	struct indirect_data_obj indirect_data;
};
struct media_state media_state;

static void free_object(struct media_state *s)
{
	int i;
#define FREE_ONE_BO(bo) drm_intel_bo_unreference(bo)
	FREE_ONE_BO(s->vfe_state.bo);
	FREE_ONE_BO(s->vfe_state.interface.bo);
	for (i = 0; i < DESCRIPTOR_NUM; i++)
		FREE_ONE_BO(s->vfe_state.interface.kernels[i].bo);
	FREE_ONE_BO(s->binding_table.bo);
	for (i = 0; i < MAX_SURFACE_NUM; i++)
		FREE_ONE_BO(s->binding_table.surface_states[i].bo);
	FREE_ONE_BO(s->indirect_data.bo);
}

static int alloc_object(struct media_state *s)
{
	int i;

	for (i = 0; i < MAX_SURFACE_NUM; i++) {
		s->binding_table.surface_states[i].bo =
		    drm_intel_bo_alloc(xvmc_driver->bufmgr, "surface_state",
				       sizeof(struct brw_surface_state),
				       0x1000);
		if (!s->binding_table.surface_states[i].bo)
			goto out;
	}
	return 0;
out:
	free_object(s);
	return BadAlloc;
}

static Status destroy_context(Display * display, XvMCContext * context)
{
	struct intel_xvmc_context *intel_ctx;
	intel_ctx = context->privData;
	free(intel_ctx->hw);
	free(intel_ctx);
	return Success;
}

#define STRIDE(w)               (w)
#define SIZE_YUV420(w, h)       (h * (STRIDE(w) + STRIDE(w >> 1)))

static void flush()
{
	struct brw_mi_flush flush;
	memset(&flush, 0, sizeof(flush));
	flush.opcode = CMD_MI_FLUSH;
	flush.flags = (1 << 1);
	BATCH_STRUCT(flush);
}

static void clear_sf_state()
{
	struct brw_sf_unit_state sf;
	memset(&sf, 0, sizeof(sf));
	/* TODO */
}

/* urb fence must be aligned to cacheline */
static void align_urb_fence()
{
	BATCH_LOCALS;
	int i, offset_to_next_cacheline;
	unsigned long batch_offset;
	BEGIN_BATCH(3);
	batch_offset = (void *)batch_ptr - xvmc_driver->alloc.ptr;
	offset_to_next_cacheline = ALIGN(batch_offset, 64) - batch_offset;
	if (offset_to_next_cacheline <= 12 && offset_to_next_cacheline != 0) {
		for (i = 0; i < offset_to_next_cacheline / 4; i++)
			OUT_BATCH(0);
		ADVANCE_BATCH();
	}
}

/* setup urb layout for media */
static void urb_layout()
{
	BATCH_LOCALS;
	align_urb_fence();
	BEGIN_BATCH(3);
	OUT_BATCH(BRW_URB_FENCE |
		  UF0_VFE_REALLOC |
		  UF0_CS_REALLOC |
		  UF0_SF_REALLOC |
		  UF0_CLIP_REALLOC | UF0_GS_REALLOC | UF0_VS_REALLOC | 1);
	OUT_BATCH((0 << UF1_CLIP_FENCE_SHIFT) |
		  (0 << UF1_GS_FENCE_SHIFT) | (0 << UF1_VS_FENCE_SHIFT));

	OUT_BATCH(((URB_SIZE) << UF2_VFE_FENCE_SHIFT) |	/* VFE_SIZE */
		  ((URB_SIZE) << UF2_CS_FENCE_SHIFT));	/* CS_SIZE is 0 */
	ADVANCE_BATCH();
}

static void media_state_pointers(struct media_state *media_state)
{
	BATCH_LOCALS;
	BEGIN_BATCH(3);
	OUT_BATCH(BRW_MEDIA_STATE_POINTERS | 1);
	OUT_BATCH(0);
	OUT_RELOC(media_state->vfe_state.bo, I915_GEM_DOMAIN_INSTRUCTION, 0, 0);
	ADVANCE_BATCH();
}

/* setup 2D surface for media_read or media_write 
 */
static Status setup_media_surface(struct media_state *media_state,
				  int surface_num, dri_bo * bo,
				  unsigned long offset, int w, int h,
				  Bool write)
{
	struct brw_surface_state s, *ss = &s;

	memset(ss, 0, sizeof(struct brw_surface_state));
	ss->ss0.surface_type = BRW_SURFACE_2D;
	ss->ss0.surface_format = BRW_SURFACEFORMAT_R8_SINT;
	ss->ss1.base_addr = offset + bo->offset;
	ss->ss2.width = w - 1;
	ss->ss2.height = h - 1;
	ss->ss3.pitch = w - 1;

	if (media_state->binding_table.surface_states[surface_num].bo)
		drm_intel_bo_unreference(media_state->
					 binding_table.surface_states
					 [surface_num].bo);
	media_state->binding_table.surface_states[surface_num].bo =
	    drm_intel_bo_alloc(xvmc_driver->bufmgr, "surface_state",
			       sizeof(struct brw_surface_state), 0x1000);
	if (!media_state->binding_table.surface_states[surface_num].bo)
		return BadAlloc;

	drm_intel_bo_subdata(media_state->
			     binding_table.surface_states[surface_num].bo, 0,
			     sizeof(*ss), ss);

	drm_intel_bo_emit_reloc(media_state->
				binding_table.surface_states[surface_num].bo,
				offsetof(struct brw_surface_state, ss1), bo,
				offset, I915_GEM_DOMAIN_RENDER,
				write ? I915_GEM_DOMAIN_RENDER : 0);

	return Success;
}

static Status setup_surfaces(struct media_state *media_state,
			     dri_bo * dst_bo, dri_bo * past_bo,
			     dri_bo * future_bo, int w, int h)
{
	Status ret;
	ret = setup_media_surface(media_state, 0, dst_bo, 0, w, h, TRUE);
	if (ret != Success)
		return ret;
	ret =
	    setup_media_surface(media_state, 1, dst_bo, w * h, w / 2, h / 2,
				TRUE);
	if (ret != Success)
		return ret;
	ret =
	    setup_media_surface(media_state, 2, dst_bo, w * h + w * h / 4,
				w / 2, h / 2, TRUE);
	if (ret != Success)
		return ret;
	if (past_bo) {
		ret =
		    setup_media_surface(media_state, 4, past_bo, 0, w, h,
					FALSE);
		if (ret != Success)
			return ret;
		ret =
		    setup_media_surface(media_state, 5, past_bo, w * h, w / 2,
					h / 2, FALSE);
		if (ret != Success)
			return ret;
		ret =
		    setup_media_surface(media_state, 6, past_bo,
					w * h + w * h / 4, w / 2, h / 2, FALSE);
		if (ret != Success)
			return ret;
	}
	if (future_bo) {
		ret =
		    setup_media_surface(media_state, 7, future_bo, 0, w, h,
					FALSE);
		if (ret != Success)
			return ret;
		ret =
		    setup_media_surface(media_state, 8, future_bo, w * h, w / 2,
					h / 2, FALSE);
		if (ret != Success)
			return ret;
		ret =
		    setup_media_surface(media_state, 9, future_bo,
					w * h + w * h / 4, w / 2, h / 2, FALSE);
		if (ret != Success)
			return ret;
	}
	return Success;
}

/* BUFFER SURFACE has a strange format
 * the size of the surface is in part of w h and d component
 */

static Status setup_blocks(struct media_state *media_state,
			   unsigned int block_size)
{
	union element {
		struct {
			unsigned int w:7;
			unsigned int h:13;
			unsigned int d:7;
			unsigned int pad:7;
		} whd;
		unsigned int size;
	} e;
	struct brw_surface_state ss;
	memset(&ss, 0, sizeof(struct brw_surface_state));
	ss.ss0.surface_type = BRW_SURFACE_BUFFER;
	ss.ss0.surface_format = BRW_SURFACEFORMAT_R8_UINT;
	ss.ss1.base_addr = media_state->indirect_data.bo->offset;

	e.size = block_size - 1;
	ss.ss2.width = e.whd.w;
	ss.ss2.height = e.whd.h;
	ss.ss3.depth = e.whd.d;
	ss.ss3.pitch = block_size - 1;

	if (media_state->binding_table.surface_states[3].bo)
		drm_intel_bo_unreference(media_state->
					 binding_table.surface_states[3].bo);

	media_state->binding_table.surface_states[3].bo =
	    drm_intel_bo_alloc(xvmc_driver->bufmgr, "surface_state",
			       sizeof(struct brw_surface_state), 0x1000);
	if (!media_state->binding_table.surface_states[3].bo)
		return BadAlloc;

	drm_intel_bo_subdata(media_state->binding_table.surface_states[3].bo, 0,
			     sizeof(ss), &ss);

	drm_intel_bo_emit_reloc(media_state->binding_table.surface_states[3].bo,
				offsetof(struct brw_surface_state, ss1),
				media_state->indirect_data.bo, 0,
				I915_GEM_DOMAIN_SAMPLER, 0);
	return Success;
}

/* setup state base address */
static void state_base_address()
{
	BATCH_LOCALS;
	BEGIN_BATCH(6);
	OUT_BATCH(BRW_STATE_BASE_ADDRESS | 4);
	OUT_BATCH(0 | BASE_ADDRESS_MODIFY);
	OUT_BATCH(0 | BASE_ADDRESS_MODIFY);
	OUT_BATCH(0 | BASE_ADDRESS_MODIFY);
	OUT_BATCH(0 | BASE_ADDRESS_MODIFY);
	OUT_BATCH(0xFFFFF000 | BASE_ADDRESS_MODIFY);
	ADVANCE_BATCH();
}

/* select media pipeline */
static void pipeline_select(struct media_state *media_state)
{
	BATCH_LOCALS;
	BEGIN_BATCH(1);
	if (media_state->is_g4x)
		OUT_BATCH(NEW_PIPELINE_SELECT | PIPELINE_SELECT_MEDIA);
	else
		OUT_BATCH(BRW_PIPELINE_SELECT | PIPELINE_SELECT_MEDIA);
	ADVANCE_BATCH();
}

/* kick media object to gpu */
static void send_media_object(XvMCMacroBlock * mb, int offset,
			      enum interface interface)
{
	BATCH_LOCALS;
	BEGIN_BATCH(13);
	OUT_BATCH(BRW_MEDIA_OBJECT | 11);
	OUT_BATCH(interface);
	if (media_state.is_965_q) {
		OUT_BATCH(0);
		OUT_BATCH(0);
	} else {
		OUT_BATCH(6 * 128);
		OUT_RELOC(media_state.indirect_data.bo,
			  I915_GEM_DOMAIN_INSTRUCTION, 0, offset);
	}

	OUT_BATCH(mb->x << 4);	//g1.0
	OUT_BATCH(mb->y << 4);
	OUT_RELOC(media_state.indirect_data.bo,	//g1.8
		  I915_GEM_DOMAIN_INSTRUCTION, 0, offset);
	OUT_BATCH_SHORT(mb->coded_block_pattern);	//g1.12
	OUT_BATCH_SHORT(mb->PMV[0][0][0]);	//g1.14
	OUT_BATCH_SHORT(mb->PMV[0][0][1]);	//g1.16
	OUT_BATCH_SHORT(mb->PMV[0][1][0]);	//g1.18
	OUT_BATCH_SHORT(mb->PMV[0][1][1]);	//g1.20

	OUT_BATCH_SHORT(mb->PMV[1][0][0]);	//g1.22
	OUT_BATCH_SHORT(mb->PMV[1][0][1]);	//g1.24
	OUT_BATCH_SHORT(mb->PMV[1][1][0]);	//g1.26
	OUT_BATCH_SHORT(mb->PMV[1][1][1]);	//g1.28
	OUT_BATCH_CHAR(mb->dct_type);	//g1.30
	OUT_BATCH_CHAR(mb->motion_vertical_field_select);	//g1.31

	if (media_state.is_965_q)
		OUT_BATCH(0x0);
	else
		OUT_BATCH(0xffffffff);
	ADVANCE_BATCH();
}

static Status binding_tables(struct media_state *media_state)
{
	unsigned int binding_table[MAX_SURFACE_NUM];
	int i;

	if (media_state->binding_table.bo)
		drm_intel_bo_unreference(media_state->binding_table.bo);
	media_state->binding_table.bo =
	    drm_intel_bo_alloc(xvmc_driver->bufmgr, "binding_table",
			       MAX_SURFACE_NUM * 4, 0x1000);
	if (!media_state->binding_table.bo)
		return BadAlloc;

	for (i = 0; i < MAX_SURFACE_NUM; i++)
		binding_table[i] =
		    media_state->binding_table.surface_states[i].bo->offset;
	drm_intel_bo_subdata(media_state->binding_table.bo, 0,
			     sizeof(binding_table), binding_table);

	for (i = 0; i < MAX_SURFACE_NUM; i++)
		drm_intel_bo_emit_reloc(media_state->binding_table.bo,
					i * sizeof(unsigned int),
					media_state->
					binding_table.surface_states[i].bo, 0,
					I915_GEM_DOMAIN_INSTRUCTION, 0);
	return Success;
}

static int media_kernels(struct media_state *media_state)
{
	struct kernel_struct *kernels;
	int kernel_array_size, i;

	if (media_state->is_g4x) {
		kernels = kernels_igd;
		kernel_array_size = ARRAY_SIZE(kernels_igd);
	} else {
		kernels = kernels_965;
		kernel_array_size = ARRAY_SIZE(kernels_965);
	}

	for (i = 0; i < kernel_array_size; i++) {
		media_state->vfe_state.interface.kernels[i].bo =
		    drm_intel_bo_alloc(xvmc_driver->bufmgr, "kernel",
				       kernels[i].size, 0x1000);
		if (!media_state->vfe_state.interface.kernels[i].bo)
			goto out;
	}

	for (i = 0; i < kernel_array_size; i++) {
		dri_bo *bo = media_state->vfe_state.interface.kernels[i].bo;
		drm_intel_bo_subdata(bo, 0, kernels[i].size, kernels[i].bin);
	}
	return 0;
out:
	free_object(media_state);
	return BadAlloc;
}

static void setup_interface(struct media_state *media_state, enum interface i)
{
	struct brw_interface_descriptor desc;
	memset(&desc, 0, sizeof(desc));

	desc.desc0.grf_reg_blocks = 15;
	desc.desc0.kernel_start_pointer =
	    media_state->vfe_state.interface.kernels[i].bo->offset >> 6;

	desc.desc1.floating_point_mode = BRW_FLOATING_POINT_NON_IEEE_754;

	/* use same binding table for all interface
	 * may change this if it affect performance
	 */
	desc.desc3.binding_table_entry_count = MAX_SURFACE_NUM;
	desc.desc3.binding_table_pointer =
	    media_state->binding_table.bo->offset >> 5;

	drm_intel_bo_subdata(media_state->vfe_state.interface.bo,
			     i * sizeof(desc), sizeof(desc), &desc);

	drm_intel_bo_emit_reloc(media_state->vfe_state.interface.bo,
				i * sizeof(desc) +
				offsetof(struct brw_interface_descriptor,
					 desc0),
				media_state->vfe_state.interface.kernels[i].bo,
				desc.desc0.grf_reg_blocks,
				I915_GEM_DOMAIN_INSTRUCTION, 0);

	drm_intel_bo_emit_reloc(media_state->vfe_state.interface.bo,
				i * sizeof(desc) +
				offsetof(struct brw_interface_descriptor,
					 desc3), media_state->binding_table.bo,
				desc.desc3.binding_table_entry_count,
				I915_GEM_DOMAIN_INSTRUCTION, 0);
}

static Status interface_descriptor(struct media_state *media_state)
{
	if (media_state->vfe_state.interface.bo)
		drm_intel_bo_unreference(media_state->vfe_state.interface.bo);
	media_state->vfe_state.interface.bo =
	    drm_intel_bo_alloc(xvmc_driver->bufmgr, "interfaces",
			       DESCRIPTOR_NUM *
			       sizeof(struct brw_interface_descriptor), 0x1000);
	if (!media_state->vfe_state.interface.bo)
		return BadAlloc;

	setup_interface(media_state, INTRA_INTERFACE);
	setup_interface(media_state, NULL_INTERFACE);
	setup_interface(media_state, FORWARD_INTERFACE);
	setup_interface(media_state, FIELD_FORWARD_INTERFACE);
	setup_interface(media_state, BACKWARD_INTERFACE);
	setup_interface(media_state, FIELD_BACKWARD_INTERFACE);
	setup_interface(media_state, F_B_INTERFACE);
	setup_interface(media_state, FIELD_F_B_INTERFACE);
	setup_interface(media_state, DUAL_PRIME_INTERFACE);
	return Success;
}

static Status vfe_state(struct media_state *media_state)
{
	struct brw_vfe_state state;
	memset(&state, 0, sizeof(state));

	/* no scratch space */
	state.vfe1.vfe_mode = VFE_GENERIC_MODE;
	state.vfe1.num_urb_entries = 1;
	/* XXX TODO */
	/* should carefully caculate those values for performance */
	state.vfe1.urb_entry_alloc_size = 2;
	state.vfe1.max_threads = 31;
	state.vfe2.interface_descriptor_base =
	    media_state->vfe_state.interface.bo->offset >> 4;

	if (media_state->vfe_state.bo)
		drm_intel_bo_unreference(media_state->vfe_state.bo);
	media_state->vfe_state.bo = drm_intel_bo_alloc(xvmc_driver->bufmgr,
						       "vfe state",
						       sizeof(struct
							      brw_vfe_state),
						       0x1000);
	if (!media_state->vfe_state.bo)
		return BadAlloc;

	drm_intel_bo_subdata(media_state->vfe_state.bo, 0, sizeof(state),
			     &state);

	drm_intel_bo_emit_reloc(media_state->vfe_state.bo,
				offsetof(struct brw_vfe_state, vfe2),
				media_state->vfe_state.interface.bo, 0,
				I915_GEM_DOMAIN_INSTRUCTION, 0);
	return Success;
}

static Status render_surface(Display * display,
			     XvMCContext * context,
			     unsigned int picture_structure,
			     XvMCSurface * target_surface,
			     XvMCSurface * past_surface,
			     XvMCSurface * future_surface,
			     unsigned int flags,
			     unsigned int num_macroblocks,
			     unsigned int first_macroblock,
			     XvMCMacroBlockArray * macroblock_array,
			     XvMCBlockArray * blocks)
{

	intel_xvmc_context_ptr intel_ctx;
	int i, j;
	struct i965_xvmc_context *i965_ctx;
	XvMCMacroBlock *mb;
	struct intel_xvmc_surface *priv_target_surface =
	    target_surface->privData;
	struct intel_xvmc_surface *priv_past_surface =
	    past_surface ? past_surface->privData : 0;
	struct intel_xvmc_surface *priv_future_surface =
	    future_surface ? future_surface->privData : 0;
	unsigned short *block_ptr;
	intel_ctx = context->privData;
	i965_ctx = context->privData;
	if (!intel_ctx) {
		XVMC_ERR("Can't find intel xvmc context\n");
		return BadValue;
	}

	if (media_state.indirect_data.bo) {
		drm_intel_gem_bo_unmap_gtt(media_state.
					   indirect_data.bo);

		drm_intel_bo_unreference(media_state.indirect_data.bo);
	}
	media_state.indirect_data.bo = drm_intel_bo_alloc(xvmc_driver->bufmgr,
							  "indirect data",
							  128 * 6 *
							  num_macroblocks, 64);
	if (!media_state.indirect_data.bo)
		return BadAlloc;
	setup_surfaces(&media_state,
		       priv_target_surface->bo,
		       past_surface ? priv_past_surface->bo : NULL,
		       future_surface ? priv_future_surface->bo : NULL,
		       context->width, context->height);
	setup_blocks(&media_state, 128 * 6 * num_macroblocks);
	binding_tables(&media_state);
	interface_descriptor(&media_state);
	vfe_state(&media_state);

	drm_intel_gem_bo_map_gtt(media_state.indirect_data.bo);

	block_ptr = media_state.indirect_data.bo->virtual;
	for (i = first_macroblock; i < num_macroblocks + first_macroblock; i++) {
		unsigned short *mb_block_ptr;
		mb = &macroblock_array->macro_blocks[i];
		mb_block_ptr = &blocks->blocks[(mb->index << 6)];
		if (mb->coded_block_pattern & 0x20) {
			for (j = 0; j < 8; j++)
				memcpy(block_ptr + 16 * j, mb_block_ptr + 8 * j,
				       16);
			mb_block_ptr += 64;
		}

		if (mb->coded_block_pattern & 0x10) {
			for (j = 0; j < 8; j++)
				memcpy(block_ptr + 16 * j + 8,
				       mb_block_ptr + 8 * j, 16);
			mb_block_ptr += 64;
		}
		block_ptr += 2 * 64;
		if (mb->coded_block_pattern & 0x08) {
			for (j = 0; j < 8; j++)
				memcpy(block_ptr + 16 * j, mb_block_ptr + 8 * j,
				       16);
			mb_block_ptr += 64;
		}

		if (mb->coded_block_pattern & 0x04) {
			for (j = 0; j < 8; j++)
				memcpy(block_ptr + 16 * j + 8,
				       mb_block_ptr + 8 * j, 16);
			mb_block_ptr += 64;
		}

		block_ptr += 2 * 64;
		if (mb->coded_block_pattern & 0x2) {
			memcpy(block_ptr, mb_block_ptr, 128);
			mb_block_ptr += 64;
		}

		block_ptr += 64;
		if (mb->coded_block_pattern & 0x1)
			memcpy(block_ptr, mb_block_ptr, 128);
		block_ptr += 64;
	}
	{
		int block_offset = 0;
		LOCK_HARDWARE(intel_ctx->hw_context);
		state_base_address();
		flush();
		clear_sf_state();
		pipeline_select(&media_state);
		urb_layout();
		media_state_pointers(&media_state);
		for (i = first_macroblock;
		     i < num_macroblocks + first_macroblock;
		     i++, block_offset += 128 * 6) {
			mb = &macroblock_array->macro_blocks[i];

			if (mb->macroblock_type & XVMC_MB_TYPE_INTRA) {
				send_media_object(mb, block_offset,
						  INTRA_INTERFACE);
			} else {
				if (((mb->motion_type & 3) ==
				     XVMC_PREDICTION_FRAME)) {
					if ((mb->macroblock_type &
					     XVMC_MB_TYPE_MOTION_FORWARD)) {
						if (((mb->macroblock_type &
						      XVMC_MB_TYPE_MOTION_BACKWARD)))
							send_media_object(mb,
									  block_offset,
									  F_B_INTERFACE);
						else
							send_media_object(mb,
									  block_offset,
									  FORWARD_INTERFACE);
					} else
					    if ((mb->macroblock_type &
						 XVMC_MB_TYPE_MOTION_BACKWARD))
					{
						send_media_object(mb,
								  block_offset,
								  BACKWARD_INTERFACE);
					}
				} else if ((mb->motion_type & 3) ==
					   XVMC_PREDICTION_FIELD) {
					if ((mb->macroblock_type &
					     XVMC_MB_TYPE_MOTION_FORWARD)) {
						if (((mb->macroblock_type &
						      XVMC_MB_TYPE_MOTION_BACKWARD)))
							send_media_object(mb,
									  block_offset,
									  FIELD_F_B_INTERFACE);
						else

							send_media_object(mb,
									  block_offset,
									  FIELD_FORWARD_INTERFACE);
					} else
					    if ((mb->macroblock_type &
						 XVMC_MB_TYPE_MOTION_BACKWARD))
					{
						send_media_object(mb,
								  block_offset,
								  FIELD_BACKWARD_INTERFACE);
					}
				} else {
					send_media_object(mb, block_offset,
							  DUAL_PRIME_INTERFACE);
				}
			}
		}
		intelFlushBatch(TRUE);
		UNLOCK_HARDWARE(intel_ctx->hw_context);
	}
	return Success;
}

static Status create_context(Display * display, XvMCContext * context,
			     int priv_count, CARD32 * priv_data)
{
	struct intel_xvmc_context *intel_ctx;
	struct intel_xvmc_hw_context *hw_ctx;
	hw_ctx = (struct intel_xvmc_hw_context *)priv_data;

	intel_ctx = calloc(1, sizeof(struct intel_xvmc_context));
	if (!intel_ctx)
		return BadAlloc;
	intel_ctx->hw = hw_ctx;
	intel_ctx->surface_bo_size
		= SIZE_YUV420(context->width, context->height);
	context->privData = intel_ctx;

	media_state.is_g4x = hw_ctx->i965.is_g4x;
	media_state.is_965_q = hw_ctx->i965.is_965_q;

	if (alloc_object(&media_state))
		return BadAlloc;
	if (media_kernels(&media_state))
		return BadAlloc;
	return Success;
}

struct _intel_xvmc_driver i965_xvmc_mc_driver = {
	.type = XVMC_I965_MPEG2_MC,
	.create_context = create_context,
	.destroy_context = destroy_context,
	.render_surface = render_surface,
};
@


1.6
log
@Update to xf86-video-intel 2.20.19.

A recent kernel with kernel modesetting support is required.
Thanks to jsg@@ and kettenis@@ for their work.
@
text
@@


1.5
log
@Update the intel driver to a more recent version based on more recent
upsteam code.

Backporting keeping UMS changes by me, some bugfixes from kettenis@@.

Has been in snapshots for a while, committed on request so we can be
sure what people are running. This is a prerequesite for sandybridge
support but has those chipsets disabled for now until the correct code
has been added.
@
text
@d247 1
a247 1
	Xfree(intel_ctx->hw);
@


1.4
log
@Update the intel driver to (mostly) a backport of 2.12.

It is missing a few commits that I have yet to verify (ones that try and
continue if we lock the gpu rendering engine and can't reset it, for
example) taht will be verified and sent out for extra testing soon.

Should contain a bunch of speedups and some correctness improvements
(though rendercheck still gives some errors that I am looking into).

This has been in snaps since the first day of c2k10, any known issues
with just this driver have (to my knowledge) been fixed since. A problem
with macbooks pointed out by otto happens with both this and the in-tree
driver and thus doesn't stop this moving forward.

As well as the 2.12 improvements, this driver also has a backport
(partially aided by the backports in RHEL 5 kindly provided by Dave
Airlie) from the kms code of modesetting support for ironlake (arrandale
and clarkdale: the IGDs build into intel nehalem cpu dies) which has
been tested on a number of chipsets. Note that Display port and eDP
displays have not yet been worked on (and probably won't until I can
find a displayport monitor), but VGA and lvds at least are known to
work, sure beats vesa.

"no objection on my side" matthieu@@, prodding (as always) from princess
marco.
@
text
@d28 2
a29 1
#include "i810_reg.h"
d33 1
a33 1
#include "i830_hwmc.h"
@


1.3
log
@Pull in the changes in rendering that I skipped when i backported
changes from later intel versions (after the UMS removal). 95% of this
is the xvmc reworks that makes that code even halfway sane. xvmc is now
enabled by default on 965+.

Tested by many on tech@@, thanks!

ok matthieu@@
@
text
@d210 1
a210 1
static int free_object(struct media_state *s)
@


1.2
log
@Update the intel driver to 2.9.1 plus backports.

2.9.1 is the last version of the intel DDX that supports UMS (User
modesetting), with 2.10 onwards being purely KMS only. As such, this
driver contains backports of almost every correctness or performance
related fix to the rendering layer in later intel drivers. This driver
*REQUIRES* a GEM enabled kernel. it claims to support non-gem mode but
this is essentially unmaintained and due to the way the abstraciton
works is slow, if it works at all (it often does not). You have been
warned.

tested by many many people on tech over the last few weeks.
@
text
@d27 1
a27 1
#include "i965_xvmc.h"
d32 1
a32 1
#include "i965_hwmc.h"
d244 4
a247 5
	struct i965_xvmc_context *private_context;
	private_context = context->privData;

	free_object(&media_state);
	Xfree(private_context);
a253 22
static Status create_surface(Display * display,
			     XvMCContext * context, XvMCSurface * surface,
			     int priv_count, CARD32 * priv_data)
{
	struct i965_xvmc_surface *priv_surface =
	    (struct i965_xvmc_surface *)priv_data;
	size_t size = SIZE_YUV420(priv_surface->w, priv_surface->h);
	surface->privData = priv_data;
	priv_surface->bo = drm_intel_bo_alloc(xvmc_driver->bufmgr, "surface",
					      size, 0x1000);
	return Success;
}

static Status destroy_surface(Display * display, XvMCSurface * surface)
{
	struct i965_xvmc_surface *priv_surface = surface->privData;
	XSync(display, False);

	drm_intel_bo_unreference(priv_surface->bo);
	return Success;
}

d696 1
a696 1
	struct i965_xvmc_surface *priv_target_surface =
d698 1
a698 1
	struct i965_xvmc_surface *priv_past_surface =
d700 1
a700 1
	struct i965_xvmc_surface *priv_future_surface =
d703 1
a703 1
	intel_ctx = intel_xvmc_find_context(context->context_id);
d711 2
a712 5
		if (xvmc_driver->kernel_exec_fencing)
			drm_intel_gem_bo_unmap_gtt(media_state.
						   indirect_data.bo);
		else
			drm_intel_bo_unmap(media_state.indirect_data.bo);
d732 1
a732 4
	if (xvmc_driver->kernel_exec_fencing)
		drm_intel_gem_bo_map_gtt(media_state.indirect_data.bo);
	else
		drm_intel_bo_map(media_state.indirect_data.bo, 1);
a850 23
static Status put_surface(Display * display, XvMCSurface * surface,
			  Drawable draw, short srcx, short srcy,
			  unsigned short srcw, unsigned short srch,
			  short destx, short desty,
			  unsigned short destw, unsigned short desth,
			  int flags, struct intel_xvmc_command *data)
{
	struct i965_xvmc_surface *private_surface = surface->privData;
	uint32_t handle = 0;

	drm_intel_bo_flink(private_surface->bo, &handle);
	data->handle = handle;

	return Success;
}

static Status get_surface_status(Display * display,
				 XvMCSurface * surface, int *stats)
{
	*stats = 0;
	return 0;
}

d854 11
a864 3
	struct i965_xvmc_context *i965_ctx;
	i965_ctx = (struct i965_xvmc_context *)priv_data;
	context->privData = i965_ctx;
d866 2
a867 2
	media_state.is_g4x = i965_ctx->is_g4x;
	media_state.is_965_q = i965_ctx->is_965_q;
a879 2
	.create_surface = create_surface,
	.destroy_surface = destroy_surface,
a880 2
	.put_surface = put_surface,
	.get_surface_status = get_surface_status,
@


1.1
log
@update to xf86-video-intel 2.7.1. Tested by many.
@
text
@d34 4
a37 1
#define URB_SIZE     256        /* XXX */
d39 9
a47 10
    INTRA_INTERFACE,            /* non field intra */
    NULL_INTERFACE,             /* fill with white, do nothing, for debug */
    FORWARD_INTERFACE,          /* non field forward predict */
    BACKWARD_INTERFACE,         /* non field backward predict */
    F_B_INTERFACE,              /* non field forward and backward predict */
    FIELD_INTRA_INTERFACE,      /* field intra */
    FIELD_FORWARD_INTERFACE,    /* field forward predict */
    FIELD_BACKWARD_INTERFACE,   /* field backward predict */
    FIELD_F_B_INTERFACE,        /* field forward and backward predict */
    DUAL_PRIME_INTERFACE
d51 1
a51 1
	#include "ipicture.g4b"
d53 1
d55 1
a55 1
	#include "null.g4b"
d57 1
d59 1
a59 1
	#include "frame_forward.g4b"
d61 1
d63 1
a63 1
	#include "frame_backward.g4b"
d65 1
d67 3
a69 2
	#include "frame_f_b.g4b"
}; 
d71 1
a71 1
	#include "field_forward.g4b"
d73 1
d75 1
a75 1
	#include "field_backward.g4b"
d77 1
d79 7
a85 5
	#include "field_f_b.g4b"
}; 
static const uint32_t dual_prime_kernel_static[][4]= {
	#include "dual_prime.g4b"
}; 
d87 1
a87 1
	#include "frame_forward_igd.g4b"
d89 1
d91 1
a91 1
	#include "frame_backward_igd.g4b"
d93 1
d95 3
a97 2
	#include "frame_f_b_igd.g4b"
}; 
d99 1
a99 1
	#include "field_forward_igd.g4b"
d101 1
d103 1
a103 1
	#include "field_backward_igd.g4b"
d105 1
d107 55
a161 5
	#include "field_f_b_igd.g4b"
}; 
static const uint32_t dual_prime_igd_kernel_static[][4]= {
	#include "dual_prime_igd.g4b"
}; 
a164 7
#define	VFE_GENERIC_MODE	0x0
#define	VFE_VLD_MODE		0x1
#define VFE_IS_MODE		0x2
#define VFE_AVC_MC_MODE		0x4
#define VFE_AVC_IT_MODE		0x7
#define VFE_VC1_IT_MODE		0x7

d168 32
d201 6
a206 19
    unsigned long state_base;
    void 	      *state_ptr;
    unsigned int  binding_table_entry_count;
    unsigned long vfe_state_offset;
    unsigned long interface_descriptor_offset[DESCRIPTOR_NUM];
    unsigned long ipicture_kernel_offset;
    unsigned long frame_forward_kernel_offset;
    unsigned long frame_backward_kernel_offset;
    unsigned long frame_f_b_kernel_offset;
    unsigned long ipicture_field_kernel_offset;
    unsigned long field_forward_kernel_offset;
    unsigned long field_backward_kernel_offset;
    unsigned long field_f_b_kernel_offset;
    unsigned long dual_prime_kernel_offset;
    unsigned long null_kernel_offset;
    unsigned long surface_offsets[MAX_SURFACE_NUM];
    unsigned long binding_table_offset;
    unsigned int  is_g4x:1;
    unsigned int  is_965_q:1;
d210 1
a210 1
static int map_buffer(struct  drm_memory_block *mem)
d212 28
a239 2
    return (drmMap(xvmc_driver->fd, 
		mem->handle, mem->size, &mem->ptr));
d242 1
a242 1
static void unmap_buffer(struct  drm_memory_block *mem)
d244 6
a249 1
    drmUnmap(mem->ptr, mem->size);
d252 2
a253 6
static Status destroy_context(Display *display, XvMCContext *context)
{
    struct i965_xvmc_context *private_context;
    private_context = context->privData;
    unmap_buffer(&private_context->static_buffer);
    unmap_buffer(&private_context->blocks);
d255 11
a265 2
    Xfree(private_context);
    return Success;
d268 1
a268 3
static Status create_surface(Display *display,
	XvMCContext *context, XvMCSurface *surface, int priv_count,
	CARD32 *priv_data)
d270 2
a271 7
    struct i965_xvmc_surface *priv_surface = 
	(struct i965_xvmc_surface *)priv_data;
    if (map_buffer(&priv_surface->buffer))
	return BadAlloc;
    surface->privData = priv_data;
    return Success;
}
d273 2
a274 6
static Status destroy_surface(Display *display, XvMCSurface *surface)
{
    struct i965_xvmc_surface *priv_surface = 
	surface->privData;
    unmap_buffer(&priv_surface->buffer);
    return Success;
d279 5
a283 5
    struct brw_mi_flush flush;
    memset(&flush, 0, sizeof(flush));
    flush.opcode = CMD_MI_FLUSH;
    flush.flags = (1<<1);
    BATCH_STRUCT(flush);
d288 3
a290 3
    struct brw_sf_unit_state sf;
    memset(&sf, 0, sizeof(sf));
    /* TODO */
a292 1

d296 11
a306 11
    BATCH_LOCALS;
    int i, offset_to_next_cacheline;
    unsigned long batch_offset;
    BEGIN_BATCH(3);
    batch_offset = (void *)batch_ptr - xvmc_driver->alloc.ptr;
    offset_to_next_cacheline = ALIGN(batch_offset, 64) - batch_offset; 
    if (offset_to_next_cacheline <= 12 && offset_to_next_cacheline != 0) {
	for (i = 0; i < offset_to_next_cacheline/4; i++)
	    OUT_BATCH(0);
	ADVANCE_BATCH();
    }
d312 14
a325 32
    BATCH_LOCALS;
    align_urb_fence();
    BEGIN_BATCH(3);
    OUT_BATCH(BRW_URB_FENCE |
	    UF0_VFE_REALLOC |
	    UF0_CS_REALLOC |
	    1);
    OUT_BATCH(0);
    OUT_BATCH(((URB_SIZE)<< UF2_VFE_FENCE_SHIFT) |	/* VFE_SIZE */
	    ((URB_SIZE)<< UF2_CS_FENCE_SHIFT));		/* CS_SIZE is 0 */
    ADVANCE_BATCH();
}

/* clear previous urb layout */
static void clear_urb_state()
{
    BATCH_LOCALS;
    align_urb_fence();
    BEGIN_BATCH(3);
    OUT_BATCH(BRW_URB_FENCE |
	    UF0_CS_REALLOC |
	    UF0_SF_REALLOC |
	    UF0_CLIP_REALLOC |
	    UF0_GS_REALLOC |
	    UF0_VS_REALLOC |
	    1);
    OUT_BATCH((0 << UF1_CLIP_FENCE_SHIFT) |
	    (0 << UF1_GS_FENCE_SHIFT) |
	    (0 << UF1_VS_FENCE_SHIFT));
    OUT_BATCH((0 << UF2_CS_FENCE_SHIFT) |
	    (0 << UF2_SF_FENCE_SHIFT));
    ADVANCE_BATCH();
d330 6
a335 16
    BATCH_LOCALS;
    BEGIN_BATCH(3);
    OUT_BATCH(BRW_MEDIA_STATE_POINTERS|1);
    OUT_BATCH(0);
    OUT_BATCH(media_state->vfe_state_offset);
    ADVANCE_BATCH();
}

static void cs_urb_layout()
{
    BATCH_LOCALS;
    BEGIN_BATCH(2);
    OUT_BATCH(BRW_CS_URB_STATE | 0);
    OUT_BATCH((0 << 4) |    /* URB Entry Allocation Size */
	      (0 << 0));    /* Number of URB Entries */
    ADVANCE_BATCH();
d340 91
a430 33
static void setup_media_surface(struct media_state *media_state,
	int surface_num, unsigned long offset, int w, int h)
{
    struct brw_surface_state *ss;
    ss = media_state->state_ptr +
	(media_state->surface_offsets[surface_num] - media_state->state_base);
    memset(ss, 0, sizeof(struct brw_surface_state));
    ss->ss0.surface_type = BRW_SURFACE_2D;
    ss->ss0.surface_format = BRW_SURFACEFORMAT_R8_SINT;
    ss->ss1.base_addr = offset;
    ss->ss2.width = w - 1;
    ss->ss2.height = h - 1;
    ss->ss3.pitch = w - 1;
}

static void setup_surfaces(struct media_state *media_state, 
	unsigned long dst_offset, unsigned long past_offset, 
	unsigned long future_offset, 
	int w, int h)
{
    setup_media_surface(media_state, 0, dst_offset, w, h);
    setup_media_surface(media_state, 1, dst_offset+w*h, w/2, h/2);
    setup_media_surface(media_state, 2, dst_offset+w*h + w*h/4, w/2, h/2);
    if (past_offset) {
	setup_media_surface(media_state, 4, past_offset, w, h);
	setup_media_surface(media_state, 5, past_offset+w*h, w/2, h/2);
	setup_media_surface(media_state, 6, past_offset+w*h + w*h/4, w/2, h/2);
    }
    if (future_offset) {
	setup_media_surface(media_state, 7, future_offset, w, h);
	setup_media_surface(media_state, 8, future_offset+w*h, w/2, h/2);
	setup_media_surface(media_state, 9, future_offset+w*h + w*h/4, w/2, h/2);
    }
d432 1
d437 2
a438 2
static void setup_blocks(struct media_state *media_state, 
	unsigned long offset, unsigned int block_size)
d440 1
a440 1
    union element{
d446 33
a478 15
		}whd;
		unsigned int size;	
    }e;
    struct brw_surface_state *ss;
    ss = media_state->state_ptr +
	(media_state->surface_offsets[3] - media_state->state_base);
    memset(ss, 0, sizeof(struct brw_surface_state));
    ss->ss0.surface_type = BRW_SURFACE_BUFFER;
    ss->ss0.surface_format = BRW_SURFACEFORMAT_R8_UINT;
    ss->ss1.base_addr = offset;
    e.size = block_size - 1;
    ss->ss2.width = e.whd.w;
    ss->ss2.height = e.whd.h;
    ss->ss3.depth = e.whd.d;
    ss->ss3.pitch = block_size - 1;
d482 1
a482 1
static void state_base_address(int offset)
d484 9
a492 9
    BATCH_LOCALS;
    BEGIN_BATCH(6);
    OUT_BATCH(BRW_STATE_BASE_ADDRESS | 4);
    OUT_BATCH(0 | BASE_ADDRESS_MODIFY);
    OUT_BATCH(0 | BASE_ADDRESS_MODIFY); 
    OUT_BATCH(0 | BASE_ADDRESS_MODIFY);
    OUT_BATCH(0 | BASE_ADDRESS_MODIFY);
    OUT_BATCH((0xFFFFF<<12) | BASE_ADDRESS_MODIFY);
    ADVANCE_BATCH();
d498 7
a504 7
    BATCH_LOCALS;
    BEGIN_BATCH(1);
    if (media_state->is_g4x)
	    OUT_BATCH(NEW_PIPELINE_SELECT | PIPELINE_SELECT_MEDIA);
    else
	    OUT_BATCH(BRW_PIPELINE_SELECT | PIPELINE_SELECT_MEDIA);
    ADVANCE_BATCH();
d508 69
a576 1
static void send_media_object(XvMCMacroBlock *mb, int offset, enum interface interface)
d578 2
a579 54
    BATCH_LOCALS;
    BEGIN_BATCH(13);
    OUT_BATCH(BRW_MEDIA_OBJECT|11);
    OUT_BATCH(interface);
    if (media_state.is_965_q) {
	OUT_BATCH(0);
	OUT_BATCH(0);
    }else {
	OUT_BATCH(6*128);
	OUT_BATCH(offset);
    }
    
    OUT_BATCH(mb->x<<4);                 //g1.0
    OUT_BATCH(mb->y<<4);
    OUT_BATCH(offset);               //g1.8
    OUT_BATCH_SHORT(mb->coded_block_pattern);  //g1.12
    OUT_BATCH_SHORT(mb->PMV[0][0][0]);         //g1.14
    OUT_BATCH_SHORT(mb->PMV[0][0][1]);         //g1.16
    OUT_BATCH_SHORT(mb->PMV[0][1][0]);         //g1.18
    OUT_BATCH_SHORT(mb->PMV[0][1][1]);         //g1.20
    
    OUT_BATCH_SHORT(mb->PMV[1][0][0]);         //g1.22 
    OUT_BATCH_SHORT(mb->PMV[1][0][1]);         //g1.24
    OUT_BATCH_SHORT(mb->PMV[1][1][0]);         //g1.26
    OUT_BATCH_SHORT(mb->PMV[1][1][1]);         //g1.28
    OUT_BATCH_CHAR(mb->dct_type);              //g1.30
    OUT_BATCH_CHAR(mb->motion_vertical_field_select);//g1.31
    
    if (media_state.is_965_q) 
	OUT_BATCH(0x0);
    else
	OUT_BATCH(0xffffffff);
    ADVANCE_BATCH();
}

static void binding_tables(struct media_state *media_state)
{
    unsigned int *binding_table;
    int i;
    binding_table = media_state->state_ptr +
	(media_state->binding_table_offset - media_state->state_base);
    for (i = 0; i < MAX_SURFACE_NUM; i++)
	binding_table[i] = media_state->surface_offsets[i];
}

static void media_kernels(struct media_state *media_state)
{
	void *kernel; 
#define LOAD_KERNEL(name) kernel = media_state->state_ptr +\
	(media_state->name##_kernel_offset - media_state->state_base);\
	memcpy(kernel, name##_kernel_static, sizeof(name##_kernel_static));
#define LOAD_KERNEL_IGD(name) kernel = media_state->state_ptr +\
	(media_state->name##_kernel_offset - media_state->state_base);\
	memcpy(kernel, name##_igd_kernel_static, sizeof(name##_igd_kernel_static));
a580 2
	LOAD_KERNEL(ipicture);
	LOAD_KERNEL(null);
d582 90
a671 66
		LOAD_KERNEL_IGD(frame_forward);
		LOAD_KERNEL_IGD(field_forward);
		LOAD_KERNEL_IGD(frame_backward);
		LOAD_KERNEL_IGD(field_backward);
		LOAD_KERNEL_IGD(frame_f_b);
		LOAD_KERNEL_IGD(field_f_b);
		LOAD_KERNEL_IGD(dual_prime);

	}else {
		LOAD_KERNEL(frame_forward);
		LOAD_KERNEL(field_forward);
		LOAD_KERNEL(frame_backward);
		LOAD_KERNEL(field_backward);
		LOAD_KERNEL(frame_f_b);
		LOAD_KERNEL(field_f_b);
		LOAD_KERNEL(dual_prime);
	}
}

static void setup_interface(struct media_state *media_state, 
	enum interface interface, unsigned int kernel_offset)
{
    struct brw_interface_descriptor *desc;
    desc = media_state->state_ptr +
	(media_state->interface_descriptor_offset[interface] 
	 - media_state->state_base);
    memset(desc, 0, sizeof(*desc));
    desc->desc0.grf_reg_blocks = 15;
    desc->desc0.kernel_start_pointer = kernel_offset >> 6;
    desc->desc1.floating_point_mode = BRW_FLOATING_POINT_NON_IEEE_754;

    /* use same binding table for all interface
     * may change this if it affect performance
     */
    desc->desc3.binding_table_entry_count = MAX_SURFACE_NUM;
    desc->desc3.binding_table_pointer = media_state->binding_table_offset >> 5;
}

static void interface_descriptor(struct media_state *media_state)
{
	setup_interface(media_state, INTRA_INTERFACE, 
		media_state->ipicture_kernel_offset);
	setup_interface(media_state, NULL_INTERFACE, 
		media_state->null_kernel_offset);
	setup_interface(media_state, FORWARD_INTERFACE, 
		media_state->frame_forward_kernel_offset);
	setup_interface(media_state, FIELD_FORWARD_INTERFACE, 
		media_state->field_forward_kernel_offset);
	setup_interface(media_state, BACKWARD_INTERFACE, 
		media_state->frame_backward_kernel_offset);
	setup_interface(media_state, FIELD_BACKWARD_INTERFACE, 
		media_state->field_backward_kernel_offset);
	setup_interface(media_state, F_B_INTERFACE, 
		media_state->frame_f_b_kernel_offset);
	setup_interface(media_state, FIELD_F_B_INTERFACE, 
		media_state->field_f_b_kernel_offset);
	setup_interface(media_state, DUAL_PRIME_INTERFACE,
		media_state->dual_prime_kernel_offset);
}

static void vfe_state(struct media_state *media_state)
{
	struct brw_vfe_state *state;
	state = media_state->state_ptr +
	    (media_state->vfe_state_offset - media_state->state_base);
	memset(state, 0, sizeof(*state));
d673 2
a674 2
	state->vfe1.vfe_mode = VFE_GENERIC_MODE;
	state->vfe1.num_urb_entries = 1; 
d677 196
a872 204
	state->vfe1.urb_entry_alloc_size = 2; 
	state->vfe1.max_threads = 31; 
	state->vfe2.interface_descriptor_base = 
		media_state->interface_descriptor_offset[0] >> 4;
}

static void calc_state_layouts(struct media_state *media_state)
{
    int i;
    media_state->vfe_state_offset = ALIGN(media_state->state_base, 64);
    media_state->interface_descriptor_offset[0] = 
	ALIGN(media_state->vfe_state_offset + sizeof(struct brw_vfe_state), 64);
    for (i = 1; i < DESCRIPTOR_NUM; i++)
	media_state->interface_descriptor_offset[i] = 
	    media_state->interface_descriptor_offset[i-1]
	    + sizeof(struct brw_interface_descriptor);
    media_state->binding_table_offset = 
	ALIGN(media_state->interface_descriptor_offset[DESCRIPTOR_NUM - 1]
		+ sizeof(struct brw_interface_descriptor), 64);
    media_state->surface_offsets[0] = 
	ALIGN(media_state->binding_table_offset
		+ 4*media_state->binding_table_entry_count , 32);
    for (i = 1; i < MAX_SURFACE_NUM; i++)
	media_state->surface_offsets[i] = 
	    ALIGN(media_state->surface_offsets[i - 1] 
		    + sizeof(struct brw_surface_state) , 32);
    media_state->ipicture_kernel_offset = 
	ALIGN(media_state->surface_offsets[MAX_SURFACE_NUM - 1] 
		+ sizeof(struct brw_surface_state) , 64);

    media_state->frame_forward_kernel_offset = 
	    ALIGN(media_state->ipicture_kernel_offset + 
			    sizeof(ipicture_kernel_static), 64);
    if(!media_state->is_g4x) {
	    media_state->field_forward_kernel_offset = 
		    ALIGN(media_state->frame_forward_kernel_offset + 
				    sizeof(frame_forward_kernel_static), 64);
	    media_state->frame_backward_kernel_offset = 
		    ALIGN(media_state->field_forward_kernel_offset + 
				    sizeof(field_forward_kernel_static), 64);
	    media_state->field_backward_kernel_offset = 
		    ALIGN(media_state->frame_backward_kernel_offset + 
				    sizeof(frame_backward_kernel_static), 64);
	    media_state->frame_f_b_kernel_offset = 
		    ALIGN(media_state->field_backward_kernel_offset + 
				    sizeof(field_backward_kernel_static), 64);
	    media_state->field_f_b_kernel_offset = 
		    ALIGN(media_state->frame_f_b_kernel_offset + 
				    sizeof(frame_f_b_kernel_static), 64);
	    media_state->null_kernel_offset =
		    ALIGN(media_state->field_f_b_kernel_offset +
				    sizeof(field_f_b_kernel_static), 64);
	    media_state->dual_prime_kernel_offset =
		    ALIGN(media_state->null_kernel_offset +
				    sizeof(null_kernel_static), 64);
    } else {
	    media_state->field_forward_kernel_offset = 
		    ALIGN(media_state->frame_forward_kernel_offset + 
				    sizeof(frame_forward_igd_kernel_static), 64);
	    media_state->frame_backward_kernel_offset = 
		    ALIGN(media_state->field_forward_kernel_offset + 
				    sizeof(field_forward_igd_kernel_static), 64);
	    media_state->field_backward_kernel_offset = 
		    ALIGN(media_state->frame_backward_kernel_offset + 
				    sizeof(frame_backward_igd_kernel_static), 64);
	    media_state->frame_f_b_kernel_offset = 
		    ALIGN(media_state->field_backward_kernel_offset + 
				    sizeof(field_backward_igd_kernel_static), 64);
	    media_state->field_f_b_kernel_offset = 
		    ALIGN(media_state->frame_f_b_kernel_offset + 
				    sizeof(frame_f_b_igd_kernel_static), 64);
	    media_state->null_kernel_offset =
		    ALIGN(media_state->field_f_b_kernel_offset +
				    sizeof(field_f_b_igd_kernel_static), 64);
	    media_state->dual_prime_kernel_offset =
		    ALIGN(media_state->null_kernel_offset +
				    sizeof(null_kernel_static), 64);
    }
}

static Status render_surface(Display *display, 
	XvMCContext *context,
	unsigned int picture_structure,
	XvMCSurface *target_surface,
	XvMCSurface *past_surface,
	XvMCSurface *future_surface,
	unsigned int flags,
	unsigned int num_macroblocks,
	unsigned int first_macroblock,
	XvMCMacroBlockArray *macroblock_array,
	XvMCBlockArray *blocks)
{

    intel_xvmc_context_ptr intel_ctx;
    int i, j;
    struct i965_xvmc_context *i965_ctx;
    XvMCMacroBlock *mb;
    struct i965_xvmc_surface *priv_target_surface = 
	target_surface->privData;
    struct i965_xvmc_surface *priv_past_surface = 
	past_surface?past_surface->privData:0;
    struct i965_xvmc_surface *priv_future_surface = 
	future_surface?future_surface->privData:0;
    unsigned short *block_ptr;
    intel_ctx = intel_xvmc_find_context(context->context_id);
    i965_ctx = context->privData;
    if (!intel_ctx) {
	XVMC_ERR("Can't find intel xvmc context\n");
	return BadValue;
    }
    setup_surfaces(&media_state, 
	    priv_target_surface->buffer.offset, 
	    past_surface? priv_past_surface->buffer.offset:0, 
	    future_surface?priv_future_surface->buffer.offset:0, 
	    context->width, context->height);

    block_ptr = i965_ctx->blocks.ptr;
    for (i = first_macroblock; 
	    i < num_macroblocks + first_macroblock; i++) {
	unsigned short *mb_block_ptr;
	mb = &macroblock_array->macro_blocks[i];
	mb_block_ptr = &blocks->blocks[(mb->index<<6)];
	if (mb->coded_block_pattern & 0x20)  {
	    for (j = 0; j < 8; j++)
		memcpy(block_ptr + 16*j, mb_block_ptr + 8*j, 16);	
	    mb_block_ptr += 64;
	}

	if (mb->coded_block_pattern & 0x10)  {
	    for (j = 0; j < 8; j++)
		memcpy(block_ptr + 16*j + 8, mb_block_ptr + 8*j, 16);	
	    mb_block_ptr += 64;
	}
	block_ptr += 2*64;
	if (mb->coded_block_pattern & 0x08)  {
	    for (j = 0; j < 8; j++)
		memcpy(block_ptr + 16*j, mb_block_ptr + 8*j, 16);	
	    mb_block_ptr += 64;
	}

	if (mb->coded_block_pattern & 0x04)  {
	    for (j = 0; j < 8; j++)
		memcpy(block_ptr + 16*j + 8, mb_block_ptr + 8*j, 16);	
	    mb_block_ptr += 64;
	}

	block_ptr += 2*64;
	if (mb->coded_block_pattern & 0x2) {
	    memcpy(block_ptr, mb_block_ptr, 128);
	    mb_block_ptr += 64;
	}

	block_ptr += 64;
	if (mb->coded_block_pattern & 0x1) 
	    memcpy(block_ptr, mb_block_ptr, 128);
	block_ptr += 64;
    }

    {
	int block_offset;
	block_offset = media_state.is_965_q?0:i965_ctx->blocks.offset;
	LOCK_HARDWARE(intel_ctx->hw_context);
	state_base_address(block_offset);
	flush();	
	clear_sf_state();
	clear_urb_state();
	pipeline_select(&media_state);
	urb_layout();	
	media_state_pointers(&media_state);
	cs_urb_layout();

	for (i = first_macroblock; 
		i < num_macroblocks + first_macroblock; 
		i++, block_offset += 128*6) {
	    mb = &macroblock_array->macro_blocks[i];

	    if (mb->macroblock_type & XVMC_MB_TYPE_INTRA) {
		send_media_object(mb, block_offset, INTRA_INTERFACE);
	    } else {
		if (((mb->motion_type & 3) == XVMC_PREDICTION_FRAME)) {
		    if ((mb->macroblock_type&XVMC_MB_TYPE_MOTION_FORWARD))
		    {
			if (((mb->macroblock_type&XVMC_MB_TYPE_MOTION_BACKWARD)))
			    send_media_object(mb, block_offset, F_B_INTERFACE);
			else
			    send_media_object(mb, block_offset, FORWARD_INTERFACE);
		    } else if ((mb->macroblock_type&XVMC_MB_TYPE_MOTION_BACKWARD))
		    {
			send_media_object(mb, block_offset, BACKWARD_INTERFACE);
		    }
		} else if ((mb->motion_type & 3) == XVMC_PREDICTION_FIELD) {
		    if ((mb->macroblock_type&XVMC_MB_TYPE_MOTION_FORWARD))
		    {
			if (((mb->macroblock_type&XVMC_MB_TYPE_MOTION_BACKWARD)))	
			    send_media_object(mb, block_offset, FIELD_F_B_INTERFACE);
			else 

			    send_media_object(mb, block_offset, FIELD_FORWARD_INTERFACE);
		    } else if ((mb->macroblock_type&XVMC_MB_TYPE_MOTION_BACKWARD))
		    {
			send_media_object(mb, block_offset, FIELD_BACKWARD_INTERFACE);
		    }
		}else {
		    send_media_object(mb, block_offset, DUAL_PRIME_INTERFACE);
d874 2
a875 1
	    }
d877 1
a877 4
	intelFlushBatch(TRUE);
	UNLOCK_HARDWARE(intel_ctx->hw_context);
    }
    return Success;
d880 6
a885 6
static Status put_surface(Display *display,XvMCSurface *surface,
	Drawable draw, short srcx, short srcy,
	unsigned short srcw, unsigned short srch,
	short destx, short desty,
	unsigned short destw, unsigned short desth,
	int flags, struct intel_xvmc_command *data)
d887 5
a891 2
	struct i965_xvmc_surface *private_surface =
		surface->privData;
a892 1
	data->surf_offset = private_surface->buffer.offset;
d896 2
a897 2
static Status get_surface_status(Display *display,
	XvMCSurface *surface, int *stats)
d899 2
a900 2
    *stats = 0;
    return 0;
d903 2
a904 2
static Status create_context(Display *display, XvMCContext *context,
	int priv_count, CARD32 *priv_data)
d906 4
a909 10
    struct i965_xvmc_context *i965_ctx;
    i965_ctx = (struct i965_xvmc_context *)priv_data;
    context->privData = i965_ctx;
    if (map_buffer(&i965_ctx->static_buffer))
	return BadAlloc;
    if(map_buffer(&i965_ctx->blocks))
	return BadAlloc;
    {
	media_state.state_base = i965_ctx->static_buffer.offset;
	media_state.state_ptr = i965_ctx->static_buffer.ptr;
d912 6
a917 11
	media_state.binding_table_entry_count = MAX_SURFACE_NUM;
	calc_state_layouts(&media_state);
	vfe_state(&media_state);
	interface_descriptor(&media_state); 
	media_kernels(&media_state);
	setup_blocks(&media_state, 
		i965_ctx->blocks.offset, 
		6*context->width*context->height*sizeof(short));
	binding_tables(&media_state);
    }
    return Success;
d921 8
a928 8
    .type               = XVMC_I965_MPEG2_MC,
    .create_context     = create_context,
    .destroy_context    = destroy_context,
    .create_surface     = create_surface,
    .destroy_surface    = destroy_surface,
    .render_surface     = render_surface,
    .put_surface        = put_surface,
    .get_surface_status = get_surface_status,
a929 1

@

