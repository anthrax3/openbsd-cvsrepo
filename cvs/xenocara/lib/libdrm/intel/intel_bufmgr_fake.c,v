head	1.13;
access;
symbols
	OPENBSD_6_1_BASE:1.13
	libdrm_2_4_75:1.1.1.5
	libdrm_2_4_73:1.1.1.4
	libdrm_2_4_71:1.1.1.4
	OPENBSD_6_0:1.11.0.2
	OPENBSD_6_0_BASE:1.11
	libdrm_2_4_67:1.1.1.3
	OPENBSD_5_9:1.10.0.2
	OPENBSD_5_9_BASE:1.10
	libdrm_2_4_65:1.1.1.2
	libdrm_2_4_64:1.1.1.1
	libdrm:1.1.1
	OPENBSD_5_8:1.9.0.2
	OPENBSD_5_8_BASE:1.9
	OPENBSD_5_7:1.8.0.2
	OPENBSD_5_7_BASE:1.8
	OPENBSD_5_6:1.7.0.2
	OPENBSD_5_6_BASE:1.7
	OPENBSD_5_5:1.6.0.6
	OPENBSD_5_5_BASE:1.6
	OPENBSD_5_4:1.6.0.4
	OPENBSD_5_4_BASE:1.6
	OPENBSD_5_3:1.6.0.2
	OPENBSD_5_3_BASE:1.6
	OPENBSD_5_2:1.5.0.6
	OPENBSD_5_2_BASE:1.5
	OPENBSD_5_1_BASE:1.5
	OPENBSD_5_1:1.5.0.4
	OPENBSD_5_0:1.5.0.2
	OPENBSD_5_0_BASE:1.5
	OPENBSD_4_9:1.4.0.2
	OPENBSD_4_9_BASE:1.4
	OPENBSD_4_8:1.4.0.4
	OPENBSD_4_8_BASE:1.4
	OPENBSD_4_7:1.3.0.4
	OPENBSD_4_7_BASE:1.3
	OPENBSD_4_6:1.3.0.2
	OPENBSD_4_6_BASE:1.3
	OPENBSD_4_5:1.1.0.2
	OPENBSD_4_5_BASE:1.1;
locks; strict;
comment	@ * @;


1.13
date	2017.02.05.05.46.41;	author jsg;	state Exp;
branches;
next	1.12;
commitid	7u73SYrl7j8vU8XM;

1.12
date	2016.10.16.06.11.11;	author jsg;	state Exp;
branches;
next	1.11;
commitid	nvLf7SydSyxGShbn;

1.11
date	2016.03.20.10.41.29;	author jsg;	state Exp;
branches;
next	1.10;
commitid	mbp2aNpFrhnQxfR8;

1.10
date	2015.12.27.09.16.53;	author jsg;	state Exp;
branches;
next	1.9;
commitid	3T6ygenFOApe7XWL;

1.9
date	2015.07.15.23.19.17;	author jsg;	state Exp;
branches;
next	1.8;
commitid	NCd5WpEjxT48SV2b;

1.8
date	2015.02.07.01.34.35;	author jsg;	state Exp;
branches;
next	1.7;
commitid	ttKh4qveShZNixll;

1.7
date	2014.05.13.05.44.06;	author jsg;	state Exp;
branches;
next	1.6;

1.6
date	2012.11.27.14.37.08;	author mpi;	state Exp;
branches;
next	1.5;

1.5
date	2011.05.01.15.48.20;	author oga;	state Exp;
branches;
next	1.4;

1.4
date	2010.03.31.06.31.45;	author oga;	state Exp;
branches;
next	1.3;

1.3
date	2009.06.21.17.39.53;	author oga;	state Exp;
branches;
next	1.2;

1.2
date	2009.05.03.19.43.26;	author oga;	state Exp;
branches;
next	1.1;

1.1
date	2009.01.26.23.14.37;	author oga;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2015.08.21.23.56.07;	author jsg;	state Exp;
branches;
next	1.1.1.2;
commitid	IINH94ZNafdY3NZg;

1.1.1.2
date	2015.12.27.08.58.06;	author jsg;	state Exp;
branches;
next	1.1.1.3;
commitid	1d5D4vlIz2Mv01RY;

1.1.1.3
date	2016.03.20.10.19.55;	author jsg;	state Exp;
branches;
next	1.1.1.4;
commitid	66ApiJCJocBDCoju;

1.1.1.4
date	2016.10.16.06.01.31;	author jsg;	state Exp;
branches;
next	1.1.1.5;
commitid	UDYjDzwIWWXeKnu7;

1.1.1.5
date	2017.02.05.05.38.28;	author jsg;	state Exp;
branches;
next	;
commitid	GTUS6FM9u0BarnEJ;


desc
@@


1.13
log
@Merge libdrm 2.4.75
@
text
@/**************************************************************************
 * 
 * Copyright 2006 Tungsten Graphics, Inc., Cedar Park, Texas.
 * All Rights Reserved.
 * 
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sub license, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 * 
 * The above copyright notice and this permission notice (including the
 * next paragraph) shall be included in all copies or substantial portions
 * of the Software.
 * 
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
 * IN NO EVENT SHALL TUNGSTEN GRAPHICS AND/OR ITS SUPPLIERS BE LIABLE FOR
 * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 * 
 **************************************************************************/

/* Originally a fake version of the buffer manager so that we can
 * prototype the changes in a driver fairly quickly, has been fleshed
 * out to a fully functional interim solution.
 *
 * Basically wraps the old style memory management in the new
 * programming interface, but is more expressive and avoids many of
 * the bugs in the old texture manager.
 */

#ifdef HAVE_CONFIG_H
#include "config.h"
#endif

#include <stdlib.h>
#include <string.h>
#include <assert.h>
#include <errno.h>
#include <strings.h>
#include <xf86drm.h>
#include <pthread.h>
#include "intel_bufmgr.h"
#include "intel_bufmgr_priv.h"
#include "drm.h"
#include "i915_drm.h"
#include "mm.h"
#include "libdrm_macros.h"
#include "libdrm_lists.h"

#define DBG(...) do {					\
	if (bufmgr_fake->bufmgr.debug)			\
		drmMsg(__VA_ARGS__);			\
} while (0)

/* Internal flags:
 */
#define BM_NO_BACKING_STORE			0x00000001
#define BM_NO_FENCE_SUBDATA			0x00000002
#define BM_PINNED				0x00000004

/* Wrapper around mm.c's mem_block, which understands that you must
 * wait for fences to expire before memory can be freed.  This is
 * specific to our use of memcpy for uploads - an upload that was
 * processed through the command queue wouldn't need to care about
 * fences.
 */
#define MAX_RELOCS 4096

struct fake_buffer_reloc {
	/** Buffer object that the relocation points at. */
	drm_intel_bo *target_buf;
	/** Offset of the relocation entry within reloc_buf. */
	uint32_t offset;
	/**
	 * Cached value of the offset when we last performed this relocation.
	 */
	uint32_t last_target_offset;
	/** Value added to target_buf's offset to get the relocation entry. */
	uint32_t delta;
	/** Cache domains the target buffer is read into. */
	uint32_t read_domains;
	/** Cache domain the target buffer will have dirty cachelines in. */
	uint32_t write_domain;
};

struct block {
	struct block *next, *prev;
	struct mem_block *mem;	/* BM_MEM_AGP */

	/**
	 * Marks that the block is currently in the aperture and has yet to be
	 * fenced.
	 */
	unsigned on_hardware:1;
	/**
	 * Marks that the block is currently fenced (being used by rendering)
	 * and can't be freed until @@fence is passed.
	 */
	unsigned fenced:1;

	/** Fence cookie for the block. */
	unsigned fence;		/* Split to read_fence, write_fence */

	drm_intel_bo *bo;
	void *virtual;
};

typedef struct _bufmgr_fake {
	drm_intel_bufmgr bufmgr;

	pthread_mutex_t lock;

	unsigned long low_offset;
	unsigned long size;
	void *virtual;

	struct mem_block *heap;

	unsigned buf_nr;	/* for generating ids */

	/**
	 * List of blocks which are currently in the GART but haven't been
	 * fenced yet.
	 */
	struct block on_hardware;
	/**
	 * List of blocks which are in the GART and have an active fence on
	 * them.
	 */
	struct block fenced;
	/**
	 * List of blocks which have an expired fence and are ready to be
	 * evicted.
	 */
	struct block lru;

	unsigned int last_fence;

	unsigned fail:1;
	unsigned need_fence:1;
	int thrashing;

	/**
	 * Driver callback to emit a fence, returning the cookie.
	 *
	 * This allows the driver to hook in a replacement for the DRM usage in
	 * bufmgr_fake.
	 *
	 * Currently, this also requires that a write flush be emitted before
	 * emitting the fence, but this should change.
	 */
	unsigned int (*fence_emit) (void *private);
	/** Driver callback to wait for a fence cookie to have passed. */
	void (*fence_wait) (unsigned int fence, void *private);
	void *fence_priv;

	/**
	 * Driver callback to execute a buffer.
	 *
	 * This allows the driver to hook in a replacement for the DRM usage in
	 * bufmgr_fake.
	 */
	int (*exec) (drm_intel_bo *bo, unsigned int used, void *priv);
	void *exec_priv;

	/** Driver-supplied argument to driver callbacks */
	void *driver_priv;
	/**
	 * Pointer to kernel-updated sarea data for the last completed user irq
	 */
	volatile int *last_dispatch;

	int fd;

	int debug;

	int performed_rendering;
} drm_intel_bufmgr_fake;

typedef struct _drm_intel_bo_fake {
	drm_intel_bo bo;

	unsigned id;		/* debug only */
	const char *name;

	unsigned dirty:1;
	/**
	 * has the card written to this buffer - we make need to copy it back
	 */
	unsigned card_dirty:1;
	unsigned int refcount;
	/* Flags may consist of any of the DRM_BO flags, plus
	 * DRM_BO_NO_BACKING_STORE and BM_NO_FENCE_SUBDATA, which are the
	 * first two driver private flags.
	 */
	uint64_t flags;
	/** Cache domains the target buffer is read into. */
	uint32_t read_domains;
	/** Cache domain the target buffer will have dirty cachelines in. */
	uint32_t write_domain;

	unsigned int alignment;
	int is_static, validated;
	unsigned int map_count;

	/** relocation list */
	struct fake_buffer_reloc *relocs;
	int nr_relocs;
	/**
	 * Total size of the target_bos of this buffer.
	 *
	 * Used for estimation in check_aperture.
	 */
	unsigned int child_size;

	struct block *block;
	void *backing_store;
	void (*invalidate_cb) (drm_intel_bo *bo, void *ptr);
	void *invalidate_ptr;
} drm_intel_bo_fake;

static int clear_fenced(drm_intel_bufmgr_fake *bufmgr_fake,
			unsigned int fence_cookie);

#define MAXFENCE 0x7fffffff

static int
FENCE_LTE(unsigned a, unsigned b)
{
	if (a == b)
		return 1;

	if (a < b && b - a < (1 << 24))
		return 1;

	if (a > b && MAXFENCE - a + b < (1 << 24))
		return 1;

	return 0;
}

void
drm_intel_bufmgr_fake_set_fence_callback(drm_intel_bufmgr *bufmgr,
					 unsigned int (*emit) (void *priv),
					 void (*wait) (unsigned int fence,
						       void *priv),
					 void *priv)
{
	drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *) bufmgr;

	bufmgr_fake->fence_emit = emit;
	bufmgr_fake->fence_wait = wait;
	bufmgr_fake->fence_priv = priv;
}

static unsigned int
_fence_emit_internal(drm_intel_bufmgr_fake *bufmgr_fake)
{
	struct drm_i915_irq_emit ie;
	int ret, seq = 1;

	if (bufmgr_fake->fence_emit != NULL) {
		seq = bufmgr_fake->fence_emit(bufmgr_fake->fence_priv);
		return seq;
	}

	ie.irq_seq = &seq;
	ret = drmCommandWriteRead(bufmgr_fake->fd, DRM_I915_IRQ_EMIT,
				  &ie, sizeof(ie));
	if (ret) {
		drmMsg("%s: drm_i915_irq_emit: %d\n", __func__, ret);
		abort();
	}

	DBG("emit 0x%08x\n", seq);
	return seq;
}

static void
_fence_wait_internal(drm_intel_bufmgr_fake *bufmgr_fake, int seq)
{
	struct drm_i915_irq_wait iw;
	int hw_seq, busy_count = 0;
	int ret;
	int kernel_lied;

	if (bufmgr_fake->fence_wait != NULL) {
		bufmgr_fake->fence_wait(seq, bufmgr_fake->fence_priv);
		clear_fenced(bufmgr_fake, seq);
		return;
	}

	iw.irq_seq = seq;

	DBG("wait 0x%08x\n", iw.irq_seq);

	/* The kernel IRQ_WAIT implementation is all sorts of broken.
	 * 1) It returns 1 to 0x7fffffff instead of using the full 32-bit
	 *    unsigned range.
	 * 2) It returns 0 if hw_seq >= seq, not seq - hw_seq < 0 on the 32-bit
	 *    signed range.
	 * 3) It waits if seq < hw_seq, not seq - hw_seq > 0 on the 32-bit
	 *    signed range.
	 * 4) It returns -EBUSY in 3 seconds even if the hardware is still
	 *    successfully chewing through buffers.
	 *
	 * Assume that in userland we treat sequence numbers as ints, which
	 * makes some of the comparisons convenient, since the sequence
	 * numbers are all positive signed integers.
	 *
	 * From this we get several cases we need to handle.  Here's a timeline.
	 * 0x2   0x7                                    0x7ffffff8   0x7ffffffd
	 *   |    |                                             |    |
	 * ------------------------------------------------------------
	 *
	 * A) Normal wait for hw to catch up
	 * hw_seq seq
	 *   |    |
	 * ------------------------------------------------------------
	 * seq - hw_seq = 5.  If we call IRQ_WAIT, it will wait for hw to
	 * catch up.
	 *
	 * B) Normal wait for a sequence number that's already passed.
	 * seq    hw_seq
	 *   |    |
	 * ------------------------------------------------------------
	 * seq - hw_seq = -5.  If we call IRQ_WAIT, it returns 0 quickly.
	 *
	 * C) Hardware has already wrapped around ahead of us
	 * hw_seq                                                    seq
	 *   |                                                       |
	 * ------------------------------------------------------------
	 * seq - hw_seq = 0x80000000 - 5.  If we called IRQ_WAIT, it would wait
	 * for hw_seq >= seq, which may never occur.  Thus, we want to catch
	 * this in userland and return 0.
	 *
	 * D) We've wrapped around ahead of the hardware.
	 * seq                                                      hw_seq
	 *   |                                                       |
	 * ------------------------------------------------------------
	 * seq - hw_seq = -(0x80000000 - 5).  If we called IRQ_WAIT, it would
	 * return 0 quickly because hw_seq >= seq, even though the hardware
	 * isn't caught up. Thus, we need to catch this early return in
	 * userland and bother the kernel until the hardware really does
	 * catch up.
	 *
	 * E) Hardware might wrap after we test in userland.
	 *                                                  hw_seq  seq
	 *                                                      |    |
	 * ------------------------------------------------------------
	 * seq - hw_seq = 5.  If we call IRQ_WAIT, it will likely see seq >=
	 * hw_seq and wait.  However, suppose hw_seq wraps before we make it
	 * into the kernel.  The kernel sees hw_seq >= seq and waits for 3
	 * seconds then returns -EBUSY.  This is case C).  We should catch
	 * this and then return successfully.
	 *
	 * F) Hardware might take a long time on a buffer.
	 * hw_seq seq
	 *   |    |
	 * -------------------------------------------------------------------
	 * seq - hw_seq = 5.  If we call IRQ_WAIT, if sequence 2 through 5
	 * take too long, it will return -EBUSY.  Batchbuffers in the
	 * gltestperf demo were seen to take up to 7 seconds.  We should
	 * catch early -EBUSY return and keep trying.
	 */

	do {
		/* Keep a copy of last_dispatch so that if the wait -EBUSYs
		 * because the hardware didn't catch up in 3 seconds, we can
		 * see if it at least made progress and retry.
		 */
		hw_seq = *bufmgr_fake->last_dispatch;

		/* Catch case C */
		if (seq - hw_seq > 0x40000000)
			return;

		ret = drmCommandWrite(bufmgr_fake->fd, DRM_I915_IRQ_WAIT,
				      &iw, sizeof(iw));
		/* Catch case D */
		kernel_lied = (ret == 0) && (seq - *bufmgr_fake->last_dispatch <
					     -0x40000000);

		/* Catch case E */
		if (ret == -EBUSY
		    && (seq - *bufmgr_fake->last_dispatch > 0x40000000))
			ret = 0;

		/* Catch case F: Allow up to 15 seconds chewing on one buffer. */
		if ((ret == -EBUSY) && (hw_seq != *bufmgr_fake->last_dispatch))
			busy_count = 0;
		else
			busy_count++;
	} while (kernel_lied || ret == -EAGAIN || ret == -EINTR ||
		 (ret == -EBUSY && busy_count < 5));

	if (ret != 0) {
		drmMsg("%s:%d: Error waiting for fence: %s.\n", __FILE__,
		       __LINE__, strerror(-ret));
		abort();
	}
	clear_fenced(bufmgr_fake, seq);
}

static int
_fence_test(drm_intel_bufmgr_fake *bufmgr_fake, unsigned fence)
{
	/* Slight problem with wrap-around:
	 */
	return fence == 0 || FENCE_LTE(fence, bufmgr_fake->last_fence);
}

/**
 * Allocate a memory manager block for the buffer.
 */
static int
alloc_block(drm_intel_bo *bo)
{
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	struct block *block = (struct block *)calloc(sizeof *block, 1);
	unsigned int align_log2 = ffs(bo_fake->alignment) - 1;
	unsigned int sz;

	if (!block)
		return 1;

	sz = (bo->size + bo_fake->alignment - 1) & ~(bo_fake->alignment - 1);

	block->mem = mmAllocMem(bufmgr_fake->heap, sz, align_log2, 0);
	if (!block->mem) {
		free(block);
		return 0;
	}

	DRMINITLISTHEAD(block);

	/* Insert at head or at tail??? */
	DRMLISTADDTAIL(block, &bufmgr_fake->lru);

	block->virtual = (uint8_t *) bufmgr_fake->virtual +
	    block->mem->ofs - bufmgr_fake->low_offset;
	block->bo = bo;

	bo_fake->block = block;

	return 1;
}

/* Release the card storage associated with buf:
 */
static void
free_block(drm_intel_bufmgr_fake *bufmgr_fake, struct block *block,
	   int skip_dirty_copy)
{
	drm_intel_bo_fake *bo_fake;
	DBG("free block %p %08x %d %d\n", block, block->mem->ofs,
	    block->on_hardware, block->fenced);

	if (!block)
		return;

	bo_fake = (drm_intel_bo_fake *) block->bo;

	if (bo_fake->flags & (BM_PINNED | BM_NO_BACKING_STORE))
		skip_dirty_copy = 1;

	if (!skip_dirty_copy && (bo_fake->card_dirty == 1)) {
		memcpy(bo_fake->backing_store, block->virtual, block->bo->size);
		bo_fake->card_dirty = 0;
		bo_fake->dirty = 1;
	}

	if (block->on_hardware) {
		block->bo = NULL;
	} else if (block->fenced) {
		block->bo = NULL;
	} else {
		DBG("    - free immediately\n");
		DRMLISTDEL(block);

		mmFreeMem(block->mem);
		free(block);
	}
}

static void
alloc_backing_store(drm_intel_bo *bo)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;
	assert(!bo_fake->backing_store);
	assert(!(bo_fake->flags & (BM_PINNED | BM_NO_BACKING_STORE)));

	bo_fake->backing_store = malloc(bo->size);

	DBG("alloc_backing - buf %d %p %lu\n", bo_fake->id,
	    bo_fake->backing_store, bo->size);
	assert(bo_fake->backing_store);
}

static void
free_backing_store(drm_intel_bo *bo)
{
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;

	if (bo_fake->backing_store) {
		assert(!(bo_fake->flags & (BM_PINNED | BM_NO_BACKING_STORE)));
		free(bo_fake->backing_store);
		bo_fake->backing_store = NULL;
	}
}

static void
set_dirty(drm_intel_bo *bo)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;

	if (bo_fake->flags & BM_NO_BACKING_STORE
	    && bo_fake->invalidate_cb != NULL)
		bo_fake->invalidate_cb(bo, bo_fake->invalidate_ptr);

	assert(!(bo_fake->flags & BM_PINNED));

	DBG("set_dirty - buf %d\n", bo_fake->id);
	bo_fake->dirty = 1;
}

static int
evict_lru(drm_intel_bufmgr_fake *bufmgr_fake, unsigned int max_fence)
{
	struct block *block, *tmp;

	DBG("%s\n", __func__);

	DRMLISTFOREACHSAFE(block, tmp, &bufmgr_fake->lru) {
		drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) block->bo;

		if (bo_fake != NULL && (bo_fake->flags & BM_NO_FENCE_SUBDATA))
			continue;

		if (block->fence && max_fence && !FENCE_LTE(block->fence,
							    max_fence))
			return 0;

		set_dirty(&bo_fake->bo);
		bo_fake->block = NULL;

		free_block(bufmgr_fake, block, 0);
		return 1;
	}

	return 0;
}

static int
evict_mru(drm_intel_bufmgr_fake *bufmgr_fake)
{
	struct block *block, *tmp;

	DBG("%s\n", __func__);

	DRMLISTFOREACHSAFEREVERSE(block, tmp, &bufmgr_fake->lru) {
		drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) block->bo;

		if (bo_fake && (bo_fake->flags & BM_NO_FENCE_SUBDATA))
			continue;

		set_dirty(&bo_fake->bo);
		bo_fake->block = NULL;

		free_block(bufmgr_fake, block, 0);
		return 1;
	}

	return 0;
}

/**
 * Removes all objects from the fenced list older than the given fence.
 */
static int
clear_fenced(drm_intel_bufmgr_fake *bufmgr_fake, unsigned int fence_cookie)
{
	struct block *block, *tmp;
	int ret = 0;

	bufmgr_fake->last_fence = fence_cookie;
	DRMLISTFOREACHSAFE(block, tmp, &bufmgr_fake->fenced) {
		assert(block->fenced);

		if (_fence_test(bufmgr_fake, block->fence)) {

			block->fenced = 0;

			if (!block->bo) {
				DBG("delayed free: offset %x sz %x\n",
				    block->mem->ofs, block->mem->size);
				DRMLISTDEL(block);
				mmFreeMem(block->mem);
				free(block);
			} else {
				DBG("return to lru: offset %x sz %x\n",
				    block->mem->ofs, block->mem->size);
				DRMLISTDEL(block);
				DRMLISTADDTAIL(block, &bufmgr_fake->lru);
			}

			ret = 1;
		} else {
			/* Blocks are ordered by fence, so if one fails, all
			 * from here will fail also:
			 */
			DBG("fence not passed: offset %x sz %x %d %d \n",
			    block->mem->ofs, block->mem->size, block->fence,
			    bufmgr_fake->last_fence);
			break;
		}
	}

	DBG("%s: %d\n", __func__, ret);
	return ret;
}

static void
fence_blocks(drm_intel_bufmgr_fake *bufmgr_fake, unsigned fence)
{
	struct block *block, *tmp;

	DRMLISTFOREACHSAFE(block, tmp, &bufmgr_fake->on_hardware) {
		DBG("Fence block %p (sz 0x%x ofs %x buf %p) with fence %d\n",
		    block, block->mem->size, block->mem->ofs, block->bo, fence);
		block->fence = fence;

		block->on_hardware = 0;
		block->fenced = 1;

		/* Move to tail of pending list here
		 */
		DRMLISTDEL(block);
		DRMLISTADDTAIL(block, &bufmgr_fake->fenced);
	}

	assert(DRMLISTEMPTY(&bufmgr_fake->on_hardware));
}

static int
evict_and_alloc_block(drm_intel_bo *bo)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;

	assert(bo_fake->block == NULL);

	/* Search for already free memory:
	 */
	if (alloc_block(bo))
		return 1;

	/* If we're not thrashing, allow lru eviction to dig deeper into
	 * recently used textures.  We'll probably be thrashing soon:
	 */
	if (!bufmgr_fake->thrashing) {
		while (evict_lru(bufmgr_fake, 0))
			if (alloc_block(bo))
				return 1;
	}

	/* Keep thrashing counter alive?
	 */
	if (bufmgr_fake->thrashing)
		bufmgr_fake->thrashing = 20;

	/* Wait on any already pending fences - here we are waiting for any
	 * freed memory that has been submitted to hardware and fenced to
	 * become available:
	 */
	while (!DRMLISTEMPTY(&bufmgr_fake->fenced)) {
		uint32_t fence = bufmgr_fake->fenced.next->fence;
		_fence_wait_internal(bufmgr_fake, fence);

		if (alloc_block(bo))
			return 1;
	}

	if (!DRMLISTEMPTY(&bufmgr_fake->on_hardware)) {
		while (!DRMLISTEMPTY(&bufmgr_fake->fenced)) {
			uint32_t fence = bufmgr_fake->fenced.next->fence;
			_fence_wait_internal(bufmgr_fake, fence);
		}

		if (!bufmgr_fake->thrashing) {
			DBG("thrashing\n");
		}
		bufmgr_fake->thrashing = 20;

		if (alloc_block(bo))
			return 1;
	}

	while (evict_mru(bufmgr_fake))
		if (alloc_block(bo))
			return 1;

	DBG("%s 0x%lx bytes failed\n", __func__, bo->size);

	return 0;
}

/***********************************************************************
 * Public functions
 */

/**
 * Wait for hardware idle by emitting a fence and waiting for it.
 */
static void
drm_intel_bufmgr_fake_wait_idle(drm_intel_bufmgr_fake *bufmgr_fake)
{
	unsigned int cookie;

	cookie = _fence_emit_internal(bufmgr_fake);
	_fence_wait_internal(bufmgr_fake, cookie);
}

/**
 * Wait for rendering to a buffer to complete.
 *
 * It is assumed that the batchbuffer which performed the rendering included
 * the necessary flushing.
 */
static void
drm_intel_fake_bo_wait_rendering_locked(drm_intel_bo *bo)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;

	if (bo_fake->block == NULL || !bo_fake->block->fenced)
		return;

	_fence_wait_internal(bufmgr_fake, bo_fake->block->fence);
}

static void
drm_intel_fake_bo_wait_rendering(drm_intel_bo *bo)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;

	pthread_mutex_lock(&bufmgr_fake->lock);
	drm_intel_fake_bo_wait_rendering_locked(bo);
	pthread_mutex_unlock(&bufmgr_fake->lock);
}

/* Specifically ignore texture memory sharing.
 *  -- just evict everything
 *  -- and wait for idle
 */
void
drm_intel_bufmgr_fake_contended_lock_take(drm_intel_bufmgr *bufmgr)
{
	drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *) bufmgr;
	struct block *block, *tmp;

	pthread_mutex_lock(&bufmgr_fake->lock);

	bufmgr_fake->need_fence = 1;
	bufmgr_fake->fail = 0;

	/* Wait for hardware idle.  We don't know where acceleration has been
	 * happening, so we'll need to wait anyway before letting anything get
	 * put on the card again.
	 */
	drm_intel_bufmgr_fake_wait_idle(bufmgr_fake);

	/* Check that we hadn't released the lock without having fenced the last
	 * set of buffers.
	 */
	assert(DRMLISTEMPTY(&bufmgr_fake->fenced));
	assert(DRMLISTEMPTY(&bufmgr_fake->on_hardware));

	DRMLISTFOREACHSAFE(block, tmp, &bufmgr_fake->lru) {
		assert(_fence_test(bufmgr_fake, block->fence));
		set_dirty(block->bo);
	}

	pthread_mutex_unlock(&bufmgr_fake->lock);
}

static drm_intel_bo *
drm_intel_fake_bo_alloc(drm_intel_bufmgr *bufmgr,
			const char *name,
			unsigned long size,
			unsigned int alignment)
{
	drm_intel_bufmgr_fake *bufmgr_fake;
	drm_intel_bo_fake *bo_fake;

	bufmgr_fake = (drm_intel_bufmgr_fake *) bufmgr;

	assert(size != 0);

	bo_fake = calloc(1, sizeof(*bo_fake));
	if (!bo_fake)
		return NULL;

	bo_fake->bo.size = size;
	bo_fake->bo.offset = -1;
	bo_fake->bo.virtual = NULL;
	bo_fake->bo.bufmgr = bufmgr;
	bo_fake->refcount = 1;

	/* Alignment must be a power of two */
	assert((alignment & (alignment - 1)) == 0);
	if (alignment == 0)
		alignment = 1;
	bo_fake->alignment = alignment;
	bo_fake->id = ++bufmgr_fake->buf_nr;
	bo_fake->name = name;
	bo_fake->flags = 0;
	bo_fake->is_static = 0;

	DBG("drm_bo_alloc: (buf %d: %s, %lu kb)\n", bo_fake->id, bo_fake->name,
	    bo_fake->bo.size / 1024);

	return &bo_fake->bo;
}

static drm_intel_bo *
drm_intel_fake_bo_alloc_tiled(drm_intel_bufmgr * bufmgr,
			      const char *name,
			      int x, int y, int cpp,
			      uint32_t *tiling_mode,
			      unsigned long *pitch,
			      unsigned long flags)
{
	unsigned long stride, aligned_y;

	/* No runtime tiling support for fake. */
	*tiling_mode = I915_TILING_NONE;

	/* Align it for being a render target.  Shouldn't need anything else. */
	stride = x * cpp;
	stride = ROUND_UP_TO(stride, 64);

	/* 965 subspan loading alignment */
	aligned_y = ALIGN(y, 2);

	*pitch = stride;

	return drm_intel_fake_bo_alloc(bufmgr, name, stride * aligned_y,
				       4096);
}

drm_intel_bo *
drm_intel_bo_fake_alloc_static(drm_intel_bufmgr *bufmgr,
			       const char *name,
			       unsigned long offset,
			       unsigned long size, void *virtual)
{
	drm_intel_bufmgr_fake *bufmgr_fake;
	drm_intel_bo_fake *bo_fake;

	bufmgr_fake = (drm_intel_bufmgr_fake *) bufmgr;

	assert(size != 0);

	bo_fake = calloc(1, sizeof(*bo_fake));
	if (!bo_fake)
		return NULL;

	bo_fake->bo.size = size;
	bo_fake->bo.offset = offset;
	bo_fake->bo.virtual = virtual;
	bo_fake->bo.bufmgr = bufmgr;
	bo_fake->refcount = 1;
	bo_fake->id = ++bufmgr_fake->buf_nr;
	bo_fake->name = name;
	bo_fake->flags = BM_PINNED;
	bo_fake->is_static = 1;

	DBG("drm_bo_alloc_static: (buf %d: %s, %lu kb)\n", bo_fake->id,
	    bo_fake->name, bo_fake->bo.size / 1024);

	return &bo_fake->bo;
}

static void
drm_intel_fake_bo_reference(drm_intel_bo *bo)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;

	pthread_mutex_lock(&bufmgr_fake->lock);
	bo_fake->refcount++;
	pthread_mutex_unlock(&bufmgr_fake->lock);
}

static void
drm_intel_fake_bo_reference_locked(drm_intel_bo *bo)
{
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;

	bo_fake->refcount++;
}

static void
drm_intel_fake_bo_unreference_locked(drm_intel_bo *bo)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;
	int i;

	if (--bo_fake->refcount == 0) {
		assert(bo_fake->map_count == 0);
		/* No remaining references, so free it */
		if (bo_fake->block)
			free_block(bufmgr_fake, bo_fake->block, 1);
		free_backing_store(bo);

		for (i = 0; i < bo_fake->nr_relocs; i++)
			drm_intel_fake_bo_unreference_locked(bo_fake->relocs[i].
							     target_buf);

		DBG("drm_bo_unreference: free buf %d %s\n", bo_fake->id,
		    bo_fake->name);

		free(bo_fake->relocs);
		free(bo);
	}
}

static void
drm_intel_fake_bo_unreference(drm_intel_bo *bo)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;

	pthread_mutex_lock(&bufmgr_fake->lock);
	drm_intel_fake_bo_unreference_locked(bo);
	pthread_mutex_unlock(&bufmgr_fake->lock);
}

/**
 * Set the buffer as not requiring backing store, and instead get the callback
 * invoked whenever it would be set dirty.
 */
void
drm_intel_bo_fake_disable_backing_store(drm_intel_bo *bo,
					void (*invalidate_cb) (drm_intel_bo *bo,
							       void *ptr),
					void *ptr)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;

	pthread_mutex_lock(&bufmgr_fake->lock);

	if (bo_fake->backing_store)
		free_backing_store(bo);

	bo_fake->flags |= BM_NO_BACKING_STORE;

	DBG("disable_backing_store set buf %d dirty\n", bo_fake->id);
	bo_fake->dirty = 1;
	bo_fake->invalidate_cb = invalidate_cb;
	bo_fake->invalidate_ptr = ptr;

	/* Note that it is invalid right from the start.  Also note
	 * invalidate_cb is called with the bufmgr locked, so cannot
	 * itself make bufmgr calls.
	 */
	if (invalidate_cb != NULL)
		invalidate_cb(bo, ptr);

	pthread_mutex_unlock(&bufmgr_fake->lock);
}

/**
 * Map a buffer into bo->virtual, allocating either card memory space (If
 * BM_NO_BACKING_STORE or BM_PINNED) or backing store, as necessary.
 */
static int
 drm_intel_fake_bo_map_locked(drm_intel_bo *bo, int write_enable)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;

	/* Static buffers are always mapped. */
	if (bo_fake->is_static) {
		if (bo_fake->card_dirty) {
			drm_intel_bufmgr_fake_wait_idle(bufmgr_fake);
			bo_fake->card_dirty = 0;
		}
		return 0;
	}

	/* Allow recursive mapping.  Mesa may recursively map buffers with
	 * nested display loops, and it is used internally in bufmgr_fake
	 * for relocation.
	 */
	if (bo_fake->map_count++ != 0)
		return 0;

	{
		DBG("drm_bo_map: (buf %d: %s, %lu kb)\n", bo_fake->id,
		    bo_fake->name, bo_fake->bo.size / 1024);

		if (bo->virtual != NULL) {
			drmMsg("%s: already mapped\n", __func__);
			abort();
		} else if (bo_fake->flags & (BM_NO_BACKING_STORE | BM_PINNED)) {

			if (!bo_fake->block && !evict_and_alloc_block(bo)) {
				DBG("%s: alloc failed\n", __func__);
				bufmgr_fake->fail = 1;
				return 1;
			} else {
				assert(bo_fake->block);
				bo_fake->dirty = 0;

				if (!(bo_fake->flags & BM_NO_FENCE_SUBDATA) &&
				    bo_fake->block->fenced) {
					drm_intel_fake_bo_wait_rendering_locked
					    (bo);
				}

				bo->virtual = bo_fake->block->virtual;
			}
		} else {
			if (write_enable)
				set_dirty(bo);

			if (bo_fake->backing_store == 0)
				alloc_backing_store(bo);

			if ((bo_fake->card_dirty == 1) && bo_fake->block) {
				if (bo_fake->block->fenced)
					drm_intel_fake_bo_wait_rendering_locked
					    (bo);

				memcpy(bo_fake->backing_store,
				       bo_fake->block->virtual,
				       bo_fake->block->bo->size);
				bo_fake->card_dirty = 0;
			}

			bo->virtual = bo_fake->backing_store;
		}
	}

	return 0;
}

static int
 drm_intel_fake_bo_map(drm_intel_bo *bo, int write_enable)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	int ret;

	pthread_mutex_lock(&bufmgr_fake->lock);
	ret = drm_intel_fake_bo_map_locked(bo, write_enable);
	pthread_mutex_unlock(&bufmgr_fake->lock);

	return ret;
}

static int
 drm_intel_fake_bo_unmap_locked(drm_intel_bo *bo)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;

	/* Static buffers are always mapped. */
	if (bo_fake->is_static)
		return 0;

	assert(bo_fake->map_count != 0);
	if (--bo_fake->map_count != 0)
		return 0;

	DBG("drm_bo_unmap: (buf %d: %s, %lu kb)\n", bo_fake->id, bo_fake->name,
	    bo_fake->bo.size / 1024);

	bo->virtual = NULL;

	return 0;
}

static int drm_intel_fake_bo_unmap(drm_intel_bo *bo)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	int ret;

	pthread_mutex_lock(&bufmgr_fake->lock);
	ret = drm_intel_fake_bo_unmap_locked(bo);
	pthread_mutex_unlock(&bufmgr_fake->lock);

	return ret;
}

static int
drm_intel_fake_bo_subdata(drm_intel_bo *bo, unsigned long offset,
			  unsigned long size, const void *data)
{
	int ret;

	if (size == 0 || data == NULL)
		return 0;

	ret = drm_intel_bo_map(bo, 1);
	if (ret)
		return ret;
	memcpy((unsigned char *)bo->virtual + offset, data, size);
	drm_intel_bo_unmap(bo);
	return 0;
}

static void
 drm_intel_fake_kick_all_locked(drm_intel_bufmgr_fake *bufmgr_fake)
{
	struct block *block, *tmp;

	bufmgr_fake->performed_rendering = 0;
	/* okay for ever BO that is on the HW kick it off.
	   seriously not afraid of the POLICE right now */
	DRMLISTFOREACHSAFE(block, tmp, &bufmgr_fake->on_hardware) {
		drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) block->bo;

		block->on_hardware = 0;
		free_block(bufmgr_fake, block, 0);
		bo_fake->block = NULL;
		bo_fake->validated = 0;
		if (!(bo_fake->flags & BM_NO_BACKING_STORE))
			bo_fake->dirty = 1;
	}

}

static int
 drm_intel_fake_bo_validate(drm_intel_bo *bo)
{
	drm_intel_bufmgr_fake *bufmgr_fake;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;

	bufmgr_fake = (drm_intel_bufmgr_fake *) bo->bufmgr;

	DBG("drm_bo_validate: (buf %d: %s, %lu kb)\n", bo_fake->id,
	    bo_fake->name, bo_fake->bo.size / 1024);

	/* Sanity check: Buffers should be unmapped before being validated.
	 * This is not so much of a problem for bufmgr_fake, but TTM refuses,
	 * and the problem is harder to debug there.
	 */
	assert(bo_fake->map_count == 0);

	if (bo_fake->is_static) {
		/* Add it to the needs-fence list */
		bufmgr_fake->need_fence = 1;
		return 0;
	}

	/* Allocate the card memory */
	if (!bo_fake->block && !evict_and_alloc_block(bo)) {
		bufmgr_fake->fail = 1;
		DBG("Failed to validate buf %d:%s\n", bo_fake->id,
		    bo_fake->name);
		return -1;
	}

	assert(bo_fake->block);
	assert(bo_fake->block->bo == &bo_fake->bo);

	bo->offset = bo_fake->block->mem->ofs;

	/* Upload the buffer contents if necessary */
	if (bo_fake->dirty) {
		DBG("Upload dirty buf %d:%s, sz %lu offset 0x%x\n", bo_fake->id,
		    bo_fake->name, bo->size, bo_fake->block->mem->ofs);

		assert(!(bo_fake->flags & (BM_NO_BACKING_STORE | BM_PINNED)));

		/* Actually, should be able to just wait for a fence on the
		 * memory, which we would be tracking when we free it. Waiting
		 * for idle is a sufficiently large hammer for now.
		 */
		drm_intel_bufmgr_fake_wait_idle(bufmgr_fake);

		/* we may never have mapped this BO so it might not have any
		 * backing store if this happens it should be rare, but 0 the
		 * card memory in any case */
		if (bo_fake->backing_store)
			memcpy(bo_fake->block->virtual, bo_fake->backing_store,
			       bo->size);
		else
			memset(bo_fake->block->virtual, 0, bo->size);

		bo_fake->dirty = 0;
	}

	bo_fake->block->fenced = 0;
	bo_fake->block->on_hardware = 1;
	DRMLISTDEL(bo_fake->block);
	DRMLISTADDTAIL(bo_fake->block, &bufmgr_fake->on_hardware);

	bo_fake->validated = 1;
	bufmgr_fake->need_fence = 1;

	return 0;
}

static void
drm_intel_fake_fence_validated(drm_intel_bufmgr *bufmgr)
{
	drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *) bufmgr;
	unsigned int cookie;

	cookie = _fence_emit_internal(bufmgr_fake);
	fence_blocks(bufmgr_fake, cookie);

	DBG("drm_fence_validated: 0x%08x cookie\n", cookie);
}

static void
drm_intel_fake_destroy(drm_intel_bufmgr *bufmgr)
{
	drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *) bufmgr;

	pthread_mutex_destroy(&bufmgr_fake->lock);
	mmDestroy(bufmgr_fake->heap);
	free(bufmgr);
}

static int
drm_intel_fake_emit_reloc(drm_intel_bo *bo, uint32_t offset,
			  drm_intel_bo *target_bo, uint32_t target_offset,
			  uint32_t read_domains, uint32_t write_domain)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	struct fake_buffer_reloc *r;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;
	drm_intel_bo_fake *target_fake = (drm_intel_bo_fake *) target_bo;
	int i;

	pthread_mutex_lock(&bufmgr_fake->lock);

	assert(bo);
	assert(target_bo);

	if (bo_fake->relocs == NULL) {
		bo_fake->relocs =
		    malloc(sizeof(struct fake_buffer_reloc) * MAX_RELOCS);
	}

	r = &bo_fake->relocs[bo_fake->nr_relocs++];

	assert(bo_fake->nr_relocs <= MAX_RELOCS);

	drm_intel_fake_bo_reference_locked(target_bo);

	if (!target_fake->is_static) {
		bo_fake->child_size +=
		    ALIGN(target_bo->size, target_fake->alignment);
		bo_fake->child_size += target_fake->child_size;
	}
	r->target_buf = target_bo;
	r->offset = offset;
	r->last_target_offset = target_bo->offset;
	r->delta = target_offset;
	r->read_domains = read_domains;
	r->write_domain = write_domain;

	if (bufmgr_fake->debug) {
		/* Check that a conflicting relocation hasn't already been
		 * emitted.
		 */
		for (i = 0; i < bo_fake->nr_relocs - 1; i++) {
			struct fake_buffer_reloc *r2 = &bo_fake->relocs[i];

			assert(r->offset != r2->offset);
		}
	}

	pthread_mutex_unlock(&bufmgr_fake->lock);

	return 0;
}

/**
 * Incorporates the validation flags associated with each relocation into
 * the combined validation flags for the buffer on this batchbuffer submission.
 */
static void
drm_intel_fake_calculate_domains(drm_intel_bo *bo)
{
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;
	int i;

	for (i = 0; i < bo_fake->nr_relocs; i++) {
		struct fake_buffer_reloc *r = &bo_fake->relocs[i];
		drm_intel_bo_fake *target_fake =
		    (drm_intel_bo_fake *) r->target_buf;

		/* Do the same for the tree of buffers we depend on */
		drm_intel_fake_calculate_domains(r->target_buf);

		target_fake->read_domains |= r->read_domains;
		target_fake->write_domain |= r->write_domain;
	}
}

static int
drm_intel_fake_reloc_and_validate_buffer(drm_intel_bo *bo)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;
	int i, ret;

	assert(bo_fake->map_count == 0);

	for (i = 0; i < bo_fake->nr_relocs; i++) {
		struct fake_buffer_reloc *r = &bo_fake->relocs[i];
		drm_intel_bo_fake *target_fake =
		    (drm_intel_bo_fake *) r->target_buf;
		uint32_t reloc_data;

		/* Validate the target buffer if that hasn't been done. */
		if (!target_fake->validated) {
			ret =
			    drm_intel_fake_reloc_and_validate_buffer(r->target_buf);
			if (ret != 0) {
				if (bo->virtual != NULL)
					drm_intel_fake_bo_unmap_locked(bo);
				return ret;
			}
		}

		/* Calculate the value of the relocation entry. */
		if (r->target_buf->offset != r->last_target_offset) {
			reloc_data = r->target_buf->offset + r->delta;

			if (bo->virtual == NULL)
				drm_intel_fake_bo_map_locked(bo, 1);

			*(uint32_t *) ((uint8_t *) bo->virtual + r->offset) =
			    reloc_data;

			r->last_target_offset = r->target_buf->offset;
		}
	}

	if (bo->virtual != NULL)
		drm_intel_fake_bo_unmap_locked(bo);

	if (bo_fake->write_domain != 0) {
		if (!(bo_fake->flags & (BM_NO_BACKING_STORE | BM_PINNED))) {
			if (bo_fake->backing_store == 0)
				alloc_backing_store(bo);
		}
		bo_fake->card_dirty = 1;
		bufmgr_fake->performed_rendering = 1;
	}

	return drm_intel_fake_bo_validate(bo);
}

static void
drm_intel_bo_fake_post_submit(drm_intel_bo *bo)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;
	int i;

	for (i = 0; i < bo_fake->nr_relocs; i++) {
		struct fake_buffer_reloc *r = &bo_fake->relocs[i];
		drm_intel_bo_fake *target_fake =
		    (drm_intel_bo_fake *) r->target_buf;

		if (target_fake->validated)
			drm_intel_bo_fake_post_submit(r->target_buf);

		DBG("%s@@0x%08x + 0x%08x -> %s@@0x%08x + 0x%08x\n",
		    bo_fake->name, (uint32_t) bo->offset, r->offset,
		    target_fake->name, (uint32_t) r->target_buf->offset,
		    r->delta);
	}

	assert(bo_fake->map_count == 0);
	bo_fake->validated = 0;
	bo_fake->read_domains = 0;
	bo_fake->write_domain = 0;
}

void
drm_intel_bufmgr_fake_set_exec_callback(drm_intel_bufmgr *bufmgr,
					     int (*exec) (drm_intel_bo *bo,
							  unsigned int used,
							  void *priv),
					     void *priv)
{
	drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *) bufmgr;

	bufmgr_fake->exec = exec;
	bufmgr_fake->exec_priv = priv;
}

static int
drm_intel_fake_bo_exec(drm_intel_bo *bo, int used,
		       drm_clip_rect_t * cliprects, int num_cliprects, int DR4)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *batch_fake = (drm_intel_bo_fake *) bo;
	struct drm_i915_batchbuffer batch;
	int ret;
	int retry_count = 0;

	pthread_mutex_lock(&bufmgr_fake->lock);

	bufmgr_fake->performed_rendering = 0;

	drm_intel_fake_calculate_domains(bo);

	batch_fake->read_domains = I915_GEM_DOMAIN_COMMAND;

	/* we've ran out of RAM so blow the whole lot away and retry */
restart:
	ret = drm_intel_fake_reloc_and_validate_buffer(bo);
	if (bufmgr_fake->fail == 1) {
		if (retry_count == 0) {
			retry_count++;
			drm_intel_fake_kick_all_locked(bufmgr_fake);
			bufmgr_fake->fail = 0;
			goto restart;
		} else		/* dump out the memory here */
			mmDumpMemInfo(bufmgr_fake->heap);
	}

	assert(ret == 0);

	if (bufmgr_fake->exec != NULL) {
		ret = bufmgr_fake->exec(bo, used, bufmgr_fake->exec_priv);
		if (ret != 0) {
			pthread_mutex_unlock(&bufmgr_fake->lock);
			return ret;
		}
	} else {
		batch.start = bo->offset;
		batch.used = used;
		batch.cliprects = cliprects;
		batch.num_cliprects = num_cliprects;
		batch.DR1 = 0;
		batch.DR4 = DR4;

		if (drmCommandWrite
		    (bufmgr_fake->fd, DRM_I915_BATCHBUFFER, &batch,
		     sizeof(batch))) {
			drmMsg("DRM_I915_BATCHBUFFER: %d\n", -errno);
			pthread_mutex_unlock(&bufmgr_fake->lock);
			return -errno;
		}
	}

	drm_intel_fake_fence_validated(bo->bufmgr);

	drm_intel_bo_fake_post_submit(bo);

	pthread_mutex_unlock(&bufmgr_fake->lock);

	return 0;
}

/**
 * Return an error if the list of BOs will exceed the aperture size.
 *
 * This is a rough guess and likely to fail, as during the validate sequence we
 * may place a buffer in an inopportune spot early on and then fail to fit
 * a set smaller than the aperture.
 */
static int
drm_intel_fake_check_aperture_space(drm_intel_bo ** bo_array, int count)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo_array[0]->bufmgr;
	unsigned int sz = 0;
	int i;

	for (i = 0; i < count; i++) {
		drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo_array[i];

		if (bo_fake == NULL)
			continue;

		if (!bo_fake->is_static)
			sz += ALIGN(bo_array[i]->size, bo_fake->alignment);
		sz += bo_fake->child_size;
	}

	if (sz > bufmgr_fake->size) {
		DBG("check_space: overflowed bufmgr size, %ukb vs %lukb\n",
		    sz / 1024, bufmgr_fake->size / 1024);
		return -1;
	}

	DBG("drm_check_space: sz %ukb vs bufgr %lukb\n", sz / 1024,
	    bufmgr_fake->size / 1024);
	return 0;
}

/**
 * Evicts all buffers, waiting for fences to pass and copying contents out
 * as necessary.
 *
 * Used by the X Server on LeaveVT, when the card memory is no longer our
 * own.
 */
void
drm_intel_bufmgr_fake_evict_all(drm_intel_bufmgr *bufmgr)
{
	drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *) bufmgr;
	struct block *block, *tmp;

	pthread_mutex_lock(&bufmgr_fake->lock);

	bufmgr_fake->need_fence = 1;
	bufmgr_fake->fail = 0;

	/* Wait for hardware idle.  We don't know where acceleration has been
	 * happening, so we'll need to wait anyway before letting anything get
	 * put on the card again.
	 */
	drm_intel_bufmgr_fake_wait_idle(bufmgr_fake);

	/* Check that we hadn't released the lock without having fenced the last
	 * set of buffers.
	 */
	assert(DRMLISTEMPTY(&bufmgr_fake->fenced));
	assert(DRMLISTEMPTY(&bufmgr_fake->on_hardware));

	DRMLISTFOREACHSAFE(block, tmp, &bufmgr_fake->lru) {
		drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) block->bo;
		/* Releases the memory, and memcpys dirty contents out if
		 * necessary.
		 */
		free_block(bufmgr_fake, block, 0);
		bo_fake->block = NULL;
	}

	pthread_mutex_unlock(&bufmgr_fake->lock);
}

void
drm_intel_bufmgr_fake_set_last_dispatch(drm_intel_bufmgr *bufmgr,
					volatile unsigned int
					*last_dispatch)
{
	drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *) bufmgr;

	bufmgr_fake->last_dispatch = (volatile int *)last_dispatch;
}

drm_intel_bufmgr *
drm_intel_bufmgr_fake_init(int fd, unsigned long low_offset,
			   void *low_virtual, unsigned long size,
			   volatile unsigned int *last_dispatch)
{
	drm_intel_bufmgr_fake *bufmgr_fake;

	bufmgr_fake = calloc(1, sizeof(*bufmgr_fake));

	if (pthread_mutex_init(&bufmgr_fake->lock, NULL) != 0) {
		free(bufmgr_fake);
		return NULL;
	}

	/* Initialize allocator */
	DRMINITLISTHEAD(&bufmgr_fake->fenced);
	DRMINITLISTHEAD(&bufmgr_fake->on_hardware);
	DRMINITLISTHEAD(&bufmgr_fake->lru);

	bufmgr_fake->low_offset = low_offset;
	bufmgr_fake->virtual = low_virtual;
	bufmgr_fake->size = size;
	bufmgr_fake->heap = mmInit(low_offset, size);

	/* Hook in methods */
	bufmgr_fake->bufmgr.bo_alloc = drm_intel_fake_bo_alloc;
	bufmgr_fake->bufmgr.bo_alloc_for_render = drm_intel_fake_bo_alloc;
	bufmgr_fake->bufmgr.bo_alloc_tiled = drm_intel_fake_bo_alloc_tiled;
	bufmgr_fake->bufmgr.bo_reference = drm_intel_fake_bo_reference;
	bufmgr_fake->bufmgr.bo_unreference = drm_intel_fake_bo_unreference;
	bufmgr_fake->bufmgr.bo_map = drm_intel_fake_bo_map;
	bufmgr_fake->bufmgr.bo_unmap = drm_intel_fake_bo_unmap;
	bufmgr_fake->bufmgr.bo_subdata = drm_intel_fake_bo_subdata;
	bufmgr_fake->bufmgr.bo_wait_rendering =
	    drm_intel_fake_bo_wait_rendering;
	bufmgr_fake->bufmgr.bo_emit_reloc = drm_intel_fake_emit_reloc;
	bufmgr_fake->bufmgr.destroy = drm_intel_fake_destroy;
	bufmgr_fake->bufmgr.bo_exec = drm_intel_fake_bo_exec;
	bufmgr_fake->bufmgr.check_aperture_space =
	    drm_intel_fake_check_aperture_space;
	bufmgr_fake->bufmgr.debug = 0;

	bufmgr_fake->fd = fd;
	bufmgr_fake->last_dispatch = (volatile int *)last_dispatch;

	return &bufmgr_fake->bufmgr;
}
@


1.12
log
@Merge libdrm 2.4.71
@
text
@d740 1
a740 1
 * It is assumed that the bathcbuffer which performed the rendering included
d1203 1
a1203 1
		 * mmory, hich we would be tracking when we free it.  Waiting
@


1.11
log
@Merge libdrm 2.4.67
@
text
@d315 1
a315 1
	 * numbers are all postive signed integers.
@


1.10
log
@Merge libdrm 2.4.65
@
text
@d45 1
@


1.9
log
@update to libdrm 2.4.62
@
text
@d1463 1
a1463 1
		int ret = bufmgr_fake->exec(bo, used, bufmgr_fake->exec_priv);
@


1.8
log
@update to libdrm 2.4.59
@
text
@d52 1
a52 1
#include "libdrm.h"
a54 5
/* Support gcc's __FUNCTION__ for people using other compilers */
#if !defined(__GNUC__) && !defined(__FUNCTION__)
# define __FUNCTION__ __func__ /* C99 */
#endif

d247 1
a247 1
drm_public void
d276 1
a276 1
		drmMsg("%s: drm_i915_irq_emit: %d\n", __FUNCTION__, ret);
d543 1
a543 1
	DBG("%s\n", __FUNCTION__);
d570 1
a570 1
	DBG("%s\n", __FUNCTION__);
d630 1
a630 1
	DBG("%s: %d\n", __FUNCTION__, ret);
d715 1
a715 1
	DBG("%s 0x%lx bytes failed\n", __FUNCTION__, bo->size);
d770 1
a770 1
drm_public void
d866 1
a866 1
drm_public drm_intel_bo *
d961 1
a961 1
drm_public void
d1025 1
a1025 1
			drmMsg("%s: already mapped\n", __FUNCTION__);
d1030 1
a1030 1
				DBG("%s: alloc failed\n", __FUNCTION__);
d1415 1
a1415 1
drm_public void
d1538 1
a1538 1
drm_public void
d1573 1
a1573 1
drm_public void
d1583 1
a1583 1
drm_public drm_intel_bufmgr *
@


1.7
log
@update to libdrm 2.4.54
ok matthieu@@
@
text
@d52 1
d252 1
a252 1
void
d775 1
a775 1
void
d871 1
a871 1
drm_intel_bo *
d966 1
a966 1
void
d1420 1
a1420 1
void
d1543 2
a1544 1
void drm_intel_bufmgr_fake_evict_all(drm_intel_bufmgr *bufmgr)
d1578 4
a1581 3
void drm_intel_bufmgr_fake_set_last_dispatch(drm_intel_bufmgr *bufmgr,
					     volatile unsigned int
					     *last_dispatch)
d1588 4
a1591 6
drm_intel_bufmgr *drm_intel_bufmgr_fake_init(int fd,
					     unsigned long low_offset,
					     void *low_virtual,
					     unsigned long size,
					     volatile unsigned int
					     *last_dispatch)
@


1.6
log
@Update to libdrm 2.4.31 and add the non-yet-linked libkms, prodded by jsg@@

Tested by jsg@@, ajacoutot@@, shadchin@@ and matthieu@@, ok matthieu@@
@
text
@d508 1
a508 1
	DBG("alloc_backing - buf %d %p %d\n", bo_fake->id,
d719 1
a719 1
	DBG("%s 0x%x bytes failed\n", __FUNCTION__, bo->size);
d838 1
a838 1
	DBG("drm_bo_alloc: (buf %d: %s, %d kb)\n", bo_fake->id, bo_fake->name,
d897 1
a897 1
	DBG("drm_bo_alloc_static: (buf %d: %s, %d kb)\n", bo_fake->id,
d1025 1
a1025 1
		DBG("drm_bo_map: (buf %d: %s, %d kb)\n", bo_fake->id,
d1103 1
a1103 1
	DBG("drm_bo_unmap: (buf %d: %s, %d kb)\n", bo_fake->id, bo_fake->name,
d1170 1
a1170 1
	DBG("drm_bo_validate: (buf %d: %s, %d kb)\n", bo_fake->id,
d1200 1
a1200 1
		DBG("Upload dirty buf %d:%s, sz %d offset 0x%x\n", bo_fake->id,
d1525 1
a1525 1
		DBG("check_space: overflowed bufmgr size, %dkb vs %dkb\n",
d1530 1
a1530 1
	DBG("drm_check_space: sz %dkb vs bufgr %dkb\n", sz / 1024,
@


1.5
log
@Update libdrm_intel to that contained in libdrm 2.4.24. Needed for
updates to the ddx and to stop mesa 7.9.2 crashing very fast.

ok matthieu@@
@
text
@a1311 36
static int
drm_intel_fake_pin(drm_intel_bo *bo, uint32_t alignment)
{
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;

   assert(bo_fake->is_static == 0);

   bo_fake->alignment = alignment;
   if (drm_intel_fake_bo_validate(bo) == -1)
      return ENOMEM;

   bo_fake->is_static = 1;
   bo->virtual = bo_fake->block->virtual;
   /* we should be on the on_hardware list, take us off for now */
   DRMLISTDEL(bo_fake->block);

   return 0;
}

static int
drm_intel_fake_unpin(drm_intel_bo *bo)
{
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;

   assert(bo_fake->is_static);

   bo_fake->is_static = 0;
   bo->virtual = NULL;
   DRMLISTDEL(bo_fake->block);
   DRMLISTADDTAIL(bo_fake->block, &bufmgr_fake->on_hardware);

   return 0;
}

a1622 3
	bufmgr_fake->bufmgr.bo_pin = drm_intel_fake_pin;
	bufmgr_fake->bufmgr.bo_unpin = drm_intel_fake_unpin;

@


1.4
log
@update libdrm to 2.4.15.

This diff seems larger than it is since upstream reindented some
sources.  This updates libdrm_intel to -current (which only affect the
GEM code) and libdrm to 2.4.15.  bumping both minor versions.

Discussed with matthieu@@
@
text
@d302 2
a305 2
	iw.irq_seq = seq;

d1124 17
d1655 1
@


1.3
log
@Add support to the fake (non GEM) bufmgr for pinning and unpinning objects.

Makes overlay XV work with intel 2.7.1. ok matthieu@@, tested by naddy@@,
sturm@@, matthieu@@ and myself. This is most important for the x40 and
other 8xx chipset which lack textured video.
@
text
@d54 4
a57 1
#define ALIGN(value, alignment)  ((value + alignment - 1) & ~(alignment - 1))
d60 2
a61 2
   if (bufmgr_fake->bufmgr.debug)			\
      drmMsg(__VA_ARGS__);				\
d78 15
a92 14
struct fake_buffer_reloc
{
   /** Buffer object that the relocation points at. */
   drm_intel_bo *target_buf;
   /** Offset of the relocation entry within reloc_buf. */
   uint32_t offset;
   /** Cached value of the offset when we last performed this relocation. */
   uint32_t last_target_offset;
   /** Value added to target_buf's offset to get the relocation entry. */
   uint32_t delta;
   /** Cache domains the target buffer is read into. */
   uint32_t read_domains;
   /** Cache domain the target buffer will have dirty cachelines in. */
   uint32_t write_domain;
d96 2
a97 2
   struct block *next, *prev;
   struct mem_block *mem;	/* BM_MEM_AGP */
d99 10
a108 10
   /**
    * Marks that the block is currently in the aperture and has yet to be
    * fenced.
    */
   unsigned on_hardware:1;
   /**
    * Marks that the block is currently fenced (being used by rendering) and
    * can't be freed until @@fence is passed.
    */
   unsigned fenced:1;
d110 2
a111 2
   /** Fence cookie for the block. */
   unsigned fence; /* Split to read_fence, write_fence */
d113 2
a114 2
   drm_intel_bo *bo;
   void *virtual;
d118 1
a118 1
   drm_intel_bufmgr bufmgr;
d120 1
a120 1
   pthread_mutex_t lock;
d122 59
a180 55
   unsigned long low_offset;
   unsigned long size;
   void *virtual;

   struct mem_block *heap;

   unsigned buf_nr;		/* for generating ids */

   /**
    * List of blocks which are currently in the GART but haven't been
    * fenced yet.
    */
   struct block on_hardware;
   /**
    * List of blocks which are in the GART and have an active fence on them.
    */
   struct block fenced;
   /**
    * List of blocks which have an expired fence and are ready to be evicted.
    */
   struct block lru;

   unsigned int last_fence;

   unsigned fail:1;
   unsigned need_fence:1;
   int thrashing;

   /**
    * Driver callback to emit a fence, returning the cookie.
    *
    * This allows the driver to hook in a replacement for the DRM usage in
    * bufmgr_fake.
    *
    * Currently, this also requires that a write flush be emitted before
    * emitting the fence, but this should change.
    */
   unsigned int (*fence_emit)(void *private);
   /** Driver callback to wait for a fence cookie to have passed. */
   void (*fence_wait)(unsigned int fence, void *private);
   void *fence_priv;

   /**
    * Driver callback to execute a buffer.
    *
    * This allows the driver to hook in a replacement for the DRM usage in
    * bufmgr_fake.
    */
   int (*exec)(drm_intel_bo *bo, unsigned int used, void *priv);
   void *exec_priv;

   /** Driver-supplied argument to driver callbacks */
   void *driver_priv;
   /* Pointer to kernel-updated sarea data for the last completed user irq */
   volatile int *last_dispatch;
d182 1
a182 1
   int fd;
d184 1
a184 1
   int debug;
d186 1
a186 1
   int performed_rendering;
d190 1
a190 1
   drm_intel_bo bo;
d192 2
a193 2
   unsigned id;			/* debug only */
   const char *name;
d195 34
a228 32
   unsigned dirty:1;
   /** has the card written to this buffer - we make need to copy it back */
   unsigned card_dirty:1;
   unsigned int refcount;
   /* Flags may consist of any of the DRM_BO flags, plus
    * DRM_BO_NO_BACKING_STORE and BM_NO_FENCE_SUBDATA, which are the first two
    * driver private flags.
    */
   uint64_t flags;
   /** Cache domains the target buffer is read into. */
   uint32_t read_domains;
   /** Cache domain the target buffer will have dirty cachelines in. */
   uint32_t write_domain;

   unsigned int alignment;
   int is_static, validated;
   unsigned int map_count;

   /** relocation list */
   struct fake_buffer_reloc *relocs;
   int nr_relocs;
   /**
    * Total size of the target_bos of this buffer.
    *
    * Used for estimation in check_aperture.
    */
   unsigned int child_size;

   struct block *block;
   void *backing_store;
   void (*invalidate_cb)(drm_intel_bo *bo, void *ptr);
   void *invalidate_ptr;
d236 2
a237 1
static int FENCE_LTE( unsigned a, unsigned b )
d239 2
a240 2
   if (a == b)
      return 1;
d242 2
a243 2
   if (a < b && b - a < (1<<24))
      return 1;
d245 2
a246 2
   if (a > b && MAXFENCE - a + b < (1<<24))
      return 1;
d248 1
a248 1
   return 0;
d251 12
a262 11
void drm_intel_bufmgr_fake_set_fence_callback(drm_intel_bufmgr *bufmgr,
					      unsigned int (*emit)(void *priv),
					      void (*wait)(unsigned int fence,
							   void *priv),
					      void *priv)
{
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bufmgr;

   bufmgr_fake->fence_emit = emit;
   bufmgr_fake->fence_wait = wait;
   bufmgr_fake->fence_priv = priv;
d268 2
a269 2
   struct drm_i915_irq_emit ie;
   int ret, seq = 1;
d271 12
a282 12
   if (bufmgr_fake->fence_emit != NULL) {
      seq = bufmgr_fake->fence_emit(bufmgr_fake->fence_priv);
      return seq;
   }

   ie.irq_seq = &seq;
   ret = drmCommandWriteRead(bufmgr_fake->fd, DRM_I915_IRQ_EMIT,
			     &ie, sizeof(ie));
   if (ret) {
      drmMsg("%s: drm_i915_irq_emit: %d\n", __FUNCTION__, ret);
      abort();
   }
d284 2
a285 2
   DBG("emit 0x%08x\n", seq);
   return seq;
d291 121
a411 118
   struct drm_i915_irq_wait iw;
   int hw_seq, busy_count = 0;
   int ret;
   int kernel_lied;

   if (bufmgr_fake->fence_wait != NULL) {
      bufmgr_fake->fence_wait(seq, bufmgr_fake->fence_priv);
      clear_fenced(bufmgr_fake, seq);
      return;
   }

   DBG("wait 0x%08x\n", iw.irq_seq);

   iw.irq_seq = seq;

   /* The kernel IRQ_WAIT implementation is all sorts of broken.
    * 1) It returns 1 to 0x7fffffff instead of using the full 32-bit unsigned
    *    range.
    * 2) It returns 0 if hw_seq >= seq, not seq - hw_seq < 0 on the 32-bit
    *    signed range.
    * 3) It waits if seq < hw_seq, not seq - hw_seq > 0 on the 32-bit
    *    signed range.
    * 4) It returns -EBUSY in 3 seconds even if the hardware is still
    *    successfully chewing through buffers.
    *
    * Assume that in userland we treat sequence numbers as ints, which makes
    * some of the comparisons convenient, since the sequence numbers are
    * all postive signed integers.
    *
    * From this we get several cases we need to handle.  Here's a timeline.
    * 0x2   0x7                                         0x7ffffff8   0x7ffffffd
    *   |    |                                                   |    |
    * -------------------------------------------------------------------
    *
    * A) Normal wait for hw to catch up
    * hw_seq seq
    *   |    |
    * -------------------------------------------------------------------
    * seq - hw_seq = 5.  If we call IRQ_WAIT, it will wait for hw to catch up.
    *
    * B) Normal wait for a sequence number that's already passed.
    * seq    hw_seq
    *   |    |
    * -------------------------------------------------------------------
    * seq - hw_seq = -5.  If we call IRQ_WAIT, it returns 0 quickly.
    *
    * C) Hardware has already wrapped around ahead of us
    * hw_seq                                                         seq
    *   |                                                             |
    * -------------------------------------------------------------------
    * seq - hw_seq = 0x80000000 - 5.  If we called IRQ_WAIT, it would wait
    * for hw_seq >= seq, which may never occur.  Thus, we want to catch this
    * in userland and return 0.
    *
    * D) We've wrapped around ahead of the hardware.
    * seq                                                           hw_seq
    *   |                                                             |
    * -------------------------------------------------------------------
    * seq - hw_seq = -(0x80000000 - 5).  If we called IRQ_WAIT, it would return
    * 0 quickly because hw_seq >= seq, even though the hardware isn't caught up.
    * Thus, we need to catch this early return in userland and bother the
    * kernel until the hardware really does catch up.
    *
    * E) Hardware might wrap after we test in userland.
    *                                                         hw_seq  seq
    *                                                            |    |
    * -------------------------------------------------------------------
    * seq - hw_seq = 5.  If we call IRQ_WAIT, it will likely see seq >= hw_seq
    * and wait.  However, suppose hw_seq wraps before we make it into the
    * kernel.  The kernel sees hw_seq >= seq and waits for 3 seconds then
    * returns -EBUSY.  This is case C).  We should catch this and then return
    * successfully.
    *
    * F) Hardware might take a long time on a buffer.
    * hw_seq seq
    *   |    |
    * -------------------------------------------------------------------
    * seq - hw_seq = 5.  If we call IRQ_WAIT, if sequence 2 through 5 take too
    * long, it will return -EBUSY.  Batchbuffers in the gltestperf demo were
    * seen to take up to 7 seconds.  We should catch early -EBUSY return
    * and keep trying.
    */

   do {
      /* Keep a copy of last_dispatch so that if the wait -EBUSYs because the
       * hardware didn't catch up in 3 seconds, we can see if it at least made
       * progress and retry.
       */
      hw_seq = *bufmgr_fake->last_dispatch;

      /* Catch case C */
      if (seq - hw_seq > 0x40000000)
	 return;

      ret = drmCommandWrite(bufmgr_fake->fd, DRM_I915_IRQ_WAIT,
			    &iw, sizeof(iw));
      /* Catch case D */
      kernel_lied = (ret == 0) && (seq - *bufmgr_fake->last_dispatch <
				   -0x40000000);

      /* Catch case E */
      if (ret == -EBUSY && (seq - *bufmgr_fake->last_dispatch > 0x40000000))
	 ret = 0;

      /* Catch case F: Allow up to 15 seconds chewing on one buffer. */
      if ((ret == -EBUSY) && (hw_seq != *bufmgr_fake->last_dispatch))
	 busy_count = 0;
      else
	 busy_count++;
   } while (kernel_lied || ret == -EAGAIN || ret == -EINTR ||
	    (ret == -EBUSY && busy_count < 5));

   if (ret != 0) {
      drmMsg("%s:%d: Error waiting for fence: %s.\n", __FILE__, __LINE__,
	     strerror(-ret));
      abort();
   }
   clear_fenced(bufmgr_fake, seq);
d417 3
a419 3
   /* Slight problem with wrap-around:
    */
   return fence == 0 || FENCE_LTE(fence, bufmgr_fake->last_fence);
d428 6
a433 5
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;
   drm_intel_bufmgr_fake *bufmgr_fake= (drm_intel_bufmgr_fake *)bo->bufmgr;
   struct block *block = (struct block *)calloc(sizeof *block, 1);
   unsigned int align_log2 = ffs(bo_fake->alignment) - 1;
   unsigned int sz;
d435 2
a436 2
   if (!block)
      return 1;
d438 1
a438 1
   sz = (bo->size + bo_fake->alignment - 1) & ~(bo_fake->alignment - 1);
d440 5
a444 5
   block->mem = mmAllocMem(bufmgr_fake->heap, sz, align_log2, 0);
   if (!block->mem) {
      free(block);
      return 0;
   }
d446 1
a446 1
   DRMINITLISTHEAD(block);
d448 2
a449 3
   /* Insert at head or at tail???   
    */
   DRMLISTADDTAIL(block, &bufmgr_fake->lru);
d451 3
a453 3
   block->virtual = (uint8_t *)bufmgr_fake->virtual +
      block->mem->ofs - bufmgr_fake->low_offset;
   block->bo = bo;
d455 1
a455 1
   bo_fake->block = block;
d457 1
a457 1
   return 1;
d462 3
a464 2
static void free_block(drm_intel_bufmgr_fake *bufmgr_fake, struct block *block,
		       int skip_dirty_copy)
d466 29
a494 30
   drm_intel_bo_fake *bo_fake;
   DBG("free block %p %08x %d %d\n", block, block->mem->ofs, block->on_hardware, block->fenced);

   if (!block)
      return;

   bo_fake = (drm_intel_bo_fake *)block->bo;

   if (bo_fake->flags & (BM_PINNED | BM_NO_BACKING_STORE))
      skip_dirty_copy = 1;

   if (!skip_dirty_copy && (bo_fake->card_dirty == 1)) {
     memcpy(bo_fake->backing_store, block->virtual, block->bo->size);
     bo_fake->card_dirty = 0;
     bo_fake->dirty = 1;
   }

   if (block->on_hardware) {
      block->bo = NULL;
   }
   else if (block->fenced) {
      block->bo = NULL;
   }
   else {
      DBG("    - free immediately\n");
      DRMLISTDEL(block);

      mmFreeMem(block->mem);
      free(block);
   }
d500 11
a510 9
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;
   assert(!bo_fake->backing_store);
   assert(!(bo_fake->flags & (BM_PINNED|BM_NO_BACKING_STORE)));

   bo_fake->backing_store = malloc(bo->size);

   DBG("alloc_backing - buf %d %p %d\n", bo_fake->id, bo_fake->backing_store, bo->size);
   assert(bo_fake->backing_store);
d516 1
a516 1
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;
d518 5
a522 5
   if (bo_fake->backing_store) {
      assert(!(bo_fake->flags & (BM_PINNED|BM_NO_BACKING_STORE)));
      free(bo_fake->backing_store);
      bo_fake->backing_store = NULL;
   }
d528 7
a534 5
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;

   if (bo_fake->flags & BM_NO_BACKING_STORE && bo_fake->invalidate_cb != NULL)
      bo_fake->invalidate_cb(bo, bo_fake->invalidate_ptr);
d536 1
a536 1
   assert(!(bo_fake->flags & BM_PINNED));
d538 2
a539 2
   DBG("set_dirty - buf %d\n", bo_fake->id);
   bo_fake->dirty = 1;
d545 1
a545 1
   struct block *block, *tmp;
d547 1
a547 1
   DBG("%s\n", __FUNCTION__);
d549 2
a550 2
   DRMLISTFOREACHSAFE(block, tmp, &bufmgr_fake->lru) {
      drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)block->bo;
d552 2
a553 2
      if (bo_fake != NULL && (bo_fake->flags & BM_NO_FENCE_SUBDATA))
	 continue;
d555 3
a557 2
      if (block->fence && max_fence && !FENCE_LTE(block->fence, max_fence))
	 return 0;
d559 2
a560 2
      set_dirty(&bo_fake->bo);
      bo_fake->block = NULL;
d562 3
a564 3
      free_block(bufmgr_fake, block, 0);
      return 1;
   }
d566 1
a566 1
   return 0;
d572 1
a572 1
   struct block *block, *tmp;
d574 1
a574 1
   DBG("%s\n", __FUNCTION__);
d576 2
a577 2
   DRMLISTFOREACHSAFEREVERSE(block, tmp, &bufmgr_fake->lru) {
      drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)block->bo;
d579 2
a580 2
      if (bo_fake && (bo_fake->flags & BM_NO_FENCE_SUBDATA))
	 continue;
d582 2
a583 2
      set_dirty(&bo_fake->bo);
      bo_fake->block = NULL;
d585 3
a587 3
      free_block(bufmgr_fake, block, 0);
      return 1;
   }
d589 1
a589 1
   return 0;
d595 2
a596 2
static int clear_fenced(drm_intel_bufmgr_fake *bufmgr_fake,
			unsigned int fence_cookie)
d598 2
a599 2
   struct block *block, *tmp;
   int ret = 0;
d601 32
a632 55
   bufmgr_fake->last_fence = fence_cookie;
   DRMLISTFOREACHSAFE(block, tmp, &bufmgr_fake->fenced) {
      assert(block->fenced);

      if (_fence_test(bufmgr_fake, block->fence)) {

	 block->fenced = 0;

	 if (!block->bo) {
	    DBG("delayed free: offset %x sz %x\n",
		block->mem->ofs, block->mem->size);
	    DRMLISTDEL(block);
	    mmFreeMem(block->mem);
	    free(block);
	 }
	 else {
	    DBG("return to lru: offset %x sz %x\n",
		block->mem->ofs, block->mem->size);
	    DRMLISTDEL(block);
	    DRMLISTADDTAIL(block, &bufmgr_fake->lru);
	 }

	 ret = 1;
      }
      else {
	 /* Blocks are ordered by fence, so if one fails, all from
	  * here will fail also:
	  */
	DBG("fence not passed: offset %x sz %x %d %d \n",
	    block->mem->ofs, block->mem->size, block->fence, bufmgr_fake->last_fence);
	 break;
      }
   }

   DBG("%s: %d\n", __FUNCTION__, ret);
   return ret;
}

static void fence_blocks(drm_intel_bufmgr_fake *bufmgr_fake, unsigned fence)
{
   struct block *block, *tmp;

   DRMLISTFOREACHSAFE(block, tmp, &bufmgr_fake->on_hardware) {
      DBG("Fence block %p (sz 0x%x ofs %x buf %p) with fence %d\n", block,
	  block->mem->size, block->mem->ofs, block->bo, fence);
      block->fence = fence;

      block->on_hardware = 0;
      block->fenced = 1;

      /* Move to tail of pending list here
       */
      DRMLISTDEL(block);
      DRMLISTADDTAIL(block, &bufmgr_fake->fenced);
   }
d634 2
a635 1
   assert(DRMLISTEMPTY(&bufmgr_fake->on_hardware));
d638 2
a639 1
static int evict_and_alloc_block(drm_intel_bo *bo)
d641 15
a655 2
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;
d657 2
a658 1
   assert(bo_fake->block == NULL);
d660 58
a717 49
   /* Search for already free memory:
    */
   if (alloc_block(bo))
      return 1;

   /* If we're not thrashing, allow lru eviction to dig deeper into
    * recently used textures.  We'll probably be thrashing soon:
    */
   if (!bufmgr_fake->thrashing) {
      while (evict_lru(bufmgr_fake, 0))
	 if (alloc_block(bo))
	    return 1;
   }

   /* Keep thrashing counter alive?
    */
   if (bufmgr_fake->thrashing)
      bufmgr_fake->thrashing = 20;

   /* Wait on any already pending fences - here we are waiting for any
    * freed memory that has been submitted to hardware and fenced to
    * become available:
    */
   while (!DRMLISTEMPTY(&bufmgr_fake->fenced)) {
      uint32_t fence = bufmgr_fake->fenced.next->fence;
      _fence_wait_internal(bufmgr_fake, fence);

      if (alloc_block(bo))
	 return 1;
   }

   if (!DRMLISTEMPTY(&bufmgr_fake->on_hardware)) {
      while (!DRMLISTEMPTY(&bufmgr_fake->fenced)) {
	 uint32_t fence = bufmgr_fake->fenced.next->fence;
	 _fence_wait_internal(bufmgr_fake, fence);
      }

      if (!bufmgr_fake->thrashing) {
	 DBG("thrashing\n");
      }
      bufmgr_fake->thrashing = 20;

      if (alloc_block(bo))
	 return 1;
   }

   while (evict_mru(bufmgr_fake))
      if (alloc_block(bo))
	 return 1;
d719 1
a719 1
   DBG("%s 0x%x bytes failed\n", __FUNCTION__, bo->size);
d721 1
a721 1
   return 0;
d734 1
a734 1
   unsigned int cookie;
d736 2
a737 2
   cookie = _fence_emit_internal(bufmgr_fake);
   _fence_wait_internal(bufmgr_fake, cookie);
d749 3
a751 2
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;
d753 2
a754 2
   if (bo_fake->block == NULL || !bo_fake->block->fenced)
      return;
d756 1
a756 1
   _fence_wait_internal(bufmgr_fake, bo_fake->block->fence);
d762 2
a763 1
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
d765 3
a767 3
   pthread_mutex_lock(&bufmgr_fake->lock);
   drm_intel_fake_bo_wait_rendering_locked(bo);
   pthread_mutex_unlock(&bufmgr_fake->lock);
d777 24
a800 2
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bufmgr;
   struct block *block, *tmp;
d802 2
a803 1
   pthread_mutex_lock(&bufmgr_fake->lock);
d805 32
a836 2
   bufmgr_fake->need_fence = 1;
   bufmgr_fake->fail = 0;
d838 2
a839 16
   /* Wait for hardware idle.  We don't know where acceleration has been
    * happening, so we'll need to wait anyway before letting anything get
    * put on the card again.
    */
   drm_intel_bufmgr_fake_wait_idle(bufmgr_fake);

   /* Check that we hadn't released the lock without having fenced the last
    * set of buffers.
    */
   assert(DRMLISTEMPTY(&bufmgr_fake->fenced));
   assert(DRMLISTEMPTY(&bufmgr_fake->on_hardware));

   DRMLISTFOREACHSAFE(block, tmp, &bufmgr_fake->lru) {
      assert(_fence_test(bufmgr_fake, block->fence));
      set_dirty(block->bo);
   }
d841 1
a841 1
   pthread_mutex_unlock(&bufmgr_fake->lock);
d845 6
a850 2
drm_intel_fake_bo_alloc(drm_intel_bufmgr *bufmgr, const char *name,
			unsigned long size, unsigned int alignment)
d852 4
a855 2
   drm_intel_bufmgr_fake *bufmgr_fake;
   drm_intel_bo_fake *bo_fake;
d857 3
a859 1
   bufmgr_fake = (drm_intel_bufmgr_fake *)bufmgr;
d861 2
a862 21
   assert(size != 0);

   bo_fake = calloc(1, sizeof(*bo_fake));
   if (!bo_fake)
      return NULL;

   bo_fake->bo.size = size;
   bo_fake->bo.offset = -1;
   bo_fake->bo.virtual = NULL;
   bo_fake->bo.bufmgr = bufmgr;
   bo_fake->refcount = 1;

   /* Alignment must be a power of two */
   assert((alignment & (alignment - 1)) == 0);
   if (alignment == 0)
      alignment = 1;
   bo_fake->alignment = alignment;
   bo_fake->id = ++bufmgr_fake->buf_nr;
   bo_fake->name = name;
   bo_fake->flags = 0;
   bo_fake->is_static = 0;
d864 1
a864 2
   DBG("drm_bo_alloc: (buf %d: %s, %d kb)\n", bo_fake->id, bo_fake->name,
       bo_fake->bo.size / 1024);
d866 2
a867 1
   return &bo_fake->bo;
d871 25
a895 24
drm_intel_bo_fake_alloc_static(drm_intel_bufmgr *bufmgr, const char *name,
			       unsigned long offset, unsigned long size,
			       void *virtual)
{
   drm_intel_bufmgr_fake *bufmgr_fake;
   drm_intel_bo_fake *bo_fake;

   bufmgr_fake = (drm_intel_bufmgr_fake *)bufmgr;

   assert(size != 0);

   bo_fake = calloc(1, sizeof(*bo_fake));
   if (!bo_fake)
      return NULL;

   bo_fake->bo.size = size;
   bo_fake->bo.offset = offset;
   bo_fake->bo.virtual = virtual;
   bo_fake->bo.bufmgr = bufmgr;
   bo_fake->refcount = 1;
   bo_fake->id = ++bufmgr_fake->buf_nr;
   bo_fake->name = name;
   bo_fake->flags = BM_PINNED;
   bo_fake->is_static = 1;
d897 2
a898 2
   DBG("drm_bo_alloc_static: (buf %d: %s, %d kb)\n", bo_fake->id, bo_fake->name,
       bo_fake->bo.size / 1024);
d900 1
a900 1
   return &bo_fake->bo;
d906 7
a912 6
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;

   pthread_mutex_lock(&bufmgr_fake->lock);
   bo_fake->refcount++;
   pthread_mutex_unlock(&bufmgr_fake->lock);
d918 1
a918 1
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;
d920 1
a920 1
   bo_fake->refcount++;
d926 22
a947 19
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;
   int i;

   if (--bo_fake->refcount == 0) {
      assert(bo_fake->map_count == 0);
      /* No remaining references, so free it */
      if (bo_fake->block)
	 free_block(bufmgr_fake, bo_fake->block, 1);
      free_backing_store(bo);

      for (i = 0; i < bo_fake->nr_relocs; i++)
	 drm_intel_fake_bo_unreference_locked(bo_fake->relocs[i].target_buf);

      DBG("drm_bo_unreference: free buf %d %s\n", bo_fake->id, bo_fake->name);

      free(bo_fake->relocs);
      free(bo);
   }
d953 2
a954 1
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
d956 3
a958 3
   pthread_mutex_lock(&bufmgr_fake->lock);
   drm_intel_fake_bo_unreference_locked(bo);
   pthread_mutex_unlock(&bufmgr_fake->lock);
d965 28
a992 12
void drm_intel_bo_fake_disable_backing_store(drm_intel_bo *bo,
					     void (*invalidate_cb)(drm_intel_bo *bo,
								   void *ptr),
					     void *ptr)
{
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;

   pthread_mutex_lock(&bufmgr_fake->lock);

   if (bo_fake->backing_store)
      free_backing_store(bo);
d994 1
a994 15
   bo_fake->flags |= BM_NO_BACKING_STORE;

   DBG("disable_backing_store set buf %d dirty\n", bo_fake->id);
   bo_fake->dirty = 1;
   bo_fake->invalidate_cb = invalidate_cb;
   bo_fake->invalidate_ptr = ptr;

   /* Note that it is invalid right from the start.  Also note
    * invalidate_cb is called with the bufmgr locked, so cannot
    * itself make bufmgr calls.
    */
   if (invalidate_cb != NULL)
      invalidate_cb(bo, ptr);

   pthread_mutex_unlock(&bufmgr_fake->lock);
d1002 1
a1002 1
drm_intel_fake_bo_map_locked(drm_intel_bo *bo, int write_enable)
d1004 66
a1069 64
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;

   /* Static buffers are always mapped. */
   if (bo_fake->is_static) {
      if (bo_fake->card_dirty) {
         drm_intel_bufmgr_fake_wait_idle(bufmgr_fake);
         bo_fake->card_dirty = 0;
      }
      return 0;
   }

   /* Allow recursive mapping.  Mesa may recursively map buffers with
    * nested display loops, and it is used internally in bufmgr_fake
    * for relocation.
    */
   if (bo_fake->map_count++ != 0)
      return 0;

   {
      DBG("drm_bo_map: (buf %d: %s, %d kb)\n", bo_fake->id, bo_fake->name,
	  bo_fake->bo.size / 1024);

      if (bo->virtual != NULL) {
	 drmMsg("%s: already mapped\n", __FUNCTION__);
	 abort();
      }
      else if (bo_fake->flags & (BM_NO_BACKING_STORE|BM_PINNED)) {

	 if (!bo_fake->block && !evict_and_alloc_block(bo)) {
	    DBG("%s: alloc failed\n", __FUNCTION__);
	    bufmgr_fake->fail = 1;
	    return 1;
	 }
	 else {
	    assert(bo_fake->block);
	    bo_fake->dirty = 0;

	    if (!(bo_fake->flags & BM_NO_FENCE_SUBDATA) &&
		bo_fake->block->fenced) {
	       drm_intel_fake_bo_wait_rendering_locked(bo);
	    }

	    bo->virtual = bo_fake->block->virtual;
	 }
      }
      else {
	 if (write_enable)
	    set_dirty(bo);

	 if (bo_fake->backing_store == 0)
	    alloc_backing_store(bo);

         if ((bo_fake->card_dirty == 1) && bo_fake->block) {
            if (bo_fake->block->fenced)
               drm_intel_fake_bo_wait_rendering_locked(bo);

            memcpy(bo_fake->backing_store, bo_fake->block->virtual, bo_fake->block->bo->size);
            bo_fake->card_dirty = 0;
         }

	 bo->virtual = bo_fake->backing_store;
      }
   }
d1071 1
a1071 1
   return 0;
d1075 1
a1075 1
drm_intel_fake_bo_map(drm_intel_bo *bo, int write_enable)
d1077 3
a1079 2
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
   int ret;
d1081 3
a1083 3
   pthread_mutex_lock(&bufmgr_fake->lock);
   ret = drm_intel_fake_bo_map_locked(bo, write_enable);
   pthread_mutex_unlock(&bufmgr_fake->lock);
d1085 1
a1085 1
   return ret;
d1089 1
a1089 1
drm_intel_fake_bo_unmap_locked(drm_intel_bo *bo)
d1091 7
a1097 2
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;
d1099 3
a1101 7
   /* Static buffers are always mapped. */
   if (bo_fake->is_static)
      return 0;

   assert(bo_fake->map_count != 0);
   if (--bo_fake->map_count != 0)
      return 0;
d1103 2
a1104 2
   DBG("drm_bo_unmap: (buf %d: %s, %d kb)\n", bo_fake->id, bo_fake->name,
       bo_fake->bo.size / 1024);
d1106 1
a1106 1
   bo->virtual = NULL;
d1108 1
a1108 1
   return 0;
d1111 1
a1111 2
static int
drm_intel_fake_bo_unmap(drm_intel_bo *bo)
d1113 3
a1115 2
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
   int ret;
d1117 3
a1119 3
   pthread_mutex_lock(&bufmgr_fake->lock);
   ret = drm_intel_fake_bo_unmap_locked(bo);
   pthread_mutex_unlock(&bufmgr_fake->lock);
d1121 1
a1121 1
   return ret;
d1125 1
a1125 1
drm_intel_fake_kick_all_locked(drm_intel_bufmgr_fake *bufmgr_fake)
d1127 1
a1127 1
   struct block *block, *tmp;
d1129 13
a1141 13
   bufmgr_fake->performed_rendering = 0;
   /* okay for ever BO that is on the HW kick it off.
      seriously not afraid of the POLICE right now */
   DRMLISTFOREACHSAFE(block, tmp, &bufmgr_fake->on_hardware) {
      drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)block->bo;

      block->on_hardware = 0;
      free_block(bufmgr_fake, block, 0);
      bo_fake->block = NULL;
      bo_fake->validated = 0;
      if (!(bo_fake->flags & BM_NO_BACKING_STORE))
         bo_fake->dirty = 1;
   }
d1146 1
a1146 1
drm_intel_fake_bo_validate(drm_intel_bo *bo)
d1148 2
a1149 2
   drm_intel_bufmgr_fake *bufmgr_fake;
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;
d1151 1
a1151 1
   bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
d1153 57
a1209 2
   DBG("drm_bo_validate: (buf %d: %s, %d kb)\n", bo_fake->id, bo_fake->name,
       bo_fake->bo.size / 1024);
d1211 2
a1212 45
   /* Sanity check: Buffers should be unmapped before being validated.
    * This is not so much of a problem for bufmgr_fake, but TTM refuses,
    * and the problem is harder to debug there.
    */
   assert(bo_fake->map_count == 0);

   if (bo_fake->is_static) {
      /* Add it to the needs-fence list */
      bufmgr_fake->need_fence = 1;
      return 0;
   }

   /* Allocate the card memory */
   if (!bo_fake->block && !evict_and_alloc_block(bo)) {
      bufmgr_fake->fail = 1;
      DBG("Failed to validate buf %d:%s\n", bo_fake->id, bo_fake->name);
      return -1;
   }

   assert(bo_fake->block);
   assert(bo_fake->block->bo == &bo_fake->bo);

   bo->offset = bo_fake->block->mem->ofs;

   /* Upload the buffer contents if necessary */
   if (bo_fake->dirty) {
      DBG("Upload dirty buf %d:%s, sz %d offset 0x%x\n", bo_fake->id,
	  bo_fake->name, bo->size, bo_fake->block->mem->ofs);

      assert(!(bo_fake->flags &
	       (BM_NO_BACKING_STORE|BM_PINNED)));

      /* Actually, should be able to just wait for a fence on the memory,
       * which we would be tracking when we free it.  Waiting for idle is
       * a sufficiently large hammer for now.
       */
      drm_intel_bufmgr_fake_wait_idle(bufmgr_fake);

      /* we may never have mapped this BO so it might not have any backing
       * store if this happens it should be rare, but 0 the card memory
       * in any case */
      if (bo_fake->backing_store)
         memcpy(bo_fake->block->virtual, bo_fake->backing_store, bo->size);
      else
         memset(bo_fake->block->virtual, 0, bo->size);
d1214 1
a1214 12
      bo_fake->dirty = 0;
   }

   bo_fake->block->fenced = 0;
   bo_fake->block->on_hardware = 1;
   DRMLISTDEL(bo_fake->block);
   DRMLISTADDTAIL(bo_fake->block, &bufmgr_fake->on_hardware);

   bo_fake->validated = 1;
   bufmgr_fake->need_fence = 1;

   return 0;
d1220 2
a1221 2
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bufmgr;
   unsigned int cookie;
d1223 2
a1224 2
   cookie = _fence_emit_internal(bufmgr_fake);
   fence_blocks(bufmgr_fake, cookie);
d1226 1
a1226 1
   DBG("drm_fence_validated: 0x%08x cookie\n", cookie);
d1232 1
a1232 1
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bufmgr;
d1234 3
a1236 3
   pthread_mutex_destroy(&bufmgr_fake->lock);
   mmDestroy(bufmgr_fake->heap);
   free(bufmgr);
d1244 45
a1288 5
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
   struct fake_buffer_reloc *r;
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;
   drm_intel_bo_fake *target_fake = (drm_intel_bo_fake *)target_bo;
   int i;
d1290 1
a1290 1
   pthread_mutex_lock(&bufmgr_fake->lock);
d1292 2
a1293 2
   assert(bo);
   assert(target_bo);
d1295 5
a1299 3
   if (bo_fake->relocs == NULL) {
      bo_fake->relocs = malloc(sizeof(struct fake_buffer_reloc) * MAX_RELOCS);
   }
d1301 1
a1301 1
   r = &bo_fake->relocs[bo_fake->nr_relocs++];
d1303 3
a1305 1
   assert(bo_fake->nr_relocs <= MAX_RELOCS);
d1307 4
a1310 1
   drm_intel_fake_bo_reference_locked(target_bo);
d1312 2
a1313 10
   if (!target_fake->is_static) {
      bo_fake->child_size += ALIGN(target_bo->size, target_fake->alignment);
      bo_fake->child_size += target_fake->child_size;
   }
   r->target_buf = target_bo;
   r->offset = offset;
   r->last_target_offset = target_bo->offset;
   r->delta = target_offset;
   r->read_domains = read_domains;
   r->write_domain = write_domain;
d1315 5
a1319 4
   if (bufmgr_fake->debug) {
      /* Check that a conflicting relocation hasn't already been emitted. */
      for (i = 0; i < bo_fake->nr_relocs - 1; i++) {
	 struct fake_buffer_reloc *r2 = &bo_fake->relocs[i];
d1321 1
a1321 3
	 assert(r->offset != r2->offset);
      }
   }
d1323 4
a1326 1
   pthread_mutex_unlock(&bufmgr_fake->lock);
d1338 2
a1339 2
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;
   int i;
d1341 11
a1351 10
   for (i = 0; i < bo_fake->nr_relocs; i++) {
      struct fake_buffer_reloc *r = &bo_fake->relocs[i];
      drm_intel_bo_fake *target_fake = (drm_intel_bo_fake *)r->target_buf;

      /* Do the same for the tree of buffers we depend on */
      drm_intel_fake_calculate_domains(r->target_buf);

      target_fake->read_domains |= r->read_domains;
      target_fake->write_domain |= r->write_domain;
   }
a1353 1

d1357 49
a1405 5
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;
   int i, ret;

   assert(bo_fake->map_count == 0);
d1407 1
a1407 41
   for (i = 0; i < bo_fake->nr_relocs; i++) {
      struct fake_buffer_reloc *r = &bo_fake->relocs[i];
      drm_intel_bo_fake *target_fake = (drm_intel_bo_fake *)r->target_buf;
      uint32_t reloc_data;

      /* Validate the target buffer if that hasn't been done. */
      if (!target_fake->validated) {
         ret = drm_intel_fake_reloc_and_validate_buffer(r->target_buf);
         if (ret != 0) {
            if (bo->virtual != NULL)
                drm_intel_fake_bo_unmap_locked(bo);
            return ret;
         }
      }

      /* Calculate the value of the relocation entry. */
      if (r->target_buf->offset != r->last_target_offset) {
	 reloc_data = r->target_buf->offset + r->delta;

	 if (bo->virtual == NULL)
	    drm_intel_fake_bo_map_locked(bo, 1);

	 *(uint32_t *)((uint8_t *)bo->virtual + r->offset) = reloc_data;

	 r->last_target_offset = r->target_buf->offset;
      }
   }

   if (bo->virtual != NULL)
      drm_intel_fake_bo_unmap_locked(bo);

   if (bo_fake->write_domain != 0) {
      if (!(bo_fake->flags & (BM_NO_BACKING_STORE|BM_PINNED))) {
         if (bo_fake->backing_store == 0)
            alloc_backing_store(bo);
      }
      bo_fake->card_dirty = 1;
      bufmgr_fake->performed_rendering = 1;
   }

   return drm_intel_fake_bo_validate(bo);
d1413 23
a1435 20
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;
   int i;

   for (i = 0; i < bo_fake->nr_relocs; i++) {
      struct fake_buffer_reloc *r = &bo_fake->relocs[i];
      drm_intel_bo_fake *target_fake = (drm_intel_bo_fake *)r->target_buf;

      if (target_fake->validated)
	 drm_intel_bo_fake_post_submit(r->target_buf);

      DBG("%s@@0x%08x + 0x%08x -> %s@@0x%08x + 0x%08x\n",
	  bo_fake->name, (uint32_t)bo->offset, r->offset,
	  target_fake->name, (uint32_t)r->target_buf->offset, r->delta);
   }

   assert(bo_fake->map_count == 0);
   bo_fake->validated = 0;
   bo_fake->read_domains = 0;
   bo_fake->write_domain = 0;
d1438 5
a1442 5

void drm_intel_bufmgr_fake_set_exec_callback(drm_intel_bufmgr *bufmgr,
					     int (*exec)(drm_intel_bo *bo,
							 unsigned int used,
							 void *priv),
d1445 1
a1445 1
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bufmgr;
d1447 2
a1448 2
   bufmgr_fake->exec = exec;
   bufmgr_fake->exec_priv = priv;
d1453 1
a1453 2
		       drm_clip_rect_t *cliprects, int num_cliprects,
		       int DR4)
d1455 52
a1506 50
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
   drm_intel_bo_fake *batch_fake = (drm_intel_bo_fake *)bo;
   struct drm_i915_batchbuffer batch;
   int ret;
   int retry_count = 0;

   pthread_mutex_lock(&bufmgr_fake->lock);

   bufmgr_fake->performed_rendering = 0;

   drm_intel_fake_calculate_domains(bo);

   batch_fake->read_domains = I915_GEM_DOMAIN_COMMAND;

   /* we've ran out of RAM so blow the whole lot away and retry */
 restart:
   ret = drm_intel_fake_reloc_and_validate_buffer(bo);
   if (bufmgr_fake->fail == 1) {
      if (retry_count == 0) {
         retry_count++;
         drm_intel_fake_kick_all_locked(bufmgr_fake);
         bufmgr_fake->fail = 0;
         goto restart;
      } else /* dump out the memory here */
         mmDumpMemInfo(bufmgr_fake->heap);
   }

   assert(ret == 0);

   if (bufmgr_fake->exec != NULL) {
      int ret = bufmgr_fake->exec(bo, used, bufmgr_fake->exec_priv);
      if (ret != 0) {
	 pthread_mutex_unlock(&bufmgr_fake->lock);
	 return ret;
      }
   } else {
      batch.start = bo->offset;
      batch.used = used;
      batch.cliprects = cliprects;
      batch.num_cliprects = num_cliprects;
      batch.DR1 = 0;
      batch.DR4 = DR4;

      if (drmCommandWrite(bufmgr_fake->fd, DRM_I915_BATCHBUFFER, &batch,
			  sizeof(batch))) {
	 drmMsg("DRM_I915_BATCHBUFFER: %d\n", -errno);
	 pthread_mutex_unlock(&bufmgr_fake->lock);
	 return -errno;
      }
   }
d1508 1
a1508 1
   drm_intel_fake_fence_validated(bo->bufmgr);
d1510 1
a1510 1
   drm_intel_bo_fake_post_submit(bo);
d1512 1
a1512 1
   pthread_mutex_unlock(&bufmgr_fake->lock);
d1514 1
a1514 37
   return 0;
}

static int
drm_intel_fake_pin(drm_intel_bo *bo, uint32_t alignment)
{
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;

   assert(bo_fake->is_static == 0);

   bo_fake->alignment = alignment;
   if (drm_intel_fake_bo_validate(bo) == -1)
      return ENOMEM;

   bo_fake->is_static = 1;
   bo->virtual = bo_fake->block->virtual;
   /* we should be on the on_hardware list, take us off for now */
   DRMLISTDEL(bo_fake->block);

   return 0;
}

static int
drm_intel_fake_unpin(drm_intel_bo *bo)
{
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo->bufmgr;
   drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo;

   assert(bo_fake->is_static);

   bo_fake->is_static = 0;
   bo->virtual = NULL;
   DRMLISTDEL(bo_fake->block);
   DRMLISTADDTAIL(bo_fake->block, &bufmgr_fake->on_hardware);

   return 0;
d1525 1
a1525 1
drm_intel_fake_check_aperture_space(drm_intel_bo **bo_array, int count)
d1527 25
a1551 24
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bo_array[0]->bufmgr;
   unsigned int sz = 0;
   int i;

   for (i = 0; i < count; i++) {
      drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)bo_array[i];

      if (bo_fake == NULL)
	 continue;

      if (!bo_fake->is_static)
	 sz += ALIGN(bo_array[i]->size, bo_fake->alignment);
      sz += bo_fake->child_size;
   }

   if (sz > bufmgr_fake->size) {
      DBG("check_space: overflowed bufmgr size, %dkb vs %dkb\n",
	  sz / 1024, bufmgr_fake->size / 1024);
      return -1;
   }

   DBG("drm_check_space: sz %dkb vs bufgr %dkb\n", sz / 1024 ,
       bufmgr_fake->size / 1024);
   return 0;
d1561 1
a1561 2
void
drm_intel_bufmgr_fake_evict_all(drm_intel_bufmgr *bufmgr)
d1563 2
a1564 2
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bufmgr;
   struct block *block, *tmp;
d1566 1
a1566 1
   pthread_mutex_lock(&bufmgr_fake->lock);
d1568 23
a1590 21
   bufmgr_fake->need_fence = 1;
   bufmgr_fake->fail = 0;

   /* Wait for hardware idle.  We don't know where acceleration has been
    * happening, so we'll need to wait anyway before letting anything get
    * put on the card again.
    */
   drm_intel_bufmgr_fake_wait_idle(bufmgr_fake);

   /* Check that we hadn't released the lock without having fenced the last
    * set of buffers.
    */
   assert(DRMLISTEMPTY(&bufmgr_fake->fenced));
   assert(DRMLISTEMPTY(&bufmgr_fake->on_hardware));

   DRMLISTFOREACHSAFE(block, tmp, &bufmgr_fake->lru) {
      drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *)block->bo;
      /* Releases the memory, and memcpys dirty contents out if necessary. */
      free_block(bufmgr_fake, block, 0);
      bo_fake->block = NULL;
   }
d1592 1
a1592 1
   pthread_mutex_unlock(&bufmgr_fake->lock);
d1594 1
d1596 2
a1597 1
					 volatile unsigned int *last_dispatch)
d1599 1
a1599 1
   drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *)bufmgr;
d1601 1
a1601 1
   bufmgr_fake->last_dispatch = (volatile int *)last_dispatch;
d1604 45
a1648 40
drm_intel_bufmgr *
drm_intel_bufmgr_fake_init(int fd,
		       unsigned long low_offset, void *low_virtual,
		       unsigned long size,
		       volatile unsigned int *last_dispatch)
{
   drm_intel_bufmgr_fake *bufmgr_fake;

   bufmgr_fake = calloc(1, sizeof(*bufmgr_fake));

   if (pthread_mutex_init(&bufmgr_fake->lock, NULL) != 0) {
      free(bufmgr_fake);
      return NULL;
   }

   /* Initialize allocator */
   DRMINITLISTHEAD(&bufmgr_fake->fenced);
   DRMINITLISTHEAD(&bufmgr_fake->on_hardware);
   DRMINITLISTHEAD(&bufmgr_fake->lru);

   bufmgr_fake->low_offset = low_offset;
   bufmgr_fake->virtual = low_virtual;
   bufmgr_fake->size = size;
   bufmgr_fake->heap = mmInit(low_offset, size);

   /* Hook in methods */
   bufmgr_fake->bufmgr.bo_alloc = drm_intel_fake_bo_alloc;
   bufmgr_fake->bufmgr.bo_alloc_for_render = drm_intel_fake_bo_alloc;
   bufmgr_fake->bufmgr.bo_reference = drm_intel_fake_bo_reference;
   bufmgr_fake->bufmgr.bo_unreference = drm_intel_fake_bo_unreference;
   bufmgr_fake->bufmgr.bo_map = drm_intel_fake_bo_map;
   bufmgr_fake->bufmgr.bo_unmap = drm_intel_fake_bo_unmap;
   bufmgr_fake->bufmgr.bo_wait_rendering = drm_intel_fake_bo_wait_rendering;
   bufmgr_fake->bufmgr.bo_emit_reloc = drm_intel_fake_emit_reloc;
   bufmgr_fake->bufmgr.bo_pin = drm_intel_fake_pin;
   bufmgr_fake->bufmgr.bo_unpin = drm_intel_fake_unpin;
   bufmgr_fake->bufmgr.destroy = drm_intel_fake_destroy;
   bufmgr_fake->bufmgr.bo_exec = drm_intel_fake_bo_exec;
   bufmgr_fake->bufmgr.check_aperture_space = drm_intel_fake_check_aperture_space;
   bufmgr_fake->bufmgr.debug = 0;
d1650 2
a1651 2
   bufmgr_fake->fd = fd;
   bufmgr_fake->last_dispatch = (volatile int *)last_dispatch;
d1653 1
a1653 1
   return &bufmgr_fake->bufmgr;
a1654 1

@


1.2
log
@update libdrm to 2.4.9 (actually to -current, but the only real changes
are to some assertions and a small change to modesetting code).

bump libdrm_intel minor due to added symbols, libdrm doesn't get bumped,
no change to symbol list.

ok matthieu@@.
@
text
@d1403 36
d1556 2
@


1.1
log
@Update libdrm to 2.4.3

This is needed for us to be able to update Mesa and xf86-video-intel.
Includes a few fixes, as well as the intel bufmgr interface and the
modesetting interface (which currently errors when you check if it's
enabled).

ok matthieu@@
@
text
@d447 2
a448 1
static void free_block(drm_intel_bufmgr_fake *bufmgr_fake, struct block *block)
d457 5
a461 1
   if (!(bo_fake->flags & (BM_PINNED | BM_NO_BACKING_STORE)) && (bo_fake->card_dirty == 1)) {
d542 1
a542 1
      free_block(bufmgr_fake, block);
d565 1
a565 1
      free_block(bufmgr_fake, block);
d880 1
a880 1
	 free_block(bufmgr_fake, bo_fake->block);
d1072 1
a1072 1
      free_block(bufmgr_fake, block);
d1470 1
d1472 2
a1473 1
      free_block(bufmgr_fake, block);
d1513 1
@


1.1.1.1
log
@Import libdrm 2.4.64
@
text
@a51 1
#include "libdrm_macros.h"
d54 2
d57 2
a58 2
	if (bufmgr_fake->bufmgr.debug)			\
		drmMsg(__VA_ARGS__);			\
d75 14
a88 15
struct fake_buffer_reloc {
	/** Buffer object that the relocation points at. */
	drm_intel_bo *target_buf;
	/** Offset of the relocation entry within reloc_buf. */
	uint32_t offset;
	/**
	 * Cached value of the offset when we last performed this relocation.
	 */
	uint32_t last_target_offset;
	/** Value added to target_buf's offset to get the relocation entry. */
	uint32_t delta;
	/** Cache domains the target buffer is read into. */
	uint32_t read_domains;
	/** Cache domain the target buffer will have dirty cachelines in. */
	uint32_t write_domain;
d92 2
a93 2
	struct block *next, *prev;
	struct mem_block *mem;	/* BM_MEM_AGP */
d95 10
a104 10
	/**
	 * Marks that the block is currently in the aperture and has yet to be
	 * fenced.
	 */
	unsigned on_hardware:1;
	/**
	 * Marks that the block is currently fenced (being used by rendering)
	 * and can't be freed until @@fence is passed.
	 */
	unsigned fenced:1;
d106 2
a107 2
	/** Fence cookie for the block. */
	unsigned fence;		/* Split to read_fence, write_fence */
d109 2
a110 2
	drm_intel_bo *bo;
	void *virtual;
d114 1
a114 1
	drm_intel_bufmgr bufmgr;
d116 1
a116 1
	pthread_mutex_t lock;
d118 55
a172 59
	unsigned long low_offset;
	unsigned long size;
	void *virtual;

	struct mem_block *heap;

	unsigned buf_nr;	/* for generating ids */

	/**
	 * List of blocks which are currently in the GART but haven't been
	 * fenced yet.
	 */
	struct block on_hardware;
	/**
	 * List of blocks which are in the GART and have an active fence on
	 * them.
	 */
	struct block fenced;
	/**
	 * List of blocks which have an expired fence and are ready to be
	 * evicted.
	 */
	struct block lru;

	unsigned int last_fence;

	unsigned fail:1;
	unsigned need_fence:1;
	int thrashing;

	/**
	 * Driver callback to emit a fence, returning the cookie.
	 *
	 * This allows the driver to hook in a replacement for the DRM usage in
	 * bufmgr_fake.
	 *
	 * Currently, this also requires that a write flush be emitted before
	 * emitting the fence, but this should change.
	 */
	unsigned int (*fence_emit) (void *private);
	/** Driver callback to wait for a fence cookie to have passed. */
	void (*fence_wait) (unsigned int fence, void *private);
	void *fence_priv;

	/**
	 * Driver callback to execute a buffer.
	 *
	 * This allows the driver to hook in a replacement for the DRM usage in
	 * bufmgr_fake.
	 */
	int (*exec) (drm_intel_bo *bo, unsigned int used, void *priv);
	void *exec_priv;

	/** Driver-supplied argument to driver callbacks */
	void *driver_priv;
	/**
	 * Pointer to kernel-updated sarea data for the last completed user irq
	 */
	volatile int *last_dispatch;
d174 1
a174 1
	int fd;
d176 1
a176 1
	int debug;
d178 1
a178 1
	int performed_rendering;
d182 1
a182 1
	drm_intel_bo bo;
d184 2
a185 2
	unsigned id;		/* debug only */
	const char *name;
d187 32
a218 34
	unsigned dirty:1;
	/**
	 * has the card written to this buffer - we make need to copy it back
	 */
	unsigned card_dirty:1;
	unsigned int refcount;
	/* Flags may consist of any of the DRM_BO flags, plus
	 * DRM_BO_NO_BACKING_STORE and BM_NO_FENCE_SUBDATA, which are the
	 * first two driver private flags.
	 */
	uint64_t flags;
	/** Cache domains the target buffer is read into. */
	uint32_t read_domains;
	/** Cache domain the target buffer will have dirty cachelines in. */
	uint32_t write_domain;

	unsigned int alignment;
	int is_static, validated;
	unsigned int map_count;

	/** relocation list */
	struct fake_buffer_reloc *relocs;
	int nr_relocs;
	/**
	 * Total size of the target_bos of this buffer.
	 *
	 * Used for estimation in check_aperture.
	 */
	unsigned int child_size;

	struct block *block;
	void *backing_store;
	void (*invalidate_cb) (drm_intel_bo *bo, void *ptr);
	void *invalidate_ptr;
d226 1
a226 2
static int
FENCE_LTE(unsigned a, unsigned b)
d228 2
a229 2
	if (a == b)
		return 1;
d231 2
a232 2
	if (a < b && b - a < (1 << 24))
		return 1;
d234 2
a235 2
	if (a > b && MAXFENCE - a + b < (1 << 24))
		return 1;
d237 1
a237 1
	return 0;
d240 11
a250 12
void
drm_intel_bufmgr_fake_set_fence_callback(drm_intel_bufmgr *bufmgr,
					 unsigned int (*emit) (void *priv),
					 void (*wait) (unsigned int fence,
						       void *priv),
					 void *priv)
{
	drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *) bufmgr;

	bufmgr_fake->fence_emit = emit;
	bufmgr_fake->fence_wait = wait;
	bufmgr_fake->fence_priv = priv;
d256 2
a257 2
	struct drm_i915_irq_emit ie;
	int ret, seq = 1;
d259 12
a270 12
	if (bufmgr_fake->fence_emit != NULL) {
		seq = bufmgr_fake->fence_emit(bufmgr_fake->fence_priv);
		return seq;
	}

	ie.irq_seq = &seq;
	ret = drmCommandWriteRead(bufmgr_fake->fd, DRM_I915_IRQ_EMIT,
				  &ie, sizeof(ie));
	if (ret) {
		drmMsg("%s: drm_i915_irq_emit: %d\n", __func__, ret);
		abort();
	}
d272 2
a273 2
	DBG("emit 0x%08x\n", seq);
	return seq;
d279 118
a396 121
	struct drm_i915_irq_wait iw;
	int hw_seq, busy_count = 0;
	int ret;
	int kernel_lied;

	if (bufmgr_fake->fence_wait != NULL) {
		bufmgr_fake->fence_wait(seq, bufmgr_fake->fence_priv);
		clear_fenced(bufmgr_fake, seq);
		return;
	}

	iw.irq_seq = seq;

	DBG("wait 0x%08x\n", iw.irq_seq);

	/* The kernel IRQ_WAIT implementation is all sorts of broken.
	 * 1) It returns 1 to 0x7fffffff instead of using the full 32-bit
	 *    unsigned range.
	 * 2) It returns 0 if hw_seq >= seq, not seq - hw_seq < 0 on the 32-bit
	 *    signed range.
	 * 3) It waits if seq < hw_seq, not seq - hw_seq > 0 on the 32-bit
	 *    signed range.
	 * 4) It returns -EBUSY in 3 seconds even if the hardware is still
	 *    successfully chewing through buffers.
	 *
	 * Assume that in userland we treat sequence numbers as ints, which
	 * makes some of the comparisons convenient, since the sequence
	 * numbers are all postive signed integers.
	 *
	 * From this we get several cases we need to handle.  Here's a timeline.
	 * 0x2   0x7                                    0x7ffffff8   0x7ffffffd
	 *   |    |                                             |    |
	 * ------------------------------------------------------------
	 *
	 * A) Normal wait for hw to catch up
	 * hw_seq seq
	 *   |    |
	 * ------------------------------------------------------------
	 * seq - hw_seq = 5.  If we call IRQ_WAIT, it will wait for hw to
	 * catch up.
	 *
	 * B) Normal wait for a sequence number that's already passed.
	 * seq    hw_seq
	 *   |    |
	 * ------------------------------------------------------------
	 * seq - hw_seq = -5.  If we call IRQ_WAIT, it returns 0 quickly.
	 *
	 * C) Hardware has already wrapped around ahead of us
	 * hw_seq                                                    seq
	 *   |                                                       |
	 * ------------------------------------------------------------
	 * seq - hw_seq = 0x80000000 - 5.  If we called IRQ_WAIT, it would wait
	 * for hw_seq >= seq, which may never occur.  Thus, we want to catch
	 * this in userland and return 0.
	 *
	 * D) We've wrapped around ahead of the hardware.
	 * seq                                                      hw_seq
	 *   |                                                       |
	 * ------------------------------------------------------------
	 * seq - hw_seq = -(0x80000000 - 5).  If we called IRQ_WAIT, it would
	 * return 0 quickly because hw_seq >= seq, even though the hardware
	 * isn't caught up. Thus, we need to catch this early return in
	 * userland and bother the kernel until the hardware really does
	 * catch up.
	 *
	 * E) Hardware might wrap after we test in userland.
	 *                                                  hw_seq  seq
	 *                                                      |    |
	 * ------------------------------------------------------------
	 * seq - hw_seq = 5.  If we call IRQ_WAIT, it will likely see seq >=
	 * hw_seq and wait.  However, suppose hw_seq wraps before we make it
	 * into the kernel.  The kernel sees hw_seq >= seq and waits for 3
	 * seconds then returns -EBUSY.  This is case C).  We should catch
	 * this and then return successfully.
	 *
	 * F) Hardware might take a long time on a buffer.
	 * hw_seq seq
	 *   |    |
	 * -------------------------------------------------------------------
	 * seq - hw_seq = 5.  If we call IRQ_WAIT, if sequence 2 through 5
	 * take too long, it will return -EBUSY.  Batchbuffers in the
	 * gltestperf demo were seen to take up to 7 seconds.  We should
	 * catch early -EBUSY return and keep trying.
	 */

	do {
		/* Keep a copy of last_dispatch so that if the wait -EBUSYs
		 * because the hardware didn't catch up in 3 seconds, we can
		 * see if it at least made progress and retry.
		 */
		hw_seq = *bufmgr_fake->last_dispatch;

		/* Catch case C */
		if (seq - hw_seq > 0x40000000)
			return;

		ret = drmCommandWrite(bufmgr_fake->fd, DRM_I915_IRQ_WAIT,
				      &iw, sizeof(iw));
		/* Catch case D */
		kernel_lied = (ret == 0) && (seq - *bufmgr_fake->last_dispatch <
					     -0x40000000);

		/* Catch case E */
		if (ret == -EBUSY
		    && (seq - *bufmgr_fake->last_dispatch > 0x40000000))
			ret = 0;

		/* Catch case F: Allow up to 15 seconds chewing on one buffer. */
		if ((ret == -EBUSY) && (hw_seq != *bufmgr_fake->last_dispatch))
			busy_count = 0;
		else
			busy_count++;
	} while (kernel_lied || ret == -EAGAIN || ret == -EINTR ||
		 (ret == -EBUSY && busy_count < 5));

	if (ret != 0) {
		drmMsg("%s:%d: Error waiting for fence: %s.\n", __FILE__,
		       __LINE__, strerror(-ret));
		abort();
	}
	clear_fenced(bufmgr_fake, seq);
d402 3
a404 3
	/* Slight problem with wrap-around:
	 */
	return fence == 0 || FENCE_LTE(fence, bufmgr_fake->last_fence);
d413 5
a417 6
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	struct block *block = (struct block *)calloc(sizeof *block, 1);
	unsigned int align_log2 = ffs(bo_fake->alignment) - 1;
	unsigned int sz;
d419 2
a420 2
	if (!block)
		return 1;
d422 1
a422 1
	sz = (bo->size + bo_fake->alignment - 1) & ~(bo_fake->alignment - 1);
d424 5
a428 5
	block->mem = mmAllocMem(bufmgr_fake->heap, sz, align_log2, 0);
	if (!block->mem) {
		free(block);
		return 0;
	}
d430 1
a430 1
	DRMINITLISTHEAD(block);
d432 3
a434 2
	/* Insert at head or at tail??? */
	DRMLISTADDTAIL(block, &bufmgr_fake->lru);
d436 3
a438 3
	block->virtual = (uint8_t *) bufmgr_fake->virtual +
	    block->mem->ofs - bufmgr_fake->low_offset;
	block->bo = bo;
d440 1
a440 1
	bo_fake->block = block;
d442 1
a442 1
	return 1;
d447 1
a447 3
static void
free_block(drm_intel_bufmgr_fake *bufmgr_fake, struct block *block,
	   int skip_dirty_copy)
d449 26
a474 29
	drm_intel_bo_fake *bo_fake;
	DBG("free block %p %08x %d %d\n", block, block->mem->ofs,
	    block->on_hardware, block->fenced);

	if (!block)
		return;

	bo_fake = (drm_intel_bo_fake *) block->bo;

	if (bo_fake->flags & (BM_PINNED | BM_NO_BACKING_STORE))
		skip_dirty_copy = 1;

	if (!skip_dirty_copy && (bo_fake->card_dirty == 1)) {
		memcpy(bo_fake->backing_store, block->virtual, block->bo->size);
		bo_fake->card_dirty = 0;
		bo_fake->dirty = 1;
	}

	if (block->on_hardware) {
		block->bo = NULL;
	} else if (block->fenced) {
		block->bo = NULL;
	} else {
		DBG("    - free immediately\n");
		DRMLISTDEL(block);

		mmFreeMem(block->mem);
		free(block);
	}
d480 9
a488 11
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;
	assert(!bo_fake->backing_store);
	assert(!(bo_fake->flags & (BM_PINNED | BM_NO_BACKING_STORE)));

	bo_fake->backing_store = malloc(bo->size);

	DBG("alloc_backing - buf %d %p %lu\n", bo_fake->id,
	    bo_fake->backing_store, bo->size);
	assert(bo_fake->backing_store);
d494 1
a494 1
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;
d496 5
a500 5
	if (bo_fake->backing_store) {
		assert(!(bo_fake->flags & (BM_PINNED | BM_NO_BACKING_STORE)));
		free(bo_fake->backing_store);
		bo_fake->backing_store = NULL;
	}
d506 2
a507 7
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;

	if (bo_fake->flags & BM_NO_BACKING_STORE
	    && bo_fake->invalidate_cb != NULL)
		bo_fake->invalidate_cb(bo, bo_fake->invalidate_ptr);
d509 2
a510 1
	assert(!(bo_fake->flags & BM_PINNED));
d512 4
a515 2
	DBG("set_dirty - buf %d\n", bo_fake->id);
	bo_fake->dirty = 1;
d521 1
a521 1
	struct block *block, *tmp;
d523 1
a523 1
	DBG("%s\n", __func__);
d525 2
a526 2
	DRMLISTFOREACHSAFE(block, tmp, &bufmgr_fake->lru) {
		drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) block->bo;
d528 2
a529 2
		if (bo_fake != NULL && (bo_fake->flags & BM_NO_FENCE_SUBDATA))
			continue;
d531 2
a532 3
		if (block->fence && max_fence && !FENCE_LTE(block->fence,
							    max_fence))
			return 0;
d534 2
a535 2
		set_dirty(&bo_fake->bo);
		bo_fake->block = NULL;
d537 3
a539 3
		free_block(bufmgr_fake, block, 0);
		return 1;
	}
d541 1
a541 1
	return 0;
d547 1
a547 1
	struct block *block, *tmp;
d549 1
a549 1
	DBG("%s\n", __func__);
d551 2
a552 2
	DRMLISTFOREACHSAFEREVERSE(block, tmp, &bufmgr_fake->lru) {
		drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) block->bo;
d554 2
a555 2
		if (bo_fake && (bo_fake->flags & BM_NO_FENCE_SUBDATA))
			continue;
d557 2
a558 2
		set_dirty(&bo_fake->bo);
		bo_fake->block = NULL;
d560 3
a562 3
		free_block(bufmgr_fake, block, 0);
		return 1;
	}
d564 1
a564 1
	return 0;
d570 2
a571 2
static int
clear_fenced(drm_intel_bufmgr_fake *bufmgr_fake, unsigned int fence_cookie)
d573 2
a574 2
	struct block *block, *tmp;
	int ret = 0;
d576 115
a690 117
	bufmgr_fake->last_fence = fence_cookie;
	DRMLISTFOREACHSAFE(block, tmp, &bufmgr_fake->fenced) {
		assert(block->fenced);

		if (_fence_test(bufmgr_fake, block->fence)) {

			block->fenced = 0;

			if (!block->bo) {
				DBG("delayed free: offset %x sz %x\n",
				    block->mem->ofs, block->mem->size);
				DRMLISTDEL(block);
				mmFreeMem(block->mem);
				free(block);
			} else {
				DBG("return to lru: offset %x sz %x\n",
				    block->mem->ofs, block->mem->size);
				DRMLISTDEL(block);
				DRMLISTADDTAIL(block, &bufmgr_fake->lru);
			}

			ret = 1;
		} else {
			/* Blocks are ordered by fence, so if one fails, all
			 * from here will fail also:
			 */
			DBG("fence not passed: offset %x sz %x %d %d \n",
			    block->mem->ofs, block->mem->size, block->fence,
			    bufmgr_fake->last_fence);
			break;
		}
	}

	DBG("%s: %d\n", __func__, ret);
	return ret;
}

static void
fence_blocks(drm_intel_bufmgr_fake *bufmgr_fake, unsigned fence)
{
	struct block *block, *tmp;

	DRMLISTFOREACHSAFE(block, tmp, &bufmgr_fake->on_hardware) {
		DBG("Fence block %p (sz 0x%x ofs %x buf %p) with fence %d\n",
		    block, block->mem->size, block->mem->ofs, block->bo, fence);
		block->fence = fence;

		block->on_hardware = 0;
		block->fenced = 1;

		/* Move to tail of pending list here
		 */
		DRMLISTDEL(block);
		DRMLISTADDTAIL(block, &bufmgr_fake->fenced);
	}

	assert(DRMLISTEMPTY(&bufmgr_fake->on_hardware));
}

static int
evict_and_alloc_block(drm_intel_bo *bo)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;

	assert(bo_fake->block == NULL);

	/* Search for already free memory:
	 */
	if (alloc_block(bo))
		return 1;

	/* If we're not thrashing, allow lru eviction to dig deeper into
	 * recently used textures.  We'll probably be thrashing soon:
	 */
	if (!bufmgr_fake->thrashing) {
		while (evict_lru(bufmgr_fake, 0))
			if (alloc_block(bo))
				return 1;
	}

	/* Keep thrashing counter alive?
	 */
	if (bufmgr_fake->thrashing)
		bufmgr_fake->thrashing = 20;

	/* Wait on any already pending fences - here we are waiting for any
	 * freed memory that has been submitted to hardware and fenced to
	 * become available:
	 */
	while (!DRMLISTEMPTY(&bufmgr_fake->fenced)) {
		uint32_t fence = bufmgr_fake->fenced.next->fence;
		_fence_wait_internal(bufmgr_fake, fence);

		if (alloc_block(bo))
			return 1;
	}

	if (!DRMLISTEMPTY(&bufmgr_fake->on_hardware)) {
		while (!DRMLISTEMPTY(&bufmgr_fake->fenced)) {
			uint32_t fence = bufmgr_fake->fenced.next->fence;
			_fence_wait_internal(bufmgr_fake, fence);
		}

		if (!bufmgr_fake->thrashing) {
			DBG("thrashing\n");
		}
		bufmgr_fake->thrashing = 20;

		if (alloc_block(bo))
			return 1;
	}

	while (evict_mru(bufmgr_fake))
		if (alloc_block(bo))
			return 1;
d692 1
a692 1
	DBG("%s 0x%lx bytes failed\n", __func__, bo->size);
d694 1
a694 1
	return 0;
d707 1
a707 1
	unsigned int cookie;
d709 2
a710 2
	cookie = _fence_emit_internal(bufmgr_fake);
	_fence_wait_internal(bufmgr_fake, cookie);
d722 2
a723 3
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;
d725 2
a726 2
	if (bo_fake->block == NULL || !bo_fake->block->fenced)
		return;
d728 1
a728 1
	_fence_wait_internal(bufmgr_fake, bo_fake->block->fence);
d734 1
a734 2
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
d736 3
a738 3
	pthread_mutex_lock(&bufmgr_fake->lock);
	drm_intel_fake_bo_wait_rendering_locked(bo);
	pthread_mutex_unlock(&bufmgr_fake->lock);
d748 2
a749 2
	drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *) bufmgr;
	struct block *block, *tmp;
d751 1
a751 1
	pthread_mutex_lock(&bufmgr_fake->lock);
d753 2
a754 2
	bufmgr_fake->need_fence = 1;
	bufmgr_fake->fail = 0;
d756 16
a771 16
	/* Wait for hardware idle.  We don't know where acceleration has been
	 * happening, so we'll need to wait anyway before letting anything get
	 * put on the card again.
	 */
	drm_intel_bufmgr_fake_wait_idle(bufmgr_fake);

	/* Check that we hadn't released the lock without having fenced the last
	 * set of buffers.
	 */
	assert(DRMLISTEMPTY(&bufmgr_fake->fenced));
	assert(DRMLISTEMPTY(&bufmgr_fake->on_hardware));

	DRMLISTFOREACHSAFE(block, tmp, &bufmgr_fake->lru) {
		assert(_fence_test(bufmgr_fake, block->fence));
		set_dirty(block->bo);
	}
d773 1
a773 1
	pthread_mutex_unlock(&bufmgr_fake->lock);
d777 2
a778 45
drm_intel_fake_bo_alloc(drm_intel_bufmgr *bufmgr,
			const char *name,
			unsigned long size,
			unsigned int alignment)
{
	drm_intel_bufmgr_fake *bufmgr_fake;
	drm_intel_bo_fake *bo_fake;

	bufmgr_fake = (drm_intel_bufmgr_fake *) bufmgr;

	assert(size != 0);

	bo_fake = calloc(1, sizeof(*bo_fake));
	if (!bo_fake)
		return NULL;

	bo_fake->bo.size = size;
	bo_fake->bo.offset = -1;
	bo_fake->bo.virtual = NULL;
	bo_fake->bo.bufmgr = bufmgr;
	bo_fake->refcount = 1;

	/* Alignment must be a power of two */
	assert((alignment & (alignment - 1)) == 0);
	if (alignment == 0)
		alignment = 1;
	bo_fake->alignment = alignment;
	bo_fake->id = ++bufmgr_fake->buf_nr;
	bo_fake->name = name;
	bo_fake->flags = 0;
	bo_fake->is_static = 0;

	DBG("drm_bo_alloc: (buf %d: %s, %lu kb)\n", bo_fake->id, bo_fake->name,
	    bo_fake->bo.size / 1024);

	return &bo_fake->bo;
}

static drm_intel_bo *
drm_intel_fake_bo_alloc_tiled(drm_intel_bufmgr * bufmgr,
			      const char *name,
			      int x, int y, int cpp,
			      uint32_t *tiling_mode,
			      unsigned long *pitch,
			      unsigned long flags)
d780 2
a781 4
	unsigned long stride, aligned_y;

	/* No runtime tiling support for fake. */
	*tiling_mode = I915_TILING_NONE;
d783 1
a783 3
	/* Align it for being a render target.  Shouldn't need anything else. */
	stride = x * cpp;
	stride = ROUND_UP_TO(stride, 64);
d785 21
a805 2
	/* 965 subspan loading alignment */
	aligned_y = ALIGN(y, 2);
d807 2
a808 1
	*pitch = stride;
d810 1
a810 2
	return drm_intel_fake_bo_alloc(bufmgr, name, stride * aligned_y,
				       4096);
d814 24
a837 25
drm_intel_bo_fake_alloc_static(drm_intel_bufmgr *bufmgr,
			       const char *name,
			       unsigned long offset,
			       unsigned long size, void *virtual)
{
	drm_intel_bufmgr_fake *bufmgr_fake;
	drm_intel_bo_fake *bo_fake;

	bufmgr_fake = (drm_intel_bufmgr_fake *) bufmgr;

	assert(size != 0);

	bo_fake = calloc(1, sizeof(*bo_fake));
	if (!bo_fake)
		return NULL;

	bo_fake->bo.size = size;
	bo_fake->bo.offset = offset;
	bo_fake->bo.virtual = virtual;
	bo_fake->bo.bufmgr = bufmgr;
	bo_fake->refcount = 1;
	bo_fake->id = ++bufmgr_fake->buf_nr;
	bo_fake->name = name;
	bo_fake->flags = BM_PINNED;
	bo_fake->is_static = 1;
d839 2
a840 2
	DBG("drm_bo_alloc_static: (buf %d: %s, %lu kb)\n", bo_fake->id,
	    bo_fake->name, bo_fake->bo.size / 1024);
d842 1
a842 1
	return &bo_fake->bo;
d848 6
a853 7
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;

	pthread_mutex_lock(&bufmgr_fake->lock);
	bo_fake->refcount++;
	pthread_mutex_unlock(&bufmgr_fake->lock);
d859 1
a859 1
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;
d861 1
a861 1
	bo_fake->refcount++;
d867 19
a885 22
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;
	int i;

	if (--bo_fake->refcount == 0) {
		assert(bo_fake->map_count == 0);
		/* No remaining references, so free it */
		if (bo_fake->block)
			free_block(bufmgr_fake, bo_fake->block, 1);
		free_backing_store(bo);

		for (i = 0; i < bo_fake->nr_relocs; i++)
			drm_intel_fake_bo_unreference_locked(bo_fake->relocs[i].
							     target_buf);

		DBG("drm_bo_unreference: free buf %d %s\n", bo_fake->id,
		    bo_fake->name);

		free(bo_fake->relocs);
		free(bo);
	}
d891 1
a891 2
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
d893 3
a895 3
	pthread_mutex_lock(&bufmgr_fake->lock);
	drm_intel_fake_bo_unreference_locked(bo);
	pthread_mutex_unlock(&bufmgr_fake->lock);
d902 26
a927 28
void
drm_intel_bo_fake_disable_backing_store(drm_intel_bo *bo,
					void (*invalidate_cb) (drm_intel_bo *bo,
							       void *ptr),
					void *ptr)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;

	pthread_mutex_lock(&bufmgr_fake->lock);

	if (bo_fake->backing_store)
		free_backing_store(bo);

	bo_fake->flags |= BM_NO_BACKING_STORE;

	DBG("disable_backing_store set buf %d dirty\n", bo_fake->id);
	bo_fake->dirty = 1;
	bo_fake->invalidate_cb = invalidate_cb;
	bo_fake->invalidate_ptr = ptr;

	/* Note that it is invalid right from the start.  Also note
	 * invalidate_cb is called with the bufmgr locked, so cannot
	 * itself make bufmgr calls.
	 */
	if (invalidate_cb != NULL)
		invalidate_cb(bo, ptr);
d929 1
a929 1
	pthread_mutex_unlock(&bufmgr_fake->lock);
d937 1
a937 1
 drm_intel_fake_bo_map_locked(drm_intel_bo *bo, int write_enable)
d939 64
a1002 66
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;

	/* Static buffers are always mapped. */
	if (bo_fake->is_static) {
		if (bo_fake->card_dirty) {
			drm_intel_bufmgr_fake_wait_idle(bufmgr_fake);
			bo_fake->card_dirty = 0;
		}
		return 0;
	}

	/* Allow recursive mapping.  Mesa may recursively map buffers with
	 * nested display loops, and it is used internally in bufmgr_fake
	 * for relocation.
	 */
	if (bo_fake->map_count++ != 0)
		return 0;

	{
		DBG("drm_bo_map: (buf %d: %s, %lu kb)\n", bo_fake->id,
		    bo_fake->name, bo_fake->bo.size / 1024);

		if (bo->virtual != NULL) {
			drmMsg("%s: already mapped\n", __func__);
			abort();
		} else if (bo_fake->flags & (BM_NO_BACKING_STORE | BM_PINNED)) {

			if (!bo_fake->block && !evict_and_alloc_block(bo)) {
				DBG("%s: alloc failed\n", __func__);
				bufmgr_fake->fail = 1;
				return 1;
			} else {
				assert(bo_fake->block);
				bo_fake->dirty = 0;

				if (!(bo_fake->flags & BM_NO_FENCE_SUBDATA) &&
				    bo_fake->block->fenced) {
					drm_intel_fake_bo_wait_rendering_locked
					    (bo);
				}

				bo->virtual = bo_fake->block->virtual;
			}
		} else {
			if (write_enable)
				set_dirty(bo);

			if (bo_fake->backing_store == 0)
				alloc_backing_store(bo);

			if ((bo_fake->card_dirty == 1) && bo_fake->block) {
				if (bo_fake->block->fenced)
					drm_intel_fake_bo_wait_rendering_locked
					    (bo);

				memcpy(bo_fake->backing_store,
				       bo_fake->block->virtual,
				       bo_fake->block->bo->size);
				bo_fake->card_dirty = 0;
			}

			bo->virtual = bo_fake->backing_store;
		}
	}
d1004 1
a1004 1
	return 0;
d1008 1
a1008 1
 drm_intel_fake_bo_map(drm_intel_bo *bo, int write_enable)
d1010 2
a1011 3
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	int ret;
d1013 3
a1015 3
	pthread_mutex_lock(&bufmgr_fake->lock);
	ret = drm_intel_fake_bo_map_locked(bo, write_enable);
	pthread_mutex_unlock(&bufmgr_fake->lock);
d1017 1
a1017 1
	return ret;
d1021 1
a1021 1
 drm_intel_fake_bo_unmap_locked(drm_intel_bo *bo)
d1023 2
a1024 7
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;

	/* Static buffers are always mapped. */
	if (bo_fake->is_static)
		return 0;
d1026 3
a1028 3
	assert(bo_fake->map_count != 0);
	if (--bo_fake->map_count != 0)
		return 0;
d1030 3
a1032 7
	DBG("drm_bo_unmap: (buf %d: %s, %lu kb)\n", bo_fake->id, bo_fake->name,
	    bo_fake->bo.size / 1024);

	bo->virtual = NULL;

	return 0;
}
d1034 2
a1035 5
static int drm_intel_fake_bo_unmap(drm_intel_bo *bo)
{
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	int ret;
d1037 1
a1037 3
	pthread_mutex_lock(&bufmgr_fake->lock);
	ret = drm_intel_fake_bo_unmap_locked(bo);
	pthread_mutex_unlock(&bufmgr_fake->lock);
d1039 1
a1039 1
	return ret;
d1043 1
a1043 2
drm_intel_fake_bo_subdata(drm_intel_bo *bo, unsigned long offset,
			  unsigned long size, const void *data)
d1045 2
a1046 1
	int ret;
d1048 3
a1050 2
	if (size == 0 || data == NULL)
		return 0;
d1052 1
a1052 6
	ret = drm_intel_bo_map(bo, 1);
	if (ret)
		return ret;
	memcpy((unsigned char *)bo->virtual + offset, data, size);
	drm_intel_bo_unmap(bo);
	return 0;
d1056 1
a1056 1
 drm_intel_fake_kick_all_locked(drm_intel_bufmgr_fake *bufmgr_fake)
d1058 1
a1058 1
	struct block *block, *tmp;
d1060 13
a1072 13
	bufmgr_fake->performed_rendering = 0;
	/* okay for ever BO that is on the HW kick it off.
	   seriously not afraid of the POLICE right now */
	DRMLISTFOREACHSAFE(block, tmp, &bufmgr_fake->on_hardware) {
		drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) block->bo;

		block->on_hardware = 0;
		free_block(bufmgr_fake, block, 0);
		bo_fake->block = NULL;
		bo_fake->validated = 0;
		if (!(bo_fake->flags & BM_NO_BACKING_STORE))
			bo_fake->dirty = 1;
	}
d1077 1
a1077 1
 drm_intel_fake_bo_validate(drm_intel_bo *bo)
d1079 2
a1080 2
	drm_intel_bufmgr_fake *bufmgr_fake;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;
d1082 1
a1082 1
	bufmgr_fake = (drm_intel_bufmgr_fake *) bo->bufmgr;
d1084 56
a1139 57
	DBG("drm_bo_validate: (buf %d: %s, %lu kb)\n", bo_fake->id,
	    bo_fake->name, bo_fake->bo.size / 1024);

	/* Sanity check: Buffers should be unmapped before being validated.
	 * This is not so much of a problem for bufmgr_fake, but TTM refuses,
	 * and the problem is harder to debug there.
	 */
	assert(bo_fake->map_count == 0);

	if (bo_fake->is_static) {
		/* Add it to the needs-fence list */
		bufmgr_fake->need_fence = 1;
		return 0;
	}

	/* Allocate the card memory */
	if (!bo_fake->block && !evict_and_alloc_block(bo)) {
		bufmgr_fake->fail = 1;
		DBG("Failed to validate buf %d:%s\n", bo_fake->id,
		    bo_fake->name);
		return -1;
	}

	assert(bo_fake->block);
	assert(bo_fake->block->bo == &bo_fake->bo);

	bo->offset = bo_fake->block->mem->ofs;

	/* Upload the buffer contents if necessary */
	if (bo_fake->dirty) {
		DBG("Upload dirty buf %d:%s, sz %lu offset 0x%x\n", bo_fake->id,
		    bo_fake->name, bo->size, bo_fake->block->mem->ofs);

		assert(!(bo_fake->flags & (BM_NO_BACKING_STORE | BM_PINNED)));

		/* Actually, should be able to just wait for a fence on the
		 * mmory, hich we would be tracking when we free it.  Waiting
		 * for idle is a sufficiently large hammer for now.
		 */
		drm_intel_bufmgr_fake_wait_idle(bufmgr_fake);

		/* we may never have mapped this BO so it might not have any
		 * backing store if this happens it should be rare, but 0 the
		 * card memory in any case */
		if (bo_fake->backing_store)
			memcpy(bo_fake->block->virtual, bo_fake->backing_store,
			       bo->size);
		else
			memset(bo_fake->block->virtual, 0, bo->size);

		bo_fake->dirty = 0;
	}

	bo_fake->block->fenced = 0;
	bo_fake->block->on_hardware = 1;
	DRMLISTDEL(bo_fake->block);
	DRMLISTADDTAIL(bo_fake->block, &bufmgr_fake->on_hardware);
d1141 2
a1142 2
	bo_fake->validated = 1;
	bufmgr_fake->need_fence = 1;
d1144 1
a1144 1
	return 0;
d1150 2
a1151 2
	drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *) bufmgr;
	unsigned int cookie;
d1153 2
a1154 2
	cookie = _fence_emit_internal(bufmgr_fake);
	fence_blocks(bufmgr_fake, cookie);
d1156 1
a1156 1
	DBG("drm_fence_validated: 0x%08x cookie\n", cookie);
d1162 1
a1162 1
	drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *) bufmgr;
d1164 3
a1166 3
	pthread_mutex_destroy(&bufmgr_fake->lock);
	mmDestroy(bufmgr_fake->heap);
	free(bufmgr);
d1174 40
a1213 45
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	struct fake_buffer_reloc *r;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;
	drm_intel_bo_fake *target_fake = (drm_intel_bo_fake *) target_bo;
	int i;

	pthread_mutex_lock(&bufmgr_fake->lock);

	assert(bo);
	assert(target_bo);

	if (bo_fake->relocs == NULL) {
		bo_fake->relocs =
		    malloc(sizeof(struct fake_buffer_reloc) * MAX_RELOCS);
	}

	r = &bo_fake->relocs[bo_fake->nr_relocs++];

	assert(bo_fake->nr_relocs <= MAX_RELOCS);

	drm_intel_fake_bo_reference_locked(target_bo);

	if (!target_fake->is_static) {
		bo_fake->child_size +=
		    ALIGN(target_bo->size, target_fake->alignment);
		bo_fake->child_size += target_fake->child_size;
	}
	r->target_buf = target_bo;
	r->offset = offset;
	r->last_target_offset = target_bo->offset;
	r->delta = target_offset;
	r->read_domains = read_domains;
	r->write_domain = write_domain;

	if (bufmgr_fake->debug) {
		/* Check that a conflicting relocation hasn't already been
		 * emitted.
		 */
		for (i = 0; i < bo_fake->nr_relocs - 1; i++) {
			struct fake_buffer_reloc *r2 = &bo_fake->relocs[i];

			assert(r->offset != r2->offset);
		}
	}
d1215 1
a1215 1
	pthread_mutex_unlock(&bufmgr_fake->lock);
d1217 1
a1217 1
	return 0;
d1227 2
a1228 2
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;
	int i;
d1230 10
a1239 11
	for (i = 0; i < bo_fake->nr_relocs; i++) {
		struct fake_buffer_reloc *r = &bo_fake->relocs[i];
		drm_intel_bo_fake *target_fake =
		    (drm_intel_bo_fake *) r->target_buf;

		/* Do the same for the tree of buffers we depend on */
		drm_intel_fake_calculate_domains(r->target_buf);

		target_fake->read_domains |= r->read_domains;
		target_fake->write_domain |= r->write_domain;
	}
d1242 1
d1246 45
a1290 49
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;
	int i, ret;

	assert(bo_fake->map_count == 0);

	for (i = 0; i < bo_fake->nr_relocs; i++) {
		struct fake_buffer_reloc *r = &bo_fake->relocs[i];
		drm_intel_bo_fake *target_fake =
		    (drm_intel_bo_fake *) r->target_buf;
		uint32_t reloc_data;

		/* Validate the target buffer if that hasn't been done. */
		if (!target_fake->validated) {
			ret =
			    drm_intel_fake_reloc_and_validate_buffer(r->target_buf);
			if (ret != 0) {
				if (bo->virtual != NULL)
					drm_intel_fake_bo_unmap_locked(bo);
				return ret;
			}
		}

		/* Calculate the value of the relocation entry. */
		if (r->target_buf->offset != r->last_target_offset) {
			reloc_data = r->target_buf->offset + r->delta;

			if (bo->virtual == NULL)
				drm_intel_fake_bo_map_locked(bo, 1);

			*(uint32_t *) ((uint8_t *) bo->virtual + r->offset) =
			    reloc_data;

			r->last_target_offset = r->target_buf->offset;
		}
	}

	if (bo->virtual != NULL)
		drm_intel_fake_bo_unmap_locked(bo);

	if (bo_fake->write_domain != 0) {
		if (!(bo_fake->flags & (BM_NO_BACKING_STORE | BM_PINNED))) {
			if (bo_fake->backing_store == 0)
				alloc_backing_store(bo);
		}
		bo_fake->card_dirty = 1;
		bufmgr_fake->performed_rendering = 1;
	}
d1292 1
a1292 1
	return drm_intel_fake_bo_validate(bo);
d1298 20
a1317 23
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo;
	int i;

	for (i = 0; i < bo_fake->nr_relocs; i++) {
		struct fake_buffer_reloc *r = &bo_fake->relocs[i];
		drm_intel_bo_fake *target_fake =
		    (drm_intel_bo_fake *) r->target_buf;

		if (target_fake->validated)
			drm_intel_bo_fake_post_submit(r->target_buf);

		DBG("%s@@0x%08x + 0x%08x -> %s@@0x%08x + 0x%08x\n",
		    bo_fake->name, (uint32_t) bo->offset, r->offset,
		    target_fake->name, (uint32_t) r->target_buf->offset,
		    r->delta);
	}

	assert(bo_fake->map_count == 0);
	bo_fake->validated = 0;
	bo_fake->read_domains = 0;
	bo_fake->write_domain = 0;
d1320 5
a1324 5
void
drm_intel_bufmgr_fake_set_exec_callback(drm_intel_bufmgr *bufmgr,
					     int (*exec) (drm_intel_bo *bo,
							  unsigned int used,
							  void *priv),
d1327 1
a1327 1
	drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *) bufmgr;
d1329 2
a1330 2
	bufmgr_fake->exec = exec;
	bufmgr_fake->exec_priv = priv;
d1335 2
a1336 1
		       drm_clip_rect_t * cliprects, int num_cliprects, int DR4)
d1338 50
a1387 52
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo->bufmgr;
	drm_intel_bo_fake *batch_fake = (drm_intel_bo_fake *) bo;
	struct drm_i915_batchbuffer batch;
	int ret;
	int retry_count = 0;

	pthread_mutex_lock(&bufmgr_fake->lock);

	bufmgr_fake->performed_rendering = 0;

	drm_intel_fake_calculate_domains(bo);

	batch_fake->read_domains = I915_GEM_DOMAIN_COMMAND;

	/* we've ran out of RAM so blow the whole lot away and retry */
restart:
	ret = drm_intel_fake_reloc_and_validate_buffer(bo);
	if (bufmgr_fake->fail == 1) {
		if (retry_count == 0) {
			retry_count++;
			drm_intel_fake_kick_all_locked(bufmgr_fake);
			bufmgr_fake->fail = 0;
			goto restart;
		} else		/* dump out the memory here */
			mmDumpMemInfo(bufmgr_fake->heap);
	}

	assert(ret == 0);

	if (bufmgr_fake->exec != NULL) {
		int ret = bufmgr_fake->exec(bo, used, bufmgr_fake->exec_priv);
		if (ret != 0) {
			pthread_mutex_unlock(&bufmgr_fake->lock);
			return ret;
		}
	} else {
		batch.start = bo->offset;
		batch.used = used;
		batch.cliprects = cliprects;
		batch.num_cliprects = num_cliprects;
		batch.DR1 = 0;
		batch.DR4 = DR4;

		if (drmCommandWrite
		    (bufmgr_fake->fd, DRM_I915_BATCHBUFFER, &batch,
		     sizeof(batch))) {
			drmMsg("DRM_I915_BATCHBUFFER: %d\n", -errno);
			pthread_mutex_unlock(&bufmgr_fake->lock);
			return -errno;
		}
	}
d1389 1
a1389 1
	drm_intel_fake_fence_validated(bo->bufmgr);
d1391 1
a1391 1
	drm_intel_bo_fake_post_submit(bo);
d1393 1
a1393 1
	pthread_mutex_unlock(&bufmgr_fake->lock);
d1395 1
a1395 1
	return 0;
d1406 1
a1406 1
drm_intel_fake_check_aperture_space(drm_intel_bo ** bo_array, int count)
d1408 24
a1431 25
	drm_intel_bufmgr_fake *bufmgr_fake =
	    (drm_intel_bufmgr_fake *) bo_array[0]->bufmgr;
	unsigned int sz = 0;
	int i;

	for (i = 0; i < count; i++) {
		drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) bo_array[i];

		if (bo_fake == NULL)
			continue;

		if (!bo_fake->is_static)
			sz += ALIGN(bo_array[i]->size, bo_fake->alignment);
		sz += bo_fake->child_size;
	}

	if (sz > bufmgr_fake->size) {
		DBG("check_space: overflowed bufmgr size, %ukb vs %lukb\n",
		    sz / 1024, bufmgr_fake->size / 1024);
		return -1;
	}

	DBG("drm_check_space: sz %ukb vs bufgr %lukb\n", sz / 1024,
	    bufmgr_fake->size / 1024);
	return 0;
d1444 7
a1450 2
	drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *) bufmgr;
	struct block *block, *tmp;
d1452 5
a1456 1
	pthread_mutex_lock(&bufmgr_fake->lock);
d1458 5
a1462 2
	bufmgr_fake->need_fence = 1;
	bufmgr_fake->fail = 0;
d1464 4
a1467 20
	/* Wait for hardware idle.  We don't know where acceleration has been
	 * happening, so we'll need to wait anyway before letting anything get
	 * put on the card again.
	 */
	drm_intel_bufmgr_fake_wait_idle(bufmgr_fake);

	/* Check that we hadn't released the lock without having fenced the last
	 * set of buffers.
	 */
	assert(DRMLISTEMPTY(&bufmgr_fake->fenced));
	assert(DRMLISTEMPTY(&bufmgr_fake->on_hardware));

	DRMLISTFOREACHSAFE(block, tmp, &bufmgr_fake->lru) {
		drm_intel_bo_fake *bo_fake = (drm_intel_bo_fake *) block->bo;
		/* Releases the memory, and memcpys dirty contents out if
		 * necessary.
		 */
		free_block(bufmgr_fake, block, 0);
		bo_fake->block = NULL;
	}
d1469 1
a1469 1
	pthread_mutex_unlock(&bufmgr_fake->lock);
d1471 2
a1472 5

void
drm_intel_bufmgr_fake_set_last_dispatch(drm_intel_bufmgr *bufmgr,
					volatile unsigned int
					*last_dispatch)
d1474 1
a1474 1
	drm_intel_bufmgr_fake *bufmgr_fake = (drm_intel_bufmgr_fake *) bufmgr;
d1476 1
a1476 1
	bufmgr_fake->last_dispatch = (volatile int *)last_dispatch;
d1480 36
a1515 40
drm_intel_bufmgr_fake_init(int fd, unsigned long low_offset,
			   void *low_virtual, unsigned long size,
			   volatile unsigned int *last_dispatch)
{
	drm_intel_bufmgr_fake *bufmgr_fake;

	bufmgr_fake = calloc(1, sizeof(*bufmgr_fake));

	if (pthread_mutex_init(&bufmgr_fake->lock, NULL) != 0) {
		free(bufmgr_fake);
		return NULL;
	}

	/* Initialize allocator */
	DRMINITLISTHEAD(&bufmgr_fake->fenced);
	DRMINITLISTHEAD(&bufmgr_fake->on_hardware);
	DRMINITLISTHEAD(&bufmgr_fake->lru);

	bufmgr_fake->low_offset = low_offset;
	bufmgr_fake->virtual = low_virtual;
	bufmgr_fake->size = size;
	bufmgr_fake->heap = mmInit(low_offset, size);

	/* Hook in methods */
	bufmgr_fake->bufmgr.bo_alloc = drm_intel_fake_bo_alloc;
	bufmgr_fake->bufmgr.bo_alloc_for_render = drm_intel_fake_bo_alloc;
	bufmgr_fake->bufmgr.bo_alloc_tiled = drm_intel_fake_bo_alloc_tiled;
	bufmgr_fake->bufmgr.bo_reference = drm_intel_fake_bo_reference;
	bufmgr_fake->bufmgr.bo_unreference = drm_intel_fake_bo_unreference;
	bufmgr_fake->bufmgr.bo_map = drm_intel_fake_bo_map;
	bufmgr_fake->bufmgr.bo_unmap = drm_intel_fake_bo_unmap;
	bufmgr_fake->bufmgr.bo_subdata = drm_intel_fake_bo_subdata;
	bufmgr_fake->bufmgr.bo_wait_rendering =
	    drm_intel_fake_bo_wait_rendering;
	bufmgr_fake->bufmgr.bo_emit_reloc = drm_intel_fake_emit_reloc;
	bufmgr_fake->bufmgr.destroy = drm_intel_fake_destroy;
	bufmgr_fake->bufmgr.bo_exec = drm_intel_fake_bo_exec;
	bufmgr_fake->bufmgr.check_aperture_space =
	    drm_intel_fake_check_aperture_space;
	bufmgr_fake->bufmgr.debug = 0;
d1517 2
a1518 2
	bufmgr_fake->fd = fd;
	bufmgr_fake->last_dispatch = (volatile int *)last_dispatch;
d1520 1
a1520 1
	return &bufmgr_fake->bufmgr;
d1522 1
@


1.1.1.2
log
@Import libdrm 2.4.65
@
text
@d1463 1
a1463 1
		ret = bufmgr_fake->exec(bo, used, bufmgr_fake->exec_priv);
@


1.1.1.3
log
@Import libdrm 2.4.67
@
text
@a44 1
#include <strings.h>
@


1.1.1.4
log
@Import libdrm 2.4.71
@
text
@d315 1
a315 1
	 * numbers are all positive signed integers.
@


1.1.1.5
log
@Import libdrm 2.4.75
@
text
@d740 1
a740 1
 * It is assumed that the batchbuffer which performed the rendering included
d1203 1
a1203 1
		 * memory, which we would be tracking when we free it. Waiting
@


