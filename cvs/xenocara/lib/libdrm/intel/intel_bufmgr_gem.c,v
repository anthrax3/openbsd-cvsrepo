head	1.30;
access;
symbols
	libdrm_2_4_82:1.1.1.8
	libdrm_2_4_79:1.1.1.7
	OPENBSD_6_1:1.28.0.2
	OPENBSD_6_1_BASE:1.28
	libdrm_2_4_75:1.1.1.6
	libdrm_2_4_73:1.1.1.5
	libdrm_2_4_71:1.1.1.4
	OPENBSD_6_0:1.25.0.2
	OPENBSD_6_0_BASE:1.25
	libdrm_2_4_67:1.1.1.3
	OPENBSD_5_9:1.24.0.2
	OPENBSD_5_9_BASE:1.24
	libdrm_2_4_65:1.1.1.2
	libdrm_2_4_64:1.1.1.1
	libdrm:1.1.1
	OPENBSD_5_8:1.22.0.2
	OPENBSD_5_8_BASE:1.22
	OPENBSD_5_7:1.20.0.2
	OPENBSD_5_7_BASE:1.20
	OPENBSD_5_6:1.19.0.2
	OPENBSD_5_6_BASE:1.19
	OPENBSD_5_5:1.18.0.2
	OPENBSD_5_5_BASE:1.18
	OPENBSD_5_4:1.16.0.2
	OPENBSD_5_4_BASE:1.16
	OPENBSD_5_3:1.11.0.2
	OPENBSD_5_3_BASE:1.11
	OPENBSD_5_2:1.9.0.6
	OPENBSD_5_2_BASE:1.9
	OPENBSD_5_1_BASE:1.9
	OPENBSD_5_1:1.9.0.4
	OPENBSD_5_0:1.9.0.2
	OPENBSD_5_0_BASE:1.9
	OPENBSD_4_9:1.8.0.2
	OPENBSD_4_9_BASE:1.8
	OPENBSD_4_8:1.8.0.4
	OPENBSD_4_8_BASE:1.8
	OPENBSD_4_7:1.4.0.2
	OPENBSD_4_7_BASE:1.4
	OPENBSD_4_6:1.2.0.2
	OPENBSD_4_6_BASE:1.2
	OPENBSD_4_5:1.1.0.2
	OPENBSD_4_5_BASE:1.1;
locks; strict;
comment	@ * @;


1.30
date	2017.08.05.14.21.16;	author jsg;	state Exp;
branches;
next	1.29;
commitid	F4EHy2QMmWYVGJb0;

1.29
date	2017.04.14.07.52.11;	author jsg;	state Exp;
branches;
next	1.28;
commitid	fiK2yxgM8SQDjLiL;

1.28
date	2017.02.05.05.46.41;	author jsg;	state Exp;
branches;
next	1.27;
commitid	7u73SYrl7j8vU8XM;

1.27
date	2016.11.19.05.45.01;	author jsg;	state Exp;
branches;
next	1.26;
commitid	tIcH9u4yWHjj9FVh;

1.26
date	2016.10.16.06.11.11;	author jsg;	state Exp;
branches;
next	1.25;
commitid	nvLf7SydSyxGShbn;

1.25
date	2016.03.20.10.41.29;	author jsg;	state Exp;
branches;
next	1.24;
commitid	mbp2aNpFrhnQxfR8;

1.24
date	2015.12.27.09.16.53;	author jsg;	state Exp;
branches;
next	1.23;
commitid	3T6ygenFOApe7XWL;

1.23
date	2015.08.22.00.30.35;	author jsg;	state Exp;
branches;
next	1.22;
commitid	qzdJ3g11hqB96r2U;

1.22
date	2015.07.15.23.19.17;	author jsg;	state Exp;
branches;
next	1.21;
commitid	NCd5WpEjxT48SV2b;

1.21
date	2015.03.26.06.00.06;	author jsg;	state Exp;
branches;
next	1.20;
commitid	xcCFtgqKNdwnnIdt;

1.20
date	2015.02.07.01.34.35;	author jsg;	state Exp;
branches;
next	1.19;
commitid	ttKh4qveShZNixll;

1.19
date	2014.03.15.05.05.55;	author jsg;	state Exp;
branches;
next	1.18;

1.18
date	2014.01.18.08.29.32;	author jsg;	state Exp;
branches;
next	1.17;

1.17
date	2013.11.21.13.30.07;	author kettenis;	state Exp;
branches;
next	1.16;

1.16
date	2013.07.08.09.10.05;	author jsg;	state Exp;
branches;
next	1.15;

1.15
date	2013.07.01.13.12.52;	author jsg;	state Exp;
branches;
next	1.14;

1.14
date	2013.06.20.09.55.30;	author jsg;	state Exp;
branches;
next	1.13;

1.13
date	2013.06.07.20.48.17;	author kettenis;	state Exp;
branches;
next	1.12;

1.12
date	2013.03.25.02.32.20;	author jsg;	state Exp;
branches;
next	1.11;

1.11
date	2013.01.09.10.47.08;	author jsg;	state Exp;
branches;
next	1.10;

1.10
date	2012.11.27.14.37.08;	author mpi;	state Exp;
branches;
next	1.9;

1.9
date	2011.05.01.15.48.20;	author oga;	state Exp;
branches;
next	1.8;

1.8
date	2010.05.15.15.51.05;	author oga;	state Exp;
branches;
next	1.7;

1.7
date	2010.04.25.14.35.49;	author oga;	state Exp;
branches;
next	1.6;

1.6
date	2010.04.11.18.04.04;	author oga;	state Exp;
branches;
next	1.5;

1.5
date	2010.03.31.06.31.45;	author oga;	state Exp;
branches;
next	1.4;

1.4
date	2009.11.22.20.16.03;	author oga;	state Exp;
branches;
next	1.3;

1.3
date	2009.07.18.14.54.42;	author oga;	state Exp;
branches;
next	1.2;

1.2
date	2009.05.03.19.43.26;	author oga;	state Exp;
branches;
next	1.1;

1.1
date	2009.01.26.23.14.37;	author oga;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2015.08.21.23.56.08;	author jsg;	state Exp;
branches;
next	1.1.1.2;
commitid	IINH94ZNafdY3NZg;

1.1.1.2
date	2015.12.27.08.58.07;	author jsg;	state Exp;
branches;
next	1.1.1.3;
commitid	1d5D4vlIz2Mv01RY;

1.1.1.3
date	2016.03.20.10.19.56;	author jsg;	state Exp;
branches;
next	1.1.1.4;
commitid	66ApiJCJocBDCoju;

1.1.1.4
date	2016.10.16.06.01.32;	author jsg;	state Exp;
branches;
next	1.1.1.5;
commitid	UDYjDzwIWWXeKnu7;

1.1.1.5
date	2016.11.19.05.36.53;	author jsg;	state Exp;
branches;
next	1.1.1.6;
commitid	J3nLYkpAOBFQLy9J;

1.1.1.6
date	2017.02.05.05.38.31;	author jsg;	state Exp;
branches;
next	1.1.1.7;
commitid	GTUS6FM9u0BarnEJ;

1.1.1.7
date	2017.04.14.07.45.54;	author jsg;	state Exp;
branches;
next	1.1.1.8;
commitid	AidaeLRxsBwWWBha;

1.1.1.8
date	2017.08.05.14.15.31;	author jsg;	state Exp;
branches;
next	;
commitid	WCJygAyfd5UnVEc0;


desc
@@


1.30
log
@Merge libdrm 2.4.82
@
text
@/**************************************************************************
 *
 * Copyright © 2007 Red Hat Inc.
 * Copyright © 2007-2012 Intel Corporation
 * Copyright 2006 Tungsten Graphics, Inc., Bismarck, ND., USA
 * All Rights Reserved.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sub license, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL
 * THE COPYRIGHT HOLDERS, AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM,
 * DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
 * OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
 * USE OR OTHER DEALINGS IN THE SOFTWARE.
 *
 * The above copyright notice and this permission notice (including the
 * next paragraph) shall be included in all copies or substantial portions
 * of the Software.
 *
 *
 **************************************************************************/
/*
 * Authors: Thomas Hellström <thomas-at-tungstengraphics-dot-com>
 *          Keith Whitwell <keithw-at-tungstengraphics-dot-com>
 *	    Eric Anholt <eric@@anholt.net>
 *	    Dave Airlie <airlied@@linux.ie>
 */

#ifdef HAVE_CONFIG_H
#include "config.h"
#endif

#include <xf86drm.h>
#include <xf86atomic.h>
#include <fcntl.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <assert.h>
#include <pthread.h>
#include <sys/ioctl.h>
#include <sys/stat.h>
#include <sys/types.h>
#include <stdbool.h>

#include "errno.h"
#ifndef ETIME
#define ETIME ETIMEDOUT
#endif
#include "libdrm_macros.h"
#include "libdrm_lists.h"
#include "intel_bufmgr.h"
#include "intel_bufmgr_priv.h"
#include "intel_chipset.h"
#include "string.h"

#include "i915_drm.h"
#include "uthash.h"

#ifdef HAVE_VALGRIND
#include <valgrind.h>
#include <memcheck.h>
#define VG(x) x
#else
#define VG(x)
#endif

#define memclear(s) memset(&s, 0, sizeof(s))

#define DBG(...) do {					\
	if (bufmgr_gem->bufmgr.debug)			\
		fprintf(stderr, __VA_ARGS__);		\
} while (0)

#define ARRAY_SIZE(x) (sizeof(x) / sizeof((x)[0]))
#define MAX2(A, B) ((A) > (B) ? (A) : (B))

/**
 * upper_32_bits - return bits 32-63 of a number
 * @@n: the number we're accessing
 *
 * A basic shift-right of a 64- or 32-bit quantity.  Use this to suppress
 * the "right shift count >= width of type" warning when that quantity is
 * 32-bits.
 */
#define upper_32_bits(n) ((__u32)(((n) >> 16) >> 16))

/**
 * lower_32_bits - return bits 0-31 of a number
 * @@n: the number we're accessing
 */
#define lower_32_bits(n) ((__u32)(n))

typedef struct _drm_intel_bo_gem drm_intel_bo_gem;

struct drm_intel_gem_bo_bucket {
	drmMMListHead head;
	unsigned long size;
};

typedef struct _drm_intel_bufmgr_gem {
	drm_intel_bufmgr bufmgr;

	atomic_t refcount;

	int fd;

	int max_relocs;

	pthread_mutex_t lock;

#ifndef __OpenBSD__
	struct drm_i915_gem_exec_object *exec_objects;
#endif
	struct drm_i915_gem_exec_object2 *exec2_objects;
	drm_intel_bo **exec_bos;
	int exec_size;
	int exec_count;

	/** Array of lists of cached gem objects of power-of-two sizes */
	struct drm_intel_gem_bo_bucket cache_bucket[14 * 4];
	int num_buckets;
	time_t time;

	drmMMListHead managers;

	drm_intel_bo_gem *name_table;
	drm_intel_bo_gem *handle_table;

	drmMMListHead vma_cache;
	int vma_count, vma_open, vma_max;

	uint64_t gtt_size;
	int available_fences;
	int pci_device;
	int gen;
	unsigned int has_bsd : 1;
	unsigned int has_blt : 1;
	unsigned int has_relaxed_fencing : 1;
	unsigned int has_llc : 1;
	unsigned int has_wait_timeout : 1;
	unsigned int bo_reuse : 1;
	unsigned int no_exec : 1;
	unsigned int has_vebox : 1;
	unsigned int has_exec_async : 1;
	bool fenced_relocs;

	struct {
		void *ptr;
		uint32_t handle;
	} userptr_active;

} drm_intel_bufmgr_gem;

#define DRM_INTEL_RELOC_FENCE (1<<0)

typedef struct _drm_intel_reloc_target_info {
	drm_intel_bo *bo;
	int flags;
} drm_intel_reloc_target;

struct _drm_intel_bo_gem {
	drm_intel_bo bo;

	atomic_t refcount;
	uint32_t gem_handle;
	const char *name;

	/**
	 * Kenel-assigned global name for this object
         *
         * List contains both flink named and prime fd'd objects
	 */
	unsigned int global_name;

	UT_hash_handle handle_hh;
	UT_hash_handle name_hh;

	/**
	 * Index of the buffer within the validation list while preparing a
	 * batchbuffer execution.
	 */
	int validate_index;

	/**
	 * Current tiling mode
	 */
	uint32_t tiling_mode;
	uint32_t swizzle_mode;
	unsigned long stride;

	unsigned long kflags;

	time_t free_time;

	/** Array passed to the DRM containing relocation information. */
	struct drm_i915_gem_relocation_entry *relocs;
	/**
	 * Array of info structs corresponding to relocs[i].target_handle etc
	 */
	drm_intel_reloc_target *reloc_target_info;
	/** Number of entries in relocs */
	int reloc_count;
	/** Array of BOs that are referenced by this buffer and will be softpinned */
	drm_intel_bo **softpin_target;
	/** Number softpinned BOs that are referenced by this buffer */
	int softpin_target_count;
	/** Maximum amount of softpinned BOs that are referenced by this buffer */
	int softpin_target_size;

	/** Mapped address for the buffer, saved across map/unmap cycles */
	void *mem_virtual;
	/** GTT virtual address for the buffer, saved across map/unmap cycles */
	void *gtt_virtual;
	/** WC CPU address for the buffer, saved across map/unmap cycles */
	void *wc_virtual;
	/**
	 * Virtual address of the buffer allocated by user, used for userptr
	 * objects only.
	 */
	void *user_virtual;
	int map_count;
	drmMMListHead vma_list;

	/** BO cache list */
	drmMMListHead head;

	/**
	 * Boolean of whether this BO and its children have been included in
	 * the current drm_intel_bufmgr_check_aperture_space() total.
	 */
	bool included_in_check_aperture;

	/**
	 * Boolean of whether this buffer has been used as a relocation
	 * target and had its size accounted for, and thus can't have any
	 * further relocations added to it.
	 */
	bool used_as_reloc_target;

	/**
	 * Boolean of whether we have encountered an error whilst building the relocation tree.
	 */
	bool has_error;

	/**
	 * Boolean of whether this buffer can be re-used
	 */
	bool reusable;

	/**
	 * Boolean of whether the GPU is definitely not accessing the buffer.
	 *
	 * This is only valid when reusable, since non-reusable
	 * buffers are those that have been shared with other
	 * processes, so we don't know their state.
	 */
	bool idle;

	/**
	 * Boolean of whether this buffer was allocated with userptr
	 */
	bool is_userptr;

	/**
	 * Size in bytes of this buffer and its relocation descendents.
	 *
	 * Used to avoid costly tree walking in
	 * drm_intel_bufmgr_check_aperture in the common case.
	 */
	int reloc_tree_size;

	/**
	 * Number of potential fence registers required by this buffer and its
	 * relocations.
	 */
	int reloc_tree_fences;

	/** Flags that we may need to do the SW_FINISH ioctl on unmap. */
	bool mapped_cpu_write;
};

static unsigned int
drm_intel_gem_estimate_batch_space(drm_intel_bo ** bo_array, int count);

static unsigned int
drm_intel_gem_compute_batch_space(drm_intel_bo ** bo_array, int count);

static int
drm_intel_gem_bo_get_tiling(drm_intel_bo *bo, uint32_t * tiling_mode,
			    uint32_t * swizzle_mode);

static int
drm_intel_gem_bo_set_tiling_internal(drm_intel_bo *bo,
				     uint32_t tiling_mode,
				     uint32_t stride);

static void drm_intel_gem_bo_unreference_locked_timed(drm_intel_bo *bo,
						      time_t time);

static void drm_intel_gem_bo_unreference(drm_intel_bo *bo);

static void drm_intel_gem_bo_free(drm_intel_bo *bo);

static inline drm_intel_bo_gem *to_bo_gem(drm_intel_bo *bo)
{
        return (drm_intel_bo_gem *)bo;
}

static unsigned long
drm_intel_gem_bo_tile_size(drm_intel_bufmgr_gem *bufmgr_gem, unsigned long size,
			   uint32_t *tiling_mode)
{
	unsigned long min_size, max_size;
	unsigned long i;

	if (*tiling_mode == I915_TILING_NONE)
		return size;

	/* 965+ just need multiples of page size for tiling */
	if (bufmgr_gem->gen >= 4)
		return ROUND_UP_TO(size, 4096);

	/* Older chips need powers of two, of at least 512k or 1M */
	if (bufmgr_gem->gen == 3) {
		min_size = 1024*1024;
		max_size = 128*1024*1024;
	} else {
		min_size = 512*1024;
		max_size = 64*1024*1024;
	}

	if (size > max_size) {
		*tiling_mode = I915_TILING_NONE;
		return size;
	}

	/* Do we need to allocate every page for the fence? */
	if (bufmgr_gem->has_relaxed_fencing)
		return ROUND_UP_TO(size, 4096);

	for (i = min_size; i < size; i <<= 1)
		;

	return i;
}

/*
 * Round a given pitch up to the minimum required for X tiling on a
 * given chip.  We use 512 as the minimum to allow for a later tiling
 * change.
 */
static unsigned long
drm_intel_gem_bo_tile_pitch(drm_intel_bufmgr_gem *bufmgr_gem,
			    unsigned long pitch, uint32_t *tiling_mode)
{
	unsigned long tile_width;
	unsigned long i;

	/* If untiled, then just align it so that we can do rendering
	 * to it with the 3D engine.
	 */
	if (*tiling_mode == I915_TILING_NONE)
		return ALIGN(pitch, 64);

	if (*tiling_mode == I915_TILING_X
			|| (IS_915(bufmgr_gem->pci_device)
			    && *tiling_mode == I915_TILING_Y))
		tile_width = 512;
	else
		tile_width = 128;

	/* 965 is flexible */
	if (bufmgr_gem->gen >= 4)
		return ROUND_UP_TO(pitch, tile_width);

	/* The older hardware has a maximum pitch of 8192 with tiled
	 * surfaces, so fallback to untiled if it's too large.
	 */
	if (pitch > 8192) {
		*tiling_mode = I915_TILING_NONE;
		return ALIGN(pitch, 64);
	}

	/* Pre-965 needs power of two tile width */
	for (i = tile_width; i < pitch; i <<= 1)
		;

	return i;
}

static struct drm_intel_gem_bo_bucket *
drm_intel_gem_bo_bucket_for_size(drm_intel_bufmgr_gem *bufmgr_gem,
				 unsigned long size)
{
	int i;

	for (i = 0; i < bufmgr_gem->num_buckets; i++) {
		struct drm_intel_gem_bo_bucket *bucket =
		    &bufmgr_gem->cache_bucket[i];
		if (bucket->size >= size) {
			return bucket;
		}
	}

	return NULL;
}

static void
drm_intel_gem_dump_validation_list(drm_intel_bufmgr_gem *bufmgr_gem)
{
	int i, j;

	for (i = 0; i < bufmgr_gem->exec_count; i++) {
		drm_intel_bo *bo = bufmgr_gem->exec_bos[i];
		drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

		if (bo_gem->relocs == NULL && bo_gem->softpin_target == NULL) {
			DBG("%2d: %d %s(%s)\n", i, bo_gem->gem_handle,
			    bo_gem->kflags & EXEC_OBJECT_PINNED ? "*" : "",
			    bo_gem->name);
			continue;
		}

		for (j = 0; j < bo_gem->reloc_count; j++) {
			drm_intel_bo *target_bo = bo_gem->reloc_target_info[j].bo;
			drm_intel_bo_gem *target_gem =
			    (drm_intel_bo_gem *) target_bo;

			DBG("%2d: %d %s(%s)@@0x%08x %08x -> "
			    "%d (%s)@@0x%08x %08x + 0x%08x\n",
			    i,
			    bo_gem->gem_handle,
			    bo_gem->kflags & EXEC_OBJECT_PINNED ? "*" : "",
			    bo_gem->name,
			    upper_32_bits(bo_gem->relocs[j].offset),
			    lower_32_bits(bo_gem->relocs[j].offset),
			    target_gem->gem_handle,
			    target_gem->name,
			    upper_32_bits(target_bo->offset64),
			    lower_32_bits(target_bo->offset64),
			    bo_gem->relocs[j].delta);
		}

		for (j = 0; j < bo_gem->softpin_target_count; j++) {
			drm_intel_bo *target_bo = bo_gem->softpin_target[j];
			drm_intel_bo_gem *target_gem =
			    (drm_intel_bo_gem *) target_bo;
			DBG("%2d: %d %s(%s) -> "
			    "%d *(%s)@@0x%08x %08x\n",
			    i,
			    bo_gem->gem_handle,
			    bo_gem->kflags & EXEC_OBJECT_PINNED ? "*" : "",
			    bo_gem->name,
			    target_gem->gem_handle,
			    target_gem->name,
			    upper_32_bits(target_bo->offset64),
			    lower_32_bits(target_bo->offset64));
		}
	}
}

static inline void
drm_intel_gem_bo_reference(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	atomic_inc(&bo_gem->refcount);
}

#ifndef __OpenBSD__
/**
 * Adds the given buffer to the list of buffers to be validated (moved into the
 * appropriate memory type) with the next batch submission.
 *
 * If a buffer is validated multiple times in a batch submission, it ends up
 * with the intersection of the memory type flags and the union of the
 * access flags.
 */
static void
drm_intel_add_validate_buffer(drm_intel_bo *bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	int index;

	if (bo_gem->validate_index != -1)
		return;

	/* Extend the array of validation entries as necessary. */
	if (bufmgr_gem->exec_count == bufmgr_gem->exec_size) {
		int new_size = bufmgr_gem->exec_size * 2;

		if (new_size == 0)
			new_size = 5;

		bufmgr_gem->exec_objects =
		    realloc(bufmgr_gem->exec_objects,
			    sizeof(*bufmgr_gem->exec_objects) * new_size);
		bufmgr_gem->exec_bos =
		    realloc(bufmgr_gem->exec_bos,
			    sizeof(*bufmgr_gem->exec_bos) * new_size);
		bufmgr_gem->exec_size = new_size;
	}

	index = bufmgr_gem->exec_count;
	bo_gem->validate_index = index;
	/* Fill in array entry */
	bufmgr_gem->exec_objects[index].handle = bo_gem->gem_handle;
	bufmgr_gem->exec_objects[index].relocation_count = bo_gem->reloc_count;
	bufmgr_gem->exec_objects[index].relocs_ptr = (uintptr_t) bo_gem->relocs;
	bufmgr_gem->exec_objects[index].alignment = bo->align;
	bufmgr_gem->exec_objects[index].offset = 0;
	bufmgr_gem->exec_bos[index] = bo;
	bufmgr_gem->exec_count++;
}
#endif

static void
drm_intel_add_validate_buffer2(drm_intel_bo *bo, int need_fence)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
	int index;
	unsigned long flags;

	flags = 0;
	if (need_fence)
		flags |= EXEC_OBJECT_NEEDS_FENCE;

	if (bo_gem->validate_index != -1) {
		bufmgr_gem->exec2_objects[bo_gem->validate_index].flags |= flags;
		return;
	}

	/* Extend the array of validation entries as necessary. */
	if (bufmgr_gem->exec_count == bufmgr_gem->exec_size) {
		int new_size = bufmgr_gem->exec_size * 2;

		if (new_size == 0)
			new_size = 5;

		bufmgr_gem->exec2_objects =
			realloc(bufmgr_gem->exec2_objects,
				sizeof(*bufmgr_gem->exec2_objects) * new_size);
		bufmgr_gem->exec_bos =
			realloc(bufmgr_gem->exec_bos,
				sizeof(*bufmgr_gem->exec_bos) * new_size);
		bufmgr_gem->exec_size = new_size;
	}

	index = bufmgr_gem->exec_count;
	bo_gem->validate_index = index;
	/* Fill in array entry */
	bufmgr_gem->exec2_objects[index].handle = bo_gem->gem_handle;
	bufmgr_gem->exec2_objects[index].relocation_count = bo_gem->reloc_count;
	bufmgr_gem->exec2_objects[index].relocs_ptr = (uintptr_t)bo_gem->relocs;
	bufmgr_gem->exec2_objects[index].alignment = bo->align;
	bufmgr_gem->exec2_objects[index].offset = bo->offset64;
	bufmgr_gem->exec2_objects[index].flags = bo_gem->kflags | flags;
	bufmgr_gem->exec2_objects[index].rsvd1 = 0;
	bufmgr_gem->exec2_objects[index].rsvd2 = 0;
	bufmgr_gem->exec_bos[index] = bo;
	bufmgr_gem->exec_count++;
}

#define RELOC_BUF_SIZE(x) ((I915_RELOC_HEADER + x * I915_RELOC0_STRIDE) * \
	sizeof(uint32_t))

static void
drm_intel_bo_gem_set_in_aperture_size(drm_intel_bufmgr_gem *bufmgr_gem,
				      drm_intel_bo_gem *bo_gem,
				      unsigned int alignment)
{
	unsigned int size;

	assert(!bo_gem->used_as_reloc_target);

	/* The older chipsets are far-less flexible in terms of tiling,
	 * and require tiled buffer to be size aligned in the aperture.
	 * This means that in the worst possible case we will need a hole
	 * twice as large as the object in order for it to fit into the
	 * aperture. Optimal packing is for wimps.
	 */
	size = bo_gem->bo.size;
	if (bufmgr_gem->gen < 4 && bo_gem->tiling_mode != I915_TILING_NONE) {
		unsigned int min_size;

		if (bufmgr_gem->has_relaxed_fencing) {
			if (bufmgr_gem->gen == 3)
				min_size = 1024*1024;
			else
				min_size = 512*1024;

			while (min_size < size)
				min_size *= 2;
		} else
			min_size = size;

		/* Account for worst-case alignment. */
		alignment = MAX2(alignment, min_size);
	}

	bo_gem->reloc_tree_size = size + alignment;
}

static int
drm_intel_setup_reloc_list(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	unsigned int max_relocs = bufmgr_gem->max_relocs;

	if (bo->size / 4 < max_relocs)
		max_relocs = bo->size / 4;

	bo_gem->relocs = malloc(max_relocs *
				sizeof(struct drm_i915_gem_relocation_entry));
	bo_gem->reloc_target_info = malloc(max_relocs *
					   sizeof(drm_intel_reloc_target));
	if (bo_gem->relocs == NULL || bo_gem->reloc_target_info == NULL) {
		bo_gem->has_error = true;

		free (bo_gem->relocs);
		bo_gem->relocs = NULL;

		free (bo_gem->reloc_target_info);
		bo_gem->reloc_target_info = NULL;

		return 1;
	}

	return 0;
}

static int
drm_intel_gem_bo_busy(drm_intel_bo *bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_busy busy;
	int ret;

	if (bo_gem->reusable && bo_gem->idle)
		return false;

	memclear(busy);
	busy.handle = bo_gem->gem_handle;

	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_BUSY, &busy);
	if (ret == 0) {
		bo_gem->idle = !busy.busy;
		return busy.busy;
	} else {
		return false;
	}
}

static int
drm_intel_gem_bo_madvise_internal(drm_intel_bufmgr_gem *bufmgr_gem,
				  drm_intel_bo_gem *bo_gem, int state)
{
	struct drm_i915_gem_madvise madv;

	memclear(madv);
	madv.handle = bo_gem->gem_handle;
	madv.madv = state;
	madv.retained = 1;
	drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_MADVISE, &madv);

	return madv.retained;
}

static int
drm_intel_gem_bo_madvise(drm_intel_bo *bo, int madv)
{
	return drm_intel_gem_bo_madvise_internal
		((drm_intel_bufmgr_gem *) bo->bufmgr,
		 (drm_intel_bo_gem *) bo,
		 madv);
}

/* drop the oldest entries that have been purged by the kernel */
static void
drm_intel_gem_bo_cache_purge_bucket(drm_intel_bufmgr_gem *bufmgr_gem,
				    struct drm_intel_gem_bo_bucket *bucket)
{
	while (!DRMLISTEMPTY(&bucket->head)) {
		drm_intel_bo_gem *bo_gem;

		bo_gem = DRMLISTENTRY(drm_intel_bo_gem,
				      bucket->head.next, head);
		if (drm_intel_gem_bo_madvise_internal
		    (bufmgr_gem, bo_gem, I915_MADV_DONTNEED))
			break;

		DRMLISTDEL(&bo_gem->head);
		drm_intel_gem_bo_free(&bo_gem->bo);
	}
}

static drm_intel_bo *
drm_intel_gem_bo_alloc_internal(drm_intel_bufmgr *bufmgr,
				const char *name,
				unsigned long size,
				unsigned long flags,
				uint32_t tiling_mode,
				unsigned long stride,
				unsigned int alignment)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bufmgr;
	drm_intel_bo_gem *bo_gem;
	unsigned int page_size = getpagesize();
	int ret;
	struct drm_intel_gem_bo_bucket *bucket;
	bool alloc_from_cache;
	unsigned long bo_size;
	bool for_render = false;

	if (flags & BO_ALLOC_FOR_RENDER)
		for_render = true;

	/* Round the allocated size up to a power of two number of pages. */
	bucket = drm_intel_gem_bo_bucket_for_size(bufmgr_gem, size);

	/* If we don't have caching at this size, don't actually round the
	 * allocation up.
	 */
	if (bucket == NULL) {
		bo_size = size;
		if (bo_size < page_size)
			bo_size = page_size;
	} else {
		bo_size = bucket->size;
	}

	pthread_mutex_lock(&bufmgr_gem->lock);
	/* Get a buffer out of the cache if available */
retry:
	alloc_from_cache = false;
	if (bucket != NULL && !DRMLISTEMPTY(&bucket->head)) {
		if (for_render) {
			/* Allocate new render-target BOs from the tail (MRU)
			 * of the list, as it will likely be hot in the GPU
			 * cache and in the aperture for us.
			 */
			bo_gem = DRMLISTENTRY(drm_intel_bo_gem,
					      bucket->head.prev, head);
			DRMLISTDEL(&bo_gem->head);
			alloc_from_cache = true;
			bo_gem->bo.align = alignment;
		} else {
			assert(alignment == 0);
			/* For non-render-target BOs (where we're probably
			 * going to map it first thing in order to fill it
			 * with data), check if the last BO in the cache is
			 * unbusy, and only reuse in that case. Otherwise,
			 * allocating a new buffer is probably faster than
			 * waiting for the GPU to finish.
			 */
			bo_gem = DRMLISTENTRY(drm_intel_bo_gem,
					      bucket->head.next, head);
			if (!drm_intel_gem_bo_busy(&bo_gem->bo)) {
				alloc_from_cache = true;
				DRMLISTDEL(&bo_gem->head);
			}
		}

		if (alloc_from_cache) {
			if (!drm_intel_gem_bo_madvise_internal
			    (bufmgr_gem, bo_gem, I915_MADV_WILLNEED)) {
				drm_intel_gem_bo_free(&bo_gem->bo);
				drm_intel_gem_bo_cache_purge_bucket(bufmgr_gem,
								    bucket);
				goto retry;
			}

			if (drm_intel_gem_bo_set_tiling_internal(&bo_gem->bo,
								 tiling_mode,
								 stride)) {
				drm_intel_gem_bo_free(&bo_gem->bo);
				goto retry;
			}
		}
	}

	if (!alloc_from_cache) {
		struct drm_i915_gem_create create;

		bo_gem = calloc(1, sizeof(*bo_gem));
		if (!bo_gem)
			goto err;

		/* drm_intel_gem_bo_free calls DRMLISTDEL() for an uninitialized
		   list (vma_list), so better set the list head here */
		DRMINITLISTHEAD(&bo_gem->vma_list);

		bo_gem->bo.size = bo_size;

		memclear(create);
		create.size = bo_size;

		ret = drmIoctl(bufmgr_gem->fd,
			       DRM_IOCTL_I915_GEM_CREATE,
			       &create);
		if (ret != 0) {
			free(bo_gem);
			goto err;
		}

		bo_gem->gem_handle = create.handle;
		HASH_ADD(handle_hh, bufmgr_gem->handle_table,
			 gem_handle, sizeof(bo_gem->gem_handle),
			 bo_gem);

		bo_gem->bo.handle = bo_gem->gem_handle;
		bo_gem->bo.bufmgr = bufmgr;
		bo_gem->bo.align = alignment;

		bo_gem->tiling_mode = I915_TILING_NONE;
		bo_gem->swizzle_mode = I915_BIT_6_SWIZZLE_NONE;
		bo_gem->stride = 0;

		if (drm_intel_gem_bo_set_tiling_internal(&bo_gem->bo,
							 tiling_mode,
							 stride))
			goto err_free;
	}

	bo_gem->name = name;
	atomic_set(&bo_gem->refcount, 1);
	bo_gem->validate_index = -1;
	bo_gem->reloc_tree_fences = 0;
	bo_gem->used_as_reloc_target = false;
	bo_gem->has_error = false;
	bo_gem->reusable = true;

	drm_intel_bo_gem_set_in_aperture_size(bufmgr_gem, bo_gem, alignment);
	pthread_mutex_unlock(&bufmgr_gem->lock);

	DBG("bo_create: buf %d (%s) %ldb\n",
	    bo_gem->gem_handle, bo_gem->name, size);

	return &bo_gem->bo;

err_free:
	drm_intel_gem_bo_free(&bo_gem->bo);
err:
	pthread_mutex_unlock(&bufmgr_gem->lock);
	return NULL;
}

static drm_intel_bo *
drm_intel_gem_bo_alloc_for_render(drm_intel_bufmgr *bufmgr,
				  const char *name,
				  unsigned long size,
				  unsigned int alignment)
{
	return drm_intel_gem_bo_alloc_internal(bufmgr, name, size,
					       BO_ALLOC_FOR_RENDER,
					       I915_TILING_NONE, 0,
					       alignment);
}

static drm_intel_bo *
drm_intel_gem_bo_alloc(drm_intel_bufmgr *bufmgr,
		       const char *name,
		       unsigned long size,
		       unsigned int alignment)
{
	return drm_intel_gem_bo_alloc_internal(bufmgr, name, size, 0,
					       I915_TILING_NONE, 0, 0);
}

static drm_intel_bo *
drm_intel_gem_bo_alloc_tiled(drm_intel_bufmgr *bufmgr, const char *name,
			     int x, int y, int cpp, uint32_t *tiling_mode,
			     unsigned long *pitch, unsigned long flags)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bufmgr;
	unsigned long size, stride;
	uint32_t tiling;

	do {
		unsigned long aligned_y, height_alignment;

		tiling = *tiling_mode;

		/* If we're tiled, our allocations are in 8 or 32-row blocks,
		 * so failure to align our height means that we won't allocate
		 * enough pages.
		 *
		 * If we're untiled, we still have to align to 2 rows high
		 * because the data port accesses 2x2 blocks even if the
		 * bottom row isn't to be rendered, so failure to align means
		 * we could walk off the end of the GTT and fault.  This is
		 * documented on 965, and may be the case on older chipsets
		 * too so we try to be careful.
		 */
		aligned_y = y;
		height_alignment = 2;

		if ((bufmgr_gem->gen == 2) && tiling != I915_TILING_NONE)
			height_alignment = 16;
		else if (tiling == I915_TILING_X
			|| (IS_915(bufmgr_gem->pci_device)
			    && tiling == I915_TILING_Y))
			height_alignment = 8;
		else if (tiling == I915_TILING_Y)
			height_alignment = 32;
		aligned_y = ALIGN(y, height_alignment);

		stride = x * cpp;
		stride = drm_intel_gem_bo_tile_pitch(bufmgr_gem, stride, tiling_mode);
		size = stride * aligned_y;
		size = drm_intel_gem_bo_tile_size(bufmgr_gem, size, tiling_mode);
	} while (*tiling_mode != tiling);
	*pitch = stride;

	if (tiling == I915_TILING_NONE)
		stride = 0;

	return drm_intel_gem_bo_alloc_internal(bufmgr, name, size, flags,
					       tiling, stride, 0);
}

static drm_intel_bo *
drm_intel_gem_bo_alloc_userptr(drm_intel_bufmgr *bufmgr,
				const char *name,
				void *addr,
				uint32_t tiling_mode,
				uint32_t stride,
				unsigned long size,
				unsigned long flags)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bufmgr;
	drm_intel_bo_gem *bo_gem;
	int ret;
	struct drm_i915_gem_userptr userptr;

	/* Tiling with userptr surfaces is not supported
	 * on all hardware so refuse it for time being.
	 */
	if (tiling_mode != I915_TILING_NONE)
		return NULL;

	bo_gem = calloc(1, sizeof(*bo_gem));
	if (!bo_gem)
		return NULL;

	atomic_set(&bo_gem->refcount, 1);
	DRMINITLISTHEAD(&bo_gem->vma_list);

	bo_gem->bo.size = size;

	memclear(userptr);
	userptr.user_ptr = (__u64)((unsigned long)addr);
	userptr.user_size = size;
	userptr.flags = flags;

	ret = drmIoctl(bufmgr_gem->fd,
			DRM_IOCTL_I915_GEM_USERPTR,
			&userptr);
	if (ret != 0) {
		DBG("bo_create_userptr: "
		    "ioctl failed with user ptr %p size 0x%lx, "
		    "user flags 0x%lx\n", addr, size, flags);
		free(bo_gem);
		return NULL;
	}

	pthread_mutex_lock(&bufmgr_gem->lock);

	bo_gem->gem_handle = userptr.handle;
	bo_gem->bo.handle = bo_gem->gem_handle;
	bo_gem->bo.bufmgr    = bufmgr;
	bo_gem->is_userptr   = true;
	bo_gem->bo.virtual   = addr;
	/* Save the address provided by user */
	bo_gem->user_virtual = addr;
	bo_gem->tiling_mode  = I915_TILING_NONE;
	bo_gem->swizzle_mode = I915_BIT_6_SWIZZLE_NONE;
	bo_gem->stride       = 0;

	HASH_ADD(handle_hh, bufmgr_gem->handle_table,
		 gem_handle, sizeof(bo_gem->gem_handle),
		 bo_gem);

	bo_gem->name = name;
	bo_gem->validate_index = -1;
	bo_gem->reloc_tree_fences = 0;
	bo_gem->used_as_reloc_target = false;
	bo_gem->has_error = false;
	bo_gem->reusable = false;

	drm_intel_bo_gem_set_in_aperture_size(bufmgr_gem, bo_gem, 0);
	pthread_mutex_unlock(&bufmgr_gem->lock);

	DBG("bo_create_userptr: "
	    "ptr %p buf %d (%s) size %ldb, stride 0x%x, tile mode %d\n",
		addr, bo_gem->gem_handle, bo_gem->name,
		size, stride, tiling_mode);

	return &bo_gem->bo;
}

static bool
has_userptr(drm_intel_bufmgr_gem *bufmgr_gem)
{
	int ret;
	void *ptr;
	long pgsz;
	struct drm_i915_gem_userptr userptr;

	pgsz = sysconf(_SC_PAGESIZE);
	assert(pgsz > 0);

	ret = posix_memalign(&ptr, pgsz, pgsz);
	if (ret) {
		DBG("Failed to get a page (%ld) for userptr detection!\n",
			pgsz);
		return false;
	}

	memclear(userptr);
	userptr.user_ptr = (__u64)(unsigned long)ptr;
	userptr.user_size = pgsz;

retry:
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_USERPTR, &userptr);
	if (ret) {
		if (errno == ENODEV && userptr.flags == 0) {
			userptr.flags = I915_USERPTR_UNSYNCHRONIZED;
			goto retry;
		}
		free(ptr);
		return false;
	}

	/* We don't release the userptr bo here as we want to keep the
	 * kernel mm tracking alive for our lifetime. The first time we
	 * create a userptr object the kernel has to install a mmu_notifer
	 * which is a heavyweight operation (e.g. it requires taking all
	 * mm_locks and stop_machine()).
	 */

	bufmgr_gem->userptr_active.ptr = ptr;
	bufmgr_gem->userptr_active.handle = userptr.handle;

	return true;
}

static drm_intel_bo *
check_bo_alloc_userptr(drm_intel_bufmgr *bufmgr,
		       const char *name,
		       void *addr,
		       uint32_t tiling_mode,
		       uint32_t stride,
		       unsigned long size,
		       unsigned long flags)
{
	if (has_userptr((drm_intel_bufmgr_gem *)bufmgr))
		bufmgr->bo_alloc_userptr = drm_intel_gem_bo_alloc_userptr;
	else
		bufmgr->bo_alloc_userptr = NULL;

	return drm_intel_bo_alloc_userptr(bufmgr, name, addr,
					  tiling_mode, stride, size, flags);
}

/**
 * Returns a drm_intel_bo wrapping the given buffer object handle.
 *
 * This can be used when one application needs to pass a buffer object
 * to another.
 */
drm_intel_bo *
drm_intel_bo_gem_create_from_name(drm_intel_bufmgr *bufmgr,
				  const char *name,
				  unsigned int handle)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bufmgr;
	drm_intel_bo_gem *bo_gem;
	int ret;
	struct drm_gem_open open_arg;
	struct drm_i915_gem_get_tiling get_tiling;

	/* At the moment most applications only have a few named bo.
	 * For instance, in a DRI client only the render buffers passed
	 * between X and the client are named. And since X returns the
	 * alternating names for the front/back buffer a linear search
	 * provides a sufficiently fast match.
	 */
	pthread_mutex_lock(&bufmgr_gem->lock);
	HASH_FIND(name_hh, bufmgr_gem->name_table,
		  &handle, sizeof(handle), bo_gem);
	if (bo_gem) {
		drm_intel_gem_bo_reference(&bo_gem->bo);
		goto out;
	}

	memclear(open_arg);
	open_arg.name = handle;
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_GEM_OPEN,
		       &open_arg);
	if (ret != 0) {
		DBG("Couldn't reference %s handle 0x%08x: %s\n",
		    name, handle, strerror(errno));
		bo_gem = NULL;
		goto out;
	}
        /* Now see if someone has used a prime handle to get this
         * object from the kernel before by looking through the list
         * again for a matching gem_handle
         */
	HASH_FIND(handle_hh, bufmgr_gem->handle_table,
		  &open_arg.handle, sizeof(open_arg.handle), bo_gem);
	if (bo_gem) {
		drm_intel_gem_bo_reference(&bo_gem->bo);
		goto out;
	}

	bo_gem = calloc(1, sizeof(*bo_gem));
	if (!bo_gem)
		goto out;

	atomic_set(&bo_gem->refcount, 1);
	DRMINITLISTHEAD(&bo_gem->vma_list);

	bo_gem->bo.size = open_arg.size;
	bo_gem->bo.offset = 0;
	bo_gem->bo.offset64 = 0;
	bo_gem->bo.virtual = NULL;
	bo_gem->bo.bufmgr = bufmgr;
	bo_gem->name = name;
	bo_gem->validate_index = -1;
	bo_gem->gem_handle = open_arg.handle;
	bo_gem->bo.handle = open_arg.handle;
	bo_gem->global_name = handle;
	bo_gem->reusable = false;

	HASH_ADD(handle_hh, bufmgr_gem->handle_table,
		 gem_handle, sizeof(bo_gem->gem_handle), bo_gem);
	HASH_ADD(name_hh, bufmgr_gem->name_table,
		 global_name, sizeof(bo_gem->global_name), bo_gem);

	memclear(get_tiling);
	get_tiling.handle = bo_gem->gem_handle;
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_GET_TILING,
		       &get_tiling);
	if (ret != 0)
		goto err_unref;

	bo_gem->tiling_mode = get_tiling.tiling_mode;
	bo_gem->swizzle_mode = get_tiling.swizzle_mode;
	/* XXX stride is unknown */
	drm_intel_bo_gem_set_in_aperture_size(bufmgr_gem, bo_gem, 0);
	DBG("bo_create_from_handle: %d (%s)\n", handle, bo_gem->name);

out:
	pthread_mutex_unlock(&bufmgr_gem->lock);
	return &bo_gem->bo;

err_unref:
	drm_intel_gem_bo_free(&bo_gem->bo);
	pthread_mutex_unlock(&bufmgr_gem->lock);
	return NULL;
}

static void
drm_intel_gem_bo_free(drm_intel_bo *bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_gem_close close;
	int ret;

	DRMLISTDEL(&bo_gem->vma_list);
	if (bo_gem->mem_virtual) {
		VG(VALGRIND_FREELIKE_BLOCK(bo_gem->mem_virtual, 0));
		drm_munmap(bo_gem->mem_virtual, bo_gem->bo.size);
		bufmgr_gem->vma_count--;
	}
	if (bo_gem->wc_virtual) {
		VG(VALGRIND_FREELIKE_BLOCK(bo_gem->wc_virtual, 0));
		drm_munmap(bo_gem->wc_virtual, bo_gem->bo.size);
		bufmgr_gem->vma_count--;
	}
	if (bo_gem->gtt_virtual) {
		drm_munmap(bo_gem->gtt_virtual, bo_gem->bo.size);
		bufmgr_gem->vma_count--;
	}

	if (bo_gem->global_name)
		HASH_DELETE(name_hh, bufmgr_gem->name_table, bo_gem);
	HASH_DELETE(handle_hh, bufmgr_gem->handle_table, bo_gem);

	/* Close this object */
	memclear(close);
	close.handle = bo_gem->gem_handle;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_GEM_CLOSE, &close);
	if (ret != 0) {
		DBG("DRM_IOCTL_GEM_CLOSE %d failed (%s): %s\n",
		    bo_gem->gem_handle, bo_gem->name, strerror(errno));
	}
	free(bo);
}

static void
drm_intel_gem_bo_mark_mmaps_incoherent(drm_intel_bo *bo)
{
#if HAVE_VALGRIND
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	if (bo_gem->mem_virtual)
		VALGRIND_MAKE_MEM_NOACCESS(bo_gem->mem_virtual, bo->size);

	if (bo_gem->wc_virtual)
		VALGRIND_MAKE_MEM_NOACCESS(bo_gem->wc_virtual, bo->size);

	if (bo_gem->gtt_virtual)
		VALGRIND_MAKE_MEM_NOACCESS(bo_gem->gtt_virtual, bo->size);
#endif
}

/** Frees all cached buffers significantly older than @@time. */
static void
drm_intel_gem_cleanup_bo_cache(drm_intel_bufmgr_gem *bufmgr_gem, time_t time)
{
	int i;

	if (bufmgr_gem->time == time)
		return;

	for (i = 0; i < bufmgr_gem->num_buckets; i++) {
		struct drm_intel_gem_bo_bucket *bucket =
		    &bufmgr_gem->cache_bucket[i];

		while (!DRMLISTEMPTY(&bucket->head)) {
			drm_intel_bo_gem *bo_gem;

			bo_gem = DRMLISTENTRY(drm_intel_bo_gem,
					      bucket->head.next, head);
			if (time - bo_gem->free_time <= 1)
				break;

			DRMLISTDEL(&bo_gem->head);

			drm_intel_gem_bo_free(&bo_gem->bo);
		}
	}

	bufmgr_gem->time = time;
}

static void drm_intel_gem_bo_purge_vma_cache(drm_intel_bufmgr_gem *bufmgr_gem)
{
	int limit;

	DBG("%s: cached=%d, open=%d, limit=%d\n", __FUNCTION__,
	    bufmgr_gem->vma_count, bufmgr_gem->vma_open, bufmgr_gem->vma_max);

	if (bufmgr_gem->vma_max < 0)
		return;

	/* We may need to evict a few entries in order to create new mmaps */
	limit = bufmgr_gem->vma_max - 2*bufmgr_gem->vma_open;
	if (limit < 0)
		limit = 0;

	while (bufmgr_gem->vma_count > limit) {
		drm_intel_bo_gem *bo_gem;

		bo_gem = DRMLISTENTRY(drm_intel_bo_gem,
				      bufmgr_gem->vma_cache.next,
				      vma_list);
		assert(bo_gem->map_count == 0);
		DRMLISTDELINIT(&bo_gem->vma_list);

		if (bo_gem->mem_virtual) {
			drm_munmap(bo_gem->mem_virtual, bo_gem->bo.size);
			bo_gem->mem_virtual = NULL;
			bufmgr_gem->vma_count--;
		}
		if (bo_gem->wc_virtual) {
			drm_munmap(bo_gem->wc_virtual, bo_gem->bo.size);
			bo_gem->wc_virtual = NULL;
			bufmgr_gem->vma_count--;
		}
		if (bo_gem->gtt_virtual) {
			drm_munmap(bo_gem->gtt_virtual, bo_gem->bo.size);
			bo_gem->gtt_virtual = NULL;
			bufmgr_gem->vma_count--;
		}
	}
}

static void drm_intel_gem_bo_close_vma(drm_intel_bufmgr_gem *bufmgr_gem,
				       drm_intel_bo_gem *bo_gem)
{
	bufmgr_gem->vma_open--;
	DRMLISTADDTAIL(&bo_gem->vma_list, &bufmgr_gem->vma_cache);
	if (bo_gem->mem_virtual)
		bufmgr_gem->vma_count++;
	if (bo_gem->wc_virtual)
		bufmgr_gem->vma_count++;
	if (bo_gem->gtt_virtual)
		bufmgr_gem->vma_count++;
	drm_intel_gem_bo_purge_vma_cache(bufmgr_gem);
}

static void drm_intel_gem_bo_open_vma(drm_intel_bufmgr_gem *bufmgr_gem,
				      drm_intel_bo_gem *bo_gem)
{
	bufmgr_gem->vma_open++;
	DRMLISTDEL(&bo_gem->vma_list);
	if (bo_gem->mem_virtual)
		bufmgr_gem->vma_count--;
	if (bo_gem->wc_virtual)
		bufmgr_gem->vma_count--;
	if (bo_gem->gtt_virtual)
		bufmgr_gem->vma_count--;
	drm_intel_gem_bo_purge_vma_cache(bufmgr_gem);
}

static void
drm_intel_gem_bo_unreference_final(drm_intel_bo *bo, time_t time)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_intel_gem_bo_bucket *bucket;
	int i;

	/* Unreference all the target buffers */
	for (i = 0; i < bo_gem->reloc_count; i++) {
		if (bo_gem->reloc_target_info[i].bo != bo) {
			drm_intel_gem_bo_unreference_locked_timed(bo_gem->
								  reloc_target_info[i].bo,
								  time);
		}
	}
	for (i = 0; i < bo_gem->softpin_target_count; i++)
		drm_intel_gem_bo_unreference_locked_timed(bo_gem->softpin_target[i],
								  time);
	bo_gem->kflags = 0;
	bo_gem->reloc_count = 0;
	bo_gem->used_as_reloc_target = false;
	bo_gem->softpin_target_count = 0;

	DBG("bo_unreference final: %d (%s)\n",
	    bo_gem->gem_handle, bo_gem->name);

	/* release memory associated with this object */
	if (bo_gem->reloc_target_info) {
		free(bo_gem->reloc_target_info);
		bo_gem->reloc_target_info = NULL;
	}
	if (bo_gem->relocs) {
		free(bo_gem->relocs);
		bo_gem->relocs = NULL;
	}
	if (bo_gem->softpin_target) {
		free(bo_gem->softpin_target);
		bo_gem->softpin_target = NULL;
		bo_gem->softpin_target_size = 0;
	}

	/* Clear any left-over mappings */
	if (bo_gem->map_count) {
		DBG("bo freed with non-zero map-count %d\n", bo_gem->map_count);
		bo_gem->map_count = 0;
		drm_intel_gem_bo_close_vma(bufmgr_gem, bo_gem);
		drm_intel_gem_bo_mark_mmaps_incoherent(bo);
	}

	bucket = drm_intel_gem_bo_bucket_for_size(bufmgr_gem, bo->size);
	/* Put the buffer into our internal cache for reuse if we can. */
	if (bufmgr_gem->bo_reuse && bo_gem->reusable && bucket != NULL &&
	    drm_intel_gem_bo_madvise_internal(bufmgr_gem, bo_gem,
					      I915_MADV_DONTNEED)) {
		bo_gem->free_time = time;

		bo_gem->name = NULL;
		bo_gem->validate_index = -1;

		DRMLISTADDTAIL(&bo_gem->head, &bucket->head);
	} else {
		drm_intel_gem_bo_free(bo);
	}
}

static void drm_intel_gem_bo_unreference_locked_timed(drm_intel_bo *bo,
						      time_t time)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	assert(atomic_read(&bo_gem->refcount) > 0);
	if (atomic_dec_and_test(&bo_gem->refcount))
		drm_intel_gem_bo_unreference_final(bo, time);
}

static void drm_intel_gem_bo_unreference(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	assert(atomic_read(&bo_gem->refcount) > 0);

	if (atomic_add_unless(&bo_gem->refcount, -1, 1)) {
		drm_intel_bufmgr_gem *bufmgr_gem =
		    (drm_intel_bufmgr_gem *) bo->bufmgr;
		struct timespec time;

		clock_gettime(CLOCK_MONOTONIC, &time);

		pthread_mutex_lock(&bufmgr_gem->lock);

		if (atomic_dec_and_test(&bo_gem->refcount)) {
			drm_intel_gem_bo_unreference_final(bo, time.tv_sec);
			drm_intel_gem_cleanup_bo_cache(bufmgr_gem, time.tv_sec);
		}

		pthread_mutex_unlock(&bufmgr_gem->lock);
	}
}

static int drm_intel_gem_bo_map(drm_intel_bo *bo, int write_enable)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_set_domain set_domain;
	int ret;

	if (bo_gem->is_userptr) {
		/* Return the same user ptr */
		bo->virtual = bo_gem->user_virtual;
		return 0;
	}

	pthread_mutex_lock(&bufmgr_gem->lock);

	if (bo_gem->map_count++ == 0)
		drm_intel_gem_bo_open_vma(bufmgr_gem, bo_gem);

	if (!bo_gem->mem_virtual) {
		struct drm_i915_gem_mmap mmap_arg;

		DBG("bo_map: %d (%s), map_count=%d\n",
		    bo_gem->gem_handle, bo_gem->name, bo_gem->map_count);

		memclear(mmap_arg);
		mmap_arg.handle = bo_gem->gem_handle;
		mmap_arg.size = bo->size;
		ret = drmIoctl(bufmgr_gem->fd,
			       DRM_IOCTL_I915_GEM_MMAP,
			       &mmap_arg);
		if (ret != 0) {
			ret = -errno;
			DBG("%s:%d: Error mapping buffer %d (%s): %s .\n",
			    __FILE__, __LINE__, bo_gem->gem_handle,
			    bo_gem->name, strerror(errno));
			if (--bo_gem->map_count == 0)
				drm_intel_gem_bo_close_vma(bufmgr_gem, bo_gem);
			pthread_mutex_unlock(&bufmgr_gem->lock);
			return ret;
		}
		VG(VALGRIND_MALLOCLIKE_BLOCK(mmap_arg.addr_ptr, mmap_arg.size, 0, 1));
		bo_gem->mem_virtual = (void *)(uintptr_t) mmap_arg.addr_ptr;
	}
	DBG("bo_map: %d (%s) -> %p\n", bo_gem->gem_handle, bo_gem->name,
	    bo_gem->mem_virtual);
	bo->virtual = bo_gem->mem_virtual;

	memclear(set_domain);
	set_domain.handle = bo_gem->gem_handle;
	set_domain.read_domains = I915_GEM_DOMAIN_CPU;
	if (write_enable)
		set_domain.write_domain = I915_GEM_DOMAIN_CPU;
	else
		set_domain.write_domain = 0;
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_SET_DOMAIN,
		       &set_domain);
	if (ret != 0) {
		DBG("%s:%d: Error setting to CPU domain %d: %s\n",
		    __FILE__, __LINE__, bo_gem->gem_handle,
		    strerror(errno));
	}

	if (write_enable)
		bo_gem->mapped_cpu_write = true;

	drm_intel_gem_bo_mark_mmaps_incoherent(bo);
	VG(VALGRIND_MAKE_MEM_DEFINED(bo_gem->mem_virtual, bo->size));
	pthread_mutex_unlock(&bufmgr_gem->lock);

	return 0;
}

static int
map_gtt(drm_intel_bo *bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	int ret;

	if (bo_gem->is_userptr)
		return -EINVAL;

	if (bo_gem->map_count++ == 0)
		drm_intel_gem_bo_open_vma(bufmgr_gem, bo_gem);

	/* Get a mapping of the buffer if we haven't before. */
	if (bo_gem->gtt_virtual == NULL) {
		struct drm_i915_gem_mmap_gtt mmap_arg;

		DBG("bo_map_gtt: mmap %d (%s), map_count=%d\n",
		    bo_gem->gem_handle, bo_gem->name, bo_gem->map_count);

		memclear(mmap_arg);
		mmap_arg.handle = bo_gem->gem_handle;

		/* Get the fake offset back... */
		ret = drmIoctl(bufmgr_gem->fd,
			       DRM_IOCTL_I915_GEM_MMAP_GTT,
			       &mmap_arg);
		if (ret != 0) {
			ret = -errno;
			DBG("%s:%d: Error preparing buffer map %d (%s): %s .\n",
			    __FILE__, __LINE__,
			    bo_gem->gem_handle, bo_gem->name,
			    strerror(errno));
			if (--bo_gem->map_count == 0)
				drm_intel_gem_bo_close_vma(bufmgr_gem, bo_gem);
			return ret;
		}

		/* and mmap it */
		bo_gem->gtt_virtual = drm_mmap(0, bo->size, PROT_READ | PROT_WRITE,
					       MAP_SHARED, bufmgr_gem->fd,
					       mmap_arg.offset);
		if (bo_gem->gtt_virtual == MAP_FAILED) {
			bo_gem->gtt_virtual = NULL;
			ret = -errno;
			DBG("%s:%d: Error mapping buffer %d (%s): %s .\n",
			    __FILE__, __LINE__,
			    bo_gem->gem_handle, bo_gem->name,
			    strerror(errno));
			if (--bo_gem->map_count == 0)
				drm_intel_gem_bo_close_vma(bufmgr_gem, bo_gem);
			return ret;
		}
	}

	bo->virtual = bo_gem->gtt_virtual;

	DBG("bo_map_gtt: %d (%s) -> %p\n", bo_gem->gem_handle, bo_gem->name,
	    bo_gem->gtt_virtual);

	return 0;
}

int
drm_intel_gem_bo_map_gtt(drm_intel_bo *bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_set_domain set_domain;
	int ret;

	pthread_mutex_lock(&bufmgr_gem->lock);

	ret = map_gtt(bo);
	if (ret) {
		pthread_mutex_unlock(&bufmgr_gem->lock);
		return ret;
	}

	/* Now move it to the GTT domain so that the GPU and CPU
	 * caches are flushed and the GPU isn't actively using the
	 * buffer.
	 *
	 * The pagefault handler does this domain change for us when
	 * it has unbound the BO from the GTT, but it's up to us to
	 * tell it when we're about to use things if we had done
	 * rendering and it still happens to be bound to the GTT.
	 */
	memclear(set_domain);
	set_domain.handle = bo_gem->gem_handle;
	set_domain.read_domains = I915_GEM_DOMAIN_GTT;
	set_domain.write_domain = I915_GEM_DOMAIN_GTT;
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_SET_DOMAIN,
		       &set_domain);
	if (ret != 0) {
		DBG("%s:%d: Error setting domain %d: %s\n",
		    __FILE__, __LINE__, bo_gem->gem_handle,
		    strerror(errno));
	}

	drm_intel_gem_bo_mark_mmaps_incoherent(bo);
	VG(VALGRIND_MAKE_MEM_DEFINED(bo_gem->gtt_virtual, bo->size));
	pthread_mutex_unlock(&bufmgr_gem->lock);

	return 0;
}

/**
 * Performs a mapping of the buffer object like the normal GTT
 * mapping, but avoids waiting for the GPU to be done reading from or
 * rendering to the buffer.
 *
 * This is used in the implementation of GL_ARB_map_buffer_range: The
 * user asks to create a buffer, then does a mapping, fills some
 * space, runs a drawing command, then asks to map it again without
 * synchronizing because it guarantees that it won't write over the
 * data that the GPU is busy using (or, more specifically, that if it
 * does write over the data, it acknowledges that rendering is
 * undefined).
 */

int
drm_intel_gem_bo_map_unsynchronized(drm_intel_bo *bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
#ifdef HAVE_VALGRIND
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
#endif
	int ret;

	/* If the CPU cache isn't coherent with the GTT, then use a
	 * regular synchronized mapping.  The problem is that we don't
	 * track where the buffer was last used on the CPU side in
	 * terms of drm_intel_bo_map vs drm_intel_gem_bo_map_gtt, so
	 * we would potentially corrupt the buffer even when the user
	 * does reasonable things.
	 */
	if (!bufmgr_gem->has_llc)
		return drm_intel_gem_bo_map_gtt(bo);

	pthread_mutex_lock(&bufmgr_gem->lock);

	ret = map_gtt(bo);
	if (ret == 0) {
		drm_intel_gem_bo_mark_mmaps_incoherent(bo);
		VG(VALGRIND_MAKE_MEM_DEFINED(bo_gem->gtt_virtual, bo->size));
	}

	pthread_mutex_unlock(&bufmgr_gem->lock);

	return ret;
}

static int drm_intel_gem_bo_unmap(drm_intel_bo *bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	int ret = 0;

	if (bo == NULL)
		return 0;

	if (bo_gem->is_userptr)
		return 0;

	bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;

	pthread_mutex_lock(&bufmgr_gem->lock);

	if (bo_gem->map_count <= 0) {
		DBG("attempted to unmap an unmapped bo\n");
		pthread_mutex_unlock(&bufmgr_gem->lock);
		/* Preserve the old behaviour of just treating this as a
		 * no-op rather than reporting the error.
		 */
		return 0;
	}

	if (bo_gem->mapped_cpu_write) {
		struct drm_i915_gem_sw_finish sw_finish;

		/* Cause a flush to happen if the buffer's pinned for
		 * scanout, so the results show up in a timely manner.
		 * Unlike GTT set domains, this only does work if the
		 * buffer should be scanout-related.
		 */
		memclear(sw_finish);
		sw_finish.handle = bo_gem->gem_handle;
		ret = drmIoctl(bufmgr_gem->fd,
			       DRM_IOCTL_I915_GEM_SW_FINISH,
			       &sw_finish);
		ret = ret == -1 ? -errno : 0;

		bo_gem->mapped_cpu_write = false;
	}

	/* We need to unmap after every innovation as we cannot track
	 * an open vma for every bo as that will exhaust the system
	 * limits and cause later failures.
	 */
	if (--bo_gem->map_count == 0) {
		drm_intel_gem_bo_close_vma(bufmgr_gem, bo_gem);
		drm_intel_gem_bo_mark_mmaps_incoherent(bo);
		bo->virtual = NULL;
	}
	pthread_mutex_unlock(&bufmgr_gem->lock);

	return ret;
}

int
drm_intel_gem_bo_unmap_gtt(drm_intel_bo *bo)
{
	return drm_intel_gem_bo_unmap(bo);
}

static int
drm_intel_gem_bo_subdata(drm_intel_bo *bo, unsigned long offset,
			 unsigned long size, const void *data)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_pwrite pwrite;
	int ret;

	if (bo_gem->is_userptr)
		return -EINVAL;

	memclear(pwrite);
	pwrite.handle = bo_gem->gem_handle;
	pwrite.offset = offset;
	pwrite.size = size;
	pwrite.data_ptr = (uint64_t) (uintptr_t) data;
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_PWRITE,
		       &pwrite);
	if (ret != 0) {
		ret = -errno;
		DBG("%s:%d: Error writing data to buffer %d: (%d %d) %s .\n",
		    __FILE__, __LINE__, bo_gem->gem_handle, (int)offset,
		    (int)size, strerror(errno));
	}

	return ret;
}

static int
drm_intel_gem_get_pipe_from_crtc_id(drm_intel_bufmgr *bufmgr, int crtc_id)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bufmgr;
	struct drm_i915_get_pipe_from_crtc_id get_pipe_from_crtc_id;
	int ret;

	memclear(get_pipe_from_crtc_id);
	get_pipe_from_crtc_id.crtc_id = crtc_id;
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GET_PIPE_FROM_CRTC_ID,
		       &get_pipe_from_crtc_id);
	if (ret != 0) {
		/* We return -1 here to signal that we don't
		 * know which pipe is associated with this crtc.
		 * This lets the caller know that this information
		 * isn't available; using the wrong pipe for
		 * vblank waiting can cause the chipset to lock up
		 */
		return -1;
	}

	return get_pipe_from_crtc_id.pipe;
}

static int
drm_intel_gem_bo_get_subdata(drm_intel_bo *bo, unsigned long offset,
			     unsigned long size, void *data)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_pread pread;
	int ret;

	if (bo_gem->is_userptr)
		return -EINVAL;

	memclear(pread);
	pread.handle = bo_gem->gem_handle;
	pread.offset = offset;
	pread.size = size;
	pread.data_ptr = (uint64_t) (uintptr_t) data;
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_PREAD,
		       &pread);
	if (ret != 0) {
		ret = -errno;
		DBG("%s:%d: Error reading data from buffer %d: (%d %d) %s .\n",
		    __FILE__, __LINE__, bo_gem->gem_handle, (int)offset,
		    (int)size, strerror(errno));
	}

	return ret;
}

/** Waits for all GPU rendering with the object to have completed. */
static void
drm_intel_gem_bo_wait_rendering(drm_intel_bo *bo)
{
	drm_intel_gem_bo_start_gtt_access(bo, 1);
}

/**
 * Waits on a BO for the given amount of time.
 *
 * @@bo: buffer object to wait for
 * @@timeout_ns: amount of time to wait in nanoseconds.
 *   If value is less than 0, an infinite wait will occur.
 *
 * Returns 0 if the wait was successful ie. the last batch referencing the
 * object has completed within the allotted time. Otherwise some negative return
 * value describes the error. Of particular interest is -ETIME when the wait has
 * failed to yield the desired result.
 *
 * Similar to drm_intel_gem_bo_wait_rendering except a timeout parameter allows
 * the operation to give up after a certain amount of time. Another subtle
 * difference is the internal locking semantics are different (this variant does
 * not hold the lock for the duration of the wait). This makes the wait subject
 * to a larger userspace race window.
 *
 * The implementation shall wait until the object is no longer actively
 * referenced within a batch buffer at the time of the call. The wait will
 * not guarantee that the buffer is re-issued via another thread, or an flinked
 * handle. Userspace must make sure this race does not occur if such precision
 * is important.
 *
 * Note that some kernels have broken the inifite wait for negative values
 * promise, upgrade to latest stable kernels if this is the case.
 */
int
drm_intel_gem_bo_wait(drm_intel_bo *bo, int64_t timeout_ns)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_wait wait;
	int ret;

	if (!bufmgr_gem->has_wait_timeout) {
		DBG("%s:%d: Timed wait is not supported. Falling back to "
		    "infinite wait\n", __FILE__, __LINE__);
		if (timeout_ns) {
			drm_intel_gem_bo_wait_rendering(bo);
			return 0;
		} else {
			return drm_intel_gem_bo_busy(bo) ? -ETIME : 0;
		}
	}

	memclear(wait);
	wait.bo_handle = bo_gem->gem_handle;
	wait.timeout_ns = timeout_ns;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_WAIT, &wait);
	if (ret == -1)
		return -errno;

	return ret;
}

/**
 * Sets the object to the GTT read and possibly write domain, used by the X
 * 2D driver in the absence of kernel support to do drm_intel_gem_bo_map_gtt().
 *
 * In combination with drm_intel_gem_bo_pin() and manual fence management, we
 * can do tiled pixmaps this way.
 */
void
drm_intel_gem_bo_start_gtt_access(drm_intel_bo *bo, int write_enable)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_set_domain set_domain;
	int ret;

	memclear(set_domain);
	set_domain.handle = bo_gem->gem_handle;
	set_domain.read_domains = I915_GEM_DOMAIN_GTT;
	set_domain.write_domain = write_enable ? I915_GEM_DOMAIN_GTT : 0;
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_SET_DOMAIN,
		       &set_domain);
	if (ret != 0) {
		DBG("%s:%d: Error setting memory domains %d (%08x %08x): %s .\n",
		    __FILE__, __LINE__, bo_gem->gem_handle,
		    set_domain.read_domains, set_domain.write_domain,
		    strerror(errno));
	}
}

static void
drm_intel_bufmgr_gem_destroy(drm_intel_bufmgr *bufmgr)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bufmgr;
	struct drm_gem_close close_bo;
	int i, ret;

	free(bufmgr_gem->exec2_objects);
#ifndef __OpenBSD__
	free(bufmgr_gem->exec_objects);
#endif
	free(bufmgr_gem->exec_bos);

	pthread_mutex_destroy(&bufmgr_gem->lock);

	/* Free any cached buffer objects we were going to reuse */
	for (i = 0; i < bufmgr_gem->num_buckets; i++) {
		struct drm_intel_gem_bo_bucket *bucket =
		    &bufmgr_gem->cache_bucket[i];
		drm_intel_bo_gem *bo_gem;

		while (!DRMLISTEMPTY(&bucket->head)) {
			bo_gem = DRMLISTENTRY(drm_intel_bo_gem,
					      bucket->head.next, head);
			DRMLISTDEL(&bo_gem->head);

			drm_intel_gem_bo_free(&bo_gem->bo);
		}
	}

	/* Release userptr bo kept hanging around for optimisation. */
	if (bufmgr_gem->userptr_active.ptr) {
		memclear(close_bo);
		close_bo.handle = bufmgr_gem->userptr_active.handle;
		ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_GEM_CLOSE, &close_bo);
		free(bufmgr_gem->userptr_active.ptr);
		if (ret)
			fprintf(stderr,
				"Failed to release test userptr object! (%d) "
				"i915 kernel driver may not be sane!\n", errno);
	}

	free(bufmgr);
}

/**
 * Adds the target buffer to the validation list and adds the relocation
 * to the reloc_buffer's relocation list.
 *
 * The relocation entry at the given offset must already contain the
 * precomputed relocation value, because the kernel will optimize out
 * the relocation entry write when the buffer hasn't moved from the
 * last known offset in target_bo.
 */
static int
do_bo_emit_reloc(drm_intel_bo *bo, uint32_t offset,
		 drm_intel_bo *target_bo, uint32_t target_offset,
		 uint32_t read_domains, uint32_t write_domain,
		 bool need_fence)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	drm_intel_bo_gem *target_bo_gem = (drm_intel_bo_gem *) target_bo;
	bool fenced_command;

	if (bo_gem->has_error)
		return -ENOMEM;

	if (target_bo_gem->has_error) {
		bo_gem->has_error = true;
		return -ENOMEM;
	}

	/* We never use HW fences for rendering on 965+ */
	if (bufmgr_gem->gen >= 4)
		need_fence = false;

	fenced_command = need_fence;
	if (target_bo_gem->tiling_mode == I915_TILING_NONE)
		need_fence = false;

	/* Create a new relocation list if needed */
	if (bo_gem->relocs == NULL && drm_intel_setup_reloc_list(bo))
		return -ENOMEM;

	/* Check overflow */
	assert(bo_gem->reloc_count < bufmgr_gem->max_relocs);

	/* Check args */
	assert(offset <= bo->size - 4);
	assert((write_domain & (write_domain - 1)) == 0);

	/* An object needing a fence is a tiled buffer, so it won't have
	 * relocs to other buffers.
	 */
	if (need_fence) {
		assert(target_bo_gem->reloc_count == 0);
		target_bo_gem->reloc_tree_fences = 1;
	}

	/* Make sure that we're not adding a reloc to something whose size has
	 * already been accounted for.
	 */
	assert(!bo_gem->used_as_reloc_target);
	if (target_bo_gem != bo_gem) {
		target_bo_gem->used_as_reloc_target = true;
		bo_gem->reloc_tree_size += target_bo_gem->reloc_tree_size;
		bo_gem->reloc_tree_fences += target_bo_gem->reloc_tree_fences;
	}

	bo_gem->reloc_target_info[bo_gem->reloc_count].bo = target_bo;
	if (target_bo != bo)
		drm_intel_gem_bo_reference(target_bo);
	if (fenced_command)
		bo_gem->reloc_target_info[bo_gem->reloc_count].flags =
			DRM_INTEL_RELOC_FENCE;
	else
		bo_gem->reloc_target_info[bo_gem->reloc_count].flags = 0;

	bo_gem->relocs[bo_gem->reloc_count].offset = offset;
	bo_gem->relocs[bo_gem->reloc_count].delta = target_offset;
	bo_gem->relocs[bo_gem->reloc_count].target_handle =
	    target_bo_gem->gem_handle;
	bo_gem->relocs[bo_gem->reloc_count].read_domains = read_domains;
	bo_gem->relocs[bo_gem->reloc_count].write_domain = write_domain;
	bo_gem->relocs[bo_gem->reloc_count].presumed_offset = target_bo->offset64;
	bo_gem->reloc_count++;

	return 0;
}

static void
drm_intel_gem_bo_use_48b_address_range(drm_intel_bo *bo, uint32_t enable)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	if (enable)
		bo_gem->kflags |= EXEC_OBJECT_SUPPORTS_48B_ADDRESS;
	else
		bo_gem->kflags &= ~EXEC_OBJECT_SUPPORTS_48B_ADDRESS;
}

static int
drm_intel_gem_bo_add_softpin_target(drm_intel_bo *bo, drm_intel_bo *target_bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	drm_intel_bo_gem *target_bo_gem = (drm_intel_bo_gem *) target_bo;
	if (bo_gem->has_error)
		return -ENOMEM;

	if (target_bo_gem->has_error) {
		bo_gem->has_error = true;
		return -ENOMEM;
	}

	if (!(target_bo_gem->kflags & EXEC_OBJECT_PINNED))
		return -EINVAL;
	if (target_bo_gem == bo_gem)
		return -EINVAL;

	if (bo_gem->softpin_target_count == bo_gem->softpin_target_size) {
		int new_size = bo_gem->softpin_target_size * 2;
		if (new_size == 0)
			new_size = bufmgr_gem->max_relocs;

		bo_gem->softpin_target = realloc(bo_gem->softpin_target, new_size *
				sizeof(drm_intel_bo *));
		if (!bo_gem->softpin_target)
			return -ENOMEM;

		bo_gem->softpin_target_size = new_size;
	}
	bo_gem->softpin_target[bo_gem->softpin_target_count] = target_bo;
	drm_intel_gem_bo_reference(target_bo);
	bo_gem->softpin_target_count++;

	return 0;
}

static int
drm_intel_gem_bo_emit_reloc(drm_intel_bo *bo, uint32_t offset,
			    drm_intel_bo *target_bo, uint32_t target_offset,
			    uint32_t read_domains, uint32_t write_domain)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
	drm_intel_bo_gem *target_bo_gem = (drm_intel_bo_gem *)target_bo;

	if (target_bo_gem->kflags & EXEC_OBJECT_PINNED)
		return drm_intel_gem_bo_add_softpin_target(bo, target_bo);
	else
		return do_bo_emit_reloc(bo, offset, target_bo, target_offset,
					read_domains, write_domain,
					!bufmgr_gem->fenced_relocs);
}

static int
drm_intel_gem_bo_emit_reloc_fence(drm_intel_bo *bo, uint32_t offset,
				  drm_intel_bo *target_bo,
				  uint32_t target_offset,
				  uint32_t read_domains, uint32_t write_domain)
{
	return do_bo_emit_reloc(bo, offset, target_bo, target_offset,
				read_domains, write_domain, true);
}

int
drm_intel_gem_bo_get_reloc_count(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	return bo_gem->reloc_count;
}

/**
 * Removes existing relocation entries in the BO after "start".
 *
 * This allows a user to avoid a two-step process for state setup with
 * counting up all the buffer objects and doing a
 * drm_intel_bufmgr_check_aperture_space() before emitting any of the
 * relocations for the state setup.  Instead, save the state of the
 * batchbuffer including drm_intel_gem_get_reloc_count(), emit all the
 * state, and then check if it still fits in the aperture.
 *
 * Any further drm_intel_bufmgr_check_aperture_space() queries
 * involving this buffer in the tree are undefined after this call.
 *
 * This also removes all softpinned targets being referenced by the BO.
 */
void
drm_intel_gem_bo_clear_relocs(drm_intel_bo *bo, int start)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	int i;
	struct timespec time;

	clock_gettime(CLOCK_MONOTONIC, &time);

	assert(bo_gem->reloc_count >= start);

	/* Unreference the cleared target buffers */
	pthread_mutex_lock(&bufmgr_gem->lock);

	for (i = start; i < bo_gem->reloc_count; i++) {
		drm_intel_bo_gem *target_bo_gem = (drm_intel_bo_gem *) bo_gem->reloc_target_info[i].bo;
		if (&target_bo_gem->bo != bo) {
			bo_gem->reloc_tree_fences -= target_bo_gem->reloc_tree_fences;
			drm_intel_gem_bo_unreference_locked_timed(&target_bo_gem->bo,
								  time.tv_sec);
		}
	}
	bo_gem->reloc_count = start;

	for (i = 0; i < bo_gem->softpin_target_count; i++) {
		drm_intel_bo_gem *target_bo_gem = (drm_intel_bo_gem *) bo_gem->softpin_target[i];
		drm_intel_gem_bo_unreference_locked_timed(&target_bo_gem->bo, time.tv_sec);
	}
	bo_gem->softpin_target_count = 0;

	pthread_mutex_unlock(&bufmgr_gem->lock);

}

#ifndef __OpenBSD__
/**
 * Walk the tree of relocations rooted at BO and accumulate the list of
 * validations to be performed and update the relocation buffers with
 * index values into the validation list.
 */
static void
drm_intel_gem_bo_process_reloc(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	int i;

	if (bo_gem->relocs == NULL)
		return;

	for (i = 0; i < bo_gem->reloc_count; i++) {
		drm_intel_bo *target_bo = bo_gem->reloc_target_info[i].bo;

		if (target_bo == bo)
			continue;

		drm_intel_gem_bo_mark_mmaps_incoherent(bo);

		/* Continue walking the tree depth-first. */
		drm_intel_gem_bo_process_reloc(target_bo);

		/* Add the target to the validate list */
		drm_intel_add_validate_buffer(target_bo);
	}
}
#endif

static void
drm_intel_gem_bo_process_reloc2(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
	int i;

	if (bo_gem->relocs == NULL && bo_gem->softpin_target == NULL)
		return;

	for (i = 0; i < bo_gem->reloc_count; i++) {
		drm_intel_bo *target_bo = bo_gem->reloc_target_info[i].bo;
		int need_fence;

		if (target_bo == bo)
			continue;

		drm_intel_gem_bo_mark_mmaps_incoherent(bo);

		/* Continue walking the tree depth-first. */
		drm_intel_gem_bo_process_reloc2(target_bo);

		need_fence = (bo_gem->reloc_target_info[i].flags &
			      DRM_INTEL_RELOC_FENCE);

		/* Add the target to the validate list */
		drm_intel_add_validate_buffer2(target_bo, need_fence);
	}

	for (i = 0; i < bo_gem->softpin_target_count; i++) {
		drm_intel_bo *target_bo = bo_gem->softpin_target[i];

		if (target_bo == bo)
			continue;

		drm_intel_gem_bo_mark_mmaps_incoherent(bo);
		drm_intel_gem_bo_process_reloc2(target_bo);
		drm_intel_add_validate_buffer2(target_bo, false);
	}
}


#ifndef __OpenBSD__
static void
drm_intel_update_buffer_offsets(drm_intel_bufmgr_gem *bufmgr_gem)
{
	int i;

	for (i = 0; i < bufmgr_gem->exec_count; i++) {
		drm_intel_bo *bo = bufmgr_gem->exec_bos[i];
		drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

		/* Update the buffer offset */
		if (bufmgr_gem->exec_objects[i].offset != bo->offset64) {
			DBG("BO %d (%s) migrated: 0x%08x %08x -> 0x%08x %08x\n",
			    bo_gem->gem_handle, bo_gem->name,
			    upper_32_bits(bo->offset64),
			    lower_32_bits(bo->offset64),
			    upper_32_bits(bufmgr_gem->exec_objects[i].offset),
			    lower_32_bits(bufmgr_gem->exec_objects[i].offset));
			bo->offset64 = bufmgr_gem->exec_objects[i].offset;
			bo->offset = bufmgr_gem->exec_objects[i].offset;
		}
	}
}
#endif

static void
drm_intel_update_buffer_offsets2 (drm_intel_bufmgr_gem *bufmgr_gem)
{
	int i;

	for (i = 0; i < bufmgr_gem->exec_count; i++) {
		drm_intel_bo *bo = bufmgr_gem->exec_bos[i];
		drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;

		/* Update the buffer offset */
		if (bufmgr_gem->exec2_objects[i].offset != bo->offset64) {
			/* If we're seeing softpinned object here it means that the kernel
			 * has relocated our object... Indicating a programming error
			 */
			assert(!(bo_gem->kflags & EXEC_OBJECT_PINNED));
			DBG("BO %d (%s) migrated: 0x%08x %08x -> 0x%08x %08x\n",
			    bo_gem->gem_handle, bo_gem->name,
			    upper_32_bits(bo->offset64),
			    lower_32_bits(bo->offset64),
			    upper_32_bits(bufmgr_gem->exec2_objects[i].offset),
			    lower_32_bits(bufmgr_gem->exec2_objects[i].offset));
			bo->offset64 = bufmgr_gem->exec2_objects[i].offset;
			bo->offset = bufmgr_gem->exec2_objects[i].offset;
		}
	}
}

void
drm_intel_gem_bo_aub_dump_bmp(drm_intel_bo *bo,
			      int x1, int y1, int width, int height,
			      enum aub_dump_bmp_format format,
			      int pitch, int offset)
{
}

#ifndef __OpenBSD__
static int
drm_intel_gem_bo_exec(drm_intel_bo *bo, int used,
		      drm_clip_rect_t * cliprects, int num_cliprects, int DR4)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	struct drm_i915_gem_execbuffer execbuf;
	int ret, i;

	if (to_bo_gem(bo)->has_error)
		return -ENOMEM;

	pthread_mutex_lock(&bufmgr_gem->lock);
	/* Update indices and set up the validate list. */
	drm_intel_gem_bo_process_reloc(bo);

	/* Add the batch buffer to the validation list.  There are no
	 * relocations pointing to it.
	 */
	drm_intel_add_validate_buffer(bo);

	memclear(execbuf);
	execbuf.buffers_ptr = (uintptr_t) bufmgr_gem->exec_objects;
	execbuf.buffer_count = bufmgr_gem->exec_count;
	execbuf.batch_start_offset = 0;
	execbuf.batch_len = used;
	execbuf.cliprects_ptr = (uintptr_t) cliprects;
	execbuf.num_cliprects = num_cliprects;
	execbuf.DR1 = 0;
	execbuf.DR4 = DR4;

	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_EXECBUFFER,
		       &execbuf);
	if (ret != 0) {
		ret = -errno;
		if (errno == ENOSPC) {
			DBG("Execbuffer fails to pin. "
			    "Estimate: %u. Actual: %u. Available: %u\n",
			    drm_intel_gem_estimate_batch_space(bufmgr_gem->exec_bos,
							       bufmgr_gem->
							       exec_count),
			    drm_intel_gem_compute_batch_space(bufmgr_gem->exec_bos,
							      bufmgr_gem->
							      exec_count),
			    (unsigned int)bufmgr_gem->gtt_size);
		}
	}
	drm_intel_update_buffer_offsets(bufmgr_gem);

	if (bufmgr_gem->bufmgr.debug)
		drm_intel_gem_dump_validation_list(bufmgr_gem);

	for (i = 0; i < bufmgr_gem->exec_count; i++) {
		drm_intel_bo_gem *bo_gem = to_bo_gem(bufmgr_gem->exec_bos[i]);

		bo_gem->idle = false;

		/* Disconnect the buffer from the validate list */
		bo_gem->validate_index = -1;
		bufmgr_gem->exec_bos[i] = NULL;
	}
	bufmgr_gem->exec_count = 0;
	pthread_mutex_unlock(&bufmgr_gem->lock);

	return ret;
}
#endif

static int
do_exec2(drm_intel_bo *bo, int used, drm_intel_context *ctx,
	 drm_clip_rect_t *cliprects, int num_cliprects, int DR4,
	 int in_fence, int *out_fence,
	 unsigned int flags)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
	struct drm_i915_gem_execbuffer2 execbuf;
	int ret = 0;
	int i;

	if (to_bo_gem(bo)->has_error)
		return -ENOMEM;

	switch (flags & 0x7) {
	default:
		return -EINVAL;
	case I915_EXEC_BLT:
		if (!bufmgr_gem->has_blt)
			return -EINVAL;
		break;
	case I915_EXEC_BSD:
		if (!bufmgr_gem->has_bsd)
			return -EINVAL;
		break;
	case I915_EXEC_VEBOX:
		if (!bufmgr_gem->has_vebox)
			return -EINVAL;
		break;
	case I915_EXEC_RENDER:
	case I915_EXEC_DEFAULT:
		break;
	}

	pthread_mutex_lock(&bufmgr_gem->lock);
	/* Update indices and set up the validate list. */
	drm_intel_gem_bo_process_reloc2(bo);

	/* Add the batch buffer to the validation list.  There are no relocations
	 * pointing to it.
	 */
	drm_intel_add_validate_buffer2(bo, 0);

	memclear(execbuf);
	execbuf.buffers_ptr = (uintptr_t)bufmgr_gem->exec2_objects;
	execbuf.buffer_count = bufmgr_gem->exec_count;
	execbuf.batch_start_offset = 0;
	execbuf.batch_len = used;
#ifndef __OpenBSD__
	execbuf.cliprects_ptr = (uintptr_t)cliprects;
	execbuf.num_cliprects = num_cliprects;
	execbuf.DR1 = 0;
	execbuf.DR4 = DR4;
#endif
	execbuf.flags = flags;
	if (ctx == NULL)
		i915_execbuffer2_set_context_id(execbuf, 0);
	else
		i915_execbuffer2_set_context_id(execbuf, ctx->ctx_id);
	execbuf.rsvd2 = 0;
	if (in_fence != -1) {
		execbuf.rsvd2 = in_fence;
		execbuf.flags |= I915_EXEC_FENCE_IN;
	}
	if (out_fence != NULL) {
		*out_fence = -1;
		execbuf.flags |= I915_EXEC_FENCE_OUT;
	}

	if (bufmgr_gem->no_exec)
		goto skip_execution;

	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_EXECBUFFER2_WR,
		       &execbuf);
	if (ret != 0) {
		ret = -errno;
		if (ret == -ENOSPC) {
			DBG("Execbuffer fails to pin. "
			    "Estimate: %u. Actual: %u. Available: %u\n",
			    drm_intel_gem_estimate_batch_space(bufmgr_gem->exec_bos,
							       bufmgr_gem->exec_count),
			    drm_intel_gem_compute_batch_space(bufmgr_gem->exec_bos,
							      bufmgr_gem->exec_count),
			    (unsigned int) bufmgr_gem->gtt_size);
		}
	}
	drm_intel_update_buffer_offsets2(bufmgr_gem);

	if (ret == 0 && out_fence != NULL)
		*out_fence = execbuf.rsvd2 >> 32;

skip_execution:
	if (bufmgr_gem->bufmgr.debug)
		drm_intel_gem_dump_validation_list(bufmgr_gem);

	for (i = 0; i < bufmgr_gem->exec_count; i++) {
		drm_intel_bo_gem *bo_gem = to_bo_gem(bufmgr_gem->exec_bos[i]);

		bo_gem->idle = false;

		/* Disconnect the buffer from the validate list */
		bo_gem->validate_index = -1;
		bufmgr_gem->exec_bos[i] = NULL;
	}
	bufmgr_gem->exec_count = 0;
	pthread_mutex_unlock(&bufmgr_gem->lock);

	return ret;
}

static int
drm_intel_gem_bo_exec2(drm_intel_bo *bo, int used,
		       drm_clip_rect_t *cliprects, int num_cliprects,
		       int DR4)
{
	return do_exec2(bo, used, NULL, cliprects, num_cliprects, DR4,
			-1, NULL, I915_EXEC_RENDER);
}

static int
drm_intel_gem_bo_mrb_exec2(drm_intel_bo *bo, int used,
			drm_clip_rect_t *cliprects, int num_cliprects, int DR4,
			unsigned int flags)
{
	return do_exec2(bo, used, NULL, cliprects, num_cliprects, DR4,
			-1, NULL, flags);
}

int
drm_intel_gem_bo_context_exec(drm_intel_bo *bo, drm_intel_context *ctx,
			      int used, unsigned int flags)
{
	return do_exec2(bo, used, ctx, NULL, 0, 0, -1, NULL, flags);
}

int
drm_intel_gem_bo_fence_exec(drm_intel_bo *bo,
			    drm_intel_context *ctx,
			    int used,
			    int in_fence,
			    int *out_fence,
			    unsigned int flags)
{
	return do_exec2(bo, used, ctx, NULL, 0, 0, in_fence, out_fence, flags);
}

static int
drm_intel_gem_bo_pin(drm_intel_bo *bo, uint32_t alignment)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_pin pin;
	int ret;

	memclear(pin);
	pin.handle = bo_gem->gem_handle;
	pin.alignment = alignment;

	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_PIN,
		       &pin);
	if (ret != 0)
		return -errno;

	bo->offset64 = pin.offset;
	bo->offset = pin.offset;
	return 0;
}

static int
drm_intel_gem_bo_unpin(drm_intel_bo *bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_unpin unpin;
	int ret;

	memclear(unpin);
	unpin.handle = bo_gem->gem_handle;

	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_UNPIN, &unpin);
	if (ret != 0)
		return -errno;

	return 0;
}

static int
drm_intel_gem_bo_set_tiling_internal(drm_intel_bo *bo,
				     uint32_t tiling_mode,
				     uint32_t stride)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_set_tiling set_tiling;
	int ret;

	if (bo_gem->global_name == 0 &&
	    tiling_mode == bo_gem->tiling_mode &&
	    stride == bo_gem->stride)
		return 0;

	memset(&set_tiling, 0, sizeof(set_tiling));
	do {
		/* set_tiling is slightly broken and overwrites the
		 * input on the error path, so we have to open code
		 * rmIoctl.
		 */
		set_tiling.handle = bo_gem->gem_handle;
		set_tiling.tiling_mode = tiling_mode;
		set_tiling.stride = stride;

		ret = ioctl(bufmgr_gem->fd,
			    DRM_IOCTL_I915_GEM_SET_TILING,
			    &set_tiling);
	} while (ret == -1 && (errno == EINTR || errno == EAGAIN));
	if (ret == -1)
		return -errno;

	bo_gem->tiling_mode = set_tiling.tiling_mode;
	bo_gem->swizzle_mode = set_tiling.swizzle_mode;
	bo_gem->stride = set_tiling.stride;
	return 0;
}

static int
drm_intel_gem_bo_set_tiling(drm_intel_bo *bo, uint32_t * tiling_mode,
			    uint32_t stride)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	int ret;

	/* Tiling with userptr surfaces is not supported
	 * on all hardware so refuse it for time being.
	 */
	if (bo_gem->is_userptr)
		return -EINVAL;

	/* Linear buffers have no stride. By ensuring that we only ever use
	 * stride 0 with linear buffers, we simplify our code.
	 */
	if (*tiling_mode == I915_TILING_NONE)
		stride = 0;

	ret = drm_intel_gem_bo_set_tiling_internal(bo, *tiling_mode, stride);
	if (ret == 0)
		drm_intel_bo_gem_set_in_aperture_size(bufmgr_gem, bo_gem, 0);

	*tiling_mode = bo_gem->tiling_mode;
	return ret;
}

static int
drm_intel_gem_bo_get_tiling(drm_intel_bo *bo, uint32_t * tiling_mode,
			    uint32_t * swizzle_mode)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	*tiling_mode = bo_gem->tiling_mode;
	*swizzle_mode = bo_gem->swizzle_mode;
	return 0;
}

static int
drm_intel_gem_bo_set_softpin_offset(drm_intel_bo *bo, uint64_t offset)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	bo->offset64 = offset;
	bo->offset = offset;
	bo_gem->kflags |= EXEC_OBJECT_PINNED;

	return 0;
}

drm_intel_bo *
drm_intel_bo_gem_create_from_prime(drm_intel_bufmgr *bufmgr, int prime_fd, int size)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bufmgr;
	int ret;
	uint32_t handle;
	drm_intel_bo_gem *bo_gem;
	struct drm_i915_gem_get_tiling get_tiling;

	pthread_mutex_lock(&bufmgr_gem->lock);
	ret = drmPrimeFDToHandle(bufmgr_gem->fd, prime_fd, &handle);
	if (ret) {
		DBG("create_from_prime: failed to obtain handle from fd: %s\n", strerror(errno));
		pthread_mutex_unlock(&bufmgr_gem->lock);
		return NULL;
	}

	/*
	 * See if the kernel has already returned this buffer to us. Just as
	 * for named buffers, we must not create two bo's pointing at the same
	 * kernel object
	 */
	HASH_FIND(handle_hh, bufmgr_gem->handle_table,
		  &handle, sizeof(handle), bo_gem);
	if (bo_gem) {
		drm_intel_gem_bo_reference(&bo_gem->bo);
		goto out;
	}

	bo_gem = calloc(1, sizeof(*bo_gem));
	if (!bo_gem)
		goto out;

	atomic_set(&bo_gem->refcount, 1);
	DRMINITLISTHEAD(&bo_gem->vma_list);

	/* Determine size of bo.  The fd-to-handle ioctl really should
	 * return the size, but it doesn't.  If we have kernel 3.12 or
	 * later, we can lseek on the prime fd to get the size.  Older
	 * kernels will just fail, in which case we fall back to the
	 * provided (estimated or guess size). */
	ret = lseek(prime_fd, 0, SEEK_END);
	if (ret != -1)
		bo_gem->bo.size = ret;
	else
		bo_gem->bo.size = size;

	bo_gem->bo.handle = handle;
	bo_gem->bo.bufmgr = bufmgr;

	bo_gem->gem_handle = handle;
	HASH_ADD(handle_hh, bufmgr_gem->handle_table,
		 gem_handle, sizeof(bo_gem->gem_handle), bo_gem);

	bo_gem->name = "prime";
	bo_gem->validate_index = -1;
	bo_gem->reloc_tree_fences = 0;
	bo_gem->used_as_reloc_target = false;
	bo_gem->has_error = false;
	bo_gem->reusable = false;

	memclear(get_tiling);
	get_tiling.handle = bo_gem->gem_handle;
	if (drmIoctl(bufmgr_gem->fd,
		     DRM_IOCTL_I915_GEM_GET_TILING,
		     &get_tiling))
		goto err;

	bo_gem->tiling_mode = get_tiling.tiling_mode;
	bo_gem->swizzle_mode = get_tiling.swizzle_mode;
	/* XXX stride is unknown */
	drm_intel_bo_gem_set_in_aperture_size(bufmgr_gem, bo_gem, 0);

out:
	pthread_mutex_unlock(&bufmgr_gem->lock);
	return &bo_gem->bo;

err:
	drm_intel_gem_bo_free(&bo_gem->bo);
	pthread_mutex_unlock(&bufmgr_gem->lock);
	return NULL;
}

int
drm_intel_bo_gem_export_to_prime(drm_intel_bo *bo, int *prime_fd)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	if (drmPrimeHandleToFD(bufmgr_gem->fd, bo_gem->gem_handle,
			       DRM_CLOEXEC, prime_fd) != 0)
		return -errno;

	bo_gem->reusable = false;

	return 0;
}

static int
drm_intel_gem_bo_flink(drm_intel_bo *bo, uint32_t * name)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	if (!bo_gem->global_name) {
		struct drm_gem_flink flink;

		memclear(flink);
		flink.handle = bo_gem->gem_handle;
		if (drmIoctl(bufmgr_gem->fd, DRM_IOCTL_GEM_FLINK, &flink))
			return -errno;

		pthread_mutex_lock(&bufmgr_gem->lock);
		if (!bo_gem->global_name) {
			bo_gem->global_name = flink.name;
			bo_gem->reusable = false;

			HASH_ADD(name_hh, bufmgr_gem->name_table,
				 global_name, sizeof(bo_gem->global_name),
				 bo_gem);
		}
		pthread_mutex_unlock(&bufmgr_gem->lock);
	}

	*name = bo_gem->global_name;
	return 0;
}

/**
 * Enables unlimited caching of buffer objects for reuse.
 *
 * This is potentially very memory expensive, as the cache at each bucket
 * size is only bounded by how many buffers of that size we've managed to have
 * in flight at once.
 */
void
drm_intel_bufmgr_gem_enable_reuse(drm_intel_bufmgr *bufmgr)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bufmgr;

	bufmgr_gem->bo_reuse = true;
}

/**
 * Disables implicit synchronisation before executing the bo
 *
 * This will cause rendering corruption unless you correctly manage explicit
 * fences for all rendering involving this buffer - including use by others.
 * Disabling the implicit serialisation is only required if that serialisation
 * is too coarse (for example, you have split the buffer into many
 * non-overlapping regions and are sharing the whole buffer between concurrent
 * independent command streams).
 *
 * Note the kernel must advertise support via I915_PARAM_HAS_EXEC_ASYNC,
 * which can be checked using drm_intel_bufmgr_can_disable_implicit_sync,
 * or subsequent execbufs involving the bo will generate EINVAL.
 */
void
drm_intel_gem_bo_disable_implicit_sync(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	bo_gem->kflags |= EXEC_OBJECT_ASYNC;
}

/**
 * Enables implicit synchronisation before executing the bo
 *
 * This is the default behaviour of the kernel, to wait upon prior writes
 * completing on the object before rendering with it, or to wait for prior
 * reads to complete before writing into the object.
 * drm_intel_gem_bo_disable_implicit_sync() can stop this behaviour, telling
 * the kernel never to insert a stall before using the object. Then this
 * function can be used to restore the implicit sync before subsequent
 * rendering.
 */
void
drm_intel_gem_bo_enable_implicit_sync(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	bo_gem->kflags &= ~EXEC_OBJECT_ASYNC;
}

/**
 * Query whether the kernel supports disabling of its implicit synchronisation
 * before execbuf. See drm_intel_gem_bo_disable_implicit_sync()
 */
int
drm_intel_bufmgr_gem_can_disable_implicit_sync(drm_intel_bufmgr *bufmgr)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bufmgr;

	return bufmgr_gem->has_exec_async;
}

/**
 * Enable use of fenced reloc type.
 *
 * New code should enable this to avoid unnecessary fence register
 * allocation.  If this option is not enabled, all relocs will have fence
 * register allocated.
 */
void
drm_intel_bufmgr_gem_enable_fenced_relocs(drm_intel_bufmgr *bufmgr)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bufmgr;

	if (bufmgr_gem->bufmgr.bo_exec == drm_intel_gem_bo_exec2)
		bufmgr_gem->fenced_relocs = true;
}

/**
 * Return the additional aperture space required by the tree of buffer objects
 * rooted at bo.
 */
static int
drm_intel_gem_bo_get_aperture_space(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	int i;
	int total = 0;

	if (bo == NULL || bo_gem->included_in_check_aperture)
		return 0;

	total += bo->size;
	bo_gem->included_in_check_aperture = true;

	for (i = 0; i < bo_gem->reloc_count; i++)
		total +=
		    drm_intel_gem_bo_get_aperture_space(bo_gem->
							reloc_target_info[i].bo);

	return total;
}

/**
 * Count the number of buffers in this list that need a fence reg
 *
 * If the count is greater than the number of available regs, we'll have
 * to ask the caller to resubmit a batch with fewer tiled buffers.
 *
 * This function over-counts if the same buffer is used multiple times.
 */
static unsigned int
drm_intel_gem_total_fences(drm_intel_bo ** bo_array, int count)
{
	int i;
	unsigned int total = 0;

	for (i = 0; i < count; i++) {
		drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo_array[i];

		if (bo_gem == NULL)
			continue;

		total += bo_gem->reloc_tree_fences;
	}
	return total;
}

/**
 * Clear the flag set by drm_intel_gem_bo_get_aperture_space() so we're ready
 * for the next drm_intel_bufmgr_check_aperture_space() call.
 */
static void
drm_intel_gem_bo_clear_aperture_space_flag(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	int i;

	if (bo == NULL || !bo_gem->included_in_check_aperture)
		return;

	bo_gem->included_in_check_aperture = false;

	for (i = 0; i < bo_gem->reloc_count; i++)
		drm_intel_gem_bo_clear_aperture_space_flag(bo_gem->
							   reloc_target_info[i].bo);
}

/**
 * Return a conservative estimate for the amount of aperture required
 * for a collection of buffers. This may double-count some buffers.
 */
static unsigned int
drm_intel_gem_estimate_batch_space(drm_intel_bo **bo_array, int count)
{
	int i;
	unsigned int total = 0;

	for (i = 0; i < count; i++) {
		drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo_array[i];
		if (bo_gem != NULL)
			total += bo_gem->reloc_tree_size;
	}
	return total;
}

/**
 * Return the amount of aperture needed for a collection of buffers.
 * This avoids double counting any buffers, at the cost of looking
 * at every buffer in the set.
 */
static unsigned int
drm_intel_gem_compute_batch_space(drm_intel_bo **bo_array, int count)
{
	int i;
	unsigned int total = 0;

	for (i = 0; i < count; i++) {
		total += drm_intel_gem_bo_get_aperture_space(bo_array[i]);
		/* For the first buffer object in the array, we get an
		 * accurate count back for its reloc_tree size (since nothing
		 * had been flagged as being counted yet).  We can save that
		 * value out as a more conservative reloc_tree_size that
		 * avoids double-counting target buffers.  Since the first
		 * buffer happens to usually be the batch buffer in our
		 * callers, this can pull us back from doing the tree
		 * walk on every new batch emit.
		 */
		if (i == 0) {
			drm_intel_bo_gem *bo_gem =
			    (drm_intel_bo_gem *) bo_array[i];
			bo_gem->reloc_tree_size = total;
		}
	}

	for (i = 0; i < count; i++)
		drm_intel_gem_bo_clear_aperture_space_flag(bo_array[i]);
	return total;
}

/**
 * Return -1 if the batchbuffer should be flushed before attempting to
 * emit rendering referencing the buffers pointed to by bo_array.
 *
 * This is required because if we try to emit a batchbuffer with relocations
 * to a tree of buffers that won't simultaneously fit in the aperture,
 * the rendering will return an error at a point where the software is not
 * prepared to recover from it.
 *
 * However, we also want to emit the batchbuffer significantly before we reach
 * the limit, as a series of batchbuffers each of which references buffers
 * covering almost all of the aperture means that at each emit we end up
 * waiting to evict a buffer from the last rendering, and we get synchronous
 * performance.  By emitting smaller batchbuffers, we eat some CPU overhead to
 * get better parallelism.
 */
static int
drm_intel_gem_check_aperture_space(drm_intel_bo **bo_array, int count)
{
	drm_intel_bufmgr_gem *bufmgr_gem =
	    (drm_intel_bufmgr_gem *) bo_array[0]->bufmgr;
	unsigned int total = 0;
	unsigned int threshold = bufmgr_gem->gtt_size * 3 / 4;
	int total_fences;

	/* Check for fence reg constraints if necessary */
	if (bufmgr_gem->available_fences) {
		total_fences = drm_intel_gem_total_fences(bo_array, count);
		if (total_fences > bufmgr_gem->available_fences)
			return -ENOSPC;
	}

	total = drm_intel_gem_estimate_batch_space(bo_array, count);

	if (total > threshold)
		total = drm_intel_gem_compute_batch_space(bo_array, count);

	if (total > threshold) {
		DBG("check_space: overflowed available aperture, "
		    "%dkb vs %dkb\n",
		    total / 1024, (int)bufmgr_gem->gtt_size / 1024);
		return -ENOSPC;
	} else {
		DBG("drm_check_space: total %dkb vs bufgr %dkb\n", total / 1024,
		    (int)bufmgr_gem->gtt_size / 1024);
		return 0;
	}
}

/*
 * Disable buffer reuse for objects which are shared with the kernel
 * as scanout buffers
 */
static int
drm_intel_gem_bo_disable_reuse(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	bo_gem->reusable = false;
	return 0;
}

static int
drm_intel_gem_bo_is_reusable(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	return bo_gem->reusable;
}

static int
_drm_intel_gem_bo_references(drm_intel_bo *bo, drm_intel_bo *target_bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	int i;

	for (i = 0; i < bo_gem->reloc_count; i++) {
		if (bo_gem->reloc_target_info[i].bo == target_bo)
			return 1;
		if (bo == bo_gem->reloc_target_info[i].bo)
			continue;
		if (_drm_intel_gem_bo_references(bo_gem->reloc_target_info[i].bo,
						target_bo))
			return 1;
	}

	for (i = 0; i< bo_gem->softpin_target_count; i++) {
		if (bo_gem->softpin_target[i] == target_bo)
			return 1;
		if (_drm_intel_gem_bo_references(bo_gem->softpin_target[i], target_bo))
			return 1;
	}

	return 0;
}

/** Return true if target_bo is referenced by bo's relocation tree. */
static int
drm_intel_gem_bo_references(drm_intel_bo *bo, drm_intel_bo *target_bo)
{
	drm_intel_bo_gem *target_bo_gem = (drm_intel_bo_gem *) target_bo;

	if (bo == NULL || target_bo == NULL)
		return 0;
	if (target_bo_gem->used_as_reloc_target)
		return _drm_intel_gem_bo_references(bo, target_bo);
	return 0;
}

static void
add_bucket(drm_intel_bufmgr_gem *bufmgr_gem, int size)
{
	unsigned int i = bufmgr_gem->num_buckets;

	assert(i < ARRAY_SIZE(bufmgr_gem->cache_bucket));

	DRMINITLISTHEAD(&bufmgr_gem->cache_bucket[i].head);
	bufmgr_gem->cache_bucket[i].size = size;
	bufmgr_gem->num_buckets++;
}

static void
init_cache_buckets(drm_intel_bufmgr_gem *bufmgr_gem)
{
	unsigned long size, cache_max_size = 64 * 1024 * 1024;

	/* OK, so power of two buckets was too wasteful of memory.
	 * Give 3 other sizes between each power of two, to hopefully
	 * cover things accurately enough.  (The alternative is
	 * probably to just go for exact matching of sizes, and assume
	 * that for things like composited window resize the tiled
	 * width/height alignment and rounding of sizes to pages will
	 * get us useful cache hit rates anyway)
	 */
	add_bucket(bufmgr_gem, 4096);
	add_bucket(bufmgr_gem, 4096 * 2);
	add_bucket(bufmgr_gem, 4096 * 3);

	/* Initialize the linked lists for BO reuse cache. */
	for (size = 4 * 4096; size <= cache_max_size; size *= 2) {
		add_bucket(bufmgr_gem, size);

		add_bucket(bufmgr_gem, size + size * 1 / 4);
		add_bucket(bufmgr_gem, size + size * 2 / 4);
		add_bucket(bufmgr_gem, size + size * 3 / 4);
	}
}

void
drm_intel_bufmgr_gem_set_vma_cache_size(drm_intel_bufmgr *bufmgr, int limit)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bufmgr;

	bufmgr_gem->vma_max = limit;

	drm_intel_gem_bo_purge_vma_cache(bufmgr_gem);
}

static int
parse_devid_override(const char *devid_override)
{
	static const struct {
		const char *name;
		int pci_id;
	} name_map[] = {
		{ "brw", PCI_CHIP_I965_GM },
		{ "g4x", PCI_CHIP_GM45_GM },
		{ "ilk", PCI_CHIP_ILD_G },
		{ "snb", PCI_CHIP_SANDYBRIDGE_M_GT2_PLUS },
		{ "ivb", PCI_CHIP_IVYBRIDGE_S_GT2 },
		{ "hsw", PCI_CHIP_HASWELL_CRW_E_GT3 },
		{ "byt", PCI_CHIP_VALLEYVIEW_3 },
		{ "bdw", 0x1620 | BDW_ULX },
		{ "skl", PCI_CHIP_SKYLAKE_DT_GT2 },
		{ "kbl", PCI_CHIP_KABYLAKE_DT_GT2 },
	};
	unsigned int i;

	for (i = 0; i < ARRAY_SIZE(name_map); i++) {
		if (!strcmp(name_map[i].name, devid_override))
			return name_map[i].pci_id;
	}

	return strtod(devid_override, NULL);
}

/**
 * Get the PCI ID for the device.  This can be overridden by setting the
 * INTEL_DEVID_OVERRIDE environment variable to the desired ID.
 */
static int
get_pci_device_id(drm_intel_bufmgr_gem *bufmgr_gem)
{
	char *devid_override;
	int devid = 0;
	int ret;
	drm_i915_getparam_t gp;

	if (geteuid() == getuid()) {
		devid_override = getenv("INTEL_DEVID_OVERRIDE");
		if (devid_override) {
			bufmgr_gem->no_exec = true;
			return parse_devid_override(devid_override);
		}
	}

	memclear(gp);
	gp.param = I915_PARAM_CHIPSET_ID;
	gp.value = &devid;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	if (ret) {
		fprintf(stderr, "get chip id failed: %d [%d]\n", ret, errno);
		fprintf(stderr, "param: %d, val: %d\n", gp.param, *gp.value);
	}
	return devid;
}

int
drm_intel_bufmgr_gem_get_devid(drm_intel_bufmgr *bufmgr)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bufmgr;

	return bufmgr_gem->pci_device;
}

/**
 * Sets the AUB filename.
 *
 * This function has to be called before drm_intel_bufmgr_gem_set_aub_dump()
 * for it to have any effect.
 */
void
drm_intel_bufmgr_gem_set_aub_filename(drm_intel_bufmgr *bufmgr,
				      const char *filename)
{
}

/**
 * Sets up AUB dumping.
 *
 * This is a trace file format that can be used with the simulator.
 * Packets are emitted in a format somewhat like GPU command packets.
 * You can set up a GTT and upload your objects into the referenced
 * space, then send off batchbuffers and get BMPs out the other end.
 */
void
drm_intel_bufmgr_gem_set_aub_dump(drm_intel_bufmgr *bufmgr, int enable)
{
	fprintf(stderr, "libdrm aub dumping is deprecated.\n\n"
		"Use intel_aubdump from intel-gpu-tools instead.  Install intel-gpu-tools,\n"
		"then run (for example)\n\n"
		"\t$ intel_aubdump --output=trace.aub glxgears -geometry 500x500\n\n"
		"See the intel_aubdump man page for more details.\n");
}

drm_intel_context *
drm_intel_gem_context_create(drm_intel_bufmgr *bufmgr)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bufmgr;
	struct drm_i915_gem_context_create create;
	drm_intel_context *context = NULL;
	int ret;

	context = calloc(1, sizeof(*context));
	if (!context)
		return NULL;

	memclear(create);
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_CONTEXT_CREATE, &create);
	if (ret != 0) {
		DBG("DRM_IOCTL_I915_GEM_CONTEXT_CREATE failed: %s\n",
		    strerror(errno));
		free(context);
		return NULL;
	}

	context->ctx_id = create.ctx_id;
	context->bufmgr = bufmgr;

	return context;
}

int
drm_intel_gem_context_get_id(drm_intel_context *ctx, uint32_t *ctx_id)
{
	if (ctx == NULL)
		return -EINVAL;

	*ctx_id = ctx->ctx_id;

	return 0;
}

void
drm_intel_gem_context_destroy(drm_intel_context *ctx)
{
	drm_intel_bufmgr_gem *bufmgr_gem;
	struct drm_i915_gem_context_destroy destroy;
	int ret;

	if (ctx == NULL)
		return;

	memclear(destroy);

	bufmgr_gem = (drm_intel_bufmgr_gem *)ctx->bufmgr;
	destroy.ctx_id = ctx->ctx_id;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_CONTEXT_DESTROY,
		       &destroy);
	if (ret != 0)
		fprintf(stderr, "DRM_IOCTL_I915_GEM_CONTEXT_DESTROY failed: %s\n",
			strerror(errno));

	free(ctx);
}

int
drm_intel_get_reset_stats(drm_intel_context *ctx,
			  uint32_t *reset_count,
			  uint32_t *active,
			  uint32_t *pending)
{
	drm_intel_bufmgr_gem *bufmgr_gem;
	struct drm_i915_reset_stats stats;
	int ret;

	if (ctx == NULL)
		return -EINVAL;

	memclear(stats);

	bufmgr_gem = (drm_intel_bufmgr_gem *)ctx->bufmgr;
	stats.ctx_id = ctx->ctx_id;
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GET_RESET_STATS,
		       &stats);
	if (ret == 0) {
		if (reset_count != NULL)
			*reset_count = stats.reset_count;

		if (active != NULL)
			*active = stats.batch_active;

		if (pending != NULL)
			*pending = stats.batch_pending;
	}

	return ret;
}

int
drm_intel_reg_read(drm_intel_bufmgr *bufmgr,
		   uint32_t offset,
		   uint64_t *result)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bufmgr;
	struct drm_i915_reg_read reg_read;
	int ret;

	memclear(reg_read);
	reg_read.offset = offset;

	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_REG_READ, &reg_read);

	*result = reg_read.val;
	return ret;
}

int
drm_intel_get_subslice_total(int fd, unsigned int *subslice_total)
{
	drm_i915_getparam_t gp;
	int ret;

	memclear(gp);
	gp.value = (int*)subslice_total;
	gp.param = I915_PARAM_SUBSLICE_TOTAL;
	ret = drmIoctl(fd, DRM_IOCTL_I915_GETPARAM, &gp);
	if (ret)
		return -errno;

	return 0;
}

int
drm_intel_get_eu_total(int fd, unsigned int *eu_total)
{
	drm_i915_getparam_t gp;
	int ret;

	memclear(gp);
	gp.value = (int*)eu_total;
	gp.param = I915_PARAM_EU_TOTAL;
	ret = drmIoctl(fd, DRM_IOCTL_I915_GETPARAM, &gp);
	if (ret)
		return -errno;

	return 0;
}

int
drm_intel_get_pooled_eu(int fd)
{
	drm_i915_getparam_t gp;
	int ret = -1;

	memclear(gp);
	gp.param = I915_PARAM_HAS_POOLED_EU;
	gp.value = &ret;
	if (drmIoctl(fd, DRM_IOCTL_I915_GETPARAM, &gp))
		return -errno;

	return ret;
}

int
drm_intel_get_min_eu_in_pool(int fd)
{
	drm_i915_getparam_t gp;
	int ret = -1;

	memclear(gp);
	gp.param = I915_PARAM_MIN_EU_IN_POOL;
	gp.value = &ret;
	if (drmIoctl(fd, DRM_IOCTL_I915_GETPARAM, &gp))
		return -errno;

	return ret;
}

/**
 * Annotate the given bo for use in aub dumping.
 *
 * \param annotations is an array of drm_intel_aub_annotation objects
 * describing the type of data in various sections of the bo.  Each
 * element of the array specifies the type and subtype of a section of
 * the bo, and the past-the-end offset of that section.  The elements
 * of \c annotations must be sorted so that ending_offset is
 * increasing.
 *
 * \param count is the number of elements in the \c annotations array.
 * If \c count is zero, then \c annotations will not be dereferenced.
 *
 * Annotations are copied into a private data structure, so caller may
 * re-use the memory pointed to by \c annotations after the call
 * returns.
 *
 * Annotations are stored for the lifetime of the bo; to reset to the
 * default state (no annotations), call this function with a \c count
 * of zero.
 */
void
drm_intel_bufmgr_gem_set_aub_annotations(drm_intel_bo *bo,
					 drm_intel_aub_annotation *annotations,
					 unsigned count)
{
}

static pthread_mutex_t bufmgr_list_mutex = PTHREAD_MUTEX_INITIALIZER;
static drmMMListHead bufmgr_list = { &bufmgr_list, &bufmgr_list };

static drm_intel_bufmgr_gem *
drm_intel_bufmgr_gem_find(int fd)
{
	drm_intel_bufmgr_gem *bufmgr_gem;

	DRMLISTFOREACHENTRY(bufmgr_gem, &bufmgr_list, managers) {
		if (bufmgr_gem->fd == fd) {
			atomic_inc(&bufmgr_gem->refcount);
			return bufmgr_gem;
		}
	}

	return NULL;
}

static void
drm_intel_bufmgr_gem_unref(drm_intel_bufmgr *bufmgr)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bufmgr;

	if (atomic_add_unless(&bufmgr_gem->refcount, -1, 1)) {
		pthread_mutex_lock(&bufmgr_list_mutex);

		if (atomic_dec_and_test(&bufmgr_gem->refcount)) {
			DRMLISTDEL(&bufmgr_gem->managers);
			drm_intel_bufmgr_gem_destroy(bufmgr);
		}

		pthread_mutex_unlock(&bufmgr_list_mutex);
	}
}

void *drm_intel_gem_bo_map__gtt(drm_intel_bo *bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	if (bo_gem->gtt_virtual)
		return bo_gem->gtt_virtual;

	if (bo_gem->is_userptr)
		return NULL;

	pthread_mutex_lock(&bufmgr_gem->lock);
	if (bo_gem->gtt_virtual == NULL) {
		struct drm_i915_gem_mmap_gtt mmap_arg;
		void *ptr;

		DBG("bo_map_gtt: mmap %d (%s), map_count=%d\n",
		    bo_gem->gem_handle, bo_gem->name, bo_gem->map_count);

		if (bo_gem->map_count++ == 0)
			drm_intel_gem_bo_open_vma(bufmgr_gem, bo_gem);

		memclear(mmap_arg);
		mmap_arg.handle = bo_gem->gem_handle;

		/* Get the fake offset back... */
		ptr = MAP_FAILED;
		if (drmIoctl(bufmgr_gem->fd,
			     DRM_IOCTL_I915_GEM_MMAP_GTT,
			     &mmap_arg) == 0) {
			/* and mmap it */
			ptr = drm_mmap(0, bo->size, PROT_READ | PROT_WRITE,
				       MAP_SHARED, bufmgr_gem->fd,
				       mmap_arg.offset);
		}
		if (ptr == MAP_FAILED) {
			if (--bo_gem->map_count == 0)
				drm_intel_gem_bo_close_vma(bufmgr_gem, bo_gem);
			ptr = NULL;
		}

		bo_gem->gtt_virtual = ptr;
	}
	pthread_mutex_unlock(&bufmgr_gem->lock);

	return bo_gem->gtt_virtual;
}

void *drm_intel_gem_bo_map__cpu(drm_intel_bo *bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	if (bo_gem->mem_virtual)
		return bo_gem->mem_virtual;

	if (bo_gem->is_userptr) {
		/* Return the same user ptr */
		return bo_gem->user_virtual;
	}

	pthread_mutex_lock(&bufmgr_gem->lock);
	if (!bo_gem->mem_virtual) {
		struct drm_i915_gem_mmap mmap_arg;

		if (bo_gem->map_count++ == 0)
			drm_intel_gem_bo_open_vma(bufmgr_gem, bo_gem);

		DBG("bo_map: %d (%s), map_count=%d\n",
		    bo_gem->gem_handle, bo_gem->name, bo_gem->map_count);

		memclear(mmap_arg);
		mmap_arg.handle = bo_gem->gem_handle;
		mmap_arg.size = bo->size;
		if (drmIoctl(bufmgr_gem->fd,
			     DRM_IOCTL_I915_GEM_MMAP,
			     &mmap_arg)) {
			DBG("%s:%d: Error mapping buffer %d (%s): %s .\n",
			    __FILE__, __LINE__, bo_gem->gem_handle,
			    bo_gem->name, strerror(errno));
			if (--bo_gem->map_count == 0)
				drm_intel_gem_bo_close_vma(bufmgr_gem, bo_gem);
		} else {
			VG(VALGRIND_MALLOCLIKE_BLOCK(mmap_arg.addr_ptr, mmap_arg.size, 0, 1));
			bo_gem->mem_virtual = (void *)(uintptr_t) mmap_arg.addr_ptr;
		}
	}
	pthread_mutex_unlock(&bufmgr_gem->lock);

	return bo_gem->mem_virtual;
}

void *drm_intel_gem_bo_map__wc(drm_intel_bo *bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	if (bo_gem->wc_virtual)
		return bo_gem->wc_virtual;

	if (bo_gem->is_userptr)
		return NULL;

	pthread_mutex_lock(&bufmgr_gem->lock);
	if (!bo_gem->wc_virtual) {
		struct drm_i915_gem_mmap mmap_arg;

		if (bo_gem->map_count++ == 0)
			drm_intel_gem_bo_open_vma(bufmgr_gem, bo_gem);

		DBG("bo_map: %d (%s), map_count=%d\n",
		    bo_gem->gem_handle, bo_gem->name, bo_gem->map_count);

		memclear(mmap_arg);
		mmap_arg.handle = bo_gem->gem_handle;
		mmap_arg.size = bo->size;
		mmap_arg.flags = I915_MMAP_WC;
		if (drmIoctl(bufmgr_gem->fd,
			     DRM_IOCTL_I915_GEM_MMAP,
			     &mmap_arg)) {
			DBG("%s:%d: Error mapping buffer %d (%s): %s .\n",
			    __FILE__, __LINE__, bo_gem->gem_handle,
			    bo_gem->name, strerror(errno));
			if (--bo_gem->map_count == 0)
				drm_intel_gem_bo_close_vma(bufmgr_gem, bo_gem);
		} else {
			VG(VALGRIND_MALLOCLIKE_BLOCK(mmap_arg.addr_ptr, mmap_arg.size, 0, 1));
			bo_gem->wc_virtual = (void *)(uintptr_t) mmap_arg.addr_ptr;
		}
	}
	pthread_mutex_unlock(&bufmgr_gem->lock);

	return bo_gem->wc_virtual;
}

/**
 * Initializes the GEM buffer manager, which uses the kernel to allocate, map,
 * and manage map buffer objections.
 *
 * \param fd File descriptor of the opened DRM device.
 */
drm_intel_bufmgr *
drm_intel_bufmgr_gem_init(int fd, int batch_size)
{
	drm_intel_bufmgr_gem *bufmgr_gem;
	struct drm_i915_gem_get_aperture aperture;
	drm_i915_getparam_t gp;
	int ret, tmp;
#ifndef __OpenBSD__
	bool exec2 = false;
#endif

	pthread_mutex_lock(&bufmgr_list_mutex);

	bufmgr_gem = drm_intel_bufmgr_gem_find(fd);
	if (bufmgr_gem)
		goto exit;

	bufmgr_gem = calloc(1, sizeof(*bufmgr_gem));
	if (bufmgr_gem == NULL)
		goto exit;

	bufmgr_gem->fd = fd;
	atomic_set(&bufmgr_gem->refcount, 1);

	if (pthread_mutex_init(&bufmgr_gem->lock, NULL) != 0) {
		free(bufmgr_gem);
		bufmgr_gem = NULL;
		goto exit;
	}

	memclear(aperture);
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_GET_APERTURE,
		       &aperture);

	if (ret == 0)
		bufmgr_gem->gtt_size = aperture.aper_available_size;
	else {
		fprintf(stderr, "DRM_IOCTL_I915_GEM_APERTURE failed: %s\n",
			strerror(errno));
		bufmgr_gem->gtt_size = 128 * 1024 * 1024;
		fprintf(stderr, "Assuming %dkB available aperture size.\n"
			"May lead to reduced performance or incorrect "
			"rendering.\n",
			(int)bufmgr_gem->gtt_size / 1024);
	}

	bufmgr_gem->pci_device = get_pci_device_id(bufmgr_gem);

	if (IS_GEN2(bufmgr_gem->pci_device))
		bufmgr_gem->gen = 2;
	else if (IS_GEN3(bufmgr_gem->pci_device))
		bufmgr_gem->gen = 3;
	else if (IS_GEN4(bufmgr_gem->pci_device))
		bufmgr_gem->gen = 4;
	else if (IS_GEN5(bufmgr_gem->pci_device))
		bufmgr_gem->gen = 5;
	else if (IS_GEN6(bufmgr_gem->pci_device))
		bufmgr_gem->gen = 6;
	else if (IS_GEN7(bufmgr_gem->pci_device))
		bufmgr_gem->gen = 7;
	else if (IS_GEN8(bufmgr_gem->pci_device))
		bufmgr_gem->gen = 8;
	else if (IS_GEN9(bufmgr_gem->pci_device))
		bufmgr_gem->gen = 9;
	else if (IS_GEN10(bufmgr_gem->pci_device))
		bufmgr_gem->gen = 10;
	else {
		free(bufmgr_gem);
		bufmgr_gem = NULL;
		goto exit;
	}

	if (IS_GEN3(bufmgr_gem->pci_device) &&
	    bufmgr_gem->gtt_size > 256*1024*1024) {
		/* The unmappable part of gtt on gen 3 (i.e. above 256MB) can't
		 * be used for tiled blits. To simplify the accounting, just
		 * subtract the unmappable part (fixed to 256MB on all known
		 * gen3 devices) if the kernel advertises it. */
		bufmgr_gem->gtt_size -= 256*1024*1024;
	}

	memclear(gp);
	gp.value = &tmp;

#ifndef __OpenBSD__
	gp.param = I915_PARAM_HAS_EXECBUF2;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	if (!ret)
		exec2 = true;
#endif

	gp.param = I915_PARAM_HAS_BSD;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	bufmgr_gem->has_bsd = ret == 0;

	gp.param = I915_PARAM_HAS_BLT;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	bufmgr_gem->has_blt = ret == 0;

	gp.param = I915_PARAM_HAS_RELAXED_FENCING;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	bufmgr_gem->has_relaxed_fencing = ret == 0;

	gp.param = I915_PARAM_HAS_EXEC_ASYNC;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	bufmgr_gem->has_exec_async = ret == 0;

	bufmgr_gem->bufmgr.bo_alloc_userptr = check_bo_alloc_userptr;

	gp.param = I915_PARAM_HAS_WAIT_TIMEOUT;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	bufmgr_gem->has_wait_timeout = ret == 0;

	gp.param = I915_PARAM_HAS_LLC;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	if (ret != 0) {
		/* Kernel does not supports HAS_LLC query, fallback to GPU
		 * generation detection and assume that we have LLC on GEN6/7
		 */
		bufmgr_gem->has_llc = (IS_GEN6(bufmgr_gem->pci_device) |
				IS_GEN7(bufmgr_gem->pci_device));
	} else
		bufmgr_gem->has_llc = *gp.value;

	gp.param = I915_PARAM_HAS_VEBOX;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	bufmgr_gem->has_vebox = (ret == 0) & (*gp.value > 0);

	gp.param = I915_PARAM_HAS_EXEC_SOFTPIN;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	if (ret == 0 && *gp.value > 0)
		bufmgr_gem->bufmgr.bo_set_softpin_offset = drm_intel_gem_bo_set_softpin_offset;

	if (bufmgr_gem->gen < 4) {
		gp.param = I915_PARAM_NUM_FENCES_AVAIL;
		gp.value = &bufmgr_gem->available_fences;
		ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
		if (ret) {
			fprintf(stderr, "get fences failed: %d [%d]\n", ret,
				errno);
			fprintf(stderr, "param: %d, val: %d\n", gp.param,
				*gp.value);
			bufmgr_gem->available_fences = 0;
		} else {
			/* XXX The kernel reports the total number of fences,
			 * including any that may be pinned.
			 *
			 * We presume that there will be at least one pinned
			 * fence for the scanout buffer, but there may be more
			 * than one scanout and the user may be manually
			 * pinning buffers. Let's move to execbuffer2 and
			 * thereby forget the insanity of using fences...
			 */
			bufmgr_gem->available_fences -= 2;
			if (bufmgr_gem->available_fences < 0)
				bufmgr_gem->available_fences = 0;
		}
	}

	if (bufmgr_gem->gen >= 8) {
		gp.param = I915_PARAM_HAS_ALIASING_PPGTT;
		ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
		if (ret == 0 && *gp.value == 3)
			bufmgr_gem->bufmgr.bo_use_48b_address_range = drm_intel_gem_bo_use_48b_address_range;
	}

	/* Let's go with one relocation per every 2 dwords (but round down a bit
	 * since a power of two will mean an extra page allocation for the reloc
	 * buffer).
	 *
	 * Every 4 was too few for the blender benchmark.
	 */
	bufmgr_gem->max_relocs = batch_size / sizeof(uint32_t) / 2 - 2;

	bufmgr_gem->bufmgr.bo_alloc = drm_intel_gem_bo_alloc;
	bufmgr_gem->bufmgr.bo_alloc_for_render =
	    drm_intel_gem_bo_alloc_for_render;
	bufmgr_gem->bufmgr.bo_alloc_tiled = drm_intel_gem_bo_alloc_tiled;
	bufmgr_gem->bufmgr.bo_reference = drm_intel_gem_bo_reference;
	bufmgr_gem->bufmgr.bo_unreference = drm_intel_gem_bo_unreference;
	bufmgr_gem->bufmgr.bo_map = drm_intel_gem_bo_map;
	bufmgr_gem->bufmgr.bo_unmap = drm_intel_gem_bo_unmap;
	bufmgr_gem->bufmgr.bo_subdata = drm_intel_gem_bo_subdata;
	bufmgr_gem->bufmgr.bo_get_subdata = drm_intel_gem_bo_get_subdata;
	bufmgr_gem->bufmgr.bo_wait_rendering = drm_intel_gem_bo_wait_rendering;
	bufmgr_gem->bufmgr.bo_emit_reloc = drm_intel_gem_bo_emit_reloc;
	bufmgr_gem->bufmgr.bo_emit_reloc_fence = drm_intel_gem_bo_emit_reloc_fence;
	bufmgr_gem->bufmgr.bo_pin = drm_intel_gem_bo_pin;
	bufmgr_gem->bufmgr.bo_unpin = drm_intel_gem_bo_unpin;
	bufmgr_gem->bufmgr.bo_get_tiling = drm_intel_gem_bo_get_tiling;
	bufmgr_gem->bufmgr.bo_set_tiling = drm_intel_gem_bo_set_tiling;
	bufmgr_gem->bufmgr.bo_flink = drm_intel_gem_bo_flink;
#ifndef __OpenBSD__
	/*
	 * Only the new one is available on OpenBSD
	 */
	/* Use the new one if available */
	if (exec2) {
#endif
		bufmgr_gem->bufmgr.bo_exec = drm_intel_gem_bo_exec2;
		bufmgr_gem->bufmgr.bo_mrb_exec = drm_intel_gem_bo_mrb_exec2;
#ifndef __OpenBSD__
	} else
		bufmgr_gem->bufmgr.bo_exec = drm_intel_gem_bo_exec;
#endif
	bufmgr_gem->bufmgr.bo_busy = drm_intel_gem_bo_busy;
	bufmgr_gem->bufmgr.bo_madvise = drm_intel_gem_bo_madvise;
	bufmgr_gem->bufmgr.destroy = drm_intel_bufmgr_gem_unref;
	bufmgr_gem->bufmgr.debug = 0;
	bufmgr_gem->bufmgr.check_aperture_space =
	    drm_intel_gem_check_aperture_space;
	bufmgr_gem->bufmgr.bo_disable_reuse = drm_intel_gem_bo_disable_reuse;
	bufmgr_gem->bufmgr.bo_is_reusable = drm_intel_gem_bo_is_reusable;
	bufmgr_gem->bufmgr.get_pipe_from_crtc_id =
	    drm_intel_gem_get_pipe_from_crtc_id;
	bufmgr_gem->bufmgr.bo_references = drm_intel_gem_bo_references;

	init_cache_buckets(bufmgr_gem);

	DRMINITLISTHEAD(&bufmgr_gem->vma_cache);
	bufmgr_gem->vma_max = -1; /* unlimited by default */

	DRMLISTADD(&bufmgr_gem->managers, &bufmgr_list);

exit:
	pthread_mutex_unlock(&bufmgr_list_mutex);

	return bufmgr_gem != NULL ? &bufmgr_gem->bufmgr : NULL;
}
@


1.29
log
@Merge libdrm 2.4.79
@
text
@a665 1
	return (ret == 0 && busy.busy);
d3681 2
@


1.28
log
@Merge libdrm 2.4.75
@
text
@a274 14
	 * Boolean of whether this buffer can be placed in the full 48-bit
	 * address range on gen8+.
	 *
	 * By default, buffers will be keep in a 32-bit range, unless this
	 * flag is explicitly set.
	 */
	bool use_48b_address_range;

	/**
	 * Whether this buffer is softpinned at offset specified by the user
	 */
	bool is_softpin;

	/**
d429 1
a429 1
			    bo_gem->is_softpin ? "*" : "",
d443 1
a443 1
			    bo_gem->is_softpin ? "*" : "",
d462 1
a462 1
			    bo_gem->is_softpin ? "*" : "",
d534 1
a534 1
	int flags = 0;
d536 1
a538 4
	if (bo_gem->use_48b_address_range)
		flags |= EXEC_OBJECT_SUPPORTS_48B_ADDRESS;
	if (bo_gem->is_softpin)
		flags |= EXEC_OBJECT_PINNED;
d569 1
a569 1
	bufmgr_gem->exec2_objects[index].flags = flags | bo_gem->kflags;
d822 4
a837 4

		HASH_ADD(handle_hh, bufmgr_gem->handle_table,
			 gem_handle, sizeof(bo_gem->gem_handle),
			 bo_gem);
a846 1
	bo_gem->use_48b_address_range = false;
a1004 1
	bo_gem->use_48b_address_range = false;
a1151 1
	bo_gem->use_48b_address_range = false;
a1397 2
		bo_gem->kflags = 0;

d2041 5
a2045 1
	bo_gem->use_48b_address_range = enable;
d2062 1
a2062 1
	if (!target_bo_gem->is_softpin)
d2094 1
a2094 1
	if (target_bo_gem->is_softpin)
d2282 1
a2282 1
			assert(!bo_gem->is_softpin);
a2641 1
	bo_gem->is_softpin = true;
d2644 2
a2708 1
	bo_gem->use_48b_address_range = false;
@


1.27
log
@Merge libdrm 2.4.73
@
text
@d154 1
d201 2
d264 1
a264 1
	 * buffers are those that have been shared wth other
d302 1
a302 1
	/** Flags that we may need to do the SW_FINSIH ioctl on unmap. */
d585 2
a586 4
	bufmgr_gem->exec2_objects[index].offset = bo_gem->is_softpin ?
		bo->offset64 : 0;
	bufmgr_gem->exec_bos[index] = bo;
	bufmgr_gem->exec2_objects[index].flags = flags;
d589 1
d1377 1
d1418 2
d1731 1
a1731 1
	 * an open vma for every bo as that will exhaasut the system
d2393 1
d2450 8
d2463 1
a2463 1
		       DRM_IOCTL_I915_GEM_EXECBUFFER2,
d2479 3
d2507 1
a2507 1
			I915_EXEC_RENDER);
d2516 1
a2516 1
			flags);
d2523 12
a2534 1
	return do_exec2(bo, used, ctx, NULL, 0, 0, flags);
d2781 3
a2786 2
			bo_gem->global_name = flink.name;
			bo_gem->reusable = false;
d2811 53
d3283 11
d3736 4
@


1.26
log
@Merge libdrm 2.4.71
@
text
@d67 1
d136 3
a138 1
	drmMMListHead named;
d183 3
a185 1
	drmMMListHead name_list;
d221 2
a810 1
	pthread_mutex_unlock(&bufmgr_gem->lock);
d817 5
a821 1
			return NULL;
a830 2
		bo_gem->gem_handle = create.handle;
		bo_gem->bo.handle = bo_gem->gem_handle;
d833 1
a833 1
			return NULL;
d835 3
a844 4
		/* drm_intel_gem_bo_free calls DRMLISTDEL() for an uninitialized
		   list (vma_list), so better set the list head here */
		DRMINITLISTHEAD(&bo_gem->name_list);
		DRMINITLISTHEAD(&bo_gem->vma_list);
d847 6
a852 4
							 stride)) {
		    drm_intel_gem_bo_free(&bo_gem->bo);
		    return NULL;
		}
d865 1
d871 6
d977 3
d998 2
d1011 3
a1013 2
	DRMINITLISTHEAD(&bo_gem->name_list);
	DRMINITLISTHEAD(&bo_gem->vma_list);
a1015 1
	atomic_set(&bo_gem->refcount, 1);
d1024 1
a1113 1
	drmMMListHead *list;
d1122 5
a1126 9
	for (list = bufmgr_gem->named.next;
	     list != &bufmgr_gem->named;
	     list = list->next) {
		bo_gem = DRMLISTENTRY(drm_intel_bo_gem, list, name_list);
		if (bo_gem->global_name == handle) {
			drm_intel_gem_bo_reference(&bo_gem->bo);
			pthread_mutex_unlock(&bufmgr_gem->lock);
			return &bo_gem->bo;
		}
d1137 2
a1138 2
		pthread_mutex_unlock(&bufmgr_gem->lock);
		return NULL;
d1144 5
a1148 9
	for (list = bufmgr_gem->named.next;
	     list != &bufmgr_gem->named;
	     list = list->next) {
		bo_gem = DRMLISTENTRY(drm_intel_bo_gem, list, name_list);
		if (bo_gem->gem_handle == open_arg.handle) {
			drm_intel_gem_bo_reference(&bo_gem->bo);
			pthread_mutex_unlock(&bufmgr_gem->lock);
			return &bo_gem->bo;
		}
d1152 5
a1156 4
	if (!bo_gem) {
		pthread_mutex_unlock(&bufmgr_gem->lock);
		return NULL;
	}
a1163 1
	atomic_set(&bo_gem->refcount, 1);
d1171 5
d1181 3
a1183 5
	if (ret != 0) {
		drm_intel_gem_bo_unreference(&bo_gem->bo);
		pthread_mutex_unlock(&bufmgr_gem->lock);
		return NULL;
	}
d1188 1
d1190 1
a1190 2
	DRMINITLISTHEAD(&bo_gem->vma_list);
	DRMLISTADDTAIL(&bo_gem->name_list, &bufmgr_gem->named);
d1192 1
a1192 1
	DBG("bo_create_from_handle: %d (%s)\n", handle, bo_gem->name);
d1194 4
a1197 1
	return &bo_gem->bo;
d1214 5
d1224 4
d1248 3
d1315 5
d1335 2
d1349 2
a1404 2
	DRMLISTDEL(&bo_gem->name_list);

a2645 1
	drmMMListHead *list;
d2660 5
a2664 9
	for (list = bufmgr_gem->named.next;
	     list != &bufmgr_gem->named;
	     list = list->next) {
		bo_gem = DRMLISTENTRY(drm_intel_bo_gem, list, name_list);
		if (bo_gem->gem_handle == handle) {
			drm_intel_gem_bo_reference(&bo_gem->bo);
			pthread_mutex_unlock(&bufmgr_gem->lock);
			return &bo_gem->bo;
		}
d2668 6
a2673 4
	if (!bo_gem) {
		pthread_mutex_unlock(&bufmgr_gem->lock);
		return NULL;
	}
d2689 2
a2690 2

	atomic_set(&bo_gem->refcount, 1);
a2699 4
	DRMINITLISTHEAD(&bo_gem->vma_list);
	DRMLISTADDTAIL(&bo_gem->name_list, &bufmgr_gem->named);
	pthread_mutex_unlock(&bufmgr_gem->lock);

d2702 5
a2706 8
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_GET_TILING,
		       &get_tiling);
	if (ret != 0) {
		DBG("create_from_prime: failed to get tiling: %s\n", strerror(errno));
		drm_intel_gem_bo_unreference(&bo_gem->bo);
		return NULL;
	}
d2712 2
d2715 5
a2727 5
	pthread_mutex_lock(&bufmgr_gem->lock);
        if (DRMLISTEMPTY(&bo_gem->name_list))
                DRMLISTADDTAIL(&bo_gem->name_list, &bufmgr_gem->named);
	pthread_mutex_unlock(&bufmgr_gem->lock);

a2741 1
	int ret;
d2748 2
d2752 6
a2757 5

		ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_GEM_FLINK, &flink);
		if (ret != 0) {
			pthread_mutex_unlock(&bufmgr_gem->lock);
			return -errno;
a2758 6

		bo_gem->global_name = flink.name;
		bo_gem->reusable = false;

                if (DRMLISTEMPTY(&bo_gem->name_list))
                        DRMLISTADDTAIL(&bo_gem->name_list, &bufmgr_gem->named);
d3077 28
d3121 1
a3121 1
			return strtod(devid_override, NULL);
d3401 135
a3753 1
	DRMINITLISTHEAD(&bufmgr_gem->named);
@


1.25
log
@Merge libdrm 2.4.67
@
text
@d3254 30
d3428 1
a3428 1
		 * substract the unmappable part (fixed to 256MB on all known
@


1.24
log
@Merge libdrm 2.4.65
@
text
@d86 16
d205 7
d265 14
d431 3
a433 2
		if (bo_gem->relocs == NULL) {
			DBG("%2d: %d (%s)\n", i, bo_gem->gem_handle,
d443 2
a444 2
			DBG("%2d: %d (%s)@@0x%08llx -> "
			    "%d (%s)@@0x%08lx + 0x%08x\n",
d446 5
a450 2
			    bo_gem->gem_handle, bo_gem->name,
			    (unsigned long long)bo_gem->relocs[j].offset,
d453 2
a454 1
			    target_bo->offset64,
d457 16
d538 8
d548 1
a548 3
		if (need_fence)
			bufmgr_gem->exec2_objects[bo_gem->validate_index].flags |=
				EXEC_OBJECT_NEEDS_FENCE;
d575 2
a576 1
	bufmgr_gem->exec2_objects[index].offset = 0;
d578 1
a578 1
	bufmgr_gem->exec2_objects[index].flags = 0;
a580 4
	if (need_fence) {
		bufmgr_gem->exec2_objects[index].flags |=
			EXEC_OBJECT_NEEDS_FENCE;
	}
d853 1
d1000 1
d1156 1
d1332 3
d1337 1
d1351 5
a1994 8
	bo_gem->relocs[bo_gem->reloc_count].offset = offset;
	bo_gem->relocs[bo_gem->reloc_count].delta = target_offset;
	bo_gem->relocs[bo_gem->reloc_count].target_handle =
	    target_bo_gem->gem_handle;
	bo_gem->relocs[bo_gem->reloc_count].read_domains = read_domains;
	bo_gem->relocs[bo_gem->reloc_count].write_domain = write_domain;
	bo_gem->relocs[bo_gem->reloc_count].presumed_offset = target_bo->offset64;

d2004 7
d2016 45
d2067 1
d2069 6
a2074 3
	return do_bo_emit_reloc(bo, offset, target_bo, target_offset,
				read_domains, write_domain,
				!bufmgr_gem->fenced_relocs);
d2107 2
d2135 6
d2183 1
a2183 1
	if (bo_gem->relocs == NULL)
d2204 11
d2230 6
a2235 4
			DBG("BO %d (%s) migrated: 0x%08lx -> 0x%08llx\n",
			    bo_gem->gem_handle, bo_gem->name, bo->offset64,
			    (unsigned long long)bufmgr_gem->exec_objects[i].
			    offset);
d2254 10
a2263 3
			DBG("BO %d (%s) migrated: 0x%08lx -> 0x%08llx\n",
			    bo_gem->gem_handle, bo_gem->name, bo->offset64,
			    (unsigned long long)bufmgr_gem->exec2_objects[i].offset);
d2589 11
d2663 1
d2979 7
d3446 5
d3475 7
@


1.23
log
@Merge libdrm 2.4.64.  This changes the build to using the upstream
auto* build system and installing/using a libdrm specific set of drm
headers.  Thanks to Emil Velikov for removing the GNU makeisms
upstream.
@
text
@d281 5
a2134 1
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
d2138 1
a2138 1
	if (bo_gem->has_error)
d2183 1
a2183 2
		drm_intel_bo *bo = bufmgr_gem->exec_bos[i];
		drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
d2208 3
d2283 1
a2283 2
		drm_intel_bo *bo = bufmgr_gem->exec_bos[i];
		drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
d2450 1
d2452 5
a2462 1
	pthread_mutex_lock(&bufmgr_gem->lock);
a2473 6
	if (ret) {
	  fprintf(stderr,"ret is %d %d\n", ret, errno);
	  pthread_mutex_unlock(&bufmgr_gem->lock);
		return NULL;
	}

d2514 1
@


1.22
log
@update to libdrm 2.4.62
@
text
@a63 1
#include "intel_aub.h"
d84 1
a141 3
	char *aub_filename;
	FILE *aub_file;
	uint32_t aub_offset;
a256 5

	uint32_t aub_offset;

	drm_intel_aub_annotation *aub_annotations;
	unsigned aub_annotation_count;
d462 1
a462 1
	bufmgr_gem->exec_objects[index].alignment = 0;
d505 1
a505 1
	bufmgr_gem->exec2_objects[index].alignment = 0;
d523 2
a524 1
				      drm_intel_bo_gem *bo_gem)
d526 1
a526 1
	int size;
d538 1
a538 1
		int min_size;
d552 1
a552 1
		size = 2 * min_size;
d555 1
a555 1
	bo_gem->reloc_tree_size = size;
d660 2
a661 1
				unsigned long stride)
d703 1
d705 1
d762 1
a786 2
	bo_gem->aub_annotations = NULL;
	bo_gem->aub_annotation_count = 0;
d788 1
a788 1
	drm_intel_bo_gem_set_in_aperture_size(bufmgr_gem, bo_gem);
d804 2
a805 1
					       I915_TILING_NONE, 0);
d815 1
a815 1
					       I915_TILING_NONE, 0);
d867 1
a867 1
					       tiling, stride);
d897 1
a897 1
	userptr.user_ptr = (uint64_t)((unsigned long)addr);
d934 1
a934 1
	drm_intel_bo_gem_set_in_aperture_size(bufmgr_gem, bo_gem);
d963 1
a963 1
	userptr.user_ptr = (uint64_t)(unsigned long)ptr;
d1102 1
a1102 1
	drm_intel_bo_gem_set_in_aperture_size(bufmgr_gem, bo_gem);
a1138 1
	free(bo_gem->aub_annotations);
a1818 1
	free(bufmgr_gem->aub_filename);
a2115 206
static void
aub_out(drm_intel_bufmgr_gem *bufmgr_gem, uint32_t data)
{
	fwrite(&data, 1, 4, bufmgr_gem->aub_file);
}

static void
aub_out_data(drm_intel_bufmgr_gem *bufmgr_gem, void *data, size_t size)
{
	fwrite(data, 1, size, bufmgr_gem->aub_file);
}

static void
aub_write_bo_data(drm_intel_bo *bo, uint32_t offset, uint32_t size)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	uint32_t *data;
	unsigned int i;

	data = malloc(bo->size);
	drm_intel_bo_get_subdata(bo, offset, size, data);

	/* Easy mode: write out bo with no relocations */
	if (!bo_gem->reloc_count) {
		aub_out_data(bufmgr_gem, data, size);
		free(data);
		return;
	}

	/* Otherwise, handle the relocations while writing. */
	for (i = 0; i < size / 4; i++) {
		int r;
		for (r = 0; r < bo_gem->reloc_count; r++) {
			struct drm_i915_gem_relocation_entry *reloc;
			drm_intel_reloc_target *info;

			reloc = &bo_gem->relocs[r];
			info = &bo_gem->reloc_target_info[r];

			if (reloc->offset == offset + i * 4) {
				drm_intel_bo_gem *target_gem;
				uint32_t val;

				target_gem = (drm_intel_bo_gem *)info->bo;

				val = reloc->delta;
				val += target_gem->aub_offset;

				aub_out(bufmgr_gem, val);
				data[i] = val;
				break;
			}
		}
		if (r == bo_gem->reloc_count) {
			/* no relocation, just the data */
			aub_out(bufmgr_gem, data[i]);
		}
	}

	free(data);
}

static void
aub_bo_get_address(drm_intel_bo *bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	/* Give the object a graphics address in the AUB file.  We
	 * don't just use the GEM object address because we do AUB
	 * dumping before execution -- we want to successfully log
	 * when the hardware might hang, and we might even want to aub
	 * capture for a driver trying to execute on a different
	 * generation of hardware by disabling the actual kernel exec
	 * call.
	 */
	bo_gem->aub_offset = bufmgr_gem->aub_offset;
	bufmgr_gem->aub_offset += bo->size;
	/* XXX: Handle aperture overflow. */
	assert(bufmgr_gem->aub_offset < 256 * 1024 * 1024);
}

static void
aub_write_trace_block(drm_intel_bo *bo, uint32_t type, uint32_t subtype,
		      uint32_t offset, uint32_t size)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	aub_out(bufmgr_gem,
		CMD_AUB_TRACE_HEADER_BLOCK |
		((bufmgr_gem->gen >= 8 ? 6 : 5) - 2));
	aub_out(bufmgr_gem,
		AUB_TRACE_MEMTYPE_GTT | type | AUB_TRACE_OP_DATA_WRITE);
	aub_out(bufmgr_gem, subtype);
	aub_out(bufmgr_gem, bo_gem->aub_offset + offset);
	aub_out(bufmgr_gem, size);
	if (bufmgr_gem->gen >= 8)
		aub_out(bufmgr_gem, 0);
	aub_write_bo_data(bo, offset, size);
}

/**
 * Break up large objects into multiple writes.  Otherwise a 128kb VBO
 * would overflow the 16 bits of size field in the packet header and
 * everything goes badly after that.
 */
static void
aub_write_large_trace_block(drm_intel_bo *bo, uint32_t type, uint32_t subtype,
			    uint32_t offset, uint32_t size)
{
	uint32_t block_size;
	uint32_t sub_offset;

	for (sub_offset = 0; sub_offset < size; sub_offset += block_size) {
		block_size = size - sub_offset;

		if (block_size > 8 * 4096)
			block_size = 8 * 4096;

		aub_write_trace_block(bo, type, subtype, offset + sub_offset,
				      block_size);
	}
}

static void
aub_write_bo(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	uint32_t offset = 0;
	unsigned i;

	aub_bo_get_address(bo);

	/* Write out each annotated section separately. */
	for (i = 0; i < bo_gem->aub_annotation_count; ++i) {
		drm_intel_aub_annotation *annotation =
			&bo_gem->aub_annotations[i];
		uint32_t ending_offset = annotation->ending_offset;
		if (ending_offset > bo->size)
			ending_offset = bo->size;
		if (ending_offset > offset) {
			aub_write_large_trace_block(bo, annotation->type,
						    annotation->subtype,
						    offset,
						    ending_offset - offset);
			offset = ending_offset;
		}
	}

	/* Write out any remaining unannotated data */
	if (offset < bo->size) {
		aub_write_large_trace_block(bo, AUB_TRACE_TYPE_NOTYPE, 0,
					    offset, bo->size - offset);
	}
}

/*
 * Make a ringbuffer on fly and dump it
 */
static void
aub_build_dump_ringbuffer(drm_intel_bufmgr_gem *bufmgr_gem,
			  uint32_t batch_buffer, int ring_flag)
{
	uint32_t ringbuffer[4096];
	int ring = AUB_TRACE_TYPE_RING_PRB0; /* The default ring */
	int ring_count = 0;

	if (ring_flag == I915_EXEC_BSD)
		ring = AUB_TRACE_TYPE_RING_PRB1;
	else if (ring_flag == I915_EXEC_BLT)
		ring = AUB_TRACE_TYPE_RING_PRB2;

	/* Make a ring buffer to execute our batchbuffer. */
	memset(ringbuffer, 0, sizeof(ringbuffer));
	if (bufmgr_gem->gen >= 8) {
		ringbuffer[ring_count++] = AUB_MI_BATCH_BUFFER_START | (3 - 2);
		ringbuffer[ring_count++] = batch_buffer;
		ringbuffer[ring_count++] = 0;
	} else {
		ringbuffer[ring_count++] = AUB_MI_BATCH_BUFFER_START;
		ringbuffer[ring_count++] = batch_buffer;
	}

	/* Write out the ring.  This appears to trigger execution of
	 * the ring in the simulator.
	 */
	aub_out(bufmgr_gem,
		CMD_AUB_TRACE_HEADER_BLOCK |
		((bufmgr_gem->gen >= 8 ? 6 : 5) - 2));
	aub_out(bufmgr_gem,
		AUB_TRACE_MEMTYPE_GTT | ring | AUB_TRACE_OP_COMMAND_WRITE);
	aub_out(bufmgr_gem, 0); /* general/surface subtype */
	aub_out(bufmgr_gem, bufmgr_gem->aub_offset);
	aub_out(bufmgr_gem, ring_count * 4);
	if (bufmgr_gem->gen >= 8)
		aub_out(bufmgr_gem, 0);

	/* FIXME: Need some flush operations here? */
	aub_out_data(bufmgr_gem, ringbuffer, ring_count * 4);

	/* Update offset pointer */
	bufmgr_gem->aub_offset += 4096;
}

a2121 79
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
	uint32_t cpp;

	switch (format) {
	case AUB_DUMP_BMP_FORMAT_8BIT:
		cpp = 1;
		break;
	case AUB_DUMP_BMP_FORMAT_ARGB_4444:
		cpp = 2;
		break;
	case AUB_DUMP_BMP_FORMAT_ARGB_0888:
	case AUB_DUMP_BMP_FORMAT_ARGB_8888:
		cpp = 4;
		break;
	default:
		printf("Unknown AUB dump format %d\n", format);
		return;
	}

	if (!bufmgr_gem->aub_file)
		return;

	aub_out(bufmgr_gem, CMD_AUB_DUMP_BMP | 4);
	aub_out(bufmgr_gem, (y1 << 16) | x1);
	aub_out(bufmgr_gem,
		(format << 24) |
		(cpp << 19) |
		pitch / 4);
	aub_out(bufmgr_gem, (height << 16) | width);
	aub_out(bufmgr_gem, bo_gem->aub_offset + offset);
	aub_out(bufmgr_gem,
		((bo_gem->tiling_mode != I915_TILING_NONE) ? (1 << 2) : 0) |
		((bo_gem->tiling_mode == I915_TILING_Y) ? (1 << 3) : 0));
}

static void
aub_exec(drm_intel_bo *bo, int ring_flag, int used)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	int i;
	bool batch_buffer_needs_annotations;

	if (!bufmgr_gem->aub_file)
		return;

	/* If batch buffer is not annotated, annotate it the best we
	 * can.
	 */
	batch_buffer_needs_annotations = bo_gem->aub_annotation_count == 0;
	if (batch_buffer_needs_annotations) {
		drm_intel_aub_annotation annotations[2] = {
			{ AUB_TRACE_TYPE_BATCH, 0, used },
			{ AUB_TRACE_TYPE_NOTYPE, 0, bo->size }
		};
		drm_intel_bufmgr_gem_set_aub_annotations(bo, annotations, 2);
	}

	/* Write out all buffers to AUB memory */
	for (i = 0; i < bufmgr_gem->exec_count; i++) {
		aub_write_bo(bufmgr_gem->exec_bos[i]);
	}

	/* Remove any annotations we added */
	if (batch_buffer_needs_annotations)
		drm_intel_bufmgr_gem_set_aub_annotations(bo, NULL, 0);

	/* Dump ring buffer */
	aub_build_dump_ringbuffer(bufmgr_gem, bo_gem->aub_offset, ring_flag);

	fflush(bufmgr_gem->aub_file);

	/*
	 * One frame has been dumped. So reset the aub_offset for the next frame.
	 *
	 * FIXME: Can we do this?
	 */
	bufmgr_gem->aub_offset = 0x10000;
a2251 2
	aub_exec(bo, flags, used);

d2418 1
a2418 1
		drm_intel_bo_gem_set_in_aperture_size(bufmgr_gem, bo_gem);
d2516 1
a2516 1
	drm_intel_bo_gem_set_in_aperture_size(bufmgr_gem, bo_gem);
a2926 5
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bufmgr;

	free(bufmgr_gem->aub_filename);
	if (filename)
		bufmgr_gem->aub_filename = strdup(filename);
d2940 5
a2944 52
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bufmgr;
	int entry = 0x200003;
	int i;
	int gtt_size = 0x10000;
	const char *filename;

	if (!enable) {
		if (bufmgr_gem->aub_file) {
			fclose(bufmgr_gem->aub_file);
			bufmgr_gem->aub_file = NULL;
		}
		return;
	}

	if (geteuid() != getuid())
		return;

	if (bufmgr_gem->aub_filename)
		filename = bufmgr_gem->aub_filename;
	else
		filename = "intel.aub";
	bufmgr_gem->aub_file = fopen(filename, "w+");
	if (!bufmgr_gem->aub_file)
		return;

	/* Start allocating objects from just after the GTT. */
	bufmgr_gem->aub_offset = gtt_size;

	/* Start with a (required) version packet. */
	aub_out(bufmgr_gem, CMD_AUB_HEADER | (13 - 2));
	aub_out(bufmgr_gem,
		(4 << AUB_HEADER_MAJOR_SHIFT) |
		(0 << AUB_HEADER_MINOR_SHIFT));
	for (i = 0; i < 8; i++) {
		aub_out(bufmgr_gem, 0); /* app name */
	}
	aub_out(bufmgr_gem, 0); /* timestamp */
	aub_out(bufmgr_gem, 0); /* timestamp */
	aub_out(bufmgr_gem, 0); /* comment len */

	/* Set up the GTT. The max we can handle is 256M */
	aub_out(bufmgr_gem, CMD_AUB_TRACE_HEADER_BLOCK | ((bufmgr_gem->gen >= 8 ? 6 : 5) - 2));
	/* Need to use GTT_ENTRY type for recent emulator */
	aub_out(bufmgr_gem, AUB_TRACE_MEMTYPE_GTT_ENTRY | 0 | AUB_TRACE_OP_DATA_WRITE);
	aub_out(bufmgr_gem, 0); /* subtype */
	aub_out(bufmgr_gem, 0); /* offset */
	aub_out(bufmgr_gem, gtt_size); /* size */
	if (bufmgr_gem->gen >= 8)
		aub_out(bufmgr_gem, 0);
	for (i = 0x000; i < gtt_size; i += 4, entry += 0x1000) {
		aub_out(bufmgr_gem, entry);
	}
a3106 13
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	unsigned size = sizeof(*annotations) * count;
	drm_intel_aub_annotation *new_annotations =
		count > 0 ? realloc(bo_gem->aub_annotations, size) : NULL;
	if (new_annotations == NULL) {
		free(bo_gem->aub_annotations);
		bo_gem->aub_annotations = NULL;
		bo_gem->aub_annotation_count = 0;
		return;
	}
	memcpy(new_annotations, annotations, size);
	bo_gem->aub_annotations = new_annotations;
	bo_gem->aub_annotation_count = count;
@


1.21
log
@update to libdrm 2.4.60
@
text
@d59 1
a59 1
#include "libdrm.h"
d137 5
d948 64
d1018 1
a1018 1
drm_public drm_intel_bo *
d1480 1
a1480 1
drm_public int
d1539 1
a1539 1
drm_public int
d1628 1
a1628 1
drm_public int
d1753 1
a1753 1
drm_public int
d1789 1
a1789 1
drm_public void
d1816 2
a1817 1
	int i;
d1843 12
d1967 1
a1967 1
drm_public int
d1988 1
a1988 1
drm_public void
d2328 1
a2328 1
drm_public void
d2603 1
a2603 1
drm_public int
d2728 1
a2728 1
drm_public drm_intel_bo *
d2814 1
a2814 1
drm_public int
d2874 1
a2874 1
drm_public void
d2889 1
a2889 1
drm_public void
d3161 1
a3161 1
drm_public void
d3202 1
a3202 1
drm_public int
d3216 1
a3216 1
drm_public void
d3235 1
a3235 1
drm_public void
d3292 1
a3292 1
drm_public drm_intel_context *
d3319 1
a3319 1
drm_public void
d3342 1
a3342 1
drm_public int
d3376 1
a3376 1
drm_public int
d3394 1
a3394 1
drm_public int
d3410 1
a3410 1
drm_public int
d3447 1
a3447 1
drm_public void
a3501 47
static bool
has_userptr(drm_intel_bufmgr_gem *bufmgr_gem)
{
	int ret;
	void *ptr;
	long pgsz;
	struct drm_i915_gem_userptr userptr;
	struct drm_gem_close close_bo;

	pgsz = sysconf(_SC_PAGESIZE);
	assert(pgsz > 0);

	ret = posix_memalign(&ptr, pgsz, pgsz);
	if (ret) {
		DBG("Failed to get a page (%ld) for userptr detection!\n",
			pgsz);
		return false;
	}

	memclear(userptr);
	userptr.user_ptr = (uint64_t)(unsigned long)ptr;
	userptr.user_size = pgsz;

retry:
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_USERPTR, &userptr);
	if (ret) {
		if (errno == ENODEV && userptr.flags == 0) {
			userptr.flags = I915_USERPTR_UNSYNCHRONIZED;
			goto retry;
		}
		free(ptr);
		return false;
	}

	memclear(close_bo);
	close_bo.handle = userptr.handle;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_GEM_CLOSE, &close_bo);
	free(ptr);
	if (ret) {
		fprintf(stderr, "Failed to release test userptr object! (%d) "
				"i915 kernel driver may not be sane!\n", errno);
		return false;
	}

	return true;
}

d3508 1
a3508 1
drm_public drm_intel_bufmgr *
d3610 1
a3610 3
	if (has_userptr(bufmgr_gem))
		bufmgr_gem->bufmgr.bo_alloc_userptr =
			drm_intel_gem_bo_alloc_userptr;
@


1.20
log
@update to libdrm 2.4.59
@
text
@d77 1
a77 1
#define VG_CLEAR(s) VG(memset(&s, 0, sizeof(s)))
d600 1
a600 1
	VG_CLEAR(busy);
d619 1
a619 1
	VG_CLEAR(madv);
d748 1
a748 1
		VG_CLEAR(create);
d895 1
a895 1
	VG_CLEAR(userptr);
d979 1
a979 1
	VG_CLEAR(open_arg);
d1024 1
a1024 1
	VG_CLEAR(get_tiling);
d1067 1
a1067 1
	VG_CLEAR(close);
d1299 1
a1299 1
		VG_CLEAR(mmap_arg);
a1300 1
		mmap_arg.offset = 0;
d1322 1
a1322 1
	VG_CLEAR(set_domain);
d1368 1
a1368 1
		VG_CLEAR(mmap_arg);
d1436 1
a1436 1
	VG_CLEAR(set_domain);
d1535 1
a1535 1
		VG_CLEAR(sw_finish);
d1577 1
a1577 1
	VG_CLEAR(pwrite);
d1602 1
a1602 1
	VG_CLEAR(get_pipe_from_crtc_id);
d1632 1
a1632 1
	VG_CLEAR(pread);
d1680 3
d1703 1
a1705 1
	wait.flags = 0;
d1728 1
a1728 1
	VG_CLEAR(set_domain);
d2355 1
a2355 1
	VG_CLEAR(execbuf);
d2443 1
a2443 1
	VG_CLEAR(execbuf);
d2536 1
a2536 1
	VG_CLEAR(pin);
d2559 1
a2559 1
	VG_CLEAR(unpin);
d2715 1
a2715 1
	VG_CLEAR(get_tiling);
d2762 1
a2762 1
		VG_CLEAR(flink);
d3097 1
a3097 1
	int devid;
d3109 1
a3109 2
	VG_CLEAR(devid);
	VG_CLEAR(gp);
d3222 1
a3222 1
	VG_CLEAR(create);
d3247 1
a3247 1
	VG_CLEAR(destroy);
d3273 1
a3273 1
	memset(&stats, 0, sizeof(stats));
d3303 1
a3303 1
	VG_CLEAR(reg_read);
d3312 31
d3439 1
a3439 1
	memset(&userptr, 0, sizeof(userptr));
d3454 1
d3503 1
d3553 1
a3553 1
	VG_CLEAR(gp);
@


1.19
log
@update to libdrm 2.4.52
ok matthieu@@
@
text
@a50 1
#include <sys/mman.h>
d59 1
d96 2
d117 2
d191 5
d235 5
d766 4
a775 3

		DRMINITLISTHEAD(&bo_gem->name_list);
		DRMINITLISTHEAD(&bo_gem->vma_list);
d869 74
d949 1
a949 1
drm_intel_bo *
d967 1
d974 1
d987 1
d1000 1
d1006 2
a1007 1
	if (!bo_gem)
d1009 1
d1031 1
d1041 1
d1058 1
a1058 1
		munmap(bo_gem->mem_virtual, bo_gem->bo.size);
d1062 1
a1062 1
		munmap(bo_gem->gtt_virtual, bo_gem->bo.size);
d1147 1
a1147 1
			munmap(bo_gem->mem_virtual, bo_gem->bo.size);
d1152 1
a1152 1
			munmap(bo_gem->gtt_virtual, bo_gem->bo.size);
d1256 2
a1257 1
	if (atomic_dec_and_test(&bo_gem->refcount)) {
d1265 6
a1270 2
		drm_intel_gem_bo_unreference_final(bo, time.tv_sec);
		drm_intel_gem_cleanup_bo_cache(bufmgr_gem, time.tv_sec);
d1282 6
d1356 3
d1388 3
a1390 3
		bo_gem->gtt_virtual = mmap(0, bo->size, PROT_READ | PROT_WRITE,
					   MAP_SHARED, bufmgr_gem->fd,
					   mmap_arg.offset);
d1412 2
a1413 1
int drm_intel_gem_bo_map_gtt(drm_intel_bo *bo)
d1471 2
a1472 1
int drm_intel_gem_bo_map_unsynchronized(drm_intel_bo *bo)
d1505 1
a1505 1
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
d1512 5
d1560 2
a1561 1
int drm_intel_gem_bo_unmap_gtt(drm_intel_bo *bo)
d1575 3
d1630 3
d1682 2
a1683 1
int drm_intel_gem_bo_wait(drm_intel_bo *bo, int64_t timeout_ns)
d1718 1
a1718 1
void
d1821 8
d1836 1
a1837 6
	/* An object needing a fence is a tiled buffer, so it won't have
	 * relocs to other buffers.
	 */
	if (need_fence)
		target_bo_gem->reloc_tree_fences = 1;
	bo_gem->reloc_tree_fences += target_bo_gem->reloc_tree_fences;
d1883 1
a1883 1
int
d1904 1
a1904 1
void
d1907 1
d1915 1
d1917 2
d1928 3
d2244 1
a2244 1
void
d2519 1
a2519 1
int
d2613 6
d2644 1
a2644 1
drm_intel_bo *
d2661 1
d2668 1
d2675 1
d2680 2
a2681 1
	if (!bo_gem)
d2683 1
a2683 1

d2711 1
d2730 1
a2730 1
int
d2736 1
d2739 1
d2763 2
d2766 2
a2767 1
		if (ret != 0)
d2769 1
d2776 1
d2790 1
a2790 1
void
d2805 1
a2805 1
void
d3077 1
a3077 1
void
d3119 1
a3119 1
int
d3133 1
a3133 1
void
d3152 1
a3152 1
void
d3197 2
a3198 1
	aub_out(bufmgr_gem, AUB_TRACE_MEMTYPE_NONLOCAL | 0 | AUB_TRACE_OP_DATA_WRITE);
d3209 1
a3209 1
drm_intel_context *
d3236 1
a3236 1
void
d3259 1
a3259 1
int
d3293 1
a3293 1
int
d3333 1
a3333 1
void
d3353 81
d3440 1
a3440 1
drm_intel_bufmgr *
d3451 6
d3459 1
a3459 1
		return NULL;
d3462 1
d3466 2
a3467 1
		return NULL;
d3502 2
d3506 2
a3507 1
		return NULL;
d3541 4
d3631 1
a3631 1
	bufmgr_gem->bufmgr.destroy = drm_intel_bufmgr_gem_destroy;
d3647 6
a3652 1
	return &bufmgr_gem->bufmgr;
@


1.18
log
@Update libdrm to 2.4.51.

ok mpi@@ kettenis@@
@
text
@d217 9
d396 1
a396 1
			    target_bo->offset,
d583 3
d590 6
a595 1

d918 1
d1362 1
d1364 1
d1716 1
a1716 1
	bo_gem->relocs[bo_gem->reloc_count].presumed_offset = target_bo->offset;
d1870 1
a1870 1
		if (bufmgr_gem->exec_objects[i].offset != bo->offset) {
d1872 1
a1872 1
			    bo_gem->gem_handle, bo_gem->name, bo->offset,
d1875 1
d1892 1
a1892 1
		if (bufmgr_gem->exec2_objects[i].offset != bo->offset) {
d1894 1
a1894 1
			    bo_gem->gem_handle, bo_gem->name, bo->offset,
d1896 1
d2253 2
d2353 2
d2408 1
d3062 4
d3071 1
a3074 1
	context = calloc(1, sizeof(*context));
@


1.17
log
@update to libdrm 2.4.47
ok mpi@@, jsg@@
@
text
@d154 2
a870 4
	bo_gem = calloc(1, sizeof(*bo_gem));
	if (!bo_gem)
		return NULL;

a878 1
		free(bo_gem);
d881 18
d1972 1
a1972 1
		(5 - 2));
d1978 2
d2056 8
a2063 2
	ringbuffer[ring_count++] = AUB_MI_BATCH_BUFFER_START;
	ringbuffer[ring_count++] = batch_buffer;
d2070 1
a2070 1
		(5 - 2));
d2076 2
d2483 1
d2486 16
a2535 1
	DRMINITLISTHEAD(&bo_gem->name_list);
d2537 1
d2562 3
d2594 2
a2595 1
		DRMLISTADDTAIL(&bo_gem->name_list, &bufmgr_gem->named);
d3015 1
a3015 1
	aub_out(bufmgr_gem, CMD_AUB_TRACE_HEADER_BLOCK | (5 - 2));
d3020 2
d3074 34
d3225 2
@


1.16
log
@update to libdrm 2.4.46
ok mpi@@ kettenis@@
@
text
@d1329 1
d1343 1
d1345 5
d2469 11
a2479 1
	bo_gem->bo.size = size;
@


1.15
log
@remove another local change that we don't need after
the gem ioctl changes kettenis made.
ok kettenis@@
@
text
@d133 1
d1584 1
d2884 17
d2915 1
d2922 1
d2928 5
a2932 1
	bufmgr_gem->aub_file = fopen("intel.aub", "w+");
@


1.14
log
@update to libdrm 2.4.45
ok kettenis@@ mpi@@, tested by ajacoutot@@
@
text
@a1336 1
#ifndef __OpenBSD__
a1337 1
#endif
@


1.13
log
@Adjust for the changes in our implementation of the DRM_I915_GEM_MMAP and
DRM_I915_GEM_MMAP_GGT ioctls, effectively reverting a local change we made.
@
text
@d130 1
d2027 2
d2231 4
d3153 4
@


1.12
log
@update to libdrm 2.4.42

tested by matthieu@@ krw@@ mpi@@ shadchin@@ and ajacoutot@@
ok mpi@@
@
text
@a1137 7
/*
 * OpenBSD only supports GTT mapping of the backing memory, not a choice of
 * faulted gtt memory, or the backing pages. This is due to cache coherency
 * issues.
 *
 * Therefore, bo_map_gtt calls bo_map.
 */
d1182 1
a1182 1
	set_domain.read_domains = I915_GEM_DOMAIN_GTT /* XXX _CPU */;
d1184 1
a1184 1
		set_domain.write_domain = I915_GEM_DOMAIN_GTT /* XXX _CPU */;
a1267 2
	return drm_intel_gem_bo_map(bo, 1);
#ifndef __OpenBSD__
a1307 1
#endif
d1382 1
@


1.11
log
@Enable more libdrm functions.  Determine if modesetting is supported
based on a modesetting ioctl that checks the DRIVER_MODESET flag on
the kernel side instead of the sysfs approach linux takes.

ok mpi@@
@
text
@d4 1
a4 1
 * Copyright © 2007 Intel Corporation
d57 3
d64 1
d69 10
d127 1
d129 1
d131 3
d228 5
d570 1
a570 1
	memset(&busy, 0, sizeof(busy));
d584 1
d712 2
a713 1
		memset(&create, 0, sizeof(create));
d749 2
d871 1
a871 1
	memset(&open_arg, 0, sizeof(open_arg));
d894 1
a894 1
	memset(&get_tiling, 0, sizeof(get_tiling));
d925 1
d935 1
a935 1
	memset(&close, 0, sizeof(close));
d942 1
d946 14
d1088 1
d1163 1
a1163 1
		memset(&mmap_arg, 0, sizeof(mmap_arg));
d1180 1
d1187 1
d1206 2
d1213 2
a1214 1
int drm_intel_gem_bo_map_gtt(drm_intel_bo *bo)
a1215 2
	return drm_intel_gem_bo_map(bo, 1);
#ifndef __OpenBSD__
a1217 1
	struct drm_i915_gem_set_domain set_domain;
a1219 2
	pthread_mutex_lock(&bufmgr_gem->lock);

d1230 1
a1230 1
		memset(&mmap_arg, 0, sizeof(mmap_arg));
a1244 1
			pthread_mutex_unlock(&bufmgr_gem->lock);
a1260 1
			pthread_mutex_unlock(&bufmgr_gem->lock);
d1270 30
a1299 1
	/* Now move it to the GTT domain so that the CPU caches are flushed */
d1312 2
d1320 38
a1361 1
	struct drm_i915_gem_sw_finish sw_finish;
d1379 2
a1380 1
#ifndef __OpenBSD__
d1386 1
a1391 1
#endif
d1401 1
d1423 1
a1423 1
	memset(&pwrite, 0, sizeof(pwrite));
d1448 1
d1475 1
a1475 1
	memset(&pread, 0, sizeof(pread));
d1501 52
d1567 1
d1753 4
a1756 3
		if (bo_gem->reloc_target_info[i].bo != bo) {
			drm_intel_gem_bo_unreference_locked_timed(bo_gem->
								  reloc_target_info[i].bo,
d1784 2
d1811 2
d1866 281
d2169 1
d2217 3
a2219 3
drm_intel_gem_bo_mrb_exec2(drm_intel_bo *bo, int used,
			drm_clip_rect_t *cliprects, int num_cliprects, int DR4,
			unsigned int flags)
d2223 2
a2224 1
	int ret, i;
d2251 1
d2263 4
a2266 1
	execbuf.rsvd1 = 0;
d2269 5
d2291 1
d2314 18
a2331 3
	return drm_intel_gem_bo_mrb_exec2(bo, used,
					cliprects, num_cliprects, DR4,
					I915_EXEC_RENDER);
d2342 1
a2342 1
	memset(&pin, 0, sizeof(pin));
d2364 1
a2364 1
	memset(&unpin, 0, sizeof(unpin));
d2445 69
a2518 1
	struct drm_gem_flink flink;
d2522 3
a2524 1
		memset(&flink, 0, sizeof(flink));
d2530 1
d2846 202
d3091 1
a3091 7
	gp.param = I915_PARAM_CHIPSET_ID;
	gp.value = &bufmgr_gem->pci_device;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	if (ret) {
		fprintf(stderr, "get chip id failed: %d [%d]\n", ret, errno);
		fprintf(stderr, "param: %d, val: %d\n", gp.param, *gp.value);
	}
d3104 5
a3108 3
        	bufmgr_gem->gen = 7;
	else
        	assert(0);
d3119 1
d3141 4
a3144 1
#ifndef __OpenBSD__
d3147 1
a3147 2
	if (ret == -EINVAL) {
#endif
a3152 1
#ifndef __OpenBSD__
d3154 1
a3154 2
		bufmgr_gem->has_llc = ret == 0;
#endif
@


1.10
log
@Update to libdrm 2.4.31 and add the non-yet-linked libkms, prodded by jsg@@

Tested by jsg@@, ajacoutot@@, shadchin@@ and matthieu@@, ok matthieu@@
@
text
@a1330 1
#ifndef __OpenBSD__
a1349 3
#endif
	/* only needed for KMS */
	return -1;
@


1.9
log
@Update libdrm_intel to that contained in libdrm 2.4.24. Needed for
updates to the ddx and to stop mesa 7.9.2 crashing very fast.

ok matthieu@@
@
text
@d42 1
d54 1
a57 3
#if 0
#include "intel_atomic.h"
#endif
d88 3
d102 2
d112 1
d114 1
a114 1
	char fenced_relocs;
d127 1
a127 1
	int refcount;
d160 6
d174 1
a174 1
	char included_in_check_aperture;
d181 1
a181 1
	char used_as_reloc_target;
d186 1
a186 1
	char has_error;
d191 1
a191 1
	char reusable;
d207 2
a208 2
	/** Mapped address for the buffer, saved across map/unmap cycles */
	void *saved_virtual;
d289 3
a291 1
	if (*tiling_mode == I915_TILING_X)
a367 1
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
d370 1
a370 4
	/* XXX atomics */
	pthread_mutex_lock(&bufmgr_gem->lock);
	bo_gem->refcount++;
	pthread_mutex_unlock(&bufmgr_gem->lock);
d373 1
d383 39
d524 1
a524 1
		bo_gem->has_error = 1;
d609 1
a609 1
	int alloc_from_cache;
d611 1
a611 1
	int for_render = 0;
d614 1
a614 1
		for_render = 1;
d633 1
a633 1
	alloc_from_cache = 0;
d643 1
a643 1
			alloc_from_cache = 1;
d655 1
a655 1
				alloc_from_cache = 1;
d713 1
d717 1
a717 1
	bo_gem->refcount = 1;
d720 3
a722 3
	bo_gem->used_as_reloc_target = 0;
	bo_gem->has_error = 0;
	bo_gem->reusable = 1;
d781 5
a785 1
		if (tiling == I915_TILING_X)
a788 3
		/* i8xx has a interleaved 2-row tile layout */
		if (IS_GEN2(bufmgr_gem) && tiling != I915_TILING_NONE)
			height_alignment *= 2;
d859 1
a859 1
	bo_gem->refcount = 1;
d864 1
a864 1
	bo_gem->reusable = 0;
d880 1
d895 9
a903 4
	if (bo->virtual)
		munmap(bo->virtual, bo_gem->bo.size);
	else if (bo_gem->saved_virtual)
		munmap(bo_gem->saved_virtual, bo_gem->bo.size);
d946 61
d1024 1
a1024 1
	bo_gem->used_as_reloc_target = 0;
d1039 7
d1069 2
a1070 3
	/* XXX atomics */
	assert(bo_gem->refcount > 0);
	if (--bo_gem->refcount == 0)
a1075 1
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
d1078 2
a1079 3
	pthread_mutex_lock(&bufmgr_gem->lock);
	assert(bo_gem->refcount > 0);
	if (--bo_gem->refcount == 0) {
d1086 1
d1089 1
a1090 1
	pthread_mutex_unlock(&bufmgr_gem->lock);
d1098 1
a1098 1
 * Therefore, bo_map_gtt calls bo_map, and bo_unmap_gtt calls bo_unmap.
a1104 1
	struct drm_i915_gem_mmap mmap_arg;
d1109 8
a1116 5
	/* Allow recursive mapping. Mesa may recursively map buffers with
	 * nested display loops.
	 */
	if (bo_gem->saved_virtual == NULL) {
		DBG("bo_map: %d (%s)\n", bo_gem->gem_handle, bo_gem->name);
d1130 2
d1135 1
a1135 2

		bo_gem->saved_virtual = (void *)(uintptr_t)mmap_arg.addr_ptr;
d1138 2
a1139 2
	    bo_gem->saved_virtual);
	bo->virtual = bo_gem->saved_virtual;
d1142 1
a1142 1
	set_domain.read_domains = I915_GEM_DOMAIN_GTT;
d1144 1
a1144 1
		set_domain.write_domain = I915_GEM_DOMAIN_GTT;
a1150 1
		ret = -errno;
a1153 2
		pthread_mutex_unlock(&bufmgr_gem->lock);
		return ret;
d1156 3
d1167 77
d1245 1
a1245 1
	
d1250 2
d1257 34
a1290 1
	bo->virtual = NULL;
d1293 1
a1293 1
	return 0;
d1331 1
a1331 1
#if 0
d1426 3
d1464 1
a1464 1
		 int need_fence)
d1469 1
a1469 1
	int fenced_command;
d1475 1
a1475 1
		bo_gem->has_error = 1;
d1481 1
a1481 1
		need_fence = 0;
d1485 1
a1485 1
		need_fence = 0;
d1503 1
a1503 1
		target_bo_gem->used_as_reloc_target = 1;
d1554 1
a1554 1
				read_domains, write_domain, 1);
d1557 43
d1606 24
d1656 23
d1698 68
d1804 6
d1985 1
a1985 1
		bo_gem->reusable = 0;
d2006 1
a2006 1
	bufmgr_gem->bo_reuse = 1;
d2022 1
a2022 1
		bufmgr_gem->fenced_relocs = 1;
d2040 1
a2040 1
	bo_gem->included_in_check_aperture = 1;
d2088 1
a2088 1
	bo_gem->included_in_check_aperture = 0;
d2205 1
a2205 1
	bo_gem->reusable = 0;
d2288 10
d2310 4
a2313 1
	int ret;
d2350 1
a2350 1
	if (IS_GEN2(bufmgr_gem))
d2352 1
a2352 1
	else if (IS_GEN3(bufmgr_gem))
d2354 1
a2354 1
	else if (IS_GEN4(bufmgr_gem))
d2356 6
d2363 19
a2381 1
		bufmgr_gem->gen = 6;
d2395 15
d2462 13
a2474 2
	bufmgr_gem->bufmgr.bo_exec = drm_intel_gem_bo_exec2;
	bufmgr_gem->bufmgr.bo_mrb_exec = drm_intel_gem_bo_mrb_exec2;
d2489 3
@


1.8
log
@Add drm_intel_bo_is_reusable(), to check if a buffer object will be put
in the userland BO cache or not. Bump minor.

From Chris Wilson in upstream git. ok matthieu@@
@
text
@d71 2
a79 4
/* Only cache objects up to 64MB.  Bigger than that, and the rounding of the
 * size makes many operations fail that wouldn't otherwise.
 */
#define DRM_INTEL_GEM_BO_BUCKETS	14
d95 5
a99 1
	struct drm_intel_gem_bo_bucket cache_bucket[DRM_INTEL_GEM_BO_BUCKETS];
d105 4
a108 1
	char bo_reuse;
d130 1
d143 1
d211 3
a213 2
drm_intel_gem_bo_set_tiling(drm_intel_bo *bo, uint32_t * tiling_mode,
			    uint32_t stride);
d250 4
d267 1
a267 1
			    unsigned long pitch, uint32_t tiling_mode)
d275 1
a275 1
	if (tiling_mode == I915_TILING_NONE)
d278 1
a278 1
	if (tiling_mode == I915_TILING_X)
d287 8
d308 1
a308 1
	for (i = 0; i < DRM_INTEL_GEM_BO_BUCKETS; i++) {
a359 1
	assert(bo_gem->refcount > 0);
d439 17
a455 2
	if (bufmgr_gem->gen < 4 && bo_gem->tiling_mode != I915_TILING_NONE)
		size *= 2;
d500 1
a500 3
	do {
		ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_BUSY, &busy);
	} while (ret == -1 && errno == EINTR);
d514 1
a514 1
	ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_MADVISE, &madv);
d551 3
a553 1
				unsigned long flags)
d619 7
d641 3
a643 5
		do {
			ret = ioctl(bufmgr_gem->fd,
				    DRM_IOCTL_I915_GEM_CREATE,
				    &create);
		} while (ret == -1 && errno == EINTR);
d651 13
a671 2
	bo_gem->tiling_mode = I915_TILING_NONE;
	bo_gem->swizzle_mode = I915_BIT_6_SWIZZLE_NONE;
d689 2
a690 1
					       BO_ALLOC_FOR_RENDER);
d699 2
a700 1
	return drm_intel_gem_bo_alloc_internal(bufmgr, name, size, 0);
d709 2
a710 3
	drm_intel_bo *bo;
	unsigned long size, stride, aligned_y = y;
	int ret;
d712 2
a713 22
	/* If we're tiled, our allocations are in 8 or 32-row blocks,
	 * so failure to align our height means that we won't allocate
	 * enough pages.
	 *
	 * If we're untiled, we still have to align to 2 rows high
	 * because the data port accesses 2x2 blocks even if the
	 * bottom row isn't to be rendered, so failure to align means
	 * we could walk off the end of the GTT and fault.  This is
	 * documented on 965, and may be the case on older chipsets
	 * too so we try to be careful.
	 */
	if (*tiling_mode == I915_TILING_NONE)
		aligned_y = ALIGN(y, 2);
	else if (*tiling_mode == I915_TILING_X)
		aligned_y = ALIGN(y, 8);
	else if (*tiling_mode == I915_TILING_Y)
		aligned_y = ALIGN(y, 32);

	stride = x * cpp;
	stride = drm_intel_gem_bo_tile_pitch(bufmgr_gem, stride, *tiling_mode);
	size = stride * aligned_y;
	size = drm_intel_gem_bo_tile_size(bufmgr_gem, size, tiling_mode);
d715 1
a715 3
	bo = drm_intel_gem_bo_alloc_internal(bufmgr, name, size, flags);
	if (!bo)
		return NULL;
d717 13
a729 5
	ret = drm_intel_gem_bo_set_tiling(bo, tiling_mode, stride);
	if (ret != 0) {
		drm_intel_gem_bo_unreference(bo);
		return NULL;
	}
d731 14
d747 5
a751 1
	return bo;
d770 17
d794 3
a796 5
	do {
		ret = ioctl(bufmgr_gem->fd,
			    DRM_IOCTL_GEM_OPEN,
			    &open_arg);
	} while (ret == -1 && errno == EINTR);
d798 2
a799 2
		fprintf(stderr, "Couldn't reference %s handle 0x%08x: %s\n",
			name, handle, strerror(errno));
d811 1
d817 3
a819 1
	ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_GET_TILING, &get_tiling);
d826 1
d829 1
d851 1
a851 1
	ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_GEM_CLOSE, &close);
d853 2
a854 3
		fprintf(stderr,
			"DRM_IOCTL_GEM_CLOSE %d failed (%s): %s\n",
			bo_gem->gem_handle, bo_gem->name, strerror(errno));
d865 4
a868 1
	for (i = 0; i < DRM_INTEL_GEM_BO_BUCKETS; i++) {
d885 2
a894 1
	uint32_t tiling_mode;
d899 5
a903 3
		drm_intel_gem_bo_unreference_locked_timed(bo_gem->
							  reloc_target_info[i].bo,
							  time);
d921 2
a924 1
	tiling_mode = I915_TILING_NONE;
a925 1
	    drm_intel_gem_bo_set_tiling(bo, &tiling_mode, 0) == 0 &&
a933 2

		drm_intel_gem_cleanup_bo_cache(bufmgr_gem, time);
d965 1
d997 3
a999 5
		do {
			ret = ioctl(bufmgr_gem->fd,
				    DRM_IOCTL_I915_GEM_MMAP,
				    &mmap_arg);
		} while (ret == -1 && errno == EINTR);
d1002 3
a1004 4
			fprintf(stderr,
				"%s:%d: Error mapping buffer %d (%s): %s .\n",
				__FILE__, __LINE__, bo_gem->gem_handle,
				bo_gem->name, strerror(errno));
d1021 3
a1023 5
	do {
		ret = ioctl(bufmgr_gem->fd,
			    DRM_IOCTL_I915_GEM_SET_DOMAIN,
			    &set_domain);
	} while (ret == -1 && errno == EINTR);
d1026 3
a1028 3
		fprintf(stderr, "%s:%d: Error setting to CPU domain %d: %s\n",
			__FILE__, __LINE__, bo_gem->gem_handle,
			strerror(errno));
a1050 1
	assert(bo_gem->saved_virtual != NULL);
d1077 3
a1079 5
	do {
		ret = ioctl(bufmgr_gem->fd,
			    DRM_IOCTL_I915_GEM_PWRITE,
			    &pwrite);
	} while (ret == -1 && errno == EINTR);
d1082 3
a1084 4
		fprintf(stderr,
			"%s:%d: Error writing data to buffer %d: (%d %d) %s .\n",
			__FILE__, __LINE__, bo_gem->gem_handle, (int)offset,
			(int)size, strerror(errno));
d1099 3
a1101 2
	ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GET_PIPE_FROM_CRTC_ID,
		    &get_pipe_from_crtc_id);
d1132 3
a1134 5
	do {
		ret = ioctl(bufmgr_gem->fd,
			    DRM_IOCTL_I915_GEM_PREAD,
			    &pread);
	} while (ret == -1 && errno == EINTR);
d1137 3
a1139 4
		fprintf(stderr,
			"%s:%d: Error reading data from buffer %d: (%d %d) %s .\n",
			__FILE__, __LINE__, bo_gem->gem_handle, (int)offset,
			(int)size, strerror(errno));
d1145 1
a1145 1
/** Waits for all GPU rendering to the object to have completed. */
d1170 3
a1172 5
	do {
		ret = ioctl(bufmgr_gem->fd,
			    DRM_IOCTL_I915_GEM_SET_DOMAIN,
			    &set_domain);
	} while (ret == -1 && errno == EINTR);
d1174 4
a1177 5
		fprintf(stderr,
			"%s:%d: Error setting memory domains %d (%08x %08x): %s .\n",
			__FILE__, __LINE__, bo_gem->gem_handle,
			set_domain.read_domains, set_domain.write_domain,
			strerror(errno));
d1193 1
a1193 1
	for (i = 0; i < DRM_INTEL_GEM_BO_BUCKETS; i++) {
d1228 1
d1238 2
a1239 1
	if (target_bo_gem->tiling_mode == I915_TILING_NONE)
d1242 2
a1243 2
	/* We never use HW fences for rendering on 965+ */
	if (bufmgr_gem->gen >= 4)
d1261 4
a1264 1
	bo_gem->reloc_tree_size += target_bo_gem->reloc_tree_size;
a1271 3
	/* Flag the target to disallow further relocations in it. */
	target_bo_gem->used_as_reloc_target = 1;

d1281 3
a1283 2
	drm_intel_gem_bo_reference(target_bo);
	if (need_fence)
d1334 3
d1368 3
a1370 3
drm_intel_gem_bo_exec2(drm_intel_bo *bo, int used,
		       drm_clip_rect_t *cliprects, int num_cliprects,
		       int DR4)
d1376 16
d1405 1
a1405 1
	execbuf.flags = 0;
d1409 3
a1411 5
	do {
		ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_EXECBUFFER2,
			    &execbuf);
	} while (ret != 0 && errno == EINTR);

d1414 8
a1421 9
		if (ret == -ENOMEM) {
			fprintf(stderr,
				"Execbuffer fails to pin. "
				"Estimate: %u. Actual: %u. Available: %u\n",
				drm_intel_gem_estimate_batch_space(bufmgr_gem->exec_bos,
								   bufmgr_gem->exec_count),
				drm_intel_gem_compute_batch_space(bufmgr_gem->exec_bos,
								  bufmgr_gem->exec_count),
				(unsigned int) bufmgr_gem->gtt_size);
d1444 10
d1465 3
a1467 6
	do {
		ret = ioctl(bufmgr_gem->fd,
			    DRM_IOCTL_I915_GEM_PIN,
			    &pin);
	} while (ret == -1 && errno == EINTR);

d1486 1
a1486 1
	ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_UNPIN, &unpin);
d1494 3
a1496 2
drm_intel_gem_bo_set_tiling(drm_intel_bo *bo, uint32_t * tiling_mode,
			    uint32_t stride)
d1503 3
a1505 1
	if (bo_gem->global_name == 0 && *tiling_mode == bo_gem->tiling_mode)
a1508 2
	set_tiling.handle = bo_gem->gem_handle;

d1510 6
a1515 1
		set_tiling.tiling_mode = *tiling_mode;
d1521 23
a1543 5
	} while (ret == -1 && errno == EINTR);
	if (ret == 0) {
		bo_gem->tiling_mode = set_tiling.tiling_mode;
		bo_gem->swizzle_mode = set_tiling.swizzle_mode;
	}
d1545 3
a1547 1
	drm_intel_bo_gem_set_in_aperture_size(bufmgr_gem, bo_gem);
d1550 1
a1550 1
	return ret == 0 ? 0 : -errno;
d1576 1
a1576 1
		ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_GEM_FLINK, &flink);
d1581 2
d1821 2
d1844 39
d1895 1
a1895 2
	int ret, i;
	unsigned long size;
d1908 3
a1910 1
	ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_GET_APERTURE, &aperture);
d1926 1
a1926 1
	ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
d1941 12
d1956 1
a1956 1
		ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
d2006 1
d2019 2
a2020 5
	/* Initialize the linked lists for BO reuse cache. */
	for (i = 0, size = 4096; i < DRM_INTEL_GEM_BO_BUCKETS; i++, size *= 2) {
		DRMINITLISTHEAD(&bufmgr_gem->cache_bucket[i].head);
		bufmgr_gem->cache_bucket[i].size = size;
	}
@


1.7
log
@*sigh* ioctls do not copyout when they return an error. DRM developers
need to learn this.

When set_tiling fails, keep tiling type at the type we already had, do
NOT use the parameters that we passed to the ioctl.

Noticed when i botched a kernel diff (locally) so that that ioctl failed
more often.

ok matthieu@@
@
text
@d1081 1
a1081 1
	drm_intel_gem_bo_start_gtt_access(bo, 0);
d1686 8
d1840 1
@


1.6
log
@don't allocate an array of pointers to structs when you want an array of
structs. Fixes cases where you need a lot of relocs so you overflow the
array.

Commited upstream about ten minutes ago. ok matthieu@@
@
text
@d1425 4
a1428 2
	bo_gem->tiling_mode = set_tiling.tiling_mode;
	bo_gem->swizzle_mode = set_tiling.swizzle_mode;
@


1.5
log
@update libdrm to 2.4.15.

This diff seems larger than it is since upstream reindented some
sources.  This updates libdrm_intel to -current (which only affect the
GEM code) and libdrm to 2.4.15.  bumping both minor versions.

Discussed with matthieu@@
@
text
@d439 1
a439 1
					   sizeof(drm_intel_reloc_target *));
@


1.4
log
@Some improvements from upstream for the GEM buffer object cache to stop the
cache growing stupidly big (like it used to).

ok matthieu@@ ages ago.
@
text
@d56 3
d67 2
a68 2
   if (bufmgr_gem->bufmgr.debug)			\
      fprintf(stderr, __VA_ARGS__);			\
d74 2
a75 11
   drmMMListHead head;

   /**
    * Limit on the number of entries in this bucket.
    *
    * 0 means that this caching at this bucket size is disabled.
    * -1 means that there is no limit to caching at this size.
    */
   int max_entries;
   int num_entries;
   unsigned long size;
d79 1
a79 1
 * size makes many operation fail that wouldn't otherwise.
d83 1
a83 1
    drm_intel_bufmgr bufmgr;
d85 1
a85 1
    int fd;
d87 1
a87 1
    int max_relocs;
d89 1
a89 1
    pthread_mutex_t lock;
d91 4
a94 4
    struct drm_i915_gem_exec_object *exec_objects;
    drm_intel_bo **exec_bos;
    int exec_size;
    int exec_count;
d96 2
a97 2
    /** Array of lists of cached gem objects of power-of-two sizes */
    struct drm_intel_gem_bo_bucket cache_bucket[DRM_INTEL_GEM_BO_BUCKETS];
d99 6
a104 3
    uint64_t gtt_size;
    int available_fences;
    int pci_device;
d107 7
d115 42
a156 1
    drm_intel_bo bo;
d158 6
a163 66
    int refcount;
    /** Boolean whether the mmap ioctl has been called for this buffer yet. */
    uint32_t gem_handle;
    const char *name;

    /**
     * Kenel-assigned global name for this object
     */
    unsigned int global_name;
    
    /**
     * Index of the buffer within the validation list while preparing a
     * batchbuffer execution.
     */
    int validate_index;

    /**
     * Boolean whether we've started swrast
     * Set when the buffer has been mapped
     * Cleared when the buffer is unmapped
     */
    int swrast;

    /**
     * Current tiling mode
     */
    uint32_t tiling_mode;
    uint32_t swizzle_mode;

    time_t free_time;

    /** Array passed to the DRM containing relocation information. */
    struct drm_i915_gem_relocation_entry *relocs;
    /** Array of bos corresponding to relocs[i].target_handle */
    drm_intel_bo **reloc_target_bo;
    /** Number of entries in relocs */
    int reloc_count;

    /** BO cache list */
    drmMMListHead head;

    /**
     * Boolean of whether this BO and its children have been included in
     * the current drm_intel_bufmgr_check_aperture_space() total.
     */
    char included_in_check_aperture;

    /**
     * Boolean of whether this buffer has been used as a relocation
     * target and had its size accounted for, and thus can't have any
     * further relocations added to it.
     */
     char used_as_reloc_target;

    /**
     * Size in bytes of this buffer and its relocation descendents.
     *
     * Used to avoid costly tree walking in drm_intel_bufmgr_check_aperture in
     * the common case.
     */
    int reloc_tree_size;
    /**
     * Number of potential fence registers required by this buffer and its
     * relocations.
     */
    int reloc_tree_fences;
d165 26
a190 3
#ifndef INTEL_ALWAYS_UNMAP
    void *saved_virtual;
#endif
a192 2
static void drm_intel_gem_bo_reference_locked(drm_intel_bo *bo);

d194 1
a194 1
drm_intel_gem_estimate_batch_space(drm_intel_bo **bo_array, int count);
d197 1
a197 1
drm_intel_gem_compute_batch_space(drm_intel_bo **bo_array, int count);
d200 2
a201 2
drm_intel_gem_bo_get_tiling(drm_intel_bo *bo, uint32_t *tiling_mode,
			    uint32_t *swizzle_mode);
d204 1
a204 1
drm_intel_gem_bo_set_tiling(drm_intel_bo *bo, uint32_t *tiling_mode,
d207 74
a280 2
static void
drm_intel_gem_bo_unreference(drm_intel_bo *bo);
d286 1
a286 1
    int i;
d288 6
a293 4
    for (i = 0; i < DRM_INTEL_GEM_BO_BUCKETS; i++) {
	struct drm_intel_gem_bo_bucket *bucket = &bufmgr_gem->cache_bucket[i];
	if (bucket->size >= size) {
	    return bucket;
a294 1
    }
d296 1
a296 1
    return NULL;
d299 2
a300 1
static void drm_intel_gem_dump_validation_list(drm_intel_bufmgr_gem *bufmgr_gem)
d302 29
a330 1
    int i, j;
d332 5
a336 3
    for (i = 0; i < bufmgr_gem->exec_count; i++) {
	drm_intel_bo *bo = bufmgr_gem->exec_bos[i];
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
d338 5
a342 17
	if (bo_gem->relocs == NULL) {
	    DBG("%2d: %d (%s)\n", i, bo_gem->gem_handle, bo_gem->name);
	    continue;
	}

	for (j = 0; j < bo_gem->reloc_count; j++) {
	    drm_intel_bo *target_bo = bo_gem->reloc_target_bo[j];
	    drm_intel_bo_gem *target_gem = (drm_intel_bo_gem *)target_bo;

	    DBG("%2d: %d (%s)@@0x%08llx -> %d (%s)@@0x%08lx + 0x%08x\n",
		i,
		bo_gem->gem_handle, bo_gem->name,
		(unsigned long long)bo_gem->relocs[j].offset,
		target_gem->gem_handle, target_gem->name, target_bo->offset,
		bo_gem->relocs[j].delta);
	}
    }
d354 1
a354 1
drm_intel_add_validate_buffer(drm_intel_bo *bo)
d356 44
a399 34
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
    int index;

    if (bo_gem->validate_index != -1)
	return;

    /* Extend the array of validation entries as necessary. */
    if (bufmgr_gem->exec_count == bufmgr_gem->exec_size) {
	int new_size = bufmgr_gem->exec_size * 2;

	if (new_size == 0)
	    new_size = 5;

	bufmgr_gem->exec_objects =
	    realloc(bufmgr_gem->exec_objects,
		    sizeof(*bufmgr_gem->exec_objects) * new_size);
	bufmgr_gem->exec_bos =
	    realloc(bufmgr_gem->exec_bos,
		    sizeof(*bufmgr_gem->exec_bos) * new_size);
	bufmgr_gem->exec_size = new_size;
    }

    index = bufmgr_gem->exec_count;
    bo_gem->validate_index = index;
    /* Fill in array entry */
    bufmgr_gem->exec_objects[index].handle = bo_gem->gem_handle;
    bufmgr_gem->exec_objects[index].relocation_count = bo_gem->reloc_count;
    bufmgr_gem->exec_objects[index].relocs_ptr = (uintptr_t)bo_gem->relocs;
    bufmgr_gem->exec_objects[index].alignment = 0;
    bufmgr_gem->exec_objects[index].offset = 0;
    bufmgr_gem->exec_bos[index] = bo;
    drm_intel_gem_bo_reference_locked(bo);
    bufmgr_gem->exec_count++;
a401 1

d405 21
d429 16
a444 2
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
d446 5
a450 4
    bo_gem->relocs = malloc(bufmgr_gem->max_relocs *
			    sizeof(struct drm_i915_gem_relocation_entry));
    bo_gem->reloc_target_bo = malloc(bufmgr_gem->max_relocs *
				     sizeof(drm_intel_bo *));
d452 1
a452 1
    return 0;
d455 5
a459 30
static drm_intel_bo *
drm_intel_gem_bo_alloc_internal(drm_intel_bufmgr *bufmgr, const char *name,
				unsigned long size, unsigned int alignment,
				int for_render)
{
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bufmgr;
    drm_intel_bo_gem *bo_gem;
    unsigned int page_size = getpagesize();
    int ret;
    struct drm_intel_gem_bo_bucket *bucket;
    int alloc_from_cache = 0;
    unsigned long bo_size;

    /* Round the allocated size up to a power of two number of pages. */
    bucket = drm_intel_gem_bo_bucket_for_size(bufmgr_gem, size);

    /* If we don't have caching at this size, don't actually round the
     * allocation up.
     */
    if (bucket == NULL || bucket->max_entries == 0) {
	bo_size = size;
	if (bo_size < page_size)
	    bo_size = page_size;
    } else {
	bo_size = bucket->size;
    }

    pthread_mutex_lock(&bufmgr_gem->lock);
    /* Get a buffer out of the cache if available */
    if (bucket != NULL && bucket->num_entries > 0) {
d461 22
d484 2
a485 17
	if (for_render) {
	    /* Allocate new render-target BOs from the tail (MRU)
	     * of the list, as it will likely be hot in the GPU cache
	     * and in the aperture for us.
	     */
	    bo_gem = DRMLISTENTRY(drm_intel_bo_gem, bucket->head.prev, head);
	    DRMLISTDEL(&bo_gem->head);
	    bucket->num_entries--;
	    alloc_from_cache = 1;
	} else {
	    /* For non-render-target BOs (where we're probably going to map it
	     * first thing in order to fill it with data), check if the
	     * last BO in the cache is unbusy, and only reuse in that case.
	     * Otherwise, allocating a new buffer is probably faster than
	     * waiting for the GPU to finish.
	     */
	    bo_gem = DRMLISTENTRY(drm_intel_bo_gem, bucket->head.next, head);
d487 8
a494 2
	    memset(&busy, 0, sizeof(busy));
	    busy.handle = bo_gem->gem_handle;
d496 13
a508 2
	    ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_BUSY, &busy);
	    alloc_from_cache = (ret == 0 && busy.busy == 0);
a509 1
	    if (alloc_from_cache) {
d511 1
a511 2
		bucket->num_entries--;
	    }
d513 19
a531 2
    }
    pthread_mutex_unlock(&bufmgr_gem->lock);
d533 2
a534 2
    if (!alloc_from_cache) {
	struct drm_i915_gem_create create;
d536 55
a590 3
	bo_gem = calloc(1, sizeof(*bo_gem));
	if (!bo_gem)
	    return NULL;
d592 20
a611 10
	bo_gem->bo.size = bo_size;
	memset(&create, 0, sizeof(create));
	create.size = bo_size;

	ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_CREATE, &create);
	bo_gem->gem_handle = create.handle;
	bo_gem->bo.handle = bo_gem->gem_handle;
	if (ret != 0) {
	    free(bo_gem);
	    return NULL;
a612 2
	bo_gem->bo.bufmgr = bufmgr;
    }
d614 11
a624 8
    bo_gem->name = name;
    bo_gem->refcount = 1;
    bo_gem->validate_index = -1;
    bo_gem->reloc_tree_size = bo_gem->bo.size;
    bo_gem->reloc_tree_fences = 0;
    bo_gem->used_as_reloc_target = 0;
    bo_gem->tiling_mode = I915_TILING_NONE;
    bo_gem->swizzle_mode = I915_BIT_6_SWIZZLE_NONE;
d626 2
a627 2
    DBG("bo_create: buf %d (%s) %ldb\n",
	bo_gem->gem_handle, bo_gem->name, size);
d629 1
a629 1
    return &bo_gem->bo;
d633 4
a636 2
drm_intel_gem_bo_alloc_for_render(drm_intel_bufmgr *bufmgr, const char *name,
				  unsigned long size, unsigned int alignment)
d638 2
a639 1
    return drm_intel_gem_bo_alloc_internal(bufmgr, name, size, alignment, 1);
d643 4
a646 2
drm_intel_gem_bo_alloc(drm_intel_bufmgr *bufmgr, const char *name,
		       unsigned long size, unsigned int alignment)
d648 49
a696 1
    return drm_intel_gem_bo_alloc_internal(bufmgr, name, size, alignment, 0);
d706 2
a707 1
drm_intel_bo_gem_create_from_name(drm_intel_bufmgr *bufmgr, const char *name,
d710 5
a714 5
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bufmgr;
    drm_intel_bo_gem *bo_gem;
    int ret;
    struct drm_gem_open open_arg;
    struct drm_i915_gem_get_tiling get_tiling;
d716 3
a718 3
    bo_gem = calloc(1, sizeof(*bo_gem));
    if (!bo_gem)
	return NULL;
d720 34
a753 32
    memset(&open_arg, 0, sizeof(open_arg));
    open_arg.name = handle;
    ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_GEM_OPEN, &open_arg);
    if (ret != 0) {
	fprintf(stderr, "Couldn't reference %s handle 0x%08x: %s\n",
	       name, handle, strerror(errno));
	free(bo_gem);
	return NULL;
    }
    bo_gem->bo.size = open_arg.size;
    bo_gem->bo.offset = 0;
    bo_gem->bo.virtual = NULL;
    bo_gem->bo.bufmgr = bufmgr;
    bo_gem->name = name;
    bo_gem->refcount = 1;
    bo_gem->validate_index = -1;
    bo_gem->gem_handle = open_arg.handle;
    bo_gem->global_name = handle;

    memset(&get_tiling, 0, sizeof(get_tiling));
    get_tiling.handle = bo_gem->gem_handle;
    ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_GET_TILING, &get_tiling);
    if (ret != 0) {
	drm_intel_gem_bo_unreference(&bo_gem->bo);
	return NULL;
    }
    bo_gem->tiling_mode = get_tiling.tiling_mode;
    bo_gem->swizzle_mode = get_tiling.swizzle_mode;
    if (bo_gem->tiling_mode == I915_TILING_NONE)
	bo_gem->reloc_tree_fences = 0;
    else
	bo_gem->reloc_tree_fences = 1;
d755 1
a755 1
    DBG("bo_create_from_handle: %d (%s)\n", handle, bo_gem->name);
d757 1
a757 22
    return &bo_gem->bo;
}

static void
drm_intel_gem_bo_reference(drm_intel_bo *bo)
{
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;

    assert(bo_gem->refcount > 0);
    pthread_mutex_lock(&bufmgr_gem->lock);
    bo_gem->refcount++;
    pthread_mutex_unlock(&bufmgr_gem->lock);
}

static void
drm_intel_gem_bo_reference_locked(drm_intel_bo *bo)
{
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;

    assert(bo_gem->refcount > 0);
    bo_gem->refcount++;
d763 20
a782 22
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
    struct drm_gem_close close;
    int ret;

    if (bo->virtual)
	munmap (bo->virtual, bo_gem->bo.size);
#ifndef INTEL_ALWAYS_UNMAP
    else if (bo_gem->saved_virtual)
	munmap(bo_gem->saved_virtual, bo->size);
#endif

    /* Close this object */
    memset(&close, 0, sizeof(close));
    close.handle = bo_gem->gem_handle;
    ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_GEM_CLOSE, &close);
    if (ret != 0) {
	fprintf(stderr,
		"DRM_IOCTL_GEM_CLOSE %d failed (%s): %s\n",
		bo_gem->gem_handle, bo_gem->name, strerror(errno));
    }
    free(bo);
d789 1
a789 1
    int i;
d791 3
a793 2
    for (i = 0; i < DRM_INTEL_GEM_BO_BUCKETS; i++) {
	struct drm_intel_gem_bo_bucket *bucket = &bufmgr_gem->cache_bucket[i];
d795 2
a796 2
	while (!DRMLISTEMPTY(&bucket->head)) {
	    drm_intel_bo_gem *bo_gem;
d798 4
a801 3
	    bo_gem = DRMLISTENTRY(drm_intel_bo_gem, bucket->head.next, head);
	    if (time - bo_gem->free_time <= 1)
		break;
d803 1
a803 2
	    DRMLISTDEL(&bo_gem->head);
	    bucket->num_entries--;
d805 2
a806 1
	    drm_intel_gem_bo_free(&bo_gem->bo);
a807 1
    }
d811 1
a811 1
drm_intel_gem_bo_unreference_locked(drm_intel_bo *bo)
d813 2
a814 5
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;

    assert(bo_gem->refcount > 0);
    if (--bo_gem->refcount == 0) {
d817 1
d819 5
a823 8
	if (bo_gem->relocs != NULL) {
	    int i;

	    /* Unreference all the target buffers */
	    for (i = 0; i < bo_gem->reloc_count; i++)
		 drm_intel_gem_bo_unreference_locked(bo_gem->reloc_target_bo[i]);
	    free(bo_gem->reloc_target_bo);
	    free(bo_gem->relocs);
d825 2
d831 10
d844 8
a851 17
	if (bo_gem->global_name == 0 &&
	    bucket != NULL &&
	    (bucket->max_entries == -1 ||
	     (bucket->max_entries > 0 &&
	      bucket->num_entries < bucket->max_entries)) &&
	    drm_intel_gem_bo_set_tiling(bo, &tiling_mode, 0) == 0)
	{
	    struct timespec time;

	    clock_gettime(CLOCK_MONOTONIC, &time);
	    bo_gem->free_time = time.tv_sec;

	    bo_gem->name = NULL;
	    bo_gem->validate_index = -1;
	    bo_gem->relocs = NULL;
	    bo_gem->reloc_target_bo = NULL;
	    bo_gem->reloc_count = 0;
d853 1
a853 2
	    DRMLISTADDTAIL(&bo_gem->head, &bucket->head);
	    bucket->num_entries++;
d855 1
a855 1
	    drm_intel_gem_cleanup_bo_cache(bufmgr_gem, time.tv_sec);
d857 1
a857 1
	    drm_intel_gem_bo_free(bo);
a858 1
    }
d861 2
a862 2
static void
drm_intel_gem_bo_unreference(drm_intel_bo *bo)
d864 1
a864 1
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
d866 4
a869 3
    pthread_mutex_lock(&bufmgr_gem->lock);
    drm_intel_gem_bo_unreference_locked(bo);
    pthread_mutex_unlock(&bufmgr_gem->lock);
d872 1
a872 2
static int
drm_intel_gem_bo_map(drm_intel_bo *bo, int write_enable)
d874 15
a888 1
    return drm_intel_gem_bo_map_gtt(bo);
d891 8
a898 2
int
drm_intel_gem_bo_map_gtt(drm_intel_bo *bo)
d900 7
a906 4
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
    struct drm_i915_gem_set_domain set_domain;
    int ret;
d908 5
a912 6
    pthread_mutex_lock(&bufmgr_gem->lock);
#ifndef INTEL_ALWAYS_UNMAP
    if (bo_gem->saved_virtual == NULL) {
#endif
    assert(bo->virtual == NULL);
    struct drm_i915_gem_mmap mmap_arg;
d914 18
a931 1
    DBG("bo_map_gtt: %d (%s)\n", bo_gem->gem_handle, bo_gem->name);
d933 4
a936 20
    memset(&mmap_arg, 0, sizeof(mmap_arg));
    mmap_arg.handle = bo_gem->gem_handle;
    mmap_arg.size = bo->size;

    ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_MMAP, &mmap_arg);
    if (ret != 0) {
        fprintf(stderr, "%s:%d: Error preparing buffer map %d (%s): %s .\n",
          __FILE__, __LINE__, bo_gem->gem_handle,
          bo_gem->name, strerror(errno));
        pthread_mutex_unlock(&bufmgr_gem->lock);
        return ret;
    }

    bo->virtual = (void *)(uintptr_t)mmap_arg.addr_ptr;

    DBG("bo_map: %d (%s) -> %p\n", bo_gem->gem_handle, bo_gem->name,
	bo->virtual);
#ifndef INTEL_ALWAYS_UNMAP
        bo_gem->saved_virtual = bo->virtual;
    } else
a937 1
#endif
d939 21
a959 1
    pthread_mutex_unlock(&bufmgr_gem->lock);
d961 1
a961 1
    return 0;
d964 1
a964 2
int
drm_intel_gem_bo_unmap_gtt(drm_intel_bo *bo)
d966 15
a980 2
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
a981 1
    if (bo == NULL)
d983 1
d985 3
a987 12
    assert(bo->virtual != NULL);
    pthread_mutex_lock(&bufmgr_gem->lock);
#ifdef INTEL_ALWAYS_UNMAP
    munmap(bo->virtual, bo->size);
#else
    assert(bo_gem->saved_virtual != NULL &&
      bo_gem->saved_virtual == bo->virtual);
#endif
    bo->virtual = NULL;
    pthread_mutex_unlock(&bufmgr_gem->lock);

    return 0;
d991 2
a992 1
drm_intel_gem_bo_unmap(drm_intel_bo *bo)
d994 24
a1017 1
    return drm_intel_gem_bo_unmap_gtt(bo);
d1021 24
a1044 23
drm_intel_gem_bo_subdata (drm_intel_bo *bo, unsigned long offset,
			  unsigned long size, const void *data)
{
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
    struct drm_i915_gem_pwrite pwrite;
    int ret;

    memset (&pwrite, 0, sizeof (pwrite));
    pwrite.handle = bo_gem->gem_handle;
    pwrite.offset = offset;
    pwrite.size = size;
    pwrite.data_ptr = (uint64_t) (uintptr_t) data;
    do {
	ret = ioctl (bufmgr_gem->fd, DRM_IOCTL_I915_GEM_PWRITE, &pwrite);
    } while (ret == -1 && errno == EINTR);
    if (ret != 0) {
	fprintf (stderr, "%s:%d: Error writing data to buffer %d: (%d %d) %s .\n",
		 __FILE__, __LINE__,
		 bo_gem->gem_handle, (int) offset, (int) size,
		 strerror (errno));
    }
    return 0;
d1048 27
a1074 23
drm_intel_gem_bo_get_subdata (drm_intel_bo *bo, unsigned long offset,
			      unsigned long size, void *data)
{
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
    struct drm_i915_gem_pread pread;
    int ret;

    memset (&pread, 0, sizeof (pread));
    pread.handle = bo_gem->gem_handle;
    pread.offset = offset;
    pread.size = size;
    pread.data_ptr = (uint64_t) (uintptr_t) data;
    do {
	ret = ioctl (bufmgr_gem->fd, DRM_IOCTL_I915_GEM_PREAD, &pread);
    } while (ret == -1 && errno == EINTR);
    if (ret != 0) {
	fprintf (stderr, "%s:%d: Error reading data from buffer %d: (%d %d) %s .\n",
		 __FILE__, __LINE__,
		 bo_gem->gem_handle, (int) offset, (int) size,
		 strerror (errno));
    }
    return 0;
d1081 1
a1081 1
    drm_intel_gem_bo_start_gtt_access(bo, 0);
d1094 20
a1113 17
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
    struct drm_i915_gem_set_domain set_domain;
    int ret;

    set_domain.handle = bo_gem->gem_handle;
    set_domain.read_domains = I915_GEM_DOMAIN_GTT;
    set_domain.write_domain = write_enable ? I915_GEM_DOMAIN_GTT : 0;
    do {
	ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_SET_DOMAIN, &set_domain);
    } while (ret == -1 && errno == EINTR);
    if (ret != 0) {
	fprintf (stderr, "%s:%d: Error setting memory domains %d (%08x %08x): %s .\n",
		 __FILE__, __LINE__,
		 bo_gem->gem_handle, set_domain.read_domains, set_domain.write_domain,
		 strerror (errno));
    }
d1119 2
a1120 2
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bufmgr;
    int i;
d1122 2
a1123 2
    free(bufmgr_gem->exec_objects);
    free(bufmgr_gem->exec_bos);
d1125 1
a1125 1
    pthread_mutex_destroy(&bufmgr_gem->lock);
d1127 10
a1136 4
    /* Free any cached buffer objects we were going to reuse */
    for (i = 0; i < DRM_INTEL_GEM_BO_BUCKETS; i++) {
	struct drm_intel_gem_bo_bucket *bucket = &bufmgr_gem->cache_bucket[i];
	drm_intel_bo_gem *bo_gem;
d1138 2
a1139 6
	while (!DRMLISTEMPTY(&bucket->head)) {
	    bo_gem = DRMLISTENTRY(drm_intel_bo_gem, bucket->head.next, head);
	    DRMLISTDEL(&bo_gem->head);
	    bucket->num_entries--;

	    drm_intel_gem_bo_free(&bo_gem->bo);
a1140 1
    }
d1142 1
a1142 1
    free(bufmgr);
d1155 16
a1170 7
drm_intel_gem_bo_emit_reloc(drm_intel_bo *bo, uint32_t offset,
			    drm_intel_bo *target_bo, uint32_t target_offset,
			    uint32_t read_domains, uint32_t write_domain)
{
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
    drm_intel_bo_gem *target_bo_gem = (drm_intel_bo_gem *)target_bo;
d1172 2
a1173 1
    pthread_mutex_lock(&bufmgr_gem->lock);
d1175 14
a1188 3
    /* Create a new relocation list if needed */
    if (bo_gem->relocs == NULL)
	drm_intel_setup_reloc_list(bo);
d1190 30
a1219 2
    /* Check overflow */
    assert(bo_gem->reloc_count < bufmgr_gem->max_relocs);
d1221 1
a1221 3
    /* Check args */
    assert (offset <= bo->size - 4);
    assert ((write_domain & (write_domain-1)) == 0);
d1223 2
a1224 6
    /* Make sure that we're not adding a reloc to something whose size has
     * already been accounted for.
     */
    assert(!bo_gem->used_as_reloc_target);
    bo_gem->reloc_tree_size += target_bo_gem->reloc_tree_size;
    bo_gem->reloc_tree_fences += target_bo_gem->reloc_tree_fences;
d1226 6
a1231 2
    /* Flag the target to disallow further relocations in it. */
    target_bo_gem->used_as_reloc_target = 1;
d1233 4
a1236 7
    bo_gem->relocs[bo_gem->reloc_count].offset = offset;
    bo_gem->relocs[bo_gem->reloc_count].delta = target_offset;
    bo_gem->relocs[bo_gem->reloc_count].target_handle =
	target_bo_gem->gem_handle;
    bo_gem->relocs[bo_gem->reloc_count].read_domains = read_domains;
    bo_gem->relocs[bo_gem->reloc_count].write_domain = write_domain;
    bo_gem->relocs[bo_gem->reloc_count].presumed_offset = target_bo->offset;
d1238 8
a1245 8
    bo_gem->reloc_target_bo[bo_gem->reloc_count] = target_bo;
    drm_intel_gem_bo_reference_locked(target_bo);

    bo_gem->reloc_count++;

    pthread_mutex_unlock(&bufmgr_gem->lock);

    return 0;
d1254 1
a1254 1
drm_intel_gem_bo_process_reloc(drm_intel_bo *bo)
d1256 5
a1260 2
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
    int i;
d1262 3
a1264 2
    if (bo_gem->relocs == NULL)
	return;
d1266 2
a1267 2
    for (i = 0; i < bo_gem->reloc_count; i++) {
	drm_intel_bo *target_bo = bo_gem->reloc_target_bo[i];
d1269 2
a1270 2
	/* Continue walking the tree depth-first. */
	drm_intel_gem_bo_process_reloc(target_bo);
d1272 3
a1274 3
	/* Add the target to the validate list */
	drm_intel_add_validate_buffer(target_bo);
    }
d1278 1
a1278 1
drm_intel_update_buffer_offsets (drm_intel_bufmgr_gem *bufmgr_gem)
d1280 1
a1280 1
    int i;
d1282 12
a1293 12
    for (i = 0; i < bufmgr_gem->exec_count; i++) {
	drm_intel_bo *bo = bufmgr_gem->exec_bos[i];
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;

	/* Update the buffer offset */
	if (bufmgr_gem->exec_objects[i].offset != bo->offset) {
	    DBG("BO %d (%s) migrated: 0x%08lx -> 0x%08llx\n",
		bo_gem->gem_handle, bo_gem->name, bo->offset,
		(unsigned long long)bufmgr_gem->exec_objects[i].offset);
	    bo->offset = bufmgr_gem->exec_objects[i].offset;
	}
    }
d1297 11
a1307 39
drm_intel_gem_bo_exec(drm_intel_bo *bo, int used,
		      drm_clip_rect_t *cliprects, int num_cliprects,
		      int DR4)
{
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
    struct drm_i915_gem_execbuffer execbuf;
    int ret, i;

    pthread_mutex_lock(&bufmgr_gem->lock);
    /* Update indices and set up the validate list. */
    drm_intel_gem_bo_process_reloc(bo);

    /* Add the batch buffer to the validation list.  There are no relocations
     * pointing to it.
     */
    drm_intel_add_validate_buffer(bo);

    execbuf.buffers_ptr = (uintptr_t)bufmgr_gem->exec_objects;
    execbuf.buffer_count = bufmgr_gem->exec_count;
    execbuf.batch_start_offset = 0;
    execbuf.batch_len = used;
    execbuf.cliprects_ptr = (uintptr_t)cliprects;
    execbuf.num_cliprects = num_cliprects;
    execbuf.DR1 = 0;
    execbuf.DR4 = DR4;

    do {
	ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_EXECBUFFER, &execbuf);
    } while (ret != 0 && errno == EAGAIN);

    if (ret != 0 && errno == ENOMEM) {
	fprintf(stderr, "Execbuffer fails to pin. Estimate: %u. Actual: %u. Available: %u\n",
		drm_intel_gem_estimate_batch_space(bufmgr_gem->exec_bos,
						   bufmgr_gem->exec_count),
		drm_intel_gem_compute_batch_space(bufmgr_gem->exec_bos,
						  bufmgr_gem->exec_count),
		(unsigned int) bufmgr_gem->gtt_size);
    }
    drm_intel_update_buffer_offsets (bufmgr_gem);
d1309 17
a1325 2
    if (bufmgr_gem->bufmgr.debug)
	drm_intel_gem_dump_validation_list(bufmgr_gem);
d1327 14
a1340 3
    for (i = 0; i < bufmgr_gem->exec_count; i++) {
	drm_intel_bo *bo = bufmgr_gem->exec_bos[i];
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
d1342 2
a1343 2
	/* Need to call swrast on next bo_map */
	bo_gem->swrast = 0;
d1345 10
a1354 7
	/* Disconnect the buffer from the validate list */
	bo_gem->validate_index = -1;
	drm_intel_gem_bo_unreference_locked(bo);
	bufmgr_gem->exec_bos[i] = NULL;
    }
    bufmgr_gem->exec_count = 0;
    pthread_mutex_unlock(&bufmgr_gem->lock);
d1356 1
a1356 1
    return 0;
d1362 14
a1375 12
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
    struct drm_i915_gem_pin pin;
    int ret;

    memset(&pin, 0, sizeof(pin));
    pin.handle = bo_gem->gem_handle;
    pin.alignment = alignment;

    do {
	ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_PIN, &pin);
    } while (ret == -1 && errno == EINTR);
d1377 2
a1378 2
    if (ret != 0)
	return -errno;
d1380 2
a1381 2
    bo->offset = pin.offset;
    return 0;
d1387 4
a1390 4
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
    struct drm_i915_gem_unpin unpin;
    int ret;
d1392 2
a1393 2
    memset(&unpin, 0, sizeof(unpin));
    unpin.handle = bo_gem->gem_handle;
d1395 3
a1397 3
    ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_UNPIN, &unpin);
    if (ret != 0)
	return -errno;
d1399 1
a1399 1
    return 0;
d1403 1
a1403 1
drm_intel_gem_bo_set_tiling(drm_intel_bo *bo, uint32_t *tiling_mode,
d1406 21
a1426 7
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
    struct drm_i915_gem_set_tiling set_tiling;
    int ret;

    if (bo_gem->global_name == 0 && *tiling_mode == bo_gem->tiling_mode)
	return 0;
d1428 1
a1428 8
    /* If we're going from non-tiling to tiling, bump fence count */
    if (bo_gem->tiling_mode == I915_TILING_NONE)
	bo_gem->reloc_tree_fences++;

    memset(&set_tiling, 0, sizeof(set_tiling));
    set_tiling.handle = bo_gem->gem_handle;
    set_tiling.tiling_mode = *tiling_mode;
    set_tiling.stride = stride;
a1429 2
    ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_SET_TILING, &set_tiling);
    if (ret != 0) {
d1431 1
a1431 11
	return -errno;
    }
    bo_gem->tiling_mode = set_tiling.tiling_mode;
    bo_gem->swizzle_mode = set_tiling.swizzle_mode;

    /* If we're going from tiling to non-tiling, drop fence count */
    if (bo_gem->tiling_mode == I915_TILING_NONE)
	bo_gem->reloc_tree_fences--;

    *tiling_mode = bo_gem->tiling_mode;
    return 0;
d1435 2
a1436 2
drm_intel_gem_bo_get_tiling(drm_intel_bo *bo, uint32_t *tiling_mode,
			    uint32_t *swizzle_mode)
d1438 1
a1438 1
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
d1440 3
a1442 3
    *tiling_mode = bo_gem->tiling_mode;
    *swizzle_mode = bo_gem->swizzle_mode;
    return 0;
d1446 1
a1446 1
drm_intel_gem_bo_flink(drm_intel_bo *bo, uint32_t *name)
d1448 15
a1462 4
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
    struct drm_gem_flink flink;
    int ret;
d1464 2
a1465 12
    if (!bo_gem->global_name) {
	memset(&flink, 0, sizeof(flink));
	flink.handle = bo_gem->gem_handle;
    
	ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_GEM_FLINK, &flink);
	if (ret != 0)
	    return -errno;
	bo_gem->global_name = flink.name;
    }
    
    *name = bo_gem->global_name;
    return 0;
d1478 1
a1478 2
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bufmgr;
    int i;
d1480 17
a1496 3
    for (i = 0; i < DRM_INTEL_GEM_BO_BUCKETS; i++) {
	bufmgr_gem->cache_bucket[i].max_entries = -1;
    }
d1506 14
a1519 6
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
    int i;
    int total = 0;

    if (bo == NULL || bo_gem->included_in_check_aperture)
	return 0;
d1521 1
a1521 7
    total += bo->size;
    bo_gem->included_in_check_aperture = 1;

    for (i = 0; i < bo_gem->reloc_count; i++)
	total += drm_intel_gem_bo_get_aperture_space(bo_gem->reloc_target_bo[i]);

    return total;
d1533 1
a1533 1
drm_intel_gem_total_fences(drm_intel_bo **bo_array, int count)
d1535 2
a1536 2
    int i;
    unsigned int total = 0;
d1538 2
a1539 2
    for (i = 0; i < count; i++) {
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo_array[i];
d1541 2
a1542 2
	if (bo_gem == NULL)
	    continue;
d1544 3
a1546 3
	total += bo_gem->reloc_tree_fences;
    }
    return total;
d1556 2
a1557 2
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
    int i;
d1559 2
a1560 2
    if (bo == NULL || !bo_gem->included_in_check_aperture)
	return;
d1562 1
a1562 1
    bo_gem->included_in_check_aperture = 0;
d1564 3
a1566 2
    for (i = 0; i < bo_gem->reloc_count; i++)
	drm_intel_gem_bo_clear_aperture_space_flag(bo_gem->reloc_target_bo[i]);
d1576 2
a1577 2
    int i;
    unsigned int total = 0;
d1579 6
a1584 6
    for (i = 0; i < count; i++) {
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo_array[i];
	if (bo_gem != NULL)
		total += bo_gem->reloc_tree_size;
    }
    return total;
d1595 2
a1596 2
    int i;
    unsigned int total = 0;
d1598 21
a1618 19
    for (i = 0; i < count; i++) {
	total += drm_intel_gem_bo_get_aperture_space(bo_array[i]);
	/* For the first buffer object in the array, we get an accurate count
	 * back for its reloc_tree size (since nothing had been flagged as
	 * being counted yet).  We can save that value out as a more
	 * conservative reloc_tree_size that avoids double-counting target
	 * buffers.  Since the first buffer happens to usually be the batch
	 * buffer in our callers, this can pull us back from doing the tree
	 * walk on every new batch emit.
	 */
	if (i == 0) {
	    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo_array[i];
	    bo_gem->reloc_tree_size = total;
	}
    }

    for (i = 0; i < count; i++)
	drm_intel_gem_bo_clear_aperture_space_flag(bo_array[i]);
    return total;
d1640 70
a1709 24
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo_array[0]->bufmgr;
    unsigned int total = 0;
    unsigned int threshold = bufmgr_gem->gtt_size * 3 / 4;
    int total_fences;

    /* Check for fence reg constraints if necessary */
    if (bufmgr_gem->available_fences) {
	total_fences = drm_intel_gem_total_fences(bo_array, count);
	if (total_fences > bufmgr_gem->available_fences)
	    return -1;
    }

    total = drm_intel_gem_estimate_batch_space(bo_array, count);

    if (total > threshold)
	total = drm_intel_gem_compute_batch_space(bo_array, count);

    if (total > threshold) {
	DBG("check_space: overflowed available aperture, %dkb vs %dkb\n",
	    total / 1024, (int)bufmgr_gem->gtt_size / 1024);
	return -1;
    } else {
	DBG("drm_check_space: total %dkb vs bufgr %dkb\n", total / 1024 ,
	    (int)bufmgr_gem->gtt_size / 1024);
a1710 1
    }
d1722 33
a1754 38
    drm_intel_bufmgr_gem *bufmgr_gem;
    struct drm_i915_gem_get_aperture aperture;
    drm_i915_getparam_t gp;
    int ret, i;
    unsigned long size;

    bufmgr_gem = calloc(1, sizeof(*bufmgr_gem));
    bufmgr_gem->fd = fd;

    if (pthread_mutex_init(&bufmgr_gem->lock, NULL) != 0) {
      free(bufmgr_gem);
      return NULL;
   }

    ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_GET_APERTURE, &aperture);

    if (ret == 0)
	bufmgr_gem->gtt_size = aperture.aper_available_size;
    else {
	fprintf(stderr, "DRM_IOCTL_I915_GEM_APERTURE failed: %s\n",
		strerror(errno));
	bufmgr_gem->gtt_size = 128 * 1024 * 1024;
	fprintf(stderr, "Assuming %dkB available aperture size.\n"
		"May lead to reduced performance or incorrect rendering.\n",
		(int)bufmgr_gem->gtt_size / 1024);
    }

    gp.param = I915_PARAM_CHIPSET_ID;
    gp.value = &bufmgr_gem->pci_device;
    ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
    if (ret) {
	fprintf(stderr, "get chip id failed: %d\n", ret);
	fprintf(stderr, "param: %d, val: %d\n", gp.param, *gp.value);
    }

    if (!IS_I965G(bufmgr_gem)) {
	gp.param = I915_PARAM_NUM_FENCES_AVAIL;
	gp.value = &bufmgr_gem->available_fences;
d1757 3
a1759 38
	    fprintf(stderr, "get fences failed: %d\n", ret);
	    fprintf(stderr, "param: %d, val: %d\n", gp.param, *gp.value);
	    bufmgr_gem->available_fences = 0;
	}
    }

    /* Let's go with one relocation per every 2 dwords (but round down a bit
     * since a power of two will mean an extra page allocation for the reloc
     * buffer).
     *
     * Every 4 was too few for the blender benchmark.
     */
    bufmgr_gem->max_relocs = batch_size / sizeof(uint32_t) / 2 - 2;

    bufmgr_gem->bufmgr.bo_alloc = drm_intel_gem_bo_alloc;
    bufmgr_gem->bufmgr.bo_alloc_for_render = drm_intel_gem_bo_alloc_for_render;
    bufmgr_gem->bufmgr.bo_reference = drm_intel_gem_bo_reference;
    bufmgr_gem->bufmgr.bo_unreference = drm_intel_gem_bo_unreference;
    bufmgr_gem->bufmgr.bo_map = drm_intel_gem_bo_map;
    bufmgr_gem->bufmgr.bo_unmap = drm_intel_gem_bo_unmap;
    bufmgr_gem->bufmgr.bo_subdata = drm_intel_gem_bo_subdata;
    bufmgr_gem->bufmgr.bo_get_subdata = drm_intel_gem_bo_get_subdata;
    bufmgr_gem->bufmgr.bo_wait_rendering = drm_intel_gem_bo_wait_rendering;
    bufmgr_gem->bufmgr.bo_emit_reloc = drm_intel_gem_bo_emit_reloc;
    bufmgr_gem->bufmgr.bo_pin = drm_intel_gem_bo_pin;
    bufmgr_gem->bufmgr.bo_unpin = drm_intel_gem_bo_unpin;
    bufmgr_gem->bufmgr.bo_get_tiling = drm_intel_gem_bo_get_tiling;
    bufmgr_gem->bufmgr.bo_set_tiling = drm_intel_gem_bo_set_tiling;
    bufmgr_gem->bufmgr.bo_flink = drm_intel_gem_bo_flink;
    bufmgr_gem->bufmgr.bo_exec = drm_intel_gem_bo_exec;
    bufmgr_gem->bufmgr.destroy = drm_intel_bufmgr_gem_destroy;
    bufmgr_gem->bufmgr.debug = 0;
    bufmgr_gem->bufmgr.check_aperture_space = drm_intel_gem_check_aperture_space;
    /* Initialize the linked lists for BO reuse cache. */
    for (i = 0, size = 4096; i < DRM_INTEL_GEM_BO_BUCKETS; i++, size *= 2) {
	DRMINITLISTHEAD(&bufmgr_gem->cache_bucket[i].head);
	bufmgr_gem->cache_bucket[i].size = size;
    }
d1761 80
a1840 1
    return &bufmgr_gem->bufmgr;
a1841 1

@


1.3
log
@Fix libdrm for the buffer mapping strategy our GEM implementation will use

We always map through the gtt, never using the cpu mapping (this decreases
complexity a lot). Keep use of the original ioctl for this. Also don't bother
with userland domain setting, the kernel handles this correctly.

Finally, don't persist mapping across calls to unmap and remapping, this aids
with debugging, but may change later.

ok matthieu@@.
@
text
@d81 1
d84 2
a85 2
/* Arbitrarily chosen, 16 means that the maximum size we'll cache for reuse
 * is 1 << 16 pages, or 256MB.
d87 1
a87 1
#define DRM_INTEL_GEM_BO_BUCKETS	16
d142 2
d179 4
a203 14
static int
logbase2(int n)
{
   int i = 1;
   int log2 = 0;

   while (n > i) {
      i *= 2;
      log2++;
   }

   return log2;
}

d210 6
a215 11
    /* We only do buckets in power of two increments */
    if ((size & (size - 1)) != 0)
	return NULL;

    /* We should only see sizes rounded to pages. */
    assert((size % 4096) == 0);

    /* We always allocate in units of pages */
    i = ffs(size / 4096) - 1;
    if (i >= DRM_INTEL_GEM_BO_BUCKETS)
	return NULL;
d217 1
a217 1
    return &bufmgr_gem->cache_bucket[i];
a219 1

d326 1
a326 4
    bo_size = 1 << logbase2(size);
    if (bo_size < page_size)
	bo_size = page_size;
    bucket = drm_intel_gem_bo_bucket_for_size(bufmgr_gem, bo_size);
d335 2
d515 4
d532 24
d590 5
d603 2
d636 3
a638 1

d661 5
a676 2
    struct drm_i915_gem_sw_finish sw_finish;
    int ret = 0;
d683 6
a688 1
    munmap(bo->virtual, bo_gem->bo.size);
d692 1
a692 1
    return ret;
d757 1
a757 1
    return drm_intel_gem_bo_start_gtt_access(bo, 0);
d1288 1
d1358 1
a1358 1
    for (i = 0; i < DRM_INTEL_GEM_BO_BUCKETS; i++)
d1360 2
@


1.2
log
@update libdrm to 2.4.9 (actually to -current, but the only real changes
are to some assertions and a small change to modesetting code).

bump libdrm_intel minor due to added symbols, libdrm doesn't get bumped,
no change to symbol list.

ok matthieu@@.
@
text
@a146 4
    /** Mapped address for the buffer, saved across map/unmap cycles */
    void *mem_virtual;
    /** GTT virtual address for the buffer, saved across map/unmap cycles */
    void *gtt_virtual;
d527 2
a528 4
    if (bo_gem->mem_virtual)
	munmap (bo_gem->mem_virtual, bo_gem->bo.size);
    if (bo_gem->gtt_virtual)
	munmap (bo_gem->gtt_virtual, bo_gem->bo.size);
d603 1
a603 57
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
    struct drm_i915_gem_set_domain set_domain;
    int ret;

    pthread_mutex_lock(&bufmgr_gem->lock);

    /* Allow recursive mapping. Mesa may recursively map buffers with
     * nested display loops.
     */
    if (!bo_gem->mem_virtual) {
	struct drm_i915_gem_mmap mmap_arg;

	DBG("bo_map: %d (%s)\n", bo_gem->gem_handle, bo_gem->name);

	memset(&mmap_arg, 0, sizeof(mmap_arg));
	mmap_arg.handle = bo_gem->gem_handle;
	mmap_arg.offset = 0;
	mmap_arg.size = bo->size;
	ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_MMAP, &mmap_arg);
	if (ret != 0) {
	    fprintf(stderr, "%s:%d: Error mapping buffer %d (%s): %s .\n",
		    __FILE__, __LINE__,
		    bo_gem->gem_handle, bo_gem->name, strerror(errno));
	    pthread_mutex_unlock(&bufmgr_gem->lock);
	    return ret;
	}
	bo_gem->mem_virtual = (void *)(uintptr_t)mmap_arg.addr_ptr;
	bo_gem->swrast = 0;
    }
    DBG("bo_map: %d (%s) -> %p\n", bo_gem->gem_handle, bo_gem->name,
	bo_gem->mem_virtual);
    bo->virtual = bo_gem->mem_virtual;

    if (bo_gem->global_name != 0 || !bo_gem->swrast) {
	set_domain.handle = bo_gem->gem_handle;
	set_domain.read_domains = I915_GEM_DOMAIN_CPU;
	if (write_enable)
	    set_domain.write_domain = I915_GEM_DOMAIN_CPU;
	else
	    set_domain.write_domain = 0;
	do {
	    ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_SET_DOMAIN,
			&set_domain);
	} while (ret == -1 && errno == EINTR);
	if (ret != 0) {
	    fprintf (stderr, "%s:%d: Error setting swrast %d: %s\n",
		     __FILE__, __LINE__, bo_gem->gem_handle, strerror (errno));
	    pthread_mutex_unlock(&bufmgr_gem->lock);
	    return ret;
	}
	bo_gem->swrast = 1;
    }

    pthread_mutex_unlock(&bufmgr_gem->lock);

    return 0;
a608 8
#ifdef __OpenBSD__
    /*
     * OpenBSD gtt mapping will work differently, but isn't written yet,
     * so just fail for now. It's only used in the modesetting paths
     * anyway.
     */
    return EINVAL;
#else
d616 2
a617 3
    /* Get a mapping of the buffer if we haven't before. */
    if (bo_gem->gtt_virtual == NULL) {
	struct drm_i915_gem_mmap_gtt mmap_arg;
d619 1
a619 1
	DBG("bo_map_gtt: %d (%s)\n", bo_gem->gem_handle, bo_gem->name);
d621 3
a623 2
	memset(&mmap_arg, 0, sizeof(mmap_arg));
	mmap_arg.handle = bo_gem->gem_handle;
d625 7
a631 25
	/* Get the fake offset back... */
	ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_MMAP_GTT, &mmap_arg);
	if (ret != 0) {
	    fprintf(stderr,
		    "%s:%d: Error preparing buffer map %d (%s): %s .\n",
		    __FILE__, __LINE__,
		    bo_gem->gem_handle, bo_gem->name,
		    strerror(errno));
	    pthread_mutex_unlock(&bufmgr_gem->lock);
	    return ret;
	}

	/* and mmap it */
	bo_gem->gtt_virtual = mmap(0, bo->size, PROT_READ | PROT_WRITE,
				   MAP_SHARED, bufmgr_gem->fd,
				   mmap_arg.offset);
	if (bo_gem->gtt_virtual == MAP_FAILED) {
	    fprintf(stderr,
		    "%s:%d: Error mapping buffer %d (%s): %s .\n",
		    __FILE__, __LINE__,
		    bo_gem->gem_handle, bo_gem->name,
		    strerror(errno));
	    pthread_mutex_unlock(&bufmgr_gem->lock);
	    return errno;
	}
d634 1
a634 1
    bo->virtual = bo_gem->gtt_virtual;
d637 1
a637 15
	bo_gem->gtt_virtual);

    /* Now move it to the GTT domain so that the CPU caches are flushed */
    set_domain.handle = bo_gem->gem_handle;
    set_domain.read_domains = I915_GEM_DOMAIN_GTT;
    set_domain.write_domain = I915_GEM_DOMAIN_GTT;
    do {
	    ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_SET_DOMAIN,
			&set_domain);
    } while (ret == -1 && errno == EINTR);

    if (ret != 0) {
	    fprintf (stderr, "%s:%d: Error setting domain %d: %s\n",
		     __FILE__, __LINE__, bo_gem->gem_handle, strerror (errno));
    }
a641 1
#endif
d655 1
a655 2
    assert(bo_gem->gtt_virtual != NULL);

d657 1
d667 1
a667 22
    drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
    drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
    struct drm_i915_gem_sw_finish sw_finish;
    int ret;

    if (bo == NULL)
	return 0;

    assert(bo_gem->mem_virtual != NULL);

    pthread_mutex_lock(&bufmgr_gem->lock);
    if (bo_gem->swrast) {
	sw_finish.handle = bo_gem->gem_handle;
	do {
	    ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_SW_FINISH,
			&sw_finish);
	} while (ret == -1 && errno == EINTR);
	bo_gem->swrast = 0;
    }
    bo->virtual = NULL;
    pthread_mutex_unlock(&bufmgr_gem->lock);
    return 0;
@


1.1
log
@Update libdrm to 2.4.3

This is needed for us to be able to update Mesa and xf86-video-intel.
Includes a few fixes, as well as the intel bufmgr interface and the
modesetting interface (which currently errors when you check if it's
enabled).

ok matthieu@@
@
text
@d55 1
d58 1
d71 2
a72 1
   drm_intel_bo_gem *head, **tail;
d105 2
d148 3
a150 1
    void *virtual;
d152 2
a153 2
    /** free list */
    drm_intel_bo_gem *next;
d175 5
d330 3
a332 2
drm_intel_gem_bo_alloc(drm_intel_bufmgr *bufmgr, const char *name,
		   unsigned long size, unsigned int alignment)
d361 8
a368 11
	
	bo_gem = bucket->head;
        busy.handle = bo_gem->gem_handle;

        ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_BUSY, &busy);
        alloc_from_cache = (ret == 0 && busy.busy == 0);

	if (alloc_from_cache) {
	    bucket->head = bo_gem->next;
	    if (bo_gem->next == NULL)
		bucket->tail = &bucket->head;
d370 20
d419 1
d430 14
d483 1
d492 4
d508 1
d519 1
d531 4
a534 2
    if (bo_gem->virtual)
	munmap (bo_gem->virtual, bo_gem->bo.size);
d537 1
d554 1
d588 1
a588 3
	    bo_gem->next = NULL;
	    *bucket->tail = bo_gem;
	    bucket->tail = &bo_gem->next;
d619 1
a619 1
    if (!bo_gem->virtual) {
d636 1
a636 1
	bo_gem->virtual = (void *)(uintptr_t)mmap_arg.addr_ptr;
d640 2
a641 2
	bo_gem->virtual);
    bo->virtual = bo_gem->virtual;
d687 1
a687 1
    if (bo_gem->virtual == NULL) {
d708 4
a711 4
	bo_gem->virtual = mmap(0, bo->size, PROT_READ | PROT_WRITE,
			       MAP_SHARED, bufmgr_gem->fd,
			       mmap_arg.offset);
	if (bo_gem->virtual == MAP_FAILED) {
d722 1
a722 1
    bo->virtual = bo_gem->virtual;
d725 1
a725 1
	bo_gem->virtual);
d737 1
a737 1
	    fprintf (stderr, "%s:%d: Error setting swrast %d: %s\n",
d747 20
d778 1
a778 1
    assert(bo_gem->virtual != NULL);
d789 1
d898 3
a900 4
	while ((bo_gem = bucket->head) != NULL) {
	    bucket->head = bo_gem->next;
	    if (bo_gem->next == NULL)
		bucket->tail = &bucket->head;
d946 1
d1083 1
d1088 1
a1088 1
        ret = ioctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_PIN, &pin);
d1106 1
d1128 5
d1145 4
d1173 1
d1228 25
d1300 1
a1300 1
    for (i = 0; i < count; i++)
d1302 13
d1343 8
d1353 1
a1353 1
    
d1379 1
d1403 19
d1431 1
d1451 1
a1451 1
	bufmgr_gem->cache_bucket[i].tail = &bufmgr_gem->cache_bucket[i].head;
@


1.1.1.1
log
@Import libdrm 2.4.64
@
text
@d4 1
a4 1
 * Copyright © 2007-2012 Intel Corporation
a41 1
#include <xf86atomic.h>
d50 1
a52 1
#include <stdbool.h>
a54 5
#ifndef ETIME
#define ETIME ETIMEDOUT
#endif
#include "libdrm_macros.h"
#include "libdrm_lists.h"
a56 1
#include "intel_chipset.h"
a60 10
#ifdef HAVE_VALGRIND
#include <valgrind.h>
#include <memcheck.h>
#define VG(x) x
#else
#define VG(x)
#endif

#define memclear(s) memset(&s, 0, sizeof(s))

d62 2
a63 2
	if (bufmgr_gem->bufmgr.debug)			\
		fprintf(stderr, __VA_ARGS__);		\
a65 3
#define ARRAY_SIZE(x) (sizeof(x) / sizeof((x)[0]))
#define MAX2(A, B) ((A) > (B) ? (A) : (B))

d69 9
a77 2
	drmMMListHead head;
	unsigned long size;
d80 4
d85 3
a87 1
	drm_intel_bufmgr bufmgr;
d89 1
a89 1
	atomic_t refcount;
d91 1
a91 1
	int fd;
d93 4
a96 1
	int max_relocs;
d98 2
a99 37
	pthread_mutex_t lock;

	struct drm_i915_gem_exec_object *exec_objects;
	struct drm_i915_gem_exec_object2 *exec2_objects;
	drm_intel_bo **exec_bos;
	int exec_size;
	int exec_count;

	/** Array of lists of cached gem objects of power-of-two sizes */
	struct drm_intel_gem_bo_bucket cache_bucket[14 * 4];
	int num_buckets;
	time_t time;

	drmMMListHead managers;

	drmMMListHead named;
	drmMMListHead vma_cache;
	int vma_count, vma_open, vma_max;

	uint64_t gtt_size;
	int available_fences;
	int pci_device;
	int gen;
	unsigned int has_bsd : 1;
	unsigned int has_blt : 1;
	unsigned int has_relaxed_fencing : 1;
	unsigned int has_llc : 1;
	unsigned int has_wait_timeout : 1;
	unsigned int bo_reuse : 1;
	unsigned int no_exec : 1;
	unsigned int has_vebox : 1;
	bool fenced_relocs;

	struct {
		void *ptr;
		uint32_t handle;
	} userptr_active;
d101 1
a103 7
#define DRM_INTEL_RELOC_FENCE (1<<0)

typedef struct _drm_intel_reloc_target_info {
	drm_intel_bo *bo;
	int flags;
} drm_intel_reloc_target;

d105 1
a105 1
	drm_intel_bo bo;
d107 62
a168 24
	atomic_t refcount;
	uint32_t gem_handle;
	const char *name;

	/**
	 * Kenel-assigned global name for this object
         *
         * List contains both flink named and prime fd'd objects
	 */
	unsigned int global_name;
	drmMMListHead name_list;

	/**
	 * Index of the buffer within the validation list while preparing a
	 * batchbuffer execution.
	 */
	int validate_index;

	/**
	 * Current tiling mode
	 */
	uint32_t tiling_mode;
	uint32_t swizzle_mode;
	unsigned long stride;
d170 1
a170 79
	time_t free_time;

	/** Array passed to the DRM containing relocation information. */
	struct drm_i915_gem_relocation_entry *relocs;
	/**
	 * Array of info structs corresponding to relocs[i].target_handle etc
	 */
	drm_intel_reloc_target *reloc_target_info;
	/** Number of entries in relocs */
	int reloc_count;
	/** Mapped address for the buffer, saved across map/unmap cycles */
	void *mem_virtual;
	/** GTT virtual address for the buffer, saved across map/unmap cycles */
	void *gtt_virtual;
	/**
	 * Virtual address of the buffer allocated by user, used for userptr
	 * objects only.
	 */
	void *user_virtual;
	int map_count;
	drmMMListHead vma_list;

	/** BO cache list */
	drmMMListHead head;

	/**
	 * Boolean of whether this BO and its children have been included in
	 * the current drm_intel_bufmgr_check_aperture_space() total.
	 */
	bool included_in_check_aperture;

	/**
	 * Boolean of whether this buffer has been used as a relocation
	 * target and had its size accounted for, and thus can't have any
	 * further relocations added to it.
	 */
	bool used_as_reloc_target;

	/**
	 * Boolean of whether we have encountered an error whilst building the relocation tree.
	 */
	bool has_error;

	/**
	 * Boolean of whether this buffer can be re-used
	 */
	bool reusable;

	/**
	 * Boolean of whether the GPU is definitely not accessing the buffer.
	 *
	 * This is only valid when reusable, since non-reusable
	 * buffers are those that have been shared wth other
	 * processes, so we don't know their state.
	 */
	bool idle;

	/**
	 * Boolean of whether this buffer was allocated with userptr
	 */
	bool is_userptr;

	/**
	 * Size in bytes of this buffer and its relocation descendents.
	 *
	 * Used to avoid costly tree walking in
	 * drm_intel_bufmgr_check_aperture in the common case.
	 */
	int reloc_tree_size;

	/**
	 * Number of potential fence registers required by this buffer and its
	 * relocations.
	 */
	int reloc_tree_fences;

	/** Flags that we may need to do the SW_FINSIH ioctl on unmap. */
	bool mapped_cpu_write;
};
d173 1
a173 1
drm_intel_gem_estimate_batch_space(drm_intel_bo ** bo_array, int count);
d176 1
a176 1
drm_intel_gem_compute_batch_space(drm_intel_bo ** bo_array, int count);
d179 2
a180 2
drm_intel_gem_bo_get_tiling(drm_intel_bo *bo, uint32_t * tiling_mode,
			    uint32_t * swizzle_mode);
d183 2
a184 3
drm_intel_gem_bo_set_tiling_internal(drm_intel_bo *bo,
				     uint32_t tiling_mode,
				     uint32_t stride);
d186 2
a187 2
static void drm_intel_gem_bo_unreference_locked_timed(drm_intel_bo *bo,
						      time_t time);
d189 2
a190 7
static void drm_intel_gem_bo_unreference(drm_intel_bo *bo);

static void drm_intel_gem_bo_free(drm_intel_bo *bo);

static unsigned long
drm_intel_gem_bo_tile_size(drm_intel_bufmgr_gem *bufmgr_gem, unsigned long size,
			   uint32_t *tiling_mode)
d192 2
a193 70
	unsigned long min_size, max_size;
	unsigned long i;

	if (*tiling_mode == I915_TILING_NONE)
		return size;

	/* 965+ just need multiples of page size for tiling */
	if (bufmgr_gem->gen >= 4)
		return ROUND_UP_TO(size, 4096);

	/* Older chips need powers of two, of at least 512k or 1M */
	if (bufmgr_gem->gen == 3) {
		min_size = 1024*1024;
		max_size = 128*1024*1024;
	} else {
		min_size = 512*1024;
		max_size = 64*1024*1024;
	}

	if (size > max_size) {
		*tiling_mode = I915_TILING_NONE;
		return size;
	}

	/* Do we need to allocate every page for the fence? */
	if (bufmgr_gem->has_relaxed_fencing)
		return ROUND_UP_TO(size, 4096);

	for (i = min_size; i < size; i <<= 1)
		;

	return i;
}

/*
 * Round a given pitch up to the minimum required for X tiling on a
 * given chip.  We use 512 as the minimum to allow for a later tiling
 * change.
 */
static unsigned long
drm_intel_gem_bo_tile_pitch(drm_intel_bufmgr_gem *bufmgr_gem,
			    unsigned long pitch, uint32_t *tiling_mode)
{
	unsigned long tile_width;
	unsigned long i;

	/* If untiled, then just align it so that we can do rendering
	 * to it with the 3D engine.
	 */
	if (*tiling_mode == I915_TILING_NONE)
		return ALIGN(pitch, 64);

	if (*tiling_mode == I915_TILING_X
			|| (IS_915(bufmgr_gem->pci_device)
			    && *tiling_mode == I915_TILING_Y))
		tile_width = 512;
	else
		tile_width = 128;

	/* 965 is flexible */
	if (bufmgr_gem->gen >= 4)
		return ROUND_UP_TO(pitch, tile_width);

	/* The older hardware has a maximum pitch of 8192 with tiled
	 * surfaces, so fallback to untiled if it's too large.
	 */
	if (pitch > 8192) {
		*tiling_mode = I915_TILING_NONE;
		return ALIGN(pitch, 64);
	}
d195 4
a198 3
	/* Pre-965 needs power of two tile width */
	for (i = tile_width; i < pitch; i <<= 1)
		;
d200 1
a200 1
	return i;
d207 1
a207 1
	int i;
d209 6
a214 7
	for (i = 0; i < bufmgr_gem->num_buckets; i++) {
		struct drm_intel_gem_bo_bucket *bucket =
		    &bufmgr_gem->cache_bucket[i];
		if (bucket->size >= size) {
			return bucket;
		}
	}
d216 3
d220 2
d224 2
a225 2
static void
drm_intel_gem_dump_validation_list(drm_intel_bufmgr_gem *bufmgr_gem)
d227 5
a231 1
	int i, j;
d233 15
a247 25
	for (i = 0; i < bufmgr_gem->exec_count; i++) {
		drm_intel_bo *bo = bufmgr_gem->exec_bos[i];
		drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

		if (bo_gem->relocs == NULL) {
			DBG("%2d: %d (%s)\n", i, bo_gem->gem_handle,
			    bo_gem->name);
			continue;
		}

		for (j = 0; j < bo_gem->reloc_count; j++) {
			drm_intel_bo *target_bo = bo_gem->reloc_target_info[j].bo;
			drm_intel_bo_gem *target_gem =
			    (drm_intel_bo_gem *) target_bo;

			DBG("%2d: %d (%s)@@0x%08llx -> "
			    "%d (%s)@@0x%08lx + 0x%08x\n",
			    i,
			    bo_gem->gem_handle, bo_gem->name,
			    (unsigned long long)bo_gem->relocs[j].offset,
			    target_gem->gem_handle,
			    target_gem->name,
			    target_bo->offset64,
			    bo_gem->relocs[j].delta);
		}
d249 1
a249 8
}

static inline void
drm_intel_gem_bo_reference(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	atomic_inc(&bo_gem->refcount);
d263 34
a296 33
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	int index;

	if (bo_gem->validate_index != -1)
		return;

	/* Extend the array of validation entries as necessary. */
	if (bufmgr_gem->exec_count == bufmgr_gem->exec_size) {
		int new_size = bufmgr_gem->exec_size * 2;

		if (new_size == 0)
			new_size = 5;

		bufmgr_gem->exec_objects =
		    realloc(bufmgr_gem->exec_objects,
			    sizeof(*bufmgr_gem->exec_objects) * new_size);
		bufmgr_gem->exec_bos =
		    realloc(bufmgr_gem->exec_bos,
			    sizeof(*bufmgr_gem->exec_bos) * new_size);
		bufmgr_gem->exec_size = new_size;
	}

	index = bufmgr_gem->exec_count;
	bo_gem->validate_index = index;
	/* Fill in array entry */
	bufmgr_gem->exec_objects[index].handle = bo_gem->gem_handle;
	bufmgr_gem->exec_objects[index].relocation_count = bo_gem->reloc_count;
	bufmgr_gem->exec_objects[index].relocs_ptr = (uintptr_t) bo_gem->relocs;
	bufmgr_gem->exec_objects[index].alignment = bo->align;
	bufmgr_gem->exec_objects[index].offset = 0;
	bufmgr_gem->exec_bos[index] = bo;
	bufmgr_gem->exec_count++;
a298 48
static void
drm_intel_add_validate_buffer2(drm_intel_bo *bo, int need_fence)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;
	int index;

	if (bo_gem->validate_index != -1) {
		if (need_fence)
			bufmgr_gem->exec2_objects[bo_gem->validate_index].flags |=
				EXEC_OBJECT_NEEDS_FENCE;
		return;
	}

	/* Extend the array of validation entries as necessary. */
	if (bufmgr_gem->exec_count == bufmgr_gem->exec_size) {
		int new_size = bufmgr_gem->exec_size * 2;

		if (new_size == 0)
			new_size = 5;

		bufmgr_gem->exec2_objects =
			realloc(bufmgr_gem->exec2_objects,
				sizeof(*bufmgr_gem->exec2_objects) * new_size);
		bufmgr_gem->exec_bos =
			realloc(bufmgr_gem->exec_bos,
				sizeof(*bufmgr_gem->exec_bos) * new_size);
		bufmgr_gem->exec_size = new_size;
	}

	index = bufmgr_gem->exec_count;
	bo_gem->validate_index = index;
	/* Fill in array entry */
	bufmgr_gem->exec2_objects[index].handle = bo_gem->gem_handle;
	bufmgr_gem->exec2_objects[index].relocation_count = bo_gem->reloc_count;
	bufmgr_gem->exec2_objects[index].relocs_ptr = (uintptr_t)bo_gem->relocs;
	bufmgr_gem->exec2_objects[index].alignment = bo->align;
	bufmgr_gem->exec2_objects[index].offset = 0;
	bufmgr_gem->exec_bos[index] = bo;
	bufmgr_gem->exec2_objects[index].flags = 0;
	bufmgr_gem->exec2_objects[index].rsvd1 = 0;
	bufmgr_gem->exec2_objects[index].rsvd2 = 0;
	if (need_fence) {
		bufmgr_gem->exec2_objects[index].flags |=
			EXEC_OBJECT_NEEDS_FENCE;
	}
	bufmgr_gem->exec_count++;
}
d303 5
a307 29
static void
drm_intel_bo_gem_set_in_aperture_size(drm_intel_bufmgr_gem *bufmgr_gem,
				      drm_intel_bo_gem *bo_gem,
				      unsigned int alignment)
{
	unsigned int size;

	assert(!bo_gem->used_as_reloc_target);

	/* The older chipsets are far-less flexible in terms of tiling,
	 * and require tiled buffer to be size aligned in the aperture.
	 * This means that in the worst possible case we will need a hole
	 * twice as large as the object in order for it to fit into the
	 * aperture. Optimal packing is for wimps.
	 */
	size = bo_gem->bo.size;
	if (bufmgr_gem->gen < 4 && bo_gem->tiling_mode != I915_TILING_NONE) {
		unsigned int min_size;

		if (bufmgr_gem->has_relaxed_fencing) {
			if (bufmgr_gem->gen == 3)
				min_size = 1024*1024;
			else
				min_size = 512*1024;

			while (min_size < size)
				min_size *= 2;
		} else
			min_size = size;
d309 4
a312 3
		/* Account for worst-case alignment. */
		alignment = MAX2(alignment, min_size);
	}
d314 1
a314 1
	bo_gem->reloc_tree_size = size + alignment;
d317 3
a319 2
static int
drm_intel_setup_reloc_list(drm_intel_bo *bo)
d321 45
a365 13
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	unsigned int max_relocs = bufmgr_gem->max_relocs;

	if (bo->size / 4 < max_relocs)
		max_relocs = bo->size / 4;

	bo_gem->relocs = malloc(max_relocs *
				sizeof(struct drm_i915_gem_relocation_entry));
	bo_gem->reloc_target_info = malloc(max_relocs *
					   sizeof(drm_intel_reloc_target));
	if (bo_gem->relocs == NULL || bo_gem->reloc_target_info == NULL) {
		bo_gem->has_error = true;
d367 3
a369 2
		free (bo_gem->relocs);
		bo_gem->relocs = NULL;
d371 3
a373 2
		free (bo_gem->reloc_target_info);
		bo_gem->reloc_target_info = NULL;
d375 6
a380 1
		return 1;
d382 10
d393 4
a396 1
	return 0;
d399 9
a407 2
static int
drm_intel_gem_bo_busy(drm_intel_bo *bo)
d409 9
a417 4
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_busy busy;
	int ret;
d419 27
a445 2
	if (bo_gem->reusable && bo_gem->idle)
		return false;
d447 1
a447 2
	memclear(busy);
	busy.handle = bo_gem->gem_handle;
d449 1
a449 8
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_BUSY, &busy);
	if (ret == 0) {
		bo_gem->idle = !busy.busy;
		return busy.busy;
	} else {
		return false;
	}
	return (ret == 0 && busy.busy);
d452 2
a453 3
static int
drm_intel_gem_bo_madvise_internal(drm_intel_bufmgr_gem *bufmgr_gem,
				  drm_intel_bo_gem *bo_gem, int state)
d455 2
a456 7
	struct drm_i915_gem_madvise madv;

	memclear(madv);
	madv.handle = bo_gem->gem_handle;
	madv.madv = state;
	madv.retained = 1;
	drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_MADVISE, &madv);
d458 3
a460 1
	return madv.retained;
d463 2
a464 2
static int
drm_intel_gem_bo_madvise(drm_intel_bo *bo, int madv)
d466 3
a468 4
	return drm_intel_gem_bo_madvise_internal
		((drm_intel_bufmgr_gem *) bo->bufmgr,
		 (drm_intel_bo_gem *) bo,
		 madv);
a470 1
/* drop the oldest entries that have been purged by the kernel */
d472 1
a472 2
drm_intel_gem_bo_cache_purge_bucket(drm_intel_bufmgr_gem *bufmgr_gem,
				    struct drm_intel_gem_bo_bucket *bucket)
d474 17
a490 12
	while (!DRMLISTEMPTY(&bucket->head)) {
		drm_intel_bo_gem *bo_gem;

		bo_gem = DRMLISTENTRY(drm_intel_bo_gem,
				      bucket->head.next, head);
		if (drm_intel_gem_bo_madvise_internal
		    (bufmgr_gem, bo_gem, I915_MADV_DONTNEED))
			break;

		DRMLISTDEL(&bo_gem->head);
		drm_intel_gem_bo_free(&bo_gem->bo);
	}
d493 2
a494 8
static drm_intel_bo *
drm_intel_gem_bo_alloc_internal(drm_intel_bufmgr *bufmgr,
				const char *name,
				unsigned long size,
				unsigned long flags,
				uint32_t tiling_mode,
				unsigned long stride,
				unsigned int alignment)
d496 4
a499 4
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bufmgr;
	drm_intel_bo_gem *bo_gem;
	unsigned int page_size = getpagesize();
	int ret;
d501 4
a504 20
	bool alloc_from_cache;
	unsigned long bo_size;
	bool for_render = false;

	if (flags & BO_ALLOC_FOR_RENDER)
		for_render = true;

	/* Round the allocated size up to a power of two number of pages. */
	bucket = drm_intel_gem_bo_bucket_for_size(bufmgr_gem, size);

	/* If we don't have caching at this size, don't actually round the
	 * allocation up.
	 */
	if (bucket == NULL) {
		bo_size = size;
		if (bo_size < page_size)
			bo_size = page_size;
	} else {
		bo_size = bucket->size;
	}
d506 5
a510 48
	pthread_mutex_lock(&bufmgr_gem->lock);
	/* Get a buffer out of the cache if available */
retry:
	alloc_from_cache = false;
	if (bucket != NULL && !DRMLISTEMPTY(&bucket->head)) {
		if (for_render) {
			/* Allocate new render-target BOs from the tail (MRU)
			 * of the list, as it will likely be hot in the GPU
			 * cache and in the aperture for us.
			 */
			bo_gem = DRMLISTENTRY(drm_intel_bo_gem,
					      bucket->head.prev, head);
			DRMLISTDEL(&bo_gem->head);
			alloc_from_cache = true;
			bo_gem->bo.align = alignment;
		} else {
			assert(alignment == 0);
			/* For non-render-target BOs (where we're probably
			 * going to map it first thing in order to fill it
			 * with data), check if the last BO in the cache is
			 * unbusy, and only reuse in that case. Otherwise,
			 * allocating a new buffer is probably faster than
			 * waiting for the GPU to finish.
			 */
			bo_gem = DRMLISTENTRY(drm_intel_bo_gem,
					      bucket->head.next, head);
			if (!drm_intel_gem_bo_busy(&bo_gem->bo)) {
				alloc_from_cache = true;
				DRMLISTDEL(&bo_gem->head);
			}
		}

		if (alloc_from_cache) {
			if (!drm_intel_gem_bo_madvise_internal
			    (bufmgr_gem, bo_gem, I915_MADV_WILLNEED)) {
				drm_intel_gem_bo_free(&bo_gem->bo);
				drm_intel_gem_bo_cache_purge_bucket(bufmgr_gem,
								    bucket);
				goto retry;
			}

			if (drm_intel_gem_bo_set_tiling_internal(&bo_gem->bo,
								 tiling_mode,
								 stride)) {
				drm_intel_gem_bo_free(&bo_gem->bo);
				goto retry;
			}
		}
a511 1
	pthread_mutex_unlock(&bufmgr_gem->lock);
d513 2
a514 2
	if (!alloc_from_cache) {
		struct drm_i915_gem_create create;
d516 22
a537 35
		bo_gem = calloc(1, sizeof(*bo_gem));
		if (!bo_gem)
			return NULL;

		bo_gem->bo.size = bo_size;

		memclear(create);
		create.size = bo_size;

		ret = drmIoctl(bufmgr_gem->fd,
			       DRM_IOCTL_I915_GEM_CREATE,
			       &create);
		bo_gem->gem_handle = create.handle;
		bo_gem->bo.handle = bo_gem->gem_handle;
		if (ret != 0) {
			free(bo_gem);
			return NULL;
		}
		bo_gem->bo.bufmgr = bufmgr;
		bo_gem->bo.align = alignment;

		bo_gem->tiling_mode = I915_TILING_NONE;
		bo_gem->swizzle_mode = I915_BIT_6_SWIZZLE_NONE;
		bo_gem->stride = 0;

		/* drm_intel_gem_bo_free calls DRMLISTDEL() for an uninitialized
		   list (vma_list), so better set the list head here */
		DRMINITLISTHEAD(&bo_gem->name_list);
		DRMINITLISTHEAD(&bo_gem->vma_list);
		if (drm_intel_gem_bo_set_tiling_internal(&bo_gem->bo,
							 tiling_mode,
							 stride)) {
		    drm_intel_gem_bo_free(&bo_gem->bo);
		    return NULL;
		}
d539 2
d542 4
a545 7
	bo_gem->name = name;
	atomic_set(&bo_gem->refcount, 1);
	bo_gem->validate_index = -1;
	bo_gem->reloc_tree_fences = 0;
	bo_gem->used_as_reloc_target = false;
	bo_gem->has_error = false;
	bo_gem->reusable = true;
d547 4
a550 1
	drm_intel_bo_gem_set_in_aperture_size(bufmgr_gem, bo_gem, alignment);
d552 7
a558 2
	DBG("bo_create: buf %d (%s) %ldb\n",
	    bo_gem->gem_handle, bo_gem->name, size);
d560 1
a560 2
	return &bo_gem->bo;
}
d562 5
a566 11
static drm_intel_bo *
drm_intel_gem_bo_alloc_for_render(drm_intel_bufmgr *bufmgr,
				  const char *name,
				  unsigned long size,
				  unsigned int alignment)
{
	return drm_intel_gem_bo_alloc_internal(bufmgr, name, size,
					       BO_ALLOC_FOR_RENDER,
					       I915_TILING_NONE, 0,
					       alignment);
}
d568 1
a568 9
static drm_intel_bo *
drm_intel_gem_bo_alloc(drm_intel_bufmgr *bufmgr,
		       const char *name,
		       unsigned long size,
		       unsigned int alignment)
{
	return drm_intel_gem_bo_alloc_internal(bufmgr, name, size, 0,
					       I915_TILING_NONE, 0, 0);
}
d570 18
a587 8
static drm_intel_bo *
drm_intel_gem_bo_alloc_tiled(drm_intel_bufmgr *bufmgr, const char *name,
			     int x, int y, int cpp, uint32_t *tiling_mode,
			     unsigned long *pitch, unsigned long flags)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bufmgr;
	unsigned long size, stride;
	uint32_t tiling;
d589 7
d597 11
a607 1
		unsigned long aligned_y, height_alignment;
d609 1
a609 1
		tiling = *tiling_mode;
d611 1
a611 36
		/* If we're tiled, our allocations are in 8 or 32-row blocks,
		 * so failure to align our height means that we won't allocate
		 * enough pages.
		 *
		 * If we're untiled, we still have to align to 2 rows high
		 * because the data port accesses 2x2 blocks even if the
		 * bottom row isn't to be rendered, so failure to align means
		 * we could walk off the end of the GTT and fault.  This is
		 * documented on 965, and may be the case on older chipsets
		 * too so we try to be careful.
		 */
		aligned_y = y;
		height_alignment = 2;

		if ((bufmgr_gem->gen == 2) && tiling != I915_TILING_NONE)
			height_alignment = 16;
		else if (tiling == I915_TILING_X
			|| (IS_915(bufmgr_gem->pci_device)
			    && tiling == I915_TILING_Y))
			height_alignment = 8;
		else if (tiling == I915_TILING_Y)
			height_alignment = 32;
		aligned_y = ALIGN(y, height_alignment);

		stride = x * cpp;
		stride = drm_intel_gem_bo_tile_pitch(bufmgr_gem, stride, tiling_mode);
		size = stride * aligned_y;
		size = drm_intel_gem_bo_tile_size(bufmgr_gem, size, tiling_mode);
	} while (*tiling_mode != tiling);
	*pitch = stride;

	if (tiling == I915_TILING_NONE)
		stride = 0;

	return drm_intel_gem_bo_alloc_internal(bufmgr, name, size, flags,
					       tiling, stride, 0);
d614 2
a615 8
static drm_intel_bo *
drm_intel_gem_bo_alloc_userptr(drm_intel_bufmgr *bufmgr,
				const char *name,
				void *addr,
				uint32_t tiling_mode,
				uint32_t stride,
				unsigned long size,
				unsigned long flags)
d617 14
a630 4
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bufmgr;
	drm_intel_bo_gem *bo_gem;
	int ret;
	struct drm_i915_gem_userptr userptr;
d632 3
a634 5
	/* Tiling with userptr surfaces is not supported
	 * on all hardware so refuse it for time being.
	 */
	if (tiling_mode != I915_TILING_NONE)
		return NULL;
d636 1
a636 3
	bo_gem = calloc(1, sizeof(*bo_gem));
	if (!bo_gem)
		return NULL;
d638 2
a639 1
	bo_gem->bo.size = size;
d641 2
a642 8
	memclear(userptr);
	userptr.user_ptr = (__u64)((unsigned long)addr);
	userptr.user_size = size;
	userptr.flags = flags;

	ret = drmIoctl(bufmgr_gem->fd,
			DRM_IOCTL_I915_GEM_USERPTR,
			&userptr);
d644 21
a664 5
		DBG("bo_create_userptr: "
		    "ioctl failed with user ptr %p size 0x%lx, "
		    "user flags 0x%lx\n", addr, size, flags);
		free(bo_gem);
		return NULL;
d666 1
d668 1
a668 10
	bo_gem->gem_handle = userptr.handle;
	bo_gem->bo.handle = bo_gem->gem_handle;
	bo_gem->bo.bufmgr    = bufmgr;
	bo_gem->is_userptr   = true;
	bo_gem->bo.virtual   = addr;
	/* Save the address provided by user */
	bo_gem->user_virtual = addr;
	bo_gem->tiling_mode  = I915_TILING_NONE;
	bo_gem->swizzle_mode = I915_BIT_6_SWIZZLE_NONE;
	bo_gem->stride       = 0;
d670 2
a671 2
	DRMINITLISTHEAD(&bo_gem->name_list);
	DRMINITLISTHEAD(&bo_gem->vma_list);
d673 8
a680 7
	bo_gem->name = name;
	atomic_set(&bo_gem->refcount, 1);
	bo_gem->validate_index = -1;
	bo_gem->reloc_tree_fences = 0;
	bo_gem->used_as_reloc_target = false;
	bo_gem->has_error = false;
	bo_gem->reusable = false;
d682 4
a685 1
	drm_intel_bo_gem_set_in_aperture_size(bufmgr_gem, bo_gem, 0);
d687 1
a687 4
	DBG("bo_create_userptr: "
	    "ptr %p buf %d (%s) size %ldb, stride 0x%x, tile mode %d\n",
		addr, bo_gem->gem_handle, bo_gem->name,
		size, stride, tiling_mode);
d689 2
a690 1
	return &bo_gem->bo;
d693 2
a694 2
static bool
has_userptr(drm_intel_bufmgr_gem *bufmgr_gem)
d696 4
a699 4
	int ret;
	void *ptr;
	long pgsz;
	struct drm_i915_gem_userptr userptr;
d701 2
a702 2
	pgsz = sysconf(_SC_PAGESIZE);
	assert(pgsz > 0);
d704 1
a704 6
	ret = posix_memalign(&ptr, pgsz, pgsz);
	if (ret) {
		DBG("Failed to get a page (%ld) for userptr detection!\n",
			pgsz);
		return false;
	}
d706 12
a717 14
	memclear(userptr);
	userptr.user_ptr = (__u64)(unsigned long)ptr;
	userptr.user_size = pgsz;

retry:
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_USERPTR, &userptr);
	if (ret) {
		if (errno == ENODEV && userptr.flags == 0) {
			userptr.flags = I915_USERPTR_UNSYNCHRONIZED;
			goto retry;
		}
		free(ptr);
		return false;
	}
d719 25
a743 6
	/* We don't release the userptr bo here as we want to keep the
	 * kernel mm tracking alive for our lifetime. The first time we
	 * create a userptr object the kernel has to install a mmu_notifer
	 * which is a heavyweight operation (e.g. it requires taking all
	 * mm_locks and stop_machine()).
	 */
d745 24
a768 4
	bufmgr_gem->userptr_active.ptr = ptr;
	bufmgr_gem->userptr_active.handle = userptr.handle;

	return true;
d771 3
a773 8
static drm_intel_bo *
check_bo_alloc_userptr(drm_intel_bufmgr *bufmgr,
		       const char *name,
		       void *addr,
		       uint32_t tiling_mode,
		       uint32_t stride,
		       unsigned long size,
		       unsigned long flags)
d775 1
a775 7
	if (has_userptr((drm_intel_bufmgr_gem *)bufmgr))
		bufmgr->bo_alloc_userptr = drm_intel_gem_bo_alloc_userptr;
	else
		bufmgr->bo_alloc_userptr = NULL;

	return drm_intel_bo_alloc_userptr(bufmgr, name, addr,
					  tiling_mode, stride, size, flags);
d779 2
a780 1
 * Returns a drm_intel_bo wrapping the given buffer object handle.
d782 2
a783 2
 * This can be used when one application needs to pass a buffer object
 * to another.
d785 24
a808 4
drm_intel_bo *
drm_intel_bo_gem_create_from_name(drm_intel_bufmgr *bufmgr,
				  const char *name,
				  unsigned int handle)
d810 2
a811 24
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bufmgr;
	drm_intel_bo_gem *bo_gem;
	int ret;
	struct drm_gem_open open_arg;
	struct drm_i915_gem_get_tiling get_tiling;
	drmMMListHead *list;

	/* At the moment most applications only have a few named bo.
	 * For instance, in a DRI client only the render buffers passed
	 * between X and the client are named. And since X returns the
	 * alternating names for the front/back buffer a linear search
	 * provides a sufficiently fast match.
	 */
	pthread_mutex_lock(&bufmgr_gem->lock);
	for (list = bufmgr_gem->named.next;
	     list != &bufmgr_gem->named;
	     list = list->next) {
		bo_gem = DRMLISTENTRY(drm_intel_bo_gem, list, name_list);
		if (bo_gem->global_name == handle) {
			drm_intel_gem_bo_reference(&bo_gem->bo);
			pthread_mutex_unlock(&bufmgr_gem->lock);
			return &bo_gem->bo;
		}
	}
d813 2
a814 25
	memclear(open_arg);
	open_arg.name = handle;
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_GEM_OPEN,
		       &open_arg);
	if (ret != 0) {
		DBG("Couldn't reference %s handle 0x%08x: %s\n",
		    name, handle, strerror(errno));
		pthread_mutex_unlock(&bufmgr_gem->lock);
		return NULL;
	}
        /* Now see if someone has used a prime handle to get this
         * object from the kernel before by looking through the list
         * again for a matching gem_handle
         */
	for (list = bufmgr_gem->named.next;
	     list != &bufmgr_gem->named;
	     list = list->next) {
		bo_gem = DRMLISTENTRY(drm_intel_bo_gem, list, name_list);
		if (bo_gem->gem_handle == open_arg.handle) {
			drm_intel_gem_bo_reference(&bo_gem->bo);
			pthread_mutex_unlock(&bufmgr_gem->lock);
			return &bo_gem->bo;
		}
	}
d816 1
a816 5
	bo_gem = calloc(1, sizeof(*bo_gem));
	if (!bo_gem) {
		pthread_mutex_unlock(&bufmgr_gem->lock);
		return NULL;
	}
d818 4
a821 32
	bo_gem->bo.size = open_arg.size;
	bo_gem->bo.offset = 0;
	bo_gem->bo.offset64 = 0;
	bo_gem->bo.virtual = NULL;
	bo_gem->bo.bufmgr = bufmgr;
	bo_gem->name = name;
	atomic_set(&bo_gem->refcount, 1);
	bo_gem->validate_index = -1;
	bo_gem->gem_handle = open_arg.handle;
	bo_gem->bo.handle = open_arg.handle;
	bo_gem->global_name = handle;
	bo_gem->reusable = false;

	memclear(get_tiling);
	get_tiling.handle = bo_gem->gem_handle;
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_GET_TILING,
		       &get_tiling);
	if (ret != 0) {
		drm_intel_gem_bo_unreference(&bo_gem->bo);
		pthread_mutex_unlock(&bufmgr_gem->lock);
		return NULL;
	}
	bo_gem->tiling_mode = get_tiling.tiling_mode;
	bo_gem->swizzle_mode = get_tiling.swizzle_mode;
	/* XXX stride is unknown */
	drm_intel_bo_gem_set_in_aperture_size(bufmgr_gem, bo_gem, 0);

	DRMINITLISTHEAD(&bo_gem->vma_list);
	DRMLISTADDTAIL(&bo_gem->name_list, &bufmgr_gem->named);
	pthread_mutex_unlock(&bufmgr_gem->lock);
	DBG("bo_create_from_handle: %d (%s)\n", handle, bo_gem->name);
d823 5
a827 2
	return &bo_gem->bo;
}
d829 1
a829 17
static void
drm_intel_gem_bo_free(drm_intel_bo *bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_gem_close close;
	int ret;

	DRMLISTDEL(&bo_gem->vma_list);
	if (bo_gem->mem_virtual) {
		VG(VALGRIND_FREELIKE_BLOCK(bo_gem->mem_virtual, 0));
		drm_munmap(bo_gem->mem_virtual, bo_gem->bo.size);
		bufmgr_gem->vma_count--;
	}
	if (bo_gem->gtt_virtual) {
		drm_munmap(bo_gem->gtt_virtual, bo_gem->bo.size);
		bufmgr_gem->vma_count--;
d831 1
d833 1
a833 9
	/* Close this object */
	memclear(close);
	close.handle = bo_gem->gem_handle;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_GEM_CLOSE, &close);
	if (ret != 0) {
		DBG("DRM_IOCTL_GEM_CLOSE %d failed (%s): %s\n",
		    bo_gem->gem_handle, bo_gem->name, strerror(errno));
	}
	free(bo);
d836 13
a848 2
static void
drm_intel_gem_bo_mark_mmaps_incoherent(drm_intel_bo *bo)
d850 3
a852 2
#if HAVE_VALGRIND
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
d854 1
a854 2
	if (bo_gem->mem_virtual)
		VALGRIND_MAKE_MEM_NOACCESS(bo_gem->mem_virtual, bo->size);
d856 6
a861 4
	if (bo_gem->gtt_virtual)
		VALGRIND_MAKE_MEM_NOACCESS(bo_gem->gtt_virtual, bo->size);
#endif
}
d863 3
a865 5
/** Frees all cached buffers significantly older than @@time. */
static void
drm_intel_gem_cleanup_bo_cache(drm_intel_bufmgr_gem *bufmgr_gem, time_t time)
{
	int i;
d867 5
a871 2
	if (bufmgr_gem->time == time)
		return;
d873 2
a874 3
	for (i = 0; i < bufmgr_gem->num_buckets; i++) {
		struct drm_intel_gem_bo_bucket *bucket =
		    &bufmgr_gem->cache_bucket[i];
d876 7
a882 2
		while (!DRMLISTEMPTY(&bucket->head)) {
			drm_intel_bo_gem *bo_gem;
d884 2
a885 4
			bo_gem = DRMLISTENTRY(drm_intel_bo_gem,
					      bucket->head.next, head);
			if (time - bo_gem->free_time <= 1)
				break;
d887 1
a887 1
			DRMLISTDEL(&bo_gem->head);
d889 1
a889 3
			drm_intel_gem_bo_free(&bo_gem->bo);
		}
	}
d891 1
a891 1
	bufmgr_gem->time = time;
d894 7
a900 1
static void drm_intel_gem_bo_purge_vma_cache(drm_intel_bufmgr_gem *bufmgr_gem)
d902 5
a906 1
	int limit;
d908 5
a912 32
	DBG("%s: cached=%d, open=%d, limit=%d\n", __FUNCTION__,
	    bufmgr_gem->vma_count, bufmgr_gem->vma_open, bufmgr_gem->vma_max);

	if (bufmgr_gem->vma_max < 0)
		return;

	/* We may need to evict a few entries in order to create new mmaps */
	limit = bufmgr_gem->vma_max - 2*bufmgr_gem->vma_open;
	if (limit < 0)
		limit = 0;

	while (bufmgr_gem->vma_count > limit) {
		drm_intel_bo_gem *bo_gem;

		bo_gem = DRMLISTENTRY(drm_intel_bo_gem,
				      bufmgr_gem->vma_cache.next,
				      vma_list);
		assert(bo_gem->map_count == 0);
		DRMLISTDELINIT(&bo_gem->vma_list);

		if (bo_gem->mem_virtual) {
			drm_munmap(bo_gem->mem_virtual, bo_gem->bo.size);
			bo_gem->mem_virtual = NULL;
			bufmgr_gem->vma_count--;
		}
		if (bo_gem->gtt_virtual) {
			drm_munmap(bo_gem->gtt_virtual, bo_gem->bo.size);
			bo_gem->gtt_virtual = NULL;
			bufmgr_gem->vma_count--;
		}
	}
}
d914 3
a916 22
static void drm_intel_gem_bo_close_vma(drm_intel_bufmgr_gem *bufmgr_gem,
				       drm_intel_bo_gem *bo_gem)
{
	bufmgr_gem->vma_open--;
	DRMLISTADDTAIL(&bo_gem->vma_list, &bufmgr_gem->vma_cache);
	if (bo_gem->mem_virtual)
		bufmgr_gem->vma_count++;
	if (bo_gem->gtt_virtual)
		bufmgr_gem->vma_count++;
	drm_intel_gem_bo_purge_vma_cache(bufmgr_gem);
}

static void drm_intel_gem_bo_open_vma(drm_intel_bufmgr_gem *bufmgr_gem,
				      drm_intel_bo_gem *bo_gem)
{
	bufmgr_gem->vma_open++;
	DRMLISTDEL(&bo_gem->vma_list);
	if (bo_gem->mem_virtual)
		bufmgr_gem->vma_count--;
	if (bo_gem->gtt_virtual)
		bufmgr_gem->vma_count--;
	drm_intel_gem_bo_purge_vma_cache(bufmgr_gem);
d920 1
a920 1
drm_intel_gem_bo_unreference_final(drm_intel_bo *bo, time_t time)
d922 1
a922 4
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_intel_gem_bo_bucket *bucket;
	int i;
d924 3
a926 13
	/* Unreference all the target buffers */
	for (i = 0; i < bo_gem->reloc_count; i++) {
		if (bo_gem->reloc_target_info[i].bo != bo) {
			drm_intel_gem_bo_unreference_locked_timed(bo_gem->
								  reloc_target_info[i].bo,
								  time);
		}
	}
	bo_gem->reloc_count = 0;
	bo_gem->used_as_reloc_target = false;

	DBG("bo_unreference final: %d (%s)\n",
	    bo_gem->gem_handle, bo_gem->name);
d928 6
a933 8
	/* release memory associated with this object */
	if (bo_gem->reloc_target_info) {
		free(bo_gem->reloc_target_info);
		bo_gem->reloc_target_info = NULL;
	}
	if (bo_gem->relocs) {
		free(bo_gem->relocs);
		bo_gem->relocs = NULL;
d935 2
d938 40
a977 7
	/* Clear any left-over mappings */
	if (bo_gem->map_count) {
		DBG("bo freed with non-zero map-count %d\n", bo_gem->map_count);
		bo_gem->map_count = 0;
		drm_intel_gem_bo_close_vma(bufmgr_gem, bo_gem);
		drm_intel_gem_bo_mark_mmaps_incoherent(bo);
	}
d979 2
a980 1
	DRMLISTDEL(&bo_gem->name_list);
d982 2
a983 753
	bucket = drm_intel_gem_bo_bucket_for_size(bufmgr_gem, bo->size);
	/* Put the buffer into our internal cache for reuse if we can. */
	if (bufmgr_gem->bo_reuse && bo_gem->reusable && bucket != NULL &&
	    drm_intel_gem_bo_madvise_internal(bufmgr_gem, bo_gem,
					      I915_MADV_DONTNEED)) {
		bo_gem->free_time = time;

		bo_gem->name = NULL;
		bo_gem->validate_index = -1;

		DRMLISTADDTAIL(&bo_gem->head, &bucket->head);
	} else {
		drm_intel_gem_bo_free(bo);
	}
}

static void drm_intel_gem_bo_unreference_locked_timed(drm_intel_bo *bo,
						      time_t time)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	assert(atomic_read(&bo_gem->refcount) > 0);
	if (atomic_dec_and_test(&bo_gem->refcount))
		drm_intel_gem_bo_unreference_final(bo, time);
}

static void drm_intel_gem_bo_unreference(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	assert(atomic_read(&bo_gem->refcount) > 0);

	if (atomic_add_unless(&bo_gem->refcount, -1, 1)) {
		drm_intel_bufmgr_gem *bufmgr_gem =
		    (drm_intel_bufmgr_gem *) bo->bufmgr;
		struct timespec time;

		clock_gettime(CLOCK_MONOTONIC, &time);

		pthread_mutex_lock(&bufmgr_gem->lock);

		if (atomic_dec_and_test(&bo_gem->refcount)) {
			drm_intel_gem_bo_unreference_final(bo, time.tv_sec);
			drm_intel_gem_cleanup_bo_cache(bufmgr_gem, time.tv_sec);
		}

		pthread_mutex_unlock(&bufmgr_gem->lock);
	}
}

static int drm_intel_gem_bo_map(drm_intel_bo *bo, int write_enable)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_set_domain set_domain;
	int ret;

	if (bo_gem->is_userptr) {
		/* Return the same user ptr */
		bo->virtual = bo_gem->user_virtual;
		return 0;
	}

	pthread_mutex_lock(&bufmgr_gem->lock);

	if (bo_gem->map_count++ == 0)
		drm_intel_gem_bo_open_vma(bufmgr_gem, bo_gem);

	if (!bo_gem->mem_virtual) {
		struct drm_i915_gem_mmap mmap_arg;

		DBG("bo_map: %d (%s), map_count=%d\n",
		    bo_gem->gem_handle, bo_gem->name, bo_gem->map_count);

		memclear(mmap_arg);
		mmap_arg.handle = bo_gem->gem_handle;
		mmap_arg.size = bo->size;
		ret = drmIoctl(bufmgr_gem->fd,
			       DRM_IOCTL_I915_GEM_MMAP,
			       &mmap_arg);
		if (ret != 0) {
			ret = -errno;
			DBG("%s:%d: Error mapping buffer %d (%s): %s .\n",
			    __FILE__, __LINE__, bo_gem->gem_handle,
			    bo_gem->name, strerror(errno));
			if (--bo_gem->map_count == 0)
				drm_intel_gem_bo_close_vma(bufmgr_gem, bo_gem);
			pthread_mutex_unlock(&bufmgr_gem->lock);
			return ret;
		}
		VG(VALGRIND_MALLOCLIKE_BLOCK(mmap_arg.addr_ptr, mmap_arg.size, 0, 1));
		bo_gem->mem_virtual = (void *)(uintptr_t) mmap_arg.addr_ptr;
	}
	DBG("bo_map: %d (%s) -> %p\n", bo_gem->gem_handle, bo_gem->name,
	    bo_gem->mem_virtual);
	bo->virtual = bo_gem->mem_virtual;

	memclear(set_domain);
	set_domain.handle = bo_gem->gem_handle;
	set_domain.read_domains = I915_GEM_DOMAIN_CPU;
	if (write_enable)
		set_domain.write_domain = I915_GEM_DOMAIN_CPU;
	else
		set_domain.write_domain = 0;
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_SET_DOMAIN,
		       &set_domain);
	if (ret != 0) {
		DBG("%s:%d: Error setting to CPU domain %d: %s\n",
		    __FILE__, __LINE__, bo_gem->gem_handle,
		    strerror(errno));
	}

	if (write_enable)
		bo_gem->mapped_cpu_write = true;

	drm_intel_gem_bo_mark_mmaps_incoherent(bo);
	VG(VALGRIND_MAKE_MEM_DEFINED(bo_gem->mem_virtual, bo->size));
	pthread_mutex_unlock(&bufmgr_gem->lock);

	return 0;
}

static int
map_gtt(drm_intel_bo *bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	int ret;

	if (bo_gem->is_userptr)
		return -EINVAL;

	if (bo_gem->map_count++ == 0)
		drm_intel_gem_bo_open_vma(bufmgr_gem, bo_gem);

	/* Get a mapping of the buffer if we haven't before. */
	if (bo_gem->gtt_virtual == NULL) {
		struct drm_i915_gem_mmap_gtt mmap_arg;

		DBG("bo_map_gtt: mmap %d (%s), map_count=%d\n",
		    bo_gem->gem_handle, bo_gem->name, bo_gem->map_count);

		memclear(mmap_arg);
		mmap_arg.handle = bo_gem->gem_handle;

		/* Get the fake offset back... */
		ret = drmIoctl(bufmgr_gem->fd,
			       DRM_IOCTL_I915_GEM_MMAP_GTT,
			       &mmap_arg);
		if (ret != 0) {
			ret = -errno;
			DBG("%s:%d: Error preparing buffer map %d (%s): %s .\n",
			    __FILE__, __LINE__,
			    bo_gem->gem_handle, bo_gem->name,
			    strerror(errno));
			if (--bo_gem->map_count == 0)
				drm_intel_gem_bo_close_vma(bufmgr_gem, bo_gem);
			return ret;
		}

		/* and mmap it */
		bo_gem->gtt_virtual = drm_mmap(0, bo->size, PROT_READ | PROT_WRITE,
					       MAP_SHARED, bufmgr_gem->fd,
					       mmap_arg.offset);
		if (bo_gem->gtt_virtual == MAP_FAILED) {
			bo_gem->gtt_virtual = NULL;
			ret = -errno;
			DBG("%s:%d: Error mapping buffer %d (%s): %s .\n",
			    __FILE__, __LINE__,
			    bo_gem->gem_handle, bo_gem->name,
			    strerror(errno));
			if (--bo_gem->map_count == 0)
				drm_intel_gem_bo_close_vma(bufmgr_gem, bo_gem);
			return ret;
		}
	}

	bo->virtual = bo_gem->gtt_virtual;

	DBG("bo_map_gtt: %d (%s) -> %p\n", bo_gem->gem_handle, bo_gem->name,
	    bo_gem->gtt_virtual);

	return 0;
}

int
drm_intel_gem_bo_map_gtt(drm_intel_bo *bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_set_domain set_domain;
	int ret;

	pthread_mutex_lock(&bufmgr_gem->lock);

	ret = map_gtt(bo);
	if (ret) {
		pthread_mutex_unlock(&bufmgr_gem->lock);
		return ret;
	}

	/* Now move it to the GTT domain so that the GPU and CPU
	 * caches are flushed and the GPU isn't actively using the
	 * buffer.
	 *
	 * The pagefault handler does this domain change for us when
	 * it has unbound the BO from the GTT, but it's up to us to
	 * tell it when we're about to use things if we had done
	 * rendering and it still happens to be bound to the GTT.
	 */
	memclear(set_domain);
	set_domain.handle = bo_gem->gem_handle;
	set_domain.read_domains = I915_GEM_DOMAIN_GTT;
	set_domain.write_domain = I915_GEM_DOMAIN_GTT;
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_SET_DOMAIN,
		       &set_domain);
	if (ret != 0) {
		DBG("%s:%d: Error setting domain %d: %s\n",
		    __FILE__, __LINE__, bo_gem->gem_handle,
		    strerror(errno));
	}

	drm_intel_gem_bo_mark_mmaps_incoherent(bo);
	VG(VALGRIND_MAKE_MEM_DEFINED(bo_gem->gtt_virtual, bo->size));
	pthread_mutex_unlock(&bufmgr_gem->lock);

	return 0;
}

/**
 * Performs a mapping of the buffer object like the normal GTT
 * mapping, but avoids waiting for the GPU to be done reading from or
 * rendering to the buffer.
 *
 * This is used in the implementation of GL_ARB_map_buffer_range: The
 * user asks to create a buffer, then does a mapping, fills some
 * space, runs a drawing command, then asks to map it again without
 * synchronizing because it guarantees that it won't write over the
 * data that the GPU is busy using (or, more specifically, that if it
 * does write over the data, it acknowledges that rendering is
 * undefined).
 */

int
drm_intel_gem_bo_map_unsynchronized(drm_intel_bo *bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
#ifdef HAVE_VALGRIND
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
#endif
	int ret;

	/* If the CPU cache isn't coherent with the GTT, then use a
	 * regular synchronized mapping.  The problem is that we don't
	 * track where the buffer was last used on the CPU side in
	 * terms of drm_intel_bo_map vs drm_intel_gem_bo_map_gtt, so
	 * we would potentially corrupt the buffer even when the user
	 * does reasonable things.
	 */
	if (!bufmgr_gem->has_llc)
		return drm_intel_gem_bo_map_gtt(bo);

	pthread_mutex_lock(&bufmgr_gem->lock);

	ret = map_gtt(bo);
	if (ret == 0) {
		drm_intel_gem_bo_mark_mmaps_incoherent(bo);
		VG(VALGRIND_MAKE_MEM_DEFINED(bo_gem->gtt_virtual, bo->size));
	}

	pthread_mutex_unlock(&bufmgr_gem->lock);

	return ret;
}

static int drm_intel_gem_bo_unmap(drm_intel_bo *bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	int ret = 0;

	if (bo == NULL)
		return 0;

	if (bo_gem->is_userptr)
		return 0;

	bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;

	pthread_mutex_lock(&bufmgr_gem->lock);

	if (bo_gem->map_count <= 0) {
		DBG("attempted to unmap an unmapped bo\n");
		pthread_mutex_unlock(&bufmgr_gem->lock);
		/* Preserve the old behaviour of just treating this as a
		 * no-op rather than reporting the error.
		 */
		return 0;
	}

	if (bo_gem->mapped_cpu_write) {
		struct drm_i915_gem_sw_finish sw_finish;

		/* Cause a flush to happen if the buffer's pinned for
		 * scanout, so the results show up in a timely manner.
		 * Unlike GTT set domains, this only does work if the
		 * buffer should be scanout-related.
		 */
		memclear(sw_finish);
		sw_finish.handle = bo_gem->gem_handle;
		ret = drmIoctl(bufmgr_gem->fd,
			       DRM_IOCTL_I915_GEM_SW_FINISH,
			       &sw_finish);
		ret = ret == -1 ? -errno : 0;

		bo_gem->mapped_cpu_write = false;
	}

	/* We need to unmap after every innovation as we cannot track
	 * an open vma for every bo as that will exhaasut the system
	 * limits and cause later failures.
	 */
	if (--bo_gem->map_count == 0) {
		drm_intel_gem_bo_close_vma(bufmgr_gem, bo_gem);
		drm_intel_gem_bo_mark_mmaps_incoherent(bo);
		bo->virtual = NULL;
	}
	pthread_mutex_unlock(&bufmgr_gem->lock);

	return ret;
}

int
drm_intel_gem_bo_unmap_gtt(drm_intel_bo *bo)
{
	return drm_intel_gem_bo_unmap(bo);
}

static int
drm_intel_gem_bo_subdata(drm_intel_bo *bo, unsigned long offset,
			 unsigned long size, const void *data)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_pwrite pwrite;
	int ret;

	if (bo_gem->is_userptr)
		return -EINVAL;

	memclear(pwrite);
	pwrite.handle = bo_gem->gem_handle;
	pwrite.offset = offset;
	pwrite.size = size;
	pwrite.data_ptr = (uint64_t) (uintptr_t) data;
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_PWRITE,
		       &pwrite);
	if (ret != 0) {
		ret = -errno;
		DBG("%s:%d: Error writing data to buffer %d: (%d %d) %s .\n",
		    __FILE__, __LINE__, bo_gem->gem_handle, (int)offset,
		    (int)size, strerror(errno));
	}

	return ret;
}

static int
drm_intel_gem_get_pipe_from_crtc_id(drm_intel_bufmgr *bufmgr, int crtc_id)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bufmgr;
	struct drm_i915_get_pipe_from_crtc_id get_pipe_from_crtc_id;
	int ret;

	memclear(get_pipe_from_crtc_id);
	get_pipe_from_crtc_id.crtc_id = crtc_id;
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GET_PIPE_FROM_CRTC_ID,
		       &get_pipe_from_crtc_id);
	if (ret != 0) {
		/* We return -1 here to signal that we don't
		 * know which pipe is associated with this crtc.
		 * This lets the caller know that this information
		 * isn't available; using the wrong pipe for
		 * vblank waiting can cause the chipset to lock up
		 */
		return -1;
	}

	return get_pipe_from_crtc_id.pipe;
}

static int
drm_intel_gem_bo_get_subdata(drm_intel_bo *bo, unsigned long offset,
			     unsigned long size, void *data)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_pread pread;
	int ret;

	if (bo_gem->is_userptr)
		return -EINVAL;

	memclear(pread);
	pread.handle = bo_gem->gem_handle;
	pread.offset = offset;
	pread.size = size;
	pread.data_ptr = (uint64_t) (uintptr_t) data;
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_PREAD,
		       &pread);
	if (ret != 0) {
		ret = -errno;
		DBG("%s:%d: Error reading data from buffer %d: (%d %d) %s .\n",
		    __FILE__, __LINE__, bo_gem->gem_handle, (int)offset,
		    (int)size, strerror(errno));
	}

	return ret;
}

/** Waits for all GPU rendering with the object to have completed. */
static void
drm_intel_gem_bo_wait_rendering(drm_intel_bo *bo)
{
	drm_intel_gem_bo_start_gtt_access(bo, 1);
}

/**
 * Waits on a BO for the given amount of time.
 *
 * @@bo: buffer object to wait for
 * @@timeout_ns: amount of time to wait in nanoseconds.
 *   If value is less than 0, an infinite wait will occur.
 *
 * Returns 0 if the wait was successful ie. the last batch referencing the
 * object has completed within the allotted time. Otherwise some negative return
 * value describes the error. Of particular interest is -ETIME when the wait has
 * failed to yield the desired result.
 *
 * Similar to drm_intel_gem_bo_wait_rendering except a timeout parameter allows
 * the operation to give up after a certain amount of time. Another subtle
 * difference is the internal locking semantics are different (this variant does
 * not hold the lock for the duration of the wait). This makes the wait subject
 * to a larger userspace race window.
 *
 * The implementation shall wait until the object is no longer actively
 * referenced within a batch buffer at the time of the call. The wait will
 * not guarantee that the buffer is re-issued via another thread, or an flinked
 * handle. Userspace must make sure this race does not occur if such precision
 * is important.
 *
 * Note that some kernels have broken the inifite wait for negative values
 * promise, upgrade to latest stable kernels if this is the case.
 */
int
drm_intel_gem_bo_wait(drm_intel_bo *bo, int64_t timeout_ns)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_wait wait;
	int ret;

	if (!bufmgr_gem->has_wait_timeout) {
		DBG("%s:%d: Timed wait is not supported. Falling back to "
		    "infinite wait\n", __FILE__, __LINE__);
		if (timeout_ns) {
			drm_intel_gem_bo_wait_rendering(bo);
			return 0;
		} else {
			return drm_intel_gem_bo_busy(bo) ? -ETIME : 0;
		}
	}

	memclear(wait);
	wait.bo_handle = bo_gem->gem_handle;
	wait.timeout_ns = timeout_ns;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_WAIT, &wait);
	if (ret == -1)
		return -errno;

	return ret;
}

/**
 * Sets the object to the GTT read and possibly write domain, used by the X
 * 2D driver in the absence of kernel support to do drm_intel_gem_bo_map_gtt().
 *
 * In combination with drm_intel_gem_bo_pin() and manual fence management, we
 * can do tiled pixmaps this way.
 */
void
drm_intel_gem_bo_start_gtt_access(drm_intel_bo *bo, int write_enable)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_set_domain set_domain;
	int ret;

	memclear(set_domain);
	set_domain.handle = bo_gem->gem_handle;
	set_domain.read_domains = I915_GEM_DOMAIN_GTT;
	set_domain.write_domain = write_enable ? I915_GEM_DOMAIN_GTT : 0;
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_SET_DOMAIN,
		       &set_domain);
	if (ret != 0) {
		DBG("%s:%d: Error setting memory domains %d (%08x %08x): %s .\n",
		    __FILE__, __LINE__, bo_gem->gem_handle,
		    set_domain.read_domains, set_domain.write_domain,
		    strerror(errno));
	}
}

static void
drm_intel_bufmgr_gem_destroy(drm_intel_bufmgr *bufmgr)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bufmgr;
	struct drm_gem_close close_bo;
	int i, ret;

	free(bufmgr_gem->exec2_objects);
	free(bufmgr_gem->exec_objects);
	free(bufmgr_gem->exec_bos);

	pthread_mutex_destroy(&bufmgr_gem->lock);

	/* Free any cached buffer objects we were going to reuse */
	for (i = 0; i < bufmgr_gem->num_buckets; i++) {
		struct drm_intel_gem_bo_bucket *bucket =
		    &bufmgr_gem->cache_bucket[i];
		drm_intel_bo_gem *bo_gem;

		while (!DRMLISTEMPTY(&bucket->head)) {
			bo_gem = DRMLISTENTRY(drm_intel_bo_gem,
					      bucket->head.next, head);
			DRMLISTDEL(&bo_gem->head);

			drm_intel_gem_bo_free(&bo_gem->bo);
		}
	}

	/* Release userptr bo kept hanging around for optimisation. */
	if (bufmgr_gem->userptr_active.ptr) {
		memclear(close_bo);
		close_bo.handle = bufmgr_gem->userptr_active.handle;
		ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_GEM_CLOSE, &close_bo);
		free(bufmgr_gem->userptr_active.ptr);
		if (ret)
			fprintf(stderr,
				"Failed to release test userptr object! (%d) "
				"i915 kernel driver may not be sane!\n", errno);
	}

	free(bufmgr);
}

/**
 * Adds the target buffer to the validation list and adds the relocation
 * to the reloc_buffer's relocation list.
 *
 * The relocation entry at the given offset must already contain the
 * precomputed relocation value, because the kernel will optimize out
 * the relocation entry write when the buffer hasn't moved from the
 * last known offset in target_bo.
 */
static int
do_bo_emit_reloc(drm_intel_bo *bo, uint32_t offset,
		 drm_intel_bo *target_bo, uint32_t target_offset,
		 uint32_t read_domains, uint32_t write_domain,
		 bool need_fence)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	drm_intel_bo_gem *target_bo_gem = (drm_intel_bo_gem *) target_bo;
	bool fenced_command;

	if (bo_gem->has_error)
		return -ENOMEM;

	if (target_bo_gem->has_error) {
		bo_gem->has_error = true;
		return -ENOMEM;
	}

	/* We never use HW fences for rendering on 965+ */
	if (bufmgr_gem->gen >= 4)
		need_fence = false;

	fenced_command = need_fence;
	if (target_bo_gem->tiling_mode == I915_TILING_NONE)
		need_fence = false;

	/* Create a new relocation list if needed */
	if (bo_gem->relocs == NULL && drm_intel_setup_reloc_list(bo))
		return -ENOMEM;

	/* Check overflow */
	assert(bo_gem->reloc_count < bufmgr_gem->max_relocs);

	/* Check args */
	assert(offset <= bo->size - 4);
	assert((write_domain & (write_domain - 1)) == 0);

	/* An object needing a fence is a tiled buffer, so it won't have
	 * relocs to other buffers.
	 */
	if (need_fence) {
		assert(target_bo_gem->reloc_count == 0);
		target_bo_gem->reloc_tree_fences = 1;
	}

	/* Make sure that we're not adding a reloc to something whose size has
	 * already been accounted for.
	 */
	assert(!bo_gem->used_as_reloc_target);
	if (target_bo_gem != bo_gem) {
		target_bo_gem->used_as_reloc_target = true;
		bo_gem->reloc_tree_size += target_bo_gem->reloc_tree_size;
		bo_gem->reloc_tree_fences += target_bo_gem->reloc_tree_fences;
	}

	bo_gem->relocs[bo_gem->reloc_count].offset = offset;
	bo_gem->relocs[bo_gem->reloc_count].delta = target_offset;
	bo_gem->relocs[bo_gem->reloc_count].target_handle =
	    target_bo_gem->gem_handle;
	bo_gem->relocs[bo_gem->reloc_count].read_domains = read_domains;
	bo_gem->relocs[bo_gem->reloc_count].write_domain = write_domain;
	bo_gem->relocs[bo_gem->reloc_count].presumed_offset = target_bo->offset64;

	bo_gem->reloc_target_info[bo_gem->reloc_count].bo = target_bo;
	if (target_bo != bo)
		drm_intel_gem_bo_reference(target_bo);
	if (fenced_command)
		bo_gem->reloc_target_info[bo_gem->reloc_count].flags =
			DRM_INTEL_RELOC_FENCE;
	else
		bo_gem->reloc_target_info[bo_gem->reloc_count].flags = 0;

	bo_gem->reloc_count++;

	return 0;
}

static int
drm_intel_gem_bo_emit_reloc(drm_intel_bo *bo, uint32_t offset,
			    drm_intel_bo *target_bo, uint32_t target_offset,
			    uint32_t read_domains, uint32_t write_domain)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;

	return do_bo_emit_reloc(bo, offset, target_bo, target_offset,
				read_domains, write_domain,
				!bufmgr_gem->fenced_relocs);
}

static int
drm_intel_gem_bo_emit_reloc_fence(drm_intel_bo *bo, uint32_t offset,
				  drm_intel_bo *target_bo,
				  uint32_t target_offset,
				  uint32_t read_domains, uint32_t write_domain)
{
	return do_bo_emit_reloc(bo, offset, target_bo, target_offset,
				read_domains, write_domain, true);
}

int
drm_intel_gem_bo_get_reloc_count(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	return bo_gem->reloc_count;
}

/**
 * Removes existing relocation entries in the BO after "start".
 *
 * This allows a user to avoid a two-step process for state setup with
 * counting up all the buffer objects and doing a
 * drm_intel_bufmgr_check_aperture_space() before emitting any of the
 * relocations for the state setup.  Instead, save the state of the
 * batchbuffer including drm_intel_gem_get_reloc_count(), emit all the
 * state, and then check if it still fits in the aperture.
 *
 * Any further drm_intel_bufmgr_check_aperture_space() queries
 * involving this buffer in the tree are undefined after this call.
 */
void
drm_intel_gem_bo_clear_relocs(drm_intel_bo *bo, int start)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	int i;
	struct timespec time;

	clock_gettime(CLOCK_MONOTONIC, &time);

	assert(bo_gem->reloc_count >= start);

	/* Unreference the cleared target buffers */
	pthread_mutex_lock(&bufmgr_gem->lock);

	for (i = start; i < bo_gem->reloc_count; i++) {
		drm_intel_bo_gem *target_bo_gem = (drm_intel_bo_gem *) bo_gem->reloc_target_info[i].bo;
		if (&target_bo_gem->bo != bo) {
			bo_gem->reloc_tree_fences -= target_bo_gem->reloc_tree_fences;
			drm_intel_gem_bo_unreference_locked_timed(&target_bo_gem->bo,
								  time.tv_sec);
		}
	}
	bo_gem->reloc_count = start;

	pthread_mutex_unlock(&bufmgr_gem->lock);

}

/**
 * Walk the tree of relocations rooted at BO and accumulate the list of
 * validations to be performed and update the relocation buffers with
 * index values into the validation list.
 */
static void
drm_intel_gem_bo_process_reloc(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	int i;

	if (bo_gem->relocs == NULL)
		return;

	for (i = 0; i < bo_gem->reloc_count; i++) {
		drm_intel_bo *target_bo = bo_gem->reloc_target_info[i].bo;

		if (target_bo == bo)
			continue;

		drm_intel_gem_bo_mark_mmaps_incoherent(bo);

		/* Continue walking the tree depth-first. */
		drm_intel_gem_bo_process_reloc(target_bo);

		/* Add the target to the validate list */
		drm_intel_add_validate_buffer(target_bo);
	}
}

static void
drm_intel_gem_bo_process_reloc2(drm_intel_bo *bo)
{
a984 235
	int i;

	if (bo_gem->relocs == NULL)
		return;

	for (i = 0; i < bo_gem->reloc_count; i++) {
		drm_intel_bo *target_bo = bo_gem->reloc_target_info[i].bo;
		int need_fence;

		if (target_bo == bo)
			continue;

		drm_intel_gem_bo_mark_mmaps_incoherent(bo);

		/* Continue walking the tree depth-first. */
		drm_intel_gem_bo_process_reloc2(target_bo);

		need_fence = (bo_gem->reloc_target_info[i].flags &
			      DRM_INTEL_RELOC_FENCE);

		/* Add the target to the validate list */
		drm_intel_add_validate_buffer2(target_bo, need_fence);
	}
}


static void
drm_intel_update_buffer_offsets(drm_intel_bufmgr_gem *bufmgr_gem)
{
	int i;

	for (i = 0; i < bufmgr_gem->exec_count; i++) {
		drm_intel_bo *bo = bufmgr_gem->exec_bos[i];
		drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

		/* Update the buffer offset */
		if (bufmgr_gem->exec_objects[i].offset != bo->offset64) {
			DBG("BO %d (%s) migrated: 0x%08lx -> 0x%08llx\n",
			    bo_gem->gem_handle, bo_gem->name, bo->offset64,
			    (unsigned long long)bufmgr_gem->exec_objects[i].
			    offset);
			bo->offset64 = bufmgr_gem->exec_objects[i].offset;
			bo->offset = bufmgr_gem->exec_objects[i].offset;
		}
	}
}

static void
drm_intel_update_buffer_offsets2 (drm_intel_bufmgr_gem *bufmgr_gem)
{
	int i;

	for (i = 0; i < bufmgr_gem->exec_count; i++) {
		drm_intel_bo *bo = bufmgr_gem->exec_bos[i];
		drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;

		/* Update the buffer offset */
		if (bufmgr_gem->exec2_objects[i].offset != bo->offset64) {
			DBG("BO %d (%s) migrated: 0x%08lx -> 0x%08llx\n",
			    bo_gem->gem_handle, bo_gem->name, bo->offset64,
			    (unsigned long long)bufmgr_gem->exec2_objects[i].offset);
			bo->offset64 = bufmgr_gem->exec2_objects[i].offset;
			bo->offset = bufmgr_gem->exec2_objects[i].offset;
		}
	}
}

void
drm_intel_gem_bo_aub_dump_bmp(drm_intel_bo *bo,
			      int x1, int y1, int width, int height,
			      enum aub_dump_bmp_format format,
			      int pitch, int offset)
{
}

static int
drm_intel_gem_bo_exec(drm_intel_bo *bo, int used,
		      drm_clip_rect_t * cliprects, int num_cliprects, int DR4)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_execbuffer execbuf;
	int ret, i;

	if (bo_gem->has_error)
		return -ENOMEM;

	pthread_mutex_lock(&bufmgr_gem->lock);
	/* Update indices and set up the validate list. */
	drm_intel_gem_bo_process_reloc(bo);

	/* Add the batch buffer to the validation list.  There are no
	 * relocations pointing to it.
	 */
	drm_intel_add_validate_buffer(bo);

	memclear(execbuf);
	execbuf.buffers_ptr = (uintptr_t) bufmgr_gem->exec_objects;
	execbuf.buffer_count = bufmgr_gem->exec_count;
	execbuf.batch_start_offset = 0;
	execbuf.batch_len = used;
	execbuf.cliprects_ptr = (uintptr_t) cliprects;
	execbuf.num_cliprects = num_cliprects;
	execbuf.DR1 = 0;
	execbuf.DR4 = DR4;

	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_EXECBUFFER,
		       &execbuf);
	if (ret != 0) {
		ret = -errno;
		if (errno == ENOSPC) {
			DBG("Execbuffer fails to pin. "
			    "Estimate: %u. Actual: %u. Available: %u\n",
			    drm_intel_gem_estimate_batch_space(bufmgr_gem->exec_bos,
							       bufmgr_gem->
							       exec_count),
			    drm_intel_gem_compute_batch_space(bufmgr_gem->exec_bos,
							      bufmgr_gem->
							      exec_count),
			    (unsigned int)bufmgr_gem->gtt_size);
		}
	}
	drm_intel_update_buffer_offsets(bufmgr_gem);

	if (bufmgr_gem->bufmgr.debug)
		drm_intel_gem_dump_validation_list(bufmgr_gem);

	for (i = 0; i < bufmgr_gem->exec_count; i++) {
		drm_intel_bo *bo = bufmgr_gem->exec_bos[i];
		drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

		bo_gem->idle = false;

		/* Disconnect the buffer from the validate list */
		bo_gem->validate_index = -1;
		bufmgr_gem->exec_bos[i] = NULL;
	}
	bufmgr_gem->exec_count = 0;
	pthread_mutex_unlock(&bufmgr_gem->lock);

	return ret;
}

static int
do_exec2(drm_intel_bo *bo, int used, drm_intel_context *ctx,
	 drm_clip_rect_t *cliprects, int num_cliprects, int DR4,
	 unsigned int flags)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bo->bufmgr;
	struct drm_i915_gem_execbuffer2 execbuf;
	int ret = 0;
	int i;

	switch (flags & 0x7) {
	default:
		return -EINVAL;
	case I915_EXEC_BLT:
		if (!bufmgr_gem->has_blt)
			return -EINVAL;
		break;
	case I915_EXEC_BSD:
		if (!bufmgr_gem->has_bsd)
			return -EINVAL;
		break;
	case I915_EXEC_VEBOX:
		if (!bufmgr_gem->has_vebox)
			return -EINVAL;
		break;
	case I915_EXEC_RENDER:
	case I915_EXEC_DEFAULT:
		break;
	}

	pthread_mutex_lock(&bufmgr_gem->lock);
	/* Update indices and set up the validate list. */
	drm_intel_gem_bo_process_reloc2(bo);

	/* Add the batch buffer to the validation list.  There are no relocations
	 * pointing to it.
	 */
	drm_intel_add_validate_buffer2(bo, 0);

	memclear(execbuf);
	execbuf.buffers_ptr = (uintptr_t)bufmgr_gem->exec2_objects;
	execbuf.buffer_count = bufmgr_gem->exec_count;
	execbuf.batch_start_offset = 0;
	execbuf.batch_len = used;
	execbuf.cliprects_ptr = (uintptr_t)cliprects;
	execbuf.num_cliprects = num_cliprects;
	execbuf.DR1 = 0;
	execbuf.DR4 = DR4;
	execbuf.flags = flags;
	if (ctx == NULL)
		i915_execbuffer2_set_context_id(execbuf, 0);
	else
		i915_execbuffer2_set_context_id(execbuf, ctx->ctx_id);
	execbuf.rsvd2 = 0;

	if (bufmgr_gem->no_exec)
		goto skip_execution;

	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_EXECBUFFER2,
		       &execbuf);
	if (ret != 0) {
		ret = -errno;
		if (ret == -ENOSPC) {
			DBG("Execbuffer fails to pin. "
			    "Estimate: %u. Actual: %u. Available: %u\n",
			    drm_intel_gem_estimate_batch_space(bufmgr_gem->exec_bos,
							       bufmgr_gem->exec_count),
			    drm_intel_gem_compute_batch_space(bufmgr_gem->exec_bos,
							      bufmgr_gem->exec_count),
			    (unsigned int) bufmgr_gem->gtt_size);
		}
	}
	drm_intel_update_buffer_offsets2(bufmgr_gem);

skip_execution:
	if (bufmgr_gem->bufmgr.debug)
		drm_intel_gem_dump_validation_list(bufmgr_gem);

	for (i = 0; i < bufmgr_gem->exec_count; i++) {
		drm_intel_bo *bo = bufmgr_gem->exec_bos[i];
		drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *)bo;

		bo_gem->idle = false;

		/* Disconnect the buffer from the validate list */
		bo_gem->validate_index = -1;
		bufmgr_gem->exec_bos[i] = NULL;
	}
	bufmgr_gem->exec_count = 0;
	pthread_mutex_unlock(&bufmgr_gem->lock);
d986 2
a987 2
	return ret;
}
d989 7
a995 17
static int
drm_intel_gem_bo_exec2(drm_intel_bo *bo, int used,
		       drm_clip_rect_t *cliprects, int num_cliprects,
		       int DR4)
{
	return do_exec2(bo, used, NULL, cliprects, num_cliprects, DR4,
			I915_EXEC_RENDER);
}

static int
drm_intel_gem_bo_mrb_exec2(drm_intel_bo *bo, int used,
			drm_clip_rect_t *cliprects, int num_cliprects, int DR4,
			unsigned int flags)
{
	return do_exec2(bo, used, NULL, cliprects, num_cliprects, DR4,
			flags);
}
d997 1
a997 5
int
drm_intel_gem_bo_context_exec(drm_intel_bo *bo, drm_intel_context *ctx,
			      int used, unsigned int flags)
{
	return do_exec2(bo, used, ctx, NULL, 0, 0, flags);
d1003 14
a1016 14
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_pin pin;
	int ret;

	memclear(pin);
	pin.handle = bo_gem->gem_handle;
	pin.alignment = alignment;

	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_PIN,
		       &pin);
	if (ret != 0)
		return -errno;
d1018 2
a1019 3
	bo->offset64 = pin.offset;
	bo->offset = pin.offset;
	return 0;
d1025 4
a1028 7
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_unpin unpin;
	int ret;

	memclear(unpin);
	unpin.handle = bo_gem->gem_handle;
d1030 1
a1030 6
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_UNPIN, &unpin);
	if (ret != 0)
		return -errno;

	return 0;
}
d1032 3
a1034 14
static int
drm_intel_gem_bo_set_tiling_internal(drm_intel_bo *bo,
				     uint32_t tiling_mode,
				     uint32_t stride)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	struct drm_i915_gem_set_tiling set_tiling;
	int ret;

	if (bo_gem->global_name == 0 &&
	    tiling_mode == bo_gem->tiling_mode &&
	    stride == bo_gem->stride)
		return 0;
d1036 1
a1036 21
	memset(&set_tiling, 0, sizeof(set_tiling));
	do {
		/* set_tiling is slightly broken and overwrites the
		 * input on the error path, so we have to open code
		 * rmIoctl.
		 */
		set_tiling.handle = bo_gem->gem_handle;
		set_tiling.tiling_mode = tiling_mode;
		set_tiling.stride = stride;

		ret = ioctl(bufmgr_gem->fd,
			    DRM_IOCTL_I915_GEM_SET_TILING,
			    &set_tiling);
	} while (ret == -1 && (errno == EINTR || errno == EAGAIN));
	if (ret == -1)
		return -errno;

	bo_gem->tiling_mode = set_tiling.tiling_mode;
	bo_gem->swizzle_mode = set_tiling.swizzle_mode;
	bo_gem->stride = set_tiling.stride;
	return 0;
d1040 1
a1040 1
drm_intel_gem_bo_set_tiling(drm_intel_bo *bo, uint32_t * tiling_mode,
d1043 4
a1046 19
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	int ret;

	/* Tiling with userptr surfaces is not supported
	 * on all hardware so refuse it for time being.
	 */
	if (bo_gem->is_userptr)
		return -EINVAL;

	/* Linear buffers have no stride. By ensuring that we only ever use
	 * stride 0 with linear buffers, we simplify our code.
	 */
	if (*tiling_mode == I915_TILING_NONE)
		stride = 0;

	ret = drm_intel_gem_bo_set_tiling_internal(bo, *tiling_mode, stride);
	if (ret == 0)
		drm_intel_bo_gem_set_in_aperture_size(bufmgr_gem, bo_gem, 0);
d1048 2
a1049 3
	*tiling_mode = bo_gem->tiling_mode;
	return ret;
}
d1051 3
a1053 5
static int
drm_intel_gem_bo_get_tiling(drm_intel_bo *bo, uint32_t * tiling_mode,
			    uint32_t * swizzle_mode)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
d1055 2
d1058 4
a1061 3
	*swizzle_mode = bo_gem->swizzle_mode;
	return 0;
}
d1063 2
a1064 84
drm_intel_bo *
drm_intel_bo_gem_create_from_prime(drm_intel_bufmgr *bufmgr, int prime_fd, int size)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bufmgr;
	int ret;
	uint32_t handle;
	drm_intel_bo_gem *bo_gem;
	struct drm_i915_gem_get_tiling get_tiling;
	drmMMListHead *list;

	ret = drmPrimeFDToHandle(bufmgr_gem->fd, prime_fd, &handle);

	/*
	 * See if the kernel has already returned this buffer to us. Just as
	 * for named buffers, we must not create two bo's pointing at the same
	 * kernel object
	 */
	pthread_mutex_lock(&bufmgr_gem->lock);
	for (list = bufmgr_gem->named.next;
	     list != &bufmgr_gem->named;
	     list = list->next) {
		bo_gem = DRMLISTENTRY(drm_intel_bo_gem, list, name_list);
		if (bo_gem->gem_handle == handle) {
			drm_intel_gem_bo_reference(&bo_gem->bo);
			pthread_mutex_unlock(&bufmgr_gem->lock);
			return &bo_gem->bo;
		}
	}

	if (ret) {
	  fprintf(stderr,"ret is %d %d\n", ret, errno);
	  pthread_mutex_unlock(&bufmgr_gem->lock);
		return NULL;
	}

	bo_gem = calloc(1, sizeof(*bo_gem));
	if (!bo_gem) {
		pthread_mutex_unlock(&bufmgr_gem->lock);
		return NULL;
	}
	/* Determine size of bo.  The fd-to-handle ioctl really should
	 * return the size, but it doesn't.  If we have kernel 3.12 or
	 * later, we can lseek on the prime fd to get the size.  Older
	 * kernels will just fail, in which case we fall back to the
	 * provided (estimated or guess size). */
	ret = lseek(prime_fd, 0, SEEK_END);
	if (ret != -1)
		bo_gem->bo.size = ret;
	else
		bo_gem->bo.size = size;

	bo_gem->bo.handle = handle;
	bo_gem->bo.bufmgr = bufmgr;

	bo_gem->gem_handle = handle;

	atomic_set(&bo_gem->refcount, 1);

	bo_gem->name = "prime";
	bo_gem->validate_index = -1;
	bo_gem->reloc_tree_fences = 0;
	bo_gem->used_as_reloc_target = false;
	bo_gem->has_error = false;
	bo_gem->reusable = false;

	DRMINITLISTHEAD(&bo_gem->vma_list);
	DRMLISTADDTAIL(&bo_gem->name_list, &bufmgr_gem->named);
	pthread_mutex_unlock(&bufmgr_gem->lock);

	memclear(get_tiling);
	get_tiling.handle = bo_gem->gem_handle;
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_GET_TILING,
		       &get_tiling);
	if (ret != 0) {
		drm_intel_gem_bo_unreference(&bo_gem->bo);
		return NULL;
	}
	bo_gem->tiling_mode = get_tiling.tiling_mode;
	bo_gem->swizzle_mode = get_tiling.swizzle_mode;
	/* XXX stride is unknown */
	drm_intel_bo_gem_set_in_aperture_size(bufmgr_gem, bo_gem, 0);

	return &bo_gem->bo;
d1067 3
a1069 2
int
drm_intel_bo_gem_export_to_prime(drm_intel_bo *bo, int *prime_fd)
d1071 1
a1071 7
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	pthread_mutex_lock(&bufmgr_gem->lock);
        if (DRMLISTEMPTY(&bo_gem->name_list))
                DRMLISTADDTAIL(&bo_gem->name_list, &bufmgr_gem->named);
	pthread_mutex_unlock(&bufmgr_gem->lock);
d1073 3
a1075 7
	if (drmPrimeHandleToFD(bufmgr_gem->fd, bo_gem->gem_handle,
			       DRM_CLOEXEC, prime_fd) != 0)
		return -errno;

	bo_gem->reusable = false;

	return 0;
d1079 1
a1079 1
drm_intel_gem_bo_flink(drm_intel_bo *bo, uint32_t * name)
d1081 4
a1084 3
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	int ret;
d1086 11
a1096 24
	if (!bo_gem->global_name) {
		struct drm_gem_flink flink;

		memclear(flink);
		flink.handle = bo_gem->gem_handle;

		pthread_mutex_lock(&bufmgr_gem->lock);

		ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_GEM_FLINK, &flink);
		if (ret != 0) {
			pthread_mutex_unlock(&bufmgr_gem->lock);
			return -errno;
		}

		bo_gem->global_name = flink.name;
		bo_gem->reusable = false;

                if (DRMLISTEMPTY(&bo_gem->name_list))
                        DRMLISTADDTAIL(&bo_gem->name_list, &bufmgr_gem->named);
		pthread_mutex_unlock(&bufmgr_gem->lock);
	}

	*name = bo_gem->global_name;
	return 0;
d1109 2
a1110 16
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bufmgr;

	bufmgr_gem->bo_reuse = true;
}

/**
 * Enable use of fenced reloc type.
 *
 * New code should enable this to avoid unnecessary fence register
 * allocation.  If this option is not enabled, all relocs will have fence
 * register allocated.
 */
void
drm_intel_bufmgr_gem_enable_fenced_relocs(drm_intel_bufmgr *bufmgr)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bufmgr;
d1112 3
a1114 2
	if (bufmgr_gem->bufmgr.bo_exec == drm_intel_gem_bo_exec2)
		bufmgr_gem->fenced_relocs = true;
d1124 3
a1126 14
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	int i;
	int total = 0;

	if (bo == NULL || bo_gem->included_in_check_aperture)
		return 0;

	total += bo->size;
	bo_gem->included_in_check_aperture = true;

	for (i = 0; i < bo_gem->reloc_count; i++)
		total +=
		    drm_intel_gem_bo_get_aperture_space(bo_gem->
							reloc_target_info[i].bo);
d1128 2
a1129 16
	return total;
}

/**
 * Count the number of buffers in this list that need a fence reg
 *
 * If the count is greater than the number of available regs, we'll have
 * to ask the caller to resubmit a batch with fewer tiled buffers.
 *
 * This function over-counts if the same buffer is used multiple times.
 */
static unsigned int
drm_intel_gem_total_fences(drm_intel_bo ** bo_array, int count)
{
	int i;
	unsigned int total = 0;
d1131 2
a1132 2
	for (i = 0; i < count; i++) {
		drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo_array[i];
d1134 2
a1135 2
		if (bo_gem == NULL)
			continue;
d1137 1
a1137 3
		total += bo_gem->reloc_tree_fences;
	}
	return total;
d1147 2
a1148 2
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	int i;
d1150 2
a1151 2
	if (bo == NULL || !bo_gem->included_in_check_aperture)
		return;
d1153 1
a1153 1
	bo_gem->included_in_check_aperture = false;
d1155 2
a1156 3
	for (i = 0; i < bo_gem->reloc_count; i++)
		drm_intel_gem_bo_clear_aperture_space_flag(bo_gem->
							   reloc_target_info[i].bo);
d1166 2
a1167 2
	int i;
	unsigned int total = 0;
d1169 6
a1174 6
	for (i = 0; i < count; i++) {
		drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo_array[i];
		if (bo_gem != NULL)
			total += bo_gem->reloc_tree_size;
	}
	return total;
d1185 2
a1186 2
	int i;
	unsigned int total = 0;
d1188 2
a1189 17
	for (i = 0; i < count; i++) {
		total += drm_intel_gem_bo_get_aperture_space(bo_array[i]);
		/* For the first buffer object in the array, we get an
		 * accurate count back for its reloc_tree size (since nothing
		 * had been flagged as being counted yet).  We can save that
		 * value out as a more conservative reloc_tree_size that
		 * avoids double-counting target buffers.  Since the first
		 * buffer happens to usually be the batch buffer in our
		 * callers, this can pull us back from doing the tree
		 * walk on every new batch emit.
		 */
		if (i == 0) {
			drm_intel_bo_gem *bo_gem =
			    (drm_intel_bo_gem *) bo_array[i];
			bo_gem->reloc_tree_size = total;
		}
	}
d1191 3
a1193 3
	for (i = 0; i < count; i++)
		drm_intel_gem_bo_clear_aperture_space_flag(bo_array[i]);
	return total;
d1215 16
a1230 332
	drm_intel_bufmgr_gem *bufmgr_gem =
	    (drm_intel_bufmgr_gem *) bo_array[0]->bufmgr;
	unsigned int total = 0;
	unsigned int threshold = bufmgr_gem->gtt_size * 3 / 4;
	int total_fences;

	/* Check for fence reg constraints if necessary */
	if (bufmgr_gem->available_fences) {
		total_fences = drm_intel_gem_total_fences(bo_array, count);
		if (total_fences > bufmgr_gem->available_fences)
			return -ENOSPC;
	}

	total = drm_intel_gem_estimate_batch_space(bo_array, count);

	if (total > threshold)
		total = drm_intel_gem_compute_batch_space(bo_array, count);

	if (total > threshold) {
		DBG("check_space: overflowed available aperture, "
		    "%dkb vs %dkb\n",
		    total / 1024, (int)bufmgr_gem->gtt_size / 1024);
		return -ENOSPC;
	} else {
		DBG("drm_check_space: total %dkb vs bufgr %dkb\n", total / 1024,
		    (int)bufmgr_gem->gtt_size / 1024);
		return 0;
	}
}

/*
 * Disable buffer reuse for objects which are shared with the kernel
 * as scanout buffers
 */
static int
drm_intel_gem_bo_disable_reuse(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	bo_gem->reusable = false;
	return 0;
}

static int
drm_intel_gem_bo_is_reusable(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	return bo_gem->reusable;
}

static int
_drm_intel_gem_bo_references(drm_intel_bo *bo, drm_intel_bo *target_bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	int i;

	for (i = 0; i < bo_gem->reloc_count; i++) {
		if (bo_gem->reloc_target_info[i].bo == target_bo)
			return 1;
		if (bo == bo_gem->reloc_target_info[i].bo)
			continue;
		if (_drm_intel_gem_bo_references(bo_gem->reloc_target_info[i].bo,
						target_bo))
			return 1;
	}

	return 0;
}

/** Return true if target_bo is referenced by bo's relocation tree. */
static int
drm_intel_gem_bo_references(drm_intel_bo *bo, drm_intel_bo *target_bo)
{
	drm_intel_bo_gem *target_bo_gem = (drm_intel_bo_gem *) target_bo;

	if (bo == NULL || target_bo == NULL)
		return 0;
	if (target_bo_gem->used_as_reloc_target)
		return _drm_intel_gem_bo_references(bo, target_bo);
	return 0;
}

static void
add_bucket(drm_intel_bufmgr_gem *bufmgr_gem, int size)
{
	unsigned int i = bufmgr_gem->num_buckets;

	assert(i < ARRAY_SIZE(bufmgr_gem->cache_bucket));

	DRMINITLISTHEAD(&bufmgr_gem->cache_bucket[i].head);
	bufmgr_gem->cache_bucket[i].size = size;
	bufmgr_gem->num_buckets++;
}

static void
init_cache_buckets(drm_intel_bufmgr_gem *bufmgr_gem)
{
	unsigned long size, cache_max_size = 64 * 1024 * 1024;

	/* OK, so power of two buckets was too wasteful of memory.
	 * Give 3 other sizes between each power of two, to hopefully
	 * cover things accurately enough.  (The alternative is
	 * probably to just go for exact matching of sizes, and assume
	 * that for things like composited window resize the tiled
	 * width/height alignment and rounding of sizes to pages will
	 * get us useful cache hit rates anyway)
	 */
	add_bucket(bufmgr_gem, 4096);
	add_bucket(bufmgr_gem, 4096 * 2);
	add_bucket(bufmgr_gem, 4096 * 3);

	/* Initialize the linked lists for BO reuse cache. */
	for (size = 4 * 4096; size <= cache_max_size; size *= 2) {
		add_bucket(bufmgr_gem, size);

		add_bucket(bufmgr_gem, size + size * 1 / 4);
		add_bucket(bufmgr_gem, size + size * 2 / 4);
		add_bucket(bufmgr_gem, size + size * 3 / 4);
	}
}

void
drm_intel_bufmgr_gem_set_vma_cache_size(drm_intel_bufmgr *bufmgr, int limit)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bufmgr;

	bufmgr_gem->vma_max = limit;

	drm_intel_gem_bo_purge_vma_cache(bufmgr_gem);
}

/**
 * Get the PCI ID for the device.  This can be overridden by setting the
 * INTEL_DEVID_OVERRIDE environment variable to the desired ID.
 */
static int
get_pci_device_id(drm_intel_bufmgr_gem *bufmgr_gem)
{
	char *devid_override;
	int devid = 0;
	int ret;
	drm_i915_getparam_t gp;

	if (geteuid() == getuid()) {
		devid_override = getenv("INTEL_DEVID_OVERRIDE");
		if (devid_override) {
			bufmgr_gem->no_exec = true;
			return strtod(devid_override, NULL);
		}
	}

	memclear(gp);
	gp.param = I915_PARAM_CHIPSET_ID;
	gp.value = &devid;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	if (ret) {
		fprintf(stderr, "get chip id failed: %d [%d]\n", ret, errno);
		fprintf(stderr, "param: %d, val: %d\n", gp.param, *gp.value);
	}
	return devid;
}

int
drm_intel_bufmgr_gem_get_devid(drm_intel_bufmgr *bufmgr)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bufmgr;

	return bufmgr_gem->pci_device;
}

/**
 * Sets the AUB filename.
 *
 * This function has to be called before drm_intel_bufmgr_gem_set_aub_dump()
 * for it to have any effect.
 */
void
drm_intel_bufmgr_gem_set_aub_filename(drm_intel_bufmgr *bufmgr,
				      const char *filename)
{
}

/**
 * Sets up AUB dumping.
 *
 * This is a trace file format that can be used with the simulator.
 * Packets are emitted in a format somewhat like GPU command packets.
 * You can set up a GTT and upload your objects into the referenced
 * space, then send off batchbuffers and get BMPs out the other end.
 */
void
drm_intel_bufmgr_gem_set_aub_dump(drm_intel_bufmgr *bufmgr, int enable)
{
	fprintf(stderr, "libdrm aub dumping is deprecated.\n\n"
		"Use intel_aubdump from intel-gpu-tools instead.  Install intel-gpu-tools,\n"
		"then run (for example)\n\n"
		"\t$ intel_aubdump --output=trace.aub glxgears -geometry 500x500\n\n"
		"See the intel_aubdump man page for more details.\n");
}

drm_intel_context *
drm_intel_gem_context_create(drm_intel_bufmgr *bufmgr)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bufmgr;
	struct drm_i915_gem_context_create create;
	drm_intel_context *context = NULL;
	int ret;

	context = calloc(1, sizeof(*context));
	if (!context)
		return NULL;

	memclear(create);
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_CONTEXT_CREATE, &create);
	if (ret != 0) {
		DBG("DRM_IOCTL_I915_GEM_CONTEXT_CREATE failed: %s\n",
		    strerror(errno));
		free(context);
		return NULL;
	}

	context->ctx_id = create.ctx_id;
	context->bufmgr = bufmgr;

	return context;
}

void
drm_intel_gem_context_destroy(drm_intel_context *ctx)
{
	drm_intel_bufmgr_gem *bufmgr_gem;
	struct drm_i915_gem_context_destroy destroy;
	int ret;

	if (ctx == NULL)
		return;

	memclear(destroy);

	bufmgr_gem = (drm_intel_bufmgr_gem *)ctx->bufmgr;
	destroy.ctx_id = ctx->ctx_id;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GEM_CONTEXT_DESTROY,
		       &destroy);
	if (ret != 0)
		fprintf(stderr, "DRM_IOCTL_I915_GEM_CONTEXT_DESTROY failed: %s\n",
			strerror(errno));

	free(ctx);
}

int
drm_intel_get_reset_stats(drm_intel_context *ctx,
			  uint32_t *reset_count,
			  uint32_t *active,
			  uint32_t *pending)
{
	drm_intel_bufmgr_gem *bufmgr_gem;
	struct drm_i915_reset_stats stats;
	int ret;

	if (ctx == NULL)
		return -EINVAL;

	memclear(stats);

	bufmgr_gem = (drm_intel_bufmgr_gem *)ctx->bufmgr;
	stats.ctx_id = ctx->ctx_id;
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GET_RESET_STATS,
		       &stats);
	if (ret == 0) {
		if (reset_count != NULL)
			*reset_count = stats.reset_count;

		if (active != NULL)
			*active = stats.batch_active;

		if (pending != NULL)
			*pending = stats.batch_pending;
	}

	return ret;
}

int
drm_intel_reg_read(drm_intel_bufmgr *bufmgr,
		   uint32_t offset,
		   uint64_t *result)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bufmgr;
	struct drm_i915_reg_read reg_read;
	int ret;

	memclear(reg_read);
	reg_read.offset = offset;

	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_REG_READ, &reg_read);

	*result = reg_read.val;
	return ret;
}

int
drm_intel_get_subslice_total(int fd, unsigned int *subslice_total)
{
	drm_i915_getparam_t gp;
	int ret;

	memclear(gp);
	gp.value = (int*)subslice_total;
	gp.param = I915_PARAM_SUBSLICE_TOTAL;
	ret = drmIoctl(fd, DRM_IOCTL_I915_GETPARAM, &gp);
	if (ret)
		return -errno;

	return 0;
}

int
drm_intel_get_eu_total(int fd, unsigned int *eu_total)
{
	drm_i915_getparam_t gp;
	int ret;

	memclear(gp);
	gp.value = (int*)eu_total;
	gp.param = I915_PARAM_EU_TOTAL;
	ret = drmIoctl(fd, DRM_IOCTL_I915_GETPARAM, &gp);
	if (ret)
		return -errno;

d1232 1
a1232 63
}

/**
 * Annotate the given bo for use in aub dumping.
 *
 * \param annotations is an array of drm_intel_aub_annotation objects
 * describing the type of data in various sections of the bo.  Each
 * element of the array specifies the type and subtype of a section of
 * the bo, and the past-the-end offset of that section.  The elements
 * of \c annotations must be sorted so that ending_offset is
 * increasing.
 *
 * \param count is the number of elements in the \c annotations array.
 * If \c count is zero, then \c annotations will not be dereferenced.
 *
 * Annotations are copied into a private data structure, so caller may
 * re-use the memory pointed to by \c annotations after the call
 * returns.
 *
 * Annotations are stored for the lifetime of the bo; to reset to the
 * default state (no annotations), call this function with a \c count
 * of zero.
 */
void
drm_intel_bufmgr_gem_set_aub_annotations(drm_intel_bo *bo,
					 drm_intel_aub_annotation *annotations,
					 unsigned count)
{
}

static pthread_mutex_t bufmgr_list_mutex = PTHREAD_MUTEX_INITIALIZER;
static drmMMListHead bufmgr_list = { &bufmgr_list, &bufmgr_list };

static drm_intel_bufmgr_gem *
drm_intel_bufmgr_gem_find(int fd)
{
	drm_intel_bufmgr_gem *bufmgr_gem;

	DRMLISTFOREACHENTRY(bufmgr_gem, &bufmgr_list, managers) {
		if (bufmgr_gem->fd == fd) {
			atomic_inc(&bufmgr_gem->refcount);
			return bufmgr_gem;
		}
	}

	return NULL;
}

static void
drm_intel_bufmgr_gem_unref(drm_intel_bufmgr *bufmgr)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *)bufmgr;

	if (atomic_add_unless(&bufmgr_gem->refcount, -1, 1)) {
		pthread_mutex_lock(&bufmgr_list_mutex);

		if (atomic_dec_and_test(&bufmgr_gem->refcount)) {
			DRMLISTDEL(&bufmgr_gem->managers);
			drm_intel_bufmgr_gem_destroy(bufmgr);
		}

		pthread_mutex_unlock(&bufmgr_list_mutex);
	}
d1244 54
a1297 24
	drm_intel_bufmgr_gem *bufmgr_gem;
	struct drm_i915_gem_get_aperture aperture;
	drm_i915_getparam_t gp;
	int ret, tmp;
	bool exec2 = false;

	pthread_mutex_lock(&bufmgr_list_mutex);

	bufmgr_gem = drm_intel_bufmgr_gem_find(fd);
	if (bufmgr_gem)
		goto exit;

	bufmgr_gem = calloc(1, sizeof(*bufmgr_gem));
	if (bufmgr_gem == NULL)
		goto exit;

	bufmgr_gem->fd = fd;
	atomic_set(&bufmgr_gem->refcount, 1);

	if (pthread_mutex_init(&bufmgr_gem->lock, NULL) != 0) {
		free(bufmgr_gem);
		bufmgr_gem = NULL;
		goto exit;
	}
d1299 2
a1300 52
	memclear(aperture);
	ret = drmIoctl(bufmgr_gem->fd,
		       DRM_IOCTL_I915_GEM_GET_APERTURE,
		       &aperture);

	if (ret == 0)
		bufmgr_gem->gtt_size = aperture.aper_available_size;
	else {
		fprintf(stderr, "DRM_IOCTL_I915_GEM_APERTURE failed: %s\n",
			strerror(errno));
		bufmgr_gem->gtt_size = 128 * 1024 * 1024;
		fprintf(stderr, "Assuming %dkB available aperture size.\n"
			"May lead to reduced performance or incorrect "
			"rendering.\n",
			(int)bufmgr_gem->gtt_size / 1024);
	}

	bufmgr_gem->pci_device = get_pci_device_id(bufmgr_gem);

	if (IS_GEN2(bufmgr_gem->pci_device))
		bufmgr_gem->gen = 2;
	else if (IS_GEN3(bufmgr_gem->pci_device))
		bufmgr_gem->gen = 3;
	else if (IS_GEN4(bufmgr_gem->pci_device))
		bufmgr_gem->gen = 4;
	else if (IS_GEN5(bufmgr_gem->pci_device))
		bufmgr_gem->gen = 5;
	else if (IS_GEN6(bufmgr_gem->pci_device))
		bufmgr_gem->gen = 6;
	else if (IS_GEN7(bufmgr_gem->pci_device))
		bufmgr_gem->gen = 7;
	else if (IS_GEN8(bufmgr_gem->pci_device))
		bufmgr_gem->gen = 8;
	else if (IS_GEN9(bufmgr_gem->pci_device))
		bufmgr_gem->gen = 9;
	else {
		free(bufmgr_gem);
		bufmgr_gem = NULL;
		goto exit;
	}

	if (IS_GEN3(bufmgr_gem->pci_device) &&
	    bufmgr_gem->gtt_size > 256*1024*1024) {
		/* The unmappable part of gtt on gen 3 (i.e. above 256MB) can't
		 * be used for tiled blits. To simplify the accounting, just
		 * substract the unmappable part (fixed to 256MB on all known
		 * gen3 devices) if the kernel advertises it. */
		bufmgr_gem->gtt_size -= 256*1024*1024;
	}

	memclear(gp);
	gp.value = &tmp;
a1301 121
	gp.param = I915_PARAM_HAS_EXECBUF2;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	if (!ret)
		exec2 = true;

	gp.param = I915_PARAM_HAS_BSD;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	bufmgr_gem->has_bsd = ret == 0;

	gp.param = I915_PARAM_HAS_BLT;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	bufmgr_gem->has_blt = ret == 0;

	gp.param = I915_PARAM_HAS_RELAXED_FENCING;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	bufmgr_gem->has_relaxed_fencing = ret == 0;

	bufmgr_gem->bufmgr.bo_alloc_userptr = check_bo_alloc_userptr;

	gp.param = I915_PARAM_HAS_WAIT_TIMEOUT;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	bufmgr_gem->has_wait_timeout = ret == 0;

	gp.param = I915_PARAM_HAS_LLC;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	if (ret != 0) {
		/* Kernel does not supports HAS_LLC query, fallback to GPU
		 * generation detection and assume that we have LLC on GEN6/7
		 */
		bufmgr_gem->has_llc = (IS_GEN6(bufmgr_gem->pci_device) |
				IS_GEN7(bufmgr_gem->pci_device));
	} else
		bufmgr_gem->has_llc = *gp.value;

	gp.param = I915_PARAM_HAS_VEBOX;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	bufmgr_gem->has_vebox = (ret == 0) & (*gp.value > 0);

	if (bufmgr_gem->gen < 4) {
		gp.param = I915_PARAM_NUM_FENCES_AVAIL;
		gp.value = &bufmgr_gem->available_fences;
		ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
		if (ret) {
			fprintf(stderr, "get fences failed: %d [%d]\n", ret,
				errno);
			fprintf(stderr, "param: %d, val: %d\n", gp.param,
				*gp.value);
			bufmgr_gem->available_fences = 0;
		} else {
			/* XXX The kernel reports the total number of fences,
			 * including any that may be pinned.
			 *
			 * We presume that there will be at least one pinned
			 * fence for the scanout buffer, but there may be more
			 * than one scanout and the user may be manually
			 * pinning buffers. Let's move to execbuffer2 and
			 * thereby forget the insanity of using fences...
			 */
			bufmgr_gem->available_fences -= 2;
			if (bufmgr_gem->available_fences < 0)
				bufmgr_gem->available_fences = 0;
		}
	}

	/* Let's go with one relocation per every 2 dwords (but round down a bit
	 * since a power of two will mean an extra page allocation for the reloc
	 * buffer).
	 *
	 * Every 4 was too few for the blender benchmark.
	 */
	bufmgr_gem->max_relocs = batch_size / sizeof(uint32_t) / 2 - 2;

	bufmgr_gem->bufmgr.bo_alloc = drm_intel_gem_bo_alloc;
	bufmgr_gem->bufmgr.bo_alloc_for_render =
	    drm_intel_gem_bo_alloc_for_render;
	bufmgr_gem->bufmgr.bo_alloc_tiled = drm_intel_gem_bo_alloc_tiled;
	bufmgr_gem->bufmgr.bo_reference = drm_intel_gem_bo_reference;
	bufmgr_gem->bufmgr.bo_unreference = drm_intel_gem_bo_unreference;
	bufmgr_gem->bufmgr.bo_map = drm_intel_gem_bo_map;
	bufmgr_gem->bufmgr.bo_unmap = drm_intel_gem_bo_unmap;
	bufmgr_gem->bufmgr.bo_subdata = drm_intel_gem_bo_subdata;
	bufmgr_gem->bufmgr.bo_get_subdata = drm_intel_gem_bo_get_subdata;
	bufmgr_gem->bufmgr.bo_wait_rendering = drm_intel_gem_bo_wait_rendering;
	bufmgr_gem->bufmgr.bo_emit_reloc = drm_intel_gem_bo_emit_reloc;
	bufmgr_gem->bufmgr.bo_emit_reloc_fence = drm_intel_gem_bo_emit_reloc_fence;
	bufmgr_gem->bufmgr.bo_pin = drm_intel_gem_bo_pin;
	bufmgr_gem->bufmgr.bo_unpin = drm_intel_gem_bo_unpin;
	bufmgr_gem->bufmgr.bo_get_tiling = drm_intel_gem_bo_get_tiling;
	bufmgr_gem->bufmgr.bo_set_tiling = drm_intel_gem_bo_set_tiling;
	bufmgr_gem->bufmgr.bo_flink = drm_intel_gem_bo_flink;
	/* Use the new one if available */
	if (exec2) {
		bufmgr_gem->bufmgr.bo_exec = drm_intel_gem_bo_exec2;
		bufmgr_gem->bufmgr.bo_mrb_exec = drm_intel_gem_bo_mrb_exec2;
	} else
		bufmgr_gem->bufmgr.bo_exec = drm_intel_gem_bo_exec;
	bufmgr_gem->bufmgr.bo_busy = drm_intel_gem_bo_busy;
	bufmgr_gem->bufmgr.bo_madvise = drm_intel_gem_bo_madvise;
	bufmgr_gem->bufmgr.destroy = drm_intel_bufmgr_gem_unref;
	bufmgr_gem->bufmgr.debug = 0;
	bufmgr_gem->bufmgr.check_aperture_space =
	    drm_intel_gem_check_aperture_space;
	bufmgr_gem->bufmgr.bo_disable_reuse = drm_intel_gem_bo_disable_reuse;
	bufmgr_gem->bufmgr.bo_is_reusable = drm_intel_gem_bo_is_reusable;
	bufmgr_gem->bufmgr.get_pipe_from_crtc_id =
	    drm_intel_gem_get_pipe_from_crtc_id;
	bufmgr_gem->bufmgr.bo_references = drm_intel_gem_bo_references;

	DRMINITLISTHEAD(&bufmgr_gem->named);
	init_cache_buckets(bufmgr_gem);

	DRMINITLISTHEAD(&bufmgr_gem->vma_cache);
	bufmgr_gem->vma_max = -1; /* unlimited by default */

	DRMLISTADD(&bufmgr_gem->managers, &bufmgr_list);

exit:
	pthread_mutex_unlock(&bufmgr_list_mutex);

	return bufmgr_gem != NULL ? &bufmgr_gem->bufmgr : NULL;
}
@


1.1.1.2
log
@Import libdrm 2.4.65
@
text
@a278 5
static inline drm_intel_bo_gem *to_bo_gem(drm_intel_bo *bo)
{
        return (drm_intel_bo_gem *)bo;
}

d2119 1
d2123 1
a2123 1
	if (to_bo_gem(bo)->has_error)
d2168 2
a2169 1
		drm_intel_bo_gem *bo_gem = to_bo_gem(bufmgr_gem->exec_bos[i]);
a2192 3
	if (to_bo_gem(bo)->has_error)
		return -ENOMEM;

d2263 2
a2264 1
		drm_intel_bo_gem *bo_gem = to_bo_gem(bufmgr_gem->exec_bos[i]);
a2430 1
	pthread_mutex_lock(&bufmgr_gem->lock);
a2431 5
	if (ret) {
		DBG("create_from_prime: failed to obtain handle from fd: %s\n", strerror(errno));
		pthread_mutex_unlock(&bufmgr_gem->lock);
		return NULL;
	}
d2438 1
d2450 6
a2495 1
		DBG("create_from_prime: failed to get tiling: %s\n", strerror(errno));
@


1.1.1.3
log
@Import libdrm 2.4.67
@
text
@a85 16
/**
 * upper_32_bits - return bits 32-63 of a number
 * @@n: the number we're accessing
 *
 * A basic shift-right of a 64- or 32-bit quantity.  Use this to suppress
 * the "right shift count >= width of type" warning when that quantity is
 * 32-bits.
 */
#define upper_32_bits(n) ((__u32)(((n) >> 16) >> 16))

/**
 * lower_32_bits - return bits 0-31 of a number
 * @@n: the number we're accessing
 */
#define lower_32_bits(n) ((__u32)(n))

a186 7
	/** Array of BOs that are referenced by this buffer and will be softpinned */
	drm_intel_bo **softpin_target;
	/** Number softpinned BOs that are referenced by this buffer */
	int softpin_target_count;
	/** Maximum amount of softpinned BOs that are referenced by this buffer */
	int softpin_target_size;

a239 14
	 * Boolean of whether this buffer can be placed in the full 48-bit
	 * address range on gen8+.
	 *
	 * By default, buffers will be keep in a 32-bit range, unless this
	 * flag is explicitly set.
	 */
	bool use_48b_address_range;

	/**
	 * Whether this buffer is softpinned at offset specified by the user
	 */
	bool is_softpin;

	/**
d392 2
a393 3
		if (bo_gem->relocs == NULL && bo_gem->softpin_target == NULL) {
			DBG("%2d: %d %s(%s)\n", i, bo_gem->gem_handle,
			    bo_gem->is_softpin ? "*" : "",
d403 2
a404 2
			DBG("%2d: %d %s(%s)@@0x%08x %08x -> "
			    "%d (%s)@@0x%08x %08x + 0x%08x\n",
d406 2
a407 5
			    bo_gem->gem_handle,
			    bo_gem->is_softpin ? "*" : "",
			    bo_gem->name,
			    upper_32_bits(bo_gem->relocs[j].offset),
			    lower_32_bits(bo_gem->relocs[j].offset),
d410 1
a410 2
			    upper_32_bits(target_bo->offset64),
			    lower_32_bits(target_bo->offset64),
a412 16

		for (j = 0; j < bo_gem->softpin_target_count; j++) {
			drm_intel_bo *target_bo = bo_gem->softpin_target[j];
			drm_intel_bo_gem *target_gem =
			    (drm_intel_bo_gem *) target_bo;
			DBG("%2d: %d %s(%s) -> "
			    "%d *(%s)@@0x%08x %08x\n",
			    i,
			    bo_gem->gem_handle,
			    bo_gem->is_softpin ? "*" : "",
			    bo_gem->name,
			    target_gem->gem_handle,
			    target_gem->name,
			    upper_32_bits(target_bo->offset64),
			    lower_32_bits(target_bo->offset64));
		}
a475 8
	int flags = 0;

	if (need_fence)
		flags |= EXEC_OBJECT_NEEDS_FENCE;
	if (bo_gem->use_48b_address_range)
		flags |= EXEC_OBJECT_SUPPORTS_48B_ADDRESS;
	if (bo_gem->is_softpin)
		flags |= EXEC_OBJECT_PINNED;
d478 3
a480 1
		bufmgr_gem->exec2_objects[bo_gem->validate_index].flags |= flags;
d507 1
a507 2
	bufmgr_gem->exec2_objects[index].offset = bo_gem->is_softpin ?
		bo->offset64 : 0;
d509 1
a509 1
	bufmgr_gem->exec2_objects[index].flags = flags;
d512 4
a787 1
	bo_gem->use_48b_address_range = false;
a933 1
	bo_gem->use_48b_address_range = false;
a1088 1
	bo_gem->use_48b_address_range = false;
a1263 3
	for (i = 0; i < bo_gem->softpin_target_count; i++)
		drm_intel_gem_bo_unreference_locked_timed(bo_gem->softpin_target[i],
								  time);
a1265 1
	bo_gem->softpin_target_count = 0;
a1278 5
	if (bo_gem->softpin_target) {
		free(bo_gem->softpin_target);
		bo_gem->softpin_target = NULL;
		bo_gem->softpin_target_size = 0;
	}
d1916 8
a1932 7
	bo_gem->relocs[bo_gem->reloc_count].offset = offset;
	bo_gem->relocs[bo_gem->reloc_count].delta = target_offset;
	bo_gem->relocs[bo_gem->reloc_count].target_handle =
	    target_bo_gem->gem_handle;
	bo_gem->relocs[bo_gem->reloc_count].read_domains = read_domains;
	bo_gem->relocs[bo_gem->reloc_count].write_domain = write_domain;
	bo_gem->relocs[bo_gem->reloc_count].presumed_offset = target_bo->offset64;
a1937 45
static void
drm_intel_gem_bo_use_48b_address_range(drm_intel_bo *bo, uint32_t enable)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	bo_gem->use_48b_address_range = enable;
}

static int
drm_intel_gem_bo_add_softpin_target(drm_intel_bo *bo, drm_intel_bo *target_bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;
	drm_intel_bo_gem *target_bo_gem = (drm_intel_bo_gem *) target_bo;
	if (bo_gem->has_error)
		return -ENOMEM;

	if (target_bo_gem->has_error) {
		bo_gem->has_error = true;
		return -ENOMEM;
	}

	if (!target_bo_gem->is_softpin)
		return -EINVAL;
	if (target_bo_gem == bo_gem)
		return -EINVAL;

	if (bo_gem->softpin_target_count == bo_gem->softpin_target_size) {
		int new_size = bo_gem->softpin_target_size * 2;
		if (new_size == 0)
			new_size = bufmgr_gem->max_relocs;

		bo_gem->softpin_target = realloc(bo_gem->softpin_target, new_size *
				sizeof(drm_intel_bo *));
		if (!bo_gem->softpin_target)
			return -ENOMEM;

		bo_gem->softpin_target_size = new_size;
	}
	bo_gem->softpin_target[bo_gem->softpin_target_count] = target_bo;
	drm_intel_gem_bo_reference(target_bo);
	bo_gem->softpin_target_count++;

	return 0;
}

a1943 1
	drm_intel_bo_gem *target_bo_gem = (drm_intel_bo_gem *)target_bo;
d1945 3
a1947 6
	if (target_bo_gem->is_softpin)
		return drm_intel_gem_bo_add_softpin_target(bo, target_bo);
	else
		return do_bo_emit_reloc(bo, offset, target_bo, target_offset,
					read_domains, write_domain,
					!bufmgr_gem->fenced_relocs);
a1979 2
 *
 * This also removes all softpinned targets being referenced by the BO.
a2005 6
	for (i = 0; i < bo_gem->softpin_target_count; i++) {
		drm_intel_bo_gem *target_bo_gem = (drm_intel_bo_gem *) bo_gem->softpin_target[i];
		drm_intel_gem_bo_unreference_locked_timed(&target_bo_gem->bo, time.tv_sec);
	}
	bo_gem->softpin_target_count = 0;

d2046 1
a2046 1
	if (bo_gem->relocs == NULL && bo_gem->softpin_target == NULL)
a2066 11

	for (i = 0; i < bo_gem->softpin_target_count; i++) {
		drm_intel_bo *target_bo = bo_gem->softpin_target[i];

		if (target_bo == bo)
			continue;

		drm_intel_gem_bo_mark_mmaps_incoherent(bo);
		drm_intel_gem_bo_process_reloc2(target_bo);
		drm_intel_add_validate_buffer2(target_bo, false);
	}
d2081 4
a2084 6
			DBG("BO %d (%s) migrated: 0x%08x %08x -> 0x%08x %08x\n",
			    bo_gem->gem_handle, bo_gem->name,
			    upper_32_bits(bo->offset64),
			    lower_32_bits(bo->offset64),
			    upper_32_bits(bufmgr_gem->exec_objects[i].offset),
			    lower_32_bits(bufmgr_gem->exec_objects[i].offset));
d2102 3
a2104 10
			/* If we're seeing softpinned object here it means that the kernel
			 * has relocated our object... Indicating a programming error
			 */
			assert(!bo_gem->is_softpin);
			DBG("BO %d (%s) migrated: 0x%08x %08x -> 0x%08x %08x\n",
			    bo_gem->gem_handle, bo_gem->name,
			    upper_32_bits(bo->offset64),
			    lower_32_bits(bo->offset64),
			    upper_32_bits(bufmgr_gem->exec2_objects[i].offset),
			    lower_32_bits(bufmgr_gem->exec2_objects[i].offset));
a2425 11
static int
drm_intel_gem_bo_set_softpin_offset(drm_intel_bo *bo, uint64_t offset)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	bo_gem->is_softpin = true;
	bo->offset64 = offset;
	bo->offset = offset;
	return 0;
}

a2488 1
	bo_gem->use_48b_address_range = false;
a2803 7
	for (i = 0; i< bo_gem->softpin_target_count; i++) {
		if (bo_gem->softpin_target[i] == target_bo)
			return 1;
		if (_drm_intel_gem_bo_references(bo_gem->softpin_target[i], target_bo))
			return 1;
	}

a3259 5
	gp.param = I915_PARAM_HAS_EXEC_SOFTPIN;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	if (ret == 0 && *gp.value > 0)
		bufmgr_gem->bufmgr.bo_set_softpin_offset = drm_intel_gem_bo_set_softpin_offset;

a3283 7
	}

	if (bufmgr_gem->gen >= 8) {
		gp.param = I915_PARAM_HAS_ALIASING_PPGTT;
		ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
		if (ret == 0 && *gp.value == 3)
			bufmgr_gem->bufmgr.bo_use_48b_address_range = drm_intel_gem_bo_use_48b_address_range;
@


1.1.1.4
log
@Import libdrm 2.4.71
@
text
@a3239 30
int
drm_intel_get_pooled_eu(int fd)
{
	drm_i915_getparam_t gp;
	int ret = -1;

	memclear(gp);
	gp.param = I915_PARAM_HAS_POOLED_EU;
	gp.value = &ret;
	if (drmIoctl(fd, DRM_IOCTL_I915_GETPARAM, &gp))
		return -errno;

	return ret;
}

int
drm_intel_get_min_eu_in_pool(int fd)
{
	drm_i915_getparam_t gp;
	int ret = -1;

	memclear(gp);
	gp.param = I915_PARAM_MIN_EU_IN_POOL;
	gp.value = &ret;
	if (drmIoctl(fd, DRM_IOCTL_I915_GETPARAM, &gp))
		return -errno;

	return ret;
}

d3382 1
a3382 1
		 * subtract the unmappable part (fixed to 256MB on all known
@


1.1.1.5
log
@Import libdrm 2.4.73
@
text
@a66 1
#include "uthash.h"
d133 1
a133 3
	drm_intel_bo_gem *name_table;
	drm_intel_bo_gem *handle_table;

d178 1
a178 3

	UT_hash_handle handle_hh;
	UT_hash_handle name_hh;
a213 2
	/** WC CPU address for the buffer, saved across map/unmap cycles */
	void *wc_virtual;
d800 1
d807 1
a807 5
			goto err;

		/* drm_intel_gem_bo_free calls DRMLISTDEL() for an uninitialized
		   list (vma_list), so better set the list head here */
		DRMINITLISTHEAD(&bo_gem->vma_list);
d817 2
d821 1
a821 1
			goto err;
a822 3

		bo_gem->gem_handle = create.handle;
		bo_gem->bo.handle = bo_gem->gem_handle;
d830 4
d836 4
a839 6
							 stride))
			goto err_free;

		HASH_ADD(handle_hh, bufmgr_gem->handle_table,
			 gem_handle, sizeof(bo_gem->gem_handle),
			 bo_gem);
a851 1
	pthread_mutex_unlock(&bufmgr_gem->lock);
a856 6

err_free:
	drm_intel_gem_bo_free(&bo_gem->bo);
err:
	pthread_mutex_unlock(&bufmgr_gem->lock);
	return NULL;
a956 3
	atomic_set(&bo_gem->refcount, 1);
	DRMINITLISTHEAD(&bo_gem->vma_list);

a974 2
	pthread_mutex_lock(&bufmgr_gem->lock);

d986 2
a987 3
	HASH_ADD(handle_hh, bufmgr_gem->handle_table,
		 gem_handle, sizeof(bo_gem->gem_handle),
		 bo_gem);
d990 1
a998 1
	pthread_mutex_unlock(&bufmgr_gem->lock);
d1088 1
d1097 9
a1105 5
	HASH_FIND(name_hh, bufmgr_gem->name_table,
		  &handle, sizeof(handle), bo_gem);
	if (bo_gem) {
		drm_intel_gem_bo_reference(&bo_gem->bo);
		goto out;
d1116 2
a1117 2
		bo_gem = NULL;
		goto out;
d1123 9
a1131 5
	HASH_FIND(handle_hh, bufmgr_gem->handle_table,
		  &open_arg.handle, sizeof(open_arg.handle), bo_gem);
	if (bo_gem) {
		drm_intel_gem_bo_reference(&bo_gem->bo);
		goto out;
d1135 4
a1138 5
	if (!bo_gem)
		goto out;

	atomic_set(&bo_gem->refcount, 1);
	DRMINITLISTHEAD(&bo_gem->vma_list);
d1146 1
a1153 5
	HASH_ADD(handle_hh, bufmgr_gem->handle_table,
		 gem_handle, sizeof(bo_gem->gem_handle), bo_gem);
	HASH_ADD(name_hh, bufmgr_gem->name_table,
		 global_name, sizeof(bo_gem->global_name), bo_gem);

d1159 5
a1163 3
	if (ret != 0)
		goto err_unref;

d1168 4
a1173 2
out:
	pthread_mutex_unlock(&bufmgr_gem->lock);
a1174 5

err_unref:
	drm_intel_gem_bo_free(&bo_gem->bo);
	pthread_mutex_unlock(&bufmgr_gem->lock);
	return NULL;
a1190 5
	if (bo_gem->wc_virtual) {
		VG(VALGRIND_FREELIKE_BLOCK(bo_gem->wc_virtual, 0));
		drm_munmap(bo_gem->wc_virtual, bo_gem->bo.size);
		bufmgr_gem->vma_count--;
	}
a1195 4
	if (bo_gem->global_name)
		HASH_DELETE(name_hh, bufmgr_gem->name_table, bo_gem);
	HASH_DELETE(handle_hh, bufmgr_gem->handle_table, bo_gem);

a1215 3
	if (bo_gem->wc_virtual)
		VALGRIND_MAKE_MEM_NOACCESS(bo_gem->wc_virtual, bo->size);

a1279 5
		if (bo_gem->wc_virtual) {
			drm_munmap(bo_gem->wc_virtual, bo_gem->bo.size);
			bo_gem->wc_virtual = NULL;
			bufmgr_gem->vma_count--;
		}
a1294 2
	if (bo_gem->wc_virtual)
		bufmgr_gem->vma_count++;
a1306 2
	if (bo_gem->wc_virtual)
		bufmgr_gem->vma_count--;
d1361 2
d2594 1
d2609 9
a2617 5
	HASH_FIND(handle_hh, bufmgr_gem->handle_table,
		  &handle, sizeof(handle), bo_gem);
	if (bo_gem) {
		drm_intel_gem_bo_reference(&bo_gem->bo);
		goto out;
d2621 4
a2624 6
	if (!bo_gem)
		goto out;

	atomic_set(&bo_gem->refcount, 1);
	DRMINITLISTHEAD(&bo_gem->vma_list);

d2640 2
a2641 2
	HASH_ADD(handle_hh, bufmgr_gem->handle_table,
		 gem_handle, sizeof(bo_gem->gem_handle), bo_gem);
d2651 4
d2657 8
a2664 5
	if (drmIoctl(bufmgr_gem->fd,
		     DRM_IOCTL_I915_GEM_GET_TILING,
		     &get_tiling))
		goto err;

a2669 2
out:
	pthread_mutex_unlock(&bufmgr_gem->lock);
a2670 5

err:
	drm_intel_gem_bo_free(&bo_gem->bo);
	pthread_mutex_unlock(&bufmgr_gem->lock);
	return NULL;
d2679 5
d2698 1
a2704 2
		if (drmIoctl(bufmgr_gem->fd, DRM_IOCTL_GEM_FLINK, &flink))
			return -errno;
d2707 5
a2711 6
		if (!bo_gem->global_name) {
			HASH_ADD(name_hh, bufmgr_gem->name_table,
				 global_name, sizeof(bo_gem->global_name),
				 bo_gem);
			bo_gem->global_name = flink.name;
			bo_gem->reusable = false;
d2713 6
a3036 28
static int
parse_devid_override(const char *devid_override)
{
	static const struct {
		const char *name;
		int pci_id;
	} name_map[] = {
		{ "brw", PCI_CHIP_I965_GM },
		{ "g4x", PCI_CHIP_GM45_GM },
		{ "ilk", PCI_CHIP_ILD_G },
		{ "snb", PCI_CHIP_SANDYBRIDGE_M_GT2_PLUS },
		{ "ivb", PCI_CHIP_IVYBRIDGE_S_GT2 },
		{ "hsw", PCI_CHIP_HASWELL_CRW_E_GT3 },
		{ "byt", PCI_CHIP_VALLEYVIEW_3 },
		{ "bdw", 0x1620 | BDW_ULX },
		{ "skl", PCI_CHIP_SKYLAKE_DT_GT2 },
		{ "kbl", PCI_CHIP_KABYLAKE_DT_GT2 },
	};
	unsigned int i;

	for (i = 0; i < ARRAY_SIZE(name_map); i++) {
		if (!strcmp(name_map[i].name, devid_override))
			return name_map[i].pci_id;
	}

	return strtod(devid_override, NULL);
}

d3053 1
a3053 1
			return parse_devid_override(devid_override);
a3332 135
void *drm_intel_gem_bo_map__gtt(drm_intel_bo *bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	if (bo_gem->gtt_virtual)
		return bo_gem->gtt_virtual;

	if (bo_gem->is_userptr)
		return NULL;

	pthread_mutex_lock(&bufmgr_gem->lock);
	if (bo_gem->gtt_virtual == NULL) {
		struct drm_i915_gem_mmap_gtt mmap_arg;
		void *ptr;

		DBG("bo_map_gtt: mmap %d (%s), map_count=%d\n",
		    bo_gem->gem_handle, bo_gem->name, bo_gem->map_count);

		if (bo_gem->map_count++ == 0)
			drm_intel_gem_bo_open_vma(bufmgr_gem, bo_gem);

		memclear(mmap_arg);
		mmap_arg.handle = bo_gem->gem_handle;

		/* Get the fake offset back... */
		ptr = MAP_FAILED;
		if (drmIoctl(bufmgr_gem->fd,
			     DRM_IOCTL_I915_GEM_MMAP_GTT,
			     &mmap_arg) == 0) {
			/* and mmap it */
			ptr = drm_mmap(0, bo->size, PROT_READ | PROT_WRITE,
				       MAP_SHARED, bufmgr_gem->fd,
				       mmap_arg.offset);
		}
		if (ptr == MAP_FAILED) {
			if (--bo_gem->map_count == 0)
				drm_intel_gem_bo_close_vma(bufmgr_gem, bo_gem);
			ptr = NULL;
		}

		bo_gem->gtt_virtual = ptr;
	}
	pthread_mutex_unlock(&bufmgr_gem->lock);

	return bo_gem->gtt_virtual;
}

void *drm_intel_gem_bo_map__cpu(drm_intel_bo *bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	if (bo_gem->mem_virtual)
		return bo_gem->mem_virtual;

	if (bo_gem->is_userptr) {
		/* Return the same user ptr */
		return bo_gem->user_virtual;
	}

	pthread_mutex_lock(&bufmgr_gem->lock);
	if (!bo_gem->mem_virtual) {
		struct drm_i915_gem_mmap mmap_arg;

		if (bo_gem->map_count++ == 0)
			drm_intel_gem_bo_open_vma(bufmgr_gem, bo_gem);

		DBG("bo_map: %d (%s), map_count=%d\n",
		    bo_gem->gem_handle, bo_gem->name, bo_gem->map_count);

		memclear(mmap_arg);
		mmap_arg.handle = bo_gem->gem_handle;
		mmap_arg.size = bo->size;
		if (drmIoctl(bufmgr_gem->fd,
			     DRM_IOCTL_I915_GEM_MMAP,
			     &mmap_arg)) {
			DBG("%s:%d: Error mapping buffer %d (%s): %s .\n",
			    __FILE__, __LINE__, bo_gem->gem_handle,
			    bo_gem->name, strerror(errno));
			if (--bo_gem->map_count == 0)
				drm_intel_gem_bo_close_vma(bufmgr_gem, bo_gem);
		} else {
			VG(VALGRIND_MALLOCLIKE_BLOCK(mmap_arg.addr_ptr, mmap_arg.size, 0, 1));
			bo_gem->mem_virtual = (void *)(uintptr_t) mmap_arg.addr_ptr;
		}
	}
	pthread_mutex_unlock(&bufmgr_gem->lock);

	return bo_gem->mem_virtual;
}

void *drm_intel_gem_bo_map__wc(drm_intel_bo *bo)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bo->bufmgr;
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	if (bo_gem->wc_virtual)
		return bo_gem->wc_virtual;

	if (bo_gem->is_userptr)
		return NULL;

	pthread_mutex_lock(&bufmgr_gem->lock);
	if (!bo_gem->wc_virtual) {
		struct drm_i915_gem_mmap mmap_arg;

		if (bo_gem->map_count++ == 0)
			drm_intel_gem_bo_open_vma(bufmgr_gem, bo_gem);

		DBG("bo_map: %d (%s), map_count=%d\n",
		    bo_gem->gem_handle, bo_gem->name, bo_gem->map_count);

		memclear(mmap_arg);
		mmap_arg.handle = bo_gem->gem_handle;
		mmap_arg.size = bo->size;
		mmap_arg.flags = I915_MMAP_WC;
		if (drmIoctl(bufmgr_gem->fd,
			     DRM_IOCTL_I915_GEM_MMAP,
			     &mmap_arg)) {
			DBG("%s:%d: Error mapping buffer %d (%s): %s .\n",
			    __FILE__, __LINE__, bo_gem->gem_handle,
			    bo_gem->name, strerror(errno));
			if (--bo_gem->map_count == 0)
				drm_intel_gem_bo_close_vma(bufmgr_gem, bo_gem);
		} else {
			VG(VALGRIND_MALLOCLIKE_BLOCK(mmap_arg.addr_ptr, mmap_arg.size, 0, 1));
			bo_gem->wc_virtual = (void *)(uintptr_t) mmap_arg.addr_ptr;
		}
	}
	pthread_mutex_unlock(&bufmgr_gem->lock);

	return bo_gem->wc_virtual;
}

d3540 1
@


1.1.1.6
log
@Import libdrm 2.4.75
@
text
@a151 1
	unsigned int has_exec_async : 1;
a197 2
	unsigned long kflags;

d259 1
a259 1
	 * buffers are those that have been shared with other
d297 1
a297 1
	/** Flags that we may need to do the SW_FINISH ioctl on unmap. */
d578 4
a581 2
	bufmgr_gem->exec2_objects[index].offset = bo->offset64;
	bufmgr_gem->exec2_objects[index].flags = flags | bo_gem->kflags;
a583 1
	bufmgr_gem->exec_bos[index] = bo;
a1370 1
	bo_gem->kflags = 0;
a1410 2
		bo_gem->kflags = 0;

d1722 1
a1722 1
	 * an open vma for every bo as that will exhaust the system
a2375 1
	 int in_fence, int *out_fence,
a2429 8
	if (in_fence != -1) {
		execbuf.rsvd2 = in_fence;
		execbuf.flags |= I915_EXEC_FENCE_IN;
	}
	if (out_fence != NULL) {
		*out_fence = -1;
		execbuf.flags |= I915_EXEC_FENCE_OUT;
	}
d2435 1
a2435 1
		       DRM_IOCTL_I915_GEM_EXECBUFFER2_WR,
a2450 3
	if (ret == 0 && out_fence != NULL)
		*out_fence = execbuf.rsvd2 >> 32;

d2476 1
a2476 1
			-1, NULL, I915_EXEC_RENDER);
d2485 1
a2485 1
			-1, NULL, flags);
d2492 1
a2492 12
	return do_exec2(bo, used, ctx, NULL, 0, 0, -1, NULL, flags);
}

int
drm_intel_gem_bo_fence_exec(drm_intel_bo *bo,
			    drm_intel_context *ctx,
			    int used,
			    int in_fence,
			    int *out_fence,
			    unsigned int flags)
{
	return do_exec2(bo, used, ctx, NULL, 0, 0, in_fence, out_fence, flags);
a2738 3
			bo_gem->global_name = flink.name;
			bo_gem->reusable = false;

d2742 2
a2767 53
 * Disables implicit synchronisation before executing the bo
 *
 * This will cause rendering corruption unless you correctly manage explicit
 * fences for all rendering involving this buffer - including use by others.
 * Disabling the implicit serialisation is only required if that serialisation
 * is too coarse (for example, you have split the buffer into many
 * non-overlapping regions and are sharing the whole buffer between concurrent
 * independent command streams).
 *
 * Note the kernel must advertise support via I915_PARAM_HAS_EXEC_ASYNC,
 * which can be checked using drm_intel_bufmgr_can_disable_implicit_sync,
 * or subsequent execbufs involving the bo will generate EINVAL.
 */
void
drm_intel_gem_bo_disable_implicit_sync(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	bo_gem->kflags |= EXEC_OBJECT_ASYNC;
}

/**
 * Enables implicit synchronisation before executing the bo
 *
 * This is the default behaviour of the kernel, to wait upon prior writes
 * completing on the object before rendering with it, or to wait for prior
 * reads to complete before writing into the object.
 * drm_intel_gem_bo_disable_implicit_sync() can stop this behaviour, telling
 * the kernel never to insert a stall before using the object. Then this
 * function can be used to restore the implicit sync before subsequent
 * rendering.
 */
void
drm_intel_gem_bo_enable_implicit_sync(drm_intel_bo *bo)
{
	drm_intel_bo_gem *bo_gem = (drm_intel_bo_gem *) bo;

	bo_gem->kflags &= ~EXEC_OBJECT_ASYNC;
}

/**
 * Query whether the kernel supports disabling of its implicit synchronisation
 * before execbuf. See drm_intel_gem_bo_disable_implicit_sync()
 */
int
drm_intel_bufmgr_gem_can_disable_implicit_sync(drm_intel_bufmgr *bufmgr)
{
	drm_intel_bufmgr_gem *bufmgr_gem = (drm_intel_bufmgr_gem *) bufmgr;

	return bufmgr_gem->has_exec_async;
}

/**
a3186 11
int
drm_intel_gem_context_get_id(drm_intel_context *ctx, uint32_t *ctx_id)
{
	if (ctx == NULL)
		return -EINVAL;

	*ctx_id = ctx->ctx_id;

	return 0;
}

a3624 4

	gp.param = I915_PARAM_HAS_EXEC_ASYNC;
	ret = drmIoctl(bufmgr_gem->fd, DRM_IOCTL_I915_GETPARAM, &gp);
	bufmgr_gem->has_exec_async = ret == 0;
@


1.1.1.7
log
@Import libdrm 2.4.79
@
text
@d273 14
d441 1
a441 1
			    bo_gem->kflags & EXEC_OBJECT_PINNED ? "*" : "",
d455 1
a455 1
			    bo_gem->kflags & EXEC_OBJECT_PINNED ? "*" : "",
d474 1
a474 1
			    bo_gem->kflags & EXEC_OBJECT_PINNED ? "*" : "",
d544 1
a544 1
	unsigned long flags;
a545 1
	flags = 0;
d548 4
d582 1
a582 1
	bufmgr_gem->exec2_objects[index].flags = bo_gem->kflags | flags;
a834 4
		HASH_ADD(handle_hh, bufmgr_gem->handle_table,
			 gem_handle, sizeof(bo_gem->gem_handle),
			 bo_gem);

d847 4
d860 1
d1019 1
d1167 1
d1414 2
d2057 1
a2057 5

	if (enable)
		bo_gem->kflags |= EXEC_OBJECT_SUPPORTS_48B_ADDRESS;
	else
		bo_gem->kflags &= ~EXEC_OBJECT_SUPPORTS_48B_ADDRESS;
d2074 1
a2074 1
	if (!(target_bo_gem->kflags & EXEC_OBJECT_PINNED))
d2106 1
a2106 1
	if (target_bo_gem->kflags & EXEC_OBJECT_PINNED)
d2290 1
a2290 1
			assert(!(bo_gem->kflags & EXEC_OBJECT_PINNED));
d2646 1
a2648 2
	bo_gem->kflags |= EXEC_OBJECT_PINNED;

d2712 1
@


1.1.1.8
log
@Import libdrm 2.4.82
@
text
@d662 1
a3665 2
	else if (IS_GEN10(bufmgr_gem->pci_device))
		bufmgr_gem->gen = 10;
@


