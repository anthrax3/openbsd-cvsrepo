head	1.8;
access;
symbols
	mesa-13_0_6:1.1.1.3
	mesa-13_0_5:1.1.1.3
	mesa-13_0_3:1.1.1.3
	mesa-13_0_2:1.1.1.3
	OPENBSD_6_0:1.6.0.4
	OPENBSD_6_0_BASE:1.6
	mesa-11_2_2:1.1.1.2
	OPENBSD_5_9:1.3.0.2
	OPENBSD_5_9_BASE:1.3
	mesa-11_0_9:1.1.1.1
	mesa-11_0_8:1.1.1.1
	mesa-11_0_6:1.1.1.1
	mesa:1.1.1;
locks; strict;
comment	@ * @;


1.8
date	2016.12.31.08.21.39;	author jsg;	state Exp;
branches;
next	1.7;
commitid	7noeaoVFCJ4eYbrA;

1.7
date	2016.12.11.08.53.22;	author jsg;	state Exp;
branches;
next	1.6;
commitid	96stMNxtiPBiO554;

1.6
date	2016.05.29.10.40.28;	author jsg;	state Exp;
branches;
next	1.5;
commitid	lFAjX4nNEOTq8hSb;

1.5
date	2016.03.18.14.38.03;	author jsg;	state Exp;
branches;
next	1.4;
commitid	bVWEdA78yMk87vGU;

1.4
date	2016.02.28.04.44.02;	author jsg;	state Exp;
branches;
next	1.3;
commitid	DoOIemGZaSIfqzi4;

1.3
date	2016.02.20.10.48.24;	author matthieu;	state Exp;
branches;
next	1.2;
commitid	lOJiiV0RQDIQTjnu;

1.2
date	2015.11.22.03.08.22;	author jsg;	state Exp;
branches;
next	1.1;
commitid	bGPTQRh6usueKbev;

1.1
date	2015.11.22.02.41.34;	author jsg;	state Exp;
branches
	1.1.1.1;
next	;
commitid	bJUptkbooQfJPk5r;

1.1.1.1
date	2015.11.22.02.41.34;	author jsg;	state Exp;
branches;
next	1.1.1.2;
commitid	bJUptkbooQfJPk5r;

1.1.1.2
date	2016.05.29.10.16.15;	author jsg;	state Exp;
branches;
next	1.1.1.3;
commitid	OwGfrJACrYJkCVJ4;

1.1.1.3
date	2016.12.11.08.34.20;	author jsg;	state Exp;
branches;
next	;
commitid	uuv5VTS15jglEDZU;


desc
@@


1.8
log
@Use the unlocked atomic path in Mesa when just __arm__ is defined and
don't assume the builtin atomics are present with __ARM_ARCH_6__ and later
as base gcc with -march=armv6 doesn't have them.
@
text
@/**
 * Many similar implementations exist. See for example libwsbm
 * or the linux kernel include/atomic.h
 *
 * No copyright claimed on this file.
 *
 */

#include "no_extern_c.h"

#ifndef U_ATOMIC_H
#define U_ATOMIC_H

#include <stdbool.h>

/* Favor OS-provided implementations.
 *
 * Where no OS-provided implementation is available, fall back to
 * locally coded assembly, compiler intrinsic or ultimately a
 * mutex-based implementation.
 */
#if defined(__sun)
#define PIPE_ATOMIC_OS_SOLARIS
#elif defined(_MSC_VER)
#define PIPE_ATOMIC_MSVC_INTRINSIC
#elif defined(__arm__) || defined(__hppa__) || defined(__sh__)
#define PIPE_ATOMIC_UNLOCKED
#elif defined(__GNUC__)
#define PIPE_ATOMIC_GCC_INTRINSIC
#else
#error "Unsupported platform"
#endif


/* Implementation using GCC-provided synchronization intrinsics
 */
#if defined(PIPE_ATOMIC_GCC_INTRINSIC)

#define PIPE_ATOMIC "GCC Sync Intrinsics"

#if defined(USE_GCC_ATOMIC_BUILTINS)

/* The builtins with explicit memory model are available since GCC 4.7. */
#define p_atomic_set(_v, _i) __atomic_store_n((_v), (_i), __ATOMIC_RELEASE)
#define p_atomic_read(_v) __atomic_load_n((_v), __ATOMIC_ACQUIRE)
#define p_atomic_dec_zero(v) (__atomic_sub_fetch((v), 1, __ATOMIC_ACQ_REL) == 0)
#define p_atomic_inc(v) (void) __atomic_add_fetch((v), 1, __ATOMIC_ACQ_REL)
#define p_atomic_dec(v) (void) __atomic_sub_fetch((v), 1, __ATOMIC_ACQ_REL)
#define p_atomic_add(v, i) (void) __atomic_add_fetch((v), (i), __ATOMIC_ACQ_REL)
#define p_atomic_inc_return(v) __atomic_add_fetch((v), 1, __ATOMIC_ACQ_REL)
#define p_atomic_dec_return(v) __atomic_sub_fetch((v), 1, __ATOMIC_ACQ_REL)

#else

#define p_atomic_set(_v, _i) (*(_v) = (_i))
#define p_atomic_read(_v) (*(_v))
#define p_atomic_dec_zero(v) (__sync_sub_and_fetch((v), 1) == 0)
#define p_atomic_inc(v) (void) __sync_add_and_fetch((v), 1)
#define p_atomic_dec(v) (void) __sync_sub_and_fetch((v), 1)
#define p_atomic_add(v, i) (void) __sync_add_and_fetch((v), (i))
#define p_atomic_inc_return(v) __sync_add_and_fetch((v), 1)
#define p_atomic_dec_return(v) __sync_sub_and_fetch((v), 1)

#endif

/* There is no __atomic_* compare and exchange that returns the current value.
 * Also, GCC 5.4 seems unable to optimize a compound statement expression that
 * uses an additional stack variable with __atomic_compare_exchange[_n].
 */
#define p_atomic_cmpxchg(v, old, _new) \
   __sync_val_compare_and_swap((v), (old), (_new))

#endif

#if defined(PIPE_ATOMIC_UNLOCKED)

#define PIPE_ATOMIC "Unlocked"

#define p_atomic_set(_v, _i) (*(_v) = (_i))
#define p_atomic_read(_v) (*(_v))
#define p_atomic_dec_zero(_v) ((*(_v) -= 1) == 0)
#define p_atomic_inc(_v) (*(_v) += 1)
#define p_atomic_dec(_v) (*(_v) -= 1)
#define p_atomic_add(_v, _i) (*(_v) += (_i))
#define p_atomic_inc_return(_v) (*(_v) += 1)
#define p_atomic_dec_return(_v) (*(_v) -= 1)
#define p_atomic_cmpxchg(_v, old, _new) ({	\
	__typeof(*_v) _r = *(_v);		\
	if (*(_v) == old)			\
		*(_v) = (_new);			\
	_r;					\
})
#endif

/* Unlocked version for single threaded environments, such as some
 * windows kernel modules.
 */
#if defined(PIPE_ATOMIC_OS_UNLOCKED) 

#define PIPE_ATOMIC "Unlocked"

#define p_atomic_set(_v, _i) (*(_v) = (_i))
#define p_atomic_read(_v) (*(_v))
#define p_atomic_dec_zero(_v) (p_atomic_dec_return(_v) == 0)
#define p_atomic_inc(_v) ((void) p_atomic_inc_return(_v))
#define p_atomic_dec(_v) ((void) p_atomic_dec_return(_v))
#define p_atomic_add(_v, _i) (*(_v) = *(_v) + (_i))
#define p_atomic_inc_return(_v) (++(*(_v)))
#define p_atomic_dec_return(_v) (--(*(_v)))
#define p_atomic_cmpxchg(_v, _old, _new) (*(_v) == (_old) ? (*(_v) = (_new), (_old)) : *(_v))

#endif


#if defined(PIPE_ATOMIC_MSVC_INTRINSIC)

#define PIPE_ATOMIC "MSVC Intrinsics"

/* We use the Windows header's Interlocked*64 functions instead of the
 * _Interlocked*64 intrinsics wherever we can, as support for the latter varies
 * with target CPU, whereas Windows headers take care of all portability
 * issues: using intrinsics where available, falling back to library
 * implementations where not.
 */
#ifndef WIN32_LEAN_AND_MEAN
#define WIN32_LEAN_AND_MEAN 1
#endif
#include <windows.h>
#include <intrin.h>
#include <assert.h>

/* MSVC supports decltype keyword, but it's only supported on C++ and doesn't
 * quite work here; and if a C++-only solution is worthwhile, then it would be
 * better to use templates / function overloading, instead of decltype magic.
 * Therefore, we rely on implicit casting to LONGLONG for the functions that return
 */

#define p_atomic_set(_v, _i) (*(_v) = (_i))
#define p_atomic_read(_v) (*(_v))

#define p_atomic_dec_zero(_v) \
   (p_atomic_dec_return(_v) == 0)

#define p_atomic_inc(_v) \
   ((void) p_atomic_inc_return(_v))

#define p_atomic_inc_return(_v) (\
   sizeof *(_v) == sizeof(short)   ? _InterlockedIncrement16((short *)  (_v)) : \
   sizeof *(_v) == sizeof(long)    ? _InterlockedIncrement  ((long *)   (_v)) : \
   sizeof *(_v) == sizeof(__int64) ? InterlockedIncrement64 ((__int64 *)(_v)) : \
                                     (assert(!"should not get here"), 0))

#define p_atomic_dec(_v) \
   ((void) p_atomic_dec_return(_v))

#define p_atomic_dec_return(_v) (\
   sizeof *(_v) == sizeof(short)   ? _InterlockedDecrement16((short *)  (_v)) : \
   sizeof *(_v) == sizeof(long)    ? _InterlockedDecrement  ((long *)   (_v)) : \
   sizeof *(_v) == sizeof(__int64) ? InterlockedDecrement64 ((__int64 *)(_v)) : \
                                     (assert(!"should not get here"), 0))

#define p_atomic_add(_v, _i) (\
   sizeof *(_v) == sizeof(char)    ? _InterlockedExchangeAdd8 ((char *)   (_v), (_i)) : \
   sizeof *(_v) == sizeof(short)   ? _InterlockedExchangeAdd16((short *)  (_v), (_i)) : \
   sizeof *(_v) == sizeof(long)    ? _InterlockedExchangeAdd  ((long *)   (_v), (_i)) : \
   sizeof *(_v) == sizeof(__int64) ? InterlockedExchangeAdd64((__int64 *)(_v), (_i)) : \
                                     (assert(!"should not get here"), 0))

#define p_atomic_cmpxchg(_v, _old, _new) (\
   sizeof *(_v) == sizeof(char)    ? _InterlockedCompareExchange8 ((char *)   (_v), (char)   (_new), (char)   (_old)) : \
   sizeof *(_v) == sizeof(short)   ? _InterlockedCompareExchange16((short *)  (_v), (short)  (_new), (short)  (_old)) : \
   sizeof *(_v) == sizeof(long)    ? _InterlockedCompareExchange  ((long *)   (_v), (long)   (_new), (long)   (_old)) : \
   sizeof *(_v) == sizeof(__int64) ? InterlockedCompareExchange64 ((__int64 *)(_v), (__int64)(_new), (__int64)(_old)) : \
                                     (assert(!"should not get here"), 0))

#endif

#if defined(PIPE_ATOMIC_OS_SOLARIS)

#define PIPE_ATOMIC "Solaris OS atomic functions"

#include <atomic.h>
#include <assert.h>

#define p_atomic_set(_v, _i) (*(_v) = (_i))
#define p_atomic_read(_v) (*(_v))

#define p_atomic_dec_zero(v) (\
   sizeof(*v) == sizeof(uint8_t)  ? atomic_dec_8_nv ((uint8_t  *)(v)) == 0 : \
   sizeof(*v) == sizeof(uint16_t) ? atomic_dec_16_nv((uint16_t *)(v)) == 0 : \
   sizeof(*v) == sizeof(uint32_t) ? atomic_dec_32_nv((uint32_t *)(v)) == 0 : \
   sizeof(*v) == sizeof(uint64_t) ? atomic_dec_64_nv((uint64_t *)(v)) == 0 : \
                                    (assert(!"should not get here"), 0))

#define p_atomic_inc(v) (void) (\
   sizeof(*v) == sizeof(uint8_t)  ? atomic_inc_8 ((uint8_t  *)(v)) : \
   sizeof(*v) == sizeof(uint16_t) ? atomic_inc_16((uint16_t *)(v)) : \
   sizeof(*v) == sizeof(uint32_t) ? atomic_inc_32((uint32_t *)(v)) : \
   sizeof(*v) == sizeof(uint64_t) ? atomic_inc_64((uint64_t *)(v)) : \
                                    (assert(!"should not get here"), 0))

#define p_atomic_inc_return(v) ((__typeof(*v)) \
   sizeof(*v) == sizeof(uint8_t)  ? atomic_inc_8_nv ((uint8_t  *)(v)) : \
   sizeof(*v) == sizeof(uint16_t) ? atomic_inc_16_nv((uint16_t *)(v)) : \
   sizeof(*v) == sizeof(uint32_t) ? atomic_inc_32_nv((uint32_t *)(v)) : \
   sizeof(*v) == sizeof(uint64_t) ? atomic_inc_64_nv((uint64_t *)(v)) : \
                                    (assert(!"should not get here"), 0))

#define p_atomic_dec(v) ((void) \
   sizeof(*v) == sizeof(uint8_t)  ? atomic_dec_8 ((uint8_t  *)(v)) : \
   sizeof(*v) == sizeof(uint16_t) ? atomic_dec_16((uint16_t *)(v)) : \
   sizeof(*v) == sizeof(uint32_t) ? atomic_dec_32((uint32_t *)(v)) : \
   sizeof(*v) == sizeof(uint64_t) ? atomic_dec_64((uint64_t *)(v)) : \
                                    (assert(!"should not get here"), 0))

#define p_atomic_dec_return(v) ((__typeof(*v)) \
   sizeof(*v) == sizeof(uint8_t)  ? atomic_dec_8_nv ((uint8_t  *)(v)) : \
   sizeof(*v) == sizeof(uint16_t) ? atomic_dec_16_nv((uint16_t *)(v)) : \
   sizeof(*v) == sizeof(uint32_t) ? atomic_dec_32_nv((uint32_t *)(v)) : \
   sizeof(*v) == sizeof(uint64_t) ? atomic_dec_64_nv((uint64_t *)(v)) : \
                                    (assert(!"should not get here"), 0))

#define p_atomic_add(v, i) ((void)				     \
   sizeof(*v) == sizeof(uint8_t)  ? atomic_add_8 ((uint8_t  *)(v), (i)) : \
   sizeof(*v) == sizeof(uint16_t) ? atomic_add_16((uint16_t *)(v), (i)) : \
   sizeof(*v) == sizeof(uint32_t) ? atomic_add_32((uint32_t *)(v), (i)) : \
   sizeof(*v) == sizeof(uint64_t) ? atomic_add_64((uint64_t *)(v), (i)) : \
                                    (assert(!"should not get here"), 0))

#define p_atomic_cmpxchg(v, old, _new) ((__typeof(*v)) \
   sizeof(*v) == sizeof(uint8_t)  ? atomic_cas_8 ((uint8_t  *)(v), (uint8_t )(old), (uint8_t )(_new)) : \
   sizeof(*v) == sizeof(uint16_t) ? atomic_cas_16((uint16_t *)(v), (uint16_t)(old), (uint16_t)(_new)) : \
   sizeof(*v) == sizeof(uint32_t) ? atomic_cas_32((uint32_t *)(v), (uint32_t)(old), (uint32_t)(_new)) : \
   sizeof(*v) == sizeof(uint64_t) ? atomic_cas_64((uint64_t *)(v), (uint64_t)(old), (uint64_t)(_new)) : \
                                    (assert(!"should not get here"), 0))

#endif

#ifndef PIPE_ATOMIC
#error "No pipe_atomic implementation selected"
#endif



#endif /* U_ATOMIC_H */
@


1.7
log
@Merge Mesa 13.0.2
@
text
@d26 1
a26 4
#elif defined(__ARM_ARCH_4__) || defined(__ARM_ARCH_4T__) || \
      defined(__ARM_ARCH_5__) || defined(__ARM_ARCH_5T__) || \
      defined(__ARM_ARCH_5TE__) || defined(__ARM_ARCH_5E__) || \
      defined(__hppa__) || defined(__sparc__) || defined(__sh__)
@


1.6
log
@Merge Mesa 11.2.2
@
text
@d44 14
d66 7
@


1.5
log
@test some more __ARM_ARCH_* builtin defines
@
text
@a113 59
#if _MSC_VER < 1600

/* Implement _InterlockedCompareExchange8 in terms of _InterlockedCompareExchange16 */
static __inline char
_InterlockedCompareExchange8(char volatile *destination8, char exchange8, char comparand8)
{
   INT_PTR destinationAddr = (INT_PTR)destination8;
   short volatile *destination16 = (short volatile *)(destinationAddr & ~1);
   const short shift8 = (destinationAddr & 1) * 8;
   const short mask8 = 0xff << shift8;
   short initial16 = *destination16;
   char initial8 = initial16 >> shift8;
   while (initial8 == comparand8) {
      /* initial *destination8 matches, so try exchange it while keeping the
       * neighboring byte untouched */
      short exchange16 = (initial16 & ~mask8) | ((short)exchange8 << shift8);
      short comparand16 = initial16;
      short initial16 = _InterlockedCompareExchange16(destination16, exchange16, comparand16);
      if (initial16 == comparand16) {
         /* succeeded */
         return comparand8;
      }
      /* something changed, retry with the new initial value */
      initial8 = initial16 >> shift8;
   }
   return initial8;
}

/* Implement _InterlockedExchangeAdd16 in terms of _InterlockedCompareExchange16 */
static __inline short
_InterlockedExchangeAdd16(short volatile *addend, short value)
{
   short initial = *addend;
   short comparand;
   do {
      short exchange = initial + value;
      comparand = initial;
      /* if *addend==comparand then *addend=exchange, return original *addend */
      initial = _InterlockedCompareExchange16(addend, exchange, comparand);
   } while(initial != comparand);
   return comparand;
}

/* Implement _InterlockedExchangeAdd8 in terms of _InterlockedCompareExchange8 */
static __inline char
_InterlockedExchangeAdd8(char volatile *addend, char value)
{
   char initial = *addend;
   char comparand;
   do {
      char exchange = initial + value;
      comparand = initial;
      initial = _InterlockedCompareExchange8(addend, exchange, comparand);
   } while(initial != comparand);
   return comparand;
}

#endif /* _MSC_VER < 1600 */

@


1.4
log
@Back out rev 1.3 and use __sync builtins on alpha.  gcc no longer
triggers an ICE with p_atomic_cmpxchg/__sync_val_compare_and_swap.

Tested by matthieu.
@
text
@d26 3
a28 1
#elif defined(__ARM_ARCH_4__) || defined(__ARM_ARCH_5__) || \
@


1.3
log
@alpha lacks p_atomic_cmpxchg() switch to non atomic ops
@
text
@d27 1
a27 2
	defined(__hppa__) || defined(__sparc__) || defined(__sh__) \
	|| defined(__alpha__)
@


1.2
log
@provide an unlocked atomic fallback path

sparc < v9, hppa, sh, arm < v6 don't provide these builtins
and we don't have kernel assistance to fake them currently.
@
text
@d27 2
a28 1
      defined(__hppa__) || defined(__sparc__) || defined(__sh__)
@


1.1
log
@Initial revision
@
text
@d26 3
d55 1
d57 17
@


1.1.1.1
log
@import Mesa 11.0.6
@
text
@@


1.1.1.2
log
@Import Mesa 11.2.2
@
text
@d91 59
@


1.1.1.3
log
@Import Mesa 13.0.2
@
text
@a38 14
#if defined(USE_GCC_ATOMIC_BUILTINS)

/* The builtins with explicit memory model are available since GCC 4.7. */
#define p_atomic_set(_v, _i) __atomic_store_n((_v), (_i), __ATOMIC_RELEASE)
#define p_atomic_read(_v) __atomic_load_n((_v), __ATOMIC_ACQUIRE)
#define p_atomic_dec_zero(v) (__atomic_sub_fetch((v), 1, __ATOMIC_ACQ_REL) == 0)
#define p_atomic_inc(v) (void) __atomic_add_fetch((v), 1, __ATOMIC_ACQ_REL)
#define p_atomic_dec(v) (void) __atomic_sub_fetch((v), 1, __ATOMIC_ACQ_REL)
#define p_atomic_add(v, i) (void) __atomic_add_fetch((v), (i), __ATOMIC_ACQ_REL)
#define p_atomic_inc_return(v) __atomic_add_fetch((v), 1, __ATOMIC_ACQ_REL)
#define p_atomic_dec_return(v) __atomic_sub_fetch((v), 1, __ATOMIC_ACQ_REL)

#else

a46 7

#endif

/* There is no __atomic_* compare and exchange that returns the current value.
 * Also, GCC 5.4 seems unable to optimize a compound statement expression that
 * uses an additional stack variable with __atomic_compare_exchange[_n].
 */
@


