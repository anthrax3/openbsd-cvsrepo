head	1.1;
branch	1.1.1;
access;
symbols
	mesa-13_0_6:1.1.1.3
	mesa-13_0_5:1.1.1.3
	mesa-13_0_3:1.1.1.3
	mesa-13_0_2:1.1.1.3
	OPENBSD_6_0:1.1.1.2.0.4
	OPENBSD_6_0_BASE:1.1.1.2
	mesa-11_2_2:1.1.1.2
	OPENBSD_5_9:1.1.1.1.0.2
	OPENBSD_5_9_BASE:1.1.1.1
	mesa-11_0_9:1.1.1.1
	mesa-11_0_8:1.1.1.1
	mesa-11_0_6:1.1.1.1
	mesa:1.1.1;
locks; strict;
comment	@ * @;


1.1
date	2015.11.22.02.40.13;	author jsg;	state Exp;
branches
	1.1.1.1;
next	;
commitid	bJUptkbooQfJPk5r;

1.1.1.1
date	2015.11.22.02.40.13;	author jsg;	state Exp;
branches;
next	1.1.1.2;
commitid	bJUptkbooQfJPk5r;

1.1.1.2
date	2016.05.29.10.15.31;	author jsg;	state Exp;
branches;
next	1.1.1.3;
commitid	OwGfrJACrYJkCVJ4;

1.1.1.3
date	2016.12.11.08.36.56;	author jsg;	state Exp;
branches;
next	;
commitid	uuv5VTS15jglEDZU;


desc
@@


1.1
log
@Initial revision
@
text
@/*
 * Copyright Â© 2011 Intel Corporation
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice (including the next
 * paragraph) shall be included in all copies or substantial portions of the
 * Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
 * IN THE SOFTWARE.
 */

/**
 * @@file brw_vue_map.c
 *
 * This file computes the "VUE map" for a (non-fragment) shader stage, which
 * describes the layout of its output varyings.  The VUE map is used to match
 * outputs from one stage with the inputs of the next.
 *
 * Largely, varyings can be placed however we like - producers/consumers simply
 * have to agree on the layout.  However, there is also a "VUE Header" that
 * prescribes a fixed-layout for items that interact with fixed function
 * hardware, such as the clipper and rasterizer.
 *
 * Authors:
 *   Paul Berry <stereotype441@@gmail.com>
 *   Chris Forbes <chrisf@@ijw.co.nz>
 *   Eric Anholt <eric@@anholt.net>
 */


#include "main/compiler.h"
#include "brw_context.h"

static inline void
assign_vue_slot(struct brw_vue_map *vue_map, int varying)
{
   /* Make sure this varying hasn't been assigned a slot already */
   assert (vue_map->varying_to_slot[varying] == -1);

   vue_map->varying_to_slot[varying] = vue_map->num_slots;
   vue_map->slot_to_varying[vue_map->num_slots++] = varying;
}

/**
 * Compute the VUE map for a shader stage.
 */
void
brw_compute_vue_map(const struct brw_device_info *devinfo,
                    struct brw_vue_map *vue_map,
                    GLbitfield64 slots_valid)
{
   vue_map->slots_valid = slots_valid;
   int i;

   /* gl_Layer and gl_ViewportIndex don't get their own varying slots -- they
    * are stored in the first VUE slot (VARYING_SLOT_PSIZ).
    */
   slots_valid &= ~(VARYING_BIT_LAYER | VARYING_BIT_VIEWPORT);

   /* Make sure that the values we store in vue_map->varying_to_slot and
    * vue_map->slot_to_varying won't overflow the signed chars that are used
    * to store them.  Note that since vue_map->slot_to_varying sometimes holds
    * values equal to BRW_VARYING_SLOT_COUNT, we need to ensure that
    * BRW_VARYING_SLOT_COUNT is <= 127, not 128.
    */
   STATIC_ASSERT(BRW_VARYING_SLOT_COUNT <= 127);

   vue_map->num_slots = 0;
   for (i = 0; i < BRW_VARYING_SLOT_COUNT; ++i) {
      vue_map->varying_to_slot[i] = -1;
      vue_map->slot_to_varying[i] = BRW_VARYING_SLOT_COUNT;
   }

   /* VUE header: format depends on chip generation and whether clipping is
    * enabled.
    *
    * See the Sandybridge PRM, Volume 2 Part 1, section 1.5.1 (page 30),
    * "Vertex URB Entry (VUE) Formats" which describes the VUE header layout.
    */
   if (devinfo->gen < 6) {
      /* There are 8 dwords in VUE header pre-Ironlake:
       * dword 0-3 is indices, point width, clip flags.
       * dword 4-7 is ndc position
       * dword 8-11 is the first vertex data.
       *
       * On Ironlake the VUE header is nominally 20 dwords, but the hardware
       * will accept the same header layout as Gen4 [and should be a bit faster]
       */
      assign_vue_slot(vue_map, VARYING_SLOT_PSIZ);
      assign_vue_slot(vue_map, BRW_VARYING_SLOT_NDC);
      assign_vue_slot(vue_map, VARYING_SLOT_POS);
   } else {
      /* There are 8 or 16 DWs (D0-D15) in VUE header on Sandybridge:
       * dword 0-3 of the header is indices, point width, clip flags.
       * dword 4-7 is the 4D space position
       * dword 8-15 of the vertex header is the user clip distance if
       * enabled.
       * dword 8-11 or 16-19 is the first vertex element data we fill.
       */
      assign_vue_slot(vue_map, VARYING_SLOT_PSIZ);
      assign_vue_slot(vue_map, VARYING_SLOT_POS);
      if (slots_valid & BITFIELD64_BIT(VARYING_SLOT_CLIP_DIST0))
         assign_vue_slot(vue_map, VARYING_SLOT_CLIP_DIST0);
      if (slots_valid & BITFIELD64_BIT(VARYING_SLOT_CLIP_DIST1))
         assign_vue_slot(vue_map, VARYING_SLOT_CLIP_DIST1);

      /* front and back colors need to be consecutive so that we can use
       * ATTRIBUTE_SWIZZLE_INPUTATTR_FACING to swizzle them when doing
       * two-sided color.
       */
      if (slots_valid & BITFIELD64_BIT(VARYING_SLOT_COL0))
         assign_vue_slot(vue_map, VARYING_SLOT_COL0);
      if (slots_valid & BITFIELD64_BIT(VARYING_SLOT_BFC0))
         assign_vue_slot(vue_map, VARYING_SLOT_BFC0);
      if (slots_valid & BITFIELD64_BIT(VARYING_SLOT_COL1))
         assign_vue_slot(vue_map, VARYING_SLOT_COL1);
      if (slots_valid & BITFIELD64_BIT(VARYING_SLOT_BFC1))
         assign_vue_slot(vue_map, VARYING_SLOT_BFC1);
   }

   /* The hardware doesn't care about the rest of the vertex outputs, so just
    * assign them contiguously.  Don't reassign outputs that already have a
    * slot.
    *
    * We generally don't need to assign a slot for VARYING_SLOT_CLIP_VERTEX,
    * since it's encoded as the clip distances by emit_clip_distances().
    * However, it may be output by transform feedback, and we'd rather not
    * recompute state when TF changes, so we just always include it.
    */
   for (int i = 0; i < VARYING_SLOT_MAX; ++i) {
      if ((slots_valid & BITFIELD64_BIT(i)) &&
          vue_map->varying_to_slot[i] == -1) {
         assign_vue_slot(vue_map, i);
      }
   }
}
@


1.1.1.1
log
@import Mesa 11.0.6
@
text
@@


1.1.1.2
log
@Import Mesa 11.2.2
@
text
@d43 1
d47 1
a47 1
assign_vue_slot(struct brw_vue_map *vue_map, int varying, int slot)
d52 2
a53 2
   vue_map->varying_to_slot[varying] = slot;
   vue_map->slot_to_varying[slot] = varying;
d62 1
a62 2
                    GLbitfield64 slots_valid,
                    bool separate)
a63 7
   /* Keep using the packed/contiguous layout on old hardware - we only need
    * the SSO layout when using geometry/tessellation shaders or 32 FS input
    * varyings, which only exist on Gen >= 6.  It's also a bit more efficient.
    */
   if (devinfo->gen < 6)
      separate = false;

d65 1
a65 1
   vue_map->separate = separate;
d80 2
a81 1
   for (int i = 0; i < BRW_VARYING_SLOT_COUNT; ++i) {
d83 1
a83 1
      vue_map->slot_to_varying[i] = BRW_VARYING_SLOT_PAD;
a85 2
   int slot = 0;

d101 3
a103 3
      assign_vue_slot(vue_map, VARYING_SLOT_PSIZ, slot++);
      assign_vue_slot(vue_map, BRW_VARYING_SLOT_NDC, slot++);
      assign_vue_slot(vue_map, VARYING_SLOT_POS, slot++);
d112 2
a113 2
      assign_vue_slot(vue_map, VARYING_SLOT_PSIZ, slot++);
      assign_vue_slot(vue_map, VARYING_SLOT_POS, slot++);
d115 1
a115 1
         assign_vue_slot(vue_map, VARYING_SLOT_CLIP_DIST0, slot++);
d117 1
a117 1
         assign_vue_slot(vue_map, VARYING_SLOT_CLIP_DIST1, slot++);
d124 1
a124 1
         assign_vue_slot(vue_map, VARYING_SLOT_COL0, slot++);
d126 1
a126 1
         assign_vue_slot(vue_map, VARYING_SLOT_BFC0, slot++);
d128 1
a128 1
         assign_vue_slot(vue_map, VARYING_SLOT_COL1, slot++);
d130 1
a130 1
         assign_vue_slot(vue_map, VARYING_SLOT_BFC1, slot++);
d133 3
a135 9
   /* The hardware doesn't care about the rest of the vertex outputs, so we
    * can assign them however we like.  For normal programs, we simply assign
    * them contiguously.
    *
    * For separate shader pipelines, we first assign built-in varyings
    * contiguous slots.  This works because ARB_separate_shader_objects
    * requires that all shaders have matching built-in varying interface
    * blocks.  Next, we assign generic varyings based on their location
    * (either explicit or linker assigned).  This guarantees a fixed layout.
d142 4
a145 133
   GLbitfield64 builtins = slots_valid & BITFIELD64_MASK(VARYING_SLOT_VAR0);
   while (builtins != 0) {
      const int varying = ffsll(builtins) - 1;
      if (vue_map->varying_to_slot[varying] == -1) {
         assign_vue_slot(vue_map, varying, slot++);
      }
      builtins &= ~BITFIELD64_BIT(varying);
   }

   const int first_generic_slot = slot;
   GLbitfield64 generics = slots_valid & ~BITFIELD64_MASK(VARYING_SLOT_VAR0);
   while (generics != 0) {
      const int varying = ffsll(generics) - 1;
      if (separate) {
         slot = first_generic_slot + varying - VARYING_SLOT_VAR0;
         assign_vue_slot(vue_map, varying, slot);
      } else {
         assign_vue_slot(vue_map, varying, slot++);
      }
      generics &= ~BITFIELD64_BIT(varying);
   }

   vue_map->num_slots = separate ? slot + 1 : slot;
   vue_map->num_per_vertex_slots = 0;
   vue_map->num_per_patch_slots = 0;
}

/**
 * Compute the VUE map for tessellation control shader outputs and
 * tessellation evaluation shader inputs.
 */
void
brw_compute_tess_vue_map(struct brw_vue_map *vue_map,
                         GLbitfield64 vertex_slots,
                         GLbitfield patch_slots)
{
   /* I don't think anything actually uses this... */
   vue_map->slots_valid = vertex_slots;

   vertex_slots &= ~(VARYING_BIT_TESS_LEVEL_OUTER |
                     VARYING_BIT_TESS_LEVEL_INNER);

   /* Make sure that the values we store in vue_map->varying_to_slot and
    * vue_map->slot_to_varying won't overflow the signed chars that are used
    * to store them.  Note that since vue_map->slot_to_varying sometimes holds
    * values equal to VARYING_SLOT_TESS_MAX , we need to ensure that
    * VARYING_SLOT_TESS_MAX is <= 127, not 128.
    */
   STATIC_ASSERT(VARYING_SLOT_TESS_MAX <= 127);

   for (int i = 0; i < VARYING_SLOT_TESS_MAX ; ++i) {
      vue_map->varying_to_slot[i] = -1;
      vue_map->slot_to_varying[i] = BRW_VARYING_SLOT_PAD;
   }

   int slot = 0;

   /* The first 8 DWords are reserved for the "Patch Header".
    *
    * VARYING_SLOT_TESS_LEVEL_OUTER / INNER live here, but the exact layout
    * depends on the domain type.  They might not be in slots 0 and 1 as
    * described here, but pretending they're separate allows us to uniquely
    * identify them by distinct slot locations.
    */
   assign_vue_slot(vue_map, VARYING_SLOT_TESS_LEVEL_INNER, slot++);
   assign_vue_slot(vue_map, VARYING_SLOT_TESS_LEVEL_OUTER, slot++);

   /* first assign per-patch varyings */
   while (patch_slots != 0) {
      const int varying = ffsll(patch_slots) - 1;
      if (vue_map->varying_to_slot[varying + VARYING_SLOT_PATCH0] == -1) {
         assign_vue_slot(vue_map, varying + VARYING_SLOT_PATCH0, slot++);
      }
      patch_slots &= ~BITFIELD64_BIT(varying);
   }

   /* apparently, including the patch header... */
   vue_map->num_per_patch_slots = slot;

   /* then assign per-vertex varyings for each vertex in our patch */
   while (vertex_slots != 0) {
      const int varying = ffsll(vertex_slots) - 1;
      if (vue_map->varying_to_slot[varying] == -1) {
         assign_vue_slot(vue_map, varying, slot++);
      }
      vertex_slots &= ~BITFIELD64_BIT(varying);
   }

   vue_map->num_per_vertex_slots = slot - vue_map->num_per_patch_slots;
   vue_map->num_slots = slot;
}

static const char *
varying_name(brw_varying_slot slot)
{
   assume(slot < BRW_VARYING_SLOT_COUNT);

   if (slot < VARYING_SLOT_MAX)
      return gl_varying_slot_name(slot);

   static const char *brw_names[] = {
      [BRW_VARYING_SLOT_NDC - VARYING_SLOT_MAX] = "BRW_VARYING_SLOT_NDC",
      [BRW_VARYING_SLOT_PAD - VARYING_SLOT_MAX] = "BRW_VARYING_SLOT_PAD",
      [BRW_VARYING_SLOT_PNTC - VARYING_SLOT_MAX] = "BRW_VARYING_SLOT_PNTC",
   };

   return brw_names[slot - VARYING_SLOT_MAX];
}

void
brw_print_vue_map(FILE *fp, const struct brw_vue_map *vue_map)
{
   if (vue_map->num_per_vertex_slots > 0 || vue_map->num_per_patch_slots > 0) {
      fprintf(fp, "PUE map (%d slots, %d/patch, %d/vertex, %s)\n",
              vue_map->num_slots,
              vue_map->num_per_patch_slots,
              vue_map->num_per_vertex_slots,
              vue_map->separate ? "SSO" : "non-SSO");
      for (int i = 0; i < vue_map->num_slots; i++) {
         if (vue_map->slot_to_varying[i] >= VARYING_SLOT_PATCH0) {
            fprintf(fp, "  [%d] VARYING_SLOT_PATCH%d\n", i,
                    vue_map->slot_to_varying[i] - VARYING_SLOT_PATCH0);
         } else {
            fprintf(fp, "  [%d] %s\n", i,
                    varying_name(vue_map->slot_to_varying[i]));
         }
      }
   } else {
      fprintf(fp, "VUE map (%d slots, %s)\n",
              vue_map->num_slots, vue_map->separate ? "SSO" : "non-SSO");
      for (int i = 0; i < vue_map->num_slots; i++) {
         fprintf(fp, "  [%d] %s\n", i,
                 varying_name(vue_map->slot_to_varying[i]));
a147 1
   fprintf(fp, "\n");
@


1.1.1.3
log
@Import Mesa 13.0.2
@
text
@d59 1
a59 1
brw_compute_vue_map(const struct gen_device_info *devinfo,
@


