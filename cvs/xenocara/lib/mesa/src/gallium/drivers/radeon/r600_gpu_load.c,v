head	1.2;
access;
symbols
	OPENBSD_6_2:1.2.0.2
	OPENBSD_6_2_BASE:1.2
	mesa-17_1_6:1.1.1.2
	OPENBSD_6_1:1.1.1.1.0.4
	OPENBSD_6_1_BASE:1.1.1.1
	mesa-13_0_6:1.1.1.1
	mesa-13_0_5:1.1.1.1
	mesa-13_0_3:1.1.1.1
	mesa-13_0_2:1.1.1.1
	OPENBSD_6_0:1.1.1.1.0.6
	OPENBSD_6_0_BASE:1.1.1.1
	mesa-11_2_2:1.1.1.1
	OPENBSD_5_9:1.1.1.1.0.2
	OPENBSD_5_9_BASE:1.1.1.1
	mesa-11_0_9:1.1.1.1
	mesa-11_0_8:1.1.1.1
	mesa-11_0_6:1.1.1.1
	mesa:1.1.1;
locks; strict;
comment	@ * @;


1.2
date	2017.08.26.16.59.26;	author jsg;	state Exp;
branches;
next	1.1;
commitid	D0k2io1oY8gcsQ2S;

1.1
date	2015.11.22.02.43.43;	author jsg;	state Exp;
branches
	1.1.1.1;
next	;
commitid	bJUptkbooQfJPk5r;

1.1.1.1
date	2015.11.22.02.43.43;	author jsg;	state Exp;
branches;
next	1.1.1.2;
commitid	bJUptkbooQfJPk5r;

1.1.1.2
date	2017.08.14.09.32.12;	author jsg;	state Exp;
branches;
next	;
commitid	enNyoMGkcgwM3Ww6;


desc
@@


1.2
log
@Revert to Mesa 13.0.6 to hopefully address rendering issues a handful of
people have reported with xpdf/fvwm on ivy bridge with modesetting driver.
@
text
@/*
 * Copyright 2015 Advanced Micro Devices, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice (including the next
 * paragraph) shall be included in all copies or substantial portions of the
 * Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 * SOFTWARE.
 *
 * Authors: Marek Olšák <maraeo@@gmail.com>
 *
 */

/* The GPU load is measured as follows.
 *
 * There is a thread which samples the GRBM_STATUS register at a certain
 * frequency and the "busy" or "idle" counter is incremented based on
 * whether the GUI_ACTIVE bit is set or not.
 *
 * Then, the user can sample the counters twice and calculate the average
 * GPU load between the two samples.
 */

#include "r600_pipe_common.h"
#include "os/os_time.h"

/* For good accuracy at 1000 fps or lower. This will be inaccurate for higher
 * fps (there are too few samples per frame). */
#define SAMPLES_PER_SEC 10000

#define GRBM_STATUS		0x8010
#define GUI_ACTIVE(x)		(((x) >> 31) & 0x1)

static bool r600_is_gpu_busy(struct r600_common_screen *rscreen)
{
	uint32_t value = 0;

	rscreen->ws->read_registers(rscreen->ws, GRBM_STATUS, 1, &value);
	return GUI_ACTIVE(value);
}

static PIPE_THREAD_ROUTINE(r600_gpu_load_thread, param)
{
	struct r600_common_screen *rscreen = (struct r600_common_screen*)param;
	const int period_us = 1000000 / SAMPLES_PER_SEC;
	int sleep_us = period_us;
	int64_t cur_time, last_time = os_time_get();

	while (!p_atomic_read(&rscreen->gpu_load_stop_thread)) {
		if (sleep_us)
			os_time_sleep(sleep_us);

		/* Make sure we sleep the ideal amount of time to match
		 * the expected frequency. */
		cur_time = os_time_get();

		if (os_time_timeout(last_time, last_time + period_us,
				    cur_time))
			sleep_us = MAX2(sleep_us - 1, 1);
		else
			sleep_us += 1;

		/*printf("Hz: %.1f\n", 1000000.0 / (cur_time - last_time));*/
		last_time = cur_time;

		/* Update the counters. */
		if (r600_is_gpu_busy(rscreen))
			p_atomic_inc(&rscreen->gpu_load_counter_busy);
		else
			p_atomic_inc(&rscreen->gpu_load_counter_idle);
	}
	p_atomic_dec(&rscreen->gpu_load_stop_thread);
	return 0;
}

void r600_gpu_load_kill_thread(struct r600_common_screen *rscreen)
{
	if (!rscreen->gpu_load_thread)
		return;

	p_atomic_inc(&rscreen->gpu_load_stop_thread);
	pipe_thread_wait(rscreen->gpu_load_thread);
	rscreen->gpu_load_thread = 0;
}

static uint64_t r600_gpu_load_read_counter(struct r600_common_screen *rscreen)
{
	/* Start the thread if needed. */
	if (!rscreen->gpu_load_thread) {
		pipe_mutex_lock(rscreen->gpu_load_mutex);
		/* Check again inside the mutex. */
		if (!rscreen->gpu_load_thread)
			rscreen->gpu_load_thread =
				pipe_thread_create(r600_gpu_load_thread, rscreen);
		pipe_mutex_unlock(rscreen->gpu_load_mutex);
	}

	/* The busy counter is in the lower 32 bits.
	 * The idle counter is in the upper 32 bits. */
	return p_atomic_read(&rscreen->gpu_load_counter_busy) |
	       ((uint64_t)p_atomic_read(&rscreen->gpu_load_counter_idle) << 32);
}

/**
 * Just return the counters.
 */
uint64_t r600_gpu_load_begin(struct r600_common_screen *rscreen)
{
	return r600_gpu_load_read_counter(rscreen);
}

unsigned r600_gpu_load_end(struct r600_common_screen *rscreen, uint64_t begin)
{
	uint64_t end = r600_gpu_load_read_counter(rscreen);
	unsigned busy = (end & 0xffffffff) - (begin & 0xffffffff);
	unsigned idle = (end >> 32) - (begin >> 32);

	/* Calculate the GPU load.
	 *
	 * If no counters have been incremented, return the current load.
	 * It's for the case when the load is queried faster than
	 * the counters are updated.
	 */
	if (idle || busy)
		return busy*100 / (busy + idle);
	else
		return r600_is_gpu_busy(rscreen) ? 100 : 0;
}
@


1.1
log
@Initial revision
@
text
@@


1.1.1.1
log
@import Mesa 11.0.6
@
text
@@


1.1.1.2
log
@Import Mesa 17.1.6
@
text
@a37 1
#include "r600_query.h"
a44 13
#define TA_BUSY(x)		(((x) >> 14) & 0x1)
#define GDS_BUSY(x)		(((x) >> 15) & 0x1)
#define VGT_BUSY(x)		(((x) >> 17) & 0x1)
#define IA_BUSY(x)		(((x) >> 19) & 0x1)
#define SX_BUSY(x)		(((x) >> 20) & 0x1)
#define WD_BUSY(x)		(((x) >> 21) & 0x1)
#define SPI_BUSY(x)		(((x) >> 22) & 0x1)
#define BCI_BUSY(x)		(((x) >> 23) & 0x1)
#define SC_BUSY(x)		(((x) >> 24) & 0x1)
#define PA_BUSY(x)		(((x) >> 25) & 0x1)
#define DB_BUSY(x)		(((x) >> 26) & 0x1)
#define CP_BUSY(x)		(((x) >> 29) & 0x1)
#define CB_BUSY(x)		(((x) >> 30) & 0x1)
d47 1
a47 24
#define SRBM_STATUS2		0x0e4c
#define SDMA_BUSY(x)		(((x) >> 5) & 0x1)

#define CP_STAT                 0x8680
#define PFP_BUSY(x)		(((x) >> 15) & 0x1)
#define MEQ_BUSY(x)		(((x) >> 16) & 0x1)
#define ME_BUSY(x)		(((x) >> 17) & 0x1)
#define SURFACE_SYNC_BUSY(x)	(((x) >> 21) & 0x1)
#define DMA_BUSY(x)		(((x) >> 22) & 0x1)
#define SCRATCH_RAM_BUSY(x)	(((x) >> 24) & 0x1)
#define CE_BUSY(x)		(((x) >> 26) & 0x1)

#define IDENTITY(x) x

#define UPDATE_COUNTER(field, mask)					\
	do {								\
		if (mask(value))					\
			p_atomic_inc(&counters->named.field.busy);	\
		else							\
			p_atomic_inc(&counters->named.field.idle);	\
	} while (0)

static void r600_update_mmio_counters(struct r600_common_screen *rscreen,
				      union r600_mmio_counters *counters)
a49 1
	bool gui_busy, sdma_busy = false;
a50 1
	/* GRBM_STATUS */
d52 1
a52 40

	UPDATE_COUNTER(ta, TA_BUSY);
	UPDATE_COUNTER(gds, GDS_BUSY);
	UPDATE_COUNTER(vgt, VGT_BUSY);
	UPDATE_COUNTER(ia, IA_BUSY);
	UPDATE_COUNTER(sx, SX_BUSY);
	UPDATE_COUNTER(wd, WD_BUSY);
	UPDATE_COUNTER(spi, SPI_BUSY);
	UPDATE_COUNTER(bci, BCI_BUSY);
	UPDATE_COUNTER(sc, SC_BUSY);
	UPDATE_COUNTER(pa, PA_BUSY);
	UPDATE_COUNTER(db, DB_BUSY);
	UPDATE_COUNTER(cp, CP_BUSY);
	UPDATE_COUNTER(cb, CB_BUSY);
	UPDATE_COUNTER(gui, GUI_ACTIVE);
	gui_busy = GUI_ACTIVE(value);

	if (rscreen->chip_class >= CIK) {
		/* SRBM_STATUS2 */
		rscreen->ws->read_registers(rscreen->ws, SRBM_STATUS2, 1, &value);

		UPDATE_COUNTER(sdma, SDMA_BUSY);
		sdma_busy = SDMA_BUSY(value);
	}

	if (rscreen->chip_class >= VI) {
		/* CP_STAT */
		rscreen->ws->read_registers(rscreen->ws, CP_STAT, 1, &value);

		UPDATE_COUNTER(pfp, PFP_BUSY);
		UPDATE_COUNTER(meq, MEQ_BUSY);
		UPDATE_COUNTER(me, ME_BUSY);
		UPDATE_COUNTER(surf_sync, SURFACE_SYNC_BUSY);
		UPDATE_COUNTER(dma, DMA_BUSY);
		UPDATE_COUNTER(scratch_ram, SCRATCH_RAM_BUSY);
		UPDATE_COUNTER(ce, CE_BUSY);
	}

	value = gui_busy || sdma_busy;
	UPDATE_COUNTER(gpu, IDENTITY);
d55 1
a55 4
#undef UPDATE_COUNTER

static int
r600_gpu_load_thread(void *param)
d80 4
a83 1
		r600_update_mmio_counters(rscreen, &rscreen->mmio_counters);
d95 1
a95 1
	thrd_join(rscreen->gpu_load_thread, NULL);
d99 1
a99 2
static uint64_t r600_read_mmio_counter(struct r600_common_screen *rscreen,
				       unsigned busy_index)
d103 1
a103 1
		mtx_lock(&rscreen->gpu_load_mutex);
d107 2
a108 2
				u_thread_create(r600_gpu_load_thread, rscreen);
		mtx_unlock(&rscreen->gpu_load_mutex);
d111 5
a115 2
	unsigned busy = p_atomic_read(&rscreen->mmio_counters.array[busy_index]);
	unsigned idle = p_atomic_read(&rscreen->mmio_counters.array[busy_index + 1]);
d117 6
a122 1
	return busy | ((uint64_t)idle << 32);
d125 1
a125 2
static unsigned r600_end_mmio_counter(struct r600_common_screen *rscreen,
				      uint64_t begin, unsigned busy_index)
d127 1
a127 1
	uint64_t end = r600_read_mmio_counter(rscreen, busy_index);
d131 1
a131 1
	/* Calculate the % of time the busy counter was being incremented.
d133 1
a133 1
	 * If no counters were incremented, return the current counter status.
d137 1
a137 1
	if (idle || busy) {
d139 2
a140 76
	} else {
		union r600_mmio_counters counters;

		memset(&counters, 0, sizeof(counters));
		r600_update_mmio_counters(rscreen, &counters);
		return counters.array[busy_index] ? 100 : 0;
	}
}

#define BUSY_INDEX(rscreen, field) (&rscreen->mmio_counters.named.field.busy - \
				    rscreen->mmio_counters.array)

static unsigned busy_index_from_type(struct r600_common_screen *rscreen,
				     unsigned type)
{
	switch (type) {
	case R600_QUERY_GPU_LOAD:
		return BUSY_INDEX(rscreen, gpu);
	case R600_QUERY_GPU_SHADERS_BUSY:
		return BUSY_INDEX(rscreen, spi);
	case R600_QUERY_GPU_TA_BUSY:
		return BUSY_INDEX(rscreen, ta);
	case R600_QUERY_GPU_GDS_BUSY:
		return BUSY_INDEX(rscreen, gds);
	case R600_QUERY_GPU_VGT_BUSY:
		return BUSY_INDEX(rscreen, vgt);
	case R600_QUERY_GPU_IA_BUSY:
		return BUSY_INDEX(rscreen, ia);
	case R600_QUERY_GPU_SX_BUSY:
		return BUSY_INDEX(rscreen, sx);
	case R600_QUERY_GPU_WD_BUSY:
		return BUSY_INDEX(rscreen, wd);
	case R600_QUERY_GPU_BCI_BUSY:
		return BUSY_INDEX(rscreen, bci);
	case R600_QUERY_GPU_SC_BUSY:
		return BUSY_INDEX(rscreen, sc);
	case R600_QUERY_GPU_PA_BUSY:
		return BUSY_INDEX(rscreen, pa);
	case R600_QUERY_GPU_DB_BUSY:
		return BUSY_INDEX(rscreen, db);
	case R600_QUERY_GPU_CP_BUSY:
		return BUSY_INDEX(rscreen, cp);
	case R600_QUERY_GPU_CB_BUSY:
		return BUSY_INDEX(rscreen, cb);
	case R600_QUERY_GPU_SDMA_BUSY:
		return BUSY_INDEX(rscreen, sdma);
	case R600_QUERY_GPU_PFP_BUSY:
		return BUSY_INDEX(rscreen, pfp);
	case R600_QUERY_GPU_MEQ_BUSY:
		return BUSY_INDEX(rscreen, meq);
	case R600_QUERY_GPU_ME_BUSY:
		return BUSY_INDEX(rscreen, me);
	case R600_QUERY_GPU_SURF_SYNC_BUSY:
		return BUSY_INDEX(rscreen, surf_sync);
	case R600_QUERY_GPU_DMA_BUSY:
		return BUSY_INDEX(rscreen, dma);
	case R600_QUERY_GPU_SCRATCH_RAM_BUSY:
		return BUSY_INDEX(rscreen, scratch_ram);
	case R600_QUERY_GPU_CE_BUSY:
		return BUSY_INDEX(rscreen, ce);
	default:
		unreachable("invalid query type");
	}
}

uint64_t r600_begin_counter(struct r600_common_screen *rscreen, unsigned type)
{
	unsigned busy_index = busy_index_from_type(rscreen, type);
	return r600_read_mmio_counter(rscreen, busy_index);
}

unsigned r600_end_counter(struct r600_common_screen *rscreen, unsigned type,
			  uint64_t begin)
{
	unsigned busy_index = busy_index_from_type(rscreen, type);
	return r600_end_mmio_counter(rscreen, begin, busy_index);
@

