head	1.2;
access;
symbols
	OPENBSD_6_2:1.2.0.2
	OPENBSD_6_2_BASE:1.2
	mesa-17_1_6:1.1.1.3
	OPENBSD_6_1:1.1.1.2.0.2
	OPENBSD_6_1_BASE:1.1.1.2
	mesa-13_0_6:1.1.1.2
	mesa-13_0_5:1.1.1.2
	mesa-13_0_3:1.1.1.2
	mesa-13_0_2:1.1.1.2
	OPENBSD_6_0:1.1.1.1.0.4
	OPENBSD_6_0_BASE:1.1.1.1
	mesa-11_2_2:1.1.1.1
	mesa:1.1.1;
locks; strict;
comment	@ * @;
expand	@o@;


1.2
date	2017.08.26.16.59.23;	author jsg;	state Exp;
branches;
next	1.1;
commitid	D0k2io1oY8gcsQ2S;

1.1
date	2016.05.29.10.18.22;	author jsg;	state Exp;
branches
	1.1.1.1;
next	;
commitid	OwGfrJACrYJkCVJ4;

1.1.1.1
date	2016.05.29.10.18.22;	author jsg;	state Exp;
branches;
next	1.1.1.2;
commitid	OwGfrJACrYJkCVJ4;

1.1.1.2
date	2016.12.11.08.32.31;	author jsg;	state Exp;
branches;
next	1.1.1.3;
commitid	uuv5VTS15jglEDZU;

1.1.1.3
date	2017.08.14.09.37.10;	author jsg;	state Exp;
branches;
next	;
commitid	enNyoMGkcgwM3Ww6;


desc
@@


1.2
log
@Revert to Mesa 13.0.6 to hopefully address rendering issues a handful of
people have reported with xpdf/fvwm on ivy bridge with modesetting driver.
@
text
@/**************************************************************************
 *
 * Copyright 2007-2008 VMware, Inc.
 * Copyright 2015 Advanced Micro Devices, Inc.
 * All Rights Reserved.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sub license, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice (including the
 * next paragraph) shall be included in all copies or substantial portions
 * of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
 * IN NO EVENT SHALL AUTHORS AND/OR ITS SUPPLIERS BE LIABLE FOR
 * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *
 **************************************************************************/

#include "pb_cache.h"
#include "util/u_memory.h"
#include "util/u_time.h"


/**
 * Actually destroy the buffer.
 */
static void
destroy_buffer_locked(struct pb_cache_entry *entry)
{
   struct pb_cache *mgr = entry->mgr;
   struct pb_buffer *buf = entry->buffer;

   assert(!pipe_is_referenced(&buf->reference));
   if (entry->head.next) {
      LIST_DEL(&entry->head);
      assert(mgr->num_buffers);
      --mgr->num_buffers;
      mgr->cache_size -= buf->size;
   }
   mgr->destroy_buffer(buf);
}

/**
 * Free as many cache buffers from the list head as possible.
 */
static void
release_expired_buffers_locked(struct list_head *cache)
{
   struct list_head *curr, *next;
   struct pb_cache_entry *entry;
   int64_t now;

   now = os_time_get();

   curr = cache->next;
   next = curr->next;
   while (curr != cache) {
      entry = LIST_ENTRY(struct pb_cache_entry, curr, head);

      if (!os_time_timeout(entry->start, entry->end, now))
         break;

      destroy_buffer_locked(entry);

      curr = next;
      next = curr->next;
   }
}

/**
 * Add a buffer to the cache. This is typically done when the buffer is
 * being released.
 */
void
pb_cache_add_buffer(struct pb_cache_entry *entry)
{
   struct pb_cache *mgr = entry->mgr;
   struct list_head *cache = &mgr->buckets[entry->bucket_index];
   struct pb_buffer *buf = entry->buffer;
   unsigned i;

   pipe_mutex_lock(mgr->mutex);
   assert(!pipe_is_referenced(&buf->reference));

   for (i = 0; i < ARRAY_SIZE(mgr->buckets); i++)
      release_expired_buffers_locked(&mgr->buckets[i]);

   /* Directly release any buffer that exceeds the limit. */
   if (mgr->cache_size + buf->size > mgr->max_cache_size) {
      mgr->destroy_buffer(buf);
      pipe_mutex_unlock(mgr->mutex);
      return;
   }

   entry->start = os_time_get();
   entry->end = entry->start + mgr->usecs;
   LIST_ADDTAIL(&entry->head, cache);
   ++mgr->num_buffers;
   mgr->cache_size += buf->size;
   pipe_mutex_unlock(mgr->mutex);
}

/**
 * \return 1   if compatible and can be reclaimed
 *         0   if incompatible
 *        -1   if compatible and can't be reclaimed
 */
static int
pb_cache_is_buffer_compat(struct pb_cache_entry *entry,
                          pb_size size, unsigned alignment, unsigned usage)
{
   struct pb_cache *mgr = entry->mgr;
   struct pb_buffer *buf = entry->buffer;

   if (!pb_check_usage(usage, buf->usage))
      return 0;

   /* be lenient with size */
   if (buf->size < size ||
       buf->size > (unsigned) (mgr->size_factor * size))
      return 0;

   if (usage & mgr->bypass_usage)
      return 0;

   if (!pb_check_alignment(alignment, buf->alignment))
      return 0;

   return mgr->can_reclaim(buf) ? 1 : -1;
}

/**
 * Find a compatible buffer in the cache, return it, and remove it
 * from the cache.
 */
struct pb_buffer *
pb_cache_reclaim_buffer(struct pb_cache *mgr, pb_size size,
                        unsigned alignment, unsigned usage,
                        unsigned bucket_index)
{
   struct pb_cache_entry *entry;
   struct pb_cache_entry *cur_entry;
   struct list_head *cur, *next;
   int64_t now;
   int ret = 0;
   struct list_head *cache = &mgr->buckets[bucket_index];

   pipe_mutex_lock(mgr->mutex);

   entry = NULL;
   cur = cache->next;
   next = cur->next;

   /* search in the expired buffers, freeing them in the process */
   now = os_time_get();
   while (cur != cache) {
      cur_entry = LIST_ENTRY(struct pb_cache_entry, cur, head);

      if (!entry && (ret = pb_cache_is_buffer_compat(cur_entry, size,
                                                     alignment, usage) > 0))
         entry = cur_entry;
      else if (os_time_timeout(cur_entry->start, cur_entry->end, now))
         destroy_buffer_locked(cur_entry);
      else
         /* This buffer (and all hereafter) are still hot in cache */
         break;

      /* the buffer is busy (and probably all remaining ones too) */
      if (ret == -1)
         break;

      cur = next;
      next = cur->next;
   }

   /* keep searching in the hot buffers */
   if (!entry && ret != -1) {
      while (cur != cache) {
         cur_entry = LIST_ENTRY(struct pb_cache_entry, cur, head);
         ret = pb_cache_is_buffer_compat(cur_entry, size, alignment, usage);

         if (ret > 0) {
            entry = cur_entry;
            break;
         }
         if (ret == -1)
            break;
         /* no need to check the timeout here */
         cur = next;
         next = cur->next;
      }
   }

   /* found a compatible buffer, return it */
   if (entry) {
      struct pb_buffer *buf = entry->buffer;

      mgr->cache_size -= buf->size;
      LIST_DEL(&entry->head);
      --mgr->num_buffers;
      pipe_mutex_unlock(mgr->mutex);
      /* Increase refcount */
      pipe_reference_init(&buf->reference, 1);
      return buf;
   }

   pipe_mutex_unlock(mgr->mutex);
   return NULL;
}

/**
 * Empty the cache. Useful when there is not enough memory.
 */
void
pb_cache_release_all_buffers(struct pb_cache *mgr)
{
   struct list_head *curr, *next;
   struct pb_cache_entry *buf;
   unsigned i;

   pipe_mutex_lock(mgr->mutex);
   for (i = 0; i < ARRAY_SIZE(mgr->buckets); i++) {
      struct list_head *cache = &mgr->buckets[i];

      curr = cache->next;
      next = curr->next;
      while (curr != cache) {
         buf = LIST_ENTRY(struct pb_cache_entry, curr, head);
         destroy_buffer_locked(buf);
         curr = next;
         next = curr->next;
      }
   }
   pipe_mutex_unlock(mgr->mutex);
}

void
pb_cache_init_entry(struct pb_cache *mgr, struct pb_cache_entry *entry,
                    struct pb_buffer *buf, unsigned bucket_index)
{
   memset(entry, 0, sizeof(*entry));
   entry->buffer = buf;
   entry->mgr = mgr;
   entry->bucket_index = bucket_index;
}

/**
 * Initialize a caching buffer manager.
 *
 * @@param mgr     The cache buffer manager
 * @@param usecs   Unused buffers may be released from the cache after this
 *                time
 * @@param size_factor  Declare buffers that are size_factor times bigger than
 *                     the requested size as cache hits.
 * @@param bypass_usage  Bitmask. If (requested usage & bypass_usage) != 0,
 *                      buffer allocation requests are rejected.
 * @@param maximum_cache_size  Maximum size of all unused buffers the cache can
 *                            hold.
 * @@param destroy_buffer  Function that destroys a buffer for good.
 * @@param can_reclaim     Whether a buffer can be reclaimed (e.g. is not busy)
 */
void
pb_cache_init(struct pb_cache *mgr, uint usecs, float size_factor,
              unsigned bypass_usage, uint64_t maximum_cache_size,
              void (*destroy_buffer)(struct pb_buffer *buf),
              bool (*can_reclaim)(struct pb_buffer *buf))
{
   unsigned i;

   for (i = 0; i < ARRAY_SIZE(mgr->buckets); i++)
      LIST_INITHEAD(&mgr->buckets[i]);

   pipe_mutex_init(mgr->mutex);
   mgr->cache_size = 0;
   mgr->max_cache_size = maximum_cache_size;
   mgr->usecs = usecs;
   mgr->num_buffers = 0;
   mgr->bypass_usage = bypass_usage;
   mgr->size_factor = size_factor;
   mgr->destroy_buffer = destroy_buffer;
   mgr->can_reclaim = can_reclaim;
}

/**
 * Deinitialize the manager completely.
 */
void
pb_cache_deinit(struct pb_cache *mgr)
{
   pb_cache_release_all_buffers(mgr);
   pipe_mutex_destroy(mgr->mutex);
}
@


1.1
log
@Initial revision
@
text
@d41 1
d43 1
a43 1
   assert(!pipe_is_referenced(&entry->buffer->reference));
d48 1
a48 1
      mgr->cache_size -= entry->buffer->size;
d50 1
a50 1
   entry->mgr->destroy_buffer(entry->buffer);
d57 1
a57 1
release_expired_buffers_locked(struct pb_cache *mgr)
d65 1
a65 1
   curr = mgr->cache.next;
d67 1
a67 1
   while (curr != &mgr->cache) {
d88 3
d93 1
a93 1
   assert(!pipe_is_referenced(&entry->buffer->reference));
d95 2
a96 1
   release_expired_buffers_locked(mgr);
d99 2
a100 2
   if (mgr->cache_size + entry->buffer->size > mgr->max_cache_size) {
      entry->mgr->destroy_buffer(entry->buffer);
d107 1
a107 1
   LIST_ADDTAIL(&entry->head, &mgr->cache);
d109 1
a109 1
   mgr->cache_size += entry->buffer->size;
d122 1
d125 1
a125 1
   if (usage & entry->mgr->bypass_usage)
d128 3
a130 1
   if (buf->size < size)
d133 1
a133 2
   /* be lenient with size */
   if (buf->size > (unsigned) (entry->mgr->size_factor * size))
d139 1
a139 4
   if (!pb_check_usage(usage, buf->usage))
      return 0;

   return entry->mgr->can_reclaim(buf) ? 1 : -1;
d148 2
a149 1
                        unsigned alignment, unsigned usage)
d156 1
d161 1
a161 1
   cur = mgr->cache.next;
d166 1
a166 1
   while (cur != &mgr->cache) {
d188 1
a188 1
      while (cur != &mgr->cache) {
d229 1
d232 4
a235 6
   curr = mgr->cache.next;
   next = curr->next;
   while (curr != &mgr->cache) {
      buf = LIST_ENTRY(struct pb_cache_entry, curr, head);
      destroy_buffer_locked(buf);
      curr = next;
d237 6
d249 1
a249 1
                    struct pb_buffer *buf)
d254 1
d278 5
a282 1
   LIST_INITHEAD(&mgr->cache);
@


1.1.1.1
log
@Import Mesa 11.2.2
@
text
@@


1.1.1.2
log
@Import Mesa 13.0.2
@
text
@a40 1
   struct pb_buffer *buf = entry->buffer;
d42 1
a42 1
   assert(!pipe_is_referenced(&buf->reference));
d47 1
a47 1
      mgr->cache_size -= buf->size;
d49 1
a49 1
   mgr->destroy_buffer(buf);
d56 1
a56 1
release_expired_buffers_locked(struct list_head *cache)
d64 1
a64 1
   curr = cache->next;
d66 1
a66 1
   while (curr != cache) {
a86 3
   struct list_head *cache = &mgr->buckets[entry->bucket_index];
   struct pb_buffer *buf = entry->buffer;
   unsigned i;
d89 1
a89 1
   assert(!pipe_is_referenced(&buf->reference));
d91 1
a91 2
   for (i = 0; i < ARRAY_SIZE(mgr->buckets); i++)
      release_expired_buffers_locked(&mgr->buckets[i]);
d94 2
a95 2
   if (mgr->cache_size + buf->size > mgr->max_cache_size) {
      mgr->destroy_buffer(buf);
d102 1
a102 1
   LIST_ADDTAIL(&entry->head, cache);
d104 1
a104 1
   mgr->cache_size += buf->size;
a116 1
   struct pb_cache *mgr = entry->mgr;
d119 4
a122 1
   if (!pb_check_usage(usage, buf->usage))
d126 1
a126 2
   if (buf->size < size ||
       buf->size > (unsigned) (mgr->size_factor * size))
d129 1
a129 1
   if (usage & mgr->bypass_usage)
d132 1
a132 1
   if (!pb_check_alignment(alignment, buf->alignment))
d135 1
a135 1
   return mgr->can_reclaim(buf) ? 1 : -1;
d144 1
a144 2
                        unsigned alignment, unsigned usage,
                        unsigned bucket_index)
a150 1
   struct list_head *cache = &mgr->buckets[bucket_index];
d155 1
a155 1
   cur = cache->next;
d160 1
a160 1
   while (cur != cache) {
d182 1
a182 1
      while (cur != cache) {
a222 1
   unsigned i;
d225 6
a230 4
   for (i = 0; i < ARRAY_SIZE(mgr->buckets); i++) {
      struct list_head *cache = &mgr->buckets[i];

      curr = cache->next;
a231 6
      while (curr != cache) {
         buf = LIST_ENTRY(struct pb_cache_entry, curr, head);
         destroy_buffer_locked(buf);
         curr = next;
         next = curr->next;
      }
d238 1
a238 1
                    struct pb_buffer *buf, unsigned bucket_index)
a242 1
   entry->bucket_index = bucket_index;
d266 1
a266 5
   unsigned i;

   for (i = 0; i < ARRAY_SIZE(mgr->buckets); i++)
      LIST_INITHEAD(&mgr->buckets[i]);

@


1.1.1.3
log
@Import Mesa 17.1.6
@
text
@d92 1
a92 1
   mtx_lock(&mgr->mutex);
d101 1
a101 1
      mtx_unlock(&mgr->mutex);
d110 1
a110 1
   mtx_unlock(&mgr->mutex);
d158 1
a158 1
   mtx_lock(&mgr->mutex);
d211 1
a211 1
      mtx_unlock(&mgr->mutex);
d217 1
a217 1
   mtx_unlock(&mgr->mutex);
d231 1
a231 1
   mtx_lock(&mgr->mutex);
d244 1
a244 1
   mtx_unlock(&mgr->mutex);
d283 1
a283 1
   (void) mtx_init(&mgr->mutex, mtx_plain);
d301 1
a301 1
   mtx_destroy(&mgr->mutex);
@


