head	1.2;
access;
symbols
	mesa-17_1_6:1.1.1.4
	OPENBSD_6_1:1.1.1.3.0.2
	OPENBSD_6_1_BASE:1.1.1.3
	mesa-13_0_6:1.1.1.3
	mesa-13_0_5:1.1.1.3
	mesa-13_0_3:1.1.1.3
	mesa-13_0_2:1.1.1.3
	OPENBSD_6_0:1.1.1.2.0.4
	OPENBSD_6_0_BASE:1.1.1.2
	mesa-11_2_2:1.1.1.2
	OPENBSD_5_9:1.1.1.1.0.2
	OPENBSD_5_9_BASE:1.1.1.1
	mesa-11_0_9:1.1.1.1
	mesa-11_0_8:1.1.1.1
	mesa-11_0_6:1.1.1.1
	mesa:1.1.1;
locks; strict;
comment	@ * @;


1.2
date	2017.08.26.16.59.31;	author jsg;	state Exp;
branches;
next	1.1;
commitid	D0k2io1oY8gcsQ2S;

1.1
date	2015.11.22.02.41.41;	author jsg;	state Exp;
branches
	1.1.1.1;
next	;
commitid	bJUptkbooQfJPk5r;

1.1.1.1
date	2015.11.22.02.41.41;	author jsg;	state Exp;
branches;
next	1.1.1.2;
commitid	bJUptkbooQfJPk5r;

1.1.1.2
date	2016.05.29.10.17.08;	author jsg;	state Exp;
branches;
next	1.1.1.3;
commitid	OwGfrJACrYJkCVJ4;

1.1.1.3
date	2016.12.11.08.31.22;	author jsg;	state Exp;
branches;
next	1.1.1.4;
commitid	uuv5VTS15jglEDZU;

1.1.1.4
date	2017.08.14.09.36.02;	author jsg;	state Exp;
branches;
next	;
commitid	enNyoMGkcgwM3Ww6;


desc
@@


1.2
log
@Revert to Mesa 13.0.6 to hopefully address rendering issues a handful of
people have reported with xpdf/fvwm on ivy bridge with modesetting driver.
@
text
@/*
 * Copyright © 2011 Marek Olšák <maraeo@@gmail.com>
 * Copyright © 2015 Advanced Micro Devices, Inc.
 * All Rights Reserved.
 *
 * Permission is hereby granted, free of charge, to any person obtaining
 * a copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sub license, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
 * OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
 * NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS, AUTHORS
 * AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
 * USE OR OTHER DEALINGS IN THE SOFTWARE.
 *
 * The above copyright notice and this permission notice (including the
 * next paragraph) shall be included in all copies or substantial portions
 * of the Software.
 */
/*
 * Authors:
 *      Marek Olšák <maraeo@@gmail.com>
 */

#ifndef AMDGPU_CS_H
#define AMDGPU_CS_H

#include "amdgpu_bo.h"
#include "util/u_memory.h"

struct amdgpu_ctx {
   struct amdgpu_winsys *ws;
   amdgpu_context_handle ctx;
   amdgpu_bo_handle user_fence_bo;
   uint64_t *user_fence_cpu_address_base;
   int refcount;
};

struct amdgpu_cs_buffer {
   struct amdgpu_winsys_bo *bo;
   union {
      struct {
         uint64_t priority_usage;
      } real;
      struct {
         uint32_t real_idx; /* index of underlying real BO */
      } slab;
   } u;
   enum radeon_bo_usage usage;
};

enum ib_type {
   IB_CONST_PREAMBLE = 0,
   IB_CONST = 1, /* the const IB must be first */
   IB_MAIN = 2,
   IB_NUM
};

struct amdgpu_ib {
   struct radeon_winsys_cs base;

   /* A buffer out of which new IBs are allocated. */
   struct pb_buffer        *big_ib_buffer;
   uint8_t                 *ib_mapped;
   unsigned                used_ib_space;
   unsigned                max_ib_size;
   uint32_t                *ptr_ib_size;
   enum ib_type            ib_type;
};

struct amdgpu_cs_context {
   struct amdgpu_cs_request    request;
   struct amdgpu_cs_ib_info    ib[IB_NUM];

   /* Buffers. */
   unsigned                    max_real_buffers;
   unsigned                    num_real_buffers;
   amdgpu_bo_handle            *handles;
   uint8_t                     *flags;
   struct amdgpu_cs_buffer     *real_buffers;

   unsigned                    num_slab_buffers;
   unsigned                    max_slab_buffers;
   struct amdgpu_cs_buffer     *slab_buffers;

   int                         buffer_indices_hashlist[4096];

   unsigned                    max_dependencies;

   struct pipe_fence_handle    *fence;

   /* the error returned from cs_flush for non-async submissions */
   int                         error_code;
};

struct amdgpu_cs {
   struct amdgpu_ib main; /* must be first because this is inherited */
   struct amdgpu_ib const_ib; /* optional constant engine IB */
   struct amdgpu_ib const_preamble_ib;
   struct amdgpu_ctx *ctx;
   enum ring_type ring_type;

   /* We flip between these two CS. While one is being consumed
    * by the kernel in another thread, the other one is being filled
    * by the pipe driver. */
   struct amdgpu_cs_context csc1;
   struct amdgpu_cs_context csc2;
   /* The currently-used CS. */
   struct amdgpu_cs_context *csc;
   /* The CS being currently-owned by the other thread. */
   struct amdgpu_cs_context *cst;

   /* Flush CS. */
   void (*flush_cs)(void *ctx, unsigned flags, struct pipe_fence_handle **fence);
   void *flush_data;

   struct util_queue_fence flush_completed;
   struct pipe_fence_handle *next_fence;
};

struct amdgpu_fence {
   struct pipe_reference reference;

   struct amdgpu_ctx *ctx;  /* submission context */
   struct amdgpu_cs_fence fence;
   uint64_t *user_fence_cpu_address;

   /* If the fence is unknown due to an IB still being submitted
    * in the other thread. */
   volatile int submission_in_progress; /* bool (int for atomicity) */
   volatile int signalled;              /* bool (int for atomicity) */
};

static inline void amdgpu_ctx_unref(struct amdgpu_ctx *ctx)
{
   if (p_atomic_dec_zero(&ctx->refcount)) {
      amdgpu_cs_ctx_free(ctx->ctx);
      amdgpu_bo_free(ctx->user_fence_bo);
      FREE(ctx);
   }
}

static inline void amdgpu_fence_reference(struct pipe_fence_handle **dst,
                                          struct pipe_fence_handle *src)
{
   struct amdgpu_fence **rdst = (struct amdgpu_fence **)dst;
   struct amdgpu_fence *rsrc = (struct amdgpu_fence *)src;

   if (pipe_reference(&(*rdst)->reference, &rsrc->reference)) {
      amdgpu_ctx_unref((*rdst)->ctx);
      FREE(*rdst);
   }
   *rdst = rsrc;
}

int amdgpu_lookup_buffer(struct amdgpu_cs_context *cs, struct amdgpu_winsys_bo *bo);

static inline struct amdgpu_ib *
amdgpu_ib(struct radeon_winsys_cs *base)
{
   return (struct amdgpu_ib *)base;
}

static inline struct amdgpu_cs *
amdgpu_cs(struct radeon_winsys_cs *base)
{
   assert(amdgpu_ib(base)->ib_type == IB_MAIN);
   return (struct amdgpu_cs*)base;
}

#define get_container(member_ptr, container_type, container_member) \
   (container_type *)((char *)(member_ptr) - offsetof(container_type, container_member))

static inline struct amdgpu_cs *
amdgpu_cs_from_ib(struct amdgpu_ib *ib)
{
   switch (ib->ib_type) {
   case IB_MAIN:
      return get_container(ib, struct amdgpu_cs, main);
   case IB_CONST:
      return get_container(ib, struct amdgpu_cs, const_ib);
   case IB_CONST_PREAMBLE:
      return get_container(ib, struct amdgpu_cs, const_preamble_ib);
   default:
      unreachable("bad ib_type");
   }
}

static inline bool
amdgpu_bo_is_referenced_by_cs(struct amdgpu_cs *cs,
                              struct amdgpu_winsys_bo *bo)
{
   int num_refs = bo->num_cs_references;
   return num_refs == bo->ws->num_cs ||
         (num_refs && amdgpu_lookup_buffer(cs->csc, bo) != -1);
}

static inline bool
amdgpu_bo_is_referenced_by_cs_with_usage(struct amdgpu_cs *cs,
                                         struct amdgpu_winsys_bo *bo,
                                         enum radeon_bo_usage usage)
{
   int index;
   struct amdgpu_cs_buffer *buffer;

   if (!bo->num_cs_references)
      return false;

   index = amdgpu_lookup_buffer(cs->csc, bo);
   if (index == -1)
      return false;

   buffer = bo->bo ? &cs->csc->real_buffers[index]
                   : &cs->csc->slab_buffers[index];

   return (buffer->usage & usage) != 0;
}

static inline bool
amdgpu_bo_is_referenced_by_any_cs(struct amdgpu_winsys_bo *bo)
{
   return bo->num_cs_references != 0;
}

bool amdgpu_fence_wait(struct pipe_fence_handle *fence, uint64_t timeout,
                       bool absolute);
void amdgpu_cs_sync_flush(struct radeon_winsys_cs *rcs);
void amdgpu_cs_init_functions(struct amdgpu_winsys *ws);
void amdgpu_cs_submit_ib(void *job, int thread_index);

#endif
@


1.1
log
@Initial revision
@
text
@d48 8
a56 1
   enum radeon_bo_domain domains;
d59 6
d66 1
a66 1
struct amdgpu_cs {
a67 5
   struct amdgpu_ctx *ctx;

   /* Flush CS. */
   void (*flush_cs)(void *ctx, unsigned flags, struct pipe_fence_handle **fence);
   void *flush_data;
d70 7
a76 4
   struct pb_buffer *big_ib_buffer; /* for holding the reference */
   struct amdgpu_winsys_bo *big_ib_winsys_buffer;
   uint8_t *ib_mapped;
   unsigned used_ib_space;
d78 1
a78 1
   /* amdgpu_cs_submit parameters */
d80 1
a80 1
   struct amdgpu_cs_ib_info    ib;
d82 3
a84 3
   /* Relocs. */
   unsigned                    max_num_buffers;
   unsigned                    num_buffers;
d87 1
a87 1
   struct amdgpu_cs_buffer     *buffers;
d89 3
a91 1
   int                         buffer_indices_hashlist[512];
d93 1
a93 2
   uint64_t                    used_vram;
   uint64_t                    used_gart;
d96 30
d135 3
d163 7
a169 1
int amdgpu_get_reloc(struct amdgpu_cs *csc, struct amdgpu_winsys_bo *bo);
d174 1
d178 19
a196 1
static inline boolean
d201 2
a202 2
   return num_refs == bo->rws->num_cs ||
         (num_refs && amdgpu_get_reloc(cs, bo) != -1);
d205 1
a205 1
static inline boolean
d211 1
d214 1
a214 1
      return FALSE;
d216 1
a216 1
   index = amdgpu_get_reloc(cs, bo);
d218 4
a221 1
      return FALSE;
d223 1
a223 1
   return (cs->buffers[index].usage & usage) != 0;
d226 1
a226 1
static inline boolean
d234 1
d236 1
@


1.1.1.1
log
@import Mesa 11.0.6
@
text
@@


1.1.1.2
log
@Import Mesa 11.2.2
@
text
@a47 1
   uint64_t priority_usage;
d71 1
a71 1
   /* Buffers. */
d118 1
a118 1
int amdgpu_lookup_buffer(struct amdgpu_cs *csc, struct amdgpu_winsys_bo *bo);
d131 2
a132 2
   return num_refs == bo->ws->num_cs ||
         (num_refs && amdgpu_lookup_buffer(cs, bo) != -1);
d145 1
a145 1
   index = amdgpu_lookup_buffer(cs, bo);
@


1.1.1.3
log
@Import Mesa 13.0.2
@
text
@d48 1
a48 8
   union {
      struct {
         uint64_t priority_usage;
      } real;
      struct {
         uint32_t real_idx; /* index of underlying real BO */
      } slab;
   } u;
d50 1
a52 6
enum ib_type {
   IB_CONST_PREAMBLE = 0,
   IB_CONST = 1, /* the const IB must be first */
   IB_MAIN = 2,
   IB_NUM
};
d54 1
a54 1
struct amdgpu_ib {
d56 5
d63 4
a66 7
   struct pb_buffer        *big_ib_buffer;
   uint8_t                 *ib_mapped;
   unsigned                used_ib_space;
   unsigned                max_ib_size;
   uint32_t                *ptr_ib_size;
   enum ib_type            ib_type;
};
d68 1
a68 1
struct amdgpu_cs_context {
d70 1
a70 1
   struct amdgpu_cs_ib_info    ib[IB_NUM];
d73 2
a74 2
   unsigned                    max_real_buffers;
   unsigned                    num_real_buffers;
d77 1
a77 1
   struct amdgpu_cs_buffer     *real_buffers;
d79 1
a79 3
   unsigned                    num_slab_buffers;
   unsigned                    max_slab_buffers;
   struct amdgpu_cs_buffer     *slab_buffers;
d81 2
a82 1
   int                         buffer_indices_hashlist[4096];
a84 30

   struct pipe_fence_handle    *fence;

   /* the error returned from cs_flush for non-async submissions */
   int                         error_code;
};

struct amdgpu_cs {
   struct amdgpu_ib main; /* must be first because this is inherited */
   struct amdgpu_ib const_ib; /* optional constant engine IB */
   struct amdgpu_ib const_preamble_ib;
   struct amdgpu_ctx *ctx;
   enum ring_type ring_type;

   /* We flip between these two CS. While one is being consumed
    * by the kernel in another thread, the other one is being filled
    * by the pipe driver. */
   struct amdgpu_cs_context csc1;
   struct amdgpu_cs_context csc2;
   /* The currently-used CS. */
   struct amdgpu_cs_context *csc;
   /* The CS being currently-owned by the other thread. */
   struct amdgpu_cs_context *cst;

   /* Flush CS. */
   void (*flush_cs)(void *ctx, unsigned flags, struct pipe_fence_handle **fence);
   void *flush_data;

   struct util_queue_fence flush_completed;
   struct pipe_fence_handle *next_fence;
a93 3
   /* If the fence is unknown due to an IB still being submitted
    * in the other thread. */
   volatile int submission_in_progress; /* bool (int for atomicity) */
d119 1
a119 7
int amdgpu_lookup_buffer(struct amdgpu_cs_context *cs, struct amdgpu_winsys_bo *bo);

static inline struct amdgpu_ib *
amdgpu_ib(struct radeon_winsys_cs *base)
{
   return (struct amdgpu_ib *)base;
}
a123 1
   assert(amdgpu_ib(base)->ib_type == IB_MAIN);
d127 1
a127 19
#define get_container(member_ptr, container_type, container_member) \
   (container_type *)((char *)(member_ptr) - offsetof(container_type, container_member))

static inline struct amdgpu_cs *
amdgpu_cs_from_ib(struct amdgpu_ib *ib)
{
   switch (ib->ib_type) {
   case IB_MAIN:
      return get_container(ib, struct amdgpu_cs, main);
   case IB_CONST:
      return get_container(ib, struct amdgpu_cs, const_ib);
   case IB_CONST_PREAMBLE:
      return get_container(ib, struct amdgpu_cs, const_preamble_ib);
   default:
      unreachable("bad ib_type");
   }
}

static inline bool
d133 1
a133 1
         (num_refs && amdgpu_lookup_buffer(cs->csc, bo) != -1);
d136 1
a136 1
static inline bool
a141 1
   struct amdgpu_cs_buffer *buffer;
d144 1
a144 1
      return false;
d146 1
a146 1
   index = amdgpu_lookup_buffer(cs->csc, bo);
d148 1
a148 4
      return false;

   buffer = bo->bo ? &cs->csc->real_buffers[index]
                   : &cs->csc->slab_buffers[index];
d150 1
a150 1
   return (buffer->usage & usage) != 0;
d153 1
a153 1
static inline bool
a160 1
void amdgpu_cs_sync_flush(struct radeon_winsys_cs *rcs);
a161 1
void amdgpu_cs_submit_ib(void *job, int thread_index);
@


1.1.1.4
log
@Import Mesa 17.1.6
@
text
@a43 2
   unsigned initial_num_total_rejected_cs;
   unsigned num_rejected_cs;
a84 3
   struct amdgpu_cs_buffer     *real_buffers;

   unsigned                    max_real_submit;
d87 1
a92 4
   unsigned                    num_sparse_buffers;
   unsigned                    max_sparse_buffers;
   struct amdgpu_cs_buffer     *sparse_buffers;

d95 1
a95 8
   struct amdgpu_winsys_bo     *last_added_bo;
   unsigned                    last_added_bo_index;
   unsigned                    last_added_bo_usage;
   uint64_t                    last_added_bo_priority_usage;

   struct pipe_fence_handle    **fence_dependencies;
   unsigned                    num_fence_dependencies;
   unsigned                    max_fence_dependencies;
d220 2
a221 3
   buffer = bo->bo ? &cs->csc->real_buffers[index] :
            bo->sparse ? &cs->csc->sparse_buffers[index] :
            &cs->csc->slab_buffers[index];
a233 3
void amdgpu_add_fences(struct amdgpu_winsys_bo *bo,
                       unsigned num_fences,
                       struct pipe_fence_handle **fences);
@


