head	1.12;
access;
symbols
	OPENBSD_6_0:1.11.0.12
	OPENBSD_6_0_BASE:1.11
	OPENBSD_5_9:1.11.0.10
	OPENBSD_5_9_BASE:1.11
	OPENBSD_5_8:1.11.0.8
	OPENBSD_5_8_BASE:1.11
	OPENBSD_5_7:1.11.0.6
	OPENBSD_5_7_BASE:1.11
	OPENBSD_5_6:1.11.0.4
	OPENBSD_5_6_BASE:1.11
	OPENBSD_5_5:1.11.0.2
	OPENBSD_5_5_BASE:1.11
	OPENBSD_5_4:1.10.0.2
	OPENBSD_5_4_BASE:1.10
	OPENBSD_5_3:1.9.0.2
	OPENBSD_5_3_BASE:1.9
	OPENBSD_5_2:1.7.0.2
	OPENBSD_5_2_BASE:1.7
	OPENBSD_5_1_BASE:1.6
	OPENBSD_5_1:1.6.0.4
	OPENBSD_5_0:1.6.0.2
	OPENBSD_5_0_BASE:1.6
	OPENBSD_4_9:1.5.0.2
	OPENBSD_4_9_BASE:1.5
	OPENBSD_4_8:1.3.0.4
	OPENBSD_4_8_BASE:1.3
	OPENBSD_4_7:1.2.0.4
	OPENBSD_4_7_BASE:1.2
	OPENBSD_4_6:1.2.0.2
	OPENBSD_4_6_BASE:1.2
	OPENBSD_4_5:1.1.0.2
	OPENBSD_4_5_BASE:1.1;
locks; strict;
comment	@ * @;


1.12
date	2016.10.01.10.17.44;	author matthieu;	state Exp;
branches;
next	1.11;
commitid	FGr8CFhVerRlpSoE;

1.11
date	2013.12.01.20.34.20;	author matthieu;	state Exp;
branches;
next	1.10;

1.10
date	2013.06.07.17.18.01;	author matthieu;	state Exp;
branches;
next	1.9;

1.9
date	2012.11.23.20.44.09;	author matthieu;	state Exp;
branches;
next	1.8;

1.8
date	2012.08.17.16.15.20;	author matthieu;	state Exp;
branches;
next	1.7;

1.7
date	2012.02.28.20.36.12;	author matthieu;	state Exp;
branches;
next	1.6;

1.6
date	2011.07.24.13.05.47;	author matthieu;	state Exp;
branches;
next	1.5;

1.5
date	2010.11.14.13.42.49;	author matthieu;	state Exp;
branches;
next	1.4;

1.4
date	2010.10.03.18.30.04;	author matthieu;	state Exp;
branches;
next	1.3;

1.3
date	2010.03.25.21.58.52;	author matthieu;	state Exp;
branches;
next	1.2;

1.2
date	2009.06.05.20.14.28;	author matthieu;	state Exp;
branches;
next	1.1;

1.1
date	2008.09.23.19.11.40;	author matthieu;	state Exp;
branches;
next	;


desc
@@


1.12
log
@Update to pixman 0.34.0.
@
text
@/*
 * Copyright © 2008 Rodrigo Kumpera
 * Copyright © 2008 André Tupinambá
 *
 * Permission to use, copy, modify, distribute, and sell this software and its
 * documentation for any purpose is hereby granted without fee, provided that
 * the above copyright notice appear in all copies and that both that
 * copyright notice and this permission notice appear in supporting
 * documentation, and that the name of Red Hat not be used in advertising or
 * publicity pertaining to distribution of the software without specific,
 * written prior permission.  Red Hat makes no representations about the
 * suitability of this software for any purpose.  It is provided "as is"
 * without express or implied warranty.
 *
 * THE COPYRIGHT HOLDERS DISCLAIM ALL WARRANTIES WITH REGARD TO THIS
 * SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND
 * FITNESS, IN NO EVENT SHALL THE COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
 * AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING
 * OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS
 * SOFTWARE.
 *
 * Author:  Rodrigo Kumpera (kumpera@@gmail.com)
 *          André Tupinambá (andrelrt@@gmail.com)
 *
 * Based on work by Owen Taylor and Søren Sandmann
 */
#ifdef HAVE_CONFIG_H
#include <config.h>
#endif

/* PSHUFD is slow on a lot of old processors, and new processors have SSSE3 */
#define PSHUFD_IS_FAST 0

#include <xmmintrin.h> /* for _mm_shuffle_pi16 and _MM_SHUFFLE */
#include <emmintrin.h> /* for SSE2 intrinsics */
#include "pixman-private.h"
#include "pixman-combine32.h"
#include "pixman-inlines.h"

static __m128i mask_0080;
static __m128i mask_00ff;
static __m128i mask_0101;
static __m128i mask_ffff;
static __m128i mask_ff000000;
static __m128i mask_alpha;

static __m128i mask_565_r;
static __m128i mask_565_g1, mask_565_g2;
static __m128i mask_565_b;
static __m128i mask_red;
static __m128i mask_green;
static __m128i mask_blue;

static __m128i mask_565_fix_rb;
static __m128i mask_565_fix_g;

static __m128i mask_565_rb;
static __m128i mask_565_pack_multiplier;

static force_inline __m128i
unpack_32_1x128 (uint32_t data)
{
    return _mm_unpacklo_epi8 (_mm_cvtsi32_si128 (data), _mm_setzero_si128 ());
}

static force_inline void
unpack_128_2x128 (__m128i data, __m128i* data_lo, __m128i* data_hi)
{
    *data_lo = _mm_unpacklo_epi8 (data, _mm_setzero_si128 ());
    *data_hi = _mm_unpackhi_epi8 (data, _mm_setzero_si128 ());
}

static force_inline __m128i
unpack_565_to_8888 (__m128i lo)
{
    __m128i r, g, b, rb, t;

    r = _mm_and_si128 (_mm_slli_epi32 (lo, 8), mask_red);
    g = _mm_and_si128 (_mm_slli_epi32 (lo, 5), mask_green);
    b = _mm_and_si128 (_mm_slli_epi32 (lo, 3), mask_blue);

    rb = _mm_or_si128 (r, b);
    t  = _mm_and_si128 (rb, mask_565_fix_rb);
    t  = _mm_srli_epi32 (t, 5);
    rb = _mm_or_si128 (rb, t);

    t  = _mm_and_si128 (g, mask_565_fix_g);
    t  = _mm_srli_epi32 (t, 6);
    g  = _mm_or_si128 (g, t);

    return _mm_or_si128 (rb, g);
}

static force_inline void
unpack_565_128_4x128 (__m128i  data,
                      __m128i* data0,
                      __m128i* data1,
                      __m128i* data2,
                      __m128i* data3)
{
    __m128i lo, hi;

    lo = _mm_unpacklo_epi16 (data, _mm_setzero_si128 ());
    hi = _mm_unpackhi_epi16 (data, _mm_setzero_si128 ());

    lo = unpack_565_to_8888 (lo);
    hi = unpack_565_to_8888 (hi);

    unpack_128_2x128 (lo, data0, data1);
    unpack_128_2x128 (hi, data2, data3);
}

static force_inline uint16_t
pack_565_32_16 (uint32_t pixel)
{
    return (uint16_t) (((pixel >> 8) & 0xf800) |
		       ((pixel >> 5) & 0x07e0) |
		       ((pixel >> 3) & 0x001f));
}

static force_inline __m128i
pack_2x128_128 (__m128i lo, __m128i hi)
{
    return _mm_packus_epi16 (lo, hi);
}

static force_inline __m128i
pack_565_2packedx128_128 (__m128i lo, __m128i hi)
{
    __m128i rb0 = _mm_and_si128 (lo, mask_565_rb);
    __m128i rb1 = _mm_and_si128 (hi, mask_565_rb);

    __m128i t0 = _mm_madd_epi16 (rb0, mask_565_pack_multiplier);
    __m128i t1 = _mm_madd_epi16 (rb1, mask_565_pack_multiplier);

    __m128i g0 = _mm_and_si128 (lo, mask_green);
    __m128i g1 = _mm_and_si128 (hi, mask_green);

    t0 = _mm_or_si128 (t0, g0);
    t1 = _mm_or_si128 (t1, g1);

    /* Simulates _mm_packus_epi32 */
    t0 = _mm_slli_epi32 (t0, 16 - 5);
    t1 = _mm_slli_epi32 (t1, 16 - 5);
    t0 = _mm_srai_epi32 (t0, 16);
    t1 = _mm_srai_epi32 (t1, 16);
    return _mm_packs_epi32 (t0, t1);
}

static force_inline __m128i
pack_565_2x128_128 (__m128i lo, __m128i hi)
{
    __m128i data;
    __m128i r, g1, g2, b;

    data = pack_2x128_128 (lo, hi);

    r  = _mm_and_si128 (data, mask_565_r);
    g1 = _mm_and_si128 (_mm_slli_epi32 (data, 3), mask_565_g1);
    g2 = _mm_and_si128 (_mm_srli_epi32 (data, 5), mask_565_g2);
    b  = _mm_and_si128 (_mm_srli_epi32 (data, 3), mask_565_b);

    return _mm_or_si128 (_mm_or_si128 (_mm_or_si128 (r, g1), g2), b);
}

static force_inline __m128i
pack_565_4x128_128 (__m128i* xmm0, __m128i* xmm1, __m128i* xmm2, __m128i* xmm3)
{
    return _mm_packus_epi16 (pack_565_2x128_128 (*xmm0, *xmm1),
			     pack_565_2x128_128 (*xmm2, *xmm3));
}

static force_inline int
is_opaque (__m128i x)
{
    __m128i ffs = _mm_cmpeq_epi8 (x, x);

    return (_mm_movemask_epi8 (_mm_cmpeq_epi8 (x, ffs)) & 0x8888) == 0x8888;
}

static force_inline int
is_zero (__m128i x)
{
    return _mm_movemask_epi8 (
	_mm_cmpeq_epi8 (x, _mm_setzero_si128 ())) == 0xffff;
}

static force_inline int
is_transparent (__m128i x)
{
    return (_mm_movemask_epi8 (
		_mm_cmpeq_epi8 (x, _mm_setzero_si128 ())) & 0x8888) == 0x8888;
}

static force_inline __m128i
expand_pixel_32_1x128 (uint32_t data)
{
    return _mm_shuffle_epi32 (unpack_32_1x128 (data), _MM_SHUFFLE (1, 0, 1, 0));
}

static force_inline __m128i
expand_alpha_1x128 (__m128i data)
{
    return _mm_shufflehi_epi16 (_mm_shufflelo_epi16 (data,
						     _MM_SHUFFLE (3, 3, 3, 3)),
				_MM_SHUFFLE (3, 3, 3, 3));
}

static force_inline void
expand_alpha_2x128 (__m128i  data_lo,
                    __m128i  data_hi,
                    __m128i* alpha_lo,
                    __m128i* alpha_hi)
{
    __m128i lo, hi;

    lo = _mm_shufflelo_epi16 (data_lo, _MM_SHUFFLE (3, 3, 3, 3));
    hi = _mm_shufflelo_epi16 (data_hi, _MM_SHUFFLE (3, 3, 3, 3));

    *alpha_lo = _mm_shufflehi_epi16 (lo, _MM_SHUFFLE (3, 3, 3, 3));
    *alpha_hi = _mm_shufflehi_epi16 (hi, _MM_SHUFFLE (3, 3, 3, 3));
}

static force_inline void
expand_alpha_rev_2x128 (__m128i  data_lo,
                        __m128i  data_hi,
                        __m128i* alpha_lo,
                        __m128i* alpha_hi)
{
    __m128i lo, hi;

    lo = _mm_shufflelo_epi16 (data_lo, _MM_SHUFFLE (0, 0, 0, 0));
    hi = _mm_shufflelo_epi16 (data_hi, _MM_SHUFFLE (0, 0, 0, 0));
    *alpha_lo = _mm_shufflehi_epi16 (lo, _MM_SHUFFLE (0, 0, 0, 0));
    *alpha_hi = _mm_shufflehi_epi16 (hi, _MM_SHUFFLE (0, 0, 0, 0));
}

static force_inline void
pix_multiply_2x128 (__m128i* data_lo,
                    __m128i* data_hi,
                    __m128i* alpha_lo,
                    __m128i* alpha_hi,
                    __m128i* ret_lo,
                    __m128i* ret_hi)
{
    __m128i lo, hi;

    lo = _mm_mullo_epi16 (*data_lo, *alpha_lo);
    hi = _mm_mullo_epi16 (*data_hi, *alpha_hi);
    lo = _mm_adds_epu16 (lo, mask_0080);
    hi = _mm_adds_epu16 (hi, mask_0080);
    *ret_lo = _mm_mulhi_epu16 (lo, mask_0101);
    *ret_hi = _mm_mulhi_epu16 (hi, mask_0101);
}

static force_inline void
pix_add_multiply_2x128 (__m128i* src_lo,
                        __m128i* src_hi,
                        __m128i* alpha_dst_lo,
                        __m128i* alpha_dst_hi,
                        __m128i* dst_lo,
                        __m128i* dst_hi,
                        __m128i* alpha_src_lo,
                        __m128i* alpha_src_hi,
                        __m128i* ret_lo,
                        __m128i* ret_hi)
{
    __m128i t1_lo, t1_hi;
    __m128i t2_lo, t2_hi;

    pix_multiply_2x128 (src_lo, src_hi, alpha_dst_lo, alpha_dst_hi, &t1_lo, &t1_hi);
    pix_multiply_2x128 (dst_lo, dst_hi, alpha_src_lo, alpha_src_hi, &t2_lo, &t2_hi);

    *ret_lo = _mm_adds_epu8 (t1_lo, t2_lo);
    *ret_hi = _mm_adds_epu8 (t1_hi, t2_hi);
}

static force_inline void
negate_2x128 (__m128i  data_lo,
              __m128i  data_hi,
              __m128i* neg_lo,
              __m128i* neg_hi)
{
    *neg_lo = _mm_xor_si128 (data_lo, mask_00ff);
    *neg_hi = _mm_xor_si128 (data_hi, mask_00ff);
}

static force_inline void
invert_colors_2x128 (__m128i  data_lo,
                     __m128i  data_hi,
                     __m128i* inv_lo,
                     __m128i* inv_hi)
{
    __m128i lo, hi;

    lo = _mm_shufflelo_epi16 (data_lo, _MM_SHUFFLE (3, 0, 1, 2));
    hi = _mm_shufflelo_epi16 (data_hi, _MM_SHUFFLE (3, 0, 1, 2));
    *inv_lo = _mm_shufflehi_epi16 (lo, _MM_SHUFFLE (3, 0, 1, 2));
    *inv_hi = _mm_shufflehi_epi16 (hi, _MM_SHUFFLE (3, 0, 1, 2));
}

static force_inline void
over_2x128 (__m128i* src_lo,
            __m128i* src_hi,
            __m128i* alpha_lo,
            __m128i* alpha_hi,
            __m128i* dst_lo,
            __m128i* dst_hi)
{
    __m128i t1, t2;

    negate_2x128 (*alpha_lo, *alpha_hi, &t1, &t2);

    pix_multiply_2x128 (dst_lo, dst_hi, &t1, &t2, dst_lo, dst_hi);

    *dst_lo = _mm_adds_epu8 (*src_lo, *dst_lo);
    *dst_hi = _mm_adds_epu8 (*src_hi, *dst_hi);
}

static force_inline void
over_rev_non_pre_2x128 (__m128i  src_lo,
                        __m128i  src_hi,
                        __m128i* dst_lo,
                        __m128i* dst_hi)
{
    __m128i lo, hi;
    __m128i alpha_lo, alpha_hi;

    expand_alpha_2x128 (src_lo, src_hi, &alpha_lo, &alpha_hi);

    lo = _mm_or_si128 (alpha_lo, mask_alpha);
    hi = _mm_or_si128 (alpha_hi, mask_alpha);

    invert_colors_2x128 (src_lo, src_hi, &src_lo, &src_hi);

    pix_multiply_2x128 (&src_lo, &src_hi, &lo, &hi, &lo, &hi);

    over_2x128 (&lo, &hi, &alpha_lo, &alpha_hi, dst_lo, dst_hi);
}

static force_inline void
in_over_2x128 (__m128i* src_lo,
               __m128i* src_hi,
               __m128i* alpha_lo,
               __m128i* alpha_hi,
               __m128i* mask_lo,
               __m128i* mask_hi,
               __m128i* dst_lo,
               __m128i* dst_hi)
{
    __m128i s_lo, s_hi;
    __m128i a_lo, a_hi;

    pix_multiply_2x128 (src_lo,   src_hi, mask_lo, mask_hi, &s_lo, &s_hi);
    pix_multiply_2x128 (alpha_lo, alpha_hi, mask_lo, mask_hi, &a_lo, &a_hi);

    over_2x128 (&s_lo, &s_hi, &a_lo, &a_hi, dst_lo, dst_hi);
}

/* load 4 pixels from a 16-byte boundary aligned address */
static force_inline __m128i
load_128_aligned (__m128i* src)
{
    return _mm_load_si128 (src);
}

/* load 4 pixels from a unaligned address */
static force_inline __m128i
load_128_unaligned (const __m128i* src)
{
    return _mm_loadu_si128 (src);
}

/* save 4 pixels using Write Combining memory on a 16-byte
 * boundary aligned address
 */
static force_inline void
save_128_write_combining (__m128i* dst,
                          __m128i  data)
{
    _mm_stream_si128 (dst, data);
}

/* save 4 pixels on a 16-byte boundary aligned address */
static force_inline void
save_128_aligned (__m128i* dst,
                  __m128i  data)
{
    _mm_store_si128 (dst, data);
}

/* save 4 pixels on a unaligned address */
static force_inline void
save_128_unaligned (__m128i* dst,
                    __m128i  data)
{
    _mm_storeu_si128 (dst, data);
}

static force_inline __m128i
load_32_1x128 (uint32_t data)
{
    return _mm_cvtsi32_si128 (data);
}

static force_inline __m128i
expand_alpha_rev_1x128 (__m128i data)
{
    return _mm_shufflelo_epi16 (data, _MM_SHUFFLE (0, 0, 0, 0));
}

static force_inline __m128i
expand_pixel_8_1x128 (uint8_t data)
{
    return _mm_shufflelo_epi16 (
	unpack_32_1x128 ((uint32_t)data), _MM_SHUFFLE (0, 0, 0, 0));
}

static force_inline __m128i
pix_multiply_1x128 (__m128i data,
		    __m128i alpha)
{
    return _mm_mulhi_epu16 (_mm_adds_epu16 (_mm_mullo_epi16 (data, alpha),
					    mask_0080),
			    mask_0101);
}

static force_inline __m128i
pix_add_multiply_1x128 (__m128i* src,
			__m128i* alpha_dst,
			__m128i* dst,
			__m128i* alpha_src)
{
    __m128i t1 = pix_multiply_1x128 (*src, *alpha_dst);
    __m128i t2 = pix_multiply_1x128 (*dst, *alpha_src);

    return _mm_adds_epu8 (t1, t2);
}

static force_inline __m128i
negate_1x128 (__m128i data)
{
    return _mm_xor_si128 (data, mask_00ff);
}

static force_inline __m128i
invert_colors_1x128 (__m128i data)
{
    return _mm_shufflelo_epi16 (data, _MM_SHUFFLE (3, 0, 1, 2));
}

static force_inline __m128i
over_1x128 (__m128i src, __m128i alpha, __m128i dst)
{
    return _mm_adds_epu8 (src, pix_multiply_1x128 (dst, negate_1x128 (alpha)));
}

static force_inline __m128i
in_over_1x128 (__m128i* src, __m128i* alpha, __m128i* mask, __m128i* dst)
{
    return over_1x128 (pix_multiply_1x128 (*src, *mask),
		       pix_multiply_1x128 (*alpha, *mask),
		       *dst);
}

static force_inline __m128i
over_rev_non_pre_1x128 (__m128i src, __m128i dst)
{
    __m128i alpha = expand_alpha_1x128 (src);

    return over_1x128 (pix_multiply_1x128 (invert_colors_1x128 (src),
					   _mm_or_si128 (alpha, mask_alpha)),
		       alpha,
		       dst);
}

static force_inline uint32_t
pack_1x128_32 (__m128i data)
{
    return _mm_cvtsi128_si32 (_mm_packus_epi16 (data, _mm_setzero_si128 ()));
}

static force_inline __m128i
expand565_16_1x128 (uint16_t pixel)
{
    __m128i m = _mm_cvtsi32_si128 (pixel);

    m = unpack_565_to_8888 (m);

    return _mm_unpacklo_epi8 (m, _mm_setzero_si128 ());
}

static force_inline uint32_t
core_combine_over_u_pixel_sse2 (uint32_t src, uint32_t dst)
{
    uint8_t a;
    __m128i xmms;

    a = src >> 24;

    if (a == 0xff)
    {
	return src;
    }
    else if (src)
    {
	xmms = unpack_32_1x128 (src);
	return pack_1x128_32 (
	    over_1x128 (xmms, expand_alpha_1x128 (xmms),
			unpack_32_1x128 (dst)));
    }

    return dst;
}

static force_inline uint32_t
combine1 (const uint32_t *ps, const uint32_t *pm)
{
    uint32_t s = *ps;

    if (pm)
    {
	__m128i ms, mm;

	mm = unpack_32_1x128 (*pm);
	mm = expand_alpha_1x128 (mm);

	ms = unpack_32_1x128 (s);
	ms = pix_multiply_1x128 (ms, mm);

	s = pack_1x128_32 (ms);
    }

    return s;
}

static force_inline __m128i
combine4 (const __m128i *ps, const __m128i *pm)
{
    __m128i xmm_src_lo, xmm_src_hi;
    __m128i xmm_msk_lo, xmm_msk_hi;
    __m128i s;

    if (pm)
    {
	xmm_msk_lo = load_128_unaligned (pm);

	if (is_transparent (xmm_msk_lo))
	    return _mm_setzero_si128 ();
    }

    s = load_128_unaligned (ps);

    if (pm)
    {
	unpack_128_2x128 (s, &xmm_src_lo, &xmm_src_hi);
	unpack_128_2x128 (xmm_msk_lo, &xmm_msk_lo, &xmm_msk_hi);

	expand_alpha_2x128 (xmm_msk_lo, xmm_msk_hi, &xmm_msk_lo, &xmm_msk_hi);

	pix_multiply_2x128 (&xmm_src_lo, &xmm_src_hi,
			    &xmm_msk_lo, &xmm_msk_hi,
			    &xmm_src_lo, &xmm_src_hi);

	s = pack_2x128_128 (xmm_src_lo, xmm_src_hi);
    }

    return s;
}

static force_inline void
core_combine_over_u_sse2_mask (uint32_t *	  pd,
			       const uint32_t*    ps,
			       const uint32_t*    pm,
			       int                w)
{
    uint32_t s, d;

    /* Align dst on a 16-byte boundary */
    while (w && ((uintptr_t)pd & 15))
    {
	d = *pd;
	s = combine1 (ps, pm);

	if (s)
	    *pd = core_combine_over_u_pixel_sse2 (s, d);
	pd++;
	ps++;
	pm++;
	w--;
    }

    while (w >= 4)
    {
	__m128i mask = load_128_unaligned ((__m128i *)pm);

	if (!is_zero (mask))
	{
	    __m128i src;
	    __m128i src_hi, src_lo;
	    __m128i mask_hi, mask_lo;
	    __m128i alpha_hi, alpha_lo;

	    src = load_128_unaligned ((__m128i *)ps);

	    if (is_opaque (_mm_and_si128 (src, mask)))
	    {
		save_128_aligned ((__m128i *)pd, src);
	    }
	    else
	    {
		__m128i dst = load_128_aligned ((__m128i *)pd);
		__m128i dst_hi, dst_lo;

		unpack_128_2x128 (mask, &mask_lo, &mask_hi);
		unpack_128_2x128 (src, &src_lo, &src_hi);

		expand_alpha_2x128 (mask_lo, mask_hi, &mask_lo, &mask_hi);
		pix_multiply_2x128 (&src_lo, &src_hi,
				    &mask_lo, &mask_hi,
				    &src_lo, &src_hi);

		unpack_128_2x128 (dst, &dst_lo, &dst_hi);

		expand_alpha_2x128 (src_lo, src_hi,
				    &alpha_lo, &alpha_hi);

		over_2x128 (&src_lo, &src_hi, &alpha_lo, &alpha_hi,
			    &dst_lo, &dst_hi);

		save_128_aligned (
		    (__m128i *)pd,
		    pack_2x128_128 (dst_lo, dst_hi));
	    }
	}

	pm += 4;
	ps += 4;
	pd += 4;
	w -= 4;
    }
    while (w)
    {
	d = *pd;
	s = combine1 (ps, pm);

	if (s)
	    *pd = core_combine_over_u_pixel_sse2 (s, d);
	pd++;
	ps++;
	pm++;

	w--;
    }
}

static force_inline void
core_combine_over_u_sse2_no_mask (uint32_t *	  pd,
				  const uint32_t*    ps,
				  int                w)
{
    uint32_t s, d;

    /* Align dst on a 16-byte boundary */
    while (w && ((uintptr_t)pd & 15))
    {
	d = *pd;
	s = *ps;

	if (s)
	    *pd = core_combine_over_u_pixel_sse2 (s, d);
	pd++;
	ps++;
	w--;
    }

    while (w >= 4)
    {
	__m128i src;
	__m128i src_hi, src_lo, dst_hi, dst_lo;
	__m128i alpha_hi, alpha_lo;

	src = load_128_unaligned ((__m128i *)ps);

	if (!is_zero (src))
	{
	    if (is_opaque (src))
	    {
		save_128_aligned ((__m128i *)pd, src);
	    }
	    else
	    {
		__m128i dst = load_128_aligned ((__m128i *)pd);

		unpack_128_2x128 (src, &src_lo, &src_hi);
		unpack_128_2x128 (dst, &dst_lo, &dst_hi);

		expand_alpha_2x128 (src_lo, src_hi,
				    &alpha_lo, &alpha_hi);
		over_2x128 (&src_lo, &src_hi, &alpha_lo, &alpha_hi,
			    &dst_lo, &dst_hi);

		save_128_aligned (
		    (__m128i *)pd,
		    pack_2x128_128 (dst_lo, dst_hi));
	    }
	}

	ps += 4;
	pd += 4;
	w -= 4;
    }
    while (w)
    {
	d = *pd;
	s = *ps;

	if (s)
	    *pd = core_combine_over_u_pixel_sse2 (s, d);
	pd++;
	ps++;

	w--;
    }
}

static force_inline void
sse2_combine_over_u (pixman_implementation_t *imp,
                     pixman_op_t              op,
                     uint32_t *               pd,
                     const uint32_t *         ps,
                     const uint32_t *         pm,
                     int                      w)
{
    if (pm)
	core_combine_over_u_sse2_mask (pd, ps, pm, w);
    else
	core_combine_over_u_sse2_no_mask (pd, ps, w);
}

static void
sse2_combine_over_reverse_u (pixman_implementation_t *imp,
                             pixman_op_t              op,
                             uint32_t *               pd,
                             const uint32_t *         ps,
                             const uint32_t *         pm,
                             int                      w)
{
    uint32_t s, d;

    __m128i xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_src_lo, xmm_src_hi;
    __m128i xmm_alpha_lo, xmm_alpha_hi;

    /* Align dst on a 16-byte boundary */
    while (w &&
           ((uintptr_t)pd & 15))
    {
	d = *pd;
	s = combine1 (ps, pm);

	*pd++ = core_combine_over_u_pixel_sse2 (d, s);
	w--;
	ps++;
	if (pm)
	    pm++;
    }

    while (w >= 4)
    {
	/* I'm loading unaligned because I'm not sure
	 * about the address alignment.
	 */
	xmm_src_hi = combine4 ((__m128i*)ps, (__m128i*)pm);
	xmm_dst_hi = load_128_aligned ((__m128i*) pd);

	unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	unpack_128_2x128 (xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);

	expand_alpha_2x128 (xmm_dst_lo, xmm_dst_hi,
			    &xmm_alpha_lo, &xmm_alpha_hi);

	over_2x128 (&xmm_dst_lo, &xmm_dst_hi,
		    &xmm_alpha_lo, &xmm_alpha_hi,
		    &xmm_src_lo, &xmm_src_hi);

	/* rebuid the 4 pixel data and save*/
	save_128_aligned ((__m128i*)pd,
			  pack_2x128_128 (xmm_src_lo, xmm_src_hi));

	w -= 4;
	ps += 4;
	pd += 4;

	if (pm)
	    pm += 4;
    }

    while (w)
    {
	d = *pd;
	s = combine1 (ps, pm);

	*pd++ = core_combine_over_u_pixel_sse2 (d, s);
	ps++;
	w--;
	if (pm)
	    pm++;
    }
}

static force_inline uint32_t
core_combine_in_u_pixel_sse2 (uint32_t src, uint32_t dst)
{
    uint32_t maska = src >> 24;

    if (maska == 0)
    {
	return 0;
    }
    else if (maska != 0xff)
    {
	return pack_1x128_32 (
	    pix_multiply_1x128 (unpack_32_1x128 (dst),
				expand_alpha_1x128 (unpack_32_1x128 (src))));
    }

    return dst;
}

static void
sse2_combine_in_u (pixman_implementation_t *imp,
                   pixman_op_t              op,
                   uint32_t *               pd,
                   const uint32_t *         ps,
                   const uint32_t *         pm,
                   int                      w)
{
    uint32_t s, d;

    __m128i xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst_lo, xmm_dst_hi;

    while (w && ((uintptr_t)pd & 15))
    {
	s = combine1 (ps, pm);
	d = *pd;

	*pd++ = core_combine_in_u_pixel_sse2 (d, s);
	w--;
	ps++;
	if (pm)
	    pm++;
    }

    while (w >= 4)
    {
	xmm_dst_hi = load_128_aligned ((__m128i*) pd);
	xmm_src_hi = combine4 ((__m128i*) ps, (__m128i*) pm);

	unpack_128_2x128 (xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);
	expand_alpha_2x128 (xmm_dst_lo, xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);

	unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	pix_multiply_2x128 (&xmm_src_lo, &xmm_src_hi,
			    &xmm_dst_lo, &xmm_dst_hi,
			    &xmm_dst_lo, &xmm_dst_hi);

	save_128_aligned ((__m128i*)pd,
			  pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	ps += 4;
	pd += 4;
	w -= 4;
	if (pm)
	    pm += 4;
    }

    while (w)
    {
	s = combine1 (ps, pm);
	d = *pd;

	*pd++ = core_combine_in_u_pixel_sse2 (d, s);
	w--;
	ps++;
	if (pm)
	    pm++;
    }
}

static void
sse2_combine_in_reverse_u (pixman_implementation_t *imp,
                           pixman_op_t              op,
                           uint32_t *               pd,
                           const uint32_t *         ps,
                           const uint32_t *         pm,
                           int                      w)
{
    uint32_t s, d;

    __m128i xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst_lo, xmm_dst_hi;

    while (w && ((uintptr_t)pd & 15))
    {
	s = combine1 (ps, pm);
	d = *pd;

	*pd++ = core_combine_in_u_pixel_sse2 (s, d);
	ps++;
	w--;
	if (pm)
	    pm++;
    }

    while (w >= 4)
    {
	xmm_dst_hi = load_128_aligned ((__m128i*) pd);
	xmm_src_hi = combine4 ((__m128i*) ps, (__m128i*)pm);

	unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	expand_alpha_2x128 (xmm_src_lo, xmm_src_hi, &xmm_src_lo, &xmm_src_hi);

	unpack_128_2x128 (xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);
	pix_multiply_2x128 (&xmm_dst_lo, &xmm_dst_hi,
			    &xmm_src_lo, &xmm_src_hi,
			    &xmm_dst_lo, &xmm_dst_hi);

	save_128_aligned (
	    (__m128i*)pd, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	ps += 4;
	pd += 4;
	w -= 4;
	if (pm)
	    pm += 4;
    }

    while (w)
    {
	s = combine1 (ps, pm);
	d = *pd;

	*pd++ = core_combine_in_u_pixel_sse2 (s, d);
	w--;
	ps++;
	if (pm)
	    pm++;
    }
}

static void
sse2_combine_out_reverse_u (pixman_implementation_t *imp,
                            pixman_op_t              op,
                            uint32_t *               pd,
                            const uint32_t *         ps,
                            const uint32_t *         pm,
                            int                      w)
{
    while (w && ((uintptr_t)pd & 15))
    {
	uint32_t s = combine1 (ps, pm);
	uint32_t d = *pd;

	*pd++ = pack_1x128_32 (
	    pix_multiply_1x128 (
		unpack_32_1x128 (d), negate_1x128 (
		    expand_alpha_1x128 (unpack_32_1x128 (s)))));

	if (pm)
	    pm++;
	ps++;
	w--;
    }

    while (w >= 4)
    {
	__m128i xmm_src_lo, xmm_src_hi;
	__m128i xmm_dst_lo, xmm_dst_hi;

	xmm_src_hi = combine4 ((__m128i*)ps, (__m128i*)pm);
	xmm_dst_hi = load_128_aligned ((__m128i*) pd);

	unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	unpack_128_2x128 (xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);

	expand_alpha_2x128 (xmm_src_lo, xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	negate_2x128       (xmm_src_lo, xmm_src_hi, &xmm_src_lo, &xmm_src_hi);

	pix_multiply_2x128 (&xmm_dst_lo, &xmm_dst_hi,
			    &xmm_src_lo, &xmm_src_hi,
			    &xmm_dst_lo, &xmm_dst_hi);

	save_128_aligned (
	    (__m128i*)pd, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	ps += 4;
	pd += 4;
	if (pm)
	    pm += 4;

	w -= 4;
    }

    while (w)
    {
	uint32_t s = combine1 (ps, pm);
	uint32_t d = *pd;

	*pd++ = pack_1x128_32 (
	    pix_multiply_1x128 (
		unpack_32_1x128 (d), negate_1x128 (
		    expand_alpha_1x128 (unpack_32_1x128 (s)))));
	ps++;
	if (pm)
	    pm++;
	w--;
    }
}

static void
sse2_combine_out_u (pixman_implementation_t *imp,
                    pixman_op_t              op,
                    uint32_t *               pd,
                    const uint32_t *         ps,
                    const uint32_t *         pm,
                    int                      w)
{
    while (w && ((uintptr_t)pd & 15))
    {
	uint32_t s = combine1 (ps, pm);
	uint32_t d = *pd;

	*pd++ = pack_1x128_32 (
	    pix_multiply_1x128 (
		unpack_32_1x128 (s), negate_1x128 (
		    expand_alpha_1x128 (unpack_32_1x128 (d)))));
	w--;
	ps++;
	if (pm)
	    pm++;
    }

    while (w >= 4)
    {
	__m128i xmm_src_lo, xmm_src_hi;
	__m128i xmm_dst_lo, xmm_dst_hi;

	xmm_src_hi = combine4 ((__m128i*) ps, (__m128i*)pm);
	xmm_dst_hi = load_128_aligned ((__m128i*) pd);

	unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	unpack_128_2x128 (xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);

	expand_alpha_2x128 (xmm_dst_lo, xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);
	negate_2x128       (xmm_dst_lo, xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);

	pix_multiply_2x128 (&xmm_src_lo, &xmm_src_hi,
			    &xmm_dst_lo, &xmm_dst_hi,
			    &xmm_dst_lo, &xmm_dst_hi);

	save_128_aligned (
	    (__m128i*)pd, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	ps += 4;
	pd += 4;
	w -= 4;
	if (pm)
	    pm += 4;
    }

    while (w)
    {
	uint32_t s = combine1 (ps, pm);
	uint32_t d = *pd;

	*pd++ = pack_1x128_32 (
	    pix_multiply_1x128 (
		unpack_32_1x128 (s), negate_1x128 (
		    expand_alpha_1x128 (unpack_32_1x128 (d)))));
	w--;
	ps++;
	if (pm)
	    pm++;
    }
}

static force_inline uint32_t
core_combine_atop_u_pixel_sse2 (uint32_t src,
                                uint32_t dst)
{
    __m128i s = unpack_32_1x128 (src);
    __m128i d = unpack_32_1x128 (dst);

    __m128i sa = negate_1x128 (expand_alpha_1x128 (s));
    __m128i da = expand_alpha_1x128 (d);

    return pack_1x128_32 (pix_add_multiply_1x128 (&s, &da, &d, &sa));
}

static void
sse2_combine_atop_u (pixman_implementation_t *imp,
                     pixman_op_t              op,
                     uint32_t *               pd,
                     const uint32_t *         ps,
                     const uint32_t *         pm,
                     int                      w)
{
    uint32_t s, d;

    __m128i xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_alpha_src_lo, xmm_alpha_src_hi;
    __m128i xmm_alpha_dst_lo, xmm_alpha_dst_hi;

    while (w && ((uintptr_t)pd & 15))
    {
	s = combine1 (ps, pm);
	d = *pd;

	*pd++ = core_combine_atop_u_pixel_sse2 (s, d);
	w--;
	ps++;
	if (pm)
	    pm++;
    }

    while (w >= 4)
    {
	xmm_src_hi = combine4 ((__m128i*)ps, (__m128i*)pm);
	xmm_dst_hi = load_128_aligned ((__m128i*) pd);

	unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	unpack_128_2x128 (xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);

	expand_alpha_2x128 (xmm_src_lo, xmm_src_hi,
			    &xmm_alpha_src_lo, &xmm_alpha_src_hi);
	expand_alpha_2x128 (xmm_dst_lo, xmm_dst_hi,
			    &xmm_alpha_dst_lo, &xmm_alpha_dst_hi);

	negate_2x128 (xmm_alpha_src_lo, xmm_alpha_src_hi,
		      &xmm_alpha_src_lo, &xmm_alpha_src_hi);

	pix_add_multiply_2x128 (
	    &xmm_src_lo, &xmm_src_hi, &xmm_alpha_dst_lo, &xmm_alpha_dst_hi,
	    &xmm_dst_lo, &xmm_dst_hi, &xmm_alpha_src_lo, &xmm_alpha_src_hi,
	    &xmm_dst_lo, &xmm_dst_hi);

	save_128_aligned (
	    (__m128i*)pd, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	ps += 4;
	pd += 4;
	w -= 4;
	if (pm)
	    pm += 4;
    }

    while (w)
    {
	s = combine1 (ps, pm);
	d = *pd;

	*pd++ = core_combine_atop_u_pixel_sse2 (s, d);
	w--;
	ps++;
	if (pm)
	    pm++;
    }
}

static force_inline uint32_t
core_combine_reverse_atop_u_pixel_sse2 (uint32_t src,
                                        uint32_t dst)
{
    __m128i s = unpack_32_1x128 (src);
    __m128i d = unpack_32_1x128 (dst);

    __m128i sa = expand_alpha_1x128 (s);
    __m128i da = negate_1x128 (expand_alpha_1x128 (d));

    return pack_1x128_32 (pix_add_multiply_1x128 (&s, &da, &d, &sa));
}

static void
sse2_combine_atop_reverse_u (pixman_implementation_t *imp,
                             pixman_op_t              op,
                             uint32_t *               pd,
                             const uint32_t *         ps,
                             const uint32_t *         pm,
                             int                      w)
{
    uint32_t s, d;

    __m128i xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_alpha_src_lo, xmm_alpha_src_hi;
    __m128i xmm_alpha_dst_lo, xmm_alpha_dst_hi;

    while (w && ((uintptr_t)pd & 15))
    {
	s = combine1 (ps, pm);
	d = *pd;

	*pd++ = core_combine_reverse_atop_u_pixel_sse2 (s, d);
	ps++;
	w--;
	if (pm)
	    pm++;
    }

    while (w >= 4)
    {
	xmm_src_hi = combine4 ((__m128i*)ps, (__m128i*)pm);
	xmm_dst_hi = load_128_aligned ((__m128i*) pd);

	unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	unpack_128_2x128 (xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);

	expand_alpha_2x128 (xmm_src_lo, xmm_src_hi,
			    &xmm_alpha_src_lo, &xmm_alpha_src_hi);
	expand_alpha_2x128 (xmm_dst_lo, xmm_dst_hi,
			    &xmm_alpha_dst_lo, &xmm_alpha_dst_hi);

	negate_2x128 (xmm_alpha_dst_lo, xmm_alpha_dst_hi,
		      &xmm_alpha_dst_lo, &xmm_alpha_dst_hi);

	pix_add_multiply_2x128 (
	    &xmm_src_lo, &xmm_src_hi, &xmm_alpha_dst_lo, &xmm_alpha_dst_hi,
	    &xmm_dst_lo, &xmm_dst_hi, &xmm_alpha_src_lo, &xmm_alpha_src_hi,
	    &xmm_dst_lo, &xmm_dst_hi);

	save_128_aligned (
	    (__m128i*)pd, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	ps += 4;
	pd += 4;
	w -= 4;
	if (pm)
	    pm += 4;
    }

    while (w)
    {
	s = combine1 (ps, pm);
	d = *pd;

	*pd++ = core_combine_reverse_atop_u_pixel_sse2 (s, d);
	ps++;
	w--;
	if (pm)
	    pm++;
    }
}

static force_inline uint32_t
core_combine_xor_u_pixel_sse2 (uint32_t src,
                               uint32_t dst)
{
    __m128i s = unpack_32_1x128 (src);
    __m128i d = unpack_32_1x128 (dst);

    __m128i neg_d = negate_1x128 (expand_alpha_1x128 (d));
    __m128i neg_s = negate_1x128 (expand_alpha_1x128 (s));

    return pack_1x128_32 (pix_add_multiply_1x128 (&s, &neg_d, &d, &neg_s));
}

static void
sse2_combine_xor_u (pixman_implementation_t *imp,
                    pixman_op_t              op,
                    uint32_t *               dst,
                    const uint32_t *         src,
                    const uint32_t *         mask,
                    int                      width)
{
    int w = width;
    uint32_t s, d;
    uint32_t* pd = dst;
    const uint32_t* ps = src;
    const uint32_t* pm = mask;

    __m128i xmm_src, xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst, xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_alpha_src_lo, xmm_alpha_src_hi;
    __m128i xmm_alpha_dst_lo, xmm_alpha_dst_hi;

    while (w && ((uintptr_t)pd & 15))
    {
	s = combine1 (ps, pm);
	d = *pd;

	*pd++ = core_combine_xor_u_pixel_sse2 (s, d);
	w--;
	ps++;
	if (pm)
	    pm++;
    }

    while (w >= 4)
    {
	xmm_src = combine4 ((__m128i*) ps, (__m128i*) pm);
	xmm_dst = load_128_aligned ((__m128i*) pd);

	unpack_128_2x128 (xmm_src, &xmm_src_lo, &xmm_src_hi);
	unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);

	expand_alpha_2x128 (xmm_src_lo, xmm_src_hi,
			    &xmm_alpha_src_lo, &xmm_alpha_src_hi);
	expand_alpha_2x128 (xmm_dst_lo, xmm_dst_hi,
			    &xmm_alpha_dst_lo, &xmm_alpha_dst_hi);

	negate_2x128 (xmm_alpha_src_lo, xmm_alpha_src_hi,
		      &xmm_alpha_src_lo, &xmm_alpha_src_hi);
	negate_2x128 (xmm_alpha_dst_lo, xmm_alpha_dst_hi,
		      &xmm_alpha_dst_lo, &xmm_alpha_dst_hi);

	pix_add_multiply_2x128 (
	    &xmm_src_lo, &xmm_src_hi, &xmm_alpha_dst_lo, &xmm_alpha_dst_hi,
	    &xmm_dst_lo, &xmm_dst_hi, &xmm_alpha_src_lo, &xmm_alpha_src_hi,
	    &xmm_dst_lo, &xmm_dst_hi);

	save_128_aligned (
	    (__m128i*)pd, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	ps += 4;
	pd += 4;
	w -= 4;
	if (pm)
	    pm += 4;
    }

    while (w)
    {
	s = combine1 (ps, pm);
	d = *pd;

	*pd++ = core_combine_xor_u_pixel_sse2 (s, d);
	w--;
	ps++;
	if (pm)
	    pm++;
    }
}

static force_inline void
sse2_combine_add_u (pixman_implementation_t *imp,
                    pixman_op_t              op,
                    uint32_t *               dst,
                    const uint32_t *         src,
                    const uint32_t *         mask,
                    int                      width)
{
    int w = width;
    uint32_t s, d;
    uint32_t* pd = dst;
    const uint32_t* ps = src;
    const uint32_t* pm = mask;

    while (w && (uintptr_t)pd & 15)
    {
	s = combine1 (ps, pm);
	d = *pd;

	ps++;
	if (pm)
	    pm++;
	*pd++ = _mm_cvtsi128_si32 (
	    _mm_adds_epu8 (_mm_cvtsi32_si128 (s), _mm_cvtsi32_si128 (d)));
	w--;
    }

    while (w >= 4)
    {
	__m128i s;

	s = combine4 ((__m128i*)ps, (__m128i*)pm);

	save_128_aligned (
	    (__m128i*)pd, _mm_adds_epu8 (s, load_128_aligned  ((__m128i*)pd)));

	pd += 4;
	ps += 4;
	if (pm)
	    pm += 4;
	w -= 4;
    }

    while (w--)
    {
	s = combine1 (ps, pm);
	d = *pd;

	ps++;
	*pd++ = _mm_cvtsi128_si32 (
	    _mm_adds_epu8 (_mm_cvtsi32_si128 (s), _mm_cvtsi32_si128 (d)));
	if (pm)
	    pm++;
    }
}

static force_inline uint32_t
core_combine_saturate_u_pixel_sse2 (uint32_t src,
                                    uint32_t dst)
{
    __m128i ms = unpack_32_1x128 (src);
    __m128i md = unpack_32_1x128 (dst);
    uint32_t sa = src >> 24;
    uint32_t da = ~dst >> 24;

    if (sa > da)
    {
	ms = pix_multiply_1x128 (
	    ms, expand_alpha_1x128 (unpack_32_1x128 (DIV_UN8 (da, sa) << 24)));
    }

    return pack_1x128_32 (_mm_adds_epu16 (md, ms));
}

static void
sse2_combine_saturate_u (pixman_implementation_t *imp,
                         pixman_op_t              op,
                         uint32_t *               pd,
                         const uint32_t *         ps,
                         const uint32_t *         pm,
                         int                      w)
{
    uint32_t s, d;

    uint32_t pack_cmp;
    __m128i xmm_src, xmm_dst;

    while (w && (uintptr_t)pd & 15)
    {
	s = combine1 (ps, pm);
	d = *pd;

	*pd++ = core_combine_saturate_u_pixel_sse2 (s, d);
	w--;
	ps++;
	if (pm)
	    pm++;
    }

    while (w >= 4)
    {
	xmm_dst = load_128_aligned  ((__m128i*)pd);
	xmm_src = combine4 ((__m128i*)ps, (__m128i*)pm);

	pack_cmp = _mm_movemask_epi8 (
	    _mm_cmpgt_epi32 (
		_mm_srli_epi32 (xmm_src, 24),
		_mm_srli_epi32 (_mm_xor_si128 (xmm_dst, mask_ff000000), 24)));

	/* if some alpha src is grater than respective ~alpha dst */
	if (pack_cmp)
	{
	    s = combine1 (ps++, pm);
	    d = *pd;
	    *pd++ = core_combine_saturate_u_pixel_sse2 (s, d);
	    if (pm)
		pm++;

	    s = combine1 (ps++, pm);
	    d = *pd;
	    *pd++ = core_combine_saturate_u_pixel_sse2 (s, d);
	    if (pm)
		pm++;

	    s = combine1 (ps++, pm);
	    d = *pd;
	    *pd++ = core_combine_saturate_u_pixel_sse2 (s, d);
	    if (pm)
		pm++;

	    s = combine1 (ps++, pm);
	    d = *pd;
	    *pd++ = core_combine_saturate_u_pixel_sse2 (s, d);
	    if (pm)
		pm++;
	}
	else
	{
	    save_128_aligned ((__m128i*)pd, _mm_adds_epu8 (xmm_dst, xmm_src));

	    pd += 4;
	    ps += 4;
	    if (pm)
		pm += 4;
	}

	w -= 4;
    }

    while (w--)
    {
	s = combine1 (ps, pm);
	d = *pd;

	*pd++ = core_combine_saturate_u_pixel_sse2 (s, d);
	ps++;
	if (pm)
	    pm++;
    }
}

static void
sse2_combine_src_ca (pixman_implementation_t *imp,
                     pixman_op_t              op,
                     uint32_t *               pd,
                     const uint32_t *         ps,
                     const uint32_t *         pm,
                     int                      w)
{
    uint32_t s, m;

    __m128i xmm_src_lo, xmm_src_hi;
    __m128i xmm_mask_lo, xmm_mask_hi;
    __m128i xmm_dst_lo, xmm_dst_hi;

    while (w && (uintptr_t)pd & 15)
    {
	s = *ps++;
	m = *pm++;
	*pd++ = pack_1x128_32 (
	    pix_multiply_1x128 (unpack_32_1x128 (s), unpack_32_1x128 (m)));
	w--;
    }

    while (w >= 4)
    {
	xmm_src_hi = load_128_unaligned ((__m128i*)ps);
	xmm_mask_hi = load_128_unaligned ((__m128i*)pm);

	unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	unpack_128_2x128 (xmm_mask_hi, &xmm_mask_lo, &xmm_mask_hi);

	pix_multiply_2x128 (&xmm_src_lo, &xmm_src_hi,
			    &xmm_mask_lo, &xmm_mask_hi,
			    &xmm_dst_lo, &xmm_dst_hi);

	save_128_aligned (
	    (__m128i*)pd, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	ps += 4;
	pd += 4;
	pm += 4;
	w -= 4;
    }

    while (w)
    {
	s = *ps++;
	m = *pm++;
	*pd++ = pack_1x128_32 (
	    pix_multiply_1x128 (unpack_32_1x128 (s), unpack_32_1x128 (m)));
	w--;
    }
}

static force_inline uint32_t
core_combine_over_ca_pixel_sse2 (uint32_t src,
                                 uint32_t mask,
                                 uint32_t dst)
{
    __m128i s = unpack_32_1x128 (src);
    __m128i expAlpha = expand_alpha_1x128 (s);
    __m128i unpk_mask = unpack_32_1x128 (mask);
    __m128i unpk_dst  = unpack_32_1x128 (dst);

    return pack_1x128_32 (in_over_1x128 (&s, &expAlpha, &unpk_mask, &unpk_dst));
}

static void
sse2_combine_over_ca (pixman_implementation_t *imp,
                      pixman_op_t              op,
                      uint32_t *               pd,
                      const uint32_t *         ps,
                      const uint32_t *         pm,
                      int                      w)
{
    uint32_t s, m, d;

    __m128i xmm_alpha_lo, xmm_alpha_hi;
    __m128i xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_mask_lo, xmm_mask_hi;

    while (w && (uintptr_t)pd & 15)
    {
	s = *ps++;
	m = *pm++;
	d = *pd;

	*pd++ = core_combine_over_ca_pixel_sse2 (s, m, d);
	w--;
    }

    while (w >= 4)
    {
	xmm_dst_hi = load_128_aligned ((__m128i*)pd);
	xmm_src_hi = load_128_unaligned ((__m128i*)ps);
	xmm_mask_hi = load_128_unaligned ((__m128i*)pm);

	unpack_128_2x128 (xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);
	unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	unpack_128_2x128 (xmm_mask_hi, &xmm_mask_lo, &xmm_mask_hi);

	expand_alpha_2x128 (xmm_src_lo, xmm_src_hi,
			    &xmm_alpha_lo, &xmm_alpha_hi);

	in_over_2x128 (&xmm_src_lo, &xmm_src_hi,
		       &xmm_alpha_lo, &xmm_alpha_hi,
		       &xmm_mask_lo, &xmm_mask_hi,
		       &xmm_dst_lo, &xmm_dst_hi);

	save_128_aligned (
	    (__m128i*)pd, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	ps += 4;
	pd += 4;
	pm += 4;
	w -= 4;
    }

    while (w)
    {
	s = *ps++;
	m = *pm++;
	d = *pd;

	*pd++ = core_combine_over_ca_pixel_sse2 (s, m, d);
	w--;
    }
}

static force_inline uint32_t
core_combine_over_reverse_ca_pixel_sse2 (uint32_t src,
                                         uint32_t mask,
                                         uint32_t dst)
{
    __m128i d = unpack_32_1x128 (dst);

    return pack_1x128_32 (
	over_1x128 (d, expand_alpha_1x128 (d),
		    pix_multiply_1x128 (unpack_32_1x128 (src),
					unpack_32_1x128 (mask))));
}

static void
sse2_combine_over_reverse_ca (pixman_implementation_t *imp,
                              pixman_op_t              op,
                              uint32_t *               pd,
                              const uint32_t *         ps,
                              const uint32_t *         pm,
                              int                      w)
{
    uint32_t s, m, d;

    __m128i xmm_alpha_lo, xmm_alpha_hi;
    __m128i xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_mask_lo, xmm_mask_hi;

    while (w && (uintptr_t)pd & 15)
    {
	s = *ps++;
	m = *pm++;
	d = *pd;

	*pd++ = core_combine_over_reverse_ca_pixel_sse2 (s, m, d);
	w--;
    }

    while (w >= 4)
    {
	xmm_dst_hi = load_128_aligned ((__m128i*)pd);
	xmm_src_hi = load_128_unaligned ((__m128i*)ps);
	xmm_mask_hi = load_128_unaligned ((__m128i*)pm);

	unpack_128_2x128 (xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);
	unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	unpack_128_2x128 (xmm_mask_hi, &xmm_mask_lo, &xmm_mask_hi);

	expand_alpha_2x128 (xmm_dst_lo, xmm_dst_hi,
			    &xmm_alpha_lo, &xmm_alpha_hi);
	pix_multiply_2x128 (&xmm_src_lo, &xmm_src_hi,
			    &xmm_mask_lo, &xmm_mask_hi,
			    &xmm_mask_lo, &xmm_mask_hi);

	over_2x128 (&xmm_dst_lo, &xmm_dst_hi,
		    &xmm_alpha_lo, &xmm_alpha_hi,
		    &xmm_mask_lo, &xmm_mask_hi);

	save_128_aligned (
	    (__m128i*)pd, pack_2x128_128 (xmm_mask_lo, xmm_mask_hi));

	ps += 4;
	pd += 4;
	pm += 4;
	w -= 4;
    }

    while (w)
    {
	s = *ps++;
	m = *pm++;
	d = *pd;

	*pd++ = core_combine_over_reverse_ca_pixel_sse2 (s, m, d);
	w--;
    }
}

static void
sse2_combine_in_ca (pixman_implementation_t *imp,
                    pixman_op_t              op,
                    uint32_t *               pd,
                    const uint32_t *         ps,
                    const uint32_t *         pm,
                    int                      w)
{
    uint32_t s, m, d;

    __m128i xmm_alpha_lo, xmm_alpha_hi;
    __m128i xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_mask_lo, xmm_mask_hi;

    while (w && (uintptr_t)pd & 15)
    {
	s = *ps++;
	m = *pm++;
	d = *pd;

	*pd++ = pack_1x128_32 (
	    pix_multiply_1x128 (
		pix_multiply_1x128 (unpack_32_1x128 (s), unpack_32_1x128 (m)),
		expand_alpha_1x128 (unpack_32_1x128 (d))));

	w--;
    }

    while (w >= 4)
    {
	xmm_dst_hi = load_128_aligned ((__m128i*)pd);
	xmm_src_hi = load_128_unaligned ((__m128i*)ps);
	xmm_mask_hi = load_128_unaligned ((__m128i*)pm);

	unpack_128_2x128 (xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);
	unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	unpack_128_2x128 (xmm_mask_hi, &xmm_mask_lo, &xmm_mask_hi);

	expand_alpha_2x128 (xmm_dst_lo, xmm_dst_hi,
			    &xmm_alpha_lo, &xmm_alpha_hi);

	pix_multiply_2x128 (&xmm_src_lo, &xmm_src_hi,
			    &xmm_mask_lo, &xmm_mask_hi,
			    &xmm_dst_lo, &xmm_dst_hi);

	pix_multiply_2x128 (&xmm_dst_lo, &xmm_dst_hi,
			    &xmm_alpha_lo, &xmm_alpha_hi,
			    &xmm_dst_lo, &xmm_dst_hi);

	save_128_aligned (
	    (__m128i*)pd, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	ps += 4;
	pd += 4;
	pm += 4;
	w -= 4;
    }

    while (w)
    {
	s = *ps++;
	m = *pm++;
	d = *pd;

	*pd++ = pack_1x128_32 (
	    pix_multiply_1x128 (
		pix_multiply_1x128 (
		    unpack_32_1x128 (s), unpack_32_1x128 (m)),
		expand_alpha_1x128 (unpack_32_1x128 (d))));

	w--;
    }
}

static void
sse2_combine_in_reverse_ca (pixman_implementation_t *imp,
                            pixman_op_t              op,
                            uint32_t *               pd,
                            const uint32_t *         ps,
                            const uint32_t *         pm,
                            int                      w)
{
    uint32_t s, m, d;

    __m128i xmm_alpha_lo, xmm_alpha_hi;
    __m128i xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_mask_lo, xmm_mask_hi;

    while (w && (uintptr_t)pd & 15)
    {
	s = *ps++;
	m = *pm++;
	d = *pd;

	*pd++ = pack_1x128_32 (
	    pix_multiply_1x128 (
		unpack_32_1x128 (d),
		pix_multiply_1x128 (unpack_32_1x128 (m),
				   expand_alpha_1x128 (unpack_32_1x128 (s)))));
	w--;
    }

    while (w >= 4)
    {
	xmm_dst_hi = load_128_aligned ((__m128i*)pd);
	xmm_src_hi = load_128_unaligned ((__m128i*)ps);
	xmm_mask_hi = load_128_unaligned ((__m128i*)pm);

	unpack_128_2x128 (xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);
	unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	unpack_128_2x128 (xmm_mask_hi, &xmm_mask_lo, &xmm_mask_hi);

	expand_alpha_2x128 (xmm_src_lo, xmm_src_hi,
			    &xmm_alpha_lo, &xmm_alpha_hi);
	pix_multiply_2x128 (&xmm_mask_lo, &xmm_mask_hi,
			    &xmm_alpha_lo, &xmm_alpha_hi,
			    &xmm_alpha_lo, &xmm_alpha_hi);

	pix_multiply_2x128 (&xmm_dst_lo, &xmm_dst_hi,
			    &xmm_alpha_lo, &xmm_alpha_hi,
			    &xmm_dst_lo, &xmm_dst_hi);

	save_128_aligned (
	    (__m128i*)pd, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	ps += 4;
	pd += 4;
	pm += 4;
	w -= 4;
    }

    while (w)
    {
	s = *ps++;
	m = *pm++;
	d = *pd;

	*pd++ = pack_1x128_32 (
	    pix_multiply_1x128 (
		unpack_32_1x128 (d),
		pix_multiply_1x128 (unpack_32_1x128 (m),
				   expand_alpha_1x128 (unpack_32_1x128 (s)))));
	w--;
    }
}

static void
sse2_combine_out_ca (pixman_implementation_t *imp,
                     pixman_op_t              op,
                     uint32_t *               pd,
                     const uint32_t *         ps,
                     const uint32_t *         pm,
                     int                      w)
{
    uint32_t s, m, d;

    __m128i xmm_alpha_lo, xmm_alpha_hi;
    __m128i xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_mask_lo, xmm_mask_hi;

    while (w && (uintptr_t)pd & 15)
    {
	s = *ps++;
	m = *pm++;
	d = *pd;

	*pd++ = pack_1x128_32 (
	    pix_multiply_1x128 (
		pix_multiply_1x128 (
		    unpack_32_1x128 (s), unpack_32_1x128 (m)),
		negate_1x128 (expand_alpha_1x128 (unpack_32_1x128 (d)))));
	w--;
    }

    while (w >= 4)
    {
	xmm_dst_hi = load_128_aligned ((__m128i*)pd);
	xmm_src_hi = load_128_unaligned ((__m128i*)ps);
	xmm_mask_hi = load_128_unaligned ((__m128i*)pm);

	unpack_128_2x128 (xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);
	unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	unpack_128_2x128 (xmm_mask_hi, &xmm_mask_lo, &xmm_mask_hi);

	expand_alpha_2x128 (xmm_dst_lo, xmm_dst_hi,
			    &xmm_alpha_lo, &xmm_alpha_hi);
	negate_2x128 (xmm_alpha_lo, xmm_alpha_hi,
		      &xmm_alpha_lo, &xmm_alpha_hi);

	pix_multiply_2x128 (&xmm_src_lo, &xmm_src_hi,
			    &xmm_mask_lo, &xmm_mask_hi,
			    &xmm_dst_lo, &xmm_dst_hi);
	pix_multiply_2x128 (&xmm_dst_lo, &xmm_dst_hi,
			    &xmm_alpha_lo, &xmm_alpha_hi,
			    &xmm_dst_lo, &xmm_dst_hi);

	save_128_aligned (
	    (__m128i*)pd, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	ps += 4;
	pd += 4;
	pm += 4;
	w -= 4;
    }

    while (w)
    {
	s = *ps++;
	m = *pm++;
	d = *pd;

	*pd++ = pack_1x128_32 (
	    pix_multiply_1x128 (
		pix_multiply_1x128 (
		    unpack_32_1x128 (s), unpack_32_1x128 (m)),
		negate_1x128 (expand_alpha_1x128 (unpack_32_1x128 (d)))));

	w--;
    }
}

static void
sse2_combine_out_reverse_ca (pixman_implementation_t *imp,
                             pixman_op_t              op,
                             uint32_t *               pd,
                             const uint32_t *         ps,
                             const uint32_t *         pm,
                             int                      w)
{
    uint32_t s, m, d;

    __m128i xmm_alpha_lo, xmm_alpha_hi;
    __m128i xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_mask_lo, xmm_mask_hi;

    while (w && (uintptr_t)pd & 15)
    {
	s = *ps++;
	m = *pm++;
	d = *pd;

	*pd++ = pack_1x128_32 (
	    pix_multiply_1x128 (
		unpack_32_1x128 (d),
		negate_1x128 (pix_multiply_1x128 (
				 unpack_32_1x128 (m),
				 expand_alpha_1x128 (unpack_32_1x128 (s))))));
	w--;
    }

    while (w >= 4)
    {
	xmm_dst_hi = load_128_aligned ((__m128i*)pd);
	xmm_src_hi = load_128_unaligned ((__m128i*)ps);
	xmm_mask_hi = load_128_unaligned ((__m128i*)pm);

	unpack_128_2x128 (xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);
	unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	unpack_128_2x128 (xmm_mask_hi, &xmm_mask_lo, &xmm_mask_hi);

	expand_alpha_2x128 (xmm_src_lo, xmm_src_hi,
			    &xmm_alpha_lo, &xmm_alpha_hi);

	pix_multiply_2x128 (&xmm_mask_lo, &xmm_mask_hi,
			    &xmm_alpha_lo, &xmm_alpha_hi,
			    &xmm_mask_lo, &xmm_mask_hi);

	negate_2x128 (xmm_mask_lo, xmm_mask_hi,
		      &xmm_mask_lo, &xmm_mask_hi);

	pix_multiply_2x128 (&xmm_dst_lo, &xmm_dst_hi,
			    &xmm_mask_lo, &xmm_mask_hi,
			    &xmm_dst_lo, &xmm_dst_hi);

	save_128_aligned (
	    (__m128i*)pd, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	ps += 4;
	pd += 4;
	pm += 4;
	w -= 4;
    }

    while (w)
    {
	s = *ps++;
	m = *pm++;
	d = *pd;

	*pd++ = pack_1x128_32 (
	    pix_multiply_1x128 (
		unpack_32_1x128 (d),
		negate_1x128 (pix_multiply_1x128 (
				 unpack_32_1x128 (m),
				 expand_alpha_1x128 (unpack_32_1x128 (s))))));
	w--;
    }
}

static force_inline uint32_t
core_combine_atop_ca_pixel_sse2 (uint32_t src,
                                 uint32_t mask,
                                 uint32_t dst)
{
    __m128i m = unpack_32_1x128 (mask);
    __m128i s = unpack_32_1x128 (src);
    __m128i d = unpack_32_1x128 (dst);
    __m128i sa = expand_alpha_1x128 (s);
    __m128i da = expand_alpha_1x128 (d);

    s = pix_multiply_1x128 (s, m);
    m = negate_1x128 (pix_multiply_1x128 (m, sa));

    return pack_1x128_32 (pix_add_multiply_1x128 (&d, &m, &s, &da));
}

static void
sse2_combine_atop_ca (pixman_implementation_t *imp,
                      pixman_op_t              op,
                      uint32_t *               pd,
                      const uint32_t *         ps,
                      const uint32_t *         pm,
                      int                      w)
{
    uint32_t s, m, d;

    __m128i xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_alpha_src_lo, xmm_alpha_src_hi;
    __m128i xmm_alpha_dst_lo, xmm_alpha_dst_hi;
    __m128i xmm_mask_lo, xmm_mask_hi;

    while (w && (uintptr_t)pd & 15)
    {
	s = *ps++;
	m = *pm++;
	d = *pd;

	*pd++ = core_combine_atop_ca_pixel_sse2 (s, m, d);
	w--;
    }

    while (w >= 4)
    {
	xmm_dst_hi = load_128_aligned ((__m128i*)pd);
	xmm_src_hi = load_128_unaligned ((__m128i*)ps);
	xmm_mask_hi = load_128_unaligned ((__m128i*)pm);

	unpack_128_2x128 (xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);
	unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	unpack_128_2x128 (xmm_mask_hi, &xmm_mask_lo, &xmm_mask_hi);

	expand_alpha_2x128 (xmm_src_lo, xmm_src_hi,
			    &xmm_alpha_src_lo, &xmm_alpha_src_hi);
	expand_alpha_2x128 (xmm_dst_lo, xmm_dst_hi,
			    &xmm_alpha_dst_lo, &xmm_alpha_dst_hi);

	pix_multiply_2x128 (&xmm_src_lo, &xmm_src_hi,
			    &xmm_mask_lo, &xmm_mask_hi,
			    &xmm_src_lo, &xmm_src_hi);
	pix_multiply_2x128 (&xmm_mask_lo, &xmm_mask_hi,
			    &xmm_alpha_src_lo, &xmm_alpha_src_hi,
			    &xmm_mask_lo, &xmm_mask_hi);

	negate_2x128 (xmm_mask_lo, xmm_mask_hi, &xmm_mask_lo, &xmm_mask_hi);

	pix_add_multiply_2x128 (
	    &xmm_dst_lo, &xmm_dst_hi, &xmm_mask_lo, &xmm_mask_hi,
	    &xmm_src_lo, &xmm_src_hi, &xmm_alpha_dst_lo, &xmm_alpha_dst_hi,
	    &xmm_dst_lo, &xmm_dst_hi);

	save_128_aligned (
	    (__m128i*)pd, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	ps += 4;
	pd += 4;
	pm += 4;
	w -= 4;
    }

    while (w)
    {
	s = *ps++;
	m = *pm++;
	d = *pd;

	*pd++ = core_combine_atop_ca_pixel_sse2 (s, m, d);
	w--;
    }
}

static force_inline uint32_t
core_combine_reverse_atop_ca_pixel_sse2 (uint32_t src,
                                         uint32_t mask,
                                         uint32_t dst)
{
    __m128i m = unpack_32_1x128 (mask);
    __m128i s = unpack_32_1x128 (src);
    __m128i d = unpack_32_1x128 (dst);

    __m128i da = negate_1x128 (expand_alpha_1x128 (d));
    __m128i sa = expand_alpha_1x128 (s);

    s = pix_multiply_1x128 (s, m);
    m = pix_multiply_1x128 (m, sa);

    return pack_1x128_32 (pix_add_multiply_1x128 (&d, &m, &s, &da));
}

static void
sse2_combine_atop_reverse_ca (pixman_implementation_t *imp,
                              pixman_op_t              op,
                              uint32_t *               pd,
                              const uint32_t *         ps,
                              const uint32_t *         pm,
                              int                      w)
{
    uint32_t s, m, d;

    __m128i xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_alpha_src_lo, xmm_alpha_src_hi;
    __m128i xmm_alpha_dst_lo, xmm_alpha_dst_hi;
    __m128i xmm_mask_lo, xmm_mask_hi;

    while (w && (uintptr_t)pd & 15)
    {
	s = *ps++;
	m = *pm++;
	d = *pd;

	*pd++ = core_combine_reverse_atop_ca_pixel_sse2 (s, m, d);
	w--;
    }

    while (w >= 4)
    {
	xmm_dst_hi = load_128_aligned ((__m128i*)pd);
	xmm_src_hi = load_128_unaligned ((__m128i*)ps);
	xmm_mask_hi = load_128_unaligned ((__m128i*)pm);

	unpack_128_2x128 (xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);
	unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	unpack_128_2x128 (xmm_mask_hi, &xmm_mask_lo, &xmm_mask_hi);

	expand_alpha_2x128 (xmm_src_lo, xmm_src_hi,
			    &xmm_alpha_src_lo, &xmm_alpha_src_hi);
	expand_alpha_2x128 (xmm_dst_lo, xmm_dst_hi,
			    &xmm_alpha_dst_lo, &xmm_alpha_dst_hi);

	pix_multiply_2x128 (&xmm_src_lo, &xmm_src_hi,
			    &xmm_mask_lo, &xmm_mask_hi,
			    &xmm_src_lo, &xmm_src_hi);
	pix_multiply_2x128 (&xmm_mask_lo, &xmm_mask_hi,
			    &xmm_alpha_src_lo, &xmm_alpha_src_hi,
			    &xmm_mask_lo, &xmm_mask_hi);

	negate_2x128 (xmm_alpha_dst_lo, xmm_alpha_dst_hi,
		      &xmm_alpha_dst_lo, &xmm_alpha_dst_hi);

	pix_add_multiply_2x128 (
	    &xmm_dst_lo, &xmm_dst_hi, &xmm_mask_lo, &xmm_mask_hi,
	    &xmm_src_lo, &xmm_src_hi, &xmm_alpha_dst_lo, &xmm_alpha_dst_hi,
	    &xmm_dst_lo, &xmm_dst_hi);

	save_128_aligned (
	    (__m128i*)pd, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	ps += 4;
	pd += 4;
	pm += 4;
	w -= 4;
    }

    while (w)
    {
	s = *ps++;
	m = *pm++;
	d = *pd;

	*pd++ = core_combine_reverse_atop_ca_pixel_sse2 (s, m, d);
	w--;
    }
}

static force_inline uint32_t
core_combine_xor_ca_pixel_sse2 (uint32_t src,
                                uint32_t mask,
                                uint32_t dst)
{
    __m128i a = unpack_32_1x128 (mask);
    __m128i s = unpack_32_1x128 (src);
    __m128i d = unpack_32_1x128 (dst);

    __m128i alpha_dst = negate_1x128 (pix_multiply_1x128 (
				       a, expand_alpha_1x128 (s)));
    __m128i dest      = pix_multiply_1x128 (s, a);
    __m128i alpha_src = negate_1x128 (expand_alpha_1x128 (d));

    return pack_1x128_32 (pix_add_multiply_1x128 (&d,
                                                &alpha_dst,
                                                &dest,
                                                &alpha_src));
}

static void
sse2_combine_xor_ca (pixman_implementation_t *imp,
                     pixman_op_t              op,
                     uint32_t *               pd,
                     const uint32_t *         ps,
                     const uint32_t *         pm,
                     int                      w)
{
    uint32_t s, m, d;

    __m128i xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_alpha_src_lo, xmm_alpha_src_hi;
    __m128i xmm_alpha_dst_lo, xmm_alpha_dst_hi;
    __m128i xmm_mask_lo, xmm_mask_hi;

    while (w && (uintptr_t)pd & 15)
    {
	s = *ps++;
	m = *pm++;
	d = *pd;

	*pd++ = core_combine_xor_ca_pixel_sse2 (s, m, d);
	w--;
    }

    while (w >= 4)
    {
	xmm_dst_hi = load_128_aligned ((__m128i*)pd);
	xmm_src_hi = load_128_unaligned ((__m128i*)ps);
	xmm_mask_hi = load_128_unaligned ((__m128i*)pm);

	unpack_128_2x128 (xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);
	unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	unpack_128_2x128 (xmm_mask_hi, &xmm_mask_lo, &xmm_mask_hi);

	expand_alpha_2x128 (xmm_src_lo, xmm_src_hi,
			    &xmm_alpha_src_lo, &xmm_alpha_src_hi);
	expand_alpha_2x128 (xmm_dst_lo, xmm_dst_hi,
			    &xmm_alpha_dst_lo, &xmm_alpha_dst_hi);

	pix_multiply_2x128 (&xmm_src_lo, &xmm_src_hi,
			    &xmm_mask_lo, &xmm_mask_hi,
			    &xmm_src_lo, &xmm_src_hi);
	pix_multiply_2x128 (&xmm_mask_lo, &xmm_mask_hi,
			    &xmm_alpha_src_lo, &xmm_alpha_src_hi,
			    &xmm_mask_lo, &xmm_mask_hi);

	negate_2x128 (xmm_alpha_dst_lo, xmm_alpha_dst_hi,
		      &xmm_alpha_dst_lo, &xmm_alpha_dst_hi);
	negate_2x128 (xmm_mask_lo, xmm_mask_hi,
		      &xmm_mask_lo, &xmm_mask_hi);

	pix_add_multiply_2x128 (
	    &xmm_dst_lo, &xmm_dst_hi, &xmm_mask_lo, &xmm_mask_hi,
	    &xmm_src_lo, &xmm_src_hi, &xmm_alpha_dst_lo, &xmm_alpha_dst_hi,
	    &xmm_dst_lo, &xmm_dst_hi);

	save_128_aligned (
	    (__m128i*)pd, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	ps += 4;
	pd += 4;
	pm += 4;
	w -= 4;
    }

    while (w)
    {
	s = *ps++;
	m = *pm++;
	d = *pd;

	*pd++ = core_combine_xor_ca_pixel_sse2 (s, m, d);
	w--;
    }
}

static void
sse2_combine_add_ca (pixman_implementation_t *imp,
                     pixman_op_t              op,
                     uint32_t *               pd,
                     const uint32_t *         ps,
                     const uint32_t *         pm,
                     int                      w)
{
    uint32_t s, m, d;

    __m128i xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_mask_lo, xmm_mask_hi;

    while (w && (uintptr_t)pd & 15)
    {
	s = *ps++;
	m = *pm++;
	d = *pd;

	*pd++ = pack_1x128_32 (
	    _mm_adds_epu8 (pix_multiply_1x128 (unpack_32_1x128 (s),
					       unpack_32_1x128 (m)),
			   unpack_32_1x128 (d)));
	w--;
    }

    while (w >= 4)
    {
	xmm_src_hi = load_128_unaligned ((__m128i*)ps);
	xmm_mask_hi = load_128_unaligned ((__m128i*)pm);
	xmm_dst_hi = load_128_aligned ((__m128i*)pd);

	unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	unpack_128_2x128 (xmm_mask_hi, &xmm_mask_lo, &xmm_mask_hi);
	unpack_128_2x128 (xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);

	pix_multiply_2x128 (&xmm_src_lo, &xmm_src_hi,
			    &xmm_mask_lo, &xmm_mask_hi,
			    &xmm_src_lo, &xmm_src_hi);

	save_128_aligned (
	    (__m128i*)pd, pack_2x128_128 (
		_mm_adds_epu8 (xmm_src_lo, xmm_dst_lo),
		_mm_adds_epu8 (xmm_src_hi, xmm_dst_hi)));

	ps += 4;
	pd += 4;
	pm += 4;
	w -= 4;
    }

    while (w)
    {
	s = *ps++;
	m = *pm++;
	d = *pd;

	*pd++ = pack_1x128_32 (
	    _mm_adds_epu8 (pix_multiply_1x128 (unpack_32_1x128 (s),
					       unpack_32_1x128 (m)),
			   unpack_32_1x128 (d)));
	w--;
    }
}

static force_inline __m128i
create_mask_16_128 (uint16_t mask)
{
    return _mm_set1_epi16 (mask);
}

/* Work around a code generation bug in Sun Studio 12. */
#if defined(__SUNPRO_C) && (__SUNPRO_C >= 0x590)
# define create_mask_2x32_128(mask0, mask1)				\
    (_mm_set_epi32 ((mask0), (mask1), (mask0), (mask1)))
#else
static force_inline __m128i
create_mask_2x32_128 (uint32_t mask0,
                      uint32_t mask1)
{
    return _mm_set_epi32 (mask0, mask1, mask0, mask1);
}
#endif

static void
sse2_composite_over_n_8888 (pixman_implementation_t *imp,
                            pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t src;
    uint32_t    *dst_line, *dst, d;
    int32_t w;
    int dst_stride;
    __m128i xmm_src, xmm_alpha;
    __m128i xmm_dst, xmm_dst_lo, xmm_dst_hi;

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    if (src == 0)
	return;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);

    xmm_src = expand_pixel_32_1x128 (src);
    xmm_alpha = expand_alpha_1x128 (xmm_src);

    while (height--)
    {
	dst = dst_line;

	dst_line += dst_stride;
	w = width;

	while (w && (uintptr_t)dst & 15)
	{
	    d = *dst;
	    *dst++ = pack_1x128_32 (over_1x128 (xmm_src,
						xmm_alpha,
						unpack_32_1x128 (d)));
	    w--;
	}

	while (w >= 4)
	{
	    xmm_dst = load_128_aligned ((__m128i*)dst);

	    unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);

	    over_2x128 (&xmm_src, &xmm_src,
			&xmm_alpha, &xmm_alpha,
			&xmm_dst_lo, &xmm_dst_hi);

	    /* rebuid the 4 pixel data and save*/
	    save_128_aligned (
		(__m128i*)dst, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	    w -= 4;
	    dst += 4;
	}

	while (w)
	{
	    d = *dst;
	    *dst++ = pack_1x128_32 (over_1x128 (xmm_src,
						xmm_alpha,
						unpack_32_1x128 (d)));
	    w--;
	}

    }
}

static void
sse2_composite_over_n_0565 (pixman_implementation_t *imp,
                            pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t src;
    uint16_t    *dst_line, *dst, d;
    int32_t w;
    int dst_stride;
    __m128i xmm_src, xmm_alpha;
    __m128i xmm_dst, xmm_dst0, xmm_dst1, xmm_dst2, xmm_dst3;

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    if (src == 0)
	return;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);

    xmm_src = expand_pixel_32_1x128 (src);
    xmm_alpha = expand_alpha_1x128 (xmm_src);

    while (height--)
    {
	dst = dst_line;

	dst_line += dst_stride;
	w = width;

	while (w && (uintptr_t)dst & 15)
	{
	    d = *dst;

	    *dst++ = pack_565_32_16 (
		pack_1x128_32 (over_1x128 (xmm_src,
					   xmm_alpha,
					   expand565_16_1x128 (d))));
	    w--;
	}

	while (w >= 8)
	{
	    xmm_dst = load_128_aligned ((__m128i*)dst);

	    unpack_565_128_4x128 (xmm_dst,
				  &xmm_dst0, &xmm_dst1, &xmm_dst2, &xmm_dst3);

	    over_2x128 (&xmm_src, &xmm_src,
			&xmm_alpha, &xmm_alpha,
			&xmm_dst0, &xmm_dst1);
	    over_2x128 (&xmm_src, &xmm_src,
			&xmm_alpha, &xmm_alpha,
			&xmm_dst2, &xmm_dst3);

	    xmm_dst = pack_565_4x128_128 (
		&xmm_dst0, &xmm_dst1, &xmm_dst2, &xmm_dst3);

	    save_128_aligned ((__m128i*)dst, xmm_dst);

	    dst += 8;
	    w -= 8;
	}

	while (w--)
	{
	    d = *dst;
	    *dst++ = pack_565_32_16 (
		pack_1x128_32 (over_1x128 (xmm_src, xmm_alpha,
					   expand565_16_1x128 (d))));
	}
    }

}

static void
sse2_composite_add_n_8888_8888_ca (pixman_implementation_t *imp,
				   pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t src;
    uint32_t    *dst_line, d;
    uint32_t    *mask_line, m;
    uint32_t pack_cmp;
    int dst_stride, mask_stride;

    __m128i xmm_src;
    __m128i xmm_dst;
    __m128i xmm_mask, xmm_mask_lo, xmm_mask_hi;

    __m128i mmx_src, mmx_mask, mmx_dest;

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    if (src == 0)
	return;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	mask_image, mask_x, mask_y, uint32_t, mask_stride, mask_line, 1);

    xmm_src = _mm_unpacklo_epi8 (
	create_mask_2x32_128 (src, src), _mm_setzero_si128 ());
    mmx_src   = xmm_src;

    while (height--)
    {
	int w = width;
	const uint32_t *pm = (uint32_t *)mask_line;
	uint32_t *pd = (uint32_t *)dst_line;

	dst_line += dst_stride;
	mask_line += mask_stride;

	while (w && (uintptr_t)pd & 15)
	{
	    m = *pm++;

	    if (m)
	    {
		d = *pd;

		mmx_mask = unpack_32_1x128 (m);
		mmx_dest = unpack_32_1x128 (d);

		*pd = pack_1x128_32 (
		    _mm_adds_epu8 (pix_multiply_1x128 (mmx_mask, mmx_src),
				   mmx_dest));
	    }

	    pd++;
	    w--;
	}

	while (w >= 4)
	{
	    xmm_mask = load_128_unaligned ((__m128i*)pm);

	    pack_cmp =
		_mm_movemask_epi8 (
		    _mm_cmpeq_epi32 (xmm_mask, _mm_setzero_si128 ()));

	    /* if all bits in mask are zero, pack_cmp are equal to 0xffff */
	    if (pack_cmp != 0xffff)
	    {
		xmm_dst = load_128_aligned ((__m128i*)pd);

		unpack_128_2x128 (xmm_mask, &xmm_mask_lo, &xmm_mask_hi);

		pix_multiply_2x128 (&xmm_src, &xmm_src,
				    &xmm_mask_lo, &xmm_mask_hi,
				    &xmm_mask_lo, &xmm_mask_hi);
		xmm_mask_hi = pack_2x128_128 (xmm_mask_lo, xmm_mask_hi);

		save_128_aligned (
		    (__m128i*)pd, _mm_adds_epu8 (xmm_mask_hi, xmm_dst));
	    }

	    pd += 4;
	    pm += 4;
	    w -= 4;
	}

	while (w)
	{
	    m = *pm++;

	    if (m)
	    {
		d = *pd;

		mmx_mask = unpack_32_1x128 (m);
		mmx_dest = unpack_32_1x128 (d);

		*pd = pack_1x128_32 (
		    _mm_adds_epu8 (pix_multiply_1x128 (mmx_mask, mmx_src),
				   mmx_dest));
	    }

	    pd++;
	    w--;
	}
    }

}

static void
sse2_composite_over_n_8888_8888_ca (pixman_implementation_t *imp,
                                    pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t src;
    uint32_t    *dst_line, d;
    uint32_t    *mask_line, m;
    uint32_t pack_cmp;
    int dst_stride, mask_stride;

    __m128i xmm_src, xmm_alpha;
    __m128i xmm_dst, xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_mask, xmm_mask_lo, xmm_mask_hi;

    __m128i mmx_src, mmx_alpha, mmx_mask, mmx_dest;

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    if (src == 0)
	return;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	mask_image, mask_x, mask_y, uint32_t, mask_stride, mask_line, 1);

    xmm_src = _mm_unpacklo_epi8 (
	create_mask_2x32_128 (src, src), _mm_setzero_si128 ());
    xmm_alpha = expand_alpha_1x128 (xmm_src);
    mmx_src   = xmm_src;
    mmx_alpha = xmm_alpha;

    while (height--)
    {
	int w = width;
	const uint32_t *pm = (uint32_t *)mask_line;
	uint32_t *pd = (uint32_t *)dst_line;

	dst_line += dst_stride;
	mask_line += mask_stride;

	while (w && (uintptr_t)pd & 15)
	{
	    m = *pm++;

	    if (m)
	    {
		d = *pd;
		mmx_mask = unpack_32_1x128 (m);
		mmx_dest = unpack_32_1x128 (d);

		*pd = pack_1x128_32 (in_over_1x128 (&mmx_src,
		                                  &mmx_alpha,
		                                  &mmx_mask,
		                                  &mmx_dest));
	    }

	    pd++;
	    w--;
	}

	while (w >= 4)
	{
	    xmm_mask = load_128_unaligned ((__m128i*)pm);

	    pack_cmp =
		_mm_movemask_epi8 (
		    _mm_cmpeq_epi32 (xmm_mask, _mm_setzero_si128 ()));

	    /* if all bits in mask are zero, pack_cmp are equal to 0xffff */
	    if (pack_cmp != 0xffff)
	    {
		xmm_dst = load_128_aligned ((__m128i*)pd);

		unpack_128_2x128 (xmm_mask, &xmm_mask_lo, &xmm_mask_hi);
		unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);

		in_over_2x128 (&xmm_src, &xmm_src,
			       &xmm_alpha, &xmm_alpha,
			       &xmm_mask_lo, &xmm_mask_hi,
			       &xmm_dst_lo, &xmm_dst_hi);

		save_128_aligned (
		    (__m128i*)pd, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));
	    }

	    pd += 4;
	    pm += 4;
	    w -= 4;
	}

	while (w)
	{
	    m = *pm++;

	    if (m)
	    {
		d = *pd;
		mmx_mask = unpack_32_1x128 (m);
		mmx_dest = unpack_32_1x128 (d);

		*pd = pack_1x128_32 (
		    in_over_1x128 (&mmx_src, &mmx_alpha, &mmx_mask, &mmx_dest));
	    }

	    pd++;
	    w--;
	}
    }

}

static void
sse2_composite_over_8888_n_8888 (pixman_implementation_t *imp,
                                 pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t    *dst_line, *dst;
    uint32_t    *src_line, *src;
    uint32_t mask;
    int32_t w;
    int dst_stride, src_stride;

    __m128i xmm_mask;
    __m128i xmm_src, xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst, xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_alpha_lo, xmm_alpha_hi;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);

    mask = _pixman_image_get_solid (imp, mask_image, PIXMAN_a8r8g8b8);

    xmm_mask = create_mask_16_128 (mask >> 24);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;
	w = width;

	while (w && (uintptr_t)dst & 15)
	{
	    uint32_t s = *src++;

	    if (s)
	    {
		uint32_t d = *dst;
		
		__m128i ms = unpack_32_1x128 (s);
		__m128i alpha    = expand_alpha_1x128 (ms);
		__m128i dest     = xmm_mask;
		__m128i alpha_dst = unpack_32_1x128 (d);
		
		*dst = pack_1x128_32 (
		    in_over_1x128 (&ms, &alpha, &dest, &alpha_dst));
	    }
	    dst++;
	    w--;
	}

	while (w >= 4)
	{
	    xmm_src = load_128_unaligned ((__m128i*)src);

	    if (!is_zero (xmm_src))
	    {
		xmm_dst = load_128_aligned ((__m128i*)dst);
		
		unpack_128_2x128 (xmm_src, &xmm_src_lo, &xmm_src_hi);
		unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);
		expand_alpha_2x128 (xmm_src_lo, xmm_src_hi,
				    &xmm_alpha_lo, &xmm_alpha_hi);
		
		in_over_2x128 (&xmm_src_lo, &xmm_src_hi,
			       &xmm_alpha_lo, &xmm_alpha_hi,
			       &xmm_mask, &xmm_mask,
			       &xmm_dst_lo, &xmm_dst_hi);
		
		save_128_aligned (
		    (__m128i*)dst, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));
	    }
		
	    dst += 4;
	    src += 4;
	    w -= 4;
	}

	while (w)
	{
	    uint32_t s = *src++;

	    if (s)
	    {
		uint32_t d = *dst;
		
		__m128i ms = unpack_32_1x128 (s);
		__m128i alpha = expand_alpha_1x128 (ms);
		__m128i mask  = xmm_mask;
		__m128i dest  = unpack_32_1x128 (d);
		
		*dst = pack_1x128_32 (
		    in_over_1x128 (&ms, &alpha, &mask, &dest));
	    }

	    dst++;
	    w--;
	}
    }

}

static void
sse2_composite_src_x888_0565 (pixman_implementation_t *imp,
                              pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint16_t    *dst_line, *dst;
    uint32_t    *src_line, *src, s;
    int dst_stride, src_stride;
    int32_t w;

    PIXMAN_IMAGE_GET_LINE (src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);
    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;
	w = width;

	while (w && (uintptr_t)dst & 15)
	{
	    s = *src++;
	    *dst = convert_8888_to_0565 (s);
	    dst++;
	    w--;
	}

	while (w >= 8)
	{
	    __m128i xmm_src0 = load_128_unaligned ((__m128i *)src + 0);
	    __m128i xmm_src1 = load_128_unaligned ((__m128i *)src + 1);

	    save_128_aligned ((__m128i*)dst, pack_565_2packedx128_128 (xmm_src0, xmm_src1));

	    w -= 8;
	    src += 8;
	    dst += 8;
	}

	while (w)
	{
	    s = *src++;
	    *dst = convert_8888_to_0565 (s);
	    dst++;
	    w--;
	}
    }
}

static void
sse2_composite_src_x888_8888 (pixman_implementation_t *imp,
			      pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t    *dst_line, *dst;
    uint32_t    *src_line, *src;
    int32_t w;
    int dst_stride, src_stride;


    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;
	w = width;

	while (w && (uintptr_t)dst & 15)
	{
	    *dst++ = *src++ | 0xff000000;
	    w--;
	}

	while (w >= 16)
	{
	    __m128i xmm_src1, xmm_src2, xmm_src3, xmm_src4;
	    
	    xmm_src1 = load_128_unaligned ((__m128i*)src + 0);
	    xmm_src2 = load_128_unaligned ((__m128i*)src + 1);
	    xmm_src3 = load_128_unaligned ((__m128i*)src + 2);
	    xmm_src4 = load_128_unaligned ((__m128i*)src + 3);
	    
	    save_128_aligned ((__m128i*)dst + 0, _mm_or_si128 (xmm_src1, mask_ff000000));
	    save_128_aligned ((__m128i*)dst + 1, _mm_or_si128 (xmm_src2, mask_ff000000));
	    save_128_aligned ((__m128i*)dst + 2, _mm_or_si128 (xmm_src3, mask_ff000000));
	    save_128_aligned ((__m128i*)dst + 3, _mm_or_si128 (xmm_src4, mask_ff000000));
	    
	    dst += 16;
	    src += 16;
	    w -= 16;
	}

	while (w)
	{
	    *dst++ = *src++ | 0xff000000;
	    w--;
	}
    }

}

static void
sse2_composite_over_x888_n_8888 (pixman_implementation_t *imp,
                                 pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t    *dst_line, *dst;
    uint32_t    *src_line, *src;
    uint32_t mask;
    int dst_stride, src_stride;
    int32_t w;

    __m128i xmm_mask, xmm_alpha;
    __m128i xmm_src, xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst, xmm_dst_lo, xmm_dst_hi;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);

    mask = _pixman_image_get_solid (imp, mask_image, PIXMAN_a8r8g8b8);

    xmm_mask = create_mask_16_128 (mask >> 24);
    xmm_alpha = mask_00ff;

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;
	w = width;

	while (w && (uintptr_t)dst & 15)
	{
	    uint32_t s = (*src++) | 0xff000000;
	    uint32_t d = *dst;

	    __m128i src   = unpack_32_1x128 (s);
	    __m128i alpha = xmm_alpha;
	    __m128i mask  = xmm_mask;
	    __m128i dest  = unpack_32_1x128 (d);

	    *dst++ = pack_1x128_32 (
		in_over_1x128 (&src, &alpha, &mask, &dest));

	    w--;
	}

	while (w >= 4)
	{
	    xmm_src = _mm_or_si128 (
		load_128_unaligned ((__m128i*)src), mask_ff000000);
	    xmm_dst = load_128_aligned ((__m128i*)dst);

	    unpack_128_2x128 (xmm_src, &xmm_src_lo, &xmm_src_hi);
	    unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);

	    in_over_2x128 (&xmm_src_lo, &xmm_src_hi,
			   &xmm_alpha, &xmm_alpha,
			   &xmm_mask, &xmm_mask,
			   &xmm_dst_lo, &xmm_dst_hi);

	    save_128_aligned (
		(__m128i*)dst, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	    dst += 4;
	    src += 4;
	    w -= 4;

	}

	while (w)
	{
	    uint32_t s = (*src++) | 0xff000000;
	    uint32_t d = *dst;

	    __m128i src  = unpack_32_1x128 (s);
	    __m128i alpha = xmm_alpha;
	    __m128i mask  = xmm_mask;
	    __m128i dest  = unpack_32_1x128 (d);

	    *dst++ = pack_1x128_32 (
		in_over_1x128 (&src, &alpha, &mask, &dest));

	    w--;
	}
    }

}

static void
sse2_composite_over_8888_8888 (pixman_implementation_t *imp,
                               pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    int dst_stride, src_stride;
    uint32_t    *dst_line, *dst;
    uint32_t    *src_line, *src;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);

    dst = dst_line;
    src = src_line;

    while (height--)
    {
	sse2_combine_over_u (imp, op, dst, src, NULL, width);

	dst += dst_stride;
	src += src_stride;
    }
}

static force_inline uint16_t
composite_over_8888_0565pixel (uint32_t src, uint16_t dst)
{
    __m128i ms;

    ms = unpack_32_1x128 (src);
    return pack_565_32_16 (
	pack_1x128_32 (
	    over_1x128 (
		ms, expand_alpha_1x128 (ms), expand565_16_1x128 (dst))));
}

static void
sse2_composite_over_8888_0565 (pixman_implementation_t *imp,
                               pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint16_t    *dst_line, *dst, d;
    uint32_t    *src_line, *src, s;
    int dst_stride, src_stride;
    int32_t w;

    __m128i xmm_alpha_lo, xmm_alpha_hi;
    __m128i xmm_src, xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst, xmm_dst0, xmm_dst1, xmm_dst2, xmm_dst3;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);

    while (height--)
    {
	dst = dst_line;
	src = src_line;

	dst_line += dst_stride;
	src_line += src_stride;
	w = width;

	/* Align dst on a 16-byte boundary */
	while (w &&
	       ((uintptr_t)dst & 15))
	{
	    s = *src++;
	    d = *dst;

	    *dst++ = composite_over_8888_0565pixel (s, d);
	    w--;
	}

	/* It's a 8 pixel loop */
	while (w >= 8)
	{
	    /* I'm loading unaligned because I'm not sure
	     * about the address alignment.
	     */
	    xmm_src = load_128_unaligned ((__m128i*) src);
	    xmm_dst = load_128_aligned ((__m128i*) dst);

	    /* Unpacking */
	    unpack_128_2x128 (xmm_src, &xmm_src_lo, &xmm_src_hi);
	    unpack_565_128_4x128 (xmm_dst,
				  &xmm_dst0, &xmm_dst1, &xmm_dst2, &xmm_dst3);
	    expand_alpha_2x128 (xmm_src_lo, xmm_src_hi,
				&xmm_alpha_lo, &xmm_alpha_hi);

	    /* I'm loading next 4 pixels from memory
	     * before to optimze the memory read.
	     */
	    xmm_src = load_128_unaligned ((__m128i*) (src + 4));

	    over_2x128 (&xmm_src_lo, &xmm_src_hi,
			&xmm_alpha_lo, &xmm_alpha_hi,
			&xmm_dst0, &xmm_dst1);

	    /* Unpacking */
	    unpack_128_2x128 (xmm_src, &xmm_src_lo, &xmm_src_hi);
	    expand_alpha_2x128 (xmm_src_lo, xmm_src_hi,
				&xmm_alpha_lo, &xmm_alpha_hi);

	    over_2x128 (&xmm_src_lo, &xmm_src_hi,
			&xmm_alpha_lo, &xmm_alpha_hi,
			&xmm_dst2, &xmm_dst3);

	    save_128_aligned (
		(__m128i*)dst, pack_565_4x128_128 (
		    &xmm_dst0, &xmm_dst1, &xmm_dst2, &xmm_dst3));

	    w -= 8;
	    dst += 8;
	    src += 8;
	}

	while (w--)
	{
	    s = *src++;
	    d = *dst;

	    *dst++ = composite_over_8888_0565pixel (s, d);
	}
    }

}

static void
sse2_composite_over_n_8_8888 (pixman_implementation_t *imp,
                              pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t src, srca;
    uint32_t *dst_line, *dst;
    uint8_t *mask_line, *mask;
    int dst_stride, mask_stride;
    int32_t w;
    uint32_t m, d;

    __m128i xmm_src, xmm_alpha, xmm_def;
    __m128i xmm_dst, xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_mask, xmm_mask_lo, xmm_mask_hi;

    __m128i mmx_src, mmx_alpha, mmx_mask, mmx_dest;

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    srca = src >> 24;
    if (src == 0)
	return;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	mask_image, mask_x, mask_y, uint8_t, mask_stride, mask_line, 1);

    xmm_def = create_mask_2x32_128 (src, src);
    xmm_src = expand_pixel_32_1x128 (src);
    xmm_alpha = expand_alpha_1x128 (xmm_src);
    mmx_src   = xmm_src;
    mmx_alpha = xmm_alpha;

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	mask = mask_line;
	mask_line += mask_stride;
	w = width;

	while (w && (uintptr_t)dst & 15)
	{
	    uint8_t m = *mask++;

	    if (m)
	    {
		d = *dst;
		mmx_mask = expand_pixel_8_1x128 (m);
		mmx_dest = unpack_32_1x128 (d);

		*dst = pack_1x128_32 (in_over_1x128 (&mmx_src,
		                                   &mmx_alpha,
		                                   &mmx_mask,
		                                   &mmx_dest));
	    }

	    w--;
	    dst++;
	}

	while (w >= 4)
	{
	    m = *((uint32_t*)mask);

	    if (srca == 0xff && m == 0xffffffff)
	    {
		save_128_aligned ((__m128i*)dst, xmm_def);
	    }
	    else if (m)
	    {
		xmm_dst = load_128_aligned ((__m128i*) dst);
		xmm_mask = unpack_32_1x128 (m);
		xmm_mask = _mm_unpacklo_epi8 (xmm_mask, _mm_setzero_si128 ());

		/* Unpacking */
		unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);
		unpack_128_2x128 (xmm_mask, &xmm_mask_lo, &xmm_mask_hi);

		expand_alpha_rev_2x128 (xmm_mask_lo, xmm_mask_hi,
					&xmm_mask_lo, &xmm_mask_hi);

		in_over_2x128 (&xmm_src, &xmm_src,
			       &xmm_alpha, &xmm_alpha,
			       &xmm_mask_lo, &xmm_mask_hi,
			       &xmm_dst_lo, &xmm_dst_hi);

		save_128_aligned (
		    (__m128i*)dst, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));
	    }

	    w -= 4;
	    dst += 4;
	    mask += 4;
	}

	while (w)
	{
	    uint8_t m = *mask++;

	    if (m)
	    {
		d = *dst;
		mmx_mask = expand_pixel_8_1x128 (m);
		mmx_dest = unpack_32_1x128 (d);

		*dst = pack_1x128_32 (in_over_1x128 (&mmx_src,
		                                   &mmx_alpha,
		                                   &mmx_mask,
		                                   &mmx_dest));
	    }

	    w--;
	    dst++;
	}
    }

}

#if defined(__GNUC__) && !defined(__x86_64__) && !defined(__amd64__)
__attribute__((__force_align_arg_pointer__))
#endif
static pixman_bool_t
sse2_fill (pixman_implementation_t *imp,
           uint32_t *               bits,
           int                      stride,
           int                      bpp,
           int                      x,
           int                      y,
           int                      width,
           int                      height,
           uint32_t		    filler)
{
    uint32_t byte_width;
    uint8_t *byte_line;

    __m128i xmm_def;

    if (bpp == 8)
    {
	uint8_t b;
	uint16_t w;

	stride = stride * (int) sizeof (uint32_t) / 1;
	byte_line = (uint8_t *)(((uint8_t *)bits) + stride * y + x);
	byte_width = width;
	stride *= 1;

	b = filler & 0xff;
	w = (b << 8) | b;
	filler = (w << 16) | w;
    }
    else if (bpp == 16)
    {
	stride = stride * (int) sizeof (uint32_t) / 2;
	byte_line = (uint8_t *)(((uint16_t *)bits) + stride * y + x);
	byte_width = 2 * width;
	stride *= 2;

        filler = (filler & 0xffff) * 0x00010001;
    }
    else if (bpp == 32)
    {
	stride = stride * (int) sizeof (uint32_t) / 4;
	byte_line = (uint8_t *)(((uint32_t *)bits) + stride * y + x);
	byte_width = 4 * width;
	stride *= 4;
    }
    else
    {
	return FALSE;
    }

    xmm_def = create_mask_2x32_128 (filler, filler);

    while (height--)
    {
	int w;
	uint8_t *d = byte_line;
	byte_line += stride;
	w = byte_width;

	if (w >= 1 && ((uintptr_t)d & 1))
	{
	    *(uint8_t *)d = filler;
	    w -= 1;
	    d += 1;
	}

	while (w >= 2 && ((uintptr_t)d & 3))
	{
	    *(uint16_t *)d = filler;
	    w -= 2;
	    d += 2;
	}

	while (w >= 4 && ((uintptr_t)d & 15))
	{
	    *(uint32_t *)d = filler;

	    w -= 4;
	    d += 4;
	}

	while (w >= 128)
	{
	    save_128_aligned ((__m128i*)(d),     xmm_def);
	    save_128_aligned ((__m128i*)(d + 16),  xmm_def);
	    save_128_aligned ((__m128i*)(d + 32),  xmm_def);
	    save_128_aligned ((__m128i*)(d + 48),  xmm_def);
	    save_128_aligned ((__m128i*)(d + 64),  xmm_def);
	    save_128_aligned ((__m128i*)(d + 80),  xmm_def);
	    save_128_aligned ((__m128i*)(d + 96),  xmm_def);
	    save_128_aligned ((__m128i*)(d + 112), xmm_def);

	    d += 128;
	    w -= 128;
	}

	if (w >= 64)
	{
	    save_128_aligned ((__m128i*)(d),     xmm_def);
	    save_128_aligned ((__m128i*)(d + 16),  xmm_def);
	    save_128_aligned ((__m128i*)(d + 32),  xmm_def);
	    save_128_aligned ((__m128i*)(d + 48),  xmm_def);

	    d += 64;
	    w -= 64;
	}

	if (w >= 32)
	{
	    save_128_aligned ((__m128i*)(d),     xmm_def);
	    save_128_aligned ((__m128i*)(d + 16),  xmm_def);

	    d += 32;
	    w -= 32;
	}

	if (w >= 16)
	{
	    save_128_aligned ((__m128i*)(d),     xmm_def);

	    d += 16;
	    w -= 16;
	}

	while (w >= 4)
	{
	    *(uint32_t *)d = filler;

	    w -= 4;
	    d += 4;
	}

	if (w >= 2)
	{
	    *(uint16_t *)d = filler;
	    w -= 2;
	    d += 2;
	}

	if (w >= 1)
	{
	    *(uint8_t *)d = filler;
	    w -= 1;
	    d += 1;
	}
    }

    return TRUE;
}

static void
sse2_composite_src_n_8_8888 (pixman_implementation_t *imp,
                             pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t src, srca;
    uint32_t    *dst_line, *dst;
    uint8_t     *mask_line, *mask;
    int dst_stride, mask_stride;
    int32_t w;
    uint32_t m;

    __m128i xmm_src, xmm_def;
    __m128i xmm_mask, xmm_mask_lo, xmm_mask_hi;

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    srca = src >> 24;
    if (src == 0)
    {
	sse2_fill (imp, dest_image->bits.bits, dest_image->bits.rowstride,
		   PIXMAN_FORMAT_BPP (dest_image->bits.format),
		   dest_x, dest_y, width, height, 0);
	return;
    }

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	mask_image, mask_x, mask_y, uint8_t, mask_stride, mask_line, 1);

    xmm_def = create_mask_2x32_128 (src, src);
    xmm_src = expand_pixel_32_1x128 (src);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	mask = mask_line;
	mask_line += mask_stride;
	w = width;

	while (w && (uintptr_t)dst & 15)
	{
	    uint8_t m = *mask++;

	    if (m)
	    {
		*dst = pack_1x128_32 (
		    pix_multiply_1x128 (xmm_src, expand_pixel_8_1x128 (m)));
	    }
	    else
	    {
		*dst = 0;
	    }

	    w--;
	    dst++;
	}

	while (w >= 4)
	{
	    m = *((uint32_t*)mask);

	    if (srca == 0xff && m == 0xffffffff)
	    {
		save_128_aligned ((__m128i*)dst, xmm_def);
	    }
	    else if (m)
	    {
		xmm_mask = unpack_32_1x128 (m);
		xmm_mask = _mm_unpacklo_epi8 (xmm_mask, _mm_setzero_si128 ());

		/* Unpacking */
		unpack_128_2x128 (xmm_mask, &xmm_mask_lo, &xmm_mask_hi);

		expand_alpha_rev_2x128 (xmm_mask_lo, xmm_mask_hi,
					&xmm_mask_lo, &xmm_mask_hi);

		pix_multiply_2x128 (&xmm_src, &xmm_src,
				    &xmm_mask_lo, &xmm_mask_hi,
				    &xmm_mask_lo, &xmm_mask_hi);

		save_128_aligned (
		    (__m128i*)dst, pack_2x128_128 (xmm_mask_lo, xmm_mask_hi));
	    }
	    else
	    {
		save_128_aligned ((__m128i*)dst, _mm_setzero_si128 ());
	    }

	    w -= 4;
	    dst += 4;
	    mask += 4;
	}

	while (w)
	{
	    uint8_t m = *mask++;

	    if (m)
	    {
		*dst = pack_1x128_32 (
		    pix_multiply_1x128 (
			xmm_src, expand_pixel_8_1x128 (m)));
	    }
	    else
	    {
		*dst = 0;
	    }

	    w--;
	    dst++;
	}
    }

}

static void
sse2_composite_over_n_8_0565 (pixman_implementation_t *imp,
                              pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t src;
    uint16_t    *dst_line, *dst, d;
    uint8_t     *mask_line, *mask;
    int dst_stride, mask_stride;
    int32_t w;
    uint32_t m;
    __m128i mmx_src, mmx_alpha, mmx_mask, mmx_dest;

    __m128i xmm_src, xmm_alpha;
    __m128i xmm_mask, xmm_mask_lo, xmm_mask_hi;
    __m128i xmm_dst, xmm_dst0, xmm_dst1, xmm_dst2, xmm_dst3;

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    if (src == 0)
	return;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	mask_image, mask_x, mask_y, uint8_t, mask_stride, mask_line, 1);

    xmm_src = expand_pixel_32_1x128 (src);
    xmm_alpha = expand_alpha_1x128 (xmm_src);
    mmx_src = xmm_src;
    mmx_alpha = xmm_alpha;

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	mask = mask_line;
	mask_line += mask_stride;
	w = width;

	while (w && (uintptr_t)dst & 15)
	{
	    m = *mask++;

	    if (m)
	    {
		d = *dst;
		mmx_mask = expand_alpha_rev_1x128 (unpack_32_1x128 (m));
		mmx_dest = expand565_16_1x128 (d);

		*dst = pack_565_32_16 (
		    pack_1x128_32 (
			in_over_1x128 (
			    &mmx_src, &mmx_alpha, &mmx_mask, &mmx_dest)));
	    }

	    w--;
	    dst++;
	}

	while (w >= 8)
	{
	    xmm_dst = load_128_aligned ((__m128i*) dst);
	    unpack_565_128_4x128 (xmm_dst,
				  &xmm_dst0, &xmm_dst1, &xmm_dst2, &xmm_dst3);

	    m = *((uint32_t*)mask);
	    mask += 4;

	    if (m)
	    {
		xmm_mask = unpack_32_1x128 (m);
		xmm_mask = _mm_unpacklo_epi8 (xmm_mask, _mm_setzero_si128 ());

		/* Unpacking */
		unpack_128_2x128 (xmm_mask, &xmm_mask_lo, &xmm_mask_hi);

		expand_alpha_rev_2x128 (xmm_mask_lo, xmm_mask_hi,
					&xmm_mask_lo, &xmm_mask_hi);

		in_over_2x128 (&xmm_src, &xmm_src,
			       &xmm_alpha, &xmm_alpha,
			       &xmm_mask_lo, &xmm_mask_hi,
			       &xmm_dst0, &xmm_dst1);
	    }

	    m = *((uint32_t*)mask);
	    mask += 4;

	    if (m)
	    {
		xmm_mask = unpack_32_1x128 (m);
		xmm_mask = _mm_unpacklo_epi8 (xmm_mask, _mm_setzero_si128 ());

		/* Unpacking */
		unpack_128_2x128 (xmm_mask, &xmm_mask_lo, &xmm_mask_hi);

		expand_alpha_rev_2x128 (xmm_mask_lo, xmm_mask_hi,
					&xmm_mask_lo, &xmm_mask_hi);
		in_over_2x128 (&xmm_src, &xmm_src,
			       &xmm_alpha, &xmm_alpha,
			       &xmm_mask_lo, &xmm_mask_hi,
			       &xmm_dst2, &xmm_dst3);
	    }

	    save_128_aligned (
		(__m128i*)dst, pack_565_4x128_128 (
		    &xmm_dst0, &xmm_dst1, &xmm_dst2, &xmm_dst3));

	    w -= 8;
	    dst += 8;
	}

	while (w)
	{
	    m = *mask++;

	    if (m)
	    {
		d = *dst;
		mmx_mask = expand_alpha_rev_1x128 (unpack_32_1x128 (m));
		mmx_dest = expand565_16_1x128 (d);

		*dst = pack_565_32_16 (
		    pack_1x128_32 (
			in_over_1x128 (
			    &mmx_src, &mmx_alpha, &mmx_mask, &mmx_dest)));
	    }

	    w--;
	    dst++;
	}
    }

}

static void
sse2_composite_over_pixbuf_0565 (pixman_implementation_t *imp,
                                 pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint16_t    *dst_line, *dst, d;
    uint32_t    *src_line, *src, s;
    int dst_stride, src_stride;
    int32_t w;
    uint32_t opaque, zero;

    __m128i ms;
    __m128i xmm_src, xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst, xmm_dst0, xmm_dst1, xmm_dst2, xmm_dst3;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;
	w = width;

	while (w && (uintptr_t)dst & 15)
	{
	    s = *src++;
	    d = *dst;

	    ms = unpack_32_1x128 (s);

	    *dst++ = pack_565_32_16 (
		pack_1x128_32 (
		    over_rev_non_pre_1x128 (ms, expand565_16_1x128 (d))));
	    w--;
	}

	while (w >= 8)
	{
	    /* First round */
	    xmm_src = load_128_unaligned ((__m128i*)src);
	    xmm_dst = load_128_aligned  ((__m128i*)dst);

	    opaque = is_opaque (xmm_src);
	    zero = is_zero (xmm_src);

	    unpack_565_128_4x128 (xmm_dst,
				  &xmm_dst0, &xmm_dst1, &xmm_dst2, &xmm_dst3);
	    unpack_128_2x128 (xmm_src, &xmm_src_lo, &xmm_src_hi);

	    /* preload next round*/
	    xmm_src = load_128_unaligned ((__m128i*)(src + 4));

	    if (opaque)
	    {
		invert_colors_2x128 (xmm_src_lo, xmm_src_hi,
				     &xmm_dst0, &xmm_dst1);
	    }
	    else if (!zero)
	    {
		over_rev_non_pre_2x128 (xmm_src_lo, xmm_src_hi,
					&xmm_dst0, &xmm_dst1);
	    }

	    /* Second round */
	    opaque = is_opaque (xmm_src);
	    zero = is_zero (xmm_src);

	    unpack_128_2x128 (xmm_src, &xmm_src_lo, &xmm_src_hi);

	    if (opaque)
	    {
		invert_colors_2x128 (xmm_src_lo, xmm_src_hi,
				     &xmm_dst2, &xmm_dst3);
	    }
	    else if (!zero)
	    {
		over_rev_non_pre_2x128 (xmm_src_lo, xmm_src_hi,
					&xmm_dst2, &xmm_dst3);
	    }

	    save_128_aligned (
		(__m128i*)dst, pack_565_4x128_128 (
		    &xmm_dst0, &xmm_dst1, &xmm_dst2, &xmm_dst3));

	    w -= 8;
	    src += 8;
	    dst += 8;
	}

	while (w)
	{
	    s = *src++;
	    d = *dst;

	    ms = unpack_32_1x128 (s);

	    *dst++ = pack_565_32_16 (
		pack_1x128_32 (
		    over_rev_non_pre_1x128 (ms, expand565_16_1x128 (d))));
	    w--;
	}
    }

}

static void
sse2_composite_over_pixbuf_8888 (pixman_implementation_t *imp,
                                 pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t    *dst_line, *dst, d;
    uint32_t    *src_line, *src, s;
    int dst_stride, src_stride;
    int32_t w;
    uint32_t opaque, zero;

    __m128i xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst_lo, xmm_dst_hi;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;
	w = width;

	while (w && (uintptr_t)dst & 15)
	{
	    s = *src++;
	    d = *dst;

	    *dst++ = pack_1x128_32 (
		over_rev_non_pre_1x128 (
		    unpack_32_1x128 (s), unpack_32_1x128 (d)));

	    w--;
	}

	while (w >= 4)
	{
	    xmm_src_hi = load_128_unaligned ((__m128i*)src);

	    opaque = is_opaque (xmm_src_hi);
	    zero = is_zero (xmm_src_hi);

	    unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);

	    if (opaque)
	    {
		invert_colors_2x128 (xmm_src_lo, xmm_src_hi,
				     &xmm_dst_lo, &xmm_dst_hi);

		save_128_aligned (
		    (__m128i*)dst, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));
	    }
	    else if (!zero)
	    {
		xmm_dst_hi = load_128_aligned  ((__m128i*)dst);

		unpack_128_2x128 (xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);

		over_rev_non_pre_2x128 (xmm_src_lo, xmm_src_hi,
					&xmm_dst_lo, &xmm_dst_hi);

		save_128_aligned (
		    (__m128i*)dst, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));
	    }

	    w -= 4;
	    dst += 4;
	    src += 4;
	}

	while (w)
	{
	    s = *src++;
	    d = *dst;

	    *dst++ = pack_1x128_32 (
		over_rev_non_pre_1x128 (
		    unpack_32_1x128 (s), unpack_32_1x128 (d)));

	    w--;
	}
    }

}

static void
sse2_composite_over_n_8888_0565_ca (pixman_implementation_t *imp,
                                    pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t src;
    uint16_t    *dst_line, *dst, d;
    uint32_t    *mask_line, *mask, m;
    int dst_stride, mask_stride;
    int w;
    uint32_t pack_cmp;

    __m128i xmm_src, xmm_alpha;
    __m128i xmm_mask, xmm_mask_lo, xmm_mask_hi;
    __m128i xmm_dst, xmm_dst0, xmm_dst1, xmm_dst2, xmm_dst3;

    __m128i mmx_src, mmx_alpha, mmx_mask, mmx_dest;

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    if (src == 0)
	return;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	mask_image, mask_x, mask_y, uint32_t, mask_stride, mask_line, 1);

    xmm_src = expand_pixel_32_1x128 (src);
    xmm_alpha = expand_alpha_1x128 (xmm_src);
    mmx_src = xmm_src;
    mmx_alpha = xmm_alpha;

    while (height--)
    {
	w = width;
	mask = mask_line;
	dst = dst_line;
	mask_line += mask_stride;
	dst_line += dst_stride;

	while (w && ((uintptr_t)dst & 15))
	{
	    m = *(uint32_t *) mask;

	    if (m)
	    {
		d = *dst;
		mmx_mask = unpack_32_1x128 (m);
		mmx_dest = expand565_16_1x128 (d);

		*dst = pack_565_32_16 (
		    pack_1x128_32 (
			in_over_1x128 (
			    &mmx_src, &mmx_alpha, &mmx_mask, &mmx_dest)));
	    }

	    w--;
	    dst++;
	    mask++;
	}

	while (w >= 8)
	{
	    /* First round */
	    xmm_mask = load_128_unaligned ((__m128i*)mask);
	    xmm_dst = load_128_aligned ((__m128i*)dst);

	    pack_cmp = _mm_movemask_epi8 (
		_mm_cmpeq_epi32 (xmm_mask, _mm_setzero_si128 ()));

	    unpack_565_128_4x128 (xmm_dst,
				  &xmm_dst0, &xmm_dst1, &xmm_dst2, &xmm_dst3);
	    unpack_128_2x128 (xmm_mask, &xmm_mask_lo, &xmm_mask_hi);

	    /* preload next round */
	    xmm_mask = load_128_unaligned ((__m128i*)(mask + 4));

	    /* preload next round */
	    if (pack_cmp != 0xffff)
	    {
		in_over_2x128 (&xmm_src, &xmm_src,
			       &xmm_alpha, &xmm_alpha,
			       &xmm_mask_lo, &xmm_mask_hi,
			       &xmm_dst0, &xmm_dst1);
	    }

	    /* Second round */
	    pack_cmp = _mm_movemask_epi8 (
		_mm_cmpeq_epi32 (xmm_mask, _mm_setzero_si128 ()));

	    unpack_128_2x128 (xmm_mask, &xmm_mask_lo, &xmm_mask_hi);

	    if (pack_cmp != 0xffff)
	    {
		in_over_2x128 (&xmm_src, &xmm_src,
			       &xmm_alpha, &xmm_alpha,
			       &xmm_mask_lo, &xmm_mask_hi,
			       &xmm_dst2, &xmm_dst3);
	    }

	    save_128_aligned (
		(__m128i*)dst, pack_565_4x128_128 (
		    &xmm_dst0, &xmm_dst1, &xmm_dst2, &xmm_dst3));

	    w -= 8;
	    dst += 8;
	    mask += 8;
	}

	while (w)
	{
	    m = *(uint32_t *) mask;

	    if (m)
	    {
		d = *dst;
		mmx_mask = unpack_32_1x128 (m);
		mmx_dest = expand565_16_1x128 (d);

		*dst = pack_565_32_16 (
		    pack_1x128_32 (
			in_over_1x128 (
			    &mmx_src, &mmx_alpha, &mmx_mask, &mmx_dest)));
	    }

	    w--;
	    dst++;
	    mask++;
	}
    }

}

static void
sse2_composite_in_n_8_8 (pixman_implementation_t *imp,
                         pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint8_t     *dst_line, *dst;
    uint8_t     *mask_line, *mask;
    int dst_stride, mask_stride;
    uint32_t d, m;
    uint32_t src;
    int32_t w;

    __m128i xmm_alpha;
    __m128i xmm_mask, xmm_mask_lo, xmm_mask_hi;
    __m128i xmm_dst, xmm_dst_lo, xmm_dst_hi;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	mask_image, mask_x, mask_y, uint8_t, mask_stride, mask_line, 1);

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    xmm_alpha = expand_alpha_1x128 (expand_pixel_32_1x128 (src));

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	mask = mask_line;
	mask_line += mask_stride;
	w = width;

	while (w && ((uintptr_t)dst & 15))
	{
	    m = (uint32_t) *mask++;
	    d = (uint32_t) *dst;

	    *dst++ = (uint8_t) pack_1x128_32 (
		pix_multiply_1x128 (
		    pix_multiply_1x128 (xmm_alpha,
				       unpack_32_1x128 (m)),
		    unpack_32_1x128 (d)));
	    w--;
	}

	while (w >= 16)
	{
	    xmm_mask = load_128_unaligned ((__m128i*)mask);
	    xmm_dst = load_128_aligned ((__m128i*)dst);

	    unpack_128_2x128 (xmm_mask, &xmm_mask_lo, &xmm_mask_hi);
	    unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);

	    pix_multiply_2x128 (&xmm_alpha, &xmm_alpha,
				&xmm_mask_lo, &xmm_mask_hi,
				&xmm_mask_lo, &xmm_mask_hi);

	    pix_multiply_2x128 (&xmm_mask_lo, &xmm_mask_hi,
				&xmm_dst_lo, &xmm_dst_hi,
				&xmm_dst_lo, &xmm_dst_hi);

	    save_128_aligned (
		(__m128i*)dst, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	    mask += 16;
	    dst += 16;
	    w -= 16;
	}

	while (w)
	{
	    m = (uint32_t) *mask++;
	    d = (uint32_t) *dst;

	    *dst++ = (uint8_t) pack_1x128_32 (
		pix_multiply_1x128 (
		    pix_multiply_1x128 (
			xmm_alpha, unpack_32_1x128 (m)),
		    unpack_32_1x128 (d)));
	    w--;
	}
    }

}

static void
sse2_composite_in_n_8 (pixman_implementation_t *imp,
		       pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint8_t     *dst_line, *dst;
    int dst_stride;
    uint32_t d;
    uint32_t src;
    int32_t w;

    __m128i xmm_alpha;
    __m128i xmm_dst, xmm_dst_lo, xmm_dst_hi;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    xmm_alpha = expand_alpha_1x128 (expand_pixel_32_1x128 (src));

    src = src >> 24;

    if (src == 0xff)
	return;

    if (src == 0x00)
    {
	pixman_fill (dest_image->bits.bits, dest_image->bits.rowstride,
		     8, dest_x, dest_y, width, height, src);

	return;
    }

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	w = width;

	while (w && ((uintptr_t)dst & 15))
	{
	    d = (uint32_t) *dst;

	    *dst++ = (uint8_t) pack_1x128_32 (
		pix_multiply_1x128 (
		    xmm_alpha,
		    unpack_32_1x128 (d)));
	    w--;
	}

	while (w >= 16)
	{
	    xmm_dst = load_128_aligned ((__m128i*)dst);

	    unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);
	    
	    pix_multiply_2x128 (&xmm_alpha, &xmm_alpha,
				&xmm_dst_lo, &xmm_dst_hi,
				&xmm_dst_lo, &xmm_dst_hi);

	    save_128_aligned (
		(__m128i*)dst, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	    dst += 16;
	    w -= 16;
	}

	while (w)
	{
	    d = (uint32_t) *dst;

	    *dst++ = (uint8_t) pack_1x128_32 (
		pix_multiply_1x128 (
		    xmm_alpha,
		    unpack_32_1x128 (d)));
	    w--;
	}
    }

}

static void
sse2_composite_in_8_8 (pixman_implementation_t *imp,
                       pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint8_t     *dst_line, *dst;
    uint8_t     *src_line, *src;
    int src_stride, dst_stride;
    int32_t w;
    uint32_t s, d;

    __m128i xmm_src, xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst, xmm_dst_lo, xmm_dst_hi;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	src_image, src_x, src_y, uint8_t, src_stride, src_line, 1);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;
	w = width;

	while (w && ((uintptr_t)dst & 15))
	{
	    s = (uint32_t) *src++;
	    d = (uint32_t) *dst;

	    *dst++ = (uint8_t) pack_1x128_32 (
		pix_multiply_1x128 (
		    unpack_32_1x128 (s), unpack_32_1x128 (d)));
	    w--;
	}

	while (w >= 16)
	{
	    xmm_src = load_128_unaligned ((__m128i*)src);
	    xmm_dst = load_128_aligned ((__m128i*)dst);

	    unpack_128_2x128 (xmm_src, &xmm_src_lo, &xmm_src_hi);
	    unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);

	    pix_multiply_2x128 (&xmm_src_lo, &xmm_src_hi,
				&xmm_dst_lo, &xmm_dst_hi,
				&xmm_dst_lo, &xmm_dst_hi);

	    save_128_aligned (
		(__m128i*)dst, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	    src += 16;
	    dst += 16;
	    w -= 16;
	}

	while (w)
	{
	    s = (uint32_t) *src++;
	    d = (uint32_t) *dst;

	    *dst++ = (uint8_t) pack_1x128_32 (
		pix_multiply_1x128 (unpack_32_1x128 (s), unpack_32_1x128 (d)));
	    w--;
	}
    }

}

static void
sse2_composite_add_n_8_8 (pixman_implementation_t *imp,
			  pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint8_t     *dst_line, *dst;
    uint8_t     *mask_line, *mask;
    int dst_stride, mask_stride;
    int32_t w;
    uint32_t src;
    uint32_t m, d;

    __m128i xmm_alpha;
    __m128i xmm_mask, xmm_mask_lo, xmm_mask_hi;
    __m128i xmm_dst, xmm_dst_lo, xmm_dst_hi;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	mask_image, mask_x, mask_y, uint8_t, mask_stride, mask_line, 1);

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    xmm_alpha = expand_alpha_1x128 (expand_pixel_32_1x128 (src));

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	mask = mask_line;
	mask_line += mask_stride;
	w = width;

	while (w && ((uintptr_t)dst & 15))
	{
	    m = (uint32_t) *mask++;
	    d = (uint32_t) *dst;

	    *dst++ = (uint8_t) pack_1x128_32 (
		_mm_adds_epu16 (
		    pix_multiply_1x128 (
			xmm_alpha, unpack_32_1x128 (m)),
		    unpack_32_1x128 (d)));
	    w--;
	}

	while (w >= 16)
	{
	    xmm_mask = load_128_unaligned ((__m128i*)mask);
	    xmm_dst = load_128_aligned ((__m128i*)dst);

	    unpack_128_2x128 (xmm_mask, &xmm_mask_lo, &xmm_mask_hi);
	    unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);

	    pix_multiply_2x128 (&xmm_alpha, &xmm_alpha,
				&xmm_mask_lo, &xmm_mask_hi,
				&xmm_mask_lo, &xmm_mask_hi);

	    xmm_dst_lo = _mm_adds_epu16 (xmm_mask_lo, xmm_dst_lo);
	    xmm_dst_hi = _mm_adds_epu16 (xmm_mask_hi, xmm_dst_hi);

	    save_128_aligned (
		(__m128i*)dst, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));

	    mask += 16;
	    dst += 16;
	    w -= 16;
	}

	while (w)
	{
	    m = (uint32_t) *mask++;
	    d = (uint32_t) *dst;

	    *dst++ = (uint8_t) pack_1x128_32 (
		_mm_adds_epu16 (
		    pix_multiply_1x128 (
			xmm_alpha, unpack_32_1x128 (m)),
		    unpack_32_1x128 (d)));

	    w--;
	}
    }

}

static void
sse2_composite_add_n_8 (pixman_implementation_t *imp,
			pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint8_t     *dst_line, *dst;
    int dst_stride;
    int32_t w;
    uint32_t src;

    __m128i xmm_src;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    src >>= 24;

    if (src == 0x00)
	return;

    if (src == 0xff)
    {
	pixman_fill (dest_image->bits.bits, dest_image->bits.rowstride,
		     8, dest_x, dest_y, width, height, 0xff);

	return;
    }

    src = (src << 24) | (src << 16) | (src << 8) | src;
    xmm_src = _mm_set_epi32 (src, src, src, src);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	w = width;

	while (w && ((uintptr_t)dst & 15))
	{
	    *dst = (uint8_t)_mm_cvtsi128_si32 (
		_mm_adds_epu8 (
		    xmm_src,
		    _mm_cvtsi32_si128 (*dst)));

	    w--;
	    dst++;
	}

	while (w >= 16)
	{
	    save_128_aligned (
		(__m128i*)dst, _mm_adds_epu8 (xmm_src, load_128_aligned  ((__m128i*)dst)));

	    dst += 16;
	    w -= 16;
	}

	while (w)
	{
	    *dst = (uint8_t)_mm_cvtsi128_si32 (
		_mm_adds_epu8 (
		    xmm_src,
		    _mm_cvtsi32_si128 (*dst)));

	    w--;
	    dst++;
	}
    }

}

static void
sse2_composite_add_8_8 (pixman_implementation_t *imp,
			pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint8_t     *dst_line, *dst;
    uint8_t     *src_line, *src;
    int dst_stride, src_stride;
    int32_t w;
    uint16_t t;

    PIXMAN_IMAGE_GET_LINE (
	src_image, src_x, src_y, uint8_t, src_stride, src_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);

    while (height--)
    {
	dst = dst_line;
	src = src_line;

	dst_line += dst_stride;
	src_line += src_stride;
	w = width;

	/* Small head */
	while (w && (uintptr_t)dst & 3)
	{
	    t = (*dst) + (*src++);
	    *dst++ = t | (0 - (t >> 8));
	    w--;
	}

	sse2_combine_add_u (imp, op,
			    (uint32_t*)dst, (uint32_t*)src, NULL, w >> 2);

	/* Small tail */
	dst += w & 0xfffc;
	src += w & 0xfffc;

	w &= 3;

	while (w)
	{
	    t = (*dst) + (*src++);
	    *dst++ = t | (0 - (t >> 8));
	    w--;
	}
    }

}

static void
sse2_composite_add_8888_8888 (pixman_implementation_t *imp,
                              pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t    *dst_line, *dst;
    uint32_t    *src_line, *src;
    int dst_stride, src_stride;

    PIXMAN_IMAGE_GET_LINE (
	src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;

	sse2_combine_add_u (imp, op, dst, src, NULL, width);
    }
}

static void
sse2_composite_add_n_8888 (pixman_implementation_t *imp,
			   pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t *dst_line, *dst, src;
    int dst_stride;

    __m128i xmm_src;

    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);
    if (src == 0)
	return;

    if (src == ~0)
    {
	pixman_fill (dest_image->bits.bits, dest_image->bits.rowstride, 32,
		     dest_x, dest_y, width, height, ~0);

	return;
    }

    xmm_src = _mm_set_epi32 (src, src, src, src);
    while (height--)
    {
	int w = width;
	uint32_t d;

	dst = dst_line;
	dst_line += dst_stride;

	while (w && (uintptr_t)dst & 15)
	{
	    d = *dst;
	    *dst++ =
		_mm_cvtsi128_si32 ( _mm_adds_epu8 (xmm_src, _mm_cvtsi32_si128 (d)));
	    w--;
	}

	while (w >= 4)
	{
	    save_128_aligned
		((__m128i*)dst,
		 _mm_adds_epu8 (xmm_src, load_128_aligned ((__m128i*)dst)));

	    dst += 4;
	    w -= 4;
	}

	while (w--)
	{
	    d = *dst;
	    *dst++ =
		_mm_cvtsi128_si32 (_mm_adds_epu8 (xmm_src,
						  _mm_cvtsi32_si128 (d)));
	}
    }
}

static void
sse2_composite_add_n_8_8888 (pixman_implementation_t *imp,
			     pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t     *dst_line, *dst;
    uint8_t     *mask_line, *mask;
    int dst_stride, mask_stride;
    int32_t w;
    uint32_t src;

    __m128i xmm_src;

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);
    if (src == 0)
	return;
    xmm_src = expand_pixel_32_1x128 (src);

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	mask_image, mask_x, mask_y, uint8_t, mask_stride, mask_line, 1);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	mask = mask_line;
	mask_line += mask_stride;
	w = width;

	while (w && ((uintptr_t)dst & 15))
	{
	    uint8_t m = *mask++;
	    if (m)
	    {
		*dst = pack_1x128_32
		    (_mm_adds_epu16
		     (pix_multiply_1x128 (xmm_src, expand_pixel_8_1x128 (m)),
		      unpack_32_1x128 (*dst)));
	    }
	    dst++;
	    w--;
	}

	while (w >= 4)
	{
	    uint32_t m = *(uint32_t*)mask;
	    if (m)
	    {
		__m128i xmm_mask_lo, xmm_mask_hi;
		__m128i xmm_dst_lo, xmm_dst_hi;

		__m128i xmm_dst = load_128_aligned ((__m128i*)dst);
		__m128i xmm_mask =
		    _mm_unpacklo_epi8 (unpack_32_1x128(m),
				       _mm_setzero_si128 ());

		unpack_128_2x128 (xmm_mask, &xmm_mask_lo, &xmm_mask_hi);
		unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);

		expand_alpha_rev_2x128 (xmm_mask_lo, xmm_mask_hi,
					&xmm_mask_lo, &xmm_mask_hi);

		pix_multiply_2x128 (&xmm_src, &xmm_src,
				    &xmm_mask_lo, &xmm_mask_hi,
				    &xmm_mask_lo, &xmm_mask_hi);

		xmm_dst_lo = _mm_adds_epu16 (xmm_mask_lo, xmm_dst_lo);
		xmm_dst_hi = _mm_adds_epu16 (xmm_mask_hi, xmm_dst_hi);

		save_128_aligned (
		    (__m128i*)dst, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));
	    }

	    w -= 4;
	    dst += 4;
	    mask += 4;
	}

	while (w)
	{
	    uint8_t m = *mask++;
	    if (m)
	    {
		*dst = pack_1x128_32
		    (_mm_adds_epu16
		     (pix_multiply_1x128 (xmm_src, expand_pixel_8_1x128 (m)),
		      unpack_32_1x128 (*dst)));
	    }
	    dst++;
	    w--;
	}
    }
}

static pixman_bool_t
sse2_blt (pixman_implementation_t *imp,
          uint32_t *               src_bits,
          uint32_t *               dst_bits,
          int                      src_stride,
          int                      dst_stride,
          int                      src_bpp,
          int                      dst_bpp,
          int                      src_x,
          int                      src_y,
          int                      dest_x,
          int                      dest_y,
          int                      width,
          int                      height)
{
    uint8_t *   src_bytes;
    uint8_t *   dst_bytes;
    int byte_width;

    if (src_bpp != dst_bpp)
	return FALSE;

    if (src_bpp == 16)
    {
	src_stride = src_stride * (int) sizeof (uint32_t) / 2;
	dst_stride = dst_stride * (int) sizeof (uint32_t) / 2;
	src_bytes =(uint8_t *)(((uint16_t *)src_bits) + src_stride * (src_y) + (src_x));
	dst_bytes = (uint8_t *)(((uint16_t *)dst_bits) + dst_stride * (dest_y) + (dest_x));
	byte_width = 2 * width;
	src_stride *= 2;
	dst_stride *= 2;
    }
    else if (src_bpp == 32)
    {
	src_stride = src_stride * (int) sizeof (uint32_t) / 4;
	dst_stride = dst_stride * (int) sizeof (uint32_t) / 4;
	src_bytes = (uint8_t *)(((uint32_t *)src_bits) + src_stride * (src_y) + (src_x));
	dst_bytes = (uint8_t *)(((uint32_t *)dst_bits) + dst_stride * (dest_y) + (dest_x));
	byte_width = 4 * width;
	src_stride *= 4;
	dst_stride *= 4;
    }
    else
    {
	return FALSE;
    }

    while (height--)
    {
	int w;
	uint8_t *s = src_bytes;
	uint8_t *d = dst_bytes;
	src_bytes += src_stride;
	dst_bytes += dst_stride;
	w = byte_width;

	while (w >= 2 && ((uintptr_t)d & 3))
	{
	    *(uint16_t *)d = *(uint16_t *)s;
	    w -= 2;
	    s += 2;
	    d += 2;
	}

	while (w >= 4 && ((uintptr_t)d & 15))
	{
	    *(uint32_t *)d = *(uint32_t *)s;

	    w -= 4;
	    s += 4;
	    d += 4;
	}

	while (w >= 64)
	{
	    __m128i xmm0, xmm1, xmm2, xmm3;

	    xmm0 = load_128_unaligned ((__m128i*)(s));
	    xmm1 = load_128_unaligned ((__m128i*)(s + 16));
	    xmm2 = load_128_unaligned ((__m128i*)(s + 32));
	    xmm3 = load_128_unaligned ((__m128i*)(s + 48));

	    save_128_aligned ((__m128i*)(d),    xmm0);
	    save_128_aligned ((__m128i*)(d + 16), xmm1);
	    save_128_aligned ((__m128i*)(d + 32), xmm2);
	    save_128_aligned ((__m128i*)(d + 48), xmm3);

	    s += 64;
	    d += 64;
	    w -= 64;
	}

	while (w >= 16)
	{
	    save_128_aligned ((__m128i*)d, load_128_unaligned ((__m128i*)s) );

	    w -= 16;
	    d += 16;
	    s += 16;
	}

	while (w >= 4)
	{
	    *(uint32_t *)d = *(uint32_t *)s;

	    w -= 4;
	    s += 4;
	    d += 4;
	}

	if (w >= 2)
	{
	    *(uint16_t *)d = *(uint16_t *)s;
	    w -= 2;
	    s += 2;
	    d += 2;
	}
    }

    return TRUE;
}

static void
sse2_composite_copy_area (pixman_implementation_t *imp,
                          pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    sse2_blt (imp, src_image->bits.bits,
	      dest_image->bits.bits,
	      src_image->bits.rowstride,
	      dest_image->bits.rowstride,
	      PIXMAN_FORMAT_BPP (src_image->bits.format),
	      PIXMAN_FORMAT_BPP (dest_image->bits.format),
	      src_x, src_y, dest_x, dest_y, width, height);
}

static void
sse2_composite_over_x888_8_8888 (pixman_implementation_t *imp,
                                 pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t    *src, *src_line, s;
    uint32_t    *dst, *dst_line, d;
    uint8_t         *mask, *mask_line;
    uint32_t m;
    int src_stride, mask_stride, dst_stride;
    int32_t w;
    __m128i ms;

    __m128i xmm_src, xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst, xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_mask, xmm_mask_lo, xmm_mask_hi;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	mask_image, mask_x, mask_y, uint8_t, mask_stride, mask_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);

    while (height--)
    {
        src = src_line;
        src_line += src_stride;
        dst = dst_line;
        dst_line += dst_stride;
        mask = mask_line;
        mask_line += mask_stride;

        w = width;

        while (w && (uintptr_t)dst & 15)
        {
            s = 0xff000000 | *src++;
            m = (uint32_t) *mask++;
            d = *dst;
            ms = unpack_32_1x128 (s);

            if (m != 0xff)
            {
		__m128i ma = expand_alpha_rev_1x128 (unpack_32_1x128 (m));
		__m128i md = unpack_32_1x128 (d);

                ms = in_over_1x128 (&ms, &mask_00ff, &ma, &md);
            }

            *dst++ = pack_1x128_32 (ms);
            w--;
        }

        while (w >= 4)
        {
            m = *(uint32_t*) mask;
            xmm_src = _mm_or_si128 (
		load_128_unaligned ((__m128i*)src), mask_ff000000);

            if (m == 0xffffffff)
            {
                save_128_aligned ((__m128i*)dst, xmm_src);
            }
            else
            {
                xmm_dst = load_128_aligned ((__m128i*)dst);

                xmm_mask = _mm_unpacklo_epi16 (unpack_32_1x128 (m), _mm_setzero_si128());

                unpack_128_2x128 (xmm_src, &xmm_src_lo, &xmm_src_hi);
                unpack_128_2x128 (xmm_mask, &xmm_mask_lo, &xmm_mask_hi);
                unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);

                expand_alpha_rev_2x128 (
		    xmm_mask_lo, xmm_mask_hi, &xmm_mask_lo, &xmm_mask_hi);

                in_over_2x128 (&xmm_src_lo, &xmm_src_hi,
			       &mask_00ff, &mask_00ff, &xmm_mask_lo, &xmm_mask_hi,
			       &xmm_dst_lo, &xmm_dst_hi);

                save_128_aligned ((__m128i*)dst, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));
            }

            src += 4;
            dst += 4;
            mask += 4;
            w -= 4;
        }

        while (w)
        {
            m = (uint32_t) *mask++;

            if (m)
            {
                s = 0xff000000 | *src;

                if (m == 0xff)
                {
                    *dst = s;
                }
                else
                {
		    __m128i ma, md, ms;

                    d = *dst;

		    ma = expand_alpha_rev_1x128 (unpack_32_1x128 (m));
		    md = unpack_32_1x128 (d);
		    ms = unpack_32_1x128 (s);

                    *dst = pack_1x128_32 (in_over_1x128 (&ms, &mask_00ff, &ma, &md));
                }

            }

            src++;
            dst++;
            w--;
        }
    }

}

static void
sse2_composite_over_8888_8_8888 (pixman_implementation_t *imp,
                                 pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t    *src, *src_line, s;
    uint32_t    *dst, *dst_line, d;
    uint8_t         *mask, *mask_line;
    uint32_t m;
    int src_stride, mask_stride, dst_stride;
    int32_t w;

    __m128i xmm_src, xmm_src_lo, xmm_src_hi, xmm_srca_lo, xmm_srca_hi;
    __m128i xmm_dst, xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_mask, xmm_mask_lo, xmm_mask_hi;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	mask_image, mask_x, mask_y, uint8_t, mask_stride, mask_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);

    while (height--)
    {
        src = src_line;
        src_line += src_stride;
        dst = dst_line;
        dst_line += dst_stride;
        mask = mask_line;
        mask_line += mask_stride;

        w = width;

        while (w && (uintptr_t)dst & 15)
        {
	    uint32_t sa;

            s = *src++;
            m = (uint32_t) *mask++;
            d = *dst;

	    sa = s >> 24;

	    if (m)
	    {
		if (sa == 0xff && m == 0xff)
		{
		    *dst = s;
		}
		else
		{
		    __m128i ms, md, ma, msa;

		    ma = expand_alpha_rev_1x128 (load_32_1x128 (m));
		    ms = unpack_32_1x128 (s);
		    md = unpack_32_1x128 (d);

		    msa = expand_alpha_rev_1x128 (load_32_1x128 (sa));

		    *dst = pack_1x128_32 (in_over_1x128 (&ms, &msa, &ma, &md));
		}
	    }

	    dst++;
            w--;
        }

        while (w >= 4)
        {
            m = *(uint32_t *) mask;

	    if (m)
	    {
		xmm_src = load_128_unaligned ((__m128i*)src);

		if (m == 0xffffffff && is_opaque (xmm_src))
		{
		    save_128_aligned ((__m128i *)dst, xmm_src);
		}
		else
		{
		    xmm_dst = load_128_aligned ((__m128i *)dst);

		    xmm_mask = _mm_unpacklo_epi16 (unpack_32_1x128 (m), _mm_setzero_si128());

		    unpack_128_2x128 (xmm_src, &xmm_src_lo, &xmm_src_hi);
		    unpack_128_2x128 (xmm_mask, &xmm_mask_lo, &xmm_mask_hi);
		    unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);

		    expand_alpha_2x128 (xmm_src_lo, xmm_src_hi, &xmm_srca_lo, &xmm_srca_hi);
		    expand_alpha_rev_2x128 (xmm_mask_lo, xmm_mask_hi, &xmm_mask_lo, &xmm_mask_hi);

		    in_over_2x128 (&xmm_src_lo, &xmm_src_hi, &xmm_srca_lo, &xmm_srca_hi,
				   &xmm_mask_lo, &xmm_mask_hi, &xmm_dst_lo, &xmm_dst_hi);

		    save_128_aligned ((__m128i*)dst, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));
		}
	    }

            src += 4;
            dst += 4;
            mask += 4;
            w -= 4;
        }

        while (w)
        {
	    uint32_t sa;

            s = *src++;
            m = (uint32_t) *mask++;
            d = *dst;

	    sa = s >> 24;

	    if (m)
	    {
		if (sa == 0xff && m == 0xff)
		{
		    *dst = s;
		}
		else
		{
		    __m128i ms, md, ma, msa;

		    ma = expand_alpha_rev_1x128 (load_32_1x128 (m));
		    ms = unpack_32_1x128 (s);
		    md = unpack_32_1x128 (d);

		    msa = expand_alpha_rev_1x128 (load_32_1x128 (sa));

		    *dst = pack_1x128_32 (in_over_1x128 (&ms, &msa, &ma, &md));
		}
	    }

	    dst++;
            w--;
        }
    }

}

static void
sse2_composite_over_reverse_n_8888 (pixman_implementation_t *imp,
				    pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t src;
    uint32_t    *dst_line, *dst;
    __m128i xmm_src;
    __m128i xmm_dst, xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_dsta_hi, xmm_dsta_lo;
    int dst_stride;
    int32_t w;

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    if (src == 0)
	return;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);

    xmm_src = expand_pixel_32_1x128 (src);

    while (height--)
    {
	dst = dst_line;

	dst_line += dst_stride;
	w = width;

	while (w && (uintptr_t)dst & 15)
	{
	    __m128i vd;

	    vd = unpack_32_1x128 (*dst);

	    *dst = pack_1x128_32 (over_1x128 (vd, expand_alpha_1x128 (vd),
					      xmm_src));
	    w--;
	    dst++;
	}

	while (w >= 4)
	{
	    __m128i tmp_lo, tmp_hi;

	    xmm_dst = load_128_aligned ((__m128i*)dst);

	    unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);
	    expand_alpha_2x128 (xmm_dst_lo, xmm_dst_hi, &xmm_dsta_lo, &xmm_dsta_hi);

	    tmp_lo = xmm_src;
	    tmp_hi = xmm_src;

	    over_2x128 (&xmm_dst_lo, &xmm_dst_hi,
			&xmm_dsta_lo, &xmm_dsta_hi,
			&tmp_lo, &tmp_hi);

	    save_128_aligned (
		(__m128i*)dst, pack_2x128_128 (tmp_lo, tmp_hi));

	    w -= 4;
	    dst += 4;
	}

	while (w)
	{
	    __m128i vd;

	    vd = unpack_32_1x128 (*dst);

	    *dst = pack_1x128_32 (over_1x128 (vd, expand_alpha_1x128 (vd),
					      xmm_src));
	    w--;
	    dst++;
	}

    }

}

static void
sse2_composite_over_8888_8888_8888 (pixman_implementation_t *imp,
				    pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t    *src, *src_line, s;
    uint32_t    *dst, *dst_line, d;
    uint32_t    *mask, *mask_line;
    uint32_t    m;
    int src_stride, mask_stride, dst_stride;
    int32_t w;

    __m128i xmm_src, xmm_src_lo, xmm_src_hi, xmm_srca_lo, xmm_srca_hi;
    __m128i xmm_dst, xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_mask, xmm_mask_lo, xmm_mask_hi;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	mask_image, mask_x, mask_y, uint32_t, mask_stride, mask_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);

    while (height--)
    {
        src = src_line;
        src_line += src_stride;
        dst = dst_line;
        dst_line += dst_stride;
        mask = mask_line;
        mask_line += mask_stride;

        w = width;

        while (w && (uintptr_t)dst & 15)
        {
	    uint32_t sa;

            s = *src++;
            m = (*mask++) >> 24;
            d = *dst;

	    sa = s >> 24;

	    if (m)
	    {
		if (sa == 0xff && m == 0xff)
		{
		    *dst = s;
		}
		else
		{
		    __m128i ms, md, ma, msa;

		    ma = expand_alpha_rev_1x128 (load_32_1x128 (m));
		    ms = unpack_32_1x128 (s);
		    md = unpack_32_1x128 (d);

		    msa = expand_alpha_rev_1x128 (load_32_1x128 (sa));

		    *dst = pack_1x128_32 (in_over_1x128 (&ms, &msa, &ma, &md));
		}
	    }

	    dst++;
            w--;
        }

        while (w >= 4)
        {
	    xmm_mask = load_128_unaligned ((__m128i*)mask);

	    if (!is_transparent (xmm_mask))
	    {
		xmm_src = load_128_unaligned ((__m128i*)src);

		if (is_opaque (xmm_mask) && is_opaque (xmm_src))
		{
		    save_128_aligned ((__m128i *)dst, xmm_src);
		}
		else
		{
		    xmm_dst = load_128_aligned ((__m128i *)dst);

		    unpack_128_2x128 (xmm_src, &xmm_src_lo, &xmm_src_hi);
		    unpack_128_2x128 (xmm_mask, &xmm_mask_lo, &xmm_mask_hi);
		    unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);

		    expand_alpha_2x128 (xmm_src_lo, xmm_src_hi, &xmm_srca_lo, &xmm_srca_hi);
		    expand_alpha_2x128 (xmm_mask_lo, xmm_mask_hi, &xmm_mask_lo, &xmm_mask_hi);

		    in_over_2x128 (&xmm_src_lo, &xmm_src_hi, &xmm_srca_lo, &xmm_srca_hi,
				   &xmm_mask_lo, &xmm_mask_hi, &xmm_dst_lo, &xmm_dst_hi);

		    save_128_aligned ((__m128i*)dst, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));
		}
	    }

            src += 4;
            dst += 4;
            mask += 4;
            w -= 4;
        }

        while (w)
        {
	    uint32_t sa;

            s = *src++;
            m = (*mask++) >> 24;
            d = *dst;

	    sa = s >> 24;

	    if (m)
	    {
		if (sa == 0xff && m == 0xff)
		{
		    *dst = s;
		}
		else
		{
		    __m128i ms, md, ma, msa;

		    ma = expand_alpha_rev_1x128 (load_32_1x128 (m));
		    ms = unpack_32_1x128 (s);
		    md = unpack_32_1x128 (d);

		    msa = expand_alpha_rev_1x128 (load_32_1x128 (sa));

		    *dst = pack_1x128_32 (in_over_1x128 (&ms, &msa, &ma, &md));
		}
	    }

	    dst++;
            w--;
        }
    }

}

/* A variant of 'sse2_combine_over_u' with minor tweaks */
static force_inline void
scaled_nearest_scanline_sse2_8888_8888_OVER (uint32_t*       pd,
                                             const uint32_t* ps,
                                             int32_t         w,
                                             pixman_fixed_t  vx,
                                             pixman_fixed_t  unit_x,
                                             pixman_fixed_t  src_width_fixed,
                                             pixman_bool_t   fully_transparent_src)
{
    uint32_t s, d;
    const uint32_t* pm = NULL;

    __m128i xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_src_lo, xmm_src_hi;
    __m128i xmm_alpha_lo, xmm_alpha_hi;

    if (fully_transparent_src)
	return;

    /* Align dst on a 16-byte boundary */
    while (w && ((uintptr_t)pd & 15))
    {
	d = *pd;
	s = combine1 (ps + pixman_fixed_to_int (vx), pm);
	vx += unit_x;
	while (vx >= 0)
	    vx -= src_width_fixed;

	*pd++ = core_combine_over_u_pixel_sse2 (s, d);
	if (pm)
	    pm++;
	w--;
    }

    while (w >= 4)
    {
	__m128i tmp;
	uint32_t tmp1, tmp2, tmp3, tmp4;

	tmp1 = *(ps + pixman_fixed_to_int (vx));
	vx += unit_x;
	while (vx >= 0)
	    vx -= src_width_fixed;
	tmp2 = *(ps + pixman_fixed_to_int (vx));
	vx += unit_x;
	while (vx >= 0)
	    vx -= src_width_fixed;
	tmp3 = *(ps + pixman_fixed_to_int (vx));
	vx += unit_x;
	while (vx >= 0)
	    vx -= src_width_fixed;
	tmp4 = *(ps + pixman_fixed_to_int (vx));
	vx += unit_x;
	while (vx >= 0)
	    vx -= src_width_fixed;

	tmp = _mm_set_epi32 (tmp4, tmp3, tmp2, tmp1);

	xmm_src_hi = combine4 ((__m128i*)&tmp, (__m128i*)pm);

	if (is_opaque (xmm_src_hi))
	{
	    save_128_aligned ((__m128i*)pd, xmm_src_hi);
	}
	else if (!is_zero (xmm_src_hi))
	{
	    xmm_dst_hi = load_128_aligned ((__m128i*) pd);

	    unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	    unpack_128_2x128 (xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);

	    expand_alpha_2x128 (
		xmm_src_lo, xmm_src_hi, &xmm_alpha_lo, &xmm_alpha_hi);

	    over_2x128 (&xmm_src_lo, &xmm_src_hi,
			&xmm_alpha_lo, &xmm_alpha_hi,
			&xmm_dst_lo, &xmm_dst_hi);

	    /* rebuid the 4 pixel data and save*/
	    save_128_aligned ((__m128i*)pd,
			      pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));
	}

	w -= 4;
	pd += 4;
	if (pm)
	    pm += 4;
    }

    while (w)
    {
	d = *pd;
	s = combine1 (ps + pixman_fixed_to_int (vx), pm);
	vx += unit_x;
	while (vx >= 0)
	    vx -= src_width_fixed;

	*pd++ = core_combine_over_u_pixel_sse2 (s, d);
	if (pm)
	    pm++;

	w--;
    }
}

FAST_NEAREST_MAINLOOP (sse2_8888_8888_cover_OVER,
		       scaled_nearest_scanline_sse2_8888_8888_OVER,
		       uint32_t, uint32_t, COVER)
FAST_NEAREST_MAINLOOP (sse2_8888_8888_none_OVER,
		       scaled_nearest_scanline_sse2_8888_8888_OVER,
		       uint32_t, uint32_t, NONE)
FAST_NEAREST_MAINLOOP (sse2_8888_8888_pad_OVER,
		       scaled_nearest_scanline_sse2_8888_8888_OVER,
		       uint32_t, uint32_t, PAD)
FAST_NEAREST_MAINLOOP (sse2_8888_8888_normal_OVER,
		       scaled_nearest_scanline_sse2_8888_8888_OVER,
		       uint32_t, uint32_t, NORMAL)

static force_inline void
scaled_nearest_scanline_sse2_8888_n_8888_OVER (const uint32_t * mask,
					       uint32_t *       dst,
					       const uint32_t * src,
					       int32_t          w,
					       pixman_fixed_t   vx,
					       pixman_fixed_t   unit_x,
					       pixman_fixed_t   src_width_fixed,
					       pixman_bool_t    zero_src)
{
    __m128i xmm_mask;
    __m128i xmm_src, xmm_src_lo, xmm_src_hi;
    __m128i xmm_dst, xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_alpha_lo, xmm_alpha_hi;

    if (zero_src || (*mask >> 24) == 0)
	return;

    xmm_mask = create_mask_16_128 (*mask >> 24);

    while (w && (uintptr_t)dst & 15)
    {
	uint32_t s = *(src + pixman_fixed_to_int (vx));
	vx += unit_x;
	while (vx >= 0)
	    vx -= src_width_fixed;

	if (s)
	{
	    uint32_t d = *dst;

	    __m128i ms = unpack_32_1x128 (s);
	    __m128i alpha     = expand_alpha_1x128 (ms);
	    __m128i dest      = xmm_mask;
	    __m128i alpha_dst = unpack_32_1x128 (d);

	    *dst = pack_1x128_32 (
		in_over_1x128 (&ms, &alpha, &dest, &alpha_dst));
	}
	dst++;
	w--;
    }

    while (w >= 4)
    {
	uint32_t tmp1, tmp2, tmp3, tmp4;

	tmp1 = *(src + pixman_fixed_to_int (vx));
	vx += unit_x;
	while (vx >= 0)
	    vx -= src_width_fixed;
	tmp2 = *(src + pixman_fixed_to_int (vx));
	vx += unit_x;
	while (vx >= 0)
	    vx -= src_width_fixed;
	tmp3 = *(src + pixman_fixed_to_int (vx));
	vx += unit_x;
	while (vx >= 0)
	    vx -= src_width_fixed;
	tmp4 = *(src + pixman_fixed_to_int (vx));
	vx += unit_x;
	while (vx >= 0)
	    vx -= src_width_fixed;

	xmm_src = _mm_set_epi32 (tmp4, tmp3, tmp2, tmp1);

	if (!is_zero (xmm_src))
	{
	    xmm_dst = load_128_aligned ((__m128i*)dst);

	    unpack_128_2x128 (xmm_src, &xmm_src_lo, &xmm_src_hi);
	    unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);
	    expand_alpha_2x128 (xmm_src_lo, xmm_src_hi,
			        &xmm_alpha_lo, &xmm_alpha_hi);

	    in_over_2x128 (&xmm_src_lo, &xmm_src_hi,
			   &xmm_alpha_lo, &xmm_alpha_hi,
			   &xmm_mask, &xmm_mask,
			   &xmm_dst_lo, &xmm_dst_hi);

	    save_128_aligned (
		(__m128i*)dst, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));
	}

	dst += 4;
	w -= 4;
    }

    while (w)
    {
	uint32_t s = *(src + pixman_fixed_to_int (vx));
	vx += unit_x;
	while (vx >= 0)
	    vx -= src_width_fixed;

	if (s)
	{
	    uint32_t d = *dst;

	    __m128i ms = unpack_32_1x128 (s);
	    __m128i alpha = expand_alpha_1x128 (ms);
	    __m128i mask  = xmm_mask;
	    __m128i dest  = unpack_32_1x128 (d);

	    *dst = pack_1x128_32 (
		in_over_1x128 (&ms, &alpha, &mask, &dest));
	}

	dst++;
	w--;
    }

}

FAST_NEAREST_MAINLOOP_COMMON (sse2_8888_n_8888_cover_OVER,
			      scaled_nearest_scanline_sse2_8888_n_8888_OVER,
			      uint32_t, uint32_t, uint32_t, COVER, TRUE, TRUE)
FAST_NEAREST_MAINLOOP_COMMON (sse2_8888_n_8888_pad_OVER,
			      scaled_nearest_scanline_sse2_8888_n_8888_OVER,
			      uint32_t, uint32_t, uint32_t, PAD, TRUE, TRUE)
FAST_NEAREST_MAINLOOP_COMMON (sse2_8888_n_8888_none_OVER,
			      scaled_nearest_scanline_sse2_8888_n_8888_OVER,
			      uint32_t, uint32_t, uint32_t, NONE, TRUE, TRUE)
FAST_NEAREST_MAINLOOP_COMMON (sse2_8888_n_8888_normal_OVER,
			      scaled_nearest_scanline_sse2_8888_n_8888_OVER,
			      uint32_t, uint32_t, uint32_t, NORMAL, TRUE, TRUE)

#if PSHUFD_IS_FAST

/***********************************************************************************/

# define BILINEAR_DECLARE_VARIABLES						\
    const __m128i xmm_wt = _mm_set_epi16 (wt, wt, wt, wt, wt, wt, wt, wt);	\
    const __m128i xmm_wb = _mm_set_epi16 (wb, wb, wb, wb, wb, wb, wb, wb);	\
    const __m128i xmm_addc = _mm_set_epi16 (0, 1, 0, 1, 0, 1, 0, 1);		\
    const __m128i xmm_ux1 = _mm_set_epi16 (unit_x, -unit_x, unit_x, -unit_x,	\
					   unit_x, -unit_x, unit_x, -unit_x);	\
    const __m128i xmm_ux4 = _mm_set_epi16 (unit_x * 4, -unit_x * 4,		\
					   unit_x * 4, -unit_x * 4,		\
					   unit_x * 4, -unit_x * 4,		\
					   unit_x * 4, -unit_x * 4);		\
    const __m128i xmm_zero = _mm_setzero_si128 ();				\
    __m128i xmm_x = _mm_set_epi16 (vx + unit_x * 3, -(vx + 1) - unit_x * 3,	\
				   vx + unit_x * 2, -(vx + 1) - unit_x * 2,	\
				   vx + unit_x * 1, -(vx + 1) - unit_x * 1,	\
				   vx + unit_x * 0, -(vx + 1) - unit_x * 0);	\
    __m128i xmm_wh_state;

#define BILINEAR_INTERPOLATE_ONE_PIXEL_HELPER(pix, phase_)			\
do {										\
    int phase = phase_;								\
    __m128i xmm_wh, xmm_a, xmm_b;						\
    /* fetch 2x2 pixel block into sse2 registers */				\
    __m128i tltr = _mm_loadl_epi64 ((__m128i *)&src_top[vx >> 16]);		\
    __m128i blbr = _mm_loadl_epi64 ((__m128i *)&src_bottom[vx >> 16]);		\
    vx += unit_x;								\
    /* vertical interpolation */						\
    xmm_a = _mm_mullo_epi16 (_mm_unpacklo_epi8 (tltr, xmm_zero), xmm_wt);	\
    xmm_b = _mm_mullo_epi16 (_mm_unpacklo_epi8 (blbr, xmm_zero), xmm_wb);	\
    xmm_a = _mm_add_epi16 (xmm_a, xmm_b);						\
    /* calculate horizontal weights */						\
    if (phase <= 0)								\
    {										\
	xmm_wh_state = _mm_add_epi16 (xmm_addc, _mm_srli_epi16 (xmm_x,		\
					16 - BILINEAR_INTERPOLATION_BITS));	\
	xmm_x = _mm_add_epi16 (xmm_x, (phase < 0) ? xmm_ux1 : xmm_ux4);		\
	phase = 0;								\
    }										\
    xmm_wh = _mm_shuffle_epi32 (xmm_wh_state, _MM_SHUFFLE (phase, phase,	\
							   phase, phase));	\
    /* horizontal interpolation */						\
    xmm_a = _mm_madd_epi16 (_mm_unpackhi_epi16 (_mm_shuffle_epi32 (		\
		xmm_a, _MM_SHUFFLE (1, 0, 3, 2)), xmm_a), xmm_wh);		\
    /* shift the result */							\
    pix = _mm_srli_epi32 (xmm_a, BILINEAR_INTERPOLATION_BITS * 2);		\
} while (0)

#else /************************************************************************/

# define BILINEAR_DECLARE_VARIABLES						\
    const __m128i xmm_wt = _mm_set_epi16 (wt, wt, wt, wt, wt, wt, wt, wt);	\
    const __m128i xmm_wb = _mm_set_epi16 (wb, wb, wb, wb, wb, wb, wb, wb);	\
    const __m128i xmm_addc = _mm_set_epi16 (0, 1, 0, 1, 0, 1, 0, 1);		\
    const __m128i xmm_ux1 = _mm_set_epi16 (unit_x, -unit_x, unit_x, -unit_x,	\
					  unit_x, -unit_x, unit_x, -unit_x);	\
    const __m128i xmm_ux4 = _mm_set_epi16 (unit_x * 4, -unit_x * 4,		\
					   unit_x * 4, -unit_x * 4,		\
					   unit_x * 4, -unit_x * 4,		\
					   unit_x * 4, -unit_x * 4);		\
    const __m128i xmm_zero = _mm_setzero_si128 ();				\
    __m128i xmm_x = _mm_set_epi16 (vx, -(vx + 1), vx, -(vx + 1),		\
				   vx, -(vx + 1), vx, -(vx + 1))

#define BILINEAR_INTERPOLATE_ONE_PIXEL_HELPER(pix, phase)			\
do {										\
    __m128i xmm_wh, xmm_a, xmm_b;						\
    /* fetch 2x2 pixel block into sse2 registers */				\
    __m128i tltr = _mm_loadl_epi64 ((__m128i *)&src_top[vx >> 16]);		\
    __m128i blbr = _mm_loadl_epi64 ((__m128i *)&src_bottom[vx >> 16]);		\
    (void)xmm_ux4; /* suppress warning: unused variable 'xmm_ux4' */		\
    vx += unit_x;								\
    /* vertical interpolation */						\
    xmm_a = _mm_mullo_epi16 (_mm_unpacklo_epi8 (tltr, xmm_zero), xmm_wt);	\
    xmm_b = _mm_mullo_epi16 (_mm_unpacklo_epi8 (blbr, xmm_zero), xmm_wb);	\
    xmm_a = _mm_add_epi16 (xmm_a, xmm_b);					\
    /* calculate horizontal weights */						\
    xmm_wh = _mm_add_epi16 (xmm_addc, _mm_srli_epi16 (xmm_x,			\
					16 - BILINEAR_INTERPOLATION_BITS));	\
    xmm_x = _mm_add_epi16 (xmm_x, xmm_ux1);					\
    /* horizontal interpolation */						\
    xmm_b = _mm_unpacklo_epi64 (/* any value is fine here */ xmm_b, xmm_a);	\
    xmm_a = _mm_madd_epi16 (_mm_unpackhi_epi16 (xmm_b, xmm_a), xmm_wh);		\
    /* shift the result */							\
    pix = _mm_srli_epi32 (xmm_a, BILINEAR_INTERPOLATION_BITS * 2);		\
} while (0)

/***********************************************************************************/

#endif

#define BILINEAR_INTERPOLATE_ONE_PIXEL(pix);					\
do {										\
	__m128i xmm_pix;							\
	BILINEAR_INTERPOLATE_ONE_PIXEL_HELPER (xmm_pix, -1);			\
	xmm_pix = _mm_packs_epi32 (xmm_pix, xmm_pix);				\
	xmm_pix = _mm_packus_epi16 (xmm_pix, xmm_pix);				\
	pix = _mm_cvtsi128_si32 (xmm_pix);					\
} while(0)

#define BILINEAR_INTERPOLATE_FOUR_PIXELS(pix);					\
do {										\
	__m128i xmm_pix1, xmm_pix2, xmm_pix3, xmm_pix4;				\
	BILINEAR_INTERPOLATE_ONE_PIXEL_HELPER (xmm_pix1, 0);			\
	BILINEAR_INTERPOLATE_ONE_PIXEL_HELPER (xmm_pix2, 1);			\
	BILINEAR_INTERPOLATE_ONE_PIXEL_HELPER (xmm_pix3, 2);			\
	BILINEAR_INTERPOLATE_ONE_PIXEL_HELPER (xmm_pix4, 3);			\
	xmm_pix1 = _mm_packs_epi32 (xmm_pix1, xmm_pix2);			\
	xmm_pix3 = _mm_packs_epi32 (xmm_pix3, xmm_pix4);			\
	pix = _mm_packus_epi16 (xmm_pix1, xmm_pix3);				\
} while(0)

#define BILINEAR_SKIP_ONE_PIXEL()						\
do {										\
    vx += unit_x;								\
    xmm_x = _mm_add_epi16 (xmm_x, xmm_ux1);					\
} while(0)

#define BILINEAR_SKIP_FOUR_PIXELS()						\
do {										\
    vx += unit_x * 4;								\
    xmm_x = _mm_add_epi16 (xmm_x, xmm_ux4);					\
} while(0)

/***********************************************************************************/

static force_inline void
scaled_bilinear_scanline_sse2_8888_8888_SRC (uint32_t *       dst,
					     const uint32_t * mask,
					     const uint32_t * src_top,
					     const uint32_t * src_bottom,
					     int32_t          w,
					     int              wt,
					     int              wb,
					     pixman_fixed_t   vx_,
					     pixman_fixed_t   unit_x_,
					     pixman_fixed_t   max_vx,
					     pixman_bool_t    zero_src)
{
    intptr_t vx = vx_;
    intptr_t unit_x = unit_x_;
    BILINEAR_DECLARE_VARIABLES;
    uint32_t pix1, pix2;

    while (w && ((uintptr_t)dst & 15))
    {
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix1);
	*dst++ = pix1;
	w--;
    }

    while ((w -= 4) >= 0) {
	__m128i xmm_src;
	BILINEAR_INTERPOLATE_FOUR_PIXELS (xmm_src);
	_mm_store_si128 ((__m128i *)dst, xmm_src);
	dst += 4;
    }

    if (w & 2)
    {
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix1);
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix2);
	*dst++ = pix1;
	*dst++ = pix2;
    }

    if (w & 1)
    {
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix1);
	*dst = pix1;
    }

}

FAST_BILINEAR_MAINLOOP_COMMON (sse2_8888_8888_cover_SRC,
			       scaled_bilinear_scanline_sse2_8888_8888_SRC,
			       uint32_t, uint32_t, uint32_t,
			       COVER, FLAG_NONE)
FAST_BILINEAR_MAINLOOP_COMMON (sse2_8888_8888_pad_SRC,
			       scaled_bilinear_scanline_sse2_8888_8888_SRC,
			       uint32_t, uint32_t, uint32_t,
			       PAD, FLAG_NONE)
FAST_BILINEAR_MAINLOOP_COMMON (sse2_8888_8888_none_SRC,
			       scaled_bilinear_scanline_sse2_8888_8888_SRC,
			       uint32_t, uint32_t, uint32_t,
			       NONE, FLAG_NONE)
FAST_BILINEAR_MAINLOOP_COMMON (sse2_8888_8888_normal_SRC,
			       scaled_bilinear_scanline_sse2_8888_8888_SRC,
			       uint32_t, uint32_t, uint32_t,
			       NORMAL, FLAG_NONE)

static force_inline void
scaled_bilinear_scanline_sse2_x888_8888_SRC (uint32_t *       dst,
					     const uint32_t * mask,
					     const uint32_t * src_top,
					     const uint32_t * src_bottom,
					     int32_t          w,
					     int              wt,
					     int              wb,
					     pixman_fixed_t   vx_,
					     pixman_fixed_t   unit_x_,
					     pixman_fixed_t   max_vx,
					     pixman_bool_t    zero_src)
{
    intptr_t vx = vx_;
    intptr_t unit_x = unit_x_;
    BILINEAR_DECLARE_VARIABLES;
    uint32_t pix1, pix2;

    while (w && ((uintptr_t)dst & 15))
    {
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix1);
	*dst++ = pix1 | 0xFF000000;
	w--;
    }

    while ((w -= 4) >= 0) {
	__m128i xmm_src;
	BILINEAR_INTERPOLATE_FOUR_PIXELS (xmm_src);
	_mm_store_si128 ((__m128i *)dst, _mm_or_si128 (xmm_src, mask_ff000000));
	dst += 4;
    }

    if (w & 2)
    {
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix1);
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix2);
	*dst++ = pix1 | 0xFF000000;
	*dst++ = pix2 | 0xFF000000;
    }

    if (w & 1)
    {
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix1);
	*dst = pix1 | 0xFF000000;
    }
}

FAST_BILINEAR_MAINLOOP_COMMON (sse2_x888_8888_cover_SRC,
			       scaled_bilinear_scanline_sse2_x888_8888_SRC,
			       uint32_t, uint32_t, uint32_t,
			       COVER, FLAG_NONE)
FAST_BILINEAR_MAINLOOP_COMMON (sse2_x888_8888_pad_SRC,
			       scaled_bilinear_scanline_sse2_x888_8888_SRC,
			       uint32_t, uint32_t, uint32_t,
			       PAD, FLAG_NONE)
FAST_BILINEAR_MAINLOOP_COMMON (sse2_x888_8888_normal_SRC,
			       scaled_bilinear_scanline_sse2_x888_8888_SRC,
			       uint32_t, uint32_t, uint32_t,
			       NORMAL, FLAG_NONE)

static force_inline void
scaled_bilinear_scanline_sse2_8888_8888_OVER (uint32_t *       dst,
					      const uint32_t * mask,
					      const uint32_t * src_top,
					      const uint32_t * src_bottom,
					      int32_t          w,
					      int              wt,
					      int              wb,
					      pixman_fixed_t   vx_,
					      pixman_fixed_t   unit_x_,
					      pixman_fixed_t   max_vx,
					      pixman_bool_t    zero_src)
{
    intptr_t vx = vx_;
    intptr_t unit_x = unit_x_;
    BILINEAR_DECLARE_VARIABLES;
    uint32_t pix1, pix2;

    while (w && ((uintptr_t)dst & 15))
    {
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix1);

	if (pix1)
	{
	    pix2 = *dst;
	    *dst = core_combine_over_u_pixel_sse2 (pix1, pix2);
	}

	w--;
	dst++;
    }

    while (w  >= 4)
    {
	__m128i xmm_src;
	__m128i xmm_src_hi, xmm_src_lo, xmm_dst_hi, xmm_dst_lo;
	__m128i xmm_alpha_hi, xmm_alpha_lo;

	BILINEAR_INTERPOLATE_FOUR_PIXELS (xmm_src);

	if (!is_zero (xmm_src))
	{
	    if (is_opaque (xmm_src))
	    {
		save_128_aligned ((__m128i *)dst, xmm_src);
	    }
	    else
	    {
		__m128i xmm_dst = load_128_aligned ((__m128i *)dst);

		unpack_128_2x128 (xmm_src, &xmm_src_lo, &xmm_src_hi);
		unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);

		expand_alpha_2x128 (xmm_src_lo, xmm_src_hi, &xmm_alpha_lo, &xmm_alpha_hi);
		over_2x128 (&xmm_src_lo, &xmm_src_hi, &xmm_alpha_lo, &xmm_alpha_hi,
			    &xmm_dst_lo, &xmm_dst_hi);

		save_128_aligned ((__m128i *)dst, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));
	    }
	}

	w -= 4;
	dst += 4;
    }

    while (w)
    {
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix1);

	if (pix1)
	{
	    pix2 = *dst;
	    *dst = core_combine_over_u_pixel_sse2 (pix1, pix2);
	}

	w--;
	dst++;
    }
}

FAST_BILINEAR_MAINLOOP_COMMON (sse2_8888_8888_cover_OVER,
			       scaled_bilinear_scanline_sse2_8888_8888_OVER,
			       uint32_t, uint32_t, uint32_t,
			       COVER, FLAG_NONE)
FAST_BILINEAR_MAINLOOP_COMMON (sse2_8888_8888_pad_OVER,
			       scaled_bilinear_scanline_sse2_8888_8888_OVER,
			       uint32_t, uint32_t, uint32_t,
			       PAD, FLAG_NONE)
FAST_BILINEAR_MAINLOOP_COMMON (sse2_8888_8888_none_OVER,
			       scaled_bilinear_scanline_sse2_8888_8888_OVER,
			       uint32_t, uint32_t, uint32_t,
			       NONE, FLAG_NONE)
FAST_BILINEAR_MAINLOOP_COMMON (sse2_8888_8888_normal_OVER,
			       scaled_bilinear_scanline_sse2_8888_8888_OVER,
			       uint32_t, uint32_t, uint32_t,
			       NORMAL, FLAG_NONE)

static force_inline void
scaled_bilinear_scanline_sse2_8888_8_8888_OVER (uint32_t *       dst,
						const uint8_t  * mask,
						const uint32_t * src_top,
						const uint32_t * src_bottom,
						int32_t          w,
						int              wt,
						int              wb,
						pixman_fixed_t   vx_,
						pixman_fixed_t   unit_x_,
						pixman_fixed_t   max_vx,
						pixman_bool_t    zero_src)
{
    intptr_t vx = vx_;
    intptr_t unit_x = unit_x_;
    BILINEAR_DECLARE_VARIABLES;
    uint32_t pix1, pix2;
    uint32_t m;

    while (w && ((uintptr_t)dst & 15))
    {
	uint32_t sa;

	m = (uint32_t) *mask++;

	if (m)
	{
	    BILINEAR_INTERPOLATE_ONE_PIXEL (pix1);
	    sa = pix1 >> 24;

	    if (sa == 0xff && m == 0xff)
	    {
		*dst = pix1;
	    }
	    else
	    {
		__m128i ms, md, ma, msa;

		pix2 = *dst;
		ma = expand_alpha_rev_1x128 (load_32_1x128 (m));
		ms = unpack_32_1x128 (pix1);
		md = unpack_32_1x128 (pix2);

		msa = expand_alpha_rev_1x128 (load_32_1x128 (sa));

		*dst = pack_1x128_32 (in_over_1x128 (&ms, &msa, &ma, &md));
	    }
	}
	else
	{
	    BILINEAR_SKIP_ONE_PIXEL ();
	}

	w--;
	dst++;
    }

    while (w >= 4)
    {
	__m128i xmm_src, xmm_src_lo, xmm_src_hi, xmm_srca_lo, xmm_srca_hi;
	__m128i xmm_dst, xmm_dst_lo, xmm_dst_hi;
	__m128i xmm_mask, xmm_mask_lo, xmm_mask_hi;

	m = *(uint32_t*)mask;

	if (m)
	{
	    BILINEAR_INTERPOLATE_FOUR_PIXELS (xmm_src);

	    if (m == 0xffffffff && is_opaque (xmm_src))
	    {
		save_128_aligned ((__m128i *)dst, xmm_src);
	    }
	    else
	    {
		xmm_dst = load_128_aligned ((__m128i *)dst);

		xmm_mask = _mm_unpacklo_epi16 (unpack_32_1x128 (m), _mm_setzero_si128());

		unpack_128_2x128 (xmm_src, &xmm_src_lo, &xmm_src_hi);
		unpack_128_2x128 (xmm_mask, &xmm_mask_lo, &xmm_mask_hi);
		unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);

		expand_alpha_2x128 (xmm_src_lo, xmm_src_hi, &xmm_srca_lo, &xmm_srca_hi);
		expand_alpha_rev_2x128 (xmm_mask_lo, xmm_mask_hi, &xmm_mask_lo, &xmm_mask_hi);

		in_over_2x128 (&xmm_src_lo, &xmm_src_hi, &xmm_srca_lo, &xmm_srca_hi,
			       &xmm_mask_lo, &xmm_mask_hi, &xmm_dst_lo, &xmm_dst_hi);

		save_128_aligned ((__m128i*)dst, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));
	    }
	}
	else
	{
	    BILINEAR_SKIP_FOUR_PIXELS ();
	}

	w -= 4;
	dst += 4;
	mask += 4;
    }

    while (w)
    {
	uint32_t sa;

	m = (uint32_t) *mask++;

	if (m)
	{
	    BILINEAR_INTERPOLATE_ONE_PIXEL (pix1);
	    sa = pix1 >> 24;

	    if (sa == 0xff && m == 0xff)
	    {
		*dst = pix1;
	    }
	    else
	    {
		__m128i ms, md, ma, msa;

		pix2 = *dst;
		ma = expand_alpha_rev_1x128 (load_32_1x128 (m));
		ms = unpack_32_1x128 (pix1);
		md = unpack_32_1x128 (pix2);

		msa = expand_alpha_rev_1x128 (load_32_1x128 (sa));

		*dst = pack_1x128_32 (in_over_1x128 (&ms, &msa, &ma, &md));
	    }
	}
	else
	{
	    BILINEAR_SKIP_ONE_PIXEL ();
	}

	w--;
	dst++;
    }
}

FAST_BILINEAR_MAINLOOP_COMMON (sse2_8888_8_8888_cover_OVER,
			       scaled_bilinear_scanline_sse2_8888_8_8888_OVER,
			       uint32_t, uint8_t, uint32_t,
			       COVER, FLAG_HAVE_NON_SOLID_MASK)
FAST_BILINEAR_MAINLOOP_COMMON (sse2_8888_8_8888_pad_OVER,
			       scaled_bilinear_scanline_sse2_8888_8_8888_OVER,
			       uint32_t, uint8_t, uint32_t,
			       PAD, FLAG_HAVE_NON_SOLID_MASK)
FAST_BILINEAR_MAINLOOP_COMMON (sse2_8888_8_8888_none_OVER,
			       scaled_bilinear_scanline_sse2_8888_8_8888_OVER,
			       uint32_t, uint8_t, uint32_t,
			       NONE, FLAG_HAVE_NON_SOLID_MASK)
FAST_BILINEAR_MAINLOOP_COMMON (sse2_8888_8_8888_normal_OVER,
			       scaled_bilinear_scanline_sse2_8888_8_8888_OVER,
			       uint32_t, uint8_t, uint32_t,
			       NORMAL, FLAG_HAVE_NON_SOLID_MASK)

static force_inline void
scaled_bilinear_scanline_sse2_8888_n_8888_OVER (uint32_t *       dst,
						const uint32_t * mask,
						const uint32_t * src_top,
						const uint32_t * src_bottom,
						int32_t          w,
						int              wt,
						int              wb,
						pixman_fixed_t   vx_,
						pixman_fixed_t   unit_x_,
						pixman_fixed_t   max_vx,
						pixman_bool_t    zero_src)
{
    intptr_t vx = vx_;
    intptr_t unit_x = unit_x_;
    BILINEAR_DECLARE_VARIABLES;
    uint32_t pix1;
    __m128i xmm_mask;

    if (zero_src || (*mask >> 24) == 0)
	return;

    xmm_mask = create_mask_16_128 (*mask >> 24);

    while (w && ((uintptr_t)dst & 15))
    {
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix1);
	if (pix1)
	{
		uint32_t d = *dst;

		__m128i ms = unpack_32_1x128 (pix1);
		__m128i alpha     = expand_alpha_1x128 (ms);
		__m128i dest      = xmm_mask;
		__m128i alpha_dst = unpack_32_1x128 (d);

		*dst = pack_1x128_32
			(in_over_1x128 (&ms, &alpha, &dest, &alpha_dst));
	}

	dst++;
	w--;
    }

    while (w >= 4)
    {
	__m128i xmm_src;
	BILINEAR_INTERPOLATE_FOUR_PIXELS (xmm_src);

	if (!is_zero (xmm_src))
	{
	    __m128i xmm_src_lo, xmm_src_hi;
	    __m128i xmm_dst, xmm_dst_lo, xmm_dst_hi;
	    __m128i xmm_alpha_lo, xmm_alpha_hi;

	    xmm_dst = load_128_aligned ((__m128i*)dst);

	    unpack_128_2x128 (xmm_src, &xmm_src_lo, &xmm_src_hi);
	    unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);
	    expand_alpha_2x128 (xmm_src_lo, xmm_src_hi,
				&xmm_alpha_lo, &xmm_alpha_hi);

	    in_over_2x128 (&xmm_src_lo, &xmm_src_hi,
			   &xmm_alpha_lo, &xmm_alpha_hi,
			   &xmm_mask, &xmm_mask,
			   &xmm_dst_lo, &xmm_dst_hi);

	    save_128_aligned
		((__m128i*)dst, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));
	}

	dst += 4;
	w -= 4;
    }

    while (w)
    {
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix1);
	if (pix1)
	{
		uint32_t d = *dst;

		__m128i ms = unpack_32_1x128 (pix1);
		__m128i alpha     = expand_alpha_1x128 (ms);
		__m128i dest      = xmm_mask;
		__m128i alpha_dst = unpack_32_1x128 (d);

		*dst = pack_1x128_32
			(in_over_1x128 (&ms, &alpha, &dest, &alpha_dst));
	}

	dst++;
	w--;
    }
}

FAST_BILINEAR_MAINLOOP_COMMON (sse2_8888_n_8888_cover_OVER,
			       scaled_bilinear_scanline_sse2_8888_n_8888_OVER,
			       uint32_t, uint32_t, uint32_t,
			       COVER, FLAG_HAVE_SOLID_MASK)
FAST_BILINEAR_MAINLOOP_COMMON (sse2_8888_n_8888_pad_OVER,
			       scaled_bilinear_scanline_sse2_8888_n_8888_OVER,
			       uint32_t, uint32_t, uint32_t,
			       PAD, FLAG_HAVE_SOLID_MASK)
FAST_BILINEAR_MAINLOOP_COMMON (sse2_8888_n_8888_none_OVER,
			       scaled_bilinear_scanline_sse2_8888_n_8888_OVER,
			       uint32_t, uint32_t, uint32_t,
			       NONE, FLAG_HAVE_SOLID_MASK)
FAST_BILINEAR_MAINLOOP_COMMON (sse2_8888_n_8888_normal_OVER,
			       scaled_bilinear_scanline_sse2_8888_n_8888_OVER,
			       uint32_t, uint32_t, uint32_t,
			       NORMAL, FLAG_HAVE_SOLID_MASK)

static const pixman_fast_path_t sse2_fast_paths[] =
{
    /* PIXMAN_OP_OVER */
    PIXMAN_STD_FAST_PATH (OVER, solid, a8, r5g6b5, sse2_composite_over_n_8_0565),
    PIXMAN_STD_FAST_PATH (OVER, solid, a8, b5g6r5, sse2_composite_over_n_8_0565),
    PIXMAN_STD_FAST_PATH (OVER, solid, null, a8r8g8b8, sse2_composite_over_n_8888),
    PIXMAN_STD_FAST_PATH (OVER, solid, null, x8r8g8b8, sse2_composite_over_n_8888),
    PIXMAN_STD_FAST_PATH (OVER, solid, null, r5g6b5, sse2_composite_over_n_0565),
    PIXMAN_STD_FAST_PATH (OVER, solid, null, b5g6r5, sse2_composite_over_n_0565),
    PIXMAN_STD_FAST_PATH (OVER, a8r8g8b8, null, a8r8g8b8, sse2_composite_over_8888_8888),
    PIXMAN_STD_FAST_PATH (OVER, a8r8g8b8, null, x8r8g8b8, sse2_composite_over_8888_8888),
    PIXMAN_STD_FAST_PATH (OVER, a8b8g8r8, null, a8b8g8r8, sse2_composite_over_8888_8888),
    PIXMAN_STD_FAST_PATH (OVER, a8b8g8r8, null, x8b8g8r8, sse2_composite_over_8888_8888),
    PIXMAN_STD_FAST_PATH (OVER, a8r8g8b8, null, r5g6b5, sse2_composite_over_8888_0565),
    PIXMAN_STD_FAST_PATH (OVER, a8b8g8r8, null, b5g6r5, sse2_composite_over_8888_0565),
    PIXMAN_STD_FAST_PATH (OVER, solid, a8, a8r8g8b8, sse2_composite_over_n_8_8888),
    PIXMAN_STD_FAST_PATH (OVER, solid, a8, x8r8g8b8, sse2_composite_over_n_8_8888),
    PIXMAN_STD_FAST_PATH (OVER, solid, a8, a8b8g8r8, sse2_composite_over_n_8_8888),
    PIXMAN_STD_FAST_PATH (OVER, solid, a8, x8b8g8r8, sse2_composite_over_n_8_8888),
    PIXMAN_STD_FAST_PATH (OVER, a8r8g8b8, a8r8g8b8, a8r8g8b8, sse2_composite_over_8888_8888_8888),
    PIXMAN_STD_FAST_PATH (OVER, a8r8g8b8, a8, x8r8g8b8, sse2_composite_over_8888_8_8888),
    PIXMAN_STD_FAST_PATH (OVER, a8r8g8b8, a8, a8r8g8b8, sse2_composite_over_8888_8_8888),
    PIXMAN_STD_FAST_PATH (OVER, a8b8g8r8, a8, x8b8g8r8, sse2_composite_over_8888_8_8888),
    PIXMAN_STD_FAST_PATH (OVER, a8b8g8r8, a8, a8b8g8r8, sse2_composite_over_8888_8_8888),
    PIXMAN_STD_FAST_PATH (OVER, x8r8g8b8, a8, x8r8g8b8, sse2_composite_over_x888_8_8888),
    PIXMAN_STD_FAST_PATH (OVER, x8r8g8b8, a8, a8r8g8b8, sse2_composite_over_x888_8_8888),
    PIXMAN_STD_FAST_PATH (OVER, x8b8g8r8, a8, x8b8g8r8, sse2_composite_over_x888_8_8888),
    PIXMAN_STD_FAST_PATH (OVER, x8b8g8r8, a8, a8b8g8r8, sse2_composite_over_x888_8_8888),
    PIXMAN_STD_FAST_PATH (OVER, x8r8g8b8, solid, a8r8g8b8, sse2_composite_over_x888_n_8888),
    PIXMAN_STD_FAST_PATH (OVER, x8r8g8b8, solid, x8r8g8b8, sse2_composite_over_x888_n_8888),
    PIXMAN_STD_FAST_PATH (OVER, x8b8g8r8, solid, a8b8g8r8, sse2_composite_over_x888_n_8888),
    PIXMAN_STD_FAST_PATH (OVER, x8b8g8r8, solid, x8b8g8r8, sse2_composite_over_x888_n_8888),
    PIXMAN_STD_FAST_PATH (OVER, a8r8g8b8, solid, a8r8g8b8, sse2_composite_over_8888_n_8888),
    PIXMAN_STD_FAST_PATH (OVER, a8r8g8b8, solid, x8r8g8b8, sse2_composite_over_8888_n_8888),
    PIXMAN_STD_FAST_PATH (OVER, a8b8g8r8, solid, a8b8g8r8, sse2_composite_over_8888_n_8888),
    PIXMAN_STD_FAST_PATH (OVER, a8b8g8r8, solid, x8b8g8r8, sse2_composite_over_8888_n_8888),
    PIXMAN_STD_FAST_PATH_CA (OVER, solid, a8r8g8b8, a8r8g8b8, sse2_composite_over_n_8888_8888_ca),
    PIXMAN_STD_FAST_PATH_CA (OVER, solid, a8r8g8b8, x8r8g8b8, sse2_composite_over_n_8888_8888_ca),
    PIXMAN_STD_FAST_PATH_CA (OVER, solid, a8b8g8r8, a8b8g8r8, sse2_composite_over_n_8888_8888_ca),
    PIXMAN_STD_FAST_PATH_CA (OVER, solid, a8b8g8r8, x8b8g8r8, sse2_composite_over_n_8888_8888_ca),
    PIXMAN_STD_FAST_PATH_CA (OVER, solid, a8r8g8b8, r5g6b5, sse2_composite_over_n_8888_0565_ca),
    PIXMAN_STD_FAST_PATH_CA (OVER, solid, a8b8g8r8, b5g6r5, sse2_composite_over_n_8888_0565_ca),
    PIXMAN_STD_FAST_PATH (OVER, pixbuf, pixbuf, a8r8g8b8, sse2_composite_over_pixbuf_8888),
    PIXMAN_STD_FAST_PATH (OVER, pixbuf, pixbuf, x8r8g8b8, sse2_composite_over_pixbuf_8888),
    PIXMAN_STD_FAST_PATH (OVER, rpixbuf, rpixbuf, a8b8g8r8, sse2_composite_over_pixbuf_8888),
    PIXMAN_STD_FAST_PATH (OVER, rpixbuf, rpixbuf, x8b8g8r8, sse2_composite_over_pixbuf_8888),
    PIXMAN_STD_FAST_PATH (OVER, pixbuf, pixbuf, r5g6b5, sse2_composite_over_pixbuf_0565),
    PIXMAN_STD_FAST_PATH (OVER, rpixbuf, rpixbuf, b5g6r5, sse2_composite_over_pixbuf_0565),
    PIXMAN_STD_FAST_PATH (OVER, x8r8g8b8, null, x8r8g8b8, sse2_composite_copy_area),
    PIXMAN_STD_FAST_PATH (OVER, x8b8g8r8, null, x8b8g8r8, sse2_composite_copy_area),
    
    /* PIXMAN_OP_OVER_REVERSE */
    PIXMAN_STD_FAST_PATH (OVER_REVERSE, solid, null, a8r8g8b8, sse2_composite_over_reverse_n_8888),
    PIXMAN_STD_FAST_PATH (OVER_REVERSE, solid, null, a8b8g8r8, sse2_composite_over_reverse_n_8888),

    /* PIXMAN_OP_ADD */
    PIXMAN_STD_FAST_PATH_CA (ADD, solid, a8r8g8b8, a8r8g8b8, sse2_composite_add_n_8888_8888_ca),
    PIXMAN_STD_FAST_PATH (ADD, a8, null, a8, sse2_composite_add_8_8),
    PIXMAN_STD_FAST_PATH (ADD, a8r8g8b8, null, a8r8g8b8, sse2_composite_add_8888_8888),
    PIXMAN_STD_FAST_PATH (ADD, a8b8g8r8, null, a8b8g8r8, sse2_composite_add_8888_8888),
    PIXMAN_STD_FAST_PATH (ADD, solid, a8, a8, sse2_composite_add_n_8_8),
    PIXMAN_STD_FAST_PATH (ADD, solid, null, a8, sse2_composite_add_n_8),
    PIXMAN_STD_FAST_PATH (ADD, solid, null, x8r8g8b8, sse2_composite_add_n_8888),
    PIXMAN_STD_FAST_PATH (ADD, solid, null, a8r8g8b8, sse2_composite_add_n_8888),
    PIXMAN_STD_FAST_PATH (ADD, solid, null, x8b8g8r8, sse2_composite_add_n_8888),
    PIXMAN_STD_FAST_PATH (ADD, solid, null, a8b8g8r8, sse2_composite_add_n_8888),
    PIXMAN_STD_FAST_PATH (ADD, solid, a8, x8r8g8b8, sse2_composite_add_n_8_8888),
    PIXMAN_STD_FAST_PATH (ADD, solid, a8, a8r8g8b8, sse2_composite_add_n_8_8888),
    PIXMAN_STD_FAST_PATH (ADD, solid, a8, x8b8g8r8, sse2_composite_add_n_8_8888),
    PIXMAN_STD_FAST_PATH (ADD, solid, a8, a8b8g8r8, sse2_composite_add_n_8_8888),

    /* PIXMAN_OP_SRC */
    PIXMAN_STD_FAST_PATH (SRC, solid, a8, a8r8g8b8, sse2_composite_src_n_8_8888),
    PIXMAN_STD_FAST_PATH (SRC, solid, a8, x8r8g8b8, sse2_composite_src_n_8_8888),
    PIXMAN_STD_FAST_PATH (SRC, solid, a8, a8b8g8r8, sse2_composite_src_n_8_8888),
    PIXMAN_STD_FAST_PATH (SRC, solid, a8, x8b8g8r8, sse2_composite_src_n_8_8888),
    PIXMAN_STD_FAST_PATH (SRC, a8r8g8b8, null, r5g6b5, sse2_composite_src_x888_0565),
    PIXMAN_STD_FAST_PATH (SRC, a8b8g8r8, null, b5g6r5, sse2_composite_src_x888_0565),
    PIXMAN_STD_FAST_PATH (SRC, x8r8g8b8, null, r5g6b5, sse2_composite_src_x888_0565),
    PIXMAN_STD_FAST_PATH (SRC, x8b8g8r8, null, b5g6r5, sse2_composite_src_x888_0565),
    PIXMAN_STD_FAST_PATH (SRC, x8r8g8b8, null, a8r8g8b8, sse2_composite_src_x888_8888),
    PIXMAN_STD_FAST_PATH (SRC, x8b8g8r8, null, a8b8g8r8, sse2_composite_src_x888_8888),
    PIXMAN_STD_FAST_PATH (SRC, a8r8g8b8, null, a8r8g8b8, sse2_composite_copy_area),
    PIXMAN_STD_FAST_PATH (SRC, a8b8g8r8, null, a8b8g8r8, sse2_composite_copy_area),
    PIXMAN_STD_FAST_PATH (SRC, a8r8g8b8, null, x8r8g8b8, sse2_composite_copy_area),
    PIXMAN_STD_FAST_PATH (SRC, a8b8g8r8, null, x8b8g8r8, sse2_composite_copy_area),
    PIXMAN_STD_FAST_PATH (SRC, x8r8g8b8, null, x8r8g8b8, sse2_composite_copy_area),
    PIXMAN_STD_FAST_PATH (SRC, x8b8g8r8, null, x8b8g8r8, sse2_composite_copy_area),
    PIXMAN_STD_FAST_PATH (SRC, r5g6b5, null, r5g6b5, sse2_composite_copy_area),
    PIXMAN_STD_FAST_PATH (SRC, b5g6r5, null, b5g6r5, sse2_composite_copy_area),

    /* PIXMAN_OP_IN */
    PIXMAN_STD_FAST_PATH (IN, a8, null, a8, sse2_composite_in_8_8),
    PIXMAN_STD_FAST_PATH (IN, solid, a8, a8, sse2_composite_in_n_8_8),
    PIXMAN_STD_FAST_PATH (IN, solid, null, a8, sse2_composite_in_n_8),

    SIMPLE_NEAREST_FAST_PATH (OVER, a8r8g8b8, x8r8g8b8, sse2_8888_8888),
    SIMPLE_NEAREST_FAST_PATH (OVER, a8b8g8r8, x8b8g8r8, sse2_8888_8888),
    SIMPLE_NEAREST_FAST_PATH (OVER, a8r8g8b8, a8r8g8b8, sse2_8888_8888),
    SIMPLE_NEAREST_FAST_PATH (OVER, a8b8g8r8, a8b8g8r8, sse2_8888_8888),

    SIMPLE_NEAREST_SOLID_MASK_FAST_PATH (OVER, a8r8g8b8, a8r8g8b8, sse2_8888_n_8888),
    SIMPLE_NEAREST_SOLID_MASK_FAST_PATH (OVER, a8b8g8r8, a8b8g8r8, sse2_8888_n_8888),
    SIMPLE_NEAREST_SOLID_MASK_FAST_PATH (OVER, a8r8g8b8, x8r8g8b8, sse2_8888_n_8888),
    SIMPLE_NEAREST_SOLID_MASK_FAST_PATH (OVER, a8b8g8r8, x8b8g8r8, sse2_8888_n_8888),

    SIMPLE_BILINEAR_FAST_PATH (SRC, a8r8g8b8, a8r8g8b8, sse2_8888_8888),
    SIMPLE_BILINEAR_FAST_PATH (SRC, a8r8g8b8, x8r8g8b8, sse2_8888_8888),
    SIMPLE_BILINEAR_FAST_PATH (SRC, x8r8g8b8, x8r8g8b8, sse2_8888_8888),
    SIMPLE_BILINEAR_FAST_PATH (SRC, a8b8g8r8, a8b8g8r8, sse2_8888_8888),
    SIMPLE_BILINEAR_FAST_PATH (SRC, a8b8g8r8, x8b8g8r8, sse2_8888_8888),
    SIMPLE_BILINEAR_FAST_PATH (SRC, x8b8g8r8, x8b8g8r8, sse2_8888_8888),

    SIMPLE_BILINEAR_FAST_PATH_COVER  (SRC, x8r8g8b8, a8r8g8b8, sse2_x888_8888),
    SIMPLE_BILINEAR_FAST_PATH_COVER  (SRC, x8b8g8r8, a8b8g8r8, sse2_x888_8888),
    SIMPLE_BILINEAR_FAST_PATH_PAD    (SRC, x8r8g8b8, a8r8g8b8, sse2_x888_8888),
    SIMPLE_BILINEAR_FAST_PATH_PAD    (SRC, x8b8g8r8, a8b8g8r8, sse2_x888_8888),
    SIMPLE_BILINEAR_FAST_PATH_NORMAL (SRC, x8r8g8b8, a8r8g8b8, sse2_x888_8888),
    SIMPLE_BILINEAR_FAST_PATH_NORMAL (SRC, x8b8g8r8, a8b8g8r8, sse2_x888_8888),

    SIMPLE_BILINEAR_FAST_PATH (OVER, a8r8g8b8, x8r8g8b8, sse2_8888_8888),
    SIMPLE_BILINEAR_FAST_PATH (OVER, a8b8g8r8, x8b8g8r8, sse2_8888_8888),
    SIMPLE_BILINEAR_FAST_PATH (OVER, a8r8g8b8, a8r8g8b8, sse2_8888_8888),
    SIMPLE_BILINEAR_FAST_PATH (OVER, a8b8g8r8, a8b8g8r8, sse2_8888_8888),

    SIMPLE_BILINEAR_SOLID_MASK_FAST_PATH (OVER, a8r8g8b8, x8r8g8b8, sse2_8888_n_8888),
    SIMPLE_BILINEAR_SOLID_MASK_FAST_PATH (OVER, a8b8g8r8, x8b8g8r8, sse2_8888_n_8888),
    SIMPLE_BILINEAR_SOLID_MASK_FAST_PATH (OVER, a8r8g8b8, a8r8g8b8, sse2_8888_n_8888),
    SIMPLE_BILINEAR_SOLID_MASK_FAST_PATH (OVER, a8b8g8r8, a8b8g8r8, sse2_8888_n_8888),

    SIMPLE_BILINEAR_A8_MASK_FAST_PATH (OVER, a8r8g8b8, x8r8g8b8, sse2_8888_8_8888),
    SIMPLE_BILINEAR_A8_MASK_FAST_PATH (OVER, a8b8g8r8, x8b8g8r8, sse2_8888_8_8888),
    SIMPLE_BILINEAR_A8_MASK_FAST_PATH (OVER, a8r8g8b8, a8r8g8b8, sse2_8888_8_8888),
    SIMPLE_BILINEAR_A8_MASK_FAST_PATH (OVER, a8b8g8r8, a8b8g8r8, sse2_8888_8_8888),

    { PIXMAN_OP_NONE },
};

static uint32_t *
sse2_fetch_x8r8g8b8 (pixman_iter_t *iter, const uint32_t *mask)
{
    int w = iter->width;
    __m128i ff000000 = mask_ff000000;
    uint32_t *dst = iter->buffer;
    uint32_t *src = (uint32_t *)iter->bits;

    iter->bits += iter->stride;

    while (w && ((uintptr_t)dst) & 0x0f)
    {
	*dst++ = (*src++) | 0xff000000;
	w--;
    }

    while (w >= 4)
    {
	save_128_aligned (
	    (__m128i *)dst, _mm_or_si128 (
		load_128_unaligned ((__m128i *)src), ff000000));

	dst += 4;
	src += 4;
	w -= 4;
    }

    while (w)
    {
	*dst++ = (*src++) | 0xff000000;
	w--;
    }

    return iter->buffer;
}

static uint32_t *
sse2_fetch_r5g6b5 (pixman_iter_t *iter, const uint32_t *mask)
{
    int w = iter->width;
    uint32_t *dst = iter->buffer;
    uint16_t *src = (uint16_t *)iter->bits;
    __m128i ff000000 = mask_ff000000;

    iter->bits += iter->stride;

    while (w && ((uintptr_t)dst) & 0x0f)
    {
	uint16_t s = *src++;

	*dst++ = convert_0565_to_8888 (s);
	w--;
    }

    while (w >= 8)
    {
	__m128i lo, hi, s;

	s = _mm_loadu_si128 ((__m128i *)src);

	lo = unpack_565_to_8888 (_mm_unpacklo_epi16 (s, _mm_setzero_si128 ()));
	hi = unpack_565_to_8888 (_mm_unpackhi_epi16 (s, _mm_setzero_si128 ()));

	save_128_aligned ((__m128i *)(dst + 0), _mm_or_si128 (lo, ff000000));
	save_128_aligned ((__m128i *)(dst + 4), _mm_or_si128 (hi, ff000000));

	dst += 8;
	src += 8;
	w -= 8;
    }

    while (w)
    {
	uint16_t s = *src++;

	*dst++ = convert_0565_to_8888 (s);
	w--;
    }

    return iter->buffer;
}

static uint32_t *
sse2_fetch_a8 (pixman_iter_t *iter, const uint32_t *mask)
{
    int w = iter->width;
    uint32_t *dst = iter->buffer;
    uint8_t *src = iter->bits;
    __m128i xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6;

    iter->bits += iter->stride;

    while (w && (((uintptr_t)dst) & 15))
    {
        *dst++ = *(src++) << 24;
        w--;
    }

    while (w >= 16)
    {
	xmm0 = _mm_loadu_si128((__m128i *)src);

	xmm1 = _mm_unpacklo_epi8  (_mm_setzero_si128(), xmm0);
	xmm2 = _mm_unpackhi_epi8  (_mm_setzero_si128(), xmm0);
	xmm3 = _mm_unpacklo_epi16 (_mm_setzero_si128(), xmm1);
	xmm4 = _mm_unpackhi_epi16 (_mm_setzero_si128(), xmm1);
	xmm5 = _mm_unpacklo_epi16 (_mm_setzero_si128(), xmm2);
	xmm6 = _mm_unpackhi_epi16 (_mm_setzero_si128(), xmm2);

	_mm_store_si128(((__m128i *)(dst +  0)), xmm3);
	_mm_store_si128(((__m128i *)(dst +  4)), xmm4);
	_mm_store_si128(((__m128i *)(dst +  8)), xmm5);
	_mm_store_si128(((__m128i *)(dst + 12)), xmm6);

	dst += 16;
	src += 16;
	w -= 16;
    }

    while (w)
    {
	*dst++ = *(src++) << 24;
	w--;
    }

    return iter->buffer;
}

#define IMAGE_FLAGS							\
    (FAST_PATH_STANDARD_FLAGS | FAST_PATH_ID_TRANSFORM |		\
     FAST_PATH_BITS_IMAGE | FAST_PATH_SAMPLES_COVER_CLIP_NEAREST)

static const pixman_iter_info_t sse2_iters[] = 
{
    { PIXMAN_x8r8g8b8, IMAGE_FLAGS, ITER_NARROW,
      _pixman_iter_init_bits_stride, sse2_fetch_x8r8g8b8, NULL
    },
    { PIXMAN_r5g6b5, IMAGE_FLAGS, ITER_NARROW,
      _pixman_iter_init_bits_stride, sse2_fetch_r5g6b5, NULL
    },
    { PIXMAN_a8, IMAGE_FLAGS, ITER_NARROW,
      _pixman_iter_init_bits_stride, sse2_fetch_a8, NULL
    },
    { PIXMAN_null },
};

#if defined(__GNUC__) && !defined(__x86_64__) && !defined(__amd64__)
__attribute__((__force_align_arg_pointer__))
#endif
pixman_implementation_t *
_pixman_implementation_create_sse2 (pixman_implementation_t *fallback)
{
    pixman_implementation_t *imp = _pixman_implementation_create (fallback, sse2_fast_paths);

    /* SSE2 constants */
    mask_565_r  = create_mask_2x32_128 (0x00f80000, 0x00f80000);
    mask_565_g1 = create_mask_2x32_128 (0x00070000, 0x00070000);
    mask_565_g2 = create_mask_2x32_128 (0x000000e0, 0x000000e0);
    mask_565_b  = create_mask_2x32_128 (0x0000001f, 0x0000001f);
    mask_red   = create_mask_2x32_128 (0x00f80000, 0x00f80000);
    mask_green = create_mask_2x32_128 (0x0000fc00, 0x0000fc00);
    mask_blue  = create_mask_2x32_128 (0x000000f8, 0x000000f8);
    mask_565_fix_rb = create_mask_2x32_128 (0x00e000e0, 0x00e000e0);
    mask_565_fix_g = create_mask_2x32_128  (0x0000c000, 0x0000c000);
    mask_0080 = create_mask_16_128 (0x0080);
    mask_00ff = create_mask_16_128 (0x00ff);
    mask_0101 = create_mask_16_128 (0x0101);
    mask_ffff = create_mask_16_128 (0xffff);
    mask_ff000000 = create_mask_2x32_128 (0xff000000, 0xff000000);
    mask_alpha = create_mask_2x32_128 (0x00ff0000, 0x00000000);
    mask_565_rb = create_mask_2x32_128 (0x00f800f8, 0x00f800f8);
    mask_565_pack_multiplier = create_mask_2x32_128 (0x20000004, 0x20000004);

    /* Set up function pointers */
    imp->combine_32[PIXMAN_OP_OVER] = sse2_combine_over_u;
    imp->combine_32[PIXMAN_OP_OVER_REVERSE] = sse2_combine_over_reverse_u;
    imp->combine_32[PIXMAN_OP_IN] = sse2_combine_in_u;
    imp->combine_32[PIXMAN_OP_IN_REVERSE] = sse2_combine_in_reverse_u;
    imp->combine_32[PIXMAN_OP_OUT] = sse2_combine_out_u;
    imp->combine_32[PIXMAN_OP_OUT_REVERSE] = sse2_combine_out_reverse_u;
    imp->combine_32[PIXMAN_OP_ATOP] = sse2_combine_atop_u;
    imp->combine_32[PIXMAN_OP_ATOP_REVERSE] = sse2_combine_atop_reverse_u;
    imp->combine_32[PIXMAN_OP_XOR] = sse2_combine_xor_u;
    imp->combine_32[PIXMAN_OP_ADD] = sse2_combine_add_u;

    imp->combine_32[PIXMAN_OP_SATURATE] = sse2_combine_saturate_u;

    imp->combine_32_ca[PIXMAN_OP_SRC] = sse2_combine_src_ca;
    imp->combine_32_ca[PIXMAN_OP_OVER] = sse2_combine_over_ca;
    imp->combine_32_ca[PIXMAN_OP_OVER_REVERSE] = sse2_combine_over_reverse_ca;
    imp->combine_32_ca[PIXMAN_OP_IN] = sse2_combine_in_ca;
    imp->combine_32_ca[PIXMAN_OP_IN_REVERSE] = sse2_combine_in_reverse_ca;
    imp->combine_32_ca[PIXMAN_OP_OUT] = sse2_combine_out_ca;
    imp->combine_32_ca[PIXMAN_OP_OUT_REVERSE] = sse2_combine_out_reverse_ca;
    imp->combine_32_ca[PIXMAN_OP_ATOP] = sse2_combine_atop_ca;
    imp->combine_32_ca[PIXMAN_OP_ATOP_REVERSE] = sse2_combine_atop_reverse_ca;
    imp->combine_32_ca[PIXMAN_OP_XOR] = sse2_combine_xor_ca;
    imp->combine_32_ca[PIXMAN_OP_ADD] = sse2_combine_add_ca;

    imp->blt = sse2_blt;
    imp->fill = sse2_fill;

    imp->iter_info = sse2_iters;

    return imp;
}
@


1.11
log
@Update to pixman 0.32.4. Tested by naddy@@ and ajacoutot@@
@
text
@d6277 4
a6280 16
    SIMPLE_NEAREST_FAST_PATH_COVER (OVER, a8r8g8b8, x8r8g8b8, sse2_8888_8888),
    SIMPLE_NEAREST_FAST_PATH_COVER (OVER, a8b8g8r8, x8b8g8r8, sse2_8888_8888),
    SIMPLE_NEAREST_FAST_PATH_COVER (OVER, a8r8g8b8, a8r8g8b8, sse2_8888_8888),
    SIMPLE_NEAREST_FAST_PATH_COVER (OVER, a8b8g8r8, a8b8g8r8, sse2_8888_8888),
    SIMPLE_NEAREST_FAST_PATH_NONE (OVER, a8r8g8b8, x8r8g8b8, sse2_8888_8888),
    SIMPLE_NEAREST_FAST_PATH_NONE (OVER, a8b8g8r8, x8b8g8r8, sse2_8888_8888),
    SIMPLE_NEAREST_FAST_PATH_NONE (OVER, a8r8g8b8, a8r8g8b8, sse2_8888_8888),
    SIMPLE_NEAREST_FAST_PATH_NONE (OVER, a8b8g8r8, a8b8g8r8, sse2_8888_8888),
    SIMPLE_NEAREST_FAST_PATH_PAD (OVER, a8r8g8b8, x8r8g8b8, sse2_8888_8888),
    SIMPLE_NEAREST_FAST_PATH_PAD (OVER, a8b8g8r8, x8b8g8r8, sse2_8888_8888),
    SIMPLE_NEAREST_FAST_PATH_PAD (OVER, a8r8g8b8, a8r8g8b8, sse2_8888_8888),
    SIMPLE_NEAREST_FAST_PATH_PAD (OVER, a8b8g8r8, a8b8g8r8, sse2_8888_8888),
    SIMPLE_NEAREST_FAST_PATH_NORMAL (OVER, a8r8g8b8, x8r8g8b8, sse2_8888_8888),
    SIMPLE_NEAREST_FAST_PATH_NORMAL (OVER, a8b8g8r8, x8b8g8r8, sse2_8888_8888),
    SIMPLE_NEAREST_FAST_PATH_NORMAL (OVER, a8r8g8b8, a8r8g8b8, sse2_8888_8888),
    SIMPLE_NEAREST_FAST_PATH_NORMAL (OVER, a8b8g8r8, a8b8g8r8, sse2_8888_8888),
a6285 4
    SIMPLE_NEAREST_SOLID_MASK_FAST_PATH_NORMAL (OVER, a8r8g8b8, a8r8g8b8, sse2_8888_n_8888),
    SIMPLE_NEAREST_SOLID_MASK_FAST_PATH_NORMAL (OVER, a8b8g8r8, a8b8g8r8, sse2_8888_n_8888),
    SIMPLE_NEAREST_SOLID_MASK_FAST_PATH_NORMAL (OVER, a8r8g8b8, x8r8g8b8, sse2_8888_n_8888),
    SIMPLE_NEAREST_SOLID_MASK_FAST_PATH_NORMAL (OVER, a8b8g8r8, x8b8g8r8, sse2_8888_n_8888),
@


1.10
log
@Update to pixman 0.30.0. Tested by several people during t2k13. Thanks.
@
text
@d33 3
d5560 4
a5563 1
#if BILINEAR_INTERPOLATION_BITS < 8
d5568 6
a5573 2
    const __m128i xmm_ux = _mm_set_epi16 (unit_x, -unit_x, unit_x, -unit_x,	\
					  unit_x, -unit_x, unit_x, -unit_x);	\
d5575 37
a5611 3
    __m128i xmm_x = _mm_set_epi16 (vx, -(vx + 1), vx, -(vx + 1),		\
				   vx, -(vx + 1), vx, -(vx + 1))
#else
d5615 7
a5621 3
    const __m128i xmm_addc = _mm_set_epi16 (0, 0, 0, 0, 1, 1, 1, 1);		\
    const __m128i xmm_ux = _mm_set_epi16 (unit_x, unit_x, unit_x, unit_x,	\
					  -unit_x, -unit_x, -unit_x, -unit_x);	\
d5623 2
a5624 3
    __m128i xmm_x = _mm_set_epi16 (vx, vx, vx, vx,				\
				   -(vx + 1), -(vx + 1), -(vx + 1), -(vx + 1))
#endif
d5626 1
a5626 1
#define BILINEAR_INTERPOLATE_ONE_PIXEL(pix)					\
d5628 1
a5628 1
    __m128i xmm_wh, xmm_lo, xmm_hi, a;						\
d5630 3
a5632 4
    __m128i tltr = _mm_loadl_epi64 (						\
			    (__m128i *)&src_top[pixman_fixed_to_int (vx)]);	\
    __m128i blbr = _mm_loadl_epi64 (						\
			    (__m128i *)&src_bottom[pixman_fixed_to_int (vx)]);	\
d5635 5
a5639 8
    a = _mm_add_epi16 (_mm_mullo_epi16 (_mm_unpacklo_epi8 (tltr, xmm_zero),	\
					xmm_wt),				\
		       _mm_mullo_epi16 (_mm_unpacklo_epi8 (blbr, xmm_zero),	\
					xmm_wb));				\
    if (BILINEAR_INTERPOLATION_BITS < 8)					\
    {										\
	/* calculate horizontal weights */					\
	xmm_wh = _mm_add_epi16 (xmm_addc, _mm_srli_epi16 (xmm_x,		\
d5641 6
a5646 22
	xmm_x = _mm_add_epi16 (xmm_x, xmm_ux);					\
	/* horizontal interpolation */						\
	a = _mm_madd_epi16 (_mm_unpackhi_epi16 (_mm_shuffle_epi32 (		\
		a, _MM_SHUFFLE (1, 0, 3, 2)), a), xmm_wh);			\
    }										\
    else									\
    {										\
	/* calculate horizontal weights */					\
	xmm_wh = _mm_add_epi16 (xmm_addc, _mm_srli_epi16 (xmm_x,		\
					16 - BILINEAR_INTERPOLATION_BITS));	\
	xmm_x = _mm_add_epi16 (xmm_x, xmm_ux);					\
	/* horizontal interpolation */						\
	xmm_lo = _mm_mullo_epi16 (a, xmm_wh);					\
	xmm_hi = _mm_mulhi_epu16 (a, xmm_wh);					\
	a = _mm_add_epi32 (_mm_unpacklo_epi16 (xmm_lo, xmm_hi),			\
			   _mm_unpackhi_epi16 (xmm_lo, xmm_hi));		\
    }										\
    /* shift and pack the result */						\
    a = _mm_srli_epi32 (a, BILINEAR_INTERPOLATION_BITS * 2);			\
    a = _mm_packs_epi32 (a, a);							\
    a = _mm_packus_epi16 (a, a);						\
    pix = _mm_cvtsi128_si32 (a);						\
d5649 25
d5677 7
a5683 1
    xmm_x = _mm_add_epi16 (xmm_x, xmm_ux);					\
d5686 2
d5696 2
a5697 2
					     pixman_fixed_t   vx,
					     pixman_fixed_t   unit_x,
d5701 2
d5704 1
a5704 1
    uint32_t pix1, pix2, pix3, pix4;
d5706 1
a5706 1
    while ((w -= 4) >= 0)
a5708 3
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix2);
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix3);
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix4);
d5710 8
a5717 3
	*dst++ = pix2;
	*dst++ = pix3;
	*dst++ = pix4;
d5754 60
d5821 2
a5822 2
					      pixman_fixed_t   vx,
					      pixman_fixed_t   unit_x,
d5826 2
d5829 1
a5829 1
    uint32_t pix1, pix2, pix3, pix4;
d5851 1
a5851 6
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix1);
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix2);
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix3);
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix4);

	xmm_src = _mm_set_epi32 (pix4, pix3, pix2, pix1);
d5918 2
a5919 2
						pixman_fixed_t   vx,
						pixman_fixed_t   unit_x,
d5923 2
d5926 1
a5926 1
    uint32_t pix1, pix2, pix3, pix4;
d5977 1
a5977 6
	    BILINEAR_INTERPOLATE_ONE_PIXEL (pix1);
	    BILINEAR_INTERPOLATE_ONE_PIXEL (pix2);
	    BILINEAR_INTERPOLATE_ONE_PIXEL (pix3);
	    BILINEAR_INTERPOLATE_ONE_PIXEL (pix4);

	    xmm_src = _mm_set_epi32 (pix4, pix3, pix2, pix1);
d6004 1
a6004 4
	    BILINEAR_SKIP_ONE_PIXEL ();
	    BILINEAR_SKIP_ONE_PIXEL ();
	    BILINEAR_SKIP_ONE_PIXEL ();
	    BILINEAR_SKIP_ONE_PIXEL ();
d6076 2
a6077 2
						pixman_fixed_t   vx,
						pixman_fixed_t   unit_x,
d6081 2
d6084 1
a6084 1
    uint32_t pix1, pix2, pix3, pix4;
d6114 2
a6115 4
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix1);
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix2);
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix3);
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix4);
d6117 1
a6117 1
	if (pix1 | pix2 | pix3 | pix4)
d6119 1
a6119 1
	    __m128i xmm_src, xmm_src_lo, xmm_src_hi;
a6122 2
	    xmm_src = _mm_set_epi32 (pix4, pix3, pix2, pix1);

d6310 7
d6463 1
a6463 20
typedef struct
{
    pixman_format_code_t	format;
    pixman_iter_get_scanline_t	get_scanline;
} fetcher_info_t;

static const fetcher_info_t fetchers[] =
{
    { PIXMAN_x8r8g8b8,		sse2_fetch_x8r8g8b8 },
    { PIXMAN_r5g6b5,		sse2_fetch_r5g6b5 },
    { PIXMAN_a8,		sse2_fetch_a8 },
    { PIXMAN_null }
};

static pixman_bool_t
sse2_src_iter_init (pixman_implementation_t *imp, pixman_iter_t *iter)
{
    pixman_image_t *image = iter->image;

#define FLAGS								\
d6467 13
a6479 4
    if ((iter->iter_flags & ITER_NARROW)			&&
	(iter->image_flags & FLAGS) == FLAGS)
    {
	const fetcher_info_t *f;
d6481 1
a6481 20
	for (f = &fetchers[0]; f->format != PIXMAN_null; f++)
	{
	    if (image->common.extended_format_code == f->format)
	    {
		uint8_t *b = (uint8_t *)image->bits.bits;
		int s = image->bits.rowstride * 4;

		iter->bits = b + s * iter->y + iter->x * PIXMAN_FORMAT_BPP (f->format) / 8;
		iter->stride = s;

		iter->get_scanline = f->get_scanline;
		return TRUE;
	    }
	}
    }

    return FALSE;
}

#if defined(__GNUC__) // && !defined(__x86_64__) && !defined(__amd64__)
d6537 1
a6537 1
    imp->src_iter_init = sse2_src_iter_init;
@


1.9
log
@Update to pixman 0.28.0. Tested by ajacoutot@@, mpi@@ and naddy@@ in a full
ports build. Tweaks from mpi@@ for macppc.
@
text
@d579 1
a579 1
    while (w && ((unsigned long)pd & 15))
d664 1
a664 1
    while (w && ((unsigned long)pd & 15))
d756 1
a756 1
           ((unsigned long)pd & 15))
d843 1
a843 1
    while (w && ((unsigned long) pd & 15))
d904 1
a904 1
    while (w && ((unsigned long) pd & 15))
d960 1
a960 1
    while (w && ((unsigned long) pd & 15))
d1029 1
a1029 1
    while (w && ((unsigned long) pd & 15))
d1116 1
a1116 1
    while (w && ((unsigned long) pd & 15))
d1200 1
a1200 1
    while (w && ((unsigned long) pd & 15))
d1288 1
a1288 1
    while (w && ((unsigned long) pd & 15))
d1360 1
a1360 1
    while (w && (unsigned long)pd & 15)
d1433 1
a1433 1
    while (w && (unsigned long)pd & 15)
d1521 1
a1521 1
    while (w && (unsigned long)pd & 15)
d1589 1
a1589 1
    while (w && (unsigned long)pd & 15)
d1665 1
a1665 1
    while (w && (unsigned long)pd & 15)
d1730 1
a1730 1
    while (w && (unsigned long)pd & 15)
d1805 1
a1805 1
    while (w && (unsigned long)pd & 15)
d1878 1
a1878 1
    while (w && (unsigned long)pd & 15)
d1954 1
a1954 1
    while (w && (unsigned long)pd & 15)
d2051 1
a2051 1
    while (w && (unsigned long)pd & 15)
d2144 1
a2144 1
    while (w && (unsigned long)pd & 15)
d2240 1
a2240 1
    while (w && (unsigned long)pd & 15)
d2316 1
a2316 1
    while (w && (unsigned long)pd & 15)
d2417 1
a2417 1
	while (w && (unsigned long)dst & 15)
d2486 1
a2486 1
	while (w && (unsigned long)dst & 15)
d2571 1
a2571 1
	while (w && (unsigned long)pd & 15)
d2685 1
a2685 1
	while (w && (unsigned long)pd & 15)
d2789 1
a2789 1
	while (w && (unsigned long)dst & 15)
d2881 1
a2881 1
	while (w && (unsigned long)dst & 15)
d2884 1
a2884 1
	    *dst = CONVERT_8888_TO_0565 (s);
d2904 1
a2904 1
	    *dst = CONVERT_8888_TO_0565 (s);
d2935 1
a2935 1
	while (w && (unsigned long)dst & 15)
d3002 1
a3002 1
	while (w && (unsigned long)dst & 15)
d3128 1
a3128 1
	       ((unsigned long)dst & 15))
d3234 1
a3234 1
	while (w && (unsigned long)dst & 15)
d3324 1
a3324 1
           uint32_t		    xor)
d3341 1
a3341 1
	b = xor & 0xff;
d3343 1
a3343 1
	xor = (w << 16) | w;
d3352 1
a3352 1
        xor = (xor & 0xffff) * 0x00010001;
d3366 1
a3366 1
    xmm_def = create_mask_2x32_128 (xor, xor);
d3375 1
a3375 1
	if (w >= 1 && ((unsigned long)d & 1))
d3377 1
a3377 1
	    *(uint8_t *)d = xor;
d3382 1
a3382 1
	while (w >= 2 && ((unsigned long)d & 3))
d3384 1
a3384 1
	    *(uint16_t *)d = xor;
d3389 1
a3389 1
	while (w >= 4 && ((unsigned long)d & 15))
d3391 1
a3391 1
	    *(uint32_t *)d = xor;
d3442 1
a3442 1
	    *(uint32_t *)d = xor;
d3450 1
a3450 1
	    *(uint16_t *)d = xor;
d3457 1
a3457 1
	    *(uint8_t *)d = xor;
d3508 1
a3508 1
	while (w && (unsigned long)dst & 15)
d3624 1
a3624 1
	while (w && (unsigned long)dst & 15)
d3748 1
a3748 1
	while (w && (unsigned long)dst & 15)
d3857 1
a3857 1
	while (w && (unsigned long)dst & 15)
d3960 1
a3960 1
	while (w && ((unsigned long)dst & 15))
d4086 1
a4086 1
	while (w && ((unsigned long)dst & 15))
d4179 1
a4179 1
	while (w && ((unsigned long)dst & 15))
d4248 1
a4248 1
	while (w && ((unsigned long)dst & 15))
d4325 1
a4325 1
	while (w && ((unsigned long)dst & 15))
d4417 1
a4417 1
	while (w && ((unsigned long)dst & 15))
d4477 1
a4477 1
	while (w && (unsigned long)dst & 3)
d4526 13
d4540 143
d4741 1
a4741 1
	while (w >= 2 && ((unsigned long)d & 3))
d4749 1
a4749 1
	while (w >= 4 && ((unsigned long)d & 15))
d4856 1
a4856 1
        while (w && (unsigned long)dst & 15)
d4980 1
a4980 1
        while (w && (unsigned long)dst & 15)
d5119 1
a5119 1
	while (w && (unsigned long)dst & 15)
d5204 1
a5204 1
        while (w && (unsigned long)dst & 15)
d5332 1
a5332 1
    while (w && ((unsigned long)pd & 15))
d5450 1
a5450 1
    while (w && (unsigned long)dst & 15)
d5557 12
a5568 3
#define BMSK ((1 << BILINEAR_INTERPOLATION_BITS) - 1)

#define BILINEAR_DECLARE_VARIABLES						\
d5571 1
a5571 4
    const __m128i xmm_xorc8 = _mm_set_epi16 (0, 0, 0, 0, BMSK, BMSK, BMSK, BMSK);\
    const __m128i xmm_addc8 = _mm_set_epi16 (0, 0, 0, 0, 1, 1, 1, 1);		\
    const __m128i xmm_xorc7 = _mm_set_epi16 (0, BMSK, 0, BMSK, 0, BMSK, 0, BMSK);\
    const __m128i xmm_addc7 = _mm_set_epi16 (0, 1, 0, 1, 0, 1, 0, 1);		\
d5573 1
a5573 1
					  unit_x, unit_x, unit_x, unit_x);	\
d5575 3
a5577 1
    __m128i xmm_x = _mm_set_epi16 (vx, vx, vx, vx, vx, vx, vx, vx)
d5596 2
a5597 2
	xmm_wh = _mm_add_epi16 (xmm_addc7, _mm_xor_si128 (xmm_xorc7,		\
		   _mm_srli_epi16 (xmm_x, 16 - BILINEAR_INTERPOLATION_BITS)));	\
d5606 2
a5607 2
	xmm_wh = _mm_add_epi16 (xmm_addc8, _mm_xor_si128 (xmm_xorc8,		\
		_mm_srli_epi16 (xmm_x, 16 - BILINEAR_INTERPOLATION_BITS)));	\
d5705 1
a5705 1
    while (w && ((unsigned long)dst & 15))
d5806 1
a5806 1
    while (w && ((unsigned long)dst & 15))
d5953 115
d6130 8
d6202 5
d6225 1
a6225 1
    while (w && ((unsigned long)dst) & 0x0f)
d6261 1
a6261 1
    while (w && ((unsigned long)dst) & 0x0f)
d6265 1
a6265 1
	*dst++ = CONVERT_0565_TO_8888 (s);
d6290 1
a6290 1
	*dst++ = CONVERT_0565_TO_8888 (s);
d6307 1
a6307 1
    while (w && (((unsigned long)dst) & 15))
@


1.8
log
@Update to pixman 0.26.2. tested at least by ajacoutot@@, mpi@@, shadchin@@.
@
text
@d56 3
d127 23
d2861 51
d3312 3
d3316 9
a3324 8
pixman_fill_sse2 (uint32_t *bits,
                  int       stride,
                  int       bpp,
                  int       x,
                  int       y,
                  int       width,
                  int       height,
                  uint32_t  data)
d3327 1
a3327 1
    uint8_t         *byte_line;
d3341 1
a3341 1
	b = data & 0xff;
d3343 1
a3343 1
	data = (w << 16) | w;
d3352 1
a3352 1
        data = (data & 0xffff) * 0x00010001;
d3366 1
a3366 1
    xmm_def = create_mask_2x32_128 (data, data);
d3377 1
a3377 1
	    *(uint8_t *)d = data;
d3384 1
a3384 1
	    *(uint16_t *)d = data;
d3391 1
a3391 1
	    *(uint32_t *)d = data;
d3442 1
a3442 1
	    *(uint32_t *)d = data;
d3450 1
a3450 1
	    *(uint16_t *)d = data;
d3457 1
a3457 1
	    *(uint8_t *)d = data;
d3486 3
a3488 3
	pixman_fill_sse2 (dest_image->bits.bits, dest_image->bits.rowstride,
	                  PIXMAN_FORMAT_BPP (dest_image->bits.format),
	                  dest_x, dest_y, width, height, 0);
d4530 13
a4542 12
pixman_blt_sse2 (uint32_t *src_bits,
                 uint32_t *dst_bits,
                 int       src_stride,
                 int       dst_stride,
                 int       src_bpp,
                 int       dst_bpp,
                 int       src_x,
                 int       src_y,
                 int       dest_x,
                 int       dest_y,
                 int       width,
                 int       height)
a4647 1

d4656 7
a4662 7
    pixman_blt_sse2 (src_image->bits.bits,
                     dest_image->bits.bits,
                     src_image->bits.rowstride,
                     dest_image->bits.rowstride,
                     PIXMAN_FORMAT_BPP (src_image->bits.format),
                     PIXMAN_FORMAT_BPP (dest_image->bits.format),
                     src_x, src_y, dest_x, dest_y, width, height);
d5162 1
a5162 1
                                             pixman_fixed_t  max_vx,
d5179 1
a5179 1
	s = combine1 (ps + (vx >> 16), pm);
d5181 2
d5195 1
a5195 1
	tmp1 = ps[vx >> 16];
d5197 3
a5199 1
	tmp2 = ps[vx >> 16];
d5201 3
a5203 1
	tmp3 = ps[vx >> 16];
d5205 3
a5207 1
	tmp4 = ps[vx >> 16];
d5209 2
d5248 1
a5248 1
	s = combine1 (ps + (vx >> 16), pm);
d5250 2
d5270 3
d5281 1
a5281 1
					       pixman_fixed_t   max_vx,
d5296 1
a5296 1
	uint32_t s = src[pixman_fixed_to_int (vx)];
d5298 2
d5321 1
a5321 1
	tmp1 = src[pixman_fixed_to_int (vx)];
d5323 3
a5325 1
	tmp2 = src[pixman_fixed_to_int (vx)];
d5327 3
a5329 1
	tmp3 = src[pixman_fixed_to_int (vx)];
d5331 3
a5333 1
	tmp4 = src[pixman_fixed_to_int (vx)];
d5335 2
d5364 1
a5364 1
	uint32_t s = src[pixman_fixed_to_int (vx)];
d5366 2
d5397 5
d5406 4
a5409 2
    const __m128i xmm_xorc = _mm_set_epi16 (0, 0, 0, 0, 0xff, 0xff, 0xff, 0xff);\
    const __m128i xmm_addc = _mm_set_epi16 (0, 0, 0, 0, 1, 1, 1, 1);		\
d5418 5
a5422 6
    /* fetch 2x2 pixel block into sse2 register */				\
    uint32_t tl = src_top [pixman_fixed_to_int (vx)];				\
    uint32_t tr = src_top [pixman_fixed_to_int (vx) + 1];			\
    uint32_t bl = src_bottom [pixman_fixed_to_int (vx)];			\
    uint32_t br = src_bottom [pixman_fixed_to_int (vx) + 1];			\
    a = _mm_set_epi32 (tr, tl, br, bl);						\
d5425 1
a5425 1
    a = _mm_add_epi16 (_mm_mullo_epi16 (_mm_unpackhi_epi8 (a, xmm_zero),	\
d5427 1
a5427 1
		       _mm_mullo_epi16 (_mm_unpacklo_epi8 (a, xmm_zero),	\
d5429 22
a5450 10
    /* calculate horizontal weights */						\
    xmm_wh = _mm_add_epi16 (xmm_addc,						\
			    _mm_xor_si128 (xmm_xorc,				\
					   _mm_srli_epi16 (xmm_x, 8)));		\
    xmm_x = _mm_add_epi16 (xmm_x, xmm_ux);					\
    /* horizontal interpolation */						\
    xmm_lo = _mm_mullo_epi16 (a, xmm_wh);					\
    xmm_hi = _mm_mulhi_epu16 (a, xmm_wh);					\
    a = _mm_add_epi32 (_mm_unpacklo_epi16 (xmm_lo, xmm_hi),			\
		       _mm_unpackhi_epi16 (xmm_lo, xmm_hi));			\
d5452 1
a5452 1
    a = _mm_srli_epi32 (a, 16);							\
d5797 1
d5857 4
d5889 4
d5898 4
d5906 3
a5922 52
static pixman_bool_t
sse2_blt (pixman_implementation_t *imp,
          uint32_t *               src_bits,
          uint32_t *               dst_bits,
          int                      src_stride,
          int                      dst_stride,
          int                      src_bpp,
          int                      dst_bpp,
          int                      src_x,
          int                      src_y,
          int                      dest_x,
          int                      dest_y,
          int                      width,
          int                      height)
{
    if (!pixman_blt_sse2 (
            src_bits, dst_bits, src_stride, dst_stride, src_bpp, dst_bpp,
            src_x, src_y, dest_x, dest_y, width, height))

    {
	return _pixman_implementation_blt (
	    imp->delegate,
	    src_bits, dst_bits, src_stride, dst_stride, src_bpp, dst_bpp,
	    src_x, src_y, dest_x, dest_y, width, height);
    }

    return TRUE;
}

#if defined(__GNUC__) && !defined(__x86_64__) && !defined(__amd64__)
__attribute__((__force_align_arg_pointer__))
#endif
static pixman_bool_t
sse2_fill (pixman_implementation_t *imp,
           uint32_t *               bits,
           int                      stride,
           int                      bpp,
           int                      x,
           int                      y,
           int                      width,
           int                      height,
           uint32_t xor)
{
    if (!pixman_fill_sse2 (bits, stride, bpp, x, y, width, height, xor))
    {
	return _pixman_implementation_fill (
	    imp->delegate, bits, stride, bpp, x, y, width, height, xor);
    }

    return TRUE;
}

d6065 1
a6065 1
static void
a6068 4
    int x = iter->x;
    int y = iter->y;
    int width = iter->width;
    int height = iter->height;
d6071 2
a6072 1
    (FAST_PATH_STANDARD_FLAGS | FAST_PATH_ID_TRANSFORM | FAST_PATH_BITS_IMAGE)
d6074 2
a6075 5
    if ((iter->flags & ITER_NARROW)				&&
	(image->common.flags & FLAGS) == FLAGS			&&
	x >= 0 && y >= 0					&&
	x + width <= image->bits.width				&&
	y + height <= image->bits.height)
d6086 1
a6086 1
		iter->bits = b + s * iter->y + x * PIXMAN_FORMAT_BPP (f->format) / 8;
d6090 1
a6090 1
		return;
d6095 1
a6095 1
    imp->delegate->src_iter_init (imp->delegate, iter);
d6122 2
@


1.7
log
@Update to pixman 0.22.4. Tested by shadchin@@, krw@@.
@
text
@d3294 1
a3294 1
	while (w >= 1 && ((unsigned long)d & 1))
@


1.6
log
@Update to pixman 0.22.2.
0.22.0 was tested by many. 0.22.2 only add a few bug fixes.
Note that on amd64 a recent ld.so is needed to avoid random bus errors.
@
text
@d37 1
a37 1
#include "pixman-fast-path.h"
d2363 1
a2363 12
                            pixman_op_t              op,
                            pixman_image_t *         src_image,
                            pixman_image_t *         mask_image,
                            pixman_image_t *         dst_image,
                            int32_t                  src_x,
                            int32_t                  src_y,
                            int32_t                  mask_x,
                            int32_t                  mask_y,
                            int32_t                  dest_x,
                            int32_t                  dest_y,
                            int32_t                  width,
                            int32_t                  height)
d2365 1
d2373 1
a2373 1
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);
d2379 1
a2379 1
	dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d2432 1
a2432 12
                            pixman_op_t              op,
                            pixman_image_t *         src_image,
                            pixman_image_t *         mask_image,
                            pixman_image_t *         dst_image,
                            int32_t                  src_x,
                            int32_t                  src_y,
                            int32_t                  mask_x,
                            int32_t                  mask_y,
                            int32_t                  dest_x,
                            int32_t                  dest_y,
                            int32_t                  width,
                            int32_t                  height)
d2434 1
d2442 1
a2442 1
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);
d2448 1
a2448 1
	dst_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);
d2507 1
a2507 12
				   pixman_op_t              op,
				   pixman_image_t *         src_image,
				   pixman_image_t *         mask_image,
				   pixman_image_t *         dst_image,
				   int32_t                  src_x,
				   int32_t                  src_y,
				   int32_t                  mask_x,
				   int32_t                  mask_y,
				   int32_t                  dest_x,
				   int32_t                  dest_y,
				   int32_t                  width,
				   int32_t                  height)
d2509 2
a2510 1
    uint32_t src, srca;
d2516 1
a2516 1
    __m128i xmm_src, xmm_alpha;
d2520 1
a2520 1
    __m128i mmx_src, mmx_alpha, mmx_mask, mmx_dest;
d2522 1
a2522 2
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);
    srca = src >> 24;
d2528 1
a2528 1
	dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
a2533 1
    xmm_alpha = expand_alpha_1x128 (xmm_src);
a2534 1
    mmx_alpha = xmm_alpha;
d2619 1
a2619 12
                                    pixman_op_t              op,
                                    pixman_image_t *         src_image,
                                    pixman_image_t *         mask_image,
                                    pixman_image_t *         dst_image,
                                    int32_t                  src_x,
                                    int32_t                  src_y,
                                    int32_t                  mask_x,
                                    int32_t                  mask_y,
                                    int32_t                  dest_x,
                                    int32_t                  dest_y,
                                    int32_t                  width,
                                    int32_t                  height)
d2621 1
d2634 1
a2634 1
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);
d2640 1
a2640 1
	dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d2732 1
a2732 12
                                 pixman_op_t              op,
                                 pixman_image_t *         src_image,
                                 pixman_image_t *         mask_image,
                                 pixman_image_t *         dst_image,
                                 int32_t                  src_x,
                                 int32_t                  src_y,
                                 int32_t                  mask_x,
                                 int32_t                  mask_y,
                                 int32_t                  dest_x,
                                 int32_t                  dest_y,
                                 int32_t                  width,
                                 int32_t                  height)
d2734 1
d2747 1
a2747 1
	dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d2836 1
a2836 12
			      pixman_op_t              op,
			      pixman_image_t *         src_image,
			      pixman_image_t *         mask_image,
			      pixman_image_t *         dst_image,
			      int32_t                  src_x,
			      int32_t                  src_y,
			      int32_t                  mask_x,
			      int32_t                  mask_y,
			      int32_t                  dest_x,
			      int32_t                  dest_y,
			      int32_t                  width,
			      int32_t                  height)
d2838 1
d2846 1
a2846 1
	dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d2894 1
a2894 12
                                 pixman_op_t              op,
                                 pixman_image_t *         src_image,
                                 pixman_image_t *         mask_image,
                                 pixman_image_t *         dst_image,
                                 int32_t                  src_x,
                                 int32_t                  src_y,
                                 int32_t                  mask_x,
                                 int32_t                  mask_y,
                                 int32_t                  dest_x,
                                 int32_t                  dest_y,
                                 int32_t                  width,
                                 int32_t                  height)
d2896 1
d2908 1
a2908 1
	dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d2985 1
a2985 12
                               pixman_op_t              op,
                               pixman_image_t *         src_image,
                               pixman_image_t *         mask_image,
                               pixman_image_t *         dst_image,
                               int32_t                  src_x,
                               int32_t                  src_y,
                               int32_t                  mask_x,
                               int32_t                  mask_y,
                               int32_t                  dest_x,
                               int32_t                  dest_y,
                               int32_t                  width,
                               int32_t                  height)
d2987 1
d2993 1
a2993 1
	dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d3023 1
a3023 12
                               pixman_op_t              op,
                               pixman_image_t *         src_image,
                               pixman_image_t *         mask_image,
                               pixman_image_t *         dst_image,
                               int32_t                  src_x,
                               int32_t                  src_y,
                               int32_t                  mask_x,
                               int32_t                  mask_y,
                               int32_t                  dest_x,
                               int32_t                  dest_y,
                               int32_t                  width,
                               int32_t                  height)
d3025 1
d3036 1
a3036 1
	dst_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);
d3116 1
a3116 12
                              pixman_op_t              op,
                              pixman_image_t *         src_image,
                              pixman_image_t *         mask_image,
                              pixman_image_t *         dst_image,
                              int32_t                  src_x,
                              int32_t                  src_y,
                              int32_t                  mask_x,
                              int32_t                  mask_y,
                              int32_t                  dest_x,
                              int32_t                  dest_y,
                              int32_t                  width,
                              int32_t                  height)
d3118 1
d3132 1
a3132 1
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);
d3139 1
a3139 1
	dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d3387 1
a3387 12
                             pixman_op_t              op,
                             pixman_image_t *         src_image,
                             pixman_image_t *         mask_image,
                             pixman_image_t *         dst_image,
                             int32_t                  src_x,
                             int32_t                  src_y,
                             int32_t                  mask_x,
                             int32_t                  mask_y,
                             int32_t                  dest_x,
                             int32_t                  dest_y,
                             int32_t                  width,
                             int32_t                  height)
d3389 1
d3400 1
a3400 1
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);
d3405 2
a3406 2
	pixman_fill_sse2 (dst_image->bits.bits, dst_image->bits.rowstride,
	                  PIXMAN_FORMAT_BPP (dst_image->bits.format),
d3412 1
a3412 1
	dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d3505 1
a3505 12
                              pixman_op_t              op,
                              pixman_image_t *         src_image,
                              pixman_image_t *         mask_image,
                              pixman_image_t *         dst_image,
                              int32_t                  src_x,
                              int32_t                  src_y,
                              int32_t                  mask_x,
                              int32_t                  mask_y,
                              int32_t                  dest_x,
                              int32_t                  dest_y,
                              int32_t                  width,
                              int32_t                  height)
d3507 2
a3508 1
    uint32_t src, srca;
d3520 1
a3520 1
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);
a3521 1
    srca = src >> 24;
d3526 1
a3526 1
	dst_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);
d3641 1
a3641 12
                                 pixman_op_t              op,
                                 pixman_image_t *         src_image,
                                 pixman_image_t *         mask_image,
                                 pixman_image_t *         dst_image,
                                 int32_t                  src_x,
                                 int32_t                  src_y,
                                 int32_t                  mask_x,
                                 int32_t                  mask_y,
                                 int32_t                  dest_x,
                                 int32_t                  dest_y,
                                 int32_t                  width,
                                 int32_t                  height)
d3643 1
d3655 1
a3655 1
	dst_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);
d3751 1
a3751 12
                                 pixman_op_t              op,
                                 pixman_image_t *         src_image,
                                 pixman_image_t *         mask_image,
                                 pixman_image_t *         dst_image,
                                 int32_t                  src_x,
                                 int32_t                  src_y,
                                 int32_t                  mask_x,
                                 int32_t                  mask_y,
                                 int32_t                  dest_x,
                                 int32_t                  dest_y,
                                 int32_t                  width,
                                 int32_t                  height)
d3753 1
d3764 1
a3764 1
	dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d3840 1
a3840 12
                                    pixman_op_t              op,
                                    pixman_image_t *         src_image,
                                    pixman_image_t *         mask_image,
                                    pixman_image_t *         dst_image,
                                    int32_t                  src_x,
                                    int32_t                  src_y,
                                    int32_t                  mask_x,
                                    int32_t                  mask_y,
                                    int32_t                  dest_x,
                                    int32_t                  dest_y,
                                    int32_t                  width,
                                    int32_t                  height)
d3842 1
d3856 1
a3856 1
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);
d3862 1
a3862 1
	dst_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);
d3974 1
a3974 12
                         pixman_op_t              op,
                         pixman_image_t *         src_image,
                         pixman_image_t *         mask_image,
                         pixman_image_t *         dst_image,
                         int32_t                  src_x,
                         int32_t                  src_y,
                         int32_t                  mask_x,
                         int32_t                  mask_y,
                         int32_t                  dest_x,
                         int32_t                  dest_y,
                         int32_t                  width,
                         int32_t                  height)
d3976 1
a3981 1
    uint8_t sa;
d3989 1
a3989 1
	dst_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);
d3993 1
a3993 3
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);

    sa = src >> 24;
d4060 1
a4060 12
		       pixman_op_t              op,
		       pixman_image_t *         src_image,
		       pixman_image_t *         mask_image,
		       pixman_image_t *         dst_image,
		       int32_t                  src_x,
		       int32_t                  src_y,
		       int32_t                  mask_x,
		       int32_t                  mask_y,
		       int32_t                  dest_x,
		       int32_t                  dest_y,
		       int32_t                  width,
		       int32_t                  height)
d4062 1
d4073 1
a4073 1
	dst_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);
d4075 1
a4075 1
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);
d4086 1
a4086 1
	pixman_fill (dst_image->bits.bits, dst_image->bits.rowstride,
d4142 1
a4142 12
                       pixman_op_t              op,
                       pixman_image_t *         src_image,
                       pixman_image_t *         mask_image,
                       pixman_image_t *         dst_image,
                       int32_t                  src_x,
                       int32_t                  src_y,
                       int32_t                  mask_x,
                       int32_t                  mask_y,
                       int32_t                  dest_x,
                       int32_t                  dest_y,
                       int32_t                  width,
                       int32_t                  height)
d4144 1
d4155 1
a4155 1
	dst_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);
d4213 1
a4213 12
			  pixman_op_t              op,
			  pixman_image_t *         src_image,
			  pixman_image_t *         mask_image,
			  pixman_image_t *         dst_image,
			  int32_t                  src_x,
			  int32_t                  src_y,
			  int32_t                  mask_x,
			  int32_t                  mask_y,
			  int32_t                  dest_x,
			  int32_t                  dest_y,
			  int32_t                  width,
			  int32_t                  height)
d4215 1
a4220 1
    uint8_t sa;
d4228 1
a4228 1
	dst_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);
d4232 1
a4232 3
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);

    sa = src >> 24;
d4299 1
a4299 12
			pixman_op_t              op,
			pixman_image_t *         src_image,
			pixman_image_t *         mask_image,
			pixman_image_t *         dst_image,
			int32_t                  src_x,
			int32_t                  src_y,
			int32_t                  mask_x,
			int32_t                  mask_y,
			int32_t                  dest_x,
			int32_t                  dest_y,
			int32_t                  width,
			int32_t                  height)
d4301 1
d4310 1
a4310 1
	dst_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);
d4312 1
a4312 1
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);
d4321 1
a4321 1
	pixman_fill (dst_image->bits.bits, dst_image->bits.rowstride,
d4372 1
a4372 12
			pixman_op_t              op,
			pixman_image_t *         src_image,
			pixman_image_t *         mask_image,
			pixman_image_t *         dst_image,
			int32_t                  src_x,
			int32_t                  src_y,
			int32_t                  mask_x,
			int32_t                  mask_y,
			int32_t                  dest_x,
			int32_t                  dest_y,
			int32_t                  width,
			int32_t                  height)
d4374 1
d4384 1
a4384 1
	dst_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);
d4424 1
a4424 12
                              pixman_op_t              op,
                              pixman_image_t *         src_image,
                              pixman_image_t *         mask_image,
                              pixman_image_t *         dst_image,
                              int32_t                  src_x,
                              int32_t                  src_y,
                              int32_t                  mask_x,
                              int32_t                  mask_y,
                              int32_t                  dest_x,
                              int32_t                  dest_y,
                              int32_t                  width,
                              int32_t                  height)
d4426 1
d4434 1
a4434 1
	dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d4457 2
a4458 2
                 int       dst_x,
                 int       dst_y,
d4474 1
a4474 1
	dst_bytes = (uint8_t *)(((uint16_t *)dst_bits) + dst_stride * (dst_y) + (dst_x));
d4484 1
a4484 1
	dst_bytes = (uint8_t *)(((uint32_t *)dst_bits) + dst_stride * (dst_y) + (dst_x));
d4572 1
a4572 12
                          pixman_op_t              op,
                          pixman_image_t *         src_image,
                          pixman_image_t *         mask_image,
                          pixman_image_t *         dst_image,
                          int32_t                  src_x,
                          int32_t                  src_y,
                          int32_t                  mask_x,
                          int32_t                  mask_y,
                          int32_t                  dest_x,
                          int32_t                  dest_y,
                          int32_t                  width,
                          int32_t                  height)
d4574 1
d4576 1
a4576 1
                     dst_image->bits.bits,
d4578 1
a4578 1
                     dst_image->bits.rowstride,
d4580 1
a4580 1
                     PIXMAN_FORMAT_BPP (dst_image->bits.format),
d4586 1
a4586 12
                                 pixman_op_t              op,
                                 pixman_image_t *         src_image,
                                 pixman_image_t *         mask_image,
                                 pixman_image_t *         dst_image,
                                 int32_t                  src_x,
                                 int32_t                  src_y,
                                 int32_t                  mask_x,
                                 int32_t                  mask_y,
                                 int32_t                  dest_x,
                                 int32_t                  dest_y,
                                 int32_t                  width,
                                 int32_t                  height)
d4588 1
d4602 1
a4602 1
	dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d4711 1
a4711 12
                                 pixman_op_t              op,
                                 pixman_image_t *         src_image,
                                 pixman_image_t *         mask_image,
                                 pixman_image_t *         dst_image,
                                 int32_t                  src_x,
                                 int32_t                  src_y,
                                 int32_t                  mask_x,
                                 int32_t                  mask_y,
                                 int32_t                  dest_x,
                                 int32_t                  dest_y,
                                 int32_t                  width,
                                 int32_t                  height)
d4713 1
d4726 1
a4726 1
	dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d4854 1
a4854 12
				    pixman_op_t              op,
				    pixman_image_t *         src_image,
				    pixman_image_t *         mask_image,
				    pixman_image_t *         dst_image,
				    int32_t                  src_x,
				    int32_t                  src_y,
				    int32_t                  mask_x,
				    int32_t                  mask_y,
				    int32_t                  dest_x,
				    int32_t                  dest_y,
				    int32_t                  width,
				    int32_t                  height)
d4856 1
d4865 1
a4865 1
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);
d4871 1
a4871 1
	dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d4935 1
a4935 12
				    pixman_op_t              op,
				    pixman_image_t *         src_image,
				    pixman_image_t *         mask_image,
				    pixman_image_t *         dst_image,
				    int32_t                  src_x,
				    int32_t                  src_y,
				    int32_t                  mask_x,
				    int32_t                  mask_y,
				    int32_t                  dest_x,
				    int32_t                  dest_y,
				    int32_t                  width,
				    int32_t                  height)
d4937 1
d4950 1
a4950 1
	dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d5290 47
a5336 77
static void
bilinear_interpolate_line_sse2 (uint32_t *       out,
                                const uint32_t * top,
                                const uint32_t * bottom,
                                int              wt,
                                int              wb,
                                pixman_fixed_t   x,
                                pixman_fixed_t   ux,
                                int              width)
{
    const __m128i xmm_wt = _mm_set_epi16 (wt, wt, wt, wt, wt, wt, wt, wt);
    const __m128i xmm_wb = _mm_set_epi16 (wb, wb, wb, wb, wb, wb, wb, wb);
    const __m128i xmm_xorc = _mm_set_epi16 (0, 0, 0, 0, 0xff, 0xff, 0xff, 0xff);
    const __m128i xmm_addc = _mm_set_epi16 (0, 0, 0, 0, 1, 1, 1, 1);
    const __m128i xmm_ux = _mm_set_epi16 (ux, ux, ux, ux, ux, ux, ux, ux);
    const __m128i xmm_zero = _mm_setzero_si128 ();
    __m128i xmm_x = _mm_set_epi16 (x, x, x, x, x, x, x, x);
    uint32_t pix1, pix2, pix3, pix4;

    #define INTERPOLATE_ONE_PIXEL(pix)						\
    do {									\
	__m128i xmm_wh, xmm_lo, xmm_hi, a;					\
	/* fetch 2x2 pixel block into sse2 register */				\
	uint32_t tl = top [pixman_fixed_to_int (x)];				\
	uint32_t tr = top [pixman_fixed_to_int (x) + 1];			\
	uint32_t bl = bottom [pixman_fixed_to_int (x)];				\
	uint32_t br = bottom [pixman_fixed_to_int (x) + 1];			\
	a = _mm_set_epi32 (tr, tl, br, bl);					\
        x += ux;								\
	/* vertical interpolation */						\
	a = _mm_add_epi16 (_mm_mullo_epi16 (_mm_unpackhi_epi8 (a, xmm_zero),	\
					    xmm_wt),				\
			   _mm_mullo_epi16 (_mm_unpacklo_epi8 (a, xmm_zero),	\
					    xmm_wb));				\
	/* calculate horizontal weights */					\
	xmm_wh = _mm_add_epi16 (xmm_addc,					\
				_mm_xor_si128 (xmm_xorc,			\
					       _mm_srli_epi16 (xmm_x, 8)));	\
	xmm_x = _mm_add_epi16 (xmm_x, xmm_ux);					\
	/* horizontal interpolation */						\
	xmm_lo = _mm_mullo_epi16 (a, xmm_wh);					\
	xmm_hi = _mm_mulhi_epu16 (a, xmm_wh);					\
	a = _mm_add_epi32 (_mm_unpacklo_epi16 (xmm_lo, xmm_hi),			\
			   _mm_unpackhi_epi16 (xmm_lo, xmm_hi));		\
	/* shift and pack the result */						\
	a = _mm_srli_epi32 (a, 16);						\
	a = _mm_packs_epi32 (a, a);						\
	a = _mm_packus_epi16 (a, a);						\
	pix = _mm_cvtsi128_si32 (a);						\
    } while (0)

    while ((width -= 4) >= 0)
    {
	INTERPOLATE_ONE_PIXEL (pix1);
	INTERPOLATE_ONE_PIXEL (pix2);
	INTERPOLATE_ONE_PIXEL (pix3);
	INTERPOLATE_ONE_PIXEL (pix4);
	*out++ = pix1;
	*out++ = pix2;
	*out++ = pix3;
	*out++ = pix4;
    }
    if (width & 2)
    {
	INTERPOLATE_ONE_PIXEL (pix1);
	INTERPOLATE_ONE_PIXEL (pix2);
	*out++ = pix1;
	*out++ = pix2;
    }
    if (width & 1)
    {
	INTERPOLATE_ONE_PIXEL (pix1);
	*out = pix1;
    }

    #undef INTERPOLATE_ONE_PIXEL
}
d5351 29
a5379 2
    bilinear_interpolate_line_sse2 (dst, src_top, src_bottom,
				    wt, wb, vx, unit_x, w);
d5385 1
a5385 1
			       COVER, FALSE, FALSE)
d5389 1
a5389 1
			       PAD, FALSE, FALSE)
d5393 269
a5661 1
			       NONE, FALSE, FALSE)
d5768 10
d5791 2
a5792 2
          int                      dst_x,
          int                      dst_y,
d5798 1
a5798 1
            src_x, src_y, dst_x, dst_y, width, height))
d5804 1
a5804 1
	    src_x, src_y, dst_x, dst_y, width, height);
d5985 1
a5985 1
    (FAST_PATH_STANDARD_FLAGS | FAST_PATH_ID_TRANSFORM)
@


1.5
log
@Update to pixman 0.20.0.
tested by ajacoutot@@, krw@@ and on a bulk ports build by landry@@.
@
text
@a32 1
#include <mmintrin.h>
a38 23
#if defined(_MSC_VER) && defined(_M_AMD64)
/* Windows 64 doesn't allow MMX to be used, so
 * the pixman-x64-mmx-emulation.h file contains
 * implementations of those MMX intrinsics that
 * are used in the SSE2 implementation.
 */
#   include "pixman-x64-mmx-emulation.h"
#endif

#ifdef USE_SSE2

/* --------------------------------------------------------------------
 * Locals
 */

static __m64 mask_x0080;
static __m64 mask_x00ff;
static __m64 mask_x0101;
static __m64 mask_x_alpha;

static __m64 mask_x565_rgb;
static __m64 mask_x565_unpack;

a55 3
/* ----------------------------------------------------------------------
 * SSE2 Inlines
 */
d373 2
a374 6
/* ------------------------------------------------------------------
 * MMX inlines
 */

static force_inline __m64
load_32_1x64 (uint32_t data)
d376 1
a376 1
    return _mm_cvtsi32_si64 (data);
d379 2
a380 2
static force_inline __m64
unpack_32_1x64 (uint32_t data)
d382 1
a382 1
    return _mm_unpacklo_pi8 (load_32_1x64 (data), _mm_setzero_si64 ());
d385 2
a386 2
static force_inline __m64
expand_alpha_1x64 (__m64 data)
d388 2
a389 1
    return _mm_shuffle_pi16 (data, _MM_SHUFFLE (3, 3, 3, 3));
d392 3
a394 2
static force_inline __m64
expand_alpha_rev_1x64 (__m64 data)
d396 3
a398 1
    return _mm_shuffle_pi16 (data, _MM_SHUFFLE (0, 0, 0, 0));
d401 5
a405 2
static force_inline __m64
expand_pixel_8_1x64 (uint8_t data)
d407 2
a408 3
    return _mm_shuffle_pi16 (
	unpack_32_1x64 ((uint32_t)data), _MM_SHUFFLE (0, 0, 0, 0));
}
d410 1
a410 7
static force_inline __m64
pix_multiply_1x64 (__m64 data,
                   __m64 alpha)
{
    return _mm_mulhi_pu16 (_mm_adds_pu16 (_mm_mullo_pi16 (data, alpha),
                                          mask_x0080),
                           mask_x0101);
d413 2
a414 5
static force_inline __m64
pix_add_multiply_1x64 (__m64* src,
                       __m64* alpha_dst,
                       __m64* dst,
                       __m64* alpha_src)
d416 1
a416 4
    __m64 t1 = pix_multiply_1x64 (*src, *alpha_dst);
    __m64 t2 = pix_multiply_1x64 (*dst, *alpha_src);

    return _mm_adds_pu8 (t1, t2);
d419 2
a420 2
static force_inline __m64
negate_1x64 (__m64 data)
d422 1
a422 1
    return _mm_xor_si64 (data, mask_x00ff);
d425 2
a426 2
static force_inline __m64
invert_colors_1x64 (__m64 data)
d428 1
a428 1
    return _mm_shuffle_pi16 (data, _MM_SHUFFLE (3, 0, 1, 2));
d431 2
a432 2
static force_inline __m64
over_1x64 (__m64 src, __m64 alpha, __m64 dst)
d434 3
a436 1
    return _mm_adds_pu8 (src, pix_multiply_1x64 (dst, negate_1x64 (alpha)));
d439 2
a440 10
static force_inline __m64
in_over_1x64 (__m64* src, __m64* alpha, __m64* mask, __m64* dst)
{
    return over_1x64 (pix_multiply_1x64 (*src, *mask),
                      pix_multiply_1x64 (*alpha, *mask),
                      *dst);
}

static force_inline __m64
over_rev_non_pre_1x64 (__m64 src, __m64 dst)
d442 1
a442 1
    __m64 alpha = expand_alpha_1x64 (src);
d444 4
a447 4
    return over_1x64 (pix_multiply_1x64 (invert_colors_1x64 (src),
                                         _mm_or_si64 (alpha, mask_x_alpha)),
                      alpha,
                      dst);
d451 1
a451 1
pack_1x64_32 (__m64 data)
d453 1
a453 1
    return _mm_cvtsi64_si32 (_mm_packs_pu16 (data, _mm_setzero_si64 ()));
d456 2
a457 16
/* Expand 16 bits positioned at @@pos (0-3) of a mmx register into
 *
 *    00RR00GG00BB
 *
 * --- Expanding 565 in the low word ---
 *
 * m = (m << (32 - 3)) | (m << (16 - 5)) | m;
 * m = m & (01f0003f001f);
 * m = m * (008404100840);
 * m = m >> 8;
 *
 * Note the trick here - the top word is shifted by another nibble to
 * avoid it bumping into the middle word
 */
static force_inline __m64
expand565_16_1x64 (uint16_t pixel)
d459 1
a459 2
    __m64 p;
    __m64 t1, t2;
d461 1
a461 1
    p = _mm_cvtsi32_si64 ((uint32_t) pixel);
d463 1
a463 9
    t1 = _mm_slli_si64 (p, 36 - 11);
    t2 = _mm_slli_si64 (p, 16 - 5);

    p = _mm_or_si64 (t1, p);
    p = _mm_or_si64 (t2, p);
    p = _mm_and_si64 (p, mask_x565_rgb);
    p = _mm_mullo_pi16 (p, mask_x565_unpack);

    return _mm_srli_pi16 (p, 8);
a465 3
/* ----------------------------------------------------------------------------
 * Compose Core transformations
 */
d470 1
a470 1
    __m64 ms;
d480 4
a483 3
	ms = unpack_32_1x64 (src);
	return pack_1x64_32 (
	    over_1x64 (ms, expand_alpha_1x64 (ms), unpack_32_1x64 (dst)));
d496 1
a496 1
	__m64 ms, mm;
d498 2
a499 2
	mm = unpack_32_1x64 (*pm);
	mm = expand_alpha_1x64 (mm);
d501 2
a502 2
	ms = unpack_32_1x64 (s);
	ms = pix_multiply_1x64 (ms, mm);
d504 1
a504 1
	s = pack_1x64_32 (ms);
d545 4
a548 4
core_combine_over_u_sse2 (uint32_t*       pd,
                          const uint32_t* ps,
                          const uint32_t* pm,
                          int             w)
a551 4
    __m128i xmm_dst_lo, xmm_dst_hi;
    __m128i xmm_src_lo, xmm_src_hi;
    __m128i xmm_alpha_lo, xmm_alpha_hi;

d558 3
a560 1
	*pd++ = core_combine_over_u_pixel_sse2 (s, d);
d562 1
a562 2
	if (pm)
	    pm++;
d568 1
a568 4
	/* I'm loading unaligned because I'm not sure about
	 * the address alignment.
	 */
	xmm_src_hi = combine4 ((__m128i*)ps, (__m128i*)pm);
d570 1
a570 1
	if (is_opaque (xmm_src_hi))
d572 36
a607 1
	    save_128_aligned ((__m128i*)pd, xmm_src_hi);
d609 50
a658 1
	else if (!is_zero (xmm_src_hi))
d660 7
a666 1
	    xmm_dst_hi = load_128_aligned ((__m128i*) pd);
d668 2
a669 2
	    unpack_128_2x128 (xmm_src_hi, &xmm_src_lo, &xmm_src_hi);
	    unpack_128_2x128 (xmm_dst_hi, &xmm_dst_lo, &xmm_dst_hi);
d671 4
a674 2
	    expand_alpha_2x128 (
		xmm_src_lo, xmm_src_hi, &xmm_alpha_lo, &xmm_alpha_hi);
d676 4
a679 7
	    over_2x128 (&xmm_src_lo, &xmm_src_hi,
			&xmm_alpha_lo, &xmm_alpha_hi,
			&xmm_dst_lo, &xmm_dst_hi);

	    /* rebuid the 4 pixel data and save*/
	    save_128_aligned ((__m128i*)pd,
			      pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));
a681 1
	w -= 4;
d684 1
a684 2
	if (pm)
	    pm += 4;
a685 1

d689 1
a689 1
	s = combine1 (ps, pm);
d691 3
a693 1
	*pd++ = core_combine_over_u_pixel_sse2 (s, d);
a694 2
	if (pm)
	    pm++;
d701 20
a720 4
core_combine_over_reverse_u_sse2 (uint32_t*       pd,
                                  const uint32_t* ps,
                                  const uint32_t* pm,
                                  int             w)
d786 1
a786 1
core_combine_in_u_pixelsse2 (uint32_t src, uint32_t dst)
d796 3
a798 3
	return pack_1x64_32 (
	    pix_multiply_1x64 (unpack_32_1x64 (dst),
			       expand_alpha_1x64 (unpack_32_1x64 (src))));
d804 7
a810 5
static force_inline void
core_combine_in_u_sse2 (uint32_t*       pd,
                        const uint32_t* ps,
                        const uint32_t* pm,
                        int             w)
d822 1
a822 1
	*pd++ = core_combine_in_u_pixelsse2 (d, s);
d857 1
a857 1
	*pd++ = core_combine_in_u_pixelsse2 (d, s);
d865 7
a871 5
static force_inline void
core_combine_reverse_in_u_sse2 (uint32_t*       pd,
                                const uint32_t* ps,
                                const uint32_t *pm,
                                int             w)
d883 1
a883 1
	*pd++ = core_combine_in_u_pixelsse2 (s, d);
d918 1
a918 1
	*pd++ = core_combine_in_u_pixelsse2 (s, d);
d926 7
a932 5
static force_inline void
core_combine_reverse_out_u_sse2 (uint32_t*       pd,
                                 const uint32_t* ps,
                                 const uint32_t* pm,
                                 int             w)
d939 4
a942 4
	*pd++ = pack_1x64_32 (
	    pix_multiply_1x64 (
		unpack_32_1x64 (d), negate_1x64 (
		    expand_alpha_1x64 (unpack_32_1x64 (s)))));
d984 4
a987 4
	*pd++ = pack_1x64_32 (
	    pix_multiply_1x64 (
		unpack_32_1x64 (d), negate_1x64 (
		    expand_alpha_1x64 (unpack_32_1x64 (s)))));
d995 7
a1001 5
static force_inline void
core_combine_out_u_sse2 (uint32_t*       pd,
                         const uint32_t* ps,
                         const uint32_t* pm,
                         int             w)
d1008 4
a1011 4
	*pd++ = pack_1x64_32 (
	    pix_multiply_1x64 (
		unpack_32_1x64 (s), negate_1x64 (
		    expand_alpha_1x64 (unpack_32_1x64 (d)))));
d1051 4
a1054 4
	*pd++ = pack_1x64_32 (
	    pix_multiply_1x64 (
		unpack_32_1x64 (s), negate_1x64 (
		    expand_alpha_1x64 (unpack_32_1x64 (d)))));
d1066 2
a1067 2
    __m64 s = unpack_32_1x64 (src);
    __m64 d = unpack_32_1x64 (dst);
d1069 2
a1070 2
    __m64 sa = negate_1x64 (expand_alpha_1x64 (s));
    __m64 da = expand_alpha_1x64 (d);
d1072 1
a1072 1
    return pack_1x64_32 (pix_add_multiply_1x64 (&s, &da, &d, &sa));
d1075 7
a1081 5
static force_inline void
core_combine_atop_u_sse2 (uint32_t*       pd,
                          const uint32_t* ps,
                          const uint32_t* pm,
                          int             w)
d1150 2
a1151 2
    __m64 s = unpack_32_1x64 (src);
    __m64 d = unpack_32_1x64 (dst);
d1153 2
a1154 2
    __m64 sa = expand_alpha_1x64 (s);
    __m64 da = negate_1x64 (expand_alpha_1x64 (d));
d1156 1
a1156 1
    return pack_1x64_32 (pix_add_multiply_1x64 (&s, &da, &d, &sa));
d1159 7
a1165 5
static force_inline void
core_combine_reverse_atop_u_sse2 (uint32_t*       pd,
                                  const uint32_t* ps,
                                  const uint32_t* pm,
                                  int             w)
d1234 2
a1235 2
    __m64 s = unpack_32_1x64 (src);
    __m64 d = unpack_32_1x64 (dst);
d1237 2
a1238 2
    __m64 neg_d = negate_1x64 (expand_alpha_1x64 (d));
    __m64 neg_s = negate_1x64 (expand_alpha_1x64 (s));
d1240 1
a1240 1
    return pack_1x64_32 (pix_add_multiply_1x64 (&s, &neg_d, &d, &neg_s));
d1243 7
a1249 5
static force_inline void
core_combine_xor_u_sse2 (uint32_t*       dst,
                         const uint32_t* src,
                         const uint32_t *mask,
                         int             width)
d1321 6
a1326 4
core_combine_add_u_sse2 (uint32_t*       dst,
                         const uint32_t* src,
                         const uint32_t* mask,
                         int             width)
d1342 2
a1343 2
	*pd++ = _mm_cvtsi64_si32 (
	    _mm_adds_pu8 (_mm_cvtsi32_si64 (s), _mm_cvtsi32_si64 (d)));
d1369 2
a1370 2
	*pd++ = _mm_cvtsi64_si32 (
	    _mm_adds_pu8 (_mm_cvtsi32_si64 (s), _mm_cvtsi32_si64 (d)));
d1380 2
a1381 2
    __m64 ms = unpack_32_1x64 (src);
    __m64 md = unpack_32_1x64 (dst);
d1387 2
a1388 2
	ms = pix_multiply_1x64 (
	    ms, expand_alpha_1x64 (unpack_32_1x64 (DIV_UN8 (da, sa) << 24)));
d1391 1
a1391 1
    return pack_1x64_32 (_mm_adds_pu16 (md, ms));
d1394 7
a1400 5
static force_inline void
core_combine_saturate_u_sse2 (uint32_t *      pd,
                              const uint32_t *ps,
                              const uint32_t *pm,
                              int             w)
d1481 7
a1487 5
static force_inline void
core_combine_src_ca_sse2 (uint32_t*       pd,
                          const uint32_t* ps,
                          const uint32_t *pm,
                          int             w)
d1499 2
a1500 2
	*pd++ = pack_1x64_32 (
	    pix_multiply_1x64 (unpack_32_1x64 (s), unpack_32_1x64 (m)));
d1529 2
a1530 2
	*pd++ = pack_1x64_32 (
	    pix_multiply_1x64 (unpack_32_1x64 (s), unpack_32_1x64 (m)));
d1540 4
a1543 4
    __m64 s = unpack_32_1x64 (src);
    __m64 expAlpha = expand_alpha_1x64 (s);
    __m64 unpk_mask = unpack_32_1x64 (mask);
    __m64 unpk_dst  = unpack_32_1x64 (dst);
d1545 1
a1545 1
    return pack_1x64_32 (in_over_1x64 (&s, &expAlpha, &unpk_mask, &unpk_dst));
d1548 7
a1554 5
static force_inline void
core_combine_over_ca_sse2 (uint32_t*       pd,
                           const uint32_t* ps,
                           const uint32_t *pm,
                           int             w)
d1616 1
a1616 1
    __m64 d = unpack_32_1x64 (dst);
d1618 4
a1621 4
    return pack_1x64_32 (
	over_1x64 (d, expand_alpha_1x64 (d),
		   pix_multiply_1x64 (unpack_32_1x64 (src),
				      unpack_32_1x64 (mask))));
d1624 7
a1630 5
static force_inline void
core_combine_over_reverse_ca_sse2 (uint32_t*       pd,
                                   const uint32_t* ps,
                                   const uint32_t *pm,
                                   int             w)
d1689 7
a1695 5
static force_inline void
core_combine_in_ca_sse2 (uint32_t *      pd,
                         const uint32_t *ps,
                         const uint32_t *pm,
                         int             w)
d1710 4
a1713 4
	*pd++ = pack_1x64_32 (
	    pix_multiply_1x64 (
		pix_multiply_1x64 (unpack_32_1x64 (s), unpack_32_1x64 (m)),
		expand_alpha_1x64 (unpack_32_1x64 (d))));
d1754 5
a1758 5
	*pd++ = pack_1x64_32 (
	    pix_multiply_1x64 (
		pix_multiply_1x64 (
		    unpack_32_1x64 (s), unpack_32_1x64 (m)),
		expand_alpha_1x64 (unpack_32_1x64 (d))));
d1764 7
a1770 5
static force_inline void
core_combine_in_reverse_ca_sse2 (uint32_t *      pd,
                                 const uint32_t *ps,
                                 const uint32_t *pm,
                                 int             w)
d1785 5
a1789 5
	*pd++ = pack_1x64_32 (
	    pix_multiply_1x64 (
		unpack_32_1x64 (d),
		pix_multiply_1x64 (unpack_32_1x64 (m),
				   expand_alpha_1x64 (unpack_32_1x64 (s)))));
d1828 5
a1832 5
	*pd++ = pack_1x64_32 (
	    pix_multiply_1x64 (
		unpack_32_1x64 (d),
		pix_multiply_1x64 (unpack_32_1x64 (m),
				   expand_alpha_1x64 (unpack_32_1x64 (s)))));
d1837 7
a1843 5
static force_inline void
core_combine_out_ca_sse2 (uint32_t *      pd,
                          const uint32_t *ps,
                          const uint32_t *pm,
                          int             w)
d1858 5
a1862 5
	*pd++ = pack_1x64_32 (
	    pix_multiply_1x64 (
		pix_multiply_1x64 (
		    unpack_32_1x64 (s), unpack_32_1x64 (m)),
		negate_1x64 (expand_alpha_1x64 (unpack_32_1x64 (d)))));
d1903 5
a1907 5
	*pd++ = pack_1x64_32 (
	    pix_multiply_1x64 (
		pix_multiply_1x64 (
		    unpack_32_1x64 (s), unpack_32_1x64 (m)),
		negate_1x64 (expand_alpha_1x64 (unpack_32_1x64 (d)))));
d1913 7
a1919 5
static force_inline void
core_combine_out_reverse_ca_sse2 (uint32_t *      pd,
                                  const uint32_t *ps,
                                  const uint32_t *pm,
                                  int             w)
d1934 6
a1939 6
	*pd++ = pack_1x64_32 (
	    pix_multiply_1x64 (
		unpack_32_1x64 (d),
		negate_1x64 (pix_multiply_1x64 (
				 unpack_32_1x64 (m),
				 expand_alpha_1x64 (unpack_32_1x64 (s))))));
d1982 6
a1987 6
	*pd++ = pack_1x64_32 (
	    pix_multiply_1x64 (
		unpack_32_1x64 (d),
		negate_1x64 (pix_multiply_1x64 (
				 unpack_32_1x64 (m),
				 expand_alpha_1x64 (unpack_32_1x64 (s))))));
d1997 5
a2001 5
    __m64 m = unpack_32_1x64 (mask);
    __m64 s = unpack_32_1x64 (src);
    __m64 d = unpack_32_1x64 (dst);
    __m64 sa = expand_alpha_1x64 (s);
    __m64 da = expand_alpha_1x64 (d);
d2003 2
a2004 2
    s = pix_multiply_1x64 (s, m);
    m = negate_1x64 (pix_multiply_1x64 (m, sa));
d2006 1
a2006 1
    return pack_1x64_32 (pix_add_multiply_1x64 (&d, &m, &s, &da));
d2009 7
a2015 5
static force_inline void
core_combine_atop_ca_sse2 (uint32_t *      pd,
                           const uint32_t *ps,
                           const uint32_t *pm,
                           int             w)
d2089 3
a2091 3
    __m64 m = unpack_32_1x64 (mask);
    __m64 s = unpack_32_1x64 (src);
    __m64 d = unpack_32_1x64 (dst);
d2093 2
a2094 2
    __m64 da = negate_1x64 (expand_alpha_1x64 (d));
    __m64 sa = expand_alpha_1x64 (s);
d2096 2
a2097 2
    s = pix_multiply_1x64 (s, m);
    m = pix_multiply_1x64 (m, sa);
d2099 1
a2099 1
    return pack_1x64_32 (pix_add_multiply_1x64 (&d, &m, &s, &da));
d2102 7
a2108 5
static force_inline void
core_combine_reverse_atop_ca_sse2 (uint32_t *      pd,
                                   const uint32_t *ps,
                                   const uint32_t *pm,
                                   int             w)
d2183 8
a2190 8
    __m64 a = unpack_32_1x64 (mask);
    __m64 s = unpack_32_1x64 (src);
    __m64 d = unpack_32_1x64 (dst);

    __m64 alpha_dst = negate_1x64 (pix_multiply_1x64 (
				       a, expand_alpha_1x64 (s)));
    __m64 dest      = pix_multiply_1x64 (s, a);
    __m64 alpha_src = negate_1x64 (expand_alpha_1x64 (d));
d2192 1
a2192 1
    return pack_1x64_32 (pix_add_multiply_1x64 (&d,
d2198 7
a2204 5
static force_inline void
core_combine_xor_ca_sse2 (uint32_t *      pd,
                          const uint32_t *ps,
                          const uint32_t *pm,
                          int             w)
d2276 7
a2282 5
static force_inline void
core_combine_add_ca_sse2 (uint32_t *      pd,
                          const uint32_t *ps,
                          const uint32_t *pm,
                          int             w)
d2296 4
a2299 4
	*pd++ = pack_1x64_32 (
	    _mm_adds_pu8 (pix_multiply_1x64 (unpack_32_1x64 (s),
					     unpack_32_1x64 (m)),
			  unpack_32_1x64 (d)));
d2334 4
a2337 4
	*pd++ = pack_1x64_32 (
	    _mm_adds_pu8 (pix_multiply_1x64 (unpack_32_1x64 (s),
					     unpack_32_1x64 (m)),
			  unpack_32_1x64 (d)));
a2341 9
/* ---------------------------------------------------
 * fb_compose_setup_sSE2
 */
static force_inline __m64
create_mask_16_64 (uint16_t mask)
{
    return _mm_set1_pi16 (mask);
}

a2347 7
static force_inline __m64
create_mask_2x32_64 (uint32_t mask0,
                     uint32_t mask1)
{
    return _mm_set_pi32 (mask0, mask1);
}

d2357 1
a2357 256
    return _mm_set_epi32 (mask0, mask1, mask0, mask1);
}
#endif

/* SSE2 code patch for fbcompose.c */

static void
sse2_combine_over_u (pixman_implementation_t *imp,
                     pixman_op_t              op,
                     uint32_t *               dst,
                     const uint32_t *         src,
                     const uint32_t *         mask,
                     int                      width)
{
    core_combine_over_u_sse2 (dst, src, mask, width);
    _mm_empty ();
}

static void
sse2_combine_over_reverse_u (pixman_implementation_t *imp,
                             pixman_op_t              op,
                             uint32_t *               dst,
                             const uint32_t *         src,
                             const uint32_t *         mask,
                             int                      width)
{
    core_combine_over_reverse_u_sse2 (dst, src, mask, width);
    _mm_empty ();
}

static void
sse2_combine_in_u (pixman_implementation_t *imp,
                   pixman_op_t              op,
                   uint32_t *               dst,
                   const uint32_t *         src,
                   const uint32_t *         mask,
                   int                      width)
{
    core_combine_in_u_sse2 (dst, src, mask, width);
    _mm_empty ();
}

static void
sse2_combine_in_reverse_u (pixman_implementation_t *imp,
                           pixman_op_t              op,
                           uint32_t *               dst,
                           const uint32_t *         src,
                           const uint32_t *         mask,
                           int                      width)
{
    core_combine_reverse_in_u_sse2 (dst, src, mask, width);
    _mm_empty ();
}

static void
sse2_combine_out_u (pixman_implementation_t *imp,
                    pixman_op_t              op,
                    uint32_t *               dst,
                    const uint32_t *         src,
                    const uint32_t *         mask,
                    int                      width)
{
    core_combine_out_u_sse2 (dst, src, mask, width);
    _mm_empty ();
}

static void
sse2_combine_out_reverse_u (pixman_implementation_t *imp,
                            pixman_op_t              op,
                            uint32_t *               dst,
                            const uint32_t *         src,
                            const uint32_t *         mask,
                            int                      width)
{
    core_combine_reverse_out_u_sse2 (dst, src, mask, width);
    _mm_empty ();
}

static void
sse2_combine_atop_u (pixman_implementation_t *imp,
                     pixman_op_t              op,
                     uint32_t *               dst,
                     const uint32_t *         src,
                     const uint32_t *         mask,
                     int                      width)
{
    core_combine_atop_u_sse2 (dst, src, mask, width);
    _mm_empty ();
}

static void
sse2_combine_atop_reverse_u (pixman_implementation_t *imp,
                             pixman_op_t              op,
                             uint32_t *               dst,
                             const uint32_t *         src,
                             const uint32_t *         mask,
                             int                      width)
{
    core_combine_reverse_atop_u_sse2 (dst, src, mask, width);
    _mm_empty ();
}

static void
sse2_combine_xor_u (pixman_implementation_t *imp,
                    pixman_op_t              op,
                    uint32_t *               dst,
                    const uint32_t *         src,
                    const uint32_t *         mask,
                    int                      width)
{
    core_combine_xor_u_sse2 (dst, src, mask, width);
    _mm_empty ();
}

static void
sse2_combine_add_u (pixman_implementation_t *imp,
                    pixman_op_t              op,
                    uint32_t *               dst,
                    const uint32_t *         src,
                    const uint32_t *         mask,
                    int                      width)
{
    core_combine_add_u_sse2 (dst, src, mask, width);
    _mm_empty ();
}

static void
sse2_combine_saturate_u (pixman_implementation_t *imp,
                         pixman_op_t              op,
                         uint32_t *               dst,
                         const uint32_t *         src,
                         const uint32_t *         mask,
                         int                      width)
{
    core_combine_saturate_u_sse2 (dst, src, mask, width);
    _mm_empty ();
}

static void
sse2_combine_src_ca (pixman_implementation_t *imp,
                     pixman_op_t              op,
                     uint32_t *               dst,
                     const uint32_t *         src,
                     const uint32_t *         mask,
                     int                      width)
{
    core_combine_src_ca_sse2 (dst, src, mask, width);
    _mm_empty ();
}

static void
sse2_combine_over_ca (pixman_implementation_t *imp,
                      pixman_op_t              op,
                      uint32_t *               dst,
                      const uint32_t *         src,
                      const uint32_t *         mask,
                      int                      width)
{
    core_combine_over_ca_sse2 (dst, src, mask, width);
    _mm_empty ();
}

static void
sse2_combine_over_reverse_ca (pixman_implementation_t *imp,
                              pixman_op_t              op,
                              uint32_t *               dst,
                              const uint32_t *         src,
                              const uint32_t *         mask,
                              int                      width)
{
    core_combine_over_reverse_ca_sse2 (dst, src, mask, width);
    _mm_empty ();
}

static void
sse2_combine_in_ca (pixman_implementation_t *imp,
                    pixman_op_t              op,
                    uint32_t *               dst,
                    const uint32_t *         src,
                    const uint32_t *         mask,
                    int                      width)
{
    core_combine_in_ca_sse2 (dst, src, mask, width);
    _mm_empty ();
}

static void
sse2_combine_in_reverse_ca (pixman_implementation_t *imp,
                            pixman_op_t              op,
                            uint32_t *               dst,
                            const uint32_t *         src,
                            const uint32_t *         mask,
                            int                      width)
{
    core_combine_in_reverse_ca_sse2 (dst, src, mask, width);
    _mm_empty ();
}

static void
sse2_combine_out_ca (pixman_implementation_t *imp,
                     pixman_op_t              op,
                     uint32_t *               dst,
                     const uint32_t *         src,
                     const uint32_t *         mask,
                     int                      width)
{
    core_combine_out_ca_sse2 (dst, src, mask, width);
    _mm_empty ();
}

static void
sse2_combine_out_reverse_ca (pixman_implementation_t *imp,
                             pixman_op_t              op,
                             uint32_t *               dst,
                             const uint32_t *         src,
                             const uint32_t *         mask,
                             int                      width)
{
    core_combine_out_reverse_ca_sse2 (dst, src, mask, width);
    _mm_empty ();
}

static void
sse2_combine_atop_ca (pixman_implementation_t *imp,
                      pixman_op_t              op,
                      uint32_t *               dst,
                      const uint32_t *         src,
                      const uint32_t *         mask,
                      int                      width)
{
    core_combine_atop_ca_sse2 (dst, src, mask, width);
    _mm_empty ();
}

static void
sse2_combine_atop_reverse_ca (pixman_implementation_t *imp,
                              pixman_op_t              op,
                              uint32_t *               dst,
                              const uint32_t *         src,
                              const uint32_t *         mask,
                              int                      width)
{
    core_combine_reverse_atop_ca_sse2 (dst, src, mask, width);
    _mm_empty ();
}

static void
sse2_combine_xor_ca (pixman_implementation_t *imp,
                     pixman_op_t              op,
                     uint32_t *               dst,
                     const uint32_t *         src,
                     const uint32_t *         mask,
                     int                      width)
{
    core_combine_xor_ca_sse2 (dst, src, mask, width);
    _mm_empty ();
d2359 1
a2359 16

static void
sse2_combine_add_ca (pixman_implementation_t *imp,
                     pixman_op_t              op,
                     uint32_t *               dst,
                     const uint32_t *         src,
                     const uint32_t *         mask,
                     int                      width)
{
    core_combine_add_ca_sse2 (dst, src, mask, width);
    _mm_empty ();
}

/* -------------------------------------------------------------------
 * composite_over_n_8888
 */
d2383 1
a2383 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d2404 3
a2406 3
	    *dst++ = pack_1x64_32 (over_1x64 (_mm_movepi64_pi64 (xmm_src),
	                                      _mm_movepi64_pi64 (xmm_alpha),
	                                      unpack_32_1x64 (d)));
d2431 3
a2433 3
	    *dst++ = pack_1x64_32 (over_1x64 (_mm_movepi64_pi64 (xmm_src),
	                                      _mm_movepi64_pi64 (xmm_alpha),
	                                      unpack_32_1x64 (d)));
a2437 1
    _mm_empty ();
a2439 3
/* ---------------------------------------------------------------------
 * composite_over_n_0565
 */
d2462 1
a2462 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d2485 3
a2487 3
		pack_1x64_32 (over_1x64 (_mm_movepi64_pi64 (xmm_src),
					 _mm_movepi64_pi64 (xmm_alpha),
					 expand565_16_1x64 (d))));
d2518 2
a2519 3
		pack_1x64_32 (over_1x64 (_mm_movepi64_pi64 (xmm_src),
					 _mm_movepi64_pi64 (xmm_alpha),
					 expand565_16_1x64 (d))));
a2522 1
    _mm_empty ();
a2524 3
/* ------------------------------
 * composite_add_n_8888_8888_ca
 */
d2550 1
a2550 1
    __m64 mmx_src, mmx_alpha, mmx_mask, mmx_dest;
d2552 1
a2552 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d2566 2
a2567 2
    mmx_src   = _mm_movepi64_pi64 (xmm_src);
    mmx_alpha = _mm_movepi64_pi64 (xmm_alpha);
d2586 2
a2587 2
		mmx_mask = unpack_32_1x64 (m);
		mmx_dest = unpack_32_1x64 (d);
d2589 3
a2591 2
		*pd = pack_1x64_32 (
		    _mm_adds_pu8 (pix_multiply_1x64 (mmx_mask, mmx_src), mmx_dest));
d2635 2
a2636 2
		mmx_mask = unpack_32_1x64 (m);
		mmx_dest = unpack_32_1x64 (d);
d2638 3
a2640 2
		*pd = pack_1x64_32 (
		    _mm_adds_pu8 (pix_multiply_1x64 (mmx_mask, mmx_src), mmx_dest));
a2647 1
    _mm_empty ();
a2649 4
/* ---------------------------------------------------------------------------
 * composite_over_n_8888_8888_ca
 */

d2675 1
a2675 1
    __m64 mmx_src, mmx_alpha, mmx_mask, mmx_dest;
d2677 1
a2677 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d2690 2
a2691 2
    mmx_src   = _mm_movepi64_pi64 (xmm_src);
    mmx_alpha = _mm_movepi64_pi64 (xmm_alpha);
d2709 2
a2710 2
		mmx_mask = unpack_32_1x64 (m);
		mmx_dest = unpack_32_1x64 (d);
d2712 1
a2712 1
		*pd = pack_1x64_32 (in_over_1x64 (&mmx_src,
d2759 2
a2760 2
		mmx_mask = unpack_32_1x64 (m);
		mmx_dest = unpack_32_1x64 (d);
d2762 2
a2763 2
		*pd = pack_1x64_32 (
		    in_over_1x64 (&mmx_src, &mmx_alpha, &mmx_mask, &mmx_dest));
a2770 1
    _mm_empty ();
a2772 4
/*---------------------------------------------------------------------
 * composite_over_8888_n_8888
 */

d2804 1
a2804 1
    mask = _pixman_image_get_solid (mask_image, PIXMAN_a8r8g8b8);
a2818 9
	    uint32_t d = *dst;

	    __m64 ms = unpack_32_1x64 (s);
	    __m64 alpha    = expand_alpha_1x64 (ms);
	    __m64 dest     = _mm_movepi64_pi64 (xmm_mask);
	    __m64 alpha_dst = unpack_32_1x64 (d);

	    *dst++ = pack_1x64_32 (
		in_over_1x64 (&ms, &alpha, &dest, &alpha_dst));
d2820 13
a2838 14
	    xmm_dst = load_128_aligned ((__m128i*)dst);

	    unpack_128_2x128 (xmm_src, &xmm_src_lo, &xmm_src_hi);
	    unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);
	    expand_alpha_2x128 (xmm_src_lo, xmm_src_hi,
				&xmm_alpha_lo, &xmm_alpha_hi);

	    in_over_2x128 (&xmm_src_lo, &xmm_src_hi,
			   &xmm_alpha_lo, &xmm_alpha_hi,
			   &xmm_mask, &xmm_mask,
			   &xmm_dst_lo, &xmm_dst_hi);

	    save_128_aligned (
		(__m128i*)dst, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));
d2840 18
a2865 1
	    uint32_t d = *dst;
d2867 12
a2878 7
	    __m64 ms = unpack_32_1x64 (s);
	    __m64 alpha = expand_alpha_1x64 (ms);
	    __m64 mask  = _mm_movepi64_pi64 (xmm_mask);
	    __m64 dest  = unpack_32_1x64 (d);

	    *dst++ = pack_1x64_32 (
		in_over_1x64 (&ms, &alpha, &mask, &dest));
d2880 1
a2884 1
    _mm_empty ();
a2886 4
/*---------------------------------------------------------------------
 * composite_over_8888_n_8888
 */

a2952 1
    _mm_empty ();
a2954 3
/* ---------------------------------------------------------------------
 * composite_over_x888_n_8888
 */
d2985 1
a2985 1
    mask = _pixman_image_get_solid (mask_image, PIXMAN_a8r8g8b8);
d3003 4
a3006 4
	    __m64 src   = unpack_32_1x64 (s);
	    __m64 alpha = _mm_movepi64_pi64 (xmm_alpha);
	    __m64 mask  = _mm_movepi64_pi64 (xmm_mask);
	    __m64 dest  = unpack_32_1x64 (d);
d3008 2
a3009 2
	    *dst++ = pack_1x64_32 (
		in_over_1x64 (&src, &alpha, &mask, &dest));
d3042 4
a3045 4
	    __m64 src  = unpack_32_1x64 (s);
	    __m64 alpha = _mm_movepi64_pi64 (xmm_alpha);
	    __m64 mask  = _mm_movepi64_pi64 (xmm_mask);
	    __m64 dest  = unpack_32_1x64 (d);
d3047 2
a3048 2
	    *dst++ = pack_1x64_32 (
		in_over_1x64 (&src, &alpha, &mask, &dest));
a3053 1
    _mm_empty ();
a3055 3
/* --------------------------------------------------------------------
 * composite_over_8888_8888
 */
d3085 1
a3085 1
	core_combine_over_u_sse2 (dst, src, NULL, width);
a3089 1
    _mm_empty ();
a3091 3
/* ------------------------------------------------------------------
 * composite_over_8888_0565
 */
d3095 1
a3095 1
    __m64 ms;
d3097 1
a3097 1
    ms = unpack_32_1x64 (src);
d3099 3
a3101 3
	pack_1x64_32 (
	    over_1x64 (
		ms, expand_alpha_1x64 (ms), expand565_16_1x64 (dst))));
a3132 9
#if 0
    /* FIXME
     *
     * I copy the code from MMX one and keep the fixme.
     * If it's a problem there, probably is a problem here.
     */
    assert (src_image->drawable == mask_image->drawable);
#endif

a3204 1
    _mm_empty ();
a3206 4
/* -----------------------------------------------------------------
 * composite_over_n_8_8888
 */

d3233 1
a3233 1
    __m64 mmx_src, mmx_alpha, mmx_mask, mmx_dest;
d3235 1
a3235 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d3249 2
a3250 2
    mmx_src   = _mm_movepi64_pi64 (xmm_src);
    mmx_alpha = _mm_movepi64_pi64 (xmm_alpha);
d3267 2
a3268 2
		mmx_mask = expand_pixel_8_1x64 (m);
		mmx_dest = unpack_32_1x64 (d);
d3270 1
a3270 1
		*dst = pack_1x64_32 (in_over_1x64 (&mmx_src,
d3322 2
a3323 2
		mmx_mask = expand_pixel_8_1x64 (m);
		mmx_dest = unpack_32_1x64 (d);
d3325 1
a3325 1
		*dst = pack_1x64_32 (in_over_1x64 (&mmx_src,
a3335 1
    _mm_empty ();
d3338 1
a3338 5
/* ----------------------------------------------------------------
 * composite_over_n_8_8888
 */

pixman_bool_t
a3484 1
    _mm_empty ();
d3513 1
a3513 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d3546 2
a3547 3
		*dst = pack_1x64_32 (
		    pix_multiply_1x64 (
			_mm_movepi64_pi64 (xmm_src), expand_pixel_8_1x64 (m)));
d3600 3
a3602 3
		*dst = pack_1x64_32 (
		    pix_multiply_1x64 (
			_mm_movepi64_pi64 (xmm_src), expand_pixel_8_1x64 (m)));
a3613 1
    _mm_empty ();
a3615 4
/*-----------------------------------------------------------------------
 * composite_over_n_8_0565
 */

d3637 1
a3637 1
    __m64 mmx_src, mmx_alpha, mmx_mask, mmx_dest;
d3643 1
a3643 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d3656 2
a3657 2
    mmx_src = _mm_movepi64_pi64 (xmm_src);
    mmx_alpha = _mm_movepi64_pi64 (xmm_alpha);
d3674 2
a3675 2
		mmx_mask = expand_alpha_rev_1x64 (unpack_32_1x64 (m));
		mmx_dest = expand565_16_1x64 (d);
d3678 2
a3679 2
		    pack_1x64_32 (
			in_over_1x64 (
d3747 2
a3748 2
		mmx_mask = expand_alpha_rev_1x64 (unpack_32_1x64 (m));
		mmx_dest = expand565_16_1x64 (d);
d3751 2
a3752 2
		    pack_1x64_32 (
			in_over_1x64 (
a3760 1
    _mm_empty ();
a3762 4
/* -----------------------------------------------------------------------
 * composite_over_pixbuf_0565
 */

d3784 1
a3784 1
    __m64 ms;
a3792 9
#if 0
    /* FIXME
     *
     * I copy the code from MMX one and keep the fixme.
     * If it's a problem there, probably is a problem here.
     */
    assert (src_image->drawable == mask_image->drawable);
#endif

d3806 1
a3806 1
	    ms = unpack_32_1x64 (s);
d3809 2
a3810 2
		pack_1x64_32 (
		    over_rev_non_pre_1x64 (ms, expand565_16_1x64 (d))));
d3872 1
a3872 1
	    ms = unpack_32_1x64 (s);
d3875 2
a3876 2
		pack_1x64_32 (
		    over_rev_non_pre_1x64 (ms, expand565_16_1x64 (d))));
a3880 1
    _mm_empty ();
a3882 4
/* -------------------------------------------------------------------------
 * composite_over_pixbuf_8888
 */

a3911 9
#if 0
    /* FIXME
     *
     * I copy the code from MMX one and keep the fixme.
     * If it's a problem there, probably is a problem here.
     */
    assert (src_image->drawable == mask_image->drawable);
#endif

d3925 3
a3927 3
	    *dst++ = pack_1x64_32 (
		over_rev_non_pre_1x64 (
		    unpack_32_1x64 (s), unpack_32_1x64 (d)));
d3972 3
a3974 3
	    *dst++ = pack_1x64_32 (
		over_rev_non_pre_1x64 (
		    unpack_32_1x64 (s), unpack_32_1x64 (d)));
a3979 1
    _mm_empty ();
a3981 4
/* -------------------------------------------------------------------------------------------------
 * composite_over_n_8888_0565_ca
 */

d4008 1
a4008 1
    __m64 mmx_src, mmx_alpha, mmx_mask, mmx_dest;
d4010 1
a4010 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d4022 2
a4023 2
    mmx_src = _mm_movepi64_pi64 (xmm_src);
    mmx_alpha = _mm_movepi64_pi64 (xmm_alpha);
d4040 2
a4041 2
		mmx_mask = unpack_32_1x64 (m);
		mmx_dest = expand565_16_1x64 (d);
d4044 2
a4045 2
		    pack_1x64_32 (
			in_over_1x64 (
d4109 2
a4110 2
		mmx_mask = unpack_32_1x64 (m);
		mmx_dest = expand565_16_1x64 (d);
d4113 2
a4114 2
		    pack_1x64_32 (
			in_over_1x64 (
a4123 1
    _mm_empty ();
a4125 4
/* -----------------------------------------------------------------------
 * composite_in_n_8_8
 */

d4158 1
a4158 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d4177 5
a4181 5
	    *dst++ = (uint8_t) pack_1x64_32 (
		pix_multiply_1x64 (
		    pix_multiply_1x64 (_mm_movepi64_pi64 (xmm_alpha),
				       unpack_32_1x64 (m)),
		    unpack_32_1x64 (d)));
d4214 5
a4218 5
	    *dst++ = (uint8_t) pack_1x64_32 (
		pix_multiply_1x64 (
		    pix_multiply_1x64 (
			_mm_movepi64_pi64 (xmm_alpha), unpack_32_1x64 (m)),
		    unpack_32_1x64 (d)));
a4222 1
    _mm_empty ();
a4224 4
/* -----------------------------------------------------------------------
 * composite_in_n_8
 */

d4252 1
a4252 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d4279 4
a4282 4
	    *dst++ = (uint8_t) pack_1x64_32 (
		pix_multiply_1x64 (
		    _mm_movepi64_pi64 (xmm_alpha),
		    unpack_32_1x64 (d)));
d4307 4
a4310 4
	    *dst++ = (uint8_t) pack_1x64_32 (
		pix_multiply_1x64 (
		    _mm_movepi64_pi64 (xmm_alpha),
		    unpack_32_1x64 (d)));
a4314 1
    _mm_empty ();
a4316 4
/* ---------------------------------------------------------------------------
 * composite_in_8_8
 */

d4359 3
a4361 3
	    *dst++ = (uint8_t) pack_1x64_32 (
		pix_multiply_1x64 (
		    unpack_32_1x64 (s), unpack_32_1x64 (d)));
d4390 2
a4391 2
	    *dst++ = (uint8_t) pack_1x64_32 (
		pix_multiply_1x64 (unpack_32_1x64 (s), unpack_32_1x64 (d)));
a4395 1
    _mm_empty ();
a4397 4
/* -------------------------------------------------------------------------
 * composite_add_n_8_8
 */

d4430 1
a4430 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d4449 5
a4453 5
	    *dst++ = (uint8_t) pack_1x64_32 (
		_mm_adds_pu16 (
		    pix_multiply_1x64 (
			_mm_movepi64_pi64 (xmm_alpha), unpack_32_1x64 (m)),
		    unpack_32_1x64 (d)));
d4485 5
a4489 5
	    *dst++ = (uint8_t) pack_1x64_32 (
		_mm_adds_pu16 (
		    pix_multiply_1x64 (
			_mm_movepi64_pi64 (xmm_alpha), unpack_32_1x64 (m)),
		    unpack_32_1x64 (d)));
a4494 1
    _mm_empty ();
a4496 4
/* -------------------------------------------------------------------------
 * composite_add_n_8_8
 */

d4522 1
a4522 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d4548 4
a4551 4
	    *dst = (uint8_t)_mm_cvtsi64_si32 (
		_mm_adds_pu8 (
		    _mm_movepi64_pi64 (xmm_src),
		    _mm_cvtsi32_si64 (*dst)));
d4568 4
a4571 4
	    *dst = (uint8_t)_mm_cvtsi64_si32 (
		_mm_adds_pu8 (
		    _mm_movepi64_pi64 (xmm_src),
		    _mm_cvtsi32_si64 (*dst)));
a4577 1
    _mm_empty ();
a4579 4
/* ----------------------------------------------------------------------
 * composite_add_8_8
 */

d4623 2
a4624 1
	core_combine_add_u_sse2 ((uint32_t*)dst, (uint32_t*)src, NULL, w >> 2);
a4639 1
    _mm_empty ();
a4641 3
/* ---------------------------------------------------------------------
 * composite_add_8888_8888
 */
d4673 1
a4673 1
	core_combine_add_u_sse2 (dst, src, NULL, width);
a4675 1
    _mm_empty ();
a4677 4
/* -------------------------------------------------------------------------------------------------
 * sse2_composite_copy_area
 */

a4795 1
    _mm_empty ();
d4845 1
a4845 1
    __m64 ms;
d4874 1
a4874 1
            ms = unpack_32_1x64 (s);
d4878 2
a4879 2
		__m64 ma = expand_alpha_rev_1x64 (unpack_32_1x64 (m));
		__m64 md = unpack_32_1x64 (d);
d4881 1
a4881 1
                ms = in_over_1x64 (&ms, &mask_x00ff, &ma, &md);
d4884 1
a4884 1
            *dst++ = pack_1x64_32 (ms);
d4891 2
a4892 1
            xmm_src = _mm_or_si128 (load_128_unaligned ((__m128i*)src), mask_ff000000);
d4908 2
a4909 1
                expand_alpha_rev_2x128 (xmm_mask_lo, xmm_mask_hi, &xmm_mask_lo, &xmm_mask_hi);
d4911 3
a4913 1
                in_over_2x128 (&xmm_src_lo, &xmm_src_hi, &mask_00ff, &mask_00ff, &xmm_mask_lo, &xmm_mask_hi, &xmm_dst_lo, &xmm_dst_hi);
d4938 1
a4938 1
		    __m64 ma, md, ms;
d4942 3
a4944 3
		    ma = expand_alpha_rev_1x64 (unpack_32_1x64 (m));
		    md = unpack_32_1x64 (d);
		    ms = unpack_32_1x64 (s);
d4946 1
a4946 1
                    *dst = pack_1x64_32 (in_over_1x64 (&ms, &mask_x00ff, &ma, &md));
a4956 1
    _mm_empty ();
d5021 1
a5021 1
		    __m64 ms, md, ma, msa;
d5023 3
a5025 3
		    ma = expand_alpha_rev_1x64 (load_32_1x64 (m));
		    ms = unpack_32_1x64 (s);
		    md = unpack_32_1x64 (d);
d5027 1
a5027 1
		    msa = expand_alpha_rev_1x64 (load_32_1x64 (sa));
d5029 1
a5029 1
		    *dst = pack_1x64_32 (in_over_1x64 (&ms, &msa, &ma, &md));
d5093 1
a5093 1
		    __m64 ms, md, ma, msa;
d5095 3
a5097 3
		    ma = expand_alpha_rev_1x64 (load_32_1x64 (m));
		    ms = unpack_32_1x64 (s);
		    md = unpack_32_1x64 (d);
d5099 1
a5099 1
		    msa = expand_alpha_rev_1x64 (load_32_1x64 (sa));
d5101 1
a5101 1
		    *dst = pack_1x64_32 (in_over_1x64 (&ms, &msa, &ma, &md));
a5109 1
    _mm_empty ();
d5135 1
a5135 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d5154 1
a5154 1
	    __m64 vd;
d5156 1
a5156 1
	    vd = unpack_32_1x64 (*dst);
d5158 2
a5159 2
	    *dst = pack_1x64_32 (over_1x64 (vd, expand_alpha_1x64 (vd),
					    _mm_movepi64_pi64 (xmm_src)));
d5189 1
a5189 1
	    __m64 vd;
d5191 1
a5191 1
	    vd = unpack_32_1x64 (*dst);
d5193 2
a5194 2
	    *dst = pack_1x64_32 (over_1x64 (vd, expand_alpha_1x64 (vd),
					    _mm_movepi64_pi64 (xmm_src)));
a5200 1
    _mm_empty ();
d5265 1
a5265 1
		    __m64 ms, md, ma, msa;
d5267 3
a5269 3
		    ma = expand_alpha_rev_1x64 (load_32_1x64 (m));
		    ms = unpack_32_1x64 (s);
		    md = unpack_32_1x64 (d);
d5271 1
a5271 1
		    msa = expand_alpha_rev_1x64 (load_32_1x64 (sa));
d5273 1
a5273 1
		    *dst = pack_1x64_32 (in_over_1x64 (&ms, &msa, &ma, &md));
d5335 1
a5335 1
		    __m64 ms, md, ma, msa;
d5337 3
a5339 3
		    ma = expand_alpha_rev_1x64 (load_32_1x64 (m));
		    ms = unpack_32_1x64 (s);
		    md = unpack_32_1x64 (d);
d5341 1
a5341 1
		    msa = expand_alpha_rev_1x64 (load_32_1x64 (sa));
d5343 1
a5343 1
		    *dst = pack_1x64_32 (in_over_1x64 (&ms, &msa, &ma, &md));
a5351 1
    _mm_empty ();
d5354 1
a5354 1
/* A variant of 'core_combine_over_u_sse2' with minor tweaks */
d5361 2
a5362 1
                                             pixman_fixed_t  max_vx)
d5371 3
a5445 1
    _mm_empty ();
d5450 1
a5450 1
		       uint32_t, uint32_t, COVER);
d5453 1
a5453 1
		       uint32_t, uint32_t, NONE);
d5456 221
a5676 1
		       uint32_t, uint32_t, PAD);
d5774 9
d5838 182
a6019 1
#if defined(__GNUC__) && !defined(__x86_64__) && !defined(__amd64__)
d6023 1
a6023 1
_pixman_implementation_create_sse2 (void)
a6024 5
#ifdef USE_MMX
    pixman_implementation_t *fallback = _pixman_implementation_create_mmx ();
#else
    pixman_implementation_t *fallback = _pixman_implementation_create_fast_path ();
#endif
a6043 11
    /* MMX constants */
    mask_x565_rgb = create_mask_2x32_64 (0x000001f0, 0x003f001f);
    mask_x565_unpack = create_mask_2x32_64 (0x00000084, 0x04100840);

    mask_x0080 = create_mask_16_64 (0x0080);
    mask_x00ff = create_mask_16_64 (0x00ff);
    mask_x0101 = create_mask_16_64 (0x0101);
    mask_x_alpha = create_mask_2x32_64 (0x00ff0000, 0x00000000);

    _mm_empty ();

a6044 2

    /* SSE code patch for fbcompose.c */
d6073 2
a6076 2

#endif /* USE_SSE2 */
@


1.4
log
@Update to pixman 0.18.4.

Tweak build to use libpthread-stubs for TLS emulation instead of forcing
every application using pixman to use -pthread.

Tested by jasper@@ and landry@@ on a bulk ports build.
@
text
@d38 1
a359 28
static force_inline void
cache_prefetch (__m128i* addr)
{
    _mm_prefetch ((void const*)addr, _MM_HINT_T0);
}

static force_inline void
cache_prefetch_next (__m128i* addr)
{
    _mm_prefetch ((void const *)(addr + 4), _MM_HINT_T0); /* 64 bytes ahead */
}

/* prefetching NULL is very slow on some systems. don't do that. */

static force_inline void
maybe_prefetch (__m128i* addr)
{
    if (addr)
	cache_prefetch (addr);
}

static force_inline void
maybe_prefetch_next (__m128i* addr)
{
    if (addr)
	cache_prefetch_next (addr);
}

a623 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a636 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a638 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	maybe_prefetch_next ((__m128i*)pm);

a699 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a713 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a715 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	maybe_prefetch_next ((__m128i*)pm);

a786 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a798 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a800 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	maybe_prefetch_next ((__m128i*)pm);

a845 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a857 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a859 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	maybe_prefetch_next ((__m128i*)pm);

a899 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a915 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a920 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	maybe_prefetch_next ((__m128i*)pm);

a966 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a981 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a986 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	maybe_prefetch_next ((__m128i*)pm);

a1051 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a1063 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a1065 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	maybe_prefetch_next ((__m128i*)pm);

a1133 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a1145 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a1147 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	maybe_prefetch_next ((__m128i*)pm);

a1219 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a1231 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a1233 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	maybe_prefetch_next ((__m128i*)pm);

a1289 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a1302 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a1306 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	maybe_prefetch_next ((__m128i*)pm);

a1360 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a1372 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    maybe_prefetch ((__m128i*)pm);

a1374 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	maybe_prefetch_next ((__m128i*)pm);

a1446 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a1455 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a1457 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	cache_prefetch_next ((__m128i*)pm);

a1512 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a1522 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a1524 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	cache_prefetch_next ((__m128i*)pm);

a1586 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a1596 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a1598 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	cache_prefetch_next ((__m128i*)pm);

a1649 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a1663 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a1665 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	cache_prefetch_next ((__m128i*)pm);

a1722 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a1736 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a1738 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	cache_prefetch_next ((__m128i*)pm);

a1793 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a1807 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a1809 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	cache_prefetch_next ((__m128i*)pm);

a1867 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a1882 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a1884 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	cache_prefetch_next ((__m128i*)pm);

a1962 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a1972 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a1974 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	cache_prefetch_next ((__m128i*)pm);

a2053 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a2063 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a2065 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	cache_prefetch_next ((__m128i*)pm);

a2147 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a2157 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a2159 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	cache_prefetch_next ((__m128i*)pm);

a2221 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a2234 5
    /* call prefetch hint to optimize cache load*/
    cache_prefetch ((__m128i*)ps);
    cache_prefetch ((__m128i*)pd);
    cache_prefetch ((__m128i*)pm);

a2236 5
	/* fill cache line with next memory */
	cache_prefetch_next ((__m128i*)ps);
	cache_prefetch_next ((__m128i*)pd);
	cache_prefetch_next ((__m128i*)pm);

a2615 3
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)dst);

a2627 2
	cache_prefetch ((__m128i*)dst);

a2629 3
	    /* fill cache line with next memory */
	    cache_prefetch_next ((__m128i*)dst);

a2698 3
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)dst);

a2712 3
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)dst);

a2714 3
	    /* fill cache line with next memory */
	    cache_prefetch_next ((__m128i*)dst);

a2804 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)pd);
	cache_prefetch ((__m128i*)pm);

a2823 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)pd);
	cache_prefetch ((__m128i*)pm);

a2825 4
	    /* fill cache line with next memory */
	    cache_prefetch_next ((__m128i*)pd);
	    cache_prefetch_next ((__m128i*)pm);

a2931 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)pd);
	cache_prefetch ((__m128i*)pm);

a2951 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)pd);
	cache_prefetch ((__m128i*)pm);

a2953 4
	    /* fill cache line with next memory */
	    cache_prefetch_next ((__m128i*)pd);
	    cache_prefetch_next ((__m128i*)pm);

a3050 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)dst);
	cache_prefetch ((__m128i*)src);

a3066 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)dst);
	cache_prefetch ((__m128i*)src);

a3068 4
	    /* fill cache line with next memory */
	    cache_prefetch_next ((__m128i*)dst);
	    cache_prefetch_next ((__m128i*)src);

d3110 73
a3228 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)dst);
	cache_prefetch ((__m128i*)src);

a3244 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)dst);
	cache_prefetch ((__m128i*)src);

a3246 4
	    /* fill cache line with next memory */
	    cache_prefetch_next ((__m128i*)dst);
	    cache_prefetch_next ((__m128i*)src);

a3385 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)src);
	cache_prefetch ((__m128i*)dst);

a3400 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)src);
	cache_prefetch ((__m128i*)dst);

a3403 4
	    /* fill cache line with next memory */
	    cache_prefetch_next ((__m128i*)src);
	    cache_prefetch_next ((__m128i*)dst);

a3512 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)mask);
	cache_prefetch ((__m128i*)dst);

a3532 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)mask);
	cache_prefetch ((__m128i*)dst);

a3534 4
	    /* fill cache line with next memory */
	    cache_prefetch_next ((__m128i*)mask);
	    cache_prefetch_next ((__m128i*)dst);

d3611 9
a3619 2
    if (bpp != 16 && bpp != 32)
	return FALSE;
d3621 5
a3625 1
    if (bpp == 16)
d3631 1
d3634 1
a3634 1
    else
d3641 4
a3645 1
    cache_prefetch ((__m128i*)byte_line);
d3655 6
a3660 2

	cache_prefetch_next ((__m128i*)d);
a3676 2
	cache_prefetch_next ((__m128i*)d);

a3678 2
	    cache_prefetch (((__m128i*)d) + 12);

a3693 2
	    cache_prefetch (((__m128i*)d) + 8);

a3702 2
	cache_prefetch_next ((__m128i*)d);

a3719 2
	cache_prefetch_next ((__m128i*)d);

d3734 7
a3798 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)mask);
	cache_prefetch ((__m128i*)dst);

a3817 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)mask);
	cache_prefetch ((__m128i*)dst);

a3819 4
	    /* fill cache line with next memory */
	    cache_prefetch_next ((__m128i*)mask);
	    cache_prefetch_next ((__m128i*)dst);

a3931 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)mask);
	cache_prefetch ((__m128i*)dst);

a3951 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)mask);
	cache_prefetch ((__m128i*)dst);

a3953 4
	    /* fill cache line with next memory */
	    cache_prefetch_next ((__m128i*)mask);
	    cache_prefetch_next ((__m128i*)dst);

a4079 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)src);
	cache_prefetch ((__m128i*)dst);

a4092 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)src);
	cache_prefetch ((__m128i*)dst);

a4094 4
	    /* fill cache line with next memory */
	    cache_prefetch_next ((__m128i*)src);
	    cache_prefetch_next ((__m128i*)dst);

a4212 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)src);
	cache_prefetch ((__m128i*)dst);

a4224 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)src);
	cache_prefetch ((__m128i*)dst);

a4226 4
	    /* fill cache line with next memory */
	    cache_prefetch_next ((__m128i*)src);
	    cache_prefetch_next ((__m128i*)dst);

a4330 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)mask);
	cache_prefetch ((__m128i*)dst);

a4351 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)mask);
	cache_prefetch ((__m128i*)dst);

a4353 4
	    /* fill cache line with next memory */
	    cache_prefetch_next ((__m128i*)mask);
	    cache_prefetch_next ((__m128i*)dst);

a4474 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)mask);
	cache_prefetch ((__m128i*)dst);

a4487 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)mask);
	cache_prefetch ((__m128i*)dst);

a4489 4
	    /* fill cache line with next memory */
	    cache_prefetch_next ((__m128i*)mask);
	    cache_prefetch_next ((__m128i*)dst);

d4529 2
a4530 2
/* ---------------------------------------------------------------------------
 * composite_in_8_8
d4534 13
a4546 13
sse2_composite_in_8_8 (pixman_implementation_t *imp,
                       pixman_op_t              op,
                       pixman_image_t *         src_image,
                       pixman_image_t *         mask_image,
                       pixman_image_t *         dst_image,
                       int32_t                  src_x,
                       int32_t                  src_y,
                       int32_t                  mask_x,
                       int32_t                  mask_y,
                       int32_t                  dest_x,
                       int32_t                  dest_y,
                       int32_t                  width,
                       int32_t                  height)
d4549 3
a4551 2
    uint8_t     *src_line, *src;
    int src_stride, dst_stride;
a4552 1
    uint32_t s, d;
d4554 1
a4554 1
    __m128i xmm_src, xmm_src_lo, xmm_src_hi;
d4559 17
a4575 2
    PIXMAN_IMAGE_GET_LINE (
	src_image, src_x, src_y, uint8_t, src_stride, src_line, 1);
a4580 2
	src = src_line;
	src_line += src_stride;
a4582 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)src);
	cache_prefetch ((__m128i*)dst);

a4584 1
	    s = (uint32_t) *src++;
d4589 2
a4590 1
		    unpack_32_1x64 (s), unpack_32_1x64 (d)));
a4593 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)src);
	cache_prefetch ((__m128i*)dst);

a4595 5
	    /* fill cache line with next memory */
	    cache_prefetch_next ((__m128i*)src);
	    cache_prefetch_next ((__m128i*)dst);

	    xmm_src = load_128_unaligned ((__m128i*)src);
a4597 1
	    unpack_128_2x128 (xmm_src, &xmm_src_lo, &xmm_src_hi);
d4599 2
a4600 2

	    pix_multiply_2x128 (&xmm_src_lo, &xmm_src_hi,
a4606 1
	    src += 16;
a4612 1
	    s = (uint32_t) *src++;
d4616 3
a4618 1
		pix_multiply_1x64 (unpack_32_1x64 (s), unpack_32_1x64 (d)));
d4626 88
a4713 2
/* -------------------------------------------------------------------------
 * composite_add_n_8_8
a4761 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)mask);
	cache_prefetch ((__m128i*)dst);

a4774 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)mask);
	cache_prefetch ((__m128i*)dst);

a4776 4
	    /* fill cache line with next memory */
	    cache_prefetch_next ((__m128i*)mask);
	    cache_prefetch_next ((__m128i*)dst);

d4816 88
d4905 1
a4905 1
 * composite_add_8000_8000
d4909 13
a4921 13
sse2_composite_add_8000_8000 (pixman_implementation_t *imp,
                              pixman_op_t              op,
                              pixman_image_t *         src_image,
                              pixman_image_t *         mask_image,
                              pixman_image_t *         dst_image,
                              int32_t                  src_x,
                              int32_t                  src_y,
                              int32_t                  mask_x,
                              int32_t                  mask_y,
                              int32_t                  dest_x,
                              int32_t                  dest_y,
                              int32_t                  width,
                              int32_t                  height)
a4938 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)src);
	cache_prefetch ((__m128i*)dst);

a5059 3
    cache_prefetch ((__m128i*)src_bytes);
    cache_prefetch ((__m128i*)dst_bytes);

a5068 3
	cache_prefetch_next ((__m128i*)s);
	cache_prefetch_next ((__m128i*)d);

a5085 3
	cache_prefetch_next ((__m128i*)s);
	cache_prefetch_next ((__m128i*)d);

a5089 4
	    /* 128 bytes ahead */
	    cache_prefetch (((__m128i*)s) + 8);
	    cache_prefetch (((__m128i*)d) + 8);

a5104 3
	cache_prefetch_next ((__m128i*)s);
	cache_prefetch_next ((__m128i*)d);

a5113 3
	cache_prefetch_next ((__m128i*)s);
	cache_prefetch_next ((__m128i*)d);

a5205 5
        /* call prefetch hint to optimize cache load*/
        cache_prefetch ((__m128i*)src);
        cache_prefetch ((__m128i*)dst);
        cache_prefetch ((__m128i*)mask);

a5224 5
        /* call prefetch hint to optimize cache load*/
        cache_prefetch ((__m128i*)src);
        cache_prefetch ((__m128i*)dst);
        cache_prefetch ((__m128i*)mask);

a5226 5
            /* fill cache line with next memory */
            cache_prefetch_next ((__m128i*)src);
            cache_prefetch_next ((__m128i*)dst);
            cache_prefetch_next ((__m128i*)mask);

a5336 5
        /* call prefetch hint to optimize cache load*/
        cache_prefetch ((__m128i *)src);
        cache_prefetch ((__m128i *)dst);
        cache_prefetch ((__m128i *)mask);

a5370 5
        /* call prefetch hint to optimize cache load*/
        cache_prefetch ((__m128i *)src);
        cache_prefetch ((__m128i *)dst);
        cache_prefetch ((__m128i *)mask);

a5372 5
            /* fill cache line with next memory */
            cache_prefetch_next ((__m128i *)src);
            cache_prefetch_next ((__m128i *)dst);
            cache_prefetch_next ((__m128i *)mask);

d5447 345
d5810 1
d5841 4
d5848 1
a5848 1
    PIXMAN_STD_FAST_PATH (ADD, a8, null, a8, sse2_composite_add_8000_8000),
d5852 1
d5859 2
d5873 14
@


1.3
log
@Update to pixman 0.16.6. Tested on a full ports build by naddy@@.
@
text
@d371 16
d432 6
d440 1
a440 1
    return _mm_unpacklo_pi8 (_mm_cvtsi32_si64 (data), _mm_setzero_si64 ());
d654 1
a654 1
    cache_prefetch ((__m128i*)pm);
d672 1
a672 1
    cache_prefetch ((__m128i*)pm);
d679 1
a679 1
	cache_prefetch_next ((__m128i*)pm);
d745 1
a745 1
    cache_prefetch ((__m128i*)pm);
d764 1
a764 1
    cache_prefetch ((__m128i*)pm);
d771 1
a771 1
	cache_prefetch_next ((__m128i*)pm);
d847 1
a847 1
    cache_prefetch ((__m128i*)pm);
d864 1
a864 1
    cache_prefetch ((__m128i*)pm);
d871 1
a871 1
	cache_prefetch_next ((__m128i*)pm);
d921 1
a921 1
    cache_prefetch ((__m128i*)pm);
d938 1
a938 1
    cache_prefetch ((__m128i*)pm);
d945 1
a945 1
	cache_prefetch_next ((__m128i*)pm);
d990 1
a990 1
    cache_prefetch ((__m128i*)pm);
d1001 1
a1001 1
	
d1011 1
a1011 1
    cache_prefetch ((__m128i*)pm);
d1021 1
a1021 1
	cache_prefetch_next ((__m128i*)pm);
d1072 1
a1072 1
    cache_prefetch ((__m128i*)pm);
d1092 1
a1092 1
    cache_prefetch ((__m128i*)pm);
d1102 1
a1102 1
	cache_prefetch_next ((__m128i*)pm);
d1172 1
a1172 1
    cache_prefetch ((__m128i*)pm);
d1189 1
a1189 1
    cache_prefetch ((__m128i*)pm);
d1196 1
a1196 1
	cache_prefetch_next ((__m128i*)pm);
d1269 1
a1269 1
    cache_prefetch ((__m128i*)pm);
d1286 1
a1286 1
    cache_prefetch ((__m128i*)pm);
d1293 1
a1293 1
	cache_prefetch_next ((__m128i*)pm);
d1370 1
a1370 1
    cache_prefetch ((__m128i*)pm);
d1387 1
a1387 1
    cache_prefetch ((__m128i*)pm);
d1394 1
a1394 1
	cache_prefetch_next ((__m128i*)pm);
d1455 1
a1455 1
    cache_prefetch ((__m128i*)pm);
d1473 1
a1473 1
    cache_prefetch ((__m128i*)pm);
d1482 1
a1482 1
	cache_prefetch_next ((__m128i*)pm);
d1541 1
a1541 1
    cache_prefetch ((__m128i*)pm);
d1558 1
a1558 1
    cache_prefetch ((__m128i*)pm);
d1565 1
a1565 1
	cache_prefetch_next ((__m128i*)pm);
d2655 2
a2656 2
# define create_mask_2x32_128(mask0, mask1) \
	(_mm_set_epi32 ((mask0), (mask1), (mask0), (mask1)))
d2953 1
a2953 1
    uint16_t w;
d3044 1
a3044 1
    uint16_t w;
d3155 1
a3155 1
    
d3190 1
a3190 1
		
d3229 1
a3229 1
		
d3246 1
a3246 1
		
d3424 1
a3424 1
    uint16_t w;
d3437 1
a3437 1
    mask = _pixman_image_get_solid (mask_image, dst_image->bits.format);
d3542 1
a3542 1
    uint16_t w;
d3553 1
a3553 1
    mask = _pixman_image_get_solid (mask_image, dst_image->bits.format);
d3710 1
a3710 1
    uint16_t w;
d3840 1
a3840 1
    uint16_t w;
a3983 3
    if (bpp == 16 && (data >> 16 != (data & 0xffff)))
	return FALSE;

d3993 1
d4123 1
a4123 1
    uint16_t w;
d4269 1
a4269 1
    uint16_t w;
d4432 1
a4432 1
    uint16_t w;
d4578 1
a4578 1
    uint16_t w;
d4864 1
a4864 1
    uint16_t w, d, m;
d4867 1
d4980 1
a4980 1
    uint16_t w;
d5057 1
a5057 1
 * composite_add_8888_8_8
d5061 13
a5073 13
sse2_composite_add_8888_8_8 (pixman_implementation_t *imp,
                             pixman_op_t              op,
                             pixman_image_t *         src_image,
                             pixman_image_t *         mask_image,
                             pixman_image_t *         dst_image,
                             int32_t                  src_x,
                             int32_t                  src_y,
                             int32_t                  mask_x,
                             int32_t                  mask_y,
                             int32_t                  dest_x,
                             int32_t                  dest_y,
                             int32_t                  width,
                             int32_t                  height)
d5078 1
a5078 1
    uint16_t w;
d5194 1
a5194 1
    uint16_t w;
d5452 1
a5452 3
#if 0
/* This code are buggy in MMX version, now the bug was translated to SSE2 version */
void
d5472 2
a5473 1
    uint16_t w;
d5488 96
a5583 6
	src = src_line;
	src_line += src_stride;
	dst = dst_line;
	dst_line += dst_stride;
	mask = mask_line;
	mask_line += mask_stride;
d5585 2
a5586 1
	w = width;
d5588 1
a5588 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)src);
	cache_prefetch ((__m128i*)dst);
	cache_prefetch ((__m128i*)mask);
d5590 5
a5594 5
	while (w && (unsigned long)dst & 15)
	{
	    s = 0xff000000 | *src++;
	    m = (uint32_t) *mask++;
	    d = *dst;
d5596 2
a5597 1
	    __m64 ms = unpack_32_1x64 (s);
d5599 21
a5619 7
	    if (m != 0xff)
	    {
		ms = in_over_1x64 (ms,
		                   mask_x00ff,
		                   expand_alpha_rev_1x64 (unpack_32_1x64 (m)),
		                   unpack_32_1x64 (d));
	    }
d5621 3
a5623 3
	    *dst++ = pack_1x64_32 (ms);
	    w--;
	}
d5625 6
a5630 4
	/* call prefetch hint to optimize cache load*/
	cache_prefetch ((__m128i*)src);
	cache_prefetch ((__m128i*)dst);
	cache_prefetch ((__m128i*)mask);
d5632 23
a5654 6
	while (w >= 4)
	{
	    /* fill cache line with next memory */
	    cache_prefetch_next ((__m128i*)src);
	    cache_prefetch_next ((__m128i*)dst);
	    cache_prefetch_next ((__m128i*)mask);
d5656 1
a5656 2
	    m = *(uint32_t*) mask;
	    xmm_src = _mm_or_si128 (load_128_unaligned ((__m128i*)src), mask_ff000000);
d5658 1
a5658 1
	    if (m == 0xffffffff)
d5660 16
a5675 1
		save_128_aligned ((__m128i*)dst, xmm_src);
d5677 20
a5696 1
	    else
d5698 9
a5706 1
		xmm_dst = load_128_aligned ((__m128i*)dst);
d5708 1
a5708 2
		xmm_mask = _mm_unpacklo_epi16 (
		    unpack_32_1x128 (m), _mm_setzero_si128 ());
d5710 3
a5712 3
		unpack_128_2x128 (xmm_src, &xmm_src_lo, &xmm_src_hi);
		unpack_128_2x128 (xmm_mask, &xmm_mask_lo, &xmm_mask_hi);
		unpack_128_2x128 (xmm_dst, &xmm_dst_lo, &xmm_dst_hi);
d5714 2
a5715 2
		expand_alpha_rev_2x128 (xmm_mask_lo, xmm_mask_hi,
					&xmm_mask_lo, &xmm_mask_hi);
d5717 2
a5718 4
		in_over_2x128 (xmm_src_lo, xmm_src_hi,
			       mask_00ff, mask_00ff,
			       xmm_mask_lo, xmm_mask_hi,
			       &xmm_dst_lo, &xmm_dst_hi);
d5720 2
a5721 2
		save_128_aligned (
		    (__m128i*)dst, pack_2x128_128 (xmm_dst_lo, xmm_dst_hi));
d5724 13
a5736 5
	    src += 4;
	    dst += 4;
	    mask += 4;
	    w -= 4;
	}
d5738 1
a5738 3
	while (w)
	{
	    m = (uint32_t) *mask++;
d5742 1
a5742 3
		s = 0xff000000 | *src;

		if (m == 0xff)
d5748 5
a5752 1
		    d = *dst;
d5754 3
a5756 6
		    *dst = pack_1x64_32 (
			in_over_1x64 (
			    unpack_32_1x64 (s),
			    mask_x00ff,
			    expand_alpha_rev_1x64 (unpack_32_1x64 (m)),
			    unpack_32_1x64 (d)));
a5757 1

a5759 1
	    src++;
d5761 2
a5762 2
	    w--;
	}
a5767 2
#endif

d5770 71
a5840 72
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_r5g6b5,   sse2_composite_over_n_8_0565,       0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_b5g6r5,   sse2_composite_over_n_8_0565,       0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_null,     PIXMAN_a8r8g8b8, sse2_composite_over_n_8888,         0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_null,     PIXMAN_x8r8g8b8, sse2_composite_over_n_8888,         0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_null,     PIXMAN_r5g6b5,   sse2_composite_over_n_0565,         0 },
    { PIXMAN_OP_OVER, PIXMAN_a8r8g8b8, PIXMAN_null,     PIXMAN_a8r8g8b8, sse2_composite_over_8888_8888,      0 },
    { PIXMAN_OP_OVER, PIXMAN_a8r8g8b8, PIXMAN_null,     PIXMAN_x8r8g8b8, sse2_composite_over_8888_8888,      0 },
    { PIXMAN_OP_OVER, PIXMAN_a8b8g8r8, PIXMAN_null,     PIXMAN_a8b8g8r8, sse2_composite_over_8888_8888,      0 },
    { PIXMAN_OP_OVER, PIXMAN_a8b8g8r8, PIXMAN_null,     PIXMAN_x8b8g8r8, sse2_composite_over_8888_8888,      0 },
    { PIXMAN_OP_OVER, PIXMAN_a8r8g8b8, PIXMAN_null,     PIXMAN_r5g6b5,   sse2_composite_over_8888_0565,      0 },
    { PIXMAN_OP_OVER, PIXMAN_a8b8g8r8, PIXMAN_null,     PIXMAN_b5g6r5,   sse2_composite_over_8888_0565,      0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_a8r8g8b8, sse2_composite_over_n_8_8888,       0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_x8r8g8b8, sse2_composite_over_n_8_8888,       0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_a8b8g8r8, sse2_composite_over_n_8_8888,       0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_x8b8g8r8, sse2_composite_over_n_8_8888,       0 },
#if 0
    /* FIXME: This code are buggy in MMX version, now the bug was translated to SSE2 version */
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8,       PIXMAN_x8r8g8b8, sse2_composite_over_x888_8_8888,    0 },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8,       PIXMAN_a8r8g8b8, sse2_composite_over_x888_8_8888,    0 },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8,       PIXMAN_x8b8g8r8, sse2_composite_over_x888_8_8888,    0 },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8,       PIXMAN_a8b8g8r8, sse2_composite_over_x888_8_8888,    0 },
#endif
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8,       PIXMAN_a8r8g8b8, sse2_composite_over_x888_n_8888,    NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8,       PIXMAN_x8r8g8b8, sse2_composite_over_x888_n_8888,    NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8,       PIXMAN_a8b8g8r8, sse2_composite_over_x888_n_8888,    NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8,       PIXMAN_x8b8g8r8, sse2_composite_over_x888_n_8888,    NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_a8r8g8b8, PIXMAN_a8,       PIXMAN_a8r8g8b8, sse2_composite_over_8888_n_8888,    NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_a8r8g8b8, PIXMAN_a8,       PIXMAN_x8r8g8b8, sse2_composite_over_8888_n_8888,    NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_a8b8g8r8, PIXMAN_a8,       PIXMAN_a8b8g8r8, sse2_composite_over_8888_n_8888,    NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_a8b8g8r8, PIXMAN_a8,       PIXMAN_x8b8g8r8, sse2_composite_over_8888_n_8888,    NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8r8g8b8, PIXMAN_a8r8g8b8, sse2_composite_over_n_8888_8888_ca, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8r8g8b8, PIXMAN_x8r8g8b8, sse2_composite_over_n_8888_8888_ca, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8b8g8r8, PIXMAN_a8b8g8r8, sse2_composite_over_n_8888_8888_ca, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8b8g8r8, PIXMAN_x8b8g8r8, sse2_composite_over_n_8888_8888_ca, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8r8g8b8, PIXMAN_r5g6b5,   sse2_composite_over_n_8888_0565_ca, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8b8g8r8, PIXMAN_b5g6r5,   sse2_composite_over_n_8888_0565_ca, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8r8g8b8, PIXMAN_a8r8g8b8, sse2_composite_over_pixbuf_8888,    NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8b8g8r8, PIXMAN_a8r8g8b8, sse2_composite_over_pixbuf_8888,    NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8r8g8b8, PIXMAN_x8r8g8b8, sse2_composite_over_pixbuf_8888,    NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8b8g8r8, PIXMAN_x8r8g8b8, sse2_composite_over_pixbuf_8888,    NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8r8g8b8, PIXMAN_a8b8g8r8, sse2_composite_over_pixbuf_8888,    NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8b8g8r8, PIXMAN_a8b8g8r8, sse2_composite_over_pixbuf_8888,    NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8r8g8b8, PIXMAN_x8b8g8r8, sse2_composite_over_pixbuf_8888,    NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8b8g8r8, PIXMAN_x8b8g8r8, sse2_composite_over_pixbuf_8888,    NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8r8g8b8, PIXMAN_r5g6b5,   sse2_composite_over_pixbuf_0565,    NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8b8g8r8, PIXMAN_r5g6b5,   sse2_composite_over_pixbuf_0565,    NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8r8g8b8, PIXMAN_b5g6r5,   sse2_composite_over_pixbuf_0565,    NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8b8g8r8, PIXMAN_b5g6r5,   sse2_composite_over_pixbuf_0565,    NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_null,     PIXMAN_x8r8g8b8, sse2_composite_copy_area,           0 },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_null,     PIXMAN_x8b8g8r8, sse2_composite_copy_area,           0 },

    { PIXMAN_OP_ADD,  PIXMAN_solid,    PIXMAN_a8r8g8b8, PIXMAN_a8r8g8b8, sse2_composite_add_n_8888_8888_ca,  NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_ADD,  PIXMAN_a8,       PIXMAN_null,     PIXMAN_a8,       sse2_composite_add_8000_8000,       0 },
    { PIXMAN_OP_ADD,  PIXMAN_a8r8g8b8, PIXMAN_null,     PIXMAN_a8r8g8b8, sse2_composite_add_8888_8888,       0 },
    { PIXMAN_OP_ADD,  PIXMAN_a8b8g8r8, PIXMAN_null,     PIXMAN_a8b8g8r8, sse2_composite_add_8888_8888,       0 },
    { PIXMAN_OP_ADD,  PIXMAN_solid,    PIXMAN_a8,       PIXMAN_a8,       sse2_composite_add_8888_8_8,        0 },

    { PIXMAN_OP_SRC, PIXMAN_solid,     PIXMAN_a8,       PIXMAN_a8r8g8b8, sse2_composite_src_n_8_8888,        0 },
    { PIXMAN_OP_SRC, PIXMAN_solid,     PIXMAN_a8,       PIXMAN_x8r8g8b8, sse2_composite_src_n_8_8888,        0 },
    { PIXMAN_OP_SRC, PIXMAN_solid,     PIXMAN_a8,       PIXMAN_a8b8g8r8, sse2_composite_src_n_8_8888,        0 },
    { PIXMAN_OP_SRC, PIXMAN_solid,     PIXMAN_a8,       PIXMAN_x8b8g8r8, sse2_composite_src_n_8_8888,        0 },
    { PIXMAN_OP_SRC, PIXMAN_a8r8g8b8,  PIXMAN_null,     PIXMAN_a8r8g8b8, sse2_composite_copy_area,           0 },
    { PIXMAN_OP_SRC, PIXMAN_a8b8g8r8,  PIXMAN_null,     PIXMAN_a8b8g8r8, sse2_composite_copy_area,           0 },
    { PIXMAN_OP_SRC, PIXMAN_a8r8g8b8,  PIXMAN_null,     PIXMAN_x8r8g8b8, sse2_composite_copy_area,           0 },
    { PIXMAN_OP_SRC, PIXMAN_a8b8g8r8,  PIXMAN_null,     PIXMAN_x8b8g8r8, sse2_composite_copy_area,           0 },
    { PIXMAN_OP_SRC, PIXMAN_x8r8g8b8,  PIXMAN_null,     PIXMAN_x8r8g8b8, sse2_composite_copy_area,           0 },
    { PIXMAN_OP_SRC, PIXMAN_x8b8g8r8,  PIXMAN_null,     PIXMAN_x8b8g8r8, sse2_composite_copy_area,           0 },
    { PIXMAN_OP_SRC, PIXMAN_r5g6b5,    PIXMAN_null,     PIXMAN_r5g6b5,   sse2_composite_copy_area,           0 },
    { PIXMAN_OP_SRC, PIXMAN_b5g6r5,    PIXMAN_null,     PIXMAN_b5g6r5,   sse2_composite_copy_area,           0 },

    { PIXMAN_OP_IN,  PIXMAN_a8,        PIXMAN_null,     PIXMAN_a8,       sse2_composite_in_8_8,              0 },
    { PIXMAN_OP_IN,  PIXMAN_solid,     PIXMAN_a8,       PIXMAN_a8,       sse2_composite_in_n_8_8,            0 },
a5844 55
/*
 * Work around GCC bug causing crashes in Mozilla with SSE2
 *
 * When using -msse, gcc generates movdqa instructions assuming that
 * the stack is 16 byte aligned. Unfortunately some applications, such
 * as Mozilla and Mono, end up aligning the stack to 4 bytes, which
 * causes the movdqa instructions to fail.
 *
 * The __force_align_arg_pointer__ makes gcc generate a prologue that
 * realigns the stack pointer to 16 bytes.
 *
 * On x86-64 this is not necessary because the standard ABI already
 * calls for a 16 byte aligned stack.
 *
 * See https://bugs.freedesktop.org/show_bug.cgi?id=15693
 */
#if defined(__GNUC__) && !defined(__x86_64__) && !defined(__amd64__)
__attribute__((__force_align_arg_pointer__))
#endif
static void
sse2_composite (pixman_implementation_t *imp,
                pixman_op_t              op,
                pixman_image_t *         src,
                pixman_image_t *         mask,
                pixman_image_t *         dest,
                int32_t                  src_x,
                int32_t                  src_y,
                int32_t                  mask_x,
                int32_t                  mask_y,
                int32_t                  dest_x,
                int32_t                  dest_y,
                int32_t                  width,
                int32_t                  height)
{
    if (_pixman_run_fast_path (sse2_fast_paths, imp,
                               op, src, mask, dest,
                               src_x, src_y,
                               mask_x, mask_y,
                               dest_x, dest_y,
                               width, height))
    {
	return;
    }

    _pixman_implementation_composite (imp->delegate, op,
                                      src, mask, dest,
                                      src_x, src_y,
                                      mask_x, mask_y,
                                      dest_x, dest_y,
                                      width, height);
}

#if defined(__GNUC__) && !defined(__x86_64__) && !defined(__amd64__)
__attribute__((__force_align_arg_pointer__))
#endif
d5903 6
a5908 2
    pixman_implementation_t *mmx = _pixman_implementation_create_mmx ();
    pixman_implementation_t *imp = _pixman_implementation_create (mmx);
a5965 1
    imp->composite = sse2_composite;
@


1.2
log
@Update to pixman 0.15.8.
@
text
@d26 1
a26 1
 * 
d37 10
d50 1
a50 1
/* -------------------------------------------------------------------------------------------------
d54 21
a74 21
static __m64 xMask0080;
static __m64 xMask00ff;
static __m64 xMask0101;
static __m64 xMaskAlpha;

static __m64 xMask565rgb;
static __m64 xMask565Unpack;

static __m128i Mask0080;
static __m128i Mask00ff;
static __m128i Mask0101;
static __m128i Maskffff;
static __m128i Maskff000000;
static __m128i MaskAlpha;

static __m128i Mask565r;
static __m128i Mask565g1, Mask565g2;
static __m128i Mask565b;
static __m128i MaskRed;
static __m128i MaskGreen;
static __m128i MaskBlue;
d76 2
a77 2
static __m128i Mask565FixRB;
static __m128i Mask565FixG;
d79 1
a79 1
/* -------------------------------------------------------------------------------------------------
d85 1
a85 1
    return _mm_unpacklo_epi8 (_mm_cvtsi32_si128 (data), _mm_setzero_si128());
d89 1
a89 1
unpack_128_2x128 (__m128i data, __m128i* dataLo, __m128i* dataHi)
d91 2
a92 2
    *dataLo = _mm_unpacklo_epi8 (data, _mm_setzero_si128 ());
    *dataHi = _mm_unpackhi_epi8 (data, _mm_setzero_si128 ());
d96 1
a96 1
unpack565to8888 (__m128i lo)
d99 4
a102 4
    
    r = _mm_and_si128 (_mm_slli_epi32 (lo, 8), MaskRed);
    g = _mm_and_si128 (_mm_slli_epi32 (lo, 5), MaskGreen);
    b = _mm_and_si128 (_mm_slli_epi32 (lo, 3), MaskBlue);
d105 1
a105 1
    t  = _mm_and_si128 (rb, Mask565FixRB);
d109 1
a109 1
    t  = _mm_and_si128 (g, Mask565FixG);
d112 1
a112 1
    
d117 5
a121 1
unpack565_128_4x128 (__m128i data, __m128i* data0, __m128i* data1, __m128i* data2, __m128i* data3)
d128 2
a129 2
    lo = unpack565to8888 (lo);
    hi = unpack565to8888 (hi);
d136 1
a136 1
pack565_32_16 (uint32_t pixel)
d138 3
a140 1
    return (uint16_t) (((pixel>>8) & 0xf800) | ((pixel>>5) & 0x07e0) | ((pixel>>3) & 0x001f));
d150 1
a150 1
pack565_2x128_128 (__m128i lo, __m128i hi)
d155 1
a155 1
    data = pack_2x128_128 ( lo, hi );
d157 4
a160 4
    r  = _mm_and_si128 (data , Mask565r);
    g1 = _mm_and_si128 (_mm_slli_epi32 (data , 3), Mask565g1);
    g2 = _mm_and_si128 (_mm_srli_epi32 (data , 5), Mask565g2);
    b  = _mm_and_si128 (_mm_srli_epi32 (data , 3), Mask565b);
d166 1
a166 1
pack565_4x128_128 (__m128i* xmm0, __m128i* xmm1, __m128i* xmm2, __m128i* xmm3)
d168 2
a169 1
    return _mm_packus_epi16 (pack565_2x128_128 (*xmm0, *xmm1), pack565_2x128_128 (*xmm2, *xmm3));
d173 1
a173 1
isOpaque (__m128i x)
d176 1
d181 1
a181 1
isZero (__m128i x)
d183 2
a184 1
    return _mm_movemask_epi8 (_mm_cmpeq_epi8 (x, _mm_setzero_si128())) == 0xffff;
d188 1
a188 1
isTransparent (__m128i x)
d190 2
a191 1
    return (_mm_movemask_epi8 (_mm_cmpeq_epi8 (x, _mm_setzero_si128())) & 0x8888) == 0x8888;
d195 1
a195 1
expandPixel_32_1x128 (uint32_t data)
d197 1
a197 1
    return _mm_shuffle_epi32 (unpack_32_1x128 (data), _MM_SHUFFLE(1, 0, 1, 0));
d201 1
a201 1
expandAlpha_1x128 (__m128i data)
d203 3
a205 1
    return _mm_shufflehi_epi16 (_mm_shufflelo_epi16 (data, _MM_SHUFFLE(3, 3, 3, 3)), _MM_SHUFFLE(3, 3, 3, 3));
d209 4
a212 1
expandAlpha_2x128 (__m128i dataLo, __m128i dataHi, __m128i* alphaLo, __m128i* alphaHi)
d216 5
a220 4
    lo = _mm_shufflelo_epi16 (dataLo, _MM_SHUFFLE(3, 3, 3, 3));
    hi = _mm_shufflelo_epi16 (dataHi, _MM_SHUFFLE(3, 3, 3, 3));
    *alphaLo = _mm_shufflehi_epi16 (lo, _MM_SHUFFLE(3, 3, 3, 3));
    *alphaHi = _mm_shufflehi_epi16 (hi, _MM_SHUFFLE(3, 3, 3, 3));
d224 4
a227 1
expandAlphaRev_2x128 (__m128i dataLo, __m128i dataHi, __m128i* alphaLo, __m128i* alphaHi)
d231 4
a234 4
    lo = _mm_shufflelo_epi16 (dataLo, _MM_SHUFFLE(0, 0, 0, 0));
    hi = _mm_shufflelo_epi16 (dataHi, _MM_SHUFFLE(0, 0, 0, 0));
    *alphaLo = _mm_shufflehi_epi16 (lo, _MM_SHUFFLE(0, 0, 0, 0));
    *alphaHi = _mm_shufflehi_epi16 (hi, _MM_SHUFFLE(0, 0, 0, 0));
d238 6
a243 1
pixMultiply_2x128 (__m128i* dataLo, __m128i* dataHi, __m128i* alphaLo, __m128i* alphaHi, __m128i* retLo, __m128i* retHi)
d247 6
a252 6
    lo = _mm_mullo_epi16 (*dataLo, *alphaLo);
    hi = _mm_mullo_epi16 (*dataHi, *alphaHi);
    lo = _mm_adds_epu16 (lo, Mask0080);
    hi = _mm_adds_epu16 (hi, Mask0080);
    *retLo = _mm_mulhi_epu16 (lo, Mask0101);
    *retHi = _mm_mulhi_epu16 (hi, Mask0101);
d256 10
a265 3
pixAddMultiply_2x128 (__m128i* srcLo, __m128i* srcHi, __m128i* alphaDstLo, __m128i* alphaDstHi,
                      __m128i* dstLo, __m128i* dstHi, __m128i* alphaSrcLo, __m128i* alphaSrcHi,
                      __m128i* retLo, __m128i* retHi)
d267 5
a271 2
    __m128i lo, hi;
    __m128i mulLo, mulHi;
d273 2
a274 10
    lo = _mm_mullo_epi16 (*srcLo, *alphaDstLo);
    hi = _mm_mullo_epi16 (*srcHi, *alphaDstHi);
    mulLo = _mm_mullo_epi16 (*dstLo, *alphaSrcLo);
    mulHi = _mm_mullo_epi16 (*dstHi, *alphaSrcHi);
    lo = _mm_adds_epu16 (lo, Mask0080);
    hi = _mm_adds_epu16 (hi, Mask0080);
    lo = _mm_adds_epu16 (lo, mulLo);
    hi = _mm_adds_epu16 (hi, mulHi);
    *retLo = _mm_mulhi_epu16 (lo, Mask0101);
    *retHi = _mm_mulhi_epu16 (hi, Mask0101);
d278 4
a281 1
negate_2x128 (__m128i dataLo, __m128i dataHi, __m128i* negLo, __m128i* negHi)
d283 2
a284 2
    *negLo = _mm_xor_si128 (dataLo, Mask00ff);
    *negHi = _mm_xor_si128 (dataHi, Mask00ff);
d288 4
a291 1
invertColors_2x128 (__m128i dataLo, __m128i dataHi, __m128i* invLo, __m128i* invHi)
d295 4
a298 4
    lo = _mm_shufflelo_epi16 (dataLo, _MM_SHUFFLE(3, 0, 1, 2));
    hi = _mm_shufflelo_epi16 (dataHi, _MM_SHUFFLE(3, 0, 1, 2));
    *invLo = _mm_shufflehi_epi16 (lo, _MM_SHUFFLE(3, 0, 1, 2));
    *invHi = _mm_shufflehi_epi16 (hi, _MM_SHUFFLE(3, 0, 1, 2));
d302 6
a307 1
over_2x128 (__m128i* srcLo, __m128i* srcHi, __m128i* alphaLo, __m128i* alphaHi, __m128i* dstLo, __m128i* dstHi)
d311 1
a311 1
    negate_2x128 (*alphaLo, *alphaHi, &t1, &t2);
d313 1
a313 1
    pixMultiply_2x128 (dstLo, dstHi, &t1, &t2, dstLo, dstHi);
d315 2
a316 2
    *dstLo = _mm_adds_epu8 (*srcLo, *dstLo);
    *dstHi = _mm_adds_epu8 (*srcHi, *dstHi);
d320 4
a323 1
overRevNonPre_2x128 (__m128i srcLo, __m128i srcHi, __m128i* dstLo, __m128i* dstHi)
d326 1
a326 1
    __m128i alphaLo, alphaHi;
d328 1
a328 1
    expandAlpha_2x128 (srcLo, srcHi, &alphaLo, &alphaHi);
d330 2
a331 2
    lo = _mm_or_si128 (alphaLo, MaskAlpha);
    hi = _mm_or_si128 (alphaHi, MaskAlpha);
d333 1
a333 1
    invertColors_2x128 (srcLo, srcHi, &srcLo, &srcHi);
d335 1
a335 1
    pixMultiply_2x128 (&srcLo, &srcHi, &lo, &hi, &lo, &hi);
d337 1
a337 1
    over_2x128 (&lo, &hi, &alphaLo, &alphaHi, dstLo, dstHi);
d341 8
a348 2
inOver_2x128 (__m128i* srcLo,  __m128i* srcHi,  __m128i*  alphaLo, __m128i*  alphaHi,
              __m128i* maskLo, __m128i* maskHi, __m128i* dstLo,   __m128i* dstHi)
d350 2
a351 2
    __m128i sLo, sHi;
    __m128i aLo, aHi;
d353 2
a354 2
    pixMultiply_2x128 (  srcLo,   srcHi, maskLo, maskHi, &sLo, &sHi);
    pixMultiply_2x128 (alphaLo, alphaHi, maskLo, maskHi, &aLo, &aHi);
d356 1
a356 1
    over_2x128 (&sLo, &sHi, &aLo, &aHi, dstLo, dstHi);
d360 1
a360 1
cachePrefetch (__m128i* addr)
d362 1
a362 1
    _mm_prefetch (addr, _MM_HINT_T0);
d366 1
a366 1
cachePrefetchNext (__m128i* addr)
d368 1
a368 1
    _mm_prefetch (addr + 4, _MM_HINT_T0); // 64 bytes ahead
d373 1
a373 1
load128Aligned (__m128i* src)
d380 1
a380 1
load128Unaligned (const __m128i* src)
d385 3
a387 1
/* save 4 pixels using Write Combining memory on a 16-byte boundary aligned address */
d389 2
a390 1
save128WriteCombining (__m128i* dst, __m128i data)
d397 2
a398 1
save128Aligned (__m128i* dst, __m128i data)
d405 2
a406 1
save128Unaligned (__m128i* dst, __m128i data)
d411 1
a411 1
/* -------------------------------------------------------------------------------------------------
d418 1
a418 1
    return _mm_unpacklo_pi8 (_mm_cvtsi32_si64 (data), _mm_setzero_si64());
d422 1
a422 1
expandAlpha_1x64 (__m64 data)
d424 1
a424 1
    return _mm_shuffle_pi16 (data, _MM_SHUFFLE(3, 3, 3, 3));
d428 1
a428 1
expandAlphaRev_1x64 (__m64 data)
d430 1
a430 1
    return _mm_shuffle_pi16 (data, _MM_SHUFFLE(0, 0, 0, 0));
d434 1
a434 1
expandPixel_8_1x64 (uint8_t data)
d436 2
a437 1
    return _mm_shuffle_pi16 (unpack_32_1x64 ((uint32_t)data), _MM_SHUFFLE(0, 0, 0, 0));
d441 2
a442 1
pixMultiply_1x64 (__m64 data, __m64 alpha)
d445 2
a446 2
                                          xMask0080),
                           xMask0101);
d450 4
a453 1
pixAddMultiply_1x64 (__m64* src, __m64* alphaDst, __m64* dst, __m64* alphaSrc)
d455 4
a458 4
    return _mm_mulhi_pu16 (_mm_adds_pu16 (_mm_adds_pu16 (_mm_mullo_pi16 (*src, *alphaDst),
                                                         xMask0080),
                                          _mm_mullo_pi16 (*dst, *alphaSrc)),
                           xMask0101);
d464 1
a464 1
    return _mm_xor_si64 (data, xMask00ff);
d468 1
a468 1
invertColors_1x64 (__m64 data)
d470 1
a470 1
    return _mm_shuffle_pi16 (data, _MM_SHUFFLE(3, 0, 1, 2));
d476 1
a476 1
    return _mm_adds_pu8 (src, pixMultiply_1x64 (dst, negate_1x64 (alpha)));
d480 1
a480 1
inOver_1x64 (__m64* src, __m64* alpha, __m64* mask, __m64* dst)
d482 2
a483 2
    return over_1x64 (pixMultiply_1x64 (*src, *mask),
                      pixMultiply_1x64 (*alpha, *mask),
d488 1
a488 1
overRevNonPre_1x64 (__m64 src, __m64 dst)
d490 1
a490 1
    __m64 alpha = expandAlpha_1x64 (src);
d492 2
a493 2
    return over_1x64 (pixMultiply_1x64 (invertColors_1x64 (src),
                                        _mm_or_si64 (alpha, xMaskAlpha)),
d499 1
a499 1
pack_1x64_32( __m64 data )
d501 1
a501 1
    return _mm_cvtsi64_si32 (_mm_packs_pu16 (data, _mm_setzero_si64()));
d531 2
a532 2
    p = _mm_and_si64 (p, xMask565rgb);
    p = _mm_mullo_pi16 (p, xMask565Unpack);
d537 1
a537 1
/* -------------------------------------------------------------------------------------------------
d541 1
a541 1
coreCombineOverUPixelsse2 (uint32_t src, uint32_t dst)
d543 2
a544 2
    uint8_t     a;
    __m64       ms;
d550 1
a550 1
        return src;
d554 3
a556 2
        ms = unpack_32_1x64 (src);
        return pack_1x64_32 (over_1x64 (ms, expandAlpha_1x64 (ms), unpack_32_1x64 (dst)));
d572 2
a573 2
	mm = expandAlpha_1x64 (mm);
	
d575 1
a575 1
	ms = pixMultiply_1x64 (ms, mm);
d586 2
a587 2
    __m128i xmmSrcLo, xmmSrcHi;
    __m128i xmmMskLo, xmmMskHi;
d589 1
a589 1
    
d592 1
a592 1
	xmmMskLo = load128Unaligned (pm);
d594 1
a594 1
	if (isTransparent (xmmMskLo))
d597 3
a599 3
    
    s = load128Unaligned (ps);
	
d602 10
a611 8
	unpack_128_2x128 (s, &xmmSrcLo, &xmmSrcHi);
	unpack_128_2x128 (xmmMskLo, &xmmMskLo, &xmmMskHi);
	
	expandAlpha_2x128 (xmmMskLo, xmmMskHi, &xmmMskLo, &xmmMskHi);
	
	pixMultiply_2x128 (&xmmSrcLo, &xmmSrcHi, &xmmMskLo, &xmmMskHi, &xmmSrcLo, &xmmSrcHi);
	
	s = pack_2x128_128 (xmmSrcLo, xmmSrcHi);
d618 4
a621 1
coreCombineOverUsse2 (uint32_t* pd, const uint32_t* ps, const uint32_t* pm, int w)
d625 3
a627 3
    __m128i xmmDstLo, xmmDstHi;
    __m128i xmmSrcLo, xmmSrcHi;
    __m128i xmmAlphaLo, xmmAlphaHi;
d630 3
a632 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d635 1
a635 2
    while (w &&
           ((unsigned long)pd & 15))
d637 2
a638 2
        d = *pd;
        s = combine1 (ps, pm);
d640 1
a640 1
        *pd++ = coreCombineOverUPixelsse2 (s, d);
d644 1
a644 1
        w--;
d648 3
a650 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d654 36
a689 30
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
	cachePrefetchNext ((__m128i*)pm);

        /* I'm loading unaligned because I'm not sure about the address alignment. */
        xmmSrcHi = combine4 ((__m128i*)ps, (__m128i*)pm);

        if (isOpaque (xmmSrcHi))
        {
            save128Aligned ((__m128i*)pd, xmmSrcHi);
        }
        else if (!isZero (xmmSrcHi))
        {
            xmmDstHi = load128Aligned ((__m128i*) pd);

            unpack_128_2x128 (xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
            unpack_128_2x128 (xmmDstHi, &xmmDstLo, &xmmDstHi);

            expandAlpha_2x128 (xmmSrcLo, xmmSrcHi, &xmmAlphaLo, &xmmAlphaHi);

            over_2x128 (&xmmSrcLo, &xmmSrcHi, &xmmAlphaLo, &xmmAlphaHi, &xmmDstLo, &xmmDstHi);

            /* rebuid the 4 pixel data and save*/
            save128Aligned ((__m128i*)pd, pack_2x128_128 (xmmDstLo, xmmDstHi));
        }

        w -= 4;
        ps += 4;
        pd += 4;
d696 2
a697 2
        d = *pd;
        s = combine1 (ps, pm);
d699 1
a699 1
        *pd++ = coreCombineOverUPixelsse2 (s, d);
d703 2
a704 1
        w--;
d709 4
a712 1
coreCombineOverReverseUsse2 (uint32_t* pd, const uint32_t* ps, const uint32_t* pm, int w)
d716 3
a718 3
    __m128i xmmDstLo, xmmDstHi;
    __m128i xmmSrcLo, xmmSrcHi;
    __m128i xmmAlphaLo, xmmAlphaHi;
d721 3
a723 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d729 2
a730 2
        d = *pd;
        s = combine1 (ps, pm);
d732 2
a733 2
        *pd++ = coreCombineOverUPixelsse2 (d, s);
        w--;
d740 3
a742 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d746 28
a773 13
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
	cachePrefetchNext ((__m128i*)pm);

        /* I'm loading unaligned because I'm not sure about the address alignment. */
        xmmSrcHi = combine4 ((__m128i*)ps, (__m128i*)pm);
        xmmDstHi = load128Aligned ((__m128i*) pd);

        unpack_128_2x128 (xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
        unpack_128_2x128 (xmmDstHi, &xmmDstLo, &xmmDstHi);

        expandAlpha_2x128 (xmmDstLo, xmmDstHi, &xmmAlphaLo, &xmmAlphaHi);
a774 8
        over_2x128 (&xmmDstLo, &xmmDstHi, &xmmAlphaLo, &xmmAlphaHi, &xmmSrcLo, &xmmSrcHi);

        /* rebuid the 4 pixel data and save*/
        save128Aligned ((__m128i*)pd, pack_2x128_128 (xmmSrcLo, xmmSrcHi));

        w -= 4;
        ps += 4;
        pd += 4;
d781 2
a782 2
        d = *pd;
        s = combine1 (ps, pm);
d784 1
a784 1
        *pd++ = coreCombineOverUPixelsse2 (d, s);
d786 1
a786 1
        w--;
d793 1
a793 1
coreCombineInUPixelsse2 (uint32_t src, uint32_t dst)
d799 1
a799 1
        return 0;
d803 3
a805 1
        return pack_1x64_32(pixMultiply_1x64 (unpack_32_1x64 (dst), expandAlpha_1x64 (unpack_32_1x64 (src))));
d812 4
a815 1
coreCombineInUsse2 (uint32_t* pd, const uint32_t* ps, const uint32_t* pm, int w)
d819 2
a820 2
    __m128i xmmSrcLo, xmmSrcHi;
    __m128i xmmDstLo, xmmDstHi;
d823 3
a825 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d829 2
a830 2
        s = combine1 (ps, pm);
        d = *pd;
d832 2
a833 2
        *pd++ = coreCombineInUPixelsse2 (d, s);
        w--;
d840 3
a842 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d846 22
a867 19
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
	cachePrefetchNext ((__m128i*)pm);

        xmmDstHi = load128Aligned ((__m128i*) pd);
        xmmSrcHi = combine4 ((__m128i*) ps, (__m128i*) pm);

        unpack_128_2x128 (xmmDstHi, &xmmDstLo, &xmmDstHi);
        expandAlpha_2x128 (xmmDstLo, xmmDstHi, &xmmDstLo, &xmmDstHi);

        unpack_128_2x128 (xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
        pixMultiply_2x128 (&xmmSrcLo, &xmmSrcHi, &xmmDstLo, &xmmDstHi, &xmmDstLo, &xmmDstHi);

        save128Aligned ((__m128i*)pd, pack_2x128_128 (xmmDstLo, xmmDstHi));

        ps += 4;
        pd += 4;
        w -= 4;
d874 2
a875 2
        s = combine1 (ps, pm);
        d = *pd;
d877 2
a878 2
        *pd++ = coreCombineInUPixelsse2 (d, s);
        w--;
d886 4
a889 1
coreCombineReverseInUsse2 (uint32_t* pd, const uint32_t* ps, const uint32_t *pm, int w)
d893 2
a894 2
    __m128i xmmSrcLo, xmmSrcHi;
    __m128i xmmDstLo, xmmDstHi;
d897 3
a899 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d903 2
a904 2
        s = combine1 (ps, pm);
        d = *pd;
d906 1
a906 1
        *pd++ = coreCombineInUPixelsse2 (s, d);
d908 1
a908 1
        w--;
d914 3
a916 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d920 22
a941 19
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
	cachePrefetchNext ((__m128i*)pm);

        xmmDstHi = load128Aligned ((__m128i*) pd);
        xmmSrcHi = combine4 ((__m128i*) ps, (__m128i*)pm);

        unpack_128_2x128 (xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
        expandAlpha_2x128 (xmmSrcLo, xmmSrcHi, &xmmSrcLo, &xmmSrcHi);

        unpack_128_2x128 (xmmDstHi, &xmmDstLo, &xmmDstHi);
        pixMultiply_2x128 (&xmmDstLo, &xmmDstHi, &xmmSrcLo, &xmmSrcHi, &xmmDstLo, &xmmDstHi);

        save128Aligned ((__m128i*)pd, pack_2x128_128 (xmmDstLo, xmmDstHi));

        ps += 4;
        pd += 4;
        w -= 4;
d948 2
a949 2
        s = combine1 (ps, pm);
        d = *pd;
d951 2
a952 2
        *pd++ = coreCombineInUPixelsse2 (s, d);
        w--;
d960 4
a963 1
coreCombineReverseOutUsse2 (uint32_t* pd, const uint32_t* ps, const uint32_t* pm, int w)
d966 3
a968 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d972 2
a973 2
        uint32_t s = combine1 (ps, pm);
        uint32_t d = *pd;
d975 5
a979 1
        *pd++ = pack_1x64_32 (pixMultiply_1x64 (unpack_32_1x64 (d), negate_1x64 (expandAlpha_1x64 (unpack_32_1x64 (s)))));
d983 1
a983 1
        w--;
d987 3
a989 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d993 2
a994 2
        __m128i xmmSrcLo, xmmSrcHi;
        __m128i xmmDstLo, xmmDstHi;
d996 4
a999 4
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
	cachePrefetchNext ((__m128i*)pm);
d1001 2
a1002 2
        xmmSrcHi = combine4 ((__m128i*)ps, (__m128i*)pm);
        xmmDstHi = load128Aligned ((__m128i*) pd);
d1004 2
a1005 2
        unpack_128_2x128 (xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
        unpack_128_2x128 (xmmDstHi, &xmmDstLo, &xmmDstHi);
d1007 2
a1008 2
        expandAlpha_2x128 (xmmSrcLo, xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
        negate_2x128      (xmmSrcLo, xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
d1010 3
a1012 1
        pixMultiply_2x128 (&xmmDstLo, &xmmDstHi, &xmmSrcLo, &xmmSrcHi, &xmmDstLo, &xmmDstHi);
d1014 2
a1015 1
        save128Aligned ((__m128i*)pd, pack_2x128_128 (xmmDstLo, xmmDstHi));
d1017 2
a1018 2
        ps += 4;
        pd += 4;
d1021 2
a1022 1
        w -= 4;
d1027 2
a1028 2
        uint32_t s = combine1 (ps, pm);
        uint32_t d = *pd;
d1030 4
a1033 1
        *pd++ = pack_1x64_32 (pixMultiply_1x64 (unpack_32_1x64 (d), negate_1x64 (expandAlpha_1x64 (unpack_32_1x64 (s)))));
d1037 1
a1037 1
        w--;
d1042 4
a1045 1
coreCombineOutUsse2 (uint32_t* pd, const uint32_t* ps, const uint32_t* pm, int w)
d1048 3
a1050 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1054 2
a1055 2
        uint32_t s = combine1 (ps, pm);
        uint32_t d = *pd;
d1057 5
a1061 2
        *pd++ = pack_1x64_32 (pixMultiply_1x64 (unpack_32_1x64 (s), negate_1x64 (expandAlpha_1x64 (unpack_32_1x64 (d)))));
        w--;
d1068 3
a1070 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1074 2
a1075 2
        __m128i xmmSrcLo, xmmSrcHi;
        __m128i xmmDstLo, xmmDstHi;
d1077 4
a1080 4
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
	cachePrefetchNext ((__m128i*)pm);
d1082 2
a1083 2
        xmmSrcHi = combine4 ((__m128i*) ps, (__m128i*)pm);
        xmmDstHi = load128Aligned ((__m128i*) pd);
d1085 2
a1086 2
        unpack_128_2x128 (xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
        unpack_128_2x128 (xmmDstHi, &xmmDstLo, &xmmDstHi);
d1088 2
a1089 2
        expandAlpha_2x128 (xmmDstLo, xmmDstHi, &xmmDstLo, &xmmDstHi);
        negate_2x128      (xmmDstLo, xmmDstHi, &xmmDstLo, &xmmDstHi);
d1091 3
a1093 1
        pixMultiply_2x128 (&xmmSrcLo, &xmmSrcHi, &xmmDstLo, &xmmDstHi, &xmmDstLo, &xmmDstHi);
d1095 2
a1096 1
        save128Aligned ((__m128i*)pd, pack_2x128_128 (xmmDstLo, xmmDstHi));
d1098 3
a1100 3
        ps += 4;
        pd += 4;
        w -= 4;
d1107 2
a1108 2
        uint32_t s = combine1 (ps, pm);
        uint32_t d = *pd;
d1110 5
a1114 2
        *pd++ = pack_1x64_32 (pixMultiply_1x64 (unpack_32_1x64 (s), negate_1x64 (expandAlpha_1x64 (unpack_32_1x64 (d)))));
        w--;
d1122 2
a1123 1
coreCombineAtopUPixelsse2 (uint32_t src, uint32_t dst)
d1128 2
a1129 2
    __m64 sa = negate_1x64 (expandAlpha_1x64 (s));
    __m64 da = expandAlpha_1x64 (d);
d1131 1
a1131 1
    return pack_1x64_32 (pixAddMultiply_1x64 (&s, &da, &d, &sa));
d1135 4
a1138 1
coreCombineAtopUsse2 (uint32_t* pd, const uint32_t* ps, const uint32_t* pm, int w)
d1142 4
a1145 4
    __m128i xmmSrcLo, xmmSrcHi;
    __m128i xmmDstLo, xmmDstHi;
    __m128i xmmAlphaSrcLo, xmmAlphaSrcHi;
    __m128i xmmAlphaDstLo, xmmAlphaDstHi;
d1148 3
a1150 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1154 2
a1155 2
        s = combine1 (ps, pm);
        d = *pd;
d1157 2
a1158 2
        *pd++ = coreCombineAtopUPixelsse2 (s, d);
        w--;
d1165 3
a1167 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1171 30
a1200 25
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
	cachePrefetchNext ((__m128i*)pm);

        xmmSrcHi = combine4 ((__m128i*)ps, (__m128i*)pm);
        xmmDstHi = load128Aligned ((__m128i*) pd);

        unpack_128_2x128 (xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
        unpack_128_2x128 (xmmDstHi, &xmmDstLo, &xmmDstHi);

        expandAlpha_2x128 (xmmSrcLo, xmmSrcHi, &xmmAlphaSrcLo, &xmmAlphaSrcHi);
        expandAlpha_2x128 (xmmDstLo, xmmDstHi, &xmmAlphaDstLo, &xmmAlphaDstHi);

        negate_2x128 (xmmAlphaSrcLo, xmmAlphaSrcHi, &xmmAlphaSrcLo, &xmmAlphaSrcHi);

        pixAddMultiply_2x128 ( &xmmSrcLo, &xmmSrcHi, &xmmAlphaDstLo, &xmmAlphaDstHi,
                               &xmmDstLo, &xmmDstHi, &xmmAlphaSrcLo, &xmmAlphaSrcHi,
                               &xmmDstLo, &xmmDstHi );

        save128Aligned ((__m128i*)pd, pack_2x128_128 (xmmDstLo, xmmDstHi));

        ps += 4;
        pd += 4;
        w -= 4;
d1207 2
a1208 2
        s = combine1 (ps, pm);
        d = *pd;
d1210 2
a1211 2
        *pd++ = coreCombineAtopUPixelsse2 (s, d);
        w--;
d1219 2
a1220 1
coreCombineReverseAtopUPixelsse2 (uint32_t src, uint32_t dst)
d1225 2
a1226 2
    __m64 sa = expandAlpha_1x64 (s);
    __m64 da = negate_1x64 (expandAlpha_1x64 (d));
d1228 1
a1228 1
    return pack_1x64_32 (pixAddMultiply_1x64 (&s, &da, &d, &sa));
d1232 4
a1235 1
coreCombineReverseAtopUsse2 (uint32_t* pd, const uint32_t* ps, const uint32_t* pm, int w)
d1239 4
a1242 4
    __m128i xmmSrcLo, xmmSrcHi;
    __m128i xmmDstLo, xmmDstHi;
    __m128i xmmAlphaSrcLo, xmmAlphaSrcHi;
    __m128i xmmAlphaDstLo, xmmAlphaDstHi;
d1245 3
a1247 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1251 2
a1252 2
        s = combine1 (ps, pm);
        d = *pd;
d1254 1
a1254 1
        *pd++ = coreCombineReverseAtopUPixelsse2 (s, d);
d1256 1
a1256 1
        w--;
d1262 3
a1264 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1268 30
a1297 25
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
	cachePrefetchNext ((__m128i*)pm);

        xmmSrcHi = combine4 ((__m128i*)ps, (__m128i*)pm);
        xmmDstHi = load128Aligned ((__m128i*) pd);

        unpack_128_2x128 (xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
        unpack_128_2x128 (xmmDstHi, &xmmDstLo, &xmmDstHi);

        expandAlpha_2x128 (xmmSrcLo, xmmSrcHi, &xmmAlphaSrcLo, &xmmAlphaSrcHi);
        expandAlpha_2x128 (xmmDstLo, xmmDstHi, &xmmAlphaDstLo, &xmmAlphaDstHi);

        negate_2x128 (xmmAlphaDstLo, xmmAlphaDstHi, &xmmAlphaDstLo, &xmmAlphaDstHi);

        pixAddMultiply_2x128 ( &xmmSrcLo, &xmmSrcHi, &xmmAlphaDstLo, &xmmAlphaDstHi,
                               &xmmDstLo, &xmmDstHi, &xmmAlphaSrcLo, &xmmAlphaSrcHi,
                               &xmmDstLo, &xmmDstHi );

        save128Aligned ((__m128i*)pd, pack_2x128_128 (xmmDstLo, xmmDstHi));

        ps += 4;
        pd += 4;
        w -= 4;
d1304 2
a1305 2
        s = combine1 (ps, pm);
        d = *pd;
d1307 1
a1307 1
        *pd++ = coreCombineReverseAtopUPixelsse2 (s, d);
d1309 1
a1309 1
        w--;
d1316 2
a1317 1
coreCombineXorUPixelsse2 (uint32_t src, uint32_t dst)
d1322 2
a1323 2
    __m64 negD = negate_1x64 (expandAlpha_1x64 (d));
    __m64 negS = negate_1x64 (expandAlpha_1x64 (s));
d1325 1
a1325 1
    return pack_1x64_32 (pixAddMultiply_1x64 (&s, &negD, &d, &negS));
d1329 4
a1332 1
coreCombineXorUsse2 (uint32_t* dst, const uint32_t* src, const uint32_t *mask, int width)
d1339 5
a1343 5
    
    __m128i xmmSrc, xmmSrcLo, xmmSrcHi;
    __m128i xmmDst, xmmDstLo, xmmDstHi;
    __m128i xmmAlphaSrcLo, xmmAlphaSrcHi;
    __m128i xmmAlphaDstLo, xmmAlphaDstHi;
d1346 3
a1348 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1352 2
a1353 2
        s = combine1 (ps, pm);
        d = *pd;
d1355 2
a1356 2
        *pd++ = coreCombineXorUPixelsse2 (s, d);
        w--;
d1363 3
a1365 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1369 32
a1400 26
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
	cachePrefetchNext ((__m128i*)pm);

        xmmSrc = combine4 ((__m128i*) ps, (__m128i*) pm);
        xmmDst = load128Aligned ((__m128i*) pd);

        unpack_128_2x128 (xmmSrc, &xmmSrcLo, &xmmSrcHi);
        unpack_128_2x128 (xmmDst, &xmmDstLo, &xmmDstHi);

        expandAlpha_2x128 (xmmSrcLo, xmmSrcHi, &xmmAlphaSrcLo, &xmmAlphaSrcHi);
        expandAlpha_2x128 (xmmDstLo, xmmDstHi, &xmmAlphaDstLo, &xmmAlphaDstHi);

        negate_2x128 (xmmAlphaSrcLo, xmmAlphaSrcHi, &xmmAlphaSrcLo, &xmmAlphaSrcHi);
        negate_2x128 (xmmAlphaDstLo, xmmAlphaDstHi, &xmmAlphaDstLo, &xmmAlphaDstHi);

        pixAddMultiply_2x128 ( &xmmSrcLo, &xmmSrcHi, &xmmAlphaDstLo, &xmmAlphaDstHi,
                               &xmmDstLo, &xmmDstHi, &xmmAlphaSrcLo, &xmmAlphaSrcHi,
                               &xmmDstLo, &xmmDstHi );

        save128Aligned ((__m128i*)pd, pack_2x128_128 (xmmDstLo, xmmDstHi));

        ps += 4;
        pd += 4;
        w -= 4;
d1407 2
a1408 2
        s = combine1 (ps, pm);
        d = *pd;
d1410 2
a1411 2
        *pd++ = coreCombineXorUPixelsse2 (s, d);
        w--;
d1419 4
a1422 1
coreCombineAddUsse2 (uint32_t* dst, const uint32_t* src, const uint32_t* mask, int width)
d1425 1
a1425 1
    uint32_t s,d;
d1431 3
a1433 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1437 3
a1439 2
        s = combine1 (ps, pm);
        d = *pd;
d1443 3
a1445 2
        *pd++ = _mm_cvtsi64_si32 (_mm_adds_pu8 (_mm_cvtsi32_si64 (s), _mm_cvtsi32_si64 (d)));
        w--;
d1449 3
a1451 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
a1455 5
	
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
	cachePrefetchNext ((__m128i*)pm);
d1457 12
a1468 6
	s = combine4((__m128i*)ps,(__m128i*)pm);
	
        save128Aligned( (__m128i*)pd,
                        _mm_adds_epu8( s, load128Aligned  ((__m128i*)pd)) );
        pd += 4;
        ps += 4;
d1471 1
a1471 1
        w -= 4;
d1476 3
a1478 2
        s = combine1 (ps, pm);
        d = *pd;
d1480 2
a1481 1
        *pd++ = _mm_cvtsi64_si32 (_mm_adds_pu8 (_mm_cvtsi32_si64 (s), _mm_cvtsi32_si64 (d)));
d1488 2
a1489 1
coreCombineSaturateUPixelsse2 (uint32_t src, uint32_t dst)
d1498 2
a1499 1
        ms = pixMultiply_1x64 (ms, expandAlpha_1x64 (unpack_32_1x64 (FbIntDiv(da, sa) << 24)));
d1506 4
a1509 1
coreCombineSaturateUsse2 (uint32_t *pd, const uint32_t *ps, const uint32_t *pm, int w)
d1511 1
a1511 1
    uint32_t s,d;
d1513 2
a1514 2
    uint32_t packCmp;
    __m128i xmmSrc, xmmDst;
d1517 3
a1519 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1523 5
a1527 4
        s = combine1 (ps, pm);
        d = *pd;
        *pd++ = coreCombineSaturateUPixelsse2 (s, d);
        w--;
d1534 3
a1536 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1540 19
a1558 17
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
	cachePrefetchNext ((__m128i*)pm);

        xmmDst = load128Aligned  ((__m128i*)pd);
        xmmSrc = combine4 ((__m128i*)ps, (__m128i*)pm);

        packCmp = _mm_movemask_epi8 (_mm_cmpgt_epi32 (_mm_srli_epi32 (xmmSrc, 24),
                                                      _mm_srli_epi32 (_mm_xor_si128 (xmmDst, Maskff000000), 24)));

        /* if some alpha src is grater than respective ~alpha dst */
        if (packCmp)
        {
            s = combine1 (ps++, pm);
            d = *pd;
            *pd++ = coreCombineSaturateUPixelsse2 (s, d);
d1562 3
a1564 3
            s = combine1 (ps++, pm);
            d = *pd;
            *pd++ = coreCombineSaturateUPixelsse2 (s, d);
d1568 3
a1570 3
            s = combine1 (ps++, pm);
            d = *pd;
            *pd++ = coreCombineSaturateUPixelsse2 (s, d);
d1574 3
a1576 3
            s = combine1 (ps++, pm);
            d = *pd;
            *pd++ = coreCombineSaturateUPixelsse2 (s, d);
d1579 4
a1582 4
        }
        else
        {
            save128Aligned ((__m128i*)pd, _mm_adds_epu8 (xmmDst, xmmSrc));
d1584 2
a1585 2
            pd += 4;
            ps += 4;
d1588 1
a1588 1
        }
d1590 1
a1590 1
        w -= 4;
d1595 4
a1598 3
        s = combine1 (ps, pm);
        d = *pd;
        *pd++ = coreCombineSaturateUPixelsse2 (s, d);
d1606 4
a1609 1
coreCombineSrcCsse2 (uint32_t* pd, const uint32_t* ps, const uint32_t *pm, int w)
d1613 3
a1615 3
    __m128i xmmSrcLo, xmmSrcHi;
    __m128i xmmMaskLo, xmmMaskHi;
    __m128i xmmDstLo, xmmDstHi;
d1618 3
a1620 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1624 5
a1628 4
        s = *ps++;
        m = *pm++;
        *pd++ = pack_1x64_32 (pixMultiply_1x64 (unpack_32_1x64 (s), unpack_32_1x64 (m)));
        w--;
d1632 3
a1634 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1638 22
a1659 19
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
        cachePrefetchNext ((__m128i*)pm);

        xmmSrcHi = load128Unaligned ((__m128i*)ps);
        xmmMaskHi = load128Unaligned ((__m128i*)pm);

        unpack_128_2x128 (xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
        unpack_128_2x128 (xmmMaskHi, &xmmMaskLo, &xmmMaskHi);

        pixMultiply_2x128 (&xmmSrcLo, &xmmSrcHi, &xmmMaskLo, &xmmMaskHi, &xmmDstLo, &xmmDstHi);

        save128Aligned( (__m128i*)pd, pack_2x128_128 (xmmDstLo, xmmDstHi));

        ps += 4;
        pd += 4;
        pm += 4;
        w -= 4;
d1664 5
a1668 4
        s = *ps++;
        m = *pm++;
        *pd++ = pack_1x64_32 (pixMultiply_1x64 (unpack_32_1x64 (s), unpack_32_1x64 (m)));
        w--;
d1673 3
a1675 1
coreCombineOverCPixelsse2 (uint32_t src, uint32_t mask, uint32_t dst)
d1678 3
a1680 3
    __m64 expAlpha = expandAlpha_1x64 (s);
    __m64 unpkMask = unpack_32_1x64 (mask);
    __m64 unpkDst  = unpack_32_1x64 (dst);
d1682 1
a1682 1
    return pack_1x64_32 (inOver_1x64 (&s, &expAlpha, &unpkMask, &unpkDst));
d1686 4
a1689 1
coreCombineOverCsse2 (uint32_t* pd, const uint32_t* ps, const uint32_t *pm, int w)
d1693 4
a1696 4
    __m128i xmmAlphaLo, xmmAlphaHi;
    __m128i xmmSrcLo, xmmSrcHi;
    __m128i xmmDstLo, xmmDstHi;
    __m128i xmmMaskLo, xmmMaskHi;
d1699 3
a1701 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1705 3
a1707 3
        s = *ps++;
        m = *pm++;
        d = *pd;
d1709 2
a1710 2
        *pd++ = coreCombineOverCPixelsse2 (s, m, d);
        w--;
d1714 3
a1716 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1720 28
a1747 23
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
        cachePrefetchNext ((__m128i*)pm);

        xmmDstHi = load128Aligned ((__m128i*)pd);
        xmmSrcHi = load128Unaligned ((__m128i*)ps);
        xmmMaskHi = load128Unaligned ((__m128i*)pm);

        unpack_128_2x128 (xmmDstHi, &xmmDstLo, &xmmDstHi);
        unpack_128_2x128 (xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
        unpack_128_2x128 (xmmMaskHi, &xmmMaskLo, &xmmMaskHi);

        expandAlpha_2x128 (xmmSrcLo, xmmSrcHi, &xmmAlphaLo, &xmmAlphaHi);

        inOver_2x128 (&xmmSrcLo, &xmmSrcHi, &xmmAlphaLo, &xmmAlphaHi, &xmmMaskLo, &xmmMaskHi, &xmmDstLo, &xmmDstHi);

        save128Aligned( (__m128i*)pd, pack_2x128_128 (xmmDstLo, xmmDstHi));

        ps += 4;
        pd += 4;
        pm += 4;
        w -= 4;
d1752 3
a1754 3
        s = *ps++;
        m = *pm++;
        d = *pd;
d1756 2
a1757 2
        *pd++ = coreCombineOverCPixelsse2 (s, m, d);
        w--;
d1762 3
a1764 1
coreCombineOverReverseCPixelsse2 (uint32_t src, uint32_t mask, uint32_t dst)
d1768 4
a1771 1
	return pack_1x64_32(over_1x64 (d, expandAlpha_1x64 (d), pixMultiply_1x64 (unpack_32_1x64 (src), unpack_32_1x64 (mask))));
d1775 4
a1778 1
coreCombineOverReverseCsse2 (uint32_t* pd, const uint32_t* ps, const uint32_t *pm, int w)
d1782 4
a1785 4
    __m128i xmmAlphaLo, xmmAlphaHi;
    __m128i xmmSrcLo, xmmSrcHi;
    __m128i xmmDstLo, xmmDstHi;
    __m128i xmmMaskLo, xmmMaskHi;
d1788 3
a1790 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1794 3
a1796 3
        s = *ps++;
        m = *pm++;
        d = *pd;
d1798 2
a1799 2
        *pd++ = coreCombineOverReverseCPixelsse2 (s, m, d);
        w--;
d1803 3
a1805 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1809 30
a1838 24
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
        cachePrefetchNext ((__m128i*)pm);

        xmmDstHi = load128Aligned ((__m128i*)pd);
        xmmSrcHi = load128Unaligned ((__m128i*)ps);
        xmmMaskHi = load128Unaligned ((__m128i*)pm);

        unpack_128_2x128 (xmmDstHi, &xmmDstLo, &xmmDstHi);
        unpack_128_2x128 (xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
        unpack_128_2x128 (xmmMaskHi, &xmmMaskLo, &xmmMaskHi);

        expandAlpha_2x128 (xmmDstLo, xmmDstHi, &xmmAlphaLo, &xmmAlphaHi);
        pixMultiply_2x128 (&xmmSrcLo, &xmmSrcHi, &xmmMaskLo, &xmmMaskHi, &xmmMaskLo, &xmmMaskHi);

        over_2x128 (&xmmDstLo, &xmmDstHi, &xmmAlphaLo, &xmmAlphaHi, &xmmMaskLo, &xmmMaskHi);

        save128Aligned( (__m128i*)pd, pack_2x128_128 (xmmMaskLo, xmmMaskHi));

        ps += 4;
        pd += 4;
        pm += 4;
        w -= 4;
d1843 3
a1845 3
        s = *ps++;
        m = *pm++;
        d = *pd;
d1847 2
a1848 2
        *pd++ = coreCombineOverReverseCPixelsse2 (s, m, d);
        w--;
d1853 4
a1856 1
coreCombineInCsse2 (uint32_t *pd, const uint32_t *ps, const uint32_t *pm, int w)
d1860 4
a1863 4
    __m128i xmmAlphaLo, xmmAlphaHi;
    __m128i xmmSrcLo, xmmSrcHi;
    __m128i xmmDstLo, xmmDstHi;
    __m128i xmmMaskLo, xmmMaskHi;
d1866 3
a1868 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1872 3
a1874 3
        s = *ps++;
        m = *pm++;
        d = *pd;
d1876 6
a1881 3
        *pd++ = pack_1x64_32 (pixMultiply_1x64 (pixMultiply_1x64 (unpack_32_1x64 (s), unpack_32_1x64 (m)),
                                                expandAlpha_1x64 (unpack_32_1x64 (d))));
        w--;
d1885 3
a1887 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1891 31
a1921 24
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
        cachePrefetchNext ((__m128i*)pm);

        xmmDstHi = load128Aligned ((__m128i*)pd);
        xmmSrcHi = load128Unaligned ((__m128i*)ps);
        xmmMaskHi = load128Unaligned ((__m128i*)pm);

        unpack_128_2x128 (xmmDstHi, &xmmDstLo, &xmmDstHi);
        unpack_128_2x128 (xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
        unpack_128_2x128 (xmmMaskHi, &xmmMaskLo, &xmmMaskHi);

        expandAlpha_2x128 (xmmDstLo, xmmDstHi, &xmmAlphaLo, &xmmAlphaHi);
        pixMultiply_2x128 (&xmmSrcLo, &xmmSrcHi, &xmmMaskLo, &xmmMaskHi, &xmmDstLo, &xmmDstHi);

        pixMultiply_2x128 (&xmmDstLo, &xmmDstHi, &xmmAlphaLo, &xmmAlphaHi, &xmmDstLo, &xmmDstHi);

        save128Aligned( (__m128i*)pd, pack_2x128_128 (xmmDstLo, xmmDstHi));

        ps += 4;
        pd += 4;
        pm += 4;
        w -= 4;
d1926 11
a1936 7
        s = *ps++;
        m = *pm++;
        d = *pd;

        *pd++ = pack_1x64_32 (pixMultiply_1x64 (pixMultiply_1x64 (unpack_32_1x64 (s), unpack_32_1x64 (m)),
                                                expandAlpha_1x64 (unpack_32_1x64 (d))));
        w--;
d1941 4
a1944 1
coreCombineInReverseCsse2 (uint32_t *pd, const uint32_t *ps, const uint32_t *pm, int w)
d1948 4
a1951 4
    __m128i xmmAlphaLo, xmmAlphaHi;
    __m128i xmmSrcLo, xmmSrcHi;
    __m128i xmmDstLo, xmmDstHi;
    __m128i xmmMaskLo, xmmMaskHi;
d1954 3
a1956 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1960 10
a1969 8
        s = *ps++;
        m = *pm++;
        d = *pd;

        *pd++ = pack_1x64_32 (pixMultiply_1x64 (unpack_32_1x64 (d),
                                                pixMultiply_1x64 (unpack_32_1x64 (m),
                                                                  expandAlpha_1x64 (unpack_32_1x64 (s)))));
        w--;
d1973 3
a1975 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d1979 30
a2008 24
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
        cachePrefetchNext ((__m128i*)pm);

        xmmDstHi = load128Aligned ((__m128i*)pd);
        xmmSrcHi = load128Unaligned ((__m128i*)ps);
        xmmMaskHi = load128Unaligned ((__m128i*)pm);

        unpack_128_2x128 (xmmDstHi, &xmmDstLo, &xmmDstHi);
        unpack_128_2x128 (xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
        unpack_128_2x128 (xmmMaskHi, &xmmMaskLo, &xmmMaskHi);

        expandAlpha_2x128 (xmmSrcLo, xmmSrcHi, &xmmAlphaLo, &xmmAlphaHi);
        pixMultiply_2x128 (&xmmMaskLo, &xmmMaskHi, &xmmAlphaLo, &xmmAlphaHi, &xmmAlphaLo, &xmmAlphaHi);

        pixMultiply_2x128 (&xmmDstLo, &xmmDstHi, &xmmAlphaLo, &xmmAlphaHi, &xmmDstLo, &xmmDstHi);

        save128Aligned( (__m128i*)pd, pack_2x128_128 (xmmDstLo, xmmDstHi));

        ps += 4;
        pd += 4;
        pm += 4;
        w -= 4;
d2013 10
a2022 8
        s = *ps++;
        m = *pm++;
        d = *pd;

        *pd++ = pack_1x64_32 (pixMultiply_1x64 (unpack_32_1x64 (d),
                                                pixMultiply_1x64 (unpack_32_1x64 (m),
                                                                  expandAlpha_1x64 (unpack_32_1x64 (s)))));
        w--;
d2027 4
a2030 1
coreCombineOutCsse2 (uint32_t *pd, const uint32_t *ps, const uint32_t *pm, int w)
d2034 4
a2037 4
    __m128i xmmAlphaLo, xmmAlphaHi;
    __m128i xmmSrcLo, xmmSrcHi;
    __m128i xmmDstLo, xmmDstHi;
    __m128i xmmMaskLo, xmmMaskHi;
d2040 3
a2042 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d2046 10
a2055 7
        s = *ps++;
        m = *pm++;
        d = *pd;

        *pd++ = pack_1x64_32 (pixMultiply_1x64 (pixMultiply_1x64 (unpack_32_1x64 (s), unpack_32_1x64 (m)),
                                                negate_1x64 (expandAlpha_1x64 (unpack_32_1x64 (d)))));
        w--;
d2059 3
a2061 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d2065 32
a2096 25
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
        cachePrefetchNext ((__m128i*)pm);

        xmmDstHi = load128Aligned ((__m128i*)pd);
        xmmSrcHi = load128Unaligned ((__m128i*)ps);
        xmmMaskHi = load128Unaligned ((__m128i*)pm);

        unpack_128_2x128 (xmmDstHi, &xmmDstLo, &xmmDstHi);
        unpack_128_2x128 (xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
        unpack_128_2x128 (xmmMaskHi, &xmmMaskLo, &xmmMaskHi);

        expandAlpha_2x128 (xmmDstLo, xmmDstHi, &xmmAlphaLo, &xmmAlphaHi);
        negate_2x128 (xmmAlphaLo, xmmAlphaHi, &xmmAlphaLo, &xmmAlphaHi);

        pixMultiply_2x128 (&xmmSrcLo, &xmmSrcHi, &xmmMaskLo, &xmmMaskHi, &xmmDstLo, &xmmDstHi);
        pixMultiply_2x128 (&xmmDstLo, &xmmDstHi, &xmmAlphaLo, &xmmAlphaHi, &xmmDstLo, &xmmDstHi);

        save128Aligned( (__m128i*)pd, pack_2x128_128 (xmmDstLo, xmmDstHi));

        ps += 4;
        pd += 4;
        pm += 4;
        w -= 4;
d2101 11
a2111 7
        s = *ps++;
        m = *pm++;
        d = *pd;

        *pd++ = pack_1x64_32 (pixMultiply_1x64 (pixMultiply_1x64 (unpack_32_1x64 (s), unpack_32_1x64 (m)),
                                                negate_1x64 (expandAlpha_1x64 (unpack_32_1x64 (d)))));
        w--;
d2116 4
a2119 1
coreCombineOutReverseCsse2 (uint32_t *pd, const uint32_t *ps, const uint32_t *pm, int w)
d2123 4
a2126 4
    __m128i xmmAlphaLo, xmmAlphaHi;
    __m128i xmmSrcLo, xmmSrcHi;
    __m128i xmmDstLo, xmmDstHi;
    __m128i xmmMaskLo, xmmMaskHi;
d2129 3
a2131 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d2135 11
a2145 8
        s = *ps++;
        m = *pm++;
        d = *pd;

        *pd++ = pack_1x64_32 (pixMultiply_1x64 (unpack_32_1x64 (d),
                                                negate_1x64 (pixMultiply_1x64 (unpack_32_1x64 (m),
                                                                               expandAlpha_1x64 (unpack_32_1x64 (s))))));
        w--;
d2149 3
a2151 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d2155 34
a2188 27
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
        cachePrefetchNext ((__m128i*)pm);

        xmmDstHi = load128Aligned ((__m128i*)pd);
        xmmSrcHi = load128Unaligned ((__m128i*)ps);
        xmmMaskHi = load128Unaligned ((__m128i*)pm);

        unpack_128_2x128 (xmmDstHi, &xmmDstLo, &xmmDstHi);
        unpack_128_2x128 (xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
        unpack_128_2x128 (xmmMaskHi, &xmmMaskLo, &xmmMaskHi);

        expandAlpha_2x128 (xmmSrcLo, xmmSrcHi, &xmmAlphaLo, &xmmAlphaHi);

        pixMultiply_2x128 (&xmmMaskLo, &xmmMaskHi, &xmmAlphaLo, &xmmAlphaHi, &xmmMaskLo, &xmmMaskHi);

        negate_2x128 (xmmMaskLo, xmmMaskHi, &xmmMaskLo, &xmmMaskHi);

        pixMultiply_2x128 (&xmmDstLo, &xmmDstHi, &xmmMaskLo, &xmmMaskHi, &xmmDstLo, &xmmDstHi);

        save128Aligned( (__m128i*)pd, pack_2x128_128 (xmmDstLo, xmmDstHi));

        ps += 4;
        pd += 4;
        pm += 4;
        w -= 4;
d2193 11
a2203 8
        s = *ps++;
        m = *pm++;
        d = *pd;

        *pd++ = pack_1x64_32 (pixMultiply_1x64 (unpack_32_1x64 (d),
                                                negate_1x64 (pixMultiply_1x64 (unpack_32_1x64 (m),
                                                                               expandAlpha_1x64 (unpack_32_1x64 (s))))));
        w--;
d2208 3
a2210 1
coreCombineAtopCPixelsse2 (uint32_t src, uint32_t mask, uint32_t dst)
d2215 2
a2216 2
    __m64 sa = expandAlpha_1x64 (s);
    __m64 da = expandAlpha_1x64 (d);
d2218 2
a2219 2
    s = pixMultiply_1x64 (s, m);
    m = negate_1x64 (pixMultiply_1x64 (m, sa));
d2221 1
a2221 1
    return pack_1x64_32 (pixAddMultiply_1x64 (&d, &m, &s, &da));
d2225 4
a2228 1
coreCombineAtopCsse2 (uint32_t *pd, const uint32_t *ps, const uint32_t *pm, int w)
d2232 5
a2236 5
    __m128i xmmSrcLo, xmmSrcHi;
    __m128i xmmDstLo, xmmDstHi;
    __m128i xmmAlphaSrcLo, xmmAlphaSrcHi;
    __m128i xmmAlphaDstLo, xmmAlphaDstHi;
    __m128i xmmMaskLo, xmmMaskHi;
d2239 3
a2241 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d2245 3
a2247 3
        s = *ps++;
        m = *pm++;
        d = *pd;
d2249 2
a2250 2
        *pd++ = coreCombineAtopCPixelsse2 (s, m, d);
        w--;
d2254 3
a2256 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d2260 39
a2298 31
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
        cachePrefetchNext ((__m128i*)pm);

        xmmDstHi = load128Aligned ((__m128i*)pd);
        xmmSrcHi = load128Unaligned ((__m128i*)ps);
        xmmMaskHi = load128Unaligned ((__m128i*)pm);

        unpack_128_2x128 (xmmDstHi, &xmmDstLo, &xmmDstHi);
        unpack_128_2x128 (xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
        unpack_128_2x128 (xmmMaskHi, &xmmMaskLo, &xmmMaskHi);

        expandAlpha_2x128 (xmmSrcLo, xmmSrcHi, &xmmAlphaSrcLo, &xmmAlphaSrcHi);
        expandAlpha_2x128 (xmmDstLo, xmmDstHi, &xmmAlphaDstLo, &xmmAlphaDstHi);

        pixMultiply_2x128 (&xmmSrcLo, &xmmSrcHi, &xmmMaskLo, &xmmMaskHi, &xmmSrcLo, &xmmSrcHi);
        pixMultiply_2x128 (&xmmMaskLo, &xmmMaskHi, &xmmAlphaSrcLo, &xmmAlphaSrcHi, &xmmMaskLo, &xmmMaskHi);

        negate_2x128 (xmmMaskLo, xmmMaskHi, &xmmMaskLo, &xmmMaskHi);

        pixAddMultiply_2x128 (&xmmDstLo, &xmmDstHi, &xmmMaskLo, &xmmMaskHi,
                              &xmmSrcLo, &xmmSrcHi, &xmmAlphaDstLo, &xmmAlphaDstHi,
                              &xmmDstLo, &xmmDstHi);

        save128Aligned( (__m128i*)pd, pack_2x128_128 (xmmDstLo, xmmDstHi));

        ps += 4;
        pd += 4;
        pm += 4;
        w -= 4;
d2303 3
a2305 3
        s = *ps++;
        m = *pm++;
        d = *pd;
d2307 2
a2308 2
        *pd++ = coreCombineAtopCPixelsse2 (s, m, d);
        w--;
d2313 3
a2315 1
coreCombineReverseAtopCPixelsse2 (uint32_t src, uint32_t mask, uint32_t dst)
d2321 2
a2322 2
    __m64 da = negate_1x64 (expandAlpha_1x64 (d));
    __m64 sa = expandAlpha_1x64 (s);
d2324 2
a2325 2
    s = pixMultiply_1x64 (s, m);
    m = pixMultiply_1x64 (m, sa);
d2327 1
a2327 1
    return pack_1x64_32 (pixAddMultiply_1x64 (&d, &m, &s, &da));
d2331 4
a2334 1
coreCombineReverseAtopCsse2 (uint32_t *pd, const uint32_t *ps, const uint32_t *pm, int w)
d2338 5
a2342 5
    __m128i xmmSrcLo, xmmSrcHi;
    __m128i xmmDstLo, xmmDstHi;
    __m128i xmmAlphaSrcLo, xmmAlphaSrcHi;
    __m128i xmmAlphaDstLo, xmmAlphaDstHi;
    __m128i xmmMaskLo, xmmMaskHi;
d2345 3
a2347 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d2351 3
a2353 3
        s = *ps++;
        m = *pm++;
        d = *pd;
d2355 2
a2356 2
        *pd++ = coreCombineReverseAtopCPixelsse2 (s, m, d);
        w--;
d2360 3
a2362 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d2366 40
a2405 31
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
        cachePrefetchNext ((__m128i*)pm);

        xmmDstHi = load128Aligned ((__m128i*)pd);
        xmmSrcHi = load128Unaligned ((__m128i*)ps);
        xmmMaskHi = load128Unaligned ((__m128i*)pm);

        unpack_128_2x128 (xmmDstHi, &xmmDstLo, &xmmDstHi);
        unpack_128_2x128 (xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
        unpack_128_2x128 (xmmMaskHi, &xmmMaskLo, &xmmMaskHi);

        expandAlpha_2x128 (xmmSrcLo, xmmSrcHi, &xmmAlphaSrcLo, &xmmAlphaSrcHi);
        expandAlpha_2x128 (xmmDstLo, xmmDstHi, &xmmAlphaDstLo, &xmmAlphaDstHi);

        pixMultiply_2x128 (&xmmSrcLo, &xmmSrcHi, &xmmMaskLo, &xmmMaskHi, &xmmSrcLo, &xmmSrcHi);
        pixMultiply_2x128 (&xmmMaskLo, &xmmMaskHi, &xmmAlphaSrcLo, &xmmAlphaSrcHi, &xmmMaskLo, &xmmMaskHi);

        negate_2x128 (xmmAlphaDstLo, xmmAlphaDstHi, &xmmAlphaDstLo, &xmmAlphaDstHi);

        pixAddMultiply_2x128 (&xmmDstLo, &xmmDstHi, &xmmMaskLo, &xmmMaskHi,
                              &xmmSrcLo, &xmmSrcHi, &xmmAlphaDstLo, &xmmAlphaDstHi,
                              &xmmDstLo, &xmmDstHi);

        save128Aligned( (__m128i*)pd, pack_2x128_128 (xmmDstLo, xmmDstHi));

        ps += 4;
        pd += 4;
        pm += 4;
        w -= 4;
d2410 3
a2412 3
        s = *ps++;
        m = *pm++;
        d = *pd;
d2414 2
a2415 2
        *pd++ = coreCombineReverseAtopCPixelsse2 (s, m, d);
        w--;
d2420 3
a2422 1
coreCombineXorCPixelsse2 (uint32_t src, uint32_t mask, uint32_t dst)
d2428 9
a2436 8
    __m64 alphaDst = negate_1x64 (pixMultiply_1x64 (a, expandAlpha_1x64 (s)));
    __m64 dest      = pixMultiply_1x64 (s, a);
    __m64 alphaSrc = negate_1x64 (expandAlpha_1x64 (d));

    return pack_1x64_32 (pixAddMultiply_1x64 (&d,
                                              &alphaDst,
                                              &dest,
                                              &alphaSrc));
d2440 4
a2443 1
coreCombineXorCsse2 (uint32_t *pd, const uint32_t *ps, const uint32_t *pm, int w)
d2447 5
a2451 5
    __m128i xmmSrcLo, xmmSrcHi;
    __m128i xmmDstLo, xmmDstHi;
    __m128i xmmAlphaSrcLo, xmmAlphaSrcHi;
    __m128i xmmAlphaDstLo, xmmAlphaDstHi;
    __m128i xmmMaskLo, xmmMaskHi;
d2454 3
a2456 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d2460 3
a2462 3
        s = *ps++;
        m = *pm++;
        d = *pd;
d2464 2
a2465 2
        *pd++ = coreCombineXorCPixelsse2 (s, m, d);
        w--;
d2469 3
a2471 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d2475 42
a2516 32
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
        cachePrefetchNext ((__m128i*)pm);

        xmmDstHi = load128Aligned ((__m128i*)pd);
        xmmSrcHi = load128Unaligned ((__m128i*)ps);
        xmmMaskHi = load128Unaligned ((__m128i*)pm);

        unpack_128_2x128 (xmmDstHi, &xmmDstLo, &xmmDstHi);
        unpack_128_2x128 (xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
        unpack_128_2x128 (xmmMaskHi, &xmmMaskLo, &xmmMaskHi);

        expandAlpha_2x128 (xmmSrcLo, xmmSrcHi, &xmmAlphaSrcLo, &xmmAlphaSrcHi);
        expandAlpha_2x128 (xmmDstLo, xmmDstHi, &xmmAlphaDstLo, &xmmAlphaDstHi);

        pixMultiply_2x128 (&xmmSrcLo, &xmmSrcHi, &xmmMaskLo, &xmmMaskHi, &xmmSrcLo, &xmmSrcHi);
        pixMultiply_2x128 (&xmmMaskLo, &xmmMaskHi, &xmmAlphaSrcLo, &xmmAlphaSrcHi, &xmmMaskLo, &xmmMaskHi);

        negate_2x128 (xmmAlphaDstLo, xmmAlphaDstHi, &xmmAlphaDstLo, &xmmAlphaDstHi);
        negate_2x128 (xmmMaskLo, xmmMaskHi, &xmmMaskLo, &xmmMaskHi);

        pixAddMultiply_2x128 (&xmmDstLo, &xmmDstHi, &xmmMaskLo, &xmmMaskHi,
                              &xmmSrcLo, &xmmSrcHi, &xmmAlphaDstLo, &xmmAlphaDstHi,
                              &xmmDstLo, &xmmDstHi);

        save128Aligned( (__m128i*)pd, pack_2x128_128 (xmmDstLo, xmmDstHi));

        ps += 4;
        pd += 4;
        pm += 4;
        w -= 4;
d2521 3
a2523 3
        s = *ps++;
        m = *pm++;
        d = *pd;
d2525 2
a2526 2
        *pd++ = coreCombineXorCPixelsse2 (s, m, d);
        w--;
d2531 4
a2534 1
coreCombineAddCsse2 (uint32_t *pd, const uint32_t *ps, const uint32_t *pm, int w)
d2538 3
a2540 3
    __m128i xmmSrcLo, xmmSrcHi;
    __m128i xmmDstLo, xmmDstHi;
    __m128i xmmMaskLo, xmmMaskHi;
d2543 3
a2545 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d2549 3
a2551 3
        s = *ps++;
        m = *pm++;
        d = *pd;
d2553 5
a2557 4
        *pd++ = pack_1x64_32 (_mm_adds_pu8 (pixMultiply_1x64 (unpack_32_1x64 (s),
                                                              unpack_32_1x64 (m)),
                                            unpack_32_1x64 (d)));
        w--;
d2561 3
a2563 3
    cachePrefetch ((__m128i*)ps);
    cachePrefetch ((__m128i*)pd);
    cachePrefetch ((__m128i*)pm);
d2567 26
a2592 22
        /* fill cache line with next memory */
        cachePrefetchNext ((__m128i*)ps);
        cachePrefetchNext ((__m128i*)pd);
        cachePrefetchNext ((__m128i*)pm);

        xmmSrcHi = load128Unaligned ((__m128i*)ps);
        xmmMaskHi = load128Unaligned ((__m128i*)pm);
        xmmDstHi = load128Aligned ((__m128i*)pd);

        unpack_128_2x128 (xmmSrcHi, &xmmSrcLo, &xmmSrcHi);
        unpack_128_2x128 (xmmMaskHi, &xmmMaskLo, &xmmMaskHi);
        unpack_128_2x128 (xmmDstHi, &xmmDstLo, &xmmDstHi);

        pixMultiply_2x128 (&xmmSrcLo, &xmmSrcHi, &xmmMaskLo, &xmmMaskHi, &xmmSrcLo, &xmmSrcHi);

        save128Aligned( (__m128i*)pd, pack_2x128_128 (_mm_adds_epu8 (xmmSrcLo, xmmDstLo),
                                                      _mm_adds_epu8 (xmmSrcHi, xmmDstHi)));

        ps += 4;
        pd += 4;
        pm += 4;
        w -= 4;
d2597 9
a2605 8
        s = *ps++;
        m = *pm++;
        d = *pd;

        *pd++ = pack_1x64_32 (_mm_adds_pu8 (pixMultiply_1x64 (unpack_32_1x64 (s),
                                                              unpack_32_1x64 (m)),
                                            unpack_32_1x64 (d)));
        w--;
d2609 2
a2610 2
/* -------------------------------------------------------------------------------------------------
 * fbComposeSetupSSE2
d2613 1
a2613 1
createMask_16_64 (uint16_t mask)
d2619 1
a2619 1
createMask_16_128 (uint16_t mask)
d2625 2
a2626 1
createMask_2x32_64 (uint32_t mask0, uint32_t mask1)
d2631 5
d2637 2
a2638 1
createMask_2x32_128 (uint32_t mask0, uint32_t mask1)
d2642 1
d2646 7
a2652 3
static FASTCALL void
sse2CombineOverU (pixman_implementation_t *imp, pixman_op_t op,
		  uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2654 2
a2655 2
    coreCombineOverUsse2 (dst, src, mask, width);
    _mm_empty();
d2658 7
a2664 3
static FASTCALL void
sse2CombineOverReverseU (pixman_implementation_t *imp, pixman_op_t op,
			 uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2666 2
a2667 2
    coreCombineOverReverseUsse2 (dst, src, mask, width);
    _mm_empty();
d2670 7
a2676 3
static FASTCALL void
sse2CombineInU (pixman_implementation_t *imp, pixman_op_t op,
		uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2678 2
a2679 2
    coreCombineInUsse2 (dst, src, mask, width);
    _mm_empty();
d2682 7
a2688 3
static FASTCALL void
sse2CombineInReverseU (pixman_implementation_t *imp, pixman_op_t op,
		       uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2690 2
a2691 2
    coreCombineReverseInUsse2 (dst, src, mask, width);
    _mm_empty();
d2694 7
a2700 3
static FASTCALL void
sse2CombineOutU (pixman_implementation_t *imp, pixman_op_t op,
		 uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2702 2
a2703 2
    coreCombineOutUsse2 (dst, src, mask, width);
    _mm_empty();
d2706 7
a2712 3
static FASTCALL void
sse2CombineOutReverseU (pixman_implementation_t *imp, pixman_op_t op,
			uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2714 2
a2715 2
    coreCombineReverseOutUsse2 (dst, src, mask, width);
    _mm_empty();
d2718 7
a2724 3
static FASTCALL void
sse2CombineAtopU (pixman_implementation_t *imp, pixman_op_t op,
		  uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2726 2
a2727 2
    coreCombineAtopUsse2 (dst, src, mask, width);
    _mm_empty();
d2730 7
a2736 3
static FASTCALL void
sse2CombineAtopReverseU (pixman_implementation_t *imp, pixman_op_t op,
			 uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2738 2
a2739 2
    coreCombineReverseAtopUsse2 (dst, src, mask, width);
    _mm_empty();
d2742 7
a2748 3
static FASTCALL void
sse2CombineXorU (pixman_implementation_t *imp, pixman_op_t op,
		 uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2750 2
a2751 2
    coreCombineXorUsse2 (dst, src, mask, width);
    _mm_empty();
d2754 7
a2760 3
static FASTCALL void
sse2CombineAddU (pixman_implementation_t *imp, pixman_op_t op,
		 uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2762 2
a2763 2
    coreCombineAddUsse2 (dst, src, mask, width);
    _mm_empty();
d2766 7
a2772 3
static FASTCALL void
sse2CombineSaturateU (pixman_implementation_t *imp, pixman_op_t op,
		      uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2774 2
a2775 2
    coreCombineSaturateUsse2 (dst, src, mask, width);
    _mm_empty();
d2778 7
a2784 3
static FASTCALL void
sse2CombineSrcC (pixman_implementation_t *imp, pixman_op_t op,
		 uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2786 2
a2787 2
    coreCombineSrcCsse2 (dst, src, mask, width);
    _mm_empty();
d2790 7
a2796 3
static FASTCALL void
sse2CombineOverC (pixman_implementation_t *imp, pixman_op_t op,
		  uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2798 2
a2799 2
    coreCombineOverCsse2 (dst, src, mask, width);
    _mm_empty();
d2802 7
a2808 3
static FASTCALL void
sse2CombineOverReverseC (pixman_implementation_t *imp, pixman_op_t op,
			 uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2810 2
a2811 2
    coreCombineOverReverseCsse2 (dst, src, mask, width);
    _mm_empty();
d2814 7
a2820 3
static FASTCALL void
sse2CombineInC (pixman_implementation_t *imp, pixman_op_t op,
		uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2822 2
a2823 2
    coreCombineInCsse2 (dst, src, mask, width);
    _mm_empty();
d2826 7
a2832 3
static FASTCALL void
sse2CombineInReverseC (pixman_implementation_t *imp, pixman_op_t op,
		       uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2834 2
a2835 2
    coreCombineInReverseCsse2 (dst, src, mask, width);
    _mm_empty();
d2838 7
a2844 3
static FASTCALL void
sse2CombineOutC (pixman_implementation_t *imp, pixman_op_t op,
		 uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2846 2
a2847 2
    coreCombineOutCsse2 (dst, src, mask, width);
    _mm_empty();
d2850 7
a2856 3
static FASTCALL void
sse2CombineOutReverseC (pixman_implementation_t *imp, pixman_op_t op,
			uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2858 2
a2859 2
    coreCombineOutReverseCsse2 (dst, src, mask, width);
    _mm_empty();
d2862 7
a2868 3
static FASTCALL void
sse2CombineAtopC (pixman_implementation_t *imp, pixman_op_t op,
		  uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2870 2
a2871 2
    coreCombineAtopCsse2 (dst, src, mask, width);
    _mm_empty();
d2874 7
a2880 3
static FASTCALL void
sse2CombineAtopReverseC (pixman_implementation_t *imp, pixman_op_t op,
			 uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2882 2
a2883 2
    coreCombineReverseAtopCsse2 (dst, src, mask, width);
    _mm_empty();
d2886 7
a2892 3
static FASTCALL void
sse2CombineXorC (pixman_implementation_t *imp, pixman_op_t op,
		 uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2894 2
a2895 2
    coreCombineXorCsse2 (dst, src, mask, width);
    _mm_empty();
d2898 7
a2904 3
static FASTCALL void
sse2CombineAddC (pixman_implementation_t *imp, pixman_op_t op,
		 uint32_t *dst, const uint32_t *src, const uint32_t *mask, int width)
d2906 2
a2907 2
    coreCombineAddCsse2 (dst, src, mask, width);
    _mm_empty();
d2910 2
a2911 2
/* -------------------------------------------------------------------------------------------------
 * fbCompositeSolid_nx8888
d2915 20
a2934 20
fbCompositeSolid_nx8888sse2 (pixman_implementation_t *imp,
			     pixman_op_t op,
			    pixman_image_t * pSrc,
			    pixman_image_t * pMask,
			    pixman_image_t * pDst,
			    int32_t	xSrc,
			    int32_t	ySrc,
			    int32_t	xMask,
			    int32_t	yMask,
			    int32_t	xDst,
			    int32_t	yDst,
			    int32_t	width,
			    int32_t	height)
{
    uint32_t	src;
    uint32_t	*dstLine, *dst, d;
    uint16_t	w;
    int	dstStride;
    __m128i xmmSrc, xmmAlpha;
    __m128i xmmDst, xmmDstLo, xmmDstHi;
d2936 1
a2936 1
    fbComposeGetSolid(pSrc, src, pDst->bits.format);
d2941 2
a2942 1
    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);
d2944 2
a2945 2
    xmmSrc = expandPixel_32_1x128 (src);
    xmmAlpha = expandAlpha_1x128 (xmmSrc);
d2949 1
a2949 23
        dst = dstLine;

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)dst);

        dstLine += dstStride;
        w = width;

        while (w && (unsigned long)dst & 15)
        {
            d = *dst;
            *dst++ = pack_1x64_32 (over_1x64 (_mm_movepi64_pi64 (xmmSrc),
                                              _mm_movepi64_pi64 (xmmAlpha),
                                              unpack_32_1x64 (d)));
            w--;
        }

        cachePrefetch ((__m128i*)dst);

        while (w >= 4)
        {
            /* fill cache line with next memory */
            cachePrefetchNext ((__m128i*)dst);
d2951 2
a2952 1
            xmmDst = load128Aligned ((__m128i*)dst);
d2954 2
a2955 1
            unpack_128_2x128 (xmmDst, &xmmDstLo, &xmmDstHi);
d2957 40
a2996 17
            over_2x128 (&xmmSrc, &xmmSrc, &xmmAlpha, &xmmAlpha, &xmmDstLo, &xmmDstHi);

            /* rebuid the 4 pixel data and save*/
            save128Aligned ((__m128i*)dst, pack_2x128_128 (xmmDstLo, xmmDstHi));

            w -= 4;
            dst += 4;
        }

        while (w)
        {
            d = *dst;
            *dst++ = pack_1x64_32 (over_1x64 (_mm_movepi64_pi64 (xmmSrc),
                                              _mm_movepi64_pi64 (xmmAlpha),
                                              unpack_32_1x64 (d)));
            w--;
        }
d2999 1
a2999 1
    _mm_empty();
d3002 2
a3003 2
/* -------------------------------------------------------------------------------------------------
 * fbCompositeSolid_nx0565
d3006 20
a3025 20
fbCompositeSolid_nx0565sse2 (pixman_implementation_t *imp,
			     pixman_op_t op,
			    pixman_image_t * pSrc,
			    pixman_image_t * pMask,
			    pixman_image_t * pDst,
			    int32_t	xSrc,
			    int32_t	ySrc,
			    int32_t	xMask,
			    int32_t	yMask,
			    int32_t	xDst,
			    int32_t	yDst,
			    int32_t	width,
			    int32_t	height)
{
    uint32_t	src;
    uint16_t	*dstLine, *dst, d;
    uint16_t	w;
    int	        dstStride;
    __m128i xmmSrc, xmmAlpha;
    __m128i xmmDst, xmmDst0, xmmDst1, xmmDst2, xmmDst3;
d3027 1
a3027 1
    fbComposeGetSolid(pSrc, src, pDst->bits.format);
d3030 1
a3030 1
        return;
d3032 2
a3033 1
    fbComposeGetStart (pDst, xDst, yDst, uint16_t, dstStride, dstLine, 1);
d3035 2
a3036 2
    xmmSrc = expandPixel_32_1x128 (src);
    xmmAlpha = expandAlpha_1x128 (xmmSrc);
d3040 1
a3040 1
        dst = dstLine;
d3042 2
a3043 2
        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)dst);
d3045 2
a3046 2
        dstLine += dstStride;
        w = width;
d3048 48
a3095 39
        while (w && (unsigned long)dst & 15)
        {
            d = *dst;

            *dst++ = pack565_32_16 (pack_1x64_32 (over_1x64 (_mm_movepi64_pi64 (xmmSrc),
                                                             _mm_movepi64_pi64 (xmmAlpha),
                                                             expand565_16_1x64 (d))));
            w--;
        }

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)dst);

        while (w >= 8)
        {
            /* fill cache line with next memory */
            cachePrefetchNext ((__m128i*)dst);

	    xmmDst = load128Aligned ((__m128i*)dst);
	    
	    unpack565_128_4x128 (xmmDst, &xmmDst0, &xmmDst1, &xmmDst2, &xmmDst3);
	    
            over_2x128 (&xmmSrc, &xmmSrc, &xmmAlpha, &xmmAlpha, &xmmDst0, &xmmDst1);
            over_2x128 (&xmmSrc, &xmmSrc, &xmmAlpha, &xmmAlpha, &xmmDst2, &xmmDst3);

            xmmDst = pack565_4x128_128 (&xmmDst0, &xmmDst1, &xmmDst2, &xmmDst3);
            save128Aligned ((__m128i*)dst, xmmDst);

            dst += 8;
            w -= 8;
        }

        while (w--)
        {
            d = *dst;
            *dst++ = pack565_32_16 (pack_1x64_32 (over_1x64 (_mm_movepi64_pi64 (xmmSrc),
                                                             _mm_movepi64_pi64 (xmmAlpha),
                                                             expand565_16_1x64 (d))));
        }
d3098 1
a3098 1
    _mm_empty();
d3101 2
a3102 2
/* -------------------------------------------------------------------------------------------------
 * fbCompositeSolidMask_nx8888x8888C
a3103 1

d3105 23
a3127 23
fbCompositeSolidMask_nx8888x8888Csse2 (pixman_implementation_t *imp,
				       pixman_op_t op,
				      pixman_image_t * pSrc,
				      pixman_image_t * pMask,
				      pixman_image_t * pDst,
				      int32_t	xSrc,
				      int32_t	ySrc,
				      int32_t	xMask,
				      int32_t	yMask,
				      int32_t	xDst,
				      int32_t	yDst,
				      int32_t	width,
				      int32_t	height)
{
    uint32_t	src;
    uint32_t	*dstLine, d;
    uint32_t	*maskLine, m;
    uint32_t    packCmp;
    int	dstStride, maskStride;

    __m128i xmmSrc, xmmAlpha;
    __m128i xmmDst, xmmDstLo, xmmDstHi;
    __m128i xmmMask, xmmMaskLo, xmmMaskHi;
d3129 1
a3129 3
    __m64 mmxSrc, mmxAlpha, mmxMask, mmxDst;

    fbComposeGetSolid(pSrc, src, pDst->bits.format);
d3131 3
d3137 10
a3146 7
    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);
    fbComposeGetStart (pMask, xMask, yMask, uint32_t, maskStride, maskLine, 1);

    xmmSrc = _mm_unpacklo_epi8 (createMask_2x32_128 (src, src), _mm_setzero_si128 ());
    xmmAlpha = expandAlpha_1x128 (xmmSrc);
    mmxSrc   = _mm_movepi64_pi64 (xmmSrc);
    mmxAlpha = _mm_movepi64_pi64 (xmmAlpha);
d3150 85
a3234 82
        int w = width;
        const uint32_t *pm = (uint32_t *)maskLine;
        uint32_t *pd = (uint32_t *)dstLine;

        dstLine += dstStride;
        maskLine += maskStride;

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)pd);
        cachePrefetch ((__m128i*)pm);

        while (w && (unsigned long)pd & 15)
        {
            m = *pm++;

            if (m)
            {
                d = *pd;
                mmxMask = unpack_32_1x64 (m);
                mmxDst = unpack_32_1x64 (d);

                *pd = pack_1x64_32 (inOver_1x64 (&mmxSrc,
                                                 &mmxAlpha,
                                                 &mmxMask,
                                                 &mmxDst));
            }

            pd++;
            w--;
        }

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)pd);
        cachePrefetch ((__m128i*)pm);

        while (w >= 4)
        {
            /* fill cache line with next memory */
            cachePrefetchNext ((__m128i*)pd);
            cachePrefetchNext ((__m128i*)pm);

            xmmMask = load128Unaligned ((__m128i*)pm);

            packCmp = _mm_movemask_epi8 (_mm_cmpeq_epi32 (xmmMask, _mm_setzero_si128()));

            /* if all bits in mask are zero, packCmp are equal to 0xffff */
            if (packCmp != 0xffff)
            {
                xmmDst = load128Aligned ((__m128i*)pd);

                unpack_128_2x128 (xmmMask, &xmmMaskLo, &xmmMaskHi);
                unpack_128_2x128 (xmmDst, &xmmDstLo, &xmmDstHi);

                inOver_2x128 (&xmmSrc, &xmmSrc, &xmmAlpha, &xmmAlpha, &xmmMaskLo, &xmmMaskHi, &xmmDstLo, &xmmDstHi);

                save128Aligned ((__m128i*)pd, pack_2x128_128 (xmmDstLo, xmmDstHi));
            }

            pd += 4;
            pm += 4;
            w -= 4;
        }

        while (w)
        {
            m = *pm++;

            if (m)
            {
                d = *pd;
                mmxMask = unpack_32_1x64 (m);
                mmxDst = unpack_32_1x64 (d);

                *pd = pack_1x64_32 (inOver_1x64 (&mmxSrc,
                                                 &mmxAlpha,
                                                 &mmxMask,
                                                 &mmxDst));
            }

            pd++;
            w--;
        }
d3237 1
a3237 1
    _mm_empty();
d3240 2
a3241 3

/* -------------------------------------------------------------------------------------------------
 * fbCompositeSrc_8888x8x8888
d3245 25
a3269 28
fbCompositeSrc_8888x8x8888sse2 (pixman_implementation_t *imp,
				pixman_op_t op,
			       pixman_image_t * pSrc,
			       pixman_image_t * pMask,
			       pixman_image_t * pDst,
			       int32_t	xSrc,
			       int32_t	ySrc,
			       int32_t      xMask,
			       int32_t      yMask,
			       int32_t      xDst,
			       int32_t      yDst,
			       int32_t     width,
			       int32_t     height)
{
    uint32_t	*dstLine, *dst;
    uint32_t	*srcLine, *src;
    uint32_t	mask;
    uint16_t	w;
    int	dstStride, srcStride;

    __m128i xmmMask;
    __m128i xmmSrc, xmmSrcLo, xmmSrcHi;
    __m128i xmmDst, xmmDstLo, xmmDstHi;
    __m128i xmmAlphaLo, xmmAlphaHi;

    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);
    fbComposeGetStart (pSrc, xSrc, ySrc, uint32_t, srcStride, srcLine, 1);
    fbComposeGetSolid (pMask, mask, pDst->bits.format);
d3271 15
a3285 1
    xmmMask = createMask_16_128 (mask >> 24);
d3289 63
a3351 19
        dst = dstLine;
        dstLine += dstStride;
        src = srcLine;
        srcLine += srcStride;
        w = width;

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)dst);
        cachePrefetch ((__m128i*)src);

        while (w && (unsigned long)dst & 15)
        {
            uint32_t s = *src++;
            uint32_t d = *dst;

            __m64 ms = unpack_32_1x64 (s);
            __m64 alpha    = expandAlpha_1x64 (ms);
            __m64 dest     = _mm_movepi64_pi64 (xmmMask);
            __m64 alphaDst = unpack_32_1x64 (d);
d3353 4
a3356 4
            *dst++ = pack_1x64_32 (inOver_1x64 (&ms,
                                                &alpha,
                                                &dest,
                                                &alphaDst));
d3358 18
a3375 2
            w--;
        }
d3377 2
a3378 3
        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)dst);
        cachePrefetch ((__m128i*)src);
d3380 3
a3382 5
        while (w >= 4)
        {
            /* fill cache line with next memory */
            cachePrefetchNext ((__m128i*)dst);
            cachePrefetchNext ((__m128i*)src);
d3384 20
a3403 2
            xmmSrc = load128Unaligned ((__m128i*)src);
            xmmDst = load128Aligned ((__m128i*)dst);
d3405 4
a3408 3
            unpack_128_2x128 (xmmSrc, &xmmSrcLo, &xmmSrcHi);
            unpack_128_2x128 (xmmDst, &xmmDstLo, &xmmDstHi);
            expandAlpha_2x128 (xmmSrcLo, xmmSrcHi, &xmmAlphaLo, &xmmAlphaHi);
d3410 4
a3413 1
            inOver_2x128 (&xmmSrcLo, &xmmSrcHi, &xmmAlphaLo, &xmmAlphaHi, &xmmMask, &xmmMask, &xmmDstLo, &xmmDstHi);
d3415 1
a3415 1
            save128Aligned( (__m128i*)dst, pack_2x128_128 (xmmDstLo, xmmDstHi));
d3417 1
a3417 4
            dst += 4;
            src += 4;
            w -= 4;
        }
d3419 68
a3486 4
        while (w)
        {
            uint32_t s = *src++;
            uint32_t d = *dst;
d3488 2
a3489 4
            __m64 ms = unpack_32_1x64 (s);
            __m64 alpha = expandAlpha_1x64 (ms);
            __m64 mask  = _mm_movepi64_pi64 (xmmMask);
            __m64 dest  = unpack_32_1x64 (d);
d3491 2
a3492 7
            *dst++ = pack_1x64_32 (inOver_1x64 (&ms,
                                                &alpha,
                                                &mask,
                                                &dest));

            w--;
        }
d3495 1
a3495 1
    _mm_empty();
d3498 2
a3499 2
/* -------------------------------------------------------------------------------------------------
 * fbCompositeSrc_x888xnx8888
d3502 30
a3531 27
fbCompositeSrc_x888xnx8888sse2 (pixman_implementation_t *imp,
				pixman_op_t op,
			       pixman_image_t * pSrc,
			       pixman_image_t * pMask,
			       pixman_image_t * pDst,
			       int32_t	xSrc,
			       int32_t	ySrc,
			       int32_t      xMask,
			       int32_t      yMask,
			       int32_t      xDst,
			       int32_t      yDst,
			       int32_t     width,
			       int32_t     height)
{
    uint32_t	*dstLine, *dst;
    uint32_t	*srcLine, *src;
    uint32_t	mask;
    int	dstStride, srcStride;
    uint16_t	w;

    __m128i xmmMask, xmmAlpha;
    __m128i xmmSrc, xmmSrcLo, xmmSrcHi;
    __m128i xmmDst, xmmDstLo, xmmDstHi;

    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);
    fbComposeGetStart (pSrc, xSrc, ySrc, uint32_t, srcStride, srcLine, 1);
    fbComposeGetSolid (pMask, mask, pDst->bits.format);
d3533 2
a3534 2
    xmmMask = createMask_16_128 (mask >> 24);
    xmmAlpha = Mask00ff;
d3538 66
a3603 53
        dst = dstLine;
        dstLine += dstStride;
        src = srcLine;
        srcLine += srcStride;
        w = width;

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)dst);
        cachePrefetch ((__m128i*)src);

        while (w && (unsigned long)dst & 15)
        {
            uint32_t s = (*src++) | 0xff000000;
            uint32_t d = *dst;

            __m64 src   = unpack_32_1x64 (s);
            __m64 alpha = _mm_movepi64_pi64 (xmmAlpha);
            __m64 mask  = _mm_movepi64_pi64 (xmmMask);
            __m64 dest  = unpack_32_1x64 (d);

            *dst++ = pack_1x64_32 (inOver_1x64 (&src,
                                                &alpha,
                                                &mask,
                                                &dest));

            w--;
        }

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)dst);
        cachePrefetch ((__m128i*)src);

        while (w >= 4)
        {
            /* fill cache line with next memory */
            cachePrefetchNext ((__m128i*)dst);
            cachePrefetchNext ((__m128i*)src);

            xmmSrc = _mm_or_si128 (load128Unaligned ((__m128i*)src), Maskff000000);
            xmmDst = load128Aligned ((__m128i*)dst);

            unpack_128_2x128 (xmmSrc, &xmmSrcLo, &xmmSrcHi);
            unpack_128_2x128 (xmmDst, &xmmDstLo, &xmmDstHi);

            inOver_2x128 (&xmmSrcLo, &xmmSrcHi, &xmmAlpha, &xmmAlpha, &xmmMask, &xmmMask, &xmmDstLo, &xmmDstHi);

            save128Aligned( (__m128i*)dst, pack_2x128_128 (xmmDstLo, xmmDstHi));

            dst += 4;
            src += 4;
            w -= 4;

        }
d3605 2
a3606 4
        while (w)
        {
            uint32_t s = (*src++) | 0xff000000;
            uint32_t d = *dst;
d3608 2
a3609 12
            __m64 src  = unpack_32_1x64 (s);
            __m64 alpha = _mm_movepi64_pi64 (xmmAlpha);
            __m64 mask  = _mm_movepi64_pi64 (xmmMask);
            __m64 dest  = unpack_32_1x64 (d);

            *dst++ = pack_1x64_32 (inOver_1x64 (&src,
                                                &alpha,
                                                &mask,
                                                &dest));

            w--;
        }
d3612 1
a3612 1
    _mm_empty();
d3615 2
a3616 2
/* -------------------------------------------------------------------------------------------------
 * fbCompositeSrc_8888x8888
d3619 22
a3640 20
fbCompositeSrc_8888x8888sse2 (pixman_implementation_t *imp,
			      pixman_op_t op,
			     pixman_image_t * pSrc,
			     pixman_image_t * pMask,
			     pixman_image_t * pDst,
			     int32_t	xSrc,
			     int32_t	ySrc,
			     int32_t      xMask,
			     int32_t      yMask,
			     int32_t      xDst,
			     int32_t      yDst,
			     int32_t     width,
			     int32_t     height)
{
    int	        dstStride, srcStride;
    uint32_t	*dstLine, *dst;
    uint32_t	*srcLine, *src;

    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);
    fbComposeGetStart (pSrc, xSrc, ySrc, uint32_t, srcStride, srcLine, 1);
d3642 2
a3643 2
    dst = dstLine;
    src = srcLine;
d3647 1
a3647 1
        coreCombineOverUsse2 (dst, src, NULL, width);
d3649 2
a3650 2
        dst += dstStride;
        src += srcStride;
d3652 1
a3652 1
    _mm_empty();
d3655 2
a3656 2
/* -------------------------------------------------------------------------------------------------
 * fbCompositeSrc_8888x0565
d3659 1
a3659 1
fbCompositeSrc_8888x0565pixel (uint32_t src, uint16_t dst)
d3661 1
a3661 1
    __m64       ms;
d3664 4
a3667 3
    return pack565_32_16( pack_1x64_32 (over_1x64 (ms,
                                                   expandAlpha_1x64 (ms),
                                                   expand565_16_1x64 (dst))));
d3671 18
a3688 22
fbCompositeSrc_8888x0565sse2 (pixman_implementation_t *imp,
			      pixman_op_t op,
			     pixman_image_t * pSrc,
			     pixman_image_t * pMask,
			     pixman_image_t * pDst,
			     int32_t      xSrc,
			     int32_t      ySrc,
			     int32_t      xMask,
			     int32_t      yMask,
			     int32_t      xDst,
			     int32_t      yDst,
			     int32_t     width,
			     int32_t     height)
{
    uint16_t	*dstLine, *dst, d;
    uint32_t	*srcLine, *src, s;
    int	dstStride, srcStride;
    uint16_t	w;

    __m128i xmmAlphaLo, xmmAlphaHi;
    __m128i xmmSrc, xmmSrcLo, xmmSrcHi;
    __m128i xmmDst, xmmDst0, xmmDst1, xmmDst2, xmmDst3;
d3690 8
a3697 2
    fbComposeGetStart (pDst, xDst, yDst, uint16_t, dstStride, dstLine, 1);
    fbComposeGetStart (pSrc, xSrc, ySrc, uint32_t, srcStride, srcLine, 1);
d3705 1
a3705 1
    assert (pSrc->pDrawable == pMask->pDrawable);
d3710 2
a3711 52
        dst = dstLine;
        src = srcLine;

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)src);
        cachePrefetch ((__m128i*)dst);

        dstLine += dstStride;
        srcLine += srcStride;
        w = width;

        /* Align dst on a 16-byte boundary */
        while (w &&
               ((unsigned long)dst & 15))
        {
            s = *src++;
            d = *dst;

            *dst++ = fbCompositeSrc_8888x0565pixel (s, d);
            w--;
        }

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)src);
        cachePrefetch ((__m128i*)dst);

        /* It's a 8 pixel loop */
        while (w >= 8)
        {
            /* fill cache line with next memory */
            cachePrefetchNext ((__m128i*)src);
            cachePrefetchNext ((__m128i*)dst);

            /* I'm loading unaligned because I'm not sure about the address alignment. */
            xmmSrc = load128Unaligned ((__m128i*) src);
            xmmDst = load128Aligned ((__m128i*) dst);

            /* Unpacking */
            unpack_128_2x128 (xmmSrc, &xmmSrcLo, &xmmSrcHi);
            unpack565_128_4x128 (xmmDst, &xmmDst0, &xmmDst1, &xmmDst2, &xmmDst3);
            expandAlpha_2x128 (xmmSrcLo, xmmSrcHi, &xmmAlphaLo, &xmmAlphaHi);

            /* I'm loading next 4 pixels from memory before to optimze the memory read. */
            xmmSrc = load128Unaligned ((__m128i*) (src+4));

            over_2x128 (&xmmSrcLo, &xmmSrcHi, &xmmAlphaLo, &xmmAlphaHi, &xmmDst0, &xmmDst1);

            /* Unpacking */
            unpack_128_2x128 (xmmSrc, &xmmSrcLo, &xmmSrcHi);
            expandAlpha_2x128 (xmmSrcLo, xmmSrcHi, &xmmAlphaLo, &xmmAlphaHi);

            over_2x128 (&xmmSrcLo, &xmmSrcHi, &xmmAlphaLo, &xmmAlphaHi, &xmmDst2, &xmmDst3);
d3713 74
a3786 1
            save128Aligned ((__m128i*)dst, pack565_4x128_128 (&xmmDst0, &xmmDst1, &xmmDst2, &xmmDst3));
d3788 2
a3789 12
            w -= 8;
            dst += 8;
            src += 8;
        }

        while (w--)
        {
            s = *src++;
            d = *dst;

            *dst++ = fbCompositeSrc_8888x0565pixel (s, d);
        }
d3792 1
a3792 1
    _mm_empty();
d3795 2
a3796 2
/* -------------------------------------------------------------------------------------------------
 * fbCompositeSolidMask_nx8x8888
d3800 19
a3818 19
fbCompositeSolidMask_nx8x8888sse2 (pixman_implementation_t *imp,
				   pixman_op_t op,
				  pixman_image_t * pSrc,
				  pixman_image_t * pMask,
				  pixman_image_t * pDst,
				  int32_t      xSrc,
				  int32_t      ySrc,
				  int32_t      xMask,
				  int32_t      yMask,
				  int32_t      xDst,
				  int32_t      yDst,
				  int32_t     width,
				  int32_t     height)
{
    uint32_t	src, srca;
    uint32_t	*dstLine, *dst;
    uint8_t	*maskLine, *mask;
    int	dstStride, maskStride;
    uint16_t	w;
d3821 3
a3823 3
    __m128i xmmSrc, xmmAlpha, xmmDef;
    __m128i xmmDst, xmmDstLo, xmmDstHi;
    __m128i xmmMask, xmmMaskLo, xmmMaskHi;
d3825 1
a3825 1
    __m64 mmxSrc, mmxAlpha, mmxMask, mmxDest;
d3827 1
a3827 1
    fbComposeGetSolid(pSrc, src, pDst->bits.format);
d3833 10
a3842 8
    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);
    fbComposeGetStart (pMask, xMask, yMask, uint8_t, maskStride, maskLine, 1);

    xmmDef = createMask_2x32_128 (src, src);
    xmmSrc = expandPixel_32_1x128 (src);
    xmmAlpha = expandAlpha_1x128 (xmmSrc);
    mmxSrc   = _mm_movepi64_pi64 (xmmSrc);
    mmxAlpha = _mm_movepi64_pi64 (xmmAlpha);
d3846 92
a3937 87
        dst = dstLine;
        dstLine += dstStride;
        mask = maskLine;
        maskLine += maskStride;
        w = width;

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)mask);
        cachePrefetch ((__m128i*)dst);

        while (w && (unsigned long)dst & 15)
        {
            uint8_t m = *mask++;

            if (m)
            {
                d = *dst;
                mmxMask = expandPixel_8_1x64 (m);
                mmxDest = unpack_32_1x64 (d);

                *dst = pack_1x64_32 (inOver_1x64 (&mmxSrc,
                                                  &mmxAlpha,
                                                  &mmxMask,
                                                  &mmxDest));
            }

            w--;
            dst++;
        }

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)mask);
        cachePrefetch ((__m128i*)dst);

        while (w >= 4)
        {
            /* fill cache line with next memory */
            cachePrefetchNext ((__m128i*)mask);
            cachePrefetchNext ((__m128i*)dst);

            m = *((uint32_t*)mask);

            if (srca == 0xff && m == 0xffffffff)
            {
                save128Aligned ((__m128i*)dst, xmmDef);
            }
            else if (m)
            {
                xmmDst = load128Aligned ((__m128i*) dst);
                xmmMask = unpack_32_1x128 (m);
                xmmMask = _mm_unpacklo_epi8 (xmmMask, _mm_setzero_si128());

                /* Unpacking */
                unpack_128_2x128 (xmmDst, &xmmDstLo, &xmmDstHi);
                unpack_128_2x128 (xmmMask, &xmmMaskLo, &xmmMaskHi);

                expandAlphaRev_2x128 (xmmMaskLo, xmmMaskHi, &xmmMaskLo, &xmmMaskHi);

                inOver_2x128 (&xmmSrc, &xmmSrc, &xmmAlpha, &xmmAlpha, &xmmMaskLo, &xmmMaskHi, &xmmDstLo, &xmmDstHi);

                save128Aligned ((__m128i*)dst, pack_2x128_128 (xmmDstLo, xmmDstHi));
            }

            w -= 4;
            dst += 4;
            mask += 4;
        }

        while (w)
        {
            uint8_t m = *mask++;

            if (m)
            {
                d = *dst;
                mmxMask = expandPixel_8_1x64 (m);
                mmxDest = unpack_32_1x64 (d);

                *dst = pack_1x64_32 (inOver_1x64 (&mmxSrc,
                                                  &mmxAlpha,
                                                  &mmxMask,
                                                  &mmxDest));
            }

            w--;
            dst++;
        }
d3940 1
a3940 1
    _mm_empty();
d3943 2
a3944 2
/* -------------------------------------------------------------------------------------------------
 * fbCompositeSolidMask_nx8x8888
d3948 8
a3955 8
pixmanFillsse2 (uint32_t *bits,
		 int stride,
		 int bpp,
		 int x,
		 int y,
		 int width,
		 int height,
		 uint32_t data)
d3957 2
a3958 2
    uint32_t	byte_width;
    uint8_t	    *byte_line;
d3960 1
a3960 1
    __m128i xmmDef;
d3970 4
a3973 4
        stride = stride * (int) sizeof (uint32_t) / 2;
        byte_line = (uint8_t *)(((uint16_t *)bits) + stride * y + x);
        byte_width = 2 * width;
        stride *= 2;
d3977 4
a3980 4
        stride = stride * (int) sizeof (uint32_t) / 4;
        byte_line = (uint8_t *)(((uint32_t *)bits) + stride * y + x);
        byte_width = 4 * width;
        stride *= 4;
d3983 2
a3984 2
    cachePrefetch ((__m128i*)byte_line);
    xmmDef = createMask_2x32_128 (data, data);
d3988 90
a4077 90
        int w;
        uint8_t *d = byte_line;
        byte_line += stride;
        w = byte_width;


        cachePrefetchNext ((__m128i*)d);

        while (w >= 2 && ((unsigned long)d & 3))
        {
            *(uint16_t *)d = data;
            w -= 2;
            d += 2;
        }

        while (w >= 4 && ((unsigned long)d & 15))
        {
            *(uint32_t *)d = data;

            w -= 4;
            d += 4;
        }

        cachePrefetchNext ((__m128i*)d);

        while (w >= 128)
        {
            cachePrefetch (((__m128i*)d) + 12);

            save128Aligned ((__m128i*)(d),     xmmDef);
            save128Aligned ((__m128i*)(d+16),  xmmDef);
            save128Aligned ((__m128i*)(d+32),  xmmDef);
            save128Aligned ((__m128i*)(d+48),  xmmDef);
            save128Aligned ((__m128i*)(d+64),  xmmDef);
            save128Aligned ((__m128i*)(d+80),  xmmDef);
            save128Aligned ((__m128i*)(d+96),  xmmDef);
            save128Aligned ((__m128i*)(d+112), xmmDef);

            d += 128;
            w -= 128;
        }

        if (w >= 64)
        {
            cachePrefetch (((__m128i*)d) + 8);

            save128Aligned ((__m128i*)(d),     xmmDef);
            save128Aligned ((__m128i*)(d+16),  xmmDef);
            save128Aligned ((__m128i*)(d+32),  xmmDef);
            save128Aligned ((__m128i*)(d+48),  xmmDef);

            d += 64;
            w -= 64;
        }

        cachePrefetchNext ((__m128i*)d);

        if (w >= 32)
        {
            save128Aligned ((__m128i*)(d),     xmmDef);
            save128Aligned ((__m128i*)(d+16),  xmmDef);

            d += 32;
            w -= 32;
        }

        if (w >= 16)
        {
            save128Aligned ((__m128i*)(d),     xmmDef);

            d += 16;
            w -= 16;
        }

        cachePrefetchNext ((__m128i*)d);

        while (w >= 4)
        {
            *(uint32_t *)d = data;

            w -= 4;
            d += 4;
        }

        if (w >= 2)
        {
            *(uint16_t *)d = data;
            w -= 2;
            d += 2;
        }
d4080 1
a4080 1
    _mm_empty();
d4085 20
a4104 20
fbCompositeSolidMaskSrc_nx8x8888sse2 (pixman_implementation_t *imp,
				      pixman_op_t op,
				     pixman_image_t * pSrc,
				     pixman_image_t * pMask,
				     pixman_image_t * pDst,
				     int32_t      xSrc,
				     int32_t      ySrc,
				     int32_t      xMask,
				     int32_t      yMask,
				     int32_t      xDst,
				     int32_t      yDst,
				     int32_t     width,
				     int32_t     height)
{
    uint32_t	src, srca;
    uint32_t	*dstLine, *dst;
    uint8_t	*maskLine, *mask;
    int	dstStride, maskStride;
    uint16_t	w;
    uint32_t    m;
d4106 2
a4107 2
    __m128i xmmSrc, xmmDef;
    __m128i xmmMask, xmmMaskLo, xmmMaskHi;
d4109 1
a4109 1
    fbComposeGetSolid(pSrc, src, pDst->bits.format);
d4114 4
a4117 4
        pixmanFillsse2 (pDst->bits.bits, pDst->bits.rowstride,
                        PIXMAN_FORMAT_BPP (pDst->bits.format),
                        xDst, yDst, width, height, 0);
        return;
d4120 4
a4123 2
    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);
    fbComposeGetStart (pMask, xMask, yMask, uint8_t, maskStride, maskLine, 1);
d4125 2
a4126 2
    xmmDef = createMask_2x32_128 (src, src);
    xmmSrc = expandPixel_32_1x128 (src);
d4130 91
a4220 83
        dst = dstLine;
        dstLine += dstStride;
        mask = maskLine;
        maskLine += maskStride;
        w = width;

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)mask);
        cachePrefetch ((__m128i*)dst);

        while (w && (unsigned long)dst & 15)
        {
            uint8_t m = *mask++;

            if (m)
            {
                *dst = pack_1x64_32 (pixMultiply_1x64 (_mm_movepi64_pi64 (xmmSrc), expandPixel_8_1x64 (m)));
            }
            else
            {
                *dst = 0;
            }

            w--;
            dst++;
        }

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)mask);
        cachePrefetch ((__m128i*)dst);

        while (w >= 4)
        {
            /* fill cache line with next memory */
            cachePrefetchNext ((__m128i*)mask);
            cachePrefetchNext ((__m128i*)dst);

            m = *((uint32_t*)mask);

            if (srca == 0xff && m == 0xffffffff)
            {
                save128Aligned ((__m128i*)dst, xmmDef);
            }
            else if (m)
            {
                xmmMask = unpack_32_1x128 (m);
                xmmMask = _mm_unpacklo_epi8 (xmmMask, _mm_setzero_si128());

                /* Unpacking */
                unpack_128_2x128 (xmmMask, &xmmMaskLo, &xmmMaskHi);

                expandAlphaRev_2x128 (xmmMaskLo, xmmMaskHi, &xmmMaskLo, &xmmMaskHi);

                pixMultiply_2x128 (&xmmSrc, &xmmSrc, &xmmMaskLo, &xmmMaskHi, &xmmMaskLo, &xmmMaskHi);

                save128Aligned ((__m128i*)dst, pack_2x128_128 (xmmMaskLo, xmmMaskHi));
            }
            else
            {
                save128Aligned ((__m128i*)dst, _mm_setzero_si128());
            }

            w -= 4;
            dst += 4;
            mask += 4;
        }

        while (w)
        {
            uint8_t m = *mask++;

            if (m)
            {
                *dst = pack_1x64_32 (pixMultiply_1x64 (_mm_movepi64_pi64 (xmmSrc), expandPixel_8_1x64 (m)));
            }
            else
            {
                *dst = 0;
            }

            w--;
            dst++;
        }
d4223 1
a4223 1
    _mm_empty();
d4226 2
a4227 2
/* -------------------------------------------------------------------------------------------------
 * fbCompositeSolidMask_nx8x0565
d4231 19
a4249 19
fbCompositeSolidMask_nx8x0565sse2 (pixman_implementation_t *imp,
				   pixman_op_t op,
				  pixman_image_t * pSrc,
				  pixman_image_t * pMask,
				  pixman_image_t * pDst,
				  int32_t      xSrc,
				  int32_t      ySrc,
				  int32_t      xMask,
				  int32_t      yMask,
				  int32_t      xDst,
				  int32_t      yDst,
				  int32_t     width,
				  int32_t     height)
{
    uint32_t	src, srca;
    uint16_t	*dstLine, *dst, d;
    uint8_t	*maskLine, *mask;
    int	dstStride, maskStride;
    uint16_t	w;
d4251 1
a4251 1
    __m64 mmxSrc, mmxAlpha, mmxMask, mmxDest;
d4253 3
a4255 3
    __m128i xmmSrc, xmmAlpha;
    __m128i xmmMask, xmmMaskLo, xmmMaskHi;
    __m128i xmmDst, xmmDst0, xmmDst1, xmmDst2, xmmDst3;
d4257 1
a4257 1
    fbComposeGetSolid(pSrc, src, pDst->bits.format);
d4263 9
a4271 7
    fbComposeGetStart (pDst, xDst, yDst, uint16_t, dstStride, dstLine, 1);
    fbComposeGetStart (pMask, xMask, yMask, uint8_t, maskStride, maskLine, 1);

    xmmSrc = expandPixel_32_1x128 (src);
    xmmAlpha = expandAlpha_1x128 (xmmSrc);
    mmxSrc = _mm_movepi64_pi64 (xmmSrc);
    mmxAlpha = _mm_movepi64_pi64 (xmmAlpha);
d4275 110
a4384 98
        dst = dstLine;
        dstLine += dstStride;
        mask = maskLine;
        maskLine += maskStride;
        w = width;

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)mask);
        cachePrefetch ((__m128i*)dst);

        while (w && (unsigned long)dst & 15)
        {
            m = *mask++;

            if (m)
            {
                d = *dst;
                mmxMask = expandAlphaRev_1x64 (unpack_32_1x64 (m));
                mmxDest = expand565_16_1x64 (d);

                *dst = pack565_32_16 (pack_1x64_32 (inOver_1x64 (&mmxSrc,
                                                                 &mmxAlpha,
                                                                 &mmxMask,
                                                                 &mmxDest)));
            }

            w--;
            dst++;
        }

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)mask);
        cachePrefetch ((__m128i*)dst);

        while (w >= 8)
        {
            /* fill cache line with next memory */
            cachePrefetchNext ((__m128i*)mask);
            cachePrefetchNext ((__m128i*)dst);

            xmmDst = load128Aligned ((__m128i*) dst);
            unpack565_128_4x128 (xmmDst, &xmmDst0, &xmmDst1, &xmmDst2, &xmmDst3);

            m = *((uint32_t*)mask);
            mask += 4;

            if (m)
            {
                xmmMask = unpack_32_1x128 (m);
                xmmMask = _mm_unpacklo_epi8 (xmmMask, _mm_setzero_si128());

                /* Unpacking */
                unpack_128_2x128 (xmmMask, &xmmMaskLo, &xmmMaskHi);

                expandAlphaRev_2x128 (xmmMaskLo, xmmMaskHi, &xmmMaskLo, &xmmMaskHi);
                inOver_2x128 (&xmmSrc, &xmmSrc, &xmmAlpha, &xmmAlpha, &xmmMaskLo, &xmmMaskHi, &xmmDst0, &xmmDst1);
            }

            m = *((uint32_t*)mask);
            mask += 4;

            if (m)
            {
                xmmMask = unpack_32_1x128 (m);
                xmmMask = _mm_unpacklo_epi8 (xmmMask, _mm_setzero_si128());

                /* Unpacking */
                unpack_128_2x128 (xmmMask, &xmmMaskLo, &xmmMaskHi);

                expandAlphaRev_2x128 (xmmMaskLo, xmmMaskHi, &xmmMaskLo, &xmmMaskHi);
                inOver_2x128 (&xmmSrc, &xmmSrc, &xmmAlpha, &xmmAlpha, &xmmMaskLo, &xmmMaskHi, &xmmDst2, &xmmDst3);
            }

            save128Aligned ((__m128i*)dst, pack565_4x128_128 (&xmmDst0, &xmmDst1, &xmmDst2, &xmmDst3));

            w -= 8;
            dst += 8;
        }

        while (w)
        {
            m = *mask++;

            if (m)
            {
                d = *dst;
                mmxMask = expandAlphaRev_1x64 (unpack_32_1x64 (m));
                mmxDest = expand565_16_1x64 (d);

                *dst = pack565_32_16 (pack_1x64_32 (inOver_1x64 (&mmxSrc,
                                                                 &mmxAlpha,
                                                                 &mmxMask,
                                                                 &mmxDest)));
            }

            w--;
            dst++;
        }
d4387 1
a4387 1
    _mm_empty();
d4390 2
a4391 2
/* -------------------------------------------------------------------------------------------------
 * fbCompositeSrc_8888RevNPx0565
d4395 19
a4413 19
fbCompositeSrc_8888RevNPx0565sse2 (pixman_implementation_t *imp,
				   pixman_op_t op,
				  pixman_image_t * pSrc,
				  pixman_image_t * pMask,
				  pixman_image_t * pDst,
				  int32_t      xSrc,
				  int32_t      ySrc,
				  int32_t      xMask,
				  int32_t      yMask,
				  int32_t      xDst,
				  int32_t      yDst,
				  int32_t     width,
				  int32_t     height)
{
    uint16_t	*dstLine, *dst, d;
    uint32_t	*srcLine, *src, s;
    int		dstStride, srcStride;
    uint16_t	w;
    uint32_t    opaque, zero;
d4416 2
a4417 2
    __m128i xmmSrc, xmmSrcLo, xmmSrcHi;
    __m128i xmmDst, xmmDst0, xmmDst1, xmmDst2, xmmDst3;
d4419 4
a4422 2
    fbComposeGetStart (pDst, xDst, yDst, uint16_t, dstStride, dstLine, 1);
    fbComposeGetStart (pSrc, xSrc, ySrc, uint32_t, srcStride, srcLine, 1);
d4430 1
a4430 1
    assert (pSrc->pDrawable == pMask->pDrawable);
d4435 96
a4530 85
        dst = dstLine;
        dstLine += dstStride;
        src = srcLine;
        srcLine += srcStride;
        w = width;

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)src);
        cachePrefetch ((__m128i*)dst);

        while (w && (unsigned long)dst & 15)
        {
            s = *src++;
            d = *dst;

            ms = unpack_32_1x64 (s);

            *dst++ = pack565_32_16 (pack_1x64_32 (overRevNonPre_1x64(ms, expand565_16_1x64 (d))));
            w--;
        }

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)src);
        cachePrefetch ((__m128i*)dst);

        while (w >= 8)
        {
            /* fill cache line with next memory */
            cachePrefetchNext ((__m128i*)src);
            cachePrefetchNext ((__m128i*)dst);

            /* First round */
            xmmSrc = load128Unaligned((__m128i*)src);
            xmmDst = load128Aligned  ((__m128i*)dst);

            opaque = isOpaque (xmmSrc);
	    zero = isZero (xmmSrc);

	    unpack565_128_4x128 (xmmDst, &xmmDst0, &xmmDst1, &xmmDst2, &xmmDst3);
            unpack_128_2x128 (xmmSrc, &xmmSrcLo, &xmmSrcHi);

            /* preload next round*/
            xmmSrc = load128Unaligned((__m128i*)(src+4));
	    
            if (opaque)
            {
                invertColors_2x128 (xmmSrcLo, xmmSrcHi, &xmmDst0, &xmmDst1);
            }
            else if (!zero)
            {
                overRevNonPre_2x128 (xmmSrcLo, xmmSrcHi, &xmmDst0, &xmmDst1);
            }

            /* Second round */
	    opaque = isOpaque (xmmSrc);
	    zero = isZero (xmmSrc);

            unpack_128_2x128 (xmmSrc, &xmmSrcLo, &xmmSrcHi);

            if (opaque)
            {
                invertColors_2x128 (xmmSrcLo, xmmSrcHi, &xmmDst2, &xmmDst3);
            }
            else if (zero)
            {
                overRevNonPre_2x128 (xmmSrcLo, xmmSrcHi, &xmmDst2, &xmmDst3);
            }

            save128Aligned ((__m128i*)dst, pack565_4x128_128 (&xmmDst0, &xmmDst1, &xmmDst2, &xmmDst3));

            w -= 8;
            src += 8;
            dst += 8;
        }

        while (w)
        {
            s = *src++;
            d = *dst;

            ms = unpack_32_1x64 (s);

            *dst++ = pack565_32_16 (pack_1x64_32 (overRevNonPre_1x64(ms, expand565_16_1x64 (d))));
            w--;
        }
d4533 1
a4533 1
    _mm_empty();
d4536 2
a4537 4
/* "8888RevNP" is GdkPixbuf's format: ABGR, non premultiplied */

/* -------------------------------------------------------------------------------------------------
 * fbCompositeSrc_8888RevNPx8888
d4541 19
a4559 19
fbCompositeSrc_8888RevNPx8888sse2 (pixman_implementation_t *imp,
				   pixman_op_t op,
				  pixman_image_t * pSrc,
				  pixman_image_t * pMask,
				  pixman_image_t * pDst,
				  int32_t      xSrc,
				  int32_t      ySrc,
				  int32_t      xMask,
				  int32_t      yMask,
				  int32_t      xDst,
				  int32_t      yDst,
				  int32_t     width,
				  int32_t     height)
{
    uint32_t	*dstLine, *dst, d;
    uint32_t	*srcLine, *src, s;
    int	dstStride, srcStride;
    uint16_t	w;
    uint32_t    opaque, zero;
d4561 2
a4562 2
    __m128i xmmSrcLo, xmmSrcHi;
    __m128i xmmDstLo, xmmDstHi;
d4564 4
a4567 2
    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);
    fbComposeGetStart (pSrc, xSrc, ySrc, uint32_t, srcStride, srcLine, 1);
d4575 1
a4575 1
    assert (pSrc->pDrawable == pMask->pDrawable);
d4580 73
a4652 40
        dst = dstLine;
        dstLine += dstStride;
        src = srcLine;
        srcLine += srcStride;
        w = width;

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)src);
        cachePrefetch ((__m128i*)dst);

        while (w && (unsigned long)dst & 15)
        {
            s = *src++;
            d = *dst;

            *dst++ = pack_1x64_32 (overRevNonPre_1x64 (unpack_32_1x64 (s), unpack_32_1x64 (d)));

            w--;
        }

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)src);
        cachePrefetch ((__m128i*)dst);

        while (w >= 4)
        {
            /* fill cache line with next memory */
            cachePrefetchNext ((__m128i*)src);
            cachePrefetchNext ((__m128i*)dst);

            xmmSrcHi = load128Unaligned((__m128i*)src);

            opaque = isOpaque (xmmSrcHi);
	    zero = isZero (xmmSrcHi);

            unpack_128_2x128 (xmmSrcHi, &xmmSrcLo, &xmmSrcHi);

            if (opaque)
            {
                invertColors_2x128( xmmSrcLo, xmmSrcHi, &xmmDstLo, &xmmDstHi);
d4654 2
a4655 27
                save128Aligned ((__m128i*)dst, pack_2x128_128 (xmmDstLo, xmmDstHi));
            }
            else if (!zero)
            {
                xmmDstHi = load128Aligned  ((__m128i*)dst);

                unpack_128_2x128 (xmmDstHi, &xmmDstLo, &xmmDstHi);

                overRevNonPre_2x128 (xmmSrcLo, xmmSrcHi, &xmmDstLo, &xmmDstHi);

                save128Aligned ((__m128i*)dst, pack_2x128_128 (xmmDstLo, xmmDstHi));
            }

            w -= 4;
            dst += 4;
            src += 4;
        }

        while (w)
        {
            s = *src++;
            d = *dst;

            *dst++ = pack_1x64_32 (overRevNonPre_1x64 (unpack_32_1x64 (s), unpack_32_1x64 (d)));

            w--;
        }
d4658 1
a4658 1
    _mm_empty();
d4662 1
a4662 1
 * fbCompositeSolidMask_nx8888x0565C
d4666 18
a4683 18
fbCompositeSolidMask_nx8888x0565Csse2 (pixman_implementation_t *imp,
				       pixman_op_t op,
				      pixman_image_t * pSrc,
				      pixman_image_t * pMask,
				      pixman_image_t * pDst,
				      int32_t      xSrc,
				      int32_t      ySrc,
				      int32_t      xMask,
				      int32_t      yMask,
				      int32_t      xDst,
				      int32_t      yDst,
				      int32_t     width,
				      int32_t     height)
{
    uint32_t	src;
    uint16_t	*dstLine, *dst, d;
    uint32_t	*maskLine, *mask, m;
    int	dstStride, maskStride;
d4685 1
a4685 1
    uint32_t packCmp;
d4687 3
a4689 3
    __m128i xmmSrc, xmmAlpha;
    __m128i xmmMask, xmmMaskLo, xmmMaskHi;
    __m128i xmmDst, xmmDst0, xmmDst1, xmmDst2, xmmDst3;
d4691 1
a4691 1
    __m64 mmxSrc, mmxAlpha, mmxMask, mmxDest;
d4693 1
a4693 1
    fbComposeGetSolid(pSrc, src, pDst->bits.format);
d4696 1
a4696 1
        return;
d4698 9
a4706 7
    fbComposeGetStart (pDst, xDst, yDst, uint16_t, dstStride, dstLine, 1);
    fbComposeGetStart (pMask, xMask, yMask, uint32_t, maskStride, maskLine, 1);

    xmmSrc = expandPixel_32_1x128 (src);
    xmmAlpha = expandAlpha_1x128 (xmmSrc);
    mmxSrc = _mm_movepi64_pi64 (xmmSrc);
    mmxAlpha = _mm_movepi64_pi64 (xmmAlpha);
d4710 107
a4816 96
        w = width;
        mask = maskLine;
        dst = dstLine;
        maskLine += maskStride;
        dstLine += dstStride;

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)mask);
        cachePrefetch ((__m128i*)dst);

        while (w && ((unsigned long)dst & 15))
        {
            m = *(uint32_t *) mask;

            if (m)
            {
                d = *dst;
                mmxMask = unpack_32_1x64 (m);
                mmxDest = expand565_16_1x64 (d);

                *dst = pack565_32_16 (pack_1x64_32 (inOver_1x64 (&mmxSrc,
                                                                 &mmxAlpha,
                                                                 &mmxMask,
                                                                 &mmxDest)));
            }

            w--;
            dst++;
            mask++;
        }

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)mask);
        cachePrefetch ((__m128i*)dst);

        while (w >= 8)
        {
            /* fill cache line with next memory */
            cachePrefetchNext ((__m128i*)mask);
            cachePrefetchNext ((__m128i*)dst);

            /* First round */
            xmmMask = load128Unaligned((__m128i*)mask);
            xmmDst = load128Aligned((__m128i*)dst);

            packCmp = _mm_movemask_epi8 (_mm_cmpeq_epi32 (xmmMask, _mm_setzero_si128()));

            unpack565_128_4x128 (xmmDst, &xmmDst0, &xmmDst1, &xmmDst2, &xmmDst3);
            unpack_128_2x128 (xmmMask, &xmmMaskLo, &xmmMaskHi);

            /* preload next round*/
            xmmMask = load128Unaligned((__m128i*)(mask+4));
            /* preload next round*/

            if (packCmp != 0xffff)
            {
                inOver_2x128(&xmmSrc, &xmmSrc, &xmmAlpha, &xmmAlpha, &xmmMaskLo, &xmmMaskHi, &xmmDst0, &xmmDst1);
            }

            /* Second round */
            packCmp = _mm_movemask_epi8 (_mm_cmpeq_epi32 (xmmMask, _mm_setzero_si128()));

            unpack_128_2x128 (xmmMask, &xmmMaskLo, &xmmMaskHi);

            if (packCmp != 0xffff)
            {
                inOver_2x128(&xmmSrc, &xmmSrc, &xmmAlpha, &xmmAlpha, &xmmMaskLo, &xmmMaskHi, &xmmDst2, &xmmDst3);
            }

            save128Aligned ((__m128i*)dst, pack565_4x128_128 (&xmmDst0, &xmmDst1, &xmmDst2, &xmmDst3));

            w -= 8;
            dst += 8;
            mask += 8;
        }

        while (w)
        {
            m = *(uint32_t *) mask;

            if (m)
            {
                d = *dst;
                mmxMask = unpack_32_1x64 (m);
                mmxDest = expand565_16_1x64 (d);

                *dst = pack565_32_16 (pack_1x64_32 (inOver_1x64 (&mmxSrc,
                                                                 &mmxAlpha,
                                                                 &mmxMask,
                                                                 &mmxDest)));
            }

            w--;
            dst++;
            mask++;
        }
d4822 2
a4823 2
/* -------------------------------------------------------------------------------------------------
 * fbCompositeIn_nx8x8
d4827 29
a4855 24
fbCompositeIn_nx8x8sse2 (pixman_implementation_t *imp,
			 pixman_op_t op,
			pixman_image_t * pSrc,
			pixman_image_t * pMask,
			pixman_image_t * pDst,
			int32_t      xSrc,
			int32_t      ySrc,
			int32_t      xMask,
			int32_t      yMask,
			int32_t      xDst,
			int32_t      yDst,
			int32_t     width,
			int32_t     height)
{
    uint8_t	*dstLine, *dst;
    uint8_t	*maskLine, *mask;
    int	dstStride, maskStride;
    uint16_t	w, d, m;
    uint32_t	src;
    uint8_t	sa;

    __m128i xmmAlpha;
    __m128i xmmMask, xmmMaskLo, xmmMaskHi;
    __m128i xmmDst, xmmDstLo, xmmDstHi;
d4857 1
a4857 4
    fbComposeGetStart (pDst, xDst, yDst, uint8_t, dstStride, dstLine, 1);
    fbComposeGetStart (pMask, xMask, yMask, uint8_t, maskStride, maskLine, 1);

    fbComposeGetSolid(pSrc, src, pDst->bits.format);
a4859 2
    if (sa == 0)
        return;
d4861 1
a4861 1
    xmmAlpha = expandAlpha_1x128 (expandPixel_32_1x128 (src));
d4865 67
a4931 55
        dst = dstLine;
        dstLine += dstStride;
        mask = maskLine;
        maskLine += maskStride;
        w = width;

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)mask);
        cachePrefetch ((__m128i*)dst);

        while (w && ((unsigned long)dst & 15))
        {
            m = (uint32_t) *mask++;
            d = (uint32_t) *dst;

            *dst++ = (uint8_t) pack_1x64_32 (pixMultiply_1x64 (pixMultiply_1x64 (_mm_movepi64_pi64 (xmmAlpha), unpack_32_1x64 (m)),
                                                               unpack_32_1x64 (d)));
            w--;
        }

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)mask);
        cachePrefetch ((__m128i*)dst);

        while (w >= 16)
        {
            /* fill cache line with next memory */
            cachePrefetchNext ((__m128i*)mask);
            cachePrefetchNext ((__m128i*)dst);

            xmmMask = load128Unaligned((__m128i*)mask);
            xmmDst = load128Aligned((__m128i*)dst);

            unpack_128_2x128 (xmmMask, &xmmMaskLo, &xmmMaskHi);
            unpack_128_2x128 (xmmDst, &xmmDstLo, &xmmDstHi);

            pixMultiply_2x128 (&xmmAlpha, &xmmAlpha, &xmmMaskLo, &xmmMaskHi, &xmmMaskLo, &xmmMaskHi);
            pixMultiply_2x128 (&xmmMaskLo, &xmmMaskHi, &xmmDstLo, &xmmDstHi, &xmmDstLo, &xmmDstHi);

            save128Aligned ((__m128i*)dst, pack_2x128_128 (xmmDstLo, xmmDstHi));

            mask += 16;
            dst += 16;
            w -= 16;
        }

        while (w)
        {
            m = (uint32_t) *mask++;
            d = (uint32_t) *dst;

            *dst++ = (uint8_t) pack_1x64_32 (pixMultiply_1x64 (pixMultiply_1x64 (_mm_movepi64_pi64 (xmmAlpha), unpack_32_1x64 (m)),
                                                               unpack_32_1x64 (d)));
            w--;
        }
d4934 1
a4934 1
    _mm_empty();
d4937 2
a4938 2
/* -------------------------------------------------------------------------------------------------
 * fbCompositeIn_8x8
d4942 19
a4960 19
fbCompositeIn_8x8sse2 (pixman_implementation_t *imp,
		       pixman_op_t op,
		      pixman_image_t * pSrc,
		      pixman_image_t * pMask,
		      pixman_image_t * pDst,
		      int32_t      xSrc,
		      int32_t      ySrc,
		      int32_t      xMask,
		      int32_t      yMask,
		      int32_t      xDst,
		      int32_t      yDst,
		      int32_t     width,
		      int32_t     height)
{
    uint8_t	*dstLine, *dst;
    uint8_t	*srcLine, *src;
    int	srcStride, dstStride;
    uint16_t	w;
    uint32_t    s, d;
d4962 2
a4963 2
    __m128i xmmSrc, xmmSrcLo, xmmSrcHi;
    __m128i xmmDst, xmmDstLo, xmmDstHi;
d4965 4
a4968 2
    fbComposeGetStart (pDst, xDst, yDst, uint8_t, dstStride, dstLine, 1);
    fbComposeGetStart (pSrc, xSrc, ySrc, uint8_t, srcStride, srcLine, 1);
d4972 58
a5029 52
        dst = dstLine;
        dstLine += dstStride;
        src = srcLine;
        srcLine += srcStride;
        w = width;

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)src);
        cachePrefetch ((__m128i*)dst);

        while (w && ((unsigned long)dst & 15))
        {
            s = (uint32_t) *src++;
            d = (uint32_t) *dst;

            *dst++ = (uint8_t) pack_1x64_32 (pixMultiply_1x64 (unpack_32_1x64 (s),unpack_32_1x64 (d)));
            w--;
        }

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)src);
        cachePrefetch ((__m128i*)dst);

        while (w >= 16)
        {
            /* fill cache line with next memory */
            cachePrefetchNext ((__m128i*)src);
            cachePrefetchNext ((__m128i*)dst);

            xmmSrc = load128Unaligned((__m128i*)src);
            xmmDst = load128Aligned((__m128i*)dst);

            unpack_128_2x128 (xmmSrc, &xmmSrcLo, &xmmSrcHi);
            unpack_128_2x128 (xmmDst, &xmmDstLo, &xmmDstHi);

            pixMultiply_2x128 (&xmmSrcLo, &xmmSrcHi, &xmmDstLo, &xmmDstHi, &xmmDstLo, &xmmDstHi);

            save128Aligned ((__m128i*)dst, pack_2x128_128 (xmmDstLo, xmmDstHi));

            src += 16;
            dst += 16;
            w -= 16;
        }

        while (w)
        {
            s = (uint32_t) *src++;
            d = (uint32_t) *dst;

            *dst++ = (uint8_t) pack_1x64_32 (pixMultiply_1x64 (unpack_32_1x64 (s),unpack_32_1x64 (d)));
            w--;
        }
d5035 2
a5036 2
/* -------------------------------------------------------------------------------------------------
 * fbCompositeSrcAdd_8888x8x8
d5040 20
a5059 20
fbCompositeSrcAdd_8888x8x8sse2 (pixman_implementation_t *imp,
				pixman_op_t op,
			       pixman_image_t * pSrc,
			       pixman_image_t * pMask,
			       pixman_image_t * pDst,
			       int32_t      xSrc,
			       int32_t      ySrc,
			       int32_t      xMask,
			       int32_t      yMask,
			       int32_t      xDst,
			       int32_t      yDst,
			       int32_t     width,
			       int32_t     height)
{
    uint8_t	*dstLine, *dst;
    uint8_t	*maskLine, *mask;
    int	dstStride, maskStride;
    uint16_t	w;
    uint32_t	src;
    uint8_t	sa;
d5062 8
a5069 6
    __m128i xmmAlpha;
    __m128i xmmMask, xmmMaskLo, xmmMaskHi;
    __m128i xmmDst, xmmDstLo, xmmDstHi;

    fbComposeGetStart (pDst, xDst, yDst, uint8_t, dstStride, dstLine, 1);
    fbComposeGetStart (pMask, xMask, yMask, uint8_t, maskStride, maskLine, 1);
d5071 1
a5071 1
    fbComposeGetSolid(pSrc, src, pDst->bits.format);
a5073 2
    if (sa == 0)
        return;
d5075 1
a5075 1
    xmmAlpha = expandAlpha_1x128 (expandPixel_32_1x128 (src));
d5079 67
a5145 57
        dst = dstLine;
        dstLine += dstStride;
        mask = maskLine;
        maskLine += maskStride;
        w = width;

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)mask);
        cachePrefetch ((__m128i*)dst);

        while (w && ((unsigned long)dst & 15))
        {
            m = (uint32_t) *mask++;
            d = (uint32_t) *dst;

            *dst++ = (uint8_t) pack_1x64_32 (_mm_adds_pu16 (pixMultiply_1x64 (_mm_movepi64_pi64 (xmmAlpha), unpack_32_1x64 (m)),
                                                                              unpack_32_1x64 (d)));
            w--;
        }

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)mask);
        cachePrefetch ((__m128i*)dst);

        while (w >= 16)
        {
            /* fill cache line with next memory */
            cachePrefetchNext ((__m128i*)mask);
            cachePrefetchNext ((__m128i*)dst);

            xmmMask = load128Unaligned((__m128i*)mask);
            xmmDst = load128Aligned((__m128i*)dst);

            unpack_128_2x128 (xmmMask, &xmmMaskLo, &xmmMaskHi);
            unpack_128_2x128 (xmmDst, &xmmDstLo, &xmmDstHi);

            pixMultiply_2x128 (&xmmAlpha, &xmmAlpha, &xmmMaskLo, &xmmMaskHi, &xmmMaskLo, &xmmMaskHi);

            xmmDstLo = _mm_adds_epu16 (xmmMaskLo, xmmDstLo);
            xmmDstHi = _mm_adds_epu16 (xmmMaskHi, xmmDstHi);

            save128Aligned ((__m128i*)dst, pack_2x128_128 (xmmDstLo, xmmDstHi));

            mask += 16;
            dst += 16;
            w -= 16;
        }

        while (w)
        {
            m = (uint32_t) *mask++;
            d = (uint32_t) *dst;

            *dst++ = (uint8_t) pack_1x64_32 (_mm_adds_pu16 (pixMultiply_1x64 (_mm_movepi64_pi64 (xmmAlpha), unpack_32_1x64 (m)),
                                                                              unpack_32_1x64 (d)));
            w--;
        }
d5148 1
a5148 1
    _mm_empty();
d5151 2
a5152 2
/* -------------------------------------------------------------------------------------------------
 * fbCompositeSrcAdd_8000x8000
d5156 19
a5174 19
fbCompositeSrcAdd_8000x8000sse2 (pixman_implementation_t *imp,
				 pixman_op_t op,
				pixman_image_t * pSrc,
				pixman_image_t * pMask,
				pixman_image_t * pDst,
				int32_t      xSrc,
				int32_t      ySrc,
				int32_t      xMask,
				int32_t      yMask,
				int32_t      xDst,
				int32_t      yDst,
				int32_t     width,
				int32_t     height)
{
    uint8_t	*dstLine, *dst;
    uint8_t	*srcLine, *src;
    int	dstStride, srcStride;
    uint16_t	w;
    uint16_t	t;
d5176 4
a5179 2
    fbComposeGetStart (pSrc, xSrc, ySrc, uint8_t, srcStride, srcLine, 1);
    fbComposeGetStart (pDst, xDst, yDst, uint8_t, dstStride, dstLine, 1);
d5183 2
a5184 2
        dst = dstLine;
        src = srcLine;
d5186 30
a5215 30
        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)src);
        cachePrefetch ((__m128i*)dst);

        dstLine += dstStride;
        srcLine += srcStride;
        w = width;

        /* Small head */
        while (w && (unsigned long)dst & 3)
        {
            t = (*dst) + (*src++);
            *dst++ = t | (0 - (t >> 8));
            w--;
        }

        coreCombineAddUsse2 ((uint32_t*)dst, (uint32_t*)src, NULL, w >> 2);

        /* Small tail */
        dst += w & 0xfffc;
        src += w & 0xfffc;

        w &= 3;

        while (w)
        {
            t = (*dst) + (*src++);
            *dst++ = t | (0 - (t >> 8));
            w--;
        }
d5218 1
a5218 1
    _mm_empty();
d5221 2
a5222 2
/* -------------------------------------------------------------------------------------------------
 * fbCompositeSrcAdd_8888x8888
d5225 22
a5246 20
fbCompositeSrcAdd_8888x8888sse2 (pixman_implementation_t *imp,
				 pixman_op_t 	op,
				pixman_image_t *	pSrc,
				pixman_image_t *	pMask,
				pixman_image_t *	 pDst,
				int32_t		 xSrc,
				int32_t      ySrc,
				int32_t      xMask,
				int32_t      yMask,
				int32_t      xDst,
				int32_t      yDst,
				int32_t     width,
				int32_t     height)
{
    uint32_t	*dstLine, *dst;
    uint32_t	*srcLine, *src;
    int	dstStride, srcStride;

    fbComposeGetStart (pSrc, xSrc, ySrc, uint32_t, srcStride, srcLine, 1);
    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);
d5250 4
a5253 4
        dst = dstLine;
        dstLine += dstStride;
        src = srcLine;
        srcLine += srcStride;
d5255 1
a5255 1
        coreCombineAddUsse2 (dst, src, NULL, width);
d5258 1
a5258 1
    _mm_empty();
d5262 1
a5262 1
 * fbCompositeCopyAreasse2
d5266 16
a5281 13
pixmanBltsse2 (uint32_t *src_bits,
	       uint32_t *dst_bits,
	       int src_stride,
	       int dst_stride,
	       int src_bpp,
	       int dst_bpp,
	       int src_x, int src_y,
	       int dst_x, int dst_y,
	       int width, int height)
{
    uint8_t *	src_bytes;
    uint8_t *	dst_bytes;
    int		byte_width;
d5284 1
a5284 1
        return FALSE;
d5288 7
a5294 7
        src_stride = src_stride * (int) sizeof (uint32_t) / 2;
        dst_stride = dst_stride * (int) sizeof (uint32_t) / 2;
        src_bytes = (uint8_t *)(((uint16_t *)src_bits) + src_stride * (src_y) + (src_x));
        dst_bytes = (uint8_t *)(((uint16_t *)dst_bits) + dst_stride * (dst_y) + (dst_x));
        byte_width = 2 * width;
        src_stride *= 2;
        dst_stride *= 2;
d5298 7
a5304 7
        src_stride = src_stride * (int) sizeof (uint32_t) / 4;
        dst_stride = dst_stride * (int) sizeof (uint32_t) / 4;
        src_bytes = (uint8_t *)(((uint32_t *)src_bits) + src_stride * (src_y) + (src_x));
        dst_bytes = (uint8_t *)(((uint32_t *)dst_bits) + dst_stride * (dst_y) + (dst_x));
        byte_width = 4 * width;
        src_stride *= 4;
        dst_stride *= 4;
d5308 1
a5308 1
        return FALSE;
d5311 2
a5312 2
    cachePrefetch ((__m128i*)src_bytes);
    cachePrefetch ((__m128i*)dst_bytes);
d5316 84
a5399 84
        int w;
        uint8_t *s = src_bytes;
        uint8_t *d = dst_bytes;
        src_bytes += src_stride;
        dst_bytes += dst_stride;
        w = byte_width;

        cachePrefetchNext ((__m128i*)s);
        cachePrefetchNext ((__m128i*)d);

        while (w >= 2 && ((unsigned long)d & 3))
        {
            *(uint16_t *)d = *(uint16_t *)s;
            w -= 2;
            s += 2;
            d += 2;
        }

        while (w >= 4 && ((unsigned long)d & 15))
        {
            *(uint32_t *)d = *(uint32_t *)s;

            w -= 4;
            s += 4;
            d += 4;
        }

        cachePrefetchNext ((__m128i*)s);
        cachePrefetchNext ((__m128i*)d);

        while (w >= 64)
        {
            __m128i xmm0, xmm1, xmm2, xmm3;

            /* 128 bytes ahead */
            cachePrefetch (((__m128i*)s) + 8);
            cachePrefetch (((__m128i*)d) + 8);

            xmm0 = load128Unaligned ((__m128i*)(s));
            xmm1 = load128Unaligned ((__m128i*)(s+16));
            xmm2 = load128Unaligned ((__m128i*)(s+32));
            xmm3 = load128Unaligned ((__m128i*)(s+48));

            save128Aligned ((__m128i*)(d),    xmm0);
            save128Aligned ((__m128i*)(d+16), xmm1);
            save128Aligned ((__m128i*)(d+32), xmm2);
            save128Aligned ((__m128i*)(d+48), xmm3);

            s += 64;
            d += 64;
            w -= 64;
        }

        cachePrefetchNext ((__m128i*)s);
        cachePrefetchNext ((__m128i*)d);

        while (w >= 16)
        {
            save128Aligned ((__m128i*)d, load128Unaligned ((__m128i*)s) );

            w -= 16;
            d += 16;
            s += 16;
        }

        cachePrefetchNext ((__m128i*)s);
        cachePrefetchNext ((__m128i*)d);

        while (w >= 4)
        {
            *(uint32_t *)d = *(uint32_t *)s;

            w -= 4;
            s += 4;
            d += 4;
        }

        if (w >= 2)
        {
            *(uint16_t *)d = *(uint16_t *)s;
            w -= 2;
            s += 2;
            d += 2;
        }
d5402 1
a5402 1
    _mm_empty();
d5408 21
a5428 21
fbCompositeCopyAreasse2 (pixman_implementation_t *imp,
			 pixman_op_t       op,
			pixman_image_t *	pSrc,
			pixman_image_t *	pMask,
			pixman_image_t *	pDst,
			int32_t		xSrc,
			int32_t		ySrc,
			int32_t		xMask,
			int32_t		yMask,
			int32_t		xDst,
			int32_t		yDst,
			int32_t		width,
			int32_t		height)
{
    pixmanBltsse2 (pSrc->bits.bits,
		    pDst->bits.bits,
		    pSrc->bits.rowstride,
		    pDst->bits.rowstride,
		    PIXMAN_FORMAT_BPP (pSrc->bits.format),
		    PIXMAN_FORMAT_BPP (pDst->bits.format),
		    xSrc, ySrc, xDst, yDst, width, height);
d5434 19
a5452 19
fbCompositeOver_x888x8x8888sse2 (pixman_implementation_t *imp,
				 pixman_op_t      op,
				pixman_image_t * pSrc,
				pixman_image_t * pMask,
				pixman_image_t * pDst,
				int32_t      xSrc,
				int32_t      ySrc,
				int32_t      xMask,
				int32_t      yMask,
				int32_t      xDst,
				int32_t      yDst,
				int32_t     width,
				int32_t     height)
{
    uint32_t	*src, *srcLine, s;
    uint32_t    *dst, *dstLine, d;
    uint8_t	    *mask, *maskLine;
    uint32_t    m;
    int		 srcStride, maskStride, dstStride;
d5455 10
a5464 7
    __m128i xmmSrc, xmmSrcLo, xmmSrcHi;
    __m128i xmmDst, xmmDstLo, xmmDstHi;
    __m128i xmmMask, xmmMaskLo, xmmMaskHi;

    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);
    fbComposeGetStart (pMask, xMask, yMask, uint8_t, maskStride, maskLine, 1);
    fbComposeGetStart (pSrc, xSrc, ySrc, uint32_t, srcStride, srcLine, 1);
d5468 112
a5579 104
        src = srcLine;
        srcLine += srcStride;
        dst = dstLine;
        dstLine += dstStride;
        mask = maskLine;
        maskLine += maskStride;

        w = width;

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)src);
        cachePrefetch ((__m128i*)dst);
        cachePrefetch ((__m128i*)mask);

        while (w && (unsigned long)dst & 15)
        {
            s = 0xff000000 | *src++;
            m = (uint32_t) *mask++;
            d = *dst;

            __m64 ms = unpack_32_1x64 (s);

            if (m != 0xff)
            {
                ms = inOver_1x64 (ms,
                                  xMask00ff,
                                  expandAlphaRev_1x64 (unpack_32_1x64 (m)),
                                  unpack_32_1x64 (d));
            }

            *dst++ = pack_1x64_32 (ms);
            w--;
        }

        /* call prefetch hint to optimize cache load*/
        cachePrefetch ((__m128i*)src);
        cachePrefetch ((__m128i*)dst);
        cachePrefetch ((__m128i*)mask);

        while (w >= 4)
        {
            /* fill cache line with next memory */
            cachePrefetchNext ((__m128i*)src);
            cachePrefetchNext ((__m128i*)dst);
            cachePrefetchNext ((__m128i*)mask);

            m = *(uint32_t*) mask;
            xmmSrc = _mm_or_si128 (load128Unaligned ((__m128i*)src), Maskff000000);

            if (m == 0xffffffff)
            {
                save128Aligned ((__m128i*)dst, xmmSrc);
            }
            else
            {
                xmmDst = load128Aligned ((__m128i*)dst);

                xmmMask = _mm_unpacklo_epi16 (unpack_32_1x128 (m), _mm_setzero_si128());

                unpack_128_2x128 (xmmSrc, &xmmSrcLo, &xmmSrcHi);
                unpack_128_2x128 (xmmMask, &xmmMaskLo, &xmmMaskHi);
                unpack_128_2x128 (xmmDst, &xmmDstLo, &xmmDstHi);

                expandAlphaRev_2x128 (xmmMaskLo, xmmMaskHi, &xmmMaskLo, &xmmMaskHi);

                inOver_2x128 (xmmSrcLo, xmmSrcHi, Mask00ff, Mask00ff, xmmMaskLo, xmmMaskHi, &xmmDstLo, &xmmDstHi);

                save128Aligned( (__m128i*)dst, pack_2x128_128 (xmmDstLo, xmmDstHi));
            }

            src += 4;
            dst += 4;
            mask += 4;
            w -= 4;
        }

        while (w)
        {
            m = (uint32_t) *mask++;

            if (m)
            {
                s = 0xff000000 | *src;

                if (m == 0xff)
                {
                    *dst = s;
                }
                else
                {
                    d = *dst;

                    *dst = pack_1x64_32 (inOver_1x64 (unpack_32_1x64 (s),
                                                      xMask00ff,
                                                      expandAlphaRev_1x64 (unpack_32_1x64 (m)),
                                                      unpack_32_1x64 (d)));
                }

            }

            src++;
            dst++;
            w--;
        }
d5582 1
a5582 1
    _mm_empty();
d5584 1
d5587 1
a5587 1
static const FastPathInfo sse2_fast_paths[] =
d5589 15
a5603 15
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_r5g6b5,   fbCompositeSolidMask_nx8x0565sse2,     0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_b5g6r5,   fbCompositeSolidMask_nx8x0565sse2,     0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_null,     PIXMAN_a8r8g8b8, fbCompositeSolid_nx8888sse2,           0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_null,     PIXMAN_x8r8g8b8, fbCompositeSolid_nx8888sse2,           0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_null,     PIXMAN_r5g6b5,   fbCompositeSolid_nx0565sse2,           0 },
    { PIXMAN_OP_OVER, PIXMAN_a8r8g8b8, PIXMAN_null,     PIXMAN_a8r8g8b8, fbCompositeSrc_8888x8888sse2,          0 },
    { PIXMAN_OP_OVER, PIXMAN_a8r8g8b8, PIXMAN_null,     PIXMAN_x8r8g8b8, fbCompositeSrc_8888x8888sse2,          0 },
    { PIXMAN_OP_OVER, PIXMAN_a8b8g8r8, PIXMAN_null,     PIXMAN_a8b8g8r8, fbCompositeSrc_8888x8888sse2,          0 },
    { PIXMAN_OP_OVER, PIXMAN_a8b8g8r8, PIXMAN_null,     PIXMAN_x8b8g8r8, fbCompositeSrc_8888x8888sse2,          0 },
    { PIXMAN_OP_OVER, PIXMAN_a8r8g8b8, PIXMAN_null,     PIXMAN_r5g6b5,   fbCompositeSrc_8888x0565sse2,          0 },
    { PIXMAN_OP_OVER, PIXMAN_a8b8g8r8, PIXMAN_null,     PIXMAN_b5g6r5,   fbCompositeSrc_8888x0565sse2,          0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_a8r8g8b8, fbCompositeSolidMask_nx8x8888sse2,     0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_x8r8g8b8, fbCompositeSolidMask_nx8x8888sse2,     0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_a8b8g8r8, fbCompositeSolidMask_nx8x8888sse2,     0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_x8b8g8r8, fbCompositeSolidMask_nx8x8888sse2,     0 },
d5606 4
a5609 4
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8,       PIXMAN_x8r8g8b8, fbCompositeOver_x888x8x8888sse2,       0 },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8,       PIXMAN_a8r8g8b8, fbCompositeOver_x888x8x8888sse2,       0 },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8,       PIXMAN_x8b8g8r8, fbCompositeOver_x888x8x8888sse2,       0 },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8,       PIXMAN_a8r8g8b8, fbCompositeOver_x888x8x8888sse2,       0 },
d5611 47
a5657 46
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8,       PIXMAN_a8r8g8b8, fbCompositeSrc_x888xnx8888sse2,        NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8,       PIXMAN_x8r8g8b8, fbCompositeSrc_x888xnx8888sse2,        NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8,       PIXMAN_a8b8g8r8, fbCompositeSrc_x888xnx8888sse2,        NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8,       PIXMAN_x8b8g8r8, fbCompositeSrc_x888xnx8888sse2,        NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_a8r8g8b8, PIXMAN_a8,       PIXMAN_a8r8g8b8, fbCompositeSrc_8888x8x8888sse2,        NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_a8r8g8b8, PIXMAN_a8,       PIXMAN_x8r8g8b8, fbCompositeSrc_8888x8x8888sse2,        NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_a8b8g8r8, PIXMAN_a8,       PIXMAN_a8b8g8r8, fbCompositeSrc_8888x8x8888sse2,        NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_a8b8g8r8, PIXMAN_a8,       PIXMAN_x8b8g8r8, fbCompositeSrc_8888x8x8888sse2,        NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8r8g8b8, PIXMAN_a8r8g8b8, fbCompositeSolidMask_nx8888x8888Csse2, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8r8g8b8, PIXMAN_x8r8g8b8, fbCompositeSolidMask_nx8888x8888Csse2, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8b8g8r8, PIXMAN_a8b8g8r8, fbCompositeSolidMask_nx8888x8888Csse2, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8b8g8r8, PIXMAN_x8b8g8r8, fbCompositeSolidMask_nx8888x8888Csse2, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8r8g8b8, PIXMAN_r5g6b5,   fbCompositeSolidMask_nx8888x0565Csse2, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8b8g8r8, PIXMAN_b5g6r5,   fbCompositeSolidMask_nx8888x0565Csse2, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8r8g8b8, PIXMAN_a8r8g8b8, fbCompositeSrc_8888RevNPx8888sse2,     NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8b8g8r8, PIXMAN_a8r8g8b8, fbCompositeSrc_8888RevNPx8888sse2,     NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8r8g8b8, PIXMAN_x8r8g8b8, fbCompositeSrc_8888RevNPx8888sse2,     NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8b8g8r8, PIXMAN_x8r8g8b8, fbCompositeSrc_8888RevNPx8888sse2,     NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8r8g8b8, PIXMAN_a8b8g8r8, fbCompositeSrc_8888RevNPx8888sse2,     NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8b8g8r8, PIXMAN_a8b8g8r8, fbCompositeSrc_8888RevNPx8888sse2,     NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8r8g8b8, PIXMAN_x8b8g8r8, fbCompositeSrc_8888RevNPx8888sse2,     NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8b8g8r8, PIXMAN_x8b8g8r8, fbCompositeSrc_8888RevNPx8888sse2,     NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8r8g8b8, PIXMAN_r5g6b5,   fbCompositeSrc_8888RevNPx0565sse2,     NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8b8g8r8, PIXMAN_r5g6b5,   fbCompositeSrc_8888RevNPx0565sse2,     NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8r8g8b8, PIXMAN_b5g6r5,   fbCompositeSrc_8888RevNPx0565sse2,     NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8b8g8r8, PIXMAN_b5g6r5,   fbCompositeSrc_8888RevNPx0565sse2,     NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_null,     PIXMAN_x8r8g8b8, fbCompositeCopyAreasse2,               0 },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_null,     PIXMAN_x8b8g8r8, fbCompositeCopyAreasse2,               0 },

    { PIXMAN_OP_ADD,  PIXMAN_a8,       PIXMAN_null,     PIXMAN_a8,       fbCompositeSrcAdd_8000x8000sse2,       0 },
    { PIXMAN_OP_ADD,  PIXMAN_a8r8g8b8, PIXMAN_null,     PIXMAN_a8r8g8b8, fbCompositeSrcAdd_8888x8888sse2,       0 },
    { PIXMAN_OP_ADD,  PIXMAN_a8b8g8r8, PIXMAN_null,     PIXMAN_a8b8g8r8, fbCompositeSrcAdd_8888x8888sse2,       0 },
    { PIXMAN_OP_ADD,  PIXMAN_solid,    PIXMAN_a8,       PIXMAN_a8,       fbCompositeSrcAdd_8888x8x8sse2,        0 },

    { PIXMAN_OP_SRC, PIXMAN_solid,     PIXMAN_a8,       PIXMAN_a8r8g8b8, fbCompositeSolidMaskSrc_nx8x8888sse2,  0 },
    { PIXMAN_OP_SRC, PIXMAN_solid,     PIXMAN_a8,       PIXMAN_x8r8g8b8, fbCompositeSolidMaskSrc_nx8x8888sse2,  0 },
    { PIXMAN_OP_SRC, PIXMAN_solid,     PIXMAN_a8,       PIXMAN_a8b8g8r8, fbCompositeSolidMaskSrc_nx8x8888sse2,  0 },
    { PIXMAN_OP_SRC, PIXMAN_solid,     PIXMAN_a8,       PIXMAN_x8b8g8r8, fbCompositeSolidMaskSrc_nx8x8888sse2,  0 },
    { PIXMAN_OP_SRC, PIXMAN_a8r8g8b8,  PIXMAN_null,     PIXMAN_a8r8g8b8, fbCompositeCopyAreasse2,               0 },
    { PIXMAN_OP_SRC, PIXMAN_a8b8g8r8,  PIXMAN_null,     PIXMAN_a8b8g8r8, fbCompositeCopyAreasse2,               0 },
    { PIXMAN_OP_SRC, PIXMAN_a8r8g8b8,  PIXMAN_null,     PIXMAN_x8r8g8b8, fbCompositeCopyAreasse2,		0 },
    { PIXMAN_OP_SRC, PIXMAN_a8b8g8r8,  PIXMAN_null,	PIXMAN_x8b8g8r8, fbCompositeCopyAreasse2,		0 },
    { PIXMAN_OP_SRC, PIXMAN_x8r8g8b8,  PIXMAN_null,     PIXMAN_x8r8g8b8, fbCompositeCopyAreasse2,               0 },
    { PIXMAN_OP_SRC, PIXMAN_x8b8g8r8,  PIXMAN_null,     PIXMAN_x8b8g8r8, fbCompositeCopyAreasse2,               0 },
    { PIXMAN_OP_SRC, PIXMAN_r5g6b5,    PIXMAN_null,     PIXMAN_r5g6b5,   fbCompositeCopyAreasse2,               0 },
    { PIXMAN_OP_SRC, PIXMAN_b5g6r5,    PIXMAN_null,     PIXMAN_b5g6r5,   fbCompositeCopyAreasse2,               0 },
d5659 2
a5660 2
    { PIXMAN_OP_IN,  PIXMAN_a8,        PIXMAN_null,     PIXMAN_a8,       fbCompositeIn_8x8sse2,                 0 },
    { PIXMAN_OP_IN,  PIXMAN_solid,     PIXMAN_a8,       PIXMAN_a8,       fbCompositeIn_nx8x8sse2,               0 },
d5667 5
a5671 4
 * 
 * When using SSE2 intrinsics, gcc assumes that the stack is 16 byte
 * aligned. Unfortunately some code, such as Mozilla and Mono contain
 * code that aligns the stack to 4 bytes.
d5681 1
d5683 1
d5686 12
a5697 12
		pixman_op_t     op,
		pixman_image_t *src,
		pixman_image_t *mask,
		pixman_image_t *dest,
		int32_t         src_x,
		int32_t         src_y,
		int32_t         mask_x,
		int32_t         mask_y,
		int32_t         dest_x,
		int32_t         dest_y,
		int32_t        width,
		int32_t        height)
d5700 5
a5704 5
			       op, src, mask, dest,
			       src_x, src_y,
			       mask_x, mask_y,
			       dest_x, dest_y,
			       width, height))
d5710 5
a5714 5
				      src, mask, dest,
				      src_x, src_y,
				      mask_x, mask_y,
				      dest_x, dest_y,
				      width, height);
d5717 1
d5719 1
d5722 16
a5737 13
	  uint32_t *src_bits,
	  uint32_t *dst_bits,
	  int src_stride,
	  int dst_stride,
	  int src_bpp,
	  int dst_bpp,
	  int src_x, int src_y,
	  int dst_x, int dst_y,
	  int width, int height)
{
    if (!pixmanBltsse2 (
	    src_bits, dst_bits, src_stride, dst_stride, src_bpp, dst_bpp,
	    src_x, src_y, dst_x, dst_y, width, height))
d5749 1
d5751 1
d5754 8
a5761 8
	   uint32_t *bits,
	   int stride,
	   int bpp,
	   int x,
	   int y,
	   int width,
	   int height,
	   uint32_t xor)
d5763 1
a5763 1
    if (!pixmanFillsse2 (bits, stride, bpp, x, y, width, height, xor))
d5772 3
d5776 1
a5776 1
_pixman_implementation_create_sse2 (pixman_implementation_t *toplevel)
d5778 2
a5779 2
    pixman_implementation_t *mmx = _pixman_implementation_create_mmx (NULL);
    pixman_implementation_t *imp = _pixman_implementation_create (toplevel, mmx);
d5782 16
a5797 16
    Mask565r  = createMask_2x32_128 (0x00f80000, 0x00f80000);
    Mask565g1 = createMask_2x32_128 (0x00070000, 0x00070000);
    Mask565g2 = createMask_2x32_128 (0x000000e0, 0x000000e0);
    Mask565b  = createMask_2x32_128 (0x0000001f, 0x0000001f);
    MaskRed   = createMask_2x32_128 (0x00f80000, 0x00f80000);
    MaskGreen = createMask_2x32_128 (0x0000fc00, 0x0000fc00);
    MaskBlue  = createMask_2x32_128 (0x000000f8, 0x000000f8);
    Mask565FixRB = createMask_2x32_128 (0x00e000e0, 0x00e000e0);
    Mask565FixG = createMask_2x32_128  (0x0000c000, 0x0000c000);
    Mask0080 = createMask_16_128 (0x0080);
    Mask00ff = createMask_16_128 (0x00ff);
    Mask0101 = createMask_16_128 (0x0101);
    Maskffff = createMask_16_128 (0xffff);
    Maskff000000 = createMask_2x32_128 (0xff000000, 0xff000000);
    MaskAlpha = createMask_2x32_128 (0x00ff0000, 0x00000000);
    
d5799 7
a5805 7
    xMask565rgb = createMask_2x32_64 (0x000001f0, 0x003f001f);
    xMask565Unpack = createMask_2x32_64 (0x00000084, 0x04100840);
    
    xMask0080 = createMask_16_64 (0x0080);
    xMask00ff = createMask_16_64 (0x00ff);
    xMask0101 = createMask_16_64 (0x0101);
    xMaskAlpha = createMask_2x32_64 (0x00ff0000, 0x00000000);
d5807 1
a5807 1
    _mm_empty();
d5810 1
a5810 1
    
d5812 25
a5836 25
    imp->combine_32[PIXMAN_OP_OVER] = sse2CombineOverU;
    imp->combine_32[PIXMAN_OP_OVER_REVERSE] = sse2CombineOverReverseU;
    imp->combine_32[PIXMAN_OP_IN] = sse2CombineInU;
    imp->combine_32[PIXMAN_OP_IN_REVERSE] = sse2CombineInReverseU;
    imp->combine_32[PIXMAN_OP_OUT] = sse2CombineOutU;
    imp->combine_32[PIXMAN_OP_OUT_REVERSE] = sse2CombineOutReverseU;
    imp->combine_32[PIXMAN_OP_ATOP] = sse2CombineAtopU;
    imp->combine_32[PIXMAN_OP_ATOP_REVERSE] = sse2CombineAtopReverseU;
    imp->combine_32[PIXMAN_OP_XOR] = sse2CombineXorU;
    imp->combine_32[PIXMAN_OP_ADD] = sse2CombineAddU;
    
    imp->combine_32[PIXMAN_OP_SATURATE] = sse2CombineSaturateU;
    
    imp->combine_32_ca[PIXMAN_OP_SRC] = sse2CombineSrcC;
    imp->combine_32_ca[PIXMAN_OP_OVER] = sse2CombineOverC;
    imp->combine_32_ca[PIXMAN_OP_OVER_REVERSE] = sse2CombineOverReverseC;
    imp->combine_32_ca[PIXMAN_OP_IN] = sse2CombineInC;
    imp->combine_32_ca[PIXMAN_OP_IN_REVERSE] = sse2CombineInReverseC;
    imp->combine_32_ca[PIXMAN_OP_OUT] = sse2CombineOutC;
    imp->combine_32_ca[PIXMAN_OP_OUT_REVERSE] = sse2CombineOutReverseC;
    imp->combine_32_ca[PIXMAN_OP_ATOP] = sse2CombineAtopC;
    imp->combine_32_ca[PIXMAN_OP_ATOP_REVERSE] = sse2CombineAtopReverseC;
    imp->combine_32_ca[PIXMAN_OP_XOR] = sse2CombineXorC;
    imp->combine_32_ca[PIXMAN_OP_ADD] = sse2CombineAddC;
    
d5840 1
a5840 1
    
@


1.1
log
@pixman 0.12.0. Tested on a full ports build by naddy@@.
@
text
@d36 1
a36 2

#include "pixman-sse2.h"
a39 9
#ifdef _MSC_VER
#undef inline
#define inline __forceinline
#endif

#ifdef __GNUC__
#    define inline __inline__ __attribute__ ((__always_inline__))
#endif

d72 1
a72 1
static inline __m128i
d78 1
a78 1
static inline void
d85 1
a85 1
static inline __m128i
d106 1
a106 1
static inline void
d121 1
a121 1
static inline uint16_t
d127 1
a127 1
static inline __m128i
d133 1
a133 1
static inline __m128i
d149 1
a149 1
static inline __m128i
d155 15
a169 2
static inline uint32_t
packAlpha (__m128i x)
d171 1
a171 3
    return _mm_cvtsi128_si32 (_mm_packus_epi16 (_mm_packus_epi16 (_mm_srli_epi32 (x, 24),
                                                                  _mm_setzero_si128 ()),
                                                _mm_setzero_si128 ()));
d174 1
a174 1
static inline __m128i
d180 1
a180 1
static inline __m128i
d186 1
a186 1
static inline void
d197 1
a197 1
static inline void
d208 1
a208 1
static inline void
d221 1
a221 1
static inline void
d241 1
a241 1
static inline void
d248 1
a248 1
static inline void
d259 1
a259 1
static inline void
d272 1
a272 1
static inline void
d290 1
a290 1
static inline void
d303 1
a303 1
static inline void
d309 1
a309 1
static inline void
d316 1
a316 1
static inline __m128i
d323 2
a324 2
static inline __m128i
load128Unaligned (__m128i* src)
d330 1
a330 1
static inline void
d337 1
a337 1
static inline void
d344 1
a344 1
static inline void
d354 1
a354 1
static inline __m64
d360 1
a360 1
static inline __m64
d366 1
a366 1
static inline __m64
d372 1
a372 1
static inline __m64
d378 1
a378 1
static inline __m64
d386 1
a386 1
static inline __m64
d395 1
a395 1
static inline __m64
d401 1
a401 1
static inline __m64
d407 1
a407 1
static inline __m64
d413 1
a413 1
static inline __m64
d421 1
a421 1
static inline __m64
d432 1
a432 1
static inline uint32_t
d452 1
a452 1
static inline __m64
d474 1
a474 1
static inline uint32_t
d486 1
a486 1
    else if (a)
d495 55
a549 2
static inline void
coreCombineOverUsse2 (uint32_t* pd, const uint32_t* ps, int w)
a550 1
    uint32_t pa;
d560 1
d567 1
a567 1
        s = *ps++;
d570 3
d579 1
d586 1
d589 1
a589 1
        xmmSrcHi = load128Unaligned ((__m128i*) ps);
d591 1
a591 4
        /* Check the alpha channel */
        pa = packAlpha (xmmSrcHi);

        if (pa == 0xffffffff)
d595 1
a595 1
        else if (pa)
d613 2
d620 1
a620 1
        s = *ps++;
d623 3
d630 2
a631 2
static inline void
coreCombineOverReverseUsse2 (uint32_t* pd, const uint32_t* ps, int w)
d642 1
d649 1
a649 1
        s = *ps++;
d653 3
d661 1
d668 1
d671 1
a671 1
        xmmSrcHi = load128Unaligned ((__m128i*) ps);
d687 2
d694 1
a694 1
        s = *ps++;
d697 1
d699 2
d704 1
a704 1
static inline uint32_t
d721 2
a722 2
static inline void
coreCombineInUsse2 (uint32_t* pd, const uint32_t* ps, int w)
d732 1
d736 1
a736 1
        s = *ps++;
d741 3
d749 1
d756 1
d759 1
a759 1
        xmmSrcHi = load128Unaligned ((__m128i*) ps);
d772 2
d778 1
a778 1
        s = *ps++;
d783 3
d789 2
a790 2
static inline void
coreCombineReverseInUsse2 (uint32_t* pd, const uint32_t* ps, int w)
d800 1
d804 1
a804 1
        s = *ps++;
d808 1
d810 2
d817 1
d824 1
d827 1
a827 1
        xmmSrcHi = load128Unaligned ((__m128i*) ps);
d840 2
d846 1
a846 1
        s = *ps++;
d851 3
d857 2
a858 2
static inline void
coreCombineReverseOutUsse2 (uint32_t* pd, const uint32_t* ps, int w)
d863 1
d867 1
a867 1
        uint32_t s = *ps++;
d871 3
d880 1
d890 1
d892 1
a892 1
        xmmSrcHi = load128Unaligned ((__m128i*) ps);
d907 2
d914 1
a914 1
        uint32_t s = *ps++;
d918 3
d925 2
a926 2
static inline void
coreCombineOutUsse2 (uint32_t* pd, const uint32_t* ps, int w)
d931 1
d935 1
a935 1
        uint32_t s = *ps++;
d940 3
d948 1
d958 1
d960 1
a960 1
        xmmSrcHi = load128Unaligned ((__m128i*) ps);
d976 2
d982 1
a982 1
        uint32_t s = *ps++;
d987 3
d993 1
a993 1
static inline uint32_t
d1005 2
a1006 2
static inline void
coreCombineAtopUsse2 (uint32_t* pd, const uint32_t* ps, int w)
d1018 1
d1022 1
a1022 1
        s = *ps++;
d1027 3
d1035 1
d1042 1
d1044 1
a1044 1
        xmmSrcHi = load128Unaligned ((__m128i*) ps);
d1064 2
d1070 1
a1070 1
        s = *ps++;
d1075 3
d1081 1
a1081 1
static inline uint32_t
d1093 2
a1094 2
static inline void
coreCombineReverseAtopUsse2 (uint32_t* pd, const uint32_t* ps, int w)
d1106 1
d1110 1
a1110 1
        s = *ps++;
d1114 1
d1116 2
d1123 1
d1130 1
d1132 1
a1132 1
        xmmSrcHi = load128Unaligned ((__m128i*) ps);
d1152 2
d1158 1
a1158 1
        s = *ps++;
d1162 1
d1164 2
d1169 1
a1169 1
static inline uint32_t
d1181 2
a1182 2
static inline void
coreCombineXorUsse2 (uint32_t* dst, const uint32_t* src, int width)
d1188 2
a1189 1

d1198 1
d1202 1
a1202 1
        s = *ps++;
d1207 3
d1215 1
d1222 1
d1224 1
a1224 1
        xmmSrc = load128Unaligned ((__m128i*) ps);
d1245 2
d1251 1
a1251 1
        s = *ps++;
d1256 3
d1262 2
a1263 2
static inline void
coreCombineAddUsse2 (uint32_t* dst, const uint32_t* src, int width)
d1269 1
d1274 1
d1278 1
a1278 1
        s = *ps++;
d1280 3
d1290 1
d1294 2
d1299 1
d1301 2
d1304 1
a1304 2
                        _mm_adds_epu8( load128Unaligned((__m128i*)ps),
                                       load128Aligned  ((__m128i*)pd)) );
d1307 2
d1314 1
a1314 1
        s = *ps++;
d1316 1
d1318 2
d1323 1
a1323 1
static inline uint32_t
d1339 2
a1340 2
static inline void
coreCombineSaturateUsse2 (uint32_t *pd, const uint32_t *ps, int w)
d1350 1
d1354 1
a1354 1
        s = *ps++;
d1358 3
d1366 1
d1373 1
d1376 1
a1376 1
        xmmSrc = load128Unaligned((__m128i*)ps);
d1384 1
a1384 1
            s = *ps++;
d1387 2
d1390 1
a1390 1
            s = *ps++;
d1393 2
d1396 1
a1396 1
            s = *ps++;
d1399 2
d1402 1
a1402 1
            s = *ps++;
d1405 2
d1414 2
d1423 1
a1423 1
        s = *ps++;
d1426 3
d1432 1
a1432 1
static inline void
d1491 1
a1491 1
static inline uint32_t
d1502 1
a1502 1
static inline void
d1570 1
a1570 1
static inline uint32_t
d1578 1
a1578 1
static inline void
d1647 2
a1648 2
static inline void
coreCombineInCsse2 (uint32_t *pd, uint32_t *ps, uint32_t *pm, int w)
d1718 2
a1719 2
static inline void
coreCombineInReverseCsse2 (uint32_t *pd, uint32_t *ps, uint32_t *pm, int w)
d1791 2
a1792 2
static inline void
coreCombineOutCsse2 (uint32_t *pd, uint32_t *ps, uint32_t *pm, int w)
d1863 2
a1864 2
static inline void
coreCombineOutReverseCsse2 (uint32_t *pd, uint32_t *ps, uint32_t *pm, int w)
d1939 1
a1939 1
static inline uint32_t
d1954 2
a1955 2
static inline void
coreCombineAtopCsse2 (uint32_t *pd, uint32_t *ps, uint32_t *pm, int w)
d2031 1
a2031 1
static inline uint32_t
d2047 2
a2048 2
static inline void
coreCombineReverseAtopCsse2 (uint32_t *pd, uint32_t *ps, uint32_t *pm, int w)
d2124 1
a2124 1
static inline uint32_t
d2141 2
a2142 2
static inline void
coreCombineXorCsse2 (uint32_t *pd, uint32_t *ps, uint32_t *pm, int w)
d2219 2
a2220 2
static inline void
coreCombineAddCsse2 (uint32_t *pd, uint32_t *ps, uint32_t *pm, int w)
d2292 1
a2292 1
static inline __m64
d2298 1
a2298 1
static inline __m128i
d2304 1
a2304 1
static inline __m64
d2310 1
a2310 1
static inline __m128i
d2319 2
a2320 8
sse2CombineMaskU (uint32_t *dst, const uint32_t *src, int width)
{
    coreCombineReverseInUsse2 (dst, src, width);
    _mm_empty();
}

static FASTCALL void
sse2CombineOverU (uint32_t *dst, const uint32_t *src, int width)
d2322 1
a2322 1
    coreCombineOverUsse2 (dst, src, width);
d2327 2
a2328 1
sse2CombineOverReverseU (uint32_t *dst, const uint32_t *src, int width)
d2330 1
a2330 1
    coreCombineOverReverseUsse2 (dst, src, width);
d2335 2
a2336 1
sse2CombineInU (uint32_t *dst, const uint32_t *src, int width)
d2338 1
a2338 1
    coreCombineInUsse2 (dst, src, width);
d2343 2
a2344 1
sse2CombineInReverseU (uint32_t *dst, const uint32_t *src, int width)
d2346 1
a2346 1
    coreCombineReverseInUsse2 (dst, src, width);
d2351 2
a2352 1
sse2CombineOutU (uint32_t *dst, const uint32_t *src, int width)
d2354 1
a2354 1
    coreCombineOutUsse2 (dst, src, width);
d2359 2
a2360 1
sse2CombineOutReverseU (uint32_t *dst, const uint32_t *src, int width)
d2362 1
a2362 1
    coreCombineReverseOutUsse2 (dst, src, width);
d2367 2
a2368 1
sse2CombineAtopU (uint32_t *dst, const uint32_t *src, int width)
d2370 1
a2370 1
    coreCombineAtopUsse2 (dst, src, width);
d2375 2
a2376 1
sse2CombineAtopReverseU (uint32_t *dst, const uint32_t *src, int width)
d2378 1
a2378 1
    coreCombineReverseAtopUsse2 (dst, src, width);
d2383 2
a2384 1
sse2CombineXorU (uint32_t *dst, const uint32_t *src, int width)
d2386 1
a2386 1
    coreCombineXorUsse2 (dst, src, width);
d2391 2
a2392 1
sse2CombineAddU (uint32_t *dst, const uint32_t *src, int width)
d2394 1
a2394 1
    coreCombineAddUsse2 (dst, src, width);
d2399 2
a2400 1
sse2CombineSaturateU (uint32_t *dst, const uint32_t *src, int width)
d2402 1
a2402 1
    coreCombineSaturateUsse2 (dst, src, width);
d2407 2
a2408 1
sse2CombineSrcC (uint32_t *dst, uint32_t *src, uint32_t *mask, int width)
d2415 2
a2416 1
sse2CombineOverC (uint32_t *dst, uint32_t *src, uint32_t *mask, int width)
d2423 2
a2424 1
sse2CombineOverReverseC (uint32_t *dst, uint32_t *src, uint32_t *mask, int width)
d2431 2
a2432 1
sse2CombineInC (uint32_t *dst, uint32_t *src, uint32_t *mask, int width)
d2439 2
a2440 1
sse2CombineInReverseC (uint32_t *dst, uint32_t *src, uint32_t *mask, int width)
d2447 2
a2448 1
sse2CombineOutC (uint32_t *dst, uint32_t *src, uint32_t *mask, int width)
d2455 2
a2456 1
sse2CombineOutReverseC (uint32_t *dst, uint32_t *src, uint32_t *mask, int width)
d2463 2
a2464 1
sse2CombineAtopC (uint32_t *dst, uint32_t *src, uint32_t *mask, int width)
d2471 2
a2472 1
sse2CombineAtopReverseC (uint32_t *dst, uint32_t *src, uint32_t *mask, int width)
d2479 2
a2480 1
sse2CombineXorC (uint32_t *dst, uint32_t *src, uint32_t *mask, int width)
d2487 2
a2488 1
sse2CombineAddC (uint32_t *dst, uint32_t *src, uint32_t *mask, int width)
a2493 73
void
fbComposeSetupSSE2(void)
{
    static pixman_bool_t initialized = FALSE;

    if (initialized)
	return;
    
    /* check if we have SSE2 support and initialize accordingly */
    if (pixman_have_sse2())
    {
        /* SSE2 constants */
        Mask565r  = createMask_2x32_128 (0x00f80000, 0x00f80000);
        Mask565g1 = createMask_2x32_128 (0x00070000, 0x00070000);
        Mask565g2 = createMask_2x32_128 (0x000000e0, 0x000000e0);
        Mask565b  = createMask_2x32_128 (0x0000001f, 0x0000001f);
        MaskRed   = createMask_2x32_128 (0x00f80000, 0x00f80000);
        MaskGreen = createMask_2x32_128 (0x0000fc00, 0x0000fc00);
        MaskBlue  = createMask_2x32_128 (0x000000f8, 0x000000f8);
	Mask565FixRB = createMask_2x32_128 (0x00e000e0, 0x00e000e0);
	Mask565FixG = createMask_2x32_128  (0x0000c000, 0x0000c000);
        Mask0080 = createMask_16_128 (0x0080);
        Mask00ff = createMask_16_128 (0x00ff);
        Mask0101 = createMask_16_128 (0x0101);
        Maskffff = createMask_16_128 (0xffff);
        Maskff000000 = createMask_2x32_128 (0xff000000, 0xff000000);
        MaskAlpha = createMask_2x32_128 (0x00ff0000, 0x00000000);

        /* MMX constants */
        xMask565rgb = createMask_2x32_64 (0x000001f0, 0x003f001f);
        xMask565Unpack = createMask_2x32_64 (0x00000084, 0x04100840);

        xMask0080 = createMask_16_64 (0x0080);
        xMask00ff = createMask_16_64 (0x00ff);
        xMask0101 = createMask_16_64 (0x0101);
        xMaskAlpha = createMask_2x32_64 (0x00ff0000, 0x00000000);

        /* SSE code patch for fbcompose.c */
        pixman_composeFunctions.combineU[PIXMAN_OP_OVER] = sse2CombineOverU;
        pixman_composeFunctions.combineU[PIXMAN_OP_OVER_REVERSE] = sse2CombineOverReverseU;
        pixman_composeFunctions.combineU[PIXMAN_OP_IN] = sse2CombineInU;
        pixman_composeFunctions.combineU[PIXMAN_OP_IN_REVERSE] = sse2CombineInReverseU;
        pixman_composeFunctions.combineU[PIXMAN_OP_OUT] = sse2CombineOutU;

        pixman_composeFunctions.combineU[PIXMAN_OP_OUT_REVERSE] = sse2CombineOutReverseU;
        pixman_composeFunctions.combineU[PIXMAN_OP_ATOP] = sse2CombineAtopU;
        pixman_composeFunctions.combineU[PIXMAN_OP_ATOP_REVERSE] = sse2CombineAtopReverseU;
        pixman_composeFunctions.combineU[PIXMAN_OP_XOR] = sse2CombineXorU;
        pixman_composeFunctions.combineU[PIXMAN_OP_ADD] = sse2CombineAddU;

        pixman_composeFunctions.combineU[PIXMAN_OP_SATURATE] = sse2CombineSaturateU;

        pixman_composeFunctions.combineC[PIXMAN_OP_SRC] = sse2CombineSrcC;
        pixman_composeFunctions.combineC[PIXMAN_OP_OVER] = sse2CombineOverC;
        pixman_composeFunctions.combineC[PIXMAN_OP_OVER_REVERSE] = sse2CombineOverReverseC;
        pixman_composeFunctions.combineC[PIXMAN_OP_IN] = sse2CombineInC;
        pixman_composeFunctions.combineC[PIXMAN_OP_IN_REVERSE] = sse2CombineInReverseC;
        pixman_composeFunctions.combineC[PIXMAN_OP_OUT] = sse2CombineOutC;
        pixman_composeFunctions.combineC[PIXMAN_OP_OUT_REVERSE] = sse2CombineOutReverseC;
        pixman_composeFunctions.combineC[PIXMAN_OP_ATOP] = sse2CombineAtopC;
        pixman_composeFunctions.combineC[PIXMAN_OP_ATOP_REVERSE] = sse2CombineAtopReverseC;
        pixman_composeFunctions.combineC[PIXMAN_OP_XOR] = sse2CombineXorC;
        pixman_composeFunctions.combineC[PIXMAN_OP_ADD] = sse2CombineAddC;

        pixman_composeFunctions.combineMaskU = sse2CombineMaskU;
    }

    initialized = TRUE;

    _mm_empty();
}


d2498 3
a2500 2
void
fbCompositeSolid_nx8888sse2 (pixman_op_t op,
d2504 8
a2511 8
			    int16_t	xSrc,
			    int16_t	ySrc,
			    int16_t	xMask,
			    int16_t	yMask,
			    int16_t	xDst,
			    int16_t	yDst,
			    uint16_t	width,
			    uint16_t	height)
d2522 1
a2522 1
    if (src >> 24 == 0)
d2585 3
a2587 2
void
fbCompositeSolid_nx0565sse2 (pixman_op_t op,
d2591 8
a2598 8
			    int16_t	xSrc,
			    int16_t	ySrc,
			    int16_t	xMask,
			    int16_t	yMask,
			    int16_t	xDst,
			    int16_t	yDst,
			    uint16_t	width,
			    uint16_t	height)
d2609 1
a2609 1
    if (src >> 24 == 0)
d2675 3
a2677 2
void
fbCompositeSolidMask_nx8888x8888Csse2 (pixman_op_t op,
d2681 8
a2688 8
				      int16_t	xSrc,
				      int16_t	ySrc,
				      int16_t	xMask,
				      int16_t	yMask,
				      int16_t	xDst,
				      int16_t	yDst,
				      uint16_t	width,
				      uint16_t	height)
d2690 1
a2690 1
    uint32_t	src, srca;
d2704 1
a2704 2
    srca = src >> 24;
    if (srca == 0)
d2718 1
a2718 1
        uint32_t *pm = (uint32_t *)maskLine;
d2809 3
a2811 2
void
fbCompositeSrc_8888x8x8888sse2 (pixman_op_t op,
d2815 8
a2822 8
			       int16_t	xSrc,
			       int16_t	ySrc,
			       int16_t      xMask,
			       int16_t      yMask,
			       int16_t      xDst,
			       int16_t      yDst,
			       uint16_t     width,
			       uint16_t     height)
d2922 3
a2924 2
void
fbCompositeSrc_x888xnx8888sse2 (pixman_op_t op,
d2928 8
a2935 8
			       int16_t	xSrc,
			       int16_t	ySrc,
			       int16_t      xMask,
			       int16_t      yMask,
			       int16_t      xDst,
			       int16_t      yDst,
			       uint16_t     width,
			       uint16_t     height)
d3035 3
a3037 2
void
fbCompositeSrc_8888x8888sse2 (pixman_op_t op,
d3041 8
a3048 8
			     int16_t	xSrc,
			     int16_t	ySrc,
			     int16_t      xMask,
			     int16_t      yMask,
			     int16_t      xDst,
			     int16_t      yDst,
			     uint16_t     width,
			     uint16_t     height)
d3062 1
a3062 1
        coreCombineOverUsse2 (dst, src, width);
d3073 1
a3073 1
static inline uint16_t
d3084 3
a3086 2
void
fbCompositeSrc_8888x0565sse2 (pixman_op_t op,
d3090 8
a3097 8
			     int16_t      xSrc,
			     int16_t      ySrc,
			     int16_t      xMask,
			     int16_t      yMask,
			     int16_t      xDst,
			     int16_t      yDst,
			     uint16_t     width,
			     uint16_t     height)
d3198 3
a3200 2
void
fbCompositeSolidMask_nx8x8888sse2 (pixman_op_t op,
d3204 8
a3211 8
				  int16_t      xSrc,
				  int16_t      ySrc,
				  int16_t      xMask,
				  int16_t      yMask,
				  int16_t      xDst,
				  int16_t      yDst,
				  uint16_t     width,
				  uint16_t     height)
d3229 1
a3229 1
    if (srca == 0)
d3476 3
a3478 2
void
fbCompositeSolidMaskSrc_nx8x8888sse2 (pixman_op_t op,
d3482 8
a3489 8
				     int16_t      xSrc,
				     int16_t      ySrc,
				     int16_t      xMask,
				     int16_t      yMask,
				     int16_t      xDst,
				     int16_t      yDst,
				     uint16_t     width,
				     uint16_t     height)
d3504 1
a3504 1
    if (srca == 0)
d3612 3
a3614 2
void
fbCompositeSolidMask_nx8x0565sse2 (pixman_op_t op,
d3618 8
a3625 8
				  int16_t      xSrc,
				  int16_t      ySrc,
				  int16_t      xMask,
				  int16_t      yMask,
				  int16_t      xDst,
				  int16_t      yDst,
				  uint16_t     width,
				  uint16_t     height)
d3642 1
a3642 1
    if (srca == 0)
d3762 3
a3764 2
void
fbCompositeSrc_8888RevNPx0565sse2 (pixman_op_t op,
d3768 8
a3775 8
				  int16_t      xSrc,
				  int16_t      ySrc,
				  int16_t      xMask,
				  int16_t      yMask,
				  int16_t      xDst,
				  int16_t      yDst,
				  uint16_t     width,
				  uint16_t     height)
d3779 1
a3779 1
    int	dstStride, srcStride;
d3781 1
a3781 1
    uint32_t    packCmp;
d3836 2
a3837 1
            packCmp = packAlpha (xmmSrc);
d3839 1
a3839 1
            unpack565_128_4x128 (xmmDst, &xmmDst0, &xmmDst1, &xmmDst2, &xmmDst3);
d3844 2
a3845 3
            /* preload next round*/

            if (packCmp == 0xffffffff)
d3849 1
a3849 1
            else if (packCmp)
d3855 2
a3856 1
            packCmp = packAlpha (xmmSrc);
d3860 1
a3860 1
            if (packCmp == 0xffffffff)
d3864 1
a3864 1
            else if (packCmp)
d3897 3
a3899 2
void
fbCompositeSrc_8888RevNPx8888sse2 (pixman_op_t op,
d3903 8
a3910 8
				  int16_t      xSrc,
				  int16_t      ySrc,
				  int16_t      xMask,
				  int16_t      yMask,
				  int16_t      xDst,
				  int16_t      yDst,
				  uint16_t     width,
				  uint16_t     height)
d3916 1
a3916 1
    uint32_t    packCmp;
d3967 2
a3968 1
            packCmp = packAlpha (xmmSrcHi);
d3972 1
a3972 1
            if (packCmp == 0xffffffff)
d3978 1
a3978 1
            else if (packCmp)
d4012 3
a4014 2
void
fbCompositeSolidMask_nx8888x0565Csse2 (pixman_op_t op,
d4018 8
a4025 8
				      int16_t      xSrc,
				      int16_t      ySrc,
				      int16_t      xMask,
				      int16_t      yMask,
				      int16_t      xDst,
				      int16_t      yDst,
				      uint16_t     width,
				      uint16_t     height)
d4027 1
a4027 1
    uint32_t	src, srca;
d4042 1
a4042 2
    srca = src >> 24;
    if (srca == 0)
d4160 3
a4162 2
void
fbCompositeIn_nx8x8sse2 (pixman_op_t op,
d4166 8
a4173 8
			int16_t      xSrc,
			int16_t      ySrc,
			int16_t      xMask,
			int16_t      yMask,
			int16_t      xDst,
			int16_t      yDst,
			uint16_t     width,
			uint16_t     height)
d4263 3
a4265 2
void
fbCompositeIn_8x8sse2 (pixman_op_t op,
d4269 8
a4276 8
		      int16_t      xSrc,
		      int16_t      ySrc,
		      int16_t      xMask,
		      int16_t      yMask,
		      int16_t      xDst,
		      int16_t      yDst,
		      uint16_t     width,
		      uint16_t     height)
d4353 3
a4355 2
void
fbCompositeSrcAdd_8888x8x8sse2 (pixman_op_t op,
d4359 8
a4366 8
			       int16_t      xSrc,
			       int16_t      ySrc,
			       int16_t      xMask,
			       int16_t      yMask,
			       int16_t      xDst,
			       int16_t      yDst,
			       uint16_t     width,
			       uint16_t     height)
d4459 3
a4461 2
void
fbCompositeSrcAdd_8000x8000sse2 (pixman_op_t op,
d4465 8
a4472 8
				int16_t      xSrc,
				int16_t      ySrc,
				int16_t      xMask,
				int16_t      yMask,
				int16_t      xDst,
				int16_t      yDst,
				uint16_t     width,
				uint16_t     height)
d4504 1
a4504 1
        coreCombineAddUsse2 ((uint32_t*)dst, (uint32_t*)src, w >> 2);
d4526 3
a4528 2
void
fbCompositeSrcAdd_8888x8888sse2 (pixman_op_t 	op,
d4532 8
a4539 8
				int16_t		 xSrc,
				int16_t      ySrc,
				int16_t      xMask,
				int16_t      yMask,
				int16_t      xDst,
				int16_t      yDst,
				uint16_t     width,
				uint16_t     height)
d4555 1
a4555 1
        coreCombineAddUsse2 (dst, src, width);
d4565 1
a4565 1
pixman_bool_t
d4567 8
a4574 8
		uint32_t *dst_bits,
		int src_stride,
		int dst_stride,
		int src_bpp,
		int dst_bpp,
		int src_x, int src_y,
		int dst_x, int dst_y,
		int width, int height)
d4704 3
a4706 2
void
fbCompositeCopyAreasse2 (pixman_op_t       op,
d4710 8
a4717 8
			int16_t		xSrc,
			int16_t		ySrc,
			int16_t		xMask,
			int16_t		yMask,
			int16_t		xDst,
			int16_t		yDst,
			uint16_t		width,
			uint16_t		height)
d4731 2
a4732 1
fbCompositeOver_x888x8x8888sse2 (pixman_op_t      op,
d4736 8
a4743 8
				int16_t      xSrc,
				int16_t      ySrc,
				int16_t      xMask,
				int16_t      yMask,
				int16_t      xDst,
				int16_t      yDst,
				uint16_t     width,
				uint16_t     height)
d4870 244
a5113 1
#endif /* #if 0 */
@

