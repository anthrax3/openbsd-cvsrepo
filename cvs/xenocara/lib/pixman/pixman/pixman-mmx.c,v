head	1.15;
access;
symbols
	OPENBSD_6_1_BASE:1.15
	OPENBSD_6_0:1.14.0.12
	OPENBSD_6_0_BASE:1.14
	OPENBSD_5_9:1.14.0.10
	OPENBSD_5_9_BASE:1.14
	OPENBSD_5_8:1.14.0.8
	OPENBSD_5_8_BASE:1.14
	OPENBSD_5_7:1.14.0.6
	OPENBSD_5_7_BASE:1.14
	OPENBSD_5_6:1.14.0.4
	OPENBSD_5_6_BASE:1.14
	OPENBSD_5_5:1.14.0.2
	OPENBSD_5_5_BASE:1.14
	OPENBSD_5_4:1.13.0.2
	OPENBSD_5_4_BASE:1.13
	OPENBSD_5_3:1.12.0.2
	OPENBSD_5_3_BASE:1.12
	OPENBSD_5_2:1.10.0.2
	OPENBSD_5_2_BASE:1.10
	OPENBSD_5_1_BASE:1.9
	OPENBSD_5_1:1.9.0.4
	OPENBSD_5_0:1.9.0.2
	OPENBSD_5_0_BASE:1.9
	OPENBSD_4_9:1.8.0.2
	OPENBSD_4_9_BASE:1.8
	OPENBSD_4_8:1.5.0.4
	OPENBSD_4_8_BASE:1.5
	OPENBSD_4_7:1.4.0.4
	OPENBSD_4_7_BASE:1.4
	OPENBSD_4_6:1.4.0.2
	OPENBSD_4_6_BASE:1.4
	OPENBSD_4_5:1.3.0.2
	OPENBSD_4_5_BASE:1.3
	OPENBSD_4_4:1.2.0.2
	OPENBSD_4_4_BASE:1.2
	OPENBSD_4_3_BASE:1.1.1.2
	OPENBSD_4_3:1.1.1.2.0.2
	v0_9_6:1.1.1.2
	xorg:1.1.1
	v0_9_5:1.1.1.1
	pixman:1.1.1;
locks; strict;
comment	@ * @;


1.15
date	2016.10.01.10.17.44;	author matthieu;	state Exp;
branches;
next	1.14;
commitid	FGr8CFhVerRlpSoE;

1.14
date	2013.12.01.20.34.20;	author matthieu;	state Exp;
branches;
next	1.13;

1.13
date	2013.06.07.17.18.01;	author matthieu;	state Exp;
branches;
next	1.12;

1.12
date	2012.11.23.20.44.09;	author matthieu;	state Exp;
branches;
next	1.11;

1.11
date	2012.08.17.16.15.20;	author matthieu;	state Exp;
branches;
next	1.10;

1.10
date	2012.02.28.20.36.12;	author matthieu;	state Exp;
branches;
next	1.9;

1.9
date	2011.07.24.13.05.47;	author matthieu;	state Exp;
branches;
next	1.8;

1.8
date	2011.01.22.08.32.55;	author matthieu;	state Exp;
branches;
next	1.7;

1.7
date	2010.11.14.13.42.49;	author matthieu;	state Exp;
branches;
next	1.6;

1.6
date	2010.10.03.18.30.04;	author matthieu;	state Exp;
branches;
next	1.5;

1.5
date	2010.03.25.21.58.52;	author matthieu;	state Exp;
branches;
next	1.4;

1.4
date	2009.06.05.20.14.28;	author matthieu;	state Exp;
branches;
next	1.3;

1.3
date	2008.09.23.19.11.40;	author matthieu;	state Exp;
branches;
next	1.2;

1.2
date	2008.04.08.19.00.26;	author matthieu;	state Exp;
branches;
next	1.1;

1.1
date	2007.10.03.20.48.54;	author matthieu;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	2007.10.03.20.48.54;	author matthieu;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	2007.12.10.21.10.22;	author matthieu;	state Exp;
branches;
next	;


desc
@@


1.15
log
@Update to pixman 0.34.0.
@
text
@/*
 * Copyright © 2004, 2005 Red Hat, Inc.
 * Copyright © 2004 Nicholas Miell
 * Copyright © 2005 Trolltech AS
 *
 * Permission to use, copy, modify, distribute, and sell this software and its
 * documentation for any purpose is hereby granted without fee, provided that
 * the above copyright notice appear in all copies and that both that
 * copyright notice and this permission notice appear in supporting
 * documentation, and that the name of Red Hat not be used in advertising or
 * publicity pertaining to distribution of the software without specific,
 * written prior permission.  Red Hat makes no representations about the
 * suitability of this software for any purpose.  It is provided "as is"
 * without express or implied warranty.
 *
 * THE COPYRIGHT HOLDERS DISCLAIM ALL WARRANTIES WITH REGARD TO THIS
 * SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND
 * FITNESS, IN NO EVENT SHALL THE COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
 * AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING
 * OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS
 * SOFTWARE.
 *
 * Author:  Søren Sandmann (sandmann@@redhat.com)
 * Minor Improvements: Nicholas Miell (nmiell@@gmail.com)
 * MMX code paths for fbcompose.c by Lars Knoll (lars@@trolltech.com)
 *
 * Based on work by Owen Taylor
 */

#ifdef HAVE_CONFIG_H
#include <config.h>
#endif

#if defined USE_X86_MMX || defined USE_ARM_IWMMXT || defined USE_LOONGSON_MMI

#ifdef USE_LOONGSON_MMI
#include <loongson-mmintrin.h>
#else
#include <mmintrin.h>
#endif
#include "pixman-private.h"
#include "pixman-combine32.h"
#include "pixman-inlines.h"

#ifdef VERBOSE
#define CHECKPOINT() error_f ("at %s %d\n", __FUNCTION__, __LINE__)
#else
#define CHECKPOINT()
#endif

#if defined USE_ARM_IWMMXT && __GNUC__ == 4 && __GNUC_MINOR__ < 8
/* Empty the multimedia state. For some reason, ARM's mmintrin.h doesn't provide this.  */
extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_empty (void)
{

}
#endif

#ifdef USE_X86_MMX
# if (defined(__SUNPRO_C) || defined(_MSC_VER) || defined(_WIN64))
#  include <xmmintrin.h>
# else
/* We have to compile with -msse to use xmmintrin.h, but that causes SSE
 * instructions to be generated that we don't want. Just duplicate the
 * functions we want to use.  */
extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_movemask_pi8 (__m64 __A)
{
    int ret;

    asm ("pmovmskb %1, %0\n\t"
	: "=r" (ret)
	: "y" (__A)
    );

    return ret;
}

extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_mulhi_pu16 (__m64 __A, __m64 __B)
{
    asm ("pmulhuw %1, %0\n\t"
	: "+y" (__A)
	: "y" (__B)
    );
    return __A;
}

# define _mm_shuffle_pi16(A, N)						\
    ({									\
	__m64 ret;							\
									\
	asm ("pshufw %2, %1, %0\n\t"					\
	     : "=y" (ret)						\
	     : "y" (A), "K" ((const int8_t)N)				\
	);								\
									\
	ret;								\
    })
# endif
#endif

#ifndef _MSC_VER
#define _MM_SHUFFLE(fp3,fp2,fp1,fp0) \
 (((fp3) << 6) | ((fp2) << 4) | ((fp1) << 2) | (fp0))
#endif

/* Notes about writing mmx code
 *
 * give memory operands as the second operand. If you give it as the
 * first, gcc will first load it into a register, then use that
 * register
 *
 *   ie. use
 *
 *         _mm_mullo_pi16 (x, mmx_constant);
 *
 *   not
 *
 *         _mm_mullo_pi16 (mmx_constant, x);
 *
 * Also try to minimize dependencies. i.e. when you need a value, try
 * to calculate it from a value that was calculated as early as
 * possible.
 */

/* --------------- MMX primitives ------------------------------------- */

/* If __m64 is defined as a struct or union, then define M64_MEMBER to be
 * the name of the member used to access the data.
 * If __m64 requires using mm_cvt* intrinsics functions to convert between
 * uint64_t and __m64 values, then define USE_CVT_INTRINSICS.
 * If __m64 and uint64_t values can just be cast to each other directly,
 * then define USE_M64_CASTS.
 * If __m64 is a double datatype, then define USE_M64_DOUBLE.
 */
#ifdef _MSC_VER
# define M64_MEMBER m64_u64
#elif defined(__ICC)
# define USE_CVT_INTRINSICS
#elif defined(USE_LOONGSON_MMI)
# define USE_M64_DOUBLE
#elif defined(__GNUC__)
# define USE_M64_CASTS
#elif defined(__SUNPRO_C)
# if (__SUNPRO_C >= 0x5120) && !defined(__NOVECTORSIZE__)
/* Solaris Studio 12.3 (Sun C 5.12) introduces __attribute__(__vector_size__)
 * support, and defaults to using it to define __m64, unless __NOVECTORSIZE__
 * is defined.   If it is used, then the mm_cvt* intrinsics must be used.
 */
#  define USE_CVT_INTRINSICS
# else
/* For Studio 12.2 or older, or when __attribute__(__vector_size__) is
 * disabled, __m64 is defined as a struct containing "unsigned long long l_".
 */
#  define M64_MEMBER l_
# endif
#endif

#if defined(USE_M64_CASTS) || defined(USE_CVT_INTRINSICS) || defined(USE_M64_DOUBLE)
typedef uint64_t mmxdatafield;
#else
typedef __m64 mmxdatafield;
#endif

typedef struct
{
    mmxdatafield mmx_4x00ff;
    mmxdatafield mmx_4x0080;
    mmxdatafield mmx_565_rgb;
    mmxdatafield mmx_565_unpack_multiplier;
    mmxdatafield mmx_565_pack_multiplier;
    mmxdatafield mmx_565_r;
    mmxdatafield mmx_565_g;
    mmxdatafield mmx_565_b;
    mmxdatafield mmx_packed_565_rb;
    mmxdatafield mmx_packed_565_g;
    mmxdatafield mmx_expand_565_g;
    mmxdatafield mmx_expand_565_b;
    mmxdatafield mmx_expand_565_r;
#ifndef USE_LOONGSON_MMI
    mmxdatafield mmx_mask_0;
    mmxdatafield mmx_mask_1;
    mmxdatafield mmx_mask_2;
    mmxdatafield mmx_mask_3;
#endif
    mmxdatafield mmx_full_alpha;
    mmxdatafield mmx_4x0101;
    mmxdatafield mmx_ff000000;
} mmx_data_t;

#if defined(_MSC_VER)
# define MMXDATA_INIT(field, val) { val ## UI64 }
#elif defined(M64_MEMBER)       /* __m64 is a struct, not an integral type */
# define MMXDATA_INIT(field, val) field =   { val ## ULL }
#else                           /* mmxdatafield is an integral type */
# define MMXDATA_INIT(field, val) field =   val ## ULL
#endif

static const mmx_data_t c =
{
    MMXDATA_INIT (.mmx_4x00ff,                   0x00ff00ff00ff00ff),
    MMXDATA_INIT (.mmx_4x0080,                   0x0080008000800080),
    MMXDATA_INIT (.mmx_565_rgb,                  0x000001f0003f001f),
    MMXDATA_INIT (.mmx_565_unpack_multiplier,    0x0000008404100840),
    MMXDATA_INIT (.mmx_565_pack_multiplier,      0x2000000420000004),
    MMXDATA_INIT (.mmx_565_r,                    0x000000f800000000),
    MMXDATA_INIT (.mmx_565_g,                    0x0000000000fc0000),
    MMXDATA_INIT (.mmx_565_b,                    0x00000000000000f8),
    MMXDATA_INIT (.mmx_packed_565_rb,            0x00f800f800f800f8),
    MMXDATA_INIT (.mmx_packed_565_g,             0x0000fc000000fc00),
    MMXDATA_INIT (.mmx_expand_565_g,             0x07e007e007e007e0),
    MMXDATA_INIT (.mmx_expand_565_b,             0x001f001f001f001f),
    MMXDATA_INIT (.mmx_expand_565_r,             0xf800f800f800f800),
#ifndef USE_LOONGSON_MMI
    MMXDATA_INIT (.mmx_mask_0,                   0xffffffffffff0000),
    MMXDATA_INIT (.mmx_mask_1,                   0xffffffff0000ffff),
    MMXDATA_INIT (.mmx_mask_2,                   0xffff0000ffffffff),
    MMXDATA_INIT (.mmx_mask_3,                   0x0000ffffffffffff),
#endif
    MMXDATA_INIT (.mmx_full_alpha,               0x00ff000000000000),
    MMXDATA_INIT (.mmx_4x0101,                   0x0101010101010101),
    MMXDATA_INIT (.mmx_ff000000,                 0xff000000ff000000),
};

#ifdef USE_CVT_INTRINSICS
#    define MC(x) to_m64 (c.mmx_ ## x)
#elif defined(USE_M64_CASTS)
#    define MC(x) ((__m64)c.mmx_ ## x)
#elif defined(USE_M64_DOUBLE)
#    define MC(x) (*(__m64 *)&c.mmx_ ## x)
#else
#    define MC(x) c.mmx_ ## x
#endif

static force_inline __m64
to_m64 (uint64_t x)
{
#ifdef USE_CVT_INTRINSICS
    return _mm_cvtsi64_m64 (x);
#elif defined M64_MEMBER        /* __m64 is a struct, not an integral type */
    __m64 res;

    res.M64_MEMBER = x;
    return res;
#elif defined USE_M64_DOUBLE
    return *(__m64 *)&x;
#else /* USE_M64_CASTS */
    return (__m64)x;
#endif
}

static force_inline uint64_t
to_uint64 (__m64 x)
{
#ifdef USE_CVT_INTRINSICS
    return _mm_cvtm64_si64 (x);
#elif defined M64_MEMBER        /* __m64 is a struct, not an integral type */
    uint64_t res = x.M64_MEMBER;
    return res;
#elif defined USE_M64_DOUBLE
    return *(uint64_t *)&x;
#else /* USE_M64_CASTS */
    return (uint64_t)x;
#endif
}

static force_inline __m64
shift (__m64 v,
       int   s)
{
    if (s > 0)
	return _mm_slli_si64 (v, s);
    else if (s < 0)
	return _mm_srli_si64 (v, -s);
    else
	return v;
}

static force_inline __m64
negate (__m64 mask)
{
    return _mm_xor_si64 (mask, MC (4x00ff));
}

/* Computes the product of two unsigned fixed-point 8-bit values from 0 to 1
 * and maps its result to the same range.
 *
 * Jim Blinn gives multiple ways to compute this in "Jim Blinn's Corner:
 * Notation, Notation, Notation", the first of which is
 *
 *   prod(a, b) = (a * b + 128) / 255.
 *
 * By approximating the division by 255 as 257/65536 it can be replaced by a
 * multiply and a right shift. This is the implementation that we use in
 * pix_multiply(), but we _mm_mulhi_pu16() by 257 (part of SSE1 or Extended
 * 3DNow!, and unavailable at the time of the book's publication) to perform
 * the multiplication and right shift in a single operation.
 *
 *   prod(a, b) = ((a * b + 128) * 257) >> 16.
 *
 * A third way (how pix_multiply() was implemented prior to 14208344) exists
 * also that performs the multiplication by 257 with adds and shifts.
 *
 * Where temp = a * b + 128
 *
 *   prod(a, b) = (temp + (temp >> 8)) >> 8.
 */
static force_inline __m64
pix_multiply (__m64 a, __m64 b)
{
    __m64 res;

    res = _mm_mullo_pi16 (a, b);
    res = _mm_adds_pu16 (res, MC (4x0080));
    res = _mm_mulhi_pu16 (res, MC (4x0101));

    return res;
}

static force_inline __m64
pix_add (__m64 a, __m64 b)
{
    return _mm_adds_pu8 (a, b);
}

static force_inline __m64
expand_alpha (__m64 pixel)
{
    return _mm_shuffle_pi16 (pixel, _MM_SHUFFLE (3, 3, 3, 3));
}

static force_inline __m64
expand_alpha_rev (__m64 pixel)
{
    return _mm_shuffle_pi16 (pixel, _MM_SHUFFLE (0, 0, 0, 0));
}

static force_inline __m64
invert_colors (__m64 pixel)
{
    return _mm_shuffle_pi16 (pixel, _MM_SHUFFLE (3, 0, 1, 2));
}

static force_inline __m64
over (__m64 src,
      __m64 srca,
      __m64 dest)
{
    return _mm_adds_pu8 (src, pix_multiply (dest, negate (srca)));
}

static force_inline __m64
over_rev_non_pre (__m64 src, __m64 dest)
{
    __m64 srca = expand_alpha (src);
    __m64 srcfaaa = _mm_or_si64 (srca, MC (full_alpha));

    return over (pix_multiply (invert_colors (src), srcfaaa), srca, dest);
}

static force_inline __m64
in (__m64 src, __m64 mask)
{
    return pix_multiply (src, mask);
}

#ifndef _MSC_VER
static force_inline __m64
in_over (__m64 src, __m64 srca, __m64 mask, __m64 dest)
{
    return over (in (src, mask), pix_multiply (srca, mask), dest);
}

#else

#define in_over(src, srca, mask, dest)					\
    over (in (src, mask), pix_multiply (srca, mask), dest)

#endif

/* Elemental unaligned loads */

static force_inline __m64 ldq_u(__m64 *p)
{
#ifdef USE_X86_MMX
    /* x86's alignment restrictions are very relaxed. */
    return *(__m64 *)p;
#elif defined USE_ARM_IWMMXT
    int align = (uintptr_t)p & 7;
    __m64 *aligned_p;
    if (align == 0)
	return *p;
    aligned_p = (__m64 *)((uintptr_t)p & ~7);
    return (__m64) _mm_align_si64 (aligned_p[0], aligned_p[1], align);
#else
    struct __una_u64 { __m64 x __attribute__((packed)); };
    const struct __una_u64 *ptr = (const struct __una_u64 *) p;
    return (__m64) ptr->x;
#endif
}

static force_inline uint32_t ldl_u(const uint32_t *p)
{
#ifdef USE_X86_MMX
    /* x86's alignment restrictions are very relaxed. */
    return *p;
#else
    struct __una_u32 { uint32_t x __attribute__((packed)); };
    const struct __una_u32 *ptr = (const struct __una_u32 *) p;
    return ptr->x;
#endif
}

static force_inline __m64
load (const uint32_t *v)
{
#ifdef USE_LOONGSON_MMI
    __m64 ret;
    asm ("lwc1 %0, %1\n\t"
	: "=f" (ret)
	: "m" (*v)
    );
    return ret;
#else
    return _mm_cvtsi32_si64 (*v);
#endif
}

static force_inline __m64
load8888 (const uint32_t *v)
{
#ifdef USE_LOONGSON_MMI
    return _mm_unpacklo_pi8_f (*(__m32 *)v, _mm_setzero_si64 ());
#else
    return _mm_unpacklo_pi8 (load (v), _mm_setzero_si64 ());
#endif
}

static force_inline __m64
load8888u (const uint32_t *v)
{
    uint32_t l = ldl_u (v);
    return load8888 (&l);
}

static force_inline __m64
pack8888 (__m64 lo, __m64 hi)
{
    return _mm_packs_pu16 (lo, hi);
}

static force_inline void
store (uint32_t *dest, __m64 v)
{
#ifdef USE_LOONGSON_MMI
    asm ("swc1 %1, %0\n\t"
	: "=m" (*dest)
	: "f" (v)
	: "memory"
    );
#else
    *dest = _mm_cvtsi64_si32 (v);
#endif
}

static force_inline void
store8888 (uint32_t *dest, __m64 v)
{
    v = pack8888 (v, _mm_setzero_si64 ());
    store (dest, v);
}

static force_inline pixman_bool_t
is_equal (__m64 a, __m64 b)
{
#ifdef USE_LOONGSON_MMI
    /* __m64 is double, we can compare directly. */
    return a == b;
#else
    return _mm_movemask_pi8 (_mm_cmpeq_pi8 (a, b)) == 0xff;
#endif
}

static force_inline pixman_bool_t
is_opaque (__m64 v)
{
#ifdef USE_LOONGSON_MMI
    return is_equal (_mm_and_si64 (v, MC (full_alpha)), MC (full_alpha));
#else
    __m64 ffs = _mm_cmpeq_pi8 (v, v);
    return (_mm_movemask_pi8 (_mm_cmpeq_pi8 (v, ffs)) & 0x40);
#endif
}

static force_inline pixman_bool_t
is_zero (__m64 v)
{
    return is_equal (v, _mm_setzero_si64 ());
}

/* Expand 16 bits positioned at @@pos (0-3) of a mmx register into
 *
 *    00RR00GG00BB
 *
 * --- Expanding 565 in the low word ---
 *
 * m = (m << (32 - 3)) | (m << (16 - 5)) | m;
 * m = m & (01f0003f001f);
 * m = m * (008404100840);
 * m = m >> 8;
 *
 * Note the trick here - the top word is shifted by another nibble to
 * avoid it bumping into the middle word
 */
static force_inline __m64
expand565 (__m64 pixel, int pos)
{
    __m64 p = pixel;
    __m64 t1, t2;

    /* move pixel to low 16 bit and zero the rest */
#ifdef USE_LOONGSON_MMI
    p = loongson_extract_pi16 (p, pos);
#else
    p = shift (shift (p, (3 - pos) * 16), -48);
#endif

    t1 = shift (p, 36 - 11);
    t2 = shift (p, 16 - 5);

    p = _mm_or_si64 (t1, p);
    p = _mm_or_si64 (t2, p);
    p = _mm_and_si64 (p, MC (565_rgb));

    pixel = _mm_mullo_pi16 (p, MC (565_unpack_multiplier));
    return _mm_srli_pi16 (pixel, 8);
}

/* Expand 4 16 bit pixels in an mmx register into two mmx registers of
 *
 *    AARRGGBBRRGGBB
 */
static force_inline void
expand_4xpacked565 (__m64 vin, __m64 *vout0, __m64 *vout1, int full_alpha)
{
    __m64 t0, t1, alpha = _mm_setzero_si64 ();
    __m64 r = _mm_and_si64 (vin, MC (expand_565_r));
    __m64 g = _mm_and_si64 (vin, MC (expand_565_g));
    __m64 b = _mm_and_si64 (vin, MC (expand_565_b));
    if (full_alpha)
	alpha = _mm_cmpeq_pi32 (alpha, alpha);

    /* Replicate high bits into empty low bits. */
    r = _mm_or_si64 (_mm_srli_pi16 (r, 8), _mm_srli_pi16 (r, 13));
    g = _mm_or_si64 (_mm_srli_pi16 (g, 3), _mm_srli_pi16 (g, 9));
    b = _mm_or_si64 (_mm_slli_pi16 (b, 3), _mm_srli_pi16 (b, 2));

    r = _mm_packs_pu16 (r, _mm_setzero_si64 ());	/* 00 00 00 00 R3 R2 R1 R0 */
    g = _mm_packs_pu16 (g, _mm_setzero_si64 ());	/* 00 00 00 00 G3 G2 G1 G0 */
    b = _mm_packs_pu16 (b, _mm_setzero_si64 ());	/* 00 00 00 00 B3 B2 B1 B0 */

    t1 = _mm_unpacklo_pi8 (r, alpha);			/* A3 R3 A2 R2 A1 R1 A0 R0 */
    t0 = _mm_unpacklo_pi8 (b, g);			/* G3 B3 G2 B2 G1 B1 G0 B0 */

    *vout0 = _mm_unpacklo_pi16 (t0, t1);		/* A1 R1 G1 B1 A0 R0 G0 B0 */
    *vout1 = _mm_unpackhi_pi16 (t0, t1);		/* A3 R3 G3 B3 A2 R2 G2 B2 */
}

static force_inline __m64
expand8888 (__m64 in, int pos)
{
    if (pos == 0)
	return _mm_unpacklo_pi8 (in, _mm_setzero_si64 ());
    else
	return _mm_unpackhi_pi8 (in, _mm_setzero_si64 ());
}

static force_inline __m64
expandx888 (__m64 in, int pos)
{
    return _mm_or_si64 (expand8888 (in, pos), MC (full_alpha));
}

static force_inline void
expand_4x565 (__m64 vin, __m64 *vout0, __m64 *vout1, __m64 *vout2, __m64 *vout3, int full_alpha)
{
    __m64 v0, v1;
    expand_4xpacked565 (vin, &v0, &v1, full_alpha);
    *vout0 = expand8888 (v0, 0);
    *vout1 = expand8888 (v0, 1);
    *vout2 = expand8888 (v1, 0);
    *vout3 = expand8888 (v1, 1);
}

static force_inline __m64
pack_565 (__m64 pixel, __m64 target, int pos)
{
    __m64 p = pixel;
    __m64 t = target;
    __m64 r, g, b;

    r = _mm_and_si64 (p, MC (565_r));
    g = _mm_and_si64 (p, MC (565_g));
    b = _mm_and_si64 (p, MC (565_b));

#ifdef USE_LOONGSON_MMI
    r = shift (r, -(32 - 8));
    g = shift (g, -(16 - 3));
    b = shift (b, -(0  + 3));

    p = _mm_or_si64 (r, g);
    p = _mm_or_si64 (p, b);
    return loongson_insert_pi16 (t, p, pos);
#else
    r = shift (r, -(32 - 8) + pos * 16);
    g = shift (g, -(16 - 3) + pos * 16);
    b = shift (b, -(0  + 3) + pos * 16);

    if (pos == 0)
	t = _mm_and_si64 (t, MC (mask_0));
    else if (pos == 1)
	t = _mm_and_si64 (t, MC (mask_1));
    else if (pos == 2)
	t = _mm_and_si64 (t, MC (mask_2));
    else if (pos == 3)
	t = _mm_and_si64 (t, MC (mask_3));

    p = _mm_or_si64 (r, t);
    p = _mm_or_si64 (g, p);

    return _mm_or_si64 (b, p);
#endif
}

static force_inline __m64
pack_4xpacked565 (__m64 a, __m64 b)
{
    __m64 rb0 = _mm_and_si64 (a, MC (packed_565_rb));
    __m64 rb1 = _mm_and_si64 (b, MC (packed_565_rb));

    __m64 t0 = _mm_madd_pi16 (rb0, MC (565_pack_multiplier));
    __m64 t1 = _mm_madd_pi16 (rb1, MC (565_pack_multiplier));

    __m64 g0 = _mm_and_si64 (a, MC (packed_565_g));
    __m64 g1 = _mm_and_si64 (b, MC (packed_565_g));

    t0 = _mm_or_si64 (t0, g0);
    t1 = _mm_or_si64 (t1, g1);

    t0 = shift(t0, -5);
#ifdef USE_ARM_IWMMXT
    t1 = shift(t1, -5);
    return _mm_packs_pu32 (t0, t1);
#else
    t1 = shift(t1, -5 + 16);
    return _mm_shuffle_pi16 (_mm_or_si64 (t0, t1), _MM_SHUFFLE (3, 1, 2, 0));
#endif
}

#ifndef _MSC_VER

static force_inline __m64
pack_4x565 (__m64 v0, __m64 v1, __m64 v2, __m64 v3)
{
    return pack_4xpacked565 (pack8888 (v0, v1), pack8888 (v2, v3));
}

static force_inline __m64
pix_add_mul (__m64 x, __m64 a, __m64 y, __m64 b)
{
    x = pix_multiply (x, a);
    y = pix_multiply (y, b);

    return pix_add (x, y);
}

#else

/* MSVC only handles a "pass by register" of up to three SSE intrinsics */

#define pack_4x565(v0, v1, v2, v3) \
    pack_4xpacked565 (pack8888 (v0, v1), pack8888 (v2, v3))

#define pix_add_mul(x, a, y, b)	 \
    ( x = pix_multiply (x, a),	 \
      y = pix_multiply (y, b),	 \
      pix_add (x, y) )

#endif

/* --------------- MMX code patch for fbcompose.c --------------------- */

static force_inline __m64
combine (const uint32_t *src, const uint32_t *mask)
{
    __m64 vsrc = load8888 (src);

    if (mask)
    {
	__m64 m = load8888 (mask);

	m = expand_alpha (m);
	vsrc = pix_multiply (vsrc, m);
    }

    return vsrc;
}

static force_inline __m64
core_combine_over_u_pixel_mmx (__m64 vsrc, __m64 vdst)
{
    vsrc = _mm_unpacklo_pi8 (vsrc, _mm_setzero_si64 ());

    if (is_opaque (vsrc))
    {
	return vsrc;
    }
    else if (!is_zero (vsrc))
    {
	return over (vsrc, expand_alpha (vsrc),
		     _mm_unpacklo_pi8 (vdst, _mm_setzero_si64 ()));
    }

    return _mm_unpacklo_pi8 (vdst, _mm_setzero_si64 ());
}

static void
mmx_combine_over_u (pixman_implementation_t *imp,
                    pixman_op_t              op,
                    uint32_t *               dest,
                    const uint32_t *         src,
                    const uint32_t *         mask,
                    int                      width)
{
    const uint32_t *end = dest + width;

    while (dest < end)
    {
	__m64 vsrc = combine (src, mask);

	if (is_opaque (vsrc))
	{
	    store8888 (dest, vsrc);
	}
	else if (!is_zero (vsrc))
	{
	    __m64 sa = expand_alpha (vsrc);
	    store8888 (dest, over (vsrc, sa, load8888 (dest)));
	}

	++dest;
	++src;
	if (mask)
	    ++mask;
    }
    _mm_empty ();
}

static void
mmx_combine_over_reverse_u (pixman_implementation_t *imp,
                            pixman_op_t              op,
                            uint32_t *               dest,
                            const uint32_t *         src,
                            const uint32_t *         mask,
                            int                      width)
{
    const uint32_t *end = dest + width;

    while (dest < end)
    {
	__m64 d, da;
	__m64 s = combine (src, mask);

	d = load8888 (dest);
	da = expand_alpha (d);
	store8888 (dest, over (d, da, s));

	++dest;
	++src;
	if (mask)
	    mask++;
    }
    _mm_empty ();
}

static void
mmx_combine_in_u (pixman_implementation_t *imp,
                  pixman_op_t              op,
                  uint32_t *               dest,
                  const uint32_t *         src,
                  const uint32_t *         mask,
                  int                      width)
{
    const uint32_t *end = dest + width;

    while (dest < end)
    {
	__m64 a;
	__m64 x = combine (src, mask);

	a = load8888 (dest);
	a = expand_alpha (a);
	x = pix_multiply (x, a);

	store8888 (dest, x);

	++dest;
	++src;
	if (mask)
	    mask++;
    }
    _mm_empty ();
}

static void
mmx_combine_in_reverse_u (pixman_implementation_t *imp,
                          pixman_op_t              op,
                          uint32_t *               dest,
                          const uint32_t *         src,
                          const uint32_t *         mask,
                          int                      width)
{
    const uint32_t *end = dest + width;

    while (dest < end)
    {
	__m64 a = combine (src, mask);
	__m64 x;

	x = load8888 (dest);
	a = expand_alpha (a);
	x = pix_multiply (x, a);
	store8888 (dest, x);

	++dest;
	++src;
	if (mask)
	    mask++;
    }
    _mm_empty ();
}

static void
mmx_combine_out_u (pixman_implementation_t *imp,
                   pixman_op_t              op,
                   uint32_t *               dest,
                   const uint32_t *         src,
                   const uint32_t *         mask,
                   int                      width)
{
    const uint32_t *end = dest + width;

    while (dest < end)
    {
	__m64 a;
	__m64 x = combine (src, mask);

	a = load8888 (dest);
	a = expand_alpha (a);
	a = negate (a);
	x = pix_multiply (x, a);
	store8888 (dest, x);

	++dest;
	++src;
	if (mask)
	    mask++;
    }
    _mm_empty ();
}

static void
mmx_combine_out_reverse_u (pixman_implementation_t *imp,
                           pixman_op_t              op,
                           uint32_t *               dest,
                           const uint32_t *         src,
                           const uint32_t *         mask,
                           int                      width)
{
    const uint32_t *end = dest + width;

    while (dest < end)
    {
	__m64 a = combine (src, mask);
	__m64 x;

	x = load8888 (dest);
	a = expand_alpha (a);
	a = negate (a);
	x = pix_multiply (x, a);

	store8888 (dest, x);

	++dest;
	++src;
	if (mask)
	    mask++;
    }
    _mm_empty ();
}

static void
mmx_combine_atop_u (pixman_implementation_t *imp,
                    pixman_op_t              op,
                    uint32_t *               dest,
                    const uint32_t *         src,
                    const uint32_t *         mask,
                    int                      width)
{
    const uint32_t *end = dest + width;

    while (dest < end)
    {
	__m64 da, d, sia;
	__m64 s = combine (src, mask);

	d = load8888 (dest);
	sia = expand_alpha (s);
	sia = negate (sia);
	da = expand_alpha (d);
	s = pix_add_mul (s, da, d, sia);
	store8888 (dest, s);

	++dest;
	++src;
	if (mask)
	    mask++;
    }
    _mm_empty ();
}

static void
mmx_combine_atop_reverse_u (pixman_implementation_t *imp,
                            pixman_op_t              op,
                            uint32_t *               dest,
                            const uint32_t *         src,
                            const uint32_t *         mask,
                            int                      width)
{
    const uint32_t *end;

    end = dest + width;

    while (dest < end)
    {
	__m64 dia, d, sa;
	__m64 s = combine (src, mask);

	d = load8888 (dest);
	sa = expand_alpha (s);
	dia = expand_alpha (d);
	dia = negate (dia);
	s = pix_add_mul (s, dia, d, sa);
	store8888 (dest, s);

	++dest;
	++src;
	if (mask)
	    mask++;
    }
    _mm_empty ();
}

static void
mmx_combine_xor_u (pixman_implementation_t *imp,
                   pixman_op_t              op,
                   uint32_t *               dest,
                   const uint32_t *         src,
                   const uint32_t *         mask,
                   int                      width)
{
    const uint32_t *end = dest + width;

    while (dest < end)
    {
	__m64 dia, d, sia;
	__m64 s = combine (src, mask);

	d = load8888 (dest);
	sia = expand_alpha (s);
	dia = expand_alpha (d);
	sia = negate (sia);
	dia = negate (dia);
	s = pix_add_mul (s, dia, d, sia);
	store8888 (dest, s);

	++dest;
	++src;
	if (mask)
	    mask++;
    }
    _mm_empty ();
}

static void
mmx_combine_add_u (pixman_implementation_t *imp,
                   pixman_op_t              op,
                   uint32_t *               dest,
                   const uint32_t *         src,
                   const uint32_t *         mask,
                   int                      width)
{
    const uint32_t *end = dest + width;

    while (dest < end)
    {
	__m64 d;
	__m64 s = combine (src, mask);

	d = load8888 (dest);
	s = pix_add (s, d);
	store8888 (dest, s);

	++dest;
	++src;
	if (mask)
	    mask++;
    }
    _mm_empty ();
}

static void
mmx_combine_saturate_u (pixman_implementation_t *imp,
                        pixman_op_t              op,
                        uint32_t *               dest,
                        const uint32_t *         src,
                        const uint32_t *         mask,
                        int                      width)
{
    const uint32_t *end = dest + width;

    while (dest < end)
    {
	uint32_t s, sa, da;
	uint32_t d = *dest;
	__m64 ms = combine (src, mask);
	__m64 md = load8888 (dest);

	store8888(&s, ms);
	da = ~d >> 24;
	sa = s >> 24;

	if (sa > da)
	{
	    uint32_t quot = DIV_UN8 (da, sa) << 24;
	    __m64 msa = load8888 (&quot);
	    msa = expand_alpha (msa);
	    ms = pix_multiply (ms, msa);
	}

	md = pix_add (md, ms);
	store8888 (dest, md);

	++src;
	++dest;
	if (mask)
	    mask++;
    }
    _mm_empty ();
}

static void
mmx_combine_src_ca (pixman_implementation_t *imp,
                    pixman_op_t              op,
                    uint32_t *               dest,
                    const uint32_t *         src,
                    const uint32_t *         mask,
                    int                      width)
{
    const uint32_t *end = src + width;

    while (src < end)
    {
	__m64 a = load8888 (mask);
	__m64 s = load8888 (src);

	s = pix_multiply (s, a);
	store8888 (dest, s);

	++src;
	++mask;
	++dest;
    }
    _mm_empty ();
}

static void
mmx_combine_over_ca (pixman_implementation_t *imp,
                     pixman_op_t              op,
                     uint32_t *               dest,
                     const uint32_t *         src,
                     const uint32_t *         mask,
                     int                      width)
{
    const uint32_t *end = src + width;

    while (src < end)
    {
	__m64 a = load8888 (mask);
	__m64 s = load8888 (src);
	__m64 d = load8888 (dest);
	__m64 sa = expand_alpha (s);

	store8888 (dest, in_over (s, sa, a, d));

	++src;
	++dest;
	++mask;
    }
    _mm_empty ();
}

static void
mmx_combine_over_reverse_ca (pixman_implementation_t *imp,
                             pixman_op_t              op,
                             uint32_t *               dest,
                             const uint32_t *         src,
                             const uint32_t *         mask,
                             int                      width)
{
    const uint32_t *end = src + width;

    while (src < end)
    {
	__m64 a = load8888 (mask);
	__m64 s = load8888 (src);
	__m64 d = load8888 (dest);
	__m64 da = expand_alpha (d);

	store8888 (dest, over (d, da, in (s, a)));

	++src;
	++dest;
	++mask;
    }
    _mm_empty ();
}

static void
mmx_combine_in_ca (pixman_implementation_t *imp,
                   pixman_op_t              op,
                   uint32_t *               dest,
                   const uint32_t *         src,
                   const uint32_t *         mask,
                   int                      width)
{
    const uint32_t *end = src + width;

    while (src < end)
    {
	__m64 a = load8888 (mask);
	__m64 s = load8888 (src);
	__m64 d = load8888 (dest);
	__m64 da = expand_alpha (d);

	s = pix_multiply (s, a);
	s = pix_multiply (s, da);
	store8888 (dest, s);

	++src;
	++dest;
	++mask;
    }
    _mm_empty ();
}

static void
mmx_combine_in_reverse_ca (pixman_implementation_t *imp,
                           pixman_op_t              op,
                           uint32_t *               dest,
                           const uint32_t *         src,
                           const uint32_t *         mask,
                           int                      width)
{
    const uint32_t *end = src + width;

    while (src < end)
    {
	__m64 a = load8888 (mask);
	__m64 s = load8888 (src);
	__m64 d = load8888 (dest);
	__m64 sa = expand_alpha (s);

	a = pix_multiply (a, sa);
	d = pix_multiply (d, a);
	store8888 (dest, d);

	++src;
	++dest;
	++mask;
    }
    _mm_empty ();
}

static void
mmx_combine_out_ca (pixman_implementation_t *imp,
                    pixman_op_t              op,
                    uint32_t *               dest,
                    const uint32_t *         src,
                    const uint32_t *         mask,
                    int                      width)
{
    const uint32_t *end = src + width;

    while (src < end)
    {
	__m64 a = load8888 (mask);
	__m64 s = load8888 (src);
	__m64 d = load8888 (dest);
	__m64 da = expand_alpha (d);

	da = negate (da);
	s = pix_multiply (s, a);
	s = pix_multiply (s, da);
	store8888 (dest, s);

	++src;
	++dest;
	++mask;
    }
    _mm_empty ();
}

static void
mmx_combine_out_reverse_ca (pixman_implementation_t *imp,
                            pixman_op_t              op,
                            uint32_t *               dest,
                            const uint32_t *         src,
                            const uint32_t *         mask,
                            int                      width)
{
    const uint32_t *end = src + width;

    while (src < end)
    {
	__m64 a = load8888 (mask);
	__m64 s = load8888 (src);
	__m64 d = load8888 (dest);
	__m64 sa = expand_alpha (s);

	a = pix_multiply (a, sa);
	a = negate (a);
	d = pix_multiply (d, a);
	store8888 (dest, d);

	++src;
	++dest;
	++mask;
    }
    _mm_empty ();
}

static void
mmx_combine_atop_ca (pixman_implementation_t *imp,
                     pixman_op_t              op,
                     uint32_t *               dest,
                     const uint32_t *         src,
                     const uint32_t *         mask,
                     int                      width)
{
    const uint32_t *end = src + width;

    while (src < end)
    {
	__m64 a = load8888 (mask);
	__m64 s = load8888 (src);
	__m64 d = load8888 (dest);
	__m64 da = expand_alpha (d);
	__m64 sa = expand_alpha (s);

	s = pix_multiply (s, a);
	a = pix_multiply (a, sa);
	a = negate (a);
	d = pix_add_mul (d, a, s, da);
	store8888 (dest, d);

	++src;
	++dest;
	++mask;
    }
    _mm_empty ();
}

static void
mmx_combine_atop_reverse_ca (pixman_implementation_t *imp,
                             pixman_op_t              op,
                             uint32_t *               dest,
                             const uint32_t *         src,
                             const uint32_t *         mask,
                             int                      width)
{
    const uint32_t *end = src + width;

    while (src < end)
    {
	__m64 a = load8888 (mask);
	__m64 s = load8888 (src);
	__m64 d = load8888 (dest);
	__m64 da = expand_alpha (d);
	__m64 sa = expand_alpha (s);

	s = pix_multiply (s, a);
	a = pix_multiply (a, sa);
	da = negate (da);
	d = pix_add_mul (d, a, s, da);
	store8888 (dest, d);

	++src;
	++dest;
	++mask;
    }
    _mm_empty ();
}

static void
mmx_combine_xor_ca (pixman_implementation_t *imp,
                    pixman_op_t              op,
                    uint32_t *               dest,
                    const uint32_t *         src,
                    const uint32_t *         mask,
                    int                      width)
{
    const uint32_t *end = src + width;

    while (src < end)
    {
	__m64 a = load8888 (mask);
	__m64 s = load8888 (src);
	__m64 d = load8888 (dest);
	__m64 da = expand_alpha (d);
	__m64 sa = expand_alpha (s);

	s = pix_multiply (s, a);
	a = pix_multiply (a, sa);
	da = negate (da);
	a = negate (a);
	d = pix_add_mul (d, a, s, da);
	store8888 (dest, d);

	++src;
	++dest;
	++mask;
    }
    _mm_empty ();
}

static void
mmx_combine_add_ca (pixman_implementation_t *imp,
                    pixman_op_t              op,
                    uint32_t *               dest,
                    const uint32_t *         src,
                    const uint32_t *         mask,
                    int                      width)
{
    const uint32_t *end = src + width;

    while (src < end)
    {
	__m64 a = load8888 (mask);
	__m64 s = load8888 (src);
	__m64 d = load8888 (dest);

	s = pix_multiply (s, a);
	d = pix_add (s, d);
	store8888 (dest, d);

	++src;
	++dest;
	++mask;
    }
    _mm_empty ();
}

/* ------------- MMX code paths called from fbpict.c -------------------- */

static void
mmx_composite_over_n_8888 (pixman_implementation_t *imp,
                           pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t src;
    uint32_t    *dst_line, *dst;
    int32_t w;
    int dst_stride;
    __m64 vsrc, vsrca;

    CHECKPOINT ();

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    if (src == 0)
	return;

    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);

    vsrc = load8888 (&src);
    vsrca = expand_alpha (vsrc);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	w = width;

	CHECKPOINT ();

	while (w && (uintptr_t)dst & 7)
	{
	    store8888 (dst, over (vsrc, vsrca, load8888 (dst)));

	    w--;
	    dst++;
	}

	while (w >= 2)
	{
	    __m64 vdest;
	    __m64 dest0, dest1;

	    vdest = *(__m64 *)dst;

	    dest0 = over (vsrc, vsrca, expand8888 (vdest, 0));
	    dest1 = over (vsrc, vsrca, expand8888 (vdest, 1));

	    *(__m64 *)dst = pack8888 (dest0, dest1);

	    dst += 2;
	    w -= 2;
	}

	CHECKPOINT ();

	if (w)
	{
	    store8888 (dst, over (vsrc, vsrca, load8888 (dst)));
	}
    }

    _mm_empty ();
}

static void
mmx_composite_over_n_0565 (pixman_implementation_t *imp,
                           pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t src;
    uint16_t    *dst_line, *dst;
    int32_t w;
    int dst_stride;
    __m64 vsrc, vsrca;

    CHECKPOINT ();

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    if (src == 0)
	return;

    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);

    vsrc = load8888 (&src);
    vsrca = expand_alpha (vsrc);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	w = width;

	CHECKPOINT ();

	while (w && (uintptr_t)dst & 7)
	{
	    uint64_t d = *dst;
	    __m64 vdest = expand565 (to_m64 (d), 0);

	    vdest = pack_565 (over (vsrc, vsrca, vdest), vdest, 0);
	    *dst = to_uint64 (vdest);

	    w--;
	    dst++;
	}

	while (w >= 4)
	{
	    __m64 vdest = *(__m64 *)dst;
	    __m64 v0, v1, v2, v3;

	    expand_4x565 (vdest, &v0, &v1, &v2, &v3, 0);

	    v0 = over (vsrc, vsrca, v0);
	    v1 = over (vsrc, vsrca, v1);
	    v2 = over (vsrc, vsrca, v2);
	    v3 = over (vsrc, vsrca, v3);

	    *(__m64 *)dst = pack_4x565 (v0, v1, v2, v3);

	    dst += 4;
	    w -= 4;
	}

	CHECKPOINT ();

	while (w)
	{
	    uint64_t d = *dst;
	    __m64 vdest = expand565 (to_m64 (d), 0);

	    vdest = pack_565 (over (vsrc, vsrca, vdest), vdest, 0);
	    *dst = to_uint64 (vdest);

	    w--;
	    dst++;
	}
    }

    _mm_empty ();
}

static void
mmx_composite_over_n_8888_8888_ca (pixman_implementation_t *imp,
                                   pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t src;
    uint32_t    *dst_line;
    uint32_t    *mask_line;
    int dst_stride, mask_stride;
    __m64 vsrc, vsrca;

    CHECKPOINT ();

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    if (src == 0)
	return;

    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (mask_image, mask_x, mask_y, uint32_t, mask_stride, mask_line, 1);

    vsrc = load8888 (&src);
    vsrca = expand_alpha (vsrc);

    while (height--)
    {
	int twidth = width;
	uint32_t *p = (uint32_t *)mask_line;
	uint32_t *q = (uint32_t *)dst_line;

	while (twidth && (uintptr_t)q & 7)
	{
	    uint32_t m = *(uint32_t *)p;

	    if (m)
	    {
		__m64 vdest = load8888 (q);
		vdest = in_over (vsrc, vsrca, load8888 (&m), vdest);
		store8888 (q, vdest);
	    }

	    twidth--;
	    p++;
	    q++;
	}

	while (twidth >= 2)
	{
	    uint32_t m0, m1;
	    m0 = *p;
	    m1 = *(p + 1);

	    if (m0 | m1)
	    {
		__m64 dest0, dest1;
		__m64 vdest = *(__m64 *)q;

		dest0 = in_over (vsrc, vsrca, load8888 (&m0),
		                 expand8888 (vdest, 0));
		dest1 = in_over (vsrc, vsrca, load8888 (&m1),
		                 expand8888 (vdest, 1));

		*(__m64 *)q = pack8888 (dest0, dest1);
	    }

	    p += 2;
	    q += 2;
	    twidth -= 2;
	}

	if (twidth)
	{
	    uint32_t m = *(uint32_t *)p;

	    if (m)
	    {
		__m64 vdest = load8888 (q);
		vdest = in_over (vsrc, vsrca, load8888 (&m), vdest);
		store8888 (q, vdest);
	    }

	    twidth--;
	    p++;
	    q++;
	}

	dst_line += dst_stride;
	mask_line += mask_stride;
    }

    _mm_empty ();
}

static void
mmx_composite_over_8888_n_8888 (pixman_implementation_t *imp,
                                pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t    *dst_line, *dst;
    uint32_t    *src_line, *src;
    uint32_t mask;
    __m64 vmask;
    int dst_stride, src_stride;
    int32_t w;

    CHECKPOINT ();

    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);

    mask = _pixman_image_get_solid (imp, mask_image, dest_image->bits.format);
    vmask = expand_alpha (load8888 (&mask));

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;
	w = width;

	while (w && (uintptr_t)dst & 7)
	{
	    __m64 s = load8888 (src);
	    __m64 d = load8888 (dst);

	    store8888 (dst, in_over (s, expand_alpha (s), vmask, d));

	    w--;
	    dst++;
	    src++;
	}

	while (w >= 2)
	{
	    __m64 vs = ldq_u ((__m64 *)src);
	    __m64 vd = *(__m64 *)dst;
	    __m64 vsrc0 = expand8888 (vs, 0);
	    __m64 vsrc1 = expand8888 (vs, 1);

	    *(__m64 *)dst = pack8888 (
	        in_over (vsrc0, expand_alpha (vsrc0), vmask, expand8888 (vd, 0)),
	        in_over (vsrc1, expand_alpha (vsrc1), vmask, expand8888 (vd, 1)));

	    w -= 2;
	    dst += 2;
	    src += 2;
	}

	if (w)
	{
	    __m64 s = load8888 (src);
	    __m64 d = load8888 (dst);

	    store8888 (dst, in_over (s, expand_alpha (s), vmask, d));
	}
    }

    _mm_empty ();
}

static void
mmx_composite_over_x888_n_8888 (pixman_implementation_t *imp,
                                pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t *dst_line, *dst;
    uint32_t *src_line, *src;
    uint32_t mask;
    __m64 vmask;
    int dst_stride, src_stride;
    int32_t w;
    __m64 srca;

    CHECKPOINT ();

    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);
    mask = _pixman_image_get_solid (imp, mask_image, dest_image->bits.format);

    vmask = expand_alpha (load8888 (&mask));
    srca = MC (4x00ff);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;
	w = width;

	while (w && (uintptr_t)dst & 7)
	{
	    uint32_t ssrc = *src | 0xff000000;
	    __m64 s = load8888 (&ssrc);
	    __m64 d = load8888 (dst);

	    store8888 (dst, in_over (s, srca, vmask, d));

	    w--;
	    dst++;
	    src++;
	}

	while (w >= 16)
	{
	    __m64 vd0 = *(__m64 *)(dst + 0);
	    __m64 vd1 = *(__m64 *)(dst + 2);
	    __m64 vd2 = *(__m64 *)(dst + 4);
	    __m64 vd3 = *(__m64 *)(dst + 6);
	    __m64 vd4 = *(__m64 *)(dst + 8);
	    __m64 vd5 = *(__m64 *)(dst + 10);
	    __m64 vd6 = *(__m64 *)(dst + 12);
	    __m64 vd7 = *(__m64 *)(dst + 14);

	    __m64 vs0 = ldq_u ((__m64 *)(src + 0));
	    __m64 vs1 = ldq_u ((__m64 *)(src + 2));
	    __m64 vs2 = ldq_u ((__m64 *)(src + 4));
	    __m64 vs3 = ldq_u ((__m64 *)(src + 6));
	    __m64 vs4 = ldq_u ((__m64 *)(src + 8));
	    __m64 vs5 = ldq_u ((__m64 *)(src + 10));
	    __m64 vs6 = ldq_u ((__m64 *)(src + 12));
	    __m64 vs7 = ldq_u ((__m64 *)(src + 14));

	    vd0 = pack8888 (
	        in_over (expandx888 (vs0, 0), srca, vmask, expand8888 (vd0, 0)),
	        in_over (expandx888 (vs0, 1), srca, vmask, expand8888 (vd0, 1)));

	    vd1 = pack8888 (
	        in_over (expandx888 (vs1, 0), srca, vmask, expand8888 (vd1, 0)),
	        in_over (expandx888 (vs1, 1), srca, vmask, expand8888 (vd1, 1)));

	    vd2 = pack8888 (
	        in_over (expandx888 (vs2, 0), srca, vmask, expand8888 (vd2, 0)),
	        in_over (expandx888 (vs2, 1), srca, vmask, expand8888 (vd2, 1)));

	    vd3 = pack8888 (
	        in_over (expandx888 (vs3, 0), srca, vmask, expand8888 (vd3, 0)),
	        in_over (expandx888 (vs3, 1), srca, vmask, expand8888 (vd3, 1)));

	    vd4 = pack8888 (
	        in_over (expandx888 (vs4, 0), srca, vmask, expand8888 (vd4, 0)),
	        in_over (expandx888 (vs4, 1), srca, vmask, expand8888 (vd4, 1)));

	    vd5 = pack8888 (
	        in_over (expandx888 (vs5, 0), srca, vmask, expand8888 (vd5, 0)),
	        in_over (expandx888 (vs5, 1), srca, vmask, expand8888 (vd5, 1)));

	    vd6 = pack8888 (
	        in_over (expandx888 (vs6, 0), srca, vmask, expand8888 (vd6, 0)),
	        in_over (expandx888 (vs6, 1), srca, vmask, expand8888 (vd6, 1)));

	    vd7 = pack8888 (
	        in_over (expandx888 (vs7, 0), srca, vmask, expand8888 (vd7, 0)),
	        in_over (expandx888 (vs7, 1), srca, vmask, expand8888 (vd7, 1)));

	    *(__m64 *)(dst + 0) = vd0;
	    *(__m64 *)(dst + 2) = vd1;
	    *(__m64 *)(dst + 4) = vd2;
	    *(__m64 *)(dst + 6) = vd3;
	    *(__m64 *)(dst + 8) = vd4;
	    *(__m64 *)(dst + 10) = vd5;
	    *(__m64 *)(dst + 12) = vd6;
	    *(__m64 *)(dst + 14) = vd7;

	    w -= 16;
	    dst += 16;
	    src += 16;
	}

	while (w)
	{
	    uint32_t ssrc = *src | 0xff000000;
	    __m64 s = load8888 (&ssrc);
	    __m64 d = load8888 (dst);

	    store8888 (dst, in_over (s, srca, vmask, d));

	    w--;
	    dst++;
	    src++;
	}
    }

    _mm_empty ();
}

static void
mmx_composite_over_8888_8888 (pixman_implementation_t *imp,
                              pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t *dst_line, *dst;
    uint32_t *src_line, *src;
    uint32_t s;
    int dst_stride, src_stride;
    uint8_t a;
    int32_t w;

    CHECKPOINT ();

    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;
	w = width;

	while (w--)
	{
	    s = *src++;
	    a = s >> 24;

	    if (a == 0xff)
	    {
		*dst = s;
	    }
	    else if (s)
	    {
		__m64 ms, sa;
		ms = load8888 (&s);
		sa = expand_alpha (ms);
		store8888 (dst, over (ms, sa, load8888 (dst)));
	    }

	    dst++;
	}
    }
    _mm_empty ();
}

static void
mmx_composite_over_8888_0565 (pixman_implementation_t *imp,
                              pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint16_t    *dst_line, *dst;
    uint32_t    *src_line, *src;
    int dst_stride, src_stride;
    int32_t w;

    CHECKPOINT ();

    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);

#if 0
    /* FIXME */
    assert (src_image->drawable == mask_image->drawable);
#endif

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;
	w = width;

	CHECKPOINT ();

	while (w && (uintptr_t)dst & 7)
	{
	    __m64 vsrc = load8888 (src);
	    uint64_t d = *dst;
	    __m64 vdest = expand565 (to_m64 (d), 0);

	    vdest = pack_565 (
		over (vsrc, expand_alpha (vsrc), vdest), vdest, 0);

	    *dst = to_uint64 (vdest);

	    w--;
	    dst++;
	    src++;
	}

	CHECKPOINT ();

	while (w >= 4)
	{
	    __m64 vdest = *(__m64 *)dst;
	    __m64 v0, v1, v2, v3;
	    __m64 vsrc0, vsrc1, vsrc2, vsrc3;

	    expand_4x565 (vdest, &v0, &v1, &v2, &v3, 0);

	    vsrc0 = load8888 ((src + 0));
	    vsrc1 = load8888 ((src + 1));
	    vsrc2 = load8888 ((src + 2));
	    vsrc3 = load8888 ((src + 3));

	    v0 = over (vsrc0, expand_alpha (vsrc0), v0);
	    v1 = over (vsrc1, expand_alpha (vsrc1), v1);
	    v2 = over (vsrc2, expand_alpha (vsrc2), v2);
	    v3 = over (vsrc3, expand_alpha (vsrc3), v3);

	    *(__m64 *)dst = pack_4x565 (v0, v1, v2, v3);

	    w -= 4;
	    dst += 4;
	    src += 4;
	}

	CHECKPOINT ();

	while (w)
	{
	    __m64 vsrc = load8888 (src);
	    uint64_t d = *dst;
	    __m64 vdest = expand565 (to_m64 (d), 0);

	    vdest = pack_565 (over (vsrc, expand_alpha (vsrc), vdest), vdest, 0);

	    *dst = to_uint64 (vdest);

	    w--;
	    dst++;
	    src++;
	}
    }

    _mm_empty ();
}

static void
mmx_composite_over_n_8_8888 (pixman_implementation_t *imp,
                             pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t src, srca;
    uint32_t *dst_line, *dst;
    uint8_t *mask_line, *mask;
    int dst_stride, mask_stride;
    int32_t w;
    __m64 vsrc, vsrca;
    uint64_t srcsrc;

    CHECKPOINT ();

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    srca = src >> 24;
    if (src == 0)
	return;

    srcsrc = (uint64_t)src << 32 | src;

    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (mask_image, mask_x, mask_y, uint8_t, mask_stride, mask_line, 1);

    vsrc = load8888 (&src);
    vsrca = expand_alpha (vsrc);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	mask = mask_line;
	mask_line += mask_stride;
	w = width;

	CHECKPOINT ();

	while (w && (uintptr_t)dst & 7)
	{
	    uint64_t m = *mask;

	    if (m)
	    {
		__m64 vdest = in_over (vsrc, vsrca,
				       expand_alpha_rev (to_m64 (m)),
				       load8888 (dst));

		store8888 (dst, vdest);
	    }

	    w--;
	    mask++;
	    dst++;
	}

	CHECKPOINT ();

	while (w >= 2)
	{
	    uint64_t m0, m1;

	    m0 = *mask;
	    m1 = *(mask + 1);

	    if (srca == 0xff && (m0 & m1) == 0xff)
	    {
		*(uint64_t *)dst = srcsrc;
	    }
	    else if (m0 | m1)
	    {
		__m64 vdest;
		__m64 dest0, dest1;

		vdest = *(__m64 *)dst;

		dest0 = in_over (vsrc, vsrca, expand_alpha_rev (to_m64 (m0)),
				 expand8888 (vdest, 0));
		dest1 = in_over (vsrc, vsrca, expand_alpha_rev (to_m64 (m1)),
				 expand8888 (vdest, 1));

		*(__m64 *)dst = pack8888 (dest0, dest1);
	    }

	    mask += 2;
	    dst += 2;
	    w -= 2;
	}

	CHECKPOINT ();

	if (w)
	{
	    uint64_t m = *mask;

	    if (m)
	    {
		__m64 vdest = load8888 (dst);

		vdest = in_over (
		    vsrc, vsrca, expand_alpha_rev (to_m64 (m)), vdest);
		store8888 (dst, vdest);
	    }
	}
    }

    _mm_empty ();
}

static pixman_bool_t
mmx_fill (pixman_implementation_t *imp,
          uint32_t *               bits,
          int                      stride,
          int                      bpp,
          int                      x,
          int                      y,
          int                      width,
          int                      height,
          uint32_t		   filler)
{
    uint64_t fill;
    __m64 vfill;
    uint32_t byte_width;
    uint8_t     *byte_line;

#if defined __GNUC__ && defined USE_X86_MMX
    __m64 v1, v2, v3, v4, v5, v6, v7;
#endif

    if (bpp != 16 && bpp != 32 && bpp != 8)
	return FALSE;

    if (bpp == 8)
    {
	stride = stride * (int) sizeof (uint32_t) / 1;
	byte_line = (uint8_t *)(((uint8_t *)bits) + stride * y + x);
	byte_width = width;
	stride *= 1;
        filler = (filler & 0xff) * 0x01010101;
    }
    else if (bpp == 16)
    {
	stride = stride * (int) sizeof (uint32_t) / 2;
	byte_line = (uint8_t *)(((uint16_t *)bits) + stride * y + x);
	byte_width = 2 * width;
	stride *= 2;
        filler = (filler & 0xffff) * 0x00010001;
    }
    else
    {
	stride = stride * (int) sizeof (uint32_t) / 4;
	byte_line = (uint8_t *)(((uint32_t *)bits) + stride * y + x);
	byte_width = 4 * width;
	stride *= 4;
    }

    fill = ((uint64_t)filler << 32) | filler;
    vfill = to_m64 (fill);

#if defined __GNUC__ && defined USE_X86_MMX
    __asm__ (
        "movq		%7,	%0\n"
        "movq		%7,	%1\n"
        "movq		%7,	%2\n"
        "movq		%7,	%3\n"
        "movq		%7,	%4\n"
        "movq		%7,	%5\n"
        "movq		%7,	%6\n"
	: "=&y" (v1), "=&y" (v2), "=&y" (v3),
	  "=&y" (v4), "=&y" (v5), "=&y" (v6), "=y" (v7)
	: "y" (vfill));
#endif

    while (height--)
    {
	int w;
	uint8_t *d = byte_line;

	byte_line += stride;
	w = byte_width;

	if (w >= 1 && ((uintptr_t)d & 1))
	{
	    *(uint8_t *)d = (filler & 0xff);
	    w--;
	    d++;
	}

	if (w >= 2 && ((uintptr_t)d & 3))
	{
	    *(uint16_t *)d = filler;
	    w -= 2;
	    d += 2;
	}

	while (w >= 4 && ((uintptr_t)d & 7))
	{
	    *(uint32_t *)d = filler;

	    w -= 4;
	    d += 4;
	}

	while (w >= 64)
	{
#if defined __GNUC__ && defined USE_X86_MMX
	    __asm__ (
	        "movq	%1,	  (%0)\n"
	        "movq	%2,	 8(%0)\n"
	        "movq	%3,	16(%0)\n"
	        "movq	%4,	24(%0)\n"
	        "movq	%5,	32(%0)\n"
	        "movq	%6,	40(%0)\n"
	        "movq	%7,	48(%0)\n"
	        "movq	%8,	56(%0)\n"
		:
		: "r" (d),
		  "y" (vfill), "y" (v1), "y" (v2), "y" (v3),
		  "y" (v4), "y" (v5), "y" (v6), "y" (v7)
		: "memory");
#else
	    *(__m64*) (d +  0) = vfill;
	    *(__m64*) (d +  8) = vfill;
	    *(__m64*) (d + 16) = vfill;
	    *(__m64*) (d + 24) = vfill;
	    *(__m64*) (d + 32) = vfill;
	    *(__m64*) (d + 40) = vfill;
	    *(__m64*) (d + 48) = vfill;
	    *(__m64*) (d + 56) = vfill;
#endif
	    w -= 64;
	    d += 64;
	}

	while (w >= 4)
	{
	    *(uint32_t *)d = filler;

	    w -= 4;
	    d += 4;
	}
	if (w >= 2)
	{
	    *(uint16_t *)d = filler;
	    w -= 2;
	    d += 2;
	}
	if (w >= 1)
	{
	    *(uint8_t *)d = (filler & 0xff);
	    w--;
	    d++;
	}

    }

    _mm_empty ();
    return TRUE;
}

static void
mmx_composite_src_x888_0565 (pixman_implementation_t *imp,
                             pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint16_t    *dst_line, *dst;
    uint32_t    *src_line, *src, s;
    int dst_stride, src_stride;
    int32_t w;

    PIXMAN_IMAGE_GET_LINE (src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);
    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;
	w = width;

	while (w && (uintptr_t)dst & 7)
	{
	    s = *src++;
	    *dst = convert_8888_to_0565 (s);
	    dst++;
	    w--;
	}

	while (w >= 4)
	{
	    __m64 vdest;
	    __m64 vsrc0 = ldq_u ((__m64 *)(src + 0));
	    __m64 vsrc1 = ldq_u ((__m64 *)(src + 2));

	    vdest = pack_4xpacked565 (vsrc0, vsrc1);

	    *(__m64 *)dst = vdest;

	    w -= 4;
	    src += 4;
	    dst += 4;
	}

	while (w)
	{
	    s = *src++;
	    *dst = convert_8888_to_0565 (s);
	    dst++;
	    w--;
	}
    }

    _mm_empty ();
}

static void
mmx_composite_src_n_8_8888 (pixman_implementation_t *imp,
                            pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t src, srca;
    uint32_t    *dst_line, *dst;
    uint8_t     *mask_line, *mask;
    int dst_stride, mask_stride;
    int32_t w;
    __m64 vsrc;
    uint64_t srcsrc;

    CHECKPOINT ();

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    srca = src >> 24;
    if (src == 0)
    {
	mmx_fill (imp, dest_image->bits.bits, dest_image->bits.rowstride,
		  PIXMAN_FORMAT_BPP (dest_image->bits.format),
		  dest_x, dest_y, width, height, 0);
	return;
    }

    srcsrc = (uint64_t)src << 32 | src;

    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (mask_image, mask_x, mask_y, uint8_t, mask_stride, mask_line, 1);

    vsrc = load8888 (&src);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	mask = mask_line;
	mask_line += mask_stride;
	w = width;

	CHECKPOINT ();

	while (w && (uintptr_t)dst & 7)
	{
	    uint64_t m = *mask;

	    if (m)
	    {
		__m64 vdest = in (vsrc, expand_alpha_rev (to_m64 (m)));

		store8888 (dst, vdest);
	    }
	    else
	    {
		*dst = 0;
	    }

	    w--;
	    mask++;
	    dst++;
	}

	CHECKPOINT ();

	while (w >= 2)
	{
	    uint64_t m0, m1;
	    m0 = *mask;
	    m1 = *(mask + 1);

	    if (srca == 0xff && (m0 & m1) == 0xff)
	    {
		*(uint64_t *)dst = srcsrc;
	    }
	    else if (m0 | m1)
	    {
		__m64 dest0, dest1;

		dest0 = in (vsrc, expand_alpha_rev (to_m64 (m0)));
		dest1 = in (vsrc, expand_alpha_rev (to_m64 (m1)));

		*(__m64 *)dst = pack8888 (dest0, dest1);
	    }
	    else
	    {
		*(uint64_t *)dst = 0;
	    }

	    mask += 2;
	    dst += 2;
	    w -= 2;
	}

	CHECKPOINT ();

	if (w)
	{
	    uint64_t m = *mask;

	    if (m)
	    {
		__m64 vdest = load8888 (dst);

		vdest = in (vsrc, expand_alpha_rev (to_m64 (m)));
		store8888 (dst, vdest);
	    }
	    else
	    {
		*dst = 0;
	    }
	}
    }

    _mm_empty ();
}

static void
mmx_composite_over_n_8_0565 (pixman_implementation_t *imp,
                             pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t src, srca;
    uint16_t *dst_line, *dst;
    uint8_t *mask_line, *mask;
    int dst_stride, mask_stride;
    int32_t w;
    __m64 vsrc, vsrca, tmp;
    __m64 srcsrcsrcsrc;

    CHECKPOINT ();

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    srca = src >> 24;
    if (src == 0)
	return;

    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (mask_image, mask_x, mask_y, uint8_t, mask_stride, mask_line, 1);

    vsrc = load8888 (&src);
    vsrca = expand_alpha (vsrc);

    tmp = pack_565 (vsrc, _mm_setzero_si64 (), 0);
    srcsrcsrcsrc = expand_alpha_rev (tmp);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	mask = mask_line;
	mask_line += mask_stride;
	w = width;

	CHECKPOINT ();

	while (w && (uintptr_t)dst & 7)
	{
	    uint64_t m = *mask;

	    if (m)
	    {
		uint64_t d = *dst;
		__m64 vd = to_m64 (d);
		__m64 vdest = in_over (
		    vsrc, vsrca, expand_alpha_rev (to_m64 (m)), expand565 (vd, 0));

		vd = pack_565 (vdest, _mm_setzero_si64 (), 0);
		*dst = to_uint64 (vd);
	    }

	    w--;
	    mask++;
	    dst++;
	}

	CHECKPOINT ();

	while (w >= 4)
	{
	    uint64_t m0, m1, m2, m3;
	    m0 = *mask;
	    m1 = *(mask + 1);
	    m2 = *(mask + 2);
	    m3 = *(mask + 3);

	    if (srca == 0xff && (m0 & m1 & m2 & m3) == 0xff)
	    {
		*(__m64 *)dst = srcsrcsrcsrc;
	    }
	    else if (m0 | m1 | m2 | m3)
	    {
		__m64 vdest = *(__m64 *)dst;
		__m64 v0, v1, v2, v3;
		__m64 vm0, vm1, vm2, vm3;

		expand_4x565 (vdest, &v0, &v1, &v2, &v3, 0);

		vm0 = to_m64 (m0);
		v0 = in_over (vsrc, vsrca, expand_alpha_rev (vm0), v0);

		vm1 = to_m64 (m1);
		v1 = in_over (vsrc, vsrca, expand_alpha_rev (vm1), v1);

		vm2 = to_m64 (m2);
		v2 = in_over (vsrc, vsrca, expand_alpha_rev (vm2), v2);

		vm3 = to_m64 (m3);
		v3 = in_over (vsrc, vsrca, expand_alpha_rev (vm3), v3);

		*(__m64 *)dst = pack_4x565 (v0, v1, v2, v3);;
	    }

	    w -= 4;
	    mask += 4;
	    dst += 4;
	}

	CHECKPOINT ();

	while (w)
	{
	    uint64_t m = *mask;

	    if (m)
	    {
		uint64_t d = *dst;
		__m64 vd = to_m64 (d);
		__m64 vdest = in_over (vsrc, vsrca, expand_alpha_rev (to_m64 (m)),
				       expand565 (vd, 0));
		vd = pack_565 (vdest, _mm_setzero_si64 (), 0);
		*dst = to_uint64 (vd);
	    }

	    w--;
	    mask++;
	    dst++;
	}
    }

    _mm_empty ();
}

static void
mmx_composite_over_pixbuf_0565 (pixman_implementation_t *imp,
                                pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint16_t    *dst_line, *dst;
    uint32_t    *src_line, *src;
    int dst_stride, src_stride;
    int32_t w;

    CHECKPOINT ();

    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);

#if 0
    /* FIXME */
    assert (src_image->drawable == mask_image->drawable);
#endif

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;
	w = width;

	CHECKPOINT ();

	while (w && (uintptr_t)dst & 7)
	{
	    __m64 vsrc = load8888 (src);
	    uint64_t d = *dst;
	    __m64 vdest = expand565 (to_m64 (d), 0);

	    vdest = pack_565 (over_rev_non_pre (vsrc, vdest), vdest, 0);

	    *dst = to_uint64 (vdest);

	    w--;
	    dst++;
	    src++;
	}

	CHECKPOINT ();

	while (w >= 4)
	{
	    uint32_t s0, s1, s2, s3;
	    unsigned char a0, a1, a2, a3;

	    s0 = *src;
	    s1 = *(src + 1);
	    s2 = *(src + 2);
	    s3 = *(src + 3);

	    a0 = (s0 >> 24);
	    a1 = (s1 >> 24);
	    a2 = (s2 >> 24);
	    a3 = (s3 >> 24);

	    if ((a0 & a1 & a2 & a3) == 0xFF)
	    {
		__m64 v0 = invert_colors (load8888 (&s0));
		__m64 v1 = invert_colors (load8888 (&s1));
		__m64 v2 = invert_colors (load8888 (&s2));
		__m64 v3 = invert_colors (load8888 (&s3));

		*(__m64 *)dst = pack_4x565 (v0, v1, v2, v3);
	    }
	    else if (s0 | s1 | s2 | s3)
	    {
		__m64 vdest = *(__m64 *)dst;
		__m64 v0, v1, v2, v3;

		__m64 vsrc0 = load8888 (&s0);
		__m64 vsrc1 = load8888 (&s1);
		__m64 vsrc2 = load8888 (&s2);
		__m64 vsrc3 = load8888 (&s3);

		expand_4x565 (vdest, &v0, &v1, &v2, &v3, 0);

		v0 = over_rev_non_pre (vsrc0, v0);
		v1 = over_rev_non_pre (vsrc1, v1);
		v2 = over_rev_non_pre (vsrc2, v2);
		v3 = over_rev_non_pre (vsrc3, v3);

		*(__m64 *)dst = pack_4x565 (v0, v1, v2, v3);
	    }

	    w -= 4;
	    dst += 4;
	    src += 4;
	}

	CHECKPOINT ();

	while (w)
	{
	    __m64 vsrc = load8888 (src);
	    uint64_t d = *dst;
	    __m64 vdest = expand565 (to_m64 (d), 0);

	    vdest = pack_565 (over_rev_non_pre (vsrc, vdest), vdest, 0);

	    *dst = to_uint64 (vdest);

	    w--;
	    dst++;
	    src++;
	}
    }

    _mm_empty ();
}

static void
mmx_composite_over_pixbuf_8888 (pixman_implementation_t *imp,
                                pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t    *dst_line, *dst;
    uint32_t    *src_line, *src;
    int dst_stride, src_stride;
    int32_t w;

    CHECKPOINT ();

    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);

#if 0
    /* FIXME */
    assert (src_image->drawable == mask_image->drawable);
#endif

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;
	w = width;

	while (w && (uintptr_t)dst & 7)
	{
	    __m64 s = load8888 (src);
	    __m64 d = load8888 (dst);

	    store8888 (dst, over_rev_non_pre (s, d));

	    w--;
	    dst++;
	    src++;
	}

	while (w >= 2)
	{
	    uint32_t s0, s1;
	    unsigned char a0, a1;
	    __m64 d0, d1;

	    s0 = *src;
	    s1 = *(src + 1);

	    a0 = (s0 >> 24);
	    a1 = (s1 >> 24);

	    if ((a0 & a1) == 0xFF)
	    {
		d0 = invert_colors (load8888 (&s0));
		d1 = invert_colors (load8888 (&s1));

		*(__m64 *)dst = pack8888 (d0, d1);
	    }
	    else if (s0 | s1)
	    {
		__m64 vdest = *(__m64 *)dst;

		d0 = over_rev_non_pre (load8888 (&s0), expand8888 (vdest, 0));
		d1 = over_rev_non_pre (load8888 (&s1), expand8888 (vdest, 1));

		*(__m64 *)dst = pack8888 (d0, d1);
	    }

	    w -= 2;
	    dst += 2;
	    src += 2;
	}

	if (w)
	{
	    __m64 s = load8888 (src);
	    __m64 d = load8888 (dst);

	    store8888 (dst, over_rev_non_pre (s, d));
	}
    }

    _mm_empty ();
}

static void
mmx_composite_over_n_8888_0565_ca (pixman_implementation_t *imp,
                                   pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t src;
    uint16_t    *dst_line;
    uint32_t    *mask_line;
    int dst_stride, mask_stride;
    __m64 vsrc, vsrca;

    CHECKPOINT ();

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    if (src == 0)
	return;

    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (mask_image, mask_x, mask_y, uint32_t, mask_stride, mask_line, 1);

    vsrc = load8888 (&src);
    vsrca = expand_alpha (vsrc);

    while (height--)
    {
	int twidth = width;
	uint32_t *p = (uint32_t *)mask_line;
	uint16_t *q = (uint16_t *)dst_line;

	while (twidth && ((uintptr_t)q & 7))
	{
	    uint32_t m = *(uint32_t *)p;

	    if (m)
	    {
		uint64_t d = *q;
		__m64 vdest = expand565 (to_m64 (d), 0);
		vdest = pack_565 (in_over (vsrc, vsrca, load8888 (&m), vdest), vdest, 0);
		*q = to_uint64 (vdest);
	    }

	    twidth--;
	    p++;
	    q++;
	}

	while (twidth >= 4)
	{
	    uint32_t m0, m1, m2, m3;

	    m0 = *p;
	    m1 = *(p + 1);
	    m2 = *(p + 2);
	    m3 = *(p + 3);

	    if ((m0 | m1 | m2 | m3))
	    {
		__m64 vdest = *(__m64 *)q;
		__m64 v0, v1, v2, v3;

		expand_4x565 (vdest, &v0, &v1, &v2, &v3, 0);

		v0 = in_over (vsrc, vsrca, load8888 (&m0), v0);
		v1 = in_over (vsrc, vsrca, load8888 (&m1), v1);
		v2 = in_over (vsrc, vsrca, load8888 (&m2), v2);
		v3 = in_over (vsrc, vsrca, load8888 (&m3), v3);

		*(__m64 *)q = pack_4x565 (v0, v1, v2, v3);
	    }
	    twidth -= 4;
	    p += 4;
	    q += 4;
	}

	while (twidth)
	{
	    uint32_t m;

	    m = *(uint32_t *)p;
	    if (m)
	    {
		uint64_t d = *q;
		__m64 vdest = expand565 (to_m64 (d), 0);
		vdest = pack_565 (in_over (vsrc, vsrca, load8888 (&m), vdest), vdest, 0);
		*q = to_uint64 (vdest);
	    }

	    twidth--;
	    p++;
	    q++;
	}

	mask_line += mask_stride;
	dst_line += dst_stride;
    }

    _mm_empty ();
}

static void
mmx_composite_in_n_8_8 (pixman_implementation_t *imp,
                        pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint8_t *dst_line, *dst;
    uint8_t *mask_line, *mask;
    int dst_stride, mask_stride;
    int32_t w;
    uint32_t src;
    uint8_t sa;
    __m64 vsrc, vsrca;

    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (mask_image, mask_x, mask_y, uint8_t, mask_stride, mask_line, 1);

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    sa = src >> 24;

    vsrc = load8888 (&src);
    vsrca = expand_alpha (vsrc);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	mask = mask_line;
	mask_line += mask_stride;
	w = width;

	while (w && (uintptr_t)dst & 7)
	{
	    uint16_t tmp;
	    uint8_t a;
	    uint32_t m, d;

	    a = *mask++;
	    d = *dst;

	    m = MUL_UN8 (sa, a, tmp);
	    d = MUL_UN8 (m, d, tmp);

	    *dst++ = d;
	    w--;
	}

	while (w >= 4)
	{
	    __m64 vmask;
	    __m64 vdest;

	    vmask = load8888u ((uint32_t *)mask);
	    vdest = load8888 ((uint32_t *)dst);

	    store8888 ((uint32_t *)dst, in (in (vsrca, vmask), vdest));

	    dst += 4;
	    mask += 4;
	    w -= 4;
	}

	while (w--)
	{
	    uint16_t tmp;
	    uint8_t a;
	    uint32_t m, d;

	    a = *mask++;
	    d = *dst;

	    m = MUL_UN8 (sa, a, tmp);
	    d = MUL_UN8 (m, d, tmp);

	    *dst++ = d;
	}
    }

    _mm_empty ();
}

static void
mmx_composite_in_8_8 (pixman_implementation_t *imp,
                      pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint8_t     *dst_line, *dst;
    uint8_t     *src_line, *src;
    int src_stride, dst_stride;
    int32_t w;

    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (src_image, src_x, src_y, uint8_t, src_stride, src_line, 1);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;
	w = width;

	while (w && (uintptr_t)dst & 3)
	{
	    uint8_t s, d;
	    uint16_t tmp;

	    s = *src;
	    d = *dst;

	    *dst = MUL_UN8 (s, d, tmp);

	    src++;
	    dst++;
	    w--;
	}

	while (w >= 4)
	{
	    uint32_t *s = (uint32_t *)src;
	    uint32_t *d = (uint32_t *)dst;

	    store8888 (d, in (load8888u (s), load8888 (d)));

	    w -= 4;
	    dst += 4;
	    src += 4;
	}

	while (w--)
	{
	    uint8_t s, d;
	    uint16_t tmp;

	    s = *src;
	    d = *dst;

	    *dst = MUL_UN8 (s, d, tmp);

	    src++;
	    dst++;
	}
    }

    _mm_empty ();
}

static void
mmx_composite_add_n_8_8 (pixman_implementation_t *imp,
			 pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint8_t     *dst_line, *dst;
    uint8_t     *mask_line, *mask;
    int dst_stride, mask_stride;
    int32_t w;
    uint32_t src;
    uint8_t sa;
    __m64 vsrc, vsrca;

    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (mask_image, mask_x, mask_y, uint8_t, mask_stride, mask_line, 1);

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    sa = src >> 24;

    if (src == 0)
	return;

    vsrc = load8888 (&src);
    vsrca = expand_alpha (vsrc);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	mask = mask_line;
	mask_line += mask_stride;
	w = width;

	while (w && (uintptr_t)dst & 3)
	{
	    uint16_t tmp;
	    uint16_t a;
	    uint32_t m, d;
	    uint32_t r;

	    a = *mask++;
	    d = *dst;

	    m = MUL_UN8 (sa, a, tmp);
	    r = ADD_UN8 (m, d, tmp);

	    *dst++ = r;
	    w--;
	}

	while (w >= 4)
	{
	    __m64 vmask;
	    __m64 vdest;

	    vmask = load8888u ((uint32_t *)mask);
	    vdest = load8888 ((uint32_t *)dst);

	    store8888 ((uint32_t *)dst, _mm_adds_pu8 (in (vsrca, vmask), vdest));

	    dst += 4;
	    mask += 4;
	    w -= 4;
	}

	while (w--)
	{
	    uint16_t tmp;
	    uint16_t a;
	    uint32_t m, d;
	    uint32_t r;

	    a = *mask++;
	    d = *dst;

	    m = MUL_UN8 (sa, a, tmp);
	    r = ADD_UN8 (m, d, tmp);

	    *dst++ = r;
	}
    }

    _mm_empty ();
}

static void
mmx_composite_add_8_8 (pixman_implementation_t *imp,
		       pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint8_t *dst_line, *dst;
    uint8_t *src_line, *src;
    int dst_stride, src_stride;
    int32_t w;
    uint8_t s, d;
    uint16_t t;

    CHECKPOINT ();

    PIXMAN_IMAGE_GET_LINE (src_image, src_x, src_y, uint8_t, src_stride, src_line, 1);
    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;
	w = width;

	while (w && (uintptr_t)dst & 7)
	{
	    s = *src;
	    d = *dst;
	    t = d + s;
	    s = t | (0 - (t >> 8));
	    *dst = s;

	    dst++;
	    src++;
	    w--;
	}

	while (w >= 8)
	{
	    *(__m64*)dst = _mm_adds_pu8 (ldq_u ((__m64 *)src), *(__m64*)dst);
	    dst += 8;
	    src += 8;
	    w -= 8;
	}

	while (w)
	{
	    s = *src;
	    d = *dst;
	    t = d + s;
	    s = t | (0 - (t >> 8));
	    *dst = s;

	    dst++;
	    src++;
	    w--;
	}
    }

    _mm_empty ();
}

static void
mmx_composite_add_0565_0565 (pixman_implementation_t *imp,
                             pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint16_t    *dst_line, *dst;
    uint32_t	d;
    uint16_t    *src_line, *src;
    uint32_t	s;
    int dst_stride, src_stride;
    int32_t w;

    CHECKPOINT ();

    PIXMAN_IMAGE_GET_LINE (src_image, src_x, src_y, uint16_t, src_stride, src_line, 1);
    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;
	w = width;

	while (w && (uintptr_t)dst & 7)
	{
	    s = *src++;
	    if (s)
	    {
		d = *dst;
		s = convert_0565_to_8888 (s);
		if (d)
		{
		    d = convert_0565_to_8888 (d);
		    UN8x4_ADD_UN8x4 (s, d);
		}
		*dst = convert_8888_to_0565 (s);
	    }
	    dst++;
	    w--;
	}

	while (w >= 4)
	{
	    __m64 vdest = *(__m64 *)dst;
	    __m64 vsrc = ldq_u ((__m64 *)src);
	    __m64 vd0, vd1;
	    __m64 vs0, vs1;

	    expand_4xpacked565 (vdest, &vd0, &vd1, 0);
	    expand_4xpacked565 (vsrc, &vs0, &vs1, 0);

	    vd0 = _mm_adds_pu8 (vd0, vs0);
	    vd1 = _mm_adds_pu8 (vd1, vs1);

	    *(__m64 *)dst = pack_4xpacked565 (vd0, vd1);

	    dst += 4;
	    src += 4;
	    w -= 4;
	}

	while (w--)
	{
	    s = *src++;
	    if (s)
	    {
		d = *dst;
		s = convert_0565_to_8888 (s);
		if (d)
		{
		    d = convert_0565_to_8888 (d);
		    UN8x4_ADD_UN8x4 (s, d);
		}
		*dst = convert_8888_to_0565 (s);
	    }
	    dst++;
	}
    }

    _mm_empty ();
}

static void
mmx_composite_add_8888_8888 (pixman_implementation_t *imp,
                             pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t    *dst_line, *dst;
    uint32_t    *src_line, *src;
    int dst_stride, src_stride;
    int32_t w;

    CHECKPOINT ();

    PIXMAN_IMAGE_GET_LINE (src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);
    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;
	w = width;

	while (w && (uintptr_t)dst & 7)
	{
	    store (dst, _mm_adds_pu8 (load ((const uint32_t *)src),
	                              load ((const uint32_t *)dst)));
	    dst++;
	    src++;
	    w--;
	}

	while (w >= 2)
	{
	    *(__m64 *)dst = _mm_adds_pu8 (ldq_u ((__m64 *)src), *(__m64*)dst);
	    dst += 2;
	    src += 2;
	    w -= 2;
	}

	if (w)
	{
	    store (dst, _mm_adds_pu8 (load ((const uint32_t *)src),
	                              load ((const uint32_t *)dst)));

	}
    }

    _mm_empty ();
}

static pixman_bool_t
mmx_blt (pixman_implementation_t *imp,
         uint32_t *               src_bits,
         uint32_t *               dst_bits,
         int                      src_stride,
         int                      dst_stride,
         int                      src_bpp,
         int                      dst_bpp,
         int                      src_x,
         int                      src_y,
         int                      dest_x,
         int                      dest_y,
         int                      width,
         int                      height)
{
    uint8_t *   src_bytes;
    uint8_t *   dst_bytes;
    int byte_width;

    if (src_bpp != dst_bpp)
	return FALSE;

    if (src_bpp == 16)
    {
	src_stride = src_stride * (int) sizeof (uint32_t) / 2;
	dst_stride = dst_stride * (int) sizeof (uint32_t) / 2;
	src_bytes = (uint8_t *)(((uint16_t *)src_bits) + src_stride * (src_y) + (src_x));
	dst_bytes = (uint8_t *)(((uint16_t *)dst_bits) + dst_stride * (dest_y) + (dest_x));
	byte_width = 2 * width;
	src_stride *= 2;
	dst_stride *= 2;
    }
    else if (src_bpp == 32)
    {
	src_stride = src_stride * (int) sizeof (uint32_t) / 4;
	dst_stride = dst_stride * (int) sizeof (uint32_t) / 4;
	src_bytes = (uint8_t *)(((uint32_t *)src_bits) + src_stride * (src_y) + (src_x));
	dst_bytes = (uint8_t *)(((uint32_t *)dst_bits) + dst_stride * (dest_y) + (dest_x));
	byte_width = 4 * width;
	src_stride *= 4;
	dst_stride *= 4;
    }
    else
    {
	return FALSE;
    }

    while (height--)
    {
	int w;
	uint8_t *s = src_bytes;
	uint8_t *d = dst_bytes;
	src_bytes += src_stride;
	dst_bytes += dst_stride;
	w = byte_width;

	if (w >= 1 && ((uintptr_t)d & 1))
	{
	    *(uint8_t *)d = *(uint8_t *)s;
	    w -= 1;
	    s += 1;
	    d += 1;
	}

	if (w >= 2 && ((uintptr_t)d & 3))
	{
	    *(uint16_t *)d = *(uint16_t *)s;
	    w -= 2;
	    s += 2;
	    d += 2;
	}

	while (w >= 4 && ((uintptr_t)d & 7))
	{
	    *(uint32_t *)d = ldl_u ((uint32_t *)s);

	    w -= 4;
	    s += 4;
	    d += 4;
	}

	while (w >= 64)
	{
#if (defined (__GNUC__) || (defined(__SUNPRO_C) && (__SUNPRO_C >= 0x590))) && defined USE_X86_MMX
	    __asm__ (
	        "movq	  (%1),	  %%mm0\n"
	        "movq	 8(%1),	  %%mm1\n"
	        "movq	16(%1),	  %%mm2\n"
	        "movq	24(%1),	  %%mm3\n"
	        "movq	32(%1),	  %%mm4\n"
	        "movq	40(%1),	  %%mm5\n"
	        "movq	48(%1),	  %%mm6\n"
	        "movq	56(%1),	  %%mm7\n"

	        "movq	%%mm0,	  (%0)\n"
	        "movq	%%mm1,	 8(%0)\n"
	        "movq	%%mm2,	16(%0)\n"
	        "movq	%%mm3,	24(%0)\n"
	        "movq	%%mm4,	32(%0)\n"
	        "movq	%%mm5,	40(%0)\n"
	        "movq	%%mm6,	48(%0)\n"
	        "movq	%%mm7,	56(%0)\n"
		:
		: "r" (d), "r" (s)
		: "memory",
		  "%mm0", "%mm1", "%mm2", "%mm3",
		  "%mm4", "%mm5", "%mm6", "%mm7");
#else
	    __m64 v0 = ldq_u ((__m64 *)(s + 0));
	    __m64 v1 = ldq_u ((__m64 *)(s + 8));
	    __m64 v2 = ldq_u ((__m64 *)(s + 16));
	    __m64 v3 = ldq_u ((__m64 *)(s + 24));
	    __m64 v4 = ldq_u ((__m64 *)(s + 32));
	    __m64 v5 = ldq_u ((__m64 *)(s + 40));
	    __m64 v6 = ldq_u ((__m64 *)(s + 48));
	    __m64 v7 = ldq_u ((__m64 *)(s + 56));
	    *(__m64 *)(d + 0)  = v0;
	    *(__m64 *)(d + 8)  = v1;
	    *(__m64 *)(d + 16) = v2;
	    *(__m64 *)(d + 24) = v3;
	    *(__m64 *)(d + 32) = v4;
	    *(__m64 *)(d + 40) = v5;
	    *(__m64 *)(d + 48) = v6;
	    *(__m64 *)(d + 56) = v7;
#endif

	    w -= 64;
	    s += 64;
	    d += 64;
	}
	while (w >= 4)
	{
	    *(uint32_t *)d = ldl_u ((uint32_t *)s);

	    w -= 4;
	    s += 4;
	    d += 4;
	}
	if (w >= 2)
	{
	    *(uint16_t *)d = *(uint16_t *)s;
	    w -= 2;
	    s += 2;
	    d += 2;
	}
    }

    _mm_empty ();

    return TRUE;
}

static void
mmx_composite_copy_area (pixman_implementation_t *imp,
                         pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);

    mmx_blt (imp, src_image->bits.bits,
	     dest_image->bits.bits,
	     src_image->bits.rowstride,
	     dest_image->bits.rowstride,
	     PIXMAN_FORMAT_BPP (src_image->bits.format),
	     PIXMAN_FORMAT_BPP (dest_image->bits.format),
	     src_x, src_y, dest_x, dest_y, width, height);
}

static void
mmx_composite_over_x888_8_8888 (pixman_implementation_t *imp,
                                pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t  *src, *src_line;
    uint32_t  *dst, *dst_line;
    uint8_t  *mask, *mask_line;
    int src_stride, mask_stride, dst_stride;
    int32_t w;

    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (mask_image, mask_x, mask_y, uint8_t, mask_stride, mask_line, 1);
    PIXMAN_IMAGE_GET_LINE (src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);

    while (height--)
    {
	src = src_line;
	src_line += src_stride;
	dst = dst_line;
	dst_line += dst_stride;
	mask = mask_line;
	mask_line += mask_stride;

	w = width;

	while (w--)
	{
	    uint64_t m = *mask;

	    if (m)
	    {
		uint32_t ssrc = *src | 0xff000000;
		__m64 s = load8888 (&ssrc);

		if (m == 0xff)
		{
		    store8888 (dst, s);
		}
		else
		{
		    __m64 sa = expand_alpha (s);
		    __m64 vm = expand_alpha_rev (to_m64 (m));
		    __m64 vdest = in_over (s, sa, vm, load8888 (dst));

		    store8888 (dst, vdest);
		}
	    }

	    mask++;
	    dst++;
	    src++;
	}
    }

    _mm_empty ();
}

static void
mmx_composite_over_reverse_n_8888 (pixman_implementation_t *imp,
                                   pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t src;
    uint32_t    *dst_line, *dst;
    int32_t w;
    int dst_stride;
    __m64 vsrc;

    CHECKPOINT ();

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    if (src == 0)
	return;

    PIXMAN_IMAGE_GET_LINE (dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);

    vsrc = load8888 (&src);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	w = width;

	CHECKPOINT ();

	while (w && (uintptr_t)dst & 7)
	{
	    __m64 vdest = load8888 (dst);

	    store8888 (dst, over (vdest, expand_alpha (vdest), vsrc));

	    w--;
	    dst++;
	}

	while (w >= 2)
	{
	    __m64 vdest = *(__m64 *)dst;
	    __m64 dest0 = expand8888 (vdest, 0);
	    __m64 dest1 = expand8888 (vdest, 1);


	    dest0 = over (dest0, expand_alpha (dest0), vsrc);
	    dest1 = over (dest1, expand_alpha (dest1), vsrc);

	    *(__m64 *)dst = pack8888 (dest0, dest1);

	    dst += 2;
	    w -= 2;
	}

	CHECKPOINT ();

	if (w)
	{
	    __m64 vdest = load8888 (dst);

	    store8888 (dst, over (vdest, expand_alpha (vdest), vsrc));
	}
    }

    _mm_empty ();
}

static force_inline void
scaled_nearest_scanline_mmx_8888_8888_OVER (uint32_t*       pd,
                                            const uint32_t* ps,
                                            int32_t         w,
                                            pixman_fixed_t  vx,
                                            pixman_fixed_t  unit_x,
                                            pixman_fixed_t  src_width_fixed,
                                            pixman_bool_t   fully_transparent_src)
{
    if (fully_transparent_src)
	return;

    while (w)
    {
	__m64 d = load (pd);
	__m64 s = load (ps + pixman_fixed_to_int (vx));
	vx += unit_x;
	while (vx >= 0)
	    vx -= src_width_fixed;

	store8888 (pd, core_combine_over_u_pixel_mmx (s, d));
	pd++;

	w--;
    }

    _mm_empty ();
}

FAST_NEAREST_MAINLOOP (mmx_8888_8888_cover_OVER,
		       scaled_nearest_scanline_mmx_8888_8888_OVER,
		       uint32_t, uint32_t, COVER)
FAST_NEAREST_MAINLOOP (mmx_8888_8888_none_OVER,
		       scaled_nearest_scanline_mmx_8888_8888_OVER,
		       uint32_t, uint32_t, NONE)
FAST_NEAREST_MAINLOOP (mmx_8888_8888_pad_OVER,
		       scaled_nearest_scanline_mmx_8888_8888_OVER,
		       uint32_t, uint32_t, PAD)
FAST_NEAREST_MAINLOOP (mmx_8888_8888_normal_OVER,
		       scaled_nearest_scanline_mmx_8888_8888_OVER,
		       uint32_t, uint32_t, NORMAL)

static force_inline void
scaled_nearest_scanline_mmx_8888_n_8888_OVER (const uint32_t * mask,
					      uint32_t *       dst,
					      const uint32_t * src,
					      int32_t          w,
					      pixman_fixed_t   vx,
					      pixman_fixed_t   unit_x,
					      pixman_fixed_t   src_width_fixed,
					      pixman_bool_t    zero_src)
{
    __m64 mm_mask;

    if (zero_src || (*mask >> 24) == 0)
    {
	/* A workaround for https://gcc.gnu.org/PR47759 */
	_mm_empty ();
	return;
    }

    mm_mask = expand_alpha (load8888 (mask));

    while (w)
    {
	uint32_t s = *(src + pixman_fixed_to_int (vx));
	vx += unit_x;
	while (vx >= 0)
	    vx -= src_width_fixed;

	if (s)
	{
	    __m64 ms = load8888 (&s);
	    __m64 alpha = expand_alpha (ms);
	    __m64 dest  = load8888 (dst);

	    store8888 (dst, (in_over (ms, alpha, mm_mask, dest)));
	}

	dst++;
	w--;
    }

    _mm_empty ();
}

FAST_NEAREST_MAINLOOP_COMMON (mmx_8888_n_8888_cover_OVER,
			      scaled_nearest_scanline_mmx_8888_n_8888_OVER,
			      uint32_t, uint32_t, uint32_t, COVER, TRUE, TRUE)
FAST_NEAREST_MAINLOOP_COMMON (mmx_8888_n_8888_pad_OVER,
			      scaled_nearest_scanline_mmx_8888_n_8888_OVER,
			      uint32_t, uint32_t, uint32_t, PAD, TRUE, TRUE)
FAST_NEAREST_MAINLOOP_COMMON (mmx_8888_n_8888_none_OVER,
			      scaled_nearest_scanline_mmx_8888_n_8888_OVER,
			      uint32_t, uint32_t, uint32_t, NONE, TRUE, TRUE)
FAST_NEAREST_MAINLOOP_COMMON (mmx_8888_n_8888_normal_OVER,
			      scaled_nearest_scanline_mmx_8888_n_8888_OVER,
			      uint32_t, uint32_t, uint32_t, NORMAL, TRUE, TRUE)

#define BSHIFT ((1 << BILINEAR_INTERPOLATION_BITS))
#define BMSK (BSHIFT - 1)

#define BILINEAR_DECLARE_VARIABLES						\
    const __m64 mm_wt = _mm_set_pi16 (wt, wt, wt, wt);				\
    const __m64 mm_wb = _mm_set_pi16 (wb, wb, wb, wb);				\
    const __m64 mm_addc7 = _mm_set_pi16 (0, 1, 0, 1);				\
    const __m64 mm_xorc7 = _mm_set_pi16 (0, BMSK, 0, BMSK);			\
    const __m64 mm_ux = _mm_set_pi16 (unit_x, unit_x, unit_x, unit_x);		\
    const __m64 mm_zero = _mm_setzero_si64 ();					\
    __m64 mm_x = _mm_set_pi16 (vx, vx, vx, vx)

#define BILINEAR_INTERPOLATE_ONE_PIXEL(pix)					\
do {										\
    /* fetch 2x2 pixel block into 2 mmx registers */				\
    __m64 t = ldq_u ((__m64 *)&src_top [pixman_fixed_to_int (vx)]);		\
    __m64 b = ldq_u ((__m64 *)&src_bottom [pixman_fixed_to_int (vx)]);		\
    /* vertical interpolation */						\
    __m64 t_hi = _mm_mullo_pi16 (_mm_unpackhi_pi8 (t, mm_zero), mm_wt);		\
    __m64 t_lo = _mm_mullo_pi16 (_mm_unpacklo_pi8 (t, mm_zero), mm_wt);		\
    __m64 b_hi = _mm_mullo_pi16 (_mm_unpackhi_pi8 (b, mm_zero), mm_wb);		\
    __m64 b_lo = _mm_mullo_pi16 (_mm_unpacklo_pi8 (b, mm_zero), mm_wb);		\
    __m64 hi = _mm_add_pi16 (t_hi, b_hi);					\
    __m64 lo = _mm_add_pi16 (t_lo, b_lo);					\
    /* calculate horizontal weights */						\
    __m64 mm_wh = _mm_add_pi16 (mm_addc7, _mm_xor_si64 (mm_xorc7,		\
			  _mm_srli_pi16 (mm_x,					\
					 16 - BILINEAR_INTERPOLATION_BITS)));	\
    /* horizontal interpolation */						\
    __m64 p = _mm_unpacklo_pi16 (lo, hi);					\
    __m64 q = _mm_unpackhi_pi16 (lo, hi);					\
    vx += unit_x;								\
    lo = _mm_madd_pi16 (p, mm_wh);						\
    hi = _mm_madd_pi16 (q, mm_wh);						\
    mm_x = _mm_add_pi16 (mm_x, mm_ux);						\
    /* shift and pack the result */						\
    hi = _mm_srli_pi32 (hi, BILINEAR_INTERPOLATION_BITS * 2);			\
    lo = _mm_srli_pi32 (lo, BILINEAR_INTERPOLATION_BITS * 2);			\
    lo = _mm_packs_pi32 (lo, hi);						\
    lo = _mm_packs_pu16 (lo, lo);						\
    pix = lo;									\
} while (0)

#define BILINEAR_SKIP_ONE_PIXEL()						\
do {										\
    vx += unit_x;								\
    mm_x = _mm_add_pi16 (mm_x, mm_ux);						\
} while(0)

static force_inline void
scaled_bilinear_scanline_mmx_8888_8888_SRC (uint32_t *       dst,
					    const uint32_t * mask,
					    const uint32_t * src_top,
					    const uint32_t * src_bottom,
					    int32_t          w,
					    int              wt,
					    int              wb,
					    pixman_fixed_t   vx,
					    pixman_fixed_t   unit_x,
					    pixman_fixed_t   max_vx,
					    pixman_bool_t    zero_src)
{
    BILINEAR_DECLARE_VARIABLES;
    __m64 pix;

    while (w--)
    {
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix);
	store (dst, pix);
	dst++;
    }

    _mm_empty ();
}

FAST_BILINEAR_MAINLOOP_COMMON (mmx_8888_8888_cover_SRC,
			       scaled_bilinear_scanline_mmx_8888_8888_SRC,
			       uint32_t, uint32_t, uint32_t,
			       COVER, FLAG_NONE)
FAST_BILINEAR_MAINLOOP_COMMON (mmx_8888_8888_pad_SRC,
			       scaled_bilinear_scanline_mmx_8888_8888_SRC,
			       uint32_t, uint32_t, uint32_t,
			       PAD, FLAG_NONE)
FAST_BILINEAR_MAINLOOP_COMMON (mmx_8888_8888_none_SRC,
			       scaled_bilinear_scanline_mmx_8888_8888_SRC,
			       uint32_t, uint32_t, uint32_t,
			       NONE, FLAG_NONE)
FAST_BILINEAR_MAINLOOP_COMMON (mmx_8888_8888_normal_SRC,
			       scaled_bilinear_scanline_mmx_8888_8888_SRC,
			       uint32_t, uint32_t, uint32_t,
			       NORMAL, FLAG_NONE)

static force_inline void
scaled_bilinear_scanline_mmx_8888_8888_OVER (uint32_t *       dst,
					     const uint32_t * mask,
					     const uint32_t * src_top,
					     const uint32_t * src_bottom,
					     int32_t          w,
					     int              wt,
					     int              wb,
					     pixman_fixed_t   vx,
					     pixman_fixed_t   unit_x,
					     pixman_fixed_t   max_vx,
					     pixman_bool_t    zero_src)
{
    BILINEAR_DECLARE_VARIABLES;
    __m64 pix1, pix2;

    while (w)
    {
	BILINEAR_INTERPOLATE_ONE_PIXEL (pix1);

	if (!is_zero (pix1))
	{
	    pix2 = load (dst);
	    store8888 (dst, core_combine_over_u_pixel_mmx (pix1, pix2));
	}

	w--;
	dst++;
    }

    _mm_empty ();
}

FAST_BILINEAR_MAINLOOP_COMMON (mmx_8888_8888_cover_OVER,
			       scaled_bilinear_scanline_mmx_8888_8888_OVER,
			       uint32_t, uint32_t, uint32_t,
			       COVER, FLAG_NONE)
FAST_BILINEAR_MAINLOOP_COMMON (mmx_8888_8888_pad_OVER,
			       scaled_bilinear_scanline_mmx_8888_8888_OVER,
			       uint32_t, uint32_t, uint32_t,
			       PAD, FLAG_NONE)
FAST_BILINEAR_MAINLOOP_COMMON (mmx_8888_8888_none_OVER,
			       scaled_bilinear_scanline_mmx_8888_8888_OVER,
			       uint32_t, uint32_t, uint32_t,
			       NONE, FLAG_NONE)
FAST_BILINEAR_MAINLOOP_COMMON (mmx_8888_8888_normal_OVER,
			       scaled_bilinear_scanline_mmx_8888_8888_OVER,
			       uint32_t, uint32_t, uint32_t,
			       NORMAL, FLAG_NONE)

static force_inline void
scaled_bilinear_scanline_mmx_8888_8_8888_OVER (uint32_t *       dst,
					       const uint8_t  * mask,
					       const uint32_t * src_top,
					       const uint32_t * src_bottom,
					       int32_t          w,
					       int              wt,
					       int              wb,
					       pixman_fixed_t   vx,
					       pixman_fixed_t   unit_x,
					       pixman_fixed_t   max_vx,
					       pixman_bool_t    zero_src)
{
    BILINEAR_DECLARE_VARIABLES;
    __m64 pix1, pix2;
    uint32_t m;

    while (w)
    {
	m = (uint32_t) *mask++;

	if (m)
	{
	    BILINEAR_INTERPOLATE_ONE_PIXEL (pix1);

	    if (m == 0xff && is_opaque (pix1))
	    {
		store (dst, pix1);
	    }
	    else
	    {
		__m64 ms, md, ma, msa;

		pix2 = load (dst);
		ma = expand_alpha_rev (to_m64 (m));
		ms = _mm_unpacklo_pi8 (pix1, _mm_setzero_si64 ());
		md = _mm_unpacklo_pi8 (pix2, _mm_setzero_si64 ());

		msa = expand_alpha (ms);

		store8888 (dst, (in_over (ms, msa, ma, md)));
	    }
	}
	else
	{
	    BILINEAR_SKIP_ONE_PIXEL ();
	}

	w--;
	dst++;
    }

    _mm_empty ();
}

FAST_BILINEAR_MAINLOOP_COMMON (mmx_8888_8_8888_cover_OVER,
			       scaled_bilinear_scanline_mmx_8888_8_8888_OVER,
			       uint32_t, uint8_t, uint32_t,
			       COVER, FLAG_HAVE_NON_SOLID_MASK)
FAST_BILINEAR_MAINLOOP_COMMON (mmx_8888_8_8888_pad_OVER,
			       scaled_bilinear_scanline_mmx_8888_8_8888_OVER,
			       uint32_t, uint8_t, uint32_t,
			       PAD, FLAG_HAVE_NON_SOLID_MASK)
FAST_BILINEAR_MAINLOOP_COMMON (mmx_8888_8_8888_none_OVER,
			       scaled_bilinear_scanline_mmx_8888_8_8888_OVER,
			       uint32_t, uint8_t, uint32_t,
			       NONE, FLAG_HAVE_NON_SOLID_MASK)
FAST_BILINEAR_MAINLOOP_COMMON (mmx_8888_8_8888_normal_OVER,
			       scaled_bilinear_scanline_mmx_8888_8_8888_OVER,
			       uint32_t, uint8_t, uint32_t,
			       NORMAL, FLAG_HAVE_NON_SOLID_MASK)

static uint32_t *
mmx_fetch_x8r8g8b8 (pixman_iter_t *iter, const uint32_t *mask)
{
    int w = iter->width;
    uint32_t *dst = iter->buffer;
    uint32_t *src = (uint32_t *)iter->bits;

    iter->bits += iter->stride;

    while (w && ((uintptr_t)dst) & 7)
    {
	*dst++ = (*src++) | 0xff000000;
	w--;
    }

    while (w >= 8)
    {
	__m64 vsrc1 = ldq_u ((__m64 *)(src + 0));
	__m64 vsrc2 = ldq_u ((__m64 *)(src + 2));
	__m64 vsrc3 = ldq_u ((__m64 *)(src + 4));
	__m64 vsrc4 = ldq_u ((__m64 *)(src + 6));

	*(__m64 *)(dst + 0) = _mm_or_si64 (vsrc1, MC (ff000000));
	*(__m64 *)(dst + 2) = _mm_or_si64 (vsrc2, MC (ff000000));
	*(__m64 *)(dst + 4) = _mm_or_si64 (vsrc3, MC (ff000000));
	*(__m64 *)(dst + 6) = _mm_or_si64 (vsrc4, MC (ff000000));

	dst += 8;
	src += 8;
	w -= 8;
    }

    while (w)
    {
	*dst++ = (*src++) | 0xff000000;
	w--;
    }

    _mm_empty ();
    return iter->buffer;
}

static uint32_t *
mmx_fetch_r5g6b5 (pixman_iter_t *iter, const uint32_t *mask)
{
    int w = iter->width;
    uint32_t *dst = iter->buffer;
    uint16_t *src = (uint16_t *)iter->bits;

    iter->bits += iter->stride;

    while (w && ((uintptr_t)dst) & 0x0f)
    {
	uint16_t s = *src++;

	*dst++ = convert_0565_to_8888 (s);
	w--;
    }

    while (w >= 4)
    {
	__m64 vsrc = ldq_u ((__m64 *)src);
	__m64 mm0, mm1;

	expand_4xpacked565 (vsrc, &mm0, &mm1, 1);

	*(__m64 *)(dst + 0) = mm0;
	*(__m64 *)(dst + 2) = mm1;

	dst += 4;
	src += 4;
	w -= 4;
    }

    while (w)
    {
	uint16_t s = *src++;

	*dst++ = convert_0565_to_8888 (s);
	w--;
    }

    _mm_empty ();
    return iter->buffer;
}

static uint32_t *
mmx_fetch_a8 (pixman_iter_t *iter, const uint32_t *mask)
{
    int w = iter->width;
    uint32_t *dst = iter->buffer;
    uint8_t *src = iter->bits;

    iter->bits += iter->stride;

    while (w && (((uintptr_t)dst) & 15))
    {
        *dst++ = *(src++) << 24;
        w--;
    }

    while (w >= 8)
    {
	__m64 mm0 = ldq_u ((__m64 *)src);

	__m64 mm1 = _mm_unpacklo_pi8  (_mm_setzero_si64(), mm0);
	__m64 mm2 = _mm_unpackhi_pi8  (_mm_setzero_si64(), mm0);
	__m64 mm3 = _mm_unpacklo_pi16 (_mm_setzero_si64(), mm1);
	__m64 mm4 = _mm_unpackhi_pi16 (_mm_setzero_si64(), mm1);
	__m64 mm5 = _mm_unpacklo_pi16 (_mm_setzero_si64(), mm2);
	__m64 mm6 = _mm_unpackhi_pi16 (_mm_setzero_si64(), mm2);

	*(__m64 *)(dst + 0) = mm3;
	*(__m64 *)(dst + 2) = mm4;
	*(__m64 *)(dst + 4) = mm5;
	*(__m64 *)(dst + 6) = mm6;

	dst += 8;
	src += 8;
	w -= 8;
    }

    while (w)
    {
	*dst++ = *(src++) << 24;
	w--;
    }

    _mm_empty ();
    return iter->buffer;
}

#define IMAGE_FLAGS							\
    (FAST_PATH_STANDARD_FLAGS | FAST_PATH_ID_TRANSFORM |		\
     FAST_PATH_BITS_IMAGE | FAST_PATH_SAMPLES_COVER_CLIP_NEAREST)

static const pixman_iter_info_t mmx_iters[] = 
{
    { PIXMAN_x8r8g8b8, IMAGE_FLAGS, ITER_NARROW,
      _pixman_iter_init_bits_stride, mmx_fetch_x8r8g8b8, NULL
    },
    { PIXMAN_r5g6b5, IMAGE_FLAGS, ITER_NARROW,
      _pixman_iter_init_bits_stride, mmx_fetch_r5g6b5, NULL
    },
    { PIXMAN_a8, IMAGE_FLAGS, ITER_NARROW,
      _pixman_iter_init_bits_stride, mmx_fetch_a8, NULL
    },
    { PIXMAN_null },
};

static const pixman_fast_path_t mmx_fast_paths[] =
{
    PIXMAN_STD_FAST_PATH    (OVER, solid,    a8,       r5g6b5,   mmx_composite_over_n_8_0565       ),
    PIXMAN_STD_FAST_PATH    (OVER, solid,    a8,       b5g6r5,   mmx_composite_over_n_8_0565       ),
    PIXMAN_STD_FAST_PATH    (OVER, solid,    a8,       a8r8g8b8, mmx_composite_over_n_8_8888       ),
    PIXMAN_STD_FAST_PATH    (OVER, solid,    a8,       x8r8g8b8, mmx_composite_over_n_8_8888       ),
    PIXMAN_STD_FAST_PATH    (OVER, solid,    a8,       a8b8g8r8, mmx_composite_over_n_8_8888       ),
    PIXMAN_STD_FAST_PATH    (OVER, solid,    a8,       x8b8g8r8, mmx_composite_over_n_8_8888       ),
    PIXMAN_STD_FAST_PATH_CA (OVER, solid,    a8r8g8b8, a8r8g8b8, mmx_composite_over_n_8888_8888_ca ),
    PIXMAN_STD_FAST_PATH_CA (OVER, solid,    a8r8g8b8, x8r8g8b8, mmx_composite_over_n_8888_8888_ca ),
    PIXMAN_STD_FAST_PATH_CA (OVER, solid,    a8r8g8b8, r5g6b5,   mmx_composite_over_n_8888_0565_ca ),
    PIXMAN_STD_FAST_PATH_CA (OVER, solid,    a8b8g8r8, a8b8g8r8, mmx_composite_over_n_8888_8888_ca ),
    PIXMAN_STD_FAST_PATH_CA (OVER, solid,    a8b8g8r8, x8b8g8r8, mmx_composite_over_n_8888_8888_ca ),
    PIXMAN_STD_FAST_PATH_CA (OVER, solid,    a8b8g8r8, b5g6r5,   mmx_composite_over_n_8888_0565_ca ),
    PIXMAN_STD_FAST_PATH    (OVER, pixbuf,   pixbuf,   a8r8g8b8, mmx_composite_over_pixbuf_8888    ),
    PIXMAN_STD_FAST_PATH    (OVER, pixbuf,   pixbuf,   x8r8g8b8, mmx_composite_over_pixbuf_8888    ),
    PIXMAN_STD_FAST_PATH    (OVER, pixbuf,   pixbuf,   r5g6b5,   mmx_composite_over_pixbuf_0565    ),
    PIXMAN_STD_FAST_PATH    (OVER, rpixbuf,  rpixbuf,  a8b8g8r8, mmx_composite_over_pixbuf_8888    ),
    PIXMAN_STD_FAST_PATH    (OVER, rpixbuf,  rpixbuf,  x8b8g8r8, mmx_composite_over_pixbuf_8888    ),
    PIXMAN_STD_FAST_PATH    (OVER, rpixbuf,  rpixbuf,  b5g6r5,   mmx_composite_over_pixbuf_0565    ),
    PIXMAN_STD_FAST_PATH    (OVER, x8r8g8b8, solid,    a8r8g8b8, mmx_composite_over_x888_n_8888    ),
    PIXMAN_STD_FAST_PATH    (OVER, x8r8g8b8, solid,    x8r8g8b8, mmx_composite_over_x888_n_8888    ),
    PIXMAN_STD_FAST_PATH    (OVER, x8b8g8r8, solid,    a8b8g8r8, mmx_composite_over_x888_n_8888    ),
    PIXMAN_STD_FAST_PATH    (OVER, x8b8g8r8, solid,    x8b8g8r8, mmx_composite_over_x888_n_8888    ),
    PIXMAN_STD_FAST_PATH    (OVER, a8r8g8b8, solid,    a8r8g8b8, mmx_composite_over_8888_n_8888    ),
    PIXMAN_STD_FAST_PATH    (OVER, a8r8g8b8, solid,    x8r8g8b8, mmx_composite_over_8888_n_8888    ),
    PIXMAN_STD_FAST_PATH    (OVER, a8b8g8r8, solid,    a8b8g8r8, mmx_composite_over_8888_n_8888    ),
    PIXMAN_STD_FAST_PATH    (OVER, a8b8g8r8, solid,    x8b8g8r8, mmx_composite_over_8888_n_8888    ),
    PIXMAN_STD_FAST_PATH    (OVER, x8r8g8b8, a8,       x8r8g8b8, mmx_composite_over_x888_8_8888    ),
    PIXMAN_STD_FAST_PATH    (OVER, x8r8g8b8, a8,       a8r8g8b8, mmx_composite_over_x888_8_8888    ),
    PIXMAN_STD_FAST_PATH    (OVER, x8b8g8r8, a8,       x8b8g8r8, mmx_composite_over_x888_8_8888    ),
    PIXMAN_STD_FAST_PATH    (OVER, x8b8g8r8, a8,       a8b8g8r8, mmx_composite_over_x888_8_8888    ),
    PIXMAN_STD_FAST_PATH    (OVER, solid,    null,     a8r8g8b8, mmx_composite_over_n_8888         ),
    PIXMAN_STD_FAST_PATH    (OVER, solid,    null,     x8r8g8b8, mmx_composite_over_n_8888         ),
    PIXMAN_STD_FAST_PATH    (OVER, solid,    null,     r5g6b5,   mmx_composite_over_n_0565         ),
    PIXMAN_STD_FAST_PATH    (OVER, solid,    null,     b5g6r5,   mmx_composite_over_n_0565         ),
    PIXMAN_STD_FAST_PATH    (OVER, x8r8g8b8, null,     x8r8g8b8, mmx_composite_copy_area           ),
    PIXMAN_STD_FAST_PATH    (OVER, x8b8g8r8, null,     x8b8g8r8, mmx_composite_copy_area           ),

    PIXMAN_STD_FAST_PATH    (OVER, a8r8g8b8, null,     a8r8g8b8, mmx_composite_over_8888_8888      ),
    PIXMAN_STD_FAST_PATH    (OVER, a8r8g8b8, null,     x8r8g8b8, mmx_composite_over_8888_8888      ),
    PIXMAN_STD_FAST_PATH    (OVER, a8r8g8b8, null,     r5g6b5,   mmx_composite_over_8888_0565      ),
    PIXMAN_STD_FAST_PATH    (OVER, a8b8g8r8, null,     a8b8g8r8, mmx_composite_over_8888_8888      ),
    PIXMAN_STD_FAST_PATH    (OVER, a8b8g8r8, null,     x8b8g8r8, mmx_composite_over_8888_8888      ),
    PIXMAN_STD_FAST_PATH    (OVER, a8b8g8r8, null,     b5g6r5,   mmx_composite_over_8888_0565      ),

    PIXMAN_STD_FAST_PATH    (OVER_REVERSE, solid, null, a8r8g8b8, mmx_composite_over_reverse_n_8888),
    PIXMAN_STD_FAST_PATH    (OVER_REVERSE, solid, null, a8b8g8r8, mmx_composite_over_reverse_n_8888),

    PIXMAN_STD_FAST_PATH    (ADD,  r5g6b5,   null,     r5g6b5,   mmx_composite_add_0565_0565       ),
    PIXMAN_STD_FAST_PATH    (ADD,  b5g6r5,   null,     b5g6r5,   mmx_composite_add_0565_0565       ),
    PIXMAN_STD_FAST_PATH    (ADD,  a8r8g8b8, null,     a8r8g8b8, mmx_composite_add_8888_8888       ),
    PIXMAN_STD_FAST_PATH    (ADD,  a8b8g8r8, null,     a8b8g8r8, mmx_composite_add_8888_8888       ),
    PIXMAN_STD_FAST_PATH    (ADD,  a8,       null,     a8,       mmx_composite_add_8_8		   ),
    PIXMAN_STD_FAST_PATH    (ADD,  solid,    a8,       a8,       mmx_composite_add_n_8_8           ),

    PIXMAN_STD_FAST_PATH    (SRC,  a8r8g8b8, null,     r5g6b5,   mmx_composite_src_x888_0565       ),
    PIXMAN_STD_FAST_PATH    (SRC,  a8b8g8r8, null,     b5g6r5,   mmx_composite_src_x888_0565       ),
    PIXMAN_STD_FAST_PATH    (SRC,  x8r8g8b8, null,     r5g6b5,   mmx_composite_src_x888_0565       ),
    PIXMAN_STD_FAST_PATH    (SRC,  x8b8g8r8, null,     b5g6r5,   mmx_composite_src_x888_0565       ),
    PIXMAN_STD_FAST_PATH    (SRC,  solid,    a8,       a8r8g8b8, mmx_composite_src_n_8_8888        ),
    PIXMAN_STD_FAST_PATH    (SRC,  solid,    a8,       x8r8g8b8, mmx_composite_src_n_8_8888        ),
    PIXMAN_STD_FAST_PATH    (SRC,  solid,    a8,       a8b8g8r8, mmx_composite_src_n_8_8888        ),
    PIXMAN_STD_FAST_PATH    (SRC,  solid,    a8,       x8b8g8r8, mmx_composite_src_n_8_8888        ),
    PIXMAN_STD_FAST_PATH    (SRC,  a8r8g8b8, null,     a8r8g8b8, mmx_composite_copy_area           ),
    PIXMAN_STD_FAST_PATH    (SRC,  a8b8g8r8, null,     a8b8g8r8, mmx_composite_copy_area           ),
    PIXMAN_STD_FAST_PATH    (SRC,  a8r8g8b8, null,     x8r8g8b8, mmx_composite_copy_area           ),
    PIXMAN_STD_FAST_PATH    (SRC,  a8b8g8r8, null,     x8b8g8r8, mmx_composite_copy_area           ),
    PIXMAN_STD_FAST_PATH    (SRC,  x8r8g8b8, null,     x8r8g8b8, mmx_composite_copy_area           ),
    PIXMAN_STD_FAST_PATH    (SRC,  x8b8g8r8, null,     x8b8g8r8, mmx_composite_copy_area           ),
    PIXMAN_STD_FAST_PATH    (SRC,  r5g6b5,   null,     r5g6b5,   mmx_composite_copy_area           ),
    PIXMAN_STD_FAST_PATH    (SRC,  b5g6r5,   null,     b5g6r5,   mmx_composite_copy_area           ),

    PIXMAN_STD_FAST_PATH    (IN,   a8,       null,     a8,       mmx_composite_in_8_8              ),
    PIXMAN_STD_FAST_PATH    (IN,   solid,    a8,       a8,       mmx_composite_in_n_8_8            ),

    SIMPLE_NEAREST_FAST_PATH (OVER,   a8r8g8b8, x8r8g8b8, mmx_8888_8888                            ),
    SIMPLE_NEAREST_FAST_PATH (OVER,   a8b8g8r8, x8b8g8r8, mmx_8888_8888                            ),
    SIMPLE_NEAREST_FAST_PATH (OVER,   a8r8g8b8, a8r8g8b8, mmx_8888_8888                            ),
    SIMPLE_NEAREST_FAST_PATH (OVER,   a8b8g8r8, a8b8g8r8, mmx_8888_8888                            ),

    SIMPLE_NEAREST_SOLID_MASK_FAST_PATH (OVER, a8r8g8b8, a8r8g8b8, mmx_8888_n_8888                 ),
    SIMPLE_NEAREST_SOLID_MASK_FAST_PATH (OVER, a8b8g8r8, a8b8g8r8, mmx_8888_n_8888                 ),
    SIMPLE_NEAREST_SOLID_MASK_FAST_PATH (OVER, a8r8g8b8, x8r8g8b8, mmx_8888_n_8888                 ),
    SIMPLE_NEAREST_SOLID_MASK_FAST_PATH (OVER, a8b8g8r8, x8b8g8r8, mmx_8888_n_8888                 ),

    SIMPLE_BILINEAR_FAST_PATH (SRC, a8r8g8b8,          a8r8g8b8, mmx_8888_8888                     ),
    SIMPLE_BILINEAR_FAST_PATH (SRC, a8r8g8b8,          x8r8g8b8, mmx_8888_8888                     ),
    SIMPLE_BILINEAR_FAST_PATH (SRC, x8r8g8b8,          x8r8g8b8, mmx_8888_8888                     ),
    SIMPLE_BILINEAR_FAST_PATH (SRC, a8b8g8r8,          a8b8g8r8, mmx_8888_8888                     ),
    SIMPLE_BILINEAR_FAST_PATH (SRC, a8b8g8r8,          x8b8g8r8, mmx_8888_8888                     ),
    SIMPLE_BILINEAR_FAST_PATH (SRC, x8b8g8r8,          x8b8g8r8, mmx_8888_8888                     ),

    SIMPLE_BILINEAR_FAST_PATH (OVER, a8r8g8b8,         x8r8g8b8, mmx_8888_8888                     ),
    SIMPLE_BILINEAR_FAST_PATH (OVER, a8b8g8r8,         x8b8g8r8, mmx_8888_8888                     ),
    SIMPLE_BILINEAR_FAST_PATH (OVER, a8r8g8b8,         a8r8g8b8, mmx_8888_8888                     ),
    SIMPLE_BILINEAR_FAST_PATH (OVER, a8b8g8r8,         a8b8g8r8, mmx_8888_8888                     ),

    SIMPLE_BILINEAR_A8_MASK_FAST_PATH (OVER, a8r8g8b8, x8r8g8b8, mmx_8888_8_8888                   ),
    SIMPLE_BILINEAR_A8_MASK_FAST_PATH (OVER, a8b8g8r8, x8b8g8r8, mmx_8888_8_8888                   ),
    SIMPLE_BILINEAR_A8_MASK_FAST_PATH (OVER, a8r8g8b8, a8r8g8b8, mmx_8888_8_8888                   ),
    SIMPLE_BILINEAR_A8_MASK_FAST_PATH (OVER, a8b8g8r8, a8b8g8r8, mmx_8888_8_8888                   ),

    { PIXMAN_OP_NONE },
};

pixman_implementation_t *
_pixman_implementation_create_mmx (pixman_implementation_t *fallback)
{
    pixman_implementation_t *imp = _pixman_implementation_create (fallback, mmx_fast_paths);

    imp->combine_32[PIXMAN_OP_OVER] = mmx_combine_over_u;
    imp->combine_32[PIXMAN_OP_OVER_REVERSE] = mmx_combine_over_reverse_u;
    imp->combine_32[PIXMAN_OP_IN] = mmx_combine_in_u;
    imp->combine_32[PIXMAN_OP_IN_REVERSE] = mmx_combine_in_reverse_u;
    imp->combine_32[PIXMAN_OP_OUT] = mmx_combine_out_u;
    imp->combine_32[PIXMAN_OP_OUT_REVERSE] = mmx_combine_out_reverse_u;
    imp->combine_32[PIXMAN_OP_ATOP] = mmx_combine_atop_u;
    imp->combine_32[PIXMAN_OP_ATOP_REVERSE] = mmx_combine_atop_reverse_u;
    imp->combine_32[PIXMAN_OP_XOR] = mmx_combine_xor_u;
    imp->combine_32[PIXMAN_OP_ADD] = mmx_combine_add_u;
    imp->combine_32[PIXMAN_OP_SATURATE] = mmx_combine_saturate_u;

    imp->combine_32_ca[PIXMAN_OP_SRC] = mmx_combine_src_ca;
    imp->combine_32_ca[PIXMAN_OP_OVER] = mmx_combine_over_ca;
    imp->combine_32_ca[PIXMAN_OP_OVER_REVERSE] = mmx_combine_over_reverse_ca;
    imp->combine_32_ca[PIXMAN_OP_IN] = mmx_combine_in_ca;
    imp->combine_32_ca[PIXMAN_OP_IN_REVERSE] = mmx_combine_in_reverse_ca;
    imp->combine_32_ca[PIXMAN_OP_OUT] = mmx_combine_out_ca;
    imp->combine_32_ca[PIXMAN_OP_OUT_REVERSE] = mmx_combine_out_reverse_ca;
    imp->combine_32_ca[PIXMAN_OP_ATOP] = mmx_combine_atop_ca;
    imp->combine_32_ca[PIXMAN_OP_ATOP_REVERSE] = mmx_combine_atop_reverse_ca;
    imp->combine_32_ca[PIXMAN_OP_XOR] = mmx_combine_xor_ca;
    imp->combine_32_ca[PIXMAN_OP_ADD] = mmx_combine_add_ca;

    imp->blt = mmx_blt;
    imp->fill = mmx_fill;

    imp->iter_info = mmx_iters;

    return imp;
}

#endif /* USE_X86_MMX || USE_ARM_IWMMXT || USE_LOONGSON_MMI */
@


1.14
log
@Update to pixman 0.32.4. Tested by naddy@@ and ajacoutot@@
@
text
@d92 1
a92 15
#  ifdef __OPTIMIZE__
extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))
_mm_shuffle_pi16 (__m64 __A, int8_t const __N)
{
    __m64 ret;

    asm ("pshufw %2, %1, %0\n\t"
	: "=y" (ret)
	: "y" (__A), "K" (__N)
    );

    return ret;
}
#  else
#   define _mm_shuffle_pi16(A, N)					\
a102 1
#  endif
d3543 99
d4081 10
@


1.13
log
@Update to pixman 0.30.0. Tested by several people during t2k13. Thanks.
@
text
@d304 23
a3563 1
    const __m64 mm_BSHIFT = _mm_set_pi16 (BSHIFT, BSHIFT, BSHIFT, BSHIFT);	\
d3582 2
a3583 5
    vx += unit_x;								\
    if (BILINEAR_INTERPOLATION_BITS < 8)					\
    {										\
	/* calculate horizontal weights */					\
	__m64 mm_wh = _mm_add_pi16 (mm_addc7, _mm_xor_si64 (mm_xorc7,		\
d3586 6
a3591 23
	/* horizontal interpolation */						\
	__m64 p = _mm_unpacklo_pi16 (lo, hi);					\
	__m64 q = _mm_unpackhi_pi16 (lo, hi);					\
	lo = _mm_madd_pi16 (p, mm_wh);						\
	hi = _mm_madd_pi16 (q, mm_wh);						\
    }										\
    else									\
    {										\
	/* calculate horizontal weights */					\
	__m64 mm_wh_lo = _mm_sub_pi16 (mm_BSHIFT, _mm_srli_pi16 (mm_x,		\
					16 - BILINEAR_INTERPOLATION_BITS));	\
	__m64 mm_wh_hi = _mm_srli_pi16 (mm_x,					\
					16 - BILINEAR_INTERPOLATION_BITS);	\
	/* horizontal interpolation */						\
	__m64 mm_lo_lo = _mm_mullo_pi16 (lo, mm_wh_lo);				\
	__m64 mm_lo_hi = _mm_mullo_pi16 (hi, mm_wh_hi);				\
	__m64 mm_hi_lo = _mm_mulhi_pu16 (lo, mm_wh_lo);				\
	__m64 mm_hi_hi = _mm_mulhi_pu16 (hi, mm_wh_hi);				\
	lo = _mm_add_pi32 (_mm_unpacklo_pi16 (mm_lo_lo, mm_hi_lo),		\
			   _mm_unpacklo_pi16 (mm_lo_hi, mm_hi_hi));		\
	hi = _mm_add_pi32 (_mm_unpackhi_pi16 (mm_lo_lo, mm_hi_lo),		\
			   _mm_unpackhi_pi16 (mm_lo_hi, mm_hi_hi));		\
    }										\
d3904 1
a3904 20
typedef struct
{
    pixman_format_code_t	format;
    pixman_iter_get_scanline_t	get_scanline;
} fetcher_info_t;

static const fetcher_info_t fetchers[] =
{
    { PIXMAN_x8r8g8b8,		mmx_fetch_x8r8g8b8 },
    { PIXMAN_r5g6b5,		mmx_fetch_r5g6b5 },
    { PIXMAN_a8,		mmx_fetch_a8 },
    { PIXMAN_null }
};

static pixman_bool_t
mmx_src_iter_init (pixman_implementation_t *imp, pixman_iter_t *iter)
{
    pixman_image_t *image = iter->image;

#define FLAGS								\
d3908 13
a3920 23
    if ((iter->iter_flags & ITER_NARROW)			&&
	(iter->image_flags & FLAGS) == FLAGS)
    {
	const fetcher_info_t *f;

	for (f = &fetchers[0]; f->format != PIXMAN_null; f++)
	{
	    if (image->common.extended_format_code == f->format)
	    {
		uint8_t *b = (uint8_t *)image->bits.bits;
		int s = image->bits.rowstride * 4;

		iter->bits = b + s * iter->y + iter->x * PIXMAN_FORMAT_BPP (f->format) / 8;
		iter->stride = s;

		iter->get_scanline = f->get_scanline;
		return TRUE;
	    }
	}
    }

    return FALSE;
}
d4050 1
a4050 1
    imp->src_iter_init = mmx_src_iter_init;
@


1.12
log
@Update to pixman 0.28.0. Tested by ajacoutot@@, mpi@@ and naddy@@ in a full
ports build. Tweaks from mpi@@ for macppc.
@
text
@a46 2
#define no_vERBOSE

d63 1
a63 1
# if (defined(__SUNPRO_C) || defined(_MSC_VER))
d1403 1
a1403 1
	while (w && (unsigned long)dst & 7)
d1469 1
a1469 1
	while (w && (unsigned long)dst & 7)
d1547 1
a1547 1
	while (twidth && (unsigned long)q & 7)
d1638 1
a1638 1
	while (w && (unsigned long)dst & 7)
d1708 1
a1708 1
	while (w && (unsigned long)dst & 7)
d1882 1
a1882 1
	while (w && (unsigned long)dst & 7)
d1985 1
a1985 1
	while (w && (unsigned long)dst & 7)
d2065 1
a2065 1
          uint32_t		   xor)
d2085 1
a2085 1
        xor = (xor & 0xff) * 0x01010101;
d2093 1
a2093 1
        xor = (xor & 0xffff) * 0x00010001;
d2103 1
a2103 1
    fill = ((uint64_t)xor << 32) | xor;
d2128 1
a2128 1
	if (w >= 1 && ((unsigned long)d & 1))
d2130 1
a2130 1
	    *(uint8_t *)d = (xor & 0xff);
d2135 1
a2135 1
	if (w >= 2 && ((unsigned long)d & 3))
d2137 1
a2137 1
	    *(uint16_t *)d = xor;
d2142 1
a2142 1
	while (w >= 4 && ((unsigned long)d & 7))
d2144 1
a2144 1
	    *(uint32_t *)d = xor;
d2183 1
a2183 1
	    *(uint32_t *)d = xor;
d2190 1
a2190 1
	    *(uint16_t *)d = xor;
d2196 1
a2196 1
	    *(uint8_t *)d = (xor & 0xff);
d2228 1
a2228 1
	while (w && (unsigned long)dst & 7)
d2231 1
a2231 1
	    *dst = CONVERT_8888_TO_0565 (s);
d2254 1
a2254 1
	    *dst = CONVERT_8888_TO_0565 (s);
d2306 1
a2306 1
	while (w && (unsigned long)dst & 7)
d2420 1
a2420 1
	while (w && (unsigned long)dst & 7)
d2537 1
a2537 1
	while (w && (unsigned long)dst & 7)
d2652 1
a2652 1
	while (w && (unsigned long)dst & 7)
d2740 1
a2740 1
	while (twidth && ((unsigned long)q & 7))
d2841 1
a2841 1
	while (w && (unsigned long)dst & 7)
d2912 1
a2912 1
	while (w && (unsigned long)dst & 3)
d2991 1
a2991 1
	while (w && (unsigned long)dst & 3)
d3068 1
a3068 1
	while (w && (unsigned long)dst & 7)
d3131 1
a3131 1
	while (w && (unsigned long)dst & 7)
d3137 1
a3137 1
		s = CONVERT_0565_TO_8888 (s);
d3140 1
a3140 1
		    d = CONVERT_0565_TO_8888 (d);
d3143 1
a3143 1
		*dst = CONVERT_8888_TO_0565 (s);
d3175 1
a3175 1
		s = CONVERT_0565_TO_8888 (s);
d3178 1
a3178 1
		    d = CONVERT_0565_TO_8888 (d);
d3181 1
a3181 1
		*dst = CONVERT_8888_TO_0565 (s);
d3213 1
a3213 1
	while (w && (unsigned long)dst & 7)
d3297 1
a3297 1
	if (w >= 1 && ((unsigned long)d & 1))
d3305 1
a3305 1
	if (w >= 2 && ((unsigned long)d & 3))
d3313 1
a3313 1
	while (w >= 4 && ((unsigned long)d & 7))
d3496 1
a3496 1
	while (w && (unsigned long)dst & 7)
d3779 1
a3779 1
    while (w && ((unsigned long)dst) & 7)
d3821 1
a3821 1
    while (w && ((unsigned long)dst) & 0x0f)
d3825 1
a3825 1
	*dst++ = CONVERT_0565_TO_8888 (s);
d3848 1
a3848 1
	*dst++ = CONVERT_0565_TO_8888 (s);
d3865 1
a3865 1
    while (w && (((unsigned long)dst) & 15))
@


1.11
log
@Update to pixman 0.26.2. tested at least by ajacoutot@@, mpi@@, shadchin@@.
@
text
@d45 1
d55 1
a55 1
#ifdef USE_ARM_IWMMXT
d108 11
a118 2
#   define _mm_shuffle_pi16(A, N) \
    ((__m64) __builtin_ia32_pshufw ((__v4hi)(__m64)(A), (int)(N)))
d198 3
d232 3
d537 30
d582 11
d707 18
d1486 1
d1488 6
a1493 4
	    __m64 v0 = over (vsrc, vsrca, expand565 (vdest, 0));
	    __m64 v1 = over (vsrc, vsrca, expand565 (vdest, 1));
	    __m64 v2 = over (vsrc, vsrca, expand565 (vdest, 2));
	    __m64 v3 = over (vsrc, vsrca, expand565 (vdest, 3));
d1630 1
a1630 3
    mask &= 0xff000000;
    mask = mask | mask >> 8 | mask >> 16 | mask >> 24;
    vmask = load8888 (&mask);
d1699 1
a1699 3
    mask &= 0xff000000;
    mask = mask | mask >> 8 | mask >> 16 | mask >> 24;
    vmask = load8888 (&mask);
d1905 4
d1910 9
a1918 9
	    __m64 vsrc0 = load8888 ((src + 0));
	    __m64 vsrc1 = load8888 ((src + 1));
	    __m64 vsrc2 = load8888 ((src + 2));
	    __m64 vsrc3 = load8888 ((src + 3));

	    __m64 v0 = over (vsrc0, expand_alpha (vsrc0), expand565 (vdest, 0));
	    __m64 v1 = over (vsrc1, expand_alpha (vsrc1), expand565 (vdest, 1));
	    __m64 v2 = over (vsrc2, expand_alpha (vsrc2), expand565 (vdest, 2));
	    __m64 v3 = over (vsrc3, expand_alpha (vsrc3), expand565 (vdest, 3));
d2058 10
a2067 9
pixman_bool_t
pixman_fill_mmx (uint32_t *bits,
                 int       stride,
                 int       bpp,
                 int       x,
                 int       y,
                 int       width,
                 int       height,
                 uint32_t xor)
d2285 3
a2287 3
	pixman_fill_mmx (dest_image->bits.bits, dest_image->bits.rowstride,
			 PIXMAN_FORMAT_BPP (dest_image->bits.format),
	                 dest_x, dest_y, width, height, 0);
d2459 13
d2473 2
a2474 12
		__m64 vm0 = to_m64 (m0);
		__m64 v0 = in_over (vsrc, vsrca, expand_alpha_rev (vm0),
					   expand565 (vdest, 0));
		__m64 vm1 = to_m64 (m1);
		__m64 v1 = in_over (vsrc, vsrca, expand_alpha_rev (vm1),
					   expand565 (vdest, 1));
		__m64 vm2 = to_m64 (m2);
		__m64 v2 = in_over (vsrc, vsrca, expand_alpha_rev (vm2),
					   expand565 (vdest, 2));
		__m64 vm3 = to_m64 (m3);
		__m64 v3 = in_over (vsrc, vsrca, expand_alpha_rev (vm3),
					   expand565 (vdest, 3));
d2583 1
d2585 11
a2595 4
		__m64 v0 = over_rev_non_pre (load8888 (&s0), expand565 (vdest, 0));
		__m64 v1 = over_rev_non_pre (load8888 (&s1), expand565 (vdest, 1));
		__m64 v2 = over_rev_non_pre (load8888 (&s2), expand565 (vdest, 2));
		__m64 v3 = over_rev_non_pre (load8888 (&s3), expand565 (vdest, 3));
d2771 1
d2773 6
a2778 4
		__m64 v0 = in_over (vsrc, vsrca, load8888 (&m0), expand565 (vdest, 0));
		__m64 v1 = in_over (vsrc, vsrca, load8888 (&m1), expand565 (vdest, 1));
		__m64 v2 = in_over (vsrc, vsrca, load8888 (&m2), expand565 (vdest, 2));
		__m64 v3 = in_over (vsrc, vsrca, load8888 (&m3), expand565 (vdest, 3));
d3109 84
d3244 13
a3256 12
pixman_blt_mmx (uint32_t *src_bits,
                uint32_t *dst_bits,
                int       src_stride,
                int       dst_stride,
                int       src_bpp,
                int       dst_bpp,
                int       src_x,
                int       src_y,
                int       dest_x,
                int       dest_y,
                int       width,
                int       height)
d3401 7
a3407 7
    pixman_blt_mmx (src_image->bits.bits,
                    dest_image->bits.bits,
                    src_image->bits.rowstride,
                    dest_image->bits.rowstride,
                    PIXMAN_FORMAT_BPP (src_image->bits.format),
                    PIXMAN_FORMAT_BPP (dest_image->bits.format),
                    src_x, src_y, dest_x, dest_y, width, height);
d3468 304
d3834 1
d3836 1
a3836 4
	__m64 mm0 = expand565 (vsrc, 0);
	__m64 mm1 = expand565 (vsrc, 1);
	__m64 mm2 = expand565 (vsrc, 2);
	__m64 mm3 = expand565 (vsrc, 3);
d3838 2
a3839 2
	*(__m64 *)(dst + 0) = _mm_or_si64 (pack8888 (mm0, mm1), MC (ff000000));
	*(__m64 *)(dst + 2) = _mm_or_si64 (pack8888 (mm2, mm3), MC (ff000000));
d3918 1
a3918 1
static void
a3921 4
    int x = iter->x;
    int y = iter->y;
    int width = iter->width;
    int height = iter->height;
d3924 2
a3925 1
    (FAST_PATH_STANDARD_FLAGS | FAST_PATH_ID_TRANSFORM | FAST_PATH_BITS_IMAGE)
d3927 2
a3928 5
    if ((iter->flags & ITER_NARROW)				&&
	(image->common.flags & FLAGS) == FLAGS			&&
	x >= 0 && y >= 0					&&
	x + width <= image->bits.width				&&
	y + height <= image->bits.height)
d3939 1
a3939 1
		iter->bits = b + s * iter->y + x * PIXMAN_FORMAT_BPP (f->format) / 8;
d3943 1
a3943 1
		return;
d3948 1
a3948 1
    imp->delegate->src_iter_init (imp->delegate, iter);
d3997 5
d4027 17
a4045 49

static pixman_bool_t
mmx_blt (pixman_implementation_t *imp,
         uint32_t *               src_bits,
         uint32_t *               dst_bits,
         int                      src_stride,
         int                      dst_stride,
         int                      src_bpp,
         int                      dst_bpp,
         int                      src_x,
         int                      src_y,
         int                      dest_x,
         int                      dest_y,
         int                      width,
         int                      height)
{
    if (!pixman_blt_mmx (
            src_bits, dst_bits, src_stride, dst_stride, src_bpp, dst_bpp,
            src_x, src_y, dest_x, dest_y, width, height))

    {
	return _pixman_implementation_blt (
	    imp->delegate,
	    src_bits, dst_bits, src_stride, dst_stride, src_bpp, dst_bpp,
	    src_x, src_y, dest_x, dest_y, width, height);
    }

    return TRUE;
}

static pixman_bool_t
mmx_fill (pixman_implementation_t *imp,
          uint32_t *               bits,
          int                      stride,
          int                      bpp,
          int                      x,
          int                      y,
          int                      width,
          int                      height,
          uint32_t xor)
{
    if (!pixman_fill_mmx (bits, stride, bpp, x, y, width, height, xor))
    {
	return _pixman_implementation_fill (
	    imp->delegate, bits, stride, bpp, x, y, width, height, xor);
    }

    return TRUE;
}
@


1.10
log
@Update to pixman 0.22.4. Tested by shadchin@@, krw@@.
@
text
@d36 1
a36 1
#if defined USE_X86_MMX || defined USE_ARM_IWMMXT
d38 3
d42 1
d63 55
d145 1
d151 2
d170 1
a170 1
#if defined(USE_M64_CASTS) || defined(USE_CVT_INTRINSICS)
d182 1
d186 3
d193 1
d195 2
a196 3
    mmxdatafield mmx_ffff0000ffff0000;
    mmxdatafield mmx_0000ffff00000000;
    mmxdatafield mmx_000000000000ffff;
d213 1
d217 3
d224 1
d226 2
a227 3
    MMXDATA_INIT (.mmx_ffff0000ffff0000,         0xffff0000ffff0000),
    MMXDATA_INIT (.mmx_0000ffff00000000,         0x0000ffff00000000),
    MMXDATA_INIT (.mmx_000000000000ffff,         0x000000000000ffff),
d234 2
d250 2
d265 2
d297 1
a297 2
    res = _mm_adds_pu16 (res, _mm_srli_pi16 (res, 8));
    res = _mm_srli_pi16 (res, 8);
d311 1
a311 9
    __m64 t1, t2;

    t1 = shift (pixel, -48);
    t2 = shift (t1, 16);
    t1 = _mm_or_si64 (t1, t2);
    t2 = shift (t1, 32);
    t1 = _mm_or_si64 (t1, t2);

    return t1;
d317 1
a317 12
    __m64 t1, t2;

    /* move alpha to low 16 bits and zero the rest */
    t1 = shift (pixel,  48);
    t1 = shift (t1, -48);

    t2 = shift (t1, 16);
    t1 = _mm_or_si64 (t1, t2);
    t2 = shift (t1, 32);
    t1 = _mm_or_si64 (t1, t2);

    return t1;
d323 1
a323 15
    __m64 x, y, z;

    x = y = z = pixel;

    x = _mm_and_si64 (x, MC (ffff0000ffff0000));
    y = _mm_and_si64 (y, MC (000000000000ffff));
    z = _mm_and_si64 (z, MC (0000ffff00000000));

    y = shift (y, 32);
    z = shift (z, -32);

    x = _mm_or_si64 (x, y);
    x = _mm_or_si64 (x, z);

    return x;
a348 8
static force_inline __m64
in_over_full_src_alpha (__m64 src, __m64 mask, __m64 dest)
{
    src = _mm_or_si64 (src, MC (full_alpha));

    return over (in (src, mask), mask, dest);
}

d365 1
a365 1
static __inline__ __m64 ldq_u(uint64_t *p)
d378 1
a378 1
    struct __una_u64 { uint64_t x __attribute__((packed)); };
d384 1
a384 1
static __inline__ uint32_t ldl_u(uint32_t *p)
d397 16
a412 1
load8888 (uint32_t v)
d414 12
a425 1
    return _mm_unpacklo_pi8 (_mm_cvtsi32_si64 (v), _mm_setzero_si64 ());
d434 2
a435 2
static force_inline uint32_t
store8888 (__m64 v)
d437 44
a480 1
    return _mm_cvtsi64_si32 (pack8888 (v, _mm_setzero_si64 ()));
d504 3
d508 1
d547 9
d573 26
d604 6
d620 5
d627 1
a627 1
      y = pix_multiply (y, a),	 \
d634 1
a634 1
static force_inline uint32_t
d637 1
a637 1
    uint32_t ssrc = *src;
d641 1
a641 2
	__m64 m = load8888 (*mask);
	__m64 s = load8888 (ssrc);
d644 1
a644 3
	s = pix_multiply (s, m);

	ssrc = store8888 (s);
d647 1
a647 1
    return ssrc;
d662 1
a662 2
	uint32_t ssrc = combine (src, mask);
	uint32_t a = ssrc >> 24;
d664 1
a664 1
	if (a == 0xff)
d666 1
a666 1
	    *dest = ssrc;
d668 1
a668 1
	else if (ssrc)
d670 2
a671 4
	    __m64 s, sa;
	    s = load8888 (ssrc);
	    sa = expand_alpha (s);
	    *dest = store8888 (over (s, sa, load8888 (*dest)));
d695 1
a695 1
	uint32_t s = combine (src, mask);
d697 1
a697 1
	d = load8888 (*dest);
d699 1
a699 1
	*dest = store8888 (over (d, da, load8888 (s)));
d721 2
a722 1
	__m64 x, a;
d724 1
a724 2
	x = load8888 (combine (src, mask));
	a = load8888 (*dest);
d728 1
a728 1
	*dest = store8888 (x);
d750 2
a751 1
	__m64 x, a;
d753 1
a753 2
	x = load8888 (*dest);
	a = load8888 (combine (src, mask));
d756 1
a756 1
	*dest = store8888 (x);
d778 2
a779 1
	__m64 x, a;
d781 1
a781 2
	x = load8888 (combine (src, mask));
	a = load8888 (*dest);
d785 1
a785 1
	*dest = store8888 (x);
d807 2
a808 1
	__m64 x, a;
d810 1
a810 2
	x = load8888 (*dest);
	a = load8888 (combine (src, mask));
d815 1
a815 1
	*dest = store8888 (x);
d837 2
a838 1
	__m64 s, da, d, sia;
d840 1
a840 2
	s = load8888 (combine (src, mask));
	d = load8888 (*dest);
d845 1
a845 1
	*dest = store8888 (s);
d869 2
a870 1
	__m64 s, dia, d, sa;
d872 1
a872 2
	s = load8888 (combine (src, mask));
	d = load8888 (*dest);
d877 1
a877 1
	*dest = store8888 (s);
d899 2
a900 1
	__m64 s, dia, d, sia;
d902 1
a902 2
	s = load8888 (combine (src, mask));
	d = load8888 (*dest);
d908 1
a908 1
	*dest = store8888 (s);
d930 2
a931 1
	__m64 s, d;
d933 1
a933 2
	s = load8888 (combine (src, mask));
	d = load8888 (*dest);
d935 1
a935 1
	*dest = store8888 (s);
d957 1
a957 1
	uint32_t s = combine (src, mask);
d959 6
a964 4
	__m64 ms = load8888 (s);
	__m64 md = load8888 (d);
	uint32_t sa = s >> 24;
	uint32_t da = ~d >> 24;
d968 2
a969 1
	    __m64 msa = load8888 (DIV_UN8 (da, sa) << 24);
d975 1
a975 1
	*dest = store8888 (md);
d997 2
a998 2
	__m64 a = load8888 (*mask);
	__m64 s = load8888 (*src);
d1001 1
a1001 1
	*dest = store8888 (s);
d1022 3
a1024 3
	__m64 a = load8888 (*mask);
	__m64 s = load8888 (*src);
	__m64 d = load8888 (*dest);
d1027 1
a1027 1
	*dest = store8888 (in_over (s, sa, a, d));
d1048 3
a1050 3
	__m64 a = load8888 (*mask);
	__m64 s = load8888 (*src);
	__m64 d = load8888 (*dest);
d1053 1
a1053 1
	*dest = store8888 (over (d, da, in (s, a)));
d1074 3
a1076 3
	__m64 a = load8888 (*mask);
	__m64 s = load8888 (*src);
	__m64 d = load8888 (*dest);
d1081 1
a1081 1
	*dest = store8888 (s);
d1102 3
a1104 3
	__m64 a = load8888 (*mask);
	__m64 s = load8888 (*src);
	__m64 d = load8888 (*dest);
d1109 1
a1109 1
	*dest = store8888 (d);
d1130 3
a1132 3
	__m64 a = load8888 (*mask);
	__m64 s = load8888 (*src);
	__m64 d = load8888 (*dest);
d1138 1
a1138 1
	*dest = store8888 (s);
d1159 3
a1161 3
	__m64 a = load8888 (*mask);
	__m64 s = load8888 (*src);
	__m64 d = load8888 (*dest);
d1167 1
a1167 1
	*dest = store8888 (d);
d1188 3
a1190 3
	__m64 a = load8888 (*mask);
	__m64 s = load8888 (*src);
	__m64 d = load8888 (*dest);
d1198 1
a1198 1
	*dest = store8888 (d);
d1219 3
a1221 3
	__m64 a = load8888 (*mask);
	__m64 s = load8888 (*src);
	__m64 d = load8888 (*dest);
d1229 1
a1229 1
	*dest = store8888 (d);
d1250 3
a1252 3
	__m64 a = load8888 (*mask);
	__m64 s = load8888 (*src);
	__m64 d = load8888 (*dest);
d1261 1
a1261 1
	*dest = store8888 (d);
d1282 3
a1284 3
	__m64 a = load8888 (*mask);
	__m64 s = load8888 (*src);
	__m64 d = load8888 (*dest);
d1288 1
a1288 1
	*dest = store8888 (d);
d1319 1
a1319 1
    vsrc = load8888 (src);
d1332 1
a1332 1
	    *dst = store8888 (over (vsrc, vsrca, load8888 (*dst)));
d1358 1
a1358 1
	    *dst = store8888 (over (vsrc, vsrca, load8888 (*dst)));
d1385 1
a1385 1
    vsrc = load8888 (src);
d1410 1
a1410 1
	    __m64 vdest;
d1412 4
a1415 6
	    vdest = *(__m64 *)dst;

	    vdest = pack_565 (over (vsrc, vsrca, expand565 (vdest, 0)), vdest, 0);
	    vdest = pack_565 (over (vsrc, vsrca, expand565 (vdest, 1)), vdest, 1);
	    vdest = pack_565 (over (vsrc, vsrca, expand565 (vdest, 2)), vdest, 2);
	    vdest = pack_565 (over (vsrc, vsrca, expand565 (vdest, 3)), vdest, 3);
d1417 1
a1417 1
	    *(__m64 *)dst = vdest;
d1462 1
a1462 1
    vsrc = load8888 (src);
d1477 3
a1479 3
		__m64 vdest = load8888 (*q);
		vdest = in_over (vsrc, vsrca, load8888 (m), vdest);
		*q = store8888 (vdest);
d1498 1
a1498 1
		dest0 = in_over (vsrc, vsrca, load8888 (m0),
d1500 1
a1500 1
		dest1 = in_over (vsrc, vsrca, load8888 (m1),
d1511 1
a1511 1
	while (twidth)
d1517 3
a1519 3
		__m64 vdest = load8888 (*q);
		vdest = in_over (vsrc, vsrca, load8888 (m), vdest);
		*q = store8888 (vdest);
d1554 1
a1554 1
    vmask = load8888 (mask);
d1566 2
a1567 2
	    __m64 s = load8888 (*src);
	    __m64 d = load8888 (*dst);
d1569 1
a1569 1
	    *dst = store8888 (in_over (s, expand_alpha (s), vmask, d));
d1578 1
a1578 1
	    __m64 vs = ldq_u((uint64_t *)src);
d1594 2
a1595 2
	    __m64 s = load8888 (*src);
	    __m64 d = load8888 (*dst);
d1597 1
a1597 1
	    *dst = store8888 (in_over (s, expand_alpha (s), vmask, d));
d1625 1
a1625 1
    vmask = load8888 (mask);
d1638 3
a1640 2
	    __m64 s = load8888 (*src | 0xff000000);
	    __m64 d = load8888 (*dst);
d1642 1
a1642 1
	    *dst = store8888 (in_over (s, srca, vmask, d));
d1660 8
a1667 8
	    __m64 vs0 = ldq_u((uint64_t *)(src + 0));
	    __m64 vs1 = ldq_u((uint64_t *)(src + 2));
	    __m64 vs2 = ldq_u((uint64_t *)(src + 4));
	    __m64 vs3 = ldq_u((uint64_t *)(src + 6));
	    __m64 vs4 = ldq_u((uint64_t *)(src + 8));
	    __m64 vs5 = ldq_u((uint64_t *)(src + 10));
	    __m64 vs6 = ldq_u((uint64_t *)(src + 12));
	    __m64 vs7 = ldq_u((uint64_t *)(src + 14));
d1717 3
a1719 2
	    __m64 s = load8888 (*src | 0xff000000);
	    __m64 d = load8888 (*dst);
d1721 1
a1721 1
	    *dst = store8888 (in_over (s, srca, vmask, d));
d1769 1
a1769 1
		ms = load8888 (s);
d1771 1
a1771 1
		*dst = store8888 (over (ms, sa, load8888 (*dst)));
d1812 1
a1812 1
	    __m64 vsrc = load8888 (*src);
d1830 1
a1830 7
	    __m64 vsrc0, vsrc1, vsrc2, vsrc3;
	    __m64 vdest;

	    vsrc0 = load8888 (*(src + 0));
	    vsrc1 = load8888 (*(src + 1));
	    vsrc2 = load8888 (*(src + 2));
	    vsrc3 = load8888 (*(src + 3));
d1832 9
a1840 1
	    vdest = *(__m64 *)dst;
d1842 1
a1842 6
	    vdest = pack_565 (over (vsrc0, expand_alpha (vsrc0), expand565 (vdest, 0)), vdest, 0);
	    vdest = pack_565 (over (vsrc1, expand_alpha (vsrc1), expand565 (vdest, 1)), vdest, 1);
	    vdest = pack_565 (over (vsrc2, expand_alpha (vsrc2), expand565 (vdest, 2)), vdest, 2);
	    vdest = pack_565 (over (vsrc3, expand_alpha (vsrc3), expand565 (vdest, 3)), vdest, 3);

	    *(__m64 *)dst = vdest;
d1853 1
a1853 1
	    __m64 vsrc = load8888 (*src);
d1896 1
a1896 1
    vsrc = load8888 (src);
d1917 1
a1917 1
				       load8888 (*dst));
d1919 1
a1919 1
		*dst = store8888 (vdest);
d1968 1
a1968 1
		__m64 vdest = load8888 (*dst);
d1972 1
a1972 1
		*dst = store8888 (vdest);
d2051 1
a2051 1
	while (w >= 1 && ((unsigned long)d & 1))
d2058 1
a2058 1
	while (w >= 2 && ((unsigned long)d & 3))
d2111 1
a2111 1
	while (w >= 2)
d2117 1
a2117 1
	while (w >= 1)
d2131 56
d2217 1
a2217 1
    vsrc = load8888 (src);
d2237 1
a2237 1
		*dst = store8888 (vdest);
d2288 1
a2288 1
		__m64 vdest = load8888 (*dst);
d2291 1
a2291 1
		*dst = store8888 (vdest);
d2314 1
a2314 1
    uint64_t srcsrcsrcsrc, src16;
d2327 1
a2327 1
    vsrc = load8888 (src);
d2331 1
a2331 5
    src16 = to_uint64 (tmp);

    srcsrcsrcsrc =
	(uint64_t)src16 << 48 | (uint64_t)src16 << 32 |
	(uint64_t)src16 << 16 | (uint64_t)src16;
d2375 1
a2375 1
		*(uint64_t *)dst = srcsrcsrcsrc;
d2379 1
a2379 2
		__m64 vdest;
		__m64 vm0, vm1, vm2, vm3;
d2381 12
a2392 1
		vdest = *(__m64 *)dst;
d2394 1
a2394 14
		vm0 = to_m64 (m0);
		vdest = pack_565 (in_over (vsrc, vsrca, expand_alpha_rev (vm0),
					   expand565 (vdest, 0)), vdest, 0);
		vm1 = to_m64 (m1);
		vdest = pack_565 (in_over (vsrc, vsrca, expand_alpha_rev (vm1),
					   expand565 (vdest, 1)), vdest, 1);
		vm2 = to_m64 (m2);
		vdest = pack_565 (in_over (vsrc, vsrca, expand_alpha_rev (vm2),
					   expand565 (vdest, 2)), vdest, 2);
		vm3 = to_m64 (m3);
		vdest = pack_565 (in_over (vsrc, vsrca, expand_alpha_rev (vm3),
					   expand565 (vdest, 3)), vdest, 3);

		*(__m64 *)dst = vdest;
d2459 1
a2459 1
	    __m64 vsrc = load8888 (*src);
d2491 4
a2494 5
		__m64 vdest;
		vdest = pack_565 (invert_colors (load8888 (s0)), _mm_setzero_si64 (), 0);
		vdest = pack_565 (invert_colors (load8888 (s1)), vdest, 1);
		vdest = pack_565 (invert_colors (load8888 (s2)), vdest, 2);
		vdest = pack_565 (invert_colors (load8888 (s3)), vdest, 3);
d2496 1
a2496 1
		*(__m64 *)dst = vdest;
d2502 4
a2505 4
		vdest = pack_565 (over_rev_non_pre (load8888 (s0), expand565 (vdest, 0)), vdest, 0);
		vdest = pack_565 (over_rev_non_pre (load8888 (s1), expand565 (vdest, 1)), vdest, 1);
		vdest = pack_565 (over_rev_non_pre (load8888 (s2), expand565 (vdest, 2)), vdest, 2);
		vdest = pack_565 (over_rev_non_pre (load8888 (s3), expand565 (vdest, 3)), vdest, 3);
d2507 1
a2507 1
		*(__m64 *)dst = vdest;
d2519 1
a2519 1
	    __m64 vsrc = load8888 (*src);
d2566 2
a2567 2
	    __m64 s = load8888 (*src);
	    __m64 d = load8888 (*dst);
d2569 1
a2569 1
	    *dst = store8888 (over_rev_non_pre (s, d));
d2578 1
a2578 1
	    uint64_t s0, s1;
d2590 2
a2591 2
		d0 = invert_colors (load8888 (s0));
		d1 = invert_colors (load8888 (s1));
d2599 2
a2600 2
		d0 = over_rev_non_pre (load8888 (s0), expand8888 (vdest, 0));
		d1 = over_rev_non_pre (load8888 (s1), expand8888 (vdest, 1));
d2612 2
a2613 2
	    __m64 s = load8888 (*src);
	    __m64 d = load8888 (*dst);
d2615 1
a2615 1
	    *dst = store8888 (over_rev_non_pre (s, d));
d2643 1
a2643 1
    vsrc = load8888 (src);
d2660 1
a2660 1
		vdest = pack_565 (in_over (vsrc, vsrca, load8888 (m), vdest), vdest, 0);
d2682 4
a2685 4
		vdest = pack_565 (in_over (vsrc, vsrca, load8888 (m0), expand565 (vdest, 0)), vdest, 0);
		vdest = pack_565 (in_over (vsrc, vsrca, load8888 (m1), expand565 (vdest, 1)), vdest, 1);
		vdest = pack_565 (in_over (vsrc, vsrca, load8888 (m2), expand565 (vdest, 2)), vdest, 2);
		vdest = pack_565 (in_over (vsrc, vsrca, load8888 (m3), expand565 (vdest, 3)), vdest, 3);
d2687 1
a2687 1
		*(__m64 *)q = vdest;
d2703 1
a2703 1
		vdest = pack_565 (in_over (vsrc, vsrca, load8888 (m), vdest), vdest, 0);
d2739 1
a2739 1
    vsrc = load8888 (src);
d2771 2
a2772 2
	    vmask = load8888 (ldl_u((uint32_t *)mask));
	    vdest = load8888 (*(uint32_t *)dst);
d2774 1
a2774 1
	    *(uint32_t *)dst = store8888 (in (in (vsrca, vmask), vdest));
d2841 1
a2841 1
	    *d = store8888 (in (load8888 (ldl_u((uint32_t *)s)), load8888 (*d)));
d2889 1
a2889 1
    vsrc = load8888 (src);
d2922 2
a2923 2
	    vmask = load8888 (ldl_u((uint32_t *)mask));
	    vdest = load8888 (*(uint32_t *)dst);
d2925 1
a2925 1
	    *(uint32_t *)dst = store8888 (_mm_adds_pu8 (in (vsrca, vmask), vdest));
d2992 1
a2992 1
	    *(__m64*)dst = _mm_adds_pu8 (ldq_u((uint64_t *)src), *(__m64*)dst);
a3019 1
    __m64 dst64;
d3040 2
a3041 2
	    *dst = _mm_cvtsi64_si32 (_mm_adds_pu8 (_mm_cvtsi32_si64 (*src),
	                                           _mm_cvtsi32_si64 (*dst)));
d3049 1
a3049 2
	    dst64 = _mm_adds_pu8 (ldq_u((uint64_t *)src), *(__m64*)dst);
	    *(uint64_t*)dst = to_uint64 (dst64);
d3057 2
a3058 2
	    *dst = _mm_cvtsi64_si32 (_mm_adds_pu8 (_mm_cvtsi32_si64 (*src),
	                                           _mm_cvtsi32_si64 (*dst)));
d3121 1
a3121 1
	while (w >= 1 && ((unsigned long)d & 1))
d3129 1
a3129 1
	while (w >= 2 && ((unsigned long)d & 3))
d3139 1
a3139 1
	    *(uint32_t *)d = ldl_u((uint32_t *)s);
d3173 8
a3180 8
	    __m64 v0 = ldq_u((uint64_t *)(s + 0));
	    __m64 v1 = ldq_u((uint64_t *)(s + 8));
	    __m64 v2 = ldq_u((uint64_t *)(s + 16));
	    __m64 v3 = ldq_u((uint64_t *)(s + 24));
	    __m64 v4 = ldq_u((uint64_t *)(s + 32));
	    __m64 v5 = ldq_u((uint64_t *)(s + 40));
	    __m64 v6 = ldq_u((uint64_t *)(s + 48));
	    __m64 v7 = ldq_u((uint64_t *)(s + 56));
d3197 1
a3197 1
	    *(uint32_t *)d = ldl_u((uint32_t *)s);
a3231 1
#if 0
d3264 2
a3265 1
		__m64 s = load8888 (*src | 0xff000000);
d3269 1
a3269 1
		    *dst = store8888 (s);
d3275 1
a3275 1
		    __m64 vdest = in_over (s, sa, vm, load8888 (*dst));
d3277 1
a3277 1
		    *dst = store8888 (vdest);
d3289 187
a3475 1
#endif
a3504 4
#if 0
    /* FIXME: This code is commented out since it's apparently
     * not actually faster than the generic code.
     */
a3508 1
#endif
d3512 1
d3528 4
d3632 2
d3637 1
a3637 1
#endif /* USE_X86_MMX || USE_ARM_IWMMXT */
@


1.9
log
@Update to pixman 0.22.2.
0.22.0 was tested by many. 0.22.2 only add a few bug fixes.
Note that on amd64 a recent ld.so is needed to avoid random bus errors.
@
text
@d36 1
a36 1
#ifdef USE_MMX
d50 9
d80 29
a108 1
#ifdef __GNUC__
a111 7
/* If __m64 is defined as a struct or union, define M64_MEMBER to be the
   name of the member used to access the data */
# ifdef _MSC_VER
#  define M64_MEMBER m64_u64
# elif defined(__SUNPRO_C)
#  define M64_MEMBER l_
# endif
d137 1
a137 1
#else                           /* __m64 is an integral type */
d160 4
a163 6
#ifdef __GNUC__
#    ifdef __ICC
#        define MC(x) to_m64 (c.mmx_ ## x)
#    else
#        define MC(x) ((__m64)c.mmx_ ## x)
#    endif
d171 1
a171 1
#ifdef __ICC
d178 1
a178 1
#else                           /* __m64 is an integral type */
d186 1
a186 1
#ifdef __ICC
d191 1
a191 1
#else                           /* __m64 is an integral type */
d329 33
d1151 1
a1151 12
                           pixman_op_t              op,
                           pixman_image_t *         src_image,
                           pixman_image_t *         mask_image,
                           pixman_image_t *         dst_image,
                           int32_t                  src_x,
                           int32_t                  src_y,
                           int32_t                  mask_x,
                           int32_t                  mask_y,
                           int32_t                  dest_x,
                           int32_t                  dest_y,
                           int32_t                  width,
                           int32_t                  height)
d1153 1
d1162 1
a1162 1
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);
d1167 1
a1167 1
    PIXMAN_IMAGE_GET_LINE (dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d1206 1
a1206 1
	while (w)
a1208 3

	    w--;
	    dst++;
d1217 1
a1217 12
                           pixman_op_t              op,
                           pixman_image_t *         src_image,
                           pixman_image_t *         mask_image,
                           pixman_image_t *         dst_image,
                           int32_t                  src_x,
                           int32_t                  src_y,
                           int32_t                  mask_x,
                           int32_t                  mask_y,
                           int32_t                  dest_x,
                           int32_t                  dest_y,
                           int32_t                  width,
                           int32_t                  height)
d1219 1
d1228 1
a1228 1
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);
d1233 1
a1233 1
    PIXMAN_IMAGE_GET_LINE (dst_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);
d1295 1
a1295 12
                                   pixman_op_t              op,
                                   pixman_image_t *         src_image,
                                   pixman_image_t *         mask_image,
                                   pixman_image_t *         dst_image,
                                   int32_t                  src_x,
                                   int32_t                  src_y,
                                   int32_t                  mask_x,
                                   int32_t                  mask_y,
                                   int32_t                  dest_x,
                                   int32_t                  dest_y,
                                   int32_t                  width,
                                   int32_t                  height)
d1297 2
a1298 1
    uint32_t src, srca;
d1306 1
a1306 1
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);
a1307 1
    srca = src >> 24;
d1311 1
a1311 1
    PIXMAN_IMAGE_GET_LINE (dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d1388 1
a1388 12
                                pixman_op_t              op,
                                pixman_image_t *         src_image,
                                pixman_image_t *         mask_image,
                                pixman_image_t *         dst_image,
                                int32_t                  src_x,
                                int32_t                  src_y,
                                int32_t                  mask_x,
                                int32_t                  mask_y,
                                int32_t                  dest_x,
                                int32_t                  dest_y,
                                int32_t                  width,
                                int32_t                  height)
d1390 1
a1396 1
    __m64 srca;
d1400 1
a1400 1
    PIXMAN_IMAGE_GET_LINE (dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d1403 1
a1403 1
    mask = _pixman_image_get_solid (imp, mask_image, dst_image->bits.format);
a1406 1
    srca = MC (4x00ff);
d1430 1
a1430 1
	    __m64 vs = *(__m64 *)src;
d1444 1
a1444 1
	while (w)
a1449 4

	    w--;
	    dst++;
	    src++;
d1458 1
a1458 12
                                pixman_op_t              op,
                                pixman_image_t *         src_image,
                                pixman_image_t *         mask_image,
                                pixman_image_t *         dst_image,
                                int32_t                  src_x,
                                int32_t                  src_y,
                                int32_t                  mask_x,
                                int32_t                  mask_y,
                                int32_t                  dest_x,
                                int32_t                  dest_y,
                                int32_t                  width,
                                int32_t                  height)
d1460 1
d1471 1
a1471 1
    PIXMAN_IMAGE_GET_LINE (dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d1473 1
a1473 1
    mask = _pixman_image_get_solid (imp, mask_image, dst_image->bits.format);
d1511 8
a1518 8
	    __m64 vs0 = *(__m64 *)(src + 0);
	    __m64 vs1 = *(__m64 *)(src + 2);
	    __m64 vs2 = *(__m64 *)(src + 4);
	    __m64 vs3 = *(__m64 *)(src + 6);
	    __m64 vs4 = *(__m64 *)(src + 8);
	    __m64 vs5 = *(__m64 *)(src + 10);
	    __m64 vs6 = *(__m64 *)(src + 12);
	    __m64 vs7 = *(__m64 *)(src + 14);
d1584 1
a1584 12
                              pixman_op_t              op,
                              pixman_image_t *         src_image,
                              pixman_image_t *         mask_image,
                              pixman_image_t *         dst_image,
                              int32_t                  src_x,
                              int32_t                  src_y,
                              int32_t                  mask_x,
                              int32_t                  mask_y,
                              int32_t                  dest_x,
                              int32_t                  dest_y,
                              int32_t                  width,
                              int32_t                  height)
d1586 1
d1596 1
a1596 1
    PIXMAN_IMAGE_GET_LINE (dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d1632 1
a1632 12
                              pixman_op_t              op,
                              pixman_image_t *         src_image,
                              pixman_image_t *         mask_image,
                              pixman_image_t *         dst_image,
                              int32_t                  src_x,
                              int32_t                  src_y,
                              int32_t                  mask_x,
                              int32_t                  mask_y,
                              int32_t                  dest_x,
                              int32_t                  dest_y,
                              int32_t                  width,
                              int32_t                  height)
d1634 1
d1642 1
a1642 1
    PIXMAN_IMAGE_GET_LINE (dst_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);
d1725 1
a1725 12
                             pixman_op_t              op,
                             pixman_image_t *         src_image,
                             pixman_image_t *         mask_image,
                             pixman_image_t *         dst_image,
                             int32_t                  src_x,
                             int32_t                  src_y,
                             int32_t                  mask_x,
                             int32_t                  mask_y,
                             int32_t                  dest_x,
                             int32_t                  dest_y,
                             int32_t                  width,
                             int32_t                  height)
d1727 1
d1738 1
a1738 1
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);
d1746 1
a1746 1
    PIXMAN_IMAGE_GET_LINE (dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d1815 1
a1815 1
	while (w)
a1826 4

	    w--;
	    mask++;
	    dst++;
d1848 1
a1848 1
#ifdef __GNUC__
d1882 1
a1882 1
#ifdef __GNUC__
d1928 1
a1928 1
#ifdef __GNUC__
d1985 1
a1985 12
                            pixman_op_t              op,
                            pixman_image_t *         src_image,
                            pixman_image_t *         mask_image,
                            pixman_image_t *         dst_image,
                            int32_t                  src_x,
                            int32_t                  src_y,
                            int32_t                  mask_x,
                            int32_t                  mask_y,
                            int32_t                  dest_x,
                            int32_t                  dest_y,
                            int32_t                  width,
                            int32_t                  height)
d1987 1
d1993 1
a1993 1
    __m64 vsrc, vsrca;
d1998 1
a1998 1
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);
d2003 2
a2004 2
	pixman_fill_mmx (dst_image->bits.bits, dst_image->bits.rowstride,
			 PIXMAN_FORMAT_BPP (dst_image->bits.format),
d2011 1
a2011 1
    PIXMAN_IMAGE_GET_LINE (dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
a2014 1
    vsrca = expand_alpha (vsrc);
a2059 1
		__m64 vdest;
a2061 2
		vdest = *(__m64 *)dst;

d2079 1
a2079 1
	while (w)
a2093 4

	    w--;
	    mask++;
	    dst++;
d2102 1
a2102 12
                             pixman_op_t              op,
                             pixman_image_t *         src_image,
                             pixman_image_t *         mask_image,
                             pixman_image_t *         dst_image,
                             int32_t                  src_x,
                             int32_t                  src_y,
                             int32_t                  mask_x,
                             int32_t                  mask_y,
                             int32_t                  dest_x,
                             int32_t                  dest_y,
                             int32_t                  width,
                             int32_t                  height)
d2104 1
d2115 1
a2115 1
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);
d2121 1
a2121 1
    PIXMAN_IMAGE_GET_LINE (dst_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);
d2233 1
a2233 12
                                pixman_op_t              op,
                                pixman_image_t *         src_image,
                                pixman_image_t *         mask_image,
                                pixman_image_t *         dst_image,
                                int32_t                  src_x,
                                int32_t                  src_y,
                                int32_t                  mask_x,
                                int32_t                  mask_y,
                                int32_t                  dest_x,
                                int32_t                  dest_y,
                                int32_t                  width,
                                int32_t                  height)
d2235 1
d2243 1
a2243 1
    PIXMAN_IMAGE_GET_LINE (dst_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);
d2343 1
a2343 12
                                pixman_op_t              op,
                                pixman_image_t *         src_image,
                                pixman_image_t *         mask_image,
                                pixman_image_t *         dst_image,
                                int32_t                  src_x,
                                int32_t                  src_y,
                                int32_t                  mask_x,
                                int32_t                  mask_y,
                                int32_t                  dest_x,
                                int32_t                  dest_y,
                                int32_t                  width,
                                int32_t                  height)
d2345 1
d2353 1
a2353 1
    PIXMAN_IMAGE_GET_LINE (dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d2415 1
a2415 1
	while (w)
a2420 4

	    w--;
	    dst++;
	    src++;
d2429 1
a2429 12
                                   pixman_op_t              op,
                                   pixman_image_t *         src_image,
                                   pixman_image_t *         mask_image,
                                   pixman_image_t *         dst_image,
                                   int32_t                  src_x,
                                   int32_t                  src_y,
                                   int32_t                  mask_x,
                                   int32_t                  mask_y,
                                   int32_t                  dest_x,
                                   int32_t                  dest_y,
                                   int32_t                  width,
                                   int32_t                  height)
d2431 2
a2432 1
    uint32_t src, srca;
d2440 1
a2440 1
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);
a2441 1
    srca = src >> 24;
d2445 1
a2445 1
    PIXMAN_IMAGE_GET_LINE (dst_image, dest_x, dest_y, uint16_t, dst_stride, dst_line, 1);
d2526 1
a2526 12
                        pixman_op_t              op,
                        pixman_image_t *         src_image,
                        pixman_image_t *         mask_image,
                        pixman_image_t *         dst_image,
                        int32_t                  src_x,
                        int32_t                  src_y,
                        int32_t                  mask_x,
                        int32_t                  mask_y,
                        int32_t                  dest_x,
                        int32_t                  dest_y,
                        int32_t                  width,
                        int32_t                  height)
d2528 1
d2537 1
a2537 1
    PIXMAN_IMAGE_GET_LINE (dst_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);
d2540 1
a2540 1
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);
d2555 1
a2555 2
	if ((((unsigned long)dst_image & 3) == 0) &&
	    (((unsigned long)src_image & 3) == 0))
d2557 13
a2569 5
	    while (w >= 4)
	    {
		uint32_t m;
		__m64 vmask;
		__m64 vdest;
d2571 4
a2574 1
		m = 0;
d2576 2
a2577 2
		vmask = load8888 (*(uint32_t *)mask);
		vdest = load8888 (*(uint32_t *)dst);
d2579 1
a2579 1
		*(uint32_t *)dst = store8888 (in (in (vsrca, vmask), vdest));
d2581 3
a2583 4
		dst += 4;
		mask += 4;
		w -= 4;
	    }
d2607 1
a2607 12
                      pixman_op_t              op,
                      pixman_image_t *         src_image,
                      pixman_image_t *         mask_image,
                      pixman_image_t *         dst_image,
                      int32_t                  src_x,
                      int32_t                  src_y,
                      int32_t                  mask_x,
                      int32_t                  mask_y,
                      int32_t                  dest_x,
                      int32_t                  dest_y,
                      int32_t                  width,
                      int32_t                  height)
d2609 1
d2615 1
a2615 1
    PIXMAN_IMAGE_GET_LINE (dst_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);
d2626 1
a2626 2
	if ((((unsigned long)dst_image & 3) == 0) &&
	    (((unsigned long)src_image & 3) == 0))
d2628 17
a2644 4
	    while (w >= 4)
	    {
		uint32_t *s = (uint32_t *)src;
		uint32_t *d = (uint32_t *)dst;
d2646 1
a2646 1
		*d = store8888 (in (load8888 (*s), load8888 (*d)));
d2648 3
a2650 4
		w -= 4;
		dst += 4;
		src += 4;
	    }
d2673 1
a2673 12
			 pixman_op_t              op,
			 pixman_image_t *         src_image,
			 pixman_image_t *         mask_image,
			 pixman_image_t *         dst_image,
			 int32_t                  src_x,
			 int32_t                  src_y,
			 int32_t                  mask_x,
			 int32_t                  mask_y,
			 int32_t                  dest_x,
			 int32_t                  dest_y,
			 int32_t                  width,
			 int32_t                  height)
d2675 1
d2684 1
a2684 1
    PIXMAN_IMAGE_GET_LINE (dst_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);
d2687 1
a2687 1
    src = _pixman_image_get_solid (imp, src_image, dst_image->bits.format);
d2705 18
a2722 2
	if ((((unsigned long)mask_image & 3) == 0) &&
	    (((unsigned long)dst_image  & 3) == 0))
d2724 5
a2728 4
	    while (w >= 4)
	    {
		__m64 vmask = load8888 (*(uint32_t *)mask);
		__m64 vdest = load8888 (*(uint32_t *)dst);
d2730 1
a2730 1
		*(uint32_t *)dst = store8888 (_mm_adds_pu8 (in (vsrca, vmask), vdest));
d2732 3
a2734 4
		w -= 4;
		dst += 4;
		mask += 4;
	    }
d2759 1
a2759 12
		       pixman_op_t              op,
		       pixman_image_t *         src_image,
		       pixman_image_t *         mask_image,
		       pixman_image_t *         dst_image,
		       int32_t                  src_x,
		       int32_t                  src_y,
		       int32_t                  mask_x,
		       int32_t                  mask_y,
		       int32_t                  dest_x,
		       int32_t                  dest_y,
		       int32_t                  width,
		       int32_t                  height)
d2761 1
d2772 1
a2772 1
    PIXMAN_IMAGE_GET_LINE (dst_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);
d2797 1
a2797 1
	    *(__m64*)dst = _mm_adds_pu8 (*(__m64*)src, *(__m64*)dst);
d2822 1
a2822 12
                             pixman_op_t              op,
                             pixman_image_t *         src_image,
                             pixman_image_t *         mask_image,
                             pixman_image_t *         dst_image,
                             int32_t                  src_x,
                             int32_t                  src_y,
                             int32_t                  mask_x,
                             int32_t                  mask_y,
                             int32_t                  dest_x,
                             int32_t                  dest_y,
                             int32_t                  width,
                             int32_t                  height)
d2824 1
d2834 1
a2834 1
    PIXMAN_IMAGE_GET_LINE (dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d2855 1
a2855 1
	    dst64 = _mm_adds_pu8 (*(__m64*)src, *(__m64*)dst);
d2882 2
a2883 2
                int       dst_x,
                int       dst_y,
d2899 1
a2899 1
	dst_bytes = (uint8_t *)(((uint16_t *)dst_bits) + dst_stride * (dst_y) + (dst_x));
d2909 1
a2909 1
	dst_bytes = (uint8_t *)(((uint32_t *)dst_bits) + dst_stride * (dst_y) + (dst_x));
d2928 8
d2946 1
a2946 1
	    *(uint32_t *)d = *(uint32_t *)s;
d2955 1
a2955 1
#if defined (__GNUC__) || (defined(__SUNPRO_C) && (__SUNPRO_C >= 0x590))
d2980 8
a2987 8
	    __m64 v0 = *(__m64 *)(s + 0);
	    __m64 v1 = *(__m64 *)(s + 8);
	    __m64 v2 = *(__m64 *)(s + 16);
	    __m64 v3 = *(__m64 *)(s + 24);
	    __m64 v4 = *(__m64 *)(s + 32);
	    __m64 v5 = *(__m64 *)(s + 40);
	    __m64 v6 = *(__m64 *)(s + 48);
	    __m64 v7 = *(__m64 *)(s + 56);
d3004 1
a3004 1
	    *(uint32_t *)d = *(uint32_t *)s;
d3026 1
a3026 12
                         pixman_op_t              op,
                         pixman_image_t *         src_image,
                         pixman_image_t *         mask_image,
                         pixman_image_t *         dst_image,
                         int32_t                  src_x,
                         int32_t                  src_y,
                         int32_t                  mask_x,
                         int32_t                  mask_y,
                         int32_t                  dest_x,
                         int32_t                  dest_y,
                         int32_t                  width,
                         int32_t                  height)
d3028 2
d3031 1
a3031 1
                    dst_image->bits.bits,
d3033 1
a3033 1
                    dst_image->bits.rowstride,
d3035 1
a3035 1
                    PIXMAN_FORMAT_BPP (dst_image->bits.format),
d3042 1
a3042 12
                                pixman_op_t              op,
                                pixman_image_t *         src_image,
                                pixman_image_t *         mask_image,
                                pixman_image_t *         dst_image,
                                int32_t                  src_x,
                                int32_t                  src_y,
                                int32_t                  mask_x,
                                int32_t                  mask_y,
                                int32_t                  dest_x,
                                int32_t                  dest_y,
                                int32_t                  width,
                                int32_t                  height)
d3044 1
d3051 1
a3051 1
    PIXMAN_IMAGE_GET_LINE (dst_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
d3132 2
a3133 2
    PIXMAN_STD_FAST_PATH    (OVER, x8b8r8g8, a8,       x8b8g8r8, mmx_composite_over_x888_8_8888    ),
    PIXMAN_STD_FAST_PATH    (OVER, x8b8r8g8, a8,       a8r8g8b8, mmx_composite_over_x888_8_8888    ),
d3182 2
a3183 2
         int                      dst_x,
         int                      dst_y,
d3189 1
a3189 1
            src_x, src_y, dst_x, dst_y, width, height))
d3195 1
a3195 1
	    src_x, src_y, dst_x, dst_y, width, height);
d3256 1
a3256 1
#endif /* USE_MMX */
@


1.8
log
@Bug-fix upgrade to pixman 0.20.2. No API/ABI change.
tested by shadchin@@, ajacoutot@@, krw@@, ok miod@@
@
text
@d1111 1
a1111 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d1190 1
a1190 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d1278 1
a1278 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d1387 1
a1387 1
    mask = _pixman_image_get_solid (mask_image, dst_image->bits.format);
d1472 1
a1472 1
    mask = _pixman_image_get_solid (mask_image, dst_image->bits.format);
d1767 1
a1767 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d2041 1
a2041 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d2176 1
a2176 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d2535 1
a2535 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d2646 1
a2646 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d2793 1
a2793 1
    src = _pixman_image_get_solid (src_image, dst_image->bits.format);
d3343 1
a3343 1
_pixman_implementation_create_mmx (void)
d3345 1
a3345 2
    pixman_implementation_t *general = _pixman_implementation_create_fast_path ();
    pixman_implementation_t *imp = _pixman_implementation_create (general, mmx_fast_paths);
@


1.7
log
@Update to pixman 0.20.0.
tested by ajacoutot@@, krw@@ and on a bulk ports build by landry@@.
@
text
@d1924 2
a1925 2
	: "=y" (v1), "=y" (v2), "=y" (v3),
	  "=y" (v4), "=y" (v5), "=y" (v6), "=y" (v7)
@


1.6
log
@Update to pixman 0.18.4.

Tweak build to use libpthread-stubs for TLS emulation instead of forcing
every application using pixman to use -pthread.

Tested by jasper@@ and landry@@ on a bulk ports build.
@
text
@d132 1
a132 1
#        define MC(x)  M64 (c.mmx_ ## x)
d141 1
a141 1
M64 (uint64_t x)
d156 1
a156 1
UINT64 (__m64 x)
d1211 1
a1211 1
	    __m64 vdest = expand565 (M64 (d), 0);
d1214 1
a1214 1
	    *dst = UINT64 (vdest);
d1242 1
a1242 1
	    __m64 vdest = expand565 (M64 (d), 0);
d1245 1
a1245 1
	    *dst = UINT64 (vdest);
d1683 1
a1683 1
	    __m64 vdest = expand565 (M64 (d), 0);
d1688 1
a1688 1
	    *dst = UINT64 (vdest);
d1727 1
a1727 1
	    __m64 vdest = expand565 (M64 (d), 0);
d1731 1
a1731 1
	    *dst = UINT64 (vdest);
d1798 1
a1798 1
				       expand_alpha_rev (M64 (m)),
d1829 1
a1829 1
		dest0 = in_over (vsrc, vsrca, expand_alpha_rev (M64 (m0)),
d1831 1
a1831 1
		dest1 = in_over (vsrc, vsrca, expand_alpha_rev (M64 (m1)),
d1853 1
a1853 1
		    vsrc, vsrca, expand_alpha_rev (M64 (m)), vdest);
d1913 1
a1913 1
    vfill = M64 (fill);
d2076 1
a2076 1
		__m64 vdest = in (vsrc, expand_alpha_rev (M64 (m)));
d2109 2
a2110 2
		dest0 = in (vsrc, expand_alpha_rev (M64 (m0)));
		dest1 = in (vsrc, expand_alpha_rev (M64 (m1)));
d2134 1
a2134 1
		vdest = in (vsrc, expand_alpha_rev (M64 (m)));
d2189 1
a2189 1
    src16 = UINT64 (tmp);
d2212 1
a2212 1
		__m64 vd = M64 (d);
d2214 1
a2214 1
		    vsrc, vsrca, expand_alpha_rev (M64 (m)), expand565 (vd, 0));
d2217 1
a2217 1
		*dst = UINT64 (vd);
d2246 1
a2246 1
		vm0 = M64 (m0);
d2249 1
a2249 1
		vm1 = M64 (m1);
d2252 1
a2252 1
		vm2 = M64 (m2);
d2255 1
a2255 1
		vm3 = M64 (m3);
d2276 2
a2277 2
		__m64 vd = M64 (d);
		__m64 vdest = in_over (vsrc, vsrca, expand_alpha_rev (M64 (m)),
d2280 1
a2280 1
		*dst = UINT64 (vd);
d2336 1
a2336 1
	    __m64 vdest = expand565 (M64 (d), 0);
d2340 1
a2340 1
	    *dst = UINT64 (vdest);
d2397 1
a2397 1
	    __m64 vdest = expand565 (M64 (d), 0);
d2401 1
a2401 1
	    *dst = UINT64 (vdest);
d2560 1
a2560 1
		__m64 vdest = expand565 (M64 (d), 0);
d2562 1
a2562 1
		*q = UINT64 (vdest);
d2603 1
a2603 1
		__m64 vdest = expand565 (M64 (d), 0);
d2605 1
a2605 1
		*q = UINT64 (vdest);
d2848 13
a2860 13
mmx_composite_add_8000_8000 (pixman_implementation_t *imp,
                             pixman_op_t              op,
                             pixman_image_t *         src_image,
                             pixman_image_t *         mask_image,
                             pixman_image_t *         dst_image,
                             int32_t                  src_x,
                             int32_t                  src_y,
                             int32_t                  mask_x,
                             int32_t                  mask_y,
                             int32_t                  dest_x,
                             int32_t                  dest_y,
                             int32_t                  width,
                             int32_t                  height)
d2966 1
a2966 1
	    *(uint64_t*)dst = UINT64 (dst64);
d3202 1
a3202 1
		    __m64 vm = expand_alpha_rev (M64 (m));
d3271 1
a3271 1
    PIXMAN_STD_FAST_PATH    (ADD,  a8,       null,     a8,       mmx_composite_add_8000_8000       ),
@


1.5
log
@Update to pixman 0.16.6. Tested on a full ports build by naddy@@.
@
text
@d488 1
a488 1
	
d514 1
a514 1
	
d519 1
a519 1
	
d543 1
a543 1
	
d571 1
a571 1
	
d600 1
a600 1
	
d630 1
a630 1
	
d662 1
a662 1
	
d692 1
a692 1
	
d723 1
a723 1
	
d788 1
a788 1
	
d867 1
a867 1
	
d895 1
a895 1
	
d923 1
a923 1
	
d1105 1
a1105 1
    uint16_t w;
d1184 1
a1184 1
    uint16_t w;
d1212 1
a1212 1
	    
d1243 1
a1243 1
	    
d1379 1
a1379 1
    uint16_t w;
d1388 1
d1465 1
a1465 1
    uint16_t w;
d1474 1
d1601 1
a1601 1
    uint16_t w;
d1620 1
a1620 1
	    
d1632 1
a1632 1
	    
d1657 1
a1657 1
    uint16_t w;
d1761 1
a1761 1
    uint16_t w;
d1800 1
a1800 1
		
d1814 1
a1814 1
	    
a1887 10
    if (bpp == 16 && (xor >> 16 != (xor & 0xffff)))
	return FALSE;

    if (bpp == 8 &&
        ((xor >> 16 != (xor & 0xffff)) ||
         (xor >> 24 != (xor & 0x00ff) >> 16)))
    {
	return FALSE;
    }

d1894 1
d1902 1
d1925 1
a1925 1
        "=y" (v4), "=y" (v5), "=y" (v6), "=y" (v7)
d1933 1
a1933 1
	
d1973 2
a1974 2
	        "y" (vfill), "y" (v1), "y" (v2), "y" (v3),
	        "y" (v4), "y" (v5), "y" (v6), "y" (v7)
d2035 1
a2035 1
    uint16_t w;
d2077 1
a2077 1
		
d2133 1
a2133 1
		
d2170 1
a2170 1
    uint16_t w;
d2215 1
a2215 1
		
d2310 1
a2310 1
    uint16_t w;
d2430 1
a2430 1
    uint16_t w;
d2638 1
a2638 1
    uint16_t w;
d2720 1
a2720 1
    uint16_t w;
d2785 1
a2785 1
    uint16_t w;
d2865 1
a2865 1
    uint16_t w;
d2939 1
a2939 1
    uint16_t w;
d3079 2
a3080 2
	        "%mm0", "%mm1", "%mm2", "%mm3",
	        "%mm4", "%mm5", "%mm6", "%mm7");
d3150 1
d3166 3
a3168 3
    uint32_t    *src, *src_line;
    uint32_t    *dst, *dst_line;
    uint8_t     *mask, *mask_line;
d3170 1
a3170 1
    uint16_t w;
d3217 1
d3221 26
a3246 32
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_r5g6b5,   mmx_composite_over_n_8_0565,     0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_b5g6r5,   mmx_composite_over_n_8_0565,     0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_a8r8g8b8, mmx_composite_over_n_8_8888,     0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_x8r8g8b8, mmx_composite_over_n_8_8888,     0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_a8b8g8r8, mmx_composite_over_n_8_8888,     0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_x8b8g8r8, mmx_composite_over_n_8_8888,     0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8r8g8b8, PIXMAN_a8r8g8b8, mmx_composite_over_n_8888_8888_ca, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8r8g8b8, PIXMAN_x8r8g8b8, mmx_composite_over_n_8888_8888_ca, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8r8g8b8, PIXMAN_r5g6b5,   mmx_composite_over_n_8888_0565_ca, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8b8g8r8, PIXMAN_a8b8g8r8, mmx_composite_over_n_8888_8888_ca, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8b8g8r8, PIXMAN_x8b8g8r8, mmx_composite_over_n_8888_8888_ca, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8b8g8r8, PIXMAN_b5g6r5,   mmx_composite_over_n_8888_0565_ca, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8r8g8b8, PIXMAN_a8r8g8b8, mmx_composite_over_pixbuf_8888, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8b8g8r8, PIXMAN_a8r8g8b8, mmx_composite_over_pixbuf_8888, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8r8g8b8, PIXMAN_x8r8g8b8, mmx_composite_over_pixbuf_8888, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8b8g8r8, PIXMAN_x8r8g8b8, mmx_composite_over_pixbuf_8888, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8r8g8b8, PIXMAN_r5g6b5,   mmx_composite_over_pixbuf_0565, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8b8g8r8, PIXMAN_r5g6b5,   mmx_composite_over_pixbuf_0565, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8r8g8b8, PIXMAN_a8b8g8r8, mmx_composite_over_pixbuf_8888, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8b8g8r8, PIXMAN_a8b8g8r8, mmx_composite_over_pixbuf_8888, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8r8g8b8, PIXMAN_x8b8g8r8, mmx_composite_over_pixbuf_8888, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8b8g8r8, PIXMAN_x8b8g8r8, mmx_composite_over_pixbuf_8888, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8r8g8b8, PIXMAN_b5g6r5,   mmx_composite_over_pixbuf_0565, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8b8g8r8, PIXMAN_b5g6r5,   mmx_composite_over_pixbuf_0565, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8,       PIXMAN_a8r8g8b8, mmx_composite_over_x888_n_8888,    NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8,       PIXMAN_x8r8g8b8, mmx_composite_over_x888_n_8888,           NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8,       PIXMAN_a8b8g8r8, mmx_composite_over_x888_n_8888,           NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8,       PIXMAN_x8b8g8r8, mmx_composite_over_x888_n_8888,           NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_a8r8g8b8, PIXMAN_a8,       PIXMAN_a8r8g8b8, mmx_composite_over_8888_n_8888,    NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_a8r8g8b8, PIXMAN_a8,       PIXMAN_x8r8g8b8, mmx_composite_over_8888_n_8888,           NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_a8b8g8r8, PIXMAN_a8,       PIXMAN_a8b8g8r8, mmx_composite_over_8888_n_8888,           NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_a8b8g8r8, PIXMAN_a8,       PIXMAN_x8b8g8r8, mmx_composite_over_8888_n_8888,           NEED_SOLID_MASK },
d3248 7
a3254 5
    /* FIXME: This code is commented out since it's apparently not actually faster than the generic code. */
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8,       PIXMAN_x8r8g8b8, mmx_composite_over_x888_8_8888,   0 },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8,       PIXMAN_a8r8g8b8, mmx_composite_over_x888_8_8888,   0 },
    { PIXMAN_OP_OVER, PIXMAN_x8b8r8g8, PIXMAN_a8,       PIXMAN_x8b8g8r8, mmx_composite_over_x888_8_8888,   0 },
    { PIXMAN_OP_OVER, PIXMAN_x8b8r8g8, PIXMAN_a8,       PIXMAN_a8r8g8b8, mmx_composite_over_x888_8_8888,   0 },
d3256 30
a3285 30
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_null,     PIXMAN_a8r8g8b8, mmx_composite_over_n_8888,       0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_null,     PIXMAN_x8r8g8b8, mmx_composite_over_n_8888,        0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_null,     PIXMAN_r5g6b5,   mmx_composite_over_n_0565,        0 },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_null,     PIXMAN_x8r8g8b8, mmx_composite_copy_area,          0 },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_null,     PIXMAN_x8b8g8r8, mmx_composite_copy_area,          0 },

    { PIXMAN_OP_OVER, PIXMAN_a8r8g8b8, PIXMAN_null,     PIXMAN_a8r8g8b8, mmx_composite_over_8888_8888,     0 },
    { PIXMAN_OP_OVER, PIXMAN_a8r8g8b8, PIXMAN_null,     PIXMAN_x8r8g8b8, mmx_composite_over_8888_8888,     0 },
    { PIXMAN_OP_OVER, PIXMAN_a8r8g8b8, PIXMAN_null,     PIXMAN_r5g6b5,   mmx_composite_over_8888_0565,     0 },
    { PIXMAN_OP_OVER, PIXMAN_a8b8g8r8, PIXMAN_null,     PIXMAN_a8b8g8r8, mmx_composite_over_8888_8888,     0 },
    { PIXMAN_OP_OVER, PIXMAN_a8b8g8r8, PIXMAN_null,     PIXMAN_x8b8g8r8, mmx_composite_over_8888_8888,     0 },
    { PIXMAN_OP_OVER, PIXMAN_a8b8g8r8, PIXMAN_null,     PIXMAN_b5g6r5,   mmx_composite_over_8888_0565,     0 },

    { PIXMAN_OP_ADD, PIXMAN_a8r8g8b8,  PIXMAN_null,     PIXMAN_a8r8g8b8, mmx_composite_add_8888_8888,   0 },
    { PIXMAN_OP_ADD, PIXMAN_a8b8g8r8,  PIXMAN_null,     PIXMAN_a8b8g8r8, mmx_composite_add_8888_8888,   0 },
    { PIXMAN_OP_ADD, PIXMAN_a8,        PIXMAN_null,     PIXMAN_a8,       mmx_composite_add_8000_8000,   0 },
    { PIXMAN_OP_ADD, PIXMAN_solid,     PIXMAN_a8,       PIXMAN_a8,       mmx_composite_add_n_8_8,    0 },

    { PIXMAN_OP_SRC, PIXMAN_solid,     PIXMAN_a8,       PIXMAN_a8r8g8b8, mmx_composite_src_n_8_8888, 0 },
    { PIXMAN_OP_SRC, PIXMAN_solid,     PIXMAN_a8,       PIXMAN_x8r8g8b8, mmx_composite_src_n_8_8888, 0 },
    { PIXMAN_OP_SRC, PIXMAN_solid,     PIXMAN_a8,       PIXMAN_a8b8g8r8, mmx_composite_src_n_8_8888, 0 },
    { PIXMAN_OP_SRC, PIXMAN_solid,     PIXMAN_a8,       PIXMAN_x8b8g8r8, mmx_composite_src_n_8_8888, 0 },
    { PIXMAN_OP_SRC, PIXMAN_a8r8g8b8,  PIXMAN_null,     PIXMAN_a8r8g8b8, mmx_composite_copy_area, 0 },
    { PIXMAN_OP_SRC, PIXMAN_a8b8g8r8,  PIXMAN_null,     PIXMAN_a8b8g8r8, mmx_composite_copy_area, 0 },
    { PIXMAN_OP_SRC, PIXMAN_a8r8g8b8,  PIXMAN_null,     PIXMAN_x8r8g8b8, mmx_composite_copy_area, 0 },
    { PIXMAN_OP_SRC, PIXMAN_a8b8g8r8,  PIXMAN_null,     PIXMAN_x8b8g8r8, mmx_composite_copy_area, 0 },
    { PIXMAN_OP_SRC, PIXMAN_x8r8g8b8,  PIXMAN_null,     PIXMAN_x8r8g8b8, mmx_composite_copy_area, 0 },
    { PIXMAN_OP_SRC, PIXMAN_x8b8g8r8,  PIXMAN_null,     PIXMAN_x8b8g8r8, mmx_composite_copy_area, 0 },
    { PIXMAN_OP_SRC, PIXMAN_r5g6b5,    PIXMAN_null,     PIXMAN_r5g6b5,   mmx_composite_copy_area, 0 },
    { PIXMAN_OP_SRC, PIXMAN_b5g6r5,    PIXMAN_null,     PIXMAN_b5g6r5,   mmx_composite_copy_area, 0 },
d3287 2
a3288 2
    { PIXMAN_OP_IN,  PIXMAN_a8,        PIXMAN_null,     PIXMAN_a8,       mmx_composite_in_8_8,   0 },
    { PIXMAN_OP_IN,  PIXMAN_solid,     PIXMAN_a8,       PIXMAN_a8,       mmx_composite_in_n_8_8, 0 },
a3292 31
static void
mmx_composite (pixman_implementation_t *imp,
               pixman_op_t              op,
               pixman_image_t *         src,
               pixman_image_t *         mask,
               pixman_image_t *         dest,
               int32_t                  src_x,
               int32_t                  src_y,
               int32_t                  mask_x,
               int32_t                  mask_y,
               int32_t                  dest_x,
               int32_t                  dest_y,
               int32_t                  width,
               int32_t                  height)
{
    if (_pixman_run_fast_path (mmx_fast_paths, imp,
                               op, src, mask, dest,
                               src_x, src_y,
                               mask_x, mask_y,
                               dest_x, dest_y,
                               width, height))
    {
	return;
    }

    _pixman_implementation_composite (imp->delegate,
                                      op, src, mask, dest, src_x, src_y,
                                      mask_x, mask_y, dest_x, dest_y,
                                      width, height);
}

d3314 3
a3316 3
	           imp->delegate,
	           src_bits, dst_bits, src_stride, dst_stride, src_bpp, dst_bpp,
	           src_x, src_y, dst_x, dst_y, width, height);
d3336 1
a3336 1
	           imp->delegate, bits, stride, bpp, x, y, width, height, xor);
d3346 1
a3346 1
    pixman_implementation_t *imp = _pixman_implementation_create (general);
a3371 1
    imp->composite = mmx_composite;
@


1.4
log
@Update to pixman 0.15.8.
@
text
@d40 1
d42 1
a42 1
#define noVERBOSE
d45 1
a45 1
#define CHECKPOINT() ErrorF ("at %s %d\n", __FUNCTION__, __LINE__)
d101 1
a101 1
} MMXData;
d104 5
a108 5
# define MMXDATA_INIT(field, val) { val##UI64 }
#elif defined(M64_MEMBER)	/* __m64 is a struct, not an integral type */
# define MMXDATA_INIT(field, val) field =   { val##ULL }
#else				/* __m64 is an integral type */
# define MMXDATA_INIT(field, val) field =   val##ULL
d111 1
a111 1
static const MMXData c =
d113 15
a127 15
    MMXDATA_INIT(.mmx_4x00ff,			0x00ff00ff00ff00ff),
    MMXDATA_INIT(.mmx_4x0080,			0x0080008000800080),
    MMXDATA_INIT(.mmx_565_rgb,			0x000001f0003f001f),
    MMXDATA_INIT(.mmx_565_unpack_multiplier,	0x0000008404100840),
    MMXDATA_INIT(.mmx_565_r,			0x000000f800000000),
    MMXDATA_INIT(.mmx_565_g,			0x0000000000fc0000),
    MMXDATA_INIT(.mmx_565_b,			0x00000000000000f8),
    MMXDATA_INIT(.mmx_mask_0,			0xffffffffffff0000),
    MMXDATA_INIT(.mmx_mask_1,			0xffffffff0000ffff),
    MMXDATA_INIT(.mmx_mask_2,			0xffff0000ffffffff),
    MMXDATA_INIT(.mmx_mask_3,			0x0000ffffffffffff),
    MMXDATA_INIT(.mmx_full_alpha,		0x00ff000000000000),
    MMXDATA_INIT(.mmx_ffff0000ffff0000,		0xffff0000ffff0000),
    MMXDATA_INIT(.mmx_0000ffff00000000,		0x0000ffff00000000),
    MMXDATA_INIT(.mmx_000000000000ffff,		0x000000000000ffff),
d132 1
a132 1
#        define MC(x)  M64(c.mmx_##x)
d134 1
a134 1
#        define MC(x) ((__m64)c.mmx_##x)
d137 1
a137 1
#    define MC(x) c.mmx_##x
d145 1
a145 1
#elif defined M64_MEMBER	/* __m64 is a struct, not an integral type */
d150 1
a150 1
#else				/* __m64 is an integral type */
d160 1
a160 1
#elif defined M64_MEMBER	/* __m64 is a struct, not an integral type */
d163 1
a163 1
#else				/* __m64 is an integral type */
d169 2
a170 1
shift (__m64 v, int s)
d183 1
a183 1
    return _mm_xor_si64 (mask, MC(4x00ff));
d192 1
a192 1
    res = _mm_adds_pu16 (res, MC(4x0080));
d202 1
a202 1
    return  _mm_adds_pu8 (a, b);
d243 3
a245 3
    x = _mm_and_si64 (x, MC(ffff0000ffff0000));
    y = _mm_and_si64 (y, MC(000000000000ffff));
    z = _mm_and_si64 (z, MC(0000ffff00000000));
d257 3
a259 1
over (__m64 src, __m64 srca, __m64 dest)
d261 1
a261 1
    return  _mm_adds_pu8 (src, pix_multiply(dest, negate(srca)));
d268 1
a268 1
    __m64 srcfaaa = _mm_or_si64 (srca, MC(full_alpha));
d270 1
a270 1
    return over(pix_multiply(invert_colors(src), srcfaaa), srca, dest);
d274 1
a274 2
in (__m64 src,
    __m64 mask)
d282 1
a282 1
    src = _mm_or_si64 (src, MC(full_alpha));
d284 1
a284 1
    return over(in (src, mask), mask, dest);
d289 1
a289 4
in_over (__m64 src,
	 __m64 srca,
	 __m64 mask,
	 __m64 dest)
d291 1
a291 1
    return over(in(src, mask), pix_multiply(srca, mask), dest);
d293 1
d295 4
a298 1
#define in_over(src, srca, mask, dest) over(in(src, mask), pix_multiply(srca, mask), dest)
d304 1
a304 1
    return _mm_unpacklo_pi8 (_mm_cvtsi32_si64 (v), _mm_setzero_si64());
d316 1
a316 1
    return _mm_cvtsi64_si32(pack8888(v, _mm_setzero_si64()));
d347 1
a347 1
    p = _mm_and_si64 (p, MC(565_rgb));
d349 1
a349 1
    pixel = _mm_mullo_pi16 (p, MC(565_unpack_multiplier));
d357 1
a357 1
	return _mm_unpacklo_pi8 (in, _mm_setzero_si64());
d359 1
a359 1
	return _mm_unpackhi_pi8 (in, _mm_setzero_si64());
d365 1
a365 1
    return _mm_or_si64 (expand8888 (in, pos), MC(full_alpha));
d369 1
a369 1
pack565 (__m64 pixel, __m64 target, int pos)
d375 7
a381 7
    r = _mm_and_si64 (p, MC(565_r));
    g = _mm_and_si64 (p, MC(565_g));
    b = _mm_and_si64 (p, MC(565_b));

    r = shift (r, - (32 - 8) + pos * 16);
    g = shift (g, - (16 - 3) + pos * 16);
    b = shift (b, - (0  + 3) + pos * 16);
d384 1
a384 1
	t = _mm_and_si64 (t, MC(mask_0));
d386 1
a386 1
	t = _mm_and_si64 (t, MC(mask_1));
d388 1
a388 1
	t = _mm_and_si64 (t, MC(mask_2));
d390 1
a390 1
	t = _mm_and_si64 (t, MC(mask_3));
d399 1
d403 2
a404 6
    x = _mm_mullo_pi16 (x, a);
    y = _mm_mullo_pi16 (y, b);
    x = _mm_adds_pu16 (x, MC(4x0080));
    x = _mm_adds_pu16 (x, y);
    x = _mm_adds_pu16 (x, _mm_srli_pi16 (x, 8));
    x = _mm_srli_pi16 (x, 8);
d406 1
a406 1
    return x;
d408 1
d410 6
a415 7
#define pix_add_mul(x, a, y, b) \
( x = _mm_mullo_pi16 (x, a), \
  y = _mm_mullo_pi16 (y, b), \
  x = _mm_adds_pu16 (x, MC(4x0080)), \
  x = _mm_adds_pu16 (x, y), \
  x = _mm_adds_pu16 (x, _mm_srli_pi16 (x, 8)), \
  _mm_srli_pi16 (x, 8) )
d439 7
a445 3
static FASTCALL void
mmxCombineOverU (pixman_implementation_t *imp, pixman_op_t op,
		 uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d449 2
a450 1
    while (dest < end) {
d453 3
a455 1
	if (a == 0xff) {
d457 3
a459 1
	} else if (ssrc) {
d461 3
a463 3
	    s = load8888(ssrc);
	    sa = expand_alpha(s);
	    *dest = store8888(over(s, sa, load8888(*dest)));
d465 1
d471 1
a471 1
    _mm_empty();
d474 7
a480 3
static FASTCALL void
mmxCombineOverReverseU (pixman_implementation_t *imp, pixman_op_t op,
			uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d484 2
a485 1
    while (dest < end) {
d488 7
a494 5
	d = load8888(*dest);
	da = expand_alpha(d);
	*dest = store8888(over (d, da, load8888(s)));
        ++dest;
        ++src;
d498 1
a498 1
    _mm_empty();
d501 7
a507 3
static FASTCALL void
mmxCombineInU (pixman_implementation_t *imp, pixman_op_t op,
	       uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d511 13
a523 9
    while (dest < end) {
        __m64 x, a;
        x = load8888 (combine (src, mask));
        a = load8888(*dest);
        a = expand_alpha(a);
        x = pix_multiply(x, a);
        *dest = store8888(x);
        ++dest;
        ++src;
d527 1
a527 1
    _mm_empty();
d530 7
a536 3
static FASTCALL void
mmxCombineInReverseU (pixman_implementation_t *imp, pixman_op_t op,
		      uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d540 12
a551 9
    while (dest < end) {
        __m64 x, a;
        x = load8888(*dest);
        a = load8888(combine (src, mask));
        a = expand_alpha(a);
        x = pix_multiply(x, a);
        *dest = store8888(x);
        ++dest;
        ++src;
d555 1
a555 1
    _mm_empty();
d558 7
a564 3
static FASTCALL void
mmxCombineOutU (pixman_implementation_t *imp, pixman_op_t op,
		uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d568 13
a580 10
    while (dest < end) {
        __m64 x, a;
        x = load8888(combine (src, mask));
        a = load8888(*dest);
        a = expand_alpha(a);
        a = negate(a);
        x = pix_multiply(x, a);
        *dest = store8888(x);
        ++dest;
        ++src;
d584 1
a584 1
    _mm_empty();
d587 7
a593 3
static FASTCALL void
mmxCombineOutReverseU (pixman_implementation_t *imp, pixman_op_t op,
		       uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d597 14
a610 10
    while (dest < end) {
        __m64 x, a;
        x = load8888(*dest);
        a = load8888(combine (src, mask));
        a = expand_alpha(a);
        a = negate(a);
        x = pix_multiply(x, a);
        *dest = store8888(x);
        ++dest;
        ++src;
d614 1
a614 1
    _mm_empty();
d617 7
a623 3
static FASTCALL void
mmxCombineAtopU (pixman_implementation_t *imp, pixman_op_t op,
		 uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d627 14
a640 11
    while (dest < end) {
        __m64 s, da, d, sia;
        s = load8888(combine (src, mask));
        d = load8888(*dest);
        sia = expand_alpha(s);
        sia = negate(sia);
        da = expand_alpha(d);
        s = pix_add_mul (s, da, d, sia);
        *dest = store8888(s);
        ++dest;
        ++src;
d644 1
a644 1
    _mm_empty();
d647 7
a653 3
static FASTCALL void
mmxCombineAtopReverseU (pixman_implementation_t *imp, pixman_op_t op,
			uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d659 9
a667 7
    while (dest < end) {
        __m64 s, dia, d, sa;
        s = load8888(combine(src, mask));
        d = load8888(*dest);
        sa = expand_alpha(s);
        dia = expand_alpha(d);
        dia = negate(dia);
d669 4
a672 3
        *dest = store8888(s);
        ++dest;
        ++src;
d676 1
a676 1
    _mm_empty();
d679 7
a685 3
static FASTCALL void
mmxCombineXorU (pixman_implementation_t *imp, pixman_op_t op,
		uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d689 10
a698 8
    while (dest < end) {
        __m64 s, dia, d, sia;
        s = load8888(combine(src, mask));
        d = load8888(*dest);
        sia = expand_alpha(s);
        dia = expand_alpha(d);
        sia = negate(sia);
        dia = negate(dia);
d700 4
a703 3
        *dest = store8888(s);
        ++dest;
        ++src;
d707 1
a707 1
    _mm_empty();
d710 7
a716 3
static FASTCALL void
mmxCombineAddU (pixman_implementation_t *imp, pixman_op_t op,
		uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d719 12
a730 8
    while (dest < end) {
        __m64 s, d;
        s = load8888(combine(src,mask));
        d = load8888(*dest);
        s = pix_add(s, d);
        *dest = store8888(s);
        ++dest;
        ++src;
d734 1
a734 1
    _mm_empty();
d737 7
a743 3
static FASTCALL void
mmxCombineSaturateU (pixman_implementation_t *imp, pixman_op_t op,
		     uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d746 22
a767 17
    while (dest < end) {
        uint32_t s = combine(src,mask);
        uint32_t d = *dest;
        __m64 ms = load8888(s);
        __m64 md = load8888(d);
        uint32_t sa = s >> 24;
        uint32_t da = ~d >> 24;

        if (sa > da) {
            __m64 msa = load8888(FbIntDiv(da, sa) << 24);
            msa = expand_alpha(msa);
            ms = pix_multiply(ms, msa);
        }
        md = pix_add(md, ms);
        *dest = store8888(md);
        ++src;
        ++dest;
d771 1
a771 1
    _mm_empty();
d774 7
a780 4

static FASTCALL void
mmxCombineSrcC (pixman_implementation_t *imp, pixman_op_t op,
		uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d783 14
a796 10
    while (src < end) {
        __m64 a = load8888(*mask);
        __m64 s = load8888(*src);
        s = pix_multiply(s, a);
        *dest = store8888(s);
        ++src;
        ++mask;
        ++dest;
    }
    _mm_empty();
d799 7
a805 3
static FASTCALL void
mmxCombineOverC (pixman_implementation_t *imp, pixman_op_t op,
		 uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
a807 5
    while (src < end) {
        __m64 a = load8888(*mask);
        __m64 s = load8888(*src);
        __m64 d = load8888(*dest);
        __m64 sa = expand_alpha(s);
d809 8
a816 1
	*dest = store8888(in_over (s, sa, a, d));
d818 3
a820 3
        ++src;
        ++dest;
        ++mask;
d822 1
a822 1
    _mm_empty();
d825 7
a831 3
static FASTCALL void
mmxCombineOverReverseC (pixman_implementation_t *imp, pixman_op_t op,
			uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
a833 5
    while (src < end) {
        __m64 a = load8888(*mask);
        __m64 s = load8888(*src);
        __m64 d = load8888(*dest);
        __m64 da = expand_alpha(d);
d835 8
a842 1
	*dest = store8888(over (d, da, in (s, a)));
d844 3
a846 3
        ++src;
        ++dest;
        ++mask;
d848 1
a848 1
    _mm_empty();
d851 7
a857 4

static FASTCALL void
mmxCombineInC (pixman_implementation_t *imp, pixman_op_t op,
	       uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d860 26
a885 18
    while (src < end) {
        __m64 a = load8888(*mask);
        __m64 s = load8888(*src);
        __m64 d = load8888(*dest);
        __m64 da = expand_alpha(d);
        s = pix_multiply(s, a);
        s = pix_multiply(s, da);
        *dest = store8888(s);
        ++src;
        ++dest;
        ++mask;
    }
    _mm_empty();
}

static FASTCALL void
mmxCombineInReverseC (pixman_implementation_t *imp, pixman_op_t op,
		      uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d888 26
a913 18
    while (src < end) {
        __m64 a = load8888(*mask);
        __m64 s = load8888(*src);
        __m64 d = load8888(*dest);
        __m64 sa = expand_alpha(s);
        a = pix_multiply(a, sa);
        d = pix_multiply(d, a);
        *dest = store8888(d);
        ++src;
        ++dest;
        ++mask;
    }
    _mm_empty();
}

static FASTCALL void
mmxCombineOutC (pixman_implementation_t *imp, pixman_op_t op,
		uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d916 27
a942 19
    while (src < end) {
        __m64 a = load8888(*mask);
        __m64 s = load8888(*src);
        __m64 d = load8888(*dest);
        __m64 da = expand_alpha(d);
        da = negate(da);
        s = pix_multiply(s, a);
        s = pix_multiply(s, da);
        *dest = store8888(s);
        ++src;
        ++dest;
        ++mask;
    }
    _mm_empty();
}

static FASTCALL void
mmxCombineOutReverseC (pixman_implementation_t *imp, pixman_op_t op,
		       uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d945 27
a971 19
    while (src < end) {
        __m64 a = load8888(*mask);
        __m64 s = load8888(*src);
        __m64 d = load8888(*dest);
        __m64 sa = expand_alpha(s);
        a = pix_multiply(a, sa);
        a = negate(a);
        d = pix_multiply(d, a);
        *dest = store8888(d);
        ++src;
        ++dest;
        ++mask;
    }
    _mm_empty();
}

static FASTCALL void
mmxCombineAtopC (pixman_implementation_t *imp, pixman_op_t op,
		 uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d974 12
a985 9
    while (src < end) {
        __m64 a = load8888(*mask);
        __m64 s = load8888(*src);
        __m64 d = load8888(*dest);
        __m64 da = expand_alpha(d);
        __m64 sa = expand_alpha(s);
        s = pix_multiply(s, a);
        a = pix_multiply(a, sa);
        a = negate(a);
d987 5
a991 4
        *dest = store8888(d);
        ++src;
        ++dest;
        ++mask;
d993 1
a993 1
    _mm_empty();
d996 7
a1002 3
static FASTCALL void
mmxCombineAtopReverseC (pixman_implementation_t *imp, pixman_op_t op,
			uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d1005 12
a1016 9
    while (src < end) {
        __m64 a = load8888(*mask);
        __m64 s = load8888(*src);
        __m64 d = load8888(*dest);
        __m64 da = expand_alpha(d);
        __m64 sa = expand_alpha(s);
        s = pix_multiply(s, a);
        a = pix_multiply(a, sa);
        da = negate(da);
d1018 5
a1022 4
        *dest = store8888(d);
        ++src;
        ++dest;
        ++mask;
d1024 1
a1024 1
    _mm_empty();
d1027 7
a1033 3
static FASTCALL void
mmxCombineXorC (pixman_implementation_t *imp, pixman_op_t op,
		uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d1036 13
a1048 10
    while (src < end) {
        __m64 a = load8888(*mask);
        __m64 s = load8888(*src);
        __m64 d = load8888(*dest);
        __m64 da = expand_alpha(d);
        __m64 sa = expand_alpha(s);
        s = pix_multiply(s, a);
        a = pix_multiply(a, sa);
        da = negate(da);
        a = negate(a);
d1050 5
a1054 4
        *dest = store8888(d);
        ++src;
        ++dest;
        ++mask;
d1056 1
a1056 1
    _mm_empty();
d1059 7
a1065 3
static FASTCALL void
mmxCombineAddC (pixman_implementation_t *imp, pixman_op_t op,
		uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
a1067 36
    while (src < end) {
        __m64 a = load8888(*mask);
        __m64 s = load8888(*src);
        __m64 d = load8888(*dest);
        s = pix_multiply(s, a);
        d = pix_add(s, d);
        *dest = store8888(d);
        ++src;
        ++dest;
        ++mask;
    }
    _mm_empty();
}

/* ------------------ MMX code paths called from fbpict.c ----------------------- */

static void
fbCompositeSolid_nx8888mmx (pixman_implementation_t *imp,
			    pixman_op_t op,
			    pixman_image_t * pSrc,
			    pixman_image_t * pMask,
			    pixman_image_t * pDst,
			    int32_t	xSrc,
			    int32_t	ySrc,
			    int32_t	xMask,
			    int32_t	yMask,
			    int32_t	xDst,
			    int32_t	yDst,
			    int32_t	width,
			    int32_t	height)
{
    uint32_t	src;
    uint32_t	*dstLine, *dst;
    uint16_t	w;
    int	dstStride;
    __m64	vsrc, vsrca;
d1069 18
a1086 1
    CHECKPOINT();
d1088 22
a1109 1
    fbComposeGetSolid(pSrc, src, pDst->bits.format);
d1111 3
a1113 1
    if (src >> 24 == 0)
d1116 1
a1116 1
    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);
d1123 2
a1124 2
	dst = dstLine;
	dstLine += dstStride;
d1127 1
a1127 1
	CHECKPOINT();
d1131 1
a1131 1
	    *dst = store8888(over(vsrc, vsrca, load8888(*dst)));
d1144 2
a1145 2
	    dest0 = over(vsrc, vsrca, expand8888(vdest, 0));
	    dest1 = over(vsrc, vsrca, expand8888(vdest, 1));
d1147 1
a1147 1
	    *(__m64 *)dst = pack8888(dest0, dest1);
d1153 1
a1153 1
	CHECKPOINT();
d1157 1
a1157 1
	    *dst = store8888(over(vsrc, vsrca, load8888(*dst)));
d1164 1
a1164 1
    _mm_empty();
d1168 13
a1180 13
fbCompositeSolid_nx0565mmx (pixman_implementation_t *imp,
			    pixman_op_t op,
			    pixman_image_t * pSrc,
			    pixman_image_t * pMask,
			    pixman_image_t * pDst,
			    int32_t	xSrc,
			    int32_t	ySrc,
			    int32_t	xMask,
			    int32_t	yMask,
			    int32_t	xDst,
			    int32_t	yDst,
			    int32_t	width,
			    int32_t	height)
d1182 5
a1186 5
    uint32_t	src;
    uint16_t	*dstLine, *dst;
    uint16_t	w;
    int	dstStride;
    __m64	vsrc, vsrca;
d1188 1
a1188 1
    CHECKPOINT();
d1190 1
a1190 1
    fbComposeGetSolid(pSrc, src, pDst->bits.format);
d1192 1
a1192 1
    if (src >> 24 == 0)
d1195 1
a1195 1
    fbComposeGetStart (pDst, xDst, yDst, uint16_t, dstStride, dstLine, 1);
d1202 2
a1203 2
	dst = dstLine;
	dstLine += dstStride;
d1206 1
a1206 1
	CHECKPOINT();
d1211 4
a1214 3
	    __m64 vdest = expand565 (M64(d), 0);
	    vdest = pack565(over(vsrc, vsrca, vdest), vdest, 0);
	    *dst = UINT64(vdest);
d1226 4
a1229 4
	    vdest = pack565 (over(vsrc, vsrca, expand565(vdest, 0)), vdest, 0);
	    vdest = pack565 (over(vsrc, vsrca, expand565(vdest, 1)), vdest, 1);
	    vdest = pack565 (over(vsrc, vsrca, expand565(vdest, 2)), vdest, 2);
	    vdest = pack565 (over(vsrc, vsrca, expand565(vdest, 3)), vdest, 3);
d1237 1
a1237 1
	CHECKPOINT();
d1242 4
a1245 3
	    __m64 vdest = expand565 (M64(d), 0);
	    vdest = pack565(over(vsrc, vsrca, vdest), vdest, 0);
	    *dst = UINT64(vdest);
d1252 1
a1252 1
    _mm_empty();
d1256 19
a1274 19
fbCompositeSolidMask_nx8888x8888Cmmx (pixman_implementation_t *imp,
				      pixman_op_t op,
				      pixman_image_t * pSrc,
				      pixman_image_t * pMask,
				      pixman_image_t * pDst,
				      int32_t	xSrc,
				      int32_t	ySrc,
				      int32_t	xMask,
				      int32_t	yMask,
				      int32_t	xDst,
				      int32_t	yDst,
				      int32_t	width,
				      int32_t	height)
{
    uint32_t	src, srca;
    uint32_t	*dstLine;
    uint32_t	*maskLine;
    int	dstStride, maskStride;
    __m64	vsrc, vsrca;
d1276 1
a1276 1
    CHECKPOINT();
d1278 1
a1278 1
    fbComposeGetSolid(pSrc, src, pDst->bits.format);
d1281 1
a1281 1
    if (srca == 0)
d1284 2
a1285 2
    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);
    fbComposeGetStart (pMask, xMask, yMask, uint32_t, maskStride, maskLine, 1);
d1287 2
a1288 2
    vsrc = load8888(src);
    vsrca = expand_alpha(vsrc);
d1293 2
a1294 2
	uint32_t *p = (uint32_t *)maskLine;
	uint32_t *q = (uint32_t *)dstLine;
d1302 3
a1304 3
		__m64 vdest = load8888(*q);
		vdest = in_over(vsrc, vsrca, load8888(m), vdest);
		*q = store8888(vdest);
d1323 4
a1326 4
		dest0 = in_over(vsrc, vsrca, load8888(m0),
				expand8888 (vdest, 0));
		dest1 = in_over(vsrc, vsrca, load8888(m1),
				expand8888 (vdest, 1));
d1328 1
a1328 1
		*(__m64 *)q = pack8888(dest0, dest1);
d1342 3
a1344 3
		__m64 vdest = load8888(*q);
		vdest = in_over(vsrc, vsrca, load8888(m), vdest);
		*q = store8888(vdest);
d1352 2
a1353 2
	dstLine += dstStride;
	maskLine += maskStride;
d1356 1
a1356 1
    _mm_empty();
d1360 21
a1380 21
fbCompositeSrc_8888x8x8888mmx (pixman_implementation_t *imp,
			       pixman_op_t op,
			       pixman_image_t * pSrc,
			       pixman_image_t * pMask,
			       pixman_image_t * pDst,
			       int32_t	xSrc,
			       int32_t	ySrc,
			       int32_t      xMask,
			       int32_t      yMask,
			       int32_t      xDst,
			       int32_t      yDst,
			       int32_t     width,
			       int32_t     height)
{
    uint32_t	*dstLine, *dst;
    uint32_t	*srcLine, *src;
    uint32_t	mask;
    __m64	vmask;
    int	dstStride, srcStride;
    uint16_t	w;
    __m64  srca;
d1382 1
a1382 1
    CHECKPOINT();
d1384 2
a1385 2
    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);
    fbComposeGetStart (pSrc, xSrc, ySrc, uint32_t, srcStride, srcLine, 1);
d1387 1
a1387 1
    fbComposeGetSolid (pMask, mask, pDst->bits.format);
d1390 1
a1390 1
    srca = MC(4x00ff);
d1394 4
a1397 4
	dst = dstLine;
	dstLine += dstStride;
	src = srcLine;
	srcLine += srcStride;
d1420 2
a1421 2
		in_over (vsrc0, expand_alpha (vsrc0), vmask, expand8888 (vd, 0)),
		in_over (vsrc1, expand_alpha (vsrc1), vmask, expand8888 (vd, 1)));
d1441 1
a1441 1
    _mm_empty();
d1445 27
a1471 27
fbCompositeSrc_x888xnx8888mmx (pixman_implementation_t *imp,
			       pixman_op_t op,
			       pixman_image_t * pSrc,
			       pixman_image_t * pMask,
			       pixman_image_t * pDst,
			       int32_t	xSrc,
			       int32_t	ySrc,
			       int32_t      xMask,
			       int32_t      yMask,
			       int32_t      xDst,
			       int32_t      yDst,
			       int32_t     width,
			       int32_t     height)
{
    uint32_t	*dstLine, *dst;
    uint32_t	*srcLine, *src;
    uint32_t	mask;
    __m64	vmask;
    int	dstStride, srcStride;
    uint16_t	w;
    __m64  srca;

    CHECKPOINT();

    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);
    fbComposeGetStart (pSrc, xSrc, ySrc, uint32_t, srcStride, srcLine, 1);
    fbComposeGetSolid (pMask, mask, pDst->bits.format);
d1475 1
a1475 1
    srca = MC(4x00ff);
d1479 4
a1482 4
	dst = dstLine;
	dstLine += dstStride;
	src = srcLine;
	srcLine += srcStride;
d1518 2
a1519 2
		in_over (expandx888 (vs0, 0), srca, vmask, expand8888 (vd0, 0)),
		in_over (expandx888 (vs0, 1), srca, vmask, expand8888 (vd0, 1)));
d1522 2
a1523 2
		in_over (expandx888 (vs1, 0), srca, vmask, expand8888 (vd1, 0)),
		in_over (expandx888 (vs1, 1), srca, vmask, expand8888 (vd1, 1)));
d1526 2
a1527 2
		in_over (expandx888 (vs2, 0), srca, vmask, expand8888 (vd2, 0)),
		in_over (expandx888 (vs2, 1), srca, vmask, expand8888 (vd2, 1)));
d1530 2
a1531 2
		in_over (expandx888 (vs3, 0), srca, vmask, expand8888 (vd3, 0)),
		in_over (expandx888 (vs3, 1), srca, vmask, expand8888 (vd3, 1)));
d1534 2
a1535 2
		in_over (expandx888 (vs4, 0), srca, vmask, expand8888 (vd4, 0)),
		in_over (expandx888 (vs4, 1), srca, vmask, expand8888 (vd4, 1)));
d1538 2
a1539 2
		in_over (expandx888 (vs5, 0), srca, vmask, expand8888 (vd5, 0)),
		in_over (expandx888 (vs5, 1), srca, vmask, expand8888 (vd5, 1)));
d1541 3
a1543 3
            vd6 = pack8888 (
		in_over (expandx888 (vs6, 0), srca, vmask, expand8888 (vd6, 0)),
		in_over (expandx888 (vs6, 1), srca, vmask, expand8888 (vd6, 1)));
d1546 2
a1547 2
		in_over (expandx888 (vs7, 0), srca, vmask, expand8888 (vd7, 0)),
		in_over (expandx888 (vs7, 1), srca, vmask, expand8888 (vd7, 1)));
d1576 1
a1576 1
    _mm_empty();
d1580 25
a1604 25
fbCompositeSrc_8888x8888mmx (pixman_implementation_t *imp,
			     pixman_op_t op,
			     pixman_image_t * pSrc,
			     pixman_image_t * pMask,
			     pixman_image_t * pDst,
			     int32_t	xSrc,
			     int32_t	ySrc,
			     int32_t      xMask,
			     int32_t      yMask,
			     int32_t      xDst,
			     int32_t      yDst,
			     int32_t     width,
			     int32_t     height)
{
    uint32_t	*dstLine, *dst;
    uint32_t	*srcLine, *src;
    uint32_t    s;
    int	dstStride, srcStride;
    uint8_t     a;
    uint16_t	w;

    CHECKPOINT();

    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);
    fbComposeGetStart (pSrc, xSrc, ySrc, uint32_t, srcStride, srcLine, 1);
d1608 4
a1611 4
	dst = dstLine;
	dstLine += dstStride;
	src = srcLine;
	srcLine += srcStride;
d1618 1
d1620 1
d1622 3
a1624 1
	    else if (s) {
d1626 3
a1628 3
		ms = load8888(s);
		sa = expand_alpha(ms);
		*dst = store8888(over(ms, sa, load8888(*dst)));
d1630 1
d1634 1
a1634 1
    _mm_empty();
d1638 18
a1655 18
fbCompositeSrc_8888x0565mmx (pixman_implementation_t *imp,
			     pixman_op_t op,
			     pixman_image_t * pSrc,
			     pixman_image_t * pMask,
			     pixman_image_t * pDst,
			     int32_t      xSrc,
			     int32_t      ySrc,
			     int32_t      xMask,
			     int32_t      yMask,
			     int32_t      xDst,
			     int32_t      yDst,
			     int32_t     width,
			     int32_t     height)
{
    uint16_t	*dstLine, *dst;
    uint32_t	*srcLine, *src;
    int	dstStride, srcStride;
    uint16_t	w;
d1657 1
a1657 1
    CHECKPOINT();
d1659 2
a1660 2
    fbComposeGetStart (pDst, xDst, yDst, uint16_t, dstStride, dstLine, 1);
    fbComposeGetStart (pSrc, xSrc, ySrc, uint32_t, srcStride, srcLine, 1);
d1664 1
a1664 1
    assert (pSrc->pDrawable == pMask->pDrawable);
d1669 4
a1672 4
	dst = dstLine;
	dstLine += dstStride;
	src = srcLine;
	srcLine += srcStride;
d1675 1
a1675 1
	CHECKPOINT();
d1681 1
a1681 1
	    __m64 vdest = expand565 (M64(d), 0);
d1683 2
a1684 1
	    vdest = pack565(over(vsrc, expand_alpha(vsrc), vdest), vdest, 0);
d1686 1
a1686 1
	    *dst = UINT64(vdest);
d1693 1
a1693 1
	CHECKPOINT();
d1700 4
a1703 4
	    vsrc0 = load8888(*(src + 0));
	    vsrc1 = load8888(*(src + 1));
	    vsrc2 = load8888(*(src + 2));
	    vsrc3 = load8888(*(src + 3));
d1707 4
a1710 4
	    vdest = pack565(over(vsrc0, expand_alpha(vsrc0), expand565(vdest, 0)), vdest, 0);
	    vdest = pack565(over(vsrc1, expand_alpha(vsrc1), expand565(vdest, 1)), vdest, 1);
	    vdest = pack565(over(vsrc2, expand_alpha(vsrc2), expand565(vdest, 2)), vdest, 2);
	    vdest = pack565(over(vsrc3, expand_alpha(vsrc3), expand565(vdest, 3)), vdest, 3);
d1719 1
a1719 1
	CHECKPOINT();
d1725 1
a1725 1
	    __m64 vdest = expand565 (M64(d), 0);
d1727 1
a1727 1
	    vdest = pack565(over(vsrc, expand_alpha(vsrc), vdest), vdest, 0);
d1729 1
a1729 1
	    *dst = UINT64(vdest);
d1737 1
a1737 1
    _mm_empty();
d1741 21
a1761 21
fbCompositeSolidMask_nx8x8888mmx (pixman_implementation_t *imp,
				  pixman_op_t op,
				  pixman_image_t * pSrc,
				  pixman_image_t * pMask,
				  pixman_image_t * pDst,
				  int32_t      xSrc,
				  int32_t      ySrc,
				  int32_t      xMask,
				  int32_t      yMask,
				  int32_t      xDst,
				  int32_t      yDst,
				  int32_t     width,
				  int32_t     height)
{
    uint32_t	src, srca;
    uint32_t	*dstLine, *dst;
    uint8_t	*maskLine, *mask;
    int	dstStride, maskStride;
    uint16_t	w;
    __m64	vsrc, vsrca;
    uint64_t	srcsrc;
d1763 1
a1763 1
    CHECKPOINT();
d1765 1
a1765 1
    fbComposeGetSolid(pSrc, src, pDst->bits.format);
d1768 1
a1768 1
    if (srca == 0)
d1773 2
a1774 2
    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);
    fbComposeGetStart (pMask, xMask, yMask, uint8_t, maskStride, maskLine, 1);
d1781 4
a1784 4
	dst = dstLine;
	dstLine += dstStride;
	mask = maskLine;
	maskLine += maskStride;
d1787 1
a1787 1
	CHECKPOINT();
d1795 5
a1799 2
		__m64 vdest = in_over(vsrc, vsrca, expand_alpha_rev (M64(m)), load8888(*dst));
		*dst = store8888(vdest);
d1807 1
a1807 1
	CHECKPOINT();
d1812 1
d1827 4
a1830 2
		dest0 = in_over(vsrc, vsrca, expand_alpha_rev (M64(m0)), expand8888(vdest, 0));
		dest1 = in_over(vsrc, vsrca, expand_alpha_rev (M64(m1)), expand8888(vdest, 1));
d1832 1
a1832 1
		*(__m64 *)dst = pack8888(dest0, dest1);
d1840 1
a1840 1
	CHECKPOINT();
d1848 5
a1852 3
		__m64 vdest = load8888(*dst);
		vdest = in_over(vsrc, vsrca, expand_alpha_rev (M64(m)), vdest);
		*dst = store8888(vdest);
d1861 1
a1861 1
    _mm_empty();
d1866 13
a1878 12
		 int stride,
		 int bpp,
		 int x,
		 int y,
		 int width,
		 int height,
		 uint32_t xor)
{
    uint64_t	fill;
    __m64	vfill;
    uint32_t	byte_width;
    uint8_t	*byte_line;
d1880 1
a1880 1
    __m64	v1, v2, v3, v4, v5, v6, v7;
d1890 2
a1891 2
	((xor >> 16 != (xor & 0xffff)) ||
	 (xor >> 24 != (xor & 0x00ff) >> 16)))
d1895 1
a1895 1
    
d1919 1
a1919 1
    vfill = M64(fill);
d1923 7
a1929 7
	"movq		%7,	%0\n"
	"movq		%7,	%1\n"
	"movq		%7,	%2\n"
	"movq		%7,	%3\n"
	"movq		%7,	%4\n"
	"movq		%7,	%5\n"
	"movq		%7,	%6\n"
d1931 1
a1931 1
	  "=y" (v4), "=y" (v5), "=y" (v6), "=y" (v7)
d1939 1
d1949 1
a1949 1
	
d1969 8
a1976 8
		"movq	%1,	  (%0)\n"
		"movq	%2,	 8(%0)\n"
		"movq	%3,	16(%0)\n"
		"movq	%4,	24(%0)\n"
		"movq	%5,	32(%0)\n"
		"movq	%6,	40(%0)\n"
		"movq	%7,	48(%0)\n"
		"movq	%8,	56(%0)\n"
d1979 2
a1980 2
		  "y" (vfill), "y" (v1), "y" (v2), "y" (v3),
		  "y" (v4), "y" (v5), "y" (v6), "y" (v7)
d2015 1
a2015 1
	
d2018 1
a2018 1
    _mm_empty();
d2023 21
a2043 21
fbCompositeSolidMaskSrc_nx8x8888mmx (pixman_implementation_t *imp,
				     pixman_op_t op,
				     pixman_image_t * pSrc,
				     pixman_image_t * pMask,
				     pixman_image_t * pDst,
				     int32_t      xSrc,
				     int32_t      ySrc,
				     int32_t      xMask,
				     int32_t      yMask,
				     int32_t      xDst,
				     int32_t      yDst,
				     int32_t     width,
				     int32_t     height)
{
    uint32_t	src, srca;
    uint32_t	*dstLine, *dst;
    uint8_t	*maskLine, *mask;
    int	dstStride, maskStride;
    uint16_t	w;
    __m64	vsrc, vsrca;
    uint64_t	srcsrc;
d2045 1
a2045 1
    CHECKPOINT();
d2047 1
a2047 1
    fbComposeGetSolid(pSrc, src, pDst->bits.format);
d2050 1
a2050 1
    if (srca == 0)
d2052 3
a2054 2
	pixman_fill_mmx (pDst->bits.bits, pDst->bits.rowstride, PIXMAN_FORMAT_BPP (pDst->bits.format),
			 xDst, yDst, width, height, 0);
d2060 2
a2061 2
    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);
    fbComposeGetStart (pMask, xMask, yMask, uint8_t, maskStride, maskLine, 1);
d2068 4
a2071 4
	dst = dstLine;
	dstLine += dstStride;
	mask = maskLine;
	maskLine += maskStride;
d2074 1
a2074 1
	CHECKPOINT();
d2082 3
a2084 2
		__m64 vdest = in(vsrc, expand_alpha_rev (M64(m)));
		*dst = store8888(vdest);
d2096 1
a2096 1
	CHECKPOINT();
d2115 2
a2116 2
		dest0 = in(vsrc, expand_alpha_rev (M64(m0)));
		dest1 = in(vsrc, expand_alpha_rev (M64(m1)));
d2118 1
a2118 1
		*(__m64 *)dst = pack8888(dest0, dest1);
d2130 1
a2130 1
	CHECKPOINT();
d2138 4
a2141 3
		__m64 vdest = load8888(*dst);
		vdest = in(vsrc, expand_alpha_rev (M64(m)));
		*dst = store8888(vdest);
d2154 1
a2154 1
    _mm_empty();
d2158 20
a2177 20
fbCompositeSolidMask_nx8x0565mmx (pixman_implementation_t *imp,
				  pixman_op_t op,
				  pixman_image_t * pSrc,
				  pixman_image_t * pMask,
				  pixman_image_t * pDst,
				  int32_t      xSrc,
				  int32_t      ySrc,
				  int32_t      xMask,
				  int32_t      yMask,
				  int32_t      xDst,
				  int32_t      yDst,
				  int32_t     width,
				  int32_t     height)
{
    uint32_t	src, srca;
    uint16_t	*dstLine, *dst;
    uint8_t	*maskLine, *mask;
    int	dstStride, maskStride;
    uint16_t	w;
    __m64	vsrc, vsrca, tmp;
d2180 1
a2180 1
    CHECKPOINT();
d2182 1
a2182 1
    fbComposeGetSolid(pSrc, src, pDst->bits.format);
d2185 1
a2185 1
    if (srca == 0)
d2188 2
a2189 2
    fbComposeGetStart (pDst, xDst, yDst, uint16_t, dstStride, dstLine, 1);
    fbComposeGetStart (pMask, xMask, yMask, uint8_t, maskStride, maskLine, 1);
d2194 2
a2195 2
    tmp = pack565(vsrc, _mm_setzero_si64(), 0);
    src16 = UINT64(tmp);
d2197 2
a2198 1
    srcsrcsrcsrc = (uint64_t)src16 << 48 | (uint64_t)src16 << 32 |
d2203 4
a2206 4
	dst = dstLine;
	dstLine += dstStride;
	mask = maskLine;
	maskLine += maskStride;
d2209 1
a2209 1
	CHECKPOINT();
d2218 6
a2223 4
		__m64 vd = M64(d);
		__m64 vdest = in_over(vsrc, vsrca, expand_alpha_rev (M64 (m)), expand565(vd, 0));
		vd = pack565(vdest, _mm_setzero_si64(), 0);
		*dst = UINT64(vd);
d2231 1
a2231 1
	CHECKPOINT();
d2252 12
a2263 8
		vm0 = M64(m0);
		vdest = pack565(in_over(vsrc, vsrca, expand_alpha_rev(vm0), expand565(vdest, 0)), vdest, 0);
		vm1 = M64(m1);
		vdest = pack565(in_over(vsrc, vsrca, expand_alpha_rev(vm1), expand565(vdest, 1)), vdest, 1);
		vm2 = M64(m2);
		vdest = pack565(in_over(vsrc, vsrca, expand_alpha_rev(vm2), expand565(vdest, 2)), vdest, 2);
		vm3 = M64(m3);
		vdest = pack565(in_over(vsrc, vsrca, expand_alpha_rev(vm3), expand565(vdest, 3)), vdest, 3);
d2273 1
a2273 1
	CHECKPOINT();
d2282 5
a2286 4
		__m64 vd = M64(d);
		__m64 vdest = in_over(vsrc, vsrca, expand_alpha_rev (M64(m)), expand565(vd, 0));
		vd = pack565(vdest, _mm_setzero_si64(), 0);
		*dst = UINT64(vd);
d2295 1
a2295 1
    _mm_empty();
d2299 18
a2316 18
fbCompositeSrc_8888RevNPx0565mmx (pixman_implementation_t *imp,
				  pixman_op_t op,
				  pixman_image_t * pSrc,
				  pixman_image_t * pMask,
				  pixman_image_t * pDst,
				  int32_t      xSrc,
				  int32_t      ySrc,
				  int32_t      xMask,
				  int32_t      yMask,
				  int32_t      xDst,
				  int32_t      yDst,
				  int32_t     width,
				  int32_t     height)
{
    uint16_t	*dstLine, *dst;
    uint32_t	*srcLine, *src;
    int	dstStride, srcStride;
    uint16_t	w;
d2318 1
a2318 1
    CHECKPOINT();
d2320 2
a2321 2
    fbComposeGetStart (pDst, xDst, yDst, uint16_t, dstStride, dstLine, 1);
    fbComposeGetStart (pSrc, xSrc, ySrc, uint32_t, srcStride, srcLine, 1);
d2325 1
a2325 1
    assert (pSrc->pDrawable == pMask->pDrawable);
d2330 4
a2333 4
	dst = dstLine;
	dstLine += dstStride;
	src = srcLine;
	srcLine += srcStride;
d2336 1
a2336 1
	CHECKPOINT();
d2342 1
a2342 1
	    __m64 vdest = expand565 (M64(d), 0);
d2344 1
a2344 1
	    vdest = pack565(over_rev_non_pre(vsrc, vdest), vdest, 0);
d2346 1
a2346 1
	    *dst = UINT64(vdest);
d2353 1
a2353 1
	CHECKPOINT();
d2373 4
a2376 4
		vdest = pack565(invert_colors(load8888(s0)), _mm_setzero_si64(), 0);
		vdest = pack565(invert_colors(load8888(s1)), vdest, 1);
		vdest = pack565(invert_colors(load8888(s2)), vdest, 2);
		vdest = pack565(invert_colors(load8888(s3)), vdest, 3);
d2384 4
a2387 4
		vdest = pack565(over_rev_non_pre(load8888(s0), expand565(vdest, 0)), vdest, 0);
	        vdest = pack565(over_rev_non_pre(load8888(s1), expand565(vdest, 1)), vdest, 1);
		vdest = pack565(over_rev_non_pre(load8888(s2), expand565(vdest, 2)), vdest, 2);
		vdest = pack565(over_rev_non_pre(load8888(s3), expand565(vdest, 3)), vdest, 3);
d2397 1
a2397 1
	CHECKPOINT();
d2403 1
a2403 1
	    __m64 vdest = expand565 (M64(d), 0);
d2405 1
a2405 1
	    vdest = pack565(over_rev_non_pre(vsrc, vdest), vdest, 0);
d2407 1
a2407 1
	    *dst = UINT64(vdest);
d2415 1
a2415 1
    _mm_empty();
a2417 2
/* "8888RevNP" is GdkPixbuf's format: ABGR, non premultiplied */

d2419 18
a2436 18
fbCompositeSrc_8888RevNPx8888mmx (pixman_implementation_t *imp,
				  pixman_op_t op,
				  pixman_image_t * pSrc,
				  pixman_image_t * pMask,
				  pixman_image_t * pDst,
				  int32_t      xSrc,
				  int32_t      ySrc,
				  int32_t      xMask,
				  int32_t      yMask,
				  int32_t      xDst,
				  int32_t      yDst,
				  int32_t     width,
				  int32_t     height)
{
    uint32_t	*dstLine, *dst;
    uint32_t	*srcLine, *src;
    int	dstStride, srcStride;
    uint16_t	w;
d2438 1
a2438 1
    CHECKPOINT();
d2440 2
a2441 2
    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);
    fbComposeGetStart (pSrc, xSrc, ySrc, uint32_t, srcStride, srcLine, 1);
d2445 1
a2445 1
    assert (pSrc->pDrawable == pMask->pDrawable);
d2450 4
a2453 4
	dst = dstLine;
	dstLine += dstStride;
	src = srcLine;
	srcLine += srcStride;
d2482 2
a2483 2
		d0 = invert_colors(load8888(s0));
		d1 = invert_colors(load8888(s1));
d2491 2
a2492 2
		d0 = over_rev_non_pre (load8888(s0), expand8888 (vdest, 0));
		d1 = over_rev_non_pre (load8888(s1), expand8888 (vdest, 1));
d2515 1
a2515 1
    _mm_empty();
d2519 19
a2537 19
fbCompositeSolidMask_nx8888x0565Cmmx (pixman_implementation_t *imp,
				      pixman_op_t op,
				      pixman_image_t * pSrc,
				      pixman_image_t * pMask,
				      pixman_image_t * pDst,
				      int32_t      xSrc,
				      int32_t      ySrc,
				      int32_t      xMask,
				      int32_t      yMask,
				      int32_t      xDst,
				      int32_t      yDst,
				      int32_t     width,
				      int32_t     height)
{
    uint32_t	src, srca;
    uint16_t	*dstLine;
    uint32_t	*maskLine;
    int	dstStride, maskStride;
    __m64  vsrc, vsrca;
d2539 1
a2539 1
    CHECKPOINT();
d2541 1
a2541 1
    fbComposeGetSolid(pSrc, src, pDst->bits.format);
d2544 1
a2544 1
    if (srca == 0)
d2547 2
a2548 2
    fbComposeGetStart (pDst, xDst, yDst, uint16_t, dstStride, dstLine, 1);
    fbComposeGetStart (pMask, xMask, yMask, uint32_t, maskStride, maskLine, 1);
d2556 2
a2557 2
	uint32_t *p = (uint32_t *)maskLine;
	uint16_t *q = (uint16_t *)dstLine;
d2566 3
a2568 3
		__m64 vdest = expand565 (M64(d), 0);
		vdest = pack565 (in_over (vsrc, vsrca, load8888 (m), vdest), vdest, 0);
		*q = UINT64(vdest);
d2589 4
a2592 4
		vdest = pack565(in_over(vsrc, vsrca, load8888(m0), expand565(vdest, 0)), vdest, 0);
		vdest = pack565(in_over(vsrc, vsrca, load8888(m1), expand565(vdest, 1)), vdest, 1);
		vdest = pack565(in_over(vsrc, vsrca, load8888(m2), expand565(vdest, 2)), vdest, 2);
		vdest = pack565(in_over(vsrc, vsrca, load8888(m3), expand565(vdest, 3)), vdest, 3);
d2609 3
a2611 3
		__m64 vdest = expand565(M64(d), 0);
		vdest = pack565 (in_over(vsrc, vsrca, load8888(m), vdest), vdest, 0);
		*q = UINT64(vdest);
d2619 2
a2620 2
	maskLine += maskStride;
	dstLine += dstStride;
d2627 21
a2647 21
fbCompositeIn_nx8x8mmx (pixman_implementation_t *imp,
			pixman_op_t op,
			pixman_image_t * pSrc,
			pixman_image_t * pMask,
			pixman_image_t * pDst,
			int32_t      xSrc,
			int32_t      ySrc,
			int32_t      xMask,
			int32_t      yMask,
			int32_t      xDst,
			int32_t      yDst,
			int32_t     width,
			int32_t     height)
{
    uint8_t	*dstLine, *dst;
    uint8_t	*maskLine, *mask;
    int	dstStride, maskStride;
    uint16_t	w;
    uint32_t	src;
    uint8_t	sa;
    __m64	vsrc, vsrca;
d2649 2
a2650 2
    fbComposeGetStart (pDst, xDst, yDst, uint8_t, dstStride, dstLine, 1);
    fbComposeGetStart (pMask, xMask, yMask, uint8_t, maskStride, maskLine, 1);
d2652 1
a2652 1
    fbComposeGetSolid(pSrc, src, pDst->bits.format);
a2654 2
    if (sa == 0)
	return;
d2656 2
a2657 2
    vsrc = load8888(src);
    vsrca = expand_alpha(vsrc);
d2661 4
a2664 4
	dst = dstLine;
	dstLine += dstStride;
	mask = maskLine;
	maskLine += maskStride;
d2667 2
a2668 2
	if ((((unsigned long)pDst & 3) == 0) &&
	    (((unsigned long)pSrc & 3) == 0))
d2691 3
a2693 4
	    uint16_t	tmp;
	    uint8_t	a;
	    uint32_t	m, d;
	    uint32_t	r;
d2698 2
a2699 2
	    m = FbInU (sa, 0, a, tmp);
	    r = FbInU (m, 0, d, tmp);
d2701 1
a2701 1
	    *dst++ = r;
d2705 1
a2705 1
    _mm_empty();
d2709 21
a2729 21
fbCompositeIn_8x8mmx (pixman_implementation_t *imp,
		      pixman_op_t op,
		      pixman_image_t * pSrc,
		      pixman_image_t * pMask,
		      pixman_image_t * pDst,
		      int32_t      xSrc,
		      int32_t      ySrc,
		      int32_t      xMask,
		      int32_t      yMask,
		      int32_t      xDst,
		      int32_t      yDst,
		      int32_t     width,
		      int32_t     height)
{
    uint8_t	*dstLine, *dst;
    uint8_t	*srcLine, *src;
    int	srcStride, dstStride;
    uint16_t	w;

    fbComposeGetStart (pDst, xDst, yDst, uint8_t, dstStride, dstLine, 1);
    fbComposeGetStart (pSrc, xSrc, ySrc, uint8_t, srcStride, srcLine, 1);
d2733 4
a2736 4
	dst = dstLine;
	dstLine += dstStride;
	src = srcLine;
	srcLine += srcStride;
d2739 2
a2740 2
	if ((((unsigned long)pDst & 3) == 0) &&
	    (((unsigned long)pSrc & 3) == 0))
d2763 1
a2763 1
	    *dst = FbInU (s, 0, d, tmp);
d2774 21
a2794 21
fbCompositeSrcAdd_8888x8x8mmx (pixman_implementation_t *imp,
			       pixman_op_t op,
			       pixman_image_t * pSrc,
			       pixman_image_t * pMask,
			       pixman_image_t * pDst,
			       int32_t      xSrc,
			       int32_t      ySrc,
			       int32_t      xMask,
			       int32_t      yMask,
			       int32_t      xDst,
			       int32_t      yDst,
			       int32_t     width,
			       int32_t     height)
{
    uint8_t	*dstLine, *dst;
    uint8_t	*maskLine, *mask;
    int	dstStride, maskStride;
    uint16_t	w;
    uint32_t	src;
    uint8_t	sa;
    __m64	vsrc, vsrca;
d2796 2
a2797 2
    fbComposeGetStart (pDst, xDst, yDst, uint8_t, dstStride, dstLine, 1);
    fbComposeGetStart (pMask, xMask, yMask, uint8_t, maskStride, maskLine, 1);
d2799 1
a2799 1
    fbComposeGetSolid(pSrc, src, pDst->bits.format);
d2802 2
a2803 1
    if (sa == 0)
d2806 2
a2807 2
    vsrc = load8888(src);
    vsrca = expand_alpha(vsrc);
d2811 4
a2814 4
	dst = dstLine;
	dstLine += dstStride;
	mask = maskLine;
	maskLine += maskStride;
d2817 2
a2818 2
	if ((((unsigned long)pMask & 3) == 0) &&
	    (((unsigned long)pDst  & 3) == 0))
d2835 4
a2838 4
	    uint16_t	tmp;
	    uint16_t	a;
	    uint32_t	m, d;
	    uint32_t	r;
d2843 2
a2844 2
	    m = FbInU (sa, 0, a, tmp);
	    r = FbAdd (m, d, 0, tmp);
d2850 1
a2850 1
    _mm_empty();
d2854 25
a2878 25
fbCompositeSrcAdd_8000x8000mmx (pixman_implementation_t *imp,
				pixman_op_t op,
				pixman_image_t * pSrc,
				pixman_image_t * pMask,
				pixman_image_t * pDst,
				int32_t      xSrc,
				int32_t      ySrc,
				int32_t      xMask,
				int32_t      yMask,
				int32_t      xDst,
				int32_t      yDst,
				int32_t     width,
				int32_t     height)
{
    uint8_t	*dstLine, *dst;
    uint8_t	*srcLine, *src;
    int	dstStride, srcStride;
    uint16_t	w;
    uint8_t	s, d;
    uint16_t	t;

    CHECKPOINT();

    fbComposeGetStart (pSrc, xSrc, ySrc, uint8_t, srcStride, srcLine, 1);
    fbComposeGetStart (pDst, xDst, yDst, uint8_t, dstStride, dstLine, 1);
d2882 4
a2885 4
	dst = dstLine;
	dstLine += dstStride;
	src = srcLine;
	srcLine += srcStride;
d2903 1
a2903 1
	    *(__m64*)dst = _mm_adds_pu8(*(__m64*)src, *(__m64*)dst);
d2923 1
a2923 1
    _mm_empty();
d2927 13
a2939 13
fbCompositeSrcAdd_8888x8888mmx (pixman_implementation_t *imp,
				pixman_op_t 	op,
				pixman_image_t *	pSrc,
				pixman_image_t *	pMask,
				pixman_image_t *	 pDst,
				int32_t		 xSrc,
				int32_t      ySrc,
				int32_t      xMask,
				int32_t      yMask,
				int32_t      xDst,
				int32_t      yDst,
				int32_t     width,
				int32_t     height)
d2942 4
a2945 4
    uint32_t	*dstLine, *dst;
    uint32_t	*srcLine, *src;
    int	dstStride, srcStride;
    uint16_t	w;
d2947 1
a2947 1
    CHECKPOINT();
d2949 2
a2950 2
    fbComposeGetStart (pSrc, xSrc, ySrc, uint32_t, srcStride, srcLine, 1);
    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);
d2954 4
a2957 4
	dst = dstLine;
	dstLine += dstStride;
	src = srcLine;
	srcLine += srcStride;
d2962 2
a2963 2
	    *dst = _mm_cvtsi64_si32(_mm_adds_pu8(_mm_cvtsi32_si64(*src),
						 _mm_cvtsi32_si64(*dst)));
d2971 2
a2972 2
	    dst64 = _mm_adds_pu8(*(__m64*)src, *(__m64*)dst);
	    *(uint64_t*)dst = UINT64(dst64);
d2980 2
a2981 2
	    *dst = _mm_cvtsi64_si32(_mm_adds_pu8(_mm_cvtsi32_si64(*src),
						 _mm_cvtsi32_si64(*dst)));
d2986 1
a2986 1
    _mm_empty();
d2991 15
a3005 12
		uint32_t *dst_bits,
		int src_stride,
		int dst_stride,
		int src_bpp,
		int dst_bpp,
		int src_x, int src_y,
		int dst_x, int dst_y,
		int width, int height)
{
    uint8_t *	src_bytes;
    uint8_t *	dst_bytes;
    int		byte_width;
d3019 3
a3021 1
    } else if (src_bpp == 32) {
d3029 3
a3031 1
    } else {
d3065 17
a3081 17
		"movq	  (%1),	  %%mm0\n"
		"movq	 8(%1),	  %%mm1\n"
		"movq	16(%1),	  %%mm2\n"
		"movq	24(%1),	  %%mm3\n"
		"movq	32(%1),	  %%mm4\n"
		"movq	40(%1),	  %%mm5\n"
		"movq	48(%1),	  %%mm6\n"
		"movq	56(%1),	  %%mm7\n"

		"movq	%%mm0,	  (%0)\n"
		"movq	%%mm1,	 8(%0)\n"
		"movq	%%mm2,	16(%0)\n"
		"movq	%%mm3,	24(%0)\n"
		"movq	%%mm4,	32(%0)\n"
		"movq	%%mm5,	40(%0)\n"
		"movq	%%mm6,	48(%0)\n"
		"movq	%%mm7,	56(%0)\n"
d3085 2
a3086 2
		  "%mm0", "%mm1", "%mm2", "%mm3",
		  "%mm4", "%mm5", "%mm6", "%mm7");
d3127 1
a3127 1
    _mm_empty();
d3133 21
a3153 21
fbCompositeCopyAreammx (pixman_implementation_t *imp,
			pixman_op_t       op,
			pixman_image_t *	pSrc,
			pixman_image_t *	pMask,
			pixman_image_t *	pDst,
			int32_t		xSrc,
			int32_t		ySrc,
			int32_t		xMask,
			int32_t		yMask,
			int32_t		xDst,
			int32_t		yDst,
			int32_t		width,
			int32_t		height)
{
    pixman_blt_mmx (pSrc->bits.bits,
		    pDst->bits.bits,
		    pSrc->bits.rowstride,
		    pDst->bits.rowstride,
		    PIXMAN_FORMAT_BPP (pSrc->bits.format),
		    PIXMAN_FORMAT_BPP (pDst->bits.format),
		    xSrc, ySrc, xDst, yDst, width, height);
d3157 18
a3174 18
fbCompositeOver_x888x8x8888mmx (pixman_implementation_t *imp,
				pixman_op_t      op,
				pixman_image_t * pSrc,
				pixman_image_t * pMask,
				pixman_image_t * pDst,
				int32_t      xSrc,
				int32_t      ySrc,
				int32_t      xMask,
				int32_t      yMask,
				int32_t      xDst,
				int32_t      yDst,
				int32_t     width,
				int32_t     height)
{
    uint32_t	*src, *srcLine;
    uint32_t    *dst, *dstLine;
    uint8_t	*mask, *maskLine;
    int		 srcStride, maskStride, dstStride;
d3177 3
a3179 3
    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);
    fbComposeGetStart (pMask, xMask, yMask, uint8_t, maskStride, maskLine, 1);
    fbComposeGetStart (pSrc, xSrc, ySrc, uint32_t, srcStride, srcLine, 1);
d3183 6
a3188 6
	src = srcLine;
	srcLine += srcStride;
	dst = dstLine;
	dstLine += dstStride;
	mask = maskLine;
	maskLine += maskStride;
d3201 1
d3203 1
d3207 2
a3208 2
		    __m64 vm = expand_alpha_rev (M64(m));
		    __m64 vdest = in_over(s, sa, vm, load8888 (*dst));
d3220 1
a3220 1
    _mm_empty();
d3223 1
a3223 1
static const FastPathInfo mmx_fast_paths[] =
d3225 32
a3256 32
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_r5g6b5,   fbCompositeSolidMask_nx8x0565mmx,     0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_b5g6r5,   fbCompositeSolidMask_nx8x0565mmx,     0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_a8r8g8b8, fbCompositeSolidMask_nx8x8888mmx,     0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_x8r8g8b8, fbCompositeSolidMask_nx8x8888mmx,     0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_a8b8g8r8, fbCompositeSolidMask_nx8x8888mmx,     0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8,       PIXMAN_x8b8g8r8, fbCompositeSolidMask_nx8x8888mmx,     0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8r8g8b8, PIXMAN_a8r8g8b8, fbCompositeSolidMask_nx8888x8888Cmmx, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8r8g8b8, PIXMAN_x8r8g8b8, fbCompositeSolidMask_nx8888x8888Cmmx, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8r8g8b8, PIXMAN_r5g6b5,   fbCompositeSolidMask_nx8888x0565Cmmx, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8b8g8r8, PIXMAN_a8b8g8r8, fbCompositeSolidMask_nx8888x8888Cmmx, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8b8g8r8, PIXMAN_x8b8g8r8, fbCompositeSolidMask_nx8888x8888Cmmx, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_a8b8g8r8, PIXMAN_b5g6r5,   fbCompositeSolidMask_nx8888x0565Cmmx, NEED_COMPONENT_ALPHA },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8r8g8b8, PIXMAN_a8r8g8b8, fbCompositeSrc_8888RevNPx8888mmx, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8b8g8r8, PIXMAN_a8r8g8b8, fbCompositeSrc_8888RevNPx8888mmx, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8r8g8b8, PIXMAN_x8r8g8b8, fbCompositeSrc_8888RevNPx8888mmx, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8b8g8r8, PIXMAN_x8r8g8b8, fbCompositeSrc_8888RevNPx8888mmx, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8r8g8b8, PIXMAN_r5g6b5,   fbCompositeSrc_8888RevNPx0565mmx, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8b8g8r8, PIXMAN_r5g6b5,   fbCompositeSrc_8888RevNPx0565mmx, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8r8g8b8, PIXMAN_a8b8g8r8, fbCompositeSrc_8888RevNPx8888mmx, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8b8g8r8, PIXMAN_a8b8g8r8, fbCompositeSrc_8888RevNPx8888mmx, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8r8g8b8, PIXMAN_x8b8g8r8, fbCompositeSrc_8888RevNPx8888mmx, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8b8g8r8, PIXMAN_x8b8g8r8, fbCompositeSrc_8888RevNPx8888mmx, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8r8g8b8, PIXMAN_b5g6r5,   fbCompositeSrc_8888RevNPx0565mmx, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8b8g8r8, PIXMAN_b5g6r5,   fbCompositeSrc_8888RevNPx0565mmx, NEED_PIXBUF },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8,       PIXMAN_a8r8g8b8, fbCompositeSrc_x888xnx8888mmx,    NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8,       PIXMAN_x8r8g8b8, fbCompositeSrc_x888xnx8888mmx,	   NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8,	PIXMAN_a8b8g8r8, fbCompositeSrc_x888xnx8888mmx,	   NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_a8,	PIXMAN_x8b8g8r8, fbCompositeSrc_x888xnx8888mmx,	   NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_a8r8g8b8, PIXMAN_a8,       PIXMAN_a8r8g8b8, fbCompositeSrc_8888x8x8888mmx,    NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_a8r8g8b8, PIXMAN_a8,       PIXMAN_x8r8g8b8, fbCompositeSrc_8888x8x8888mmx,	   NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_a8b8g8r8, PIXMAN_a8,	PIXMAN_a8b8g8r8, fbCompositeSrc_8888x8x8888mmx,	   NEED_SOLID_MASK },
    { PIXMAN_OP_OVER, PIXMAN_a8b8g8r8, PIXMAN_a8,	PIXMAN_x8b8g8r8, fbCompositeSrc_8888x8x8888mmx,	   NEED_SOLID_MASK },
d3259 4
a3262 4
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8,	PIXMAN_x8r8g8b8, fbCompositeOver_x888x8x8888mmx,   0 },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_a8,	PIXMAN_a8r8g8b8, fbCompositeOver_x888x8x8888mmx,   0 },
    { PIXMAN_OP_OVER, PIXMAN_x8b8r8g8, PIXMAN_a8,	PIXMAN_x8b8g8r8, fbCompositeOver_x888x8x8888mmx,   0 },
    { PIXMAN_OP_OVER, PIXMAN_x8b8r8g8, PIXMAN_a8,	PIXMAN_a8r8g8b8, fbCompositeOver_x888x8x8888mmx,   0 },
d3264 30
a3293 30
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_null,	PIXMAN_a8r8g8b8, fbCompositeSolid_nx8888mmx,       0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_null,     PIXMAN_x8r8g8b8, fbCompositeSolid_nx8888mmx,	   0 },
    { PIXMAN_OP_OVER, PIXMAN_solid,    PIXMAN_null,     PIXMAN_r5g6b5,   fbCompositeSolid_nx0565mmx,	   0 },
    { PIXMAN_OP_OVER, PIXMAN_x8r8g8b8, PIXMAN_null,     PIXMAN_x8r8g8b8, fbCompositeCopyAreammx,	   0 },
    { PIXMAN_OP_OVER, PIXMAN_x8b8g8r8, PIXMAN_null,     PIXMAN_x8b8g8r8, fbCompositeCopyAreammx,	   0 },

    { PIXMAN_OP_OVER, PIXMAN_a8r8g8b8, PIXMAN_null,     PIXMAN_a8r8g8b8, fbCompositeSrc_8888x8888mmx,	   0 },
    { PIXMAN_OP_OVER, PIXMAN_a8r8g8b8, PIXMAN_null,	PIXMAN_x8r8g8b8, fbCompositeSrc_8888x8888mmx,	   0 },
    { PIXMAN_OP_OVER, PIXMAN_a8r8g8b8, PIXMAN_null,	PIXMAN_r5g6b5,	 fbCompositeSrc_8888x0565mmx,	   0 },
    { PIXMAN_OP_OVER, PIXMAN_a8b8g8r8, PIXMAN_null,	PIXMAN_a8b8g8r8, fbCompositeSrc_8888x8888mmx,	   0 },
    { PIXMAN_OP_OVER, PIXMAN_a8b8g8r8, PIXMAN_null,	PIXMAN_x8b8g8r8, fbCompositeSrc_8888x8888mmx,	   0 },
    { PIXMAN_OP_OVER, PIXMAN_a8b8g8r8, PIXMAN_null,     PIXMAN_b5g6r5,   fbCompositeSrc_8888x0565mmx,	   0 },

    { PIXMAN_OP_ADD, PIXMAN_a8r8g8b8,  PIXMAN_null,	PIXMAN_a8r8g8b8, fbCompositeSrcAdd_8888x8888mmx,   0 },
    { PIXMAN_OP_ADD, PIXMAN_a8b8g8r8,  PIXMAN_null,	PIXMAN_a8b8g8r8, fbCompositeSrcAdd_8888x8888mmx,   0 },
    { PIXMAN_OP_ADD, PIXMAN_a8,        PIXMAN_null,     PIXMAN_a8,       fbCompositeSrcAdd_8000x8000mmx,   0 },
    { PIXMAN_OP_ADD, PIXMAN_solid,     PIXMAN_a8,       PIXMAN_a8,       fbCompositeSrcAdd_8888x8x8mmx,    0 },

    { PIXMAN_OP_SRC, PIXMAN_solid,     PIXMAN_a8,       PIXMAN_a8r8g8b8, fbCompositeSolidMaskSrc_nx8x8888mmx, 0 },
    { PIXMAN_OP_SRC, PIXMAN_solid,     PIXMAN_a8,       PIXMAN_x8r8g8b8, fbCompositeSolidMaskSrc_nx8x8888mmx, 0 },
    { PIXMAN_OP_SRC, PIXMAN_solid,     PIXMAN_a8,       PIXMAN_a8b8g8r8, fbCompositeSolidMaskSrc_nx8x8888mmx, 0 },
    { PIXMAN_OP_SRC, PIXMAN_solid,     PIXMAN_a8,       PIXMAN_x8b8g8r8, fbCompositeSolidMaskSrc_nx8x8888mmx, 0 },
    { PIXMAN_OP_SRC, PIXMAN_a8r8g8b8,  PIXMAN_null,	PIXMAN_a8r8g8b8, fbCompositeCopyAreammx, 0 },
    { PIXMAN_OP_SRC, PIXMAN_a8b8g8r8,  PIXMAN_null,	PIXMAN_a8b8g8r8, fbCompositeCopyAreammx, 0 },
    { PIXMAN_OP_SRC, PIXMAN_a8r8g8b8,  PIXMAN_null,	PIXMAN_x8r8g8b8, fbCompositeCopyAreammx, 0 },
    { PIXMAN_OP_SRC, PIXMAN_a8b8g8r8,  PIXMAN_null,	PIXMAN_x8b8g8r8, fbCompositeCopyAreammx, 0 },
    { PIXMAN_OP_SRC, PIXMAN_x8r8g8b8,  PIXMAN_null,	PIXMAN_x8r8g8b8, fbCompositeCopyAreammx, 0 },
    { PIXMAN_OP_SRC, PIXMAN_x8b8g8r8,  PIXMAN_null,	PIXMAN_x8b8g8r8, fbCompositeCopyAreammx, 0 },
    { PIXMAN_OP_SRC, PIXMAN_r5g6b5,    PIXMAN_null,     PIXMAN_r5g6b5,   fbCompositeCopyAreammx, 0 },
    { PIXMAN_OP_SRC, PIXMAN_b5g6r5,    PIXMAN_null,     PIXMAN_b5g6r5,   fbCompositeCopyAreammx, 0 },    
d3295 2
a3296 2
    { PIXMAN_OP_IN,  PIXMAN_a8,        PIXMAN_null,     PIXMAN_a8,       fbCompositeIn_8x8mmx,   0 },
    { PIXMAN_OP_IN,  PIXMAN_solid,     PIXMAN_a8,	PIXMAN_a8,	 fbCompositeIn_nx8x8mmx, 0 },
d3303 12
a3314 12
	       pixman_op_t     op,
	       pixman_image_t *src,
	       pixman_image_t *mask,
	       pixman_image_t *dest,
	       int32_t         src_x,
	       int32_t         src_y,
	       int32_t         mask_x,
	       int32_t         mask_y,
	       int32_t         dest_x,
	       int32_t         dest_y,
	       int32_t         width,
	       int32_t         height)
d3317 6
a3322 5
			       op, src, mask, dest,
			       src_x, src_y,
			       mask_x, mask_y,
			       dest_x, dest_y,
			       width, height))
d3324 1
d3327 3
a3329 3
				      op, src, mask, dest, src_x, src_y,
				      mask_x, mask_y, dest_x, dest_y,
				      width, height);
d3334 12
a3345 9
	 uint32_t *src_bits,
	 uint32_t *dst_bits,
	 int src_stride,
	 int dst_stride,
	 int src_bpp,
	 int dst_bpp,
	 int src_x, int src_y,
	 int dst_x, int dst_y,
	 int width, int height)
d3348 2
a3349 2
	    src_bits, dst_bits, src_stride, dst_stride, src_bpp, dst_bpp,
	    src_x, src_y, dst_x, dst_y, width, height))
d3353 3
a3355 3
	    imp->delegate,
	    src_bits, dst_bits, src_stride, dst_stride, src_bpp, dst_bpp,
	    src_x, src_y, dst_x, dst_y, width, height);
d3363 8
a3370 8
	  uint32_t *bits,
	  int stride,
	  int bpp,
	  int x,
	  int y,
	  int width,
	  int height,
	  uint32_t xor)
d3375 1
a3375 1
	    imp->delegate, bits, stride, bpp, x, y, width, height, xor);
d3382 1
a3382 1
_pixman_implementation_create_mmx (pixman_implementation_t *toplevel)
d3384 2
a3385 2
    pixman_implementation_t *general = _pixman_implementation_create_fast_path (NULL);
    pixman_implementation_t *imp = _pixman_implementation_create (toplevel, general);
d3387 23
a3409 23
    imp->combine_32[PIXMAN_OP_OVER] = mmxCombineOverU;
    imp->combine_32[PIXMAN_OP_OVER_REVERSE] = mmxCombineOverReverseU;
    imp->combine_32[PIXMAN_OP_IN] = mmxCombineInU;
    imp->combine_32[PIXMAN_OP_IN_REVERSE] = mmxCombineInReverseU;
    imp->combine_32[PIXMAN_OP_OUT] = mmxCombineOutU;
    imp->combine_32[PIXMAN_OP_OUT_REVERSE] = mmxCombineOutReverseU;
    imp->combine_32[PIXMAN_OP_ATOP] = mmxCombineAtopU;
    imp->combine_32[PIXMAN_OP_ATOP_REVERSE] = mmxCombineAtopReverseU;
    imp->combine_32[PIXMAN_OP_XOR] = mmxCombineXorU; 
    imp->combine_32[PIXMAN_OP_ADD] = mmxCombineAddU;
    imp->combine_32[PIXMAN_OP_SATURATE] = mmxCombineSaturateU;
    
    imp->combine_32_ca[PIXMAN_OP_SRC] = mmxCombineSrcC;
    imp->combine_32_ca[PIXMAN_OP_OVER] = mmxCombineOverC;
    imp->combine_32_ca[PIXMAN_OP_OVER_REVERSE] = mmxCombineOverReverseC;
    imp->combine_32_ca[PIXMAN_OP_IN] = mmxCombineInC;
    imp->combine_32_ca[PIXMAN_OP_IN_REVERSE] = mmxCombineInReverseC;
    imp->combine_32_ca[PIXMAN_OP_OUT] = mmxCombineOutC;
    imp->combine_32_ca[PIXMAN_OP_OUT_REVERSE] = mmxCombineOutReverseC;
    imp->combine_32_ca[PIXMAN_OP_ATOP] = mmxCombineAtopC;
    imp->combine_32_ca[PIXMAN_OP_ATOP_REVERSE] = mmxCombineAtopReverseC;
    imp->combine_32_ca[PIXMAN_OP_XOR] = mmxCombineXorC;
    imp->combine_32_ca[PIXMAN_OP_ADD] = mmxCombineAddC;
d3414 1
a3414 1
    
@


1.3
log
@pixman 0.12.0. Tested on a full ports build by naddy@@.
@
text
@d39 1
a39 7

#include "pixman-mmx.h"

#undef READ
#undef WRITE
#define READ(img,x) *(x)
#define WRITE(img,ptr,v) (*(ptr) = (v));
d68 1
a68 1
/* --------------- MMX primitivess ------------------------------------ */
d71 2
a72 5
typedef unsigned long long ullong;
typedef ullong mmxdatafield;
#endif
#ifdef _MSC_VER
typedef unsigned __int64 ullong;
d74 7
d102 8
d112 15
a126 34
#ifdef __GNUC__
    .mmx_4x00ff =			0x00ff00ff00ff00ffULL,
    .mmx_4x0080 =			0x0080008000800080ULL,
    .mmx_565_rgb =			0x000001f0003f001fULL,
    .mmx_565_unpack_multiplier =	0x0000008404100840ULL,
    .mmx_565_r =			0x000000f800000000ULL,
    .mmx_565_g =			0x0000000000fc0000ULL,
    .mmx_565_b =			0x00000000000000f8ULL,
    .mmx_mask_0 =			0xffffffffffff0000ULL,
    .mmx_mask_1 =			0xffffffff0000ffffULL,
    .mmx_mask_2 =			0xffff0000ffffffffULL,
    .mmx_mask_3 =			0x0000ffffffffffffULL,
    .mmx_full_alpha =			0x00ff000000000000ULL,
    .mmx_ffff0000ffff0000 =		0xffff0000ffff0000ULL,
    .mmx_0000ffff00000000 =		0x0000ffff00000000ULL,
    .mmx_000000000000ffff =		0x000000000000ffffULL,
#endif
#ifdef _MSC_VER
    { 0x00ff00ff00ff00ffUI64 },
    { 0x0080008000800080UI64 },
    { 0x000001f0003f001fUI64 },
    { 0x0000008404100840UI64 },
    { 0x000000f800000000UI64 },
    { 0x0000000000fc0000UI64 },
    { 0x00000000000000f8UI64 },
    { 0xffffffffffff0000UI64 },
    { 0xffffffff0000ffffUI64 },
    { 0xffff0000ffffffffUI64 },
    { 0x0000ffffffffffffUI64 },
    { 0x00ff000000000000UI64 },
    { 0xffff0000ffff0000UI64 },
    { 0x0000ffff00000000UI64 },
    { 0x000000000000ffffUI64 },
#endif
d135 1
a135 4
#    define inline __inline__ __attribute__ ((__always_inline__))
#endif

#ifdef _MSC_VER
a136 2
#    undef inline
#    define inline __forceinline
d139 2
a140 2
static inline __m64
M64 (ullong x)
d144 1
a144 5
#elif defined (__GNUC__)
    return (__m64)x;
#endif

#ifdef _MSC_VER
d147 1
a147 1
    res.m64_u64 = x;
d149 2
d154 2
a155 2
static inline ullong
ULLONG (__m64 x)
d159 2
a160 8
#elif defined (__GNUC__)
    return (ullong)x;
#endif

#ifdef _MSC_VER
    ullong res;

    res = x.m64_u64;
d162 2
d167 1
a167 1
static inline __m64
d178 1
a178 1
static inline __m64
d184 1
a184 1
static inline __m64
d197 1
a197 1
static inline __m64
d203 1
a203 1
static inline __m64
d217 1
a217 1
static inline __m64
d234 1
a234 1
static inline __m64
d254 1
a254 1
static inline __m64
d260 1
a260 1
static inline __m64
d269 1
a269 1
static inline __m64
d276 1
a276 1
static inline __m64
d285 1
a285 1
static inline __m64
d297 1
a297 1
static inline __m64
d303 1
a303 1
static inline __m64
d309 1
a309 1
static inline uint32_t
d329 1
a329 1
static inline __m64
d349 1
a349 1
static inline __m64
d358 1
a358 1
static inline __m64
d364 1
a364 1
static inline __m64
d395 1
a395 1
static inline __m64
d419 2
a420 2
static FASTCALL void
mmxCombineMaskU (uint32_t *src, const uint32_t *mask, int width)
d422 11
a432 15
    const uint32_t *end = mask + width;
    while (mask < end) {
        uint32_t mmask = *mask;
	uint32_t maska = mmask >> 24;
	if (maska == 0) {
	    *src = 0;
	} else if (maska != 0xff) {
	    __m64 a = load8888(mmask);
	    __m64 s = load8888(*src);
	    a = expand_alpha(a);
	    s = pix_multiply(s, a);
	    *src = store8888(s);
	}
	++src;
	++mask;
d434 2
a435 1
    _mm_empty();
a437 1

d439 2
a440 1
mmxCombineOverU (uint32_t *dest, const uint32_t *src, int width)
d445 1
a445 1
	uint32_t ssrc = *src;
d449 1
a449 1
	} else if (a) {
d457 2
d464 2
a465 1
mmxCombineOverReverseU (uint32_t *dest, const uint32_t *src, int width)
d471 1
d474 1
a474 1
	*dest = store8888(over (d, da, load8888(*src)));
d477 2
d484 2
a485 1
mmxCombineInU (uint32_t *dest, const uint32_t *src, int width)
d491 1
a491 1
        x = load8888(*src);
d498 2
d505 2
a506 1
mmxCombineInReverseU (uint32_t *dest, const uint32_t *src, int width)
d513 1
a513 1
        a = load8888(*src);
d519 2
d526 2
a527 1
mmxCombineOutU (uint32_t *dest, const uint32_t *src, int width)
d533 1
a533 1
        x = load8888(*src);
d541 2
d548 2
a549 1
mmxCombineOutReverseU (uint32_t *dest, const uint32_t *src, int width)
d556 1
a556 1
        a = load8888(*src);
d563 2
d570 2
a571 1
mmxCombineAtopU (uint32_t *dest, const uint32_t *src, int width)
d577 1
a577 1
        s = load8888(*src);
d586 2
d593 2
a594 1
mmxCombineAtopReverseU (uint32_t *dest, const uint32_t *src, int width)
d602 1
a602 1
        s = load8888(*src);
d611 2
d618 2
a619 1
mmxCombineXorU (uint32_t *dest, const uint32_t *src, int width)
d625 1
a625 1
        s = load8888(*src);
d635 2
d642 2
a643 1
mmxCombineAddU (uint32_t *dest, const uint32_t *src, int width)
d648 1
a648 1
        s = load8888(*src);
d654 2
d661 2
a662 1
mmxCombineSaturateU (uint32_t *dest, const uint32_t *src, int width)
d666 1
a666 1
        uint32_t s = *src;
d682 2
d690 2
a691 1
mmxCombineSrcC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d707 2
a708 1
mmxCombineOverC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d727 2
a728 1
mmxCombineOverReverseC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d748 2
a749 1
mmxCombineInC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d768 2
a769 1
mmxCombineInReverseC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d788 2
a789 1
mmxCombineOutC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d809 2
a810 1
mmxCombineOutReverseC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d830 2
a831 1
mmxCombineAtopC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d853 2
a854 1
mmxCombineAtopReverseC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d876 2
a877 1
mmxCombineXorC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d900 2
a901 1
mmxCombineAddC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
a917 42
void
fbComposeSetupMMX(void)
{
    static pixman_bool_t initialized = FALSE;

    if (initialized)
	return;
    
    /* check if we have MMX support and initialize accordingly */
    if (pixman_have_mmx())
    {
        pixman_composeFunctions.combineU[PIXMAN_OP_OVER] = mmxCombineOverU;
        pixman_composeFunctions.combineU[PIXMAN_OP_OVER_REVERSE] = mmxCombineOverReverseU;
        pixman_composeFunctions.combineU[PIXMAN_OP_IN] = mmxCombineInU;
        pixman_composeFunctions.combineU[PIXMAN_OP_IN_REVERSE] = mmxCombineInReverseU;
        pixman_composeFunctions.combineU[PIXMAN_OP_OUT] = mmxCombineOutU;
        pixman_composeFunctions.combineU[PIXMAN_OP_OUT_REVERSE] = mmxCombineOutReverseU;
        pixman_composeFunctions.combineU[PIXMAN_OP_ATOP] = mmxCombineAtopU;
        pixman_composeFunctions.combineU[PIXMAN_OP_ATOP_REVERSE] = mmxCombineAtopReverseU;
        pixman_composeFunctions.combineU[PIXMAN_OP_XOR] = mmxCombineXorU;
        pixman_composeFunctions.combineU[PIXMAN_OP_ADD] = mmxCombineAddU;
        pixman_composeFunctions.combineU[PIXMAN_OP_SATURATE] = mmxCombineSaturateU;

        pixman_composeFunctions.combineC[PIXMAN_OP_SRC] = mmxCombineSrcC;
        pixman_composeFunctions.combineC[PIXMAN_OP_OVER] = mmxCombineOverC;
        pixman_composeFunctions.combineC[PIXMAN_OP_OVER_REVERSE] = mmxCombineOverReverseC;
        pixman_composeFunctions.combineC[PIXMAN_OP_IN] = mmxCombineInC;
        pixman_composeFunctions.combineC[PIXMAN_OP_IN_REVERSE] = mmxCombineInReverseC;
        pixman_composeFunctions.combineC[PIXMAN_OP_OUT] = mmxCombineOutC;
        pixman_composeFunctions.combineC[PIXMAN_OP_OUT_REVERSE] = mmxCombineOutReverseC;
        pixman_composeFunctions.combineC[PIXMAN_OP_ATOP] = mmxCombineAtopC;
        pixman_composeFunctions.combineC[PIXMAN_OP_ATOP_REVERSE] = mmxCombineAtopReverseC;
        pixman_composeFunctions.combineC[PIXMAN_OP_XOR] = mmxCombineXorC;
        pixman_composeFunctions.combineC[PIXMAN_OP_ADD] = mmxCombineAddC;

        pixman_composeFunctions.combineMaskU = mmxCombineMaskU;
    }

    initialized = TRUE;
}


d920 3
a922 2
void
fbCompositeSolid_nx8888mmx (pixman_op_t op,
d926 8
a933 8
			    int16_t	xSrc,
			    int16_t	ySrc,
			    int16_t	xMask,
			    int16_t	yMask,
			    int16_t	xDst,
			    int16_t	yDst,
			    uint16_t	width,
			    uint16_t	height)
d999 3
a1001 2
void
fbCompositeSolid_nx0565mmx (pixman_op_t op,
d1005 8
a1012 8
			    int16_t	xSrc,
			    int16_t	ySrc,
			    int16_t	xMask,
			    int16_t	yMask,
			    int16_t	xDst,
			    int16_t	yDst,
			    uint16_t	width,
			    uint16_t	height)
d1042 1
a1042 1
	    ullong d = *dst;
d1045 1
a1045 1
	    *dst = ULLONG(vdest);
d1072 1
a1072 1
	    ullong d = *dst;
d1075 1
a1075 1
	    *dst = ULLONG(vdest);
d1085 3
a1087 2
void
fbCompositeSolidMask_nx8888x8888Cmmx (pixman_op_t op,
d1091 8
a1098 8
				      int16_t	xSrc,
				      int16_t	ySrc,
				      int16_t	xMask,
				      int16_t	yMask,
				      int16_t	xDst,
				      int16_t	yDst,
				      uint16_t	width,
				      uint16_t	height)
d1189 3
a1191 2
void
fbCompositeSrc_8888x8x8888mmx (pixman_op_t op,
d1195 8
a1202 8
			       int16_t	xSrc,
			       int16_t	ySrc,
			       int16_t      xMask,
			       int16_t      yMask,
			       int16_t      xDst,
			       int16_t      yDst,
			       uint16_t     width,
			       uint16_t     height)
d1274 3
a1276 2
void
fbCompositeSrc_x888xnx8888mmx (pixman_op_t op,
d1280 8
a1287 8
			       int16_t	xSrc,
			       int16_t	ySrc,
			       int16_t      xMask,
			       int16_t      yMask,
			       int16_t      xDst,
			       int16_t      yDst,
			       uint16_t     width,
			       uint16_t     height)
d1409 3
a1411 2
void
fbCompositeSrc_8888x8888mmx (pixman_op_t op,
d1415 8
a1422 8
			     int16_t	xSrc,
			     int16_t	ySrc,
			     int16_t      xMask,
			     int16_t      yMask,
			     int16_t      xDst,
			     int16_t      yDst,
			     uint16_t     width,
			     uint16_t     height)
d1450 1
a1450 1
	    else if (a) {
d1462 3
a1464 2
void
fbCompositeSrc_8888x0565mmx (pixman_op_t op,
d1468 8
a1475 8
			     int16_t      xSrc,
			     int16_t      ySrc,
			     int16_t      xMask,
			     int16_t      yMask,
			     int16_t      xDst,
			     int16_t      yDst,
			     uint16_t     width,
			     uint16_t     height)
d1505 1
a1505 1
	    ullong d = *dst;
d1510 1
a1510 1
	    *dst = ULLONG(vdest);
d1548 1
a1548 1
	    ullong d = *dst;
d1553 1
a1553 1
	    *dst = ULLONG(vdest);
d1564 3
a1566 2
void
fbCompositeSolidMask_nx8x8888mmx (pixman_op_t op,
d1570 8
a1577 8
				  int16_t      xSrc,
				  int16_t      ySrc,
				  int16_t      xMask,
				  int16_t      yMask,
				  int16_t      xDst,
				  int16_t      yDst,
				  uint16_t     width,
				  uint16_t     height)
d1585 1
a1585 1
    ullong	srcsrc;
d1595 1
a1595 1
    srcsrc = (ullong)src << 32 | src;
d1615 1
a1615 1
	    ullong m = *mask;
d1632 1
a1632 1
	    ullong m0, m1;
d1638 1
a1638 1
		*(ullong *)dst = srcsrc;
d1662 1
a1662 1
	    ullong m = *mask;
d1690 1
a1690 1
    ullong	fill;
d1733 1
a1733 1
    fill = ((ullong)xor << 32) | xor;
d1836 3
a1838 2
void
fbCompositeSolidMaskSrc_nx8x8888mmx (pixman_op_t op,
d1842 8
a1849 8
				     int16_t      xSrc,
				     int16_t      ySrc,
				     int16_t      xMask,
				     int16_t      yMask,
				     int16_t      xDst,
				     int16_t      yDst,
				     uint16_t     width,
				     uint16_t     height)
d1857 1
a1857 1
    ullong	srcsrc;
d1871 1
a1871 1
    srcsrc = (ullong)src << 32 | src;
d1891 1
a1891 1
	    ullong m = *mask;
d1912 1
a1912 1
	    ullong m0, m1;
d1918 1
a1918 1
		*(ullong *)dst = srcsrc;
d1934 1
a1934 1
		*(ullong *)dst = 0;
d1946 1
a1946 1
	    ullong m = *mask;
d1968 3
a1970 2
void
fbCompositeSolidMask_nx8x0565mmx (pixman_op_t op,
d1974 8
a1981 8
				  int16_t      xSrc,
				  int16_t      ySrc,
				  int16_t      xMask,
				  int16_t      yMask,
				  int16_t      xDst,
				  int16_t      yDst,
				  uint16_t     width,
				  uint16_t     height)
d1989 1
a1989 1
    ullong srcsrcsrcsrc, src16;
d2006 1
a2006 1
    src16 = ULLONG(tmp);
d2008 2
a2009 2
    srcsrcsrcsrc = (ullong)src16 << 48 | (ullong)src16 << 32 |
	(ullong)src16 << 16 | (ullong)src16;
d2023 1
a2023 1
	    ullong m = *mask;
d2027 1
a2027 1
		ullong d = *dst;
d2031 1
a2031 1
		*dst = ULLONG(vd);
d2043 1
a2043 1
	    ullong m0, m1, m2, m3;
d2051 1
a2051 1
		*(ullong *)dst = srcsrcsrcsrc;
d2081 1
a2081 1
	    ullong m = *mask;
d2085 1
a2085 1
		ullong d = *dst;
d2089 1
a2089 1
		*dst = ULLONG(vd);
d2101 3
a2103 2
void
fbCompositeSrc_8888RevNPx0565mmx (pixman_op_t op,
d2107 8
a2114 8
				  int16_t      xSrc,
				  int16_t      ySrc,
				  int16_t      xMask,
				  int16_t      yMask,
				  int16_t      xDst,
				  int16_t      yDst,
				  uint16_t     width,
				  uint16_t     height)
d2144 1
a2144 1
	    ullong d = *dst;
d2149 1
a2149 1
	    *dst = ULLONG(vdest);
d2183 1
a2183 1
	    else if (a0 | a1 | a2 | a3)
d2205 1
a2205 1
	    ullong d = *dst;
d2210 1
a2210 1
	    *dst = ULLONG(vdest);
d2223 3
a2225 2
void
fbCompositeSrc_8888RevNPx8888mmx (pixman_op_t op,
d2229 8
a2236 8
				  int16_t      xSrc,
				  int16_t      ySrc,
				  int16_t      xMask,
				  int16_t      yMask,
				  int16_t      xDst,
				  int16_t      yDst,
				  uint16_t     width,
				  uint16_t     height)
d2275 1
a2275 1
	    ullong s0, s1;
d2292 1
a2292 1
	    else if (a0 | a1)
d2323 3
a2325 2
void
fbCompositeSolidMask_nx8888x0565Cmmx (pixman_op_t op,
d2329 8
a2336 8
				      int16_t      xSrc,
				      int16_t      ySrc,
				      int16_t      xMask,
				      int16_t      yMask,
				      int16_t      xDst,
				      int16_t      yDst,
				      uint16_t     width,
				      uint16_t     height)
d2370 1
a2370 1
		ullong d = *q;
d2373 1
a2373 1
		*q = ULLONG(vdest);
d2413 1
a2413 1
		ullong d = *q;
d2416 1
a2416 1
		*q = ULLONG(vdest);
d2431 3
a2433 2
void
fbCompositeIn_nx8x8mmx (pixman_op_t op,
d2437 8
a2444 8
			int16_t      xSrc,
			int16_t      ySrc,
			int16_t      xMask,
			int16_t      yMask,
			int16_t      xDst,
			int16_t      yDst,
			uint16_t     width,
			uint16_t     height)
d2516 3
a2518 2
void
fbCompositeIn_8x8mmx (pixman_op_t op,
d2522 8
a2529 8
		      int16_t      xSrc,
		      int16_t      ySrc,
		      int16_t      xMask,
		      int16_t      yMask,
		      int16_t      xDst,
		      int16_t      yDst,
		      uint16_t     width,
		      uint16_t     height)
d2581 3
a2583 2
void
fbCompositeSrcAdd_8888x8x8mmx (pixman_op_t op,
d2587 8
a2594 8
			       int16_t      xSrc,
			       int16_t      ySrc,
			       int16_t      xMask,
			       int16_t      yMask,
			       int16_t      xDst,
			       int16_t      yDst,
			       uint16_t     width,
			       uint16_t     height)
d2660 3
a2662 2
void
fbCompositeSrcAdd_8000x8000mmx (pixman_op_t op,
d2666 8
a2673 8
				int16_t      xSrc,
				int16_t      ySrc,
				int16_t      xMask,
				int16_t      yMask,
				int16_t      xDst,
				int16_t      yDst,
				uint16_t     width,
				uint16_t     height)
d2733 3
a2735 2
void
fbCompositeSrcAdd_8888x8888mmx (pixman_op_t 	op,
d2739 8
a2746 8
				int16_t		 xSrc,
				int16_t      ySrc,
				int16_t      xMask,
				int16_t      yMask,
				int16_t      xDst,
				int16_t      yDst,
				uint16_t     width,
				uint16_t     height)
d2779 1
a2779 1
	    *(ullong*)dst = ULLONG(dst64);
d2796 1
a2796 1
pixman_bool_t
d2863 1
a2863 1
#ifdef __GNUC__
d2932 3
a2934 2
void
fbCompositeCopyAreammx (pixman_op_t       op,
d2938 8
a2945 8
			int16_t		xSrc,
			int16_t		ySrc,
			int16_t		xMask,
			int16_t		yMask,
			int16_t		xDst,
			int16_t		yDst,
			uint16_t		width,
			uint16_t		height)
d2956 3
a2958 2
void
fbCompositeOver_x888x8x8888mmx (pixman_op_t      op,
d2962 8
a2969 8
				int16_t      xSrc,
				int16_t      ySrc,
				int16_t      xMask,
				int16_t      yMask,
				int16_t      xDst,
				int16_t      yDst,
				uint16_t     width,
				uint16_t     height)
d2994 1
a2994 1
	    ullong m = *mask;
d3021 149
d3171 39
@


1.2
log
@Update to pixman 0.10, with one small fix to the sse2 test in configure.ac.
@
text
@a38 3
#ifdef USE_SSE
#include <xmmintrin.h> /* for _mm_shuffle_pi16 and _MM_SHUFFLE */
#endif
d142 7
a148 3
#ifdef _MSC_VER
#undef inline
#define inline __forceinline
a150 3
#ifdef __GNUC__
#define MC(x) ((__m64) c.mmx_##x)
#endif
d152 3
a154 1
#define MC(x) c.mmx_##x
d160 3
a162 1
#ifdef __GNUC__
d168 1
a168 1
    
d177 3
a179 1
#ifdef __GNUC__
a226 22
#ifdef USE_SSE

static inline __m64
expand_alpha (__m64 pixel)
{
    return _mm_shuffle_pi16 (pixel, _MM_SHUFFLE(3, 3, 3, 3));
}

static inline __m64
expand_alpha_rev (__m64 pixel)
{
    return _mm_shuffle_pi16 (pixel, _MM_SHUFFLE(0, 0, 0, 0));
}

static inline __m64
invert_colors (__m64 pixel)
{
    return _mm_shuffle_pi16 (pixel, _MM_SHUFFLE(3, 0, 1, 2));
}

#else

a277 2
#endif

d383 6
d1368 2
a1369 2
		in_over (expand8888 (vs0, 0), srca, vmask, expand8888 (vd0, 0)),
		in_over (expand8888 (vs0, 1), srca, vmask, expand8888 (vd0, 1)));
d1372 2
a1373 2
		in_over (expand8888 (vs1, 0), srca, vmask, expand8888 (vd1, 0)),
		in_over (expand8888 (vs1, 1), srca, vmask, expand8888 (vd1, 1)));
d1376 2
a1377 2
		in_over (expand8888 (vs2, 0), srca, vmask, expand8888 (vd2, 0)),
		in_over (expand8888 (vs2, 1), srca, vmask, expand8888 (vd2, 1)));
d1380 2
a1381 2
		in_over (expand8888 (vs3, 0), srca, vmask, expand8888 (vd3, 0)),
		in_over (expand8888 (vs3, 1), srca, vmask, expand8888 (vd3, 1)));
d1384 2
a1385 2
		in_over (expand8888 (vs4, 0), srca, vmask, expand8888 (vd4, 0)),
		in_over (expand8888 (vs4, 1), srca, vmask, expand8888 (vd4, 1)));
d1388 2
a1389 2
		in_over (expand8888 (vs5, 0), srca, vmask, expand8888 (vd5, 0)),
		in_over (expand8888 (vs5, 1), srca, vmask, expand8888 (vd5, 1)));
d1392 2
a1393 2
		in_over (expand8888 (vs6, 0), srca, vmask, expand8888 (vd6, 0)),
		in_over (expand8888 (vs6, 1), srca, vmask, expand8888 (vd6, 1)));
d1396 2
a1397 2
		in_over (expand8888 (vs7, 0), srca, vmask, expand8888 (vd7, 0)),
		in_over (expand8888 (vs7, 1), srca, vmask, expand8888 (vd7, 1)));
d1715 3
d1721 4
a1724 1
    if (bpp != 16 && bpp != 32)
d1726 10
a1735 2

    if (bpp == 16)
d1774 7
d1834 1
a1834 1
	if (w >= 2)
d1840 7
@


1.1
log
@Initial revision
@
text
@d27 1
a27 1
 * MMX code paths for fbcompose.c by Lars Knoll (lars@@trolltech.com) 
d31 2
d34 1
a37 4
#if defined(__amd64__) || defined(__x86_64__)
#define USE_SSE
#endif

d47 2
a48 2
#define READ(x) *(x)
#define WRITE(ptr,v)   (*(ptr) = (v));
d79 1
a80 2

#ifdef __GNUC__
d158 30
d208 1
a208 1
    
d213 1
a213 1
    
d235 1
a235 1
}    
d249 1
a249 1
    
d309 1
a309 1
    
d362 1
a362 1
 * 
d364 1
a364 1
 * 
d369 1
a369 1
 * 
d378 1
a378 1
    
d380 2
a381 2
    p = shift (shift (p, (3 - pos) * 16), -48); 
    
d384 1
a384 1
    
d388 1
a388 1
    
d408 1
a408 1
    
d412 1
a412 1
    
d416 1
a416 1
    
d425 1
a425 1
    
d428 1
a428 1
    
d484 1
a484 1
    
d722 1
a722 1
	
d724 1
a724 1
	
d743 1
a743 1
	
d839 1
a839 1
        __m64 sa = expand_alpha(s); 
d915 2
a916 1
void fbComposeSetupMMX(void)
d918 5
d951 3
a953 1
    } 
d978 1
a978 1
    
d980 1
a980 1
    
d982 1
a982 1
    
d985 1
a985 1
    
d987 1
a987 1
    
d990 1
a990 1
    
d996 1
a996 1
	
d998 1
a998 1
	
d1002 1
a1002 1
	    
d1006 1
a1006 1
	
d1011 1
a1011 1
	    
d1013 1
a1013 1
	    
d1016 1
a1016 1
	    
d1018 1
a1018 1
	    
d1022 1
a1022 1
	
d1024 1
a1024 1
	
d1028 1
a1028 1
	    
d1033 1
a1033 1
    
d1056 1
a1056 1
    
d1058 1
a1058 1
    
d1060 1
a1060 1
    
d1063 1
a1063 1
    
d1065 1
a1065 1
    
d1068 1
a1068 1
    
d1074 1
a1074 1
	
d1076 1
a1076 1
	
d1080 1
a1080 1
	    __m64 vdest = expand565 ((__m64)d, 0);
d1082 2
a1083 2
	    *dst = (ullong)vdest;
	    
d1087 1
a1087 1
	
d1091 1
a1091 1
	    
d1093 1
a1093 1
	    
d1098 1
a1098 1
	    
d1100 1
a1100 1
	    
d1104 1
a1104 1
	
d1106 1
a1106 1
	
d1110 1
a1110 1
	    __m64 vdest = expand565 ((__m64)d, 0);
d1112 2
a1113 2
	    *dst = (ullong)vdest;
	    
d1118 1
a1118 1
    
d1141 1
a1141 1
    
d1143 1
a1143 1
    
d1145 1
a1145 1
    
d1149 1
a1149 1
    
d1152 1
a1152 1
    
d1155 1
a1155 1
    
d1161 1
a1161 1
	
d1165 1
a1165 1
	    
d1172 1
a1172 1
	    
d1177 1
a1177 1
	
d1183 1
a1183 1
	    
d1188 1
a1188 1
		
d1193 1
a1193 1
		
d1196 1
a1196 1
	    
d1201 1
a1201 1
	
d1205 1
a1205 1
	    
d1212 1
a1212 1
	    
d1217 1
a1217 1
	
d1221 1
a1221 1
    
d1371 1
a1371 1
	    
d1380 1
a1380 1
	    
d1384 1
a1384 1
	    
d1388 1
a1388 1
	    
d1392 1
a1392 1
	    
d1396 1
a1396 1
	    
d1400 1
a1400 1
	    
d1426 1
a1426 1
	
d1463 1
a1463 1
    
d1465 1
a1465 1
    
d1492 1
a1492 1
    _mm_empty(); 
d1513 1
a1513 1
    
d1515 1
a1515 1
    
d1518 1
a1518 1
    
d1523 1
a1523 1
    
d1531 1
a1531 1
	
d1533 1
a1533 1
	
d1538 2
a1539 2
	    __m64 vdest = expand565 ((__m64)d, 0);
	    
d1541 3
a1543 3
	    
	    *dst = (ullong)vdest;
	    
d1548 1
a1548 1
	
d1550 1
a1550 1
	
d1562 1
a1562 1
	    
d1567 1
a1567 1
	    
d1576 1
a1576 1
	
d1581 2
a1582 2
	    __m64 vdest = expand565 ((__m64)d, 0);
	    
d1584 3
a1586 3
	    
	    *dst = (ullong)vdest;
	    
d1592 1
a1592 1
    
d1617 1
a1617 1
    
d1619 1
a1619 1
    
d1621 1
a1621 1
    
d1625 3
a1627 3
    
    srcsrc = (unsigned long long)src << 32 | src;
    
d1630 1
a1630 1
    
d1633 1
a1633 1
    
d1641 1
a1641 1
	
d1643 1
a1643 1
	
d1647 1
a1647 1
	    
d1650 1
a1650 1
		__m64 vdest = in_over(vsrc, vsrca, expand_alpha_rev ((__m64)m), load8888(*dst));
d1653 1
a1653 1
	    
d1658 1
a1658 1
	
d1660 1
a1660 1
	
d1666 1
a1666 1
	    
d1669 1
a1669 1
		*(unsigned long long *)dst = srcsrc;
d1675 1
a1675 1
		
d1677 4
a1680 4
		
		dest0 = in_over(vsrc, vsrca, expand_alpha_rev ((__m64)m0), expand8888(vdest, 0));
		dest1 = in_over(vsrc, vsrca, expand_alpha_rev ((__m64)m1), expand8888(vdest, 1));
		
d1683 1
a1683 1
	    
d1688 1
a1688 1
	
d1690 1
a1690 1
	
d1694 1
a1694 1
	    
d1698 1
a1698 1
		vdest = in_over(vsrc, vsrca, expand_alpha_rev ((__m64)m), vdest);
d1701 1
a1701 1
	    
d1707 1
a1707 1
    
d1728 1
a1728 1
    
d1731 1
a1731 1
    
d1734 1
a1734 1
    
d1737 1
a1737 1
	stride = stride * sizeof (uint32_t) / 2;
d1744 1
a1744 1
	stride = stride * sizeof (uint32_t) / 4;
d1749 1
a1749 1
    
d1751 2
a1752 2
    vfill = (__m64)fill;
    
d1766 1
a1766 1
    
d1773 1
a1773 1
	
d1780 1
a1780 1
	
d1784 1
a1784 1
	    
d1815 1
a1815 1
#endif    
d1819 1
a1819 1
	
d1823 1
a1823 1
	    
d1834 1
a1834 1
    
d1897 1
a1897 1
		__m64 vdest = in(vsrc, expand_alpha_rev ((__m64)m));
d1929 2
a1930 2
		dest0 = in(vsrc, expand_alpha_rev ((__m64)m0));
		dest1 = in(vsrc, expand_alpha_rev ((__m64)m1));
d1953 1
a1953 1
		vdest = in(vsrc, expand_alpha_rev ((__m64)m));
d1989 3
a1991 3
    __m64	vsrc, vsrca;
    unsigned long long srcsrcsrcsrc, src16;
    
d1993 1
a1993 1
    
d1995 1
a1995 1
    
d1999 1
a1999 1
    
d2002 1
a2002 1
    
d2005 4
a2008 3
    
    src16 = (ullong)pack565(vsrc, _mm_setzero_si64(), 0);
    
d2011 1
a2011 1
    
d2019 1
a2019 1
	
d2021 1
a2021 1
	
d2025 1
a2025 1
	    
d2029 4
a2032 3
		__m64 vd = (__m64)d;
		__m64 vdest = in_over(vsrc, vsrca, expand_alpha_rev ((__m64)m), expand565(vd, 0));
		*dst = (ullong)pack565(vdest, _mm_setzero_si64(), 0);
d2034 1
a2034 1
	    
d2039 1
a2039 1
	
d2041 1
a2041 1
	
d2049 1
a2049 1
	    
d2052 1
a2052 1
		*(unsigned long long *)dst = srcsrcsrcsrc;
d2058 1
a2058 1
		
d2060 2
a2061 2
		
		vm0 = (__m64)m0;
d2063 1
a2063 1
		vm1 = (__m64)m1;
d2065 1
a2065 1
		vm2 = (__m64)m2;
d2067 1
a2067 1
		vm3 = (__m64)m3;
d2069 1
a2069 1
		
d2072 1
a2072 1
	    
d2077 1
a2077 1
	
d2079 1
a2079 1
	
d2083 1
a2083 1
	    
d2087 4
a2090 3
		__m64 vd = (__m64)d;
		__m64 vdest = in_over(vsrc, vsrca, expand_alpha_rev ((__m64)m), expand565(vd, 0));
		*dst = (ullong)pack565(vdest, _mm_setzero_si64(), 0);
d2092 1
a2092 1
	    
d2098 1
a2098 1
    
d2120 1
a2120 1
    
d2122 1
a2122 1
    
d2125 1
a2125 1
    
d2130 1
a2130 1
    
d2138 1
a2138 1
	
d2140 1
a2140 1
	
d2145 2
a2146 2
	    __m64 vdest = expand565 ((__m64)d, 0);
	    
d2148 3
a2150 3
	    
	    *dst = (ullong)vdest;
	    
d2155 1
a2155 1
	
d2157 1
a2157 1
	
d2162 1
a2162 1
	    
d2167 1
a2167 1
	    
d2172 1
a2172 1
	    
d2180 1
a2180 1
		
d2186 1
a2186 1
		
d2191 1
a2191 1
		
d2194 1
a2194 1
	    
d2199 1
a2199 1
	
d2201 1
a2201 1
	
d2206 2
a2207 2
	    __m64 vdest = expand565 ((__m64)d, 0);
	    
d2209 3
a2211 3
	    
	    *dst = (ullong)vdest;
	    
d2217 1
a2217 1
    
d2241 1
a2241 1
    
d2243 1
a2243 1
    
d2246 1
a2246 1
    
d2251 1
a2251 1
    
d2259 1
a2259 1
	
d2264 1
a2264 1
	    
d2266 1
a2266 1
	    
d2271 1
a2271 1
	
d2277 1
a2277 1
	    
d2280 1
a2280 1
	    
d2283 1
a2283 1
	    
d2288 1
a2288 1
		
d2294 1
a2294 1
		
d2297 1
a2297 1
		
d2300 1
a2300 1
	    
d2305 1
a2305 1
	
d2310 1
a2310 1
	    
d2312 1
a2312 1
	    
d2318 1
a2318 1
    
d2341 1
a2341 1
    
d2343 1
a2343 1
    
d2345 1
a2345 1
    
d2349 1
a2349 1
    
d2352 1
a2352 1
    
d2355 1
a2355 1
    
d2361 1
a2361 1
	
d2365 1
a2365 1
	    
d2369 1
a2369 1
		__m64 vdest = expand565 ((__m64)d, 0);
d2371 1
a2371 1
		*q = (ullong)vdest;
d2373 1
a2373 1
	    
d2378 1
a2378 1
	
d2382 1
a2382 1
	    
d2387 1
a2387 1
	    
d2391 1
a2391 1
		
d2396 1
a2396 1
		
d2403 1
a2403 1
	
d2407 1
a2407 1
	    
d2412 1
a2412 1
		__m64 vdest = expand565((__m64)d, 0);
d2414 1
a2414 1
		*q = (ullong)vdest;
d2416 1
a2416 1
	    
d2421 1
a2421 1
	
d2425 1
a2425 1
    
d2675 1
a2675 1
    
d2677 1
a2677 1
    
d2680 1
a2680 1
    
d2688 1
a2688 1
	
d2696 1
a2696 1
	    
d2701 1
a2701 1
	
d2709 1
a2709 1
	
d2717 1
a2717 1
	    
d2723 1
a2723 1
    
d2741 1
d2746 1
a2746 1
    
d2748 1
a2748 1
    
d2751 1
a2751 1
    
d2759 1
a2759 1
	
d2768 1
a2768 1
	
d2771 2
a2772 1
	    *(ullong*)dst = (ullong) _mm_adds_pu8(*(__m64*)src, *(__m64*)dst);
d2777 1
a2777 1
	
d2782 1
a2782 1
	    
d2785 1
a2785 1
    
d2789 1
a2789 1
pixman_bool_t 
d2803 1
a2803 1
    
d2806 1
a2806 1
    
d2809 2
a2810 2
	src_stride = src_stride * sizeof (uint32_t) / 2;
	dst_stride = dst_stride * sizeof (uint32_t) / 2;
d2817 2
a2818 2
	src_stride = src_stride * sizeof (uint32_t) / 4;
	dst_stride = dst_stride * sizeof (uint32_t) / 4;
d2836 1
a2836 1
	
d2844 1
a2844 1
	
d2848 1
a2848 1
	    
d2853 1
a2853 1
	
d2897 2
a2898 2
#endif	    
	    
d2919 1
a2919 1
    
a2965 2
    __m64 m;
    uint32_t s, d;
d2996 1
a2996 1
		    __m64 vm = expand_alpha_rev ((__m64)m);
d3002 1
a3002 1
	    
@


1.1.1.1
log
@import pixman 0.9.5
@
text
@@


1.1.1.2
log
@pixman 0.9.6
@
text
@a30 2

#ifdef HAVE_CONFIG_H
a31 1
#endif
d34 4
@

