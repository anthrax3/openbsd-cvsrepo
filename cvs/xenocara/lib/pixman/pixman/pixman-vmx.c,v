head	1.9;
access;
symbols
	OPENBSD_6_1_BASE:1.9
	OPENBSD_6_0:1.7.0.12
	OPENBSD_6_0_BASE:1.7
	OPENBSD_5_9:1.7.0.10
	OPENBSD_5_9_BASE:1.7
	OPENBSD_5_8:1.7.0.8
	OPENBSD_5_8_BASE:1.7
	OPENBSD_5_7:1.7.0.6
	OPENBSD_5_7_BASE:1.7
	OPENBSD_5_6:1.7.0.4
	OPENBSD_5_6_BASE:1.7
	OPENBSD_5_5:1.7.0.2
	OPENBSD_5_5_BASE:1.7
	OPENBSD_5_4:1.5.0.10
	OPENBSD_5_4_BASE:1.5
	OPENBSD_5_3:1.5.0.8
	OPENBSD_5_3_BASE:1.5
	OPENBSD_5_2:1.5.0.6
	OPENBSD_5_2_BASE:1.5
	OPENBSD_5_1_BASE:1.5
	OPENBSD_5_1:1.5.0.4
	OPENBSD_5_0:1.5.0.2
	OPENBSD_5_0_BASE:1.5
	OPENBSD_4_9:1.4.0.2
	OPENBSD_4_9_BASE:1.4
	OPENBSD_4_8:1.3.0.4
	OPENBSD_4_8_BASE:1.3
	OPENBSD_4_7:1.2.0.4
	OPENBSD_4_7_BASE:1.2
	OPENBSD_4_6:1.2.0.2
	OPENBSD_4_6_BASE:1.2
	OPENBSD_4_5:1.1.0.2
	OPENBSD_4_5_BASE:1.1;
locks; strict;
comment	@ * @;


1.9
date	2016.10.03.06.57.44;	author matthieu;	state Exp;
branches;
next	1.8;
commitid	C2cUfHWMQYyHw8OX;

1.8
date	2016.10.01.10.17.44;	author matthieu;	state Exp;
branches;
next	1.7;
commitid	FGr8CFhVerRlpSoE;

1.7
date	2013.12.01.20.34.20;	author matthieu;	state Exp;
branches;
next	1.6;

1.6
date	2013.08.25.17.42.39;	author matthieu;	state Exp;
branches;
next	1.5;

1.5
date	2011.07.24.13.05.47;	author matthieu;	state Exp;
branches;
next	1.4;

1.4
date	2010.10.03.18.30.04;	author matthieu;	state Exp;
branches;
next	1.3;

1.3
date	2010.03.25.21.58.52;	author matthieu;	state Exp;
branches;
next	1.2;

1.2
date	2009.06.05.20.14.28;	author matthieu;	state Exp;
branches;
next	1.1;

1.1
date	2008.09.23.19.11.40;	author matthieu;	state Exp;
branches;
next	;


desc
@@


1.9
log
@revert pixman-vmx.c to the version of pixman-0.32.8.
gcc 4.2 is not able to compile the new version.
XXX switch back to 0.34 once macppc switches to clang.
@
text
@/*
 * Copyright © 2007 Luca Barbato
 *
 * Permission to use, copy, modify, distribute, and sell this software and its
 * documentation for any purpose is hereby granted without fee, provided that
 * the above copyright notice appear in all copies and that both that
 * copyright notice and this permission notice appear in supporting
 * documentation, and that the name of Luca Barbato not be used in advertising or
 * publicity pertaining to distribution of the software without specific,
 * written prior permission.  Luca Barbato makes no representations about the
 * suitability of this software for any purpose.  It is provided "as is"
 * without express or implied warranty.
 *
 * THE COPYRIGHT HOLDERS DISCLAIM ALL WARRANTIES WITH REGARD TO THIS
 * SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND
 * FITNESS, IN NO EVENT SHALL THE COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN
 * AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING
 * OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS
 * SOFTWARE.
 *
 * Author:  Luca Barbato (lu_zero@@gentoo.org)
 *
 * Based on fbmmx.c by Owen Taylor, Søren Sandmann and Nicholas Miell
 */

#ifdef HAVE_CONFIG_H
#include <config.h>
#endif
#include "pixman-private.h"
#include "pixman-combine32.h"
#include <altivec.h>

#define AVV(x...) {x}

static force_inline vector unsigned int
splat_alpha (vector unsigned int pix)
{
    return vec_perm (pix, pix,
		     (vector unsigned char)AVV (
			 0x00, 0x00, 0x00, 0x00, 0x04, 0x04, 0x04, 0x04,
			 0x08, 0x08, 0x08, 0x08, 0x0C, 0x0C, 0x0C, 0x0C));
}

static force_inline vector unsigned int
pix_multiply (vector unsigned int p, vector unsigned int a)
{
    vector unsigned short hi, lo, mod;

    /* unpack to short */
    hi = (vector unsigned short)
	vec_mergeh ((vector unsigned char)AVV (0),
		    (vector unsigned char)p);

    mod = (vector unsigned short)
	vec_mergeh ((vector unsigned char)AVV (0),
		    (vector unsigned char)a);

    hi = vec_mladd (hi, mod, (vector unsigned short)
                    AVV (0x0080, 0x0080, 0x0080, 0x0080,
                         0x0080, 0x0080, 0x0080, 0x0080));

    hi = vec_adds (hi, vec_sr (hi, vec_splat_u16 (8)));

    hi = vec_sr (hi, vec_splat_u16 (8));

    /* unpack to short */
    lo = (vector unsigned short)
	vec_mergel ((vector unsigned char)AVV (0),
		    (vector unsigned char)p);
    mod = (vector unsigned short)
	vec_mergel ((vector unsigned char)AVV (0),
		    (vector unsigned char)a);

    lo = vec_mladd (lo, mod, (vector unsigned short)
                    AVV (0x0080, 0x0080, 0x0080, 0x0080,
                         0x0080, 0x0080, 0x0080, 0x0080));

    lo = vec_adds (lo, vec_sr (lo, vec_splat_u16 (8)));

    lo = vec_sr (lo, vec_splat_u16 (8));

    return (vector unsigned int)vec_packsu (hi, lo);
}

static force_inline vector unsigned int
pix_add (vector unsigned int a, vector unsigned int b)
{
    return (vector unsigned int)vec_adds ((vector unsigned char)a,
                                          (vector unsigned char)b);
}

static force_inline vector unsigned int
pix_add_mul (vector unsigned int x,
             vector unsigned int a,
             vector unsigned int y,
             vector unsigned int b)
{
    vector unsigned int t1, t2;

    t1 = pix_multiply (x, a);
    t2 = pix_multiply (y, b);

    return pix_add (t1, t2);
}

static force_inline vector unsigned int
negate (vector unsigned int src)
{
    return vec_nor (src, src);
}

/* dest*~srca + src */
static force_inline vector unsigned int
over (vector unsigned int src,
      vector unsigned int srca,
      vector unsigned int dest)
{
    vector unsigned char tmp = (vector unsigned char)
	pix_multiply (dest, negate (srca));

    tmp = vec_adds ((vector unsigned char)src, tmp);
    return (vector unsigned int)tmp;
}

/* in == pix_multiply */
#define in_over(src, srca, mask, dest)					\
    over (pix_multiply (src, mask),					\
          pix_multiply (srca, mask), dest)


#define COMPUTE_SHIFT_MASK(source)					\
    source ## _mask = vec_lvsl (0, source);

#define COMPUTE_SHIFT_MASKS(dest, source)				\
    dest ## _mask = vec_lvsl (0, dest);					\
    source ## _mask = vec_lvsl (0, source);				\
    store_mask = vec_lvsr (0, dest);

#define COMPUTE_SHIFT_MASKC(dest, source, mask)				\
    mask ## _mask = vec_lvsl (0, mask);					\
    dest ## _mask = vec_lvsl (0, dest);					\
    source ## _mask = vec_lvsl (0, source);				\
    store_mask = vec_lvsr (0, dest);

/* notice you have to declare temp vars...
 * Note: tmp3 and tmp4 must remain untouched!
 */

#define LOAD_VECTORS(dest, source)			  \
    tmp1 = (typeof(tmp1))vec_ld (0, source);		  \
    tmp2 = (typeof(tmp2))vec_ld (15, source);		  \
    tmp3 = (typeof(tmp3))vec_ld (0, dest);		  \
    v ## source = (typeof(v ## source))			  \
	vec_perm (tmp1, tmp2, source ## _mask);		  \
    tmp4 = (typeof(tmp4))vec_ld (15, dest);		  \
    v ## dest = (typeof(v ## dest))			  \
	vec_perm (tmp3, tmp4, dest ## _mask);

#define LOAD_VECTORSC(dest, source, mask)		  \
    tmp1 = (typeof(tmp1))vec_ld (0, source);		  \
    tmp2 = (typeof(tmp2))vec_ld (15, source);		  \
    tmp3 = (typeof(tmp3))vec_ld (0, dest);		  \
    v ## source = (typeof(v ## source))			  \
	vec_perm (tmp1, tmp2, source ## _mask);		  \
    tmp4 = (typeof(tmp4))vec_ld (15, dest);		  \
    tmp1 = (typeof(tmp1))vec_ld (0, mask);		  \
    v ## dest = (typeof(v ## dest))			  \
	vec_perm (tmp3, tmp4, dest ## _mask);		  \
    tmp2 = (typeof(tmp2))vec_ld (15, mask);		  \
    v ## mask = (typeof(v ## mask))			  \
	vec_perm (tmp1, tmp2, mask ## _mask);

#define LOAD_VECTORSM(dest, source, mask)				\
    LOAD_VECTORSC (dest, source, mask)					\
    v ## source = pix_multiply (v ## source,				\
                                splat_alpha (v ## mask));

#define STORE_VECTOR(dest)						\
    edges = vec_perm (tmp4, tmp3, dest ## _mask);			\
    tmp3 = vec_perm ((vector unsigned char)v ## dest, edges, store_mask); \
    tmp1 = vec_perm (edges, (vector unsigned char)v ## dest, store_mask); \
    vec_st ((vector unsigned int) tmp3, 15, dest);			\
    vec_st ((vector unsigned int) tmp1, 0, dest);

static void
vmx_combine_over_u_no_mask (uint32_t *      dest,
                            const uint32_t *src,
                            int             width)
{
    int i;
    vector unsigned int vdest, vsrc;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKS (dest, src);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {

	LOAD_VECTORS (dest, src);

	vdest = over (vsrc, splat_alpha (vsrc), vdest);

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t s = src[i];
	uint32_t d = dest[i];
	uint32_t ia = ALPHA_8 (~s);

	UN8x4_MUL_UN8_ADD_UN8x4 (d, ia, s);

	dest[i] = d;
    }
}

static void
vmx_combine_over_u_mask (uint32_t *      dest,
                         const uint32_t *src,
                         const uint32_t *mask,
                         int             width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, mask_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORSM (dest, src, mask);

	vdest = over (vsrc, splat_alpha (vsrc), vdest);

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
	mask += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t m = ALPHA_8 (mask[i]);
	uint32_t s = src[i];
	uint32_t d = dest[i];
	uint32_t ia;

	UN8x4_MUL_UN8 (s, m);

	ia = ALPHA_8 (~s);

	UN8x4_MUL_UN8_ADD_UN8x4 (d, ia, s);
	dest[i] = d;
    }
}

static void
vmx_combine_over_u (pixman_implementation_t *imp,
                    pixman_op_t              op,
                    uint32_t *               dest,
                    const uint32_t *         src,
                    const uint32_t *         mask,
                    int                      width)
{
    if (mask)
	vmx_combine_over_u_mask (dest, src, mask, width);
    else
	vmx_combine_over_u_no_mask (dest, src, width);
}

static void
vmx_combine_over_reverse_u_no_mask (uint32_t *      dest,
                                    const uint32_t *src,
                                    int             width)
{
    int i;
    vector unsigned int vdest, vsrc;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKS (dest, src);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {

	LOAD_VECTORS (dest, src);

	vdest = over (vdest, splat_alpha (vdest), vsrc);

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t s = src[i];
	uint32_t d = dest[i];
	uint32_t ia = ALPHA_8 (~dest[i]);

	UN8x4_MUL_UN8_ADD_UN8x4 (s, ia, d);
	dest[i] = s;
    }
}

static void
vmx_combine_over_reverse_u_mask (uint32_t *      dest,
                                 const uint32_t *src,
                                 const uint32_t *mask,
                                 int             width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, mask_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {

	LOAD_VECTORSM (dest, src, mask);

	vdest = over (vdest, splat_alpha (vdest), vsrc);

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
	mask += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t m = ALPHA_8 (mask[i]);
	uint32_t s = src[i];
	uint32_t d = dest[i];
	uint32_t ia = ALPHA_8 (~dest[i]);

	UN8x4_MUL_UN8 (s, m);

	UN8x4_MUL_UN8_ADD_UN8x4 (s, ia, d);
	dest[i] = s;
    }
}

static void
vmx_combine_over_reverse_u (pixman_implementation_t *imp,
                            pixman_op_t              op,
                            uint32_t *               dest,
                            const uint32_t *         src,
                            const uint32_t *         mask,
                            int                      width)
{
    if (mask)
	vmx_combine_over_reverse_u_mask (dest, src, mask, width);
    else
	vmx_combine_over_reverse_u_no_mask (dest, src, width);
}

static void
vmx_combine_in_u_no_mask (uint32_t *      dest,
                          const uint32_t *src,
                          int             width)
{
    int i;
    vector unsigned int vdest, vsrc;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKS (dest, src);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORS (dest, src);

	vdest = pix_multiply (vsrc, splat_alpha (vdest));

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t s = src[i];
	uint32_t a = ALPHA_8 (dest[i]);

	UN8x4_MUL_UN8 (s, a);
	dest[i] = s;
    }
}

static void
vmx_combine_in_u_mask (uint32_t *      dest,
                       const uint32_t *src,
                       const uint32_t *mask,
                       int             width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, mask_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORSM (dest, src, mask);

	vdest = pix_multiply (vsrc, splat_alpha (vdest));

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
	mask += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t m = ALPHA_8 (mask[i]);
	uint32_t s = src[i];
	uint32_t a = ALPHA_8 (dest[i]);

	UN8x4_MUL_UN8 (s, m);
	UN8x4_MUL_UN8 (s, a);

	dest[i] = s;
    }
}

static void
vmx_combine_in_u (pixman_implementation_t *imp,
                  pixman_op_t              op,
                  uint32_t *               dest,
                  const uint32_t *         src,
                  const uint32_t *         mask,
                  int                      width)
{
    if (mask)
	vmx_combine_in_u_mask (dest, src, mask, width);
    else
	vmx_combine_in_u_no_mask (dest, src, width);
}

static void
vmx_combine_in_reverse_u_no_mask (uint32_t *      dest,
                                  const uint32_t *src,
                                  int             width)
{
    int i;
    vector unsigned int vdest, vsrc;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKS (dest, src);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORS (dest, src);

	vdest = pix_multiply (vdest, splat_alpha (vsrc));

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t d = dest[i];
	uint32_t a = ALPHA_8 (src[i]);

	UN8x4_MUL_UN8 (d, a);

	dest[i] = d;
    }
}

static void
vmx_combine_in_reverse_u_mask (uint32_t *      dest,
                               const uint32_t *src,
                               const uint32_t *mask,
                               int             width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, mask_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORSM (dest, src, mask);

	vdest = pix_multiply (vdest, splat_alpha (vsrc));

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
	mask += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t m = ALPHA_8 (mask[i]);
	uint32_t d = dest[i];
	uint32_t a = src[i];

	UN8x4_MUL_UN8 (a, m);
	a = ALPHA_8 (a);
	UN8x4_MUL_UN8 (d, a);

	dest[i] = d;
    }
}

static void
vmx_combine_in_reverse_u (pixman_implementation_t *imp,
                          pixman_op_t              op,
                          uint32_t *               dest,
                          const uint32_t *         src,
                          const uint32_t *         mask,
                          int                      width)
{
    if (mask)
	vmx_combine_in_reverse_u_mask (dest, src, mask, width);
    else
	vmx_combine_in_reverse_u_no_mask (dest, src, width);
}

static void
vmx_combine_out_u_no_mask (uint32_t *      dest,
                           const uint32_t *src,
                           int             width)
{
    int i;
    vector unsigned int vdest, vsrc;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKS (dest, src);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORS (dest, src);

	vdest = pix_multiply (vsrc, splat_alpha (negate (vdest)));

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t s = src[i];
	uint32_t a = ALPHA_8 (~dest[i]);

	UN8x4_MUL_UN8 (s, a);

	dest[i] = s;
    }
}

static void
vmx_combine_out_u_mask (uint32_t *      dest,
                        const uint32_t *src,
                        const uint32_t *mask,
                        int             width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, mask_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORSM (dest, src, mask);

	vdest = pix_multiply (vsrc, splat_alpha (negate (vdest)));

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
	mask += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t m = ALPHA_8 (mask[i]);
	uint32_t s = src[i];
	uint32_t a = ALPHA_8 (~dest[i]);

	UN8x4_MUL_UN8 (s, m);
	UN8x4_MUL_UN8 (s, a);

	dest[i] = s;
    }
}

static void
vmx_combine_out_u (pixman_implementation_t *imp,
                   pixman_op_t              op,
                   uint32_t *               dest,
                   const uint32_t *         src,
                   const uint32_t *         mask,
                   int                      width)
{
    if (mask)
	vmx_combine_out_u_mask (dest, src, mask, width);
    else
	vmx_combine_out_u_no_mask (dest, src, width);
}

static void
vmx_combine_out_reverse_u_no_mask (uint32_t *      dest,
                                   const uint32_t *src,
                                   int             width)
{
    int i;
    vector unsigned int vdest, vsrc;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKS (dest, src);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {

	LOAD_VECTORS (dest, src);

	vdest = pix_multiply (vdest, splat_alpha (negate (vsrc)));

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t d = dest[i];
	uint32_t a = ALPHA_8 (~src[i]);

	UN8x4_MUL_UN8 (d, a);

	dest[i] = d;
    }
}

static void
vmx_combine_out_reverse_u_mask (uint32_t *      dest,
                                const uint32_t *src,
                                const uint32_t *mask,
                                int             width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, mask_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORSM (dest, src, mask);

	vdest = pix_multiply (vdest, splat_alpha (negate (vsrc)));

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
	mask += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t m = ALPHA_8 (mask[i]);
	uint32_t d = dest[i];
	uint32_t a = src[i];

	UN8x4_MUL_UN8 (a, m);
	a = ALPHA_8 (~a);
	UN8x4_MUL_UN8 (d, a);

	dest[i] = d;
    }
}

static void
vmx_combine_out_reverse_u (pixman_implementation_t *imp,
                           pixman_op_t              op,
                           uint32_t *               dest,
                           const uint32_t *         src,
                           const uint32_t *         mask,
                           int                      width)
{
    if (mask)
	vmx_combine_out_reverse_u_mask (dest, src, mask, width);
    else
	vmx_combine_out_reverse_u_no_mask (dest, src, width);
}

static void
vmx_combine_atop_u_no_mask (uint32_t *      dest,
                            const uint32_t *src,
                            int             width)
{
    int i;
    vector unsigned int vdest, vsrc;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKS (dest, src);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORS (dest, src);

	vdest = pix_add_mul (vsrc, splat_alpha (vdest),
			     vdest, splat_alpha (negate (vsrc)));

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t s = src[i];
	uint32_t d = dest[i];
	uint32_t dest_a = ALPHA_8 (d);
	uint32_t src_ia = ALPHA_8 (~s);

	UN8x4_MUL_UN8_ADD_UN8x4_MUL_UN8 (s, dest_a, d, src_ia);

	dest[i] = s;
    }
}

static void
vmx_combine_atop_u_mask (uint32_t *      dest,
                         const uint32_t *src,
                         const uint32_t *mask,
                         int             width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, mask_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORSM (dest, src, mask);

	vdest = pix_add_mul (vsrc, splat_alpha (vdest),
			     vdest, splat_alpha (negate (vsrc)));

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
	mask += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t m = ALPHA_8 (mask[i]);
	uint32_t s = src[i];
	uint32_t d = dest[i];
	uint32_t dest_a = ALPHA_8 (d);
	uint32_t src_ia;

	UN8x4_MUL_UN8 (s, m);

	src_ia = ALPHA_8 (~s);

	UN8x4_MUL_UN8_ADD_UN8x4_MUL_UN8 (s, dest_a, d, src_ia);

	dest[i] = s;
    }
}

static void
vmx_combine_atop_u (pixman_implementation_t *imp,
                    pixman_op_t              op,
                    uint32_t *               dest,
                    const uint32_t *         src,
                    const uint32_t *         mask,
                    int                      width)
{
    if (mask)
	vmx_combine_atop_u_mask (dest, src, mask, width);
    else
	vmx_combine_atop_u_no_mask (dest, src, width);
}

static void
vmx_combine_atop_reverse_u_no_mask (uint32_t *      dest,
                                    const uint32_t *src,
                                    int             width)
{
    int i;
    vector unsigned int vdest, vsrc;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKS (dest, src);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORS (dest, src);

	vdest = pix_add_mul (vdest, splat_alpha (vsrc),
			     vsrc, splat_alpha (negate (vdest)));

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t s = src[i];
	uint32_t d = dest[i];
	uint32_t src_a = ALPHA_8 (s);
	uint32_t dest_ia = ALPHA_8 (~d);

	UN8x4_MUL_UN8_ADD_UN8x4_MUL_UN8 (s, dest_ia, d, src_a);

	dest[i] = s;
    }
}

static void
vmx_combine_atop_reverse_u_mask (uint32_t *      dest,
                                 const uint32_t *src,
                                 const uint32_t *mask,
                                 int             width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, mask_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORSM (dest, src, mask);

	vdest = pix_add_mul (vdest, splat_alpha (vsrc),
			     vsrc, splat_alpha (negate (vdest)));

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
	mask += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t m = ALPHA_8 (mask[i]);
	uint32_t s = src[i];
	uint32_t d = dest[i];
	uint32_t src_a;
	uint32_t dest_ia = ALPHA_8 (~d);

	UN8x4_MUL_UN8 (s, m);

	src_a = ALPHA_8 (s);

	UN8x4_MUL_UN8_ADD_UN8x4_MUL_UN8 (s, dest_ia, d, src_a);

	dest[i] = s;
    }
}

static void
vmx_combine_atop_reverse_u (pixman_implementation_t *imp,
                            pixman_op_t              op,
                            uint32_t *               dest,
                            const uint32_t *         src,
                            const uint32_t *         mask,
                            int                      width)
{
    if (mask)
	vmx_combine_atop_reverse_u_mask (dest, src, mask, width);
    else
	vmx_combine_atop_reverse_u_no_mask (dest, src, width);
}

static void
vmx_combine_xor_u_no_mask (uint32_t *      dest,
                           const uint32_t *src,
                           int             width)
{
    int i;
    vector unsigned int vdest, vsrc;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKS (dest, src);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORS (dest, src);

	vdest = pix_add_mul (vsrc, splat_alpha (negate (vdest)),
			     vdest, splat_alpha (negate (vsrc)));

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t s = src[i];
	uint32_t d = dest[i];
	uint32_t src_ia = ALPHA_8 (~s);
	uint32_t dest_ia = ALPHA_8 (~d);

	UN8x4_MUL_UN8_ADD_UN8x4_MUL_UN8 (s, dest_ia, d, src_ia);

	dest[i] = s;
    }
}

static void
vmx_combine_xor_u_mask (uint32_t *      dest,
                        const uint32_t *src,
                        const uint32_t *mask,
                        int             width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, mask_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORSM (dest, src, mask);

	vdest = pix_add_mul (vsrc, splat_alpha (negate (vdest)),
			     vdest, splat_alpha (negate (vsrc)));

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
	mask += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t m = ALPHA_8 (mask[i]);
	uint32_t s = src[i];
	uint32_t d = dest[i];
	uint32_t src_ia;
	uint32_t dest_ia = ALPHA_8 (~d);

	UN8x4_MUL_UN8 (s, m);

	src_ia = ALPHA_8 (~s);

	UN8x4_MUL_UN8_ADD_UN8x4_MUL_UN8 (s, dest_ia, d, src_ia);

	dest[i] = s;
    }
}

static void
vmx_combine_xor_u (pixman_implementation_t *imp,
                   pixman_op_t              op,
                   uint32_t *               dest,
                   const uint32_t *         src,
                   const uint32_t *         mask,
                   int                      width)
{
    if (mask)
	vmx_combine_xor_u_mask (dest, src, mask, width);
    else
	vmx_combine_xor_u_no_mask (dest, src, width);
}

static void
vmx_combine_add_u_no_mask (uint32_t *      dest,
                           const uint32_t *src,
                           int             width)
{
    int i;
    vector unsigned int vdest, vsrc;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKS (dest, src);
    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORS (dest, src);

	vdest = pix_add (vsrc, vdest);

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t s = src[i];
	uint32_t d = dest[i];

	UN8x4_ADD_UN8x4 (d, s);

	dest[i] = d;
    }
}

static void
vmx_combine_add_u_mask (uint32_t *      dest,
                        const uint32_t *src,
                        const uint32_t *mask,
                        int             width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, mask_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORSM (dest, src, mask);

	vdest = pix_add (vsrc, vdest);

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
	mask += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t m = ALPHA_8 (mask[i]);
	uint32_t s = src[i];
	uint32_t d = dest[i];

	UN8x4_MUL_UN8 (s, m);
	UN8x4_ADD_UN8x4 (d, s);

	dest[i] = d;
    }
}

static void
vmx_combine_add_u (pixman_implementation_t *imp,
                   pixman_op_t              op,
                   uint32_t *               dest,
                   const uint32_t *         src,
                   const uint32_t *         mask,
                   int                      width)
{
    if (mask)
	vmx_combine_add_u_mask (dest, src, mask, width);
    else
	vmx_combine_add_u_no_mask (dest, src, width);
}

static void
vmx_combine_src_ca (pixman_implementation_t *imp,
                    pixman_op_t              op,
                    uint32_t *               dest,
                    const uint32_t *         src,
                    const uint32_t *         mask,
                    int                      width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORSC (dest, src, mask);

	vdest = pix_multiply (vsrc, vmask);

	STORE_VECTOR (dest);

	mask += 4;
	src += 4;
	dest += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t a = mask[i];
	uint32_t s = src[i];

	UN8x4_MUL_UN8x4 (s, a);

	dest[i] = s;
    }
}

static void
vmx_combine_over_ca (pixman_implementation_t *imp,
                     pixman_op_t              op,
                     uint32_t *               dest,
                     const uint32_t *         src,
                     const uint32_t *         mask,
                     int                      width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORSC (dest, src, mask);

	vdest = in_over (vsrc, splat_alpha (vsrc), vmask, vdest);

	STORE_VECTOR (dest);

	mask += 4;
	src += 4;
	dest += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t a = mask[i];
	uint32_t s = src[i];
	uint32_t d = dest[i];
	uint32_t sa = ALPHA_8 (s);

	UN8x4_MUL_UN8x4 (s, a);
	UN8x4_MUL_UN8 (a, sa);
	UN8x4_MUL_UN8x4_ADD_UN8x4 (d, ~a, s);

	dest[i] = d;
    }
}

static void
vmx_combine_over_reverse_ca (pixman_implementation_t *imp,
                             pixman_op_t              op,
                             uint32_t *               dest,
                             const uint32_t *         src,
                             const uint32_t *         mask,
                             int                      width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORSC (dest, src, mask);

	vdest = over (vdest, splat_alpha (vdest), pix_multiply (vsrc, vmask));

	STORE_VECTOR (dest);

	mask += 4;
	src += 4;
	dest += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t a = mask[i];
	uint32_t s = src[i];
	uint32_t d = dest[i];
	uint32_t ida = ALPHA_8 (~d);

	UN8x4_MUL_UN8x4 (s, a);
	UN8x4_MUL_UN8_ADD_UN8x4 (s, ida, d);

	dest[i] = s;
    }
}

static void
vmx_combine_in_ca (pixman_implementation_t *imp,
                   pixman_op_t              op,
                   uint32_t *               dest,
                   const uint32_t *         src,
                   const uint32_t *         mask,
                   int                      width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORSC (dest, src, mask);

	vdest = pix_multiply (pix_multiply (vsrc, vmask), splat_alpha (vdest));

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
	mask += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t a = mask[i];
	uint32_t s = src[i];
	uint32_t da = ALPHA_8 (dest[i]);

	UN8x4_MUL_UN8x4 (s, a);
	UN8x4_MUL_UN8 (s, da);

	dest[i] = s;
    }
}

static void
vmx_combine_in_reverse_ca (pixman_implementation_t *imp,
                           pixman_op_t              op,
                           uint32_t *               dest,
                           const uint32_t *         src,
                           const uint32_t *         mask,
                           int                      width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {

	LOAD_VECTORSC (dest, src, mask);

	vdest = pix_multiply (vdest, pix_multiply (vmask, splat_alpha (vsrc)));

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
	mask += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t a = mask[i];
	uint32_t d = dest[i];
	uint32_t sa = ALPHA_8 (src[i]);

	UN8x4_MUL_UN8 (a, sa);
	UN8x4_MUL_UN8x4 (d, a);

	dest[i] = d;
    }
}

static void
vmx_combine_out_ca (pixman_implementation_t *imp,
                    pixman_op_t              op,
                    uint32_t *               dest,
                    const uint32_t *         src,
                    const uint32_t *         mask,
                    int                      width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORSC (dest, src, mask);

	vdest = pix_multiply (
	    pix_multiply (vsrc, vmask), splat_alpha (negate (vdest)));

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
	mask += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t a = mask[i];
	uint32_t s = src[i];
	uint32_t d = dest[i];
	uint32_t da = ALPHA_8 (~d);

	UN8x4_MUL_UN8x4 (s, a);
	UN8x4_MUL_UN8 (s, da);

	dest[i] = s;
    }
}

static void
vmx_combine_out_reverse_ca (pixman_implementation_t *imp,
                            pixman_op_t              op,
                            uint32_t *               dest,
                            const uint32_t *         src,
                            const uint32_t *         mask,
                            int                      width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORSC (dest, src, mask);

	vdest = pix_multiply (
	    vdest, negate (pix_multiply (vmask, splat_alpha (vsrc))));

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
	mask += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t a = mask[i];
	uint32_t s = src[i];
	uint32_t d = dest[i];
	uint32_t sa = ALPHA_8 (s);

	UN8x4_MUL_UN8 (a, sa);
	UN8x4_MUL_UN8x4 (d, ~a);

	dest[i] = d;
    }
}

static void
vmx_combine_atop_ca (pixman_implementation_t *imp,
                     pixman_op_t              op,
                     uint32_t *               dest,
                     const uint32_t *         src,
                     const uint32_t *         mask,
                     int                      width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask, vsrca;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORSC (dest, src, mask);

	vsrca = splat_alpha (vsrc);

	vsrc = pix_multiply (vsrc, vmask);
	vmask = pix_multiply (vmask, vsrca);

	vdest = pix_add_mul (vsrc, splat_alpha (vdest),
			     negate (vmask), vdest);

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
	mask += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t a = mask[i];
	uint32_t s = src[i];
	uint32_t d = dest[i];
	uint32_t sa = ALPHA_8 (s);
	uint32_t da = ALPHA_8 (d);

	UN8x4_MUL_UN8x4 (s, a);
	UN8x4_MUL_UN8 (a, sa);
	UN8x4_MUL_UN8x4_ADD_UN8x4_MUL_UN8 (d, ~a, s, da);

	dest[i] = d;
    }
}

static void
vmx_combine_atop_reverse_ca (pixman_implementation_t *imp,
                             pixman_op_t              op,
                             uint32_t *               dest,
                             const uint32_t *         src,
                             const uint32_t *         mask,
                             int                      width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORSC (dest, src, mask);

	vdest = pix_add_mul (vdest,
			     pix_multiply (vmask, splat_alpha (vsrc)),
			     pix_multiply (vsrc, vmask),
			     negate (splat_alpha (vdest)));

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
	mask += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t a = mask[i];
	uint32_t s = src[i];
	uint32_t d = dest[i];
	uint32_t sa = ALPHA_8 (s);
	uint32_t da = ALPHA_8 (~d);

	UN8x4_MUL_UN8x4 (s, a);
	UN8x4_MUL_UN8 (a, sa);
	UN8x4_MUL_UN8x4_ADD_UN8x4_MUL_UN8 (d, a, s, da);

	dest[i] = d;
    }
}

static void
vmx_combine_xor_ca (pixman_implementation_t *imp,
                    pixman_op_t              op,
                    uint32_t *               dest,
                    const uint32_t *         src,
                    const uint32_t *         mask,
                    int                      width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORSC (dest, src, mask);

	vdest = pix_add_mul (vdest,
			     negate (pix_multiply (vmask, splat_alpha (vsrc))),
			     pix_multiply (vsrc, vmask),
			     negate (splat_alpha (vdest)));

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
	mask += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t a = mask[i];
	uint32_t s = src[i];
	uint32_t d = dest[i];
	uint32_t sa = ALPHA_8 (s);
	uint32_t da = ALPHA_8 (~d);

	UN8x4_MUL_UN8x4 (s, a);
	UN8x4_MUL_UN8 (a, sa);
	UN8x4_MUL_UN8x4_ADD_UN8x4_MUL_UN8 (d, ~a, s, da);

	dest[i] = d;
    }
}

static void
vmx_combine_add_ca (pixman_implementation_t *imp,
                    pixman_op_t              op,
                    uint32_t *               dest,
                    const uint32_t *         src,
                    const uint32_t *         mask,
                    int                      width)
{
    int i;
    vector unsigned int vdest, vsrc, vmask;
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;

    COMPUTE_SHIFT_MASKC (dest, src, mask);

    /* printf ("%s\n",__PRETTY_FUNCTION__); */
    for (i = width / 4; i > 0; i--)
    {
	LOAD_VECTORSC (dest, src, mask);

	vdest = pix_add (pix_multiply (vsrc, vmask), vdest);

	STORE_VECTOR (dest);

	src += 4;
	dest += 4;
	mask += 4;
    }

    for (i = width % 4; --i >= 0;)
    {
	uint32_t a = mask[i];
	uint32_t s = src[i];
	uint32_t d = dest[i];

	UN8x4_MUL_UN8x4 (s, a);
	UN8x4_ADD_UN8x4 (s, d);

	dest[i] = s;
    }
}

static const pixman_fast_path_t vmx_fast_paths[] =
{
    {   PIXMAN_OP_NONE	},
};

pixman_implementation_t *
_pixman_implementation_create_vmx (pixman_implementation_t *fallback)
{
    pixman_implementation_t *imp = _pixman_implementation_create (fallback, vmx_fast_paths);

    /* Set up function pointers */

    imp->combine_32[PIXMAN_OP_OVER] = vmx_combine_over_u;
    imp->combine_32[PIXMAN_OP_OVER_REVERSE] = vmx_combine_over_reverse_u;
    imp->combine_32[PIXMAN_OP_IN] = vmx_combine_in_u;
    imp->combine_32[PIXMAN_OP_IN_REVERSE] = vmx_combine_in_reverse_u;
    imp->combine_32[PIXMAN_OP_OUT] = vmx_combine_out_u;
    imp->combine_32[PIXMAN_OP_OUT_REVERSE] = vmx_combine_out_reverse_u;
    imp->combine_32[PIXMAN_OP_ATOP] = vmx_combine_atop_u;
    imp->combine_32[PIXMAN_OP_ATOP_REVERSE] = vmx_combine_atop_reverse_u;
    imp->combine_32[PIXMAN_OP_XOR] = vmx_combine_xor_u;

    imp->combine_32[PIXMAN_OP_ADD] = vmx_combine_add_u;

    imp->combine_32_ca[PIXMAN_OP_SRC] = vmx_combine_src_ca;
    imp->combine_32_ca[PIXMAN_OP_OVER] = vmx_combine_over_ca;
    imp->combine_32_ca[PIXMAN_OP_OVER_REVERSE] = vmx_combine_over_reverse_ca;
    imp->combine_32_ca[PIXMAN_OP_IN] = vmx_combine_in_ca;
    imp->combine_32_ca[PIXMAN_OP_IN_REVERSE] = vmx_combine_in_reverse_ca;
    imp->combine_32_ca[PIXMAN_OP_OUT] = vmx_combine_out_ca;
    imp->combine_32_ca[PIXMAN_OP_OUT_REVERSE] = vmx_combine_out_reverse_ca;
    imp->combine_32_ca[PIXMAN_OP_ATOP] = vmx_combine_atop_ca;
    imp->combine_32_ca[PIXMAN_OP_ATOP_REVERSE] = vmx_combine_atop_reverse_ca;
    imp->combine_32_ca[PIXMAN_OP_XOR] = vmx_combine_xor_ca;
    imp->combine_32_ca[PIXMAN_OP_ADD] = vmx_combine_add_ca;

    return imp;
}
@


1.8
log
@Update to pixman 0.34.0.
@
text
@a32 1
#include "pixman-inlines.h"
a36 7
static vector unsigned int mask_ff000000;
static vector unsigned int mask_red;
static vector unsigned int mask_green;
static vector unsigned int mask_blue;
static vector unsigned int mask_565_fix_rb;
static vector unsigned int mask_565_fix_g;

a39 1
#ifdef WORDS_BIGENDIAN
a43 15
#else
    return vec_perm (pix, pix,
		     (vector unsigned char)AVV (
			 0x03, 0x03, 0x03, 0x03, 0x07, 0x07, 0x07, 0x07,
			 0x0B, 0x0B, 0x0B, 0x0B, 0x0F, 0x0F, 0x0F, 0x0F));
#endif
}

static force_inline vector unsigned int
splat_pixel (vector unsigned int pix)
{
    return vec_perm (pix, pix,
		     (vector unsigned char)AVV (
			 0x00, 0x00, 0x00, 0x00, 0x01, 0x01, 0x01, 0x01,
			 0x02, 0x02, 0x02, 0x02, 0x03, 0x03, 0x03, 0x03));
a52 1
#ifdef WORDS_BIGENDIAN
a54 4
#else
	vec_mergeh ((vector unsigned char) p,
		    (vector unsigned char) AVV (0));
#endif
a56 1
#ifdef WORDS_BIGENDIAN
a58 4
#else
	vec_mergeh ((vector unsigned char) a,
		    (vector unsigned char) AVV (0));
#endif
a69 1
#ifdef WORDS_BIGENDIAN
a71 5
#else
	vec_mergel ((vector unsigned char) p,
		    (vector unsigned char) AVV (0));
#endif

a72 1
#ifdef WORDS_BIGENDIAN
a74 4
#else
	vec_mergel ((vector unsigned char) a,
		    (vector unsigned char) AVV (0));
#endif
a131 1
#ifdef WORDS_BIGENDIAN
d137 3
a139 1
    source ## _mask = vec_lvsl (0, source);
d143 7
a149 1
    source ## _mask = vec_lvsl (0, source);
d151 1
a151 4
#define LOAD_VECTOR(source)				  \
do							  \
{							  \
    vector unsigned char tmp1, tmp2;			  \
d154 2
a155 1
    v ## source = (typeof(v ## source)) 		  \
d157 3
a159 8
} while (0)

#define LOAD_VECTORS(dest, source)			  \
do							  \
{							  \
    LOAD_VECTOR(source);				  \
    v ## dest = (typeof(v ## dest))vec_ld (0, dest);	  \
} while (0)
d162 12
a173 37
do							  \
{							  \
    LOAD_VECTORS(dest, source); 			  \
    LOAD_VECTOR(mask);					  \
} while (0)

#define DECLARE_SRC_MASK_VAR vector unsigned char src_mask
#define DECLARE_MASK_MASK_VAR vector unsigned char mask_mask

#else

/* Now the COMPUTE_SHIFT_{MASK, MASKS, MASKC} below are just no-op.
 * They are defined that way because little endian altivec can do unaligned
 * reads natively and have no need for constructing the permutation pattern
 * variables.
 */
#define COMPUTE_SHIFT_MASK(source)

#define COMPUTE_SHIFT_MASKS(dest, source)

#define COMPUTE_SHIFT_MASKC(dest, source, mask)

# define LOAD_VECTOR(source)				\
    v ## source = *((typeof(v ## source)*)source);

# define LOAD_VECTORS(dest, source)			\
    LOAD_VECTOR(source);				\
    LOAD_VECTOR(dest);					\

# define LOAD_VECTORSC(dest, source, mask)		\
    LOAD_VECTORS(dest, source); 			\
    LOAD_VECTOR(mask);					\

#define DECLARE_SRC_MASK_VAR
#define DECLARE_MASK_MASK_VAR

#endif /* WORDS_BIGENDIAN */
d176 1
a176 1
    LOAD_VECTORSC (dest, source, mask); 				\
d181 5
a185 236
    vec_st ((vector unsigned int) v ## dest, 0, dest);

/* load 4 pixels from a 16-byte boundary aligned address */
static force_inline vector unsigned int
load_128_aligned (const uint32_t* src)
{
    return *((vector unsigned int *) src);
}

/* load 4 pixels from a unaligned address */
static force_inline vector unsigned int
load_128_unaligned (const uint32_t* src)
{
    vector unsigned int vsrc;
    DECLARE_SRC_MASK_VAR;

    COMPUTE_SHIFT_MASK (src);
    LOAD_VECTOR (src);

    return vsrc;
}

/* save 4 pixels on a 16-byte boundary aligned address */
static force_inline void
save_128_aligned (uint32_t* data,
		  vector unsigned int vdata)
{
    STORE_VECTOR(data)
}

static force_inline vector unsigned int
create_mask_1x32_128 (const uint32_t *src)
{
    vector unsigned int vsrc;
    DECLARE_SRC_MASK_VAR;

    COMPUTE_SHIFT_MASK (src);
    LOAD_VECTOR (src);
    return vec_splat(vsrc, 0);
}

static force_inline vector unsigned int
create_mask_32_128 (uint32_t mask)
{
    return create_mask_1x32_128(&mask);
}

static force_inline vector unsigned int
unpacklo_128_16x8 (vector unsigned int data1, vector unsigned int data2)
{
    vector unsigned char lo;

    /* unpack to short */
    lo = (vector unsigned char)
#ifdef WORDS_BIGENDIAN
	vec_mergel ((vector unsigned char) data2,
		    (vector unsigned char) data1);
#else
	vec_mergel ((vector unsigned char) data1,
		    (vector unsigned char) data2);
#endif

    return (vector unsigned int) lo;
}

static force_inline vector unsigned int
unpackhi_128_16x8 (vector unsigned int data1, vector unsigned int data2)
{
    vector unsigned char hi;

    /* unpack to short */
    hi = (vector unsigned char)
#ifdef WORDS_BIGENDIAN
	vec_mergeh ((vector unsigned char) data2,
		    (vector unsigned char) data1);
#else
	vec_mergeh ((vector unsigned char) data1,
		    (vector unsigned char) data2);
#endif

    return (vector unsigned int) hi;
}

static force_inline vector unsigned int
unpacklo_128_8x16 (vector unsigned int data1, vector unsigned int data2)
{
    vector unsigned short lo;

    /* unpack to char */
    lo = (vector unsigned short)
#ifdef WORDS_BIGENDIAN
	vec_mergel ((vector unsigned short) data2,
		    (vector unsigned short) data1);
#else
	vec_mergel ((vector unsigned short) data1,
		    (vector unsigned short) data2);
#endif

    return (vector unsigned int) lo;
}

static force_inline vector unsigned int
unpackhi_128_8x16 (vector unsigned int data1, vector unsigned int data2)
{
    vector unsigned short hi;

    /* unpack to char */
    hi = (vector unsigned short)
#ifdef WORDS_BIGENDIAN
	vec_mergeh ((vector unsigned short) data2,
		    (vector unsigned short) data1);
#else
	vec_mergeh ((vector unsigned short) data1,
		    (vector unsigned short) data2);
#endif

    return (vector unsigned int) hi;
}

static force_inline void
unpack_128_2x128 (vector unsigned int data1, vector unsigned int data2,
		    vector unsigned int* data_lo, vector unsigned int* data_hi)
{
    *data_lo = unpacklo_128_16x8(data1, data2);
    *data_hi = unpackhi_128_16x8(data1, data2);
}

static force_inline void
unpack_128_2x128_16 (vector unsigned int data1, vector unsigned int data2,
		    vector unsigned int* data_lo, vector unsigned int* data_hi)
{
    *data_lo = unpacklo_128_8x16(data1, data2);
    *data_hi = unpackhi_128_8x16(data1, data2);
}

static force_inline vector unsigned int
unpack_565_to_8888 (vector unsigned int lo)
{
    vector unsigned int r, g, b, rb, t;

    r = vec_and (vec_sl(lo, create_mask_32_128(8)), mask_red);
    g = vec_and (vec_sl(lo, create_mask_32_128(5)), mask_green);
    b = vec_and (vec_sl(lo, create_mask_32_128(3)), mask_blue);

    rb = vec_or (r, b);
    t  = vec_and (rb, mask_565_fix_rb);
    t  = vec_sr (t, create_mask_32_128(5));
    rb = vec_or (rb, t);

    t  = vec_and (g, mask_565_fix_g);
    t  = vec_sr (t, create_mask_32_128(6));
    g  = vec_or (g, t);

    return vec_or (rb, g);
}

static force_inline int
is_opaque (vector unsigned int x)
{
    uint32_t cmp_result;
    vector bool int ffs = vec_cmpeq(x, x);

    cmp_result = vec_all_eq(x, ffs);

    return (cmp_result & 0x8888) == 0x8888;
}

static force_inline int
is_zero (vector unsigned int x)
{
    uint32_t cmp_result;

    cmp_result = vec_all_eq(x, (vector unsigned int) AVV(0));

    return cmp_result == 0xffff;
}

static force_inline int
is_transparent (vector unsigned int x)
{
    uint32_t cmp_result;

    cmp_result = vec_all_eq(x, (vector unsigned int) AVV(0));
    return (cmp_result & 0x8888) == 0x8888;
}

static force_inline uint32_t
core_combine_over_u_pixel_vmx (uint32_t src, uint32_t dst)
{
    uint32_t a;

    a = ALPHA_8(src);

    if (a == 0xff)
    {
	return src;
    }
    else if (src)
    {
	UN8x4_MUL_UN8_ADD_UN8x4(dst, (~a & MASK), src);
    }

    return dst;
}

static force_inline uint32_t
combine1 (const uint32_t *ps, const uint32_t *pm)
{
    uint32_t s = *ps;

    if (pm)
	UN8x4_MUL_UN8(s, ALPHA_8(*pm));

    return s;
}

static force_inline vector unsigned int
combine4 (const uint32_t* ps, const uint32_t* pm)
{
    vector unsigned int src, msk;

    if (pm)
    {
	msk = load_128_unaligned(pm);

	if (is_transparent(msk))
	    return (vector unsigned int) AVV(0);
    }

    src = load_128_unaligned(ps);

    if (pm)
	src = pix_multiply(src, msk);

    return src;
}
d194 2
a195 13
    DECLARE_SRC_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t s = *src++;
	uint32_t d = *dest;
	uint32_t ia = ALPHA_8 (~s);

	UN8x4_MUL_UN8_ADD_UN8x4 (d, ia, s);

	*dest++ = d;
	width--;
    }
d233 2
a234 18
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t m = ALPHA_8 (*mask++);
	uint32_t s = *src++;
	uint32_t d = *dest;
	uint32_t ia;

	UN8x4_MUL_UN8 (s, m);

	ia = ALPHA_8 (~s);

	UN8x4_MUL_UN8_ADD_UN8x4 (d, ia, s);
	*dest++ = d;
	width--;
    }
d289 2
a290 12
    DECLARE_SRC_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t s = *src++;
	uint32_t d = *dest;
	uint32_t ia = ALPHA_8 (~d);

	UN8x4_MUL_UN8_ADD_UN8x4 (s, ia, d);
	*dest++ = s;
	width--;
    }
d327 2
a328 16
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t m = ALPHA_8 (*mask++);
	uint32_t s = *src++;
	uint32_t d = *dest;
	uint32_t ia = ALPHA_8 (~d);

	UN8x4_MUL_UN8 (s, m);

	UN8x4_MUL_UN8_ADD_UN8x4 (s, ia, d);
	*dest++ = s;
	width--;
    }
d382 2
a383 11
    DECLARE_SRC_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t s = *src++;
	uint32_t a = ALPHA_8 (*dest);

	UN8x4_MUL_UN8 (s, a);
	*dest++ = s;
	width--;
    }
d418 2
a419 15
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t m = ALPHA_8 (*mask++);
	uint32_t s = *src++;
	uint32_t a = ALPHA_8 (*dest);

	UN8x4_MUL_UN8 (s, m);
	UN8x4_MUL_UN8 (s, a);

	*dest++ = s;
	width--;
    }
d471 2
a472 12
    DECLARE_SRC_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t d = *dest;
	uint32_t a = ALPHA_8 (*src++);

	UN8x4_MUL_UN8 (d, a);

	*dest++ = d;
	width--;
    }
d508 2
a509 16
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t m = ALPHA_8 (*mask++);
	uint32_t d = *dest;
	uint32_t a = *src++;

	UN8x4_MUL_UN8 (a, m);
	a = ALPHA_8 (a);
	UN8x4_MUL_UN8 (d, a);

	*dest++ = d;
	width--;
    }
d562 2
a563 12
    DECLARE_SRC_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t s = *src++;
	uint32_t a = ALPHA_8 (~(*dest));

	UN8x4_MUL_UN8 (s, a);

	*dest++ = s;
	width--;
    }
d599 2
a600 15
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t m = ALPHA_8 (*mask++);
	uint32_t s = *src++;
	uint32_t a = ALPHA_8 (~(*dest));

	UN8x4_MUL_UN8 (s, m);
	UN8x4_MUL_UN8 (s, a);

	*dest++ = s;
	width--;
    }
d652 2
a653 12
    DECLARE_SRC_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t d = *dest;
	uint32_t a = ALPHA_8 (~(*src++));

	UN8x4_MUL_UN8 (d, a);

	*dest++ = d;
	width--;
    }
d690 2
a691 16
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t m = ALPHA_8 (*mask++);
	uint32_t d = *dest;
	uint32_t a = *src++;

	UN8x4_MUL_UN8 (a, m);
	a = ALPHA_8 (~a);
	UN8x4_MUL_UN8 (d, a);

	*dest++ = d;
	width--;
    }
d744 2
a745 14
    DECLARE_SRC_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t s = *src++;
	uint32_t d = *dest;
	uint32_t dest_a = ALPHA_8 (d);
	uint32_t src_ia = ALPHA_8 (~s);

	UN8x4_MUL_UN8_ADD_UN8x4_MUL_UN8 (s, dest_a, d, src_ia);

	*dest++ = s;
	width--;
    }
d784 2
a785 20
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t m = ALPHA_8 (*mask++);
	uint32_t s = *src++;
	uint32_t d = *dest;
	uint32_t dest_a = ALPHA_8 (d);
	uint32_t src_ia;

	UN8x4_MUL_UN8 (s, m);

	src_ia = ALPHA_8 (~s);

	UN8x4_MUL_UN8_ADD_UN8x4_MUL_UN8 (s, dest_a, d, src_ia);

	*dest++ = s;
	width--;
    }
d843 2
a844 14
    DECLARE_SRC_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t s = *src++;
	uint32_t d = *dest;
	uint32_t src_a = ALPHA_8 (s);
	uint32_t dest_ia = ALPHA_8 (~d);

	UN8x4_MUL_UN8_ADD_UN8x4_MUL_UN8 (s, dest_ia, d, src_a);

	*dest++ = s;
	width--;
    }
d883 2
a884 20
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t m = ALPHA_8 (*mask++);
	uint32_t s = *src++;
	uint32_t d = *dest;
	uint32_t src_a;
	uint32_t dest_ia = ALPHA_8 (~d);

	UN8x4_MUL_UN8 (s, m);

	src_a = ALPHA_8 (s);

	UN8x4_MUL_UN8_ADD_UN8x4_MUL_UN8 (s, dest_ia, d, src_a);

	*dest++ = s;
	width--;
    }
d942 4
a945 16
    DECLARE_SRC_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t s = *src++;
	uint32_t d = *dest;
	uint32_t src_ia = ALPHA_8 (~s);
	uint32_t dest_ia = ALPHA_8 (~d);

	UN8x4_MUL_UN8_ADD_UN8x4_MUL_UN8 (s, dest_ia, d, src_ia);

	*dest++ = s;
	width--;
    }

    COMPUTE_SHIFT_MASKS (dest, src);
d982 2
a983 20
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t m = ALPHA_8 (*mask++);
	uint32_t s = *src++;
	uint32_t d = *dest;
	uint32_t src_ia;
	uint32_t dest_ia = ALPHA_8 (~d);

	UN8x4_MUL_UN8 (s, m);

	src_ia = ALPHA_8 (~s);

	UN8x4_MUL_UN8_ADD_UN8x4_MUL_UN8 (s, dest_ia, d, src_ia);

	*dest++ = s;
	width--;
    }
d1041 2
a1042 12
    DECLARE_SRC_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t s = *src++;
	uint32_t d = *dest;

	UN8x4_ADD_UN8x4 (d, s);

	*dest++ = d;
	width--;
    }
d1077 2
a1078 15
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t m = ALPHA_8 (*mask++);
	uint32_t s = *src++;
	uint32_t d = *dest;

	UN8x4_MUL_UN8 (s, m);
	UN8x4_ADD_UN8x4 (d, s);

	*dest++ = d;
	width--;
    }
d1133 2
a1134 13
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t a = *mask++;
	uint32_t s = *src++;

	UN8x4_MUL_UN8x4 (s, a);

	*dest++ = s;
	width--;
    }
d1173 2
a1174 17
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t a = *mask++;
	uint32_t s = *src++;
	uint32_t d = *dest;
	uint32_t sa = ALPHA_8 (s);

	UN8x4_MUL_UN8x4 (s, a);
	UN8x4_MUL_UN8 (a, sa);
	UN8x4_MUL_UN8x4_ADD_UN8x4 (d, ~a, s);

	*dest++ = d;
	width--;
    }
d1217 2
a1218 16
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t a = *mask++;
	uint32_t s = *src++;
	uint32_t d = *dest;
	uint32_t ida = ALPHA_8 (~d);

	UN8x4_MUL_UN8x4 (s, a);
	UN8x4_MUL_UN8_ADD_UN8x4 (s, ida, d);

	*dest++ = s;
	width--;
    }
d1260 2
a1261 15
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t a = *mask++;
	uint32_t s = *src++;
	uint32_t da = ALPHA_8 (*dest);

	UN8x4_MUL_UN8x4 (s, a);
	UN8x4_MUL_UN8 (s, da);

	*dest++ = s;
	width--;
    }
d1302 2
a1303 15
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t a = *mask++;
	uint32_t d = *dest;
	uint32_t sa = ALPHA_8 (*src++);

	UN8x4_MUL_UN8 (a, sa);
	UN8x4_MUL_UN8x4 (d, a);

	*dest++ = d;
	width--;
    }
d1345 2
a1346 16
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t a = *mask++;
	uint32_t s = *src++;
	uint32_t d = *dest;
	uint32_t da = ALPHA_8 (~d);

	UN8x4_MUL_UN8x4 (s, a);
	UN8x4_MUL_UN8 (s, da);

	*dest++ = s;
	width--;
    }
d1389 2
a1390 16
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t a = *mask++;
	uint32_t s = *src++;
	uint32_t d = *dest;
	uint32_t sa = ALPHA_8 (s);

	UN8x4_MUL_UN8 (a, sa);
	UN8x4_MUL_UN8x4 (d, ~a);

	*dest++ = d;
	width--;
    }
d1433 2
a1434 18
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t a = *mask++;
	uint32_t s = *src++;
	uint32_t d = *dest;
	uint32_t sa = ALPHA_8 (s);
	uint32_t da = ALPHA_8 (d);

	UN8x4_MUL_UN8x4 (s, a);
	UN8x4_MUL_UN8 (a, sa);
	UN8x4_MUL_UN8x4_ADD_UN8x4_MUL_UN8 (d, ~a, s, da);

	*dest++ = d;
	width--;
    }
d1484 2
a1485 18
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t a = *mask++;
	uint32_t s = *src++;
	uint32_t d = *dest;
	uint32_t sa = ALPHA_8 (s);
	uint32_t da = ALPHA_8 (~d);

	UN8x4_MUL_UN8x4 (s, a);
	UN8x4_MUL_UN8 (a, sa);
	UN8x4_MUL_UN8x4_ADD_UN8x4_MUL_UN8 (d, a, s, da);

	*dest++ = d;
	width--;
    }
d1532 2
a1533 18
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t a = *mask++;
	uint32_t s = *src++;
	uint32_t d = *dest;
	uint32_t sa = ALPHA_8 (s);
	uint32_t da = ALPHA_8 (~d);

	UN8x4_MUL_UN8x4 (s, a);
	UN8x4_MUL_UN8 (a, sa);
	UN8x4_MUL_UN8x4_ADD_UN8x4_MUL_UN8 (d, ~a, s, da);

	*dest++ = d;
	width--;
    }
d1580 2
a1581 15
    DECLARE_SRC_MASK_VAR;
    DECLARE_MASK_MASK_VAR;

    while (width && ((uintptr_t)dest & 15))
    {
	uint32_t a = *mask++;
	uint32_t s = *src++;
	uint32_t d = *dest;

	UN8x4_MUL_UN8x4 (s, a);
	UN8x4_ADD_UN8x4 (s, d);

	*dest++ = s;
	width--;
    }
a1611 664
static void
vmx_composite_over_n_8_8888 (pixman_implementation_t *imp,
                              pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t src, srca;
    uint32_t *dst_line, *dst;
    uint8_t *mask_line;
    int dst_stride, mask_stride;
    int32_t w;
    uint32_t m, d, s, ia;

    vector unsigned int vsrc, valpha, vmask, vdst;

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    srca = ALPHA_8(src);
    if (src == 0)
	return;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	mask_image, mask_x, mask_y, uint8_t, mask_stride, mask_line, 1);

    vsrc = (vector unsigned int) {src, src, src, src};
    valpha = splat_alpha(vsrc);

    while (height--)
    {
	const uint8_t *pm = mask_line;
	dst = dst_line;
	dst_line += dst_stride;
	mask_line += mask_stride;
	w = width;

	while (w && (uintptr_t)dst & 15)
	{
	    s = src;
	    m = *pm++;

	    if (m)
	    {
		d = *dst;
		UN8x4_MUL_UN8 (s, m);
		ia = ALPHA_8 (~s);
		UN8x4_MUL_UN8_ADD_UN8x4 (d, ia, s);
		*dst = d;
	    }

	    w--;
	    dst++;
	}

	while (w >= 4)
	{
	    m = *((uint32_t*)pm);

	    if (srca == 0xff && m == 0xffffffff)
	    {
		save_128_aligned(dst, vsrc);
	    }
	    else if (m)
	    {
		vmask = splat_pixel((vector unsigned int) {m, m, m, m});

		/* dst is 16-byte aligned */
		vdst = in_over (vsrc, valpha, vmask, load_128_aligned (dst));

		save_128_aligned(dst, vdst);
	    }

	    w -= 4;
	    dst += 4;
	    pm += 4;
	}

	while (w)
	{
	    s = src;
	    m = *pm++;

	    if (m)
	    {
		d = *dst;
		UN8x4_MUL_UN8 (s, m);
		ia = ALPHA_8 (~s);
		UN8x4_MUL_UN8_ADD_UN8x4 (d, ia, s);
		*dst = d;
	    }

	    w--;
	    dst++;
	}
    }

}

static pixman_bool_t
vmx_fill (pixman_implementation_t *imp,
           uint32_t *               bits,
           int                      stride,
           int                      bpp,
           int                      x,
           int                      y,
           int                      width,
           int                      height,
           uint32_t		    filler)
{
    uint32_t byte_width;
    uint8_t *byte_line;

    vector unsigned int vfiller;

    if (bpp == 8)
    {
	uint8_t b;
	uint16_t w;

	stride = stride * (int) sizeof (uint32_t) / 1;
	byte_line = (uint8_t *)(((uint8_t *)bits) + stride * y + x);
	byte_width = width;
	stride *= 1;

	b = filler & 0xff;
	w = (b << 8) | b;
	filler = (w << 16) | w;
    }
    else if (bpp == 16)
    {
	stride = stride * (int) sizeof (uint32_t) / 2;
	byte_line = (uint8_t *)(((uint16_t *)bits) + stride * y + x);
	byte_width = 2 * width;
	stride *= 2;

        filler = (filler & 0xffff) * 0x00010001;
    }
    else if (bpp == 32)
    {
	stride = stride * (int) sizeof (uint32_t) / 4;
	byte_line = (uint8_t *)(((uint32_t *)bits) + stride * y + x);
	byte_width = 4 * width;
	stride *= 4;
    }
    else
    {
	return FALSE;
    }

    vfiller = create_mask_1x32_128(&filler);

    while (height--)
    {
	int w;
	uint8_t *d = byte_line;
	byte_line += stride;
	w = byte_width;

	if (w >= 1 && ((uintptr_t)d & 1))
	{
	    *(uint8_t *)d = filler;
	    w -= 1;
	    d += 1;
	}

	while (w >= 2 && ((uintptr_t)d & 3))
	{
	    *(uint16_t *)d = filler;
	    w -= 2;
	    d += 2;
	}

	while (w >= 4 && ((uintptr_t)d & 15))
	{
	    *(uint32_t *)d = filler;

	    w -= 4;
	    d += 4;
	}

	while (w >= 128)
	{
	    vec_st(vfiller, 0, (uint32_t *) d);
	    vec_st(vfiller, 0, (uint32_t *) d + 4);
	    vec_st(vfiller, 0, (uint32_t *) d + 8);
	    vec_st(vfiller, 0, (uint32_t *) d + 12);
	    vec_st(vfiller, 0, (uint32_t *) d + 16);
	    vec_st(vfiller, 0, (uint32_t *) d + 20);
	    vec_st(vfiller, 0, (uint32_t *) d + 24);
	    vec_st(vfiller, 0, (uint32_t *) d + 28);

	    d += 128;
	    w -= 128;
	}

	if (w >= 64)
	{
	    vec_st(vfiller, 0, (uint32_t *) d);
	    vec_st(vfiller, 0, (uint32_t *) d + 4);
	    vec_st(vfiller, 0, (uint32_t *) d + 8);
	    vec_st(vfiller, 0, (uint32_t *) d + 12);

	    d += 64;
	    w -= 64;
	}

	if (w >= 32)
	{
	    vec_st(vfiller, 0, (uint32_t *) d);
	    vec_st(vfiller, 0, (uint32_t *) d + 4);

	    d += 32;
	    w -= 32;
	}

	if (w >= 16)
	{
	    vec_st(vfiller, 0, (uint32_t *) d);

	    d += 16;
	    w -= 16;
	}

	while (w >= 4)
	{
	    *(uint32_t *)d = filler;

	    w -= 4;
	    d += 4;
	}

	if (w >= 2)
	{
	    *(uint16_t *)d = filler;
	    w -= 2;
	    d += 2;
	}

	if (w >= 1)
	{
	    *(uint8_t *)d = filler;
	    w -= 1;
	    d += 1;
	}
    }

    return TRUE;
}

static void
vmx_composite_src_x888_8888 (pixman_implementation_t *imp,
			      pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t    *dst_line, *dst;
    uint32_t    *src_line, *src;
    int32_t w;
    int dst_stride, src_stride;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;
	w = width;

	while (w && (uintptr_t)dst & 15)
	{
	    *dst++ = *src++ | 0xff000000;
	    w--;
	}

	while (w >= 16)
	{
	    vector unsigned int vmx_src1, vmx_src2, vmx_src3, vmx_src4;

	    vmx_src1 = load_128_unaligned (src);
	    vmx_src2 = load_128_unaligned (src + 4);
	    vmx_src3 = load_128_unaligned (src + 8);
	    vmx_src4 = load_128_unaligned (src + 12);

	    save_128_aligned (dst, vec_or (vmx_src1, mask_ff000000));
	    save_128_aligned (dst + 4, vec_or (vmx_src2, mask_ff000000));
	    save_128_aligned (dst + 8, vec_or (vmx_src3, mask_ff000000));
	    save_128_aligned (dst + 12, vec_or (vmx_src4, mask_ff000000));

	    dst += 16;
	    src += 16;
	    w -= 16;
	}

	while (w)
	{
	    *dst++ = *src++ | 0xff000000;
	    w--;
	}
    }
}

static void
vmx_composite_over_n_8888 (pixman_implementation_t *imp,
                           pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t *dst_line, *dst;
    uint32_t src, ia;
    int      i, w, dst_stride;
    vector unsigned int vdst, vsrc, via;

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    if (src == 0)
	return;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);

    vsrc = (vector unsigned int){src, src, src, src};
    via = negate (splat_alpha (vsrc));
    ia = ALPHA_8 (~src);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	w = width;

	while (w && ((uintptr_t)dst & 15))
	{
	    uint32_t d = *dst;
	    UN8x4_MUL_UN8_ADD_UN8x4 (d, ia, src);
	    *dst++ = d;
	    w--;
	}

	for (i = w / 4; i > 0; i--)
	{
	    vdst = pix_multiply (load_128_aligned (dst), via);
	    save_128_aligned (dst, pix_add (vsrc, vdst));
	    dst += 4;
	}

	for (i = w % 4; --i >= 0;)
	{
	    uint32_t d = dst[i];
	    UN8x4_MUL_UN8_ADD_UN8x4 (d, ia, src);
	    dst[i] = d;
	}
    }
}

static void
vmx_composite_over_8888_8888 (pixman_implementation_t *imp,
                               pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    int dst_stride, src_stride;
    uint32_t    *dst_line, *dst;
    uint32_t    *src_line, *src;

    PIXMAN_IMAGE_GET_LINE (
    dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
    src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);

    dst = dst_line;
    src = src_line;

    while (height--)
    {
        vmx_combine_over_u (imp, op, dst, src, NULL, width);

        dst += dst_stride;
        src += src_stride;
    }
}

static void
vmx_composite_over_n_8888_8888_ca (pixman_implementation_t *imp,
                                    pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t src, ia;
    uint32_t    *dst_line, d;
    uint32_t    *mask_line, m;
    uint32_t pack_cmp;
    int dst_stride, mask_stride;

    vector unsigned int vsrc, valpha, vmask, vdest;

    src = _pixman_image_get_solid (imp, src_image, dest_image->bits.format);

    if (src == 0)
	return;

    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	mask_image, mask_x, mask_y, uint32_t, mask_stride, mask_line, 1);

    vsrc = (vector unsigned int) {src, src, src, src};
    valpha = splat_alpha(vsrc);
    ia = ALPHA_8 (src);

    while (height--)
    {
	int w = width;
	const uint32_t *pm = (uint32_t *)mask_line;
	uint32_t *pd = (uint32_t *)dst_line;
	uint32_t s;

	dst_line += dst_stride;
	mask_line += mask_stride;

	while (w && (uintptr_t)pd & 15)
	{
	    s = src;
	    m = *pm++;

	    if (m)
	    {
		d = *pd;
		UN8x4_MUL_UN8x4 (s, m);
		UN8x4_MUL_UN8 (m, ia);
		m = ~m;
		UN8x4_MUL_UN8x4_ADD_UN8x4 (d, m, s);
		*pd = d;
	    }

	    pd++;
	    w--;
	}

	while (w >= 4)
	{
	    /* pm is NOT necessarily 16-byte aligned */
	    vmask = load_128_unaligned (pm);

	    pack_cmp = vec_all_eq(vmask, (vector unsigned int) AVV(0));

	    /* if all bits in mask are zero, pack_cmp is not 0 */
	    if (pack_cmp == 0)
	    {
		/* pd is 16-byte aligned */
		vdest = in_over (vsrc, valpha, vmask, load_128_aligned (pd));

		save_128_aligned(pd, vdest);
	    }

	    pd += 4;
	    pm += 4;
	    w -= 4;
	}

	while (w)
	{
	    s = src;
	    m = *pm++;

	    if (m)
	    {
		d = *pd;
		UN8x4_MUL_UN8x4 (s, m);
		UN8x4_MUL_UN8 (m, ia);
		m = ~m;
		UN8x4_MUL_UN8x4_ADD_UN8x4 (d, m, s);
		*pd = d;
	    }

	    pd++;
	    w--;
	}
    }
}

static void
vmx_composite_add_8_8 (pixman_implementation_t *imp,
            pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint8_t     *dst_line, *dst;
    uint8_t     *src_line, *src;
    int dst_stride, src_stride;
    int32_t w;
    uint16_t t;

    PIXMAN_IMAGE_GET_LINE (
    src_image, src_x, src_y, uint8_t, src_stride, src_line, 1);
    PIXMAN_IMAGE_GET_LINE (
    dest_image, dest_x, dest_y, uint8_t, dst_stride, dst_line, 1);

    while (height--)
    {
	dst = dst_line;
	src = src_line;

	dst_line += dst_stride;
	src_line += src_stride;
	w = width;

	/* Small head */
	while (w && (uintptr_t)dst & 3)
	{
	    t = (*dst) + (*src++);
	    *dst++ = t | (0 - (t >> 8));
	    w--;
	}

	vmx_combine_add_u (imp, op,
		    (uint32_t*)dst, (uint32_t*)src, NULL, w >> 2);

	/* Small tail */
	dst += w & 0xfffc;
	src += w & 0xfffc;

	w &= 3;

	while (w)
	{
	    t = (*dst) + (*src++);
	    *dst++ = t | (0 - (t >> 8));
	    w--;
	}
    }
}

static void
vmx_composite_add_8888_8888 (pixman_implementation_t *imp,
                              pixman_composite_info_t *info)
{
    PIXMAN_COMPOSITE_ARGS (info);
    uint32_t    *dst_line, *dst;
    uint32_t    *src_line, *src;
    int dst_stride, src_stride;

    PIXMAN_IMAGE_GET_LINE (
	src_image, src_x, src_y, uint32_t, src_stride, src_line, 1);
    PIXMAN_IMAGE_GET_LINE (
	dest_image, dest_x, dest_y, uint32_t, dst_stride, dst_line, 1);

    while (height--)
    {
	dst = dst_line;
	dst_line += dst_stride;
	src = src_line;
	src_line += src_stride;

	vmx_combine_add_u (imp, op, dst, src, NULL, width);
    }
}

static force_inline void
scaled_nearest_scanline_vmx_8888_8888_OVER (uint32_t*       pd,
                                            const uint32_t* ps,
                                            int32_t         w,
                                            pixman_fixed_t  vx,
                                            pixman_fixed_t  unit_x,
                                            pixman_fixed_t  src_width_fixed,
                                            pixman_bool_t   fully_transparent_src)
{
    uint32_t s, d;
    const uint32_t* pm = NULL;

    vector unsigned int vsrc, vdst;

    if (fully_transparent_src)
	return;

    /* Align dst on a 16-byte boundary */
    while (w && ((uintptr_t)pd & 15))
    {
	d = *pd;
	s = combine1 (ps + pixman_fixed_to_int (vx), pm);
	vx += unit_x;
	while (vx >= 0)
	    vx -= src_width_fixed;

	*pd++ = core_combine_over_u_pixel_vmx (s, d);
	if (pm)
	    pm++;
	w--;
    }

    while (w >= 4)
    {
	vector unsigned int tmp;
	uint32_t tmp1, tmp2, tmp3, tmp4;

	tmp1 = *(ps + pixman_fixed_to_int (vx));
	vx += unit_x;
	while (vx >= 0)
	    vx -= src_width_fixed;
	tmp2 = *(ps + pixman_fixed_to_int (vx));
	vx += unit_x;
	while (vx >= 0)
	    vx -= src_width_fixed;
	tmp3 = *(ps + pixman_fixed_to_int (vx));
	vx += unit_x;
	while (vx >= 0)
	    vx -= src_width_fixed;
	tmp4 = *(ps + pixman_fixed_to_int (vx));
	vx += unit_x;
	while (vx >= 0)
	    vx -= src_width_fixed;

	tmp[0] = tmp1;
	tmp[1] = tmp2;
	tmp[2] = tmp3;
	tmp[3] = tmp4;

	vsrc = combine4 ((const uint32_t *) &tmp, pm);

	if (is_opaque (vsrc))
	{
	    save_128_aligned (pd, vsrc);
	}
	else if (!is_zero (vsrc))
	{
	    vdst = over(vsrc, splat_alpha(vsrc), load_128_aligned (pd));

	    save_128_aligned (pd, vdst);
	}

	w -= 4;
	pd += 4;
	if (pm)
	    pm += 4;
    }

    while (w)
    {
	d = *pd;
	s = combine1 (ps + pixman_fixed_to_int (vx), pm);
	vx += unit_x;
	while (vx >= 0)
	    vx -= src_width_fixed;

	*pd++ = core_combine_over_u_pixel_vmx (s, d);
	if (pm)
	    pm++;

	w--;
    }
}

FAST_NEAREST_MAINLOOP (vmx_8888_8888_cover_OVER,
		       scaled_nearest_scanline_vmx_8888_8888_OVER,
		       uint32_t, uint32_t, COVER)
FAST_NEAREST_MAINLOOP (vmx_8888_8888_none_OVER,
		       scaled_nearest_scanline_vmx_8888_8888_OVER,
		       uint32_t, uint32_t, NONE)
FAST_NEAREST_MAINLOOP (vmx_8888_8888_pad_OVER,
		       scaled_nearest_scanline_vmx_8888_8888_OVER,
		       uint32_t, uint32_t, PAD)
FAST_NEAREST_MAINLOOP (vmx_8888_8888_normal_OVER,
		       scaled_nearest_scanline_vmx_8888_8888_OVER,
		       uint32_t, uint32_t, NORMAL)

a1613 29
    PIXMAN_STD_FAST_PATH (OVER, solid,    null, a8r8g8b8, vmx_composite_over_n_8888),
    PIXMAN_STD_FAST_PATH (OVER, solid,    null, x8r8g8b8, vmx_composite_over_n_8888),
    PIXMAN_STD_FAST_PATH (OVER, a8r8g8b8, null, a8r8g8b8, vmx_composite_over_8888_8888),
    PIXMAN_STD_FAST_PATH (OVER, a8r8g8b8, null, x8r8g8b8, vmx_composite_over_8888_8888),
    PIXMAN_STD_FAST_PATH (OVER, a8b8g8r8, null, a8b8g8r8, vmx_composite_over_8888_8888),
    PIXMAN_STD_FAST_PATH (OVER, a8b8g8r8, null, x8b8g8r8, vmx_composite_over_8888_8888),
    PIXMAN_STD_FAST_PATH (OVER, solid, a8, a8r8g8b8, vmx_composite_over_n_8_8888),
    PIXMAN_STD_FAST_PATH (OVER, solid, a8, x8r8g8b8, vmx_composite_over_n_8_8888),
    PIXMAN_STD_FAST_PATH (OVER, solid, a8, a8b8g8r8, vmx_composite_over_n_8_8888),
    PIXMAN_STD_FAST_PATH (OVER, solid, a8, x8b8g8r8, vmx_composite_over_n_8_8888),
    PIXMAN_STD_FAST_PATH_CA (OVER, solid, a8r8g8b8, a8r8g8b8, vmx_composite_over_n_8888_8888_ca),
    PIXMAN_STD_FAST_PATH_CA (OVER, solid, a8r8g8b8, x8r8g8b8, vmx_composite_over_n_8888_8888_ca),
    PIXMAN_STD_FAST_PATH_CA (OVER, solid, a8b8g8r8, a8b8g8r8, vmx_composite_over_n_8888_8888_ca),
    PIXMAN_STD_FAST_PATH_CA (OVER, solid, a8b8g8r8, x8b8g8r8, vmx_composite_over_n_8888_8888_ca),

    /* PIXMAN_OP_ADD */
    PIXMAN_STD_FAST_PATH (ADD, a8, null, a8, vmx_composite_add_8_8),
    PIXMAN_STD_FAST_PATH (ADD, a8r8g8b8, null, a8r8g8b8, vmx_composite_add_8888_8888),
    PIXMAN_STD_FAST_PATH (ADD, a8b8g8r8, null, a8b8g8r8, vmx_composite_add_8888_8888),

    /* PIXMAN_OP_SRC */
    PIXMAN_STD_FAST_PATH (SRC, x8r8g8b8, null, a8r8g8b8, vmx_composite_src_x888_8888),
    PIXMAN_STD_FAST_PATH (SRC, x8b8g8r8, null, a8b8g8r8, vmx_composite_src_x888_8888),

    SIMPLE_NEAREST_FAST_PATH (OVER, a8r8g8b8, x8r8g8b8, vmx_8888_8888),
    SIMPLE_NEAREST_FAST_PATH (OVER, a8b8g8r8, x8b8g8r8, vmx_8888_8888),
    SIMPLE_NEAREST_FAST_PATH (OVER, a8r8g8b8, a8r8g8b8, vmx_8888_8888),
    SIMPLE_NEAREST_FAST_PATH (OVER, a8b8g8r8, a8b8g8r8, vmx_8888_8888),

a1616 92
static uint32_t *
vmx_fetch_x8r8g8b8 (pixman_iter_t *iter, const uint32_t *mask)
{
    int w = iter->width;
    vector unsigned int ff000000 = mask_ff000000;
    uint32_t *dst = iter->buffer;
    uint32_t *src = (uint32_t *)iter->bits;

    iter->bits += iter->stride;

    while (w && ((uintptr_t)dst) & 0x0f)
    {
	*dst++ = (*src++) | 0xff000000;
	w--;
    }

    while (w >= 4)
    {
	save_128_aligned(dst, vec_or(load_128_unaligned(src), ff000000));

	dst += 4;
	src += 4;
	w -= 4;
    }

    while (w)
    {
	*dst++ = (*src++) | 0xff000000;
	w--;
    }

    return iter->buffer;
}

static uint32_t *
vmx_fetch_a8 (pixman_iter_t *iter, const uint32_t *mask)
{
    int w = iter->width;
    uint32_t *dst = iter->buffer;
    uint8_t *src = iter->bits;
    vector unsigned int vmx0, vmx1, vmx2, vmx3, vmx4, vmx5, vmx6;

    iter->bits += iter->stride;

    while (w && (((uintptr_t)dst) & 15))
    {
        *dst++ = *(src++) << 24;
        w--;
    }

    while (w >= 16)
    {
	vmx0 = load_128_unaligned((uint32_t *) src);

	unpack_128_2x128((vector unsigned int) AVV(0), vmx0, &vmx1, &vmx2);
	unpack_128_2x128_16((vector unsigned int) AVV(0), vmx1, &vmx3, &vmx4);
	unpack_128_2x128_16((vector unsigned int) AVV(0), vmx2, &vmx5, &vmx6);

	save_128_aligned(dst, vmx6);
	save_128_aligned((dst +  4), vmx5);
	save_128_aligned((dst +  8), vmx4);
	save_128_aligned((dst + 12), vmx3);

	dst += 16;
	src += 16;
	w -= 16;
    }

    while (w)
    {
	*dst++ = *(src++) << 24;
	w--;
    }

    return iter->buffer;
}

#define IMAGE_FLAGS							\
    (FAST_PATH_STANDARD_FLAGS | FAST_PATH_ID_TRANSFORM |		\
     FAST_PATH_BITS_IMAGE | FAST_PATH_SAMPLES_COVER_CLIP_NEAREST)

static const pixman_iter_info_t vmx_iters[] =
{
    { PIXMAN_x8r8g8b8, IMAGE_FLAGS, ITER_NARROW,
      _pixman_iter_init_bits_stride, vmx_fetch_x8r8g8b8, NULL
    },
    { PIXMAN_a8, IMAGE_FLAGS, ITER_NARROW,
      _pixman_iter_init_bits_stride, vmx_fetch_a8, NULL
    },
    { PIXMAN_null },
};

a1621 8
    /* VMX constants */
    mask_ff000000 = create_mask_32_128 (0xff000000);
    mask_red   = create_mask_32_128 (0x00f80000);
    mask_green = create_mask_32_128 (0x0000fc00);
    mask_blue  = create_mask_32_128 (0x000000f8);
    mask_565_fix_rb = create_mask_32_128 (0x00e000e0);
    mask_565_fix_g = create_mask_32_128  (0x0000c000);

a1646 4

    imp->fill = vmx_fill;

    imp->iter_info = vmx_iters;
@


1.7
log
@Update to pixman 0.32.4. Tested by naddy@@ and ajacoutot@@
@
text
@d33 1
d38 7
d48 1
d53 15
d77 1
d80 4
d86 1
d89 4
d104 1
d107 5
d113 1
d116 4
d177 1
d189 4
a192 5
/* notice you have to declare temp vars...
 * Note: tmp3 and tmp4 must remain untouched!
 */

#define LOAD_VECTORS(dest, source)			  \
d195 1
a195 1
    v ## source = (typeof(v ## source))			  \
d197 8
a204 1
    v ## dest = (typeof(v ## dest))vec_ld (0, dest);
d207 37
a243 9
    tmp1 = (typeof(tmp1))vec_ld (0, source);		  \
    tmp2 = (typeof(tmp2))vec_ld (15, source);		  \
    v ## source = (typeof(v ## source))			  \
	vec_perm (tmp1, tmp2, source ## _mask);		  \
    tmp1 = (typeof(tmp1))vec_ld (0, mask);		  \
    v ## dest = (typeof(v ## dest))vec_ld (0, dest);	  \
    tmp2 = (typeof(tmp2))vec_ld (15, mask);		  \
    v ## mask = (typeof(v ## mask))			  \
	vec_perm (tmp1, tmp2, mask ## _mask);
d246 1
a246 1
    LOAD_VECTORSC (dest, source, mask)					\
d253 235
d495 1
a495 1
    vector unsigned char tmp1, tmp2, src_mask;
d545 2
a546 1
    vector unsigned char tmp1, tmp2, src_mask, mask_mask;
d617 1
a617 1
    vector unsigned char tmp1, tmp2, src_mask;
d665 2
a666 1
    vector unsigned char tmp1, tmp2, src_mask, mask_mask;
d734 1
a734 1
    vector unsigned char tmp1, tmp2, src_mask;
d779 2
a780 1
    vector unsigned char tmp1, tmp2, src_mask, mask_mask;
d845 1
a845 1
    vector unsigned char tmp1, tmp2, src_mask;
d892 2
a893 1
    vector unsigned char tmp1, tmp2, src_mask, mask_mask;
d960 1
a960 1
    vector unsigned char tmp1, tmp2, src_mask;
d1007 2
a1008 1
    vector unsigned char tmp1, tmp2, src_mask, mask_mask;
d1073 1
a1073 1
    vector unsigned char tmp1, tmp2, src_mask;
d1121 2
a1122 1
    vector unsigned char tmp1, tmp2, src_mask, mask_mask;
d1189 1
a1189 1
    vector unsigned char tmp1, tmp2, src_mask;
d1241 2
a1242 1
    vector unsigned char tmp1, tmp2, src_mask, mask_mask;
d1318 1
a1318 1
    vector unsigned char tmp1, tmp2, src_mask;
d1370 2
a1371 1
    vector unsigned char tmp1, tmp2, src_mask, mask_mask;
d1447 1
a1447 1
    vector unsigned char tmp1, tmp2, src_mask;
d1499 2
a1500 1
    vector unsigned char tmp1, tmp2, src_mask, mask_mask;
d1576 1
a1576 1
    vector unsigned char tmp1, tmp2, src_mask;
d1622 2
a1623 1
    vector unsigned char tmp1, tmp2, src_mask, mask_mask;
d1691 2
a1692 1
    vector unsigned char tmp1, tmp2, mask_mask, src_mask;
d1742 2
a1743 1
    vector unsigned char tmp1, tmp2, mask_mask, src_mask;
d1801 2
a1802 1
    vector unsigned char tmp1, tmp2, mask_mask, src_mask;
d1858 2
a1859 1
    vector unsigned char tmp1, tmp2, mask_mask, src_mask;
d1913 2
a1914 1
    vector unsigned char tmp1, tmp2, mask_mask, src_mask;
d1969 2
a1970 1
    vector unsigned char tmp1, tmp2, mask_mask, src_mask;
d2027 2
a2028 1
    vector unsigned char tmp1, tmp2, mask_mask, src_mask;
d2085 2
a2086 1
    vector unsigned char tmp1, tmp2, mask_mask, src_mask;
d2152 2
a2153 1
    vector unsigned char tmp1, tmp2, mask_mask, src_mask;
d2216 2
a2217 1
    vector unsigned char tmp1, tmp2, mask_mask, src_mask;
d2280 2
a2281 1
    vector unsigned char tmp1, tmp2, mask_mask, src_mask;
d2325 664
d2991 29
d3023 92
d3120 8
d3153 4
@


1.6
log
@Update to pixman 0.30.2. No functional change.
@
text
@d137 1
a137 3
    dest ## _mask = vec_lvsl (0, dest);					\
    source ## _mask = vec_lvsl (0, source);				\
    store_mask = vec_lvsr (0, dest);
d141 1
a141 3
    dest ## _mask = vec_lvsl (0, dest);					\
    source ## _mask = vec_lvsl (0, source);				\
    store_mask = vec_lvsr (0, dest);
a149 1
    tmp3 = (typeof(tmp3))vec_ld (0, dest);		  \
d152 1
a152 3
    tmp4 = (typeof(tmp4))vec_ld (15, dest);		  \
    v ## dest = (typeof(v ## dest))			  \
	vec_perm (tmp3, tmp4, dest ## _mask);
a156 1
    tmp3 = (typeof(tmp3))vec_ld (0, dest);		  \
a158 1
    tmp4 = (typeof(tmp4))vec_ld (15, dest);		  \
d160 1
a160 2
    v ## dest = (typeof(v ## dest))			  \
	vec_perm (tmp3, tmp4, dest ## _mask);		  \
d171 1
a171 5
    edges = vec_perm (tmp4, tmp3, dest ## _mask);			\
    tmp3 = vec_perm ((vector unsigned char)v ## dest, edges, store_mask); \
    tmp1 = vec_perm (edges, (vector unsigned char)v ## dest, store_mask); \
    vec_st ((vector unsigned int) tmp3, 15, dest);			\
    vec_st ((vector unsigned int) tmp1, 0, dest);
d180 13
a192 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, store_mask;
d230 17
a246 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, mask_mask, store_mask;
d301 12
a312 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, store_mask;
d349 15
a363 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, mask_mask, store_mask;
d417 11
a427 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, store_mask;
d462 14
a475 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, mask_mask, store_mask;
d527 12
a538 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, store_mask;
d574 15
a588 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, mask_mask, store_mask;
d641 12
a652 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, store_mask;
d688 14
a701 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, mask_mask, store_mask;
d753 12
a764 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, store_mask;
d801 15
a815 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, mask_mask, store_mask;
d868 14
a881 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, store_mask;
d920 19
a938 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, mask_mask, store_mask;
d996 14
a1009 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, store_mask;
d1048 19
a1066 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, mask_mask, store_mask;
d1124 14
a1137 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, store_mask;
d1176 19
a1194 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, mask_mask, store_mask;
d1252 12
a1263 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, store_mask;
d1298 14
a1311 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, src_mask, mask_mask, store_mask;
d1366 12
a1377 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;
d1416 16
a1431 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;
d1474 15
a1488 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;
d1530 14
a1543 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;
d1584 14
a1597 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;
d1639 15
a1653 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;
d1696 15
a1710 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;
d1753 17
a1769 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;
d1819 17
a1835 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;
d1882 17
a1898 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;
d1945 14
a1958 2
    vector unsigned char tmp1, tmp2, tmp3, tmp4, edges,
	dest_mask, mask_mask, src_mask, store_mask;
@


1.5
log
@Update to pixman 0.22.2.
0.22.0 was tested by many. 0.22.2 only add a few bug fixes.
Note that on amd64 a recent ld.so is needed to avoid random bus errors.
@
text
@d28 1
d30 1
@


1.4
log
@Update to pixman 0.18.4.

Tweak build to use libpthread-stubs for TLS emulation instead of forcing
every application using pixman to use -pthread.

Tested by jasper@@ and landry@@ on a bulk ports build.
@
text
@d1616 1
a1616 1
_pixman_implementation_create_vmx (void)
d1618 1
a1618 2
    pixman_implementation_t *fast = _pixman_implementation_create_fast_path ();
    pixman_implementation_t *imp = _pixman_implementation_create (fast, vmx_fast_paths);
@


1.3
log
@Update to pixman 0.16.6. Tested on a full ports build by naddy@@.
@
text
@d1610 5
d1619 1
a1619 1
    pixman_implementation_t *imp = _pixman_implementation_create (fast);
@


1.2
log
@Update to pixman 0.15.8.
@
text
@d36 2
a37 1
splat_alpha (vector unsigned int pix) {
d39 3
a41 2
    (vector unsigned char)AVV(0x00,0x00,0x00,0x00, 0x04,0x04,0x04,0x04,
                               0x08,0x08,0x08,0x08, 0x0C,0x0C,0x0C,0x0C));
d48 1
d51 3
a53 2
                    vec_mergeh ((vector unsigned char)AVV(0),
                                (vector unsigned char)p);
d55 2
a56 2
                    vec_mergeh ((vector unsigned char)AVV(0),
                                (vector unsigned char)a);
d59 2
a60 2
                            AVV(0x0080,0x0080,0x0080,0x0080,
                                 0x0080,0x0080,0x0080,0x0080));
d68 2
a69 2
                    vec_mergel ((vector unsigned char)AVV(0),
                                (vector unsigned char)p);
d71 2
a72 2
                    vec_mergel ((vector unsigned char)AVV(0),
                                (vector unsigned char)a);
d75 2
a76 2
                            AVV(0x0080,0x0080,0x0080,0x0080,
                                 0x0080,0x0080,0x0080,0x0080));
d89 1
a89 1
                     (vector unsigned char)b);
d93 4
a96 2
pix_add_mul (vector unsigned int x, vector unsigned int a,
             vector unsigned int y, vector unsigned int b)
d98 1
a98 1
    vector unsigned short hi, lo, mod, hiy, loy, mody;
d100 2
a101 16
    hi = (vector unsigned short)
                    vec_mergeh ((vector unsigned char)AVV(0),
                                (vector unsigned char)x);
    mod = (vector unsigned short)
                    vec_mergeh ((vector unsigned char)AVV(0),
                                (vector unsigned char)a);
    hiy = (vector unsigned short)
                    vec_mergeh ((vector unsigned char)AVV(0),
                                (vector unsigned char)y);
    mody = (vector unsigned short)
                    vec_mergeh ((vector unsigned char)AVV(0),
                                (vector unsigned char)b);

    hi = vec_mladd (hi, mod, (vector unsigned short)
                             AVV(0x0080,0x0080,0x0080,0x0080,
                                  0x0080,0x0080,0x0080,0x0080));
d103 1
a103 31
    hi = vec_mladd (hiy, mody, hi);

    hi = vec_adds (hi, vec_sr (hi, vec_splat_u16 (8)));

    hi = vec_sr (hi, vec_splat_u16 (8));

    lo = (vector unsigned short)
                    vec_mergel ((vector unsigned char)AVV(0),
                                (vector unsigned char)x);
    mod = (vector unsigned short)
                    vec_mergel ((vector unsigned char)AVV(0),
                                (vector unsigned char)a);

    loy = (vector unsigned short)
                    vec_mergel ((vector unsigned char)AVV(0),
                                (vector unsigned char)y);
    mody = (vector unsigned short)
                    vec_mergel ((vector unsigned char)AVV(0),
                                (vector unsigned char)b);

    lo = vec_mladd (lo, mod, (vector unsigned short)
                             AVV(0x0080,0x0080,0x0080,0x0080,
                                  0x0080,0x0080,0x0080,0x0080));

    lo = vec_mladd (loy, mody, lo);

    lo = vec_adds (lo, vec_sr (lo, vec_splat_u16 (8)));

    lo = vec_sr (lo, vec_splat_u16 (8));

    return (vector unsigned int)vec_packsu (hi, lo);
d111 1
d114 2
a115 1
over (vector unsigned int src, vector unsigned int srca,
d119 2
a120 1
                                pix_multiply (dest, negate (srca));
d126 3
a128 2
#define in_over(src, srca, mask, dest) over (pix_multiply (src, mask),\
                                             pix_multiply (srca, mask), dest)
d131 1
a131 1
#define COMPUTE_SHIFT_MASK(source) \
d134 3
a136 3
#define COMPUTE_SHIFT_MASKS(dest, source) \
    dest ## _mask = vec_lvsl (0, dest); \
    source ## _mask = vec_lvsl (0, source); \
d139 4
a142 4
#define COMPUTE_SHIFT_MASKC(dest, source, mask) \
    mask ## _mask = vec_lvsl (0, mask); \
    dest ## _mask = vec_lvsl (0, dest); \
    source ## _mask = vec_lvsl (0, source); \
d149 35
a183 9
#define LOAD_VECTORS(dest, source) \
        tmp1 = (typeof(tmp1))vec_ld(0, source); \
        tmp2 = (typeof(tmp2))vec_ld(15, source); \
        tmp3 = (typeof(tmp3))vec_ld(0, dest); \
        v ## source = (typeof(v ## source)) \
                       vec_perm(tmp1, tmp2, source ## _mask); \
        tmp4 = (typeof(tmp4))vec_ld(15, dest); \
        v ## dest = (typeof(v ## dest)) \
                     vec_perm(tmp3, tmp4, dest ## _mask);
d185 4
a188 28
#define LOAD_VECTORSC(dest, source, mask) \
        tmp1 = (typeof(tmp1))vec_ld(0, source); \
        tmp2 = (typeof(tmp2))vec_ld(15, source); \
        tmp3 = (typeof(tmp3))vec_ld(0, dest); \
        v ## source = (typeof(v ## source)) \
                       vec_perm(tmp1, tmp2, source ## _mask); \
        tmp4 = (typeof(tmp4))vec_ld(15, dest); \
        tmp1 = (typeof(tmp1))vec_ld(0, mask); \
        v ## dest = (typeof(v ## dest)) \
                     vec_perm(tmp3, tmp4, dest ## _mask); \
        tmp2 = (typeof(tmp2))vec_ld(15, mask); \
        v ## mask = (typeof(v ## mask)) \
                     vec_perm(tmp1, tmp2, mask ## _mask);

#define LOAD_VECTORSM(dest, source, mask) \
        LOAD_VECTORSC(dest, source, mask) \
        v ## source = pix_multiply(v ## source, \
                                   splat_alpha (v ## mask));

#define STORE_VECTOR(dest) \
        edges = vec_perm (tmp4, tmp3, dest ## _mask); \
        tmp3 = vec_perm ((vector unsigned char)v ## dest, edges, store_mask); \
        tmp1 = vec_perm (edges, (vector unsigned char)v ## dest, store_mask); \
        vec_st ((vector unsigned int) tmp3, 15, dest ); \
        vec_st ((vector unsigned int) tmp1, 0, dest );

static FASTCALL void
vmxCombineOverUnomask (uint32_t *dest, const uint32_t *src, int width)
d191 1
a191 1
    vector unsigned int  vdest, vsrc;
d193 1
a193 1
                         dest_mask, src_mask, store_mask;
d195 1
a195 1
    COMPUTE_SHIFT_MASKS(dest, src)
d198 2
a199 1
    for (i = width/4; i > 0; i--) {
d201 1
a201 1
        LOAD_VECTORS(dest, src)
d203 1
a203 1
        vdest = over (vsrc, splat_alpha (vsrc), vdest);
d205 1
a205 1
        STORE_VECTOR(dest)
d207 2
a208 2
        src+=4;
        dest+=4;
d211 5
a215 4
    for (i = width%4; --i >=0;) {
        uint32_t s = src[i];
        uint32_t d = dest[i];
        uint32_t ia = Alpha (~s);
d217 3
a219 2
        FbByteMulAdd (d, ia, s);
        dest[i] = d;
d223 5
a227 5
static FASTCALL void
vmxCombineOverUmask (uint32_t *dest,
                     const uint32_t *src,
                     const uint32_t *mask,
                     int width)
d230 1
a230 1
    vector unsigned int  vdest, vsrc, vmask;
d232 1
a232 1
                         dest_mask, src_mask, mask_mask, store_mask;
d234 1
a234 1
    COMPUTE_SHIFT_MASKC(dest, src, mask)
d237 3
a239 3
    for (i = width/4; i > 0; i--) {

        LOAD_VECTORSM(dest, src, mask);
d241 1
a241 1
        vdest = over (vsrc, splat_alpha (vsrc), vdest);
d243 1
a243 1
        STORE_VECTOR(dest)
d245 3
a247 3
        src+=4;
        dest+=4;
        mask+=4;
d250 6
a255 5
    for (i = width%4; --i >=0;) {
        uint32_t m = Alpha (mask[i]);
        uint32_t s = src[i];
        uint32_t d = dest[i];
        uint32_t ia;
d257 1
a257 1
        FbByteMul (s, m);
d259 1
a259 1
        ia = Alpha (~s);
d261 2
a262 2
        FbByteMulAdd (d, ia, s);
        dest[i] = d;
d266 7
a272 4
static FASTCALL void
vmxCombineOverU(pixman_implementation_t *imp, pixman_op_t op,
		uint32_t *dest, const uint32_t *src, const uint32_t *mask,
                int width)
d275 1
a275 1
        vmxCombineOverUmask(dest, src, mask, width);
d277 1
a277 1
        vmxCombineOverUnomask(dest, src, width);
d280 4
a283 2
static FASTCALL void
vmxCombineOverReverseUnomask (uint32_t *dest, const uint32_t *src, int width)
d286 1
a286 1
    vector unsigned int  vdest, vsrc;
d288 1
a288 1
                         dest_mask, src_mask, store_mask;
d290 1
a290 1
    COMPUTE_SHIFT_MASKS(dest, src)
d293 2
a294 1
    for (i = width/4; i > 0; i--) {
d296 1
a296 1
        LOAD_VECTORS(dest, src)
d298 1
a298 1
        vdest = over (vdest, splat_alpha (vdest) , vsrc);
d300 1
a300 1
        STORE_VECTOR(dest)
d302 2
a303 2
        src+=4;
        dest+=4;
d306 5
a310 4
    for (i = width%4; --i >=0;) {
        uint32_t s = src[i];
        uint32_t d = dest[i];
        uint32_t ia = Alpha (~dest[i]);
d312 2
a313 2
        FbByteMulAdd (s, ia, d);
        dest[i] = s;
d317 5
a321 5
static FASTCALL void
vmxCombineOverReverseUmask (uint32_t *dest,
                            const uint32_t *src,
                            const uint32_t *mask,
                            int width)
d324 1
a324 1
    vector unsigned int  vdest, vsrc, vmask;
d326 1
a326 1
                         dest_mask, src_mask, mask_mask, store_mask;
d328 1
a328 1
    COMPUTE_SHIFT_MASKC(dest, src, mask)
d331 2
a332 1
    for (i = width/4; i > 0; i--) {
d334 1
a334 1
        LOAD_VECTORSM(dest, src, mask)
d336 1
a336 1
        vdest = over (vdest, splat_alpha (vdest) , vsrc);
d338 1
a338 1
        STORE_VECTOR(dest)
d340 3
a342 3
        src+=4;
        dest+=4;
        mask+=4;
d345 6
a350 5
    for (i = width%4; --i >=0;) {
        uint32_t m = Alpha (mask[i]);
        uint32_t s = src[i];
        uint32_t d = dest[i];
        uint32_t ia = Alpha (~dest[i]);
d352 1
a352 1
        FbByteMul (s, m);
d354 2
a355 2
        FbByteMulAdd (s, ia, d);
        dest[i] = s;
d359 7
a365 4
static FASTCALL void
vmxCombineOverReverseU (pixman_implementation_t *imp, pixman_op_t op,
			uint32_t *dest, const uint32_t *src,
                        const uint32_t *mask, int width)
d368 1
a368 1
        vmxCombineOverReverseUmask(dest, src, mask, width);
d370 1
a370 1
        vmxCombineOverReverseUnomask(dest, src, width);
d373 4
a376 2
static FASTCALL void
vmxCombineInUnomask (uint32_t *dest, const uint32_t *src, int width)
d379 1
a379 1
    vector unsigned int  vdest, vsrc;
d381 1
a381 1
                         dest_mask, src_mask, store_mask;
d383 1
a383 1
    COMPUTE_SHIFT_MASKS(dest, src)
d386 3
a388 3
    for (i = width/4; i > 0; i--) {

        LOAD_VECTORS(dest, src)
d390 1
a390 1
        vdest = pix_multiply (vsrc, splat_alpha (vdest));
d392 1
a392 1
        STORE_VECTOR(dest)
d394 2
a395 2
        src+=4;
        dest+=4;
d398 4
a401 1
    for (i = width%4; --i >=0;) {
d403 2
a404 4
        uint32_t s = src[i];
        uint32_t a = Alpha (dest[i]);
        FbByteMul (s, a);
        dest[i] = s;
d408 5
a412 5
static FASTCALL void
vmxCombineInUmask (uint32_t *dest,
                   const uint32_t *src,
                   const uint32_t *mask,
                   int width)
d415 1
a415 1
    vector unsigned int  vdest, vsrc, vmask;
d417 1
a417 1
                         dest_mask, src_mask, mask_mask, store_mask;
d419 1
a419 1
    COMPUTE_SHIFT_MASKC(dest, src, mask)
d422 3
a424 3
    for (i = width/4; i > 0; i--) {

        LOAD_VECTORSM(dest, src, mask)
d426 1
a426 1
        vdest = pix_multiply (vsrc, splat_alpha (vdest));
d428 1
a428 1
        STORE_VECTOR(dest)
d430 3
a432 3
        src+=4;
        dest+=4;
        mask+=4;
d435 5
a439 4
    for (i = width%4; --i >=0;) {
        uint32_t m = Alpha (mask[i]);
        uint32_t s = src[i];
        uint32_t a = Alpha (dest[i]);
d441 2
a442 1
        FbByteMul (s, m);
d444 1
a444 2
        FbByteMul (s, a);
        dest[i] = s;
d448 7
a454 4
static FASTCALL void
vmxCombineInU (pixman_implementation_t *imp, pixman_op_t op,
	       uint32_t *dest, const uint32_t *src, const uint32_t *mask,
               int width)
d457 1
a457 1
        vmxCombineInUmask(dest, src, mask, width);
d459 1
a459 1
        vmxCombineInUnomask(dest, src, width);
d462 4
a465 2
static FASTCALL void
vmxCombineInReverseUnomask (uint32_t *dest, const uint32_t *src, int width)
d468 1
a468 1
    vector unsigned int  vdest, vsrc;
d470 1
a470 1
                         dest_mask, src_mask, store_mask;
d472 1
a472 1
    COMPUTE_SHIFT_MASKS(dest, src)
d475 5
a479 1
    for (i = width/4; i > 0; i--) {
d481 1
a481 1
        LOAD_VECTORS(dest, src)
d483 3
a485 1
        vdest = pix_multiply (vdest, splat_alpha (vsrc));
d487 4
a490 1
        STORE_VECTOR(dest)
d492 1
a492 3
        src+=4;
        dest+=4;
    }
d494 1
a494 5
    for (i = width%4; --i >=0;) {
        uint32_t d = dest[i];
        uint32_t a = Alpha (src[i]);
        FbByteMul (d, a);
        dest[i] = d;
d498 5
a502 5
static FASTCALL void
vmxCombineInReverseUmask (uint32_t *dest,
                          const uint32_t *src,
                          const uint32_t *mask,
                          int width)
d505 1
a505 1
    vector unsigned int  vdest, vsrc, vmask;
d507 1
a507 1
                         dest_mask, src_mask, mask_mask, store_mask;
d509 1
a509 1
    COMPUTE_SHIFT_MASKC(dest, src, mask)
d512 3
a514 1
    for (i = width/4; i > 0; i--) {
d516 1
a516 1
        LOAD_VECTORSM(dest, src, mask)
d518 1
a518 1
        vdest = pix_multiply (vdest, splat_alpha (vsrc));
d520 3
a522 5
        STORE_VECTOR(dest)

        src+=4;
        dest+=4;
        mask+=4;
d525 5
a529 4
    for (i = width%4; --i >=0;) {
        uint32_t m = Alpha (mask[i]);
        uint32_t d = dest[i];
        uint32_t a = src[i];
d531 3
a533 1
        FbByteMul (a, m);
d535 1
a535 3
        a = Alpha (a);
        FbByteMul (d, a);
        dest[i] = d;
d539 7
a545 4
static FASTCALL void
vmxCombineInReverseU (pixman_implementation_t *imp, pixman_op_t op,
		      uint32_t *dest, const uint32_t *src,
                      const uint32_t *mask, int width)
d548 1
a548 1
        vmxCombineInReverseUmask(dest, src, mask, width);
d550 1
a550 1
        vmxCombineInReverseUnomask(dest, src, width);
d553 4
a556 2
static FASTCALL void
vmxCombineOutUnomask (uint32_t *dest, const uint32_t *src, int width)
d559 1
a559 1
    vector unsigned int  vdest, vsrc;
d561 1
a561 1
                         dest_mask, src_mask, store_mask;
d563 1
a563 1
    COMPUTE_SHIFT_MASKS(dest, src)
d566 3
a568 1
    for (i = width/4; i > 0; i--) {
d570 1
a570 1
        LOAD_VECTORS(dest, src)
d572 1
a572 1
        vdest = pix_multiply (vsrc, splat_alpha (negate (vdest)));
d574 3
a576 1
        STORE_VECTOR(dest)
d578 6
a583 3
        src+=4;
        dest+=4;
    }
d585 1
a585 5
    for (i = width%4; --i >=0;) {
        uint32_t s = src[i];
        uint32_t a = Alpha (~dest[i]);
        FbByteMul (s, a);
        dest[i] = s;
d589 5
a593 5
static FASTCALL void
vmxCombineOutUmask (uint32_t *dest,
                    const uint32_t *src,
                    const uint32_t *mask,
                    int width)
d596 1
a596 1
    vector unsigned int  vdest, vsrc, vmask;
d598 1
a598 1
                         dest_mask, src_mask, mask_mask, store_mask;
d600 1
a600 1
    COMPUTE_SHIFT_MASKC(dest, src, mask)
d603 3
a605 3
    for (i = width/4; i > 0; i--) {

        LOAD_VECTORSM(dest, src, mask)
d607 1
a607 1
        vdest = pix_multiply (vsrc, splat_alpha (negate (vdest)));
d609 1
a609 1
        STORE_VECTOR(dest)
d611 3
a613 3
        src+=4;
        dest+=4;
        mask+=4;
d616 5
a620 4
    for (i = width%4; --i >=0;) {
        uint32_t m = Alpha (mask[i]);
        uint32_t s = src[i];
        uint32_t a = Alpha (~dest[i]);
d622 2
a623 1
        FbByteMul (s, m);
d625 1
a625 2
        FbByteMul (s, a);
        dest[i] = s;
d629 7
a635 4
static FASTCALL void
vmxCombineOutU (pixman_implementation_t *imp, pixman_op_t op,
		uint32_t *dest, const uint32_t *src, const uint32_t *mask,
                int width)
d638 1
a638 1
        vmxCombineOutUmask(dest, src, mask, width);
d640 1
a640 1
        vmxCombineOutUnomask(dest, src, width);
d643 4
a646 2
static FASTCALL void
vmxCombineOutReverseUnomask (uint32_t *dest, const uint32_t *src, int width)
d649 1
a649 1
    vector unsigned int  vdest, vsrc;
d651 1
a651 1
                         dest_mask, src_mask, store_mask;
d653 1
a653 1
    COMPUTE_SHIFT_MASKS(dest, src)
d656 2
a657 1
    for (i = width/4; i > 0; i--) {
d659 1
a659 1
        LOAD_VECTORS(dest, src)
d661 1
a661 1
        vdest = pix_multiply (vdest, splat_alpha (negate (vsrc)));
d663 1
a663 1
        STORE_VECTOR(dest)
d665 2
a666 2
        src+=4;
        dest+=4;
d669 8
a676 5
    for (i = width%4; --i >=0;) {
        uint32_t d = dest[i];
        uint32_t a = Alpha (~src[i]);
        FbByteMul (d, a);
        dest[i] = d;
d680 5
a684 5
static FASTCALL void
vmxCombineOutReverseUmask (uint32_t *dest,
                           const uint32_t *src,
                           const uint32_t *mask,
                           int width)
d687 1
a687 1
    vector unsigned int  vdest, vsrc, vmask;
d689 1
a689 1
                         dest_mask, src_mask, mask_mask, store_mask;
d691 1
a691 1
    COMPUTE_SHIFT_MASKC(dest, src, mask)
d694 3
a696 3
    for (i = width/4; i > 0; i--) {

        LOAD_VECTORSM(dest, src, mask)
d698 1
a698 1
        vdest = pix_multiply (vdest, splat_alpha (negate (vsrc)));
d700 1
a700 1
        STORE_VECTOR(dest)
d702 3
a704 3
        src+=4;
        dest+=4;
        mask+=4;
d707 5
a711 4
    for (i = width%4; --i >=0;) {
        uint32_t m = Alpha (mask[i]);
        uint32_t d = dest[i];
        uint32_t a = src[i];
d713 3
a715 1
        FbByteMul (a, m);
d717 1
a717 3
        a = Alpha (~a);
        FbByteMul (d, a);
        dest[i] = d;
d721 7
a727 6
static FASTCALL void
vmxCombineOutReverseU (pixman_implementation_t *imp, pixman_op_t op,
		       uint32_t *dest,
                       const uint32_t *src,
                       const uint32_t *mask,
                       int width)
d730 1
a730 1
        vmxCombineOutReverseUmask(dest, src, mask, width);
d732 1
a732 1
        vmxCombineOutReverseUnomask(dest, src, width);
d735 4
a738 2
static FASTCALL void
vmxCombineAtopUnomask (uint32_t *dest, const uint32_t *src, int width)
d741 1
a741 1
    vector unsigned int  vdest, vsrc;
d743 1
a743 1
                         dest_mask, src_mask, store_mask;
d745 1
a745 1
    COMPUTE_SHIFT_MASKS(dest, src)
d748 3
a750 1
    for (i = width/4; i > 0; i--) {
d752 2
a753 1
        LOAD_VECTORS(dest, src)
d755 1
a755 2
        vdest = pix_add_mul (vsrc, splat_alpha (vdest),
                            vdest, splat_alpha (negate (vsrc)));
d757 3
a759 1
        STORE_VECTOR(dest)
d761 6
a766 3
        src+=4;
        dest+=4;
    }
d768 1
a768 5
    for (i = width%4; --i >=0;) {
        uint32_t s = src[i];
        uint32_t d = dest[i];
        uint32_t dest_a = Alpha (d);
        uint32_t src_ia = Alpha (~s);
d770 1
a770 2
        FbByteAddMul (s, dest_a, d, src_ia);
        dest[i] = s;
d774 5
a778 5
static FASTCALL void
vmxCombineAtopUmask (uint32_t *dest,
                     const uint32_t *src,
                     const uint32_t *mask,
                     int width)
d781 1
a781 1
    vector unsigned int  vdest, vsrc, vmask;
d783 1
a783 1
                         dest_mask, src_mask, mask_mask, store_mask;
d785 1
a785 1
    COMPUTE_SHIFT_MASKC(dest, src, mask)
d788 3
a790 1
    for (i = width/4; i > 0; i--) {
d792 2
a793 1
        LOAD_VECTORSM(dest, src, mask)
d795 1
a795 2
        vdest = pix_add_mul (vsrc, splat_alpha (vdest),
                            vdest, splat_alpha (negate (vsrc)));
d797 4
a800 1
        STORE_VECTOR(dest)
d802 7
a808 4
        src+=4;
        dest+=4;
        mask+=4;
    }
d810 1
a810 6
    for (i = width%4; --i >=0;) {
        uint32_t m = Alpha (mask[i]);
        uint32_t s = src[i];
        uint32_t d = dest[i];
        uint32_t dest_a = Alpha (d);
        uint32_t src_ia;
d812 1
a812 1
        FbByteMul (s, m);
d814 1
a814 1
        src_ia = Alpha (~s);
d816 1
a816 2
        FbByteAddMul (s, dest_a, d, src_ia);
        dest[i] = s;
d820 7
a826 6
static FASTCALL void
vmxCombineAtopU (pixman_implementation_t *imp, pixman_op_t op,
		 uint32_t *dest,
                 const uint32_t *src,
                 const uint32_t *mask,
                 int width)
d829 1
a829 1
        vmxCombineAtopUmask(dest, src, mask, width);
d831 1
a831 1
        vmxCombineAtopUnomask(dest, src, width);
d834 4
a837 2
static FASTCALL void
vmxCombineAtopReverseUnomask (uint32_t *dest, const uint32_t *src, int width)
d840 1
a840 1
    vector unsigned int  vdest, vsrc;
d842 1
a842 1
                         dest_mask, src_mask, store_mask;
d844 1
a844 1
    COMPUTE_SHIFT_MASKS(dest, src)
d847 3
a849 1
    for (i = width/4; i > 0; i--) {
d851 2
a852 1
        LOAD_VECTORS(dest, src)
d854 1
a854 2
        vdest = pix_add_mul (vdest, splat_alpha (vsrc),
                            vsrc, splat_alpha (negate (vdest)));
d856 3
a858 1
        STORE_VECTOR(dest)
d860 6
a865 3
        src+=4;
        dest+=4;
    }
d867 1
a867 5
    for (i = width%4; --i >=0;) {
        uint32_t s = src[i];
        uint32_t d = dest[i];
        uint32_t src_a = Alpha (s);
        uint32_t dest_ia = Alpha (~d);
d869 1
a869 2
        FbByteAddMul (s, dest_ia, d, src_a);
        dest[i] = s;
d873 5
a877 5
static FASTCALL void
vmxCombineAtopReverseUmask (uint32_t *dest,
                            const uint32_t *src,
                            const uint32_t *mask,
                            int width)
d880 1
a880 1
    vector unsigned int  vdest, vsrc, vmask;
d882 1
a882 1
                         dest_mask, src_mask, mask_mask, store_mask;
d884 1
a884 1
    COMPUTE_SHIFT_MASKC(dest, src, mask)
d887 3
a889 1
    for (i = width/4; i > 0; i--) {
d891 2
a892 1
        LOAD_VECTORSM(dest, src, mask)
d894 1
a894 2
        vdest = pix_add_mul (vdest, splat_alpha (vsrc),
                            vsrc, splat_alpha (negate (vdest)));
d896 4
a899 1
        STORE_VECTOR(dest)
d901 7
a907 4
        src+=4;
        dest+=4;
        mask+=4;
    }
d909 1
a909 6
    for (i = width%4; --i >=0;) {
        uint32_t m = Alpha (mask[i]);
        uint32_t s = src[i];
        uint32_t d = dest[i];
        uint32_t src_a;
        uint32_t dest_ia = Alpha (~d);
d911 1
a911 1
        FbByteMul (s, m);
d913 1
a913 1
        src_a = Alpha (s);
d915 1
a915 2
        FbByteAddMul (s, dest_ia, d, src_a);
        dest[i] = s;
d919 7
a925 6
static FASTCALL void
vmxCombineAtopReverseU (pixman_implementation_t *imp, pixman_op_t op,
			uint32_t *dest,
                        const uint32_t *src,
                        const uint32_t *mask,
                        int width)
d928 1
a928 1
        vmxCombineAtopReverseUmask(dest, src, mask, width);
d930 1
a930 1
        vmxCombineAtopReverseUnomask(dest, src, width);
d933 4
a936 2
static FASTCALL void
vmxCombineXorUnomask (uint32_t *dest, const uint32_t *src, int width)
d939 1
a939 1
    vector unsigned int  vdest, vsrc;
d941 1
a941 1
                         dest_mask, src_mask, store_mask;
d943 1
a943 1
    COMPUTE_SHIFT_MASKS(dest, src)
d946 3
a948 1
    for (i = width/4; i > 0; i--) {
d950 2
a951 1
        LOAD_VECTORS (dest, src)
d953 1
a953 2
        vdest = pix_add_mul (vsrc, splat_alpha (negate (vdest)),
                            vdest, splat_alpha (negate (vsrc)));
d955 3
a957 1
        STORE_VECTOR(dest)
d959 6
a964 3
        src+=4;
        dest+=4;
    }
d966 1
a966 5
    for (i = width%4; --i >=0;) {
        uint32_t s = src[i];
        uint32_t d = dest[i];
        uint32_t src_ia = Alpha (~s);
        uint32_t dest_ia = Alpha (~d);
d968 1
a968 2
        FbByteAddMul (s, dest_ia, d, src_ia);
        dest[i] = s;
d972 5
a976 5
static FASTCALL void
vmxCombineXorUmask (uint32_t *dest,
                    const uint32_t *src,
                    const uint32_t *mask,
                    int width)
d979 1
a979 1
    vector unsigned int  vdest, vsrc, vmask;
d981 1
a981 1
                         dest_mask, src_mask, mask_mask, store_mask;
d983 1
a983 1
    COMPUTE_SHIFT_MASKC(dest, src, mask)
d986 3
a988 1
    for (i = width/4; i > 0; i--) {
d990 2
a991 1
        LOAD_VECTORSM(dest, src, mask)
d993 1
a993 2
        vdest = pix_add_mul (vsrc, splat_alpha (negate (vdest)),
                            vdest, splat_alpha (negate (vsrc)));
d995 4
a998 1
        STORE_VECTOR(dest)
d1000 7
a1006 4
        src+=4;
        dest+=4;
        mask+=4;
    }
d1008 1
a1008 6
    for (i = width%4; --i >=0;) {
        uint32_t m = Alpha (mask[i]);
        uint32_t s = src[i];
        uint32_t d = dest[i];
        uint32_t src_ia;
        uint32_t dest_ia = Alpha (~d);
d1010 1
a1010 1
        FbByteMul (s, m);
d1012 1
a1012 1
        src_ia = Alpha (~s);
d1014 1
a1014 2
        FbByteAddMul (s, dest_ia, d, src_ia);
        dest[i] = s;
d1018 7
a1024 6
static FASTCALL void
vmxCombineXorU (pixman_implementation_t *imp, pixman_op_t op,
		uint32_t *dest,
                const uint32_t *src,
                const uint32_t *mask,
                int width)
d1027 1
a1027 1
        vmxCombineXorUmask(dest, src, mask, width);
d1029 1
a1029 1
        vmxCombineXorUnomask(dest, src, width);
d1032 4
a1035 2
static FASTCALL void
vmxCombineAddUnomask (uint32_t *dest, const uint32_t *src, int width)
d1038 1
a1038 1
    vector unsigned int  vdest, vsrc;
d1040 1
a1040 1
                         dest_mask, src_mask, store_mask;
d1042 1
a1042 1
    COMPUTE_SHIFT_MASKS(dest, src)
d1044 5
a1048 1
    for (i = width/4; i > 0; i--) {
d1050 1
a1050 1
        LOAD_VECTORS(dest, src)
d1052 3
a1054 1
        vdest = pix_add (vsrc, vdest);
d1056 4
a1059 1
        STORE_VECTOR(dest)
d1061 1
a1061 3
        src+=4;
        dest+=4;
    }
d1063 1
a1063 5
    for (i = width%4; --i >=0;) {
        uint32_t s = src[i];
        uint32_t d = dest[i];
        FbByteAdd (d, s);
        dest[i] = d;
d1067 5
a1071 5
static FASTCALL void
vmxCombineAddUmask (uint32_t *dest,
                    const uint32_t *src,
                    const uint32_t *mask,
                    int width)
d1074 1
a1074 1
    vector unsigned int  vdest, vsrc, vmask;
d1076 1
a1076 1
                         dest_mask, src_mask, mask_mask, store_mask;
d1078 1
a1078 1
    COMPUTE_SHIFT_MASKC(dest, src, mask)
d1081 3
a1083 3
    for (i = width/4; i > 0; i--) {

        LOAD_VECTORSM(dest, src, mask)
d1085 1
a1085 1
        vdest = pix_add (vsrc, vdest);
d1087 1
a1087 1
        STORE_VECTOR(dest)
d1089 3
a1091 3
        src+=4;
        dest+=4;
        mask+=4;
d1094 5
a1098 4
    for (i = width%4; --i >=0;) {
        uint32_t m = Alpha (mask[i]);
        uint32_t s = src[i];
        uint32_t d = dest[i];
d1100 2
a1101 1
        FbByteMul (s, m);
d1103 1
a1103 2
        FbByteAdd (d, s);
        dest[i] = d;
d1107 7
a1113 6
static FASTCALL void
vmxCombineAddU (pixman_implementation_t *imp, pixman_op_t op,
		uint32_t *dest,
                const uint32_t *src,
                const uint32_t *mask,
                int width)
d1116 1
a1116 1
        vmxCombineAddUmask(dest, src, mask, width);
d1118 1
a1118 1
        vmxCombineAddUnomask(dest, src, width);
d1121 7
a1127 3
static FASTCALL void
vmxCombineSrcC (pixman_implementation_t *imp, pixman_op_t op,
		uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d1130 1
a1130 1
    vector unsigned int  vdest, vsrc, vmask;
d1132 3
a1134 1
                         dest_mask, mask_mask, src_mask, store_mask;
a1135 1
    COMPUTE_SHIFT_MASKC(dest, src, mask);
d1137 3
a1139 1
    for (i = width/4; i > 0; i--) {
d1141 1
a1141 1
        LOAD_VECTORSC(dest, src, mask)
d1143 1
a1143 1
        vdest = pix_multiply (vsrc, vmask);
d1145 9
a1153 1
        STORE_VECTOR(dest)
d1155 1
a1155 4
        mask+=4;
        src+=4;
        dest+=4;
    }
d1157 1
a1157 5
    for (i = width%4; --i >=0;) {
        uint32_t a = mask[i];
        uint32_t s = src[i];
        FbByteMulC (s, a);
        dest[i] = s;
d1161 7
a1167 3
static FASTCALL void
vmxCombineOverC (pixman_implementation_t *imp, pixman_op_t op,
		 uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d1170 1
a1170 1
    vector unsigned int  vdest, vsrc, vmask;
d1172 3
a1174 1
                         dest_mask, mask_mask, src_mask, store_mask;
a1175 1
    COMPUTE_SHIFT_MASKC(dest, src, mask);
d1177 3
a1179 1
    for (i = width/4; i > 0; i--) {
d1181 1
a1181 1
        LOAD_VECTORSC(dest, src, mask)
d1183 1
a1183 1
        vdest = in_over (vsrc, splat_alpha (vsrc), vmask, vdest);
d1185 11
a1195 1
        STORE_VECTOR(dest)
d1197 3
a1199 4
        mask+=4;
        src+=4;
        dest+=4;
    }
d1201 1
a1201 7
    for (i = width%4; --i >=0;) {
        uint32_t a = mask[i];
        uint32_t s = src[i];
        uint32_t d = dest[i];
        FbByteMulC (s, a);
        FbByteMulAddC (d, ~a, s);
        dest[i] = d;
d1205 7
a1211 3
static FASTCALL void
vmxCombineOverReverseC (pixman_implementation_t *imp, pixman_op_t op,
			uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d1214 1
a1214 1
    vector unsigned int  vdest, vsrc, vmask;
d1216 3
a1218 1
                         dest_mask, mask_mask, src_mask, store_mask;
a1219 1
    COMPUTE_SHIFT_MASKC(dest, src, mask);
d1221 3
a1223 1
    for (i = width/4; i > 0; i--) {
d1225 1
a1225 1
        LOAD_VECTORSC (dest, src, mask)
d1227 1
a1227 1
        vdest = over (vdest, splat_alpha (vdest), pix_multiply (vsrc, vmask));
d1229 11
a1239 1
        STORE_VECTOR(dest)
d1241 2
a1242 4
        mask+=4;
        src+=4;
        dest+=4;
    }
d1244 1
a1244 8
    for (i = width%4; --i >=0;) {
        uint32_t a = mask[i];
        uint32_t s = src[i];
        uint32_t d = dest[i];
        uint32_t da = Alpha (d);
        FbByteMulC (s, a);
        FbByteMulAddC (s, ~da, d);
        dest[i] = s;
d1248 7
a1254 3
static FASTCALL void
vmxCombineInC (pixman_implementation_t *imp, pixman_op_t op,
	       uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d1257 1
a1257 1
    vector unsigned int  vdest, vsrc, vmask;
d1259 1
a1259 1
                         dest_mask, mask_mask, src_mask, store_mask;
d1261 1
a1261 1
    COMPUTE_SHIFT_MASKC(dest, src, mask)
d1264 5
a1268 1
    for (i = width/4; i > 0; i--) {
d1270 1
a1270 1
        LOAD_VECTORSC(dest, src, mask)
d1272 4
a1275 1
        vdest = pix_multiply (pix_multiply (vsrc, vmask), splat_alpha (vdest));
d1277 5
a1281 1
        STORE_VECTOR(dest)
d1283 2
a1284 4
        src+=4;
        dest+=4;
        mask+=4;
    }
d1286 1
a1286 7
    for (i = width%4; --i >=0;) {
        uint32_t a = mask[i];
        uint32_t s = src[i];
        uint32_t da = Alpha (dest[i]);
        FbByteMul (s, a);
        FbByteMul (s, da);
        dest[i] = s;
d1290 7
a1296 3
static FASTCALL void
vmxCombineInReverseC (pixman_implementation_t *imp, pixman_op_t op,
		      uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d1299 1
a1299 1
    vector unsigned int  vdest, vsrc, vmask;
d1301 1
a1301 1
                         dest_mask, mask_mask, src_mask, store_mask;
d1303 1
a1303 1
    COMPUTE_SHIFT_MASKC(dest, src, mask)
d1306 2
a1307 1
    for (i = width/4; i > 0; i--) {
d1309 1
a1309 1
        LOAD_VECTORSC(dest, src, mask)
d1311 1
a1311 1
        vdest = pix_multiply (vdest, pix_multiply (vmask, splat_alpha (vsrc)));
d1313 1
a1313 1
        STORE_VECTOR(dest)
d1315 3
a1317 3
        src+=4;
        dest+=4;
        mask+=4;
d1320 10
a1329 7
    for (i = width%4; --i >=0;) {
        uint32_t a = mask[i];
        uint32_t d = dest[i];
        uint32_t sa = Alpha (src[i]);
        FbByteMul (a, sa);
        FbByteMulC (d, a);
        dest[i] = d;
d1333 7
a1339 3
static FASTCALL void
vmxCombineOutC (pixman_implementation_t *imp, pixman_op_t op,
		uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d1342 1
a1342 1
    vector unsigned int  vdest, vsrc, vmask;
d1344 1
a1344 1
                         dest_mask, mask_mask, src_mask, store_mask;
d1346 1
a1346 1
    COMPUTE_SHIFT_MASKC(dest, src, mask)
d1349 3
a1351 1
    for (i = width/4; i > 0; i--) {
d1353 2
a1354 1
        LOAD_VECTORSC(dest, src, mask)
d1356 1
a1356 1
        vdest = pix_multiply (pix_multiply (vsrc, vmask), splat_alpha (vdest));
d1358 4
a1361 1
        STORE_VECTOR(dest)
d1363 9
a1371 4
        src+=4;
        dest+=4;
        mask+=4;
    }
d1373 1
a1373 8
    for (i = width%4; --i >=0;) {
        uint32_t a = mask[i];
        uint32_t s = src[i];
        uint32_t d = dest[i];
        uint32_t da = Alpha (~d);
        FbByteMulC (s, a);
        FbByteMulC (s, da);
        dest[i] = s;
d1377 7
a1383 3
static FASTCALL void
vmxCombineOutReverseC (pixman_implementation_t *imp, pixman_op_t op,
		       uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d1386 1
a1386 1
    vector unsigned int  vdest, vsrc, vmask;
d1388 1
a1388 1
                         dest_mask, mask_mask, src_mask, store_mask;
d1390 1
a1390 1
    COMPUTE_SHIFT_MASKC(dest, src, mask)
d1393 3
a1395 1
    for (i = width/4; i > 0; i--) {
d1397 2
a1398 1
        LOAD_VECTORSC(dest, src, mask)
d1400 1
a1400 2
        vdest = pix_multiply (vdest,
                             negate (pix_multiply (vmask, splat_alpha (vsrc))));
d1402 11
a1412 1
        STORE_VECTOR(dest)
d1414 2
a1415 4
        src+=4;
        dest+=4;
        mask+=4;
    }
d1417 1
a1417 8
    for (i = width%4; --i >=0;) {
        uint32_t a = mask[i];
        uint32_t s = src[i];
        uint32_t d = dest[i];
        uint32_t sa = Alpha (s);
        FbByteMulC (a, sa);
        FbByteMulC (d, ~a);
        dest[i] = d;
d1421 7
a1427 3
static FASTCALL void
vmxCombineAtopC (pixman_implementation_t *imp, pixman_op_t op,
		 uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d1430 1
a1430 1
    vector unsigned int  vdest, vsrc, vmask;
d1432 1
a1432 1
                         dest_mask, mask_mask, src_mask, store_mask;
d1434 1
a1434 1
    COMPUTE_SHIFT_MASKC(dest, src, mask)
d1437 5
a1441 1
    for (i = width/4; i > 0; i--) {
d1443 2
a1444 1
        LOAD_VECTORSC(dest, src, mask)
d1446 2
a1447 4
        vdest = pix_add_mul (pix_multiply (vsrc, vmask), splat_alpha (vdest),
                            vdest,
                            negate (pix_multiply (vmask,
                                                splat_alpha (vmask))));
d1449 1
a1449 1
        STORE_VECTOR(dest)
d1451 3
a1453 3
        src+=4;
        dest+=4;
        mask+=4;
d1456 11
a1466 6
    for (i = width%4; --i >=0;) {
        uint32_t a = mask[i];
        uint32_t s = src[i];
        uint32_t d = dest[i];
        uint32_t sa = Alpha (s);
        uint32_t da = Alpha (d);
d1468 1
a1468 4
        FbByteMulC (s, a);
        FbByteMul (a, sa);
        FbByteAddMulC (d, ~a, s, da);
        dest[i] = d;
d1472 7
a1478 3
static FASTCALL void
vmxCombineAtopReverseC (pixman_implementation_t *imp, pixman_op_t op,
			uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d1481 1
a1481 1
    vector unsigned int  vdest, vsrc, vmask;
d1483 1
a1483 1
                         dest_mask, mask_mask, src_mask, store_mask;
d1485 1
a1485 1
    COMPUTE_SHIFT_MASKC(dest, src, mask)
d1488 3
a1490 1
    for (i = width/4; i > 0; i--) {
d1492 4
a1495 1
        LOAD_VECTORSC(dest, src, mask)
d1497 1
a1497 4
        vdest = pix_add_mul (vdest,
                            pix_multiply (vmask, splat_alpha (vsrc)),
                            pix_multiply (vsrc, vmask),
                            negate (splat_alpha (vdest)));
d1499 4
a1502 1
        STORE_VECTOR(dest)
d1504 7
a1510 4
        src+=4;
        dest+=4;
        mask+=4;
    }
d1512 3
a1514 6
    for (i = width%4; --i >=0;) {
        uint32_t a = mask[i];
        uint32_t s = src[i];
        uint32_t d = dest[i];
        uint32_t sa = Alpha (s);
        uint32_t da = Alpha (d);
d1516 1
a1516 4
        FbByteMulC (s, a);
        FbByteMul (a, sa);
        FbByteAddMulC (d, a, s, ~da);
        dest[i] = d;
d1520 7
a1526 3
static FASTCALL void
vmxCombineXorC (pixman_implementation_t *imp, pixman_op_t op,
		uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d1529 1
a1529 1
    vector unsigned int  vdest, vsrc, vmask;
d1531 1
a1531 1
                         dest_mask, mask_mask, src_mask, store_mask;
d1533 1
a1533 1
    COMPUTE_SHIFT_MASKC(dest, src, mask)
d1536 3
a1538 1
    for (i = width/4; i > 0; i--) {
d1540 4
a1543 1
        LOAD_VECTORSC(dest, src, mask)
d1545 1
a1545 4
        vdest = pix_add_mul (vdest,
                            negate (pix_multiply (vmask, splat_alpha (vsrc))),
                            pix_multiply (vsrc, vmask),
                            negate (splat_alpha (vdest)));
d1547 4
a1550 1
        STORE_VECTOR(dest)
d1552 7
a1558 4
        src+=4;
        dest+=4;
        mask+=4;
    }
d1560 3
a1562 6
    for (i = width%4; --i >=0;) {
        uint32_t a = mask[i];
        uint32_t s = src[i];
        uint32_t d = dest[i];
        uint32_t sa = Alpha (s);
        uint32_t da = Alpha (d);
d1564 1
a1564 4
        FbByteMulC (s, a);
        FbByteMul (a, sa);
        FbByteAddMulC (d, ~a, s, ~da);
        dest[i] = d;
d1568 7
a1574 3
static FASTCALL void
vmxCombineAddC (pixman_implementation_t *imp, pixman_op_t op,
		uint32_t *dest, const uint32_t *src, const uint32_t *mask, int width)
d1577 1
a1577 1
    vector unsigned int  vdest, vsrc, vmask;
d1579 1
a1579 1
                         dest_mask, mask_mask, src_mask, store_mask;
d1581 1
a1581 1
    COMPUTE_SHIFT_MASKC(dest, src, mask)
d1584 3
a1586 3
    for (i = width/4; i > 0; i--) {

        LOAD_VECTORSC(dest, src, mask)
d1588 1
a1588 1
        vdest = pix_add (pix_multiply (vsrc, vmask), vdest);
d1590 1
a1590 1
        STORE_VECTOR(dest)
d1592 3
a1594 3
        src+=4;
        dest+=4;
        mask+=4;
d1597 1
a1597 39
    for (i = width%4; --i >=0;) {
        uint32_t a = mask[i];
        uint32_t s = src[i];
        uint32_t d = dest[i];

        FbByteMulC (s, a);
        FbByteAdd (s, d);
        dest[i] = s;
    }
}


#if 0
void
fbCompositeSolid_nx8888vmx (pixman_operator_t	op,
			    pixman_image_t * pSrc,
			    pixman_image_t * pMask,
			    pixman_image_t * pDst,
			    int16_t	xSrc,
			    int16_t	ySrc,
			    int16_t	xMask,
			    int16_t	yMask,
			    int16_t	xDst,
			    int16_t	yDst,
			    uint16_t	width,
			    uint16_t	height)
{
    uint32_t	src;
    uint32_t	*dstLine, *dst;
    int	dstStride;

    fbComposeGetSolid (pSrc, pDst, src);

    if (src >> 24 == 0)
	return;

    fbComposeGetStart (pDst, xDst, yDst, uint32_t, dstStride, dstLine, 1);

    while (height--)
d1599 3
a1601 5
	dst = dstLine;
	dstLine += dstStride;
	/* XXX vmxCombineOverU (dst, src, width); */
    }
}
d1603 2
a1604 23
void
fbCompositeSolid_nx0565vmx (pixman_operator_t	op,
			    pixman_image_t * pSrc,
			    pixman_image_t * pMask,
			    pixman_image_t * pDst,
			    int16_t	xSrc,
			    int16_t	ySrc,
			    int16_t	xMask,
			    int16_t	yMask,
			    int16_t	xDst,
			    int16_t	yDst,
			    uint16_t	width,
			    uint16_t	height)
{
    uint32_t	src;
    uint16_t	*dstLine, *dst;
    uint16_t	w;
    int	dstStride;

    fbComposeGetSolid (pSrc, pDst, src);

    if (src >> 24 == 0)
	return;
d1606 1
a1606 7
    fbComposeGetStart (pDst, xDst, yDst, uint16_t, dstStride, dstLine, 1);

    while (height--)
    {
	dst = dstLine;
	dstLine += dstStride;
       vmxCombineOverU565(dst, src, width);
d1610 2
a1611 1
static const FastPathInfo vmx_fast_path_array[] =
d1613 2
a1614 2
    { PIXMAN_OP_NONE },
};
d1616 1
a1616 3
const FastPathInfo *const vmx_fast_paths = vmx_fast_path_array;

#endif
d1618 23
a1640 5
pixman_implementation_t *
_pixman_implementation_create_vmx (pixman_implementation_t *toplevel)
{
    pixman_implementation_t *fast = _pixman_implementation_create_fast_path (NULL);
    pixman_implementation_t *imp = _pixman_implementation_create (toplevel, fast);
a1641 27
    /* Set up function pointers */
    
    /* SSE code patch for fbcompose.c */
    imp->combine_32[PIXMAN_OP_OVER] = vmxCombineOverU;
    imp->combine_32[PIXMAN_OP_OVER_REVERSE] = vmxCombineOverReverseU;
    imp->combine_32[PIXMAN_OP_IN] = vmxCombineInU;
    imp->combine_32[PIXMAN_OP_IN_REVERSE] = vmxCombineInReverseU;
    imp->combine_32[PIXMAN_OP_OUT] = vmxCombineOutU;
    imp->combine_32[PIXMAN_OP_OUT_REVERSE] = vmxCombineOutReverseU;
    imp->combine_32[PIXMAN_OP_ATOP] = vmxCombineAtopU;
    imp->combine_32[PIXMAN_OP_ATOP_REVERSE] = vmxCombineAtopReverseU;
    imp->combine_32[PIXMAN_OP_XOR] = vmxCombineXorU;

    imp->combine_32[PIXMAN_OP_ADD] = vmxCombineAddU;

    imp->combine_32_ca[PIXMAN_OP_SRC] = vmxCombineSrcC;
    imp->combine_32_ca[PIXMAN_OP_OVER] = vmxCombineOverC;
    imp->combine_32_ca[PIXMAN_OP_OVER_REVERSE] = vmxCombineOverReverseC;
    imp->combine_32_ca[PIXMAN_OP_IN] = vmxCombineInC;
    imp->combine_32_ca[PIXMAN_OP_IN_REVERSE] = vmxCombineInReverseC;
    imp->combine_32_ca[PIXMAN_OP_OUT] = vmxCombineOutC;
    imp->combine_32_ca[PIXMAN_OP_OUT_REVERSE] = vmxCombineOutReverseC;
    imp->combine_32_ca[PIXMAN_OP_ATOP] = vmxCombineAtopC;
    imp->combine_32_ca[PIXMAN_OP_ATOP_REVERSE] = vmxCombineAtopReverseC;
    imp->combine_32_ca[PIXMAN_OP_XOR] = vmxCombineXorC;
    imp->combine_32_ca[PIXMAN_OP_ADD] = vmxCombineAddC;
    
a1643 1

@


1.1
log
@pixman 0.12.0. Tested on a full ports build by naddy@@.
@
text
@d29 1
a29 1
#include "pixman-vmx.h"
d33 1
a33 3
#ifdef __GNUC__
#   define inline __inline__ __attribute__ ((__always_inline__))
#endif
d35 1
a35 1
static inline vector unsigned int
d42 1
a42 1
static inline vector unsigned int
d81 1
a81 1
static inline vector unsigned int
d88 1
a88 1
static inline vector unsigned int
d144 1
a144 1
static inline vector unsigned int
d150 1
a150 1
static inline vector unsigned int
d206 6
d220 1
a220 1
vmxCombineMaskU (uint32_t *src, const uint32_t *msk, int width)
d223 1
a223 1
    vector unsigned int  vsrc, vmsk;
d225 1
a225 1
                         src_mask, msk_mask, store_mask;
d227 1
a227 1
    COMPUTE_SHIFT_MASKS(src, msk)
d232 1
a232 1
        LOAD_VECTORS(src, msk)
d234 1
a234 1
        vsrc = pix_multiply (vsrc, splat_alpha (vmsk));
d236 1
a236 1
        STORE_VECTOR(src)
a237 1
        msk+=4;
d239 1
d242 1
a242 2
    for (i = width%4; --i >= 0;) {
        uint32_t a = msk[i] >> 24;
d244 5
a248 2
        FbByteMul (s, a);
        src[i] = s;
d253 4
a256 1
vmxCombineOverU (uint32_t *dest, const uint32_t *src, int width)
d259 1
a259 1
    vector unsigned int  vdest, vsrc;
d261 1
a261 1
                         dest_mask, src_mask, store_mask;
d263 1
a263 1
    COMPUTE_SHIFT_MASKS(dest, src)
d268 1
a268 1
        LOAD_VECTORS(dest, src)
d276 1
d280 1
d283 5
a287 1
        uint32_t ia = Alpha (~s);
d294 10
d306 1
a306 1
vmxCombineOverReverseU (uint32_t *dest, const uint32_t *src, int width)
d339 52
a390 1
vmxCombineInU (uint32_t *dest, const uint32_t *src, int width)
d422 51
a472 1
vmxCombineInReverseU (uint32_t *dest, const uint32_t *src, int width)
d503 52
a554 1
vmxCombineOutU (uint32_t *dest, const uint32_t *src, int width)
d585 51
a635 1
vmxCombineOutReverseU (uint32_t *dest, const uint32_t *src, int width)
d666 54
a719 1
vmxCombineAtopU (uint32_t *dest, const uint32_t *src, int width)
d754 58
a811 1
vmxCombineAtopReverseU (uint32_t *dest, const uint32_t *src, int width)
d846 58
a903 1
vmxCombineXorU (uint32_t *dest, const uint32_t *src, int width)
d938 58
a995 1
vmxCombineAddU (uint32_t *dest, const uint32_t *src, int width)
d1025 54
a1078 1
vmxCombineSrcC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d1109 2
a1110 1
vmxCombineOverC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d1143 2
a1144 1
vmxCombineOverReverseC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d1178 2
a1179 1
vmxCombineInC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d1213 2
a1214 1
vmxCombineInReverseC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d1248 2
a1249 1
vmxCombineOutC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d1284 2
a1285 1
vmxCombineOutReverseC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d1321 2
a1322 1
vmxCombineAtopC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d1363 2
a1364 1
vmxCombineAtopReverseC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d1405 2
a1406 1
vmxCombineXorC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d1447 2
a1448 1
vmxCombineAddC (uint32_t *dest, uint32_t *src, uint32_t *mask, int width)
d1551 7
d1560 2
a1561 1
void fbComposeSetupVMX (void)
d1563 2
a1564 24
    /* check if we have VMX support and initialize accordingly */
    if (pixman_have_vmx ()) {
        pixman_composeFunctions.combineU[PIXMAN_OP_OVER] = vmxCombineOverU;
        pixman_composeFunctions.combineU[PIXMAN_OP_OVER_REVERSE] = vmxCombineOverReverseU;
        pixman_composeFunctions.combineU[PIXMAN_OP_IN] = vmxCombineInU;
        pixman_composeFunctions.combineU[PIXMAN_OP_IN_REVERSE] = vmxCombineInReverseU;
        pixman_composeFunctions.combineU[PIXMAN_OP_OUT] = vmxCombineOutU;
        pixman_composeFunctions.combineU[PIXMAN_OP_OUT_REVERSE] = vmxCombineOutReverseU;
        pixman_composeFunctions.combineU[PIXMAN_OP_ATOP] = vmxCombineAtopU;
        pixman_composeFunctions.combineU[PIXMAN_OP_ATOP_REVERSE] = vmxCombineAtopReverseU;
        pixman_composeFunctions.combineU[PIXMAN_OP_XOR] = vmxCombineXorU;
        pixman_composeFunctions.combineU[PIXMAN_OP_ADD] = vmxCombineAddU;

        pixman_composeFunctions.combineC[PIXMAN_OP_SRC] = vmxCombineSrcC;
        pixman_composeFunctions.combineC[PIXMAN_OP_OVER] = vmxCombineOverC;
        pixman_composeFunctions.combineC[PIXMAN_OP_OVER_REVERSE] = vmxCombineOverReverseC;
        pixman_composeFunctions.combineC[PIXMAN_OP_IN] = vmxCombineInC;
        pixman_composeFunctions.combineC[PIXMAN_OP_IN_REVERSE] = vmxCombineInReverseC;
        pixman_composeFunctions.combineC[PIXMAN_OP_OUT] = vmxCombineOutC;
        pixman_composeFunctions.combineC[PIXMAN_OP_OUT_REVERSE] = vmxCombineOutReverseC;
        pixman_composeFunctions.combineC[PIXMAN_OP_ATOP] = vmxCombineAtopC;
        pixman_composeFunctions.combineC[PIXMAN_OP_ATOP_REVERSE] = vmxCombineAtopReverseC;
        pixman_composeFunctions.combineC[PIXMAN_OP_XOR] = vmxCombineXorC;
        pixman_composeFunctions.combineC[PIXMAN_OP_ADD] = vmxCombineAddC;
d1566 28
a1593 2
        pixman_composeFunctions.combineMaskU = vmxCombineMaskU;
    }
d1595 1
@

