head	1.12;
access;
symbols
	OPENBSD_4_6:1.11.0.6
	OPENBSD_4_6_BASE:1.11
	OPENBSD_4_5:1.11.0.2
	OPENBSD_4_5_BASE:1.11
	PERL_5_10_0:1.1.1.11
	OPENBSD_4_4:1.10.0.10
	OPENBSD_4_4_BASE:1.10
	OPENBSD_4_3:1.10.0.8
	OPENBSD_4_3_BASE:1.10
	OPENBSD_4_2:1.10.0.6
	OPENBSD_4_2_BASE:1.10
	OPENBSD_4_1:1.10.0.4
	OPENBSD_4_1_BASE:1.10
	OPENBSD_4_0:1.10.0.2
	OPENBSD_4_0_BASE:1.10
	PERL_5_8_8:1.1.1.10
	OPENBSD_3_9:1.9.0.8
	OPENBSD_3_9_BASE:1.9
	OPENBSD_3_8:1.9.0.6
	OPENBSD_3_8_BASE:1.9
	OPENBSD_3_7:1.9.0.4
	OPENBSD_3_7_BASE:1.9
	PERL_5_8_6:1.1.1.9
	OPENBSD_3_6:1.9.0.2
	OPENBSD_3_6_BASE:1.9
	PERL_5_8_5:1.1.1.8
	PERL_5_8_3:1.1.1.7
	OPENBSD_3_5:1.7.0.2
	OPENBSD_3_5_BASE:1.7
	PERL_5_8_2:1.1.1.6
	OPENBSD_3_4:1.6.0.4
	OPENBSD_3_4_BASE:1.6
	OPENBSD_3_3:1.6.0.2
	OPENBSD_3_3_BASE:1.6
	PERL_5_8_0:1.1.1.5
	OPENBSD_3_2:1.5.0.6
	OPENBSD_3_2_BASE:1.5
	OPENBSD_3_1:1.5.0.4
	OPENBSD_3_1_BASE:1.5
	OPENBSD_3_0:1.5.0.2
	OPENBSD_3_0_BASE:1.5
	PERL_5_6_1:1.1.1.4
	OPENBSD_2_9:1.4.0.6
	OPENBSD_2_9_BASE:1.4
	OPENBSD_2_8:1.4.0.4
	OPENBSD_2_8_BASE:1.4
	OPENBSD_2_7:1.4.0.2
	OPENBSD_2_7_BASE:1.4
	PERL_5_6_0:1.1.1.3
	OPENBSD_2_6:1.3.0.2
	OPENBSD_2_6_BASE:1.3
	PERL_500503:1.1.1.2
	CPAN:1.1.1
	OPENBSD_2_5:1.2.0.6
	OPENBSD_2_5_BASE:1.2
	OPENBSD_2_4:1.2.0.4
	OPENBSD_2_4_BASE:1.2
	OPENBSD_2_3:1.2.0.2
	OPENBSD_2_3_BASE:1.2
	OPENBSD_2_2:1.1.1.1.0.6
	OPENBSD_2_2_BASE:1.1.1.1
	OPENBSD_2_1:1.1.1.1.0.4
	OPENBSD_2_1_BASE:1.1.1.1
	OPENBSD_2_0:1.1.1.1.0.2
	OPENBSD_2_0_BASE:1.1.1.1
	perl5003:1.1.1.1
	lwall:1.1.1;
locks; strict;
comment	@# @;


1.12
date	2009.10.12.18.24.42;	author millert;	state dead;
branches;
next	1.11;

1.11
date	2008.09.29.17.36.14;	author millert;	state Exp;
branches;
next	1.10;

1.10
date	2006.03.28.19.23.08;	author millert;	state Exp;
branches;
next	1.9;

1.9
date	2004.08.09.18.09.48;	author millert;	state Exp;
branches;
next	1.8;

1.8
date	2004.04.07.21.33.06;	author millert;	state Exp;
branches;
next	1.7;

1.7
date	2003.12.03.03.02.41;	author millert;	state Exp;
branches;
next	1.6;

1.6
date	2002.10.27.22.25.27;	author millert;	state Exp;
branches;
next	1.5;

1.5
date	2001.05.24.18.35.38;	author millert;	state Exp;
branches;
next	1.4;

1.4
date	2000.04.06.17.06.53;	author millert;	state Exp;
branches;
next	1.3;

1.3
date	99.04.29.22.52.00;	author millert;	state Exp;
branches;
next	1.2;

1.2
date	97.11.30.07.58.01;	author millert;	state Exp;
branches;
next	1.1;

1.1
date	96.08.19.10.12.50;	author downsj;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	96.08.19.10.12.50;	author downsj;	state Exp;
branches;
next	1.1.1.2;

1.1.1.2
date	99.04.29.22.40.18;	author millert;	state Exp;
branches;
next	1.1.1.3;

1.1.1.3
date	2000.04.06.16.09.49;	author millert;	state Exp;
branches;
next	1.1.1.4;

1.1.1.4
date	2001.05.24.18.23.37;	author millert;	state Exp;
branches;
next	1.1.1.5;

1.1.1.5
date	2002.10.27.22.15.01;	author millert;	state Exp;
branches;
next	1.1.1.6;

1.1.1.6
date	2003.12.03.02.44.09;	author millert;	state Exp;
branches;
next	1.1.1.7;

1.1.1.7
date	2004.04.07.21.13.22;	author millert;	state Exp;
branches;
next	1.1.1.8;

1.1.1.8
date	2004.08.09.17.47.22;	author millert;	state Exp;
branches;
next	1.1.1.9;

1.1.1.9
date	2005.01.15.21.17.30;	author millert;	state Exp;
branches;
next	1.1.1.10;

1.1.1.10
date	2006.03.28.18.48.58;	author millert;	state Exp;
branches;
next	1.1.1.11;

1.1.1.11
date	2008.09.29.17.18.42;	author millert;	state Exp;
branches;
next	;


desc
@@


1.12
log
@Merge in perl 5.10.1
@
text
@# -*- Mode: cperl; cperl-indent-level: 4 -*-

package Test::Harness;

require 5.00405;
use Test::Harness::Straps;
use Test::Harness::Assert;
use Exporter;
use Benchmark;
use Config;
use strict;


use vars qw(
    $VERSION 
    @@ISA @@EXPORT @@EXPORT_OK 
    $Verbose $Switches $Debug
    $verbose $switches $debug
    $Columns
    $Timer
    $ML $Last_ML_Print
    $Strap
    $has_time_hires
);

BEGIN {
    eval q{use Time::HiRes 'time'};
    $has_time_hires = !$@@;
}

=head1 NAME

Test::Harness - Run Perl standard test scripts with statistics

=head1 VERSION

Version 2.64

=cut

$VERSION = '2.64';

# Backwards compatibility for exportable variable names.
*verbose  = *Verbose;
*switches = *Switches;
*debug    = *Debug;

$ENV{HARNESS_ACTIVE} = 1;
$ENV{HARNESS_VERSION} = $VERSION;

END {
    # For VMS.
    delete $ENV{HARNESS_ACTIVE};
    delete $ENV{HARNESS_VERSION};
}

my $Files_In_Dir = $ENV{HARNESS_FILELEAK_IN_DIR};

# Stolen from Params::Util
sub _CLASS {
    (defined $_[0] and ! ref $_[0] and $_[0] =~ m/^[^\W\d]\w*(?:::\w+)*$/s) ? $_[0] : undef;
}

# Strap Overloading
if ( $ENV{HARNESS_STRAPS_CLASS} ) {
    die 'Set HARNESS_STRAP_CLASS, singular, not HARNESS_STRAPS_CLASS';
}
my $HARNESS_STRAP_CLASS  = $ENV{HARNESS_STRAP_CLASS} || 'Test::Harness::Straps';
if ( $HARNESS_STRAP_CLASS =~ /\.pm$/ ) {
    # "Class" is actually a filename, that should return the
    # class name as its true return value.
    $HARNESS_STRAP_CLASS = require $HARNESS_STRAP_CLASS;
    if ( !_CLASS($HARNESS_STRAP_CLASS) ) {
        die "HARNESS_STRAP_CLASS '$HARNESS_STRAP_CLASS' is not a valid class name";
    }
}
else {
    # It is a class name within the current @@INC
    if ( !_CLASS($HARNESS_STRAP_CLASS) ) {
        die "HARNESS_STRAP_CLASS '$HARNESS_STRAP_CLASS' is not a valid class name";
    }
    eval "require $HARNESS_STRAP_CLASS";
    die $@@ if $@@;
}
if ( !$HARNESS_STRAP_CLASS->isa('Test::Harness::Straps') ) {
    die "HARNESS_STRAP_CLASS '$HARNESS_STRAP_CLASS' must be a Test::Harness::Straps subclass";
}

$Strap = $HARNESS_STRAP_CLASS->new;

sub strap { return $Strap };

@@ISA = ('Exporter');
@@EXPORT    = qw(&runtests);
@@EXPORT_OK = qw(&execute_tests $verbose $switches);

$Verbose  = $ENV{HARNESS_VERBOSE} || 0;
$Debug    = $ENV{HARNESS_DEBUG} || 0;
$Switches = '-w';
$Columns  = $ENV{HARNESS_COLUMNS} || $ENV{COLUMNS} || 80;
$Columns--;             # Some shells have trouble with a full line of text.
$Timer    = $ENV{HARNESS_TIMER} || 0;

=head1 SYNOPSIS

  use Test::Harness;

  runtests(@@test_files);

=head1 DESCRIPTION

B<STOP!> If all you want to do is write a test script, consider
using Test::Simple.  Test::Harness is the module that reads the
output from Test::Simple, Test::More and other modules based on
Test::Builder.  You don't need to know about Test::Harness to use
those modules.

Test::Harness runs tests and expects output from the test in a
certain format.  That format is called TAP, the Test Anything
Protocol.  It is defined in L<Test::Harness::TAP>.

C<Test::Harness::runtests(@@tests)> runs all the testscripts named
as arguments and checks standard output for the expected strings
in TAP format.

The F<prove> utility is a thin wrapper around Test::Harness.

=head2 Taint mode

Test::Harness will honor the C<-T> or C<-t> in the #! line on your
test files.  So if you begin a test with:

    #!perl -T

the test will be run with taint mode on.

=head2 Configuration variables.

These variables can be used to configure the behavior of
Test::Harness.  They are exported on request.

=over 4

=item C<$Test::Harness::Verbose>

The package variable C<$Test::Harness::Verbose> is exportable and can be
used to let C<runtests()> display the standard output of the script
without altering the behavior otherwise.  The F<prove> utility's C<-v>
flag will set this.

=item C<$Test::Harness::switches>

The package variable C<$Test::Harness::switches> is exportable and can be
used to set perl command line options used for running the test
script(s). The default value is C<-w>. It overrides C<HARNESS_PERL_SWITCHES>.

=item C<$Test::Harness::Timer>

If set to true, and C<Time::HiRes> is available, print elapsed seconds
after each test file.

=back


=head2 Failure

When tests fail, analyze the summary report:

  t/base..............ok
  t/nonumbers.........ok
  t/ok................ok
  t/test-harness......ok
  t/waterloo..........dubious
          Test returned status 3 (wstat 768, 0x300)
  DIED. FAILED tests 1, 3, 5, 7, 9, 11, 13, 15, 17, 19
          Failed 10/20 tests, 50.00% okay
  Failed Test  Stat Wstat Total Fail  List of Failed
  ---------------------------------------------------------------
  t/waterloo.t    3   768    20   10  1 3 5 7 9 11 13 15 17 19
  Failed 1/5 test scripts, 80.00% okay. 10/44 subtests failed, 77.27% okay.

Everything passed but F<t/waterloo.t>.  It failed 10 of 20 tests and
exited with non-zero status indicating something dubious happened.

The columns in the summary report mean:

=over 4

=item B<Failed Test>

The test file which failed.

=item B<Stat>

If the test exited with non-zero, this is its exit status.

=item B<Wstat>

The wait status of the test.

=item B<Total>

Total number of tests expected to run.

=item B<Fail>

Number which failed, either from "not ok" or because they never ran.

=item B<List of Failed>

A list of the tests which failed.  Successive failures may be
abbreviated (ie. 15-20 to indicate that tests 15, 16, 17, 18, 19 and
20 failed).

=back


=head1 FUNCTIONS

The following functions are available.

=head2 runtests( @@test_files )

This runs all the given I<@@test_files> and divines whether they passed
or failed based on their output to STDOUT (details above).  It prints
out each individual test which failed along with a summary report and
a how long it all took.

It returns true if everything was ok.  Otherwise it will C<die()> with
one of the messages in the DIAGNOSTICS section.

=cut

sub runtests {
    my(@@tests) = @@_;

    local ($\, $,);

    my ($tot, $failedtests,$todo_passed) = execute_tests(tests => \@@tests);
    print get_results($tot, $failedtests,$todo_passed);

    my $ok = _all_ok($tot);

    assert(($ok xor keys %$failedtests), 
           q{ok status jives with $failedtests});

    if (! $ok) {
        die("Failed $tot->{bad}/$tot->{tests} test programs. " .
            "@@{[$tot->{max} - $tot->{ok}]}/$tot->{max} subtests failed.\n");
    }

    return $ok;
}

# my $ok = _all_ok(\%tot);
# Tells you if this test run is overall successful or not.

sub _all_ok {
    my($tot) = shift;

    return $tot->{bad} == 0 && ($tot->{max} || $tot->{skipped}) ? 1 : 0;
}

# Returns all the files in a directory.  This is shorthand for backwards
# compatibility on systems where C<glob()> doesn't work right.

sub _globdir {
    local *DIRH;

    opendir DIRH, shift;
    my @@f = readdir DIRH;
    closedir DIRH;

    return @@f;
}

=head2 execute_tests( tests => \@@test_files, out => \*FH )

Runs all the given C<@@test_files> (just like C<runtests()>) but
doesn't generate the final report.  During testing, progress
information will be written to the currently selected output
filehandle (usually C<STDOUT>), or to the filehandle given by the
C<out> parameter.  The I<out> is optional.

Returns a list of two values, C<$total> and C<$failed>, describing the
results.  C<$total> is a hash ref summary of all the tests run.  Its
keys and values are this:

    bonus           Number of individual todo tests unexpectedly passed
    max             Number of individual tests ran
    ok              Number of individual tests passed
    sub_skipped     Number of individual tests skipped
    todo            Number of individual todo tests

    files           Number of test files ran
    good            Number of test files passed
    bad             Number of test files failed
    tests           Number of test files originally given
    skipped         Number of test files skipped

If C<< $total->{bad} == 0 >> and C<< $total->{max} > 0 >>, you've
got a successful test.

C<$failed> is a hash ref of all the test scripts that failed.  Each key
is the name of a test script, each value is another hash representing
how that script failed.  Its keys are these:

    name        Name of the test which failed
    estat       Script's exit value
    wstat       Script's wait status
    max         Number of individual tests
    failed      Number which failed
    canon       List of tests which failed (as string).

C<$failed> should be empty if everything passed.

=cut

sub execute_tests {
    my %args = @@_;
    my @@tests = @@{$args{tests}};
    my $out = $args{out} || select();

    # We allow filehandles that are symbolic refs
    no strict 'refs';
    _autoflush($out);
    _autoflush(\*STDERR);

    my %failedtests;
    my %todo_passed;

    # Test-wide totals.
    my(%tot) = (
                bonus    => 0,
                max      => 0,
                ok       => 0,
                files    => 0,
                bad      => 0,
                good     => 0,
                tests    => scalar @@tests,
                sub_skipped  => 0,
                todo     => 0,
                skipped  => 0,
                bench    => 0,
               );

    my @@dir_files;
    @@dir_files = _globdir $Files_In_Dir if defined $Files_In_Dir;
    my $run_start_time = new Benchmark;

    my $width = _leader_width(@@tests);
    foreach my $tfile (@@tests) {
        $Last_ML_Print = 0;  # so each test prints at least once
        my($leader, $ml) = _mk_leader($tfile, $width);
        local $ML = $ml;

        print $out $leader;

        $tot{files}++;

        $Strap->{_seen_header} = 0;
        if ( $Test::Harness::Debug ) {
            print $out "# Running: ", $Strap->_command_line($tfile), "\n";
        }
        my $test_start_time = $Timer ? time : 0;
        my $results = $Strap->analyze_file($tfile) or
          do { warn $Strap->{error}, "\n";  next };
        my $elapsed;
        if ( $Timer ) {
            $elapsed = time - $test_start_time;
            if ( $has_time_hires ) {
                $elapsed = sprintf( " %8d ms", $elapsed*1000 );
            }
            else {
                $elapsed = sprintf( " %8s s", $elapsed ? $elapsed : "<1" );
            }
        }
        else {
            $elapsed = "";
        }

        # state of the current test.
        my @@failed = grep { !$results->details->[$_-1]{ok} }
                     1..@@{$results->details};
        my @@todo_pass = grep { $results->details->[$_-1]{actual_ok} &&
                               $results->details->[$_-1]{type} eq 'todo' }
                        1..@@{$results->details};

        my %test = (
            ok          => $results->ok,
            'next'      => $Strap->{'next'},
            max         => $results->max,
            failed      => \@@failed,
            todo_pass   => \@@todo_pass,
            todo        => $results->todo,
            bonus       => $results->bonus,
            skipped     => $results->skip,
            skip_reason => $results->skip_reason,
            skip_all    => $Strap->{skip_all},
            ml          => $ml,
        );

        $tot{bonus}       += $results->bonus;
        $tot{max}         += $results->max;
        $tot{ok}          += $results->ok;
        $tot{todo}        += $results->todo;
        $tot{sub_skipped} += $results->skip;

        my $estatus = $results->exit;
        my $wstatus = $results->wait;

        if ( $results->passing ) {
            # XXX Combine these first two
            if ($test{max} and $test{skipped} + $test{bonus}) {
                my @@msg;
                push(@@msg, "$test{skipped}/$test{max} skipped: $test{skip_reason}")
                    if $test{skipped};
                if ($test{bonus}) {
                    my ($txt, $canon) = _canondetail($test{todo},0,'TODO passed',
                                                    @@{$test{todo_pass}});
                    $todo_passed{$tfile} = {
                        canon   => $canon,
                        max     => $test{todo},
                        failed  => $test{bonus},
                        name    => $tfile,
                        estat   => '',
                        wstat   => '',
                    };

                    push(@@msg, "$test{bonus}/$test{max} unexpectedly succeeded\n$txt");
                }
                print $out "$test{ml}ok$elapsed\n        ".join(', ', @@msg)."\n";
            }
            elsif ( $test{max} ) {
                print $out "$test{ml}ok$elapsed\n";
            }
            elsif ( defined $test{skip_all} and length $test{skip_all} ) {
                print $out "skipped\n        all skipped: $test{skip_all}\n";
                $tot{skipped}++;
            }
            else {
                print $out "skipped\n        all skipped: no reason given\n";
                $tot{skipped}++;
            }
            $tot{good}++;
        }
        else {
            # List unrun tests as failures.
            if ($test{'next'} <= $test{max}) {
                push @@{$test{failed}}, $test{'next'}..$test{max};
            }
            # List overruns as failures.
            else {
                my $details = $results->details;
                foreach my $overrun ($test{max}+1..@@$details) {
                    next unless ref $details->[$overrun-1];
                    push @@{$test{failed}}, $overrun
                }
            }

            if ($wstatus) {
                $failedtests{$tfile} = _dubious_return(\%test, \%tot, 
                                                       $estatus, $wstatus);
                $failedtests{$tfile}{name} = $tfile;
            }
            elsif ( $results->seen ) {
                if (@@{$test{failed}} and $test{max}) {
                    my ($txt, $canon) = _canondetail($test{max},$test{skipped},'Failed',
                                                    @@{$test{failed}});
                    print $out "$test{ml}$txt";
                    $failedtests{$tfile} = { canon   => $canon,
                                             max     => $test{max},
                                             failed  => scalar @@{$test{failed}},
                                             name    => $tfile, 
                                             estat   => '',
                                             wstat   => '',
                                           };
                }
                else {
                    print $out "Don't know which tests failed: got $test{ok} ok, ".
                          "expected $test{max}\n";
                    $failedtests{$tfile} = { canon   => '??',
                                             max     => $test{max},
                                             failed  => '??',
                                             name    => $tfile, 
                                             estat   => '', 
                                             wstat   => '',
                                           };
                }
                $tot{bad}++;
            }
            else {
                print $out "FAILED before any test output arrived\n";
                $tot{bad}++;
                $failedtests{$tfile} = { canon       => '??',
                                         max         => '??',
                                         failed      => '??',
                                         name        => $tfile,
                                         estat       => '', 
                                         wstat       => '',
                                       };
            }
        }

        if (defined $Files_In_Dir) {
            my @@new_dir_files = _globdir $Files_In_Dir;
            if (@@new_dir_files != @@dir_files) {
                my %f;
                @@f{@@new_dir_files} = (1) x @@new_dir_files;
                delete @@f{@@dir_files};
                my @@f = sort keys %f;
                print $out "LEAKED FILES: @@f\n";
                @@dir_files = @@new_dir_files;
            }
        }
    } # foreach test
    $tot{bench} = timediff(new Benchmark, $run_start_time);

    $Strap->_restore_PERL5LIB;

    return(\%tot, \%failedtests, \%todo_passed);
}

# Turns on autoflush for the handle passed
sub _autoflush {
    my $flushy_fh = shift;
    my $old_fh = select $flushy_fh;
    $| = 1;
    select $old_fh;
}

=for private _mk_leader

    my($leader, $ml) = _mk_leader($test_file, $width);

Generates the 't/foo........' leader for the given C<$test_file> as well
as a similar version which will overwrite the current line (by use of
\r and such).  C<$ml> may be empty if Test::Harness doesn't think you're
on TTY.

The C<$width> is the width of the "yada/blah.." string.

=cut

sub _mk_leader {
    my($te, $width) = @@_;
    chomp($te);
    $te =~ s/\.\w+$/./;

    if ($^O eq 'VMS') {
        $te =~ s/^.*\.t\./\[.t./s;
    }
    my $leader = "$te" . '.' x ($width - length($te));
    my $ml = "";

    if ( -t STDOUT and not $ENV{HARNESS_NOTTY} and not $Verbose ) {
        $ml = "\r" . (' ' x 77) . "\r$leader"
    }

    return($leader, $ml);
}

=for private _leader_width

  my($width) = _leader_width(@@test_files);

Calculates how wide the leader should be based on the length of the
longest test name.

=cut

sub _leader_width {
    my $maxlen = 0;
    my $maxsuflen = 0;
    foreach (@@_) {
        my $suf    = /\.(\w+)$/ ? $1 : '';
        my $len    = length;
        my $suflen = length $suf;
        $maxlen    = $len    if $len    > $maxlen;
        $maxsuflen = $suflen if $suflen > $maxsuflen;
    }
    # + 3 : we want three dots between the test name and the "ok"
    return $maxlen + 3 - $maxsuflen;
}

sub get_results {
    my $tot = shift;
    my $failedtests = shift;
    my $todo_passed = shift;

    my $out = '';

    my $bonusmsg = _bonusmsg($tot);

    if (_all_ok($tot)) {
        $out .= "All tests successful$bonusmsg.\n";
        if ($tot->{bonus}) {
            my($fmt_top, $fmt) = _create_fmts("Passed TODO",$todo_passed);
            # Now write to formats
            $out .= swrite( $fmt_top );
            for my $script (sort keys %{$todo_passed||{}}) {
                my $Curtest = $todo_passed->{$script};
                $out .= swrite( $fmt, @@{ $Curtest }{qw(name estat wstat max failed canon)} );
            }
        }
    }
    elsif (!$tot->{tests}){
        die "FAILED--no tests were run for some reason.\n";
    }
    elsif (!$tot->{max}) {
        my $blurb = $tot->{tests}==1 ? "script" : "scripts";
        die "FAILED--$tot->{tests} test $blurb could be run, ".
            "alas--no output ever seen\n";
    }
    else {
        my $subresults = sprintf( " %d/%d subtests failed.",
                              $tot->{max} - $tot->{ok}, $tot->{max} );

        my($fmt_top, $fmt1, $fmt2) = _create_fmts("Failed Test",$failedtests);

        # Now write to formats
        $out .= swrite( $fmt_top );
        for my $script (sort keys %$failedtests) {
            my $Curtest = $failedtests->{$script};
            $out .= swrite( $fmt1, @@{ $Curtest }{qw(name estat wstat max failed canon)} );
            $out .= swrite( $fmt2, $Curtest->{canon} );
        }
        if ($tot->{bad}) {
            $bonusmsg =~ s/^,\s*//;
            $out .= "$bonusmsg.\n" if $bonusmsg;
            $out .= "Failed $tot->{bad}/$tot->{tests} test scripts.$subresults\n";
        }
    }

    $out .= sprintf("Files=%d, Tests=%d, %s\n",
           $tot->{files}, $tot->{max}, timestr($tot->{bench}, 'nop'));
    return $out;
}

sub swrite {
    my $format = shift;
    $^A = '';
    formline($format,@@_);
    my $out = $^A;
    $^A = '';
    return $out;
}


my %Handlers = (
    header  => \&header_handler,
    test    => \&test_handler,
    bailout => \&bailout_handler,
);

$Strap->set_callback(\&strap_callback);
sub strap_callback {
    my($self, $line, $type, $totals) = @@_;
    print $line if $Verbose;

    my $meth = $Handlers{$type};
    $meth->($self, $line, $type, $totals) if $meth;
};


sub header_handler {
    my($self, $line, $type, $totals) = @@_;

    warn "Test header seen more than once!\n" if $self->{_seen_header};

    $self->{_seen_header}++;

    warn "1..M can only appear at the beginning or end of tests\n"
      if $totals->seen && ($totals->max < $totals->seen);
};

sub test_handler {
    my($self, $line, $type, $totals) = @@_;

    my $curr = $totals->seen;
    my $next = $self->{'next'};
    my $max  = $totals->max;
    my $detail = $totals->details->[-1];

    if( $detail->{ok} ) {
        _print_ml_less("ok $curr/$max");

        if( $detail->{type} eq 'skip' ) {
            $totals->set_skip_reason( $detail->{reason} )
              unless defined $totals->skip_reason;
            $totals->set_skip_reason( 'various reasons' )
              if $totals->skip_reason ne $detail->{reason};
        }
    }
    else {
        _print_ml("NOK $curr/$max");
    }

    if( $curr > $next ) {
        print "Test output counter mismatch [test $curr]\n";
    }
    elsif( $curr < $next ) {
        print "Confused test output: test $curr answered after ".
              "test ", $next - 1, "\n";
    }

};

sub bailout_handler {
    my($self, $line, $type, $totals) = @@_;

    die "FAILED--Further testing stopped" .
      ($self->{bailout_reason} ? ": $self->{bailout_reason}\n" : ".\n");
};


sub _print_ml {
    print join '', $ML, @@_ if $ML;
}


# Print updates only once per second.
sub _print_ml_less {
    my $now = CORE::time;
    if ( $Last_ML_Print != $now ) {
        _print_ml(@@_);
        $Last_ML_Print = $now;
    }
}

sub _bonusmsg {
    my($tot) = @@_;

    my $bonusmsg = '';
    $bonusmsg = (" ($tot->{bonus} subtest".($tot->{bonus} > 1 ? 's' : '').
               " UNEXPECTEDLY SUCCEEDED)")
        if $tot->{bonus};

    if ($tot->{skipped}) {
        $bonusmsg .= ", $tot->{skipped} test"
                     . ($tot->{skipped} != 1 ? 's' : '');
        if ($tot->{sub_skipped}) {
            $bonusmsg .= " and $tot->{sub_skipped} subtest"
                         . ($tot->{sub_skipped} != 1 ? 's' : '');
        }
        $bonusmsg .= ' skipped';
    }
    elsif ($tot->{sub_skipped}) {
        $bonusmsg .= ", $tot->{sub_skipped} subtest"
                     . ($tot->{sub_skipped} != 1 ? 's' : '')
                     . " skipped";
    }
    return $bonusmsg;
}

# Test program go boom.
sub _dubious_return {
    my($test, $tot, $estatus, $wstatus) = @@_;

    my $failed = '??';
    my $canon  = '??';

    printf "$test->{ml}dubious\n\tTest returned status $estatus ".
           "(wstat %d, 0x%x)\n",
           $wstatus,$wstatus;
    print "\t\t(VMS status is $estatus)\n" if $^O eq 'VMS';

    $tot->{bad}++;

    if ($test->{max}) {
        if ($test->{'next'} == $test->{max} + 1 and not @@{$test->{failed}}) {
            print "\tafter all the subtests completed successfully\n";
            $failed = 0;        # But we do not set $canon!
        }
        else {
            push @@{$test->{failed}}, $test->{'next'}..$test->{max};
            $failed = @@{$test->{failed}};
            (my $txt, $canon) = _canondetail($test->{max},$test->{skipped},'Failed',@@{$test->{failed}});
            print "DIED. ",$txt;
        }
    }

    return { canon => $canon,  max => $test->{max} || '??',
             failed => $failed, 
             estat => $estatus, wstat => $wstatus,
           };
}


sub _create_fmts {
    my $failed_str = shift;
    my $failedtests = shift;

    my ($type) = split /\s/,$failed_str;
    my $short = substr($type,0,4);
    my $total = $short eq 'Pass' ? 'TODOs' : 'Total';
    my $middle_str = " Stat Wstat $total $short  ";
    my $list_str = "List of $type";

    # Figure out our longest name string for formatting purposes.
    my $max_namelen = length($failed_str);
    foreach my $script (keys %$failedtests) {
        my $namelen = length $failedtests->{$script}->{name};
        $max_namelen = $namelen if $namelen > $max_namelen;
    }

    my $list_len = $Columns - length($middle_str) - $max_namelen;
    if ($list_len < length($list_str)) {
        $list_len = length($list_str);
        $max_namelen = $Columns - length($middle_str) - $list_len;
        if ($max_namelen < length($failed_str)) {
            $max_namelen = length($failed_str);
            $Columns = $max_namelen + length($middle_str) + $list_len;
        }
    }

    my $fmt_top =   sprintf("%-${max_namelen}s", $failed_str)
                  . $middle_str
                  . $list_str . "\n"
                  . "-" x $Columns
                  . "\n";

    my $fmt1 =  "@@" . "<" x ($max_namelen - 1)
              . "  @@>> @@>>>> @@>>>> @@>>>  "
              . "^" . "<" x ($list_len - 1) . "\n";
    my $fmt2 =  "~~" . " " x ($Columns - $list_len - 2) . "^"
              . "<" x ($list_len - 1) . "\n";

    return($fmt_top, $fmt1, $fmt2);
}

sub _canondetail {
    my $max = shift;
    my $skipped = shift;
    my $type = shift;
    my @@detail = @@_;
    my %seen;
    @@detail = sort {$a <=> $b} grep !$seen{$_}++, @@detail;
    my $detail = @@detail;
    my @@result = ();
    my @@canon = ();
    my $min;
    my $last = $min = shift @@detail;
    my $canon;
    my $uc_type = uc($type);
    if (@@detail) {
        for (@@detail, $detail[-1]) { # don't forget the last one
            if ($_ > $last+1 || $_ == $last) {
                push @@canon, ($min == $last) ? $last : "$min-$last";
                $min = $_;
            }
            $last = $_;
        }
        local $" = ", ";
        push @@result, "$uc_type tests @@canon\n";
        $canon = join ' ', @@canon;
    }
    else {
        push @@result, "$uc_type test $last\n";
        $canon = $last;
    }

    return (join("", @@result), $canon)
        if $type=~/todo/i;
    push @@result, "\t$type $detail/$max tests, ";
    if ($max) {
	push @@result, sprintf("%.2f",100*(1-$detail/$max)), "% okay";
    }
    else {
	push @@result, "?% okay";
    }
    my $ender = 's' x ($skipped > 1);
    if ($skipped) {
        my $good = $max - $detail - $skipped;
	my $skipmsg = " (less $skipped skipped test$ender: $good okay, ";
	if ($max) {
	    my $goodper = sprintf("%.2f",100*($good/$max));
	    $skipmsg .= "$goodper%)";
        }
        else {
	    $skipmsg .= "?%)";
	}
	push @@result, $skipmsg;
    }
    push @@result, "\n";
    my $txt = join "", @@result;
    return ($txt, $canon);
}

1;
__END__


=head1 EXPORT

C<&runtests> is exported by Test::Harness by default.

C<&execute_tests>, C<$verbose>, C<$switches> and C<$debug> are
exported upon request.

=head1 DIAGNOSTICS

=over 4

=item C<All tests successful.\nFiles=%d,  Tests=%d, %s>

If all tests are successful some statistics about the performance are
printed.

=item C<FAILED tests %s\n\tFailed %d/%d tests, %.2f%% okay.>

For any single script that has failing subtests statistics like the
above are printed.

=item C<Test returned status %d (wstat %d)>

Scripts that return a non-zero exit status, both C<$? E<gt>E<gt> 8>
and C<$?> are printed in a message similar to the above.

=item C<Failed 1 test, %.2f%% okay. %s>

=item C<Failed %d/%d tests, %.2f%% okay. %s>

If not all tests were successful, the script dies with one of the
above messages.

=item C<FAILED--Further testing stopped: %s>

If a single subtest decides that further testing will not make sense,
the script dies with this message.

=back

=head1 ENVIRONMENT VARIABLES THAT TEST::HARNESS SETS

Test::Harness sets these before executing the individual tests.

=over 4

=item C<HARNESS_ACTIVE>

This is set to a true value.  It allows the tests to determine if they
are being executed through the harness or by any other means.

=item C<HARNESS_VERSION>

This is the version of Test::Harness.

=back

=head1 ENVIRONMENT VARIABLES THAT AFFECT TEST::HARNESS

=over 4

=item C<HARNESS_COLUMNS>

This value will be used for the width of the terminal. If it is not
set then it will default to C<COLUMNS>. If this is not set, it will
default to 80. Note that users of Bourne-sh based shells will need to
C<export COLUMNS> for this module to use that variable.

=item C<HARNESS_COMPILE_TEST>

When true it will make harness attempt to compile the test using
C<perlcc> before running it.

B<NOTE> This currently only works when sitting in the perl source
directory!

=item C<HARNESS_DEBUG>

If true, Test::Harness will print debugging information about itself as
it runs the tests.  This is different from C<HARNESS_VERBOSE>, which prints
the output from the test being run.  Setting C<$Test::Harness::Debug> will
override this, or you can use the C<-d> switch in the F<prove> utility.

=item C<HARNESS_FILELEAK_IN_DIR>

When set to the name of a directory, harness will check after each
test whether new files appeared in that directory, and report them as

  LEAKED FILES: scr.tmp 0 my.db

If relative, directory name is with respect to the current directory at
the moment runtests() was called.  Putting absolute path into 
C<HARNESS_FILELEAK_IN_DIR> may give more predictable results.

=item C<HARNESS_NOTTY>

When set to a true value, forces it to behave as though STDOUT were
not a console.  You may need to set this if you don't want harness to
output more frequent progress messages using carriage returns.  Some
consoles may not handle carriage returns properly (which results in a
somewhat messy output).

=item C<HARNESS_PERL>

Usually your tests will be run by C<$^X>, the currently-executing Perl.
However, you may want to have it run by a different executable, such as
a threading perl, or a different version.

If you're using the F<prove> utility, you can use the C<--perl> switch.

=item C<HARNESS_PERL_SWITCHES>

Its value will be prepended to the switches used to invoke perl on
each test.  For example, setting C<HARNESS_PERL_SWITCHES> to C<-W> will
run all tests with all warnings enabled.

=item C<HARNESS_TIMER>

Setting this to true will make the harness display the number of
milliseconds each test took.  You can also use F<prove>'s C<--timer>
switch.

=item C<HARNESS_VERBOSE>

If true, Test::Harness will output the verbose results of running
its tests.  Setting C<$Test::Harness::verbose> will override this,
or you can use the C<-v> switch in the F<prove> utility.

If true, Test::Harness will output the verbose results of running
its tests.  Setting C<$Test::Harness::verbose> will override this,
or you can use the C<-v> switch in the F<prove> utility.

=item C<HARNESS_STRAP_CLASS>

Defines the Test::Harness::Straps subclass to use.  The value may either
be a filename or a class name.

If HARNESS_STRAP_CLASS is a class name, the class must be in C<@@INC>
like any other class.

If HARNESS_STRAP_CLASS is a filename, the .pm file must return the name
of the class, instead of the canonical "1".

=back

=head1 EXAMPLE

Here's how Test::Harness tests itself

  $ cd ~/src/devel/Test-Harness
  $ perl -Mblib -e 'use Test::Harness qw(&runtests $verbose);
    $verbose=0; runtests @@ARGV;' t/*.t
  Using /home/schwern/src/devel/Test-Harness/blib
  t/base..............ok
  t/nonumbers.........ok
  t/ok................ok
  t/test-harness......ok
  All tests successful.
  Files=4, Tests=24, 2 wallclock secs ( 0.61 cusr + 0.41 csys = 1.02 CPU)

=head1 SEE ALSO

The included F<prove> utility for running test scripts from the command line,
L<Test> and L<Test::Simple> for writing test scripts, L<Benchmark> for
the underlying timing routines, and L<Devel::Cover> for test coverage
analysis.

=head1 TODO

Provide a way of running tests quietly (ie. no printing) for automated
validation of tests.  This will probably take the form of a version
of runtests() which rather than printing its output returns raw data
on the state of the tests.  (Partially done in Test::Harness::Straps)

Document the format.

Fix HARNESS_COMPILE_TEST without breaking its core usage.

Figure a way to report test names in the failure summary.

Rework the test summary so long test names are not truncated as badly.
(Partially done with new skip test styles)

Add option for coverage analysis.

Trap STDERR.

Implement Straps total_results()

Remember exit code

Completely redo the print summary code.

Straps->analyze_file() not taint clean, don't know if it can be

Fix that damned VMS nit.

Add a test for verbose.

Change internal list of test results to a hash.

Fix stats display when there's an overrun.

Fix so perls with spaces in the filename work.

Keeping whittling away at _run_all_tests()

Clean up how the summary is printed.  Get rid of those damned formats.

=head1 BUGS

Please report any bugs or feature requests to
C<bug-test-harness at rt.cpan.org>, or through the web interface at
L<http://rt.cpan.org/NoAuth/ReportBug.html?Queue=Test-Harness>.
I will be notified, and then you'll automatically be notified of progress on
your bug as I make changes.

=head1 SUPPORT

You can find documentation for this module with the F<perldoc> command.

    perldoc Test::Harness

You can get docs for F<prove> with

    prove --man

You can also look for information at:

=over 4

=item * AnnoCPAN: Annotated CPAN documentation

L<http://annocpan.org/dist/Test-Harness>

=item * CPAN Ratings

L<http://cpanratings.perl.org/d/Test-Harness>

=item * RT: CPAN's request tracker

L<http://rt.cpan.org/NoAuth/Bugs.html?Dist=Test-Harness>

=item * Search CPAN

L<http://search.cpan.org/dist/Test-Harness>

=back

=head1 SOURCE CODE

The source code repository for Test::Harness is at
L<http://svn.perl.org/modules/Test-Harness>.

=head1 AUTHORS

Either Tim Bunce or Andreas Koenig, we don't know. What we know for
sure is, that it was inspired by Larry Wall's F<TEST> script that came
with perl distributions for ages. Numerous anonymous contributors
exist.  Andreas Koenig held the torch for many years, and then
Michael G Schwern.

Current maintainer is Andy Lester C<< <andy at petdance.com> >>.

=head1 COPYRIGHT

Copyright 2002-2006
by Michael G Schwern C<< <schwern at pobox.com> >>,
Andy Lester C<< <andy at petdance.com> >>.

This program is free software; you can redistribute it and/or 
modify it under the same terms as Perl itself.

See L<http://www.perl.com/perl/misc/Artistic.html>.

=cut
@


1.11
log
@fix conflicts and merge in local changes to perl 5.10.0
@
text
@@


1.10
log
@merge in perl 5.8.8
@
text
@d19 1
a19 2
    $Curtest
    $Columns 
d27 1
a27 1
    eval "use Time::HiRes 'time'";
d37 1
a37 1
Version 2.56
d41 1
a41 1
$VERSION = "2.56";
d57 6
a62 2
# Some experimental versions of OS/2 build have broken $?
my $Ignore_Exitcode = $ENV{HARNESS_IGNORE_EXITCODE};
d64 24
a87 1
my $Files_In_Dir = $ENV{HARNESS_FILELEAK_IN_DIR};
d89 1
a89 1
$Strap = Test::Harness::Straps->new;
d95 1
a95 1
@@EXPORT_OK = qw($verbose $switches);
d99 1
a99 1
$Switches = "-w";
d155 1
a155 1
script(s). The default value is C<-w>. It overrides C<HARNESS_SWITCHES>.
d177 3
a179 3
  Failed Test  Stat Wstat Total Fail  Failed  List of Failed
  -----------------------------------------------------------------------
  t/waterloo.t    3   768    20   10  50.00%  1 3 5 7 9 11 13 15 17 19
a208 4
=item B<Failed>

Percentage of the total tests which failed.

d218 1
a218 5
=head2 Functions

Test::Harness currently only has one function, here it is.

=over 4
d220 1
a220 1
=item B<runtests>
d222 1
a222 1
  my $allok = runtests(@@test_files);
d239 2
a240 2
    my($tot, $failedtests) = _run_all_tests(@@tests);
    _show_results($tot, $failedtests);
d247 5
d255 2
a256 9
=begin _private

=item B<_all_ok>

  my $ok = _all_ok(\%tot);

Tells you if this test run is overall successful or not.

=cut
d264 2
a265 3
=item B<_globdir>

  my @@files = _globdir $dir;
d267 2
a268 2
Returns all the files in a directory.  This is shorthand for backwards
compatibility on systems where C<glob()> doesn't work right.
d270 3
a272 6
=cut

sub _globdir { 
    opendir DIRH, shift; 
    my @@f = readdir DIRH; 
    closedir DIRH; 
d277 1
a277 3
=item B<_run_all_tests>

  my($total, $failed) = _run_all_tests(@@test_files);
d279 9
a287 3
Runs all the given C<@@test_files> (as C<runtests()>) but does it
quietly (no report).  $total is a hash ref summary of all the tests
run.  Its keys and values are this:
d304 1
a304 1
$failed is a hash ref of all the test scripts which failed.  Each key
a312 1
    percent     Percentage of tests which failed
a316 2
B<NOTE> Currently this function is still noisy.  I'm working on it.

d319 8
a326 12
# Turns on autoflush for the handle passed
sub _autoflush {
    my $flushy_fh = shift;
    my $old_fh = select $flushy_fh;
    $| = 1;
    select $old_fh;
}

sub _run_all_tests {
    my @@tests = @@_;

    _autoflush(\*STDOUT);
d329 2
a330 1
    my(%failedtests);
d347 2
a348 1
    my @@dir_files = _globdir $Files_In_Dir if defined $Files_In_Dir;
d357 1
a357 1
        print $leader;
d363 1
a363 1
            print "# Running: ", $Strap->_command_line($tfile), "\n";
d366 1
a366 1
        my %results = $Strap->analyze_file($tfile) or
d372 1
a372 1
                $elapsed = sprintf( " %8.3fs", $elapsed );
d375 1
a375 1
                $elapsed = sprintf( " %8ss", $elapsed ? $elapsed : "<1" );
d383 6
a388 2
        my @@failed = grep { !$results{details}[$_-1]{ok} }
                     1..@@{$results{details}};
d390 18
a407 16
                    ok          => $results{ok},
                    'next'      => $Strap->{'next'},
                    max         => $results{max},
                    failed      => \@@failed,
                    bonus       => $results{bonus},
                    skipped     => $results{skip},
                    skip_reason => $results{skip_reason},
                    skip_all    => $Strap->{skip_all},
                    ml          => $ml,
                   );

        $tot{bonus}       += $results{bonus};
        $tot{max}         += $results{max};
        $tot{ok}          += $results{ok};
        $tot{todo}        += $results{todo};
        $tot{sub_skipped} += $results{skip};
d409 2
a410 1
        my($estatus, $wstatus) = @@results{qw(exit wait)};
d412 1
a412 1
        if ($results{passing}) {
d418 15
a432 3
                push(@@msg, "$test{bonus}/$test{max} unexpectedly succeeded")
                    if $test{bonus};
                print "$test{ml}ok$elapsed\n        ".join(', ', @@msg)."\n";
d435 1
a435 1
                print "$test{ml}ok$elapsed\n";
d438 1
a438 1
                print "skipped\n        all skipped: $test{skip_all}\n";
d442 1
a442 1
                print "skipped\n        all skipped: no reason given\n";
d454 1
a454 1
                my $details = $results{details};
d466 1
a466 1
            elsif($results{seen}) {
d468 1
a468 1
                    my ($txt, $canon) = _canonfailed($test{max},$test{skipped},
d470 1
a470 1
                    print "$test{ml}$txt";
a474 1
                                             percent => 100*(scalar @@{$test{failed}})/$test{max},
d480 1
a480 1
                    print "Don't know which tests failed: got $test{ok} ok, ".
a485 1
                                             percent => undef,
d493 1
a493 1
                print "FAILED before any test output arrived\n";
a498 1
                                         percent     => undef,
d512 1
a512 1
                print "LEAKED FILES: @@f\n";
d521 9
a529 1
    return(\%tot, \%failedtests);
d532 1
a532 1
=item B<_mk_leader>
d534 1
a534 1
  my($leader, $ml) = _mk_leader($test_file, $width);
d563 1
a563 1
=item B<_leader_width>
d586 4
d591 1
a591 2
sub _show_results {
    my($tot, $failedtests) = @@_;
a592 1
    my $pct;
d596 10
a605 1
        print "All tests successful$bonusmsg.\n";
d616 2
a617 5
        $pct = sprintf("%.2f", $tot->{good} / $tot->{tests} * 100);
        my $percent_ok = 100*$tot->{ok}/$tot->{max};
        my $subpct = sprintf " %d/%d subtests failed, %.2f%% okay.",
                              $tot->{max} - $tot->{ok}, $tot->{max}, 
                              $percent_ok;
d619 1
a619 1
        my($fmt_top, $fmt) = _create_fmts($failedtests);
d622 1
d624 3
a626 2
          $Curtest = $failedtests->{$script};
          write;
d630 2
a631 3
            print "$bonusmsg.\n" if $bonusmsg;
            die "Failed $tot->{bad}/$tot->{tests} test scripts, $pct% okay.".
                "$subpct\n";
d635 1
a635 1
    printf("Files=%d, Tests=%d, %s\n",
d637 10
d651 2
a652 2
    header => \&header_handler,
    test => \&test_handler,
d656 1
a656 1
$Strap->{callback} = \&strap_callback;
d674 1
a674 2
      if $totals->{seen} && 
         $totals->{max}  < $totals->{seen};
d680 1
a680 1
    my $curr = $totals->{seen};
d682 2
a683 2
    my $max  = $totals->{max};
    my $detail = $totals->{details}[-1];
d689 4
a692 4
            $totals->{skip_reason} = $detail->{reason}
              unless defined $totals->{skip_reason};
            $totals->{skip_reason} = 'various reasons'
              if $totals->{skip_reason} ne $detail->{reason};
d696 1
a696 1
        _print_ml("NOK $curr");
a752 1

d759 3
a761 1
    my ($failed, $canon, $percent) = ('??', '??');
a772 1
            $percent = 0;
d778 1
a778 2
            (my $txt, $canon) = _canonfailed($test->{max},$test->{skipped},@@{$test->{failed}});
            $percent = 100*(scalar @@{$test->{failed}})/$test->{max};
a784 1
             percent => $percent,
d791 2
a792 1
    my($failedtests) = @@_;
d794 5
a798 3
    my $failed_str = "Failed Test";
    my $middle_str = " Stat Wstat Total Fail  Failed  ";
    my $list_str = "List of Failed";
d817 1
a817 2
    my $fmt_top = "format STDOUT_TOP =\n"
                  . sprintf("%-${max_namelen}s", $failed_str)
d821 1
a821 1
                  . "\n.\n";
d823 5
a827 13
    my $fmt = "format STDOUT =\n"
              . "@@" . "<" x ($max_namelen - 1)
              . "  @@>> @@>>>> @@>>>> @@>>> ^##.##%  "
              . "^" . "<" x ($list_len - 1) . "\n"
              . '{ $Curtest->{name}, $Curtest->{estat},'
              . '  $Curtest->{wstat}, $Curtest->{max},'
              . '  $Curtest->{failed}, $Curtest->{percent},'
              . '  $Curtest->{canon}'
              . "\n}\n"
              . "~~" . " " x ($Columns - $list_len - 2) . "^"
              . "<" x ($list_len - 1) . "\n"
              . '$Curtest->{canon}'
              . "\n.\n";
d829 1
a829 6
    eval $fmt_top;
    die $@@ if $@@;
    eval $fmt;
    die $@@ if $@@;

    return($fmt_top, $fmt);
d832 5
a836 2
sub _canonfailed ($$@@) {
    my($max,$skipped,@@failed) = @@_;
d838 2
a839 2
    @@failed = sort {$a <=> $b} grep !$seen{$_}++, @@failed;
    my $failed = @@failed;
d843 1
a843 1
    my $last = $min = shift @@failed;
d845 3
a847 2
    if (@@failed) {
        for (@@failed, $failed[-1]) { # don't forget the last one
d855 1
a855 1
        push @@result, "FAILED tests @@canon\n";
d859 1
a859 1
        push @@result, "FAILED test $last\n";
d863 3
a865 1
    push @@result, "\tFailed $failed/$max tests, ";
d867 1
a867 1
	push @@result, sprintf("%.2f",100*(1-$failed/$max)), "% okay";
d874 1
a874 1
        my $good = $max - $failed - $skipped;
d887 1
a887 1
    ($txt, $canon);
a889 7
=end _private

=back

=cut


d898 2
a899 1
C<$verbose>, C<$switches> and C<$debug> are exported upon request.
a987 4
=item C<HARNESS_IGNORE_EXITCODE>

Makes harness ignore the exit status of child processes when defined.

d1010 6
d1022 15
a1086 2
Implement Straps callbacks.  (experimentally implemented)

a1090 2
HARNESS_TODOFAIL to display TODO failures

d1105 39
a1143 2
HARNESS_COMPILE_TEST currently assumes it's run from the Perl source
directory.
d1145 2
a1146 3
Please use the CPAN bug ticketing system at L<http://rt.cpan.org/>.
You can also mail bugs, fixes and enhancements to 
C<< <bug-test-harness >> at C<< rt.cpan.org> >>.
d1151 1
a1151 1
sure is, that it was inspired by Larry Wall's TEST script that came
d1160 1
a1160 1
Copyright 2002-2005
@


1.9
log
@merge 5.8.5 into HEAD
remove now-unused files
crank libperl shared library major number
update Makefile.bsd-wrapper
tweak openbsd hints file for arm and m68k
@
text
@a1 1
# $Id: Harness.pm,v 1.8 2004/04/07 21:33:06 millert Exp $
d5 1
a5 1
require 5.004;
d13 1
a18 1
    $Have_Devel_Corestack
d21 1
d24 1
d27 5
d38 1
a38 3
Version 2.42

    $Header: /cvs/src/gnu/usr.bin/perl/lib/Test/Harness.pm,v 1.8 2004/04/07 21:33:06 millert Exp $
d42 1
a42 1
$VERSION = '2.42';
a48 2
$Have_Devel_Corestack = 0;

d50 1
d55 1
d63 1
a63 1
my $Ok_Slow = $ENV{HARNESS_OK_SLOW};
d65 1
a65 1
$Strap = Test::Harness::Straps->new;
d76 1
d86 13
a98 96
B<STOP!> If all you want to do is write a test script, consider using
Test::Simple.  Otherwise, read on.

(By using the Test module, you can write test scripts without
knowing the exact output this module expects.  However, if you need to
know the specifics, read on!)

Perl test scripts print to standard output C<"ok N"> for each single
test, where C<N> is an increasing sequence of integers. The first line
output by a standard test script is C<"1..M"> with C<M> being the
number of tests that should be run within the test
script. Test::Harness::runtests(@@tests) runs all the testscripts
named as arguments and checks standard output for the expected
C<"ok N"> strings.

After all tests have been performed, runtests() prints some
performance statistics that are computed by the Benchmark module.

=head2 The test script output

The following explains how Test::Harness interprets the output of your
test program.

=over 4

=item B<'1..M'>

This header tells how many tests there will be.  For example, C<1..10>
means you plan on running 10 tests.  This is a safeguard in case your
test dies quietly in the middle of its run.

It should be the first non-comment line output by your test program.

In certain instances, you may not know how many tests you will
ultimately be running.  In this case, it is permitted for the 1..M
header to appear as the B<last> line output by your test (again, it
can be followed by further comments).

Under B<no> circumstances should 1..M appear in the middle of your
output or more than once.


=item B<'ok', 'not ok'.  Ok?>

Any output from the testscript to standard error is ignored and
bypassed, thus will be seen by the user. Lines written to standard
output containing C</^(not\s+)?ok\b/> are interpreted as feedback for
runtests().  All other lines are discarded.

C</^not ok/> indicates a failed test.  C</^ok/> is a successful test.


=item B<test numbers>

Perl normally expects the 'ok' or 'not ok' to be followed by a test
number.  It is tolerated if the test numbers after 'ok' are
omitted. In this case Test::Harness maintains temporarily its own
counter until the script supplies test numbers again. So the following
test script

    print <<END;
    1..6
    not ok
    ok
    not ok
    ok
    ok
    END

will generate

    FAILED tests 1, 3, 6
    Failed 3/6 tests, 50.00% okay

=item B<test names>

Anything after the test number but before the # is considered to be
the name of the test.

  ok 42 this is the name of the test

Currently, Test::Harness does nothing with this information.

=item B<Skipping tests>

If the standard output line contains the substring C< # Skip> (with
variations in spacing and case) after C<ok> or C<ok NUMBER>, it is
counted as a skipped test.  If the whole testscript succeeds, the
count of skipped tests is included in the generated output.
C<Test::Harness> reports the text after C< # Skip\S*\s+> as a reason
for skipping.

  ok 23 # skip Insufficient flogiston pressure.

Similarly, one can include a similar explanation in a C<1..0> line
emitted if the test script is skipped completely:
d100 1
a100 62
  1..0 # Skipped: no leverage found

=item B<Todo tests>

If the standard output line contains the substring C< # TODO > after
C<not ok> or C<not ok NUMBER>, it is counted as a todo test.  The text
afterwards is the thing that has to be done before this test will
succeed.

  not ok 13 # TODO harness the power of the atom

Note that the TODO must have a space after it. 

=begin _deprecated

Alternatively, you can specify a list of what tests are todo as part
of the test header.

  1..23 todo 5 12 23

This only works if the header appears at the beginning of the test.

This style is B<deprecated>.

=end _deprecated

These tests represent a feature to be implemented or a bug to be fixed
and act as something of an executable "thing to do" list.  They are
B<not> expected to succeed.  Should a todo test begin succeeding,
Test::Harness will report it as a bonus.  This indicates that whatever
you were supposed to do has been done and you should promote this to a
normal test.

=item B<Bail out!>

As an emergency measure, a test script can decide that further tests
are useless (e.g. missing dependencies) and testing should stop
immediately. In that case the test script prints the magic words

  Bail out!

to standard output. Any message after these words will be displayed by
C<Test::Harness> as the reason why testing is stopped.

=item B<Comments>

Additional comments may be put into the testing output on their own
lines.  Comment lines should begin with a '#', Test::Harness will
ignore them.

  ok 1
  # Life is good, the sun is shining, RAM is cheap.
  not ok 2
  # got 'Bush' expected 'Gore'

=item B<Anything else>

Any other output Test::Harness sees it will silently ignore B<BUT WE
PLAN TO CHANGE THIS!> If you wish to place additional output in your
test script, please use a comment.

=back
d118 1
a118 1
=item B<$Test::Harness::Verbose>
d120 1
a120 1
The global variable C<$Test::Harness::Verbose> is exportable and can be
d125 1
a125 1
=item B<$Test::Harness::switches>
d127 1
a127 1
The global variable C<$Test::Harness::switches> is exportable and can be
d131 5
d141 1
a141 2
It will happen: your tests will fail.  After you mop up your ego, you
can begin examining the summary report:
d156 1
a156 1
Everything passed but t/waterloo.t.  It failed 10 of 20 tests and
d206 1
a206 1
This runs all the given @@test_files and divines whether they passed
d211 1
a211 1
It returns true if everything was ok.  Otherwise it will die() with
a213 4
=for _private

This is just _run_all_tests() plus _show_results()

d253 1
a253 1
compatibility on systems where glob() doesn't work right.
d306 8
a313 1
#'#
d315 5
a319 2
    my(@@tests) = @@_;
    local($|) = 1;
d338 1
a338 1
    my $t_start = new Benchmark;
a341 4
	if ( $Test::Harness::Debug ) {
	    print "# Running: ", $Strap->_command_line($tfile), "\n";
	}

d351 4
d357 13
d395 1
d402 6
a407 4
                print "$test{ml}ok\n        ".join(', ', @@msg)."\n";
            } elsif ($test{max}) {
                print "$test{ml}ok\n";
            } elsif (defined $test{skip_all} and length $test{skip_all}) {
d410 2
a411 1
            } else {
d425 1
a425 2
                foreach my $overrun ($test{max}+1..@@$details)
                {
d449 2
a450 1
                } else {
d463 2
a464 1
            } else {
d489 2
a490 2
    }
    $tot{bench} = timediff(new Benchmark, $t_start);
d501 1
a501 1
Generates the 't/foo........' $leader for the given C<$test_file> as well
d515 3
a517 2
    if ($^O eq 'VMS') { $te =~ s/^.*\.t\./\[.t./s; }
    my $blank = (' ' x 77);
d521 3
a523 2
    $ml = "\r$blank\r$leader"
      if -t STDOUT and not $ENV{HARNESS_NOTTY} and not $Verbose;
d560 2
a561 1
    } elsif (!$tot->{tests}){
d563 2
a564 1
    } elsif (!$tot->{max}) {
d568 2
a569 1
    } else {
d596 8
a603 2
my %Handlers = ();
$Strap->{callback} = sub {
d612 1
a612 1
$Handlers{header} = sub {
d624 1
a624 1
$Handlers{test} = sub {
d656 1
a656 1
$Handlers{bailout} = sub {
d669 1
a669 2
# For slow connections, we save lots of bandwidth by printing only once
# per second.
d671 2
a672 1
    if( !$Ok_Slow || $Last_ML_Print != time ) {
d674 1
a674 1
        $Last_ML_Print = time;
a713 8
    if (_corestatus($wstatus)) { # until we have a wait module
        if ($Have_Devel_Corestack) {
            Devel::CoreStack::stack($^X);
        } else {
            print "\ttest program seems to have generated a core\n";
        }
    }

a791 23
{
    my $tried_devel_corestack;

    sub _corestatus {
        my($st) = @@_;

        my $did_core;
        eval { # we may not have a WCOREDUMP
            local $^W = 0;  # *.ph files are often *very* noisy
            require 'wait.ph';
            $did_core = WCOREDUMP($st);
        };
        if( $@@ ) {
            $did_core = $st & 0200;
        }

        eval { require Devel::CoreStack; $Have_Devel_Corestack++ } 
          unless $tried_devel_corestack++;

        return $did_core;
    }
}

d805 1
a805 5
                if ($min == $last) {
                    push @@canon, $last;
                } else {
                    push @@canon, "$min-$last";
                }
d813 2
a814 1
    } else {
d822 2
a823 1
    } else {
a826 1
    my $good = $max - $failed - $skipped;
d828 1
d833 2
a834 1
	} else {
d894 3
a896 1
=head1 ENVIRONMENT
d902 12
a913 3
Harness sets this before executing the individual tests.  This allows
the tests to determine if they are being executed through the harness
or by any other means.
a959 6
=item C<HARNESS_OK_SLOW>

If true, the C<ok> messages are printed out only every second.  This
reduces output and may help increase testing speed over slow
connections, or with very large numbers of tests.

d1001 1
a1001 2
the underlying timing routines, L<Devel::CoreStack> to generate core
dumps from failed tests and L<Devel::Cover> for test coverage
a1003 17
=head1 AUTHORS

Either Tim Bunce or Andreas Koenig, we don't know. What we know for
sure is, that it was inspired by Larry Wall's TEST script that came
with perl distributions for ages. Numerous anonymous contributors
exist.  Andreas Koenig held the torch for many years, and then
Michael G Schwern.

Current maintainer is Andy Lester C<< <andy@@petdance.com> >>.

=head1 LICENSE

This program is free software; you can redistribute it and/or 
modify it under the same terms as Perl itself.

See L<http://www.perl.com/perl/misc/Artistic.html>

a1019 2
Deal with VMS's "not \nok 4\n" mistake.

a1045 2
=for _private

a1047 2
=for _private

d1057 1
a1057 1
C<< <bug-test-harness@@rt.cpan.org> >>.
d1061 7
a1067 1
Original code by Michael G Schwern, maintained by Andy Lester.
d1071 3
a1073 2
Copyright 2003 by Michael G Schwern C<< <schwern@@pobox.com> >>,
                  Andy Lester C<< <andy@@petdance.com> >>.
@


1.8
log
@merge local changes into perl-5.8.3
@
text
@d2 1
a2 1
# $Id: Harness.pm,v 1.7 2003/12/03 03:02:41 millert Exp $
d32 1
a32 1
Version 2.40
d34 1
a34 1
    $Header: /home/cvs/test-harness/lib/Test/Harness.pm,v 1.80 2003/12/31 02:39:21 andy Exp $
d38 1
a38 1
$VERSION = '2.40';
d1126 1
@


1.7
log
@Resolve conflicts for perl 5.8.2, remove old files, and add OpenBSD-specific scaffolding
@
text
@d2 1
a2 1
# $Id: Harness.pm,v 1.54 2003/08/15 01:05:00 andy Exp $
d14 25
a38 4
use vars qw($VERSION $Verbose $Switches $Have_Devel_Corestack $Curtest
            $Columns $verbose $switches $ML $Strap
            @@ISA @@EXPORT @@EXPORT_OK $Last_ML_Print
           );
d43 1
a46 2
$VERSION = '2.30';

d68 1
a72 5

=head1 NAME

Test::Harness - run perl standard test scripts with statistics

d182 1
a182 1
If the standard output line contains the substring C< # TODO> after
d189 2
a240 1

d243 2
a244 2
Test::Harness will honor the C<-T> in the #! line on your test files.  So
if you begin a test with:
a249 1

d257 1
a257 1
=item B<$Test::Harness::verbose>
d259 4
a262 3
The global variable $Test::Harness::verbose is exportable and can be
used to let runtests() display the standard output of the script
without altering the behavior otherwise.
d266 1
a266 1
The global variable $Test::Harness::switches is exportable and can be
d268 1
a268 1
script(s). The default value is C<-w>.
d275 1
a275 1
It will happen, your tests will fail.  After you mop up your ego, you
d308 1
a308 1
The wait status of the test I<umm, I need a better explanation here>.
d408 3
a410 3
Runs all the given @@test_files (as runtests()) but does it quietly (no
report).  $total is a hash ref summary of all the tests run.  Its keys
and values are this:
d424 2
a425 2
If $total->{bad} == 0 and $total->{max} > 0, you've got a successful
test.
d439 1
a439 1
Needless to say, $failed should be empty if everything passed.
d471 4
d478 1
d485 1
a485 1
          do { warn "$Strap->{error}\n";  next };
d551 1
a551 1
                    my ($txt, $canon) = canonfailed($test{max},$test{skipped},
d612 1
a612 1
Generates the 't/foo........' $leader for the given $test_file as well
d614 1
a614 1
\r and such).  $ml may be empty if Test::Harness doesn't think you're
d617 1
a617 1
The $width is the width of the "yada/blah.." string.
d814 1
a814 1
    if (corestatus($wstatus)) { # until we have a wait module
d833 1
a833 1
            (my $txt, $canon) = canonfailed($test->{max},$test->{skipped},@@{$test->{failed}});
d903 1
a903 1
    sub corestatus {
d923 1
a923 1
sub canonfailed ($$@@) {
d991 1
a991 2
C<$verbose> and C<$switches> are exported upon request.

d1051 7
d1083 11
a1093 3
If true, the C<ok> messages are printed out only every second.
This reduces output and therefore may for example help testing
over slow connections.
d1104 2
a1105 1
its tests.  Setting $Test::Harness::verbose will override this.
d1204 18
@


1.6
log
@Resolve conflicts, remove old files, merge local changes
@
text
@d2 1
a2 1
# $Id: Harness.pm,v 1.38 2002/06/19 21:01:01 schwern Exp $
d16 1
a16 1
            @@ISA @@EXPORT @@EXPORT_OK
d20 2
a21 2
*verbose  = \$Verbose;
*switches = \$Switches;
d25 1
a25 1
$VERSION = '2.26';
d39 2
d333 1
d454 1
a454 1

d475 1
a475 1
                    skip_reason => $Strap->{_skip_reason},
d488 1
a488 6
        if ($wstatus) {
            $failedtests{$tfile} = _dubious_return(\%test, \%tot, 
                                                  $estatus, $wstatus);
            $failedtests{$tfile}{name} = $tfile;
        }
        elsif ($results{passing}) {
d508 11
a518 3
            if ($test{max}) {
                if ($test{'next'} <= $test{max}) {
                    push @@{$test{failed}}, $test{'next'}..$test{max};
d520 9
a528 1
                if (@@{$test{failed}}) {
d553 1
a553 1
            } elsif ($test{'next'} == 0) {
d711 1
a711 1
        _print_ml("ok $curr/$max");
d714 4
a717 4
            $self->{_skip_reason} = $detail->{reason}
              unless defined $self->{_skip_reason};
            $self->{_skip_reason} = 'various reasons'
              if $self->{_skip_reason} ne $detail->{reason};
d747 9
d884 2
a885 1
        eval {
d887 2
a888 1
            require 'wait.ph'
d890 3
a892 2
        return if $@@;
        my $did_core = defined &WCOREDUMP ? WCOREDUMP($st) : $st & 0200;
d901 1
a901 1
sub canonfailed ($@@) {
d932 5
a936 1
    push @@result, sprintf("%.2f",100*(1-$failed/$max)), "% okay";
d939 10
a948 4
    my $goodper = sprintf("%.2f",100*($good/$max));
    push @@result, " (less $skipped skipped test$ender: $good okay, ".
                  "$goodper%)"
         if $skipped;
d967 1
a967 1
C<&runtests> is exported by Test::Harness per default.
d1053 6
d1099 2
a1100 1
exist.  Andreas Koenig held the torch for many years.
d1102 8
a1109 1
Current maintainer is Michael G Schwern E<lt>schwern@@pobox.comE<gt>
d1118 2
d1131 24
d1156 1
d1160 1
@


1.5
log
@merge in perl 5.6.1 with our local changes
@
text
@d1 3
d6 3
a8 1
use 5.005_64;
a11 1
use FileHandle;
d14 10
a23 3
our($VERSION, $verbose, $switches, $have_devel_corestack, $curtest,
    $columns, @@ISA, @@EXPORT, @@EXPORT_OK);
$have_devel_corestack = 0;
d25 1
a25 1
$VERSION = "1.1604";
d29 5
d35 1
a35 42
my $ignore_exitcode = $ENV{HARNESS_IGNORE_EXITCODE};

my $files_in_dir = $ENV{HARNESS_FILELEAK_IN_DIR};

my $tests_skipped = 0;
my $subtests_skipped = 0;

@@ISA=('Exporter');
@@EXPORT= qw(&runtests);
@@EXPORT_OK= qw($verbose $switches);

$verbose = 0;
$switches = "-w";
$columns = $ENV{HARNESS_COLUMNS} || $ENV{COLUMNS} || 80;

sub globdir { opendir DIRH, shift; my @@f = readdir DIRH; closedir DIRH; @@f }

sub runtests {
    my(@@tests) = @@_;
    local($|) = 1;
    my($test,$te,$ok,$next,$max,$pct,$totbonus,@@failed,%failedtests);
    my $totmax = 0;
    my $totok = 0;
    my $files = 0;
    my $bad = 0;
    my $good = 0;
    my $total = @@tests;

    # pass -I flags to children
    my $old5lib = $ENV{PERL5LIB};

    # VMS has a 255-byte limit on the length of %ENV entries, so
    # toss the ones that involve perl_root, the install location
    # for VMS
    my $new5lib;
    if ($^O eq 'VMS') {
	$new5lib = join($Config{path_sep}, grep {!/perl_root/i;} @@INC);
	$switches =~ s/-(\S*[A-Z]\S*)/"-$1"/g;
    }
    else {
        $new5lib = join($Config{path_sep}, @@INC);
    }
d37 1
a37 1
    local($ENV{'PERL5LIB'}) = $new5lib;
d39 1
a39 291
    my @@dir_files = globdir $files_in_dir if defined $files_in_dir;
    my $t_start = new Benchmark;
    while ($test = shift(@@tests)) {
	$te = $test;
	chop($te);
	if ($^O eq 'VMS') { $te =~ s/^.*\.t\./[.t./s; }
	my $blank = (' ' x 77);
	my $leader = "$te" . '.' x (20 - length($te));
	my $ml = "";
	$ml = "\r$blank\r$leader"
	    if -t STDOUT and not $ENV{HARNESS_NOTTY} and not $verbose;
	print $leader;
	my $fh = new FileHandle;
	$fh->open($test) or print "can't open $test. $!\n";
	my $first = <$fh>;
	my $s = $switches;
	$s .= " $ENV{'HARNESS_PERL_SWITCHES'}"
	    if exists $ENV{'HARNESS_PERL_SWITCHES'};
	$s .= join " ", q[ "-T"], map {qq["-I$_"]} @@INC
	    if $first =~ /^#!.*\bperl.*-\w*T/;
	$fh->close or print "can't close $test. $!\n";
	my $cmd = ($ENV{'HARNESS_COMPILE_TEST'})
		? "./perl -I../lib ../utils/perlcc $test "
		  . "-r 2>> ./compilelog |" 
		: "$^X $s $test|";
	$cmd = "MCR $cmd" if $^O eq 'VMS';
	$fh->open($cmd) or print "can't run $test. $!\n";
	$ok = $next = $max = 0;
	@@failed = ();
	my %todo = ();
        my $bonus = 0;
	my $skipped = 0;
	my $skip_reason;
	while (<$fh>) {
	    if( $verbose ){
		print $_;
	    }
	    if (/^1\.\.([0-9]+) todo([\d\s]+)\;/) {
		$max = $1;
		for (split(/\s+/, $2)) { $todo{$_} = 1; }
		$totmax += $max;
		$files++;
		$next = 1;
	    } elsif (/^1\.\.([0-9]+)(\s*\#\s*[Ss]kip\S*(?>\s+)(.+))?/) {
		$max = $1;
		$totmax += $max;
		$files++;
		$next = 1;
		$skip_reason = $3 if not $max and defined $3;
	    } elsif ($max && /^(not\s+)?ok\b/) {
		my $this = $next;
		if (/^not ok\s*(\d*)/){
		    $this = $1 if $1 > 0;
		    print "${ml}NOK $this" if $ml;
		    if (!$todo{$this}) {
			push @@failed, $this;
		    } else {
			$ok++;
			$totok++;
		    }
		} elsif (/^ok\s*(\d*)(\s*\#\s*[Ss]kip\S*(?:(?>\s+)(.+))?)?/) {
		    $this = $1 if $1 > 0;
		    print "${ml}ok $this/$max" if $ml;
		    $ok++;
		    $totok++;
		    $skipped++ if defined $2;
		    my $reason;
		    $reason = 'unknown reason' if defined $2;
		    $reason = $3 if defined $3;
		    if (defined $reason and defined $skip_reason) {
		      # print "was: '$skip_reason' new '$reason'\n";
		      $skip_reason = 'various reasons'
			if $skip_reason ne $reason;
		    } elsif (defined $reason) {
		      $skip_reason = $reason;
		    }
		    $bonus++, $totbonus++ if $todo{$this};
		}
		if ($this > $next) {
		    # print "Test output counter mismatch [test $this]\n";
		    # no need to warn probably
		    push @@failed, $next..$this-1;
		} elsif ($this < $next) {
		    #we have seen more "ok" lines than the number suggests
		    print "Confused test output: test $this answered after test ", $next-1, "\n";
		    $next = $this;
		}
		$next = $this + 1;
	    }
	}
	$fh->close; # must close to reap child resource values
	my $wstatus = $ignore_exitcode ? 0 : $?;	# Can trust $? ?
	my $estatus;
	$estatus = ($^O eq 'VMS'
		       ? eval 'use vmsish "status"; $estatus = $?'
		       : $wstatus >> 8);
	if ($wstatus) {
	    my ($failed, $canon, $percent) = ('??', '??');
	    printf "${ml}dubious\n\tTest returned status $estatus (wstat %d, 0x%x)\n",
		    $wstatus,$wstatus;
	    print "\t\t(VMS status is $estatus)\n" if $^O eq 'VMS';
	    if (corestatus($wstatus)) { # until we have a wait module
		if ($have_devel_corestack) {
		    Devel::CoreStack::stack($^X);
		} else {
		    print "\ttest program seems to have generated a core\n";
		}
	    }
	    $bad++;
	    if ($max) {
	      if ($next == $max + 1 and not @@failed) {
		print "\tafter all the subtests completed successfully\n";
		$percent = 0;
		$failed = 0;	# But we do not set $canon!
	      } else {
		push @@failed, $next..$max;
		$failed = @@failed;
		(my $txt, $canon) = canonfailed($max,$skipped,@@failed);
		$percent = 100*(scalar @@failed)/$max;
		print "DIED. ",$txt;
	      }
	    }
	    $failedtests{$test} = { canon => $canon,  max => $max || '??',
				    failed => $failed, 
				    name => $test, percent => $percent,
				    estat => $estatus, wstat => $wstatus,
				  };
	} elsif ($ok == $max && $next == $max+1) {
	    if ($max and $skipped + $bonus) {
		my @@msg;
		push(@@msg, "$skipped/$max skipped: $skip_reason")
		    if $skipped;
		push(@@msg, "$bonus/$max unexpectedly succeeded")
		    if $bonus;
		print "${ml}ok, ".join(', ', @@msg)."\n";
	    } elsif ($max) {
		print "${ml}ok\n";
	    } elsif (defined $skip_reason) {
		print "skipped: $skip_reason\n";
		$tests_skipped++;
	    } else {
		print "skipped test on this platform\n";
		$tests_skipped++;
	    }
	    $good++;
	} elsif ($max) {
	    if ($next <= $max) {
		push @@failed, $next..$max;
	    }
	    if (@@failed) {
		my ($txt, $canon) = canonfailed($max,$skipped,@@failed);
		print "${ml}$txt";
		$failedtests{$test} = { canon => $canon,  max => $max,
					failed => scalar @@failed,
					name => $test, percent => 100*(scalar @@failed)/$max,
					estat => '', wstat => '',
				      };
	    } else {
		print "Don't know which tests failed: got $ok ok, expected $max\n";
		$failedtests{$test} = { canon => '??',  max => $max,
					failed => '??', 
					name => $test, percent => undef,
					estat => '', wstat => '',
				      };
	    }
	    $bad++;
	} elsif ($next == 0) {
	    print "FAILED before any test output arrived\n";
	    $bad++;
	    $failedtests{$test} = { canon => '??',  max => '??',
				    failed => '??',
				    name => $test, percent => undef,
				    estat => '', wstat => '',
				  };
	}
	$subtests_skipped += $skipped;
	if (defined $files_in_dir) {
	    my @@new_dir_files = globdir $files_in_dir;
	    if (@@new_dir_files != @@dir_files) {
		my %f;
		@@f{@@new_dir_files} = (1) x @@new_dir_files;
		delete @@f{@@dir_files};
		my @@f = sort keys %f;
		print "LEAKED FILES: @@f\n";
		@@dir_files = @@new_dir_files;
	    }
	}
    }
    my $t_total = timediff(new Benchmark, $t_start);
    
    if ($^O eq 'VMS') {
	if (defined $old5lib) {
	    $ENV{PERL5LIB} = $old5lib;
	} else {
	    delete $ENV{PERL5LIB};
	}
    }
    my $bonusmsg = '';
    $bonusmsg = (" ($totbonus subtest".($totbonus>1?'s':'').
	       " UNEXPECTEDLY SUCCEEDED)")
	if $totbonus;
    if ($tests_skipped) {
	$bonusmsg .= ", $tests_skipped test" . ($tests_skipped != 1 ? 's' : '');
	if ($subtests_skipped) {
	    $bonusmsg .= " and $subtests_skipped subtest"
			 . ($subtests_skipped != 1 ? 's' : '');
	}
	$bonusmsg .= ' skipped';
    }
    elsif ($subtests_skipped) {
	$bonusmsg .= ", $subtests_skipped subtest"
	             . ($subtests_skipped != 1 ? 's' : '')
		     . " skipped";
    }
    if ($bad == 0 && $totmax) {
	print "All tests successful$bonusmsg.\n";
    } elsif ($total==0){
	die "FAILED--no tests were run for some reason.\n";
    } elsif ($totmax==0) {
	my $blurb = $total==1 ? "script" : "scripts";
	die "FAILED--$total test $blurb could be run, alas--no output ever seen\n";
    } else {
	$pct = sprintf("%.2f", $good / $total * 100);
	my $subpct = sprintf " %d/%d subtests failed, %.2f%% okay.",
	$totmax - $totok, $totmax, 100*$totok/$totmax;
	# Create formats
	#    First, figure out max length of test names
	my $failed_str = "Failed Test";
	my $middle_str = " Status Wstat Total Fail  Failed  ";
	my $list_str = "List of Failed";
	my $max_namelen = length($failed_str);
	my $script;
	foreach $script (keys %failedtests) {
	    $max_namelen =
		(length $failedtests{$script}->{name} > $max_namelen) ?
		    length $failedtests{$script}->{name} : $max_namelen;
	}
	my $list_len = $columns - length($middle_str) - $max_namelen;
	if ($list_len < length($list_str)) {
	    $list_len = length($list_str);
	    $max_namelen = $columns - length($middle_str) - $list_len;
	    if ($max_namelen < length($failed_str)) {
		$max_namelen = length($failed_str);
		$columns = $max_namelen + length($middle_str) + $list_len;
	    }
	}

	my $fmt_top = "format STDOUT_TOP =\n"
		      . sprintf("%-${max_namelen}s", $failed_str)
		      . $middle_str
		      . $list_str . "\n"
		      . "-" x $columns
		      . "\n.\n";
	my $fmt = "format STDOUT =\n"
		  . "@@" . "<" x ($max_namelen - 1)
		  . "	 @@>> @@>>>> @@>>>> @@>>> ^##.##%  "
		  . "^" . "<" x ($list_len - 1) . "\n"
		  . '{ $curtest->{name}, $curtest->{estat},'
		  . '  $curtest->{wstat}, $curtest->{max},'
		  . '  $curtest->{failed}, $curtest->{percent},'
		  . '  $curtest->{canon}'
		  . "\n}\n"
		  . "~~" . " " x ($columns - $list_len - 2) . "^"
		  . "<" x ($list_len - 1) . "\n"
		  . '$curtest->{canon}'
		  . "\n.\n";

	eval $fmt_top;
	die $@@ if $@@;
	eval $fmt;
	die $@@ if $@@;

	# Now write to formats
	for $script (sort keys %failedtests) {
	  $curtest = $failedtests{$script};
	  write;
	}
	if ($bad) {
	    $bonusmsg =~ s/^,\s*//;
	    print "$bonusmsg.\n" if $bonusmsg;
	    die "Failed $bad/$total test scripts, $pct% okay.$subpct\n";
	}
    }
    printf("Files=%d, Tests=%d, %s\n", $files, $totmax, timestr($t_total, 'nop'));

    return ($bad == 0 && $totmax) ;
}

my $tried_devel_corestack;
sub corestatus {
    my($st) = @@_;
d41 3
a43 2
    eval {require 'wait.ph'};
    my $ret = defined &WCOREDUMP ? WCOREDUMP($st) : $st & 0200;
d45 4
a48 46
    eval { require Devel::CoreStack; $have_devel_corestack++ } 
      unless $tried_devel_corestack++;

    $ret;
}

sub canonfailed ($@@) {
    my($max,$skipped,@@failed) = @@_;
    my %seen;
    @@failed = sort {$a <=> $b} grep !$seen{$_}++, @@failed;
    my $failed = @@failed;
    my @@result = ();
    my @@canon = ();
    my $min;
    my $last = $min = shift @@failed;
    my $canon;
    if (@@failed) {
	for (@@failed, $failed[-1]) { # don't forget the last one
	    if ($_ > $last+1 || $_ == $last) {
		if ($min == $last) {
		    push @@canon, $last;
		} else {
		    push @@canon, "$min-$last";
		}
		$min = $_;
	    }
	    $last = $_;
	}
	local $" = ", ";
	push @@result, "FAILED tests @@canon\n";
	$canon = "@@canon";
    } else {
	push @@result, "FAILED test $last\n";
	$canon = $last;
    }

    push @@result, "\tFailed $failed/$max tests, ";
    push @@result, sprintf("%.2f",100*(1-$failed/$max)), "% okay";
    my $ender = 's' x ($skipped > 1);
    my $good = $max - $failed - $skipped;
    my $goodper = sprintf("%.2f",100*($good/$max));
    push @@result, " (-$skipped skipped test$ender: $good okay, $goodper%)" if $skipped;
    push @@result, "\n";
    my $txt = join "", @@result;
    ($txt, $canon);
}
a49 2
1;
__END__
d57 1
a57 1
use Test::Harness;
d59 1
a59 1
runtests(@@tests);
d63 4
a66 1
(By using the L<Test> module, you can write test scripts without
d83 24
d112 10
a121 3
It is tolerated if the test numbers after C<ok> are omitted. In this
case Test::Harness maintains temporarily its own counter until the
script supplies test numbers again. So the following test script
d132 1
a132 1
will generate 
d137 104
d245 2
d251 673
a923 4
If the standard output line contains substring C< # Skip> (with
variations in spacing and case) after C<ok> or C<ok NUMBER>, it is
counted as a skipped test.  If the whole testscript succeeds, the
count of skipped tests is included in the generated output.
d925 2
a926 3
C<Test::Harness> reports the text after C< # Skip(whatever)> as a
reason for skipping.  Similarly, one can include a similar explanation
in a C<1..0> line emitted if the test is skipped completely:
a927 1
  1..0 # Skipped: no leverage found
d933 3
d952 2
a953 2
Scripts that return a non-zero exit status, both C<$? E<gt>E<gt> 8> and C<$?> are
printed in a message similar to the above.
d962 5
d971 14
a984 2
Setting C<HARNESS_IGNORE_EXITCODE> makes harness ignore the exit status
of child processes.
d986 12
a997 12
Setting C<HARNESS_NOTTY> to a true value forces it to behave as though
STDOUT were not a console.  You may need to set this if you don't want
harness to output more frequent progress messages using carriage returns.
Some consoles may not handle carriage returns properly (which results
in a somewhat messy output).

Setting C<HARNESS_COMPILE_TEST> to a true value will make harness attempt
to compile the test using C<perlcc> before running it.

If C<HARNESS_FILELEAK_IN_DIR> is set to the name of a directory, harness
will check after each test whether new files appeared in that directory,
and report them as
d1003 3
a1005 1
C<HARNESS_FILELEAK_IN_DIR> may give more predicatable results.
d1007 37
a1043 14
The value of C<HARNESS_PERL_SWITCHES> will be prepended to the
switches used to invoke perl on each test.  For example, setting
C<HARNESS_PERL_SWITCHES> to "-W" will run all tests with all
warnings enabled.

If C<HARNESS_COLUMNS> is set, then this value will be used for the
width of the terminal. If it is not set then it will default to
C<COLUMNS>. If this is not set, it will default to 80. Note that users
of Bourne-sh based shells will need to C<export COLUMNS> for this
module to use that variable.

Harness sets C<HARNESS_ACTIVE> before executing the individual tests.
This allows the tests to determine if they are being executed through the
harness or by any other means.
d1047 4
a1050 2
L<Test> for writing test scripts and also L<Benchmark> for the
underlying timing routines.
d1057 27
a1083 1
exist. Current maintainer is Andreas Koenig.
d1087 2
a1088 5
Test::Harness uses $^X to determine the perl binary to run the tests
with. Test scripts running via the shebang (C<#!>) line may not be
portable because $^X is not consistent for shebang scripts across
platforms. This is no problem when Test::Harness is run with an
absolute path to the perl binary or when $^X can be found in the path.
@


1.4
log
@perl-5.6.0 + local changes
@
text
@d11 1
a11 1
    @@ISA, @@EXPORT, @@EXPORT_OK);
a29 20
format STDOUT_TOP =
Failed Test  Status Wstat Total Fail  Failed  List of failed
-------------------------------------------------------------------------------
.

format STDOUT =
@@<<<<<<<<<<<<<< @@>> @@>>>> @@>>>> @@>>> ^##.##%  ^<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
{ $curtest->{name},
                $curtest->{estat},
                    $curtest->{wstat},
                          $curtest->{max},
                                $curtest->{failed},
                                     $curtest->{percent},
                                              $curtest->{canon}
}
~~                                            ^<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
                                              $curtest->{canon}
.


d32 1
d39 1
a39 1
    my($test,$te,$ok,$next,$max,$pct,$totok,$totbonus,@@failed,%failedtests);
d41 1
d87 1
a87 1
		  . "-run 2>> ./compilelog |" 
d143 1
a143 1
		    # warn "Test output counter mismatch [test $this]\n";
d148 1
a148 1
		    warn "Confused test output: test $this answered after test ", $next-1, "\n";
d215 1
a215 1
		print $txt;
d289 6
d296 41
a354 1
    my($ret);
d357 1
a357 7
    if ($@@) {
      SWITCH: {
	    $ret = ($st & 0200); # Tim says, this is for 90%
	}
    } else {
	$ret = WCOREDUMP($st);
    }
d540 6
@


1.3
log
@perl5.005_03 (stock)
@
text
@d3 1
a3 1
BEGIN {require 5.002;}
d10 2
a11 2
use vars qw($VERSION $verbose $switches $have_devel_corestack $curtest
	    @@ISA @@EXPORT @@EXPORT_OK);
d14 3
a16 1
$VERSION = "1.1602";
a66 1
    local($ENV{'PERL5LIB'}) = join($Config{path_sep}, @@INC);
d68 13
a80 1
    if ($^O eq 'VMS') { $switches =~ s/-(\S*[A-Z]\S*)/"-$1"/g }
d87 7
a93 2
	if ($^O eq 'VMS') { $te =~ s/^.*\.t\./[.t./; }
	print "$te" . '.' x (20 - length($te));
d98 4
a101 1
	$s .= q[ "-T"] if $first =~ /^#!.*\bperl.*-\w*T/;
d103 4
a106 3
	my $cmd = ($ENV{'COMPILE_TEST'})? 
"./perl -I../lib ../utils/perlcc $test -run -verbose dcf -log ./compilelog |" 
															:  "$^X $s $test|";
d114 1
d125 1
a125 1
	    } elsif (/^1\.\.([0-9]+)/) {
d130 1
d135 1
d142 1
a142 1
		} elsif (/^ok\s*(\d*)(\s*\#\s*[Ss]kip)?/) {
d144 1
d148 10
d180 1
a180 1
	    printf "dubious\n\tTest returned status $estatus (wstat %d, 0x%x)\n",
d212 1
a212 1
		push(@@msg, "$skipped/$max subtest".($skipped>1?'s':'')." skipped")
d214 1
a214 2
		push(@@msg, "$bonus subtest".($bonus>1?'s':'').
		     " unexpectedly succeeded")
d216 1
a216 1
		print "ok, ".join(', ', @@msg)."\n";
d218 4
a221 1
		print "ok\n";
d223 1
a223 1
		print "skipping test on this platform\n";
d284 6
a289 2
	$bonusmsg .= ", $tests_skipped test" . ($tests_skipped != 1 ? 's' : '') .
			' skipped';
d291 4
a294 5
    if ($subtests_skipped) {
	$bonusmsg .= ($tests_skipped ? ', plus ' : ', '). 
			"$subtests_skipped subtest"
			. ($subtests_skipped != 1 ? 's' : '') .
			" skipped";
d318 1
a318 1
    printf("Files=%d,  Tests=%d, %s\n", $files, $totmax, timestr($t_total, 'nop'));
d452 6
d495 9
d513 9
@


1.2
log
@perl 5.004_04
@
text
@d14 9
a22 1
$VERSION = "1.1502";
d51 2
d56 1
a56 1
    my($test,$te,$ok,$next,$max,$pct,$totok,@@failed,%failedtests);
d69 1
d82 3
a84 1
	my $cmd = "$^X $s $test|";
d89 3
d96 7
a102 1
	    if (/^1\.\.([0-9]+)/) {
d111 7
a117 2
		    push @@failed, $this;
		} elsif (/^ok\s*(\d*)/) {
d121 2
d137 1
a137 1
	my $wstatus = $?;
d163 1
a163 1
		(my $txt, $canon) = canonfailed($max,@@failed);
d174 9
a182 1
	    if ($max) {
d186 1
d194 1
a194 1
		my ($txt, $canon) = canonfailed($max,@@failed);
d219 12
d241 14
d256 1
a256 1
	    print "All tests successful.\n";
d272 2
d303 1
a303 1
    my($max,@@failed) = @@_;
d333 6
a338 1
    push @@result, sprintf("%.2f",100*(1-$failed/$max)), "% okay\n";
d358 4
d406 5
d443 15
d460 2
a461 1
See L<Benchmark> for the underlying timing routines.
@


1.1
log
@Initial revision
@
text
@d3 1
d8 1
a8 2
use vars qw($VERSION $verbose $switches);
require 5.002;
d10 5
a14 1
$VERSION = "1.07";
d20 19
d46 1
a46 1
    my($test,$te,$ok,$next,$max,$pct);
d52 6
a57 1
    local($ENV{'PERL5LIB'}) = join($Config{path_sep}, @@INC); # pass -I flags to children
d63 1
d66 8
a73 1
	$fh->open("$^X $switches $test|") || (print "can't run. $!\n");
d80 23
a102 26
	    unless (/^\s*\#/) {
		if (/^1\.\.([0-9]+)/) {
		    $max = $1;
		    $totmax += $max;
		    $files++;
		    $next = 1;
		} elsif ($max && /^(not\s+)?ok\b/) {
		    my $this = $next;
		    if (/^not ok\s*(\d*)/){
			$this = $1 if $1 > 0;
			push @@failed, $this;
		    } elsif (/^ok\s*(\d*)/) {
			$this = $1 if $1 > 0;
			$ok++;
			$totok++;
		    }
		    if ($this > $next) {
			# warn "Test output counter mismatch [test $this]\n";
			# no need to warn probably
			push @@failed, $next..$this-1;
		    } elsif ($this < $next) {
			#we have seen more "ok" lines than the number suggests
			warn "Aborting test: output counter mismatch [test $this answered when test $next expected]\n";
			last;
		    }
		    $next = $this + 1;
d104 1
d109 41
a149 3
	my $estatus = $wstatus >> 8;
	if ($ok == $max && $next == $max+1 && ! $estatus) {
	    print "ok\n";
d156 7
a162 1
		print canonfailed($max,@@failed);
d164 6
a169 1
		print "Don't know which tests failed for some reason\n";
d175 5
a179 3
	}
	if ($wstatus) {
	    print "\tTest returned status $estatus (wstat $wstatus)\n";
d184 7
d197 1
a197 1
	die "FAILED--$total test $blurb could be run, alas -- no output ever seen\n";
d202 6
a207 3
	if ($bad == 1) {
	    die "Failed 1 test script, $pct% okay.$subpct\n";
	} else {
d212 22
d245 1
d260 1
d263 1
d268 2
a269 1
    join "", @@result;
d289 1
a289 1
output by a standard test scxript is C<"1..M"> with C<M> being the
d291 1
a291 1
script. Test::Harness::runscripts(@@tests) runs all the testscripts
d295 1
a295 1
After all tests have been performed, runscripts() prints some
d302 2
a303 3
output that look like perl comments (start with C</^\s*\#/>) are
discarded. Lines containing C</^(not\s+)?ok\b/> are interpreted as
feedback for runtests().
d324 1
a324 1
used to let runscripts() display the standard output of the script
d327 4
d333 1
a333 1
C<&runscripts> is exported by Test::Harness per default.
d351 1
a351 1
Scripts that return a non-zero exit status, both $?>>8 and $? are
d371 2
a372 2
with perl distributions for ages. Current maintainer is Andreas
Koenig.
@


1.1.1.1
log
@Import of Perl 5.003 into the tree.  Makefile.bsd-wrapper and
config.sh.OpenBSD are the only local changes.
@
text
@@


1.1.1.2
log
@perl5.005_03
@
text
@a2 1
BEGIN {require 5.002;}
d7 2
a8 1
use strict;
d10 1
a10 13
use vars qw($VERSION $verbose $switches $have_devel_corestack $curtest
	    @@ISA @@EXPORT @@EXPORT_OK);
$have_devel_corestack = 0;

$VERSION = "1.1602";

# Some experimental versions of OS/2 build have broken $?
my $ignore_exitcode = $ENV{HARNESS_IGNORE_EXITCODE};

my $files_in_dir = $ENV{HARNESS_FILELEAK_IN_DIR};

my $tests_skipped = 0;
my $subtests_skipped = 0;
a15 19
format STDOUT_TOP =
Failed Test  Status Wstat Total Fail  Failed  List of failed
-------------------------------------------------------------------------------
.

format STDOUT =
@@<<<<<<<<<<<<<< @@>> @@>>>> @@>>>> @@>>> ^##.##%  ^<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
{ $curtest->{name},
                $curtest->{estat},
                    $curtest->{wstat},
                          $curtest->{max},
                                $curtest->{failed},
                                     $curtest->{percent},
                                              $curtest->{canon}
}
~~                                            ^<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
                                              $curtest->{canon}
.

a19 2
sub globdir { opendir DIRH, shift; my @@f = readdir DIRH; closedir DIRH; @@f }

d23 1
a23 1
    my($test,$te,$ok,$next,$max,$pct,$totok,$totbonus,@@failed,%failedtests);
d29 1
a30 7
    # pass -I flags to children
    my $old5lib = $ENV{PERL5LIB};
    local($ENV{'PERL5LIB'}) = join($Config{path_sep}, @@INC);

    if ($^O eq 'VMS') { $switches =~ s/-(\S*[A-Z]\S*)/"-$1"/g }

    my @@dir_files = globdir $files_in_dir if defined $files_in_dir;
a34 1
	if ($^O eq 'VMS') { $te =~ s/^.*\.t\./[.t./; }
d37 1
a37 10
	$fh->open($test) or print "can't open $test. $!\n";
	my $first = <$fh>;
	my $s = $switches;
	$s .= q[ "-T"] if $first =~ /^#!.*\bperl.*-\w*T/;
	$fh->close or print "can't close $test. $!\n";
	my $cmd = ($ENV{'COMPILE_TEST'})? 
"./perl -I../lib ../utils/perlcc $test -run -verbose dcf -log ./compilelog |" 
															:  "$^X $s $test|";
	$cmd = "MCR $cmd" if $^O eq 'VMS';
	$fh->open($cmd) or print "can't run $test. $!\n";
a39 3
	my %todo = ();
        my $bonus = 0;
	my $skipped = 0;
d44 10
a53 16
	    if (/^1\.\.([0-9]+) todo([\d\s]+)\;/) {
		$max = $1;
		for (split(/\s+/, $2)) { $todo{$_} = 1; }
		$totmax += $max;
		$files++;
		$next = 1;
	    } elsif (/^1\.\.([0-9]+)/) {
		$max = $1;
		$totmax += $max;
		$files++;
		$next = 1;
	    } elsif ($max && /^(not\s+)?ok\b/) {
		my $this = $next;
		if (/^not ok\s*(\d*)/){
		    $this = $1 if $1 > 0;
		    if (!$todo{$this}) {
d55 2
a56 1
		    } else {
d60 10
a69 15
		} elsif (/^ok\s*(\d*)(\s*\#\s*[Ss]kip)?/) {
		    $this = $1 if $1 > 0;
		    $ok++;
		    $totok++;
		    $skipped++ if defined $2;
		    $bonus++, $totbonus++ if $todo{$this};
		}
		if ($this > $next) {
		    # warn "Test output counter mismatch [test $this]\n";
		    # no need to warn probably
		    push @@failed, $next..$this-1;
		} elsif ($this < $next) {
		    #we have seen more "ok" lines than the number suggests
		    warn "Confused test output: test $this answered after test ", $next-1, "\n";
		    $next = $this;
a70 1
		$next = $this + 1;
d74 4
a77 51
	my $wstatus = $ignore_exitcode ? 0 : $?;	# Can trust $? ?
	my $estatus;
	$estatus = ($^O eq 'VMS'
		       ? eval 'use vmsish "status"; $estatus = $?'
		       : $wstatus >> 8);
	if ($wstatus) {
	    my ($failed, $canon, $percent) = ('??', '??');
	    printf "dubious\n\tTest returned status $estatus (wstat %d, 0x%x)\n",
		    $wstatus,$wstatus;
	    print "\t\t(VMS status is $estatus)\n" if $^O eq 'VMS';
	    if (corestatus($wstatus)) { # until we have a wait module
		if ($have_devel_corestack) {
		    Devel::CoreStack::stack($^X);
		} else {
		    print "\ttest program seems to have generated a core\n";
		}
	    }
	    $bad++;
	    if ($max) {
	      if ($next == $max + 1 and not @@failed) {
		print "\tafter all the subtests completed successfully\n";
		$percent = 0;
		$failed = 0;	# But we do not set $canon!
	      } else {
		push @@failed, $next..$max;
		$failed = @@failed;
		(my $txt, $canon) = canonfailed($max,$skipped,@@failed);
		$percent = 100*(scalar @@failed)/$max;
		print "DIED. ",$txt;
	      }
	    }
	    $failedtests{$test} = { canon => $canon,  max => $max || '??',
				    failed => $failed, 
				    name => $test, percent => $percent,
				    estat => $estatus, wstat => $wstatus,
				  };
	} elsif ($ok == $max && $next == $max+1) {
	    if ($max and $skipped + $bonus) {
		my @@msg;
		push(@@msg, "$skipped/$max subtest".($skipped>1?'s':'')." skipped")
		    if $skipped;
		push(@@msg, "$bonus subtest".($bonus>1?'s':'').
		     " unexpectedly succeeded")
		    if $bonus;
		print "ok, ".join(', ', @@msg)."\n";
	    } elsif ($max) {
		print "ok\n";
	    } else {
		print "skipping test on this platform\n";
		$tests_skipped++;
	    }
d84 1
a84 7
		my ($txt, $canon) = canonfailed($max,$skipped,@@failed);
		print $txt;
		$failedtests{$test} = { canon => $canon,  max => $max,
					failed => scalar @@failed,
					name => $test, percent => 100*(scalar @@failed)/$max,
					estat => '', wstat => '',
				      };
d86 1
a86 6
		print "Don't know which tests failed: got $ok ok, expected $max\n";
		$failedtests{$test} = { canon => '??',  max => $max,
					failed => '??', 
					name => $test, percent => undef,
					estat => '', wstat => '',
				      };
a91 5
	    $failedtests{$test} = { canon => '??',  max => '??',
				    failed => '??',
				    name => $test, percent => undef,
				    estat => '', wstat => '',
				  };
d93 2
a94 11
	$subtests_skipped += $skipped;
	if (defined $files_in_dir) {
	    my @@new_dir_files = globdir $files_in_dir;
	    if (@@new_dir_files != @@dir_files) {
		my %f;
		@@f{@@new_dir_files} = (1) x @@new_dir_files;
		delete @@f{@@dir_files};
		my @@f = sort keys %f;
		print "LEAKED FILES: @@f\n";
		@@dir_files = @@new_dir_files;
	    }
a98 21
    if ($^O eq 'VMS') {
	if (defined $old5lib) {
	    $ENV{PERL5LIB} = $old5lib;
	} else {
	    delete $ENV{PERL5LIB};
	}
    }
    my $bonusmsg = '';
    $bonusmsg = (" ($totbonus subtest".($totbonus>1?'s':'').
	       " UNEXPECTEDLY SUCCEEDED)")
	if $totbonus;
    if ($tests_skipped) {
	$bonusmsg .= ", $tests_skipped test" . ($tests_skipped != 1 ? 's' : '') .
			' skipped';
    }
    if ($subtests_skipped) {
	$bonusmsg .= ($tests_skipped ? ', plus ' : ', '). 
			"$subtests_skipped subtest"
			. ($subtests_skipped != 1 ? 's' : '') .
			" skipped";
    }
d100 1
a100 1
	print "All tests successful$bonusmsg.\n";
d105 1
a105 1
	die "FAILED--$total test $blurb could be run, alas--no output ever seen\n";
d110 3
a112 8
	my $script;
	for $script (sort keys %failedtests) {
	  $curtest = $failedtests{$script};
	  write;
	}
	if ($bad) {
	    $bonusmsg =~ s/^,\s*//;
	    print "$bonusmsg.\n" if $bonusmsg;
a116 22

    return ($bad == 0 && $totmax) ;
}

my $tried_devel_corestack;
sub corestatus {
    my($st) = @@_;
    my($ret);

    eval {require 'wait.ph'};
    if ($@@) {
      SWITCH: {
	    $ret = ($st & 0200); # Tim says, this is for 90%
	}
    } else {
	$ret = WCOREDUMP($st);
    }

    eval { require Devel::CoreStack; $have_devel_corestack++ } 
      unless $tried_devel_corestack++;

    $ret;
d120 1
a120 1
    my($max,$skipped,@@failed) = @@_;
a127 1
    my $canon;
a141 1
	$canon = "@@canon";
a143 1
	$canon = $last;
d147 2
a148 8
    push @@result, sprintf("%.2f",100*(1-$failed/$max)), "% okay";
    my $ender = 's' x ($skipped > 1);
    my $good = $max - $failed - $skipped;
    my $goodper = sprintf("%.2f",100*($good/$max));
    push @@result, " (-$skipped skipped test$ender: $good okay, $goodper%)" if $skipped;
    push @@result, "\n";
    my $txt = join "", @@result;
    ($txt, $canon);
a165 4
(By using the L<Test> module, you can write test scripts without
knowing the exact output this module expects.  However, if you need to
know the specifics, read on!)

d168 1
a168 1
output by a standard test script is C<"1..M"> with C<M> being the
d170 1
a170 1
script. Test::Harness::runtests(@@tests) runs all the testscripts
d174 1
a174 1
After all tests have been performed, runtests() prints some
d181 3
a183 2
output containing C</^(not\s+)?ok\b/> are interpreted as feedback for
runtests().  All other lines are discarded.
d204 1
a204 1
used to let runtests() display the standard output of the script
a206 9
The global variable $Test::Harness::switches is exportable and can be
used to set perl command line options used for running the test
script(s). The default value is C<-w>.

If the standard output line contains substring C< # Skip> (with
variations in spacing and case) after C<ok> or C<ok NUMBER>, it is
counted as a skipped test.  If the whole testscript succeeds, the
count of skipped tests is included in the generated output.

d209 1
a209 1
C<&runtests> is exported by Test::Harness per default.
d227 1
a227 1
Scripts that return a non-zero exit status, both C<$? E<gt>E<gt> 8> and C<$?> are
a238 15
=head1 ENVIRONMENT

Setting C<HARNESS_IGNORE_EXITCODE> makes harness ignore the exit status
of child processes.

If C<HARNESS_FILELEAK_IN_DIR> is set to the name of a directory, harness
will check after each test whether new files appeared in that directory,
and report them as

  LEAKED FILES: scr.tmp 0 my.db

If relative, directory name is with respect to the current directory at
the moment runtests() was called.  Putting absolute path into 
C<HARNESS_FILELEAK_IN_DIR> may give more predicatable results.

d241 1
a241 2
L<Test> for writing test scripts and also L<Benchmark> for the
underlying timing routines.
d247 2
a248 2
with perl distributions for ages. Numerous anonymous contributors
exist. Current maintainer is Andreas Koenig.
@


1.1.1.3
log
@virgin perl 5.6.0
@
text
@d3 1
a3 1
use 5.005_64;
d10 2
a11 2
our($VERSION, $verbose, $switches, $have_devel_corestack, $curtest,
    @@ISA, @@EXPORT, @@EXPORT_OK);
d14 1
a14 3
$VERSION = "1.1604";

$ENV{HARNESS_ACTIVE} = 1;
d65 1
d67 1
a67 13
    # VMS has a 255-byte limit on the length of %ENV entries, so
    # toss the ones that involve perl_root, the install location
    # for VMS
    my $new5lib;
    if ($^O eq 'VMS') {
	$new5lib = join($Config{path_sep}, grep {!/perl_root/i;} @@INC);
	$switches =~ s/-(\S*[A-Z]\S*)/"-$1"/g;
    }
    else {
        $new5lib = join($Config{path_sep}, @@INC);
    }

    local($ENV{'PERL5LIB'}) = $new5lib;
d74 2
a75 7
	if ($^O eq 'VMS') { $te =~ s/^.*\.t\./[.t./s; }
	my $blank = (' ' x 77);
	my $leader = "$te" . '.' x (20 - length($te));
	my $ml = "";
	$ml = "\r$blank\r$leader"
	    if -t STDOUT and not $ENV{HARNESS_NOTTY} and not $verbose;
	print $leader;
d80 1
a80 4
	$s .= " $ENV{'HARNESS_PERL_SWITCHES'}"
	    if exists $ENV{'HARNESS_PERL_SWITCHES'};
	$s .= join " ", q[ "-T"], map {qq["-I$_"]} @@INC
	    if $first =~ /^#!.*\bperl.*-\w*T/;
d82 3
a84 4
	my $cmd = ($ENV{'HARNESS_COMPILE_TEST'})
		? "./perl -I../lib ../utils/perlcc $test "
		  . "-run 2>> ./compilelog |" 
		: "$^X $s $test|";
a91 1
	my $skip_reason;
d102 1
a102 1
	    } elsif (/^1\.\.([0-9]+)(\s*\#\s*[Ss]kip\S*(?>\s+)(.+))?/) {
a106 1
		$skip_reason = $3 if not $max and defined $3;
a110 1
		    print "${ml}NOK $this" if $ml;
d117 1
a117 1
		} elsif (/^ok\s*(\d*)(\s*\#\s*[Ss]kip\S*(?:(?>\s+)(.+))?)?/) {
a118 1
		    print "${ml}ok $this/$max" if $ml;
a121 10
		    my $reason;
		    $reason = 'unknown reason' if defined $2;
		    $reason = $3 if defined $3;
		    if (defined $reason and defined $skip_reason) {
		      # print "was: '$skip_reason' new '$reason'\n";
		      $skip_reason = 'various reasons'
			if $skip_reason ne $reason;
		    } elsif (defined $reason) {
		      $skip_reason = $reason;
		    }
d144 1
a144 1
	    printf "${ml}dubious\n\tTest returned status $estatus (wstat %d, 0x%x)\n",
d176 1
a176 1
		push(@@msg, "$skipped/$max skipped: $skip_reason")
d178 2
a179 1
		push(@@msg, "$bonus/$max unexpectedly succeeded")
d181 1
a181 1
		print "${ml}ok, ".join(', ', @@msg)."\n";
d183 1
a183 4
		print "${ml}ok\n";
	    } elsif (defined $skip_reason) {
		print "skipped: $skip_reason\n";
		$tests_skipped++;
d185 1
a185 1
		print "skipped test on this platform\n";
d246 2
a247 6
	$bonusmsg .= ", $tests_skipped test" . ($tests_skipped != 1 ? 's' : '');
	if ($subtests_skipped) {
	    $bonusmsg .= " and $subtests_skipped subtest"
			 . ($subtests_skipped != 1 ? 's' : '');
	}
	$bonusmsg .= ' skipped';
d249 5
a253 4
    elsif ($subtests_skipped) {
	$bonusmsg .= ", $subtests_skipped subtest"
	             . ($subtests_skipped != 1 ? 's' : '')
		     . " skipped";
d277 1
a277 1
    printf("Files=%d, Tests=%d, %s\n", $files, $totmax, timestr($t_total, 'nop'));
a410 6
C<Test::Harness> reports the text after C< # Skip(whatever)> as a
reason for skipping.  Similarly, one can include a similar explanation
in a C<1..0> line emitted if the test is skipped completely:

  1..0 # Skipped: no leverage found

a447 9
Setting C<HARNESS_NOTTY> to a true value forces it to behave as though
STDOUT were not a console.  You may need to set this if you don't want
harness to output more frequent progress messages using carriage returns.
Some consoles may not handle carriage returns properly (which results
in a somewhat messy output).

Setting C<HARNESS_COMPILE_TEST> to a true value will make harness attempt
to compile the test using C<perlcc> before running it.

a456 9

The value of C<HARNESS_PERL_SWITCHES> will be prepended to the
switches used to invoke perl on each test.  For example, setting
C<HARNESS_PERL_SWITCHES> to "-W" will run all tests with all
warnings enabled.

Harness sets C<HARNESS_ACTIVE> before executing the individual tests.
This allows the tests to determine if they are being executed through the
harness or by any other means.
@


1.1.1.4
log
@stock perl 5.6.1
@
text
@d11 1
a11 1
    $columns, @@ISA, @@EXPORT, @@EXPORT_OK);
d30 20
a51 1
$columns = $ENV{HARNESS_COLUMNS} || $ENV{COLUMNS} || 80;
d58 1
a58 1
    my($test,$te,$ok,$next,$max,$pct,$totbonus,@@failed,%failedtests);
a59 1
    my $totok = 0;
d105 1
a105 1
		  . "-r 2>> ./compilelog |" 
d161 1
a161 1
		    # print "Test output counter mismatch [test $this]\n";
d166 1
a166 1
		    print "Confused test output: test $this answered after test ", $next-1, "\n";
d233 1
a233 1
		print "${ml}$txt";
a306 6
	# Create formats
	#    First, figure out max length of test names
	my $failed_str = "Failed Test";
	my $middle_str = " Status Wstat Total Fail  Failed  ";
	my $list_str = "List of Failed";
	my $max_namelen = length($failed_str);
a307 41
	foreach $script (keys %failedtests) {
	    $max_namelen =
		(length $failedtests{$script}->{name} > $max_namelen) ?
		    length $failedtests{$script}->{name} : $max_namelen;
	}
	my $list_len = $columns - length($middle_str) - $max_namelen;
	if ($list_len < length($list_str)) {
	    $list_len = length($list_str);
	    $max_namelen = $columns - length($middle_str) - $list_len;
	    if ($max_namelen < length($failed_str)) {
		$max_namelen = length($failed_str);
		$columns = $max_namelen + length($middle_str) + $list_len;
	    }
	}

	my $fmt_top = "format STDOUT_TOP =\n"
		      . sprintf("%-${max_namelen}s", $failed_str)
		      . $middle_str
		      . $list_str . "\n"
		      . "-" x $columns
		      . "\n.\n";
	my $fmt = "format STDOUT =\n"
		  . "@@" . "<" x ($max_namelen - 1)
		  . "	 @@>> @@>>>> @@>>>> @@>>> ^##.##%  "
		  . "^" . "<" x ($list_len - 1) . "\n"
		  . '{ $curtest->{name}, $curtest->{estat},'
		  . '  $curtest->{wstat}, $curtest->{max},'
		  . '  $curtest->{failed}, $curtest->{percent},'
		  . '  $curtest->{canon}'
		  . "\n}\n"
		  . "~~" . " " x ($columns - $list_len - 2) . "^"
		  . "<" x ($list_len - 1) . "\n"
		  . '$curtest->{canon}'
		  . "\n.\n";

	eval $fmt_top;
	die $@@ if $@@;
	eval $fmt;
	die $@@ if $@@;

	# Now write to formats
d326 1
d329 7
a335 1
    my $ret = defined &WCOREDUMP ? WCOREDUMP($st) : $st & 0200;
a517 6

If C<HARNESS_COLUMNS> is set, then this value will be used for the
width of the terminal. If it is not set then it will default to
C<COLUMNS>. If this is not set, it will default to 80. Note that users
of Bourne-sh based shells will need to C<export COLUMNS> for this
module to use that variable.
@


1.1.1.5
log
@stock perl 5.8.0 from CPAN
@
text
@a0 3
# -*- Mode: cperl; cperl-indent-level: 4 -*-
# $Id: Harness.pm,v 1.38 2002/06/19 21:01:01 schwern Exp $

d3 1
a3 3
require 5.004;
use Test::Harness::Straps;
use Test::Harness::Assert;
d7 1
d10 10
a19 8
use vars qw($VERSION $Verbose $Switches $Have_Devel_Corestack $Curtest
            $Columns $verbose $switches $ML $Strap
            @@ISA @@EXPORT @@EXPORT_OK
           );

# Backwards compatibility for exportable variable names.
*verbose  = \$Verbose;
*switches = \$Switches;
d21 1
a21 1
$Have_Devel_Corestack = 0;
d23 38
a60 1
$VERSION = '2.26';
d62 1
a62 1
$ENV{HARNESS_ACTIVE} = 1;
d64 291
a354 4
END {
    # For VMS.
    delete $ENV{HARNESS_ACTIVE};
}
d356 2
a357 2
# Some experimental versions of OS/2 build have broken $?
my $Ignore_Exitcode = $ENV{HARNESS_IGNORE_EXITCODE};
d359 2
a360 1
my $Files_In_Dir = $ENV{HARNESS_FILELEAK_IN_DIR};
d362 2
a363 1
$Strap = Test::Harness::Straps->new;
d365 29
a393 3
@@ISA = ('Exporter');
@@EXPORT    = qw(&runtests);
@@EXPORT_OK = qw($verbose $switches);
d395 10
a404 4
$Verbose  = $ENV{HARNESS_VERBOSE} || 0;
$Switches = "-w";
$Columns  = $ENV{HARNESS_COLUMNS} || $ENV{COLUMNS} || 80;
$Columns--;             # Some shells have trouble with a full line of text.
d406 2
d415 1
a415 1
  use Test::Harness;
d417 1
a417 1
  runtests(@@test_files);
d421 1
a421 4
B<STOP!> If all you want to do is write a test script, consider using
Test::Simple.  Otherwise, read on.

(By using the Test module, you can write test scripts without
a437 24
The following explains how Test::Harness interprets the output of your
test program.

=over 4

=item B<'1..M'>

This header tells how many tests there will be.  For example, C<1..10>
means you plan on running 10 tests.  This is a safeguard in case your
test dies quietly in the middle of its run.

It should be the first non-comment line output by your test program.

In certain instances, you may not know how many tests you will
ultimately be running.  In this case, it is permitted for the 1..M
header to appear as the B<last> line output by your test (again, it
can be followed by further comments).

Under B<no> circumstances should 1..M appear in the middle of your
output or more than once.


=item B<'ok', 'not ok'.  Ok?>

d443 3
a445 10
C</^not ok/> indicates a failed test.  C</^ok/> is a successful test.


=item B<test numbers>

Perl normally expects the 'ok' or 'not ok' to be followed by a test
number.  It is tolerated if the test numbers after 'ok' are
omitted. In this case Test::Harness maintains temporarily its own
counter until the script supplies test numbers again. So the following
test script
d456 1
a456 1
will generate
a460 104
=item B<test names>

Anything after the test number but before the # is considered to be
the name of the test.

  ok 42 this is the name of the test

Currently, Test::Harness does nothing with this information.

=item B<Skipping tests>

If the standard output line contains the substring C< # Skip> (with
variations in spacing and case) after C<ok> or C<ok NUMBER>, it is
counted as a skipped test.  If the whole testscript succeeds, the
count of skipped tests is included in the generated output.
C<Test::Harness> reports the text after C< # Skip\S*\s+> as a reason
for skipping.

  ok 23 # skip Insufficient flogiston pressure.

Similarly, one can include a similar explanation in a C<1..0> line
emitted if the test script is skipped completely:

  1..0 # Skipped: no leverage found

=item B<Todo tests>

If the standard output line contains the substring C< # TODO> after
C<not ok> or C<not ok NUMBER>, it is counted as a todo test.  The text
afterwards is the thing that has to be done before this test will
succeed.

  not ok 13 # TODO harness the power of the atom

=begin _deprecated

Alternatively, you can specify a list of what tests are todo as part
of the test header.

  1..23 todo 5 12 23

This only works if the header appears at the beginning of the test.

This style is B<deprecated>.

=end _deprecated

These tests represent a feature to be implemented or a bug to be fixed
and act as something of an executable "thing to do" list.  They are
B<not> expected to succeed.  Should a todo test begin succeeding,
Test::Harness will report it as a bonus.  This indicates that whatever
you were supposed to do has been done and you should promote this to a
normal test.

=item B<Bail out!>

As an emergency measure, a test script can decide that further tests
are useless (e.g. missing dependencies) and testing should stop
immediately. In that case the test script prints the magic words

  Bail out!

to standard output. Any message after these words will be displayed by
C<Test::Harness> as the reason why testing is stopped.

=item B<Comments>

Additional comments may be put into the testing output on their own
lines.  Comment lines should begin with a '#', Test::Harness will
ignore them.

  ok 1
  # Life is good, the sun is shining, RAM is cheap.
  not ok 2
  # got 'Bush' expected 'Gore'

=item B<Anything else>

Any other output Test::Harness sees it will silently ignore B<BUT WE
PLAN TO CHANGE THIS!> If you wish to place additional output in your
test script, please use a comment.

=back


=head2 Taint mode

Test::Harness will honor the C<-T> in the #! line on your test files.  So
if you begin a test with:

    #!perl -T

the test will be run with taint mode on.


=head2 Configuration variables.

These variables can be used to configure the behavior of
Test::Harness.  They are exported on request.

=over 4

=item B<$Test::Harness::verbose>

a464 2
=item B<$Test::Harness::switches>

d469 4
a472 1
=back
d474 3
d478 1
a478 674
=head2 Failure

It will happen, your tests will fail.  After you mop up your ego, you
can begin examining the summary report:

  t/base..............ok
  t/nonumbers.........ok
  t/ok................ok
  t/test-harness......ok
  t/waterloo..........dubious
          Test returned status 3 (wstat 768, 0x300)
  DIED. FAILED tests 1, 3, 5, 7, 9, 11, 13, 15, 17, 19
          Failed 10/20 tests, 50.00% okay
  Failed Test  Stat Wstat Total Fail  Failed  List of Failed
  -----------------------------------------------------------------------
  t/waterloo.t    3   768    20   10  50.00%  1 3 5 7 9 11 13 15 17 19
  Failed 1/5 test scripts, 80.00% okay. 10/44 subtests failed, 77.27% okay.

Everything passed but t/waterloo.t.  It failed 10 of 20 tests and
exited with non-zero status indicating something dubious happened.

The columns in the summary report mean:

=over 4

=item B<Failed Test>

The test file which failed.

=item B<Stat>

If the test exited with non-zero, this is its exit status.

=item B<Wstat>

The wait status of the test I<umm, I need a better explanation here>.

=item B<Total>

Total number of tests expected to run.

=item B<Fail>

Number which failed, either from "not ok" or because they never ran.

=item B<Failed>

Percentage of the total tests which failed.

=item B<List of Failed>

A list of the tests which failed.  Successive failures may be
abbreviated (ie. 15-20 to indicate that tests 15, 16, 17, 18, 19 and
20 failed).

=back


=head2 Functions

Test::Harness currently only has one function, here it is.

=over 4

=item B<runtests>

  my $allok = runtests(@@test_files);

This runs all the given @@test_files and divines whether they passed
or failed based on their output to STDOUT (details above).  It prints
out each individual test which failed along with a summary report and
a how long it all took.

It returns true if everything was ok.  Otherwise it will die() with
one of the messages in the DIAGNOSTICS section.

=for _private
This is just _run_all_tests() plus _show_results()

=cut

sub runtests {
    my(@@tests) = @@_;

    local ($\, $,);

    my($tot, $failedtests) = _run_all_tests(@@tests);
    _show_results($tot, $failedtests);

    my $ok = _all_ok($tot);

    assert(($ok xor keys %$failedtests), 
           q{ok status jives with $failedtests});

    return $ok;
}

=begin _private

=item B<_all_ok>

  my $ok = _all_ok(\%tot);

Tells you if this test run is overall successful or not.

=cut

sub _all_ok {
    my($tot) = shift;

    return $tot->{bad} == 0 && ($tot->{max} || $tot->{skipped}) ? 1 : 0;
}

=item B<_globdir>

  my @@files = _globdir $dir;

Returns all the files in a directory.  This is shorthand for backwards
compatibility on systems where glob() doesn't work right.

=cut

sub _globdir { 
    opendir DIRH, shift; 
    my @@f = readdir DIRH; 
    closedir DIRH; 

    return @@f;
}

=item B<_run_all_tests>

  my($total, $failed) = _run_all_tests(@@test_files);

Runs all the given @@test_files (as runtests()) but does it quietly (no
report).  $total is a hash ref summary of all the tests run.  Its keys
and values are this:

    bonus           Number of individual todo tests unexpectedly passed
    max             Number of individual tests ran
    ok              Number of individual tests passed
    sub_skipped     Number of individual tests skipped
    todo            Number of individual todo tests

    files           Number of test files ran
    good            Number of test files passed
    bad             Number of test files failed
    tests           Number of test files originally given
    skipped         Number of test files skipped

If $total->{bad} == 0 and $total->{max} > 0, you've got a successful
test.

$failed is a hash ref of all the test scripts which failed.  Each key
is the name of a test script, each value is another hash representing
how that script failed.  Its keys are these:

    name        Name of the test which failed
    estat       Script's exit value
    wstat       Script's wait status
    max         Number of individual tests
    failed      Number which failed
    percent     Percentage of tests which failed
    canon       List of tests which failed (as string).

Needless to say, $failed should be empty if everything passed.

B<NOTE> Currently this function is still noisy.  I'm working on it.

=cut

#'#
sub _run_all_tests {
    my(@@tests) = @@_;
    local($|) = 1;
    my(%failedtests);

    # Test-wide totals.
    my(%tot) = (
                bonus    => 0,
                max      => 0,
                ok       => 0,
                files    => 0,
                bad      => 0,
                good     => 0,
                tests    => scalar @@tests,
                sub_skipped  => 0,
                todo     => 0,
                skipped  => 0,
                bench    => 0,
               );

    my @@dir_files = _globdir $Files_In_Dir if defined $Files_In_Dir;
    my $t_start = new Benchmark;

    my $width = _leader_width(@@tests);
    foreach my $tfile (@@tests) {

        my($leader, $ml) = _mk_leader($tfile, $width);
        local $ML = $ml;
        print $leader;

        $tot{files}++;

        $Strap->{_seen_header} = 0;
        my %results = $Strap->analyze_file($tfile) or
          do { warn "$Strap->{error}\n";  next };

        # state of the current test.
        my @@failed = grep { !$results{details}[$_-1]{ok} }
                     1..@@{$results{details}};
        my %test = (
                    ok          => $results{ok},
                    'next'      => $Strap->{'next'},
                    max         => $results{max},
                    failed      => \@@failed,
                    bonus       => $results{bonus},
                    skipped     => $results{skip},
                    skip_reason => $Strap->{_skip_reason},
                    skip_all    => $Strap->{skip_all},
                    ml          => $ml,
                   );

        $tot{bonus}       += $results{bonus};
        $tot{max}         += $results{max};
        $tot{ok}          += $results{ok};
        $tot{todo}        += $results{todo};
        $tot{sub_skipped} += $results{skip};

        my($estatus, $wstatus) = @@results{qw(exit wait)};

        if ($wstatus) {
            $failedtests{$tfile} = _dubious_return(\%test, \%tot, 
                                                  $estatus, $wstatus);
            $failedtests{$tfile}{name} = $tfile;
        }
        elsif ($results{passing}) {
            if ($test{max} and $test{skipped} + $test{bonus}) {
                my @@msg;
                push(@@msg, "$test{skipped}/$test{max} skipped: $test{skip_reason}")
                    if $test{skipped};
                push(@@msg, "$test{bonus}/$test{max} unexpectedly succeeded")
                    if $test{bonus};
                print "$test{ml}ok\n        ".join(', ', @@msg)."\n";
            } elsif ($test{max}) {
                print "$test{ml}ok\n";
            } elsif (defined $test{skip_all} and length $test{skip_all}) {
                print "skipped\n        all skipped: $test{skip_all}\n";
                $tot{skipped}++;
            } else {
                print "skipped\n        all skipped: no reason given\n";
                $tot{skipped}++;
            }
            $tot{good}++;
        }
        else {
            if ($test{max}) {
                if ($test{'next'} <= $test{max}) {
                    push @@{$test{failed}}, $test{'next'}..$test{max};
                }
                if (@@{$test{failed}}) {
                    my ($txt, $canon) = canonfailed($test{max},$test{skipped},
                                                    @@{$test{failed}});
                    print "$test{ml}$txt";
                    $failedtests{$tfile} = { canon   => $canon,
                                             max     => $test{max},
                                             failed  => scalar @@{$test{failed}},
                                             name    => $tfile, 
                                             percent => 100*(scalar @@{$test{failed}})/$test{max},
                                             estat   => '',
                                             wstat   => '',
                                           };
                } else {
                    print "Don't know which tests failed: got $test{ok} ok, ".
                          "expected $test{max}\n";
                    $failedtests{$tfile} = { canon   => '??',
                                             max     => $test{max},
                                             failed  => '??',
                                             name    => $tfile, 
                                             percent => undef,
                                             estat   => '', 
                                             wstat   => '',
                                           };
                }
                $tot{bad}++;
            } elsif ($test{'next'} == 0) {
                print "FAILED before any test output arrived\n";
                $tot{bad}++;
                $failedtests{$tfile} = { canon       => '??',
                                         max         => '??',
                                         failed      => '??',
                                         name        => $tfile,
                                         percent     => undef,
                                         estat       => '', 
                                         wstat       => '',
                                       };
            }
        }

        if (defined $Files_In_Dir) {
            my @@new_dir_files = _globdir $Files_In_Dir;
            if (@@new_dir_files != @@dir_files) {
                my %f;
                @@f{@@new_dir_files} = (1) x @@new_dir_files;
                delete @@f{@@dir_files};
                my @@f = sort keys %f;
                print "LEAKED FILES: @@f\n";
                @@dir_files = @@new_dir_files;
            }
        }
    }
    $tot{bench} = timediff(new Benchmark, $t_start);

    $Strap->_restore_PERL5LIB;

    return(\%tot, \%failedtests);
}

=item B<_mk_leader>

  my($leader, $ml) = _mk_leader($test_file, $width);

Generates the 't/foo........' $leader for the given $test_file as well
as a similar version which will overwrite the current line (by use of
\r and such).  $ml may be empty if Test::Harness doesn't think you're
on TTY.

The $width is the width of the "yada/blah.." string.

=cut

sub _mk_leader {
    my($te, $width) = @@_;
    chomp($te);
    $te =~ s/\.\w+$/./;

    if ($^O eq 'VMS') { $te =~ s/^.*\.t\./\[.t./s; }
    my $blank = (' ' x 77);
    my $leader = "$te" . '.' x ($width - length($te));
    my $ml = "";

    $ml = "\r$blank\r$leader"
      if -t STDOUT and not $ENV{HARNESS_NOTTY} and not $Verbose;

    return($leader, $ml);
}

=item B<_leader_width>

  my($width) = _leader_width(@@test_files);

Calculates how wide the leader should be based on the length of the
longest test name.

=cut

sub _leader_width {
    my $maxlen = 0;
    my $maxsuflen = 0;
    foreach (@@_) {
        my $suf    = /\.(\w+)$/ ? $1 : '';
        my $len    = length;
        my $suflen = length $suf;
        $maxlen    = $len    if $len    > $maxlen;
        $maxsuflen = $suflen if $suflen > $maxsuflen;
    }
    # + 3 : we want three dots between the test name and the "ok"
    return $maxlen + 3 - $maxsuflen;
}


sub _show_results {
    my($tot, $failedtests) = @@_;

    my $pct;
    my $bonusmsg = _bonusmsg($tot);

    if (_all_ok($tot)) {
        print "All tests successful$bonusmsg.\n";
    } elsif (!$tot->{tests}){
        die "FAILED--no tests were run for some reason.\n";
    } elsif (!$tot->{max}) {
        my $blurb = $tot->{tests}==1 ? "script" : "scripts";
        die "FAILED--$tot->{tests} test $blurb could be run, ".
            "alas--no output ever seen\n";
    } else {
        $pct = sprintf("%.2f", $tot->{good} / $tot->{tests} * 100);
        my $percent_ok = 100*$tot->{ok}/$tot->{max};
        my $subpct = sprintf " %d/%d subtests failed, %.2f%% okay.",
                              $tot->{max} - $tot->{ok}, $tot->{max}, 
                              $percent_ok;

        my($fmt_top, $fmt) = _create_fmts($failedtests);

        # Now write to formats
        for my $script (sort keys %$failedtests) {
          $Curtest = $failedtests->{$script};
          write;
        }
        if ($tot->{bad}) {
            $bonusmsg =~ s/^,\s*//;
            print "$bonusmsg.\n" if $bonusmsg;
            die "Failed $tot->{bad}/$tot->{tests} test scripts, $pct% okay.".
                "$subpct\n";
        }
    }

    printf("Files=%d, Tests=%d, %s\n",
           $tot->{files}, $tot->{max}, timestr($tot->{bench}, 'nop'));
}


my %Handlers = ();
$Strap->{callback} = sub {
    my($self, $line, $type, $totals) = @@_;
    print $line if $Verbose;

    my $meth = $Handlers{$type};
    $meth->($self, $line, $type, $totals) if $meth;
};


$Handlers{header} = sub {
    my($self, $line, $type, $totals) = @@_;

    warn "Test header seen more than once!\n" if $self->{_seen_header};

    $self->{_seen_header}++;

    warn "1..M can only appear at the beginning or end of tests\n"
      if $totals->{seen} && 
         $totals->{max}  < $totals->{seen};
};

$Handlers{test} = sub {
    my($self, $line, $type, $totals) = @@_;

    my $curr = $totals->{seen};
    my $next = $self->{'next'};
    my $max  = $totals->{max};
    my $detail = $totals->{details}[-1];

    if( $detail->{ok} ) {
        _print_ml("ok $curr/$max");

        if( $detail->{type} eq 'skip' ) {
            $self->{_skip_reason} = $detail->{reason}
              unless defined $self->{_skip_reason};
            $self->{_skip_reason} = 'various reasons'
              if $self->{_skip_reason} ne $detail->{reason};
        }
    }
    else {
        _print_ml("NOK $curr");
    }

    if( $curr > $next ) {
        print "Test output counter mismatch [test $curr]\n";
    }
    elsif( $curr < $next ) {
        print "Confused test output: test $curr answered after ".
              "test ", $next - 1, "\n";
    }

};

$Handlers{bailout} = sub {
    my($self, $line, $type, $totals) = @@_;

    die "FAILED--Further testing stopped" .
      ($self->{bailout_reason} ? ": $self->{bailout_reason}\n" : ".\n");
};


sub _print_ml {
    print join '', $ML, @@_ if $ML;
}


sub _bonusmsg {
    my($tot) = @@_;

    my $bonusmsg = '';
    $bonusmsg = (" ($tot->{bonus} subtest".($tot->{bonus} > 1 ? 's' : '').
               " UNEXPECTEDLY SUCCEEDED)")
        if $tot->{bonus};

    if ($tot->{skipped}) {
        $bonusmsg .= ", $tot->{skipped} test"
                     . ($tot->{skipped} != 1 ? 's' : '');
        if ($tot->{sub_skipped}) {
            $bonusmsg .= " and $tot->{sub_skipped} subtest"
                         . ($tot->{sub_skipped} != 1 ? 's' : '');
        }
        $bonusmsg .= ' skipped';
    }
    elsif ($tot->{sub_skipped}) {
        $bonusmsg .= ", $tot->{sub_skipped} subtest"
                     . ($tot->{sub_skipped} != 1 ? 's' : '')
                     . " skipped";
    }

    return $bonusmsg;
}

# Test program go boom.
sub _dubious_return {
    my($test, $tot, $estatus, $wstatus) = @@_;
    my ($failed, $canon, $percent) = ('??', '??');

    printf "$test->{ml}dubious\n\tTest returned status $estatus ".
           "(wstat %d, 0x%x)\n",
           $wstatus,$wstatus;
    print "\t\t(VMS status is $estatus)\n" if $^O eq 'VMS';

    if (corestatus($wstatus)) { # until we have a wait module
        if ($Have_Devel_Corestack) {
            Devel::CoreStack::stack($^X);
        } else {
            print "\ttest program seems to have generated a core\n";
        }
    }

    $tot->{bad}++;

    if ($test->{max}) {
        if ($test->{'next'} == $test->{max} + 1 and not @@{$test->{failed}}) {
            print "\tafter all the subtests completed successfully\n";
            $percent = 0;
            $failed = 0;        # But we do not set $canon!
        }
        else {
            push @@{$test->{failed}}, $test->{'next'}..$test->{max};
            $failed = @@{$test->{failed}};
            (my $txt, $canon) = canonfailed($test->{max},$test->{skipped},@@{$test->{failed}});
            $percent = 100*(scalar @@{$test->{failed}})/$test->{max};
            print "DIED. ",$txt;
        }
    }

    return { canon => $canon,  max => $test->{max} || '??',
             failed => $failed, 
             percent => $percent,
             estat => $estatus, wstat => $wstatus,
           };
}


sub _create_fmts {
    my($failedtests) = @@_;

    my $failed_str = "Failed Test";
    my $middle_str = " Stat Wstat Total Fail  Failed  ";
    my $list_str = "List of Failed";

    # Figure out our longest name string for formatting purposes.
    my $max_namelen = length($failed_str);
    foreach my $script (keys %$failedtests) {
        my $namelen = length $failedtests->{$script}->{name};
        $max_namelen = $namelen if $namelen > $max_namelen;
    }

    my $list_len = $Columns - length($middle_str) - $max_namelen;
    if ($list_len < length($list_str)) {
        $list_len = length($list_str);
        $max_namelen = $Columns - length($middle_str) - $list_len;
        if ($max_namelen < length($failed_str)) {
            $max_namelen = length($failed_str);
            $Columns = $max_namelen + length($middle_str) + $list_len;
        }
    }

    my $fmt_top = "format STDOUT_TOP =\n"
                  . sprintf("%-${max_namelen}s", $failed_str)
                  . $middle_str
                  . $list_str . "\n"
                  . "-" x $Columns
                  . "\n.\n";

    my $fmt = "format STDOUT =\n"
              . "@@" . "<" x ($max_namelen - 1)
              . "  @@>> @@>>>> @@>>>> @@>>> ^##.##%  "
              . "^" . "<" x ($list_len - 1) . "\n"
              . '{ $Curtest->{name}, $Curtest->{estat},'
              . '  $Curtest->{wstat}, $Curtest->{max},'
              . '  $Curtest->{failed}, $Curtest->{percent},'
              . '  $Curtest->{canon}'
              . "\n}\n"
              . "~~" . " " x ($Columns - $list_len - 2) . "^"
              . "<" x ($list_len - 1) . "\n"
              . '$Curtest->{canon}'
              . "\n.\n";

    eval $fmt_top;
    die $@@ if $@@;
    eval $fmt;
    die $@@ if $@@;

    return($fmt_top, $fmt);
}

{
    my $tried_devel_corestack;

    sub corestatus {
        my($st) = @@_;

        eval {
            local $^W = 0;  # *.ph files are often *very* noisy
            require 'wait.ph'
        };
        return if $@@;
        my $did_core = defined &WCOREDUMP ? WCOREDUMP($st) : $st & 0200;

        eval { require Devel::CoreStack; $Have_Devel_Corestack++ } 
          unless $tried_devel_corestack++;

        return $did_core;
    }
}

sub canonfailed ($@@) {
    my($max,$skipped,@@failed) = @@_;
    my %seen;
    @@failed = sort {$a <=> $b} grep !$seen{$_}++, @@failed;
    my $failed = @@failed;
    my @@result = ();
    my @@canon = ();
    my $min;
    my $last = $min = shift @@failed;
    my $canon;
    if (@@failed) {
        for (@@failed, $failed[-1]) { # don't forget the last one
            if ($_ > $last+1 || $_ == $last) {
                if ($min == $last) {
                    push @@canon, $last;
                } else {
                    push @@canon, "$min-$last";
                }
                $min = $_;
            }
            $last = $_;
        }
        local $" = ", ";
        push @@result, "FAILED tests @@canon\n";
        $canon = join ' ', @@canon;
    } else {
        push @@result, "FAILED test $last\n";
        $canon = $last;
    }

    push @@result, "\tFailed $failed/$max tests, ";
    push @@result, sprintf("%.2f",100*(1-$failed/$max)), "% okay";
    my $ender = 's' x ($skipped > 1);
    my $good = $max - $failed - $skipped;
    my $goodper = sprintf("%.2f",100*($good/$max));
    push @@result, " (less $skipped skipped test$ender: $good okay, ".
                  "$goodper%)"
         if $skipped;
    push @@result, "\n";
    my $txt = join "", @@result;
    ($txt, $canon);
}

=end _private

=back

=cut


1;
__END__

a483 3
C<$verbose> and C<$switches> are exported upon request.


d500 2
a501 2
Scripts that return a non-zero exit status, both C<$? E<gt>E<gt> 8>
and C<$?> are printed in a message similar to the above.
a509 5
=item C<FAILED--Further testing stopped: %s>

If a single subtest decides that further testing will not make sense,
the script dies with this message.

d514 2
a515 14
=over 4

=item C<HARNESS_ACTIVE>

Harness sets this before executing the individual tests.  This allows
the tests to determine if they are being executed through the harness
or by any other means.

=item C<HARNESS_COLUMNS>

This value will be used for the width of the terminal. If it is not
set then it will default to C<COLUMNS>. If this is not set, it will
default to 80. Note that users of Bourne-sh based shells will need to
C<export COLUMNS> for this module to use that variable.
d517 12
a528 12
=item C<HARNESS_COMPILE_TEST>

When true it will make harness attempt to compile the test using
C<perlcc> before running it.

B<NOTE> This currently only works when sitting in the perl source
directory!

=item C<HARNESS_FILELEAK_IN_DIR>

When set to the name of a directory, harness will check after each
test whether new files appeared in that directory, and report them as
d534 1
a534 3
C<HARNESS_FILELEAK_IN_DIR> may give more predictable results.

=item C<HARNESS_IGNORE_EXITCODE>
d536 14
a549 37
Makes harness ignore the exit status of child processes when defined.

=item C<HARNESS_NOTTY>

When set to a true value, forces it to behave as though STDOUT were
not a console.  You may need to set this if you don't want harness to
output more frequent progress messages using carriage returns.  Some
consoles may not handle carriage returns properly (which results in a
somewhat messy output).

=item C<HARNESS_PERL_SWITCHES>

Its value will be prepended to the switches used to invoke perl on
each test.  For example, setting C<HARNESS_PERL_SWITCHES> to C<-W> will
run all tests with all warnings enabled.

=item C<HARNESS_VERBOSE>

If true, Test::Harness will output the verbose results of running
its tests.  Setting $Test::Harness::verbose will override this.

=back

=head1 EXAMPLE

Here's how Test::Harness tests itself

  $ cd ~/src/devel/Test-Harness
  $ perl -Mblib -e 'use Test::Harness qw(&runtests $verbose);
    $verbose=0; runtests @@ARGV;' t/*.t
  Using /home/schwern/src/devel/Test-Harness/blib
  t/base..............ok
  t/nonumbers.........ok
  t/ok................ok
  t/test-harness......ok
  All tests successful.
  Files=4, Tests=24, 2 wallclock secs ( 0.61 cusr + 0.41 csys = 1.02 CPU)
d553 2
a554 4
L<Test> and L<Test::Simple> for writing test scripts, L<Benchmark> for
the underlying timing routines, L<Devel::CoreStack> to generate core
dumps from failed tests and L<Devel::Cover> for test coverage
analysis.
d561 1
a561 27
exist.  Andreas Koenig held the torch for many years.

Current maintainer is Michael G Schwern E<lt>schwern@@pobox.comE<gt>

=head1 TODO

Provide a way of running tests quietly (ie. no printing) for automated
validation of tests.  This will probably take the form of a version
of runtests() which rather than printing its output returns raw data
on the state of the tests.  (Partially done in Test::Harness::Straps)

Fix HARNESS_COMPILE_TEST without breaking its core usage.

Figure a way to report test names in the failure summary.

Rework the test summary so long test names are not truncated as badly.
(Partially done with new skip test styles)

Deal with VMS's "not \nok 4\n" mistake.

Add option for coverage analysis.

=for _private
Keeping whittling away at _run_all_tests()

=for _private
Clean up how the summary is printed.  Get rid of those damned formats.
d565 5
a569 2
HARNESS_COMPILE_TEST currently assumes it's run from the Perl source
directory.
@


1.1.1.6
log
@perl 5.8.2 from CPAN
@
text
@d2 1
a2 1
# $Id: Harness.pm,v 1.54 2003/08/15 01:05:00 andy Exp $
d16 1
a16 1
            @@ISA @@EXPORT @@EXPORT_OK $Last_ML_Print
d20 2
a21 2
*verbose  = *Verbose;
*switches = *Switches;
d25 1
a25 1
$VERSION = '2.30';
a38 2
my $Ok_Slow = $ENV{HARNESS_OK_SLOW};

a330 1

d451 1
a451 1
        $Last_ML_Print = 0;  # so each test prints at least once
d472 1
a472 1
                    skip_reason => $results{skip_reason},
d485 6
a490 1
        if ($results{passing}) {
d510 3
a512 11
            # List unrun tests as failures.
            if ($test{'next'} <= $test{max}) {
                push @@{$test{failed}}, $test{'next'}..$test{max};
            }
            # List overruns as failures.
            else {
                my $details = $results{details};
                foreach my $overrun ($test{max}+1..@@$details)
                {
                    next unless ref $details->[$overrun-1];
                    push @@{$test{failed}}, $overrun
d514 1
a514 9
            }

            if ($wstatus) {
                $failedtests{$tfile} = _dubious_return(\%test, \%tot, 
                                                       $estatus, $wstatus);
                $failedtests{$tfile}{name} = $tfile;
            }
            elsif($results{seen}) {
                if (@@{$test{failed}} and $test{max}) {
d539 1
a539 1
            } else {
d697 1
a697 1
        _print_ml_less("ok $curr/$max");
d700 4
a703 4
            $totals->{skip_reason} = $detail->{reason}
              unless defined $totals->{skip_reason};
            $totals->{skip_reason} = 'various reasons'
              if $totals->{skip_reason} ne $detail->{reason};
a732 9
# For slow connections, we save lots of bandwidth by printing only once
# per second.
sub _print_ml_less {
    if( !$Ok_Slow || $Last_ML_Print != time ) {
        _print_ml(@@_);
        $Last_ML_Print = time;
    }
}

d861 1
a861 2
        my $did_core;
        eval { # we may not have a WCOREDUMP
d863 1
a863 2
            require 'wait.ph';
            $did_core = WCOREDUMP($st);
d865 2
a866 3
        if( $@@ ) {
            $did_core = $st & 0200;
        }
d875 1
a875 1
sub canonfailed ($$@@) {
d906 1
a906 5
    if ($max) {
	push @@result, sprintf("%.2f",100*(1-$failed/$max)), "% okay";
    } else {
	push @@result, "?% okay";
    }
d909 4
a912 10
    if ($skipped) {
	my $skipmsg = " (less $skipped skipped test$ender: $good okay, ";
	if ($max) {
	    my $goodper = sprintf("%.2f",100*($good/$max));
	    $skipmsg .= "$goodper%)";
	} else {
	    $skipmsg .= "?%)";
	}
	push @@result, $skipmsg;
    }
d931 1
a931 1
C<&runtests> is exported by Test::Harness by default.
a1016 6
=item C<HARNESS_OK_SLOW>

If true, the C<ok> messages are printed out only every second.
This reduces output and therefore may for example help testing
over slow connections.

d1057 1
a1057 2
exist.  Andreas Koenig held the torch for many years, and then
Michael G Schwern.
d1059 1
a1059 8
Current maintainer is Andy Lester C<< <andy@@petdance.com> >>.

=head1 LICENSE

This program is free software; you can redistribute it and/or 
modify it under the same terms as Perl itself.

See L<http://www.perl.com/perl/misc/Artistic.html>
a1067 2
Document the format.

a1078 24
Trap STDERR.

Implement Straps total_results()

Remember exit code

Completely redo the print summary code.

Implement Straps callbacks.  (experimentally implemented)

Straps->analyze_file() not taint clean, don't know if it can be

Fix that damned VMS nit.

HARNESS_TODOFAIL to display TODO failures

Add a test for verbose.

Change internal list of test results to a hash.

Fix stats display when there's an overrun.

Fix so perls with spaces in the filename work.

a1079 1

a1082 1

@


1.1.1.7
log
@perl 5.8.3 from CPAN
@
text
@d2 1
a2 1
# $Id: Harness.pm,v 1.80 2003/12/31 02:39:21 andy Exp $
d14 4
a17 25
use vars qw(
    $VERSION 
    @@ISA @@EXPORT @@EXPORT_OK 
    $Verbose $Switches $Debug
    $verbose $switches $debug
    $Have_Devel_Corestack
    $Curtest
    $Columns 
    $ML $Last_ML_Print
    $Strap
);

=head1 NAME

Test::Harness - Run Perl standard test scripts with statistics

=head1 VERSION

Version 2.40

    $Header: /home/cvs/test-harness/lib/Test/Harness.pm,v 1.80 2003/12/31 02:39:21 andy Exp $

=cut

$VERSION = '2.40';
a21 1
*debug    = *Debug;
d25 2
a47 1
$Debug    = $ENV{HARNESS_DEBUG} || 0;
d52 5
d166 1
a166 1
If the standard output line contains the substring C< # TODO > after
a172 2
Note that the TODO must have a space after it. 

d223 1
d226 2
a227 2
Test::Harness will honor the C<-T> or C<-t> in the #! line on your
test files.  So if you begin a test with:
d233 1
d241 1
a241 1
=item B<$Test::Harness::Verbose>
d243 3
a245 4
The global variable C<$Test::Harness::Verbose> is exportable and can be
used to let C<runtests()> display the standard output of the script
without altering the behavior otherwise.  The F<prove> utility's C<-v>
flag will set this.
d249 1
a249 1
The global variable C<$Test::Harness::switches> is exportable and can be
d251 1
a251 1
script(s). The default value is C<-w>. It overrides C<HARNESS_SWITCHES>.
d258 1
a258 1
It will happen: your tests will fail.  After you mop up your ego, you
d291 1
a291 1
The wait status of the test.
d391 3
a393 3
Runs all the given C<@@test_files> (as C<runtests()>) but does it
quietly (no report).  $total is a hash ref summary of all the tests
run.  Its keys and values are this:
d407 2
a408 2
If C<< $total->{bad} == 0 >> and C<< $total->{max} > 0 >>, you've
got a successful test.
d422 1
a422 1
C<$failed> should be empty if everything passed.
a453 4
	if ( $Test::Harness::Debug ) {
	    print "# Running: ", $Strap->_command_line($tfile), "\n";
	}

a456 1

d463 1
a463 1
          do { warn $Strap->{error}, "\n";  next };
d529 1
a529 1
                    my ($txt, $canon) = _canonfailed($test{max},$test{skipped},
d590 1
a590 1
Generates the 't/foo........' $leader for the given C<$test_file> as well
d592 1
a592 1
\r and such).  C<$ml> may be empty if Test::Harness doesn't think you're
d595 1
a595 1
The C<$width> is the width of the "yada/blah.." string.
d792 1
a792 1
    if (_corestatus($wstatus)) { # until we have a wait module
d811 1
a811 1
            (my $txt, $canon) = _canonfailed($test->{max},$test->{skipped},@@{$test->{failed}});
d881 1
a881 1
    sub _corestatus {
d901 1
a901 1
sub _canonfailed ($$@@) {
d969 2
a970 1
C<$verbose>, C<$switches> and C<$debug> are exported upon request.
a1029 7
=item C<HARNESS_DEBUG>

If true, Test::Harness will print debugging information about itself as
it runs the tests.  This is different from C<HARNESS_VERBOSE>, which prints
the output from the test being run.  Setting C<$Test::Harness::Debug> will
override this, or you can use the C<-d> switch in the F<prove> utility.

d1055 3
a1057 11
If true, the C<ok> messages are printed out only every second.  This
reduces output and may help increase testing speed over slow
connections, or with very large numbers of tests.

=item C<HARNESS_PERL>

Usually your tests will be run by C<$^X>, the currently-executing Perl.
However, you may want to have it run by a different executable, such as
a threading perl, or a different version.

If you're using the F<prove> utility, you can use the C<--perl> switch.
d1068 1
a1068 2
its tests.  Setting C<$Test::Harness::verbose> will override this,
or you can use the C<-v> switch in the F<prove> utility.
a1166 18

Please use the CPAN bug ticketing system at L<http://rt.cpan.org/>.
You can also mail bugs, fixes and enhancements to 
C<< <bug-test-harness@@rt.cpan.org> >>.

=head1 AUTHORS

Original code by Michael G Schwern, maintained by Andy Lester.

=head1 COPYRIGHT

Copyright 2003 by Michael G Schwern C<< <schwern@@pobox.com> >>,
                  Andy Lester C<< <andy@@petdance.com> >>.

This program is free software; you can redistribute it and/or 
modify it under the same terms as Perl itself.

See L<http://www.perl.com/perl/misc/Artistic.html>.
@


1.1.1.8
log
@Import of stock perl 5.8.5
@
text
@d2 1
a2 1
# $Id: Harness.pm,v 1.8 2004/04/07 21:33:06 millert Exp $
d32 1
a32 1
Version 2.42
d34 1
a34 1
    $Header: /cvs/src/gnu/usr.bin/perl/lib/Test/Harness.pm,v 1.8 2004/04/07 21:33:06 millert Exp $
d38 1
a38 1
$VERSION = '2.42';
a1125 1
The included F<prove> utility for running test scripts from the command line,
@


1.1.1.9
log
@perl 5.8.6 from CPAN
@
text
@d2 1
a2 1
# $Id: Harness.pm,v 1.85 2004/04/29 03:13:43 andy Exp $
d32 1
a32 1
Version 2.42;
d34 1
a34 1
    $Header: /home/cvs/test-harness/lib/Test/Harness.pm,v 1.85 2004/04/29 03:13:43 andy Exp $
@


1.1.1.10
log
@perl 5.8.8 import
@
text
@d2 1
d6 1
a6 1
require 5.00405;
a13 1

d19 1
a21 1
    $Timer
a23 1
    $has_time_hires
a25 5
BEGIN {
    eval "use Time::HiRes 'time'";
    $has_time_hires = !$@@;
}

d32 3
a34 1
Version 2.56
d38 1
a38 1
$VERSION = "2.56";
d45 2
a47 1
$ENV{HARNESS_VERSION} = $VERSION;
a51 1
    delete $ENV{HARNESS_VERSION};
d59 2
a62 2
sub strap { return $Strap };

a71 1
$Timer    = $ENV{HARNESS_TIMER} || 0;
d81 96
a176 13
B<STOP!> If all you want to do is write a test script, consider
using Test::Simple.  Test::Harness is the module that reads the
output from Test::Simple, Test::More and other modules based on
Test::Builder.  You don't need to know about Test::Harness to use
those modules.

Test::Harness runs tests and expects output from the test in a
certain format.  That format is called TAP, the Test Anything
Protocol.  It is defined in L<Test::Harness::TAP>.

C<Test::Harness::runtests(@@tests)> runs all the testscripts named
as arguments and checks standard output for the expected strings
in TAP format.
d178 62
a239 1
The F<prove> utility is a thin wrapper around Test::Harness.
d257 1
a257 1
=item C<$Test::Harness::Verbose>
d259 1
a259 1
The package variable C<$Test::Harness::Verbose> is exportable and can be
d264 1
a264 1
=item C<$Test::Harness::switches>
d266 1
a266 1
The package variable C<$Test::Harness::switches> is exportable and can be
a269 5
=item C<$Test::Harness::Timer>

If set to true, and C<Time::HiRes> is available, print elapsed seconds
after each test file.

d275 2
a276 1
When tests fail, analyze the summary report:
d291 1
a291 1
Everything passed but F<t/waterloo.t>.  It failed 10 of 20 tests and
d341 1
a341 1
This runs all the given I<@@test_files> and divines whether they passed
d346 1
a346 1
It returns true if everything was ok.  Otherwise it will C<die()> with
d349 4
d392 1
a392 1
compatibility on systems where C<glob()> doesn't work right.
d445 1
a445 8
# Turns on autoflush for the handle passed
sub _autoflush {
    my $flushy_fh = shift;
    my $old_fh = select $flushy_fh;
    $| = 1;
    select $old_fh;
}

d447 2
a448 5
    my @@tests = @@_;

    _autoflush(\*STDOUT);
    _autoflush(\*STDERR);

d467 1
a467 1
    my $run_start_time = new Benchmark;
d471 4
a483 4
        if ( $Test::Harness::Debug ) {
            print "# Running: ", $Strap->_command_line($tfile), "\n";
        }
        my $test_start_time = $Timer ? time : 0;
a485 13
        my $elapsed;
        if ( $Timer ) {
            $elapsed = time - $test_start_time;
            if ( $has_time_hires ) {
                $elapsed = sprintf( " %8.3fs", $elapsed );
            }
            else {
                $elapsed = sprintf( " %8ss", $elapsed ? $elapsed : "<1" );
            }
        }
        else {
            $elapsed = "";
        }
a510 1
            # XXX Combine these first two
d517 4
a520 6
                print "$test{ml}ok$elapsed\n        ".join(', ', @@msg)."\n";
            }
            elsif ( $test{max} ) {
                print "$test{ml}ok$elapsed\n";
            }
            elsif ( defined $test{skip_all} and length $test{skip_all} ) {
d523 1
a523 2
            }
            else {
d537 2
a538 1
                foreach my $overrun ($test{max}+1..@@$details) {
d562 1
a562 2
                }
                else {
d575 1
a575 2
            }
            else {
d600 2
a601 2
    } # foreach test
    $tot{bench} = timediff(new Benchmark, $run_start_time);
d612 1
a612 1
Generates the 't/foo........' leader for the given C<$test_file> as well
d626 2
a627 3
    if ($^O eq 'VMS') {
        $te =~ s/^.*\.t\./\[.t./s;
    }
d631 2
a632 3
    if ( -t STDOUT and not $ENV{HARNESS_NOTTY} and not $Verbose ) {
        $ml = "\r" . (' ' x 77) . "\r$leader"
    }
d669 1
a669 2
    }
    elsif (!$tot->{tests}){
d671 1
a671 2
    }
    elsif (!$tot->{max}) {
d675 1
a675 2
    }
    else {
d702 2
a703 8
my %Handlers = (
    header => \&header_handler,
    test => \&test_handler,
    bailout => \&bailout_handler,
);

$Strap->{callback} = \&strap_callback;
sub strap_callback {
d712 1
a712 1
sub header_handler {
d724 1
a724 1
sub test_handler {
d756 1
a756 1
sub bailout_handler {
d769 2
a770 1
# Print updates only once per second.
d772 1
a772 2
    my $now = CORE::time;
    if ( $Last_ML_Print != $now ) {
d774 1
a774 1
        $Last_ML_Print = $now;
d814 8
d900 23
d936 5
a940 1
                push @@canon, ($min == $last) ? $last : "$min-$last";
d948 1
a948 2
    }
    else {
d956 1
a956 2
    }
    else {
d960 1
a961 1
        my $good = $max - $failed - $skipped;
d966 1
a966 2
        }
        else {
d1026 1
a1026 3
=head1 ENVIRONMENT VARIABLES THAT TEST::HARNESS SETS

Test::Harness sets these before executing the individual tests.
d1032 3
a1034 12
This is set to a true value.  It allows the tests to determine if they
are being executed through the harness or by any other means.

=item C<HARNESS_VERSION>

This is the version of Test::Harness.

=back

=head1 ENVIRONMENT VARIABLES THAT AFFECT TEST::HARNESS

=over 4
d1081 6
d1128 2
a1129 1
the underlying timing routines, and L<Devel::Cover> for test coverage
d1132 17
d1165 2
d1193 2
d1197 2
d1208 1
a1208 1
C<< <bug-test-harness >> at C<< rt.cpan.org> >>.
d1212 1
a1212 7
Either Tim Bunce or Andreas Koenig, we don't know. What we know for
sure is, that it was inspired by Larry Wall's TEST script that came
with perl distributions for ages. Numerous anonymous contributors
exist.  Andreas Koenig held the torch for many years, and then
Michael G Schwern.

Current maintainer is Andy Lester C<< <andy at petdance.com> >>.
d1216 2
a1217 3
Copyright 2002-2005
by Michael G Schwern C<< <schwern at pobox.com> >>,
Andy Lester C<< <andy at petdance.com> >>.
@


1.1.1.11
log
@import perl 5.10.0 from CPAN
@
text
@d19 2
a20 1
    $Columns
d28 1
a28 1
    eval q{use Time::HiRes 'time'};
d38 1
a38 1
Version 2.64
d42 1
a42 1
$VERSION = '2.64';
d58 3
d63 1
a63 31
# Stolen from Params::Util
sub _CLASS {
    (defined $_[0] and ! ref $_[0] and $_[0] =~ m/^[^\W\d]\w*(?:::\w+)*$/s) ? $_[0] : undef;
}

# Strap Overloading
if ( $ENV{HARNESS_STRAPS_CLASS} ) {
    die 'Set HARNESS_STRAP_CLASS, singular, not HARNESS_STRAPS_CLASS';
}
my $HARNESS_STRAP_CLASS  = $ENV{HARNESS_STRAP_CLASS} || 'Test::Harness::Straps';
if ( $HARNESS_STRAP_CLASS =~ /\.pm$/ ) {
    # "Class" is actually a filename, that should return the
    # class name as its true return value.
    $HARNESS_STRAP_CLASS = require $HARNESS_STRAP_CLASS;
    if ( !_CLASS($HARNESS_STRAP_CLASS) ) {
        die "HARNESS_STRAP_CLASS '$HARNESS_STRAP_CLASS' is not a valid class name";
    }
}
else {
    # It is a class name within the current @@INC
    if ( !_CLASS($HARNESS_STRAP_CLASS) ) {
        die "HARNESS_STRAP_CLASS '$HARNESS_STRAP_CLASS' is not a valid class name";
    }
    eval "require $HARNESS_STRAP_CLASS";
    die $@@ if $@@;
}
if ( !$HARNESS_STRAP_CLASS->isa('Test::Harness::Straps') ) {
    die "HARNESS_STRAP_CLASS '$HARNESS_STRAP_CLASS' must be a Test::Harness::Straps subclass";
}

$Strap = $HARNESS_STRAP_CLASS->new;
d69 1
a69 1
@@EXPORT_OK = qw(&execute_tests $verbose $switches);
d73 1
a73 1
$Switches = '-w';
d129 1
a129 1
script(s). The default value is C<-w>. It overrides C<HARNESS_PERL_SWITCHES>.
d151 3
a153 3
  Failed Test  Stat Wstat Total Fail  List of Failed
  ---------------------------------------------------------------
  t/waterloo.t    3   768    20   10  1 3 5 7 9 11 13 15 17 19
d183 4
d196 5
a200 1
=head1 FUNCTIONS
d202 1
a202 1
The following functions are available.
d204 1
a204 1
=head2 runtests( @@test_files )
d221 2
a222 2
    my ($tot, $failedtests,$todo_passed) = execute_tests(tests => \@@tests);
    print get_results($tot, $failedtests,$todo_passed);
a228 5
    if (! $ok) {
        die("Failed $tot->{bad}/$tot->{tests} test programs. " .
            "@@{[$tot->{max} - $tot->{ok}]}/$tot->{max} subtests failed.\n");
    }

d232 9
a240 2
# my $ok = _all_ok(\%tot);
# Tells you if this test run is overall successful or not.
d248 3
a250 2
# Returns all the files in a directory.  This is shorthand for backwards
# compatibility on systems where C<glob()> doesn't work right.
d252 2
a253 2
sub _globdir {
    local *DIRH;
d255 6
a260 3
    opendir DIRH, shift;
    my @@f = readdir DIRH;
    closedir DIRH;
d265 3
a267 1
=head2 execute_tests( tests => \@@test_files, out => \*FH )
d269 3
a271 9
Runs all the given C<@@test_files> (just like C<runtests()>) but
doesn't generate the final report.  During testing, progress
information will be written to the currently selected output
filehandle (usually C<STDOUT>), or to the filehandle given by the
C<out> parameter.  The I<out> is optional.

Returns a list of two values, C<$total> and C<$failed>, describing the
results.  C<$total> is a hash ref summary of all the tests run.  Its
keys and values are this:
d288 1
a288 1
C<$failed> is a hash ref of all the test scripts that failed.  Each key
d297 1
d302 2
d306 12
a317 8
sub execute_tests {
    my %args = @@_;
    my @@tests = @@{$args{tests}};
    my $out = $args{out} || select();

    # We allow filehandles that are symbolic refs
    no strict 'refs';
    _autoflush($out);
d320 1
a320 2
    my %failedtests;
    my %todo_passed;
d337 1
a337 2
    my @@dir_files;
    @@dir_files = _globdir $Files_In_Dir if defined $Files_In_Dir;
d346 1
a346 1
        print $out $leader;
d352 1
a352 1
            print $out "# Running: ", $Strap->_command_line($tfile), "\n";
d355 1
a355 1
        my $results = $Strap->analyze_file($tfile) or
d361 1
a361 1
                $elapsed = sprintf( " %8d ms", $elapsed*1000 );
d364 1
a364 1
                $elapsed = sprintf( " %8s s", $elapsed ? $elapsed : "<1" );
d372 2
a373 6
        my @@failed = grep { !$results->details->[$_-1]{ok} }
                     1..@@{$results->details};
        my @@todo_pass = grep { $results->details->[$_-1]{actual_ok} &&
                               $results->details->[$_-1]{type} eq 'todo' }
                        1..@@{$results->details};

d375 16
a390 18
            ok          => $results->ok,
            'next'      => $Strap->{'next'},
            max         => $results->max,
            failed      => \@@failed,
            todo_pass   => \@@todo_pass,
            todo        => $results->todo,
            bonus       => $results->bonus,
            skipped     => $results->skip,
            skip_reason => $results->skip_reason,
            skip_all    => $Strap->{skip_all},
            ml          => $ml,
        );

        $tot{bonus}       += $results->bonus;
        $tot{max}         += $results->max;
        $tot{ok}          += $results->ok;
        $tot{todo}        += $results->todo;
        $tot{sub_skipped} += $results->skip;
d392 1
a392 2
        my $estatus = $results->exit;
        my $wstatus = $results->wait;
d394 1
a394 1
        if ( $results->passing ) {
d400 3
a402 15
                if ($test{bonus}) {
                    my ($txt, $canon) = _canondetail($test{todo},0,'TODO passed',
                                                    @@{$test{todo_pass}});
                    $todo_passed{$tfile} = {
                        canon   => $canon,
                        max     => $test{todo},
                        failed  => $test{bonus},
                        name    => $tfile,
                        estat   => '',
                        wstat   => '',
                    };

                    push(@@msg, "$test{bonus}/$test{max} unexpectedly succeeded\n$txt");
                }
                print $out "$test{ml}ok$elapsed\n        ".join(', ', @@msg)."\n";
d405 1
a405 1
                print $out "$test{ml}ok$elapsed\n";
d408 1
a408 1
                print $out "skipped\n        all skipped: $test{skip_all}\n";
d412 1
a412 1
                print $out "skipped\n        all skipped: no reason given\n";
d424 1
a424 1
                my $details = $results->details;
d436 1
a436 1
            elsif ( $results->seen ) {
d438 1
a438 1
                    my ($txt, $canon) = _canondetail($test{max},$test{skipped},'Failed',
d440 1
a440 1
                    print $out "$test{ml}$txt";
d445 1
d451 1
a451 1
                    print $out "Don't know which tests failed: got $test{ok} ok, ".
d457 1
d465 1
a465 1
                print $out "FAILED before any test output arrived\n";
d471 1
d485 1
a485 1
                print $out "LEAKED FILES: @@f\n";
d494 1
a494 9
    return(\%tot, \%failedtests, \%todo_passed);
}

# Turns on autoflush for the handle passed
sub _autoflush {
    my $flushy_fh = shift;
    my $old_fh = select $flushy_fh;
    $| = 1;
    select $old_fh;
d497 1
a497 1
=for private _mk_leader
d499 1
a499 1
    my($leader, $ml) = _mk_leader($test_file, $width);
d528 1
a528 1
=for private _leader_width
a550 4
sub get_results {
    my $tot = shift;
    my $failedtests = shift;
    my $todo_passed = shift;
d552 2
a553 1
    my $out = '';
d555 1
d559 1
a559 10
        $out .= "All tests successful$bonusmsg.\n";
        if ($tot->{bonus}) {
            my($fmt_top, $fmt) = _create_fmts("Passed TODO",$todo_passed);
            # Now write to formats
            $out .= swrite( $fmt_top );
            for my $script (sort keys %{$todo_passed||{}}) {
                my $Curtest = $todo_passed->{$script};
                $out .= swrite( $fmt, @@{ $Curtest }{qw(name estat wstat max failed canon)} );
            }
        }
d570 5
a574 2
        my $subresults = sprintf( " %d/%d subtests failed.",
                              $tot->{max} - $tot->{ok}, $tot->{max} );
d576 1
a576 1
        my($fmt_top, $fmt1, $fmt2) = _create_fmts("Failed Test",$failedtests);
a578 1
        $out .= swrite( $fmt_top );
d580 2
a581 3
            my $Curtest = $failedtests->{$script};
            $out .= swrite( $fmt1, @@{ $Curtest }{qw(name estat wstat max failed canon)} );
            $out .= swrite( $fmt2, $Curtest->{canon} );
d585 3
a587 2
            $out .= "$bonusmsg.\n" if $bonusmsg;
            $out .= "Failed $tot->{bad}/$tot->{tests} test scripts.$subresults\n";
d591 1
a591 1
    $out .= sprintf("Files=%d, Tests=%d, %s\n",
a592 10
    return $out;
}

sub swrite {
    my $format = shift;
    $^A = '';
    formline($format,@@_);
    my $out = $^A;
    $^A = '';
    return $out;
d597 2
a598 2
    header  => \&header_handler,
    test    => \&test_handler,
d602 1
a602 1
$Strap->set_callback(\&strap_callback);
d620 2
a621 1
      if $totals->seen && ($totals->max < $totals->seen);
d627 1
a627 1
    my $curr = $totals->seen;
d629 2
a630 2
    my $max  = $totals->max;
    my $detail = $totals->details->[-1];
d636 4
a639 4
            $totals->set_skip_reason( $detail->{reason} )
              unless defined $totals->skip_reason;
            $totals->set_skip_reason( 'various reasons' )
              if $totals->skip_reason ne $detail->{reason};
d643 1
a643 1
        _print_ml("NOK $curr/$max");
d700 1
d707 1
a707 3

    my $failed = '??';
    my $canon  = '??';
d719 1
d725 2
a726 1
            (my $txt, $canon) = _canondetail($test->{max},$test->{skipped},'Failed',@@{$test->{failed}});
d733 1
d740 1
a740 2
    my $failed_str = shift;
    my $failedtests = shift;
d742 3
a744 5
    my ($type) = split /\s/,$failed_str;
    my $short = substr($type,0,4);
    my $total = $short eq 'Pass' ? 'TODOs' : 'Total';
    my $middle_str = " Stat Wstat $total $short  ";
    my $list_str = "List of $type";
d763 2
a764 1
    my $fmt_top =   sprintf("%-${max_namelen}s", $failed_str)
d768 1
a768 1
                  . "\n";
d770 13
a782 5
    my $fmt1 =  "@@" . "<" x ($max_namelen - 1)
              . "  @@>> @@>>>> @@>>>> @@>>>  "
              . "^" . "<" x ($list_len - 1) . "\n";
    my $fmt2 =  "~~" . " " x ($Columns - $list_len - 2) . "^"
              . "<" x ($list_len - 1) . "\n";
d784 6
a789 1
    return($fmt_top, $fmt1, $fmt2);
d792 2
a793 5
sub _canondetail {
    my $max = shift;
    my $skipped = shift;
    my $type = shift;
    my @@detail = @@_;
d795 2
a796 2
    @@detail = sort {$a <=> $b} grep !$seen{$_}++, @@detail;
    my $detail = @@detail;
d800 1
a800 1
    my $last = $min = shift @@detail;
d802 2
a803 3
    my $uc_type = uc($type);
    if (@@detail) {
        for (@@detail, $detail[-1]) { # don't forget the last one
d811 1
a811 1
        push @@result, "$uc_type tests @@canon\n";
d815 1
a815 1
        push @@result, "$uc_type test $last\n";
d819 1
a819 3
    return (join("", @@result), $canon)
        if $type=~/todo/i;
    push @@result, "\t$type $detail/$max tests, ";
d821 1
a821 1
	push @@result, sprintf("%.2f",100*(1-$detail/$max)), "% okay";
d828 1
a828 1
        my $good = $max - $detail - $skipped;
d841 1
a841 1
    return ($txt, $canon);
d844 7
d859 1
a859 2
C<&execute_tests>, C<$verbose>, C<$switches> and C<$debug> are
exported upon request.
d948 4
a973 6
=item C<HARNESS_TIMER>

Setting this to true will make the harness display the number of
milliseconds each test took.  You can also use F<prove>'s C<--timer>
switch.

a979 15
If true, Test::Harness will output the verbose results of running
its tests.  Setting C<$Test::Harness::verbose> will override this,
or you can use the C<-v> switch in the F<prove> utility.

=item C<HARNESS_STRAP_CLASS>

Defines the Test::Harness::Straps subclass to use.  The value may either
be a filename or a class name.

If HARNESS_STRAP_CLASS is a class name, the class must be in C<@@INC>
like any other class.

If HARNESS_STRAP_CLASS is a filename, the .pm file must return the name
of the class, instead of the canonical "1".

d1030 2
d1036 2
d1052 2
a1053 39
Please report any bugs or feature requests to
C<bug-test-harness at rt.cpan.org>, or through the web interface at
L<http://rt.cpan.org/NoAuth/ReportBug.html?Queue=Test-Harness>.
I will be notified, and then you'll automatically be notified of progress on
your bug as I make changes.

=head1 SUPPORT

You can find documentation for this module with the F<perldoc> command.

    perldoc Test::Harness

You can get docs for F<prove> with

    prove --man

You can also look for information at:

=over 4

=item * AnnoCPAN: Annotated CPAN documentation

L<http://annocpan.org/dist/Test-Harness>

=item * CPAN Ratings

L<http://cpanratings.perl.org/d/Test-Harness>

=item * RT: CPAN's request tracker

L<http://rt.cpan.org/NoAuth/Bugs.html?Dist=Test-Harness>

=item * Search CPAN

L<http://search.cpan.org/dist/Test-Harness>

=back

=head1 SOURCE CODE
d1055 3
a1057 2
The source code repository for Test::Harness is at
L<http://svn.perl.org/modules/Test-Harness>.
d1062 1
a1062 1
sure is, that it was inspired by Larry Wall's F<TEST> script that came
d1071 1
a1071 1
Copyright 2002-2006
@


