head	1.38;
access;
symbols
	OPENBSD_6_2:1.38.0.4
	OPENBSD_6_2_BASE:1.38
	OPENBSD_6_1:1.37.0.4
	OPENBSD_6_1_BASE:1.37
	OPENBSD_6_0:1.36.0.2
	OPENBSD_6_0_BASE:1.36
	OPENBSD_5_9:1.34.0.2
	OPENBSD_5_9_BASE:1.34
	OPENBSD_5_8:1.34.0.4
	OPENBSD_5_8_BASE:1.34
	OPENBSD_5_7:1.33.0.2
	OPENBSD_5_7_BASE:1.33
	OPENBSD_5_6:1.29.0.4
	OPENBSD_5_6_BASE:1.29
	OPENBSD_5_5:1.28.0.6
	OPENBSD_5_5_BASE:1.28
	OPENBSD_5_4:1.28.0.2
	OPENBSD_5_4_BASE:1.28
	OPENBSD_5_3:1.23.0.2
	OPENBSD_5_3_BASE:1.23
	OPENBSD_5_2:1.18.0.4
	OPENBSD_5_2_BASE:1.18
	OPENBSD_5_1_BASE:1.18
	OPENBSD_5_1:1.18.0.2
	OPENBSD_5_0:1.17.0.2
	OPENBSD_5_0_BASE:1.17
	OPENBSD_4_9:1.14.0.4
	OPENBSD_4_9_BASE:1.14
	OPENBSD_4_8:1.14.0.2
	OPENBSD_4_8_BASE:1.14
	OPENBSD_4_7:1.12.0.2
	OPENBSD_4_7_BASE:1.12
	OPENBSD_4_6:1.9.0.4
	OPENBSD_4_6_BASE:1.9
	OPENBSD_4_5:1.4.0.2
	OPENBSD_4_5_BASE:1.4
	OPENBSD_4_4:1.3.0.2
	OPENBSD_4_4_BASE:1.3;
locks; strict;
comment	@ * @;


1.38
date	2017.04.16.14.25.42;	author beck;	state Exp;
branches;
next	1.37;
commitid	MPqr2iIfn6hJVsVl;

1.37
date	2016.09.17.19.33.59;	author guenther;	state Exp;
branches;
next	1.36;
commitid	sM6KB4Vklm3bb3xC;

1.36
date	2016.04.28.13.13.02;	author beck;	state Exp;
branches;
next	1.35;
commitid	dGFWuxQKEgKClDgA;

1.35
date	2016.03.17.03.57.51;	author beck;	state Exp;
branches;
next	1.34;
commitid	UsjcESyd1wIYVdEr;

1.34
date	2015.07.19.21.21.14;	author beck;	state Exp;
branches;
next	1.33;
commitid	5wyRonomeQfnkJsF;

1.33
date	2014.12.17.06.58.11;	author guenther;	state Exp;
branches;
next	1.32;
commitid	DImukoCWyTxwdbuh;

1.32
date	2014.12.15.02.24.23;	author guenther;	state Exp;
branches;
next	1.31;
commitid	ZxaujiOM0aYQRjFY;

1.31
date	2014.11.16.12.31.00;	author deraadt;	state Exp;
branches;
next	1.30;
commitid	yv0ECmCdICvq576h;

1.30
date	2014.08.11.19.16.56;	author miod;	state Exp;
branches;
next	1.29;
commitid	y9v8aJI2WklqBsm8;

1.29
date	2014.03.28.17.57.11;	author mpi;	state Exp;
branches;
next	1.28;

1.28
date	2013.07.09.15.37.43;	author beck;	state Exp;
branches;
next	1.27;

1.27
date	2013.06.27.00.04.16;	author beck;	state Exp;
branches;
next	1.26;

1.26
date	2013.06.13.15.00.04;	author tedu;	state Exp;
branches;
next	1.25;

1.25
date	2013.06.11.21.51.55;	author tedu;	state Exp;
branches;
next	1.24;

1.24
date	2013.06.11.19.01.20;	author beck;	state Exp;
branches;
next	1.23;

1.23
date	2013.01.18.10.07.37;	author beck;	state Exp;
branches;
next	1.22;

1.22
date	2013.01.18.08.52.04;	author beck;	state Exp;
branches;
next	1.21;

1.21
date	2012.12.02.19.42.36;	author beck;	state Exp;
branches;
next	1.20;

1.20
date	2012.11.18.16.56.41;	author beck;	state Exp;
branches;
next	1.19;

1.19
date	2012.11.17.23.08.22;	author beck;	state Exp;
branches;
next	1.18;

1.18
date	2011.09.19.14.48.04;	author beck;	state Exp;
branches;
next	1.17;

1.17
date	2011.04.07.19.07.42;	author beck;	state Exp;
branches;
next	1.16;

1.16
date	2011.04.05.21.31.58;	author beck;	state Exp;
branches;
next	1.15;

1.15
date	2011.04.02.16.47.17;	author beck;	state Exp;
branches;
next	1.14;

1.14
date	2010.04.30.21.56.39;	author oga;	state Exp;
branches;
next	1.13;

1.13
date	2010.04.21.03.04.04;	author deraadt;	state Exp;
branches;
next	1.12;

1.12
date	2009.08.09.17.45.02;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2009.08.09.14.37.46;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2009.08.06.15.28.14;	author oga;	state Exp;
branches;
next	1.9;

1.9
date	2009.06.25.15.49.26;	author thib;	state Exp;
branches;
next	1.8;

1.8
date	2009.06.16.00.11.29;	author oga;	state Exp;
branches;
next	1.7;

1.7
date	2009.06.06.18.06.22;	author art;	state Exp;
branches;
next	1.6;

1.6
date	2009.06.02.23.00.19;	author oga;	state Exp;
branches;
next	1.5;

1.5
date	2009.04.22.13.12.26;	author art;	state Exp;
branches;
next	1.4;

1.4
date	2008.11.08.23.20.50;	author pedro;	state Exp;
branches;
next	1.3;

1.3
date	2008.08.03.18.08.54;	author kettenis;	state Exp;
branches;
next	1.2;

1.2
date	2008.06.11.00.11.03;	author thib;	state Exp;
branches;
next	1.1;

1.1
date	2008.06.10.20.14.36;	author beck;	state Exp;
branches;
next	;


desc
@@


1.38
log
@Flip previously warm pages in the buffer cache to memory above the DMA
region if uvm tells us it is available.
nits from deraadt@@
ok krw@@ guenther@@ kettenis@@
@
text
@/*	$OpenBSD: vfs_biomem.c,v 1.37 2016/09/17 19:33:59 guenther Exp $ */

/*
 * Copyright (c) 2007 Artur Grabowski <art@@openbsd.org>
 * Copyright (c) 2012-2016 Bob Beck <beck@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */


#include <sys/param.h>
#include <sys/systm.h>
#include <sys/buf.h>
#include <sys/pool.h>
#include <sys/proc.h>		/* XXX for atomic */
#include <sys/mount.h>

#include <uvm/uvm_extern.h>

vaddr_t buf_kva_start, buf_kva_end;
int buf_needva;
TAILQ_HEAD(,buf) buf_valist;

extern struct bcachestats bcstats;

/*
 * Pages are allocated from a uvm object (we only use it for page storage,
 * all pages are wired). Since every buffer contains a contiguous range of
 * pages, reusing the pages could be very painful. Fortunately voff_t is
 * 64 bits, so we can just increment buf_page_offset all the time and ignore
 * wraparound. Even if you reuse 4GB worth of buffers every second
 * you'll still run out of time_t faster than buffers.
 *
 */
voff_t buf_page_offset;
struct uvm_object *buf_object, buf_object_store;

vaddr_t buf_unmap(struct buf *);

void
buf_mem_init(vsize_t size)
{
	TAILQ_INIT(&buf_valist);

	buf_kva_start = vm_map_min(kernel_map);
	if (uvm_map(kernel_map, &buf_kva_start, size, NULL,
	    UVM_UNKNOWN_OFFSET, PAGE_SIZE, UVM_MAPFLAG(PROT_NONE,
	    PROT_NONE, MAP_INHERIT_NONE, MADV_NORMAL, 0)))
		panic("bufinit: can't reserve VM for buffers");
	buf_kva_end = buf_kva_start + size;

	/* Contiguous mapping */
	bcstats.kvaslots = bcstats.kvaslots_avail = size / MAXPHYS;

	buf_object = &buf_object_store;

	uvm_objinit(buf_object, NULL, 1);
}

/*
 * buf_acquire and buf_release manage the kvm mappings of buffers.
 */
void
buf_acquire(struct buf *bp)
{
	KASSERT((bp->b_flags & B_BUSY) == 0);
	splassert(IPL_BIO);
	/*
	 * Busy before waiting for kvm.
	 */
	SET(bp->b_flags, B_BUSY);
	buf_map(bp);
}

/*
 * Acquire a buf but do not map it. Preserve any mapping it did have.
 */
void
buf_acquire_nomap(struct buf *bp)
{
	splassert(IPL_BIO);
	SET(bp->b_flags, B_BUSY);
	if (bp->b_data != NULL) {
		TAILQ_REMOVE(&buf_valist, bp, b_valist);
		bcstats.kvaslots_avail--;
		bcstats.busymapped++;
	}
}

void
buf_map(struct buf *bp)
{
	vaddr_t va;

	splassert(IPL_BIO);

	if (bp->b_data == NULL) {
		unsigned long i;

		/*
		 * First, just use the pre-allocated space until we run out.
		 */
		if (buf_kva_start < buf_kva_end) {
			va = buf_kva_start;
			buf_kva_start += MAXPHYS;
			bcstats.kvaslots_avail--;
		} else {
			struct buf *vbp;

			/*
			 * Find some buffer we can steal the space from.
			 */
			vbp = TAILQ_FIRST(&buf_valist);
			while ((curproc != syncerproc &&
			   curproc != cleanerproc &&
			   bcstats.kvaslots_avail <= RESERVE_SLOTS) ||
			   vbp == NULL) {
				buf_needva++;
				tsleep(&buf_needva, PRIBIO, "buf_needva", 0);
				vbp = TAILQ_FIRST(&buf_valist);
			}
			va = buf_unmap(vbp);
		}

		for (i = 0; i < atop(bp->b_bufsize); i++) {
			struct vm_page *pg = uvm_pagelookup(bp->b_pobj,
			    bp->b_poffs + ptoa(i));

			KASSERT(pg != NULL);

			pmap_kenter_pa(va + ptoa(i), VM_PAGE_TO_PHYS(pg),
			    PROT_READ | PROT_WRITE);
		}
		pmap_update(pmap_kernel());
		bp->b_data = (caddr_t)va;
	} else {
		TAILQ_REMOVE(&buf_valist, bp, b_valist);
		bcstats.kvaslots_avail--;
	}

	bcstats.busymapped++;
}

void
buf_release(struct buf *bp)
{

	KASSERT(bp->b_flags & B_BUSY);
	splassert(IPL_BIO);

	if (bp->b_data) {
		bcstats.busymapped--;
		TAILQ_INSERT_TAIL(&buf_valist, bp, b_valist);
		bcstats.kvaslots_avail++;
		if (buf_needva) {
			buf_needva=0;
			wakeup(&buf_needva);
		}
	}
	CLR(bp->b_flags, B_BUSY);
}

/*
 * Deallocate all memory resources for this buffer. We need to be careful
 * to not drop kvm since we have no way to reclaim it. So, if the buffer
 * has kvm, we need to free it later. We put it on the front of the
 * freelist just so it gets picked up faster.
 *
 * Also, lots of assertions count on bp->b_data being NULL, so we
 * set it temporarily to NULL.
 *
 * Return non-zero if we take care of the freeing later.
 */
int
buf_dealloc_mem(struct buf *bp)
{
	caddr_t data;

	splassert(IPL_BIO);

	data = bp->b_data;
	bp->b_data = NULL;

	if (data) {
		if (bp->b_flags & B_BUSY)
			bcstats.busymapped--;
		pmap_kremove((vaddr_t)data, bp->b_bufsize);
		pmap_update(pmap_kernel());
	}

	if (bp->b_pobj)
		buf_free_pages(bp);

	if (data == NULL)
		return (0);

	bp->b_data = data;
	if (!(bp->b_flags & B_BUSY)) {		/* XXX - need better test */
		TAILQ_REMOVE(&buf_valist, bp, b_valist);
		bcstats.kvaslots_avail--;
	} else {
		CLR(bp->b_flags, B_BUSY);
		if (buf_needva) {
			buf_needva = 0;
			wakeup(&buf_needva);
		}
	}
	SET(bp->b_flags, B_RELEASED);
	TAILQ_INSERT_HEAD(&buf_valist, bp, b_valist);
	bcstats.kvaslots_avail++;

	return (1);
}

/*
 * Only used by bread_cluster.
 */
void
buf_fix_mapping(struct buf *bp, vsize_t newsize)
{
	vaddr_t va = (vaddr_t)bp->b_data;

	if (newsize < bp->b_bufsize) {
		pmap_kremove(va + newsize, bp->b_bufsize - newsize);
		pmap_update(pmap_kernel());
		/*
		 * Note: the size we lost is actually with the other
		 * buffers read in by bread_cluster
		 */
		bp->b_bufsize = newsize;
	}
}

vaddr_t
buf_unmap(struct buf *bp)
{
	vaddr_t va;

	KASSERT((bp->b_flags & B_BUSY) == 0);
	KASSERT(bp->b_data != NULL);
	splassert(IPL_BIO);

	TAILQ_REMOVE(&buf_valist, bp, b_valist);
	bcstats.kvaslots_avail--;
	va = (vaddr_t)bp->b_data;
	bp->b_data = 0;
	pmap_kremove(va, bp->b_bufsize);
	pmap_update(pmap_kernel());

	if (bp->b_flags & B_RELEASED)
		pool_put(&bufpool, bp);

	return (va);
}

/* Always allocates in dma-reachable memory */
void
buf_alloc_pages(struct buf *bp, vsize_t size)
{
	voff_t offs;
	int i;

	KASSERT(size == round_page(size));
	KASSERT(bp->b_pobj == NULL);
	KASSERT(bp->b_data == NULL);
	splassert(IPL_BIO);

	offs = buf_page_offset;
	buf_page_offset += size;

	KASSERT(buf_page_offset > 0);

	/*
	 * Attempt to allocate with NOWAIT. if we can't, then throw
	 * away some clean pages and try again. Finally, if that
	 * fails, do a WAITOK allocation so the page daemon can find
	 * memory for us.
	 */
	do {
		i = uvm_pagealloc_multi(buf_object, offs, size,
		    UVM_PLA_NOWAIT);
		if (i == 0)
			break;
	} while	(bufbackoff(&dma_constraint, 100) == 0);
	if (i != 0)
		i = uvm_pagealloc_multi(buf_object, offs, size,
		    UVM_PLA_WAITOK);
	/* should not happen */
	if (i != 0)
		panic("uvm_pagealloc_multi unable to allocate an buf_object of size %lu", size);
	bcstats.numbufpages += atop(size);
	bcstats.dmapages += atop(size);
	SET(bp->b_flags, B_DMA);
	bp->b_pobj = buf_object;
	bp->b_poffs = offs;
	bp->b_bufsize = size;
}

void
buf_free_pages(struct buf *bp)
{
	struct uvm_object *uobj = bp->b_pobj;
	struct vm_page *pg;
	voff_t off, i;

	KASSERT(bp->b_data == NULL);
	KASSERT(uobj != NULL);
	splassert(IPL_BIO);

	off = bp->b_poffs;
	bp->b_pobj = NULL;
	bp->b_poffs = 0;

	for (i = 0; i < atop(bp->b_bufsize); i++) {
		pg = uvm_pagelookup(uobj, off + ptoa(i));
		KASSERT(pg != NULL);
		KASSERT(pg->wire_count == 1);
		pg->wire_count = 0;
		uvm_pagefree(pg);
		bcstats.numbufpages--;
		if (ISSET(bp->b_flags, B_DMA))
			bcstats.dmapages--;
	}
	CLR(bp->b_flags, B_DMA);
}

/* Reallocate a buf into a particular pmem range specified by "where". */
int
buf_realloc_pages(struct buf *bp, struct uvm_constraint_range *where,
    int flags)
{
	vaddr_t va;
	int dma;
  	int i, r;
	KASSERT(!(flags & UVM_PLA_WAITOK) ^ !(flags & UVM_PLA_NOWAIT));

	splassert(IPL_BIO);
	KASSERT(ISSET(bp->b_flags, B_BUSY));
	dma = ISSET(bp->b_flags, B_DMA);

	/* if the original buf is mapped, unmap it */
	if (bp->b_data != NULL) {
		va = (vaddr_t)bp->b_data;
		pmap_kremove(va, bp->b_bufsize);
		pmap_update(pmap_kernel());
	}

	do {
		r = uvm_pagerealloc_multi(bp->b_pobj, bp->b_poffs,
		    bp->b_bufsize, UVM_PLA_NOWAIT, where);
		if (r == 0)
			break;
	} while	((bufbackoff(where, atop(bp->b_bufsize)) == 0));

	/*
	 * bufbackoff() failed, so there's no more we can do without
	 * waiting.  If allowed do, make that attempt.
	 */
	if (r != 0 && (flags & UVM_PLA_WAITOK))
		r = uvm_pagerealloc_multi(bp->b_pobj, bp->b_poffs,
		    bp->b_bufsize, flags, where);

	/*
	 * If the allocation has succeeded, we may be somewhere different.
	 * If the allocation has failed, we are in the same place.
	 *
	 * We still have to re-map the buffer before returning.
	 */

	/* take it out of dma stats until we know where we are */
	if (dma)
		bcstats.dmapages -= atop(bp->b_bufsize);

	dma = 1;
	/* if the original buf was mapped, re-map it */
	for (i = 0; i < atop(bp->b_bufsize); i++) {
		struct vm_page *pg = uvm_pagelookup(bp->b_pobj,
		    bp->b_poffs + ptoa(i));
		KASSERT(pg != NULL);
		if  (!PADDR_IS_DMA_REACHABLE(VM_PAGE_TO_PHYS(pg)))
			dma = 0;
		if (bp->b_data != NULL) {
			pmap_kenter_pa(va + ptoa(i), VM_PAGE_TO_PHYS(pg),
			    PROT_READ|PROT_WRITE);
			pmap_update(pmap_kernel());
		}
	}
	if (dma) {
		SET(bp->b_flags, B_DMA);
		bcstats.dmapages += atop(bp->b_bufsize);
	} else
		CLR(bp->b_flags, B_DMA);
	return(r);
}
@


1.37
log
@Make the flag tests consistent in buf_realloc_pages() and explain what's
going on more clearly

ok beck@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.36 2016/04/28 13:13:02 beck Exp $ */
d363 1
a363 1
	} while	((bufbackoff(where, 100) == 0) && (flags & UVM_PLA_WAITOK));
@


1.36
log
@fix logic bug in deciding if we have UVM_PLA_NOWAIT or not in buf_realloc_pages
noticed by miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.35 2016/03/17 03:57:51 beck Exp $ */
d364 6
a369 1
	if (r != 0 && !(flags & UVM_PLA_NOWAIT))
@


1.35
log
@- add realloc_pages to move a buffer's physmem from one range to another.
- modify B_DMA handling to be in vfs_biomem.c
- change buffer allocation to try allocations with NOWAIT and to throw away clean pages
  if allocation fails - allocate with WAITOK only if you can't throw away enough pages to
  succeed
"probably sound" deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.34 2015/07/19 21:21:14 beck Exp $ */
d364 1
a364 1
	if (r != 0 && (! flags & UVM_PLA_NOWAIT))
@


1.34
log
@Change uvm_page[re]alloc_multi to actually use the flags passed in, and return
a value so that they may be called with UVM_PLA_NOWAIT
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.33 2014/12/17 06:58:11 guenther Exp $ */
d5 1
d226 1
a226 1
 * Only used by bread_cluster. 
d271 1
d283 18
a300 1
	(void) uvm_pagealloc_multi(buf_object, offs, size, UVM_PLA_WAITOK);
d302 2
d331 2
d334 1
d337 63
a399 4
/*
 * XXX - it might make sense to make a buf_realloc_pages to avoid
 *       bouncing through the free list all the time.
 */
@


1.33
log
@Prefer MADV_* over POSIX_MADV_* in kernel for consistency: the latter
doesn't have all the values and therefore can't be used everywhere.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.32 2014/12/15 02:24:23 guenther Exp $ */
d281 1
a281 1
	uvm_pagealloc_multi(buf_object, offs, size, UVM_PLA_WAITOK);
@


1.32
log
@Use MAP_INHERIT_* for the 'inh' argument to the UMV_MAPFLAG() macro,
eliminating the must-be-kept-in-sync UVM_INH_* macros

ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.31 2014/11/16 12:31:00 deraadt Exp $ */
d57 1
a57 1
	    PROT_NONE, MAP_INHERIT_NONE, POSIX_MADV_NORMAL, 0)))
@


1.31
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.30 2014/08/11 19:16:56 miod Exp $ */
d57 1
a57 1
	    PROT_NONE, UVM_INH_NONE, POSIX_MADV_NORMAL, 0)))
@


1.30
log
@Do not pmap_update(pmap_kernel()) within the `create mappings' loop, but only
once after it is over.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.29 2014/03/28 17:57:11 mpi Exp $ */
d56 2
a57 2
	    UVM_UNKNOWN_OFFSET, PAGE_SIZE, UVM_MAPFLAG(UVM_PROT_NONE,
	    UVM_PROT_NONE, UVM_INH_NONE, UVM_ADV_NORMAL, 0)))
d141 1
a141 1
			    VM_PROT_READ|VM_PROT_WRITE);
@


1.29
log
@Reduce uvm include madness.  Use <uvm/uvm_extern.h> instead of
<uvm/uvm.h> if possible and remove double inclusions.

ok beck@@, mlarkin@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.28 2013/07/09 15:37:43 beck Exp $ */
a141 1
			pmap_update(pmap_kernel());
d143 1
@


1.28
log
@back out the cache flipper temporarily to work out of tree.
will come back soon.
ok deraadt@@
@
text
@d1 2
a2 1
/*	$OpenBSD: vfs_biomem.c,v 1.23 2013/01/18 10:07:37 beck Exp $ */
a27 1
#include <uvm/uvm.h>
@


1.27
log
@B_WANTED is set when we want a B_BUSY buffer - Therefore we should always
check and awaken B_WANTED sleepers when clearing B_BUSY.
ok guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.26 2013/06/13 15:00:04 tedu Exp $ */
a3 1
 * Copyright (c) 2012,2013 Bob Beck <beck@@openbsd.org>
a169 4
	if (ISSET(bp->b_flags, B_WANTED)) {
		CLR(bp->b_flags, B_WANTED);
		wakeup(bp);
	}
a269 1
	int i;
d281 1
a281 9
	do {
		i = uvm_pagealloc_multi(buf_object, offs, size,
		    UVM_PLA_NOWAIT);
		if (i == 0)
			break;
	} while	(bufbackoff(&dma_constraint, 100) == 0);
	if (i != 0)
		i = uvm_pagealloc_multi(buf_object, offs, size,
		    UVM_PLA_WAITOK);
a282 2
	bcstats.dmapages += atop(size);
	SET(bp->b_flags, B_DMA);
a309 2
		if (ISSET(bp->b_flags, B_DMA))
			bcstats.dmapages--;
a310 1
	CLR(bp->b_flags, B_DMA);
d313 4
a316 59
/* Reallocate a buf into a particular pmem range specified by "where". */
int
buf_realloc_pages(struct buf *bp, struct uvm_constraint_range *where,
    int flags)
{
	vaddr_t va;
	int dma;
  	int i, r;
	KASSERT(!(flags & UVM_PLA_WAITOK) ^ !(flags & UVM_PLA_NOWAIT));

	splassert(IPL_BIO);
	KASSERT(ISSET(bp->b_flags, B_BUSY));
	dma = ISSET(bp->b_flags, B_DMA);

	/* if the original buf is mapped, unmap it */
	if (bp->b_data != NULL) {
		va = (vaddr_t)bp->b_data;
		pmap_kremove(va, bp->b_bufsize);
		pmap_update(pmap_kernel());
	}

	r = 0;
	do {
		r = uvm_pagerealloc_multi(bp->b_pobj, bp->b_poffs,
		    bp->b_bufsize, UVM_PLA_NOWAIT, where);
		if (r == 0)
			break;
	} while	((bufbackoff(where, 100) == 0) && (flags & UVM_PLA_WAITOK));
	if (r != 0 && (! flags & UVM_PLA_NOWAIT))
		r = uvm_pagerealloc_multi(bp->b_pobj, bp->b_poffs,
		    bp->b_bufsize, flags, where);

	/*
	 * do this now, and put it back later when we know where we are
	 */
	if (dma)
		bcstats.dmapages -= atop(bp->b_bufsize);

	dma = 1;
	/* if the original buf was mapped, re-map it */
	for (i = 0; i < atop(bp->b_bufsize); i++) {
		struct vm_page *pg = uvm_pagelookup(bp->b_pobj,
		    bp->b_poffs + ptoa(i));
		KASSERT(pg != NULL);
		if  (!PADDR_IS_DMA_REACHABLE(VM_PAGE_TO_PHYS(pg)))
			dma = 0;
		if (bp->b_data != NULL) {
			pmap_kenter_pa(va + ptoa(i), VM_PAGE_TO_PHYS(pg),
			    VM_PROT_READ|VM_PROT_WRITE);
			pmap_update(pmap_kernel());
		}
	}
	if (dma) {
		SET(bp->b_flags, B_DMA);
		bcstats.dmapages += atop(bp->b_bufsize);
	} else
		CLR(bp->b_flags, B_DMA);
	return(r);
}
@


1.26
log
@beck would prefer to keep things just as they were for a while longer.
undo style changes.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.25 2013/06/11 21:51:55 tedu Exp $ */
d171 4
@


1.25
log
@sprinkle knf fairy dust over new buf code
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.24 2013/06/11 19:01:20 beck Exp $ */
a335 1

@


1.24
log
@High memory page flipping for the buffer cache.

This change splits the buffer cache free lists into lists of dma reachable
buffers and high memory buffers based on the ranges returned by pmemrange.
Buffers move from dma to high memory as they age, but are flipped to dma
reachable memory if IO is needed to/from and high mem buffer. The total
amount of buffers  allocated is now bufcachepercent of both the dma and
the high memory region.

This change allows the use of large buffer caches on amd64 using more than
4 GB of memory

ok tedu@@ krw@@ - testing by many.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.23 2013/01/18 10:07:37 beck Exp $ */
d336 1
@


1.23
log
@oops. this kassert should have gone with the B_NOTMAPPED commit
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.22 2013/01/18 08:52:04 beck Exp $ */
d4 1
d271 1
d283 9
a291 1
	uvm_pagealloc_multi(buf_object, offs, size, UVM_PLA_WAITOK);
d293 2
d322 2
d325 1
d328 59
a386 4
/*
 * XXX - it might make sense to make a buf_realloc_pages to avoid
 *       bouncing through the free list all the time.
 */
@


1.22
log
@Give buf_acquire_unmapped and B_NOTMAPPED a viking funeral as they should
really have been called "maybemapped and hope it all works out". - use
buf_acquire_nomap instead which acounts for busymapped bufs correctly.

ok krw@@ guenther@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.21 2012/12/02 19:42:36 beck Exp $ */
a157 1
	KASSERT(bp->b_data != NULL);
@


1.21
log
@Fix kva reserve - ensure that kva reserve is checked for, as well
as fix the case where buffers can be returned on the vinvalbuf path
and we do not get woken up when waiting for kva.

An earlier version looked at and ok'd by guenther@@ in coimbra. - helpful
comments from kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.20 2012/11/18 16:56:41 beck Exp $ */
a84 12
 * Busy a buffer, but don't map it.
 * If it has a mapping, we keep it, but we also keep the mapping on
 * the list since we assume that it won't be used anymore.
 */
void
buf_acquire_unmapped(struct buf *bp)
{
	splassert(IPL_BIO);
	SET(bp->b_flags, B_BUSY|B_NOTMAPPED);
}

/*
d92 1
a92 3
	if (bp->b_data == NULL)
		SET(bp->b_flags, B_NOTMAPPED);
	else {
a150 2

	CLR(bp->b_flags, B_NOTMAPPED);
d158 1
a158 1
	KASSERT((bp->b_data != NULL) || (bp->b_flags & B_NOTMAPPED));
d170 1
a170 1
	CLR(bp->b_flags, B_BUSY|B_NOTMAPPED);
@


1.20
log
@These functions all should be called with splbio, so splassert(IPL_BIO)
everywhere instead of setting splbio.
ok krw@@ pirofti@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.19 2012/11/17 23:08:22 beck Exp $ */
a32 2
int buf_nkvmsleep;

d136 5
a140 1
			while ((vbp = TAILQ_FIRST(&buf_valist)) == NULL) {
a141 1
				buf_nkvmsleep++;
d143 1
d182 2
a183 2
			buf_needva--;
			wakeup_one(&buf_needva);
d227 1
a227 1
	} else
d229 5
@


1.19
log
@
 Don't map a buffer (and potentially sleep) when invalidating it in vinvalbuf.
This fixes a problem where we could sleep for kva and then our pointers
would not be valid on the next pass through the loop. We do this
by adding buf_acquire_nomap() - which can be used to busy up the buffer
without changing its mapped or unmapped state. We do not need to have
the buffer mapped to invalidate it, so it is sufficient to acquire it
for that. In the case where we write the buffer, we do map the buffer, and
potentially sleep.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.18 2011/09/19 14:48:04 beck Exp $ */
a44 2
 * XXX - the spl locking in here is extreme paranoia right now until I figure
 *       it all out.
a76 2
	int s;

d78 1
a78 2

	s = splbio();
a83 2

	splx(s);
d94 1
a94 3
	int s;

	s = splbio();
a95 1
	splx(s);
a169 1
	int s;
d173 1
a174 1
	s = splbio();
a184 1
	splx(s);
a201 1
	int s;
d203 1
a203 1
	s = splbio();
d218 1
a218 2
	if (data == NULL) {
		splx(s);
a219 1
	}
a229 1
	splx(s);
a256 1
	int s;
d260 1
a261 1
	s = splbio();
a271 2
	splx(s);

a279 1
	int s;
d284 1
a284 2

	s = splbio();
a295 1
	splx(s);
a303 1
	int s;
d307 1
a307 2

	s = splbio();
a320 1
	splx(s);
@


1.18
log
@clean up buffer cache statistics somewhat to
remove some now useless statistics, and add some
relevant ones regarding kva usage in the cache.

make systat io and show bcstats in ddb both show
these counters.

ok deraadt@@ krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.17 2011/04/07 19:07:42 beck Exp $ */
d106 17
@


1.17
log
@Revert previous diff decrementing bcstats.numbufpages here. This function
does not do what it purports to do, it shrinks mapping, not allocation, as
the pages have already been given away to other buffers. This also renames
the function to make this a little more obvious

and art should not name funcitons

ok thib@@, art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.16 2011/04/05 21:31:58 beck Exp $ */
d65 3
d124 1
d152 1
d172 1
d220 1
a220 1
	if (!(bp->b_flags & B_BUSY))		/* XXX - need better test */
d222 2
a223 1
	else
d227 1
a227 1

d263 1
@


1.16
log
@fix nasty buffer cache bug where we could forget about pages shrunk off of a
buffer as the result of bread_cluster.
ok art@@ thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.15 2011/04/02 16:47:17 beck Exp $ */
d226 3
d230 1
a230 1
buf_shrink_mem(struct buf *bp, vsize_t newsize)
d237 4
a240 1
		bcstats.numbufpages -= atop(bp->b_bufsize - newsize);
@


1.15
log
@Constrain the buffer cache to use only the dma reachable region of memory.
With this change bufcachepercent will be the percentage of dma reachable
memory that the buffer cache will attempt to use.
ok deraadt@@ thib@@ oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.14 2010/04/30 21:56:39 oga Exp $ */
d234 1
@


1.14
log
@Right now, if anything internal changes with a uvm object, diverse
places in the tree need to be touched to update the object
initialisation with respect to that.

So, make a function (uvm_initobj) that takes the refcount, object and
pager ops and does this initialisation for us. This should save on
maintainance in the future.

looked good to fgs@@. Tedu complained about the British spelling but OKed
it anyway.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.13 2010/04/21 03:04:04 deraadt Exp $ */
d262 1
d266 1
a266 2
	struct vm_page *pg;
	voff_t offs, i;
d280 2
a281 16
	for (i = 0; i < atop(size); i++) {
#if defined(DEBUG) || 1
		if ((pg = uvm_pagelookup(buf_object, offs + ptoa(i))))
			panic("buf_alloc_pages: overlap buf: %p page: %p",
			    bp, pg);
#endif

		while ((pg = uvm_pagealloc(buf_object, offs + ptoa(i),
			    NULL, 0)) == NULL) {
			uvm_wait("buf_alloc_pages");
		}
		pg->wire_count = 1;
		atomic_clearbits_int(&pg->pg_flags, PG_BUSY);
		bcstats.numbufpages++;
	}

@


1.13
log
@the atomic primitives are still impossible to get at without using proc.h
(because it pulls in so much of the world) so include it for now, but
mark it XXX
ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.12 2009/08/09 17:45:02 art Exp $ */
d67 1
a67 4
	buf_object->pgops = NULL;
	RB_INIT(&buf_object->memt);
	buf_object->uo_npages = 0;
	buf_object->uo_refs = 1;
@


1.12
log
@buf_dealloc_mem can be called without B_BUSY. Don't decrement busymapped in that case.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.11 2009/08/09 14:37:46 art Exp $ */
d23 1
@


1.11
log
@Keep track of number of currently mapped and B_BUSY buffers. beck@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.10 2009/08/06 15:28:14 oga Exp $ */
d201 2
a202 1
		bcstats.busymapped--;
@


1.10
log
@reintroduce the uvm_tree commit.

Now instead of the global object hashtable, we have a per object tree.

Testing shows no performance difference and a slight code shrink. OTOH when
locking is more fine grained this should be faster due to lock contention on
uvm.hashlock.

ok thib@@, art@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.9 2009/06/25 15:49:26 thib Exp $ */
d152 2
d167 1
d201 1
@


1.9
log
@backout the buf_acquire() does the bremfree() since all callers
where doing bremfree() befure calling buf_acquire().

This is causing us headache pinning down a bug that showed up
when deraadt@@ too cvs to current, and will have to be done
anyway as a preperation for backouts.

OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.7 2009/06/06 18:06:22 art Exp $ */
d67 1
a67 1
	TAILQ_INIT(&buf_object->memq);
@


1.8
log
@Backout all changes to uvm after pmemrange (which will be backed out
separately).

a change at or just before the hackathon has either exposed or added a
very very nasty memory corruption bug that is giving us hell right now.
So in the interest of kernel stability these diffs are being backed out
until such a time as that corruption bug has been found and squashed,
then the ones that are proven good may slowly return.

a quick hitlist of the main commits this backs out:

mine:
uvm_objwire
the lock change in uvm_swap.c
using trees for uvm objects instead of the hash
removing the pgo_releasepg callback.

art@@'s:
putting pmap_page_protect(VM_PROT_NONE) in uvm_pagedeactivate() since
all callers called that just prior anyway.

ok beck@@, ariane@@.

prompted by deraadt@@.
@
text
@a82 1

a86 1
	bremfree(bp);
@


1.7
log
@All caller of buf_acquire were doing bremfree before the call.
Just put it in the buf_acquire function.
oga@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.6 2009/06/02 23:00:19 oga Exp $ */
d67 1
a67 1
	RB_INIT(&buf_object->memt);
@


1.6
log
@Instead of the global hash table with the terrible hashfunction and a
global lock, switch the uvm object pages to being kept in a per-object
RB_TREE. Right now this is approximately the same speed, but cleaner.
When biglock usage is reduced this will improve concurrency due to lock
contention..

ok beck@@ art@@. Thanks to jasper for the speed testing.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.5 2009/04/22 13:12:26 art Exp $ */
d83 1
d88 1
@


1.5
log
@Make the interactions in allocating buffers less confusing.

- getnewbuf dies. instead of having getnewbuf, buf_get, buf_stub and
 buf_init we now have buf_get that is smaller than some of those
 functions were before.

- Instead of allocating anonymous buffers and then freeing them if we
 happened to lose the race to the hash, always allocate a buffer knowing
 which <vnode, block> it will belong to.

- In cluster read, instead of allocating an anonymous buffer to cover
 the whole read and then stubs for every buffer under it, make the
 first buffer in the cluster cover the whole range and then shrink it
 in the callback.

now, all buffers are always on the correct hash and we always know their
identity.

discussed with many, kettenis@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.4 2008/11/08 23:20:50 pedro Exp $ */
d67 1
a67 1
	TAILQ_INIT(&buf_object->memq);
@


1.4
log
@Use atop() and ptoa() in buf_free_pages() to make it consistent with
the rest of the code, okay art@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.3 2008/08/03 18:08:54 kettenis Exp $ */
a77 1
	vaddr_t va;
d87 26
a150 2
	splx(s);
}
d152 1
a152 13
/*
 * Busy a buffer, but don't map it.
 * If it has a mapping, we keep it, but we also keep the mapping on
 * the list since we assume that it won't be used anymore.
 */
void
buf_acquire_unmapped(struct buf *bp)
{
	int s;

	s = splbio();
	SET(bp->b_flags, B_BUSY|B_NOTMAPPED);
	splx(s);
d221 12
@


1.3
log
@Make buf_dealloc_mem() remove the mapping even if b_pobj is NULL.  Otherwise
bread_cluster() leaks mappings which causes grief on VAC architectures.

Fixes bsd.rd on hppa and armish.

ok art@@, beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_biomem.c,v 1.2 2008/06/11 00:11:03 thib Exp $ */
d293 2
a294 2
	for (i = 0; i < (bp->b_bufsize >> PAGE_SHIFT); i++) {
		pg = uvm_pagelookup(uobj, off + (i * PAGE_SIZE));
@


1.2
log
@add $OpenBSD$ tag.

I hate it, art@@
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
d178 1
a178 1
	caddr_t data = bp->b_data;
d183 1
d186 6
a191 5
	if (bp->b_pobj) {
		if (data) {
			pmap_kremove((vaddr_t)data, bp->b_bufsize);
			pmap_update(pmap_kernel());
		}
a192 1
	}
@


1.1
log
@
Buffer cache revamp

1) remove multiple size queues, introduced as a stopgap.
2) decouple pages containing data from their mappings
3) only keep buffers mapped when they actually have to be mapped
  (right now, this is when buffers are B_BUSY)
4) New functions to make a buffer busy, and release the busy flag
   (buf_acquire and buf_release)
5) Move high/low water marks and statistics counters into a structure
6) Add a sysctl to retrieve buffer cache statistics

Tested in several variants and beat upon by bob and art for a year. run
accidentally on henning's nfs server for a few months...

ok deraadt@@, krw@@, art@@ - who promises to be around to deal with any fallout
@
text
@d1 1
@

