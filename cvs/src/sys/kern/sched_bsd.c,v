head	1.46;
access;
symbols
	OPENBSD_6_2:1.46.0.2
	OPENBSD_6_2_BASE:1.46
	OPENBSD_6_1:1.46.0.4
	OPENBSD_6_1_BASE:1.46
	OPENBSD_6_0:1.43.0.2
	OPENBSD_6_0_BASE:1.43
	OPENBSD_5_9:1.42.0.2
	OPENBSD_5_9_BASE:1.42
	OPENBSD_5_8:1.41.0.4
	OPENBSD_5_8_BASE:1.41
	OPENBSD_5_7:1.40.0.2
	OPENBSD_5_7_BASE:1.40
	OPENBSD_5_6:1.35.0.4
	OPENBSD_5_6_BASE:1.35
	OPENBSD_5_5:1.33.0.6
	OPENBSD_5_5_BASE:1.33
	OPENBSD_5_4:1.33.0.2
	OPENBSD_5_4_BASE:1.33
	OPENBSD_5_3:1.30.0.4
	OPENBSD_5_3_BASE:1.30
	OPENBSD_5_2:1.30.0.2
	OPENBSD_5_2_BASE:1.30
	OPENBSD_5_1_BASE:1.27
	OPENBSD_5_1:1.27.0.4
	OPENBSD_5_0:1.27.0.2
	OPENBSD_5_0_BASE:1.27
	OPENBSD_4_9:1.24.0.2
	OPENBSD_4_9_BASE:1.24
	OPENBSD_4_8:1.23.0.2
	OPENBSD_4_8_BASE:1.23
	OPENBSD_4_7:1.22.0.2
	OPENBSD_4_7_BASE:1.22
	OPENBSD_4_6:1.21.0.4
	OPENBSD_4_6_BASE:1.21
	OPENBSD_4_5:1.19.0.2
	OPENBSD_4_5_BASE:1.19
	OPENBSD_4_4:1.17.0.2
	OPENBSD_4_4_BASE:1.17
	OPENBSD_4_3:1.15.0.2
	OPENBSD_4_3_BASE:1.15
	OPENBSD_4_2:1.12.0.2
	OPENBSD_4_2_BASE:1.12
	OPENBSD_4_1:1.10.0.2
	OPENBSD_4_1_BASE:1.10
	OPENBSD_4_0:1.5.0.6
	OPENBSD_4_0_BASE:1.5
	OPENBSD_3_9:1.5.0.4
	OPENBSD_3_9_BASE:1.5
	OPENBSD_3_8:1.5.0.2
	OPENBSD_3_8_BASE:1.5
	OPENBSD_3_7:1.1.0.4
	OPENBSD_3_7_BASE:1.1
	OPENBSD_3_6:1.1.0.2
	OPENBSD_3_6_BASE:1.1;
locks; strict;
comment	@ * @;


1.46
date	2017.02.14.10.31.15;	author mpi;	state Exp;
branches;
next	1.45;
commitid	PmGi4EGraGC0Z0ml;

1.45
date	2017.02.09.10.27.03;	author mpi;	state Exp;
branches;
next	1.44;
commitid	ZZyYGFeWXohUNLgi;

1.44
date	2017.01.25.06.15.50;	author mpi;	state Exp;
branches;
next	1.43;
commitid	X7Hk1efefaYrWlw3;

1.43
date	2016.03.09.13.38.50;	author mpi;	state Exp;
branches;
next	1.42;
commitid	THpTza9IRRtZUKne;

1.42
date	2015.11.08.20.45.57;	author naddy;	state Exp;
branches;
next	1.41;
commitid	xMgxEPz4COvjCUcL;

1.41
date	2015.03.14.03.38.50;	author jsg;	state Exp;
branches;
next	1.40;
commitid	p4LJxGKbi0BU2cG6;

1.40
date	2014.12.13.21.05.33;	author doug;	state Exp;
branches;
next	1.39;
commitid	20ZyHa9gTJxHxhwD;

1.39
date	2014.11.12.22.27.45;	author tedu;	state Exp;
branches;
next	1.38;
commitid	CsudtvS9yq7uz2e7;

1.38
date	2014.11.03.03.08.00;	author deraadt;	state Exp;
branches;
next	1.37;
commitid	3SGDR5EjcwE01W8S;

1.37
date	2014.10.17.15.34.55;	author deraadt;	state Exp;
branches;
next	1.36;
commitid	mgQLrOovbL1WuWTc;

1.36
date	2014.10.17.01.51.39;	author tedu;	state Exp;
branches;
next	1.35;
commitid	r6zxCLSQFWXeVG73;

1.35
date	2014.07.04.05.58.31;	author guenther;	state Exp;
branches;
next	1.34;
commitid	vhXZZxMGVTWiFaF3;

1.34
date	2014.05.15.03.52.25;	author guenther;	state Exp;
branches;
next	1.33;

1.33
date	2013.06.03.16.55.22;	author guenther;	state Exp;
branches;
next	1.32;

1.32
date	2013.06.02.20.59.09;	author guenther;	state Exp;
branches;
next	1.31;

1.31
date	2013.03.28.16.55.25;	author deraadt;	state Exp;
branches;
next	1.30;

1.30
date	2012.07.09.17.27.32;	author haesbaert;	state Exp;
branches;
next	1.29;

1.29
date	2012.03.23.15.51.26;	author guenther;	state Exp;
branches;
next	1.28;

1.28
date	2012.02.20.22.23.39;	author guenther;	state Exp;
branches;
next	1.27;

1.27
date	2011.07.07.18.00.33;	author guenther;	state Exp;
branches;
next	1.26;

1.26
date	2011.07.06.01.49.42;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2011.03.07.07.07.13;	author guenther;	state Exp;
branches;
next	1.24;

1.24
date	2010.09.24.13.21.30;	author matthew;	state Exp;
branches;
next	1.23;

1.23
date	2010.06.30.22.38.17;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2010.01.03.19.17.33;	author kettenis;	state Exp;
branches;
next	1.21;

1.21
date	2009.04.14.09.13.25;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2009.03.23.13.25.11;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2008.11.06.22.11.36;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2008.09.10.14.01.23;	author blambert;	state Exp;
branches;
next	1.17;

1.17
date	2008.07.18.23.43.31;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2008.05.22.14.07.14;	author thib;	state Exp;
branches;
next	1.15;

1.15
date	2007.11.26.17.15.29;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2007.10.11.10.34.08;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2007.10.10.15.53.53;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2007.05.18.16.10.15;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2007.05.16.17.27.30;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2007.02.06.18.42.37;	author art;	state Exp;
branches;
next	1.9;

1.9
date	2006.11.29.12.24.18;	author miod;	state Exp;
branches;
next	1.8;

1.8
date	2006.11.15.17.25.40;	author jmc;	state Exp;
branches;
next	1.7;

1.7
date	2006.10.21.02.18.00;	author tedu;	state Exp;
branches;
next	1.6;

1.6
date	2006.10.09.00.31.11;	author tedu;	state Exp;
branches;
next	1.5;

1.5
date	2005.06.17.22.33.34;	author niklas;	state Exp;
branches;
next	1.4;

1.4
date	2005.05.29.03.20.41;	author deraadt;	state Exp;
branches;
next	1.3;

1.3
date	2005.05.26.18.10.40;	author art;	state Exp;
branches;
next	1.2;

1.2
date	2005.05.25.23.17.47;	author niklas;	state Exp;
branches;
next	1.1;

1.1
date	2004.07.29.06.25.45;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.46
log
@Convert most of the manual checks for CPU hogging to sched_pause().

The distinction between preempt() and yield() stays as it is usueful
to know if a thread decided to yield by itself or if the kernel told
him to go away.

ok tedu@@, guenther@@
@
text
@/*	$OpenBSD: sched_bsd.c,v 1.45 2017/02/09 10:27:03 mpi Exp $	*/
/*	$NetBSD: kern_synch.c,v 1.37 1996/04/22 01:38:37 christos Exp $	*/

/*-
 * Copyright (c) 1982, 1986, 1990, 1991, 1993
 *	The Regents of the University of California.  All rights reserved.
 * (c) UNIX System Laboratories, Inc.
 * All or some portions of this file are derived from material licensed
 * to the University of California by American Telephone and Telegraph
 * Co. or Unix System Laboratories, Inc. and are reproduced herein with
 * the permission of UNIX System Laboratories, Inc.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)kern_synch.c	8.6 (Berkeley) 1/21/94
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/kernel.h>
#include <sys/malloc.h>
#include <sys/signalvar.h>
#include <sys/resourcevar.h>
#include <uvm/uvm_extern.h>
#include <sys/sched.h>
#include <sys/timeout.h>

#ifdef KTRACE
#include <sys/ktrace.h>
#endif


int	lbolt;			/* once a second sleep address */
int	rrticks_init;		/* # of hardclock ticks per roundrobin() */

#ifdef MULTIPROCESSOR
struct __mp_lock sched_lock;
#endif

void	 schedcpu(void *);
void	 updatepri(struct proc *);

void
scheduler_start(void)
{
	static struct timeout schedcpu_to;

	/*
	 * We avoid polluting the global namespace by keeping the scheduler
	 * timeouts static in this function.
	 * We setup the timeout here and kick schedcpu once to make it do
	 * its job.
	 */
	timeout_set(&schedcpu_to, schedcpu, &schedcpu_to);

	rrticks_init = hz / 10;
	schedcpu(&schedcpu_to);
}

/*
 * Force switch among equal priority processes every 100ms.
 */
void
roundrobin(struct cpu_info *ci)
{
	struct schedstate_percpu *spc = &ci->ci_schedstate;

	spc->spc_rrticks = rrticks_init;

	if (ci->ci_curproc != NULL) {
		if (spc->spc_schedflags & SPCF_SEENRR) {
			/*
			 * The process has already been through a roundrobin
			 * without switching and may be hogging the CPU.
			 * Indicate that the process should yield.
			 */
			atomic_setbits_int(&spc->spc_schedflags,
			    SPCF_SHOULDYIELD);
		} else {
			atomic_setbits_int(&spc->spc_schedflags,
			    SPCF_SEENRR);
		}
	}

	if (spc->spc_nrun)
		need_resched(ci);
}

/*
 * Constants for digital decay and forget:
 *	90% of (p_estcpu) usage in 5 * loadav time
 *	95% of (p_pctcpu) usage in 60 seconds (load insensitive)
 *          Note that, as ps(1) mentions, this can let percentages
 *          total over 100% (I've seen 137.9% for 3 processes).
 *
 * Note that hardclock updates p_estcpu and p_cpticks independently.
 *
 * We wish to decay away 90% of p_estcpu in (5 * loadavg) seconds.
 * That is, the system wants to compute a value of decay such
 * that the following for loop:
 * 	for (i = 0; i < (5 * loadavg); i++)
 * 		p_estcpu *= decay;
 * will compute
 * 	p_estcpu *= 0.1;
 * for all values of loadavg:
 *
 * Mathematically this loop can be expressed by saying:
 * 	decay ** (5 * loadavg) ~= .1
 *
 * The system computes decay as:
 * 	decay = (2 * loadavg) / (2 * loadavg + 1)
 *
 * We wish to prove that the system's computation of decay
 * will always fulfill the equation:
 * 	decay ** (5 * loadavg) ~= .1
 *
 * If we compute b as:
 * 	b = 2 * loadavg
 * then
 * 	decay = b / (b + 1)
 *
 * We now need to prove two things:
 *	1) Given factor ** (5 * loadavg) ~= .1, prove factor == b/(b+1)
 *	2) Given b/(b+1) ** power ~= .1, prove power == (5 * loadavg)
 *	
 * Facts:
 *         For x close to zero, exp(x) =~ 1 + x, since
 *              exp(x) = 0! + x**1/1! + x**2/2! + ... .
 *              therefore exp(-1/b) =~ 1 - (1/b) = (b-1)/b.
 *         For x close to zero, ln(1+x) =~ x, since
 *              ln(1+x) = x - x**2/2 + x**3/3 - ...     -1 < x < 1
 *              therefore ln(b/(b+1)) = ln(1 - 1/(b+1)) =~ -1/(b+1).
 *         ln(.1) =~ -2.30
 *
 * Proof of (1):
 *    Solve (factor)**(power) =~ .1 given power (5*loadav):
 *	solving for factor,
 *      ln(factor) =~ (-2.30/5*loadav), or
 *      factor =~ exp(-1/((5/2.30)*loadav)) =~ exp(-1/(2*loadav)) =
 *          exp(-1/b) =~ (b-1)/b =~ b/(b+1).                    QED
 *
 * Proof of (2):
 *    Solve (factor)**(power) =~ .1 given factor == (b/(b+1)):
 *	solving for power,
 *      power*ln(b/(b+1)) =~ -2.30, or
 *      power =~ 2.3 * (b + 1) = 4.6*loadav + 2.3 =~ 5*loadav.  QED
 *
 * Actual power values for the implemented algorithm are as follows:
 *      loadav: 1       2       3       4
 *      power:  5.68    10.32   14.94   19.55
 */

/* calculations for digital decay to forget 90% of usage in 5*loadav sec */
#define	loadfactor(loadav)	(2 * (loadav))
#define	decay_cpu(loadfac, cpu)	(((loadfac) * (cpu)) / ((loadfac) + FSCALE))

/* decay 95% of `p_pctcpu' in 60 seconds; see CCPU_SHIFT before changing */
fixpt_t	ccpu = 0.95122942450071400909 * FSCALE;		/* exp(-1/20) */

/*
 * If `ccpu' is not equal to `exp(-1/20)' and you still want to use the
 * faster/more-accurate formula, you'll have to estimate CCPU_SHIFT below
 * and possibly adjust FSHIFT in "param.h" so that (FSHIFT >= CCPU_SHIFT).
 *
 * To estimate CCPU_SHIFT for exp(-1/20), the following formula was used:
 *	1 - exp(-1/20) ~= 0.0487 ~= 0.0488 == 1 (fixed pt, *11* bits).
 *
 * If you don't want to bother with the faster/more-accurate formula, you
 * can set CCPU_SHIFT to (FSHIFT + 1) which will use a slower/less-accurate
 * (more general) method of calculating the %age of CPU used by a process.
 */
#define	CCPU_SHIFT	11

/*
 * Recompute process priorities, every second.
 */
void
schedcpu(void *arg)
{
	struct timeout *to = (struct timeout *)arg;
	fixpt_t loadfac = loadfactor(averunnable.ldavg[0]);
	struct proc *p;
	int s;
	unsigned int newcpu;
	int phz;

	/*
	 * If we have a statistics clock, use that to calculate CPU
	 * time, otherwise revert to using the profiling clock (which,
	 * in turn, defaults to hz if there is no separate profiling
	 * clock available)
	 */
	phz = stathz ? stathz : profhz;
	KASSERT(phz);

	LIST_FOREACH(p, &allproc, p_list) {
		/*
		 * Increment sleep time (if sleeping). We ignore overflow.
		 */
		if (p->p_stat == SSLEEP || p->p_stat == SSTOP)
			p->p_slptime++;
		p->p_pctcpu = (p->p_pctcpu * ccpu) >> FSHIFT;
		/*
		 * If the process has slept the entire second,
		 * stop recalculating its priority until it wakes up.
		 */
		if (p->p_slptime > 1)
			continue;
		SCHED_LOCK(s);
		/*
		 * p_pctcpu is only for diagnostic tools such as ps.
		 */
#if	(FSHIFT >= CCPU_SHIFT)
		p->p_pctcpu += (phz == 100)?
			((fixpt_t) p->p_cpticks) << (FSHIFT - CCPU_SHIFT):
                	100 * (((fixpt_t) p->p_cpticks)
				<< (FSHIFT - CCPU_SHIFT)) / phz;
#else
		p->p_pctcpu += ((FSCALE - ccpu) *
			(p->p_cpticks * FSCALE / phz)) >> FSHIFT;
#endif
		p->p_cpticks = 0;
		newcpu = (u_int) decay_cpu(loadfac, p->p_estcpu);
		p->p_estcpu = newcpu;
		resetpriority(p);
		if (p->p_priority >= PUSER) {
			if (p->p_stat == SRUN &&
			    (p->p_priority / SCHED_PPQ) !=
			    (p->p_usrpri / SCHED_PPQ)) {
				remrunqueue(p);
				p->p_priority = p->p_usrpri;
				setrunqueue(p);
			} else
				p->p_priority = p->p_usrpri;
		}
		SCHED_UNLOCK(s);
	}
	uvm_meter();
	wakeup(&lbolt);
	timeout_add_sec(to, 1);
}

/*
 * Recalculate the priority of a process after it has slept for a while.
 * For all load averages >= 1 and max p_estcpu of 255, sleeping for at
 * least six times the loadfactor will decay p_estcpu to zero.
 */
void
updatepri(struct proc *p)
{
	unsigned int newcpu = p->p_estcpu;
	fixpt_t loadfac = loadfactor(averunnable.ldavg[0]);

	SCHED_ASSERT_LOCKED();

	if (p->p_slptime > 5 * loadfac)
		p->p_estcpu = 0;
	else {
		p->p_slptime--;	/* the first time was done in schedcpu */
		while (newcpu && --p->p_slptime)
			newcpu = (int) decay_cpu(loadfac, newcpu);
		p->p_estcpu = newcpu;
	}
	resetpriority(p);
}

/*
 * General yield call.  Puts the current process back on its run queue and
 * performs a voluntary context switch.
 */
void
yield(void)
{
	struct proc *p = curproc;
	int s;

	NET_ASSERT_UNLOCKED();

	SCHED_LOCK(s);
	p->p_priority = p->p_usrpri;
	p->p_stat = SRUN;
	setrunqueue(p);
	p->p_ru.ru_nvcsw++;
	mi_switch();
	SCHED_UNLOCK(s);
}

/*
 * General preemption call.  Puts the current process back on its run queue
 * and performs an involuntary context switch.  If a process is supplied,
 * we switch to that process.  Otherwise, we use the normal process selection
 * criteria.
 */
void
preempt(void)
{
	struct proc *p = curproc;
	int s;

	SCHED_LOCK(s);
	p->p_priority = p->p_usrpri;
	p->p_stat = SRUN;
	setrunqueue(p);
	p->p_ru.ru_nivcsw++;
	mi_switch();
	SCHED_UNLOCK(s);
}

void
mi_switch(void)
{
	struct schedstate_percpu *spc = &curcpu()->ci_schedstate;
	struct proc *p = curproc;
	struct proc *nextproc;
	struct process *pr = p->p_p;
	struct rlimit *rlim;
	rlim_t secs;
	struct timespec ts;
#ifdef MULTIPROCESSOR
	int hold_count;
	int sched_count;
#endif

	assertwaitok();
	KASSERT(p->p_stat != SONPROC);

	SCHED_ASSERT_LOCKED();

#ifdef MULTIPROCESSOR
	/*
	 * Release the kernel_lock, as we are about to yield the CPU.
	 */
	sched_count = __mp_release_all_but_one(&sched_lock);
	if (__mp_lock_held(&kernel_lock))
		hold_count = __mp_release_all(&kernel_lock);
	else
		hold_count = 0;
#endif

	/*
	 * Compute the amount of time during which the current
	 * process was running, and add that to its total so far.
	 */
	nanouptime(&ts);
	if (timespeccmp(&ts, &spc->spc_runtime, <)) {
#if 0
		printf("uptime is not monotonic! "
		    "ts=%lld.%09lu, runtime=%lld.%09lu\n",
		    (long long)tv.tv_sec, tv.tv_nsec,
		    (long long)spc->spc_runtime.tv_sec,
		    spc->spc_runtime.tv_nsec);
#endif
	} else {
		timespecsub(&ts, &spc->spc_runtime, &ts);
		timespecadd(&p->p_rtime, &ts, &p->p_rtime);
	}

	/* add the time counts for this thread to the process's total */
	tuagg_unlocked(pr, p);

	/*
	 * Check if the process exceeds its cpu resource allocation.
	 * If over max, kill it.
	 */
	rlim = &pr->ps_limit->pl_rlimit[RLIMIT_CPU];
	secs = pr->ps_tu.tu_runtime.tv_sec;
	if (secs >= rlim->rlim_cur) {
		if (secs >= rlim->rlim_max) {
			psignal(p, SIGKILL);
		} else {
			psignal(p, SIGXCPU);
			if (rlim->rlim_cur < rlim->rlim_max)
				rlim->rlim_cur += 5;
		}
	}

	/*
	 * Process is about to yield the CPU; clear the appropriate
	 * scheduling flags.
	 */
	atomic_clearbits_int(&spc->spc_schedflags, SPCF_SWITCHCLEAR);

	nextproc = sched_chooseproc();

	if (p != nextproc) {
		uvmexp.swtch++;
		cpu_switchto(p, nextproc);
	} else {
		p->p_stat = SONPROC;
	}

	clear_resched(curcpu());

	SCHED_ASSERT_LOCKED();

	/*
	 * To preserve lock ordering, we need to release the sched lock
	 * and grab it after we grab the big lock.
	 * In the future, when the sched lock isn't recursive, we'll
	 * just release it here.
	 */
#ifdef MULTIPROCESSOR
	__mp_unlock(&sched_lock);
#endif

	SCHED_ASSERT_UNLOCKED();

	/*
	 * We're running again; record our new start time.  We might
	 * be running on a new CPU now, so don't use the cache'd
	 * schedstate_percpu pointer.
	 */
	KASSERT(p->p_cpu == curcpu());

	nanouptime(&p->p_cpu->ci_schedstate.spc_runtime);

#ifdef MULTIPROCESSOR
	/*
	 * Reacquire the kernel_lock now.  We do this after we've
	 * released the scheduler lock to avoid deadlock, and before
	 * we reacquire the interlock and the scheduler lock.
	 */
	if (hold_count)
		__mp_acquire_count(&kernel_lock, hold_count);
	__mp_acquire_count(&sched_lock, sched_count + 1);
#endif
}

static __inline void
resched_proc(struct proc *p, u_char pri)
{
	struct cpu_info *ci;

	/*
	 * XXXSMP
	 * This does not handle the case where its last
	 * CPU is running a higher-priority process, but every
	 * other CPU is running a lower-priority process.  There
	 * are ways to handle this situation, but they're not
	 * currently very pretty, and we also need to weigh the
	 * cost of moving a process from one CPU to another.
	 *
	 * XXXSMP
	 * There is also the issue of locking the other CPU's
	 * sched state, which we currently do not do.
	 */
	ci = (p->p_cpu != NULL) ? p->p_cpu : curcpu();
	if (pri < ci->ci_schedstate.spc_curpriority)
		need_resched(ci);
}

/*
 * Change process state to be runnable,
 * placing it on the run queue if it is in memory,
 * and awakening the swapper if it isn't in memory.
 */
void
setrunnable(struct proc *p)
{
	SCHED_ASSERT_LOCKED();

	switch (p->p_stat) {
	case 0:
	case SRUN:
	case SONPROC:
	case SDEAD:
	case SIDL:
	default:
		panic("setrunnable");
	case SSTOP:
		/*
		 * If we're being traced (possibly because someone attached us
		 * while we were stopped), check for a signal from the debugger.
		 */
		if ((p->p_p->ps_flags & PS_TRACED) != 0 && p->p_xstat != 0)
			atomic_setbits_int(&p->p_siglist, sigmask(p->p_xstat));
	case SSLEEP:
		unsleep(p);		/* e.g. when sending signals */
		break;
	}
	p->p_stat = SRUN;
	p->p_cpu = sched_choosecpu(p);
	setrunqueue(p);
	if (p->p_slptime > 1)
		updatepri(p);
	p->p_slptime = 0;
	resched_proc(p, p->p_priority);
}

/*
 * Compute the priority of a process when running in user mode.
 * Arrange to reschedule if the resulting priority is better
 * than that of the current process.
 */
void
resetpriority(struct proc *p)
{
	unsigned int newpriority;

	SCHED_ASSERT_LOCKED();

	newpriority = PUSER + p->p_estcpu +
	    NICE_WEIGHT * (p->p_p->ps_nice - NZERO);
	newpriority = min(newpriority, MAXPRI);
	p->p_usrpri = newpriority;
	resched_proc(p, p->p_usrpri);
}

/*
 * We adjust the priority of the current process.  The priority of a process
 * gets worse as it accumulates CPU time.  The cpu usage estimator (p_estcpu)
 * is increased here.  The formula for computing priorities (in kern_synch.c)
 * will compute a different value each time p_estcpu increases. This can
 * cause a switch, but unless the priority crosses a PPQ boundary the actual
 * queue will not change.  The cpu usage estimator ramps up quite quickly
 * when the process is running (linearly), and decays away exponentially, at
 * a rate which is proportionally slower when the system is busy.  The basic
 * principle is that the system will 90% forget that the process used a lot
 * of CPU time in 5 * loadav seconds.  This causes the system to favor
 * processes which haven't run much recently, and to round-robin among other
 * processes.
 */
void
schedclock(struct proc *p)
{
	int s;

	SCHED_LOCK(s);
	p->p_estcpu = ESTCPULIM(p->p_estcpu + 1);
	resetpriority(p);
	if (p->p_priority >= PUSER)
		p->p_priority = p->p_usrpri;
	SCHED_UNLOCK(s);
}

void (*cpu_setperf)(int);

#define PERFPOL_MANUAL 0
#define PERFPOL_AUTO 1
#define PERFPOL_HIGH 2
int perflevel = 100;
int perfpolicy = PERFPOL_MANUAL;

#ifndef SMALL_KERNEL
/*
 * The code below handles CPU throttling.
 */
#include <sys/sysctl.h>

void setperf_auto(void *);
struct timeout setperf_to = TIMEOUT_INITIALIZER(setperf_auto, NULL);

void
setperf_auto(void *v)
{
	static uint64_t *idleticks, *totalticks;
	static int downbeats;

	int i, j;
	int speedup;
	CPU_INFO_ITERATOR cii;
	struct cpu_info *ci;
	uint64_t idle, total, allidle, alltotal;

	if (perfpolicy != PERFPOL_AUTO)
		return;

	if (!idleticks)
		if (!(idleticks = mallocarray(ncpusfound, sizeof(*idleticks),
		    M_DEVBUF, M_NOWAIT | M_ZERO)))
			return;
	if (!totalticks)
		if (!(totalticks = mallocarray(ncpusfound, sizeof(*totalticks),
		    M_DEVBUF, M_NOWAIT | M_ZERO))) {
			free(idleticks, M_DEVBUF,
			    sizeof(*idleticks) * ncpusfound);
			return;
		}

	alltotal = allidle = 0;
	j = 0;
	speedup = 0;
	CPU_INFO_FOREACH(cii, ci) {
		total = 0;
		for (i = 0; i < CPUSTATES; i++) {
			total += ci->ci_schedstate.spc_cp_time[i];
		}
		total -= totalticks[j];
		idle = ci->ci_schedstate.spc_cp_time[CP_IDLE] - idleticks[j];
		if (idle < total / 3)
			speedup = 1;
		alltotal += total;
		allidle += idle;
		idleticks[j] += idle;
		totalticks[j] += total;
		j++;
	}
	if (allidle < alltotal / 2)
		speedup = 1;
	if (speedup)
		downbeats = 5;

	if (speedup && perflevel != 100) {
		perflevel = 100;
		cpu_setperf(perflevel);
	} else if (!speedup && perflevel != 0 && --downbeats <= 0) {
		perflevel = 0;
		cpu_setperf(perflevel);
	}
	
	timeout_add_msec(&setperf_to, 100);
}

int
sysctl_hwsetperf(void *oldp, size_t *oldlenp, void *newp, size_t newlen)
{
	int err, newperf;

	if (!cpu_setperf)
		return EOPNOTSUPP;

	if (perfpolicy != PERFPOL_MANUAL)
		return sysctl_rdint(oldp, oldlenp, newp, perflevel);
	
	newperf = perflevel;
	err = sysctl_int(oldp, oldlenp, newp, newlen, &newperf);
	if (err)
		return err;
	if (newperf > 100)
		newperf = 100;
	if (newperf < 0)
		newperf = 0;
	perflevel = newperf;
	cpu_setperf(perflevel);

	return 0;
}

int
sysctl_hwperfpolicy(void *oldp, size_t *oldlenp, void *newp, size_t newlen)
{
	char policy[32];
	int err;

	if (!cpu_setperf)
		return EOPNOTSUPP;

	switch (perfpolicy) {
	case PERFPOL_MANUAL:
		strlcpy(policy, "manual", sizeof(policy));
		break;
	case PERFPOL_AUTO:
		strlcpy(policy, "auto", sizeof(policy));
		break;
	case PERFPOL_HIGH:
		strlcpy(policy, "high", sizeof(policy));
		break;
	default:
		strlcpy(policy, "unknown", sizeof(policy));
		break;
	}

	if (newp == NULL)
		return sysctl_rdstring(oldp, oldlenp, newp, policy);

	err = sysctl_string(oldp, oldlenp, newp, newlen, policy, sizeof(policy));
	if (err)
		return err;
	if (strcmp(policy, "manual") == 0)
		perfpolicy = PERFPOL_MANUAL;
	else if (strcmp(policy, "auto") == 0)
		perfpolicy = PERFPOL_AUTO;
	else if (strcmp(policy, "high") == 0)
		perfpolicy = PERFPOL_HIGH;
	else
		return EINVAL;

	if (perfpolicy == PERFPOL_AUTO) {
		timeout_add_msec(&setperf_to, 200);
	} else if (perfpolicy == PERFPOL_HIGH) {
		perflevel = 100;
		cpu_setperf(perflevel);
	}
	return 0;
}
#endif
@


1.45
log
@Do no select a CPU to execute the current thread when being preempt()ed.

Calling sched_choosecpu() at this moment often result in moving the thread
to a different CPU.  This does not help the scheduler and creates a domino
effect, resulting in kernel thread moving to other CPUs.

Tested by many without performance impact.  Simon Mages measured a small
performance improvement and a smaller variance with an http proxy.

Discussed with kettenis@@, ok martijn@@, beck@@, visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.44 2017/01/25 06:15:50 mpi Exp $	*/
d318 1
a318 1
preempt(struct proc *newp)
a321 6

	/*
	 * XXX Switching to a specific process is not supported yet.
	 */
	if (newp != NULL)
		panic("preempt: cpu_preempt not yet implemented");
@


1.44
log
@Enable the NET_LOCK(), take 2.

Recursions are currently known and marked a XXXSMP.

Please report any assert to bugs@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.43 2016/03/09 13:38:50 mpi Exp $	*/
a331 1
	p->p_cpu = sched_choosecpu(p);
@


1.43
log
@Correct some comments and definitions, from Michal Mazurek.
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.42 2015/11/08 20:45:57 naddy Exp $	*/
d299 2
@


1.42
log
@keep all the setperf timeout(9) handling in one place; ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.41 2015/03/14 03:38:50 jsg Exp $	*/
d63 2
a64 1
void schedcpu(void *);
d74 2
a75 2
	 * We setup the timeouts here and kick schedcpu and roundrobin once to
	 * make them do their job.
a76 1

d221 1
a221 3
		 * Increment time in/out of memory and sleep time
		 * (if sleeping).  We ignore overflow; with 16-bit int's
		 * (remember them?) overflow takes 45 days.
d234 1
a234 1
		 * p_pctcpu is only for ps.
a550 1

a714 1

@


1.41
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.40 2014/12/13 21:05:33 doug Exp $	*/
a580 1
struct timeout setperf_to;
d582 1
@


1.40
log
@yet more mallocarray() changes.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.39 2014/11/12 22:27:45 tedu Exp $	*/
a44 1
#include <sys/buf.h>
@


1.39
log
@take a few more ticks to actually throttle down. hopefully helps in
situations where e.g. web browsing is cpu intense but intermittently idle.
subject to further refinement and tuning.
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.38 2014/11/03 03:08:00 deraadt Exp $	*/
d601 1
a601 1
		if (!(idleticks = malloc(sizeof(*idleticks) * ncpusfound,
d605 1
a605 1
		if (!(totalticks = malloc(sizeof(*totalticks) * ncpusfound,
@


1.38
log
@pass size argument to free()
ok doug tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.37 2014/10/17 15:34:55 deraadt Exp $	*/
d589 1
d632 2
d638 1
a638 1
	} else if (!speedup && perflevel != 0) {
@


1.37
log
@cpu_setperf and perflevel must remain exposed, otherwise a bunch of
MD code needs excess #ifndef SMALL_KERNEL
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.36 2014/10/17 01:51:39 tedu Exp $	*/
d606 2
a607 1
			free(idleticks, M_DEVBUF, 0);
@


1.36
log
@redo the performance throttling in the kernel.
introduce a new sysctl, hw.perfpolicy, that governs the policy.
when set to anything other than manual, hw.setperf then becomes read only.
phessler was heading in this direction, but this is slightly different. :)
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.35 2014/07/04 05:58:31 guenther Exp $	*/
d568 1
a568 5
#ifndef SMALL_KERNEL
/*
 * The code below handles CPU throttling.
 */
#include <sys/sysctl.h>
d576 5
a580 1
void (*cpu_setperf)(int);
@


1.35
log
@Track whether a process is a zombie or not yet fully built via flags
PS_{ZOMBIE,EMBRYO} on the process instead of peeking into the process's
thread data.  This eliminates the need for the thread-level SDEAD state.

Change kvm_getprocs() (both the sysctl() and kvm backends) to report the
"most active" scheduler state for the process's threads.

tweaks kettenis@@
feedback and ok matthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.34 2014/05/15 03:52:25 guenther Exp $	*/
d44 1
d567 149
@


1.34
log
@Move from struct proc to process the reference-count-holding pointers
to the process's vmspace and filedescs.  struct proc continues to
keep copies of the pointers, copying them on fork, clearing them
on exit, and (for vmspace) refreshing on exec.
Also, make uvm_swapout_threads() thread aware, eliminating p_swtime
in kernel.

particular testing by ajacoutot@@ and sebastia@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.33 2013/06/03 16:55:22 guenther Exp $	*/
a495 1
	case SZOMB:
@


1.33
log
@Convert some internal APIs to use timespecs instead of timevals

ok matthew@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.32 2013/06/02 20:59:09 guenther Exp $	*/
a224 1
		p->p_swtime++;
@


1.32
log
@Use long long and %lld for printing tv_sec values

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.31 2013/03/28 16:55:25 deraadt Exp $	*/
d349 1
a349 1
	struct timeval tv;
d375 2
a376 2
	microuptime(&tv);
	if (timercmp(&tv, &spc->spc_runtime, <)) {
d379 2
a380 2
		    "tv=%lld.%06lu, runtime=%lld.%06lu\n",
		    (long long)tv.tv_sec, tv.tv_usec,
d382 1
a382 1
		    spc->spc_runtime.tv_usec);
d385 2
a386 2
		timersub(&tv, &spc->spc_runtime, &tv);
		timeradd(&p->p_rtime, &tv, &p->p_rtime);
d446 1
a446 1
	microuptime(&p->p_cpu->ci_schedstate.spc_runtime);
@


1.31
log
@do not include machine/cpu.h from a .c file; it is the responsibility of
.h files to pull it in, if needed
ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.30 2012/07/09 17:27:32 haesbaert Exp $	*/
d379 3
a381 2
		    "tv=%lu.%06lu, runtime=%lu.%06lu\n",
		    tv.tv_sec, tv.tv_usec, spc->spc_runtime.tv_sec,
@


1.30
log
@Tedu old comment concerning cpu affinity which does not apply anymore.

ok blambert@@ krw@@ tedu@@ miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.29 2012/03/23 15:51:26 guenther Exp $	*/
a54 1
#include <machine/cpu.h>
@


1.29
log
@Make rusage totals, itimers, and profile settings per-process instead
of per-rthread.  Handling of per-thread tick and runtime counters
inspired by how FreeBSD does it.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.28 2012/02/20 22:23:39 guenther Exp $	*/
d467 1
a467 10
	 * Since p->p_cpu persists across a context switch,
	 * this gives us *very weak* processor affinity, in
	 * that we notify the CPU on which the process last
	 * ran that it should try to switch.
	 *
	 * This does not guarantee that the process will run on
	 * that processor next, because another processor might
	 * grab it the next time it performs a context switch.
	 *
	 * This also does not handle the case where its last
@


1.28
log
@First steps for making ptrace work with rthreads:
 - move the P_TRACED and P_INEXEC flags, and p_oppid, p_ptmask, and
   p_ptstat member from struct proc to struct process
 - sort the PT_* requests into those that take a PID vs those that
   can also take a TID
 - stub in PT_GET_THREAD_FIRST and PT_GET_THREAD_NEXT

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.27 2011/07/07 18:00:33 guenther Exp $	*/
d308 1
a308 1
	p->p_stats->p_ru.ru_nvcsw++;
d336 1
a336 1
	p->p_stats->p_ru.ru_nivcsw++;
d347 1
d349 1
d389 3
d396 4
a399 3
	rlim = &p->p_rlimit[RLIMIT_CPU];
	if ((rlim_t)p->p_rtime.tv_sec >= rlim->rlim_cur) {
		if ((rlim_t)p->p_rtime.tv_sec >= rlim->rlim_max) {
@


1.27
log
@Functions used in files other than where they are defined should be
declared in .h files, not in each .c.  Apply that rule to endtsleep(),
scheduler_start(), updatepri(), and realitexpire()

ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.26 2011/07/06 01:49:42 art Exp $	*/
d510 1
a510 1
		if ((p->p_flag & P_TRACED) != 0 && p->p_xstat != 0)
@


1.26
log
@Stop using the P_BIGLOCK flag to figure out when we should release the
biglock in mi_switch and just check if we're holding the biglock.

The idea is that the first entry point into the kernel uses KERNEL_PROC_LOCK
and recursive calls use KERNEL_LOCK. This assumption is violated in at
least one place and has been causing confusion for lots of people.

Initial bug report and analysis from Pedro.

kettenis@@ beck@@ oga@@ thib@@ dlg@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.25 2011/03/07 07:07:13 guenther Exp $	*/
a63 3
void scheduler_start(void);

void roundrobin(struct cpu_info *);
a64 2
void updatepri(struct proc *);
void endtsleep(void *);
@


1.25
log
@The scheduling 'nice' value is per-process, not per-thread, so move it
into struct process.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.24 2010/09/24 13:21:30 matthew Exp $	*/
d369 1
a369 1
	if (p->p_flag & P_BIGLOCK)
d371 2
d453 1
a453 1
	if (p->p_flag & P_BIGLOCK)
@


1.24
log
@Add stricter asserts to DIAGNOSTIC kernels to help catch mutex and
rwlock misuse.  In particular, this commit makes the following
changes:

  1. i386 and amd64 now count the number of active mutexes so that
assertwaitok(9) can detect attempts to sleep while holding a mutex.

  2. i386 and amd64 check that we actually hold mutexes when passed to
mtx_leave().

  3. Calls to rw_exit*() now call rw_assert_{rd,wr}lock() as
appropriate.

ok krw@@, oga@@; "sounds good to me" deraadt@@; assembly bits double
checked by pirofti@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.23 2010/06/30 22:38:17 art Exp $	*/
d540 2
a541 1
	newpriority = PUSER + p->p_estcpu + NICE_WEIGHT * (p->p_nice - NZERO);
@


1.23
log
@This comment is unnecessarily confusing.
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.22 2010/01/03 19:17:33 kettenis Exp $	*/
d359 1
@


1.22
log
@Use atomic operations to access the per-cpu scheduler flags.
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.21 2009/04/14 09:13:25 art Exp $	*/
d204 1
a204 1
 * Recompute process priorities, every hz ticks.
@


1.21
log
@Some tweaks to the cpu affinity code.
 - Split up choosing of cpu between fork and "normal" cases. Fork is
   very different and should be treated as such.
 - Instead of implicitly choosing a cpu in setrunqueue, do it outside
   where it actually makes sense.
 - Just because a cpu is marked as idle doesn't mean it will be soon.
   There could be a thundering herd effect if we call wakeup from an
   interrupt handler, so subtract cpus with queued processes when
   deciding which cpu is actually idle.
 - some simplifications allowed by the above.

kettenis@@ ok (except one bugfix that was not in the intial diff)
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.20 2009/03/23 13:25:11 art Exp $	*/
a95 1
	int s;
a99 1
		s = splstatclock();
d106 2
a107 1
			spc->spc_schedflags |= SPCF_SHOULDYIELD;
d109 2
a110 1
			spc->spc_schedflags |= SPCF_SEENRR;
a111 1
		splx(s);
d408 1
a408 1
	spc->spc_schedflags &= ~SPCF_SWITCHCLEAR;
@


1.20
log
@Processor affinity for processes.
 - Split up run queues so that every cpu has one.
 - Make setrunqueue choose the cpu where we want to make this process
   runnable (this should be refined and less brutal in the future).
 - When choosing the cpu where we want to run, make some kind of educated
   guess where it will be best to run (very naive right now).
Other:
 - Set operations for sets of cpus.
 - load average calculations per cpu.
 - sched_is_idle() -> curcpu_is_idle()

tested, debugged and prodded by many@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.19 2008/11/06 22:11:36 art Exp $	*/
d340 1
d520 1
@


1.19
log
@Some paranoia and deconfusion.
 - setrunnable should never be run on SIDL processes. That's a bug and will
   cause all kinds of trouble. Change the switch statement to panic
   if that happens.
 - p->p_stat == SRUN implies that p != curproc since curproc will always be
   SONPROC. This is a leftover from before SONPROC.

deraadt@@ "commit"
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.18 2008/09/10 14:01:23 blambert Exp $	*/
d100 1
a100 1
	if (curproc != NULL) {
d115 2
a116 1
	need_resched(curcpu());
d261 2
a262 1
			    (p->p_priority / PPQ) != (p->p_usrpri / PPQ)) {
@


1.18
log
@Convert timeout_add() calls using multiples of hz to timeout_add_sec()

Really just the low-hanging fruit of (hopefully) forthcoming timeout
conversions.

ok art@@, krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.17 2008/07/18 23:43:31 art Exp $	*/
d259 1
a259 2
			if ((p != curproc) &&
			    p->p_stat == SRUN &&
d502 1
a513 2
		break;
	case SIDL:
@


1.17
log
@Add a macro that clears the want_resched flag that need_resched sets.
Right now when mi_switch picks up the same proc, we didn't clear the
flag which would mean that every time we service an AST we would attempt
a context switch. For some architectures, amd64 being probably the
most extreme, that meant attempting to context switch for every
trap and interrupt.

Now we clear_resched explicitly after every context switch, even if it
didn't do anything. Which also allows us to remove some more code
in cpu_switchto (not done yet).

miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.16 2008/05/22 14:07:14 thib Exp $	*/
d272 1
a272 1
	timeout_add(to, hz);
@


1.16
log
@kill 2 bogus ARGUSED and use the LIST_FOREACH() macro
instead of handrolling...

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.15 2007/11/26 17:15:29 art Exp $	*/
d417 2
@


1.15
log
@Move the implementation of __mp_lock (biglock) into machine dependent
code. At this moment all architectures get the copy of the old code
except i386 which gets a new shiny implementation that doesn't spin
at splhigh (doh!) and doesn't try to grab the biglock when releasing
the biglock (double doh!).

Shaves 10% of system time during kernel compile and might solve a few
bugs as a bonus.

Other architectures coming shortly.

miod@@ deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.14 2007/10/11 10:34:08 art Exp $	*/
a91 1
/* ARGSUSED */
a205 1
/* ARGSUSED */
d225 1
a225 1
	for (p = LIST_FIRST(&allproc); p != NULL; p = LIST_NEXT(p, p_list)) {
@


1.14
log
@sched_lock_idle and sched_unlock_idle are obsolete now.
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.13 2007/10/10 15:53:53 art Exp $	*/
d60 3
a62 1
struct SIMPLELOCK sched_lock;
@


1.13
log
@Make context switching much more MI:
 - Move the functionality of choosing a process from cpu_switch into
   a much simpler function: cpu_switchto. Instead of having the locore
   code walk the run queues, let the MI code choose the process we
   want to run and only implement the context switching itself in MD
   code.
 - Let MD context switching run without worrying about spls or locks.
 - Instead of having the idle loop implemented with special contexts
   in MD code, implement one idle proc for each cpu. make the idle
   loop MI with MD hooks.
 - Change the proc lists from the old style vax queues to TAILQs.
 - Change the sleep queue from vax queues to TAILQs. This makes
   wakeup() go from O(n^2) to O(n)

there will be some MD fallout, but it will be fixed shortly.
There's also a few cleanups to be done after this.

deraadt@@, kettenis@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.12 2007/05/18 16:10:15 art Exp $	*/
a297 14

#if defined(MULTIPROCESSOR) || defined(LOCKDEBUG)
void
sched_unlock_idle(void)
{
	SIMPLE_UNLOCK(&sched_lock);
}

void
sched_lock_idle(void)
{
	SIMPLE_LOCK(&sched_lock);
}
#endif /* MULTIPROCESSOR || LOCKDEBUG */
@


1.12
log
@Widen the SCHED_LOCK in two cases to protect p_estcpu and p_priority.

kettenis@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.11 2007/05/16 17:27:30 art Exp $	*/
a59 3
int whichqs;			/* Bit mask summary of non-empty Q's. */
struct prochd qs[NQS];

a358 4

/*
 * Must be called at splstatclock() or higher.
 */
d362 3
a364 1
	struct proc *p = curproc;	/* XXX */
d367 1
a367 1
#if defined(MULTIPROCESSOR)
d371 2
a372 1
	struct schedstate_percpu *spc = &p->p_cpu->ci_schedstate;
d376 1
a376 1
#if defined(MULTIPROCESSOR)
a378 2
	 * The scheduler lock is still held until cpu_switch()
	 * selects a new process and removes it from the run queue.
a387 1
	 * XXX - use microuptime here to avoid strangeness.
d423 11
d435 4
a438 1
	 * Pick a new current process and record its start time.
d440 3
a442 2
	uvmexp.swtch++;
	cpu_switch(p);
a443 4
	/*
	 * Make sure that MD code released the scheduler lock before
	 * resuming us.
	 */
d451 2
a452 2
	KDASSERT(p->p_cpu != NULL);
	KDASSERT(p->p_cpu == curcpu());
d455 1
a455 1
#if defined(MULTIPROCESSOR)
a464 14
}

/*
 * Initialize the (doubly-linked) run queues
 * to be empty.
 */
void
rqinit(void)
{
	int i;

	for (i = 0; i < NQS; i++)
		qs[i].ph_link = qs[i].ph_rlink = (struct proc *)&qs[i];
	SIMPLE_LOCK_INIT(&sched_lock);
@


1.11
log
@The world of __HAVEs and __HAVE_NOTs is reducing. All architectures
have cpu_info now, so kill the option.

eyeballed by jsg@@ and grange@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.10 2007/02/06 18:42:37 art Exp $	*/
d244 1
a244 1
		s = splstatclock();	/* prevent state changes */
a259 2
		splx(s);
		SCHED_LOCK(s);
d586 1
a587 1
	SCHED_LOCK(s);
a588 1
	SCHED_UNLOCK(s);
d591 1
@


1.10
log
@Use atomic.h operation for manipulating p_siglist in struct proc. Solves
the problem with lost signals in MP kernels.

miod@@, kettenis@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.9 2006/11/29 12:24:18 miod Exp $	*/
a56 3
#ifndef __HAVE_CPUINFO
u_char	curpriority;		/* usrpri of curproc */
#endif
a57 1
#ifdef __HAVE_CPUINFO
a58 1
#endif
a66 1
#ifdef __HAVE_CPUINFO
a67 3
#else
void roundrobin(void *);
#endif
a74 3
#ifndef __HAVE_CPUINFO
	static struct timeout roundrobin_to;
#endif
a83 3
#ifndef __HAVE_CPUINFO
	timeout_set(&roundrobin_to, roundrobin, &roundrobin_to);
#endif
a85 1
#ifdef __HAVE_CPUINFO
a86 3
#else
	roundrobin(&roundrobin_to);
#endif
a93 1
#ifdef __HAVE_CPUINFO
a118 27
#else
void
roundrobin(void *arg)
{
	struct timeout *to = (struct timeout *)arg;
	struct proc *p = curproc;
	int s;

	if (p != NULL) {
		s = splstatclock();
		if (p->p_schedflags & PSCHED_SEENRR) {
			/*
			 * The process has already been through a roundrobin
			 * without switching and may be hogging the CPU.
			 * Indicate that the process should yield.
			 */
			p->p_schedflags |= PSCHED_SHOULDYIELD;
		} else {
			p->p_schedflags |= PSCHED_SEENRR;
		}
		splx(s);
	}

	need_resched(NULL);
	timeout_add(to, hz / 10);
}
#endif
a377 1
#ifdef __HAVE_CPUINFO
a378 1
#endif
a398 1
#ifdef __HAVE_CPUINFO
a409 12
#else
	if (timercmp(&tv, &runtime, <)) {
#if 0
		printf("uptime is not monotonic! "
		    "tv=%lu.%06lu, runtime=%lu.%06lu\n",
		    tv.tv_sec, tv.tv_usec, runtime.tv_sec, runtime.tv_usec);
#endif
	} else {
		timersub(&tv, &runtime, &tv);
		timeradd(&p->p_rtime, &tv, &p->p_rtime);
	}
#endif
a429 1
#ifdef __HAVE_CPUINFO
a430 3
#else
	p->p_schedflags &= ~PSCHED_SWITCHCLEAR;
#endif
a448 1
#ifdef __HAVE_CPUINFO
a451 3
#else
	microuptime(&runtime);
#endif
a481 1
#ifdef __HAVE_CPUINFO
a482 1
#endif
a505 1
#ifdef __HAVE_CPUINFO
a508 4
#else
	if (pri < curpriority)
		need_resched(NULL);
#endif
@


1.9
log
@Kernel stack can be swapped. This means that stuff that's on the stack
should never be referenced outside the context of the process to which
this stack belongs unless we do the PHOLD/PRELE dance. Loads of code
doesn't follow the rules here. Instead of trying to track down all
offenders and fix this hairy situation, it makes much more sense
to not swap kernel stacks.

From art@@, tested by many some time ago.
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.8 2006/11/15 17:25:40 jmc Exp $	*/
d612 1
a612 1
			p->p_siglist |= sigmask(p->p_xstat);
@


1.8
log
@typos; from bret lambert
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.7 2006/10/21 02:18:00 tedu Exp $	*/
a312 1
			    (p->p_flag & P_INMEM) &&
d620 1
a620 2
	if (p->p_flag & P_INMEM)
		setrunqueue(p);
d624 1
a624 4
	if ((p->p_flag & P_INMEM) == 0)
		wakeup(&proc0);
	else
		resched_proc(p, p->p_priority);
@


1.7
log
@tbert sent me a diff to change some 0 to NULL
i got carried away and deleted a whole bunch of useless casts
this is C, not C++.  ok md5
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.6 2006/10/09 00:31:11 tedu Exp $	*/
d246 1
a246 1
 * If you dont want to bother with the faster/more-accurate formula, you
@


1.6
log
@bret lambert sent a patch removing register.  i made it ansi.
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.5 2005/06/17 22:33:34 niklas Exp $	*/
d162 1
a162 1
	need_resched(0);
d275 1
a275 1
	for (p = LIST_FIRST(&allproc); p != 0; p = LIST_NEXT(p, p_list)) {
d324 1
a324 1
	wakeup((caddr_t)&lbolt);
d585 1
a585 1
		need_resched(0);
d627 1
a627 1
		wakeup((caddr_t)&proc0);
@


1.5
log
@A second approach at fixing the telnet localhost & problem
(but I tend to call it ssh localhost & now when telnetd is
history).  This is more localized patch, but leaves us with
a recursive lock for protecting scheduling and signal state.
Better care is taken to actually be symmetric over mi_switch.
Also, the dolock cruft in psignal can go with this solution.
Better test runs by more people for longer time has been
carried out compared to the c2k5 patch.

Long term the current mess with interruptible sleep, the
default action on stop signals and wakeup interactions need
to be revisited.  ok deraadt@@, art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.4 2005/05/29 03:20:41 deraadt Exp $	*/
d82 1
a82 1
scheduler_start()
d257 1
a257 2
schedcpu(arg)
	void *arg;
d334 1
a334 2
updatepri(p)
	register struct proc *p;
d336 2
a337 2
	register unsigned int newcpu = p->p_estcpu;
	register fixpt_t loadfac = loadfactor(averunnable.ldavg[0]);
d371 1
a371 1
yield()
d392 1
a392 2
preempt(newp)
	struct proc *newp;
d417 1
a417 1
mi_switch()
d541 1
a541 1
rqinit()
d543 1
a543 1
	register int i;
d595 1
a595 2
setrunnable(p)
	register struct proc *p;
d638 1
a638 2
resetpriority(p)
	register struct proc *p;
d640 1
a640 1
	register unsigned int newpriority;
d666 1
a666 2
schedclock(p)
	struct proc *p;
@


1.4
log
@sched work by niklas and art backed out; causes panics
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.1 2004/07/29 06:25:45 tedu Exp $	*/
d384 1
a384 2
	SCHED_ASSERT_UNLOCKED();
	splx(s);
d412 1
a412 2
	SCHED_ASSERT_UNLOCKED();
	splx(s);
d427 1
d441 1
a442 3
#ifdef notyet
		hold_count = spinlock_release_all(&kernel_lock);
#else
a444 1
#endif
d531 1
a531 1
	 * we reacquire the interlock.
a533 3
#ifdef notyet
		spinlock_acquire_count(&kernel_lock, hold_count);
#else
d535 1
a535 1
#endif
@


1.3
log
@Fix yield() to change p_stat from SONPROC to SRUN.
yield() is not used anywhere yet, that's why we didn't notice this.
Noticed by tedu@@ who just started using it.
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.2 2005/05/25 23:17:47 niklas Exp $	*/
d68 1
a68 3
#ifdef MULTIPROCESSOR
struct mutex sched_mutex = MUTEX_INITIALIZER(IPL_SCHED);
#endif
d263 1
a263 1
	int s, t;
a276 1
		SCHED_LOCK(s);
d290 1
a290 2
		if (p->p_slptime > 1) {
			SCHED_UNLOCK(s);
d292 1
a292 2
		}
		t = splstatclock();
d308 2
a309 1
		splx(t);
d358 1
a358 1
	mtx_leave(&sched_mutex);
d364 1
a364 1
	mtx_enter(&sched_mutex);
d383 3
a385 1
	mi_switch(s);
d412 3
a414 1
	mi_switch(s);
d419 1
a419 1
 * Must be called at splsched() or higher.
d422 1
a422 1
mi_switch(int s)
d424 1
a424 1
	struct proc *p = curproc;
d436 14
a486 1
		SCHED_UNLOCK(s);
a493 1
		SCHED_LOCK(s);
a495 14
#if defined(MULTIPROCESSOR)
	/*
	 * Release the kernel_lock, as we are about to yield the CPU.
	 * The scheduler lock is still held until cpu_switch()
	 * selects a new process and removes it from the run queue.
	 */
	if (p->p_flag & P_BIGLOCK)
#ifdef notyet
		hold_count = spinlock_release_all(&kernel_lock);
#else
		hold_count = __mp_release_all(&kernel_lock);
#endif
#endif

d538 3
d543 1
a543 1
	splx(s);
d557 1
d605 2
a606 1
setrunnable(struct proc *p)
d638 1
a638 1
		sched_wakeup((caddr_t)&proc0);
d683 1
a684 1
	p->p_estcpu = ESTCPULIM(p->p_estcpu + 1);
d686 1
a688 1
	SCHED_UNLOCK(s);
@


1.2
log
@This patch is mortly art's work and was done *a year* ago.  Art wants to thank
everyone for the prompt review and ok of this work ;-)  Yeah, that includes me
too, or maybe especially me.  I am sorry.

Change the sched_lock to a mutex. This fixes, among other things, the infamous
"telnet localhost &" problem.  The real bug in that case was that the sched_lock
which is by design a non-recursive lock, was recursively acquired, and not
enough releases made us hold the lock in the idle loop, blocking scheduling
on the other processors.  Some of the other processors would hold the biglock though,
which made it impossible for cpu 0 to enter the kernel...  A nice deadlock.
Let me just say debugging this for days just to realize that it was all fixed
in an old diff noone ever ok'd was somewhat of an anti-climax.

This diff also changes splsched to be correct for all our architectures.
@
text
@d1 1
a1 1
/*	$OpenBSD: sched_bsd.c,v 1.1 2004/07/29 06:25:45 tedu Exp $	*/
d384 1
@


1.1
log
@put the scheduler in its own file.  reduces clutter, and logically separates
"put this process to sleep" and "find a process to run" operations.
no functional change.  ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.60 2004/07/25 20:50:51 tedu Exp $	*/
d68 3
a70 1
struct SIMPLELOCK sched_lock;
d265 1
a265 1
	int s;
d279 1
d293 2
a294 1
		if (p->p_slptime > 1)
d296 2
a297 1
		s = splstatclock();	/* prevent state changes */
d313 1
a313 2
		splx(s);
		SCHED_LOCK(s);
d362 1
a362 1
	SIMPLE_UNLOCK(&sched_lock);
d368 1
a368 1
	SIMPLE_LOCK(&sched_lock);
d386 1
a386 3
	mi_switch();
	SCHED_ASSERT_UNLOCKED();
	splx(s);
d413 1
a413 3
	mi_switch();
	SCHED_ASSERT_UNLOCKED();
	splx(s);
d418 1
a418 1
 * Must be called at splstatclock() or higher.
d421 1
a421 1
mi_switch()
d423 1
a423 1
	struct proc *p = curproc;	/* XXX */
a434 14
#if defined(MULTIPROCESSOR)
	/*
	 * Release the kernel_lock, as we are about to yield the CPU.
	 * The scheduler lock is still held until cpu_switch()
	 * selects a new process and removes it from the run queue.
	 */
	if (p->p_flag & P_BIGLOCK)
#ifdef notyet
		hold_count = spinlock_release_all(&kernel_lock);
#else
		hold_count = __mp_release_all(&kernel_lock);
#endif
#endif

d472 1
d480 1
d483 14
a538 3
#ifdef notyet
		spinlock_acquire_count(&kernel_lock, hold_count);
#else
d541 1
a541 1
#endif
a554 1
	SIMPLE_LOCK_INIT(&sched_lock);
d602 1
a602 2
setrunnable(p)
	register struct proc *p;
d634 1
a634 1
		wakeup((caddr_t)&proc0);
d679 1
a680 1
	SCHED_LOCK(s);
a681 1
	SCHED_UNLOCK(s);
d684 1
@

