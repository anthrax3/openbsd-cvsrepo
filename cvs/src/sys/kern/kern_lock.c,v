head	1.49;
access;
symbols
	OPENBSD_6_1:1.47.0.6
	OPENBSD_6_1_BASE:1.47
	OPENBSD_6_0:1.47.0.2
	OPENBSD_6_0_BASE:1.47
	OPENBSD_5_9:1.46.0.4
	OPENBSD_5_9_BASE:1.46
	OPENBSD_5_8:1.46.0.6
	OPENBSD_5_8_BASE:1.46
	OPENBSD_5_7:1.46.0.2
	OPENBSD_5_7_BASE:1.46
	OPENBSD_5_6:1.45.0.4
	OPENBSD_5_6_BASE:1.45
	OPENBSD_5_5:1.43.0.4
	OPENBSD_5_5_BASE:1.43
	OPENBSD_5_4:1.42.0.2
	OPENBSD_5_4_BASE:1.42
	OPENBSD_5_3:1.38.0.6
	OPENBSD_5_3_BASE:1.38
	OPENBSD_5_2:1.38.0.4
	OPENBSD_5_2_BASE:1.38
	OPENBSD_5_1_BASE:1.38
	OPENBSD_5_1:1.38.0.2
	OPENBSD_5_0:1.37.0.2
	OPENBSD_5_0_BASE:1.37
	OPENBSD_4_9:1.35.0.4
	OPENBSD_4_9_BASE:1.35
	OPENBSD_4_8:1.35.0.2
	OPENBSD_4_8_BASE:1.35
	OPENBSD_4_7:1.34.0.2
	OPENBSD_4_7_BASE:1.34
	OPENBSD_4_6:1.33.0.4
	OPENBSD_4_6_BASE:1.33
	OPENBSD_4_5:1.32.0.2
	OPENBSD_4_5_BASE:1.32
	OPENBSD_4_4:1.31.0.4
	OPENBSD_4_4_BASE:1.31
	OPENBSD_4_3:1.31.0.2
	OPENBSD_4_3_BASE:1.31
	OPENBSD_4_2:1.30.0.2
	OPENBSD_4_2_BASE:1.30
	OPENBSD_4_1:1.24.0.2
	OPENBSD_4_1_BASE:1.24
	OPENBSD_4_0:1.22.0.4
	OPENBSD_4_0_BASE:1.22
	OPENBSD_3_9:1.22.0.2
	OPENBSD_3_9_BASE:1.22
	OPENBSD_3_8:1.19.0.2
	OPENBSD_3_8_BASE:1.19
	OPENBSD_3_7:1.17.0.2
	OPENBSD_3_7_BASE:1.17
	OPENBSD_3_6:1.16.0.2
	OPENBSD_3_6_BASE:1.16
	SMP_SYNC_A:1.15
	SMP_SYNC_B:1.15
	OPENBSD_3_5:1.15.0.4
	OPENBSD_3_5_BASE:1.15
	OPENBSD_3_4:1.15.0.2
	OPENBSD_3_4_BASE:1.15
	UBC_SYNC_A:1.14
	OPENBSD_3_3:1.14.0.4
	OPENBSD_3_3_BASE:1.14
	OPENBSD_3_2:1.14.0.2
	OPENBSD_3_2_BASE:1.14
	OPENBSD_3_1:1.13.0.2
	OPENBSD_3_1_BASE:1.13
	UBC_SYNC_B:1.14
	UBC:1.11.0.2
	UBC_BASE:1.11
	OPENBSD_3_0:1.9.0.12
	OPENBSD_3_0_BASE:1.9
	OPENBSD_2_9_BASE:1.9
	OPENBSD_2_9:1.9.0.10
	OPENBSD_2_8:1.9.0.8
	OPENBSD_2_8_BASE:1.9
	OPENBSD_2_7:1.9.0.6
	OPENBSD_2_7_BASE:1.9
	SMP:1.9.0.4
	SMP_BASE:1.9
	kame_19991208:1.9
	OPENBSD_2_6:1.9.0.2
	OPENBSD_2_6_BASE:1.9
	OPENBSD_2_5:1.8.0.2
	OPENBSD_2_5_BASE:1.8
	OPENBSD_2_4:1.5.0.4
	OPENBSD_2_4_BASE:1.5
	OPENBSD_2_3:1.5.0.2
	OPENBSD_2_3_BASE:1.5;
locks; strict;
comment	@ * @;


1.49
date	2017.04.20.15.06.47;	author visa;	state Exp;
branches;
next	1.48;
commitid	rYfI14CGOkXdTWOm;

1.48
date	2017.04.20.13.20.17;	author visa;	state Exp;
branches;
next	1.47;
commitid	sZ4zTrZbj3VK0Rkg;

1.47
date	2016.06.19.11.54.33;	author natano;	state Exp;
branches;
next	1.46;
commitid	wHLNY5GFNXJSFYaC;

1.46
date	2014.09.14.14.17.25;	author jsg;	state Exp;
branches;
next	1.45;
commitid	uzzBR7hz9ncd4O6G;

1.45
date	2014.07.13.15.46.21;	author uebayasi;	state Exp;
branches;
next	1.44;
commitid	Scu6CelRPdB3AwwA;

1.44
date	2014.07.09.13.32.00;	author guenther;	state Exp;
branches;
next	1.43;
commitid	ZU9xQk5IchFd0Jm2;

1.43
date	2014.01.21.01.48.44;	author tedu;	state Exp;
branches;
next	1.42;

1.42
date	2013.05.06.16.37.55;	author tedu;	state Exp;
branches;
next	1.41;

1.41
date	2013.05.01.17.18.55;	author tedu;	state Exp;
branches;
next	1.40;

1.40
date	2013.05.01.17.13.05;	author tedu;	state Exp;
branches;
next	1.39;

1.39
date	2013.03.28.16.55.25;	author deraadt;	state Exp;
branches;
next	1.38;

1.38
date	2011.08.28.02.35.34;	author guenther;	state Exp;
branches;
next	1.37;

1.37
date	2011.07.06.21.41.37;	author art;	state Exp;
branches;
next	1.36;

1.36
date	2011.07.06.01.49.42;	author art;	state Exp;
branches;
next	1.35;

1.35
date	2010.04.26.05.48.17;	author deraadt;	state Exp;
branches;
next	1.34;

1.34
date	2010.01.14.23.12.11;	author schwarze;	state Exp;
branches;
next	1.33;

1.33
date	2009.03.25.21.20.26;	author oga;	state Exp;
branches;
next	1.32;

1.32
date	2009.01.15.07.48.55;	author grange;	state Exp;
branches;
next	1.31;

1.31
date	2007.11.26.15.23.26;	author art;	state Exp;
branches;
next	1.30;

1.30
date	2007.05.31.22.07.53;	author thib;	state Exp;
branches;
next	1.29;

1.29
date	2007.05.11.10.06.56;	author pedro;	state Exp;
branches;
next	1.28;

1.28
date	2007.05.08.20.25.56;	author deraadt;	state Exp;
branches;
next	1.27;

1.27
date	2007.04.12.22.20.14;	author thib;	state Exp;
branches;
next	1.26;

1.26
date	2007.04.11.12.06.37;	author miod;	state Exp;
branches;
next	1.25;

1.25
date	2007.03.15.10.22.30;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2007.02.14.00.53.48;	author jsg;	state Exp;
branches;
next	1.23;

1.23
date	2007.02.03.16.48.23;	author miod;	state Exp;
branches;
next	1.22;

1.22
date	2006.01.03.15.34.21;	author jmc;	state Exp;
branches;
next	1.21;

1.21
date	2005.11.28.00.14.28;	author jsg;	state Exp;
branches;
next	1.20;

1.20
date	2005.11.19.02.18.01;	author pedro;	state Exp;
branches;
next	1.19;

1.19
date	2005.05.29.03.20.41;	author deraadt;	state Exp;
branches;
next	1.18;

1.18
date	2005.05.25.23.17.47;	author niklas;	state Exp;
branches;
next	1.17;

1.17
date	2004.11.01.06.39.39;	author marius;	state Exp;
branches;
next	1.16;

1.16
date	2004.06.13.21.49.26;	author niklas;	state Exp;
branches;
next	1.15;

1.15
date	2003.06.02.23.28.05;	author millert;	state Exp;
branches;
next	1.14;

1.14
date	2002.07.12.14.03.12;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2002.03.17.18.26.51;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2002.03.14.01.27.04;	author millert;	state Exp;
branches;
next	1.11;

1.11
date	2001.12.04.21.56.18;	author millert;	state Exp;
branches
	1.11.2.1;
next	1.10;

1.10
date	2001.11.07.02.44.10;	author art;	state Exp;
branches;
next	1.9;

1.9
date	99.07.09.15.17.59;	author art;	state Exp;
branches
	1.9.4.1;
next	1.8;

1.8
date	99.01.11.05.12.22;	author millert;	state Exp;
branches;
next	1.7;

1.7
date	98.12.28.19.13.04;	author art;	state Exp;
branches;
next	1.6;

1.6
date	98.11.10.22.20.36;	author art;	state Exp;
branches;
next	1.5;

1.5
date	97.11.07.10.27.43;	author niklas;	state Exp;
branches;
next	1.4;

1.4
date	97.11.06.22.41.06;	author csapuntz;	state Exp;
branches;
next	1.3;

1.3
date	97.11.06.05.58.18;	author csapuntz;	state Exp;
branches;
next	1.2;

1.2
date	97.10.06.20.19.55;	author deraadt;	state dead;
branches;
next	1.1;

1.1
date	97.10.06.15.12.21;	author csapuntz;	state Exp;
branches;
next	;

1.9.4.1
date	2001.11.13.23.04.23;	author niklas;	state Exp;
branches;
next	1.9.4.2;

1.9.4.2
date	2002.03.06.02.13.23;	author niklas;	state Exp;
branches;
next	1.9.4.3;

1.9.4.3
date	2002.03.28.11.43.04;	author niklas;	state Exp;
branches;
next	1.9.4.4;

1.9.4.4
date	2003.03.28.00.41.26;	author niklas;	state Exp;
branches;
next	1.9.4.5;

1.9.4.5
date	2003.05.15.04.08.02;	author niklas;	state Exp;
branches;
next	1.9.4.6;

1.9.4.6
date	2003.05.15.16.45.54;	author niklas;	state Exp;
branches;
next	1.9.4.7;

1.9.4.7
date	2003.05.18.17.41.16;	author niklas;	state Exp;
branches;
next	1.9.4.8;

1.9.4.8
date	2003.05.18.18.25.03;	author niklas;	state Exp;
branches;
next	1.9.4.9;

1.9.4.9
date	2003.06.07.11.03.40;	author ho;	state Exp;
branches;
next	1.9.4.10;

1.9.4.10
date	2004.06.05.23.18.25;	author tedu;	state Exp;
branches;
next	1.9.4.11;

1.9.4.11
date	2004.06.06.23.11.51;	author niklas;	state Exp;
branches;
next	1.9.4.12;

1.9.4.12
date	2004.06.08.21.32.06;	author grange;	state Exp;
branches;
next	1.9.4.13;

1.9.4.13
date	2004.06.13.08.14.06;	author niklas;	state Exp;
branches;
next	;

1.11.2.1
date	2002.06.11.03.29.40;	author art;	state Exp;
branches;
next	1.11.2.2;

1.11.2.2
date	2002.10.29.00.36.44;	author art;	state Exp;
branches;
next	;


desc
@@


1.49
log
@Drop unnecessary headers. This fixes kernel build on platforms
without <machine/mplock.h>.
@
text
@/*	$OpenBSD: kern_lock.c,v 1.48 2017/04/20 13:20:17 visa Exp $	*/

/* 
 * Copyright (c) 1995
 *	The Regents of the University of California.  All rights reserved.
 *
 * This code contains ideas from software contributed to Berkeley by
 * Avadis Tevanian, Jr., Michael Wayne Young, and the Mach Operating
 * System project at Carnegie-Mellon University.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)kern_lock.c	8.18 (Berkeley) 5/21/95
 */

#include <sys/param.h>
#include <sys/lock.h>
#include <sys/systm.h>
#include <sys/sched.h>
#include <sys/witness.h>

#ifdef MP_LOCKDEBUG
/* CPU-dependent timing, this needs to be settable from ddb. */
int __mp_lock_spinout = 200000000;
#endif

#if defined(MULTIPROCESSOR)
/*
 * Functions for manipulating the kernel_lock.  We put them here
 * so that they show up in profiles.
 */

struct __mp_lock kernel_lock;

void
_kernel_lock_init(void)
{
	__mp_lock_init(&kernel_lock);
}

/*
 * Acquire/release the kernel lock.  Intended for use in the scheduler
 * and the lower half of the kernel.
 */

void
_kernel_lock(const char *file, int line)
{
	SCHED_ASSERT_UNLOCKED();
#ifdef WITNESS
	___mp_lock(&kernel_lock, file, line);
#else
	__mp_lock(&kernel_lock);
#endif
}

void
_kernel_unlock(void)
{
	__mp_unlock(&kernel_lock);
}

int
_kernel_lock_held(void)
{
	return (__mp_lock_held(&kernel_lock));
}

#ifdef WITNESS
void
_mp_lock_init(struct __mp_lock *mpl, struct lock_type *type)
{
	mpl->mpl_lock_obj.lo_name = type->lt_name;
	mpl->mpl_lock_obj.lo_type = type;
	if (mpl == &kernel_lock)
		mpl->mpl_lock_obj.lo_flags = LO_WITNESS | LO_INITIALIZED |
		    LO_SLEEPABLE | (LO_CLASS_KERNEL_LOCK << LO_CLASSSHIFT);
	else if (mpl == &sched_lock)
		mpl->mpl_lock_obj.lo_flags = LO_WITNESS | LO_INITIALIZED |
		    LO_RECURSABLE | (LO_CLASS_SCHED_LOCK << LO_CLASSSHIFT);
	WITNESS_INIT(&mpl->mpl_lock_obj, type);

	___mp_lock_init(mpl);
}
#endif /* WITNESS */

#endif /* MULTIPROCESSOR */
@


1.48
log
@Hook up mplock to witness(4) on amd64 and i386.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.47 2016/06/19 11:54:33 natano Exp $	*/
a39 1
#include <sys/mplock.h>
a42 1
#include <sys/_lock.h>
@


1.47
log
@Remove the lockmgr() API. It is only used by filesystems, where it is a
trivial change to use rrw locks instead. All it needs is LK_* defines
for the RW_* flags.

tested by naddy and sthen on package building infrastructure
input and ok jmc mpi tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.46 2014/09/14 14:17:25 jsg Exp $	*/
d40 1
d43 2
d71 1
a71 1
_kernel_lock(void)
d74 3
d78 1
d92 19
@


1.46
log
@remove uneeded proc.h includes
ok mpi@@ kspillner@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.45 2014/07/13 15:46:21 uebayasi Exp $	*/
a46 57

/*
 * Initialize a lock; required before use.
 */
void
lockinit(struct lock *lkp, int prio, char *wmesg, int timo, int flags)
{
	KASSERT(flags == 0);

	memset(lkp, 0, sizeof(struct lock));
	rrw_init(&lkp->lk_lck, wmesg);
}

int
lockstatus(struct lock *lkp)
{
	switch (rrw_status(&lkp->lk_lck)) {
	case RW_WRITE:
		return (LK_EXCLUSIVE);
	case RW_WRITE_OTHER:
		return (LK_EXCLOTHER);
	case RW_READ:
		return (LK_SHARED);
	case 0:
	default:
		return (0);
	}
}

int
lockmgr(struct lock *lkp, u_int flags, void *notused)
{
	int rwflags = 0;

	KASSERT(!((flags & (LK_SHARED|LK_EXCLUSIVE)) ==
	    (LK_SHARED|LK_EXCLUSIVE)));
	KASSERT(!((flags & (LK_CANRECURSE|LK_RECURSEFAIL)) ==
	    (LK_CANRECURSE|LK_RECURSEFAIL)));
	KASSERT((flags & LK_RELEASE) ||
	    (flags & (LK_SHARED|LK_EXCLUSIVE|LK_DRAIN)));

	if (flags & LK_RELEASE) {
		rrw_exit(&lkp->lk_lck);
		return (0);
	}
	if (flags & LK_SHARED)
		rwflags |= RW_READ;
	if (flags & (LK_EXCLUSIVE|LK_DRAIN))
		rwflags |= RW_WRITE;
	if (flags & LK_RECURSEFAIL)
		rwflags |= RW_RECURSEFAIL;
	if (flags & LK_NOWAIT)
		rwflags |= RW_NOSLEEP;

	return (rrw_enter(&lkp->lk_lck, rwflags));

}
@


1.45
log
@KERNEL_ASSERT_LOCKED(9): Assertion for kernel lock (Rev. 3)

This adds a new assertion macro, KERNEL_ASSERT_LOCKED(), to assert that
kernel_lock is held.  In the long process of removing kernel_lock, there will
be a lot (hundreds or thousands) of use of this; virtually almost all functions
in !MP-safe subsystems should have this assertion.  Thus this assertion should
have a short, good name.

Not only that "KERNEL_ASSERT_LOCKED" is consistent with other KERNEL_* and
SCHED_ASSERT_LOCKED() macros.

Input from dlg@@ guenther@@ kettenis@@.

OK dlg@@ guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a38 1
#include <sys/proc.h>
@


1.44
log
@Teach rw_status() and rrw_status() to return LK_EXCLOTHER if it's write
locked by a different thread.  Teach lockstatus() to return LK_EXCLUSIVE
if an exclusive lock is held by some other thread.

ok beck@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.43 2014/01/21 01:48:44 tedu Exp $	*/
d136 6
@


1.43
log
@bzero -> memset
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.42 2013/05/06 16:37:55 tedu Exp $	*/
d67 2
@


1.42
log
@restore original gangster lockstatus return values for compat
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.41 2013/05/01 17:18:55 tedu Exp $	*/
d57 1
a57 1
	bzero(lkp, sizeof(struct lock));
@


1.41
log
@a few tweaks noticed by jsing
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.40 2013/05/01 17:13:05 tedu Exp $	*/
d64 9
a72 1
	return (rrw_status(&lkp->lk_lck));
@


1.40
log
@exorcise lockmgr. the api remains, but is now backed by recursive rwlocks.
originally by thib.
ok deraadt jsing and anyone who tested
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.39 2013/03/28 16:55:25 deraadt Exp $	*/
d45 1
a45 1
/* CPU-dependent timing, needs this to be settable from ddb. */
@


1.39
log
@do not include machine/cpu.h from a .c file; it is the responsibility of
.h files to pull it in, if needed
ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.38 2011/08/28 02:35:34 guenther Exp $	*/
d44 4
a47 33

/*
 * Locking primitives implementation.
 * Locks provide shared/exclusive synchronization.
 */

/*
 * Acquire a resource.  We sleep on the address of the lk_sharecount
 * member normally; if waiting for it to drain we sleep on the address
 * of the lk_waitcount member instead.
 */
#define ACQUIRE(lkp, error, extflags, drain, wanted)			\
do {									\
	for (error = 0; wanted; ) {					\
		if ((drain))						\
			(lkp)->lk_flags |= LK_WAITDRAIN;		\
		else							\
			(lkp)->lk_waitcount++;				\
		error = tsleep((drain) ?				\
		    &(lkp)->lk_waitcount : &(lkp)->lk_sharecount,	\
		    (lkp)->lk_prio, (lkp)->lk_wmesg, (lkp)->lk_timo);	\
		if ((drain) == 0)					\
			(lkp)->lk_waitcount--;				\
		if (error)						\
			break;						\
	}								\
} while (0)

#define	SETHOLDER(lkp, pid, cpu_id)					\
	(lkp)->lk_lockholder = (pid)

#define	WEHOLDIT(lkp, pid, cpu_id)					\
	((lkp)->lk_lockholder == (pid))
d55 1
d58 1
a58 5
	lkp->lk_flags = flags & LK_EXTFLG_MASK;
	lkp->lk_lockholder = LK_NOPROC;
	lkp->lk_prio = prio;
	lkp->lk_timo = timo;
	lkp->lk_wmesg = wmesg;	/* just a name for spin locks */
a60 3
/*
 * Determine the status of a lock.
 */
d64 1
a64 7
	int lock_type = 0;

	if (lkp->lk_exclusivecount != 0)
		lock_type = LK_EXCLUSIVE;
	else if (lkp->lk_sharecount != 0)
		lock_type = LK_SHARED;
	return (lock_type);
a66 7
/*
 * Set, change, or release a lock.
 *
 * Shared requests increment the shared count. Exclusive requests set the
 * LK_WANT_EXCL flag (preventing further shared locks), and wait for already
 * accepted shared locks and shared-to-exclusive upgrades to go away.
 */
d68 1
a68 1
lockmgr(__volatile struct lock *lkp, u_int flags, void *notused)
d70 1
a70 38
	int error;
	pid_t pid;
	int extflags;
	cpuid_t cpu_id;
	struct proc *p = curproc;

	error = 0;
	extflags = (flags | lkp->lk_flags) & LK_EXTFLG_MASK;

#ifdef DIAGNOSTIC
	if (p == NULL)
		panic("lockmgr: process context required");
#endif		
	/* Process context required. */
	pid = p->p_pid;
	cpu_id = cpu_number();

	/*
	 * Once a lock has drained, the LK_DRAINING flag is set and an
	 * exclusive lock is returned. The only valid operation thereafter
	 * is a single release of that exclusive lock. This final release
	 * clears the LK_DRAINING flag and sets the LK_DRAINED flag. Any
	 * further requests of any sort will result in a panic. The bits
	 * selected for these two flags are chosen so that they will be set
	 * in memory that is freed (freed memory is filled with 0xdeadbeef).
	 */
	if (lkp->lk_flags & (LK_DRAINING|LK_DRAINED)) {
#ifdef DIAGNOSTIC
		if (lkp->lk_flags & LK_DRAINED)
			panic("lockmgr: using decommissioned lock");
		if ((flags & LK_TYPE_MASK) != LK_RELEASE ||
		    WEHOLDIT(lkp, pid, cpu_id) == 0)
			panic("lockmgr: non-release on draining lock: %d",
			    flags & LK_TYPE_MASK);
#endif /* DIAGNOSTIC */
		lkp->lk_flags &= ~LK_DRAINING;
		lkp->lk_flags |= LK_DRAINED;
	}
d72 10
a81 149
	/*
	 * Check if the caller is asking us to be schizophrenic.
	 */
	if ((lkp->lk_flags & (LK_CANRECURSE|LK_RECURSEFAIL)) ==
	    (LK_CANRECURSE|LK_RECURSEFAIL))
		panic("lockmgr: make up your mind");

	switch (flags & LK_TYPE_MASK) {

	case LK_SHARED:
		if (WEHOLDIT(lkp, pid, cpu_id) == 0) {
			/*
			 * If just polling, check to see if we will block.
			 */
			if ((extflags & LK_NOWAIT) && (lkp->lk_flags &
			    (LK_HAVE_EXCL | LK_WANT_EXCL))) {
				error = EBUSY;
				break;
			}
			/*
			 * Wait for exclusive locks and upgrades to clear.
			 */
			ACQUIRE(lkp, error, extflags, 0, lkp->lk_flags &
			    (LK_HAVE_EXCL | LK_WANT_EXCL));
			if (error)
				break;
			lkp->lk_sharecount++;
			break;
		}
		/*
		 * We hold an exclusive lock, so downgrade it to shared.
		 * An alternative would be to fail with EDEADLK.
		 */
		lkp->lk_sharecount++;

		if (WEHOLDIT(lkp, pid, cpu_id) == 0 ||
		    lkp->lk_exclusivecount == 0)
			panic("lockmgr: not holding exclusive lock");
		lkp->lk_sharecount += lkp->lk_exclusivecount;
		lkp->lk_exclusivecount = 0;
		lkp->lk_flags &= ~LK_HAVE_EXCL;
		SETHOLDER(lkp, LK_NOPROC, LK_NOCPU);
		if (lkp->lk_waitcount)
			wakeup(&lkp->lk_sharecount);
		break;

	case LK_EXCLUSIVE:
		if (WEHOLDIT(lkp, pid, cpu_id)) {
			/*
			 * Recursive lock.
			 */
			if ((extflags & LK_CANRECURSE) == 0) {
				if (extflags & LK_RECURSEFAIL) {
					error = EDEADLK;
					break;
				} else
					panic("lockmgr: locking against myself");
			}
			lkp->lk_exclusivecount++;
			break;
		}
		/*
		 * If we are just polling, check to see if we will sleep.
		 */
		if ((extflags & LK_NOWAIT) && ((lkp->lk_flags &
		     (LK_HAVE_EXCL | LK_WANT_EXCL)) ||
		     lkp->lk_sharecount != 0)) {
			error = EBUSY;
			break;
		}
		/*
		 * Try to acquire the want_exclusive flag.
		 */
		ACQUIRE(lkp, error, extflags, 0, lkp->lk_flags &
		    (LK_HAVE_EXCL | LK_WANT_EXCL));
		if (error)
			break;
		lkp->lk_flags |= LK_WANT_EXCL;
		/*
		 * Wait for shared locks and upgrades to finish.
		 */
		ACQUIRE(lkp, error, extflags, 0, lkp->lk_sharecount != 0);
		lkp->lk_flags &= ~LK_WANT_EXCL;
		if (error)
			break;
		lkp->lk_flags |= LK_HAVE_EXCL;
		SETHOLDER(lkp, pid, cpu_id);
		if (lkp->lk_exclusivecount != 0)
			panic("lockmgr: non-zero exclusive count");
		lkp->lk_exclusivecount = 1;
		break;

	case LK_RELEASE:
		if (lkp->lk_exclusivecount != 0) {
			if (WEHOLDIT(lkp, pid, cpu_id) == 0) {
				panic("lockmgr: pid %d, not exclusive lock "
				    "holder %d unlocking",
				    pid, lkp->lk_lockholder);
			}
			lkp->lk_exclusivecount--;
			if (lkp->lk_exclusivecount == 0) {
				lkp->lk_flags &= ~LK_HAVE_EXCL;
				SETHOLDER(lkp, LK_NOPROC, LK_NOCPU);
			}
		} else if (lkp->lk_sharecount != 0) {
			lkp->lk_sharecount--;
		}
#ifdef DIAGNOSTIC
		else
			panic("lockmgr: release of unlocked lock!");
#endif
		if (lkp->lk_waitcount)
			wakeup(&lkp->lk_sharecount);
		break;

	case LK_DRAIN:
		/*
		 * Check that we do not already hold the lock, as it can 
		 * never drain if we do. Unfortunately, we have no way to
		 * check for holding a shared lock, but at least we can
		 * check for an exclusive one.
		 */
		if (WEHOLDIT(lkp, pid, cpu_id))
			panic("lockmgr: draining against myself");
		/*
		 * If we are just polling, check to see if we will sleep.
		 */
		if ((extflags & LK_NOWAIT) && ((lkp->lk_flags &
		     (LK_HAVE_EXCL | LK_WANT_EXCL)) ||
		     lkp->lk_sharecount != 0 || lkp->lk_waitcount != 0)) {
			error = EBUSY;
			break;
		}
		ACQUIRE(lkp, error, extflags, 1,
		    ((lkp->lk_flags &
		     (LK_HAVE_EXCL | LK_WANT_EXCL)) ||
		     lkp->lk_sharecount != 0 ||
		     lkp->lk_waitcount != 0));
		if (error)
			break;
		lkp->lk_flags |= LK_DRAINING | LK_HAVE_EXCL;
		SETHOLDER(lkp, pid, cpu_id);
		lkp->lk_exclusivecount = 1;
		break;

	default:
		panic("lockmgr: unknown locktype request %d",
		    flags & LK_TYPE_MASK);
		/* NOTREACHED */
d83 8
a90 9
	if ((lkp->lk_flags & LK_WAITDRAIN) != 0 &&
	    ((lkp->lk_flags &
	    (LK_HAVE_EXCL | LK_WANT_EXCL)) == 0 &&
	    lkp->lk_sharecount == 0 && lkp->lk_waitcount == 0)) {
		lkp->lk_flags &= ~LK_WAITDRAIN;
		wakeup(&lkp->lk_waitcount);
	}
	return (error);
}
d92 1
a92 8
#ifdef DIAGNOSTIC
/*
 * Print out information about state of a lock. Used by VOP_PRINT
 * routines to display status about contained locks.
 */
void
lockmgr_printinfo(__volatile struct lock *lkp)
{
a93 11
	if (lkp->lk_sharecount)
		printf(" lock type %s: SHARED (count %d)", lkp->lk_wmesg,
		    lkp->lk_sharecount);
	else if (lkp->lk_flags & LK_HAVE_EXCL) {
		printf(" lock type %s: EXCL (count %d) by ",
		    lkp->lk_wmesg, lkp->lk_exclusivecount);
		printf("pid %d", lkp->lk_lockholder);
	} else
		printf(" not locked");
	if (lkp->lk_waitcount > 0)
		printf(" with %d pending", lkp->lk_waitcount);
a94 1
#endif /* DIAGNOSTIC */
d102 1
a102 1
struct __mp_lock kernel_lock; 
a126 6

#ifdef MP_LOCKDEBUG
/* CPU-dependent timing, needs this to be settable from ddb. */
int __mp_lock_spinout = 200000000;
#endif

@


1.38
log
@lockmgr() wants to use a different address for the wchan when draining
the lock, but a change in member ordering meant it was using the same
address.  Explicitly use different members instead of mixing address
of member and address of the lock itself.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.37 2011/07/06 21:41:37 art Exp $	*/
a43 1
#include <machine/cpu.h>
@


1.37
log
@Clean up after P_BIGLOCK removal.
KERNEL_PROC_LOCK -> KERNEL_LOCK
KERNEL_PROC_UNLOCK -> KERNEL_UNLOCK

oga@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.36 2011/07/06 01:49:42 art Exp $	*/
d52 3
a54 1
 * Acquire a resource.
a62 1
		/* XXX Cast away volatile. */				\
d64 1
a64 1
		    (void *)&(lkp)->lk_flags : (void *)(lkp),		\
d201 1
a201 1
			wakeup((void *)(lkp));
d270 1
a270 1
			wakeup((void *)(lkp));
d313 1
a313 1
		wakeup((void *)&lkp->lk_flags);
@


1.36
log
@Stop using the P_BIGLOCK flag to figure out when we should release the
biglock in mi_switch and just check if we're holding the biglock.

The idea is that the first entry point into the kernel uses KERNEL_PROC_LOCK
and recursive calls use KERNEL_LOCK. This assumption is violated in at
least one place and has been causing confusion for lots of people.

Initial bug report and analysis from Pedro.

kettenis@@ beck@@ oga@@ thib@@ dlg@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.35 2010/04/26 05:48:17 deraadt Exp $	*/
a367 17
{
	__mp_unlock(&kernel_lock);
}

/*
 * Acquire/release the kernel_lock on behalf of a process.  Intended for
 * use in the top half of the kernel.
 */
void
_kernel_proc_lock(struct proc *p)
{
	SCHED_ASSERT_UNLOCKED();
	__mp_lock(&kernel_lock);
}

void
_kernel_proc_unlock(struct proc *p)
@


1.35
log
@cut down simple locks (so simple that they don't even lock) to the point
where there is almost nothing left to them, so that we can continue getting
rid of them
ok oga
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.34 2010/01/14 23:12:11 schwarze Exp $	*/
a380 1
	atomic_setbits_int(&p->p_flag, P_BIGLOCK);
a385 1
	atomic_clearbits_int(&p->p_flag, P_BIGLOCK);
@


1.34
log
@fix typos in comments, no code changes;
from Brad Tilley <brad at 16systems dot com>;
ok oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.33 2009/03/25 21:20:26 oga Exp $	*/
a45 13
#ifndef spllock
#define spllock() splhigh()
#endif

#ifdef MULTIPROCESSOR
#define CPU_NUMBER() cpu_number()
#else
#define CPU_NUMBER() 0
#endif

void record_stacktrace(int *, int);
void playback_stacktrace(int *, int);

a50 15
#ifdef DDB /* { */
#ifdef MULTIPROCESSOR
int simple_lock_debugger = 1;	/* more serious on MP */
#else
int simple_lock_debugger = 0;
#endif
#define	SLOCK_DEBUGGER()	if (simple_lock_debugger) Debugger()
#define	SLOCK_TRACE()							\
	db_stack_trace_print((db_expr_t)__builtin_frame_address(0),	\
	    TRUE, 65535, "", lock_printf);
#else
#define	SLOCK_DEBUGGER()	/* nothing */
#define	SLOCK_TRACE()		/* nothing */
#endif /* } */

a77 38
#define	WAKEUP_WAITER(lkp)						\
do {									\
	if ((lkp)->lk_waitcount) 				{	\
		/* XXX Cast away volatile. */				\
		wakeup((void *)(lkp));					\
	}								\
} while (/*CONSTCOND*/0)

#define	HAVEIT(lkp)							\
do {									\
} while (/*CONSTCOND*/0)

#define	DONTHAVEIT(lkp)							\
do {									\
} while (/*CONSTCOND*/0)

#if defined(LOCKDEBUG)
/*
 * Lock debug printing routine; can be configured to print to console
 * or log to syslog.
 */
void
lock_printf(const char *fmt, ...)
{
	char b[150];
	va_list ap;

	va_start(ap, fmt);
	if (lock_debug_syslog)
		vlog(LOG_DEBUG, fmt, ap);
	else {
		vsnprintf(b, sizeof(b), fmt, ap);
		printf_nolog("%s", b);
	}
	va_end(ap);
}
#endif /* LOCKDEBUG */

a90 4
#if defined(LOCKDEBUG)
	lkp->lk_lock_file = NULL;
	lkp->lk_unlock_file = NULL;
#endif
d116 1
a116 1
lockmgr(__volatile struct lock *lkp, u_int flags, struct simplelock *interlkp)
d133 1
a133 1
	cpu_id = CPU_NUMBER();
d199 2
a200 6
#if defined(LOCKDEBUG)
		lkp->lk_unlock_file = file;
		lkp->lk_unlock_line = line;
#endif
		DONTHAVEIT(lkp);
		WAKEUP_WAITER(lkp);
a243 5
#if defined(LOCKDEBUG)
		lkp->lk_lock_file = file;
		lkp->lk_lock_line = line;
#endif
		HAVEIT(lkp);
a259 5
#if defined(LOCKDEBUG)
				lkp->lk_unlock_file = file;
				lkp->lk_unlock_line = line;
#endif
				DONTHAVEIT(lkp);
d268 2
a269 1
		WAKEUP_WAITER(lkp);
a298 5
#if defined(LOCKDEBUG)
		lkp->lk_lock_file = file;
		lkp->lk_lock_line = line;
#endif
		HAVEIT(lkp);
a338 320

#if defined(LOCKDEBUG)
TAILQ_HEAD(, simplelock) simplelock_list =
    TAILQ_HEAD_INITIALIZER(simplelock_list);

#if defined(MULTIPROCESSOR) /* { */
struct simplelock simplelock_list_slock = SIMPLELOCK_INITIALIZER;

#define	SLOCK_LIST_LOCK()						\
	__cpu_simple_lock(&simplelock_list_slock.lock_data)

#define	SLOCK_LIST_UNLOCK()						\
	__cpu_simple_unlock(&simplelock_list_slock.lock_data)

#define	SLOCK_COUNT(x)							\
	curcpu()->ci_simple_locks += (x)
#else
u_long simple_locks;

#define	SLOCK_LIST_LOCK()	/* nothing */

#define	SLOCK_LIST_UNLOCK()	/* nothing */

#define	SLOCK_COUNT(x)		simple_locks += (x)
#endif /* MULTIPROCESSOR */ /* } */

#ifdef MULTIPROCESSOR
#define SLOCK_MP()		lock_printf("on cpu %ld\n", 		\
				    (u_long) cpu_number())
#else
#define SLOCK_MP()		/* nothing */
#endif

#define	SLOCK_WHERE(str, alp, id, l)					\
do {									\
	lock_printf("\n");						\
	lock_printf(str);						\
	lock_printf("lock: %p, currently at: %s:%d\n", (alp), (id), (l)); \
	SLOCK_MP();							\
	if ((alp)->lock_file != NULL)					\
		lock_printf("last locked: %s:%d\n", (alp)->lock_file,	\
		    (alp)->lock_line);					\
	if ((alp)->unlock_file != NULL)					\
		lock_printf("last unlocked: %s:%d\n", (alp)->unlock_file, \
		    (alp)->unlock_line);				\
	SLOCK_TRACE()							\
	SLOCK_DEBUGGER();						\
} while (/*CONSTCOND*/0)

/*
 * Simple lock functions so that the debugger can see from whence
 * they are being called.
 */
void
simple_lock_init(struct simplelock *lkp)
{

#if defined(MULTIPROCESSOR) /* { */
	__cpu_simple_lock_init(&alp->lock_data);
#else
	alp->lock_data = __SIMPLELOCK_UNLOCKED;
#endif /* } */
	alp->lock_file = NULL;
	alp->lock_line = 0;
	alp->unlock_file = NULL;
	alp->unlock_line = 0;
	alp->lock_holder = LK_NOCPU;
}

void
_simple_lock(__volatile struct simplelock *lkp, const char *id, int l)
{
	cpuid_t cpu_id = CPU_NUMBER();
	int s;

	s = spllock();

	/*
	 * MULTIPROCESSOR case: This is `safe' since if it's not us, we
	 * don't take any action, and just fall into the normal spin case.
	 */
	if (alp->lock_data == __SIMPLELOCK_LOCKED) {
#if defined(MULTIPROCESSOR) /* { */
		if (alp->lock_holder == cpu_id) {
			SLOCK_WHERE("simple_lock: locking against myself\n",
			    alp, id, l);
			goto out;
		}
#else
		SLOCK_WHERE("simple_lock: lock held\n", alp, id, l);
		goto out;
#endif /* MULTIPROCESSOR */ /* } */
	}

#if defined(MULTIPROCESSOR) /* { */
	/* Acquire the lock before modifying any fields. */
	splx(s);
	__cpu_simple_lock(&alp->lock_data);
	s = spllock();
#else
	alp->lock_data = __SIMPLELOCK_LOCKED;
#endif /* } */

	if (alp->lock_holder != LK_NOCPU) {
		SLOCK_WHERE("simple_lock: uninitialized lock\n",
		    alp, id, l);
	}
	alp->lock_file = id;
	alp->lock_line = l;
	alp->lock_holder = cpu_id;

	SLOCK_LIST_LOCK();
	/* XXX Cast away volatile */
	TAILQ_INSERT_TAIL(&simplelock_list, (struct simplelock *)alp, list);
	SLOCK_LIST_UNLOCK();

	SLOCK_COUNT(1);

 out:
	splx(s);
}

int
_simple_lock_held(__volatile struct simplelock *alp)
{
	cpuid_t cpu_id = CPU_NUMBER();
	int s, locked = 0;

	s = spllock();

#if defined(MULTIPROCESSOR)
	if (__cpu_simple_lock_try(&alp->lock_data) == 0)
		locked = (alp->lock_holder == cpu_id);
	else
		__cpu_simple_unlock(&alp->lock_data);
#else
	if (alp->lock_data == __SIMPLELOCK_LOCKED) {
		locked = 1;
		KASSERT(alp->lock_holder == cpu_id);
	}
#endif

	splx(s);

	return (locked);
}

int
_simple_lock_try(__volatile struct simplelock *lkp, const char *id, int l)
{
	cpuid_t cpu_id = CPU_NUMBER();
	int s, rv = 0;

	s = spllock();

	/*
	 * MULTIPROCESSOR case: This is `safe' since if it's not us, we
	 * don't take any action.
	 */
#if defined(MULTIPROCESSOR) /* { */
	if ((rv = __cpu_simple_lock_try(&alp->lock_data)) == 0) {
		if (alp->lock_holder == cpu_id)
			SLOCK_WHERE("simple_lock_try: locking against myself\n",
			    alp, id, l);
		goto out;
	}
#else
	if (alp->lock_data == __SIMPLELOCK_LOCKED) {
		SLOCK_WHERE("simple_lock_try: lock held\n", alp, id, l);
		goto out;
	}
	alp->lock_data = __SIMPLELOCK_LOCKED;
#endif /* MULTIPROCESSOR */ /* } */

	/*
	 * At this point, we have acquired the lock.
	 */

	rv = 1;

	alp->lock_file = id;
	alp->lock_line = l;
	alp->lock_holder = cpu_id;

	SLOCK_LIST_LOCK();
	/* XXX Cast away volatile. */
	TAILQ_INSERT_TAIL(&simplelock_list, (struct simplelock *)alp, list);
	SLOCK_LIST_UNLOCK();

	SLOCK_COUNT(1);

 out:
	splx(s);
	return (rv);
}

void
_simple_unlock(__volatile struct simplelock *lkp, const char *id, int l)
{
	int s;

	s = spllock();

	/*
	 * MULTIPROCESSOR case: This is `safe' because we think we hold
	 * the lock, and if we don't, we don't take any action.
	 */
	if (alp->lock_data == __SIMPLELOCK_UNLOCKED) {
		SLOCK_WHERE("simple_unlock: lock not held\n",
		    alp, id, l);
		goto out;
	}

	SLOCK_LIST_LOCK();
	TAILQ_REMOVE(&simplelock_list, alp, list);
	SLOCK_LIST_UNLOCK();

	SLOCK_COUNT(-1);

	alp->list.tqe_next = NULL;	/* sanity */
	alp->list.tqe_prev = NULL;	/* sanity */

	alp->unlock_file = id;
	alp->unlock_line = l;

#if defined(MULTIPROCESSOR) /* { */
	alp->lock_holder = LK_NOCPU;
	/* Now that we've modified all fields, release the lock. */
	__cpu_simple_unlock(&alp->lock_data);
#else
	alp->lock_data = __SIMPLELOCK_UNLOCKED;
	KASSERT(alp->lock_holder == CPU_NUMBER());
	alp->lock_holder = LK_NOCPU;
#endif /* } */

 out:
	splx(s);
}

void
simple_lock_dump(void)
{
	struct simplelock *alp;
	int s;

	s = spllock();
	SLOCK_LIST_LOCK();
	lock_printf("all simple locks:\n");
	TAILQ_FOREACH(alp, &simplelock_list, list) {
		lock_printf("%p CPU %lu %s:%d\n", alp, alp->lock_holder,
		    alp->lock_file, alp->lock_line);
	}
	SLOCK_LIST_UNLOCK();
	splx(s);
}

void
simple_lock_freecheck(void *start, void *end)
{
	struct simplelock *alp;
	int s;

	s = spllock();
	SLOCK_LIST_LOCK();
	TAILQ_FOREACH(alp, &simplelock_list, list) {
		if ((void *)alp >= start && (void *)alp < end) {
			lock_printf("freeing simple_lock %p CPU %lu %s:%d\n",
			    alp, alp->lock_holder, alp->lock_file,
			    alp->lock_line);
			SLOCK_DEBUGGER();
		}
	}
	SLOCK_LIST_UNLOCK();
	splx(s);
 }

/*
 * We must be holding exactly one lock: the sched_lock.
 */

#ifdef notyet
void
simple_lock_switchcheck(void)
{

	simple_lock_only_held(&sched_lock, "switching");
}
#endif

void
simple_lock_only_held(volatile struct simplelock *lp, const char *where)
{
	struct simplelock *alp;
	cpuid_t cpu_id = CPU_NUMBER();
	int s;

	if (lp) {
		LOCK_ASSERT(simple_lock_held(lp));
	}
	s = spllock();
	SLOCK_LIST_LOCK();
	TAILQ_FOREACH(alp, &simplelock_list, list) {
		if (alp == lp)
			continue;
		if (alp->lock_holder == cpu_id)
			break;
	}
	SLOCK_LIST_UNLOCK();
	splx(s);

	if (alp != NULL) {
		lock_printf("\n%s with held simple_lock %p "
		    "CPU %lu %s:%d\n",
		    where, alp, alp->lock_holder, alp->lock_file,
		    alp->lock_line);
		SLOCK_TRACE();
		SLOCK_DEBUGGER();
	}
}
#endif /* LOCKDEBUG */
@


1.33
log
@ntfs was the last user, LK_SLEEFAIL can die now.

ok blambert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.32 2009/01/15 07:48:55 grange Exp $	*/
d408 1
a408 1
 * routines to display ststus about contained locks.
@


1.32
log
@Surround WEHOLDIT() macro with braces to make it more safe.
No binary change.

ok otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.31 2007/11/26 15:23:26 art Exp $	*/
a96 4
		if ((extflags) & LK_SLEEPFAIL) {			\
			error = ENOLCK;					\
			break;						\
		}							\
@


1.31
log
@Remove some dead code that is confusing my greps.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.30 2007/05/31 22:07:53 thib Exp $	*/
d108 1
a108 1
	(lkp)->lk_lockholder == (pid)
@


1.30
log
@remove p_lock from struct proc; unused debug goo for lockmgr,
wich gets set and never checked etc...

ok art@@,tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.29 2007/05/11 10:06:56 pedro Exp $	*/
a116 19

#if defined(LOCKDEBUG) /* { */
#if defined(MULTIPROCESSOR) /* { */
struct simplelock spinlock_list_slock = SIMPLELOCK_INITIALIZER;

#define	SPINLOCK_LIST_LOCK()						\
	__cpu_simple_lock(&spinlock_list_slock.lock_data)

#define	SPINLOCK_LIST_UNLOCK()						\
	__cpu_simple_unlock(&spinlock_list_slock.lock_data)
#else
#define	SPINLOCK_LIST_LOCK()	/* nothing */

#define	SPINLOCK_LIST_UNLOCK()	/* nothing */
#endif /* MULTIPROCESSOR */ /* } */

TAILQ_HEAD(, lock) spinlock_list =
    TAILQ_HEAD_INITIALIZER(spinlock_list);
#endif /* LOCKDEBUG */ /* } */
@


1.29
log
@Don't use LK_CANRECURSE for the kernel lock, okay miod@@ art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.28 2007/05/08 20:25:56 deraadt Exp $	*/
a63 7
#if defined(LOCKDEBUG) || defined(DIAGNOSTIC) /* { */
#define	COUNT(lkp, p, cpu_id, x)					\
	(p)->p_locks += (x)
#else
#define COUNT(lkp, p, cpu_id, x)
#endif /* LOCKDEBUG || DIAGNOSTIC */ /* } */

a276 1
			COUNT(lkp, p, cpu_id, 1);
a283 1
		COUNT(lkp, p, cpu_id, 1);
a312 1
			COUNT(lkp, p, cpu_id, 1);
a348 1
		COUNT(lkp, p, cpu_id, 1);
a358 1
			COUNT(lkp, p, cpu_id, -1);
a369 1
			COUNT(lkp, p, cpu_id, -1);
a410 1
		COUNT(lkp, p, cpu_id, 1);
@


1.28
log
@lockmgr_printinfo() is only called from #ifdef DIAGNOSTIC positions, so #ifdef DIAGNOSTIC it too
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.27 2007/04/12 22:20:14 thib Exp $	*/
a803 1
/* XXX The flag should go, all callers want equal behaviour. */
d805 1
a805 1
_kernel_lock(int flag)
@


1.27
log
@Remove the lk_interlock from struct lock; Also remove the LK_INTERLOCK
flag. This effectively makes the simplelock argument to lockmgr() fluff.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.26 2007/04/11 12:06:37 miod Exp $	*/
d442 1
d463 1
@


1.26
log
@lockmgr keeps losing code, call 911!

ok pedro@@ art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.25 2007/03/15 10:22:30 art Exp $	*/
a70 10
#define	INTERLOCK_ACQUIRE(lkp, flags)					\
do {									\
	simple_lock(&(lkp)->lk_interlock);				\
} while (/*CONSTCOND*/ 0)

#define	INTERLOCK_RELEASE(lkp, flags)					\
do {									\
	simple_unlock(&(lkp)->lk_interlock);				\
} while (/*CONSTCOND*/ 0)

d97 1
a97 1
		error = ltsleep((drain) ?				\
d99 1
a99 2
		    (lkp)->lk_prio, (lkp)->lk_wmesg, (lkp)->lk_timo,	\
		    &(lkp)->lk_interlock);				\
a181 1
	simple_lock_init(&lkp->lk_interlock);
a200 1
	INTERLOCK_ACQUIRE(lkp, lkp->lk_flags);
a204 1
	INTERLOCK_RELEASE(lkp, lkp->lk_flags);
a224 4

	INTERLOCK_ACQUIRE(lkp, lkp->lk_flags);
	if (flags & LK_INTERLOCK)
		simple_unlock(interlkp);
a427 1
		INTERLOCK_RELEASE(lkp, lkp->lk_flags);
a438 1
	INTERLOCK_RELEASE(lkp, lkp->lk_flags);
@


1.25
log
@Since p_flag is often manipulated in interrupts and without biglock
it's a good idea to use atomic.h operations on it. This mechanic
change updates all bit operations on p_flag to atomic_{set,clear}bits_int.

Only exception is that P_OWEUPC is set by MI code before calling
need_proftick and it's automatically cleared by ADDUPC. There's
no reason for MD handling of that flag since everyone handles it the
same way.

kettenis@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.24 2007/02/14 00:53:48 jsg Exp $	*/
d290 1
a290 1
			    (LK_HAVE_EXCL | LK_WANT_EXCL | LK_WANT_UPGRADE))) {
d298 1
a298 1
			    (LK_HAVE_EXCL | LK_WANT_EXCL | LK_WANT_UPGRADE));
a310 1
		/* FALLTHROUGH */
a311 1
	case LK_DOWNGRADE:
a326 55
	case LK_UPGRADE:
		/*
		 * Upgrade a shared lock to an exclusive one. If another
		 * shared lock has already requested an upgrade to an
		 * exclusive lock, our shared lock is released and an
		 * exclusive lock is requested (which will be granted
		 * after the upgrade). If we return an error, the file
		 * will always be unlocked.
		 */
		if (WEHOLDIT(lkp, pid, cpu_id) || lkp->lk_sharecount <= 0)
			panic("lockmgr: upgrade exclusive lock");
		lkp->lk_sharecount--;
		COUNT(lkp, p, cpu_id, -1);
		/*
		 * If we are just polling, check to see if we will block.
		 */
		if ((extflags & LK_NOWAIT) &&
		    ((lkp->lk_flags & LK_WANT_UPGRADE) ||
		     lkp->lk_sharecount > 1)) {
			error = EBUSY;
			break;
		}
		if ((lkp->lk_flags & LK_WANT_UPGRADE) == 0) {
			/*
			 * We are first shared lock to request an upgrade, so
			 * request upgrade and wait for the shared count to
			 * drop to zero, then take exclusive lock.
			 */
			lkp->lk_flags |= LK_WANT_UPGRADE;
			ACQUIRE(lkp, error, extflags, 0, lkp->lk_sharecount);
			lkp->lk_flags &= ~LK_WANT_UPGRADE;
			if (error)
				break;
			lkp->lk_flags |= LK_HAVE_EXCL;
			SETHOLDER(lkp, pid, cpu_id);
#if defined(LOCKDEBUG)
			lkp->lk_lock_file = file;
			lkp->lk_lock_line = line;
#endif
			HAVEIT(lkp);
			if (lkp->lk_exclusivecount != 0)
				panic("lockmgr: non-zero exclusive count");
			lkp->lk_exclusivecount = 1;
			COUNT(lkp, p, cpu_id, 1);
			break;
		}
		/*
		 * Someone else has requested upgrade. Release our shared
		 * lock, awaken upgrade requestor if we are the last shared
		 * lock, then request an exclusive lock.
		 */
		if (lkp->lk_sharecount == 0)
			WAKEUP_WAITER(lkp);
		/* FALLTHROUGH */

d347 1
a347 1
		     (LK_HAVE_EXCL | LK_WANT_EXCL | LK_WANT_UPGRADE)) ||
d363 1
a363 2
		ACQUIRE(lkp, error, extflags, 0, lkp->lk_sharecount != 0 ||
		       (lkp->lk_flags & LK_WANT_UPGRADE));
d422 1
a422 1
		     (LK_HAVE_EXCL | LK_WANT_EXCL | LK_WANT_UPGRADE)) ||
d429 1
a429 1
		     (LK_HAVE_EXCL | LK_WANT_EXCL | LK_WANT_UPGRADE)) ||
d453 1
a453 1
	    (LK_HAVE_EXCL | LK_WANT_EXCL | LK_WANT_UPGRADE)) == 0 &&
@


1.24
log
@Consistently spell FALLTHROUGH to appease lint.
ok kettenis@@ cloder@@ tom@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.23 2007/02/03 16:48:23 miod Exp $	*/
d903 1
a903 1
	p->p_flag |= P_BIGLOCK;
d909 1
a909 1
	p->p_flag &= ~P_BIGLOCK;
@


1.23
log
@Remove unused functionality from lockmgr():
- LK_EXCLUPGRADE is never used.
- LK_REENABLE is never used.
- LK_SETRECURSE is never used. Because of this, the lk_recurselevel
  field is always zero, so it can be removed to.
- the spinlock version (and LK_SPIN) is never used, since it was decided
  to use different locking structure for MP-safe protection.

Tested by many
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.22 2006/01/03 15:34:21 jmc Exp $	*/
d311 1
a311 1
		/* fall into downgrade */
d382 1
a382 1
		/* fall into exclusive request */
@


1.22
log
@remove duplicate comment;
from thordur i. bjornsson;
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.21 2005/11/28 00:14:28 jsg Exp $	*/
a64 8
#if defined(MULTIPROCESSOR) /* { */
#define	COUNT_CPU(cpu_id, x)						\
	curcpu()->ci_spin_locks += (x)
#else
u_long	spin_locks;
#define	COUNT_CPU(cpu_id, x)	spin_locks += (x)
#endif /* MULTIPROCESSOR */ /* } */

d66 1
a66 6
do {									\
	if ((lkp)->lk_flags & LK_SPIN)					\
		COUNT_CPU((cpu_id), (x));				\
	else								\
		(p)->p_locks += (x);					\
} while (/*CONSTCOND*/0)
a68 1
#define COUNT_CPU(cpu_id, x)
d71 1
a71 5
#ifndef SPINLOCK_SPIN_HOOK		/* from <machine/lock.h> */
#define	SPINLOCK_SPIN_HOOK		/* nothing */
#endif

#define	INTERLOCK_ACQUIRE(lkp, flags, s)				\
a72 2
	if ((flags) & LK_SPIN)						\
		s = spllock();						\
d76 1
a76 1
#define	INTERLOCK_RELEASE(lkp, flags, s)				\
a78 2
	if ((flags) & LK_SPIN)						\
		splx(s);						\
a95 34
#if defined(LOCKDEBUG)
#if defined(DDB)
#define	SPINLOCK_SPINCHECK_DEBUGGER	Debugger()
#else
#define	SPINLOCK_SPINCHECK_DEBUGGER	/* nothing */
#endif

#define	SPINLOCK_SPINCHECK_DECL						\
	/* 32-bits of count -- wrap constitutes a "spinout" */		\
	uint32_t __spinc = 0

#define	SPINLOCK_SPINCHECK						\
do {									\
	if (++__spinc == 0) {						\
		lock_printf("LK_SPIN spinout, excl %d, share %d\n",	\
		    lkp->lk_exclusivecount, lkp->lk_sharecount);	\
		if (lkp->lk_exclusivecount)				\
			lock_printf("held by CPU %lu\n",		\
			    (u_long) lkp->lk_cpu);			\
		if (lkp->lk_lock_file)					\
			lock_printf("last locked at %s:%d\n",		\
			    lkp->lk_lock_file, lkp->lk_lock_line);	\
		if (lkp->lk_unlock_file)				\
			lock_printf("last unlocked at %s:%d\n",		\
			    lkp->lk_unlock_file, lkp->lk_unlock_line);	\
		SLOCK_TRACE();						\
		SPINLOCK_SPINCHECK_DEBUGGER;				\
	}								\
} while (/*CONSTCOND*/ 0)
#else
#define	SPINLOCK_SPINCHECK_DECL			/* nothing */
#define	SPINLOCK_SPINCHECK			/* nothing */
#endif /* LOCKDEBUG && DDB */

d100 5
a104 5
	if ((extflags) & LK_SPIN) {					\
		int interlocked;					\
		SPINLOCK_SPINCHECK_DECL;				\
									\
		if ((drain) == 0)					\
d106 5
a110 16
		for (interlocked = 1;;) {				\
			SPINLOCK_SPINCHECK;				\
			if (wanted) {					\
				if (interlocked) {			\
					INTERLOCK_RELEASE((lkp),	\
					    LK_SPIN, s);		\
					interlocked = 0;		\
				}					\
				SPINLOCK_SPIN_HOOK;			\
			} else if (interlocked) {			\
				break;					\
			} else {					\
				INTERLOCK_ACQUIRE((lkp), LK_SPIN, s);	\
				interlocked = 1;			\
			}						\
		}							\
d113 5
a117 22
		KASSERT((wanted) == 0);					\
		error = 0;	/* sanity */				\
	} else {							\
		for (error = 0; wanted; ) {				\
			if ((drain))					\
				(lkp)->lk_flags |= LK_WAITDRAIN;	\
			else						\
				(lkp)->lk_waitcount++;			\
			/* XXX Cast away volatile. */			\
			error = ltsleep((drain) ?			\
			    (void *)&(lkp)->lk_flags :			\
			    (void *)(lkp), (lkp)->lk_prio,		\
			    (lkp)->lk_wmesg, (lkp)->lk_timo,		\
			    &(lkp)->lk_interlock);			\
			if ((drain) == 0)				\
				(lkp)->lk_waitcount--;			\
			if (error)					\
				break;					\
			if ((extflags) & LK_SLEEPFAIL) {		\
				error = ENOLCK;				\
				break;					\
			}						\
d119 2
a120 1
	}
d123 1
a123 6
do {									\
	if ((lkp)->lk_flags & LK_SPIN)					\
		(lkp)->lk_cpu = cpu_id;					\
	else								\
		(lkp)->lk_lockholder = pid;				\
} while (/*CONSTCOND*/0)
d126 1
a126 3
	(((lkp)->lk_flags & LK_SPIN) != 0 ?				\
	 ((lkp)->lk_cpu == (cpu_id)) :					\
	 ((lkp)->lk_lockholder == (pid)))
d130 1
a130 1
	if (((lkp)->lk_flags & LK_SPIN) == 0 && (lkp)->lk_waitcount) {	\
d153 1
a156 9
	if ((lkp)->lk_flags & LK_SPIN) {				\
		int s = spllock();					\
		SPINLOCK_LIST_LOCK();					\
		/* XXX Cast away volatile. */				\
		TAILQ_INSERT_TAIL(&spinlock_list, (struct lock *)(lkp),	\
		    lk_list);						\
		SPINLOCK_LIST_UNLOCK();					\
		splx(s);						\
	}								\
a160 9
	if ((lkp)->lk_flags & LK_SPIN) {				\
		int s = spllock();					\
		SPINLOCK_LIST_LOCK();					\
		/* XXX Cast away volatile. */				\
		TAILQ_REMOVE(&spinlock_list, (struct lock *)(lkp),	\
		    lk_list);						\
		SPINLOCK_LIST_UNLOCK();					\
		splx(s);						\
	}								\
a161 5
#else
#define	HAVEIT(lkp)		/* nothing */

#define	DONTHAVEIT(lkp)		/* nothing */
#endif /* LOCKDEBUG */ /* } */
d195 3
a197 7
	if (flags & LK_SPIN)
		lkp->lk_cpu = LK_NOCPU;
	else {
		lkp->lk_lockholder = LK_NOPROC;
		lkp->lk_prio = prio;
		lkp->lk_timo = timo;
	}
d211 1
a211 1
	int s = 0, lock_type = 0;
d213 1
a213 1
	INTERLOCK_ACQUIRE(lkp, lkp->lk_flags, s);
d218 1
a218 1
	INTERLOCK_RELEASE(lkp, lkp->lk_flags, s);
a235 1
	int s = 0;
d240 1
a240 1
	INTERLOCK_ACQUIRE(lkp, lkp->lk_flags, s);
a244 12
#ifdef DIAGNOSTIC /* { */
	/*
	 * Don't allow spins on sleep locks and don't allow sleeps
	 * on spin locks.
	 */
	if ((flags ^ lkp->lk_flags) & LK_SPIN)
		panic("lockmgr: sleep/spin mismatch");
#endif /* } */

	if (extflags & LK_SPIN) {
		pid = LK_KERNPROC;
	} else {
d246 2
a247 2
		if (p == NULL)
			panic("lockmgr: process context required");
d249 2
a250 3
		/* Process context required. */
		pid = p->p_pid;
	}
a260 2
	 * The final release is permitted to give a new lease on life to
	 * the lock by specifying LK_REENABLE.
d272 1
a272 2
		if ((flags & LK_REENABLE) == 0)
			lkp->lk_flags |= LK_DRAINED;
a318 1
		lkp->lk_recurselevel = 0;
a328 14
	case LK_EXCLUPGRADE:
		/*
		 * If another process is ahead of us to get an upgrade,
		 * then we want to fail rather than have an intervening
		 * exclusive access.
		 */
		if (lkp->lk_flags & LK_WANT_UPGRADE) {
			lkp->lk_sharecount--;
			COUNT(lkp, p, cpu_id, -1);
			error = EBUSY;
			break;
		}
		/* fall into normal upgrade */

a371 2
			if (extflags & LK_SETRECURSE)
				lkp->lk_recurselevel = 1;
d389 1
a389 2
			if ((extflags & LK_CANRECURSE) == 0 &&
			     lkp->lk_recurselevel == 0) {
a396 3
			if (extflags & LK_SETRECURSE &&
			    lkp->lk_recurselevel == 0)
				lkp->lk_recurselevel = lkp->lk_exclusivecount;
a434 2
		if (extflags & LK_SETRECURSE)
			lkp->lk_recurselevel = 1;
d441 3
a443 10
				if (lkp->lk_flags & LK_SPIN) {
					panic("lockmgr: processor %lu, not "
					    "exclusive lock holder %lu "
					    "unlocking", cpu_id, lkp->lk_cpu);
				} else {
					panic("lockmgr: pid %d, not "
					    "exclusive lock holder %d "
					    "unlocking", pid,
					    lkp->lk_lockholder);
				}
a444 2
			if (lkp->lk_exclusivecount == lkp->lk_recurselevel)
				lkp->lk_recurselevel = 0;
a499 3
		/* XXX unlikely that we'd want this */
		if (extflags & LK_SETRECURSE)
			lkp->lk_recurselevel = 1;
d504 1
a504 1
		INTERLOCK_RELEASE(lkp, lkp->lk_flags, s);
d509 1
a509 1
	if ((lkp->lk_flags & (LK_WAITDRAIN | LK_SPIN)) == LK_WAITDRAIN &&
d516 1
a516 1
	INTERLOCK_RELEASE(lkp, lkp->lk_flags, s);
a520 112
 * For a recursive spinlock held one or more times by the current CPU,
 * release all N locks, and return N.
 * Intended for use in mi_switch() shortly before context switching.
 */

#ifdef notyet
int
#if defined(LOCKDEBUG)
_spinlock_release_all(__volatile struct lock *lkp, const char *file, int line)
#else
spinlock_release_all(__volatile struct lock *lkp)
#endif
{
	int s, count;
	cpuid_t cpu_id;

	KASSERT(lkp->lk_flags & LK_SPIN);
	
	INTERLOCK_ACQUIRE(lkp, LK_SPIN, s);

	cpu_id = CPU_NUMBER();
	count = lkp->lk_exclusivecount;
	
	if (count != 0) {
#ifdef DIAGNOSTIC		
		if (WEHOLDIT(lkp, 0, cpu_id) == 0) {
			panic("spinlock_release_all: processor %lu, not "
			    "exclusive lock holder %lu "
			    "unlocking", (long)cpu_id, lkp->lk_cpu);
		}
#endif
		lkp->lk_recurselevel = 0;
		lkp->lk_exclusivecount = 0;
		COUNT_CPU(cpu_id, -count);
		lkp->lk_flags &= ~LK_HAVE_EXCL;
		SETHOLDER(lkp, LK_NOPROC, LK_NOCPU);
#if defined(LOCKDEBUG)
		lkp->lk_unlock_file = file;
		lkp->lk_unlock_line = line;
#endif
		DONTHAVEIT(lkp);
	}
#ifdef DIAGNOSTIC
	else if (lkp->lk_sharecount != 0)
		panic("spinlock_release_all: release of shared lock!");
	else
		panic("spinlock_release_all: release of unlocked lock!");
#endif
	INTERLOCK_RELEASE(lkp, LK_SPIN, s);	

	return (count);
}
#endif

/*
 * For a recursive spinlock held one or more times by the current CPU,
 * release all N locks, and return N.
 * Intended for use in mi_switch() right after resuming execution.
 */

#ifdef notyet
void
#if defined(LOCKDEBUG)
_spinlock_acquire_count(__volatile struct lock *lkp, int count,
    const char *file, int line)
#else
spinlock_acquire_count(__volatile struct lock *lkp, int count)
#endif
{
	int s, error;
	cpuid_t cpu_id;
	
	KASSERT(lkp->lk_flags & LK_SPIN);
	
	INTERLOCK_ACQUIRE(lkp, LK_SPIN, s);

	cpu_id = CPU_NUMBER();

#ifdef DIAGNOSTIC
	if (WEHOLDIT(lkp, LK_NOPROC, cpu_id))
		panic("spinlock_acquire_count: processor %lu already holds lock", (long)cpu_id);
#endif
	/*
	 * Try to acquire the want_exclusive flag.
	 */
	ACQUIRE(lkp, error, LK_SPIN, 0, lkp->lk_flags &
	    (LK_HAVE_EXCL | LK_WANT_EXCL));
	lkp->lk_flags |= LK_WANT_EXCL;
	/*
	 * Wait for shared locks and upgrades to finish.
	 */
	ACQUIRE(lkp, error, LK_SPIN, 0, lkp->lk_sharecount != 0 ||
	    (lkp->lk_flags & LK_WANT_UPGRADE));
	lkp->lk_flags &= ~LK_WANT_EXCL;
	lkp->lk_flags |= LK_HAVE_EXCL;
	SETHOLDER(lkp, LK_NOPROC, cpu_id);
#if defined(LOCKDEBUG)
	lkp->lk_lock_file = file;
	lkp->lk_lock_line = line;
#endif
	HAVEIT(lkp);
	if (lkp->lk_exclusivecount != 0)
		panic("lockmgr: non-zero exclusive count");
	lkp->lk_exclusivecount = count;
	lkp->lk_recurselevel = 1;
	COUNT_CPU(cpu_id, count);

	INTERLOCK_RELEASE(lkp, lkp->lk_flags, s);	
}
#endif

/*
d534 1
a534 4
		if (lkp->lk_flags & LK_SPIN)
			printf("processor %lu", lkp->lk_cpu);
		else
			printf("pid %d", lkp->lk_lockholder);
d537 1
a537 1
	if ((lkp->lk_flags & LK_SPIN) == 0 && lkp->lk_waitcount > 0)
a866 54
/*
 * XXX Instead of using struct lock for the kernel lock and thus requiring us
 * XXX to implement simplelocks, causing all sorts of fine-grained locks all
 * XXX over our tree getting activated consuming both time and potentially
 * XXX introducing locking protocol bugs.
 */
#ifdef notyet

struct lock kernel_lock; 

void
_kernel_lock_init(void)
{
	spinlockinit(&kernel_lock, "klock", 0);
}

/*
 * Acquire/release the kernel lock.  Intended for use in the scheduler
 * and the lower half of the kernel.
 */
void
_kernel_lock(int flag)
{
	SCHED_ASSERT_UNLOCKED();
	spinlockmgr(&kernel_lock, flag, 0);
}

void
_kernel_unlock(void)
{
	spinlockmgr(&kernel_lock, LK_RELEASE, 0);
}

/*
 * Acquire/release the kernel_lock on behalf of a process.  Intended for
 * use in the top half of the kernel.
 */
void
_kernel_proc_lock(struct proc *p)
{
	SCHED_ASSERT_UNLOCKED();
	spinlockmgr(&kernel_lock, LK_EXCLUSIVE, 0);
	p->p_flag |= P_BIGLOCK;
}

void
_kernel_proc_unlock(struct proc *p)
{
	p->p_flag &= ~P_BIGLOCK;
	spinlockmgr(&kernel_lock, LK_RELEASE, 0);
}

#else

a911 2

#endif
@


1.21
log
@ansi/deregister.
'go for it' deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.20 2005/11/19 02:18:01 pedro Exp $	*/
a57 5

/*
 * Locking primitives implementation.
 * Locks provide shared/exclusive sychronization.
 */
@


1.20
log
@Remove unnecessary lockmgr() archaism that was costing too much in terms
of panics and bugfixes. Access curproc directly, do not expect a process
pointer as an argument. Should fix many "process context required" bugs.
Incentive and okay millert@@, okay marc@@. Various testing, thanks.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.19 2005/05/29 03:20:41 deraadt Exp $	*/
d306 1
a306 6
lockinit(lkp, prio, wmesg, timo, flags)
	struct lock *lkp;
	int prio;
	char *wmesg;
	int timo;
	int flags;
d330 1
a330 2
lockstatus(lkp)
	struct lock *lkp;
d351 1
a351 4
lockmgr(lkp, flags, interlkp)
	__volatile struct lock *lkp;
	u_int flags;
	struct simplelock *interlkp;
d810 1
a810 2
lockmgr_printinfo(lkp)
	__volatile struct lock *lkp;
d882 1
a882 2
simple_lock_init(lkp)
	struct simplelock *lkp;
d898 1
a898 4
_simple_lock(lkp, id, l)
	__volatile struct simplelock *lkp;
	const char *id;
	int l;
d976 1
a976 4
_simple_lock_try(lkp, id, l)
	__volatile struct simplelock *lkp;
	const char *id;
	int l;
d1025 1
a1025 4
_simple_unlock(lkp, id, l)
	__volatile struct simplelock *lkp;
	const char *id;
	int l;
@


1.19
log
@sched work by niklas and art backed out; causes panics
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.17 2004/11/01 06:39:39 marius Exp $	*/
d357 1
a357 1
lockmgr(lkp, flags, interlkp, p)
a360 1
	struct proc *p;
d367 1
@


1.18
log
@This patch is mortly art's work and was done *a year* ago.  Art wants to thank
everyone for the prompt review and ok of this work ;-)  Yeah, that includes me
too, or maybe especially me.  I am sorry.

Change the sched_lock to a mutex. This fixes, among other things, the infamous
"telnet localhost &" problem.  The real bug in that case was that the sched_lock
which is by design a non-recursive lock, was recursively acquired, and not
enough releases made us hold the lock in the idle loop, blocking scheduling
on the other processors.  Some of the other processors would hold the biglock though,
which made it impossible for cpu 0 to enter the kernel...  A nice deadlock.
Let me just say debugging this for days just to realize that it was all fixed
in an old diff noone ever ok'd was somewhat of an anti-climax.

This diff also changes splsched to be correct for all our architectures.
@
text
@d1124 12
d1175 54
d1274 2
@


1.17
log
@additional DIAGNOSTIC, checking for proc in lockmgr when required

ok pedro@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.16 2004/06/13 21:49:26 niklas Exp $	*/
a1123 12
/*
 * We must be holding exactly one lock: the sched_lock.
 */

#ifdef notyet
void
simple_lock_switchcheck(void)
{

	simple_lock_only_held(&sched_lock, "switching");
}
#endif
a1162 54
/*
 * XXX Instead of using struct lock for the kernel lock and thus requiring us
 * XXX to implement simplelocks, causing all sorts of fine-grained locks all
 * XXX over our tree getting activated consuming both time and potentially
 * XXX introducing locking protocol bugs.
 */
#ifdef notyet

struct lock kernel_lock; 

void
_kernel_lock_init(void)
{
	spinlockinit(&kernel_lock, "klock", 0);
}

/*
 * Acquire/release the kernel lock.  Intended for use in the scheduler
 * and the lower half of the kernel.
 */
void
_kernel_lock(int flag)
{
	SCHED_ASSERT_UNLOCKED();
	spinlockmgr(&kernel_lock, flag, 0);
}

void
_kernel_unlock(void)
{
	spinlockmgr(&kernel_lock, LK_RELEASE, 0);
}

/*
 * Acquire/release the kernel_lock on behalf of a process.  Intended for
 * use in the top half of the kernel.
 */
void
_kernel_proc_lock(struct proc *p)
{
	SCHED_ASSERT_UNLOCKED();
	spinlockmgr(&kernel_lock, LK_EXCLUSIVE, 0);
	p->p_flag |= P_BIGLOCK;
}

void
_kernel_proc_unlock(struct proc *p)
{
	p->p_flag &= ~P_BIGLOCK;
	spinlockmgr(&kernel_lock, LK_RELEASE, 0);
}

#else

a1207 2

#endif
@


1.16
log
@debranch SMP, have fun
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d388 4
@


1.15
log
@Remove the advertising clause in the UCB license which Berkeley
rescinded 22 July 1999.  Proofed by myself and Theo.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.14 2002/07/12 14:03:12 art Exp $	*/
d42 1
d46 10
d64 9
a72 3
#if 0
#ifdef DEBUG
#define COUNT(p, x) if (p) (p)->p_locks += (x)
d74 18
a91 1
#define COUNT(p, x)
d93 20
d114 8
d123 6
a128 1
#define COUNT(p, x)
d130 26
a155 1
#if NCPUS > 1
d158 1
a158 4
 * For multiprocessor system, try spin lock first.
 *
 * This should be inline expanded below, but we cannot have #if
 * inside a multiline define.
d160 4
a163 4
int lock_wait_time = 100;
#define PAUSE(lkp, wanted)						\
		if (lock_wait_time > 0) {				\
			int i;						\
d165 17
a181 5
			simple_unlock(&lkp->lk_interlock);		\
			for (i = lock_wait_time; i > 0; i--)		\
				if (!(wanted))				\
					break;				\
			simple_lock(&lkp->lk_interlock);		\
d183 62
a244 2
		if (!(wanted))						\
			break;
d246 2
a247 1
#else /* NCPUS == 1 */
d249 27
a275 5
/*
 * It is an error to spin on a uniprocessor as nothing will ever cause
 * the simple lock to clear while we are executing.
 */
#define PAUSE(lkp, wanted)
d277 2
a278 1
#endif /* NCPUS == 1 */
d280 1
d282 2
a283 1
 * Acquire a resource.
d285 12
a296 15
#define ACQUIRE(lkp, error, extflags, wanted)				\
	PAUSE(lkp, wanted);						\
	for (error = 0; wanted; ) {					\
		(lkp)->lk_waitcount++;					\
		simple_unlock(&(lkp)->lk_interlock);			\
		error = tsleep((void *)lkp, (lkp)->lk_prio,		\
		    (lkp)->lk_wmesg, (lkp)->lk_timo);			\
		simple_lock(&(lkp)->lk_interlock);			\
		(lkp)->lk_waitcount--;					\
		if (error)						\
			break;						\
		if ((extflags) & LK_SLEEPFAIL) {			\
			error = ENOLCK;					\
			break;						\
		}							\
d298 3
d317 12
a328 4
	lkp->lk_prio = prio;
	lkp->lk_timo = timo;
	lkp->lk_wmesg = wmesg;
	lkp->lk_lockholder = LK_NOPROC;
d338 1
a338 1
	int lock_type = 0;
d340 1
a340 1
	simple_lock(&lkp->lk_interlock);
d345 1
a345 1
	simple_unlock(&lkp->lk_interlock);
d366 2
d370 2
a371 5
	if (p)
		pid = p->p_pid;
	else
		pid = LK_KERNPROC;
	simple_lock(&lkp->lk_interlock);
d375 18
a392 1
#ifdef DIAGNOSTIC
d405 1
d409 1
a409 1
		    lkp->lk_lockholder != pid)
d412 1
a423 1
#endif /* DIAGNOSTIC */
d428 1
a428 1
		if (lkp->lk_lockholder != pid) {
d440 1
a440 1
			ACQUIRE(lkp, error, extflags, lkp->lk_flags &
d445 1
a445 1
			COUNT(p, 1);
d453 1
a453 1
		COUNT(p, 1);
d457 2
a458 1
		if (lkp->lk_lockholder != pid || lkp->lk_exclusivecount == 0)
d462 1
d464 7
a470 3
		lkp->lk_lockholder = LK_NOPROC;
		if (lkp->lk_waitcount)
			wakeup((void *)lkp);
d481 1
a481 1
			COUNT(p, -1);
d496 1
a496 1
		if (lkp->lk_lockholder == pid || lkp->lk_sharecount <= 0)
d499 1
a499 1
		COUNT(p, -1);
d516 1
a516 1
			ACQUIRE(lkp, error, extflags, lkp->lk_sharecount);
d521 6
a526 1
			lkp->lk_lockholder = pid;
d530 3
a532 1
			COUNT(p, 1);
d540 2
a541 2
		if (lkp->lk_sharecount == 0 && lkp->lk_waitcount)
			wakeup((void *)lkp);
d545 1
a545 1
		if (lkp->lk_lockholder == pid && pid != LK_KERNPROC) {
d547 1
a547 1
			 *	Recursive lock.
d549 2
a550 1
			if ((extflags & LK_CANRECURSE) == 0) {
d554 2
a555 2
				}
				panic("lockmgr: locking against myself");
d558 4
a561 1
			COUNT(p, 1);
d576 1
a576 1
		ACQUIRE(lkp, error, extflags, lkp->lk_flags &
d584 1
a584 1
		ACQUIRE(lkp, error, extflags, lkp->lk_sharecount != 0 ||
d590 6
a595 1
		lkp->lk_lockholder = pid;
d599 3
a601 1
		COUNT(p, 1);
d606 14
a619 4
			if (pid != lkp->lk_lockholder)
				panic("lockmgr: pid %d, not %s %d unlocking",
				    pid, "exclusive lock holder",
				    lkp->lk_lockholder);
d621 1
a621 1
			COUNT(p, -1);
d624 6
a629 1
				lkp->lk_lockholder = LK_NOPROC;
d633 7
a639 5
			COUNT(p, -1);
		} else
			panic("lockmgr: LK_RELEASE of unlocked lock");
		if (lkp->lk_waitcount)
			wakeup((void *)lkp);
d649 1
a649 1
		if (lkp->lk_lockholder == pid)
d660 2
a661 1
		PAUSE(lkp, ((lkp->lk_flags &
d663 4
a666 13
		     lkp->lk_sharecount != 0 || lkp->lk_waitcount != 0));
		for (error = 0; ((lkp->lk_flags &
		     (LK_HAVE_EXCL | LK_WANT_EXCL | LK_WANT_UPGRADE)) ||
		     lkp->lk_sharecount != 0 || lkp->lk_waitcount != 0); ) {
			lkp->lk_flags |= LK_WAITDRAIN;
			simple_unlock(&lkp->lk_interlock);
			if ((error = tsleep((void *)&lkp->lk_flags, lkp->lk_prio,
			    lkp->lk_wmesg, lkp->lk_timo)) != 0)
				return (error);
			if ((extflags) & LK_SLEEPFAIL)
				return (ENOLCK);
			simple_lock(&lkp->lk_interlock);
		}
d668 6
a673 1
		lkp->lk_lockholder = pid;
d675 4
a678 1
		COUNT(p, 1);
d682 1
a682 1
		simple_unlock(&lkp->lk_interlock);
d687 4
a690 3
	if ((lkp->lk_flags & LK_WAITDRAIN) && ((lkp->lk_flags &
	     (LK_HAVE_EXCL | LK_WANT_EXCL | LK_WANT_UPGRADE)) == 0 &&
	     lkp->lk_sharecount == 0 && lkp->lk_waitcount == 0)) {
d694 1
a694 1
	simple_unlock(&lkp->lk_interlock);
d699 112
d816 1
a816 1
	struct lock *lkp;
d822 10
a831 4
	else if (lkp->lk_flags & LK_HAVE_EXCL)
		printf(" lock type %s: EXCL (count %d) by pid %d",
		    lkp->lk_wmesg, lkp->lk_exclusivecount, lkp->lk_lockholder);
	if (lkp->lk_waitcount > 0)
d836 20
d857 25
a881 2
int lockdebug_print = 0;
int lockdebug_debugger = 0;
d892 10
a901 1
	lkp->lock_data = SLOCK_UNLOCKED;
d910 2
d913 31
a943 5
	if (lkp->lock_data == SLOCK_LOCKED) {
		if (lockdebug_print)
			printf("%s:%d simple_lock: lock held...\n", id, l);
		if (lockdebug_debugger)
			Debugger();
d945 13
a957 1
	lkp->lock_data = SLOCK_LOCKED;
d960 24
d991 4
d996 10
a1005 5
	if (lkp->lock_data == SLOCK_LOCKED) {
		if (lockdebug_print)
			printf("%s:%d simple_lock: lock held...\n", id, l);
		if (lockdebug_debugger)
			Debugger();
d1007 28
a1034 1
	return lkp->lock_data = SLOCK_LOCKED;
d1043 62
d1106 9
a1114 6
	if (lkp->lock_data == SLOCK_UNLOCKED) {
		if (lockdebug_print)
			printf("%s:%d simple_unlock: lock not held...\n",
			       id, l);
		if (lockdebug_debugger)
			Debugger();
d1116 14
a1129 1
	lkp->lock_data = SLOCK_UNLOCKED;
d1131 1
d1134 1
a1134 5
_simple_lock_assert(lkp, state, id, l)
	__volatile struct simplelock *lkp;
	int state;
	const char *id;
	int l;
d1136 25
a1160 6
	if (lkp->lock_data != state) {
		if (lockdebug_print)
			printf("%s:%d simple_lock_assert: wrong state: %d",
			       id, l, lkp->lock_data);
		if (lockdebug_debugger)
			Debugger();
d1164 115
@


1.14
log
@Detect the case where we LK_RELEASE a lock when noone is holding it.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.13 2002/03/17 18:26:51 art Exp $	*/
d19 1
a19 5
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.13
log
@Add a DIAGNOSTIC check for when both LK_CANRECURSE and
LK_RECURSEFAIL are set.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.12 2002/03/14 01:27:04 millert Exp $	*/
d387 2
a388 1
		}
@


1.12
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.11 2001/12/04 21:56:18 millert Exp $	*/
d208 7
@


1.11
log
@Add declaration of "state" to _simple_lock_assert().  Since it is
an int this can safely be omitted but it is bad style to do so.
Closes PR 2223.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.10 2001/11/07 02:44:10 art Exp $	*/
d49 2
a50 2
void record_stacktrace __P((int *, int));
void playback_stacktrace __P((int *, int));
@


1.11.2.1
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.11 2001/12/04 21:56:18 millert Exp $	*/
d49 2
a50 2
void record_stacktrace(int *, int);
void playback_stacktrace(int *, int);
a207 7

	/*
	 * Check if the caller is asking us to be schizophrenic.
	 */
	if ((lkp->lk_flags & (LK_CANRECURSE|LK_RECURSEFAIL)) ==
	    (LK_CANRECURSE|LK_RECURSEFAIL))
		panic("lockmgr: make up your mind");
@


1.11.2.2
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.11.2.1 2002/06/11 03:29:40 art Exp $	*/
d387 1
a387 2
		} else
			panic("lockmgr: LK_RELEASE of unlocked lock");
@


1.10
log
@new flag to lockmgr. LK_RECURSEFAIL - even if the lock can recurse fail.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.9 1999/07/09 15:17:59 art Exp $	*/
d529 1
@


1.9
log
@rename SIMPLELOCK_DEBUG to LOCKDEBUG
clean up the debug functions
add simple_lock_assert
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.8 1999/01/11 05:12:22 millert Exp $	*/
d321 5
a325 1
			if ((extflags & LK_CANRECURSE) == 0)
d327 1
@


1.9.4.1
log
@merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d321 1
a321 5
			if ((extflags & LK_CANRECURSE) == 0) {
				if (extflags & LK_RECURSEFAIL) {
					error = EDEADLK;
					break;
				}
a322 1
			}
@


1.9.4.2
log
@Merge in trunk
@
text
@a528 1
	int state;
@


1.9.4.3
log
@Merge in -current from about a week ago
@
text
@d49 2
a50 2
void record_stacktrace(int *, int);
void playback_stacktrace(int *, int);
a207 7

	/*
	 * Check if the caller is asking us to be schizophrenic.
	 */
	if ((lkp->lk_flags & (LK_CANRECURSE|LK_RECURSEFAIL)) ==
	    (LK_CANRECURSE|LK_RECURSEFAIL))
		panic("lockmgr: make up your mind");
@


1.9.4.4
log
@Sync the SMP branch with 3.3
@
text
@d387 1
a387 2
		} else
			panic("lockmgr: LK_RELEASE of unlocked lock");
@


1.9.4.5
log
@Biglock!  Most of the logic
comes from NetBSD.
Also a lot of fixes, enough to get a dual cpu machine actually run MP for a
very short while (we are just talking about seconds) before starving out one
of the cpus.  More coming very soon.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.9.4.4 2003/03/28 00:41:26 niklas Exp $	*/
a118 21
#define	SETHOLDER(lkp, pid, cpu_id)					\
do {									\
	if ((lkp)->lk_flags & LK_SPIN)					\
		(lkp)->lk_cpu = cpu_id;					\
	else {								\
		(lkp)->lk_lockholder = pid;				\
	}								\
} while (/*CONSTCOND*/0)

#define	WEHOLDIT(lkp, pid, cpu_id)					\
	(((lkp)->lk_flags & LK_SPIN) != 0 ?				\
	 ((lkp)->lk_cpu == (cpu_id)) : ((lkp)->lk_lockholder == (pid)))

#define	WAKEUP_WAITER(lkp)						\
do {									\
	if (((lkp)->lk_flags & LK_SPIN) == 0 && (lkp)->lk_waitcount) {	\
		/* XXX Cast away volatile. */				\
		wakeup((void *)(lkp));					\
	}								\
} while (/*CONSTCOND*/0)

a174 1
	cpuid_t cpu_id;
d177 4
a180 1

d185 1
a185 18

#ifdef DIAGNOSTIC /* { */
	/*
	 * Don't allow spins on sleep locks and don't allow sleeps
	 * on spin locks.
	 */
	if ((flags ^ lkp->lk_flags) & LK_SPIN)
		panic("lockmgr: sleep/spin mismatch");
#endif /* } */

	if (extflags & LK_SPIN) {
		pid = LK_KERNPROC;
	} else {
		/* XXX Check for p == NULL */
		pid = p->p_pid;
	}
	cpu_id = cpu_number();

a197 1
#ifdef DIAGNOSTIC
d201 1
a201 1
		    WEHOLDIT(lkp, pid, cpu_id) == 0)
a203 1
#endif /* DIAGNOSTIC */
d215 1
d220 1
a220 1
		if (WEHOLDIT(lkp, pid, cpu_id) == 0) {
d249 1
a249 2
		if (WEHOLDIT(lkp, pid, cpu_id) == 0 ||
		    lkp->lk_exclusivecount == 0)
d254 3
a256 2
		SETHOLDER(lkp, LK_NOPROC, LK_NOCPU);
		WAKEUP_WAITER(lkp);
d282 1
a282 1
		if (WEHOLDIT(lkp, pid, cpu_id) || lkp->lk_sharecount <= 0)
d307 1
a307 1
			SETHOLDER(lkp, pid, cpu_id);
d319 2
a320 2
		if (lkp->lk_sharecount == 0)
			WAKEUP_WAITER(lkp);
d324 1
a324 1
		if (WEHOLDIT(lkp, pid, cpu_id)) {
d365 1
a365 1
		SETHOLDER(lkp, pid, cpu_id);
d374 4
a377 12
			if (WEHOLDIT(lkp, pid, cpu_id) == 0) {
				if (lkp->lk_flags & LK_SPIN) {
					panic("lockmgr: processor %lu, not "
					    "exclusive lock holder %lu "
					    "unlocking", cpu_id, lkp->lk_cpu);
				} else {
					panic("lockmgr: pid %d, not "
					    "exclusive lock holder %d "
					    "unlocking", pid,
					    lkp->lk_lockholder);
				}
			}
d382 1
a382 1
				SETHOLDER(lkp, LK_NOPROC, LK_NOCPU);
d389 2
a390 1
		WAKEUP_WAITER(lkp);
d400 1
a400 1
		if (WEHOLDIT(lkp, pid, cpu_id))
d427 1
a427 1
		SETHOLDER(lkp, pid, cpu_id);
a447 110
#ifdef notyet
/*
 * For a recursive spinlock held one or more times by the current CPU,
 * release all N locks, and return N.
 * Intended for use in mi_switch() shortly before context switching.
 */

int
#if defined(LOCKDEBUG)
_spinlock_release_all(__volatile struct lock *lkp, const char *file, int line)
#else
spinlock_release_all(__volatile struct lock *lkp)
#endif
{
	int s, count;
	cpuid_t cpu_id;
	
	KASSERT(lkp->lk_flags & LK_SPIN);
	
	INTERLOCK_ACQUIRE(lkp, LK_SPIN, s);

	cpu_id = cpu_number();
	count = lkp->lk_exclusivecount;
	
	if (count != 0) {
#ifdef DIAGNOSTIC		
		if (WEHOLDIT(lkp, 0, 0, cpu_id) == 0) {
			panic("spinlock_release_all: processor %lu, not "
			    "exclusive lock holder %lu "
			    "unlocking", (long)cpu_id, lkp->lk_cpu);
		}
#endif
		lkp->lk_recurselevel = 0;
		lkp->lk_exclusivecount = 0;
		COUNT_CPU(cpu_id, -count);
		lkp->lk_flags &= ~LK_HAVE_EXCL;
		SETHOLDER(lkp, LK_NOPROC, 0, LK_NOCPU);
#if defined(LOCKDEBUG)
		lkp->lk_unlock_file = file;
		lkp->lk_unlock_line = line;
#endif
		DONTHAVEIT(lkp);
	}
#ifdef DIAGNOSTIC
	else if (lkp->lk_sharecount != 0)
		panic("spinlock_release_all: release of shared lock!");
	else
		panic("spinlock_release_all: release of unlocked lock!");
#endif
	INTERLOCK_RELEASE(lkp, LK_SPIN, s);	

	return (count);
}

/*
 * For a recursive spinlock held one or more times by the current CPU,
 * release all N locks, and return N.
 * Intended for use in mi_switch() right after resuming execution.
 */

void
#if defined(LOCKDEBUG)
_spinlock_acquire_count(__volatile struct lock *lkp, int count,
    const char *file, int line)
#else
spinlock_acquire_count(__volatile struct lock *lkp, int count)
#endif
{
	int s, error;
	cpuid_t cpu_id;
	
	KASSERT(lkp->lk_flags & LK_SPIN);
	
	INTERLOCK_ACQUIRE(lkp, LK_SPIN, s);

	cpu_id = cpu_number();

#ifdef DIAGNOSTIC
	if (WEHOLDIT(lkp, LK_NOPROC, 0, cpu_id))
		panic("spinlock_acquire_count: processor %lu already holds lock", (long)cpu_id);
#endif
	/*
	 * Try to acquire the want_exclusive flag.
	 */
	ACQUIRE(lkp, error, LK_SPIN, 0, lkp->lk_flags &
	    (LK_HAVE_EXCL | LK_WANT_EXCL));
	lkp->lk_flags |= LK_WANT_EXCL;
	/*
	 * Wait for shared locks and upgrades to finish.
	 */
	ACQUIRE(lkp, error, LK_SPIN, 0, lkp->lk_sharecount != 0 ||
	    (lkp->lk_flags & LK_WANT_UPGRADE));
	lkp->lk_flags &= ~LK_WANT_EXCL;
	lkp->lk_flags |= LK_HAVE_EXCL;
	SETHOLDER(lkp, LK_NOPROC, 0, cpu_id);
#if defined(LOCKDEBUG)
	lkp->lk_lock_file = file;
	lkp->lk_lock_line = line;
#endif
	HAVEIT(lkp);
	if (lkp->lk_exclusivecount != 0)
		panic("lockmgr: non-zero exclusive count");
	lkp->lk_exclusivecount = count;
	lkp->lk_recurselevel = 1;
	COUNT_CPU(cpu_id, count);

	INTERLOCK_RELEASE(lkp, lkp->lk_flags, s);	
}
#endif

a549 51

#if defined(MULTIPROCESSOR)
/*
 * Functions for manipulating the kernel_lock.  We put them here
 * so that they show up in profiles.
 */

struct lock kernel_lock; 

void
_kernel_lock_init(void)
{
	spinlockinit(&kernel_lock, "klock", 0);
}

/*
 * Acquire/release the kernel lock.  Intended for use in the scheduler
 * and the lower half of the kernel.
 */
void
_kernel_lock(int flag)
{
	SCHED_ASSERT_UNLOCKED();
	spinlockmgr(&kernel_lock, flag, 0);
}

void
_kernel_unlock(void)
{
	spinlockmgr(&kernel_lock, LK_RELEASE, 0);
}

/*
 * Acquire/release the kernel_lock on behalf of a process.  Intended for
 * use in the top half of the kernel.
 */
void
_kernel_proc_lock(struct proc *p)
{
	SCHED_ASSERT_UNLOCKED();
	spinlockmgr(&kernel_lock, LK_EXCLUSIVE, 0);
	p->p_flag |= P_BIGLOCK;
}

void
_kernel_proc_unlock(struct proc *p)
{
	p->p_flag &= ~P_BIGLOCK;
	spinlockmgr(&kernel_lock, LK_RELEASE, 0);
}
#endif /* MULTIPROCESSOR */
@


1.9.4.6
log
@merge netbsd lockmgr better, makes us halfway through rc
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.9.4.5 2003/05/15 04:08:02 niklas Exp $	*/
d57 3
a59 9
/*
 * Locking primitives implementation.
 * Locks provide shared/exclusive synchronization.
 */

#if defined(LOCKDEBUG) || defined(DIAGNOSTIC) /* { */
#if defined(MULTIPROCESSOR) /* { */
#define	COUNT_CPU(cpu_id, x)						\
	curcpu()->ci_spin_locks += (x)
d61 3
a63 15
u_long	spin_locks;
#define	COUNT_CPU(cpu_id, x)	spin_locks += (x)
#endif /* MULTIPROCESSOR */ /* } */

#define	COUNT(lkp, p, cpu_id, x)					\
do {									\
	if ((lkp)->lk_flags & LK_SPIN)					\
		COUNT_CPU((cpu_id), (x));				\
	else								\
		(p)->p_locks += (x);					\
} while (/*CONSTCOND*/0)
#else
#define COUNT(lkp, p, cpu_id, x)
#define COUNT_CPU(cpu_id, x)
#endif /* LOCKDEBUG || DIAGNOSTIC */ /* } */
d65 1
a65 3
#ifndef SPINLOCK_SPIN_HOOK		/* from <machine/lock.h> */
#define	SPINLOCK_SPIN_HOOK		/* nothing */
#endif
d67 1
a67 6
#define	INTERLOCK_ACQUIRE(lkp, flags, s)				\
do {									\
	if ((flags) & LK_SPIN)						\
		s = spllock();						\
	simple_lock(&(lkp)->lk_interlock);				\
} while (/*CONSTCOND*/ 0)
d69 19
a87 21
#define	INTERLOCK_RELEASE(lkp, flags, s)				\
do {									\
	simple_unlock(&(lkp)->lk_interlock);				\
	if ((flags) & LK_SPIN)						\
		splx(s);						\
} while (/*CONSTCOND*/ 0)

#ifdef DDB /* { */
#ifdef MULTIPROCESSOR
int simple_lock_debugger = 1;	/* more serious on MP */
#else
int simple_lock_debugger = 0;
#endif
#define	SLOCK_DEBUGGER()	if (simple_lock_debugger) Debugger()
#define	SLOCK_TRACE()							\
	db_stack_trace_print((db_expr_t)__builtin_frame_address(0),	\
	    TRUE, 65535, "", lock_printf);
#else
#define	SLOCK_DEBUGGER()	/* nothing */
#define	SLOCK_TRACE()		/* nothing */
#endif /* } */
d89 1
a89 6
#if defined(LOCKDEBUG)
#if defined(DDB)
#define	SPINLOCK_SPINCHECK_DEBUGGER	Debugger()
#else
#define	SPINLOCK_SPINCHECK_DEBUGGER	/* nothing */
#endif
d91 5
a95 3
#define	SPINLOCK_SPINCHECK_DECL						\
	/* 32-bits of count -- wrap constitutes a "spinout" */		\
	uint32_t __spinc = 0
d97 1
a97 22
#define	SPINLOCK_SPINCHECK						\
do {									\
	if (++__spinc == 0) {						\
		lock_printf("LK_SPIN spinout, excl %d, share %d\n",	\
		    lkp->lk_exclusivecount, lkp->lk_sharecount);	\
		if (lkp->lk_exclusivecount)				\
			lock_printf("held by CPU %lu\n",		\
			    (u_long) lkp->lk_cpu);			\
		if (lkp->lk_lock_file)					\
			lock_printf("last locked at %s:%d\n",		\
			    lkp->lk_lock_file, lkp->lk_lock_line);	\
		if (lkp->lk_unlock_file)				\
			lock_printf("last unlocked at %s:%d\n",		\
			    lkp->lk_unlock_file, lkp->lk_unlock_line);	\
		SLOCK_TRACE();						\
		SPINLOCK_SPINCHECK_DEBUGGER;				\
	}								\
} while (/*CONSTCOND*/ 0)
#else
#define	SPINLOCK_SPINCHECK_DECL			/* nothing */
#define	SPINLOCK_SPINCHECK			/* nothing */
#endif /* LOCKDEBUG && DDB */
d102 14
a115 47
#define ACQUIRE(lkp, error, extflags, drain, wanted)			\
	if ((extflags) & LK_SPIN) {					\
		int interlocked;					\
		SPINLOCK_SPINCHECK_DECL;				\
									\
		if ((drain) == 0)					\
			(lkp)->lk_waitcount++;				\
		for (interlocked = 1;;) {				\
			SPINLOCK_SPINCHECK;				\
			if (wanted) {					\
				if (interlocked) {			\
					INTERLOCK_RELEASE((lkp),	\
					    LK_SPIN, s);		\
					interlocked = 0;		\
				}					\
				SPINLOCK_SPIN_HOOK;			\
			} else if (interlocked) {			\
				break;					\
			} else {					\
				INTERLOCK_ACQUIRE((lkp), LK_SPIN, s);	\
				interlocked = 1;			\
			}						\
		}							\
		if ((drain) == 0)					\
			(lkp)->lk_waitcount--;				\
		KASSERT((wanted) == 0);					\
		error = 0;	/* sanity */				\
	} else {							\
		for (error = 0; wanted; ) {				\
			if ((drain))					\
				(lkp)->lk_flags |= LK_WAITDRAIN;	\
			else						\
				(lkp)->lk_waitcount++;			\
			/* XXX Cast away volatile. */			\
			error = ltsleep((drain) ?			\
			    (void *)&(lkp)->lk_flags :			\
			    (void *)(lkp), (lkp)->lk_prio,		\
			    (lkp)->lk_wmesg, (lkp)->lk_timo,		\
			    &(lkp)->lk_interlock);			\
			if ((drain) == 0)				\
				(lkp)->lk_waitcount--;			\
			if (error)					\
				break;					\
			if ((extflags) & LK_SLEEPFAIL) {		\
				error = ENOLCK;				\
				break;					\
			}						\
d123 1
a123 1
	else								\
d125 1
d130 1
a130 2
	 ((lkp)->lk_cpu == (cpu_id)) :					\
	 ((lkp)->lk_lockholder == (pid)))
a139 71
#if defined(LOCKDEBUG) /* { */
#if defined(MULTIPROCESSOR) /* { */
struct simplelock spinlock_list_slock = SIMPLELOCK_INITIALIZER;

#define	SPINLOCK_LIST_LOCK()						\
	__cpu_simple_lock(&spinlock_list_slock.lock_data)

#define	SPINLOCK_LIST_UNLOCK()						\
	__cpu_simple_unlock(&spinlock_list_slock.lock_data)
#else
#define	SPINLOCK_LIST_LOCK()	/* nothing */

#define	SPINLOCK_LIST_UNLOCK()	/* nothing */
#endif /* MULTIPROCESSOR */ /* } */

TAILQ_HEAD(, lock) spinlock_list =
    TAILQ_HEAD_INITIALIZER(spinlock_list);

#define	HAVEIT(lkp)							\
do {									\
	if ((lkp)->lk_flags & LK_SPIN) {				\
		int s = spllock();					\
		SPINLOCK_LIST_LOCK();					\
		/* XXX Cast away volatile. */				\
		TAILQ_INSERT_TAIL(&spinlock_list, (struct lock *)(lkp),	\
		    lk_list);						\
		SPINLOCK_LIST_UNLOCK();					\
		splx(s);						\
	}								\
} while (/*CONSTCOND*/0)

#define	DONTHAVEIT(lkp)							\
do {									\
	if ((lkp)->lk_flags & LK_SPIN) {				\
		int s = spllock();					\
		SPINLOCK_LIST_LOCK();					\
		/* XXX Cast away volatile. */				\
		TAILQ_REMOVE(&spinlock_list, (struct lock *)(lkp),	\
		    lk_list);						\
		SPINLOCK_LIST_UNLOCK();					\
		splx(s);						\
	}								\
} while (/*CONSTCOND*/0)
#else
#define	HAVEIT(lkp)		/* nothing */

#define	DONTHAVEIT(lkp)		/* nothing */
#endif /* LOCKDEBUG */ /* } */

#if defined(LOCKDEBUG)
/*
 * Lock debug printing routine; can be configured to print to console
 * or log to syslog.
 */
void
lock_printf(const char *fmt, ...)
{
	char b[150];
	va_list ap;

	va_start(ap, fmt);
	if (lock_debug_syslog)
		vlog(LOG_DEBUG, fmt, ap);
	else {
		vsnprintf(b, sizeof(b), fmt, ap);
		printf_nolog("%s", b);
	}
	va_end(ap);
}
#endif /* LOCKDEBUG */

d155 4
a158 12
	if (flags & LK_SPIN)
		lkp->lk_cpu = LK_NOCPU;
	else {
		lkp->lk_lockholder = LK_NOPROC;
		lkp->lk_prio = prio;
		lkp->lk_timo = timo;
	}
	lkp->lk_wmesg = wmesg;	/* just a name for spin locks */
#if defined(LOCKDEBUG)
	lkp->lk_lock_file = NULL;
	lkp->lk_unlock_file = NULL;
#endif
d168 1
a168 1
	int s = 0, lock_type = 0;
d170 1
a170 1
	INTERLOCK_ACQUIRE(lkp, lkp->lk_flags, s);
d175 1
a175 1
	INTERLOCK_RELEASE(lkp, lkp->lk_flags, s);
a196 1
	int s = 0;
d200 1
a200 1
	INTERLOCK_ACQUIRE(lkp, lkp->lk_flags, s);
d269 1
a269 1
			ACQUIRE(lkp, error, extflags, 0, lkp->lk_flags &
d274 1
a274 1
			COUNT(lkp, p, cpu_id, 1);
d282 1
a282 1
		COUNT(lkp, p, cpu_id, 1);
a290 1
		lkp->lk_recurselevel = 0;
a292 5
#if defined(LOCKDEBUG)
		lkp->lk_unlock_file = file;
		lkp->lk_unlock_line = line;
#endif
		DONTHAVEIT(lkp);
d304 1
a304 1
			COUNT(lkp, p, cpu_id, -1);
d322 1
a322 1
		COUNT(lkp, p, cpu_id, -1);
d339 1
a339 1
			ACQUIRE(lkp, error, extflags, 0, lkp->lk_sharecount);
a344 5
#if defined(LOCKDEBUG)
			lkp->lk_lock_file = file;
			lkp->lk_lock_line = line;
#endif
			HAVEIT(lkp);
d348 1
a348 3
			if (extflags & LK_SETRECURSE)
				lkp->lk_recurselevel = 1;
			COUNT(lkp, p, cpu_id, 1);
d363 1
a363 1
			 * Recursive lock.
d365 1
a365 2
			if ((extflags & LK_CANRECURSE) == 0 &&
			     lkp->lk_recurselevel == 0) {
d369 2
a370 2
				} else
					panic("lockmgr: locking against myself");
d373 1
a373 4
			if (extflags & LK_SETRECURSE &&
			    lkp->lk_recurselevel == 0)
				lkp->lk_recurselevel = lkp->lk_exclusivecount;
			COUNT(lkp, p, cpu_id, 1);
d388 1
a388 1
		ACQUIRE(lkp, error, extflags, 0, lkp->lk_flags &
d396 1
a396 1
		ACQUIRE(lkp, error, extflags, 0, lkp->lk_sharecount != 0 ||
a402 5
#if defined(LOCKDEBUG)
		lkp->lk_lock_file = file;
		lkp->lk_lock_line = line;
#endif
		HAVEIT(lkp);
d406 1
a406 3
		if (extflags & LK_SETRECURSE)
			lkp->lk_recurselevel = 1;
		COUNT(lkp, p, cpu_id, 1);
a422 2
			if (lkp->lk_exclusivecount == lkp->lk_recurselevel)
				lkp->lk_recurselevel = 0;
d424 1
a424 1
			COUNT(lkp, p, cpu_id, -1);
a427 5
#if defined(LOCKDEBUG)
				lkp->lk_unlock_file = file;
				lkp->lk_unlock_line = line;
#endif
				DONTHAVEIT(lkp);
d431 3
a433 6
			COUNT(lkp, p, cpu_id, -1);
		}
#ifdef DIAGNOSTIC
		else
			panic("lockmgr: release of unlocked lock!");
#endif
d455 4
a458 2
		ACQUIRE(lkp, error, extflags, 1,
		    ((lkp->lk_flags &
d460 10
a469 4
		     lkp->lk_sharecount != 0 ||
		     lkp->lk_waitcount != 0));
		if (error)
			break;
a471 5
#if defined(LOCKDEBUG)
		lkp->lk_lock_file = file;
		lkp->lk_lock_line = line;
#endif
		HAVEIT(lkp);
d473 1
a473 4
		/* XXX unlikely that we'd want this */
		if (extflags & LK_SETRECURSE)
			lkp->lk_recurselevel = 1;
		COUNT(lkp, p, cpu_id, 1);
d477 1
a477 1
		INTERLOCK_RELEASE(lkp, lkp->lk_flags, s);
d482 3
a484 4
	if ((lkp->lk_flags & (LK_WAITDRAIN | LK_SPIN)) == LK_WAITDRAIN &&
	    ((lkp->lk_flags &
	    (LK_HAVE_EXCL | LK_WANT_EXCL | LK_WANT_UPGRADE)) == 0 &&
	    lkp->lk_sharecount == 0 && lkp->lk_waitcount == 0)) {
d488 1
a488 1
	INTERLOCK_RELEASE(lkp, lkp->lk_flags, s);
d492 1
d518 1
a518 1
		if (WEHOLDIT(lkp, 0, cpu_id) == 0) {
d528 1
a528 1
		SETHOLDER(lkp, LK_NOPROC, LK_NOCPU);
d570 1
a570 1
	if (WEHOLDIT(lkp, LK_NOPROC, cpu_id))
d586 1
a586 1
	SETHOLDER(lkp, LK_NOPROC, cpu_id);
d600 1
d608 1
a608 1
	__volatile struct lock *lkp;
d614 4
a617 10
	else if (lkp->lk_flags & LK_HAVE_EXCL) {
		printf(" lock type %s: EXCL (count %d) by ",
		    lkp->lk_wmesg, lkp->lk_exclusivecount);
		if (lkp->lk_flags & LK_SPIN)
			printf("processor %lu", lkp->lk_cpu);
		else
			printf("pid %d", lkp->lk_lockholder);
	} else
		printf(" not locked");
	if ((lkp->lk_flags & LK_SPIN) == 0 && lkp->lk_waitcount > 0)
a621 23
TAILQ_HEAD(, simplelock) simplelock_list =
    TAILQ_HEAD_INITIALIZER(simplelock_list);

#if defined(MULTIPROCESSOR) /* { */
struct simplelock simplelock_list_slock = SIMPLELOCK_INITIALIZER;

#define	SLOCK_LIST_LOCK()						\
	__cpu_simple_lock(&simplelock_list_slock.lock_data)

#define	SLOCK_LIST_UNLOCK()						\
	__cpu_simple_unlock(&simplelock_list_slock.lock_data)

#define	SLOCK_COUNT(x)							\
	curcpu()->ci_simple_locks += (x)
#else
u_long simple_locks;

#define	SLOCK_LIST_LOCK()	/* nothing */

#define	SLOCK_LIST_UNLOCK()	/* nothing */

#define	SLOCK_COUNT(x)		simple_locks += (x)
#endif /* MULTIPROCESSOR */ /* } */
d623 2
a624 22
#ifdef MULTIPROCESSOR
#define SLOCK_MP()		lock_printf("on cpu %ld\n", 		\
				    (u_long) cpu_number())
#else
#define SLOCK_MP()		/* nothing */
#endif

#define	SLOCK_WHERE(str, alp, id, l)					\
do {									\
	lock_printf("\n");						\
	lock_printf(str);						\
	lock_printf("lock: %p, currently at: %s:%d\n", (alp), (id), (l)); \
	SLOCK_MP();							\
	if ((alp)->lock_file != NULL)					\
		lock_printf("last locked: %s:%d\n", (alp)->lock_file,	\
		    (alp)->lock_line);					\
	if ((alp)->unlock_file != NULL)					\
		lock_printf("last unlocked: %s:%d\n", (alp)->unlock_file, \
		    (alp)->unlock_line);				\
	SLOCK_TRACE()							\
	SLOCK_DEBUGGER();						\
} while (/*CONSTCOND*/0)
d635 1
a635 10
#if defined(MULTIPROCESSOR) /* { */
	__cpu_simple_lock_init(&alp->lock_data);
#else
	alp->lock_data = __SIMPLELOCK_UNLOCKED;
#endif /* } */
	alp->lock_file = NULL;
	alp->lock_line = 0;
	alp->unlock_file = NULL;
	alp->unlock_line = 0;
	alp->lock_holder = LK_NOCPU;
a643 30
	cpuid_t cpu_id = cpu_number();
	int s;

	s = spllock();

	/*
	 * MULTIPROCESSOR case: This is `safe' since if it's not us, we
	 * don't take any action, and just fall into the normal spin case.
	 */
	if (alp->lock_data == __SIMPLELOCK_LOCKED) {
#if defined(MULTIPROCESSOR) /* { */
		if (alp->lock_holder == cpu_id) {
			SLOCK_WHERE("simple_lock: locking against myself\n",
			    alp, id, l);
			goto out;
		}
#else
		SLOCK_WHERE("simple_lock: lock held\n", alp, id, l);
		goto out;
#endif /* MULTIPROCESSOR */ /* } */
	}

#if defined(MULTIPROCESSOR) /* { */
	/* Acquire the lock before modifying any fields. */
	splx(s);
	__cpu_simple_lock(&alp->lock_data);
	s = spllock();
#else
	alp->lock_data = __SIMPLELOCK_LOCKED;
#endif /* } */
d645 5
a649 3
	if (alp->lock_holder != LK_NOCPU) {
		SLOCK_WHERE("simple_lock: uninitialized lock\n",
		    alp, id, l);
d651 1
a651 13
	alp->lock_file = id;
	alp->lock_line = l;
	alp->lock_holder = cpu_id;

	SLOCK_LIST_LOCK();
	/* XXX Cast away volatile */
	TAILQ_INSERT_TAIL(&simplelock_list, (struct simplelock *)alp, list);
	SLOCK_LIST_UNLOCK();

	SLOCK_COUNT(1);

 out:
	splx(s);
a653 26
int
_simple_lock_held(__volatile struct simplelock *alp)
{
#if defined(MULTIPROCESSOR) || defined(DIAGNOSTIC)
	cpuid_t cpu_id = cpu_number();
#endif
	int s, locked = 0;

	s = spllock();

#if defined(MULTIPROCESSOR)
	if (__cpu_simple_lock_try(&alp->lock_data) == 0)
		locked = (alp->lock_holder == cpu_id);
	else
		__cpu_simple_unlock(&alp->lock_data);
#else
	if (alp->lock_data == __SIMPLELOCK_LOCKED) {
		locked = 1;
		KASSERT(alp->lock_holder == cpu_id);
	}
#endif

	splx(s);

	return (locked);
}
a660 2
	cpuid_t cpu_id = cpu_number();
	int s, rv = 0;
d662 5
a666 17
	s = spllock();

	/*
	 * MULTIPROCESSOR case: This is `safe' since if it's not us, we
	 * don't take any action.
	 */
#if defined(MULTIPROCESSOR) /* { */
	if ((rv = __cpu_simple_lock_try(&alp->lock_data)) == 0) {
		if (alp->lock_holder == cpu_id)
			SLOCK_WHERE("simple_lock_try: locking against myself\n",
			    alp, id, l);
		goto out;
	}
#else
	if (alp->lock_data == __SIMPLELOCK_LOCKED) {
		SLOCK_WHERE("simple_lock_try: lock held\n", alp, id, l);
		goto out;
d668 1
a668 23
	alp->lock_data = __SIMPLELOCK_LOCKED;
#endif /* MULTIPROCESSOR */ /* } */

	/*
	 * At this point, we have acquired the lock.
	 */

	rv = 1;

	alp->lock_file = id;
	alp->lock_line = l;
	alp->lock_holder = cpu_id;

	SLOCK_LIST_LOCK();
	/* XXX Cast away volatile. */
	TAILQ_INSERT_TAIL(&simplelock_list, (struct simplelock *)alp, list);
	SLOCK_LIST_UNLOCK();

	SLOCK_COUNT(1);

 out:
	splx(s);
	return (rv);
a676 3
	int s;

	s = spllock();
d678 6
a683 8
	/*
	 * MULTIPROCESSOR case: This is `safe' because we think we hold
	 * the lock, and if we don't, we don't take any action.
	 */
	if (alp->lock_data == __SIMPLELOCK_UNLOCKED) {
		SLOCK_WHERE("simple_unlock: lock not held\n",
		    alp, id, l);
		goto out;
d685 1
a685 25

	SLOCK_LIST_LOCK();
	TAILQ_REMOVE(&simplelock_list, alp, list);
	SLOCK_LIST_UNLOCK();

	SLOCK_COUNT(-1);

	alp->list.tqe_next = NULL;	/* sanity */
	alp->list.tqe_prev = NULL;	/* sanity */

	alp->unlock_file = id;
	alp->unlock_line = l;

#if defined(MULTIPROCESSOR) /* { */
	alp->lock_holder = LK_NOCPU;
	/* Now that we've modified all fields, release the lock. */
	__cpu_simple_unlock(&alp->lock_data);
#else
	alp->lock_data = __SIMPLELOCK_UNLOCKED;
	KASSERT(alp->lock_holder == cpu_number());
	alp->lock_holder = LK_NOCPU;
#endif /* } */

 out:
	splx(s);
d689 5
a693 1
simple_lock_dump(void)
d695 6
a700 73
	struct simplelock *alp;
	int s;

	s = spllock();
	SLOCK_LIST_LOCK();
	lock_printf("all simple locks:\n");
	TAILQ_FOREACH(alp, &simplelock_list, list) {
		lock_printf("%p CPU %lu %s:%d\n", alp, alp->lock_holder,
		    alp->lock_file, alp->lock_line);
	}
	SLOCK_LIST_UNLOCK();
	splx(s);
}

void
simple_lock_freecheck(void *start, void *end)
{
	struct simplelock *alp;
	int s;

	s = spllock();
	SLOCK_LIST_LOCK();
	TAILQ_FOREACH(alp, &simplelock_list, list) {
		if ((void *)alp >= start && (void *)alp < end) {
			lock_printf("freeing simple_lock %p CPU %lu %s:%d\n",
			    alp, alp->lock_holder, alp->lock_file,
			    alp->lock_line);
			SLOCK_DEBUGGER();
		}
	}
	SLOCK_LIST_UNLOCK();
	splx(s);
 }

/*
 * We must be holding exactly one lock: the sched_lock.
 */

void
simple_lock_switchcheck(void)
{

	simple_lock_only_held(&sched_lock, "switching");
}

void
simple_lock_only_held(volatile struct simplelock *lp, const char *where)
{
	struct simplelock *alp;
	cpuid_t cpu_id = cpu_number();
	int s;

	if (lp) {
		LOCK_ASSERT(simple_lock_held(lp));
	}
	s = spllock();
	SLOCK_LIST_LOCK();
	TAILQ_FOREACH(alp, &simplelock_list, list) {
		if (alp == lp)
			continue;
		if (alp->lock_holder == cpu_id)
			break;
	}
	SLOCK_LIST_UNLOCK();
	splx(s);

	if (alp != NULL) {
		lock_printf("\n%s with held simple_lock %p "
		    "CPU %lu %s:%d\n",
		    where, alp, alp->lock_holder, alp->lock_file,
		    alp->lock_line);
		SLOCK_TRACE();
		SLOCK_DEBUGGER();
@


1.9.4.7
log
@Go back to defining simplelocks as noops, even if MULTIPROCESSOR.  Instead use
a new real simple recursive-lock capable lock implementation for the few
necessary locks (kernel, scheduler, tlb shootdown, printf and ddb MP).
This because we cannot trust the old fine-grained locks spread out all over
our kernel, and not really tested.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.9.4.6 2003/05/15 16:45:54 niklas Exp $	*/
a1114 1
#ifdef notyet
a1120 1
#endif
a1159 8
/*
 * XXX Instead of using struct lock for the kernel lock and thus requiring us
 * XXX to implement simplelocks, causing all sorts of fine-grained locks all
 * XXX over our tree getting activated consuming both time and potentially
 * XXX introducing locking protocol bugs.
 */
#ifdef notyet

a1202 51

#else

struct __mp_lock kernel_lock; 

void
_kernel_lock_init(void)
{
	__mp_lock_init(&kernel_lock);
}

/*
 * Acquire/release the kernel lock.  Intended for use in the scheduler
 * and the lower half of the kernel.
 */

/* XXX The flag should go, all callers want equal behaviour. */
void
_kernel_lock(int flag)
{
	SCHED_ASSERT_UNLOCKED();
	__mp_lock(&kernel_lock);
}

void
_kernel_unlock(void)
{
	__mp_unlock(&kernel_lock);
}

/*
 * Acquire/release the kernel_lock on behalf of a process.  Intended for
 * use in the top half of the kernel.
 */
void
_kernel_proc_lock(struct proc *p)
{
	SCHED_ASSERT_UNLOCKED();
	__mp_lock(&kernel_lock);
	p->p_flag |= P_BIGLOCK;
}

void
_kernel_proc_unlock(struct proc *p)
{
	p->p_flag &= ~P_BIGLOCK;
	__mp_unlock(&kernel_lock);
}

#endif

@


1.9.4.8
log
@do not create new commons
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.9.4.7 2003/05/18 17:41:16 niklas Exp $	*/
a1261 5
#endif

#ifdef MP_LOCKDEBUG
/* CPU-dependent timing, needs this to be settable from ddb. */
int __mp_lock_spinout = 200000000;
@


1.9.4.9
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.9.4.8 2003/05/18 18:25:03 niklas Exp $	*/
d19 5
a23 1
 * 3. Neither the name of the University nor the names of its contributors
@


1.9.4.10
log
@few fixes to make non-smp compile
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.9.4.9 2003/06/07 11:03:40 ho Exp $	*/
a41 1
#include <sys/sched.h>
@


1.9.4.11
log
@stab at compiling in !MP for more archs
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.9.4.10 2004/06/05 23:18:25 tedu Exp $	*/
a45 10
#ifndef spllock
#define spllock() splhigh()
#endif

#ifdef MULTIPROCESSOR
#define CPU_NUMBER() cpu_number()
#else
#define CPU_NUMBER() 0
#endif

d381 1
a381 1
	cpu_id = CPU_NUMBER();
d703 1
a703 1

d708 1
a708 1
	cpu_id = CPU_NUMBER();
d762 1
a762 1
	cpu_id = CPU_NUMBER();
d896 1
a896 1
	cpuid_t cpu_id = CPU_NUMBER();
d949 3
a951 1
	cpuid_t cpu_id = CPU_NUMBER();
d979 1
a979 1
	cpuid_t cpu_id = CPU_NUMBER();
d1063 1
a1063 1
	KASSERT(alp->lock_holder == CPU_NUMBER());
d1125 1
a1125 1
	cpuid_t cpu_id = CPU_NUMBER();
@


1.9.4.12
log
@spinlock_release_all() and spinlock_acquire_count() are not ready yet,
disable it and save some space for install media.

ok niklas@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.9.4.11 2004/06/06 23:11:51 niklas Exp $	*/
a703 1
#ifdef notyet
a749 1
#endif
a756 1
#ifdef notyet
a804 1
#endif
@


1.9.4.13
log
@Change an XXX comment to a true statement
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d388 1
a388 1
		/* Process context required. */
@


1.8
log
@panic prints a newline for you, don't do it in the panic string
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.7 1998/12/28 19:13:04 art Exp $	*/
d454 4
a457 1
#if defined(SIMPLELOCK_DEBUG) && NCPUS == 1
d464 2
a465 2
simple_lock_init(alp)
	struct simplelock *alp;
d468 1
a468 1
	alp->lock_data = 0;
d472 2
a473 2
_simple_lock(alp, id, l)
	__volatile struct simplelock *alp;
d478 7
a484 3
	if (alp->lock_data)
		printf("%s:%d simple_lock: lock held...\n", id, l);
	alp->lock_data = 1;
d489 2
a490 2
_simple_lock_try(alp, id, l)
	__volatile struct simplelock *alp;
d495 7
a501 3
	if (alp->lock_data)
		printf("%s:%d simple_lock: lock held...\n", id, l);
	return alp->lock_data = 1;
d505 2
a506 2
_simple_unlock(alp, id, l)
	__volatile struct simplelock *alp;
d511 23
a533 3
	if (!alp->lock_data)
		printf("%s:%d simple_unlock: lock not held...\n", id, l);
	alp->lock_data = 0;
d535 1
a535 1
#endif /* SIMPLELOCK_DEBUG && NCPUS == 1 */
@


1.7
log
@simple debugging for simple locks
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.6 1998/11/10 22:20:36 art Exp $	*/
d202 1
a202 1
			panic("lockmgr: non-release on draining lock: %d\n",
@


1.6
log
@put text after endif into comments
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_lock.c,v 1.5 1997/11/07 10:27:43 niklas Exp $	*/
d454 2
a455 7
#if defined(DEBUG) && NCPUS == 1
#include <sys/kernel.h>
#include <vm/vm.h>
#include <sys/sysctl.h>
int lockpausetime = 0;
struct ctldebug debug2 = { "lockpausetime", &lockpausetime };
int simplelockrecurse;
a473 16
#if 0
	if (simplelockrecurse)
		return;
	if (alp->lock_data == 1) {
		if (lockpausetime == -1)
			panic("%s:%d: simple_lock: lock held", id, l);
		printf("%s:%d: simple_lock: lock held\n", id, l);
		if (lockpausetime == 1) {
			BACKTRACE(curproc);
		} else if (lockpausetime > 1) {
			printf("%s:%d: simple_lock: lock held...", id, l);
			tsleep(&lockpausetime, PCATCH | PPAUSE, "slock",
			    lockpausetime * hz);
			printf(" continuing\n");
		}
	}
d475 2
a477 4

	if (curproc)
		curproc->p_simple_locks++;
#endif
a486 1
#if 0
d489 2
a490 8
		return (0);
	if (simplelockrecurse)
		return (1);
	alp->lock_data = 1;
	if (curproc)
		curproc->p_simple_locks++;
#endif
	return (1);
d499 3
a501 16
#if 0
	if (simplelockrecurse)
		return;
	if (alp->lock_data == 0) {
		if (lockpausetime == -1)
			panic("%s:%d: simple_unlock: lock not held", id, l);
		printf("%s:%d: simple_unlock: lock not held\n", id, l);
		if (lockpausetime == 1) {
			BACKTRACE(curproc);
		} else if (lockpausetime > 1) {
			printf("%s:%d: simple_unlock: lock not held...", id, l);
			tsleep(&lockpausetime, PCATCH | PPAUSE, "sunlock",
			    lockpausetime * hz);
			printf(" continuing\n");
		}
	}
a502 3
	if (curproc)
		curproc->p_simple_locks--;
#endif
d504 1
a504 1
#endif /* DEBUG && NCPUS == 1 */
@


1.5
log
@$OpenBSD$
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d208 1
a208 1
#endif DIAGNOSTIC
@


1.4
log
@Fixes so that it compiles with #ifdef DEBUG
@
text
@d1 2
@


1.3
log
@Updates for VFS Lite 2 + soft update.
@
text
@d55 1
d61 3
d477 1
a477 1

d493 1
d495 1
d498 1
d501 1
d508 1
d517 1
d527 1
a527 1

d546 1
@


1.2
log
@back out vfs lite2 till after 2.2
@
text
@@


1.1
log
@VFS Lite2 Changes
@
text
@@
