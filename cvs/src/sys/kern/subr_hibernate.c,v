head	1.120;
access;
symbols
	OPENBSD_6_0:1.116.0.6
	OPENBSD_6_0_BASE:1.116
	OPENBSD_5_9:1.116.0.2
	OPENBSD_5_9_BASE:1.116
	OPENBSD_5_8:1.116.0.4
	OPENBSD_5_8_BASE:1.116
	OPENBSD_5_7:1.115.0.2
	OPENBSD_5_7_BASE:1.115
	OPENBSD_5_6:1.99.0.4
	OPENBSD_5_6_BASE:1.99
	OPENBSD_5_5:1.84.0.4
	OPENBSD_5_5_BASE:1.84
	OPENBSD_5_4:1.59.0.2
	OPENBSD_5_4_BASE:1.59
	OPENBSD_5_3:1.50.0.2
	OPENBSD_5_3_BASE:1.50
	OPENBSD_5_2:1.46.0.2
	OPENBSD_5_2_BASE:1.46
	OPENBSD_5_1_BASE:1.32
	OPENBSD_5_1:1.32.0.2
	OPENBSD_5_0:1.15.0.2
	OPENBSD_5_0_BASE:1.15;
locks; strict;
comment	@ * @;


1.120
date	2016.09.27.02.53.49;	author dlg;	state Exp;
branches;
next	1.119;
commitid	IKbDU06cxpmqw0qB;

1.119
date	2016.09.16.02.35.41;	author dlg;	state Exp;
branches;
next	1.118;
commitid	Fei4687v68qad1tP;

1.118
date	2016.09.05.22.27.23;	author beck;	state Exp;
branches;
next	1.117;
commitid	0VnJax3NyJtBJzsw;

1.117
date	2016.09.01.13.12.59;	author akfaew;	state Exp;
branches;
next	1.116;
commitid	G1JY1y3kdRtEod0Y;

1.116
date	2015.05.04.02.18.05;	author mlarkin;	state Exp;
branches;
next	1.115;
commitid	UGsm9bW438YPbfsb;

1.115
date	2015.02.07.02.50.53;	author mlarkin;	state Exp;
branches;
next	1.114;
commitid	tBE29wjqvDd0kOBr;

1.114
date	2015.02.07.01.19.40;	author deraadt;	state Exp;
branches;
next	1.113;
commitid	J7ySI1ZREP62T8hM;

1.113
date	2015.02.06.05.17.48;	author mlarkin;	state Exp;
branches;
next	1.112;
commitid	6FJl8jQDXxYzHcqC;

1.112
date	2015.01.12.07.11.41;	author deraadt;	state Exp;
branches;
next	1.111;
commitid	0bmBxyttAZ0iRFyC;

1.111
date	2014.12.22.22.22.35;	author mlarkin;	state Exp;
branches;
next	1.110;
commitid	CwbbsEpmOZcPLq4x;

1.110
date	2014.12.17.19.42.15;	author tedu;	state Exp;
branches;
next	1.109;
commitid	G4ldVK4QwvfU3tRp;

1.109
date	2014.11.16.12.31.00;	author deraadt;	state Exp;
branches;
next	1.108;
commitid	yv0ECmCdICvq576h;

1.108
date	2014.11.05.05.48.45;	author mlarkin;	state Exp;
branches;
next	1.107;
commitid	Xi6I0Mucz9QeiOAq;

1.107
date	2014.11.02.22.59.58;	author mlarkin;	state Exp;
branches;
next	1.106;
commitid	qRkTnY4UtmTvnSNn;

1.106
date	2014.10.22.05.44.00;	author mlarkin;	state Exp;
branches;
next	1.105;
commitid	W2RGitBpWzEBm9CQ;

1.105
date	2014.10.22.04.46.05;	author mlarkin;	state Exp;
branches;
next	1.104;
commitid	c2QwwqhgzyXCmvRI;

1.104
date	2014.10.16.04.19.33;	author mlarkin;	state Exp;
branches;
next	1.103;
commitid	jIJ7tba7KDE9GfBI;

1.103
date	2014.10.09.00.50.54;	author mlarkin;	state Exp;
branches;
next	1.102;
commitid	xZrHvLUQmVly5n3v;

1.102
date	2014.10.09.00.42.05;	author mlarkin;	state Exp;
branches;
next	1.101;
commitid	WIUdGGyVzW22TmR2;

1.101
date	2014.09.26.09.25.38;	author kettenis;	state Exp;
branches;
next	1.100;
commitid	UmzZe6RDH5pRTdO3;

1.100
date	2014.09.19.20.02.25;	author kettenis;	state Exp;
branches;
next	1.99;
commitid	lZNawJpIJwhgF8VB;

1.99
date	2014.07.21.01.57.55;	author mlarkin;	state Exp;
branches;
next	1.98;
commitid	aEIbYb4US2rQD5Cs;

1.98
date	2014.07.20.18.05.21;	author mlarkin;	state Exp;
branches;
next	1.97;
commitid	F1K1yInguabWnn54;

1.97
date	2014.07.16.07.42.51;	author mlarkin;	state Exp;
branches;
next	1.96;
commitid	BhZaVf1oRCQi7Ocn;

1.96
date	2014.07.09.15.12.34;	author mlarkin;	state Exp;
branches;
next	1.95;
commitid	6Yg9q6umS5ss9Kqh;

1.95
date	2014.07.09.15.03.12;	author mlarkin;	state Exp;
branches;
next	1.94;
commitid	umg7lvIvVAUp4krC;

1.94
date	2014.07.09.14.10.25;	author mlarkin;	state Exp;
branches;
next	1.93;
commitid	T9vAIJG20qGyXHaR;

1.93
date	2014.07.09.12.43.51;	author mlarkin;	state Exp;
branches;
next	1.92;
commitid	zvLzdPz8mUqSem4o;

1.92
date	2014.05.31.04.36.59;	author mlarkin;	state Exp;
branches;
next	1.91;

1.91
date	2014.05.29.08.00.24;	author mlarkin;	state Exp;
branches;
next	1.90;

1.90
date	2014.05.21.02.26.49;	author mlarkin;	state Exp;
branches;
next	1.89;

1.89
date	2014.04.26.05.43.00;	author mlarkin;	state Exp;
branches;
next	1.88;

1.88
date	2014.04.20.14.02.57;	author mlarkin;	state Exp;
branches;
next	1.87;

1.87
date	2014.04.19.16.19.07;	author mlarkin;	state Exp;
branches;
next	1.86;

1.86
date	2014.03.21.21.39.36;	author miod;	state Exp;
branches;
next	1.85;

1.85
date	2014.03.13.03.52.56;	author dlg;	state Exp;
branches;
next	1.84;

1.84
date	2014.02.01.07.10.33;	author mlarkin;	state Exp;
branches;
next	1.83;

1.83
date	2014.01.21.01.48.44;	author tedu;	state Exp;
branches;
next	1.82;

1.82
date	2014.01.14.09.57.51;	author mlarkin;	state Exp;
branches;
next	1.81;

1.81
date	2013.11.21.00.13.33;	author dlg;	state Exp;
branches;
next	1.80;

1.80
date	2013.11.09.06.54.00;	author mlarkin;	state Exp;
branches;
next	1.79;

1.79
date	2013.11.09.04.38.42;	author deraadt;	state Exp;
branches;
next	1.78;

1.78
date	2013.11.06.19.53.08;	author deraadt;	state Exp;
branches;
next	1.77;

1.77
date	2013.11.06.19.50.56;	author deraadt;	state Exp;
branches;
next	1.76;

1.76
date	2013.11.06.19.48.37;	author deraadt;	state Exp;
branches;
next	1.75;

1.75
date	2013.11.06.19.47.30;	author deraadt;	state Exp;
branches;
next	1.74;

1.74
date	2013.11.06.19.45.47;	author deraadt;	state Exp;
branches;
next	1.73;

1.73
date	2013.11.06.18.41.00;	author deraadt;	state Exp;
branches;
next	1.72;

1.72
date	2013.11.06.17.03.00;	author deraadt;	state Exp;
branches;
next	1.71;

1.71
date	2013.11.06.12.06.58;	author deraadt;	state Exp;
branches;
next	1.70;

1.70
date	2013.11.06.00.52.46;	author mlarkin;	state Exp;
branches;
next	1.69;

1.69
date	2013.11.05.07.38.26;	author mlarkin;	state Exp;
branches;
next	1.68;

1.68
date	2013.11.05.06.02.44;	author deraadt;	state Exp;
branches;
next	1.67;

1.67
date	2013.11.05.00.51.58;	author krw;	state Exp;
branches;
next	1.66;

1.66
date	2013.10.20.17.16.47;	author mlarkin;	state Exp;
branches;
next	1.65;

1.65
date	2013.10.20.10.08.05;	author mlarkin;	state Exp;
branches;
next	1.64;

1.64
date	2013.10.20.10.01.29;	author mlarkin;	state Exp;
branches;
next	1.63;

1.63
date	2013.10.20.09.44.17;	author mlarkin;	state Exp;
branches;
next	1.62;

1.62
date	2013.10.20.09.27.39;	author mlarkin;	state Exp;
branches;
next	1.61;

1.61
date	2013.10.03.03.51.16;	author mlarkin;	state Exp;
branches;
next	1.60;

1.60
date	2013.09.29.15.47.35;	author mlarkin;	state Exp;
branches;
next	1.59;

1.59
date	2013.06.01.19.06.34;	author mlarkin;	state Exp;
branches;
next	1.58;

1.58
date	2013.06.01.17.39.20;	author mlarkin;	state Exp;
branches;
next	1.57;

1.57
date	2013.05.31.20.00.00;	author mlarkin;	state Exp;
branches;
next	1.56;

1.56
date	2013.05.30.19.00.59;	author mlarkin;	state Exp;
branches;
next	1.55;

1.55
date	2013.05.30.16.00.54;	author mlarkin;	state Exp;
branches;
next	1.54;

1.54
date	2013.04.09.18.58.03;	author mlarkin;	state Exp;
branches;
next	1.53;

1.53
date	2013.03.28.16.58.45;	author deraadt;	state Exp;
branches;
next	1.52;

1.52
date	2013.03.07.01.26.54;	author mlarkin;	state Exp;
branches;
next	1.51;

1.51
date	2013.03.06.08.34.05;	author mlarkin;	state Exp;
branches;
next	1.50;

1.50
date	2013.01.17.02.51.08;	author pirofti;	state Exp;
branches;
next	1.49;

1.49
date	2013.01.17.02.36.45;	author deraadt;	state Exp;
branches;
next	1.48;

1.48
date	2013.01.17.01.28.01;	author mlarkin;	state Exp;
branches;
next	1.47;

1.47
date	2013.01.17.00.11.24;	author mlarkin;	state Exp;
branches;
next	1.46;

1.46
date	2012.07.19.18.07.03;	author deraadt;	state Exp;
branches;
next	1.45;

1.45
date	2012.07.16.12.31.15;	author stsp;	state Exp;
branches;
next	1.44;

1.44
date	2012.07.16.12.15.58;	author jsing;	state Exp;
branches;
next	1.43;

1.43
date	2012.07.15.16.09.14;	author stsp;	state Exp;
branches;
next	1.42;

1.42
date	2012.07.12.09.44.09;	author mlarkin;	state Exp;
branches;
next	1.41;

1.41
date	2012.07.11.16.19.04;	author mlarkin;	state Exp;
branches;
next	1.40;

1.40
date	2012.07.09.09.47.42;	author deraadt;	state Exp;
branches;
next	1.39;

1.39
date	2012.07.08.21.11.49;	author mlarkin;	state Exp;
branches;
next	1.38;

1.38
date	2012.07.08.14.29.52;	author deraadt;	state Exp;
branches;
next	1.37;

1.37
date	2012.07.08.12.22.26;	author mlarkin;	state Exp;
branches;
next	1.36;

1.36
date	2012.06.21.12.46.30;	author jmatthew;	state Exp;
branches;
next	1.35;

1.35
date	2012.06.20.17.31.55;	author mlarkin;	state Exp;
branches;
next	1.34;

1.34
date	2012.04.12.14.57.36;	author ariane;	state Exp;
branches;
next	1.33;

1.33
date	2012.03.26.16.15.42;	author mlarkin;	state Exp;
branches;
next	1.32;

1.32
date	2011.11.29.05.21.08;	author deraadt;	state Exp;
branches;
next	1.31;

1.31
date	2011.11.29.04.59.22;	author mlarkin;	state Exp;
branches;
next	1.30;

1.30
date	2011.11.23.07.11.31;	author deraadt;	state Exp;
branches;
next	1.29;

1.29
date	2011.11.22.07.59.06;	author mlarkin;	state Exp;
branches;
next	1.28;

1.28
date	2011.11.18.01.31.37;	author mlarkin;	state Exp;
branches;
next	1.27;

1.27
date	2011.11.18.00.51.27;	author jasper;	state Exp;
branches;
next	1.26;

1.26
date	2011.11.18.00.28.46;	author mlarkin;	state Exp;
branches;
next	1.25;

1.25
date	2011.11.17.23.18.13;	author mlarkin;	state Exp;
branches;
next	1.24;

1.24
date	2011.11.16.23.52.27;	author mlarkin;	state Exp;
branches;
next	1.23;

1.23
date	2011.11.15.17.13.53;	author deraadt;	state Exp;
branches;
next	1.22;

1.22
date	2011.11.14.00.25.17;	author mlarkin;	state Exp;
branches;
next	1.21;

1.21
date	2011.11.13.23.13.29;	author mlarkin;	state Exp;
branches;
next	1.20;

1.20
date	2011.11.13.22.36.27;	author mlarkin;	state Exp;
branches;
next	1.19;

1.19
date	2011.11.13.18.38.10;	author mlarkin;	state Exp;
branches;
next	1.18;

1.18
date	2011.09.22.22.12.45;	author deraadt;	state Exp;
branches;
next	1.17;

1.17
date	2011.09.21.06.13.39;	author mlarkin;	state Exp;
branches;
next	1.16;

1.16
date	2011.09.21.02.51.23;	author mlarkin;	state Exp;
branches;
next	1.15;

1.15
date	2011.07.18.16.50.56;	author ariane;	state Exp;
branches;
next	1.14;

1.14
date	2011.07.18.16.48.26;	author ariane;	state Exp;
branches;
next	1.13;

1.13
date	2011.07.11.03.30.32;	author mlarkin;	state Exp;
branches;
next	1.12;

1.12
date	2011.07.09.03.10.27;	author mlarkin;	state Exp;
branches;
next	1.11;

1.11
date	2011.07.09.01.30.39;	author mlarkin;	state Exp;
branches;
next	1.10;

1.10
date	2011.07.09.00.55.00;	author mlarkin;	state Exp;
branches;
next	1.9;

1.9
date	2011.07.09.00.27.31;	author mlarkin;	state Exp;
branches;
next	1.8;

1.8
date	2011.07.09.00.08.04;	author mlarkin;	state Exp;
branches;
next	1.7;

1.7
date	2011.07.08.21.02.49;	author ariane;	state Exp;
branches;
next	1.6;

1.6
date	2011.07.08.21.00.53;	author ariane;	state Exp;
branches;
next	1.5;

1.5
date	2011.07.08.18.34.46;	author ariane;	state Exp;
branches;
next	1.4;

1.4
date	2011.07.08.18.31.16;	author ariane;	state Exp;
branches;
next	1.3;

1.3
date	2011.07.08.18.25.56;	author ariane;	state Exp;
branches;
next	1.2;

1.2
date	2011.07.08.18.20.10;	author ariane;	state Exp;
branches;
next	1.1;

1.1
date	2011.07.08.17.58.16;	author ariane;	state Exp;
branches;
next	;


desc
@@


1.120
log
@move from RB macros to RBT functions
@
text
@/*	$OpenBSD: subr_hibernate.c,v 1.119 2016/09/16 02:35:41 dlg Exp $	*/

/*
 * Copyright (c) 2011 Ariane van der Steldt <ariane@@stack.nl>
 * Copyright (c) 2011 Mike Larkin <mlarkin@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include <sys/hibernate.h>
#include <sys/malloc.h>
#include <sys/param.h>
#include <sys/tree.h>
#include <sys/systm.h>
#include <sys/disklabel.h>
#include <sys/disk.h>
#include <sys/conf.h>
#include <sys/buf.h>
#include <sys/fcntl.h>
#include <sys/stat.h>
#include <sys/atomic.h>

#include <uvm/uvm.h>
#include <uvm/uvm_swap.h>

#include <machine/hibernate.h>

/*
 * Hibernate piglet layout information
 *
 * The piglet is a scratch area of memory allocated by the suspending kernel.
 * Its phys and virt addrs are recorded in the signature block. The piglet is
 * used to guarantee an unused area of memory that can be used by the resuming
 * kernel for various things. The piglet is excluded during unpack operations.
 * The piglet size is presently 4*HIBERNATE_CHUNK_SIZE (typically 4*4MB).
 *
 * Offset from piglet_base	Purpose
 * ----------------------------------------------------------------------------
 * 0				Private page for suspend I/O write functions
 * 1*PAGE_SIZE			I/O page used during hibernate suspend
 * 2*PAGE_SIZE			I/O page used during hibernate suspend
 * 3*PAGE_SIZE			copy page used during hibernate suspend
 * 4*PAGE_SIZE			final chunk ordering list (24 pages)
 * 28*PAGE_SIZE			RLE utility page
 * 29*PAGE_SIZE			start of hiballoc area
 * 30*PAGE_SIZE			preserved entropy
 * 110*PAGE_SIZE		end of hiballoc area (80 pages)
 * ...				unused
 * HIBERNATE_CHUNK_SIZE		start of hibernate chunk table
 * 2*HIBERNATE_CHUNK_SIZE	bounce area for chunks being unpacked
 * 4*HIBERNATE_CHUNK_SIZE	end of piglet
 */

/* Temporary vaddr ranges used during hibernate */
vaddr_t hibernate_temp_page;
vaddr_t hibernate_copy_page;
vaddr_t hibernate_rle_page;

/* Hibernate info as read from disk during resume */
union hibernate_info disk_hib;

/*
 * Global copy of the pig start address. This needs to be a global as we
 * switch stacks after computing it - it can't be stored on the stack.
 */
paddr_t global_pig_start;

/*
 * Global copies of the piglet start addresses (PA/VA). We store these
 * as globals to avoid having to carry them around as parameters, as the
 * piglet is allocated early and freed late - its lifecycle extends beyond
 * that of the hibernate info union which is calculated on suspend/resume.
 */
vaddr_t global_piglet_va;
paddr_t global_piglet_pa;

/* #define HIB_DEBUG */
#ifdef HIB_DEBUG
int	hib_debug = 99;
#define DPRINTF(x...)     do { if (hib_debug) printf(x); } while (0)
#define DNPRINTF(n,x...)  do { if (hib_debug > (n)) printf(x); } while (0)
#else
#define DPRINTF(x...)
#define DNPRINTF(n,x...)
#endif

#ifndef NO_PROPOLICE
extern long __guard_local;
#endif /* ! NO_PROPOLICE */

void hibernate_copy_chunk_to_piglet(paddr_t, vaddr_t, size_t);
int hibernate_calc_rle(paddr_t, paddr_t);
int hibernate_write_rle(union hibernate_info *, paddr_t, paddr_t, daddr_t *,
	size_t *);

#define MAX_RLE (HIBERNATE_CHUNK_SIZE / PAGE_SIZE)

/*
 * Hib alloc enforced alignment.
 */
#define HIB_ALIGN		8 /* bytes alignment */

/*
 * sizeof builtin operation, but with alignment constraint.
 */
#define HIB_SIZEOF(_type)	roundup(sizeof(_type), HIB_ALIGN)

struct hiballoc_entry {
	size_t			hibe_use;
	size_t			hibe_space;
	RBT_ENTRY(hiballoc_entry) hibe_entry;
};

/*
 * Sort hibernate memory ranges by ascending PA
 */
void
hibernate_sort_ranges(union hibernate_info *hib_info)
{
	int i, j;
	struct hibernate_memory_range *ranges;
	paddr_t base, end;

	ranges = hib_info->ranges;

	for (i = 1; i < hib_info->nranges; i++) {
		j = i;
		while (j > 0 && ranges[j - 1].base > ranges[j].base) {
			base = ranges[j].base;
			end = ranges[j].end;
			ranges[j].base = ranges[j - 1].base;
			ranges[j].end = ranges[j - 1].end;
			ranges[j - 1].base = base;
			ranges[j - 1].end = end;
			j--;
		}
	}
}

/*
 * Compare hiballoc entries based on the address they manage.
 *
 * Since the address is fixed, relative to struct hiballoc_entry,
 * we just compare the hiballoc_entry pointers.
 */
static __inline int
hibe_cmp(const struct hiballoc_entry *l, const struct hiballoc_entry *r)
{
	vaddr_t vl = (vaddr_t)l;
	vaddr_t vr = (vaddr_t)r;

	return vl < vr ? -1 : (vl > vr);
}

RBT_PROTOTYPE(hiballoc_addr, hiballoc_entry, hibe_entry, hibe_cmp)

/*
 * Given a hiballoc entry, return the address it manages.
 */
static __inline void *
hib_entry_to_addr(struct hiballoc_entry *entry)
{
	caddr_t addr;

	addr = (caddr_t)entry;
	addr += HIB_SIZEOF(struct hiballoc_entry);
	return addr;
}

/*
 * Given an address, find the hiballoc that corresponds.
 */
static __inline struct hiballoc_entry*
hib_addr_to_entry(void *addr_param)
{
	caddr_t addr;

	addr = (caddr_t)addr_param;
	addr -= HIB_SIZEOF(struct hiballoc_entry);
	return (struct hiballoc_entry*)addr;
}

RBT_GENERATE(hiballoc_addr, hiballoc_entry, hibe_entry, hibe_cmp);

/*
 * Allocate memory from the arena.
 *
 * Returns NULL if no memory is available.
 */
void *
hib_alloc(struct hiballoc_arena *arena, size_t alloc_sz)
{
	struct hiballoc_entry *entry, *new_entry;
	size_t find_sz;

	/*
	 * Enforce alignment of HIB_ALIGN bytes.
	 *
	 * Note that, because the entry is put in front of the allocation,
	 * 0-byte allocations are guaranteed a unique address.
	 */
	alloc_sz = roundup(alloc_sz, HIB_ALIGN);

	/*
	 * Find an entry with hibe_space >= find_sz.
	 *
	 * If the root node is not large enough, we switch to tree traversal.
	 * Because all entries are made at the bottom of the free space,
	 * traversal from the end has a slightly better chance of yielding
	 * a sufficiently large space.
	 */
	find_sz = alloc_sz + HIB_SIZEOF(struct hiballoc_entry);
	entry = RBT_ROOT(hiballoc_addr, &arena->hib_addrs);
	if (entry != NULL && entry->hibe_space < find_sz) {
		RBT_FOREACH_REVERSE(entry, hiballoc_addr, &arena->hib_addrs) {
			if (entry->hibe_space >= find_sz)
				break;
		}
	}

	/*
	 * Insufficient or too fragmented memory.
	 */
	if (entry == NULL)
		return NULL;

	/*
	 * Create new entry in allocated space.
	 */
	new_entry = (struct hiballoc_entry*)(
	    (caddr_t)hib_entry_to_addr(entry) + entry->hibe_use);
	new_entry->hibe_space = entry->hibe_space - find_sz;
	new_entry->hibe_use = alloc_sz;

	/*
	 * Insert entry.
	 */
	if (RBT_INSERT(hiballoc_addr, &arena->hib_addrs, new_entry) != NULL)
		panic("hib_alloc: insert failure");
	entry->hibe_space = 0;

	/* Return address managed by entry. */
	return hib_entry_to_addr(new_entry);
}

void
hib_getentropy(char **bufp, size_t *bufplen)
{
	if (!bufp || !bufplen)
		return;

	*bufp = (char *)(global_piglet_va + (29 * PAGE_SIZE));
	*bufplen = PAGE_SIZE;
}

/*
 * Free a pointer previously allocated from this arena.
 *
 * If addr is NULL, this will be silently accepted.
 */
void
hib_free(struct hiballoc_arena *arena, void *addr)
{
	struct hiballoc_entry *entry, *prev;

	if (addr == NULL)
		return;

	/*
	 * Derive entry from addr and check it is really in this arena.
	 */
	entry = hib_addr_to_entry(addr);
	if (RBT_FIND(hiballoc_addr, &arena->hib_addrs, entry) != entry)
		panic("hib_free: freed item %p not in hib arena", addr);

	/*
	 * Give the space in entry to its predecessor.
	 *
	 * If entry has no predecessor, change its used space into free space
	 * instead.
	 */
	prev = RBT_PREV(hiballoc_addr, entry);
	if (prev != NULL &&
	    (void *)((caddr_t)prev + HIB_SIZEOF(struct hiballoc_entry) +
	    prev->hibe_use + prev->hibe_space) == entry) {
		/* Merge entry. */
		RBT_REMOVE(hiballoc_addr, &arena->hib_addrs, entry);
		prev->hibe_space += HIB_SIZEOF(struct hiballoc_entry) +
		    entry->hibe_use + entry->hibe_space;
	} else {
		/* Flip used memory to free space. */
		entry->hibe_space += entry->hibe_use;
		entry->hibe_use = 0;
	}
}

/*
 * Initialize hiballoc.
 *
 * The allocator will manage memmory at ptr, which is len bytes.
 */
int
hiballoc_init(struct hiballoc_arena *arena, void *p_ptr, size_t p_len)
{
	struct hiballoc_entry *entry;
	caddr_t ptr;
	size_t len;

	RBT_INIT(hiballoc_addr, &arena->hib_addrs);

	/*
	 * Hib allocator enforces HIB_ALIGN alignment.
	 * Fixup ptr and len.
	 */
	ptr = (caddr_t)roundup((vaddr_t)p_ptr, HIB_ALIGN);
	len = p_len - ((size_t)ptr - (size_t)p_ptr);
	len &= ~((size_t)HIB_ALIGN - 1);

	/*
	 * Insufficient memory to be able to allocate and also do bookkeeping.
	 */
	if (len <= HIB_SIZEOF(struct hiballoc_entry))
		return ENOMEM;

	/*
	 * Create entry describing space.
	 */
	entry = (struct hiballoc_entry*)ptr;
	entry->hibe_use = 0;
	entry->hibe_space = len - HIB_SIZEOF(struct hiballoc_entry);
	RBT_INSERT(hiballoc_addr, &arena->hib_addrs, entry);

	return 0;
}

/*
 * Zero all free memory.
 */
void
uvm_pmr_zero_everything(void)
{
	struct uvm_pmemrange	*pmr;
	struct vm_page		*pg;
	int			 i;

	uvm_lock_fpageq();
	TAILQ_FOREACH(pmr, &uvm.pmr_control.use, pmr_use) {
		/* Zero single pages. */
		while ((pg = TAILQ_FIRST(&pmr->single[UVM_PMR_MEMTYPE_DIRTY]))
		    != NULL) {
			uvm_pmr_remove(pmr, pg);
			uvm_pagezero(pg);
			atomic_setbits_int(&pg->pg_flags, PG_ZERO);
			uvmexp.zeropages++;
			uvm_pmr_insert(pmr, pg, 0);
		}

		/* Zero multi page ranges. */
		while ((pg = RBT_ROOT(uvm_pmr_size,
		    &pmr->size[UVM_PMR_MEMTYPE_DIRTY])) != NULL) {
			pg--; /* Size tree always has second page. */
			uvm_pmr_remove(pmr, pg);
			for (i = 0; i < pg->fpgsz; i++) {
				uvm_pagezero(&pg[i]);
				atomic_setbits_int(&pg[i].pg_flags, PG_ZERO);
				uvmexp.zeropages++;
			}
			uvm_pmr_insert(pmr, pg, 0);
		}
	}
	uvm_unlock_fpageq();
}

/*
 * Mark all memory as dirty.
 *
 * Used to inform the system that the clean memory isn't clean for some
 * reason, for example because we just came back from hibernate.
 */
void
uvm_pmr_dirty_everything(void)
{
	struct uvm_pmemrange	*pmr;
	struct vm_page		*pg;
	int			 i;

	uvm_lock_fpageq();
	TAILQ_FOREACH(pmr, &uvm.pmr_control.use, pmr_use) {
		/* Dirty single pages. */
		while ((pg = TAILQ_FIRST(&pmr->single[UVM_PMR_MEMTYPE_ZERO]))
		    != NULL) {
			uvm_pmr_remove(pmr, pg);
			atomic_clearbits_int(&pg->pg_flags, PG_ZERO);
			uvm_pmr_insert(pmr, pg, 0);
		}

		/* Dirty multi page ranges. */
		while ((pg = RBT_ROOT(uvm_pmr_size,
		    &pmr->size[UVM_PMR_MEMTYPE_ZERO])) != NULL) {
			pg--; /* Size tree always has second page. */
			uvm_pmr_remove(pmr, pg);
			for (i = 0; i < pg->fpgsz; i++)
				atomic_clearbits_int(&pg[i].pg_flags, PG_ZERO);
			uvm_pmr_insert(pmr, pg, 0);
		}
	}

	uvmexp.zeropages = 0;
	uvm_unlock_fpageq();
}

/*
 * Allocate an area that can hold sz bytes and doesn't overlap with
 * the piglet at piglet_pa.
 */
int
uvm_pmr_alloc_pig(paddr_t *pa, psize_t sz, paddr_t piglet_pa)
{
	struct uvm_constraint_range pig_constraint;
	struct kmem_pa_mode kp_pig = {
		.kp_constraint = &pig_constraint,
		.kp_maxseg = 1
	};
	vaddr_t va;

	sz = round_page(sz);

	pig_constraint.ucr_low = piglet_pa + 4 * HIBERNATE_CHUNK_SIZE;
	pig_constraint.ucr_high = -1;

	va = (vaddr_t)km_alloc(sz, &kv_any, &kp_pig, &kd_nowait);
	if (va == 0) {
		pig_constraint.ucr_low = 0;
		pig_constraint.ucr_high = piglet_pa - 1;

		va = (vaddr_t)km_alloc(sz, &kv_any, &kp_pig, &kd_nowait);
		if (va == 0)
			return ENOMEM;
	}

	pmap_extract(pmap_kernel(), va, pa);
	return 0;
}

/*
 * Allocate a piglet area.
 *
 * This needs to be in DMA-safe memory.
 * Piglets are aligned.
 *
 * sz and align in bytes.
 *
 * The call will sleep for the pagedaemon to attempt to free memory.
 * The pagedaemon may decide its not possible to free enough memory, causing
 * the allocation to fail.
 */
int
uvm_pmr_alloc_piglet(vaddr_t *va, paddr_t *pa, vsize_t sz, paddr_t align)
{
	struct kmem_pa_mode kp_piglet = {
		.kp_constraint = &dma_constraint,
		.kp_align = align,
		.kp_maxseg = 1
	};

	/* Ensure align is a power of 2 */
	KASSERT((align & (align - 1)) == 0);

	/*
	 * Fixup arguments: align must be at least PAGE_SIZE,
	 * sz will be converted to pagecount, since that is what
	 * pmemrange uses internally.
	 */
	if (align < PAGE_SIZE)
		kp_piglet.kp_align = PAGE_SIZE;
		
	sz = round_page(sz);

	*va = (vaddr_t)km_alloc(sz, &kv_any, &kp_piglet, &kd_nowait);
	if (*va == 0)
		return ENOMEM;

	pmap_extract(pmap_kernel(), *va, pa);
	return 0;
}

/*
 * Free a piglet area.
 */
void
uvm_pmr_free_piglet(vaddr_t va, vsize_t sz)
{
	/*
	 * Fix parameters.
	 */
	sz = round_page(sz);

	/*
	 * Free the physical and virtual memory.
	 */
	km_free((void *)va, sz, &kv_any, &kp_dma_contig);
}

/*
 * Physmem RLE compression support.
 *
 * Given a physical page address, return the number of pages starting at the
 * address that are free.  Clamps to the number of pages in
 * HIBERNATE_CHUNK_SIZE. Returns 0 if the page at addr is not free.
 */
int
uvm_page_rle(paddr_t addr)
{
	struct vm_page		*pg, *pg_end;
	struct vm_physseg	*vmp;
	int			 pseg_idx, off_idx;

	pseg_idx = vm_physseg_find(atop(addr), &off_idx);
	if (pseg_idx == -1)
		return 0;

	vmp = &vm_physmem[pseg_idx];
	pg = &vmp->pgs[off_idx];
	if (!(pg->pg_flags & PQ_FREE))
		return 0;

	/*
	 * Search for the first non-free page after pg.
	 * Note that the page may not be the first page in a free pmemrange,
	 * therefore pg->fpgsz cannot be used.
	 */
	for (pg_end = pg; pg_end <= vmp->lastpg &&
	    (pg_end->pg_flags & PQ_FREE) == PQ_FREE; pg_end++)
		;
	return min((pg_end - pg), HIBERNATE_CHUNK_SIZE/PAGE_SIZE);
}

/*
 * Fills out the hibernate_info union pointed to by hib
 * with information about this machine (swap signature block
 * offsets, number of memory ranges, kernel in use, etc)
 */
int
get_hibernate_info(union hibernate_info *hib, int suspend)
{
	struct disklabel dl;
	char err_string[128], *dl_ret;

#ifndef NO_PROPOLICE
	/* Save propolice guard */
	hib->guard = __guard_local;
#endif /* ! NO_PROPOLICE */

	/* Determine I/O function to use */
	hib->io_func = get_hibernate_io_function(swdevt[0].sw_dev);
	if (hib->io_func == NULL)
		return (1);

	/* Calculate hibernate device */
	hib->dev = swdevt[0].sw_dev;

	/* Read disklabel (used to calculate signature and image offsets) */
	dl_ret = disk_readlabel(&dl, hib->dev, err_string, sizeof(err_string));

	if (dl_ret) {
		printf("Hibernate error reading disklabel: %s\n", dl_ret);
		return (1);
	}

	/* Make sure we have a swap partition. */
	if (dl.d_partitions[1].p_fstype != FS_SWAP ||
	    DL_GETPSIZE(&dl.d_partitions[1]) == 0)
		return (1);

	/* Make sure the signature can fit in one block */
	if (sizeof(union hibernate_info) > DEV_BSIZE)
		return (1);

	/* Magic number */
	hib->magic = HIBERNATE_MAGIC;
	
	/* Calculate signature block location */
	hib->sig_offset = DL_GETPSIZE(&dl.d_partitions[1]) -
	    sizeof(union hibernate_info)/DEV_BSIZE;

	/* Stash kernel version information */
	memset(&hib->kernel_version, 0, 128);
	bcopy(version, &hib->kernel_version,
	    min(strlen(version), sizeof(hib->kernel_version)-1));

	if (suspend) {
		/* Grab the previously-allocated piglet addresses */
		hib->piglet_va = global_piglet_va;
		hib->piglet_pa = global_piglet_pa;
		hib->io_page = (void *)hib->piglet_va;

		/*
		 * Initialization of the hibernate IO function for drivers
		 * that need to do prep work (such as allocating memory or
		 * setting up data structures that cannot safely be done
		 * during suspend without causing side effects). There is
		 * a matching HIB_DONE call performed after the write is
		 * completed.	
		 */
		if (hib->io_func(hib->dev, DL_GETPOFFSET(&dl.d_partitions[1]),
		    (vaddr_t)NULL, DL_GETPSIZE(&dl.d_partitions[1]),
		    HIB_INIT, hib->io_page))
			goto fail;

	} else {
		/*
		 * Resuming kernels use a regular private page for the driver
		 * No need to free this I/O page as it will vanish as part of
		 * the resume.
		 */
		hib->io_page = malloc(PAGE_SIZE, M_DEVBUF, M_NOWAIT);
		if (!hib->io_page)
			goto fail;
	}

	if (get_hibernate_info_md(hib))
		goto fail;

	return (0);

fail:
	return (1);
}

/*
 * Allocate nitems*size bytes from the hiballoc area presently in use
 */
void *
hibernate_zlib_alloc(void *unused, int nitems, int size)
{
	struct hibernate_zlib_state *hibernate_state;

	hibernate_state =
	    (struct hibernate_zlib_state *)HIBERNATE_HIBALLOC_PAGE;

	return hib_alloc(&hibernate_state->hiballoc_arena, nitems*size);
}

/*
 * Free the memory pointed to by addr in the hiballoc area presently in
 * use
 */
void
hibernate_zlib_free(void *unused, void *addr)
{
	struct hibernate_zlib_state *hibernate_state;

	hibernate_state =
	    (struct hibernate_zlib_state *)HIBERNATE_HIBALLOC_PAGE;

	hib_free(&hibernate_state->hiballoc_arena, addr);
}

/*
 * Inflate next page of data from the image stream.
 * The rle parameter is modified on exit to contain the number of pages to
 * skip in the output stream (or 0 if this page was inflated into).
 *
 * Returns 0 if the stream contains additional data, or 1 if the stream is
 * finished.
 */
int
hibernate_inflate_page(int *rle)
{
	struct hibernate_zlib_state *hibernate_state;
	int i;

	hibernate_state =
	    (struct hibernate_zlib_state *)HIBERNATE_HIBALLOC_PAGE;

	/* Set up the stream for RLE code inflate */
	hibernate_state->hib_stream.next_out = (unsigned char *)rle;
	hibernate_state->hib_stream.avail_out = sizeof(*rle);

	/* Inflate RLE code */
	i = inflate(&hibernate_state->hib_stream, Z_SYNC_FLUSH);
	if (i != Z_OK && i != Z_STREAM_END) {
		/*
		 * XXX - this will likely reboot/hang most machines
		 *       since the console output buffer will be unmapped,
		 *       but there's not much else we can do here.
		 */
		panic("rle inflate stream error");
	}

	if (hibernate_state->hib_stream.avail_out != 0) {
		/*
		 * XXX - this will likely reboot/hang most machines
		 *       since the console output buffer will be unmapped,
		 *       but there's not much else we can do here.
		 */
		panic("rle short inflate error");
	}
	
	if (*rle < 0 || *rle > 1024) {
		/*
		 * XXX - this will likely reboot/hang most machines
		 *       since the console output buffer will be unmapped,
		 *       but there's not much else we can do here.
		 */
		panic("invalid rle count");
	}

	if (i == Z_STREAM_END)
		return (1);

	if (*rle != 0)
		return (0);

	/* Set up the stream for page inflate */
	hibernate_state->hib_stream.next_out =
		(unsigned char *)HIBERNATE_INFLATE_PAGE;
	hibernate_state->hib_stream.avail_out = PAGE_SIZE;

	/* Process next block of data */
	i = inflate(&hibernate_state->hib_stream, Z_SYNC_FLUSH);
	if (i != Z_OK && i != Z_STREAM_END) {
		/*
		 * XXX - this will likely reboot/hang most machines
		 *       since the console output buffer will be unmapped,
		 *       but there's not much else we can do here.
		 */
		panic("inflate error");
	}

	/* We should always have extracted a full page ... */
	if (hibernate_state->hib_stream.avail_out != 0) {
		/*
		 * XXX - this will likely reboot/hang most machines
		 *       since the console output buffer will be unmapped,
		 *       but there's not much else we can do here.
		 */
		panic("incomplete page");
	}

	return (i == Z_STREAM_END);
}

/*
 * Inflate size bytes from src into dest, skipping any pages in
 * [src..dest] that are special (see hibernate_inflate_skip)
 *
 * This function executes while using the resume-time stack
 * and pmap, and therefore cannot use ddb/printf/etc. Doing so
 * will likely hang or reset the machine since the console output buffer
 * will be unmapped.
 */
void
hibernate_inflate_region(union hibernate_info *hib, paddr_t dest,
    paddr_t src, size_t size)
{
	int end_stream = 0, rle;
	struct hibernate_zlib_state *hibernate_state;

	hibernate_state =
	    (struct hibernate_zlib_state *)HIBERNATE_HIBALLOC_PAGE;

	hibernate_state->hib_stream.next_in = (unsigned char *)src;
	hibernate_state->hib_stream.avail_in = size;

	do {
		/*
		 * Is this a special page? If yes, redirect the
		 * inflate output to a scratch page (eg, discard it)
		 */
		if (hibernate_inflate_skip(hib, dest)) {
			hibernate_enter_resume_mapping(
			    HIBERNATE_INFLATE_PAGE,
			    HIBERNATE_INFLATE_PAGE, 0);
		} else {
			hibernate_enter_resume_mapping(
			    HIBERNATE_INFLATE_PAGE, dest, 0);
		}

		hibernate_flush();
		end_stream = hibernate_inflate_page(&rle);

		if (rle == 0)
			dest += PAGE_SIZE;
		else
			dest += (rle * PAGE_SIZE);
	} while (!end_stream);
}

/*
 * deflate from src into the I/O page, up to 'remaining' bytes
 *
 * Returns number of input bytes consumed, and may reset
 * the 'remaining' parameter if not all the output space was consumed
 * (this information is needed to know how much to write to disk
 */
size_t
hibernate_deflate(union hibernate_info *hib, paddr_t src,
    size_t *remaining)
{
	vaddr_t hibernate_io_page = hib->piglet_va + PAGE_SIZE;
	struct hibernate_zlib_state *hibernate_state;

	hibernate_state =
	    (struct hibernate_zlib_state *)HIBERNATE_HIBALLOC_PAGE;

	/* Set up the stream for deflate */
	hibernate_state->hib_stream.next_in = (unsigned char *)src;
	hibernate_state->hib_stream.avail_in = PAGE_SIZE - (src & PAGE_MASK);
	hibernate_state->hib_stream.next_out =
		(unsigned char *)hibernate_io_page + (PAGE_SIZE - *remaining);
	hibernate_state->hib_stream.avail_out = *remaining;

	/* Process next block of data */
	if (deflate(&hibernate_state->hib_stream, Z_SYNC_FLUSH) != Z_OK)
		panic("hibernate zlib deflate error");

	/* Update pointers and return number of bytes consumed */
	*remaining = hibernate_state->hib_stream.avail_out;
	return (PAGE_SIZE - (src & PAGE_MASK)) -
	    hibernate_state->hib_stream.avail_in;
}

/*
 * Write the hibernation information specified in hiber_info
 * to the location in swap previously calculated (last block of
 * swap), called the "signature block".
 */
int
hibernate_write_signature(union hibernate_info *hib)
{
	/* Write hibernate info to disk */
	return (hib->io_func(hib->dev, hib->sig_offset,
	    (vaddr_t)hib, DEV_BSIZE, HIB_W,
	    hib->io_page));
}

/*
 * Write the memory chunk table to the area in swap immediately
 * preceding the signature block. The chunk table is stored
 * in the piglet when this function is called.  Returns errno.
 */
int
hibernate_write_chunktable(union hibernate_info *hib)
{
	vaddr_t hibernate_chunk_table_start;
	size_t hibernate_chunk_table_size;
	int i, err;

	hibernate_chunk_table_size = HIBERNATE_CHUNK_TABLE_SIZE;

	hibernate_chunk_table_start = hib->piglet_va +
	    HIBERNATE_CHUNK_SIZE;

	/* Write chunk table */
	for (i = 0; i < hibernate_chunk_table_size; i += MAXPHYS) {
		if ((err = hib->io_func(hib->dev,
		    hib->chunktable_offset + (i/DEV_BSIZE),
		    (vaddr_t)(hibernate_chunk_table_start + i),
		    MAXPHYS, HIB_W, hib->io_page))) {
			DPRINTF("chunktable write error: %d\n", err);
			return (err);
		}
	}

	return (0);
}

/*
 * Write an empty hiber_info to the swap signature block, which is
 * guaranteed to not match any valid hib.
 */
int
hibernate_clear_signature(void)
{
	union hibernate_info blank_hiber_info;
	union hibernate_info hib;

	/* Zero out a blank hiber_info */
	memset(&blank_hiber_info, 0, sizeof(union hibernate_info));

	/* Get the signature block location */
	if (get_hibernate_info(&hib, 0))
		return (1);

	/* Write (zeroed) hibernate info to disk */
	DPRINTF("clearing hibernate signature block location: %lld\n",
		hib.sig_offset);
	if (hibernate_block_io(&hib,
	    hib.sig_offset,
	    DEV_BSIZE, (vaddr_t)&blank_hiber_info, 1))
		printf("Warning: could not clear hibernate signature\n");

	return (0);
}

/*
 * Compare two hibernate_infos to determine if they are the same (eg,
 * we should be performing a hibernate resume on this machine.
 * Not all fields are checked - just enough to verify that the machine
 * has the same memory configuration and kernel as the one that
 * wrote the signature previously.
 */
int
hibernate_compare_signature(union hibernate_info *mine,
    union hibernate_info *disk)
{
	u_int i;

	if (mine->nranges != disk->nranges) {
		DPRINTF("hibernate memory range count mismatch\n");
		return (1);
	}

	if (strcmp(mine->kernel_version, disk->kernel_version) != 0) {
		DPRINTF("hibernate kernel version mismatch\n");
		return (1);
	}

	for (i = 0; i < mine->nranges; i++) {
		if ((mine->ranges[i].base != disk->ranges[i].base) ||
		    (mine->ranges[i].end != disk->ranges[i].end) ) {
			DPRINTF("hib range %d mismatch [%p-%p != %p-%p]\n",
				i,
				(void *)mine->ranges[i].base,
				(void *)mine->ranges[i].end,
				(void *)disk->ranges[i].base,
				(void *)disk->ranges[i].end);
			return (1);
		}
	}

	return (0);
}

/*
 * Transfers xfer_size bytes between the hibernate device specified in
 * hib_info at offset blkctr and the vaddr specified at dest.
 *
 * Separate offsets and pages are used to handle misaligned reads (reads
 * that span a page boundary).
 *
 * blkctr specifies a relative offset (relative to the start of swap),
 * not an absolute disk offset
 *
 */
int
hibernate_block_io(union hibernate_info *hib, daddr_t blkctr,
    size_t xfer_size, vaddr_t dest, int iswrite)
{
	struct buf *bp;
	struct bdevsw *bdsw;
	int error;

	bp = geteblk(xfer_size);
	bdsw = &bdevsw[major(hib->dev)];

	error = (*bdsw->d_open)(hib->dev, FREAD, S_IFCHR, curproc);
	if (error) {
		printf("hibernate_block_io open failed\n");
		return (1);
	}

	if (iswrite)
		bcopy((caddr_t)dest, bp->b_data, xfer_size);

	bp->b_bcount = xfer_size;
	bp->b_blkno = blkctr;
	CLR(bp->b_flags, B_READ | B_WRITE | B_DONE);
	SET(bp->b_flags, B_BUSY | (iswrite ? B_WRITE : B_READ) | B_RAW);
	bp->b_dev = hib->dev;
	(*bdsw->d_strategy)(bp);

	error = biowait(bp);
	if (error) {
		printf("hib block_io biowait error %d blk %lld size %zu\n",
			error, (long long)blkctr, xfer_size);
		error = (*bdsw->d_close)(hib->dev, 0, S_IFCHR,
		    curproc);
		if (error)
			printf("hibernate_block_io error close failed\n");
		return (1);
	}

	error = (*bdsw->d_close)(hib->dev, FREAD, S_IFCHR, curproc);
	if (error) {
		printf("hibernate_block_io close failed\n");
		return (1);
	}

	if (!iswrite)
		bcopy(bp->b_data, (caddr_t)dest, xfer_size);

	bp->b_flags |= B_INVAL;
	brelse(bp);

	return (0);
}

/*
 * Preserve one page worth of random data, generated from the resuming
 * kernel's arc4random. After resume, this preserved entropy can be used
 * to further improve the un-hibernated machine's entropy pool. This
 * random data is stored in the piglet, which is preserved across the
 * unpack operation, and is restored later in the resume process (see
 * hib_getentropy)
 */
void
hibernate_preserve_entropy(union hibernate_info *hib)
{
	void *entropy;

	entropy = km_alloc(PAGE_SIZE, &kv_any, &kp_none, &kd_nowait);

	if (!entropy)
		return;

	pmap_activate(curproc);
	pmap_kenter_pa((vaddr_t)entropy,
	    (paddr_t)(hib->piglet_pa + (29 * PAGE_SIZE)),
	    PROT_READ | PROT_WRITE);

	arc4random_buf((void *)entropy, PAGE_SIZE);
	pmap_kremove((vaddr_t)entropy, PAGE_SIZE);
	km_free(entropy, PAGE_SIZE, &kv_any, &kp_none);
}

#ifndef NO_PROPOLICE
vaddr_t
hibernate_unprotect_ssp(void)
{
	struct kmem_dyn_mode kd_avoidalias;
	vaddr_t va = trunc_page((vaddr_t)&__guard_local);
	paddr_t pa;

	pmap_extract(pmap_kernel(), va, &pa);

	memset(&kd_avoidalias, 0, sizeof kd_avoidalias);
	kd_avoidalias.kd_prefer = pa;
	kd_avoidalias.kd_waitok = 1;
	va = (vaddr_t)km_alloc(PAGE_SIZE, &kv_any, &kp_none, &kd_avoidalias);
	if (!va)
		panic("hibernate_unprotect_ssp");

	pmap_kenter_pa(va, pa, PROT_READ | PROT_WRITE);
	pmap_update(pmap_kernel());

	return va;
}

void
hibernate_reprotect_ssp(vaddr_t va)
{
	pmap_kremove(va, PAGE_SIZE);
	km_free((void *)va, PAGE_SIZE, &kv_any, &kp_none);
}
#endif /* NO_PROPOLICE */

/*
 * Reads the signature block from swap, checks against the current machine's
 * information. If the information matches, perform a resume by reading the
 * saved image into the pig area, and unpacking.
 *
 * Must be called with interrupts enabled.
 */
void
hibernate_resume(void)
{
	union hibernate_info hib;
	int s;
#ifndef NO_PROPOLICE
	vsize_t off = (vaddr_t)&__guard_local -
	    trunc_page((vaddr_t)&__guard_local);
	vaddr_t guard_va;
#endif

	/* Get current running machine's hibernate info */
	memset(&hib, 0, sizeof(hib));
	if (get_hibernate_info(&hib, 0)) {
		DPRINTF("couldn't retrieve machine's hibernate info\n");
		return;
	}

	/* Read hibernate info from disk */
	s = splbio();

	DPRINTF("reading hibernate signature block location: %lld\n",
		hib.sig_offset);

	if (hibernate_block_io(&hib,
	    hib.sig_offset,
	    DEV_BSIZE, (vaddr_t)&disk_hib, 0)) {
		DPRINTF("error in hibernate read");
		splx(s);
		return;
	}

	/* Check magic number */
	if (disk_hib.magic != HIBERNATE_MAGIC) {
		DPRINTF("wrong magic number in hibernate signature: %x\n",
			disk_hib.magic);
		splx(s);
		return;
	}

	/*
	 * We (possibly) found a hibernate signature. Clear signature first,
	 * to prevent accidental resume or endless resume cycles later.
	 */
	if (hibernate_clear_signature()) {
		DPRINTF("error clearing hibernate signature block\n");
		splx(s);
		return;
	}

	/*
	 * If on-disk and in-memory hibernate signatures match,
	 * this means we should do a resume from hibernate.
	 */
	if (hibernate_compare_signature(&hib, &disk_hib)) {
		DPRINTF("mismatched hibernate signature block\n");
		splx(s);
		return;
	}

#ifdef MULTIPROCESSOR
	/* XXX - if we fail later, we may need to rehatch APs on some archs */
	DPRINTF("hibernate: quiescing APs\n");
	hibernate_quiesce_cpus();
#endif /* MULTIPROCESSOR */

	/* Read the image from disk into the image (pig) area */
	if (hibernate_read_image(&disk_hib))
		goto fail;

	DPRINTF("hibernate: quiescing devices\n");
	if (config_suspend_all(DVACT_QUIESCE) != 0)
		goto fail;

#ifndef NO_PROPOLICE
	guard_va = hibernate_unprotect_ssp();
#endif /* NO_PROPOLICE */

	(void) splhigh();
	hibernate_disable_intr_machdep();
	cold = 1;

	DPRINTF("hibernate: suspending devices\n");
	if (config_suspend_all(DVACT_SUSPEND) != 0) {
		cold = 0;
		hibernate_enable_intr_machdep();
#ifndef NO_PROPOLICE
		hibernate_reprotect_ssp(guard_va);
#endif /* ! NO_PROPOLICE */
		goto fail;
	}

	hibernate_preserve_entropy(&disk_hib);

	printf("Unpacking image...\n");

	/* Switch stacks */
	DPRINTF("hibernate: switching stacks\n");
	hibernate_switch_stack_machdep();

#ifndef NO_PROPOLICE
	/* Start using suspended kernel's propolice guard */
	*(long *)(guard_va + off) = disk_hib.guard;
	hibernate_reprotect_ssp(guard_va);
#endif /* ! NO_PROPOLICE */

	/* Unpack and resume */
	hibernate_unpack_image(&disk_hib);

fail:
	splx(s);
	printf("\nUnable to resume hibernated image\n");
}

/*
 * Unpack image from pig area to original location by looping through the
 * list of output chunks in the order they should be restored (fchunks).
 *
 * Note that due to the stack smash protector and the fact that we have
 * switched stacks, it is not permitted to return from this function.
 */
void
hibernate_unpack_image(union hibernate_info *hib)
{
	struct hibernate_disk_chunk *chunks;
	union hibernate_info local_hib;
	paddr_t image_cur = global_pig_start;
	short i, *fchunks;
	char *pva;

	/* Piglet will be identity mapped (VA == PA) */
	pva = (char *)hib->piglet_pa;

	fchunks = (short *)(pva + (4 * PAGE_SIZE));

	chunks = (struct hibernate_disk_chunk *)(pva + HIBERNATE_CHUNK_SIZE);

	/* Can't use hiber_info that's passed in after this point */
	bcopy(hib, &local_hib, sizeof(union hibernate_info));

	/* VA == PA */
	local_hib.piglet_va = local_hib.piglet_pa;

	/*
	 * Point of no return. Once we pass this point, only kernel code can
	 * be accessed. No global variables or other kernel data structures
	 * are guaranteed to be coherent after unpack starts.
	 *
	 * The image is now in high memory (pig area), we unpack from the pig
	 * to the correct location in memory. We'll eventually end up copying
	 * on top of ourself, but we are assured the kernel code here is the
	 * same between the hibernated and resuming kernel, and we are running
	 * on our own stack, so the overwrite is ok.
	 */
	DPRINTF("hibernate: activating alt. pagetable and starting unpack\n");
	hibernate_activate_resume_pt_machdep();

	for (i = 0; i < local_hib.chunk_ctr; i++) {
		/* Reset zlib for inflate */
		if (hibernate_zlib_reset(&local_hib, 0) != Z_OK)
			panic("hibernate failed to reset zlib for inflate");

		hibernate_process_chunk(&local_hib, &chunks[fchunks[i]],
		    image_cur);

		image_cur += chunks[fchunks[i]].compressed_size;

	}

	/*
	 * Resume the loaded kernel by jumping to the MD resume vector.
	 * We won't be returning from this call.
	 */
	hibernate_resume_machdep();
}

/*
 * Bounce a compressed image chunk to the piglet, entering mappings for the
 * copied pages as needed
 */
void
hibernate_copy_chunk_to_piglet(paddr_t img_cur, vaddr_t piglet, size_t size)
{
	size_t ct, ofs;
	paddr_t src = img_cur;
	vaddr_t dest = piglet;

	/* Copy first partial page */
	ct = (PAGE_SIZE) - (src & PAGE_MASK);
	ofs = (src & PAGE_MASK);

	if (ct < PAGE_SIZE) {
		hibernate_enter_resume_mapping(HIBERNATE_INFLATE_PAGE,
			(src - ofs), 0);
		hibernate_flush();
		bcopy((caddr_t)(HIBERNATE_INFLATE_PAGE + ofs), (caddr_t)dest, ct);
		src += ct;
		dest += ct;
	}

	/* Copy remaining pages */	
	while (src < size + img_cur) {
		hibernate_enter_resume_mapping(HIBERNATE_INFLATE_PAGE, src, 0);
		hibernate_flush();
		ct = PAGE_SIZE;
		bcopy((caddr_t)(HIBERNATE_INFLATE_PAGE), (caddr_t)dest, ct);
		hibernate_flush();
		src += ct;
		dest += ct;
	}
}

/*
 * Process a chunk by bouncing it to the piglet, followed by unpacking 
 */
void
hibernate_process_chunk(union hibernate_info *hib,
    struct hibernate_disk_chunk *chunk, paddr_t img_cur)
{
	char *pva = (char *)hib->piglet_va;

	hibernate_copy_chunk_to_piglet(img_cur,
	 (vaddr_t)(pva + (HIBERNATE_CHUNK_SIZE * 2)), chunk->compressed_size);
	hibernate_inflate_region(hib, chunk->base,
	    (vaddr_t)(pva + (HIBERNATE_CHUNK_SIZE * 2)),
	    chunk->compressed_size);
}

/*
 * Calculate RLE component for 'inaddr'. Clamps to max RLE pages between
 * inaddr and range_end.
 */
int
hibernate_calc_rle(paddr_t inaddr, paddr_t range_end)
{
	int rle;

	rle = uvm_page_rle(inaddr);
	KASSERT(rle >= 0 && rle <= MAX_RLE);

	/* Clamp RLE to range end */
	if (rle > 0 && inaddr + (rle * PAGE_SIZE) > range_end)
		rle = (range_end - inaddr) / PAGE_SIZE;

	return (rle);
}

/*
 * Write the RLE byte for page at 'inaddr' to the output stream.
 * Returns the number of pages to be skipped at 'inaddr'.
 */
int
hibernate_write_rle(union hibernate_info *hib, paddr_t inaddr,
	paddr_t range_end, daddr_t *blkctr,
	size_t *out_remaining)
{
	int rle, err, *rleloc;
	struct hibernate_zlib_state *hibernate_state;
	vaddr_t hibernate_io_page = hib->piglet_va + PAGE_SIZE;

	hibernate_state =
	    (struct hibernate_zlib_state *)HIBERNATE_HIBALLOC_PAGE;

	rle = hibernate_calc_rle(inaddr, range_end);

	rleloc = (int *)hibernate_rle_page + MAX_RLE - 1;
	*rleloc = rle;

	/* Deflate the RLE byte into the stream */
	hibernate_deflate(hib, (paddr_t)rleloc, out_remaining);

	/* Did we fill the output page? If so, flush to disk */
	if (*out_remaining == 0) {
		if ((err = hib->io_func(hib->dev, *blkctr + hib->image_offset,
			(vaddr_t)hibernate_io_page, PAGE_SIZE, HIB_W,
			hib->io_page))) {
				DPRINTF("hib write error %d\n", err);
				return (err);
		}

		*blkctr += PAGE_SIZE / DEV_BSIZE;
		*out_remaining = PAGE_SIZE;

		/* If we didn't deflate the entire RLE byte, finish it now */
		if (hibernate_state->hib_stream.avail_in != 0)
			hibernate_deflate(hib,
				(vaddr_t)hibernate_state->hib_stream.next_in,
				out_remaining);
	}

	return (rle);
}

/*
 * Write a compressed version of this machine's memory to disk, at the
 * precalculated swap offset:
 *
 * end of swap - signature block size - chunk table size - memory size
 *
 * The function begins by looping through each phys mem range, cutting each
 * one into MD sized chunks. These chunks are then compressed individually
 * and written out to disk, in phys mem order. Some chunks might compress
 * more than others, and for this reason, each chunk's size is recorded
 * in the chunk table, which is written to disk after the image has
 * properly been compressed and written (in hibernate_write_chunktable).
 *
 * When this function is called, the machine is nearly suspended - most
 * devices are quiesced/suspended, interrupts are off, and cold has
 * been set. This means that there can be no side effects once the
 * write has started, and the write function itself can also have no
 * side effects. This also means no printfs are permitted (since printf
 * has side effects.)
 *
 * Return values :
 *
 * 0      - success
 * EIO    - I/O error occurred writing the chunks
 * EINVAL - Failed to write a complete range
 * ENOMEM - Memory allocation failure during preparation of the zlib arena
 */
int
hibernate_write_chunks(union hibernate_info *hib)
{
	paddr_t range_base, range_end, inaddr, temp_inaddr;
	size_t nblocks, out_remaining, used;
	struct hibernate_disk_chunk *chunks;
	vaddr_t hibernate_io_page = hib->piglet_va + PAGE_SIZE;
	daddr_t blkctr = 0;
	int i, rle, err;
	struct hibernate_zlib_state *hibernate_state;

	hibernate_state =
	    (struct hibernate_zlib_state *)HIBERNATE_HIBALLOC_PAGE;

	hib->chunk_ctr = 0;

	/*
	 * Map the utility VAs to the piglet. See the piglet map at the
	 * top of this file for piglet layout information.
	 */
	hibernate_copy_page = hib->piglet_va + 3 * PAGE_SIZE;
	hibernate_rle_page = hib->piglet_va + 28 * PAGE_SIZE;

	chunks = (struct hibernate_disk_chunk *)(hib->piglet_va +
	    HIBERNATE_CHUNK_SIZE);

	/* Calculate the chunk regions */
	for (i = 0; i < hib->nranges; i++) {
		range_base = hib->ranges[i].base;
		range_end = hib->ranges[i].end;

		inaddr = range_base;

		while (inaddr < range_end) {
			chunks[hib->chunk_ctr].base = inaddr;
			if (inaddr + HIBERNATE_CHUNK_SIZE < range_end)
				chunks[hib->chunk_ctr].end = inaddr +
				    HIBERNATE_CHUNK_SIZE;
			else
				chunks[hib->chunk_ctr].end = range_end;

			inaddr += HIBERNATE_CHUNK_SIZE;
			hib->chunk_ctr ++;
		}
	}

	uvm_pmr_dirty_everything();
	uvm_pmr_zero_everything();

	/* Compress and write the chunks in the chunktable */
	for (i = 0; i < hib->chunk_ctr; i++) {
		range_base = chunks[i].base;
		range_end = chunks[i].end;

		chunks[i].offset = blkctr + hib->image_offset;

		/* Reset zlib for deflate */
		if (hibernate_zlib_reset(hib, 1) != Z_OK) {
			DPRINTF("hibernate_zlib_reset failed for deflate\n");
			return (ENOMEM);
		}

		inaddr = range_base;

		/*
		 * For each range, loop through its phys mem region
		 * and write out the chunks (the last chunk might be
		 * smaller than the chunk size).
		 */
		while (inaddr < range_end) {
			out_remaining = PAGE_SIZE;
			while (out_remaining > 0 && inaddr < range_end) {
				/*
				 * Adjust for regions that are not evenly
				 * divisible by PAGE_SIZE or overflowed
				 * pages from the previous iteration.
				 */
				temp_inaddr = (inaddr & PAGE_MASK) +
				    hibernate_copy_page;
				
				/* Deflate from temp_inaddr to IO page */
				if (inaddr != range_end) {
					if (inaddr % PAGE_SIZE == 0) {
						rle = hibernate_write_rle(hib,
							inaddr,
							range_end,
							&blkctr,
							&out_remaining);
					}
				
					if (rle == 0) {
						pmap_kenter_pa(hibernate_temp_page,
							inaddr & PMAP_PA_MASK,
							PROT_READ);

						bcopy((caddr_t)hibernate_temp_page,
							(caddr_t)hibernate_copy_page,
							PAGE_SIZE);
						inaddr += hibernate_deflate(hib,
							temp_inaddr,
							&out_remaining);
					} else {
						inaddr += rle * PAGE_SIZE;
						if (inaddr > range_end)
							inaddr = range_end;
					}

				}

				if (out_remaining == 0) {
					/* Filled up the page */
					nblocks = PAGE_SIZE / DEV_BSIZE;

					if ((err = hib->io_func(hib->dev,
					    blkctr + hib->image_offset,
					    (vaddr_t)hibernate_io_page,
					    PAGE_SIZE, HIB_W, hib->io_page))) {
						DPRINTF("hib write error %d\n",
						    err);
						return (err);
					}

					blkctr += nblocks;
				}
			}
		}

		if (inaddr != range_end) {
			DPRINTF("deflate range ended prematurely\n");
			return (EINVAL);
		}

		/*
		 * End of range. Round up to next secsize bytes
		 * after finishing compress
		 */
		if (out_remaining == 0)
			out_remaining = PAGE_SIZE;

		/* Finish compress */
		hibernate_state->hib_stream.next_in = (unsigned char *)inaddr;
		hibernate_state->hib_stream.avail_in = 0;
		hibernate_state->hib_stream.next_out =
		    (unsigned char *)hibernate_io_page +
			(PAGE_SIZE - out_remaining);

		/* We have an extra output page available for finalize */
		hibernate_state->hib_stream.avail_out =
			out_remaining + PAGE_SIZE;

		if ((err = deflate(&hibernate_state->hib_stream, Z_FINISH)) !=
		    Z_STREAM_END) {
			DPRINTF("deflate error in output stream: %d\n", err);
			return (err);
		}

		out_remaining = hibernate_state->hib_stream.avail_out;

		used = 2 * PAGE_SIZE - out_remaining;
		nblocks = used / DEV_BSIZE;

		/* Round up to next block if needed */
		if (used % DEV_BSIZE != 0)
			nblocks ++;

		/* Write final block(s) for this chunk */
		if ((err = hib->io_func(hib->dev, blkctr + hib->image_offset,
		    (vaddr_t)hibernate_io_page, nblocks*DEV_BSIZE,
		    HIB_W, hib->io_page))) {
			DPRINTF("hib final write error %d\n", err);
			return (err);
		}

		blkctr += nblocks;

		chunks[i].compressed_size = (blkctr + hib->image_offset -
		    chunks[i].offset) * DEV_BSIZE;
	}

	hib->chunktable_offset = hib->image_offset + blkctr;
	return (0);
}

/*
 * Reset the zlib stream state and allocate a new hiballoc area for either
 * inflate or deflate. This function is called once for each hibernate chunk.
 * Calling hiballoc_init multiple times is acceptable since the memory it is
 * provided is unmanaged memory (stolen). We use the memory provided to us
 * by the piglet allocated via the supplied hib.
 */
int
hibernate_zlib_reset(union hibernate_info *hib, int deflate)
{
	vaddr_t hibernate_zlib_start;
	size_t hibernate_zlib_size;
	char *pva = (char *)hib->piglet_va;
	struct hibernate_zlib_state *hibernate_state;

	hibernate_state =
	    (struct hibernate_zlib_state *)HIBERNATE_HIBALLOC_PAGE;

	if (!deflate)
		pva = (char *)((paddr_t)pva & (PIGLET_PAGE_MASK));

	/*
	 * See piglet layout information at the start of this file for
	 * information on the zlib page assignments.
	 */
	hibernate_zlib_start = (vaddr_t)(pva + (30 * PAGE_SIZE));
	hibernate_zlib_size = 80 * PAGE_SIZE;

	memset((void *)hibernate_zlib_start, 0, hibernate_zlib_size);
	memset(hibernate_state, 0, PAGE_SIZE);

	/* Set up stream structure */
	hibernate_state->hib_stream.zalloc = (alloc_func)hibernate_zlib_alloc;
	hibernate_state->hib_stream.zfree = (free_func)hibernate_zlib_free;

	/* Initialize the hiballoc arena for zlib allocs/frees */
	hiballoc_init(&hibernate_state->hiballoc_arena,
	    (caddr_t)hibernate_zlib_start, hibernate_zlib_size);

	if (deflate) {
		return deflateInit(&hibernate_state->hib_stream,
		    Z_BEST_SPEED);
	} else
		return inflateInit(&hibernate_state->hib_stream);
}

/*
 * Reads the hibernated memory image from disk, whose location and
 * size are recorded in hib. Begin by reading the persisted
 * chunk table, which records the original chunk placement location
 * and compressed size for each. Next, allocate a pig region of
 * sufficient size to hold the compressed image. Next, read the
 * chunks into the pig area (calling hibernate_read_chunks to do this),
 * and finally, if all of the above succeeds, clear the hibernate signature.
 * The function will then return to hibernate_resume, which will proceed
 * to unpack the pig image to the correct place in memory.
 */
int
hibernate_read_image(union hibernate_info *hib)
{
	size_t compressed_size, disk_size, chunktable_size, pig_sz;
	paddr_t image_start, image_end, pig_start, pig_end;
	struct hibernate_disk_chunk *chunks;
	daddr_t blkctr;
	vaddr_t chunktable = (vaddr_t)NULL;
	paddr_t piglet_chunktable = hib->piglet_pa +
	    HIBERNATE_CHUNK_SIZE;
	int i, status;

	status = 0;
	pmap_activate(curproc);

	/* Calculate total chunk table size in disk blocks */
	chunktable_size = HIBERNATE_CHUNK_TABLE_SIZE / DEV_BSIZE;

	blkctr = hib->chunktable_offset;

	chunktable = (vaddr_t)km_alloc(HIBERNATE_CHUNK_TABLE_SIZE, &kv_any,
	    &kp_none, &kd_nowait);

	if (!chunktable)
		return (1);

	/* Map chunktable pages */
	for (i = 0; i < HIBERNATE_CHUNK_TABLE_SIZE; i += PAGE_SIZE)
		pmap_kenter_pa(chunktable + i, piglet_chunktable + i,
		    PROT_READ | PROT_WRITE);
	pmap_update(pmap_kernel());

	/* Read the chunktable from disk into the piglet chunktable */
	for (i = 0; i < HIBERNATE_CHUNK_TABLE_SIZE;
	    i += MAXPHYS, blkctr += MAXPHYS/DEV_BSIZE)
		hibernate_block_io(hib, blkctr, MAXPHYS,
		    chunktable + i, 0);

	blkctr = hib->image_offset;
	compressed_size = 0;

	chunks = (struct hibernate_disk_chunk *)chunktable;

	for (i = 0; i < hib->chunk_ctr; i++)
		compressed_size += chunks[i].compressed_size;

	disk_size = compressed_size;

	printf("unhibernating @@ block %lld length %lu bytes\n",
	    hib->sig_offset - chunktable_size,
	    compressed_size);

	/* Allocate the pig area */
	pig_sz = compressed_size + HIBERNATE_CHUNK_SIZE;
	if (uvm_pmr_alloc_pig(&pig_start, pig_sz, hib->piglet_pa) == ENOMEM) {
		status = 1;
		goto unmap;
	}

	pig_end = pig_start + pig_sz;

	/* Calculate image extents. Pig image must end on a chunk boundary. */
	image_end = pig_end & ~(HIBERNATE_CHUNK_SIZE - 1);
	image_start = image_end - disk_size;

	hibernate_read_chunks(hib, image_start, image_end, disk_size,
	    chunks);

	/* Prepare the resume time pmap/page table */
	hibernate_populate_resume_pt(hib, image_start, image_end);

unmap:
	/* Unmap chunktable pages */
	pmap_kremove(chunktable, HIBERNATE_CHUNK_TABLE_SIZE);
	pmap_update(pmap_kernel());

	return (status);
}

/*
 * Read the hibernated memory chunks from disk (chunk information at this
 * point is stored in the piglet) into the pig area specified by
 * [pig_start .. pig_end]. Order the chunks so that the final chunk is the
 * only chunk with overlap possibilities.
 */
int
hibernate_read_chunks(union hibernate_info *hib, paddr_t pig_start,
    paddr_t pig_end, size_t image_compr_size,
    struct hibernate_disk_chunk *chunks)
{
	paddr_t img_cur, piglet_base;
	daddr_t blkctr;
	size_t processed, compressed_size, read_size;
	int nchunks, nfchunks, num_io_pages;
	vaddr_t tempva, hibernate_fchunk_area;
	short *fchunks, i, j;

	tempva = (vaddr_t)NULL;
	hibernate_fchunk_area = (vaddr_t)NULL;
	nfchunks = 0;
	piglet_base = hib->piglet_pa;
	global_pig_start = pig_start;

	/*
	 * These mappings go into the resuming kernel's page table, and are
	 * used only during image read. They dissappear from existence
	 * when the suspended kernel is unpacked on top of us.
	 */
	tempva = (vaddr_t)km_alloc(MAXPHYS + PAGE_SIZE, &kv_any, &kp_none,
		&kd_nowait);
	if (!tempva)
		return (1);
	hibernate_fchunk_area = (vaddr_t)km_alloc(24 * PAGE_SIZE, &kv_any,
	    &kp_none, &kd_nowait);
	if (!hibernate_fchunk_area)
		return (1);

	/* Final output chunk ordering VA */
	fchunks = (short *)hibernate_fchunk_area;

	/* Map the chunk ordering region */
	for(i = 0; i < 24 ; i++)
		pmap_kenter_pa(hibernate_fchunk_area + (i * PAGE_SIZE),
			piglet_base + ((4 + i) * PAGE_SIZE),
			PROT_READ | PROT_WRITE);
	pmap_update(pmap_kernel());

	nchunks = hib->chunk_ctr;

	/* Initially start all chunks as unplaced */
	for (i = 0; i < nchunks; i++)
		chunks[i].flags = 0;

	/*
	 * Search the list for chunks that are outside the pig area. These
	 * can be placed first in the final output list.
	 */
	for (i = 0; i < nchunks; i++) {
		if (chunks[i].end <= pig_start || chunks[i].base >= pig_end) {
			fchunks[nfchunks] = i;
			nfchunks++;
			chunks[i].flags |= HIBERNATE_CHUNK_PLACED;
		}
	}

	/*
	 * Walk the ordering, place the chunks in ascending memory order.
	 */
	for (i = 0; i < nchunks; i++) {
		if (chunks[i].flags != HIBERNATE_CHUNK_PLACED) {
			fchunks[nfchunks] = i;
			nfchunks++;
			chunks[i].flags = HIBERNATE_CHUNK_PLACED;
		}
	}

	img_cur = pig_start;

	for (i = 0; i < nfchunks; i++) {
		blkctr = chunks[fchunks[i]].offset;
		processed = 0;
		compressed_size = chunks[fchunks[i]].compressed_size;

		while (processed < compressed_size) {
			if (compressed_size - processed >= MAXPHYS)
				read_size = MAXPHYS;
			else
				read_size = compressed_size - processed;

			/*
			 * We're reading read_size bytes, offset from the
			 * start of a page by img_cur % PAGE_SIZE, so the
			 * end will be read_size + (img_cur % PAGE_SIZE)
			 * from the start of the first page.  Round that
			 * up to the next page size.
			 */
			num_io_pages = (read_size + (img_cur % PAGE_SIZE)
				+ PAGE_SIZE - 1) / PAGE_SIZE;

			KASSERT(num_io_pages <= MAXPHYS/PAGE_SIZE + 1);

			/* Map pages for this read */
			for (j = 0; j < num_io_pages; j ++)
				pmap_kenter_pa(tempva + j * PAGE_SIZE,
				    img_cur + j * PAGE_SIZE,
				    PROT_READ | PROT_WRITE);

			pmap_update(pmap_kernel());

			hibernate_block_io(hib, blkctr, read_size,
			    tempva + (img_cur & PAGE_MASK), 0);

			blkctr += (read_size / DEV_BSIZE);

			pmap_kremove(tempva, num_io_pages * PAGE_SIZE);
			pmap_update(pmap_kernel());

			processed += read_size;
			img_cur += read_size;
		}
	}

	pmap_kremove(hibernate_fchunk_area, 24 * PAGE_SIZE);
	pmap_update(pmap_kernel());	

	return (0);
}

/*
 * Hibernating a machine comprises the following operations:
 *  1. Calculating this machine's hibernate_info information
 *  2. Allocating a piglet and saving the piglet's physaddr
 *  3. Calculating the memory chunks
 *  4. Writing the compressed chunks to disk
 *  5. Writing the chunk table
 *  6. Writing the signature block (hibernate_info)
 *
 * On most architectures, the function calling hibernate_suspend would
 * then power off the machine using some MD-specific implementation.
 */
int
hibernate_suspend(void)
{
	union hibernate_info hib;
	u_long start, end;

	/*
	 * Calculate memory ranges, swap offsets, etc.
	 * This also allocates a piglet whose physaddr is stored in
	 * hib->piglet_pa and vaddr stored in hib->piglet_va
	 */
	if (get_hibernate_info(&hib, 1)) {
		DPRINTF("failed to obtain hibernate info\n");
		return (1);
	}

	/* Find a page-addressed region in swap [start,end] */
	if (uvm_hibswap(hib.dev, &start, &end)) {
		printf("hibernate: cannot find any swap\n");
		return (1);
	}

	if (end - start < 1000) {
		printf("hibernate: insufficient swap (%lu is too small)\n",
			end - start);
		return (1);
	}

	/* Calculate block offsets in swap */
	hib.image_offset = ctod(start);

	DPRINTF("hibernate @@ block %lld max-length %lu blocks\n",
	    hib.image_offset, ctod(end) - ctod(start));

	pmap_activate(curproc);
	DPRINTF("hibernate: writing chunks\n");
	if (hibernate_write_chunks(&hib)) {
		DPRINTF("hibernate_write_chunks failed\n");
		return (1);
	}

	DPRINTF("hibernate: writing chunktable\n");
	if (hibernate_write_chunktable(&hib)) {
		DPRINTF("hibernate_write_chunktable failed\n");
		return (1);
	}

	DPRINTF("hibernate: writing signature\n");
	if (hibernate_write_signature(&hib)) {
		DPRINTF("hibernate_write_signature failed\n");
		return (1);
	}

	/* Allow the disk to settle */
	delay(500000);

	/*
	 * Give the device-specific I/O function a notification that we're
	 * done, and that it can clean up or shutdown as needed.
	 */
	hib.io_func(hib.dev, 0, (vaddr_t)NULL, 0, HIB_DONE, hib.io_page);
	return (0);
}

int
hibernate_alloc(void)
{
	KASSERT(global_piglet_va == 0);
	KASSERT(hibernate_temp_page == 0);

	pmap_activate(curproc);
	pmap_kenter_pa(HIBERNATE_HIBALLOC_PAGE, HIBERNATE_HIBALLOC_PAGE,
		PROT_READ | PROT_WRITE);

	/* Allocate a piglet, store its addresses in the supplied globals */
	if (uvm_pmr_alloc_piglet(&global_piglet_va, &global_piglet_pa,
	    HIBERNATE_CHUNK_SIZE * 4, HIBERNATE_CHUNK_SIZE))
		return (ENOMEM);

	/*
	 * Allocate VA for the temp page.
	 * 
	 * This will become part of the suspended kernel and will
	 * be freed in hibernate_free, upon resume (or hibernate
	 * failure)
	 */
	hibernate_temp_page = (vaddr_t)km_alloc(PAGE_SIZE, &kv_any,
	    &kp_none, &kd_nowait);
	if (!hibernate_temp_page) {
		DPRINTF("out of memory allocating hibernate_temp_page\n");
		return (ENOMEM);
	}

	return (0);
}

/*
 * Free items allocated by hibernate_alloc()
 */
void
hibernate_free(void)
{
	pmap_activate(curproc);

	if (global_piglet_va)
		uvm_pmr_free_piglet(global_piglet_va,
		    4 * HIBERNATE_CHUNK_SIZE);

	if (hibernate_temp_page) {
		pmap_kremove(hibernate_temp_page, PAGE_SIZE);
		km_free((void *)hibernate_temp_page, PAGE_SIZE,
		    &kv_any, &kp_none);
	}

	global_piglet_va = 0;
	hibernate_temp_page = 0;
	pmap_kremove(HIBERNATE_HIBALLOC_PAGE, PAGE_SIZE);
	pmap_update(pmap_kernel());
}
@


1.119
log
@move the vm_page struct from being stored in RB macro trees to RBT functions

vm_page structs go into three trees, uvm_objtree, uvm_pmr_addr, and
uvm_pmr_size. all these have been moved to RBT code.

this should give us a decent chunk of code space back.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.118 2016/09/05 22:27:23 beck Exp $	*/
d121 1
a121 1
	RB_ENTRY(hiballoc_entry) hibe_entry;
d157 1
a157 1
hibe_cmp(struct hiballoc_entry *l, struct hiballoc_entry *r)
d165 1
a165 1
RB_PROTOTYPE(hiballoc_addr, hiballoc_entry, hibe_entry, hibe_cmp)
d193 1
a193 1
RB_GENERATE(hiballoc_addr, hiballoc_entry, hibe_entry, hibe_cmp)
d223 1
a223 1
	entry = RB_ROOT(&arena->hib_addrs);
d225 1
a225 1
		RB_FOREACH_REVERSE(entry, hiballoc_addr, &arena->hib_addrs) {
d248 1
a248 1
	if (RB_INSERT(hiballoc_addr, &arena->hib_addrs, new_entry) != NULL)
d283 1
a283 1
	if (RB_FIND(hiballoc_addr, &arena->hib_addrs, entry) != entry)
d292 1
a292 1
	prev = RB_PREV(hiballoc_addr, &arena->hib_addrs, entry);
d297 1
a297 1
		RB_REMOVE(hiballoc_addr, &arena->hib_addrs, entry);
d319 1
a319 1
	RB_INIT(&arena->hib_addrs);
d341 1
a341 1
	RB_INSERT(hiballoc_addr, &arena->hib_addrs, entry);
@


1.118
log
@Fix hibernation - make stack protector writable during resume
Committing for guenther@@ because he is on United and apparently
they break ssh (as well as guitars)

ok deraadt@@ in the car from cambridge
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.117 2016/09/01 13:12:59 akfaew Exp $	*/
d369 2
a370 2
		while ((pg = RB_ROOT(&pmr->size[UVM_PMR_MEMTYPE_DIRTY]))
		    != NULL) {
d408 2
a409 2
		while ((pg = RB_ROOT(&pmr->size[UVM_PMR_MEMTYPE_ZERO]))
		    != NULL) {
@


1.117
log
@Fix undefined behaviour when comparing pointers by casting them to vaddr_t.

OK mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.116 2015/05/04 02:18:05 mlarkin Exp $	*/
d1038 31
d1081 5
d1150 4
d1162 3
d1178 2
a1179 1
	__guard_local = disk_hib.guard;
@


1.116
log
@
Clarify that hibernate_resume must be called with interrupts enabled.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.115 2015/02/07 02:50:53 mlarkin Exp $	*/
d159 4
a162 1
	return l < r ? -1 : (l > r);
@


1.115
log
@
Preserve a page's worth of random data on hibernate resume. Used to improve
entropy after resuming.

Tested on i386 and amd64.
ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.114 2015/02/07 01:19:40 deraadt Exp $	*/
d1039 2
@


1.114
log
@New framework that allows hibernate to pass in entropy from it's fresh
boot.
ok mlarkin
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.113 2015/02/06 05:17:48 mlarkin Exp $	*/
d56 2
a57 1
 * 109*PAGE_SIZE		end of hiballoc area (80 pages)
d256 5
a260 1
	/* fill in */
d1008 28
d1120 2
d1555 1
a1555 1
	hibernate_zlib_start = (vaddr_t)(pva + (29 * PAGE_SIZE));
@


1.113
log
@
Fix a hibernate crash on some machines due to unmapping a page that
may not have been mapped previously (in the failure to hibernate case).

Also ensure that the lowmem ptp is mapped in all cases (not just MP).

ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.112 2015/01/12 07:11:41 deraadt Exp $	*/
d250 6
@


1.112
log
@hibernate_suspend() should not pmap_kremove by itself; hibernate_free()
must do that.  otherwise, pmap_kremove is called twice.  i386 in particular
does not tolerate that, found by sebastia
ok mlarkin kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.111 2014/12/22 22:22:35 mlarkin Exp $	*/
a1400 2
						pmap_activate(curproc);

a1799 2
	pmap_kenter_pa(HIBERNATE_HIBALLOC_PAGE, HIBERNATE_HIBALLOC_PAGE,
		PROT_READ | PROT_WRITE);
a1800 1

d1836 4
d1868 2
@


1.111
log
@
fix an error in piglet allocation when requesting an alignment < PAGE_SIZE
(which we never did, but it was a bug nonetheless).

ok kettenis, deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.110 2014/12/17 19:42:15 tedu Exp $	*/
d1809 1
a1809 1
		goto fail;
d1815 1
a1815 1
		goto fail;
d1821 1
a1821 1
		goto fail;
a1831 1

a1832 4
fail:
	pmap_kremove(HIBERNATE_HIBALLOC_PAGE, PAGE_SIZE);
	pmap_update(pmap_kernel());
	return (1);
@


1.110
log
@remove lock.h from uvm_extern.h. another holdover from the simpletonlock
era. fix uvm including c files to include lock.h or atomic.h as necessary.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.109 2014/11/16 12:31:00 deraadt Exp $	*/
d471 2
a472 1
		align = PAGE_SIZE;
@


1.109
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.108 2014/11/05 05:48:45 mlarkin Exp $	*/
d31 2
d35 1
@


1.108
log
@
No reason to have things like the hibernate allocation area and chunk
ordering regions mapped executable, so remove those permissions.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.107 2014/11/02 22:59:58 mlarkin Exp $	*/
d1395 1
a1395 1
							VM_PROT_READ);
d1573 1
a1573 1
		    VM_PROT_READ | VM_PROT_WRITE);
d1668 1
a1668 1
			VM_PROT_READ | VM_PROT_WRITE);
d1728 2
a1729 2
					img_cur + j * PAGE_SIZE,
					VM_PROT_READ | VM_PROT_WRITE);
d1799 1
a1799 1
		VM_PROT_READ | VM_PROT_WRITE);
@


1.107
log
@
Unmap the hibernate hiballoc page after we are done with it.

ok deraadt, kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.106 2014/10/22 05:44:00 mlarkin Exp $	*/
d1395 1
a1395 1
							VM_PROT_ALL);
d1573 1
a1573 1
		    VM_PROT_ALL);
d1667 2
a1668 1
			piglet_base + ((4 + i) * PAGE_SIZE), VM_PROT_ALL);
d1728 2
a1729 1
					img_cur + j * PAGE_SIZE, VM_PROT_ALL);
d1799 1
a1799 1
		VM_PROT_ALL);
@


1.106
log
@Remove some unused/unneeded code.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.105 2014/10/22 04:46:05 mlarkin Exp $	*/
a1646 2
	pmap_activate(curproc);

d1803 1
a1803 1
		return (1);
d1809 1
a1809 1
		return (1);
d1815 1
a1815 1
		return (1);
d1828 4
a1873 1
		pmap_update(pmap_kernel());
d1880 2
@


1.105
log
@
Use the global piglet address variables as sparingly as possible to avoid
redundant copies of the same information. No functional change.

Also add some comments as to how these globals are used.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.104 2014/10/16 04:19:33 mlarkin Exp $	*/
a1076 4
	pmap_kenter_pa(HIBERNATE_HIBALLOC_PAGE, HIBERNATE_HIBALLOC_PAGE,
	    VM_PROT_ALL);
	pmap_activate(curproc);

a1110 4
	struct hibernate_zlib_state *hibernate_state;

	hibernate_state =
	    (struct hibernate_zlib_state *)HIBERNATE_HIBALLOC_PAGE;
@


1.104
log
@
No need to reserve separate KVA ranges for the RLE and copy pages in
hibernate anymore.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.103 2014/10/09 00:50:54 mlarkin Exp $	*/
d67 5
d73 7
d584 1
a612 1

d1330 2
a1331 2
	hibernate_copy_page = global_piglet_va + 3 * PAGE_SIZE;
	hibernate_rle_page = global_piglet_va + 28 * PAGE_SIZE;
d1846 1
d1855 2
a1856 1
	 * be freed in hibernate_free, upon resume.
d1878 1
a1878 1
	if (hibernate_temp_page)
d1880 1
a1880 4

	pmap_update(pmap_kernel());

	if (hibernate_temp_page)
d1883 1
@


1.103
log
@

fix some data type mismatches in the zlib paths in hibernate
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.102 2014/10/09 00:42:05 mlarkin Exp $	*/
d1318 2
a1319 6
	pmap_kenter_pa(hibernate_copy_page,
		(hib->piglet_pa + 3 * PAGE_SIZE), VM_PROT_ALL);
	pmap_kenter_pa(hibernate_rle_page,
		(hib->piglet_pa + 28 * PAGE_SIZE), VM_PROT_ALL);

	pmap_activate(curproc);
a1832 2
	KASSERT(hibernate_copy_page == 0);
	KASSERT(hibernate_rle_page == 0);
d1839 1
a1839 1
	 * Allocate VA for the temp and copy page.
d1841 1
a1841 1
	 * These will become part of the suspended kernel and will
a1850 14
	hibernate_copy_page = (vaddr_t)km_alloc(PAGE_SIZE, &kv_any,
	    &kp_none, &kd_nowait);
	if (!hibernate_copy_page) {
		DPRINTF("out of memory allocating hibernate_copy_page\n");
		return (ENOMEM);
	}

	hibernate_rle_page = (vaddr_t)km_alloc(PAGE_SIZE, &kv_any,
	    &kp_none, &kd_nowait);
	if (!hibernate_rle_page) {
		DPRINTF("out of memory allocating hibernate_rle_page\n");
		return (ENOMEM);
	}

a1863 2
	if (hibernate_copy_page)
		pmap_kremove(hibernate_copy_page, PAGE_SIZE);
a1865 2
	if (hibernate_rle_page)
		pmap_kremove(hibernate_rle_page, PAGE_SIZE);
a1868 3
	if (hibernate_copy_page)
		km_free((void *)hibernate_copy_page, PAGE_SIZE,
		    &kv_any, &kp_none);
a1871 3
	if (hibernate_rle_page)
		km_free((void *)hibernate_rle_page, PAGE_SIZE,
		    &kv_any, &kp_none);
a1873 1
	hibernate_copy_page = 0;
a1874 1
	hibernate_rle_page = 0;
@


1.102
log
@

remove an unused function and some unused variables in hibernate
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.101 2014/09/26 09:25:38 kettenis Exp $	*/
d657 1
a657 1
	hibernate_state->hib_stream.next_out = (char *)rle;
d695 3
a697 2
	/* Set up the stream for page inflate */	
	hibernate_state->hib_stream.next_out = (char *)HIBERNATE_INFLATE_PAGE;
d743 1
a743 1
	hibernate_state->hib_stream.next_in = (char *)src;
d788 1
a788 1
	hibernate_state->hib_stream.next_in = (caddr_t)src;
d790 2
a791 2
	hibernate_state->hib_stream.next_out = (caddr_t)hibernate_io_page +
	    (PAGE_SIZE - *remaining);
d1444 1
a1444 1
		hibernate_state->hib_stream.next_in = (caddr_t)inaddr;
d1447 2
a1448 1
		    (caddr_t)hibernate_io_page + (PAGE_SIZE - out_remaining);
@


1.101
log
@Rework piglet and pig allocation.  Currently the piglet gets allocated
deep down in the suspend path, where it is really hard to recover from
allocation failure.  So allocate the piglet early on in the suspend path.
Also change the piglet and piglet allocation functions to use km_alloc(9)
instead of doing pmemrange magic.  This removes a bunch of code which, in the
case of the piglet allocation, is broken since it results in a NULL pointer
dereference.  Also switch the piglet allocation to not wait.  If we can't
allocate 16MB of phys contig memory on a halfway modern machine we're almost
certainly under a lot of memory pressure and we're better off not trying to
hibernate anyway.

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.100 2014/09/19 20:02:25 kettenis Exp $	*/
d50 1
a50 3
 * 4*PAGE_SIZE			final chunk ordering list (8 pages)
 * 12*PAGE_SIZE			piglet chunk ordering list (8 pages)
 * 20*PAGE_SIZE			temp chunk ordering list (8 pages)
a525 1
	int chunktable_size;
a565 2
	chunktable_size = HIBERNATE_CHUNK_TABLE_SIZE / DEV_BSIZE;

a824 1
	struct hibernate_disk_chunk *chunks;
a833 3
	chunks = (struct hibernate_disk_chunk *)(hib->piglet_va +
	    HIBERNATE_CHUNK_SIZE);

a871 28

	return (0);
}

/*
 * Check chunk range overlap when calculating whether or not to copy a
 * compressed chunk to the piglet area before decompressing.
 *
 * returns zero if the ranges do not overlap, non-zero otherwise.
 */
int
hibernate_check_overlap(paddr_t r1s, paddr_t r1e, paddr_t r2s, paddr_t r2e)
{
	/* case A : end of r1 overlaps start of r2 */
	if (r1s < r2s && r1e > r2s)
		return (1);

	/* case B : r1 entirely inside r2 */
	if (r1s >= r2s && r1e <= r2e)
		return (1);

	/* case C : r2 entirely inside r1 */
	if (r2s >= r1s && r2e <= r1e)
		return (1);

	/* case D : end of r2 overlaps start of r1 */
	if (r2s < r1s && r2e > r1s)
		return (1);
@


1.100
log
@Use config_suspend_all(9).
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.99 2014/07/21 01:57:55 mlarkin Exp $	*/
d71 1
d396 2
a397 3
 * Allocate the highest address that can hold sz.
 *
 * sz in bytes.
d400 1
a400 1
uvm_pmr_alloc_pig(paddr_t *addr, psize_t sz)
d402 6
a407 2
	struct uvm_pmemrange	*pmr;
	struct vm_page		*pig_pg, *pg;
d409 1
a409 33
	/*
	 * Convert sz to pages, since that is what pmemrange uses internally.
	 */
	sz = atop(round_page(sz));

	uvm_lock_fpageq();

	TAILQ_FOREACH(pmr, &uvm.pmr_control.use, pmr_use) {
		RB_FOREACH_REVERSE(pig_pg, uvm_pmr_addr, &pmr->addr) {
			if (pig_pg->fpgsz >= sz) {
				goto found;
			}
		}
	}

	/*
	 * Allocation failure.
	 */
	uvm_unlock_fpageq();
	return ENOMEM;

found:
	/* Remove page from freelist. */
	uvm_pmr_remove_size(pmr, pig_pg);
	pig_pg->fpgsz -= sz;
	pg = pig_pg + pig_pg->fpgsz;
	if (pig_pg->fpgsz == 0)
		uvm_pmr_remove_addr(pmr, pig_pg);
	else
		uvm_pmr_insert_size(pmr, pig_pg);

	uvmexp.free -= sz;
	*addr = VM_PAGE_TO_PHYS(pg);
d411 2
a412 14
	/*
	 * Update pg flags.
	 *
	 * Note that we trash the sz argument now.
	 */
	while (sz > 0) {
		KASSERT(pg->pg_flags & PQ_FREE);

		atomic_clearbits_int(&pg->pg_flags, PG_PMAPMASK);

		if (pg->pg_flags & PG_ZERO)
			uvmexp.zeropages -= sz;
		atomic_clearbits_int(&pg->pg_flags,
		    PG_ZERO|PQ_FREE);
d414 4
a417 3
		pg->uobject = NULL;
		pg->uanon = NULL;
		pg->pg_version++;
d419 3
a421 5
		/*
		 * Next.
		 */
		pg++;
		sz--;
d424 1
a424 2
	/* Return. */
	uvm_unlock_fpageq();
d431 1
a431 1
 * This is as low as possible.
d443 5
a447 6
	paddr_t			 pg_addr, piglet_addr;
	struct uvm_pmemrange	*pmr;
	struct vm_page		*pig_pg, *pg;
	struct pglist		 pageq;
	int			 pdaemon_woken;
	vaddr_t			 piglet_va;
a451 2
	pdaemon_woken = 0; /* Didn't wake the pagedaemon. */

d461 2
a462 79
	uvm_lock_fpageq();

	TAILQ_FOREACH_REVERSE(pmr, &uvm.pmr_control.use, uvm_pmemrange_use,
	    pmr_use) {
retry:
		/*
		 * Search for a range with enough space.
		 * Use the address tree, to ensure the range is as low as
		 * possible.
		 */
		RB_FOREACH(pig_pg, uvm_pmr_addr, &pmr->addr) {
			pg_addr = VM_PAGE_TO_PHYS(pig_pg);
			piglet_addr = (pg_addr + (align - 1)) & ~(align - 1);

			if (atop(pg_addr) + pig_pg->fpgsz >=
			    atop(piglet_addr) + atop(sz))
				goto found;
		}
	}

	/*
	 * Try to coerce the pagedaemon into freeing memory
	 * for the piglet.
	 *
	 * pdaemon_woken is set to prevent the code from
	 * falling into an endless loop.
	 */
	if (!pdaemon_woken) {
		pdaemon_woken = 1;
		if (uvm_wait_pla(ptoa(pmr->low), ptoa(pmr->high) - 1,
		    sz, UVM_PLA_FAILOK) == 0)
			goto retry;
	}

	/* Return failure. */
	uvm_unlock_fpageq();
	return ENOMEM;

found:
	/*
	 * Extract piglet from pigpen.
	 */
	TAILQ_INIT(&pageq);
	uvm_pmr_extract_range(pmr, pig_pg,
	    atop(piglet_addr), atop(piglet_addr) + atop(sz), &pageq);

	*pa = piglet_addr;
	uvmexp.free -= atop(sz);

	/*
	 * Update pg flags.
	 *
	 * Note that we trash the sz argument now.
	 */
	TAILQ_FOREACH(pg, &pageq, pageq) {
		KASSERT(pg->pg_flags & PQ_FREE);

		atomic_clearbits_int(&pg->pg_flags, PG_PMAPMASK);

		if (pg->pg_flags & PG_ZERO)
			uvmexp.zeropages--;
		atomic_clearbits_int(&pg->pg_flags,
		    PG_ZERO|PQ_FREE);

		pg->uobject = NULL;
		pg->uanon = NULL;
		pg->pg_version++;
	}

	uvm_unlock_fpageq();

	/*
	 * Now allocate a va.
	 * Use direct mappings for the pages.
	 */

	piglet_va = *va = (vaddr_t)km_alloc(sz, &kv_any, &kp_none, &kd_waitok);
	if (!piglet_va) {
		uvm_pglistfree(&pageq);
a463 10
	}

	/*
	 * Map piglet to va.
	 */
	TAILQ_FOREACH(pg, &pageq, pageq) {
		pmap_kenter_pa(piglet_va, VM_PAGE_TO_PHYS(pg), UVM_PROT_RW);
		piglet_va += PAGE_SIZE;
	}
	pmap_update(pmap_kernel());
d465 1
a474 3
	paddr_t			 pa;
	struct vm_page		*pg;

a480 16
	 * Find the first page in piglet.
	 * Since piglets are contiguous, the first pg is all we need.
	 */
	if (!pmap_extract(pmap_kernel(), va, &pa))
		panic("uvm_pmr_free_piglet: piglet 0x%lx has no pages", va);
	pg = PHYS_TO_VM_PAGE(pa);
	if (pg == NULL)
		panic("uvm_pmr_free_piglet: unmanaged page 0x%lx", pa);

	/*
	 * Unmap.
	 */
	pmap_kremove(va, sz);
	pmap_update(pmap_kernel());

	/*
d483 1
a483 2
	uvm_pmr_freepages(pg, atop(sz));
	km_free((void *)va, sz, &kv_any, &kp_none);
d577 2
a578 7
		/* Allocate piglet region */
		if (uvm_pmr_alloc_piglet(&hib->piglet_va,
		    &hib->piglet_pa, HIBERNATE_CHUNK_SIZE * 4,
		    HIBERNATE_CHUNK_SIZE)) {
			printf("Hibernate failed to allocate the piglet\n");
			return (1);
		}
d610 1
a611 4
	if (suspend)
		uvm_pmr_free_piglet(hib->piglet_va,
		    HIBERNATE_CHUNK_SIZE * 4);

a1350 27
	 * Allocate VA for the temp and copy page.
	 * 
	 * These will become part of the suspended kernel and will
	 * be freed in hibernate_free, upon resume.
	 */
	hibernate_temp_page = (vaddr_t)km_alloc(PAGE_SIZE, &kv_any,
	    &kp_none, &kd_nowait);
	if (!hibernate_temp_page) {
		DPRINTF("out of memory allocating hibernate_temp_page\n");
		return (ENOMEM);
	}

	hibernate_copy_page = (vaddr_t)km_alloc(PAGE_SIZE, &kv_any,
	    &kp_none, &kd_nowait);
	if (!hibernate_copy_page) {
		DPRINTF("out of memory allocating hibernate_copy_page\n");
		return (ENOMEM);
	}

	hibernate_rle_page = (vaddr_t)km_alloc(PAGE_SIZE, &kv_any,
	    &kp_none, &kd_nowait);
	if (!hibernate_rle_page) {
		DPRINTF("out of memory allocating hibernate_rle_page\n");
		return (ENOMEM);
	}

	/*
d1633 1
a1633 1
	if (uvm_pmr_alloc_pig(&pig_start, pig_sz) == ENOMEM) {
a1836 3
	/* Stash the piglet VA so we can free it in the resuming kernel */
	global_piglet_va = hib.piglet_va;

d1867 42
d1910 1
a1910 1
 * Free items allocated by hibernate_suspend()
@


1.99
log
@

fix some wrong comments and a bit of KNF
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.98 2014/07/20 18:05:21 mlarkin Exp $	*/
d1246 1
a1246 1
	if (config_suspend(device_mainbus(), DVACT_QUIESCE) != 0)
d1254 1
a1254 1
	if (config_suspend(device_mainbus(), DVACT_SUSPEND) != 0) {
@


1.98
log
@
Support hibernating to softraid crypto volumes.

much help and ok from deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.97 2014/07/16 07:42:51 mlarkin Exp $	*/
d46 3
a48 3
 * 0				I/O page used during resume
 * 1*PAGE_SIZE		 	I/O page used during hibernate suspend
 * 2*PAGE_SIZE		 	I/O page used during hibernate suspend
d752 3
a754 4
		 * Resuming kernels use a regular I/O page since we won't
		 * have access to the suspended kernel's piglet VA at this
		 * point. No need to free this I/O page as it will vanish
		 * as part of the resume.
d758 1
a758 1
			return (1);
a764 1

d1683 1
a1683 1
		used = 2*PAGE_SIZE - out_remaining;
@


1.97
log
@

Reenable hibernate RLE support and flush+zero all memory after unpack.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.96 2014/07/09 15:12:34 mlarkin Exp $	*/
d689 1
a689 1
	hib->io_func = get_hibernate_io_function();
d697 1
a697 1
	dl_ret = disk_readlabel(&dl, hib->dev, err_string, 128);
@


1.96
log
@

Minor cleanups in wording in some comments and printfs, and added some
dprintfs to help debugging in the field (disabled by default)
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.95 2014/07/09 15:03:12 mlarkin Exp $	*/
d53 3
a55 2
 * 28*PAGE_SIZE			start of hiballoc area
 * 108*PAGE_SIZE		end of hiballoc area (80 pages)
d65 1
d87 5
d806 6
a811 1
 * Inflate next page of data from the image stream
d814 1
a814 1
hibernate_inflate_page(void)
d822 40
a861 1
	/* Set up the stream for inflate */
d902 1
a902 1
	int end_stream = 0 ;
d926 1
a926 1
		end_stream = hibernate_inflate_page();
d928 4
a931 1
		dest += PAGE_SIZE;
d1404 65
d1503 1
a1503 1
	int i, err;
d1519 2
a1520 1
	if (!hibernate_temp_page)
d1522 1
d1531 11
d1543 3
a1545 1
	    (hib->piglet_pa + 3*PAGE_SIZE), VM_PROT_ALL);
d1572 3
a1597 1

d1608 26
a1633 2
					pmap_kenter_pa(hibernate_temp_page,
					    inaddr & PMAP_PA_MASK, VM_PROT_ALL);
a1634 7
					pmap_activate(curproc);

					bcopy((caddr_t)hibernate_temp_page,
					    (caddr_t)hibernate_copy_page,
					    PAGE_SIZE);
					inaddr += hibernate_deflate(hib,
					    temp_inaddr, &out_remaining);
d1639 1
a1639 2
					nblocks =
					    PAGE_SIZE / DEV_BSIZE;
d1731 5
a1735 1
	hibernate_zlib_start = (vaddr_t)(pva + (28 * PAGE_SIZE));
d2072 2
d2083 3
d2090 1
@


1.95
log
@

Don't use the suspending kernel's VA mapping for the piglet. It's far
easier and much less error-prone to just identity map it in the resuming
kernel as we have more control over the VA space layout there (otherwise
we are at the mercy of the suspending kernel's placement of the piglet VA).

This diff also increases the size of the piglet to 4 chunks, to avoid an
overwrite issue seen in m2k14 where the start of the kernel text was
overwritten with a bounced chunk before unpack.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.94 2014/07/09 14:10:25 mlarkin Exp $	*/
d665 1
a665 1
 * Fills out the hibernate_info union pointed to by hiber_info
d1184 2
d1193 1
d1201 1
d1215 1
d1275 1
a1343 1

d1849 1
a1849 1
		printf("cannot find any swap\n");
d1854 2
a1855 1
		printf("%lu\n is too small", end - start);
a1861 1
	/* XXX side effect */
@


1.94
log
@

Cleanup the chunk placement routine by removing the conflict resolver.
Chunks are now sorted by ascending PA and all chunks are bounced before
unpack. This fixes an issue where the trampoline chunks were being placed
at the end of the unpack ordering, causing overwrite during unpack.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.93 2014/07/09 12:43:51 mlarkin Exp $	*/
d42 1
a42 1
 * The piglet size is presently 3*HIBERNATE_CHUNK_SIZE (typically 3*4MB).
d58 1
a58 1
 * 3*HIBERNATE_CHUNK_SIZE	end of piglet
d723 1
a723 1
		    &hib->piglet_pa, HIBERNATE_CHUNK_SIZE*3,
d764 1
a764 1
		    HIBERNATE_CHUNK_SIZE * 3);
d1240 1
a1240 1
	char *pva = (char *)hib->piglet_va;
d1246 3
a1248 2
	/* Mask off based on arch-specific piglet page size */
	pva = (char *)((paddr_t)pva & (PIGLET_PAGE_MASK));
d1251 1
a1251 1
	chunks = (struct hibernate_disk_chunk *)(pva +  HIBERNATE_CHUNK_SIZE);
d1256 3
d1905 1
a1905 1
		    3*HIBERNATE_CHUNK_SIZE);
@


1.93
log
@

Use suspending kernel's stack smash guard to avoid panicing during unpack.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.92 2014/05/31 04:36:59 mlarkin Exp $	*/
d103 26
d1692 1
a1692 4
	paddr_t img_index, img_cur, r1s, r1e, r2s, r2e;
	paddr_t copy_start, copy_end;
	paddr_t piglet_base = hib->piglet_pa;
	paddr_t piglet_end = piglet_base + HIBERNATE_CHUNK_SIZE;
d1695 8
a1702 5
	int overlap, found, nchunks, nochunks = 0, nfchunks = 0, npchunks = 0;
	int num_io_pages;
	short *ochunks, *pchunks, *fchunks, i, j;
	vaddr_t tempva = (vaddr_t)NULL, hibernate_fchunk_area = (vaddr_t)NULL;

d1716 1
a1716 1
	hibernate_fchunk_area = (vaddr_t)km_alloc(24*PAGE_SIZE, &kv_any,
a1723 6
	/* Piglet chunk ordering VA */
	pchunks = (short *)(hibernate_fchunk_area + (8*PAGE_SIZE));

	/* Final chunk ordering VA */
	ochunks = (short *)(hibernate_fchunk_area + (16*PAGE_SIZE));

d1725 3
a1727 3
	for(i=0; i<24 ; i++)
		pmap_kenter_pa(hibernate_fchunk_area + (i*PAGE_SIZE),
			piglet_base + ((4+i)*PAGE_SIZE), VM_PROT_ALL);
a1741 1
			ochunks[nochunks] = i;
a1742 1
			nochunks++;
d1744 1
a1744 1
			chunks[i].flags |= HIBERNATE_CHUNK_USED;
a1749 1
	 * Conflicts might arise, these are handled next.
d1751 5
a1755 79
	do {
		img_index = -1;
		found = 0;
		j = -1;
		for (i = 0; i < nchunks; i++)
			if (chunks[i].base < img_index &&
			    chunks[i].flags == 0 ) {
				j = i;
				img_index = chunks[i].base;
			}

		if (j != -1) {
			found = 1;
			ochunks[nochunks] = j;
			nochunks++;
			chunks[j].flags |= HIBERNATE_CHUNK_PLACED;
		}
	} while (found);

	img_index = pig_start;

	/*
	 * Identify chunk output conflicts (chunks whose pig load area
	 * corresponds to their original memory placement location)
	 */
	for (i = 0; i < nochunks ; i++) {
		overlap = 0;
		r1s = img_index;
		r1e = img_index + chunks[ochunks[i]].compressed_size;
		r2s = chunks[ochunks[i]].base;
		r2e = chunks[ochunks[i]].end;

		overlap = hibernate_check_overlap(r1s, r1e, r2s, r2e);
		if (overlap)
			chunks[ochunks[i]].flags |= HIBERNATE_CHUNK_CONFLICT;
		img_index += chunks[ochunks[i]].compressed_size;
	}

	/*
	 * Prepare the final output chunk list. Calculate an output
	 * inflate strategy for overlapping chunks if needed.
	 */
	for (i = 0; i < nochunks ; i++) {
		/*
		 * If a conflict is detected, consume enough compressed
		 * output chunks to fill the piglet
		 */
		if (chunks[ochunks[i]].flags & HIBERNATE_CHUNK_CONFLICT) {
			copy_start = piglet_base;
			copy_end = piglet_end;
			npchunks = 0;
			j = i;

			while (copy_start < copy_end && j < nochunks) {
				pchunks[npchunks] = ochunks[j];
				npchunks++;
				copy_start +=
				    chunks[ochunks[j]].compressed_size;
				i++;
				j++;
			}

			for (j = 0; j < npchunks; j++) {
				fchunks[nfchunks] = pchunks[j];
				chunks[pchunks[j]].flags |=
				    HIBERNATE_CHUNK_USED;
				nfchunks++;
			}
		} else {
			/*
			 * No conflict, chunk can be added without copying
			 */
			if ((chunks[ochunks[i]].flags &
			    HIBERNATE_CHUNK_USED) == 0) {
				fchunks[nfchunks] = ochunks[i];
				chunks[ochunks[i]].flags |=
				    HIBERNATE_CHUNK_USED;
				nfchunks++;
			}
d1804 1
a1804 3
	pmap_kremove(hibernate_fchunk_area, PAGE_SIZE);
	pmap_kremove((vaddr_t)pchunks, PAGE_SIZE);
	pmap_kremove((vaddr_t)fchunks, PAGE_SIZE);
@


1.92
log
@
Fix a format specifier in a debug printf.

Noticed by Josh Grosse.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.91 2014/05/29 08:00:24 mlarkin Exp $	*/
d80 4
d650 5
d1186 5
@


1.91
log
@
Read MAXPHYS bytes at a time in the hibernate image read, instead of
PAGE_SIZE bytes. Completes the MAXPHYS optimizations in the read path.

with input from guenther@@ who suggested a version that was a little easier
to understand. Tested on i386 and amd64.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.90 2014/05/21 02:26:49 mlarkin Exp $	*/
d1012 5
a1016 2
				i, mine->ranges[i].base, mine->ranges[i].end,
				disk->ranges[i].base, disk->ranges[i].end);
@


1.90
log
@Change use of Z_PARTIAL_FLUSH to Z_SYNC_FLUSH in the hibernate code as per
a warning/comment in zlib.h that Z_PARTIAL_FLUSH "will be removed, use
Z_SYNC_FLUSH instead".
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.89 2014/04/26 05:43:00 mlarkin Exp $	*/
d1656 1
d1669 2
a1670 1
	tempva = (vaddr_t)km_alloc(2*PAGE_SIZE, &kv_any, &kp_none, &kd_nowait);
d1688 1
a1688 1
	for(i=0; i<24 ; i++) {
d1691 1
a1691 2
		pmap_update(pmap_kernel());
	}
d1807 2
a1808 7
			pmap_kenter_pa(tempva, img_cur, VM_PROT_ALL);
			pmap_kenter_pa(tempva + PAGE_SIZE, img_cur+PAGE_SIZE,
			    VM_PROT_ALL);
			pmap_update(pmap_kernel());

			if (compressed_size - processed >= PAGE_SIZE)
				read_size = PAGE_SIZE;
d1812 19
d1836 3
a1838 2
			pmap_kremove(tempva, PAGE_SIZE);
			pmap_kremove(tempva + PAGE_SIZE, PAGE_SIZE);
@


1.89
log
@

Perform MAXPHYS-sized reads for the chunktable instead of PAGE_SIZE-sized
reads.

Also fix a VA leak in the chunktable read error path.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.88 2014/04/20 14:02:57 mlarkin Exp $	*/
d780 1
a780 1
	i = inflate(&hibernate_state->hib_stream, Z_PARTIAL_FLUSH);
d871 1
a871 1
	if (deflate(&hibernate_state->hib_stream, Z_PARTIAL_FLUSH) != Z_OK)
@


1.88
log
@

More dead stores removal in subr_hibernate.c
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.87 2014/04/19 16:19:07 mlarkin Exp $	*/
d1569 1
a1569 1
	int i;
d1571 1
d1585 6
d1593 2
a1594 5
	    i += PAGE_SIZE, blkctr += PAGE_SIZE/DEV_BSIZE) {
		pmap_kenter_pa(chunktable + i, piglet_chunktable + i,
		    VM_PROT_ALL);
		pmap_update(pmap_kernel());
		hibernate_block_io(hib, blkctr, PAGE_SIZE,
a1595 1
	}
d1613 4
a1616 2
	if (uvm_pmr_alloc_pig(&pig_start, pig_sz) == ENOMEM)
		return (1);
a1626 3
	pmap_kremove(chunktable, PAGE_SIZE);
	pmap_update(pmap_kernel());

d1630 6
a1635 1
	return (0);
@


1.87
log
@

Dead stores in subr_hibernate.c
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.86 2014/03/21 21:39:36 miod Exp $	*/
a1750 1
	img_index = pig_start;
a1766 1
				img_index += chunks[ochunks[j]].compressed_size;
a1787 1
			img_index += chunks[ochunks[i]].compressed_size;
a1788 5
	}

	img_index = pig_start;
	for (i = 0; i < nfchunks; i++) {
		img_index += chunks[fchunks[i]].compressed_size;
@


1.86
log
@Allow for two more pmap-specific bits in vm_page pg_flags. Define
PG_PMAPMASK as all the possible pmap-specific bits (similar to the other
PG_fooMASK) to make sure MI code does not need to be updated, the next time
more bits are allocated to greedy pmaps.

No functional change, soon to be used by the (greedy) mips64 pmap.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.85 2014/03/13 03:52:56 dlg Exp $	*/
d1643 1
a1643 1
	paddr_t copy_start, copy_end, piglet_cur;
a1759 1
			piglet_cur = piglet_base;
a1763 2
				piglet_cur +=
				    chunks[ochunks[j]].compressed_size;
a1772 1
			piglet_cur = piglet_base;
a1773 2
				piglet_cur +=
				    chunks[pchunks[j]].compressed_size;
a1795 1
		piglet_cur = piglet_base;
@


1.85
log
@get rid of the assumption that the head of the alldevs list is the
"mainbus" device. this breaks when mpath is enabled because it
attaches before mainbus and therefore takes the head position.

have autoconf provide device_mainbus() which looks up mainbus_cd,
and use that instead.

discussed with deraadt who just wants mpath stuff to move forward
despite there being many ways to shine this particular turd.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.84 2014/02/01 07:10:33 mlarkin Exp $	*/
d410 1
a410 2
		atomic_clearbits_int(&pg->pg_flags,
		    PG_PMAP0|PG_PMAP1|PG_PMAP2|PG_PMAP3);
d526 1
a526 2
		atomic_clearbits_int(&pg->pg_flags,
		    PG_PMAP0|PG_PMAP1|PG_PMAP2|PG_PMAP3);
@


1.84
log
@

Remove some of the excessive cache and TLB flushing going on during
hibernate unpack - these were added a while ago when we were fighting
different issues that have now been solved.

Tested by myself and dcoppa on a variety of machines
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.83 2014/01/21 01:48:44 tedu Exp $	*/
d1155 1
a1155 1
	if (config_suspend(TAILQ_FIRST(&alldevs), DVACT_QUIESCE) != 0)
d1162 1
a1162 1
	if (config_suspend(TAILQ_FIRST(&alldevs), DVACT_SUSPEND) != 0) {
@


1.83
log
@bzero -> memset
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.82 2014/01/14 09:57:51 mlarkin Exp $	*/
a827 3
		/* Flush cache and TLB */
		hibernate_flush();

a1268 1
	wbinvd();
a1279 3

	hibernate_flush();
	wbinvd();
@


1.82
log
@

Typo in a printf, should be 'bytes' not 'blocks'
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.81 2013/11/21 00:13:33 dlg Exp $	*/
d683 1
a683 1
	bzero(&hib->kernel_version, 128);
d945 1
a945 1
	bzero(&blank_hiber_info, sizeof(union hibernate_info));
d1102 1
a1102 1
	bzero(&hib, sizeof(hib));
d1539 2
a1540 2
	bzero((caddr_t)hibernate_zlib_start, hibernate_zlib_size);
	bzero((caddr_t)hibernate_state, PAGE_SIZE);
@


1.81
log
@remove the #define b_cylinder b_resid from bufs. i hated the
overloading of that thing.

the only hardware that seems to care about cylinders in our tree
are floppy drives, and the drivers for those calculate their own
cylinders from logical block addresses and ignore whatever the rest
of the kernel thought b_cylinders should be.

most of this diff is moving the floppy drivers to using b_resid as
a resid and using that as part of the calculation for real cylinder
values.

the rest of the diff is getting rid of the useless assignments to
b_cylinder that dont get used by anything (now that disksort is
gone).

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.80 2013/11/09 06:54:00 mlarkin Exp $	*/
d1613 1
a1613 1
	printf("unhibernating @@ block %lld length %lu blocks\n",
@


1.80
log
@

Remove hibernate_get_next_rle function (unused, and we need to redo it
anyway as we move toward a streamed implmentation)
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.79 2013/11/09 04:38:42 deraadt Exp $	*/
a1061 1
	bp->b_cylinder = 0;
@


1.79
log
@unbias the chunks and chunktable writing and reading.  as a result, it
is now possible to move the chunktable right after the chunks, not at
the end of the swap.
ok mlarkin
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.78 2013/11/06 19:53:08 deraadt Exp $	*/
a762 36
}

/*
 * Gets the next RLE value from the image stream
 */
int
hibernate_get_next_rle(void)
{
	int rle, i;
	struct hibernate_zlib_state *hibernate_state;

	hibernate_state =
	    (struct hibernate_zlib_state *)HIBERNATE_HIBALLOC_PAGE;

	/* Read RLE code */
	hibernate_state->hib_stream.next_out = (char *)&rle;
	hibernate_state->hib_stream.avail_out = sizeof(rle);

	i = inflate(&hibernate_state->hib_stream, Z_FULL_FLUSH);
	if (i != Z_OK && i != Z_STREAM_END) {
		/*
		 * XXX - this will likely reboot/hang most machines
		 *       since the console output buffer will be unmapped,
		 *       but there's not much else we can do here.
		 */
		panic("inflate rle error");
	}

	/* Sanity check what RLE value we got */
	if (rle > HIBERNATE_CHUNK_SIZE/PAGE_SIZE || rle < 0)
		panic("invalid RLE code");

	if (i == Z_STREAM_END)
		rle = -1;

	return rle;
@


1.78
log
@simplify a crazy expression
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.77 2013/11/06 19:50:56 deraadt Exp $	*/
a945 1
	daddr_t chunkbase;
a949 3
	chunkbase = hib->sig_offset -
	    (hibernate_chunk_table_size / DEV_BSIZE);

d959 1
a959 1
		    chunkbase + (i/DEV_BSIZE),
d1548 1
d1622 1
a1622 1
	blkctr = hib->sig_offset - chunktable_size;
@


1.77
log
@In hibernate_write_chunks(), keep track of relative block numbers instead
of absolute, so that we can add range checking.
ok mlarkin
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.76 2013/11/06 19:48:37 deraadt Exp $	*/
d1381 1
a1381 1
	daddr_t blkctr = 0, offset = 0;
d1548 2
a1549 3
		offset = blkctr + hib->image_offset;
		chunks[i].compressed_size = (offset - chunks[i].offset) *
		    DEV_BSIZE;
@


1.76
log
@spacing
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.75 2013/11/06 19:47:30 deraadt Exp $	*/
d1381 1
a1381 1
	daddr_t blkctr = hib->image_offset, offset = 0;
d1441 1
a1441 1
		chunks[i].offset = blkctr;
d1487 4
a1490 5
					if ((err = hib->io_func(
					    hib->dev,
					    blkctr, (vaddr_t)hibernate_io_page,
					    PAGE_SIZE, HIB_W,
					    hib->io_page))) {
d1492 1
a1492 1
							err);
d1539 1
a1539 1
		if ((err = hib->io_func(hib->dev, blkctr,
d1548 1
a1548 1
		offset = blkctr;
@


1.75
log
@return the known error, instead of EIO
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.74 2013/11/06 19:45:47 deraadt Exp $	*/
d670 1
a670 1
	if(sizeof(union hibernate_info) > DEV_BSIZE)
d705 1
a705 2
		if (hib->io_func(hib->dev,
		    DL_GETPOFFSET(&dl.d_partitions[1]),
d1575 1
a1575 1
	if(!deflate)
d1973 1
a1973 2
	hib.io_func(hib.dev, 0, (vaddr_t)NULL, 0,
	    HIB_DONE, hib.io_page);
@


1.74
log
@shorten dev_t in hib_info
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.73 2013/11/06 18:41:00 deraadt Exp $	*/
d939 1
a939 6
 * in the piglet when this function is called.
 *
 * Return values:
 *
 * 0   -  success
 * EIO -  I/O error writing the chunktable
d968 1
a968 1
			return (EIO);
d1495 1
a1495 1
						return (EIO);
d1528 1
a1528 1
			return (EIO);
d1545 1
a1545 1
			return (EIO);
@


1.73
log
@shorten variable name for the hibernate info struct, throughout.  Much
easier to read now.
ok mlarkin
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.71 2013/11/06 12:06:58 deraadt Exp $	*/
d654 1
a654 1
	hib->device = swdevt[0].sw_dev;
d657 1
a657 1
	dl_ret = disk_readlabel(&dl, hib->device, err_string, 128);
d705 1
a705 1
		if (hib->io_func(hib->device,
d931 1
a931 1
	return (hib->io_func(hib->device, hib->sig_offset,
d968 1
a968 1
		if ((err = hib->io_func(hib->device,
d1092 1
a1092 1
	bdsw = &bdevsw[major(hib->device)];
d1094 1
a1094 1
	error = (*bdsw->d_open)(hib->device, FREAD, S_IFCHR, curproc);
d1107 1
a1107 1
	bp->b_dev = hib->device;
d1115 1
a1115 1
		error = (*bdsw->d_close)(hib->device, 0, S_IFCHR,
d1122 1
a1122 1
	error = (*bdsw->d_close)(hib->device, FREAD, S_IFCHR, curproc);
d1494 1
a1494 1
					    hib->device,
d1546 1
a1546 1
		if ((err = hib->io_func(hib->device, blkctr,
d1930 1
a1930 1
	if (uvm_hibswap(hib.device, &start, &end)) {
d1979 1
a1979 1
	hib.io_func(hib.device, 0, (vaddr_t)NULL, 0,
@


1.72
log
@use DEV_BSIZE instead of "secsize"
ok mlarkin
@
text
@d66 1
a66 1
union hibernate_info disk_hiber_info;
d642 1
a642 1
get_hibernate_info(union hibernate_info *hiber_info, int suspend)
d649 2
a650 2
	hiber_info->io_func = get_hibernate_io_function();
	if (hiber_info->io_func == NULL)
d654 1
a654 1
	hiber_info->device = swdevt[0].sw_dev;
d657 1
a657 1
	dl_ret = disk_readlabel(&dl, hiber_info->device, err_string, 128);
d674 1
a674 1
	hiber_info->magic = HIBERNATE_MAGIC;
d677 1
a677 1
	hiber_info->sig_offset = DL_GETPSIZE(&dl.d_partitions[1]) -
d683 3
a685 3
	bzero(&hiber_info->kernel_version, 128);
	bcopy(version, &hiber_info->kernel_version,
	    min(strlen(version), sizeof(hiber_info->kernel_version)-1));
d689 2
a690 2
		if (uvm_pmr_alloc_piglet(&hiber_info->piglet_va,
		    &hiber_info->piglet_pa, HIBERNATE_CHUNK_SIZE*3,
d695 1
a695 1
		hiber_info->io_page = (void *)hiber_info->piglet_va;
d705 1
a705 1
		if (hiber_info->io_func(hiber_info->device,
d708 1
a708 1
		    HIB_INIT, hiber_info->io_page))
d718 2
a719 2
		hiber_info->io_page = malloc(PAGE_SIZE, M_DEVBUF, M_NOWAIT);
		if (!hiber_info->io_page)
d724 1
a724 1
	if (get_hibernate_info_md(hiber_info))
d731 1
a731 1
		uvm_pmr_free_piglet(hiber_info->piglet_va,
d852 1
a852 1
hibernate_inflate_region(union hibernate_info *hiber_info, paddr_t dest,
d872 1
a872 1
		if (hibernate_inflate_skip(hiber_info, dest)) {
d896 1
a896 1
hibernate_deflate(union hibernate_info *hiber_info, paddr_t src,
d899 1
a899 1
	vaddr_t hibernate_io_page = hiber_info->piglet_va + PAGE_SIZE;
d928 1
a928 1
hibernate_write_signature(union hibernate_info *hiber_info)
d931 3
a933 3
	return (hiber_info->io_func(hiber_info->device, hiber_info->sig_offset,
	    (vaddr_t)hiber_info, DEV_BSIZE, HIB_W,
	    hiber_info->io_page));
d947 1
a947 1
hibernate_write_chunktable(union hibernate_info *hiber_info)
d957 1
a957 1
	chunkbase = hiber_info->sig_offset -
d960 1
a960 1
	hibernate_chunk_table_start = hiber_info->piglet_va +
d963 1
a963 1
	chunks = (struct hibernate_disk_chunk *)(hiber_info->piglet_va +
d968 1
a968 1
		if ((err = hiber_info->io_func(hiber_info->device,
d971 1
a971 1
		    MAXPHYS, HIB_W, hiber_info->io_page))) {
d982 1
a982 1
 * guaranteed to not match any valid hiber_info.
d988 1
a988 1
	union hibernate_info hiber_info;
d994 1
a994 1
	if (get_hibernate_info(&hiber_info, 0))
d999 3
a1001 3
		hiber_info.sig_offset);
	if (hibernate_block_io(&hiber_info,
	    hiber_info.sig_offset,
d1084 1
a1084 1
hibernate_block_io(union hibernate_info *hib_info, daddr_t blkctr,
d1092 1
a1092 1
	bdsw = &bdevsw[major(hib_info->device)];
d1094 1
a1094 1
	error = (*bdsw->d_open)(hib_info->device, FREAD, S_IFCHR, curproc);
d1107 1
a1107 1
	bp->b_dev = hib_info->device;
d1115 1
a1115 1
		error = (*bdsw->d_close)(hib_info->device, 0, S_IFCHR,
d1122 1
a1122 1
	error = (*bdsw->d_close)(hib_info->device, FREAD, S_IFCHR, curproc);
d1145 1
a1145 1
	union hibernate_info hiber_info;
d1149 2
a1150 2
	bzero(&hiber_info, sizeof(hiber_info));
	if (get_hibernate_info(&hiber_info, 0)) {
d1159 1
a1159 1
		hiber_info.sig_offset);
d1161 3
a1163 3
	if (hibernate_block_io(&hiber_info,
	    hiber_info.sig_offset,
	    DEV_BSIZE, (vaddr_t)&disk_hiber_info, 0)) {
d1170 1
a1170 1
	if (disk_hiber_info.magic != HIBERNATE_MAGIC) {
d1172 1
a1172 1
			disk_hiber_info.magic);
d1191 1
a1191 1
	if (hibernate_compare_signature(&hiber_info, &disk_hiber_info)) {
d1202 1
a1202 1
	if (hibernate_read_image(&disk_hiber_info))
d1228 1
a1228 1
	hibernate_unpack_image(&disk_hiber_info);
d1243 1
a1243 1
hibernate_unpack_image(union hibernate_info *hiber_info)
d1246 1
a1246 1
	union hibernate_info local_hiber_info;
d1249 1
a1249 1
	char *pva = (char *)hiber_info->piglet_va;
d1262 1
a1262 1
	bcopy(hiber_info, &local_hiber_info, sizeof(union hibernate_info));
d1277 1
a1277 1
	for (i = 0; i < local_hiber_info.chunk_ctr; i++) {
d1279 1
a1279 1
		if (hibernate_zlib_reset(&local_hiber_info, 0) != Z_OK)
d1282 1
a1282 1
		hibernate_process_chunk(&local_hiber_info, &chunks[fchunks[i]],
d1340 1
a1340 1
hibernate_process_chunk(union hibernate_info *hiber_info,
d1343 1
a1343 1
	char *pva = (char *)hiber_info->piglet_va;
d1348 1
a1348 1
	hibernate_inflate_region(hiber_info, chunk->base,
d1381 1
a1381 1
hibernate_write_chunks(union hibernate_info *hiber_info)
d1386 2
a1387 2
	vaddr_t hibernate_io_page = hiber_info->piglet_va + PAGE_SIZE;
	daddr_t blkctr = hiber_info->image_offset, offset = 0;
d1394 1
a1394 1
	hiber_info->chunk_ctr = 0;
d1415 1
a1415 1
	    (hiber_info->piglet_pa + 3*PAGE_SIZE), VM_PROT_ALL);
d1419 1
a1419 1
	chunks = (struct hibernate_disk_chunk *)(hiber_info->piglet_va +
d1423 3
a1425 3
	for (i = 0; i < hiber_info->nranges; i++) {
		range_base = hiber_info->ranges[i].base;
		range_end = hiber_info->ranges[i].end;
d1430 1
a1430 1
			chunks[hiber_info->chunk_ctr].base = inaddr;
d1432 1
a1432 1
				chunks[hiber_info->chunk_ctr].end = inaddr +
d1435 1
a1435 1
				chunks[hiber_info->chunk_ctr].end = range_end;
d1438 1
a1438 1
			hiber_info->chunk_ctr ++;
d1443 1
a1443 1
	for (i = 0; i < hiber_info->chunk_ctr; i++) {
d1450 1
a1450 1
		if (hibernate_zlib_reset(hiber_info, 1) != Z_OK) {
d1484 1
a1484 1
					inaddr += hibernate_deflate(hiber_info,
d1493 2
a1494 2
					if ((err = hiber_info->io_func(
					    hiber_info->device,
d1497 1
a1497 1
					    hiber_info->io_page))) {
d1546 1
a1546 1
		if ((err = hiber_info->io_func(hiber_info->device, blkctr,
d1548 1
a1548 1
		    HIB_W, hiber_info->io_page))) {
d1568 1
a1568 1
 * by the piglet allocated via the supplied hiber_info.
d1571 1
a1571 1
hibernate_zlib_reset(union hibernate_info *hiber_info, int deflate)
d1575 1
a1575 1
	char *pva = (char *)hiber_info->piglet_va;
d1607 1
a1607 1
 * size are recorded in hiber_info. Begin by reading the persisted
d1617 1
a1617 1
hibernate_read_image(union hibernate_info *hiber_info)
d1624 1
a1624 1
	paddr_t piglet_chunktable = hiber_info->piglet_pa +
d1633 1
a1633 1
	blkctr = hiber_info->sig_offset - chunktable_size;
d1647 1
a1647 1
		hibernate_block_io(hiber_info, blkctr, PAGE_SIZE,
d1651 1
a1651 1
	blkctr = hiber_info->image_offset;
d1656 1
a1656 1
	for (i = 0; i < hiber_info->chunk_ctr; i++)
d1662 1
a1662 1
	    hiber_info->sig_offset - chunktable_size,
d1676 1
a1676 1
	hibernate_read_chunks(hiber_info, image_start, image_end, disk_size,
d1683 1
a1683 1
	hibernate_populate_resume_pt(hiber_info, image_start, image_end);
d1695 1
a1695 1
hibernate_read_chunks(union hibernate_info *hib_info, paddr_t pig_start,
d1701 1
a1701 1
	paddr_t piglet_base = hib_info->piglet_pa;
d1742 1
a1742 1
	nchunks = hib_info->chunk_ctr;
d1881 1
a1881 1
			hibernate_block_io(hib_info, blkctr, read_size,
d1916 1
a1916 1
	union hibernate_info hib_info;
d1922 1
a1922 1
	 * hib_info->piglet_pa and vaddr stored in hib_info->piglet_va
d1924 1
a1924 1
	if (get_hibernate_info(&hib_info, 1)) {
d1930 1
a1930 1
	if (uvm_hibswap(hib_info.device, &start, &end)) {
d1941 1
a1941 1
	hib_info.image_offset = ctod(start);
d1945 1
a1945 1
	    hib_info.image_offset, ctod(end) - ctod(start));
d1952 1
a1952 1
	global_piglet_va = hib_info.piglet_va;
d1955 1
a1955 1
	if (hibernate_write_chunks(&hib_info)) {
d1961 1
a1961 1
	if (hibernate_write_chunktable(&hib_info)) {
d1967 1
a1967 1
	if (hibernate_write_signature(&hib_info)) {
d1979 2
a1980 2
	hib_info.io_func(hib_info.device, 0, (vaddr_t)NULL, 0,
	    HIB_DONE, hib_info.io_page);
@


1.71
log
@teach the side-effect free drivers about the partition they are dealing
by passing a start/length in the HIB_INIT op.  Then rebase all
hibernate-time block offsets to be relative to the start of that partition.
This simplifies things a lot.
ok mlarkin
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.70 2013/11/06 00:52:46 mlarkin Exp $	*/
a668 2
	hiber_info->secsize = dl.d_secsize;

d670 1
a670 1
	if(sizeof(union hibernate_info) > hiber_info->secsize)
d678 1
a678 1
	    sizeof(union hibernate_info)/hiber_info->secsize;
d680 1
a680 1
	chunktable_size = HIBERNATE_CHUNK_TABLE_SIZE / hiber_info->secsize;
d932 1
a932 1
	    (vaddr_t)hiber_info, hiber_info->secsize, HIB_W,
d958 1
a958 1
	    (hibernate_chunk_table_size / hiber_info->secsize);
d969 1
a969 1
		    chunkbase + (i/hiber_info->secsize),
d1002 1
a1002 1
	    hiber_info.secsize, (vaddr_t)&blank_hiber_info, 1))
d1163 1
a1163 1
	    hiber_info.secsize, (vaddr_t)&disk_hiber_info, 0)) {
d1491 1
a1491 1
					    PAGE_SIZE / hiber_info->secsize;
d1539 1
a1539 1
		nblocks = used / hiber_info->secsize;
d1542 1
a1542 1
		if (used % hiber_info->secsize != 0)
d1547 1
a1547 1
		    (vaddr_t)hibernate_io_page, nblocks*hiber_info->secsize,
d1557 1
a1557 1
		    hiber_info->secsize;
d1631 1
a1631 1
	chunktable_size = HIBERNATE_CHUNK_TABLE_SIZE / hiber_info->secsize;
d1643 1
a1643 1
	    i += PAGE_SIZE, blkctr += PAGE_SIZE/hiber_info->secsize) {
d1884 1
a1884 1
			blkctr += (read_size / hib_info->secsize);
@


1.70
log
@

Errant assignment that snuck in long ago. Pointed out by deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.69 2013/11/05 07:38:26 mlarkin Exp $	*/
a677 3
	/* Calculate swap offset from start of disk */
	hiber_info->swap_offset = dl.d_partitions[1].p_offset;

d679 1
a679 2
	hiber_info->sig_offset = DL_GETPOFFSET(&dl.d_partitions[1]) +
	    DL_GETPSIZE(&dl.d_partitions[1]) -
d707 4
a710 2
		if (hiber_info->io_func(hiber_info->device, 0,
		    (vaddr_t)NULL, 0, HIB_INIT, hiber_info->io_page))
d1001 1
a1001 1
		hiber_info.sig_offset - hiber_info.swap_offset);
d1003 1
a1003 1
	    hiber_info.sig_offset - hiber_info.swap_offset,
d1161 1
a1161 1
		hiber_info.sig_offset - hiber_info.swap_offset);
d1164 1
a1164 1
	    hiber_info.sig_offset - hiber_info.swap_offset,
d1635 1
a1635 2
	blkctr = hiber_info->sig_offset - chunktable_size -
			hiber_info->swap_offset;
d1868 1
a1868 1
		blkctr = chunks[fchunks[i]].offset - hib_info->swap_offset;
d1943 1
a1943 1
	hib_info.image_offset = ctod(start) + hib_info.swap_offset;
@


1.69
log
@

Change an #if 0 surrounding a debug printf into a DPRINTF instead.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.68 2013/11/05 06:02:44 deraadt Exp $	*/
a1678 2
	image_start = pig_start;

@


1.68
log
@new function uvm_hibswap() finds a the largest free zone in swap, which
hibernate can use place the data.
ok mlarkin
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.67 2013/11/05 00:51:58 krw Exp $	*/
a1949 1
#if 0
d1951 1
a1951 1
	printf("hibernate @@ block %lld max-length %lu blocks\n",
a1952 1
#endif
@


1.67
log
@Replace direct references to p_size, p_offset and d_secperunit with
DL_[GET|SET]PSIZE(), DL_[GET|SET]POFFSET(), DL_[GET|SET]DSIZE() in
order to get|set correct value that includes the high bits of the
value.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.66 2013/10/20 17:16:47 mlarkin Exp $	*/
a730 6
	/* Calculate memory image location in swap */
	hiber_info->image_offset = DL_GETPOFFSET(&dl.d_partitions[1]) +
	    DL_GETPSIZE(&dl.d_partitions[1]) -
	    (hiber_info->image_size / hiber_info->secsize) -
	    sizeof(union hibernate_info)/hiber_info->secsize -
	    chunktable_size;
a1204 2
	printf("Unhibernating...");

d1666 3
a1668 1
	printf(" (image size: %zu)\n", compressed_size);
d1924 1
a1924 1
	size_t swap_size;
d1936 5
a1940 2
	swap_size = hib_info.image_size + hib_info.secsize +
		HIBERNATE_CHUNK_TABLE_SIZE;
d1942 2
a1943 2
	if (uvm_swap_check_range(hib_info.device, swap_size)) {
		printf("insufficient swap space for hibernate\n");
d1946 9
@


1.66
log
@

Informational message on unpack start
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.65 2013/10/20 10:08:05 mlarkin Exp $	*/
d666 1
a666 1
	    dl.d_partitions[1].p_size == 0)
d682 2
a683 2
	hiber_info->sig_offset = dl.d_partitions[1].p_offset +
	    dl.d_partitions[1].p_size -
d732 2
a733 2
	hiber_info->image_offset = dl.d_partitions[1].p_offset +
	    dl.d_partitions[1].p_size -
@


1.65
log
@

Provide more debugging aids when failing to read the packed image from disk
during hibernate resume.

requested by deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.64 2013/10/20 10:01:29 mlarkin Exp $	*/
d1233 2
@


1.64
log
@

DPRINTFs for various failure cases, disabled by default (uncomment
HIB_DEBUG to see these)
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.63 2013/10/20 09:44:17 mlarkin Exp $	*/
d1123 2
a1124 1
		printf("hibernate_block_io biowait failed %d\n", error);
@


1.63
log
@

Flush the cache after bouncing the inflate page to the piglet.

Fixes resume time stream corruption seen on x230 with large (16GB)
unhibernation
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.62 2013/10/20 09:27:39 mlarkin Exp $	*/
d1159 2
a1160 1
	if (get_hibernate_info(&hiber_info, 0))
d1162 1
d1172 5
a1176 2
	    hiber_info.secsize, (vaddr_t)&disk_hiber_info, 0))
		panic("error in hibernate read");
d1180 2
d1191 1
d1201 1
d1954 1
d1960 1
d1966 1
@


1.62
log
@

Use a second inflate page for stream finalize, since it is possible
(though unlikely) for the end of stream marker to overflow from one page.
This should fix a few (but not all) spurious failure-to-hibernate errors.

Also remove an unnecessary cache flush during deflate.

ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.61 2013/10/03 03:51:16 mlarkin Exp $	*/
d1319 1
d1331 3
@


1.61
log
@

Adds DPRINTF/DNPRINTF macros to hibernate, and adds invocation of said
macros in various places to assist in debugging "failure to hibernate"
errors.

Macros are disabled by default - Uncomment #define HIB_DEBUG if you want
more verbose messages during hibernate.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.60 2013/09/29 15:47:35 mlarkin Exp $	*/
d48 1
a48 1
 * 2*PAGE_SIZE			unused
d1521 4
a1524 1
		hibernate_state->hib_stream.avail_out = out_remaining;
d1534 1
a1534 1
		used = PAGE_SIZE - out_remaining;
a1882 1
			hibernate_flush();
@


1.60
log
@

Print out the compressed image size during unhibernate, for diagnostic
purposes and to give the user an idea as to how much is going to be read
in.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.59 2013/06/01 19:06:34 mlarkin Exp $	*/
d70 10
d963 1
a963 1
	int i;
d978 1
a978 1
		if (hiber_info->io_func(hiber_info->device,
d981 2
a982 1
		    MAXPHYS, HIB_W, hiber_info->io_page))
d984 1
d1008 1
a1008 2
#ifdef HIBERNATE_DEBUG
	printf("clearing hibernate signature block location: %lld\n",
a1009 1
#endif /* HIBERNATE_DEBUG */
d1060 1
a1060 3
#ifdef HIBERNATE_DEBUG
		printf("hibernate memory range count mismatch\n");
#endif
d1065 1
a1065 3
#ifdef HIBERNATE_DEBUG
		printf("hibernate kernel version mismatch\n");
#endif
d1072 1
a1072 2
#ifdef HIBERNATE_DEBUG
			printf("hib range %d mismatch [%p-%p != %p-%p]\n",
a1074 1
#endif
d1165 1
a1165 2
#ifdef HIBERNATE_DEBUG
	printf("reading hibernate signature block location: %lld\n",
a1166 1
#endif /* HIBERNATE_DEBUG */
d1384 1
a1384 1
	int i;
d1405 2
a1406 1
	if (!hibernate_copy_page)
d1408 1
d1446 2
a1447 1
		if (hibernate_zlib_reset(hiber_info, 1) != Z_OK)
d1449 1
d1489 1
a1489 1
					if (hiber_info->io_func(
d1493 3
a1495 1
					    hiber_info->io_page))
d1497 1
d1504 2
a1505 1
		if (inaddr != range_end)
d1507 1
d1523 3
a1525 2
		if (deflate(&hibernate_state->hib_stream, Z_FINISH) !=
		    Z_STREAM_END)
d1527 1
d1539 1
a1539 1
		if (hiber_info->io_func(hiber_info->device, blkctr,
d1541 2
a1542 1
		    HIB_W, hiber_info->io_page))
d1544 1
d1919 2
a1920 1
	if (get_hibernate_info(&hib_info, 1))
d1922 1
d1939 2
a1940 1
	if (hibernate_write_chunks(&hib_info))
d1942 1
d1944 2
a1945 1
	if (hibernate_write_chunktable(&hib_info))
d1947 1
d1949 2
a1950 1
	if (hibernate_write_signature(&hib_info))
d1952 1
@


1.59
log
@

Work around stack smash protector getting confused because we switched
stacks by not returning (ever) from hibernate_unpack_image.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.58 2013/06/01 17:39:20 mlarkin Exp $	*/
d1199 1
a1199 1
	printf("Unhibernating...\n");
d1230 1
a1230 1
	printf("Unable to resume hibernated image\n");
d1639 2
@


1.58
log
@

Change return codes in various hibernate functions to be able to later
distinguish one failure path from another. Comment the same.

Also removed some extraneous comments regarding pmap_activate.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.57 2013/05/31 20:00:00 mlarkin Exp $	*/
d1225 1
a1225 11
	/*
	 * Point of no return. Once we pass this point, only kernel code can
	 * be accessed. No global variables or other kernel data structures
	 * are guaranteed to be coherent after unpack starts.
	 *
	 * The image is now in high memory (pig area), we unpack from the pig
	 * to the correct location in memory. We'll eventually end up copying
	 * on top of ourself, but we are assured the kernel code here is the
	 * same between the hibernated and resuming kernel, and we are running
	 * on our own stack, so the overwrite is ok.
	 */
a1227 6
	/*
	 * Resume the loaded kernel by jumping to the MD resume vector.
	 * We won't be returning from this call.
	 */
	hibernate_resume_machdep();

d1236 3
d1262 11
d1286 6
@


1.57
log
@

We need to halt the APs on MP hibernate resume or else they will be
executing code possibly causing side effects during the image unpack
operation. But before we can halt the APs, we need to complete their init
(as they will be hatched but idling, possibly with interrupts off).

Introduces MD function hibernate_quiesce_cpus to do this, called from the
MI hibernate resume code.

ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.56 2013/05/30 19:00:59 mlarkin Exp $	*/
d940 5
d972 1
a972 1
			return (1);
d1362 7
d1395 1
a1395 1
		return (1);
d1400 1
a1400 1
		return (1);
a1404 1
	/* XXX - not needed on all archs */
d1439 1
a1439 1
			return (1);
a1464 1
					/* XXX - not needed on all archs */
d1484 1
a1484 1
						return (1);
d1492 1
a1492 1
			return (1);
d1510 1
a1510 1
			return (1);
d1525 1
a1525 1
			return (1);
a1683 1
	/* XXX - dont need this on all archs */
@


1.56
log
@

Make interrupt handling in hibernate resume MI by providing MD-specific
functions to enable and disable interrupts, if needed. If a platform doesnt
need interrupt handling in this way, the MD function can be a no-op.

discussed with pirofti and deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.55 2013/05/30 16:00:54 mlarkin Exp $	*/
d1189 4
@


1.55
log
@

Comment a KASSERT whose purpose wasn't immediately apparent
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.54 2013/04/09 18:58:03 mlarkin Exp $	*/
d1200 1
a1200 1
	disable_intr();
d1205 1
a1205 1
		enable_intr();
@


1.54
log
@

Add a magic number to the head of the signature block. Check for magic
number match during signature block read during speculative unhibernate on
boot. If the magic number matches but we have otherwise chosen to not
unhibernate (due to kernel/memory mismatch), clear the signature block
early to avoid accidentally trying to unhibernate on subsequent boots. This
prevents accidental unhibernates and endless unhibernate/reboot cycles.

Add a define for HIBERNATE_DEBUG for various debugging printfs (disabled by
default).

Finally, change some KASSERTs to warning printfs (they probably shouldn't
have been KASSERTs in the first place).

"looks good" deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.53 2013/03/28 16:58:45 deraadt Exp $	*/
d446 1
d448 1
@


1.53
log
@sys/param.h gets you sys/types.h automatically
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.52 2013/03/07 01:26:54 mlarkin Exp $	*/
d660 2
a661 1
	KASSERT(sizeof(union hibernate_info) <= hiber_info->secsize);
d663 3
d982 1
a982 1
	bzero(&blank_hiber_info, sizeof(hiber_info));
d984 1
d989 4
d996 1
a996 1
		panic("error hibernate write 6");
d1042 4
a1045 1
	if (mine->nranges != disk->nranges)
d1047 1
d1049 4
a1052 1
	if (strcmp(mine->kernel_version, disk->kernel_version) != 0)
d1054 1
d1058 6
a1063 1
		    (mine->ranges[i].end != disk->ranges[i].end) )
d1065 1
d1154 5
d1164 15
d1642 1
a1642 2
	/* Read complete, clear the signature and return */
	return hibernate_clear_signature();
@


1.52
log
@

Reword some wrong comments and some improperly formatted comments and add
information about piglet memory layout. No functional changes.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.51 2013/03/06 08:34:05 mlarkin Exp $	*/
a23 1
#include <sys/types.h>
@


1.51
log
@

Fix a bad comparison when calculating the size of the hibernate signature
block

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.50 2013/01/17 02:51:08 pirofti Exp $	*/
d36 26
d480 1
a480 1
	 * Try to coerse the pagedaemon into freeing memory
d594 2
a595 2
 * Given a physical page address, it will return the number of pages
 * starting at the address, that are free.  Clamps to the number of pages in
d689 6
a694 2
		 * Initialize of the hibernate IO function (for drivers which
		 * need that)
d716 1
a716 1
	/* Calculate memory image location */
d780 2
a781 1
		 * XXX - this will likely reboot/hang most machines,
d817 2
a818 1
		 * XXX - this will likely reboot/hang most machines,
a820 1

d825 6
a830 1
	if (hibernate_state->hib_stream.avail_out != 0)
d832 1
d843 2
a844 1
 * will likely hang or reset the machine.
a920 3
 *
 * Write the memory chunk table to the area in swap immediately
 * preceding the signature block.
a1164 2
	/* Point of no return ... */

d1173 9
a1181 5
	 * Image is now in high memory (pig area), copy to correct location
	 * in memory. We'll eventually end up copying on top of ourself, but
	 * we are assured the kernel code here is the same between the
	 * hibernated and resuming kernel, and we are running on our own
	 * stack, so the overwrite is ok.
a1198 4
 * This ordering is used to avoid having inflate overwrite a chunk in the
 * middle of processing that chunk. This will, of course, happen during the
 * final output chunk, where we copy the chunk to the piglet area first,
 * before inflating.
d1307 1
a1307 1
 * side effects. This also means no printfs are permitted (since it
d1328 1
d1872 4
@


1.50
log
@Style, no functional changes.

Discussed with mlarkin@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.49 2013/01/17 02:36:45 deraadt Exp $	*/
d635 1
a635 1
	KASSERT(sizeof(union hibernate_info)/hiber_info->secsize == 1);
@


1.49
log
@Add a hibernate HIB_DONE op.  After the memory-side-effect driver finishes
IO to the disk, DVACT_RESUME the controller back to normal operation.  That
allows us to do the full DVACT_POWERDOWN sequence afterwards.
ok mlarkin
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.48 2013/01/17 01:28:01 mlarkin Exp $	*/
d696 2
a697 1
		uvm_pmr_free_piglet(hiber_info->piglet_va, HIBERNATE_CHUNK_SIZE*3);
d705 2
a706 2
void
*hibernate_zlib_alloc(void *unused, int nitems, int size)
d710 2
a711 1
	hibernate_state = (struct hibernate_zlib_state *)HIBERNATE_HIBALLOC_PAGE;
d725 2
a726 1
	hibernate_state = (struct hibernate_zlib_state *)HIBERNATE_HIBALLOC_PAGE;
d740 2
a741 1
	hibernate_state = (struct hibernate_zlib_state *)HIBERNATE_HIBALLOC_PAGE;
d775 2
a776 1
	hibernate_state = (struct hibernate_zlib_state *)HIBERNATE_HIBALLOC_PAGE;
d815 2
a816 1
	hibernate_state = (struct hibernate_zlib_state *)HIBERNATE_HIBALLOC_PAGE;
d859 2
a860 1
	hibernate_state = (struct hibernate_zlib_state *)HIBERNATE_HIBALLOC_PAGE;
d1177 2
a1178 1
	hibernate_state = (struct hibernate_zlib_state *)HIBERNATE_HIBALLOC_PAGE;
d1288 2
a1289 1
	hibernate_state = (struct hibernate_zlib_state *)HIBERNATE_HIBALLOC_PAGE;
d1376 2
a1377 1
					    (caddr_t)hibernate_copy_page, PAGE_SIZE);
d1384 2
a1385 1
					nblocks = PAGE_SIZE / hiber_info->secsize;
d1387 2
a1388 1
					if (hiber_info->io_func(hiber_info->device,
d1390 2
a1391 1
					    PAGE_SIZE, HIB_W, hiber_info->io_page))
d1460 2
a1461 1
	hibernate_state = (struct hibernate_zlib_state *)HIBERNATE_HIBALLOC_PAGE;
d1527 2
a1528 1
		pmap_kenter_pa(chunktable + i, piglet_chunktable + i, VM_PROT_ALL);
d1705 2
a1706 1
				piglet_cur += chunks[ochunks[j]].compressed_size;
d1709 2
a1710 1
				copy_start += chunks[ochunks[j]].compressed_size;
d1718 2
a1719 1
				piglet_cur += chunks[pchunks[j]].compressed_size;
d1721 2
a1722 1
				chunks[pchunks[j]].flags |= HIBERNATE_CHUNK_USED;
d1732 2
a1733 1
				chunks[ochunks[i]].flags |= HIBERNATE_CHUNK_USED;
@


1.48
log
@

increase the number of pages used to hold the chunk ordering map and change
the index type from int to short. Allows amd64 to hibernate with up to 64GB
phys memory
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.47 2013/01/17 00:11:24 mlarkin Exp $	*/
d1815 1
d1817 4
@


1.47
log
@

fix an error in the amd64 asm unhibernate code and a slight adjustment to
the MI hibernate code to handle 64 bit archs
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.46 2012/07/19 18:07:03 deraadt Exp $	*/
d1166 1
a1166 1
	int i, *fchunks;
d1174 1
a1174 1
	fchunks = (int *)(pva + (4 * PAGE_SIZE));
d1573 1
a1573 2
	int *ochunks, *pchunks, *fchunks;
	int i, j;
d1595 1
a1595 1
	fchunks = (int *)hibernate_fchunk_area;
d1598 1
a1598 1
	pchunks = (int *)(hibernate_fchunk_area + (PAGE_SIZE));
d1601 1
a1601 1
	ochunks = (int *)(hibernate_fchunk_area + (2*PAGE_SIZE));
d1604 1
a1604 1
	for(i=0; i<3 ; i++) {
@


1.46
log
@say that we are unhibernating..
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.45 2012/07/16 12:31:15 stsp Exp $	*/
d45 2
d1166 1
a1166 1
	int *fchunks, i;
d1174 1
a1174 1
	fchunks = (int *)(pva + (6 * PAGE_SIZE));
d1197 37
a1233 1
 * Process a chunk by ensuring its proper placement, followed by unpacking 
d1241 6
a1246 15
	/*
	 * If there is a conflict, copy the chunk to the piglet area
	 * before unpacking it to its original location.
	 */
	if ((chunk->flags & HIBERNATE_CHUNK_CONFLICT) == 0)
		hibernate_inflate_region(hiber_info, chunk->base,
		    img_cur, chunk->compressed_size);
	else {
		bcopy((caddr_t)img_cur,
		    pva + (HIBERNATE_CHUNK_SIZE * 2),
		    chunk->compressed_size);
		hibernate_inflate_region(hiber_info, chunk->base,
		    (vaddr_t)(pva + (HIBERNATE_CHUNK_SIZE * 2)),
		    chunk->compressed_size);
	}
d1286 1
a1286 1
	 * These will becomee part of the suspended kernel and will
d1452 1
a1452 1
	hibernate_zlib_start = (vaddr_t)(pva + (8 * PAGE_SIZE));
d1572 1
a1572 2
	int i, j, overlap, found, nchunks;
	int nochunks = 0, nfchunks = 0, npchunks = 0;
d1574 1
d1590 1
a1590 1
	hibernate_fchunk_area = (vaddr_t)km_alloc(3*PAGE_SIZE, &kv_any,
d1595 2
a1596 2
	/* Temporary output chunk ordering VA */
	ochunks = (int *)hibernate_fchunk_area;
d1599 1
a1599 1
	pchunks = (int *)(hibernate_fchunk_area + PAGE_SIZE);
d1602 1
a1602 1
	fchunks = (int *)(hibernate_fchunk_area + (2*PAGE_SIZE));
d1605 5
a1609 9
	pmap_kenter_pa(hibernate_fchunk_area,
	    piglet_base + (4*PAGE_SIZE), VM_PROT_ALL);
	pmap_update(pmap_kernel());	
	pmap_kenter_pa((vaddr_t)pchunks, piglet_base + (5*PAGE_SIZE),
	    VM_PROT_ALL);
	pmap_update(pmap_kernel());	
	pmap_kenter_pa((vaddr_t)fchunks, piglet_base + (6*PAGE_SIZE),
	    VM_PROT_ALL);
	pmap_update(pmap_kernel());	
d1648 1
a1648 1
			ochunks[nochunks] = (short)j;
@


1.45
log
@Back out my workaround from r1.43 now that jsing has committed a better fix.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.44 2012/07/16 12:15:58 jsing Exp $	*/
d1101 2
@


1.44
log
@Make sure we have a swap partition before trying to read or write
hibernate info.

ok mlarkin@@ stsp@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.42 2012/07/12 09:44:09 mlarkin Exp $	*/
d1090 2
a1091 4
	    hiber_info.secsize, (vaddr_t)&disk_hiber_info, 0)) {
		printf("error in hibernate read\n");
		goto fail;
	}
@


1.43
log
@Don't panic in hibernate resume if no swap partition exists on the root disk.
ok mlarkin
@
text
@d625 5
@


1.42
log
@

Temporarily don't do RLE page encoding in hibernate to fix some unpacking
corruption seen earlier. This makes hibernate much slower but seems to fix
the corruption problems seen on all machines we've tested on.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.41 2012/07/11 16:19:04 mlarkin Exp $	*/
d1085 4
a1088 2
	    hiber_info.secsize, (vaddr_t)&disk_hiber_info, 0))
		panic("error in hibernate read");
@


1.41
log
@

Enable the swap checker for hibernate
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.40 2012/07/09 09:47:42 deraadt Exp $	*/
d800 1
a800 1
	int rle, end_stream = 0 ;
a811 13
		/* Consume RLE skipped pages */
		do {
			rle = hibernate_get_next_rle();
			if (rle == -1) {
				end_stream = 1;
				goto next_page;
			}
	
			if (rle != 0)
				dest += (rle * PAGE_SIZE);

		} while (rle != 0);

a827 1
next_page:
d1241 1
a1241 1
	int i, rle;
d1322 4
a1325 15
				if (hibernate_inflate_skip(hiber_info, inaddr))
					rle = 1;
				else
					rle = uvm_page_rle(inaddr);

				while (rle != 0 && inaddr < range_end) {
					hibernate_state->hib_stream.next_in =
					    (char *)&rle;
					hibernate_state->hib_stream.avail_in =
					    sizeof(rle);
					hibernate_state->hib_stream.next_out =
					    (caddr_t)hibernate_io_page +
					    (PAGE_SIZE - out_remaining);
					hibernate_state->hib_stream.avail_out =
					    out_remaining;
d1327 2
a1328 3
					if (deflate(&hibernate_state->hib_stream,
					    Z_PARTIAL_FLUSH) != Z_OK)
						return (1);
d1330 4
a1333 7
					out_remaining =
					    hibernate_state->hib_stream.avail_out;
					inaddr += (rle * PAGE_SIZE);
					if (inaddr > range_end)
						inaddr = range_end;
					else
						rle = uvm_page_rle(inaddr);
a1345 21
					out_remaining = PAGE_SIZE;
				}

				/* Write '0' RLE code */
				if (inaddr < range_end) {
					hibernate_state->hib_stream.next_in =
					    (char *)&rle;
					hibernate_state->hib_stream.avail_in =
					    sizeof(rle);
					hibernate_state->hib_stream.next_out =
				    	    (caddr_t)hibernate_io_page +
					    (PAGE_SIZE - out_remaining);
					hibernate_state->hib_stream.avail_out =
					    out_remaining;

					if (deflate(&hibernate_state->hib_stream,
					    Z_PARTIAL_FLUSH) != Z_OK)
						return (1);

					out_remaining =
					    hibernate_state->hib_stream.avail_out;
a1346 39

				if (out_remaining == 0) {
					/* Filled up the page */
					nblocks = PAGE_SIZE / hiber_info->secsize;

					if (hiber_info->io_func(hiber_info->device,
					    blkctr, (vaddr_t)hibernate_io_page,
					    PAGE_SIZE, HIB_W, hiber_info->io_page))
						return (1);

					blkctr += nblocks;
					out_remaining = PAGE_SIZE;
				}

				/* Deflate from temp_inaddr to IO page */
				if (inaddr != range_end) {
					pmap_kenter_pa(hibernate_temp_page,
					    inaddr & PMAP_PA_MASK, VM_PROT_ALL);

					/* XXX - not needed on all archs */
					pmap_activate(curproc);

					bcopy((caddr_t)hibernate_temp_page,
					    (caddr_t)hibernate_copy_page, PAGE_SIZE);
					inaddr += hibernate_deflate(hiber_info,
					    temp_inaddr, &out_remaining);
				}
			}

			if (out_remaining == 0) {
				/* Filled up the page */
				nblocks = PAGE_SIZE / hiber_info->secsize;

				if (hiber_info->io_func(hiber_info->device,
				    blkctr, (vaddr_t)hibernate_io_page,
				    PAGE_SIZE, HIB_W, hiber_info->io_page))
					return (1);

				blkctr += nblocks;
@


1.40
log
@DVACT_SUSPEND must be done at cold with interrupts disabled. Failure
to read a hibernate image should restore the spl
ok mlarkin
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.39 2012/07/08 21:11:49 mlarkin Exp $	*/
d33 1
d1839 1
d1849 10
a1858 1
	pmap_kenter_pa(HIBERNATE_HIBALLOC_PAGE, HIBERNATE_HIBALLOC_PAGE, VM_PROT_ALL);
@


1.39
log
@

Quiesce and suspend devices in the resuming kernel for hibernate.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.38 2012/07/08 14:29:52 deraadt Exp $	*/
d1105 2
a1106 1
	if (hibernate_compare_signature(&hiber_info, &disk_hiber_info))
d1108 1
a1108 2

	uvm_pmr_zero_everything();
d1117 7
a1123 1
	if (config_suspend(TAILQ_FIRST(&alldevs), DVACT_SUSPEND) != 0)
d1125 1
d1129 2
a1130 4
	disable_intr();
	cold = 1;

	pmap_kenter_pa(HIBERNATE_HIBALLOC_PAGE, HIBERNATE_HIBALLOC_PAGE, VM_PROT_ALL);
d1152 1
@


1.38
log
@Make hibernate_free() safe to be called even if hibernate areas
allocation failed
ok mlarkin
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.37 2012/07/08 12:22:26 mlarkin Exp $	*/
d1112 6
@


1.37
log
@

Reorganize some hibernate functions for easier readability.
Fix some incorrect/old comments.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.36 2012/06/21 12:46:30 jmatthew Exp $	*/
d1855 1
a1855 1
 * Free items allocated during hibernate
d1860 8
a1867 1
	uvm_pmr_free_piglet(global_piglet_va, 3*HIBERNATE_CHUNK_SIZE);
a1868 2
	pmap_kremove(hibernate_copy_page, PAGE_SIZE);
	pmap_kremove(hibernate_temp_page, PAGE_SIZE);
d1871 10
a1880 2
	km_free((void *)hibernate_copy_page, PAGE_SIZE, &kv_any, &kp_none);
	km_free((void *)hibernate_temp_page, PAGE_SIZE, &kv_any, &kp_none);
@


1.36
log
@use regular i/o functions in the hibernate resume path.  ahci(4) hibernate
i/o will not mix with regular i/o so it can only be used in the hibernate
path.

ok deraadt@@ mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.35 2012/06/20 17:31:55 mlarkin Exp $	*/
d721 67
d796 1
a796 1
hibernate_inflate(union hibernate_info *hiber_info, paddr_t dest,
d799 1
a799 1
	int i, rle;
d811 1
d813 4
a816 11
			/* Read RLE code */
			hibernate_state->hib_stream.next_out = (char *)&rle;
			hibernate_state->hib_stream.avail_out = sizeof(rle);

			i = inflate(&hibernate_state->hib_stream, Z_FULL_FLUSH);
			if (i != Z_OK && i != Z_STREAM_END) {
				/*
				 * XXX - this will likely reboot/hang most machines,
				 *       but there's not much else we can do here.
				 */
				panic("inflate rle error");
d818 1
a818 5

			/* Sanity check what RLE value we got */
			if (rle > HIBERNATE_CHUNK_SIZE/PAGE_SIZE || rle < 0)
				panic("inflate rle error 3");

a820 2
			if (i == Z_STREAM_END)
				goto next_page;
d838 1
a838 16

		/* Set up the stream for inflate */
		hibernate_state->hib_stream.next_out =
		    (char *)HIBERNATE_INFLATE_PAGE;
		hibernate_state->hib_stream.avail_out = PAGE_SIZE;

		/* Process next block of data */
		i = inflate(&hibernate_state->hib_stream, Z_PARTIAL_FLUSH);
		if (i != Z_OK && i != Z_STREAM_END) {
			/*
			 * XXX - this will likely reboot/hang most machines,
			 *       but there's not much else we can do here.
			 */

 			panic("inflate error");
		}
d841 2
a842 2
		dest += PAGE_SIZE - hibernate_state->hib_stream.avail_out;
	} while (i != Z_STREAM_END);
d1170 1
a1170 1
	/* Can't use hiber_info that's passed in after here */
d1180 3
a1182 17
		/*
		 * If there is a conflict, copy the chunk to the piglet area
		 * before unpacking it to its original location.
		 */
		if ((chunks[fchunks[i]].flags & HIBERNATE_CHUNK_CONFLICT) == 0)
			hibernate_inflate(&local_hiber_info,
			    chunks[fchunks[i]].base, image_cur,
			    chunks[fchunks[i]].compressed_size);
		else {
			bcopy((caddr_t)image_cur,
			    pva + (HIBERNATE_CHUNK_SIZE * 2),
			    chunks[fchunks[i]].compressed_size);
			hibernate_inflate(&local_hiber_info,
			    chunks[fchunks[i]].base,
			    (vaddr_t)(pva + (HIBERNATE_CHUNK_SIZE * 2)),
			    chunks[fchunks[i]].compressed_size);
		}
d1184 27
d1221 1
a1221 1
 * one into 4MB chunks. These chunks are then compressed individually
d1231 2
a1232 18
 * side effects.
 *
 * This function uses the piglet area during this process as follows:
 *
 * offset from piglet base	use
 * -----------------------	--------------------
 * 0				i/o allocation area
 * PAGE_SIZE			i/o write area
 * 2*PAGE_SIZE			temp/scratch page
 * 3*PAGE_SIZE			temp/scratch page
 * 4*PAGE_SIZE			hiballoc arena
 * 5*PAGE_SIZE to 85*PAGE_SIZE	zlib deflate area
 * ...
 * HIBERNATE_CHUNK_SIZE		chunk table temporary area
 *
 * Some transient piglet content is saved as part of deflate,
 * but it is irrelevant during resume as it will be repurposed
 * at that time for other things.
a1599 14
 *
 * This function uses the piglet area during this process as follows:
 *
 * offset from piglet base	use
 * -----------------------	--------------------
 * 0				i/o allocation area
 * PAGE_SIZE			i/o write area
 * 2*PAGE_SIZE			temp/scratch page
 * 3*PAGE_SIZE			temp/scratch page
 * 4*PAGE_SIZE to 6*PAGE_SIZE	chunk ordering area
 * 7*PAGE_SIZE			hiballoc arena
 * 8*PAGE_SIZE to 88*PAGE_SIZE	zlib deflate area
 * ...
 * HIBERNATE_CHUNK_SIZE		chunk table temporary area
d1838 1
@


1.35
log
@

Fix some hibernate issues on machines with > 3.5GB phys mem

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.34 2012/04/12 14:57:36 ariane Exp $	*/
d653 9
a674 8
	/*
	 * Initialize of the hibernate IO function (for drivers which
	 * need that)
	 */
	if (hiber_info->io_func(hiber_info->device, 0,
	    (vaddr_t)NULL, 0, HIB_INIT, hiber_info->io_page))
		goto fail;

d909 3
a911 4
	/* XXX - use regular kernel write routine for this */
	if (hiber_info.io_func(hiber_info.device, hiber_info.sig_offset,
	    (vaddr_t)&blank_hiber_info, hiber_info.secsize, HIB_W,
	    hiber_info.io_page))
d974 2
a975 3
 * Reads read_size bytes from the hibernate device specified in
 * hib_info at offset blkctr. Output is placed into the vaddr specified
 * at dest.
d985 2
a986 2
hibernate_read_block(union hibernate_info *hib_info, daddr_t blkctr,
    size_t read_size, vaddr_t dest)
d992 1
a992 1
	bp = geteblk(read_size);
d997 1
a997 1
		printf("hibernate_read_block open failed\n");
d1001 4
a1004 1
	bp->b_bcount = read_size;
d1007 1
a1007 1
	SET(bp->b_flags, B_BUSY | B_READ | B_RAW);
d1014 1
a1014 1
		printf("hibernate_read_block biowait failed %d\n", error);
d1018 1
a1018 1
			printf("hibernate_read_block error close failed\n");
d1024 1
a1024 1
		printf("hibernate_read_block close failed\n");
d1028 2
a1029 1
	bcopy(bp->b_data, (caddr_t)dest, read_size);
d1056 3
a1058 4
	/* XXX use regular kernel read routine here */
	if (hiber_info.io_func(hiber_info.device, hiber_info.sig_offset,
	    (vaddr_t)&disk_hiber_info, hiber_info.secsize, HIB_R,
	    hiber_info.io_page))
d1518 2
a1519 2
		hibernate_read_block(hiber_info, blkctr, PAGE_SIZE,
		    chunktable + i);
d1766 2
a1767 2
			hibernate_read_block(hib_info, blkctr, read_size,
			    tempva + (img_cur & PAGE_MASK));
@


1.34
log
@hibernate: fix lock/unlock mismatch

Unlock missed an 'f', which caused it to unlock the in-use pageqs, rather
than the free pageqs as it was supposed to.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.33 2012/03/26 16:15:42 mlarkin Exp $	*/
a34 2
struct hibernate_zlib_state *hibernate_state;

d698 4
d712 4
d732 3
d743 2
a744 19
		/* Read RLE code */
		hibernate_state->hib_stream.next_out = (char *)&rle;
		hibernate_state->hib_stream.avail_out = sizeof(rle);

		i = inflate(&hibernate_state->hib_stream, Z_FULL_FLUSH);
		if (i != Z_OK && i != Z_STREAM_END) {
			/*
			 * XXX - this will likely reboot/hang most machines,
			 *       but there's not much else we can do here.
			 */
			panic("inflate rle error");
		}

		if (i == Z_STREAM_END)
			goto next_page;

		/* Skip while RLE code is != 0 */
		while (rle != 0) {
			dest += (rle * PAGE_SIZE);
d748 1
a748 2
			i = inflate(&hibernate_state->hib_stream,
			    Z_FULL_FLUSH);
d751 2
a752 3
				 * XXX - this will likely reboot/hang most
				 *       machines but there's not much else
				 *       we can do here.
d754 1
a754 1
				panic("inflate rle error 2");
a755 1
		}
d757 10
a766 2
		if (i == Z_STREAM_END)
			goto next_page;
d772 1
a772 1
		if (hibernate_inflate_skip(hiber_info, dest))
d776 1
a776 1
		else
d779 1
d795 2
a796 1
			panic("inflate error");
d816 3
d1066 2
d1077 3
d1118 3
a1130 3
	hibernate_state = (struct hibernate_zlib_state *)
	    (pva + (7 * PAGE_SIZE));

d1204 3
d1283 5
a1288 1
				rle = uvm_page_rle(inaddr);
d1446 1
d1448 4
a1451 2
	hibernate_state = (struct hibernate_zlib_state *)
	    (pva + (7 * PAGE_SIZE));
d1497 2
d1515 1
d1522 1
a1522 1
	pmap_kenter_pa(chunktable, piglet_chunktable, VM_PROT_ALL);
d1546 3
d1610 1
a1610 1
	/* Temporary output chunk ordering */
d1613 1
a1613 1
	/* Piglet chunk ordering */
d1616 1
a1616 1
	/* Final chunk ordering */
d1622 1
d1625 1
d1628 1
d1757 1
a1758 2
			/* XXX - not needed on all archs */
			pmap_activate(curproc);
d1777 5
d1809 3
@


1.33
log
@

Fix an integer math error when using the result of uvm_page_rle, and
at the same time increase said function's max RLE page count return value.

Add hooks in the right places to call the hibernate suspend and resume
routines, so that we can enable hibernation with a HIBERNATE option
line in GENERIC and appropriate acpi.c goo.

discussed on and off with deraadt@@ over the past few months
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.32 2011/11/29 05:21:08 deraadt Exp $	*/
d350 1
a350 1
	uvm_unlock_pageq();
@


1.32
log
@ready this for big-endian support later on
ok mlarkin
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.31 2011/11/29 04:59:22 mlarkin Exp $	*/
d568 2
a569 2
 * starting at the address, that are free.  Clamps to a max of 255 pages.
 * Returns 0 if the page at addr is not free.
d571 1
a571 1
u_char
d595 1
a595 1
	return max(pg_end - pg, 255);
d725 1
a725 2
	int i;
	u_char rle;
d1192 1
a1192 1
	size_t nblocks, out_remaining, used, offset = 0;
d1195 2
a1196 2
	daddr_t blkctr = hiber_info->image_offset;
	int i;
a1264 1
				u_char rle;
d1616 2
a1617 2
			ochunks[nochunks] = (u_int8_t)i;
			fchunks[nfchunks] = (u_int8_t)i;
d1682 1
@


1.31
log
@

Only free the piglet on hibernate fail if we are suspending.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.30 2011/11/23 07:11:31 deraadt Exp $	*/
d571 1
a571 1
psize_t
d726 1
a726 1
	psize_t rle;
d728 1
a729 1
	hibernate_state->hib_stream.next_in = (char *)src;
a735 1
		hibernate_state->hib_stream.avail_out = sizeof(psize_t);
d737 1
a753 2
			hibernate_state->hib_stream.avail_out =
			    sizeof(psize_t);
d755 1
a786 1
		hibernate_state->hib_stream.avail_out = PAGE_SIZE;
d789 1
d820 1
a821 2
	hibernate_state->hib_stream.avail_out = *remaining;
	hibernate_state->hib_stream.next_in = (caddr_t)src;
d824 1
d833 1
a833 1
		hibernate_state->hib_stream.avail_in;
a1197 1
	psize_t rle;
d1266 1
d1277 1
a1277 5
				while (rle > 0 && inaddr < range_end) {
					hibernate_state->hib_stream.avail_in =
					    sizeof(psize_t);
					hibernate_state->hib_stream.avail_out =
					    out_remaining;
d1280 2
d1285 2
a1315 4
					hibernate_state->hib_stream.avail_in =
					    sizeof(psize_t);
					hibernate_state->hib_stream.avail_out =
					    out_remaining;
d1318 2
d1323 2
d1386 1
a1387 2
		hibernate_state->hib_stream.avail_out = out_remaining;
		hibernate_state->hib_stream.next_in = (caddr_t)inaddr;
d1390 1
@


1.30
log
@clamp uvm_page_rle() to 255 pages at a time
ok mlarkin
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.29 2011/11/22 07:59:06 mlarkin Exp $	*/
d688 3
a690 1
	uvm_pmr_free_piglet(hiber_info->piglet_va, HIBERNATE_CHUNK_SIZE*3);
@


1.29
log
@

Ensure we are unpacking the right source address after skipping a range of
RLE pages in hibernate.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.28 2011/11/18 01:31:37 mlarkin Exp $	*/
d568 1
a568 1
 * starting at the address, that are free.
d593 3
a595 2
	    (pg_end->pg_flags & PQ_FREE) == PQ_FREE; pg_end++);
	return pg_end - pg;
@


1.28
log
@

no need to zero all the free pages since we will be skipping them with
rle.

noticed by jasper
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.27 2011/11/18 00:51:27 jasper Exp $	*/
a731 12
		/*
		 * Is this a special page? If yes, redirect the
		 * inflate output to a scratch page (eg, discard it)
		 */
		if (hibernate_inflate_skip(hiber_info, dest))
			hibernate_enter_resume_mapping(
			    HIBERNATE_INFLATE_PAGE,
			    HIBERNATE_INFLATE_PAGE, 0);
		else
			hibernate_enter_resume_mapping(
			    HIBERNATE_INFLATE_PAGE, dest, 0);

d745 3
d767 17
d799 1
d1120 3
a1264 8
				pmap_kenter_pa(hibernate_temp_page,
				    inaddr & PMAP_PA_MASK, VM_PROT_ALL);

				/* XXX - not needed on all archs */
				pmap_activate(curproc);

				bcopy((caddr_t)hibernate_temp_page,
				    (caddr_t)hibernate_copy_page, PAGE_SIZE);
d1268 2
a1269 1
				 * divisible by PAGE_SIZE
d1346 9
a1354 1
				if (inaddr != range_end)
d1357 1
@


1.27
log
@- fix a few trailing whitespaces and a spello
- panic strings already get printed with a '\n', so remove the extra ones

ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.26 2011/11/18 00:28:46 mlarkin Exp $	*/
a1765 3

	/* XXX - Won't need to zero everything with RLE */
	uvm_pmr_zero_everything();
@


1.26
log
@

fix a corner case in rle processing where the rle byte was the last byte
being written to an output buffer
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.25 2011/11/17 23:18:13 mlarkin Exp $	*/
d763 1
a763 1
		
d770 1
a770 1
				 *       we can do here. 
d778 1
a778 1
		hibernate_state->hib_stream.next_out = 
d817 1
a817 1
		panic("hibernate zlib deflate error\n");
d900 1
a900 1
		panic("error hibernate write 6\n");
d1045 1
a1045 1
		panic("error in hibernate read\n");
d1116 1
a1116 1
			panic("hibernate failed to reset zlib for inflate\n");
d1326 1
a1326 1
	
d1463 1
a1463 1
	    HIBERNATE_CHUNK_SIZE; 
@


1.25
log
@

physmem run length encoding (rle) for hibernate - don't compress or write
pages that are free
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.24 2011/11/16 23:52:27 mlarkin Exp $	*/
d1294 13
d1325 13
@


1.24
log
@

Reduce use of globals in hibernate code.

discussed with deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.23 2011/11/15 17:13:53 deraadt Exp $	*/
a713 5
 * For each page of output data, we map HIBERNATE_TEMP_PAGE
 * to the current output page, and tell inflate() to inflate
 * its data there, resulting in the inflated data being placed
 * at the proper paddr.
 *
d723 1
d744 32
d1184 1
d1269 45
d1315 3
a1317 2
				inaddr += hibernate_deflate(hiber_info,
				    temp_inaddr, &out_remaining);
d1412 1
a1412 1
		    Z_DEFAULT_COMPRESSION);
@


1.23
log
@on failure, hibernate io functions return an errno.  not 1.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.22 2011/11/14 00:25:17 mlarkin Exp $	*/
a39 3
vaddr_t hibernate_stack_page;
vaddr_t hibernate_fchunk_area;
vaddr_t	hibernate_chunktable_area;
a1004 6
	/* Scrub temporary vaddr ranges used during resume */
	hibernate_temp_page = (vaddr_t)NULL;
	hibernate_fchunk_area = (vaddr_t)NULL;
	hibernate_chunktable_area = (vaddr_t)NULL;
	hibernate_stack_page = (vaddr_t)NULL;

a1025 27
	/*
	 * Allocate several regions of vaddrs for use during read.
	 * These mappings go into the resuming kernel's page table, and are
	 * used only during image read.
	 */
	hibernate_temp_page = (vaddr_t)km_alloc(2*PAGE_SIZE, &kv_any,
	    &kp_none, &kd_nowait);
	if (!hibernate_temp_page)
		goto fail;

	hibernate_fchunk_area = (vaddr_t)km_alloc(3*PAGE_SIZE, &kv_any,
	    &kp_none, &kd_nowait);
	if (!hibernate_fchunk_area)
		goto fail;

	/* Allocate a temporary chunktable area */
	hibernate_chunktable_area = (vaddr_t)malloc(HIBERNATE_CHUNK_TABLE_SIZE,
					   M_DEVBUF, M_NOWAIT);
	if (!hibernate_chunktable_area)
		goto fail;

	/* Allocate one temporary page of VAs for the resume time stack */
	hibernate_stack_page = (vaddr_t)km_alloc(PAGE_SIZE, &kv_any,
	    &kp_none, &kd_nowait);
	if (!hibernate_stack_page)
		goto fail;

a1054 11

	if (hibernate_temp_page)
		km_free((void *)hibernate_temp_page, 2*PAGE_SIZE, &kv_any,
		    &kp_none);

	if (hibernate_fchunk_area)
		km_free((void *)hibernate_fchunk_area, 3*PAGE_SIZE, &kv_any,
		    &kp_none);

	if (hibernate_chunktable_area)
		free((void *)hibernate_chunktable_area, M_DEVBUF);
a1070 1
	vaddr_t tempva;
a1077 10
	/* Copy temporary chunktable to piglet */
	tempva = (vaddr_t)km_alloc(HIBERNATE_CHUNK_TABLE_SIZE, &kv_any,
	    &kp_none, &kd_nowait);
	for (i = 0; i < HIBERNATE_CHUNK_TABLE_SIZE; i += PAGE_SIZE)
		pmap_kenter_pa(tempva + i, hiber_info->piglet_pa +
		    HIBERNATE_CHUNK_SIZE + i, VM_PROT_ALL);

	bcopy((caddr_t)hibernate_chunktable_area, (caddr_t)tempva,
	    HIBERNATE_CHUNK_TABLE_SIZE);

d1161 2
a1163 1

d1360 3
d1371 7
d1379 5
a1383 3
	    i += MAXPHYS, blkctr += MAXPHYS/hiber_info->secsize)
		hibernate_read_block(hiber_info, blkctr, MAXPHYS,
		    hibernate_chunktable_area + i);
d1387 2
a1388 1
	chunks = (struct hibernate_disk_chunk *)hibernate_chunktable_area;
d1408 2
a1409 1
	hibernate_read_chunks(hiber_info, image_start, image_end, disk_size);
d1440 2
a1441 1
    paddr_t pig_end, size_t image_compr_size)
a1450 1
	struct hibernate_disk_chunk *chunks;
d1452 1
d1459 13
a1489 1
	chunks = (struct hibernate_disk_chunk *)hibernate_chunktable_area;
d1612 2
a1613 1
			pmap_kenter_pa(hibernate_temp_page, img_cur,
a1614 2
			pmap_kenter_pa(hibernate_temp_page + PAGE_SIZE,
			    img_cur+PAGE_SIZE, VM_PROT_ALL);
d1624 1
a1624 1
			    hibernate_temp_page + (img_cur & PAGE_MASK));
d1629 2
a1630 3
			pmap_kremove(hibernate_temp_page, PAGE_SIZE);
			pmap_kremove(hibernate_temp_page + PAGE_SIZE,
			    PAGE_SIZE);
a1693 1
	km_free((void *)hibernate_fchunk_area, 3*PAGE_SIZE, &kv_any, &kp_none);
@


1.22
log
@

Use a fixed io_page for all hibernate I/O, which is needed for
ahci_hibernate_io, a skeleton of which is also provided in this diff.

This code is from deraadt@@. Tested on a few wd machines to ensure it works
there as well.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.21 2011/11/13 23:13:29 mlarkin Exp $	*/
d671 2
a672 2
	 * operation -1 (HIB_INIT) requests initialization of the hibernate
	 * IO function
d675 1
a675 1
	    (vaddr_t)NULL, 0, HIB_INIT, hiber_info->io_page) == -1)
@


1.21
log
@

Add some #defines for the various hibernate I/O modes, some of the
groundwork for *_hibernate_io functions other than wd_hibernate_io

These changes were sent to me by deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.20 2011/11/13 22:36:27 mlarkin Exp $	*/
d656 11
d669 9
d679 1
a679 1
		return (1);
d689 3
a810 7
	u_int8_t *io_page;
	int result = 0;

	io_page = malloc(PAGE_SIZE, M_DEVBUF, M_NOWAIT);
	if (!io_page)
		return (1);

d812 3
a814 6
	if (hiber_info->io_func(hiber_info->device, hiber_info->sig_offset,
	    (vaddr_t)hiber_info, hiber_info->secsize, HIB_W, io_page))
		result = 1;

	free(io_page, M_DEVBUF);
	return (result);
a827 1
	u_int8_t *io_page;
a830 4
	io_page = malloc(PAGE_SIZE, M_DEVBUF, M_NOWAIT);
	if (!io_page)
		return (1);

d847 1
a847 2
		    MAXPHYS, HIB_W, io_page)) {
			free(io_page, M_DEVBUF);
a848 1
		}
a850 1
	free(io_page, M_DEVBUF);
a862 1
	u_int8_t *io_page;
a869 4
	io_page = malloc(PAGE_SIZE, M_DEVBUF, M_NOWAIT);
	if (!io_page)
		return (1);

d873 2
a874 1
	    (vaddr_t)&blank_hiber_info, hiber_info.secsize, HIB_W, io_page))
a876 2
	free(io_page, M_DEVBUF);

a1005 1
	u_int8_t *io_page;
a1018 4
	io_page = malloc(PAGE_SIZE, M_DEVBUF, M_NOWAIT);
	if (!io_page)
		return;

d1024 2
a1025 1
	    (vaddr_t)&disk_hiber_info, hiber_info.secsize, HIB_R, io_page))
a1027 2
	free(io_page, M_DEVBUF);

a1099 3
	if (io_page)
		free((void *)io_page, M_DEVBUF);

a1210 1
	vaddr_t hibernate_alloc_page = hiber_info->piglet_va;
d1308 1
a1308 1
				    PAGE_SIZE, HIB_W, (void *)hibernate_alloc_page))
d1348 1
a1348 1
		    HIB_W, (void *)hibernate_alloc_page))
@


1.20
log
@

In hibernate resume, free the piglet and other VAs we allocated during
suspend.

ok pirofti@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.19 2011/11/13 18:38:10 mlarkin Exp $	*/
d797 1
a797 1
	    (vaddr_t)hiber_info, hiber_info->secsize, 1, io_page))
d839 1
a839 1
		    MAXPHYS, 1, io_page)) {
d873 1
a873 1
	    (vaddr_t)&blank_hiber_info, hiber_info.secsize, 1, io_page))
d1030 1
a1030 1
	    (vaddr_t)&disk_hiber_info, hiber_info.secsize, 0, io_page))
d1319 1
a1319 1
				    PAGE_SIZE, 1, (void *)hibernate_alloc_page))
d1359 1
a1359 1
		    1, (void *)hibernate_alloc_page))
@


1.19
log
@

Fix a handful of bugs that were causing reboots and other bad behavior
during hibernate resumes.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.18 2011/09/22 22:12:45 deraadt Exp $	*/
d47 1
d1707 2
d1721 1
a1721 1
	delay(100000);
d1723 17
@


1.18
log
@KNF of mlarkin's code, requested by him.  Some improvements to the interface
for talking to the disk driver snuck in.
ok mlarkin
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.17 2011/09/21 06:13:39 mlarkin Exp $	*/
a42 1
vaddr_t hibernate_inflate_page;
d46 1
a710 2
	hibernate_inflate_page = hiber_info->piglet_va + 2 * PAGE_SIZE;

d721 2
a722 2
			    hibernate_inflate_page,
			    hiber_info->piglet_pa + 2 * PAGE_SIZE, 0);
d725 1
a725 1
			    hibernate_inflate_page, dest, 0);
d729 2
a730 2
		hibernate_state->hib_stream.next_out =
		    (char *)hiber_info->piglet_va + 2 * PAGE_SIZE;
d1125 2
a1126 1
	paddr_t image_cur;
d1129 1
a1129 3
	char *pva;

	pva = (char *)hiber_info->piglet_va;
d1131 3
a1133 1
	fchunks = (int *)(pva + (4 * PAGE_SIZE));
d1147 3
d1152 1
a1152 1
	for (i = 0; i < hiber_info->chunk_ctr; i++) {
d1154 1
a1154 1
		if (hibernate_zlib_reset(hiber_info, 0) != Z_OK)
d1162 3
a1164 2
			hibernate_inflate(hiber_info, chunks[fchunks[i]].base,
			    image_cur, chunks[fchunks[i]].compressed_size);
d1167 1
a1167 2
			    (caddr_t)hiber_info->piglet_va +
			    HIBERNATE_CHUNK_SIZE * 2,
d1169 3
a1171 2
			hibernate_inflate(hiber_info, chunks[fchunks[i]].base,
			    hiber_info->piglet_va + HIBERNATE_CHUNK_SIZE * 2,
d1244 1
a1244 1
	/* XXX - needed on i386. check other archs */
d1293 2
d1300 4
a1303 1
				/* Adjust for non page-sized regions */
d1383 1
d1385 2
a1386 2
	hibernate_state = (struct hibernate_zlib_state *)hiber_info->piglet_va +
	    (4 * PAGE_SIZE);
d1388 1
a1388 1
	hibernate_zlib_start = hiber_info->piglet_va + (5 * PAGE_SIZE);
d1501 2
a1502 1
	int i, j, overlap, found, nchunks, nochunks = 0, nfchunks = 0, npchunks = 0;
d1504 3
a1506 1
	u_int8_t *ochunks, *pchunks, *fchunks;
d1508 2
a1509 7
	/* Map the chunk ordering region */
	pmap_kenter_pa(hibernate_fchunk_area,
	    piglet_base + (4*PAGE_SIZE), VM_PROT_ALL);
	pmap_kenter_pa(hibernate_fchunk_area + PAGE_SIZE,
	    piglet_base + (5*PAGE_SIZE), VM_PROT_ALL);
	pmap_kenter_pa(hibernate_fchunk_area + 2*PAGE_SIZE,
	    piglet_base + (6*PAGE_SIZE), VM_PROT_ALL);
d1512 1
a1512 1
	ochunks = (u_int8_t *)hibernate_fchunk_area;
d1515 1
a1515 1
	pchunks = (u_int8_t *)hibernate_fchunk_area + PAGE_SIZE;
d1518 9
a1526 1
	fchunks = (u_int8_t *)hibernate_fchunk_area + 2*PAGE_SIZE;
d1657 1
a1657 1
			/* XXX - needed on i386. check other archs */
d1715 5
a1719 1
	return hibernate_write_signature(&hib_info);
@


1.17
log
@

Cleanup page calculation for final memory chunk ordering list for
hibernate resume.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.16 2011/09/21 02:51:23 mlarkin Exp $	*/
d27 1
a34 2
extern char *disk_readlabel(struct disklabel *, dev_t, char *, size_t);

d58 1
a58 2
struct hiballoc_entry
{
d81 1
a81 1
static __inline void*
d95 1
a95 1
hib_addr_to_entry(void* addr_param)
d111 1
a111 1
void*
d169 1
a169 1
 * 
d195 1
a195 1
	    (void*)((caddr_t)prev + HIB_SIZEOF(struct hiballoc_entry) +
d202 1
a202 1
	  	/* Flip used memory to free space. */
a246 1

d449 1
a449 1
			    atop(piglet_addr) + atop(sz)) {
a450 1
			}
a505 1

d563 1
a563 1
	km_free((void*)va, sz, &kv_any, &kp_none);
d569 1
a569 1
 * Given a physical page address, it will return the number of pages 
a602 1
 *
d637 2
a638 2
		dl.d_partitions[1].p_size -
		sizeof(union hibernate_info)/hiber_info->secsize;
d644 2
a645 2
	bcopy(version, &hiber_info->kernel_version, 
		min(strlen(version), sizeof(hiber_info->kernel_version)-1));
d650 2
a651 3
					&hiber_info->piglet_pa,
					HIBERNATE_CHUNK_SIZE*3,
					HIBERNATE_CHUNK_SIZE)) {
d662 4
a665 4
		dl.d_partitions[1].p_size -
		(hiber_info->image_size / hiber_info->secsize) -
		sizeof(union hibernate_info)/hiber_info->secsize -
		chunktable_size;
d672 1
a672 1
 */ 
d703 2
a704 2
hibernate_inflate(union hibernate_info *hiber_info,
	paddr_t dest, paddr_t src, size_t size)
d723 2
a724 3
				hibernate_inflate_page,
				hiber_info->piglet_pa + 2 * PAGE_SIZE,
				0);
d727 1
a727 2
				hibernate_inflate_page,
				dest, 0);
d732 1
a732 1
			(char *)hiber_info->piglet_va + 2 * PAGE_SIZE;
d757 1
a757 1
	size_t *remaining)
d762 1
a762 2
	hibernate_state->hib_stream.avail_in = PAGE_SIZE -
		(src & PAGE_MASK);
d766 1
a766 1
		(PAGE_SIZE - *remaining);
d798 2
a799 3
		(vaddr_t)hiber_info, hiber_info->secsize, 1, io_page)) {
			result = 1;
	}
d813 3
d817 1
a818 4
	daddr_t chunkbase;
	vaddr_t hibernate_chunk_table_start;
	size_t hibernate_chunk_table_size;
	struct hibernate_disk_chunk *chunks;
d827 1
a827 1
		    (hibernate_chunk_table_size / hiber_info->secsize);
d830 1
a830 1
					HIBERNATE_CHUNK_SIZE;
d833 1
a833 1
		HIBERNATE_CHUNK_SIZE);
d836 7
a842 9
	for(i=0; i < hibernate_chunk_table_size; i += MAXPHYS) {
		if(hiber_info->io_func(hiber_info->device,
			chunkbase + (i/hiber_info->secsize),
			(vaddr_t)(hibernate_chunk_table_start + i),
			MAXPHYS,
			1,
			io_page)) {
				free(io_page, M_DEVBUF);
				return (1);
a846 1

d855 1
a855 1
hibernate_clear_signature()
d873 3
a875 3
	if(hiber_info.io_func(hiber_info.device, hiber_info.sig_offset,
		(vaddr_t)&blank_hiber_info, hiber_info.secsize, 1, io_page))
			panic("error hibernate write 6\n");
d878 1
a878 1
	
d892 1
a892 1
	if (r1s < r2s && r1e > r2s) 
d919 1
a919 1
	union hibernate_info *disk)
d929 1
a929 1
	for (i=0; i< mine->nranges; i++) {
d931 2
a932 2
			(mine->ranges[i].end != disk->ranges[i].end) )
		return (1);
d952 1
a952 1
	size_t read_size, vaddr_t dest)
d979 2
a980 2
				curproc);
		if (error) 
d1005 1
a1005 1
hibernate_resume()
d1030 3
a1032 4
	if(hiber_info.io_func(hiber_info.device, hiber_info.sig_offset,
				(vaddr_t)&disk_hiber_info,
				hiber_info.secsize, 0, io_page))
			panic("error in hibernate read\n");
d1040 1
a1040 2
	if (hibernate_compare_signature(&hiber_info,
		&disk_hiber_info))
d1049 1
a1049 1
						&kp_none, &kd_nowait);
d1054 1
a1054 1
						&kp_none, &kd_nowait);
d1066 1
a1066 1
						&kp_none, &kd_nowait);
d1089 2
a1090 2
	hibernate_unpack_image(&disk_hiber_info);	
	
d1098 1
a1098 1
	printf("Unable to resume hibernated image\n");	
d1102 1
a1102 1
			&kp_none);
d1106 1
a1106 1
			&kp_none);
d1126 1
a1126 1
	int i;
d1129 1
a1129 1
	struct hibernate_disk_chunk *chunks;
a1130 1
	int *fchunks;
d1138 2
a1139 2
			&kp_none, &kd_nowait);
	for (i=0; i<HIBERNATE_CHUNK_TABLE_SIZE; i += PAGE_SIZE)
d1141 1
a1141 1
			HIBERNATE_CHUNK_SIZE + i, VM_PROT_ALL);
d1144 1
a1144 1
		HIBERNATE_CHUNK_TABLE_SIZE);
d1150 1
a1150 1
	for (i=0; i<hiber_info->chunk_ctr; i++) {
d1155 1
a1155 1
		/*	
d1159 3
a1161 4
		if((chunks[fchunks[i]].flags & HIBERNATE_CHUNK_CONFLICT) == 0) 
			hibernate_inflate(hiber_info,
				chunks[fchunks[i]].base, image_cur,
				chunks[fchunks[i]].compressed_size);
d1164 6
a1169 8
				(caddr_t)hiber_info->piglet_va +
				HIBERNATE_CHUNK_SIZE * 2,
				chunks[fchunks[i]].compressed_size);
			hibernate_inflate(hiber_info,
				chunks[fchunks[i]].base,
				hiber_info->piglet_va +
				HIBERNATE_CHUNK_SIZE * 2,
				chunks[fchunks[i]].compressed_size);
d1208 1
a1208 1
 * but it is irrelevant during resume as it will be repurposed 
d1215 1
a1215 3
	daddr_t blkctr;
	int i;
	size_t nblocks, out_remaining, used, offset;
d1219 2
a1221 1
	blkctr = hiber_info->image_offset;
a1222 1
	offset = 0;
d1229 1
a1229 1
						&kp_none, &kd_nowait);
d1234 1
a1234 1
						&kp_none, &kd_nowait);
d1239 1
a1239 2
			(hiber_info->piglet_pa + 3*PAGE_SIZE),
			VM_PROT_ALL);
d1245 1
a1245 1
			HIBERNATE_CHUNK_SIZE);
d1248 3
a1250 3
	for (i=0; i < hiber_info->nranges; i++) {
                range_base = hiber_info->ranges[i].base;
                range_end = hiber_info->ranges[i].end;
d1258 1
a1258 1
					HIBERNATE_CHUNK_SIZE;
d1265 1
a1265 1
	} 
d1268 1
a1268 1
	for (i=0; i < hiber_info->chunk_ctr; i++) {
d1272 1
a1272 1
		chunks[i].offset = blkctr; 
d1289 1
a1289 1
					inaddr & PMAP_PA_MASK, VM_PROT_ALL);
d1293 1
a1293 1
					(caddr_t)hibernate_copy_page, PAGE_SIZE);
d1297 1
a1297 1
					hibernate_copy_page;
d1301 1
a1301 2
						temp_inaddr,
						&out_remaining);
d1308 4
a1311 4
				if(hiber_info->io_func(hiber_info->device, blkctr,
					(vaddr_t)hibernate_io_page, PAGE_SIZE,
					1, (void *)hibernate_alloc_page))
						return (1);
a1314 1
			
d1331 2
a1332 2
		hibernate_state->hib_stream.next_out = 
			(caddr_t)hibernate_io_page + (PAGE_SIZE - out_remaining);
d1335 2
a1336 2
			Z_STREAM_END)
				return (1);
d1348 4
a1351 4
		if( hiber_info->io_func(hiber_info->device, blkctr,
			(vaddr_t)hibernate_io_page, nblocks*hiber_info->secsize,
			1, (void *)hibernate_alloc_page))
				return (1);
d1356 2
a1357 3
		chunks[i].compressed_size=
			(offset-chunks[i].offset)*hiber_info->secsize;

d1377 1
a1377 1
				(4 * PAGE_SIZE);
d1391 1
a1391 1
		(caddr_t)hibernate_zlib_start, hibernate_zlib_size);
d1395 2
a1396 3
			Z_DEFAULT_COMPRESSION);
	}
	else
d1414 1
a1414 1
	int i;
d1416 1
d1418 1
a1418 2
	struct hibernate_disk_chunk *chunks;
	size_t compressed_size, disk_size, chunktable_size, pig_sz;
d1426 1
a1426 1
	for(i=0; i < HIBERNATE_CHUNK_TABLE_SIZE;
d1429 1
a1429 1
			hibernate_chunktable_area + i);
d1435 1
a1435 1
	for (i=0; i<hiber_info->chunk_ctr; i++)
d1441 1
a1441 1
	pig_sz =  compressed_size + HIBERNATE_CHUNK_SIZE;
d1484 1
a1484 1
			paddr_t pig_end, size_t image_compr_size)
d1492 1
a1492 1
	int i, j, overlap, found, nchunks, nochunks=0, nfchunks=0, npchunks=0;
d1498 1
a1498 1
		piglet_base + (4*PAGE_SIZE), VM_PROT_ALL);
d1500 1
a1500 1
		piglet_base + (5*PAGE_SIZE), VM_PROT_ALL);
d1502 1
a1502 2
		piglet_base + (6*PAGE_SIZE),
	 	VM_PROT_ALL);
d1517 2
a1518 2
	for (i=0; i < nchunks; i++)
		chunks[i].flags=0;
d1524 2
a1525 2
	for (i=0; i < nchunks; i++) {
		if(chunks[i].end <= pig_start || chunks[i].base >= pig_end) { 
d1533 1
a1533 1
 
d1540 4
a1543 4
		found=0;
		j=-1;
		for (i=0; i < nchunks; i++)
			if (chunks[i].base < img_index && 
d1557 1
a1557 1
	img_index=pig_start;
d1563 2
a1564 2
	for(i=0; i< nochunks ; i++) {
		overlap=0;
d1572 1
a1572 2
 			chunks[ochunks[i]].flags |= HIBERNATE_CHUNK_CONFLICT;
		
d1580 2
a1581 2
	img_index=pig_start;
	for (i=0; i < nochunks ; i++) {
d1591 1
a1591 1
			j=i;
d1600 1
a1600 1
			}	
d1603 1
a1603 1
			for (j=0; j < npchunks; j++) {
d1608 1
a1608 1
			}	
a1618 1
				
d1624 1
a1624 1
	for(i=0 ; i< nfchunks; i++) {
d1627 1
a1627 1
	}	
d1630 2
a1631 2
	
	for(i=0; i<nfchunks; i++) {
d1638 1
a1638 1
				VM_PROT_ALL);
d1640 1
a1640 1
				img_cur+PAGE_SIZE, VM_PROT_ALL);
d1648 1
a1648 1
			
d1650 1
a1650 1
				hibernate_temp_page + (img_cur & PAGE_MASK));
d1657 1
a1657 1
				PAGE_SIZE);
d1675 1
a1675 1
 * On most architectures, the function calling hibernate_suspend would 
d1679 1
a1679 1
hibernate_suspend()
d1684 1
a1684 1
	 * Calculate memory ranges, swap offsets, etc. 
@


1.16
log
@

Perform most of the remaining refactoring of hibernate code into
MI/MD parts. This also introduces a chunk placement routine that was
originally developed at c2k11 with help from drahn and ariane.

There are still a few more things to do for hibernate, but those can be
worked on in-tree. This code is disabled by default, and not yet called.

ok deraadt@@ (and deraadt@@ said kettenis@@ also ok'ed it :) )
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.15 2011/07/18 16:50:56 ariane Exp $	*/
d1146 2
a1147 1
	u_int8_t *fchunks;
d1149 3
a1151 2
	fchunks = (u_int8_t *)((char *)(hiber_info->piglet_va) +
			(4 * PAGE_SIZE));
d1163 1
a1163 2
	chunks = (struct hibernate_disk_chunk *)((char *)(hiber_info->piglet_va)) +
			HIBERNATE_CHUNK_SIZE;
@


1.15
log
@Allocations fit if [the end of free space] >= [the end of allocated space].
Change > in comparison to >=.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.14 2011/07/18 16:48:26 ariane Exp $	*/
d5 1
d36 12
a47 1
struct hibernate_state *hibernate_state;
d416 1
a416 1
uvm_pmr_alloc_piglet(paddr_t *addr, psize_t sz, paddr_t align)
d418 1
a418 1
	vaddr_t			 pg_addr, piglet_addr;
d423 1
d435 1
a435 1
	sz = atop(round_page(sz));
d452 1
a452 1
			    atop(piglet_addr) + sz) {
d456 1
d458 12
a469 13
		/*
		 * Try to coerse the pagedaemon into freeing memory
		 * for the piglet.
		 *
		 * pdaemon_woken is set to prevent the code from
		 * falling into an endless loop.
		 */
		if (!pdaemon_woken) {
			pdaemon_woken = 1;
			if (uvm_wait_pla(ptoa(pmr->low), ptoa(pmr->high) - 1,
			    ptoa(sz), UVM_PLA_FAILOK) == 0)
				goto retry;
		}
d482 1
a482 1
	    atop(piglet_addr), atop(piglet_addr) + sz, &pageq);
d484 2
a485 2
	*addr = piglet_addr;
	uvmexp.free -= sz;
d509 22
d535 37
a604 2
 * get_hibernate_info
 *
d611 1
a611 1
get_hibernate_info(union hibernate_info *hiber_info)
d648 19
d674 1
a674 13
	/* Stash kernel version information */
	bzero(&hiber_info->kernel_version, 128);
	bcopy(version, &hiber_info->kernel_version, 
		min(strlen(version), sizeof(hiber_info->kernel_version)-1));

	/* Allocate piglet region */
	if (uvm_pmr_alloc_piglet(&hiber_info->piglet_base, HIBERNATE_CHUNK_SIZE,
		HIBERNATE_CHUNK_SIZE)) {
		printf("Hibernate failed to allocate the piglet\n");
		return (1);
	}

	return get_hibernate_info_md(hiber_info);
a677 2
 * hibernate_zlib_alloc
 *
a678 1
 *
a686 2
 * hibernate_zlib_free
 *
a688 1
 *
a696 2
 * hibernate_inflate
 *
a707 1
 *
d710 2
a711 1
hibernate_inflate(paddr_t dest, paddr_t src, size_t size)
d718 2
d728 5
a732 3
		if (hibernate_inflate_skip(dest))
			hibernate_enter_resume_mapping(HIBERNATE_TEMP_PAGE,
				HIBERNATE_TEMP_PAGE, 0);
d734 2
a735 1
			hibernate_enter_resume_mapping(HIBERNATE_TEMP_PAGE,
d741 1
a741 1
			(char *)HIBERNATE_TEMP_PAGE;
a757 2
 * hibernate_deflate
 *
a762 1
 *
d765 2
a766 1
hibernate_deflate(paddr_t src, size_t *remaining)
d768 2
d775 1
a775 1
	hibernate_state->hib_stream.next_out = (caddr_t)HIBERNATE_IO_PAGE +
a788 2
 * hibernate_write_signature
 *
d800 26
d827 3
a829 1
	size_t i;
d835 1
a835 4
	/* Write hibernate info to disk */
	if( hiber_info->io_func(hiber_info->device, hiber_info->sig_offset,
		(vaddr_t)hiber_info, hiber_info->secsize, 1, io_page))
			panic("error in hibernate write sig\n");
d838 7
a844 1
		    (HIBERNATE_CHUNK_TABLE_SIZE / hiber_info->secsize);
d847 1
a847 1
	for(i=0; i < HIBERNATE_CHUNK_TABLE_SIZE; i += NBPG) {
d850 2
a851 2
			(vaddr_t)(HIBERNATE_CHUNK_TABLE_START + i),
			NBPG,
d853 4
a856 2
			io_page))
				panic("error in hibernate write chunks\n");
a864 2
 * hibernate_clear_signature
 *
d878 1
a878 1
	if (get_hibernate_info(&hiber_info))
d886 1
a896 2
 * hibernate_check_overlap
 *
a924 2
 * hibernate_compare_signature
 *
d953 1
a953 3
 * hibernate_read_block
 *
 * Reads read_size blocks from the hibernate device specified in
d1013 718
@


1.14
log
@Fix uvm_pmr_alloc_piglet.
A wrong check could cause the piglet allocator to attempt to extract memory
from a range in which the alignment caused it not to fit.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.13 2011/07/11 03:30:32 mlarkin Exp $	*/
d438 1
a438 1
			if (atop(pg_addr) + pig_pg->fpgsz >
@


1.13
log
@

Add hibernate_read_block and fix a couple of typos in the previous commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.12 2011/07/09 03:10:27 mlarkin Exp $	*/
a436 4

			if (pig_pg->fpgsz >= sz) {
				goto found;
			}
@


1.12
log
@

Call (temporarily, until we have RLE page encoding) uvm_pmr_zero_everything
on suspend to ensure we get good zlib compression.

Add MI signature block (hibernate_info) comparison routine
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.11 2011/07/09 01:30:39 mlarkin Exp $	*/
d27 3
d852 63
@


1.11
log
@

Extract hibernate_write_signature and hibernate_clear_signature to the MI
hibernate code, and add chunk range overlap checking.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.10 2011/07/09 00:55:00 mlarkin Exp $	*/
d822 31
@


1.10
log
@

Extract MI pmap function hibernate_enter_resume_mapping, refactor old i386
resume pmap code to match.

Add hibernate deflater and inflater and cache flush routines.

Code is not presently called or automatically built.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.9 2011/07/09 00:27:31 mlarkin Exp $	*/
d20 1
d614 13
d713 108
@


1.9
log
@

Add zlib reset, alloc, and free functions for hibernate image compression
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.8 2011/07/09 00:08:04 mlarkin Exp $	*/
d613 13
a625 1
 * hibernate_zlib_free
a626 3
 * Free the memory pointed to by addr in the hiballoc area presently in
 * use
 * 
d629 53
a681 1
hibernate_zlib_free(void *unused, void *addr)
d683 16
a698 1
	hib_free(&hibernate_state->hiballoc_arena, addr);
@


1.8
log
@

Separate some MD and MI bits and a bit of refactoring to make subsequent
commits easier.

Work in progress, hibernate will still not work for you.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.7 2011/07/08 21:02:49 ariane Exp $	*/
d31 2
d598 25
@


1.7
log
@no reason to zero pages we are about to mark dirty
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.6 2011/07/08 21:00:53 ariane Exp $	*/
d24 2
d27 1
d29 1
d294 1
a294 1
		while ((pg = RB_ROOT(&pmr->size[UVM_PMR_MEMTYPE_ZEOR]))
d528 68
@


1.6
log
@Ensure all pages in pmemrange can be marked as dirty.
It'd be a very bad idea to hand out dirty pages as zeroed, just because
we came back from hibernate.

No callers at the moment, will be called on hibernate resume path.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.5 2011/07/08 18:34:46 ariane Exp $	*/
a284 1
			uvm_pagezero(pg);
d294 1
a294 2
			for (i = 0; i < pg->fpgsz; i++) {
				uvm_pagezero(&pg[i]);
a295 1
			}
@


1.5
log
@Put in RLE logic for hibernate compressor.

These have the potential to compress 1MB of physmem into 1 byte.
This works by noting the page is not in use and therefor skipping it.

Needed by mlarkin@@ for hibernate.  No callers yet.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.4 2011/07/08 18:31:16 ariane Exp $	*/
d263 41
@


1.4
log
@Change pig allocator to a highest-address selection.
Add piglet allocator, which does a lowest-address selection.

No callers yet, needed by mlarkin@@ for hibernate voodoo.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.3 2011/07/08 18:25:56 ariane Exp $	*/
d453 33
@


1.3
log
@Move uvm_pmr_alloc_pig to kern/subr_hibernate.c

No callers, no functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.2 2011/07/08 18:20:10 ariane Exp $	*/
d267 3
a269 1
 * Allocate the biggest contig chunk of memory.
d272 1
a272 1
uvm_pmr_alloc_pig(paddr_t *addr, psize_t *sz)
d274 1
a274 1
	struct uvm_pmemrange	*pig_pmr, *pmr;
d276 5
a280 1
	int			 memtype;
d283 1
a283 1
	pig_pg = NULL;
d285 3
a287 12
		for (memtype = 0; memtype < UVM_PMR_MEMTYPE_MAX; memtype++) {
			/* Find biggest page in this memtype pmr. */
			pg = RB_MAX(uvm_pmr_size, &pmr->size[memtype]);
			if (pg == NULL)
				pg = TAILQ_FIRST(&pmr->single[memtype]);
			else
				pg--;

			if (pig_pg == NULL || (pg != NULL && pig_pg != NULL &&
			    pig_pg->fpgsz < pg->fpgsz)) {
				pig_pmr = pmr;
				pig_pg = pg;
d292 7
d300 36
a335 7
	if (pig_pg != NULL) {
		uvm_pmr_remove(pig_pmr, pig_pg);
		uvmexp.free -= pig_pg->fpgsz;
		if (pig_pg->pg_flags & PG_ZERO)
			uvmexp.zeropages -= pig_pg->fpgsz;
		*addr = VM_PAGE_TO_PHYS(pig_pg);
		*sz = pig_pg->fpgsz;
d337 2
d340 26
d367 86
a452 2
	/* Return. */
	return (pig_pg != NULL ? 0 : ENOMEM);
@


1.2
log
@Move uvm_pmr_zero_everything() to subr_hibernate.

This function will probably die before ever being called
from the in-tree code, since hibernate will move to RLE encoding.

No functional change, function had no callers.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hibernate.c,v 1.1 2011/07/08 17:58:16 ariane Exp $	*/
d264 44
@


1.1
log
@Move hiballoc to hibernate.h, subr_hibernate.c
Next few commits will move other hibernate-specific functionality,
like the pig-allocator, to subr_hibernate.

No functional change, no callers either.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_hiballoc.c,v 1.5 2011/07/06 20:47:07 miod Exp $	*/
d24 1
d225 39
@

