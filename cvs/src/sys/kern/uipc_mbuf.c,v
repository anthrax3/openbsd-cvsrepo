head	1.245;
access;
symbols
	OPENBSD_6_1:1.245.0.2
	OPENBSD_6_1_BASE:1.245
	OPENBSD_6_0:1.226.0.2
	OPENBSD_6_0_BASE:1.226
	OPENBSD_5_9:1.219.0.2
	OPENBSD_5_9_BASE:1.219
	OPENBSD_5_8:1.206.0.4
	OPENBSD_5_8_BASE:1.206
	OPENBSD_5_7:1.201.0.2
	OPENBSD_5_7_BASE:1.201
	OPENBSD_5_6:1.192.0.4
	OPENBSD_5_6_BASE:1.192
	OPENBSD_5_5:1.178.0.4
	OPENBSD_5_5_BASE:1.178
	OPENBSD_5_4:1.173.0.2
	OPENBSD_5_4_BASE:1.173
	OPENBSD_5_3:1.170.0.2
	OPENBSD_5_3_BASE:1.170
	OPENBSD_5_2:1.166.0.2
	OPENBSD_5_2_BASE:1.166
	OPENBSD_5_1_BASE:1.165
	OPENBSD_5_1:1.165.0.2
	OPENBSD_5_0:1.160.0.2
	OPENBSD_5_0_BASE:1.160
	OPENBSD_4_9:1.149.0.2
	OPENBSD_4_9_BASE:1.149
	OPENBSD_4_8:1.143.0.2
	OPENBSD_4_8_BASE:1.143
	OPENBSD_4_7:1.136.0.2
	OPENBSD_4_7_BASE:1.136
	OPENBSD_4_6:1.122.0.4
	OPENBSD_4_6_BASE:1.122
	OPENBSD_4_5:1.118.0.2
	OPENBSD_4_5_BASE:1.118
	OPENBSD_4_4:1.90.0.2
	OPENBSD_4_4_BASE:1.90
	OPENBSD_4_3:1.88.0.2
	OPENBSD_4_3_BASE:1.88
	OPENBSD_4_2:1.85.0.2
	OPENBSD_4_2_BASE:1.85
	OPENBSD_4_1:1.79.0.2
	OPENBSD_4_1_BASE:1.79
	OPENBSD_4_0:1.76.0.2
	OPENBSD_4_0_BASE:1.76
	OPENBSD_3_9:1.72.0.2
	OPENBSD_3_9_BASE:1.72
	OPENBSD_3_8:1.70.0.6
	OPENBSD_3_8_BASE:1.70
	OPENBSD_3_7:1.70.0.4
	OPENBSD_3_7_BASE:1.70
	OPENBSD_3_6:1.70.0.2
	OPENBSD_3_6_BASE:1.70
	SMP_SYNC_A:1.70
	SMP_SYNC_B:1.70
	OPENBSD_3_5:1.64.0.2
	OPENBSD_3_5_BASE:1.64
	OPENBSD_3_4:1.63.0.2
	OPENBSD_3_4_BASE:1.63
	UBC_SYNC_A:1.60
	OPENBSD_3_3:1.59.0.2
	OPENBSD_3_3_BASE:1.59
	OPENBSD_3_2:1.58.0.2
	OPENBSD_3_2_BASE:1.58
	OPENBSD_3_1:1.57.0.2
	OPENBSD_3_1_BASE:1.57
	UBC_SYNC_B:1.58
	UBC:1.44.0.2
	UBC_BASE:1.44
	OPENBSD_3_0:1.41.0.2
	OPENBSD_3_0_BASE:1.41
	OPENBSD_2_9_BASE:1.26
	OPENBSD_2_9:1.26.0.2
	OPENBSD_2_8:1.22.0.4
	OPENBSD_2_8_BASE:1.22
	OPENBSD_2_7:1.22.0.2
	OPENBSD_2_7_BASE:1.22
	SMP:1.19.0.2
	SMP_BASE:1.19
	kame_19991208:1.18
	OPENBSD_2_6:1.17.0.2
	OPENBSD_2_6_BASE:1.17
	OPENBSD_2_5:1.9.0.2
	OPENBSD_2_5_BASE:1.9
	OPENBSD_2_4:1.7.0.4
	OPENBSD_2_4_BASE:1.7
	OPENBSD_2_3:1.7.0.2
	OPENBSD_2_3_BASE:1.7
	OPENBSD_2_2:1.6.0.2
	OPENBSD_2_2_BASE:1.6
	OPENBSD_2_1:1.5.0.4
	OPENBSD_2_1_BASE:1.5
	OPENBSD_2_0:1.5.0.2
	OPENBSD_2_0_BASE:1.5
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.245
date	2017.02.07.07.00.21;	author dlg;	state Exp;
branches;
next	1.244;
commitid	WDtPJ3ZJbKf8VZTt;

1.244
date	2017.02.07.06.51.58;	author dlg;	state Exp;
branches;
next	1.243;
commitid	WWdq3lXKnxKhvKAa;

1.243
date	2017.02.07.06.27.18;	author dlg;	state Exp;
branches;
next	1.242;
commitid	sgyhXmV5BI3vhxqp;

1.242
date	2017.02.07.06.21.37;	author dlg;	state Exp;
branches;
next	1.241;
commitid	M9u3NpAWPQRI3SGw;

1.241
date	2017.02.05.16.23.38;	author jca;	state Exp;
branches;
next	1.240;
commitid	b4PzKhPOLWkSDY9t;

1.240
date	2017.01.25.09.41.45;	author mpi;	state Exp;
branches;
next	1.239;
commitid	7jImvKW180CUngtr;

1.239
date	2016.11.29.10.22.30;	author jsg;	state Exp;
branches;
next	1.238;
commitid	ZQetSMB5ilG2z10X;

1.238
date	2016.11.09.08.55.11;	author mpi;	state Exp;
branches;
next	1.237;
commitid	FmFYcVLMzdE5b7KM;

1.237
date	2016.10.27.03.29.55;	author dlg;	state Exp;
branches;
next	1.236;
commitid	DPIe5aBrzc8pHGO6;

1.236
date	2016.10.27.03.00.35;	author dlg;	state Exp;
branches;
next	1.235;
commitid	mVw8XVKgCJUdoyQ9;

1.235
date	2016.10.24.23.58.33;	author dlg;	state Exp;
branches;
next	1.234;
commitid	3TeGbJaxAzwxTMGK;

1.234
date	2016.10.24.04.38.44;	author dlg;	state Exp;
branches;
next	1.233;
commitid	FCHpeu0V7ZGrMD9k;

1.233
date	2016.10.10.00.41.17;	author dlg;	state Exp;
branches;
next	1.232;
commitid	BqSs7dIzGXp6Vk8M;

1.232
date	2016.10.10.00.34.50;	author dlg;	state Exp;
branches;
next	1.231;
commitid	0bBns6UHUViuBsDw;

1.231
date	2016.09.15.02.00.16;	author dlg;	state Exp;
branches;
next	1.230;
commitid	RlO92XR575sygHqm;

1.230
date	2016.09.15.00.52.08;	author dlg;	state Exp;
branches;
next	1.229;
commitid	3SlpRicmkMbocH8n;

1.229
date	2016.09.15.00.00.40;	author dlg;	state Exp;
branches;
next	1.228;
commitid	4cpeDghFp8TKfeYT;

1.228
date	2016.09.13.19.56.55;	author markus;	state Exp;
branches;
next	1.227;
commitid	MhzOL3PcQJk4ZhiI;

1.227
date	2016.09.03.14.17.37;	author bluhm;	state Exp;
branches;
next	1.226;
commitid	LVZeqqxKtcragV0l;

1.226
date	2016.06.13.21.24.43;	author bluhm;	state Exp;
branches;
next	1.225;
commitid	t4rQrmZwTmcYiFNW;

1.225
date	2016.05.23.15.22.44;	author tedu;	state Exp;
branches;
next	1.224;
commitid	Hsu9ZZbSw737UJHI;

1.224
date	2016.04.15.05.05.21;	author dlg;	state Exp;
branches;
next	1.223;
commitid	ZmGLFaWO5K4UN2cP;

1.223
date	2016.04.08.03.13.38;	author dlg;	state Exp;
branches;
next	1.222;
commitid	Q2yKjnl59QzPTDM9;

1.222
date	2016.04.06.01.36.06;	author dlg;	state Exp;
branches;
next	1.221;
commitid	BtLiIV04X4F3FLjx;

1.221
date	2016.03.29.10.34.42;	author sashan;	state Exp;
branches;
next	1.220;
commitid	3oT5Q5z024mhykOL;

1.220
date	2016.03.22.06.17.00;	author dlg;	state Exp;
branches;
next	1.219;
commitid	kDFWBJ8UDEZDbXyk;

1.219
date	2016.02.23.01.39.14;	author dlg;	state Exp;
branches
	1.219.2.1;
next	1.218;
commitid	jBer7f94lwmNypE0;

1.218
date	2016.01.31.00.18.07;	author sashan;	state Exp;
branches;
next	1.217;
commitid	XnM42P9Sq7TQR88T;

1.217
date	2016.01.07.22.23.13;	author sashan;	state Exp;
branches;
next	1.216;
commitid	uGcSilygSiLkUqPZ;

1.216
date	2015.12.23.21.04.55;	author jasper;	state Exp;
branches;
next	1.215;
commitid	fJOVhuW2zBjxPtee;

1.215
date	2015.12.22.13.33.26;	author sashan;	state Exp;
branches;
next	1.214;
commitid	oBRhtWcDV0ThviVT;

1.214
date	2015.11.21.11.46.24;	author mpi;	state Exp;
branches;
next	1.213;
commitid	R3ua3lEs0X7VVO7b;

1.213
date	2015.11.13.10.12.39;	author mpi;	state Exp;
branches;
next	1.212;
commitid	BHDkuvEKkivQfKxx;

1.212
date	2015.11.12.10.07.14;	author mpi;	state Exp;
branches;
next	1.211;
commitid	DLHQ4NjCPIqpY9kD;

1.211
date	2015.11.02.09.21.48;	author dlg;	state Exp;
branches;
next	1.210;
commitid	PesTrvy25aL3WZ7G;

1.210
date	2015.10.30.19.47.40;	author bluhm;	state Exp;
branches;
next	1.209;
commitid	yzL4UuVOqvvNd4tp;

1.209
date	2015.10.30.12.54.36;	author reyk;	state Exp;
branches;
next	1.208;
commitid	suP3B6WRHhMgnRWI;

1.208
date	2015.10.22.05.26.06;	author dlg;	state Exp;
branches;
next	1.207;
commitid	Z4k0lD49Bt5dFQva;

1.207
date	2015.08.14.05.25.29;	author dlg;	state Exp;
branches;
next	1.206;
commitid	zw6gETat7LvrmWZO;

1.206
date	2015.07.15.22.29.32;	author deraadt;	state Exp;
branches;
next	1.205;
commitid	3Sd4TGrB1NgqrNaE;

1.205
date	2015.06.16.11.09.39;	author mpi;	state Exp;
branches;
next	1.204;
commitid	h7z8lokZ0dFyuWpg;

1.204
date	2015.05.31.20.10.44;	author bluhm;	state Exp;
branches;
next	1.203;
commitid	6yPTtylK2epwVf5X;

1.203
date	2015.04.13.08.45.48;	author mpi;	state Exp;
branches;
next	1.202;
commitid	aiRvgNOa4qke9vft;

1.202
date	2015.03.14.03.38.51;	author jsg;	state Exp;
branches;
next	1.201;
commitid	p4LJxGKbi0BU2cG6;

1.201
date	2015.02.07.02.52.09;	author dlg;	state Exp;
branches;
next	1.200;
commitid	TaNzeFJ7ubLcwrhA;

1.200
date	2015.02.07.02.30.00;	author dlg;	state Exp;
branches;
next	1.199;
commitid	6DIoqO063ylosXnd;

1.199
date	2014.12.11.19.21.57;	author tedu;	state Exp;
branches;
next	1.198;
commitid	KtikWduHBwfG1emb;

1.198
date	2014.11.05.00.28.15;	author dlg;	state Exp;
branches;
next	1.197;
commitid	ODy7q1XP45abHYPs;

1.197
date	2014.10.03.02.16.21;	author dlg;	state Exp;
branches;
next	1.196;
commitid	CN7rhvvNUpU8DtlN;

1.196
date	2014.10.03.01.11.17;	author dlg;	state Exp;
branches;
next	1.195;
commitid	79RygqGLpgNGlnTn;

1.195
date	2014.10.03.01.02.47;	author dlg;	state Exp;
branches;
next	1.194;
commitid	dj3z1FWbO3vYKmfh;

1.194
date	2014.09.14.14.17.26;	author jsg;	state Exp;
branches;
next	1.193;
commitid	uzzBR7hz9ncd4O6G;

1.193
date	2014.08.18.04.06.16;	author dlg;	state Exp;
branches;
next	1.192;
commitid	EElxWqZWwFLxGTC9;

1.192
date	2014.07.13.15.52.38;	author tedu;	state Exp;
branches;
next	1.191;
commitid	iyde0xIVfkKugN9I;

1.191
date	2014.07.13.09.52.48;	author dlg;	state Exp;
branches;
next	1.190;
commitid	EPHrRog2Cgp9jFyM;

1.190
date	2014.07.09.13.05.45;	author dlg;	state Exp;
branches;
next	1.189;
commitid	4rfOQH2rwDpu2Cn0;

1.189
date	2014.07.09.11.22.53;	author dlg;	state Exp;
branches;
next	1.188;
commitid	3BuBUtq7KUGR1P50;

1.188
date	2014.07.08.07.10.12;	author dlg;	state Exp;
branches;
next	1.187;
commitid	LjJqbuphCuSi7amX;

1.187
date	2014.07.08.05.35.19;	author dlg;	state Exp;
branches;
next	1.186;
commitid	0QJleeeWqZmC5anF;

1.186
date	2014.06.18.11.09.58;	author dlg;	state Exp;
branches;
next	1.185;
commitid	C6I3EmscplZbvlKr;

1.185
date	2014.06.13.07.28.13;	author mpi;	state Exp;
branches;
next	1.184;
commitid	LDGswZYWsVqKEviC;

1.184
date	2014.05.04.19.27.08;	author sf;	state Exp;
branches;
next	1.183;

1.183
date	2014.04.22.14.41.03;	author mpi;	state Exp;
branches;
next	1.182;

1.182
date	2014.04.21.11.10.54;	author henning;	state Exp;
branches;
next	1.181;

1.181
date	2014.04.14.09.06.41;	author mpi;	state Exp;
branches;
next	1.180;

1.180
date	2014.03.28.17.57.11;	author mpi;	state Exp;
branches;
next	1.179;

1.179
date	2014.03.27.10.30.58;	author mpi;	state Exp;
branches;
next	1.178;

1.178
date	2014.01.19.03.04.54;	author claudio;	state Exp;
branches;
next	1.177;

1.177
date	2014.01.10.00.47.17;	author bluhm;	state Exp;
branches;
next	1.176;

1.176
date	2013.11.09.06.38.42;	author dlg;	state Exp;
branches;
next	1.175;

1.175
date	2013.08.21.05.21.45;	author dlg;	state Exp;
branches;
next	1.174;

1.174
date	2013.08.08.23.25.06;	author syl;	state Exp;
branches;
next	1.173;

1.173
date	2013.06.11.13.29.50;	author dlg;	state Exp;
branches;
next	1.172;

1.172
date	2013.06.11.01.01.15;	author dlg;	state Exp;
branches;
next	1.171;

1.171
date	2013.03.28.16.55.25;	author deraadt;	state Exp;
branches;
next	1.170;

1.170
date	2013.02.17.17.39.29;	author miod;	state Exp;
branches;
next	1.169;

1.169
date	2013.02.09.20.56.35;	author miod;	state Exp;
branches;
next	1.168;

1.168
date	2013.02.07.11.06.42;	author mikeb;	state Exp;
branches;
next	1.167;

1.167
date	2012.09.26.14.53.23;	author markus;	state Exp;
branches;
next	1.166;

1.166
date	2012.04.13.09.38.32;	author deraadt;	state Exp;
branches;
next	1.165;

1.165
date	2011.12.02.10.55.46;	author dlg;	state Exp;
branches;
next	1.164;

1.164
date	2011.11.30.10.26.56;	author dlg;	state Exp;
branches;
next	1.163;

1.163
date	2011.11.30.01.16.09;	author dlg;	state Exp;
branches;
next	1.162;

1.162
date	2011.11.29.10.39.11;	author dlg;	state Exp;
branches;
next	1.161;

1.161
date	2011.09.18.23.20.38;	author miod;	state Exp;
branches;
next	1.160;

1.160
date	2011.07.08.18.48.50;	author henning;	state Exp;
branches;
next	1.159;

1.159
date	2011.07.05.05.53.17;	author claudio;	state Exp;
branches;
next	1.158;

1.158
date	2011.06.23.21.42.05;	author ariane;	state Exp;
branches;
next	1.157;

1.157
date	2011.05.04.16.05.49;	author blambert;	state Exp;
branches;
next	1.156;

1.156
date	2011.04.18.19.23.46;	author art;	state Exp;
branches;
next	1.155;

1.155
date	2011.04.11.13.10.13;	author claudio;	state Exp;
branches;
next	1.154;

1.154
date	2011.04.10.23.25.02;	author bluhm;	state Exp;
branches;
next	1.153;

1.153
date	2011.04.06.15.52.13;	author art;	state Exp;
branches;
next	1.152;

1.152
date	2011.04.05.11.48.28;	author blambert;	state Exp;
branches;
next	1.151;

1.151
date	2011.04.05.01.28.05;	author art;	state Exp;
branches;
next	1.150;

1.150
date	2011.04.04.21.33.27;	author blambert;	state Exp;
branches;
next	1.149;

1.149
date	2011.01.29.13.15.39;	author bluhm;	state Exp;
branches;
next	1.148;

1.148
date	2010.12.21.14.00.43;	author claudio;	state Exp;
branches;
next	1.147;

1.147
date	2010.11.05.15.17.50;	author claudio;	state Exp;
branches;
next	1.146;

1.146
date	2010.10.28.16.28.56;	author claudio;	state Exp;
branches;
next	1.145;

1.145
date	2010.10.05.13.29.40;	author mikeb;	state Exp;
branches;
next	1.144;

1.144
date	2010.09.23.10.49.55;	author dlg;	state Exp;
branches;
next	1.143;

1.143
date	2010.07.15.09.45.09;	author claudio;	state Exp;
branches;
next	1.142;

1.142
date	2010.07.14.10.31.54;	author matthew;	state Exp;
branches;
next	1.141;

1.141
date	2010.07.03.03.33.16;	author tedu;	state Exp;
branches;
next	1.140;

1.140
date	2010.07.02.02.40.16;	author blambert;	state Exp;
branches;
next	1.139;

1.139
date	2010.07.01.19.23.51;	author beck;	state Exp;
branches;
next	1.138;

1.138
date	2010.06.27.03.03.48;	author thib;	state Exp;
branches;
next	1.137;

1.137
date	2010.06.07.19.47.25;	author blambert;	state Exp;
branches;
next	1.136;

1.136
date	2010.01.14.23.12.11;	author schwarze;	state Exp;
branches;
next	1.135;

1.135
date	2010.01.12.04.05.45;	author deraadt;	state Exp;
branches;
next	1.134;

1.134
date	2009.09.13.14.42.52;	author krw;	state Exp;
branches;
next	1.133;

1.133
date	2009.08.12.21.44.49;	author henning;	state Exp;
branches;
next	1.132;

1.132
date	2009.08.12.20.02.42;	author dlg;	state Exp;
branches;
next	1.131;

1.131
date	2009.08.12.14.39.05;	author dlg;	state Exp;
branches;
next	1.130;

1.130
date	2009.08.11.11.53.19;	author deraadt;	state Exp;
branches;
next	1.129;

1.129
date	2009.08.11.10.48.39;	author deraadt;	state Exp;
branches;
next	1.128;

1.128
date	2009.08.09.21.08.30;	author deraadt;	state Exp;
branches;
next	1.127;

1.127
date	2009.08.09.16.19.08;	author deraadt;	state Exp;
branches;
next	1.126;

1.126
date	2009.08.09.12.50.09;	author henning;	state Exp;
branches;
next	1.125;

1.125
date	2009.08.09.12.42.11;	author deraadt;	state Exp;
branches;
next	1.124;

1.124
date	2009.08.09.12.24.40;	author deraadt;	state Exp;
branches;
next	1.123;

1.123
date	2009.08.09.11.40.58;	author deraadt;	state Exp;
branches;
next	1.122;

1.122
date	2009.06.22.10.51.06;	author thib;	state Exp;
branches;
next	1.121;

1.121
date	2009.06.05.00.05.21;	author claudio;	state Exp;
branches;
next	1.120;

1.120
date	2009.06.02.00.05.13;	author blambert;	state Exp;
branches;
next	1.119;

1.119
date	2009.03.02.23.52.18;	author dlg;	state Exp;
branches;
next	1.118;

1.118
date	2009.02.09.21.36.10;	author claudio;	state Exp;
branches;
next	1.117;

1.117
date	2009.02.04.20.02.11;	author claudio;	state Exp;
branches;
next	1.116;

1.116
date	2009.01.27.09.17.51;	author dlg;	state Exp;
branches;
next	1.115;

1.115
date	2009.01.26.15.16.39;	author claudio;	state Exp;
branches;
next	1.114;

1.114
date	2008.12.23.01.06.35;	author deraadt;	state Exp;
branches;
next	1.113;

1.113
date	2008.12.22.18.35.52;	author claudio;	state Exp;
branches;
next	1.112;

1.112
date	2008.12.20.22.27.38;	author deraadt;	state Exp;
branches;
next	1.111;

1.111
date	2008.12.14.22.31.46;	author kettenis;	state Exp;
branches;
next	1.110;

1.110
date	2008.12.13.00.18.46;	author deraadt;	state Exp;
branches;
next	1.109;

1.109
date	2008.12.11.16.45.45;	author deraadt;	state Exp;
branches;
next	1.108;

1.108
date	2008.12.04.23.40.44;	author dlg;	state Exp;
branches;
next	1.107;

1.107
date	2008.11.29.19.57.09;	author deraadt;	state Exp;
branches;
next	1.106;

1.106
date	2008.11.26.23.47.14;	author claudio;	state Exp;
branches;
next	1.105;

1.105
date	2008.11.26.22.56.07;	author claudio;	state Exp;
branches;
next	1.104;

1.104
date	2008.11.26.21.39.57;	author deraadt;	state Exp;
branches;
next	1.103;

1.103
date	2008.11.25.19.09.34;	author claudio;	state Exp;
branches;
next	1.102;

1.102
date	2008.11.25.17.01.14;	author dlg;	state Exp;
branches;
next	1.101;

1.101
date	2008.11.25.15.43.32;	author dlg;	state Exp;
branches;
next	1.100;

1.100
date	2008.11.25.12.47.00;	author deraadt;	state Exp;
branches;
next	1.99;

1.99
date	2008.11.25.12.07.55;	author claudio;	state Exp;
branches;
next	1.98;

1.98
date	2008.11.24.19.17.16;	author dlg;	state Exp;
branches;
next	1.97;

1.97
date	2008.11.24.18.34.40;	author dlg;	state Exp;
branches;
next	1.96;

1.96
date	2008.11.24.14.26.54;	author deraadt;	state Exp;
branches;
next	1.95;

1.95
date	2008.11.24.12.57.37;	author dlg;	state Exp;
branches;
next	1.94;

1.94
date	2008.10.14.18.01.53;	author naddy;	state Exp;
branches;
next	1.93;

1.93
date	2008.09.28.14.08.51;	author naddy;	state Exp;
branches;
next	1.92;

1.92
date	2008.08.14.19.39.40;	author claudio;	state Exp;
branches;
next	1.91;

1.91
date	2008.08.08.08.54.08;	author thib;	state Exp;
branches;
next	1.90;

1.90
date	2008.06.11.02.46.34;	author henning;	state Exp;
branches;
next	1.89;

1.89
date	2008.05.06.02.16.26;	author krw;	state Exp;
branches;
next	1.88;

1.88
date	2008.01.16.19.28.23;	author thib;	state Exp;
branches;
next	1.87;

1.87
date	2007.11.27.16.38.50;	author tedu;	state Exp;
branches;
next	1.86;

1.86
date	2007.09.26.13.05.52;	author henning;	state Exp;
branches;
next	1.85;

1.85
date	2007.07.20.09.59.19;	author claudio;	state Exp;
branches;
next	1.84;

1.84
date	2007.06.02.09.45.32;	author art;	state Exp;
branches;
next	1.83;

1.83
date	2007.05.28.19.20.14;	author tedu;	state Exp;
branches;
next	1.82;

1.82
date	2007.05.28.17.16.39;	author henning;	state Exp;
branches;
next	1.81;

1.81
date	2007.05.27.20.54.25;	author claudio;	state Exp;
branches;
next	1.80;

1.80
date	2007.03.15.11.48.09;	author claudio;	state Exp;
branches;
next	1.79;

1.79
date	2006.12.29.13.04.37;	author pedro;	state Exp;
branches;
next	1.78;

1.78
date	2006.11.29.12.39.48;	author miod;	state Exp;
branches;
next	1.77;

1.77
date	2006.10.11.22.39.46;	author mpf;	state Exp;
branches;
next	1.76;

1.76
date	2006.07.14.01.58.58;	author pedro;	state Exp;
branches;
next	1.75;

1.75
date	2006.05.07.20.06.50;	author tedu;	state Exp;
branches;
next	1.74;

1.74
date	2006.03.17.04.21.57;	author brad;	state Exp;
branches;
next	1.73;

1.73
date	2006.03.05.00.44.25;	author brad;	state Exp;
branches;
next	1.72;

1.72
date	2006.01.05.05.05.06;	author jsg;	state Exp;
branches;
next	1.71;

1.71
date	2005.12.31.19.18.05;	author krw;	state Exp;
branches;
next	1.70;

1.70
date	2004.05.27.04.55.28;	author tedu;	state Exp;
branches;
next	1.69;

1.69
date	2004.05.23.19.41.23;	author tedu;	state Exp;
branches;
next	1.68;

1.68
date	2004.05.23.19.37.24;	author tedu;	state Exp;
branches;
next	1.67;

1.67
date	2004.04.19.22.52.33;	author tedu;	state Exp;
branches;
next	1.66;

1.66
date	2004.04.17.10.18.12;	author mcbride;	state Exp;
branches;
next	1.65;

1.65
date	2004.04.01.23.56.05;	author tedu;	state Exp;
branches;
next	1.64;

1.64
date	2004.01.28.20.19.24;	author dhartmei;	state Exp;
branches;
next	1.63;

1.63
date	2003.08.12.05.09.18;	author mickey;	state Exp;
branches;
next	1.62;

1.62
date	2003.06.02.23.28.06;	author millert;	state Exp;
branches;
next	1.61;

1.61
date	2003.06.01.16.23.41;	author art;	state Exp;
branches;
next	1.60;

1.60
date	2003.04.23.01.36.52;	author jason;	state Exp;
branches;
next	1.59;

1.59
date	2003.02.12.14.41.07;	author jason;	state Exp;
branches;
next	1.58;

1.58
date	2002.07.03.21.19.08;	author miod;	state Exp;
branches;
next	1.57;

1.57
date	2002.03.14.01.27.05;	author millert;	state Exp;
branches;
next	1.56;

1.56
date	2002.02.25.04.53.16;	author dhartmei;	state Exp;
branches;
next	1.55;

1.55
date	2002.02.17.22.59.53;	author maja;	state Exp;
branches;
next	1.54;

1.54
date	2002.02.05.22.06.43;	author angelos;	state Exp;
branches;
next	1.53;

1.53
date	2002.02.05.21.59.18;	author angelos;	state Exp;
branches;
next	1.52;

1.52
date	2002.02.05.21.47.59;	author angelos;	state Exp;
branches;
next	1.51;

1.51
date	2002.02.04.21.44.16;	author angelos;	state Exp;
branches;
next	1.50;

1.50
date	2002.02.04.20.50.42;	author jason;	state Exp;
branches;
next	1.49;

1.49
date	2002.01.25.15.50.22;	author art;	state Exp;
branches;
next	1.48;

1.48
date	2002.01.23.17.51.52;	author art;	state Exp;
branches;
next	1.47;

1.47
date	2002.01.23.17.35.57;	author art;	state Exp;
branches;
next	1.46;

1.46
date	2002.01.23.00.39.47;	author art;	state Exp;
branches;
next	1.45;

1.45
date	2002.01.16.20.50.17;	author miod;	state Exp;
branches;
next	1.44;

1.44
date	2001.12.18.23.07.49;	author deraadt;	state Exp;
branches
	1.44.2.1;
next	1.43;

1.43
date	2001.11.28.16.13.29;	author art;	state Exp;
branches;
next	1.42;

1.42
date	2001.11.06.19.53.20;	author miod;	state Exp;
branches;
next	1.41;

1.41
date	2001.09.12.00.23.33;	author art;	state Exp;
branches;
next	1.40;

1.40
date	2001.06.27.04.49.47;	author art;	state Exp;
branches;
next	1.39;

1.39
date	2001.06.27.03.53.50;	author angelos;	state Exp;
branches;
next	1.38;

1.38
date	2001.06.27.03.49.52;	author angelos;	state Exp;
branches;
next	1.37;

1.37
date	2001.06.26.06.27.40;	author aaron;	state Exp;
branches;
next	1.36;

1.36
date	2001.06.25.02.52.18;	author angelos;	state Exp;
branches;
next	1.35;

1.35
date	2001.06.25.01.50.16;	author fgsch;	state Exp;
branches;
next	1.34;

1.34
date	2001.05.26.06.59.14;	author angelos;	state Exp;
branches;
next	1.33;

1.33
date	2001.05.26.05.46.33;	author angelos;	state Exp;
branches;
next	1.32;

1.32
date	2001.05.24.10.59.23;	author angelos;	state Exp;
branches;
next	1.31;

1.31
date	2001.05.20.08.31.46;	author angelos;	state Exp;
branches;
next	1.30;

1.30
date	2001.05.18.23.29.33;	author millert;	state Exp;
branches;
next	1.29;

1.29
date	2001.05.17.18.41.44;	author provos;	state Exp;
branches;
next	1.28;

1.28
date	2001.05.16.08.59.04;	author art;	state Exp;
branches;
next	1.27;

1.27
date	2001.05.05.20.57.00;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2001.04.05.10.52.45;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2001.03.30.19.24.35;	author angelos;	state Exp;
branches;
next	1.24;

1.24
date	2001.03.28.20.03.00;	author angelos;	state Exp;
branches;
next	1.23;

1.23
date	2001.03.25.07.07.57;	author csapuntz;	state Exp;
branches;
next	1.22;

1.22
date	2000.03.03.11.15.43;	author angelos;	state Exp;
branches;
next	1.21;

1.21
date	2000.03.03.06.19.22;	author angelos;	state Exp;
branches;
next	1.20;

1.20
date	2000.03.02.21.40.49;	author angelos;	state Exp;
branches;
next	1.19;

1.19
date	99.12.31.23.37.08;	author provos;	state Exp;
branches
	1.19.2.1;
next	1.18;

1.18
date	99.12.05.07.30.31;	author angelos;	state Exp;
branches;
next	1.17;

1.17
date	99.10.01.02.00.12;	author jason;	state Exp;
branches;
next	1.16;

1.16
date	99.09.12.11.46.53;	author niklas;	state Exp;
branches;
next	1.15;

1.15
date	99.08.17.12.31.22;	author millert;	state Exp;
branches;
next	1.14;

1.14
date	99.08.09.21.41.51;	author deraadt;	state Exp;
branches;
next	1.13;

1.13
date	99.07.02.01.02.53;	author cmetz;	state Exp;
branches;
next	1.12;

1.12
date	99.05.14.02.12.29;	author cmetz;	state Exp;
branches;
next	1.11;

1.11
date	99.05.14.02.05.42;	author cmetz;	state Exp;
branches;
next	1.10;

1.10
date	99.05.12.21.11.40;	author ho;	state Exp;
branches;
next	1.9;

1.9
date	99.02.26.04.49.07;	author art;	state Exp;
branches;
next	1.8;

1.8
date	99.01.07.22.28.01;	author deraadt;	state Exp;
branches;
next	1.7;

1.7
date	98.02.03.19.06.27;	author deraadt;	state Exp;
branches;
next	1.6;

1.6
date	97.08.20.05.42.26;	author millert;	state Exp;
branches;
next	1.5;

1.5
date	96.09.06.07.21.41;	author niklas;	state Exp;
branches;
next	1.4;

1.4
date	96.09.02.18.14.15;	author dm;	state Exp;
branches;
next	1.3;

1.3
date	96.06.20.10.50.22;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	96.03.03.17.20.17;	author niklas;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.52.46;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.52.46;	author deraadt;	state Exp;
branches;
next	;

1.19.2.1
date	2000.03.24.09.09.26;	author niklas;	state Exp;
branches;
next	1.19.2.2;

1.19.2.2
date	2001.05.14.22.32.45;	author niklas;	state Exp;
branches;
next	1.19.2.3;

1.19.2.3
date	2001.07.04.10.48.43;	author niklas;	state Exp;
branches;
next	1.19.2.4;

1.19.2.4
date	2001.10.31.03.26.29;	author nate;	state Exp;
branches;
next	1.19.2.5;

1.19.2.5
date	2001.11.13.23.04.23;	author niklas;	state Exp;
branches;
next	1.19.2.6;

1.19.2.6
date	2001.12.05.01.02.39;	author niklas;	state Exp;
branches;
next	1.19.2.7;

1.19.2.7
date	2002.03.06.02.13.23;	author niklas;	state Exp;
branches;
next	1.19.2.8;

1.19.2.8
date	2002.03.28.11.43.04;	author niklas;	state Exp;
branches;
next	1.19.2.9;

1.19.2.9
date	2003.03.28.00.41.27;	author niklas;	state Exp;
branches;
next	1.19.2.10;

1.19.2.10
date	2003.05.13.19.21.28;	author ho;	state Exp;
branches;
next	1.19.2.11;

1.19.2.11
date	2003.06.07.11.03.40;	author ho;	state Exp;
branches;
next	1.19.2.12;

1.19.2.12
date	2004.02.19.10.56.38;	author niklas;	state Exp;
branches;
next	1.19.2.13;

1.19.2.13
date	2004.06.05.23.13.02;	author niklas;	state Exp;
branches;
next	;

1.44.2.1
date	2002.01.31.22.55.41;	author niklas;	state Exp;
branches;
next	1.44.2.2;

1.44.2.2
date	2002.02.02.03.28.25;	author art;	state Exp;
branches;
next	1.44.2.3;

1.44.2.3
date	2002.06.11.03.29.40;	author art;	state Exp;
branches;
next	1.44.2.4;

1.44.2.4
date	2002.10.29.00.36.44;	author art;	state Exp;
branches;
next	1.44.2.5;

1.44.2.5
date	2003.05.19.22.31.57;	author tedu;	state Exp;
branches;
next	;

1.219.2.1
date	2016.04.28.22.31.55;	author sthen;	state Exp;
branches;
next	1.219.2.2;
commitid	2QG7G72vD4WX99Hw;

1.219.2.2
date	2016.07.14.02.56.15;	author tedu;	state Exp;
branches;
next	;
commitid	0mSmzl4NTWthAxT7;


desc
@@


1.245
log
@enable per cpu caches on the mbuf pools.

this didnt make sense previously since the mbuf pools had item
limits that meant the cpus had to coordinate via a single counter
to make sure the limit wasnt exceeded.

mbufs are now limited by how much memory can be allocated for pages
from the system. individual pool items are no longer counted and
therefore do not have to be coordinated.

ok bluhm@@ as part of a larger diff.
@
text
@/*	$OpenBSD: uipc_mbuf.c,v 1.244 2017/02/07 06:51:58 dlg Exp $	*/
/*	$NetBSD: uipc_mbuf.c,v 1.15.4.1 1996/06/13 17:11:44 cgd Exp $	*/

/*
 * Copyright (c) 1982, 1986, 1988, 1991, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)uipc_mbuf.c	8.2 (Berkeley) 1/4/94
 */

/*
 *	@@(#)COPYRIGHT	1.1 (NRL) 17 January 1995
 * 
 * NRL grants permission for redistribution and use in source and binary
 * forms, with or without modification, of the software and documentation
 * created at NRL provided that the following conditions are met:
 * 
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgements:
 * 	This product includes software developed by the University of
 * 	California, Berkeley and its contributors.
 * 	This product includes software developed at the Information
 * 	Technology Division, US Naval Research Laboratory.
 * 4. Neither the name of the NRL nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 * 
 * THE SOFTWARE PROVIDED BY NRL IS PROVIDED BY NRL AND CONTRIBUTORS ``AS
 * IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A
 * PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL NRL OR
 * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
 * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 * 
 * The views and conclusions contained in the software and documentation
 * are those of the authors and should not be interpreted as representing
 * official policies, either expressed or implied, of the US Naval
 * Research Laboratory (NRL).
 */

#include "pf.h"

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/malloc.h>
#include <sys/mbuf.h>
#include <sys/kernel.h>
#include <sys/syslog.h>
#include <sys/domain.h>
#include <sys/protosw.h>
#include <sys/pool.h>
#include <sys/percpu.h>

#include <sys/socket.h>
#include <sys/socketvar.h>
#include <net/if.h>


#include <uvm/uvm_extern.h>

#ifdef DDB
#include <machine/db_machdep.h>
#endif

#if NPF > 0
#include <net/pfvar.h>
#endif	/* NPF > 0 */

/* mbuf stats */
COUNTERS_BOOT_MEMORY(mbstat_boot, MBSTAT_COUNT);
struct cpumem *mbstat = COUNTERS_BOOT_INITIALIZER(mbstat_boot);
/* mbuf pools */
struct	pool mbpool;
struct	pool mtagpool;

/* mbuf cluster pools */
u_int	mclsizes[MCLPOOLS] = {
	MCLBYTES,	/* must be at slot 0 */
	MCLBYTES + 2,	/* ETHER_ALIGNED 2k mbufs */
	4 * 1024,
	8 * 1024,
	9 * 1024,
	12 * 1024,
	16 * 1024,
	64 * 1024
};
static	char mclnames[MCLPOOLS][8];
struct	pool mclpools[MCLPOOLS];

struct pool *m_clpool(u_int);

int max_linkhdr;		/* largest link-level header */
int max_protohdr;		/* largest protocol header */
int max_hdr;			/* largest link+protocol header */

struct	mutex m_extref_mtx = MUTEX_INITIALIZER(IPL_NET);

void	m_extfree(struct mbuf *);
void	nmbclust_update(void);
void	m_zero(struct mbuf *);

struct mutex m_pool_mtx = MUTEX_INITIALIZER(IPL_NET);
unsigned int mbuf_mem_limit; /* how much memory can be allocated */
unsigned int mbuf_mem_alloc; /* how much memory has been allocated */

void	*m_pool_alloc(struct pool *, int, int *);
void	m_pool_free(struct pool *, void *);

struct pool_allocator m_pool_allocator = {
	m_pool_alloc,
	m_pool_free,
	0 /* will be copied from pool_allocator_multi */
};

static void (*mextfree_fns[4])(caddr_t, u_int, void *);
static u_int num_extfree_fns;

const char *mclpool_warnmsg =
    "WARNING: mclpools limit reached; increase kern.maxclusters";

/*
 * Initialize the mbuf allocator.
 */
void
mbinit(void)
{
	int i;
	unsigned int lowbits;

	m_pool_allocator.pa_pagesz = pool_allocator_multi.pa_pagesz;

	nmbclust_update();
	mbuf_mem_alloc = 0;

#if DIAGNOSTIC
	if (mclsizes[0] != MCLBYTES)
		panic("mbinit: the smallest cluster size != MCLBYTES");
	if (mclsizes[nitems(mclsizes) - 1] != MAXMCLBYTES)
		panic("mbinit: the largest cluster size != MAXMCLBYTES");
#endif

	m_pool_init(&mbpool, MSIZE, 64, "mbufpl");

	pool_init(&mtagpool, PACKET_TAG_MAXSIZE + sizeof(struct m_tag), 0,
	    IPL_NET, 0, "mtagpl", NULL);

	for (i = 0; i < nitems(mclsizes); i++) {
		lowbits = mclsizes[i] & ((1 << 10) - 1);
		if (lowbits) {
			snprintf(mclnames[i], sizeof(mclnames[0]),
			    "mcl%dk%u", mclsizes[i] >> 10, lowbits);
		} else {
			snprintf(mclnames[i], sizeof(mclnames[0]), "mcl%dk",
			    mclsizes[i] >> 10);
		}

		m_pool_init(&mclpools[i], mclsizes[i], 64, mclnames[i]);
	}

	(void)mextfree_register(m_extfree_pool);
	KASSERT(num_extfree_fns == 1);
}

void
mbcpuinit()
{
	int i;

	mbstat = counters_alloc_ncpus(mbstat, MBSTAT_COUNT);

	pool_cache_init(&mbpool);
	pool_cache_init(&mtagpool);

	for (i = 0; i < nitems(mclsizes); i++)
		pool_cache_init(&mclpools[i]);
}

void
nmbclust_update(void)
{
	/* update the global mbuf memory limit */
	mbuf_mem_limit = nmbclust * MCLBYTES;
}

/*
 * Space allocation routines.
 */
struct mbuf *
m_get(int nowait, int type)
{
	struct mbuf *m;
	struct counters_ref cr;
	uint64_t *counters;
	int s;

	KDASSERT(type < MT_NTYPES);

	m = pool_get(&mbpool, nowait == M_WAIT ? PR_WAITOK : PR_NOWAIT);
	if (m == NULL)
		return (NULL);

	s = splnet();
	counters = counters_enter(&cr, mbstat);
	counters[type]++;
	counters_leave(&cr, mbstat);
	splx(s);

	m->m_type = type;
	m->m_next = NULL;
	m->m_nextpkt = NULL;
	m->m_data = m->m_dat;
	m->m_flags = 0;

	return (m);
}

/*
 * ATTN: When changing anything here check m_inithdr() and m_defrag() those
 * may need to change as well.
 */
struct mbuf *
m_gethdr(int nowait, int type)
{
	struct mbuf *m;
	struct counters_ref cr;
	uint64_t *counters;
	int s;

	KDASSERT(type < MT_NTYPES);

	m = pool_get(&mbpool, nowait == M_WAIT ? PR_WAITOK : PR_NOWAIT);
	if (m == NULL)
		return (NULL);

	s = splnet();
	counters = counters_enter(&cr, mbstat);
	counters[type]++;
	counters_leave(&cr, mbstat);
	splx(s);

	m->m_type = type;

	return (m_inithdr(m));
}

struct mbuf *
m_inithdr(struct mbuf *m)
{
	/* keep in sync with m_gethdr */
	m->m_next = NULL;
	m->m_nextpkt = NULL;
	m->m_data = m->m_pktdat;
	m->m_flags = M_PKTHDR;
	memset(&m->m_pkthdr, 0, sizeof(m->m_pkthdr));
	m->m_pkthdr.pf.prio = IFQ_DEFPRIO;

	return (m);
}

void
m_resethdr(struct mbuf *m)
{
	int len = m->m_pkthdr.len;
	u_int8_t loopcnt = m->m_pkthdr.ph_loopcnt;

	KASSERT(m->m_flags & M_PKTHDR);
	m->m_flags &= (M_EXT|M_PKTHDR|M_EOR|M_EXTWR|M_ZEROIZE);

	/* delete all mbuf tags to reset the state */
	m_tag_delete_chain(m);

#if NPF > 0
	pf_pkt_unlink_state_key(m);
#endif	/* NPF > 0 */

	/* like m_inithdr(), but keep any associated data and mbufs */
	memset(&m->m_pkthdr, 0, sizeof(m->m_pkthdr));
	m->m_pkthdr.pf.prio = IFQ_DEFPRIO;
	m->m_pkthdr.len = len;
	m->m_pkthdr.ph_loopcnt = loopcnt;
}

struct mbuf *
m_getclr(int nowait, int type)
{
	struct mbuf *m;

	MGET(m, nowait, type);
	if (m == NULL)
		return (NULL);
	memset(mtod(m, caddr_t), 0, MLEN);
	return (m);
}

struct pool *
m_clpool(u_int pktlen)
{
	struct pool *pp;
	int pi;

	for (pi = 0; pi < nitems(mclpools); pi++) {
		pp = &mclpools[pi];
		if (pktlen <= pp->pr_size)
			return (pp);
	}

	return (NULL);
}

struct mbuf *
m_clget(struct mbuf *m, int how, u_int pktlen)
{
	struct mbuf *m0 = NULL;
	struct pool *pp;
	caddr_t buf;

	pp = m_clpool(pktlen);
#ifdef DIAGNOSTIC
	if (pp == NULL)
		panic("m_clget: request for %u byte cluster", pktlen);
#endif

	if (m == NULL) {
		m0 = m_gethdr(how, MT_DATA);
		if (m0 == NULL)
			return (NULL);

		m = m0;
	}
	buf = pool_get(pp, how == M_WAIT ? PR_WAITOK : PR_NOWAIT);
	if (buf == NULL) {
		m_freem(m0);
		return (NULL);
	}

	MEXTADD(m, buf, pp->pr_size, M_EXTWR, MEXTFREE_POOL, pp);
	return (m);
}

void
m_extfree_pool(caddr_t buf, u_int size, void *pp)
{
	pool_put(pp, buf);
}

struct mbuf *
m_free(struct mbuf *m)
{
	struct mbuf *n;
	struct counters_ref cr;
	uint64_t *counters;
	int s;

	if (m == NULL)
		return (NULL);

	s = splnet();
	counters = counters_enter(&cr, mbstat);
	counters[m->m_type]--;
	counters_leave(&cr, mbstat);
	splx(s);

	n = m->m_next;
	if (m->m_flags & M_ZEROIZE) {
		m_zero(m);
		/* propagate M_ZEROIZE to the next mbuf in the chain */
		if (n)
			n->m_flags |= M_ZEROIZE;
	}
	if (m->m_flags & M_PKTHDR) {
		m_tag_delete_chain(m);
#if NPF > 0
		pf_pkt_unlink_state_key(m);
#endif	/* NPF > 0 */
	}
	if (m->m_flags & M_EXT)
		m_extfree(m);

	pool_put(&mbpool, m);

	return (n);
}

void
m_extref(struct mbuf *o, struct mbuf *n)
{
	int refs = MCLISREFERENCED(o);

	n->m_flags |= o->m_flags & (M_EXT|M_EXTWR);

	if (refs)
		mtx_enter(&m_extref_mtx);
	n->m_ext.ext_nextref = o->m_ext.ext_nextref;
	n->m_ext.ext_prevref = o;
	o->m_ext.ext_nextref = n;
	n->m_ext.ext_nextref->m_ext.ext_prevref = n;
	if (refs)
		mtx_leave(&m_extref_mtx);

	MCLREFDEBUGN((n), __FILE__, __LINE__);
}

static inline u_int
m_extunref(struct mbuf *m)
{
	int refs = 1;

	if (!MCLISREFERENCED(m))
		return (0);

	mtx_enter(&m_extref_mtx);
	if (MCLISREFERENCED(m)) {
		m->m_ext.ext_nextref->m_ext.ext_prevref =
		    m->m_ext.ext_prevref;
		m->m_ext.ext_prevref->m_ext.ext_nextref =
		    m->m_ext.ext_nextref;
	} else
		refs = 0;
	mtx_leave(&m_extref_mtx);

	return (refs);
}

/*
 * Returns a number for use with MEXTADD.
 * Should only be called once per function.
 * Drivers can be assured that the index will be non zero.
 */
u_int
mextfree_register(void (*fn)(caddr_t, u_int, void *))
{
	KASSERT(num_extfree_fns < nitems(mextfree_fns));
	mextfree_fns[num_extfree_fns] = fn;
	return num_extfree_fns++;
}

void
m_extfree(struct mbuf *m)
{
	if (m_extunref(m) == 0) {
		KASSERT(m->m_ext.ext_free_fn < num_extfree_fns);
		mextfree_fns[m->m_ext.ext_free_fn](m->m_ext.ext_buf,
		    m->m_ext.ext_size, m->m_ext.ext_arg);
	}

	m->m_flags &= ~(M_EXT|M_EXTWR);
}

struct mbuf *
m_freem(struct mbuf *m)
{
	struct mbuf *n;

	if (m == NULL)
		return (NULL);

	n = m->m_nextpkt;

	do
		m = m_free(m);
	while (m != NULL);

	return (n);
}

void
m_purge(struct mbuf *m)
{
	while (m != NULL)
		m = m_freem(m);
}

/*
 * mbuf chain defragmenter. This function uses some evil tricks to defragment
 * an mbuf chain into a single buffer without changing the mbuf pointer.
 * This needs to know a lot of the mbuf internals to make this work.
 */
int
m_defrag(struct mbuf *m, int how)
{
	struct mbuf *m0;

	if (m->m_next == NULL)
		return (0);

#ifdef DIAGNOSTIC
	if (!(m->m_flags & M_PKTHDR))
		panic("m_defrag: no packet hdr or not a chain");
#endif

	if ((m0 = m_gethdr(how, m->m_type)) == NULL)
		return (ENOBUFS);
	if (m->m_pkthdr.len > MHLEN) {
		MCLGETI(m0, how, NULL, m->m_pkthdr.len);
		if (!(m0->m_flags & M_EXT)) {
			m_free(m0);
			return (ENOBUFS);
		}
	}
	m_copydata(m, 0, m->m_pkthdr.len, mtod(m0, caddr_t));
	m0->m_pkthdr.len = m0->m_len = m->m_pkthdr.len;

	/* free chain behind and possible ext buf on the first mbuf */
	m_freem(m->m_next);
	m->m_next = NULL;
	if (m->m_flags & M_EXT)
		m_extfree(m);

	/*
	 * Bounce copy mbuf over to the original mbuf and set everything up.
	 * This needs to reset or clear all pointers that may go into the
	 * original mbuf chain.
	 */
	if (m0->m_flags & M_EXT) {
		memcpy(&m->m_ext, &m0->m_ext, sizeof(struct mbuf_ext));
		MCLINITREFERENCE(m);
		m->m_flags |= m0->m_flags & (M_EXT|M_EXTWR);
		m->m_data = m->m_ext.ext_buf;
	} else {
		m->m_data = m->m_pktdat;
		memcpy(m->m_data, m0->m_data, m0->m_len);
	}
	m->m_pkthdr.len = m->m_len = m0->m_len;

	m0->m_flags &= ~(M_EXT|M_EXTWR);	/* cluster is gone */
	m_free(m0);

	return (0);
}

/*
 * Mbuffer utility routines.
 */

/*
 * Ensure len bytes of contiguous space at the beginning of the mbuf chain
 */
struct mbuf *
m_prepend(struct mbuf *m, int len, int how)
{
	struct mbuf *mn;

	if (len > MHLEN)
		panic("mbuf prepend length too big");

	if (M_LEADINGSPACE(m) >= len) {
		m->m_data -= len;
		m->m_len += len;
	} else {
		MGET(mn, how, m->m_type);
		if (mn == NULL) {
			m_freem(m);
			return (NULL);
		}
		if (m->m_flags & M_PKTHDR)
			M_MOVE_PKTHDR(mn, m);
		mn->m_next = m;
		m = mn;
		MH_ALIGN(m, len);
		m->m_len = len;
	}
	if (m->m_flags & M_PKTHDR)
		m->m_pkthdr.len += len;
	return (m);
}

/*
 * Make a copy of an mbuf chain starting "off" bytes from the beginning,
 * continuing for "len" bytes.  If len is M_COPYALL, copy to end of mbuf.
 * The wait parameter is a choice of M_WAIT/M_DONTWAIT from caller.
 */
struct mbuf *
m_copym(struct mbuf *m0, int off, int len, int wait)
{
	struct mbuf *m, *n, **np;
	struct mbuf *top;
	int copyhdr = 0;

	if (off < 0 || len < 0)
		panic("m_copym0: off %d, len %d", off, len);
	if (off == 0 && m0->m_flags & M_PKTHDR)
		copyhdr = 1;
	if ((m = m_getptr(m0, off, &off)) == NULL)
		panic("m_copym0: short mbuf chain");
	np = &top;
	top = NULL;
	while (len > 0) {
		if (m == NULL) {
			if (len != M_COPYALL)
				panic("m_copym0: m == NULL and not COPYALL");
			break;
		}
		MGET(n, wait, m->m_type);
		*np = n;
		if (n == NULL)
			goto nospace;
		if (copyhdr) {
			if (m_dup_pkthdr(n, m0, wait))
				goto nospace;
			if (len != M_COPYALL)
				n->m_pkthdr.len = len;
			copyhdr = 0;
		}
		n->m_len = min(len, m->m_len - off);
		if (m->m_flags & M_EXT) {
			n->m_data = m->m_data + off;
			n->m_ext = m->m_ext;
			MCLADDREFERENCE(m, n);
		} else {
			n->m_data += m->m_data -
			    (m->m_flags & M_PKTHDR ? m->m_pktdat : m->m_dat);
			n->m_data += off;
			memcpy(mtod(n, caddr_t), mtod(m, caddr_t) + off,
			    n->m_len);
		}
		if (len != M_COPYALL)
			len -= n->m_len;
		off += n->m_len;
#ifdef DIAGNOSTIC
		if (off > m->m_len)
			panic("m_copym0 overrun");
#endif
		if (off == m->m_len) {
			m = m->m_next;
			off = 0;
		}
		np = &n->m_next;
	}
	return (top);
nospace:
	m_freem(top);
	return (NULL);
}

/*
 * Copy data from an mbuf chain starting "off" bytes from the beginning,
 * continuing for "len" bytes, into the indicated buffer.
 */
void
m_copydata(struct mbuf *m, int off, int len, caddr_t cp)
{
	unsigned count;

	if (off < 0)
		panic("m_copydata: off %d < 0", off);
	if (len < 0)
		panic("m_copydata: len %d < 0", len);
	if ((m = m_getptr(m, off, &off)) == NULL)
		panic("m_copydata: short mbuf chain");
	while (len > 0) {
		if (m == NULL)
			panic("m_copydata: null mbuf");
		count = min(m->m_len - off, len);
		memmove(cp, mtod(m, caddr_t) + off, count);
		len -= count;
		cp += count;
		off = 0;
		m = m->m_next;
	}
}

/*
 * Copy data from a buffer back into the indicated mbuf chain,
 * starting "off" bytes from the beginning, extending the mbuf
 * chain if necessary. The mbuf needs to be properly initialized
 * including the setting of m_len.
 */
int
m_copyback(struct mbuf *m0, int off, int len, const void *_cp, int wait)
{
	int mlen, totlen = 0;
	struct mbuf *m = m0, *n;
	caddr_t cp = (caddr_t)_cp;
	int error = 0;

	if (m0 == NULL)
		return (0);
	while (off > (mlen = m->m_len)) {
		off -= mlen;
		totlen += mlen;
		if (m->m_next == NULL) {
			if ((n = m_get(wait, m->m_type)) == NULL) {
				error = ENOBUFS;
				goto out;
			}

			if (off + len > MLEN) {
				MCLGETI(n, wait, NULL, off + len);
				if (!(n->m_flags & M_EXT)) {
					m_free(n);
					error = ENOBUFS;
					goto out;
				}
			}
			memset(mtod(n, caddr_t), 0, off);
			n->m_len = len + off;
			m->m_next = n;
		}
		m = m->m_next;
	}
	while (len > 0) {
		/* extend last packet to be filled fully */
		if (m->m_next == NULL && (len > m->m_len - off))
			m->m_len += min(len - (m->m_len - off),
			    M_TRAILINGSPACE(m));
		mlen = min(m->m_len - off, len);
		memmove(mtod(m, caddr_t) + off, cp, mlen);
		cp += mlen;
		len -= mlen;
		totlen += mlen + off;
		if (len == 0)
			break;
		off = 0;

		if (m->m_next == NULL) {
			if ((n = m_get(wait, m->m_type)) == NULL) {
				error = ENOBUFS;
				goto out;
			}

			if (len > MLEN) {
				MCLGETI(n, wait, NULL, len);
				if (!(n->m_flags & M_EXT)) {
					m_free(n);
					error = ENOBUFS;
					goto out;
				}
			}
			n->m_len = len;
			m->m_next = n;
		}
		m = m->m_next;
	}
out:
	if (((m = m0)->m_flags & M_PKTHDR) && (m->m_pkthdr.len < totlen))
		m->m_pkthdr.len = totlen;

	return (error);
}

/*
 * Concatenate mbuf chain n to m.
 * n might be copied into m (when n->m_len is small), therefore data portion of
 * n could be copied into an mbuf of different mbuf type.
 * Therefore both chains should be of the same type (e.g. MT_DATA).
 * Any m_pkthdr is not updated.
 */
void
m_cat(struct mbuf *m, struct mbuf *n)
{
	while (m->m_next)
		m = m->m_next;
	while (n) {
		if (M_READONLY(m) || n->m_len > M_TRAILINGSPACE(m)) {
			/* just join the two chains */
			m->m_next = n;
			return;
		}
		/* splat the data from one into the other */
		memcpy(mtod(m, caddr_t) + m->m_len, mtod(n, caddr_t),
		    n->m_len);
		m->m_len += n->m_len;
		n = m_free(n);
	}
}

void
m_adj(struct mbuf *mp, int req_len)
{
	int len = req_len;
	struct mbuf *m;
	int count;

	if ((m = mp) == NULL)
		return;
	if (len >= 0) {
		/*
		 * Trim from head.
		 */
		while (m != NULL && len > 0) {
			if (m->m_len <= len) {
				len -= m->m_len;
				m->m_len = 0;
				m = m->m_next;
			} else {
				m->m_len -= len;
				m->m_data += len;
				len = 0;
			}
		}
		if (mp->m_flags & M_PKTHDR)
			mp->m_pkthdr.len -= (req_len - len);
	} else {
		/*
		 * Trim from tail.  Scan the mbuf chain,
		 * calculating its length and finding the last mbuf.
		 * If the adjustment only affects this mbuf, then just
		 * adjust and return.  Otherwise, rescan and truncate
		 * after the remaining size.
		 */
		len = -len;
		count = 0;
		for (;;) {
			count += m->m_len;
			if (m->m_next == NULL)
				break;
			m = m->m_next;
		}
		if (m->m_len >= len) {
			m->m_len -= len;
			if (mp->m_flags & M_PKTHDR)
				mp->m_pkthdr.len -= len;
			return;
		}
		count -= len;
		if (count < 0)
			count = 0;
		/*
		 * Correct length for chain is "count".
		 * Find the mbuf with last data, adjust its length,
		 * and toss data from remaining mbufs on chain.
		 */
		m = mp;
		if (m->m_flags & M_PKTHDR)
			m->m_pkthdr.len = count;
		for (; m; m = m->m_next) {
			if (m->m_len >= count) {
				m->m_len = count;
				break;
			}
			count -= m->m_len;
		}
		while ((m = m->m_next) != NULL)
			m->m_len = 0;
	}
}

/*
 * Rearrange an mbuf chain so that len bytes are contiguous
 * and in the data area of an mbuf (so that mtod will work
 * for a structure of size len).  Returns the resulting
 * mbuf chain on success, frees it and returns null on failure.
 */
struct mbuf *
m_pullup(struct mbuf *n, int len)
{
	struct mbuf *m;
	unsigned int adj;
	caddr_t head, tail;
	unsigned int space;

	/* if n is already contig then don't do any work */
	if (len <= n->m_len)
		return (n);

	adj = (unsigned long)n->m_data & ALIGNBYTES;
	head = (caddr_t)ALIGN(mtod(n, caddr_t) - M_LEADINGSPACE(n)) + adj;
	tail = mtod(n, caddr_t) + n->m_len + M_TRAILINGSPACE(n);

	if (head < tail && len <= tail - head) {
		/* there's enough space in the first mbuf */

		if (len > tail - mtod(n, caddr_t)) {
			/* need to memmove to make space at the end */
			memmove(head, mtod(n, caddr_t), n->m_len);
			n->m_data = head;
		}

		len -= n->m_len;
		m = n;
		n = m->m_next;
	} else {
		/* the first mbuf is too small so prepend one with space */
		space = adj + len;

		if (space > MAXMCLBYTES)
			goto bad;

		MGET(m, M_DONTWAIT, n->m_type);
		if (m == NULL)
			goto bad;
		if (space > MHLEN) {
			MCLGETI(m, M_DONTWAIT, NULL, space);
			if ((m->m_flags & M_EXT) == 0) {
				m_free(m);
				goto bad;
			}
		}

		if (n->m_flags & M_PKTHDR)
			M_MOVE_PKTHDR(m, n);

		m->m_len = 0;
		m->m_data += adj;
	}

	KASSERT(M_TRAILINGSPACE(m) >= len);

	do {
		if (n == NULL) {
			(void)m_free(m);
			goto bad;
		}

		space = min(len, n->m_len);
		memcpy(mtod(m, caddr_t) + m->m_len, mtod(n, caddr_t), space);
		len -= space;
		m->m_len += space;
		n->m_len -= space;

		if (n->m_len > 0)
			n->m_data += space;
		else
			n = m_free(n);
	} while (len > 0);

	m->m_next = n;

	return (m);

bad:
	m_freem(n);
	return (NULL);
}

/*
 * Return a pointer to mbuf/offset of location in mbuf chain.
 */
struct mbuf *
m_getptr(struct mbuf *m, int loc, int *off)
{
	while (loc >= 0) {
		/* Normal end of search */
		if (m->m_len > loc) {
			*off = loc;
			return (m);
		} else {
			loc -= m->m_len;

			if (m->m_next == NULL) {
				if (loc == 0) {
					/* Point at the end of valid data */
					*off = m->m_len;
					return (m);
				} else {
					return (NULL);
				}
			} else {
				m = m->m_next;
			}
		}
	}

	return (NULL);
}

/*
 * Partition an mbuf chain in two pieces, returning the tail --
 * all but the first len0 bytes.  In case of failure, it returns NULL and
 * attempts to restore the chain to its original state.
 */
struct mbuf *
m_split(struct mbuf *m0, int len0, int wait)
{
	struct mbuf *m, *n;
	unsigned len = len0, remain, olen;

	for (m = m0; m && len > m->m_len; m = m->m_next)
		len -= m->m_len;
	if (m == NULL)
		return (NULL);
	remain = m->m_len - len;
	if (m0->m_flags & M_PKTHDR) {
		MGETHDR(n, wait, m0->m_type);
		if (n == NULL)
			return (NULL);
		if (m_dup_pkthdr(n, m0, wait)) {
			m_freem(n);
			return (NULL);
		}
		n->m_pkthdr.len -= len0;
		olen = m0->m_pkthdr.len;
		m0->m_pkthdr.len = len0;
		if (remain == 0) {
			n->m_next = m->m_next;
			m->m_next = NULL;
			n->m_len = 0;
			return (n);
		}
		if (m->m_flags & M_EXT)
			goto extpacket;
		if (remain > MHLEN) {
			/* m can't be the lead packet */
			MH_ALIGN(n, 0);
			n->m_next = m_split(m, len, wait);
			if (n->m_next == NULL) {
				(void) m_free(n);
				m0->m_pkthdr.len = olen;
				return (NULL);
			} else {
				n->m_len = 0;
				return (n);
			}
		} else
			MH_ALIGN(n, remain);
	} else if (remain == 0) {
		n = m->m_next;
		m->m_next = NULL;
		return (n);
	} else {
		MGET(n, wait, m->m_type);
		if (n == NULL)
			return (NULL);
		M_ALIGN(n, remain);
	}
extpacket:
	if (m->m_flags & M_EXT) {
		n->m_ext = m->m_ext;
		MCLADDREFERENCE(m, n);
		n->m_data = m->m_data + len;
	} else {
		memcpy(mtod(n, caddr_t), mtod(m, caddr_t) + len, remain);
	}
	n->m_len = remain;
	m->m_len = len;
	n->m_next = m->m_next;
	m->m_next = NULL;
	return (n);
}

/*
 * Make space for a new header of length hlen at skip bytes
 * into the packet.  When doing this we allocate new mbufs only
 * when absolutely necessary.  The mbuf where the new header
 * is to go is returned together with an offset into the mbuf.
 * If NULL is returned then the mbuf chain may have been modified;
 * the caller is assumed to always free the chain.
 */
struct mbuf *
m_makespace(struct mbuf *m0, int skip, int hlen, int *off)
{
	struct mbuf *m;
	unsigned remain;

	KASSERT(m0 != NULL);
	KASSERT(hlen < MHLEN);

	for (m = m0; m && skip > m->m_len; m = m->m_next)
		skip -= m->m_len;
	if (m == NULL)
		return (NULL);
	/*
	 * At this point skip is the offset into the mbuf m
	 * where the new header should be placed.  Figure out
	 * if there's space to insert the new header.  If so,
	 * and copying the remainder makese sense then do so.
	 * Otherwise insert a new mbuf in the chain, splitting
	 * the contents of m as needed.
	 */
	remain = m->m_len - skip;		/* data to move */
	if (skip < remain && hlen <= M_LEADINGSPACE(m)) {
		if (skip)
			memmove(m->m_data-hlen, m->m_data, skip);
		m->m_data -= hlen;
		m->m_len += hlen;
		(*off) = skip;
	} else if (hlen > M_TRAILINGSPACE(m)) {
		struct mbuf *n0, *n, **np;
		int todo, len, done, alloc;

		n0 = NULL;
		np = &n0;
		alloc = 0;
		done = 0;
		todo = remain;
		while (todo > 0) {
			MGET(n, M_DONTWAIT, m->m_type);
			len = MHLEN;
			if (n && todo > MHLEN) {
				MCLGET(n, M_DONTWAIT);
				len = MCLBYTES;
				if ((n->m_flags & M_EXT) == 0) {
					m_free(n);
					n = NULL;
				}
			}
			if (n == NULL) {
				m_freem(n0);
				return NULL;
			}
			*np = n;
			np = &n->m_next;
			alloc++;
			len = min(todo, len);
			memcpy(n->m_data, mtod(m, char *) + skip + done, len);
			n->m_len = len;
			done += len;
			todo -= len;
		}

		if (hlen <= M_TRAILINGSPACE(m) + remain) {
			m->m_len = skip + hlen;
			*off = skip;
			if (n0 != NULL) {
				*np = m->m_next;
				m->m_next = n0;
			}
		}
		else {
			n = m_get(M_DONTWAIT, m->m_type);
			if (n == NULL) {
				m_freem(n0);
				return NULL;
			}
			alloc++;

			if ((n->m_next = n0) == NULL)
				np = &n->m_next;
			n0 = n;

			*np = m->m_next;
			m->m_next = n0;

			n->m_len = hlen;
			m->m_len = skip;

			m = n;			/* header is at front ... */
			*off = 0;		/* ... of new mbuf */
		}
	} else {
		/*
		 * Copy the remainder to the back of the mbuf
		 * so there's space to write the new header.
		 */
		if (remain > 0)
			memmove(mtod(m, caddr_t) + skip + hlen,
			      mtod(m, caddr_t) + skip, remain);
		m->m_len += hlen;
		*off = skip;
	}
	m0->m_pkthdr.len += hlen;		/* adjust packet length */
	return m;
}


/*
 * Routine to copy from device local memory into mbufs.
 */
struct mbuf *
m_devget(char *buf, int totlen, int off)
{
	struct mbuf	*m;
	struct mbuf	*top, **mp;
	int		 len;

	top = NULL;
	mp = &top;

	if (off < 0 || off > MHLEN)
		return (NULL);

	MGETHDR(m, M_DONTWAIT, MT_DATA);
	if (m == NULL)
		return (NULL);

	m->m_pkthdr.len = totlen;

	len = MHLEN;

	while (totlen > 0) {
		if (top != NULL) {
			MGET(m, M_DONTWAIT, MT_DATA);
			if (m == NULL) {
				/*
				 * As we might get called by pfkey, make sure
				 * we do not leak sensitive data.
				 */
				top->m_flags |= M_ZEROIZE;
				m_freem(top);
				return (NULL);
			}
			len = MLEN;
		}

		if (totlen + off >= MINCLSIZE) {
			MCLGET(m, M_DONTWAIT);
			if (m->m_flags & M_EXT)
				len = MCLBYTES;
		} else {
			/* Place initial small packet/header at end of mbuf. */
			if (top == NULL && totlen + off + max_linkhdr <= len) {
				m->m_data += max_linkhdr;
				len -= max_linkhdr;
			}
		}

		if (off) {
			m->m_data += off;
			len -= off;
			off = 0;
		}

		m->m_len = len = min(totlen, len);
		memcpy(mtod(m, void *), buf, (size_t)len);

		buf += len;
		*mp = m;
		mp = &m->m_next;
		totlen -= len;
	}
	return (top);
}

void
m_zero(struct mbuf *m)
{
#ifdef DIAGNOSTIC
	if (M_READONLY(m))
		panic("m_zero: M_READONLY");
#endif /* DIAGNOSTIC */

	if (m->m_flags & M_EXT)
		explicit_bzero(m->m_ext.ext_buf, m->m_ext.ext_size);
	else {
		if (m->m_flags & M_PKTHDR)
			explicit_bzero(m->m_pktdat, MHLEN);
		else
			explicit_bzero(m->m_dat, MLEN);
	}
}

/*
 * Apply function f to the data in an mbuf chain starting "off" bytes from the
 * beginning, continuing for "len" bytes.
 */
int
m_apply(struct mbuf *m, int off, int len,
    int (*f)(caddr_t, caddr_t, unsigned int), caddr_t fstate)
{
	int rval;
	unsigned int count;

	if (len < 0)
		panic("m_apply: len %d < 0", len);
	if (off < 0)
		panic("m_apply: off %d < 0", off);
	while (off > 0) {
		if (m == NULL)
			panic("m_apply: null mbuf in skip");
		if (off < m->m_len)
			break;
		off -= m->m_len;
		m = m->m_next;
	}
	while (len > 0) {
		if (m == NULL)
			panic("m_apply: null mbuf");
		count = min(m->m_len - off, len);

		rval = f(fstate, mtod(m, caddr_t) + off, count);
		if (rval)
			return (rval);

		len -= count;
		off = 0;
		m = m->m_next;
	}

	return (0);
}

int
m_leadingspace(struct mbuf *m)
{
	if (M_READONLY(m))
		return 0;
	return (m->m_flags & M_EXT ? m->m_data - m->m_ext.ext_buf :
	    m->m_flags & M_PKTHDR ? m->m_data - m->m_pktdat :
	    m->m_data - m->m_dat);
}

int
m_trailingspace(struct mbuf *m)
{
	if (M_READONLY(m))
		return 0;
	return (m->m_flags & M_EXT ? m->m_ext.ext_buf +
	    m->m_ext.ext_size - (m->m_data + m->m_len) :
	    &m->m_dat[MLEN] - (m->m_data + m->m_len));
}


/*
 * Duplicate mbuf pkthdr from from to to.
 * from must have M_PKTHDR set, and to must be empty.
 */
int
m_dup_pkthdr(struct mbuf *to, struct mbuf *from, int wait)
{
	int error;

	KASSERT(from->m_flags & M_PKTHDR);

	to->m_flags = (to->m_flags & (M_EXT | M_EXTWR));
	to->m_flags |= (from->m_flags & M_COPYFLAGS);
	to->m_pkthdr = from->m_pkthdr;

#if NPF > 0
	pf_pkt_state_key_ref(to);
#endif	/* NPF > 0 */

	SLIST_INIT(&to->m_pkthdr.ph_tags);

	if ((error = m_tag_copy_chain(to, from, wait)) != 0)
		return (error);

	if ((to->m_flags & M_EXT) == 0)
		to->m_data = to->m_pktdat;

	return (0);
}

struct mbuf *
m_dup_pkt(struct mbuf *m0, unsigned int adj, int wait)
{
	struct mbuf *m;
	int len;

	len = m0->m_pkthdr.len + adj;
	if (len > MAXMCLBYTES) /* XXX */
		return (NULL);

	m = m_get(wait, m0->m_type);
	if (m == NULL)
		return (NULL);

	if (m_dup_pkthdr(m, m0, wait) != 0)
		goto fail;

	if (len > MHLEN) {
		MCLGETI(m, wait, NULL, len);
		if (!ISSET(m->m_flags, M_EXT))
			goto fail;
	}

	m->m_len = m->m_pkthdr.len = len;
	m_adj(m, adj);
	m_copydata(m0, 0, m0->m_pkthdr.len, mtod(m, caddr_t));

	return (m);

fail:
	m_freem(m);
	return (NULL);
}

void *
m_pool_alloc(struct pool *pp, int flags, int *slowdown)
{
	void *v = NULL;
	int avail = 1;

	if (mbuf_mem_alloc + pp->pr_pgsize > mbuf_mem_limit)
		return (NULL);

	mtx_enter(&m_pool_mtx);
	if (mbuf_mem_alloc + pp->pr_pgsize > mbuf_mem_limit)
		avail = 0;
	else
		mbuf_mem_alloc += pp->pr_pgsize;
	mtx_leave(&m_pool_mtx);

	if (avail) {
		v = (*pool_allocator_multi.pa_alloc)(pp, flags, slowdown);

		if (v == NULL) {
			mtx_enter(&m_pool_mtx);
			mbuf_mem_alloc -= pp->pr_pgsize;
			mtx_leave(&m_pool_mtx);
		}
	}

	return (v);
}

void
m_pool_free(struct pool *pp, void *v)
{
	(*pool_allocator_multi.pa_free)(pp, v);

	mtx_enter(&m_pool_mtx);
	mbuf_mem_alloc -= pp->pr_pgsize;
	mtx_leave(&m_pool_mtx);
}

void
m_pool_init(struct pool *pp, u_int size, u_int align, const char *wmesg)
{
	pool_init(pp, size, align, IPL_NET, 0, wmesg, &m_pool_allocator);
	pool_set_constraints(pp, &kp_dma_contig);
}

#ifdef DDB
void
m_print(void *v,
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))))
{
	struct mbuf *m = v;

	(*pr)("mbuf %p\n", m);
	(*pr)("m_type: %i\tm_flags: %b\n", m->m_type, m->m_flags, M_BITS);
	(*pr)("m_next: %p\tm_nextpkt: %p\n", m->m_next, m->m_nextpkt);
	(*pr)("m_data: %p\tm_len: %u\n", m->m_data, m->m_len);
	(*pr)("m_dat: %p\tm_pktdat: %p\n", m->m_dat, m->m_pktdat);
	if (m->m_flags & M_PKTHDR) {
		(*pr)("m_ptkhdr.ph_ifidx: %u\tm_pkthdr.len: %i\n",
		    m->m_pkthdr.ph_ifidx, m->m_pkthdr.len);
		(*pr)("m_ptkhdr.ph_tags: %p\tm_pkthdr.ph_tagsset: %b\n",
		    SLIST_FIRST(&m->m_pkthdr.ph_tags),
		    m->m_pkthdr.ph_tagsset, MTAG_BITS);
		(*pr)("m_pkthdr.ph_flowid: %u\tm_pkthdr.ph_loopcnt: %u\n",
		    m->m_pkthdr.ph_flowid, m->m_pkthdr.ph_loopcnt);
		(*pr)("m_pkthdr.csum_flags: %b\n",
		    m->m_pkthdr.csum_flags, MCS_BITS);
		(*pr)("m_pkthdr.ether_vtag: %u\tm_ptkhdr.ph_rtableid: %u\n",
		    m->m_pkthdr.ether_vtag, m->m_pkthdr.ph_rtableid);
		(*pr)("m_pkthdr.pf.statekey: %p\tm_pkthdr.pf.inp %p\n",
		    m->m_pkthdr.pf.statekey, m->m_pkthdr.pf.inp);
		(*pr)("m_pkthdr.pf.qid: %u\tm_pkthdr.pf.tag: %u\n",
		    m->m_pkthdr.pf.qid, m->m_pkthdr.pf.tag);
		(*pr)("m_pkthdr.pf.flags: %b\n",
		    m->m_pkthdr.pf.flags, MPF_BITS);
		(*pr)("m_pkthdr.pf.routed: %u\tm_pkthdr.pf.prio: %u\n",
		    m->m_pkthdr.pf.routed, m->m_pkthdr.pf.prio);
	}
	if (m->m_flags & M_EXT) {
		(*pr)("m_ext.ext_buf: %p\tm_ext.ext_size: %u\n",
		    m->m_ext.ext_buf, m->m_ext.ext_size);
		(*pr)("m_ext.ext_free_fn: %u\tm_ext.ext_arg: %p\n",
		    m->m_ext.ext_free_fn, m->m_ext.ext_arg);
		(*pr)("m_ext.ext_nextref: %p\tm_ext.ext_prevref: %p\n",
		    m->m_ext.ext_nextref, m->m_ext.ext_prevref);

	}
}
#endif

/*
 * mbuf lists
 */

void
ml_init(struct mbuf_list *ml)
{
	ml->ml_head = ml->ml_tail = NULL;
	ml->ml_len = 0;
}

void
ml_enqueue(struct mbuf_list *ml, struct mbuf *m)
{
	if (ml->ml_tail == NULL)
		ml->ml_head = ml->ml_tail = m;
	else {
		ml->ml_tail->m_nextpkt = m;
		ml->ml_tail = m;
	}

	m->m_nextpkt = NULL;
	ml->ml_len++;
}

void
ml_enlist(struct mbuf_list *mla, struct mbuf_list *mlb)
{
	if (!ml_empty(mlb)) {
		if (ml_empty(mla))
			mla->ml_head = mlb->ml_head;
		else
			mla->ml_tail->m_nextpkt = mlb->ml_head;
		mla->ml_tail = mlb->ml_tail;
		mla->ml_len += mlb->ml_len;

		ml_init(mlb);
	}
}

struct mbuf *
ml_dequeue(struct mbuf_list *ml)
{
	struct mbuf *m;

	m = ml->ml_head;
	if (m != NULL) {
		ml->ml_head = m->m_nextpkt;
		if (ml->ml_head == NULL)
			ml->ml_tail = NULL;

		m->m_nextpkt = NULL;
		ml->ml_len--;
	}

	return (m);
}

struct mbuf *
ml_dechain(struct mbuf_list *ml)
{
	struct mbuf *m0;

	m0 = ml->ml_head;

	ml_init(ml);

	return (m0);
}

unsigned int
ml_purge(struct mbuf_list *ml)
{
	struct mbuf *m, *n;
	unsigned int len;

	for (m = ml->ml_head; m != NULL; m = n) {
		n = m->m_nextpkt;
		m_freem(m);
	}

	len = ml->ml_len;
	ml_init(ml);

	return (len);
}

/*
 * mbuf queues
 */

void
mq_init(struct mbuf_queue *mq, u_int maxlen, int ipl)
{
	mtx_init(&mq->mq_mtx, ipl);
	ml_init(&mq->mq_list);
	mq->mq_maxlen = maxlen;
}

int
mq_enqueue(struct mbuf_queue *mq, struct mbuf *m)
{
	int dropped = 0;

	mtx_enter(&mq->mq_mtx);
	if (mq_len(mq) < mq->mq_maxlen)
		ml_enqueue(&mq->mq_list, m);
	else {
		mq->mq_drops++;
		dropped = 1;
	}
	mtx_leave(&mq->mq_mtx);

	if (dropped)
		m_freem(m);

	return (dropped);
}

struct mbuf *
mq_dequeue(struct mbuf_queue *mq)
{
	struct mbuf *m;

	mtx_enter(&mq->mq_mtx);
	m = ml_dequeue(&mq->mq_list);
	mtx_leave(&mq->mq_mtx);

	return (m);
}

int
mq_enlist(struct mbuf_queue *mq, struct mbuf_list *ml)
{
	struct mbuf *m;
	int dropped = 0;

	mtx_enter(&mq->mq_mtx);
	if (mq_len(mq) < mq->mq_maxlen)
		ml_enlist(&mq->mq_list, ml);
	else {
		dropped = ml_len(ml);
		mq->mq_drops += dropped;
	}
	mtx_leave(&mq->mq_mtx);

	if (dropped) {
		while ((m = ml_dequeue(ml)) != NULL)
			m_freem(m);
	}

	return (dropped);
}

void
mq_delist(struct mbuf_queue *mq, struct mbuf_list *ml)
{
	mtx_enter(&mq->mq_mtx);
	*ml = mq->mq_list;
	ml_init(&mq->mq_list);
	mtx_leave(&mq->mq_mtx);
}

struct mbuf *
mq_dechain(struct mbuf_queue *mq)
{
	struct mbuf *m0;

	mtx_enter(&mq->mq_mtx);
	m0 = ml_dechain(&mq->mq_list);
	mtx_leave(&mq->mq_mtx);

	return (m0);
}

unsigned int
mq_purge(struct mbuf_queue *mq)
{
	struct mbuf_list ml;

	mq_delist(mq, &ml);

	return (ml_purge(&ml));
}
@


1.244
log
@move the mbuf pools to m_pool_init and a single global memory limit

this replaces individual calls to pool_init, pool_set_constraints, and
pool_sethardlimit with calls to m_pool_init. m_pool_init inits the
mbuf pools with the mbuf pool allocator, and because of that doesnt
set per pool limits.

ok bluhm@@ as part of a larger diff
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.243 2017/02/07 06:27:18 dlg Exp $	*/
d201 2
d204 6
@


1.243
log
@add m_pool_init(), a wrapper around pool_init for mbuf clusters.

m_pool_init is basically a call to pool_init with everythign except
the size and alignment specified, and a call to pool_set_constraints
so the memroy is always dma reachable. it also wires up the memory
with the custom mbuf pool allocator.

ok bluhm@@ as part of a larger diff
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.242 2017/02/07 06:21:37 dlg Exp $	*/
d166 3
d176 1
a176 3
	pool_init(&mbpool, MSIZE, 0, IPL_NET, 0, "mbufpl", NULL);
	pool_set_constraints(&mbpool, &kp_dma_contig);
	pool_setlowat(&mbpool, mblowat);
d190 2
a191 4
		pool_init(&mclpools[i], mclsizes[i], 64, IPL_NET, 0,
		    mclnames[i], NULL);
		pool_set_constraints(&mclpools[i], &kp_dma_contig);
		pool_setlowat(&mclpools[i], mcllowat);
a195 2

	nmbclust_update();
d207 1
a207 21
	unsigned int i, n;

	/*
	 * Set the hard limit on the mclpools to the number of
	 * mbuf clusters the kernel is to support.  Log the limit
	 * reached message max once a minute.
	 */
	for (i = 0; i < nitems(mclsizes); i++) {
		n = (unsigned long long)nmbclust * MCLBYTES / mclsizes[i];
		(void)pool_sethardlimit(&mclpools[i], n, mclpool_warnmsg, 60);
		/*
		 * XXX this needs to be reconsidered.
		 * Setting the high water mark to nmbclust is too high
		 * but we need to have enough spare buffers around so that
		 * allocations in interrupt context don't fail or mclgeti()
		 * drivers may end up with empty rings.
		 */
		pool_sethiwat(&mclpools[i], n);
	}
	pool_sethiwat(&mbpool, nmbclust);

@


1.242
log
@provide a custom pool page allocator for mbufs, but dont use it yet.

the custom allocator is basically a wrapper around the multi page
pool allocator, but it has a single global memory limit managed by
the wrapper.

currently each of the mbuf pools has their own memory limit (or
none in the case of the myx pool) independent of the other pools.
this means each pool can allocate up to nmbclust worth of mbufs,
rather than all of them sharing the one limit. wrapping the allocator
like this means we can move to a single memory limit for all mbufs
in the system.

ok bluhm@@ as part of a larger diff
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.241 2017/02/05 16:23:38 jca Exp $	*/
d1443 7
@


1.241
log
@Always allocate counters memory using type M_COUNTERS.

This makes the API simpler, and is probably more useful than spreading
counters memory other several types, making it harder to track.

Prodded by mpi, ok mpi@@ stsp@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.240 2017/01/25 09:41:45 mpi Exp $	*/
d136 13
d164 2
d230 2
d1404 39
@


1.240
log
@Clear the reference of the original mbuf chain after m_split()'ing
a mbuf and properly intialize m_len.

From FreeBSD via Imre Vadasz.

ok bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.239 2016/11/29 10:22:30 jsg Exp $	*/
d189 1
a189 1
	mbstat = counters_alloc_ncpus(mbstat, MBSTAT_COUNT, M_DEVBUF);
@


1.239
log
@m_free() and m_freem() test for NULL.  Simplify callers which had their own
NULL tests.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.238 2016/11/09 08:55:11 mpi Exp $	*/
d1016 6
d1032 2
a1033 1
			} else
d1035 1
@


1.238
log
@Do not dereference a variable without initializing it beforehand.

Fix a typo introduced in m_pullup(9) refactoring and found the hard
way by semarie@@ while testing another diff.

ok mikeb@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.237 2016/10/27 03:29:55 dlg Exp $	*/
d364 1
a364 2
		if (m0)
			m_freem(m0);
@


1.237
log
@refactor m_pullup a bit.

the most important change is that if the requested data is already
in the first mbuf in the chain, return quickly.

if that isnt true, the code will try to use the first mbuf to fit
the requested data.

if that isnt true, it will prepend an mbuf, and maybe a cluster,
to fit the requested data.

m_pullup will now try to maintain the alignment of the original
payload, even when prepending a new mbuf for it.

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.236 2016/10/27 03:00:35 dlg Exp $	*/
d899 1
a899 1
			m->m_data = head;
@


1.236
log
@add a new pool for 2k + 2 byte (mcl2k2) clusters.

a certain vendor likes to make chips that specify the rx buffer
sizes in kilobyte increments. unfortunately it places the ethernet
header on the start of the rx buffer, which means if you give it a
mcl2k cluster, the ethernet header will not be ETHER_ALIGNed cos
mcl2k clusters are always allocated on 2k boundarys (cos they pack
into pages well). that in turn means the ip header wont be aligned
correctly.

the current workaround on these chips has been to let non-strict
alignment archs just use the normal 2k cluster, but use whatever
cluster can fit 2k + 2 on strict archs. that turns out to be the
4k cluster, meaning we waste nearly 2k of space on every packet.

properly aligning the ethernet header and ip headers gives a
performance boost, even on non-strict archs.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.235 2016/10/24 23:58:33 dlg Exp $	*/
d881 20
a900 1
	int count;
d902 1
a902 16
	/*
	 * If first mbuf has no cluster, and has room for len bytes
	 * without shifting current data, pullup into it,
	 * otherwise allocate a new mbuf to prepend to the chain.
	 */
	if ((n->m_flags & M_EXT) == 0 && n->m_next &&
	    n->m_data + len < &n->m_dat[MLEN]) {
		if (n->m_len >= len)
			return (n);
		m = n;
		n = n->m_next;
		len -= m->m_len;
	} else if ((n->m_flags & M_EXT) != 0 && len > MHLEN && n->m_next &&
	    n->m_data + len < &n->m_ext.ext_buf[n->m_ext.ext_size]) {
		if (n->m_len >= len)
			return (n);
d904 1
a904 2
		n = n->m_next;
		len -= m->m_len;
d906 4
a909 1
		if (len > MAXMCLBYTES)
d911 1
d915 2
a916 2
		if (len > MHLEN) {
			MCLGETI(m, M_DONTWAIT, NULL, len);
d922 1
a922 1
		m->m_len = 0;
d925 3
d930 2
d933 13
a945 8
		count = min(len, n->m_len);
		memcpy(mtod(m, caddr_t) + m->m_len, mtod(n, caddr_t),
		    count);
		len -= count;
		m->m_len += count;
		n->m_len -= count;
		if (n->m_len)
			n->m_data += count;
d948 2
a949 5
	} while (len > 0 && n);
	if (len > 0) {
		(void)m_free(m);
		goto bad;
	}
d953 1
@


1.235
log
@avoid using realloc in the name of things that dont work like realloc.

cpumem_realloc and counters_realloc actually allocated new per cpu data
for new cpus, they didnt resize the existing allocation.

specifically, this renames cpumem_reallod to cpumem_malloc_ncpus, and
counters_realloc to counters_alloc_ncpus.

ok (and with some fixes by) bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.234 2016/10/24 04:38:44 dlg Exp $	*/
d113 1
d149 1
d166 9
a174 3
		snprintf(mclnames[i], sizeof(mclnames[0]), "mcl%dk",
		    mclsizes[i] >> 10);
		pool_init(&mclpools[i], mclsizes[i], 0, IPL_NET, 0,
@


1.234
log
@move the mbstat structure to percpu counters

each cpus counters still have to be protected by splnet, but this
is better thana single set of counters protected by a global mutex.

ok bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.233 2016/10/10 00:41:17 dlg Exp $	*/
d181 1
a181 1
	mbstat = counters_realloc(mbstat, MBSTAT_COUNT, M_DEVBUF);
@


1.233
log
@white space fixes.

no functional change
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.232 2016/10/10 00:34:50 dlg Exp $	*/
d86 1
d103 5
a107 3
struct	mbstat mbstat;		/* mbuf stats */
struct	mutex mbstatmtx = MUTEX_INITIALIZER(IPL_NET);
struct	pool mbpool;		/* mbuf pool */
d179 6
d216 5
d226 5
a230 3
	mtx_enter(&mbstatmtx);
	mbstat.m_mtypes[type]++;
	mtx_leave(&mbstatmtx);
d249 5
d259 5
a263 3
	mtx_enter(&mbstatmtx);
	mbstat.m_mtypes[type]++;
	mtx_leave(&mbstatmtx);
d375 3
d382 5
a386 3
	mtx_enter(&mbstatmtx);
	mbstat.m_mtypes[m->m_type]--;
	mtx_leave(&mbstatmtx);
@


1.232
log
@copy the offset of data inside mbufs in m_copym().

this is cheap since it is basic math. it also means that payloads
which have been aligned carefully will also be aligned in their
copy.

ok yasuoka@@ claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.231 2016/09/15 02:00:16 dlg Exp $	*/
d917 2
a918 2
	    		*off = loc;
	    		return (m);
d920 1
a920 1
	    		loc -= m->m_len;
d922 1
a922 1
	    		if (m->m_next == NULL) {
d924 3
a926 3
 					/* Point at the end of valid data */
		    			*off = m->m_len;
		    			return (m);
d928 1
a928 1
		  			return (NULL);
d930 1
a930 1
	    		} else {
d934 1
a934 1
    	}
@


1.231
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.230 2016/09/15 00:52:08 dlg Exp $	*/
d606 4
a609 1
		} else
d612 1
@


1.230
log
@we dont need m_copym0 with m_copym as a single wrapper, so merge them.

cos m_copym only does shallow copies, we can make the code do them
unconditionally.

for millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.229 2016/09/15 00:00:40 dlg Exp $	*/
d153 1
a153 2
	pool_init(&mbpool, MSIZE, 0, 0, 0, "mbufpl", NULL);
	pool_setipl(&mbpool, IPL_NET);
d157 2
a158 3
	pool_init(&mtagpool, PACKET_TAG_MAXSIZE + sizeof(struct m_tag),
	    0, 0, 0, "mtagpl", NULL);
	pool_setipl(&mtagpool, IPL_NET);
d163 1
a163 1
		pool_init(&mclpools[i], mclsizes[i], 0, 0, 0,
a164 1
		pool_setipl(&mclpools[i], IPL_NET);
@


1.229
log
@remove m_copym2 as its use has been replaced by m_dup_pkt

ok millert@@ mpi@@ henning@@ claudio@@ markus@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.228 2016/09/13 19:56:55 markus Exp $	*/
a128 1
struct mbuf *m_copym0(struct mbuf *, int, int, int, int);
d573 1
a573 7
m_copym(struct mbuf *m, int off, int len, int wait)
{
	return m_copym0(m, off, len, wait, 0);	/* shallow copy on M_EXT */
}

struct mbuf *
m_copym0(struct mbuf *m0, int off, int len, int wait, int deep)
d606 3
a608 17
			if (!deep) {
				n->m_data = m->m_data + off;
				n->m_ext = m->m_ext;
				MCLADDREFERENCE(m, n);
			} else {
				/*
				 * we are unsure about the way m was allocated.
				 * copy into multiple MCLBYTES cluster mbufs.
				 */
				MCLGET(n, wait);
				n->m_len = 0;
				n->m_len = M_TRAILINGSPACE(n);
				n->m_len = min(n->m_len, len);
				n->m_len = min(n->m_len, m->m_len - off);
				memcpy(mtod(n, caddr_t), mtod(m, caddr_t) + off,
				    n->m_len);
			}
@


1.228
log
@avoid extensive mbuf allocation for IPsec by replacing m_inject(4)
with m_makespace(4) from freebsd; ok mpi@@, bluhm@@, mikeb@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.227 2016/09/03 14:17:37 bluhm Exp $	*/
a576 10
}

/*
 * m_copym2() is like m_copym(), except it COPIES cluster mbufs, instead
 * of merely bumping the reference count.
 */
struct mbuf *
m_copym2(struct mbuf *m, int off, int len, int wait)
{
	return m_copym0(m, off, len, wait, 1);	/* deep copy */
@


1.227
log
@Limit all mbuf cluster pools to the same memory size.  Having limits
by number would allow the large clusters using too much memory.
Set size of mclsizes array explicitly to keep it in sync with
mclpools.
OK claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.226 2016/06/13 21:24:43 bluhm Exp $	*/
a969 61
 * Inject a new mbuf chain of length siz in mbuf chain m0 at
 * position len0. Returns a pointer to the first injected mbuf, or
 * NULL on failure (m0 is left undisturbed). Note that if there is
 * enough space for an object of size siz in the appropriate position,
 * no memory will be allocated. Also, there will be no data movement in
 * the first len0 bytes (pointers to that will remain valid).
 *
 * XXX It is assumed that siz is less than the size of an mbuf at the moment.
 */
struct mbuf *
m_inject(struct mbuf *m0, int len0, int siz, int wait)
{
	struct mbuf *m, *n, *n2 = NULL, *n3;
	unsigned len = len0, remain;

	if ((siz >= MHLEN) || (len0 <= 0))
		return (NULL);
	for (m = m0; m && len > m->m_len; m = m->m_next)
		len -= m->m_len;
	if (m == NULL)
		return (NULL);
	remain = m->m_len - len;
	if (remain == 0) {
		if ((m->m_next) && (M_LEADINGSPACE(m->m_next) >= siz)) {
			m->m_next->m_len += siz;
			if (m0->m_flags & M_PKTHDR)
				m0->m_pkthdr.len += siz;
			m->m_next->m_data -= siz;
			return m->m_next;
		}
	} else {
		n2 = m_copym2(m, len, remain, wait);
		if (n2 == NULL)
			return (NULL);
	}

	MGET(n, wait, MT_DATA);
	if (n == NULL) {
		if (n2)
			m_freem(n2);
		return (NULL);
	}

	n->m_len = siz;
	if (m0->m_flags & M_PKTHDR)
		m0->m_pkthdr.len += siz;
	m->m_len -= remain; /* Trim */
	if (n2)	{
		for (n3 = n; n3->m_next != NULL; n3 = n3->m_next)
			;
		n3->m_next = n2;
	} else
		n3 = n;
	for (; n3->m_next != NULL; n3 = n3->m_next)
		;
	n3->m_next = m->m_next;
	m->m_next = n;
	return n;
}

/*
d1034 115
@


1.226
log
@On localhost a user program may create a socket splicing loop.
After writing data into this loop, it was spinning forever causing
a kernel hang.  Detect the loop by counting how often the same mbuf
is spliced.  If that happens 128 times, assume that there is a loop
and abort the splicing with ELOOP.
Bug found by tedu@@;  OK tedu@@ millert@@ benno@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.225 2016/05/23 15:22:44 tedu Exp $	*/
d108 1
a108 1
u_int	mclsizes[] = {
d182 2
a183 1
	int i;
d190 2
a191 2
		(void)pool_sethardlimit(&mclpools[i], nmbclust,
		    mclpool_warnmsg, 60);
d199 1
a199 1
		pool_sethiwat(&mclpools[i], nmbclust);
@


1.225
log
@remove the function pointer from mbufs. this memory is shared with data
via unions, and we don't want to make it easy to control the target.
instead an integer index into an array of acceptable functions is used.
drivers using custom functions must register them to receive an index.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.224 2016/04/15 05:05:21 dlg Exp $	*/
d268 1
d284 1
d1324 2
a1325 1
		(*pr)("m_pkthdr.ph_flowid: %u\n", m->m_pkthdr.ph_flowid);
@


1.224
log
@remove ml_filter, mq_filter, niq_filter.

theyre currently unused, so no functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.223 2016/04/08 03:13:38 dlg Exp $	*/
d133 2
d173 3
d339 1
a339 1
	MEXTADD(m, buf, pp->pr_size, M_EXTWR, m_extfree_pool, pp);
d422 13
d439 2
a440 1
		(*(m->m_ext.ext_free))(m->m_ext.ext_buf,
d1339 2
a1340 2
		(*pr)("m_ext.ext_free: %p\tm_ext.ext_arg: %p\n",
		    m->m_ext.ext_free, m->m_ext.ext_arg);
@


1.223
log
@add m_purge for freeing a list of mbufs linked via m_nextpkt

this tweaks m_freem so it returns the m_nextpkt from the mbuf it freed,
like how m_free returns the m_next from the mbuf it frees.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.222 2016/04/06 01:36:06 dlg Exp $	*/
a1398 29
struct mbuf *
ml_filter(struct mbuf_list *ml,
    int (*filter)(void *, const struct mbuf *), void *ctx)
{
	struct mbuf_list matches = MBUF_LIST_INITIALIZER();
	struct mbuf *m, *n;
	struct mbuf **mp;

	mp = &ml->ml_head;

	for (m = ml->ml_head; m != NULL; m = n) {
		n = m->m_nextpkt;
		if ((*filter)(ctx, m)) {
			*mp = n;
			ml_enqueue(&matches, m);
		} else {
			mp = &m->m_nextpkt;
			ml->ml_tail = m;
		}
	}

	/* fixup ml */
	if (ml->ml_head == NULL)
		ml->ml_tail = NULL;
	ml->ml_len -= ml_len(&matches);

	return (matches.ml_head); /* ml_dechain */
}

a1498 13
	mtx_leave(&mq->mq_mtx);

	return (m0);
}

struct mbuf *
mq_filter(struct mbuf_queue *mq,
    int (*filter)(void *, const struct mbuf *), void *ctx)
{
	struct mbuf *m0;

	mtx_enter(&mq->mq_mtx);
	m0 = ml_filter(&mq->mq_list, filter, ctx);
@


1.222
log
@correct the order of arguments to m_get in m_dup_pkt
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.221 2016/03/29 10:34:42 sashan Exp $	*/
d428 17
d446 1
a446 1
m_freem(struct mbuf *m)
d449 1
a449 1
		m = m_free(m);
@


1.221
log
@- packet must keep reference to statekey
  this is the second attempt to get it in, the first
  attempt got backed out on Jan 31 2016

  the change also contains fixes contributed by Stefan Kempf
  in earlier iteration.

OK srhen@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.220 2016/03/22 06:17:00 dlg Exp $	*/
d1244 1
a1244 1
	m = m_get(m0->m_type, wait);
@


1.220
log
@dont mix up the len and flats argument to MCLGETI in m_dup_pkt
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.219 2016/02/23 01:39:14 dlg Exp $	*/
d75 2
d98 4
d270 4
d363 1
a363 1
	if (m->m_flags & M_PKTHDR)
d365 4
d1218 4
@


1.219
log
@provide m_dup_pkt() for doing fast deep mbuf copies with a specified alignment

if a physical interface receives a multicast/broadcast packet and
has carp interfaces on it, that packet needs to be copied for
reception by each of those carp interfaces.

previously it was using m_copym2, but that doesn't respect the
alignment of the source packet. this meant the ip header in the
copies were aligned incorrectly for the network stack, which breaks
strict alignment archs.

m_dup_pkt lets carp specify that the payload needs an ETHER_ALIGN
adjustment, so the ip header inside will be aligned correctly.

reported and tested by anthony eden who hit this on armv7
i reproduced the problem on sparc64 and verified the fix on amd64
and sparc64
ok mpi@@ mikeb@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.218 2016/01/31 00:18:07 sashan Exp $	*/
d1234 1
a1234 1
		MCLGETI(m, len, NULL, wait);
@


1.219.2.1
log
@MFC argument order fix for MCLGETI/m_get, from Armin Wolfermann via dlg, ok mpi
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.222 2016/04/06 01:36:06 dlg Exp $	*/
d1226 1
a1226 1
	m = m_get(wait, m0->m_type);
d1234 1
a1234 1
		MCLGETI(m, wait, NULL, len);
@


1.219.2.2
log
@backport splice loop fix:
On localhost a user program may create a socket splicing loop.
After writing data into this loop, it was spinning forever causing
a kernel hang.  Detect the loop by counting how often the same mbuf
is spliced.  If that happens 128 times, assume that there is a loop
and abort the splicing with ELOOP.
Bug found by tedu@@;  OK tedu@@ millert@@ benno@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.219.2.1 2016/04/28 22:31:55 sthen Exp $	*/
a256 1
	u_int8_t loopcnt = m->m_pkthdr.ph_loopcnt;
a267 1
	m->m_pkthdr.ph_loopcnt = loopcnt;
d1268 1
a1268 2
		(*pr)("m_pkthdr.ph_flowid: %u\tm_pkthdr.ph_loopcnt: %u\n",
		    m->m_pkthdr.ph_flowid, m->m_pkthdr.ph_loopcnt);
@


1.218
log
@- m_pkthdr.pf.statekey changes are not ready for 5.9, I must back them out

OK sthen@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.216 2015/12/23 21:04:55 jasper Exp $	*/
d1214 34
@


1.217
log
@- retrying to commit earlier change, which got backed out

    - yet another tiny step towards MP PF. This time we need to make sure
      statekey attached to packet stays around, while accepted packet is
      routed through IP stack.

  this time I'm also bringing fix contributed by Stefan Kempf. Stefan's fix
  makes sure we grab reference in m_dup_pkthdr()

OK bluhm@@
@
text
@a74 2
#include "pf.h"

a87 3
#if NPF > 0
#include <net/pfvar.h>
#endif	/* NPF > 0 */
a263 4
#if NPF > 0
	pf_pkt_unlink_state_key(m);
#endif	/* NPF > 0 */

d353 1
a353 1
	if (m->m_flags & M_PKTHDR) {
a354 4
#if NPF > 0
		pf_pkt_unlink_state_key(m);
#endif	/* NPF > 0 */
	}
a1203 4

#if NPF > 0
	pf_pkt_state_key_ref(to);
#endif /* NPF > 0 */
@


1.216
log
@revert previous:
----------------------------------------------------------------------
revision 1.961
date: 2015/12/22 13:33:26;  author: sashan;  state: Exp;  lines: +153 -44;
commitid: oBRhtWcDV0ThviVT;
- yet another tiny step towards MP PF. This time we need to make sure
  statekey attached to packet stays around, while accepted packet is
  routed through IP stack.

OK mpi@@, henning@@
----------------------------------------------------------------------

there have been multiple reports of KASSERT(!pf_state_key_isvalid(sk)) being
triggered without much effort, so back this out for now.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.214 2015/11/21 11:46:24 mpi Exp $	*/
d75 2
d90 3
d269 4
d362 1
a362 1
	if (m->m_flags & M_PKTHDR)
d364 4
d1217 4
@


1.215
log
@- yet another tiny step towards MP PF. This time we need to make sure
  statekey attached to packet stays around, while accepted packet is
  routed through IP stack.

OK mpi@@, henning@@
@
text
@a74 2
#include "pf.h"

a87 3
#if NPF > 0
#include <net/pfvar.h>
#endif	/* NPF > 0 */
a263 4
#if NPF > 0
	pf_pkt_unlink_state_key(m);
#endif	/* NPF > 0 */

d353 1
a353 1
	if (m->m_flags & M_PKTHDR) {
a354 4
#if NPF > 0
		pf_pkt_unlink_state_key(m);
#endif	/* NPF > 0 */
	}
@


1.214
log
@Retire ml_requeue(9) and mq_requeue(9).

As Kenjiro Cho pointed out it is very hard to cancel a dequeue operation
for some queueing disciplines when such it keeps some internal states.

As you can see, APIs can also Live Fast & Die Young.

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.213 2015/11/13 10:12:39 mpi Exp $	*/
d75 2
d90 3
d269 4
d362 1
a362 1
	if (m->m_flags & M_PKTHDR)
d364 4
@


1.213
log
@Use ph_ prefix for tag-related fields.

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.212 2015/11/12 10:07:14 mpi Exp $	*/
a1317 13
void
ml_requeue(struct mbuf_list *ml, struct mbuf *m)
{
	if (ml->ml_tail == NULL)
		ml->ml_head = ml->ml_tail = m;
	else {
		m->m_nextpkt = ml->ml_head;
		ml->ml_head = m;
	}

	ml->ml_len++;
}

a1417 13
}

int
mq_requeue(struct mbuf_queue *mq, struct mbuf *m)
{
	int full;

	mtx_enter(&mq->mq_mtx);
	ml_requeue(&mq->mq_list, m);
	full = mq_len(mq) > mq->mq_maxlen;
	mtx_leave(&mq->mq_mtx);

	return (full);
@


1.212
log
@Prefix flowid with ph_ and print it in m_print().

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.211 2015/11/02 09:21:48 dlg Exp $	*/
d1205 1
a1205 1
	SLIST_INIT(&to->m_pkthdr.tags);
d1231 3
a1233 3
		(*pr)("m_ptkhdr.tags: %p\tm_pkthdr.tagsset: %b\n",
		    SLIST_FIRST(&m->m_pkthdr.tags),
		    m->m_pkthdr.tagsset, MTAG_BITS);
@


1.211
log
@provide ml_purge and mq_purge.

these are modelled on IF_PURGE or IFQ_PURGE. they m_freem all the
mbufs on an mbuf list or queue.

ok jmatthew@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.210 2015/10/30 19:47:40 bluhm Exp $	*/
d1234 1
@


1.210
log
@Let m_resethdr() clear the whole mbuf packet header, not only the
pf part.  This allows to reuse this function in socket splicing.
Reset the mbuf flags that are related to the packet header, but
preserve the data flags.
pair(4) tested by reyk@@; sosplice(9) tested by bluhm@@; OK mikeb@@ reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.209 2015/10/30 12:54:36 reyk Exp $	*/
d1371 17
d1500 10
@


1.209
log
@Add m_resethdr() to clear any state (pf, tags, flags) of an mbuf packet.
Start using it in pair(4) to clear state on the receiving interface;
m_resethdr() will also be used in other parts of the stack.

OK bluhm@@ mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.208 2015/10/22 05:26:06 dlg Exp $	*/
d256 4
a259 4
	/* like the previous, but keep any associated data and mbufs */
	m->m_flags = M_PKTHDR;
	memset(&m->m_pkthdr.pf, 0, sizeof(m->m_pkthdr.pf));
	m->m_pkthdr.pf.prio = IFQ_DEFPRIO;
d261 1
a261 1
	/* also delete all mbuf tags to reset the state */
d263 5
@


1.208
log
@rename ml_join to ml_enlist and expose it to the rest of the kernel.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.207 2015/08/14 05:25:29 dlg Exp $	*/
d251 12
@


1.207
log
@provide ml_requeue and mq_requeue for prepending mbufs on lists/queues

ok mpi@@ claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.206 2015/07/15 22:29:32 deraadt Exp $	*/
a1245 2
void ml_join(struct mbuf_list *, struct mbuf_list *);

d1268 1
a1268 1
ml_join(struct mbuf_list *mla, struct mbuf_list *mlb)
d1419 1
a1419 1
		ml_join(&mq->mq_list, ml);
@


1.206
log
@m_free() can now accept NULL, as a normal free() function.  This makes
calling code simpler.
ok stsp mpi
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.205 2015/06/16 11:09:39 mpi Exp $	*/
d1302 13
d1398 13
@


1.205
log
@Store a unique ID, an interface index, rather than a pointer to the
receiving interface in the packet header of every mbuf.

The interface pointer should now be retrieved when necessary with
if_get().  If a NULL pointer is returned by if_get(), the interface
has probably been destroy/removed and the mbuf should be freed.

Such mechanism will simplify garbage collection of mbufs and limit
problems with dangling ifp pointers.

Tested by jmatthew@@ and krw@@, discussed with many.

ok mikeb@@, bluhm@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.204 2015/05/31 20:10:44 bluhm Exp $	*/
d321 3
@


1.204
log
@If the first list was empty, ml_join() did not not clear the second
list after transferring all elements away.  Reorder the conditionals
to make sure that ml_init() is always called for a non empty second
list.  This makes all cases consistent and is less surprising.
OK dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.203 2015/04/13 08:45:48 mpi Exp $	*/
d1209 2
a1210 2
		(*pr)("m_ptkhdr.rcvif: %p\tm_pkthdr.len: %i\n",
		    m->m_pkthdr.rcvif, m->m_pkthdr.len);
@


1.203
log
@Now that if_input() set the receiving interface pointer on mbufs for us
there's no need to do it in m_devget(9).

Stop passing an ``ifp'' will help for upcoming interface pointer -> index
conversion.

While here remove unused ``ifp'' argument from m_clget(9) and kill two
birds^W layer violations in one commit.

ok henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.202 2015/03/14 03:38:51 jsg Exp $	*/
d1269 5
a1273 4
	if (mla->ml_tail == NULL)
		*mla = *mlb;
	else if (mlb->ml_tail != NULL) {
		mla->ml_tail->m_nextpkt = mlb->ml_head;
@


1.202
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.201 2015/02/07 02:52:09 dlg Exp $	*/
d281 1
a281 1
m_clget(struct mbuf *m, int how, struct ifnet *ifp, u_int pktlen)
d1027 1
a1027 1
m_devget(char *buf, int totlen, int off, struct ifnet *ifp)
a1042 1
	m->m_pkthdr.rcvif = ifp;
@


1.201
log
@make mq_enlist drop mbufs is the queues length is exceeded.

ok mpi@@ claudio@@ henning@@ and more at s2k15
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.200 2015/02/07 02:30:00 dlg Exp $	*/
a93 1
#include <ddb/db_interface.h>
@


1.200
log
@add ml_filter and mq_filter functions to the mbuf list and queue apis.

this lets you run a filter function against each mbuf on a list or
queue. if the filter matches on an mbuf, it can return non-zero to
have ml_filter or mq_filter remove the mbuf and return it as part
of a chain of mbufs.

ok mpi@@ claudio@@ henning@@ and s2k15 generally.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.199 2014/12/11 19:21:57 tedu Exp $	*/
d1388 2
a1389 1
	int full;
d1392 6
a1397 2
	ml_join(&mq->mq_list, ml);
	full = mq_len(mq) >= mq->mq_maxlen;
d1400 6
a1405 1
	return (full);
@


1.199
log
@convert bcopy to memcpy/memmove. ok krw
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.198 2014/11/05 00:28:15 dlg Exp $	*/
d1312 29
d1414 13
@


1.198
log
@change the mbuf pool wait channel name from mbpl to mbufpl. "mb"
isnt descriptive enough for me.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.197 2014/10/03 02:16:21 dlg Exp $	*/
d609 1
a609 1
		bcopy(mtod(m, caddr_t) + off, cp, count);
d662 1
a662 1
		bcopy(cp, mtod(m, caddr_t) + off, (size_t)mlen);
d715 2
a716 2
		bcopy(mtod(n, caddr_t), mtod(m, caddr_t) + m->m_len,
		    (u_int)n->m_len);
d844 2
a845 2
		bcopy(mtod(n, caddr_t), mtod(m, caddr_t) + m->m_len,
		    (unsigned)count);
d1015 1
a1015 1
		bcopy(mtod(m, caddr_t) + len, mtod(n, caddr_t), remain);
@


1.197
log
@if you're adding the first cluster reference, you dont have to
coordinate with other mbufs so you can add all the pointers without
taking the extref lock.

looks good deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.196 2014/10/03 01:11:17 dlg Exp $	*/
d147 1
a147 1
	pool_init(&mbpool, MSIZE, 0, 0, 0, "mbpl", NULL);
@


1.196
log
@i moved some macros into functions, and a trailing \ on a statement
snuck in.

someone who knows how cpp/cc works can explain to me why this
compiled.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.195 2014/10/03 01:02:47 dlg Exp $	*/
d347 2
d351 2
a352 1
	mtx_enter(&m_extref_mtx);
d357 2
a358 1
	mtx_leave(&m_extref_mtx);
@


1.195
log
@now that pools are mpsafe, we can make the mbuf allocators on top
of pools mpsafe too.

this calles pool_setipl(IPL_NET) against the mbuf and cluster pools,
and removes the use of splnet().

the other locking done in the mbuf layer is for external cluster
references. again, they relied on splnet to serialise these operations.
because there is no shared memory associated with external clusters
(except the cluster itself, which is completely dedicated to data
payload, not meta info like a refcount or lock), this has been
replaced with a single mutex that all reference ops are serialised
with.

tested by me, jmatthew@@, bcook@@, and phessler@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.194 2014/09/14 14:17:26 jsg Exp $	*/
d356 1
a356 1
	MCLREFDEBUGN((n), __FILE__, __LINE__);                  \
@


1.194
log
@remove uneeded proc.h includes
ok mpi@@ kspillner@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.193 2014/08/18 04:06:16 dlg Exp $	*/
d98 1
d121 2
d141 2
d148 1
d161 1
a192 15
void
m_reclaim(void *arg, int flags)
{
	struct domain *dp;
	struct protosw *pr;
	int s = splnet();

	for (dp = domains; dp; dp = dp->dom_next)
		for (pr = dp->dom_protosw; pr < dp->dom_protoswNPROTOSW; pr++)
			if (pr->pr_drain)
				(*pr->pr_drain)();
	mbstat.m_drain++;
	splx(s);
}

a199 1
	int s;
a200 1
	s = splnet();
d202 13
a214 10
	if (m)
		mbstat.m_mtypes[type]++;
	splx(s);
	if (m) {
		m->m_type = type;
		m->m_next = NULL;
		m->m_nextpkt = NULL;
		m->m_data = m->m_dat;
		m->m_flags = 0;
	}
a225 1
	int s;
a226 1
	s = splnet();
d228 10
a237 15
	if (m)
		mbstat.m_mtypes[type]++;
	splx(s);
	if (m) {
		m->m_type = type;

		/* keep in sync with m_inithdr */
		m->m_next = NULL;
		m->m_nextpkt = NULL;
		m->m_data = m->m_pktdat;
		m->m_flags = M_PKTHDR;
		memset(&m->m_pkthdr, 0, sizeof(m->m_pkthdr));
		m->m_pkthdr.pf.prio = IFQ_DEFPRIO;
	}
	return (m);
a286 1
	int s;
a293 1
	s = splnet();
d295 2
a296 3
		MGETHDR(m0, M_DONTWAIT, MT_DATA);
		if (m0 == NULL) {
			splx(s);
d298 1
a298 1
		}
a304 1
		splx(s);
a306 1
	splx(s);
a314 1
	splassert(IPL_NET);
d319 1
a319 1
m_free_unlocked(struct mbuf *m)
d323 1
d325 2
d338 1
d344 2
a345 2
struct mbuf *
m_free(struct mbuf *m)
d347 1
a347 2
	struct mbuf *n;
	int s;
d349 6
a354 3
	s = splnet();
	n = m_free_unlocked(m);
	splx(s);
d356 1
a356 1
	return (n);
d359 2
a360 2
void
m_extfree(struct mbuf *m)
d362 6
d373 11
a383 1
	} else if (m->m_ext.ext_free)
d386 2
a387 3
	else
		panic("unknown type of extension buffer");
	m->m_ext.ext_size = 0;
d394 2
a395 10
	struct mbuf *n;
	int s;

	if (m == NULL)
		return;
	s = splnet();
	do {
		n = m_free_unlocked(m);
	} while ((m = n) != NULL);
	splx(s);
d431 1
a431 3

	if (m->m_flags & M_EXT) {
		int s = splnet();
a432 2
		splx(s);
	}
@


1.193
log
@introduce mbuf list and queue apis. both manage fifo lists of mbufs
and a count of the mbufs.

struct mbuf_list and the ml_foo() apis can be used to build lists of
mbufs where you dont need locking (eg, on the stack).

struct mbuf_queue and mq_foo() wrap mbuf_lists with a mutex, and
limits the number of mbufs that can be queued. they can be useful
for moving mbufs between contexts/subsystems.

with help from jmc@@ for the manpage bits
mpi@@ is keen
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.192 2014/07/13 15:52:38 tedu Exp $	*/
a76 1
#include <sys/proc.h>
@


1.192
log
@bzero -> memset. for the speeds.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.191 2014/07/13 09:52:48 dlg Exp $	*/
d1244 1
d1248 149
@


1.191
log
@treat external storage allocated by the mbuf layer the same as
external storage attached to an mbuf anywhere else. this means it
uses MEXTADD to wire it up to the mbuf, and it relies on the ext_free
and ext_arg bits in the header to call the right free function
against the right pool.

M_CLUSTER gets renamed to M_EXTWR. the type field in MEXTADD gets
reused as a flags field so anything attaching storage to an mbuf
can say if it is writable or not.

ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.190 2014/07/09 13:05:45 dlg Exp $	*/
d249 1
a249 1
		bzero(&m->m_pkthdr, sizeof(m->m_pkthdr));
d263 1
a263 1
	bzero(&m->m_pkthdr, sizeof(m->m_pkthdr));
d658 1
a658 1
			bzero(mtod(n, caddr_t), off);
@


1.190
log
@ext_type is set but never read. its a waste of space.

MEXTADD will be fixed later.

ok henning@@ deraadt@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.189 2014/07/09 11:22:53 dlg Exp $	*/
d115 1
a115 1
int	m_clpool(u_int);
d281 1
a281 1
int
d284 1
d287 4
a290 3
	for (pi = 0; pi < MCLPOOLS; pi++) {
		if (pktlen <= mclsizes[pi])
			return (pi);
d293 1
a293 1
	return (-1);
d300 2
a301 1
	int pi;
d304 1
a304 1
	pi = m_clpool(pktlen);
d306 1
a306 1
	if (pi == -1)
d319 2
a320 3
	m->m_ext.ext_buf = pool_get(&mclpools[pi],
	    how == M_WAIT ? PR_WAITOK : PR_NOWAIT);
	if (!m->m_ext.ext_buf) {
d328 1
a328 6
	m->m_data = m->m_ext.ext_buf;
	m->m_flags |= M_EXT|M_CLUSTER;
	m->m_ext.ext_size = mclpools[pi].pr_size;
	m->m_ext.ext_free = NULL;
	m->m_ext.ext_arg = &mclpools[pi];
	MCLINITREFERENCE(m);
d332 7
a381 2
	} else if (m->m_flags & M_CLUSTER) {
		pool_put(m->m_ext.ext_arg, m->m_ext.ext_buf);
d388 1
a388 1
	m->m_flags &= ~(M_EXT|M_CLUSTER);
d454 1
a454 1
		m->m_flags |= M_EXT|M_CLUSTER;
d462 1
a462 1
	m0->m_flags &= ~(M_EXT|M_CLUSTER);	/* cluster is gone */
d1191 1
a1191 1
	to->m_flags = (to->m_flags & (M_EXT | M_CLUSTER));
@


1.189
log
@now that mclgeti doesnt do the rx ring accounting, cluster allocation
doesnt need to know which ifp an mbuf was allocated on so it can uncount
it on free.

while here, remove the ext_backend field which identifies the pool the
cluster came from and use ext_arg instead.

ok henning@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.188 2014/07/08 07:10:12 dlg Exp $	*/
a1237 1
		(*pr)("m_ext.ext_type: %x\n", m->m_ext.ext_type);
@


1.188
log
@pull the rx ring accounting out of the mbuf layer now that its all done
via if_rxring things. this effectively deprecates the third argument
for MCLGETI and m_clget and makes the mbuf layer no longer care about
interfaces and simplifies the allocation paths.

the timeout used to measure livelock has been moved to net/if.c.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.187 2014/07/08 05:35:19 dlg Exp $	*/
d330 1
a330 3
	m->m_ext.ext_arg = NULL;
	m->m_ext.ext_backend = pi;
	m->m_ext.ext_ifidx = 0;
d379 1
a379 2
		pool_put(&mclpools[m->m_ext.ext_backend],
		    m->m_ext.ext_buf);
d1238 1
a1238 3
		(*pr)("m_ext.ext_type: %x\tm_ext.ext_backend: %i\n",
		    m->m_ext.ext_type, m->m_ext.ext_backend);
		(*pr)("m_ext.ext_ifidx: %u\n", m->m_ext.ext_ifidx);
@


1.187
log
@cut things that relied on mclgeti for rx ring accounting/restriction over
to using if_rxr.

cut the reporting systat did over to the rxr ioctl.

tested as much as i can on alpha, amd64, and sparc64.
mpi@@ has run it on macppc.
ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.186 2014/06/18 11:09:58 dlg Exp $	*/
a120 4
struct timeout m_cltick_tmo;
int	m_clticks;
void	m_cltick(void *);

a160 3

	timeout_set(&m_cltick_tmo, m_cltick, NULL);
	m_cltick(NULL);
a293 128
u_int mcllivelocks;

void
m_clinitifp(struct ifnet *ifp)
{
	struct mclpool *mclp = ifp->if_data.ifi_mclpool;
	int i;

	/* Initialize high water marks for use of cluster pools */
	for (i = 0; i < MCLPOOLS; i++) {
		mclp = &ifp->if_data.ifi_mclpool[i];

		if (mclp->mcl_lwm == 0)
			mclp->mcl_lwm = 2;
		if (mclp->mcl_hwm == 0)
			mclp->mcl_hwm = 32768;

		mclp->mcl_cwm = MAX(4, mclp->mcl_lwm);
		mclp->mcl_livelocks = mcllivelocks;
	}
}

void
m_clsetwms(struct ifnet *ifp, u_int pktlen, u_int lwm, u_int hwm)
{
	int pi;

	pi = m_clpool(pktlen);
	if (pi == -1)
		return;

	ifp->if_data.ifi_mclpool[pi].mcl_lwm = lwm;
	ifp->if_data.ifi_mclpool[pi].mcl_hwm = hwm;
}

/*
 * Record when the last timeout has been run.  If the delta is
 * too high, m_cldrop() will notice and decrease the interface
 * high water marks.
 */
void
m_cltick(void *arg)
{
	extern int ticks;

	if (ticks - m_clticks > 1)
		mcllivelocks++;

	m_clticks = ticks;
	timeout_add(&m_cltick_tmo, 1);
}

int m_livelock;

int
m_cldrop(struct ifnet *ifp, int pi)
{
	static int liveticks;
	struct mclpool *mclp;
	extern int ticks;
	u_int diff, adj;

	if (ticks - m_clticks > 1) {
		/*
		 * Timeout did not run, so we are in some kind of livelock.
		 *
		 * Increase the livelock counter to tell the interfaces to
		 * decrease their cluster allocation high water marks.
		 */
		m_livelock = 1;
		m_clticks = liveticks = ticks;
	} else if (m_livelock && (ticks - liveticks) > 4)
		m_livelock = 0;	/* Let the high water marks grow again */

	mclp = &ifp->if_data.ifi_mclpool[pi];

	/*
	 * If at least a livelock happened since the last time a cluster
	 * was requested, decrease its pool allocation high water mark to
	 * prevent it from growth for the very near future.
	 *
	 * The decrease is proportional to the number of livelocks since
	 * the last request for a given pool.
	 */
	adj = mcllivelocks - mclp->mcl_livelocks;
	if (adj != 0) {
		diff = max((mclp->mcl_cwm / 8) * adj, 2);
		mclp->mcl_cwm = max(mclp->mcl_lwm, mclp->mcl_cwm - diff);
	}
	mclp->mcl_livelocks = mcllivelocks;

	if (m_livelock == 0 && ISSET(ifp->if_flags, IFF_RUNNING) &&
	    mclp->mcl_alive <= 4 && mclp->mcl_cwm < mclp->mcl_hwm &&
	    mclp->mcl_grown - ticks < 0) {
		/* About to run out, so increase the current watermark */
		mclp->mcl_cwm++;
		mclp->mcl_grown = ticks;
	} else if (mclp->mcl_alive >= mclp->mcl_cwm)
		return (1);		/* No more packets given */

	return (0);
}

void
m_clcount(struct ifnet *ifp, int pi)
{
	ifp->if_data.ifi_mclpool[pi].mcl_alive++;
}

void
m_cluncount(struct mbuf *m, int all)
{
	struct mbuf_ext *me;
	struct ifnet *ifp;

	do {
		me = &m->m_ext;
		if (((m->m_flags & (M_EXT|M_CLUSTER)) != (M_EXT|M_CLUSTER)) ||
		    (me->ext_ifidx == 0))
			continue;

		ifp = if_get(me->ext_ifidx);
		if (ifp != NULL)
			ifp->if_data.ifi_mclpool[me->ext_backend].mcl_alive--;
		me->ext_ifidx = 0;
	} while (all && (m = m->m_next));
}

a307 6

	if (ifp != NULL && m_cldrop(ifp, pi)) {
		splx(s);
		return (NULL);
	}

a323 2
	if (ifp != NULL)
		m_clcount(ifp, pi);
d332 1
a332 4
	if (ifp != NULL)
		m->m_ext.ext_ifidx = ifp->if_index;
	else
		m->m_ext.ext_ifidx = 0;
a380 1
		m_cluncount(m, 0);
@


1.186
log
@trailing tabs arent needed.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.185 2014/06/13 07:28:13 mpi Exp $	*/
d346 3
a370 1
		mcllivelocks++;
@


1.185
log
@Instead of updating all the cluster allocation water marks of all the
interfaces when the kernel is livelocked, only do it for the current
pool and defer the other updates.

This allow us to get rid of an interface list iteration in a critical
path.

Ridding the libc crank since this change introduce an ABI break.

ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.184 2014/05/04 19:27:08 sf Exp $	*/
d454 1
a454 1
	}			
@


1.184
log
@remove some hh and h format specifiers

gcc does not like hh in kprintf and it does not make any difference due
to integer promotion rules. While there, remove some h's, too.

suggested by kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.183 2014/04/22 14:41:03 mpi Exp $	*/
d301 2
d319 1
a350 1
u_int mcllivelocks;
d358 1
a358 1
	int i;
a360 2
		struct ifnet *aifp;

d363 3
a365 3
		 * Decrease the cluster allocation high water marks on all
		 * interfaces and prevent them from growth for the very near
		 * future.
a369 8
		TAILQ_FOREACH(aifp, &ifnet, if_list) {
			mclp = aifp->if_data.ifi_mclpool;
			for (i = 0; i < MCLPOOLS; i++) {
				int diff = max(mclp[i].mcl_cwm / 8, 2);
				mclp[i].mcl_cwm = max(mclp[i].mcl_lwm,
				    mclp[i].mcl_cwm - diff);
			}
		}
d374 16
@


1.183
log
@Remove some altq tentacles.

ok pelikan@@, henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.182 2014/04/21 11:10:54 henning Exp $	*/
d1352 1
a1352 1
	(*pr)("m_type: %hi\tm_flags: %hb\n", m->m_type, m->m_flags, M_BITS);
d1359 1
a1359 1
		(*pr)("m_ptkhdr.tags: %p\tm_pkthdr.tagsset: %hb\n",
d1362 1
a1362 1
		(*pr)("m_pkthdr.csum_flags: %hb\n",
d1364 1
a1364 1
		(*pr)("m_pkthdr.ether_vtag: %hu\tm_ptkhdr.ph_rtableid: %u\n",
d1368 1
a1368 1
		(*pr)("m_pkthdr.pf.qid: %u\tm_pkthdr.pf.tag: %hu\n",
d1370 1
a1370 1
		(*pr)("m_pkthdr.pf.flags: %hhb\n",
d1372 1
a1372 1
		(*pr)("m_pkthdr.pf.routed: %hhu\tm_pkthdr.pf.prio: %hhu\n",
@


1.182
log
@we'll do fine without casting NULL to struct foo * / void *
ok gcc & md5 (alas, no binary change)
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.180 2014/03/28 17:57:11 mpi Exp $	*/
a598 1
	m->m_pkthdr.pf.hdr = NULL;	/* altq will cope */
a1365 2
		(*pr)("m_pkthdr.pf.hdr: %p\n",
		    m->m_pkthdr.pf.hdr);
@


1.181
log
@"struct pkthdr" holds a routing table ID, not a routing domain one.
Avoid the confusion by using an appropriate name for the variable.

Note that since routing domain IDs are a subset of the set of routing
table IDs, the following idiom is correct:

	rtableid = rdomain

But to get the routing domain ID corresponding to a given routing table
ID, you must call rtable_l2(9).

claudio@@ likes it, ok mikeb@@
@
text
@d225 2
a226 2
		m->m_next = (struct mbuf *)NULL;
		m->m_nextpkt = (struct mbuf *)NULL;
d252 2
a253 2
		m->m_next = (struct mbuf *)NULL;
		m->m_nextpkt = (struct mbuf *)NULL;
d266 2
a267 2
	m->m_next = (struct mbuf *)NULL;
	m->m_nextpkt = (struct mbuf *)NULL;
@


1.180
log
@Reduce uvm include madness.  Use <uvm/uvm_extern.h> instead of
<uvm/uvm.h> if possible and remove double inclusions.

ok beck@@, mlarkin@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.179 2014/03/27 10:30:58 mpi Exp $	*/
d1365 2
a1366 2
		(*pr)("m_pkthdr.ether_vtag: %hu\tm_ptkhdr.rdomain: %u\n",
		    m->m_pkthdr.ether_vtag, m->m_pkthdr.rdomain);
@


1.179
log
@Store an ifp index instead of a pointer in the "struct mbuf_ext".

This is part of the plan to remove the ifp pointer from the packet
header that will allow us to stop garbage collecting mbuf(9)s when
an ifp is detached/destroyed.

ok mikeb@@, lteo@@, benno@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.178 2014/01/19 03:04:54 claudio Exp $	*/
a90 1
#include <uvm/uvm.h>
@


1.178
log
@Remove max_datalen. It is only used once an can be replaced easily with
MHLEN - max_hdr in that place. OK mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.177 2014/01/10 00:47:17 bluhm Exp $	*/
d405 1
d410 1
a410 1
		    (me->ext_ifp == NULL))
d413 4
a416 2
		me->ext_ifp->if_data.ifi_mclpool[me->ext_backend].mcl_alive--;
		me->ext_ifp = NULL;
d466 4
a469 1
	m->m_ext.ext_ifp = ifp;
d1384 1
a1384 1
		(*pr)("m_ext.ext_ifp: %p\n", m->m_ext.ext_ifp);
@


1.177
log
@Let "ddb show mbuf" print all mbuf fields in a consistent way.  Move
bit field names into the header file below the definitions to keep
them in sync.
OK mikeb@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.176 2013/11/09 06:38:42 dlg Exp $	*/
a120 1
int max_datalen;		/* MHLEN - max_hdr */
@


1.176
log
@ticks is compared against mcl_grown to see if time has elapsed since
the rx ring was last allowed to grow and then assigned to it. it
is erroneous to do this because mcl_grown is a u_int and ticks is an
int.

this makes mcl_grown an int, and follows the idiom in kern_timeout.c
of going "thing - ticks < diff", which better copes with ticks
wrapping around and being used to calculate relative intervals.

ok pirofti@@ guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.175 2013/08/21 05:21:45 dlg Exp $	*/
d1349 1
a1349 4
	(*pr)("m_type: %hi\tm_flags: %b\n", m->m_type, m->m_flags,
	    "\20\1M_EXT\2M_PKTHDR\3M_EOR\4M_CLUSTER\5M_PROTO1\6M_VLANTAG"
	    "\7M_LOOP\10M_FILDROP\11M_BCAST\12M_MCAST\13M_CONF\14M_AUTH"
	    "\15M_TUNNEL\16M_ZEROIZE\17M_LINK0");
d1352 1
a1352 1
	(*pr)("m_dat: %p m_pktdat: %p\n", m->m_dat, m->m_pktdat);
d1354 14
a1367 14
		(*pr)("m_pkthdr.len: %i\tm_ptkhdr.rcvif: %p\t"
		    "m_ptkhdr.rdomain: %u\n", m->m_pkthdr.len,
		    m->m_pkthdr.rcvif, m->m_pkthdr.rdomain);
		(*pr)("m_ptkhdr.tags: %p\tm_pkthdr.tagsset: %hx\n",
		    SLIST_FIRST(&m->m_pkthdr.tags), m->m_pkthdr.tagsset);
		(*pr)("m_pkthdr.csum_flags: %hx\tm_pkthdr.ether_vtag: %hu\n",
		    m->m_pkthdr.csum_flags, m->m_pkthdr.ether_vtag);
		(*pr)("m_pkthdr.pf.flags: %b\n",
		    m->m_pkthdr.pf.flags, "\20\1GENERATED\2FRAGCACHE"
		    "\3TRANSLATE_LOCALHOST\4DIVERTED\5DIVERTED_PACKET"
		    "\6PF_TAG_REROUTE");
		(*pr)("m_pkthdr.pf.hdr: %p\tm_pkthdr.pf.statekey: %p\n",
		    m->m_pkthdr.pf.hdr, m->m_pkthdr.pf.statekey);
		(*pr)("m_pkthdr.pf.qid:\t%u m_pkthdr.pf.tag: %hu\n",
d1369 4
a1372 3
		(*pr)("m_pkthdr.pf.prio:\t%u m_pkthdr.pf.tag: %hu\n",
		    m->m_pkthdr.pf.prio, m->m_pkthdr.pf.tag);
		(*pr)("m_pkthdr.pf.routed: %hx\n", m->m_pkthdr.pf.routed);
@


1.175
log
@get rid of the copy argument in m_devget that let you provide an
alternative to bcopy since noone uses it.

while there use memcpy instead of bcopy because we know the memory cannot
overlap.

ok henning@@ matthew@@ mikeb@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.174 2013/08/08 23:25:06 syl Exp $	*/
d386 1
a386 1
	    mclp->mcl_grown < ticks) {
@


1.174
log
@Uncomment kprintf format attributes for sys/kern

tested on vax (gcc3) ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.173 2013/06/11 13:29:50 dlg Exp $	*/
d1171 1
a1171 2
m_devget(char *buf, int totlen, int off, struct ifnet *ifp,
    void (*copy)(const void *, void *, size_t))
d1226 1
a1226 5

		if (copy)
			copy(buf, mtod(m, caddr_t), (size_t)len);
		else
			bcopy(buf, mtod(m, caddr_t), (size_t)len);
@


1.173
log
@replace bcopy with memcpy in m_defrag. this is safe because the
memory we're copying between is guaranteed to be non-overlapping
since the target is newly allocated.

ok kettenis@@ henning@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.172 2013/06/11 01:01:15 dlg Exp $	*/
d1349 1
a1349 1
    int (*pr)(const char *, ...) /* __attribute__((__format__(__kprintf__,1,2))) */)
@


1.172
log
@return ENOBUFS on failure from m_defrag and m_tag_copy_chain.

m_foo functions that return errors are now consistent as far as i can tell.

ok bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.171 2013/03/28 16:55:25 deraadt Exp $	*/
d586 1
a586 1
		bcopy(&m0->m_ext, &m->m_ext, sizeof(struct mbuf_ext));
d592 1
a592 1
		bcopy(m0->m_data, m->m_data, m0->m_len);
@


1.171
log
@do not include machine/cpu.h from a .c file; it is the responsibility of
.h files to pull it in, if needed
ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.170 2013/02/17 17:39:29 miod Exp $	*/
d551 1
a551 1
		return 0;
d559 1
a559 1
		return -1;
d564 1
a564 1
			return -1;
d600 1
a600 1
	return 0;
@


1.170
log
@Comment out recently added __attribute__((__format__(__kprintf__))) annotations
in MI code; gcc 2.95 does not accept such annotation for function pointer
declarations, only function prototypes.
To be uncommented once gcc 2.95 bites the dust.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.169 2013/02/09 20:56:35 miod Exp $	*/
a89 1
#include <machine/cpu.h>
@


1.169
log
@Add explicit __attribute__ ((__format__(__kprintf__)))) to the functions and
function pointer arguments which are {used as,} wrappers around the kernel
printf function.
No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.168 2013/02/07 11:06:42 mikeb Exp $	*/
d1350 1
a1350 1
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))))
@


1.168
log
@convert mbuf tags to use pool(9) as a backend storage;
ok markus claudio haesbaert henning
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.167 2012/09/26 14:53:23 markus Exp $	*/
d1349 2
a1350 1
m_print(void *v, int (*pr)(const char *, ...))
@


1.167
log
@add M_ZEROIZE as an mbuf flag, so copied PFKEY messages (with embedded keys)
are cleared as well; from hshoexer@@, feedback and ok bluhm@@, ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.166 2012/04/13 09:38:32 deraadt Exp $	*/
d102 1
d153 4
@


1.166
log
@unneccessary casts to unsigned; ok claudio
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.165 2011/12/02 10:55:46 dlg Exp $	*/
d130 1
d471 7
a481 1
	n = m->m_next;
d1193 5
a1239 1
	while (m) {
d1241 2
a1242 2
		if (M_READONLY(m))
			panic("m_zero: M_READONLY");
d1244 8
a1251 9
		if (m->m_flags & M_EXT)
			memset(m->m_ext.ext_buf, 0, m->m_ext.ext_size);
		else {
			if (m->m_flags & M_PKTHDR)
				memset(m->m_pktdat, 0, MHLEN);
			else
				memset(m->m_dat, 0, MLEN);
		}
		m = m->m_next;
d1352 1
a1352 1
	    "\15M_TUNNEL\16M_AUTH_AH\17M_LINK0");
@


1.165
log
@dont put MAXMCLBYTES in the mclsizes array with a comment saying its 64k
when the macro can change without automatically fixing the comment.

instead add a diagnostic that checks that the biggest cluster size is
always MAXMCLBYTES.

requested by and ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.164 2011/11/30 10:26:56 dlg Exp $	*/
d698 1
a698 1
				    (unsigned)n->m_len);
d702 1
a702 1
			    (unsigned)n->m_len);
@


1.164
log
@this diff introduces the MAXMCLBYTES macro to describe the largest
cluster the generic network stack will be able to give you.

it also recognises that external storage on an mbuf may be bigger than
MCLBYTES. its only when m_pullup or m_pulldown need to allocate
another cluster that they now check the len argument, and now they
do it against MAXMCLBYTES.

this is required for me to do pfsync on jumbo frames as the m_pulldown
for the subregions fail beyond MCLBYTES into the packet.

ok deraadt@@ mikeb@@ henning@@ blambert@@
manpage changes ok jmc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.163 2011/11/30 01:16:09 dlg Exp $	*/
d111 1
a111 1
	MAXMCLBYTES	/* 64k */
d142 5
@


1.163
log
@correctly calculate the space available in external storage in m_pullup.

ok deraadt@@ claudio@@ blambert@@ mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.162 2011/11/29 10:39:11 dlg Exp $	*/
d111 1
a111 1
	64 * 1024
d952 1
a952 1
		if (len > MCLBYTES)
d958 1
a958 1
			MCLGET(m, M_DONTWAIT);
@


1.162
log
@whitespace fixes. no binary change.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.161 2011/09/18 23:20:38 miod Exp $	*/
d937 2
a938 2
	if ((n->m_flags & M_EXT) == 0 &&
	    n->m_data + len < &n->m_dat[MLEN] && n->m_next) {
d944 2
a945 2
	} else if ((n->m_flags & M_EXT) != 0 && len > MHLEN &&
	    n->m_data + len < &n->m_data[MCLBYTES] && n->m_next) {
@


1.161
log
@One more %hh format string.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.160 2011/07/08 18:48:50 henning Exp $	*/
d152 1
a152 1
		pool_set_constraints(&mclpools[i], &kp_dma_contig); 
d286 1
a286 1
                if (pktlen <= mclsizes[pi])
d926 2
a927 2
struct mbuf *   
m_pullup(struct mbuf *n, int len)       
d1016 1
a1016 1
	      			m = m->m_next;
d1041 1
a1041 1
	        return (NULL);
d1048 2
a1049 2
	        if ((m->m_next) && (M_LEADINGSPACE(m->m_next) >= siz)) {
		        m->m_next->m_len += siz;
d1056 1
a1056 1
	        n2 = m_copym2(m, len, remain, wait);
d1058 1
a1058 1
		        return (NULL);
d1063 2
a1064 2
	        if (n2)
		        m_freem(n2);
d1073 2
a1074 2
	        for (n3 = n; n3->m_next != NULL; n3 = n3->m_next)
		        ;
d1077 1
a1077 1
	        n3 = n;
d1079 1
a1079 1
	        ;
d1343 1
a1343 1
		    "m_ptkhdr.rdomain: %u\n", m->m_pkthdr.len, 
@


1.160
log
@new priority queueing implementation, extremely low overhead, thus fast.
unconditional, always on. 8 priority levels, as every better switch, the
vlan header etc etc. ok ryan mpf sthen, pea tested as well
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.157 2011/05/04 16:05:49 blambert Exp $	*/
d1359 1
a1359 1
		(*pr)("m_pkthdr.pf.routed: %hhx\n", m->m_pkthdr.pf.routed);
@


1.159
log
@Minor cleanup. OK blambert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.158 2011/06/23 21:42:05 ariane Exp $	*/
d249 1
d263 1
d1357 2
@


1.158
log
@Make mbufs and dma_alloc be contig allocations.
Requested by dlg@@

ok oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.157 2011/05/04 16:05:49 blambert Exp $	*/
a870 1
		m = mp;
d872 1
a872 1
			m->m_pkthdr.len -= (req_len - len);
@


1.157
log
@Collapse m_pullup and m_pullup2 into a single function, as they're
essentially identical; the only difference being that m_pullup2 is
capable of handling mbuf clusters, but called m_pullup for shorter
lengths (!).

testing dlg@@ ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.156 2011/04/18 19:23:46 art Exp $	*/
d144 1
a144 1
	pool_set_constraints(&mbpool, &kp_dma);
d152 1
a152 1
		pool_set_constraints(&mclpools[i], &kp_dma); 
@


1.156
log
@Put back the change of pool and malloc into the new km_alloc(9) api.
The problems during the hackathon were not caused by this (most likely).

prodded by deraadt@@ and beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.155 2011/04/11 13:10:13 claudio Exp $	*/
d920 3
a922 3
 * Rearange an mbuf chain so that len bytes are contiguous
 * and in the data area of an mbuf (so that mtod and dtom
 * will work for a structure of size len).  Returns the resulting
a923 2
 * If there is room, it will add up to max_protohdr-len extra bytes to the
 * contiguous region in an attempt to avoid being called next time.
d925 2
a926 2
struct mbuf *
m_pullup(struct mbuf *n, int len)
a929 2
	int space;
	int s;
d943 1
a943 56
	} else {
		if (len > MHLEN)
			goto bad;
		MGET(m, M_DONTWAIT, n->m_type);
		if (m == NULL)
			goto bad;
		m->m_len = 0;
		if (n->m_flags & M_PKTHDR)
			M_MOVE_PKTHDR(m, n);
	}
	space = &m->m_dat[MLEN] - (m->m_data + m->m_len);
	s = splnet();
	do {
		count = min(min(max(len, max_protohdr), space), n->m_len);
		bcopy(mtod(n, caddr_t), mtod(m, caddr_t) + m->m_len,
		    (unsigned)count);
		len -= count;
		m->m_len += count;
		n->m_len -= count;
		space -= count;
		if (n->m_len)
			n->m_data += count;
		else
			n = m_free_unlocked(n);
	} while (len > 0 && n);
	if (len > 0) {
		(void)m_free_unlocked(m);
		splx(s);
		goto bad;
	}
	splx(s);
	m->m_next = n;
	return (m);
bad:
	m_freem(n);
	return (NULL);
}

/*
 * m_pullup2() works like m_pullup, save that len can be <= MCLBYTES.
 * m_pullup2() only works on values of len such that MHLEN < len <= MCLBYTES,
 * it calls m_pullup() for values <= MHLEN.  It also only coagulates the
 * requested number of bytes.  (For those of us who expect unwieldy option
 * headers.
 *
 * KEBE SAYS:  Remember that dtom() calls with data in clusters does not work!
 */
struct mbuf *   
m_pullup2(struct mbuf *n, int len)       
{
	struct mbuf *m;
	int count;

	if (len <= MHLEN)
		return m_pullup(n, len);
	if ((n->m_flags & M_EXT) != 0 &&
d956 6
a961 4
		MCLGET(m, M_DONTWAIT);
		if ((m->m_flags & M_EXT) == 0) {
			m_free(m);
			goto bad;
d964 2
a965 8
		if (n->m_flags & M_PKTHDR) {
			/* Too many adverse side effects. */
			/* M_MOVE_PKTHDR(m, n); */
			m->m_flags = (n->m_flags & M_COPYFLAGS) |
			    M_EXT | M_CLUSTER;
			M_MOVE_HDR(m, n);
			/* n->m_data is cool. */
		}
@


1.155
log
@Move the DDB includes up in the file so that the m_print() prototype is
defined in all of uipc_mbuf.c. I use this function a lot for quick
printf debugging.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.154 2011/04/10 23:25:02 bluhm Exp $	*/
d144 1
a144 1
	pool_set_constraints(&mbpool, &dma_constraint, 1);
d152 1
a152 1
		pool_set_constraints(&mclpools[i], &dma_constraint, 1); 
@


1.154
log
@Backout m_split_mbuf() from revision 1.150.  It seems that m_split()
got broken.  Most /usr/src/regress/sys/kern/splice/args-oobinline-*
regression tests fail when they split an mbuf at out-of-band data.
ok claudio@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.153 2011/04/06 15:52:13 art Exp $	*/
d95 5
a1389 3
#include <machine/db_machdep.h>
#include <ddb/db_interface.h>

@


1.153
log
@Backout the uvm_km_getpage -> km_alloc conversion. Weird things are happening
and we aren't sure what's causing them.

shouted oks by many before I even built a kernel with the diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.152 2011/04/05 11:48:28 blambert Exp $	*/
a112 2
struct mbuf *m_split_mbuf(struct mbuf *, int, int, struct mbuf *);

d1082 4
a1085 4
 * Inject a new mbuf chain of length len in mbuf chain m at
 * position off. Returns a pointer to the first injected mbuf, or
 * NULL on failure (m is left undisturbed). Note that if there is
 * enough space for an object of size len in the appropriate position,
d1087 1
a1087 1
 * the first off bytes (pointers to that will remain valid).
d1089 1
a1089 1
 * XXX It is assumed that len is less than the size of an mbuf at the moment.
d1092 1
a1092 1
m_inject(struct mbuf *m, int off, int len, int wait)
d1094 2
a1095 2
	struct mbuf *n, *o, *p;
	int off1;
d1097 5
a1101 7
	if (len > MHLEN || len <= 0)
		return (NULL);

	if ((n = m_getptr(m, off, &off1)) == NULL)
		return (NULL);

	if ((o = m_get(wait, MT_DATA)) == NULL)
d1103 14
d1118 4
a1121 2
	if ((p = m_split_mbuf(n, off1, wait, NULL)) == NULL) {
		m_freem(o);
d1125 15
a1139 8
	o->m_len = len;
	o->m_next = p;
	n->m_next = o;

	if (m->m_flags & M_PKTHDR)
		m->m_pkthdr.len += len;

	return (o);
d1143 3
a1145 1
 * Split a single mbuf, leaving the chain intact.
d1148 1
a1148 1
m_split_mbuf(struct mbuf *m, int off, int wait, struct mbuf *mhdr)
d1150 2
a1151 2
	struct mbuf *n;
	int copyhdr;
d1153 3
a1155 1
	if (off > m->m_len)
d1157 7
a1163 14

	copyhdr = (mhdr && mhdr->m_flags & M_PKTHDR);

	if (copyhdr)
		n = m_gethdr(wait, MT_DATA);
	else
		n = m_get(wait, MT_DATA);
	if (!n)
		return (NULL);

	if (m->m_len - off > (copyhdr ? MHLEN : MLEN)) {
		MCLGET(n, wait);
		if (!(n->m_flags & M_EXT)) {
			m_free(n);
d1166 26
d1193 7
a1199 4

	if (copyhdr && m_dup_pkthdr(mhdr, n, wait)) {
		m_free(n);
		return (NULL);
d1201 2
a1202 5

	bcopy(mtod(m, caddr_t) + off, mtod(n, caddr_t), m->m_len - off);

	n->m_len = m->m_len - off;
	m->m_len = off;
d1204 1
a1204 2
	m->m_next = n;

a1205 27
}

/*
 * Break mbuf chain into two parts at the specified offset.
 */
struct mbuf *
m_split(struct mbuf *m, int off, int wait)
{
	struct mbuf *n, *o;
	int off1, copyhdr;

	copyhdr = m->m_flags & M_PKTHDR;

	if ((n = m_getptr(m, off, &off1)) == NULL)
		return (NULL);

	if ((o = m_split_mbuf(n, off1, wait, m)) == NULL)
		return (NULL);

	if (copyhdr) {
		o->m_pkthdr.len = m->m_pkthdr.len - off;
		m->m_pkthdr.len = off;
	}

	n->m_next = NULL;

	return (o);
@


1.152
log
@Passing M_WAITOK to mbuf functions is supposed to be a contract between
the caller and the function that the function will not fail to allocate
memory and return a NULL pointer. However, m_dup_pkthdr() violates
this contract, making it possible for functions that pass M_WAITOK to
be surprised in ways that hurt.

Fix this by passing the wait flag all the way down the functions that
actually do the allocation for m_dup_pkthdr() so that we won't be
surprised.

man page update forthcoming

ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.151 2011/04/05 01:28:05 art Exp $	*/
d141 1
a141 1
	pool_set_constraints(&mbpool, &kp_dma);
d149 1
a149 1
		pool_set_constraints(&mclpools[i], &kp_dma); 
@


1.151
log
@ - Change pool constraints to use kmem_pa_mode instead of uvm_constraint_range
 - Use km_alloc for all backend allocations in pools.
 - Use km_alloc for the emergmency kentry allocations in uvm_mapent_alloc
 - Garbage collect uvm_km_getpage, uvm_km_getpage_pla and uvm_km_putpage

ariane@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.150 2011/04/04 21:33:27 blambert Exp $	*/
d665 1
a665 1
			if (m_dup_pkthdr(n, m0))
d1152 1
a1152 1
	if (copyhdr && m_dup_pkthdr(mhdr, n)) {
d1349 1
a1349 1
m_dup_pkthdr(struct mbuf *to, struct mbuf *from)
d1361 1
a1361 1
	if ((error = m_tag_copy_chain(to, from)) != 0)
@


1.150
log
@both m_inject() and m_split() reached a point at which they needed
to cleave a single mbuf in twain, but managed to fail in divergent
and horrible ways in doing so in anything resembling a sane manner

introduce m_split_mbuf() and remake the previous into wrappers
around that

pounded by phessler@@
ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.149 2011/01/29 13:15:39 bluhm Exp $	*/
d141 1
a141 1
	pool_set_constraints(&mbpool, &dma_constraint, 1);
d149 1
a149 1
		pool_set_constraints(&mclpools[i], &dma_constraint, 1); 
@


1.149
log
@The function m_tag_copy_chain() returned 0 on success and 1 on
failure.  The man page explained it the other way around.  Change
the return code to the more obvious 0 or ENOMEM and document this.
ok krw@@ markus@@ miod@@ jmc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.148 2010/12/21 14:00:43 claudio Exp $	*/
d113 2
d1084 4
a1087 4
 * Inject a new mbuf chain of length siz in mbuf chain m0 at
 * position len0. Returns a pointer to the first injected mbuf, or
 * NULL on failure (m0 is left undisturbed). Note that if there is
 * enough space for an object of size siz in the appropriate position,
d1089 1
a1089 1
 * the first len0 bytes (pointers to that will remain valid).
d1091 1
a1091 1
 * XXX It is assumed that siz is less than the size of an mbuf at the moment.
d1094 1
a1094 1
m_inject(struct mbuf *m0, int len0, int siz, int wait)
d1096 5
a1100 2
	struct mbuf *m, *n, *n2 = NULL, *n3;
	unsigned len = len0, remain;
d1102 4
a1105 5
	if ((siz >= MHLEN) || (len0 <= 0))
	        return (NULL);
	for (m = m0; m && len > m->m_len; m = m->m_next)
		len -= m->m_len;
	if (m == NULL)
a1106 14
	remain = m->m_len - len;
	if (remain == 0) {
	        if ((m->m_next) && (M_LEADINGSPACE(m->m_next) >= siz)) {
		        m->m_next->m_len += siz;
			if (m0->m_flags & M_PKTHDR)
				m0->m_pkthdr.len += siz;
			m->m_next->m_data -= siz;
			return m->m_next;
		}
	} else {
	        n2 = m_copym2(m, len, remain, wait);
		if (n2 == NULL)
		        return (NULL);
	}
d1108 2
a1109 4
	MGET(n, wait, MT_DATA);
	if (n == NULL) {
	        if (n2)
		        m_freem(n2);
d1113 8
a1120 15
	n->m_len = siz;
	if (m0->m_flags & M_PKTHDR)
		m0->m_pkthdr.len += siz;
	m->m_len -= remain; /* Trim */
	if (n2)	{
	        for (n3 = n; n3->m_next != NULL; n3 = n3->m_next)
		        ;
		n3->m_next = n2;
	} else
	        n3 = n;
	for (; n3->m_next != NULL; n3 = n3->m_next)
	        ;
	n3->m_next = m->m_next;
	m->m_next = n;
	return n;
d1124 1
a1124 3
 * Partition an mbuf chain in two pieces, returning the tail --
 * all but the first len0 bytes.  In case of failure, it returns NULL and
 * attempts to restore the chain to its original state.
d1127 1
a1127 1
m_split(struct mbuf *m0, int len0, int wait)
d1129 5
a1133 2
	struct mbuf *m, *n;
	unsigned len = len0, remain, olen;
d1135 7
a1141 3
	for (m = m0; m && len > m->m_len; m = m->m_next)
		len -= m->m_len;
	if (m == NULL)
d1143 5
a1147 7
	remain = m->m_len - len;
	if (m0->m_flags & M_PKTHDR) {
		MGETHDR(n, wait, m0->m_type);
		if (n == NULL)
			return (NULL);
		if (m_dup_pkthdr(n, m0)) {
			m_freem(n);
a1149 26
		n->m_pkthdr.len -= len0;
		olen = m0->m_pkthdr.len;
		m0->m_pkthdr.len = len0;
		if (m->m_flags & M_EXT)
			goto extpacket;
		if (remain > MHLEN) {
			/* m can't be the lead packet */
			MH_ALIGN(n, 0);
			n->m_next = m_split(m, len, wait);
			if (n->m_next == NULL) {
				(void) m_free(n);
				m0->m_pkthdr.len = olen;
				return (NULL);
			} else
				return (n);
		} else
			MH_ALIGN(n, remain);
	} else if (remain == 0) {
		n = m->m_next;
		m->m_next = NULL;
		return (n);
	} else {
		MGET(n, wait, m->m_type);
		if (n == NULL)
			return (NULL);
		M_ALIGN(n, remain);
d1151 4
a1154 7
extpacket:
	if (m->m_flags & M_EXT) {
		n->m_ext = m->m_ext;
		MCLADDREFERENCE(m, n);
		n->m_data = m->m_data + len;
	} else {
		bcopy(mtod(m, caddr_t) + len, mtod(n, caddr_t), remain);
d1156 5
a1160 2
	n->m_len = remain;
	m->m_len = len;
d1162 2
a1163 1
	m->m_next = NULL;
d1165 27
@


1.148
log
@Ugly workaround in nmbclust_update(). Additionally to setting the limit
also modify the hiwat mark. This was done in pool_sethardlimit() until
rev. 1.99. Without this the mbuf cluster pool may return free pages too
quickly with the result that m_clget() may fail while populating DMA rings.
Seems to fix some hangs seen on MCLGETI() interfaces on i386 e.g. PR 6524.
A proper fix is to make all drivers handle empty rings but that will take
a while to implement.  With and OK mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.147 2010/11/05 15:17:50 claudio Exp $	*/
d1365 2
d1375 2
a1376 2
	if (m_tag_copy_chain(to, from))
		return (ENOMEM);
@


1.147
log
@Implement m_print as real ddb command "show mbuf addr" in the way other
such commands are implemented.
"Ja! You'll need to update ddb.4 as well, of course." miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.146 2010/10/28 16:28:56 claudio Exp $	*/
d169 8
@


1.146
log
@Add m_print() a function to print mbuf headers. Can be called from ddb
with an mbuf pointer as argument to see the contents of it.
OK thib@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.145 2010/10/05 13:29:40 mikeb Exp $	*/
d1375 3
d1379 1
a1379 1
m_print(struct mbuf *m)
d1381 4
a1384 2
	printf("mbuf %p\n", m);
	printf("m_type: %hi\tm_flags: %b\n", m->m_type, m->m_flags,
d1388 3
a1390 3
	printf("m_next: %p\tm_nextpkt: %p\n", m->m_next, m->m_nextpkt);
	printf("m_data: %p\tm_len: %u\n", m->m_data, m->m_len);
	printf("m_dat: %p m_pktdat: %p\n", m->m_dat, m->m_pktdat);
d1392 1
a1392 1
		printf("m_pkthdr.len: %i\tm_ptkhdr.rcvif: %p\t"
d1395 1
a1395 1
		printf("m_ptkhdr.tags: %p\tm_pkthdr.tagsset: %hx\n",
d1397 1
a1397 1
		printf("m_pkthdr.csum_flags: %hx\tm_pkthdr.ether_vtag: %hu\n",
d1399 1
a1399 1
		printf("m_pkthdr.pf.flags: %b\n",
d1403 1
a1403 1
		printf("m_pkthdr.pf.hdr: %p\tm_pkthdr.pf.statekey: %p\n",
d1405 1
a1405 1
		printf("m_pkthdr.pf.qid:\t%u m_pkthdr.pf.tag: %hu\n",
d1407 1
a1407 1
		printf("m_pkthdr.pf.routed: %hhx\n", m->m_pkthdr.pf.routed);
d1410 1
a1410 1
		printf("m_ext.ext_buf: %p\tm_ext.ext_size: %u\n",
d1412 1
a1412 1
		printf("m_ext.ext_type: %x\tm_ext.ext_backend: %i\n",
d1414 2
a1415 2
		printf("m_ext.ext_ifp: %p\n", m->m_ext.ext_ifp);
		printf("m_ext.ext_free: %p\tm_ext.ext_arg: %p\n",
d1417 1
a1417 1
		printf("m_ext.ext_nextref: %p\tm_ext.ext_prevref: %p\n",
@


1.145
log
@implicitly protect m_cldrop with splnet;  ok claudio dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.144 2010/09/23 10:49:55 dlg Exp $	*/
d1373 44
@


1.144
log
@tweak the mclgeti algorithm to behave better under load.

instead of letting hardware rings grow on every interrupt, restrict
it so it can only grow once per softclock tick. we can only punish
the rings on softclock ticks, so it make sense to only grow on
softclock tick boundaries too.

the rings are now punished after >1 lost softclock tick rather than
>2. mclgeti is now more aggressive at detecting livelock.

the rings get punished by an 8th, rather than by half.

we now allow the rings to be punished again even if the system is
already considered in livelock.

without this diff a livelocked system will have its rx ring sizes
scale up and down very rapidly, while holding the rings low for too
long. this affected throughput significantly.

discussed and tested heavily at j2k10. there are still some games
with softnet we can play, but this is a good first step.

"put it in" and ok deraadt@@
ok claudio@@ krw@@ henning@@ mcbride@@

if we find out that it sucks we can pull it out again later. till then
we'll run with it and see how it goes.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.143 2010/07/15 09:45:09 claudio Exp $	*/
d406 4
a409 1
	if (ifp != NULL && m_cldrop(ifp, pi))
d411 1
a412 1
	s = splnet();
@


1.143
log
@m_getptr(m, 0, ...) may return an mbuf different from m -- if m has no
data in it. m_getptr() hops over empty buffers and points to the first
allocated data byte. Because of this the m_dup_pkthdr() call done by
m_copym0() can panic because not the first mbuf is passed.
Found the hard way by myself, diff by blambert@@ commiitting for him since
he is not around. Tested and OK myself
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.142 2010/07/14 10:31:54 matthew Exp $	*/
d325 1
d335 1
a335 1
	if (m_livelock == 0 && ticks - m_clticks > 2) {
d345 2
a346 2
		ifp->if_data.ifi_livelocks++;
		liveticks = ticks;
d350 3
a352 2
				mclp[i].mcl_cwm =
				    max(mclp[i].mcl_cwm / 2, mclp[i].mcl_lwm);
d355 1
a355 1
	} else if (m_livelock && ticks - liveticks > 5)
d360 2
a361 1
	    mclp->mcl_alive <= 2 && mclp->mcl_cwm < mclp->mcl_hwm) {
d364 1
@


1.142
log
@Eliminate some unused malloc(9) type defines.  Also get rid of the
mysterious and unused mbtypes[] array in mbuf.h.

ok tedu@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.141 2010/07/03 03:33:16 tedu Exp $	*/
d623 1
a623 1
m_copym0(struct mbuf *m, int off, int len, int wait, int deep)
d625 1
a625 1
	struct mbuf *n, **np;
d631 1
a631 1
	if (off == 0 && m->m_flags & M_PKTHDR)
d633 1
a633 1
	if ((m = m_getptr(m, off, &off)) == NULL)
d648 1
a648 1
			if (m_dup_pkthdr(n, m))
@


1.141
log
@replace 0 with equivalent allocation flag. ok thib
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.140 2010/07/02 02:40:16 blambert Exp $	*/
a78 1
#define MBTYPES
@


1.140
log
@m_copyback can fail to allocate memory, but is a void fucntion so gymnastics
are required to detect that.

Change the function to take a wait argument (used in nfs server, but
M_NOWAIT everywhere else for now) and to return an error

ok claudio@@ henning@@ krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.139 2010/07/01 19:23:51 beck Exp $	*/
d199 1
a199 1
	m = pool_get(&mbpool, nowait == M_WAIT ? PR_WAITOK : 0);
d224 1
a224 1
	m = pool_get(&mbpool, nowait == M_WAIT ? PR_WAITOK : 0);
d416 1
a416 1
	    how == M_WAIT ? PR_WAITOK : 0);
@


1.139
log
@pool setconstraints must be called immediately after pool_init, otherwise
if we allocate memory before applying constraints we get memory outside
the constraints allocated in the pool. this is bad.
ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.138 2010/06/27 03:03:48 thib Exp $	*/
d729 2
a730 2
void
m_copyback(struct mbuf *m0, int off, int len, const void *_cp)
d735 1
d738 1
a738 1
		return;
d743 2
a744 1
			if ((n = m_get(M_DONTWAIT, m->m_type)) == NULL)
d746 1
d749 1
a749 1
				MCLGETI(n, M_DONTWAIT, NULL, off + len);
d752 1
d777 2
a778 1
			if ((n = m_get(M_DONTWAIT, m->m_type)) == NULL)
d780 1
d783 1
a783 1
				MCLGETI(n, M_DONTWAIT, NULL, len);
d786 1
d798 2
@


1.138
log
@uvm constraints. Add two mandatory MD symbols, uvm_md_constraints
which contains the constraints for DMA/memory allocation for each
architecture, and dma_constraints which contains the range of addresses
that are dma accessable by the system.

This is based on ariane@@'s physcontig diff, with lots of bugfixes and
additions the following additions by my self:

Introduce a new function pool_set_constraints() which sets the address
range for which we allocate pages for the pool from, this is now used
for the mbuf/mbuf cluster pools to keep them dma accessible.

The !direct archs no longer stuff pages into the kernel object in
uvm_km_getpage_pla but rather do a pmap_extract() in uvm_km_putpages.

Tested heavily by my self on i386, amd64 and sparc64. Some tests on
alpha and SGI.

"commit it" beck, art, oga, deraadt
"i like the diff" deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.136 2010/01/14 23:12:11 schwarze Exp $	*/
d140 1
a141 1
	pool_set_constraints(&mbpool, &dma_constraint, 1);
d148 1
a149 1
		pool_set_constraints(&mclpools[i], &dma_constraint, 1); 
@


1.137
log
@Replace some handrolled instances of m_getptr() with that function, which
also gets a bit of a KNF scrubbing at claudio@@'s insistence. Shaves some
bytes from the kernel as well.

tested by phessler@@ and zinovnik@@, thanks

ok claudio@@
@
text
@d93 1
d141 1
d146 2
a147 2
		pool_init(&mclpools[i], mclsizes[i], 0, 0, 0, mclnames[i],
		    NULL);
d149 1
@


1.136
log
@fix typos in comments, no code changes;
from Brad Tilley <brad at 16systems dot com>;
ok oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.135 2010/01/12 04:05:45 deraadt Exp $	*/
d631 2
a632 8
	while (off > 0) {
		if (m == NULL)
			panic("m_copym0: null mbuf");
		if (off < m->m_len)
			break;
		off -= m->m_len;
		m = m->m_next;
	}
d706 2
a707 8
	while (off > 0) {
		if (m == NULL)
			panic("m_copydata: null mbuf in skip");
		if (off < m->m_len)
			break;
		off -= m->m_len;
		m = m->m_next;
	}
d1035 1
a1035 2
		}
		else {
d1043 2
d1046 1
a1046 3
				else
		  			return (NULL);
	    		} else
d1048 1
@


1.135
log
@Move initialization of the MCLGETI ticker to mbinit(), instead of ifinit()
ok henning
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.134 2009/09/13 14:42:52 krw Exp $	*/
d970 1
a970 1
 * reqested number of bytes.  (For those of us who expect unwieldly option
@


1.134
log
@M_DUP_PKTHDR() define -> m_dup_pkthdr() function to properly deal
with m_tag_copy_chain() failures.

Use m_defrag() to eliminate hand rolled defragging of mbufs and
some uses of M_DUP_PKTHDR().

Original diff from thib@@, claudio@@'s feedback integrated by me.

Tests kevlo@@ claudio@@, "reads ok" blambert@@

ok thib@@ claudio@@, "m_defrag() bits ok" kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.133 2009/08/12 21:44:49 henning Exp $	*/
d118 4
d150 3
d308 14
a321 1
extern int m_clticks;
@


1.133
log
@just bzero the pkthdr instead of setting each and every member of that
struct to 0/NULL. no performance impact but way less error prone on
addition of new pkthdr field (as just ran into with a theo diff). ok theo
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.132 2009/08/12 20:02:42 dlg Exp $	*/
d632 2
a633 1
			M_DUP_PKTHDR(n, m);
d1128 4
a1131 1
		M_DUP_PKTHDR(n, m0);
d1321 25
@


1.132
log
@revert my change to m_cluncount which tries to prevent the system
running out of mbufs for rx rings.

if the system low watermark is lower than a rx rings low watermark,
we'll never send a packet up the stack, we'll always recycle it.

found by thib@@ on a bge
sadface
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.131 2009/08/12 14:39:05 dlg Exp $	*/
d226 1
a226 13
		m->m_pkthdr.rcvif = NULL;
		m->m_pkthdr.rdomain = 0;
		SLIST_INIT(&m->m_pkthdr.tags);
		m->m_pkthdr.tagsset = 0;
		m->m_pkthdr.csum_flags = 0;
		m->m_pkthdr.ether_vtag = 0;
		m->m_pkthdr.pf.hdr = NULL;
		m->m_pkthdr.pf.statekey = NULL;
		m->m_pkthdr.pf.rtableid = 0;
		m->m_pkthdr.pf.qid = 0;
		m->m_pkthdr.pf.tag = 0;
		m->m_pkthdr.pf.flags = 0;
		m->m_pkthdr.pf.routed = 0;
d239 1
a239 13
	m->m_pkthdr.rcvif = NULL;
	m->m_pkthdr.rdomain = 0;
	SLIST_INIT(&m->m_pkthdr.tags);
	m->m_pkthdr.tagsset = 0;
	m->m_pkthdr.csum_flags = 0;
	m->m_pkthdr.ether_vtag = 0;
	m->m_pkthdr.pf.hdr = NULL;
	m->m_pkthdr.pf.statekey = NULL;
	m->m_pkthdr.pf.rtableid = 0;
	m->m_pkthdr.pf.qid = 0;
	m->m_pkthdr.pf.tag = 0;
	m->m_pkthdr.pf.flags = 0;
	m->m_pkthdr.pf.routed = 0;
@


1.131
log
@if we get dangerously low on clusters during interrupts, we need
to free some for use on the rx rings on network cards.

this modifies m_cluncount to advise callers when we're in such a
situation, and makes them responsible for freeing up the cluster
for allocation by MCLGETI later.

fixes an awesome lockup with sis(4) henning has been experiencing.
this is not the best fix, but it is better than the current situation.

yep deraadt@@ tested by henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.130 2009/08/11 11:53:19 deraadt Exp $	*/
d375 2
a376 2
int
m_cluncount(struct mbuf *m)
d378 1
a378 15
	struct mbuf_ext *me = &m->m_ext;
	int pi;

	splassert(IPL_NET);

	if (((m->m_flags & (M_EXT|M_CLUSTER)) != (M_EXT|M_CLUSTER)) ||
	    (me->ext_ifp == NULL))
		return (0);

	pi = me->ext_backend;
	me->ext_ifp->if_data.ifi_mclpool[pi].mcl_alive--;
	me->ext_ifp = NULL;

	if (mclpools[pi].pr_nitems <= mclpools[pi].pr_minitems)
		return (1);
d380 9
a388 1
	return (0);
a470 3
	struct mbuf_ext *me = &m->m_ext;
	int pi;

d472 4
a475 2
		me->ext_nextref->m_ext.ext_prevref = me->ext_prevref;
		me->ext_prevref->m_ext.ext_nextref = me->ext_nextref;
d477 6
a482 8
		pi = me->ext_backend;
		if (me->ext_ifp != NULL) {
			me->ext_ifp->if_data.ifi_mclpool[pi].mcl_alive--;
			me->ext_ifp = NULL;
		}
		pool_put(&mclpools[pi], me->ext_buf);
	} else if (me->ext_free)
		me->ext_free(me->ext_buf, me->ext_size, me->ext_arg);
d485 1
a485 2

	me->ext_size = 0;
@


1.130
log
@optimise m_clget so that it holds spl for even less
ok dlg thib
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.129 2009/08/11 10:48:39 deraadt Exp $	*/
d375 2
a376 2
void
m_cluncount(struct mbuf *m, int all)
d378 15
a392 1
	struct mbuf_ext *me;
d394 1
a394 9
	do {
		me = &m->m_ext;
		if (((m->m_flags & (M_EXT|M_CLUSTER)) != (M_EXT|M_CLUSTER)) ||
		    (me->ext_ifp == NULL))
			continue;

		me->ext_ifp->if_data.ifi_mclpool[me->ext_backend].mcl_alive--;
		me->ext_ifp = NULL;
	} while (all && (m = m->m_next));
d477 3
d481 2
a482 4
		m->m_ext.ext_nextref->m_ext.ext_prevref =
		    m->m_ext.ext_prevref;
		m->m_ext.ext_prevref->m_ext.ext_nextref =
		    m->m_ext.ext_nextref;
d484 8
a491 6
		m_cluncount(m, 0);
		pool_put(&mclpools[m->m_ext.ext_backend],
		    m->m_ext.ext_buf);
	} else if (m->m_ext.ext_free)
		(*(m->m_ext.ext_free))(m->m_ext.ext_buf,
		    m->m_ext.ext_size, m->m_ext.ext_arg);
d494 2
a495 1
	m->m_ext.ext_size = 0;
@


1.129
log
@Must move the splx() lower in m_clget() so that it protects atomic access
to the per-ipf mbuf cluster reference counters
ok dlg claudio
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.128 2009/08/09 21:08:30 deraadt Exp $	*/
d424 3
a426 11
	if (m->m_ext.ext_buf != NULL) {
		m->m_data = m->m_ext.ext_buf;
		m->m_flags |= M_EXT|M_CLUSTER;
		m->m_ext.ext_size = mclpools[pi].pr_size;
		m->m_ext.ext_free = NULL;
		m->m_ext.ext_arg = NULL;

		m->m_ext.ext_backend = pi;
		m->m_ext.ext_ifp = ifp;
		if (ifp != NULL)
			m_clcount(ifp, pi);
d428 8
a435 3
		MCLINITREFERENCE(m);
	}
	splx(s);
@


1.128
log
@remove the ancient MCFail / MPFail debugging technique; ok claudio
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.127 2009/08/09 16:19:08 deraadt Exp $	*/
a423 1
	splx(s);
d438 1
@


1.127
log
@use m_free_unlocked() in m_pullup() to avoid iterative splnet/splx down
the chain
ok henning
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.126 2009/08/09 12:50:09 henning Exp $	*/
a610 2
int MCFail;

a698 2
	if (top == NULL)
		MCFail++;
a701 1
	MCFail++;
a913 2
int MPFail;

a968 1
	MPFail++;
a1038 1
	MPFail++;
@


1.126
log
@make mbuf tags suck a bit less, performance wise.
the most common operation is checking for a particular tag to be there.
in the majority of the cases it is not.
introduce a "tagsset" in the mbuf packet header which has a bit for
each mbuf tag type that is in the chain set, checking for its existance
is now as easy and cheap as (tagsset & type) != 0. theo ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.125 2009/08/09 12:42:11 deraadt Exp $	*/
d927 1
d952 1
d964 1
a964 1
			n = m_free(n);
d967 2
a968 1
		(void)m_free(m);
d971 1
@


1.125
log
@create a m_free_unlocked which is now used by both m_freem() and m_free().
this lets m_freem() only do one splnet/splx instead of repeating this all
the way down a chain
ok henning claudio dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.124 2009/08/09 12:24:40 deraadt Exp $	*/
d229 1
d254 1
@


1.124
log
@do not clear a field before we pool_put it into oblivion; ok henning
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.123 2009/08/09 11:40:58 deraadt Exp $	*/
d441 1
a441 1
m_free(struct mbuf *m)
a443 1
	int s;
a444 1
	s = splnet();
d452 12
d494 1
d498 1
d500 1
a500 1
		MFREE(m, n);
d502 1
@


1.123
log
@MCLGETI() will now allocate a mbuf header if it is not provided, thus
reducing the amount of splnet/splx dancing required.. especially in the
worst case (of m_cldrop)
ok dlg kettenis damien
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.122 2009/06/22 10:51:06 thib Exp $	*/
a451 1
	m->m_flags = 0;
@


1.122
log
@sync m_inithdr() with m_gethdr() after the
addition of rdomain to struct pkthdr.

"Doh!" claudio@@
ok henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.120 2009/06/02 00:05:13 blambert Exp $	*/
d389 1
a389 1
void
d392 1
d403 1
a403 1
		return;
d406 8
d416 6
d437 1
@


1.121
log
@Initial support for routing domains. This allows to bind interfaces to
alternate routing table and separate them from other interfaces in distinct
routing tables. The same network can now be used in any doamin at the same
time without causing conflicts.
This diff is mostly mechanical and adds the necessary rdomain checks accross
net and netinet. L2 and IPv4 are mostly covered still missing pf and IPv6.
input and tested by jsg@@, phessler@@ and reyk@@. "put it in" deraadt@@
@
text
@d251 1
@


1.120
log
@Move M_PREPEND macro code to be entirely into m_prepend the function;
calling M_PREPEND is now #define'd to be calling m_prepend.

Shaves an unknown but assumed-to-be-nontrivial amount from the kernel.

ok claudio@@ henning@@(who may have had to okay this twice to get me to notice)
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.119 2009/03/02 23:52:18 dlg Exp $	*/
d227 1
@


1.119
log
@the packet length passed to m_clget is a u_int, print it with %u not %d in
the panic string.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.117 2009/02/04 20:02:11 claudio Exp $	*/
d543 1
a543 3
 * Lesser-used path for M_PREPEND:
 * allocate new mbuf to prepend to chain,
 * copy junk along.
d553 15
a567 4
	MGET(mn, how, m->m_type);
	if (mn == NULL) {
		m_freem(m);
		return (NULL);
d570 1
a570 5
		M_MOVE_PKTHDR(mn, m);
	mn->m_next = m;
	m = mn;
	MH_ALIGN(m, len);
	m->m_len = len;
@


1.118
log
@Don't panic if m_copyback() is working on a M_READONLY() mbuf. The old version
did not care either and with this packets from drivers with external buffers
(e.g. wpi(4)) would trigger this panic through pf(4).
Found the hard way by Tim van der Molen tbvdm (at) xs4all (dot) nl
@
text
@d396 1
a396 1
		panic("m_clget: request for %d sized cluster", pktlen);
@


1.117
log
@Make m_copyback() a lot smarter. Make it use all of the last mbuf
(M_TRAILINGSPACE()) and allocate one cluster if needed (instead of chaining
many mbufs). Somewhat needed for the rl(4) fix to ensure that the ethernet
header is in one mbuf for sure. Tested by landry@@ and myself
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.116 2009/01/27 09:17:51 dlg Exp $	*/
a741 2
		if (M_READONLY(m))
			panic("m_copyback called on a readonly cluster");
@


1.116
log
@make drivers tell the mclgeti allocator what their maximum ring size is
to prevent the hwm growing beyond that. this allows the livelock mitigation
to do something where the hwm used to grow beyond twice the rx rings size.

ok kettenis@@ claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.115 2009/01/26 15:16:39 claudio Exp $	*/
d715 1
a715 1
	int mlen;
a716 1
	int totlen = 0;
d725 1
a725 2
			n = m_getclr(M_DONTWAIT, m->m_type);
			if (n == NULL)
d727 10
a736 1
			n->m_len = min(MLEN, len + off);
d742 8
a749 2
		mlen = min (m->m_len - off, len);
		bcopy(cp, off + mtod(m, caddr_t), (unsigned)mlen);
d752 1
a752 3
		mlen += off;
		off = 0;
		totlen += mlen;
d755 2
d758 11
a768 4
			n = m_get(M_DONTWAIT, m->m_type);
			if (n == NULL)
				break;
			n->m_len = min(MLEN, len);
d773 2
a774 1
out:	if (((m = m0)->m_flags & M_PKTHDR) && (m->m_pkthdr.len < totlen))
@


1.115
log
@Remove unneeded brackets that where left over from when these were macros.
OK dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.114 2008/12/23 01:06:35 deraadt Exp $	*/
d111 2
d276 13
a292 1
	extern u_int mclsizes[];
d297 8
a304 4
		if (mclp[i].mcl_lwm == 0)
			mclp[i].mcl_lwm = 2;
		mclp[i].mcl_hwm = MAX(4, mclp[i].mcl_lwm);
		mclp[i].mcl_size = mclsizes[i];
d309 1
a309 1
m_clsetlwm(struct ifnet *ifp, u_int pktlen, u_int lwm)
d311 1
a311 2
	extern u_int mclsizes[];
	int i;
d313 2
a314 5
	for (i = 0; i < MCLPOOLS; i++) {
                if (pktlen <= mclsizes[i])
			break;
        }
	if (i >= MCLPOOLS)
d317 2
a318 1
	ifp->if_data.ifi_mclpool[i].mcl_lwm = lwm;
d346 4
a349 3
			for (i = 0; i < nitems(aifp->if_data.ifi_mclpool); i++)
				mclp[i].mcl_hwm =
				    max(mclp[i].mcl_hwm / 2, mclp[i].mcl_lwm);
d354 6
a359 6
	mclp = ifp->if_data.ifi_mclpool;
	if (mclp[pi].mcl_alive <= 2 && mclp[pi].mcl_hwm < 32768 &&
	    ISSET(ifp->if_flags, IFF_RUNNING) && m_livelock == 0) {
		/* About to run out, so increase the watermark */
		mclp[pi].mcl_hwm++;
	} else if (mclp[pi].mcl_alive >= mclp[pi].mcl_hwm)
a389 1
	struct pool *mclp;
d393 1
a393 6
	for (pi = 0; pi < nitems(mclpools); pi++) {
		mclp = &mclpools[pi];
		if (pktlen <= mclp->pr_size)
			break;
	}

d395 1
a395 1
	if (mclp == NULL)
d403 2
a404 1
	m->m_ext.ext_buf = pool_get(mclp, how == M_WAIT ? PR_WAITOK : 0);
d409 1
a409 1
		m->m_ext.ext_size = mclp->pr_size;
@


1.114
log
@The splvm() protection is way outdated, only splnet is needed to protect
this pool (and the mbstat variables, and a few other things in certain
cases)
ok mikeb, tedu, and discussion with others...
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.114 2008/12/23 01:04:55 deraadt Exp $	*/
d1267 1
a1267 1
	if (M_READONLY((m)))
d1269 3
a1271 3
	return ((m)->m_flags & M_EXT ? (m)->m_data - (m)->m_ext.ext_buf :
	    (m)->m_flags & M_PKTHDR ? (m)->m_data - (m)->m_pktdat :
	    (m)->m_data - (m)->m_dat);
d1279 3
a1281 3
	return ((m)->m_flags & M_EXT ? (m)->m_ext.ext_buf +
	    (m)->m_ext.ext_size - ((m)->m_data + (m)->m_len) :
	    &(m)->m_dat[MLEN] - ((m)->m_data + (m)->m_len));
@


1.113
log
@Doh. Correct address in bcopy -- m->m_data not &m->m_data -- this should fix
the issues seen by damien@@. OK dlg@@, damien@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.112 2008/12/20 22:27:38 deraadt Exp $	*/
d167 1
a167 1
	int s = splvm();
d186 1
a186 1
	s = splvm();
d211 1
a211 1
	s = splvm();
d392 1
a392 1
	s = splvm();
d417 1
a417 1
	s = splvm();
d499 1
a499 1
		int s = splvm();
@


1.112
log
@protect mbstat with spl; ok claudio mikeb
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.111 2008/12/14 22:31:46 kettenis Exp $	*/
d516 1
a516 1
		bcopy(&m0->m_data, &m->m_data, m0->m_len);
@


1.111
log
@Make sure the low water mark for cluster pools isn't 0 such that network
drivers are guaranteed to make progress.  We could probably set it to 1,
but we set it to 2, to make sure drivers that link descriptors don't link
a descriptor back to itself.

ok deraadt@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.110 2008/12/13 00:18:46 deraadt Exp $	*/
d173 1
a174 1
	mbstat.m_drain++;
d188 2
a192 1
		mbstat.m_mtypes[type]++;
d213 2
a217 1
		mbstat.m_mtypes[type]++;
@


1.110
log
@note to everyone:  nitems(pointer) is 0, not what you want
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.109 2008/12/11 16:45:45 deraadt Exp $	*/
d281 2
d331 1
a331 1
				    max(mclp[i].mcl_hwm / 2,mclp[i].mcl_lwm);
@


1.109
log
@export per-interface mbuf cluster pool use statistics out to userland
inside if_data, so that netstat(1) and systat(1) can see them
ok dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.108 2008/12/04 23:40:44 dlg Exp $	*/
d327 1
a327 1
			for (i = 0; i < nitems(mclp); i++)
@


1.108
log
@enable to large clusters again now that arts put the pool allocator for
them in again.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.106 2008/11/26 23:47:14 claudio Exp $	*/
d270 95
@


1.107
log
@need splvm() around the call to m_extfree() in the defragger
@
text
@a101 1
#if 0
a106 1
#endif
@


1.106
log
@Doh, do not commit before compile. Found by sthen@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.105 2008/11/26 22:56:07 claudio Exp $	*/
d401 2
a402 1
	if (m->m_flags & M_EXT)
d404 2
@


1.105
log
@Do a quick return if m->m_next is NULL in m_defrag() because there is nothing
todo. Discussed with deraadt@@ and dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.104 2008/11/26 21:39:57 deraadt Exp $	*/
d378 1
a378 1
		return;
@


1.104
log
@only the pool_get() needs to be spl protected; ok claudio dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.103 2008/11/25 19:09:34 claudio Exp $	*/
d377 3
d381 1
a381 1
	if (!(m->m_flags & M_PKTHDR) || m->m_next == NULL)
@


1.103
log
@m_defrag() a mbuf chain defragmenter. It will collaps a mbuf chain into a
single buffer without changing the head mbuf. This is done with a lot of
magic so there will be dragons.
Tested and OK dlg@@, kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.102 2008/11/25 17:01:14 dlg Exp $	*/
d190 1
a198 1
	splx(s);
d214 1
a235 1
	splx(s);
@


1.102
log
@backout large cluster allocators.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.101 2008/11/25 15:43:32 dlg Exp $	*/
d118 1
d202 4
d324 2
a325 17
	if (m->m_flags & M_EXT) {
		if (MCLISREFERENCED(m)) {
			m->m_ext.ext_nextref->m_ext.ext_prevref =
			    m->m_ext.ext_prevref;
			m->m_ext.ext_prevref->m_ext.ext_nextref =
			    m->m_ext.ext_nextref;
		} else if (m->m_flags & M_CLUSTER) {
			m_cluncount(m, 0);
			pool_put(&mclpools[m->m_ext.ext_backend],
			    m->m_ext.ext_buf);
		} else if (m->m_ext.ext_free)
			(*(m->m_ext.ext_free))(m->m_ext.ext_buf,
			    m->m_ext.ext_size, m->m_ext.ext_arg);
		else
			free(m->m_ext.ext_buf,m->m_ext.ext_type);
		m->m_ext.ext_size = 0;
	}
d335 21
d365 57
@


1.101
log
@art says he doesnt suck anymore, so enable the really big cluster
allocators again.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.100 2008/11/25 12:47:00 deraadt Exp $	*/
d102 1
d108 1
@


1.100
log
@Factor increases are not needed, +1 appears to work as well.
ok dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.99 2008/11/25 12:07:55 claudio Exp $	*/
d99 1
a99 1
u_short	mclsizes[] = {
a101 1
#if art_doesnt_suck
a106 1
#endif
@


1.99
log
@m_cluncount() needs to walk the mbuf chain to correctly uncount all clusters
but don't do that in m_free() as that will cause a double loop behaviour when
called via m_freem().
OK dlg@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.98 2008/11/24 19:17:16 dlg Exp $	*/
d99 3
a101 3
struct	mclsizes mclsizes[] = {
	{  MCLBYTES, 4, 1 }, /* must be at slot 0 */
	{  4 * 1024, 4, 2 },
d103 5
a107 5
	{  8 * 1024, 4, 2 },
	{  9 * 1024, 4, 2 },
	{ 12 * 1024, 4, 2 },
	{ 16 * 1024, 4, 2 },
	{ 64 * 1024, 4, 2 }
d138 2
a139 2
		    mclsizes[i].size >> 10);
		pool_init(&mclpools[i], mclsizes[i].size, 0, 0, 0, mclnames[i],
@


1.98
log
@art isnt handling spls properly in the pool allocator backends for big
objects. dmesg is being spammed with splasserts.

disable the 8k, 9k, 12k, 16k, and 64k backend pools for the cluster
allocator.

art will fix this when he gets back from dinner, otherwise i'll nag more.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.97 2008/11/24 18:34:40 dlg Exp $	*/
d326 1
a326 1
			m_cluncount(m);
@


1.97
log
@enable the 8k, 9k, 12k, 16k, and 64k backend pools for the cluster
allocator.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.96 2008/11/24 14:26:54 deraadt Exp $	*/
d102 1
d108 1
@


1.96
log
@move MCLPOOLS to if.h and force uipc_mbuf.c to get if.h, there is no
other option
ok dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.95 2008/11/24 12:57:37 dlg Exp $	*/
a101 2
#ifdef notyet
	/* pool allocator cannot cope with >PAGESIZE objects */
a106 1
#endif
@


1.95
log
@add several backend pools to allocate mbufs clusters of various sizes out
of. currently limited to MCLBYTES (2048 bytes) and 4096 bytes until pools
can allocate objects of sizes greater than PAGESIZE.

this allows drivers to ask for "jumbo" packets to fill rx rings with.

the second half of this change is per interface mbuf cluster allocator
statistics. drivers can use the new interface (MCLGETI), which will use
these stats to selectively fail allocations based on demand for mbufs. if
the driver isnt rapidly consuming rx mbufs, we dont allow it to allocate
many to put on its rx ring.

drivers require modifications to take advantage of both the new allocation
semantic and large clusters.

this was written and developed with deraadt@@ over the last two days
ok deraadt@@ claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.94 2008/10/14 18:01:53 naddy Exp $	*/
d86 4
@


1.94
log
@Change m_devget()'s outdated and unused "offset" argument:  It is
now the offset into the first mbuf of the target chain before copying
the source data over.  From FreeBSD.

Convert drivers' use of m_devget().  Mostly from thib@@.

Update mbuf(9) man page.

ok claudio@@, thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.93 2008/09/28 14:08:51 naddy Exp $	*/
d93 16
a108 1
struct	pool mclpool;		/* mbuf cluster pool */
d120 1
a120 1
    "WARNING: mclpool limit reached; increase kern.maxclusters";
d128 2
d131 9
a139 1
	pool_init(&mclpool, MCLBYTES, 0, 0, 0, "mclpl", NULL);
a141 9

	/*
	 * Set a low water mark for both mbufs and clusters.  This should
	 * help ensure that they can be allocated in a memory starvation
	 * situation.  This is important for e.g. diskless systems which
	 * must allocate mbufs in order for the pagedaemon to clean pages.
	 */
	pool_setlowat(&mbpool, mblowat);
	pool_setlowat(&mclpool, mcllowat);
d147 1
d149 1
a149 1
	 * Set the hard limit on the mclpool to the number of
d153 4
a156 1
	(void)pool_sethardlimit(&mclpool, nmbclust, mclpool_warnmsg, 60);
d267 1
a267 1
m_clget(struct mbuf *m, int how)
d269 2
d273 14
d288 1
a288 2
	m->m_ext.ext_buf =
	    pool_get(&mclpool, how == M_WAIT ? PR_WAITOK : 0);
d293 1
a293 1
		m->m_ext.ext_size = MCLBYTES;
d296 6
d322 5
a326 3
		} else if (m->m_flags & M_CLUSTER)
			pool_put(&mclpool, m->m_ext.ext_buf);
		else if (m->m_ext.ext_free)
@


1.93
log
@initialize the ether_vtag field like the rest of the packet header
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.92 2008/08/14 19:39:40 claudio Exp $	*/
d925 10
a934 16
	struct mbuf *m;
	struct mbuf *top = NULL, **mp = &top;
	int len;
	char *cp;
	char *epkt;

	cp = buf;
	epkt = cp + totlen;
	if (off) {
		/*
		 * If 'off' is non-zero, packet is trailer-encapsulated,
		 * so we have to skip the type and length fields.
		 */
		cp += off + 2 * sizeof(u_int16_t);
		totlen -= 2 * sizeof(u_int16_t);
	}
d938 1
d941 2
a942 1
	m->m_len = MHLEN;
d951 1
a951 1
			m->m_len = MLEN;
d953 2
a954 2
		len = min(totlen, epkt - cp);
		if (len >= MINCLSIZE) {
d957 1
a957 3
				m->m_len = len = min(len, MCLBYTES);
			else
				len = m->m_len;
d959 11
a969 10
			/*
			 * Place initial small packet/header at end of mbuf.
			 */
			if (len < m->m_len) {
				if (top == NULL &&
				    len + max_linkhdr <= m->m_len)
					m->m_data += max_linkhdr;
				m->m_len = len;
			} else
				len = m->m_len;
d971 3
d975 1
a975 1
			copy(cp, mtod(m, caddr_t), (size_t)len);
d977 3
a979 2
			bcopy(cp, mtod(m, caddr_t), (size_t)len);
		cp += len;
a982 2
		if (cp == epkt)
			cp = buf;
@


1.92
log
@Kill the _MCLDEREFERENCE() macro it was only used once and it should be only
used once -- in m_free(). Removed so that people don't get stupid ideas.
OK thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.91 2008/08/08 08:54:08 thib Exp $	*/
d198 1
d222 1
@


1.91
log
@plug an mbuf leak in m_pullup2(); If we fail to get a cluster for an mbuf,
free the mbuf before bailing out.

ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.90 2008/06/11 02:46:34 henning Exp $	*/
d274 6
a279 3
		if (MCLISREFERENCED(m))
			_MCLDEREFERENCE(m);
		else if (m->m_flags & M_CLUSTER)
@


1.90
log
@store a pointer to the stack side state key in the mbuf packet
header inbound. on the outbound side, we take that and look for the key
that is the exact opposite, and store that mapping in the state key. on
subsequent packets we don't have to do the lookup on outbound any more.
almost unable to get real benchmarks going here, we know for sure this
gives a more than 5% increase in forwarding performance.
many thanks to ckuethe for stress- and performance-testing.
ok ryan theo
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.89 2008/05/06 02:16:26 krw Exp $	*/
d718 2
a719 1
		if ((m->m_flags & M_EXT) == 0)
d721 1
@


1.89
log
@Rather than clearing particularly dangerous flags when found, just
zero all flags when we pool_put mbufs.

ok claudio@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.88 2008/01/16 19:28:23 thib Exp $	*/
d199 1
d222 1
@


1.88
log
@Dont use PR_LIMITFAIL with PR_WAITOK when M_WAIT is passed with
the flag argument to the m_*get* functions. Since PR_LIMITFAIL
can cause us to return NULL if hit the limits.

ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.87 2007/11/27 16:38:50 tedu Exp $	*/
a280 1
		m->m_flags &= ~(M_CLUSTER|M_EXT);
d283 1
@


1.87
log
@make the deceptively complicated leading and trailing space into functions.
this reduces kernel size quite a bit. ok claudio
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.86 2007/09/26 13:05:52 henning Exp $	*/
d165 1
a165 1
	m = pool_get(&mbpool, nowait == M_WAIT ? PR_WAITOK|PR_LIMITFAIL : 0);
d185 1
a185 1
	m = pool_get(&mbpool, nowait == M_WAIT ? PR_WAITOK|PR_LIMITFAIL : 0);
d249 1
a249 1
	    pool_get(&mclpool, how == M_WAIT ? (PR_WAITOK|PR_LIMITFAIL) : 0);
@


1.86
log
@provide m_inithdr(), which takes an mbuf and gives an initialized M_PKTHDR
mbuf back.
for fixing PR5563 in a few, tested janjaap@@stack.nl, ok claudio
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.85 2007/07/20 09:59:19 claudio Exp $	*/
d1039 20
@


1.85
log
@Remove the MFREE() macro and replace it with a call to m_free().
Also remove the _MEXTREMOVE macro which was only used by MFREE.
This time with the uipc_mbuf.c change that I missed last time.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.84 2007/06/02 09:45:32 art Exp $	*/
d189 2
d206 21
@


1.84
log
@Initialize pkthdr.rcvif.
ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.83 2007/05/28 19:20:14 tedu Exp $	*/
d242 22
a264 1
	MFREE(m, n);
@


1.83
log
@set a hiwat mark for mbpool.  we spend quite a bit of time bouncing pages
in and out with the very low default.
ok dlg henning ryan
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.82 2007/05/28 17:16:39 henning Exp $	*/
d193 1
@


1.82
log
@double pf performance.
boring details:
pf used to use an mbuf tag to keep track of route-to etc, altq, tags,
routing table IDs, packets redirected to localhost etc. so each and every
packet going through pf got an mbuf tag. mbuf tags use malloc'd memory,
and that is knda slow.
instead, stuff the information into the mbuf header directly.
bridging soekris with just "pass" as ruleset went from 29 MBit/s to
58 MBit/s with that (before ryan's randomness fix, now it is even betterer)
thanks to chris for the test setup!
ok ryan ryan ckuethe reyk
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.81 2007/05/27 20:54:25 claudio Exp $	*/
d137 1
@


1.81
log
@Kill the nasty MGET, MGETHDR and MCLGET makros and replace them with normal
functions. The world is no longer running on a PDP11 so function call overhead
is not an issue. Diff by tbert, tested by many, OK art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.80 2007/03/15 11:48:09 claudio Exp $	*/
d194 6
@


1.80
log
@m_prepend() works only for sizes smaller than MHLEN.
OK beck@@ deraadt@@ pyr@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.79 2006/12/29 13:04:37 pedro Exp $	*/
a155 2
 * These are also available as macros
 * for critical paths.
d161 1
d163 11
a173 1
	MGET(m, nowait, type);
d181 1
d183 13
a195 1
	MGETHDR(m, nowait, type);
d211 19
a987 28

#ifdef SMALL_KERNEL
/*
 * The idea of adding code in a small kernel might look absurd, but this is
 * instead of macros.
 */
struct mbuf *
_sk_mget(int how, int type)
{
	struct mbuf *m;
	_MGET(m, how, type);
	return m;
}

struct mbuf *
_sk_mgethdr(int how, int type)
{
	struct mbuf *m;
	_MGETHDR(m, how, type);
	return m;
}

void
_sk_mclget(struct mbuf *m, int how)
{
	_MCLGET(m, how);
}
#endif /* SMALL_KERNEL */
@


1.79
log
@Avoid void * arithmetic, okay deraadt@@, suggestions from millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.78 2006/11/29 12:39:48 miod Exp $	*/
d224 3
d236 1
a236 2
	if (len < MHLEN)
		MH_ALIGN(m, len);
@


1.78
log
@We don't use mb_map anymore since a long time already. Remove it.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.77 2006/10/11 22:39:46 mpf Exp $	*/
d383 1
a383 1
m_copyback(struct mbuf *m0, int off, int len, const void *cp)
d388 1
@


1.77
log
@Remove unused variable and simplify m_copym0(). Diff from bret.lambert at gmail.com.
Kill another unused variable in m_devget(). Pointed out by mcbride.
Rename all offset variables from off0 to off.
OK markus@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.76 2006/07/14 01:58:58 pedro Exp $	*/
a93 2

struct vm_map *mb_map;
@


1.76
log
@Typo
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.75 2006/05/07 20:06:50 tedu Exp $	*/
d242 1
a242 1
 * Make a copy of an mbuf chain starting "off0" bytes from the beginning,
d249 1
a249 1
m_copym(struct mbuf *m, int off0, int len, int wait)
d251 1
a251 1
	return m_copym0(m, off0, len, wait, 0);	/* shallow copy on M_EXT */
d259 1
a259 1
m_copym2(struct mbuf *m, int off0, int len, int wait)
d261 1
a261 1
	return m_copym0(m, off0, len, wait, 1);	/* deep copy */
d265 1
a265 1
m_copym0(struct mbuf *m, int off0, int len, int wait, int deep)
a267 1
	int off = off0;
d297 1
a297 3
			if (len == M_COPYALL)
				n->m_pkthdr.len -= off0;
			else
d321 1
a321 1
			memcpy(mtod(n, caddr_t), mtod(m, caddr_t)+off,
d819 1
a819 1
m_devget(char *buf, int totlen, int off0, struct ifnet *ifp,
d824 1
a824 1
	int off = off0, len;
@


1.75
log
@remove drain hooks from pool.
1.  drain hooks and lists of allocators make the code complicated
2.  the only hooks in the system are the mbuf reclaim routines
3.  if reclaim is actually able to put a meaningful amount of memory back
in the system, i think something else is dicked up.  ie, if reclaiming
your ip fragment buffers makes the difference thrashing swap and not,
your system is in a load of trouble.
4.  it's a scary amount of code running with very weird spl requirements
and i'd say it's pretty much totally untested.  raise your hand if your
router is running at the edge of swap.
5.  the reclaim stuff goes back to when mbufs lived in a tiny vm_map and
you could run out of va.  that's very unlikely (like impossible) now.
ok/tested pedro krw sturm
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.74 2006/03/17 04:21:57 brad Exp $	*/
d110 1
a110 1
 * Initialize the mbuf allcator.
@


1.74
log
@rev 1.77

m_cat() - if it is safe, copy data portion into 1st mbuf even if 1st mbuf
is M_EXT mbuf.

rev 1.72

clarify comment on m_cat().

From itojun NetBSD

ok claudio@@ mcbride@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.73 2006/03/05 00:44:25 brad Exp $	*/
a116 3

	pool_set_drain_hook(&mbpool, m_reclaim, NULL);
	pool_set_drain_hook(&mclpool, m_reclaim, NULL);
@


1.73
log
@splimp -> splvm
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.72 2006/01/05 05:05:06 jsg Exp $	*/
d436 3
a438 1
 * Both chains must be of the same type (e.g. MT_DATA).
d447 1
a447 2
		if (m->m_flags & M_EXT ||
		    m->m_data + m->m_len + n->m_len >= &m->m_dat[MLEN]) {
@


1.72
log
@ansi/deregister
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.71 2005/12/31 19:18:05 krw Exp $	*/
d149 1
a149 1
	int s = splimp();
@


1.71
log
@Nuke unused variable 'space' found by lint. Eliminate some trailing
whitespace. No binary diff on i386.

ok pedro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.70 2004/05/27 04:55:28 tedu Exp $	*/
d113 1
a113 1
mbinit()
d147 2
a148 2
	register struct domain *dp;
	register struct protosw *pr;
d165 1
a165 2
m_get(nowait, type)
	int nowait, type;
d167 1
a167 1
	register struct mbuf *m;
d174 1
a174 2
m_gethdr(nowait, type)
	int nowait, type;
d176 1
a176 1
	register struct mbuf *m;
d183 1
a183 2
m_getclr(nowait, type)
	int nowait, type;
d185 1
a185 1
	register struct mbuf *m;
d195 1
a195 2
m_free(m)
	struct mbuf *m;
d197 1
a197 1
	register struct mbuf *n;
d204 1
a204 2
m_freem(m)
	register struct mbuf *m;
d206 1
a206 1
	register struct mbuf *n;
d225 1
a225 3
m_prepend(m, len, how)
	register struct mbuf *m;
	int len, how;
d252 1
a252 4
m_copym(m, off0, len, wait)
	struct mbuf *m;
	int off0, wait;
	int len;
d262 1
a262 4
m_copym2(m, off0, len, wait)
	struct mbuf *m;
	int off0, wait;
	int len;
d268 1
a268 5
m_copym0(m, off0, len, wait, deep)
	struct mbuf *m;
	int off0, wait;
	int len;
	int deep;	/* deep copy */
d356 1
a356 5
m_copydata(m, off, len, cp)
	register struct mbuf *m;
	register int off;
	register int len;
	caddr_t cp;
d358 1
a358 1
	register unsigned count;
d391 1
a391 5
m_copyback(m0, off, len, cp)
	struct	mbuf *m0;
	register int off;
	register int len;
	const void *cp;
d393 2
a394 2
	register int mlen;
	register struct mbuf *m = m0, *n;
d440 1
a440 2
m_cat(m, n)
	register struct mbuf *m, *n;
d460 5
a464 7
m_adj(mp, req_len)
	struct mbuf *mp;
	int req_len;
{
	register int len = req_len;
	register struct mbuf *m;
	register int count;
d542 1
a542 3
m_pullup(n, len)
	register struct mbuf *n;
	int len;
d544 2
a545 2
	register struct mbuf *m;
	register int count;
d605 2
a606 4
struct mbuf *
m_pullup2(n, len)
	struct mbuf *n;
	int len;
d669 1
a669 4
m_getptr(m, loc, off)
	struct mbuf *m;
	int loc;
	int *off;
d707 1
a707 3
m_inject(m0, len0, siz, wait)
	register struct mbuf *m0;
	int len0, siz, wait;
d709 1
a709 1
	register struct mbuf *m, *n, *n2 = NULL, *n3;
d763 1
a763 3
m_split(m0, len0, wait)
	register struct mbuf *m0;
	int len0, wait;
d765 1
a765 1
	register struct mbuf *m, *n;
d824 2
a825 5
m_devget(buf, totlen, off0, ifp, copy)
	char *buf;
	int totlen, off0;
	struct ifnet *ifp;
	void (*copy)(const void *, void *, size_t);
d827 1
a827 1
	register struct mbuf *m;
d829 2
a830 2
	register int off = off0, len;
	register char *cp;
d893 1
a893 2
m_zero(m)
	struct mbuf *m;
d917 2
a918 7
m_apply(m, off, len, f, fstate)
	struct mbuf *m;
	int off;
	int len;
	/* fstate, data, len */
	int (*f)(caddr_t, caddr_t, unsigned int);
	caddr_t fstate;
@


1.70
log
@change uvm_km_getpage to take waitok argument and sleep if appropriate.
change both the nointr and default pool allocators to using uvm_km_getpage.
change pools to default to a maxpages value of 8, so they hoard less memory.
change mbuf pools to use default pool allocator.
pools are now more efficient, use less of kmem_map, and a bit faster.
tested mcbride, deraadt, pedro, drahn, miod to work everywhere
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.69 2004/05/23 19:41:23 tedu Exp $	*/
d631 2
a632 2
 * headers.     
 *		      
d635 3
a637 3
struct mbuf *   
m_pullup2(n, len)       
	register struct mbuf *n;
d640 2
a641 3
	register struct mbuf *m;
	register int count;
	int space; 
d657 1
a657 1
			goto bad; 
d665 1
a665 1
			m->m_flags = (n->m_flags & M_COPYFLAGS) | 
a678 1
		space -= count;
d687 1
a687 1
	}	 
d689 1
a689 1
		
d691 1
a691 1
bad:	    
@


1.69
log
@bad stuff escaped by accident
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.67 2004/04/19 22:52:33 tedu Exp $	*/
a101 2
void	*mclpool_alloc(struct pool *, int);
void	mclpool_release(struct pool *, void *);
a108 4
struct pool_allocator mclpool_allocator = {
	mclpool_alloc, mclpool_release, 0,
};

d115 2
a116 2
	pool_init(&mbpool, MSIZE, 0, 0, 0, "mbpl", &mclpool_allocator);
	pool_init(&mclpool, MCLBYTES, 0, 0, 0, "mclpl", &mclpool_allocator);
a141 14
}



void *
mclpool_alloc(struct pool *pp, int flags)
{
	return uvm_km_getpage();
}

void
mclpool_release(struct pool *pp, void *v)
{
	uvm_km_putpage(v);
@


1.68
log
@according to fork1(9), retval is optional.  make it so.
from form@@pdp-11.org.ru via mpech.  ok millert
@
text
@a94 3
struct pool_cache mbpool_cache;
struct pool_cache mclpool_cache;

a125 3

	pool_cache_init(&mbpool_cache, &mbpool, NULL, NULL, NULL);
	pool_cache_init(&mclpool_cache, &mclpool, NULL, NULL, NULL);
@


1.67
log
@introduce a new km_page allocator that gets pages from kernel_map using
an interrupt safe thread.
use this as the new backend for mbpool and mclpool, eliminating the mb_map.
introduce a sysctl kern.maxclusters which controls the limit of clusters
allocated.
testing by many people, works everywhere but m68k.  ok deraadt@@

this essentially deprecates the NMBCLUSTERS option, don't use it.
this should reduce pressure on the kmem_map and the uvm reserve of static
map entries.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.66 2004/04/17 10:18:12 mcbride Exp $	*/
d95 3
d129 3
@


1.66
log
@Don't forget to apply the M_CLUSTER flag when copying flags in m_pullup2().
Fixes pr3740. Confirmed with pb@@, ok markus@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.65 2004/04/01 23:56:05 tedu Exp $	*/
d105 2
d109 1
a109 1
    "WARNING: mclpool limit reached; increase NMBCLUSTERS";
d121 1
a121 7
	vaddr_t minaddr, maxaddr;

	minaddr = vm_map_min(kernel_map);
	mb_map = uvm_km_suballoc(kernel_map, &minaddr, &maxaddr,
	    nmbclust*(MCLBYTES), VM_MAP_INTRSAFE, FALSE, NULL);

	pool_init(&mbpool, MSIZE, 0, 0, 0, "mbpl", NULL);
d127 1
a127 6
	/*
	 * Set the hard limit on the mclpool to the number of
	 * mbuf clusters the kernel is to support.  Log the limit
	 * reached message max once a minute.
	 */
	(void)pool_sethardlimit(&mclpool, nmbclust, mclpool_warnmsg, 60);
d139 12
d155 1
a155 4
	boolean_t waitok = (flags & PR_WAITOK) ? TRUE : FALSE;

	return ((void *)uvm_km_alloc_poolpage1(mb_map, uvmexp.mb_object,
	    waitok));
d161 1
a161 1
	uvm_km_free_poolpage1(mb_map, (vaddr_t)v);
@


1.65
log
@use NULL for ptrs.  parts from Joris Vink
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.64 2004/01/28 20:19:24 dhartmei Exp $	*/
d686 2
a687 1
			m->m_flags = (n->m_flags & M_COPYFLAGS) | M_EXT;
@


1.64
log
@oh, i'll happily steal the commit. ok henning@@, markus@@, otto@@ (iirc)
from KOZUKA Masahiro, fixes PR 3651
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.63 2003/08/12 05:09:18 mickey Exp $	*/
d317 1
a317 1
		if (m == 0)
d325 1
a325 1
	top = 0;
d327 1
a327 1
		if (m == 0) {
d329 1
a329 1
				panic("m_copym0: m == 0 and not COPYALL");
d334 1
a334 1
		if (n == 0)
d379 1
a379 1
	if (top == 0)
d385 1
a385 1
	return (0);
d442 1
a442 1
	if (m0 == 0)
d447 1
a447 1
		if (m->m_next == 0) {
d449 1
a449 1
			if (n == 0)
d466 1
a466 1
		if (m->m_next == 0) {
d468 1
a468 1
			if (n == 0)
d744 1
a744 2
	    		}
	    		else
@


1.63
log
@src argument to m_copyback() can be a const; itojun@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.62 2003/06/02 23:28:06 millert Exp $	*/
d868 1
a868 1
		n->m_flags |= M_EXT;
@


1.62
log
@Remove the advertising clause in the UCB license which Berkeley
rescinded 22 July 1999.  Proofed by myself and Theo.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.61 2003/06/01 16:23:41 art Exp $	*/
d436 1
a436 1
	caddr_t cp;
@


1.61
log
@uvm_km_suballoc passes the 'min' argument untouched to uvm_map. uvm_map
uses it as a hint for where to steal space from the parent map. We've been
passing random stack garbage as that hint for ages. It's a wonder it didn't
break things until we started working on Hammer.

noone objected for at least a week.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.60 2003/04/23 01:36:52 jason Exp $	*/
d16 1
a16 5
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.60
log
@Move m_copyback() to uipc_mbuf where it makes some kinda sense; ok dhartmei
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.59 2003/02/12 14:41:07 jason Exp $	*/
d125 1
@


1.59
log
@Remove commons; inspired by netbsd.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.58 2002/07/03 21:19:08 miod Exp $	*/
d426 54
@


1.58
log
@Change all variables definitions (int foo) in sys/sys/*.h to variable
declarations (extern int foo), and compensate in the appropriate locations.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.57 2002/03/14 01:27:05 millert Exp $	*/
d95 1
@


1.57
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.56 2002/02/25 04:53:16 dhartmei Exp $	*/
d99 5
@


1.56
log
@Make pool_sethardlimit() check that it doesn't decrease the limit below
the current size of the pool. ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.55 2002/02/17 22:59:53 maja Exp $	*/
d100 3
a102 3
void	*mclpool_alloc __P((struct pool *, int));
void	mclpool_release __P((struct pool *, void *));
struct mbuf *m_copym0 __P((struct mbuf *, int, int, int, int));
d832 1
a832 1
	void (*copy) __P((const void *, void *, size_t));
@


1.55
log
@Patch from Daniel Lucq <daniel@@lucq.org>

The patch allows you to change the value of NMBCLUSTERS, BUFCACHEPERCENT
and NKMEMPAGES using the config command, instead of recompiling the kernel.

This is the kernel part of the patch. I have compiled it on i386, sparc64,
alpha and macppc. -moj ok art@@ maja@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.54 2002/02/05 22:06:43 angelos Exp $	*/
d133 1
a133 1
	pool_sethardlimit(&mclpool, nmbclust, mclpool_warnmsg, 60);
@


1.54
log
@Zero-ize ext even if it has a free method defined.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.53 2002/02/05 21:59:18 angelos Exp $	*/
d120 1
a120 1
	    VM_MBUF_SIZE, VM_MAP_INTRSAFE, FALSE, NULL);
d133 1
a133 1
	pool_sethardlimit(&mclpool, nmbclusters, mclpool_warnmsg, 60);
@


1.53
log
@panic if a read-only mbuf is given to m_zero() --- from art@@openbsd.org
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.52 2002/02/05 21:47:59 angelos Exp $	*/
d908 1
a908 2
		if ((m->m_flags & M_EXT) &&
		    (m->m_ext.ext_free == NULL))
@


1.52
log
@Fix m_zero() yet again -- yesterday's fix wasn't sufficient.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.51 2002/02/04 21:44:16 angelos Exp $	*/
d904 4
d909 1
a909 2
		    (m->m_ext.ext_free == NULL) &&
		    !MCLISREFERENCED(m))
@


1.51
log
@Also, cleanup any external buffers first (paranoid).
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.50 2002/02/04 20:50:42 jason Exp $	*/
d908 6
a913 5
		if (m->m_flags & M_PKTHDR)
			memset((caddr_t)m + sizeof(struct m_hdr) +
			    sizeof(struct pkthdr), 0, MHLEN);
		else
			memset((caddr_t)m + sizeof(struct m_hdr), 0, MLEN);
@


1.50
log
@Revision 1.37 was borked... This time, fix the casts and address the
void * arithmetic problem correctly in m_zero()
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.49 2002/01/25 15:50:22 art Exp $	*/
d904 4
a912 4
		if ((m->m_flags & M_EXT) &&
		    (m->m_ext.ext_free == NULL) &&
		    !MCLISREFERENCED(m))
			memset(m->m_ext.ext_buf, 0, m->m_ext.ext_size);
@


1.49
log
@Add a drain hook to each pool. This hook is called in three cases.
1. When a pool hit the hard limit. Just before bailing out/sleeping.
2. When an allocator fails to allocate memory (with PR_NOWAIT).
3. Just before trying to reclaim some page in pool_reclaim.

The function called form the hook should try to free some items to the
pool if possible.

Convert m_reclaim hooks that were embedded in MCLGET, MGET and MGETHDR
into a pool drain hook (making the code much cleaner).
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.48 2002/01/23 17:51:52 art Exp $	*/
d905 2
a906 2
			memset((void *)(m + sizeof(struct m_hdr) +
			    sizeof(struct pkthdr)), 0, MHLEN);
d908 1
a908 1
			memset((void *)(m + sizeof(struct m_hdr)), 0, MLEN);
@


1.48
log
@move mb_map allocation to mbinit()
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.47 2002/01/23 17:35:57 art Exp $	*/
a98 1
int	needqueuedrain;
d125 3
a160 51
/*
 * When MGET failes, ask protocols to free space when short of memory,
 * then re-attempt to allocate an mbuf.
 */
struct mbuf *
m_retry(i, t)
	int i, t;
{
	register struct mbuf *m;

	if (i & M_DONTWAIT) {
		needqueuedrain = 1;
		setsoftnet();
		return (NULL);
	}
	m_reclaim();
#define m_retry(i, t)	NULL
	MGET(m, i, t);
#undef m_retry
	if (m != NULL)
		mbstat.m_wait++;
	else
		mbstat.m_drops++;
	return (m);
}

/*
 * As above; retry an MGETHDR.
 */
struct mbuf *
m_retryhdr(i, t)
	int i, t;
{
	register struct mbuf *m;

	if (i & M_DONTWAIT) {
		needqueuedrain = 1;
		setsoftnet();
		return (NULL);
	}
	m_reclaim();
#define m_retryhdr(i, t) NULL
	MGETHDR(m, i, t);
#undef m_retryhdr
	if (m != NULL)
		mbstat.m_wait++;
	else
		mbstat.m_drops++;
	return (m);
}

d162 1
a162 1
m_reclaim()
a167 1
	needqueuedrain = 0;
@


1.47
log
@move definition of mb_map from zillions of machdep.c to uipc_mbuf.c
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.46 2002/01/23 00:39:47 art Exp $	*/
d118 5
@


1.46
log
@Pool deals fairly well with physical memory shortage, but it doesn't deal
well (not at all) with shortages of the vm_map where the pages are mapped
(usually kmem_map).

Try to deal with it:
 - group all information the backend allocator for a pool in a separate
   struct. The pool will only have a pointer to that struct.
 - change the pool_init API to reflect that.
 - link all pools allocating from the same allocator on a linked list.
 - Since an allocator is responsible to wait for physical memory it will
   only fail (waitok) when it runs out of its backing vm_map, carefully
   drain pools using the same allocator so that va space is freed.
   (see comments in code for caveats and details).
 - change pool_reclaim to return if it actually succeeded to free some
   memory, use that information to make draining easier and more efficient.
 - get rid of PR_URGENT, noone uses it.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.45 2002/01/16 20:50:17 miod Exp $	*/
d98 1
a98 1
extern	struct vm_map *mb_map;
@


1.45
log
@Don't include <sys/map.h> when you don't need what's in it.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.44 2001/12/18 23:07:49 deraadt Exp $	*/
d101 2
a102 2
void	*mclpool_alloc __P((unsigned long, int, int));
void	mclpool_release __P((void *, unsigned long, int));
d108 4
d118 2
a119 3
	pool_init(&mbpool, MSIZE, 0, 0, 0, "mbpl", 0, NULL, NULL, 0);
	pool_init(&mclpool, MCLBYTES, 0, 0, 0, "mclpl", 0, mclpool_alloc,
	    mclpool_release, 0);
d140 1
a140 4
mclpool_alloc(sz, flags, mtype)
	unsigned long sz;
	int flags;
	int mtype;
d149 1
a149 4
mclpool_release(v, sz, mtype)
	void *v;
	unsigned long sz;
	int mtype;
@


1.44
log
@NRL license cleaning
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.43 2001/11/28 16:13:29 art Exp $	*/
a82 1
#include <sys/map.h>
@


1.44.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.49 2002/01/25 15:50:22 art Exp $	*/
d83 1
d99 2
a100 1
struct vm_map *mb_map;
d102 2
a103 2
void	*mclpool_alloc __P((struct pool *, int));
void	mclpool_release __P((struct pool *, void *));
a108 4
struct pool_allocator mclpool_allocator = {
	mclpool_alloc, mclpool_release, 0,
};

d115 3
a117 10
	vaddr_t minaddr, maxaddr;

	mb_map = uvm_km_suballoc(kernel_map, &minaddr, &maxaddr,
	    VM_MBUF_SIZE, VM_MAP_INTRSAFE, FALSE, NULL);

	pool_init(&mbpool, MSIZE, 0, 0, 0, "mbpl", NULL);
	pool_init(&mclpool, MCLBYTES, 0, 0, 0, "mclpl", &mclpool_allocator);

	pool_set_drain_hook(&mbpool, m_reclaim, NULL);
	pool_set_drain_hook(&mclpool, m_reclaim, NULL);
d138 4
a141 1
mclpool_alloc(struct pool *pp, int flags)
d150 4
a153 1
mclpool_release(struct pool *pp, void *v)
d158 51
d210 1
a210 1
m_reclaim(void *arg, int flags)
d216 1
@


1.44.2.2
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.44.2.1 2002/01/31 22:55:41 niklas Exp $	*/
d151 2
a152 1
	return ((void *)uvm_km_alloc_poolpage1(mb_map, NULL, waitok));
@


1.44.2.3
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.44.2.2 2002/02/02 03:28:25 art Exp $	*/
d100 3
a102 3
void	*mclpool_alloc(struct pool *, int);
void	mclpool_release(struct pool *, void *);
struct mbuf *m_copym0(struct mbuf *, int, int, int, int);
d120 1
a120 1
	    nmbclust*(MCLBYTES), VM_MAP_INTRSAFE, FALSE, NULL);
d133 1
a133 1
	(void)pool_sethardlimit(&mclpool, nmbclust, mclpool_warnmsg, 60);
d831 1
a831 1
	void (*copy)(const void *, void *, size_t);
d903 8
a910 5
#ifdef DIAGNOSTIC
		if (M_READONLY(m))
			panic("m_zero: M_READONLY");
#endif /* DIAGNOSTIC */
		if (m->m_flags & M_EXT)
a911 6
		else {
			if (m->m_flags & M_PKTHDR)
				memset(m->m_pktdat, 0, MHLEN);
			else
				memset(m->m_dat, 0, MLEN);
		}
@


1.44.2.4
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.44.2.3 2002/06/11 03:29:40 art Exp $	*/
a98 5

int max_linkhdr;		/* largest link-level header */
int max_protohdr;		/* largest protocol header */
int max_hdr;			/* largest link+protocol header */
int max_datalen;		/* MHLEN - max_hdr */
@


1.44.2.5
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a94 1
struct	mbstat mbstat;		/* mbuf stats */
a423 54
}

/*
 * Copy data from a buffer back into the indicated mbuf chain,
 * starting "off" bytes from the beginning, extending the mbuf
 * chain if necessary. The mbuf needs to be properly initialized
 * including the setting of m_len.
 */
void
m_copyback(m0, off, len, cp)
	struct	mbuf *m0;
	register int off;
	register int len;
	caddr_t cp;
{
	register int mlen;
	register struct mbuf *m = m0, *n;
	int totlen = 0;

	if (m0 == 0)
		return;
	while (off > (mlen = m->m_len)) {
		off -= mlen;
		totlen += mlen;
		if (m->m_next == 0) {
			n = m_getclr(M_DONTWAIT, m->m_type);
			if (n == 0)
				goto out;
			n->m_len = min(MLEN, len + off);
			m->m_next = n;
		}
		m = m->m_next;
	}
	while (len > 0) {
		mlen = min (m->m_len - off, len);
		bcopy(cp, off + mtod(m, caddr_t), (unsigned)mlen);
		cp += mlen;
		len -= mlen;
		mlen += off;
		off = 0;
		totlen += mlen;
		if (len == 0)
			break;
		if (m->m_next == 0) {
			n = m_get(M_DONTWAIT, m->m_type);
			if (n == 0)
				break;
			n->m_len = min(MLEN, len);
			m->m_next = n;
		}
		m = m->m_next;
	}
out:	if (((m = m0)->m_flags & M_PKTHDR) && (m->m_pkthdr.len < totlen))
		m->m_pkthdr.len = totlen;
@


1.43
log
@zap some typedefs.
vm_map_t -> struct vm_map *
vm_map_entry_t -> struct vm_map_entry *
simple_lock_data_t -> struct simplelock

(uvm not done yet, coming in the next commit)
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.42 2001/11/06 19:53:20 miod Exp $	*/
d40 38
a77 10
%%% portions-copyright-nrl-95
Portions of this software are Copyright 1995-1998 by Randall Atkinson,
Ronald Lee, Daniel McDonald, Bao Phan, and Chris Winters. All Rights
Reserved. All rights under this copyright have been assigned to the US
Naval Research Laboratory (NRL). The NRL Copyright Notice and License
Agreement Version 1.1 (January 17, 1995) applies to these portions of the
software.
You should have received a copy of the license with this software. If you
didn't get a copy, you may request one from <license@@ipv6.nrl.navy.mil>.
*/
@


1.42
log
@Replace inclusion of <vm/foo.h> with the correct <uvm/bar.h> when necessary.
(Look ma, I might have broken the tree)
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.41 2001/09/12 00:23:33 art Exp $	*/
d71 1
a71 1
extern	vm_map_t mb_map;
@


1.41
log
@mbutl no more
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.40 2001/06/27 04:49:47 art Exp $	*/
a64 2

#include <vm/vm.h>
@


1.40
log
@remove old vm
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.39 2001/06/27 03:53:50 angelos Exp $	*/
a73 1
struct	mbuf *mbutl;
@


1.39
log
@Update comment.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.38 2001/06/27 03:49:52 angelos Exp $	*/
a67 1
#if defined(UVM)
a68 1
#endif
a117 1
#if defined(UVM)
a121 3
#else
	return pool_page_alloc_nointr(sz, flags, mtype);
#endif
a129 1
#if defined(UVM)
a130 3
#else
	pool_page_free_nointr(v, sz, mtype);
#endif
@


1.38
log
@Get rid of M_COPY_* macros; either use M_MOVE_* or M_DUP_*, depending
on how macros should be treated. Code by fgsch@@, ok by me and itojun@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.37 2001/06/26 06:27:40 aaron Exp $	*/
d662 1
a662 1
			/* M_COPY_PKTHDR(m, n); */
@


1.37
log
@Appease gcc by not using void pointers in arithmetic operations; art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.36 2001/06/25 02:52:18 angelos Exp $	*/
d293 1
a293 1
		M_COPY_PKTHDR(mn, m);
d595 1
a595 1
			M_COPY_PKTHDR(m, n);
d664 1
a664 1
			M_COPY_HDR(m, n);
@


1.36
log
@Don't reset M_PKTHDR before we copy the mbuf flags (bad Federico!)
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.35 2001/06/25 01:50:16 fgsch Exp $	*/
d939 2
a940 2
			memset((void *)m + sizeof(struct m_hdr) +
			    sizeof(struct pkthdr), 0, MHLEN);
d942 1
a942 1
			memset((void *)m + sizeof(struct m_hdr), 0, MLEN);
@


1.35
log
@Move common post M_COPY_HDR manipulation to the macro itself; angelos@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.34 2001/05/26 06:59:14 angelos Exp $	*/
d292 1
a292 1
	if (m->m_flags & M_PKTHDR) {
a293 3
		m->m_flags &= ~M_PKTHDR;
		m_tag_init(m);
	}
d594 1
a594 1
		if (n->m_flags & M_PKTHDR) {
a595 3
			n->m_flags &= ~M_PKTHDR;
			m_tag_init(n);
		}
d663 1
a664 1
			m->m_flags = (n->m_flags & M_COPYFLAGS) | M_EXT;
@


1.34
log
@Use m_tag_init() and M_COPY_HDR().
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.33 2001/05/26 05:46:33 angelos Exp $	*/
d667 2
a668 1
			/* M_COPY_PKTHDR(m, n);*//* Too many adverse side effects. */
a670 2
			n->m_flags &= ~M_PKTHDR;
			m_tag_init(n);
@


1.33
log
@Fix backpointer.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.32 2001/05/24 10:59:23 angelos Exp $	*/
d295 1
a295 1
		TAILQ_INIT(&m->m_pkthdr.tags);
d600 1
a600 1
			TAILQ_INIT(&n->m_pkthdr.tags);
d668 1
a668 5
			m->m_pkthdr = n->m_pkthdr;
			if (TAILQ_EMPTY(&n->m_pkthdr.tags))
				TAILQ_INIT(&m->m_pkthdr.tags);
			else 
				TAILQ_FIRST(&m->m_pkthdr.tags)->m_tag_link.tqe_prev = &m->m_pkthdr.tags.tqh_first;
d671 1
a671 1
			TAILQ_INIT(&n->m_pkthdr.tags);
@


1.32
log
@If the copied tag container is empty, initialize it properly.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.31 2001/05/20 08:31:46 angelos Exp $	*/
d646 1
a648 1
 
d671 2
@


1.31
log
@m_tag_* routines, plus minor changes (convert from tdbi to tags)
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.30 2001/05/18 23:29:33 millert Exp $	*/
d669 2
@


1.30
log
@Use pool_page_alloc_nointr() and pool_page_free_nointr() since they
are exported wheras pool_page_alloc() and pool_page_free() are
static.  In the non-UVM case these just call pool_page_alloc() and
pool_page_free() anyway.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.29 2001/05/17 18:41:44 provos Exp $	*/
d295 1
a295 1
		m->m_pkthdr.tdbi = NULL;
d600 1
a600 1
			n->m_pkthdr.tdbi = NULL;
d671 1
a671 1
			n->m_pkthdr.tdbi = NULL;
@


1.29
log
@convert mbuf and cluster allocation to pool, mostly from NetBSD
okay art@@ miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.28 2001/05/16 08:59:04 art Exp $	*/
d126 1
a126 1
	return pool_page_alloc(sz, flags, mtype);
d139 1
a139 1
	pool_page_free(v, sz, mtype);
@


1.28
log
@Introduce a new kernel option "SMALL_KERNEL" that will be used to keep the
kernel size down.

Two changes. Uninline MALLOC and uninline a few mbuf macros. Saves 140k
on alpha RAMDISK (although only 11k after gzip).
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.27 2001/05/05 20:57:00 art Exp $	*/
d62 1
d72 3
a76 1
char	*mclrefcnt;
d79 10
d92 10
a101 1
	int s;
d103 8
a110 8
	s = splimp();
	if (m_clalloc(max(4096 / PAGE_SIZE, 1), M_DONTWAIT) == 0)
		goto bad;
	splx(s);
	return;
bad:
	splx(s);
	panic("mbinit");
a112 16
/*
 * Allocate some number of mbuf clusters
 * and place on cluster free list.
 * Must be called at splimp.
 */
/* ARGSUSED */
int
m_clalloc(ncl, nowait)
	register int ncl;
	int nowait;
{
	volatile static struct timeval lastlogged;
	struct timeval curtime, logdiff;
	register caddr_t p;
	register int i;
	int npg, s;
d114 22
a135 1
	npg = ncl;
d137 1
a137 2
	p = (caddr_t)uvm_km_kmemalloc(mb_map, uvmexp.mb_object, ctob(npg),
	    nowait ? 0 : UVM_KMF_NOWAIT);
d139 1
a139 1
	p = (caddr_t)kmem_malloc(mb_map, ctob(npg), !nowait);
a140 21
	if (p == NULL) {
		s = splclock();
		curtime = time;
		splx(s);
		timersub(&curtime, &lastlogged, &logdiff);
		if (logdiff.tv_sec >= 60) {
			lastlogged = curtime;
			log(LOG_ERR, "mb_map full\n");
		}
		m_reclaim();
		return (mclfree != NULL);
	}
	ncl = ncl * PAGE_SIZE / MCLBYTES;
	for (i = 0; i < ncl; i++) {
		((union mcluster *)p)->mcl_next = mclfree;
		mclfree = (union mcluster *)p;
		p += MCLBYTES;
		mbstat.m_clfree++;
	}
	mbstat.m_clusters += ncl;
	return (1);
d162 4
d187 4
d244 1
a244 1
	bzero(mtod(m, caddr_t), MLEN);
d314 1
a314 1
	register struct mbuf *m;
d316 1
a316 1
	register int len;
d318 1
a318 64
	register struct mbuf *n, **np;
	register int off = off0;
	struct mbuf *top;
	int copyhdr = 0;

	if (off < 0)
		panic("m_copym: off %d < 0", off);
	if (len < 0)
		panic("m_copym: len %d < 0", len);
	if (off == 0 && m->m_flags & M_PKTHDR)
		copyhdr = 1;
	while (off > 0) {
		if (m == NULL)
			panic("m_copym: null mbuf");
		if (off < m->m_len)
			break;
		off -= m->m_len;
		m = m->m_next;
	}
	np = &top;
	top = NULL;
	while (len > 0) {
		if (m == NULL) {
			if (len != M_COPYALL)
				panic("m_copym: %d not M_COPYALL", len);
			break;
		}
		MGET(n, wait, m->m_type);
		*np = n;
		if (n == NULL)
			goto nospace;
		if (copyhdr) {
			M_DUP_PKTHDR(n, m);
			if (len == M_COPYALL)
				n->m_pkthdr.len -= off0;
			else
				n->m_pkthdr.len = len;
			copyhdr = 0;
		}
		n->m_len = min(len, m->m_len - off);
		if (m->m_flags & M_EXT) {
			n->m_data = m->m_data + off;
			if (!m->m_ext.ext_ref)
				mclrefcnt[mtocl(m->m_ext.ext_buf)]++;
			else
				(*(m->m_ext.ext_ref))(m);
			n->m_ext = m->m_ext;
			n->m_flags |= M_EXT;
		} else
			bcopy(mtod(m, caddr_t)+off, mtod(n, caddr_t),
			    (unsigned)n->m_len);
		if (len != M_COPYALL)
			len -= n->m_len;
		off = 0;
		m = m->m_next;
		np = &n->m_next;
	}
	if (top == NULL)
		MCFail++;
	return (top);
nospace:
	m_freem(top);
	MCFail++;
	return (NULL);
d327 10
a336 1
	register struct mbuf *m;
d338 2
a339 1
	register int len;
d341 2
a342 2
	register struct mbuf *n, **np;
	register int off = off0;
d346 2
a347 4
	if (len < 0)
		panic("m_copym2: len %d < 0", len);
	if (off < 0)
		panic("m_copym2: off %d < 0", off);
d351 2
a352 2
		if (m == NULL)
			panic("m_copym2: null mbuf");
d359 1
a359 1
	top = NULL;
d361 1
a361 1
		if (m == NULL) {
d363 1
a363 1
				panic("m_copym2: %d != M_COPYALL", len);
d368 1
a368 1
		if (n == NULL)
d379 16
a394 5
		if ((m->m_flags & M_EXT) && (n->m_len > MHLEN)) {
			/* This is a cheesy hack. */
			MCLGET(n, wait);
			if (n->m_flags & M_EXT)
				bcopy(mtod(m, caddr_t) + off, mtod(n, caddr_t),
d396 1
a396 2
			else
				goto nospace;
d398 1
a398 1
			bcopy(mtod(m, caddr_t) + off, mtod(n, caddr_t),
d402 9
a410 2
		off = 0;
		m = m->m_next;
d413 1
a413 1
	if (top == NULL)
d419 1
a419 1
	return (NULL);
d853 1
a853 6
		n->m_ext = m->m_ext;
		if(!m->m_ext.ext_ref)
			mclrefcnt[mtocl(m->m_ext.ext_buf)]++;
		else
			(*(m->m_ext.ext_ref))(m);
		m->m_ext.ext_size = 0; /* For Accounting XXXXXX danger */
d946 2
a947 2
			bzero((void *)m + sizeof(struct m_hdr) +
			    sizeof(struct pkthdr), MHLEN);
d949 1
a949 1
			bzero((void *)m + sizeof(struct m_hdr), MLEN);
d952 2
a953 2
		    !mclrefcnt[mtocl((m)->m_ext.ext_buf)])
			bzero(m->m_ext.ext_buf, m->m_ext.ext_size);
d1029 1
a1029 1
#endif /* SMALL_KERNEL */@


1.27
log
@Get rid of CLSIZE and all related stuff.
CLSIZE -> 1
CLBYTES -> PAGE_SIZE
OLOFSET -> PAGE_MASK
etc.
At the same time some archs needed some cleaning in vmparam.h so that
goes in at the same time.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.26 2001/04/05 10:52:45 art Exp $	*/
d1032 28
@


1.26
log
@From angelos: (he can't commit this himself right now)
> I must have been on drugs...a deep copy is needed, or else there's double
> free's when there's IPsec.
[...]
> This should solve the crash problems.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.25 2001/03/30 19:24:35 angelos Exp $	*/
d82 1
a82 1
	if (m_clalloc(max(4096 / CLBYTES, 1), M_DONTWAIT) == 0)
d108 1
a108 1
	npg = ncl * CLSIZE;
d127 1
a127 1
	ncl = ncl * CLBYTES / MCLBYTES;
@


1.25
log
@Be consistent when adjusting pkthdr.len; it doesn't matter currently,
since the callers always do the right thing, but it might in the
future. Pointed out by art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.24 2001/03/28 20:03:00 angelos Exp $	*/
d337 1
a337 1
			M_COPY_PKTHDR(n, m);
d413 1
a413 1
			M_COPY_PKTHDR(n, m);
@


1.24
log
@Allow tdbi's to appear in mbufs throughout the stack; this allows
security properties of the packets to be pushed up to the application
(not done yet). Eventually, this will be turned into a packet
attributes framework.

Make sure tdbi's are free'd/cleared properly whenever drivers (or NFS)
does weird things with mbufs.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.23 2001/03/25 07:07:57 csapuntz Exp $	*/
d809 2
a810 1
	m0->m_pkthdr.len += siz;
@


1.23
log
@Not really a bug but one day panic might throw an exception or something. THanks to dawson and team for fix
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.22 2000/03/03 11:15:43 angelos Exp $	*/
d282 1
d625 1
d696 1
d788 1
a788 1
	        if ((m->m_next) &&  (M_LEADINGSPACE(m->m_next) >= siz)) {
d790 2
a791 1
			m0->m_pkthdr.len += siz;
d846 1
a846 1
		n->m_pkthdr = m0->m_pkthdr;
d893 1
@


1.22
log
@Style.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.21 2000/03/03 06:19:22 angelos Exp $	*/
d87 1
@


1.21
log
@Remove extraneous newline.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.20 2000/03/02 21:40:49 angelos Exp $	*/
d727 4
a730 1
m_getptr(struct mbuf *m, int loc, int *off)
d732 20
a751 18
    while (loc >= 0)
    {
	/* Normal end of search */
	if (m->m_len > loc)
	{
	    *off = loc;
	    return m;
	}
	else
	{
	    loc -= m->m_len;

	    if (m->m_next == NULL)
	    {
		if (loc == 0)
		{
		    *off = m->m_len; /* Point at the end of valid data */
		    return m;
d753 1
a753 7
		else
		  return NULL;
	    }
	    else
	      m = m->m_next;
	}
    }
d755 1
a755 1
    return NULL;
@


1.20
log
@New function: m_getptr(), takes as argument an mbuf chain and an
offset, returns a pointer to them specific mbuf and the offset inside
it that corresponds to the offset argument (so one can find where the
n'th byte is in an mbuf).
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.19 1999/12/31 23:37:08 provos Exp $	*/
a1025 1

@


1.19
log
@copy pkthdr correctly in m_split, okay angelos@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.18 1999/12/05 07:30:31 angelos Exp $	*/
d721 36
@


1.19.2.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a723 35
 * Return a pointer to mbuf/offset of location in mbuf chain.
 */
struct mbuf *
m_getptr(m, loc, off)
	struct mbuf *m;
	int loc;
	int *off;
{
	while (loc >= 0) {
		/* Normal end of search */
		if (m->m_len > loc) {
	    		*off = loc;
	    		return (m);
		}
		else {
	    		loc -= m->m_len;

	    		if (m->m_next == NULL) {
				if (loc == 0) {
 					/* Point at the end of valid data */
		    			*off = m->m_len;
		    			return (m);
				}
				else
		  			return (NULL);
	    		}
	    		else
	      			m = m->m_next;
		}
    	}

	return (NULL);
}

/*
d990 1
@


1.19.2.2
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.26 2001/04/05 10:52:45 art Exp $	*/
a86 1
	splx(s);
a280 1
		m->m_pkthdr.tdbi = NULL;
d335 1
a335 1
			M_DUP_PKTHDR(n, m);
d411 1
a411 1
			M_DUP_PKTHDR(n, m);
a622 1
			n->m_pkthdr.tdbi = NULL;
a692 1
			n->m_pkthdr.tdbi = NULL;
d784 1
a784 1
	        if ((m->m_next) && (M_LEADINGSPACE(m->m_next) >= siz)) {
d786 1
a786 2
			if (m0->m_flags & M_PKTHDR)
				m0->m_pkthdr.len += siz;
d804 1
a804 2
	if (m0->m_flags & M_PKTHDR)
		m0->m_pkthdr.len += siz;
d841 1
a841 1
		M_DUP_PKTHDR(n, m0);
a887 1

@


1.19.2.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.19.2.2 2001/05/14 22:32:45 niklas Exp $	*/
a61 1
#include <sys/pool.h>
d67 1
d69 1
a69 3

struct	pool mbpool;		/* mbuf pool */
struct	pool mclpool;		/* mbuf cluster pool */
d73 1
a75 10
void	*mclpool_alloc __P((unsigned long, int, int));
void	mclpool_release __P((void *, unsigned long, int));
struct mbuf *m_copym0 __P((struct mbuf *, int, int, int, int));

const char *mclpool_warnmsg =
    "WARNING: mclpool limit reached; increase NMBCLUSTERS";

/*
 * Initialize the mbuf allcator.
 */
d79 1
a79 3
	pool_init(&mbpool, MSIZE, 0, 0, 0, "mbpl", 0, NULL, NULL, 0);
	pool_init(&mclpool, MCLBYTES, 0, 0, 0, "mclpl", 0, mclpool_alloc,
	    mclpool_release, 0);
d81 8
a88 15
	/*
	 * Set the hard limit on the mclpool to the number of
	 * mbuf clusters the kernel is to support.  Log the limit
	 * reached message max once a minute.
	 */
	pool_sethardlimit(&mclpool, nmbclusters, mclpool_warnmsg, 60);

	/*
	 * Set a low water mark for both mbufs and clusters.  This should
	 * help ensure that they can be allocated in a memory starvation
	 * situation.  This is important for e.g. diskless systems which
	 * must allocate mbufs in order for the pagedaemon to clean pages.
	 */
	pool_setlowat(&mbpool, mblowat);
	pool_setlowat(&mclpool, mcllowat);
d91 45
a135 20

void *
mclpool_alloc(sz, flags, mtype)
	unsigned long sz;
	int flags;
	int mtype;
{
	boolean_t waitok = (flags & PR_WAITOK) ? TRUE : FALSE;

	return ((void *)uvm_km_alloc_poolpage1(mb_map, uvmexp.mb_object,
	    waitok));
}

void
mclpool_release(v, sz, mtype)
	void *v;
	unsigned long sz;
	int mtype;
{
	uvm_km_free_poolpage1(mb_map, (vaddr_t)v);
a156 4
	if (m != NULL)
		mbstat.m_wait++;
	else
		mbstat.m_drops++;
a177 4
	if (m != NULL)
		mbstat.m_wait++;
	else
		mbstat.m_drops++;
d231 1
a231 1
	memset(mtod(m, caddr_t), 0, MLEN);
d279 5
a283 2
	if (m->m_flags & M_PKTHDR)
		M_MOVE_PKTHDR(mn, m);
d301 1
a301 1
	struct mbuf *m;
d303 1
a303 1
	int len;
d305 64
a368 1
	return m_copym0(m, off0, len, wait, 0);	/* shallow copy on M_EXT */
d377 1
a377 1
	struct mbuf *m;
d379 1
a379 11
	int len;
{
	return m_copym0(m, off0, len, wait, 1);	/* deep copy */
}

struct mbuf *
m_copym0(m, off0, len, wait, deep)
	struct mbuf *m;
	int off0, wait;
	int len;
	int deep;	/* deep copy */
d381 2
a382 2
	struct mbuf *n, **np;
	int off = off0;
d386 4
a389 2
	if (off < 0 || len < 0)
		panic("m_copym0: off %d, len %d", off, len);
d393 2
a394 2
		if (m == 0)
			panic("m_copym0: null mbuf");
d401 1
a401 1
	top = 0;
d403 1
a403 1
		if (m == 0) {
d405 1
a405 1
				panic("m_copym0: m == 0 and not COPYALL");
d410 1
a410 1
		if (n == 0)
d421 5
a425 16
		if (m->m_flags & M_EXT) {
			if (!deep) {
				n->m_data = m->m_data + off;
				n->m_ext = m->m_ext;
				MCLADDREFERENCE(m, n);
			} else {
				/*
				 * we are unsure about the way m was allocated.
				 * copy into multiple MCLBYTES cluster mbufs.
				 */
				MCLGET(n, wait);
				n->m_len = 0;
				n->m_len = M_TRAILINGSPACE(n);
				n->m_len = min(n->m_len, len);
				n->m_len = min(n->m_len, m->m_len - off);
				memcpy(mtod(n, caddr_t), mtod(m, caddr_t) + off,
d427 2
a428 1
			}
d430 1
a430 1
			memcpy(mtod(n, caddr_t), mtod(m, caddr_t)+off,
d434 2
a435 9
		off += n->m_len;
#ifdef DIAGNOSTIC
		if (off > m->m_len)
			panic("m_copym0 overrun");
#endif
		if (off == m->m_len) {
			m = m->m_next;
			off = 0;
		}
d438 1
a438 1
	if (top == 0)
d444 1
a444 1
	return (0);
d622 5
a626 2
		if (n->m_flags & M_PKTHDR)
			M_MOVE_PKTHDR(m, n);
a670 1

d673 1
d692 2
a693 2
			/* Too many adverse side effects. */
			/* M_MOVE_PKTHDR(m, n); */
d695 2
a696 1
			M_MOVE_HDR(m, n);
d878 6
a883 1
		MCLADDREFERENCE(m, n);
d976 2
a977 2
			memset((void *)(m + sizeof(struct m_hdr) +
			    sizeof(struct pkthdr)), 0, MHLEN);
d979 1
a979 1
			memset((void *)(m + sizeof(struct m_hdr)), 0, MLEN);
d982 2
a983 2
		    !MCLISREFERENCED(m))
			memset(m->m_ext.ext_buf, 0, m->m_ext.ext_size);
a1031 28

#ifdef SMALL_KERNEL
/*
 * The idea of adding code in a small kernel might look absurd, but this is
 * instead of macros.
 */
struct mbuf *
_sk_mget(int how, int type)
{
	struct mbuf *m;
	_MGET(m, how, type);
	return m;
}

struct mbuf *
_sk_mgethdr(int how, int type)
{
	struct mbuf *m;
	_MGETHDR(m, how, type);
	return m;
}

void
_sk_mclget(struct mbuf *m, int how)
{
	_MCLGET(m, how);
}
#endif /* SMALL_KERNEL */
@


1.19.2.4
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.19.2.3 2001/07/04 10:48:43 niklas Exp $	*/
d74 1
@


1.19.2.5
log
@merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d65 2
@


1.19.2.6
log
@Merge in -current
@
text
@d71 1
a71 1
extern	struct vm_map *mb_map;
@


1.19.2.7
log
@Merge in trunk
@
text
@d40 10
a49 38
 *	@@(#)COPYRIGHT	1.1 (NRL) 17 January 1995
 * 
 * NRL grants permission for redistribution and use in source and binary
 * forms, with or without modification, of the software and documentation
 * created at NRL provided that the following conditions are met:
 * 
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgements:
 * 	This product includes software developed by the University of
 * 	California, Berkeley and its contributors.
 * 	This product includes software developed at the Information
 * 	Technology Division, US Naval Research Laboratory.
 * 4. Neither the name of the NRL nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 * 
 * THE SOFTWARE PROVIDED BY NRL IS PROVIDED BY NRL AND CONTRIBUTORS ``AS
 * IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A
 * PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL NRL OR
 * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
 * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 * 
 * The views and conclusions contained in the software and documentation
 * are those of the authors and should not be interpreted as representing
 * official policies, either expressed or implied, of the US Naval
 * Research Laboratory (NRL).
 */
d55 1
d71 2
a72 1
struct vm_map *mb_map;
d74 2
a75 2
void	*mclpool_alloc __P((struct pool *, int));
void	mclpool_release __P((struct pool *, void *));
a80 4
struct pool_allocator mclpool_allocator = {
	mclpool_alloc, mclpool_release, 0,
};

d87 3
a89 10
	vaddr_t minaddr, maxaddr;

	mb_map = uvm_km_suballoc(kernel_map, &minaddr, &maxaddr,
	    nmbclust*(MCLBYTES), VM_MAP_INTRSAFE, FALSE, NULL);

	pool_init(&mbpool, MSIZE, 0, 0, 0, "mbpl", NULL);
	pool_init(&mclpool, MCLBYTES, 0, 0, 0, "mclpl", &mclpool_allocator);

	pool_set_drain_hook(&mbpool, m_reclaim, NULL);
	pool_set_drain_hook(&mclpool, m_reclaim, NULL);
d96 1
a96 1
	(void)pool_sethardlimit(&mclpool, nmbclust, mclpool_warnmsg, 60);
d110 4
a113 1
mclpool_alloc(struct pool *pp, int flags)
d122 4
a125 1
mclpool_release(struct pool *pp, void *v)
d130 51
d182 1
a182 1
m_reclaim(void *arg, int flags)
d188 1
d925 8
a932 5
#ifdef DIAGNOSTIC
		if (M_READONLY(m))
			panic("m_zero: M_READONLY");
#endif /* DIAGNOSTIC */
		if (m->m_flags & M_EXT)
a933 6
		else {
			if (m->m_flags & M_PKTHDR)
				memset(m->m_pktdat, 0, MHLEN);
			else
				memset(m->m_dat, 0, MLEN);
		}
@


1.19.2.8
log
@Merge in -current from about a week ago
@
text
@d100 3
a102 3
void	*mclpool_alloc(struct pool *, int);
void	mclpool_release(struct pool *, void *);
struct mbuf *m_copym0(struct mbuf *, int, int, int, int);
d832 1
a832 1
	void (*copy)(const void *, void *, size_t);
@


1.19.2.9
log
@Sync the SMP branch with 3.3
@
text
@a94 1
struct	mbstat mbstat;		/* mbuf stats */
a98 5

int max_linkhdr;		/* largest link-level header */
int max_protohdr;		/* largest protocol header */
int max_hdr;			/* largest link+protocol header */
int max_datalen;		/* MHLEN - max_hdr */
@


1.19.2.10
log
@Sync the SMP branch to -current. This includes moving to ELF.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.19.2.9 2003/03/28 00:41:27 niklas Exp $	*/
a425 54
}

/*
 * Copy data from a buffer back into the indicated mbuf chain,
 * starting "off" bytes from the beginning, extending the mbuf
 * chain if necessary. The mbuf needs to be properly initialized
 * including the setting of m_len.
 */
void
m_copyback(m0, off, len, cp)
	struct	mbuf *m0;
	register int off;
	register int len;
	caddr_t cp;
{
	register int mlen;
	register struct mbuf *m = m0, *n;
	int totlen = 0;

	if (m0 == 0)
		return;
	while (off > (mlen = m->m_len)) {
		off -= mlen;
		totlen += mlen;
		if (m->m_next == 0) {
			n = m_getclr(M_DONTWAIT, m->m_type);
			if (n == 0)
				goto out;
			n->m_len = min(MLEN, len + off);
			m->m_next = n;
		}
		m = m->m_next;
	}
	while (len > 0) {
		mlen = min (m->m_len - off, len);
		bcopy(cp, off + mtod(m, caddr_t), (unsigned)mlen);
		cp += mlen;
		len -= mlen;
		mlen += off;
		off = 0;
		totlen += mlen;
		if (len == 0)
			break;
		if (m->m_next == 0) {
			n = m_get(M_DONTWAIT, m->m_type);
			if (n == 0)
				break;
			n->m_len = min(MLEN, len);
			m->m_next = n;
		}
		m = m->m_next;
	}
out:	if (((m = m0)->m_flags & M_PKTHDR) && (m->m_pkthdr.len < totlen))
		m->m_pkthdr.len = totlen;
@


1.19.2.11
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.19.2.10 2003/05/13 19:21:28 ho Exp $	*/
d16 5
a20 1
 * 3. Neither the name of the University nor the names of its contributors
a124 1
	minaddr = vm_map_min(kernel_map);
@


1.19.2.12
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d436 1
a436 1
	const void *cp;
d868 1
a868 1
		n->m_ext = m->m_ext;
@


1.19.2.13
log
@Merge with the trunk
@
text
@d102 2
a104 1
void	nmbclust_update(void);
d106 2
d109 3
a111 2
const char *mclpool_warnmsg =
    "WARNING: mclpool limit reached; increase kern.maxclusters";
d119 6
d126 1
a126 1
	pool_init(&mclpool, MCLBYTES, 0, 0, 0, "mclpl", NULL);
d131 6
a136 1
	nmbclust_update();
d148 10
d159 1
a159 1
nmbclust_update(void)
d161 1
a161 6
	/*
	 * Set the hard limit on the mclpool to the number of
	 * mbuf clusters the kernel is to support.  Log the limit
	 * reached message max once a minute.
	 */
	(void)pool_sethardlimit(&mclpool, nmbclust, mclpool_warnmsg, 60);
d317 1
a317 1
		if (m == NULL)
d325 1
a325 1
	top = NULL;
d327 1
a327 1
		if (m == NULL) {
d329 1
a329 1
				panic("m_copym0: m == NULL and not COPYALL");
d334 1
a334 1
		if (n == NULL)
d379 1
a379 1
	if (top == NULL)
d385 1
a385 1
	return (NULL);
d442 1
a442 1
	if (m0 == NULL)
d447 1
a447 1
		if (m->m_next == NULL) {
d449 1
a449 1
			if (n == NULL)
d466 1
a466 1
		if (m->m_next == NULL) {
d468 1
a468 1
			if (n == NULL)
d686 1
a686 2
			m->m_flags = (n->m_flags & M_COPYFLAGS) | 
			    M_EXT | M_CLUSTER;
d744 2
a745 1
	    		} else
@


1.18
log
@Add an m_inject()
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.17 1999/10/01 02:00:12 jason Exp $	*/
d806 2
a807 2
		n->m_pkthdr.rcvif = m0->m_pkthdr.rcvif;
		n->m_pkthdr.len = m0->m_pkthdr.len - len0;
@


1.17
log
@remove dependency on external storage managed by mclusters and
add more support for external managers
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.16 1999/09/12 11:46:53 niklas Exp $	*/
d721 61
@


1.16
log
@style(9)
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.15 1999/08/17 12:31:22 millert Exp $	*/
d345 4
a348 1
			mclrefcnt[mtocl(m->m_ext.ext_buf)]++;
d777 4
a780 1
		mclrefcnt[mtocl(m->m_ext.ext_buf)]++;
d877 3
a879 2
		if ((m->m_flags & M_EXT) && 
			!mclrefcnt[mtocl((m)->m_ext.ext_buf)])
@


1.15
log
@There was a line missing in the commit from rev 1.6 that resulted in
an uninitialized variable; art@@stacken.kth.se
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.14 1999/08/09 21:41:51 deraadt Exp $	*/
d82 1
a82 1
	if (m_clalloc(max(4096/CLBYTES, 1), M_DONTWAIT) == 0)
d110 1
a110 1
			      nowait ? 0 : UVM_KMF_NOWAIT);
d149 1
a149 1
		setsoftnet ();
d153 1
a153 1
#define m_retry(i, t)	(struct mbuf *)0
d170 1
a170 1
		setsoftnet ();
d174 1
a174 1
#define m_retryhdr(i, t) (struct mbuf *)0
d228 2
a229 2
	if (m == 0)
		return (0);
d274 1
a274 1
	if (mn == (struct mbuf *)NULL) {
d276 1
a276 1
		return ((struct mbuf *)NULL);
d315 1
a315 1
		if (m == 0)
d323 1
a323 1
	top = 0;
d325 1
a325 1
		if (m == 0) {
d332 1
a332 1
		if (n == 0)
d357 1
a357 1
	if (top == 0)
d363 1
a363 1
	return (0);
d388 1
a388 1
		if (m == 0)
d396 1
a396 1
	top = 0;
d398 1
a398 1
		if (m == 0) {
d405 1
a405 1
		if (n == 0)
d416 1
a416 1
		if ((m->m_flags & M_EXT) && (n->m_len >MHLEN)) {
d418 1
a418 1
			MCLGET(n,wait);
d420 1
a420 1
				bcopy(mtod(m,caddr_t)+off,mtod(n,caddr_t),
d425 1
a425 1
			bcopy(mtod(m, caddr_t)+off, mtod(n, caddr_t),
d433 1
a433 1
	if (top == 0)
d439 1
a439 1
	return (0);
d460 1
a460 1
		if (m == 0)
d468 1
a468 1
		if (m == 0)
d546 1
a546 1
			if (m->m_next == (struct mbuf *)0)
d614 1
a614 1
		if (m == 0)
d626 1
a626 1
		  (unsigned)count);
d637 1
a637 1
		(void) m_free(m);
d645 1
a645 1
	return (0);
d679 1
a679 1
		if (m == 0)
d681 1
a681 1
		MCLGET(m,M_DONTWAIT);
d708 1
a708 1
		(void) m_free(m);
d717 1
a717 1
	return (0);
d735 2
a736 2
	if (m == 0)
		return (0);
d740 2
a741 2
		if (n == 0)
			return (0);
d752 1
a752 1
			if (n->m_next == 0) {
d755 1
a755 1
				return (0);
d762 1
a762 1
		m->m_next = 0;
d766 2
a767 2
		if (n == 0)
			return (0);
d783 1
a783 1
	m->m_next = 0;
d797 1
a797 1
	struct mbuf *top = 0, **mp = &top;
d813 2
a814 2
	if (m == 0)
		return (0);
d820 1
a820 1
		if (top) {
d822 1
a822 1
			if (m == 0) {
d824 1
a824 1
				return (0);
d840 2
a841 1
				if (top == 0 && len + max_linkhdr <= m->m_len)
d899 1
a899 1
		if (m == 0)
d907 1
a907 1
		if (m == 0)
@


1.14
log
@make panic messages unique
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.13 1999/07/02 01:02:53 cmetz Exp $	*/
d744 1
@


1.13
log
@Added a simple but potentially very useful new mbuf function, m_apply().
It applies the supplied function f(state, p, len) to every contiguous region
in a mbuf, thus mostly handling all the mbuf-isms for you.

It's used by my TCP MD5 signature implementation to run MD5 over the TCP
payload data in a mbuf so that I don't have to spread mbufism-loops all over.
It might also be useful for IPsec.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.12 1999/05/14 02:12:29 cmetz Exp $	*/
d308 4
a311 2
	if (off < 0 || len < 0)
		panic("m_copym");
d316 1
a316 1
			panic("m_copym");
d327 1
a327 1
				panic("m_copym");
d381 4
a384 2
	if (off < 0 || len < 0)
		panic("m_copym");
d389 1
a389 1
			panic("m_copym2 (null mbuf)");
d400 1
a400 1
				panic("m_copym2 (len != M_COPYALL)");
d455 4
a458 2
	if (off < 0 || len < 0)
		panic("m_copydata");
d461 1
a461 1
			panic("m_copydata");
d469 1
a469 1
			panic("m_copydata");
d892 4
a895 2
	if (off < 0 || len < 0)
		panic("m_apply");
d898 1
a898 1
			panic("m_apply");
d906 1
a906 1
			panic("m_apply");
@


1.12
log
@OpenBSD has no ext_func, so that test is bogus.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.11 1999/05/14 02:05:42 cmetz Exp $	*/
d869 44
@


1.11
log
@m_zero will no longer zero the contents of a cluster if there's an alias to it.
(TCP uses cluster mbuf aliases for retransmission, and this would cause your
data to get retransmitted zeroed-out)
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.10 1999/05/12 21:11:40 ho Exp $	*/
d863 1
a863 1
		if ((m->m_flags & M_EXT) && !m->m_ext.ext_func &&
@


1.10
log
@Fix problem with data corruption for retransmitted TCP packets
in an IPSec ESP tunnel. OpenBSD PR 819.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.9 1999/02/26 04:49:07 art Exp $	*/
d863 2
a864 1
		if (m->m_flags & M_EXT)
@


1.9
log
@kmem allocation changes for uvm
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.8 1999/01/07 22:28:01 deraadt Exp $	*/
a366 2
 *
 * The hope is to obsolete this function someday.
@


1.8
log
@new APIs: m_pullup2, m_copym2, m_zero, MCL_ALIGN; NRL/cmetz
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.7 1998/02/03 19:06:27 deraadt Exp $	*/
d67 4
d108 4
d113 1
@


1.7
log
@bad types; wileyc@@sekiya.twics.co.jp
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.6 1997/08/20 05:42:26 millert Exp $	*/
d39 12
d356 76
d636 72
d844 16
@


1.6
log
@In m_split(), restore m_pkthdr.len to original length if we
get an error.  From Koji Imada <koji@@math.human.nagoya-u.ac.jp>
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.5 1996/09/06 07:21:41 niklas Exp $	*/
d411 1
a411 1
	register count;
@


1.5
log
@Need cpu.h included to get at setsoftnet define
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.4 1996/09/02 18:14:15 dm Exp $	*/
d558 1
a558 1
	unsigned len = len0, remain;
d580 1
@


1.4
log
@Don't drain the protocol queues at interrupt level.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.3 1996/06/20 10:50:22 deraadt Exp $	*/
d50 2
@


1.3
log
@if kmem_malloc() fails to allocate a cluster, call m_reclaim(). log
"mb_map full" message only once a minute.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_mbuf.c,v 1.2 1996/03/03 17:20:17 niklas Exp $	*/
d56 1
d124 5
d145 5
d164 1
@


1.2
log
@From NetBSD: 960217 merge
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uipc_mbuf.c,v 1.15 1996/02/09 19:00:45 christos Exp $	*/
d82 2
a83 1
	static int logged;
d86 1
a86 1
	int npg;
d91 6
a96 2
		if (logged == 0) {
			logged++;
d99 2
a100 1
		return (0);
@


1.1
log
@Initial revision
@
text
@d1 2
a2 1
/*	$NetBSD: uipc_mbuf.c,v 1.13 1994/10/30 21:48:06 cgd Exp $	*/
d77 1
d140 1
d213 1
a213 1
	} while (m = n);
d327 1
d363 1
d453 1
a453 1
		while (m = m->m_next)
d599 1
a599 1
	void (*copy)();
d652 1
a652 1
			copy(cp, mtod(m, caddr_t), (unsigned)len);
d654 1
a654 1
			bcopy(cp, mtod(m, caddr_t), (unsigned)len);
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@
