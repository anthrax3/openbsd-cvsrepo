head	1.50;
access;
symbols
	OPENBSD_6_2:1.50.0.6
	OPENBSD_6_2_BASE:1.50
	OPENBSD_6_1:1.50.0.4
	OPENBSD_6_1_BASE:1.50
	OPENBSD_6_0:1.48.0.2
	OPENBSD_6_0_BASE:1.48
	OPENBSD_5_9:1.43.0.2
	OPENBSD_5_9_BASE:1.43
	OPENBSD_5_8:1.43.0.4
	OPENBSD_5_8_BASE:1.43
	OPENBSD_5_7:1.41.0.2
	OPENBSD_5_7_BASE:1.41
	OPENBSD_5_6:1.41.0.6
	OPENBSD_5_6_BASE:1.41
	OPENBSD_5_5:1.41.0.4
	OPENBSD_5_5_BASE:1.41
	OPENBSD_5_4:1.35.0.6
	OPENBSD_5_4_BASE:1.35
	OPENBSD_5_3:1.35.0.4
	OPENBSD_5_3_BASE:1.35
	OPENBSD_5_2:1.35.0.2
	OPENBSD_5_2_BASE:1.35
	OPENBSD_5_1_BASE:1.33
	OPENBSD_5_1:1.33.0.4
	OPENBSD_5_0:1.33.0.2
	OPENBSD_5_0_BASE:1.33
	OPENBSD_4_9:1.32.0.6
	OPENBSD_4_9_BASE:1.32
	OPENBSD_4_8:1.32.0.4
	OPENBSD_4_8_BASE:1.32
	OPENBSD_4_7:1.32.0.2
	OPENBSD_4_7_BASE:1.32
	OPENBSD_4_6:1.31.0.4
	OPENBSD_4_6_BASE:1.31
	OPENBSD_4_5:1.29.0.2
	OPENBSD_4_5_BASE:1.29
	OPENBSD_4_4:1.28.0.2
	OPENBSD_4_4_BASE:1.28
	OPENBSD_4_3:1.26.0.2
	OPENBSD_4_3_BASE:1.26
	OPENBSD_4_2:1.25.0.6
	OPENBSD_4_2_BASE:1.25
	OPENBSD_4_1:1.25.0.4
	OPENBSD_4_1_BASE:1.25
	OPENBSD_4_0:1.25.0.2
	OPENBSD_4_0_BASE:1.25
	OPENBSD_3_9:1.22.0.6
	OPENBSD_3_9_BASE:1.22
	OPENBSD_3_8:1.22.0.4
	OPENBSD_3_8_BASE:1.22
	OPENBSD_3_7:1.22.0.2
	OPENBSD_3_7_BASE:1.22
	OPENBSD_3_6:1.19.0.2
	OPENBSD_3_6_BASE:1.19
	SMP_SYNC_A:1.18
	SMP_SYNC_B:1.18
	OPENBSD_3_5:1.18.0.4
	OPENBSD_3_5_BASE:1.18
	OPENBSD_3_4:1.18.0.2
	OPENBSD_3_4_BASE:1.18
	UBC_SYNC_A:1.16
	OPENBSD_3_3:1.15.0.2
	OPENBSD_3_3_BASE:1.15
	OPENBSD_3_2:1.14.0.4
	OPENBSD_3_2_BASE:1.14
	OPENBSD_3_1:1.14.0.2
	OPENBSD_3_1_BASE:1.14
	UBC_SYNC_B:1.14
	UBC:1.11.0.4
	UBC_BASE:1.11
	OPENBSD_3_0:1.11.0.2
	OPENBSD_3_0_BASE:1.11
	OPENBSD_2_9_BASE:1.8
	OPENBSD_2_9:1.8.0.2
	OPENBSD_2_8:1.5.0.2
	OPENBSD_2_8_BASE:1.5
	OPENBSD_2_7:1.4.0.2
	OPENBSD_2_7_BASE:1.4
	SMP:1.3.0.2;
locks; strict;
comment	@ * @;


1.50
date	2016.10.03.11.54.29;	author dlg;	state Exp;
branches;
next	1.49;
commitid	dpu7wDzN3voo07VU;

1.49
date	2016.09.22.12.55.24;	author mpi;	state Exp;
branches;
next	1.48;
commitid	yHpOPoIDV9B9PJbt;

1.48
date	2016.07.06.15.53.01;	author tedu;	state Exp;
branches;
next	1.47;
commitid	dZWS1TCbA5Wj2uVG;

1.47
date	2016.06.23.18.41.44;	author stefan;	state Exp;
branches;
next	1.46;
commitid	npAPpPAxzoY1yiRp;

1.46
date	2016.06.14.15.58.03;	author bluhm;	state Exp;
branches;
next	1.45;
commitid	CQEa6xVpTizjgoPL;

1.45
date	2016.03.20.07.56.08;	author uebayasi;	state Exp;
branches;
next	1.44;
commitid	TKHLMSkOKIynBe4v;

1.44
date	2016.03.17.10.53.57;	author uebayasi;	state Exp;
branches;
next	1.43;
commitid	ylnKY5ecsqg65vot;

1.43
date	2015.07.20.23.47.20;	author uebayasi;	state Exp;
branches
	1.43.2.1
	1.43.4.1;
next	1.42;
commitid	CJFzTqJwP65tMk7k;

1.42
date	2015.03.14.03.38.50;	author jsg;	state Exp;
branches;
next	1.41;
commitid	p4LJxGKbi0BU2cG6;

1.41
date	2013.11.27.04.28.32;	author dlg;	state Exp;
branches;
next	1.40;

1.40
date	2013.10.06.04.34.35;	author guenther;	state Exp;
branches;
next	1.39;

1.39
date	2013.10.02.21.03.21;	author sf;	state Exp;
branches;
next	1.38;

1.38
date	2013.10.01.19.36.25;	author sf;	state Exp;
branches;
next	1.37;

1.37
date	2013.09.17.04.52.53;	author guenther;	state Exp;
branches;
next	1.36;

1.36
date	2013.08.03.06.47.15;	author guenther;	state Exp;
branches;
next	1.35;

1.35
date	2012.06.02.00.11.16;	author guenther;	state Exp;
branches;
next	1.34;

1.34
date	2012.05.24.07.17.42;	author guenther;	state Exp;
branches;
next	1.33;

1.33
date	2011.05.10.00.58.42;	author dlg;	state Exp;
branches;
next	1.32;

1.32
date	2009.11.04.19.14.10;	author kettenis;	state Exp;
branches;
next	1.31;

1.31
date	2009.06.02.22.05.54;	author guenther;	state Exp;
branches;
next	1.30;

1.30
date	2009.03.03.19.09.13;	author miod;	state Exp;
branches;
next	1.29;

1.29
date	2008.10.22.08.38.06;	author blambert;	state Exp;
branches;
next	1.28;

1.28
date	2008.07.14.15.17.08;	author art;	state Exp;
branches;
next	1.27;

1.27
date	2008.07.11.14.18.39;	author blambert;	state Exp;
branches;
next	1.26;

1.26
date	2008.01.20.18.23.38;	author miod;	state Exp;
branches;
next	1.25;

1.25
date	2006.07.19.20.25.08;	author miod;	state Exp;
branches;
next	1.24;

1.24
date	2006.04.21.01.35.27;	author dlg;	state Exp;
branches;
next	1.23;

1.23
date	2006.04.18.21.48.34;	author dlg;	state Exp;
branches;
next	1.22;

1.22
date	2004.12.28.22.48.30;	author deraadt;	state Exp;
branches;
next	1.21;

1.21
date	2004.12.12.20.37.01;	author espie;	state Exp;
branches;
next	1.20;

1.20
date	2004.11.10.11.00.00;	author grange;	state Exp;
branches;
next	1.19;

1.19
date	2004.07.20.20.20.52;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2003.06.03.12.05.25;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2003.05.17.14.02.06;	author grange;	state Exp;
branches;
next	1.16;

1.16
date	2003.04.13.21.04.52;	author tedu;	state Exp;
branches;
next	1.15;

1.15
date	2002.12.08.04.21.07;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2002.03.14.01.27.04;	author millert;	state Exp;
branches;
next	1.13;

1.13
date	2002.02.15.18.10.15;	author nordin;	state Exp;
branches;
next	1.12;

1.12
date	2001.12.22.16.41.51;	author nordin;	state Exp;
branches;
next	1.11;

1.11
date	2001.09.12.15.48.45;	author art;	state Exp;
branches
	1.11.4.1;
next	1.10;

1.10
date	2001.08.23.11.20.05;	author art;	state Exp;
branches;
next	1.9;

1.9
date	2001.08.23.08.18.57;	author miod;	state Exp;
branches;
next	1.8;

1.8
date	2001.03.28.07.33.51;	author art;	state Exp;
branches;
next	1.7;

1.7
date	2001.03.15.16.47.50;	author csapuntz;	state Exp;
branches;
next	1.6;

1.6
date	2001.02.16.13.47.40;	author espie;	state Exp;
branches;
next	1.5;

1.5
date	2000.06.20.02.45.55;	author art;	state Exp;
branches;
next	1.4;

1.4
date	2000.05.08.01.28.59;	author mickey;	state Exp;
branches;
next	1.3;

1.3
date	2000.03.23.11.07.34;	author art;	state Exp;
branches
	1.3.2.1;
next	1.2;

1.2
date	2000.03.23.10.27.05;	author art;	state Exp;
branches;
next	1.1;

1.1
date	2000.03.23.09.59.57;	author art;	state Exp;
branches;
next	;

1.3.2.1
date	2000.03.24.09.09.25;	author niklas;	state Exp;
branches;
next	1.3.2.2;

1.3.2.2
date	2001.05.14.22.32.42;	author niklas;	state Exp;
branches;
next	1.3.2.3;

1.3.2.3
date	2001.10.31.03.26.29;	author nate;	state Exp;
branches;
next	1.3.2.4;

1.3.2.4
date	2002.03.06.02.13.23;	author niklas;	state Exp;
branches;
next	1.3.2.5;

1.3.2.5
date	2002.03.28.11.43.04;	author niklas;	state Exp;
branches;
next	1.3.2.6;

1.3.2.6
date	2003.03.28.00.41.26;	author niklas;	state Exp;
branches;
next	1.3.2.7;

1.3.2.7
date	2003.05.13.19.21.28;	author ho;	state Exp;
branches;
next	1.3.2.8;

1.3.2.8
date	2003.06.07.11.03.40;	author ho;	state Exp;
branches;
next	;

1.11.4.1
date	2002.01.31.22.55.40;	author niklas;	state Exp;
branches;
next	1.11.4.2;

1.11.4.2
date	2002.06.11.03.29.40;	author art;	state Exp;
branches;
next	1.11.4.3;

1.11.4.3
date	2003.05.19.22.31.10;	author tedu;	state Exp;
branches;
next	;

1.43.2.1
date	2016.07.14.02.45.23;	author tedu;	state Exp;
branches;
next	;
commitid	iy5C78vcnVfjlmst;

1.43.4.1
date	2016.07.14.02.46.11;	author tedu;	state Exp;
branches;
next	;
commitid	kzYtCgZfgZewqezu;


desc
@@


1.50
log
@avoid holding timeout_mutex while interacting with the scheduler.

as noted by haesbaert, this is necessary to avoid deadlocks because
the scheduler can call back into the timeout subsystem while its
holding its own locks.

this happened in two places. firstly, in softclock() it would take
timeout_mutex to find pending work. if that pending work needs a
process context, it would queue the work for the thread and call
wakeup, which enters the scheduler locks. if another cpu is trying
to tsleep (or msleep) with a timeout specified, the sleep code would
be holding the sched lock and call timeout_add, which takes
timeout_mutex.

this is solved by deferring the wakeup to after timeout_mutex is
left. this also has the benefit of mitigating the number of wakeups
done per softclock tick.

secondly, the timeout worker thread takes timeout_mutex and calls
msleep when there's no work to do (ie, the queue is empty). msleep
will take the sched locks. again, if another cpu does a tsleep
with a timeout, you get a deadlock.

to solve this im using sleep_setup and sleep_finish to sleep on an
empty queue, which is safe to do outside the lock as it is comparisons
of the queue head pointers, not derefs of the contents of the queue.

as long as the sleeps and wakeups are ordered correctly with the
enqueue and dequeue operations under the mutex, this all works.
you can think of the queue as a single descriptor ring, and the
wakeup as an interrupt.

the second deadlock was identified by guenther@@
ok tedu@@ mpi@@
@
text
@/*	$OpenBSD: kern_timeout.c,v 1.49 2016/09/22 12:55:24 mpi Exp $	*/
/*
 * Copyright (c) 2001 Thomas Nordin <nordin@@openbsd.org>
 * Copyright (c) 2000-2001 Artur Grabowski <art@@openbsd.org>
 * All rights reserved. 
 *
 * Redistribution and use in source and binary forms, with or without 
 * modification, are permitted provided that the following conditions 
 * are met: 
 *
 * 1. Redistributions of source code must retain the above copyright 
 *    notice, this list of conditions and the following disclaimer. 
 * 2. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission. 
 *
 * THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES,
 * INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY
 * AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL
 * THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 * EXEMPLARY, OR CONSEQUENTIAL  DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
 * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
 * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
 * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
 * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. 
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/kthread.h>
#include <sys/timeout.h>
#include <sys/mutex.h>
#include <sys/kernel.h>
#include <sys/queue.h>			/* _Q_INVALIDATE */

#ifdef DDB
#include <machine/db_machdep.h>
#include <ddb/db_interface.h>
#include <ddb/db_sym.h>
#include <ddb/db_output.h>
#endif

/*
 * Timeouts are kept in a hierarchical timing wheel. The to_time is the value
 * of the global variable "ticks" when the timeout should be called. There are
 * four levels with 256 buckets each. See 'Scheme 7' in
 * "Hashed and Hierarchical Timing Wheels: Efficient Data Structures for
 * Implementing a Timer Facility" by George Varghese and Tony Lauck.
 */
#define BUCKETS 1024
#define WHEELSIZE 256
#define WHEELMASK 255
#define WHEELBITS 8

struct circq timeout_wheel[BUCKETS];	/* Queues of timeouts */
struct circq timeout_todo;		/* Worklist */
struct circq timeout_proc;		/* Due timeouts needing proc. context */

#define MASKWHEEL(wheel, time) (((time) >> ((wheel)*WHEELBITS)) & WHEELMASK)

#define BUCKET(rel, abs)						\
    (timeout_wheel[							\
	((rel) <= (1 << (2*WHEELBITS)))					\
	    ? ((rel) <= (1 << WHEELBITS))				\
		? MASKWHEEL(0, (abs))					\
		: MASKWHEEL(1, (abs)) + WHEELSIZE			\
	    : ((rel) <= (1 << (3*WHEELBITS)))				\
		? MASKWHEEL(2, (abs)) + 2*WHEELSIZE			\
		: MASKWHEEL(3, (abs)) + 3*WHEELSIZE])

#define MOVEBUCKET(wheel, time)						\
    CIRCQ_APPEND(&timeout_todo,						\
        &timeout_wheel[MASKWHEEL((wheel), (time)) + (wheel)*WHEELSIZE])

/*
 * The first thing in a struct timeout is its struct circq, so we
 * can get back from a pointer to the latter to a pointer to the
 * whole timeout with just a cast.
 */
static __inline struct timeout *
timeout_from_circq(struct circq *p)
{
	return ((struct timeout *)(p));
}

/*
 * All wheels are locked with the same mutex.
 *
 * We need locking since the timeouts are manipulated from hardclock that's
 * not behind the big lock.
 */
struct mutex timeout_mutex = MUTEX_INITIALIZER(IPL_HIGH);

/*
 * Circular queue definitions.
 */

#define CIRCQ_INIT(elem) do {                   \
        (elem)->next = (elem);                  \
        (elem)->prev = (elem);                  \
} while (0)

#define CIRCQ_INSERT(elem, list) do {           \
        (elem)->prev = (list)->prev;            \
        (elem)->next = (list);                  \
        (list)->prev->next = (elem);            \
        (list)->prev = (elem);                  \
} while (0)

#define CIRCQ_APPEND(fst, snd) do {             \
        if (!CIRCQ_EMPTY(snd)) {                \
                (fst)->prev->next = (snd)->next;\
                (snd)->next->prev = (fst)->prev;\
                (snd)->prev->next = (fst);      \
                (fst)->prev = (snd)->prev;      \
                CIRCQ_INIT(snd);                \
        }                                       \
} while (0)

#define CIRCQ_REMOVE(elem) do {                 \
        (elem)->next->prev = (elem)->prev;      \
        (elem)->prev->next = (elem)->next;      \
	_Q_INVALIDATE((elem)->prev);		\
	_Q_INVALIDATE((elem)->next);		\
} while (0)

#define CIRCQ_FIRST(elem) ((elem)->next)

#define CIRCQ_EMPTY(elem) (CIRCQ_FIRST(elem) == (elem))

void softclock_thread(void *);
void softclock_create_thread(void *);

/*
 * Some of the "math" in here is a bit tricky.
 *
 * We have to beware of wrapping ints.
 * We use the fact that any element added to the queue must be added with a
 * positive time. That means that any element `to' on the queue cannot be
 * scheduled to timeout further in time than INT_MAX, but to->to_time can
 * be positive or negative so comparing it with anything is dangerous.
 * The only way we can use the to->to_time value in any predictable way
 * is when we calculate how far in the future `to' will timeout -
 * "to->to_time - ticks". The result will always be positive for future
 * timeouts and 0 or negative for due timeouts.
 */

void
timeout_startup(void)
{
	int b;

	CIRCQ_INIT(&timeout_todo);
	CIRCQ_INIT(&timeout_proc);
	for (b = 0; b < nitems(timeout_wheel); b++)
		CIRCQ_INIT(&timeout_wheel[b]);
}

void
timeout_proc_init(void)
{
	kthread_create_deferred(softclock_create_thread, NULL);
}

void
timeout_set(struct timeout *new, void (*fn)(void *), void *arg)
{
	new->to_func = fn;
	new->to_arg = arg;
	new->to_flags = TIMEOUT_INITIALIZED;
}

void
timeout_set_proc(struct timeout *new, void (*fn)(void *), void *arg)
{
	timeout_set(new, fn, arg);
	new->to_flags |= TIMEOUT_NEEDPROCCTX;
}

int
timeout_add(struct timeout *new, int to_ticks)
{
	int old_time;
	int ret = 1;

#ifdef DIAGNOSTIC
	if (!(new->to_flags & TIMEOUT_INITIALIZED))
		panic("timeout_add: not initialized");
	if (to_ticks < 0)
		panic("timeout_add: to_ticks (%d) < 0", to_ticks);
#endif

	mtx_enter(&timeout_mutex);
	/* Initialize the time here, it won't change. */
	old_time = new->to_time;
	new->to_time = to_ticks + ticks;
	new->to_flags &= ~TIMEOUT_TRIGGERED;

	/*
	 * If this timeout already is scheduled and now is moved
	 * earlier, reschedule it now. Otherwise leave it in place
	 * and let it be rescheduled later.
	 */
	if (new->to_flags & TIMEOUT_ONQUEUE) {
		if (new->to_time - ticks < old_time - ticks) {
			CIRCQ_REMOVE(&new->to_list);
			CIRCQ_INSERT(&new->to_list, &timeout_todo);
		}
		ret = 0;
	} else {
		new->to_flags |= TIMEOUT_ONQUEUE;
		CIRCQ_INSERT(&new->to_list, &timeout_todo);
	}
	mtx_leave(&timeout_mutex);

	return (ret);
}

int
timeout_add_tv(struct timeout *to, const struct timeval *tv)
{
	uint64_t to_ticks;

	to_ticks = (uint64_t)hz * tv->tv_sec + tv->tv_usec / tick;
	if (to_ticks > INT_MAX)
		to_ticks = INT_MAX;
	if (to_ticks == 0 && tv->tv_usec > 0)
		to_ticks = 1;

	return (timeout_add(to, (int)to_ticks));
}

int
timeout_add_ts(struct timeout *to, const struct timespec *ts)
{
	uint64_t to_ticks;

	to_ticks = (uint64_t)hz * ts->tv_sec + ts->tv_nsec / (tick * 1000);
	if (to_ticks > INT_MAX)
		to_ticks = INT_MAX;
	if (to_ticks == 0 && ts->tv_nsec > 0)
		to_ticks = 1;

	return (timeout_add(to, (int)to_ticks));
}

int
timeout_add_bt(struct timeout *to, const struct bintime *bt)
{
	uint64_t to_ticks;

	to_ticks = (uint64_t)hz * bt->sec + (long)(((uint64_t)1000000 *
	    (uint32_t)(bt->frac >> 32)) >> 32) / tick;
	if (to_ticks > INT_MAX)
		to_ticks = INT_MAX;
	if (to_ticks == 0 && bt->frac > 0)
		to_ticks = 1;

	return (timeout_add(to, (int)to_ticks));
}

int
timeout_add_sec(struct timeout *to, int secs)
{
	uint64_t to_ticks;

	to_ticks = (uint64_t)hz * secs;
	if (to_ticks > INT_MAX)
		to_ticks = INT_MAX;

	return (timeout_add(to, (int)to_ticks));
}

int
timeout_add_msec(struct timeout *to, int msecs)
{
	uint64_t to_ticks;

	to_ticks = (uint64_t)msecs * 1000 / tick;
	if (to_ticks > INT_MAX)
		to_ticks = INT_MAX;
	if (to_ticks == 0 && msecs > 0)
		to_ticks = 1;

	return (timeout_add(to, (int)to_ticks));
}

int
timeout_add_usec(struct timeout *to, int usecs)
{
	int to_ticks = usecs / tick;

	if (to_ticks == 0 && usecs > 0)
		to_ticks = 1;

	return (timeout_add(to, to_ticks));
}

int
timeout_add_nsec(struct timeout *to, int nsecs)
{
	int to_ticks = nsecs / (tick * 1000);

	if (to_ticks == 0 && nsecs > 0)
		to_ticks = 1;

	return (timeout_add(to, to_ticks));
}

int
timeout_del(struct timeout *to)
{
	int ret = 0;

	mtx_enter(&timeout_mutex);
	if (to->to_flags & TIMEOUT_ONQUEUE) {
		CIRCQ_REMOVE(&to->to_list);
		to->to_flags &= ~TIMEOUT_ONQUEUE;
		ret = 1;
	}
	to->to_flags &= ~TIMEOUT_TRIGGERED;
	mtx_leave(&timeout_mutex);

	return (ret);
}

/*
 * This is called from hardclock() once every tick.
 * We return !0 if we need to schedule a softclock.
 */
int
timeout_hardclock_update(void)
{
	int ret;

	mtx_enter(&timeout_mutex);

	MOVEBUCKET(0, ticks);
	if (MASKWHEEL(0, ticks) == 0) {
		MOVEBUCKET(1, ticks);
		if (MASKWHEEL(1, ticks) == 0) {
			MOVEBUCKET(2, ticks);
			if (MASKWHEEL(2, ticks) == 0)
				MOVEBUCKET(3, ticks);
		}
	}
	ret = !CIRCQ_EMPTY(&timeout_todo);
	mtx_leave(&timeout_mutex);

	return (ret);
}

void
timeout_run(struct timeout *to)
{
	void (*fn)(void *);
	void *arg;

	MUTEX_ASSERT_LOCKED(&timeout_mutex);

	to->to_flags &= ~TIMEOUT_ONQUEUE;
	to->to_flags |= TIMEOUT_TRIGGERED;

	fn = to->to_func;
	arg = to->to_arg;

	mtx_leave(&timeout_mutex);
	fn(arg);
	mtx_enter(&timeout_mutex);
}

void
softclock(void *arg)
{
	int delta;
	struct circq *bucket;
	struct timeout *to;
	int needsproc = 0;

	mtx_enter(&timeout_mutex);
	while (!CIRCQ_EMPTY(&timeout_todo)) {
		to = timeout_from_circq(CIRCQ_FIRST(&timeout_todo));
		CIRCQ_REMOVE(&to->to_list);

		/*
		 * If due run it or defer execution to the thread,
		 * otherwise insert it into the right bucket.
		 */
		delta = to->to_time - ticks;
		if (delta > 0) {
			bucket = &BUCKET(delta, to->to_time);
			CIRCQ_INSERT(&to->to_list, bucket);
		} else if (to->to_flags & TIMEOUT_NEEDPROCCTX) {
			CIRCQ_INSERT(&to->to_list, &timeout_proc);
			needsproc = 1;
		} else {
#ifdef DEBUG
			if (delta < 0)
				printf("timeout delayed %d\n", delta);
#endif
			timeout_run(to);
		}
	}
	mtx_leave(&timeout_mutex);

	if (needsproc)
		wakeup(&timeout_proc);
}

void
softclock_create_thread(void *arg)
{
	if (kthread_create(softclock_thread, NULL, NULL, "softclock"))
		panic("fork softclock");
}

void
softclock_thread(void *arg)
{
	CPU_INFO_ITERATOR cii;
	struct cpu_info *ci;
	struct sleep_state sls;
	struct timeout *to;

	KERNEL_ASSERT_LOCKED();

	/* Be conservative for the moment */
	CPU_INFO_FOREACH(cii, ci) {
		if (CPU_IS_PRIMARY(ci))
			break;
	}
	KASSERT(ci != NULL);
	sched_peg_curproc(ci);

	for (;;) {
		sleep_setup(&sls, &timeout_proc, PSWP, "bored");
		sleep_finish(&sls, CIRCQ_EMPTY(&timeout_proc));

		mtx_enter(&timeout_mutex);
		while (!CIRCQ_EMPTY(&timeout_proc)) {
			to = timeout_from_circq(CIRCQ_FIRST(&timeout_proc));
			CIRCQ_REMOVE(&to->to_list);
			timeout_run(to);
		}
		mtx_leave(&timeout_mutex);
	}
}

#ifndef SMALL_KERNEL
void
timeout_adjust_ticks(int adj)
{
	struct timeout *to;
	struct circq *p;
	int new_ticks, b;

	/* adjusting the monotonic clock backwards would be a Bad Thing */
	if (adj <= 0)
		return;

	mtx_enter(&timeout_mutex);
	new_ticks = ticks + adj;
	for (b = 0; b < nitems(timeout_wheel); b++) {
		p = CIRCQ_FIRST(&timeout_wheel[b]);
		while (p != &timeout_wheel[b]) {
			to = timeout_from_circq(p);
			p = CIRCQ_FIRST(p);

			/* when moving a timeout forward need to reinsert it */
			if (to->to_time - ticks < adj)
				to->to_time = new_ticks;
			CIRCQ_REMOVE(&to->to_list);
			CIRCQ_INSERT(&to->to_list, &timeout_todo);
		}
	}
	ticks = new_ticks;
	mtx_leave(&timeout_mutex);
}
#endif

#ifdef DDB
void db_show_callout_bucket(struct circq *);

void
db_show_callout_bucket(struct circq *bucket)
{
	struct timeout *to;
	struct circq *p;
	db_expr_t offset;
	char *name;

	for (p = CIRCQ_FIRST(bucket); p != bucket; p = CIRCQ_FIRST(p)) {
		to = timeout_from_circq(p);
		db_find_sym_and_offset((db_addr_t)to->to_func, &name, &offset);
		name = name ? name : "?";
		db_printf("%9d %2td/%-4td %p  %s\n", to->to_time - ticks,
		    (bucket - timeout_wheel) / WHEELSIZE,
		    bucket - timeout_wheel, to->to_arg, name);
	}
}

void
db_show_callout(db_expr_t addr, int haddr, db_expr_t count, char *modif)
{
	int b;

	db_printf("ticks now: %d\n", ticks);
	db_printf("    ticks  wheel       arg  func\n");

	db_show_callout_bucket(&timeout_todo);
	for (b = 0; b < nitems(timeout_wheel); b++)
		db_show_callout_bucket(&timeout_wheel[b]);
}
#endif
@


1.49
log
@Introduce a new 'softclock' thread that will be used to execute timeout
callbacks needing a process context.

The function timeout_set_proc(9) has to be used instead of timeout_set(9)
when a timeout callback needs a process context.

Note that if such a timeout is waiting, understand sleeping, for a non
negligible amount of time it might delay other timeouts needing a process
context.

dlg@@ agrees with this as a temporary solution.

Manpage tweaks from jmc@@

ok kettenis@@, bluhm@@, mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.48 2016/07/06 15:53:01 tedu Exp $	*/
d378 1
d395 1
a395 1
			wakeup(&timeout_proc);
d405 3
d422 1
a434 1
	mtx_enter(&timeout_mutex);
d436 2
a437 2
		while (CIRCQ_EMPTY(&timeout_proc))
			msleep(&timeout_proc, &timeout_mutex, PSWP, "bored", 0);
d439 7
a445 3
		to = timeout_from_circq(CIRCQ_FIRST(&timeout_proc));
		CIRCQ_REMOVE(&to->to_list);
		timeout_run(to);
a446 1
	mtx_leave(&timeout_mutex);
@


1.48
log
@fix several places where calculating ticks could overflow.
it's not enough to assign to an unsigned type because if the arithmetic
overflows the compiler may decide to do anything. so change all the
long long casts to uint64_t so that we start with the right type.
reported by Tim Newsham of NCC.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.47 2016/06/23 18:41:44 stefan Exp $	*/
d30 1
a30 1
#include <sys/lock.h>
d57 1
d131 3
d154 1
d160 6
d173 6
d354 19
a377 1
	void (*fn)(void *);
d384 4
a387 1
		/* If due run it, otherwise insert it into the right bucket. */
d392 3
d400 5
a404 2
			to->to_flags &= ~TIMEOUT_ONQUEUE;
			to->to_flags |= TIMEOUT_TRIGGERED;
d406 6
a411 2
			fn = to->to_func;
			arg = to->to_arg;
d413 25
a437 4
			mtx_leave(&timeout_mutex);
			fn(arg);
			mtx_enter(&timeout_mutex);
		}
@


1.47
log
@Avoid multiple evaluation of macro arguments in softclock()

ok mikeb@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.46 2016/06/14 15:58:03 bluhm Exp $	*/
d205 1
a205 1
	long long to_ticks;
d207 1
a207 1
	to_ticks = (long long)hz * tv->tv_sec + tv->tv_usec / tick;
d219 1
a219 1
	long long to_ticks;
d221 1
a221 1
	to_ticks = (long long)hz * ts->tv_sec + ts->tv_nsec / (tick * 1000);
d233 1
a233 1
	long long to_ticks;
d235 1
a235 1
	to_ticks = (long long)hz * bt->sec + (long)(((uint64_t)1000000 *
d248 1
a248 1
	long long to_ticks;
d250 1
a250 1
	to_ticks = (long long)hz * secs;
d260 1
a260 1
	long long to_ticks;
d262 1
a262 1
	to_ticks = (long long)msecs * 1000 / tick;
@


1.46
log
@Prevent a round to zero in the timeout_add_...() functions.  Getting
an immediate timeout if a positive value is specified is unexpected
behavior.  Defer calling the handler for at least one tick.  Do not
change that timeout_add(0) gives you an immediate timeout.
OK millert@@ uebayasi@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.45 2016/03/20 07:56:08 uebayasi Exp $	*/
d339 2
d350 4
a353 3
		if (to->to_time - ticks > 0) {
			CIRCQ_INSERT(&to->to_list,
			    &BUCKET((to->to_time - ticks), to->to_time));
d356 2
a357 3
			if (to->to_time - ticks < 0)
				printf("timeout delayed %d\n", to->to_time -
				    ticks);
@


1.45
log
@Update ticks in hardclock().

OK mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.44 2016/03/17 10:53:57 uebayasi Exp $	*/
d210 2
d224 2
d239 2
d265 2
d276 3
d286 3
@


1.44
log
@KNF: Remove a blank line.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.43 2015/07/20 23:47:20 uebayasi Exp $	*/
a305 2

	ticks++;
@


1.43
log
@Move `ticks' declaration to sys/kernel.h.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a331 1

@


1.43.4.1
log
@backport timeout overflow fixes:
fix several places where calculating ticks could overflow.
it's not enough to assign to an unsigned type because if the arithmetic
overflows the compiler may decide to do anything. so change all the
long long casts to uint64_t so that we start with the right type.
reported by Tim Newsham of NCC.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.43 2015/07/20 23:47:20 uebayasi Exp $	*/
d205 1
a205 1
	uint64_t to_ticks;
d207 1
a207 1
	to_ticks = (uint64_t)hz * tv->tv_sec + tv->tv_usec / tick;
d217 1
a217 1
	uint64_t to_ticks;
d219 1
a219 1
	to_ticks = (uint64_t)hz * ts->tv_sec + ts->tv_nsec / (tick * 1000);
d229 1
a229 1
	uint64_t to_ticks;
d231 1
a231 1
	to_ticks = (uint64_t)hz * bt->sec + (long)(((uint64_t)1000000 *
d242 1
a242 1
	uint64_t to_ticks;
d244 1
a244 1
	to_ticks = (uint64_t)hz * secs;
d254 1
a254 1
	uint64_t to_ticks;
d256 1
a256 1
	to_ticks = (uint64_t)msecs * 1000 / tick;
@


1.43.2.1
log
@backport timeout overflow fixes:
fix several places where calculating ticks could overflow.
it's not enough to assign to an unsigned type because if the arithmetic
overflows the compiler may decide to do anything. so change all the
long long casts to uint64_t so that we start with the right type.
reported by Tim Newsham of NCC.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.43 2015/07/20 23:47:20 uebayasi Exp $	*/
d205 1
a205 1
	uint64_t to_ticks;
d207 1
a207 1
	to_ticks = (uint64_t)hz * tv->tv_sec + tv->tv_usec / tick;
d217 1
a217 1
	uint64_t to_ticks;
d219 1
a219 1
	to_ticks = (uint64_t)hz * ts->tv_sec + ts->tv_nsec / (tick * 1000);
d229 1
a229 1
	uint64_t to_ticks;
d231 1
a231 1
	to_ticks = (uint64_t)hz * bt->sec + (long)(((uint64_t)1000000 *
d242 1
a242 1
	uint64_t to_ticks;
d244 1
a244 1
	to_ticks = (uint64_t)hz * secs;
d254 1
a254 1
	uint64_t to_ticks;
d256 1
a256 1
	to_ticks = (uint64_t)msecs * 1000 / tick;
@


1.42
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.41 2013/11/27 04:28:32 dlg Exp $	*/
a142 1
extern int ticks;		/* XXX - move to sys/X.h */
@


1.41
log
@make timeout_add and its wrappers return whether the timeout was scheduled
in this call by returning 1, or a previous call by returning 0. this makes
it easy to refcount the stuff we're scheduling a timeout for, and brings
the api in line with what task_add(9) provides.

ok mpi@@ matthew@@ mikeb@@ guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.40 2013/10/06 04:34:35 guenther Exp $	*/
a38 1
#include <ddb/db_access.h>
@


1.40
log
@Replace some XXX casts with an inline function that explains what's going on

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.39 2013/10/02 21:03:21 sf Exp $	*/
d165 1
a165 1
void
d169 1
d194 1
d200 2
d204 1
a204 1
void
d213 1
a213 1
	timeout_add(to, (int)to_ticks);
d216 1
a216 1
void
d225 1
a225 1
	timeout_add(to, (int)to_ticks);
d228 1
a228 1
void
d238 1
a238 1
	timeout_add(to, (int)to_ticks);
d241 1
a241 1
void
d250 1
a250 1
	timeout_add(to, (int)to_ticks);
d253 1
a253 1
void
d262 1
a262 1
	timeout_add(to, (int)to_ticks);
d265 1
a265 1
void
d270 1
a270 1
	timeout_add(to, to_ticks);
d273 1
a273 1
void
d278 1
a278 1
	timeout_add(to, to_ticks);
@


1.39
log
@Format string fix: Use %td for pointer difference
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.38 2013/10/01 19:36:25 sf Exp $	*/
d76 11
d331 1
a331 1
		to = (struct timeout *)CIRCQ_FIRST(&timeout_todo); /* XXX */
d375 1
a375 1
			to = (struct timeout *)p; /* XXX */
d402 1
a402 1
		to = (struct timeout *)p; /* XXX */
@


1.38
log
@format string fix

to_arg is void *
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.37 2013/09/17 04:52:53 guenther Exp $	*/
d394 1
a394 1
		db_printf("%9d %2d/%-4d %p  %s\n", to->to_time - ticks,
@


1.37
log
@Fix a misaligned backslash
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.36 2013/08/03 06:47:15 guenther Exp $	*/
d394 1
a394 1
		db_printf("%9d %2d/%-4d %8x  %s\n", to->to_time - ticks,
@


1.36
log
@Delete variable left over from the diagnostic code removed by previous commit

pointed out by Artturi Alm (artturi.alm (at) gmail.com)
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.35 2012/06/02 00:11:16 guenther Exp $	*/
d63 1
a63 1
	((rel) <= (1 << (2*WHEELBITS)))				\
@


1.35
log
@Delete diagnostic code that reports timeout adjustments on resume.
It was useful for tracking down the last devices which weren't deleting
their timeouts on suspend and recreating them on resume, but it's too
verbose to keep around.

noted by deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.34 2012/05/24 07:17:42 guenther Exp $	*/
d353 1
a353 1
	int new_ticks, b, old;
a365 2

			old = to->to_time;
@


1.34
log
@On resume, run forward the monotonic and realtimes clocks instead of jumping
just the realtime clock, triggering and adjusting timeouts to reflect that.

ok matthew@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.33 2011/05/10 00:58:42 dlg Exp $	*/
a352 4
#ifdef DDB
	char *name;
	db_expr_t offset;
#endif
a373 8

#ifdef DDB
			db_find_sym_and_offset((db_addr_t)to->to_func, &name,
			    &offset);
			name = name ? name : "?";
			printf("adjusted timeout %6d -> %6d for %s\n",
			    old - ticks, to->to_time - new_ticks, name);
#endif
@


1.33
log
@tweak timeout_del so it can tell the caller if it actually did remove a
timeout or not.

without this it is impossible to tell if the timeout was removed
or if it is just about to run. if the caller of timeout_del is about
to free some state the timeout itself might use, this could lead
to a use after free.

now if timeout_del returns 1, you know the timeout wont fire and
you can proceed with cleanup. how you cope with the timeout being
about to fire is up to the caller of timeout_del.

discussed with drinking art and art, and most of k2k11
ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.32 2009/11/04 19:14:10 kettenis Exp $	*/
d346 46
@


1.32
log
@Get rid of __HAVE_GENERIC_SOFT_INTERRUPTS now that all our platforms support it.

ok jsing@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.31 2009/06/02 22:05:54 guenther Exp $	*/
d266 1
a266 1
void
d269 2
d275 1
d279 2
@


1.31
log
@Constipate the second argument to timeout_add_*().  Also, use
nitems() in two places instead of coding the array size and fix a
spot of whitespace.

ok miod@@ blambert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.30 2009/03/03 19:09:13 miod Exp $	*/
d307 1
a307 1
softclock(void)
a310 1
	void *arg;
@


1.30
log
@put back r1.25 (poisoning chain pointers after removing items from the
wheel). This was safe, except for osiop bugs.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.29 2008/10/22 08:38:06 blambert Exp $	*/
d141 1
a141 1
	for (b = 0; b < BUCKETS; b++)
d190 1
a190 1
timeout_add_tv(struct timeout *to, struct timeval *tv)
d202 1
a202 1
timeout_add_ts(struct timeout *to, struct timespec *ts)
d214 1
a214 1
timeout_add_bt(struct timeout *to, struct bintime *bt)
d373 1
a373 1
	for (b = 0; b < BUCKETS; b++)
@


1.29
log
@Add timeout_add_msec(), for timeouts in milliseconds.

Idea and original patch mk@@

ok mk@@, krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.28 2008/07/14 15:17:08 art Exp $	*/
d34 1
d112 2
@


1.28
log
@Don't lock timeouts in db_show_callout.

All cpus are stopped and this cpu blocks all interrupts. It doesn't make
sense to grab locks that ddb can then jump past with longjmp.

Noticed by Pierre Riteau. I just forgot about the bug until reminded
today.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.27 2008/07/11 14:18:39 blambert Exp $	*/
d229 12
@


1.27
log
@Add timeout_add_{tv,ts,bt,sec,usec,nsec} so that we can add timeouts
in something other than clock ticks. From art@@'s punchlist and (for
the time being) not yet used.

"you're doing it wrong" art@@,ray@@,otto@@,tedu@@

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.26 2008/01/20 18:23:38 miod Exp $	*/
a356 1
	mtx_enter(&timeout_mutex);
a359 1
	mtx_leave(&timeout_mutex);
@


1.26
log
@Revert 1.25 and do not use _Q_INVALIDATE on timeout structures; being
subtly different from CIRCLEQ, it is possible, when emptying the whole
timeout chain, to end up with CIRCQ_EMPTY being false, and bad things
happen. Back to the drawing board...
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.25 2006/07/19 20:25:08 miod Exp $	*/
d33 1
d184 65
@


1.25
log
@Sprinkle some _Q_INVALIDATE love for timeout structures as well; ok otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.24 2006/04/21 01:35:27 dlg Exp $	*/
a32 1
#include <sys/queue.h>
a109 2
	_Q_INVALIDATE((elem)->prev);		\
	_Q_INVALIDATE((elem)->next);		\
@


1.24
log
@back out my last commit.

art pointed out that timeout_set is the initializer of timeout structs.
this means that the ONQUEUE flag could be set when timeout_set is given
freshly allocated memory. my commit suddenly introduced the requirement
that you bzero a timeout before initialising it. without the bzero we
could generate false positives about the timeout being already queued.

art did produce a diff that would walk the queues when the flag was set
to see if it really was in the lists, but deraadt considers this too much
of a hit.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.23 2006/04/18 21:48:34 dlg Exp $	*/
d33 1
d111 2
@


1.23
log
@if you go timeout_set, timeout_add, and then timeout_set again you can
screw up the queues that tie all the timeouts together. this makes us
panic if we detect that happening. its a lot easier to debug that the
weird side effects of broken timeout queues.

ok mickey@@ kettenis@@ deraadt@@ pedro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.22 2004/12/28 22:48:30 deraadt Exp $	*/
a143 5
#ifdef DIAGNOSTIC
	if (new->to_flags & TIMEOUT_ONQUEUE)
		panic("timeout_set: already queued");
#endif

@


1.22
log
@when panicing from timeout_add being < 0, print the value; ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.21 2004/12/12 20:37:01 espie Exp $	*/
d144 5
@


1.21
log
@simplify slightly, kill lvalue weirdness that's not ANSI C.
okay nordin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.20 2004/11/10 11:00:00 grange Exp $	*/
d159 1
a159 1
		panic("timeout_add: to_ticks < 0");
@


1.20
log
@Diff from art@@:

Update ticks in timeout_hardclock_update to avoid errors in hardclock (this
is the third time we mess up here). ticks is only used for timeouts anyway.
At the same protect updating ticks with timeout_mutex and be slightly
more paranoid in timeout_hardclock_update.

ok tdeval@@ miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.19 2004/07/20 20:20:52 art Exp $	*/
d60 8
a67 7
    (((rel) <= (1 << (2*WHEELBITS)))					\
    	? ((rel) <= (1 << WHEELBITS))					\
            ? timeout_wheel[MASKWHEEL(0, (abs))]			\
            : timeout_wheel[MASKWHEEL(1, (abs)) + WHEELSIZE]		\
        : ((rel) <= (1 << (3*WHEELBITS)))				\
            ? timeout_wheel[MASKWHEEL(2, (abs)) + 2*WHEELSIZE]		\
            : timeout_wheel[MASKWHEEL(3, (abs)) + 3*WHEELSIZE])
@


1.19
log
@The timeout wheels are manipulated by hardclock that's not protected with
biglock. We need to protect them with a mutex.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.18 2003/06/03 12:05:25 art Exp $	*/
d203 2
d206 3
d218 1
d220 2
a221 1
	return (!CIRCQ_EMPTY(&timeout_todo));
@


1.18
log
@Two term license with approval from nordin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.17 2003/05/17 14:02:06 grange Exp $	*/
d32 1
d73 4
a76 2
 * All wheels are locked with the same lock (which must also block out all
 * interrupts).
d78 1
a78 6
struct simplelock _timeout_lock;

#define timeout_wheel_lock(s) \
	do { *(s) = splhigh(); simple_lock(&_timeout_lock); } while (0)
#define timeout_wheel_unlock(s) \
	do { simple_unlock(&_timeout_lock); splx(s); } while (0)
a137 1
	simple_lock_init(&_timeout_lock);
a151 1
	int s;
d161 1
a161 1
	timeout_wheel_lock(&s);
d181 1
a181 2

	timeout_wheel_unlock(s);
d187 1
a187 3
	int s;

	timeout_wheel_lock(&s);
d193 1
a193 1
	timeout_wheel_unlock(s);
a198 2
 *
 * We don't need locking in here.
d203 1
d213 1
a220 1
	int s;
d224 1
a224 1
	timeout_wheel_lock(&s);
d246 1
a246 1
			timeout_wheel_unlock(s);
d248 1
a248 1
			timeout_wheel_lock(&s);
d251 1
a251 1
	timeout_wheel_unlock(s);
a277 1
	int s;
d283 1
a283 2
	timeout_wheel_lock(&s);

d287 1
a287 2

	timeout_wheel_unlock(s);
@


1.17
log
@Typos; from Julien Bordet <zejames@@greyhats.org>
Close PR 3262
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.16 2003/04/13 21:04:52 tedu Exp $	*/
d13 1
a13 4
 * 2. Redistributions in binary form must reproduce the above copyright 
 *    notice, this list of conditions and the following disclaimer in the 
 *    documentation and/or other materials provided with the distribution. 
 * 3. The name of the author may not be used to endorse or promote products
@


1.16
log
@diff from nordin@@ to prevent wraparound on long uptime machines.
ok art@@ deraadt@@ miod@@ tdeval@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.15 2002/12/08 04:21:07 art Exp $	*/
d129 1
a129 1
 * is when we caluculate how far in the future `to' will timeout -
@


1.15
log
@ - Lock the timeout wheel after the diagnostic checks.
 - show timeout_todo in the ddb command.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.14 2002/03/14 01:27:04 millert Exp $	*/
d180 1
a180 1
		if (new->to_time < old_time) {
@


1.14
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.13 2002/02/15 18:10:15 nordin Exp $	*/
a160 1
	timeout_wheel_lock(&s);
d167 2
d297 1
a297 1
	/* XXX: Show timeout_todo? */
@


1.13
log
@Make rescheduling to a later time faster. ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.12 2001/12/22 16:41:51 nordin Exp $	*/
d265 1
a265 1
void db_show_callout_bucket __P((struct circq *));
@


1.12
log
@New scalable implementation with constant time add and delete. ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.11 2001/09/12 15:48:45 art Exp $	*/
d159 1
a167 6
	/* If this timeout was already on a queue we remove it. */
	if (new->to_flags & TIMEOUT_ONQUEUE)
		CIRCQ_REMOVE(&new->to_list);
	else
		new->to_flags |= TIMEOUT_ONQUEUE;

d169 1
d173 15
a187 1
	CIRCQ_INSERT(&new->to_list, &timeout_todo);
@


1.11
log
@Rename timeout_init to timeout_startup to deconfuse a bit.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.10 2001/08/23 11:20:05 art Exp $	*/
d3 2
a4 1
 * Copyright (c) 2000 Artur Grabowski <art@@openbsd.org>
d45 5
a49 6
 * Timeouts are kept on a queue. The to_time is the value of the global
 * variable "ticks" when the timeout should be called.
 *
 * In the future we might want to build a timer wheel to improve the speed
 * of timeout_add (right now it's linear). See "Redesigning the BSD Callout 
 * and Timer Facilities" by Adam M. Costello and Geroge Varghese.
d51 22
a72 2

TAILQ_HEAD(,timeout) timeout_todo;	/* Queue of timeouts. */
d75 1
a75 1
 * All lists are locked with the same lock (which must also block out all
d80 1
a80 1
#define timeout_list_lock(s) \
d82 1
a82 1
#define timeout_list_unlock(s) \
d86 35
d124 2
a125 2
 * We use the fact that any element added to the list must be added with a
 * positive time. That means that any element `to' on the list cannot be
d129 2
a130 2
 * is when we caluculate how far in the future `to' will timeout - 
 *"to->to_time - ticks". The result will always be positive for future
d136 1
a136 1
timeout_startup()
d138 1
d140 3
a142 1
	TAILQ_INIT(&timeout_todo);
d147 1
a147 4
timeout_set(new, fn, arg)
	struct timeout *new;
	void (*fn)(void *);
	void *arg;
a148 20

#ifdef DIAGNOSTIC
	struct timeout *to;
	int s;

	/*
	 * Be careful. We could be called with random non-zero memory, but
	 * on the other hand we could be called with a timeout that's
	 * already queued.
	 * XXX - this is expensive.
	 */
	timeout_list_lock(&s);
	if (new->to_flags & TIMEOUT_ONQUEUE) {
		TAILQ_FOREACH(to, &timeout_todo, to_list)
			if (to == new)
				panic("timeout_set: on queue");
	}
	timeout_list_unlock(s);
#endif

d154 1
d156 1
a156 3
timeout_add(new, to_ticks)
	struct timeout *new;
	int to_ticks;
a157 1
	struct timeout *to;
d160 1
a160 4
	/*
	 * You are supposed to understand this function before you fiddle.
	 */

a166 8

	timeout_list_lock(&s);

	/*
	 * First we prepare the new timeout so that we can return right
	 * after the insertion in the queue (makes the code simpler).
	 */

d169 1
a169 1
		TAILQ_REMOVE(&timeout_todo, new, to_list);
d172 1
d177 2
a178 15
	/*
	 * Walk the list of pending timeouts and find an entry which
	 * will timeout after we do, insert the new timeout there.
	 */
	TAILQ_FOREACH(to, &timeout_todo, to_list) {
		if (to->to_time - ticks > to_ticks) {
			TAILQ_INSERT_BEFORE(to, new, to_list);
			goto out;
		}
	}

	/* We can only get here if we're the last (or only) entry */
	TAILQ_INSERT_TAIL(&timeout_todo, new, to_list);
out:
	timeout_list_unlock(s);
d182 1
a182 2
timeout_del(to)
	struct timeout *to;
d186 1
a186 1
	timeout_list_lock(&s);
d188 1
a188 1
		TAILQ_REMOVE(&timeout_todo, to, to_list);
d192 1
a192 1
	timeout_list_unlock(s);
d202 1
a202 1
timeout_hardclock_update()
d204 10
a213 8
	struct timeout *to;

	to = TAILQ_FIRST(&timeout_todo);

	if (to == NULL)
		return 0;

	return (to->to_time - ticks <= 0);
d217 1
a217 1
softclock()
d219 1
d221 1
a221 2
	struct timeout *to;
	void (*fn) __P((void *));
d224 15
a238 6
	timeout_list_lock(&s);
	while ((to = TAILQ_FIRST(&timeout_todo)) != NULL &&
	       to->to_time - ticks <= 0) {
#ifdef DIAGNOSTIC
		if (!(to->to_flags & TIMEOUT_ONQUEUE))
			panic("softclock: not onqueue");
d240 2
a241 3
		TAILQ_REMOVE(&timeout_todo, to, to_list);
		to->to_flags &= ~TIMEOUT_ONQUEUE;
		to->to_flags |= TIMEOUT_TRIGGERED;
d243 2
a244 2
		fn = to->to_func;
		arg = to->to_arg;
d246 4
a249 3
		timeout_list_unlock(s);
		fn(arg);
		timeout_list_lock(&s);
d251 1
a251 1
	timeout_list_unlock(s);
d255 2
d258 1
a258 5
db_show_callout(addr, haddr, count, modif)
	db_expr_t addr; 
	int haddr; 
	db_expr_t count;
	char *modif;
d261 1
a261 1
	int s;
d265 9
a273 2
	db_printf("ticks now: %d\n", ticks);
	db_printf("    ticks      arg  func\n");
d275 5
a279 1
	timeout_list_lock(&s);
d281 2
a282 2
	TAILQ_FOREACH(to, &timeout_todo, to_list) {
		db_find_sym_and_offset((db_addr_t)to->to_func, &name, &offset);
d284 1
a284 1
		name = name ? name : "?";
d286 3
a288 2
		db_printf("%9d %8x  %s\n", to->to_time, to->to_arg, name);
	}
d290 1
a290 2
	timeout_list_unlock(s);
		
@


1.11.4.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.12 2001/12/22 16:41:51 nordin Exp $	*/
d3 1
a3 2
 * Copyright (c) 2001 Thomas Nordin <nordin@@openbsd.org>
 * Copyright (c) 2000-2001 Artur Grabowski <art@@openbsd.org>
d44 6
a49 5
 * Timeouts are kept in a hierarchical timing wheel. The to_time is the value
 * of the global variable "ticks" when the timeout should be called. There are
 * four levels with 256 buckets each. See 'Scheme 7' in
 * "Hashed and Hierarchical Timing Wheels: Efficient Data Structures for
 * Implementing a Timer Facility" by George Varghese and Tony Lauck.
d51 2
a52 22
#define BUCKETS 1024
#define WHEELSIZE 256
#define WHEELMASK 255
#define WHEELBITS 8

struct circq timeout_wheel[BUCKETS];	/* Queues of timeouts */
struct circq timeout_todo;		/* Worklist */

#define MASKWHEEL(wheel, time) (((time) >> ((wheel)*WHEELBITS)) & WHEELMASK)

#define BUCKET(rel, abs)						\
    (((rel) <= (1 << (2*WHEELBITS)))					\
    	? ((rel) <= (1 << WHEELBITS))					\
            ? timeout_wheel[MASKWHEEL(0, (abs))]			\
            : timeout_wheel[MASKWHEEL(1, (abs)) + WHEELSIZE]		\
        : ((rel) <= (1 << (3*WHEELBITS)))				\
            ? timeout_wheel[MASKWHEEL(2, (abs)) + 2*WHEELSIZE]		\
            : timeout_wheel[MASKWHEEL(3, (abs)) + 3*WHEELSIZE])

#define MOVEBUCKET(wheel, time)						\
    CIRCQ_APPEND(&timeout_todo,						\
        &timeout_wheel[MASKWHEEL((wheel), (time)) + (wheel)*WHEELSIZE])
d55 1
a55 1
 * All wheels are locked with the same lock (which must also block out all
d60 1
a60 1
#define timeout_wheel_lock(s) \
d62 1
a62 1
#define timeout_wheel_unlock(s) \
a65 35
 * Circular queue definitions.
 */

#define CIRCQ_INIT(elem) do {                   \
        (elem)->next = (elem);                  \
        (elem)->prev = (elem);                  \
} while (0)

#define CIRCQ_INSERT(elem, list) do {           \
        (elem)->prev = (list)->prev;            \
        (elem)->next = (list);                  \
        (list)->prev->next = (elem);            \
        (list)->prev = (elem);                  \
} while (0)

#define CIRCQ_APPEND(fst, snd) do {             \
        if (!CIRCQ_EMPTY(snd)) {                \
                (fst)->prev->next = (snd)->next;\
                (snd)->next->prev = (fst)->prev;\
                (snd)->prev->next = (fst);      \
                (fst)->prev = (snd)->prev;      \
                CIRCQ_INIT(snd);                \
        }                                       \
} while (0)

#define CIRCQ_REMOVE(elem) do {                 \
        (elem)->next->prev = (elem)->prev;      \
        (elem)->prev->next = (elem)->next;      \
} while (0)

#define CIRCQ_FIRST(elem) ((elem)->next)

#define CIRCQ_EMPTY(elem) (CIRCQ_FIRST(elem) == (elem))

/*
d69 2
a70 2
 * We use the fact that any element added to the queue must be added with a
 * positive time. That means that any element `to' on the queue cannot be
d74 2
a75 2
 * is when we caluculate how far in the future `to' will timeout -
 * "to->to_time - ticks". The result will always be positive for future
d81 1
a81 1
timeout_startup(void)
a82 1
	int b;
d84 1
a84 3
	CIRCQ_INIT(&timeout_todo);
	for (b = 0; b < BUCKETS; b++)
		CIRCQ_INIT(&timeout_wheel[b]);
d89 4
a92 1
timeout_set(struct timeout *new, void (*fn)(void *), void *arg)
d94 20
a118 1

d120 3
a122 1
timeout_add(struct timeout *new, int to_ticks)
d124 1
d127 4
a130 1
	timeout_wheel_lock(&s);
d137 8
d147 1
a147 1
		CIRCQ_REMOVE(&new->to_list);
a149 1

d154 15
a168 2
	CIRCQ_INSERT(&new->to_list, &timeout_todo);
	timeout_wheel_unlock(s);
d172 2
a173 1
timeout_del(struct timeout *to)
d177 1
a177 1
	timeout_wheel_lock(&s);
d179 1
a179 1
		CIRCQ_REMOVE(&to->to_list);
d183 1
a183 1
	timeout_wheel_unlock(s);
d193 1
a193 1
timeout_hardclock_update(void)
d195 8
a202 10
	MOVEBUCKET(0, ticks);
	if (MASKWHEEL(0, ticks) == 0) {
		MOVEBUCKET(1, ticks);
		if (MASKWHEEL(1, ticks) == 0) {
			MOVEBUCKET(2, ticks);
			if (MASKWHEEL(2, ticks) == 0)
				MOVEBUCKET(3, ticks);
		}
	}
	return (!CIRCQ_EMPTY(&timeout_todo));
d206 1
a206 1
softclock(void)
d208 1
d210 1
a210 2
	int s;
	void (*fn)(void *);
d213 6
a218 15
	timeout_wheel_lock(&s);
	while (!CIRCQ_EMPTY(&timeout_todo)) {

		to = (struct timeout *)CIRCQ_FIRST(&timeout_todo); /* XXX */
		CIRCQ_REMOVE(&to->to_list);

		/* If due run it, otherwise insert it into the right bucket. */
		if (to->to_time - ticks > 0) {
			CIRCQ_INSERT(&to->to_list,
			    &BUCKET((to->to_time - ticks), to->to_time));
		} else {
#ifdef DEBUG
			if (to->to_time - ticks < 0)
				printf("timeout delayed %d\n", to->to_time -
				    ticks);
d220 3
a222 2
			to->to_flags &= ~TIMEOUT_ONQUEUE;
			to->to_flags |= TIMEOUT_TRIGGERED;
d224 2
a225 2
			fn = to->to_func;
			arg = to->to_arg;
d227 3
a229 4
			timeout_wheel_unlock(s);
			fn(arg);
			timeout_wheel_lock(&s);
		}
d231 1
a231 1
	timeout_wheel_unlock(s);
a234 2
void db_show_callout_bucket __P((struct circq *));

d236 5
a240 1
db_show_callout_bucket(struct circq *bucket)
d243 1
a243 1
	struct circq *p;
d247 2
a248 9
	for (p = CIRCQ_FIRST(bucket); p != bucket; p = CIRCQ_FIRST(p)) {
		to = (struct timeout *)p; /* XXX */
		db_find_sym_and_offset((db_addr_t)to->to_func, &name, &offset);
		name = name ? name : "?";
		db_printf("%9d %2d/%-4d %8x  %s\n", to->to_time - ticks,
		    (bucket - timeout_wheel) / WHEELSIZE,
		    bucket - timeout_wheel, to->to_arg, name);
	}
}
d250 1
a250 5
void
db_show_callout(db_expr_t addr, int haddr, db_expr_t count, char *modif)
{
	int s;
	int b;
d252 2
a253 2
	db_printf("ticks now: %d\n", ticks);
	db_printf("    ticks  wheel       arg  func\n");
d255 1
a255 1
	timeout_wheel_lock(&s);
d257 2
a258 3
	/* XXX: Show timeout_todo? */
	for (b = 0; b < BUCKETS; b++)
		db_show_callout_bucket(&timeout_wheel[b]);
d260 2
a261 1
	timeout_wheel_unlock(s);
@


1.11.4.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.11.4.1 2002/01/31 22:55:40 niklas Exp $	*/
a158 1
	int old_time;
d167 6
a173 1
	old_time = new->to_time;
d177 1
a177 15
	/*
	 * If this timeout already is scheduled and now is moved
	 * earlier, reschedule it now. Otherwise leave it in place
	 * and let it be rescheduled later.
	 */
	if (new->to_flags & TIMEOUT_ONQUEUE) {
		if (new->to_time < old_time) {
			CIRCQ_REMOVE(&new->to_list);
			CIRCQ_INSERT(&new->to_list, &timeout_todo);
		}
	} else {
		new->to_flags |= TIMEOUT_ONQUEUE;
		CIRCQ_INSERT(&new->to_list, &timeout_todo);
	}

d255 1
a255 1
void db_show_callout_bucket(struct circq *);
@


1.11.4.3
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d161 1
a167 2

	timeout_wheel_lock(&s);
d179 1
a179 1
		if (new->to_time - ticks < old_time - ticks) {
d296 1
a296 1
	db_show_callout_bucket(&timeout_todo);
@


1.10
log
@Remove even more leftovers from old timeouts.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.9 2001/08/23 08:18:57 miod Exp $	*/
d81 1
a81 1
timeout_init()
@


1.9
log
@Remove the old timeout legacy code.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.8 2001/03/28 07:33:51 art Exp $	*/
a52 1
TAILQ_HEAD(,timeout) timeout_static;	/* Static pool of timeouts. */
a82 1
	int i;
a84 1
	TAILQ_INIT(&timeout_static);
a85 3

	for (i = 0; i < ntimeout; i++)
		TAILQ_INSERT_HEAD(&timeout_static, &timeouts[i], to_list);
a226 2
		if (to->to_flags & TIMEOUT_STATIC)
			TAILQ_INSERT_HEAD(&timeout_static, to, to_list);
@


1.8
log
@Add some DIAGNOSTIC checks that panic on some of the common mistakes.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.7 2001/03/15 16:47:50 csapuntz Exp $	*/
a237 63
	}
	timeout_list_unlock(s);
}

/*
 * Legacy interfaces. timeout() and untimeout()
 *
 * Kill those when everything is converted. They are slow and use the
 * static pool (which causes (potential and real) problems).
 */

void
timeout(fn, arg, to_ticks)
	void (*fn) __P((void *));
	void *arg;
	int to_ticks;
{
	struct timeout *to;
	int s;

	if (to_ticks <= 0)
		to_ticks = 1;

	/*
	 * Get a timeout struct from the static list.
	 */
	timeout_list_lock(&s);

	to = TAILQ_FIRST(&timeout_static);
	if (to == NULL)
		panic("timeout table full");
	TAILQ_REMOVE(&timeout_static, to, to_list);

	timeout_list_unlock(s);

	timeout_set(to, fn, arg);
	to->to_flags |= TIMEOUT_STATIC;
	timeout_add(to, to_ticks);
}

void
untimeout(fn, arg)
	void (*fn) __P((void *));
	void *arg;
{
	int s;
	struct timeout *to;

	timeout_list_lock(&s);
	TAILQ_FOREACH(to, &timeout_todo, to_list) {
		if (to->to_func == fn && to->to_arg == arg) {
#ifdef DIAGNOSTIC
			if ((to->to_flags & TIMEOUT_ONQUEUE) == 0)
				panic("untimeout: not TIMEOUT_ONQUEUE");
			if ((to->to_flags & TIMEOUT_STATIC) == 0)
				panic("untimeout: not static");
#endif
			TAILQ_REMOVE(&timeout_todo, to, to_list);
			to->to_flags &= ~TIMEOUT_ONQUEUE;
			/* return it to the static pool */
			TAILQ_INSERT_HEAD(&timeout_static, to, to_list);
			break;
		}
@


1.7
log
@

Triggered mechanism allows a handler to figure out whether a given
timeout is actually executing.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.6 2001/02/16 13:47:40 espie Exp $	*/
d95 2
a96 2
timeout_set(to, fn, arg)
	struct timeout *to;
d101 22
a122 3
	to->to_func = fn;
	to->to_arg = arg;
	to->to_flags = TIMEOUT_INITIALIZED;
d138 2
d222 4
a225 1

@


1.6
log
@typo (incidentally, removed a bogus -k in the cvs file, the update
will trigger $OpenBSD$ fill-in)
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d137 1
d167 1
d204 1
@


1.5
log
@timeout_add: Remove the right timeout when we see that it is on the list.
@
text
@d126 1
a126 1
	 * First we prepare the now timeout so that we can return right
@


1.4
log
@from art@@:
in timeout_hardclock_update() check out if there is
anything in the timeout queue before calculating the return value.
thanks to Adam Rogoyski <rogoyski@@cs.utexas.edu> for
debugging and testing help.
@
text
@d132 1
a132 1
		TAILQ_REMOVE(&timeout_todo, to, to_list);
@


1.3
log
@Provide methods to check if a timeout was initalized and if it is scheduled.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.2 2000/03/23 10:27:05 art Exp $	*/
d178 8
a185 1
	return (TAILQ_FIRST(&timeout_todo)->to_time - ticks <= 0);
@


1.3.2.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
@


1.3.2.2
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.8 2001/03/28 07:33:51 art Exp $	*/
d95 2
a96 2
timeout_set(new, fn, arg)
	struct timeout *new;
d101 3
a103 22
#ifdef DIAGNOSTIC
	struct timeout *to;
	int s;

	/*
	 * Be careful. We could be called with random non-zero memory, but
	 * on the other hand we could be called with a timeout that's
	 * already queued.
	 * XXX - this is expensive.
	 */
	timeout_list_lock(&s);
	if (new->to_flags & TIMEOUT_ONQUEUE) {
		TAILQ_FOREACH(to, &timeout_todo, to_list)
			if (to == new)
				panic("timeout_set: on queue");
	}
	timeout_list_unlock(s);
#endif

	new->to_func = fn;
	new->to_arg = arg;
	new->to_flags = TIMEOUT_INITIALIZED;
a118 2
	if (!(new->to_flags & TIMEOUT_INITIALIZED))
		panic("timeout_add: not initialized");
d126 1
a126 1
	 * First we prepare the new timeout so that we can return right
d132 1
a132 1
		TAILQ_REMOVE(&timeout_todo, new, to_list);
a136 1
	new->to_flags &= ~TIMEOUT_TRIGGERED;
a165 1
	to->to_flags &= ~TIMEOUT_TRIGGERED;
d178 1
a178 8
	struct timeout *to;

	to = TAILQ_FIRST(&timeout_todo);

	if (to == NULL)
		return 0;

	return (to->to_time - ticks <= 0);
d192 1
a192 4
#ifdef DIAGNOSTIC
		if (!(to->to_flags & TIMEOUT_ONQUEUE))
			panic("softclock: not onqueue");
#endif
a194 1
		to->to_flags |= TIMEOUT_TRIGGERED;
@


1.3.2.3
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.3.2.2 2001/05/14 22:32:42 niklas Exp $	*/
d53 1
d82 1
a82 1
timeout_startup()
d84 1
d87 1
d89 3
d233 2
d238 63
@


1.3.2.4
log
@Merge in trunk
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d3 1
a3 2
 * Copyright (c) 2001 Thomas Nordin <nordin@@openbsd.org>
 * Copyright (c) 2000-2001 Artur Grabowski <art@@openbsd.org>
d44 6
a49 5
 * Timeouts are kept in a hierarchical timing wheel. The to_time is the value
 * of the global variable "ticks" when the timeout should be called. There are
 * four levels with 256 buckets each. See 'Scheme 7' in
 * "Hashed and Hierarchical Timing Wheels: Efficient Data Structures for
 * Implementing a Timer Facility" by George Varghese and Tony Lauck.
d51 2
a52 22
#define BUCKETS 1024
#define WHEELSIZE 256
#define WHEELMASK 255
#define WHEELBITS 8

struct circq timeout_wheel[BUCKETS];	/* Queues of timeouts */
struct circq timeout_todo;		/* Worklist */

#define MASKWHEEL(wheel, time) (((time) >> ((wheel)*WHEELBITS)) & WHEELMASK)

#define BUCKET(rel, abs)						\
    (((rel) <= (1 << (2*WHEELBITS)))					\
    	? ((rel) <= (1 << WHEELBITS))					\
            ? timeout_wheel[MASKWHEEL(0, (abs))]			\
            : timeout_wheel[MASKWHEEL(1, (abs)) + WHEELSIZE]		\
        : ((rel) <= (1 << (3*WHEELBITS)))				\
            ? timeout_wheel[MASKWHEEL(2, (abs)) + 2*WHEELSIZE]		\
            : timeout_wheel[MASKWHEEL(3, (abs)) + 3*WHEELSIZE])

#define MOVEBUCKET(wheel, time)						\
    CIRCQ_APPEND(&timeout_todo,						\
        &timeout_wheel[MASKWHEEL((wheel), (time)) + (wheel)*WHEELSIZE])
d55 1
a55 1
 * All wheels are locked with the same lock (which must also block out all
d60 1
a60 1
#define timeout_wheel_lock(s) \
d62 1
a62 1
#define timeout_wheel_unlock(s) \
a65 35
 * Circular queue definitions.
 */

#define CIRCQ_INIT(elem) do {                   \
        (elem)->next = (elem);                  \
        (elem)->prev = (elem);                  \
} while (0)

#define CIRCQ_INSERT(elem, list) do {           \
        (elem)->prev = (list)->prev;            \
        (elem)->next = (list);                  \
        (list)->prev->next = (elem);            \
        (list)->prev = (elem);                  \
} while (0)

#define CIRCQ_APPEND(fst, snd) do {             \
        if (!CIRCQ_EMPTY(snd)) {                \
                (fst)->prev->next = (snd)->next;\
                (snd)->next->prev = (fst)->prev;\
                (snd)->prev->next = (fst);      \
                (fst)->prev = (snd)->prev;      \
                CIRCQ_INIT(snd);                \
        }                                       \
} while (0)

#define CIRCQ_REMOVE(elem) do {                 \
        (elem)->next->prev = (elem)->prev;      \
        (elem)->prev->next = (elem)->next;      \
} while (0)

#define CIRCQ_FIRST(elem) ((elem)->next)

#define CIRCQ_EMPTY(elem) (CIRCQ_FIRST(elem) == (elem))

/*
d69 2
a70 2
 * We use the fact that any element added to the queue must be added with a
 * positive time. That means that any element `to' on the queue cannot be
d74 2
a75 2
 * is when we caluculate how far in the future `to' will timeout -
 * "to->to_time - ticks". The result will always be positive for future
d81 1
a81 1
timeout_startup(void)
a82 1
	int b;
d84 1
a84 3
	CIRCQ_INIT(&timeout_todo);
	for (b = 0; b < BUCKETS; b++)
		CIRCQ_INIT(&timeout_wheel[b]);
d89 4
a92 1
timeout_set(struct timeout *new, void (*fn)(void *), void *arg)
d94 20
a118 1

d120 3
a122 1
timeout_add(struct timeout *new, int to_ticks)
d124 1
a125 1
	int old_time;
d127 4
a130 1
	timeout_wheel_lock(&s);
d137 13
a150 1
	old_time = new->to_time;
d155 2
a156 3
	 * If this timeout already is scheduled and now is moved
	 * earlier, reschedule it now. Otherwise leave it in place
	 * and let it be rescheduled later.
d158 4
a161 4
	if (new->to_flags & TIMEOUT_ONQUEUE) {
		if (new->to_time < old_time) {
			CIRCQ_REMOVE(&new->to_list);
			CIRCQ_INSERT(&new->to_list, &timeout_todo);
a162 3
	} else {
		new->to_flags |= TIMEOUT_ONQUEUE;
		CIRCQ_INSERT(&new->to_list, &timeout_todo);
d165 4
a168 1
	timeout_wheel_unlock(s);
d172 2
a173 1
timeout_del(struct timeout *to)
d177 1
a177 1
	timeout_wheel_lock(&s);
d179 1
a179 1
		CIRCQ_REMOVE(&to->to_list);
d183 1
a183 1
	timeout_wheel_unlock(s);
d193 1
a193 1
timeout_hardclock_update(void)
d195 8
a202 10
	MOVEBUCKET(0, ticks);
	if (MASKWHEEL(0, ticks) == 0) {
		MOVEBUCKET(1, ticks);
		if (MASKWHEEL(1, ticks) == 0) {
			MOVEBUCKET(2, ticks);
			if (MASKWHEEL(2, ticks) == 0)
				MOVEBUCKET(3, ticks);
		}
	}
	return (!CIRCQ_EMPTY(&timeout_todo));
d206 1
a206 1
softclock(void)
d208 1
d210 1
a210 2
	int s;
	void (*fn)(void *);
d213 6
a218 15
	timeout_wheel_lock(&s);
	while (!CIRCQ_EMPTY(&timeout_todo)) {

		to = (struct timeout *)CIRCQ_FIRST(&timeout_todo); /* XXX */
		CIRCQ_REMOVE(&to->to_list);

		/* If due run it, otherwise insert it into the right bucket. */
		if (to->to_time - ticks > 0) {
			CIRCQ_INSERT(&to->to_list,
			    &BUCKET((to->to_time - ticks), to->to_time));
		} else {
#ifdef DEBUG
			if (to->to_time - ticks < 0)
				printf("timeout delayed %d\n", to->to_time -
				    ticks);
d220 3
a222 2
			to->to_flags &= ~TIMEOUT_ONQUEUE;
			to->to_flags |= TIMEOUT_TRIGGERED;
d224 2
a225 2
			fn = to->to_func;
			arg = to->to_arg;
d227 3
a229 4
			timeout_wheel_unlock(s);
			fn(arg);
			timeout_wheel_lock(&s);
		}
d231 1
a231 1
	timeout_wheel_unlock(s);
a234 2
void db_show_callout_bucket __P((struct circq *));

d236 5
a240 1
db_show_callout_bucket(struct circq *bucket)
d243 1
a243 1
	struct circq *p;
d247 2
a248 9
	for (p = CIRCQ_FIRST(bucket); p != bucket; p = CIRCQ_FIRST(p)) {
		to = (struct timeout *)p; /* XXX */
		db_find_sym_and_offset((db_addr_t)to->to_func, &name, &offset);
		name = name ? name : "?";
		db_printf("%9d %2d/%-4d %8x  %s\n", to->to_time - ticks,
		    (bucket - timeout_wheel) / WHEELSIZE,
		    bucket - timeout_wheel, to->to_arg, name);
	}
}
d250 1
a250 5
void
db_show_callout(db_expr_t addr, int haddr, db_expr_t count, char *modif)
{
	int s;
	int b;
d252 2
a253 2
	db_printf("ticks now: %d\n", ticks);
	db_printf("    ticks  wheel       arg  func\n");
d255 1
a255 1
	timeout_wheel_lock(&s);
d257 2
a258 3
	/* XXX: Show timeout_todo? */
	for (b = 0; b < BUCKETS; b++)
		db_show_callout_bucket(&timeout_wheel[b]);
d260 2
a261 1
	timeout_wheel_unlock(s);
@


1.3.2.5
log
@Merge in -current from about a week ago
@
text
@d265 1
a265 1
void db_show_callout_bucket(struct circq *);
@


1.3.2.6
log
@Sync the SMP branch with 3.3
@
text
@d161 1
a167 2

	timeout_wheel_lock(&s);
d296 1
a296 1
	db_show_callout_bucket(&timeout_todo);
@


1.3.2.7
log
@Sync the SMP branch to -current. This includes moving to ELF.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.3.2.6 2003/03/28 00:41:26 niklas Exp $	*/
d180 1
a180 1
		if (new->to_time - ticks < old_time - ticks) {
@


1.3.2.8
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.3.2.7 2003/05/13 19:21:28 ho Exp $	*/
d13 4
a16 1
 * 2. The name of the author may not be used to endorse or promote products
d129 1
a129 1
 * is when we calculate how far in the future `to' will timeout -
@


1.2
log
@Preserve the FIFO order of issued timeouts.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_timeout.c,v 1.1 2000/03/23 09:59:57 art Exp $	*/
d103 1
a103 1
	to->to_flags = 0;
d240 1
a240 1
	to->to_flags = TIMEOUT_STATIC;
@


1.1
log
@New API for timeouts. Replaces the old timeout()/untimeout() API and
makes it the callers responsibility to allocate resources for the
timeouts.

This is a KISS implementation and does _not_ solve the problems of slow
handling of a large number of pending timeouts (this will be solved in
future work) (although hardclock is now guarateed to take constant time
for handling of timeouts).

Old timeout() and untimeout() are implemented as wrappers around the new
API and kept for compatibility. They will be removed as soon as all
subsystems are converted to use the new API.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d143 1
a143 1
		if (to->to_time - ticks >= to_ticks) {
@

