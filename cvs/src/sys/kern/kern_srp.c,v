head	1.12;
access;
symbols
	OPENBSD_6_2:1.12.0.4
	OPENBSD_6_2_BASE:1.12
	OPENBSD_6_1:1.11.0.4
	OPENBSD_6_1_BASE:1.11
	OPENBSD_6_0:1.10.0.2
	OPENBSD_6_0_BASE:1.10
	OPENBSD_5_9:1.7.0.2
	OPENBSD_5_9_BASE:1.7
	OPENBSD_5_8:1.1.0.4
	OPENBSD_5_8_BASE:1.1;
locks; strict;
comment	@ * @;


1.12
date	2017.09.08.05.36.53;	author deraadt;	state Exp;
branches;
next	1.11;
commitid	uRv5pa9QDlZaYgwD;

1.11
date	2016.09.15.02.00.16;	author dlg;	state Exp;
branches;
next	1.10;
commitid	RlO92XR575sygHqm;

1.10
date	2016.06.01.03.34.32;	author dlg;	state Exp;
branches;
next	1.9;
commitid	PIzWjVZVk1r0yDGW;

1.9
date	2016.05.18.03.58.13;	author dlg;	state Exp;
branches;
next	1.8;
commitid	ti6TjacQuFYBXsPy;

1.8
date	2016.05.18.03.46.03;	author dlg;	state Exp;
branches;
next	1.7;
commitid	q5zkugIMulsP5tHa;

1.7
date	2015.11.23.10.56.19;	author mpi;	state Exp;
branches;
next	1.6;
commitid	cBdJiiWQBFMz5dcw;

1.6
date	2015.09.11.20.21.01;	author dlg;	state Exp;
branches;
next	1.5;
commitid	ugYzuA98NNh9wMyO;

1.5
date	2015.09.11.19.22.37;	author dlg;	state Exp;
branches;
next	1.4;
commitid	MvBLHlwTZChYgi41;

1.4
date	2015.09.11.14.08.03;	author dlg;	state Exp;
branches;
next	1.3;
commitid	0BW56hM8LRMZLYgB;

1.3
date	2015.09.09.11.21.51;	author dlg;	state Exp;
branches;
next	1.2;
commitid	ro0JXFZlprcZI1rg;

1.2
date	2015.09.01.03.47.58;	author dlg;	state Exp;
branches;
next	1.1;
commitid	l5lNCKsqAztANagW;

1.1
date	2015.07.02.01.34.00;	author dlg;	state Exp;
branches;
next	;
commitid	HBmwORlhlW47BLMN;


desc
@@


1.12
log
@If you use sys/param.h, you don't need sys/types.h
@
text
@/*	$OpenBSD: kern_srp.c,v 1.11 2016/09/15 02:00:16 dlg Exp $ */

/*
 * Copyright (c) 2014 Jonathan Matthew <jmatthew@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/timeout.h>
#include <sys/srp.h>
#include <sys/atomic.h>

void	srp_v_gc_start(struct srp_gc *, struct srp *, void *);

void
srpl_rc_init(struct srpl_rc *rc,  void (*ref)(void *, void *),
    void (*unref)(void *, void *), void *cookie)
{
	rc->srpl_ref = ref;
	srp_gc_init(&rc->srpl_gc, unref, cookie);
}

void
srp_gc_init(struct srp_gc *srp_gc, void (*dtor)(void *, void *), void *cookie)
{
	srp_gc->srp_gc_dtor = dtor;
	srp_gc->srp_gc_cookie = cookie;
	refcnt_init(&srp_gc->srp_gc_refcnt);
}

void
srp_init(struct srp *srp)
{
	srp->ref = NULL;
}

void *
srp_swap_locked(struct srp *srp, void *nv)
{
	void *ov;

	/*
	 * this doesn't have to be as careful as the caller has already
	 * prevented concurrent updates, eg. by holding the kernel lock.
	 * can't be mixed with non-locked updates though.
	 */

	ov = srp->ref;
	srp->ref = nv;

	return (ov);
}

void
srp_update_locked(struct srp_gc *srp_gc, struct srp *srp, void *v)
{
	if (v != NULL)
		refcnt_take(&srp_gc->srp_gc_refcnt);

	v = srp_swap_locked(srp, v);

	if (v != NULL)
		srp_v_gc_start(srp_gc, srp, v);
}

void *
srp_get_locked(struct srp *srp)
{
	return (srp->ref);
}

void
srp_gc_finalize(struct srp_gc *srp_gc)
{
	refcnt_finalize(&srp_gc->srp_gc_refcnt, "srpfini");
}

#ifdef MULTIPROCESSOR
#include <machine/cpu.h>
#include <sys/pool.h>

struct srp_gc_ctx {
	struct srp_gc		*srp_gc;
	struct timeout		tick;
	struct srp_hazard	hzrd;
};

int	srp_v_referenced(struct srp *, void *);
void	srp_v_gc(void *);

struct pool srp_gc_ctx_pool;

void
srp_startup(void)
{
	pool_init(&srp_gc_ctx_pool, sizeof(struct srp_gc_ctx), 0,
	    IPL_SOFTCLOCK, PR_WAITOK, "srpgc", NULL);
}

int
srp_v_referenced(struct srp *srp, void *v)
{
	struct cpu_info *ci;
	CPU_INFO_ITERATOR cii;
	u_int i;
	struct srp_hazard *hzrd;

	CPU_INFO_FOREACH(cii, ci) {
		for (i = 0; i < nitems(ci->ci_srp_hazards); i++) {
			hzrd = &ci->ci_srp_hazards[i];

			if (hzrd->sh_p != srp)
				continue;
			membar_consumer();
			if (hzrd->sh_v != v)
				continue;

			return (1);
		}
	}

	return (0);
}

void
srp_v_dtor(struct srp_gc *srp_gc, void *v)
{
	(*srp_gc->srp_gc_dtor)(srp_gc->srp_gc_cookie, v);

	refcnt_rele_wake(&srp_gc->srp_gc_refcnt);
}

void
srp_v_gc_start(struct srp_gc *srp_gc, struct srp *srp, void *v)
{
	struct srp_gc_ctx *ctx;

	if (!srp_v_referenced(srp, v)) {
		/* we win */
		srp_v_dtor(srp_gc, v);
		return;
	}

	/* in use, try later */

	ctx = pool_get(&srp_gc_ctx_pool, PR_WAITOK);
	ctx->srp_gc = srp_gc;
	ctx->hzrd.sh_p = srp;
	ctx->hzrd.sh_v = v;

	timeout_set(&ctx->tick, srp_v_gc, ctx);
	timeout_add(&ctx->tick, 1);
}

void
srp_v_gc(void *x)
{
	struct srp_gc_ctx *ctx = x;

	if (srp_v_referenced(ctx->hzrd.sh_p, ctx->hzrd.sh_v)) {
		/* oh well, try again later */
		timeout_add(&ctx->tick, 1);
		return;
	}

	srp_v_dtor(ctx->srp_gc, ctx->hzrd.sh_v);
	pool_put(&srp_gc_ctx_pool, ctx);
}

void *
srp_swap(struct srp *srp, void *v)
{
	return (atomic_swap_ptr(&srp->ref, v));
}

void
srp_update(struct srp_gc *srp_gc, struct srp *srp, void *v)
{
	if (v != NULL)
		refcnt_take(&srp_gc->srp_gc_refcnt);

	v = srp_swap(srp, v);
	if (v != NULL)
		srp_v_gc_start(srp_gc, srp, v);
}

static inline void *
srp_v(struct srp_hazard *hzrd, struct srp *srp)
{
	void *v;

	hzrd->sh_p = srp;

	/*
	 * ensure we update this cpu's hazard pointer to a value that's still
	 * current after the store finishes, otherwise the gc task may already
	 * be destroying it
	 */
	do {
		v = srp->ref;
		hzrd->sh_v = v;
		membar_consumer();
	} while (__predict_false(v != srp->ref));

	return (v);
}

void *
srp_enter(struct srp_ref *sr, struct srp *srp)
{
	struct cpu_info *ci = curcpu();
	struct srp_hazard *hzrd;
	u_int i;

	for (i = 0; i < nitems(ci->ci_srp_hazards); i++) {
		hzrd = &ci->ci_srp_hazards[i];
		if (hzrd->sh_p == NULL) {
			sr->hz = hzrd;
			return (srp_v(hzrd, srp));
		}
	}

	panic("%s: not enough srp hazard records", __func__);

	/* NOTREACHED */
	return (NULL);
}

void *
srp_follow(struct srp_ref *sr, struct srp *srp)
{
	return (srp_v(sr->hz, srp));
}

void
srp_leave(struct srp_ref *sr)
{
	sr->hz->sh_p = NULL;
}

static inline int
srp_referenced(void *v)
{
	struct cpu_info *ci;
	CPU_INFO_ITERATOR cii;
	u_int i;
	struct srp_hazard *hzrd;

	CPU_INFO_FOREACH(cii, ci) {
		for (i = 0; i < nitems(ci->ci_srp_hazards); i++) {
			hzrd = &ci->ci_srp_hazards[i];

			if (hzrd->sh_p != NULL && hzrd->sh_v == v)
				return (1);
		}
	}

	return (0);
}

void
srp_finalize(void *v, const char *wmesg)
{
	while (srp_referenced(v))
		tsleep(v, PWAIT, wmesg, 1);
}

#else /* MULTIPROCESSOR */

void
srp_startup(void)
{

}

void
srp_v_gc_start(struct srp_gc *srp_gc, struct srp *srp, void *v)
{
	(*srp_gc->srp_gc_dtor)(srp_gc->srp_gc_cookie, v);
	refcnt_rele_wake(&srp_gc->srp_gc_refcnt);
}

#endif /* MULTIPROCESSOR */
@


1.11
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_srp.c,v 1.10 2016/06/01 03:34:32 dlg Exp $ */
a19 1
#include <sys/types.h>
@


1.10
log
@add support for using SRPs without the garbage collection machinery.

the gc machinery may sleep during srp_update, which makes it hard
to use from an interrupt context. srp_swap simply swaps the references
in an srp and relies ont he caller to schedule work in a process
context where it may sleep with srp_finalise until the reference
is no longer in use.

our network stack currently modifies routing tables in an interrupt
context, so this is built to be used to support rtable updates in
our current stack while supporting concurrent lookups.

ok jmatthew@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_srp.c,v 1.9 2016/05/18 03:58:13 dlg Exp $ */
d109 2
a110 5
	pool_init(&srp_gc_ctx_pool, sizeof(struct srp_gc_ctx), 0, 0,
	    PR_WAITOK, "srpgc", NULL);

	/* items are allocated in a process, but freed from a timeout */
	pool_setipl(&srp_gc_ctx_pool, IPL_SOFTCLOCK);
@


1.9
log
@rename srp_finalize to srp_gc_finalize
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_srp.c,v 1.8 2016/05/18 03:46:03 dlg Exp $ */
d50 2
a51 2
void
srp_update_locked(struct srp_gc *srp_gc, struct srp *srp, void *nv)
a54 3
	if (nv != NULL)
		refcnt_take(&srp_gc->srp_gc_refcnt);

d63 14
a76 2
	if (ov != NULL)
		srp_v_gc_start(srp_gc, srp, ov);
d186 6
d198 1
a198 1
	v = atomic_swap_ptr(&srp->ref, v);
d255 27
@


1.8
log
@rework the srp api so it takes an srp_ref struct that the caller provides.

the srp_ref struct is used to track the location of the callers
hazard pointer so later calls to srp_follow and srp_enter already
know what to clear. this in turn means most of the caveats around
using srps go away. specifically, you can now:

- switch cpus while holding an srp ref
  - ie, you can sleep while holding an srp ref
- you can take and release srp refs in any order

the original intent was to simplify use of the api when dealing
with complicated data structures. the caller now no longer has to
track the location of the srp a value was fetched from, the srp_ref
effectively does that for you.

srp lists have been refactored to use srp_refs instead of srpl_iter
structs.

this is in preparation of using srps inside the ART code. ART is a
complicated data structure, and lookups require overlapping holds
of srp references.

ok mpi@@ jmatthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_srp.c,v 1.7 2015/11/23 10:56:19 mpi Exp $ */
d77 1
a77 1
srp_finalize(struct srp_gc *srp_gc)
@


1.7
log
@Do not include <sys/atomic.h> inside <sys/refcnt.h>.

Prevent lazy developers, like David and I, to use atomic operations
without including <sys/atomic.h>.

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_srp.c,v 1.6 2015/09/11 20:21:01 dlg Exp $ */
d210 1
a210 1
srp_enter(struct srp *srp)
d218 2
a219 1
		if (hzrd->sh_p == NULL)
d221 1
d231 1
a231 1
srp_follow(struct srp *srp, void *v, struct srp *next)
d233 1
a233 13
	struct cpu_info *ci = curcpu();
	struct srp_hazard *hzrd;

	hzrd = ci->ci_srp_hazards + nitems(ci->ci_srp_hazards);
	while (hzrd-- != ci->ci_srp_hazards) {
		if (hzrd->sh_p == srp && hzrd->sh_v == v)
			return (srp_v(hzrd, next));
	}

	panic("%s: unexpected ref %p via %p", __func__, v, srp);

	/* NOTREACHED */
	return (NULL);
d237 1
a237 1
srp_leave(struct srp *srp, void *v)
d239 1
a239 12
	struct cpu_info *ci = curcpu();
	struct srp_hazard *hzrd;

	hzrd = ci->ci_srp_hazards + nitems(ci->ci_srp_hazards);
	while (hzrd-- != ci->ci_srp_hazards) {
		if (hzrd->sh_p == srp && hzrd->sh_v == v) {
			hzrd->sh_p = NULL;
			return;
		}
	}

	panic("%s: unexpected ref %p via %p", __func__, v, srp);
@


1.6
log
@unbreak build on UP kernels.

found by deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_srp.c,v 1.5 2015/09/11 19:22:37 dlg Exp $ */
d24 1
@


1.5
log
@make srp use refcnts so it can use refcnt_finalize instead of
sleep_setup/sleep_finish.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_srp.c,v 1.3 2015/09/09 11:21:51 dlg Exp $ */
d75 6
a186 6
void
srp_finalize(struct srp_gc *srp_gc)
{
	refcnt_finalize(&srp_gc->srp_gc_refcnt, "srpfini");
}

a270 8
srp_finalize(struct srp_gc *srp_gc)
{
	KASSERT(srp_gc->srp_gc_refcount == 1);

	srp_gc->srp_gc_refcount--;
}

void
d274 1
a274 1
	srp_gc->srp_gc_refcount--;
@


1.4
log
@remove some bits of srp.h i had pasted in here by accident
@
text
@d22 1
a22 3
#include <sys/proc.h>
#include <sys/atomic.h>

d40 1
a40 1
	srp_gc->srp_gc_refcount = 1;
d55 1
a55 1
		atomic_inc_int(&srp_gc->srp_gc_refcount);
d130 1
a130 2
	if (atomic_dec_int_nv(&srp_gc->srp_gc_refcount) == 0)
		wakeup_one(&srp_gc->srp_gc_refcount);
d174 1
a174 1
		atomic_inc_int(&srp_gc->srp_gc_refcount);
d184 1
a184 9
	struct sleep_state sls;
	u_int r;

	r = atomic_dec_int_nv(&srp_gc->srp_gc_refcount);
	while (r > 0) {
		sleep_setup(&sls, &srp_gc->srp_gc_refcount, PWAIT, "srpfini");
		r = srp_gc->srp_gc_refcount;
		sleep_finish(&sls, r);
	}
@


1.3
log
@implement a singly linked list built with SRPs.

this allows us to build lists of things that can be followed by
multiple cpus.

ok mpi@@ claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_srp.c,v 1.2 2015/09/01 03:47:58 dlg Exp $ */
a49 6

void            srpl_refs_init(struct srpl_rc *, void (*)(void *, void *),
                    void (*)(void *, void *), void *);

#define SRPL_RC_INITIALIZER(_r, _u, _c) { _r, SRP_GC_INITIALIZER(_u, _c) }

@


1.2
log
@mattieu baptiste reported a problem with bpf+srps where the per cpu
hazard pointers were becoming corrupt and therefore panics.

the problem turned out to be that bridge_input calls if_input on
behalf of a hardware interface which then calls bpf_mtap at splsoftnet,
while the actual hardware nic calls if_input and bpf_mtap at splnet.
the hardware interrupts ran in the middle of the bpf calls bridge
runs at softnet. this means the same srps are being entered and
left on the same cpu at different ipls, which led to races because
of the order of operations on the per cpu hazard pointers.

after a lot of experimentation, jmatthew@@ figured out how to deal
with this problem without introducing per cpu critical sections
(ie, splhigh) calls in srp_enter and srp_leave, and without introducing
atomic operations.

the solution is to iterate forward through the array of hazard
pointers in srp_enter, and backward in srp_leave to clear. if you
guarantee that you leave srps in the reverse order to entering them,
then you can use the same set of SRPs at different IPLs on the same
CPU.

the ordering requirement is a problem if we want to build linked
data structures out of srps because you need to hold a ref to the
current element containing the next srp to use it, before giving
up the current ref. we're adding srp_follow() to support taking the
next ref and giving up the current one while preserving the structure
of the hazard pointer list. srp_follow() does this by reusing the
hazard pointer for the current reference for the next ref.

both mattieu baptiste and jmatthew@@ have been hitting this pretty
hard with a tweaked version of srp+bpf that uses srp_follow instead
of interleaved srp_enter/srp_leave sequences. neither can reproduce
the panics anymore.

thanks to mattieu for the report and tests
ok jmatthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_srp.c,v 1.1 2015/07/02 01:34:00 dlg Exp $ */
d30 8
d50 6
@


1.1
log
@introduce srp, which according to the manpage i wrote is short for
"shared reference pointers".

srp allows concurrent access to a data structure by multiple cpus
while avoiding interlocking cpu opcodes. it manages its own reference
counts and the garbage collection of those data structure to avoid
use after frees.

internally srp is a twisted version of hazard pointers, which are
a relative of RCU.

jmatthew wrote the bulk of a hazard pointer implementation and
changed bpf to use it to allow mpsafe access to bpfilters. however,
at s2k15 we were trying to apply it to other data structures but
the memory overhead of every hazard pointer would have blown out
significantly in several uses cases. a bulk of our time at s2k15
was spent reworking hazard pointers into srp.

this diff adds the srp api and adds the necessary metadata to struct
cpuinfo on our MP architectures. srp on uniprocessor platforms has
alternate code that is optimised because it knows there'll be no
concurrent access to data by multiple cpus.

srp is made available to the system via param.h, so it should be
available everywhere in the kernel.

the docs likely need improvement cos im too close to the implementation.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
d190 2
a191 2
void *
srp_enter(struct srp *srp)
a192 2
	struct cpu_info *ci = curcpu();
	struct srp_hazard *hzrd;
a193 9
	u_int i;

	for (i = 0; i < nitems(ci->ci_srp_hazards); i++) {
		hzrd = &ci->ci_srp_hazards[i];
		if (hzrd->sh_p == NULL)
			break;
	}
	if (__predict_false(i == nitems(ci->ci_srp_hazards)))
		panic("%s: not enough srp hazard records", __func__);
a195 1
	membar_producer();
d211 2
a212 2
void
srp_leave(struct srp *srp, void *v)
d220 37
a256 1
		if (hzrd->sh_p == srp) {
a257 1
			hzrd->sh_v = NULL;
@

