head	1.24;
access;
symbols
	OPENBSD_6_2:1.24.0.2
	OPENBSD_6_2_BASE:1.24
	OPENBSD_6_1:1.24.0.4
	OPENBSD_6_1_BASE:1.24
	OPENBSD_6_0:1.21.0.4
	OPENBSD_6_0_BASE:1.21
	OPENBSD_5_9:1.21.0.2
	OPENBSD_5_9_BASE:1.21
	OPENBSD_5_8:1.20.0.4
	OPENBSD_5_8_BASE:1.20
	OPENBSD_5_7:1.19.0.2
	OPENBSD_5_7_BASE:1.19
	OPENBSD_5_6:1.17.0.12
	OPENBSD_5_6_BASE:1.17
	OPENBSD_5_5:1.17.0.10
	OPENBSD_5_5_BASE:1.17
	OPENBSD_5_4:1.17.0.6
	OPENBSD_5_4_BASE:1.17
	OPENBSD_5_3:1.17.0.4
	OPENBSD_5_3_BASE:1.17
	OPENBSD_5_2:1.17.0.2
	OPENBSD_5_2_BASE:1.17
	OPENBSD_5_1_BASE:1.16
	OPENBSD_5_1:1.16.0.8
	OPENBSD_5_0:1.16.0.6
	OPENBSD_5_0_BASE:1.16
	OPENBSD_4_9:1.16.0.4
	OPENBSD_4_9_BASE:1.16
	OPENBSD_4_8:1.16.0.2
	OPENBSD_4_8_BASE:1.16
	OPENBSD_4_7:1.15.0.2
	OPENBSD_4_7_BASE:1.15
	OPENBSD_4_6:1.15.0.4
	OPENBSD_4_6_BASE:1.15
	OPENBSD_4_5:1.13.0.2
	OPENBSD_4_5_BASE:1.13
	OPENBSD_4_4:1.12.0.12
	OPENBSD_4_4_BASE:1.12
	OPENBSD_4_3:1.12.0.10
	OPENBSD_4_3_BASE:1.12
	OPENBSD_4_2:1.12.0.8
	OPENBSD_4_2_BASE:1.12
	OPENBSD_4_1:1.12.0.6
	OPENBSD_4_1_BASE:1.12
	OPENBSD_4_0:1.12.0.4
	OPENBSD_4_0_BASE:1.12
	OPENBSD_3_9:1.12.0.2
	OPENBSD_3_9_BASE:1.12
	OPENBSD_3_8:1.11.0.4
	OPENBSD_3_8_BASE:1.11
	OPENBSD_3_7:1.11.0.2
	OPENBSD_3_7_BASE:1.11
	OPENBSD_3_6:1.9.0.2
	OPENBSD_3_6_BASE:1.9
	SMP_SYNC_A:1.9
	SMP_SYNC_B:1.9
	OPENBSD_3_5:1.8.0.2
	OPENBSD_3_5_BASE:1.8
	OPENBSD_3_4:1.7.0.2
	OPENBSD_3_4_BASE:1.7
	UBC_SYNC_A:1.5
	OPENBSD_3_3:1.5.0.10
	OPENBSD_3_3_BASE:1.5
	OPENBSD_3_2:1.5.0.8
	OPENBSD_3_2_BASE:1.5
	OPENBSD_3_1:1.5.0.6
	OPENBSD_3_1_BASE:1.5
	UBC_SYNC_B:1.5
	UBC:1.5.0.4
	UBC_BASE:1.5
	OPENBSD_3_0:1.5.0.2
	OPENBSD_3_0_BASE:1.5
	OPENBSD_2_9_BASE:1.2
	OPENBSD_2_9:1.2.0.22
	OPENBSD_2_8:1.2.0.20
	OPENBSD_2_8_BASE:1.2
	OPENBSD_2_7:1.2.0.18
	OPENBSD_2_7_BASE:1.2
	SMP:1.2.0.16
	SMP_BASE:1.2
	kame_19991208:1.2
	OPENBSD_2_6:1.2.0.14
	OPENBSD_2_6_BASE:1.2
	OPENBSD_2_5:1.2.0.12
	OPENBSD_2_5_BASE:1.2
	OPENBSD_2_4:1.2.0.10
	OPENBSD_2_4_BASE:1.2
	OPENBSD_2_3:1.2.0.8
	OPENBSD_2_3_BASE:1.2
	OPENBSD_2_2:1.2.0.6
	OPENBSD_2_2_BASE:1.2
	OPENBSD_2_1:1.2.0.4
	OPENBSD_2_1_BASE:1.2
	OPENBSD_2_0:1.2.0.2
	OPENBSD_2_0_BASE:1.2
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.24
date	2016.11.07.00.26.33;	author guenther;	state Exp;
branches;
next	1.23;
commitid	W7ztnDZwvjCaeQTS;

1.23
date	2016.09.15.02.00.16;	author dlg;	state Exp;
branches;
next	1.22;
commitid	RlO92XR575sygHqm;

1.22
date	2016.08.25.00.01.13;	author dlg;	state Exp;
branches;
next	1.21;
commitid	qGDK47LQhxLxADuT;

1.21
date	2015.12.22.21.39.34;	author mmcc;	state Exp;
branches;
next	1.20;
commitid	RjmBJYQIC8t1LJ1k;

1.20
date	2015.03.14.03.38.51;	author jsg;	state Exp;
branches;
next	1.19;
commitid	p4LJxGKbi0BU2cG6;

1.19
date	2014.12.19.05.59.21;	author tedu;	state Exp;
branches;
next	1.18;
commitid	zdJTCwdpqRUwO1SL;

1.18
date	2014.11.03.21.28.35;	author tedu;	state Exp;
branches;
next	1.17;
commitid	EkuwmBeHv2Tqmdnx;

1.17
date	2012.04.10.09.07.20;	author guenther;	state Exp;
branches;
next	1.16;

1.16
date	2010.06.29.03.14.42;	author tedu;	state Exp;
branches;
next	1.15;

1.15
date	2009.03.24.09.04.30;	author otto;	state Exp;
branches;
next	1.14;

1.14
date	2009.03.23.09.29.33;	author otto;	state Exp;
branches;
next	1.13;

1.13
date	2008.09.19.12.24.55;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2005.11.20.21.55.15;	author pedro;	state Exp;
branches;
next	1.11;

1.11
date	2005.03.15.03.47.58;	author tedu;	state Exp;
branches;
next	1.10;

1.10
date	2005.03.10.17.26.10;	author tedu;	state Exp;
branches;
next	1.9;

1.9
date	2004.04.13.00.15.28;	author tedu;	state Exp;
branches;
next	1.8;

1.8
date	2004.01.14.19.34.05;	author grange;	state Exp;
branches;
next	1.7;

1.7
date	2003.07.21.22.44.50;	author tedu;	state Exp;
branches;
next	1.6;

1.6
date	2003.06.02.23.28.07;	author millert;	state Exp;
branches;
next	1.5;

1.5
date	2001.09.05.19.22.23;	author deraadt;	state Exp;
branches;
next	1.4;

1.4
date	2001.07.28.17.03.50;	author gluk;	state Exp;
branches;
next	1.3;

1.3
date	2001.06.20.23.23.14;	author gluk;	state Exp;
branches;
next	1.2;

1.2
date	96.03.03.17.20.26;	author niklas;	state Exp;
branches
	1.2.16.1;
next	1.1;

1.1
date	95.10.18.08.52.47;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.52.47;	author deraadt;	state Exp;
branches;
next	;

1.2.16.1
date	2001.07.04.10.48.51;	author niklas;	state Exp;
branches;
next	1.2.16.2;

1.2.16.2
date	2001.10.31.03.26.29;	author nate;	state Exp;
branches;
next	1.2.16.3;

1.2.16.3
date	2003.06.07.11.03.41;	author ho;	state Exp;
branches;
next	1.2.16.4;

1.2.16.4
date	2004.02.19.10.56.39;	author niklas;	state Exp;
branches;
next	1.2.16.5;

1.2.16.5
date	2004.06.05.23.13.03;	author niklas;	state Exp;
branches;
next	;


desc
@@


1.24
log
@Split PID from TID, giving processes a PID unrelated to the TID of their
initial thread

ok jsing@@ kettenis@@
@
text
@/*	$OpenBSD: vfs_lockf.c,v 1.23 2016/09/15 02:00:16 dlg Exp $	*/
/*	$NetBSD: vfs_lockf.c,v 1.7 1996/02/04 02:18:21 christos Exp $	*/

/*
 * Copyright (c) 1982, 1986, 1989, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * Scooter Morris at Genentech Inc.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)ufs_lockf.c	8.3 (Berkeley) 1/6/94
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/kernel.h>
#include <sys/proc.h>
#include <sys/vnode.h>
#include <sys/pool.h>
#include <sys/fcntl.h>
#include <sys/lockf.h>
#include <sys/unistd.h>

struct pool lockfpool;

/*
 * This variable controls the maximum number of processes that will
 * be checked in doing deadlock detection.
 */
int maxlockdepth = MAXDEPTH;

#define SELF	0x1
#define OTHERS	0x2

#ifdef LOCKF_DEBUG

#define	DEBUG_SETLOCK		0x01
#define	DEBUG_CLEARLOCK		0x02
#define	DEBUG_GETLOCK		0x04
#define	DEBUG_FINDOVR		0x08
#define	DEBUG_SPLIT		0x10
#define	DEBUG_WAKELOCK		0x20

int	lockf_debug = DEBUG_SETLOCK|DEBUG_CLEARLOCK|DEBUG_WAKELOCK;

#define	DPRINTF(args, level)	if (lockf_debug & (level)) printf args
#else
#define	DPRINTF(args, level)
#endif

void
lf_init(void)
{
	pool_init(&lockfpool, sizeof(struct lockf), 0, IPL_NONE, PR_WAITOK,
	    "lockfpl", NULL);
}

struct lockf *lf_alloc(uid_t, int);
void lf_free(struct lockf *);

/*
 * We enforce a limit on locks by uid, so that a single user cannot
 * run the kernel out of memory.  For now, the limit is pretty coarse.
 * There is no limit on root.
 *
 * Splitting a lock will always succeed, regardless of current allocations.
 * If you're slightly above the limit, we still have to permit an allocation
 * so that the unlock can succeed.  If the unlocking causes too many splits,
 * however, you're totally cutoff.
 */
int maxlocksperuid = 1024;

/*
 * 3 options for allowfail.
 * 0 - always allocate.  1 - cutoff at limit.  2 - cutoff at double limit.
 */
struct lockf *
lf_alloc(uid_t uid, int allowfail)
{
	struct uidinfo *uip;
	struct lockf *lock;

	uip = uid_find(uid);
	if (uid && allowfail && uip->ui_lockcnt >
	    (allowfail == 1 ? maxlocksperuid : (maxlocksperuid * 2)))
		return (NULL);
	uip->ui_lockcnt++;
	lock = pool_get(&lockfpool, PR_WAITOK);
	lock->lf_uid = uid;
	return (lock);
}

void
lf_free(struct lockf *lock)
{
	struct uidinfo *uip;

	uip = uid_find(lock->lf_uid);
	uip->ui_lockcnt--;
	pool_put(&lockfpool, lock);
}


/*
 * Do an advisory lock operation.
 */
int
lf_advlock(struct lockf **head, off_t size, caddr_t id, int op,
    struct flock *fl, int flags)
{
	struct proc *p = curproc;
	struct lockf *lock;
	off_t start, end;
	int error;

	/*
	 * Convert the flock structure into a start and end.
	 */
	switch (fl->l_whence) {
	case SEEK_SET:
	case SEEK_CUR:
		/*
		 * Caller is responsible for adding any necessary offset
		 * when SEEK_CUR is used.
		 */
		start = fl->l_start;
		break;
	case SEEK_END:
		start = size + fl->l_start;
		break;
	default:
		return (EINVAL);
	}
	if (start < 0)
		return (EINVAL);
	if (fl->l_len == 0) {
		end = -1;
	} else {
		end = start + fl->l_len - 1;
		if (end < start)
			return (EINVAL);
	}

	/*
	 * Avoid the common case of unlocking when inode has no locks.
	 */
	if (*head == NULL) {
		if (op != F_SETLK) {
			fl->l_type = F_UNLCK;
			return (0);
		}
	}

	lock = lf_alloc(p->p_ucred->cr_uid, op == F_SETLK ? 1 : 2);
	if (!lock)
		return (ENOLCK);
	lock->lf_start = start;
	lock->lf_end = end;
	lock->lf_id = id;
	lock->lf_head = head;
	lock->lf_type = fl->l_type;
	lock->lf_next = NULL;
	TAILQ_INIT(&lock->lf_blkhd);
	lock->lf_flags = flags;
	lock->lf_pid = (flags & F_POSIX) ? p->p_p->ps_pid : -1;

	switch (op) {
	case F_SETLK:
		return (lf_setlock(lock));
	case F_UNLCK:
		error = lf_clearlock(lock);
		lf_free(lock);
		return (error);
	case F_GETLK:
		error = lf_getlock(lock, fl);
		lf_free(lock);
		return (error);
	default:
		lf_free(lock);
		return (EINVAL);
	}
	/* NOTREACHED */
}

/*
 * Set a byte-range lock.
 */
int
lf_setlock(struct lockf *lock)
{
	struct lockf *block;
	struct lockf **head = lock->lf_head;
	struct lockf **prev, *overlap, *ltmp;
	static char lockstr[] = "lockf";
	int ovcase, priority, needtolink, error;

#ifdef LOCKF_DEBUG
	if (lockf_debug & DEBUG_SETLOCK)
		lf_print("lf_setlock", lock);
#endif /* LOCKF_DEBUG */

	priority = PLOCK;
	if (lock->lf_type == F_WRLCK)
		priority += 4;
	priority |= PCATCH;
	/*
	 * Scan lock list for this file looking for locks that would block us.
	 */
	while ((block = lf_getblock(lock)) != NULL) {
		if ((lock->lf_flags & F_WAIT) == 0) {
			lf_free(lock);
			return (EAGAIN);
		}
		/*
		 * We are blocked. Since flock style locks cover
		 * the whole file, there is no chance for deadlock.
		 * For byte-range locks we must check for deadlock.
		 *
		 * Deadlock detection is done by looking through the
		 * wait channels to see if there are any cycles that
		 * involve us. MAXDEPTH is set just to make sure we
		 * do not go off into neverland.
		 */
		if ((lock->lf_flags & F_POSIX) &&
		    (block->lf_flags & F_POSIX)) {
			struct proc *wproc;
			struct lockf *waitblock;
			int i = 0;

			/* The block is waiting on something */
			wproc = (struct proc *)block->lf_id;
			while (wproc->p_wchan &&
			    (wproc->p_wmesg == lockstr) &&
			    (i++ < maxlockdepth)) {
				waitblock = (struct lockf *)wproc->p_wchan;
				/* Get the owner of the blocking lock */
				waitblock = waitblock->lf_next;
				if ((waitblock->lf_flags & F_POSIX) == 0)
					break;
				wproc = (struct proc *)waitblock->lf_id;
				if (wproc == (struct proc *)lock->lf_id) {
					lf_free(lock);
					return (EDEADLK);
				}
			}
		}
		/*
		 * For flock type locks, we must first remove
		 * any shared locks that we hold before we sleep
		 * waiting for an exclusive lock.
		 */
		if ((lock->lf_flags & F_FLOCK) && lock->lf_type == F_WRLCK) {
			lock->lf_type = F_UNLCK;
			(void)lf_clearlock(lock);
			lock->lf_type = F_WRLCK;
		}
		/*
		 * Add our lock to the blocked list and sleep until we're free.
		 * Remember who blocked us (for deadlock detection).
		 */
		lock->lf_next = block;
#ifdef LOCKF_DEBUG
		if (lockf_debug & DEBUG_SETLOCK) {
			lf_print("lf_setlock", lock);
			lf_print("lf_setlock: blocking on", block);
		}
#endif /* LOCKF_DEBUG */
		TAILQ_INSERT_TAIL(&block->lf_blkhd, lock, lf_block);
		error = tsleep(lock, priority, lockstr, 0);
		if (lock->lf_next != NULL) {
			TAILQ_REMOVE(&lock->lf_next->lf_blkhd, lock, lf_block);
			lock->lf_next = NULL;
		}
		if (error) {
			lf_free(lock);
			return (error);
		}
	}
	/*
	 * No blocks!!  Add the lock.  Note that we will
	 * downgrade or upgrade any overlapping locks this
	 * process already owns.
	 *
	 * Skip over locks owned by other processes.
	 * Handle any locks that overlap and are owned by ourselves.
	 */
	prev = head;
	block = *head;
	needtolink = 1;
	for (;;) {
		ovcase = lf_findoverlap(block, lock, SELF, &prev, &overlap);
		if (ovcase)
			block = overlap->lf_next;
		/*
		 * Six cases:
		 *	0) no overlap
		 *	1) overlap == lock
		 *	2) overlap contains lock
		 *	3) lock contains overlap
		 *	4) overlap starts before lock
		 *	5) overlap ends after lock
		 */
		switch (ovcase) {
		case 0: /* no overlap */
			if (needtolink) {
				*prev = lock;
				lock->lf_next = overlap;
			}
			break;
		case 1: /* overlap == lock */
			/*
			 * If downgrading lock, others may be
			 * able to acquire it.
			 */
			if (lock->lf_type == F_RDLCK &&
			    overlap->lf_type == F_WRLCK)
				lf_wakelock(overlap);
			overlap->lf_type = lock->lf_type;
			lf_free(lock);
			lock = overlap; /* for debug output below */
			break;
		case 2: /* overlap contains lock */
			/*
			 * Check for common starting point and different types.
			 */
			if (overlap->lf_type == lock->lf_type) {
				lf_free(lock);
				lock = overlap; /* for debug output below */
				break;
			}
			if (overlap->lf_start == lock->lf_start) {
				*prev = lock;
				lock->lf_next = overlap;
				overlap->lf_start = lock->lf_end + 1;
			} else
				lf_split(overlap, lock);
			lf_wakelock(overlap);
			break;
		case 3: /* lock contains overlap */
			/*
			 * If downgrading lock, others may be able to
			 * acquire it, otherwise take the list.
			 */
			if (lock->lf_type == F_RDLCK &&
			    overlap->lf_type == F_WRLCK) {
				lf_wakelock(overlap);
			} else {
				while ((ltmp =
				    TAILQ_FIRST(&overlap->lf_blkhd))) {
					TAILQ_REMOVE(&overlap->lf_blkhd, ltmp,
					    lf_block);
					ltmp->lf_next = lock;
					TAILQ_INSERT_TAIL(&lock->lf_blkhd,
					    ltmp, lf_block);
				}
			}
			/*
			 * Add the new lock if necessary and delete the overlap.
			 */
			if (needtolink) {
				*prev = lock;
				lock->lf_next = overlap->lf_next;
				prev = &lock->lf_next;
				needtolink = 0;
			} else
				*prev = overlap->lf_next;
			lf_free(overlap);
			continue;
		case 4: /* overlap starts before lock */
			/*
			 * Add lock after overlap on the list.
			 */
			lock->lf_next = overlap->lf_next;
			overlap->lf_next = lock;
			overlap->lf_end = lock->lf_start - 1;
			prev = &lock->lf_next;
			lf_wakelock(overlap);
			needtolink = 0;
			continue;
		case 5: /* overlap ends after lock */
			/*
			 * Add the new lock before overlap.
			 */
			if (needtolink) {
				*prev = lock;
				lock->lf_next = overlap;
			}
			overlap->lf_start = lock->lf_end + 1;
			lf_wakelock(overlap);
			break;
		}
		break;
	}
#ifdef LOCKF_DEBUG
	if (lockf_debug & DEBUG_SETLOCK) {
		lf_print("lf_setlock: got the lock", lock);
	}
#endif /* LOCKF_DEBUG */
	return (0);
}

/*
 * Remove a byte-range lock on an inode.
 *
 * Generally, find the lock (or an overlap to that lock)
 * and remove it (or shrink it), then wakeup anyone we can.
 */
int
lf_clearlock(struct lockf *lock)
{
	struct lockf **head = lock->lf_head;
	struct lockf *lf = *head;
	struct lockf *overlap, **prev;
	int ovcase;

	if (lf == NULL)
		return (0);
#ifdef LOCKF_DEBUG
	if (lockf_debug & DEBUG_CLEARLOCK)
		lf_print("lf_clearlock", lock);
#endif /* LOCKF_DEBUG */
	prev = head;
	while ((ovcase = lf_findoverlap(lf, lock, SELF, &prev, &overlap))) {
		lf_wakelock(overlap);

		switch (ovcase) {
		case 1: /* overlap == lock */
			*prev = overlap->lf_next;
			lf_free(overlap);
			break;
		case 2: /* overlap contains lock: split it */
			if (overlap->lf_start == lock->lf_start) {
				overlap->lf_start = lock->lf_end + 1;
				break;
			}
			lf_split(overlap, lock);
			overlap->lf_next = lock->lf_next;
			break;
		case 3: /* lock contains overlap */
			*prev = overlap->lf_next;
			lf = overlap->lf_next;
			lf_free(overlap);			
			continue;
		case 4: /* overlap starts before lock */
			overlap->lf_end = lock->lf_start - 1;
			prev = &overlap->lf_next;
			lf = overlap->lf_next;
			continue;
		case 5: /* overlap ends after lock */
			overlap->lf_start = lock->lf_end + 1;
			break;
		}
		break;
	}
	return (0);
}

/*
 * Check whether there is a blocking lock,
 * and if so return its process identifier.
 */
int
lf_getlock(struct lockf *lock, struct flock *fl)
{
	struct lockf *block;

#ifdef LOCKF_DEBUG
	if (lockf_debug & DEBUG_CLEARLOCK)
		lf_print("lf_getlock", lock);
#endif /* LOCKF_DEBUG */

	if ((block = lf_getblock(lock)) != NULL) {
		fl->l_type = block->lf_type;
		fl->l_whence = SEEK_SET;
		fl->l_start = block->lf_start;
		if (block->lf_end == -1)
			fl->l_len = 0;
		else
			fl->l_len = block->lf_end - block->lf_start + 1;
		fl->l_pid = block->lf_pid;
	} else {
		fl->l_type = F_UNLCK;
	}
	return (0);
}

/*
 * Walk the list of locks for an inode and
 * return the first blocking lock.
 */
struct lockf *
lf_getblock(struct lockf *lock)
{
	struct lockf **prev, *overlap, *lf;

	prev = lock->lf_head;
	lf = *prev;
	while (lf_findoverlap(lf, lock, OTHERS, &prev, &overlap) != 0) {
		/*
		 * We've found an overlap, see if it blocks us
		 */
		if ((lock->lf_type == F_WRLCK || overlap->lf_type == F_WRLCK))
			return (overlap);
		/*
		 * Nope, point to the next one on the list and
		 * see if it blocks us
		 */
		lf = overlap->lf_next;
	}
	return (NULL);
}

/*
 * Walk the list of locks for an inode to
 * find an overlapping lock (if any).
 *
 * NOTE: this returns only the FIRST overlapping lock.  There
 *	 may be more than one.
 */
int
lf_findoverlap(struct lockf *lf, struct lockf *lock, int type,
    struct lockf ***prev, struct lockf **overlap)
{
	off_t start, end;

#ifdef LOCKF_DEBUG
	if (lf && lockf_debug & DEBUG_FINDOVR)
		lf_print("lf_findoverlap: looking for overlap in", lock);
#endif /* LOCKF_DEBUG */

	*overlap = lf;
	start = lock->lf_start;
	end = lock->lf_end;
	while (lf != NULL) {
		if (((type & SELF) && lf->lf_id != lock->lf_id) ||
		    ((type & OTHERS) && lf->lf_id == lock->lf_id)) {
			*prev = &lf->lf_next;
			*overlap = lf = lf->lf_next;
			continue;
		}
#ifdef LOCKF_DEBUG
		if (lockf_debug & DEBUG_FINDOVR)
			lf_print("\tchecking", lf);
#endif /* LOCKF_DEBUG */
		/*
		 * OK, check for overlap
		 *
		 * Six cases:
		 *	0) no overlap
		 *	1) overlap == lock
		 *	2) overlap contains lock
		 *	3) lock contains overlap
		 *	4) overlap starts before lock
		 *	5) overlap ends after lock
		 */

		/* Case 0 */
		if ((lf->lf_end != -1 && start > lf->lf_end) ||
		    (end != -1 && lf->lf_start > end)) {
			DPRINTF(("no overlap\n"), DEBUG_FINDOVR);
			if ((type & SELF) && end != -1 && lf->lf_start > end)
				return (0);
			*prev = &lf->lf_next;
			*overlap = lf = lf->lf_next;
			continue;
		}
		/* Case 1 */
		if ((lf->lf_start == start) && (lf->lf_end == end)) {
			DPRINTF(("overlap == lock\n"), DEBUG_FINDOVR);
			return (1);
		}
		/* Case 2 */
		if ((lf->lf_start <= start) &&
		    (lf->lf_end == -1 || (end != -1 && lf->lf_end >= end))) {
			DPRINTF(("overlap contains lock\n"), DEBUG_FINDOVR);
			return (2);
		}
		/* Case 3 */
		if (start <= lf->lf_start &&
		    (end == -1 || (lf->lf_end != -1 && end >= lf->lf_end))) {
			DPRINTF(("lock contains overlap\n"), DEBUG_FINDOVR);
			return (3);
		}
		/* Case 4 */
		if ((lf->lf_start < start) &&
		    ((lf->lf_end >= start) || (lf->lf_end == -1))) {
			DPRINTF(("overlap starts before lock\n"),
			    DEBUG_FINDOVR);
			return (4);
		}
		/* Case 5 */
		if ((lf->lf_start > start) && (end != -1) &&
		    ((lf->lf_end > end) || (lf->lf_end == -1))) {
			DPRINTF(("overlap ends after lock\n"), DEBUG_FINDOVR);
			return (5);
		}
		panic("lf_findoverlap: default");
	}
	return (0);
}

/*
 * Split a lock and a contained region into
 * two or three locks as necessary.
 */
void
lf_split(struct lockf *lock1, struct lockf *lock2)
{
	struct lockf *splitlock;

#ifdef LOCKF_DEBUG
	if (lockf_debug & DEBUG_SPLIT) {
		lf_print("lf_split", lock1);
		lf_print("splitting from", lock2);
	}
#endif /* LOCKF_DEBUG */
	/*
	 * Check to see if splitting into only two pieces.
	 */
	if (lock1->lf_start == lock2->lf_start) {
		lock1->lf_start = lock2->lf_end + 1;
		lock2->lf_next = lock1;
		return;
	}
	if (lock1->lf_end == lock2->lf_end) {
		lock1->lf_end = lock2->lf_start - 1;
		lock2->lf_next = lock1->lf_next;
		lock1->lf_next = lock2;
		return;
	}
	/*
	 * Make a new lock consisting of the last part of
	 * the encompassing lock
	 */
	splitlock = lf_alloc(lock1->lf_uid, 0);
	memcpy(splitlock, lock1, sizeof(*splitlock));
	splitlock->lf_start = lock2->lf_end + 1;
	splitlock->lf_block.tqe_next = NULL;
	TAILQ_INIT(&splitlock->lf_blkhd);
	lock1->lf_end = lock2->lf_start - 1;

	lock2->lf_next = splitlock;
	lock1->lf_next = lock2;
}

/*
 * Wakeup a blocklist
 */
void
lf_wakelock(struct lockf *lock)
{
	struct lockf *wakelock;

	while ((wakelock = TAILQ_FIRST(&lock->lf_blkhd))) {
		TAILQ_REMOVE(&lock->lf_blkhd, wakelock, lf_block);
		wakelock->lf_next = NULL;
		wakeup_one(wakelock);
	}
}

#ifdef LOCKF_DEBUG
/*
 * Print out a lock.
 */
void
lf_print(char *tag, struct lockf *lock)
{
	struct lockf	*block;
	
	printf("%s: lock %p for ", tag, lock);
	if (lock->lf_flags & F_POSIX)
		printf("thread %d", ((struct proc *)(lock->lf_id))->p_tid);
	else
		printf("id %p", lock->lf_id);
	printf(" %s, start %llx, end %llx",
		lock->lf_type == F_RDLCK ? "shared" :
		lock->lf_type == F_WRLCK ? "exclusive" :
		lock->lf_type == F_UNLCK ? "unlock" :
		"unknown", lock->lf_start, lock->lf_end);
	block = TAILQ_FIRST(&lock->lf_blkhd);
	if (block)
		printf(" block");
	TAILQ_FOREACH(block, &lock->lf_blkhd, lf_block)
		printf(" %p,", block);
	printf("\n");

}

void
lf_printlist(char *tag, struct lockf *lock)
{
	struct lockf *lf;

	printf("%s: Lock list:\n", tag);
	for (lf = *lock->lf_head; lf; lf = lf->lf_next) {
		printf("\tlock %p for ", lf);
		if (lf->lf_flags & F_POSIX)
			printf("thread %d", ((struct proc*)(lf->lf_id))->p_tid);
		else
			printf("id %p", lf->lf_id);
		printf(" %s, start %llx, end %llx",
			lf->lf_type == F_RDLCK ? "shared" :
			lf->lf_type == F_WRLCK ? "exclusive" :
			lf->lf_type == F_UNLCK ? "unlock" :
			"unknown", lf->lf_start, lf->lf_end);
		printf("\n");
	}
}
#endif /* LOCKF_DEBUG */
@


1.23
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.22 2016/08/25 00:01:13 dlg Exp $	*/
d696 1
a696 1
		printf("proc %d", ((struct proc *)(lock->lf_id))->p_pid);
d722 1
a722 1
			printf("proc %d", ((struct proc*)(lf->lf_id))->p_pid);
@


1.22
log
@pool_setipl

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.21 2015/12/22 21:39:34 mmcc Exp $	*/
d78 1
a78 1
	pool_init(&lockfpool, sizeof(struct lockf), 0, 0, PR_WAITOK,
a79 1
	pool_setipl(&lockfpool, IPL_NONE);
@


1.21
log
@spliting -> splitting
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.20 2015/03/14 03:38:51 jsg Exp $	*/
d80 1
@


1.20
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.19 2014/12/19 05:59:21 tedu Exp $	*/
d642 1
a642 1
	 * Check to see if spliting into only two pieces.
@


1.19
log
@start retiring the nointr allocator. specify PR_WAITOK as a flag as a
marker for which pools are not interrupt safe. ok dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.18 2014/11/03 21:28:35 tedu Exp $	*/
a40 1
#include <sys/file.h>
@


1.18
log
@include sys/unistd.h where needed instead of indirect reliance. ok jsg
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.17 2012/04/10 09:07:20 guenther Exp $	*/
d79 2
a80 2
	pool_init(&lockfpool, sizeof(struct lockf), 0, 0, 0,
	    "lockfpl", &pool_allocator_nointr);
@


1.17
log
@POSIX locks should track the process's pid and not the thread's id
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.16 2010/06/29 03:14:42 tedu Exp $	*/
d47 1
@


1.16
log
@correct some minor style violations
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.15 2009/03/24 09:04:30 otto Exp $	*/
d189 1
a189 1
	lock->lf_pid = (flags & F_POSIX) ? curproc->p_pid : -1;
@


1.15
log
@only apply the strict limit on F_SETLK; ok deraadt@@ fgsch@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.14 2009/03/23 09:29:33 otto Exp $	*/
a143 1

a151 1

a154 1

d160 1
a160 1
	if (fl->l_len == 0)
d162 1
a162 1
	else {
a177 3
	/*
	 * Create the lockf structure.
	 */
d190 1
a190 3
	/*
	 * Do the requested operation.
	 */
a191 1

a193 1

a197 1

a201 1

a225 3
	/*
	 * Set the priority
	 */
a233 3
		/*
		 * Free the structure and return if nonblocking.
		 */
d276 1
a276 2
		if ((lock->lf_flags & F_FLOCK) &&
		    lock->lf_type == F_WRLCK) {
d278 1
a278 1
			(void) lf_clearlock(lock);
a293 10
#if 0
		if (error) {
			/*
			 * Delete ourselves from the waiting to lock list.
			 */
			TAILQ_REMOVE(&lock->lf_next->lf_blkhd, lock, lf_block);
			lf_free(lock);
			return (error);
		}
#else
a301 1
#endif
a333 1

a345 1

a362 1

a392 1

a403 1

d447 1
a447 5
	while ((ovcase = lf_findoverlap(lf, lock, SELF,
					&prev, &overlap)) != 0) {
		/*
		 * Wakeup the list of locks to be retried.
		 */
a450 1

a454 1

a462 1

a467 1

a472 1

d598 1
a598 2
		    (lf->lf_end == -1 ||
		    (end != -1 && lf->lf_end >= end))) {
d604 1
a604 2
		    (end == -1 ||
		    (lf->lf_end != -1 && end >= lf->lf_end))) {
d616 1
a616 2
		if ((lf->lf_start > start) &&
		    (end != -1) &&
d665 1
a665 3
	/*
	 * OK, now link it in
	 */
@


1.14
log
@advisory locks should return ENOLCK if we're out of locks.
ok deraadt@@ fgs@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.13 2008/09/19 12:24:55 art Exp $	*/
d184 1
a184 1
	lock = lf_alloc(p->p_ucred->cr_uid, op != F_UNLCK ? 1 : 2);
@


1.13
log
@Fix a bunch of problems and races with posix file locking.

- file descriptor table becomes the owner of the lock instead of the proc.
- When grabbing the lock, we check if the fd hasn't changed under our
  feet, this is more or less impossible to solve without a hack like
  this. I've banged my head against the wall, I figured out a solution,
  but implementing it correctly would cost me 12 gray hairs. Screw it,
  this is ugly, but it works.
- Wait until usecount drains before releasing the posix lock in closef.
- Add missing FREF/FRELE to sys_flock
- keep the pid in the flock struct instead of abusing the fact that we
  used to use the proc as the lock owner.

Pointed out by and discussed with Al Viro, big thanks.
miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.12 2005/11/20 21:55:15 pedro Exp $	*/
d186 1
a186 1
		return (ENOMEM);
@


1.12
log
@Use ANSI function declarations and deregister, no binary change
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.11 2005/03/15 03:47:58 tedu Exp $	*/
d195 1
d548 1
a548 4
		if (block->lf_flags & F_POSIX)
			fl->l_pid = ((struct proc *)(block->lf_id))->p_pid;
		else
			fl->l_pid = -1;
@


1.11
log
@revise lockf limit policy.  after exceeding "soft limit", start enforcing
a hard limit of 2x.  add a comment to this effect.  reviewed by miod
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.10 2005/03/10 17:26:10 tedu Exp $	*/
d132 2
a133 7
lf_advlock(head, size, id, op, fl, flags)
	struct lockf **head;
	off_t size;
	caddr_t id;
	int op;
	register struct flock *fl;
	int flags;
d136 1
a136 1
	register struct lockf *lock;
d224 1
a224 2
lf_setlock(lock)
	register struct lockf *lock;
d226 1
a226 1
	register struct lockf *block;
d267 2
a268 2
			register struct proc *wproc;
			register struct lockf *waitblock;
d467 1
a467 2
lf_clearlock(lock)
	register struct lockf *lock;
d470 1
a470 1
	register struct lockf *lf = *head;
d530 1
a530 3
lf_getlock(lock, fl)
	register struct lockf *lock;
	register struct flock *fl;
d532 1
a532 1
	register struct lockf *block;
d562 1
a562 2
lf_getblock(lock)
	register struct lockf *lock;
d591 2
a592 6
lf_findoverlap(lf, lock, type, prev, overlap)
	register struct lockf *lf;
	struct lockf *lock;
	int type;
	struct lockf ***prev;
	struct lockf **overlap;
d680 1
a680 3
lf_split(lock1, lock2)
	register struct lockf *lock1;
	register struct lockf *lock2;
d682 1
a682 1
	register struct lockf *splitlock;
d725 1
a725 2
lf_wakelock(lock)
	struct lockf *lock;
d741 1
a741 3
lf_print(tag, lock)
	char *tag;
	register struct lockf *lock;
d765 1
a765 3
lf_printlist(tag, lock)
	char *tag;
	struct lockf *lock;
d767 1
a767 1
	register struct lockf *lf;
@


1.10
log
@split out uidinfo from kern_proc.c private, use it to store lock count,
restrict lock count per uid to a global limit, add sysctl to adjust limit.
this prevents a user from creating too many locks.  problem noticed
by devon o'dell.  ok deraadt miod pedro
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.9 2004/04/13 00:15:28 tedu Exp $	*/
d89 5
d97 4
d108 2
a109 1
	if (uid && allowfail && uip->ui_lockcnt > maxlocksperuid)
d116 1
d189 1
a189 1
	lock = lf_alloc(p->p_ucred->cr_uid, op != F_UNLCK);
@


1.9
log
@useless caddr_t cast removal.  same md5s.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.8 2004/01/14 19:34:05 grange Exp $	*/
d82 35
d129 1
d178 3
a180 1
	lock = pool_get(&lockfpool, PR_WAITOK);
d199 1
a199 1
		pool_put(&lockfpool, lock);
d204 1
a204 1
		pool_put(&lockfpool, lock);
d208 1
a208 1
		pool_put(&lockfpool, lock);
d247 1
a247 1
			pool_put(&lockfpool, lock);
d278 1
a278 1
					pool_put(&lockfpool, lock);
d313 1
a313 1
			pool_put(&lockfpool, lock);
d322 1
a322 1
			pool_put(&lockfpool, lock);
d368 1
a368 1
			pool_put(&lockfpool, lock);
d377 1
a377 1
				pool_put(&lockfpool, lock);
d418 1
a418 1
			pool_put(&lockfpool, overlap);
d488 1
a488 1
			pool_put(&lockfpool, overlap);
d503 1
a503 1
			pool_put(&lockfpool, overlap);			
d713 1
a713 1
	splitlock = pool_get(&lockfpool, PR_WAITOK);
@


1.8
log
@Get rid of M_LOCKF and use pool for allocating lockf structures.
 From NetBSD.

Tested by many people, ok art@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.7 2003/07/21 22:44:50 tedu Exp $	*/
d676 1
a676 1
	memcpy(splitlock, lock1, sizeof (*splitlock));
@


1.7
log
@remove caddr_t casts.  it's just silly to cast something when the function
takes a void *.  convert uiomove to take a void * as well.  ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.6 2003/06/02 23:28:07 millert Exp $	*/
d44 1
a44 1
#include <sys/malloc.h>
d48 2
d75 7
d142 1
a142 1
	MALLOC(lock, struct lockf *, sizeof *lock, M_LOCKF, M_WAITOK);
d161 1
a161 1
		FREE(lock, M_LOCKF);
d166 1
a166 1
		FREE(lock, M_LOCKF);
d170 1
a170 1
		FREE(lock, M_LOCKF);
d209 1
a209 1
			FREE(lock, M_LOCKF);
d240 1
a240 1
					FREE(lock, M_LOCKF);
d275 1
a275 1
			FREE(lock, M_LOCKF);
d284 1
a284 1
			FREE(lock, M_LOCKF);
d330 1
a330 1
			FREE(lock, M_LOCKF);
d339 1
a339 1
				FREE(lock, M_LOCKF);
d380 1
a380 1
			FREE(overlap, M_LOCKF);
d450 1
a450 1
			FREE(overlap, M_LOCKF);
d465 1
a465 1
			FREE(overlap, M_LOCKF);
d675 1
a675 1
	MALLOC(splitlock, struct lockf *, sizeof *splitlock, M_LOCKF, M_WAITOK);
@


1.6
log
@Remove the advertising clause in the UCB license which Berkeley
rescinded 22 July 1999.  Proofed by myself and Theo.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.5 2001/09/05 19:22:23 deraadt Exp $	*/
d259 1
a259 1
		error = tsleep((caddr_t)lock, priority, lockstr, 0);
d667 1
a667 1
	memcpy((caddr_t)splitlock, (caddr_t)lock1, sizeof (*splitlock));
@


1.5
log
@use %ll instead of %q
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.4 2001/07/28 17:03:50 gluk Exp $	*/
d19 1
a19 5
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.4
log
@Check for negative lock length.
costa@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.3 2001/06/20 23:23:14 gluk Exp $	*/
d715 1
a715 1
	printf(" %s, start %qx, end %qx",
d743 1
a743 1
		printf(" %s, start %qx, end %qx",
@


1.3
log
@Fix PR1826. tsleep in lf_setlock can return 0 if process was ptrace'd,
but not wakeup'ed. In this case one entry can be placed twice at list of
blocked locks. As a result block list contain an entry which points to
itself. lf_wakelock can't remove such an entry and system livelocks
trying to remove a bad entry from block list.

Other changes include:
 - repair debug functions and make vfs_lockf.c compilable with LOCKF_DEBUG
 - simplify debug printing and remove useless debugging
 - remove unnecessary casting, replace NOLOCKF with NULL
 - free -> FREE (use one form over the file)
 - convert list of blocked locks to use TAILQ_* macroses
 - lf_addblock() -> TAILQ_INSERT_TAIL
 - Fix bug in lf_findoverlap(): in old code
   if (end == -1) && (lf->lf_end == -1) && (lf->lf_start <= start)
   then lf_findoverlap() return 4 instead of 2
 - work more carefully with pointers (probably fix one or two bugs)
 - use wakeup_one()
 - KNF
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.2 1996/03/03 17:20:26 niklas Exp $	*/
a93 9
	 * Avoid the common case of unlocking when inode has no locks.
	 */
	if (*head == NULL) {
		if (op != F_SETLK) {
			fl->l_type = F_UNLCK;
			return (0);
		}
	}
	/*
d118 1
a118 1
	else
d120 13
@


1.2
log
@From NetBSD: 960217 merge
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d58 3
d62 13
a74 1
int	lockf_debug = 0;
a76 4
#define NOLOCKF (struct lockf *)0
#define SELF	0x1
#define OTHERS	0x2

d96 1
a96 1
	if (*head == (struct lockf *)0) {
d129 1
d139 2
a140 2
	lock->lf_next = (struct lockf *)0;
	lock->lf_block = (struct lockf *)0;
d181 1
a181 1
	if (lockf_debug & 1)
d222 2
a223 2
			       (wproc->p_wmesg == lockstr) &&
			       (i++ < maxlockdepth)) {
d231 1
a231 1
					free(lock, M_LOCKF);
a251 1
		lf_addblock(block, lock);
d253 2
a254 1
		if (lockf_debug & 1) {
a255 1
			lf_printlist("lf_setlock", block);
d258 1
d260 1
d265 11
a275 16
			for (block = lock->lf_next;
			     block != NOLOCKF;
			     block = block->lf_block) {
				if (block->lf_block != lock)
					continue;
				block->lf_block = block->lf_block->lf_block;
				break;
			}
			/*
			 * If we did not find ourselves on the list, but
			 * are still linked onto a lock list, then something
			 * is very wrong.
			 */
			if (block == NOLOCKF && lock->lf_next != NOLOCKF)
				panic("lf_setlock: lost lock");
			free(lock, M_LOCKF);
d278 1
d330 1
a330 1
				free(lock, M_LOCKF);
d352 8
a359 3
				ltmp = lock->lf_block;
				lock->lf_block = overlap->lf_block;
				lf_addblock(lock, ltmp);
d371 1
a371 1
			free(overlap, M_LOCKF);
d401 1
a401 1
	if (lockf_debug & 1) {
a402 1
		lf_printlist("lf_setlock", lock);
d415 2
a416 2
lf_clearlock(unlock)
	register struct lockf *unlock;
d418 1
a418 1
	struct lockf **head = unlock->lf_head;
d423 1
a423 1
	if (lf == NOLOCKF)
d426 2
a427 4
	if (unlock->lf_type != F_UNLCK)
		panic("lf_clearlock: bad type");
	if (lockf_debug & 1)
		lf_print("lf_clearlock", unlock);
d430 1
a430 1
	while ((ovcase = lf_findoverlap(lf, unlock, SELF,
d445 2
a446 2
			if (overlap->lf_start == unlock->lf_start) {
				overlap->lf_start = unlock->lf_end + 1;
d449 2
a450 2
			lf_split(overlap, unlock);
			overlap->lf_next = unlock->lf_next;
d456 1
a456 1
			free(overlap, M_LOCKF);
d460 1
a460 1
			overlap->lf_end = unlock->lf_start - 1;
d466 1
a466 1
			overlap->lf_start = unlock->lf_end + 1;
a470 4
#ifdef LOCKF_DEBUG
	if (lockf_debug & 1)
		lf_printlist("lf_clearlock", unlock);
#endif /* LOCKF_DEBUG */
d486 1
a486 1
	if (lockf_debug & 1)
d516 1
a516 2
	struct lockf **prev, *overlap, *lf = *(lock->lf_head);
	int ovcase;
d519 2
a520 2
	while ((ovcase = lf_findoverlap(lf, lock, OTHERS,
					&prev, &overlap)) != 0) {
d532 1
a532 1
	return (NOLOCKF);
a551 3
	*overlap = lf;
	if (lf == NOLOCKF)
		return (0);
d553 1
a553 1
	if (lockf_debug & 2)
d556 2
d560 1
a560 1
	while (lf != NOLOCKF) {
d568 1
a568 1
		if (lockf_debug & 2)
d582 2
d586 1
a586 5
			/* Case 0 */
#ifdef LOCKF_DEBUG
			if (lockf_debug & 2)
				printf("no overlap\n");
#endif /* LOCKF_DEBUG */
d593 1
d595 1
a595 5
			/* Case 1 */
#ifdef LOCKF_DEBUG
			if (lockf_debug & 2)
				printf("overlap == lock\n");
#endif /* LOCKF_DEBUG */
d598 1
d600 3
a602 7
		    (end != -1) &&
		    ((lf->lf_end >= end) || (lf->lf_end == -1))) {
			/* Case 2 */
#ifdef LOCKF_DEBUG
			if (lockf_debug & 2)
				printf("overlap contains lock\n");
#endif /* LOCKF_DEBUG */
d605 1
d607 3
a609 7
		           (end == -1 ||
			   (lf->lf_end != -1 && end >= lf->lf_end))) {
			/* Case 3 */
#ifdef LOCKF_DEBUG
			if (lockf_debug & 2)
				printf("lock contains overlap\n");
#endif /* LOCKF_DEBUG */
d612 1
d614 3
a616 6
			((lf->lf_end >= start) || (lf->lf_end == -1))) {
			/* Case 4 */
#ifdef LOCKF_DEBUG
			if (lockf_debug & 2)
				printf("overlap starts before lock\n");
#endif /* LOCKF_DEBUG */
d619 1
d621 3
a623 7
			(end != -1) &&
			((lf->lf_end > end) || (lf->lf_end == -1))) {
			/* Case 5 */
#ifdef LOCKF_DEBUG
			if (lockf_debug & 2)
				printf("overlap ends after lock\n");
#endif /* LOCKF_DEBUG */
a631 28
 * Add a lock to the end of the blocked list.
 */
void
lf_addblock(lock, blocked)
	struct lockf *lock;
	struct lockf *blocked;
{
	register struct lockf *lf;

	if (blocked == NOLOCKF)
		return;
#ifdef LOCKF_DEBUG
	if (lockf_debug & 2) {
		lf_print("addblock: adding", blocked);
		lf_print("to blocked list of", lock);
	}
#endif /* LOCKF_DEBUG */
	if ((lf = lock->lf_block) == NOLOCKF) {
		lock->lf_block = blocked;
		return;
	}
	while (lf->lf_block != NOLOCKF)
		lf = lf->lf_block;
	lf->lf_block = blocked;
	return;
}

/*
d643 1
a643 1
	if (lockf_debug & 2) {
d667 1
a667 1
	bcopy((caddr_t)lock1, (caddr_t)splitlock, sizeof *splitlock);
d669 2
a670 1
	splitlock->lf_block = NOLOCKF;
a674 1
	splitlock->lf_next = lock1->lf_next;
d683 2
a684 2
lf_wakelock(listhead)
	struct lockf *listhead;
d686 1
a686 1
	register struct lockf *blocklist, *wakelock;
d688 4
a691 12
	blocklist = listhead->lf_block;
	listhead->lf_block = NOLOCKF;
	while (blocklist != NOLOCKF) {
		wakelock = blocklist;
		blocklist = blocklist->lf_block;
		wakelock->lf_block = NOLOCKF;
		wakelock->lf_next = NOLOCKF;
#ifdef LOCKF_DEBUG
		if (lockf_debug & 2)
			lf_print("lf_wakelock: awakening", wakelock);
#endif /* LOCKF_DEBUG */
		wakeup((caddr_t)wakelock);
d704 1
d710 2
a711 5
		printf("id 0x%x", lock->lf_id);
	printf(" in ino %d on dev <%d, %d>, %s, start %d, end %d",
		lock->lf_inode->i_number,
		major(lock->lf_inode->i_dev),
		minor(lock->lf_inode->i_dev),
d716 7
a722 4
	if (lock->lf_block)
		printf(" block %p\n", lock->lf_block);
	else
		printf("\n");
d732 2
a733 5
	printf("%s: Lock list for ino %d on dev <%d, %d>:\n",
		tag, lock->lf_inode->i_number,
		major(lock->lf_inode->i_dev),
		minor(lock->lf_inode->i_dev));
	for (lf = lock->lf_inode->i_lockf; lf; lf = lf->lf_next) {
d736 1
a736 1
			printf("proc %d", ((struct proc *)(lf->lf_id))->p_pid);
d738 2
a739 2
			printf("id 0x%x", lf->lf_id);
		printf(", %s, start %d, end %d",
d744 1
a744 4
		if (lf->lf_block)
			printf(" block %p\n", lf->lf_block);
		else
			printf("\n");
@


1.2.16.1
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.2 1996/03/03 17:20:26 niklas Exp $	*/
d58 5
a65 16
#ifdef LOCKF_DEBUG

#define	DEBUG_SETLOCK		0x01
#define	DEBUG_CLEARLOCK		0x02
#define	DEBUG_GETLOCK		0x04
#define	DEBUG_FINDOVR		0x08
#define	DEBUG_SPLIT		0x10
#define	DEBUG_WAKELOCK		0x20

int	lockf_debug = DEBUG_SETLOCK|DEBUG_CLEARLOCK|DEBUG_WAKELOCK;

#define	DPRINTF(args, level)	if (lockf_debug & (level)) printf args
#else
#define	DPRINTF(args, level)
#endif

d85 1
a85 1
	if (*head == NULL) {
a117 1

d127 2
a128 2
	lock->lf_next = NULL;
	TAILQ_INIT(&lock->lf_blkhd);
d169 1
a169 1
	if (lockf_debug & DEBUG_SETLOCK)
d210 2
a211 2
			    (wproc->p_wmesg == lockstr) &&
			    (i++ < maxlockdepth)) {
d219 1
a219 1
					FREE(lock, M_LOCKF);
d240 1
d242 1
a242 2
		if (lockf_debug & DEBUG_SETLOCK) {
			lf_print("lf_setlock", lock);
d244 1
a246 1
		TAILQ_INSERT_TAIL(&block->lf_blkhd, lock, lf_block);
a247 1
#if 0
d252 16
a267 2
			TAILQ_REMOVE(&lock->lf_next->lf_blkhd, lock, lf_block);
			FREE(lock, M_LOCKF);
a269 10
#else
		if (lock->lf_next != NULL) {
			TAILQ_REMOVE(&lock->lf_next->lf_blkhd, lock, lf_block);
			lock->lf_next = NULL;
		}
		if (error) {
			FREE(lock, M_LOCKF);
			return (error);
		}
#endif
d321 1
a321 1
				FREE(lock, M_LOCKF);
d343 3
a345 8
				while ((ltmp =
				    TAILQ_FIRST(&overlap->lf_blkhd))) {
					TAILQ_REMOVE(&overlap->lf_blkhd, ltmp,
					    lf_block);
					ltmp->lf_next = lock;
					TAILQ_INSERT_TAIL(&lock->lf_blkhd,
					    ltmp, lf_block);
				}
d357 1
a357 1
			FREE(overlap, M_LOCKF);
d387 1
a387 1
	if (lockf_debug & DEBUG_SETLOCK) {
d389 1
d402 2
a403 2
lf_clearlock(lock)
	register struct lockf *lock;
d405 1
a405 1
	struct lockf **head = lock->lf_head;
d410 1
a410 1
	if (lf == NULL)
d413 4
a416 2
	if (lockf_debug & DEBUG_CLEARLOCK)
		lf_print("lf_clearlock", lock);
d419 1
a419 1
	while ((ovcase = lf_findoverlap(lf, lock, SELF,
d434 2
a435 2
			if (overlap->lf_start == lock->lf_start) {
				overlap->lf_start = lock->lf_end + 1;
d438 2
a439 2
			lf_split(overlap, lock);
			overlap->lf_next = lock->lf_next;
d445 1
a445 1
			FREE(overlap, M_LOCKF);
d449 1
a449 1
			overlap->lf_end = lock->lf_start - 1;
d455 1
a455 1
			overlap->lf_start = lock->lf_end + 1;
d460 4
d479 1
a479 1
	if (lockf_debug & DEBUG_CLEARLOCK)
d509 2
a510 1
	struct lockf **prev, *overlap, *lf;
d513 2
a514 2
	lf = *prev;
	while (lf_findoverlap(lf, lock, OTHERS, &prev, &overlap) != 0) {
d526 1
a526 1
	return (NULL);
d546 3
d550 1
a550 1
	if (lf && lockf_debug & DEBUG_FINDOVR)
a552 2

	*overlap = lf;
d555 1
a555 1
	while (lf != NULL) {
d563 1
a563 1
		if (lockf_debug & DEBUG_FINDOVR)
a576 2

		/* Case 0 */
d579 5
a583 1
			DPRINTF(("no overlap\n"), DEBUG_FINDOVR);
a589 1
		/* Case 1 */
d591 5
a595 1
			DPRINTF(("overlap == lock\n"), DEBUG_FINDOVR);
a597 1
		/* Case 2 */
d599 7
a605 3
		    (lf->lf_end == -1 ||
		    (end != -1 && lf->lf_end >= end))) {
			DPRINTF(("overlap contains lock\n"), DEBUG_FINDOVR);
a607 1
		/* Case 3 */
d609 7
a615 3
		    (end == -1 ||
		    (lf->lf_end != -1 && end >= lf->lf_end))) {
			DPRINTF(("lock contains overlap\n"), DEBUG_FINDOVR);
a617 1
		/* Case 4 */
d619 6
a624 3
		    ((lf->lf_end >= start) || (lf->lf_end == -1))) {
			DPRINTF(("overlap starts before lock\n"),
			    DEBUG_FINDOVR);
a626 1
		/* Case 5 */
d628 7
a634 3
		    (end != -1) &&
		    ((lf->lf_end > end) || (lf->lf_end == -1))) {
			DPRINTF(("overlap ends after lock\n"), DEBUG_FINDOVR);
d643 28
d682 1
a682 1
	if (lockf_debug & DEBUG_SPLIT) {
d706 1
a706 1
	memcpy((caddr_t)splitlock, (caddr_t)lock1, sizeof (*splitlock));
d708 1
a708 2
	splitlock->lf_block.tqe_next = NULL;
	TAILQ_INIT(&splitlock->lf_blkhd);
d713 1
d722 2
a723 2
lf_wakelock(lock)
	struct lockf *lock;
d725 1
a725 1
	struct lockf *wakelock;
d727 12
a738 4
	while ((wakelock = TAILQ_FIRST(&lock->lf_blkhd))) {
		TAILQ_REMOVE(&lock->lf_blkhd, wakelock, lf_block);
		wakelock->lf_next = NULL;
		wakeup_one(wakelock);
a750 1
	struct lockf	*block;
d756 5
a760 2
		printf("id %p", lock->lf_id);
	printf(" %s, start %qx, end %qx",
d765 4
a768 7
	block = TAILQ_FIRST(&lock->lf_blkhd);
	if (block)
		printf(" block");
	TAILQ_FOREACH(block, &lock->lf_blkhd, lf_block)
		printf(" %p,", block);
	printf("\n");

d778 5
a782 2
	printf("%s: Lock list:\n", tag);
	for (lf = *lock->lf_head; lf; lf = lf->lf_next) {
d785 1
a785 1
			printf("proc %d", ((struct proc*)(lf->lf_id))->p_pid);
d787 2
a788 2
			printf("id %p", lf->lf_id);
		printf(" %s, start %qx, end %qx",
d793 4
a796 1
		printf("\n");
@


1.2.16.2
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.2.16.1 2001/07/04 10:48:51 niklas Exp $	*/
d94 9
d127 1
a127 1
	else {
a128 13
		if (end < start)
			return (EINVAL);
	}

	/*
	 * Avoid the common case of unlocking when inode has no locks.
	 */
	if (*head == NULL) {
		if (op != F_SETLK) {
			fl->l_type = F_UNLCK;
			return (0);
		}
	}
d711 1
a711 1
	printf(" %s, start %llx, end %llx",
d739 1
a739 1
		printf(" %s, start %llx, end %llx",
@


1.2.16.3
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_lockf.c,v 1.2.16.2 2001/10/31 03:26:29 nate Exp $	*/
d19 5
a23 1
 * 3. Neither the name of the University nor the names of its contributors
@


1.2.16.4
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d44 1
a44 1
#include <sys/pool.h>
a47 2
struct pool lockfpool;

a72 7
void
lf_init(void)
{
	pool_init(&lockfpool, sizeof(struct lockf), 0, 0, 0,
	    "lockfpl", &pool_allocator_nointr);
}

d133 1
a133 1
	lock = pool_get(&lockfpool, PR_WAITOK);
d152 1
a152 1
		pool_put(&lockfpool, lock);
d157 1
a157 1
		pool_put(&lockfpool, lock);
d161 1
a161 1
		pool_put(&lockfpool, lock);
d200 1
a200 1
			pool_put(&lockfpool, lock);
d231 1
a231 1
					pool_put(&lockfpool, lock);
d259 1
a259 1
		error = tsleep(lock, priority, lockstr, 0);
d266 1
a266 1
			pool_put(&lockfpool, lock);
d275 1
a275 1
			pool_put(&lockfpool, lock);
d321 1
a321 1
			pool_put(&lockfpool, lock);
d330 1
a330 1
				pool_put(&lockfpool, lock);
d371 1
a371 1
			pool_put(&lockfpool, overlap);
d441 1
a441 1
			pool_put(&lockfpool, overlap);
d456 1
a456 1
			pool_put(&lockfpool, overlap);			
d666 2
a667 2
	splitlock = pool_get(&lockfpool, PR_WAITOK);
	memcpy(splitlock, lock1, sizeof (*splitlock));
@


1.2.16.5
log
@Merge with the trunk
@
text
@d676 1
a676 1
	memcpy(splitlock, lock1, sizeof(*splitlock));
@


1.1
log
@Initial revision
@
text
@d1 2
a2 1
/*	$NetBSD: vfs_lockf.c,v 1.6 1995/03/19 23:45:03 mycroft Exp $	*/
d183 1
a183 1
	while (block = lf_getblock(lock)) {
d247 2
a248 1
		if (error = tsleep((caddr_t)lock, priority, lockstr, 0)) {
d283 2
a284 1
		if (ovcase = lf_findoverlap(block, lock, SELF, &prev, &overlap))
d419 2
a420 1
	while (ovcase = lf_findoverlap(lf, unlock, SELF, &prev, &overlap)) {
d483 1
a483 1
	if (block = lf_getblock(lock)) {
d513 2
a514 1
	while (ovcase = lf_findoverlap(lf, lock, OTHERS, &prev, &overlap)) {
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@
