head	1.45;
access;
symbols
	OPENBSD_6_2:1.45.0.6
	OPENBSD_6_2_BASE:1.45
	OPENBSD_6_1:1.45.0.4
	OPENBSD_6_1_BASE:1.45
	OPENBSD_6_0:1.43.0.2
	OPENBSD_6_0_BASE:1.43
	OPENBSD_5_9:1.41.0.2
	OPENBSD_5_9_BASE:1.41
	OPENBSD_5_8:1.36.0.4
	OPENBSD_5_8_BASE:1.36
	OPENBSD_5_7:1.35.0.2
	OPENBSD_5_7_BASE:1.35
	OPENBSD_5_6:1.34.0.4
	OPENBSD_5_6_BASE:1.34
	OPENBSD_5_5:1.31.0.4
	OPENBSD_5_5_BASE:1.31
	OPENBSD_5_4:1.30.0.2
	OPENBSD_5_4_BASE:1.30
	OPENBSD_5_3:1.27.0.4
	OPENBSD_5_3_BASE:1.27
	OPENBSD_5_2:1.27.0.2
	OPENBSD_5_2_BASE:1.27
	OPENBSD_5_1_BASE:1.24
	OPENBSD_5_1:1.24.0.2
	OPENBSD_5_0:1.23.0.2
	OPENBSD_5_0_BASE:1.23
	OPENBSD_4_9:1.22.0.4
	OPENBSD_4_9_BASE:1.22
	OPENBSD_4_8:1.22.0.2
	OPENBSD_4_8_BASE:1.22
	OPENBSD_4_7:1.17.0.2
	OPENBSD_4_7_BASE:1.17
	OPENBSD_4_6:1.13.0.4
	OPENBSD_4_6_BASE:1.13
	OPENBSD_4_5:1.8.0.2
	OPENBSD_4_5_BASE:1.8
	OPENBSD_4_4:1.6.0.2
	OPENBSD_4_4_BASE:1.6
	OPENBSD_4_3:1.2.0.2
	OPENBSD_4_3_BASE:1.2;
locks; strict;
comment	@ * @;


1.45
date	2017.02.12.04.55.08;	author guenther;	state Exp;
branches;
next	1.44;
commitid	1rXx7AiXIWFC9gYa;

1.44
date	2017.01.21.05.42.03;	author guenther;	state Exp;
branches;
next	1.43;
commitid	CHRb0fCqa8XxUAMH;

1.43
date	2016.06.03.15.21.23;	author kettenis;	state Exp;
branches;
next	1.42;
commitid	YZe6QofTSvwygUKE;

1.42
date	2016.03.17.13.18.47;	author mpi;	state Exp;
branches;
next	1.41;
commitid	YYXwHgvcvZhstSh4;

1.41
date	2015.12.23.14.51.17;	author kettenis;	state Exp;
branches;
next	1.40;
commitid	uKsuvaQ9n9dxqA2a;

1.40
date	2015.12.17.22.14.40;	author kettenis;	state Exp;
branches;
next	1.39;
commitid	xGlmrOSASwtlP204;

1.39
date	2015.10.16.19.07.24;	author mpi;	state Exp;
branches;
next	1.38;
commitid	D4wsJYXG7qgGpzUK;

1.38
date	2015.09.20.22.05.14;	author kettenis;	state Exp;
branches;
next	1.37;
commitid	p5GvWddfT4abUzm2;

1.37
date	2015.09.13.11.15.11;	author kettenis;	state Exp;
branches;
next	1.36;
commitid	FyAroUyNFmSVBNGF;

1.36
date	2015.03.14.03.38.50;	author jsg;	state Exp;
branches;
next	1.35;
commitid	p4LJxGKbi0BU2cG6;

1.35
date	2014.09.24.09.13.20;	author mpi;	state Exp;
branches;
next	1.34;
commitid	utkIKumzoTFKvrDM;

1.34
date	2014.07.26.16.07.39;	author kettenis;	state Exp;
branches;
next	1.33;
commitid	FvJfklSZDHB2oaxb;

1.33
date	2014.07.13.21.44.58;	author matthew;	state Exp;
branches;
next	1.32;
commitid	qOVqHdGKxG0eesdt;

1.32
date	2014.05.04.05.03.26;	author guenther;	state Exp;
branches;
next	1.31;

1.31
date	2014.02.12.05.47.36;	author guenther;	state Exp;
branches;
next	1.30;

1.30
date	2013.06.06.13.09.37;	author haesbaert;	state Exp;
branches;
next	1.29;

1.29
date	2013.06.03.16.55.22;	author guenther;	state Exp;
branches;
next	1.28;

1.28
date	2013.04.19.21.44.08;	author tedu;	state Exp;
branches;
next	1.27;

1.27
date	2012.07.10.18.20.37;	author kettenis;	state Exp;
branches;
next	1.26;

1.26
date	2012.03.23.15.51.26;	author guenther;	state Exp;
branches;
next	1.25;

1.25
date	2012.03.10.22.02.32;	author haesbaert;	state Exp;
branches;
next	1.24;

1.24
date	2011.10.12.18.30.09;	author miod;	state Exp;
branches;
next	1.23;

1.23
date	2011.07.06.21.41.37;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2010.05.28.14.23.37;	author guenther;	state Exp;
branches;
next	1.21;

1.21
date	2010.05.25.19.59.35;	author kettenis;	state Exp;
branches;
next	1.20;

1.20
date	2010.05.14.18.47.56;	author kettenis;	state Exp;
branches;
next	1.19;

1.19
date	2010.04.23.21.34.38;	author deraadt;	state Exp;
branches;
next	1.18;

1.18
date	2010.04.06.20.33.28;	author kettenis;	state Exp;
branches;
next	1.17;

1.17
date	2010.01.09.02.44.17;	author kettenis;	state Exp;
branches;
next	1.16;

1.16
date	2009.11.29.23.12.30;	author kettenis;	state Exp;
branches;
next	1.15;

1.15
date	2009.11.25.11.01.14;	author kettenis;	state Exp;
branches;
next	1.14;

1.14
date	2009.10.05.17.43.08;	author deraadt;	state Exp;
branches;
next	1.13;

1.13
date	2009.04.22.08.35.54;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2009.04.20.08.48.17;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2009.04.14.09.13.25;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2009.04.03.09.29.15;	author art;	state Exp;
branches;
next	1.9;

1.9
date	2009.03.23.13.25.11;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2008.11.06.19.49.13;	author deraadt;	state Exp;
branches;
next	1.7;

1.7
date	2008.11.06.19.19.04;	author deraadt;	state Exp;
branches;
next	1.6;

1.6
date	2008.06.12.06.58.39;	author deraadt;	state Exp;
branches;
next	1.5;

1.5
date	2008.06.11.12.35.46;	author deraadt;	state Exp;
branches;
next	1.4;

1.4
date	2008.06.10.20.14.36;	author beck;	state Exp;
branches;
next	1.3;

1.3
date	2008.06.08.20.13.13;	author thib;	state Exp;
branches;
next	1.2;

1.2
date	2007.11.26.17.15.29;	author art;	state Exp;
branches;
next	1.1;

1.1
date	2007.10.10.15.53.53;	author art;	state Exp;
branches;
next	;


desc
@@


1.45
log
@Split up fork1():
 - FORK_THREAD handling is a totally separate function, thread_fork(),
   that is only used by sys___tfork() and which loses the flags, func,
   arg, and newprocp parameters and gains tcb parameter to guarantee
   the new thread's TCB is set before the creating thread returns
 - fork1() loses its stack and tidptr parameters
Common bits factor out:
 - struct proc allocation and initialization moves to thread_new()
 - maxthread handling moves to fork_check_maxthread()
 - setting the new thread running moves to fork_thread_start()
The MD cpu_fork() function swaps its unused stacksize parameter for
a tcb parameter.

luna88k testing by aoyama@@, alpha testing by dlg@@
ok mpi@@
@
text
@/*	$OpenBSD: kern_sched.c,v 1.44 2017/01/21 05:42:03 guenther Exp $	*/
/*
 * Copyright (c) 2007, 2008 Artur Grabowski <art@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include <sys/param.h>

#include <sys/sched.h>
#include <sys/proc.h>
#include <sys/kthread.h>
#include <sys/systm.h>
#include <sys/resourcevar.h>
#include <sys/signalvar.h>
#include <sys/mutex.h>
#include <sys/task.h>

#include <uvm/uvm_extern.h>

void sched_kthreads_create(void *);

int sched_proc_to_cpu_cost(struct cpu_info *ci, struct proc *p);
struct proc *sched_steal_proc(struct cpu_info *);

/*
 * To help choosing which cpu should run which process we keep track
 * of cpus which are currently idle and which cpus have processes
 * queued.
 */
struct cpuset sched_idle_cpus;
struct cpuset sched_queued_cpus;
struct cpuset sched_all_cpus;

/*
 * Some general scheduler counters.
 */
uint64_t sched_nmigrations;	/* Cpu migration counter */
uint64_t sched_nomigrations;	/* Cpu no migration counter */
uint64_t sched_noidle;		/* Times we didn't pick the idle task */
uint64_t sched_stolen;		/* Times we stole proc from other cpus */
uint64_t sched_choose;		/* Times we chose a cpu */
uint64_t sched_wasidle;		/* Times we came out of idle */

#ifdef MULTIPROCESSOR
struct taskq *sbartq;
#endif

/*
 * A few notes about cpu_switchto that is implemented in MD code.
 *
 * cpu_switchto takes two arguments, the old proc and the proc
 * it should switch to. The new proc will never be NULL, so we always have
 * a saved state that we need to switch to. The old proc however can
 * be NULL if the process is exiting. NULL for the old proc simply
 * means "don't bother saving old state".
 *
 * cpu_switchto is supposed to atomically load the new state of the process
 * including the pcb, pmap and setting curproc, the p_cpu pointer in the
 * proc and p_stat to SONPROC. Atomically with respect to interrupts, other
 * cpus in the system must not depend on this state being consistent.
 * Therefore no locking is necessary in cpu_switchto other than blocking
 * interrupts during the context switch.
 */

/*
 * sched_init_cpu is called from main() for the boot cpu, then it's the
 * responsibility of the MD code to call it for all other cpus.
 */
void
sched_init_cpu(struct cpu_info *ci)
{
	struct schedstate_percpu *spc = &ci->ci_schedstate;
	int i;

	for (i = 0; i < SCHED_NQS; i++)
		TAILQ_INIT(&spc->spc_qs[i]);

	spc->spc_idleproc = NULL;

	kthread_create_deferred(sched_kthreads_create, ci);

	LIST_INIT(&spc->spc_deadproc);

	/*
	 * Slight hack here until the cpuset code handles cpu_info
	 * structures.
	 */
	cpuset_init_cpu(ci);
	cpuset_add(&sched_all_cpus, ci);
}

void
sched_kthreads_create(void *v)
{
	struct cpu_info *ci = v;
	struct schedstate_percpu *spc = &ci->ci_schedstate;
	static int num;

	if (fork1(&proc0, FORK_SHAREVM|FORK_SHAREFILES|FORK_NOZOMBIE|
	    FORK_SYSTEM|FORK_SIGHAND|FORK_IDLE, sched_idle, ci, NULL,
	    &spc->spc_idleproc))
		panic("fork idle");

	/* Name it as specified. */
	snprintf(spc->spc_idleproc->p_p->ps_comm,
	    sizeof(spc->spc_idleproc->p_p->ps_comm),
	    "idle%d", num);

	num++;
}

void
sched_idle(void *v)
{
	struct schedstate_percpu *spc;
	struct proc *p = curproc;
	struct cpu_info *ci = v;
	int s;

	KERNEL_UNLOCK();

	spc = &ci->ci_schedstate;

	/*
	 * First time we enter here, we're not supposed to idle,
	 * just go away for a while.
	 */
	SCHED_LOCK(s);
	cpuset_add(&sched_idle_cpus, ci);
	p->p_stat = SSLEEP;
	p->p_cpu = ci;
	atomic_setbits_int(&p->p_flag, P_CPUPEG);
	mi_switch();
	cpuset_del(&sched_idle_cpus, ci);
	SCHED_UNLOCK(s);

	KASSERT(ci == curcpu());
	KASSERT(curproc == spc->spc_idleproc);

	while (1) {
		while (!cpu_is_idle(curcpu())) {
			struct proc *dead;

			SCHED_LOCK(s);
			p->p_stat = SSLEEP;
			mi_switch();
			SCHED_UNLOCK(s);

			while ((dead = LIST_FIRST(&spc->spc_deadproc))) {
				LIST_REMOVE(dead, p_hash);
				exit2(dead);
			}
		}

		splassert(IPL_NONE);

		cpuset_add(&sched_idle_cpus, ci);
		cpu_idle_enter();
		while (spc->spc_whichqs == 0) {
#ifdef MULTIPROCESSOR
			if (spc->spc_schedflags & SPCF_SHOULDHALT &&
			    (spc->spc_schedflags & SPCF_HALTED) == 0) {
				cpuset_del(&sched_idle_cpus, ci);
				SCHED_LOCK(s);
				atomic_setbits_int(&spc->spc_schedflags,
				    spc->spc_whichqs ? 0 : SPCF_HALTED);
				SCHED_UNLOCK(s);
				wakeup(spc);
			}
#endif
			cpu_idle_cycle();
		}
		cpu_idle_leave();
		cpuset_del(&sched_idle_cpus, ci);
	}
}

/*
 * To free our address space we have to jump through a few hoops.
 * The freeing is done by the reaper, but until we have one reaper
 * per cpu, we have no way of putting this proc on the deadproc list
 * and waking up the reaper without risking having our address space and
 * stack torn from under us before we manage to switch to another proc.
 * Therefore we have a per-cpu list of dead processes where we put this
 * proc and have idle clean up that list and move it to the reaper list.
 * All this will be unnecessary once we can bind the reaper this cpu
 * and not risk having it switch to another in case it sleeps.
 */
void
sched_exit(struct proc *p)
{
	struct schedstate_percpu *spc = &curcpu()->ci_schedstate;
	struct timespec ts;
	struct proc *idle;
	int s;

	nanouptime(&ts);
	timespecsub(&ts, &spc->spc_runtime, &ts);
	timespecadd(&p->p_rtime, &ts, &p->p_rtime);

	LIST_INSERT_HEAD(&spc->spc_deadproc, p, p_hash);

	/* This process no longer needs to hold the kernel lock. */
	KERNEL_UNLOCK();

	SCHED_LOCK(s);
	idle = spc->spc_idleproc;
	idle->p_stat = SRUN;
	cpu_switchto(NULL, idle);
	panic("cpu_switchto returned");
}

/*
 * Run queue management.
 */
void
sched_init_runqueues(void)
{
#ifdef MULTIPROCESSOR
	sbartq = taskq_create("sbar", 1, IPL_NONE,
	    TASKQ_MPSAFE | TASKQ_CANTSLEEP);
	if (sbartq == NULL)
		panic("unable to create sbar taskq");
#endif
}

void
setrunqueue(struct proc *p)
{
	struct schedstate_percpu *spc;
	int queue = p->p_priority >> 2;

	SCHED_ASSERT_LOCKED();
	spc = &p->p_cpu->ci_schedstate;
	spc->spc_nrun++;

	TAILQ_INSERT_TAIL(&spc->spc_qs[queue], p, p_runq);
	spc->spc_whichqs |= (1 << queue);
	cpuset_add(&sched_queued_cpus, p->p_cpu);

	if (cpuset_isset(&sched_idle_cpus, p->p_cpu))
		cpu_unidle(p->p_cpu);
}

void
remrunqueue(struct proc *p)
{
	struct schedstate_percpu *spc;
	int queue = p->p_priority >> 2;

	SCHED_ASSERT_LOCKED();
	spc = &p->p_cpu->ci_schedstate;
	spc->spc_nrun--;

	TAILQ_REMOVE(&spc->spc_qs[queue], p, p_runq);
	if (TAILQ_EMPTY(&spc->spc_qs[queue])) {
		spc->spc_whichqs &= ~(1 << queue);
		if (spc->spc_whichqs == 0)
			cpuset_del(&sched_queued_cpus, p->p_cpu);
	}
}

struct proc *
sched_chooseproc(void)
{
	struct schedstate_percpu *spc = &curcpu()->ci_schedstate;
	struct proc *p;
	int queue;

	SCHED_ASSERT_LOCKED();

#ifdef MULTIPROCESSOR
	if (spc->spc_schedflags & SPCF_SHOULDHALT) {
		if (spc->spc_whichqs) {
			for (queue = 0; queue < SCHED_NQS; queue++) {
				while ((p = TAILQ_FIRST(&spc->spc_qs[queue]))) {
					remrunqueue(p);
					p->p_cpu = sched_choosecpu(p);
					setrunqueue(p);
					if (p->p_cpu == curcpu()) {
						KASSERT(p->p_flag & P_CPUPEG);
						goto again;
					}
				}
			}
		}
		p = spc->spc_idleproc;
		KASSERT(p);
		KASSERT(p->p_wchan == NULL);
		p->p_stat = SRUN;
		return (p);
	}
#endif

again:
	if (spc->spc_whichqs) {
		queue = ffs(spc->spc_whichqs) - 1;
		p = TAILQ_FIRST(&spc->spc_qs[queue]);
		remrunqueue(p);
		sched_noidle++;
		KASSERT(p->p_stat == SRUN);
	} else if ((p = sched_steal_proc(curcpu())) == NULL) {
		p = spc->spc_idleproc;
		if (p == NULL) {
                        int s;
			/*
			 * We get here if someone decides to switch during
			 * boot before forking kthreads, bleh.
			 * This is kind of like a stupid idle loop.
			 */
#ifdef MULTIPROCESSOR
			__mp_unlock(&sched_lock);
#endif
			spl0();
			delay(10);
			SCHED_LOCK(s);
			goto again;
                }
		KASSERT(p);
		p->p_stat = SRUN;
	} 

	KASSERT(p->p_wchan == NULL);
	return (p);	
}

struct cpu_info *
sched_choosecpu_fork(struct proc *parent, int flags)
{
#ifdef MULTIPROCESSOR
	struct cpu_info *choice = NULL;
	fixpt_t load, best_load = ~0;
	int run, best_run = INT_MAX;
	struct cpu_info *ci;
	struct cpuset set;

#if 0
	/*
	 * XXX
	 * Don't do this until we have a painless way to move the cpu in exec.
	 * Preferably when nuking the old pmap and getting a new one on a
	 * new cpu.
	 */
	/*
	 * PPWAIT forks are simple. We know that the parent will not
	 * run until we exec and choose another cpu, so we just steal its
	 * cpu.
	 */
	if (flags & FORK_PPWAIT)
		return (parent->p_cpu);
#endif

	/*
	 * Look at all cpus that are currently idle and have nothing queued.
	 * If there are none, pick the one with least queued procs first,
	 * then the one with lowest load average.
	 */
	cpuset_complement(&set, &sched_queued_cpus, &sched_idle_cpus);
	cpuset_intersection(&set, &set, &sched_all_cpus);
	if (cpuset_first(&set) == NULL)
		cpuset_copy(&set, &sched_all_cpus);

	while ((ci = cpuset_first(&set)) != NULL) {
		cpuset_del(&set, ci);

		load = ci->ci_schedstate.spc_ldavg;
		run = ci->ci_schedstate.spc_nrun;

		if (choice == NULL || run < best_run ||
		    (run == best_run &&load < best_load)) {
			choice = ci;
			best_load = load;
			best_run = run;
		}
	}

	return (choice);
#else
	return (curcpu());
#endif
}

struct cpu_info *
sched_choosecpu(struct proc *p)
{
#ifdef MULTIPROCESSOR
	struct cpu_info *choice = NULL;
	int last_cost = INT_MAX;
	struct cpu_info *ci;
	struct cpuset set;

	/*
	 * If pegged to a cpu, don't allow it to move.
	 */
	if (p->p_flag & P_CPUPEG)
		return (p->p_cpu);

	sched_choose++;

	/*
	 * Look at all cpus that are currently idle and have nothing queued.
	 * If there are none, pick the cheapest of those.
	 * (idle + queued could mean that the cpu is handling an interrupt
	 * at this moment and haven't had time to leave idle yet).
	 */
	cpuset_complement(&set, &sched_queued_cpus, &sched_idle_cpus);
	cpuset_intersection(&set, &set, &sched_all_cpus);

	/*
	 * First, just check if our current cpu is in that set, if it is,
	 * this is simple.
	 * Also, our cpu might not be idle, but if it's the current cpu
	 * and it has nothing else queued and we're curproc, take it.
	 */
	if (cpuset_isset(&set, p->p_cpu) ||
	    (p->p_cpu == curcpu() && p->p_cpu->ci_schedstate.spc_nrun == 0 &&
	    (p->p_cpu->ci_schedstate.spc_schedflags & SPCF_SHOULDHALT) == 0 &&
	    curproc == p)) {
		sched_wasidle++;
		return (p->p_cpu);
	}

	if (cpuset_first(&set) == NULL)
		cpuset_copy(&set, &sched_all_cpus);

	while ((ci = cpuset_first(&set)) != NULL) {
		int cost = sched_proc_to_cpu_cost(ci, p);

		if (choice == NULL || cost < last_cost) {
			choice = ci;
			last_cost = cost;
		}
		cpuset_del(&set, ci);
	}

	if (p->p_cpu != choice)
		sched_nmigrations++;
	else
		sched_nomigrations++;

	return (choice);
#else
	return (curcpu());
#endif
}

/*
 * Attempt to steal a proc from some cpu.
 */
struct proc *
sched_steal_proc(struct cpu_info *self)
{
	struct proc *best = NULL;
#ifdef MULTIPROCESSOR
	struct schedstate_percpu *spc;
	int bestcost = INT_MAX;
	struct cpu_info *ci;
	struct cpuset set;

	KASSERT((self->ci_schedstate.spc_schedflags & SPCF_SHOULDHALT) == 0);

	cpuset_copy(&set, &sched_queued_cpus);

	while ((ci = cpuset_first(&set)) != NULL) {
		struct proc *p;
		int queue;
		int cost;

		cpuset_del(&set, ci);

		spc = &ci->ci_schedstate;

		queue = ffs(spc->spc_whichqs) - 1;
		TAILQ_FOREACH(p, &spc->spc_qs[queue], p_runq) {
			if (p->p_flag & P_CPUPEG)
				continue;

			cost = sched_proc_to_cpu_cost(self, p);

			if (best == NULL || cost < bestcost) {
				best = p;
				bestcost = cost;
			}
		}
	}
	if (best == NULL)
		return (NULL);

	spc = &best->p_cpu->ci_schedstate;
	remrunqueue(best);
	best->p_cpu = self;

	sched_stolen++;
#endif
	return (best);
}

#ifdef MULTIPROCESSOR
/*
 * Base 2 logarithm of an int. returns 0 for 0 (yeye, I know).
 */
static int
log2(unsigned int i)
{
	int ret = 0;

	while (i >>= 1)
		ret++;

	return (ret);
}

/*
 * Calculate the cost of moving the proc to this cpu.
 * 
 * What we want is some guesstimate of how much "performance" it will
 * cost us to move the proc here. Not just for caches and TLBs and NUMA
 * memory, but also for the proc itself. A highly loaded cpu might not
 * be the best candidate for this proc since it won't get run.
 *
 * Just total guesstimates for now.
 */

int sched_cost_load = 1;
int sched_cost_priority = 1;
int sched_cost_runnable = 3;
int sched_cost_resident = 1;
#endif

int
sched_proc_to_cpu_cost(struct cpu_info *ci, struct proc *p)
{
	int cost = 0;
#ifdef MULTIPROCESSOR
	struct schedstate_percpu *spc;
	int l2resident = 0;

	spc = &ci->ci_schedstate;

	/*
	 * First, account for the priority of the proc we want to move.
	 * More willing to move, the lower the priority of the destination
	 * and the higher the priority of the proc.
	 */
	if (!cpuset_isset(&sched_idle_cpus, ci)) {
		cost += (p->p_priority - spc->spc_curpriority) *
		    sched_cost_priority;
		cost += sched_cost_runnable;
	}
	if (cpuset_isset(&sched_queued_cpus, ci))
		cost += spc->spc_nrun * sched_cost_runnable;

	/*
	 * Try to avoid the primary cpu as it handles hardware interrupts.
	 *
	 * XXX Needs to be revisited when we distribute interrupts
	 * over cpus.
	 */
	if (CPU_IS_PRIMARY(ci))
		cost += sched_cost_runnable;

	/*
	 * Higher load on the destination means we don't want to go there.
	 */
	cost += ((sched_cost_load * spc->spc_ldavg) >> FSHIFT);

	/*
	 * If the proc is on this cpu already, lower the cost by how much
	 * it has been running and an estimate of its footprint.
	 */
	if (p->p_cpu == ci && p->p_slptime == 0) {
		l2resident =
		    log2(pmap_resident_count(p->p_vmspace->vm_map.pmap));
		cost -= l2resident * sched_cost_resident;
	}
#endif
	return (cost);
}

/*
 * Peg a proc to a cpu.
 */
void
sched_peg_curproc(struct cpu_info *ci)
{
	struct proc *p = curproc;
	int s;

	SCHED_LOCK(s);
	p->p_priority = p->p_usrpri;
	p->p_stat = SRUN;
	p->p_cpu = ci;
	atomic_setbits_int(&p->p_flag, P_CPUPEG);
	setrunqueue(p);
	p->p_ru.ru_nvcsw++;
	mi_switch();
	SCHED_UNLOCK(s);
}

#ifdef MULTIPROCESSOR

void
sched_start_secondary_cpus(void)
{
	CPU_INFO_ITERATOR cii;
	struct cpu_info *ci;

	CPU_INFO_FOREACH(cii, ci) {
		struct schedstate_percpu *spc = &ci->ci_schedstate;

		if (CPU_IS_PRIMARY(ci))
			continue;
		cpuset_add(&sched_all_cpus, ci);
		atomic_clearbits_int(&spc->spc_schedflags,
		    SPCF_SHOULDHALT | SPCF_HALTED);
	}
}

void
sched_stop_secondary_cpus(void)
{
	CPU_INFO_ITERATOR cii;
	struct cpu_info *ci;

	/*
	 * Make sure we stop the secondary CPUs.
	 */
	CPU_INFO_FOREACH(cii, ci) {
		struct schedstate_percpu *spc = &ci->ci_schedstate;

		if (CPU_IS_PRIMARY(ci))
			continue;
		cpuset_del(&sched_all_cpus, ci);
		atomic_setbits_int(&spc->spc_schedflags, SPCF_SHOULDHALT);
	}
	CPU_INFO_FOREACH(cii, ci) {
		struct schedstate_percpu *spc = &ci->ci_schedstate;
		struct sleep_state sls;

		if (CPU_IS_PRIMARY(ci))
			continue;
		while ((spc->spc_schedflags & SPCF_HALTED) == 0) {
			sleep_setup(&sls, spc, PZERO, "schedstate");
			sleep_finish(&sls,
			    (spc->spc_schedflags & SPCF_HALTED) == 0);
		}
	}
}

void
sched_barrier_task(void *arg)
{
	struct cpu_info *ci = arg;

	sched_peg_curproc(ci);
	ci->ci_schedstate.spc_barrier = 1;
	wakeup(&ci->ci_schedstate.spc_barrier);
	atomic_clearbits_int(&curproc->p_flag, P_CPUPEG);
}

void
sched_barrier(struct cpu_info *ci)
{
	struct sleep_state sls;
	struct task task;
	CPU_INFO_ITERATOR cii;
	struct schedstate_percpu *spc;

	if (ci == NULL) {
		CPU_INFO_FOREACH(cii, ci) {
			if (CPU_IS_PRIMARY(ci))
				break;
		}
	}
	KASSERT(ci != NULL);

	if (ci == curcpu())
		return;

	task_set(&task, sched_barrier_task, ci);
	spc = &ci->ci_schedstate;
	spc->spc_barrier = 0;
	task_add(sbartq, &task);
	while (!spc->spc_barrier) {
		sleep_setup(&sls, &spc->spc_barrier, PWAIT, "sbar");
		sleep_finish(&sls, !spc->spc_barrier);
	}
}

#else

void
sched_barrier(struct cpu_info *ci)
{
}

#endif

/*
 * Functions to manipulate cpu sets.
 */
struct cpu_info *cpuset_infos[MAXCPUS];
static struct cpuset cpuset_all;

void
cpuset_init_cpu(struct cpu_info *ci)
{
	cpuset_add(&cpuset_all, ci);
	cpuset_infos[CPU_INFO_UNIT(ci)] = ci;
}

void
cpuset_clear(struct cpuset *cs)
{
	memset(cs, 0, sizeof(*cs));
}

void
cpuset_add(struct cpuset *cs, struct cpu_info *ci)
{
	unsigned int num = CPU_INFO_UNIT(ci);
	atomic_setbits_int(&cs->cs_set[num/32], (1 << (num % 32)));
}

void
cpuset_del(struct cpuset *cs, struct cpu_info *ci)
{
	unsigned int num = CPU_INFO_UNIT(ci);
	atomic_clearbits_int(&cs->cs_set[num/32], (1 << (num % 32)));
}

int
cpuset_isset(struct cpuset *cs, struct cpu_info *ci)
{
	unsigned int num = CPU_INFO_UNIT(ci);
	return (cs->cs_set[num/32] & (1 << (num % 32)));
}

void
cpuset_add_all(struct cpuset *cs)
{
	cpuset_copy(cs, &cpuset_all);
}

void
cpuset_copy(struct cpuset *to, struct cpuset *from)
{
	memcpy(to, from, sizeof(*to));
}

struct cpu_info *
cpuset_first(struct cpuset *cs)
{
	int i;

	for (i = 0; i < CPUSET_ASIZE(ncpus); i++)
		if (cs->cs_set[i])
			return (cpuset_infos[i * 32 + ffs(cs->cs_set[i]) - 1]);

	return (NULL);
}

void
cpuset_union(struct cpuset *to, struct cpuset *a, struct cpuset *b)
{
	int i;

	for (i = 0; i < CPUSET_ASIZE(ncpus); i++)
		to->cs_set[i] = a->cs_set[i] | b->cs_set[i];
}

void
cpuset_intersection(struct cpuset *to, struct cpuset *a, struct cpuset *b)
{
	int i;

	for (i = 0; i < CPUSET_ASIZE(ncpus); i++)
		to->cs_set[i] = a->cs_set[i] & b->cs_set[i];
}

void
cpuset_complement(struct cpuset *to, struct cpuset *a, struct cpuset *b)
{
	int i;

	for (i = 0; i < CPUSET_ASIZE(ncpus); i++)
		to->cs_set[i] = b->cs_set[i] & ~a->cs_set[i];
}
@


1.44
log
@p_comm is the process's command and isn't per thread, so move it from
struct proc to struct process.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.43 2016/06/03 15:21:23 kettenis Exp $	*/
d111 1
a111 1
	    FORK_SYSTEM|FORK_SIGHAND|FORK_IDLE, NULL, 0, sched_idle, ci, NULL,
@


1.43
log
@Allow pegged process on secondary CPUs to continue to be scheduled when
halting a CPU.  Necessary for intr_barrier(9) to work when interrupts are
targeted at secondary CPUs.

ok mpi@@, mikeb@@ (a while back)
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.42 2016/03/17 13:18:47 mpi Exp $	*/
d116 2
a117 1
	snprintf(spc->spc_idleproc->p_comm, sizeof(spc->spc_idleproc->p_comm),
@


1.42
log
@Replace curcpu_is_idle() by cpu_is_idle() and use it instead of rolling
our own.

From Michal Mazurek, ok mmcc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.41 2015/12/23 14:51:17 kettenis Exp $	*/
a288 1
					KASSERT(p->p_cpu != curcpu());
d290 4
@


1.41
log
@One "sbar" taskq is enough.

ok visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.40 2015/12/17 22:14:40 kettenis Exp $	*/
d151 1
a151 1
		while (!curcpu_is_idle()) {
@


1.40
log
@Make the cost of moving a process to the primary cpu a bit higher.  This is
the CPU that handles most hardware interrupts but we don't account for that
in any way in the scheduler.  So processes (and kernel threads) that are
unlucky enough to end up on this CPU will get less CPU cycles than those
running on other CPUs.  This is especially true for the softnet taskq.
There network interrupts will prevent the softnet taskq from running.  This
means that the more packets we receive, the less packets we can actually
process and/or forward.  This is why "unlocking" network drivers actually
decreases the forwarding performance.  This diff restores most of the lost
performance by making it less likely that the softnet taskq ends up on the
same CPU that handles network interrupts.

Tested by Hrvoje Popovski

ok mpi@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.39 2015/10/16 19:07:24 mpi Exp $	*/
a100 7

#ifdef MULTIPROCESSOR
	sbartq = taskq_create("sbar", 1, IPL_NONE,
	    TASKQ_MPSAFE | TASKQ_CANTSLEEP);
	if (sbartq == NULL)
		panic("unable to create sbar taskq");
#endif
d229 6
@


1.39
log
@Make sched_barrier() use its own task queue to avoid deadlocks.

Prevent a deadlock from occuring when intr_barrier() is called from
a non-primary CPU in the watchdog task, also enqueued on ``systq''.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.38 2015/09/20 22:05:14 kettenis Exp $	*/
d560 9
@


1.38
log
@Short circuit if we're running on the CPU that we want to sync with.  Fixes
suspend on machines with em(4) now that it uses intr_barrier(9).

ok krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.37 2015/09/13 11:15:11 kettenis Exp $	*/
d55 4
d101 7
d682 1
a682 1
	task_add(systq, &task);
@


1.37
log
@Introduce sched_barrier(9), an interface that acts as a scheduler barrier in
the sense that it guarantees that the specified CPU went through the
scheduler.  This also guarantees that interrupt handlers running on that CPU
will have finished when sched_barrier() returns.

ok miod@@, guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.36 2015/03/14 03:38:50 jsg Exp $	*/
d664 3
a666 1
	spc = &ci->ci_schedstate;
d669 1
@


1.36
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.35 2014/09/24 09:13:20 mpi Exp $	*/
d27 1
d636 44
@


1.35
log
@Keep under #ifdef MULTIPROCESSOR the code that deals with SPCF_SHOULDHALT
and SPCF_HALTED, these flags only make sense on secondary CPUs which are
unlikely to be present on a SP kernel.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.34 2014/07/26 16:07:39 kettenis Exp $	*/
a28 3

#include <sys/malloc.h>

@


1.34
log
@If we're stopping a secondary cpu, don't let sched_choosecpu() short-circuit
and return the current current CPU, otherwise sched_stop_secondary_cpus()
will spin forever trying to empty its run queues.  Fixes hangs during suspend
that many people reported over the last couple of days.

ok bcook@@, guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.33 2014/07/13 21:44:58 matthew Exp $	*/
d168 1
d178 1
d274 1
d292 1
@


1.33
log
@Fix sched_stop_secondary_cpus() to properly drain CPUs

TAILQ_FOREACH() isn't safe to use in sched_chooseproc() to iterate
over the run queues because within the loop body we remove the threads
from their run queues and reinsert them elsewhere.  As a result, we
end up only draining the first thread of each run queue rather than
all of them.

ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.32 2014/05/04 05:03:26 guenther Exp $	*/
d278 1
d412 1
@


1.32
log
@Add PS_SYSTEM, the process-level mirror of the thread-level P_SYSTEM,
and FORK_SYSTEM as a flag to set them.  This eliminates needing to
peek into other processes threads in various places.  Inspired by NetBSD

ok miod@@ matthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.31 2014/02/12 05:47:36 guenther Exp $	*/
d275 1
a275 1
				TAILQ_FOREACH(p, &spc->spc_qs[queue], p_runq) {
@


1.31
log
@Eliminate the exit sig handling, which was only invokable via the
Linux-compat clone() syscall when *not* using CLONE_THREAD.  pirofti@@
confirms Opera runs in compat without this, so out it goes; one less hair
to choke on in kern_exit.c

ok tedu@@ pirofti@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.30 2013/06/06 13:09:37 haesbaert Exp $	*/
d109 1
a109 1
	    FORK_SIGHAND|FORK_IDLE, NULL, 0, sched_idle, ci, NULL,
a111 5

	/*
	 * Mark it as a system process.
	 */
	atomic_setbits_int(&spc->spc_idleproc->p_flag, P_SYSTEM);
@


1.30
log
@Prevent idle thread from being stolen on startup.

There is a race condition which might trigger a case where two cpus try
to run the same idle thread.

The problem arises when one cpu steals the idle proc of another cpu and
this other cpu ends up running the idle thread via spc->spc_idleproc,
resulting in two cpus trying to cpu_switchto(idleX).

On startup, idle procs are scaterred around different runqueues, the
decision for scheduling is:

1 look at my runqueue.
2 if empty, look at other dudes runqueue.
3 if empty, select idle proc via spc->spc_idleproc.

The problem is that cpu0's idle0 might be running on cpu1 due to step 1
or 2 and cpu0 hits step 3.

So cpu0 will select idle0, while cpu1 is in fact running it already.

The solution is to never place idle on a runqueue, therefore being
only selectable through spc->spc_idleproc.

This race can be more easily triggered on a HT cpu on virtualized
environments, where the guest more often than not doesn't have the cpu
for itself, so timing gets shuffled.

ok tedu@@ guenther@@
go ahead after t2k13 deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.29 2013/06/03 16:55:22 guenther Exp $	*/
d108 1
a108 1
	if (fork1(&proc0, 0, FORK_SHAREVM|FORK_SHAREFILES|FORK_NOZOMBIE|
@


1.29
log
@Convert some internal APIs to use timespecs instead of timevals

ok matthew@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.28 2013/04/19 21:44:08 tedu Exp $	*/
d108 3
a110 1
	if (kthread_create(sched_idle, ci, &spc->spc_idleproc, "idle%d", num))
d112 9
@


1.28
log
@sprinkle ifdef MP to disable cpu migration code when not needed.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.27 2012/07/10 18:20:37 kettenis Exp $	*/
d193 1
a193 1
	struct timeval tv;
d197 3
a199 3
	microuptime(&tv);
	timersub(&tv, &spc->spc_runtime, &tv);
	timeradd(&p->p_rtime, &tv, &p->p_rtime);
@


1.27
log
@Make sure that we don't schedule processes on CPUs that we havetaken out of
the scheduler.

ok hasbaert@@. deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.26 2012/03/23 15:51:26 guenther Exp $	*/
d318 1
d366 3
d374 1
d429 3
d440 2
a442 1
	struct proc *best = NULL;
d481 1
a481 1

d485 1
d515 1
d520 2
a523 1
	int cost;
a526 2
	cost = 0;

d554 1
a554 1

@


1.26
log
@Make rusage totals, itimers, and profile settings per-process instead
of per-rthread.  Handling of per-thread tick and runtime counters
inspired by how FreeBSD does it.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.25 2012/03/10 22:02:32 haesbaert Exp $	*/
d346 1
d390 1
d437 2
@


1.25
log
@Account for sched_noidle and document the scheduler variables.

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.24 2011/10/12 18:30:09 miod Exp $	*/
d559 1
a559 1
	p->p_stats->p_ru.ru_nvcsw++;
@


1.24
log
@Remove all MD diagnostics in cpu_switchto(), and move them to MI code if
they apply.

ok oga@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.23 2011/07/06 21:41:37 art Exp $	*/
d48 10
d288 1
a314 8
uint64_t sched_nmigrations;
uint64_t sched_noidle;
uint64_t sched_stolen;

uint64_t sched_choose;
uint64_t sched_wasidle;
uint64_t sched_nomigrations;

d523 1
a523 1
	if (cpuset_isset(&sched_queued_cpus, ci)) {
a524 1
	}
@


1.23
log
@Clean up after P_BIGLOCK removal.
KERNEL_PROC_LOCK -> KERNEL_LOCK
KERNEL_PROC_UNLOCK -> KERNEL_UNLOCK

oga@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.22 2010/05/28 14:23:37 guenther Exp $	*/
d268 1
d278 1
d300 1
@


1.22
log
@Delete a fallback definition for CPU_INFO_UNIT that's both unnecessary
and incorrect.  Kills an XXX comment.

ok syuu, thib, art, kettenis, millert, deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.21 2010/05/25 19:59:35 kettenis Exp $	*/
d112 1
a112 1
	KERNEL_PROC_UNLOCK(p);
d194 1
a194 1
	KERNEL_PROC_UNLOCK(p);
@


1.21
log
@Actively remove processes from the runqueues of a CPU when we stop it.
Also make sure not to take the scheduler lock once we have stopped a CPU such
that we can safely take it away without having to worry about deadlock
because it happened to own the scheduler lock.

Fixes issues with suspen on SMP machines.

ok mlarkin@@, marco@@, art@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.20 2010/05/14 18:47:56 kettenis Exp $	*/
a628 7

/*
 * XXX - implement it on SP architectures too
 */
#ifndef CPU_INFO_UNIT
#define CPU_INFO_UNIT 0
#endif
@


1.20
log
@Make sure we initialize sched_lock before we try to use it.

ok miod@@, thib@@, oga@@, jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.19 2010/04/23 21:34:38 deraadt Exp $	*/
d45 1
d88 1
d152 4
a155 1
			if (spc->spc_schedflags & SPCF_SHOULDHALT) {
d157 2
a158 1
				    SPCF_HALTED);
d257 9
d341 1
a341 1
		cpuset_add_all(&set);
d398 1
a398 1
		cpuset_add_all(&set);
d572 1
d592 1
@


1.19
log
@Merge the only relevant (for now) parts of simplelock.h into lock.h
since it is time to start transitioning away from the no-op behaviour.
ok oga kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.18 2010/04/06 20:33:28 kettenis Exp $	*/
a202 3
#ifdef MULTIPROCESSOR
	__mp_lock_init(&sched_lock);
#endif
@


1.18
log
@Implement functions to take away the secondary CPUs from the scheduler and
give them back again, effectively stopping and starting these CPUs.  Use
the stop function in sys_reboot().

ok marco@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.17 2010/01/09 02:44:17 kettenis Exp $	*/
a26 1
#include <machine/atomic.h>
@


1.17
log
@Add code to stop scheduling processes on CPUs, effectively halting that CPU.
Use this to do a shutdown with only the boot processor running.  This should
avoid nasty races during shutdown.

help from art@@, ok deraadt@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.16 2009/11/29 23:12:30 kettenis Exp $	*/
a34 1
void sched_idle(void *);
d547 50
@


1.16
log
@Backout previous commit.  There is a possible race which makes it possible
for sys_reboot() to hang forever.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.14 2009/10/05 17:43:08 deraadt Exp $	*/
d151 6
a156 1
		while (spc->spc_whichqs == 0)
d158 1
d254 7
@


1.15
log
@Add a mechanism to stop the scheduler from scheduling processes on a
particular CPU such that it just sits and spins in the idle loop, effectively
halting that CPU.

ok deraadt@@, miod@@
@
text
@a148 5
		if (spc->spc_schedflags & SPCF_SHOULDHALT) {
			spc->spc_schedflags |= SPCF_HALTED;
			wakeup(spc);
		}

a247 7

	if (spc->spc_schedflags & SPCF_SHOULDHALT) {
		p = spc->spc_idleproc;
		KASSERT(p);
		p->p_stat = SRUN;
		return (p);
	}
@


1.14
log
@Don't drop the big lock at the end of exit1(), but move it into the middle of
sched_exit().  This means that cpu_exit() and whatever it does (for instance
calling free(), as well as the deadproc p_hash handling are now locked as well.
This may have been one of the causes of the reaper panics, especially with
rthread patches... which were terminating a lot of threads very quickly onto
the deadproc p_hash list.
ok kurt kettenis miod
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.13 2009/04/22 08:35:54 art Exp $	*/
d149 5
d253 7
@


1.13
log
@When starting up idle, explicitly set p_cpu and the peg flag for the
idle proc. p_cpu might be necessary in the future and pegging is just
to be extra safe (although we'll be horribly broken if the idle proc
ever ends up where that flag is checked).
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.12 2009/04/20 08:48:17 art Exp $	*/
d183 2
a184 3
#ifdef MULTIPROCESSOR
	KASSERT(__mp_lock_held(&kernel_lock) == 0);
#endif
@


1.12
log
@Make pegging a proc work when there are idle cpus that are looking for
something to do. Walk the highest priority queue looking for a proc
to steal and skip those that are pegged.

We could consider walking the other queues in the future too, but this
should do for now.

kettenis@@ guenther@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.11 2009/04/14 09:13:25 art Exp $	*/
d123 2
@


1.11
log
@Some tweaks to the cpu affinity code.
 - Split up choosing of cpu between fork and "normal" cases. Fork is
   very different and should be treated as such.
 - Instead of implicitly choosing a cpu in setrunqueue, do it outside
   where it actually makes sense.
 - Just because a cpu is marked as idle doesn't mean it will be soon.
   There could be a thundering herd effect if we call wakeup from an
   interrupt handler, so subtract cpus with queued processes when
   deciding which cpu is actually idle.
 - some simplifications allowed by the above.

kettenis@@ ok (except one bugfix that was not in the intial diff)
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.10 2009/04/03 09:29:15 art Exp $	*/
d410 1
d417 11
a427 7
		p = TAILQ_FIRST(&spc->spc_qs[ffs(spc->spc_whichqs) - 1]);
		KASSERT(p);
		cost = sched_proc_to_cpu_cost(self, p);

		if (best == NULL || cost < bestcost) {
			best = p;
			bestcost = cost;
@


1.10
log
@sched_peg_curproc_to_cpu() - function to force a proc to stay on a cpu
forever.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.9 2009/03/23 13:25:11 art Exp $	*/
a209 1
	sched_choosecpu(p);
d217 1
a217 1
	if (p->p_cpu != curcpu())
d285 52
a336 1
void
d348 1
a348 1
		return;
d353 12
a364 6
	 * The simplest case. Our cpu of choice was idle. This happens
	 * when we were sleeping and something woke us up.
	 *
	 * We also need to check sched_queued_cpus to make sure that
	 * we're not thundering herding one cpu that hasn't managed to
	 * get out of the idle loop yet.
d366 3
a368 2
	if (p->p_cpu && cpuset_isset(&sched_idle_cpus, p->p_cpu) &&
	    !cpuset_isset(&sched_queued_cpus, p->p_cpu)) {
d370 1
a370 1
		return;
d373 2
a374 11
#if 0

		/* Most likely, this is broken. don't do it. */
	/*
	 * Second case. (shouldn't be necessary in the future)
	 * If our cpu is not idle, but has nothing else queued (which
	 * means that we are curproc and roundrobin asks us to reschedule).
	 */
	if (p->p_cpu && p->p_cpu->ci_schedstate.spc_nrun == 0)
		return;
#endif
a375 5
	/*
	 * Look at all cpus that are currently idle. Pick the cheapest of
	 * those.
	 */
	cpuset_copy(&set, &sched_idle_cpus);
d386 1
a386 24
	/*
	 * All cpus are busy. Pick one.
	 */
	if (choice == NULL) {
		CPU_INFO_ITERATOR cii;

		sched_noidle++;

		/*
		 * Not curproc, pick the cpu with the lowest cost to switch to.
		 */
		CPU_INFO_FOREACH(cii, ci) {
			int cost = sched_proc_to_cpu_cost(ci, p);

			if (choice == NULL || cost < last_cost) {
				choice = ci;
				last_cost = cost;
			}
		}
	}

	KASSERT(choice);

	if (p->p_cpu && p->p_cpu != choice)
d388 1
a388 1
	else if (p->p_cpu != NULL)
d391 1
a391 1
	p->p_cpu = choice;
d599 27
@


1.9
log
@Processor affinity for processes.
 - Split up run queues so that every cpu has one.
 - Make setrunqueue choose the cpu where we want to make this process
   runnable (this should be refined and less brutal in the future).
 - When choosing the cpu where we want to run, make some kind of educated
   guess where it will be best to run (very naive right now).
Other:
 - Set operations for sets of cpus.
 - load average calculations per cpu.
 - sched_is_idle() -> curcpu_is_idle()

tested, debugged and prodded by many@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.8 2008/11/06 19:49:13 deraadt Exp $	*/
d294 6
d488 20
@


1.8
log
@oops
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.7 2008/11/06 19:19:04 deraadt Exp $	*/
d3 1
a3 1
 * Copyright (c) 2007 Artur Grabowski <art@@openbsd.org>
d27 1
d31 3
d37 11
d73 4
d83 6
d107 1
d114 2
d121 1
d124 1
d127 3
d131 1
a131 5
		KASSERT(ci == curcpu());
		KASSERT(curproc == ci->ci_schedstate.spc_idleproc);

		while (!sched_is_idle()) {
			struct schedstate_percpu *spc = &ci->ci_schedstate;
d147 1
d149 1
a149 1
		while (sched_is_idle())
d152 1
a193 3
 *
 * The run queue management is just like before, except that it's with
 * a bit more modern queue handling.
a194 4

TAILQ_HEAD(prochead, proc) sched_qs[NQS];
volatile int sched_whichqs;

a197 5
	int i;

	for (i = 0; i < NQS; i++)
		TAILQ_INIT(&sched_qs[i]);

d206 1
d210 7
d218 2
a219 2
	TAILQ_INSERT_TAIL(&sched_qs[queue], p, p_runq);
	sched_whichqs |= (1 << queue);
d225 1
d229 2
d232 6
a237 3
	TAILQ_REMOVE(&sched_qs[queue], p, p_runq);
	if (TAILQ_EMPTY(&sched_qs[queue]))
		sched_whichqs &= ~(1 << queue);
d243 1
d250 6
a255 2
	if (sched_is_idle()) {
		p = curcpu()->ci_schedstate.spc_idleproc;
d273 206
a478 6
	} else {
		queue = ffs(sched_whichqs) - 1;
		p = TAILQ_FIRST(&sched_qs[queue]);
		TAILQ_REMOVE(&sched_qs[queue], p, p_runq);
		if (TAILQ_EMPTY(&sched_qs[queue]))
			sched_whichqs &= ~(1 << queue);
d481 72
a552 1
	return (p);	
@


1.7
log
@panic if cpu_switchto() returns from a dead process
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.6 2008/06/12 06:58:39 deraadt Exp $	*/
d158 1
a158 1
	panic("cpu_switchto returned);
@


1.6
log
@Bring biomem diff back into the tree after the nfs_bio.c fix went in.
ok thib beck art
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.4 2008/06/10 20:14:36 beck Exp $	*/
d158 1
@


1.5
log
@back out biomem diff since it is not right yet.  Doing very large
file copies to nfsv2 causes the system to eventually peg the console.
On the console ^T indicates that the load is increasing rapidly, ddb
indicates many calls to getbuf, there is some very slow nfs traffic
making none (or extremely slow) progress.  Eventually some machines
seize up entirely.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.3 2008/06/08 20:13:13 thib Exp $	*/
d115 2
@


1.4
log
@
Buffer cache revamp

1) remove multiple size queues, introduced as a stopgap.
2) decouple pages containing data from their mappings
3) only keep buffers mapped when they actually have to be mapped
  (right now, this is when buffers are B_BUSY)
4) New functions to make a buffer busy, and release the busy flag
   (buf_acquire and buf_release)
5) Move high/low water marks and statistics counters into a structure
6) Add a sysctl to retrieve buffer cache statistics

Tested in several variants and beat upon by bob and art for a year. run
accidentally on henning's nfs server for a few months...

ok deraadt@@, krw@@, art@@ - who promises to be around to deal with any fallout
@
text
@a115 2
		splassert(IPL_NONE);

@


1.3
log
@use sched_is_idle() and nuke the sched_chooseproc prototype since we
already have on in sched.h
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.2 2007/11/26 17:15:29 art Exp $	*/
d115 2
@


1.2
log
@Move the implementation of __mp_lock (biglock) into machine dependent
code. At this moment all architectures get the copy of the old code
except i386 which gets a new shiny implementation that doesn't spin
at splhigh (doh!) and doesn't try to grab the biglock when releasing
the biglock (double doh!).

Shaves 10% of system time during kernel compile and might solve a few
bugs as a bonus.

Other architectures coming shortly.

miod@@ deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_sched.c,v 1.1 2007/10/10 15:53:53 art Exp $	*/
a29 1
struct proc *sched_chooseproc(void);
d213 1
a213 1
	if (sched_whichqs == 0) {
@


1.1
log
@Make context switching much more MI:
 - Move the functionality of choosing a process from cpu_switch into
   a much simpler function: cpu_switchto. Instead of having the locore
   code walk the run queues, let the MI code choose the process we
   want to run and only implement the context switching itself in MD
   code.
 - Let MD context switching run without worrying about spls or locks.
 - Instead of having the idle loop implemented with special contexts
   in MD code, implement one idle proc for each cpu. make the idle
   loop MI with MD hooks.
 - Change the proc lists from the old style vax queues to TAILQs.
 - Change the sleep queue from vax queues to TAILQs. This makes
   wakeup() go from O(n^2) to O(n)

there will be some MD fallout, but it will be fixed shortly.
There's also a few cleanups to be done after this.

deraadt@@, kettenis@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d178 1
a178 1
	SIMPLE_LOCK_INIT(&sched_lock);
@

