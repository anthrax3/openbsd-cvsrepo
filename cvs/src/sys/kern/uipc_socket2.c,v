head	1.83;
access;
symbols
	OPENBSD_6_1:1.75.0.4
	OPENBSD_6_1_BASE:1.75
	OPENBSD_6_0:1.64.0.2
	OPENBSD_6_0_BASE:1.64
	OPENBSD_5_9:1.63.0.2
	OPENBSD_5_9_BASE:1.63
	OPENBSD_5_8:1.62.0.4
	OPENBSD_5_8_BASE:1.62
	OPENBSD_5_7:1.59.0.2
	OPENBSD_5_7_BASE:1.59
	OPENBSD_5_6:1.56.0.8
	OPENBSD_5_6_BASE:1.56
	OPENBSD_5_5:1.56.0.6
	OPENBSD_5_5_BASE:1.56
	OPENBSD_5_4:1.56.0.2
	OPENBSD_5_4_BASE:1.56
	OPENBSD_5_3:1.55.0.2
	OPENBSD_5_3_BASE:1.55
	OPENBSD_5_2:1.53.0.2
	OPENBSD_5_2_BASE:1.53
	OPENBSD_5_1_BASE:1.52
	OPENBSD_5_1:1.52.0.4
	OPENBSD_5_0:1.52.0.2
	OPENBSD_5_0_BASE:1.52
	OPENBSD_4_9:1.51.0.2
	OPENBSD_4_9_BASE:1.51
	OPENBSD_4_8:1.50.0.4
	OPENBSD_4_8_BASE:1.50
	OPENBSD_4_7:1.50.0.2
	OPENBSD_4_7_BASE:1.50
	OPENBSD_4_6:1.48.0.4
	OPENBSD_4_6_BASE:1.48
	OPENBSD_4_5:1.46.0.2
	OPENBSD_4_5_BASE:1.46
	OPENBSD_4_4:1.44.0.2
	OPENBSD_4_4_BASE:1.44
	OPENBSD_4_3:1.43.0.2
	OPENBSD_4_3_BASE:1.43
	OPENBSD_4_2:1.42.0.4
	OPENBSD_4_2_BASE:1.42
	OPENBSD_4_1:1.42.0.2
	OPENBSD_4_1_BASE:1.42
	OPENBSD_4_0:1.41.0.4
	OPENBSD_4_0_BASE:1.41
	OPENBSD_3_9:1.41.0.2
	OPENBSD_3_9_BASE:1.41
	OPENBSD_3_8:1.40.0.2
	OPENBSD_3_8_BASE:1.40
	OPENBSD_3_7:1.38.0.4
	OPENBSD_3_7_BASE:1.38
	OPENBSD_3_6:1.38.0.2
	OPENBSD_3_6_BASE:1.38
	SMP_SYNC_A:1.38
	SMP_SYNC_B:1.38
	OPENBSD_3_5:1.35.0.4
	OPENBSD_3_5_BASE:1.35
	OPENBSD_3_4:1.35.0.2
	OPENBSD_3_4_BASE:1.35
	UBC_SYNC_A:1.33
	OPENBSD_3_3:1.33.0.2
	OPENBSD_3_3_BASE:1.33
	OPENBSD_3_2:1.31.0.2
	OPENBSD_3_2_BASE:1.31
	OPENBSD_3_1:1.25.0.4
	OPENBSD_3_1_BASE:1.25
	UBC_SYNC_B:1.33
	UBC:1.25.0.2
	UBC_BASE:1.25
	OPENBSD_3_0:1.20.0.2
	OPENBSD_3_0_BASE:1.20
	OPENBSD_2_9_BASE:1.15
	OPENBSD_2_9:1.15.0.2
	OPENBSD_2_8:1.14.0.4
	OPENBSD_2_8_BASE:1.14
	OPENBSD_2_7:1.14.0.2
	OPENBSD_2_7_BASE:1.14
	SMP:1.12.0.2
	SMP_BASE:1.12
	kame_19991208:1.11
	OPENBSD_2_6:1.10.0.4
	OPENBSD_2_6_BASE:1.10
	OPENBSD_2_5:1.10.0.2
	OPENBSD_2_5_BASE:1.10
	OPENBSD_2_4:1.7.0.4
	OPENBSD_2_4_BASE:1.7
	OPENBSD_2_3:1.7.0.2
	OPENBSD_2_3_BASE:1.7
	OPENBSD_2_2:1.6.0.2
	OPENBSD_2_2_BASE:1.6
	OPENBSD_2_1:1.5.0.2
	OPENBSD_2_1_BASE:1.5
	OPENBSD_2_0:1.4.0.2
	OPENBSD_2_0_BASE:1.4
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.83
date	2017.07.04.12.58.32;	author mpi;	state Exp;
branches;
next	1.82;
commitid	SVn8rvXqimriZ7Tw;

1.82
date	2017.07.04.12.52.48;	author mpi;	state Exp;
branches;
next	1.81;
commitid	YylK59t8psvIVrgf;

1.81
date	2017.07.04.12.51.18;	author mpi;	state Exp;
branches;
next	1.80;
commitid	QYpBJWhKtwR5RANd;

1.80
date	2017.06.27.12.02.43;	author mpi;	state Exp;
branches;
next	1.79;
commitid	yfTuwTXinKA8ckkX;

1.79
date	2017.06.26.09.32.31;	author mpi;	state Exp;
branches;
next	1.78;
commitid	gZMpLuRopIsWa0cT;

1.78
date	2017.06.07.13.41.02;	author mpi;	state Exp;
branches;
next	1.77;
commitid	V9fHR6ZsEIAUWeCM;

1.77
date	2017.05.27.18.50.53;	author claudio;	state Exp;
branches;
next	1.76;
commitid	zgiH4da2QPzswD5J;

1.76
date	2017.05.15.12.26.00;	author mpi;	state Exp;
branches;
next	1.75;
commitid	WMZaI3vIHNC1J8ol;

1.75
date	2017.03.17.17.19.16;	author mpi;	state Exp;
branches;
next	1.74;
commitid	CxqvXOMqotM60GAI;

1.74
date	2017.03.13.20.18.21;	author claudio;	state Exp;
branches;
next	1.73;
commitid	ZsxSSZJSFxZH81LL;

1.73
date	2017.03.07.09.23.27;	author mpi;	state Exp;
branches;
next	1.72;
commitid	ilH9l7TRBZ95J9uu;

1.72
date	2017.02.14.09.46.21;	author mpi;	state Exp;
branches;
next	1.71;
commitid	4bln7omqWkS0RJo9;

1.71
date	2017.01.25.06.15.50;	author mpi;	state Exp;
branches;
next	1.70;
commitid	X7Hk1efefaYrWlw3;

1.70
date	2016.12.29.12.12.43;	author mpi;	state Exp;
branches;
next	1.69;
commitid	RhxGXGNe4WuNtTZs;

1.69
date	2016.12.19.08.36.49;	author mpi;	state Exp;
branches;
next	1.68;
commitid	QqHqT2WhCBWqYgGJ;

1.68
date	2016.11.15.11.57.02;	author bluhm;	state Exp;
branches;
next	1.67;
commitid	TmC8TU5i870QwbIb;

1.67
date	2016.10.09.19.33.34;	author bluhm;	state Exp;
branches;
next	1.66;
commitid	LD54zXcjkAH5EsVk;

1.66
date	2016.10.06.19.09.08;	author bluhm;	state Exp;
branches;
next	1.65;
commitid	IO8dsedjngxSg7J1;

1.65
date	2016.09.02.13.28.21;	author bluhm;	state Exp;
branches;
next	1.64;
commitid	r3mOGuiVTMz26H65;

1.64
date	2016.06.28.14.47.00;	author tedu;	state Exp;
branches;
next	1.63;
commitid	Qxo29L0RKF7XsK5P;

1.63
date	2015.10.06.14.38.32;	author claudio;	state Exp;
branches;
next	1.62;
commitid	NGHaCuItxltwiICe;

1.62
date	2015.07.08.07.21.50;	author mpi;	state Exp;
branches;
next	1.61;
commitid	9ERVupAoYqW4Iok9;

1.61
date	2015.06.30.15.30.17;	author mpi;	state Exp;
branches;
next	1.60;
commitid	J4OPNuggl4DOKGzM;

1.60
date	2015.03.14.03.38.51;	author jsg;	state Exp;
branches;
next	1.59;
commitid	p4LJxGKbi0BU2cG6;

1.59
date	2014.12.11.19.21.57;	author tedu;	state Exp;
branches;
next	1.58;
commitid	KtikWduHBwfG1emb;

1.58
date	2014.09.14.14.17.26;	author jsg;	state Exp;
branches;
next	1.57;
commitid	uzzBR7hz9ncd4O6G;

1.57
date	2014.09.09.02.07.17;	author guenther;	state Exp;
branches;
next	1.56;
commitid	8mNtcvWyqXdDfneL;

1.56
date	2013.04.05.08.25.30;	author tedu;	state Exp;
branches;
next	1.55;

1.55
date	2013.01.15.11.12.57;	author bluhm;	state Exp;
branches;
next	1.54;

1.54
date	2012.12.31.13.45.14;	author bluhm;	state Exp;
branches;
next	1.53;

1.53
date	2012.04.13.09.38.32;	author deraadt;	state Exp;
branches;
next	1.52;

1.52
date	2011.04.04.21.11.22;	author claudio;	state Exp;
branches;
next	1.51;

1.51
date	2010.09.24.02.59.45;	author claudio;	state Exp;
branches;
next	1.50;

1.50
date	2009.11.09.17.53.39;	author nicm;	state Exp;
branches;
next	1.49;

1.49
date	2009.08.10.16.49.39;	author thib;	state Exp;
branches;
next	1.48;

1.48
date	2009.03.30.14.29.30;	author blambert;	state Exp;
branches;
next	1.47;

1.47
date	2009.03.15.19.40.41;	author miod;	state Exp;
branches;
next	1.46;

1.46
date	2009.01.13.13.36.12;	author blambert;	state Exp;
branches;
next	1.45;

1.45
date	2008.11.24.12.57.37;	author dlg;	state Exp;
branches;
next	1.44;

1.44
date	2008.05.23.15.51.12;	author thib;	state Exp;
branches;
next	1.43;

1.43
date	2007.09.19.15.08.29;	author blambert;	state Exp;
branches;
next	1.42;

1.42
date	2007.02.26.23.53.33;	author kurt;	state Exp;
branches;
next	1.41;

1.41
date	2006.01.05.05.05.07;	author jsg;	state Exp;
branches;
next	1.40;

1.40
date	2005.07.18.02.43.27;	author fgsch;	state Exp;
branches;
next	1.39;

1.39
date	2005.05.27.17.16.13;	author dhartmei;	state Exp;
branches;
next	1.38;

1.38
date	2004.04.25.16.25.05;	author markus;	state Exp;
branches;
next	1.37;

1.37
date	2004.04.19.22.38.39;	author deraadt;	state Exp;
branches;
next	1.36;

1.36
date	2004.04.01.23.56.05;	author tedu;	state Exp;
branches;
next	1.35;

1.35
date	2003.07.21.22.44.50;	author tedu;	state Exp;
branches;
next	1.34;

1.34
date	2003.06.02.23.28.07;	author millert;	state Exp;
branches;
next	1.33;

1.33
date	2002.10.12.01.09.45;	author krw;	state Exp;
branches;
next	1.32;

1.32
date	2002.10.10.22.27.30;	author art;	state Exp;
branches;
next	1.31;

1.31
date	2002.08.26.16.39.25;	author dhartmei;	state Exp;
branches;
next	1.30;

1.30
date	2002.08.08.19.18.12;	author provos;	state Exp;
branches;
next	1.29;

1.29
date	2002.08.08.18.26.37;	author todd;	state Exp;
branches;
next	1.28;

1.28
date	2002.08.08.17.07.32;	author provos;	state Exp;
branches;
next	1.27;

1.27
date	2002.06.11.05.07.43;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2002.05.11.00.06.33;	author deraadt;	state Exp;
branches;
next	1.25;

1.25
date	2001.11.30.19.48.09;	author provos;	state Exp;
branches
	1.25.2.1;
next	1.24;

1.24
date	2001.11.28.13.49.08;	author provos;	state Exp;
branches;
next	1.23;

1.23
date	2001.11.28.02.28.55;	author provos;	state Exp;
branches;
next	1.22;

1.22
date	2001.11.27.22.53.19;	author provos;	state Exp;
branches;
next	1.21;

1.21
date	2001.11.27.15.51.36;	author provos;	state Exp;
branches;
next	1.20;

1.20
date	2001.09.26.03.39.59;	author deraadt;	state Exp;
branches;
next	1.19;

1.19
date	2001.07.05.08.10.30;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2001.06.22.14.14.10;	author deraadt;	state Exp;
branches;
next	1.17;

1.17
date	2001.05.26.04.38.32;	author angelos;	state Exp;
branches;
next	1.16;

1.16
date	2001.05.02.08.33.49;	author provos;	state Exp;
branches;
next	1.15;

1.15
date	2000.11.16.20.02.19;	author provos;	state Exp;
branches;
next	1.14;

1.14
date	2000.02.29.19.16.46;	author itojun;	state Exp;
branches;
next	1.13;

1.13
date	2000.02.18.05.21.01;	author itojun;	state Exp;
branches;
next	1.12;

1.12
date	2000.02.04.20.32.04;	author angelos;	state Exp;
branches
	1.12.2.1;
next	1.11;

1.11
date	99.12.08.06.50.17;	author itojun;	state Exp;
branches;
next	1.10;

1.10
date	99.02.19.15.06.52;	author millert;	state Exp;
branches;
next	1.9;

1.9
date	99.02.18.22.56.58;	author deraadt;	state Exp;
branches;
next	1.8;

1.8
date	99.01.21.03.27.42;	author millert;	state Exp;
branches;
next	1.7;

1.7
date	98.02.14.10.55.09;	author deraadt;	state Exp;
branches;
next	1.6;

1.6
date	97.08.31.20.42.26;	author deraadt;	state Exp;
branches;
next	1.5;

1.5
date	97.02.21.08.45.00;	author deraadt;	state Exp;
branches;
next	1.4;

1.4
date	96.09.20.22.53.10;	author deraadt;	state Exp;
branches;
next	1.3;

1.3
date	96.08.24.04.56.37;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	96.03.03.17.20.20;	author niklas;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.52.47;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.52.47;	author deraadt;	state Exp;
branches;
next	;

1.12.2.1
date	2000.02.20.11.57.18;	author niklas;	state Exp;
branches;
next	1.12.2.2;

1.12.2.2
date	2000.03.02.07.04.41;	author niklas;	state Exp;
branches;
next	1.12.2.3;

1.12.2.3
date	2001.05.14.22.32.45;	author niklas;	state Exp;
branches;
next	1.12.2.4;

1.12.2.4
date	2001.07.04.10.48.45;	author niklas;	state Exp;
branches;
next	1.12.2.5;

1.12.2.5
date	2001.10.31.03.26.29;	author nate;	state Exp;
branches;
next	1.12.2.6;

1.12.2.6
date	2001.12.05.01.02.39;	author niklas;	state Exp;
branches;
next	1.12.2.7;

1.12.2.7
date	2003.03.28.00.41.27;	author niklas;	state Exp;
branches;
next	1.12.2.8;

1.12.2.8
date	2003.06.07.11.03.41;	author ho;	state Exp;
branches;
next	1.12.2.9;

1.12.2.9
date	2004.02.19.10.56.38;	author niklas;	state Exp;
branches;
next	1.12.2.10;

1.12.2.10
date	2004.06.05.23.13.02;	author niklas;	state Exp;
branches;
next	;

1.25.2.1
date	2002.06.11.03.29.40;	author art;	state Exp;
branches;
next	1.25.2.2;

1.25.2.2
date	2002.10.29.00.36.44;	author art;	state Exp;
branches;
next	;


desc
@@


1.83
log
@Always hold the socket lock when calling sblock().

Implicitely protects `so_state' with the socket lock in sosend().

ok visa@@, bluhm@@
@
text
@/*	$OpenBSD: uipc_socket2.c,v 1.82 2017/07/04 12:52:48 mpi Exp $	*/
/*	$NetBSD: uipc_socket2.c,v 1.11 1996/02/04 02:17:55 christos Exp $	*/

/*
 * Copyright (c) 1982, 1986, 1988, 1990, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)uipc_socket2.c	8.1 (Berkeley) 6/10/93
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/file.h>
#include <sys/malloc.h>
#include <sys/mbuf.h>
#include <sys/protosw.h>
#include <sys/domain.h>
#include <sys/socket.h>
#include <sys/socketvar.h>
#include <sys/signalvar.h>
#include <sys/event.h>
#include <sys/pool.h>

/*
 * Primitive routines for operating on sockets and socket buffers
 */

u_long	sb_max = SB_MAX;		/* patchable */

extern struct pool mclpools[];
extern struct pool mbpool;

/*
 * Procedures to manipulate state flags of socket
 * and do appropriate wakeups.  Normal sequence from the
 * active (originating) side is that soisconnecting() is
 * called during processing of connect() call,
 * resulting in an eventual call to soisconnected() if/when the
 * connection is established.  When the connection is torn down
 * soisdisconnecting() is called during processing of disconnect() call,
 * and soisdisconnected() is called when the connection to the peer
 * is totally severed.  The semantics of these routines are such that
 * connectionless protocols can call soisconnected() and soisdisconnected()
 * only, bypassing the in-progress calls when setting up a ``connection''
 * takes no time.
 *
 * From the passive side, a socket is created with
 * two queues of sockets: so_q0 for connections in progress
 * and so_q for connections already made and awaiting user acceptance.
 * As a protocol is preparing incoming connections, it creates a socket
 * structure queued on so_q0 by calling sonewconn().  When the connection
 * is established, soisconnected() is called, and transfers the
 * socket structure to so_q, making it available to accept().
 *
 * If a socket is closed with sockets on either
 * so_q0 or so_q, these sockets are dropped.
 *
 * If higher level protocols are implemented in
 * the kernel, the wakeups done here will sometimes
 * cause software-interrupt process scheduling.
 */

void
soisconnecting(struct socket *so)
{
	soassertlocked(so);
	so->so_state &= ~(SS_ISCONNECTED|SS_ISDISCONNECTING);
	so->so_state |= SS_ISCONNECTING;
}

void
soisconnected(struct socket *so)
{
	struct socket *head = so->so_head;

	soassertlocked(so);
	so->so_state &= ~(SS_ISCONNECTING|SS_ISDISCONNECTING);
	so->so_state |= SS_ISCONNECTED;
	if (head && soqremque(so, 0)) {
		soqinsque(head, so, 1);
		sorwakeup(head);
		wakeup_one(&head->so_timeo);
	} else {
		wakeup(&so->so_timeo);
		sorwakeup(so);
		sowwakeup(so);
	}
}

void
soisdisconnecting(struct socket *so)
{
	soassertlocked(so);
	so->so_state &= ~SS_ISCONNECTING;
	so->so_state |= (SS_ISDISCONNECTING|SS_CANTRCVMORE|SS_CANTSENDMORE);
	wakeup(&so->so_timeo);
	sowwakeup(so);
	sorwakeup(so);
}

void
soisdisconnected(struct socket *so)
{
	soassertlocked(so);
	so->so_state &= ~(SS_ISCONNECTING|SS_ISCONNECTED|SS_ISDISCONNECTING);
	so->so_state |= (SS_CANTRCVMORE|SS_CANTSENDMORE|SS_ISDISCONNECTED);
	wakeup(&so->so_timeo);
	sowwakeup(so);
	sorwakeup(so);
}

/*
 * When an attempt at a new connection is noted on a socket
 * which accepts connections, sonewconn is called.  If the
 * connection is possible (subject to space constraints, etc.)
 * then we allocate a new structure, properly linked into the
 * data structure of the original socket, and return this.
 * Connstatus may be 0 or SS_ISCONNECTED.
 */
struct socket *
sonewconn(struct socket *head, int connstatus)
{
	struct socket *so;
	int soqueue = connstatus ? 1 : 0;

	/*
	 * XXXSMP as long as `so' and `head' share the same lock, we
	 * can call soreserve() and pr_attach() below w/o expliclitly
	 * locking `so'.
	 */
	soassertlocked(head);

	if (mclpools[0].pr_nout > mclpools[0].pr_hardlimit * 95 / 100)
		return (NULL);
	if (head->so_qlen + head->so_q0len > head->so_qlimit * 3)
		return (NULL);
	so = pool_get(&socket_pool, PR_NOWAIT|PR_ZERO);
	if (so == NULL)
		return (NULL);
	so->so_type = head->so_type;
	so->so_options = head->so_options &~ SO_ACCEPTCONN;
	so->so_linger = head->so_linger;
	so->so_state = head->so_state | SS_NOFDREF;
	so->so_proto = head->so_proto;
	so->so_timeo = head->so_timeo;
	so->so_pgid = head->so_pgid;
	so->so_euid = head->so_euid;
	so->so_ruid = head->so_ruid;
	so->so_egid = head->so_egid;
	so->so_rgid = head->so_rgid;
	so->so_cpid = head->so_cpid;
	so->so_siguid = head->so_siguid;
	so->so_sigeuid = head->so_sigeuid;

	/*
	 * Inherit watermarks but those may get clamped in low mem situations.
	 */
	if (soreserve(so, head->so_snd.sb_hiwat, head->so_rcv.sb_hiwat)) {
		pool_put(&socket_pool, so);
		return (NULL);
	}
	so->so_snd.sb_wat = head->so_snd.sb_wat;
	so->so_snd.sb_lowat = head->so_snd.sb_lowat;
	so->so_snd.sb_timeo = head->so_snd.sb_timeo;
	so->so_rcv.sb_wat = head->so_rcv.sb_wat;
	so->so_rcv.sb_lowat = head->so_rcv.sb_lowat;
	so->so_rcv.sb_timeo = head->so_rcv.sb_timeo;

	soqinsque(head, so, soqueue);
	if ((*so->so_proto->pr_attach)(so, 0)) {
		(void) soqremque(so, soqueue);
		pool_put(&socket_pool, so);
		return (NULL);
	}
	if (connstatus) {
		sorwakeup(head);
		wakeup(&head->so_timeo);
		so->so_state |= connstatus;
	}
	return (so);
}

void
soqinsque(struct socket *head, struct socket *so, int q)
{
	soassertlocked(head);

#ifdef DIAGNOSTIC
	if (so->so_onq != NULL)
		panic("soqinsque");
#endif

	so->so_head = head;
	if (q == 0) {
		head->so_q0len++;
		so->so_onq = &head->so_q0;
	} else {
		head->so_qlen++;
		so->so_onq = &head->so_q;
	}
	TAILQ_INSERT_TAIL(so->so_onq, so, so_qe);
}

int
soqremque(struct socket *so, int q)
{
	struct socket *head = so->so_head;

	soassertlocked(head);

	if (q == 0) {
		if (so->so_onq != &head->so_q0)
			return (0);
		head->so_q0len--;
	} else {
		if (so->so_onq != &head->so_q)
			return (0);
		head->so_qlen--;
	}
	TAILQ_REMOVE(so->so_onq, so, so_qe);
	so->so_onq = NULL;
	so->so_head = NULL;
	return (1);
}

/*
 * Socantsendmore indicates that no more data will be sent on the
 * socket; it would normally be applied to a socket when the user
 * informs the system that no more data is to be sent, by the protocol
 * code (in case PRU_SHUTDOWN).  Socantrcvmore indicates that no more data
 * will be received, and will normally be applied to the socket by a
 * protocol when it detects that the peer will send no more data.
 * Data queued for reading in the socket may yet be read.
 */

void
socantsendmore(struct socket *so)
{
	soassertlocked(so);
	so->so_state |= SS_CANTSENDMORE;
	sowwakeup(so);
}

void
socantrcvmore(struct socket *so)
{
	soassertlocked(so);
	so->so_state |= SS_CANTRCVMORE;
	sorwakeup(so);
}

int
solock(struct socket *so)
{
	int s;

	if ((so->so_proto->pr_domain->dom_family != PF_LOCAL) &&
	    (so->so_proto->pr_domain->dom_family != PF_ROUTE) &&
	    (so->so_proto->pr_domain->dom_family != PF_KEY))
		NET_LOCK(s);
	else
		s = -42;

	return (s);
}

void
sounlock(int s)
{
	if (s != -42)
		NET_UNLOCK(s);
}

void
soassertlocked(struct socket *so)
{
	switch (so->so_proto->pr_domain->dom_family) {
	case PF_INET:
	case PF_INET6:
		NET_ASSERT_LOCKED();
		break;
	case PF_LOCAL:
	case PF_ROUTE:
	case PF_KEY:
	default:
		KERNEL_ASSERT_LOCKED();
		break;
	}
}

int
sosleep(struct socket *so, void *ident, int prio, const char *wmesg, int timo)
{
	if ((so->so_proto->pr_domain->dom_family != PF_LOCAL) &&
	    (so->so_proto->pr_domain->dom_family != PF_ROUTE) &&
	    (so->so_proto->pr_domain->dom_family != PF_KEY)) {
		return rwsleep(ident, &netlock, prio, wmesg, timo);
	} else
		return tsleep(ident, prio, wmesg, timo);
}

/*
 * Wait for data to arrive at/drain from a socket buffer.
 */
int
sbwait(struct socket *so, struct sockbuf *sb)
{
	soassertlocked(so);

	sb->sb_flagsintr |= SB_WAIT;
	return (sosleep(so, &sb->sb_cc,
	    (sb->sb_flags & SB_NOINTR) ? PSOCK : PSOCK | PCATCH, "netio",
	    sb->sb_timeo));
}

int
sblock(struct socket *so, struct sockbuf *sb, int wait)
{
	int error, prio = (sb->sb_flags & SB_NOINTR) ? PSOCK : PSOCK | PCATCH;

	KERNEL_ASSERT_LOCKED();
	soassertlocked(so);

	if ((sb->sb_flags & SB_LOCK) == 0) {
		sb->sb_flags |= SB_LOCK;
		return (0);
	}
	if (wait & M_NOWAIT)
		return (EWOULDBLOCK);

	while (sb->sb_flags & SB_LOCK) {
		sb->sb_flags |= SB_WANT;
		error = sosleep(so, &sb->sb_flags, prio, "netlck", 0);
		if (error)
			return (error);
	}
	sb->sb_flags |= SB_LOCK;
	return (0);
}

void
sbunlock(struct sockbuf *sb)
{
	KERNEL_ASSERT_LOCKED();

	sb->sb_flags &= ~SB_LOCK;
	if (sb->sb_flags & SB_WANT) {
		sb->sb_flags &= ~SB_WANT;
		wakeup(&sb->sb_flags);
	}
}

/*
 * Wakeup processes waiting on a socket buffer.
 * Do asynchronous notification via SIGIO
 * if the socket has the SS_ASYNC flag set.
 */
void
sowakeup(struct socket *so, struct sockbuf *sb)
{
	soassertlocked(so);

	selwakeup(&sb->sb_sel);
	sb->sb_flagsintr &= ~SB_SEL;
	if (sb->sb_flagsintr & SB_WAIT) {
		sb->sb_flagsintr &= ~SB_WAIT;
		wakeup(&sb->sb_cc);
	}
	if (so->so_state & SS_ASYNC)
		csignal(so->so_pgid, SIGIO, so->so_siguid, so->so_sigeuid);
}

/*
 * Socket buffer (struct sockbuf) utility routines.
 *
 * Each socket contains two socket buffers: one for sending data and
 * one for receiving data.  Each buffer contains a queue of mbufs,
 * information about the number of mbufs and amount of data in the
 * queue, and other fields allowing select() statements and notification
 * on data availability to be implemented.
 *
 * Data stored in a socket buffer is maintained as a list of records.
 * Each record is a list of mbufs chained together with the m_next
 * field.  Records are chained together with the m_nextpkt field. The upper
 * level routine soreceive() expects the following conventions to be
 * observed when placing information in the receive buffer:
 *
 * 1. If the protocol requires each message be preceded by the sender's
 *    name, then a record containing that name must be present before
 *    any associated data (mbuf's must be of type MT_SONAME).
 * 2. If the protocol supports the exchange of ``access rights'' (really
 *    just additional data associated with the message), and there are
 *    ``rights'' to be received, then a record containing this data
 *    should be present (mbuf's must be of type MT_CONTROL).
 * 3. If a name or rights record exists, then it must be followed by
 *    a data record, perhaps of zero length.
 *
 * Before using a new socket structure it is first necessary to reserve
 * buffer space to the socket, by calling sbreserve().  This should commit
 * some of the available buffer space in the system buffer pool for the
 * socket (currently, it does nothing but enforce limits).  The space
 * should be released by calling sbrelease() when the socket is destroyed.
 */

int
soreserve(struct socket *so, u_long sndcc, u_long rcvcc)
{

	if (sbreserve(so, &so->so_snd, sndcc))
		goto bad;
	if (sbreserve(so, &so->so_rcv, rcvcc))
		goto bad2;
	so->so_snd.sb_wat = sndcc;
	so->so_rcv.sb_wat = rcvcc;
	if (so->so_rcv.sb_lowat == 0)
		so->so_rcv.sb_lowat = 1;
	if (so->so_snd.sb_lowat == 0)
		so->so_snd.sb_lowat = MCLBYTES;
	if (so->so_snd.sb_lowat > so->so_snd.sb_hiwat)
		so->so_snd.sb_lowat = so->so_snd.sb_hiwat;
	return (0);
bad2:
	sbrelease(so, &so->so_snd);
bad:
	return (ENOBUFS);
}

/*
 * Allot mbufs to a sockbuf.
 * Attempt to scale mbmax so that mbcnt doesn't become limiting
 * if buffering efficiency is near the normal case.
 */
int
sbreserve(struct socket *so, struct sockbuf *sb, u_long cc)
{
	KASSERT(sb == &so->so_rcv || sb == &so->so_snd);
	soassertlocked(so);

	if (cc == 0 || cc > sb_max)
		return (1);
	sb->sb_hiwat = cc;
	sb->sb_mbmax = max(3 * MAXMCLBYTES,
	    min(cc * 2, sb_max + (sb_max / MCLBYTES) * MSIZE));
	if (sb->sb_lowat > sb->sb_hiwat)
		sb->sb_lowat = sb->sb_hiwat;
	return (0);
}

/*
 * In low memory situation, do not accept any greater than normal request.
 */
int
sbcheckreserve(u_long cnt, u_long defcnt)
{
	if (cnt > defcnt && sbchecklowmem())
		return (ENOBUFS);
	return (0);
}

int
sbchecklowmem(void)
{
	static int sblowmem;

	if (mclpools[0].pr_nout < mclpools[0].pr_hardlimit * 60 / 100 ||
	    mbpool.pr_nout < mbpool.pr_hardlimit * 60 / 100)
		sblowmem = 0;
	if (mclpools[0].pr_nout > mclpools[0].pr_hardlimit * 80 / 100 ||
	    mbpool.pr_nout > mbpool.pr_hardlimit * 80 / 100)
		sblowmem = 1;
	return (sblowmem);
}

/*
 * Free mbufs held by a socket, and reserved mbuf space.
 */
void
sbrelease(struct socket *so, struct sockbuf *sb)
{

	sbflush(so, sb);
	sb->sb_hiwat = sb->sb_mbmax = 0;
}

/*
 * Routines to add and remove
 * data from an mbuf queue.
 *
 * The routines sbappend() or sbappendrecord() are normally called to
 * append new mbufs to a socket buffer, after checking that adequate
 * space is available, comparing the function sbspace() with the amount
 * of data to be added.  sbappendrecord() differs from sbappend() in
 * that data supplied is treated as the beginning of a new record.
 * To place a sender's address, optional access rights, and data in a
 * socket receive buffer, sbappendaddr() should be used.  To place
 * access rights and data in a socket receive buffer, sbappendrights()
 * should be used.  In either case, the new data begins a new record.
 * Note that unlike sbappend() and sbappendrecord(), these routines check
 * for the caller that there will be enough space to store the data.
 * Each fails if there is not enough space, or if it cannot find mbufs
 * to store additional information in.
 *
 * Reliable protocols may use the socket send buffer to hold data
 * awaiting acknowledgement.  Data is normally copied from a socket
 * send buffer in a protocol with m_copym for output to a peer,
 * and then removing the data from the socket buffer with sbdrop()
 * or sbdroprecord() when the data is acknowledged by the peer.
 */

#ifdef SOCKBUF_DEBUG
void
sblastrecordchk(struct sockbuf *sb, const char *where)
{
	struct mbuf *m = sb->sb_mb;

	while (m && m->m_nextpkt)
		m = m->m_nextpkt;

	if (m != sb->sb_lastrecord) {
		printf("sblastrecordchk: sb_mb %p sb_lastrecord %p last %p\n",
		    sb->sb_mb, sb->sb_lastrecord, m);
		printf("packet chain:\n");
		for (m = sb->sb_mb; m != NULL; m = m->m_nextpkt)
			printf("\t%p\n", m);
		panic("sblastrecordchk from %s", where);
	}
}

void
sblastmbufchk(struct sockbuf *sb, const char *where)
{
	struct mbuf *m = sb->sb_mb;
	struct mbuf *n;

	while (m && m->m_nextpkt)
		m = m->m_nextpkt;

	while (m && m->m_next)
		m = m->m_next;

	if (m != sb->sb_mbtail) {
		printf("sblastmbufchk: sb_mb %p sb_mbtail %p last %p\n",
		    sb->sb_mb, sb->sb_mbtail, m);
		printf("packet tree:\n");
		for (m = sb->sb_mb; m != NULL; m = m->m_nextpkt) {
			printf("\t");
			for (n = m; n != NULL; n = n->m_next)
				printf("%p ", n);
			printf("\n");
		}
		panic("sblastmbufchk from %s", where);
	}
}
#endif /* SOCKBUF_DEBUG */

#define	SBLINKRECORD(sb, m0)						\
do {									\
	if ((sb)->sb_lastrecord != NULL)				\
		(sb)->sb_lastrecord->m_nextpkt = (m0);			\
	else								\
		(sb)->sb_mb = (m0);					\
	(sb)->sb_lastrecord = (m0);					\
} while (/*CONSTCOND*/0)

/*
 * Append mbuf chain m to the last record in the
 * socket buffer sb.  The additional space associated
 * the mbuf chain is recorded in sb.  Empty mbufs are
 * discarded and mbufs are compacted where possible.
 */
void
sbappend(struct socket *so, struct sockbuf *sb, struct mbuf *m)
{
	struct mbuf *n;

	if (m == NULL)
		return;

	SBLASTRECORDCHK(sb, "sbappend 1");

	if ((n = sb->sb_lastrecord) != NULL) {
		/*
		 * XXX Would like to simply use sb_mbtail here, but
		 * XXX I need to verify that I won't miss an EOR that
		 * XXX way.
		 */
		do {
			if (n->m_flags & M_EOR) {
				sbappendrecord(so, sb, m); /* XXXXXX!!!! */
				return;
			}
		} while (n->m_next && (n = n->m_next));
	} else {
		/*
		 * If this is the first record in the socket buffer, it's
		 * also the last record.
		 */
		sb->sb_lastrecord = m;
	}
	sbcompress(sb, m, n);
	SBLASTRECORDCHK(sb, "sbappend 2");
}

/*
 * This version of sbappend() should only be used when the caller
 * absolutely knows that there will never be more than one record
 * in the socket buffer, that is, a stream protocol (such as TCP).
 */
void
sbappendstream(struct socket *so, struct sockbuf *sb, struct mbuf *m)
{
	KASSERT(sb == &so->so_rcv || sb == &so->so_snd);
	soassertlocked(so);
	KDASSERT(m->m_nextpkt == NULL);
	KASSERT(sb->sb_mb == sb->sb_lastrecord);

	SBLASTMBUFCHK(sb, __func__);

	sbcompress(sb, m, sb->sb_mbtail);

	sb->sb_lastrecord = sb->sb_mb;
	SBLASTRECORDCHK(sb, __func__);
}

#ifdef SOCKBUF_DEBUG
void
sbcheck(struct sockbuf *sb)
{
	struct mbuf *m, *n;
	u_long len = 0, mbcnt = 0;

	for (m = sb->sb_mb; m; m = m->m_nextpkt) {
		for (n = m; n; n = n->m_next) {
			len += n->m_len;
			mbcnt += MSIZE;
			if (n->m_flags & M_EXT)
				mbcnt += n->m_ext.ext_size;
			if (m != n && n->m_nextpkt)
				panic("sbcheck nextpkt");
		}
	}
	if (len != sb->sb_cc || mbcnt != sb->sb_mbcnt) {
		printf("cc %lu != %lu || mbcnt %lu != %lu\n", len, sb->sb_cc,
		    mbcnt, sb->sb_mbcnt);
		panic("sbcheck");
	}
}
#endif

/*
 * As above, except the mbuf chain
 * begins a new record.
 */
void
sbappendrecord(struct socket *so, struct sockbuf *sb, struct mbuf *m0)
{
	struct mbuf *m;

	KASSERT(sb == &so->so_rcv || sb == &so->so_snd);
	soassertlocked(so);

	if (m0 == NULL)
		return;

	/*
	 * Put the first mbuf on the queue.
	 * Note this permits zero length records.
	 */
	sballoc(sb, m0);
	SBLASTRECORDCHK(sb, "sbappendrecord 1");
	SBLINKRECORD(sb, m0);
	m = m0->m_next;
	m0->m_next = NULL;
	if (m && (m0->m_flags & M_EOR)) {
		m0->m_flags &= ~M_EOR;
		m->m_flags |= M_EOR;
	}
	sbcompress(sb, m, m0);
	SBLASTRECORDCHK(sb, "sbappendrecord 2");
}

/*
 * As above except that OOB data
 * is inserted at the beginning of the sockbuf,
 * but after any other OOB data.
 */
void
sbinsertoob(struct sockbuf *sb, struct mbuf *m0)
{
	struct mbuf *m, **mp;

	if (m0 == NULL)
		return;

	SBLASTRECORDCHK(sb, "sbinsertoob 1");

	for (mp = &sb->sb_mb; (m = *mp) != NULL; mp = &((*mp)->m_nextpkt)) {
	    again:
		switch (m->m_type) {

		case MT_OOBDATA:
			continue;		/* WANT next train */

		case MT_CONTROL:
			if ((m = m->m_next) != NULL)
				goto again;	/* inspect THIS train further */
		}
		break;
	}
	/*
	 * Put the first mbuf on the queue.
	 * Note this permits zero length records.
	 */
	sballoc(sb, m0);
	m0->m_nextpkt = *mp;
	if (*mp == NULL) {
		/* m0 is actually the new tail */
		sb->sb_lastrecord = m0;
	}
	*mp = m0;
	m = m0->m_next;
	m0->m_next = NULL;
	if (m && (m0->m_flags & M_EOR)) {
		m0->m_flags &= ~M_EOR;
		m->m_flags |= M_EOR;
	}
	sbcompress(sb, m, m0);
	SBLASTRECORDCHK(sb, "sbinsertoob 2");
}

/*
 * Append address and data, and optionally, control (ancillary) data
 * to the receive queue of a socket.  If present,
 * m0 must include a packet header with total length.
 * Returns 0 if no space in sockbuf or insufficient mbufs.
 */
int
sbappendaddr(struct socket *so, struct sockbuf *sb, struct sockaddr *asa,
    struct mbuf *m0, struct mbuf *control)
{
	struct mbuf *m, *n, *nlast;
	int space = asa->sa_len;

	if (m0 && (m0->m_flags & M_PKTHDR) == 0)
		panic("sbappendaddr");
	if (m0)
		space += m0->m_pkthdr.len;
	for (n = control; n; n = n->m_next) {
		space += n->m_len;
		if (n->m_next == NULL)	/* keep pointer to last control buf */
			break;
	}
	if (space > sbspace(so, sb))
		return (0);
	if (asa->sa_len > MLEN)
		return (0);
	MGET(m, M_DONTWAIT, MT_SONAME);
	if (m == NULL)
		return (0);
	m->m_len = asa->sa_len;
	memcpy(mtod(m, caddr_t), asa, asa->sa_len);
	if (n)
		n->m_next = m0;		/* concatenate data to control */
	else
		control = m0;
	m->m_next = control;

	SBLASTRECORDCHK(sb, "sbappendaddr 1");

	for (n = m; n->m_next != NULL; n = n->m_next)
		sballoc(sb, n);
	sballoc(sb, n);
	nlast = n;
	SBLINKRECORD(sb, m);

	sb->sb_mbtail = nlast;
	SBLASTMBUFCHK(sb, "sbappendaddr");

	SBLASTRECORDCHK(sb, "sbappendaddr 2");

	return (1);
}

int
sbappendcontrol(struct socket *so, struct sockbuf *sb, struct mbuf *m0,
    struct mbuf *control)
{
	struct mbuf *m, *mlast, *n;
	int space = 0;

	if (control == NULL)
		panic("sbappendcontrol");
	for (m = control; ; m = m->m_next) {
		space += m->m_len;
		if (m->m_next == NULL)
			break;
	}
	n = m;			/* save pointer to last control buffer */
	for (m = m0; m; m = m->m_next)
		space += m->m_len;
	if (space > sbspace(so, sb))
		return (0);
	n->m_next = m0;			/* concatenate data to control */

	SBLASTRECORDCHK(sb, "sbappendcontrol 1");

	for (m = control; m->m_next != NULL; m = m->m_next)
		sballoc(sb, m);
	sballoc(sb, m);
	mlast = m;
	SBLINKRECORD(sb, control);

	sb->sb_mbtail = mlast;
	SBLASTMBUFCHK(sb, "sbappendcontrol");

	SBLASTRECORDCHK(sb, "sbappendcontrol 2");

	return (1);
}

/*
 * Compress mbuf chain m into the socket
 * buffer sb following mbuf n.  If n
 * is null, the buffer is presumed empty.
 */
void
sbcompress(struct sockbuf *sb, struct mbuf *m, struct mbuf *n)
{
	int eor = 0;
	struct mbuf *o;

	while (m) {
		eor |= m->m_flags & M_EOR;
		if (m->m_len == 0 &&
		    (eor == 0 ||
		    (((o = m->m_next) || (o = n)) &&
		    o->m_type == m->m_type))) {
			if (sb->sb_lastrecord == m)
				sb->sb_lastrecord = m->m_next;
			m = m_free(m);
			continue;
		}
		if (n && (n->m_flags & M_EOR) == 0 &&
		    /* M_TRAILINGSPACE() checks buffer writeability */
		    m->m_len <= MCLBYTES / 4 && /* XXX Don't copy too much */
		    m->m_len <= M_TRAILINGSPACE(n) &&
		    n->m_type == m->m_type) {
			memcpy(mtod(n, caddr_t) + n->m_len, mtod(m, caddr_t),
			    m->m_len);
			n->m_len += m->m_len;
			sb->sb_cc += m->m_len;
			if (m->m_type != MT_CONTROL && m->m_type != MT_SONAME)
				sb->sb_datacc += m->m_len;
			m = m_free(m);
			continue;
		}
		if (n)
			n->m_next = m;
		else
			sb->sb_mb = m;
		sb->sb_mbtail = m;
		sballoc(sb, m);
		n = m;
		m->m_flags &= ~M_EOR;
		m = m->m_next;
		n->m_next = NULL;
	}
	if (eor) {
		if (n)
			n->m_flags |= eor;
		else
			printf("semi-panic: sbcompress");
	}
	SBLASTMBUFCHK(sb, __func__);
}

/*
 * Free all mbufs in a sockbuf.
 * Check that all resources are reclaimed.
 */
void
sbflush(struct socket *so, struct sockbuf *sb)
{
	KASSERT(sb == &so->so_rcv || sb == &so->so_snd);
	KASSERT((sb->sb_flags & SB_LOCK) == 0);

	while (sb->sb_mbcnt)
		sbdrop(so, sb, (int)sb->sb_cc);

	KASSERT(sb->sb_cc == 0);
	KASSERT(sb->sb_datacc == 0);
	KASSERT(sb->sb_mb == NULL);
	KASSERT(sb->sb_mbtail == NULL);
	KASSERT(sb->sb_lastrecord == NULL);
}

/*
 * Drop data from (the front of) a sockbuf.
 */
void
sbdrop(struct socket *so, struct sockbuf *sb, int len)
{
	struct mbuf *m, *mn;
	struct mbuf *next;

	KASSERT(sb == &so->so_rcv || sb == &so->so_snd);
	soassertlocked(so);

	next = (m = sb->sb_mb) ? m->m_nextpkt : 0;
	while (len > 0) {
		if (m == NULL) {
			if (next == NULL)
				panic("sbdrop");
			m = next;
			next = m->m_nextpkt;
			continue;
		}
		if (m->m_len > len) {
			m->m_len -= len;
			m->m_data += len;
			sb->sb_cc -= len;
			if (m->m_type != MT_CONTROL && m->m_type != MT_SONAME)
				sb->sb_datacc -= len;
			break;
		}
		len -= m->m_len;
		sbfree(sb, m);
		mn = m_free(m);
		m = mn;
	}
	while (m && m->m_len == 0) {
		sbfree(sb, m);
		mn = m_free(m);
		m = mn;
	}
	if (m) {
		sb->sb_mb = m;
		m->m_nextpkt = next;
	} else
		sb->sb_mb = next;
	/*
	 * First part is an inline SB_EMPTY_FIXUP().  Second part
	 * makes sure sb_lastrecord is up-to-date if we dropped
	 * part of the last record.
	 */
	m = sb->sb_mb;
	if (m == NULL) {
		sb->sb_mbtail = NULL;
		sb->sb_lastrecord = NULL;
	} else if (m->m_nextpkt == NULL)
		sb->sb_lastrecord = m;
}

/*
 * Drop a record off the front of a sockbuf
 * and move the next record to the front.
 */
void
sbdroprecord(struct sockbuf *sb)
{
	struct mbuf *m, *mn;

	m = sb->sb_mb;
	if (m) {
		sb->sb_mb = m->m_nextpkt;
		do {
			sbfree(sb, m);
			mn = m_free(m);
		} while ((m = mn) != NULL);
	}
	SB_EMPTY_FIXUP(sb);
}

/*
 * Create a "control" mbuf containing the specified data
 * with the specified type for presentation on a socket buffer.
 */
struct mbuf *
sbcreatecontrol(caddr_t p, int size, int type, int level)
{
	struct cmsghdr *cp;
	struct mbuf *m;

	if (CMSG_SPACE(size) > MCLBYTES) {
		printf("sbcreatecontrol: message too large %d\n", size);
		return NULL;
	}

	if ((m = m_get(M_DONTWAIT, MT_CONTROL)) == NULL)
		return (NULL);
	if (CMSG_SPACE(size) > MLEN) {
		MCLGET(m, M_DONTWAIT);
		if ((m->m_flags & M_EXT) == 0) {
			m_free(m);
			return NULL;
		}
	}
	cp = mtod(m, struct cmsghdr *);
	memset(cp, 0, CMSG_SPACE(size));
	memcpy(CMSG_DATA(cp), p, size);
	m->m_len = CMSG_SPACE(size);
	cp->cmsg_len = CMSG_LEN(size);
	cp->cmsg_level = level;
	cp->cmsg_type = type;
	return (m);
}
@


1.82
log
@Assert that the socket lock is held when `so_state' is modified.

ok bluhm@@, visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.81 2017/07/04 12:51:18 mpi Exp $	*/
a56 2
int sbsleep(struct sockbuf *, struct rwlock *);

d341 1
a341 1
sbsleep(struct sockbuf *sb, struct rwlock *lock)
a344 13
	if (lock != NULL)
		error = rwsleep(&sb->sb_flags, lock, prio, "netlck", 0);
	else
		error = tsleep(&sb->sb_flags, prio, "netlck", 0);

	return (error);
}

int
sblock(struct sockbuf *sb, int wait, struct rwlock *lock)
{
	int error;

d346 1
d357 1
a357 1
		error = sbsleep(sb, lock);
@


1.81
log
@Assert that the socket lock is held when `so_qlen' is modified.

ok bluhm@@, visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.80 2017/06/27 12:02:43 mpi Exp $	*/
d92 1
a92 1

d102 1
d119 1
a119 1

d130 1
a130 1

d265 1
a265 1

d273 1
a273 1

@


1.80
log
@Add missing solock()/sounlock() dances around sbreserve().

While here document an abuse of parent socket's lock.

Problem reported by krw@@, analysis and ok bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.79 2017/06/26 09:32:31 mpi Exp $	*/
d211 1
d232 3
a234 1
	struct socket *head;
a235 1
	head = so->so_head;
@


1.79
log
@Assert that the corresponding socket is locked when manipulating socket
buffers.

This is one step towards unlocking TCP input path.  Note that all the
functions asserting for the socket lock are not necessarilly MP-safe.
All the fields of 'struct socket' aren't protected.

Introduce a new kernel-only kqueue hint, NOTE_SUBMIT, to be able to
tell when a filter needs to lock the underlying data structures.  Logic
and name taken from NetBSD.

Tested by Hrvoje Popovski.

ok claudio@@, bluhm@@, mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.78 2017/06/07 13:41:02 mpi Exp $	*/
d151 5
@


1.78
log
@Assert that the KERNEL_LOCK() is held when messing with routing,
pfkey and unix sockets.

ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.77 2017/05/27 18:50:53 claudio Exp $	*/
d439 1
a439 1
	if (sbreserve(&so->so_snd, sndcc))
d441 1
a441 1
	if (sbreserve(&so->so_rcv, rcvcc))
d453 1
a453 1
	sbrelease(&so->so_snd);
d464 1
a464 1
sbreserve(struct sockbuf *sb, u_long cc)
d466 2
d508 1
a508 1
sbrelease(struct sockbuf *sb)
d511 1
a511 1
	sbflush(sb);
d602 1
a602 1
sbappend(struct sockbuf *sb, struct mbuf *m)
d619 1
a619 1
				sbappendrecord(sb, m); /* XXXXXX!!!! */
d640 1
a640 1
sbappendstream(struct sockbuf *sb, struct mbuf *m)
d642 2
a643 1

d685 1
a685 1
sbappendrecord(struct sockbuf *sb, struct mbuf *m0)
d689 3
d768 2
a769 2
sbappendaddr(struct sockbuf *sb, struct sockaddr *asa, struct mbuf *m0,
    struct mbuf *control)
d783 1
a783 1
	if (space > sbspace(sb))
d815 2
a816 1
sbappendcontrol(struct sockbuf *sb, struct mbuf *m0, struct mbuf *control)
d831 1
a831 1
	if (space > sbspace(sb))
d912 1
a912 1
sbflush(struct sockbuf *sb)
d914 1
a914 1

d918 1
a918 1
		sbdrop(sb, (int)sb->sb_cc);
d931 1
a931 1
sbdrop(struct sockbuf *sb, int len)
d935 3
@


1.77
log
@Push the NET_LOCK down into PF_KEY so that it can be treated like PF_ROUTE.
Only pfkeyv2_send() needs the NET_LOCK() so grab it at the start and release
at the end.  This should allow to push the locks down in other places.
OK mpi@@, bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.76 2017/05/15 12:26:00 mpi Exp $	*/
d295 3
a297 3
	if ((so->so_proto->pr_domain->dom_family != PF_LOCAL) &&
	    (so->so_proto->pr_domain->dom_family != PF_ROUTE) &&
	    (so->so_proto->pr_domain->dom_family != PF_KEY))
d299 8
@


1.76
log
@Enable the NET_LOCK(), take 3.

Recursions are still marked as XXXSMP.

ok deraadt@@, bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.75 2017/03/17 17:19:16 mpi Exp $	*/
d276 2
a277 1
	    (so->so_proto->pr_domain->dom_family != PF_ROUTE))
d296 2
a297 1
	    (so->so_proto->pr_domain->dom_family != PF_ROUTE))
d305 2
a306 1
	    (so->so_proto->pr_domain->dom_family != PF_ROUTE)) {
@


1.75
log
@Revert the NET_LOCK() and bring back pf's contention lock for release.

For the moment the NET_LOCK() is always taken by threads running under
KERNEL_LOCK().  That means it doesn't buy us anything except a possible
deadlock that we did not spot.  So make sure this doesn't happen, we'll
have plenty of time in the next release cycle to stress test it.

ok visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.74 2017/03/13 20:18:21 claudio Exp $	*/
d302 5
a306 1
	return tsleep(ident, prio, wmesg, timo);
@


1.74
log
@Move PRU_ATTACH out of the pr_usrreq functions into pr_attach.
Attach is quite a different thing to the other PRU functions and
this should make locking a bit simpler. This also removes the ugly
hack on how proto was passed to the attach function.
OK bluhm@@ and mpi@@ on a previous version
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.73 2017/03/07 09:23:27 mpi Exp $	*/
d302 1
a302 5
	if ((so->so_proto->pr_domain->dom_family != PF_LOCAL) &&
	    (so->so_proto->pr_domain->dom_family != PF_ROUTE)) {
		return rwsleep(ident, &netlock, prio, wmesg, timo);
	} else
		return tsleep(ident, prio, wmesg, timo);
@


1.73
log
@Do not grab the NET_LOCK() for routing sockets operations.

The only function that need the lock is rtm_output() as it messes with
the routing table.  So grab the lock there since it is safe to sleep
in a process context.

ok bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.72 2017/02/14 09:46:21 mpi Exp $	*/
d190 1
a190 2
	if ((*so->so_proto->pr_usrreq)(so, PRU_ATTACH, NULL, NULL, NULL,
	    curproc)) {
@


1.72
log
@Wrap the NET_LOCK() into a per-socket solock() that does nothing for
unix domain sockets.

This should prevent the multiple deadlock related to unix domain sockets.

Inputs from millert@@ and bluhm@@, ok bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.71 2017/01/25 06:15:50 mpi Exp $	*/
d276 2
a277 1
	if (so->so_proto->pr_domain->dom_family != PF_LOCAL)
d295 2
a296 1
	if (so->so_proto->pr_domain->dom_family != PF_LOCAL)
d303 2
a304 1
	if (so->so_proto->pr_domain->dom_family != PF_LOCAL)
d306 1
a306 1
	else
@


1.71
log
@Enable the NET_LOCK(), take 2.

Recursions are currently known and marked a XXXSMP.

Please report any assert to bugs@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.70 2016/12/29 12:12:43 mpi Exp $	*/
d41 1
d151 1
a151 1
	NET_ASSERT_LOCKED();
d271 36
d311 1
a311 1
sbwait(struct sockbuf *sb)
d313 1
a313 1
	NET_ASSERT_LOCKED();
d316 1
a316 1
	return (rwsleep(&sb->sb_cc, &netlock,
d378 1
a378 1
	NET_ASSERT_LOCKED();
@


1.70
log
@Change NET_LOCK()/NET_UNLOCK() to be simple wrappers around
splsoftnet()/splx() until the known issues are fixed.

In other words, stop using a rwlock since it creates a deadlock when
chrome is used.

Issue reported by Dimitris Papastamos and kettenis@@

ok visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.69 2016/12/19 08:36:49 mpi Exp $	*/
d279 1
a279 1
	return (tsleep(&sb->sb_cc,
@


1.69
log
@Introduce the NET_LOCK() a rwlock used to serialize accesses to the parts
of the network stack that are not yet ready to be executed in parallel or
where new sleeping points are not possible.

This first pass replace all the entry points leading to ip_output(). This
is done to not introduce new sleeping points when trying to acquire ART's
write lock, needed when a new L2 entry is created via the RT_RESOLVE.

Inputs from and ok bluhm@@, ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.68 2016/11/15 11:57:02 bluhm Exp $	*/
d279 1
a279 1
	return (rwsleep(&sb->sb_cc, &netlock,
@


1.68
log
@Bring back the SB_LOCK and SB_WANT flags to lock the socket buffers
in process context.  The read/write lock introduced in rev 1.64
would create lock ordering problems with the upcoming SOCKET_LOCK()
mechanism.  The current tsleep() in sblock() must be replaced with
rwsleep(&socketlock) later.  The sb_flags are protected by
KERNEL_LOCK().  They must not be accessed from interrupt context,
but nowadays softnet() is not an interrupt anyway.
OK mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.67 2016/10/09 19:33:34 bluhm Exp $	*/
d56 2
d150 1
a150 1
	splsoftassert(IPL_SOFTNET);
d276 1
a276 1
	splsoftassert(IPL_SOFTNET);
d279 1
a279 1
	return (tsleep(&sb->sb_cc,
d285 14
a298 1
sblock(struct sockbuf *sb, int wait)
d313 1
a313 3
		error = tsleep(&sb->sb_flags,
		    (sb->sb_flags & SB_NOINTR) ?
		    PSOCK : PSOCK|PCATCH, "netlck", 0);
d341 1
a341 1
	splsoftassert(IPL_SOFTNET);
@


1.67
log
@sowakeup() is only called from sorwakeup() and sowwakeup().  Both
have an splsoftassert(IPL_SOFTNET) now, so sowakeup() does not need
to call splsoftnet() anymore.
From mpi@@'s netlock diff; OK mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.66 2016/10/06 19:09:08 bluhm Exp $	*/
a185 3
	rw_init(&so->so_rcv.sb_lock, "sbsndl");
	rw_init(&so->so_snd.sb_lock, "sbrcvl");

a281 4
/*
 * Lock a sockbuf already known to be locked;
 * return any error returned from sleep (EINTR).
 */
d283 1
a283 1
sblock(struct sockbuf *sb, int wf)
d287 19
a305 7
	error = rw_enter(&sb->sb_lock, RW_WRITE |
	    (sb->sb_flags & SB_NOINTR ? 0 : RW_INTR) |
	    (wf == M_WAITOK ? 0 : RW_NOSLEEP));

	if (error == EBUSY)
		error = EWOULDBLOCK;
	return (error);
d311 7
a317 1
	rw_exit(&sb->sb_lock);
a319 1

d845 1
a845 1
	rw_assert_unlocked(&sb->sb_lock);
@


1.66
log
@Remove redundant comments that say a function must be called at
splsoftnet() if the function does a splsoftassert(IPL_SOFTNET)
anyway.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.65 2016/09/02 13:28:21 bluhm Exp $	*/
d318 1
a318 1
	int s = splsoftnet();
a325 1
	splx(s);
@


1.65
log
@After allocating a single 64 KB mbuf cluster in sosend(), the sending
socket buffer had no space anymore.  The default mbuf space limit
was only 32 KB.  So no more data from user-land was accepted.  As
tcp_output() keeps the mbuf cluster for retransmits, it will be
freed only after all ACKs have been received.  That has killed our
TCP send performance totally.  To allow cycling through the mbufs
periodically, we need space for at least 3 of them.
Reported by Andreas Bartelt; testing with mikeb@@; OK mikeb@@ claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.64 2016/06/28 14:47:00 tedu Exp $	*/
a140 2
 *
 * Must be called at splsoftnet()
@


1.64
log
@introduce rwlock for socketbuf instead of the old flag and tsleep dance.
ok mikeb bluhm
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.63 2015/10/06 14:38:32 claudio Exp $	*/
d400 2
a401 1
	sb->sb_mbmax = min(cc * 2, sb_max + (sb_max / MCLBYTES) * MSIZE);
@


1.63
log
@Make sure that all padding bytes in cmsgs are actually zero by memset
CMSG_SIZE(len) bytes of the mbuf.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.62 2015/07/08 07:21:50 mpi Exp $	*/
d188 3
d292 1
a292 1
sb_lock(struct sockbuf *sb)
d296 13
a308 10
	while (sb->sb_flags & SB_LOCK) {
		sb->sb_flags |= SB_WANT;
		error = tsleep(&sb->sb_flags,
		    (sb->sb_flags & SB_NOINTR) ?
		    PSOCK : PSOCK|PCATCH, "netlck", 0);
		if (error)
			return (error);
	}
	sb->sb_flags |= SB_LOCK;
	return (0);
d311 1
d837 1
a837 1
	KASSERT((sb->sb_flags & SB_LOCK) == 0);
@


1.62
log
@MFREE(9) is dead, long live m_freem(9)!

ok bluhm@@, claudio@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.61 2015/06/30 15:30:17 mpi Exp $	*/
d941 1
@


1.61
log
@Get rid of the undocumented & temporary* m_copy() macro added for
compatibility with 4.3BSD in September 1989.

*Pick your own definition for "temporary".

ok bluhm@@, claudio@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.60 2015/03/14 03:38:51 jsg Exp $	*/
d870 1
a870 1
		MFREE(m, mn);
d875 1
a875 1
		MFREE(m, mn);
d910 1
a910 1
			MFREE(m, mn);
@


1.60
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.59 2014/12/11 19:21:57 tedu Exp $	*/
d455 1
a455 1
 * send buffer in a protocol with m_copy for output to a peer,
@


1.59
log
@convert bcopy to memcpy/memmove. ok krw
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.58 2014/09/14 14:17:26 jsg Exp $	*/
a37 1
#include <sys/buf.h>
@


1.58
log
@remove uneeded proc.h includes
ok mpi@@ kspillner@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.57 2014/09/09 02:07:17 guenther Exp $	*/
d708 1
a708 1
	bcopy(asa, mtod(m, caddr_t), asa->sa_len);
d794 1
a794 1
			bcopy(mtod(m, caddr_t), mtod(n, caddr_t) + n->m_len,
d942 1
a942 1
	bcopy(p, CMSG_DATA(cp), size);
@


1.57
log
@Delete the SS_ISCONFIRMING flag that supported delayed connection
confirmation: it was only used for netiso, which was deleted a *decade* ago

ok mpi@@ claudio@@  ports scan by sthen@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.56 2013/04/05 08:25:30 tedu Exp $	*/
a36 1
#include <sys/proc.h>
@


1.56
log
@remove some obsolete casts
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.55 2013/01/15 11:12:57 bluhm Exp $	*/
d101 1
a101 1
	so->so_state &= ~(SS_ISCONNECTING|SS_ISDISCONNECTING|SS_ISCONFIRMING);
d142 1
a142 1
 * Connstatus may be 0, or SS_ISCONFIRMING, or SS_ISCONNECTED.
@


1.55
log
@Changing the socket buffer flags sb_flags was not interrupt safe
as |= and &= are non-atomic operations.  To avoid additional locks,
put the flags that have to be accessed from interrupt into a separate
sb_flagsintr 32 bit integer field.  sb_flagsintr is protected by
splsoftnet.
Input from miod@@ deraadt@@; OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.54 2012/12/31 13:45:14 bluhm Exp $	*/
d155 1
a155 1
		return ((struct socket *)0);
d157 1
a157 1
		return ((struct socket *)0);
d160 1
a160 1
		return ((struct socket *)0);
d181 1
a181 1
		return ((struct socket *)0);
d195 1
a195 1
		return ((struct socket *)0);
d934 1
a934 1
		return ((struct mbuf *) NULL);
@


1.54
log
@Extend the sbcheck() function to make it work with socket buffers
containing m_nextpkt chains.
OK markus@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.53 2012/04/13 09:38:32 deraadt Exp $	*/
d278 1
d280 1
a280 1
	sb->sb_flags |= SB_WAIT;
d315 2
d318 3
a320 3
	sb->sb_flags &= ~SB_SEL;
	if (sb->sb_flags & SB_WAIT) {
		sb->sb_flags &= ~SB_WAIT;
d323 1
@


1.53
log
@unneccessary casts to unsigned; ok claudio
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.52 2011/04/04 21:11:22 claudio Exp $	*/
d576 1
a576 1
	struct mbuf *m;
d579 9
a587 7
	for (m = sb->sb_mb; m; m = m->m_next) {
		len += m->m_len;
		mbcnt += MSIZE;
		if (m->m_flags & M_EXT)
			mbcnt += m->m_ext.ext_size;
		if (m->m_nextpkt)
			panic("sbcheck nextpkt");
@


1.52
log
@Correctly inherit and set the watermarks on socketbuffers.
This fixes the NFS problems reported on the mailing list
and ensures that accepted sockets have correct socketbuffer
setting.  OK blambert@@, henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.51 2010/09/24 02:59:45 claudio Exp $	*/
d790 1
a790 1
			    (unsigned)m->m_len);
@


1.51
log
@TCP send and recv buffer scaling.
Send buffer is scaled by not accounting unacknowledged on the wire
data against the buffer limit. Receive buffer scaling is done similar
to FreeBSD -- measure the delay * bandwith product and base the
buffer on that. The problem is that our RTT measurment is coarse
so it overshoots on low delay links. This does not matter that much
since the recvbuffer is almost always empty.
Add a back pressure mechanism to control the amount of memory
assigned to socketbuffers that kicks in when 80% of the cluster
pool is used.
Increases the download speed from 300kB/s to 4.4MB/s on ftp.eu.openbsd.org.

Based on work by markus@@ and djm@@.

OK dlg@@, henning@@, put it in deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.50 2009/11/09 17:53:39 nicm Exp $	*/
d179 4
d184 2
d187 2
d364 2
@


1.50
log
@Every selwakeup() should have a matching KNOTE() (even if kqueue isn't
supported it doesn't do any harm), so put the KNOTE() in selwakeup() itself and
remove it from any occurences where both are used, except one for kqueue itself
and one in sys_pipe.c (where the selwakeup is under a PIPE_SEL flag).

Based on a diff from tedu.

ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.49 2009/08/10 16:49:39 thib Exp $	*/
d56 1
a150 2
	extern u_long unpst_sendspace, unpst_recvspace;
	u_long snd_sb_hiwat, rcv_sb_hiwat;
d177 1
a177 2
	 * If we are tight on mbuf clusters, create the new socket
	 * with the minimum.  Sorry, you lose.
d179 2
a180 6
	snd_sb_hiwat = head->so_snd.sb_hiwat;
	if (sbcheckreserve(snd_sb_hiwat, unpst_sendspace))
		snd_sb_hiwat = unpst_sendspace;		/* and udp? */
	rcv_sb_hiwat = head->so_rcv.sb_hiwat;
	if (sbcheckreserve(rcv_sb_hiwat, unpst_recvspace))
		rcv_sb_hiwat = unpst_recvspace;		/* and udp? */
a181 1
	(void) soreserve(so, snd_sb_hiwat, rcv_sb_hiwat);
d388 1
a388 2
 * If over 50% of mbuf clusters in use, do not accept any
 * greater than normal request.
d393 1
a393 2
	if (cnt > defcnt &&
	    mclpools[0].pr_nout> mclpools[0].pr_hardlimit / 2)
d396 14
@


1.49
log
@Don't use char arrays for sleep wchans and reuse them.
just use strings and make things unique.

ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.47 2009/03/15 19:40:41 miod Exp $	*/
a320 1
	KNOTE(&sb->sb_sel.si_note, 0);
@


1.48
log
@bzero -> PR_ZERO

ok art@@, henning@@
@
text
@a52 6
/* strings for sleep message: */
const char	netcon[] = "netcon";
const char	netcls[] = "netcls";
const char	netio[] = "netio";
const char	netlck[] = "netlck";

d280 1
a280 1
	    (sb->sb_flags & SB_NOINTR) ? PSOCK : PSOCK | PCATCH, netio,
d297 1
a297 1
		    PSOCK : PSOCK|PCATCH, netlck, 0);
@


1.47
log
@Introduce splsoftassert(), similar to splassert() but for soft interrupt
levels. This will allow for platforms where soft interrupt levels do not
map to real hardware interrupt levels to have soft ipl values overlapping
hard ipl values without breaking spl asserts.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.46 2009/01/13 13:36:12 blambert Exp $	*/
d165 1
a165 1
	so = pool_get(&socket_pool, PR_NOWAIT);
a167 1
	bzero(so, sizeof(*so));
@


1.46
log
@Change sbreserve() to return 0 on success, 1 on failure, as god intended.
This sort of breaking with traditional and expected behavior annoys me.

"yes!" henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.45 2008/11/24 12:57:37 dlg Exp $	*/
d159 1
a159 1
	splassert(IPL_SOFTNET);
@


1.45
log
@add several backend pools to allocate mbufs clusters of various sizes out
of. currently limited to MCLBYTES (2048 bytes) and 4096 bytes until pools
can allocate objects of sizes greater than PAGESIZE.

this allows drivers to ask for "jumbo" packets to fill rx rings with.

the second half of this change is per interface mbuf cluster allocator
statistics. drivers can use the new interface (MCLGETI), which will use
these stats to selectively fail allocations based on demand for mbufs. if
the driver isnt rapidly consuming rx mbufs, we dont allow it to allocate
many to put on its rx ring.

drivers require modifications to take advantage of both the new allocation
semantic and large clusters.

this was written and developed with deraadt@@ over the last two days
ok deraadt@@ claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.44 2008/05/23 15:51:12 thib Exp $	*/
d367 1
a367 1
	if (sbreserve(&so->so_snd, sndcc) == 0)
d369 1
a369 1
	if (sbreserve(&so->so_rcv, rcvcc) == 0)
d394 1
a394 1
		return (0);
d399 1
a399 1
	return (1);
@


1.44
log
@Deal with the situation when TCP nfs mounts timeout and processes
get hung in nfs_reconnect() because they do not have the proper
privilages to bind to a socket, by adding a struct proc * argument
to sobind() (and the *_usrreq() routines, and finally in{6}_pcbbind)
and do the sobind() with proc0 in nfs_connect.

OK markus@@, blambert@@.
"go ahead" deraadt@@.

Fixes an issue reported by bernd@@ (Tested by bernd@@).
Fixes PR5135 too.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.43 2007/09/19 15:08:29 blambert Exp $	*/
d61 1
a61 1
extern struct pool mclpool;
d161 1
a161 1
	if (mclpool.pr_nout > mclpool.pr_hardlimit * 95 / 100)
d410 1
a410 1
	    mclpool.pr_nout> mclpool.pr_hardlimit / 2)
@


1.43
log
@instead of relying on mbuf.h to include pool.h and declare
mclpool as an extern, do so explicitly

ok henning@@ claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.42 2007/02/26 23:53:33 kurt Exp $	*/
d197 2
a198 2
	if ((*so->so_proto->pr_usrreq)(so, PRU_ATTACH,
	    (struct mbuf *)0, (struct mbuf *)0, (struct mbuf *)0)) {
@


1.42
log
@exclude control data from the number of bytes returned by FIONREAD ioctl()
by adding a sb_datacc count to sockbuf that counts data excluding
MT_CONTROL and MT_SONAME mbuf types.  w/help from deraadt@@.
okay deraadt@@ claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.41 2006/01/05 05:05:07 jsg Exp $	*/
d47 1
d60 2
@


1.41
log
@ansi/deregister
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.40 2005/07/18 02:43:27 fgsch Exp $	*/
d783 2
d822 1
d850 2
@


1.40
log
@remove trailing newline in panic(9); ok millert@@ and deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.39 2005/05/27 17:16:13 dhartmei Exp $	*/
d91 1
a91 2
soisconnecting(so)
	register struct socket *so;
d99 1
a99 2
soisconnected(so)
	register struct socket *so;
d101 1
a101 1
	register struct socket *head = so->so_head;
d117 1
a117 2
soisdisconnecting(so)
	register struct socket *so;
d128 1
a128 2
soisdisconnected(so)
	register struct socket *so;
d260 1
a260 2
socantsendmore(so)
	struct socket *so;
d268 1
a268 2
socantrcvmore(so)
	struct socket *so;
d279 1
a279 2
sbwait(sb)
	struct sockbuf *sb;
d293 1
a293 2
sb_lock(sb)
	register struct sockbuf *sb;
d315 1
a315 3
sowakeup(so, sb)
	register struct socket *so;
	register struct sockbuf *sb;
d361 1
a361 3
soreserve(so, sndcc, rcvcc)
	register struct socket *so;
	u_long sndcc, rcvcc;
d387 1
a387 3
sbreserve(sb, cc)
	struct sockbuf *sb;
	u_long cc;
d416 1
a416 2
sbrelease(sb)
	struct sockbuf *sb;
d510 1
a510 3
sbappend(sb, m)
	struct sockbuf *sb;
	struct mbuf *m;
d512 1
a512 1
	register struct mbuf *n;
d902 1
a902 4
sbcreatecontrol(p, size, type, level)
	caddr_t p;
	register int size;
	int type, level;
d904 1
a904 1
	register struct cmsghdr *cp;
@


1.39
log
@add a field to struct socket that stores the pid of the process that
created the socket, and populate it. ok bob@@, henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.38 2004/04/25 16:25:05 markus Exp $	*/
d818 1
a818 1
			printf("semi-panic: sbcompress\n");
@


1.38
log
@change sb_mbmax to: (sb_max/MCLBYTES) * (MSIZE+MCLBYTES); ok deraadt
CV ----------------------------------------------------------------------
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.37 2004/04/19 22:38:39 deraadt Exp $	*/
d181 1
@


1.37
log
@this is only a work in progress, we can perfect afterwards, but it is time
to get some experience with these ideas.
add sbcheckreserve() api; called by accepting sockets.  if over 95% of
mbuf clusters are busy, consider this a resource starvation just like the
other reasons for accept failing.  also, if over 50% of mbuf clusters are
busy, shrink recv & send sockbuf reserves to "the minimum".
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.37 2004/04/19 22:35:31 deraadt Exp $	*/
d403 1
a403 2
	if (cc == 0 ||
	    (u_int64_t)cc > (u_int64_t)sb_max * MCLBYTES / (MSIZE + MCLBYTES))
d406 1
a406 1
	sb->sb_mbmax = min(cc * 2, sb_max);
@


1.36
log
@use NULL for ptrs.  parts from Joris Vink
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.35 2003/07/21 22:44:50 tedu Exp $	*/
d157 2
d162 2
d183 13
a195 1
	(void) soreserve(so, head->so_snd.sb_hiwat, head->so_rcv.sb_hiwat);
d411 13
@


1.35
log
@remove caddr_t casts.  it's just silly to cast something when the function
takes a void *.  convert uiomove to take a void * as well.  ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.34 2003/06/02 23:28:07 millert Exp $	*/
d502 1
a502 1
	if (m == 0)
d582 1
a582 1
	if (m0 == 0)
d593 1
a593 1
	m0->m_next = 0;
d612 1
a612 1
	if (m0 == 0)
d642 1
a642 1
	m0->m_next = 0;
d670 1
a670 1
		if (n->m_next == 0)	/* keep pointer to last control buf */
d678 1
a678 1
	if (m == 0)
d710 1
a710 1
	if (control == 0)
d714 1
a714 1
		if (m->m_next == 0)
d783 1
a783 1
		n->m_next = 0;
d824 2
a825 2
		if (m == 0) {
			if (next == 0)
@


1.34
log
@Remove the advertising clause in the UCB license which Berkeley
rescinded 22 July 1999.  Proofed by myself and Theo.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.33 2002/10/12 01:09:45 krw Exp $	*/
d110 1
a110 1
		wakeup_one((caddr_t)&head->so_timeo);
d112 1
a112 1
		wakeup((caddr_t)&so->so_timeo);
d125 1
a125 1
	wakeup((caddr_t)&so->so_timeo);
d137 1
a137 1
	wakeup((caddr_t)&so->so_timeo);
d165 1
a165 1
	bzero((caddr_t)so, sizeof(*so));
d189 1
a189 1
		wakeup((caddr_t)&head->so_timeo);
d273 1
a273 1
	return (tsleep((caddr_t)&sb->sb_cc,
d290 1
a290 1
		error = tsleep((caddr_t)&sb->sb_flags,
d314 1
a314 1
		wakeup((caddr_t)&sb->sb_cc);
d681 1
a681 1
	bcopy((caddr_t)asa, mtod(m, caddr_t), asa->sa_len);
@


1.33
log
@Remove more '\n's from panic() statements. Both trailing and leading.

Diff generated by Chris Kuethe.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.32 2002/10/10 22:27:30 art Exp $	*/
d16 1
a16 5
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.32
log
@constify a few strings. various@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.31 2002/08/26 16:39:25 dhartmei Exp $	*/
d453 1
a453 1
		panic("sblastrecordchk from %s\n", where);
@


1.31
log
@Update sb_lastrecord in sbcompress() when the mbuf pointed to is removed.
Bug report from Alistair Kerr, tested miod@@, inspected art@@, ok provos@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.30 2002/08/08 19:18:12 provos Exp $	*/
d57 4
a60 4
char	netcon[] = "netcon";
char	netcls[] = "netcls";
char	netio[] = "netio";
char	netlck[] = "netlck";
@


1.30
log
@redo socketbuf speedup.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.28 2002/08/08 17:07:32 provos Exp $	*/
d761 2
@


1.29
log
@backout the tree break. ok pb@@, art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.27 2002/06/11 05:07:43 art Exp $	*/
d438 55
d508 9
a516 3
	if ((n = sb->sb_mb) != NULL) {
		while (n->m_nextpkt)
			n = n->m_nextpkt;
d523 6
d531 21
d556 1
a556 2
sbcheck(sb)
	register struct sockbuf *sb;
d558 2
a559 2
	register struct mbuf *m;
	register int len = 0, mbcnt = 0;
d570 1
a570 1
		printf("cc %d != %d || mbcnt %d != %d\n", len, sb->sb_cc,
d582 1
a582 3
sbappendrecord(sb, m0)
	register struct sockbuf *sb;
	register struct mbuf *m0;
d584 1
a584 1
	register struct mbuf *m;
d588 1
a588 3
	if ((m = sb->sb_mb) != NULL)
		while (m->m_nextpkt)
			m = m->m_nextpkt;
d594 2
a595 4
	if (m)
		m->m_nextpkt = m0;
	else
		sb->sb_mb = m0;
d603 1
d612 1
a612 3
sbinsertoob(sb, m0)
	register struct sockbuf *sb;
	register struct mbuf *m0;
d614 1
a614 2
	register struct mbuf *m;
	register struct mbuf **mp;
d618 3
d640 4
d652 1
d662 2
a663 4
sbappendaddr(sb, asa, m0, control)
	register struct sockbuf *sb;
	struct sockaddr *asa;
	struct mbuf *m0, *control;
d665 1
a665 1
	register struct mbuf *m, *n;
d691 4
a694 1
	for (n = m; n; n = n->m_next)
d696 9
a704 6
	if ((n = sb->sb_mb) != NULL) {
		while (n->m_nextpkt)
			n = n->m_nextpkt;
		n->m_nextpkt = m;
	} else
		sb->sb_mb = m;
d709 1
a709 3
sbappendcontrol(sb, m0, control)
	struct sockbuf *sb;
	struct mbuf *m0, *control;
d711 1
a711 1
	register struct mbuf *m, *n;
d727 4
a730 1
	for (m = control; m; m = m->m_next)
d732 9
a740 6
	if ((n = sb->sb_mb) != NULL) {
		while (n->m_nextpkt)
			n = n->m_nextpkt;
		n->m_nextpkt = control;
	} else
		sb->sb_mb = control;
d780 1
d793 1
d801 1
a801 2
sbflush(sb)
	register struct sockbuf *sb;
d804 2
a805 2
	if (sb->sb_flags & SB_LOCK)
		panic("sbflush");
d808 5
a812 2
	if (sb->sb_cc || sb->sb_mb)
		panic("sbflush 2");
d819 1
a819 3
sbdrop(sb, len)
	register struct sockbuf *sb;
	register int len;
d821 1
a821 1
	register struct mbuf *m, *mn;
d854 11
d872 1
a872 2
sbdroprecord(sb)
	register struct sockbuf *sb;
d874 1
a874 1
	register struct mbuf *m, *mn;
d884 1
@


1.28
log
@socket buf speedup from thorpej@@netbsd, okay art@@ ericj@@:

Make insertion of data into socket buffers O(C):
* Keep pointers to the first and last mbufs of the last record in the
  socket buffer.
* Use the sb_lastrecord pointer in the sbappend*() family of functions
  to avoid traversing the packet chain to find the last record.
* Add a new sbappend_stream() function for stream protocols which
  guarantee that there will never be more than one record in the
  socket buffer.  This function uses the sb_mbtail pointer to perform
  the data insertion.  Make TCP use sbappend_stream(). On a profiling
run, this makes sbappend of a TCP transmission using
a 1M socket buffer go from 50% of the time to .02% of the time. Thanks
to Bill Sommerfeld and YAMAMOTO Takashi for their debugging
assistance!
@
text
@a437 55
#ifdef SOCKBUF_DEBUG
void
sblastrecordchk(struct sockbuf *sb, const char *where)
{
	struct mbuf *m = sb->sb_mb;

	while (m && m->m_nextpkt)
		m = m->m_nextpkt;

	if (m != sb->sb_lastrecord) {
		printf("sblastrecordchk: sb_mb %p sb_lastrecord %p last %p\n",
		    sb->sb_mb, sb->sb_lastrecord, m);
		printf("packet chain:\n");
		for (m = sb->sb_mb; m != NULL; m = m->m_nextpkt)
			printf("\t%p\n", m);
		panic("sblastrecordchk from %s\n", where);
	}
}

void
sblastmbufchk(struct sockbuf *sb, const char *where)
{
	struct mbuf *m = sb->sb_mb;
	struct mbuf *n;

	while (m && m->m_nextpkt)
		m = m->m_nextpkt;

	while (m && m->m_next)
		m = m->m_next;

	if (m != sb->sb_mbtail) {
		printf("sblastmbufchk: sb_mb %p sb_mbtail %p last %p\n",
		    sb->sb_mb, sb->sb_mbtail, m);
		printf("packet tree:\n");
		for (m = sb->sb_mb; m != NULL; m = m->m_nextpkt) {
			printf("\t");
			for (n = m; n != NULL; n = n->m_next)
				printf("%p ", n);
			printf("\n");
		}
		panic("sblastmbufchk from %s", where);
	}
}
#endif /* SOCKBUF_DEBUG */

#define	SBLINKRECORD(sb, m0)						\
do {									\
	if ((sb)->sb_lastrecord != NULL)				\
		(sb)->sb_lastrecord->m_nextpkt = (m0);			\
	else								\
		(sb)->sb_mb = (m0);					\
	(sb)->sb_lastrecord = (m0);					\
} while (/*CONSTCOND*/0)

d453 3
a455 9

	SBLASTRECORDCHK(sb, "sbappend 1");

	if ((n = sb->sb_lastrecord) != NULL) {
		/*
		 * XXX Would like to simply use sb_mbtail here, but
		 * XXX I need to verify that I won't miss an EOR that
		 * XXX way.
		 */
a461 6
	} else {
		/*
		 * If this is the first record in the socket buffer, it's
		 * also the last record.
		 */
		sb->sb_lastrecord = m;
a463 21
	SBLASTRECORDCHK(sb, "sbappend 2");
}

/*
 * This version of sbappend() should only be used when the caller
 * absolutely knows that there will never be more than one record
 * in the socket buffer, that is, a stream protocol (such as TCP).
 */
void
sbappendstream(struct sockbuf *sb, struct mbuf *m)
{

	KDASSERT(m->m_nextpkt == NULL);
	KASSERT(sb->sb_mb == sb->sb_lastrecord);

	SBLASTMBUFCHK(sb, __func__);

	sbcompress(sb, m, sb->sb_mbtail);

	sb->sb_lastrecord = sb->sb_mb;
	SBLASTRECORDCHK(sb, __func__);
d468 2
a469 1
sbcheck(struct sockbuf *sb)
d471 2
a472 2
	struct mbuf *m;
	u_long len = 0, mbcnt = 0;
d483 1
a483 1
		printf("cc %lu != %lu || mbcnt %lu != %lu\n", len, sb->sb_cc,
d495 3
a497 1
sbappendrecord(struct sockbuf *sb, struct mbuf *m0)
d499 1
a499 1
	struct mbuf *m;
d503 3
a505 1

d511 4
a514 2
	SBLASTRECORDCHK(sb, "sbappendrecord 1");
	SBLINKRECORD(sb, m0);
a521 1
	SBLASTRECORDCHK(sb, "sbappendrecord 2");
d530 3
a532 1
sbinsertoob(struct sockbuf *sb, struct mbuf *m0)
d534 2
a535 1
	struct mbuf *m, **mp;
a538 3

	SBLASTRECORDCHK(sb, "sbinsertoob 1");

a557 4
	if (*mp == NULL) {
		/* m0 is actually the new tail */
		sb->sb_lastrecord = m0;
	}
a565 1
	SBLASTRECORDCHK(sb, "sbinsertoob 2");
d575 4
a578 2
sbappendaddr(struct sockbuf *sb, struct sockaddr *asa, struct mbuf *m0,
    struct mbuf *control)
d580 1
a580 1
	struct mbuf *m, *n, *nlast;
d606 1
a606 4

	SBLASTRECORDCHK(sb, "sbappendaddr 1");

	for (n = m; n->m_next != NULL; n = n->m_next)
d608 6
a613 9
	sballoc(sb, n);
	nlast = n;
	SBLINKRECORD(sb, m);

	sb->sb_mbtail = nlast;
	SBLASTMBUFCHK(sb, "sbappendaddr");

	SBLASTRECORDCHK(sb, "sbappendaddr 2");

d618 3
a620 1
sbappendcontrol(struct sockbuf *sb, struct mbuf *m0, struct mbuf *control)
d622 1
a622 1
	struct mbuf *m, *mlast, *n;
d638 1
a638 4

	SBLASTRECORDCHK(sb, "sbappendcontrol 1");

	for (m = control; m->m_next != NULL; m = m->m_next)
d640 6
a645 9
	sballoc(sb, m);
	mlast = m;
	SBLINKRECORD(sb, control);

	sb->sb_mbtail = mlast;
	SBLASTMBUFCHK(sb, "sbappendcontrol");

	SBLASTRECORDCHK(sb, "sbappendcontrol 2");

a684 1
		sb->sb_mbtail = m;
a696 1
	SBLASTMBUFCHK(sb, __func__);
d704 2
a705 1
sbflush(struct sockbuf *sb)
d708 2
a709 2
	KASSERT((sb->sb_flags & SB_LOCK) == 0);

d712 2
a713 5

	KASSERT(sb->sb_cc == 0);
	KASSERT(sb->sb_mb == NULL);
	KASSERT(sb->sb_mbtail == NULL);
	KASSERT(sb->sb_lastrecord == NULL);
d720 3
a722 1
sbdrop(struct sockbuf *sb, int len)
d724 1
a724 1
	struct mbuf *m, *mn;
a756 11
	/*
	 * First part is an inline SB_EMPTY_FIXUP().  Second part
	 * makes sure sb_lastrecord is up-to-date if we dropped
	 * part of the last record.
	 */
	m = sb->sb_mb;
	if (m == NULL) {
		sb->sb_mbtail = NULL;
		sb->sb_lastrecord = NULL;
	} else if (m->m_nextpkt == NULL)
		sb->sb_lastrecord = m;
d764 2
a765 1
sbdroprecord(struct sockbuf *sb)
d767 1
a767 1
	struct mbuf *m, *mn;
a776 1
	SB_EMPTY_FIXUP(sb);
@


1.27
log
@splassert where necessary
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.26 2002/05/11 00:06:33 deraadt Exp $	*/
d438 55
d508 9
a516 3
	if ((n = sb->sb_mb) != NULL) {
		while (n->m_nextpkt)
			n = n->m_nextpkt;
d523 6
d531 21
d556 1
a556 2
sbcheck(sb)
	register struct sockbuf *sb;
d558 2
a559 2
	register struct mbuf *m;
	register int len = 0, mbcnt = 0;
d570 1
a570 1
		printf("cc %d != %d || mbcnt %d != %d\n", len, sb->sb_cc,
d582 1
a582 3
sbappendrecord(sb, m0)
	register struct sockbuf *sb;
	register struct mbuf *m0;
d584 1
a584 1
	register struct mbuf *m;
d588 1
a588 3
	if ((m = sb->sb_mb) != NULL)
		while (m->m_nextpkt)
			m = m->m_nextpkt;
d594 2
a595 4
	if (m)
		m->m_nextpkt = m0;
	else
		sb->sb_mb = m0;
d603 1
d612 1
a612 3
sbinsertoob(sb, m0)
	register struct sockbuf *sb;
	register struct mbuf *m0;
d614 1
a614 2
	register struct mbuf *m;
	register struct mbuf **mp;
d618 3
d640 4
d652 1
d662 2
a663 4
sbappendaddr(sb, asa, m0, control)
	register struct sockbuf *sb;
	struct sockaddr *asa;
	struct mbuf *m0, *control;
d665 1
a665 1
	register struct mbuf *m, *n;
d691 4
a694 1
	for (n = m; n; n = n->m_next)
d696 9
a704 6
	if ((n = sb->sb_mb) != NULL) {
		while (n->m_nextpkt)
			n = n->m_nextpkt;
		n->m_nextpkt = m;
	} else
		sb->sb_mb = m;
d709 1
a709 3
sbappendcontrol(sb, m0, control)
	struct sockbuf *sb;
	struct mbuf *m0, *control;
d711 1
a711 1
	register struct mbuf *m, *n;
d727 4
a730 1
	for (m = control; m; m = m->m_next)
d732 9
a740 6
	if ((n = sb->sb_mb) != NULL) {
		while (n->m_nextpkt)
			n = n->m_nextpkt;
		n->m_nextpkt = control;
	} else
		sb->sb_mb = control;
d780 1
d793 1
d801 1
a801 2
sbflush(sb)
	register struct sockbuf *sb;
d804 2
a805 2
	if (sb->sb_flags & SB_LOCK)
		panic("sbflush");
d808 5
a812 2
	if (sb->sb_cc || sb->sb_mb)
		panic("sbflush 2");
d819 1
a819 3
sbdrop(sb, len)
	register struct sockbuf *sb;
	register int len;
d821 1
a821 1
	register struct mbuf *m, *mn;
d854 11
d872 1
a872 2
sbdroprecord(sb)
	register struct sockbuf *sb;
d874 1
a874 1
	register struct mbuf *m, *mn;
d884 1
@


1.26
log
@track egid/rgid on bound/connected sockets too (pf will use this)
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.25 2001/11/30 19:48:09 provos Exp $	*/
d157 1
a157 3
sonewconn(head, connstatus)
	struct socket *head;
	int connstatus;
d161 2
@


1.25
log
@sbcompress() can compact mbuf clusters now; from thorpej@@netbsd
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.24 2001/11/28 13:49:08 provos Exp $	*/
d179 2
@


1.25.2.1
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.25 2001/11/30 19:48:09 provos Exp $	*/
a178 2
	so->so_egid = head->so_egid;
	so->so_rgid = head->so_rgid;
@


1.25.2.2
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.25.2.1 2002/06/11 03:29:40 art Exp $	*/
d57 4
a60 4
const char	netcon[] = "netcon";
const char	netcls[] = "netcls";
const char	netio[] = "netio";
const char	netlck[] = "netlck";
d157 3
a159 1
sonewconn(struct socket *head, int connstatus)
a163 2
	splassert(IPL_SOFTNET);

a437 55
#ifdef SOCKBUF_DEBUG
void
sblastrecordchk(struct sockbuf *sb, const char *where)
{
	struct mbuf *m = sb->sb_mb;

	while (m && m->m_nextpkt)
		m = m->m_nextpkt;

	if (m != sb->sb_lastrecord) {
		printf("sblastrecordchk: sb_mb %p sb_lastrecord %p last %p\n",
		    sb->sb_mb, sb->sb_lastrecord, m);
		printf("packet chain:\n");
		for (m = sb->sb_mb; m != NULL; m = m->m_nextpkt)
			printf("\t%p\n", m);
		panic("sblastrecordchk from %s", where);
	}
}

void
sblastmbufchk(struct sockbuf *sb, const char *where)
{
	struct mbuf *m = sb->sb_mb;
	struct mbuf *n;

	while (m && m->m_nextpkt)
		m = m->m_nextpkt;

	while (m && m->m_next)
		m = m->m_next;

	if (m != sb->sb_mbtail) {
		printf("sblastmbufchk: sb_mb %p sb_mbtail %p last %p\n",
		    sb->sb_mb, sb->sb_mbtail, m);
		printf("packet tree:\n");
		for (m = sb->sb_mb; m != NULL; m = m->m_nextpkt) {
			printf("\t");
			for (n = m; n != NULL; n = n->m_next)
				printf("%p ", n);
			printf("\n");
		}
		panic("sblastmbufchk from %s", where);
	}
}
#endif /* SOCKBUF_DEBUG */

#define	SBLINKRECORD(sb, m0)						\
do {									\
	if ((sb)->sb_lastrecord != NULL)				\
		(sb)->sb_lastrecord->m_nextpkt = (m0);			\
	else								\
		(sb)->sb_mb = (m0);					\
	(sb)->sb_lastrecord = (m0);					\
} while (/*CONSTCOND*/0)

d453 3
a455 9

	SBLASTRECORDCHK(sb, "sbappend 1");

	if ((n = sb->sb_lastrecord) != NULL) {
		/*
		 * XXX Would like to simply use sb_mbtail here, but
		 * XXX I need to verify that I won't miss an EOR that
		 * XXX way.
		 */
a461 6
	} else {
		/*
		 * If this is the first record in the socket buffer, it's
		 * also the last record.
		 */
		sb->sb_lastrecord = m;
a463 21
	SBLASTRECORDCHK(sb, "sbappend 2");
}

/*
 * This version of sbappend() should only be used when the caller
 * absolutely knows that there will never be more than one record
 * in the socket buffer, that is, a stream protocol (such as TCP).
 */
void
sbappendstream(struct sockbuf *sb, struct mbuf *m)
{

	KDASSERT(m->m_nextpkt == NULL);
	KASSERT(sb->sb_mb == sb->sb_lastrecord);

	SBLASTMBUFCHK(sb, __func__);

	sbcompress(sb, m, sb->sb_mbtail);

	sb->sb_lastrecord = sb->sb_mb;
	SBLASTRECORDCHK(sb, __func__);
d468 2
a469 1
sbcheck(struct sockbuf *sb)
d471 2
a472 2
	struct mbuf *m;
	u_long len = 0, mbcnt = 0;
d483 1
a483 1
		printf("cc %lu != %lu || mbcnt %lu != %lu\n", len, sb->sb_cc,
d495 3
a497 1
sbappendrecord(struct sockbuf *sb, struct mbuf *m0)
d499 1
a499 1
	struct mbuf *m;
d503 3
a505 1

d511 4
a514 2
	SBLASTRECORDCHK(sb, "sbappendrecord 1");
	SBLINKRECORD(sb, m0);
a521 1
	SBLASTRECORDCHK(sb, "sbappendrecord 2");
d530 3
a532 1
sbinsertoob(struct sockbuf *sb, struct mbuf *m0)
d534 2
a535 1
	struct mbuf *m, **mp;
a538 3

	SBLASTRECORDCHK(sb, "sbinsertoob 1");

a557 4
	if (*mp == NULL) {
		/* m0 is actually the new tail */
		sb->sb_lastrecord = m0;
	}
a565 1
	SBLASTRECORDCHK(sb, "sbinsertoob 2");
d575 4
a578 2
sbappendaddr(struct sockbuf *sb, struct sockaddr *asa, struct mbuf *m0,
    struct mbuf *control)
d580 1
a580 1
	struct mbuf *m, *n, *nlast;
d606 1
a606 4

	SBLASTRECORDCHK(sb, "sbappendaddr 1");

	for (n = m; n->m_next != NULL; n = n->m_next)
d608 6
a613 9
	sballoc(sb, n);
	nlast = n;
	SBLINKRECORD(sb, m);

	sb->sb_mbtail = nlast;
	SBLASTMBUFCHK(sb, "sbappendaddr");

	SBLASTRECORDCHK(sb, "sbappendaddr 2");

d618 3
a620 1
sbappendcontrol(struct sockbuf *sb, struct mbuf *m0, struct mbuf *control)
d622 1
a622 1
	struct mbuf *m, *mlast, *n;
d638 1
a638 4

	SBLASTRECORDCHK(sb, "sbappendcontrol 1");

	for (m = control; m->m_next != NULL; m = m->m_next)
d640 6
a645 9
	sballoc(sb, m);
	mlast = m;
	SBLINKRECORD(sb, control);

	sb->sb_mbtail = mlast;
	SBLASTMBUFCHK(sb, "sbappendcontrol");

	SBLASTRECORDCHK(sb, "sbappendcontrol 2");

a665 2
			if (sb->sb_lastrecord == m)
				sb->sb_lastrecord = m->m_next;
a684 1
		sb->sb_mbtail = m;
a696 1
	SBLASTMBUFCHK(sb, __func__);
d704 2
a705 1
sbflush(struct sockbuf *sb)
d708 2
a709 2
	KASSERT((sb->sb_flags & SB_LOCK) == 0);

d712 2
a713 5

	KASSERT(sb->sb_cc == 0);
	KASSERT(sb->sb_mb == NULL);
	KASSERT(sb->sb_mbtail == NULL);
	KASSERT(sb->sb_lastrecord == NULL);
d720 3
a722 1
sbdrop(struct sockbuf *sb, int len)
d724 1
a724 1
	struct mbuf *m, *mn;
a756 11
	/*
	 * First part is an inline SB_EMPTY_FIXUP().  Second part
	 * makes sure sb_lastrecord is up-to-date if we dropped
	 * part of the last record.
	 */
	m = sb->sb_mb;
	if (m == NULL) {
		sb->sb_mbtail = NULL;
		sb->sb_lastrecord = NULL;
	} else if (m->m_nextpkt == NULL)
		sb->sb_lastrecord = m;
d764 2
a765 1
sbdroprecord(struct sockbuf *sb)
d767 1
a767 1
	struct mbuf *m, *mn;
a776 1
	SB_EMPTY_FIXUP(sb);
@


1.24
log
@avoid "thundering herd" problem in accept by waking just one process.
based on freebsd.  okay art@@ markus@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.23 2001/11/28 02:28:55 provos Exp $	*/
d653 1
a653 3
sbcompress(sb, m, n)
	register struct sockbuf *sb;
	register struct mbuf *m, *n;
d655 2
a656 2
	register int eor = 0;
	register struct mbuf *o;
d667 4
a670 2
		if (n && (n->m_flags & (M_EXT | M_EOR)) == 0 &&
		    (n->m_data + n->m_len + m->m_len) < &n->m_dat[MLEN] &&
@


1.23
log
@from enami@@netbsd:
Give different names for different wait channels
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.22 2001/11/27 22:53:19 provos Exp $	*/
d114 1
a114 1
		wakeup((caddr_t)&head->so_timeo);
@


1.22
log
@change socket allocation to pool allocator; from netbsd; okay niklas@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.21 2001/11/27 15:51:36 provos Exp $	*/
a56 1
char	netio[] = "netio";
d59 2
d294 1
a294 1
		    PSOCK : PSOCK|PCATCH, netio, 0);
@


1.21
log
@change socket connection queues to use TAILQ_

from NetBSD:
Wed Jan  7 23:47:08 1998 UTC by thorpej

Make insertion and removal of sockets from the partial and incoming
connections queues O(C) rather than O(N).
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.20 2001/09/26 03:39:59 deraadt Exp $	*/
d152 2
d165 1
a165 1
	MALLOC(so, struct socket *, sizeof(*so), M_SOCKET, M_DONTWAIT);
d185 1
a185 1
		(void) free((caddr_t)so, M_SOCKET);
@


1.20
log
@At sonewconn() time, copy so_siguid & so_sigeuid to the newly created socket.
Stops a nasty little program supplied by gustavo@@core-sdi.com
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.19 2001/07/05 08:10:30 art Exp $	*/
d195 1
a195 3
soqinsque(head, so, q)
	register struct socket *head, *so;
	int q;
d198 5
a202 1
	register struct socket **prev;
d206 1
a206 3
		so->so_q0 = 0;
		for (prev = &(head->so_q0); *prev; )
			prev = &((*prev)->so_q0);
d209 1
a209 3
		so->so_q = 0;
		for (prev = &(head->so_q); *prev; )
			prev = &((*prev)->so_q);
d211 1
a211 1
	*prev = so;
d215 1
a215 3
soqremque(so, q)
	register struct socket *so;
	int q;
d217 1
a217 1
	register struct socket *head, *prev, *next;
d220 2
a221 6
	prev = head;
	for (;;) {
		next = q ? prev->so_q : prev->so_q0;
		if (next == so)
			break;
		if (next == 0)
a222 4
		prev = next;
	}
	if (q == 0) {
		prev->so_q0 = next->so_q0;
d225 2
a226 1
		prev->so_q = next->so_q;
d229 3
a231 2
	next->so_q0 = next->so_q = 0;
	next->so_head = 0;
@


1.19
log
@It feels a bit pointless to have:
#define        sonewconn(head, connstatus)     sonewconn1((head), (connstatus))

Just wastes preprocessor time.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.18 2001/06/22 14:14:10 deraadt Exp $	*/
d176 2
@


1.18
log
@KNF
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.17 2001/05/26 04:38:32 angelos Exp $	*/
a151 3
 *
 * Currently, sonewconn() is defined as sonewconn1() in socketvar.h
 * to catch calls that are missing the (new) second parameter.
d154 2
a155 2
sonewconn1(head, connstatus)
	register struct socket *head;
d158 1
a158 1
	register struct socket *so;
@


1.17
log
@Style.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.16 2001/05/02 08:33:49 provos Exp $	*/
d84 1
a84 1
 * 
d167 1
a167 1
	if (so == NULL) 
d288 1
a288 1
/* 
d300 3
a302 3
		error = tsleep((caddr_t)&sb->sb_flags, 
			       (sb->sb_flags & SB_NOINTR) ?
					PSOCK : PSOCK|PCATCH, netio, 0);
d672 2
a673 2
		     (((o = m->m_next) || (o = n)) &&
		      o->m_type == m->m_type))) {
@


1.16
log
@prevent overflow in sbreserve; from wollman@@freebsd via netbsd
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.15 2000/11/16 20:02:19 provos Exp $	*/
d589 2
a590 2
if (m0 && (m0->m_flags & M_PKTHDR) == 0)
panic("sbappendaddr");
@


1.15
log
@support kernel event queues, from FreeBSD by Jonathan Lemon,
okay art@@, millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.14 2000/02/29 19:16:46 itojun Exp $	*/
d397 2
a398 1
	if (cc == 0 || cc > sb_max * MCLBYTES / (MSIZE + MCLBYTES))
@


1.14
log
@more fix to ancillary data alignment.  we need padding after
last cmsg_data item (see the figure on RFC2292 page 18).
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.13 2000/02/18 05:21:01 itojun Exp $	*/
d50 1
d328 1
@


1.13
log
@fix alignment problem in ancillary data (alpha).

only ipv6 tools (which touches ancillary data) are affected.

From: =?iso-8859-1?Q?G=F6ran_Bengtson?= <goeran@@cdg.chalmers.se>
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.12 2000/02/04 20:32:04 angelos Exp $	*/
d795 1
a795 1
	if (CMSG_LEN(size) > MCLBYTES) {
d802 1
a802 1
	if (CMSG_LEN(size) > MLEN) {
d811 2
a812 3
	size = CMSG_LEN(size);
	m->m_len = size;
	cp->cmsg_len = size;
@


1.12
log
@Fix misleading comment.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.11 1999/12/08 06:50:17 itojun Exp $	*/
d795 1
a795 1
	if (size + sizeof(*cp) > MCLBYTES) {
d802 1
a802 1
	if (size + sizeof(*cp) > MLEN) {
d811 1
a811 1
	size += sizeof(*cp);
@


1.12.2.1
log
@Merge in recent code from the trunk
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.13 2000/02/18 05:21:01 itojun Exp $	*/
d795 1
a795 1
	if (CMSG_LEN(size) > MCLBYTES) {
d802 1
a802 1
	if (CMSG_LEN(size) > MLEN) {
d811 1
a811 1
	size = CMSG_LEN(size);
@


1.12.2.2
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d795 1
a795 1
	if (CMSG_SPACE(size) > MCLBYTES) {
d802 1
a802 1
	if (CMSG_SPACE(size) > MLEN) {
d811 3
a813 2
	m->m_len = CMSG_SPACE(size);
	cp->cmsg_len = CMSG_LEN(size);
@


1.12.2.3
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.15 2000/11/16 20:02:19 provos Exp $	*/
a49 1
#include <sys/event.h>
a326 1
	KNOTE(&sb->sb_sel.si_note, 0);
@


1.12.2.4
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.12.2.3 2001/05/14 22:32:45 niklas Exp $	*/
d84 1
a84 1
 *
d167 1
a167 1
	if (so == NULL)
d288 1
a288 1
/*
d300 3
a302 3
		error = tsleep((caddr_t)&sb->sb_flags,
		    (sb->sb_flags & SB_NOINTR) ?
		    PSOCK : PSOCK|PCATCH, netio, 0);
d397 1
a397 2
	if (cc == 0 ||
	    (u_int64_t)cc > (u_int64_t)sb_max * MCLBYTES / (MSIZE + MCLBYTES))
d588 2
a589 2
	if (m0 && (m0->m_flags & M_PKTHDR) == 0)
		panic("sbappendaddr");
d671 2
a672 2
		    (((o = m->m_next) || (o = n)) &&
		    o->m_type == m->m_type))) {
@


1.12.2.5
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.12.2.4 2001/07/04 10:48:45 niklas Exp $	*/
d152 3
d157 2
a158 2
sonewconn(head, connstatus)
	struct socket *head;
d161 1
a161 1
	struct socket *so;
a178 2
	so->so_siguid = head->so_siguid;
	so->so_sigeuid = head->so_sigeuid;
@


1.12.2.6
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d57 1
a59 2
char	netio[] = "netio";
char	netlck[] = "netlck";
d113 1
a113 1
		wakeup_one((caddr_t)&head->so_timeo);
a151 2
 *
 * Must be called at splsoftnet()
d163 1
a163 1
	so = pool_get(&socket_pool, PR_NOWAIT);
d183 1
a183 1
		pool_put(&socket_pool, so);
d195 3
a197 1
soqinsque(struct socket *head, struct socket *so, int q)
d200 1
a200 5
#ifdef DIAGNOSTIC
	if (so->so_onq != NULL)
		panic("soqinsque");
#endif

d204 3
a206 1
		so->so_onq = &head->so_q0;
d209 3
a211 1
		so->so_onq = &head->so_q;
d213 1
a213 1
	TAILQ_INSERT_TAIL(so->so_onq, so, so_qe);
d217 3
a219 1
soqremque(struct socket *so, int q)
d221 1
a221 1
	struct socket *head;
d224 9
d234 1
a234 2
		if (so->so_onq != &head->so_q0)
			return (0);
d237 1
a237 2
		if (so->so_onq != &head->so_q)
			return (0);
d240 2
a241 3
	TAILQ_REMOVE(so->so_onq, so, so_qe);
	so->so_onq = NULL;
	so->so_head = NULL;
d301 1
a301 1
		    PSOCK : PSOCK|PCATCH, netlck, 0);
d660 3
a662 1
sbcompress(struct sockbuf *sb, struct mbuf *m, struct mbuf *n)
d664 2
a665 2
	int eor = 0;
	struct mbuf *o;
d676 2
a677 4
		if (n && (n->m_flags & M_EOR) == 0 &&
		    /* M_TRAILINGSPACE() checks buffer writeability */
		    m->m_len <= MCLBYTES / 4 && /* XXX Don't copy too much */
		    m->m_len <= M_TRAILINGSPACE(n) &&
@


1.12.2.7
log
@Sync the SMP branch with 3.3
@
text
@d57 4
a60 4
const char	netcon[] = "netcon";
const char	netcls[] = "netcls";
const char	netio[] = "netio";
const char	netlck[] = "netlck";
d157 3
a159 1
sonewconn(struct socket *head, int connstatus)
a163 2
	splassert(IPL_SOFTNET);

a178 2
	so->so_egid = head->so_egid;
	so->so_rgid = head->so_rgid;
a435 55
#ifdef SOCKBUF_DEBUG
void
sblastrecordchk(struct sockbuf *sb, const char *where)
{
	struct mbuf *m = sb->sb_mb;

	while (m && m->m_nextpkt)
		m = m->m_nextpkt;

	if (m != sb->sb_lastrecord) {
		printf("sblastrecordchk: sb_mb %p sb_lastrecord %p last %p\n",
		    sb->sb_mb, sb->sb_lastrecord, m);
		printf("packet chain:\n");
		for (m = sb->sb_mb; m != NULL; m = m->m_nextpkt)
			printf("\t%p\n", m);
		panic("sblastrecordchk from %s", where);
	}
}

void
sblastmbufchk(struct sockbuf *sb, const char *where)
{
	struct mbuf *m = sb->sb_mb;
	struct mbuf *n;

	while (m && m->m_nextpkt)
		m = m->m_nextpkt;

	while (m && m->m_next)
		m = m->m_next;

	if (m != sb->sb_mbtail) {
		printf("sblastmbufchk: sb_mb %p sb_mbtail %p last %p\n",
		    sb->sb_mb, sb->sb_mbtail, m);
		printf("packet tree:\n");
		for (m = sb->sb_mb; m != NULL; m = m->m_nextpkt) {
			printf("\t");
			for (n = m; n != NULL; n = n->m_next)
				printf("%p ", n);
			printf("\n");
		}
		panic("sblastmbufchk from %s", where);
	}
}
#endif /* SOCKBUF_DEBUG */

#define	SBLINKRECORD(sb, m0)						\
do {									\
	if ((sb)->sb_lastrecord != NULL)				\
		(sb)->sb_lastrecord->m_nextpkt = (m0);			\
	else								\
		(sb)->sb_mb = (m0);					\
	(sb)->sb_lastrecord = (m0);					\
} while (/*CONSTCOND*/0)

d451 3
a453 9

	SBLASTRECORDCHK(sb, "sbappend 1");

	if ((n = sb->sb_lastrecord) != NULL) {
		/*
		 * XXX Would like to simply use sb_mbtail here, but
		 * XXX I need to verify that I won't miss an EOR that
		 * XXX way.
		 */
a459 6
	} else {
		/*
		 * If this is the first record in the socket buffer, it's
		 * also the last record.
		 */
		sb->sb_lastrecord = m;
a461 21
	SBLASTRECORDCHK(sb, "sbappend 2");
}

/*
 * This version of sbappend() should only be used when the caller
 * absolutely knows that there will never be more than one record
 * in the socket buffer, that is, a stream protocol (such as TCP).
 */
void
sbappendstream(struct sockbuf *sb, struct mbuf *m)
{

	KDASSERT(m->m_nextpkt == NULL);
	KASSERT(sb->sb_mb == sb->sb_lastrecord);

	SBLASTMBUFCHK(sb, __func__);

	sbcompress(sb, m, sb->sb_mbtail);

	sb->sb_lastrecord = sb->sb_mb;
	SBLASTRECORDCHK(sb, __func__);
d466 2
a467 1
sbcheck(struct sockbuf *sb)
d469 2
a470 2
	struct mbuf *m;
	u_long len = 0, mbcnt = 0;
d481 1
a481 1
		printf("cc %lu != %lu || mbcnt %lu != %lu\n", len, sb->sb_cc,
d493 3
a495 1
sbappendrecord(struct sockbuf *sb, struct mbuf *m0)
d497 1
a497 1
	struct mbuf *m;
d501 3
a503 1

d509 4
a512 2
	SBLASTRECORDCHK(sb, "sbappendrecord 1");
	SBLINKRECORD(sb, m0);
a519 1
	SBLASTRECORDCHK(sb, "sbappendrecord 2");
d528 3
a530 1
sbinsertoob(struct sockbuf *sb, struct mbuf *m0)
d532 2
a533 1
	struct mbuf *m, **mp;
a536 3

	SBLASTRECORDCHK(sb, "sbinsertoob 1");

a555 4
	if (*mp == NULL) {
		/* m0 is actually the new tail */
		sb->sb_lastrecord = m0;
	}
a563 1
	SBLASTRECORDCHK(sb, "sbinsertoob 2");
d573 4
a576 2
sbappendaddr(struct sockbuf *sb, struct sockaddr *asa, struct mbuf *m0,
    struct mbuf *control)
d578 1
a578 1
	struct mbuf *m, *n, *nlast;
d604 1
a604 4

	SBLASTRECORDCHK(sb, "sbappendaddr 1");

	for (n = m; n->m_next != NULL; n = n->m_next)
d606 6
a611 9
	sballoc(sb, n);
	nlast = n;
	SBLINKRECORD(sb, m);

	sb->sb_mbtail = nlast;
	SBLASTMBUFCHK(sb, "sbappendaddr");

	SBLASTRECORDCHK(sb, "sbappendaddr 2");

d616 3
a618 1
sbappendcontrol(struct sockbuf *sb, struct mbuf *m0, struct mbuf *control)
d620 1
a620 1
	struct mbuf *m, *mlast, *n;
d636 1
a636 4

	SBLASTRECORDCHK(sb, "sbappendcontrol 1");

	for (m = control; m->m_next != NULL; m = m->m_next)
d638 6
a643 9
	sballoc(sb, m);
	mlast = m;
	SBLINKRECORD(sb, control);

	sb->sb_mbtail = mlast;
	SBLASTMBUFCHK(sb, "sbappendcontrol");

	SBLASTRECORDCHK(sb, "sbappendcontrol 2");

a663 2
			if (sb->sb_lastrecord == m)
				sb->sb_lastrecord = m->m_next;
a682 1
		sb->sb_mbtail = m;
a694 1
	SBLASTMBUFCHK(sb, __func__);
d702 2
a703 1
sbflush(struct sockbuf *sb)
d706 2
a707 2
	KASSERT((sb->sb_flags & SB_LOCK) == 0);

d710 2
a711 5

	KASSERT(sb->sb_cc == 0);
	KASSERT(sb->sb_mb == NULL);
	KASSERT(sb->sb_mbtail == NULL);
	KASSERT(sb->sb_lastrecord == NULL);
d718 3
a720 1
sbdrop(struct sockbuf *sb, int len)
d722 1
a722 1
	struct mbuf *m, *mn;
a754 11
	/*
	 * First part is an inline SB_EMPTY_FIXUP().  Second part
	 * makes sure sb_lastrecord is up-to-date if we dropped
	 * part of the last record.
	 */
	m = sb->sb_mb;
	if (m == NULL) {
		sb->sb_mbtail = NULL;
		sb->sb_lastrecord = NULL;
	} else if (m->m_nextpkt == NULL)
		sb->sb_lastrecord = m;
d762 2
a763 1
sbdroprecord(struct sockbuf *sb)
d765 1
a765 1
	struct mbuf *m, *mn;
a774 1
	SB_EMPTY_FIXUP(sb);
@


1.12.2.8
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.12.2.7 2003/03/28 00:41:27 niklas Exp $	*/
d16 5
a20 1
 * 3. Neither the name of the University nor the names of its contributors
@


1.12.2.9
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d110 1
a110 1
		wakeup_one(&head->so_timeo);
d112 1
a112 1
		wakeup(&so->so_timeo);
d125 1
a125 1
	wakeup(&so->so_timeo);
d137 1
a137 1
	wakeup(&so->so_timeo);
d165 1
a165 1
	bzero(so, sizeof(*so));
d189 1
a189 1
		wakeup(&head->so_timeo);
d273 1
a273 1
	return (tsleep(&sb->sb_cc,
d290 1
a290 1
		error = tsleep(&sb->sb_flags,
d314 1
a314 1
		wakeup(&sb->sb_cc);
d681 1
a681 1
	bcopy(asa, mtod(m, caddr_t), asa->sa_len);
@


1.12.2.10
log
@Merge with the trunk
@
text
@a156 2
	extern u_long unpst_sendspace, unpst_recvspace;
	u_long snd_sb_hiwat, rcv_sb_hiwat;
a159 2
	if (mclpool.pr_nout > mclpool.pr_hardlimit * 95 / 100)
		return ((struct socket *)0);
d179 1
a179 13

	/*
	 * If we are tight on mbuf clusters, create the new socket
	 * with the minimum.  Sorry, you lose.
	 */
	snd_sb_hiwat = head->so_snd.sb_hiwat;
	if (sbcheckreserve(snd_sb_hiwat, unpst_sendspace))
		snd_sb_hiwat = unpst_sendspace;		/* and udp? */
	rcv_sb_hiwat = head->so_rcv.sb_hiwat;
	if (sbcheckreserve(rcv_sb_hiwat, unpst_recvspace))
		rcv_sb_hiwat = unpst_recvspace;		/* and udp? */

	(void) soreserve(so, snd_sb_hiwat, rcv_sb_hiwat);
d387 2
a388 1
	if (cc == 0 || cc > sb_max)
d391 1
a391 1
	sb->sb_mbmax = min(cc * 2, sb_max + (sb_max / MCLBYTES) * MSIZE);
a397 13
 * If over 50% of mbuf clusters in use, do not accept any
 * greater than normal request.
 */
int
sbcheckreserve(u_long cnt, u_long defcnt)
{
	if (cnt > defcnt &&
	    mclpool.pr_nout> mclpool.pr_hardlimit / 2)
		return (ENOBUFS);
	return (0);
}

/*
d502 1
a502 1
	if (m == NULL)
d582 1
a582 1
	if (m0 == NULL)
d593 1
a593 1
	m0->m_next = NULL;
d612 1
a612 1
	if (m0 == NULL)
d642 1
a642 1
	m0->m_next = NULL;
d670 1
a670 1
		if (n->m_next == NULL)	/* keep pointer to last control buf */
d678 1
a678 1
	if (m == NULL)
d710 1
a710 1
	if (control == NULL)
d714 1
a714 1
		if (m->m_next == NULL)
d783 1
a783 1
		n->m_next = NULL;
d824 2
a825 2
		if (m == NULL) {
			if (next == NULL)
@


1.11
log
@bring in KAME IPv6 code, dated 19991208.
replaces NRL IPv6 layer.  reuses NRL pcb layer.  no IPsec-on-v6 support.
see sys/netinet6/{TODO,IMPLEMENTATION} for more details.

GENERIC configuration should work fine as before.  GENERIC.v6 works fine
as well, but you'll need KAME userland tools to play with IPv6 (will be
bringed into soon).
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.10 1999/02/19 15:06:52 millert Exp $	*/
d148 1
a148 1
 * then we allocate a new structure, propoerly linked into the
d150 1
a150 1
 * Connstatus may be 0, or SO_ISCONFIRMING, or SO_ISCONNECTED.
@


1.10
log
@fixed patch for accept/select race; mycroft@@netbsd.org
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.8 1999/01/21 03:27:42 millert Exp $	*/
d780 37
@


1.9
log
@undo select/accept patch, which causes full listen queues apparently
@
text
@d138 1
a138 1
	so->so_state |= (SS_CANTRCVMORE|SS_CANTSENDMORE);
@


1.8
log
@Fixes select(2)/accept(2) race condition which permits DoS; mycroft@@netbsd.org
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.7 1998/02/14 10:55:09 deraadt Exp $	*/
d138 1
a138 1
	so->so_state |= (SS_CANTRCVMORE|SS_CANTSENDMORE|SS_ISDISCONNECTED);
@


1.7
log
@add seperate so_euid & so_ruid to struct socket, so that identd is still fast.. Sigh. I will change this again later
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.6 1997/08/31 20:42:26 deraadt Exp $	*/
d138 1
a138 1
	so->so_state |= (SS_CANTRCVMORE|SS_CANTSENDMORE);
@


1.6
log
@for non-tty TIOCSPGRP/F_SETOWN/FIOSETOWN pgid setting calls, store uid
and euid as well, then deliver them using new csignal() interface
which ensures that pgid setting process is permitted to signal the
pgid process(es). Thanks to newsham@@aloha.net for extensive help and
discussion.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.5 1997/02/21 08:45:00 deraadt Exp $	*/
d176 2
a177 1
	so->so_uid = head->so_uid;
@


1.5
log
@do not allow SO_SNDBUF/SO_RCVBUF len 0
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.4 1996/09/20 22:53:10 deraadt Exp $	*/
a317 2
	struct proc *p;

d324 2
a325 6
	if (so->so_state & SS_ASYNC) {
		if (so->so_pgid < 0)
			gsignal(-so->so_pgid, SIGIO);
		else if (so->so_pgid > 0 && (p = pfind(so->so_pgid)) != 0)
			psignal(p, SIGIO);
	}
@


1.4
log
@`solve' the syn bomb problem as well as currently known; add sysctl's for
SOMAXCONN (kern.somaxconn), SOMINCONN (kern.sominconn), and TCPTV_KEEP_INIT
(net.inet.tcp.keepinittime). when this is not enough (ie. overfull), start
doing tail drop, but slightly prefer the same port.
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.3 1996/08/24 04:56:37 deraadt Exp $	*/
d400 1
a400 1
	if (cc > sb_max * MCLBYTES / (MSIZE + MCLBYTES))
@


1.3
log
@change to so_uid, also fix a missing credential found by dm
@
text
@d1 1
a1 1
/*	$OpenBSD: uipc_socket2.c,v 1.2 1996/03/03 17:20:20 niklas Exp $	*/
d163 1
a163 1
	if (head->so_qlen + head->so_q0len > 3 * head->so_qlimit / 2)
@


1.2
log
@From NetBSD: 960217 merge
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d176 1
@


1.1
log
@Initial revision
@
text
@d1 2
a2 1
/*	$NetBSD: uipc_socket2.c,v 1.10 1995/08/16 01:03:19 mycroft Exp $	*/
d49 1
d297 4
a300 3
		if (error = tsleep((caddr_t)&sb->sb_flags, 
		    (sb->sb_flags & SB_NOINTR) ? PSOCK : PSOCK|PCATCH,
		    netio, 0))
d460 1
a460 1
	if (n = sb->sb_mb) {
d510 1
a510 1
	if (m = sb->sb_mb)
d546 1
a546 1
	for (mp = &sb->sb_mb; m = *mp; mp = &((*mp)->m_nextpkt)) {
d554 1
a554 1
			if (m = m->m_next)
d615 1
a615 1
	if (n = sb->sb_mb) {
d647 1
a647 1
	if (n = sb->sb_mb) {
d782 1
a782 1
		} while (m = mn);
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@
