head	1.182;
access;
symbols
	OPENBSD_6_1:1.180.0.4
	OPENBSD_6_1_BASE:1.180
	OPENBSD_6_0:1.175.0.2
	OPENBSD_6_0_BASE:1.175
	OPENBSD_5_9:1.171.0.2
	OPENBSD_5_9_BASE:1.171
	OPENBSD_5_8:1.170.0.4
	OPENBSD_5_8_BASE:1.170
	OPENBSD_5_7:1.168.0.2
	OPENBSD_5_7_BASE:1.168
	OPENBSD_5_6:1.160.0.4
	OPENBSD_5_6_BASE:1.160
	OPENBSD_5_5:1.154.0.4
	OPENBSD_5_5_BASE:1.154
	OPENBSD_5_4:1.151.0.2
	OPENBSD_5_4_BASE:1.151
	OPENBSD_5_3:1.146.0.2
	OPENBSD_5_3_BASE:1.146
	OPENBSD_5_2:1.136.0.2
	OPENBSD_5_2_BASE:1.136
	OPENBSD_5_1_BASE:1.134
	OPENBSD_5_1:1.134.0.2
	OPENBSD_5_0:1.133.0.2
	OPENBSD_5_0_BASE:1.133
	OPENBSD_4_9:1.127.0.2
	OPENBSD_4_9_BASE:1.127
	OPENBSD_4_8:1.126.0.2
	OPENBSD_4_8_BASE:1.126
	OPENBSD_4_7:1.121.0.2
	OPENBSD_4_7_BASE:1.121
	OPENBSD_4_6:1.118.0.4
	OPENBSD_4_6_BASE:1.118
	OPENBSD_4_5:1.110.0.2
	OPENBSD_4_5_BASE:1.110
	OPENBSD_4_4:1.107.0.2
	OPENBSD_4_4_BASE:1.107
	OPENBSD_4_3:1.102.0.2
	OPENBSD_4_3_BASE:1.102
	OPENBSD_4_2:1.99.0.2
	OPENBSD_4_2_BASE:1.99
	OPENBSD_4_1:1.87.0.2
	OPENBSD_4_1_BASE:1.87
	OPENBSD_4_0:1.84.0.2
	OPENBSD_4_0_BASE:1.84
	OPENBSD_3_9:1.79.0.2
	OPENBSD_3_9_BASE:1.79
	OPENBSD_3_8:1.77.0.2
	OPENBSD_3_8_BASE:1.77
	OPENBSD_3_7:1.75.0.2
	OPENBSD_3_7_BASE:1.75
	OPENBSD_3_6:1.69.0.2
	OPENBSD_3_6_BASE:1.69
	SMP_SYNC_A:1.68
	SMP_SYNC_B:1.68
	OPENBSD_3_5:1.68.0.4
	OPENBSD_3_5_BASE:1.68
	OPENBSD_3_4:1.68.0.2
	OPENBSD_3_4_BASE:1.68
	UBC_SYNC_A:1.67
	OPENBSD_3_3:1.65.0.4
	OPENBSD_3_3_BASE:1.65
	OPENBSD_3_2:1.65.0.2
	OPENBSD_3_2_BASE:1.65
	OPENBSD_3_1:1.58.0.2
	OPENBSD_3_1_BASE:1.58
	UBC_SYNC_B:1.65
	UBC:1.54.0.2
	UBC_BASE:1.54
	OPENBSD_3_0:1.45.0.2
	OPENBSD_3_0_BASE:1.45
	OPENBSD_2_9_BASE:1.37
	OPENBSD_2_9:1.37.0.2
	OPENBSD_2_8:1.27.0.2
	OPENBSD_2_8_BASE:1.27
	OPENBSD_2_7:1.26.0.4
	OPENBSD_2_7_BASE:1.26
	SMP:1.26.0.2
	SMP_BASE:1.26
	kame_19991208:1.26
	OPENBSD_2_6:1.24.0.2
	OPENBSD_2_6_BASE:1.24
	OPENBSD_2_5:1.21.0.2
	OPENBSD_2_5_BASE:1.21
	OPENBSD_2_4:1.20.0.4
	OPENBSD_2_4_BASE:1.20
	OPENBSD_2_3:1.20.0.2
	OPENBSD_2_3_BASE:1.20
	OPENBSD_2_2:1.17.0.2
	OPENBSD_2_2_BASE:1.17
	OPENBSD_2_1:1.14.0.2
	OPENBSD_2_1_BASE:1.14
	OPENBSD_2_0:1.11.0.2
	OPENBSD_2_0_BASE:1.11
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.182
date	2017.04.18.13.41.32;	author beck;	state Exp;
branches;
next	1.181;
commitid	CvLiz7ZuuaRSaDtH;

1.181
date	2017.04.16.14.25.42;	author beck;	state Exp;
branches;
next	1.180;
commitid	MPqr2iIfn6hJVsVl;

1.180
date	2017.02.28.10.49.37;	author natano;	state Exp;
branches;
next	1.179;
commitid	9645cvu5lMcc7gZM;

1.179
date	2017.02.14.10.31.15;	author mpi;	state Exp;
branches;
next	1.178;
commitid	PmGi4EGraGC0Z0ml;

1.178
date	2016.09.16.02.54.51;	author dlg;	state Exp;
branches;
next	1.177;
commitid	KstuxUpRI6RRN5mJ;

1.177
date	2016.09.15.02.00.16;	author dlg;	state Exp;
branches;
next	1.176;
commitid	RlO92XR575sygHqm;

1.176
date	2016.09.04.10.51.24;	author naddy;	state Exp;
branches;
next	1.175;
commitid	4OWaAfwqMvhOCLOI;

1.175
date	2016.06.07.01.31.54;	author tedu;	state Exp;
branches;
next	1.174;
commitid	37V3kDRiYRccnYvB;

1.174
date	2016.03.17.03.57.51;	author beck;	state Exp;
branches;
next	1.173;
commitid	UsjcESyd1wIYVdEr;

1.173
date	2016.03.10.03.09.45;	author beck;	state Exp;
branches;
next	1.172;
commitid	dsXn6BcOyNzEQWHh;

1.172
date	2016.03.06.19.15.23;	author mpi;	state Exp;
branches;
next	1.171;
commitid	JqIpALEKjAVw5uJc;

1.171
date	2015.11.28.21.52.02;	author beck;	state Exp;
branches;
next	1.170;
commitid	RTnjsXZgItH5OlTY;

1.170
date	2015.07.19.16.21.11;	author beck;	state Exp;
branches;
next	1.169;
commitid	LZPv3kiZFL3zSeSL;

1.169
date	2015.03.14.03.38.51;	author jsg;	state Exp;
branches;
next	1.168;
commitid	p4LJxGKbi0BU2cG6;

1.168
date	2015.02.07.08.21.24;	author miod;	state Exp;
branches;
next	1.167;
commitid	3WN6O42yLCDNeEYy;

1.167
date	2015.01.18.14.01.54;	author miod;	state Exp;
branches;
next	1.166;
commitid	4y62fBJLneCSqCbF;

1.166
date	2015.01.09.05.04.22;	author tedu;	state Exp;
branches;
next	1.165;
commitid	9zgEJHeOLE2yUxI8;

1.165
date	2014.12.16.18.30.04;	author tedu;	state Exp;
branches;
next	1.164;
commitid	P6Av4XGqOi3rFasL;

1.164
date	2014.12.11.14.33.48;	author jmc;	state Exp;
branches;
next	1.163;
commitid	RKudwifEHkgrUydW;

1.163
date	2014.10.08.07.33.14;	author blambert;	state Exp;
branches;
next	1.162;
commitid	wc1ZrjhHAvKS3riW;

1.162
date	2014.09.09.07.07.39;	author blambert;	state Exp;
branches;
next	1.161;
commitid	R0IvGgmM8zlXVXKS;

1.161
date	2014.08.31.21.08.48;	author tedu;	state Exp;
branches;
next	1.160;
commitid	xu4Eijui8okOw1L6;

1.160
date	2014.07.13.15.48.41;	author tedu;	state Exp;
branches;
next	1.159;
commitid	Szz2w3fAfwxRfkCN;

1.159
date	2014.07.13.15.29.04;	author tedu;	state Exp;
branches;
next	1.158;
commitid	bFFVdS3JEaMhyZxJ;

1.158
date	2014.07.12.18.43.32;	author tedu;	state Exp;
branches;
next	1.157;
commitid	QlVV51SZgNFxsXxC;

1.157
date	2014.07.11.03.06.08;	author mlarkin;	state Exp;
branches;
next	1.156;
commitid	85Nfk4jewp1BRfpK;

1.156
date	2014.07.08.17.19.25;	author deraadt;	state Exp;
branches;
next	1.155;
commitid	EF98ch02VpFassUi;

1.155
date	2014.04.10.13.48.24;	author tedu;	state Exp;
branches;
next	1.154;

1.154
date	2014.01.25.04.23.31;	author beck;	state Exp;
branches;
next	1.153;

1.153
date	2013.12.09.17.16.35;	author beck;	state Exp;
branches;
next	1.152;

1.152
date	2013.08.08.23.25.06;	author syl;	state Exp;
branches;
next	1.151;

1.151
date	2013.07.09.15.37.43;	author beck;	state Exp;
branches;
next	1.150;

1.150
date	2013.06.13.15.00.04;	author tedu;	state Exp;
branches;
next	1.149;

1.149
date	2013.06.11.21.51.55;	author tedu;	state Exp;
branches;
next	1.148;

1.148
date	2013.06.11.19.01.20;	author beck;	state Exp;
branches;
next	1.147;

1.147
date	2013.06.11.16.42.16;	author deraadt;	state Exp;
branches;
next	1.146;

1.146
date	2013.02.17.17.39.29;	author miod;	state Exp;
branches;
next	1.145;

1.145
date	2013.02.09.20.56.35;	author miod;	state Exp;
branches;
next	1.144;

1.144
date	2013.01.18.08.52.04;	author beck;	state Exp;
branches;
next	1.143;

1.143
date	2013.01.13.03.58.09;	author beck;	state Exp;
branches;
next	1.142;

1.142
date	2012.12.28.14.05.39;	author jsing;	state Exp;
branches;
next	1.141;

1.141
date	2012.12.02.19.42.36;	author beck;	state Exp;
branches;
next	1.140;

1.140
date	2012.12.02.19.34.14;	author beck;	state Exp;
branches;
next	1.139;

1.139
date	2012.11.07.17.50.48;	author beck;	state Exp;
branches;
next	1.138;

1.138
date	2012.10.16.02.30.54;	author beck;	state Exp;
branches;
next	1.137;

1.137
date	2012.10.09.15.12.15;	author beck;	state Exp;
branches;
next	1.136;

1.136
date	2012.05.30.19.32.19;	author miod;	state Exp;
branches;
next	1.135;

1.135
date	2012.03.23.15.51.26;	author guenther;	state Exp;
branches;
next	1.134;

1.134
date	2011.09.19.14.48.04;	author beck;	state Exp;
branches;
next	1.133;

1.133
date	2011.07.06.20.50.05;	author beck;	state Exp;
branches;
next	1.132;

1.132
date	2011.07.04.20.35.35;	author deraadt;	state Exp;
branches;
next	1.131;

1.131
date	2011.07.04.04.30.41;	author tedu;	state Exp;
branches;
next	1.130;

1.130
date	2011.06.05.19.41.04;	author deraadt;	state Exp;
branches;
next	1.129;

1.129
date	2011.04.07.19.07.42;	author beck;	state Exp;
branches;
next	1.128;

1.128
date	2011.04.02.16.47.17;	author beck;	state Exp;
branches;
next	1.127;

1.127
date	2010.11.13.17.45.44;	author deraadt;	state Exp;
branches;
next	1.126;

1.126
date	2010.08.03.06.30.19;	author deraadt;	state Exp;
branches;
next	1.125;

1.125
date	2010.08.03.04.10.16;	author matthew;	state Exp;
branches;
next	1.124;

1.124
date	2010.07.01.16.23.09;	author thib;	state Exp;
branches;
next	1.123;

1.123
date	2010.06.30.22.41.43;	author thib;	state Exp;
branches;
next	1.122;

1.122
date	2010.06.29.18.52.20;	author kettenis;	state Exp;
branches;
next	1.121;

1.121
date	2010.02.05.12.24.32;	author jsing;	state Exp;
branches;
next	1.120;

1.120
date	2009.08.08.15.04.43;	author beck;	state Exp;
branches;
next	1.119;

1.119
date	2009.08.02.16.28.40;	author beck;	state Exp;
branches;
next	1.118;

1.118
date	2009.06.25.15.49.26;	author thib;	state Exp;
branches;
next	1.117;

1.117
date	2009.06.15.17.01.26;	author beck;	state Exp;
branches;
next	1.116;

1.116
date	2009.06.06.18.06.22;	author art;	state Exp;
branches;
next	1.115;

1.115
date	2009.06.05.04.29.14;	author beck;	state Exp;
branches;
next	1.114;

1.114
date	2009.06.03.21.30.20;	author beck;	state Exp;
branches;
next	1.113;

1.113
date	2009.06.03.04.30.57;	author beck;	state Exp;
branches;
next	1.112;

1.112
date	2009.04.22.13.12.26;	author art;	state Exp;
branches;
next	1.111;

1.111
date	2009.03.23.15.10.44;	author beck;	state Exp;
branches;
next	1.110;

1.110
date	2009.01.11.17.40.00;	author oga;	state Exp;
branches;
next	1.109;

1.109
date	2009.01.09.20.28.10;	author thib;	state Exp;
branches;
next	1.108;

1.108
date	2008.11.22.12.40.39;	author pedro;	state Exp;
branches;
next	1.107;

1.107
date	2008.06.14.00.49.35;	author art;	state Exp;
branches;
next	1.106;

1.106
date	2008.06.12.06.58.39;	author deraadt;	state Exp;
branches;
next	1.105;

1.105
date	2008.06.11.12.35.46;	author deraadt;	state Exp;
branches;
next	1.104;

1.104
date	2008.06.10.20.14.36;	author beck;	state Exp;
branches;
next	1.103;

1.103
date	2008.03.16.19.42.57;	author otto;	state Exp;
branches;
next	1.102;

1.102
date	2007.10.21.15.54.55;	author beck;	state Exp;
branches;
next	1.101;

1.101
date	2007.10.18.20.26.55;	author beck;	state Exp;
branches;
next	1.100;

1.100
date	2007.09.15.10.10.37;	author martin;	state Exp;
branches;
next	1.99;

1.99
date	2007.08.07.04.32.45;	author beck;	state Exp;
branches;
next	1.98;

1.98
date	2007.07.09.15.30.25;	author miod;	state Exp;
branches;
next	1.97;

1.97
date	2007.06.17.20.06.10;	author jasper;	state Exp;
branches;
next	1.96;

1.96
date	2007.06.09.08.21.34;	author pedro;	state Exp;
branches;
next	1.95;

1.95
date	2007.06.03.20.25.12;	author otto;	state Exp;
branches;
next	1.94;

1.94
date	2007.06.01.23.35.42;	author pedro;	state Exp;
branches;
next	1.93;

1.93
date	2007.06.01.17.34.08;	author dlg;	state Exp;
branches;
next	1.92;

1.92
date	2007.05.29.21.34.43;	author art;	state Exp;
branches;
next	1.91;

1.91
date	2007.05.29.18.50.08;	author art;	state Exp;
branches;
next	1.90;

1.90
date	2007.05.28.22.18.48;	author thib;	state Exp;
branches;
next	1.89;

1.89
date	2007.05.27.01.25.56;	author pedro;	state Exp;
branches;
next	1.88;

1.88
date	2007.05.26.20.26.51;	author pedro;	state Exp;
branches;
next	1.87;

1.87
date	2006.10.21.18.09.52;	author thib;	state Exp;
branches;
next	1.86;

1.86
date	2006.10.19.12.04.31;	author mickey;	state Exp;
branches;
next	1.85;

1.85
date	2006.10.16.11.27.53;	author pedro;	state Exp;
branches;
next	1.84;

1.84
date	2006.08.28.16.15.29;	author tom;	state Exp;
branches;
next	1.83;

1.83
date	2006.08.28.12.48.53;	author jmc;	state Exp;
branches;
next	1.82;

1.82
date	2006.08.17.13.55.57;	author mickey;	state Exp;
branches;
next	1.81;

1.81
date	2006.08.09.12.00.03;	author pedro;	state Exp;
branches;
next	1.80;

1.80
date	2006.04.24.15.08.48;	author pedro;	state Exp;
branches;
next	1.79;

1.79
date	2005.11.06.13.07.47;	author pedro;	state Exp;
branches;
next	1.78;

1.78
date	2005.10.08.16.36.23;	author pedro;	state Exp;
branches;
next	1.77;

1.77
date	2005.06.27.22.08.39;	author pedro;	state Exp;
branches
	1.77.2.1;
next	1.76;

1.76
date	2005.06.17.16.45.02;	author pedro;	state Exp;
branches;
next	1.75;

1.75
date	2004.12.26.21.22.13;	author miod;	state Exp;
branches
	1.75.2.1;
next	1.74;

1.74
date	2004.12.11.14.26.31;	author pedro;	state Exp;
branches;
next	1.73;

1.73
date	2004.12.05.04.42.42;	author jsg;	state Exp;
branches;
next	1.72;

1.72
date	2004.11.30.12.39.43;	author pedro;	state Exp;
branches;
next	1.71;

1.71
date	2004.11.01.15.55.38;	author pedro;	state Exp;
branches;
next	1.70;

1.70
date	2004.09.20.10.56.51;	author pedro;	state Exp;
branches;
next	1.69;

1.69
date	2004.06.24.19.35.24;	author tholo;	state Exp;
branches
	1.69.2.1;
next	1.68;

1.68
date	2003.06.02.23.28.07;	author millert;	state Exp;
branches;
next	1.67;

1.67
date	2003.05.13.17.47.15;	author deraadt;	state Exp;
branches;
next	1.66;

1.66
date	2003.05.13.02.09.46;	author jason;	state Exp;
branches;
next	1.65;

1.65
date	2002.06.09.04.34.12;	author art;	state Exp;
branches;
next	1.64;

1.64
date	2002.06.09.01.11.18;	author art;	state Exp;
branches;
next	1.63;

1.63
date	2002.05.24.14.06.34;	author art;	state Exp;
branches;
next	1.62;

1.62
date	2002.05.24.13.59.33;	author art;	state Exp;
branches;
next	1.61;

1.61
date	2002.05.22.00.22.06;	author art;	state Exp;
branches;
next	1.60;

1.60
date	2002.05.16.00.03.05;	author art;	state Exp;
branches;
next	1.59;

1.59
date	2002.04.27.15.29.30;	author art;	state Exp;
branches;
next	1.58;

1.58
date	2002.03.14.01.27.06;	author millert;	state Exp;
branches;
next	1.57;

1.57
date	2002.01.30.20.45.35;	author nordin;	state Exp;
branches;
next	1.56;

1.56
date	2002.01.23.00.39.48;	author art;	state Exp;
branches;
next	1.55;

1.55
date	2001.12.19.08.58.06;	author art;	state Exp;
branches;
next	1.54;

1.54
date	2001.11.30.05.45.33;	author csapuntz;	state Exp;
branches
	1.54.2.1;
next	1.53;

1.53
date	2001.11.27.06.21.37;	author art;	state Exp;
branches;
next	1.52;

1.52
date	2001.11.27.05.27.11;	author art;	state Exp;
branches;
next	1.51;

1.51
date	2001.11.15.23.25.37;	author art;	state Exp;
branches;
next	1.50;

1.50
date	2001.11.15.23.15.15;	author art;	state Exp;
branches;
next	1.49;

1.49
date	2001.11.09.15.32.22;	author art;	state Exp;
branches;
next	1.48;

1.48
date	2001.11.06.19.53.20;	author miod;	state Exp;
branches;
next	1.47;

1.47
date	2001.10.28.19.19.00;	author deraadt;	state Exp;
branches;
next	1.46;

1.46
date	2001.10.28.00.42.43;	author art;	state Exp;
branches;
next	1.45;

1.45
date	2001.10.11.14.44.10;	author art;	state Exp;
branches;
next	1.44;

1.44
date	2001.10.11.08.07.12;	author gluk;	state Exp;
branches;
next	1.43;

1.43
date	2001.09.20.08.22.26;	author gluk;	state Exp;
branches;
next	1.42;

1.42
date	2001.09.19.18.05.27;	author art;	state Exp;
branches;
next	1.41;

1.41
date	2001.09.17.19.17.30;	author gluk;	state Exp;
branches;
next	1.40;

1.40
date	2001.09.10.22.05.38;	author gluk;	state Exp;
branches;
next	1.39;

1.39
date	2001.08.30.12.38.52;	author gluk;	state Exp;
branches;
next	1.38;

1.38
date	2001.05.05.20.57.01;	author art;	state Exp;
branches;
next	1.37;

1.37
date	2001.04.06.19.10.49;	author gluk;	state Exp;
branches;
next	1.36;

1.36
date	2001.03.30.10.30.26;	author art;	state Exp;
branches;
next	1.35;

1.35
date	2001.03.14.14.41.04;	author art;	state Exp;
branches;
next	1.34;

1.34
date	2001.03.13.16.47.50;	author gluk;	state Exp;
branches;
next	1.33;

1.33
date	2001.03.09.17.08.43;	author art;	state Exp;
branches;
next	1.32;

1.32
date	2001.02.27.09.13.51;	author csapuntz;	state Exp;
branches;
next	1.31;

1.31
date	2001.02.24.19.07.08;	author csapuntz;	state Exp;
branches;
next	1.30;

1.30
date	2001.02.23.14.52.50;	author csapuntz;	state Exp;
branches;
next	1.29;

1.29
date	2001.02.21.23.24.30;	author csapuntz;	state Exp;
branches;
next	1.28;

1.28
date	2001.02.13.19.51.49;	author art;	state Exp;
branches;
next	1.27;

1.27
date	2000.06.23.02.14.38;	author mickey;	state Exp;
branches;
next	1.26;

1.26
date	99.12.05.08.09.01;	author art;	state Exp;
branches
	1.26.2.1;
next	1.25;

1.25
date	99.12.02.20.55.47;	author art;	state Exp;
branches;
next	1.24;

1.24
date	99.09.10.22.14.39;	author art;	state Exp;
branches;
next	1.23;

1.23
date	99.07.15.14.07.41;	author art;	state Exp;
branches;
next	1.22;

1.22
date	99.04.28.09.28.15;	author art;	state Exp;
branches;
next	1.21;

1.21
date	98.11.29.01.46.58;	author art;	state Exp;
branches;
next	1.20;

1.20
date	98.01.10.23.44.28;	author csapuntz;	state Exp;
branches;
next	1.19;

1.19
date	97.11.07.23.01.36;	author csapuntz;	state Exp;
branches;
next	1.18;

1.18
date	97.11.06.05.58.25;	author csapuntz;	state Exp;
branches;
next	1.17;

1.17
date	97.10.06.20.20.07;	author deraadt;	state Exp;
branches;
next	1.16;

1.16
date	97.10.06.15.12.35;	author csapuntz;	state Exp;
branches;
next	1.15;

1.15
date	97.06.14.06.10.36;	author tholo;	state Exp;
branches;
next	1.14;

1.14
date	97.04.14.04.23.23;	author tholo;	state Exp;
branches;
next	1.13;

1.13
date	97.01.05.11.09.01;	author niklas;	state Exp;
branches;
next	1.12;

1.12
date	96.10.19.13.26.02;	author mickey;	state Exp;
branches;
next	1.11;

1.11
date	96.08.29.07.46.37;	author deraadt;	state Exp;
branches;
next	1.10;

1.10
date	96.07.21.08.05.34;	author tholo;	state Exp;
branches;
next	1.9;

1.9
date	96.07.02.06.52.01;	author niklas;	state Exp;
branches;
next	1.8;

1.8
date	96.06.26.19.44.59;	author tholo;	state Exp;
branches;
next	1.7;

1.7
date	96.06.14.06.36.24;	author deraadt;	state Exp;
branches;
next	1.6;

1.6
date	96.06.11.03.25.13;	author tholo;	state Exp;
branches;
next	1.5;

1.5
date	96.05.02.13.12.29;	author deraadt;	state Exp;
branches;
next	1.4;

1.4
date	96.04.19.16.09.09;	author niklas;	state Exp;
branches;
next	1.3;

1.3
date	96.03.29.12.05.51;	author mickey;	state Exp;
branches;
next	1.2;

1.2
date	96.02.29.13.38.55;	author niklas;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.52.47;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.52.47;	author deraadt;	state Exp;
branches;
next	;

1.26.2.1
date	2001.05.14.22.32.45;	author niklas;	state Exp;
branches;
next	1.26.2.2;

1.26.2.2
date	2001.07.04.10.48.47;	author niklas;	state Exp;
branches;
next	1.26.2.3;

1.26.2.3
date	2001.10.31.03.26.29;	author nate;	state Exp;
branches;
next	1.26.2.4;

1.26.2.4
date	2001.11.13.23.04.23;	author niklas;	state Exp;
branches;
next	1.26.2.5;

1.26.2.5
date	2001.12.05.01.02.39;	author niklas;	state Exp;
branches;
next	1.26.2.6;

1.26.2.6
date	2002.03.06.02.13.23;	author niklas;	state Exp;
branches;
next	1.26.2.7;

1.26.2.7
date	2002.03.28.11.43.04;	author niklas;	state Exp;
branches;
next	1.26.2.8;

1.26.2.8
date	2003.03.28.00.41.27;	author niklas;	state Exp;
branches;
next	1.26.2.9;

1.26.2.9
date	2003.06.07.11.03.41;	author ho;	state Exp;
branches;
next	;

1.54.2.1
date	2002.01.31.22.55.41;	author niklas;	state Exp;
branches;
next	1.54.2.2;

1.54.2.2
date	2002.06.11.03.29.40;	author art;	state Exp;
branches;
next	1.54.2.3;

1.54.2.3
date	2002.11.04.18.02.31;	author art;	state Exp;
branches;
next	;

1.69.2.1
date	2005.10.13.15.52.11;	author brad;	state Exp;
branches;
next	;

1.75.2.1
date	2005.10.12.22.01.05;	author brad;	state Exp;
branches;
next	;

1.77.2.1
date	2005.11.01.00.57.01;	author brad;	state Exp;
branches;
next	;


desc
@@


1.182
log
@ensure the buffer cache backs off all the way with the correct type
of memory, handling the fact that both queues are actually in dma
space when not flipping buffers high
@
text
@/*	$OpenBSD: vfs_bio.c,v 1.181 2017/04/16 14:25:42 beck Exp $	*/
/*	$NetBSD: vfs_bio.c,v 1.44 1996/06/11 11:15:36 pk Exp $	*/

/*
 * Copyright (c) 1994 Christopher G. Demetriou
 * Copyright (c) 1982, 1986, 1989, 1993
 *	The Regents of the University of California.  All rights reserved.
 * (c) UNIX System Laboratories, Inc.
 * All or some portions of this file are derived from material licensed
 * to the University of California by American Telephone and Telegraph
 * Co. or Unix System Laboratories, Inc. and are reproduced herein with
 * the permission of UNIX System Laboratories, Inc.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)vfs_bio.c	8.6 (Berkeley) 1/11/94
 */

/*
 * Some references:
 *	Bach: The Design of the UNIX Operating System (Prentice Hall, 1986)
 *	Leffler, et al.: The Design and Implementation of the 4.3BSD
 *		UNIX Operating System (Addison Welley, 1989)
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/buf.h>
#include <sys/vnode.h>
#include <sys/mount.h>
#include <sys/malloc.h>
#include <sys/pool.h>
#include <sys/resourcevar.h>
#include <sys/conf.h>
#include <sys/kernel.h>
#include <sys/specdev.h>
#include <uvm/uvm_extern.h>

/* XXX Should really be in buf.h, but for uvm_constraint_range.. */
int	buf_realloc_pages(struct buf *, struct uvm_constraint_range *, int);

struct uvm_constraint_range high_constraint;
int fliphigh;

int nobuffers;
int needbuffer;
struct bio_ops bioops;

/* private bufcache functions */
void bufcache_init(void);
void bufcache_adjust(void);
struct buf *bufcache_gethighcleanbuf(void);
struct buf *bufcache_getdmacleanbuf(void);

/*
 * Buffer pool for I/O buffers.
 */
struct pool bufpool;
struct bufhead bufhead = LIST_HEAD_INITIALIZER(bufhead);
void buf_put(struct buf *);

struct buf *bio_doread(struct vnode *, daddr_t, int, int);
struct buf *buf_get(struct vnode *, daddr_t, size_t);
void bread_cluster_callback(struct buf *);

struct bcachestats bcstats;  /* counters */
long lodirtypages;      /* dirty page count low water mark */
long hidirtypages;      /* dirty page count high water mark */
long targetpages;   	/* target number of pages for cache size */
long buflowpages;	/* smallest size cache allowed */
long bufhighpages; 	/* largest size cache allowed */
long bufbackpages; 	/* minimum number of pages we shrink when asked to */

vsize_t bufkvm;

struct proc *cleanerproc;
int bd_req;			/* Sleep point for cleaner daemon. */

#define NUM_CACHES 2
#define DMA_CACHE 0
struct bufcache cleancache[NUM_CACHES];
struct bufqueue dirtyqueue;

void
buf_put(struct buf *bp)
{
	splassert(IPL_BIO);

#ifdef DIAGNOSTIC
	if (bp->b_pobj != NULL)
		KASSERT(bp->b_bufsize > 0);
	if (ISSET(bp->b_flags, B_DELWRI))
		panic("buf_put: releasing dirty buffer");
	if (bp->b_freelist.tqe_next != NOLIST &&
	    bp->b_freelist.tqe_next != (void *)-1)
		panic("buf_put: still on the free list");
	if (bp->b_vnbufs.le_next != NOLIST &&
	    bp->b_vnbufs.le_next != (void *)-1)
		panic("buf_put: still on the vnode list");
	if (!LIST_EMPTY(&bp->b_dep))
		panic("buf_put: b_dep is not empty");
#endif

	LIST_REMOVE(bp, b_list);
	bcstats.numbufs--;

	if (buf_dealloc_mem(bp) != 0)
		return;
	pool_put(&bufpool, bp);
}

/*
 * Initialize buffers and hash links for buffers.
 */
void
bufinit(void)
{
	u_int64_t dmapages;
	u_int64_t highpages;

	dmapages = uvm_pagecount(&dma_constraint);
	/* take away a guess at how much of this the kernel will consume */
	dmapages -= (atop(physmem) - atop(uvmexp.free));

	/* See if we have memory above the dma accessible region. */
	high_constraint.ucr_low = dma_constraint.ucr_high;
	high_constraint.ucr_high = no_constraint.ucr_high;
	if (high_constraint.ucr_low != high_constraint.ucr_high)
		high_constraint.ucr_low++;
	highpages = uvm_pagecount(&high_constraint);

	/*
	 * Do we have any significant amount of high memory above
	 * the DMA region? if so enable moving buffers there, if not,
	 * don't bother.
	 */
	if (highpages > dmapages / 4)
		fliphigh = 1;
	else
		fliphigh = 0;

	/*
	 * If MD code doesn't say otherwise, use up to 10% of DMA'able
	 * memory for buffers.
	 */
	if (bufcachepercent == 0)
		bufcachepercent = 10;

	/*
	 * XXX these values and their same use in kern_sysctl
	 * need to move into buf.h
	 */
	KASSERT(bufcachepercent <= 90);
	KASSERT(bufcachepercent >= 5);
	if (bufpages == 0)
		bufpages = dmapages * bufcachepercent / 100;
	if (bufpages < BCACHE_MIN)
		bufpages = BCACHE_MIN;
	KASSERT(bufpages < dmapages);

	bufhighpages = bufpages;

	/*
	 * Set the base backoff level for the buffer cache.  We will
	 * not allow uvm to steal back more than this number of pages.
	 */
	buflowpages = dmapages * 5 / 100;
	if (buflowpages < BCACHE_MIN)
		buflowpages = BCACHE_MIN;

	/*
	 * set bufbackpages to 100 pages, or 10 percent of the low water mark
	 * if we don't have that many pages.
	 */

	bufbackpages = buflowpages * 10 / 100;
	if (bufbackpages > 100)
		bufbackpages = 100;

	/*
	 * If the MD code does not say otherwise, reserve 10% of kva
	 * space for mapping buffers.
	 */
	if (bufkvm == 0)
		bufkvm = VM_KERNEL_SPACE_SIZE / 10;

	/*
	 * Don't use more than twice the amount of bufpages for mappings.
	 * It's twice since we map things sparsely.
	 */
	if (bufkvm > bufpages * PAGE_SIZE)
		bufkvm = bufpages * PAGE_SIZE;
	/*
	 * Round bufkvm to MAXPHYS because we allocate chunks of va space
	 * in MAXPHYS chunks.
	 */
	bufkvm &= ~(MAXPHYS - 1);

	pool_init(&bufpool, sizeof(struct buf), 0, IPL_BIO, 0, "bufpl", NULL);

	bufcache_init();

	/*
	 * hmm - bufkvm is an argument because it's static, while
	 * bufpages is global because it can change while running.
 	 */
	buf_mem_init(bufkvm);

	/*
	 * Set the dirty page high water mark to be less than the low
	 * water mark for pages in the buffer cache. This ensures we
	 * can always back off by throwing away clean pages, and give
	 * ourselves a chance to write out the dirty pages eventually.
	 */
	hidirtypages = (buflowpages / 4) * 3;
	lodirtypages = buflowpages / 2;

	/*
	 * We are allowed to use up to the reserve.
	 */
	targetpages = bufpages - RESERVE_PAGES;
}

/*
 * Change cachepct
 */
void
bufadjust(int newbufpages)
{
	struct buf *bp;
	int s;

	if (newbufpages < buflowpages)
		newbufpages = buflowpages;

	s = splbio();
	bufpages = newbufpages;

	/*
	 * We are allowed to use up to the reserve
	 */
	targetpages = bufpages - RESERVE_PAGES;

	/*
	 * Shrinking the cache happens here only if someone has manually
	 * adjusted bufcachepercent - or the pagedaemon has told us
	 * to give back memory *now* - so we give it all back.
	 */
	while ((bp = bufcache_getdmacleanbuf()) &&
	    (bcstats.dmapages > targetpages)) {
		bufcache_take(bp);
		if (bp->b_vp) {
			RBT_REMOVE(buf_rb_bufs, &bp->b_vp->v_bufs_tree, bp);
			brelvp(bp);
		}
		buf_put(bp);
	}
	bufcache_adjust();

	/*
	 * Wake up the cleaner if we have lots of dirty pages,
	 * or if we are getting low on buffer cache kva.
	 */
	if ((UNCLEAN_PAGES >= hidirtypages) ||
	    bcstats.kvaslots_avail <= 2 * RESERVE_SLOTS)
		wakeup(&bd_req);

	splx(s);
}

/*
 * Make the buffer cache back off from cachepct.
 */
int
bufbackoff(struct uvm_constraint_range *range, long size)
{
	/*
	 * Back off "size" buffer cache pages. Called by the page
	 * daemon to consume buffer cache pages rather than scanning.
	 *
	 * It returns 0 to the pagedaemon to indicate that it has
	 * succeeded in freeing enough pages. It returns -1 to
	 * indicate that it could not and the pagedaemon should take
	 * other measures.
	 *
	 */
	long pdelta, oldbufpages;

	/*
	 * If we will accept high memory for this backoff
	 * try to steal it from the high memory buffer cache.
	 */
	if (range->ucr_high > dma_constraint.ucr_high) {
		struct buf *bp;
		int64_t start = bcstats.numbufpages, recovered = 0;
		int s = splbio();

		while ((recovered < size) &&
		    (bp = bufcache_gethighcleanbuf())) {
			bufcache_take(bp);
			if (bp->b_vp) {
				RBT_REMOVE(buf_rb_bufs,
				    &bp->b_vp->v_bufs_tree, bp);
				brelvp(bp);
			}
			buf_put(bp);
			recovered = start - bcstats.numbufpages;
		}
		bufcache_adjust();
		splx(s);

		/* If we got enough, return success */
		if (recovered >= size)
			return 0;

		/*
		 * If we needed only memory above DMA,
		 * return failure
		 */
		if (range->ucr_low > dma_constraint.ucr_high)
			return -1;

		/* Otherwise get the rest from DMA */
		size -= recovered;
	}

	/*
	 * XXX Otherwise do the dma memory cache dance. this needs
	 * refactoring later to get rid of 'bufpages'
	 */

	/*
	 * Back off by at least bufbackpages. If the page daemon gave us
	 * a larger size, back off by that much.
	 */
	pdelta = (size > bufbackpages) ? size : bufbackpages;

	if (bufpages <= buflowpages)
		return(-1);
	if (bufpages - pdelta < buflowpages)
		pdelta = bufpages - buflowpages;
	oldbufpages = bufpages;
	bufadjust(bufpages - pdelta);
	if (oldbufpages - bufpages < size)
		return (-1); /* we did not free what we were asked */
	else
		return(0);
}


/*
 * Opportunistically flip a buffer into high memory. Will move the buffer
 * if memory is available without sleeping, and return 0, otherwise will
 * fail and return -1 with the buffer unchanged.
 */

int
buf_flip_high(struct buf *bp)
{
	int s;
	int ret = -1;

	KASSERT(ISSET(bp->b_flags, B_BC));
	KASSERT(ISSET(bp->b_flags, B_DMA));
	KASSERT(bp->cache == DMA_CACHE);
	KASSERT(fliphigh);

	/* Attempt to move the buffer to high memory if we can */
	s = splbio();
	if (buf_realloc_pages(bp, &high_constraint, UVM_PLA_NOWAIT) == 0) {
		KASSERT(!ISSET(bp->b_flags, B_DMA));
		bcstats.highflips++;
		ret = 0;
	} else
		bcstats.highflops++;
	splx(s);

	return ret;
}

/*
 * Flip a buffer to dma reachable memory, when we need it there for
 * I/O. This can sleep since it will wait for memory alloacation in the
 * DMA reachable area since we have to have the buffer there to proceed.
 */
void
buf_flip_dma(struct buf *bp)
{
	KASSERT(ISSET(bp->b_flags, B_BC));
	KASSERT(ISSET(bp->b_flags, B_BUSY));
	KASSERT(bp->cache < NUM_CACHES);

	if (!ISSET(bp->b_flags, B_DMA)) {
		int s = splbio();

		/* move buf to dma reachable memory */
		(void) buf_realloc_pages(bp, &dma_constraint, UVM_PLA_WAITOK);
		KASSERT(ISSET(bp->b_flags, B_DMA));
		bcstats.dmaflips++;
		splx(s);
	}

	if (bp->cache > DMA_CACHE) {
		CLR(bp->b_flags, B_COLD);
		CLR(bp->b_flags, B_WARM);
		bp->cache = DMA_CACHE;
	}
}

struct buf *
bio_doread(struct vnode *vp, daddr_t blkno, int size, int async)
{
	struct buf *bp;
	struct mount *mp;

	bp = getblk(vp, blkno, size, 0, 0);

	/*
	 * If buffer does not have valid data, start a read.
	 * Note that if buffer is B_INVAL, getblk() won't return it.
	 * Therefore, it's valid if its I/O has completed or been delayed.
	 */
	if (!ISSET(bp->b_flags, (B_DONE | B_DELWRI))) {
		SET(bp->b_flags, B_READ | async);
		bcstats.pendingreads++;
		bcstats.numreads++;
		VOP_STRATEGY(bp);
		/* Pay for the read. */
		curproc->p_ru.ru_inblock++;			/* XXX */
	} else if (async) {
		brelse(bp);
	}

	mp = vp->v_type == VBLK? vp->v_specmountpoint : vp->v_mount;

	/*
	 * Collect statistics on synchronous and asynchronous reads.
	 * Reads from block devices are charged to their associated
	 * filesystem (if any).
	 */
	if (mp != NULL) {
		if (async == 0)
			mp->mnt_stat.f_syncreads++;
		else
			mp->mnt_stat.f_asyncreads++;
	}

	return (bp);
}

/*
 * Read a disk block.
 * This algorithm described in Bach (p.54).
 */
int
bread(struct vnode *vp, daddr_t blkno, int size, struct buf **bpp)
{
	struct buf *bp;

	/* Get buffer for block. */
	bp = *bpp = bio_doread(vp, blkno, size, 0);

	/* Wait for the read to complete, and return result. */
	return (biowait(bp));
}

/*
 * Read-ahead multiple disk blocks. The first is sync, the rest async.
 * Trivial modification to the breada algorithm presented in Bach (p.55).
 */
int
breadn(struct vnode *vp, daddr_t blkno, int size, daddr_t rablks[],
    int rasizes[], int nrablks, struct buf **bpp)
{
	struct buf *bp;
	int i;

	bp = *bpp = bio_doread(vp, blkno, size, 0);

	/*
	 * For each of the read-ahead blocks, start a read, if necessary.
	 */
	for (i = 0; i < nrablks; i++) {
		/* If it's in the cache, just go on to next one. */
		if (incore(vp, rablks[i]))
			continue;

		/* Get a buffer for the read-ahead block */
		(void) bio_doread(vp, rablks[i], rasizes[i], B_ASYNC);
	}

	/* Otherwise, we had to start a read for it; wait until it's valid. */
	return (biowait(bp));
}

/*
 * Called from interrupt context.
 */
void
bread_cluster_callback(struct buf *bp)
{
	struct buf **xbpp = bp->b_saveaddr;
	int i;

	if (xbpp[1] != NULL) {
		size_t newsize = xbpp[1]->b_bufsize;

		/*
		 * Shrink this buffer's mapping to only cover its part of
		 * the total I/O.
		 */
		buf_fix_mapping(bp, newsize);
		bp->b_bcount = newsize;
	}

	for (i = 1; xbpp[i] != 0; i++) {
		if (ISSET(bp->b_flags, B_ERROR))
			SET(xbpp[i]->b_flags, B_INVAL | B_ERROR);
		biodone(xbpp[i]);
	}

	free(xbpp, M_TEMP, 0);

	if (ISSET(bp->b_flags, B_ASYNC)) {
		brelse(bp);
	} else {
		CLR(bp->b_flags, B_WANTED);
		wakeup(bp);
	}
}

int
bread_cluster(struct vnode *vp, daddr_t blkno, int size, struct buf **rbpp)
{
	struct buf *bp, **xbpp;
	int howmany, maxra, i, inc;
	daddr_t sblkno;

	*rbpp = bio_doread(vp, blkno, size, 0);

	/*
	 * If the buffer is in the cache skip any I/O operation.
	 */
	if (ISSET((*rbpp)->b_flags, B_CACHE))
		goto out;

	if (size != round_page(size))
		goto out;

	if (VOP_BMAP(vp, blkno + 1, NULL, &sblkno, &maxra))
		goto out;

	maxra++;
	if (sblkno == -1 || maxra < 2)
		goto out;

	howmany = MAXPHYS / size;
	if (howmany > maxra)
		howmany = maxra;

	xbpp = mallocarray(howmany + 1, sizeof(struct buf *), M_TEMP, M_NOWAIT);
	if (xbpp == NULL)
		goto out;

	for (i = howmany - 1; i >= 0; i--) {
		size_t sz;

		/*
		 * First buffer allocates big enough size to cover what
		 * all the other buffers need.
		 */
		sz = i == 0 ? howmany * size : 0;

		xbpp[i] = buf_get(vp, blkno + i + 1, sz);
		if (xbpp[i] == NULL) {
			for (++i; i < howmany; i++) {
				SET(xbpp[i]->b_flags, B_INVAL);
				brelse(xbpp[i]);
			}
			free(xbpp, M_TEMP, 0);
			goto out;
		}
	}

	bp = xbpp[0];

	xbpp[howmany] = 0;

	inc = btodb(size);

	for (i = 1; i < howmany; i++) {
		bcstats.pendingreads++;
		bcstats.numreads++;
                /*
                * We set B_DMA here because bp above will be B_DMA,
                * and we are playing buffer slice-n-dice games from
                * the memory allocated in bp.
                */
		SET(xbpp[i]->b_flags, B_DMA | B_READ | B_ASYNC);
		xbpp[i]->b_blkno = sblkno + (i * inc);
		xbpp[i]->b_bufsize = xbpp[i]->b_bcount = size;
		xbpp[i]->b_data = NULL;
		xbpp[i]->b_pobj = bp->b_pobj;
		xbpp[i]->b_poffs = bp->b_poffs + (i * size);
	}

	KASSERT(bp->b_lblkno == blkno + 1);
	KASSERT(bp->b_vp == vp);

	bp->b_blkno = sblkno;
	SET(bp->b_flags, B_READ | B_ASYNC | B_CALL);

	bp->b_saveaddr = (void *)xbpp;
	bp->b_iodone = bread_cluster_callback;

	bcstats.pendingreads++;
	bcstats.numreads++;
	VOP_STRATEGY(bp);
	curproc->p_ru.ru_inblock++;

out:
	return (biowait(*rbpp));
}

/*
 * Block write.  Described in Bach (p.56)
 */
int
bwrite(struct buf *bp)
{
	int rv, async, wasdelayed, s;
	struct vnode *vp;
	struct mount *mp;

	vp = bp->b_vp;
	if (vp != NULL)
		mp = vp->v_type == VBLK? vp->v_specmountpoint : vp->v_mount;
	else
		mp = NULL;

	/*
	 * Remember buffer type, to switch on it later.  If the write was
	 * synchronous, but the file system was mounted with MNT_ASYNC,
	 * convert it to a delayed write.
	 * XXX note that this relies on delayed tape writes being converted
	 * to async, not sync writes (which is safe, but ugly).
	 */
	async = ISSET(bp->b_flags, B_ASYNC);
	if (!async && mp && ISSET(mp->mnt_flag, MNT_ASYNC)) {
		bdwrite(bp);
		return (0);
	}

	/*
	 * Collect statistics on synchronous and asynchronous writes.
	 * Writes to block devices are charged to their associated
	 * filesystem (if any).
	 */
	if (mp != NULL) {
		if (async)
			mp->mnt_stat.f_asyncwrites++;
		else
			mp->mnt_stat.f_syncwrites++;
	}
	bcstats.pendingwrites++;
	bcstats.numwrites++;

	wasdelayed = ISSET(bp->b_flags, B_DELWRI);
	CLR(bp->b_flags, (B_READ | B_DONE | B_ERROR | B_DELWRI));

	s = splbio();

	/*
	 * If not synchronous, pay for the I/O operation and make
	 * sure the buf is on the correct vnode queue.  We have
	 * to do this now, because if we don't, the vnode may not
	 * be properly notified that its I/O has completed.
	 */
	if (wasdelayed) {
		reassignbuf(bp);
	} else
		curproc->p_ru.ru_oublock++;


	/* Initiate disk write.  Make sure the appropriate party is charged. */
	bp->b_vp->v_numoutput++;
	splx(s);
	buf_flip_dma(bp);
	SET(bp->b_flags, B_WRITEINPROG);
	VOP_STRATEGY(bp);

	/*
	 * If the queue is above the high water mark, wait till
	 * the number of outstanding write bufs drops below the low
	 * water mark.
	 */
	if (bp->b_bq)
		bufq_wait(bp->b_bq);

	if (async)
		return (0);

	/*
	 * If I/O was synchronous, wait for it to complete.
	 */
	rv = biowait(bp);

	/* Release the buffer. */
	brelse(bp);

	return (rv);
}


/*
 * Delayed write.
 *
 * The buffer is marked dirty, but is not queued for I/O.
 * This routine should be used when the buffer is expected
 * to be modified again soon, typically a small write that
 * partially fills a buffer.
 *
 * NB: magnetic tapes cannot be delayed; they must be
 * written in the order that the writes are requested.
 *
 * Described in Leffler, et al. (pp. 208-213).
 */
void
bdwrite(struct buf *bp)
{
	int s;

	/*
	 * If the block hasn't been seen before:
	 *	(1) Mark it as having been seen,
	 *	(2) Charge for the write.
	 *	(3) Make sure it's on its vnode's correct block list,
	 *	(4) If a buffer is rewritten, move it to end of dirty list
	 */
	if (!ISSET(bp->b_flags, B_DELWRI)) {
		SET(bp->b_flags, B_DELWRI);
		s = splbio();
		buf_flip_dma(bp);
		reassignbuf(bp);
		splx(s);
		curproc->p_ru.ru_oublock++;		/* XXX */
	}

	/* The "write" is done, so mark and release the buffer. */
	CLR(bp->b_flags, B_NEEDCOMMIT);
	SET(bp->b_flags, B_DONE);
	brelse(bp);
}

/*
 * Asynchronous block write; just an asynchronous bwrite().
 */
void
bawrite(struct buf *bp)
{

	SET(bp->b_flags, B_ASYNC);
	VOP_BWRITE(bp);
}

/*
 * Must be called at splbio()
 */
void
buf_dirty(struct buf *bp)
{
	splassert(IPL_BIO);

#ifdef DIAGNOSTIC
	if (!ISSET(bp->b_flags, B_BUSY))
		panic("Trying to dirty buffer on freelist!");
#endif

	if (ISSET(bp->b_flags, B_DELWRI) == 0) {
		SET(bp->b_flags, B_DELWRI);
		buf_flip_dma(bp);
		reassignbuf(bp);
	}
}

/*
 * Must be called at splbio()
 */
void
buf_undirty(struct buf *bp)
{
	splassert(IPL_BIO);

#ifdef DIAGNOSTIC
	if (!ISSET(bp->b_flags, B_BUSY))
		panic("Trying to undirty buffer on freelist!");
#endif
	if (ISSET(bp->b_flags, B_DELWRI)) {
		CLR(bp->b_flags, B_DELWRI);
		reassignbuf(bp);
	}
}

/*
 * Release a buffer on to the free lists.
 * Described in Bach (p. 46).
 */
void
brelse(struct buf *bp)
{
	int s;

	s = splbio();

	if (bp->b_data != NULL)
		KASSERT(bp->b_bufsize > 0);

	/*
	 * Determine which queue the buffer should be on, then put it there.
	 */

	/* If it's not cacheable, or an error, mark it invalid. */
	if (ISSET(bp->b_flags, (B_NOCACHE|B_ERROR)))
		SET(bp->b_flags, B_INVAL);

	if (ISSET(bp->b_flags, B_INVAL)) {
		/*
		 * If the buffer is invalid, free it now rather than leaving
		 * it in a queue and wasting memory.
		 */
		if (LIST_FIRST(&bp->b_dep) != NULL)
			buf_deallocate(bp);

		if (ISSET(bp->b_flags, B_DELWRI)) {
			CLR(bp->b_flags, B_DELWRI);
		}

		if (bp->b_vp) {
			RBT_REMOVE(buf_rb_bufs, &bp->b_vp->v_bufs_tree, bp);
			brelvp(bp);
		}
		bp->b_vp = NULL;

		/*
		 * Wake up any processes waiting for _this_ buffer to
		 * become free. They are not allowed to grab it
		 * since it will be freed. But the only sleeper is
		 * getblk and it will restart the operation after
		 * sleep.
		 */
		if (ISSET(bp->b_flags, B_WANTED)) {
			CLR(bp->b_flags, B_WANTED);
			wakeup(bp);
		}
		buf_put(bp);
	} else {
		/*
		 * It has valid data.  Put it on the end of the appropriate
		 * queue, so that it'll stick around for as long as possible.
		 */
		bufcache_release(bp);

		/* Unlock the buffer. */
		CLR(bp->b_flags, (B_AGE | B_ASYNC | B_NOCACHE | B_DEFERRED));
		buf_release(bp);

		/* Wake up any processes waiting for _this_ buffer to
		 * become free. */
		if (ISSET(bp->b_flags, B_WANTED)) {
			CLR(bp->b_flags, B_WANTED);
			wakeup(bp);
		}
	}

	/* Wake up syncer and cleaner processes waiting for buffers. */
	if (nobuffers) {
		nobuffers = 0;
		wakeup(&nobuffers);
	}

	/* Wake up any processes waiting for any buffer to become free. */
	if (needbuffer && bcstats.dmapages < targetpages &&
	    bcstats.kvaslots_avail > RESERVE_SLOTS) {
		needbuffer = 0;
		wakeup(&needbuffer);
	}

	splx(s);
}

/*
 * Determine if a block is in the cache. Just look on what would be its hash
 * chain. If it's there, return a pointer to it, unless it's marked invalid.
 */
struct buf *
incore(struct vnode *vp, daddr_t blkno)
{
	struct buf *bp;
	struct buf b;
	int s;

	s = splbio();

	/* Search buf lookup tree */
	b.b_lblkno = blkno;
	bp = RBT_FIND(buf_rb_bufs, &vp->v_bufs_tree, &b);
	if (bp != NULL && ISSET(bp->b_flags, B_INVAL))
		bp = NULL;

	splx(s);
	return (bp);
}

/*
 * Get a block of requested size that is associated with
 * a given vnode and block offset. If it is found in the
 * block cache, mark it as having been found, make it busy
 * and return it. Otherwise, return an empty block of the
 * correct size. It is up to the caller to ensure that the
 * cached blocks be of the correct size.
 */
struct buf *
getblk(struct vnode *vp, daddr_t blkno, int size, int slpflag, int slptimeo)
{
	struct buf *bp;
	struct buf b;
	int s, error;

	/*
	 * XXX
	 * The following is an inlined version of 'incore()', but with
	 * the 'invalid' test moved to after the 'busy' test.  It's
	 * necessary because there are some cases in which the NFS
	 * code sets B_INVAL prior to writing data to the server, but
	 * in which the buffers actually contain valid data.  In this
	 * case, we can't allow the system to allocate a new buffer for
	 * the block until the write is finished.
	 */
start:
	s = splbio();
	b.b_lblkno = blkno;
	bp = RBT_FIND(buf_rb_bufs, &vp->v_bufs_tree, &b);
	if (bp != NULL) {
		if (ISSET(bp->b_flags, B_BUSY)) {
			SET(bp->b_flags, B_WANTED);
			error = tsleep(bp, slpflag | (PRIBIO + 1), "getblk",
			    slptimeo);
			splx(s);
			if (error)
				return (NULL);
			goto start;
		}

		if (!ISSET(bp->b_flags, B_INVAL)) {
			bcstats.cachehits++;
			SET(bp->b_flags, B_CACHE);
			bufcache_take(bp);
			buf_acquire(bp);
			splx(s);
			return (bp);
		}
	}
	splx(s);

	if ((bp = buf_get(vp, blkno, size)) == NULL)
		goto start;

	return (bp);
}

/*
 * Get an empty, disassociated buffer of given size.
 */
struct buf *
geteblk(size_t size)
{
	struct buf *bp;

	while ((bp = buf_get(NULL, 0, size)) == NULL)
		continue;

	return (bp);
}

/*
 * Allocate a buffer.
 */
struct buf *
buf_get(struct vnode *vp, daddr_t blkno, size_t size)
{
	struct buf *bp;
	int poolwait = size == 0 ? PR_NOWAIT : PR_WAITOK;
	int npages;
	int s;

	s = splbio();
	if (size) {
		/*
		 * Wake up the cleaner if we have lots of dirty pages,
		 * or if we are getting low on buffer cache kva.
		 */
		if (UNCLEAN_PAGES >= hidirtypages ||
			bcstats.kvaslots_avail <= 2 * RESERVE_SLOTS)
			wakeup(&bd_req);

		npages = atop(round_page(size));

		/*
		 * if our cache has been previously shrunk,
		 * allow it to grow again with use up to
		 * bufhighpages (cachepercent)
		 */
		if (bufpages < bufhighpages)
			bufadjust(bufhighpages);

		/*
		 * If we would go over the page target with our
		 * new allocation, free enough buffers first
		 * to stay at the target with our new allocation.
		 */
		while ((bcstats.dmapages + npages > targetpages) &&
		    (bp = bufcache_getdmacleanbuf())) {
			bufcache_take(bp);
			if (bp->b_vp) {
				RBT_REMOVE(buf_rb_bufs,
				    &bp->b_vp->v_bufs_tree, bp);
				brelvp(bp);
			}
			buf_put(bp);
		}

		/*
		 * If we get here, we tried to free the world down
		 * above, and couldn't get down - Wake the cleaner
		 * and wait for it to push some buffers out.
		 */
		if ((bcstats.dmapages + npages > targetpages ||
		    bcstats.kvaslots_avail <= RESERVE_SLOTS) &&
		    curproc != syncerproc && curproc != cleanerproc) {
			wakeup(&bd_req);
			needbuffer++;
			tsleep(&needbuffer, PRIBIO, "needbuffer", 0);
			splx(s);
			return (NULL);
		}
		if (bcstats.dmapages + npages > bufpages) {
			/* cleaner or syncer */
			nobuffers = 1;
			tsleep(&nobuffers, PRIBIO, "nobuffers", 0);
			splx(s);
			return (NULL);
		}
	}

	bp = pool_get(&bufpool, poolwait|PR_ZERO);

	if (bp == NULL) {
		splx(s);
		return (NULL);
	}

	bp->b_freelist.tqe_next = NOLIST;
	bp->b_dev = NODEV;
	LIST_INIT(&bp->b_dep);
	bp->b_bcount = size;

	buf_acquire_nomap(bp);

	if (vp != NULL) {
		/*
		 * We insert the buffer into the hash with B_BUSY set
		 * while we allocate pages for it. This way any getblk
		 * that happens while we allocate pages will wait for
		 * this buffer instead of starting its own buf_get.
		 *
		 * But first, we check if someone beat us to it.
		 */
		if (incore(vp, blkno)) {
			pool_put(&bufpool, bp);
			splx(s);
			return (NULL);
		}

		bp->b_blkno = bp->b_lblkno = blkno;
		bgetvp(vp, bp);
		if (RBT_INSERT(buf_rb_bufs, &vp->v_bufs_tree, bp))
			panic("buf_get: dup lblk vp %p bp %p", vp, bp);
	} else {
		bp->b_vnbufs.le_next = NOLIST;
		SET(bp->b_flags, B_INVAL);
		bp->b_vp = NULL;
	}

	LIST_INSERT_HEAD(&bufhead, bp, b_list);
	bcstats.numbufs++;

	if (size) {
		buf_alloc_pages(bp, round_page(size));
		KASSERT(ISSET(bp->b_flags, B_DMA));
		buf_map(bp);
	}

	SET(bp->b_flags, B_BC);
	splx(s);

	return (bp);
}

/*
 * Buffer cleaning daemon.
 */
void
buf_daemon(struct proc *p)
{
	struct buf *bp = NULL;
	int s, pushed = 0;

	cleanerproc = curproc;

	s = splbio();
	for (;;) {
		if (bp == NULL || (pushed >= 16 &&
		    UNCLEAN_PAGES < hidirtypages &&
		    bcstats.kvaslots_avail > 2 * RESERVE_SLOTS)){
			pushed = 0;
			/*
			 * Wake up anyone who was waiting for buffers
			 * to be released.
			 */
			if (needbuffer) {
				needbuffer = 0;
				wakeup(&needbuffer);
			}
			tsleep(&bd_req, PRIBIO - 7, "cleaner", 0);
		}

		while ((bp = bufcache_getdirtybuf())) {

			if (UNCLEAN_PAGES < lodirtypages &&
			    bcstats.kvaslots_avail > 2 * RESERVE_SLOTS &&
			    pushed >= 16)
				break;

			bufcache_take(bp);
			buf_acquire(bp);
			splx(s);

			if (ISSET(bp->b_flags, B_INVAL)) {
				brelse(bp);
				s = splbio();
				continue;
			}
#ifdef DIAGNOSTIC
			if (!ISSET(bp->b_flags, B_DELWRI))
				panic("Clean buffer on dirty queue");
#endif
			if (LIST_FIRST(&bp->b_dep) != NULL &&
			    !ISSET(bp->b_flags, B_DEFERRED) &&
			    buf_countdeps(bp, 0, 0)) {
				SET(bp->b_flags, B_DEFERRED);
				s = splbio();
				bufcache_release(bp);
				buf_release(bp);
				continue;
			}

			bawrite(bp);
			pushed++;

			sched_pause(yield);

			s = splbio();
		}
	}
}

/*
 * Wait for operations on the buffer to complete.
 * When they do, extract and return the I/O's error value.
 */
int
biowait(struct buf *bp)
{
	int s;

	KASSERT(!(bp->b_flags & B_ASYNC));

	s = splbio();
	while (!ISSET(bp->b_flags, B_DONE))
		tsleep(bp, PRIBIO + 1, "biowait", 0);
	splx(s);

	/* check for interruption of I/O (e.g. via NFS), then errors. */
	if (ISSET(bp->b_flags, B_EINTR)) {
		CLR(bp->b_flags, B_EINTR);
		return (EINTR);
	}

	if (ISSET(bp->b_flags, B_ERROR))
		return (bp->b_error ? bp->b_error : EIO);
	else
		return (0);
}

/*
 * Mark I/O complete on a buffer.
 *
 * If a callback has been requested, e.g. the pageout
 * daemon, do so. Otherwise, awaken waiting processes.
 *
 * [ Leffler, et al., says on p.247:
 *	"This routine wakes up the blocked process, frees the buffer
 *	for an asynchronous write, or, for a request by the pagedaemon
 *	process, invokes a procedure specified in the buffer structure" ]
 *
 * In real life, the pagedaemon (or other system processes) wants
 * to do async stuff to, and doesn't want the buffer brelse()'d.
 * (for swap pager, that puts swap buffers on the free lists (!!!),
 * for the vn device, that puts malloc'd buffers on the free lists!)
 *
 * Must be called at splbio().
 */
void
biodone(struct buf *bp)
{
	splassert(IPL_BIO);

	if (ISSET(bp->b_flags, B_DONE))
		panic("biodone already");
	SET(bp->b_flags, B_DONE);		/* note that it's done */

	if (bp->b_bq)
		bufq_done(bp->b_bq, bp);

	if (LIST_FIRST(&bp->b_dep) != NULL)
		buf_complete(bp);

	if (!ISSET(bp->b_flags, B_READ)) {
		CLR(bp->b_flags, B_WRITEINPROG);
		vwakeup(bp->b_vp);
	}
	if (bcstats.numbufs &&
	    (!(ISSET(bp->b_flags, B_RAW) || ISSET(bp->b_flags, B_PHYS)))) {
		if (!ISSET(bp->b_flags, B_READ)) {
			bcstats.pendingwrites--;
		} else
			bcstats.pendingreads--;
	}
	if (ISSET(bp->b_flags, B_CALL)) {	/* if necessary, call out */
		CLR(bp->b_flags, B_CALL);	/* but note callout done */
		(*bp->b_iodone)(bp);
	} else {
		if (ISSET(bp->b_flags, B_ASYNC)) {/* if async, release it */
			brelse(bp);
		} else {			/* or just wakeup the buffer */
			CLR(bp->b_flags, B_WANTED);
			wakeup(bp);
		}
	}
}

#ifdef DDB
void	bcstats_print(int (*)(const char *, ...)
    __attribute__((__format__(__kprintf__,1,2))));
/*
 * bcstats_print: ddb hook to print interesting buffer cache counters
 */
void
bcstats_print(
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))))
{
	(*pr)("Current Buffer Cache status:\n");
	(*pr)("numbufs %lld busymapped %lld, delwri %lld\n",
	    bcstats.numbufs, bcstats.busymapped, bcstats.delwribufs);
	(*pr)("kvaslots %lld avail kva slots %lld\n",
	    bcstats.kvaslots, bcstats.kvaslots_avail);
    	(*pr)("bufpages %lld, dmapages %lld, dirtypages %lld\n",
	    bcstats.numbufpages, bcstats.dmapages, bcstats.numdirtypages);
	(*pr)("pendingreads %lld, pendingwrites %lld\n",
	    bcstats.pendingreads, bcstats.pendingwrites);
	(*pr)("highflips %lld, highflops %lld, dmaflips %lld\n",
	    bcstats.highflips, bcstats.highflops, bcstats.dmaflips);
}
#endif

void
buf_adjcnt(struct buf *bp, long ncount)
{
	KASSERT(ncount <= bp->b_bufsize);
	bp->b_bcount = ncount;
}

/* bufcache freelist code below */
/*
 * Copyright (c) 2014 Ted Unangst <tedu@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

/*
 * The code below implements a variant of the 2Q buffer cache algorithm by
 * Johnson and Shasha.
 *
 * General Outline
 * We divide the buffer cache into three working sets: current, previous,
 * and long term. Each list is itself LRU and buffers get promoted and moved
 * around between them. A buffer starts its life in the current working set.
 * As time passes and newer buffers push it out, it will turn into the previous
 * working set and is subject to recycling. But if it's accessed again from
 * the previous working set, that's an indication that it's actually in the
 * long term working set, so we promote it there. The separation of current
 * and previous working sets prevents us from promoting a buffer that's only
 * temporarily hot to the long term cache.
 *
 * The objective is to provide scan resistance by making the long term
 * working set ineligible for immediate recycling, even as the current 
 * working set is rapidly turned over.
 *
 * Implementation
 * The code below identifies the current, previous, and long term sets as
 * hotqueue, coldqueue, and warmqueue. The hot and warm queues are capped at
 * 1/3 of the total clean pages, after which point they start pushing their
 * oldest buffers into coldqueue.
 * A buf always starts out with neither WARM or COLD flags set (implying HOT).
 * When released, it will be returned to the tail of the hotqueue list.
 * When the hotqueue gets too large, the oldest hot buf will be moved to the
 * coldqueue, with the B_COLD flag set. When a cold buf is released, we set
 * the B_WARM flag and put it onto the warmqueue. Warm bufs are also
 * directly returned to the end of the warmqueue. As with the hotqueue, when
 * the warmqueue grows too large, B_WARM bufs are moved onto the coldqueue.
 *
 * Note that this design does still support large working sets, greater
 * than the cap of hotqueue or warmqueue would imply. The coldqueue is still
 * cached and has no maximum length. The hot and warm queues form a Y feeding
 * into the coldqueue. Moving bufs between queues is constant time, so this
 * design decays to one long warm->cold queue.
 *
 * In the 2Q paper, hotqueue and coldqueue are A1in and A1out. The warmqueue
 * is Am. We always cache pages, as opposed to pointers to pages for A1.
 *
 * This implementation adds support for multiple 2q caches.
 *
 * If we have more than one 2q cache, as bufs fall off the cold queue
 * for recyclying, bufs that have been warm before (which retain the
 * B_WARM flag in addition to B_COLD) can be put into the hot queue of
 * a second level 2Q cache. buffers which are only B_COLD are
 * recycled. Bufs falling off the last cache's cold queue are always
 * recycled.
 *
 */

/*
 * this function is called when a hot or warm queue may have exceeded its
 * size limit. it will move a buf to the coldqueue.
 */
int chillbufs(struct
    bufcache *cache, struct bufqueue *queue, int64_t *queuepages);

void
bufcache_init(void)
{
	int i;
	for (i=0; i < NUM_CACHES; i++) {
		TAILQ_INIT(&cleancache[i].hotqueue);
		TAILQ_INIT(&cleancache[i].coldqueue);
		TAILQ_INIT(&cleancache[i].warmqueue);
	}
	TAILQ_INIT(&dirtyqueue);
}

/*
 * if the buffer caches have shrunk, we may need to rebalance our queues.
 */
void
bufcache_adjust(void)
{
	int i;
	for (i=0; i < NUM_CACHES; i++) {
		while (chillbufs(&cleancache[i], &cleancache[i].warmqueue,
		    &cleancache[i].warmbufpages) ||
		    chillbufs(&cleancache[i], &cleancache[i].hotqueue,
		    &cleancache[i].hotbufpages))
			continue;
	}
}

/*
 * Get a clean buffer from the cache. if "discard" is set do not promote
 * previously warm buffers as normal, because we are tossing everything
 * away such as in a hibernation
 */
struct buf *
bufcache_getcleanbuf(int cachenum, int discard)
{
	struct buf *bp = NULL;
	struct bufcache *cache = &cleancache[cachenum];

	splassert(IPL_BIO);

	/* try  cold queue */
	while ((bp = TAILQ_FIRST(&cache->coldqueue))) {
		if ((!discard) &&
		    cachenum < NUM_CACHES - 1 && ISSET(bp->b_flags, B_WARM)) {
			int64_t pages = atop(bp->b_bufsize);
			struct bufcache *newcache;

			KASSERT(bp->cache == cachenum);

			/*
			 * If this buffer was warm before, move it to
			 * the hot queue in the next cache
			 */

			if (fliphigh) {
				/*
				 * If we are in the DMA cache, try to flip the
				 * buffer up high to move it on to the other
				 * caches. if we can't move the buffer to high
				 * memory without sleeping, we give it up and
				 * return it rather than fight for more memory
				 * against non buffer cache competitors.
				 */
				SET(bp->b_flags, B_BUSY);
				if (bp->cache == 0 && buf_flip_high(bp) == -1) {
					CLR(bp->b_flags, B_BUSY);
					return bp;
				}
				CLR(bp->b_flags, B_BUSY);
			}

			/* Move the buffer to the hot queue in the next cache */
			TAILQ_REMOVE(&cache->coldqueue, bp, b_freelist);
			CLR(bp->b_flags, B_WARM);
			CLR(bp->b_flags, B_COLD);
			bp->cache++;
			newcache= &cleancache[bp->cache];
			newcache->cachepages += pages;
			newcache->hotbufpages += pages;
			chillbufs(newcache, &newcache->hotqueue,
			    &newcache->hotbufpages);
			TAILQ_INSERT_TAIL(&newcache->hotqueue, bp, b_freelist);
		}
		else
			/* buffer is cold - give it up */
			return bp;
	}
	if ((bp = TAILQ_FIRST(&cache->warmqueue)))
		return bp;
	if ((bp = TAILQ_FIRST(&cache->hotqueue)))
 		return bp;
	return bp;
}

struct buf *
bufcache_getcleanbuf_range(int start, int end, int discard)
{
	int i, j = start, q = end;
	struct buf *bp = NULL;

	/*
	 * XXX in theory we could promote warm buffers into a previous queue
	 * so in the pathological case of where we go through all the caches
	 * without getting a buffer we have to start at the beginning again.
	 */
	while (j <= q)	{
		for (i = q; i >= j; i--)
			if ((bp = bufcache_getcleanbuf(i, discard)))
				return(bp);
		j++;
	}
	return bp;
}

struct buf *
bufcache_gethighcleanbuf(void)
{
	if (!fliphigh)
		return NULL;
	return bufcache_getcleanbuf_range(DMA_CACHE + 1, NUM_CACHES - 1, 0);
}

struct buf *
bufcache_getdmacleanbuf(void)
{
	if (fliphigh)
		return bufcache_getcleanbuf_range(DMA_CACHE, DMA_CACHE, 0);
	return bufcache_getcleanbuf_range(DMA_CACHE, NUM_CACHES - 1, 0);
}

struct buf *
bufcache_getdirtybuf(void)
{
	return TAILQ_FIRST(&dirtyqueue);
}

void
bufcache_take(struct buf *bp)
{
	struct bufqueue *queue;
	int64_t pages;

	splassert(IPL_BIO);
	KASSERT(ISSET(bp->b_flags, B_BC));
	KASSERT(bp->cache >= DMA_CACHE);
	KASSERT((bp->cache < NUM_CACHES));

	pages = atop(bp->b_bufsize);
	struct bufcache *cache = &cleancache[bp->cache];
	if (!ISSET(bp->b_flags, B_DELWRI)) {
                if (ISSET(bp->b_flags, B_COLD)) {
			queue = &cache->coldqueue;
		} else if (ISSET(bp->b_flags, B_WARM)) {
			queue = &cache->warmqueue;
			cache->warmbufpages -= pages;
		} else {
			queue = &cache->hotqueue;
			cache->hotbufpages -= pages;
		}
		bcstats.numcleanpages -= pages;
		cache->cachepages -= pages;
	} else {
		queue = &dirtyqueue;
		bcstats.numdirtypages -= pages;
		bcstats.delwribufs--;
	}
	TAILQ_REMOVE(queue, bp, b_freelist);
}

/* move buffers from a hot or warm queue to a cold queue in a cache */
int
chillbufs(struct bufcache *cache, struct bufqueue *queue, int64_t *queuepages)
{
	struct buf *bp;
	int64_t limit, pages;

	/*
	 * The warm and hot queues are allowed to be up to one third each.
	 * We impose a minimum size of 96 to prevent too much "wobbling".
	 */
	limit = cache->cachepages / 3;
	if (*queuepages > 96 && *queuepages > limit) {
		bp = TAILQ_FIRST(queue);
		if (!bp)
			panic("inconsistent bufpage counts");
		pages = atop(bp->b_bufsize);
		*queuepages -= pages;
		TAILQ_REMOVE(queue, bp, b_freelist);
		/* we do not clear B_WARM */
		SET(bp->b_flags, B_COLD);
		TAILQ_INSERT_TAIL(&cache->coldqueue, bp, b_freelist);
		return 1;
	}
	return 0;
}

void
bufcache_release(struct buf *bp)
{
	struct bufqueue *queue;
	int64_t pages;
	struct bufcache *cache = &cleancache[bp->cache];

	pages = atop(bp->b_bufsize);
	KASSERT(ISSET(bp->b_flags, B_BC));
	if (fliphigh) {
		if (ISSET(bp->b_flags, B_DMA) && bp->cache > 0)
			panic("B_DMA buffer release from cache %d",
			    bp->cache);
		else if ((!ISSET(bp->b_flags, B_DMA)) && bp->cache == 0)
			panic("Non B_DMA buffer release from cache %d",
			    bp->cache);
	}

	if (!ISSET(bp->b_flags, B_DELWRI)) {
		int64_t *queuepages;
		if (ISSET(bp->b_flags, B_WARM | B_COLD)) {
			SET(bp->b_flags, B_WARM);
			CLR(bp->b_flags, B_COLD);
			queue = &cache->warmqueue;
			queuepages = &cache->warmbufpages;
		} else {
			queue = &cache->hotqueue;
			queuepages = &cache->hotbufpages;
		}
		*queuepages += pages;
		bcstats.numcleanpages += pages;
		cache->cachepages += pages;
		chillbufs(cache, queue, queuepages);
	} else {
		queue = &dirtyqueue;
		bcstats.numdirtypages += pages;
		bcstats.delwribufs++;
	}
	TAILQ_INSERT_TAIL(queue, bp, b_freelist);
}

#ifdef HIBERNATE
/*
 * Nuke the buffer cache from orbit when hibernating. We do not want to save
 * any clean cache pages to swap and read them back. the original disk files
 * are just as good.
 */
void
hibernate_suspend_bufcache(void)
{
	struct buf *bp;
	int s;

	s = splbio();
	/* Chuck away all the cache pages.. discard bufs, do not promote */
	while ((bp = bufcache_getcleanbuf_range(DMA_CACHE, NUM_CACHES - 1, 1))) {
		bufcache_take(bp);
		if (bp->b_vp) {
			RBT_REMOVE(buf_rb_bufs, &bp->b_vp->v_bufs_tree, bp);
			brelvp(bp);
		}
		buf_put(bp);
	}
	splx(s);
}

void
hibernate_resume_bufcache(void)
{
	/* XXX Nothing needed here for now */
}
#endif /* HIBERNATE */
@


1.181
log
@Flip previously warm pages in the buffer cache to memory above the DMA
region if uvm tells us it is available.
nits from deraadt@@
ok krw@@ guenther@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.180 2017/02/28 10:49:37 natano Exp $	*/
d1510 3
a1512 1
	return bufcache_getcleanbuf_range(DMA_CACHE + 1, NUM_CACHES -1, 0);
d1518 3
a1520 1
	return bufcache_getcleanbuf_range(DMA_CACHE, DMA_CACHE, 0);
@


1.180
log
@Switch geteblks()'s size argument from int to size_t. It's called with
unsigned variables as argument in most places anyway. Decrease the
chance of signedness/range mismatch issues.

ok stefan
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.179 2017/02/14 10:31:15 mpi Exp $	*/
d62 6
d75 2
d142 1
d148 17
d272 2
a273 2
	while ((bp = bufcache_getanycleanbuf()) &&
	    (bcstats.numbufpages > targetpages)) {
d313 43
d373 8
a380 1
void
d383 3
d389 13
a401 2
	CLR(bp->b_flags, B_DMA);
	/* XXX does nothing to buffer for now */
d404 5
d414 2
d417 10
a426 4
		KASSERT(bp->cache > DMA_CACHE);
		KASSERT(bp->cache < NUM_CACHES);
		/* XXX does not flip buffer for now */
		/* make buffer hot, in DMA_CACHE, once it gets released. */
a428 1
		SET(bp->b_flags, B_DMA);
d905 1
a905 1
	if (needbuffer && bcstats.numbufpages < targetpages &&
d1044 2
a1045 2
		while ((bcstats.numbufpages + npages > targetpages) &&
		    (bp = bufcache_getanycleanbuf())) {
d1060 1
a1060 1
		if ((bcstats.numbufpages + npages > targetpages ||
d1069 1
a1069 1
		if (bcstats.numbufpages + npages > bufpages) {
d1300 2
a1301 2
    	(*pr)("bufpages %lld, dirtypages %lld\n",
	    bcstats.numbufpages,  bcstats.numdirtypages);
d1304 2
d1437 5
d1444 1
a1444 1
			 *  the hot queue in the next cache
d1446 19
a1467 4
			int64_t pages = atop(bp->b_bufsize);
			KASSERT(bp->cache == cachenum);
			if (bp->cache == 0)
				buf_flip_high(bp);
d1469 1
a1469 1
			struct bufcache *newcache = &cleancache[bp->cache];
d1508 1
a1508 1
bufcache_getanycleanbuf(void)
d1510 1
a1510 1
	return bufcache_getcleanbuf_range(DMA_CACHE, NUM_CACHES -1, 0);
d1513 5
a1531 1

d1535 1
d1591 1
d1594 9
a1602 2
	KASSERT((ISSET(bp->b_flags, B_DMA) && bp->cache == 0)
	    || ((!ISSET(bp->b_flags, B_DMA)) && bp->cache > 0));
@


1.179
log
@Convert most of the manual checks for CPU hogging to sched_pause().

The distinction between preempt() and yield() stays as it is usueful
to know if a thread decided to yield by itself or if the kernel told
him to go away.

ok tedu@@, guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.178 2016/09/16 02:54:51 dlg Exp $	*/
d896 1
a896 1
geteblk(int size)
@


1.178
log
@move buf_rb_bufs from RB macros to RBT functions

i had to shuffle the order of some header bits cos RBT_PROTOTYPE
needs to see what RBT_HEAD produces.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.177 2016/09/15 02:00:16 dlg Exp $	*/
d1091 1
a1091 1
			sched_pause();
@


1.177
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.176 2016/09/04 10:51:24 naddy Exp $	*/
d250 1
a250 2
			RB_REMOVE(buf_rb_bufs,
			    &bp->b_vp->v_bufs_tree, bp);
d760 1
a760 2
			RB_REMOVE(buf_rb_bufs, &bp->b_vp->v_bufs_tree,
			    bp);
d827 1
a827 1
	bp = RB_FIND(buf_rb_bufs, &vp->v_bufs_tree, &b);
d863 1
a863 1
	bp = RB_FIND(buf_rb_bufs, &vp->v_bufs_tree, &b);
d946 1
a946 1
				RB_REMOVE(buf_rb_bufs,
d1007 1
a1007 1
		if (RB_INSERT(buf_rb_bufs, &vp->v_bufs_tree, bp))
d1506 1
a1506 2
			RB_REMOVE(buf_rb_bufs,
			    &bp->b_vp->v_bufs_tree, bp);
@


1.176
log
@Remove support for tape block devices. Nobody mount(8)s tapes any longer.
ok deraadt@@ guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.175 2016/06/07 01:31:54 tedu Exp $	*/
d196 1
a196 2
	pool_init(&bufpool, sizeof(struct buf), 0, 0, 0, "bufpl", NULL);
	pool_setipl(&bufpool, IPL_BIO);
@


1.175
log
@per trending style, add continue to empty loops.
ok mglocker
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.174 2016/03/17 03:57:51 beck Exp $	*/
d672 1
a672 8
	/* If this is a tape block, write the block now. */
	if (major(bp->b_dev) < nblkdev &&
	    bdevsw[major(bp->b_dev)].d_type == D_TAPE) {
		bawrite(bp);
		return;
	}

	/* Otherwise, the "write" is done, so mark and release the buffer. */
@


1.174
log
@- add realloc_pages to move a buffer's physmem from one range to another.
- modify B_DMA handling to be in vfs_biomem.c
- change buffer allocation to try allocations with NOWAIT and to throw away clean pages
  if allocation fails - allocate with WAITOK only if you can't throw away enough pages to
  succeed
"probably sound" deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.173 2016/03/10 03:09:45 beck Exp $	*/
d911 1
a911 1
		;
d1322 1
a1322 1
			;
@


1.173
log
@Start some refactoring in here.  this gets bufadjust  out
of the hibernate path and starts preparing for some other work in here
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.172 2016/03/06 19:15:23 mpi Exp $	*/
d1030 1
a1030 1
		SET(bp->b_flags, B_DMA);
@


1.172
log
@Do not fetch the same block multiple times if it is already present
in the buffer cache.

When the Dynamic Buffer Cache has been introduced bread_cluster()
became the replacement of cluster_read().  However this function
did not consider the B_CACHE flag of the first buffer of a cluster
like its predecessor did.

This improves a lot read operations on MSDOSFS while reducing the
number of DMA operations.

ok beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.171 2015/11/28 21:52:02 beck Exp $	*/
d1326 5
d1332 1
a1332 1
bufcache_getcleanbuf(int cachenum)
d1341 2
a1342 1
		if (cachenum < NUM_CACHES - 1 && ISSET(bp->b_flags, B_WARM)) {
a1372 1

d1374 1
a1374 1
bufcache_getanycleanbuf(void)
d1376 1
a1376 1
	int i, j = 0, q = NUM_CACHES - 1;
d1386 1
a1386 1
			if ((bp = bufcache_getcleanbuf(i)))
d1393 6
d1501 3
a1503 1
 * Flush buffercache to lowest value on hibernate suspend
d1508 2
a1509 1
	long save_buflowpages = buflowpages;
d1511 12
a1522 5
	/* Shrink buffercache to 16MB (4096 pages) */
	buflowpages = 4096;
	bufadjust(buflowpages);
	buflowpages = save_buflowpages;
	bufhighpages = bufpages;
d1528 1
a1528 6
	uint64_t dmapages, pgs;

	dmapages = uvm_pagecount(&dma_constraint);
	pgs = bufcachepercent * dmapages / 100;
	bufadjust(pgs);
	bufhighpages = bufpages;
@


1.171
log
@move buffer size adjustment to buf_adjcnt - from Walter Neto
ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.170 2015/07/19 16:21:11 beck Exp $	*/
d464 6
d476 1
a476 1
	maxra++; 
@


1.170
log
@Use two 2q caches for the buffer cache, moving previously warm buffers from the
first queue to the second.
Mark the first queue as DMA in preparation for being able to use more memory
by flipping. Flipper code currently only sets and clears the flag.
ok tedu@@ guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.169 2015/03/14 03:38:51 jsg Exp $	*/
d1208 7
@


1.169
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.168 2015/02/07 08:21:24 miod Exp $	*/
d94 5
d247 1
a247 1
	while ((bp = bufcache_getcleanbuf()) &&
d306 27
d511 6
a516 1
		SET(xbpp[i]->b_flags, B_READ | B_ASYNC);
d605 1
d660 1
d705 1
d942 1
a942 1
		 * If would go over the page target with our
d947 1
a947 1
		    (bp = bufcache_getcleanbuf())) {
d1024 1
d1028 1
d1169 1
a1169 1
		if (!ISSET(bp->b_flags, B_READ))
d1171 1
a1171 1
		else
d1256 1
a1256 1
 * the warmqueue grows too large, bufs are moved onto the coldqueue.
d1266 10
a1275 5
 * 
 */

/*
 * 
a1276 7
TAILQ_HEAD(bufqueue, buf);
struct bufqueue hotqueue;
int64_t hotbufpages;
struct bufqueue coldqueue;
struct bufqueue warmqueue;
int64_t warmbufpages;
struct bufqueue dirtyqueue;
d1282 2
a1283 1
int chillbufs(struct bufqueue *queue, int64_t *queuepages);
d1288 6
a1293 4

	TAILQ_INIT(&hotqueue);
	TAILQ_INIT(&coldqueue);
	TAILQ_INIT(&warmqueue);
d1298 1
a1298 1
 * if the buffer cache shrunk, we may need to rebalance our queues.
d1303 8
a1310 3
	while (chillbufs(&warmqueue, &warmbufpages) ||
	    chillbufs(&hotqueue, &hotbufpages))
		;
d1314 1
a1314 1
bufcache_getcleanbuf(void)
d1316 4
a1319 1
	struct buf *bp;
d1321 27
a1347 1
	if ((bp = TAILQ_FIRST(&coldqueue)))
d1349 24
a1372 3
	if ((bp = TAILQ_FIRST(&warmqueue)))
		return bp;
	return TAILQ_FIRST(&hotqueue);
d1375 1
d1390 3
d1394 1
d1396 5
a1400 5
		if (ISSET(bp->b_flags, B_WARM)) {
			queue = &warmqueue;
			warmbufpages -= pages;
		} else if (ISSET(bp->b_flags, B_COLD)) {
			queue = &coldqueue;
d1402 2
a1403 2
			queue = &hotqueue;
			hotbufpages -= pages;
d1406 1
d1415 1
d1417 1
a1417 1
chillbufs(struct bufqueue *queue, int64_t *queuepages)
d1426 1
a1426 1
	limit = bcstats.numcleanpages / 3;
d1434 1
a1434 1
		CLR(bp->b_flags, B_WARM);
d1436 1
a1436 1
		TAILQ_INSERT_TAIL(&coldqueue, bp, b_freelist);
d1447 1
a1447 1
	
d1449 3
d1456 3
a1458 2
			queue = &warmqueue;
			queuepages = &warmbufpages;
d1460 2
a1461 2
			queue = &hotqueue;
			queuepages = &hotbufpages;
d1465 2
a1466 1
		chillbufs(queue, queuepages);
@


1.168
log
@Introduce VM_KERNEL_SPACE_SIZE as a replacement for
(VM_MAX_KERNEL_ADDRESS - VM_MIN_KERNEL_ADDRESS). This will allow these to no
longer be constants in the future.

ok guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.167 2015/01/18 14:01:54 miod Exp $	*/
a59 5

#ifdef HIBERNATE
#include <sys/hibernate.h>
#endif /* HIBERNATE */

@


1.167
log
@Revert 1.166 (but keep the bufq_wait() interface change), for this is wrong
and the bufq pointer might be NULL at the time it is `saved'.

Found the hard way on sparc due to the limited kva, with all disk active
processes ending up sleeping on "buf_needva".

ok kettenis@@ krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.166 2015/01/09 05:04:22 tedu Exp $	*/
d182 1
a182 1
		bufkvm = (VM_MAX_KERNEL_ADDRESS - VM_MIN_KERNEL_ADDRESS) / 10;
@


1.166
log
@save the bufq pointer from the buf before we turn it loose so it won't
change on us. also, remove unused second arg to bufq_wait.
from pedro at bitrig via david hill.
ok beck kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.165 2014/12/16 18:30:04 tedu Exp $	*/
a518 1
	struct bufq *bq;
a571 1
	bq = bp->b_bq;
d581 2
a582 2
	if (bq)
		bufq_wait(bq);
@


1.165
log
@primary change: move uvm_vnode out of vnode, keeping only a pointer.
objective: vnode.h doesn't include uvm_extern.h anymore.
followup changes: include uvm_extern.h or lock.h where necessary.
ok and help from deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.164 2014/12/11 14:33:48 jmc Exp $	*/
d519 1
d573 1
d583 2
a584 2
	if (bp->b_bq)
		bufq_wait(bp->b_bq, bp);
@


1.164
log
@typos; from kaspars bankovskis
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.163 2014/10/08 07:33:14 blambert Exp $	*/
d64 2
@


1.163
log
@Now that the cleaner yields the cpu, we can stop checking
to see if we've hogged the cpu for >1 second.

okay miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.162 2014/09/09 07:07:39 blambert Exp $	*/
d89 1
a89 1
long bufhighpages; 	/* largerst size cache allowed */
d962 1
a962 1
		 * this buffer instead of starting its own guf_get.
@


1.162
log
@Make the cleaner, syncer, pagedaemon, aiodone daemons all
yield() if the cpu is marked SHOULDYIELD.

ok miod@@ tedu@@ phessler@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.161 2014/08/31 21:08:48 tedu Exp $	*/
a1000 1
	struct timeval starttime, timediff;
a1022 2
		getmicrouptime(&starttime);

a1023 1
			struct timeval tv;
a1057 3
			/* Never allow processing to run for more than 1 sec */
			getmicrouptime(&tv);
			timersub(&tv, &starttime, &timediff);
a1058 3
			if (timediff.tv_sec)
				break;

@


1.161
log
@replace LRU bufcache with something originally modelled after 2Q.
this should provide a degree of scan resistance, and also serves as a
midway point for further development of multi queue algorithms.
i've tried to minimize the risk and degree of regressions.
probably ok beck
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.160 2014/07/13 15:48:41 tedu Exp $	*/
d1059 2
@


1.160
log
@trim some casts
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.159 2014/07/13 15:29:04 tedu Exp $	*/
d69 4
d255 1
d1179 58
d1239 1
a1239 1
 * simple LRU queues, one clean and one dirty
d1242 5
a1246 1
struct bufqueue cleanqueue;
d1249 6
d1259 3
a1261 1
	TAILQ_INIT(&cleanqueue);
d1265 11
d1279 7
a1285 1
	return TAILQ_FIRST(&cleanqueue);
d1298 1
d1302 1
d1304 10
a1313 2
		queue = &cleanqueue;
		bcstats.numcleanpages -= atop(bp->b_bufsize);
d1316 1
a1316 1
		bcstats.numdirtypages -= atop(bp->b_bufsize);
d1322 26
d1352 1
d1354 1
d1356 12
a1367 2
		queue = &cleanqueue;
		bcstats.numcleanpages += atop(bp->b_bufsize);
d1370 1
a1370 1
		bcstats.numdirtypages += atop(bp->b_bufsize);
@


1.159
log
@use mallocarray where arguments are multipled. ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.158 2014/07/12 18:43:32 tedu Exp $	*/
d444 1
a444 1
	xbpp = mallocarray((howmany + 1), sizeof(struct buf *), M_TEMP, M_NOWAIT);
@


1.158
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.157 2014/07/11 03:06:08 mlarkin Exp $	*/
d444 1
a444 1
	xbpp = malloc((howmany + 1) * sizeof(struct buf *), M_TEMP, M_NOWAIT);
@


1.157
log
@Flush the buffercache to 16MB on hibernate and restore its previous max
size (kern.bufcachepercent) on resume, for better hibernate performance.

ok beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.156 2014/07/08 17:19:25 deraadt Exp $	*/
d411 1
a411 1
	free(xbpp, M_TEMP);
d463 1
a463 1
			free(xbpp, M_TEMP);
@


1.156
log
@decouple struct uvmexp into a new file, so that uvm_extern.h and sysctl.h
don't need to be married.
ok guenther miod beck jsing kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.155 2014/04/10 13:48:24 tedu Exp $	*/
d61 4
d1235 28
@


1.155
log
@pull the bufcache freelist code out into separate functions to allow new
algorithms to be tested. in the process, drop support for unused B_AGE and
b_synctime options.
previous versions ok beck deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.154 2014/01/25 04:23:31 beck Exp $	*/
a59 2

#include <uvm/uvm_extern.h>
@


1.154
log
@get rid of the kernel high and low water marks for pages and replace with
a single target of used pages in the cache - so we don't to a ton of work
throwing away pages sporadically while holding the biglock - noticed by dlg.
ok dlg@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.153 2013/12/09 17:16:35 beck Exp $	*/
a62 9
/*
 * Definitions for the buffer free lists.
 */
#define	BQUEUES		2		/* number of free buffer queues */

#define	BQ_DIRTY	0		/* LRU queue with dirty buffers */
#define	BQ_CLEAN	1		/* LRU queue with clean buffers */

TAILQ_HEAD(bqueues, buf) bufqueues[BQUEUES];
a73 6
/*
 * Insq/Remq for the buffer free lists.
 */
#define	binsheadfree(bp, dp)	TAILQ_INSERT_HEAD(dp, bp, b_freelist)
#define	binstailfree(bp, dp)	TAILQ_INSERT_TAIL(dp, bp, b_freelist)

a91 30
bremfree(struct buf *bp)
{
	struct bqueues *dp = NULL;

	splassert(IPL_BIO);

	/*
	 * We only calculate the head of the freelist when removing
	 * the last element of the list as that is the only time that
	 * it is needed (e.g. to reset the tail pointer).
	 *
	 * NB: This makes an assumption about how tailq's are implemented.
	 */
	if (TAILQ_NEXT(bp, b_freelist) == NULL) {
		for (dp = bufqueues; dp < &bufqueues[BQUEUES]; dp++)
			if (dp->tqh_last == &TAILQ_NEXT(bp, b_freelist))
				break;
		if (dp == &bufqueues[BQUEUES])
			panic("bremfree: lost tail");
	}
	if (!ISSET(bp->b_flags, B_DELWRI)) {
		bcstats.numcleanpages -= atop(bp->b_bufsize);
	} else {
		bcstats.numdirtypages -= atop(bp->b_bufsize);
		bcstats.delwribufs--;
	}
	TAILQ_REMOVE(dp, bp, b_freelist);
}

void
a125 1
	struct bqueues *dp;
d191 1
a191 2
	for (dp = bufqueues; dp < &bufqueues[BQUEUES]; dp++)
		TAILQ_INIT(dp);
d239 1
a239 1
	while ((bp = TAILQ_FIRST(&bufqueues[BQ_CLEAN])) &&
d241 1
a241 1
		bremfree(bp);
a690 1
	struct bqueues *bufq;
d742 1
a743 15
		if (!ISSET(bp->b_flags, B_DELWRI)) {
			bcstats.numcleanpages += atop(bp->b_bufsize);
			bufq = &bufqueues[BQ_CLEAN];
		} else {
			bcstats.numdirtypages += atop(bp->b_bufsize);
			bcstats.delwribufs++;
			bufq = &bufqueues[BQ_DIRTY];
		}
		if (ISSET(bp->b_flags, B_AGE)) {
			binsheadfree(bp, bufq);
			bp->b_synctime = time_uptime + 30;
		} else {
			binstailfree(bp, bufq);
			bp->b_synctime = time_uptime + 300;
		}
d838 1
a838 1
			bremfree(bp);
d903 2
a904 2
		    (bp = TAILQ_FIRST(&bufqueues[BQ_CLEAN]))) {
			bremfree(bp);
a943 1
	bp->b_synctime = time_uptime + 300;
d1019 1
a1019 1
		while ((bp = TAILQ_FIRST(&bufqueues[BQ_DIRTY]))) {
d1027 1
a1027 1
			bremfree(bp);
d1038 1
a1038 1
				panic("Clean buffer on BQ_DIRTY");
d1045 1
a1045 3
				bcstats.numdirtypages += atop(bp->b_bufsize);
				bcstats.delwribufs++;
				binstailfree(bp, &bufqueues[BQ_DIRTY]);
d1170 63
@


1.153
log
@Don't keep removed files (B_INVAL bufs) in the buffer cache. Free buffers
once brelse() is called and the buffer is B_INVAL.
ok jsing@@ krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.152 2013/08/08 23:25:06 syl Exp $	*/
d96 4
a99 5
long lopages;	      	/* page recycling low water mark */
long hipages;   	/* page recycling high water mark */
long buflowpages;	/* bufpages absolute low water mark */
long bufhighpages; 	/* bufpages absolute high water mark */
long bufbackpages; 	/* number of pages we back off when asked to shrink */
d256 1
a256 2
	 * We are allowed to use up to the reserve. When we hit it,
	 * we free 10% of the cache size to allow room to recycle.
d258 1
a258 2
	hipages = bufpages - RESERVE_PAGES;
	lopages = hipages - (hipages / 10);
d268 1
a268 1
	int s, growing = 0;
a273 2
	if (newbufpages >= bufpages)
		growing = 1;
d277 1
a277 2
	 * We are allowed to use up to the reserve. When we hit it,
	 * we free 10% of the cache size to allow room to recycle.
d279 1
a279 2
	hipages = bufpages - RESERVE_PAGES;
	lopages = hipages - (hipages / 10);
d282 3
a284 3
	 * If we are shrinking the cache we are under some memory pressure.
	 * If we have more buffers allocated than our new low water mark,
	 * immediately free them.
d286 2
a287 2
	while (!growing && (bp = TAILQ_FIRST(&bufqueues[BQ_CLEAN])) &&
	    (bcstats.numbufpages > lopages)) {
d301 2
a302 2
	if (!growing && (UNCLEAN_PAGES >= hidirtypages ||
	    bcstats.kvaslots_avail <= 2 * RESERVE_SLOTS))
d825 1
a825 1
	if (needbuffer && bcstats.numbufpages < hipages &&
d952 3
a954 3
		 * If our allocation would take us over the
		 * high water mark, see if we can grow the
		 * cache.
d956 2
a957 7
		if (bcstats.numbufpages + npages > hipages &&
		    bufpages < bufhighpages) {
			int i = bufbackpages;
			if (bufpages + i > bufhighpages)
				i = bufhighpages - bufpages;
			bufadjust(bufpages + i);
		}
d960 3
a962 2
		 * If we're still above the high water mark for pages,
		 * free down to the low water mark.
d964 7
a970 10
		if (bcstats.numbufpages + npages > hipages) {
			while ((bcstats.numbufpages > lopages) &&
			    (bp = TAILQ_FIRST(&bufqueues[BQ_CLEAN]))) {
				bremfree(bp);
				if (bp->b_vp) {
					RB_REMOVE(buf_rb_bufs,
					    &bp->b_vp->v_bufs_tree, bp);
					brelvp(bp);
				}
				buf_put(bp);
d972 1
d980 1
a980 1
		if ((bcstats.numbufpages + npages > hipages ||
@


1.152
log
@Uncomment kprintf format attributes for sys/kern

tested on vax (gcc3) ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.151 2013/07/09 15:37:43 beck Exp $	*/
d763 2
a764 2
		 * If the buffer is invalid, place it in the clean queue, so it
		 * can be reused.
d781 5
a785 2
		 * If the buffer has no associated data, place it back in the
		 * pool.
d787 3
a789 18
		if (bp->b_data == NULL && bp->b_pobj == NULL) {
			/*
			 * Wake up any processes waiting for _this_ buffer to
			 * become free. They are not allowed to grab it
			 * since it will be freed. But the only sleeper is
			 * getblk and it's restarting the operation after
			 * sleep.
			 */
			if (ISSET(bp->b_flags, B_WANTED)) {
				CLR(bp->b_flags, B_WANTED);
				wakeup(bp);
			}
			if (bp->b_vp != NULL)
				RB_REMOVE(buf_rb_bufs,
				    &bp->b_vp->v_bufs_tree, bp);
			buf_put(bp);
			splx(s);
			return;
d791 1
a791 3

		bcstats.numcleanpages += atop(bp->b_bufsize);
		binsheadfree(bp, &bufqueues[BQ_CLEAN]);
d813 10
a824 4
	/* Unlock the buffer. */
	CLR(bp->b_flags, (B_AGE | B_ASYNC | B_NOCACHE | B_DEFERRED));
	buf_release(bp);

a835 6
	}

	/* Wake up any processes waiting for _this_ buffer to become free. */
	if (ISSET(bp->b_flags, B_WANTED)) {
		CLR(bp->b_flags, B_WANTED);
		wakeup(bp);
@


1.151
log
@back out the cache flipper temporarily to work out of tree.
will come back soon.
ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.147 2013/06/11 16:42:16 deraadt Exp $	*/
d1242 2
a1243 1
void	bcstats_print(int (*)(const char *, ...) /* __attribute__((__format__(__kprintf__,1,2))) */);
d1249 1
a1249 1
    int (*pr)(const char *, ...) /* __attribute__((__format__(__kprintf__,1,2))) */)
@


1.150
log
@beck would prefer to keep things just as they were for a while longer.
undo style changes.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.149 2013/06/11 21:51:55 tedu Exp $	*/
a4 1
 * Copyright (c) 2012,2013 Bob Beck <beck@@openbsd.org>
d66 1
a66 1
#define	BQUEUES		3		/* number of free buffer queues */
d69 1
a69 2
#define	BQ_CLEANL	1		/* LRU queue with clean low buffers */
#define	BQ_CLEANH	2		/* LRU queue with clean high buffers */
a71 4
int	bfreeclean(int, struct bqueues *);
struct uvm_constraint_range high_constraint;
psize_t b_dmapages_total, b_highpages_total, b_dmamaxpages;
int needda;
a112 5
	KASSERT(ISSET(bp->b_flags, B_BC));
	KASSERT(!ISSET(bp->b_flags, B_BUSY));
	if (bp->b_freelist.tqe_next == NOLIST ||
	    bp->b_freelist.tqe_next == (void *)-1)
		panic("bremfree: - buf %p not on a free list!", bp);
d114 14
a128 4
		if (ISSET(bp->b_flags, B_DMA))
			dp = &bufqueues[BQ_CLEANL];
		else
			dp = &bufqueues[BQ_CLEANH];
a130 1
		dp = &bufqueues[BQ_DIRTY];
a136 23
int
bfreeclean(int npages, struct bqueues *dp)
{
	struct buf *bp;
	int i = 0;

	splassert(IPL_BIO);
	while (i < npages) {
 		bp = TAILQ_FIRST(dp);
		if (bp == NULL)
			return(-1);
		i += atop(bp->b_bufsize);
		bremfree(bp);
		if (bp->b_vp) {
			RB_REMOVE(buf_rb_bufs,
			    &bp->b_vp->v_bufs_tree, bp);
			brelvp(bp);
		}
		buf_put(bp);
	}
	return(0);
}

d161 1
a161 1
		 return;
d171 1
d174 3
a176 13
	/* How much DMA accessible memory will we consider? */
	b_dmapages_total = uvm_pagecount(&dma_constraint);
	/* Take away a guess at how much of this the kernel will consume. */
	b_dmapages_total -= (atop(physmem) - atop(uvmexp.free));

	/* See if we have memory above the dma accessible region. */
	high_constraint.ucr_low = dma_constraint.ucr_high;
	high_constraint.ucr_high = no_constraint.ucr_high;
	if (high_constraint.ucr_low != high_constraint.ucr_high) {
		high_constraint.ucr_low++;
		b_highpages_total = uvm_pagecount(&high_constraint);
	} else
		b_highpages_total = 0;
d192 1
a192 2
		bufpages = (b_dmapages_total + b_highpages_total)
		    * bufcachepercent / 100;
d195 1
d198 1
a198 1
	b_dmamaxpages = b_dmapages_total * bufcachepercent / 100;
d203 1
a203 1
	buflowpages = b_dmapages_total * 5 / 100;
d270 1
d293 9
a301 5
	if (!growing && (bcstats.numbufpages > lopages)) {
		if (bfreeclean(bcstats.numbufpages - lopages,
			&bufqueues[BQ_CLEANH]) != 0)
			(void) bfreeclean(bcstats.numbufpages - lopages,
			    &bufqueues[BQ_CLEANL]);
a323 2
	 * Also called buy the buffer cache to back off if memory
	 * allocation in a particular range fails.
d325 1
a325 1
	 * It returns 0 to the caller to indicate that it has
a342 1

d344 1
a344 15
	if (b_highpages_total
	    && (range->ucr_high <= dma_constraint.ucr_high)) {
		/*
		 * Free up DMA accessible memory by moving pages to
		 * the high range.
		 */
		if (bufhigh(pdelta) == 0)
			return(0); /* we moved enough pages up high */
		else {
			bufadjust(bufpages - pdelta); /* shrink the cache. */
		}
	} else {
		/* Free memory by shrinking the cache. */
		bufadjust(bufpages - pdelta);
	}
d529 1
a529 6
		/*
		 * We set B_DMA here because bp above will be B_DMA,
		 * and we are playing buffer slice-n-dice games from
		 * the memory allocated in bp.
		 */
		SET(xbpp[i]->b_flags, B_DMA | B_READ | B_ASYNC);
a534 1
		buf_dma(xbpp[i]);
d763 2
a764 5
		 * If the buffer is invalid, free it now rather than
		 * putting it on any queue and wasting cache space.
		 *
		 * XXX we could queue it here for a later TRIM operation.
		 *
d781 2
a782 5
		 * Wake up any processes waiting for _this_ buffer to
		 * become free. They are not allowed to grab it
		 * since it will be freed. But the only sleeper is
		 * getblk and it's restarting the operation after
		 * sleep.
d784 18
a801 3
		if (ISSET(bp->b_flags, B_WANTED)) {
			CLR(bp->b_flags, B_WANTED);
			wakeup(bp);
d803 3
a805 3
		if (ISSET(bp->b_flags, B_DMA) && needda)
			wakeup(&needda);
		buf_put(bp);
d812 4
a815 1
		if (ISSET(bp->b_flags, B_DELWRI)) {
a818 6
		} else {
			bcstats.numcleanpages += atop(bp->b_bufsize);
			if (ISSET(bp->b_flags, B_DMA))
				bufq = &bufqueues[BQ_CLEANL];
			else
				bufq = &bufqueues[BQ_CLEANH];
d827 5
a831 3
		/* Unlock the buffer. */
		CLR(bp->b_flags, (B_AGE | B_ASYNC | B_NOCACHE | B_DEFERRED));
		buf_release(bp);
a832 10
		if (ISSET(bp->b_flags, B_DMA) && needda) {
			wakeup(&needda);
		}
		/* Wake up any processes waiting for _this_ buffer to
		 * become free. */
		if (ISSET(bp->b_flags, B_WANTED)) {
			CLR(bp->b_flags, B_WANTED);
			wakeup(bp);
		}
	}
d846 6
d893 10
d990 10
a999 4
			if (bfreeclean(bcstats.numbufpages - lopages,
				&bufqueues[BQ_CLEANH]) != 0)
				(void) bfreeclean(bcstats.numbufpages
				    - lopages, &bufqueues[BQ_CLEANL]);
a1001 5

		if (b_highpages_total && bcstats.dmapages + npages >
		    b_dmamaxpages)
			bufhigh(bcstats.dmapages + npages - b_dmamaxpages);

a1031 2
	/* Mark buffer as the cache's */
	SET(bp->b_flags, B_BC);
a1070 1
		buf_dma(bp);
a1240 122
/*
 * Ensure buffer is DMA reachable
 */
void
buf_dma(struct buf *buf)
{
	struct buf *b;
	int s;

start:
	KASSERT(ISSET(buf->b_flags, B_BC));
	KASSERT(ISSET(buf->b_flags, B_BUSY));
	KASSERT(buf->b_pobj != NULL);
	s = splbio();
	/*
	 * If we are adding to the queue, and we are not the cleaner or
	 * the syncer, ensure we free down below the max
	 */
	while (b_highpages_total &&
	    curproc != syncerproc && curproc != cleanerproc &&
	    (!ISSET(buf->b_flags, B_DMA)) &&
	    (bcstats.dmapages > (b_dmamaxpages - atop(buf->b_bufsize)))) {
		b = TAILQ_FIRST(&bufqueues[BQ_CLEANL]);
		KASSERT(!ISSET(b->b_flags, B_BUSY));
		if (b == NULL) {
			/* no non-busy buffers. */
			needda++;
			tsleep(&needda, PRIBIO, "needda", 0);
			needda--;
			splx(s);
			goto start;
		} else {
			bremfree(b);
			buf_acquire_nomap(b);
			if (buf_realloc_pages(b, &high_constraint,
			    UVM_PLA_NOWAIT) == 0) {
				/* move the buffer to high memory if we can */
				if (ISSET(b->b_flags, B_DMA))
					panic("B_DMA after high flip %p", b);
				binstailfree(b, &bufqueues[BQ_CLEANH]);
				buf_release(b);
			} else {
				/* otherwise just free the buffer */
				buf_release(b);
				if (b->b_vp) {
					RB_REMOVE(buf_rb_bufs,
					    &b->b_vp->v_bufs_tree, b);
					brelvp(b);
				}
				buf_put(b);
			}
		}
	}
	if (!ISSET(buf->b_flags, B_DMA)) {
		/* move buf to dma reachable memory */
		(void) buf_realloc_pages(buf, &dma_constraint, UVM_PLA_WAITOK);
		if (!ISSET(buf->b_flags, B_DMA))
			panic("non-dma buffer after dma move %p\n", buf);
	}
	splx(s);
	return;
}

/*
 * Attempt to flip "delta" dma reachable cache pages high. return 0 if we can,
 * -1 otherwise.
 */
int
bufhigh(int delta)
{
	psize_t newdmapages;
	struct buf *b, *bn;
	int s;
	if (!b_highpages_total)
		return(-1);
       	s = splbio();
	newdmapages = bcstats.dmapages - delta;
	b = TAILQ_FIRST(&bufqueues[BQ_CLEANL]);
	while ((bcstats.dmapages > newdmapages) && (b != NULL)) {
		while (ISSET(b->b_flags, B_BUSY)) {
			b = TAILQ_NEXT(b, b_freelist);
		}
		if (b != NULL) {
			bn = TAILQ_NEXT(b, b_freelist);
			bremfree(b);
			buf_acquire_nomap(b);
		moveit:
			if (buf_realloc_pages(b, &high_constraint,
			    UVM_PLA_NOWAIT) == 0) {
				/* move the buffer to high memory if we can */
				if (ISSET(b->b_flags, B_DMA))
					panic("B_DMA after high flip %p", b);
				binstailfree(b, &bufqueues[BQ_CLEANH]);
				buf_release(b);
			} else {
				/* free up some high memory and try again. */
				if (bfreeclean(delta, &bufqueues[BQ_CLEANH])
				    == 0)
					goto moveit;
				else {
					/* otherwise just free the buffer */
					buf_release(b);
					if (b->b_vp) {
						RB_REMOVE(buf_rb_bufs,
						    &b->b_vp->v_bufs_tree, b);
						brelvp(b);
					}
					buf_put(b);
				}
			}
			b = bn;
		}
	}
	wakeup(&needda);
	splx(s);
	if (bcstats.dmapages > newdmapages)
	  	return(-1);
	else
		return(0);
}


d1255 2
a1256 2
    	(*pr)("total bufpages %lld, dmapages %lld, dirtypages %lld\n",
	    bcstats.numbufpages, bcstats.dmapages, bcstats.numdirtypages);
@


1.149
log
@sprinkle knf fairy dust over new buf code
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.148 2013/06/11 19:01:20 beck Exp $	*/
d186 1
a186 1
		return;
d226 2
a227 2
		bufpages = (b_dmapages_total + b_highpages_total) *
		    bufcachepercent / 100;
d329 1
a329 1
			(void)bfreeclean(bcstats.numbufpages - lopages,
d376 2
a377 2
	if (b_highpages_total &&
	    (range->ucr_high <= dma_constraint.ucr_high)) {
d384 1
a384 1
		else
d386 1
d1029 3
a1031 3
			    &bufqueues[BQ_CLEANH]) != 0)
				(void)bfreeclean(bcstats.numbufpages -
				    lopages, &bufqueues[BQ_CLEANL]);
a1353 1

d1377 2
a1378 2
				if (bfreeclean(delta,
				    &bufqueues[BQ_CLEANH]) == 0) {
d1380 1
a1380 1
				} else {
@


1.148
log
@High memory page flipping for the buffer cache.

This change splits the buffer cache free lists into lists of dma reachable
buffers and high memory buffers based on the ranges returned by pmemrange.
Buffers move from dma to high memory as they age, but are flipped to dma
reachable memory if IO is needed to/from and high mem buffer. The total
amount of buffers  allocated is now bufcachepercent of both the dma and
the high memory region.

This change allows the use of large buffer caches on amd64 using more than
4 GB of memory

ok tedu@@ krw@@ - testing by many.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.147 2013/06/11 16:42:16 deraadt Exp $	*/
d186 1
a186 1
		 return;
d226 2
a227 2
		bufpages = (b_dmapages_total + b_highpages_total)
		    * bufcachepercent / 100;
d329 1
a329 1
			(void) bfreeclean(bcstats.numbufpages - lopages,
d376 2
a377 2
	if (b_highpages_total
	    && (range->ucr_high <= dma_constraint.ucr_high)) {
d384 1
a384 1
		else {
a385 1
		}
d1028 3
a1030 3
				&bufqueues[BQ_CLEANH]) != 0)
				(void) bfreeclean(bcstats.numbufpages
				    - lopages, &bufqueues[BQ_CLEANL]);
d1353 1
d1377 2
a1378 2
				if (bfreeclean(delta, &bufqueues[BQ_CLEANH])
				    == 0)
d1380 1
a1380 1
				else {
@


1.147
log
@final removal of daddr64_t.  daddr_t has been 64 bit for a long enough
test period; i think 3 years ago the last bugs fell out.
ok otto beck others
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.146 2013/02/17 17:39:29 miod Exp $	*/
d5 1
d67 1
a67 1
#define	BQUEUES		2		/* number of free buffer queues */
d70 2
a71 1
#define	BQ_CLEAN	1		/* LRU queue with clean buffers */
d74 4
d119 5
a124 14
	/*
	 * We only calculate the head of the freelist when removing
	 * the last element of the list as that is the only time that
	 * it is needed (e.g. to reset the tail pointer).
	 *
	 * NB: This makes an assumption about how tailq's are implemented.
	 */
	if (TAILQ_NEXT(bp, b_freelist) == NULL) {
		for (dp = bufqueues; dp < &bufqueues[BQUEUES]; dp++)
			if (dp->tqh_last == &TAILQ_NEXT(bp, b_freelist))
				break;
		if (dp == &bufqueues[BQUEUES])
			panic("bremfree: lost tail");
	}
d126 4
d132 1
d139 23
d186 1
a186 1
		return;
a195 1
	u_int64_t dmapages;
d198 13
a210 3
	dmapages = uvm_pagecount(&dma_constraint);
	/* take away a guess at how much of this the kernel will consume */
	dmapages -= (atop(physmem) - atop(uvmexp.free));
d226 2
a227 1
		bufpages = dmapages * bufcachepercent / 100;
a229 1
	KASSERT(bufpages < dmapages);
d232 1
a232 1

d237 1
a237 1
	buflowpages = dmapages * 5 / 100;
a303 1
	struct buf *bp;
d326 5
a330 9
	while (!growing && (bp = TAILQ_FIRST(&bufqueues[BQ_CLEAN])) &&
	    (bcstats.numbufpages > lopages)) {
		bremfree(bp);
		if (bp->b_vp) {
			RB_REMOVE(buf_rb_bufs,
			    &bp->b_vp->v_bufs_tree, bp);
			brelvp(bp);
		}
		buf_put(bp);
d353 2
d356 1
a356 1
	 * It returns 0 to the pagedaemon to indicate that it has
d374 1
d376 15
a390 1
	bufadjust(bufpages - pdelta);
d575 6
a580 1
		SET(xbpp[i]->b_flags, B_READ | B_ASYNC);
d586 1
d815 5
a819 2
		 * If the buffer is invalid, place it in the clean queue, so it
		 * can be reused.
d836 5
a840 2
		 * If the buffer has no associated data, place it back in the
		 * pool.
d842 3
a844 18
		if (bp->b_data == NULL && bp->b_pobj == NULL) {
			/*
			 * Wake up any processes waiting for _this_ buffer to
			 * become free. They are not allowed to grab it
			 * since it will be freed. But the only sleeper is
			 * getblk and it's restarting the operation after
			 * sleep.
			 */
			if (ISSET(bp->b_flags, B_WANTED)) {
				CLR(bp->b_flags, B_WANTED);
				wakeup(bp);
			}
			if (bp->b_vp != NULL)
				RB_REMOVE(buf_rb_bufs,
				    &bp->b_vp->v_bufs_tree, bp);
			buf_put(bp);
			splx(s);
			return;
d846 3
a848 3

		bcstats.numcleanpages += atop(bp->b_bufsize);
		binsheadfree(bp, &bufqueues[BQ_CLEAN]);
d855 1
a855 4
		if (!ISSET(bp->b_flags, B_DELWRI)) {
			bcstats.numcleanpages += atop(bp->b_bufsize);
			bufq = &bufqueues[BQ_CLEAN];
		} else {
d859 6
d873 13
a886 5

	/* Unlock the buffer. */
	CLR(bp->b_flags, (B_AGE | B_ASYNC | B_NOCACHE | B_DEFERRED));
	buf_release(bp);

a899 6
	/* Wake up any processes waiting for _this_ buffer to become free. */
	if (ISSET(bp->b_flags, B_WANTED)) {
		CLR(bp->b_flags, B_WANTED);
		wakeup(bp);
	}

a940 10
	/*
	 * XXX
	 * The following is an inlined version of 'incore()', but with
	 * the 'invalid' test moved to after the 'busy' test.  It's
	 * necessary because there are some cases in which the NFS
	 * code sets B_INVAL prior to writing data to the server, but
	 * in which the buffers actually contain valid data.  In this
	 * case, we can't allow the system to allocate a new buffer for
	 * the block until the write is finished.
	 */
d1028 4
a1031 10
			while ((bcstats.numbufpages > lopages) &&
			    (bp = TAILQ_FIRST(&bufqueues[BQ_CLEAN]))) {
				bremfree(bp);
				if (bp->b_vp) {
					RB_REMOVE(buf_rb_bufs,
					    &bp->b_vp->v_bufs_tree, bp);
					brelvp(bp);
				}
				buf_put(bp);
			}
d1034 5
d1069 2
d1110 1
d1281 122
d1417 2
a1418 2
    	(*pr)("bufpages %lld, dirtypages %lld\n",
	    bcstats.numbufpages,  bcstats.numdirtypages);
@


1.146
log
@Comment out recently added __attribute__((__format__(__kprintf__))) annotations
in MI code; gcc 2.95 does not accept such annotation for function pointer
declarations, only function prototypes.
To be uncommented once gcc 2.95 bites the dust.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.145 2013/02/09 20:56:35 miod Exp $	*/
d89 2
a90 2
struct buf *bio_doread(struct vnode *, daddr64_t, int, int);
struct buf *buf_get(struct vnode *, daddr64_t, size_t);
d352 1
a352 1
bio_doread(struct vnode *vp, daddr64_t blkno, int size, int async)
d397 1
a397 1
bread(struct vnode *vp, daddr64_t blkno, int size, struct buf **bpp)
d413 1
a413 1
breadn(struct vnode *vp, daddr64_t blkno, int size, daddr64_t rablks[],
d474 1
a474 1
bread_cluster(struct vnode *vp, daddr64_t blkno, int size, struct buf **rbpp)
d478 1
a478 1
	daddr64_t sblkno;
d860 1
a860 1
incore(struct vnode *vp, daddr64_t blkno)
d887 1
a887 1
getblk(struct vnode *vp, daddr64_t blkno, int size, int slpflag, int slptimeo)
d953 1
a953 1
buf_get(struct vnode *vp, daddr64_t blkno, size_t size)
@


1.145
log
@Add explicit __attribute__ ((__format__(__kprintf__)))) to the functions and
function pointer arguments which are {used as,} wrappers around the kernel
printf function.
No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.144 2013/01/18 08:52:04 beck Exp $	*/
d1242 1
a1242 1
void	bcstats_print(int (*)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))));
d1248 1
a1248 1
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))))
@


1.144
log
@Give buf_acquire_unmapped and B_NOTMAPPED a viking funeral as they should
really have been called "maybemapped and hope it all works out". - use
buf_acquire_nomap instead which acounts for busymapped bufs correctly.

ok krw@@ guenther@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.143 2013/01/13 03:58:09 beck Exp $	*/
d1242 1
a1242 1
void	bcstats_print(int (*)(const char *, ...));
d1247 2
a1248 1
bcstats_print(int (*pr)(const char *, ...))
@


1.143
log
@After some discussion with deraadt and kettenis, revert previous decision to
set a high water mark, as this will likely cause us problems in low memory
situations where we can't get a struct buf.
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.142 2012/12/28 14:05:39 jsing Exp $	*/
d1038 1
a1038 1
	buf_acquire_unmapped(bp);
@


1.142
log
@Avoid spinning in the cleaner when there are insufficient clean pages, but
there are no buffers on the dirty queue to clean.

ok beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.140 2012/12/02 19:34:14 beck Exp $	*/
a236 1
	pool_sethiwat(&bufpool, buflowpages / 4);
@


1.141
log
@Fix kva reserve - ensure that kva reserve is checked for, as well
as fix the case where buffers can be returned on the vinvalbuf path
and we do not get woken up when waiting for kva.

An earlier version looked at and ok'd by guenther@@ in coimbra. - helpful
comments from kettenis@@
@
text
@d1086 2
a1087 2
	struct buf *bp;
	int s, pushed;
a1090 1
	pushed = 16;
d1093 2
a1094 1
		if (pushed >= 16 && (UNCLEAN_PAGES < hidirtypages &&
a1153 2
		if (bp == NULL)
			pushed = 16; /* No dirty bufs - sleep */
@


1.140
log
@Don't wake the cleaner and potentially throw away pages we shouldn't
be throwing away when growing the buffer cache - ok mlarkin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.139 2012/11/07 17:50:48 beck Exp $	*/
a102 13

/*
 * RESERVE_SLOTS of kva space, and the corresponding amount
 * of buffer pages are reserved for the cleaner and syncer's
 * exclusive use. Since we reserve kva slots to map the buffers
 * along with the buffer space, this ensures the cleaner and
 * syncer can always map and push out buffers if we get low
 * on buffer pages or kva space in which to map them.
 */
#define RESERVE_SLOTS 4
#define RESERVE_PAGES (RESERVE_SLOTS * MAXPHYS / PAGE_SIZE)
#define BCACHE_MIN (RESERVE_PAGES * 2)
#define UNCLEAN_PAGES (bcstats.numbufpages - bcstats.numcleanpages)
@


1.139
log
@
Fix the buffer cache.

A long time ago (in vienna) the reserves for the cleaner and syncer were
removed. softdep and many things have not performed ths same ever since.
Follow on generations of buffer cache hackers assumed the exising code
was the reference and have been in frustrating state of coprophagia ever
since.

This commit

0) Brings back a (small) reserve allotment of buffer pages, and the kva to
   map them, to allow the cleaner and syncer to run even when under intense
   memory or kva pressure.
1) Fixes a lot of comments and variables to represent reality.
2) Simplifies and corrects how the buffer cache backs off down to the lowest
   level.
3) Corrects how the page daemons asks the buffer cache to back off, ensuring
   that uvmpd_scan is done to recover inactive pages in low memory situaitons
4) Adds a high water mark to the pool used to allocate struct buf's
5) Correct the cleaner and the sleep/wakeup cases in both low memory and low
   kva situations. (including accounting for the cleaner/syncer reserve)

Tested by many, with very much helpful input from deraadt, miod, tobiasu,
kettenis and others.

ok kettenis@@ deraadt@@ jj@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.138 2012/10/16 02:30:54 beck Exp $	*/
d322 2
a323 2
	if (UNCLEAN_PAGES >= hidirtypages ||
	    bcstats.kvaslots_avail <= 2 * RESERVE_SLOTS)
@


1.138
log
@Cleanup.
- Whitespace KNF
- Removal/fixing of old useless comments
- Removal of unused counter
- Removal of pointless test that had no effect
ok krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.137 2012/10/09 15:12:15 beck Exp $	*/
d72 1
d96 4
a99 5
long locleanpages;      /* clean page count low water mark */
long hicleanpages;      /* clean page count high water mark */
long backoffpages;	/* backoff counter for page allocations */
long buflowpages;	/* bufpages low water mark */
long bufhighpages; 	/* bufpages high water mark */
d104 13
a171 5
	if (backoffpages) {
		backoffpages -= atop(bp->b_bufsize);
		if (backoffpages < 0)
			backoffpages = 0;
	}
d188 2
d192 2
a193 2
	 * If MD code doesn't say otherwise, use 10% of kvm for mappings and
	 * 10% of dmaable pages for cache pages.
d197 7
d206 3
d213 2
a214 3
	 * set the base backoff level for the buffer cache to bufpages.
	 * we will not allow uvm to steal back more than this number of
	 * pages
d216 3
a218 1
	buflowpages = dmapages * 10 / 100;
d229 4
d250 2
d261 8
a268 2
	hidirtypages = (bufpages / 4) * 3;
	lodirtypages = bufpages / 2;
d271 2
a272 2
	 * When we hit 95% of pages being clean, we bring them down to
	 * 90% to have some slack.
d274 2
a275 2
	hicleanpages = bufpages - (bufpages / 20);
	locleanpages = bufpages - (bufpages / 10);
a283 4
	/*
	 * XXX - note, bufkvm was allocated once, based on 10% of physmem
	 * see above.
	 */
d285 4
a288 1
	int s;
d291 2
a294 3
	hidirtypages = (bufpages / 4) * 3;
	lodirtypages = bufpages / 2;

d296 2
a297 2
	 * When we hit 95% of pages being clean, we bring them down to
	 * 90% to have some slack.
d299 2
a300 2
	hicleanpages = bufpages - (bufpages / 20);
	locleanpages = bufpages - (bufpages / 10);
d303 3
a305 3
	 * If we we have more buffers allocated than bufpages,
	 * free them up to get back down. this may possibly consume
	 * all our clean pages...
d307 2
a308 2
	while ((bp = TAILQ_FIRST(&bufqueues[BQ_CLEAN])) &&
	    (bcstats.numbufpages > bufpages)) {
d319 2
a320 4
	 * Wake up cleaner if we're getting low on pages. We might
	 * now have too much dirty, or have fallen below our low
	 * water mark on clean pages so we need to free more stuff
	 * up.
d322 2
a323 1
	if (bcstats.numdirtypages >= hidirtypages)
a325 9
	/*
	 * if immediate action has not freed up enough goo for us
	 * to proceed - we tsleep and wait for the cleaner above
	 * to do it's work and get us reduced down to sanity.
	 */
	while (bcstats.numbufpages > bufpages) {
		needbuffer++;
		tsleep(&needbuffer, PRIBIO, "needbuffer", 0);
	}
d336 2
a337 2
	 * Back off the amount of buffer cache pages. Called by the page
	 * daemon to consume buffer cache pages rather than swapping.
d339 4
a342 4
	 * On success, it frees N pages from the buffer cache, and sets
	 * a flag so that the next N allocations from buf_get will recycle
	 * a buffer rather than allocate a new one. It then returns 0 to the
	 * caller.
a343 2
	 * on failure, it could free no pages from the buffer cache, does
	 * nothing and returns -1 to the caller.
d345 7
a351 1
	long d;
d355 6
a360 3

	if (bufpages - bufbackpages >= buflowpages)
		d = bufbackpages;
d362 1
a362 5
		d = bufpages - buflowpages;
	backoffpages = bufbackpages;
	bufadjust(bufpages - d);
	backoffpages = 0;
	return(0);
d847 6
d854 3
a856 2
	if (needbuffer) {
		needbuffer--;
a968 1
	static int gcount = 0;
a973 12
	/*
	 * if we were previously backed off, slowly climb back up
	 * to the high water mark again.
	 */
	if (backoffpages == 0 && bufpages < bufhighpages) {
		if (gcount == 0)  {
			bufadjust(bufpages + bufbackpages);
			gcount += bufbackpages;
		} else
			gcount--;
	}

d977 2
a978 1
		 * Wake up cleaner if we're getting low on pages.
d980 2
a981 1
		if (bcstats.numdirtypages >= hidirtypages)
d984 2
d987 14
a1000 1
		 * If we're above the high water mark for clean pages,
d1003 3
a1005 3
		if (bcstats.numcleanpages > hicleanpages) {
			while (bcstats.numcleanpages > locleanpages) {
				bp = TAILQ_FIRST(&bufqueues[BQ_CLEAN]);
a1015 2
		npages = atop(round_page(size));

d1017 3
a1019 1
		 * Free some buffers until we have enough space.
d1021 15
a1035 21
		while ((bcstats.numbufpages + npages > bufpages)
		    || backoffpages) {
			int freemax = 5;
			int i = freemax;
			while ((bp = TAILQ_FIRST(&bufqueues[BQ_CLEAN])) && i--) {
				bremfree(bp);
				if (bp->b_vp) {
					RB_REMOVE(buf_rb_bufs,
					    &bp->b_vp->v_bufs_tree, bp);
					brelvp(bp);
				}
				buf_put(bp);
			}
			if (freemax == i &&
			    (bcstats.numbufpages + npages > bufpages ||
			     backoffpages)) {
				needbuffer++;
				tsleep(&needbuffer, PRIBIO, "needbuffer", 0);
				splx(s);
				return (NULL);
			}
d1100 1
a1100 1
	int s;
d1104 1
d1107 11
a1117 1
		if (bcstats.numdirtypages < hidirtypages)
d1119 1
d1126 3
a1128 1
			if (bcstats.numdirtypages < lodirtypages)
d1157 1
d1167 2
@


1.137
log
@bufq write limiting

This change ensures that writes in flight from the buffer cache via bufq
are limited to a high water mark - when the limit is reached the writes sleep
until the amount of IO in flight reaches a low water mark. This avoids the
problem where userland can queue an unlimited amount of asynchronous writes
resulting in the consumption of all/most of our available buffer mapping kva,
and a long queue of writes to the disk.

ok kettenis@@, krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.136 2012/05/30 19:32:19 miod Exp $	*/
d92 5
a96 19
/*
 * We keep a few counters to monitor the utilization of the buffer cache
 *
 *  numbufpages   - number of pages totally allocated.
 *  numdirtypages - number of pages on BQ_DIRTY queue.
 *  lodirtypages  - low water mark for buffer cleaning daemon.
 *  hidirtypages  - high water mark for buffer cleaning daemon.
 *  numcleanpages - number of pages on BQ_CLEAN queue.
 *		    Used to track the need to speedup the cleaner and 
 *		    as a reserve for special processes like syncer.
 *  maxcleanpages - the highest page count on BQ_CLEAN.
 */

struct bcachestats bcstats;
long lodirtypages;
long hidirtypages;
long locleanpages;
long hicleanpages;
long maxcleanpages;
a242 2

	maxcleanpages = locleanpages;
a270 2
	maxcleanpages = locleanpages;

d293 1
a293 2
	if (bcstats.numdirtypages >= hidirtypages ||
	    bcstats.numcleanpages <= locleanpages)
d321 1
a321 1
	 * caller. 
d324 1
a324 1
	 * nothing and returns -1 to the caller. 
d328 1
a328 1
	if (bufpages <= buflowpages) 
d603 1
a603 1
	
a794 2
		if (maxcleanpages < bcstats.numcleanpages)
			maxcleanpages = bcstats.numcleanpages;
a803 2
			if (maxcleanpages < bcstats.numcleanpages)
				maxcleanpages = bcstats.numcleanpages;
d961 1
a961 2
		if (bcstats.numdirtypages >= hidirtypages ||
		    bcstats.numcleanpages <= locleanpages)
@


1.136
log
@Fix a few issues in the pressure logic when the available buffers run low:
- make sure the buffer reclaiming loop in buf_get() actually does something
  but spin, if `backoffpages' is nonzero and all free queues have been drained.
- don't forget to set a poor man's condition variable to nonzero before
  tsleeping on it in bufadjust(), otherwise you'll never get woken up.
- don't be too greedy and reassing backoffpages a large amount immediately
  after bufadjust() has been called.

This fixes reproduceable hangs seen during heavy I/O (such as `make install'
of many large files, e.g. run in /usr/src/lib with NOMAN=) on systems with
a challenged number of pages (less than a few thousands, total).

Part of this is temporary bandaid until a better pressure logic is devised,
but it's solving an immediate problem. Been in snapshots for a solid month.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.135 2012/03/23 15:51:26 guenther Exp $	*/
d629 8
@


1.135
log
@Make rusage totals, itimers, and profile settings per-process instead
of per-rthread.  Handling of per-thread tick and runtime counters
inspired by how FreeBSD does it.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.134 2011/09/19 14:48:04 beck Exp $	*/
d321 1
d356 1
a356 1
	backoffpages = bufbackpages;
d963 2
a964 2
	if ((backoffpages == 0) && (bufpages < bufhighpages)) {
		if ( gcount == 0 )  {
d1016 2
a1017 1
			    (bcstats.numbufpages + npages > bufpages)) {
@


1.134
log
@clean up buffer cache statistics somewhat to
remove some now useless statistics, and add some
relevant ones regarding kva usage in the cache.

make systat io and show bcstats in ddb both show
these counters.

ok deraadt@@ krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.133 2011/07/06 20:50:05 beck Exp $	*/
d378 1
a378 1
		curproc->p_stats->p_ru.ru_inblock++;		/* XXX */
d557 1
a557 1
	curproc->p_stats->p_ru.ru_inblock++;
d620 1
a620 1
		curproc->p_stats->p_ru.ru_oublock++;
d674 1
a674 1
		curproc->p_stats->p_ru.ru_oublock++;	/* XXX */
@


1.133
log
@the rest of the uvm commit - I commited from uvm instead of sys
(part missed from previous commit)
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.132 2011/07/04 20:35:35 deraadt Exp $	*/
d146 1
a148 1
	bcstats.freebufs--;
d821 1
a833 1
	bcstats.freebufs++;
d1121 1
a1122 1
				bcstats.freebufs++;
d1234 6
a1239 4
	(*pr)("numbufs %lld, freebufs %lld\n",
	    bcstats.numbufs, bcstats.freebufs);
    	(*pr)("bufpages %lld, freepages %lld, dirtypages %lld\n",
	    bcstats.numbufpages, bcstats.numfreepages, bcstats.numdirtypages);
@


1.132
log
@move the specfs code to a place people can see it; ok guenther thib krw
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.131 2011/07/04 04:30:41 tedu Exp $	*/
d330 1
a330 1
bufbackoff()
@


1.131
log
@bread does nothing with its ucred argument.  remove it.  ok matthew
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.130 2011/06/05 19:41:04 deraadt Exp $	*/
d59 1
a61 2

#include <miscfs/specfs/specdev.h>
@


1.130
log
@Move the bufcachepercent setting code to MI locations -- set it to 42%
for now; that is unlikely to hit some of the remaining starvation bugs.
Repair the bufpages calculation too; i386 was doing it ahead of time
(incorrectly) and then re-calculating it.
ok thib
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.129 2011/04/07 19:07:42 beck Exp $	*/
d406 1
a406 2
bread(struct vnode *vp, daddr64_t blkno, int size, struct ucred *cred,
    struct buf **bpp)
d423 1
a423 1
    int rasizes[], int nrablks, struct ucred *cred, struct buf **bpp)
@


1.129
log
@Revert previous diff decrementing bcstats.numbufpages here. This function
does not do what it purports to do, it shrinks mapping, not allocation, as
the pages have already been given away to other buffers. This also renames
the function to make this a little more obvious

and art should not name funcitons

ok thib@@, art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.128 2011/04/02 16:47:17 beck Exp $	*/
a116 3
/* XXX - should be defined here. */
extern int bufcachepercent;

a192 3

	/* XXX - for now */
	bufhighpages = buflowpages = bufpages = bufcachepercent = bufkvm = 0;
@


1.128
log
@Constrain the buffer cache to use only the dma reachable region of memory.
With this change bufcachepercent will be the percentage of dma reachable
memory that the buffer cache will attempt to use.
ok deraadt@@ thib@@ oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.127 2010/11/13 17:45:44 deraadt Exp $	*/
d466 2
a467 1
		 * Shrink this buffer to only cover its part of the total I/O.
d469 1
a469 1
		buf_shrink_mem(bp, newsize);
@


1.127
log
@backout 1.86
it is totally wrong to convert bdwrite into bawrite on the fly.  this just
causes way bigger issues.
ok beck blambert
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.126 2010/08/03 06:30:19 deraadt Exp $	*/
d194 1
d200 2
d204 1
a204 1
	 * 10% physmem for pages.
d209 1
a209 1
		bufpages = physmem * bufcachepercent / 100;
d218 1
a218 1
	buflowpages = physmem * 10 / 100;
@


1.126
log
@matthew did not commit the diff he passed around for us to inspect...
repair that situation.  Darn newbies...
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.124 2010/07/01 16:23:09 thib Exp $	*/
a674 1
		bp->b_synctime = time_uptime + 35;
a678 9
	} else {
		/*
		 * see if this buffer has slacked through the syncer
		 * and enforce an async write upon it.
		 */
		if (bp->b_synctime < time_uptime) {
			bawrite(bp);
			return;
		}
a719 1
		bp->b_synctime = time_uptime + 35;
@


1.125
log
@If an asynchronous request invalidates a buf, then we might remove it
from its vnode's buffer cache in an interrupt context.  Therefore we
need interrupt protection when searching the buffer red-black tree.

ok deraadt@@, thib@@, art@@
@
text
@d887 1
a887 1
	return(NULL);
@


1.124
log
@Call bufq_done at the top of biodone, so we don't call it on
a freed buf as that causes problems...
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.123 2010/06/30 22:41:43 thib Exp $	*/
d876 3
d883 4
a886 2
	if (bp && !ISSET(bp->b_flags, B_INVAL))
		return(bp);
d916 1
a919 2

		s = splbio();
a937 1
		splx(s);
d939 1
@


1.123
log
@Disable/partially backout the bufq quiesce changes as this
is causing havoc with vnds and release must be buildable.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.122 2010/06/29 18:52:20 kettenis Exp $	*/
d1205 3
@


1.122
log
@Introduce bufq_quiesce(), which will block I/O ifrom getting on the queues,
and waits until all I/O currently on the queues has been completed.  To get
I/O going again, call bufq_restart().

To be used for suspend/resume.

Joint effort with thib@@, tedu@@; tested by mlarkin@@, marco@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.121 2010/02/05 12:24:32 jsing Exp $	*/
a1230 3

	if (bp->b_bq)
		bufq_done(bp->b_bq, bp);
@


1.121
log
@Use correct format specifiers for 'show bcstats'.

ok beck@@ krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.118 2009/06/25 15:49:26 thib Exp $	*/
d1231 3
@


1.120
log
@two things:
1) fix buffer cache low water mark to allow for extremely low memory machines
without dying
2) Add "show bcstats" to ddb to allow for looking at the buffer cache statistics in ddb

ok art@@ oga@@
@
text
@d1242 5
a1246 4
	(*pr)("numbufs %d freebufs %d\n", bcstats.numbufs, bcstats.freebufs);
    	(*pr)("bufpages %d freepages %d dirtypages %d\n", bcstats.numbufpages,
	    bcstats.numfreepages, bcstats.numdirtypages);
	(*pr)("pendingreads %d, pendingwrites %d\n",
@


1.119
log
@
Dynamic buffer cache support - a re-commit of what was backed out
after c2k9

allows buffer cache to be extended and grow/shrink dynamically

tested by many, ok oga@@, "why not just commit it" deraadt@@
@
text
@d115 1
d217 8
d353 2
a354 2
	if (bufpages - BACKPAGES >= buflowpages)
		d = BACKPAGES;
d357 1
a357 1
	backoffpages = BACKPAGES;
d359 1
a359 1
	backoffpages = BACKPAGES;
d975 2
a976 2
			bufadjust(bufpages + BACKPAGES);
			gcount += BACKPAGES;
d1232 17
@


1.118
log
@backout the buf_acquire() does the bremfree() since all callers
where doing bremfree() befure calling buf_acquire().

This is causing us headache pinning down a bug that showed up
when deraadt@@ too cvs to current, and will have to be done
anyway as a preperation for backouts.

OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.116 2009/06/06 18:06:22 art Exp $	*/
d4 1
a4 1
/*-
a64 14
 * Definitions for the buffer hash lists.
 */
#define	BUFHASH(dvp, lbn)	\
	(&bufhashtbl[((long)(dvp) / sizeof(*(dvp)) + (int)(lbn)) & bufhash])
LIST_HEAD(bufhashhdr, buf) *bufhashtbl, invalhash;
u_long	bufhash;

/*
 * Insq/Remq for the buffer hash lists.
 */
#define	binshash(bp, dp)	LIST_INSERT_HEAD(dp, bp, b_hash)
#define	bremhash(bp)		LIST_REMOVE(bp, b_hash)

/*
d112 3
a173 1
	bremhash(bp);
d176 5
d196 1
a196 1
	bufpages = bufcachepercent = bufkvm = 0;
d207 10
a242 1
	bufhashtbl = hashinit(bufpages / 4, M_CACHE, M_WAITOK, &bufhash);
d256 98
d779 3
a781 1
		if (bp->b_vp)
d783 2
a784 2
		bremhash(bp);
		binshash(bp, &invalhash);
d802 3
d866 1
d868 6
a873 8
	/* Search hash chain */
	LIST_FOREACH(bp, BUFHASH(vp, blkno), b_hash) {
		if (bp->b_lblkno == blkno && bp->b_vp == vp &&
		    !ISSET(bp->b_flags, B_INVAL))
			return (bp);
	}

	return (NULL);
d888 1
d902 3
a904 3
	LIST_FOREACH(bp, BUFHASH(vp, blkno), b_hash) {
		if (bp->b_lblkno != blkno || bp->b_vp != vp)
			continue;
d954 1
d960 12
d989 3
a991 1
				if (bp->b_vp)
d993 1
d1003 2
a1004 1
		while (bcstats.numbufpages + npages > bufpages) {
d1009 3
a1011 1
				if (bp->b_vp)
d1013 1
d1016 2
a1017 1
			if (freemax == i) {
d1058 2
a1059 1
		binshash(bp, BUFHASH(vp, blkno));
d1063 1
a1063 1
		binshash(bp, &invalhash);
@


1.117
log
@Back out all the buffer cache changes I committed during c2k9. This reverts three
commits:

1) The sysctl allowing bufcachepercent to be changed at boot time.
2) The change moving the buffer cache hash chains to a red-black tree
3) The dynamic buffer cache (Which depended on the earlier too).

ok on the backout from marco and todd
@
text
@d815 1
d977 1
@


1.116
log
@All caller of buf_acquire were doing bremfree before the call.
Just put it in the buf_acquire function.
oga@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.115 2009/06/05 04:29:14 beck Exp $	*/
d4 1
a4 1
/*
d65 14
a125 3
long backoffpages;	/* backoff counter for page allocations */
long buflowpages;	/* bufpages low water mark */
long bufhighpages; 	/* bufpages high water mark */
d185 1
a187 5
	if (backoffpages) {
		backoffpages -= atop(bp->b_bufsize);
		if (backoffpages < 0)
			backoffpages = 0;
	}
d203 1
a203 1
	bufhighpages = buflowpages = bufpages = bufcachepercent = bufkvm = 0;
a213 10
	bufhighpages = bufpages;

	/*
	 * set the base backoff level for the buffer cache to bufpages.
	 * we will not allow uvm to steal back more than this number of
	 * pages
	 */
	buflowpages = physmem * 10 / 100;


d240 1
a253 98
/*
 * Change cachepct
 */
void
bufadjust(int newbufpages)
{
	/*
	 * XXX - note, bufkvm was allocated once, based on 10% of physmem
	 * see above.
	 */
	struct buf *bp;
	int s;

	s = splbio();
	bufpages = newbufpages;

	hidirtypages = (bufpages / 4) * 3;
	lodirtypages = bufpages / 2;

	/*
	 * When we hit 95% of pages being clean, we bring them down to
	 * 90% to have some slack.
	 */
	hicleanpages = bufpages - (bufpages / 20);
	locleanpages = bufpages - (bufpages / 10);

	maxcleanpages = locleanpages;

	/*
	 * If we we have more buffers allocated than bufpages,
	 * free them up to get back down. this may possibly consume
	 * all our clean pages...
	 */
	while ((bp = TAILQ_FIRST(&bufqueues[BQ_CLEAN])) &&
	    (bcstats.numbufpages > bufpages)) {
		bremfree(bp);
		if (bp->b_vp) {
			RB_REMOVE(buf_rb_bufs,
			    &bp->b_vp->v_bufs_tree, bp);
			brelvp(bp);
		}
		buf_put(bp);
	}

	/*
	 * Wake up cleaner if we're getting low on pages. We might
	 * now have too much dirty, or have fallen below our low
	 * water mark on clean pages so we need to free more stuff
	 * up.
	 */
	if (bcstats.numdirtypages >= hidirtypages ||
	    bcstats.numcleanpages <= locleanpages)
		wakeup(&bd_req);

	/*
	 * if immediate action has not freed up enough goo for us
	 * to proceed - we tsleep and wait for the cleaner above
	 * to do it's work and get us reduced down to sanity.
	 */
	while (bcstats.numbufpages > bufpages) {
		tsleep(&needbuffer, PRIBIO, "needbuffer", 0);
	}
	splx(s);
}

/*
 * Make the buffer cache back off from cachepct.
 */
int
bufbackoff()
{
	/*
	 * Back off the amount of buffer cache pages. Called by the page
	 * daemon to consume buffer cache pages rather than swapping.
	 *
	 * On success, it frees N pages from the buffer cache, and sets
	 * a flag so that the next N allocations from buf_get will recycle
	 * a buffer rather than allocate a new one. It then returns 0 to the
	 * caller. 
	 *
	 * on failure, it could free no pages from the buffer cache, does
	 * nothing and returns -1 to the caller. 
	 */
	long d;

	if (bufpages <= buflowpages) 
		return(-1);

	if (bufpages - BACKPAGES >= buflowpages)
		d = BACKPAGES;
	else
		d = bufpages - buflowpages;
	backoffpages = BACKPAGES;
	bufadjust(bufpages - d);
	backoffpages = BACKPAGES;
	return(0);
}

d679 1
a679 3
		if (bp->b_vp) {
			RB_REMOVE(buf_rb_bufs, &bp->b_vp->v_bufs_tree,
			    bp);
d681 2
a682 2
		}
		bp->b_vp = NULL;
a699 3
			if (bp->b_vp != NULL)
				RB_REMOVE(buf_rb_bufs,
				    &bp->b_vp->v_bufs_tree, bp);
a760 1
	struct buf b;
d762 8
a769 6
	/* Search buf lookup tree */
	b.b_lblkno = blkno;
	bp = RB_FIND(buf_rb_bufs, &vp->v_bufs_tree, &b);
	if (bp && !ISSET(bp->b_flags, B_INVAL))
		return(bp);
	return(NULL);
a783 1
	struct buf b;
d797 3
a799 3
	b.b_lblkno = blkno;
	bp = RB_FIND(buf_rb_bufs, &vp->v_bufs_tree, &b);
	if (bp != NULL) {
a847 1
	static int gcount = 0;
a852 12
	/*
	 * if we were previously backed off, slowly climb back up
	 * to the high water mark again.
	 */
	if ((backoffpages == 0) && (bufpages < bufhighpages)) {
		if ( gcount == 0 )  {
			bufadjust(bufpages + BACKPAGES);
			gcount += BACKPAGES;
		} else
			gcount--;
	}

d870 1
a870 3
				if (bp->b_vp) {
					RB_REMOVE(buf_rb_bufs,
					    &bp->b_vp->v_bufs_tree, bp);
a871 1
				}
d881 1
a881 2
		while ((bcstats.numbufpages + npages > bufpages)
		    || backoffpages) {
d886 1
a886 3
				if (bp->b_vp) {
					RB_REMOVE(buf_rb_bufs,
					    &bp->b_vp->v_bufs_tree, bp);
a887 1
				}
d890 1
a890 2
			if (freemax == i &&
			    (bcstats.numbufpages + npages > bufpages)) {
d931 1
a931 2
		if (RB_INSERT(buf_rb_bufs, &vp->v_bufs_tree, bp))
			panic("buf_get: dup lblk vp %p bp %p", vp, bp);
d935 1
a935 1
		bp->b_vp = NULL;
@


1.115
log
@Dynamic buffer cache sizing.

This commit won't change the default behaviour of the system unless the
buffer cache size is increased with sysctl kern.bufcachepercent. By default
our buffer cache is 10% of memory, which with this commit is now treated
as a low water mark.  If the buffer cache size is increased, the new size
is treated as a high water mark and the buffer cache is permitted to grow
to that percentage of memory.

If the page daemon is invoked, the page daemon will ask the buffer cache
to relenquish pages. if the buffer cache has more than the low water mark it
will relenquish pages allowing them to be consumed by uvm. after a short
period the buffer cache will attempt to re-grow back to the high water mark.

This permits the use of a large buffer cache without penalizing the available
memory for other purposes.

Above the low water mark the buffer cache remains entirely subservient to
the page daemon, so if uvm requires pages, the buffer cache will abandon
them.

ok art@@ thib@@ oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.114 2009/06/03 21:30:20 beck Exp $	*/
a918 1
			bremfree(bp);
a1102 1
			bremfree(bp);
@


1.114
log
@add kern.bufcachepercent sysctl to allow adjusting the buffer cache
size on a running system.
ok art@@, oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.113 2009/06/03 04:30:57 beck Exp $	*/
d4 1
a4 1
/*-
d112 3
d176 5
d196 1
a196 1
	bufpages = bufcachepercent = bufkvm = 0;
d207 10
d266 2
d269 1
d283 36
d321 32
d954 1
d960 12
d1003 2
a1004 1
		while (bcstats.numbufpages + npages > bufpages) {
d1016 2
a1017 1
			if (freemax == i) {
@


1.113
log
@Change bufhash from the old grotty hash table to red-black trees hanging
off the vnode.
ok art@@, oga@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.112 2009/04/22 13:12:26 art Exp $	*/
d237 27
@


1.112
log
@Make the interactions in allocating buffers less confusing.

- getnewbuf dies. instead of having getnewbuf, buf_get, buf_stub and
 buf_init we now have buf_get that is smaller than some of those
 functions were before.

- Instead of allocating anonymous buffers and then freeing them if we
 happened to lose the race to the hash, always allocate a buffer knowing
 which <vnode, block> it will belong to.

- In cluster read, instead of allocating an anonymous buffer to cover
 the whole read and then stubs for every buffer under it, make the
 first buffer in the cluster cover the whole range and then shrink it
 in the callback.

now, all buffers are always on the correct hash and we always know their
identity.

discussed with many, kettenis@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.111 2009/03/23 15:10:44 beck Exp $	*/
a64 14
 * Definitions for the buffer hash lists.
 */
#define	BUFHASH(dvp, lbn)	\
	(&bufhashtbl[((long)(dvp) / sizeof(*(dvp)) + (int)(lbn)) & bufhash])
LIST_HEAD(bufhashhdr, buf) *bufhashtbl, invalhash;
u_long	bufhash;

/*
 * Insq/Remq for the buffer hash lists.
 */
#define	binshash(bp, dp)	LIST_INSERT_HEAD(dp, bp, b_hash)
#define	bremhash(bp)		LIST_REMOVE(bp, b_hash)

/*
a170 1
	bremhash(bp);
a224 1
	bufhashtbl = hashinit(bufpages / 4, M_CACHE, M_WAITOK, &bufhash);
d663 3
a665 1
		if (bp->b_vp)
d667 2
a668 2
		bremhash(bp);
		binshash(bp, &invalhash);
d686 3
d750 1
d752 6
a757 8
	/* Search hash chain */
	LIST_FOREACH(bp, BUFHASH(vp, blkno), b_hash) {
		if (bp->b_lblkno == blkno && bp->b_vp == vp &&
		    !ISSET(bp->b_flags, B_INVAL))
			return (bp);
	}

	return (NULL);
d772 1
d786 3
a788 3
	LIST_FOREACH(bp, BUFHASH(vp, blkno), b_hash) {
		if (bp->b_lblkno != blkno || bp->b_vp != vp)
			continue;
d860 3
a862 1
				if (bp->b_vp)
d864 1
d879 3
a881 1
				if (bp->b_vp)
d883 1
d927 2
a928 1
		binshash(bp, BUFHASH(vp, blkno));
d932 1
a932 1
		binshash(bp, &invalhash);
@


1.111
log
@fix buffer cache pending writs statistic so it does not go negative.
this ensures we ignore counting any buffers returning through biodone()
for which B_PHYS has been set - which should be set on all transfers
that manually do raw io bypassing the buffer cache by setting up their
own buffer and calling strategy..

ok thib@@, todd@@, and now that he is a buffer cache and nfs hacker oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.110 2009/01/11 17:40:00 oga Exp $	*/
a94 2
struct buf *buf_get(size_t);
struct buf *buf_stub(struct vnode *, daddr64_t);
d104 1
a104 2
struct buf *getnewbuf(size_t, int, int, int *);
void buf_init(struct buf *);
a165 92
buf_init(struct buf *bp)
{
	splassert(IPL_BIO);

	bzero((char *)bp, sizeof *bp);
	bp->b_vnbufs.le_next = NOLIST;
	bp->b_freelist.tqe_next = NOLIST;
	bp->b_synctime = time_uptime + 300;
	bp->b_dev = NODEV;
	LIST_INIT(&bp->b_dep);
}

/*
 * This is a non-sleeping expanded equivalent of getblk() that allocates only
 * the buffer structure, and not its contents.
 */
struct buf *
buf_stub(struct vnode *vp, daddr64_t lblkno)
{
	struct buf *bp;
	int s;

	s = splbio();
	bp = pool_get(&bufpool, PR_NOWAIT);
	splx(s);

	if (bp == NULL)
		return (NULL);

	bzero((char *)bp, sizeof *bp);
	bp->b_vnbufs.le_next = NOLIST;
	bp->b_freelist.tqe_next = NOLIST;
	bp->b_synctime = time_uptime + 300;
	bp->b_dev = NODEV;
	bp->b_bufsize = 0;
	bp->b_data = NULL;
	bp->b_flags = 0;
	bp->b_dev = NODEV;
	bp->b_blkno = bp->b_lblkno = lblkno;
	bp->b_iodone = NULL;
	bp->b_error = 0;
	bp->b_resid = 0;
	bp->b_bcount = 0;
	bp->b_dirtyoff = bp->b_dirtyend = 0;
	bp->b_validoff = bp->b_validend = 0;

	LIST_INIT(&bp->b_dep);

	buf_acquire_unmapped(bp);

	s = splbio();
	LIST_INSERT_HEAD(&bufhead, bp, b_list);
	bcstats.numbufs++;
	bgetvp(vp, bp);
	splx(s);

	return (bp);
}

struct buf *
buf_get(size_t size)
{
	struct buf *bp;
	int npages;

	splassert(IPL_BIO);

	KASSERT(size > 0);

	size = round_page(size);
	npages = atop(size);

	if (bcstats.numbufpages + npages > bufpages)
		return (NULL);

	bp = pool_get(&bufpool, PR_WAITOK);

	buf_init(bp);
	bp->b_flags = B_INVAL;
	buf_alloc_pages(bp, size);
	bp->b_data = NULL;
	binsheadfree(bp, &bufqueues[BQ_CLEAN]);
	binshash(bp, &invalhash);
	LIST_INSERT_HEAD(&bufhead, bp, b_list);
	bcstats.numbufs++;
	bcstats.freebufs++;
	bcstats.numcleanpages += atop(bp->b_bufsize);

	return (bp);
}

void
d185 1
d347 1
a348 1
	struct buf **xbpp;
d350 2
a351 1
	xbpp = (struct buf **)bp->b_saveaddr;
d353 8
a360 1
	for (i = 0; xbpp[i] != 0; i++) {
d367 7
a373 2
	bp->b_pobj = NULL;
	buf_put(bp);
d386 1
a386 1
		return (biowait(*rbpp));
d389 1
a389 1
		return (biowait(*rbpp));
d393 1
a393 1
		return (biowait(*rbpp));
d401 1
a401 1
		return (biowait(*rbpp));
d403 10
a412 10
	for (i = 0; i < howmany; i++) {
		if (incore(vp, blkno + i + 1)) {
			for (--i; i >= 0; i--) {
				SET(xbpp[i]->b_flags, B_INVAL);
				brelse(xbpp[i]);
			}
			free(xbpp, M_TEMP);
			return (biowait(*rbpp));
		}
		xbpp[i] = buf_stub(vp, blkno + i + 1);
d414 1
a414 1
			for (--i; i >= 0; i--) {
d419 1
a419 1
			return (biowait(*rbpp));
d423 2
a426 10
	bp = getnewbuf(howmany * size, 0, 0, NULL);
	if (bp == NULL) {
		for (i = 0; i < howmany; i++) {
			SET(xbpp[i]->b_flags, B_INVAL);
			brelse(xbpp[i]);
		}
		free(xbpp, M_TEMP);
		return (biowait(*rbpp));
	}

d429 1
a429 1
	for (i = 0; i < howmany; i++) {
a432 1
		binshash(xbpp[i], BUFHASH(vp, xbpp[i]->b_lblkno));
a437 1
		buf_acquire_unmapped(xbpp[i]);
d440 3
a443 1
	bp->b_lblkno = blkno + 1;
d445 1
d448 1
a448 1
	bp->b_vp = vp;
d454 1
a653 1
	/* Block disk interrupts. */
d681 2
d741 1
a741 1
		wakeup_one(&needbuffer);
d783 1
a783 2
	struct bufhashhdr *bh;
	struct buf *bp, *nb = NULL;
a795 1
	bh = BUFHASH(vp, blkno);
a802 6
			if (nb != NULL) {
				SET(nb->b_flags, B_INVAL);
				binshash(nb, &invalhash);
				brelse(nb);
				nb = NULL;
			}
d818 1
a818 1
			break;
d822 2
a823 12
	if (nb && bp) {
		SET(nb->b_flags, B_INVAL);
		binshash(nb, &invalhash);
		brelse(nb);
		nb = NULL;
	}
	if (bp == NULL && nb == NULL) {
		nb = getnewbuf(size, slpflag, slptimeo, &error);
		if (nb == NULL) {
			if (error == ERESTART || error == EINTR)
				return (NULL);
		}
d825 1
a825 13
	}
	if (nb) {
		bp = nb;
		binshash(bp, bh);
		bp->b_blkno = bp->b_lblkno = blkno;
		s = splbio();
		bgetvp(vp, bp);
		splx(s);
	}
#ifdef DIAGNOSTIC
	if (!ISSET(bp->b_flags, B_BUSY))
		panic("getblk buffer not B_BUSY");
#endif
d837 1
a837 1
	while ((bp = getnewbuf(size, 0, 0, NULL)) == NULL)
a838 2
	SET(bp->b_flags, B_INVAL);
	binshash(bp, &invalhash);
d844 1
a844 1
 * Find a buffer which is available for use.
d847 1
a847 1
getnewbuf(size_t size, int slpflag, int slptimeo, int *ep)
d850 2
d854 22
a875 3
#if 0		/* we would really like this but sblock update kills it */
	KASSERT(curproc != syncerproc && curproc != cleanerproc);
#endif
d877 1
a877 6
	s = splbio();
	/*
	 * Wake up cleaner if we're getting low on pages.
	 */
	if (bcstats.numdirtypages >= hidirtypages || bcstats.numcleanpages <= locleanpages)
		wakeup(&bd_req);
d879 18
a896 12
	/*
	 * If we're above the high water mark for clean pages,
	 * free down to the low water mark.
	 */
	if (bcstats.numcleanpages > hicleanpages) {
		while (bcstats.numcleanpages > locleanpages) {
			bp = TAILQ_FIRST(&bufqueues[BQ_CLEAN]);
			bremfree(bp);
			if (bp->b_vp)
				brelvp(bp);
			bremhash(bp);
			buf_put(bp);
d900 2
a901 3
	/* we just ask. it can say no.. */
getsome:
	bp = buf_get(size);
a902 11
		int freemax = 5;
		int i = freemax;
		while ((bp = TAILQ_FIRST(&bufqueues[BQ_CLEAN])) && i--) {
			bremfree(bp);
			if (bp->b_vp)
				brelvp(bp);
			bremhash(bp);
			buf_put(bp);
		}
		if (freemax != i)
			goto getsome;
d907 39
a945 4
	bremfree(bp);
	/* Buffer is no longer on free lists. */
	bp->b_flags = 0;
	buf_acquire(bp);
a948 11
	/* clear out various other fields */
	bp->b_dev = NODEV;
	bp->b_blkno = bp->b_lblkno = 0;
	bp->b_iodone = NULL;
	bp->b_error = 0;
	bp->b_resid = 0;
	bp->b_bcount = size;
	bp->b_dirtyoff = bp->b_dirtyend = 0;
	bp->b_validoff = bp->b_validend = 0;

	bremhash(bp);
@


1.110
log
@backout revision 1.109
"keep b_proc set to the process, thats doing the io as advertised"

This broke dvd playing on my laptop (page fault trap in vmapbuf in the
physio path).

thib's cookie privileges are hereby suspended until further notice.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.108 2008/11/22 12:40:39 pedro Exp $	*/
a1158 1
		bcstats.pendingwrites--;
d1160 8
a1167 4
	} else if (bcstats.numbufs && 
	           (!(ISSET(bp->b_flags, B_RAW) || ISSET(bp->b_flags, B_PHYS))))
		bcstats.pendingreads--;

@


1.109
log
@keep b_proc set to the proccess,
thats doing the io as advertised

closes PR3948

OK tedu@@ (and blambert@@ I think).
@
text
@a364 1
		bp->b_proc = curproc;
a538 1
	bp->b_proc = curproc;
a687 1
	bp->b_proc = NULL;
a1148 1
	bp->b_proc = NULL;
@


1.108
log
@Move diagnostic assertions concerning the recycle process of buffers
from getnewbuf() to buf_put(), since getnewbuf() does not directly
recycle buffers anymore. While at it, remove two lines of dead code
from getnewbuf(), which used to disassociate a vnode from a buffer.
"just go for it, because everyone had a chance" deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.107 2008/06/14 00:49:35 art Exp $	*/
d365 1
d540 1
d690 1
d1152 1
@


1.107
log
@Belt, suspenders, duct tape and glue.

In brelse, if we end up in the B_INVAL case without mappings, check
for B_WANTED and wake up the sleeper if there's one before freeing the
buffer. This shouldn't happen, but it looks like there might actually
be some dodgy corner cases in nfs where this could just happen if the
phase of the moon is right and the wind is blowing from the right
direction.

thib@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.104 2008/06/10 20:14:36 beck Exp $	*/
d264 1
d268 2
a269 2
#endif
#ifdef DIAGNOSTIC
a272 1

a275 2
#endif
#ifdef DIAGNOSTIC
d279 1
a1019 9
#ifdef DIAGNOSTIC
	if (ISSET(bp->b_flags, B_DELWRI))
		panic("Dirty buffer on BQ_CLEAN");
#endif

	/* disassociate us from our vnode, if we had one... */
	if (bp->b_vp)
		brelvp(bp);

a1020 6

#ifdef DIAGNOSTIC
	/* CLEAN buffers must have no dependencies */ 
	if (LIST_FIRST(&bp->b_dep) != NULL)
		panic("BQ_CLEAN has buffer with dependencies");
#endif
@


1.106
log
@Bring biomem diff back into the tree after the nfs_bio.c fix went in.
ok thib beck art
@
text
@d776 11
@


1.105
log
@back out biomem diff since it is not right yet.  Doing very large
file copies to nfsv2 causes the system to eventually peg the console.
On the console ^T indicates that the load is increasing rapidly, ddb
indicates many calls to getbuf, there is some very slow nfs traffic
making none (or extremely slow) progress.  Eventually some machines
seize up entirely.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.103 2008/03/16 19:42:57 otto Exp $	*/
d81 1
a81 1
#define	BQUEUES		6		/* number of free buffer queues */
d84 1
a84 1

a86 2
int bqpages[BQUEUES];		/* pages allocated, per queue */
int bqpagelow;
a93 1
struct vm_map *buf_map;
d107 1
a107 1
void buf_init(struct buf *, int);
a116 1
 *  numfreepages  - number of pages on BQ_CLEAN and BQ_DIRTY queues. unused.
d122 2
a123 2
long numbufpages;
long numdirtypages;
a125 2
long numfreepages;
long numcleanpages;
d130 5
a137 24
int size2cqueue(int *size);

int
size2cqueue(int *size)
{
	int i = 0, q;
	int s = *size;
	s -= 1;
	while (s > 0) {
		s = s >> 1;
		i++;
	}
	if (i < PAGE_SHIFT) {
		i = PAGE_SHIFT; /* < 4096 -> 4096 */
        }
	*size = 1 << i;
	q = (i + 1 - PAGE_SHIFT); /* XXX 4096 is queue 1 */
	if (q >= BQUEUES)
		panic("queue %d > BQUEUES %d", q, BQUEUES);
	if (q == 0)
		panic("can't return dirty q");
	return(q);
}

d142 2
a143 1
	int queue;
a158 1
	numfreepages -= atop(bp->b_bufsize);
d160 4
a163 6
		int qs = bp->b_bufsize;
		queue = size2cqueue(&qs);
		numcleanpages -= atop(bp->b_bufsize);
		bqpages[queue] -= atop(bp->b_bufsize);
	} else
		numdirtypages -= atop(bp->b_bufsize);
d165 1
d169 1
a169 1
buf_init(struct buf *bp, int size)
a170 2
	int npages, queue;

a172 1
	npages = atop(size);
a177 1
	queue = size2cqueue(&size);
a178 6
	numbufpages += npages;
	numfreepages += npages;
	numcleanpages += npages;
	bqpages[queue] += npages;
	if (maxcleanpages < numcleanpages)
		maxcleanpages = numcleanpages;
d205 1
a205 1
	bp->b_flags = B_BUSY;
d217 2
d221 1
a230 1
	struct bqueues *dp;
a232 2
	int queue, qs;
	void *data;
d239 1
a239 3
	qs = size;
	queue = size2cqueue(&qs);
	npages = atop(qs);
d241 1
a241 1
	if (numbufpages + npages > bufpages)
d246 1
a246 6
	data = (void *)uvm_km_alloc(buf_map, qs);
	if (data == NULL) {
		pool_put(&bufpool, bp);
		return (NULL);
	}
	buf_init(bp, qs);
d248 3
a250 4
	bp->b_bufsize = qs;
	bp->b_data = data;
	dp = &bufqueues[queue];
	binsheadfree(bp, dp);
d253 3
d265 1
a265 1
	if (bp->b_data != NULL)
d282 1
d284 2
a285 6
	if (bp->b_data != NULL) {
		bremhash(bp);
		numbufpages -= atop(bp->b_bufsize);
		uvm_km_free(buf_map, (vaddr_t)bp->b_data, bp->b_bufsize);
	}

a294 1
	vaddr_t minaddr, maxaddr;
d297 27
d328 6
a333 9
	minaddr = vm_map_min(kernel_map);
	buf_map = uvm_km_suballoc(kernel_map, &minaddr, &maxaddr,
	    ptoa(bufpages), 0, FALSE, NULL);

	/* 
	 * XXX don't starve any one queue below 5% of the total number
	 * of buffer cache pages.
	 */
	bqpagelow = bufpages / 20; 
d340 2
a341 4
	 * Reserve 5% of bufpages for syncer's needs,
	 * but not more than 25% and if possible
	 * not less than 2 * MAXBSIZE. locleanpages
	 * value must be not too small
d343 2
a344 6
	hicleanpages = bufpages / 2;
	locleanpages = hicleanpages / 2;
	if (locleanpages < atop(2 * MAXBSIZE))
		locleanpages = atop(2 * MAXBSIZE);
	if (locleanpages > bufpages / 4)
		locleanpages = bufpages / 4;
d364 2
a366 1

d454 1
a454 1
	bp->b_data = NULL;
d462 1
a462 1
	int howmany, i, maxra, inc;
a463 1
	size_t spill;
d520 2
d526 4
a529 1
		xbpp[i]->b_data = bp->b_data + (i * size);
d538 2
a539 6
	spill = bp->b_bufsize - bp->b_bcount;
	if (spill) {
		uvm_km_free(buf_map, (vaddr_t) bp->b_data + bp->b_bcount,
		    spill);
		numbufpages -= atop(spill);
	}
d586 2
d702 5
d722 4
a756 2
		int queue, qs;

d775 1
a775 1
		if (bp->b_data == NULL) {
d781 4
a784 7
		qs = bp->b_bufsize;
		queue = size2cqueue(&qs);
		numcleanpages += atop(bp->b_bufsize);
		bqpages[queue] += atop(bp->b_bufsize);
		if (maxcleanpages < numcleanpages)
			maxcleanpages = numcleanpages;
		binsheadfree(bp, &bufqueues[queue]);
a789 4
		int queue, qs;
		numfreepages += atop(bp->b_bufsize);
		qs = bp->b_bufsize;
		queue = size2cqueue(&qs);
d792 4
a795 5
			numcleanpages += atop(bp->b_bufsize);
			bqpages[queue] += atop(bp->b_bufsize);
			if (maxcleanpages < numcleanpages)
				maxcleanpages = numcleanpages;
			bufq = &bufqueues[queue];
d797 1
a797 1
			numdirtypages += atop(bp->b_bufsize);
d810 3
a812 1
	CLR(bp->b_flags, (B_AGE | B_ASYNC | B_BUSY | B_NOCACHE | B_DEFERRED));
d897 1
a897 1
			SET(bp->b_flags, (B_BUSY | B_CACHE));
d899 2
d928 4
d958 1
a958 1
	int s, error, queue, qs;
d968 1
a968 1
	if (numdirtypages >= hidirtypages || numcleanpages <= locleanpages)
d971 15
d988 1
a988 3
	qs = size;
	queue = size2cqueue(&qs);
	bp = buf_get(qs); /* XXX use qs instead and no need in buf_get? */
d990 8
a997 35
		/*
		 * No free ones, try to reuse a clean one of the same or
		 * larger size.
		 */
		do {
			bp = TAILQ_FIRST(&bufqueues[queue]);
			queue++;
		} while (bp == NULL && queue < BQUEUES);
	}
	if (bp == NULL) {
		/* we couldn't reuse a free one, nothing of the right size */
		/* XXX free 20 buffers per q - ugly hack  should really
		 * reuse big ones without truncating. fix later 
		 */
		int q, gotsome = 0;
		int freemax = 20; 
		for (q = 1; q < BQUEUES; q++) {
			int i = freemax;
			while (bqpages[q] > bqpagelow
			    && (bp = TAILQ_FIRST(&bufqueues[q]))
			    && i--) {
				gotsome++;
				bremfree(bp);
				if (LIST_FIRST(&bp->b_dep) != NULL)
					buf_deallocate(bp);

				if (ISSET(bp->b_flags, B_DELWRI)) {
					CLR(bp->b_flags, B_DELWRI);
				}

				if (bp->b_vp)
					brelvp(bp);

				buf_put(bp);
			}
d999 1
a999 1
		if (gotsome)
d1001 2
a1002 14
	}
	if (bp == NULL) {
		/* wait for a free buffer of any kind */
		needbuffer++;
		error = tsleep(&needbuffer, slpflag | (PRIBIO + 1),
		    "getnewbuf", slptimeo);
		if (ep != NULL) {
			*ep = error;
			if (error) {
				splx(s);
				return (NULL);
			}
		}
		goto getsome;
d1007 2
a1008 1
	SET(bp->b_flags, B_BUSY);
a1027 1
	bp->b_flags = B_BUSY;
d1055 1
a1055 2
		if (!numdirtypages ||
		    (numdirtypages < hidirtypages && !needbuffer))
d1063 1
a1063 1
			if (numdirtypages < lodirtypages && !needbuffer)
d1067 1
a1067 1
			SET(bp->b_flags, B_BUSY);
d1084 1
a1084 2
				numfreepages += atop(bp->b_bufsize);
				numdirtypages += atop(bp->b_bufsize);
d1086 2
a1087 1
				CLR(bp->b_flags, B_BUSY);
d1113 2
d1164 1
d1166 3
a1168 1
	}
a1181 63

#if 1
void
vfs_bufstats(void) {
	return;
}
/* #ifdef DDB */
#else
/*
 * Print out statistics on the current allocation of the buffer pool.
 * Can be enabled to print out on every ``sync'' by setting "syncprt"
 * in vfs_syscalls.c using sysctl.
 */
void
vfs_bufstats(void)
{
	int s, i, j, count;
	struct buf *bp;
	struct bqueues *dp;
	int counts[MAXBSIZE/PAGE_SIZE+1];
	int totals[BQUEUES];
	long ptotals[BQUEUES];
	long pages;
	static char *bname[BQUEUES] = { "CLEAN", "DIRTY", "EMPTY" };

	s = splbio();
	for (dp = bufqueues, i = 0; dp < &bufqueues[BQUEUES]; dp++, i++) {
		count = 0;
		pages = 0;
		for (j = 0; j <= MAXBSIZE/PAGE_SIZE; j++)
			counts[j] = 0;
		TAILQ_FOREACH(bp, dp, b_freelist) {
			counts[bp->b_bufsize/PAGE_SIZE]++;
			count++;
			pages += atop(bp->b_bufsize);
		}
		totals[i] = count;
		ptotals[i] = pages;
		printf("%s: total-%d(%d pages)", bname[i], count, pages);
		for (j = 0; j <= MAXBSIZE/PAGE_SIZE; j++)
			if (counts[j] != 0)
				printf(", %d-%d", j * PAGE_SIZE, counts[j]);
		printf("\n");
	}
	if ((ptotals[BQ_CLEAN] + ptotals[BQ_DIRTY]) != numfreepages)
		printf("numfreepages counter wrong: %ld != %ld\n",
		    numfreepages, ptotals[BQ_CLEAN] + ptotals[BQ_DIRTY]);
	if (ptotals[BQ_CLEAN] != numcleanpages)
		printf("numcleanpages counter wrong: %ld != %ld\n",
		    numcleanpages, ptotals[<BQ_CLEAN]);
	else
		printf("numcleanpages: %ld\n", numcleanpages);
	if (numdirtypages != ptotals[BQ_DIRTY])
		printf("numdirtypages counter wrong: %ld != %ld\n",
		    numdirtypages, ptotals[BQ_DIRTY]);
	else
		printf("numdirtypages: %ld\n", numdirtypages);

	printf("syncer eating up to %ld pages from %ld reserved\n",
	    maxcleanpages - hicleanpages, locleanpages);
	splx(s);
}
#endif /* DEBUG */
@


1.104
log
@
Buffer cache revamp

1) remove multiple size queues, introduced as a stopgap.
2) decouple pages containing data from their mappings
3) only keep buffers mapped when they actually have to be mapped
  (right now, this is when buffers are B_BUSY)
4) New functions to make a buffer busy, and release the busy flag
   (buf_acquire and buf_release)
5) Move high/low water marks and statistics counters into a structure
6) Add a sysctl to retrieve buffer cache statistics

Tested in several variants and beat upon by bob and art for a year. run
accidentally on henning's nfs server for a few months...

ok deraadt@@, krw@@, art@@ - who promises to be around to deal with any fallout
@
text
@d81 1
a81 1
#define	BQUEUES		2		/* number of free buffer queues */
d84 1
a84 1
#define	BQ_CLEAN	1		/* LRU queue with clean buffers */
d87 2
d96 1
d110 1
a110 1
void buf_init(struct buf *);
d120 1
d126 2
a127 2

struct bcachestats bcstats;
d130 2
d136 2
a137 2
/* XXX - should be defined here. */
extern int bufcachepercent;
d139 1
a139 1
vsize_t bufkvm;
d141 21
a161 2
struct proc *cleanerproc;
int bd_req;			/* Sleep point for cleaner daemon. */
d167 1
a167 2

	splassert(IPL_BIO);
d183 1
d185 6
a190 4
		bcstats.numcleanpages -= atop(bp->b_bufsize);
	} else {
		bcstats.numdirtypages -= atop(bp->b_bufsize);
	}
a191 1
	bcstats.freebufs--;
d195 1
a195 1
buf_init(struct buf *bp)
d197 2
d201 1
d207 1
d209 6
d241 1
a241 1
	bp->b_flags = 0;
a252 2
	buf_acquire_unmapped(bp);

a254 1
	bcstats.numbufs++;
d264 1
d267 2
d275 3
a277 1
	npages = atop(size);
d279 1
a279 1
	if (bcstats.numbufpages + npages > bufpages)
d284 6
a289 1
	buf_init(bp);
d291 4
a294 3
	buf_alloc_pages(bp, size);
	bp->b_data = NULL;
	binsheadfree(bp, &bufqueues[BQ_CLEAN]);
a296 3
	bcstats.numbufs++;
	bcstats.freebufs++;
	bcstats.numcleanpages += atop(bp->b_bufsize);
d306 1
a306 1
	if (bp->b_pobj != NULL)
a322 1
	bcstats.numbufs--;
d324 6
a329 2
	if (buf_dealloc_mem(bp) != 0)
		return;
d339 1
a341 27
	/* XXX - for now */
	bufpages = bufcachepercent = bufkvm = 0;

	/*
	 * If MD code doesn't say otherwise, use 10% of kvm for mappings and
	 * 10% physmem for pages.
	 */
	if (bufcachepercent == 0)
		bufcachepercent = 10;
	if (bufpages == 0)
		bufpages = physmem * bufcachepercent / 100;

	if (bufkvm == 0)
		bufkvm = (VM_MAX_KERNEL_ADDRESS - VM_MIN_KERNEL_ADDRESS) / 10;

	/*
	 * Don't use more than twice the amount of bufpages for mappings.
	 * It's twice since we map things sparsely.
	 */
	if (bufkvm > bufpages * PAGE_SIZE)
		bufkvm = bufpages * PAGE_SIZE;
	/*
	 * Round bufkvm to MAXPHYS because we allocate chunks of va space
	 * in MAXPHYS chunks.
	 */
	bufkvm &= ~(MAXPHYS - 1);

d346 9
a354 6

	/*
	 * hmm - bufkvm is an argument because it's static, while
	 * bufpages is global because it can change while running.
 	 */
	buf_mem_init(bufkvm);
d361 4
a364 2
	 * When we hit 95% of pages being clean, we bring them down to
	 * 90% to have some slack.
d366 6
a371 2
	hicleanpages = bufpages - (bufpages / 20);
	locleanpages = bufpages - (bufpages / 10);
a390 2
		bcstats.pendingreads++;
		bcstats.numreads++;
d392 1
d480 1
a480 1
	bp->b_pobj = NULL;
d488 1
a488 1
	int howmany, maxra, i, inc;
d490 1
a546 2
		bcstats.pendingreads++;
		bcstats.numreads++;
d551 1
a551 4
		xbpp[i]->b_data = NULL;
		xbpp[i]->b_pobj = bp->b_pobj;
		xbpp[i]->b_poffs = bp->b_poffs + (i * size);
		buf_acquire_unmapped(xbpp[i]);
d560 6
a565 2
	bcstats.pendingreads++;
	bcstats.numreads++;
a611 2
	bcstats.pendingwrites++;
	bcstats.numwrites++;
a725 5
#ifdef DIAGNOSTIC
	if (!ISSET(bp->b_flags, B_BUSY))
		panic("Trying to dirty buffer on freelist!");
#endif

a740 4
#ifdef DIAGNOSTIC
	if (!ISSET(bp->b_flags, B_BUSY))
		panic("Trying to undirty buffer on freelist!");
#endif
d772 2
d792 1
a792 1
		if (bp->b_data == NULL && bp->b_pobj == NULL) {
d798 7
a804 4
		bcstats.numcleanpages += atop(bp->b_bufsize);
		if (maxcleanpages < bcstats.numcleanpages)
			maxcleanpages = bcstats.numcleanpages;
		binsheadfree(bp, &bufqueues[BQ_CLEAN]);
d810 4
d816 5
a820 4
			bcstats.numcleanpages += atop(bp->b_bufsize);
			if (maxcleanpages < bcstats.numcleanpages)
				maxcleanpages = bcstats.numcleanpages;
			bufq = &bufqueues[BQ_CLEAN];
d822 1
a822 1
			bcstats.numdirtypages += atop(bp->b_bufsize);
d835 1
a835 3
	bcstats.freebufs++;
	CLR(bp->b_flags, (B_AGE | B_ASYNC | B_NOCACHE | B_DEFERRED));
	buf_release(bp);
d920 1
a920 1
			bcstats.cachehits++;
a921 2
			SET(bp->b_flags, B_CACHE);
			buf_acquire(bp);
a948 4
#ifdef DIAGNOSTIC
	if (!ISSET(bp->b_flags, B_BUSY))
		panic("getblk buffer not B_BUSY");
#endif
d975 1
a975 1
	int s;
d985 1
a985 1
	if (bcstats.numdirtypages >= hidirtypages || bcstats.numcleanpages <= locleanpages)
a987 15
	/*
	 * If we're above the high water mark for clean pages,
	 * free down to the low water mark.
	 */
	if (bcstats.numcleanpages > hicleanpages) {
		while (bcstats.numcleanpages > locleanpages) {
			bp = TAILQ_FIRST(&bufqueues[BQ_CLEAN]);
			bremfree(bp);
			if (bp->b_vp)
				brelvp(bp);
			bremhash(bp);
			buf_put(bp);
		}
	}

d990 13
a1002 1
	bp = buf_get(size);
d1004 25
a1028 8
		int freemax = 5;
		int i = freemax;
		while ((bp = TAILQ_FIRST(&bufqueues[BQ_CLEAN])) && i--) {
			bremfree(bp);
			if (bp->b_vp)
				brelvp(bp);
			bremhash(bp);
			buf_put(bp);
d1030 1
a1030 1
		if (freemax != i)
d1032 14
a1045 2
		splx(s);
		return (NULL);
d1050 1
a1050 2
	bp->b_flags = 0;
	buf_acquire(bp);
d1070 1
d1098 2
a1099 1
		if (bcstats.numdirtypages < hidirtypages)
d1107 1
a1107 1
			if (bcstats.numdirtypages < lodirtypages)
d1111 1
a1111 1
			buf_acquire(bp);
d1128 2
a1129 1
				bcstats.numdirtypages += atop(bp->b_bufsize);
d1131 1
a1131 2
				bcstats.freebufs++;
				buf_release(bp);
a1156 2
	KASSERT(!(bp->b_flags & B_ASYNC));

a1205 1
		bcstats.pendingwrites--;
d1207 1
a1207 3
	} else if (bcstats.numbufs && 
	           (!(ISSET(bp->b_flags, B_RAW) || ISSET(bp->b_flags, B_PHYS))))
		bcstats.pendingreads--;
d1221 63
@


1.103
log
@Widen some struct statfs fields to support large filesystem stata
and add some to be able to support statvfs(2). Do the compat dance
to provide backward compatibility.  ok thib@@ miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.102 2007/10/21 15:54:55 beck Exp $	*/
d81 1
a81 1
#define	BQUEUES		6		/* number of free buffer queues */
d84 1
a84 1

a86 2
int bqpages[BQUEUES];		/* pages allocated, per queue */
int bqpagelow;
a93 1
struct vm_map *buf_map;
d107 1
a107 1
void buf_init(struct buf *, int);
a116 1
 *  numfreepages  - number of pages on BQ_CLEAN and BQ_DIRTY queues. unused.
d122 2
a123 2
long numbufpages;
long numdirtypages;
a125 2
long numfreepages;
long numcleanpages;
d130 5
a137 24
int size2cqueue(int *size);

int
size2cqueue(int *size)
{
	int i = 0, q;
	int s = *size;
	s -= 1;
	while (s > 0) {
		s = s >> 1;
		i++;
	}
	if (i < PAGE_SHIFT) {
		i = PAGE_SHIFT; /* < 4096 -> 4096 */
        }
	*size = 1 << i;
	q = (i + 1 - PAGE_SHIFT); /* XXX 4096 is queue 1 */
	if (q >= BQUEUES)
		panic("queue %d > BQUEUES %d", q, BQUEUES);
	if (q == 0)
		panic("can't return dirty q");
	return(q);
}

d142 2
a143 1
	int queue;
a158 1
	numfreepages -= atop(bp->b_bufsize);
d160 4
a163 6
		int qs = bp->b_bufsize;
		queue = size2cqueue(&qs);
		numcleanpages -= atop(bp->b_bufsize);
		bqpages[queue] -= atop(bp->b_bufsize);
	} else
		numdirtypages -= atop(bp->b_bufsize);
d165 1
d169 1
a169 1
buf_init(struct buf *bp, int size)
a170 2
	int npages, queue;

a172 1
	npages = atop(size);
a177 1
	queue = size2cqueue(&size);
a178 6
	numbufpages += npages;
	numfreepages += npages;
	numcleanpages += npages;
	bqpages[queue] += npages;
	if (maxcleanpages < numcleanpages)
		maxcleanpages = numcleanpages;
d205 1
a205 1
	bp->b_flags = B_BUSY;
d217 2
d221 1
a230 1
	struct bqueues *dp;
a232 2
	int queue, qs;
	void *data;
d239 1
a239 3
	qs = size;
	queue = size2cqueue(&qs);
	npages = atop(qs);
d241 1
a241 1
	if (numbufpages + npages > bufpages)
d246 1
a246 6
	data = (void *)uvm_km_alloc(buf_map, qs);
	if (data == NULL) {
		pool_put(&bufpool, bp);
		return (NULL);
	}
	buf_init(bp, qs);
d248 3
a250 4
	bp->b_bufsize = qs;
	bp->b_data = data;
	dp = &bufqueues[queue];
	binsheadfree(bp, dp);
d253 3
d265 1
a265 1
	if (bp->b_data != NULL)
d282 1
d284 2
a285 6
	if (bp->b_data != NULL) {
		bremhash(bp);
		numbufpages -= atop(bp->b_bufsize);
		uvm_km_free(buf_map, (vaddr_t)bp->b_data, bp->b_bufsize);
	}

a294 1
	vaddr_t minaddr, maxaddr;
d297 27
d328 6
a333 9
	minaddr = vm_map_min(kernel_map);
	buf_map = uvm_km_suballoc(kernel_map, &minaddr, &maxaddr,
	    ptoa(bufpages), 0, FALSE, NULL);

	/* 
	 * XXX don't starve any one queue below 5% of the total number
	 * of buffer cache pages.
	 */
	bqpagelow = bufpages / 20; 
d340 2
a341 4
	 * Reserve 5% of bufpages for syncer's needs,
	 * but not more than 25% and if possible
	 * not less than 2 * MAXBSIZE. locleanpages
	 * value must be not too small
d343 2
a344 6
	hicleanpages = bufpages / 2;
	locleanpages = hicleanpages / 2;
	if (locleanpages < atop(2 * MAXBSIZE))
		locleanpages = atop(2 * MAXBSIZE);
	if (locleanpages > bufpages / 4)
		locleanpages = bufpages / 4;
d364 2
a366 1

d454 1
a454 1
	bp->b_data = NULL;
d462 1
a462 1
	int howmany, i, maxra, inc;
a463 1
	size_t spill;
d520 2
d526 4
a529 1
		xbpp[i]->b_data = bp->b_data + (i * size);
d538 2
a539 6
	spill = bp->b_bufsize - bp->b_bcount;
	if (spill) {
		uvm_km_free(buf_map, (vaddr_t) bp->b_data + bp->b_bcount,
		    spill);
		numbufpages -= atop(spill);
	}
d586 2
d702 5
d722 4
a756 2
		int queue, qs;

d775 1
a775 1
		if (bp->b_data == NULL) {
d781 4
a784 7
		qs = bp->b_bufsize;
		queue = size2cqueue(&qs);
		numcleanpages += atop(bp->b_bufsize);
		bqpages[queue] += atop(bp->b_bufsize);
		if (maxcleanpages < numcleanpages)
			maxcleanpages = numcleanpages;
		binsheadfree(bp, &bufqueues[queue]);
a789 4
		int queue, qs;
		numfreepages += atop(bp->b_bufsize);
		qs = bp->b_bufsize;
		queue = size2cqueue(&qs);
d792 4
a795 5
			numcleanpages += atop(bp->b_bufsize);
			bqpages[queue] += atop(bp->b_bufsize);
			if (maxcleanpages < numcleanpages)
				maxcleanpages = numcleanpages;
			bufq = &bufqueues[queue];
d797 1
a797 1
			numdirtypages += atop(bp->b_bufsize);
d810 3
a812 1
	CLR(bp->b_flags, (B_AGE | B_ASYNC | B_BUSY | B_NOCACHE | B_DEFERRED));
d897 1
a897 1
			SET(bp->b_flags, (B_BUSY | B_CACHE));
d899 2
d928 4
d958 1
a958 1
	int s, error, queue, qs;
d968 1
a968 1
	if (numdirtypages >= hidirtypages || numcleanpages <= locleanpages)
d971 15
d988 1
a988 3
	qs = size;
	queue = size2cqueue(&qs);
	bp = buf_get(qs); /* XXX use qs instead and no need in buf_get? */
d990 8
a997 35
		/*
		 * No free ones, try to reuse a clean one of the same or
		 * larger size.
		 */
		do {
			bp = TAILQ_FIRST(&bufqueues[queue]);
			queue++;
		} while (bp == NULL && queue < BQUEUES);
	}
	if (bp == NULL) {
		/* we couldn't reuse a free one, nothing of the right size */
		/* XXX free 20 buffers per q - ugly hack  should really
		 * reuse big ones without truncating. fix later 
		 */
		int q, gotsome = 0;
		int freemax = 20; 
		for (q = 1; q < BQUEUES; q++) {
			int i = freemax;
			while (bqpages[q] > bqpagelow
			    && (bp = TAILQ_FIRST(&bufqueues[q]))
			    && i--) {
				gotsome++;
				bremfree(bp);
				if (LIST_FIRST(&bp->b_dep) != NULL)
					buf_deallocate(bp);

				if (ISSET(bp->b_flags, B_DELWRI)) {
					CLR(bp->b_flags, B_DELWRI);
				}

				if (bp->b_vp)
					brelvp(bp);

				buf_put(bp);
			}
d999 1
a999 1
		if (gotsome)
d1001 2
a1002 14
	}
	if (bp == NULL) {
		/* wait for a free buffer of any kind */
		needbuffer++;
		error = tsleep(&needbuffer, slpflag | (PRIBIO + 1),
		    "getnewbuf", slptimeo);
		if (ep != NULL) {
			*ep = error;
			if (error) {
				splx(s);
				return (NULL);
			}
		}
		goto getsome;
d1007 2
a1008 1
	SET(bp->b_flags, B_BUSY);
a1027 1
	bp->b_flags = B_BUSY;
d1055 1
a1055 2
		if (!numdirtypages ||
		    (numdirtypages < hidirtypages && !needbuffer))
d1063 1
a1063 1
			if (numdirtypages < lodirtypages && !needbuffer)
d1067 1
a1067 1
			SET(bp->b_flags, B_BUSY);
d1084 1
a1084 2
				numfreepages += atop(bp->b_bufsize);
				numdirtypages += atop(bp->b_bufsize);
d1086 2
a1087 1
				CLR(bp->b_flags, B_BUSY);
d1113 2
d1164 1
d1166 3
a1168 1
	}
a1181 63

#if 1
void
vfs_bufstats(void) {
	return;
}
/* #ifdef DDB */
#else
/*
 * Print out statistics on the current allocation of the buffer pool.
 * Can be enabled to print out on every ``sync'' by setting "syncprt"
 * in vfs_syscalls.c using sysctl.
 */
void
vfs_bufstats(void)
{
	int s, i, j, count;
	struct buf *bp;
	struct bqueues *dp;
	int counts[MAXBSIZE/PAGE_SIZE+1];
	int totals[BQUEUES];
	long ptotals[BQUEUES];
	long pages;
	static char *bname[BQUEUES] = { "CLEAN", "DIRTY", "EMPTY" };

	s = splbio();
	for (dp = bufqueues, i = 0; dp < &bufqueues[BQUEUES]; dp++, i++) {
		count = 0;
		pages = 0;
		for (j = 0; j <= MAXBSIZE/PAGE_SIZE; j++)
			counts[j] = 0;
		TAILQ_FOREACH(bp, dp, b_freelist) {
			counts[bp->b_bufsize/PAGE_SIZE]++;
			count++;
			pages += atop(bp->b_bufsize);
		}
		totals[i] = count;
		ptotals[i] = pages;
		printf("%s: total-%d(%d pages)", bname[i], count, pages);
		for (j = 0; j <= MAXBSIZE/PAGE_SIZE; j++)
			if (counts[j] != 0)
				printf(", %d-%d", j * PAGE_SIZE, counts[j]);
		printf("\n");
	}
	if ((ptotals[BQ_CLEAN] + ptotals[BQ_DIRTY]) != numfreepages)
		printf("numfreepages counter wrong: %ld != %ld\n",
		    numfreepages, ptotals[BQ_CLEAN] + ptotals[BQ_DIRTY]);
	if (ptotals[BQ_CLEAN] != numcleanpages)
		printf("numcleanpages counter wrong: %ld != %ld\n",
		    numcleanpages, ptotals[<BQ_CLEAN]);
	else
		printf("numcleanpages: %ld\n", numcleanpages);
	if (numdirtypages != ptotals[BQ_DIRTY])
		printf("numdirtypages counter wrong: %ld != %ld\n",
		    numdirtypages, ptotals[BQ_DIRTY]);
	else
		printf("numdirtypages: %ld\n", numdirtypages);

	printf("syncer eating up to %ld pages from %ld reserved\n",
	    maxcleanpages - hicleanpages, locleanpages);
	splx(s);
}
#endif /* DEBUG */
@


1.102
log
@This QUEUE_DEBUG should really be DIAGNOSTIC - we need these checks
normally.
ok deraadt@@ tedu@@ otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.101 2007/10/18 20:26:55 beck Exp $	*/
d380 1
d397 14
@


1.101
log
@Correct possible spl problem in buffer cleaning daemon - the buffer cleaning
daemon requires splbio when doing dirty buffer queue manipulation. Since
version 1.88 of vfs_bio.c, it was possible to break out of the processing
loop when the cleaner had been running long enough, and this early exit would
mean a future pass through would manipulate the buffer queues not at splbio.
This change corrects this.
ok krw@@, deraadt@@, tedu@@, thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.100 2007/09/15 10:10:37 martin Exp $	*/
d309 1
a309 1
#ifdef QUEUE_MACRO_DEBUG
@


1.100
log
@replace ctob and btoc with ptoa and atop respectively

help and ok miod@@ thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.99 2007/08/07 04:32:45 beck Exp $	*/
d1125 1
a1128 1
			s = splbio();
@


1.99
log
@
   A few changes to deal with multi-user performance issues seen. this
brings us back roughly to 4.1 level performance, although this is still
far from optimal as we have seen in a number of cases. This change

	1) puts a lower bound on buffer cache queues to prevent starvation
	2) fixes the code which looks for a buffer to recycle
	3) reduces the number of vnodes back to 4.1 levels to avoid complex
	   performance issues better addressed after 4.2

ok art@@ deraadt@@, tested by many
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.98 2007/07/09 15:30:25 miod Exp $	*/
d183 1
a183 1
	numfreepages -= btoc(bp->b_bufsize);
d187 2
a188 2
		numcleanpages -= btoc(bp->b_bufsize);
		bqpages[queue] -= btoc(bp->b_bufsize);
d190 1
a190 1
		numdirtypages -= btoc(bp->b_bufsize);
d201 1
a201 1
	npages = btoc(size);
d277 1
a277 1
	npages = btoc(qs);
d326 1
a326 1
		numbufpages -= btoc(bp->b_bufsize);
d368 2
a369 2
	if (locleanpages < btoc(2 * MAXBSIZE))
		locleanpages = btoc(2 * MAXBSIZE);
d785 2
a786 2
		numcleanpages += btoc(bp->b_bufsize);
		bqpages[queue] += btoc(bp->b_bufsize);
d796 1
a796 1
		numfreepages += btoc(bp->b_bufsize);
d801 2
a802 2
			numcleanpages += btoc(bp->b_bufsize);
			bqpages[queue] += btoc(bp->b_bufsize);
d807 1
a807 1
			numdirtypages += btoc(bp->b_bufsize);
d1113 2
a1114 2
				numfreepages += btoc(bp->b_bufsize);
				numdirtypages += btoc(bp->b_bufsize);
d1240 1
a1240 1
			pages += btoc(bp->b_bufsize);
@


1.98
log
@Do not allow clustering read for filesystems which block size is smaller
than the hardware page size, as was the case in the old clustering code.
This fixes vnd reads on alpha and sparc64

On behalf of pedro@@, ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.97 2007/06/17 20:06:10 jasper Exp $	*/
d87 2
d167 1
d184 3
a186 1
	if (!ISSET(bp->b_flags, B_DELWRI))
d188 2
a189 1
	else
d197 1
a197 1
	int npages;
d207 1
d212 1
d350 6
d786 1
d802 1
d979 8
a986 5
		/* no free ones, try to reuse a clean one.. */
		for (bp = TAILQ_FIRST(&bufqueues[queue]);
		     bp != NULL && queue < BQUEUES; queue++) {
				/* XXX */
		}
d997 3
a999 1
			while ((bp = TAILQ_FIRST(&bufqueues[q])) && i--) {
@


1.97
log
@de-register

ok thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.96 2007/06/09 08:21:34 pedro Exp $	*/
d465 2
a466 2
	size = round_page(size);
	howmany = MAXPHYS / size;
d470 1
a471 1

d475 1
@


1.96
log
@Protect access to 'bufhead' with splbio(), okay art@@ millert@@ marco@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.95 2007/06/03 20:25:12 otto Exp $	*/
d1201 2
a1202 2
	register struct buf *bp;
	register struct bqueues *dp;
@


1.95
log
@backout rev 1.91 and 1.92, it causes proceses to hang on low mem
machines. ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.90 2007/05/28 22:18:48 thib Exp $	*/
a243 1
	LIST_INSERT_HEAD(&bufhead, bp, b_list);
d246 1
@


1.94
log
@Uninline bio_doread(), okay art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.93 2007/06/01 17:34:08 dlg Exp $	*/
d107 1
a107 2
struct buf *getnewbuf(size_t);
int buf_wait(int, int);
d504 1
a504 1
	bp = getnewbuf(howmany * size);
d902 1
a902 1
		nb = getnewbuf(size);
a903 1
			error = buf_wait(slpflag, slptimeo);
d928 2
a929 2
	while ((bp = getnewbuf(size)) == NULL)
		buf_wait(0, 0);
d940 1
a940 1
getnewbuf(size_t size)
d943 1
a943 1
	int s, queue, qs;
d997 12
a1008 2
		splx(s);
		return (NULL);
a1044 13
}

int
buf_wait(int flag, int timeo)
{
	int s, error;

	s = splbio();
	needbuffer++;
	error = tsleep(&needbuffer, flag | (PRIBIO + 1), "buf_wait", timeo);
	splx(s);

	return (error);
@


1.93
log
@dont request zeroed memory when we allocate data regions for buffers. this
moves memset from the 20th most expensive function in the kernel to the
331st when doing heavy io.

ok tedu@@ thib@@ pedro@@ beck@@ art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.92 2007/05/29 21:34:43 art Exp $	*/
d106 1
a106 1
static __inline struct buf *bio_doread(struct vnode *, daddr64_t, int, int);
d363 1
a363 1
static __inline struct buf *
@


1.92
log
@I suck. Forgot splx() in the early return path.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.91 2007/05/29 18:50:08 art Exp $	*/
d277 1
a277 1
	data = (void *)uvm_km_zalloc(buf_map, qs);
@


1.91
log
@Change getnewbuf from taking lots of sleep arguments and then doing magic
with them and trying to pass an error back after sleep, to just fail with
NULL and let the caller wait for buffers.

Introduce buf_wait() that does all the sleep magic and use buf_wait in
the getnewbuf callers where it matters.

pedro@@ beck@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.90 2007/05/28 22:18:48 thib Exp $	*/
d999 1
@


1.90
log
@pool_setipl() on the bufpool, to make sure that
every get/put is at IPL_BIO.

ok pedro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.89 2007/05/27 01:25:56 pedro Exp $	*/
d107 2
a108 1
struct buf *getnewbuf(size_t, int, int, int *);
d505 1
a505 1
	bp = getnewbuf(howmany * size, 0, 0, NULL);
d903 1
a903 1
		nb = getnewbuf(size, slpflag, slptimeo, &error);
d905 1
d930 2
a931 2
	while ((bp = getnewbuf(size, 0, 0, NULL)) == NULL)
		;
d942 1
a942 1
getnewbuf(size_t size, int slpflag, int slptimeo, int *ep)
d945 1
a945 1
	int s, error, queue, qs;
d999 1
a999 12
		/* wait for a free buffer of any kind */
		needbuffer++;
		error = tsleep(&needbuffer, slpflag | (PRIBIO + 1),
		    "getnewbuf", slptimeo);
		if (ep != NULL) {
			*ep = error;
			if (error) {
				splx(s);
				return (NULL);
			}
		}
		goto getsome;
d1036 13
@


1.89
log
@remove silly comment, okay deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.88 2007/05/26 20:26:51 pedro Exp $	*/
d335 1
@


1.88
log
@Dynamic buffer cache. Initial diff from mickey@@, okay art@@ beck@@ toby@@
deraadt@@ dlg@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.87 2006/10/21 18:09:52 thib Exp $	*/
a944 1
	/* some pricks are not allowed! */
@


1.87
log
@Retire B_LOCKED and BQ_LOCKED. The last real usage of the
flag and the buffer queue went away when LFS was removed.

ok, pedro@@
"looks sane", deraadt@@
testing: tybollt@@solace.miun.se
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.86 2006/10/19 12:04:31 mickey Exp $	*/
d81 3
a83 1
#define	BQUEUES		3		/* number of free buffer queues */
a84 3
#define	BQ_CLEAN	0		/* LRU queue with clean buffers */
#define	BQ_DIRTY	1		/* LRU queue with dirty buffers */
#define	BQ_EMPTY	2		/* buffer headers with no memory */
a87 1
int nobuffers;
d94 5
d107 3
a109 1
struct buf *getnewbuf(int, int, int *);
d114 1
d122 1
a122 2
 *  mincleanpages - the lowest byte count on BQ_CLEAN.
 *  numemptybufs  - number of buffers on BQ_EMPTY. unused.
d124 1
d131 2
a132 4
int numemptybufs;
#ifdef DEBUG
long mincleanpages;
#endif
d137 24
d180 133
a312 9
	if (bp->b_bufsize <= 0) {
		numemptybufs--;
	} else {
		numfreepages -= btoc(bp->b_bufsize);
		if (!ISSET(bp->b_flags, B_DELWRI)) {
			numcleanpages -= btoc(bp->b_bufsize);
#ifdef DEBUG
			if (mincleanpages > numcleanpages)
				mincleanpages = numcleanpages;
d314 6
a319 3
		} else {
			numdirtypages -= btoc(bp->b_bufsize);
		}
d321 2
a322 1
	TAILQ_REMOVE(dp, bp, b_freelist);
d331 1
a331 1
	struct buf *bp;
a332 2
	int i;
	int base, residual;
d337 7
a343 29
	bufhashtbl = hashinit(nbuf, M_CACHE, M_WAITOK, &bufhash);
	base = bufpages / nbuf;
	residual = bufpages % nbuf;
	for (i = 0; i < nbuf; i++) {
		bp = &buf[i];
		bzero((char *)bp, sizeof *bp);
		bp->b_dev = NODEV;
		bp->b_vnbufs.le_next = NOLIST;
		bp->b_data = buffers + i * MAXBSIZE;
		LIST_INIT(&bp->b_dep);
		if (i < residual)
			bp->b_bufsize = (base + 1) * PAGE_SIZE;
		else
			bp->b_bufsize = base * PAGE_SIZE;
		bp->b_flags = B_INVAL;
		if (bp->b_bufsize) {
			dp = &bufqueues[BQ_CLEAN];
			numfreepages += btoc(bp->b_bufsize);
			numcleanpages += btoc(bp->b_bufsize);
		} else {
			dp = &bufqueues[BQ_EMPTY];
			numemptybufs++;
		}
		binsheadfree(bp, dp);
		binshash(bp, &invalhash);
	}

	hidirtypages = bufpages / 4;
	lodirtypages = hidirtypages / 2;
d349 1
a349 2
	 * value must be not too small, but probably
	 * there is no reason to set it to more than 1-2 MB.
d351 2
a352 1
	locleanpages = bufpages / 20;
a356 2
	if (locleanpages > btoc(2 * 1024 * 1024))
		locleanpages = btoc(2 * 1024 * 1024);
d358 1
a358 3
#ifdef DEBUG
	mincleanpages = locleanpages;
#endif
d433 109
d647 1
a647 1
		bp->b_synctime = time_second + 35;
d657 1
a657 1
		if (bp->b_synctime < time_second) {
d697 1
a697 1
		bp->b_synctime = time_second + 35;
d729 3
d740 3
a742 1
	if ((bp->b_bufsize <= 0) || ISSET(bp->b_flags, B_INVAL)) {
d744 2
a745 2
		 * If it's invalid or empty, dissociate it from its vnode
		 * and put on the head of the appropriate queue.
d757 8
a764 9
		if (bp->b_bufsize <= 0) {
			/* no data */
			bufq = &bufqueues[BQ_EMPTY];
			numemptybufs++;
		} else {
			/* invalid data */
			bufq = &bufqueues[BQ_CLEAN];
			numfreepages += btoc(bp->b_bufsize);
			numcleanpages += btoc(bp->b_bufsize);
d766 7
a772 1
		binsheadfree(bp, bufq);
d778 1
d780 2
d785 3
a787 1
			bufq = &bufqueues[BQ_CLEAN];
d792 1
a792 1
		if (ISSET(bp->b_flags, B_AGE))
d794 2
a795 1
		else
d797 2
a803 7

	/* Wake up syncer and cleaner processes waiting for buffers */
	if (nobuffers) {
		wakeup(&nobuffers);
		nobuffers = 0;
	}

d805 1
a805 1
	if (needbuffer && (numcleanpages > locleanpages)) {
d850 1
a850 1
	struct buf *bp;
d871 6
d894 9
a902 4

	if (bp == NULL) {
		bp = getnewbuf(slpflag, slptimeo, &error);
		if (bp == NULL) {
a904 1
			goto start;
d906 4
a909 1

a915 2
	allocbuf(bp, size);

d927 1
a927 1
	while ((bp = getnewbuf(0, 0, NULL)) == NULL)
a930 1
	allocbuf(bp, size);
a935 88
 * Expand or contract the actual memory allocated to a buffer.
 *
 * If the buffer shrinks, data is lost, so it's up to the
 * caller to have written it out *first*; this routine will not
 * start a write.  If the buffer grows, it's the caller's
 * responsibility to fill out the buffer's additional contents.
 */
void
allocbuf(struct buf *bp, int size)
{
	struct buf	*nbp;
	vsize_t		desired_size;
	int		s;

	desired_size = round_page(size);
	if (desired_size > MAXBSIZE)
		panic("allocbuf: buffer larger than MAXBSIZE requested");

	if (bp->b_bufsize == desired_size)
		goto out;

	/*
	 * If the buffer is smaller than the desired size, we need to snarf
	 * it from other buffers.  Get buffers (via getnewbuf()), and
	 * steal their pages.
	 */
	while (bp->b_bufsize < desired_size) {
		int amt;

		/* find a buffer */
		while ((nbp = getnewbuf(0, 0, NULL)) == NULL)
			;
 		SET(nbp->b_flags, B_INVAL);
		binshash(nbp, &invalhash);

		/* and steal its pages, up to the amount we need */
		amt = MIN(nbp->b_bufsize, (desired_size - bp->b_bufsize));
		pagemove((nbp->b_data + nbp->b_bufsize - amt),
			 bp->b_data + bp->b_bufsize, amt);
		bp->b_bufsize += amt;
		nbp->b_bufsize -= amt;

		/* reduce transfer count if we stole some data */
		if (nbp->b_bcount > nbp->b_bufsize)
			nbp->b_bcount = nbp->b_bufsize;

#ifdef DIAGNOSTIC
		if (nbp->b_bufsize < 0)
			panic("allocbuf: negative bufsize");
#endif

		brelse(nbp);
	}

	/*
	 * If we want a buffer smaller than the current size,
	 * shrink this buffer.  Grab a buf head from the EMPTY queue,
	 * move a page onto it, and put it on front of the AGE queue.
	 * If there are no free buffer headers, leave the buffer alone.
	 */
	if (bp->b_bufsize > desired_size) {
		s = splbio();
		if ((nbp = TAILQ_FIRST(&bufqueues[BQ_EMPTY])) == NULL) {
			/* No free buffer head */
			splx(s);
			goto out;
		}
		bremfree(nbp);
		SET(nbp->b_flags, B_BUSY);
		splx(s);

		/* move the page to it and note this change */
		pagemove(bp->b_data + desired_size,
		    nbp->b_data, bp->b_bufsize - desired_size);
		nbp->b_bufsize = bp->b_bufsize - desired_size;
		bp->b_bufsize = desired_size;
		nbp->b_bcount = 0;
		SET(nbp->b_flags, B_INVAL);

		/* release the newly-filled buffer and leave */
		brelse(nbp);
	}

out:
	bp->b_bcount = size;
}

/*
d939 1
a939 1
getnewbuf(int slpflag, int slptimeo, int *ep)
d942 6
a947 1
	int s, error;
d951 1
a951 1
	 * Wake up cleaner if we're getting low on buffers.
d953 1
a953 1
	if (numdirtypages >= hidirtypages)
d956 11
a966 9
	if ((numcleanpages <= locleanpages) &&
	    curproc != syncerproc && curproc != cleanerproc) {
		needbuffer++;
		error = tsleep(&needbuffer, slpflag | (PRIBIO + 1),
		    "getnewbuf", slptimeo);
		splx(s);
		if (ep != NULL)
			*ep = error;
		return (NULL);
d968 21
d990 6
a995 1
	bp = TAILQ_FIRST(&bufqueues[BQ_CLEAN]);
d998 2
a999 2
		nobuffers = 1;
		error = tsleep(&nobuffers, slpflag | (PRIBIO - 3),
d1001 1
a1001 2
		splx(s);
		if (ep != NULL)
d1003 6
a1008 1
		return (NULL);
a1011 1

d1039 1
a1039 1
	bp->b_bcount = 0;
d1053 2
a1055 2
	struct buf *bp;
	struct timeval starttime, timediff;
d1059 1
d1061 2
a1062 1
		if (numdirtypages < hidirtypages) {
a1063 1
		}
d1066 1
a1066 1
		s = splbio();
d1070 3
a1099 2
			if (numdirtypages < lodirtypages)
				break;
d1185 7
a1191 1
#ifdef DEBUG
a1227 3
	if (totals[BQ_EMPTY] != numemptybufs)
		printf("numemptybufs counter wrong: %d != %d\n",
			numemptybufs, totals[BQ_EMPTY]);
d1230 1
a1230 1
			numfreepages, ptotals[BQ_CLEAN] + ptotals[BQ_DIRTY]);
d1233 1
a1233 1
			numcleanpages, ptotals[BQ_CLEAN]);
d1238 1
a1238 1
			numdirtypages, ptotals[BQ_DIRTY]);
d1243 1
a1243 1
			locleanpages - mincleanpages, locleanpages);
@


1.86
log
@some buffers (containing metadata) can only be written during the
bdwrite call made on 'em due to otherwise pending softdeps and thus
being deferred by the sybcer. promote bdwrite into bawrite for
those cases. tested by many.
pedro@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.85 2006/10/16 11:27:53 pedro Exp $	*/
d81 1
a81 1
#define	BQUEUES		4		/* number of free buffer queues */
d83 3
a85 4
#define	BQ_LOCKED	0		/* super-blocks &c */
#define	BQ_CLEAN	1		/* LRU queue with clean buffers */
#define	BQ_DIRTY	2		/* LRU queue with dirty buffers */
#define	BQ_EMPTY	3		/* buffer headers with no memory */
d154 1
a154 1
	} else if (!ISSET(bp->b_flags, B_LOCKED)) {
a495 4
	/* If it's locked, don't report an error; try again later. */
	if (ISSET(bp->b_flags, (B_LOCKED|B_ERROR)) == (B_LOCKED|B_ERROR))
		CLR(bp->b_flags, B_ERROR);

d531 8
a538 12
		if (ISSET(bp->b_flags, B_LOCKED))
			/* locked in core */
			bufq = &bufqueues[BQ_LOCKED];
		else {
			numfreepages += btoc(bp->b_bufsize);
			if (!ISSET(bp->b_flags, B_DELWRI)) {
				numcleanpages += btoc(bp->b_bufsize);
				bufq = &bufqueues[BQ_CLEAN];
			} else {
				numdirtypages += btoc(bp->b_bufsize);
				bufq = &bufqueues[BQ_DIRTY];
			}
d993 1
a993 1
	static char *bname[BQUEUES] = { "LOCKED", "CLEAN", "DIRTY", "EMPTY" };
@


1.85
log
@Use daddr64_t for logical blocks, okay krw@@ thib@@ mickey@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.84 2006/08/28 16:15:29 tom Exp $	*/
d411 1
d416 9
d461 1
@


1.84
log
@Another grammar nit; "please go ahead" jmc@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.83 2006/08/28 12:48:53 jmc Exp $	*/
d104 1
a104 1
static __inline struct buf *bio_doread(struct vnode *, daddr_t, int, int);
d235 1
a235 1
bio_doread(struct vnode *vp, daddr_t blkno, int size, int async)
d264 1
a264 1
bread(struct vnode *vp, daddr_t blkno, int size, struct ucred *cred,
d281 1
a281 1
breadn(struct vnode *vp, daddr_t blkno, int size, daddr_t rablks[],
d574 1
a574 1
incore(struct vnode *vp, daddr_t blkno)
d597 1
a597 1
getblk(struct vnode *vp, daddr_t blkno, int size, int slpflag, int slptimeo)
@


1.83
log
@typos; from tbert
(one not taken)
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.82 2006/08/17 13:55:57 mickey Exp $	*/
d219 1
a219 1
	 * there is no reason to set it more than 1-2 MB.
@


1.82
log
@chose proper mountpoint in making decision on upgrading
bwrte() to async op. this fixes in particular panics w/
softdep ffs mounted at a dir in an async mounted (mfs)
file system and also a few other evil scenarios.
this also matches a netbsd change 1.76 .
tested by many on many archs; pedro@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.81 2006/08/09 12:00:03 pedro Exp $	*/
d219 1
a219 1
	 * there are no reason to set it more than 1-2 MB.
d242 1
a242 1
	 * If buffer does not have data valid, start a read.
d593 1
a593 1
 * correct size. It is up to the caller to insure that the
d680 1
a680 1
 * start a write.  If the buffer grows, it's the callers
@


1.81
log
@Propagate error on getnewbuf(), effectively allowing callers to be
notified of posted signals for the current process, okay tedu@@.
Based on initial diagnosis by R. Tyler Ballance <tyler@@bleepsoft.com>
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.80 2006/04/24 15:08:48 pedro Exp $	*/
d315 6
d329 1
a329 2
	if (!async && bp->b_vp && bp->b_vp->v_mount &&
	    ISSET(bp->b_vp->v_mount->mnt_flag, MNT_ASYNC)) {
d339 3
a341 3
	if ((vp = bp->b_vp) != NULL) {
		if (vp->v_type == VBLK)
			mp = vp->v_specmountpoint;
d343 1
a343 7
			mp = vp->v_mount;
		if (mp != NULL) {
			if (async)
				mp->mnt_stat.f_asyncwrites++;
			else
				mp->mnt_stat.f_syncwrites++;
		}
@


1.80
log
@Use NULL where NULL is meant, from thib, no binary change
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.79 2005/11/06 13:07:47 pedro Exp $	*/
d105 1
a105 1
struct buf *getnewbuf(int slpflag, int slptimeo);
d602 1
a602 1
	int s, err;
d623 1
a623 1
			err = tsleep(bp, slpflag | (PRIBIO + 1), "getblk",
d626 1
a626 1
			if (err)
d641 4
a644 1
		if ((bp = getnewbuf(slpflag, slptimeo)) == NULL)
d646 2
d667 1
a667 1
	while ((bp = getnewbuf(0, 0)) == NULL)
d707 1
a707 1
		while ((nbp = getnewbuf(0, 0)) == NULL)
d768 1
a768 1
getnewbuf(int slpflag, int slptimeo)
d771 1
a771 1
	int s;
d783 2
a784 1
		tsleep(&needbuffer, slpflag|(PRIBIO+1), "getnewbuf", slptimeo);
d786 2
d790 3
a792 1
	if ((bp = TAILQ_FIRST(&bufqueues[BQ_CLEAN])) == NULL) {
d795 2
a796 1
		tsleep(&nobuffers, slpflag|(PRIBIO-3), "getnewbuf", slptimeo);
d798 2
@


1.79
log
@Use ANSI-style function declarations, no binary change, okay jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.78 2005/10/08 16:36:23 pedro Exp $	*/
d662 1
a662 1
	while ((bp = getnewbuf(0, 0)) == 0)
d780 1
a780 1
		return (0);
d787 1
a787 1
		return (0);
d816 1
a816 1
	bp->b_iodone = 0;
@


1.78
log
@Revert revision 1.39. Under heavy load, it could cause severe corruption
in the buffer lists by removing a buffer from the hash twice. Problem
identified in discussion with Alexander Bluhm <Alexander_Bluhm@@genua.de>.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.77 2005/06/27 22:08:39 pedro Exp $	*/
d970 1
a970 1
vfs_bufstats()
@


1.77
log
@When releasing a buffer and disassociating it from its vnode, there's
no point in placing the buffer in the vnode's clean list just to remove
it afterwards. Talked over with art@@, various testing for a while.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.76 2005/06/17 16:45:02 pedro Exp $	*/
d105 1
a105 1
int getnewbuf(int slpflag, int slptimeo, struct buf **);
d601 1
a601 1
	struct buf *bp, *nbp = NULL;
d641 1
a641 1
		if (nbp == NULL && getnewbuf(slpflag, slptimeo, &nbp) != 0) {
a642 2
		}
		bp = nbp;
a647 7
	} else if (nbp != NULL) {
		/*
		 * Set B_AGE so that buffer appear at BQ_CLEAN head
		 * and gets reused ASAP.
		 */
		SET(nbp->b_flags, B_AGE);
		brelse(nbp);
d662 2
a663 1
	getnewbuf(0, 0, &bp);
d702 2
a703 1
		getnewbuf(0, 0, &nbp);
a760 4
 *
 * We must notify getblk if we slept during the buffer allocation. When
 * that happens, we allocate a buffer anyway (unless tsleep is interrupted
 * or times out) and return !0.
d762 2
a763 2
int
getnewbuf(int slpflag, int slptimeo, struct buf **bpp)
d766 1
a766 1
	int s, ret, error;
a767 4
	*bpp = NULL;
	ret = 0;

start:
d778 1
a778 2
		error = tsleep(&needbuffer, slpflag|(PRIBIO+1), "getnewbuf",
				slptimeo);
d780 1
a780 4
		if (error)
			return (1);
		ret = 1;
		goto start;
d785 1
a785 2
		error = tsleep(&nobuffers, slpflag|(PRIBIO-3),
				"getnewbuf", slptimeo);
d787 1
a787 4
		if (error)
			return (1);
		ret = 1;
		goto start;
d824 1
a824 2
	*bpp = bp;
	return (ret);
@


1.77.2.1
log
@MFC:
Fix by pedro@@

Revert revision 1.39. Under heavy load, it could cause severe corruption
in the buffer lists by removing a buffer from the hash twice. Problem
identified in discussion with Alexander Bluhm <Alexander_Bluhm@@genua.de>.

ok deraadt@@ pedro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.77 2005/06/27 22:08:39 pedro Exp $	*/
d105 1
a105 1
struct buf *getnewbuf(int slpflag, int slptimeo);
d601 1
a601 1
	struct buf *bp;
d641 1
a641 1
		if ((bp = getnewbuf(slpflag, slptimeo)) == NULL)
d643 2
d650 7
d671 1
a671 2
	while ((bp = getnewbuf(0, 0)) == 0)
		;
d710 1
a710 2
		while ((nbp = getnewbuf(0, 0)) == NULL)
			;
d768 4
d773 2
a774 2
struct buf *
getnewbuf(int slpflag, int slptimeo)
d777 4
a780 1
	int s;
d782 1
d793 2
a794 1
		tsleep(&needbuffer, slpflag|(PRIBIO+1), "getnewbuf", slptimeo);
d796 4
a799 1
		return (0);
d804 2
a805 1
		tsleep(&nobuffers, slpflag|(PRIBIO-3), "getnewbuf", slptimeo);
d807 4
a810 1
		return (0);
d847 2
a848 1
	return (bp);
@


1.76
log
@Protect buf_countdeps() call in buf_daemon() with splbio(), okay art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.75 2004/12/26 21:22:13 miod Exp $	*/
d507 1
a507 2
		if (bp->b_vp) {
			reassignbuf(bp);
d509 1
a509 1
		}
@


1.75
log
@Use list and queue macros where applicable to make the code easier to read;
no change in compiler assembly output.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.74 2004/12/11 14:26:31 pedro Exp $	*/
d889 1
a889 1
			    buf_countdeps(bp, 0, 1)) {
@


1.75.2.1
log
@MFC:
Fix by pedro@@

Revert revision 1.39. Under heavy load, it could cause severe corruption
in the buffer lists by removing a buffer from the hash twice. Problem
identified in discussion with Alexander Bluhm <Alexander_Bluhm@@genua.de>.

ok deraadt@@ pedro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.75 2004/12/26 21:22:13 miod Exp $	*/
d105 1
a105 1
struct buf *getnewbuf(int slpflag, int slptimeo);
d602 1
a602 1
	struct buf *bp;
d642 1
a642 1
		if ((bp = getnewbuf(slpflag, slptimeo)) == NULL)
d644 2
d651 7
d672 1
a672 2
	while ((bp = getnewbuf(0, 0)) == 0)
		;
d711 1
a711 2
		while ((nbp = getnewbuf(0, 0)) == NULL)
			;
d769 4
d774 2
a775 2
struct buf *
getnewbuf(int slpflag, int slptimeo)
d778 4
a781 1
	int s;
d783 1
d794 2
a795 1
		tsleep(&needbuffer, slpflag|(PRIBIO+1), "getnewbuf", slptimeo);
d797 4
a800 1
		return (0);
d805 2
a806 1
		tsleep(&nobuffers, slpflag|(PRIBIO-3), "getnewbuf", slptimeo);
d808 4
a811 1
		return (0);
d848 2
a849 1
	return (bp);
@


1.74
log
@match comments with reality and use 'null' for pointers in incore()
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.73 2004/12/05 04:42:42 jsg Exp $	*/
d146 1
a146 1
	if (bp->b_freelist.tqe_next == NULL) {
d148 1
a148 1
			if (dp->tqh_last == &bp->b_freelist.tqe_next)
d742 1
a742 1
		if ((nbp = bufqueues[BQ_EMPTY].tqh_first) == NULL) {
d1012 1
a1012 1
		for (bp = dp->tqh_first; bp; bp = bp->b_freelist.tqe_next) {
@


1.73
log
@less then -> less than
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.72 2004/11/30 12:39:43 pedro Exp $	*/
d572 2
a573 5
 * Determine if a block is in the cache.
 * Just look on what would be its hash chain.  If it's there, return
 * a pointer to it, unless it's marked invalid.  If it's marked invalid,
 * we normally don't return the buffer, unless the caller explicitly
 * wants us to.
d587 1
a587 1
	return (0);
@


1.72
log
@kill breada(), from Sven Dehmlow. ok tedu@@ millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.71 2004/11/01 15:55:38 pedro Exp $	*/
d217 1
a217 1
	 * not less then 2 * MAXBSIZE. locleanpages
@


1.71
log
@english
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.70 2004/09/20 10:56:51 pedro Exp $	*/
a302 13
}

/*
 * Read with single-block read-ahead.  Defined in Bach (p.55), but
 * implemented as a call to breadn().
 * XXX for compatibility with old file systems.
 */
int
breada(struct vnode *vp, daddr_t blkno, int size, daddr_t rablkno, int rabsize,
    struct ucred *cred, struct buf **bpp)
{

	return (breadn(vp, blkno, size, &rablkno, &rabsize, 1, cred, bpp));	
@


1.70
log
@spl dance to protect buffer flags in a sensitive context
ok millert@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.69 2004/06/24 19:35:24 tholo Exp $	*/
d244 1
a244 1
	 * Therefore, it's valid if it's I/O has completed or been delayed.
@


1.69
log
@This moves access to wall and uptime variables in MI code,
encapsulating all such access into wall-defined functions
that makes sure locking is done as needed.

It also cleans up some uses of wall time vs. uptime some
places, but there is sure to be more of these needed as
well, particularily in MD code.  Also, many current calls
to microtime() should probably be changed to getmicrotime(),
or to the {,get}microuptime() versions.

ok art@@ deraadt@@ aaron@@ matthieu@@ beck@@ sturm@@ millert@@ others
"Oh, that is not your problem!" from miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.68 2003/06/02 23:28:07 millert Exp $	*/
a574 2
	splx(s);

d580 2
@


1.69.2.1
log
@MFC:
Fix by pedro@@

Revert revision 1.39. Under heavy load, it could cause severe corruption
in the buffer lists by removing a buffer from the hash twice. Problem
identified in discussion with Alexander Bluhm <Alexander_Bluhm@@genua.de>.

ok deraadt@@ pedro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.69 2004/06/24 19:35:24 tholo Exp $	*/
d105 1
a105 1
struct buf *getnewbuf(int slpflag, int slptimeo);
d618 1
a618 1
	struct buf *bp;
d658 1
a658 1
		if ((bp = getnewbuf(slpflag, slptimeo)) == NULL)
d660 2
d667 7
d688 1
a688 2
	while ((bp = getnewbuf(0, 0)) == 0)
		;
d727 1
a727 2
		while ((nbp = getnewbuf(0, 0)) == NULL)
			;
d785 4
d790 2
a791 2
struct buf *
getnewbuf(int slpflag, int slptimeo)
d794 4
a797 1
	int s;
d799 1
d810 2
a811 1
		tsleep(&needbuffer, slpflag|(PRIBIO+1), "getnewbuf", slptimeo);
d813 4
a816 1
		return (0);
d821 2
a822 1
		tsleep(&nobuffers, slpflag|(PRIBIO-3), "getnewbuf", slptimeo);
d824 4
a827 1
		return (0);
d864 2
a865 1
	return (bp);
@


1.68
log
@Remove the advertising clause in the UCB license which Berkeley
rescinded 22 July 1999.  Proofed by myself and Theo.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.67 2003/05/13 17:47:15 deraadt Exp $	*/
d885 1
a885 1
		starttime = time;
d888 2
d920 2
a921 1
			timersub(&time, &starttime, &timediff);
@


1.67
log
@Fix the tree.  Jason, you were such a rock star yesterday, but it obviously
blinded you to the fact you were breaking ALL of our install media!
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.65 2002/06/09 04:34:12 art Exp $	*/
d22 1
a22 5
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.66
log
@Kill a bunch more commons (very few left =)
@
text
@d95 1
a95 1
extern struct bio_ops bioops;
@


1.65
log
@Use LIST_FOREACH.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.64 2002/06/09 01:11:18 art Exp $	*/
d95 1
a95 1
struct bio_ops bioops;
@


1.64
log
@ANSIfy (started as some other change that just ran away).
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.63 2002/05/24 14:06:34 art Exp $	*/
a599 2
	bp = BUFHASH(vp, blkno)->lh_first;

d601 1
a601 1
	for (; bp != NULL; bp = bp->b_hash.le_next) {
d637 1
a637 2
	bp = bh->lh_first;
	for (; bp != NULL; bp = bp->b_hash.le_next) {
@


1.63
log
@protect more of the bookkeeping variables with splbio.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.62 2002/05/24 13:59:33 art Exp $	*/
d139 1
a139 2
bremfree(bp)
	struct buf *bp;
d178 1
a178 1
bufinit()
d180 1
a180 1
	register struct buf *bp;
d182 1
a182 1
	register int i;
d239 1
a239 5
bio_doread(vp, blkno, size, async)
	struct vnode *vp;
	daddr_t blkno;
	int size;
	int async;
d241 1
a241 1
	register struct buf *bp;
d268 2
a269 6
bread(vp, blkno, size, cred, bpp)
	struct vnode *vp;
	daddr_t blkno;
	int size;
	struct ucred *cred;
	struct buf **bpp;
d271 1
a271 1
	register struct buf *bp;
d285 2
a286 7
breadn(vp, blkno, size, rablks, rasizes, nrablks, cred, bpp)
	struct vnode *vp;
	daddr_t blkno; int size;
	daddr_t rablks[]; int rasizes[];
	int nrablks;
	struct ucred *cred;
	struct buf **bpp;
d288 1
a288 1
	register struct buf *bp;
d315 2
a316 6
breada(vp, blkno, size, rablkno, rabsize, cred, bpp)
	struct vnode *vp;
	daddr_t blkno; int size;
	daddr_t rablkno; int rabsize;
	struct ucred *cred;
	struct buf **bpp;
d326 1
a326 2
bwrite(bp)
	struct buf *bp;
d416 1
a416 2
bdwrite(bp)
	struct buf *bp;
d452 1
a452 2
bawrite(bp)
	struct buf *bp;
d463 1
a463 2
buf_dirty(bp)
	struct buf *bp;
d477 1
a477 2
buf_undirty(bp)
	struct buf *bp;
d492 1
a492 2
brelse(bp)
	struct buf *bp;
d596 1
a596 3
incore(vp, blkno)
	struct vnode *vp;
	daddr_t blkno;
d621 1
a621 4
getblk(vp, blkno, size, slpflag, slptimeo)
	register struct vnode *vp;
	daddr_t blkno;
	int size, slpflag, slptimeo;
d691 1
a691 2
geteblk(size)
	int size;
d712 1
a712 3
allocbuf(bp, size)
	struct buf *bp;
	int size;
d798 1
a798 3
getnewbuf(slpflag, slptimeo, bpp)
	int slpflag, slptimeo;
	struct buf **bpp;
d939 1
a939 2
biowait(bp)
	struct buf *bp;
d979 1
a979 2
biodone(bp)
	struct buf *bp;
@


1.62
log
@typo
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.61 2002/05/22 00:22:06 art Exp $	*/
a589 2
	/* Allow disk interrupts. */
	splx(s);
d602 2
@


1.61
log
@some b_iodone handlers and vwakeup require splbio.
Mark biodone with splassert(IPL_BIO).
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.60 2002/05/16 00:03:05 art Exp $	*/
d605 1
a605 1
	/* Wake up any proceeses waiting for _this_ buffer to become free. */
@


1.60
log
@sprinkle some splassert(IPL_BIO) in some functions that are commented as "should be called at splbio()"
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.59 2002/04/27 15:29:30 art Exp $	*/
d1017 2
@


1.59
log
@Comment biodone as "must be called at splbio".
It calls vwakeup and vwakeup is marked as "must be at splbio".
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.58 2002/03/14 01:27:06 millert Exp $	*/
d487 2
d502 2
@


1.58
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.57 2002/01/30 20:45:35 nordin Exp $	*/
d1006 2
@


1.57
log
@Move SET/CLR/ISSET macros to param.h. fgsch@@ and millert@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.56 2002/01/23 00:39:48 art Exp $	*/
d108 2
a109 2
static __inline struct buf *bio_doread __P((struct vnode *, daddr_t, int, int));
int getnewbuf __P((int slpflag, int slptimeo, struct buf **));
@


1.56
log
@Pool deals fairly well with physical memory shortage, but it doesn't deal
well (not at all) with shortages of the vm_map where the pages are mapped
(usually kmem_map).

Try to deal with it:
 - group all information the backend allocator for a pool in a separate
   struct. The pool will only have a pointer to that struct.
 - change the pool_init API to reflect that.
 - link all pools allocating from the same allocator on a linked list.
 - Since an allocator is responsible to wait for physical memory it will
   only fail (waitok) when it runs out of its backing vm_map, carefully
   drain pools using the same allocator so that va space is freed.
   (see comments in code for caveats and details).
 - change pool_reclaim to return if it actually succeeded to free some
   memory, use that information to make draining easier and more efficient.
 - get rid of PR_URGENT, noone uses it.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.55 2001/12/19 08:58:06 art Exp $	*/
a66 5

/* Macros to clear/set/test flags. */
#define	SET(t, f)	(t) |= (f)
#define	CLR(t, f)	(t) &= ~(f)
#define	ISSET(t, f)	((t) & (f))
@


1.55
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.51 2001/11/15 23:25:37 art Exp $	*/
d191 1
a191 2
	pool_init(&bufpool, sizeof(struct buf), 0, 0, 0, "bufpl", 0,
	    NULL, NULL, M_DEVBUF);
@


1.54
log
@Call buf_cleanout, which handles wakeups
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.53 2001/11/27 06:21:37 art Exp $	*/
d330 17
d409 1
d469 1
a590 1
	SET(bp->b_flags, B_CACHE);
d654 1
d658 11
d670 5
a674 2
	bp = incore(vp, blkno);
	if (bp != NULL) {
d681 1
a681 5
			if (err) {
				if (nbp != NULL) {
					SET(nbp->b_flags, B_AGE);
					brelse(nbp);
				}
a682 1
			}
d686 6
a691 2
		SET(bp->b_flags, (B_BUSY | B_CACHE));
		bremfree(bp);
d700 1
a700 1
		binshash(bp, BUFHASH(vp, blkno));
d903 2
d1025 1
a1100 13

int
buf_cleanout(struct buf *bp) {
	if (bp->b_vp != NULL)
		brelvp(bp);

	if (bp->b_flags & B_WANTED) {
		bp->b_flags &= ~B_WANTED;
		wakeup(bp);
	}

	return (0);
}
@


1.54.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.57 2002/01/30 20:45:35 nordin Exp $	*/
d68 5
d191 2
a192 1
	pool_init(&bufpool, sizeof(struct buf), 0, 0, 0, "bufpl", NULL);
@


1.54.2.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.54.2.1 2002/01/31 22:55:41 niklas Exp $	*/
d108 2
a109 2
static __inline struct buf *bio_doread(struct vnode *, daddr_t, int, int);
int getnewbuf(int slpflag, int slptimeo, struct buf **);
d139 2
a140 1
bremfree(struct buf *bp)
d179 1
a179 1
bufinit(void)
d181 1
a181 1
	struct buf *bp;
d183 1
a183 1
	int i;
d240 5
a244 1
bio_doread(struct vnode *vp, daddr_t blkno, int size, int async)
d246 1
a246 1
	struct buf *bp;
d273 6
a278 2
bread(struct vnode *vp, daddr_t blkno, int size, struct ucred *cred,
    struct buf **bpp)
d280 1
a280 1
	struct buf *bp;
d294 7
a300 2
breadn(struct vnode *vp, daddr_t blkno, int size, daddr_t rablks[],
    int rasizes[], int nrablks, struct ucred *cred, struct buf **bpp)
d302 1
a302 1
	struct buf *bp;
d327 2
a328 1
bwrite(struct buf *bp)
d417 2
a418 1
bdwrite(struct buf *bp)
d453 2
a454 1
bawrite(struct buf *bp)
d465 2
a466 1
buf_dirty(struct buf *bp)
a467 2
	splassert(IPL_BIO);

d478 2
a479 1
buf_undirty(struct buf *bp)
a480 2
	splassert(IPL_BIO);

d492 2
a493 1
brelse(struct buf *bp)
d568 2
d583 1
a583 3
	splx(s);

	/* Wake up any processes waiting for _this_ buffer to become free. */
d598 3
a600 1
incore(struct vnode *vp, daddr_t blkno)
d604 2
d607 1
a607 1
	LIST_FOREACH(bp, BUFHASH(vp, blkno), b_hash) {
d625 4
a628 1
getblk(struct vnode *vp, daddr_t blkno, int size, int slpflag, int slptimeo)
d684 2
a685 1
geteblk(int size)
d706 3
a708 1
allocbuf(struct buf *bp, int size)
d794 3
a796 1
getnewbuf(int slpflag, int slptimeo, struct buf **bpp)
d935 2
a936 1
biowait(struct buf *bp)
a971 2
 *
 * Must be called at splbio().
d974 2
a975 1
biodone(struct buf *bp)
a976 2
	splassert(IPL_BIO);

@


1.54.2.3
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.54.2.2 2002/06/11 03:29:40 art Exp $	*/
a94 1
#ifndef FFS_SOFTDEP
a95 1
#endif
d912 1
a912 1
	while (!ISSET(bp->b_flags, B_DONE/* | B_DELWRI*/))
@


1.53
log
@kill breada
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.52 2001/11/27 05:27:11 art Exp $	*/
d1066 13
@


1.52
log
@Merge in the unified buffer cache code as found in NetBSD 2001/03/10. The
code is written mostly by Chuck Silvers <chuq@@chuq.com>/<chs@@netbsd.org>.

Tested for the past few weeks by many developers, should be in a pretty stable
state, but will require optimizations and additional cleanups.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.51 2001/11/15 23:25:37 art Exp $	*/
a326 17
}

/*
 * Read with single-block read-ahead.  Defined in Bach (p.55), but
 * implemented as a call to breadn().
 * XXX for compatibility with old file systems.
 */
int
breada(vp, blkno, size, rablkno, rabsize, cred, bpp)
	struct vnode *vp;
	daddr_t blkno; int size;
	daddr_t rablkno; int rabsize;
	struct ucred *cred;
	struct buf **bpp;
{

	return (breadn(vp, blkno, size, &rablkno, &rabsize, 1, cred, bpp));	
@


1.51
log
@bio_doread doesn't need a cred anymore
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.50 2001/11/15 23:15:15 art Exp $	*/
a408 1
	SET(bp->b_flags, B_WRITEINPROG);
a467 1
	CLR(bp->b_flags, B_NEEDCOMMIT);
d589 1
a652 1
	struct bufhashhdr *bh;
a655 11
	/*
	 * XXX
	 * The following is an inlined version of 'incore()', but with
	 * the 'invalid' test moved to after the 'busy' test.  It's
	 * necessary because there are some cases in which the NFS
	 * code sets B_INVAL prior to writing data to the server, but
	 * in which the buffers actually contain valid data.  In this
	 * case, we can't allow the system to allocate a new buffer for
	 * the block until the write is finished.
	 */
	bh = BUFHASH(vp, blkno);
d657 2
a658 5
	bp = bh->lh_first;
	for (; bp != NULL; bp = bp->b_hash.le_next) {
		if (bp->b_lblkno != blkno || bp->b_vp != vp)
			continue;

d665 5
a669 1
			if (err)
d671 1
d675 2
a676 6
		if (!ISSET(bp->b_flags, B_INVAL)) {
			SET(bp->b_flags, (B_BUSY | B_CACHE));
			bremfree(bp);
			splx(s);
			break;
		}
d685 1
a685 1
		binshash(bp, bh);
a887 2
	bp->b_dirtyoff = bp->b_dirtyend = 0;
	bp->b_validoff = bp->b_validend = 0;
a1007 1
		CLR(bp->b_flags, B_WRITEINPROG);
@


1.50
log
@Remove creds from struct buf, move the creds that nfs need into the nfs node.
While in the area, convert nfs node allocation from malloc to pool and do
some cleanups.
Based on the UBC changes in NetBSD. niklas@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.49 2001/11/09 15:32:22 art Exp $	*/
d113 1
a113 2
static __inline struct buf *bio_doread __P((struct vnode *, daddr_t, int,
					    struct ucred *, int));
d246 1
a246 1
bio_doread(vp, blkno, size, cred, async)
a249 1
	struct ucred *cred;
a261 1
		/* Start I/O for the buffer (keeping credentials). */
d289 1
a289 1
	bp = *bpp = bio_doread(vp, blkno, size, cred, 0);
d311 1
a311 1
	bp = *bpp = bio_doread(vp, blkno, size, cred, 0);
d322 1
a322 1
		(void) bio_doread(vp, rablks[i], rasizes[i], cred, B_ASYNC);
@


1.49
log
@Create bufpool - a pool of struct bufs.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.48 2001/11/06 19:53:20 miod Exp $	*/
a202 2
		bp->b_rcred = NOCRED;
		bp->b_wcred = NOCRED;
a265 4
		if (cred != NOCRED && bp->b_rcred == NOCRED) {
			crhold(cred);
			bp->b_rcred = cred;
		}
a907 10

	/* nuke any credentials we were holding */
	if (bp->b_rcred != NOCRED) {
		crfree(bp->b_rcred);
		bp->b_rcred = NOCRED;
	}
	if (bp->b_wcred != NOCRED) {
		crfree(bp->b_wcred);
		bp->b_wcred = NOCRED;
	}
@


1.48
log
@Replace inclusion of <vm/foo.h> with the correct <uvm/bar.h> when necessary.
(Look ma, I might have broken the tree)
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.47 2001/10/28 19:19:00 deraadt Exp $	*/
d59 1
d103 5
d192 2
@


1.47
log
@we try to test things before commit, art
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.45 2001/10/11 14:44:10 art Exp $	*/
d63 1
a63 1
#include <vm/vm.h>
@


1.46
log
@Sprinkle some assertions in the buffer code.
Add a new DEBUG function "buf_print" that prints the contents of struct buf.
@
text
@a526 2
	KASSERT(ISSET(bp->b_flags, B_BUSY));

a759 1
	KASSERT(!ISSET(bp->b_flags, (B_CACHE|B_DONE|B_DELWRI)));
d800 1
a800 1
		if ((nbp = TAILQ_FIRST(&bufqueues[BQ_EMPTY])) == NULL) {
a875 2
	KASSERT(!ISSET(bp->b_flags, B_BUSY));

d1044 1
a1044 2
		if (ISSET(bp->b_flags, B_ASYNC)) {
			/* if async, release it */
d1046 1
a1046 2
		} else {
			/* or just wakeup the buffer */
a1053 57
/*
 * Print as much as possible about a buffer.
 */
void
buf_print(struct buf *bp)
{
	int f = 1;

#define FLAG(fl) if (bp->b_flags & (fl)) { if (!f) printf(", "); else f = 0; printf(#fl); }

	printf("b_proc: %p\n",		bp->b_proc);
	printf("b_error: %d\n",		bp->b_error);
	printf("b_bufsize: %ld\n",	bp->b_bufsize);
	printf("b_bcount: %ld\n",	bp->b_bcount);
	printf("b_resid: %ld\n",	(long)bp->b_resid);
	printf("b_dev: (%d,%d)\n",	minor(bp->b_dev), major(bp->b_dev));
	printf("b_data: %p\n",		bp->b_data);
	printf("b_blkno: %ld\n",	(long)bp->b_blkno);
	printf("b_lblkno: %ld\n",	(long)bp->b_lblkno);

	printf("b_flags: ");

	FLAG(B_AGE);
	FLAG(B_NEEDCOMMIT);
	FLAG(B_ASYNC);
	FLAG(B_BAD);
	FLAG(B_BUSY);
	FLAG(B_CACHE);
	FLAG(B_CALL);
	FLAG(B_DELWRI);
	FLAG(B_DIRTY);
	FLAG(B_DONE);
	FLAG(B_EINTR);
	FLAG(B_ERROR);
	FLAG(B_GATHERED);
	FLAG(B_INVAL);
	FLAG(B_LOCKED);
	FLAG(B_NOCACHE);
	FLAG(B_PAGET);
	FLAG(B_PGIN);
	FLAG(B_PHYS);
	FLAG(B_RAW);
	FLAG(B_READ);
	FLAG(B_TAPE);
	FLAG(B_UAREA);
	FLAG(B_WANTED);
	FLAG(B_WRITE);	
	FLAG(B_WRITEINPROG);
	FLAG(B_XXX);
	FLAG(B_DEFERRED);
	FLAG(B_SCANNED);

	printf("\n");

#undef FLAG
}

@


1.45
log
@Fix flawed logic when deciding if we should sleep when
we are below the low watermark or if we should try to use up all buffers.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.44 2001/10/11 08:07:12 gluk Exp $	*/
d527 2
d762 1
d803 1
a803 1
		if ((nbp = bufqueues[BQ_EMPTY].tqh_first) == NULL) {
d879 2
d1049 2
a1050 1
		if (ISSET(bp->b_flags, B_ASYNC)) {/* if async, release it */
d1052 2
a1053 1
		} else {			/* or just wakeup the buffer */
d1061 57
@


1.44
log
@Count pages not buffers. This fixes deadlock condition which mainly
occurs on the fs with large block size. We can have situation where
numcleanbufs < locleanbufs and numdirtybufs < hidirtybufs.  So, buffer
flushing daemon never wakeups and other processes asleep forever waiting
for a clean buffers. We count pages only for the dirty buffers which are
on freelist(BQ_DIRTY).

niklas@@ found this.

Rename flasher to cleaner. Suggested by costa@@.

Discussed with niklas@@, costa@@, millert@@, art@@.
Ok deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.43 2001/09/20 08:22:26 gluk Exp $	*/
d851 2
a852 2
	if ((numcleanpages <= locleanpages)
	    && (curproc != syncerproc || curproc != cleanerproc)) {
@


1.43
log
@Get rid of B_VFLUSH. Tested by costa and me.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.42 2001/09/19 18:05:27 art Exp $	*/
d114 16
a129 14
 *  numdirtybufs - number of all dirty (B_DELWRI) buffers.
 *  lodirtybufs  - low water mark for buffer flushing daemon.
 *  hidirtybufs  - high water mark for buffer flushing daemon.
 *  numfreebufs  - number of buffers on BQ_CLEAN and BQ_DIRTY. unused.
 *  numcleanbufs - number of clean (!B_DELWRI) buffers on BQ_CLEAN.
 *    Used to track the need to speedup the flusher and for the syncer reserve.
 *  numemptybufs - number of buffers on BQ_EMPTY. unused.
 *  mincleanbufs - the lowest number of clean buffers this far.
 */
int numdirtybufs;
int lodirtybufs;
int hidirtybufs;
int numfreebufs;
int numcleanbufs;
a130 1
int locleanbufs;
d132 1
a132 1
int mincleanbufs;
d135 2
a136 2
struct proc *flusherproc;
int bd_req;			/* Sleep point for flusher daemon. */
d161 1
a161 1
		numfreebufs--;
d163 1
a163 1
			numcleanbufs--;
d165 2
a166 2
			if (mincleanbufs > numcleanbufs)
				mincleanbufs = numcleanbufs;
d168 2
d207 2
a208 2
			numfreebufs++;
			numcleanbufs++;
d217 2
a218 3
	numdirtybufs = 0;
	hidirtybufs = nbuf / 4 + 20;
	lodirtybufs = hidirtybufs / 2;
d221 1
a221 1
	 * Reserve 5% of bufs for syncer's needs,
d223 1
a223 1
	 * not less then 16 bufs. locleanbufs
d225 1
a225 1
	 * there are no reason to set it more than 32.
d227 7
a233 5
	locleanbufs = nbuf / 20;
	if (locleanbufs < 16)
		locleanbufs = 16;
	if (locleanbufs > nbuf / 4)
		locleanbufs = nbuf / 4;
d236 1
a236 1
	mincleanbufs = locleanbufs;
a401 1
		--numdirtybufs;
a457 1
		++numdirtybufs;
a496 5
		++numdirtybufs;
#ifdef DIAGNOSTIC
		if (numdirtybufs > nbuf)
			panic("buf_dirty: incorrect number of dirty bufs");
#endif
a509 5
		--numdirtybufs;
#ifdef DIAGNOSTIC
		if (numdirtybufs < 0)
			panic("buf_undirty: incorrect number of dirty bufs");
#endif
a547 1
			--numdirtybufs;
d562 2
a563 2
			numfreebufs++;
			numcleanbufs++;
d575 1
a575 1
			numfreebufs++;
d577 1
a577 1
				numcleanbufs++;
d579 2
a580 1
			} else
d582 1
d596 1
a596 1
	/* Wake up syncer and flusher processes waiting for buffers */
d603 1
a603 1
	if (needbuffer && (numcleanbufs > locleanbufs)) {
d846 1
a846 1
	 * Wake up flusher if we're getting low on buffers.
d848 1
a848 1
	if (numdirtybufs >= hidirtybufs)
d851 2
a852 2
	if ((numcleanbufs <= locleanbufs)
	    && (curproc != syncerproc || curproc != flusherproc)) {
d923 1
a923 1
 * Buffer flushing daemon.
d932 1
a932 1
	flusherproc = curproc;
d935 2
a936 2
		if (numdirtybufs < hidirtybufs) {
			tsleep(&bd_req, PRIBIO - 7, "flusher", 0);
d960 2
a961 1
				++numfreebufs;
d969 1
a969 1
			if (numdirtybufs < lodirtybufs)
d1067 2
d1074 1
d1080 1
d1083 2
a1084 1
		printf("%s: total-%d", bname[i], count);
d1093 6
a1098 6
	if ((totals[BQ_CLEAN] + totals[BQ_DIRTY]) != numfreebufs)
		printf("numfreebufs counter wrong: %d != %d\n",
			numfreebufs, totals[BQ_CLEAN] + totals[BQ_DIRTY]);
	if (totals[BQ_CLEAN] != numcleanbufs)
		printf("numcleanbufs counter wrong: %d != %d\n",
			numcleanbufs, totals[BQ_CLEAN]);
d1100 4
a1103 4
		printf("numcleanbufs: %d\n", numcleanbufs);
	if (numdirtybufs < totals[BQ_DIRTY])
		printf("numdirtybufs counter wrong: %d < %d\n",
			numdirtybufs, totals[BQ_DIRTY]);
d1105 4
a1108 3
		printf("numdirtybufs: %d\n", numdirtybufs);
	printf("syncer eating up to %d bufs from %d reserved\n",
			locleanbufs - mincleanbufs, locleanbufs);
@


1.42
log
@No need for this complicated (and bug-prone) method for waking up the flusher.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.41 2001/09/17 19:17:30 gluk Exp $	*/
a546 14
	if (ISSET(bp->b_flags, B_VFLUSH)) {
		/*
		 * This is a delayed write buffer that was just flushed to
		 * disk.  It is still on the DIRTY queue.  If it's become
		 * invalid, then we need to move it to a different queue;
		 * If buffer was redirtied (because it has dependencies),
		 * leave it in its current position.
		 */
		CLR(bp->b_flags, B_VFLUSH);
		if (!ISSET(bp->b_flags, B_ERROR|B_INVAL|B_LOCKED|B_AGE))
			goto already_queued;
		bremfree(bp);
	}

a596 1
already_queued:
d936 1
a936 1
	struct buf *bp, *nbp;
d948 1
a948 4
		for (bp = TAILQ_FIRST(&bufqueues[BQ_DIRTY]); bp; bp = nbp) {
			nbp = TAILQ_NEXT(bp, b_freelist);
			if (ISSET(bp->b_flags, B_VFLUSH))
				continue;
d952 1
a952 4
#ifdef DIAGNOSTIC
			if (!ISSET(bp->b_flags, B_DELWRI))
				panic("Clean buffer on BQ_DIRTY");
#endif
d958 4
a961 1

a982 1
			nbp = TAILQ_FIRST(&bufqueues[BQ_DIRTY]);
@


1.41
log
@ The first implementation of the buffer flushing daemon. It solves our
problem when syncer can't do its work because of vnode locks (PR1983).
This also solves our problem where bigger number of buffers results in a
much worse perfomance. In my configuration (i386, 128mb, BUFCACHEPERCENT=35)
this speedup tar -xzf ports.tar.gz in 2-4 times. In configuration with
low number of buffers and softupdates this may slowdown some operations
up to 15%.

 The major difference with current buffer cache is that new implementation
uses separate queues for dirty and clean buffers. I.e. BQ_LRU and BQ_AGE
replaced by BQ_CLEAN and BQ_DIRTY. This simplifies things a lot and
doesn't affect perfomance in a bad manner.

Thanks to art and costa for pointing on errors.

Tested by brad, millert, naddy, art, jj, camield

art, millert ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.40 2001/09/10 22:05:38 gluk Exp $	*/
a109 1
void wakeup_flusher __P((void));
d135 1
a135 1
int bd_req;			/* 1 if buf_daemon already runnable */
d870 2
a871 2
	if (bd_req == 0 && numdirtybufs >= hidirtybufs)
		wakeup_flusher();
d958 1
a958 2
			bd_req = 0;
			tsleep(&flusherproc, PRIBIO - 7, "flusher", 0);
a1003 16
}

/*
 * Wakeup the buffer flushing daemon.
 */
void
wakeup_flusher()
{
	int s;

	s = splhigh();
	if (flusherproc && flusherproc->p_wchan != NULL) {
		setrunnable(flusherproc);
		bd_req = 1;
	}
	splx(s);
@


1.40
log
@remove useless debug function.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.39 2001/08/30 12:38:52 gluk Exp $	*/
d92 2
a93 2
#define	BQ_LRU		1		/* lru, useful buffers */
#define	BQ_AGE		2		/* rubbish */
d98 1
a98 1
int syncer_needbuffer;
d110 1
d115 6
a120 6
 *  numdirtybufs - number of dirty (B_DELWRI) buffers. unused.
 *  lodirtybufs  - ? unused.
 *  hidirtybufs  - ? unused.
 *  numfreebufs  - number of buffers on BQ_LRU and BQ_AGE. unused.
 *  numcleanbufs - number of clean (!B_DELWRI) buffers on BQ_LRU and BQ_AGE.
 *    Used to track the need to speedup the syncer and for the syncer reserve.
d124 6
a129 5
int numdirtybufs;	/* number of all dirty buffers */
int lodirtybufs, hidirtybufs;
int numfreebufs;	/* number of buffers on LRU+AGE free lists */
int numcleanbufs;	/* number of clean buffers on LRU+AGE free lists */
int numemptybufs;	/* number of buffers on EMPTY list */
d135 3
d204 1
a204 1
			dp = &bufqueues[BQ_AGE];
d215 1
a216 1
	numdirtybufs = 0;
d229 1
a229 1
	if (locleanbufs > nbuf/4)
d231 1
d551 1
a551 1
		 * disk.  It is still on the LRU queue.  If it's become
d553 2
a554 1
		 * otherwise leave it in its current position.
d559 1
a559 2
		else
			bremfree(bp);
d585 1
a585 1
			bufq = &bufqueues[BQ_AGE];
a593 4
		 * If buf is AGE, but has dependencies, must put it on last
		 * bufqueue to be scanned, ie LRU. This protects against the
		 * livelock where BQ_AGE only has buffers with dependencies,
		 * and we thus never get to the dependent buffers in BQ_LRU.
d600 1
a600 1
			if (!ISSET(bp->b_flags, B_DELWRI))
d602 3
a604 7
			if (ISSET(bp->b_flags, B_AGE))
				/* stale but valid data */
				bufq = buf_countdeps(bp, 0, 1) ?
				    &bufqueues[BQ_LRU] : &bufqueues[BQ_AGE];
			else
				/* valid data */
				bufq = &bufqueues[BQ_LRU];
d606 4
a609 1
		binstailfree(bp, bufq);
d614 1
a614 1
	CLR(bp->b_flags, (B_AGE | B_ASYNC | B_BUSY | B_NOCACHE));
d619 4
a622 4
	/* Wake up syncer process waiting for buffers */
	if (syncer_needbuffer) {
		wakeup(&syncer_needbuffer);
		syncer_needbuffer = 0;
d730 5
a849 2
 * Select something from a free list.
 * Preference is to AGE list, then LRU list.
d869 1
a869 1
	 * If we're getting low on buffers kick the syncer to work harder.
d871 2
a872 2
	if (numcleanbufs < locleanbufs + min(locleanbufs, 4))
		speedup_syncer();
d874 2
a875 2
	if ((numcleanbufs <= locleanbufs) && curproc != syncerproc) {
		/* wait for a free buffer of any kind */
d885 1
a885 2
	if ((bp = bufqueues[BQ_AGE].tqh_first) == NULL &&
	    (bp = bufqueues[BQ_LRU].tqh_first) == NULL) {
d887 2
a888 2
		syncer_needbuffer = 1;
		error = tsleep(&syncer_needbuffer, slpflag|(PRIBIO-3),
a898 12
	if (ISSET(bp->b_flags, B_VFLUSH)) {
		/*
		 * This is a delayed write buffer being flushed to disk.  Make
		 * sure it gets aged out of the queue when it's finished, and
		 * leave it off the LRU queue.
		 */
		CLR(bp->b_flags, B_VFLUSH);
		SET(bp->b_flags, B_AGE);
		splx(s);
		goto start;
	}

d902 4
a905 13
	/* If buffer was a delayed write, start it, and go back to the top. */
	if (ISSET(bp->b_flags, B_DELWRI)) {
		splx(s);
		/*
		 * This buffer has gone through the LRU, so make sure it gets
		 * reused ASAP.
		 */
		SET(bp->b_flags, B_AGE);
		bawrite(bp);
		/* bawrite can sleep, return 1 */
		ret = 1;
		goto start;
	}
d913 2
d916 2
a917 1
		buf_deallocate(bp);
d946 79
d1110 1
a1110 1
	static char *bname[BQUEUES] = { "LOCKED", "LRU", "AGE", "EMPTY" };
d1130 2
a1131 2
			totals[BQ_EMPTY], numemptybufs);
	if ((totals[BQ_LRU] + totals[BQ_AGE]) != numfreebufs)
d1133 11
a1143 6
			totals[BQ_LRU] + totals[BQ_AGE], numemptybufs);
	if ((totals[BQ_LRU] + totals[BQ_AGE]) < numcleanbufs ||
	    (numcleanbufs < 0))
		printf("numcleanbufs counter wrong: %d < %d\n",
			totals[BQ_LRU] + totals[BQ_AGE], numcleanbufs);
	printf("numcleanbufs: %d\n", numcleanbufs);
a1145 1
	printf("numdirtybufs: %d\n", numdirtybufs);
@


1.39
log
@Change getnewbuf interface so that getnewbuf always return
a new buffer and indicate if it sleep while getting that buffer.
This isn't make a much sense, but farther modifications will use it.

Work by art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.38 2001/05/05 20:57:01 art Exp $	*/
a108 1
int count_lock_queue __P((void));
a178 3
	numfreebufs = 0;
	numcleanbufs = 0;
	numemptybufs = 0;
a1030 17

#ifdef DEBUG
/*
 * Return a count of buffers on the "locked" queue.
 */
int
count_lock_queue()
{
	register struct buf *bp;
	register int n = 0;

	for (bp = bufqueues[BQ_LOCKED].tqh_first; bp;
	    bp = bp->b_freelist.tqe_next)
		n++;
	return (n);
}
#endif /* DEBUG */
@


1.38
log
@Get rid of CLSIZE and all related stuff.
CLSIZE -> 1
CLBYTES -> PAGE_SIZE
OLOFSET -> PAGE_MASK
etc.
At the same time some archs needed some cleaning in vmparam.h so that
goes in at the same time.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.37 2001/04/06 19:10:49 gluk Exp $	*/
d110 1
d682 1
a682 1
	struct buf *bp;
d723 1
a723 1
		if ((bp = getnewbuf(slpflag, slptimeo)) == NULL)
d725 2
d732 2
d749 1
a749 2
	while ((bp = getnewbuf(0, 0)) == 0)
		;
d790 1
a790 2
		while ((nbp = getnewbuf(0, 0)) == NULL)
			;
d850 4
d855 2
a856 2
struct buf *
getnewbuf(slpflag, slptimeo)
d858 1
d860 5
a864 2
	register struct buf *bp;
	int s;
d877 2
a878 1
		tsleep(&needbuffer, slpflag|(PRIBIO+1), "getnewbuf", slptimeo);
d880 4
a883 1
		return (0);
d889 2
a890 2
		tsleep(&syncer_needbuffer, slpflag|(PRIBIO-3), "getnewbuf",
			slptimeo);
d892 4
a895 1
		return (0);
d924 3
a926 1
		return (0);
d960 2
a961 1
	return (bp);
@


1.37
log
@Avoid a livelock problem where the buffer cache code would be
recycling B_AGE buffers with dependencies.

>From NetBSD.  costa@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.36 2001/03/30 10:30:26 art Exp $	*/
d197 1
a197 1
			bp->b_bufsize = (base + 1) * CLBYTES;
d199 1
a199 1
			bp->b_bufsize = base * CLBYTES;
d770 1
a770 1
	desired_size = clrnd(round_page(size));
d1044 1
a1044 1
	int counts[MAXBSIZE/CLBYTES+1];
d1051 1
a1051 1
		for (j = 0; j <= MAXBSIZE/CLBYTES; j++)
d1054 1
a1054 1
			counts[bp->b_bufsize/CLBYTES]++;
d1059 1
a1059 1
		for (j = 0; j <= MAXBSIZE/CLBYTES; j++)
d1061 1
a1061 1
				printf(", %d-%d", j * CLBYTES, counts[j]);
@


1.36
log
@
Avoid a 'thundering herd' problem when many processes wait for free buffers.
Just wakeup one process (there is a possible bug here that will be fixed
in the next round of cleanup).

Some misc cleanup, especially in the comments.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.35 2001/03/14 14:41:04 art Exp $	*/
d591 4
d605 2
a606 1
				bufq = &bufqueues[BQ_AGE];
@


1.35
log
@Slight cleanup.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.34 2001/03/13 16:47:50 gluk Exp $	*/
d111 12
a122 1
/* We are currently only using *cleanbufs, but count all num*bufs */
d616 1
a616 1
	/* Wake up syncer process waiting for any buffer to become free. */
d618 1
a619 1
		wakeup_one(&syncer_needbuffer);
a620 2
	if (numcleanbufs < locleanbufs + min(locleanbufs, 4))
		speedup_syncer();
d624 2
a625 2
		needbuffer = 0;
		wakeup(&needbuffer);
d726 1
d852 6
d860 1
a860 3
		if (needbuffer == 0)
			speedup_syncer();
		needbuffer = 1;
@


1.34
log
@Reserve some buffers for syncer daemon. This prevent deadlock in getblk
and getnewbuf. One process can sleep at "getnewbuf" waiting for a free
buffer and it may held buffer 'A' busy. Other processes can return buffers
on free lists, but they sleep on "getblk" waiting for buffer 'A'.

art@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.32 2001/02/27 09:13:51 csapuntz Exp $	*/
d97 2
a98 2
int needbuffer = 0;
int syncer_needbuffer = 0;
d111 1
a111 1
/* We are currently use only *cleanbufs, but count all num*bufs */
d208 1
a208 1
	 * but not more then 25% and if possible
d211 1
a211 1
	 * there are no reason to set it more then 32.
@


1.33
log
@Remove random trailing whitespace.
From gluk.
@
text
@d97 2
a98 1
int needbuffer;
d111 10
a120 2

int lodirtybufs, hidirtybufs, numdirtybufs;
d142 12
d168 3
d190 8
a197 1
		dp = bp->b_bufsize ? &bufqueues[BQ_AGE] : &bufqueues[BQ_EMPTY];
a203 1

d206 15
d483 4
d501 4
d565 1
a565 1
		if (bp->b_bufsize <= 0)
d568 2
a569 1
		else
d572 3
d584 11
a594 6
		else if (ISSET(bp->b_flags, B_AGE))
			/* stale but valid data */
			bufq = &bufqueues[BQ_AGE];
		else
			/* valid data */
			bufq = &bufqueues[BQ_LRU];
d605 8
d614 1
a614 1
	if (needbuffer) {
d682 3
a684 3
        bp = bh->lh_first;
        for (; bp != NULL; bp = bp->b_hash.le_next) {
                if (bp->b_lblkno != blkno || bp->b_vp != vp)
d705 1
a705 1
        }
d842 9
d854 3
a856 2
		needbuffer = 1;
		tsleep(&needbuffer, slpflag|(PRIBIO+1), "getnewbuf", slptimeo);
d919 1
a919 1
	
d996 1
d1011 1
d1026 1
d1029 1
a1033 1
		s = splbio();
d1038 1
a1038 1
		splx(s);
d1045 15
@


1.32
log
@

art@@ found a race in getnewbuf. bawrite can block so we need to restart
the whole buffer allocation process
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.31 2001/02/24 19:07:08 csapuntz Exp $	*/
d299 1
a299 1
	 * convert it to a delayed write.  
d605 1
a605 1
	 * code sets B_INVAL prior to writing data to the server, but 
d657 1
a657 1
	struct buf *bp; 
d761 1
a761 1
 * Preference is to AGE list, then LRU list.    
d779 1
a779 1
	} 
d833 1
a833 1
		bp->b_rcred = NOCRED; 
d841 1
a841 1
	return (bp); 
d863 1
a863 1
	} 
@


1.31
log
@

Cleanup of vnode interface continues. Get rid of VHOLD/HOLDRELE.
Change VM/UVM to use buf_replacevnode to change the vnode associated
with a buffer.

Addition v_bioflag for flags written in interrupt handlers
(and read at splbio, though not strictly necessary)

Add vwaitforio and use it instead of a while loop of v_numoutput.

Fix race conditions when manipulation vnode free list
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.30 2001/02/23 14:52:50 csapuntz Exp $	*/
d807 1
a807 1
		goto start;
@


1.30
log
@

Change the B_DELWRI flag using buf_dirty and buf_undirty instead of
manually twiddling it. This allows the buffer cache to more easily
keep track of dirty buffers and decide when it is appropriate to speed
up the syncer.

Insipired by FreeBSD.
Look over by art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.29 2001/02/21 23:24:30 csapuntz Exp $	*/
d341 1
a341 1
		reassignbuf(bp, bp->b_vp);
d396 1
a396 1
		reassignbuf(bp, bp->b_vp);
d436 1
a436 1
		reassignbuf(bp, bp->b_vp);
d450 1
a450 1
		reassignbuf(bp, bp->b_vp);
d509 1
a509 1
			reassignbuf(bp, bp->b_vp);
d898 4
a901 2
	if (!ISSET(bp->b_flags, B_READ))	/* wake up reader */
		vwakeup(bp);
@


1.29
log
@

Latest soft updates from FreeBSD/Kirk McKusick

Snapshot-related code has been commented out.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.28 2001/02/13 19:51:49 art Exp $	*/
d110 3
d170 6
d339 2
a340 1
	if (wasdelayed)
d342 1
a342 1
	else
d397 1
d427 3
d431 1
a431 1
bdirty(bp)
a433 3
	struct proc *p = curproc;       /* XXX */
	int s;

a435 1
		s = splbio();
d437 15
a451 3
		splx(s);
		if (p)
			p->p_stats->p_ru.ru_oublock++;
d503 5
a507 1
		CLR(bp->b_flags, B_DELWRI);
@


1.28
log
@Use MIN, not min when counting the pages we steal.
min is a function taking u_int arguments and we are counting signed longs
here.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.27 2000/06/23 02:14:38 mickey Exp $	*/
d478 3
a480 3
		if (LIST_FIRST(&bp->b_dep) != NULL && bioops.io_deallocate) {
			(*bioops.io_deallocate)(bp);
		}
d790 2
a791 2
	if (LIST_FIRST(&bp->b_dep) != NULL && bioops.io_deallocate)
		(*bioops.io_deallocate)(bp);
d869 2
a870 2
	if (LIST_FIRST(&bp->b_dep) != NULL && bioops.io_complete)
		(*bioops.io_complete)(bp);
@


1.27
log
@remove obsolete vtrace guts; art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.26 1999/12/05 08:09:01 art Exp $	*/
d681 1
a681 1
		amt = min(nbp->b_bufsize, (desired_size - bp->b_bufsize));
@


1.26
log
@Collect statistics on sync and async writes.
From NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.25 1999/12/02 20:55:47 art Exp $	*/
a57 1
#include <sys/trace.h>
@


1.26.2.1
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.37 2001/04/06 19:10:49 gluk Exp $	*/
d58 1
a98 1
int syncer_needbuffer;
a110 22
/*
 * We keep a few counters to monitor the utilization of the buffer cache
 *
 *  numdirtybufs - number of dirty (B_DELWRI) buffers. unused.
 *  lodirtybufs  - ? unused.
 *  hidirtybufs  - ? unused.
 *  numfreebufs  - number of buffers on BQ_LRU and BQ_AGE. unused.
 *  numcleanbufs - number of clean (!B_DELWRI) buffers on BQ_LRU and BQ_AGE.
 *    Used to track the need to speedup the syncer and for the syncer reserve.
 *  numemptybufs - number of buffers on BQ_EMPTY. unused.
 *  mincleanbufs - the lowest number of clean buffers this far.
 */
int numdirtybufs;	/* number of all dirty buffers */
int lodirtybufs, hidirtybufs;
int numfreebufs;	/* number of buffers on LRU+AGE free lists */
int numcleanbufs;	/* number of clean buffers on LRU+AGE free lists */
int numemptybufs;	/* number of buffers on EMPTY list */
int locleanbufs;
#ifdef DEBUG
int mincleanbufs;
#endif

a130 12
	if (bp->b_bufsize <= 0) {
		numemptybufs--;
	} else if (!ISSET(bp->b_flags, B_LOCKED)) {
		numfreebufs--;
		if (!ISSET(bp->b_flags, B_DELWRI)) {
			numcleanbufs--;
#ifdef DEBUG
			if (mincleanbufs > numcleanbufs)
				mincleanbufs = numcleanbufs;
#endif
		}
	}
a144 3
	numfreebufs = 0;
	numcleanbufs = 0;
	numemptybufs = 0;
d164 1
a164 8
		if (bp->b_bufsize) {
			dp = &bufqueues[BQ_AGE];
			numfreebufs++;
			numcleanbufs++;
		} else {
			dp = &bufqueues[BQ_EMPTY];
			numemptybufs++;
		}
a167 20

	hidirtybufs = nbuf / 4 + 20;
	numdirtybufs = 0;
	lodirtybufs = hidirtybufs / 2;

	/*
	 * Reserve 5% of bufs for syncer's needs,
	 * but not more than 25% and if possible
	 * not less then 16 bufs. locleanbufs
	 * value must be not too small, but probably
	 * there are no reason to set it more than 32.
	 */
	locleanbufs = nbuf / 20;
	if (locleanbufs < 16)
		locleanbufs = 16;
	if (locleanbufs > nbuf/4)
		locleanbufs = nbuf / 4;
#ifdef DEBUG
	mincleanbufs = locleanbufs;
#endif
d291 1
a291 1
	 * convert it to a delayed write.
d331 3
a333 4
	if (wasdelayed) {
		--numdirtybufs;
		reassignbuf(bp);
	} else
d387 1
a387 2
		reassignbuf(bp);
		++numdirtybufs;
a416 3
/*
 * Must be called at splbio()
 */
d418 1
a418 1
buf_dirty(bp)
d421 3
d426 5
a430 24
		reassignbuf(bp);
		++numdirtybufs;
#ifdef DIAGNOSTIC
		if (numdirtybufs > nbuf)
			panic("buf_dirty: incorrect number of dirty bufs");
#endif
	}
}

/*
 * Must be called at splbio()
 */
void
buf_undirty(bp)
	struct buf *bp;
{
	if (ISSET(bp->b_flags, B_DELWRI)) {
		CLR(bp->b_flags, B_DELWRI);
		reassignbuf(bp);
		--numdirtybufs;
#ifdef DIAGNOSTIC
		if (numdirtybufs < 0)
			panic("buf_undirty: incorrect number of dirty bufs");
#endif
d479 2
a480 6
		if (LIST_FIRST(&bp->b_dep) != NULL)
			buf_deallocate(bp);

		if (ISSET(bp->b_flags, B_DELWRI)) {
			--numdirtybufs;
			CLR(bp->b_flags, B_DELWRI);
d482 1
a482 1

d484 1
a484 1
			reassignbuf(bp);
d487 1
a487 1
		if (bp->b_bufsize <= 0) {
d490 1
a490 2
			numemptybufs++;
		} else {
a492 3
			numfreebufs++;
			numcleanbufs++;
		}
a497 4
		 * If buf is AGE, but has dependencies, must put it on last
		 * bufqueue to be scanned, ie LRU. This protects against the
		 * livelock where BQ_AGE only has buffers with dependencies,
		 * and we thus never get to the dependent buffers in BQ_LRU.
d502 6
a507 12
		else {
			numfreebufs++;
			if (!ISSET(bp->b_flags, B_DELWRI))
				numcleanbufs++;
			if (ISSET(bp->b_flags, B_AGE))
				/* stale but valid data */
				bufq = buf_countdeps(bp, 0, 1) ?
				    &bufqueues[BQ_LRU] : &bufqueues[BQ_AGE];
			else
				/* valid data */
				bufq = &bufqueues[BQ_LRU];
		}
a517 6
	/* Wake up syncer process waiting for buffers */
	if (syncer_needbuffer) {
		wakeup(&syncer_needbuffer);
		syncer_needbuffer = 0;
	}

d519 3
a521 3
	if (needbuffer && (numcleanbufs > locleanbufs)) {
		needbuffer--;
		wakeup_one(&needbuffer);
d580 1
a580 1
	 * code sets B_INVAL prior to writing data to the server, but
d587 3
a589 3
	bp = bh->lh_first;
	for (; bp != NULL; bp = bp->b_hash.le_next) {
		if (bp->b_lblkno != blkno || bp->b_vp != vp)
d610 1
a610 1
	}
a621 1

d632 1
a632 1
	struct buf *bp;
d682 1
a682 1
		amt = MIN(nbp->b_bufsize, (desired_size - bp->b_bufsize));
d736 1
a736 1
 * Preference is to AGE list, then LRU list.
a746 13
	/*
	 * If we're getting low on buffers kick the syncer to work harder.
	 */
	if (numcleanbufs < locleanbufs + min(locleanbufs, 4))
		speedup_syncer();

	if ((numcleanbufs <= locleanbufs) && curproc != syncerproc) {
		/* wait for a free buffer of any kind */
		needbuffer++;
		tsleep(&needbuffer, slpflag|(PRIBIO+1), "getnewbuf", slptimeo);
		splx(s);
		return (0);
	}
d750 2
a751 3
		syncer_needbuffer = 1;
		tsleep(&syncer_needbuffer, slpflag|(PRIBIO-3), "getnewbuf",
			slptimeo);
d754 1
a754 1
	}
d782 1
a782 1
		return (0);
d791 2
a792 2
	if (LIST_FIRST(&bp->b_dep) != NULL)
		buf_deallocate(bp);
d808 1
a808 1
		bp->b_rcred = NOCRED;
d814 1
a814 1

d816 1
a816 1
	return (bp);
d838 1
a838 1
	}
d870 2
a871 2
	if (LIST_FIRST(&bp->b_dep) != NULL)
		buf_complete(bp);
d873 2
a874 4
	if (!ISSET(bp->b_flags, B_READ)) {
		CLR(bp->b_flags, B_WRITEINPROG);
		vwakeup(bp->b_vp);
	}
a888 1
#ifdef DEBUG
a902 1
#endif /* DEBUG */
a916 1
	int totals[BQUEUES];
a918 1
	s = splbio();
d923 1
d928 1
a928 1
		totals[i] = count;
a934 15
	if (totals[BQ_EMPTY] != numemptybufs)
		printf("numemptybufs counter wrong: %d != %d\n",
			totals[BQ_EMPTY], numemptybufs);
	if ((totals[BQ_LRU] + totals[BQ_AGE]) != numfreebufs)
		printf("numfreebufs counter wrong: %d != %d\n",
			totals[BQ_LRU] + totals[BQ_AGE], numemptybufs);
	if ((totals[BQ_LRU] + totals[BQ_AGE]) < numcleanbufs ||
	    (numcleanbufs < 0))
		printf("numcleanbufs counter wrong: %d < %d\n",
			totals[BQ_LRU] + totals[BQ_AGE], numcleanbufs);
	printf("numcleanbufs: %d\n", numcleanbufs);
	printf("syncer eating up to %d bufs from %d reserved\n",
			locleanbufs - mincleanbufs, locleanbufs);
	printf("numdirtybufs: %d\n", numdirtybufs);
	splx(s);
@


1.26.2.2
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.26.2.1 2001/05/14 22:32:45 niklas Exp $	*/
d197 1
a197 1
			bp->b_bufsize = (base + 1) * PAGE_SIZE;
d199 1
a199 1
			bp->b_bufsize = base * PAGE_SIZE;
d770 1
a770 1
	desired_size = round_page(size);
d1044 1
a1044 1
	int counts[MAXBSIZE/PAGE_SIZE+1];
d1051 1
a1051 1
		for (j = 0; j <= MAXBSIZE/PAGE_SIZE; j++)
d1054 1
a1054 1
			counts[bp->b_bufsize/PAGE_SIZE]++;
d1059 1
a1059 1
		for (j = 0; j <= MAXBSIZE/PAGE_SIZE; j++)
d1061 1
a1061 1
				printf(", %d-%d", j * PAGE_SIZE, counts[j]);
@


1.26.2.3
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.26.2.2 2001/07/04 10:48:47 niklas Exp $	*/
d92 2
a93 2
#define	BQ_CLEAN	1		/* LRU queue with clean buffers */
#define	BQ_DIRTY	2		/* LRU queue with dirty buffers */
d98 1
a98 1
int nobuffers;
d109 1
a109 1
int getnewbuf __P((int slpflag, int slptimeo, struct buf **));
d114 15
a128 17
 *  numdirtypages - number of pages on BQ_DIRTY queue.
 *  lodirtypages  - low water mark for buffer cleaning daemon.
 *  hidirtypages  - high water mark for buffer cleaning daemon.
 *  numfreepages  - number of pages on BQ_CLEAN and BQ_DIRTY queues. unused.
 *  numcleanpages - number of pages on BQ_CLEAN queue.
 *		    Used to track the need to speedup the cleaner and 
 *		    as a reserve for special processes like syncer.
 *  mincleanpages - the lowest byte count on BQ_CLEAN.
 *  numemptybufs  - number of buffers on BQ_EMPTY. unused.
 */
long numdirtypages;
long lodirtypages;
long hidirtypages;
long numfreepages;
long numcleanpages;
long locleanpages;
int numemptybufs;
d130 1
a130 1
long mincleanpages;
a132 3
struct proc *cleanerproc;
int bd_req;			/* Sleep point for cleaner daemon. */

d156 1
a156 1
		numfreepages -= btoc(bp->b_bufsize);
d158 1
a158 1
			numcleanpages -= btoc(bp->b_bufsize);
d160 2
a161 2
			if (mincleanpages > numcleanpages)
				mincleanpages = numcleanpages;
a162 2
		} else {
			numdirtypages -= btoc(bp->b_bufsize);
d179 3
d202 3
a204 3
			dp = &bufqueues[BQ_CLEAN];
			numfreepages += btoc(bp->b_bufsize);
			numcleanpages += btoc(bp->b_bufsize);
d213 3
a215 2
	hidirtypages = bufpages / 4;
	lodirtypages = hidirtypages / 2;
d218 1
a218 1
	 * Reserve 5% of bufpages for syncer's needs,
d220 1
a220 1
	 * not less then 2 * MAXBSIZE. locleanpages
d222 1
a222 1
	 * there are no reason to set it more than 1-2 MB.
d224 5
a228 8
	locleanpages = bufpages / 20;
	if (locleanpages < btoc(2 * MAXBSIZE))
		locleanpages = btoc(2 * MAXBSIZE);
	if (locleanpages > bufpages / 4)
		locleanpages = bufpages / 4;
	if (locleanpages > btoc(2 * 1024 * 1024))
		locleanpages = btoc(2 * 1024 * 1024);

d230 1
a230 1
	mincleanpages = locleanpages;
d396 1
d453 1
d493 5
d511 5
d545 14
d568 1
d582 3
a584 3
			bufq = &bufqueues[BQ_CLEAN];
			numfreepages += btoc(bp->b_bufsize);
			numcleanpages += btoc(bp->b_bufsize);
d591 4
d600 10
a609 8
			numfreepages += btoc(bp->b_bufsize);
			if (!ISSET(bp->b_flags, B_DELWRI)) {
				numcleanpages += btoc(bp->b_bufsize);
				bufq = &bufqueues[BQ_CLEAN];
			} else {
				numdirtypages += btoc(bp->b_bufsize);
				bufq = &bufqueues[BQ_DIRTY];
			}
d611 1
a611 4
		if (ISSET(bp->b_flags, B_AGE))
			binsheadfree(bp, bufq);
		else
			binstailfree(bp, bufq);
d614 1
d616 1
a616 1
	CLR(bp->b_flags, (B_AGE | B_ASYNC | B_BUSY | B_NOCACHE | B_DEFERRED));
d621 4
a624 4
	/* Wake up syncer and cleaner processes waiting for buffers */
	if (nobuffers) {
		wakeup(&nobuffers);
		nobuffers = 0;
d628 1
a628 1
	if (needbuffer && (numcleanpages > locleanpages)) {
d681 1
a681 1
	struct buf *bp, *nbp = NULL;
d722 1
a722 1
		if (nbp == NULL && getnewbuf(slpflag, slptimeo, &nbp) != 0) {
a723 2
		}
		bp = nbp;
a728 7
	} else if (nbp != NULL) {
		/*
		 * Set B_AGE so that buffer appear at BQ_CLEAN head
		 * and gets reused ASAP.
		 */
		SET(nbp->b_flags, B_AGE);
		brelse(nbp);
d744 2
a745 1
	getnewbuf(0, 0, &bp);
d786 2
a787 1
		getnewbuf(0, 0, &nbp);
d845 2
a846 4
 *
 * We must notify getblk if we slept during the buffer allocation. When
 * that happens, we allocate a buffer anyway (unless tsleep is interrupted
 * or times out) and return !0.
d848 2
a849 2
int
getnewbuf(slpflag, slptimeo, bpp)
a850 1
	struct buf **bpp;
d852 2
a853 5
	struct buf *bp;
	int s, ret, error;

	*bpp = NULL;
	ret = 0;
d858 1
a858 1
	 * Wake up cleaner if we're getting low on buffers.
d860 2
a861 2
	if (numdirtypages >= hidirtypages)
		wakeup(&bd_req);
d863 2
a864 2
	if ((numcleanpages <= locleanpages) &&
	    curproc != syncerproc && curproc != cleanerproc) {
d866 1
a866 2
		error = tsleep(&needbuffer, slpflag|(PRIBIO+1), "getnewbuf",
				slptimeo);
d868 1
a868 4
		if (error)
			return (1);
		ret = 1;
		goto start;
d870 2
a871 1
	if ((bp = TAILQ_FIRST(&bufqueues[BQ_CLEAN])) == NULL) {
d873 3
a875 3
		nobuffers = 1;
		error = tsleep(&nobuffers, slpflag|(PRIBIO-3),
				"getnewbuf", slptimeo);
d877 1
a877 4
		if (error)
			return (1);
		ret = 1;
		goto start;
d882 12
d897 11
a907 4
#ifdef DIAGNOSTIC
	if (ISSET(bp->b_flags, B_DELWRI))
		panic("Dirty buffer on BQ_CLEAN");
#endif
a914 2
#ifdef DIAGNOSTIC
	/* CLEAN buffers must have no dependencies */ 
d916 1
a916 2
		panic("BQ_CLEAN has buffer with dependencies");
#endif
d940 1
a940 61
	*bpp = bp;
	return (ret);
}

/*
 * Buffer cleaning daemon.
 */
void
buf_daemon(struct proc *p)
{
	int s;
	struct buf *bp;
	struct timeval starttime, timediff;

	cleanerproc = curproc;

	for (;;) {
		if (numdirtypages < hidirtypages) {
			tsleep(&bd_req, PRIBIO - 7, "cleaner", 0);
		}

		starttime = time;
		s = splbio();
		while ((bp = TAILQ_FIRST(&bufqueues[BQ_DIRTY]))) {
			bremfree(bp);
			SET(bp->b_flags, B_BUSY);
			splx(s);

			if (ISSET(bp->b_flags, B_INVAL)) {
				brelse(bp);
				s = splbio();
				continue;
			}
#ifdef DIAGNOSTIC
			if (!ISSET(bp->b_flags, B_DELWRI))
				panic("Clean buffer on BQ_DIRTY");
#endif
			if (LIST_FIRST(&bp->b_dep) != NULL &&
			    !ISSET(bp->b_flags, B_DEFERRED) &&
			    buf_countdeps(bp, 0, 1)) {
				SET(bp->b_flags, B_DEFERRED);
				s = splbio();
				numfreepages += btoc(bp->b_bufsize);
				numdirtypages += btoc(bp->b_bufsize);
				binstailfree(bp, &bufqueues[BQ_DIRTY]);
				CLR(bp->b_flags, B_BUSY);
				continue;
			}

			bawrite(bp);

			if (numdirtypages < lodirtypages)
				break;
			/* Never allow processing to run for more than 1 sec */
			timersub(&time, &starttime, &timediff);
			if (timediff.tv_sec)
				break;

			s = splbio();
		}
	}
d1017 17
d1046 1
a1046 3
	long ptotals[BQUEUES];
	long pages;
	static char *bname[BQUEUES] = { "LOCKED", "CLEAN", "DIRTY", "EMPTY" };
a1050 1
		pages = 0;
a1055 1
			pages += btoc(bp->b_bufsize);
d1058 1
a1058 2
		ptotals[i] = pages;
		printf("%s: total-%d(%d pages)", bname[i], count, pages);
d1066 12
a1077 17
			numemptybufs, totals[BQ_EMPTY]);
	if ((ptotals[BQ_CLEAN] + ptotals[BQ_DIRTY]) != numfreepages)
		printf("numfreepages counter wrong: %ld != %ld\n",
			numfreepages, ptotals[BQ_CLEAN] + ptotals[BQ_DIRTY]);
	if (ptotals[BQ_CLEAN] != numcleanpages)
		printf("numcleanpages counter wrong: %ld != %ld\n",
			numcleanpages, ptotals[BQ_CLEAN]);
	else
		printf("numcleanpages: %ld\n", numcleanpages);
	if (numdirtypages != ptotals[BQ_DIRTY])
		printf("numdirtypages counter wrong: %ld != %ld\n",
			numdirtypages, ptotals[BQ_DIRTY]);
	else
		printf("numdirtypages: %ld\n", numdirtypages);

	printf("syncer eating up to %ld pages from %ld reserved\n",
			locleanpages - mincleanpages, locleanpages);
@


1.26.2.4
log
@merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a58 1
#include <sys/pool.h>
d63 1
a63 1
#include <uvm/uvm_extern.h>
a101 5
 * Buffer pool for I/O buffers.
 */
struct pool bufpool;

/*
a185 2
	pool_init(&bufpool, sizeof(struct buf), 0, 0, 0, "bufpl", 0,
	    NULL, NULL, M_DEVBUF);
@


1.26.2.5
log
@Merge in -current
@
text
@d113 2
a114 1
static __inline struct buf *bio_doread __P((struct vnode *, daddr_t, int, int));
d203 2
d249 1
a249 1
bio_doread(vp, blkno, size, async)
d253 1
d266 1
d268 4
d298 1
a298 1
	bp = *bpp = bio_doread(vp, blkno, size, 0);
d320 1
a320 1
	bp = *bpp = bio_doread(vp, blkno, size, 0);
d331 1
a331 1
		(void) bio_doread(vp, rablks[i], rasizes[i], B_ASYNC);
d339 17
d418 1
d478 1
a599 1
	SET(bp->b_flags, B_CACHE);
d663 1
d667 11
d679 5
a683 2
	bp = incore(vp, blkno);
	if (bp != NULL) {
d690 1
a690 5
			if (err) {
				if (nbp != NULL) {
					SET(nbp->b_flags, B_AGE);
					brelse(nbp);
				}
a691 1
			}
d695 6
a700 2
		SET(bp->b_flags, (B_BUSY | B_CACHE));
		bremfree(bp);
d709 1
a709 1
		binshash(bp, BUFHASH(vp, blkno));
d912 12
d1044 1
a1119 13

int
buf_cleanout(struct buf *bp) {
	if (bp->b_vp != NULL)
		brelvp(bp);

	if (bp->b_flags & B_WANTED) {
		bp->b_flags &= ~B_WANTED;
		wakeup(bp);
	}

	return (0);
}
@


1.26.2.6
log
@Merge in trunk
@
text
@d68 5
d191 2
a192 1
	pool_init(&bufpool, sizeof(struct buf), 0, 0, 0, "bufpl", NULL);
a329 17
 * Read with single-block read-ahead.  Defined in Bach (p.55), but
 * implemented as a call to breadn().
 * XXX for compatibility with old file systems.
 */
int
breada(vp, blkno, size, rablkno, rabsize, cred, bpp)
	struct vnode *vp;
	daddr_t blkno; int size;
	daddr_t rablkno; int rabsize;
	struct ucred *cred;
	struct buf **bpp;
{

	return (breadn(vp, blkno, size, &rablkno, &rabsize, 1, cred, bpp));	
}

/*
a391 1
	SET(bp->b_flags, B_WRITEINPROG);
a450 1
	CLR(bp->b_flags, B_NEEDCOMMIT);
d572 1
a635 1
	struct bufhashhdr *bh;
a638 11
	/*
	 * XXX
	 * The following is an inlined version of 'incore()', but with
	 * the 'invalid' test moved to after the 'busy' test.  It's
	 * necessary because there are some cases in which the NFS
	 * code sets B_INVAL prior to writing data to the server, but
	 * in which the buffers actually contain valid data.  In this
	 * case, we can't allow the system to allocate a new buffer for
	 * the block until the write is finished.
	 */
	bh = BUFHASH(vp, blkno);
d640 2
a641 5
	bp = bh->lh_first;
	for (; bp != NULL; bp = bp->b_hash.le_next) {
		if (bp->b_lblkno != blkno || bp->b_vp != vp)
			continue;

d648 5
a652 1
			if (err)
d654 1
d658 2
a659 6
		if (!ISSET(bp->b_flags, B_INVAL)) {
			SET(bp->b_flags, (B_BUSY | B_CACHE));
			bremfree(bp);
			splx(s);
			break;
		}
d668 1
a668 1
		binshash(bp, bh);
a870 2
	bp->b_dirtyoff = bp->b_dirtyend = 0;
	bp->b_validoff = bp->b_validend = 0;
a990 1
		CLR(bp->b_flags, B_WRITEINPROG);
d1066 13
@


1.26.2.7
log
@Merge in -current from about a week ago
@
text
@d108 2
a109 2
static __inline struct buf *bio_doread(struct vnode *, daddr_t, int, int);
int getnewbuf(int slpflag, int slptimeo, struct buf **);
@


1.26.2.8
log
@Sync the SMP branch with 3.3
@
text
@d139 2
a140 1
bremfree(struct buf *bp)
d179 1
a179 1
bufinit(void)
d181 1
a181 1
	struct buf *bp;
d183 1
a183 1
	int i;
d240 5
a244 1
bio_doread(struct vnode *vp, daddr_t blkno, int size, int async)
d246 1
a246 1
	struct buf *bp;
d273 6
a278 2
bread(struct vnode *vp, daddr_t blkno, int size, struct ucred *cred,
    struct buf **bpp)
d280 1
a280 1
	struct buf *bp;
d294 7
a300 2
breadn(struct vnode *vp, daddr_t blkno, int size, daddr_t rablks[],
    int rasizes[], int nrablks, struct ucred *cred, struct buf **bpp)
d302 1
a302 1
	struct buf *bp;
d329 6
a334 2
breada(struct vnode *vp, daddr_t blkno, int size, daddr_t rablkno, int rabsize,
    struct ucred *cred, struct buf **bpp)
d344 2
a345 1
bwrite(struct buf *bp)
d435 2
a436 1
bdwrite(struct buf *bp)
d472 2
a473 1
bawrite(struct buf *bp)
d484 2
a485 1
buf_dirty(struct buf *bp)
a486 2
	splassert(IPL_BIO);

d497 2
a498 1
buf_undirty(struct buf *bp)
a499 2
	splassert(IPL_BIO);

d511 2
a512 1
brelse(struct buf *bp)
d586 2
d601 1
a601 3
	splx(s);

	/* Wake up any processes waiting for _this_ buffer to become free. */
d616 3
a618 1
incore(struct vnode *vp, daddr_t blkno)
d622 2
d625 1
a625 1
	LIST_FOREACH(bp, BUFHASH(vp, blkno), b_hash) {
d643 4
a646 1
getblk(struct vnode *vp, daddr_t blkno, int size, int slpflag, int slptimeo)
d664 2
a665 1
	LIST_FOREACH(bp, BUFHASH(vp, blkno), b_hash) {
d716 2
a717 1
geteblk(int size)
d738 3
a740 1
allocbuf(struct buf *bp, int size)
d826 3
a828 1
getnewbuf(int slpflag, int slptimeo, struct buf **bpp)
d969 2
a970 1
biowait(struct buf *bp)
a1005 2
 *
 * Must be called at splbio().
d1008 2
a1009 1
biodone(struct buf *bp)
a1010 2
	splassert(IPL_BIO);

@


1.26.2.9
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.26.2.8 2003/03/28 00:41:27 niklas Exp $	*/
d22 5
a26 1
 * 3. Neither the name of the University nor the names of its contributors
@


1.25
log
@Indentation to make the code more readable.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.24 1999/09/10 22:14:39 art Exp $	*/
d285 2
d301 19
@


1.24
log
@use clrnd(round_page(size)) instead of roundup(size, CLBYTES).
They do the same thing, but the former is noticeably faster on sparc
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.23 1999/07/15 14:07:41 art Exp $	*/
d858 7
a864 5
	} else if (ISSET(bp->b_flags, B_ASYNC))	/* if async, release it */
		brelse(bp);
	else {					/* or just wakeup the buffer */
		CLR(bp->b_flags, B_WANTED);
		wakeup(bp);
@


1.23
log
@vm_offset_t -> {v,p}addr_t ; vm_size_t -> {v,p}size_t
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.22 1999/04/28 09:28:15 art Exp $	*/
d639 1
a639 1
	desired_size = roundup(size, CLBYTES);
@


1.22
log
@zap the newhashinit hack.
Add an extra flag to hashinit telling if it should wait in malloc.
update all calls to hashinit.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.21 1998/11/29 01:46:58 art Exp $	*/
d635 3
a637 3
	struct buf      *nbp;
	vm_size_t       desired_size;
	int	     s;
@


1.21
log
@indent
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.20 1998/01/10 23:44:28 csapuntz Exp $	*/
d147 1
a147 1
	bufhashtbl = hashinit(nbuf, M_CACHE, &bufhash);
@


1.20
log
@A couple more splbio()s in vfs_bio plus moving around a couple functions.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.19 1997/11/07 23:01:36 csapuntz Exp $	*/
d530 1
a530 1
		return (bp);
@


1.19
log
@Fixed hang on shutdown
Disabled vop_nolock for now. Filesystems still need to be cleaned up.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.18 1997/11/06 05:58:25 csapuntz Exp $	*/
a335 8
int
vn_bwrite(v)
	void *v;
{
	struct vop_bwrite_args *ap = v;

	return (bwrite(ap->a_bp));
}
d354 2
d365 1
d367 1
d401 1
d405 1
d407 1
@


1.18
log
@Updates for VFS Lite 2 + soft update.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.17 1997/10/06 20:20:07 deraadt Exp $	*/
d853 3
d859 1
a859 6
	} 

	if (!ISSET(bp->b_flags, B_READ))	/* wake up reader */
		vwakeup(bp);

	if (ISSET(bp->b_flags, B_ASYNC))	/* if async, release it */
@


1.17
log
@back out vfs lite2 till after 2.2
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.15 1997/06/14 06:10:36 tholo Exp $	*/
d66 2
d99 1
a144 1
	TAILQ_INIT(&bdirties);
d158 1
d284 1
a284 1
	int rv, sync, wasdelayed, s;
d293 2
a294 2
	sync = !ISSET(bp->b_flags, B_ASYNC);
	if (sync && bp->b_vp && bp->b_vp->v_mount &&
d301 3
d305 4
a308 2
	 * If this was a delayed write, remove it from the
	 * list of dirty blocks now
d311 4
a314 15
		TAILQ_REMOVE(&bdirties, bp, b_synclist);

	s = splbio();
	if (!sync) {
		/*
		 * If not synchronous, pay for the I/O operation and make
		 * sure the buf is on the correct vnode queue.  We have
		 * to do this now, because if we don't, the vnode may not
		 * be properly notified that its I/O has completed.
		 */
		if (wasdelayed)
			reassignbuf(bp, bp->b_vp);
		else
			curproc->p_stats->p_ru.ru_oublock++;
	}
d322 2
a323 5
	if (sync) {
		/*
		 * If I/O was synchronous, wait for it to complete.
		 */
		rv = biowait(bp);
d325 4
a328 11
		/*
		 * Pay for the I/O operation, if it's not been paid for, and
		 * make sure it's on the correct vnode queue. (async operatings
		 * were payed for above.)
		 */
		s = splbio();
		if (wasdelayed)
			reassignbuf(bp, bp->b_vp);
		else
			curproc->p_stats->p_ru.ru_oublock++;
		splx(s);
d330 2
a331 2
		/* Release the buffer. */
		brelse(bp);
d333 1
a333 4
		return (rv);
	} else {
		return (0);
	}
a368 1
	bp->b_synctime = time.tv_sec + 30;
a369 12
		/*
		 * Add the buffer to the list of dirty blocks.
		 * If it is the first entry on the list, schedule
		 * a timeout to flush it to disk
		 */
		TAILQ_INSERT_TAIL(&bdirties, bp, b_synclist);
		if (bdirties.tqh_first == bp) {
			untimeout((void (*)__P((void *)))wakeup,
				  &bdirties);		/* XXX */
			timeout((void (*)__P((void *)))wakeup,
				&bdirties, 30 * hz);
		}
d371 1
a372 1
		reassignbuf(bp, bp->b_vp);
a399 5
/*
 * Write out dirty buffers if they have been on the dirty
 * list for more than 30 seconds; scan for such buffers
 * once a second.
 */
d401 2
a402 1
vn_update()
d404 1
a404 5
	struct mount *mp, *nmp;
	struct timespec ts;
	struct vnode *vp;
	struct buf *bp;
	int async, s;
d406 5
a410 122
	/*
	 * In case any buffers got scheduled for write before the
	 * process got started (should never happen)
	 */
	untimeout((void (*)__P((void *)))wakeup,
		  &bdirties);
	for (;;) {
		s = splbio();
		/*
		 * Schedule a wakeup when the next buffer is to
		 * be flushed to disk.  If no buffers are enqueued,
		 * a wakeup will be scheduled at the time a new
		 * buffer is enqueued
		 */
		if ((bp = bdirties.tqh_first) != NULL) {
                        untimeout((void (*)__P((void *)))wakeup,
				  &bdirties);		/* XXX */
                        timeout((void (*)__P((void *)))wakeup,
				&bdirties, (bp->b_synctime - time.tv_sec) * hz);
		}
		tsleep(&bdirties, PZERO - 1, "dirty", 0);
		/*
		 * Walk the dirty block list, starting an asyncroneous
		 * write of any block that has timed out
		 */
		while ((bp = bdirties.tqh_first) != NULL &&
		       bp->b_synctime <= time.tv_sec) {
			/*
			 * If the block is currently busy (perhaps being
			 * written), move it to the end of the dirty list
			 * and go to the next block
			 */
			if (ISSET(bp->b_flags, B_BUSY)) {
				TAILQ_REMOVE(&bdirties, bp, b_synclist);
				TAILQ_INSERT_TAIL(&bdirties, bp, b_synclist);
				bp->b_synctime = time.tv_sec + 30;
				continue;
			}
			/*
			 * Remove the block from the per-vnode dirty
			 * list and mark it as busy
			 */
			bremfree(bp);
			SET(bp->b_flags, B_BUSY);
			splx(s);
			/*
			 * Start an asyncroneous write of the buffer.
			 * Note that this will also remove the buffer
			 * from the dirty list
			 */
			bawrite(bp);
			s = splbio();
		}
		splx(s);
		/*
		 * We also need to flush out modified vnodes
		 */
		for (mp = mountlist.cqh_last;
		     mp != (void *)&mountlist;
		     mp = nmp) {
			/*
			 * Get the next pointer in case we hang of vfs_busy()
			 * while being unmounted
			 */
			nmp = mp->mnt_list.cqe_prev;
			/*
			 * The lock check below is to avoid races with mount
			 * and unmount
			 */
			if ((mp->mnt_flag & (MNT_MLOCK | MNT_RDONLY | MNT_MPBUSY)) == 0 &&
			    !vfs_busy(mp)) {
				/*
				 * Turn off the file system async flag until
				 * we are done writing out vnodes
				 */
				async = mp->mnt_flag & MNT_ASYNC;
				mp->mnt_flag &= ~MNT_ASYNC;
				/*
				 * Walk the vnode list for the file system,
				 * writing each modified vnode out
				 */
loop:
				for (vp = mp->mnt_vnodelist.lh_first;
				     vp != NULL;
				     vp = vp->v_mntvnodes.le_next) {
					/*
					 * If the vnode is no longer associated
					 * with the file system in question, skip
					 * it
					 */
					if (vp->v_mount != mp)
						goto loop;
					/*
					 * If the vnode is currently locked,
					 * ignore it
					 */
					if (VOP_ISLOCKED(vp))
						continue;
					/*
					 * Lock the vnode, start a write and
					 * release the vnode
					 */
					if (vget(vp, 1))
						goto loop;
					TIMEVAL_TO_TIMESPEC(&time, &ts);
					VOP_UPDATE(vp, &ts, &ts, 0);
					vput(vp);
				}
				/*
				 * Restore the file system async flag if it
				 * were previously set for this file system
				 */
				mp->mnt_flag |= async;
				/*
				 * Get the next pointer again as the next
				 * file system might have been unmounted
				 * while we were flushing vnodes
				 */
				nmp = mp->mnt_list.cqe_prev;
				vfs_unbusy(mp);
			}
		}
a424 12
	/* Wake up any processes waiting for any buffer to become free. */
	if (needbuffer) {
		needbuffer = 0;
		wakeup(&needbuffer);
	}

	/* Wake up any proceeses waiting for _this_ buffer to become free. */
	if (ISSET(bp->b_flags, B_WANTED)) {
		CLR(bp->b_flags, B_WANTED);
		wakeup(bp);
	}

d459 6
a464 1
		if (bp->b_vp)
d466 1
a466 3
		if (ISSET(bp->b_flags, B_DELWRI))
			TAILQ_REMOVE(&bdirties, bp, b_synclist);
		CLR(bp->b_flags, B_DELWRI);
d497 12
d658 1
a658 1
		SET(nbp->b_flags, B_INVAL);
d727 2
a728 4
	if ((bp = bufqueues[BQ_AGE].tqh_first) != NULL ||
	    (bp = bufqueues[BQ_LRU].tqh_first) != NULL) {
		bremfree(bp);
	} else {
d734 3
a736 1
	}
d768 1
d771 3
d818 3
a820 1
	} else if (ISSET(bp->b_flags, B_ERROR))
d850 2
a851 2
	if (!ISSET(bp->b_flags, B_READ))	/* wake up reader */
		vwakeup(bp);
d856 6
a861 1
	} else if (ISSET(bp->b_flags, B_ASYNC))	/* if async, release it */
@


1.16
log
@VFS Lite2 Changes
@
text
@a65 2
#include <miscfs/specfs/specdev.h>

a96 1
struct bio_ops bioops;
d142 1
a155 1
		LIST_INIT(&bp->b_dep);
d281 1
a281 1
	int rv, async, wasdelayed, s;
d290 2
a291 2
	async = ISSET(bp->b_flags, B_ASYNC);
	if (!async && bp->b_vp && bp->b_vp->v_mount &&
a297 3

	s = splbio();

d299 2
a300 4
	 * If not synchronous, pay for the I/O operation and make
	 * sure the buf is on the correct vnode queue.  We have
	 * to do this now, because if we don't, the vnode may not
	 * be properly notified that its I/O has completed.
d303 15
a317 4
		reassignbuf(bp, bp->b_vp);
	else
		curproc->p_stats->p_ru.ru_oublock++;
	
d325 5
a329 2
	if (async)
		return (0);
d331 11
a341 4
	/*
	 * If I/O was synchronous, wait for it to complete.
	 */
	rv = biowait(bp);
d343 2
a344 2
	/* Release the buffer. */
	brelse(bp);
d346 4
a349 1
	return (rv);
d385 1
d387 12
d400 1
a401 1
		curproc->p_stats->p_ru.ru_oublock++;	/* XXX */
d429 5
d435 5
a439 1
bdirty(bp)
d441 1
a441 2
{
	struct proc *p = curproc;       /* XXX */
d443 122
a564 5
	if (ISSET(bp->b_flags, B_DELWRI) == 0) {
		SET(bp->b_flags, B_DELWRI);
		reassignbuf(bp, bp->b_vp);
		if (p)
			p->p_stats->p_ru.ru_oublock++;
d579 12
d625 4
a628 3
		if (LIST_FIRST(&bp->b_dep) != NULL && bioops.io_deallocate) {
			(*bioops.io_deallocate)(bp);
		}
a629 4
		if (bp->b_vp) {
			reassignbuf(bp, bp->b_vp);
			brelvp(bp);
		}
a659 12

	/* Wake up any processes waiting for any buffer to become free. */
	if (needbuffer) {
		needbuffer = 0;
		wakeup(&needbuffer);
	}

	/* Wake up any proceeses waiting for _this_ buffer to become free. */
	if (ISSET(bp->b_flags, B_WANTED)) {
		CLR(bp->b_flags, B_WANTED);
		wakeup(bp);
	}
d809 1
a809 1
 		SET(nbp->b_flags, B_INVAL);
d878 4
a881 2
	if ((bp = bufqueues[BQ_AGE].tqh_first) == NULL &&
	    (bp = bufqueues[BQ_LRU].tqh_first) == NULL) {
d887 1
a887 3
	} 

	bremfree(bp);
a918 1

a920 3
	if (LIST_FIRST(&bp->b_dep) != NULL && bioops.io_deallocate)
		(*bioops.io_deallocate)(bp);

d965 1
a965 3
	} 

	if (ISSET(bp->b_flags, B_ERROR))
d995 2
a996 2
	if (LIST_FIRST(&bp->b_dep) != NULL && bioops.io_complete)
		(*bioops.io_complete)(bp);
d1001 1
a1001 6
	} 

	if (!ISSET(bp->b_flags, B_READ))	/* wake up reader */
		vwakeup(bp);

	if (ISSET(bp->b_flags, B_ASYNC))	/* if async, release it */
@


1.15
log
@Don't look beyond the end of bdevsw[], from NetBSD PR 3748 by Michael L Hitch
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.14 1997/04/14 04:23:23 tholo Exp $	*/
d66 2
d99 1
a144 1
	TAILQ_INIT(&bdirties);
d158 1
d284 1
a284 1
	int rv, sync, wasdelayed, s;
d293 2
a294 2
	sync = !ISSET(bp->b_flags, B_ASYNC);
	if (sync && bp->b_vp && bp->b_vp->v_mount &&
d301 3
d305 4
a308 2
	 * If this was a delayed write, remove it from the
	 * list of dirty blocks now
d311 4
a314 15
		TAILQ_REMOVE(&bdirties, bp, b_synclist);

	s = splbio();
	if (!sync) {
		/*
		 * If not synchronous, pay for the I/O operation and make
		 * sure the buf is on the correct vnode queue.  We have
		 * to do this now, because if we don't, the vnode may not
		 * be properly notified that its I/O has completed.
		 */
		if (wasdelayed)
			reassignbuf(bp, bp->b_vp);
		else
			curproc->p_stats->p_ru.ru_oublock++;
	}
d322 2
a323 5
	if (sync) {
		/*
		 * If I/O was synchronous, wait for it to complete.
		 */
		rv = biowait(bp);
d325 4
a328 11
		/*
		 * Pay for the I/O operation, if it's not been paid for, and
		 * make sure it's on the correct vnode queue. (async operatings
		 * were payed for above.)
		 */
		s = splbio();
		if (wasdelayed)
			reassignbuf(bp, bp->b_vp);
		else
			curproc->p_stats->p_ru.ru_oublock++;
		splx(s);
d330 2
a331 2
		/* Release the buffer. */
		brelse(bp);
d333 1
a333 4
		return (rv);
	} else {
		return (0);
	}
a368 1
	bp->b_synctime = time.tv_sec + 30;
a369 12
		/*
		 * Add the buffer to the list of dirty blocks.
		 * If it is the first entry on the list, schedule
		 * a timeout to flush it to disk
		 */
		TAILQ_INSERT_TAIL(&bdirties, bp, b_synclist);
		if (bdirties.tqh_first == bp) {
			untimeout((void (*)__P((void *)))wakeup,
				  &bdirties);		/* XXX */
			timeout((void (*)__P((void *)))wakeup,
				&bdirties, 30 * hz);
		}
d371 1
a372 1
		reassignbuf(bp, bp->b_vp);
a399 5
/*
 * Write out dirty buffers if they have been on the dirty
 * list for more than 30 seconds; scan for such buffers
 * once a second.
 */
d401 2
a402 1
vn_update()
d404 1
a404 5
	struct mount *mp, *nmp;
	struct timespec ts;
	struct vnode *vp;
	struct buf *bp;
	int async, s;
d406 5
a410 122
	/*
	 * In case any buffers got scheduled for write before the
	 * process got started (should never happen)
	 */
	untimeout((void (*)__P((void *)))wakeup,
		  &bdirties);
	for (;;) {
		s = splbio();
		/*
		 * Schedule a wakeup when the next buffer is to
		 * be flushed to disk.  If no buffers are enqueued,
		 * a wakeup will be scheduled at the time a new
		 * buffer is enqueued
		 */
		if ((bp = bdirties.tqh_first) != NULL) {
                        untimeout((void (*)__P((void *)))wakeup,
				  &bdirties);		/* XXX */
                        timeout((void (*)__P((void *)))wakeup,
				&bdirties, (bp->b_synctime - time.tv_sec) * hz);
		}
		tsleep(&bdirties, PZERO - 1, "dirty", 0);
		/*
		 * Walk the dirty block list, starting an asyncroneous
		 * write of any block that has timed out
		 */
		while ((bp = bdirties.tqh_first) != NULL &&
		       bp->b_synctime <= time.tv_sec) {
			/*
			 * If the block is currently busy (perhaps being
			 * written), move it to the end of the dirty list
			 * and go to the next block
			 */
			if (ISSET(bp->b_flags, B_BUSY)) {
				TAILQ_REMOVE(&bdirties, bp, b_synclist);
				TAILQ_INSERT_TAIL(&bdirties, bp, b_synclist);
				bp->b_synctime = time.tv_sec + 30;
				continue;
			}
			/*
			 * Remove the block from the per-vnode dirty
			 * list and mark it as busy
			 */
			bremfree(bp);
			SET(bp->b_flags, B_BUSY);
			splx(s);
			/*
			 * Start an asyncroneous write of the buffer.
			 * Note that this will also remove the buffer
			 * from the dirty list
			 */
			bawrite(bp);
			s = splbio();
		}
		splx(s);
		/*
		 * We also need to flush out modified vnodes
		 */
		for (mp = mountlist.cqh_last;
		     mp != (void *)&mountlist;
		     mp = nmp) {
			/*
			 * Get the next pointer in case we hang of vfs_busy()
			 * while being unmounted
			 */
			nmp = mp->mnt_list.cqe_prev;
			/*
			 * The lock check below is to avoid races with mount
			 * and unmount
			 */
			if ((mp->mnt_flag & (MNT_MLOCK | MNT_RDONLY | MNT_MPBUSY)) == 0 &&
			    !vfs_busy(mp)) {
				/*
				 * Turn off the file system async flag until
				 * we are done writing out vnodes
				 */
				async = mp->mnt_flag & MNT_ASYNC;
				mp->mnt_flag &= ~MNT_ASYNC;
				/*
				 * Walk the vnode list for the file system,
				 * writing each modified vnode out
				 */
loop:
				for (vp = mp->mnt_vnodelist.lh_first;
				     vp != NULL;
				     vp = vp->v_mntvnodes.le_next) {
					/*
					 * If the vnode is no longer associated
					 * with the file system in question, skip
					 * it
					 */
					if (vp->v_mount != mp)
						goto loop;
					/*
					 * If the vnode is currently locked,
					 * ignore it
					 */
					if (VOP_ISLOCKED(vp))
						continue;
					/*
					 * Lock the vnode, start a write and
					 * release the vnode
					 */
					if (vget(vp, 1))
						goto loop;
					TIMEVAL_TO_TIMESPEC(&time, &ts);
					VOP_UPDATE(vp, &ts, &ts, 0);
					vput(vp);
				}
				/*
				 * Restore the file system async flag if it
				 * were previously set for this file system
				 */
				mp->mnt_flag |= async;
				/*
				 * Get the next pointer again as the next
				 * file system might have been unmounted
				 * while we were flushing vnodes
				 */
				nmp = mp->mnt_list.cqe_prev;
				vfs_unbusy(mp);
			}
		}
a424 12
	/* Wake up any processes waiting for any buffer to become free. */
	if (needbuffer) {
		needbuffer = 0;
		wakeup(&needbuffer);
	}

	/* Wake up any proceeses waiting for _this_ buffer to become free. */
	if (ISSET(bp->b_flags, B_WANTED)) {
		CLR(bp->b_flags, B_WANTED);
		wakeup(bp);
	}

d459 6
a464 1
		if (bp->b_vp)
d466 1
a466 3
		if (ISSET(bp->b_flags, B_DELWRI))
			TAILQ_REMOVE(&bdirties, bp, b_synclist);
		CLR(bp->b_flags, B_DELWRI);
d497 12
d658 1
a658 1
		SET(nbp->b_flags, B_INVAL);
d727 2
a728 4
	if ((bp = bufqueues[BQ_AGE].tqh_first) != NULL ||
	    (bp = bufqueues[BQ_LRU].tqh_first) != NULL) {
		bremfree(bp);
	} else {
d734 3
a736 1
	}
d768 1
d771 3
d818 3
a820 1
	} else if (ISSET(bp->b_flags, B_ERROR))
d850 2
a851 2
	if (!ISSET(bp->b_flags, B_READ))	/* wake up reader */
		vwakeup(bp);
d856 6
a861 1
	} else if (ISSET(bp->b_flags, B_ASYNC))	/* if async, release it */
@


1.14
log
@Minor performance enhancements from NetBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.13 1997/01/05 11:09:01 niklas Exp $	*/
d405 2
a406 1
	if (bdevsw[major(bp->b_dev)].d_type == D_TAPE) {
@


1.13
log
@Remove lots of timer_state structs as they just ate memory and only a few was
ever used.  Now a single state is kept for net, tty and disk events resp.
Also, call the randomness from disk_unbusy instead of biodone, as biodone
gets a lot of virtual events (from virtual filesystems etc), and as a bonus:
feed xfer time and size into the entropy pool too.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.12 1996/10/19 13:26:02 mickey Exp $	*/
d605 14
d653 1
d888 12
d906 6
a911 1
		bawrite (bp);
@


1.12
log
@random device is permanent now.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.11 1996/08/29 07:46:37 deraadt Exp $	*/
a64 1
#include <dev/rndvar.h>
a960 2

	add_blkdev_randomness(bp->b_dev);	/* grow universe entropy */
@


1.11
log
@rnd -> random
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.10 1996/07/21 08:05:34 tholo Exp $	*/
a64 3

#include "random.h"
#if NRANDOM > 0
a65 1
#endif
d963 1
a963 3
#if NRANDOM > 0
	add_blkdev_randomness(bp->b_dev);
#endif
@


1.10
log
@Ensure we never use more than one callout table slot
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.9 1996/07/02 06:52:01 niklas Exp $	*/
d66 2
a67 2
#include "rnd.h"
#if	NRND > 0
d967 1
a967 1
#if	NRND > 0
@


1.9
log
@-Wall & -Wstrict-prototype fixes
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.8 1996/06/26 19:44:59 tholo Exp $	*/
d398 3
a400 1
		if (bdirties.tqh_first == bp)
d403 1
d461 3
a463 1
		if ((bp = bdirties.tqh_first) != NULL)
d466 1
@


1.8
log
@When a dirty buffer is written to, don't move it to the tail of the
dirty list.  Doing that can, for a block that is written to with a
period of less than 30 seconds, cause the block to never be flushed
to disk.  Idea from Jeffrey Mogul's paper covering the same basic
changes.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.7 1996/06/14 06:36:24 deraadt Exp $	*/
a382 2
	int setit;

@


1.7
log
@from pk; protect some more stuff with splbio
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.6 1996/06/11 03:25:13 tholo Exp $	*/
a405 21
	}
	else {
		/*
		 * The buffer has been rewritten.  Move it to the
		 * end of the dirty block list, and if it was the
		 * first entry before being moved, reschedule the
		 * timeout
		 */
		if (bdirties.tqh_first == bp) {
			untimeout((void (*)__P((void *)))wakeup,
				  &bdirties);
			setit = 1;
		}
		else
			setit = 0;
		TAILQ_REMOVE(&bdirties, bp, b_synclist);
		TAILQ_INSERT_TAIL(&bdirties, bp, b_synclist);
		if (setit && bdirties.tqh_first != bp)
                        timeout((void (*)__P((void *)))wakeup,
				&bdirties,
				(bdirties.tqh_first->b_synctime - time.tv_sec) * hz);
@


1.6
log
@Kernel-implementation of update(8) my me
@
text
@d1 2
a2 2
/*	$OpenBSD: vfs_bio.c,v 1.5 1996/05/02 13:12:29 deraadt Exp $	*/
/*	$NetBSD: vfs_bio.c,v 1.43 1996/04/22 01:38:59 christos Exp $	*/
d286 1
a286 1
	int rv, sync, wasdelayed;
d310 1
d325 2
a327 1
	bp->b_vp->v_numoutput++;
d341 1
d346 1
@


1.5
log
@sync syscalls, no sys/cpu.h
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_bio.c,v 1.4 1996/04/19 16:09:09 niklas Exp $	*/
d62 1
d147 1
d303 6
d379 1
d386 1
d388 1
d390 9
d403 21
d450 136
d630 2
@


1.4
log
@NetBSD 960317 merge
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: vfs_bio.c,v 1.42 1996/02/18 11:57:08 fvdl Exp $	*/
d62 2
a63 1
#include <sys/cpu.h>
@


1.3
log
@Initial commit of random source driver.
@
text
@d1 2
a2 1
/*	$NetBSD: vfs_bio.c,v 1.41 1996/02/09 19:00:53 christos Exp $	*/
d390 1
@


1.2
log
@From NetBSD: Merge with NetBSD 960217
@
text
@d63 5
d797 4
@


1.1
log
@Initial revision
@
text
@d1 1
a1 1
/*	$NetBSD: vfs_bio.c,v 1.39 1995/08/02 22:01:46 cgd Exp $	*/
d61 1
d101 4
d163 1
a163 1
__inline struct buf *
d202 1
d223 1
d258 1
d273 1
d277 1
a277 1
	int rv, s, sync, wasdelayed;
d339 2
a340 2
vn_bwrite(ap)
	struct vop_bwrite_args *ap;
d342 1
d597 1
d630 1
a630 1
			bp->b_data + bp->b_bufsize, amt);
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@
