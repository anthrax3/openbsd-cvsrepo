head	1.79;
access;
symbols
	OPENBSD_6_2:1.79.0.2
	OPENBSD_6_2_BASE:1.79
	OPENBSD_6_1:1.78.0.4
	OPENBSD_6_1_BASE:1.78
	OPENBSD_6_0:1.74.0.2
	OPENBSD_6_0_BASE:1.74
	OPENBSD_5_9:1.71.0.2
	OPENBSD_5_9_BASE:1.71
	OPENBSD_5_8:1.61.0.6
	OPENBSD_5_8_BASE:1.61
	OPENBSD_5_7:1.61.0.2
	OPENBSD_5_7_BASE:1.61
	OPENBSD_5_6:1.58.0.4
	OPENBSD_5_6_BASE:1.58
	OPENBSD_5_5:1.55.0.4
	OPENBSD_5_5_BASE:1.55
	OPENBSD_5_4:1.50.0.2
	OPENBSD_5_4_BASE:1.50
	OPENBSD_5_3:1.48.0.4
	OPENBSD_5_3_BASE:1.48
	OPENBSD_5_2:1.48.0.2
	OPENBSD_5_2_BASE:1.48
	OPENBSD_5_1_BASE:1.41
	OPENBSD_5_1:1.41.0.4
	OPENBSD_5_0:1.41.0.2
	OPENBSD_5_0_BASE:1.41
	OPENBSD_4_9:1.38.0.4
	OPENBSD_4_9_BASE:1.38
	OPENBSD_4_8:1.38.0.2
	OPENBSD_4_8_BASE:1.38
	OPENBSD_4_7:1.35.0.2
	OPENBSD_4_7_BASE:1.35
	OPENBSD_4_6:1.34.0.4
	OPENBSD_4_6_BASE:1.34
	OPENBSD_4_5:1.33.0.4
	OPENBSD_4_5_BASE:1.33
	OPENBSD_4_4:1.33.0.2
	OPENBSD_4_4_BASE:1.33
	OPENBSD_4_3:1.32.0.2
	OPENBSD_4_3_BASE:1.32
	OPENBSD_4_2:1.31.0.2
	OPENBSD_4_2_BASE:1.31
	OPENBSD_4_1:1.28.0.2
	OPENBSD_4_1_BASE:1.28
	OPENBSD_4_0:1.26.0.4
	OPENBSD_4_0_BASE:1.26
	OPENBSD_3_9:1.26.0.2
	OPENBSD_3_9_BASE:1.26
	OPENBSD_3_8:1.25.0.4
	OPENBSD_3_8_BASE:1.25
	OPENBSD_3_7:1.25.0.2
	OPENBSD_3_7_BASE:1.25
	OPENBSD_3_6:1.24.0.2
	OPENBSD_3_6_BASE:1.24
	SMP_SYNC_A:1.23
	SMP_SYNC_B:1.23
	OPENBSD_3_5:1.22.0.2
	OPENBSD_3_5_BASE:1.22
	OPENBSD_3_4:1.20.0.2
	OPENBSD_3_4_BASE:1.20
	UBC_SYNC_A:1.18
	OPENBSD_3_3:1.18.0.4
	OPENBSD_3_3_BASE:1.18
	OPENBSD_3_2:1.18.0.2
	OPENBSD_3_2_BASE:1.18
	OPENBSD_3_1:1.17.0.2
	OPENBSD_3_1_BASE:1.17
	UBC_SYNC_B:1.18
	UBC:1.10.0.2
	UBC_BASE:1.10
	OPENBSD_3_0:1.9.0.2
	OPENBSD_3_0_BASE:1.9
	SMP:1.8.0.2
	OPENBSD_2_9_BASE:1.7
	OPENBSD_2_9:1.7.0.2;
locks; strict;
comment	@ * @;


1.79
date	2017.05.31.14.52.05;	author mikeb;	state Exp;
branches;
next	1.78;
commitid	AP08WxbAgBQjeXQY;

1.78
date	2017.02.11.19.51.06;	author guenther;	state Exp;
branches;
next	1.77;
commitid	dp1qF9REvzwtzfTw;

1.77
date	2016.09.24.18.39.17;	author tedu;	state Exp;
branches;
next	1.76;
commitid	GXzKcsQAvRY5ZZxt;

1.76
date	2016.09.15.02.00.16;	author dlg;	state Exp;
branches;
next	1.75;
commitid	RlO92XR575sygHqm;

1.75
date	2016.08.25.00.00.02;	author dlg;	state Exp;
branches;
next	1.74;
commitid	iTfOIfeIXH3Xx3fA;

1.74
date	2016.07.14.05.55.08;	author guenther;	state Exp;
branches;
next	1.73;
commitid	lclMlOztWVAw0FCc;

1.73
date	2016.07.14.02.35.17;	author tedu;	state Exp;
branches;
next	1.72;
commitid	MnYJsqw7pOxCgVTu;

1.72
date	2016.05.13.19.05.07;	author tedu;	state Exp;
branches;
next	1.71;
commitid	cIyu9r6pWzjRSEuJ;

1.71
date	2016.01.06.17.58.46;	author tedu;	state Exp;
branches
	1.71.2.1;
next	1.70;
commitid	lpqz8HYKr8elJryh;

1.70
date	2015.12.20.17.56.18;	author tedu;	state Exp;
branches;
next	1.69;
commitid	zhOXoDo5V9y52kdv;

1.69
date	2015.12.17.17.00.48;	author tedu;	state Exp;
branches;
next	1.68;
commitid	rcDW5KET9lryaqw7;

1.68
date	2015.12.17.16.57.20;	author tedu;	state Exp;
branches;
next	1.67;
commitid	RrVbaJSRZIO3MZ6E;

1.67
date	2015.12.05.10.11.53;	author tedu;	state Exp;
branches;
next	1.66;
commitid	Cl55DD2g2xm69E6W;

1.66
date	2015.10.29.13.20.44;	author jsing;	state Exp;
branches;
next	1.65;
commitid	pKeJsXWFzPeltjRz;

1.65
date	2015.10.11.01.53.39;	author guenther;	state Exp;
branches;
next	1.64;
commitid	Bz6GBMDfalLCOrdl;

1.64
date	2015.10.10.16.35.08;	author deraadt;	state Exp;
branches;
next	1.63;
commitid	EF2rFPwyXxtcv9Sh;

1.63
date	2015.10.09.11.47.30;	author deraadt;	state Exp;
branches;
next	1.62;
commitid	2iYAnIFUi5VEzBCv;

1.62
date	2015.10.09.01.11.12;	author deraadt;	state Exp;
branches;
next	1.61;
commitid	925fvYttovA55HOT;

1.61
date	2014.12.19.05.59.21;	author tedu;	state Exp;
branches
	1.61.2.1
	1.61.6.1;
next	1.60;
commitid	zdJTCwdpqRUwO1SL;

1.60
date	2014.12.09.07.05.06;	author doug;	state Exp;
branches;
next	1.59;
commitid	zM5ckwX4kwwmipG0;

1.59
date	2014.11.03.03.08.00;	author deraadt;	state Exp;
branches;
next	1.58;
commitid	3SGDR5EjcwE01W8S;

1.58
date	2014.07.12.18.43.32;	author tedu;	state Exp;
branches
	1.58.4.1;
next	1.57;
commitid	QlVV51SZgNFxsXxC;

1.57
date	2014.05.15.04.43.25;	author guenther;	state Exp;
branches;
next	1.56;

1.56
date	2014.03.30.21.54.48;	author guenther;	state Exp;
branches;
next	1.55;

1.55
date	2014.01.22.02.31.30;	author guenther;	state Exp;
branches;
next	1.54;

1.54
date	2014.01.21.01.48.44;	author tedu;	state Exp;
branches;
next	1.53;

1.53
date	2013.11.14.18.09.39;	author chl;	state Exp;
branches;
next	1.52;

1.52
date	2013.09.14.01.35.00;	author guenther;	state Exp;
branches;
next	1.51;

1.51
date	2013.08.13.05.52.23;	author guenther;	state Exp;
branches;
next	1.50;

1.50
date	2013.05.31.19.01.56;	author yasuoka;	state Exp;
branches;
next	1.49;

1.49
date	2013.04.24.09.52.54;	author nicm;	state Exp;
branches;
next	1.48;

1.48
date	2012.07.08.17.21.08;	author guenther;	state Exp;
branches;
next	1.47;

1.47
date	2012.06.06.04.47.43;	author guenther;	state Exp;
branches;
next	1.46;

1.46
date	2012.04.22.05.43.14;	author guenther;	state Exp;
branches;
next	1.45;

1.45
date	2012.03.25.20.33.54;	author deraadt;	state Exp;
branches;
next	1.44;

1.44
date	2012.03.19.09.05.39;	author guenther;	state Exp;
branches;
next	1.43;

1.43
date	2012.03.10.05.54.28;	author guenther;	state Exp;
branches;
next	1.42;

1.42
date	2012.02.15.04.26.27;	author guenther;	state Exp;
branches;
next	1.41;

1.41
date	2011.07.02.22.20.08;	author nicm;	state Exp;
branches;
next	1.40;

1.40
date	2011.05.03.15.59.50;	author marco;	state Exp;
branches;
next	1.39;

1.39
date	2011.04.02.17.04.35;	author guenther;	state Exp;
branches;
next	1.38;

1.38
date	2010.08.02.19.54.07;	author guenther;	state Exp;
branches;
next	1.37;

1.37
date	2010.07.28.21.44.41;	author nicm;	state Exp;
branches;
next	1.36;

1.36
date	2010.05.18.22.26.09;	author tedu;	state Exp;
branches;
next	1.35;

1.35
date	2009.11.09.17.53.39;	author nicm;	state Exp;
branches;
next	1.34;

1.34
date	2009.06.02.11.04.55;	author guenther;	state Exp;
branches;
next	1.33;

1.33
date	2008.05.06.20.57.19;	author thib;	state Exp;
branches;
next	1.32;

1.32
date	2007.10.29.14.12.19;	author chl;	state Exp;
branches;
next	1.31;

1.31
date	2007.05.30.02.24.59;	author tedu;	state Exp;
branches;
next	1.30;

1.30
date	2007.05.30.00.23.48;	author tedu;	state Exp;
branches;
next	1.29;

1.29
date	2007.03.30.14.21.51;	author reyk;	state Exp;
branches;
next	1.28;

1.28
date	2006.12.01.07.17.25;	author camield;	state Exp;
branches
	1.28.2.1;
next	1.27;

1.27
date	2006.11.15.17.25.40;	author jmc;	state Exp;
branches;
next	1.26;

1.26
date	2005.11.21.18.16.45;	author millert;	state Exp;
branches
	1.26.2.1
	1.26.4.1;
next	1.25;

1.25
date	2004.09.16.18.46.01;	author millert;	state Exp;
branches;
next	1.24;

1.24
date	2004.06.24.19.35.24;	author tholo;	state Exp;
branches;
next	1.23;

1.23
date	2004.04.01.00.27.51;	author tedu;	state Exp;
branches;
next	1.22;

1.22
date	2004.01.12.04.47.01;	author tedu;	state Exp;
branches;
next	1.21;

1.21
date	2003.09.23.16.51.12;	author millert;	state Exp;
branches;
next	1.20;

1.20
date	2003.08.15.20.32.18;	author tedu;	state Exp;
branches;
next	1.19;

1.19
date	2003.06.27.16.20.58;	author nate;	state Exp;
branches;
next	1.18;

1.18
date	2002.10.01.14.06.53;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2002.03.01.12.17.58;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2002.02.08.13.53.28;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2002.02.05.16.02.27;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2002.02.01.15.32.43;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2002.02.01.14.24.08;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2002.01.25.04.03.29;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2002.01.23.00.39.47;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2001.10.26.12.03.27;	author art;	state Exp;
branches
	1.10.2.1;
next	1.9;

1.9
date	2001.07.17.01.51.37;	author provos;	state Exp;
branches;
next	1.8;

1.8
date	2001.05.14.12.38.46;	author art;	state Exp;
branches
	1.8.2.1;
next	1.7;

1.7
date	2001.03.01.20.54.33;	author provos;	state Exp;
branches;
next	1.6;

1.6
date	2000.11.21.21.49.57;	author provos;	state Exp;
branches;
next	1.5;

1.5
date	2000.11.18.22.16.49;	author provos;	state Exp;
branches;
next	1.4;

1.4
date	2000.11.17.06.34.23;	author provos;	state Exp;
branches;
next	1.3;

1.3
date	2000.11.17.05.18.44;	author provos;	state Exp;
branches;
next	1.2;

1.2
date	2000.11.16.20.24.35;	author mickey;	state Exp;
branches;
next	1.1;

1.1
date	2000.11.16.20.02.15;	author provos;	state Exp;
branches;
next	;

1.8.2.1
date	2001.05.14.22.32.40;	author niklas;	state Exp;
branches;
next	1.8.2.2;

1.8.2.2
date	2001.07.04.10.48.16;	author niklas;	state Exp;
branches;
next	1.8.2.3;

1.8.2.3
date	2001.10.31.03.26.29;	author nate;	state Exp;
branches;
next	1.8.2.4;

1.8.2.4
date	2002.03.06.02.13.23;	author niklas;	state Exp;
branches;
next	1.8.2.5;

1.8.2.5
date	2003.03.28.00.41.26;	author niklas;	state Exp;
branches;
next	1.8.2.6;

1.8.2.6
date	2004.02.19.10.56.37;	author niklas;	state Exp;
branches;
next	1.8.2.7;

1.8.2.7
date	2004.06.05.23.13.01;	author niklas;	state Exp;
branches;
next	;

1.10.2.1
date	2002.01.31.22.55.40;	author niklas;	state Exp;
branches;
next	1.10.2.2;

1.10.2.2
date	2002.06.11.03.29.40;	author art;	state Exp;
branches;
next	1.10.2.3;

1.10.2.3
date	2002.10.29.00.36.44;	author art;	state Exp;
branches;
next	;

1.26.2.1
date	2007.03.30.22.33.57;	author ckuethe;	state Exp;
branches;
next	;

1.26.4.1
date	2007.03.30.22.34.43;	author ckuethe;	state Exp;
branches;
next	;

1.28.2.1
date	2007.04.28.01.53.46;	author ckuethe;	state Exp;
branches;
next	;

1.58.4.1
date	2015.10.14.18.12.52;	author sthen;	state Exp;
branches;
next	;
commitid	gSh3HqXyXH6Hrn4w;

1.61.2.1
date	2015.10.14.18.12.25;	author sthen;	state Exp;
branches;
next	;
commitid	lKOoprbyJfUkWbjR;

1.61.6.1
date	2015.10.14.18.10.27;	author sthen;	state Exp;
branches;
next	1.61.6.2;
commitid	Vl7LA9RFQKglNMjm;

1.61.6.2
date	2016.07.14.02.40.40;	author tedu;	state Exp;
branches;
next	;
commitid	ZKmbwCSIkv5fwaPD;

1.71.2.1
date	2016.07.14.02.40.04;	author tedu;	state Exp;
branches;
next	;
commitid	FA373MZWHx41Wzp7;


desc
@@


1.79
log
@Add support for EV_RECEIPT and EV_DISPATCH flags

From FreeBSD via Jan Schreiber <jes at posteo ! de>, thanks!
OK tedu, bluhm
@
text
@/*	$OpenBSD: kern_event.c,v 1.78 2017/02/11 19:51:06 guenther Exp $	*/

/*-
 * Copyright (c) 1999,2000,2001 Jonathan Lemon <jlemon@@FreeBSD.org>
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 * $FreeBSD: src/sys/kern/kern_event.c,v 1.22 2001/02/23 20:32:42 jlemon Exp $
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/kernel.h>
#include <sys/proc.h>
#include <sys/pledge.h>
#include <sys/malloc.h>
#include <sys/unistd.h>
#include <sys/file.h>
#include <sys/filedesc.h>
#include <sys/fcntl.h>
#include <sys/selinfo.h>
#include <sys/queue.h>
#include <sys/event.h>
#include <sys/eventvar.h>
#include <sys/ktrace.h>
#include <sys/pool.h>
#include <sys/protosw.h>
#include <sys/socket.h>
#include <sys/socketvar.h>
#include <sys/stat.h>
#include <sys/uio.h>
#include <sys/mount.h>
#include <sys/poll.h>
#include <sys/syscallargs.h>
#include <sys/timeout.h>

int	kqueue_scan(struct kqueue *kq, int maxevents,
		    struct kevent *ulistp, const struct timespec *timeout,
		    struct proc *p, int *retval);

int	kqueue_read(struct file *fp, off_t *poff, struct uio *uio,
		    struct ucred *cred);
int	kqueue_write(struct file *fp, off_t *poff, struct uio *uio,
		    struct ucred *cred);
int	kqueue_ioctl(struct file *fp, u_long com, caddr_t data,
		    struct proc *p);
int	kqueue_poll(struct file *fp, int events, struct proc *p);
int	kqueue_kqfilter(struct file *fp, struct knote *kn);
int	kqueue_stat(struct file *fp, struct stat *st, struct proc *p);
int	kqueue_close(struct file *fp, struct proc *p);
void	kqueue_wakeup(struct kqueue *kq);

struct fileops kqueueops = {
	kqueue_read,
	kqueue_write,
	kqueue_ioctl,
	kqueue_poll,
	kqueue_kqfilter,
	kqueue_stat,
	kqueue_close
};

void	knote_attach(struct knote *kn, struct filedesc *fdp);
void	knote_drop(struct knote *kn, struct proc *p, struct filedesc *fdp);
void	knote_enqueue(struct knote *kn);
void	knote_dequeue(struct knote *kn);
#define knote_alloc() ((struct knote *)pool_get(&knote_pool, PR_WAITOK))
#define knote_free(kn) pool_put(&knote_pool, (kn))

void	filt_kqdetach(struct knote *kn);
int	filt_kqueue(struct knote *kn, long hint);
int	filt_procattach(struct knote *kn);
void	filt_procdetach(struct knote *kn);
int	filt_proc(struct knote *kn, long hint);
int	filt_fileattach(struct knote *kn);
void	filt_timerexpire(void *knx);
int	filt_timerattach(struct knote *kn);
void	filt_timerdetach(struct knote *kn);
int	filt_timer(struct knote *kn, long hint);
void	filt_seltruedetach(struct knote *kn);

struct filterops kqread_filtops =
	{ 1, NULL, filt_kqdetach, filt_kqueue };
struct filterops proc_filtops =
	{ 0, filt_procattach, filt_procdetach, filt_proc };
struct filterops file_filtops =
	{ 1, filt_fileattach, NULL, NULL };
struct filterops timer_filtops =
        { 0, filt_timerattach, filt_timerdetach, filt_timer };

struct	pool knote_pool;
struct	pool kqueue_pool;
int kq_ntimeouts = 0;
int kq_timeoutmax = (4 * 1024);

#define KNOTE_ACTIVATE(kn) do {						\
	kn->kn_status |= KN_ACTIVE;					\
	if ((kn->kn_status & (KN_QUEUED | KN_DISABLED)) == 0)		\
		knote_enqueue(kn);					\
} while(0)

#define KN_HASH(val, mask)	(((val) ^ (val >> 8)) & (mask))

extern struct filterops sig_filtops;
#ifdef notyet
extern struct filterops aio_filtops;
#endif

/*
 * Table for for all system-defined filters.
 */
struct filterops *sysfilt_ops[] = {
	&file_filtops,			/* EVFILT_READ */
	&file_filtops,			/* EVFILT_WRITE */
	NULL, /*&aio_filtops,*/		/* EVFILT_AIO */
	&file_filtops,			/* EVFILT_VNODE */
	&proc_filtops,			/* EVFILT_PROC */
	&sig_filtops,			/* EVFILT_SIGNAL */
	&timer_filtops,			/* EVFILT_TIMER */
};

void KQREF(struct kqueue *);
void KQRELE(struct kqueue *);

void
KQREF(struct kqueue *kq)
{
	++kq->kq_refs;
}

void
KQRELE(struct kqueue *kq)
{
	if (--kq->kq_refs == 0) {
		pool_put(&kqueue_pool, kq);
	}
}

void kqueue_init(void);

void
kqueue_init(void)
{

	pool_init(&kqueue_pool, sizeof(struct kqueue), 0, IPL_NONE, PR_WAITOK,
	    "kqueuepl", NULL);
	pool_init(&knote_pool, sizeof(struct knote), 0, IPL_NONE, PR_WAITOK,
	    "knotepl", NULL);
}

int
filt_fileattach(struct knote *kn)
{
	struct file *fp = kn->kn_fp;

	return fp->f_ops->fo_kqfilter(fp, kn);
}

int
kqueue_kqfilter(struct file *fp, struct knote *kn)
{
	struct kqueue *kq = kn->kn_fp->f_data;

	if (kn->kn_filter != EVFILT_READ)
		return (EINVAL);

	kn->kn_fop = &kqread_filtops;
	SLIST_INSERT_HEAD(&kq->kq_sel.si_note, kn, kn_selnext);
	return (0);
}

void
filt_kqdetach(struct knote *kn)
{
	struct kqueue *kq = kn->kn_fp->f_data;

	SLIST_REMOVE(&kq->kq_sel.si_note, kn, knote, kn_selnext);
}

int
filt_kqueue(struct knote *kn, long hint)
{
	struct kqueue *kq = kn->kn_fp->f_data;

	kn->kn_data = kq->kq_count;
	return (kn->kn_data > 0);
}

int
filt_procattach(struct knote *kn)
{
	struct process *pr;

	if ((curproc->p_p->ps_flags & PS_PLEDGE) &&
	    (curproc->p_p->ps_pledge & PLEDGE_PROC) == 0)
		return pledge_fail(curproc, EPERM, PLEDGE_PROC);

	if (kn->kn_id > PID_MAX)
		return ESRCH;

	pr = prfind(kn->kn_id);
	if (pr == NULL)
		return (ESRCH);

	/* exiting processes can't be specified */
	if (pr->ps_flags & PS_EXITING)
		return (ESRCH);

	kn->kn_ptr.p_process = pr;
	kn->kn_flags |= EV_CLEAR;		/* automatically set */

	/*
	 * internal flag indicating registration done by kernel
	 */
	if (kn->kn_flags & EV_FLAG1) {
		kn->kn_data = kn->kn_sdata;		/* ppid */
		kn->kn_fflags = NOTE_CHILD;
		kn->kn_flags &= ~EV_FLAG1;
	}

	/* XXX lock the proc here while adding to the list? */
	SLIST_INSERT_HEAD(&pr->ps_klist, kn, kn_selnext);

	return (0);
}

/*
 * The knote may be attached to a different process, which may exit,
 * leaving nothing for the knote to be attached to.  So when the process
 * exits, the knote is marked as DETACHED and also flagged as ONESHOT so
 * it will be deleted when read out.  However, as part of the knote deletion,
 * this routine is called, so a check is needed to avoid actually performing
 * a detach, because the original process does not exist any more.
 */
void
filt_procdetach(struct knote *kn)
{
	struct process *pr = kn->kn_ptr.p_process;

	if (kn->kn_status & KN_DETACHED)
		return;

	/* XXX locking?  this might modify another process. */
	SLIST_REMOVE(&pr->ps_klist, kn, knote, kn_selnext);
}

int
filt_proc(struct knote *kn, long hint)
{
	u_int event;

	/*
	 * mask off extra data
	 */
	event = (u_int)hint & NOTE_PCTRLMASK;

	/*
	 * if the user is interested in this event, record it.
	 */
	if (kn->kn_sfflags & event)
		kn->kn_fflags |= event;

	/*
	 * process is gone, so flag the event as finished and remove it
	 * from the process's klist
	 */
	if (event == NOTE_EXIT) {
		struct process *pr = kn->kn_ptr.p_process;

		kn->kn_status |= KN_DETACHED;
		kn->kn_flags |= (EV_EOF | EV_ONESHOT);
		kn->kn_data = pr->ps_mainproc->p_xstat;
		SLIST_REMOVE(&pr->ps_klist, kn, knote, kn_selnext);
		return (1);
	}

	/*
	 * process forked, and user wants to track the new process,
	 * so attach a new knote to it, and immediately report an
	 * event with the parent's pid.
	 */
	if ((event == NOTE_FORK) && (kn->kn_sfflags & NOTE_TRACK)) {
		struct kevent kev;
		int error;

		/*
		 * register knote with new process.
		 */
		kev.ident = hint & NOTE_PDATAMASK;	/* pid */
		kev.filter = kn->kn_filter;
		kev.flags = kn->kn_flags | EV_ADD | EV_ENABLE | EV_FLAG1;
		kev.fflags = kn->kn_sfflags;
		kev.data = kn->kn_id;			/* parent */
		kev.udata = kn->kn_kevent.udata;	/* preserve udata */
		error = kqueue_register(kn->kn_kq, &kev, NULL);
		if (error)
			kn->kn_fflags |= NOTE_TRACKERR;
	}

	return (kn->kn_fflags != 0);
}

static void
filt_timer_timeout_add(struct knote *kn)
{
	struct timeval tv;
	int tticks;

	tv.tv_sec = kn->kn_sdata / 1000;
	tv.tv_usec = (kn->kn_sdata % 1000) * 1000;
	tticks = tvtohz(&tv);
	timeout_add(kn->kn_hook, tticks ? tticks : 1);
}

void
filt_timerexpire(void *knx)
{
	struct knote *kn = knx;

	kn->kn_data++;
	KNOTE_ACTIVATE(kn);

	if ((kn->kn_flags & EV_ONESHOT) == 0)
		filt_timer_timeout_add(kn);
}


/*
 * data contains amount of time to sleep, in milliseconds
 */
int
filt_timerattach(struct knote *kn)
{
	struct timeout *to;

	if (kq_ntimeouts > kq_timeoutmax)
		return (ENOMEM);
	kq_ntimeouts++;

	kn->kn_flags |= EV_CLEAR;	/* automatically set */
	to = malloc(sizeof(*to), M_KEVENT, M_WAITOK);
	timeout_set(to, filt_timerexpire, kn);
	kn->kn_hook = to;
	filt_timer_timeout_add(kn);

	return (0);
}

void
filt_timerdetach(struct knote *kn)
{
	struct timeout *to;

	to = (struct timeout *)kn->kn_hook;
	timeout_del(to);
	free(to, M_KEVENT, sizeof(*to));
	kq_ntimeouts--;
}

int
filt_timer(struct knote *kn, long hint)
{
	return (kn->kn_data != 0);
}


/*
 * filt_seltrue:
 *
 *	This filter "event" routine simulates seltrue().
 */
int
filt_seltrue(struct knote *kn, long hint)
{

	/*
	 * We don't know how much data can be read/written,
	 * but we know that it *can* be.  This is about as
	 * good as select/poll does as well.
	 */
	kn->kn_data = 0;
	return (1);
}

/*
 * This provides full kqfilter entry for device switch tables, which
 * has same effect as filter using filt_seltrue() as filter method.
 */
void
filt_seltruedetach(struct knote *kn)
{
	/* Nothing to do */
}

const struct filterops seltrue_filtops =
	{ 1, NULL, filt_seltruedetach, filt_seltrue };

int
seltrue_kqfilter(dev_t dev, struct knote *kn)
{
	switch (kn->kn_filter) {
	case EVFILT_READ:
	case EVFILT_WRITE:
		kn->kn_fop = &seltrue_filtops;
		break;
	default:
		return (EINVAL);
	}

	/* Nothing more to do */
	return (0);
}

int
sys_kqueue(struct proc *p, void *v, register_t *retval)
{
	struct filedesc *fdp = p->p_fd;
	struct kqueue *kq;
	struct file *fp;
	int fd, error;

	fdplock(fdp);
	error = falloc(p, 0, &fp, &fd);
	fdpunlock(fdp);
	if (error)
		return (error);
	fp->f_flag = FREAD | FWRITE;
	fp->f_type = DTYPE_KQUEUE;
	fp->f_ops = &kqueueops;
	kq = pool_get(&kqueue_pool, PR_WAITOK|PR_ZERO);
	TAILQ_INIT(&kq->kq_head);
	fp->f_data = kq;
	KQREF(kq);
	*retval = fd;
	if (fdp->fd_knlistsize < 0)
		fdp->fd_knlistsize = 0;		/* this process has a kq */
	kq->kq_fdp = fdp;
	FILE_SET_MATURE(fp, p);
	return (0);
}

int
sys_kevent(struct proc *p, void *v, register_t *retval)
{
	struct filedesc* fdp = p->p_fd;
	struct sys_kevent_args /* {
		syscallarg(int)	fd;
		syscallarg(const struct kevent *) changelist;
		syscallarg(int)	nchanges;
		syscallarg(struct kevent *) eventlist;
		syscallarg(int)	nevents;
		syscallarg(const struct timespec *) timeout;
	} */ *uap = v;
	struct kevent *kevp;
	struct kqueue *kq;
	struct file *fp;
	struct timespec ts;
	int i, n, nerrors, error;

	if ((fp = fd_getfile(fdp, SCARG(uap, fd))) == NULL ||
	    (fp->f_type != DTYPE_KQUEUE))
		return (EBADF);

	FREF(fp);

	if (SCARG(uap, timeout) != NULL) {
		error = copyin(SCARG(uap, timeout), &ts, sizeof(ts));
		if (error)
			goto done;
#ifdef KTRACE
		if (KTRPOINT(p, KTR_STRUCT))
			ktrreltimespec(p, &ts);
#endif
		SCARG(uap, timeout) = &ts;
	}

	kq = fp->f_data;
	nerrors = 0;

	while (SCARG(uap, nchanges) > 0) {
		n = SCARG(uap, nchanges) > KQ_NEVENTS ?
		    KQ_NEVENTS : SCARG(uap, nchanges);
		error = copyin(SCARG(uap, changelist), kq->kq_kev,
		    n * sizeof(struct kevent));
		if (error)
			goto done;
#ifdef KTRACE
		if (KTRPOINT(p, KTR_STRUCT))
			ktrevent(p, kq->kq_kev, n);
#endif
		for (i = 0; i < n; i++) {
			kevp = &kq->kq_kev[i];
			kevp->flags &= ~EV_SYSFLAGS;
			error = kqueue_register(kq, kevp, p);
			if (error || (kevp->flags & EV_RECEIPT)) {
				if (SCARG(uap, nevents) != 0) {
					kevp->flags = EV_ERROR;
					kevp->data = error;
					copyout(kevp, SCARG(uap, eventlist),
					    sizeof(*kevp));
					SCARG(uap, eventlist)++;
					SCARG(uap, nevents)--;
					nerrors++;
				} else {
					goto done;
				}
			}
		}
		SCARG(uap, nchanges) -= n;
		SCARG(uap, changelist) += n;
	}
	if (nerrors) {
		*retval = nerrors;
		error = 0;
		goto done;
	}

	KQREF(kq);
	FRELE(fp, p);
	error = kqueue_scan(kq, SCARG(uap, nevents), SCARG(uap, eventlist),
	    SCARG(uap, timeout), p, &n);
	KQRELE(kq);
	*retval = n;
	return (error);

 done:
	FRELE(fp, p);
	return (error);
}

int
kqueue_register(struct kqueue *kq, struct kevent *kev, struct proc *p)
{
	struct filedesc *fdp = kq->kq_fdp;
	struct filterops *fops = NULL;
	struct file *fp = NULL;
	struct knote *kn = NULL;
	int s, error = 0;

	if (kev->filter < 0) {
		if (kev->filter + EVFILT_SYSCOUNT < 0)
			return (EINVAL);
		fops = sysfilt_ops[~kev->filter];	/* to 0-base index */
	}

	if (fops == NULL) {
		/*
		 * XXX
		 * filter attach routine is responsible for ensuring that
		 * the identifier can be attached to it.
		 */
		return (EINVAL);
	}

	if (fops->f_isfd) {
		/* validate descriptor */
		if (kev->ident > INT_MAX)
			return (EBADF);
		if ((fp = fd_getfile(fdp, kev->ident)) == NULL)
			return (EBADF);
		FREF(fp);

		if (kev->ident < fdp->fd_knlistsize) {
			SLIST_FOREACH(kn, &fdp->fd_knlist[kev->ident], kn_link) {
				if (kq == kn->kn_kq &&
				    kev->filter == kn->kn_filter)
					break;
			}
		}
	} else {
		if (fdp->fd_knhashmask != 0) {
			struct klist *list;

			list = &fdp->fd_knhash[
			    KN_HASH((u_long)kev->ident, fdp->fd_knhashmask)];
			SLIST_FOREACH(kn, list, kn_link) {
				if (kev->ident == kn->kn_id &&
				    kq == kn->kn_kq &&
				    kev->filter == kn->kn_filter)
					break;
			}
		}
	}

	if (kn == NULL && ((kev->flags & EV_ADD) == 0)) {
		error = ENOENT;
		goto done;
	}

	/*
	 * kn now contains the matching knote, or NULL if no match
	 */
	if (kev->flags & EV_ADD) {

		if (kn == NULL) {
			kn = knote_alloc();
			if (kn == NULL) {
				error = ENOMEM;
				goto done;
			}
			kn->kn_fp = fp;
			kn->kn_kq = kq;
			kn->kn_fop = fops;

			/*
			 * apply reference count to knote structure, and
			 * do not release it at the end of this routine.
			 */
			fp = NULL;

			kn->kn_sfflags = kev->fflags;
			kn->kn_sdata = kev->data;
			kev->fflags = 0;
			kev->data = 0;
			kn->kn_kevent = *kev;

			knote_attach(kn, fdp);
			if ((error = fops->f_attach(kn)) != 0) {
				knote_drop(kn, p, fdp);
				goto done;
			}
		} else {
			/*
			 * The user may change some filter values after the
			 * initial EV_ADD, but doing so will not reset any
			 * filters which have already been triggered.
			 */
			kn->kn_sfflags = kev->fflags;
			kn->kn_sdata = kev->data;
			kn->kn_kevent.udata = kev->udata;
		}

		s = splhigh();
		if (kn->kn_fop->f_event(kn, 0))
			KNOTE_ACTIVATE(kn);
		splx(s);

	} else if (kev->flags & EV_DELETE) {
		kn->kn_fop->f_detach(kn);
		knote_drop(kn, p, p->p_fd);
		goto done;
	}

	if ((kev->flags & EV_DISABLE) &&
	    ((kn->kn_status & KN_DISABLED) == 0)) {
		s = splhigh();
		kn->kn_status |= KN_DISABLED;
		splx(s);
	}

	if ((kev->flags & EV_ENABLE) && (kn->kn_status & KN_DISABLED)) {
		s = splhigh();
		kn->kn_status &= ~KN_DISABLED;
		if ((kn->kn_status & KN_ACTIVE) &&
		    ((kn->kn_status & KN_QUEUED) == 0))
			knote_enqueue(kn);
		splx(s);
	}

done:
	if (fp != NULL)
		FRELE(fp, p);
	return (error);
}

int
kqueue_scan(struct kqueue *kq, int maxevents, struct kevent *ulistp,
	const struct timespec *tsp, struct proc *p, int *retval)
{
	struct kevent *kevp;
	struct timeval atv, rtv, ttv;
	struct knote *kn, marker;
	int s, count, timeout, nkev = 0, error = 0;

	count = maxevents;
	if (count == 0)
		goto done;

	if (tsp != NULL) {
		TIMESPEC_TO_TIMEVAL(&atv, tsp);
		if (tsp->tv_sec == 0 && tsp->tv_nsec == 0) {
			/* No timeout, just poll */
			timeout = -1;
			goto start;
		}
		if (itimerfix(&atv)) {
			error = EINVAL;
			goto done;
		}

		timeout = atv.tv_sec > 24 * 60 * 60 ?
		    24 * 60 * 60 * hz : tvtohz(&atv);

		getmicrouptime(&rtv);
		timeradd(&atv, &rtv, &atv);
	} else {
		atv.tv_sec = 0;
		atv.tv_usec = 0;
		timeout = 0;
	}
	goto start;

retry:
	if (atv.tv_sec || atv.tv_usec) {
		getmicrouptime(&rtv);
		if (timercmp(&rtv, &atv, >=))
			goto done;
		ttv = atv;
		timersub(&ttv, &rtv, &ttv);
		timeout = ttv.tv_sec > 24 * 60 * 60 ?
		    24 * 60 * 60 * hz : tvtohz(&ttv);
	}

start:
	if (kq->kq_state & KQ_DYING) {
		error = EBADF;
		goto done;
	}

	kevp = kq->kq_kev;
	s = splhigh();
	if (kq->kq_count == 0) {
		if (timeout < 0) {
			error = EWOULDBLOCK;
		} else {
			kq->kq_state |= KQ_SLEEP;
			error = tsleep(kq, PSOCK | PCATCH, "kqread", timeout);
		}
		splx(s);
		if (error == 0)
			goto retry;
		/* don't restart after signals... */
		if (error == ERESTART)
			error = EINTR;
		else if (error == EWOULDBLOCK)
			error = 0;
		goto done;
	}

	TAILQ_INSERT_TAIL(&kq->kq_head, &marker, kn_tqe);
	while (count) {
		kn = TAILQ_FIRST(&kq->kq_head);
		TAILQ_REMOVE(&kq->kq_head, kn, kn_tqe);
		if (kn == &marker) {
			splx(s);
			if (count == maxevents)
				goto retry;
			goto done;
		}
		if (kn->kn_status & KN_DISABLED) {
			kn->kn_status &= ~KN_QUEUED;
			kq->kq_count--;
			continue;
		}
		if ((kn->kn_flags & EV_ONESHOT) == 0 &&
		    kn->kn_fop->f_event(kn, 0) == 0) {
			kn->kn_status &= ~(KN_QUEUED | KN_ACTIVE);
			kq->kq_count--;
			continue;
		}
		*kevp = kn->kn_kevent;
		kevp++;
		nkev++;
		if (kn->kn_flags & EV_ONESHOT) {
			kn->kn_status &= ~KN_QUEUED;
			kq->kq_count--;
			splx(s);
			kn->kn_fop->f_detach(kn);
			knote_drop(kn, p, p->p_fd);
			s = splhigh();
		} else if (kn->kn_flags & (EV_CLEAR | EV_DISPATCH)) {
			if (kn->kn_flags & EV_CLEAR) {
				kn->kn_data = 0;
				kn->kn_fflags = 0;
			}
			if (kn->kn_flags & EV_DISPATCH)
				kn->kn_status |= KN_DISABLED;
			kn->kn_status &= ~(KN_QUEUED | KN_ACTIVE);
			kq->kq_count--;
		} else {
			TAILQ_INSERT_TAIL(&kq->kq_head, kn, kn_tqe);
		}
		count--;
		if (nkev == KQ_NEVENTS) {
			splx(s);
#ifdef KTRACE
			if (KTRPOINT(p, KTR_STRUCT))
				ktrevent(p, kq->kq_kev, nkev);
#endif
			error = copyout(kq->kq_kev, ulistp,
			    sizeof(struct kevent) * nkev);
			ulistp += nkev;
			nkev = 0;
			kevp = kq->kq_kev;
			s = splhigh();
			if (error)
				break;
		}
	}
	TAILQ_REMOVE(&kq->kq_head, &marker, kn_tqe);
	splx(s);
done:
	if (nkev != 0) {
#ifdef KTRACE
		if (KTRPOINT(p, KTR_STRUCT))
			ktrevent(p, kq->kq_kev, nkev);
#endif
		error = copyout(kq->kq_kev, ulistp,
		    sizeof(struct kevent) * nkev);
	}
	*retval = maxevents - count;
	return (error);
}

/*
 * XXX
 * This could be expanded to call kqueue_scan, if desired.
 */
int
kqueue_read(struct file *fp, off_t *poff, struct uio *uio, struct ucred *cred)
{
	return (ENXIO);
}

int
kqueue_write(struct file *fp, off_t *poff, struct uio *uio, struct ucred *cred)

{
	return (ENXIO);
}

int
kqueue_ioctl(struct file *fp, u_long com, caddr_t data, struct proc *p)
{
	return (ENOTTY);
}

int
kqueue_poll(struct file *fp, int events, struct proc *p)
{
	struct kqueue *kq = (struct kqueue *)fp->f_data;
	int revents = 0;
	int s = splhigh();

	if (events & (POLLIN | POLLRDNORM)) {
		if (kq->kq_count) {
			revents |= events & (POLLIN | POLLRDNORM);
		} else {
			selrecord(p, &kq->kq_sel);
			kq->kq_state |= KQ_SEL;
		}
	}
	splx(s);
	return (revents);
}

int
kqueue_stat(struct file *fp, struct stat *st, struct proc *p)
{
	struct kqueue *kq = fp->f_data;

	memset(st, 0, sizeof(*st));
	st->st_size = kq->kq_count;
	st->st_blksize = sizeof(struct kevent);
	st->st_mode = S_IFIFO;
	return (0);
}

int
kqueue_close(struct file *fp, struct proc *p)
{
	struct kqueue *kq = fp->f_data;
	struct filedesc *fdp = p->p_fd;
	struct knote **knp, *kn, *kn0;
	int i;

	for (i = 0; i < fdp->fd_knlistsize; i++) {
		knp = &SLIST_FIRST(&fdp->fd_knlist[i]);
		kn = *knp;
		while (kn != NULL) {
			kn0 = SLIST_NEXT(kn, kn_link);
			if (kq == kn->kn_kq) {
				kn->kn_fop->f_detach(kn);
				FRELE(kn->kn_fp, p);
				knote_free(kn);
				*knp = kn0;
			} else {
				knp = &SLIST_NEXT(kn, kn_link);
			}
			kn = kn0;
		}
	}
	if (fdp->fd_knhashmask != 0) {
		for (i = 0; i < fdp->fd_knhashmask + 1; i++) {
			knp = &SLIST_FIRST(&fdp->fd_knhash[i]);
			kn = *knp;
			while (kn != NULL) {
				kn0 = SLIST_NEXT(kn, kn_link);
				if (kq == kn->kn_kq) {
					kn->kn_fop->f_detach(kn);
		/* XXX non-fd release of kn->kn_ptr */
					knote_free(kn);
					*knp = kn0;
				} else {
					knp = &SLIST_NEXT(kn, kn_link);
				}
				kn = kn0;
			}
		}
	}
	fp->f_data = NULL;

	kq->kq_state |= KQ_DYING;
	kqueue_wakeup(kq);
	KQRELE(kq);

	return (0);
}

void
kqueue_wakeup(struct kqueue *kq)
{

	if (kq->kq_state & KQ_SLEEP) {
		kq->kq_state &= ~KQ_SLEEP;
		wakeup(kq);
	}
	if (kq->kq_state & KQ_SEL) {
		kq->kq_state &= ~KQ_SEL;
		selwakeup(&kq->kq_sel);
	} else
		KNOTE(&kq->kq_sel.si_note, 0);
}

/*
 * activate one knote.
 */
void
knote_activate(struct knote *kn)
{
	KNOTE_ACTIVATE(kn);
}

/*
 * walk down a list of knotes, activating them if their event has triggered.
 */
void
knote(struct klist *list, long hint)
{
	struct knote *kn, *kn0;

	SLIST_FOREACH_SAFE(kn, list, kn_selnext, kn0)
		if (kn->kn_fop->f_event(kn, hint))
			KNOTE_ACTIVATE(kn);
}

/*
 * remove all knotes from a specified klist
 */
void
knote_remove(struct proc *p, struct klist *list)
{
	struct knote *kn;

	while ((kn = SLIST_FIRST(list)) != NULL) {
		kn->kn_fop->f_detach(kn);
		knote_drop(kn, p, p->p_fd);
	}
}

/*
 * remove all knotes referencing a specified fd
 */
void
knote_fdclose(struct proc *p, int fd)
{
	struct filedesc *fdp = p->p_fd;
	struct klist *list = &fdp->fd_knlist[fd];

	knote_remove(p, list);
}

/*
 * handle a process exiting, including the triggering of NOTE_EXIT notes
 * XXX this could be more efficient, doing a single pass down the klist
 */
void
knote_processexit(struct proc *p)
{
	struct process *pr = p->p_p;

	KNOTE(&pr->ps_klist, NOTE_EXIT);

	/* remove other knotes hanging off the process */
	knote_remove(p, &pr->ps_klist);
}

void
knote_attach(struct knote *kn, struct filedesc *fdp)
{
	struct klist *list;
	int size;

	if (!kn->kn_fop->f_isfd) {
		if (fdp->fd_knhashmask == 0)
			fdp->fd_knhash = hashinit(KN_HASHSIZE, M_TEMP,
			    M_WAITOK, &fdp->fd_knhashmask);
		list = &fdp->fd_knhash[KN_HASH(kn->kn_id, fdp->fd_knhashmask)];
		goto done;
	}

	if (fdp->fd_knlistsize <= kn->kn_id) {
		size = fdp->fd_knlistsize;
		while (size <= kn->kn_id)
			size += KQEXTENT;
		list = mallocarray(size, sizeof(struct klist), M_TEMP,
		    M_WAITOK);
		memcpy(list, fdp->fd_knlist,
		    fdp->fd_knlistsize * sizeof(struct klist));
		memset(&list[fdp->fd_knlistsize], 0,
		    (size - fdp->fd_knlistsize) * sizeof(struct klist));
		free(fdp->fd_knlist, M_TEMP,
		    fdp->fd_knlistsize * sizeof(struct klist));
		fdp->fd_knlistsize = size;
		fdp->fd_knlist = list;
	}
	list = &fdp->fd_knlist[kn->kn_id];
done:
	SLIST_INSERT_HEAD(list, kn, kn_link);
	kn->kn_status = 0;
}

/*
 * should be called at spl == 0, since we don't want to hold spl
 * while calling FRELE and knote_free.
 */
void
knote_drop(struct knote *kn, struct proc *p, struct filedesc *fdp)
{
	struct klist *list;

	if (kn->kn_fop->f_isfd)
		list = &fdp->fd_knlist[kn->kn_id];
	else
		list = &fdp->fd_knhash[KN_HASH(kn->kn_id, fdp->fd_knhashmask)];

	SLIST_REMOVE(list, kn, knote, kn_link);
	if (kn->kn_status & KN_QUEUED)
		knote_dequeue(kn);
	if (kn->kn_fop->f_isfd)
		FRELE(kn->kn_fp, p);
	knote_free(kn);
}


void
knote_enqueue(struct knote *kn)
{
	struct kqueue *kq = kn->kn_kq;
	int s = splhigh();

	KASSERT((kn->kn_status & KN_QUEUED) == 0);

	TAILQ_INSERT_TAIL(&kq->kq_head, kn, kn_tqe);
	kn->kn_status |= KN_QUEUED;
	kq->kq_count++;
	splx(s);
	kqueue_wakeup(kq);
}

void
knote_dequeue(struct knote *kn)
{
	struct kqueue *kq = kn->kn_kq;
	int s = splhigh();

	KASSERT(kn->kn_status & KN_QUEUED);

	TAILQ_REMOVE(&kq->kq_head, kn, kn_tqe);
	kn->kn_status &= ~KN_QUEUED;
	kq->kq_count--;
	splx(s);
}

void
klist_invalidate(struct klist *list)
{
	struct knote *kn;

	SLIST_FOREACH(kn, list, kn_selnext) {
		kn->kn_status |= KN_DETACHED;
		kn->kn_flags |= EV_EOF | EV_ONESHOT;
	}
}
@


1.78
log
@Add a flags argument to falloc() that lets it optionally set the
close-on-exec flag on the newly allocated fd.  Make falloc()'s
return arguments non-optional: assert that they're not NULL.

ok mpi@@ millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.77 2016/09/24 18:39:17 tedu Exp $	*/
d515 1
a515 1
			if (error) {
d791 7
a797 3
		} else if (kn->kn_flags & EV_CLEAR) {
			kn->kn_data = 0;
			kn->kn_fflags = 0;
@


1.77
log
@move knhash size to event.h, use it for hashfree. from Mathieu -
ok guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.76 2016/09/15 02:00:16 dlg Exp $	*/
d443 1
a443 1
	error = falloc(p, &fp, &fd);
@


1.76
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.75 2016/08/25 00:00:02 dlg Exp $	*/
a121 1
#define	KN_HASHSIZE		64		/* XXX should be tunable */
@


1.75
log
@pool_setipl

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.74 2016/07/14 05:55:08 guenther Exp $	*/
d166 1
a166 1
	pool_init(&kqueue_pool, sizeof(struct kqueue), 0, 0, PR_WAITOK,
d168 1
a168 2
	pool_setipl(&kqueue_pool, IPL_NONE);
	pool_init(&knote_pool, sizeof(struct knote), 0, 0, PR_WAITOK,
a169 1
	pool_setipl(&knote_pool, IPL_NONE);
@


1.74
log
@Prevent silly states via knotes on pids > 2^32 and on nonexistent signals.

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.73 2016/07/14 02:35:17 tedu Exp $	*/
d168 1
d171 1
@


1.73
log
@kevent validates that ident is a valid fd by getting the file. one sad
quirk: uint64 to int32 truncation can lead to false positives, and then
later in the array sizing code, very big mallocs panic the kernel.
add a check that the ident isn't larger than INT_MAX in the fd case.
reported by Tim Newsham
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.72 2016/05/13 19:05:07 tedu Exp $	*/
d218 3
@


1.72
log
@contrary to documentation and other implementations, kevent was preventing
a process from watching other users' procs. but there are no secrets here.
remove that check.
at the same time, note that as far as pledge is concerned, while most of
kevent is a "stdio" type operation, process monitoring belongs to the
"proc" family, so add an additional check here.
ok deraadt millert
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.71 2016/01/06 17:58:46 tedu Exp $	*/
d575 2
@


1.71
log
@tidy up whitespace, etc.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.70 2015/12/20 17:56:18 tedu Exp $	*/
d35 1
d215 4
a225 9

	/*
	 * Fail if it's not owned by you, or the last exec gave us
	 * setuid/setgid privs (unless you're root).
	 */
	if (pr != curproc->p_p &&
	    (pr->ps_ucred->cr_ruid != curproc->p_ucred->cr_ruid ||
	    (pr->ps_flags & PS_SUGID)) && suser(curproc, 0) != 0)
		return (EACCES);
@


1.71.2.1
log
@backport 1.73
kevent validates that ident is a valid fd by getting the file. one sad
quirk: uint64 to int32 truncation can lead to false positives, and then
later in the array sizing code, very big mallocs panic the kernel.
add a check that the ident isn't larger than INT_MAX in the fd case.
reported by Tim Newsham
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.71 2016/01/06 17:58:46 tedu Exp $	*/
a578 2
		if (kev->ident > INT_MAX)
			return (EBADF);
@


1.70
log
@size for free, sent and reminded by Mathieu. also delete null check.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.69 2015/12/17 17:00:48 tedu Exp $	*/
d176 1
a176 1
	return ((*fp->f_ops->fo_kqfilter)(fp, kn));
d182 1
a182 1
	struct kqueue *kq = (struct kqueue *)kn->kn_fp->f_data;
d195 1
a195 1
	struct kqueue *kq = (struct kqueue *)kn->kn_fp->f_data;
d203 1
a203 1
	struct kqueue *kq = (struct kqueue *)kn->kn_fp->f_data;
d499 1
a499 1
	kq = (struct kqueue *)fp->f_data;
d503 2
a504 2
		n = SCARG(uap, nchanges) > KQ_NEVENTS
			? KQ_NEVENTS : SCARG(uap, nchanges);
d543 1
a543 1
			    SCARG(uap, timeout), p, &n);
d584 1
a584 1
			SLIST_FOREACH(kn, &fdp->fd_knlist[kev->ident], kn_link)
d588 1
d596 1
a596 1
			SLIST_FOREACH(kn, list, kn_link)
d601 1
d712 1
a712 1
			24 * 60 * 60 * hz : tvtohz(&atv);
d731 1
a731 1
			24 * 60 * 60 * hz : tvtohz(&ttv);
d876 1
a876 1
	struct kqueue *kq = (struct kqueue *)fp->f_data;
d888 1
a888 1
	struct kqueue *kq = (struct kqueue *)fp->f_data;
d1020 1
a1020 1
	if (! kn->kn_fop->f_isfd) {
@


1.69
log
@arrays decay to pointers without needing &
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.68 2015/12/17 16:57:20 tedu Exp $	*/
d1036 2
a1037 2
		if (fdp->fd_knlist != NULL)
			free(fdp->fd_knlist, M_TEMP, 0);
@


1.68
log
@add ktrace to kevent. ok guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.67 2015/12/05 10:11:53 tedu Exp $	*/
d802 1
a802 1
				ktrevent(p, &kq->kq_kev, nkev);
d804 1
a804 1
			error = copyout(&kq->kq_kev, ulistp,
d820 1
a820 1
			ktrevent(p, &kq->kq_kev, nkev);
d822 1
a822 1
		error = copyout(&kq->kq_kev, ulistp,
@


1.67
log
@remove stale lint annotations
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.66 2015/10/29 13:20:44 jsing Exp $	*/
d509 4
d800 4
d817 5
a821 1
	if (nkev != 0)
d824 1
@


1.66
log
@In knote(), use SLIST_FOREACH_SAFE when walking the klist since a call to
an f_event() handler may remove an entry.

Currently knote_processexit() calls knote() with NOTE_EXIT, knote() walks
the list and calls f_event(), one of these happens to be filt_proc(), which
happily SLIST_REMOVEs the NOTE_EXIT note from the same list that knote() is
currently walking, then we get back to knote() and go boom...

Flushed out by bluhm@@'s invalidation change to sys/queue.h and found the
hard way by naddy@@.

ok doug@@ nicm@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.65 2015/10/11 01:53:39 guenther Exp $	*/
a199 1
/*ARGSUSED*/
a819 1
/*ARGSUSED*/
a825 1
/*ARGSUSED*/
a832 1
/*ARGSUSED*/
a838 1
/*ARGSUSED*/
a857 1
/*ARGSUSED*/
a869 1
/*ARGSUSED*/
@


1.65
log
@Always set the timeout at least one tick in the future for EVFILT_TIMER
to avoid looping in softclock()

based on diff by sthen@@
ok sthen@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.64 2015/10/10 16:35:08 deraadt Exp $	*/
d958 1
a958 1
	struct knote *kn;
d960 1
a960 1
	SLIST_FOREACH(kn, list, kn_selnext)
d1058 1
a1058 1
	if (kn->kn_fop->f_isfd) {
a1059 1
	}
@


1.64
log
@shuffle #ifdef TIOCSTI block to avoid a future /*FALLTHROUGH*/ mistake.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.61 2014/12/19 05:59:21 tedu Exp $	*/
d326 12
a341 2
	struct timeval tv;
	int tticks;
d346 2
a347 6
	if ((kn->kn_flags & EV_ONESHOT) == 0) {
		tv.tv_sec = kn->kn_sdata / 1000;
		tv.tv_usec = (kn->kn_sdata % 1000) * 1000;
		tticks = tvtohz(&tv);
		timeout_add((struct timeout *)kn->kn_hook, tticks);
	}
a357 2
	struct timeval tv;
	int tticks;
a362 4
	tv.tv_sec = kn->kn_sdata / 1000;
	tv.tv_usec = (kn->kn_sdata % 1000) * 1000;
	tticks = tvtohz(&tv);

a365 1
	timeout_add(to, tticks);
d367 1
@


1.63
log
@oops, snuck into a syscalls sync; spotted by sthen
@
text
@@


1.62
log
@sync
@
text
@a325 12
static void
filt_timer_timeout_add(struct knote *kn)
{
	struct timeval tv;
	int tticks;

	tv.tv_sec = kn->kn_sdata / 1000;
	tv.tv_usec = (kn->kn_sdata % 1000) * 1000;
	tticks = tvtohz(&tv);
	timeout_add(kn->kn_hook, tticks ? tticks : 1);
}

d330 2
d336 6
a341 2
	if ((kn->kn_flags & EV_ONESHOT) == 0)
		filt_timer_timeout_add(kn);
d352 2
d359 4
d366 1
a367 1
	filt_timer_timeout_add(kn);
@


1.61
log
@start retiring the nointr allocator. specify PR_WAITOK as a flag as a
marker for which pools are not interrupt safe. ok dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.60 2014/12/09 07:05:06 doug Exp $	*/
d326 12
a341 2
	struct timeval tv;
	int tticks;
d346 2
a347 6
	if ((kn->kn_flags & EV_ONESHOT) == 0) {
		tv.tv_sec = kn->kn_sdata / 1000;
		tv.tv_usec = (kn->kn_sdata % 1000) * 1000;
		tticks = tvtohz(&tv);
		timeout_add((struct timeout *)kn->kn_hook, tticks);
	}
a357 2
	struct timeval tv;
	int tticks;
a362 4
	tv.tv_sec = kn->kn_sdata / 1000;
	tv.tv_usec = (kn->kn_sdata % 1000) * 1000;
	tticks = tvtohz(&tv);

a365 1
	timeout_add(to, tticks);
d367 1
@


1.61.2.1
log
@MFC: Always set the timeout at least one tick in the future for EVFILT_TIMER
to avoid looping in softclock()
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.61 2014/12/19 05:59:21 tedu Exp $	*/
a325 12
static void
filt_timer_timeout_add(struct knote *kn)
{
	struct timeval tv;
	int tticks;

	tv.tv_sec = kn->kn_sdata / 1000;
	tv.tv_usec = (kn->kn_sdata % 1000) * 1000;
	tticks = tvtohz(&tv);
	timeout_add(kn->kn_hook, tticks ? tticks : 1);
}

d330 2
d336 6
a341 2
	if ((kn->kn_flags & EV_ONESHOT) == 0)
		filt_timer_timeout_add(kn);
d352 2
d359 4
d366 1
a367 1
	filt_timer_timeout_add(kn);
@


1.61.6.1
log
@MFC: Always set the timeout at least one tick in the future for EVFILT_TIMER
to avoid looping in softclock()
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.61 2014/12/19 05:59:21 tedu Exp $	*/
a325 12
static void
filt_timer_timeout_add(struct knote *kn)
{
	struct timeval tv;
	int tticks;

	tv.tv_sec = kn->kn_sdata / 1000;
	tv.tv_usec = (kn->kn_sdata % 1000) * 1000;
	tticks = tvtohz(&tv);
	timeout_add(kn->kn_hook, tticks ? tticks : 1);
}

d330 2
d336 6
a341 2
	if ((kn->kn_flags & EV_ONESHOT) == 0)
		filt_timer_timeout_add(kn);
d352 2
d359 4
d366 1
a367 1
	filt_timer_timeout_add(kn);
@


1.61.6.2
log
@backport 1.73
kevent validates that ident is a valid fd by getting the file. one sad
quirk: uint64 to int32 truncation can lead to false positives, and then
later in the array sizing code, very big mallocs panic the kernel.
add a check that the ident isn't larger than INT_MAX in the fd case.
reported by Tim Newsham
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.61.6.1 2015/10/14 18:10:27 sthen Exp $	*/
a575 2
		if (kev->ident > INT_MAX)
			return (EBADF);
@


1.60
log
@More malloc() -> mallocarray() in the kernel.

ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.59 2014/11/03 03:08:00 deraadt Exp $	*/
d165 4
a168 4
	pool_init(&kqueue_pool, sizeof(struct kqueue), 0, 0, 0, "kqueuepl",
	    &pool_allocator_nointr);
	pool_init(&knote_pool, sizeof(struct knote), 0, 0, 0, "knotepl",
	    &pool_allocator_nointr);
@


1.59
log
@pass size argument to free()
ok doug tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.58 2014/07/12 18:43:32 tedu Exp $	*/
d1024 2
a1025 1
		list = malloc(size * sizeof(struct klist), M_TEMP, M_WAITOK);
@


1.58
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.57 2014/05/15 04:43:25 guenther Exp $	*/
d379 1
a379 1
	free(to, M_KEVENT, 0);
@


1.58.4.1
log
@MFC: Always set the timeout at least one tick in the future for EVFILT_TIMER
to avoid looping in softclock()
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.58 2014/07/12 18:43:32 tedu Exp $	*/
a325 12
static void
filt_timer_timeout_add(struct knote *kn)
{
	struct timeval tv;
	int tticks;

	tv.tv_sec = kn->kn_sdata / 1000;
	tv.tv_usec = (kn->kn_sdata % 1000) * 1000;
	tticks = tvtohz(&tv);
	timeout_add(kn->kn_hook, tticks ? tticks : 1);
}

d330 2
d336 6
a341 2
	if ((kn->kn_flags & EV_ONESHOT) == 0)
		filt_timer_timeout_add(kn);
d352 2
d359 4
d366 1
a367 1
	filt_timer_timeout_add(kn);
@


1.57
log
@knote_processexit() needs the thread to pass down to FRELE(), so pass it
the exiting thread instead of assuming that that's ps_mainproc.
Also, panic no matter which thread of init takes it down.

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.56 2014/03/30 21:54:48 guenther Exp $	*/
d379 1
a379 1
	free(to, M_KEVENT);
d1030 1
a1030 1
			free(fdp->fd_knlist, M_TEMP);
@


1.56
log
@Eliminates struct pcred by moving the real and saved ugids into
struct ucred; struct process then directly links to the ucred

Based on a discussion at c2k10 or so before noting that FreeBSD and
NetBSD did this too.

ok matthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.55 2014/01/22 02:31:30 guenther Exp $	*/
d996 1
a996 1
knote_processexit(struct process *pr)
d998 2
d1003 1
a1003 1
	knote_remove(pr->ps_mainproc, &pr->ps_klist);
@


1.55
log
@Delete casts in assignments to void* and passed void* arguments.
bcopy() can be memcpy() because target is freshly malloced.
&array[n] is simpler than (char*)array + n*sizeof(array[0])

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.54 2014/01/21 01:48:44 tedu Exp $	*/
d228 1
a228 1
	    (pr->ps_cred->p_ruid != curproc->p_cred->p_ruid ||
@


1.54
log
@bzero -> memset
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.53 2013/11/14 18:09:39 chl Exp $	*/
d455 1
a455 1
	fp->f_data = (caddr_t)kq;
d518 1
a518 2
					(void) copyout((caddr_t)kevp,
					    (caddr_t)SCARG(uap, eventlist),
d797 1
a797 1
			error = copyout((caddr_t)&kq->kq_kev, (caddr_t)ulistp,
d811 1
a811 1
		error = copyout((caddr_t)&kq->kq_kev, (caddr_t)ulistp,
d1023 1
a1023 1
		bcopy((caddr_t)fdp->fd_knlist, (caddr_t)list,
d1025 1
a1025 2
		memset((caddr_t)list +
		    fdp->fd_knlistsize * sizeof(struct klist), 0,
@


1.53
log
@Use (N * sizeof(struct klist)) instead of (N * sizeof(struct klist *))
when malloc'ing struct klist *.

Similar diff found upstream:
http://svnweb.freebsd.org/base?view=revision&revision=197575

Found by LLVM/Clang Static Analyzer.

ok tedu@@ krw@@ guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.52 2013/09/14 01:35:00 guenther Exp $	*/
d870 1
a870 1
	bzero((void *)st, sizeof(*st));
d1026 2
a1027 2
		bzero((caddr_t)list +
		    fdp->fd_knlistsize * sizeof(struct klist),
@


1.52
log
@Snapshots for all archs have been built, so remove the T32 code
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.51 2013/08/13 05:52:23 guenther Exp $	*/
d1023 1
a1023 1
		list = malloc(size * sizeof(struct klist *), M_TEMP, M_WAITOK);
d1025 1
a1025 1
		    fdp->fd_knlistsize * sizeof(struct klist *));
d1027 2
a1028 2
		    fdp->fd_knlistsize * sizeof(struct klist *),
		    (size - fdp->fd_knlistsize) * sizeof(struct klist *));
@


1.51
log
@Switch time_t, ino_t, clock_t, and struct kevent's ident and data
members to 64bit types.  Assign new syscall numbers for (almost
all) the syscalls that involve the affected types, including anything
with time_t, timeval, itimerval, timespec, rusage, dirent, stat,
or kevent arguments.  Add a d_off member to struct dirent and replace
getdirentries() with getdents(), thus immensely simplifying and
accelerating telldir/seekdir.  Build perl with -DBIG_TIME.

Bump the major on every single base library: the compat bits included
here are only good enough to make the transition; the T32 compat
option will be burned as soon as we've reached the new world are
are happy with the snapshots for all architectures.

DANGER: ABI incompatibility.  Updating to this kernel requires extra
work or you won't be able to login: install a snapshot instead.

Much assistance in fixing userland issues from deraadt@@ and tedu@@
and build assistance from todd@@ and otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.50 2013/05/31 19:01:56 yasuoka Exp $	*/
a549 253

#ifdef T32

struct kevent32 {
	u_int		ident;
	short		filter;
	u_short		flags;
	u_int		fflags;
	int		data;
	void		*udata;
};

static int
kqueue32_scan(struct kqueue *kq, int maxevents, struct kevent32 *ulistp,
	const struct timespec *tsp, struct proc *p, int *retval,
	struct kevent32 *k32)
{
	struct kevent32 *kevp;
	struct timeval atv, rtv, ttv;
	struct knote *kn, marker;
	int s, count, timeout, nkev = 0, error = 0;

	count = maxevents;
	if (count == 0)
		goto done;

	if (tsp != NULL) {
		TIMESPEC_TO_TIMEVAL(&atv, tsp);
		if (tsp->tv_sec == 0 && tsp->tv_nsec == 0) {
			/* No timeout, just poll */
			timeout = -1;
			goto start;
		}
		if (itimerfix(&atv)) {
			error = EINVAL;
			goto done;
		}

		timeout = atv.tv_sec > 24 * 60 * 60 ?
			24 * 60 * 60 * hz : tvtohz(&atv);

		getmicrouptime(&rtv);
		timeradd(&atv, &rtv, &atv);
	} else {
		atv.tv_sec = 0;
		atv.tv_usec = 0;
		timeout = 0;
	}
	goto start;

retry:
	if (atv.tv_sec || atv.tv_usec) {
		getmicrouptime(&rtv);
		if (timercmp(&rtv, &atv, >=))
			goto done;
		ttv = atv;
		timersub(&ttv, &rtv, &ttv);
		timeout = ttv.tv_sec > 24 * 60 * 60 ?
			24 * 60 * 60 * hz : tvtohz(&ttv);
	}

start:
	if (kq->kq_state & KQ_DYING) {
		error = EBADF;
		goto done;
	}

	kevp = k32;
	s = splhigh();
	if (kq->kq_count == 0) {
		if (timeout < 0) {
			error = EWOULDBLOCK;
		} else {
			kq->kq_state |= KQ_SLEEP;
			error = tsleep(kq, PSOCK | PCATCH, "kqread", timeout);
		}
		splx(s);
		if (error == 0)
			goto retry;
		/* don't restart after signals... */
		if (error == ERESTART)
			error = EINTR;
		else if (error == EWOULDBLOCK)
			error = 0;
		goto done;
	}

	TAILQ_INSERT_TAIL(&kq->kq_head, &marker, kn_tqe);
	while (count) {
		kn = TAILQ_FIRST(&kq->kq_head);
		TAILQ_REMOVE(&kq->kq_head, kn, kn_tqe);
		if (kn == &marker) {
			splx(s);
			if (count == maxevents)
				goto retry;
			goto done;
		}
		if (kn->kn_status & KN_DISABLED) {
			kn->kn_status &= ~KN_QUEUED;
			kq->kq_count--;
			continue;
		}
		if ((kn->kn_flags & EV_ONESHOT) == 0 &&
		    kn->kn_fop->f_event(kn, 0) == 0) {
			kn->kn_status &= ~(KN_QUEUED | KN_ACTIVE);
			kq->kq_count--;
			continue;
		}
		kevp->ident	= kn->kn_kevent.ident;
		kevp->filter	= kn->kn_kevent.filter;
		kevp->flags	= kn->kn_kevent.flags;
		kevp->fflags	= kn->kn_kevent.fflags;
		kevp->data	= kn->kn_kevent.data;
		kevp->udata	= kn->kn_kevent.udata;
		kevp++;
		nkev++;
		if (kn->kn_flags & EV_ONESHOT) {
			kn->kn_status &= ~KN_QUEUED;
			kq->kq_count--;
			splx(s);
			kn->kn_fop->f_detach(kn);
			knote_drop(kn, p, p->p_fd);
			s = splhigh();
		} else if (kn->kn_flags & EV_CLEAR) {
			kn->kn_data = 0;
			kn->kn_fflags = 0;
			kn->kn_status &= ~(KN_QUEUED | KN_ACTIVE);
			kq->kq_count--;
		} else {
			TAILQ_INSERT_TAIL(&kq->kq_head, kn, kn_tqe);
		}
		count--;
		if (nkev == KQ_NEVENTS) {
			splx(s);
			error = copyout(k32, ulistp, sizeof(*k32) * nkev);
			ulistp += nkev;
			nkev = 0;
			kevp = k32;
			s = splhigh();
			if (error)
				break;
		}
	}
	TAILQ_REMOVE(&kq->kq_head, &marker, kn_tqe);
	splx(s);
done:
	if (nkev != 0)
		error = copyout(k32, ulistp, sizeof(*k32) * nkev);
	*retval = maxevents - count;
	return (error);
}


int
t32_sys_kevent(struct proc *p, void *v, register_t *retval)
{
	struct filedesc* fdp = p->p_fd;
	struct t32_sys_kevent_args /* {
		syscallarg(int)	fd;
		syscallarg(const struct kevent32 *) changelist;
		syscallarg(int)	nchanges;
		syscallarg(struct kevent32 *) eventlist;
		syscallarg(int)	nevents;
		syscallarg(const struct timespec32 *) timeout;
	} */ *uap = v;
	struct kevent32 k32[KQ_NEVENTS];
	struct kevent *kevp;
	struct kqueue *kq;
	struct file *fp;
	struct timespec32 ts32;
	struct timespec ts, *tsp;
	int i, n, nerrors, error;

	if ((fp = fd_getfile(fdp, SCARG(uap, fd))) == NULL ||
	    (fp->f_type != DTYPE_KQUEUE))
		return (EBADF);

	FREF(fp);

	if (SCARG(uap, timeout) != NULL) {
		error = copyin(SCARG(uap, timeout), &ts32, sizeof(ts32));
		if (error)
			goto done;
		TIMESPEC_FROM_32(&ts, &ts32);
#ifdef KTRACE
		if (KTRPOINT(p, KTR_STRUCT))
			ktrreltimespec(p, &ts);
#endif
		tsp = &ts;
	} else
		tsp = NULL;

	kq = (struct kqueue *)fp->f_data;
	nerrors = 0;

	while (SCARG(uap, nchanges) > 0) {
		n = SCARG(uap, nchanges) > KQ_NEVENTS
			? KQ_NEVENTS : SCARG(uap, nchanges);
		error = copyin(SCARG(uap, changelist), k32, n * sizeof(*k32));
		if (error)
			goto done;
		for (i = 0; i < n; i++) {
			kq->kq_kev[i].ident	= k32[i].ident;
			kq->kq_kev[i].filter	= k32[i].filter;
			kq->kq_kev[i].flags	= k32[i].flags;
			kq->kq_kev[i].fflags	= k32[i].fflags;
			kq->kq_kev[i].data	= k32[i].data;
			kq->kq_kev[i].udata	= k32[i].udata;
		}
		for (i = 0; i < n; i++) {
			kevp = &kq->kq_kev[i];
			kevp->flags &= ~EV_SYSFLAGS;
			error = kqueue_register(kq, kevp, p);
			if (error) {
				if (SCARG(uap, nevents) != 0) {
					k32->ident	= kevp->ident;
					k32->filter	= kevp->filter;
					k32->flags	= EV_ERROR;
					k32->fflags	= kevp->fflags;
					k32->data	= error;
					k32->udata	= kevp->udata;
					copyout(k32, SCARG(uap, eventlist),
					    sizeof(*k32));
					SCARG(uap, eventlist)++;
					SCARG(uap, nevents)--;
					nerrors++;
				} else {
					goto done;
				}
			}
		}
		SCARG(uap, nchanges) -= n;
		SCARG(uap, changelist) += n;
	}
	if (nerrors) {
		*retval = nerrors;
		error = 0;
		goto done;
	}

	KQREF(kq);
	FRELE(fp, p);
	error = kqueue32_scan(kq, SCARG(uap, nevents), SCARG(uap, eventlist),
			    tsp, p, &n, k32);
	KQRELE(kq);
	*retval = n;
	return (error);

 done:
	FRELE(fp, p);
	return (error);
}
#endif
@


1.50
log
@On NOTE_EXIT event of EVFILT_PROC, store the exit status in kn_data.

ok guenther tedu deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.49 2013/04/24 09:52:54 nicm Exp $	*/
d550 253
@


1.49
log
@When a ucom(4) is removed, it frees the tty with ttyfree(). However if
anyone is waiting with kqueue their knotes may still have a reference to
the tty and later try to use it in the filt_tty* functions.

To avoid this, walk the knotes in ttyfree(), remove them from the tty's
list and invalidate them by setting kn_hook to NODEV. The filter
functions can then check for this and safely ignore the knotes.

ok tedu matthieu
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.48 2012/07/08 17:21:08 guenther Exp $	*/
d295 1
@


1.48
log
@kevent(EV_DELETE) shouldn't be calling closef(), as that releases POSIX
file locks.  c.f. regress/sys/kern/kqueue/kqueue-flock.c

ok krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.47 2012/06/06 04:47:43 guenther Exp $	*/
d941 9
@


1.47
log
@EVFILT_SIGNAL and EVFILT_PROC events need to track the process they're
attached to and not just the thread, which can go away.

Problem observed by jsg@@; ok jsg@@ matthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.46 2012/04/22 05:43:14 guenther Exp $	*/
a578 1
		fp->f_count++;
a623 2
			if (fp != NULL)
				FRELE(fp, p);
d677 1
a677 1
		closef(fp, p);
a890 1
				FREF(kn->kn_fp);
d892 1
a892 1
				closef(kn->kn_fp, p);
d1032 1
a1032 1
 * while calling closef and free.
d1048 1
a1048 2
		FREF(kn->kn_fp);
		closef(kn->kn_fp, p);
@


1.46
log
@Add struct proc * argument to FRELE() and FILE_SET_MATURE() in
anticipation of further changes to closef().  No binary change.

ok krw@@ miod@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.45 2012/03/25 20:33:54 deraadt Exp $	*/
d213 1
a213 1
	struct proc *p;
d215 2
a216 2
	p = pfind(kn->kn_id);
	if (p == NULL)
d219 2
a220 2
	/* threads and exiting processes can't be specified */
	if (p->p_flag & P_THREAD || p->p_p->ps_flags & PS_EXITING)
d227 3
a229 3
	if (p->p_p != curproc->p_p &&
	    (p->p_cred->p_ruid != curproc->p_cred->p_ruid ||
	    (p->p_p->ps_flags & PS_SUGID)) && suser(curproc, 0) != 0)
d232 1
a232 1
	kn->kn_ptr.p_proc = p;
d245 1
a245 1
	SLIST_INSERT_HEAD(&p->p_p->ps_klist, kn, kn_selnext);
d261 1
a261 1
	struct proc *p = kn->kn_ptr.p_proc;
d267 1
a267 1
	SLIST_REMOVE(&p->p_p->ps_klist, kn, knote, kn_selnext);
d291 1
a291 1
		struct process *pr = kn->kn_ptr.p_proc->p_p;
@


1.45
log
@release the file ref to the kqueue while in kevent(), so that close()
can terminate.  a new ref on the kqueue itself allows us to free it
properly in that case.  wakeups were missing too (for both kevent and poll).
similar to netbsd pr46248. fixes a number of threaded ports.
this version of the fix from matthew.
ok tedu guenther matthew
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.44 2012/03/19 09:05:39 guenther Exp $	*/
d460 1
a460 1
	FILE_SET_MATURE(fp);
d538 1
a538 1
	FRELE(fp);
d546 1
a546 1
	FRELE(fp);
d626 1
a626 1
				FRELE(fp);
@


1.44
log
@Add tracing and dumping of "pointer to struct" syscall arguments for
structs timespec, timeval, sigaction, and rlimit.

ok otto@@ jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.43 2012/03/10 05:54:28 guenther Exp $	*/
d56 1
a56 1
int	kqueue_scan(struct file *fp, int maxevents,
d142 17
d455 1
d537 3
a539 1
	error = kqueue_scan(fp, SCARG(uap, nevents), SCARG(uap, eventlist),
d541 1
d543 2
d685 1
a685 1
kqueue_scan(struct file *fp, int maxevents, struct kevent *ulistp,
a687 1
	struct kqueue *kq = (struct kqueue *)fp->f_data;
d733 5
a922 1
	pool_put(&kqueue_pool, kq);
d924 4
@


1.43
log
@Add PS_EXITING to better differentiate between the process exiting and
the main thread exiting.  c.f. regress/sys/kern/main-thread-exited/
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.42 2012/02/15 04:26:27 guenther Exp $	*/
d44 1
d474 4
@


1.42
log
@Hold struct filedesc's fd_lock when writing to the fd_ofiles, fd_ofileflags,
or fd_{lo,hi}maps members, or when doing a read for a write.  Fixes hangs
when an rthreaded processes sleeps while copying the fd table for fork()
and catches another thread with the lock.

ok jsing@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.41 2011/07/02 22:20:08 nicm Exp $	*/
d202 1
a202 1
	if (p->p_flag & (P_THREAD|P_WEXIT))
@


1.41
log
@kqueue attach functions should return an errno or 0, not a plain 1. Fix
the obvious cases to return EINVAL and ENXIO.

ok tedu deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.40 2011/05/03 15:59:50 marco Exp $	*/
d426 1
d428 1
@


1.40
log
@spaces
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.39 2011/04/02 17:04:35 guenther Exp $	*/
d167 1
a167 1
		return (1);
@


1.39
log
@Move P_SUGID and P_SUGIDEXEC from struct proc to struct process, so
that you can't evade the checks by doing the dirty work in an rthread

ok blambert@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.38 2010/08/02 19:54:07 guenther Exp $	*/
d66 1
a66 1
int 	kqueue_kqfilter(struct file *fp, struct knote *kn);
d328 1
a328 1
 */ 
@


1.38
log
@Fix knote handling for exiting processes: when triggering a NOTE_EXIT
knote, remove it from the process's klist; after handling those,
remove and drop any remaining knotes from the process's klist.  Ban
attaching knotes to processes that have started exiting or attaching
them via the pid of a thread other than the main thread.

ok tedu@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.37 2010/07/28 21:44:41 nicm Exp $	*/
d211 1
a211 1
	    (p->p_flag & P_SUGID)) && suser(curproc, 0) != 0)
@


1.37
log
@Add a dummy kqueue filter similar to seltrue and use it for anything
using seltrue for poll. Based on code from NetBSD.

Also remove a stray duplicate lpt entry from loongson, from deraadt.

ok tedu deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.36 2010/05/18 22:26:09 tedu Exp $	*/
d201 4
d269 2
a270 1
	 * process is gone, so flag the event as finished.
d273 2
d277 1
d947 13
@


1.36
log
@move knote list to struct process.  ok guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.35 2009/11/09 17:53:39 nicm Exp $	*/
d98 1
d379 29
@


1.35
log
@Every selwakeup() should have a matching KNOTE() (even if kqueue isn't
supported it doesn't do any harm), so put the KNOTE() in selwakeup() itself and
remove it from any occurences where both are used, except one for kqueue itself
and one in sys_pipe.c (where the selwakeup is under a PIPE_SEL flag).

Based on a diff from tedu.

ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.34 2009/06/02 11:04:55 guenther Exp $	*/
d222 1
a222 1
	SLIST_INSERT_HEAD(&p->p_klist, kn, kn_selnext);
d244 1
a244 1
	SLIST_REMOVE(&p->p_klist, kn, knote, kn_selnext);
@


1.34
log
@A process should always be able to attach kevent filters to its own
threads, even when it has changed uid or gid in the past.  As is, a
P_SUGID process using rthreads leaks the stack on thread exit.

requested and approved by tedu@@ a while ago
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.33 2008/05/06 20:57:19 thib Exp $	*/
d868 2
a869 2
	}
	KNOTE(&kq->kq_sel.si_note, 0);
@


1.33
log
@Add a PR_ZERO flag for pools, to compliment the M_ZERO
malloc flag, does the same thing.
use it in a few places.

OK tedu@@, "then go ahead. and don't forget the manpage (-:" miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.32 2007/10/29 14:12:19 chl Exp $	*/
d204 2
a205 1
	if ((p->p_cred->p_ruid != curproc->p_cred->p_ruid ||
@


1.32
log
@MALLOC/FREE -> malloc/free
replace an hard coded value with M_WAITOK

ok krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.31 2007/05/30 02:24:59 tedu Exp $	*/
d393 1
a393 2
	kq = pool_get(&kqueue_pool, PR_WAITOK);
	bzero(kq, sizeof(*kq));
@


1.31
log
@openbsd has timeouts, not callouts so rename the variables.  i applied an older
diff from brad.  from brad.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.30 2007/05/30 00:23:48 tedu Exp $	*/
d335 1
a335 1
	MALLOC(to, struct timeout *, sizeof(*to), M_KEVENT, 0);
d350 1
a350 1
	FREE(to, M_KEVENT);
@


1.30
log
@add a new kevent filter type for timers.  this allows processes to create
a series of oneshot or periodic timers.  capped to a global limit.
from freebsd via brad.
ok art pedro
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.29 2007/03/30 14:21:51 reyk Exp $	*/
d94 4
a97 4
void    filt_timerexpire(void *knx);
int     filt_timerattach(struct knote *kn);
void    filt_timerdetach(struct knote *kn);
int     filt_timer(struct knote *kn, long hint);
d110 2
a111 2
int kq_ncallouts = 0;
int kq_calloutmax = (4 * 1024);
d326 1
a326 1
	if (kq_ncallouts > kq_calloutmax)
d328 1
a328 1
	kq_ncallouts++;
d351 1
a351 1
	kq_ncallouts--;
@


1.29
log
@change a misplaced splnet() in kqueue_poll() to splhigh(). fixes a
random panics with kqueue under high load with many events.

tested by me
cookies for dlg@@ deraadt@@
ok dlg@@ tedu@@ art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.28 2006/12/01 07:17:25 camield Exp $	*/
d53 1
d94 4
d105 2
d110 2
d137 1
d296 64
@


1.28
log
@spell kqueue, not kqeue

ok jmc
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.27 2006/11/15 17:25:40 jmc Exp $	*/
d706 1
a706 1
	int s = splnet();
@


1.28.2.1
log
@stability fix from reyk

Original commit message:
============================================================
the splnet() does not make sense at this place and can lead to a panic
with heavy kqueue usage. changing it to splhigh() fixes the problem.

suggested by dlg@@ deraadt@@, ok dlg@@ tedu@@ art@@
============================================================
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.28 2006/12/01 07:17:25 camield Exp $	*/
d706 1
a706 1
	int s = splhigh();
@


1.27
log
@typos; from bret lambert
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.26 2005/11/21 18:16:45 millert Exp $	*/
d136 1
a136 1
	pool_init(&kqueue_pool, sizeof(struct kqueue), 0, 0, 0, "kqeuepl",
@


1.26
log
@Move contents of sys/select.h to sys/selinfo.h in preparation for a
userland-visible sys/select.h.  Consistent with what Net and Free do.
OK deraadt@@, tested with full ports build by naddy@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.25 2004/09/16 18:46:01 millert Exp $	*/
d426 1
a426 1
		 * filter attach routine is responsible for insuring that
d502 1
a502 1
			 * filter which have already been triggered.
@


1.26.4.1
log
@MFC, stability fix from reyk

Original commit message:
============================================================
the splnet() does not make sense at this place and can lead to a panic
with heavy kqueue usage. changing it to splhigh() fixes the problem.

suggested by dlg@@ deraadt@@, ok dlg@@ tedu@@ art@@
============================================================
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.26 2005/11/21 18:16:45 millert Exp $	*/
d706 1
a706 1
	int s = splhigh();
@


1.26.2.1
log
@MFC, stability fix from reyk

Original commit message:
============================================================
the splnet() does not make sense at this place and can lead to a panic
with heavy kqueue usage. changing it to splhigh() fixes the problem.

suggested by dlg@@ deraadt@@, ok dlg@@ tedu@@ art@@
============================================================
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.26 2005/11/21 18:16:45 millert Exp $	*/
d706 1
a706 1
	int s = splhigh();
@


1.25
log
@Don't put parens around the call to pool_put but do use parens to
protect expansion of the kn parameter.  OK miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.24 2004/06/24 19:35:24 tholo Exp $	*/
d40 1
a40 1
#include <sys/select.h>
@


1.24
log
@This moves access to wall and uptime variables in MI code,
encapsulating all such access into wall-defined functions
that makes sure locking is done as needed.

It also cleans up some uses of wall time vs. uptime some
places, but there is sure to be more of these needed as
well, particularily in MD code.  Also, many current calls
to microtime() should probably be changed to getmicrotime(),
or to the {,get}microuptime() versions.

ok art@@ deraadt@@ aaron@@ matthieu@@ beck@@ sturm@@ millert@@ others
"Oh, that is not your problem!" from miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.23 2004/04/01 00:27:51 tedu Exp $	*/
d85 1
a85 1
#define knote_free(kn) (pool_put(&knote_pool, kn))
@


1.23
log
@init kqueues normally, from pedro martelletto
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.22 2004/01/12 04:47:01 tedu Exp $	*/
d548 1
a548 1
	struct timeval atv;
d568 5
a572 4
		s = splclock();
		timeradd(&atv, &time, &atv);
		timeout = hzto(&atv);
		splx(s);
d582 2
a583 2
		timeout = hzto(&atv);
		if (timeout <= 0)
d585 4
@


1.22
log
@klist_invalidate to help clean up when the backend disappears, tested by mpf@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.21 2003/09/23 16:51:12 millert Exp $	*/
a129 1
/* XXX - call this on startup instead. */
a131 2
int kqueue_initialized;

d135 1
a139 2

	kqueue_initialized = 1;
a311 3

	if (!kqueue_initialized)
		kqueue_init();
@


1.21
log
@Replace select backends with poll backends.  selscan() and pollscan()
now call the poll backend.  With this change we implement greater
poll(2) functionality instead of emulating it via the select backend.
Adapted from NetBSD and including some changes from FreeBSD.
Tested by many, deraadt@@ OK
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.20 2003/08/15 20:32:18 tedu Exp $	*/
d926 11
@


1.20
log
@change arguments to suser.  suser now takes the process, and a flags
argument.  old cred only calls user suser_ucred.  this will allow future
work to more flexibly implement the idea of a root process.  looks like
something i saw in freebsd, but a little different.
use of suser_ucred vs suser in file system code should be looked at again,
for the moment semantics remain unchanged.
review and input from art@@  testing and further review miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.19 2003/06/27 16:20:58 nate Exp $	*/
d51 1
d64 1
a64 1
int	kqueue_select(struct file *fp, int which, struct proc *p);
d74 1
a74 1
	kqueue_select,
d704 1
a704 1
kqueue_select(struct file *fp, int which, struct proc *p)
d707 1
a707 1
	int res = 0;
d710 1
a710 1
	if (which == FREAD) {
d712 1
a712 1
			res = 1;
d719 1
a719 1
	return (res);
@


1.19
log
@filter event that simulates seltrue(). From NetBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.18 2002/10/01 14:06:53 art Exp $	*/
d198 1
a198 2
	        (p->p_flag & P_SUGID)) &&
	    suser(curproc->p_ucred, &curproc->p_acflag) != 0)
@


1.18
log
@Prevent two localhost crashes with proc filters.
 - don't send NOTE_SIGNAL to exiting processes.
 - null dereference on error condition.
from Peter Werner <peterw at ifost.org.au>
deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.17 2002/03/01 12:17:58 art Exp $	*/
d289 18
@


1.17
log
@fp doesn't necessarily have to be set when we do the early FRELE.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.16 2002/02/08 13:53:28 art Exp $	*/
d80 1
a80 1
void	knote_drop(struct knote *kn, struct proc *p);
d484 1
a484 1
				knote_drop(kn, p);
d505 1
a505 1
		knote_drop(kn, p);
d625 1
a625 1
			knote_drop(kn, p);
d805 1
a805 1
		knote_drop(kn, p);
d861 1
a861 1
knote_drop(struct knote *kn, struct proc *p)
a862 1
	struct filedesc *fdp = p->p_fd;
@


1.16
log
@- Rename FILE_{,UN}USE to FREF and FRELE. USE is a bad verb and we don't have
  the same semantics as NetBSD anyway, so it's good to avoid name collissions.
- Always fdremove before freeing the file, not the other way around.
- falloc FREFs the file.
- have FILE_SET_MATURE FRELE the file (It feels like a good ortogonality to
  falloc FREFing the file).
- Use closef as much as possible instead of ffree in error paths of
  falloc:ing functions. closef is much more careful with the fd and can
  deal with the fd being forcibly closed by dup2. Also try to avoid
  manually calling *fo_close when closef can do that for us (this makes
  some error paths mroe complicated (sys_socketpair and sys_pipe), but
  others become simpler (sys_open)).
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.15 2002/02/05 16:02:27 art Exp $	*/
d472 2
a473 1
			FRELE(fp);
@


1.15
log
@Add counting of temporary references to a struct file (as opposed to references
from fd tables and other long-lived objects). This is to avoid races between
using a file descriptor and having another process (with shared fd table)
close it. We use a separate refence count so that error values from close(2)
will be correctly returned to the caller of close(2).

The macros for those reference counts are FILE_USE(fp) and FILE_UNUSE(fp).

Make sure that the cases where closef can be called "incorrectly" (most notably
dup2(2)) are handled.

Right now only callers of closef (and {,p}read) use FILE_{,UN}USE correctly,
more fixes incoming soon.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.14 2002/02/01 15:32:43 art Exp $	*/
d342 1
a342 1
	FILE_USE(fp);
d393 1
a393 1
	FILE_UNUSE(fp);
d425 1
a425 1
		FILE_USE(fp);
d472 1
a472 1
			FILE_UNUSE(fp);
d731 1
a731 1
				FILE_USE(kn->kn_fp);
d874 1
a874 1
		FILE_USE(kn->kn_fp);
@


1.14
log
@Allocate kqueues with pool.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.13 2002/02/01 14:24:08 art Exp $	*/
d334 1
a334 1
	struct file *fp = NULL;
d342 1
a342 1
	fp->f_count++;
d393 1
a393 2
	if (fp != NULL)
		closef(fp, p);
d425 1
d472 1
d731 1
d873 2
a874 1
	if (kn->kn_fop->f_isfd)
d876 1
@


1.13
log
@Don't MALLOC with variable size.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.12 2002/01/25 04:03:29 art Exp $	*/
d83 2
a84 3
void	knote_init(void);
struct	knote *knote_alloc(void);
void	knote_free(struct knote *kn);
d101 1
d129 16
d299 3
d308 1
a308 1
	kq = malloc(sizeof(struct kqueue), M_TEMP, M_WAITOK);
d758 1
a758 1
	free(kq, M_TEMP);
a903 26
}

void
knote_init(void)
{
	pool_init(&knote_pool, sizeof(struct knote), 0, 0, 0, "knotepl",
	    &pool_allocator_nointr);
}

struct knote *
knote_alloc(void)
{
	static int knote_pool_initialised;

	if (!knote_pool_initialised) { 
		knote_init();
		knote_pool_initialised++;
	}

	return (pool_get(&knote_pool, PR_WAITOK));
}

void
knote_free(struct knote *kn)
{
	pool_put(&knote_pool, kn);
@


1.12
log
@kernel printfs triggerable by a simple mistake in userland are just wrong.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.11 2002/01/23 00:39:47 art Exp $	*/
d817 1
a817 2
		MALLOC(list, struct klist *,
		    size * sizeof(struct klist *), M_TEMP, M_WAITOK);
d824 1
a824 1
			FREE(fdp->fd_knlist, M_TEMP);
@


1.11
log
@Pool deals fairly well with physical memory shortage, but it doesn't deal
well (not at all) with shortages of the vm_map where the pages are mapped
(usually kmem_map).

Try to deal with it:
 - group all information the backend allocator for a pool in a separate
   struct. The pool will only have a pointer to that struct.
 - change the pool_init API to reflect that.
 - link all pools allocating from the same allocator on a linked list.
 - Since an allocator is responsible to wait for physical memory it will
   only fail (waitok) when it runs out of its backing vm_map, carefully
   drain pools using the same allocator so that va space is freed.
   (see comments in code for caveats and details).
 - change pool_reclaim to return if it actually succeeded to free some
   memory, use that information to make draining easier and more efficient.
 - get rid of PR_URGENT, noone uses it.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.10 2001/10/26 12:03:27 art Exp $	*/
a399 1
		printf("unknown filter: %d\n", kev->filter);
@


1.10
log
@ - every new fd created by falloc() is marked as larval and should not be used
   any anyone. Every caller of falloc matures the fd when it's usable.
 - Since every lookup in the fd table must now check this flag and all of
   them do the same thing, move all the necessary checks into a function -
   fd_getfile.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.9 2001/07/17 01:51:37 provos Exp $	*/
d893 1
a893 1
	    0, pool_page_alloc_nointr, pool_page_free_nointr, M_KNOTE);
@


1.10.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.12 2002/01/25 04:03:29 art Exp $	*/
d400 1
d893 1
a893 1
	    &pool_allocator_nointr);
@


1.10.2.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.10.2.1 2002/01/31 22:55:40 niklas Exp $	*/
d83 3
a85 2
#define knote_alloc() ((struct knote *)pool_get(&knote_pool, PR_WAITOK))
#define knote_free(kn) (pool_put(&knote_pool, kn))
a101 1
struct	pool kqueue_pool;
a128 16
/* XXX - call this on startup instead. */
void kqueue_init(void);

int kqueue_initialized;

void
kqueue_init(void)
{
	pool_init(&kqueue_pool, sizeof(struct kqueue), 0, 0, 0, "kqeuepl",
	    &pool_allocator_nointr);
	pool_init(&knote_pool, sizeof(struct knote), 0, 0, 0, "knotepl",
	    &pool_allocator_nointr);

	kqueue_initialized = 1;
}

a282 3
	if (!kqueue_initialized)
		kqueue_init();

d289 1
a289 1
	kq = pool_get(&kqueue_pool, PR_WAITOK);
d315 1
a315 1
	struct file *fp;
d323 1
a323 1
	FREF(fp);
d374 2
a375 1
	FRELE(fp);
a406 1
		FREF(fp);
a452 2
			if (fp != NULL)
				FRELE(fp);
a710 1
				FREF(kn->kn_fp);
d739 1
a739 1
	pool_put(&kqueue_pool, kq);
d817 2
a818 1
		list = malloc(size * sizeof(struct klist *), M_TEMP, M_WAITOK);
d825 1
a825 1
			free(fdp->fd_knlist, M_TEMP);
d853 1
a853 2
	if (kn->kn_fop->f_isfd) {
		FREF(kn->kn_fp);
a854 1
	}
d886 26
@


1.10.2.3
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.10.2.2 2002/06/11 03:29:40 art Exp $	*/
d80 1
a80 1
void	knote_drop(struct knote *kn, struct proc *p, struct filedesc *fdp);
d484 1
a484 1
				knote_drop(kn, p, fdp);
d505 1
a505 1
		knote_drop(kn, p, p->p_fd);
d625 1
a625 1
			knote_drop(kn, p, p->p_fd);
d805 1
a805 1
		knote_drop(kn, p, p->p_fd);
d861 1
a861 1
knote_drop(struct knote *kn, struct proc *p, struct filedesc *fdp)
d863 1
@


1.9
log
@use pool allocator for knotes, adapted from lukem@@netbsd, okay art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.8 2001/05/14 12:38:46 art Exp $	*/
d297 2
a298 1
	return (error);
d319 1
a319 2
	if (((u_int)SCARG(uap, fd)) >= fdp->fd_nfiles ||
	    (fp = fdp->fd_ofiles[SCARG(uap, fd)]) == NULL ||
d406 1
a406 2
		if ((u_int)kev->ident >= fdp->fd_nfiles ||
		    (fp = fdp->fd_ofiles[kev->ident]) == NULL)
@


1.8
log
@Add a fo_stat member to struct fileops. Used soon.
Also add a stat function for kqueue from FreeBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.7 2001/03/01 20:54:33 provos Exp $	*/
d44 1
d83 1
d101 2
d890 7
d900 8
a907 1
	return (malloc(sizeof (struct knote), M_KNOTE, M_NOWAIT));
d913 1
a913 1
	free(kn, M_KNOTE);
@


1.8.2.1
log
@merge in approximately 2.9 into SMP branch
@
text
@d64 1
d74 1
d679 13
@


1.8.2.2
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.8.2.1 2001/05/14 22:32:40 niklas Exp $	*/
a63 1
int	kqueue_stat(struct file *fp, struct stat *st, struct proc *p);
a72 1
	kqueue_stat,
a676 13
}

/*ARGSUSED*/
int
kqueue_stat(struct file *fp, struct stat *st, struct proc *p)
{
	struct kqueue *kq = (struct kqueue *)fp->f_data;

	bzero((void *)st, sizeof(*st));
	st->st_size = kq->kq_count;
	st->st_blksize = sizeof(struct kevent);
	st->st_mode = S_IFIFO;
	return (0);
@


1.8.2.3
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.8.2.2 2001/07/04 10:48:16 niklas Exp $	*/
a43 1
#include <sys/pool.h>
a81 1
void	knote_init(void);
a98 2
struct	pool knote_pool;

d293 1
a293 2
	FILE_SET_MATURE(fp);
	return (0);
d314 2
a315 1
	if ((fp = fd_getfile(fdp, SCARG(uap, fd))) == NULL ||
d402 2
a403 1
		if ((fp = fd_getfile(fdp, kev->ident)) == NULL)
a885 7
void
knote_init(void)
{
	pool_init(&knote_pool, sizeof(struct knote), 0, 0, 0, "knotepl",
	    0, pool_page_alloc_nointr, pool_page_free_nointr, M_KNOTE);
}

d889 1
a889 8
	static int knote_pool_initialised;

	if (!knote_pool_initialised) { 
		knote_init();
		knote_pool_initialised++;
	}

	return (pool_get(&knote_pool, PR_WAITOK));
d895 1
a895 1
	pool_put(&knote_pool, kn);
@


1.8.2.4
log
@Merge in trunk
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d83 3
a85 2
#define knote_alloc() ((struct knote *)pool_get(&knote_pool, PR_WAITOK))
#define knote_free(kn) (pool_put(&knote_pool, kn))
a101 1
struct	pool kqueue_pool;
a128 16
/* XXX - call this on startup instead. */
void kqueue_init(void);

int kqueue_initialized;

void
kqueue_init(void)
{
	pool_init(&kqueue_pool, sizeof(struct kqueue), 0, 0, 0, "kqeuepl",
	    &pool_allocator_nointr);
	pool_init(&knote_pool, sizeof(struct knote), 0, 0, 0, "knotepl",
	    &pool_allocator_nointr);

	kqueue_initialized = 1;
}

a282 3
	if (!kqueue_initialized)
		kqueue_init();

d289 1
a289 1
	kq = pool_get(&kqueue_pool, PR_WAITOK);
d315 1
a315 1
	struct file *fp;
d323 1
a323 1
	FREF(fp);
d374 2
a375 1
	FRELE(fp);
d400 1
a407 1
		FREF(fp);
a453 2
			if (fp != NULL)
				FRELE(fp);
a711 1
				FREF(kn->kn_fp);
d740 1
a740 1
	pool_put(&kqueue_pool, kq);
d818 2
a819 1
		list = malloc(size * sizeof(struct klist *), M_TEMP, M_WAITOK);
d826 1
a826 1
			free(fdp->fd_knlist, M_TEMP);
d854 1
a854 2
	if (kn->kn_fop->f_isfd) {
		FREF(kn->kn_fp);
a855 1
	}
d887 26
@


1.8.2.5
log
@Sync the SMP branch with 3.3
@
text
@d80 1
a80 1
void	knote_drop(struct knote *kn, struct proc *p, struct filedesc *fdp);
d484 1
a484 1
				knote_drop(kn, p, fdp);
d505 1
a505 1
		knote_drop(kn, p, p->p_fd);
d625 1
a625 1
			knote_drop(kn, p, p->p_fd);
d805 1
a805 1
		knote_drop(kn, p, p->p_fd);
d861 1
a861 1
knote_drop(struct knote *kn, struct proc *p, struct filedesc *fdp)
d863 1
@


1.8.2.6
log
@Merge of current from two weeks agointo the SMP branch
@
text
@a50 1
#include <sys/poll.h>
d63 1
a63 1
int	kqueue_poll(struct file *fp, int events, struct proc *p);
d73 1
a73 1
	kqueue_poll,
d198 2
a199 1
	    (p->p_flag & P_SUGID)) && suser(curproc, 0) != 0)
a290 18
/*
 * filt_seltrue:
 *
 *	This filter "event" routine simulates seltrue().
 */
int
filt_seltrue(struct knote *kn, long hint)
{

	/*
	 * We don't know how much data can be read/written,
	 * but we know that it *can* be.  This is about as
	 * good as select/poll does as well.
	 */
	kn->kn_data = 0;
	return (1);
}

d686 1
a686 1
kqueue_poll(struct file *fp, int events, struct proc *p)
d689 1
a689 1
	int revents = 0;
d692 1
a692 1
	if (events & (POLLIN | POLLRDNORM)) {
d694 1
a694 1
			revents |= events & (POLLIN | POLLRDNORM);
d701 1
a701 1
	return (revents);
a907 11
}

void
klist_invalidate(struct klist *list)
{
	struct knote *kn;

	SLIST_FOREACH(kn, list, kn_selnext) {
		kn->kn_status |= KN_DETACHED;
		kn->kn_flags |= EV_EOF | EV_ONESHOT;
	}
@


1.8.2.7
log
@Merge with the trunk
@
text
@d130 1
d133 2
a137 1

d142 2
d316 3
@


1.7
log
@port kqueue changes from freebsd, plus all required openbsd glue.
okay deraadt@@, millert@@
from jlemon@@freebsd.org:
extend kqueue down to the device layer, backwards compatible approach
suggested by peter@@freebsd.org
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.6 2000/11/21 21:49:57 provos Exp $	*/
d64 1
d74 1
d679 13
@


1.6
log
@support for kernel events on vnodes, from jlemon@@freebsd.org, okay art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.5 2000/11/18 22:16:49 provos Exp $	*/
d4 1
a4 1
 * Copyright (c) 1999,2000 Jonathan Lemon <jlemon@@FreeBSD.org>
d28 1
a28 1
 * $FreeBSD: src/sys/kern/kern_event.c,v 1.15 2000/08/07 16:45:42 jlemon Exp $
a51 9
int	filt_nullattach(struct knote *kn);
int	filt_rwtypattach(struct knote *kn);
int	filt_kqattach(struct knote *kn);
void	filt_kqdetach(struct knote *kn);
int	filt_kqueue(struct knote *kn, long hint);
int	filt_procattach(struct knote *kn);
void	filt_procdetach(struct knote *kn);
int	filt_proc(struct knote *kn, long hint);

d63 1
d67 9
d83 14
a105 20
struct fileops kqueueops = {
	kqueue_read,
	kqueue_write,
	kqueue_ioctl,
	kqueue_select,
	kqueue_close
};

extern struct filterops so_rwfiltops[];
extern struct filterops pipe_rwfiltops[];
#ifdef notyet
extern struct filterops fifo_rwfiltops[];
#endif
extern struct filterops vn_rwfiltops[];

struct filterops kq_rwfiltops[] = {
    { 1, filt_kqattach, filt_kqdetach, filt_kqueue },
    { 1, filt_nullattach, NULL, NULL },
};

a109 19
extern struct filterops vn_filtops;

struct filterops rwtype_filtops =
	{ 1, filt_rwtypattach, NULL, NULL };
struct filterops proc_filtops =
	{ 0, filt_procattach, filt_procdetach, filt_proc };

/*
 * XXX
 * These must match the order of defines in <sys/file.h>
 */
struct filterops *rwtypfilt_sw[] = {
	NULL,				/* 0 */
	vn_rwfiltops,			/* DTYPE_VNODE */
	so_rwfiltops,			/* DTYPE_SOCKET */
	pipe_rwfiltops,			/* DTYPE_PIPE */
	/* fifo_rwfiltops, */		/* DTYPE_FIFO */
	kq_rwfiltops,			/* DTYPE_KQUEUE */
};
d112 1
a112 1
 * table for all system-defined filters.
d115 2
a116 2
	&rwtype_filtops,		/* EVFILT_READ */
	&rwtype_filtops,		/* EVFILT_WRITE */
d118 1
a118 1
	&vn_filtops,			/* EVFILT_VNODE */
d124 1
a124 1
filt_nullattach(struct knote *kn)
d126 1
a126 2
	return (ENXIO);
}
d128 1
a128 13
/*
 * file-type specific attach routine for read/write filters
 */
int
filt_rwtypattach(struct knote *kn)
{
	struct filterops *fops;

	fops = rwtypfilt_sw[kn->kn_fp->f_type];
	if (fops == NULL)
		return (EINVAL);
	kn->kn_fop = &fops[~kn->kn_filter];	/* convert to 0-base index */
	return (kn->kn_fop->f_attach(kn));
d132 1
a132 1
filt_kqattach(struct knote *kn)
d136 4
d308 1
a308 1
	struct file *fp;
d317 2
d322 1
a322 1
			return error;
d335 1
a335 1
			return (error);
d351 1
a351 1
					return (error);
d360 2
a361 1
		return (0);
d367 3
d403 1
d425 4
a428 2
	if (kn == NULL && ((kev->flags & EV_ADD) == 0))
		return (ENOENT);
d437 4
a440 4
			if (kn == NULL)
				return (ENOMEM);
			if (fp != NULL)
				fp->f_count++;
d445 6
d501 2
@


1.5
log
@better permission check; okay art@@, millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.4 2000/11/17 06:34:23 provos Exp $	*/
d103 1
a104 1
#endif
d114 1
a115 1
#endif
d128 1
a128 1
	NULL, /* vn_rwfiltops, */	/* DTYPE_VNODE */
d142 1
a142 1
	NULL, /*&vn_filtops,*/		/* EVFILT_VNODE */
@


1.4
log
@calculate timeouts correctly, simpler.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.3 2000/11/17 05:18:44 provos Exp $	*/
d203 8
a210 1
	if (suser(p->p_ucred, &p->p_acflag) != 0)
@


1.3
log
@correct timersub
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_event.c,v 1.2 2000/11/16 20:24:35 mickey Exp $	*/
d521 1
a521 1
	struct timeval atv, ttv;
d531 5
d540 2
a541 5
		if (tsp->tv_sec == 0 && tsp->tv_nsec == 0)
			timeout = -1;
		else
			timeout = atv.tv_sec > 24 * 60 * 60 ?
			    24 * 60 * 60 * hz : hzto(&atv);
d543 2
d554 2
a555 1
		if (timercmp(&time, &atv, >=))
a556 4
		ttv = atv;
		timersub(&ttv, &time, &ttv);
		timeout = ttv.tv_sec > 24 * 60 * 60 ?
			24 * 60 * 60 * hz : hzto(&ttv);
@


1.2
log
@rcsid; fix comment; lots of bad tabs and spaces
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d553 1
a553 1
		timersub(&ttv, &ttv, &time);
@


1.1
log
@support kernel event queues, from FreeBSD by Jonathan Lemon,
okay art@@, millert@@
@
text
@d1 2
d35 1
a35 1
#include <sys/malloc.h> 
d52 2
a53 2
int 	filt_nullattach(struct knote *kn);
int 	filt_rwtypattach(struct knote *kn);
d65 1
a65 1
int 	kqueue_read(struct file *fp, off_t *poff, struct uio *uio,
d71 10
a80 10
int 	kqueue_select(struct file *fp, int which, struct proc *p);
int 	kqueue_close(struct file *fp, struct proc *p);
void 	kqueue_wakeup(struct kqueue *kq);

void 	knote_attach(struct knote *kn, struct filedesc *fdp);
void 	knote_drop(struct knote *kn, struct proc *p);
void 	knote_enqueue(struct knote *kn);
void 	knote_dequeue(struct knote *kn);
struct 	knote *knote_alloc(void);
void 	knote_free(struct knote *kn);
d82 1
a82 1
#define KNOTE_ACTIVATE(kn) do { 					\
d136 1
a136 1
 * table for for all system-defined filters.
d265 1
a265 1
		kn->kn_flags |= (EV_EOF | EV_ONESHOT); 
d338 2
a339 2
        if (((u_int)SCARG(uap, fd)) >= fdp->fd_nfiles ||
            (fp = fdp->fd_ofiles[SCARG(uap, fd)]) == NULL ||
d383 1
a383 1
        	*retval = nerrors;
d433 1
a433 1
			
d476 1
a476 1
			 * initial EV_ADD, but doing so will not reset any 
d537 1
a537 1
		else 
d562 1
a562 1
		if (timeout < 0) { 
d579 1
a579 1
	TAILQ_INSERT_TAIL(&kq->kq_head, &marker, kn_tqe); 
d582 1
a582 1
		TAILQ_REMOVE(&kq->kq_head, kn, kn_tqe); 
d616 1
a616 1
			TAILQ_INSERT_TAIL(&kq->kq_head, kn, kn_tqe); 
d631 1
a631 1
	TAILQ_REMOVE(&kq->kq_head, &marker, kn_tqe); 
d637 1
a637 1
        *retval = maxevents - count;
d675 3
a677 3
        if (which == FREAD) {
                if (kq->kq_count) {
                        res = 1;
d679 1
a679 1
                        selrecord(p, &kq->kq_sel);
d833 1
a833 1
        struct filedesc *fdp = p->p_fd;
d858 1
a858 1
	TAILQ_INSERT_TAIL(&kq->kq_head, kn, kn_tqe); 
d873 1
a873 1
	TAILQ_REMOVE(&kq->kq_head, kn, kn_tqe); 
@

