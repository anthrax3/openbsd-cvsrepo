head	1.43;
access;
symbols
	OPENBSD_6_2:1.43.0.2
	OPENBSD_6_2_BASE:1.43
	OPENBSD_6_1:1.43.0.4
	OPENBSD_6_1_BASE:1.43
	OPENBSD_6_0:1.42.0.2
	OPENBSD_6_0_BASE:1.42
	OPENBSD_5_9:1.41.0.2
	OPENBSD_5_9_BASE:1.41
	OPENBSD_5_8:1.41.0.4
	OPENBSD_5_8_BASE:1.41
	OPENBSD_5_7:1.40.0.2
	OPENBSD_5_7_BASE:1.40
	OPENBSD_5_6:1.39.0.16
	OPENBSD_5_6_BASE:1.39
	OPENBSD_5_5:1.39.0.14
	OPENBSD_5_5_BASE:1.39
	OPENBSD_5_4:1.39.0.10
	OPENBSD_5_4_BASE:1.39
	OPENBSD_5_3:1.39.0.8
	OPENBSD_5_3_BASE:1.39
	OPENBSD_5_2:1.39.0.6
	OPENBSD_5_2_BASE:1.39
	OPENBSD_5_1_BASE:1.39
	OPENBSD_5_1:1.39.0.4
	OPENBSD_5_0:1.39.0.2
	OPENBSD_5_0_BASE:1.39
	OPENBSD_4_9:1.37.0.12
	OPENBSD_4_9_BASE:1.37
	OPENBSD_4_8:1.37.0.10
	OPENBSD_4_8_BASE:1.37
	OPENBSD_4_7:1.37.0.6
	OPENBSD_4_7_BASE:1.37
	OPENBSD_4_6:1.37.0.8
	OPENBSD_4_6_BASE:1.37
	OPENBSD_4_5:1.37.0.4
	OPENBSD_4_5_BASE:1.37
	OPENBSD_4_4:1.37.0.2
	OPENBSD_4_4_BASE:1.37
	OPENBSD_4_3:1.36.0.2
	OPENBSD_4_3_BASE:1.36
	OPENBSD_4_2:1.34.0.2
	OPENBSD_4_2_BASE:1.34
	OPENBSD_4_1:1.31.0.2
	OPENBSD_4_1_BASE:1.31
	OPENBSD_4_0:1.30.0.4
	OPENBSD_4_0_BASE:1.30
	OPENBSD_3_9:1.30.0.2
	OPENBSD_3_9_BASE:1.30
	OPENBSD_3_8:1.26.0.2
	OPENBSD_3_8_BASE:1.26
	OPENBSD_3_7:1.25.0.4
	OPENBSD_3_7_BASE:1.25
	OPENBSD_3_6:1.25.0.2
	OPENBSD_3_6_BASE:1.25
	SMP_SYNC_A:1.25
	SMP_SYNC_B:1.25
	OPENBSD_3_5:1.22.0.4
	OPENBSD_3_5_BASE:1.22
	OPENBSD_3_4:1.22.0.2
	OPENBSD_3_4_BASE:1.22
	UBC_SYNC_A:1.19
	OPENBSD_3_3:1.19.0.6
	OPENBSD_3_3_BASE:1.19
	OPENBSD_3_2:1.19.0.4
	OPENBSD_3_2_BASE:1.19
	OPENBSD_3_1:1.19.0.2
	OPENBSD_3_1_BASE:1.19
	UBC_SYNC_B:1.19
	UBC:1.15.0.2
	UBC_BASE:1.15
	OPENBSD_3_0:1.7.0.2
	OPENBSD_3_0_BASE:1.7
	OPENBSD_2_9_BASE:1.4
	OPENBSD_2_9:1.4.0.2
	OPENBSD_2_8:1.3.0.12
	OPENBSD_2_8_BASE:1.3
	OPENBSD_2_7:1.3.0.10
	OPENBSD_2_7_BASE:1.3
	SMP:1.3.0.8
	SMP_BASE:1.3
	kame_19991208:1.3
	OPENBSD_2_6:1.3.0.6
	OPENBSD_2_6_BASE:1.3
	OPENBSD_2_5:1.3.0.4
	OPENBSD_2_5_BASE:1.3
	OPENBSD_2_4:1.3.0.2
	OPENBSD_2_4_BASE:1.3
	OPENBSD_2_3:1.2.0.2
	OPENBSD_2_3_BASE:1.2;
locks; strict;
comment	@ * @;


1.43
date	2017.01.10.19.48.32;	author bluhm;	state Exp;
branches;
next	1.42;
commitid	nzxicVheAaFpfTKW;

1.42
date	2016.03.12.00.27.15;	author bluhm;	state Exp;
branches;
next	1.41;
commitid	qJ5CKInI1Ya4H3Lk;

1.41
date	2015.03.14.03.38.51;	author jsg;	state Exp;
branches;
next	1.40;
commitid	p4LJxGKbi0BU2cG6;

1.40
date	2014.09.14.14.17.26;	author jsg;	state Exp;
branches;
next	1.39;
commitid	uzzBR7hz9ncd4O6G;

1.39
date	2011.07.04.20.35.35;	author deraadt;	state Exp;
branches;
next	1.38;

1.38
date	2011.07.02.22.20.08;	author nicm;	state Exp;
branches;
next	1.37;

1.37
date	2008.05.03.14.41.29;	author thib;	state Exp;
branches;
next	1.36;

1.36
date	2007.12.27.13.59.12;	author thib;	state Exp;
branches;
next	1.35;

1.35
date	2007.09.15.19.22.18;	author bluhm;	state Exp;
branches;
next	1.34;

1.34
date	2007.06.01.23.47.56;	author deraadt;	state Exp;
branches;
next	1.33;

1.33
date	2007.05.26.18.42.21;	author thib;	state Exp;
branches;
next	1.32;

1.32
date	2007.03.21.17.29.31;	author thib;	state Exp;
branches;
next	1.31;

1.31
date	2007.01.16.17.52.18;	author thib;	state Exp;
branches;
next	1.30;

1.30
date	2005.11.07.01.02.32;	author pedro;	state Exp;
branches;
next	1.29;

1.29
date	2005.11.06.14.15.08;	author pedro;	state Exp;
branches;
next	1.28;

1.28
date	2005.11.06.13.07.47;	author pedro;	state Exp;
branches;
next	1.27;

1.27
date	2005.10.19.16.50.46;	author pedro;	state Exp;
branches;
next	1.26;

1.26
date	2005.07.03.20.13.59;	author drahn;	state Exp;
branches;
next	1.25;

1.25
date	2004.06.09.22.54.14;	author tedu;	state Exp;
branches;
next	1.24;

1.24
date	2004.05.14.04.00.33;	author tedu;	state Exp;
branches;
next	1.23;

1.23
date	2004.04.25.03.21.19;	author jolan;	state Exp;
branches;
next	1.22;

1.22
date	2003.09.01.18.06.03;	author henning;	state Exp;
branches;
next	1.21;

1.21
date	2003.07.21.22.44.50;	author tedu;	state Exp;
branches;
next	1.20;

1.20
date	2003.06.02.23.28.07;	author millert;	state Exp;
branches;
next	1.19;

1.19
date	2002.03.14.01.27.06;	author millert;	state Exp;
branches;
next	1.18;

1.18
date	2002.02.26.05.47.47;	author fgsch;	state Exp;
branches;
next	1.17;

1.17
date	2002.02.22.20.37.45;	author drahn;	state Exp;
branches;
next	1.16;

1.16
date	2001.12.19.08.58.06;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2001.12.10.04.45.31;	author art;	state Exp;
branches
	1.15.2.1;
next	1.14;

1.14
date	2001.12.10.02.19.34;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2001.12.04.22.44.31;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2001.11.30.05.45.33;	author csapuntz;	state Exp;
branches;
next	1.11;

1.11
date	2001.11.29.15.51.48;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2001.11.29.02.08.21;	author art;	state Exp;
branches;
next	1.9;

1.9
date	2001.11.29.01.58.57;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2001.11.27.05.27.12;	author art;	state Exp;
branches;
next	1.7;

1.7
date	2001.06.25.03.28.03;	author csapuntz;	state Exp;
branches;
next	1.6;

1.6
date	2001.06.22.14.14.10;	author deraadt;	state Exp;
branches;
next	1.5;

1.5
date	2001.06.05.20.54.52;	author provos;	state Exp;
branches;
next	1.4;

1.4
date	2001.03.16.01.09.48;	author csapuntz;	state Exp;
branches;
next	1.3;

1.3
date	98.08.06.19.34.24;	author csapuntz;	state Exp;
branches
	1.3.8.1;
next	1.2;

1.2
date	98.01.11.02.10.44;	author csapuntz;	state Exp;
branches;
next	1.1;

1.1
date	98.01.10.23.44.29;	author csapuntz;	state Exp;
branches;
next	;

1.3.8.1
date	2001.05.14.22.32.46;	author niklas;	state Exp;
branches;
next	1.3.8.2;

1.3.8.2
date	2001.07.04.10.48.49;	author niklas;	state Exp;
branches;
next	1.3.8.3;

1.3.8.3
date	2001.12.05.01.02.39;	author niklas;	state Exp;
branches;
next	1.3.8.4;

1.3.8.4
date	2002.03.06.02.13.24;	author niklas;	state Exp;
branches;
next	1.3.8.5;

1.3.8.5
date	2002.03.28.11.43.04;	author niklas;	state Exp;
branches;
next	1.3.8.6;

1.3.8.6
date	2003.06.07.11.03.41;	author ho;	state Exp;
branches;
next	1.3.8.7;

1.3.8.7
date	2004.02.19.10.56.38;	author niklas;	state Exp;
branches;
next	1.3.8.8;

1.3.8.8
date	2004.06.05.23.13.03;	author niklas;	state Exp;
branches;
next	1.3.8.9;

1.3.8.9
date	2004.06.10.11.40.34;	author niklas;	state Exp;
branches;
next	;

1.15.2.1
date	2002.06.11.03.29.40;	author art;	state Exp;
branches;
next	1.15.2.2;

1.15.2.2
date	2003.05.20.04.07.34;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.43
log
@Remove the unused olddp parameter from function dounmount().
OK mpi@@ millert@@
@
text
@/*	$OpenBSD: vfs_default.c,v 1.42 2016/03/12 00:27:15 bluhm Exp $  */

/*
 * Portions of this code are:
 *
 * Copyright (c) 1989, 1993
 *	The Regents of the University of California.  All rights reserved.
 * (c) UNIX System Laboratories, Inc.
 * All or some portions of this file are derived from material licensed
 * to the University of California by American Telephone and Telegraph
 * Co. or Unix System Laboratories, Inc. and are reproduced herein with
 * the permission of UNIX System Laboratories, Inc.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/mount.h>
#include <sys/vnode.h>
#include <sys/namei.h>
#include <sys/pool.h>
#include <sys/event.h>
#include <sys/specdev.h>

int filt_generic_readwrite(struct knote *, long);
void filt_generic_detach(struct knote *);

/*
 * Eliminate all activity associated with the requested vnode
 * and with all vnodes aliased to the requested vnode.
 */
int
vop_generic_revoke(void *v)
{
	struct vop_revoke_args *ap = v;
	struct vnode *vp, *vq;
	struct proc *p = curproc;

#ifdef DIAGNOSTIC
	if ((ap->a_flags & REVOKEALL) == 0)
		panic("vop_generic_revoke");
#endif

	vp = ap->a_vp;
 
	while (vp->v_type == VBLK && vp->v_specinfo != NULL &&
	    vp->v_specmountpoint != NULL) {
		struct mount *mp = vp->v_specmountpoint;

		/*
		 * If we have a mount point associated with the vnode, we must
		 * flush it out now, as to not leave a dangling zombie mount
		 * point laying around in VFS.
		 */
		if (!vfs_busy(mp, VB_WRITE|VB_WAIT)) {
			dounmount(mp, MNT_FORCE | MNT_DOOMED, p);
			break;
		}
	}

	if (vp->v_flag & VALIASED) {
		/*
		 * If a vgone (or vclean) is already in progress,
		 * wait until it is done and return.
		 */
		if (vp->v_flag & VXLOCK) {
			vp->v_flag |= VXWANT;
			tsleep(vp, PINOD, "vop_generic_revokeall", 0);

			return(0);
		}

		/*
		 * Ensure that vp will not be vgone'd while we
		 * are eliminating its aliases.
		 */
		vp->v_flag |= VXLOCK;
		while (vp->v_flag & VALIASED) {
			for (vq = *vp->v_hashchain; vq; vq = vq->v_specnext) {
				if (vq->v_rdev != vp->v_rdev ||
				    vq->v_type != vp->v_type || vp == vq)
					continue;
				vgone(vq);
				break;
			}
		}

		/*
		 * Remove the lock so that vgone below will
		 * really eliminate the vnode after which time
		 * vgone will awaken any sleepers.
		 */
		vp->v_flag &= ~VXLOCK;
	}

	vgonel(vp, p);

	return (0);
}

int
vop_generic_bmap(void *v)
{
	struct vop_bmap_args *ap = v;

	if (ap->a_vpp)
		*ap->a_vpp = ap->a_vp;
	if (ap->a_bnp)
		*ap->a_bnp = ap->a_bn;
	if (ap->a_runp)
		*ap->a_runp = 0;

	return (0);
}

int
vop_generic_bwrite(void *v)
{
	struct vop_bwrite_args *ap = v;

	return (bwrite(ap->a_bp));
}

int
vop_generic_abortop(void *v)
{
	struct vop_abortop_args *ap = v;
 
	if ((ap->a_cnp->cn_flags & (HASBUF | SAVESTART)) == HASBUF)
		pool_put(&namei_pool, ap->a_cnp->cn_pnbuf);

	return (0);
}

/*
 * Stubs to use when there is no locking to be done on the underlying object.
 * A minimal shared lock is necessary to ensure that the underlying object
 * is not revoked while an operation is in progress. So, an active shared
 * count should be maintained in an auxiliary vnode lock structure. However,
 * that's not done now.
 */
int
vop_generic_lock(void *v)
{
	return (0);
}
 
/*
 * Decrement the active use count. (Not done currently)
 */
int
vop_generic_unlock(void *v)
{
	return (0);
}

/*
 * Return whether or not the node is in use. (Not done currently)
 */
int
vop_generic_islocked(void *v)
{
	return (0);
}

struct filterops generic_filtops = 
	{ 1, NULL, filt_generic_detach, filt_generic_readwrite };

int
vop_generic_kqfilter(void *v)
{
	struct vop_kqfilter_args *ap = v;
	struct knote *kn = ap->a_kn;

	switch (kn->kn_filter) {
	case EVFILT_READ:
	case EVFILT_WRITE:
		kn->kn_fop = &generic_filtops;
		break;
	default:
		return (EINVAL);
	}

	return (0);
}

/* Trivial lookup routine that always fails. */
int
vop_generic_lookup(void *v)
{
	struct vop_lookup_args	*ap = v;

	*ap->a_vpp = NULL;
	return (ENOTDIR);
}

void
filt_generic_detach(struct knote *kn)
{
}

int
filt_generic_readwrite(struct knote *kn, long hint)
{
	/*
	 * filesystem is gone, so set the EOF flag and schedule 
	 * the knote for deletion.
	 */
	if (hint == NOTE_REVOKE) {
		kn->kn_flags |= (EV_EOF | EV_ONESHOT);
		return (1);
	}

        kn->kn_data = 0;

        return (1);
}
@


1.42
log
@When vfs_busy() is sleeping, it uses RW_SLEEPFAIL and will fail.
So if vop_generic_revoke() does not unmount because the mount point
is busy, this could result in a mount point without a valid device.
It is better to check and sleep in a loop to avoid a corrupt mount
point.
OK natano@@ krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.41 2015/03/14 03:38:51 jsg Exp $  */
d79 1
a79 1
			dounmount(mp, MNT_FORCE | MNT_DOOMED, p, NULL);
@


1.41
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.40 2014/09/14 14:17:26 jsg Exp $  */
d69 2
a70 1
	if (vp->v_type == VBLK && vp->v_specinfo != 0) {
d78 1
a78 1
		if (mp != NULL && !vfs_busy(mp, VB_WRITE|VB_WAIT))
d80 2
@


1.40
log
@remove uneeded proc.h includes
ok mpi@@ kspillner@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.39 2011/07/04 20:35:35 deraadt Exp $  */
a43 1
#include <sys/malloc.h>
@


1.39
log
@move the specfs code to a place people can see it; ok guenther thib krw
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.38 2011/07/02 22:20:08 nicm Exp $  */
a40 1
#include <sys/proc.h>
@


1.38
log
@kqueue attach functions should return an errno or 0, not a plain 1. Fix
the obvious cases to return EINVAL and ENXIO.

ok tedu deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.37 2008/05/03 14:41:29 thib Exp $  */
d48 1
a48 1
#include <miscfs/specfs/specdev.h>
@


1.37
log
@Introduce vop_generic_bmap(); use it where applicable.
one thing of note, fifofs changes in that its bmap now
sets the runp too 0, but that was an oversight in the
old code.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.36 2007/12/27 13:59:12 thib Exp $  */
d203 1
a203 1
		return (1);
@


1.36
log
@fifofs, deadfs and specfs all have the same "trivial lookup routine that
always fails". Introduce vop_generic_lookup(), which is a trivial lookup
routine that always fails and use that instead, zap the redundant copies.

ok toby@@, tedu@@, art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.35 2007/09/15 19:22:18 bluhm Exp $  */
d119 15
@


1.35
log
@Allow to pull out an usb stick with ffs filesystem while mounted
and a file is written onto the stick.  Without these fixes the
machine panics or hangs.
The usb fix calls the callback when the stick is pulled out to free
the associated buffers.  Otherwise we have busy buffers for ever
and the automatic unmount will panic.
The change in the scsi layer prevents passing down further dirty
buffers to usb after the stick has been deactivated.
In vfs the automatic unmount has moved from the function vgonel()
to vop_generic_revoke().  Both are called when the sd device's vnode
is removed.  In vgonel() the VXLOCK is already held which can cause
a deadlock.  So call dounmount() earlier.

ok krw@@, I like this marco@@, tested by ian@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.34 2007/06/01 23:47:56 deraadt Exp $  */
d192 10
@


1.34
log
@pedro ok'd this ~3500 line diff which removes the vop argument
"ap = v" comments in under 8 seconds, so it must be ok.  and it compiles
too.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.33 2007/05/26 18:42:21 thib Exp $  */
d71 12
@


1.33
log
@Nuke a bunch of simpelocks and associated goo.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.32 2007/03/21 17:29:31 thib Exp $  */
d60 1
a60 5
	struct vop_revoke_args /* {
		struct vnodeop_desc *a_desc;
		struct vnode *a_vp;
		int a_flags;
	} */ *ap = v;
d122 1
a122 5
	struct vop_abortop_args /* {
		struct vnodeop_desc *a_desc;
		struct vnode *a_dvp;
		struct componentname *a_cnp;
	} */ *ap = v;
d167 1
a167 5
	struct vop_kqfilter_args /* {
		struct vnodeop_desc *a_desc;
		struct vnode *a_vp;
		struct knote *a_kn;
	} */ *ap = v;
@


1.32
log
@Remove the v_interlock simplelock from the vnode structure.
Zap all calls to simple_lock/unlock() on it (those calls are
#defined away though). Remove the LK_INTERLOCK from the calls
to vn_lock() and cleanup the filesystems wich implement VOP_LOCK().
(by remvoing the v_interlock from there calls to lockmgr()).

ok pedro@@, art@@, tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.31 2007/01/16 17:52:18 thib Exp $  */
a49 2
extern struct simplelock spechash_slock;

a92 1
			simple_lock(&spechash_slock);
a96 1
				simple_unlock(&spechash_slock);
a99 1
			simple_unlock(&spechash_slock);
@


1.31
log
@Retire VOP_LEASE(); It was a bit for NQNFS and has
effectively been a no-op for quite some time now,
without promise for future usage.

ok pedro@@
Testing by krw@@ (earlier diff)
and Johan Mson Lindman (tybollt@@solace.miun.se)
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.30 2005/11/07 01:02:32 pedro Exp $  */
a75 1
	simple_lock(&vp->v_interlock);
a83 1
			simple_unlock(&vp->v_interlock);
a93 1
		simple_unlock(&vp->v_interlock);
a111 1
		simple_lock(&vp->v_interlock);
a152 14
	struct vop_lock_args /* {
		struct vnodeop_desc *a_desc;
		struct vnode *a_vp;
		int a_flags;
		struct proc *a_p;
	} */ *ap = v;

	/*
	 * Since we are not using the lock manager, we must clear
	 * the interlock here.
	 */
	if (ap->a_flags & LK_INTERLOCK)
		simple_unlock(&ap->a_vp->v_interlock);

@


1.30
log
@nit
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.29 2005/11/06 14:15:08 pedro Exp $  */
a236 8
}

int lease_check(void *);

int
lease_check(void *v)
{
	return (0);
@


1.29
log
@Make comments match reality, space the code a bit while at it
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.28 2005/11/06 13:07:47 pedro Exp $  */
d56 1
a56 1
 * Eliminate all activity associated with  the requested vnode
@


1.28
log
@Use ANSI-style function declarations, no binary change, okay jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.27 2005/10/19 16:50:46 pedro Exp $  */
d4 1
a4 1
 *    Portions of this code are:
d87 1
d90 1
d109 1
d118 1
d120 1
d143 1
d151 2
a152 1
 * count is maintained in an auxiliary vnode lock structure.
d170 1
d175 1
a175 1
 * Decrement the active use count.
d184 1
a184 1
 * Return whether or not the node is in use.
d235 1
@


1.27
log
@Remove v_vnlock from struct vnode, okay krw@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.26 2005/07/03 20:13:59 drahn Exp $  */
a38 1

a49 1

d52 2
a53 2
int filt_generic_readwrite(struct knote *kn, long hint);
void filt_generic_detach(struct knote *kn);
d60 1
a60 2
vop_generic_revoke(v)
	void *v;
a118 1

d120 1
a120 2
vop_generic_bwrite(v)
	void *v;
a126 1

d128 1
a128 2
vop_generic_abortop(v)
	void *v;
d148 1
a148 2
vop_generic_lock(v)
	void *v;
a168 1

d170 1
a170 2
vop_generic_unlock(v)
	void *v;
d179 1
a179 2
vop_generic_islocked(v)
	void *v;
d188 1
a188 2
vop_generic_kqfilter(v)
	void *v;
@


1.26
log
@Extended Attributes was a piece to get to ACLs, however ACLs have not
been worked on, so EA is pointless. Also the code is not enabled
in GENERIC so it is not being tested or maintained.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.25 2004/06/09 22:54:14 tedu Exp $  */
a164 45
#ifdef notyet
	/*
	 * This code cannot be used until all the non-locking filesystems
	 * (notably NFS) are converted to properly lock and release nodes.
	 * Also, certain vnode operations change the locking state within
	 * the operation (create, mknod, remove, link, rename, mkdir, rmdir,
	 * and symlink). Ideally these operations should not change the
	 * lock state, but should be changed to let the caller of the
	 * function unlock them. Otherwise all intermediate vnode layers
	 * (such as union, umapfs, etc) must catch these functions to do
	 * the necessary locking at their layer. Note that the inactive
	 * and lookup operations also change their lock state, but this 
	 * cannot be avoided, so these two operations will always need
	 * to be handled in intermediate layers.
	 */
	struct vnode *vp = ap->a_vp;
	int vnflags, flags = ap->a_flags;

	if (vp->v_vnlock == NULL) {
		if ((flags & LK_TYPE_MASK) == LK_DRAIN)
			return (0);
		MALLOC(vp->v_vnlock, struct lock *, sizeof(struct lock),
		    M_VNODE, M_WAITOK);
		lockinit(vp->v_vnlock, PVFS, "vnlock", 0, 0);
	}
	switch (flags & LK_TYPE_MASK) {
	case LK_DRAIN:
		vnflags = LK_DRAIN;
		break;
	case LK_EXCLUSIVE:
	case LK_SHARED:
		vnflags = LK_SHARED;
		break;
	case LK_UPGRADE:
	case LK_EXCLUPGRADE:
	case LK_DOWNGRADE:
		return (0);
	case LK_RELEASE:
	default:
		panic("vop_generic_lock: bad operation %d", flags & LK_TYPE_MASK);
	}
	if (flags & LK_INTERLOCK)
		vnflags |= LK_INTERLOCK;
	return(lockmgr(vp->v_vnlock, vnflags, &vp->v_interlock, ap->a_p));
#else /* for now */
a171 1
#endif
@


1.25
log
@in theory, vnlock should alays be NULL in the generic lock (sic) functions.
in reality, sometimes it's not.  we don't trust vnlock, and since it's 100%
guaranteed to panic if it gets here, just completely stop using it.
crash by henning, ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.24 2004/05/14 04:00:33 tedu Exp $  */
a293 16
}

/* 
 * vfs default ops
 * used to fill the vfs function table to get reasonable default return values.
 */
int
vfs_stdextattrctl(mp, cmd, filename_vp, attrnamespace, attrname, td)
	struct mount *mp;
	int cmd;
	struct vnode *filename_vp;
	int attrnamespace;
	const char *attrname;
	struct proc *td;
{
	return(EOPNOTSUPP);
@


1.24
log
@use pool for namei pathbuf.  testing ok millert@@ tdeval@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.23 2004/04/25 03:21:19 jolan Exp $  */
d228 1
a228 12
	struct vop_unlock_args /* {
		struct vnodeop_desc *a_desc;
		struct vnode *a_vp;
		int a_flags;
		struct proc *a_p;
	} */ *ap = v;

	struct vnode *vp = ap->a_vp;

	if (vp->v_vnlock == NULL)
		return (0);
	return (lockmgr(vp->v_vnlock, LK_RELEASE, NULL, ap->a_p));
d238 1
a238 10
	struct vop_islocked_args /* {
		struct vnodeop_desc *a_desc;
		struct vnode *a_vp;
	} */ *ap = v;

	struct vnode *vp = ap->a_vp;

	if (vp->v_vnlock == NULL)
		return (0);
	return (lockstatus(vp->v_vnlock));
@


1.23
log
@fix typos/spelling in comments, from pedro martelletto
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.23 2004/04/25 03:07:53 jolan Exp $  */
d47 1
d144 1
a144 1
		FREE(ap->a_cnp->cn_pnbuf, M_NAMEI);
@


1.22
log
@match syscallargs comments with reality
from Patrick Latifi <patrick.l@@hermes.usherb.ca>
ok jason@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.21 2003/07/21 22:44:50 tedu Exp $  */
d151 1
a151 1
 * count is maintained in an auxillary vnode lock structure.
@


1.21
log
@remove caddr_t casts.  it's just silly to cast something when the function
takes a void *.  convert uiomove to take a void * as well.  ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.20 2003/06/02 23:28:07 millert Exp $  */
d65 1
d137 1
d158 1
d228 1
d249 1
d268 1
@


1.20
log
@Remove the advertising clause in the UCB license which Berkeley
rescinded 22 July 1999.  Proofed by myself and Theo.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.19 2002/03/14 01:27:06 millert Exp $  */
d87 1
a87 1
			tsleep((caddr_t)vp, PINOD, "vop_generic_revokeall", 0);
@


1.19
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.18 2002/02/26 05:47:47 fgsch Exp $  */
d22 1
a22 5
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.18
log
@fucntion->function.
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.17 2002/02/22 20:37:45 drahn Exp $  */
d57 2
a58 2
int filt_generic_readwrite __P((struct knote *kn, long hint));
void filt_generic_detach __P((struct knote *kn));
@


1.17
log
@Extended Attribute support from FreeBSD/TrustedBSD ok art@@, deraadt@@
@
text
@d1 1
a1 2
/*	$OpenBSD: vfs_default.c,v 1.16 2001/12/19 08:58:06 art Exp $  */

d312 1
d315 1
a315 1
 * used to fill the vfs fucntion table to get reasonable default return values.
a316 1

@


1.16
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/*       $OpenBSD: vfs_default.c,v 1.7 2001/06/25 03:28:03 csapuntz Exp $  */
d312 16
@


1.15
log
@Big cleanup inspired by NetBSD with some parts of the code from NetBSD.
 - get rid of VOP_BALLOCN and VOP_SIZE
 - move the generic getpages and putpages into miscfs/genfs
 - create a genfs_node which must be added to the top of the private portion
   of each vnode for filsystems that want to use genfs_{get,put}pages
 - rename genfs_mmap to vop_generic_mmap
@
text
@d1 2
a2 1
/*       $OpenBSD: vfs_default.c,v 1.14 2001/12/10 02:19:34 art Exp $  */
d7 1
a7 1
 * Copyright (c) 1982, 1986, 1989, 1993
a51 1
#include <sys/pool.h>
a54 1
#include <uvm/uvm.h>
a311 7
}

int
vop_generic_mmap(v)
	void *v;
{
	return 0;
@


1.15.2.1
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d59 2
a60 2
int filt_generic_readwrite(struct knote *kn, long hint);
void filt_generic_detach(struct knote *kn);
a320 17

/* 
 * vfs default ops
 * used to fill the vfs function table to get reasonable default return values.
 */
int
vfs_stdextattrctl(mp, cmd, filename_vp, attrnamespace, attrname, td)
	struct mount *mp;
	int cmd;
	struct vnode *filename_vp;
	int attrnamespace;
	const char *attrname;
	struct proc *td;
{
	return(EOPNOTSUPP);
}

@


1.15.2.2
log
@byebye vop_generic_lock
@
text
@d151 110
@


1.14
log
@Merge in struct uvm_vnode into struct vnode.
@
text
@d1 1
a1 1
/*       $OpenBSD: vfs_default.c,v 1.13 2001/12/04 22:44:31 art Exp $  */
a314 682
/*
 * generic VM getpages routine.
 * Return PG_BUSY pages for the given range,
 * reading from backing store if necessary.
 */

int
genfs_getpages(v)
	void *v;
{
	struct vop_getpages_args /* {
		struct vnode *a_vp;
		voff_t a_offset;
		vm_page_t *a_m;
		int *a_count;
		int a_centeridx;
		vm_prot_t a_access_type;
		int a_advice;
		int a_flags;
	} */ *ap = v;

	off_t newsize, diskeof, memeof;
	off_t offset, origoffset, startoffset, endoffset, raoffset;
	daddr_t lbn, blkno;
	int s, i, error, npages, orignpages, npgs, run, ridx, pidx, pcount;
	int fs_bshift, fs_bsize, dev_bshift, dev_bsize;
	int flags = ap->a_flags;
	size_t bytes, iobytes, tailbytes, totalbytes, skipbytes;
	vaddr_t kva;
	struct buf *bp, *mbp;
	struct vnode *vp = ap->a_vp;
	struct uvm_object *uobj = &vp->v_uobj;
	struct vm_page *pgs[16];			/* XXXUBC 16 */
	struct ucred *cred = curproc->p_ucred;		/* XXXUBC curproc */
	boolean_t async = (flags & PGO_SYNCIO) == 0;
	boolean_t write = (ap->a_access_type & VM_PROT_WRITE) != 0;
	boolean_t sawhole = FALSE;
	struct proc *p = curproc;
	UVMHIST_FUNC("genfs_getpages"); UVMHIST_CALLED(ubchist);

	UVMHIST_LOG(ubchist, "vp %p off 0x%x/%x count %d",
		    vp, ap->a_offset >> 32, ap->a_offset, *ap->a_count);

	/* XXXUBC temp limit */
	if (*ap->a_count > 16) {
		return EINVAL;
	}

	error = 0;
	origoffset = ap->a_offset;
	orignpages = *ap->a_count;
	error = VOP_SIZE(vp, vp->v_size, &diskeof);
	if (error) {
		return error;
	}
	if (flags & PGO_PASTEOF) {
		newsize = MAX(vp->v_size,
		    origoffset + (orignpages << PAGE_SHIFT));
		error = VOP_SIZE(vp, newsize, &memeof);
		if (error) {
			return error;
		}
	} else {
		memeof = diskeof;
	}
	KASSERT(ap->a_centeridx >= 0 || ap->a_centeridx <= orignpages);
	KASSERT((origoffset & (PAGE_SIZE - 1)) == 0 && origoffset >= 0);
	KASSERT(orignpages > 0);

	/*
	 * Bounds-check the request.
	 */

	if (origoffset + (ap->a_centeridx << PAGE_SHIFT) >= memeof) {
		if ((flags & PGO_LOCKED) == 0) {
			simple_unlock(&uobj->vmobjlock);
		}
		UVMHIST_LOG(ubchist, "off 0x%x count %d goes past EOF 0x%x",
			    origoffset, *ap->a_count, memeof,0);
		return EINVAL;
	}

	/*
	 * For PGO_LOCKED requests, just return whatever's in memory.
	 */

	if (flags & PGO_LOCKED) {
		uvn_findpages(uobj, origoffset, ap->a_count, ap->a_m,
			      UFP_NOWAIT|UFP_NOALLOC|UFP_NORDONLY);

		return ap->a_m[ap->a_centeridx] == NULL ? EBUSY : 0;
	}

	/* vnode is VOP_LOCKed, uobj is locked */

	if (write && (vp->v_bioflag & VBIOONSYNCLIST) == 0) {
		vn_syncer_add_to_worklist(vp, syncdelay);
	}

	/*
	 * find the requested pages and make some simple checks.
	 * leave space in the page array for a whole block.
	 */

	fs_bshift = vp->v_mount->mnt_fs_bshift;
	fs_bsize = 1 << fs_bshift;
	dev_bshift = vp->v_mount->mnt_dev_bshift;
	dev_bsize = 1 << dev_bshift;
	KASSERT((diskeof & (dev_bsize - 1)) == 0);
	KASSERT((memeof & (dev_bsize - 1)) == 0);

	orignpages = MIN(orignpages,
	    round_page(memeof - origoffset) >> PAGE_SHIFT);
	npages = orignpages;
	startoffset = origoffset & ~(fs_bsize - 1);
	endoffset = round_page((origoffset + (npages << PAGE_SHIFT)
				+ fs_bsize - 1) & ~(fs_bsize - 1));
	endoffset = MIN(endoffset, round_page(memeof));
	ridx = (origoffset - startoffset) >> PAGE_SHIFT;

	memset(pgs, 0, sizeof(pgs));
	uvn_findpages(uobj, origoffset, &npages, &pgs[ridx], UFP_ALL);

	/*
	 * if PGO_OVERWRITE is set, don't bother reading the pages.
	 * PGO_OVERWRITE also means that the caller guarantees
	 * that the pages already have backing store allocated.
	 */

	if (flags & PGO_OVERWRITE) {
		UVMHIST_LOG(ubchist, "PGO_OVERWRITE",0,0,0,0);

		for (i = 0; i < npages; i++) {
			struct vm_page *pg = pgs[ridx + i];

			if (pg->flags & PG_FAKE) {
				uvm_pagezero(pg);
				pg->flags &= ~(PG_FAKE);
			}
			pg->flags &= ~(PG_RDONLY);
		}
		npages += ridx;
		goto out;
	}

	/*
	 * if the pages are already resident, just return them.
	 */

	for (i = 0; i < npages; i++) {
		struct vm_page *pg = pgs[ridx + i];

		if ((pg->flags & PG_FAKE) ||
		    (write && (pg->flags & PG_RDONLY))) {
			break;
		}
	}
	if (i == npages) {
		UVMHIST_LOG(ubchist, "returning cached pages", 0,0,0,0);
		raoffset = origoffset + (orignpages << PAGE_SHIFT);
		npages += ridx;
		goto raout;
	}

	/*
	 * the page wasn't resident and we're not overwriting,
	 * so we're going to have to do some i/o.
	 * find any additional pages needed to cover the expanded range.
	 */

	npages = (endoffset - startoffset) >> PAGE_SHIFT;
	if (startoffset != origoffset || npages != orignpages) {

		/*
		 * XXXUBC we need to avoid deadlocks caused by locking
		 * additional pages at lower offsets than pages we
		 * already have locked.  for now, unlock them all and
		 * start over.
		 */

		for (i = 0; i < orignpages; i++) {
			struct vm_page *pg = pgs[ridx + i];

			if (pg->flags & PG_FAKE) {
				pg->flags |= PG_RELEASED;
			}
		}
		uvm_page_unbusy(&pgs[ridx], orignpages);
		memset(pgs, 0, sizeof(pgs));

		UVMHIST_LOG(ubchist, "reset npages start 0x%x end 0x%x",
			    startoffset, endoffset, 0,0);
		npgs = npages;
		uvn_findpages(uobj, startoffset, &npgs, pgs, UFP_ALL);
	}
	simple_unlock(&uobj->vmobjlock);

	/*
	 * read the desired page(s).
	 */

	totalbytes = npages << PAGE_SHIFT;
	bytes = MIN(totalbytes, MAX(diskeof - startoffset, 0));
	tailbytes = totalbytes - bytes;
	skipbytes = 0;

	kva = uvm_pagermapin(pgs, npages, UVMPAGER_MAPIN_WAITOK |
			     UVMPAGER_MAPIN_READ);

	s = splbio();
	mbp = pool_get(&bufpool, PR_WAITOK);
	splx(s);
	mbp->b_bufsize = totalbytes;
	mbp->b_data = (void *)kva;
	mbp->b_resid = mbp->b_bcount = bytes;
	mbp->b_flags = B_BUSY|B_READ| (async ? B_CALL : 0);
	mbp->b_iodone = uvm_aio_biodone;
	mbp->b_vp = NULL;
	LIST_INIT(&mbp->b_dep);
	bgetvp(vp, mbp);

	/*
	 * if EOF is in the middle of the range, zero the part past EOF.
	 */

	if (tailbytes > 0) {
		memset((void *)(kva + bytes), 0, tailbytes);
	}

	/*
	 * now loop over the pages, reading as needed.
	 */

	if (write) {
		lockmgr(&vp->v_glock, LK_EXCLUSIVE, NULL, p);
	} else {
		lockmgr(&vp->v_glock, LK_SHARED, NULL, p);
	}

	bp = NULL;
	for (offset = startoffset;
	     bytes > 0;
	     offset += iobytes, bytes -= iobytes) {

		/*
		 * skip pages which don't need to be read.
		 */

		pidx = (offset - startoffset) >> PAGE_SHIFT;
		while ((pgs[pidx]->flags & (PG_FAKE|PG_RDONLY)) == 0) {
			size_t b;

			KASSERT((offset & (PAGE_SIZE - 1)) == 0);
			b = MIN(PAGE_SIZE, bytes);
			offset += b;
			bytes -= b;
			skipbytes += b;
			pidx++;
			UVMHIST_LOG(ubchist, "skipping, new offset 0x%x",
				    offset, 0,0,0);
			if (bytes == 0) {
				goto loopdone;
			}
		}

		/*
		 * bmap the file to find out the blkno to read from and
		 * how much we can read in one i/o.  if bmap returns an error,
		 * skip the rest of the top-level i/o.
		 */

		lbn = offset >> fs_bshift;
		error = VOP_BMAP(vp, lbn, NULL, &blkno, &run);
		if (error) {
			UVMHIST_LOG(ubchist, "VOP_BMAP lbn 0x%x -> %d\n",
				    lbn, error,0,0);
			skipbytes += bytes;
			goto loopdone;
		}

		/*
		 * see how many pages can be read with this i/o.
		 * reduce the i/o size if necessary to avoid
		 * overwriting pages with valid data.
		 */

		iobytes = MIN((((off_t)lbn + 1 + run) << fs_bshift) - offset,
		    bytes);
		if (offset + iobytes > round_page(offset)) {
			pcount = 1;
			while (pidx + pcount < npages &&
			       pgs[pidx + pcount]->flags & PG_FAKE) {
				pcount++;
			}
			iobytes = MIN(iobytes, (pcount << PAGE_SHIFT) -
				      (offset - trunc_page(offset)));
		}

		/*
		 * if this block isn't allocated, zero it instead of reading it.
		 * if this is a read access, mark the pages we zeroed PG_RDONLY.
		 */

		if (blkno < 0) {
			int holepages = (round_page(offset + iobytes) - 
					 trunc_page(offset)) >> PAGE_SHIFT;
			UVMHIST_LOG(ubchist, "lbn 0x%x -> HOLE", lbn,0,0,0);

			sawhole = TRUE;
			memset((char *)kva + (offset - startoffset), 0,
			       iobytes);
			skipbytes += iobytes;

			for (i = 0; i < holepages; i++) {
				if (write) {
					pgs[pidx + i]->flags &= ~PG_CLEAN;
				} else {
					pgs[pidx + i]->flags |= PG_RDONLY;
				}
			}
			continue;
		}

		/*
		 * allocate a sub-buf for this piece of the i/o
		 * (or just use mbp if there's only 1 piece),
		 * and start it going.
		 */

		if (offset == startoffset && iobytes == bytes) {
			bp = mbp;
		} else {
			s = splbio();
			bp = pool_get(&bufpool, PR_WAITOK);
			splx(s);
			bp->b_data = (char *)kva + offset - startoffset;
			bp->b_resid = bp->b_bcount = iobytes;
			bp->b_flags = B_BUSY|B_READ|B_CALL;
			bp->b_iodone = uvm_aio_biodone1;
			bp->b_vp = vp;
			LIST_INIT(&bp->b_dep);
		}
		bp->b_lblkno = 0;
		bp->b_private = mbp;

		/* adjust physical blkno for partial blocks */
		bp->b_blkno = blkno + ((offset - ((off_t)lbn << fs_bshift)) >>
				       dev_bshift);

		UVMHIST_LOG(ubchist, "bp %p offset 0x%x bcount 0x%x blkno 0x%x",
			    bp, offset, iobytes, bp->b_blkno);

		VOP_STRATEGY(bp);
	}

loopdone:
	if (skipbytes) {
		s = splbio();
		if (error) {
			mbp->b_flags |= B_ERROR;
			mbp->b_error = error;
		}
		mbp->b_resid -= skipbytes;
		if (mbp->b_resid == 0) {
			biodone(mbp);
		}
		splx(s);
	}

	if (async) {
		UVMHIST_LOG(ubchist, "returning 0 (async)",0,0,0,0);
		lockmgr(&vp->v_glock, LK_RELEASE, NULL, p);
		return 0;
	}
	if (bp != NULL) {
		error = biowait(mbp);
	}
	s = splbio();
	(void) buf_cleanout(mbp);
	pool_put(&bufpool, mbp);
	splx(s);
	uvm_pagermapout(kva, npages);
	raoffset = startoffset + totalbytes;

	/*
	 * if this we encountered a hole then we have to do a little more work.
	 * for read faults, we marked the page PG_RDONLY so that future
	 * write accesses to the page will fault again.
	 * for write faults, we must make sure that the backing store for
	 * the page is completely allocated while the pages are locked.
	 */

	if (error == 0 && sawhole && write) {
		error = VOP_BALLOCN(vp, startoffset, npages << PAGE_SHIFT,
				   cred, 0);
		if (error) {
			UVMHIST_LOG(ubchist, "balloc lbn 0x%x -> %d",
				    lbn, error,0,0);
			lockmgr(&vp->v_glock, LK_RELEASE, NULL, p);
			simple_lock(&uobj->vmobjlock);
			goto out;
		}
	}
	lockmgr(&vp->v_glock, LK_RELEASE, NULL, p);
	simple_lock(&uobj->vmobjlock);

	/*
	 * see if we want to start any readahead.
	 * XXXUBC for now, just read the next 128k on 64k boundaries.
	 * this is pretty nonsensical, but it is 50% faster than reading
	 * just the next 64k.
	 */

raout:
	if (!error && !async && !write && ((int)raoffset & 0xffff) == 0 &&
	    PAGE_SHIFT <= 16) {
		int racount;

		racount = 1 << (16 - PAGE_SHIFT);
		(void) VOP_GETPAGES(vp, raoffset, NULL, &racount, 0,
				    VM_PROT_READ, 0, 0);
		simple_lock(&uobj->vmobjlock);

		racount = 1 << (16 - PAGE_SHIFT);
		(void) VOP_GETPAGES(vp, raoffset + 0x10000, NULL, &racount, 0,
				    VM_PROT_READ, 0, 0);
		simple_lock(&uobj->vmobjlock);
	}

	/*
	 * we're almost done!  release the pages...
	 * for errors, we free the pages.
	 * otherwise we activate them and mark them as valid and clean.
	 * also, unbusy pages that were not actually requested.
	 */

out:
	if (error) {
		uvm_lock_pageq();
		for (i = 0; i < npages; i++) {
			if (pgs[i] == NULL) {
				continue;
			}
			UVMHIST_LOG(ubchist, "examining pg %p flags 0x%x",
				    pgs[i], pgs[i]->flags, 0,0);
			if (pgs[i]->flags & PG_WANTED) {
				wakeup(pgs[i]);
			}
			if (pgs[i]->flags & PG_RELEASED) {
				uvm_unlock_pageq();
				(uobj->pgops->pgo_releasepg)(pgs[i], NULL);
				uvm_lock_pageq();
				continue;
			}
			if (pgs[i]->flags & PG_FAKE) {
				uvm_pagefree(pgs[i]);
				continue;
			}
			uvm_pageactivate(pgs[i]);
			pgs[i]->flags &= ~(PG_WANTED|PG_BUSY);
			UVM_PAGE_OWN(pgs[i], NULL);
		}
		uvm_unlock_pageq();
		simple_unlock(&uobj->vmobjlock);
		UVMHIST_LOG(ubchist, "returning error %d", error,0,0,0);
		return error;
	}

	UVMHIST_LOG(ubchist, "succeeding, npages %d", npages,0,0,0);
	uvm_lock_pageq();
	for (i = 0; i < npages; i++) {
		if (pgs[i] == NULL) {
			continue;
		}
		UVMHIST_LOG(ubchist, "examining pg %p flags 0x%x",
			    pgs[i], pgs[i]->flags, 0,0);
		if (pgs[i]->flags & PG_FAKE) {
			UVMHIST_LOG(ubchist, "unfaking pg %p offset 0x%x",
				    pgs[i], pgs[i]->offset,0,0);
			pgs[i]->flags &= ~(PG_FAKE);
			pmap_clear_modify(pgs[i]);
			pmap_clear_reference(pgs[i]);
		}
		if (write) {
			pgs[i]->flags &= ~(PG_RDONLY);
		}
		if (i < ridx || i >= ridx + orignpages || async) {
			UVMHIST_LOG(ubchist, "unbusy pg %p offset 0x%x",
				    pgs[i], pgs[i]->offset,0,0);
			if (pgs[i]->flags & PG_WANTED) {
				wakeup(pgs[i]);
			}
			if (pgs[i]->flags & PG_RELEASED) {
				uvm_unlock_pageq();
				(uobj->pgops->pgo_releasepg)(pgs[i], NULL);
				uvm_lock_pageq();
				continue;
			}
			uvm_pageactivate(pgs[i]);
			pgs[i]->flags &= ~(PG_WANTED|PG_BUSY);
			UVM_PAGE_OWN(pgs[i], NULL);
		}
	}
	uvm_unlock_pageq();
	simple_unlock(&uobj->vmobjlock);
	if (ap->a_m != NULL) {
		memcpy(ap->a_m, &pgs[ridx],
		       orignpages * sizeof(struct vm_page *));
	}
	return 0;
}

/*
 * generic VM putpages routine.
 * Write the given range of pages to backing store.
 */

int
genfs_putpages(v)
	void *v;
{
	struct vop_putpages_args /* {
		struct vnode *a_vp;
		struct vm_page **a_m;
		int a_count;
		int a_flags;
		int *a_rtvals;
	} */ *ap = v;

	int s, error, npages, run;
	int fs_bshift, dev_bshift, dev_bsize;
	vaddr_t kva;
	off_t eof, offset, startoffset;
	size_t bytes, iobytes, skipbytes;
	daddr_t lbn, blkno;
	struct vm_page *pg;
	struct buf *mbp, *bp;
	struct vnode *vp = ap->a_vp;
	boolean_t async = (ap->a_flags & PGO_SYNCIO) == 0;
	UVMHIST_FUNC("genfs_putpages"); UVMHIST_CALLED(ubchist);
	UVMHIST_LOG(ubchist, "vp %p offset 0x%x count %d",
		    vp, ap->a_m[0]->offset, ap->a_count, 0);

	simple_unlock(&vp->v_uobj.vmobjlock);

	error = VOP_SIZE(vp, vp->v_size, &eof);
	if (error) {
		return error;
	}

	error = 0;
	npages = ap->a_count;
	fs_bshift = vp->v_mount->mnt_fs_bshift;
	dev_bshift = vp->v_mount->mnt_dev_bshift;
	dev_bsize = 1 << dev_bshift;
	KASSERT((eof & (dev_bsize - 1)) == 0);

	pg = ap->a_m[0];
	startoffset = pg->offset;
	bytes = MIN(npages << PAGE_SHIFT, eof - startoffset);
	skipbytes = 0;
	KASSERT(bytes != 0);

	kva = uvm_pagermapin(ap->a_m, npages, UVMPAGER_MAPIN_WAITOK);

	s = splbio();
	vp->v_numoutput += 2;
	mbp = pool_get(&bufpool, PR_WAITOK);
	UVMHIST_LOG(ubchist, "vp %p mbp %p num now %d bytes 0x%x",
		    vp, mbp, vp->v_numoutput, bytes);
	splx(s);
	mbp->b_bufsize = npages << PAGE_SHIFT;
	mbp->b_data = (void *)kva;
	mbp->b_resid = mbp->b_bcount = bytes;
	mbp->b_flags = B_BUSY|B_WRITE|B_AGE |
		(async ? B_CALL : 0) |
		(curproc == uvm.pagedaemon_proc ? B_PDAEMON : 0);
	mbp->b_iodone = uvm_aio_biodone;
	mbp->b_vp = NULL;
	LIST_INIT(&mbp->b_dep);
	bgetvp(vp, mbp);

	bp = NULL;
	for (offset = startoffset;
	     bytes > 0;
	     offset += iobytes, bytes -= iobytes) {
		lbn = offset >> fs_bshift;
		error = VOP_BMAP(vp, lbn, NULL, &blkno, &run);
		if (error) {
			UVMHIST_LOG(ubchist, "VOP_BMAP() -> %d", error,0,0,0);
			skipbytes += bytes;
			bytes = 0;
			break;
		}

		iobytes = MIN((((off_t)lbn + 1 + run) << fs_bshift) - offset,
		    bytes);
		if (blkno == (daddr_t)-1) {
			skipbytes += iobytes;
			continue;
		}

		/* if it's really one i/o, don't make a second buf */
		if (offset == startoffset && iobytes == bytes) {
			bp = mbp;
		} else {
			s = splbio();
			vp->v_numoutput++;
			bp = pool_get(&bufpool, PR_WAITOK);
			UVMHIST_LOG(ubchist, "vp %p bp %p num now %d",
				    vp, bp, vp->v_numoutput, 0);
			splx(s);
			bp->b_data = (char *)kva +
				(vaddr_t)(offset - pg->offset);
			bp->b_resid = bp->b_bcount = iobytes;
			bp->b_flags = B_BUSY|B_WRITE|B_CALL|B_ASYNC;
			bp->b_iodone = uvm_aio_biodone1;
			bp->b_vp = vp;
			LIST_INIT(&bp->b_dep);
		}
		bp->b_lblkno = 0;
		bp->b_private = mbp;

		/* adjust physical blkno for partial blocks */
		bp->b_blkno = blkno + ((offset - ((off_t)lbn << fs_bshift)) >>
				       dev_bshift);
		UVMHIST_LOG(ubchist, "vp %p offset 0x%x bcount 0x%x blkno 0x%x",
			    vp, offset, bp->b_bcount, bp->b_blkno);
		VOP_STRATEGY(bp);
	}
	if (skipbytes) {
		UVMHIST_LOG(ubchist, "skipbytes %d", skipbytes, 0,0,0);
		s = splbio();
		mbp->b_resid -= skipbytes;
		if (error) {
			mbp->b_flags |= B_ERROR;
			mbp->b_error = error;
		}
		if (mbp->b_resid == 0) {
			biodone(mbp);
		}
		splx(s);
	}
	if (async) {
		UVMHIST_LOG(ubchist, "returning 0 (async)", 0,0,0,0);
		return 0;
	}
	if (bp != NULL) {
		UVMHIST_LOG(ubchist, "waiting for mbp %p", mbp,0,0,0);
		error = biowait(mbp);
	}
	if (bioops.io_pageiodone) {
		(*bioops.io_pageiodone)(mbp);
	}
	s = splbio();
	if (mbp->b_vp) {
		vwakeup(mbp->b_vp);
	}
	buf_cleanout(mbp);
	pool_put(&bufpool, mbp);
	splx(s);
	uvm_pagermapout(kva, npages);
	UVMHIST_LOG(ubchist, "returning, error %d", error,0,0,0);
	return error;
}

int
genfs_size(v)
	void *v;
{
	struct vop_size_args /* {
		struct vnode *a_vp;
		off_t a_size;
		off_t *a_eobp;
	} */ *ap = v;
	int bsize;

	bsize = 1 << ap->a_vp->v_mount->mnt_fs_bshift;
	*ap->a_eobp = (ap->a_size + bsize - 1) & ~(bsize - 1);
	return 0;
}

d316 1
a316 1
genfs_mmap(v)
@


1.13
log
@Readd VOP_MMAP, will be used soon. Right now it's just a question to
the filesystem if we're allowed to mmap the file.
@
text
@d1 1
a1 1
/*       $OpenBSD: vfs_default.c,v 1.12 2001/11/30 05:45:33 csapuntz Exp $  */
d346 1
a346 1
	struct uvm_object *uobj = &vp->v_uvm.u_obj;
d366 1
a366 1
	error = VOP_SIZE(vp, vp->v_uvm.u_size, &diskeof);
d371 2
a372 2
		newsize = MAX(vp->v_uvm.u_size,
			      origoffset + (orignpages << PAGE_SHIFT));
d858 1
a858 1
	simple_unlock(&vp->v_uvm.u_obj.vmobjlock);
d860 1
a860 1
	error = VOP_SIZE(vp, vp->v_uvm.u_size, &eof);
@


1.12
log
@Call buf_cleanout, which handles wakeups
@
text
@d1 1
a1 1
/*       $OpenBSD: vfs_default.c,v 1.11 2001/11/29 15:51:48 art Exp $  */
d994 7
@


1.11
log
@Bunch of bug fixes from NetBSD.
Mostly dealing with holes in files.
@
text
@d1 1
a1 1
/*       $OpenBSD: vfs_default.c,v 1.10 2001/11/29 02:08:21 art Exp $  */
d693 1
a693 3
	if (mbp->b_vp != NULL) {
		brelvp(mbp);
	}
a971 1
		brelvp(mbp);
d973 1
@


1.10
log
@Make sure the whole buffer is initialized before calling bgetvp.
Recommended by csapuntz@@
@
text
@d1 1
a1 1
/*       $OpenBSD: vfs_default.c,v 1.9 2001/11/29 01:58:57 art Exp $  */
d485 2
a486 1
	if (startoffset != origoffset) {
d495 1
a495 1
		for (i = 0; i < npages; i++) {
d502 1
a502 1
		uvm_page_unbusy(&pgs[ridx], npages);
a506 1
		npages = (endoffset - startoffset) >> PAGE_SHIFT;
d564 1
a564 1
		while ((pgs[pidx]->flags & PG_FAKE) == 0) {
d619 2
d628 4
a631 5
			if (!write) {
				int holepages =
					(round_page(offset + iobytes) - 
					 trunc_page(offset)) >> PAGE_SHIFT;
				for (i = 0; i < holepages; i++) {
d642 1
a642 1
 		 */
d685 1
a685 1
		UVMHIST_LOG(ubchist, "returning PEND",0,0,0,0);
d687 1
a687 1
		return EINPROGRESS;
d961 2
a962 2
		UVMHIST_LOG(ubchist, "returning PEND", 0,0,0,0);
		return EINPROGRESS;
@


1.9
log
@Correctly handle b_vp with bgetvp and brelvp in {get,put}pages.
Prevents panics caused by vnodes being recycled under our feet.
@
text
@d1 1
a1 1
/*       $OpenBSD: vfs_default.c,v 1.8 2001/11/27 05:27:12 art Exp $  */
d533 1
a534 1
	LIST_INIT(&mbp->b_dep);
d895 1
a896 1
	LIST_INIT(&mbp->b_dep);
@


1.8
log
@Merge in the unified buffer cache code as found in NetBSD 2001/03/10. The
code is written mostly by Chuck Silvers <chuq@@chuq.com>/<chs@@netbsd.org>.

Tested for the past few weeks by many developers, should be in a pretty stable
state, but will require optimizations and additional cleanups.
@
text
@d1 1
a1 1
/*       $OpenBSD: vfs_default.c,v 1.7 2001/06/25 03:28:03 csapuntz Exp $  */
d532 2
a533 1
	mbp->b_vp = vp;
d641 1
a641 1
		 */
d692 3
d894 2
a895 1
	mbp->b_vp = vp;
d971 1
a971 1
	if (mbp->b_vp)
d973 2
@


1.7
log
@Remove NQNFS
@
text
@d1 1
a1 2
/*       $OpenBSD: vfs_default.c,v 1.6 2001/06/22 14:14:10 deraadt Exp $  */

d6 1
a6 1
 * Copyright (c) 1989, 1993
d51 1
d55 1
d313 676
@


1.6
log
@KNF
@
text
@d1 1
a1 1
/*       $OpenBSD: vfs_default.c,v 1.5 2001/06/05 20:54:52 provos Exp $  */
d304 8
@


1.5
log
@generic kqfilter that just like select returns ready for read and write.
@
text
@d1 1
a1 1
/*       $OpenBSD: vfs_default.c,v 1.4 2001/03/16 01:09:48 csapuntz Exp $  */
d70 1
a70 1
	        struct vnode *a_vp;
d211 7
a217 7
        /*
         * Since we are not using the lock manager, we must clear
         * the interlock here.
         */
        if (ap->a_flags & LK_INTERLOCK)
                simple_unlock(&ap->a_vp->v_interlock);
        return (0);
@


1.4
log
@Locking bug on spechash. Thanks to Dawson Engler and team
@
text
@d1 1
a1 1
/*       $OpenBSD: vfs_default.c,v 1.3 1998/08/06 19:34:24 csapuntz Exp $  */
d52 1
d58 3
d258 46
@


1.3
log
@

Rename vop_revoke, vn_bwrite, vop_noislocked, vop_nolock, vop_nounlock
to be vop_generic_revoke, vop_generic_bwrite, vop_generic_islocked,
vop_generic_lock and vop_generic_unlock.

Create vop_generic_abortop and propogate change to all file systems.

Fix PR/371.

Get rid of locking in NULLFS (should be mostly unnecessary now except for
forced unmounts).
@
text
@d1 1
a1 1
/*       $OpenBSD: vfs_default.c,v 1.2 1998/01/11 02:10:44 csapuntz Exp $  */
d107 1
@


1.3.8.1
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 1
/*       $OpenBSD: vfs_default.c,v 1.4 2001/03/16 01:09:48 csapuntz Exp $  */
a106 1
			simple_unlock(&spechash_slock);
@


1.3.8.2
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*       $OpenBSD: vfs_default.c,v 1.3.8.1 2001/05/14 22:32:46 niklas Exp $  */
a51 1
#include <sys/event.h>
a56 3
int filt_generic_readwrite __P((struct knote *kn, long hint));
void filt_generic_detach __P((struct knote *kn));

d66 1
a66 1
		struct vnode *a_vp;
d207 7
a213 7
	/*
	 * Since we are not using the lock manager, we must clear
	 * the interlock here.
	 */
	if (ap->a_flags & LK_INTERLOCK)
		simple_unlock(&ap->a_vp->v_interlock);
	return (0);
a253 54
}

struct filterops generic_filtops = 
	{ 1, NULL, filt_generic_detach, filt_generic_readwrite };

int
vop_generic_kqfilter(v)
	void *v;
{
	struct vop_kqfilter_args /* {
		struct vnode *a_vp;
		struct knote *a_kn;
	} */ *ap = v;
	struct knote *kn = ap->a_kn;

	switch (kn->kn_filter) {
	case EVFILT_READ:
	case EVFILT_WRITE:
		kn->kn_fop = &generic_filtops;
		break;
	default:
		return (1);
	}

	return (0);
}

void
filt_generic_detach(struct knote *kn)
{
}

int
filt_generic_readwrite(struct knote *kn, long hint)
{
	/*
	 * filesystem is gone, so set the EOF flag and schedule 
	 * the knote for deletion.
	 */
	if (hint == NOTE_REVOKE) {
		kn->kn_flags |= (EV_EOF | EV_ONESHOT);
		return (1);
	}

        kn->kn_data = 0;
        return (1);
}

int lease_check(void *);

int
lease_check(void *v)
{
	return (0);
@


1.3.8.3
log
@Merge in -current
@
text
@d1 2
a2 1
/*       $OpenBSD$  */
d7 1
a7 1
 * Copyright (c) 1982, 1986, 1989, 1993
a51 1
#include <sys/pool.h>
a54 1
#include <uvm/uvm.h>
a311 682
}

/*
 * generic VM getpages routine.
 * Return PG_BUSY pages for the given range,
 * reading from backing store if necessary.
 */

int
genfs_getpages(v)
	void *v;
{
	struct vop_getpages_args /* {
		struct vnode *a_vp;
		voff_t a_offset;
		vm_page_t *a_m;
		int *a_count;
		int a_centeridx;
		vm_prot_t a_access_type;
		int a_advice;
		int a_flags;
	} */ *ap = v;

	off_t newsize, diskeof, memeof;
	off_t offset, origoffset, startoffset, endoffset, raoffset;
	daddr_t lbn, blkno;
	int s, i, error, npages, orignpages, npgs, run, ridx, pidx, pcount;
	int fs_bshift, fs_bsize, dev_bshift, dev_bsize;
	int flags = ap->a_flags;
	size_t bytes, iobytes, tailbytes, totalbytes, skipbytes;
	vaddr_t kva;
	struct buf *bp, *mbp;
	struct vnode *vp = ap->a_vp;
	struct uvm_object *uobj = &vp->v_uvm.u_obj;
	struct vm_page *pgs[16];			/* XXXUBC 16 */
	struct ucred *cred = curproc->p_ucred;		/* XXXUBC curproc */
	boolean_t async = (flags & PGO_SYNCIO) == 0;
	boolean_t write = (ap->a_access_type & VM_PROT_WRITE) != 0;
	boolean_t sawhole = FALSE;
	struct proc *p = curproc;
	UVMHIST_FUNC("genfs_getpages"); UVMHIST_CALLED(ubchist);

	UVMHIST_LOG(ubchist, "vp %p off 0x%x/%x count %d",
		    vp, ap->a_offset >> 32, ap->a_offset, *ap->a_count);

	/* XXXUBC temp limit */
	if (*ap->a_count > 16) {
		return EINVAL;
	}

	error = 0;
	origoffset = ap->a_offset;
	orignpages = *ap->a_count;
	error = VOP_SIZE(vp, vp->v_uvm.u_size, &diskeof);
	if (error) {
		return error;
	}
	if (flags & PGO_PASTEOF) {
		newsize = MAX(vp->v_uvm.u_size,
			      origoffset + (orignpages << PAGE_SHIFT));
		error = VOP_SIZE(vp, newsize, &memeof);
		if (error) {
			return error;
		}
	} else {
		memeof = diskeof;
	}
	KASSERT(ap->a_centeridx >= 0 || ap->a_centeridx <= orignpages);
	KASSERT((origoffset & (PAGE_SIZE - 1)) == 0 && origoffset >= 0);
	KASSERT(orignpages > 0);

	/*
	 * Bounds-check the request.
	 */

	if (origoffset + (ap->a_centeridx << PAGE_SHIFT) >= memeof) {
		if ((flags & PGO_LOCKED) == 0) {
			simple_unlock(&uobj->vmobjlock);
		}
		UVMHIST_LOG(ubchist, "off 0x%x count %d goes past EOF 0x%x",
			    origoffset, *ap->a_count, memeof,0);
		return EINVAL;
	}

	/*
	 * For PGO_LOCKED requests, just return whatever's in memory.
	 */

	if (flags & PGO_LOCKED) {
		uvn_findpages(uobj, origoffset, ap->a_count, ap->a_m,
			      UFP_NOWAIT|UFP_NOALLOC|UFP_NORDONLY);

		return ap->a_m[ap->a_centeridx] == NULL ? EBUSY : 0;
	}

	/* vnode is VOP_LOCKed, uobj is locked */

	if (write && (vp->v_bioflag & VBIOONSYNCLIST) == 0) {
		vn_syncer_add_to_worklist(vp, syncdelay);
	}

	/*
	 * find the requested pages and make some simple checks.
	 * leave space in the page array for a whole block.
	 */

	fs_bshift = vp->v_mount->mnt_fs_bshift;
	fs_bsize = 1 << fs_bshift;
	dev_bshift = vp->v_mount->mnt_dev_bshift;
	dev_bsize = 1 << dev_bshift;
	KASSERT((diskeof & (dev_bsize - 1)) == 0);
	KASSERT((memeof & (dev_bsize - 1)) == 0);

	orignpages = MIN(orignpages,
	    round_page(memeof - origoffset) >> PAGE_SHIFT);
	npages = orignpages;
	startoffset = origoffset & ~(fs_bsize - 1);
	endoffset = round_page((origoffset + (npages << PAGE_SHIFT)
				+ fs_bsize - 1) & ~(fs_bsize - 1));
	endoffset = MIN(endoffset, round_page(memeof));
	ridx = (origoffset - startoffset) >> PAGE_SHIFT;

	memset(pgs, 0, sizeof(pgs));
	uvn_findpages(uobj, origoffset, &npages, &pgs[ridx], UFP_ALL);

	/*
	 * if PGO_OVERWRITE is set, don't bother reading the pages.
	 * PGO_OVERWRITE also means that the caller guarantees
	 * that the pages already have backing store allocated.
	 */

	if (flags & PGO_OVERWRITE) {
		UVMHIST_LOG(ubchist, "PGO_OVERWRITE",0,0,0,0);

		for (i = 0; i < npages; i++) {
			struct vm_page *pg = pgs[ridx + i];

			if (pg->flags & PG_FAKE) {
				uvm_pagezero(pg);
				pg->flags &= ~(PG_FAKE);
			}
			pg->flags &= ~(PG_RDONLY);
		}
		npages += ridx;
		goto out;
	}

	/*
	 * if the pages are already resident, just return them.
	 */

	for (i = 0; i < npages; i++) {
		struct vm_page *pg = pgs[ridx + i];

		if ((pg->flags & PG_FAKE) ||
		    (write && (pg->flags & PG_RDONLY))) {
			break;
		}
	}
	if (i == npages) {
		UVMHIST_LOG(ubchist, "returning cached pages", 0,0,0,0);
		raoffset = origoffset + (orignpages << PAGE_SHIFT);
		npages += ridx;
		goto raout;
	}

	/*
	 * the page wasn't resident and we're not overwriting,
	 * so we're going to have to do some i/o.
	 * find any additional pages needed to cover the expanded range.
	 */

	npages = (endoffset - startoffset) >> PAGE_SHIFT;
	if (startoffset != origoffset || npages != orignpages) {

		/*
		 * XXXUBC we need to avoid deadlocks caused by locking
		 * additional pages at lower offsets than pages we
		 * already have locked.  for now, unlock them all and
		 * start over.
		 */

		for (i = 0; i < orignpages; i++) {
			struct vm_page *pg = pgs[ridx + i];

			if (pg->flags & PG_FAKE) {
				pg->flags |= PG_RELEASED;
			}
		}
		uvm_page_unbusy(&pgs[ridx], orignpages);
		memset(pgs, 0, sizeof(pgs));

		UVMHIST_LOG(ubchist, "reset npages start 0x%x end 0x%x",
			    startoffset, endoffset, 0,0);
		npgs = npages;
		uvn_findpages(uobj, startoffset, &npgs, pgs, UFP_ALL);
	}
	simple_unlock(&uobj->vmobjlock);

	/*
	 * read the desired page(s).
	 */

	totalbytes = npages << PAGE_SHIFT;
	bytes = MIN(totalbytes, MAX(diskeof - startoffset, 0));
	tailbytes = totalbytes - bytes;
	skipbytes = 0;

	kva = uvm_pagermapin(pgs, npages, UVMPAGER_MAPIN_WAITOK |
			     UVMPAGER_MAPIN_READ);

	s = splbio();
	mbp = pool_get(&bufpool, PR_WAITOK);
	splx(s);
	mbp->b_bufsize = totalbytes;
	mbp->b_data = (void *)kva;
	mbp->b_resid = mbp->b_bcount = bytes;
	mbp->b_flags = B_BUSY|B_READ| (async ? B_CALL : 0);
	mbp->b_iodone = uvm_aio_biodone;
	mbp->b_vp = NULL;
	LIST_INIT(&mbp->b_dep);
	bgetvp(vp, mbp);

	/*
	 * if EOF is in the middle of the range, zero the part past EOF.
	 */

	if (tailbytes > 0) {
		memset((void *)(kva + bytes), 0, tailbytes);
	}

	/*
	 * now loop over the pages, reading as needed.
	 */

	if (write) {
		lockmgr(&vp->v_glock, LK_EXCLUSIVE, NULL, p);
	} else {
		lockmgr(&vp->v_glock, LK_SHARED, NULL, p);
	}

	bp = NULL;
	for (offset = startoffset;
	     bytes > 0;
	     offset += iobytes, bytes -= iobytes) {

		/*
		 * skip pages which don't need to be read.
		 */

		pidx = (offset - startoffset) >> PAGE_SHIFT;
		while ((pgs[pidx]->flags & (PG_FAKE|PG_RDONLY)) == 0) {
			size_t b;

			KASSERT((offset & (PAGE_SIZE - 1)) == 0);
			b = MIN(PAGE_SIZE, bytes);
			offset += b;
			bytes -= b;
			skipbytes += b;
			pidx++;
			UVMHIST_LOG(ubchist, "skipping, new offset 0x%x",
				    offset, 0,0,0);
			if (bytes == 0) {
				goto loopdone;
			}
		}

		/*
		 * bmap the file to find out the blkno to read from and
		 * how much we can read in one i/o.  if bmap returns an error,
		 * skip the rest of the top-level i/o.
		 */

		lbn = offset >> fs_bshift;
		error = VOP_BMAP(vp, lbn, NULL, &blkno, &run);
		if (error) {
			UVMHIST_LOG(ubchist, "VOP_BMAP lbn 0x%x -> %d\n",
				    lbn, error,0,0);
			skipbytes += bytes;
			goto loopdone;
		}

		/*
		 * see how many pages can be read with this i/o.
		 * reduce the i/o size if necessary to avoid
		 * overwriting pages with valid data.
		 */

		iobytes = MIN((((off_t)lbn + 1 + run) << fs_bshift) - offset,
		    bytes);
		if (offset + iobytes > round_page(offset)) {
			pcount = 1;
			while (pidx + pcount < npages &&
			       pgs[pidx + pcount]->flags & PG_FAKE) {
				pcount++;
			}
			iobytes = MIN(iobytes, (pcount << PAGE_SHIFT) -
				      (offset - trunc_page(offset)));
		}

		/*
		 * if this block isn't allocated, zero it instead of reading it.
		 * if this is a read access, mark the pages we zeroed PG_RDONLY.
		 */

		if (blkno < 0) {
			int holepages = (round_page(offset + iobytes) - 
					 trunc_page(offset)) >> PAGE_SHIFT;
			UVMHIST_LOG(ubchist, "lbn 0x%x -> HOLE", lbn,0,0,0);

			sawhole = TRUE;
			memset((char *)kva + (offset - startoffset), 0,
			       iobytes);
			skipbytes += iobytes;

			for (i = 0; i < holepages; i++) {
				if (write) {
					pgs[pidx + i]->flags &= ~PG_CLEAN;
				} else {
					pgs[pidx + i]->flags |= PG_RDONLY;
				}
			}
			continue;
		}

		/*
		 * allocate a sub-buf for this piece of the i/o
		 * (or just use mbp if there's only 1 piece),
		 * and start it going.
		 */

		if (offset == startoffset && iobytes == bytes) {
			bp = mbp;
		} else {
			s = splbio();
			bp = pool_get(&bufpool, PR_WAITOK);
			splx(s);
			bp->b_data = (char *)kva + offset - startoffset;
			bp->b_resid = bp->b_bcount = iobytes;
			bp->b_flags = B_BUSY|B_READ|B_CALL;
			bp->b_iodone = uvm_aio_biodone1;
			bp->b_vp = vp;
			LIST_INIT(&bp->b_dep);
		}
		bp->b_lblkno = 0;
		bp->b_private = mbp;

		/* adjust physical blkno for partial blocks */
		bp->b_blkno = blkno + ((offset - ((off_t)lbn << fs_bshift)) >>
				       dev_bshift);

		UVMHIST_LOG(ubchist, "bp %p offset 0x%x bcount 0x%x blkno 0x%x",
			    bp, offset, iobytes, bp->b_blkno);

		VOP_STRATEGY(bp);
	}

loopdone:
	if (skipbytes) {
		s = splbio();
		if (error) {
			mbp->b_flags |= B_ERROR;
			mbp->b_error = error;
		}
		mbp->b_resid -= skipbytes;
		if (mbp->b_resid == 0) {
			biodone(mbp);
		}
		splx(s);
	}

	if (async) {
		UVMHIST_LOG(ubchist, "returning 0 (async)",0,0,0,0);
		lockmgr(&vp->v_glock, LK_RELEASE, NULL, p);
		return 0;
	}
	if (bp != NULL) {
		error = biowait(mbp);
	}
	s = splbio();
	(void) buf_cleanout(mbp);
	pool_put(&bufpool, mbp);
	splx(s);
	uvm_pagermapout(kva, npages);
	raoffset = startoffset + totalbytes;

	/*
	 * if this we encountered a hole then we have to do a little more work.
	 * for read faults, we marked the page PG_RDONLY so that future
	 * write accesses to the page will fault again.
	 * for write faults, we must make sure that the backing store for
	 * the page is completely allocated while the pages are locked.
	 */

	if (error == 0 && sawhole && write) {
		error = VOP_BALLOCN(vp, startoffset, npages << PAGE_SHIFT,
				   cred, 0);
		if (error) {
			UVMHIST_LOG(ubchist, "balloc lbn 0x%x -> %d",
				    lbn, error,0,0);
			lockmgr(&vp->v_glock, LK_RELEASE, NULL, p);
			simple_lock(&uobj->vmobjlock);
			goto out;
		}
	}
	lockmgr(&vp->v_glock, LK_RELEASE, NULL, p);
	simple_lock(&uobj->vmobjlock);

	/*
	 * see if we want to start any readahead.
	 * XXXUBC for now, just read the next 128k on 64k boundaries.
	 * this is pretty nonsensical, but it is 50% faster than reading
	 * just the next 64k.
	 */

raout:
	if (!error && !async && !write && ((int)raoffset & 0xffff) == 0 &&
	    PAGE_SHIFT <= 16) {
		int racount;

		racount = 1 << (16 - PAGE_SHIFT);
		(void) VOP_GETPAGES(vp, raoffset, NULL, &racount, 0,
				    VM_PROT_READ, 0, 0);
		simple_lock(&uobj->vmobjlock);

		racount = 1 << (16 - PAGE_SHIFT);
		(void) VOP_GETPAGES(vp, raoffset + 0x10000, NULL, &racount, 0,
				    VM_PROT_READ, 0, 0);
		simple_lock(&uobj->vmobjlock);
	}

	/*
	 * we're almost done!  release the pages...
	 * for errors, we free the pages.
	 * otherwise we activate them and mark them as valid and clean.
	 * also, unbusy pages that were not actually requested.
	 */

out:
	if (error) {
		uvm_lock_pageq();
		for (i = 0; i < npages; i++) {
			if (pgs[i] == NULL) {
				continue;
			}
			UVMHIST_LOG(ubchist, "examining pg %p flags 0x%x",
				    pgs[i], pgs[i]->flags, 0,0);
			if (pgs[i]->flags & PG_WANTED) {
				wakeup(pgs[i]);
			}
			if (pgs[i]->flags & PG_RELEASED) {
				uvm_unlock_pageq();
				(uobj->pgops->pgo_releasepg)(pgs[i], NULL);
				uvm_lock_pageq();
				continue;
			}
			if (pgs[i]->flags & PG_FAKE) {
				uvm_pagefree(pgs[i]);
				continue;
			}
			uvm_pageactivate(pgs[i]);
			pgs[i]->flags &= ~(PG_WANTED|PG_BUSY);
			UVM_PAGE_OWN(pgs[i], NULL);
		}
		uvm_unlock_pageq();
		simple_unlock(&uobj->vmobjlock);
		UVMHIST_LOG(ubchist, "returning error %d", error,0,0,0);
		return error;
	}

	UVMHIST_LOG(ubchist, "succeeding, npages %d", npages,0,0,0);
	uvm_lock_pageq();
	for (i = 0; i < npages; i++) {
		if (pgs[i] == NULL) {
			continue;
		}
		UVMHIST_LOG(ubchist, "examining pg %p flags 0x%x",
			    pgs[i], pgs[i]->flags, 0,0);
		if (pgs[i]->flags & PG_FAKE) {
			UVMHIST_LOG(ubchist, "unfaking pg %p offset 0x%x",
				    pgs[i], pgs[i]->offset,0,0);
			pgs[i]->flags &= ~(PG_FAKE);
			pmap_clear_modify(pgs[i]);
			pmap_clear_reference(pgs[i]);
		}
		if (write) {
			pgs[i]->flags &= ~(PG_RDONLY);
		}
		if (i < ridx || i >= ridx + orignpages || async) {
			UVMHIST_LOG(ubchist, "unbusy pg %p offset 0x%x",
				    pgs[i], pgs[i]->offset,0,0);
			if (pgs[i]->flags & PG_WANTED) {
				wakeup(pgs[i]);
			}
			if (pgs[i]->flags & PG_RELEASED) {
				uvm_unlock_pageq();
				(uobj->pgops->pgo_releasepg)(pgs[i], NULL);
				uvm_lock_pageq();
				continue;
			}
			uvm_pageactivate(pgs[i]);
			pgs[i]->flags &= ~(PG_WANTED|PG_BUSY);
			UVM_PAGE_OWN(pgs[i], NULL);
		}
	}
	uvm_unlock_pageq();
	simple_unlock(&uobj->vmobjlock);
	if (ap->a_m != NULL) {
		memcpy(ap->a_m, &pgs[ridx],
		       orignpages * sizeof(struct vm_page *));
	}
	return 0;
}

/*
 * generic VM putpages routine.
 * Write the given range of pages to backing store.
 */

int
genfs_putpages(v)
	void *v;
{
	struct vop_putpages_args /* {
		struct vnode *a_vp;
		struct vm_page **a_m;
		int a_count;
		int a_flags;
		int *a_rtvals;
	} */ *ap = v;

	int s, error, npages, run;
	int fs_bshift, dev_bshift, dev_bsize;
	vaddr_t kva;
	off_t eof, offset, startoffset;
	size_t bytes, iobytes, skipbytes;
	daddr_t lbn, blkno;
	struct vm_page *pg;
	struct buf *mbp, *bp;
	struct vnode *vp = ap->a_vp;
	boolean_t async = (ap->a_flags & PGO_SYNCIO) == 0;
	UVMHIST_FUNC("genfs_putpages"); UVMHIST_CALLED(ubchist);
	UVMHIST_LOG(ubchist, "vp %p offset 0x%x count %d",
		    vp, ap->a_m[0]->offset, ap->a_count, 0);

	simple_unlock(&vp->v_uvm.u_obj.vmobjlock);

	error = VOP_SIZE(vp, vp->v_uvm.u_size, &eof);
	if (error) {
		return error;
	}

	error = 0;
	npages = ap->a_count;
	fs_bshift = vp->v_mount->mnt_fs_bshift;
	dev_bshift = vp->v_mount->mnt_dev_bshift;
	dev_bsize = 1 << dev_bshift;
	KASSERT((eof & (dev_bsize - 1)) == 0);

	pg = ap->a_m[0];
	startoffset = pg->offset;
	bytes = MIN(npages << PAGE_SHIFT, eof - startoffset);
	skipbytes = 0;
	KASSERT(bytes != 0);

	kva = uvm_pagermapin(ap->a_m, npages, UVMPAGER_MAPIN_WAITOK);

	s = splbio();
	vp->v_numoutput += 2;
	mbp = pool_get(&bufpool, PR_WAITOK);
	UVMHIST_LOG(ubchist, "vp %p mbp %p num now %d bytes 0x%x",
		    vp, mbp, vp->v_numoutput, bytes);
	splx(s);
	mbp->b_bufsize = npages << PAGE_SHIFT;
	mbp->b_data = (void *)kva;
	mbp->b_resid = mbp->b_bcount = bytes;
	mbp->b_flags = B_BUSY|B_WRITE|B_AGE |
		(async ? B_CALL : 0) |
		(curproc == uvm.pagedaemon_proc ? B_PDAEMON : 0);
	mbp->b_iodone = uvm_aio_biodone;
	mbp->b_vp = NULL;
	LIST_INIT(&mbp->b_dep);
	bgetvp(vp, mbp);

	bp = NULL;
	for (offset = startoffset;
	     bytes > 0;
	     offset += iobytes, bytes -= iobytes) {
		lbn = offset >> fs_bshift;
		error = VOP_BMAP(vp, lbn, NULL, &blkno, &run);
		if (error) {
			UVMHIST_LOG(ubchist, "VOP_BMAP() -> %d", error,0,0,0);
			skipbytes += bytes;
			bytes = 0;
			break;
		}

		iobytes = MIN((((off_t)lbn + 1 + run) << fs_bshift) - offset,
		    bytes);
		if (blkno == (daddr_t)-1) {
			skipbytes += iobytes;
			continue;
		}

		/* if it's really one i/o, don't make a second buf */
		if (offset == startoffset && iobytes == bytes) {
			bp = mbp;
		} else {
			s = splbio();
			vp->v_numoutput++;
			bp = pool_get(&bufpool, PR_WAITOK);
			UVMHIST_LOG(ubchist, "vp %p bp %p num now %d",
				    vp, bp, vp->v_numoutput, 0);
			splx(s);
			bp->b_data = (char *)kva +
				(vaddr_t)(offset - pg->offset);
			bp->b_resid = bp->b_bcount = iobytes;
			bp->b_flags = B_BUSY|B_WRITE|B_CALL|B_ASYNC;
			bp->b_iodone = uvm_aio_biodone1;
			bp->b_vp = vp;
			LIST_INIT(&bp->b_dep);
		}
		bp->b_lblkno = 0;
		bp->b_private = mbp;

		/* adjust physical blkno for partial blocks */
		bp->b_blkno = blkno + ((offset - ((off_t)lbn << fs_bshift)) >>
				       dev_bshift);
		UVMHIST_LOG(ubchist, "vp %p offset 0x%x bcount 0x%x blkno 0x%x",
			    vp, offset, bp->b_bcount, bp->b_blkno);
		VOP_STRATEGY(bp);
	}
	if (skipbytes) {
		UVMHIST_LOG(ubchist, "skipbytes %d", skipbytes, 0,0,0);
		s = splbio();
		mbp->b_resid -= skipbytes;
		if (error) {
			mbp->b_flags |= B_ERROR;
			mbp->b_error = error;
		}
		if (mbp->b_resid == 0) {
			biodone(mbp);
		}
		splx(s);
	}
	if (async) {
		UVMHIST_LOG(ubchist, "returning 0 (async)", 0,0,0,0);
		return 0;
	}
	if (bp != NULL) {
		UVMHIST_LOG(ubchist, "waiting for mbp %p", mbp,0,0,0);
		error = biowait(mbp);
	}
	if (bioops.io_pageiodone) {
		(*bioops.io_pageiodone)(mbp);
	}
	s = splbio();
	if (mbp->b_vp) {
		vwakeup(mbp->b_vp);
	}
	buf_cleanout(mbp);
	pool_put(&bufpool, mbp);
	splx(s);
	uvm_pagermapout(kva, npages);
	UVMHIST_LOG(ubchist, "returning, error %d", error,0,0,0);
	return error;
}

int
genfs_size(v)
	void *v;
{
	struct vop_size_args /* {
		struct vnode *a_vp;
		off_t a_size;
		off_t *a_eobp;
	} */ *ap = v;
	int bsize;

	bsize = 1 << ap->a_vp->v_mount->mnt_fs_bshift;
	*ap->a_eobp = (ap->a_size + bsize - 1) & ~(bsize - 1);
	return 0;
@


1.3.8.4
log
@Merge in trunk
@
text
@d1 1
a1 1
/*	$OpenBSD$  */
d6 1
a6 1
 * Copyright (c) 1989, 1993
d51 1
d55 1
d315 4
a318 3
/* 
 * vfs default ops
 * used to fill the vfs function table to get reasonable default return values.
d320 1
d322 2
a323 7
vfs_stdextattrctl(mp, cmd, filename_vp, attrnamespace, attrname, td)
	struct mount *mp;
	int cmd;
	struct vnode *filename_vp;
	int attrnamespace;
	const char *attrname;
	struct proc *td;
d325 670
a994 1
	return(EOPNOTSUPP);
@


1.3.8.5
log
@Merge in -current from about a week ago
@
text
@d57 2
a58 2
int filt_generic_readwrite(struct knote *kn, long hint);
void filt_generic_detach(struct knote *kn);
@


1.3.8.6
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: vfs_default.c,v 1.3.8.5 2002/03/28 11:43:04 niklas Exp $  */
d22 5
a26 1
 * 3. Neither the name of the University nor the names of its contributors
@


1.3.8.7
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$  */
a64 1
		struct vnodeop_desc *a_desc;
d87 1
a87 1
			tsleep(vp, PINOD, "vop_generic_revokeall", 0);
a135 1
		struct vnodeop_desc *a_desc;
a155 1
		struct vnodeop_desc *a_desc;
a224 1
		struct vnodeop_desc *a_desc;
a244 1
		struct vnodeop_desc *a_desc;
a262 1
		struct vnodeop_desc *a_desc;
@


1.3.8.8
log
@Merge with the trunk
@
text
@a46 1
#include <sys/pool.h>
d143 1
a143 1
		pool_put(&namei_pool, ap->a_cnp->cn_pnbuf);
d151 1
a151 1
 * count is maintained in an auxiliary vnode lock structure.
@


1.3.8.9
log
@sync with head, make i386 __HAVE_CPUINFO
@
text
@d228 12
a239 1
	return (0);
d249 10
a258 1
	return (0);
@


1.2
log
@Fix a couple spinlock references. More code motion in vfs_subr.c
@
text
@d1 1
a1 1
/*       $OpenBSD: vfs_default.c,v 1.1 1998/01/10 23:44:29 csapuntz Exp $  */
d50 2
a51 1

d62 1
a62 1
vop_revoke(v)
d74 1
a74 1
		panic("vop_revoke");
d88 1
a88 1
			tsleep((caddr_t)vp, PINOD, "vop_revokeall", 0);
d122 1
a122 1
vn_bwrite(v)
d130 15
d152 1
a152 1
vop_nolock(v)
d200 1
a200 1
		panic("vop_nolock: bad operation %d", flags & LK_TYPE_MASK);
d221 1
a221 1
vop_nounlock(v)
d241 1
a241 1
vop_noislocked(v)
@


1.1
log
@A couple more splbio()s in vfs_bio plus moving around a couple functions.
@
text
@d1 1
a1 1
/*       $OpenBSD: $  */
d53 2
@
