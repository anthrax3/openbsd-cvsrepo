head	1.220;
access;
symbols
	OPENBSD_6_2:1.220.0.2
	OPENBSD_6_2_BASE:1.220
	OPENBSD_6_1:1.207.0.4
	OPENBSD_6_1_BASE:1.207
	OPENBSD_6_0:1.194.0.4
	OPENBSD_6_0_BASE:1.194
	OPENBSD_5_9:1.194.0.2
	OPENBSD_5_9_BASE:1.194
	OPENBSD_5_8:1.187.0.4
	OPENBSD_5_8_BASE:1.187
	OPENBSD_5_7:1.180.0.2
	OPENBSD_5_7_BASE:1.180
	OPENBSD_5_6:1.138.0.4
	OPENBSD_5_6_BASE:1.138
	OPENBSD_5_5:1.124.0.4
	OPENBSD_5_5_BASE:1.124
	OPENBSD_5_4:1.122.0.2
	OPENBSD_5_4_BASE:1.122
	OPENBSD_5_3:1.114.0.2
	OPENBSD_5_3_BASE:1.114
	OPENBSD_5_2:1.111.0.4
	OPENBSD_5_2_BASE:1.111
	OPENBSD_5_1_BASE:1.111
	OPENBSD_5_1:1.111.0.2
	OPENBSD_5_0:1.108.0.2
	OPENBSD_5_0_BASE:1.108
	OPENBSD_4_9:1.99.0.2
	OPENBSD_4_9_BASE:1.99
	OPENBSD_4_8:1.96.0.2
	OPENBSD_4_8_BASE:1.96
	OPENBSD_4_7:1.91.0.2
	OPENBSD_4_7_BASE:1.91
	OPENBSD_4_6:1.85.0.4
	OPENBSD_4_6_BASE:1.85
	OPENBSD_4_5:1.78.0.2
	OPENBSD_4_5_BASE:1.78
	OPENBSD_4_4:1.62.0.2
	OPENBSD_4_4_BASE:1.62
	OPENBSD_4_3:1.58.0.2
	OPENBSD_4_3_BASE:1.58
	OPENBSD_4_2:1.55.0.2
	OPENBSD_4_2_BASE:1.55
	OPENBSD_4_1:1.48.0.2
	OPENBSD_4_1_BASE:1.48
	OPENBSD_4_0:1.47.0.2
	OPENBSD_4_0_BASE:1.47
	OPENBSD_3_9:1.45.0.8
	OPENBSD_3_9_BASE:1.45
	OPENBSD_3_8:1.45.0.6
	OPENBSD_3_8_BASE:1.45
	OPENBSD_3_7:1.45.0.4
	OPENBSD_3_7_BASE:1.45
	OPENBSD_3_6:1.45.0.2
	OPENBSD_3_6_BASE:1.45
	SMP_SYNC_A:1.41
	SMP_SYNC_B:1.41
	OPENBSD_3_5:1.39.0.2
	OPENBSD_3_5_BASE:1.39
	OPENBSD_3_4:1.38.0.4
	OPENBSD_3_4_BASE:1.38
	UBC_SYNC_A:1.38
	OPENBSD_3_3:1.38.0.2
	OPENBSD_3_3_BASE:1.38
	OPENBSD_3_2:1.32.0.2
	OPENBSD_3_2_BASE:1.32
	OPENBSD_3_1:1.29.0.2
	OPENBSD_3_1_BASE:1.29
	UBC_SYNC_B:1.36
	UBC:1.14.0.2
	UBC_BASE:1.14
	OPENBSD_3_0:1.13.0.2
	OPENBSD_3_0_BASE:1.13
	OPENBSD_2_9_BASE:1.5
	OPENBSD_2_9:1.5.0.2
	OPENBSD_2_8:1.2.0.6
	OPENBSD_2_8_BASE:1.2
	OPENBSD_2_7:1.2.0.4
	OPENBSD_2_7_BASE:1.2
	SMP:1.2.0.2
	SMP_BASE:1.2
	kame_19991208:1.2
	OPENBSD_2_6:1.1.0.4
	OPENBSD_2_6_BASE:1.1
	OPENBSD_2_5:1.1.0.2
	OPENBSD_2_5_BASE:1.1;
locks; strict;
comment	@ * @;


1.220
date	2017.08.13.20.26.33;	author guenther;	state Exp;
branches;
next	1.219;
commitid	ianHWtiz9mgGqNn1;

1.219
date	2017.07.12.08.51.42;	author visa;	state Exp;
branches;
next	1.218;
commitid	nvbTf9NxFV1ajK1z;

1.218
date	2017.07.12.06.39.13;	author visa;	state Exp;
branches;
next	1.217;
commitid	YR6PUZZooxubrdmJ;

1.217
date	2017.06.23.01.21.55;	author dlg;	state Exp;
branches;
next	1.216;
commitid	gm5JD6rPj5mSADno;

1.216
date	2017.06.23.01.02.18;	author dlg;	state Exp;
branches;
next	1.215;
commitid	1LPfavEKmDMmtvIN;

1.215
date	2017.06.19.23.57.12;	author dlg;	state Exp;
branches;
next	1.214;
commitid	OISx8rUonYRdFDmY;

1.214
date	2017.06.16.01.55.45;	author dlg;	state Exp;
branches;
next	1.213;
commitid	lmVxndefRxY5hqss;

1.213
date	2017.06.16.01.33.20;	author dlg;	state Exp;
branches;
next	1.212;
commitid	leZJR70c9dSWCx65;

1.212
date	2017.06.15.03.50.50;	author dlg;	state Exp;
branches;
next	1.211;
commitid	GJEtoSPtB0Hbx6yI;

1.211
date	2017.06.15.03.48.50;	author dlg;	state Exp;
branches;
next	1.210;
commitid	WJqV28gTcrDE374F;

1.210
date	2017.06.15.03.44.17;	author dlg;	state Exp;
branches;
next	1.209;
commitid	PavjbCGvJ1V35dBJ;

1.209
date	2017.06.13.11.41.11;	author dlg;	state Exp;
branches;
next	1.208;
commitid	n2LMNFSlQRIC1mSK;

1.208
date	2017.04.20.14.13.00;	author visa;	state Exp;
branches;
next	1.207;
commitid	GnoPKa34InShCqYl;

1.207
date	2017.02.20.00.43.25;	author dlg;	state Exp;
branches;
next	1.206;
commitid	oUVoeHemWSB8Y9aw;

1.206
date	2017.02.08.05.28.30;	author dlg;	state Exp;
branches;
next	1.205;
commitid	PIgFQZWYthJF3Zdo;

1.205
date	2017.01.24.09.54.41;	author mpi;	state Exp;
branches;
next	1.204;
commitid	c4URq992SDnzQ4jW;

1.204
date	2016.11.21.01.44.06;	author dlg;	state Exp;
branches;
next	1.203;
commitid	JIO0WWMEivOpisVL;

1.203
date	2016.11.07.23.45.27;	author dlg;	state Exp;
branches;
next	1.202;
commitid	805X7HiU6gaUaPlw;

1.202
date	2016.11.02.06.26.16;	author dlg;	state Exp;
branches;
next	1.201;
commitid	ZrUh4wW5fuEb8W9n;

1.201
date	2016.11.02.03.29.48;	author dlg;	state Exp;
branches;
next	1.200;
commitid	IPNJ4jR6zqNpTQqg;

1.200
date	2016.11.02.01.58.07;	author dlg;	state Exp;
branches;
next	1.199;
commitid	hsKr6e0ZYx5Tobqr;

1.199
date	2016.11.02.01.20.50;	author dlg;	state Exp;
branches;
next	1.198;
commitid	U4ALOb3m8BAwcI9m;

1.198
date	2016.09.15.02.00.16;	author dlg;	state Exp;
branches;
next	1.197;
commitid	RlO92XR575sygHqm;

1.197
date	2016.09.15.01.24.08;	author dlg;	state Exp;
branches;
next	1.196;
commitid	3DliHSP24LcOtzRL;

1.196
date	2016.09.05.09.04.31;	author dlg;	state Exp;
branches;
next	1.195;
commitid	6XQAtywm0KL8IsZY;

1.195
date	2016.09.05.07.38.32;	author dlg;	state Exp;
branches;
next	1.194;
commitid	DmvWxtez1rgiAAMH;

1.194
date	2016.01.15.11.21.58;	author dlg;	state Exp;
branches;
next	1.193;
commitid	7LVjORDaEDODwOzR;

1.193
date	2015.09.11.09.26.13;	author kettenis;	state Exp;
branches;
next	1.192;
commitid	vm2PKo2tV2ZBW9Lz;

1.192
date	2015.09.08.21.28.36;	author kettenis;	state Exp;
branches;
next	1.191;
commitid	WSD3bUAMn8qMj0PM;

1.191
date	2015.09.08.13.37.21;	author kettenis;	state Exp;
branches;
next	1.190;
commitid	KsziKFPoqbe9UYlY;

1.190
date	2015.09.06.20.58.14;	author kettenis;	state Exp;
branches;
next	1.189;
commitid	LLsfWTvyCCBhWdG6;

1.189
date	2015.09.01.08.22.45;	author kettenis;	state Exp;
branches;
next	1.188;
commitid	KTJTY6t67v0DzPw6;

1.188
date	2015.08.21.03.03.44;	author dlg;	state Exp;
branches;
next	1.187;
commitid	5XCOzJ8JxtKBfXEx;

1.187
date	2015.07.23.12.44.43;	author dlg;	state Exp;
branches;
next	1.186;
commitid	vWVDnxRGrupOQ5w0;

1.186
date	2015.07.20.23.47.20;	author uebayasi;	state Exp;
branches;
next	1.185;
commitid	CJFzTqJwP65tMk7k;

1.185
date	2015.04.21.13.15.54;	author dlg;	state Exp;
branches;
next	1.184;
commitid	B5LHO4bKFmoDLEhk;

1.184
date	2015.04.07.11.15.02;	author dlg;	state Exp;
branches;
next	1.183;
commitid	w5V4l6pVDMXVfHit;

1.183
date	2015.04.07.11.07.56;	author dlg;	state Exp;
branches;
next	1.182;
commitid	8xqaL0s0mVMHEk93;

1.182
date	2015.03.20.11.33.17;	author dlg;	state Exp;
branches;
next	1.181;
commitid	15ob5YmmjZl3gPnh;

1.181
date	2015.03.14.03.38.50;	author jsg;	state Exp;
branches;
next	1.180;
commitid	p4LJxGKbi0BU2cG6;

1.180
date	2015.02.10.06.16.13;	author dlg;	state Exp;
branches;
next	1.179;
commitid	ujIKFiO4a0xbEB6g;

1.179
date	2015.01.22.05.09.41;	author dlg;	state Exp;
branches;
next	1.178;
commitid	FVa9nJmp6BIt43BS;

1.178
date	2015.01.19.03.57.22;	author dlg;	state Exp;
branches;
next	1.177;
commitid	bJA2iD8hdlNFLIGq;

1.177
date	2015.01.05.23.54.18;	author dlg;	state Exp;
branches;
next	1.176;
commitid	GRKDO7XbkdNF8sek;

1.176
date	2015.01.04.08.54.01;	author dlg;	state Exp;
branches;
next	1.175;
commitid	qMsp04qwfw4zZXi0;

1.175
date	2015.01.04.02.53.19;	author jsg;	state Exp;
branches;
next	1.174;
commitid	lOqfgFqZCfktdRcY;

1.174
date	2014.12.22.02.59.53;	author tedu;	state Exp;
branches;
next	1.173;
commitid	IDPPHd75YO0b2Obx;

1.173
date	2014.12.22.00.33.40;	author dlg;	state Exp;
branches;
next	1.172;
commitid	tUlwqeYIhXjNv2dw;

1.172
date	2014.12.19.02.49.07;	author dlg;	state Exp;
branches;
next	1.171;
commitid	NZZastVxg4mJJUIK;

1.171
date	2014.12.19.02.46.47;	author dlg;	state Exp;
branches;
next	1.170;
commitid	SVYMzwzWUALDqZwD;

1.170
date	2014.12.19.02.15.25;	author dlg;	state Exp;
branches;
next	1.169;
commitid	8bRoeApKJPUxFCoQ;

1.169
date	2014.12.04.03.12.05;	author dlg;	state Exp;
branches;
next	1.168;
commitid	kPQFHsAryLcVBE8V;

1.168
date	2014.11.18.02.37.31;	author tedu;	state Exp;
branches;
next	1.167;
commitid	Z1vcFtHO8wRH0yRt;

1.167
date	2014.11.15.06.55.32;	author dlg;	state Exp;
branches;
next	1.166;
commitid	jEqCTs0j6wss01JN;

1.166
date	2014.11.14.02.02.42;	author tedu;	state Exp;
branches;
next	1.165;
commitid	04YP3YrhzVxHHrPq;

1.165
date	2014.11.10.18.55.43;	author kettenis;	state Exp;
branches;
next	1.164;
commitid	req2XJv75N5Emfm0;

1.164
date	2014.11.01.23.58.07;	author tedu;	state Exp;
branches;
next	1.163;
commitid	po5B2vOrAoI2l87Z;

1.163
date	2014.10.13.00.12.51;	author dlg;	state Exp;
branches;
next	1.162;
commitid	aO8zl1umHcAVA82j;

1.162
date	2014.10.10.00.48.58;	author dlg;	state Exp;
branches;
next	1.161;
commitid	WVEzWKxZ1yNIwbl2;

1.161
date	2014.09.28.10.03.05;	author tedu;	state Exp;
branches;
next	1.160;
commitid	eGNgyJ3QOazk17em;

1.160
date	2014.09.26.05.43.14;	author dlg;	state Exp;
branches;
next	1.159;
commitid	0rtc9EbI7skic3ec;

1.159
date	2014.09.23.19.54.47;	author miod;	state Exp;
branches;
next	1.158;
commitid	D6a2aECozizjhOsY;

1.158
date	2014.09.22.01.04.58;	author dlg;	state Exp;
branches;
next	1.157;
commitid	hZsc7X90BQCBOwmw;

1.157
date	2014.09.17.03.16.00;	author dlg;	state Exp;
branches;
next	1.156;
commitid	OLyc7xQ3S3zPTlnO;

1.156
date	2014.09.16.23.05.34;	author dlg;	state Exp;
branches;
next	1.155;
commitid	fWmX2Pu3Z8Tt6ymB;

1.155
date	2014.09.16.21.45.12;	author dlg;	state Exp;
branches;
next	1.154;
commitid	dv6ATnzSZ4zc5tZY;

1.154
date	2014.09.16.03.26.08;	author dlg;	state Exp;
branches;
next	1.153;
commitid	WsnvgTKcfl8dBlWe;

1.153
date	2014.09.14.14.17.25;	author jsg;	state Exp;
branches;
next	1.152;
commitid	uzzBR7hz9ncd4O6G;

1.152
date	2014.09.08.23.50.45;	author dlg;	state Exp;
branches;
next	1.151;
commitid	skrdXTRxYFS8Wjcl;

1.151
date	2014.09.08.00.00.05;	author dlg;	state Exp;
branches;
next	1.150;
commitid	yDOHf63nOnS1TEoY;

1.150
date	2014.09.05.03.13.52;	author dlg;	state Exp;
branches;
next	1.149;
commitid	e2QR6DTb0rQpfFSj;

1.149
date	2014.09.04.00.36.00;	author dlg;	state Exp;
branches;
next	1.148;
commitid	YMCPXWjFrLw20rLn;

1.148
date	2014.08.27.00.22.26;	author dlg;	state Exp;
branches;
next	1.147;
commitid	qJ6y3Y7RSE51lOmV;

1.147
date	2014.08.20.00.00.46;	author dlg;	state Exp;
branches;
next	1.146;
commitid	lyf0V1r2Q8b6DdLa;

1.146
date	2014.08.18.01.28.44;	author dlg;	state Exp;
branches;
next	1.145;
commitid	BKcyLhDPdzTugsLB;

1.145
date	2014.08.12.01.31.43;	author dlg;	state Exp;
branches;
next	1.144;
commitid	UVKxlGLpV9aWs4aF;

1.144
date	2014.08.12.01.25.21;	author dlg;	state Exp;
branches;
next	1.143;
commitid	2nZL42hCDLfxvRRX;

1.143
date	2014.08.12.01.05.46;	author dlg;	state Exp;
branches;
next	1.142;
commitid	HC2KE6Ccdh82XFiN;

1.142
date	2014.08.12.01.01.11;	author dlg;	state Exp;
branches;
next	1.141;
commitid	SjJLflkof2hTc9d3;

1.141
date	2014.08.12.00.59.27;	author dlg;	state Exp;
branches;
next	1.140;
commitid	BZApIxhO3a7XeQDC;

1.140
date	2014.08.11.13.31.42;	author dlg;	state Exp;
branches;
next	1.139;
commitid	s5QhZwxxR9DC6i8p;

1.139
date	2014.08.11.12.37.36;	author dlg;	state Exp;
branches;
next	1.138;
commitid	kofzGww9MPGmX7sJ;

1.138
date	2014.07.10.13.34.39;	author tedu;	state Exp;
branches;
next	1.137;
commitid	REMvIfHxrHOyXfjr;

1.137
date	2014.07.10.07.50.27;	author tedu;	state Exp;
branches;
next	1.136;
commitid	EldLGVtcRJJgIA98;

1.136
date	2014.07.03.07.47.56;	author guenther;	state Exp;
branches;
next	1.135;
commitid	saas2zCnDREMWz5M;

1.135
date	2014.07.02.07.15.31;	author dlg;	state Exp;
branches;
next	1.134;
commitid	CYZe02COe3DRrYL4;

1.134
date	2014.07.02.06.02.48;	author dlg;	state Exp;
branches;
next	1.133;
commitid	zwCM6yS0LmkMbTOr;

1.133
date	2014.07.02.06.01.25;	author dlg;	state Exp;
branches;
next	1.132;
commitid	WgZ7uJBSFK0iDMYE;

1.132
date	2014.07.02.05.52.45;	author dlg;	state Exp;
branches;
next	1.131;
commitid	PDUX5u9V0L3X1he9;

1.131
date	2014.07.02.05.49.59;	author dlg;	state Exp;
branches;
next	1.130;
commitid	ppAM9U5Qx8pMpG7T;

1.130
date	2014.07.02.05.42.40;	author dlg;	state Exp;
branches;
next	1.129;
commitid	D14t6rqKv7kCoku6;

1.129
date	2014.07.02.00.12.34;	author dlg;	state Exp;
branches;
next	1.128;
commitid	zNbAzxmEBZMksTSx;

1.128
date	2014.05.19.14.30.03;	author tedu;	state Exp;
branches;
next	1.127;

1.127
date	2014.05.01.04.25.02;	author tedu;	state Exp;
branches;
next	1.126;

1.126
date	2014.04.03.21.36.59;	author tedu;	state Exp;
branches;
next	1.125;

1.125
date	2014.03.28.17.57.11;	author mpi;	state Exp;
branches;
next	1.124;

1.124
date	2013.11.05.03.28.45;	author dlg;	state Exp;
branches;
next	1.123;

1.123
date	2013.08.08.23.25.06;	author syl;	state Exp;
branches;
next	1.122;

1.122
date	2013.06.05.00.44.06;	author tedu;	state Exp;
branches;
next	1.121;

1.121
date	2013.05.31.20.44.10;	author tedu;	state Exp;
branches;
next	1.120;

1.120
date	2013.05.03.18.26.07;	author tedu;	state Exp;
branches;
next	1.119;

1.119
date	2013.04.17.17.44.03;	author tedu;	state Exp;
branches;
next	1.118;

1.118
date	2013.04.06.13.41.11;	author deraadt;	state Exp;
branches;
next	1.117;

1.117
date	2013.04.06.03.53.25;	author tedu;	state Exp;
branches;
next	1.116;

1.116
date	2013.03.31.00.03.26;	author tedu;	state Exp;
branches;
next	1.115;

1.115
date	2013.03.26.16.37.45;	author tedu;	state Exp;
branches;
next	1.114;

1.114
date	2013.02.17.17.39.29;	author miod;	state Exp;
branches;
next	1.113;

1.113
date	2013.02.09.20.56.35;	author miod;	state Exp;
branches;
next	1.112;

1.112
date	2012.12.24.19.43.11;	author guenther;	state Exp;
branches;
next	1.111;

1.111
date	2011.11.23.02.05.17;	author dlg;	state Exp;
branches;
next	1.110;

1.110
date	2011.09.23.10.08.31;	author dlg;	state Exp;
branches;
next	1.109;

1.109
date	2011.09.23.07.27.09;	author dlg;	state Exp;
branches;
next	1.108;

1.108
date	2011.07.06.06.00.20;	author tedu;	state Exp;
branches;
next	1.107;

1.107
date	2011.07.06.02.56.53;	author tedu;	state Exp;
branches;
next	1.106;

1.106
date	2011.07.05.20.00.18;	author tedu;	state Exp;
branches;
next	1.105;

1.105
date	2011.07.05.16.36.15;	author tedu;	state Exp;
branches;
next	1.104;

1.104
date	2011.04.18.19.23.46;	author art;	state Exp;
branches;
next	1.103;

1.103
date	2011.04.06.15.52.13;	author art;	state Exp;
branches;
next	1.102;

1.102
date	2011.04.05.01.28.05;	author art;	state Exp;
branches;
next	1.101;

1.101
date	2011.04.04.11.13.55;	author deraadt;	state Exp;
branches;
next	1.100;

1.100
date	2011.04.03.22.07.37;	author ariane;	state Exp;
branches;
next	1.99;

1.99
date	2010.11.03.17.49.42;	author mikeb;	state Exp;
branches;
next	1.98;

1.98
date	2010.09.26.21.03.57;	author tedu;	state Exp;
branches;
next	1.97;

1.97
date	2010.09.21.01.09.10;	author matthew;	state Exp;
branches;
next	1.96;

1.96
date	2010.07.03.03.04.55;	author tedu;	state Exp;
branches;
next	1.95;

1.95
date	2010.07.02.01.25.05;	author art;	state Exp;
branches;
next	1.94;

1.94
date	2010.06.29.20.39.27;	author thib;	state Exp;
branches;
next	1.93;

1.93
date	2010.06.27.03.03.48;	author thib;	state Exp;
branches;
next	1.92;

1.92
date	2010.06.17.16.11.20;	author miod;	state Exp;
branches;
next	1.91;

1.91
date	2010.01.16.03.08.00;	author tedu;	state Exp;
branches;
next	1.90;

1.90
date	2009.09.05.16.06.57;	author thib;	state Exp;
branches;
next	1.89;

1.89
date	2009.08.26.00.37.17;	author thib;	state Exp;
branches;
next	1.88;

1.88
date	2009.08.13.13.49.20;	author thib;	state Exp;
branches;
next	1.87;

1.87
date	2009.08.09.13.41.03;	author thib;	state Exp;
branches;
next	1.86;

1.86
date	2009.07.30.18.19.26;	author deraadt;	state Exp;
branches;
next	1.85;

1.85
date	2009.06.24.11.23.33;	author deraadt;	state Exp;
branches;
next	1.84;

1.84
date	2009.06.12.14.56.21;	author oga;	state Exp;
branches;
next	1.83;

1.83
date	2009.06.04.18.48.54;	author deraadt;	state Exp;
branches;
next	1.82;

1.82
date	2009.06.04.17.42.23;	author deraadt;	state Exp;
branches;
next	1.81;

1.81
date	2009.06.04.09.58.40;	author oga;	state Exp;
branches;
next	1.80;

1.80
date	2009.05.31.17.11.14;	author miod;	state Exp;
branches;
next	1.79;

1.79
date	2009.04.22.01.16.11;	author dlg;	state Exp;
branches;
next	1.78;

1.78
date	2009.02.17.07.53.55;	author deraadt;	state Exp;
branches;
next	1.77;

1.77
date	2009.02.16.23.48.17;	author deraadt;	state Exp;
branches;
next	1.76;

1.76
date	2009.02.16.22.11.41;	author deraadt;	state Exp;
branches;
next	1.75;

1.75
date	2008.12.23.08.15.06;	author dlg;	state Exp;
branches;
next	1.74;

1.74
date	2008.12.23.06.53.12;	author dlg;	state Exp;
branches;
next	1.73;

1.73
date	2008.12.23.06.50.48;	author dlg;	state Exp;
branches;
next	1.72;

1.72
date	2008.12.04.12.40.35;	author art;	state Exp;
branches;
next	1.71;

1.71
date	2008.11.25.17.58.01;	author art;	state Exp;
branches;
next	1.70;

1.70
date	2008.11.25.13.05.51;	author art;	state Exp;
branches;
next	1.69;

1.69
date	2008.11.24.21.36.07;	author art;	state Exp;
branches;
next	1.68;

1.68
date	2008.11.24.17.42.34;	author art;	state Exp;
branches;
next	1.67;

1.67
date	2008.11.22.17.31.52;	author deraadt;	state Exp;
branches;
next	1.66;

1.66
date	2008.10.31.17.17.06;	author deraadt;	state Exp;
branches;
next	1.65;

1.65
date	2008.10.31.17.15.30;	author deraadt;	state Exp;
branches;
next	1.64;

1.64
date	2008.10.24.00.08.43;	author tedu;	state Exp;
branches;
next	1.63;

1.63
date	2008.10.23.23.54.02;	author tedu;	state Exp;
branches;
next	1.62;

1.62
date	2008.06.26.05.42.20;	author ray;	state Exp;
branches;
next	1.61;

1.61
date	2008.06.14.03.56.41;	author art;	state Exp;
branches;
next	1.60;

1.60
date	2008.05.16.17.21.36;	author thib;	state Exp;
branches;
next	1.59;

1.59
date	2008.05.06.20.57.19;	author thib;	state Exp;
branches;
next	1.58;

1.58
date	2007.12.11.15.04.58;	author tedu;	state Exp;
branches;
next	1.57;

1.57
date	2007.12.11.15.04.01;	author tedu;	state Exp;
branches;
next	1.56;

1.56
date	2007.12.09.00.24.04;	author tedu;	state Exp;
branches;
next	1.55;

1.55
date	2007.08.16.15.18.54;	author art;	state Exp;
branches;
next	1.54;

1.54
date	2007.05.28.23.46.28;	author tedu;	state Exp;
branches;
next	1.53;

1.53
date	2007.05.28.19.18.45;	author tedu;	state Exp;
branches;
next	1.52;

1.52
date	2007.05.28.17.55.56;	author tedu;	state Exp;
branches;
next	1.51;

1.51
date	2007.04.23.09.27.59;	author art;	state Exp;
branches;
next	1.50;

1.50
date	2007.04.12.21.47.45;	author miod;	state Exp;
branches;
next	1.49;

1.49
date	2007.04.11.12.10.42;	author art;	state Exp;
branches;
next	1.48;

1.48
date	2006.11.17.11.50.09;	author jmc;	state Exp;
branches;
next	1.47;

1.47
date	2006.05.20.18.29.23;	author mickey;	state Exp;
branches;
next	1.46;

1.46
date	2006.05.07.20.06.50;	author tedu;	state Exp;
branches;
next	1.45;

1.45
date	2004.07.29.09.18.17;	author mickey;	state Exp;
branches;
next	1.44;

1.44
date	2004.07.20.23.47.08;	author art;	state Exp;
branches;
next	1.43;

1.43
date	2004.06.24.19.35.24;	author tholo;	state Exp;
branches;
next	1.42;

1.42
date	2004.06.13.21.49.26;	author niklas;	state Exp;
branches;
next	1.41;

1.41
date	2004.06.02.22.17.22;	author tedu;	state Exp;
branches;
next	1.40;

1.40
date	2004.05.27.04.55.27;	author tedu;	state Exp;
branches;
next	1.39;

1.39
date	2003.11.18.06.08.18;	author tedu;	state Exp;
branches;
next	1.38;

1.38
date	2002.12.20.07.48.00;	author art;	state Exp;
branches;
next	1.37;

1.37
date	2002.12.11.06.20.31;	author art;	state Exp;
branches;
next	1.36;

1.36
date	2002.10.27.21.31.56;	author art;	state Exp;
branches;
next	1.35;

1.35
date	2002.10.14.20.09.41;	author art;	state Exp;
branches;
next	1.34;

1.34
date	2002.10.13.18.26.12;	author krw;	state Exp;
branches;
next	1.33;

1.33
date	2002.10.12.01.09.45;	author krw;	state Exp;
branches;
next	1.32;

1.32
date	2002.07.23.15.31.36;	author art;	state Exp;
branches
	1.32.2.1;
next	1.31;

1.31
date	2002.07.23.15.26.48;	author art;	state Exp;
branches;
next	1.30;

1.30
date	2002.06.09.00.15.33;	author niklas;	state Exp;
branches;
next	1.29;

1.29
date	2002.03.14.00.07.57;	author art;	state Exp;
branches;
next	1.28;

1.28
date	2002.02.25.04.53.16;	author dhartmei;	state Exp;
branches;
next	1.27;

1.27
date	2002.02.23.02.52.56;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2002.02.23.00.05.14;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2002.02.23.00.03.14;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2002.01.29.00.14.23;	author miod;	state Exp;
branches;
next	1.23;

1.23
date	2002.01.28.03.23.52;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2002.01.25.15.50.22;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2002.01.23.01.44.20;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2002.01.23.00.39.47;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2002.01.10.18.56.03;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2002.01.10.14.31.17;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2002.01.10.14.21.34;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2002.01.10.14.19.30;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2002.01.10.14.16.53;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2001.11.06.19.53.20;	author miod;	state Exp;
branches
	1.14.2.1;
next	1.13;

1.13
date	2001.09.19.20.50.58;	author mickey;	state Exp;
branches;
next	1.12;

1.12
date	2001.08.07.21.02.22;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2001.07.15.11.03.05;	author assar;	state Exp;
branches;
next	1.10;

1.10
date	2001.06.27.04.49.46;	author art;	state Exp;
branches;
next	1.9;

1.9
date	2001.06.24.17.06.14;	author miod;	state Exp;
branches;
next	1.8;

1.8
date	2001.06.24.16.00.47;	author art;	state Exp;
branches;
next	1.7;

1.7
date	2001.06.23.17.15.46;	author art;	state Exp;
branches;
next	1.6;

1.6
date	2001.06.23.16.13.01;	author art;	state Exp;
branches;
next	1.5;

1.5
date	2001.03.21.23.24.51;	author art;	state Exp;
branches;
next	1.4;

1.4
date	2001.02.19.11.34.12;	author art;	state Exp;
branches;
next	1.3;

1.3
date	2000.12.05.16.43.41;	author art;	state Exp;
branches;
next	1.2;

1.2
date	99.11.11.16.42.10;	author art;	state Exp;
branches
	1.2.2.1;
next	1.1;

1.1
date	99.02.26.03.13.30;	author art;	state Exp;
branches;
next	;

1.2.2.1
date	2001.05.14.22.32.43;	author niklas;	state Exp;
branches;
next	1.2.2.2;

1.2.2.2
date	2001.07.04.10.48.31;	author niklas;	state Exp;
branches;
next	1.2.2.3;

1.2.2.3
date	2001.10.31.03.26.29;	author nate;	state Exp;
branches;
next	1.2.2.4;

1.2.2.4
date	2001.11.13.23.04.23;	author niklas;	state Exp;
branches;
next	1.2.2.5;

1.2.2.5
date	2002.03.06.02.13.23;	author niklas;	state Exp;
branches;
next	1.2.2.6;

1.2.2.6
date	2002.03.28.11.43.04;	author niklas;	state Exp;
branches;
next	1.2.2.7;

1.2.2.7
date	2003.03.28.00.41.27;	author niklas;	state Exp;
branches;
next	1.2.2.8;

1.2.2.8
date	2003.05.15.04.08.02;	author niklas;	state Exp;
branches;
next	1.2.2.9;

1.2.2.9
date	2004.02.19.10.56.37;	author niklas;	state Exp;
branches;
next	1.2.2.10;

1.2.2.10
date	2004.06.05.23.13.01;	author niklas;	state Exp;
branches;
next	;

1.14.2.1
date	2002.01.31.22.55.41;	author niklas;	state Exp;
branches;
next	1.14.2.2;

1.14.2.2
date	2002.06.11.03.29.40;	author art;	state Exp;
branches;
next	1.14.2.3;

1.14.2.3
date	2002.10.29.00.36.44;	author art;	state Exp;
branches;
next	1.14.2.4;

1.14.2.4
date	2003.05.19.22.31.57;	author tedu;	state Exp;
branches;
next	;

1.32.2.1
date	2002.11.04.15.30.36;	author jason;	state Exp;
branches;
next	;


desc
@@


1.220
log
@New flag PR_RWLOCK for pool_init(9) makes the pool use rwlocks instead
of mutexes.  Use this immediately for the pool_cache futex pools.

Mostly worked out with dlg@@ during e2k17
ok mpi@@ tedu@@
@
text
@/*	$OpenBSD: subr_pool.c,v 1.219 2017/07/12 08:51:42 visa Exp $	*/
/*	$NetBSD: subr_pool.c,v 1.61 2001/09/26 07:14:56 chs Exp $	*/

/*-
 * Copyright (c) 1997, 1999, 2000 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Paul Kranenburg; by Jason R. Thorpe of the Numerical Aerospace
 * Simulation Facility, NASA Ames Research Center.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/errno.h>
#include <sys/kernel.h>
#include <sys/malloc.h>
#include <sys/pool.h>
#include <sys/syslog.h>
#include <sys/sysctl.h>
#include <sys/task.h>
#include <sys/timeout.h>
#include <sys/percpu.h>

#include <uvm/uvm_extern.h>

/*
 * Pool resource management utility.
 *
 * Memory is allocated in pages which are split into pieces according to
 * the pool item size. Each page is kept on one of three lists in the
 * pool structure: `pr_emptypages', `pr_fullpages' and `pr_partpages',
 * for empty, full and partially-full pages respectively. The individual
 * pool items are on a linked list headed by `ph_items' in each page
 * header. The memory for building the page list is either taken from
 * the allocated pages themselves (for small pool items) or taken from
 * an internal pool of page headers (`phpool').
 */

/* List of all pools */
SIMPLEQ_HEAD(,pool) pool_head = SIMPLEQ_HEAD_INITIALIZER(pool_head);

/*
 * Every pool gets a unique serial number assigned to it. If this counter
 * wraps, we're screwed, but we shouldn't create so many pools anyway.
 */
unsigned int pool_serial;
unsigned int pool_count;

/* Lock the previous variables making up the global pool state */
struct rwlock pool_lock = RWLOCK_INITIALIZER("pools");

/* Private pool for page header structures */
struct pool phpool;

struct pool_lock_ops {
	void	(*pl_init)(struct pool *, union pool_lock *,
		    struct lock_type *);
	void	(*pl_enter)(union pool_lock * LOCK_FL_VARS);
	int	(*pl_enter_try)(union pool_lock * LOCK_FL_VARS);
	void	(*pl_leave)(union pool_lock * LOCK_FL_VARS);
	void	(*pl_assert_locked)(union pool_lock *);
	void	(*pl_assert_unlocked)(union pool_lock *);
	int	(*pl_sleep)(void *, union pool_lock *, int, const char *, int);
};

static const struct pool_lock_ops pool_lock_ops_mtx;
static const struct pool_lock_ops pool_lock_ops_rw;

#ifdef WITNESS
#define pl_init(pp, pl) do {						\
	static struct lock_type __lock_type = { .lt_name = #pl };	\
	(pp)->pr_lock_ops->pl_init(pp, pl, &__lock_type);		\
} while (0)
#else /* WITNESS */
#define pl_init(pp, pl)		(pp)->pr_lock_ops->pl_init(pp, pl, NULL)
#endif /* WITNESS */

static inline void
pl_enter(struct pool *pp, union pool_lock *pl LOCK_FL_VARS)
{
	pp->pr_lock_ops->pl_enter(pl LOCK_FL_ARGS);
}
static inline int
pl_enter_try(struct pool *pp, union pool_lock *pl LOCK_FL_VARS)
{
	return pp->pr_lock_ops->pl_enter_try(pl LOCK_FL_ARGS);
}
static inline void
pl_leave(struct pool *pp, union pool_lock *pl LOCK_FL_VARS)
{
	pp->pr_lock_ops->pl_leave(pl LOCK_FL_ARGS);
}
static inline void
pl_assert_locked(struct pool *pp, union pool_lock *pl)
{
	pp->pr_lock_ops->pl_assert_locked(pl);
}
static inline void
pl_assert_unlocked(struct pool *pp, union pool_lock *pl)
{
	pp->pr_lock_ops->pl_assert_unlocked(pl);
}
static inline int
pl_sleep(struct pool *pp, void *ident, union pool_lock *lock, int priority,
    const char *wmesg, int timo)
{
	return pp->pr_lock_ops->pl_sleep(ident, lock, priority, wmesg, timo);
}

#ifdef WITNESS
# define pl_enter(pp,pl)	pl_enter(pp,pl LOCK_FILE_LINE)
# define pl_enter_try(pp,pl)	pl_enter_try(pp,pl LOCK_FILE_LINE)
# define pl_leave(pp,pl)	pl_leave(pp,pl LOCK_FILE_LINE)
#endif

struct pool_item {
	u_long				pi_magic;
	XSIMPLEQ_ENTRY(pool_item)	pi_list;
};
#define POOL_IMAGIC(ph, pi) ((u_long)(pi) ^ (ph)->ph_magic)

struct pool_page_header {
	/* Page headers */
	TAILQ_ENTRY(pool_page_header)
				ph_entry;	/* pool page list */
	XSIMPLEQ_HEAD(, pool_item)
				ph_items;	/* free items on the page */
	RBT_ENTRY(pool_page_header)
				ph_node;	/* off-page page headers */
	unsigned int		ph_nmissing;	/* # of chunks in use */
	caddr_t			ph_page;	/* this page's address */
	caddr_t			ph_colored;	/* page's colored address */
	unsigned long		ph_magic;
	int			ph_tick;
};
#define POOL_MAGICBIT (1 << 3) /* keep away from perturbed low bits */
#define POOL_PHPOISON(ph) ISSET((ph)->ph_magic, POOL_MAGICBIT)

#ifdef MULTIPROCESSOR
struct pool_cache_item {
	struct pool_cache_item	*ci_next;	/* next item in list */
	unsigned long		 ci_nitems;	/* number of items in list */
	TAILQ_ENTRY(pool_cache_item)
				 ci_nextl;	/* entry in list of lists */
};

/* we store whether the cached item is poisoned in the high bit of nitems */
#define POOL_CACHE_ITEM_NITEMS_MASK	0x7ffffffUL
#define POOL_CACHE_ITEM_NITEMS_POISON	0x8000000UL

#define POOL_CACHE_ITEM_NITEMS(_ci)					\
    ((_ci)->ci_nitems & POOL_CACHE_ITEM_NITEMS_MASK)

#define POOL_CACHE_ITEM_POISONED(_ci)					\
    ISSET((_ci)->ci_nitems, POOL_CACHE_ITEM_NITEMS_POISON)

struct pool_cache {
	struct pool_cache_item	*pc_actv;	/* active list of items */
	unsigned long		 pc_nactv;	/* actv head nitems cache */
	struct pool_cache_item	*pc_prev;	/* previous list of items */

	uint64_t		 pc_gen;	/* generation number */
	uint64_t		 pc_nget;	/* # of successful requests */
	uint64_t		 pc_nfail;	/* # of unsuccessful reqs */
	uint64_t		 pc_nput;	/* # of releases */
	uint64_t		 pc_nlget;	/* # of list requests */
	uint64_t		 pc_nlfail;	/* # of fails getting a list */
	uint64_t		 pc_nlput;	/* # of list releases */

	int			 pc_nout;
};

void	*pool_cache_get(struct pool *);
void	 pool_cache_put(struct pool *, void *);
void	 pool_cache_destroy(struct pool *);
void	 pool_cache_gc(struct pool *);
#endif
void	 pool_cache_pool_info(struct pool *, struct kinfo_pool *);
int	 pool_cache_info(struct pool *, void *, size_t *);
int	 pool_cache_cpus_info(struct pool *, void *, size_t *);

#ifdef POOL_DEBUG
int	pool_debug = 1;
#else
int	pool_debug = 0;
#endif

#define POOL_INPGHDR(pp) ((pp)->pr_phoffset != 0)

struct pool_page_header *
	 pool_p_alloc(struct pool *, int, int *);
void	 pool_p_insert(struct pool *, struct pool_page_header *);
void	 pool_p_remove(struct pool *, struct pool_page_header *);
void	 pool_p_free(struct pool *, struct pool_page_header *);

void	 pool_update_curpage(struct pool *);
void	*pool_do_get(struct pool *, int, int *);
void	 pool_do_put(struct pool *, void *);
int	 pool_chk_page(struct pool *, struct pool_page_header *, int);
int	 pool_chk(struct pool *);
void	 pool_get_done(struct pool *, void *, void *);
void	 pool_runqueue(struct pool *, int);

void	*pool_allocator_alloc(struct pool *, int, int *);
void	 pool_allocator_free(struct pool *, void *);

/*
 * The default pool allocator.
 */
void	*pool_page_alloc(struct pool *, int, int *);
void	pool_page_free(struct pool *, void *);

/*
 * safe for interrupts; this is the default allocator
 */
struct pool_allocator pool_allocator_single = {
	pool_page_alloc,
	pool_page_free,
	POOL_ALLOC_SIZE(PAGE_SIZE, POOL_ALLOC_ALIGNED)
};

void	*pool_multi_alloc(struct pool *, int, int *);
void	pool_multi_free(struct pool *, void *);

struct pool_allocator pool_allocator_multi = {
	pool_multi_alloc,
	pool_multi_free,
	POOL_ALLOC_SIZES(PAGE_SIZE, (1UL << 31), POOL_ALLOC_ALIGNED)
};

void	*pool_multi_alloc_ni(struct pool *, int, int *);
void	pool_multi_free_ni(struct pool *, void *);

struct pool_allocator pool_allocator_multi_ni = {
	pool_multi_alloc_ni,
	pool_multi_free_ni,
	POOL_ALLOC_SIZES(PAGE_SIZE, (1UL << 31), POOL_ALLOC_ALIGNED)
};

#ifdef DDB
void	 pool_print_pagelist(struct pool_pagelist *, int (*)(const char *, ...)
	     __attribute__((__format__(__kprintf__,1,2))));
void	 pool_print1(struct pool *, const char *, int (*)(const char *, ...)
	     __attribute__((__format__(__kprintf__,1,2))));
#endif

/* stale page garbage collectors */
void	pool_gc_sched(void *);
struct timeout pool_gc_tick = TIMEOUT_INITIALIZER(pool_gc_sched, NULL);
void	pool_gc_pages(void *);
struct task pool_gc_task = TASK_INITIALIZER(pool_gc_pages, NULL);
int pool_wait_free = 1;
int pool_wait_gc = 8;

RBT_PROTOTYPE(phtree, pool_page_header, ph_node, phtree_compare);

static inline int
phtree_compare(const struct pool_page_header *a,
    const struct pool_page_header *b)
{
	vaddr_t va = (vaddr_t)a->ph_page;
	vaddr_t vb = (vaddr_t)b->ph_page;

	/* the compares in this order are important for the NFIND to work */
	if (vb < va)
		return (-1);
	if (vb > va)
		return (1);

	return (0);
}

RBT_GENERATE(phtree, pool_page_header, ph_node, phtree_compare);

/*
 * Return the pool page header based on page address.
 */
static inline struct pool_page_header *
pr_find_pagehead(struct pool *pp, void *v)
{
	struct pool_page_header *ph, key;

	if (POOL_INPGHDR(pp)) {
		caddr_t page;

		page = (caddr_t)((vaddr_t)v & pp->pr_pgmask);

		return ((struct pool_page_header *)(page + pp->pr_phoffset));
	}

	key.ph_page = v;
	ph = RBT_NFIND(phtree, &pp->pr_phtree, &key);
	if (ph == NULL)
		panic("%s: %s: page header missing", __func__, pp->pr_wchan);

	KASSERT(ph->ph_page <= (caddr_t)v);
	if (ph->ph_page + pp->pr_pgsize <= (caddr_t)v)
		panic("%s: %s: incorrect page", __func__, pp->pr_wchan);

	return (ph);
}

/*
 * Initialize the given pool resource structure.
 *
 * We export this routine to allow other kernel parts to declare
 * static pools that must be initialized before malloc() is available.
 */
void
pool_init(struct pool *pp, size_t size, u_int align, int ipl, int flags,
    const char *wchan, struct pool_allocator *palloc)
{
	int off = 0, space;
	unsigned int pgsize = PAGE_SIZE, items;
	size_t pa_pagesz;
#ifdef DIAGNOSTIC
	struct pool *iter;
#endif

	if (align == 0)
		align = ALIGN(1);

	if (size < sizeof(struct pool_item))
		size = sizeof(struct pool_item);

	size = roundup(size, align);

	while (size * 8 > pgsize)
		pgsize <<= 1;

	if (palloc == NULL) {
		if (pgsize > PAGE_SIZE) {
			palloc = ISSET(flags, PR_WAITOK) ?
			    &pool_allocator_multi_ni : &pool_allocator_multi;
		} else
			palloc = &pool_allocator_single;

		pa_pagesz = palloc->pa_pagesz;
	} else {
		size_t pgsizes;

		pa_pagesz = palloc->pa_pagesz;
		if (pa_pagesz == 0)
			pa_pagesz = POOL_ALLOC_DEFAULT;

		pgsizes = pa_pagesz & ~POOL_ALLOC_ALIGNED;

		/* make sure the allocator can fit at least one item */
		if (size > pgsizes) {
			panic("%s: pool %s item size 0x%zx > "
			    "allocator %p sizes 0x%zx", __func__, wchan,
			    size, palloc, pgsizes);
		}

		/* shrink pgsize until it fits into the range */
		while (!ISSET(pgsizes, pgsize))
			pgsize >>= 1;
	}
	KASSERT(ISSET(pa_pagesz, pgsize));

	items = pgsize / size;

	/*
	 * Decide whether to put the page header off page to avoid
	 * wasting too large a part of the page. Off-page page headers
	 * go into an RB tree, so we can match a returned item with
	 * its header based on the page address.
	 */
	if (ISSET(pa_pagesz, POOL_ALLOC_ALIGNED)) {
		if (pgsize - (size * items) >
		    sizeof(struct pool_page_header)) {
			off = pgsize - sizeof(struct pool_page_header);
		} else if (sizeof(struct pool_page_header) * 2 >= size) {
			off = pgsize - sizeof(struct pool_page_header);
			items = off / size;
		}
	}

	KASSERT(items > 0);

	/*
	 * Initialize the pool structure.
	 */
	memset(pp, 0, sizeof(*pp));
	if (ISSET(flags, PR_RWLOCK)) {
		KASSERT(flags & PR_WAITOK);
		pp->pr_lock_ops = &pool_lock_ops_rw;
	} else
		pp->pr_lock_ops = &pool_lock_ops_mtx;
	TAILQ_INIT(&pp->pr_emptypages);
	TAILQ_INIT(&pp->pr_fullpages);
	TAILQ_INIT(&pp->pr_partpages);
	pp->pr_curpage = NULL;
	pp->pr_npages = 0;
	pp->pr_minitems = 0;
	pp->pr_minpages = 0;
	pp->pr_maxpages = 8;
	pp->pr_size = size;
	pp->pr_pgsize = pgsize;
	pp->pr_pgmask = ~0UL ^ (pgsize - 1);
	pp->pr_phoffset = off;
	pp->pr_itemsperpage = items;
	pp->pr_wchan = wchan;
	pp->pr_alloc = palloc;
	pp->pr_nitems = 0;
	pp->pr_nout = 0;
	pp->pr_hardlimit = UINT_MAX;
	pp->pr_hardlimit_warning = NULL;
	pp->pr_hardlimit_ratecap.tv_sec = 0;
	pp->pr_hardlimit_ratecap.tv_usec = 0;
	pp->pr_hardlimit_warning_last.tv_sec = 0;
	pp->pr_hardlimit_warning_last.tv_usec = 0;
	RBT_INIT(phtree, &pp->pr_phtree);

	/*
	 * Use the space between the chunks and the page header
	 * for cache coloring.
	 */
	space = POOL_INPGHDR(pp) ? pp->pr_phoffset : pp->pr_pgsize;
	space -= pp->pr_itemsperpage * pp->pr_size;
	pp->pr_align = align;
	pp->pr_maxcolors = (space / align) + 1;

	pp->pr_nget = 0;
	pp->pr_nfail = 0;
	pp->pr_nput = 0;
	pp->pr_npagealloc = 0;
	pp->pr_npagefree = 0;
	pp->pr_hiwat = 0;
	pp->pr_nidle = 0;

	pp->pr_ipl = ipl;
	pp->pr_flags = flags;

	pl_init(pp, &pp->pr_lock);
	pl_init(pp, &pp->pr_requests_lock);
	TAILQ_INIT(&pp->pr_requests);

	if (phpool.pr_size == 0) {
		pool_init(&phpool, sizeof(struct pool_page_header), 0,
		    IPL_HIGH, 0, "phpool", NULL);

		/* make sure phpool wont "recurse" */
		KASSERT(POOL_INPGHDR(&phpool));
	}

	/* pglistalloc/constraint parameters */
	pp->pr_crange = &kp_dirty;

	/* Insert this into the list of all pools. */
	rw_enter_write(&pool_lock);
#ifdef DIAGNOSTIC
	SIMPLEQ_FOREACH(iter, &pool_head, pr_poollist) {
		if (iter == pp)
			panic("%s: pool %s already on list", __func__, wchan);
	}
#endif

	pp->pr_serial = ++pool_serial;
	if (pool_serial == 0)
		panic("%s: too much uptime", __func__);

	SIMPLEQ_INSERT_HEAD(&pool_head, pp, pr_poollist);
	pool_count++;
	rw_exit_write(&pool_lock);
}

/*
 * Decommission a pool resource.
 */
void
pool_destroy(struct pool *pp)
{
	struct pool_page_header *ph;
	struct pool *prev, *iter;

#ifdef MULTIPROCESSOR
	if (pp->pr_cache != NULL)
		pool_cache_destroy(pp);
#endif

#ifdef DIAGNOSTIC
	if (pp->pr_nout != 0)
		panic("%s: pool busy: still out: %u", __func__, pp->pr_nout);
#endif

	/* Remove from global pool list */
	rw_enter_write(&pool_lock);
	pool_count--;
	if (pp == SIMPLEQ_FIRST(&pool_head))
		SIMPLEQ_REMOVE_HEAD(&pool_head, pr_poollist);
	else {
		prev = SIMPLEQ_FIRST(&pool_head);
		SIMPLEQ_FOREACH(iter, &pool_head, pr_poollist) {
			if (iter == pp) {
				SIMPLEQ_REMOVE_AFTER(&pool_head, prev,
				    pr_poollist);
				break;
			}
			prev = iter;
		}
	}
	rw_exit_write(&pool_lock);

	/* Remove all pages */
	while ((ph = TAILQ_FIRST(&pp->pr_emptypages)) != NULL) {
		pl_enter(pp, &pp->pr_lock);
		pool_p_remove(pp, ph);
		pl_leave(pp, &pp->pr_lock);
		pool_p_free(pp, ph);
	}
	KASSERT(TAILQ_EMPTY(&pp->pr_fullpages));
	KASSERT(TAILQ_EMPTY(&pp->pr_partpages));
}

void
pool_request_init(struct pool_request *pr,
    void (*handler)(struct pool *, void *, void *), void *cookie)
{
	pr->pr_handler = handler;
	pr->pr_cookie = cookie;
	pr->pr_item = NULL;
}

void
pool_request(struct pool *pp, struct pool_request *pr)
{
	pl_enter(pp, &pp->pr_requests_lock);
	TAILQ_INSERT_TAIL(&pp->pr_requests, pr, pr_entry);
	pool_runqueue(pp, PR_NOWAIT);
	pl_leave(pp, &pp->pr_requests_lock);
}

struct pool_get_memory {
	union pool_lock lock;
	void * volatile v;
};

/*
 * Grab an item from the pool.
 */
void *
pool_get(struct pool *pp, int flags)
{
	void *v = NULL;
	int slowdown = 0;

	KASSERT(flags & (PR_WAITOK | PR_NOWAIT));
	if (pp->pr_flags & PR_RWLOCK)
		KASSERT(flags & PR_WAITOK);

#ifdef MULTIPROCESSOR
	if (pp->pr_cache != NULL) {
		v = pool_cache_get(pp);
		if (v != NULL)
			goto good;
	}
#endif

	pl_enter(pp, &pp->pr_lock);
	if (pp->pr_nout >= pp->pr_hardlimit) {
		if (ISSET(flags, PR_NOWAIT|PR_LIMITFAIL))
			goto fail;
	} else if ((v = pool_do_get(pp, flags, &slowdown)) == NULL) {
		if (ISSET(flags, PR_NOWAIT))
			goto fail;
	}
	pl_leave(pp, &pp->pr_lock);

	if ((slowdown || pool_debug == 2) && ISSET(flags, PR_WAITOK))
		yield();

	if (v == NULL) {
		struct pool_get_memory mem = { .v = NULL };
		struct pool_request pr;

		pl_init(pp, &mem.lock);
		pool_request_init(&pr, pool_get_done, &mem);
		pool_request(pp, &pr);

		pl_enter(pp, &mem.lock);
		while (mem.v == NULL)
			pl_sleep(pp, &mem, &mem.lock, PSWP, pp->pr_wchan, 0);
		pl_leave(pp, &mem.lock);

		v = mem.v;
	}

#ifdef MULTIPROCESSOR
good:
#endif
	if (ISSET(flags, PR_ZERO))
		memset(v, 0, pp->pr_size);

	return (v);

fail:
	pp->pr_nfail++;
	pl_leave(pp, &pp->pr_lock);
	return (NULL);
}

void
pool_get_done(struct pool *pp, void *xmem, void *v)
{
	struct pool_get_memory *mem = xmem;

	pl_enter(pp, &mem->lock);
	mem->v = v;
	pl_leave(pp, &mem->lock);

	wakeup_one(mem);
}

void
pool_runqueue(struct pool *pp, int flags)
{
	struct pool_requests prl = TAILQ_HEAD_INITIALIZER(prl);
	struct pool_request *pr;

	pl_assert_unlocked(pp, &pp->pr_lock);
	pl_assert_locked(pp, &pp->pr_requests_lock);

	if (pp->pr_requesting++)
		return;

	do {
		pp->pr_requesting = 1;

		/* no TAILQ_JOIN? :( */
		while ((pr = TAILQ_FIRST(&pp->pr_requests)) != NULL) {
			TAILQ_REMOVE(&pp->pr_requests, pr, pr_entry);
			TAILQ_INSERT_TAIL(&prl, pr, pr_entry);
		}
		if (TAILQ_EMPTY(&prl))
			continue;

		pl_leave(pp, &pp->pr_requests_lock);

		pl_enter(pp, &pp->pr_lock);
		pr = TAILQ_FIRST(&prl);
		while (pr != NULL) {
			int slowdown = 0;

			if (pp->pr_nout >= pp->pr_hardlimit)
				break;

			pr->pr_item = pool_do_get(pp, flags, &slowdown);
			if (pr->pr_item == NULL) /* || slowdown ? */
				break;

			pr = TAILQ_NEXT(pr, pr_entry);
		}
		pl_leave(pp, &pp->pr_lock);

		while ((pr = TAILQ_FIRST(&prl)) != NULL &&
		    pr->pr_item != NULL) {
			TAILQ_REMOVE(&prl, pr, pr_entry);
			(*pr->pr_handler)(pp, pr->pr_cookie, pr->pr_item);
		}

		pl_enter(pp, &pp->pr_requests_lock);
	} while (--pp->pr_requesting);

	/* no TAILQ_JOIN :( */
	while ((pr = TAILQ_FIRST(&prl)) != NULL) {
		TAILQ_REMOVE(&prl, pr, pr_entry);
		TAILQ_INSERT_TAIL(&pp->pr_requests, pr, pr_entry);
	}
}

void *
pool_do_get(struct pool *pp, int flags, int *slowdown)
{
	struct pool_item *pi;
	struct pool_page_header *ph;

	pl_assert_locked(pp, &pp->pr_lock);

	splassert(pp->pr_ipl);

	/*
	 * Account for this item now to avoid races if we need to give up
	 * pr_lock to allocate a page.
	 */
	pp->pr_nout++;

	if (pp->pr_curpage == NULL) {
		pl_leave(pp, &pp->pr_lock);
		ph = pool_p_alloc(pp, flags, slowdown);
		pl_enter(pp, &pp->pr_lock);

		if (ph == NULL) {
			pp->pr_nout--;
			return (NULL);
		}

		pool_p_insert(pp, ph);
	}

	ph = pp->pr_curpage;
	pi = XSIMPLEQ_FIRST(&ph->ph_items);
	if (__predict_false(pi == NULL))
		panic("%s: %s: page empty", __func__, pp->pr_wchan);

	if (__predict_false(pi->pi_magic != POOL_IMAGIC(ph, pi))) {
		panic("%s: %s free list modified: "
		    "page %p; item addr %p; offset 0x%x=0x%lx != 0x%lx",
		    __func__, pp->pr_wchan, ph->ph_page, pi,
		    0, pi->pi_magic, POOL_IMAGIC(ph, pi));
	}

	XSIMPLEQ_REMOVE_HEAD(&ph->ph_items, pi_list);

#ifdef DIAGNOSTIC
	if (pool_debug && POOL_PHPOISON(ph)) {
		size_t pidx;
		uint32_t pval;
		if (poison_check(pi + 1, pp->pr_size - sizeof(*pi),
		    &pidx, &pval)) {
			int *ip = (int *)(pi + 1);
			panic("%s: %s free list modified: "
			    "page %p; item addr %p; offset 0x%zx=0x%x",
			    __func__, pp->pr_wchan, ph->ph_page, pi,
			    (pidx * sizeof(int)) + sizeof(*pi), ip[pidx]);
		}
	}
#endif /* DIAGNOSTIC */

	if (ph->ph_nmissing++ == 0) {
		/*
		 * This page was previously empty.  Move it to the list of
		 * partially-full pages.  This page is already curpage.
		 */
		TAILQ_REMOVE(&pp->pr_emptypages, ph, ph_entry);
		TAILQ_INSERT_TAIL(&pp->pr_partpages, ph, ph_entry);

		pp->pr_nidle--;
	}

	if (ph->ph_nmissing == pp->pr_itemsperpage) {
		/*
		 * This page is now full.  Move it to the full list
		 * and select a new current page.
		 */
		TAILQ_REMOVE(&pp->pr_partpages, ph, ph_entry);
		TAILQ_INSERT_TAIL(&pp->pr_fullpages, ph, ph_entry);
		pool_update_curpage(pp);
	}

	pp->pr_nget++;

	return (pi);
}

/*
 * Return resource to the pool.
 */
void
pool_put(struct pool *pp, void *v)
{
	struct pool_page_header *ph, *freeph = NULL;

#ifdef DIAGNOSTIC
	if (v == NULL)
		panic("%s: NULL item", __func__);
#endif

#ifdef MULTIPROCESSOR
	if (pp->pr_cache != NULL && TAILQ_EMPTY(&pp->pr_requests)) {
		pool_cache_put(pp, v);
		return;
	}
#endif

	pl_enter(pp, &pp->pr_lock);

	pool_do_put(pp, v);

	pp->pr_nout--;
	pp->pr_nput++;

	/* is it time to free a page? */
	if (pp->pr_nidle > pp->pr_maxpages &&
	    (ph = TAILQ_FIRST(&pp->pr_emptypages)) != NULL &&
	    (ticks - ph->ph_tick) > (hz * pool_wait_free)) {
		freeph = ph;
		pool_p_remove(pp, freeph);
	}

	pl_leave(pp, &pp->pr_lock);

	if (freeph != NULL)
		pool_p_free(pp, freeph);

	if (!TAILQ_EMPTY(&pp->pr_requests)) {
		pl_enter(pp, &pp->pr_requests_lock);
		pool_runqueue(pp, PR_NOWAIT);
		pl_leave(pp, &pp->pr_requests_lock);
	}
}

void
pool_do_put(struct pool *pp, void *v)
{
	struct pool_item *pi = v;
	struct pool_page_header *ph;

	splassert(pp->pr_ipl);

	ph = pr_find_pagehead(pp, v);

#ifdef DIAGNOSTIC
	if (pool_debug) {
		struct pool_item *qi;
		XSIMPLEQ_FOREACH(qi, &ph->ph_items, pi_list) {
			if (pi == qi) {
				panic("%s: %s: double pool_put: %p", __func__,
				    pp->pr_wchan, pi);
			}
		}
	}
#endif /* DIAGNOSTIC */

	pi->pi_magic = POOL_IMAGIC(ph, pi);
	XSIMPLEQ_INSERT_HEAD(&ph->ph_items, pi, pi_list);
#ifdef DIAGNOSTIC
	if (POOL_PHPOISON(ph))
		poison_mem(pi + 1, pp->pr_size - sizeof(*pi));
#endif /* DIAGNOSTIC */

	if (ph->ph_nmissing-- == pp->pr_itemsperpage) {
		/*
		 * The page was previously completely full, move it to the
		 * partially-full list.
		 */
		TAILQ_REMOVE(&pp->pr_fullpages, ph, ph_entry);
		TAILQ_INSERT_TAIL(&pp->pr_partpages, ph, ph_entry);
	}

	if (ph->ph_nmissing == 0) {
		/*
		 * The page is now empty, so move it to the empty page list.
		 */
		pp->pr_nidle++;

		ph->ph_tick = ticks;
		TAILQ_REMOVE(&pp->pr_partpages, ph, ph_entry);
		TAILQ_INSERT_TAIL(&pp->pr_emptypages, ph, ph_entry);
		pool_update_curpage(pp);
	}
}

/*
 * Add N items to the pool.
 */
int
pool_prime(struct pool *pp, int n)
{
	struct pool_pagelist pl = TAILQ_HEAD_INITIALIZER(pl);
	struct pool_page_header *ph;
	int newpages;

	newpages = roundup(n, pp->pr_itemsperpage) / pp->pr_itemsperpage;

	while (newpages-- > 0) {
		int slowdown = 0;

		ph = pool_p_alloc(pp, PR_NOWAIT, &slowdown);
		if (ph == NULL) /* or slowdown? */
			break;

		TAILQ_INSERT_TAIL(&pl, ph, ph_entry);
	}

	pl_enter(pp, &pp->pr_lock);
	while ((ph = TAILQ_FIRST(&pl)) != NULL) {
		TAILQ_REMOVE(&pl, ph, ph_entry);
		pool_p_insert(pp, ph);
	}
	pl_leave(pp, &pp->pr_lock);

	return (0);
}

struct pool_page_header *
pool_p_alloc(struct pool *pp, int flags, int *slowdown)
{
	struct pool_page_header *ph;
	struct pool_item *pi;
	caddr_t addr;
	int n;

	pl_assert_unlocked(pp, &pp->pr_lock);
	KASSERT(pp->pr_size >= sizeof(*pi));

	addr = pool_allocator_alloc(pp, flags, slowdown);
	if (addr == NULL)
		return (NULL);

	if (POOL_INPGHDR(pp))
		ph = (struct pool_page_header *)(addr + pp->pr_phoffset);
	else {
		ph = pool_get(&phpool, flags);
		if (ph == NULL) {
			pool_allocator_free(pp, addr);
			return (NULL);
		}
	}

	XSIMPLEQ_INIT(&ph->ph_items);
	ph->ph_page = addr;
	addr += pp->pr_align * (pp->pr_npagealloc % pp->pr_maxcolors);
	ph->ph_colored = addr;
	ph->ph_nmissing = 0;
	arc4random_buf(&ph->ph_magic, sizeof(ph->ph_magic));
#ifdef DIAGNOSTIC
	/* use a bit in ph_magic to record if we poison page items */
	if (pool_debug)
		SET(ph->ph_magic, POOL_MAGICBIT);
	else
		CLR(ph->ph_magic, POOL_MAGICBIT);
#endif /* DIAGNOSTIC */

	n = pp->pr_itemsperpage;
	while (n--) {
		pi = (struct pool_item *)addr;
		pi->pi_magic = POOL_IMAGIC(ph, pi);
		XSIMPLEQ_INSERT_TAIL(&ph->ph_items, pi, pi_list);

#ifdef DIAGNOSTIC
		if (POOL_PHPOISON(ph))
			poison_mem(pi + 1, pp->pr_size - sizeof(*pi));
#endif /* DIAGNOSTIC */

		addr += pp->pr_size;
	}

	return (ph);
}

void
pool_p_free(struct pool *pp, struct pool_page_header *ph)
{
	struct pool_item *pi;

	pl_assert_unlocked(pp, &pp->pr_lock);
	KASSERT(ph->ph_nmissing == 0);

	XSIMPLEQ_FOREACH(pi, &ph->ph_items, pi_list) {
		if (__predict_false(pi->pi_magic != POOL_IMAGIC(ph, pi))) {
			panic("%s: %s free list modified: "
			    "page %p; item addr %p; offset 0x%x=0x%lx",
			    __func__, pp->pr_wchan, ph->ph_page, pi,
			    0, pi->pi_magic);
		}

#ifdef DIAGNOSTIC
		if (POOL_PHPOISON(ph)) {
			size_t pidx;
			uint32_t pval;
			if (poison_check(pi + 1, pp->pr_size - sizeof(*pi),
			    &pidx, &pval)) {
				int *ip = (int *)(pi + 1);
				panic("%s: %s free list modified: "
				    "page %p; item addr %p; offset 0x%zx=0x%x",
				    __func__, pp->pr_wchan, ph->ph_page, pi,
				    pidx * sizeof(int), ip[pidx]);
			}
		}
#endif
	}

	pool_allocator_free(pp, ph->ph_page);

	if (!POOL_INPGHDR(pp))
		pool_put(&phpool, ph);
}

void
pool_p_insert(struct pool *pp, struct pool_page_header *ph)
{
	pl_assert_locked(pp, &pp->pr_lock);

	/* If the pool was depleted, point at the new page */
	if (pp->pr_curpage == NULL)
		pp->pr_curpage = ph;

	TAILQ_INSERT_TAIL(&pp->pr_emptypages, ph, ph_entry);
	if (!POOL_INPGHDR(pp))
		RBT_INSERT(phtree, &pp->pr_phtree, ph);

	pp->pr_nitems += pp->pr_itemsperpage;
	pp->pr_nidle++;

	pp->pr_npagealloc++;
	if (++pp->pr_npages > pp->pr_hiwat)
		pp->pr_hiwat = pp->pr_npages;
}

void
pool_p_remove(struct pool *pp, struct pool_page_header *ph)
{
	pl_assert_locked(pp, &pp->pr_lock);

	pp->pr_npagefree++;
	pp->pr_npages--;
	pp->pr_nidle--;
	pp->pr_nitems -= pp->pr_itemsperpage;

	if (!POOL_INPGHDR(pp))
		RBT_REMOVE(phtree, &pp->pr_phtree, ph);
	TAILQ_REMOVE(&pp->pr_emptypages, ph, ph_entry);

	pool_update_curpage(pp);
}

void
pool_update_curpage(struct pool *pp)
{
	pp->pr_curpage = TAILQ_LAST(&pp->pr_partpages, pool_pagelist);
	if (pp->pr_curpage == NULL) {
		pp->pr_curpage = TAILQ_LAST(&pp->pr_emptypages, pool_pagelist);
	}
}

void
pool_setlowat(struct pool *pp, int n)
{
	int prime = 0;

	pl_enter(pp, &pp->pr_lock);
	pp->pr_minitems = n;
	pp->pr_minpages = (n == 0)
		? 0
		: roundup(n, pp->pr_itemsperpage) / pp->pr_itemsperpage;

	if (pp->pr_nitems < n)
		prime = n - pp->pr_nitems;
	pl_leave(pp, &pp->pr_lock);

	if (prime > 0)
		pool_prime(pp, prime);
}

void
pool_sethiwat(struct pool *pp, int n)
{
	pp->pr_maxpages = (n == 0)
		? 0
		: roundup(n, pp->pr_itemsperpage) / pp->pr_itemsperpage;
}

int
pool_sethardlimit(struct pool *pp, u_int n, const char *warnmsg, int ratecap)
{
	int error = 0;

	if (n < pp->pr_nout) {
		error = EINVAL;
		goto done;
	}

	pp->pr_hardlimit = n;
	pp->pr_hardlimit_warning = warnmsg;
	pp->pr_hardlimit_ratecap.tv_sec = ratecap;
	pp->pr_hardlimit_warning_last.tv_sec = 0;
	pp->pr_hardlimit_warning_last.tv_usec = 0;

done:
	return (error);
}

void
pool_set_constraints(struct pool *pp, const struct kmem_pa_mode *mode)
{
	pp->pr_crange = mode;
}

/*
 * Release all complete pages that have not been used recently.
 *
 * Returns non-zero if any pages have been reclaimed.
 */
int
pool_reclaim(struct pool *pp)
{
	struct pool_page_header *ph, *phnext;
	struct pool_pagelist pl = TAILQ_HEAD_INITIALIZER(pl);

	pl_enter(pp, &pp->pr_lock);
	for (ph = TAILQ_FIRST(&pp->pr_emptypages); ph != NULL; ph = phnext) {
		phnext = TAILQ_NEXT(ph, ph_entry);

		/* Check our minimum page claim */
		if (pp->pr_npages <= pp->pr_minpages)
			break;

		/*
		 * If freeing this page would put us below
		 * the low water mark, stop now.
		 */
		if ((pp->pr_nitems - pp->pr_itemsperpage) <
		    pp->pr_minitems)
			break;

		pool_p_remove(pp, ph);
		TAILQ_INSERT_TAIL(&pl, ph, ph_entry);
	}
	pl_leave(pp, &pp->pr_lock);

	if (TAILQ_EMPTY(&pl))
		return (0);

	while ((ph = TAILQ_FIRST(&pl)) != NULL) {
		TAILQ_REMOVE(&pl, ph, ph_entry);
		pool_p_free(pp, ph);
	}

	return (1);
}

/*
 * Release all complete pages that have not been used recently
 * from all pools.
 */
void
pool_reclaim_all(void)
{
	struct pool	*pp;

	rw_enter_read(&pool_lock);
	SIMPLEQ_FOREACH(pp, &pool_head, pr_poollist)
		pool_reclaim(pp);
	rw_exit_read(&pool_lock);
}

#ifdef DDB
#include <machine/db_machdep.h>
#include <ddb/db_output.h>

/*
 * Diagnostic helpers.
 */
void
pool_printit(struct pool *pp, const char *modif,
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))))
{
	pool_print1(pp, modif, pr);
}

void
pool_print_pagelist(struct pool_pagelist *pl,
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))))
{
	struct pool_page_header *ph;
	struct pool_item *pi;

	TAILQ_FOREACH(ph, pl, ph_entry) {
		(*pr)("\t\tpage %p, color %p, nmissing %d\n",
		    ph->ph_page, ph->ph_colored, ph->ph_nmissing);
		XSIMPLEQ_FOREACH(pi, &ph->ph_items, pi_list) {
			if (pi->pi_magic != POOL_IMAGIC(ph, pi)) {
				(*pr)("\t\t\titem %p, magic 0x%lx\n",
				    pi, pi->pi_magic);
			}
		}
	}
}

void
pool_print1(struct pool *pp, const char *modif,
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))))
{
	struct pool_page_header *ph;
	int print_pagelist = 0;
	char c;

	while ((c = *modif++) != '\0') {
		if (c == 'p')
			print_pagelist = 1;
		modif++;
	}

	(*pr)("POOL %s: size %u maxcolors %u\n", pp->pr_wchan, pp->pr_size,
	    pp->pr_maxcolors);
	(*pr)("\talloc %p\n", pp->pr_alloc);
	(*pr)("\tminitems %u, minpages %u, maxpages %u, npages %u\n",
	    pp->pr_minitems, pp->pr_minpages, pp->pr_maxpages, pp->pr_npages);
	(*pr)("\titemsperpage %u, nitems %u, nout %u, hardlimit %u\n",
	    pp->pr_itemsperpage, pp->pr_nitems, pp->pr_nout, pp->pr_hardlimit);

	(*pr)("\n\tnget %lu, nfail %lu, nput %lu\n",
	    pp->pr_nget, pp->pr_nfail, pp->pr_nput);
	(*pr)("\tnpagealloc %lu, npagefree %lu, hiwat %u, nidle %lu\n",
	    pp->pr_npagealloc, pp->pr_npagefree, pp->pr_hiwat, pp->pr_nidle);

	if (print_pagelist == 0)
		return;

	if ((ph = TAILQ_FIRST(&pp->pr_emptypages)) != NULL)
		(*pr)("\n\tempty page list:\n");
	pool_print_pagelist(&pp->pr_emptypages, pr);
	if ((ph = TAILQ_FIRST(&pp->pr_fullpages)) != NULL)
		(*pr)("\n\tfull page list:\n");
	pool_print_pagelist(&pp->pr_fullpages, pr);
	if ((ph = TAILQ_FIRST(&pp->pr_partpages)) != NULL)
		(*pr)("\n\tpartial-page list:\n");
	pool_print_pagelist(&pp->pr_partpages, pr);

	if (pp->pr_curpage == NULL)
		(*pr)("\tno current page\n");
	else
		(*pr)("\tcurpage %p\n", pp->pr_curpage->ph_page);
}

void
db_show_all_pools(db_expr_t expr, int haddr, db_expr_t count, char *modif)
{
	struct pool *pp;
	char maxp[16];
	int ovflw;
	char mode;

	mode = modif[0];
	if (mode != '\0' && mode != 'a') {
		db_printf("usage: show all pools [/a]\n");
		return;
	}

	if (mode == '\0')
		db_printf("%-10s%4s%9s%5s%9s%6s%6s%6s%6s%6s%6s%5s\n",
		    "Name",
		    "Size",
		    "Requests",
		    "Fail",
		    "Releases",
		    "Pgreq",
		    "Pgrel",
		    "Npage",
		    "Hiwat",
		    "Minpg",
		    "Maxpg",
		    "Idle");
	else
		db_printf("%-12s %18s %18s\n",
		    "Name", "Address", "Allocator");

	SIMPLEQ_FOREACH(pp, &pool_head, pr_poollist) {
		if (mode == 'a') {
			db_printf("%-12s %18p %18p\n", pp->pr_wchan, pp,
			    pp->pr_alloc);
			continue;
		}

		if (!pp->pr_nget)
			continue;

		if (pp->pr_maxpages == UINT_MAX)
			snprintf(maxp, sizeof maxp, "inf");
		else
			snprintf(maxp, sizeof maxp, "%u", pp->pr_maxpages);

#define PRWORD(ovflw, fmt, width, fixed, val) do {	\
	(ovflw) += db_printf((fmt),			\
	    (width) - (fixed) - (ovflw) > 0 ?		\
	    (width) - (fixed) - (ovflw) : 0,		\
	    (val)) - (width);				\
	if ((ovflw) < 0)				\
		(ovflw) = 0;				\
} while (/* CONSTCOND */0)

		ovflw = 0;
		PRWORD(ovflw, "%-*s", 10, 0, pp->pr_wchan);
		PRWORD(ovflw, " %*u", 4, 1, pp->pr_size);
		PRWORD(ovflw, " %*lu", 9, 1, pp->pr_nget);
		PRWORD(ovflw, " %*lu", 5, 1, pp->pr_nfail);
		PRWORD(ovflw, " %*lu", 9, 1, pp->pr_nput);
		PRWORD(ovflw, " %*lu", 6, 1, pp->pr_npagealloc);
		PRWORD(ovflw, " %*lu", 6, 1, pp->pr_npagefree);
		PRWORD(ovflw, " %*d", 6, 1, pp->pr_npages);
		PRWORD(ovflw, " %*d", 6, 1, pp->pr_hiwat);
		PRWORD(ovflw, " %*d", 6, 1, pp->pr_minpages);
		PRWORD(ovflw, " %*s", 6, 1, maxp);
		PRWORD(ovflw, " %*lu\n", 5, 1, pp->pr_nidle);

		pool_chk(pp);
	}
}
#endif /* DDB */

#if defined(POOL_DEBUG) || defined(DDB)
int
pool_chk_page(struct pool *pp, struct pool_page_header *ph, int expected)
{
	struct pool_item *pi;
	caddr_t page;
	int n;
	const char *label = pp->pr_wchan;

	page = (caddr_t)((u_long)ph & pp->pr_pgmask);
	if (page != ph->ph_page && POOL_INPGHDR(pp)) {
		printf("%s: ", label);
		printf("pool(%p:%s): page inconsistency: page %p; "
		    "at page head addr %p (p %p)\n",
		    pp, pp->pr_wchan, ph->ph_page, ph, page);
		return 1;
	}

	for (pi = XSIMPLEQ_FIRST(&ph->ph_items), n = 0;
	     pi != NULL;
	     pi = XSIMPLEQ_NEXT(&ph->ph_items, pi, pi_list), n++) {
		if ((caddr_t)pi < ph->ph_page ||
		    (caddr_t)pi >= ph->ph_page + pp->pr_pgsize) {
			printf("%s: ", label);
			printf("pool(%p:%s): page inconsistency: page %p;"
			    " item ordinal %d; addr %p\n", pp,
			    pp->pr_wchan, ph->ph_page, n, pi);
			return (1);
		}

		if (pi->pi_magic != POOL_IMAGIC(ph, pi)) {
			printf("%s: ", label);
			printf("pool(%p:%s): free list modified: "
			    "page %p; item ordinal %d; addr %p "
			    "(p %p); offset 0x%x=0x%lx\n",
			    pp, pp->pr_wchan, ph->ph_page, n, pi, page,
			    0, pi->pi_magic);
		}

#ifdef DIAGNOSTIC
		if (POOL_PHPOISON(ph)) {
			size_t pidx;
			uint32_t pval;
			if (poison_check(pi + 1, pp->pr_size - sizeof(*pi),
			    &pidx, &pval)) {
				int *ip = (int *)(pi + 1);
				printf("pool(%s): free list modified: "
				    "page %p; item ordinal %d; addr %p "
				    "(p %p); offset 0x%zx=0x%x\n",
				    pp->pr_wchan, ph->ph_page, n, pi,
				    page, pidx * sizeof(int), ip[pidx]);
			}
		}
#endif /* DIAGNOSTIC */
	}
	if (n + ph->ph_nmissing != pp->pr_itemsperpage) {
		printf("pool(%p:%s): page inconsistency: page %p;"
		    " %d on list, %d missing, %d items per page\n", pp,
		    pp->pr_wchan, ph->ph_page, n, ph->ph_nmissing,
		    pp->pr_itemsperpage);
		return 1;
	}
	if (expected >= 0 && n != expected) {
		printf("pool(%p:%s): page inconsistency: page %p;"
		    " %d on list, %d missing, %d expected\n", pp,
		    pp->pr_wchan, ph->ph_page, n, ph->ph_nmissing,
		    expected);
		return 1;
	}
	return 0;
}

int
pool_chk(struct pool *pp)
{
	struct pool_page_header *ph;
	int r = 0;

	TAILQ_FOREACH(ph, &pp->pr_emptypages, ph_entry)
		r += pool_chk_page(pp, ph, pp->pr_itemsperpage);
	TAILQ_FOREACH(ph, &pp->pr_fullpages, ph_entry)
		r += pool_chk_page(pp, ph, 0);
	TAILQ_FOREACH(ph, &pp->pr_partpages, ph_entry)
		r += pool_chk_page(pp, ph, -1);

	return (r);
}
#endif /* defined(POOL_DEBUG) || defined(DDB) */

#ifdef DDB
void
pool_walk(struct pool *pp, int full,
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))),
    void (*func)(void *, int, int (*)(const char *, ...)
	    __attribute__((__format__(__kprintf__,1,2)))))
{
	struct pool_page_header *ph;
	struct pool_item *pi;
	caddr_t cp;
	int n;

	TAILQ_FOREACH(ph, &pp->pr_fullpages, ph_entry) {
		cp = ph->ph_colored;
		n = ph->ph_nmissing;

		while (n--) {
			func(cp, full, pr);
			cp += pp->pr_size;
		}
	}

	TAILQ_FOREACH(ph, &pp->pr_partpages, ph_entry) {
		cp = ph->ph_colored;
		n = ph->ph_nmissing;

		do {
			XSIMPLEQ_FOREACH(pi, &ph->ph_items, pi_list) {
				if (cp == (caddr_t)pi)
					break;
			}
			if (cp != (caddr_t)pi) {
				func(cp, full, pr);
				n--;
			}

			cp += pp->pr_size;
		} while (n > 0);
	}
}
#endif

/*
 * We have three different sysctls.
 * kern.pool.npools - the number of pools.
 * kern.pool.pool.<pool#> - the pool struct for the pool#.
 * kern.pool.name.<pool#> - the name for pool#.
 */
int
sysctl_dopool(int *name, u_int namelen, char *oldp, size_t *oldlenp)
{
	struct kinfo_pool pi;
	struct pool *pp;
	int rv = ENOENT;

	switch (name[0]) {
	case KERN_POOL_NPOOLS:
		if (namelen != 1)
			return (ENOTDIR);
		return (sysctl_rdint(oldp, oldlenp, NULL, pool_count));

	case KERN_POOL_NAME:
	case KERN_POOL_POOL:
	case KERN_POOL_CACHE:
	case KERN_POOL_CACHE_CPUS:
		break;
	default:
		return (EOPNOTSUPP);
	}

	if (namelen != 2)
		return (ENOTDIR);

	rw_enter_read(&pool_lock);

	SIMPLEQ_FOREACH(pp, &pool_head, pr_poollist) {
		if (name[1] == pp->pr_serial)
			break;
	}

	if (pp == NULL)
		goto done;

	switch (name[0]) {
	case KERN_POOL_NAME:
		rv = sysctl_rdstring(oldp, oldlenp, NULL, pp->pr_wchan);
		break;
	case KERN_POOL_POOL:
		memset(&pi, 0, sizeof(pi));

		pl_enter(pp, &pp->pr_lock);
		pi.pr_size = pp->pr_size;
		pi.pr_pgsize = pp->pr_pgsize;
		pi.pr_itemsperpage = pp->pr_itemsperpage;
		pi.pr_npages = pp->pr_npages;
		pi.pr_minpages = pp->pr_minpages;
		pi.pr_maxpages = pp->pr_maxpages;
		pi.pr_hardlimit = pp->pr_hardlimit;
		pi.pr_nout = pp->pr_nout;
		pi.pr_nitems = pp->pr_nitems;
		pi.pr_nget = pp->pr_nget;
		pi.pr_nput = pp->pr_nput;
		pi.pr_nfail = pp->pr_nfail;
		pi.pr_npagealloc = pp->pr_npagealloc;
		pi.pr_npagefree = pp->pr_npagefree;
		pi.pr_hiwat = pp->pr_hiwat;
		pi.pr_nidle = pp->pr_nidle;
		pl_leave(pp, &pp->pr_lock);

		pool_cache_pool_info(pp, &pi);

		rv = sysctl_rdstruct(oldp, oldlenp, NULL, &pi, sizeof(pi));
		break;

	case KERN_POOL_CACHE:
		rv = pool_cache_info(pp, oldp, oldlenp);
		break;

	case KERN_POOL_CACHE_CPUS:
		rv = pool_cache_cpus_info(pp, oldp, oldlenp);
		break;
	}

done:
	rw_exit_read(&pool_lock);

	return (rv);
}

void
pool_gc_sched(void *null)
{
	task_add(systqmp, &pool_gc_task);
}

void
pool_gc_pages(void *null)
{
	struct pool *pp;
	struct pool_page_header *ph, *freeph;
	int s;

	rw_enter_read(&pool_lock);
	s = splvm(); /* XXX go to splvm until all pools _setipl properly */
	SIMPLEQ_FOREACH(pp, &pool_head, pr_poollist) {
#ifdef MULTIPROCESSOR
		if (pp->pr_cache != NULL)
			pool_cache_gc(pp);
#endif

		if (pp->pr_nidle <= pp->pr_minpages || /* guess */
		    !pl_enter_try(pp, &pp->pr_lock)) /* try */
			continue;

		/* is it time to free a page? */
		if (pp->pr_nidle > pp->pr_minpages &&
		    (ph = TAILQ_FIRST(&pp->pr_emptypages)) != NULL &&
		    (ticks - ph->ph_tick) > (hz * pool_wait_gc)) {
			freeph = ph;
			pool_p_remove(pp, freeph);
		} else
			freeph = NULL;

		pl_leave(pp, &pp->pr_lock);

		if (freeph != NULL)
			pool_p_free(pp, freeph);
	}
	splx(s);
	rw_exit_read(&pool_lock);

	timeout_add_sec(&pool_gc_tick, 1);
}

/*
 * Pool backend allocators.
 */

void *
pool_allocator_alloc(struct pool *pp, int flags, int *slowdown)
{
	void *v;

	v = (*pp->pr_alloc->pa_alloc)(pp, flags, slowdown);

#ifdef DIAGNOSTIC
	if (v != NULL && POOL_INPGHDR(pp)) {
		vaddr_t addr = (vaddr_t)v;
		if ((addr & pp->pr_pgmask) != addr) {
			panic("%s: %s page address %p isnt aligned to %u",
			    __func__, pp->pr_wchan, v, pp->pr_pgsize);
		}
	}
#endif

	return (v);
}

void
pool_allocator_free(struct pool *pp, void *v)
{
	struct pool_allocator *pa = pp->pr_alloc;

	(*pa->pa_free)(pp, v);
}

void *
pool_page_alloc(struct pool *pp, int flags, int *slowdown)
{
	struct kmem_dyn_mode kd = KMEM_DYN_INITIALIZER;

	kd.kd_waitok = ISSET(flags, PR_WAITOK);
	kd.kd_slowdown = slowdown;

	return (km_alloc(pp->pr_pgsize, &kv_page, pp->pr_crange, &kd));
}

void
pool_page_free(struct pool *pp, void *v)
{
	km_free(v, pp->pr_pgsize, &kv_page, pp->pr_crange);
}

void *
pool_multi_alloc(struct pool *pp, int flags, int *slowdown)
{
	struct kmem_va_mode kv = kv_intrsafe;
	struct kmem_dyn_mode kd = KMEM_DYN_INITIALIZER;
	void *v;
	int s;

	if (POOL_INPGHDR(pp))
		kv.kv_align = pp->pr_pgsize;

	kd.kd_waitok = ISSET(flags, PR_WAITOK);
	kd.kd_slowdown = slowdown;

	s = splvm();
	v = km_alloc(pp->pr_pgsize, &kv, pp->pr_crange, &kd);
	splx(s);

	return (v);
}

void
pool_multi_free(struct pool *pp, void *v)
{
	struct kmem_va_mode kv = kv_intrsafe;
	int s;

	if (POOL_INPGHDR(pp))
		kv.kv_align = pp->pr_pgsize;

	s = splvm();
	km_free(v, pp->pr_pgsize, &kv, pp->pr_crange);
	splx(s);
}

void *
pool_multi_alloc_ni(struct pool *pp, int flags, int *slowdown)
{
	struct kmem_va_mode kv = kv_any;
	struct kmem_dyn_mode kd = KMEM_DYN_INITIALIZER;
	void *v;

	if (POOL_INPGHDR(pp))
		kv.kv_align = pp->pr_pgsize;

	kd.kd_waitok = ISSET(flags, PR_WAITOK);
	kd.kd_slowdown = slowdown;

	KERNEL_LOCK();
	v = km_alloc(pp->pr_pgsize, &kv, pp->pr_crange, &kd);
	KERNEL_UNLOCK();

	return (v);
}

void
pool_multi_free_ni(struct pool *pp, void *v)
{
	struct kmem_va_mode kv = kv_any;

	if (POOL_INPGHDR(pp))
		kv.kv_align = pp->pr_pgsize;

	KERNEL_LOCK();
	km_free(v, pp->pr_pgsize, &kv, pp->pr_crange);
	KERNEL_UNLOCK();
}

#ifdef MULTIPROCESSOR

struct pool pool_caches; /* per cpu cache entries */

void
pool_cache_init(struct pool *pp)
{
	struct cpumem *cm;
	struct pool_cache *pc;
	struct cpumem_iter i;

	if (pool_caches.pr_size == 0) {
		pool_init(&pool_caches, sizeof(struct pool_cache),
		    CACHELINESIZE, IPL_NONE, PR_WAITOK | PR_RWLOCK,
		    "plcache", NULL);
	}

	/* must be able to use the pool items as cache list items */
	KASSERT(pp->pr_size >= sizeof(struct pool_cache_item));

	cm = cpumem_get(&pool_caches);

	pl_init(pp, &pp->pr_cache_lock);
	arc4random_buf(pp->pr_cache_magic, sizeof(pp->pr_cache_magic));
	TAILQ_INIT(&pp->pr_cache_lists);
	pp->pr_cache_nitems = 0;
	pp->pr_cache_tick = ticks;
	pp->pr_cache_items = 8;
	pp->pr_cache_contention = 0;
	pp->pr_cache_ngc = 0;

	CPUMEM_FOREACH(pc, &i, cm) {
		pc->pc_actv = NULL;
		pc->pc_nactv = 0;
		pc->pc_prev = NULL;

		pc->pc_nget = 0;
		pc->pc_nfail = 0;
		pc->pc_nput = 0;
		pc->pc_nlget = 0;
		pc->pc_nlfail = 0;
		pc->pc_nlput = 0;
		pc->pc_nout = 0;
	}

	membar_producer();

	pp->pr_cache = cm;
}

static inline void
pool_cache_item_magic(struct pool *pp, struct pool_cache_item *ci)
{
	unsigned long *entry = (unsigned long *)&ci->ci_nextl;

	entry[0] = pp->pr_cache_magic[0] ^ (u_long)ci;
	entry[1] = pp->pr_cache_magic[1] ^ (u_long)ci->ci_next;
}

static inline void
pool_cache_item_magic_check(struct pool *pp, struct pool_cache_item *ci)
{
	unsigned long *entry;
	unsigned long val;

	entry = (unsigned long *)&ci->ci_nextl;
	val = pp->pr_cache_magic[0] ^ (u_long)ci;
	if (*entry != val)
		goto fail;

	entry++;
	val = pp->pr_cache_magic[1] ^ (u_long)ci->ci_next;
	if (*entry != val)
		goto fail;

	return;

fail:
	panic("%s: %s cpu free list modified: item addr %p+%zu 0x%lx!=0x%lx",
	    __func__, pp->pr_wchan, ci, (caddr_t)entry - (caddr_t)ci,
	    *entry, val);
}

static inline void
pool_list_enter(struct pool *pp)
{
	if (pl_enter_try(pp, &pp->pr_cache_lock) == 0) {
		pl_enter(pp, &pp->pr_cache_lock);
		pp->pr_cache_contention++;
	}
}

static inline void
pool_list_leave(struct pool *pp)
{
	pl_leave(pp, &pp->pr_cache_lock);
}

static inline struct pool_cache_item *
pool_cache_list_alloc(struct pool *pp, struct pool_cache *pc)
{
	struct pool_cache_item *pl;

	pool_list_enter(pp);
	pl = TAILQ_FIRST(&pp->pr_cache_lists);
	if (pl != NULL) {
		TAILQ_REMOVE(&pp->pr_cache_lists, pl, ci_nextl);
		pp->pr_cache_nitems -= POOL_CACHE_ITEM_NITEMS(pl);

		pool_cache_item_magic(pp, pl);

		pc->pc_nlget++;
	} else
		pc->pc_nlfail++;

	/* fold this cpus nout into the global while we have the lock */
	pp->pr_cache_nout += pc->pc_nout;
	pc->pc_nout = 0;
	pool_list_leave(pp);

	return (pl);
}

static inline void
pool_cache_list_free(struct pool *pp, struct pool_cache *pc,
    struct pool_cache_item *ci)
{
	pool_list_enter(pp);
	if (TAILQ_EMPTY(&pp->pr_cache_lists))
		pp->pr_cache_tick = ticks;

	pp->pr_cache_nitems += POOL_CACHE_ITEM_NITEMS(ci);
	TAILQ_INSERT_TAIL(&pp->pr_cache_lists, ci, ci_nextl);

	pc->pc_nlput++;

	/* fold this cpus nout into the global while we have the lock */
	pp->pr_cache_nout += pc->pc_nout;
	pc->pc_nout = 0;
	pool_list_leave(pp);
}

static inline struct pool_cache *
pool_cache_enter(struct pool *pp, int *s)
{
	struct pool_cache *pc;

	pc = cpumem_enter(pp->pr_cache);
	*s = splraise(pp->pr_ipl);
	pc->pc_gen++;

	return (pc);
}

static inline void
pool_cache_leave(struct pool *pp, struct pool_cache *pc, int s)
{
	pc->pc_gen++;
	splx(s);
	cpumem_leave(pp->pr_cache, pc);
}

void *
pool_cache_get(struct pool *pp)
{
	struct pool_cache *pc;
	struct pool_cache_item *ci;
	int s;

	pc = pool_cache_enter(pp, &s);

	if (pc->pc_actv != NULL) {
		ci = pc->pc_actv;
	} else if (pc->pc_prev != NULL) {
		ci = pc->pc_prev;
		pc->pc_prev = NULL;
	} else if ((ci = pool_cache_list_alloc(pp, pc)) == NULL) {
		pc->pc_nfail++;
		goto done;
	}

	pool_cache_item_magic_check(pp, ci);
#ifdef DIAGNOSTIC
	if (pool_debug && POOL_CACHE_ITEM_POISONED(ci)) {
		size_t pidx;
		uint32_t pval;

		if (poison_check(ci + 1, pp->pr_size - sizeof(*ci),
		    &pidx, &pval)) {
			int *ip = (int *)(ci + 1);
			ip += pidx;

			panic("%s: %s cpu free list modified: "
			    "item addr %p+%zu 0x%x!=0x%x",
			    __func__, pp->pr_wchan, ci,
			    (caddr_t)ip - (caddr_t)ci, *ip, pval);
		}
	}
#endif

	pc->pc_actv = ci->ci_next;
	pc->pc_nactv = POOL_CACHE_ITEM_NITEMS(ci) - 1;
	pc->pc_nget++;
	pc->pc_nout++;

done:
	pool_cache_leave(pp, pc, s);

	return (ci);
}

void
pool_cache_put(struct pool *pp, void *v)
{
	struct pool_cache *pc;
	struct pool_cache_item *ci = v;
	unsigned long nitems;
	int s;
#ifdef DIAGNOSTIC
	int poison = pool_debug && pp->pr_size > sizeof(*ci);

	if (poison)
		poison_mem(ci + 1, pp->pr_size - sizeof(*ci));
#endif

	pc = pool_cache_enter(pp, &s);

	nitems = pc->pc_nactv;
	if (nitems >= pp->pr_cache_items) {
		if (pc->pc_prev != NULL)
			pool_cache_list_free(pp, pc, pc->pc_prev);

		pc->pc_prev = pc->pc_actv;

		pc->pc_actv = NULL;
		pc->pc_nactv = 0;
		nitems = 0;
	}

	ci->ci_next = pc->pc_actv;
	ci->ci_nitems = ++nitems;
#ifdef DIAGNOSTIC
	ci->ci_nitems |= poison ? POOL_CACHE_ITEM_NITEMS_POISON : 0;
#endif
	pool_cache_item_magic(pp, ci);

	pc->pc_actv = ci;
	pc->pc_nactv = nitems;

	pc->pc_nput++;
	pc->pc_nout--;

	pool_cache_leave(pp, pc, s);
}

struct pool_cache_item *
pool_cache_list_put(struct pool *pp, struct pool_cache_item *pl)
{
	struct pool_cache_item *rpl, *next;

	if (pl == NULL)
		return (NULL);

	rpl = TAILQ_NEXT(pl, ci_nextl);

	pl_enter(pp, &pp->pr_lock);
	do {
		next = pl->ci_next;
		pool_do_put(pp, pl);
		pl = next;
	} while (pl != NULL);
	pl_leave(pp, &pp->pr_lock);

	return (rpl);
}

void
pool_cache_destroy(struct pool *pp)
{
	struct pool_cache *pc;
	struct pool_cache_item *pl;
	struct cpumem_iter i;
	struct cpumem *cm;

	rw_enter_write(&pool_lock); /* serialise with the gc */
	cm = pp->pr_cache;
	pp->pr_cache = NULL; /* make pool_put avoid the cache */
	rw_exit_write(&pool_lock);

	CPUMEM_FOREACH(pc, &i, cm) {
		pool_cache_list_put(pp, pc->pc_actv);
		pool_cache_list_put(pp, pc->pc_prev);
	}

	cpumem_put(&pool_caches, cm);

	pl = TAILQ_FIRST(&pp->pr_cache_lists);
	while (pl != NULL)
		pl = pool_cache_list_put(pp, pl);
}

void
pool_cache_gc(struct pool *pp)
{
	unsigned int contention, delta;

	if ((ticks - pp->pr_cache_tick) > (hz * pool_wait_gc) &&
	    !TAILQ_EMPTY(&pp->pr_cache_lists) &&
	    pl_enter_try(pp, &pp->pr_cache_lock)) {
		struct pool_cache_item *pl = NULL;

		pl = TAILQ_FIRST(&pp->pr_cache_lists);
		if (pl != NULL) {
			TAILQ_REMOVE(&pp->pr_cache_lists, pl, ci_nextl);
			pp->pr_cache_nitems -= POOL_CACHE_ITEM_NITEMS(pl);
			pp->pr_cache_tick = ticks;

			pp->pr_cache_ngc++;
		}

		pl_leave(pp, &pp->pr_cache_lock);

		pool_cache_list_put(pp, pl);
	}

	/*
	 * if there's a lot of contention on the pr_cache_mtx then consider
	 * growing the length of the list to reduce the need to access the
	 * global pool.
	 */

	contention = pp->pr_cache_contention;
	delta = contention - pp->pr_cache_contention_prev;
	if (delta > 8 /* magic */) {
		if ((ncpusfound * 8 * 2) <= pp->pr_cache_nitems)
			pp->pr_cache_items += 8;
	} else if (delta == 0) {
		if (pp->pr_cache_items > 8)
			pp->pr_cache_items--;
	}
	pp->pr_cache_contention_prev = contention;
}

void
pool_cache_pool_info(struct pool *pp, struct kinfo_pool *pi)
{
	struct pool_cache *pc;
	struct cpumem_iter i;

	if (pp->pr_cache == NULL)
		return;

	/* loop through the caches twice to collect stats */

	/* once without the lock so we can yield while reading nget/nput */
	CPUMEM_FOREACH(pc, &i, pp->pr_cache) {
		uint64_t gen, nget, nput;

		do {
			while ((gen = pc->pc_gen) & 1)
				yield();

			nget = pc->pc_nget;
			nput = pc->pc_nput;
		} while (gen != pc->pc_gen);

		pi->pr_nget += nget;
		pi->pr_nput += nput;
	}

	/* and once with the mtx so we can get consistent nout values */
	pl_enter(pp, &pp->pr_cache_lock);
	CPUMEM_FOREACH(pc, &i, pp->pr_cache)
		pi->pr_nout += pc->pc_nout;

	pi->pr_nout += pp->pr_cache_nout;
	pl_leave(pp, &pp->pr_cache_lock);
}

int
pool_cache_info(struct pool *pp, void *oldp, size_t *oldlenp)
{
	struct kinfo_pool_cache kpc;

	if (pp->pr_cache == NULL)
		return (EOPNOTSUPP);

	memset(&kpc, 0, sizeof(kpc)); /* don't leak padding */

	pl_enter(pp, &pp->pr_cache_lock);
	kpc.pr_ngc = pp->pr_cache_ngc;
	kpc.pr_len = pp->pr_cache_items;
	kpc.pr_nitems = pp->pr_cache_nitems;
	kpc.pr_contention = pp->pr_cache_contention;
	pl_leave(pp, &pp->pr_cache_lock);

	return (sysctl_rdstruct(oldp, oldlenp, NULL, &kpc, sizeof(kpc)));
}

int
pool_cache_cpus_info(struct pool *pp, void *oldp, size_t *oldlenp)
{
	struct pool_cache *pc;
	struct kinfo_pool_cache_cpu *kpcc, *info;
	unsigned int cpu = 0;
	struct cpumem_iter i;
	int error = 0;
	size_t len;

	if (pp->pr_cache == NULL)
		return (EOPNOTSUPP);
	if (*oldlenp % sizeof(*kpcc))
		return (EINVAL);

	kpcc = mallocarray(ncpusfound, sizeof(*kpcc), M_TEMP,
	    M_WAITOK|M_CANFAIL|M_ZERO);
	if (kpcc == NULL)
		return (EIO);

	len = ncpusfound * sizeof(*kpcc);

	CPUMEM_FOREACH(pc, &i, pp->pr_cache) {
		uint64_t gen;

		if (cpu >= ncpusfound) {
			error = EIO;
			goto err;
		}

		info = &kpcc[cpu];
		info->pr_cpu = cpu;

		do {
			while ((gen = pc->pc_gen) & 1)
				yield();

			info->pr_nget = pc->pc_nget;
			info->pr_nfail = pc->pc_nfail;
			info->pr_nput = pc->pc_nput;
			info->pr_nlget = pc->pc_nlget;
			info->pr_nlfail = pc->pc_nlfail;
			info->pr_nlput = pc->pc_nlput;
		} while (gen != pc->pc_gen);

		cpu++;
	}

	error = sysctl_rdstruct(oldp, oldlenp, NULL, kpcc, len);
err:
	free(kpcc, M_TEMP, len);

	return (error);
}
#else /* MULTIPROCESSOR */
void
pool_cache_init(struct pool *pp)
{
	/* nop */
}

void
pool_cache_pool_info(struct pool *pp, struct kinfo_pool *pi)
{
	/* nop */
}

int
pool_cache_info(struct pool *pp, void *oldp, size_t *oldlenp)
{
	return (EOPNOTSUPP);
}

int
pool_cache_cpus_info(struct pool *pp, void *oldp, size_t *oldlenp)
{
	return (EOPNOTSUPP);
}
#endif /* MULTIPROCESSOR */


void
pool_lock_mtx_init(struct pool *pp, union pool_lock *lock,
    struct lock_type *type)
{
	_mtx_init_flags(&lock->prl_mtx, pp->pr_ipl, pp->pr_wchan, 0, type);
}

void
pool_lock_mtx_enter(union pool_lock *lock LOCK_FL_VARS)
{
	_mtx_enter(&lock->prl_mtx LOCK_FL_ARGS);
}

int
pool_lock_mtx_enter_try(union pool_lock *lock LOCK_FL_VARS)
{
	return (_mtx_enter_try(&lock->prl_mtx LOCK_FL_ARGS));
}

void
pool_lock_mtx_leave(union pool_lock *lock LOCK_FL_VARS)
{
	_mtx_leave(&lock->prl_mtx LOCK_FL_ARGS);
}

void
pool_lock_mtx_assert_locked(union pool_lock *lock)
{
	MUTEX_ASSERT_LOCKED(&lock->prl_mtx);
}

void
pool_lock_mtx_assert_unlocked(union pool_lock *lock)
{
	MUTEX_ASSERT_UNLOCKED(&lock->prl_mtx);
}

int
pool_lock_mtx_sleep(void *ident, union pool_lock *lock, int priority,
    const char *wmesg, int timo)
{
	return msleep(ident, &lock->prl_mtx, priority, wmesg, timo);
}

static const struct pool_lock_ops pool_lock_ops_mtx = {
	pool_lock_mtx_init,
	pool_lock_mtx_enter,
	pool_lock_mtx_enter_try,
	pool_lock_mtx_leave,
	pool_lock_mtx_assert_locked,
	pool_lock_mtx_assert_unlocked,
	pool_lock_mtx_sleep,
};

void
pool_lock_rw_init(struct pool *pp, union pool_lock *lock,
    struct lock_type *type)
{
	_rw_init_flags(&lock->prl_rwlock, pp->pr_wchan, 0, type);
}

void
pool_lock_rw_enter(union pool_lock *lock LOCK_FL_VARS)
{
	_rw_enter_write(&lock->prl_rwlock LOCK_FL_ARGS);
}

int
pool_lock_rw_enter_try(union pool_lock *lock LOCK_FL_VARS)
{
	return (_rw_enter(&lock->prl_rwlock, RW_WRITE | RW_NOSLEEP
	    LOCK_FL_ARGS) == 0);
}

void
pool_lock_rw_leave(union pool_lock *lock LOCK_FL_VARS)
{
	_rw_exit_write(&lock->prl_rwlock LOCK_FL_ARGS);
}

void
pool_lock_rw_assert_locked(union pool_lock *lock)
{
	rw_assert_wrlock(&lock->prl_rwlock);
}

void
pool_lock_rw_assert_unlocked(union pool_lock *lock)
{
	KASSERT(rw_status(&lock->prl_rwlock) != RW_WRITE);
}

int
pool_lock_rw_sleep(void *ident, union pool_lock *lock, int priority,
    const char *wmesg, int timo)
{
	return rwsleep(ident, &lock->prl_rwlock, priority, wmesg, timo);
}

static const struct pool_lock_ops pool_lock_ops_rw = {
	pool_lock_rw_init,
	pool_lock_rw_enter,
	pool_lock_rw_enter_try,
	pool_lock_rw_leave,
	pool_lock_rw_assert_locked,
	pool_lock_rw_assert_unlocked,
	pool_lock_rw_sleep,
};
@


1.219
log
@Compute the level of contention only once.

Suggested by and OK dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.218 2017/07/12 06:39:13 visa Exp $	*/
a40 1
#include <sys/rwlock.h>
d77 61
d223 1
a223 1
void	 pool_get_done(void *, void *);
d407 5
d455 4
a458 2
	mtx_init_flags(&pp->pr_mtx, pp->pr_ipl, wchan, 0);
	mtx_init_flags(&pp->pr_requests_mtx, pp->pr_ipl, wchan, 0);
d529 1
a529 1
		mtx_enter(&pp->pr_mtx);
d531 1
a531 1
		mtx_leave(&pp->pr_mtx);
d540 1
a540 1
    void (*handler)(void *, void *), void *cookie)
d550 1
a550 1
	mtx_enter(&pp->pr_requests_mtx);
d553 1
a553 1
	mtx_leave(&pp->pr_requests_mtx);
d557 1
a557 1
	struct mutex mtx;
d570 4
d582 1
a582 3
	KASSERT(flags & (PR_WAITOK | PR_NOWAIT));

	mtx_enter(&pp->pr_mtx);
d590 1
a590 1
	mtx_leave(&pp->pr_mtx);
d596 1
a596 3
		struct pool_get_memory mem = {
		    MUTEX_INITIALIZER(pp->pr_ipl),
		    NULL };
d599 1
d603 1
a603 1
		mtx_enter(&mem.mtx);
d605 2
a606 2
			msleep(&mem, &mem.mtx, PSWP, pp->pr_wchan, 0);
		mtx_leave(&mem.mtx);
d621 1
a621 1
	mtx_leave(&pp->pr_mtx);
d626 1
a626 1
pool_get_done(void *xmem, void *v)
d630 1
a630 1
	mtx_enter(&mem->mtx);
d632 1
a632 1
	mtx_leave(&mem->mtx);
d643 2
a644 2
	MUTEX_ASSERT_UNLOCKED(&pp->pr_mtx);
	MUTEX_ASSERT_LOCKED(&pp->pr_requests_mtx);
d660 1
a660 1
		mtx_leave(&pp->pr_requests_mtx);
d662 1
a662 1
		mtx_enter(&pp->pr_mtx);
d676 1
a676 1
		mtx_leave(&pp->pr_mtx);
d681 1
a681 1
			(*pr->pr_handler)(pr->pr_cookie, pr->pr_item);
d684 1
a684 1
		mtx_enter(&pp->pr_requests_mtx);
d700 1
a700 1
	MUTEX_ASSERT_LOCKED(&pp->pr_mtx);
d706 1
a706 1
	 * pr_mtx to allocate a page.
d711 1
a711 1
		mtx_leave(&pp->pr_mtx);
d713 1
a713 1
		mtx_enter(&pp->pr_mtx);
d747 1
a747 1
			    pidx * sizeof(int), ip[pidx]);
d798 1
a798 1
	mtx_enter(&pp->pr_mtx);
d813 1
a813 1
	mtx_leave(&pp->pr_mtx);
d819 1
a819 1
		mtx_enter(&pp->pr_requests_mtx);
d821 1
a821 1
		mtx_leave(&pp->pr_requests_mtx);
d898 1
a898 1
	mtx_enter(&pp->pr_mtx);
d903 1
a903 1
	mtx_leave(&pp->pr_mtx);
d916 1
a916 1
	MUTEX_ASSERT_UNLOCKED(&pp->pr_mtx);
d969 1
a969 1
	MUTEX_ASSERT_UNLOCKED(&pp->pr_mtx);
d1005 1
a1005 1
	MUTEX_ASSERT_LOCKED(&pp->pr_mtx);
d1026 1
a1026 1
	MUTEX_ASSERT_LOCKED(&pp->pr_mtx);
d1054 1
a1054 1
	mtx_enter(&pp->pr_mtx);
d1062 1
a1062 1
	mtx_leave(&pp->pr_mtx);
d1113 1
a1113 1
	mtx_enter(&pp->pr_mtx);
d1132 1
a1132 1
	mtx_leave(&pp->pr_mtx);
d1493 1
a1493 1
		mtx_enter(&pp->pr_mtx);
d1510 1
a1510 1
		mtx_leave(&pp->pr_mtx);
d1554 1
a1554 1
		    !mtx_enter_try(&pp->pr_mtx)) /* try */
d1566 1
a1566 1
		mtx_leave(&pp->pr_mtx);
d1707 2
a1708 1
		    CACHELINESIZE, IPL_NONE, PR_WAITOK, "plcache", NULL);
d1716 1
a1716 1
	mtx_init(&pp->pr_cache_mtx, pp->pr_ipl);
d1780 2
a1781 2
	if (mtx_enter_try(&pp->pr_cache_mtx) == 0) {
		mtx_enter(&pp->pr_cache_mtx);
d1789 1
a1789 1
	mtx_leave(&pp->pr_cache_mtx);
d1959 1
a1959 1
	mtx_enter(&pp->pr_mtx);
d1965 1
a1965 1
	mtx_leave(&pp->pr_mtx);
d2002 1
a2002 1
	    mtx_enter_try(&pp->pr_cache_mtx)) {
d2014 1
a2014 1
		mtx_leave(&pp->pr_cache_mtx);
d2048 1
a2048 1
	/* once without the mtx so we can yield while reading nget/nput */
d2065 1
a2065 1
	mtx_enter(&pp->pr_cache_mtx);
d2070 1
a2070 1
	mtx_leave(&pp->pr_cache_mtx);
d2083 1
a2083 1
	mtx_enter(&pp->pr_cache_mtx);
d2088 1
a2088 1
	mtx_leave(&pp->pr_cache_mtx);
d2172 110
@


1.218
log
@When there is no contention on a pool cache lock, lower the number
of items that a cache list is allowed to hold. This lets the cache
release resources back to the common pool after pressure on the cache
has decreased.

OK dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.217 2017/06/23 01:21:55 dlg Exp $	*/
d1929 1
a1929 1
	unsigned int contention;
d1957 2
a1958 1
	if ((contention - pp->pr_cache_contention_prev) > 8 /* magic */) {
d1961 1
a1961 1
	} else if ((contention - pp->pr_cache_contention_prev) == 0) {
@


1.217
log
@set the alignment of the per cpu cache structures to CACHELINESIZE.

hardcoding 64 is too optimistic.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.215 2017/06/19 23:57:12 dlg Exp $	*/
d1960 3
@


1.216
log
@change the semantic for calculating when to grow the size of a cache list.

previously it would figure out if there's enough items overall for
all the cpus to have full active an inactive free lists. this
included currently allocated items, which pools wont actually hold
on a free list and cannot predict when they will come back.

instead, see if there's enough items in the idle lists in the depot
that could instead go on all the free lists on the cpus. if there's
enough idle items, then we can grow.

tested by hrvoje popovski and amit kulkarni
ok visa@@
@
text
@d1638 2
a1639 2
		pool_init(&pool_caches, sizeof(struct pool_cache), 64,
		    IPL_NONE, PR_WAITOK, "plcache", NULL);
@


1.215
log
@dynamically scale the size of the per cpu cache lists.

if the lock around the global depot of extra cache lists is contented
a lot in between the gc task runs, consider growing the number of
entries a free list can hold.

the size of the list is bounded by the number of pool items the
current set of pages can represent to avoid having cpus starve each
other. im not sure this semantic is right (or the least worst) but
we're putting it in now to see what happens.

this also means reality matches the documentation i just committed
in pool_cache_init.9.

tested by hrvoje popovski and amit kulkarni
ok visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.214 2017/06/16 01:55:45 dlg Exp $	*/
d1650 1
a1650 1
	pp->pr_cache_nlist = 0;
d1732 1
a1732 1
		pp->pr_cache_nlist--;
d1756 1
a1757 1
	pp->pr_cache_nlist++;
d1939 1
a1940 1
			pp->pr_cache_nlist--;
d1958 2
a1959 8
		unsigned int limit = pp->pr_npages * pp->pr_itemsperpage;
		unsigned int items = pp->pr_cache_items + 8;
		unsigned int cache = ncpusfound * items * 2;

		/* are there enough items around so every cpu can hold some? */

		if (cache < limit)
			pp->pr_cache_items = items;
d2013 1
a2013 1
	kpc.pr_nlist = pp->pr_cache_nlist;
@


1.214
log
@add garbage collection of unused lists percpu cached items.

the cpu caches in pools amortise the cost of accessing global
structures by moving lists of items around instead of individual
items. excess lists of items are stored in the global pool struct,
but these idle lists never get returned back to the system for use
elsewhere.

this adds a timestamp to the global idle list, which is updated
when the idle list stops being empty. if the idle list hasn't been
empty for a while, it means the per cpu caches arent using the idle
entries and they can be recovered. timestamping the pages prevents
recovery of a lot of items that may be used again shortly. eg, rx
ring processing and replenishing from rate limited interrupts tends
to allocate and free items in large chunks, which the timestamping
smooths out.

gc'ed lists are returned to the pool pages, which in turn get gc'ed
back to uvm.

ok visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.211 2017/06/15 03:48:50 dlg Exp $	*/
d1929 2
d1949 19
@


1.213
log
@split returning an item to the pool pages out of pool_put as pool_do_put.

this lets pool_cache_list_put return items to the pages. currently,
if pool_cache_list_put is called while the per cpu caches are
enabled, the items on the list will put put straight back onto
another list in the cpu cache. this also avoids counting puts for
these items twice. a put for the items have already been coutned
when the items went to a cpu cache, it doesnt need to be counted
again when it goes back to the pool pages.

another side effect of this is that pool_cache_list_put can take
the pool mutex once when returning all the items in the list with
pool_do_put, rather than once per item.

ok visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.212 2017/06/15 03:50:50 dlg Exp $	*/
d138 1
d1480 5
d1651 1
d1654 1
d1670 2
d1753 3
d1909 1
d1912 1
d1927 23
d1996 1
a1996 1
	kpc.pr_ngc = 0; /* notyet */
@


1.212
log
@report contention on caches global data to userland.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.211 2017/06/15 03:48:50 dlg Exp $	*/
d159 1
a714 1
	struct pool_item *pi = v;
d731 31
a804 21

	pp->pr_nout--;
	pp->pr_nput++;

	/* is it time to free a page? */
	if (pp->pr_nidle > pp->pr_maxpages &&
	    (ph = TAILQ_FIRST(&pp->pr_emptypages)) != NULL &&
	    (ticks - ph->ph_tick) > (hz * pool_wait_free)) {
		freeph = ph;
		pool_p_remove(pp, freeph);
	}
	mtx_leave(&pp->pr_mtx);

	if (freeph != NULL)
		pool_p_free(pp, freeph);

	if (!TAILQ_EMPTY(&pp->pr_requests)) {
		mtx_enter(&pp->pr_requests_mtx);
		pool_runqueue(pp, PR_NOWAIT);
		mtx_leave(&pp->pr_requests_mtx);
	}
d1877 1
d1880 1
a1880 1
		pool_put(pp, pl);
d1883 1
@


1.211
log
@white space tweaks. no functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.210 2017/06/15 03:44:17 dlg Exp $	*/
d1949 1
@


1.210
log
@implement the backend of the sysctls that report pool cache info.

KERN_POOL_CACHE reports info about the global cache info, like how long
the lists of cache items the cpus build should be and how many of these
lists are idle on the pool struct.

KERN_POOL_CACHE_CPUS reports counters from each each. the counters
are for how many item and list operations the cache has handled on
a cpu. the sysctl provides an array of ncpusfound * struct
kinfo_pool_cache_cpu, not a single struct kinfo_pool_cache_cpu.

tested by hrvoje popovski
ok mikeb@@ millert@@
----------------------------------------------------------------------
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.209 2017/06/13 11:41:11 dlg Exp $	*/
d766 1
a766 1
	 	 */
d1626 1
a1626 1
        /* must be able to use the pool items as cache list items */
d1833 1
a1833 1
			
@


1.209
log
@when enabling cpu caches, check the item size against the right thing

lists of free items on the per cpu caches are built out the pool items
as struct pool_cache_items, not struct pool_cache. make the KASSERT
in pool_cache_init check that properly.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.208 2017/04/20 14:13:00 visa Exp $	*/
d125 6
a130 3
	uint64_t		 pc_gets;
	uint64_t		 pc_puts;
	uint64_t		 pc_fails;
d139 3
a141 1
void	 pool_cache_info(struct pool *, struct kinfo_pool *);
d1387 2
d1433 1
a1433 1
		pool_cache_info(pp, &pi);
d1437 8
d1643 6
a1648 3
		pc->pc_gets = 0;
		pc->pc_puts = 0;
		pc->pc_fails = 0;
d1715 4
a1718 1
	}
d1736 2
d1779 1
a1779 1
		pc->pc_fails++;
d1804 1
a1804 1
	pc->pc_gets++;
d1851 1
a1851 1
	pc->pc_puts++;
d1900 1
a1900 1
pool_cache_info(struct pool *pp, struct kinfo_pool *pi)
d1918 2
a1919 2
			nget = pc->pc_gets;
			nput = pc->pc_puts;
d1934 74
d2016 1
a2016 1
pool_cache_info(struct pool *pp, struct kinfo_pool *pi)
d2019 12
@


1.208
log
@Tweak lock inits to make the system runnable with witness(4)
on amd64 and i386.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.207 2017/02/20 00:43:25 dlg Exp $	*/
d1611 2
a1612 1
	KASSERT(pp->pr_size >= sizeof(*pc));
@


1.207
log
@revert 1.206 because it allows deadlocks.

if the gc task is running on a cpu that handles interrupts it is
possible to allow a deadlock. the gc task my be cleaning up a pool
and holding its mutex when an non-MPSAFE interrupt arrives and tries
to take the kernel lock. another cpu may already be holding the
kernel lock when it then tries use the same pool thats the pool GC
is currently processing.

thanks to sthen@@ and mpi@@ for chasing this down.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.205 2017/01/24 09:54:41 mpi Exp $	*/
d383 2
a384 2
	mtx_init(&pp->pr_mtx, pp->pr_ipl);
	mtx_init(&pp->pr_requests_mtx, pp->pr_ipl);
@


1.206
log
@the splvm() in pool_gc_pages is unecessary now.

all pools set their ipls unconditionally now, so there isn't a need
to second guess them.

pointed out by and ok jmatthew@@
@
text
@d1449 1
d1452 1
d1472 1
@


1.205
log
@Force a context switch for every pool_get(9) with the PR_WAITOK flag
if pool_debug is equal to 2, just like we do for malloc(9).

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.204 2016/11/21 01:44:06 dlg Exp $	*/
a1448 1
	int s;
a1450 1
	s = splvm(); /* XXX go to splvm until all pools _setipl properly */
a1469 1
	splx(s);
@


1.204
log
@let pool page allocators advertise what sizes they can provide.

to keep things concise i let the multi page allocators provide
multiple sizes of pages, but this feature was implicit inside
pool_init and only usable if the caller of pool_init did not specify
a page allocator.

callers of pool_init can now suplly a page allocator that provides
multiple page sizes. pool_init will try to fit 8 items onto a page
still, but will scale its page size down until it fits into what
the allocator provides.

supported page sizes are specified as a bit field in the pa_pagesz
member of a pool_allocator. setting the low bit in that word indicates
that the pages can be aligned to their size.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.203 2016/11/07 23:45:27 dlg Exp $	*/
d516 1
a516 1
	if (slowdown && ISSET(flags, PR_WAITOK))
@


1.203
log
@rename some types and functions to make the code easier to read.

pool_item_header is now pool_page_header. the more useful change
is pool_list is now pool_cache_item. that's what items going into
the per cpu pool caches are cast to, and they get linked together
to make a list.

the functions operating on what is now pool_cache_items have been
renamed to make it more obvious what they manipulate.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.202 2016/11/02 06:26:16 dlg Exp $	*/
d173 2
a174 1
	pool_page_free
d182 2
a183 1
	pool_multi_free
d191 2
a192 1
	pool_multi_free_ni
d270 1
d283 3
a286 3
		while (size * 8 > pgsize)
			pgsize <<= 1;

d292 23
a314 2
	} else
		pgsize = palloc->pa_pagesz ? palloc->pa_pagesz : PAGE_SIZE;
d324 8
a331 5
	if (pgsize - (size * items) > sizeof(struct pool_page_header)) {
		off = pgsize - sizeof(struct pool_page_header);
	} else if (sizeof(struct pool_page_header) * 2 >= size) {
		off = pgsize - sizeof(struct pool_page_header);
		items = off / size;
@


1.202
log
@poison the TAILQ_ENTRY in items in the per cpu pool cache.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.201 2016/11/02 03:29:48 dlg Exp $	*/
d56 1
a56 1
 * pool items are on a linked list headed by `ph_itemlist' in each page
d78 7
a84 1
struct pool_item_header {
d86 7
a92 6
	TAILQ_ENTRY(pool_item_header)
				ph_pagelist;	/* pool page list */
	XSIMPLEQ_HEAD(,pool_item) ph_itemlist;	/* chunk list for this page */
	RBT_ENTRY(pool_item_header)
				ph_node;	/* Off-page page headers */
	int			ph_nmissing;	/* # of chunks in use */
d95 1
a95 1
	u_long			ph_magic;
a100 6
struct pool_item {
	u_long				pi_magic;
	XSIMPLEQ_ENTRY(pool_item)	pi_list;
};
#define POOL_IMAGIC(ph, pi) ((u_long)(pi) ^ (ph)->ph_magic)

d102 5
a106 4
struct pool_list {
	struct pool_list	*pl_next;	/* next in list */
	unsigned long		 pl_nitems;	/* items in list */
	TAILQ_ENTRY(pool_list)	 pl_nextl;	/* list of lists */
d109 3
a111 2
#define POOL_LIST_NITEMS_MASK		0x7ffffffUL
#define POOL_LIST_NITEMS_POISON		0x8000000UL
d113 2
a114 2
#define POOL_LIST_POISONED(_pl)						\
    ISSET((_pl)->pl_nitems, POOL_LIST_NITEMS_POISON)
d116 2
a117 2
#define POOL_LIST_NITEMS(_pl)						\
    ((_pl)->pl_nitems & POOL_LIST_NITEMS_MASK)
d120 3
a122 3
	struct pool_list	*pc_actv;
	unsigned long		 pc_nactv;	/* cache pc_actv nitems */
	struct pool_list	*pc_prev;
d146 1
a146 1
struct pool_item_header *
d148 3
a150 3
void	 pool_p_insert(struct pool *, struct pool_item_header *);
void	 pool_p_remove(struct pool *, struct pool_item_header *);
void	 pool_p_free(struct pool *, struct pool_item_header *);
d154 1
a154 1
int	 pool_chk_page(struct pool *, struct pool_item_header *, int);
d207 1
a207 1
RBT_PROTOTYPE(phtree, pool_item_header, ph_node, phtree_compare);
d210 2
a211 2
phtree_compare(const struct pool_item_header *a,
    const struct pool_item_header *b)
d225 1
a225 1
RBT_GENERATE(phtree, pool_item_header, ph_node, phtree_compare);
d230 1
a230 1
static inline struct pool_item_header *
d233 1
a233 1
	struct pool_item_header *ph, key;
d240 1
a240 1
		return ((struct pool_item_header *)(page + pp->pr_phoffset));
d299 4
a302 4
	if (pgsize - (size * items) > sizeof(struct pool_item_header)) {
		off = pgsize - sizeof(struct pool_item_header);
	} else if (sizeof(struct pool_item_header) * 2 >= size) {
		off = pgsize - sizeof(struct pool_item_header);
d360 1
a360 1
		pool_init(&phpool, sizeof(struct pool_item_header), 0,
d394 1
a394 1
	struct pool_item_header *ph;
d595 1
a595 1
	struct pool_item_header *ph;
d621 1
a621 1
	pi = XSIMPLEQ_FIRST(&ph->ph_itemlist);
d632 1
a632 1
	XSIMPLEQ_REMOVE_HEAD(&ph->ph_itemlist, pi_list);
d654 2
a655 2
		TAILQ_REMOVE(&pp->pr_emptypages, ph, ph_pagelist);
		TAILQ_INSERT_TAIL(&pp->pr_partpages, ph, ph_pagelist);
d665 2
a666 2
		TAILQ_REMOVE(&pp->pr_partpages, ph, ph_pagelist);
		TAILQ_INSERT_TAIL(&pp->pr_fullpages, ph, ph_pagelist);
d682 1
a682 1
	struct pool_item_header *ph, *freeph = NULL;
d705 1
a705 1
		XSIMPLEQ_FOREACH(qi, &ph->ph_itemlist, pi_list) {
d715 1
a715 1
	XSIMPLEQ_INSERT_HEAD(&ph->ph_itemlist, pi, pi_list);
d726 2
a727 2
		TAILQ_REMOVE(&pp->pr_fullpages, ph, ph_pagelist);
		TAILQ_INSERT_TAIL(&pp->pr_partpages, ph, ph_pagelist);
d737 2
a738 2
		TAILQ_REMOVE(&pp->pr_partpages, ph, ph_pagelist);
		TAILQ_INSERT_TAIL(&pp->pr_emptypages, ph, ph_pagelist);
d771 1
a771 1
	struct pool_item_header *ph;
d783 1
a783 1
		TAILQ_INSERT_TAIL(&pl, ph, ph_pagelist);
d788 1
a788 1
		TAILQ_REMOVE(&pl, ph, ph_pagelist);
d796 1
a796 1
struct pool_item_header *
d799 1
a799 1
	struct pool_item_header *ph;
d812 1
a812 1
		ph = (struct pool_item_header *)(addr + pp->pr_phoffset);
d821 1
a821 1
	XSIMPLEQ_INIT(&ph->ph_itemlist);
d839 1
a839 1
		XSIMPLEQ_INSERT_TAIL(&ph->ph_itemlist, pi, pi_list);
d853 1
a853 1
pool_p_free(struct pool *pp, struct pool_item_header *ph)
d860 1
a860 1
	XSIMPLEQ_FOREACH(pi, &ph->ph_itemlist, pi_list) {
d891 1
a891 1
pool_p_insert(struct pool *pp, struct pool_item_header *ph)
d899 1
a899 1
	TAILQ_INSERT_TAIL(&pp->pr_emptypages, ph, ph_pagelist);
d912 1
a912 1
pool_p_remove(struct pool *pp, struct pool_item_header *ph)
d923 1
a923 1
	TAILQ_REMOVE(&pp->pr_emptypages, ph, ph_pagelist);
d998 1
a998 1
	struct pool_item_header *ph, *phnext;
d1003 1
a1003 1
		phnext = TAILQ_NEXT(ph, ph_pagelist);
d1018 1
a1018 1
		TAILQ_INSERT_TAIL(&pl, ph, ph_pagelist);
d1026 1
a1026 1
		TAILQ_REMOVE(&pl, ph, ph_pagelist);
d1066 1
a1066 1
	struct pool_item_header *ph;
d1069 1
a1069 1
	TAILQ_FOREACH(ph, pl, ph_pagelist) {
d1072 1
a1072 1
		XSIMPLEQ_FOREACH(pi, &ph->ph_itemlist, pi_list) {
d1085 1
a1085 1
	struct pool_item_header *ph;
d1204 1
a1204 1
pool_chk_page(struct pool *pp, struct pool_item_header *ph, int expected)
d1220 1
a1220 1
	for (pi = XSIMPLEQ_FIRST(&ph->ph_itemlist), n = 0;
d1222 1
a1222 1
	     pi = XSIMPLEQ_NEXT(&ph->ph_itemlist, pi, pi_list), n++) {
d1277 1
a1277 1
	struct pool_item_header *ph;
d1280 1
a1280 1
	TAILQ_FOREACH(ph, &pp->pr_emptypages, ph_pagelist)
d1282 1
a1282 1
	TAILQ_FOREACH(ph, &pp->pr_fullpages, ph_pagelist)
d1284 1
a1284 1
	TAILQ_FOREACH(ph, &pp->pr_partpages, ph_pagelist)
d1298 1
a1298 1
	struct pool_item_header *ph;
d1303 1
a1303 1
	TAILQ_FOREACH(ph, &pp->pr_fullpages, ph_pagelist) {
d1313 1
a1313 1
	TAILQ_FOREACH(ph, &pp->pr_partpages, ph_pagelist) {
d1318 1
a1318 1
			XSIMPLEQ_FOREACH(pi, &ph->ph_itemlist, pi_list) {
d1420 1
a1420 1
	struct pool_item_header *ph, *freeph;
d1609 1
a1609 1
pool_list_magic(struct pool *pp, struct pool_list *pl)
d1611 1
a1611 1
	unsigned long *entry = (unsigned long *)&pl->pl_nextl;
d1613 2
a1614 2
	entry[0] = pp->pr_cache_magic[0] ^ (u_long)pl;
	entry[1] = pp->pr_cache_magic[1] ^ (u_long)pl->pl_next;
d1618 1
a1618 1
pool_list_magic_check(struct pool *pp, struct pool_list *pl)
d1623 2
a1624 2
	entry = (unsigned long *)&pl->pl_nextl;
	val = pp->pr_cache_magic[0] ^ (u_long)pl;
d1629 1
a1629 1
	val = pp->pr_cache_magic[1] ^ (u_long)pl->pl_next;
d1637 1
a1637 1
	    __func__, pp->pr_wchan, pl, (caddr_t)entry - (caddr_t)pl,
d1656 2
a1657 2
static inline struct pool_list *
pool_list_alloc(struct pool *pp, struct pool_cache *pc)
d1659 1
a1659 1
	struct pool_list *pl;
d1664 1
a1664 1
		TAILQ_REMOVE(&pp->pr_cache_lists, pl, pl_nextl);
d1667 1
a1667 1
		pool_list_magic(pp, pl);
d1670 1
d1679 2
a1680 1
pool_list_free(struct pool *pp, struct pool_cache *pc, struct pool_list *pl)
d1683 1
a1683 1
	TAILQ_INSERT_TAIL(&pp->pr_cache_lists, pl, pl_nextl);
d1686 1
d1716 1
a1716 1
	struct pool_list *pl;
d1722 1
a1722 1
		pl = pc->pc_actv;
d1724 1
a1724 1
		pl = pc->pc_prev;
d1726 1
a1726 1
	} else if ((pl = pool_list_alloc(pp, pc)) == NULL) {
d1731 1
a1731 1
	pool_list_magic_check(pp, pl);
d1733 1
a1733 1
	if (pool_debug && POOL_LIST_POISONED(pl)) {
d1737 1
a1737 1
		if (poison_check(pl + 1, pp->pr_size - sizeof(*pl),
d1739 1
a1739 1
			int *ip = (int *)(pl + 1);
d1744 2
a1745 2
			    __func__, pp->pr_wchan, pl,
			    (caddr_t)ip - (caddr_t)pl, *ip, pval);
d1750 2
a1751 2
	pc->pc_actv = pl->pl_next;
	pc->pc_nactv = POOL_LIST_NITEMS(pl) - 1;
d1758 1
a1758 1
	return (pl);
d1765 1
a1765 1
	struct pool_list *pl = v;
d1769 1
a1769 1
	int poison = pool_debug && pp->pr_size > sizeof(*pl);
d1772 1
a1772 1
		poison_mem(pl + 1, pp->pr_size - sizeof(*pl));
d1780 1
a1780 1
			pool_list_free(pp, pc, pc->pc_prev);
d1789 2
a1790 2
	pl->pl_next = pc->pc_actv;
	pl->pl_nitems = ++nitems;
d1792 1
a1792 1
	pl->pl_nitems |= poison ? POOL_LIST_NITEMS_POISON : 0;
d1794 1
a1794 1
	pool_list_magic(pp, pl);
d1796 1
a1796 1
	pc->pc_actv = pl;
d1805 2
a1806 2
struct pool_list *
pool_list_put(struct pool *pp, struct pool_list *pl)
d1808 1
a1808 1
	struct pool_list *rpl, *npl;
d1813 1
a1813 1
	rpl = TAILQ_NEXT(pl, pl_nextl);
d1816 1
a1816 1
		npl = pl->pl_next;
d1818 1
a1818 1
		pl = npl;
d1828 1
a1828 1
	struct pool_list *pl;
d1836 2
a1837 2
		pool_list_put(pp, pc->pc_actv);
		pool_list_put(pp, pc->pc_prev);
d1844 1
a1844 1
		pl = pool_list_put(pp, pl);
@


1.201
log
@add poisoning of items on the per cpu caches.

it copies the existing pool code, except it works on pool_list
structures instead of pool_item structures.

after this id like to poison the words used by the TAILQ_ENTRY in
the pool_list struct that arent used until a list of items is moved
into the global depot.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.200 2016/11/02 01:58:07 dlg Exp $	*/
d1585 1
d1606 33
d1663 2
d1725 1
a1725 8
	pc->pc_actv = pl->pl_next;
	pc->pc_nactv = POOL_LIST_NITEMS(pl) - 1;
	pc->pc_gets++;
	pc->pc_nout++;

done:
	pool_cache_leave(pp, pc, s);

d1727 1
a1727 1
	if (pool_debug && pl != NULL && POOL_LIST_POISONED(pl)) {
d1734 2
d1737 1
a1737 1
			    "item addr %p; offset 0x%zx=0x%x",
d1739 1
a1739 1
			    pidx * sizeof(int) + sizeof(*pl), ip[pidx]);
d1744 8
d1788 1
@


1.200
log
@use a TAILQ to maintain the list of item lists used by the percpu code.

it makes it more readable, and fixes a bug in pool_list_put where it
was returning the next item in the current list rather than the next
list to be freed.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.199 2016/11/02 01:20:50 dlg Exp $	*/
d107 9
d1690 1
a1690 1
	pc->pc_nactv = pl->pl_nitems - 1;
d1693 1
d1697 16
a1720 1
	unsigned long cache_items = pp->pr_cache_items;
d1723 6
d1733 1
a1733 1
	if (nitems >= cache_items) {
d1746 3
@


1.199
log
@add per cpu caches for free pool items.

this is modelled on whats described in the "Magazines and Vmem:
Extending the Slab Allocator to Many CPUs and Arbitrary Resources"
paper by Jeff Bonwick and Jonathan Adams.

the main semantic borrowed from the paper is the use of two lists
of free pool items on each cpu, and only moving one of the lists
in and out of a global depot of free lists to mitigate against a
cpu thrashing against that global depot.

unlike slabs, pools do not maintain or cache constructed items,
which allows us to use the items themselves to build the free list
rather than having to allocate arrays to point at constructed pool
items.

the per cpu caches are build on top of the cpumem api.

this has been kicked a bit by hrvoje popovski and simon mages (thank you).
im putting it in now so it is easier to work on and test.
ok jmatthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.198 2016/09/15 02:00:16 dlg Exp $	*/
a102 2
	unsigned long		 pl_cookie;
	struct pool_list	*pl_nextl;	/* next list */
d104 1
d1576 1
a1576 1
	pp->pr_cache_list = NULL;
d1616 1
a1616 1
	pl = pp->pr_cache_list;
d1618 1
a1618 1
		pp->pr_cache_list = pl->pl_nextl;
d1633 1
a1633 2
	pl->pl_nextl = pp->pr_cache_list;
	pp->pr_cache_list = pl;
d1733 1
a1733 1
	rpl = (struct pool_list *)pl->pl_next;
d1762 1
a1762 1
	pl = pp->pr_cache_list;
@


1.198
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.197 2016/09/15 01:24:08 dlg Exp $	*/
d45 1
d100 27
d386 5
d457 8
d497 3
d678 7
d1387 2
d1555 262
@


1.197
log
@move pools to using the subr_tree version of rb trees

this is half way to recovering the space used by the subr_tree code.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.196 2016/09/05 09:04:31 dlg Exp $	*/
d223 1
a223 1
pool_init(struct pool *pp, size_t size, u_int align, u_int ioff, int flags,
a229 1
	KASSERT(ioff == 0);
d315 3
a317 3
	pp->pr_ipl = -1;
	mtx_init(&pp->pr_mtx, IPL_NONE);
	mtx_init(&pp->pr_requests_mtx, IPL_NONE);
d321 2
a322 3
		pool_init(&phpool, sizeof(struct pool_item_header), 0, 0,
		    0, "phpool", NULL);
		pool_setipl(&phpool, IPL_HIGH);
a348 8
void
pool_setipl(struct pool *pp, int ipl)
{
	pp->pr_ipl = ipl;
	mtx_init(&pp->pr_mtx, ipl);
	mtx_init(&pp->pr_requests_mtx, ipl);
}

a425 1

d441 2
a442 2
		    MUTEX_INITIALIZER((pp->pr_ipl == -1) ?
		    IPL_NONE : pp->pr_ipl), NULL };
d544 1
a544 2
	if (pp->pr_ipl != -1)
		splassert(pp->pr_ipl);
d636 1
a636 2
	if (pp->pr_ipl != -1)
		splassert(pp->pr_ipl);
d1317 1
a1317 2
		if (pp->pr_ipl != -1)
			mtx_enter(&pp->pr_mtx);
d1334 1
a1334 2
		if (pp->pr_ipl != -1)
			mtx_leave(&pp->pr_mtx);
@


1.196
log
@revert moving pools from tree.h to subr_tree.c rb trees.

itll go in again when i dont break userland.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.194 2016/01/15 11:21:58 dlg Exp $	*/
d82 1
a82 1
	RB_ENTRY(pool_item_header)
d168 2
d171 2
a172 1
phtree_compare(struct pool_item_header *a, struct pool_item_header *b)
d186 1
a186 2
RB_PROTOTYPE(phtree, pool_item_header, ph_node, phtree_compare);
RB_GENERATE(phtree, pool_item_header, ph_node, phtree_compare);
d205 1
a205 1
	ph = RB_NFIND(phtree, &pp->pr_phtree, &key);
d297 1
a297 1
	RB_INIT(&pp->pr_phtree);
d852 1
a852 1
		RB_INSERT(phtree, &pp->pr_phtree, ph);
d873 1
a873 1
		RB_REMOVE(phtree, &pp->pr_phtree, ph);
@


1.195
log
@move pool red-black trees from tree.h code to subr_tree.c code

ok tedu@@
@
text
@d82 1
a82 1
	RBT_ENTRY(pool_item_header)
a167 2
RBT_PROTOTYPE(phtree, pool_item_header, ph_node, phtree_compare);

d169 1
a169 2
phtree_compare(const struct pool_item_header *a,
    const struct pool_item_header *b)
d183 2
a184 1
RBT_GENERATE(phtree, pool_item_header, ph_node, phtree_compare);
d203 1
a203 1
	ph = RBT_NFIND(phtree, &pp->pr_phtree, &key);
d295 1
a295 1
	RBT_INIT(phtree, &pp->pr_phtree);
d850 1
a850 1
		RBT_INSERT(phtree, &pp->pr_phtree, ph);
d871 1
a871 1
		RBT_REMOVE(phtree, &pp->pr_phtree, ph);
@


1.194
log
@add a "show socket" command to ddb

should help inspecting socket issues in the future.

enthusiasm from mpi@@ bluhm@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.193 2015/09/11 09:26:13 kettenis Exp $	*/
d82 1
a82 1
	RB_ENTRY(pool_item_header)
d168 2
d171 2
a172 1
phtree_compare(struct pool_item_header *a, struct pool_item_header *b)
d186 1
a186 2
RB_PROTOTYPE(phtree, pool_item_header, ph_node, phtree_compare);
RB_GENERATE(phtree, pool_item_header, ph_node, phtree_compare);
d205 1
a205 1
	ph = RB_NFIND(phtree, &pp->pr_phtree, &key);
d297 1
a297 1
	RB_INIT(&pp->pr_phtree);
d852 1
a852 1
		RB_INSERT(phtree, &pp->pr_phtree, ph);
d873 1
a873 1
		RB_REMOVE(phtree, &pp->pr_phtree, ph);
@


1.193
log
@Now that interrupt-safe uvm maps are porperly locked, the interrupt-safe
multi page backend allocator implementation no longer needs to grab the
kernel lock.

ok mlarkin@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.192 2015/09/08 21:28:36 kettenis Exp $	*/
d706 5
a710 3
	mtx_enter(&pp->pr_requests_mtx);
	pool_runqueue(pp, PR_NOWAIT);
	mtx_leave(&pp->pr_requests_mtx);
@


1.192
log
@Give the pool page allocator backends more sensible names.  We now have:
* pool_allocator_single: single page allocator, always interrupt safe
* pool_allocator_multi: multi-page allocator, interrupt safe
* pool_allocator_multi_ni: multi-page allocator, not interrupt-safe

ok deraadt@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.191 2015/09/08 13:37:21 kettenis Exp $	*/
a1460 1
	KERNEL_LOCK();
a1461 1
	KERNEL_UNLOCK();
a1476 1
	KERNEL_LOCK();
a1477 1
	KERNEL_UNLOCK();
@


1.191
log
@Now that msleep(9) no longer requires the kernel lock (as long as PCATCH
isn't specified) the default backend allocator implementation no longer
needs to grab the kernel lock.

ok visa@@, guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.190 2015/09/06 20:58:14 kettenis Exp $	*/
d130 1
a130 2
 * safe for interrupts, name preserved for compat this is the default
 * allocator
d132 1
a132 1
struct pool_allocator pool_allocator_nointr = {
d137 2
a138 2
void	*pool_large_alloc(struct pool *, int, int *);
void	pool_large_free(struct pool *, void *);
d140 3
a142 3
struct pool_allocator pool_allocator_large = {
	pool_large_alloc,
	pool_large_free
d145 2
a146 2
void	*pool_large_alloc_ni(struct pool *, int, int *);
void	pool_large_free_ni(struct pool *, void *);
d148 3
a150 3
struct pool_allocator pool_allocator_large_ni = {
	pool_large_alloc_ni,
	pool_large_free_ni
d245 1
a245 1
			    &pool_allocator_large_ni : &pool_allocator_large;
d247 1
a247 1
			palloc = &pool_allocator_nointr;
d1447 1
a1447 1
pool_large_alloc(struct pool *pp, int flags, int *slowdown)
d1470 1
a1470 1
pool_large_free(struct pool *pp, void *v)
d1486 1
a1486 1
pool_large_alloc_ni(struct pool *pp, int flags, int *slowdown)
d1506 1
a1506 1
pool_large_free_ni(struct pool *pp, void *v)
@


1.190
log
@We no longer need to grab the kernel lock for allocating and freeing pages
in the (default) single page pool backend allocator.  This means it is now
safe to call pool_get(9) and pool_put(9) for "small" items while holding
a mutex without holding the kernel lock as well as these functions will
no longer acquire the kernel lock under any circumstances.  For "large" items
(where large is larger than 1/8th of a page) this still isn't safe though.

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.189 2015/09/01 08:22:45 kettenis Exp $	*/
a1433 1
	void *v;
d1438 1
a1438 13
	/* 
	 * XXX Until we can call msleep(9) without holding the kernel
	 * lock.
	 */
	if (ISSET(flags, PR_WAITOK))
		KERNEL_LOCK();

	v = km_alloc(pp->pr_pgsize, &kv_page, pp->pr_crange, &kd);

	if (ISSET(flags, PR_WAITOK))
		KERNEL_UNLOCK();

	return (v);
@


1.189
log
@Push down the KERNEL_LOCK/KERNEL_UNLOCK calls into the back-end allocator
functions.  Note that these calls are deliberately not added to the
special-purpose back-end allocators in the various pmaps.  Those allocators
either don't need to grab the kernel lock, are always called with the kernel
lock already held, or are only used on non-MULTIPROCESSOR platforms.

pk tedu@@, deraadt@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.188 2015/08/21 03:03:44 dlg Exp $	*/
d1439 7
a1445 1
	KERNEL_LOCK();
d1447 3
a1449 1
	KERNEL_UNLOCK();
a1456 1
	KERNEL_LOCK();
a1457 1
	KERNEL_UNLOCK();
@


1.188
log
@re-enable *8.

if we're allowed to try and use large pages, we try and fit at least
8 of the items. this amortises the per page cost of an item a bit.

"be careful" deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.187 2015/07/23 12:44:43 dlg Exp $	*/
a1406 1
	KERNEL_LOCK();
a1407 1
	KERNEL_UNLOCK();
a1426 1
	KERNEL_LOCK();
a1427 1
	KERNEL_UNLOCK();
d1434 1
d1439 5
a1443 1
	return (km_alloc(pp->pr_pgsize, &kv_page, pp->pr_crange, &kd));
d1449 1
d1451 1
d1469 1
d1471 1
d1487 1
d1489 1
d1498 1
d1506 5
a1510 1
	return (km_alloc(pp->pr_pgsize, &kv, pp->pr_crange, &kd));
d1521 1
d1523 1
@


1.187
log
@remove the POOL_NEEDS_CATCHUP macro, it isnt used.

from martin natano
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.186 2015/07/20 23:47:20 uebayasi Exp $	*/
d241 1
a241 1
		while (size > pgsize)
@


1.186
log
@Move `ticks' declaration to sys/kernel.h.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a103 3

#define	POOL_NEEDS_CATCHUP(pp)						\
	((pp)->pr_nitems < (pp)->pr_minitems)
@


1.185
log
@disable *8 again for now. incoherent archs arent having much fun with it.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.184 2015/04/07 11:15:02 dlg Exp $	*/
a641 1
	extern int ticks;
a1369 1
	extern int ticks;
@


1.184
log
@nothing uses pool_sleep, so get rid of it
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.183 2015/04/07 11:07:56 dlg Exp $	*/
d244 1
a244 1
		while (size * 8 > pgsize)
@


1.183
log
@introduce a garbage collector for (very) idle pool pages.

now that idle pool pages are timestamped we can tell how long theyve
been idle. this adds a task that runs every second that iterates
over all the pools looking for pages that have been idle for 8
seconds so it can free them.

this idea probably came from a conversation with tedu@@ months ago.

ok tedu@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.182 2015/03/20 11:33:17 dlg Exp $	*/
a162 2

#define pool_sleep(pl) msleep(pl, &pl->pr_mtx, PSWP, pl->pr_wchan, 0)
@


1.182
log
@reintroduce r1.173:

> if we're able to use large page allocators, try and place at least
> 8 items on a page. this reduces the number of allocator operations
> we have to do per item on large items.

this was backed out because of fallout on landisk which has since
been fixed. putting this in again early in the cycle so we can look
for more fallout. hopefully it will stick.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.181 2015/03/14 03:38:50 jsg Exp $	*/
d43 2
d166 8
d704 1
a704 1
	    (ticks - ph->ph_tick) > hz) {
d1362 41
@


1.181
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.180 2015/02/10 06:16:13 dlg Exp $	*/
d236 1
a236 1
		while (size > pgsize)
@


1.180
log
@reintroduce page item cache colouring.

if you're having trouble understanding what this helps, imagine
your cpus caches are a hash table. by moving the base address of
items around (colouring them), you give it more bits to hash with.
in turn that makes it less likely that you will overflow buckets
in your hash. i mean cache.

it was inadvertantly removed in my churn of this subsystem, but as
tedu has said on this issue:

> The history of pool is filled with features getting trimmed because they
> seemed unnecessary or in the way, only to later discover how important they
> were. Having slowly learned that lesson, I think our default should be "if
> bonwick says do it, we do it" until proven otherwise.

until proven otherwise we can keep the functionality, especially
as the code cost is minimal.

ok many including tedu@@ guenther@@ deraadt@@ millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.179 2015/01/22 05:09:41 dlg Exp $	*/
a993 1
#include <ddb/db_interface.h>
@


1.179
log
@pool_chk_page iterates over a pages free item lists and checks that
the items address is within the page. it does that by masking the
item address with the page mask and comparing that to the page
address.

however, if we're using large pages with external page headers, we
dont request that the large page be aligned to its size. eg, on an
arch with 4k pages, an 8k large page could be aligned to 4k, so
masking bits to get the page address wont work.

these incorrect checks were distracting while i was debugging large
pages on landisk.

this changes it to do range checks to see if the item is within the
page. it also checks if the item is on the page before checking if
its magic values or poison is right.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.178 2015/01/19 03:57:22 dlg Exp $	*/
d84 1
d220 1
a220 1
	int off = 0;
d293 9
d767 2
d1015 2
a1016 2
		(*pr)("\t\tpage %p, nmissing %d\n",
		    ph->ph_page, ph->ph_nmissing);
d1040 2
a1041 1
	(*pr)("POOL %s: size %u\n", pp->pr_wchan, pp->pr_size);
d1249 1
a1249 1
		cp = ph->ph_page;
d1259 1
a1259 1
		cp = ph->ph_page;
@


1.178
log
@white space fixes. no binary change.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.177 2015/01/05 23:54:18 dlg Exp $	*/
d1155 9
a1187 10

		page = (caddr_t)((u_long)pi & pp->pr_pgmask);
		if (page == ph->ph_page)
			continue;

		printf("%s: ", label);
		printf("pool(%p:%s): page inconsistency: page %p;"
		    " item ordinal %d; addr %p (p %p)\n", pp,
		    pp->pr_wchan, ph->ph_page, n, pi, page);
		return 1;
@


1.177
log
@splassert on some archs (or just sparc64) check that you're not in
an interrupt handler at an ipl level higher than what you're
splasserting you should be at. if you think code should be protected
by IPL_BIO and its entered from an interrupt handler established
at IPL_NET, you have a bug.

add some asserts to gets and puts so we can pick those cases up.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.176 2015/01/04 08:54:01 dlg Exp $	*/
d736 1
a736 1
	int n; 
d789 2
a790 2
        MUTEX_ASSERT_UNLOCKED(&pp->pr_mtx);
        KASSERT(ph->ph_nmissing == 0);
d814 1
a814 1
        }
d816 1
a816 1
        pool_allocator_free(pp, ph->ph_page);
d825 1
a825 1
        MUTEX_ASSERT_LOCKED(&pp->pr_mtx);
@


1.176
log
@back out r1.173, aka the "* 8" diff. it tickles a problem on some
landisk machines. we've been unable to figure out due to a lack of
hardware (on my part) or time.

discussed with and ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.175 2015/01/04 02:53:19 jsg Exp $	*/
d539 3
d632 3
@


1.175
log
@avoid the use of an uninitialised variable in one of the codepaths in
pool_setlowat()

ok dlg@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.174 2014/12/22 02:59:53 tedu Exp $	*/
d235 1
a235 1
		while (size * 8 > pgsize)
@


1.174
log
@remove some unused fields from pool. ok dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.173 2014/12/22 00:33:40 dlg Exp $	*/
d866 1
a866 1
	int prime;
@


1.173
log
@if we're able to use large page allocators, try and place at least
8 items on a page. this reduces the number of allocator operations
we have to do per item on large items.

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.172 2014/12/19 02:49:07 dlg Exp $	*/
a274 2
	pp->pr_roflags = flags;
	pp->pr_flags = 0;
a279 1
	pp->pr_align = align;
d1022 1
a1022 3
	(*pr)("POOL %s: size %u, align %u, roflags 0x%08x\n",
	    pp->pr_wchan, pp->pr_size, pp->pr_align,
	    pp->pr_roflags);
@


1.172
log
@timestamp empty pages, and only free them if theyve been idle for at least
a second.

this basically brings back the functionality that was trimmed in r1.53,
except this version uses ticks instead of very slow hardware clock reads.

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.171 2014/12/19 02:46:47 dlg Exp $	*/
d235 1
a235 1
		while (size > pgsize)
@


1.171
log
@the last commit changed LIST_INSERT_HEAD to TAILQ_INSERT_TAIL cos the
latter is cheaper, but i forgot to change the thing that pulls pages off
those lists to match the change in direction. the page lists went from LIFO
to FIFO.

this changes pool_update_curpage to use TAILQ_LAST so we go back to LIFO.

pointed out by and ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.170 2014/12/19 02:15:25 dlg Exp $	*/
d85 1
d624 1
d669 1
d680 3
a682 1
	    (freeph = TAILQ_FIRST(&pp->pr_emptypages)) != NULL)
d684 1
@


1.170
log
@replace the page LISTS with page TAILQs. this will let me pull pages from
either end of the lists cheaply.

ok kettenis@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.169 2014/12/04 03:12:05 dlg Exp $	*/
d854 1
a854 1
	pp->pr_curpage = TAILQ_FIRST(&pp->pr_partpages);
d856 1
a856 1
		pp->pr_curpage = TAILQ_FIRST(&pp->pr_emptypages);
@


1.169
log
@init the mutex used in sleeping pool_gets with the right ipl if the
pool hasnt had pool_setipl called.

ok kettenis@@ ages ago
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.168 2014/11/18 02:37:31 tedu Exp $	*/
d77 1
a77 1
	LIST_ENTRY(pool_item_header)
d266 3
a268 3
	LIST_INIT(&pp->pr_emptypages);
	LIST_INIT(&pp->pr_fullpages);
	LIST_INIT(&pp->pr_partpages);
d378 1
a378 1
	while ((ph = LIST_FIRST(&pp->pr_emptypages)) != NULL) {
d384 2
a385 2
	KASSERT(LIST_EMPTY(&pp->pr_fullpages));
	KASSERT(LIST_EMPTY(&pp->pr_partpages));
d594 2
a595 2
		LIST_REMOVE(ph, ph_pagelist);
		LIST_INSERT_HEAD(&pp->pr_partpages, ph, ph_pagelist);
d605 2
a606 2
		LIST_REMOVE(ph, ph_pagelist);
		LIST_INSERT_HEAD(&pp->pr_fullpages, ph, ph_pagelist);
d657 2
a658 2
		LIST_REMOVE(ph, ph_pagelist);
		LIST_INSERT_HEAD(&pp->pr_partpages, ph, ph_pagelist);
d667 2
a668 2
		LIST_REMOVE(ph, ph_pagelist);
		LIST_INSERT_HEAD(&pp->pr_emptypages, ph, ph_pagelist);
d677 1
a677 1
	    (freeph = LIST_FIRST(&pp->pr_emptypages)) != NULL)
d695 1
a695 1
	struct pool_pagelist pl = LIST_HEAD_INITIALIZER(pl);
d708 1
a708 1
		LIST_INSERT_HEAD(&pl, ph, ph_pagelist);
d712 2
a713 2
	while ((ph = LIST_FIRST(&pl)) != NULL) {
		LIST_REMOVE(ph, ph_pagelist);
d822 1
a822 1
	LIST_INSERT_HEAD(&pp->pr_emptypages, ph, ph_pagelist);
d846 1
a846 1
	LIST_REMOVE(ph, ph_pagelist);
d854 1
a854 1
	pp->pr_curpage = LIST_FIRST(&pp->pr_partpages);
d856 1
a856 1
		pp->pr_curpage = LIST_FIRST(&pp->pr_emptypages);
d922 1
a922 1
	struct pool_pagelist pl = LIST_HEAD_INITIALIZER(pl);
d925 2
a926 2
	for (ph = LIST_FIRST(&pp->pr_emptypages); ph != NULL; ph = phnext) {
		phnext = LIST_NEXT(ph, ph_pagelist);
d941 1
a941 1
		LIST_INSERT_HEAD(&pl, ph, ph_pagelist);
d945 1
a945 1
	if (LIST_EMPTY(&pl))
d948 2
a949 2
	while ((ph = LIST_FIRST(&pl)) != NULL) {
		LIST_REMOVE(ph, ph_pagelist);
d993 1
a993 1
	LIST_FOREACH(ph, pl, ph_pagelist) {
d1036 1
a1036 1
	if ((ph = LIST_FIRST(&pp->pr_emptypages)) != NULL)
d1039 1
a1039 1
	if ((ph = LIST_FIRST(&pp->pr_fullpages)) != NULL)
d1042 1
a1042 1
	if ((ph = LIST_FIRST(&pp->pr_partpages)) != NULL)
d1206 1
a1206 1
	LIST_FOREACH(ph, &pp->pr_emptypages, ph_pagelist)
d1208 1
a1208 1
	LIST_FOREACH(ph, &pp->pr_fullpages, ph_pagelist)
d1210 1
a1210 1
	LIST_FOREACH(ph, &pp->pr_partpages, ph_pagelist)
d1229 1
a1229 1
	LIST_FOREACH(ph, &pp->pr_fullpages, ph_pagelist) {
d1239 1
a1239 1
	LIST_FOREACH(ph, &pp->pr_partpages, ph_pagelist) {
@


1.168
log
@move arc4random prototype to systm.h. more appropriate for most code
to include that than rdnvar.h. ok deraadt dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.167 2014/11/15 06:55:32 dlg Exp $	*/
d437 3
a439 2
		struct pool_get_memory mem =
		    { MUTEX_INITIALIZER(pp->pr_ipl), NULL };
@


1.167
log
@hoist the slowdown handling up to the pool_do_get callers. this lets
us handle the slowdown where we already give up pr_mtx and gets rid of
an ugly goto.

ok tedu@@ who i think has more tweaks coming
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.166 2014/11/14 02:02:42 tedu Exp $	*/
a44 1
#include <dev/rndvar.h>
@


1.166
log
@move the slowdown back up. it needs to take place after the allocated page
has been added to the pool, else it doesn't help because the memory isn't
available. lost in locking rework.
tested blambert sthen
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.165 2014/11/10 18:55:43 kettenis Exp $	*/
d114 1
a114 1
void	*pool_do_get(struct pool *, int);
d419 1
d428 1
a428 1
	} else if ((v = pool_do_get(pp, flags)) == NULL) {
d434 3
d504 2
d509 2
a510 2
			pr->pr_item = pool_do_get(pp, flags);
			if (pr->pr_item == NULL)
d534 1
a534 1
pool_do_get(struct pool *pp, int flags)
a546 1
again:
a547 2
		int slowdown = 0;

d549 1
a549 1
		ph = pool_p_alloc(pp, flags, &slowdown);
a557 7

		if (slowdown && ISSET(flags, PR_WAITOK)) {
			mtx_leave(&pp->pr_mtx);
			yield();
			mtx_enter(&pp->pr_mtx);
			goto again;
		}
a697 1
	int slowdown = 0;
d702 2
d705 1
a705 1
		if (ph == NULL)
@


1.165
log
@Grab the pool mutex in sysctl_dopool(), but only for pools for which
pool_setipl(9) has been called.  This avoids the panic introduced in rev 1.139
(which was subsequently backed out) while still effectively guaranteeing a
consistent snapshot.  Pools used from interrupt handlers should use the
appropriate pool IPL.

ok dlg@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.164 2014/11/01 23:58:07 tedu Exp $	*/
d108 1
a108 1
	 pool_p_alloc(struct pool *, int);
d541 1
d543 2
d546 1
a546 1
		ph = pool_p_alloc(pp, flags);
d555 7
d702 1
d707 1
a707 1
		ph = pool_p_alloc(pp, PR_NOWAIT);
d725 1
a725 1
pool_p_alloc(struct pool *pp, int flags)
d730 1
a730 1
	int n, slowdown = 0;
d735 1
a735 1
	addr = pool_allocator_alloc(pp, flags, &slowdown);
a737 3

	if (slowdown && ISSET(flags, PR_WAITOK))
		yield();
@


1.164
log
@remove color support. discussed with dlg and mikeb
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.163 2014/10/13 00:12:51 dlg Exp $	*/
d1300 2
a1301 2
		/* XXX can't mtx until all pools setipl correctly */
		/* mtx_enter(&pp->pr_mtx); */
d1318 2
a1319 1
		/* mtx_leave(&pp->pr_mtx); */
@


1.163
log
@take the pool_item pi_magic touching out from under #ifdef DIAGNOSTIC.

i couldnt measure a significant performance difference with or
without it. this is likely a function of the memory involved being
close to bits that are already being touched, the implemention being
simple macros that mean registers can stay hot, and a lack of
conditionals that would cause a cpu pipeline to crash.

this means we're unconditionally poisoning the first two u_longs
of pool items on all kernels. i think it also makes the code easier
to read.

discussed with deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.162 2014/10/10 00:48:58 dlg Exp $	*/
a84 1
	caddr_t			ph_colored;	/* page's colored address */
d219 1
a219 1
	int off = 0, space;
a294 9
	/*
	 * Use the space between the chunks and the page header
	 * for "cache coloring".
	 */
	space = POOL_INPGHDR(pp) ? pp->pr_phoffset : pp->pr_pgsize;
	space -= pp->pr_itemsperpage * pp->pr_size;
	pp->pr_maxcolor = (space / align) * align;
	pp->pr_curcolor = 0;

d1225 1
a1225 1
		cp = ph->ph_colored;
d1235 1
a1235 1
		cp = ph->ph_colored;
@


1.162
log
@massage the pool item header and pool item magic words.

previously they were ints, but this bumps them to long sized words.
in the pool item headers they were followed by the XSIMPLEQ entries,
which are basically pointers which got long word alignment. this
meant there was a 4 byte gap on 64bit architectures between the
magic and list entry that wasnt being poisoned or checked.

this change also uses the header magic (which is sourced from
arc4random) with an xor of the item address to poison the item magic
value. this is inspired by tedu's XSIMPLEQ lists, and means we'll
be exercising memory with more bit patterns.

lastly, this takes more care around the handling of the pool_debug
flag. pool pages read it when theyre created and stash a local copy
of it. from then on all items returned to the page will be poisoned
based on the pages local copy of the flag. items allocated off the
page will be checked for valid poisoning only if both the page and
pool_debug flags are both set.

this avoids a race where pool_debug was not set when an item is
freed (so it wouldnt get poisoned), then gets set, then an item
gets allocated and fails the poison checks because pool_debug wasnt
set when it was freed.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.161 2014/09/28 10:03:05 tedu Exp $	*/
a568 1
#ifdef DIAGNOSTIC
a574 1
#endif /* DIAGNOSTIC */
a648 1
#ifdef DIAGNOSTIC
a649 1
#endif /* DIAGNOSTIC */
a765 1
#ifdef DIAGNOSTIC
a766 1
#endif
a782 1
#ifdef DIAGNOSTIC
a783 1
#endif
a787 1
#ifdef DIAGNOSTIC
d796 1
d809 1
a810 1
#endif
a995 1
#ifdef DIAGNOSTIC
a996 1
#endif
a1000 1
#ifdef DIAGNOSTIC
a1006 1
#endif
a1152 1
#ifdef DIAGNOSTIC
d1162 1
@


1.161
log
@in pool_destroy, enter and leave mutex as necessary to satisfy assertions.
ok dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.160 2014/09/26 05:43:14 dlg Exp $	*/
d86 1
a86 1
	int			ph_magic;
d88 2
d92 1
a92 2
	u_int32_t pi_magic;
	/* Other entries use only this list entry */
d95 1
d566 1
a566 1
	if (pi == NULL)
a567 1
	XSIMPLEQ_REMOVE_HEAD(&ph->ph_itemlist, pi_list);
d570 1
a570 1
	if (pi->pi_magic != poison_value(pi)) {
d572 3
a574 2
		    "page %p; item addr %p; offset 0x%x=0x%x", __func__,
		    pp->pr_wchan, ph->ph_page, pi, 0, pi->pi_magic);
d576 3
d580 2
a581 1
	if (pool_debug && ph->ph_magic) {
a639 2
	pi->pi_magic = poison_value(pi);

a647 3

		if (ph->ph_magic)
			poison_mem(pi + 1, pp->pr_size - sizeof(*pi));
d651 3
d655 4
d758 8
a765 6
	if (pool_debug) {
		do {
			arc4random_buf(&ph->ph_magic, sizeof(ph->ph_magic));
		} while (ph->ph_magic == 0);
	} else
		ph->ph_magic = 0;
d771 1
a771 1
		pi->pi_magic = poison_value(pi);
d776 1
a776 1
		if (pool_debug && ph->ph_magic)
d798 1
a798 1
		if (pi->pi_magic != poison_value(pi)) {
d800 16
a815 2
			    "page %p; item addr %p; offset 0x%x=0x%x", __func__,
			    pp->pr_wchan, ph->ph_page, pi, 0, pi->pi_magic);
d1013 2
a1014 2
			if (pi->pi_magic != poison_value(pi)) {
				(*pr)("\t\t\titem %p, magic 0x%x\n",
d1166 1
a1166 1
		if (pi->pi_magic != poison_value(pi)) {
d1170 1
a1170 1
			    "(p %p); offset 0x%x=0x%x\n",
d1175 1
a1175 1
		if (pool_debug && ph->ph_magic) {
@


1.160
log
@fix the calculation of the number of items to prime the pool with
in pool_setlowat.

this was stopping arm things from getting spare items into their
pmap entry pools, so things that really needed them in a delicate
part of boot were failing.

reported by rapha@@
co-debugging with miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.159 2014/09/23 19:54:47 miod Exp $	*/
d388 1
d390 1
@


1.159
log
@Only compile poison-related code if DIAGNOSTIC instead of if !SMALL_KERNEL,
for subr_poison.c will not get compiled at all on !DIAGNOSTIC kernels.
Found the hard way by deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.158 2014/09/22 01:04:58 dlg Exp $	*/
d858 2
a859 1
	prime = pp->pr_nitems - n;
@


1.158
log
@rework the pool code to make the locking more obvious (to me at
least). after this i am confident that pools are mpsafe, ie, can
be called without the kernel biglock being held.

the page allocation and setup code has been split into four parts:

pool_p_alloc is called without any locks held to ask the pool_allocator
backend to get a page and page header and set up the item list.

pool_p_insert is called with the pool lock held to insert the newly
minted page on the pools internal free page list and update its
internal accounting.

once the pool has finished with a page it calls the following:

pool_p_remove is called with the pool lock help to take the now
unnecessary page off the free page list and uncount it.

pool_p_free is called without the pool lock and does a bunch of
checks to verify that the items arent corrupted and have all been
returned to the page before giving it back to the pool_allocator
to be freed.

instead of pool_do_get doing all the work for pool_get, it is now
only responsible for doing a single item allocation. if for any
reason it cant get an item, it just returns NULL. pool_get is now
responsible for checking if the allocation is allowed (according
to hi watermarks etc), and for potentially sleeping waiting for
resources if required.

sleeping for resources is now built on top of pool_requests, which
are modelled on how the scsi midlayer schedules access to scsibus
resources.

the pool code now calls pool_allocator backends inside its own
calls to KERNEL_LOCK and KERNEL_UNLOCK, so users of pools dont
have to hold biglock to call pool_get or pool_put.

tested by krw@@ (who found a SMALL_KERNEL issue, thank you)
noone objected
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.157 2014/09/17 03:16:00 dlg Exp $	*/
d566 1
a566 1
#ifndef SMALL_KERNEL
a571 1
#endif
a572 1
#ifdef DIAGNOSTIC
d631 1
a631 1
#ifndef SMALL_KERNEL
a632 1
#endif
a633 1
#ifdef DIAGNOSTIC
d758 1
a758 1
#ifndef SMALL_KERNEL
d777 1
a777 1
#ifndef SMALL_KERNEL
d784 1
a784 1
#ifndef SMALL_KERNEL
d1138 1
a1147 1
#ifdef DIAGNOSTIC
@


1.157
log
@if userland asks for an unknown sysctl, return EOPNOTSUPP instead
of EINVAL like other sysctl things do.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.156 2014/09/16 23:05:34 dlg Exp $	*/
d106 6
a111 2
int	 pool_catchup(struct pool *);
void	 pool_prime_page(struct pool *, caddr_t, struct pool_item_header *);
a112 1
void	 pool_swizzle_curpage(struct pool *);
a113 3
void	 pool_do_put(struct pool *, void *);
void	 pr_rmpage(struct pool *, struct pool_item_header *,
	    struct pool_pagelist *);
d116 2
a117 1
struct pool_item_header *pool_alloc_item_header(struct pool *, caddr_t , int);
a208 42
 * Remove a page from the pool.
 */
void
pr_rmpage(struct pool *pp, struct pool_item_header *ph,
    struct pool_pagelist *pq)
{

	/*
	 * If the page was idle, decrement the idle page count.
	 */
	if (ph->ph_nmissing == 0) {
#ifdef DIAGNOSTIC
		if (pp->pr_nidle == 0)
			panic("%s: nidle inconsistent", __func__);
		if (pp->pr_nitems < pp->pr_itemsperpage)
			panic("%s: nitems inconsistent", __func__);
#endif
		pp->pr_nidle--;
	}

	pp->pr_nitems -= pp->pr_itemsperpage;

	/*
	 * Unlink a page from the pool and release it (or queue it for release).
	 */
	LIST_REMOVE(ph, ph_pagelist);
	if (!POOL_INPGHDR(pp))
		RB_REMOVE(phtree, &pp->pr_phtree, ph);
	pp->pr_npages--;
	pp->pr_npagefree++;
	pool_update_curpage(pp);

	if (pq) {
		LIST_INSERT_HEAD(pq, ph, ph_pagelist);
	} else {
		pool_allocator_free(pp, ph->ph_page);
		if (!POOL_INPGHDR(pp))
			pool_put(&phpool, ph);
	}
}

/*
d313 2
d351 1
d363 5
d379 1
a379 1
				goto removed;
a382 3
#ifdef DIAGNOSTIC
		panic("%s: pool not on list", __func__);
#endif
a383 1
removed:
a384 4
#ifdef DIAGNOSTIC
	if (pp->pr_nout != 0)
		panic("%s: pool busy: still out: %u", __func__, pp->pr_nout);
#endif
d387 4
a390 2
	while ((ph = LIST_FIRST(&pp->pr_emptypages)) != NULL)
		pr_rmpage(pp, ph, NULL);
d393 1
d395 7
d404 2
a405 2
struct pool_item_header *
pool_alloc_item_header(struct pool *pp, caddr_t storage, int flags)
d407 5
a411 1
	struct pool_item_header *ph;
d413 4
a416 11
	if (POOL_INPGHDR(pp))
		ph = (struct pool_item_header *)(storage + pp->pr_phoffset);
	else
		ph = pool_get(&phpool, (flags & ~(PR_WAITOK | PR_ZERO)) |
		    PR_NOWAIT);
#ifdef DIAGNOSTIC
	if (pool_debug && ph != NULL)
		ph->ph_magic = poison_value(ph);
#endif
	return (ph);
}
d419 1
a419 1
 * Grab an item from the pool; must be called at appropriate spl level
d424 1
a424 1
	void *v;
a427 11
	if ((flags & PR_WAITOK) != 0) {
#ifdef DIAGNOSTIC
		assertwaitok();
		if (pool_debug == 2)
			yield();
#endif
		if (!cold && pool_debug) {
			KERNEL_UNLOCK();
			KERNEL_LOCK();
		}
	}
d430 6
a435 4
#ifdef POOL_DEBUG
	if (pp->pr_roflags & PR_DEBUGCHK) {
		if (pool_chk(pp))
			panic("%s: before", __func__);
d437 16
a452 6
#endif
	v = pool_do_get(pp, flags);
#ifdef POOL_DEBUG
	if (pp->pr_roflags & PR_DEBUGCHK) {
		if (pool_chk(pp))
			panic("%s: after", __func__);
a453 6
#endif
	if (v != NULL)
		pp->pr_nget++;
	mtx_leave(&pp->pr_mtx);
	if (v == NULL)
		return (v);
d455 1
a455 1
	if (flags & PR_ZERO)
d459 72
a537 23
	void *v;
	int slowdown = 0;

startover:
	/*
	 * Check to see if we've reached the hard limit.  If we have,
	 * and we can wait, then wait until an item has been returned to
	 * the pool.
	 */
#ifdef DIAGNOSTIC
	if (pp->pr_nout > pp->pr_hardlimit)
		panic("%s: %s: crossed hard limit", __func__, pp->pr_wchan);
#endif
	if (pp->pr_nout == pp->pr_hardlimit) {
		if ((flags & PR_WAITOK) && !(flags & PR_LIMITFAIL)) {
			/*
			 * XXX: A warning isn't logged in this case.  Should
			 * it be?
			 */
			pp->pr_flags |= PR_WANTED;
			pool_sleep(pp);
			goto startover;
		}
d539 1
a539 7
		/*
		 * Log a message that the hard limit has been hit.
		 */
		if (pp->pr_hardlimit_warning != NULL &&
		    ratecheck(&pp->pr_hardlimit_warning_last,
		    &pp->pr_hardlimit_ratecap))
			log(LOG_ERR, "%s\n", pp->pr_hardlimit_warning);
a540 5
		pp->pr_nfail++;
		return (NULL);
	}

	pool_swizzle_curpage(pp);
d542 2
a543 2
	 * The convention we use is that if `curpage' is not NULL, then
	 * it points at a non-empty bucket.
d545 1
a545 8
	if ((ph = pp->pr_curpage) == NULL) {
#ifdef DIAGNOSTIC
		if (pp->pr_nitems != 0) {
			printf("pool_do_get: %s: curpage NULL, nitems %u\n",
			    pp->pr_wchan, pp->pr_nitems);
			panic("%s: nitems inconsistent", __func__);
		}
#endif
d547 4
a550 15
		/*
		 * Call the back-end page allocator for more memory.
		 */
		v = pool_allocator_alloc(pp, flags, &slowdown);
		if (v != NULL)
			ph = pool_alloc_item_header(pp, v, flags);

		if (v == NULL || ph == NULL) {
			if (v != NULL)
				pool_allocator_free(pp, v);

			if ((flags & PR_WAITOK) == 0) {
				pp->pr_nfail++;
				return (NULL);
			}
d552 3
a554 9
			/*
			 * Wait for items to be returned to this pool.
			 *
			 * XXX: maybe we should wake up once a second and
			 * try again?
			 */
			pp->pr_flags |= PR_WANTED;
			pool_sleep(pp);
			goto startover;
d557 2
a558 9
		/* We have more memory; add it to the pool */
		pool_prime_page(pp, v, ph);
		pp->pr_npagealloc++;

		if (slowdown && (flags & PR_WAITOK)) {
			mtx_leave(&pp->pr_mtx);
			yield();
			mtx_enter(&pp->pr_mtx);
		}
d560 3
a562 4
		/* Start the allocation process over. */
		goto startover;
	}
	if ((v = pi = XSIMPLEQ_FIRST(&ph->ph_itemlist)) == NULL)
d564 1
d566 5
a570 5
#ifdef DIAGNOSTIC
	if (pp->pr_nitems == 0) {
		printf("%s: %s: items on itemlist, nitems %u\n", __func__,
		    pp->pr_wchan, pp->pr_nitems);
		panic("%s: nitems inconsistent", __func__);
a574 4
	if (pi->pi_magic != poison_value(pi))
		panic("%s(%s): free list modified: "
		    "page %p; item addr %p; offset 0x%x=0x%x", __func__,
		    pp->pr_wchan, ph->ph_page, pi, 0, pi->pi_magic);
d581 1
a581 1
			panic("%s(%s): free list modified: "
d589 1
a589 13
	/*
	 * Remove from item list.
	 */
	XSIMPLEQ_REMOVE_HEAD(&ph->ph_itemlist, pi_list);
	pp->pr_nitems--;
	pp->pr_nout++;
	if (ph->ph_nmissing == 0) {
#ifdef DIAGNOSTIC
		if (pp->pr_nidle == 0)
			panic("%s: nidle inconsistent", __func__);
#endif
		pp->pr_nidle--;

d596 2
d599 2
a600 8
	ph->ph_nmissing++;
	if (XSIMPLEQ_EMPTY(&ph->ph_itemlist)) {
#ifdef DIAGNOSTIC
		if (ph->ph_nmissing != pp->pr_itemsperpage) {
			panic("%s: %s: nmissing inconsistent", __func__,
			    pp->pr_wchan);
		}
#endif
d610 3
a612 12
	/*
	 * If we have a low water mark and we are now below that low
	 * water mark, add more items to the pool.
	 */
	if (POOL_NEEDS_CATCHUP(pp) && pool_catchup(pp) != 0) {
		/*
		 * XXX: Should we log a warning?  Should we set up a timeout
		 * to try again in a second or so?  The latter could break
		 * a caller's assumptions about interrupt protection, etc.
		 */
	}
	return (v);
d616 1
a616 1
 * Return resource to the pool; must be called at appropriate spl level
a620 24
	mtx_enter(&pp->pr_mtx);
#ifdef POOL_DEBUG
	if (pp->pr_roflags & PR_DEBUGCHK) {
		if (pool_chk(pp))
			panic("%s: before", __func__);
	}
#endif
	pool_do_put(pp, v);
#ifdef POOL_DEBUG
	if (pp->pr_roflags & PR_DEBUGCHK) {
		if (pool_chk(pp))
			panic("%s: after", __func__);
	}
#endif
	pp->pr_nput++;
	mtx_leave(&pp->pr_mtx);
}

/*
 * Internal version of pool_put().
 */
void
pool_do_put(struct pool *pp, void *v)
{
d622 1
a622 1
	struct pool_item_header *ph;
d624 1
d627 1
d629 3
a631 3
#ifdef DIAGNOSTIC
	if (pp->pr_ipl != -1)
		splassert(pp->pr_ipl);
d633 2
a634 2
	if (pp->pr_nout == 0)
		panic("%s: %s: putting with none out", __func__, pp->pr_wchan);
a636 5
	ph = pr_find_pagehead(pp, v);

	/*
	 * Return to item list.
	 */
d646 3
a648 4
	}
	pi->pi_magic = poison_value(pi);
	if (ph->ph_magic) {
		poison_mem(pi + 1, pp->pr_size - sizeof(*pi));
a652 3
	ph->ph_nmissing--;
	pp->pr_nitems++;
	pp->pr_nout--;
d654 7
a660 7
	/* Cancel "pool empty" condition if it exists */
	if (pp->pr_curpage == NULL)
		pp->pr_curpage = ph;

	if (pp->pr_flags & PR_WANTED) {
		pp->pr_flags &= ~PR_WANTED;
		wakeup(pp);
a662 11
	/*
	 * If this page is now empty, do one of two things:
	 *
	 *	(1) If we have more pages than the page high water mark,
	 *	    free the page back to the system.
	 *
	 *	(2) Otherwise, move the page to the empty page list.
	 *
	 * Either way, select a new current page (so we use a partially-full
	 * page if one is available).
	 */
d664 3
d668 1
a668 13
		if (pp->pr_nidle > pp->pr_maxpages) {
			pr_rmpage(pp, ph, NULL);
		} else {
			LIST_REMOVE(ph, ph_pagelist);
			LIST_INSERT_HEAD(&pp->pr_emptypages, ph, ph_pagelist);
			pool_update_curpage(pp);
		}
	}
	/*
	 * If the page was previously completely full, move it to the
	 * partially-full list.
	 */
	else if (ph->ph_nmissing == (pp->pr_itemsperpage - 1)) {
d670 2
a671 1
		LIST_INSERT_HEAD(&pp->pr_partpages, ph, ph_pagelist);
d673 16
d697 1
a698 1
	caddr_t cp;
a699 1
	int slowdown;
a700 1
	mtx_enter(&pp->pr_mtx);
d704 2
a705 6
		cp = pool_allocator_alloc(pp, PR_NOWAIT, &slowdown);
		if (cp != NULL)
			ph = pool_alloc_item_header(pp, cp, PR_NOWAIT);
		if (cp == NULL || ph == NULL) {
			if (cp != NULL)
				pool_allocator_free(pp, cp);
a706 1
		}
d708 1
a708 3
		pool_prime_page(pp, cp, ph);
		pp->pr_npagealloc++;
		pp->pr_minpages++;
d711 6
a716 2
	if (pp->pr_minpages >= pp->pr_maxpages)
		pp->pr_maxpages = pp->pr_minpages + 1;	/* XXX */
a717 1
	mtx_leave(&pp->pr_mtx);
d721 2
a722 7
/*
 * Add a page worth of items to the pool.
 *
 * Note, we must be called with the pool descriptor LOCKED.
 */
void
pool_prime_page(struct pool *pp, caddr_t storage, struct pool_item_header *ph)
d724 1
d726 5
a730 3
	caddr_t cp = storage;
	unsigned int align = pp->pr_align;
	int n;
d732 3
a734 9
	/*
	 * Insert page header.
	 */
	LIST_INSERT_HEAD(&pp->pr_emptypages, ph, ph_pagelist);
	XSIMPLEQ_INIT(&ph->ph_itemlist);
	ph->ph_page = storage;
	ph->ph_nmissing = 0;
	if (!POOL_INPGHDR(pp))
		RB_INSERT(phtree, &pp->pr_phtree, ph);
d736 2
a737 1
	pp->pr_nidle++;
d739 9
a747 6
	/*
	 * Color this page.
	 */
	cp = (caddr_t)(cp + pp->pr_curcolor);
	if ((pp->pr_curcolor += align) > pp->pr_maxcolor)
		pp->pr_curcolor = 0;
d749 9
a757 1
	ph->ph_colored = cp;
a758 3
	/*
	 * Insert remaining chunks on the bucket list.
	 */
a759 2
	pp->pr_nitems += n;

d761 4
a764 3
		pi = (struct pool_item *)cp;

		/* Insert on page list */
d768 1
a768 2
		pi->pi_magic = poison_value(pi);
		if (ph->ph_magic) {
a769 1
		}
d771 2
a772 1
		cp = (caddr_t)(cp + pp->pr_size);
d775 35
a809 3
	/*
	 * If the pool was depleted, point at the new page.
	 */
d813 8
d825 2
a826 8
/*
 * Used by pool_get() when nitems drops below the low water mark.  This
 * is used to catch up pr_nitems with the low water mark.
 *
 * Note we never wait for memory here, we let the caller decide what to do.
 */
int
pool_catchup(struct pool *pp)
d828 6
a833 4
	struct pool_item_header *ph;
	caddr_t cp;
	int error = 0;
	int slowdown;
d835 3
a837 16
	while (POOL_NEEDS_CATCHUP(pp)) {
		/*
		 * Call the page back-end allocator for more memory.
		 */
		cp = pool_allocator_alloc(pp, PR_NOWAIT, &slowdown);
		if (cp != NULL)
			ph = pool_alloc_item_header(pp, cp, PR_NOWAIT);
		if (cp == NULL || ph == NULL) {
			if (cp != NULL)
				pool_allocator_free(pp, cp);
			error = ENOMEM;
			break;
		}
		pool_prime_page(pp, cp, ph);
		pp->pr_npagealloc++;
	}
d839 1
a839 1
	return (error);
a844 1

a851 21
pool_swizzle_curpage(struct pool *pp)
{
	struct pool_item_header *ph, *next;

	if ((ph = pp->pr_curpage) == NULL)
		return;
	if (arc4random_uniform(16) != 0)
		return;
	next = LIST_FIRST(&pp->pr_partpages);
	if (next == ph)
		next = LIST_NEXT(next, ph_pagelist);
	if (next == NULL) {
		next = LIST_FIRST(&pp->pr_emptypages);
		if (next == ph)
			next = LIST_NEXT(next, ph_pagelist);
	}
	if (next != NULL)
		pp->pr_curpage = next;
}

void
d854 1
d856 1
d862 1
a862 9
	mtx_enter(&pp->pr_mtx);
	/* Make sure we're caught up with the newly-set low water mark. */
	if (POOL_NEEDS_CATCHUP(pp) && pool_catchup(pp) != 0) {
		/*
		 * XXX: Should we log a warning?  Should we set up a timeout
		 * to try again in a second or so?  The latter could break
		 * a caller's assumptions about interrupt protection, etc.
		 */
	}
d864 3
a871 1

d912 1
a912 3
	struct pool_pagelist pq;

	LIST_INIT(&pq);
a921 2
		KASSERT(ph->ph_nmissing == 0);

d930 2
a931 1
		pr_rmpage(pp, ph, &pq);
d935 1
a935 1
	if (LIST_EMPTY(&pq))
d937 2
a938 1
	while ((ph = LIST_FIRST(&pq)) != NULL) {
d940 1
a940 4
		pool_allocator_free(pp, ph->ph_page);
		if (POOL_INPGHDR(pp))
			continue;
		pool_put(&phpool, ph);
a1141 2

#ifdef DIAGNOSTIC
d1144 1
a1144 1
			printf("pool(%s): free list modified: "
d1147 1
a1147 1
			    pp->pr_wchan, ph->ph_page, n, pi, page,
d1150 2
d1166 1
a1335 1
	int waitok = ISSET(flags, PR_WAITOK);
d1338 3
a1340 5
	if (waitok)
		mtx_leave(&pp->pr_mtx);
	v = pp->pr_alloc->pa_alloc(pp, flags, slowdown);
	if (waitok)
		mtx_enter(&pp->pr_mtx);
d1360 1
d1362 1
@


1.156
log
@disable taking the mutex to read pool stats.

some pool users (eg, mbufs and mbuf clusters) protect calls to pools
with their own locks that operate at high spl levels, rather than
pool_setipl() to have pools protect themselves.

this means pools mtx_enter doesnt necessarily prevent interrupts
that will use a pool, so we get code paths that try to mtx_enter
twice, which blows up.

reported by vlado at bsdbg dot net and matt bettinger
diagnosed by kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.155 2014/09/16 21:45:12 dlg Exp $	*/
d1385 1
a1385 1
		return (EINVAL);
@


1.155
log
@tweak panics so they use __func__ consistently.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.154 2014/09/16 03:26:08 dlg Exp $	*/
d1408 2
a1409 1
		mtx_enter(&pp->pr_mtx);
d1426 1
a1426 1
		mtx_leave(&pp->pr_mtx);
@


1.154
log
@deprecate PR_DEBUG and MALLOC_DEBUG in pools.

poked by kspillner@@
ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.153 2014/09/14 14:17:25 jsg Exp $	*/
d197 2
a198 4
	if (ph == NULL) {
		panic("pr_find_pagehead: %s: page header missing",
		    pp->pr_wchan);
	}
d201 2
a202 4
	if (ph->ph_page + pp->pr_pgsize <= (caddr_t)v) {
		panic("pr_find_pagehead: %s: incorrect page",
		    pp->pr_wchan);
	}
d221 1
a221 1
			panic("pr_rmpage: nidle inconsistent");
d223 1
a223 1
			panic("pr_rmpage: nitems inconsistent");
d372 1
a372 1
			panic("init pool already on list");
d378 1
a378 1
		panic("pool_init: too much uptime");
d417 1
a417 1
		panic("destroyed pool not on list");
d424 1
a424 1
		panic("pool_destroy: pool busy: still out: %u", pp->pr_nout);
d478 1
a478 1
			panic("before pool_get");
d485 1
a485 1
			panic("after pool_get");
d516 1
a516 1
		panic("pool_do_get: %s: crossed hard limit", pp->pr_wchan);
d551 1
a551 1
			panic("pool_do_get: nitems inconsistent");
d595 3
a597 3
	if ((v = pi = XSIMPLEQ_FIRST(&ph->ph_itemlist)) == NULL) {
		panic("pool_do_get: %s: page empty", pp->pr_wchan);
	}
d600 1
a600 1
		printf("pool_do_get: %s: items on itemlist, nitems %u\n",
d602 1
a602 1
		panic("pool_do_get: nitems inconsistent");
d608 2
a609 2
		panic("pool_do_get(%s): free list modified: "
		    "page %p; item addr %p; offset 0x%x=0x%x",
d617 1
a617 1
			panic("pool_do_get(%s): free list modified: "
d619 1
a619 1
			    pp->pr_wchan, ph->ph_page, pi,
d634 1
a634 1
			panic("pool_do_get: nidle inconsistent");
d649 1
a649 1
			panic("pool_do_get: %s: nmissing inconsistent",
d686 1
a686 1
			panic("before pool_put");
d693 1
a693 1
			panic("after pool_put");
d710 1
a710 1
		panic("pool_put of NULL");
d716 2
a717 5
	if (pp->pr_nout == 0) {
		printf("pool %s: putting with none out\n",
		    pp->pr_wchan);
		panic("pool_do_put");
	}
d728 6
a733 3
		XSIMPLEQ_FOREACH(qi, &ph->ph_itemlist, pi_list)
			if (pi == qi)
				panic("double pool_put: %p", pi);
@


1.153
log
@remove uneeded proc.h includes
ok mpi@@ kspillner@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.152 2014/09/08 23:50:45 dlg Exp $	*/
a269 5
#ifdef MALLOC_DEBUG
	if ((flags & PR_DEBUG) && align != 0)
		flags &= ~PR_DEBUG;
#endif

a511 11
#ifdef MALLOC_DEBUG
	if (pp->pr_roflags & PR_DEBUG) {
		void *addr;

		addr = NULL;
		debug_malloc(pp->pr_size, M_DEBUG,
		    (flags & PR_WAITOK) ? M_WAITOK : M_NOWAIT, &addr);
		return (addr);
	}
#endif

a714 7

#ifdef MALLOC_DEBUG
	if (pp->pr_roflags & PR_DEBUG) {
		debug_free(v, M_DEBUG);
		return;
	}
#endif
@


1.152
log
@change some (flags & PR_WAITOK) to ISSET(flags, PR_WAITOK)

no functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.151 2014/09/08 00:00:05 dlg Exp $	*/
a35 1
#include <sys/proc.h>
@


1.151
log
@deprecate the use of the PR_PHINPAGE flag by replacing it with a test
of pr_phoffset.

ok doug@@ guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.150 2014/09/05 03:13:52 dlg Exp $	*/
d1472 1
a1472 1
	int waitok = flags & PR_WAITOK;
d1507 1
a1507 1
	kd.kd_waitok = (flags & PR_WAITOK);
d1530 1
a1530 1
	kd.kd_waitok = (flags & PR_WAITOK);
d1563 1
a1563 1
	kd.kd_waitok = (flags & PR_WAITOK);
@


1.150
log
@KASSERT that the page header pool will use in page headers.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.149 2014/09/04 00:36:00 dlg Exp $	*/
d105 2
d188 1
a188 1
	if ((pp->pr_roflags & PR_PHINPAGE) != 0) {
d239 1
a239 1
	if ((pp->pr_roflags & PR_PHINPAGE) == 0)
d249 1
a249 1
		if ((pp->pr_roflags & PR_PHINPAGE) == 0)
d264 1
a264 1
	int off = 0, slack;
a304 1
		flags |= PR_PHINPAGE;
a306 1
		flags |= PR_PHINPAGE;
d309 1
a309 2
	} else
		off = pgsize;
d346 1
a346 1
	 * Use the slack between the chunks and the page header
d349 3
a351 2
	slack = off - pp->pr_itemsperpage * pp->pr_size;
	pp->pr_maxcolor = (slack / align) * align;
d371 1
a371 1
		KASSERT(ISSET(phpool.pr_roflags, PR_PHINPAGE));
d450 1
a450 1
	if ((pp->pr_roflags & PR_PHINPAGE) != 0)
d565 1
a565 3
	 * it points at a non-empty bucket. In particular, `curpage'
	 * never points at a page header which has PR_PHINPAGE set and
	 * has no items in its bucket.
d870 1
a870 1
	if ((pp->pr_roflags & PR_PHINPAGE) == 0)
d1076 1
a1076 1
		if (pp->pr_roflags & PR_PHINPAGE)
d1269 1
a1269 2
	if (page != ph->ph_page &&
	    (pp->pr_roflags & PR_PHINPAGE) != 0) {
d1482 1
a1482 1
	if (v != NULL && ISSET(pp->pr_roflags, PR_PHINPAGE)) {
d1527 1
a1527 1
	if (ISSET(pp->pr_roflags, PR_PHINPAGE))
d1546 1
a1546 1
	if (ISSET(pp->pr_roflags, PR_PHINPAGE))
d1560 1
a1560 1
	if (ISSET(pp->pr_roflags, PR_PHINPAGE))
d1574 1
a1574 1
	if (ISSET(pp->pr_roflags, PR_PHINPAGE))
@


1.149
log
@rework how pools with large pages (>PAGE_SIZE) are implemented.

this moves the size of the pool page (not arch page) out of the
pool allocator into struct pool. this lets us create only two pools
for the automatically determined large page allocations instead of
256 of them.

while here support using slack space in large pages for the
pool_item_header by requiring km_alloc provide pool page aligned
memory.

lastly, instead of doing incorrect math to figure how how many arch
pages to use for large pool pages, just use powers of two.

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.148 2014/08/27 00:22:26 dlg Exp $	*/
d369 3
@


1.148
log
@deprecate the "item offset" handling. nothing uses it, so we can
cut it out of the code to simplify things.

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.147 2014/08/20 00:00:46 dlg Exp $	*/
a86 1
	int			ph_pagesize;
d121 1
a121 6
 * XXX - quick hack. For pools with large items we want to use a special
 *       allocator. For now, instead of having the allocator figure out
 *       the allocation size from the pool (which can be done trivially
 *       with round_page(pr_itemsperpage * pr_size)) which would require
 *	 lots of changes everywhere, we just create allocators for each
 *	 size. We limit those to 128 pages.
d123 12
a134 3
#define POOL_LARGE_MAXPAGES 128
struct pool_allocator pool_allocator_large[POOL_LARGE_MAXPAGES];
struct pool_allocator pool_allocator_large_ni[POOL_LARGE_MAXPAGES];
d137 6
d146 4
d189 1
a189 1
		page = (caddr_t)((vaddr_t)v & pp->pr_alloc->pa_pagemask);
d202 1
a202 1
	if (ph->ph_page + ph->ph_pagesize <= (caddr_t)v) {
d262 2
a263 1
	int off, slack;
a272 42
	/*
	 * Check arguments and construct default values.
	 */
	if (palloc == NULL) {
		if (size > PAGE_SIZE) {
			int psize;

			/*
			 * XXX - should take align into account as well.
			 */
			if (size == round_page(size))
				psize = size / PAGE_SIZE;
			else
				psize = PAGE_SIZE / roundup(size % PAGE_SIZE,
				    1024);
			if (psize > POOL_LARGE_MAXPAGES)
				psize = POOL_LARGE_MAXPAGES;
			if (flags & PR_WAITOK)
				palloc = &pool_allocator_large_ni[psize-1];
			else
				palloc = &pool_allocator_large[psize-1];
			if (palloc->pa_pagesz == 0) {
				palloc->pa_pagesz = psize * PAGE_SIZE;
				if (flags & PR_WAITOK) {
					palloc->pa_alloc = pool_large_alloc_ni;
					palloc->pa_free = pool_large_free_ni;
				} else {
					palloc->pa_alloc = pool_large_alloc;
					palloc->pa_free = pool_large_free;
				}
			}
		} else {
			palloc = &pool_allocator_nointr;
		}
	}
	if (palloc->pa_pagesz == 0) {
		palloc->pa_pagesz = PAGE_SIZE;
	}
	if (palloc->pa_pagemask == 0) {
		palloc->pa_pagemask = ~(palloc->pa_pagesz - 1);
		palloc->pa_pageshift = ffs(palloc->pa_pagesz) - 1;
	}
d281 32
a312 5
#ifdef DIAGNOSTIC
	if (size > palloc->pa_pagesz)
		panic("pool_init: pool item size (%lu) too large",
		    (u_long)size);
#endif
d317 1
d329 4
d344 1
a344 22

	/*
	 * Decide whether to put the page header off page to avoid
	 * wasting too large a part of the page. Off-page page headers
	 * go into an RB tree, so we can match a returned item with
	 * its header based on the page address.
	 * We use 1/16 of the page size as the threshold (XXX: tune)
	 */
	if (pp->pr_size < palloc->pa_pagesz/16 && pp->pr_size < PAGE_SIZE) {
		/* Use the end of the page for the page header */
		pp->pr_roflags |= PR_PHINPAGE;
		pp->pr_phoffset = off = palloc->pa_pagesz -
		    ALIGN(sizeof(struct pool_item_header));
	} else {
		/* The page header will be taken from our page header pool */
		pp->pr_phoffset = 0;
		off = palloc->pa_pagesz;
		RB_INIT(&pp->pr_phtree);
	}

	pp->pr_itemsperpage = off / pp->pr_size;
	KASSERT(pp->pr_itemsperpage != 0);
a867 1
	ph->ph_pagesize = pp->pr_alloc->pa_pagesz;
d1267 1
a1267 1
	page = (caddr_t)((u_long)ph & pp->pr_alloc->pa_pagemask);
d1304 1
a1304 2
		page =
		    (caddr_t)((u_long)pi & pp->pr_alloc->pa_pagemask);
d1438 1
a1438 1
		pi.pr_pgsize = pp->pr_alloc->pa_pagesz;
a1466 27
 *
 * Each pool has a backend allocator that handles allocation, deallocation
 */
void	*pool_page_alloc(struct pool *, int, int *);
void	pool_page_free(struct pool *, void *);

/*
 * safe for interrupts, name preserved for compat this is the default
 * allocator
 */
struct pool_allocator pool_allocator_nointr = {
	pool_page_alloc, pool_page_free, 0,
};

/*
 * XXX - we have at least three different resources for the same allocation
 *  and each resource can be depleted. First we have the ready elements in
 *  the pool. Then we have the resource (typically a vm_map) for this
 *  allocator, then we have physical memory. Waiting for any of these can
 *  be unnecessary when any other is freed, but the kernel doesn't support
 *  sleeping on multiple addresses, so we have to fake. The caller sleeps on
 *  the pool (so that we can be awakened when an item is returned to the pool),
 *  but we set PA_WANT on the allocator. When a page is returned to
 *  the allocator and PA_WANT is set pool_allocator_free will wakeup all
 *  sleeping pools belonging to this allocator. (XXX - thundering herd).
 *  We also wake up the allocator in case someone without a pool (malloc)
 *  is sleeping waiting for this allocator.
d1481 10
d1510 1
a1510 1
	return (km_alloc(PAGE_SIZE, &kv_page, pp->pr_crange, &kd));
d1516 1
a1516 1
	km_free(v, PAGE_SIZE, &kv_page, pp->pr_crange);
d1522 1
d1527 3
d1534 1
a1534 2
	v = km_alloc(pp->pr_alloc->pa_pagesz, &kv_intrsafe, pp->pr_crange,
	    &kd);
d1543 1
d1546 3
d1550 1
a1550 1
	km_free(v, pp->pr_alloc->pa_pagesz, &kv_intrsafe, pp->pr_crange);
d1557 1
d1560 3
d1566 1
a1566 1
	return (km_alloc(pp->pr_alloc->pa_pagesz, &kv_any, pp->pr_crange, &kd));
d1572 6
a1577 1
	km_free(v, pp->pr_alloc->pa_pagesz, &kv_any, pp->pr_crange);
@


1.147
log
@bring back r1.130:

add an explicit rwlock around the global state (the pool list and serial
number) rather than rely on implicit process exclusion, splhigh and splvm.

the only things touching the global state come from process context so we
can get away with an rwlock instead of a mutex. thankfully.

ok matthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.146 2014/08/18 01:28:44 dlg Exp $	*/
d252 1
d256 1
a256 1
	if ((flags & PR_DEBUG) && (ioff != 0 || align != 0))
d360 1
a360 9
	/*
	 * Alignment is to take place at `ioff' within the item. This means
	 * we must reserve up to `align - 1' bytes on the page to allow
	 * appropriate positioning of each item.
	 *
	 * Silently enforce `0 <= ioff < align'.
	 */
	pp->pr_itemoffset = ioff = ioff % align;
	pp->pr_itemsperpage = (off - ((align - ioff) % align)) / pp->pr_size;
a876 1
	unsigned int ioff = pp->pr_itemoffset;
a898 5
	/*
	 * Adjust storage to apply alignment to `pr_itemoffset' in each item.
	 */
	if (ioff != 0)
		cp = (caddr_t)(cp + (align - ioff));
a909 2
		KASSERT(((((vaddr_t)pi) + ioff) & (align - 1)) == 0);

d1168 2
a1169 2
	(*pr)("POOL %s: size %u, align %u, ioff %u, roflags 0x%08x\n",
	    pp->pr_wchan, pp->pr_size, pp->pr_align, pp->pr_itemoffset,
@


1.146
log
@external page headers use an RB tree to find the page header
containing an item when its returned to the pool. this means you
need to do an inexact comparison between an items address and the
page address, cos a pool page can contain many items.

previously this used RB_FIND with a compare function that would do math
on every node comparison to see if one node (the key) was within the other
node (the tree element).

this cuts it over to using RB_NFIND to find the closest tree node
instead of the exact tree node. the node compares turns into simple
< and > operations, which inline very nicely with the RB_NFIND. the
constraint (an item must be within a page) is then checked only
once after the NFIND call.

feedback from matthew@@ and tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.145 2014/08/12 01:31:43 dlg Exp $	*/
d42 1
d64 10
a105 7
/*
 * Every pool gets a unique serial number assigned to it. If this counter
 * wraps, we're screwed, but we shouldn't create so many pools anyway.
 */
unsigned int pool_serial;
unsigned int pool_count;

a251 5

	SIMPLEQ_FOREACH(iter, &pool_head, pr_poollist) {
		if (iter == pp)
			panic("init pool already on list");
	}
a338 3
	pp->pr_serial = ++pool_serial;
	if (pool_serial == 0)
		panic("pool_init: too much uptime");
d399 12
d413 1
d433 1
d452 1
a1123 1
	int		s;
d1125 1
a1125 1
	s = splhigh();
d1128 1
a1128 1
	splx(s);
a1435 1
	int s;
d1453 1
a1453 1
	s = splvm();
d1490 1
d1494 1
a1494 1
	splx(s);
@


1.145
log
@sigh. when returning ENOENT in the sysctl path, unlock on the way out.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.144 2014/08/12 01:25:21 dlg Exp $	*/
d146 10
a155 7
	long diff = (vaddr_t)a->ph_page - (vaddr_t)b->ph_page;
	if (diff < 0)
		return -(-diff >= a->ph_pagesize);
	else if (diff > 0)
		return (diff >= b->ph_pagesize);
	else
		return (0);
d167 1
a167 1
	struct pool_item_header *ph, tmp;
d177 11
a187 18
	/*
	 * The trick we're using in the tree compare function is to compare
	 * two elements equal when they overlap. We want to return the
	 * page header that belongs to the element just before this address.
	 * We don't want this element to compare equal to the next element,
	 * so the compare function takes the pagesize from the lower element.
	 * If this header is the lower, its pagesize is zero, so it can't
	 * overlap with the next header. But if the header we're looking for
	 * is lower, we'll use its pagesize and it will overlap and return
	 * equal.
	 */
	tmp.ph_page = v;
	tmp.ph_pagesize = 0;
	ph = RB_FIND(phtree, &pp->pr_phtree, &tmp);

	if (ph) {
		KASSERT(ph->ph_page <= (caddr_t)v);
		KASSERT(ph->ph_page + ph->ph_pagesize > (caddr_t)v);
d189 2
a190 1
	return ph;
d763 1
a763 3
	if ((ph = pr_find_pagehead(pp, v)) == NULL) {
		panic("pool_do_put: %s: page header missing", pp->pr_wchan);
	}
@


1.144
log
@i accidentally removed the check for whether the requested pool in
the sysctl path exists. return ENOENT instead of trying a NULL
deref.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.143 2014/08/12 01:05:46 dlg Exp $	*/
d1488 1
a1490 1
done:
@


1.143
log
@bring back r1.135:

matthew@@ noticed i wasnt populating npages in the kinfo_pool sent to
userland.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.142 2014/08/12 01:01:11 dlg Exp $	*/
d1430 1
a1430 1
	int rv;
d1456 3
d1490 1
@


1.142
log
@bring back r1.134:

inline is the new __inline
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.141 2014/08/12 00:59:27 dlg Exp $	*/
d1467 1
@


1.141
log
@bring back r1.133. this is a bit different cos we're still using splvm to
protect pool_list rather than the rwlock that made i386 blow up:

use pool_count to report the number of pools to userland rather
than walking the list and counting the elements as we go.

use sysctl_rdint, sysctl_rdstring, and sysctl_rdstruct instead of
handcrafted copyouts.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.140 2014/08/11 13:31:42 dlg Exp $	*/
d143 1
a143 1
static __inline int
d161 1
a161 1
static __inline struct pool_item_header *
@


1.140
log
@bring back r1.132:

provide a pool_count global so we can figure out how many pools there are
active without having to walk the global pool_list.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.132 2014/07/02 05:52:45 dlg Exp $	*/
d1426 1
a1426 1
sysctl_dopool(int *name, u_int namelen, char *where, size_t *sizep)
d1430 2
a1431 4
	size_t buflen = where != NULL ? *sizep : 0;
	int npools = 0, s;
	unsigned int lookfor;
	size_t len;
d1433 1
a1433 1
	switch (*name) {
d1435 4
a1438 4
		if (namelen != 1 || buflen != sizeof(int))
			return (EINVAL);
		lookfor = 0;
		break;
a1439 4
		if (namelen != 2 || buflen < 1)
			return (EINVAL);
		lookfor = name[1];
		break;
a1440 3
		if (namelen != 2 || buflen != sizeof(pi))
			return (EINVAL);
		lookfor = name[1];
d1446 3
d1452 1
a1452 2
		npools++;
		if (lookfor == pp->pr_serial)
d1456 1
a1456 8
	splx(s);

	if (*name != KERN_POOL_NPOOLS && pp == NULL)
		return (ENOENT);

	switch (*name) {
	case KERN_POOL_NPOOLS:
		return copyout(&npools, where, buflen);
d1458 2
a1459 5
		len = strlen(pp->pr_wchan) + 1;
		if (*sizep < len)
			return (ENOMEM);
		*sizep = len;
		return copyout(pp->pr_wchan, where, len);
d1481 1
a1481 1
		return copyout(&pi, where, buflen);
d1483 4
a1486 2
	/* NOTREACHED */
	return (0); /* XXX - Stupid gcc */
@


1.139
log
@bring back r1.131:

take the pools mutex when copying stats out of it in the sysctl
path so we are guaranteed a consistent snapshot.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.138 2014/07/10 13:34:39 tedu Exp $	*/
d100 1
d407 1
d427 1
@


1.138
log
@drain some boolean_t poison
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.137 2014/07/10 07:50:27 tedu Exp $	*/
d1476 2
d1493 2
@


1.137
log
@hide the biglock thrashing under pool_debug so it can be turned off
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.136 2014/07/03 07:47:56 guenther Exp $	*/
d1531 1
a1531 1
	boolean_t waitok = (flags & PR_WAITOK) ? TRUE : FALSE;
@


1.136
log
@Revert back to 1.129: pool_init() is called before rwlocks can be
used on some archs.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d488 1
a488 1
		if (!cold) {
@


1.135
log
@matthew@@ noticed i wasnt populating npages in the kinfo_pool sent to
userland.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.134 2014/07/02 06:02:48 dlg Exp $	*/
a42 1
#include <sys/rwlock.h>
a62 10
/*
 * Every pool gets a unique serial number assigned to it. If this counter
 * wraps, we're screwed, but we shouldn't create so many pools anyway.
 */
unsigned int pool_serial;
unsigned int pool_count;

/* Lock the previous variables making up the global pool state */
struct rwlock pool_lock = RWLOCK_INITIALIZER("pools");

d95 6
d142 1
a142 1
static inline int
d160 1
a160 1
static inline struct pool_item_header *
d250 5
d342 4
a394 3
	/* pglistalloc/constraint parameters */
	pp->pr_crange = &kp_dirty;

d401 3
a404 13
	rw_enter_write(&pool_lock);
#ifdef DIAGNOSTIC
	SIMPLEQ_FOREACH(iter, &pool_head, pr_poollist) {
		if (iter == pp)
			panic("init pool already on list");
	}
#endif

	pp->pr_serial = ++pool_serial;
	if (pool_serial == 0)
		panic("pool_init: too much uptime");

	pool_count++;
a405 1
	rw_exit_write(&pool_lock);
a423 1
	rw_enter_write(&pool_lock);
a445 2
	pool_count--;
	rw_exit_write(&pool_lock);
d1115 1
d1117 1
a1117 1
	rw_enter_read(&pool_lock);
d1120 1
a1120 1
	rw_exit_read(&pool_lock);
d1423 1
a1423 1
sysctl_dopool(int *name, u_int namelen, char *oldp, size_t *oldlenp)
d1427 4
a1430 1
	int rv = ENOENT;
d1432 1
a1432 1
	switch (name[0]) {
d1434 4
a1437 4
		if (namelen != 1)
			return (ENOTDIR);
		return (sysctl_rdint(oldp, oldlenp, NULL, pool_count));

d1439 4
d1444 3
a1447 1

d1452 1
a1452 2
	if (namelen != 2)
		return (ENOTDIR);
a1453 1
	rw_enter_read(&pool_lock);
d1455 2
a1456 1
		if (name[1] == pp->pr_serial)
a1458 2
	if (pp == NULL)
		goto done;
d1460 8
a1467 1
	switch (name[0]) {
d1469 5
a1473 3
		rv = sysctl_rdstring(oldp, oldlenp, NULL, pp->pr_wchan);
		break;

a1475 2

		mtx_enter(&pp->pr_mtx);
a1478 1
		pi.pr_npages = pp->pr_npages;
d1491 1
a1491 4
		mtx_leave(&pp->pr_mtx);

		rv = sysctl_rdstruct(oldp, oldlenp, NULL, &pi, sizeof(pi));
		break;
d1493 2
a1494 5

done:
	rw_exit_read(&pool_lock);

	return (rv);
@


1.134
log
@inline is the new __inline
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.133 2014/07/02 06:01:25 dlg Exp $	*/
d1478 1
@


1.133
log
@use pool_count to report the number of pools to userland rather
than walking the list and counting the elements as we go.

use sysctl_rdint, sysctl_rdstring, and sysctl_rdstruct instead of
handcrafted copyouts.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.132 2014/07/02 05:52:45 dlg Exp $	*/
d147 1
a147 1
static __inline int
d165 1
a165 1
static __inline struct pool_item_header *
@


1.132
log
@provide a pool_count global so we can figure out how many pools there are
active without having to walk the global pool_list.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.131 2014/07/02 05:49:59 dlg Exp $	*/
d1435 1
a1435 1
sysctl_dopool(int *name, u_int namelen, char *where, size_t *sizep)
a1438 4
	size_t buflen = where != NULL ? *sizep : 0;
	int npools = 0;
	unsigned int lookfor;
	size_t len;
d1441 1
a1441 1
	switch (*name) {
d1443 4
a1446 4
		if (namelen != 1 || buflen != sizeof(int))
			return (EINVAL);
		lookfor = 0;
		break;
a1447 4
		if (namelen != 2 || buflen < 1)
			return (EINVAL);
		lookfor = name[1];
		break;
a1448 3
		if (namelen != 2 || buflen != sizeof(pi))
			return (EINVAL);
		lookfor = name[1];
d1450 1
d1455 3
d1460 1
a1460 2
		npools++;
		if (lookfor == pp->pr_serial)
d1463 1
a1463 2

	if (*name != KERN_POOL_NPOOLS && pp == NULL)
d1466 1
a1466 4
	switch (*name) {
	case KERN_POOL_NPOOLS:
		rv = copyout(&npools, where, buflen);
		break;
d1468 1
a1468 7
		len = strlen(pp->pr_wchan) + 1;
		if (*sizep < len) {
			rv = ENOMEM;
			goto done;
		}
		*sizep = len;
		rv = copyout(pp->pr_wchan, where, len);
d1470 1
d1492 1
a1492 1
		rv = copyout(&pi, where, buflen);
d1498 1
@


1.131
log
@take the pools mutex when copying stats out of it in the sysctl
path so we are guaranteed a consistent snapshot.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.130 2014/07/02 05:42:40 dlg Exp $	*/
d69 1
d413 1
d457 1
@


1.130
log
@add an explicit rwlock around the global state (the pool list and serial
number) rather than rely on implicit process exclusion, splhigh and splvm.

the only things touching the global state come from process context so we
can get away with an rwlock instead of a mutex. thankfully.

ok matthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.129 2014/07/02 00:12:34 dlg Exp $	*/
d1487 2
d1504 2
@


1.129
log
@info about pools is currently given to userland by copying each
pools struct out. however, struct pool in the kernel contains lots
of things that userland probably isnt interested in, like actual
mutexes, and probably shouldnt get easy access to, like pointers
to kernel memory via all the lists/trees.

this implements a kinfo_pool structure that has only the data that
userland needs to know about. it cuts the sysctl code over to
building it from struct pool as required and copying that out
instead, and cuts userland over to only handling kinfo_pool.

the only problem with this is vmstat, which can read kernel images
via kvm, which needs some understanding of struct pool. to cope,
the struct pool definition is guarded by if defined(_KERNEL) ||
defined(_LIBKVM) as inspired by sysctl which needs to do the same
thing sometimes. struct pool itself is generally not visible to
userland though, which is good.

matthew@@ suggested struct kinfo_pool instead of struct pool_info.
the kinfo prefix has precedent.
lots of people liked this.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.128 2014/05/19 14:30:03 tedu Exp $	*/
d43 1
d64 9
a104 6
/*
 * Every pool gets a unique serial number assigned to it. If this counter
 * wraps, we're screwed, but we shouldn't create so many pools anyway.
 */
unsigned int pool_serial;

a253 5

	SIMPLEQ_FOREACH(iter, &pool_head, pr_poollist) {
		if (iter == pp)
			panic("init pool already on list");
	}
a340 4
	pp->pr_serial = ++pool_serial;
	if (pool_serial == 0)
		panic("pool_init: too much uptime");

d390 3
d399 12
a410 2
	/* pglistalloc/constraint parameters */
	pp->pr_crange = &kp_dirty;
a411 1
	/* Insert this into the list of all pools. */
d413 1
d432 1
d455 1
a1124 1
	int		s;
d1126 1
a1126 1
	s = splhigh();
d1129 1
a1129 1
	splx(s);
d1437 1
a1437 1
	int npools = 0, s;
d1440 1
d1462 1
a1462 2
	s = splvm();

a1468 2
	splx(s);

d1470 1
a1470 1
		return (ENOENT);
d1474 2
a1475 1
		return copyout(&npools, where, buflen);
d1478 4
a1481 2
		if (*sizep < len)
			return (ENOMEM);
d1483 2
a1484 1
		return copyout(pp->pr_wchan, where, len);
d1502 2
a1503 1
		return copyout(&pi, where, buflen);
d1505 4
a1508 2
	/* NOTREACHED */
	return (0); /* XXX - Stupid gcc */
@


1.128
log
@consistent use of uint32_t for poison values
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.127 2014/05/01 04:25:02 tedu Exp $	*/
d1425 2
a1426 1
	struct pool *pp, *foundpool = NULL;
d1444 1
a1444 1
		if (namelen != 2 || buflen != sizeof(struct pool))
d1456 1
a1456 2
		if (lookfor == pp->pr_serial) {
			foundpool = pp;
a1457 1
		}
d1462 1
a1462 1
	if (*name != KERN_POOL_NPOOLS && foundpool == NULL)
d1469 1
a1469 1
		len = strlen(foundpool->pr_wchan) + 1;
d1473 1
a1473 1
		return copyout(foundpool->pr_wchan, where, len);
d1475 17
a1491 1
		return copyout(foundpool, where, buflen);
@


1.127
log
@with some random chance, swizzle the current page for the pool to avoid
fully deterministic behavior. ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.126 2014/04/03 21:36:59 tedu Exp $	*/
d646 1
a646 1
		int pval;
d1317 1
a1317 1
			int pval;
@


1.126
log
@if it's ok to wait, it must also be ok to give the kernel lock. do so.
(then immediately reacquire it). this has the effect of giving interrupts
on other CPUs to a chance to run and reduces latency in many cases.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.125 2014/03/28 17:57:11 mpi Exp $	*/
d104 1
d572 1
a817 1

d820 1
a820 3
	 * partially-full list and make it the current page.  The next
	 * allocation will get the item from this page, instead of
	 * further fragmenting the pool.
a824 1
		pp->pr_curpage = ph;
d980 21
@


1.125
log
@Reduce uvm include madness.  Use <uvm/uvm_extern.h> instead of
<uvm/uvm.h> if possible and remove double inclusions.

ok beck@@, mlarkin@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.124 2013/11/05 03:28:45 dlg Exp $	*/
d481 1
a482 1
	if ((flags & PR_WAITOK) != 0) {
d486 5
a491 1
#endif /* DIAGNOSTIC */
@


1.124
log
@remove pool constructors and destructors. theyre called for every
get and put, so they dont save us anything by caching constructed
objects. there were no real users of them, and this api was never
documented. removing conditionals in a hot path cant be a bad idea
either.

ok deraadt@@ krw@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.123 2013/08/08 23:25:06 syl Exp $	*/
d44 1
a44 1
#include <uvm/uvm.h>
@


1.123
log
@Uncomment kprintf format attributes for sys/kern

tested on vax (gcc3) ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.122 2013/06/05 00:44:06 tedu Exp $	*/
a344 5
        /* constructor, destructor, and arg */
	pp->pr_ctor = NULL;
	pp->pr_dtor = NULL;
	pp->pr_arg = NULL;

d509 3
a511 14
	if (pp->pr_ctor) {
		if (flags & PR_ZERO)
			panic("pool_get: PR_ZERO when ctor set");
		if (pp->pr_ctor(pp->pr_arg, v, flags)) {
			mtx_enter(&pp->pr_mtx);
			pp->pr_nget--;
			pool_do_put(pp, v);
			mtx_leave(&pp->pr_mtx);
			v = NULL;
		}
	} else {
		if (flags & PR_ZERO)
			memset(v, 0, pp->pr_size);
	}
a708 2
	if (pp->pr_dtor)
		pp->pr_dtor(pp->pr_arg, v);
a1035 8
void
pool_set_ctordtor(struct pool *pp, int (*ctor)(void *, void *, int),
    void (*dtor)(void *, void *), void *arg)
{
	pp->pr_ctor = ctor;
	pp->pr_dtor = dtor;
	pp->pr_arg = arg;
}
@


1.122
log
@blow up sooner rather than later for double pool_put
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.121 2013/05/31 20:44:10 tedu Exp $	*/
d134 1
a134 1
	    /* __attribute__((__format__(__kprintf__,1,2))) */);
d136 1
a136 1
	    /* __attribute__((__format__(__kprintf__,1,2))) */);
d1136 1
a1136 1
    int (*pr)(const char *, ...) /* __attribute__((__format__(__kprintf__,1,2))) */)
d1143 1
a1143 1
    int (*pr)(const char *, ...) /* __attribute__((__format__(__kprintf__,1,2))) */)
d1166 1
a1166 1
    int (*pr)(const char *, ...) /* __attribute__((__format__(__kprintf__,1,2))) */)
d1380 3
a1382 2
    int (*pr)(const char *, ...) /* __attribute__((__format__(__kprintf__,1,2))) */,
    void (*func)(void *, int, int (*)(const char *, ...) /* __attribute__((__format__(__kprintf__,1,2))) */))
@


1.121
log
@open up some races. if pool_debug == 2, force a yield() whenever waitok.
ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.120 2013/05/03 18:26:07 tedu Exp $	*/
d783 6
@


1.120
log
@switch the malloc and pool freelists to using xor simpleq.
this adds a tiny bit more protection from list manipulation.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.119 2013/04/17 17:44:03 tedu Exp $	*/
d487 1
a487 1
	if ((flags & PR_WAITOK) != 0)
d489 3
@


1.119
log
@check that the pool we are about to init isn't already on the list in
order to detect double init mistakes. add a similar check and rearrange
pool_destory to detect the opposite mistake.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.118 2013/04/06 13:41:11 deraadt Exp $	*/
d70 1
a70 1
	SIMPLEQ_HEAD(,pool_item) ph_itemlist;	/* chunk list for this page */
d83 1
a83 1
	SIMPLEQ_ENTRY(pool_item)	pi_list;
d635 1
a635 1
	if ((v = pi = SIMPLEQ_FIRST(&ph->ph_itemlist)) == NULL) {
d668 1
a668 1
	SIMPLEQ_REMOVE_HEAD(&ph->ph_itemlist, pi_list);
d686 1
a686 1
	if (SIMPLEQ_EMPTY(&ph->ph_itemlist)) {
d786 1
a786 1
	SIMPLEQ_INSERT_HEAD(&ph->ph_itemlist, pi, pi_list);
d889 1
a889 1
	SIMPLEQ_INIT(&ph->ph_itemlist);
d924 1
a924 1
		SIMPLEQ_INSERT_TAIL(&ph->ph_itemlist, pi, pi_list);
d1145 1
a1145 1
		SIMPLEQ_FOREACH(pi, &ph->ph_itemlist, pi_list) {
d1296 1
a1296 1
	for (pi = SIMPLEQ_FIRST(&ph->ph_itemlist), n = 0;
d1298 1
a1298 1
	     pi = SIMPLEQ_NEXT(pi,pi_list), n++) {
d1394 1
a1394 1
			SIMPLEQ_FOREACH(pi, &ph->ph_itemlist, pi_list) {
@


1.118
log
@make kernel compile
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.117 2013/04/06 03:53:25 tedu Exp $	*/
d247 8
a427 11
#ifdef DIAGNOSTIC
	if (pp->pr_nout != 0)
		panic("pool_destroy: pool busy: still out: %u", pp->pr_nout);
#endif

	/* Remove all pages */
	while ((ph = LIST_FIRST(&pp->pr_emptypages)) != NULL)
		pr_rmpage(pp, ph, NULL);
	KASSERT(LIST_EMPTY(&pp->pr_fullpages));
	KASSERT(LIST_EMPTY(&pp->pr_partpages));

d437 1
a437 1
				break;
d441 3
d445 12
@


1.117
log
@shuffle around some poison code, prototypes, values...
allow some more pool debug code to be enabled if not compiled in
bump poison size back up to 64
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.116 2013/03/31 00:03:26 tedu Exp $	*/
d457 1
d460 1
@


1.116
log
@replace pool debug magic with shared mem poison code
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.115 2013/03/26 16:37:45 tedu Exp $	*/
a80 1
#ifdef DIAGNOSTIC
a81 1
#endif
a85 6
#ifdef DEADBEEF1
#define	PI_MAGIC DEADBEEF1
#else
#define	PI_MAGIC 0xdeafbeef
#endif

d458 1
a458 1
		ph->ph_magic = PI_MAGIC;
d633 1
a633 1
	if (pi->pi_magic != PI_MAGIC)
a636 1
#ifdef POOL_DEBUG
a648 1
#endif /* POOL_DEBUG */
d766 1
a766 2
	pi->pi_magic = PI_MAGIC;
#ifdef POOL_DEBUG
a769 1
#endif /* POOL_DEBUG */
d913 1
a913 2
		pi->pi_magic = PI_MAGIC;
#ifdef POOL_DEBUG
a916 1
#endif /* POOL_DEBUG */
d1132 1
a1132 1
			if (pi->pi_magic != PI_MAGIC) {
d1287 1
a1287 1
		if (pi->pi_magic != PI_MAGIC) {
a1294 1
#ifdef POOL_DEBUG
a1307 2

#endif /* POOL_DEBUG */
@


1.115
log
@simpleq is lighter weight and sufficient for pool's needs.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.114 2013/02/17 17:39:29 miod Exp $	*/
a528 3
#if defined(DIAGNOSTIC) && defined(POOL_DEBUG)
	int i, *ip;
#endif
d647 9
a655 8
		for (ip = (int *)pi, i = sizeof(*pi) / sizeof(int);
		    i < pp->pr_size / sizeof(int); i++) {
			if (ip[i] != ph->ph_magic) {
				panic("pool_do_get(%s): free list modified: "
				    "page %p; item addr %p; offset 0x%zx=0x%x",
				    pp->pr_wchan, ph->ph_page, pi,
				    i * sizeof(int), ip[i]);
			}
a745 3
#if defined(DIAGNOSTIC) && defined(POOL_DEBUG)
	int i, *ip;
#endif
d779 1
a779 3
		for (ip = (int *)pi, i = sizeof(*pi)/sizeof(int);
		    i < pp->pr_size / sizeof(int); i++)
			ip[i] = ph->ph_magic;
a881 3
#if defined(DIAGNOSTIC) && defined(POOL_DEBUG)
	int i, *ip;
#endif
d928 1
a928 3
			for (ip = (int *)pi, i = sizeof(*pi)/sizeof(int);
			    i < pp->pr_size / sizeof(int); i++)
				ip[i] = ph->ph_magic;
a1283 3
#if defined(DIAGNOSTIC) && defined(POOL_DEBUG)
	int i, *ip;
#endif
d1311 10
a1320 9
			for (ip = (int *)pi, i = sizeof(*pi) / sizeof(int);
			    i < pp->pr_size / sizeof(int); i++) {
				if (ip[i] != ph->ph_magic) {
					printf("pool(%s): free list modified: "
					    "page %p; item ordinal %d; addr %p "
					    "(p %p); offset 0x%zx=0x%x\n",
					    pp->pr_wchan, ph->ph_page, n, pi,
					    page, i * sizeof(int), ip[i]);
				}
@


1.114
log
@Comment out recently added __attribute__((__format__(__kprintf__))) annotations
in MI code; gcc 2.95 does not accept such annotation for function pointer
declarations, only function prototypes.
To be uncommented once gcc 2.95 bites the dust.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.113 2013/02/09 20:56:35 miod Exp $	*/
d61 1
a61 1
TAILQ_HEAD(,pool) pool_head = TAILQ_HEAD_INITIALIZER(pool_head);
d70 1
a70 1
	TAILQ_HEAD(,pool_item)	ph_itemlist;	/* chunk list for this page */
d85 1
a85 1
	TAILQ_ENTRY(pool_item)	pi_list;
d409 1
a409 1
	TAILQ_INSERT_HEAD(&pool_head, pp, pr_poollist);
d426 1
d440 13
a452 1
	TAILQ_REMOVE(&pool_head, pp, pr_poollist);
d632 1
a632 1
	if ((v = pi = TAILQ_FIRST(&ph->ph_itemlist)) == NULL) {
d666 1
a666 1
	TAILQ_REMOVE(&ph->ph_itemlist, pi, pi_list);
d684 1
a684 1
	if (TAILQ_EMPTY(&ph->ph_itemlist)) {
d791 1
a791 1
	TAILQ_INSERT_HEAD(&ph->ph_itemlist, pi, pi_list);
d897 1
a897 1
	TAILQ_INIT(&ph->ph_itemlist);
d932 1
a932 1
		TAILQ_INSERT_TAIL(&ph->ph_itemlist, pi, pi_list);
d1124 1
a1124 1
	TAILQ_FOREACH(pp, &pool_head, pr_poollist)
d1157 1
a1157 1
		TAILQ_FOREACH(pi, &ph->ph_itemlist, pi_list) {
d1246 1
a1246 1
	TAILQ_FOREACH(pp, &pool_head, pr_poollist) {
d1311 1
a1311 1
	for (pi = TAILQ_FIRST(&ph->ph_itemlist), n = 0;
d1313 1
a1313 1
	     pi = TAILQ_NEXT(pi,pi_list), n++) {
d1411 1
a1411 1
			TAILQ_FOREACH(pi, &ph->ph_itemlist, pi_list) {
d1463 1
a1463 1
	TAILQ_FOREACH(pp, &pool_head, pr_poollist) {
@


1.113
log
@Add explicit __attribute__ ((__format__(__kprintf__)))) to the functions and
function pointer arguments which are {used as,} wrappers around the kernel
printf function.
No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.112 2012/12/24 19:43:11 guenther Exp $	*/
d142 1
a142 1
	    __attribute__((__format__(__kprintf__,1,2))));
d144 1
a144 1
	    __attribute__((__format__(__kprintf__,1,2))));
d1126 1
a1126 1
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))))
d1133 1
a1133 1
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))))
d1156 1
a1156 1
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))))
d1375 2
a1376 2
    int (*pr)(const char *, ...) __attribute__((__format__(__kprintf__,1,2))),
    void (*func)(void *, int, int (*)(const char *, ...) __attribute__((__format__(__kprintf__,1,2)))))
@


1.112
log
@Fix compilation with POOL_DEBUG but !DDB

ok jsing@@ krw@@ mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.111 2011/11/23 02:05:17 dlg Exp $	*/
d141 4
a144 3
void	 pool_print_pagelist(struct pool_pagelist *,
	    int (*)(const char *, ...));
void	 pool_print1(struct pool *, const char *, int (*)(const char *, ...));
d641 1
a641 1
				    "page %p; item addr %p; offset 0x%x=0x%x",
d1125 2
a1126 1
pool_printit(struct pool *pp, const char *modif, int (*pr)(const char *, ...))
d1132 2
a1133 1
pool_print_pagelist(struct pool_pagelist *pl, int (*pr)(const char *, ...))
d1155 2
a1156 1
pool_print1(struct pool *pp, const char *modif, int (*pr)(const char *, ...))
d1230 1
a1230 1
		db_printf("%-10s %18s %18s\n",
d1235 1
a1235 1
			db_printf("%-10s %18p %18p\n", pp->pr_wchan, pp,
d1318 1
a1318 1
					    "(p %p); offset 0x%x=0x%x\n",
d1374 3
a1376 2
pool_walk(struct pool *pp, int full, int (*pr)(const char *, ...),
    void (*func)(void *, int, int (*)(const char *, ...)))
@


1.111
log
@block interrupts while pool_reclaim_all is grubbing around in every pools
internals. this fixes a panic i got where a network interrupt tried to use
the mbuf pools mutex while pool_reclaim_all already held it which lead
to the same cpu trying to lock that mutex twice.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.110 2011/09/23 10:08:31 dlg Exp $	*/
d116 2
a117 1
int	pool_chk_page(struct pool *, struct pool_item_header *, int);
d1270 1
d1272 1
d1366 1
d1368 1
@


1.110
log
@touching pr_nput and pr_nget outside the lock might be racy since ++ and --
arent necessarily atomic.

this is an update of a diff matthew@@ posted to tech@@ over a year ago.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.109 2011/09/23 07:27:09 dlg Exp $	*/
d1106 3
d1111 1
@


1.109
log
@ph = pool_get(&phpool, PR_NOWAIT) can return NULL, so dont unconditionally
write to ph.

ok blambert@@ matthew@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.108 2011/07/06 06:00:20 tedu Exp $	*/
d484 2
d495 1
a503 2
	if (v != NULL)
		pp->pr_nget++;
d721 1
a722 1
	pp->pr_nput++;
@


1.108
log
@remove the predict_whatever stuffs.  ok deraadt dlg henning
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.107 2011/07/06 02:56:53 tedu Exp $	*/
d450 1
a450 1
	if (pool_debug)
@


1.107
log
@move a few accounting adjustments closer to removing the page, just to be safe.
also, rmpage updates curpage, no need to do it twice.
ok art deraadt guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.106 2011/07/05 20:00:18 tedu Exp $	*/
d535 1
a535 1
	if (__predict_false(pp->pr_nout > pp->pr_hardlimit))
d538 1
a538 1
	if (__predict_false(pp->pr_nout == pp->pr_hardlimit)) {
d580 1
a580 1
		if (__predict_true(v != NULL))
d583 1
a583 1
		if (__predict_false(v == NULL || ph == NULL)) {
d616 1
a616 1
	if (__predict_false((v = pi = TAILQ_FIRST(&ph->ph_itemlist)) == NULL)) {
d620 1
a620 1
	if (__predict_false(pp->pr_nitems == 0)) {
d628 1
a628 1
	if (__predict_false(pi->pi_magic != PI_MAGIC))
d655 1
a655 1
		if (__predict_false(pp->pr_nidle == 0))
d670 1
a670 1
		if (__predict_false(ph->ph_nmissing != pp->pr_itemsperpage)) {
d750 1
a750 1
	if (__predict_false(pp->pr_nout == 0)) {
d757 1
a757 1
	if (__predict_false((ph = pr_find_pagehead(pp, v)) == NULL)) {
d840 1
a840 1
		if (__predict_true(cp != NULL))
d842 1
a842 1
		if (__predict_false(cp == NULL || ph == NULL)) {
d960 1
a960 1
		if (__predict_true(cp != NULL))
d962 1
a962 1
		if (__predict_false(cp == NULL || ph == NULL)) {
@


1.106
log
@Remove a broken optimization found by the new pool_chk code.  It
leaves an empty page in curpage, and this inconsistency slowly spreads
until finally one of the other pool checks freaks out.
ok art deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.105 2011/07/05 16:36:15 tedu Exp $	*/
d229 4
a239 4
	pp->pr_npages--;
	pp->pr_npagefree++;

	pool_update_curpage(pp);
d807 1
a808 1
		pool_update_curpage(pp);
@


1.105
log
@when all you have is a hammer, make it a big one.  add more checks to pool_chk
and a pool_init flag to aggressively run pool_chk.  ok art deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.104 2011/04/18 19:23:46 art Exp $	*/
a785 2
		if (ph->ph_nmissing == 0)
			pp->pr_nidle++;
a786 1
		return;
@


1.104
log
@Put back the change of pool and malloc into the new km_alloc(9) api.
The problems during the hackathon were not caused by this (most likely).

prodded by deraadt@@ and beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.103 2011/04/06 15:52:13 art Exp $	*/
d116 1
a116 1
int	pool_chk_page(struct pool *, const char *, struct pool_item_header *);
d471 6
d478 6
d707 6
d714 6
d1264 1
a1264 1
		pool_chk(pp, pp->pr_wchan);
d1269 1
a1269 1
pool_chk_page(struct pool *pp, const char *label, struct pool_item_header *ph)
d1277 1
d1282 1
a1282 2
		if (label != NULL)
			printf("%s: ", label);
d1295 1
a1295 2
			if (label != NULL)
				printf("%s: ", label);
d1323 1
a1323 2
		if (label != NULL)
			printf("%s: ", label);
d1329 14
d1347 1
a1347 1
pool_chk(struct pool *pp, const char *label)
d1353 1
a1353 1
		r += pool_chk_page(pp, label, ph);
d1355 1
a1355 1
		r += pool_chk_page(pp, label, ph);
d1357 1
a1357 1
		r += pool_chk_page(pp, label, ph);
@


1.103
log
@Backout the uvm_km_getpage -> km_alloc conversion. Weird things are happening
and we aren't sure what's causing them.

shouted oks by many before I even built a kernel with the diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.102 2011/04/05 01:28:05 art Exp $	*/
d404 1
a404 2
	pp->pr_crange = &no_constraint;
	pp->pr_pa_nsegs = 0;
d1015 1
a1015 2
pool_set_constraints(struct pool *pp, struct uvm_constraint_range *range,
    int nsegs)
d1017 1
a1017 9
	/*
	 * Subsequent changes to the constrictions are only
	 * allowed to make them _more_ strict.
	 */
	KASSERT(pp->pr_crange->ucr_high >= range->ucr_high &&
	    pp->pr_crange->ucr_low <= range->ucr_low);

	pp->pr_crange = range;
	pp->pr_pa_nsegs = nsegs;
d1488 4
a1491 1
	int kfl = (flags & PR_WAITOK) ? 0 : UVM_KMF_NOWAIT;
d1493 1
a1493 2
	return (uvm_km_getpage_pla(kfl, slowdown, pp->pr_crange->ucr_low,
	    pp->pr_crange->ucr_high, 0, 0));
d1499 1
a1499 1
	uvm_km_putpage(v);
d1505 2
a1506 2
	int kfl = (flags & PR_WAITOK) ? 0 : UVM_KMF_NOWAIT;
	vaddr_t va;
d1509 3
d1513 2
a1514 3
	va = uvm_km_kmemalloc_pla(kmem_map, NULL, pp->pr_alloc->pa_pagesz, 0,
	    kfl, pp->pr_crange->ucr_low, pp->pr_crange->ucr_high,
	    0, 0, pp->pr_pa_nsegs);
d1517 1
a1517 1
	return ((void *)va);
d1526 1
a1526 1
	uvm_km_free(kmem_map, (vaddr_t)v, pp->pr_alloc->pa_pagesz);
d1533 4
a1536 1
	int kfl = (flags & PR_WAITOK) ? 0 : UVM_KMF_NOWAIT;
d1538 1
a1538 4
	return ((void *)uvm_km_kmemalloc_pla(kernel_map, uvm.kernel_object,
	    pp->pr_alloc->pa_pagesz, 0, kfl,
	    pp->pr_crange->ucr_low, pp->pr_crange->ucr_high,
	    0, 0, pp->pr_pa_nsegs));
d1544 1
a1544 1
	uvm_km_free(kernel_map, (vaddr_t)v, pp->pr_alloc->pa_pagesz);
@


1.102
log
@ - Change pool constraints to use kmem_pa_mode instead of uvm_constraint_range
 - Use km_alloc for all backend allocations in pools.
 - Use km_alloc for the emergmency kentry allocations in uvm_mapent_alloc
 - Garbage collect uvm_km_getpage, uvm_km_getpage_pla and uvm_km_putpage

ariane@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.101 2011/04/04 11:13:55 deraadt Exp $	*/
d404 2
a405 1
	pp->pr_crange = &kp_dirty;
d1016 2
a1017 1
pool_set_constraints(struct pool *pp, struct kmem_pa_mode *mode)
d1019 9
a1027 1
	pp->pr_crange = mode;
d1498 1
a1498 4
	struct kmem_dyn_mode kd = KMEM_DYN_INITIALIZER;

	kd.kd_waitok = (flags & PR_WAITOK);
	kd.kd_slowdown = slowdown;
d1500 2
a1501 1
	return (km_alloc(PAGE_SIZE, &kv_page, pp->pr_crange, &kd));
d1507 1
a1507 1
	km_free(v, PAGE_SIZE, &kv_page, pp->pr_crange);
d1513 2
a1514 2
	struct kmem_dyn_mode kd = KMEM_DYN_INITIALIZER;
	void *v;
a1516 3
	kd.kd_waitok = (flags & PR_WAITOK);
	kd.kd_slowdown = slowdown;

d1518 3
a1520 2
	v = km_alloc(pp->pr_alloc->pa_pagesz, &kv_intrsafe, pp->pr_crange,
	    &kd);
d1523 1
a1523 1
	return (v);
d1532 1
a1532 1
	km_free(v, pp->pr_alloc->pa_pagesz, &kv_intrsafe, pp->pr_crange);
d1539 1
a1539 4
	struct kmem_dyn_mode kd = KMEM_DYN_INITIALIZER;

	kd.kd_waitok = (flags & PR_WAITOK);
	kd.kd_slowdown = slowdown;
d1541 4
a1544 1
	return (km_alloc(pp->pr_alloc->pa_pagesz, &kv_any, pp->pr_crange, &kd));
d1550 1
a1550 1
	km_free(v, pp->pr_alloc->pa_pagesz, &kv_any, pp->pr_crange);
@


1.101
log
@sysctl kern.pool_debug=0 will disable POOL_DEBUG on the fly (still defaults
to on, if POOL_DEBUG is compiled in, so that boot-time pool corruption
can be found.  When the sysctl is turned off, performance is almost as
as good as compiling with POOL_DEBUG compiled out.  Not all pool page
headers can be purged of the magic checks.
performance tests by henning
ok ariane kettenis mikeb
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.100 2011/04/03 22:07:37 ariane Exp $	*/
d404 1
a404 2
	pp->pr_crange = &no_constraint;
	pp->pr_pa_nsegs = 0;
d1015 1
a1015 2
pool_set_constraints(struct pool *pp, struct uvm_constraint_range *range,
    int nsegs)
d1017 1
a1017 9
	/*
	 * Subsequent changes to the constrictions are only
	 * allowed to make them _more_ strict.
	 */
	KASSERT(pp->pr_crange->ucr_high >= range->ucr_high &&
	    pp->pr_crange->ucr_low <= range->ucr_low);

	pp->pr_crange = range;
	pp->pr_pa_nsegs = nsegs;
d1488 4
a1491 1
	int kfl = (flags & PR_WAITOK) ? 0 : UVM_KMF_NOWAIT;
d1493 1
a1493 2
	return (uvm_km_getpage_pla(kfl, slowdown, pp->pr_crange->ucr_low,
	    pp->pr_crange->ucr_high, 0, 0));
d1499 1
a1499 1
	uvm_km_putpage(v);
d1505 2
a1506 2
	int kfl = (flags & PR_WAITOK) ? 0 : UVM_KMF_NOWAIT;
	vaddr_t va;
d1509 3
d1513 2
a1514 3
	va = uvm_km_kmemalloc_pla(kmem_map, NULL, pp->pr_alloc->pa_pagesz, 0,
	    kfl, pp->pr_crange->ucr_low, pp->pr_crange->ucr_high,
	    0, 0, pp->pr_pa_nsegs);
d1517 1
a1517 1
	return ((void *)va);
d1526 1
a1526 1
	uvm_km_free(kmem_map, (vaddr_t)v, pp->pr_alloc->pa_pagesz);
d1533 4
a1536 1
	int kfl = (flags & PR_WAITOK) ? 0 : UVM_KMF_NOWAIT;
d1538 1
a1538 4
	return ((void *)uvm_km_kmemalloc_pla(kernel_map, uvm.kernel_object,
	    pp->pr_alloc->pa_pagesz, 0, kfl,
	    pp->pr_crange->ucr_low, pp->pr_crange->ucr_high,
	    0, 0, pp->pr_pa_nsegs));
d1544 1
a1544 1
	uvm_km_free(kernel_map, (vaddr_t)v, pp->pr_alloc->pa_pagesz);
@


1.100
log
@Helper functions for suspend.

Allow reclaiming pages from all pools.
Allow zeroing all pages.
Allocate the more equal pig.

mlarking@@ needs this.
Not called yet.

ok mlarkin@@, theo@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.99 2010/11/03 17:49:42 mikeb Exp $	*/
d45 1
a45 1

d77 1
d94 6
d451 2
a452 1

d622 9
a630 7
	for (ip = (int *)pi, i = sizeof(*pi) / sizeof(int);
	    i < pp->pr_size / sizeof(int); i++) {
		if (ip[i] != PI_MAGIC) {
			panic("pool_do_get(%s): free list modified: "
			    "page %p; item addr %p; offset 0x%x=0x%x",
			    pp->pr_wchan, ph->ph_page, pi,
			    i * sizeof(int), ip[i]);
d744 5
a748 3
	for (ip = (int *)pi, i = sizeof(*pi)/sizeof(int);
	    i < pp->pr_size / sizeof(int); i++)
		ip[i] = PI_MAGIC;
d901 5
a905 3
		for (ip = (int *)pi, i = sizeof(*pi)/sizeof(int);
		    i < pp->pr_size / sizeof(int); i++)
			ip[i] = PI_MAGIC;
d1290 10
a1299 8
		for (ip = (int *)pi, i = sizeof(*pi) / sizeof(int);
		    i < pp->pr_size / sizeof(int); i++) {
			if (ip[i] != PI_MAGIC) {
				printf("pool(%s): free list modified: "
				    "page %p; item ordinal %d; addr %p "
				    "(p %p); offset 0x%x=0x%x\n",
				    pp->pr_wchan, ph->ph_page, n, pi,
				    page, i * sizeof(int), ip[i]);
@


1.99
log
@pool_sethardlimit should not imply pool_sethiwat;  figured out with claudio

ok claudio tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.98 2010/09/26 21:03:57 tedu Exp $	*/
d1070 12
@


1.98
log
@unify some pool and malloc flag values.  the important bit is that all flags
have real values, no 0 values anymore.
ok deraadt kettenis krw matthew oga thib
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.97 2010/09/21 01:09:10 matthew Exp $	*/
a995 7

	/*
	 * In-line version of pool_sethiwat().
	 */
	pp->pr_maxpages = (n == 0 || n == UINT_MAX)
		? n
		: roundup(n, pp->pr_itemsperpage) / pp->pr_itemsperpage;
@


1.97
log
@Add assertwaitok(9) to declare code paths that assume they can sleep.
Currently only checks that we're not in an interrupt context, but will
soon check that we're not holding any mutexes either.

Update malloc(9) and pool(9) to use assertwaitok(9) as appropriate.

"i like it" art@@, oga@@, marco@@; "i see no harm" deraadt@@; too trivial
for me to bother prying actual oks from people.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.96 2010/07/03 03:04:55 tedu Exp $	*/
d455 2
@


1.96
log
@explicitly specify flags to malloc and pool_get instead of relying on 0.
This is more clear, and as thib pointed out, the default in softraid was
wrong.  ok thib.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.95 2010/07/02 01:25:05 art Exp $	*/
d458 1
a458 1
		splassert(IPL_NONE);
@


1.95
log
@add an align argument to uvm_km_kmemalloc_pla.

Use uvm_km_kmemalloc_pla with the dma constraint to allocate kernel stacks.

Yes, that means DMA is possible to kernel stacks, but only until we've fixed
all the scary drivers.

deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.94 2010/06/29 20:39:27 thib Exp $	*/
d442 2
a443 1
		ph = pool_get(&phpool, flags & ~(PR_WAITOK | PR_ZERO));
@


1.94
log
@Add a no_constraint uvm_constraint_range; use it in the pool code.

ok tedu@@, beck@@, oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.93 2010/06/27 03:03:48 thib Exp $	*/
d1494 2
a1495 2
	va = uvm_km_kmemalloc_pla(kmem_map, NULL, pp->pr_alloc->pa_pagesz, kfl,
	    pp->pr_crange->ucr_low, pp->pr_crange->ucr_high,
d1518 1
a1518 1
	    pp->pr_alloc->pa_pagesz, kfl,
@


1.93
log
@uvm constraints. Add two mandatory MD symbols, uvm_md_constraints
which contains the constraints for DMA/memory allocation for each
architecture, and dma_constraints which contains the range of addresses
that are dma accessable by the system.

This is based on ariane@@'s physcontig diff, with lots of bugfixes and
additions the following additions by my self:

Introduce a new function pool_set_constraints() which sets the address
range for which we allocate pages for the pool from, this is now used
for the mbuf/mbuf cluster pools to keep them dma accessible.

The !direct archs no longer stuff pages into the kernel object in
uvm_km_getpage_pla but rather do a pmap_extract() in uvm_km_putpages.

Tested heavily by my self on i386, amd64 and sparc64. Some tests on
alpha and SGI.

"commit it" beck, art, oga, deraadt
"i like the diff" deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.91 2010/01/16 03:08:00 tedu Exp $	*/
a96 6
 * Default constraint range for pools, that cover the whole
 * address space.
 */
struct uvm_constraint_range	pool_full_range = { 0x0, (paddr_t)-1 };

/*
d397 1
a397 1
	pp->pr_crange = &pool_full_range;
@


1.92
log
@aligment -> alignment
@
text
@d97 6
d402 4
d1012 15
d1480 1
a1480 1
	boolean_t waitok = (flags & PR_WAITOK) ? TRUE : FALSE;
d1482 2
a1483 1
	return (uvm_km_getpage(waitok, slowdown));
a1488 1

d1500 3
a1502 1
	va = uvm_km_kmemalloc(kmem_map, NULL, pp->pr_alloc->pa_pagesz, kfl);
d1523 4
a1526 2
	return ((void *)uvm_km_kmemalloc(kernel_map, uvm.kernel_object,
	    pp->pr_alloc->pa_pagesz, kfl));
@


1.91
log
@When allocating from the item header pool, we can't sleep, as we may be holding a mutex which won't be released.  From Christian Ehrhardt.
While here, fix another buglet: no need to pass down PR_ZERO either, as noticed by blambert@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.90 2009/09/05 16:06:57 thib Exp $	*/
d859 1
a859 1
	 * Adjust storage to apply aligment to `pr_itemoffset' in each item.
@


1.90
log
@sync comment to reality, off-page page headers go into
an RB tree, not into a hashtable.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.85 2009/06/24 11:23:33 deraadt Exp $	*/
d437 2
a438 3
	else {
		ph = pool_get(&phpool, flags);
	}
@


1.89
log
@add commented out options for PAGEFASTRECYCLE, KVA_GUARDPAGES, shuffle VFSDEBUG
around and add POOL_DEBUG as an enabled option, removing the define from subr_pool.c.

comments & ok deraadt@@.
@
text
@d344 2
a345 2
	 * go on a hash table, so we can match a returned item
	 * with its header based on the page address.
@


1.88
log
@add a show all vnodes command, use dlg's nice pool_walk() to accomplish
this.

ok beck@@, dlg@@
@
text
@a58 1
#define POOL_DEBUG
@


1.87
log
@Use an RB tree instead of a SPLAY tree for the page headers tree.

ok beck@@, dlg@@
@
text
@d1296 2
a1297 1
pool_walk(struct pool *pp, void (*func)(void *))
d1309 1
a1309 1
			func(cp);
d1324 1
a1324 1
				func(cp);
@


1.86
log
@We enable POOL_DEBUG (except in a release)
@
text
@d72 1
a72 1
	SPLAY_ENTRY(pool_item_header)
d153 2
a154 2
SPLAY_PROTOTYPE(phtree, pool_item_header, ph_node, phtree_compare);
SPLAY_GENERATE(phtree, pool_item_header, ph_node, phtree_compare);
d185 1
a185 1
	ph = SPLAY_FIND(phtree, &pp->pr_phtree, &tmp);
d222 1
a222 1
		SPLAY_REMOVE(phtree, &pp->pr_phtree, ph);
d358 1
a358 1
		SPLAY_INIT(&pp->pr_phtree);
d849 1
a849 1
		SPLAY_INSERT(phtree, &pp->pr_phtree, ph);
@


1.85
log
@turn off POOL_DEBUG as we go into release; pointed out by mpf
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.84 2009/06/12 14:56:21 oga Exp $	*/
d59 1
a59 1
/* #define POOL_DEBUG */
@


1.84
log
@rework pool_get() a bit so that if you call if with a constructor set
*and* PR_ZERO in flags, you will no longer zero our your nicely
constructed object.

Instead, now if you have a contructor set, and you set PR_ZERO, you will
panic (it's invalid due to how constructor work).

ok miod@@ deraadt@@ on earlier versions of the diff. ok tedu@@ after he
pointed out a couple of places I messed up.

Problem initally noticed by ariane@@ a while ago.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.83 2009/06/04 18:48:54 deraadt Exp $	*/
d59 1
a59 1
#define POOL_DEBUG
@


1.83
log
@POOL_DEBUG and DIAGNOSTIC should be better friends
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.82 2009/06/04 17:42:23 deraadt Exp $	*/
d461 13
a473 8
	if (v && pp->pr_ctor && pp->pr_ctor(pp->pr_arg, v, flags)) {
		mtx_enter(&pp->pr_mtx);
		pool_do_put(pp, v);
		mtx_leave(&pp->pr_mtx);
		v = NULL;
	}
	if (v) {
		pp->pr_nget++;
d477 2
@


1.82
log
@the POOL_DEBUG checks needed to be more friendly with DIAGNOSTIC
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.81 2009/06/04 09:58:40 oga Exp $	*/
d687 1
a687 1
#if defined(POOL_DEBUG) && defined(DIAGNOSTIC)
d829 1
a829 1
#if defined(POOL_DEBUG) && defined(DIAGNOSTIC)
d1214 1
a1214 1
#ifdef POOL_DEBUG
@


1.81
log
@enable POOL_DEBUG again just for the hackathon.

slackers now get more bugs to fix, yay!

discussed with deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.80 2009/05/31 17:11:14 miod Exp $	*/
d482 1
a482 1
#ifdef POOL_DEBUG
d687 1
a687 1
#ifdef POOL_DEBUG
d829 1
a829 1
#ifdef POOL_DEBUG
@


1.80
log
@Move splassert checks from pool_do_get to pool_get(). Since the former
is invoked with the pool mutex held, the asserts are satisfied by design.

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.79 2009/04/22 01:16:11 dlg Exp $	*/
d59 1
@


1.79
log
@initialise the constructor and destructor function pointers to NULL
in pool_init so you the pool struct doesn't have to be zeroed before
you init it.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.77 2009/02/16 23:48:17 deraadt Exp $	*/
d452 5
a483 7

#ifdef DIAGNOSTIC
	if ((flags & PR_WAITOK) != 0)
		splassert(IPL_NONE);
	if (pp->pr_ipl != -1)
		splassert(pp->pr_ipl);
#endif /* DIAGNOSTIC */
@


1.78
log
@ensure all pi_magic checks are inside DIAGNOSTIC
@
text
@d336 5
@


1.77
log
@at tedu's request, bring back the basic single "first word" PI_MAGIC check
since it is essentially free.  To turn on the checking of the rest of the
allocation, use 'option POOL_DEBUG'
ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.76 2009/02/16 22:11:41 deraadt Exp $	*/
d1229 1
d1251 2
a1252 1
#endif
@


1.76
log
@Disable pool debug stuff for the release (it has a performance hit, but
between releases we may want to turn it on, since it has uncovered real
bugs)
ok miod henning etc etc
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.75 2008/12/23 08:15:06 dlg Exp $	*/
a86 1
#if 0		/* Enable to debug pool corruption */
a91 1
#endif
d471 1
a471 1
#ifdef PI_MAGIC
d592 1
a592 1
#ifdef PI_MAGIC
d597 1
d607 2
a608 1
#endif
d683 1
a683 1
#ifdef PI_MAGIC
d715 1
a715 1
#ifdef PI_MAGIC
d717 1
d721 2
a722 1
#endif
d825 1
a825 1
#ifdef PI_MAGIC
d869 2
a870 1
#ifdef PI_MAGIC
d872 1
d876 2
a877 1
#endif
d1066 1
a1066 1
#ifdef PI_MAGIC
d1073 1
a1073 1
#ifdef PI_MAGIC
d1210 1
a1210 1
#ifdef PI_MAGIC
a1228 1
#ifdef PI_MAGIC
d1238 1
@


1.75
log
@i got emptypages and fullpages mixed up in pool_walk. this now shows items
in fullpages that have been allocated.

spotted by claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.74 2008/12/23 06:53:12 dlg Exp $	*/
d83 5
d93 1
a93 3
	/* Other entries use only this list entry */
	TAILQ_ENTRY(pool_item)	pi_list;
};
d473 1
a473 1
#ifdef DIAGNOSTIC
d594 1
a594 1
#ifdef DIAGNOSTIC
d683 1
a683 1
#ifdef DIAGNOSTIC
d715 1
a715 1
#ifdef DIAGNOSTIC
d823 1
a823 1
#ifdef DIAGNOSTIC
d867 1
a867 1
#ifdef DIAGNOSTIC
d1061 1
a1061 1
#ifdef DIAGNOSTIC
d1068 1
a1068 1
#ifdef DIAGNOSTIC
d1205 1
a1205 1
#ifdef DIAGNOSTIC
d1224 1
a1224 1
#ifdef DIAGNOSTIC
@


1.74
log
@add pool_walk as debug code.

this can be used to walk over all the items allocated with a pool and have
them examined by a function the caller provides.

with help from and ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.70 2008/11/25 13:05:51 art Exp $	*/
d1282 1
a1282 1
	LIST_FOREACH(ph, &pp->pr_emptypages, ph_pagelist) {
@


1.73
log
@record the offset into each pool page that item allocations actually begin
on, aka, its coloring.

ok tedu@@
@
text
@d1273 37
@


1.72
log
@Put back the support for pools > PAGE_SIZE. This time the compare function
works and there's even some sanity checks that it actually returns what we
expect it to return.
@
text
@d75 1
d849 1
@


1.71
log
@Back out the large page pools for now. The compare function is
borked and instead of stressing to figure out how to fix it, I'll
let peoples kernels to work.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.67 2008/11/22 17:31:52 deraadt Exp $	*/
d75 1
d113 17
d141 5
a145 4
	if (a->ph_page < b->ph_page)
		return (-1);
	else if (a->ph_page > b->ph_page)
		return (1);
d157 1
a157 1
pr_find_pagehead(struct pool *pp, caddr_t page)
d161 5
a165 1
	if ((pp->pr_roflags & PR_PHINPAGE) != 0)
d167 1
d169 13
a181 1
	tmp.ph_page = page;
d183 5
d252 32
a283 2
	if (palloc == NULL)
		palloc = &pool_allocator_nointr;
d286 2
d341 1
a341 1
	if (pp->pr_size < palloc->pa_pagesz/16) {
a678 1
	caddr_t page;
a692 2
	page = (caddr_t)((vaddr_t)v & pp->pr_alloc->pa_pagemask);

d704 1
a704 1
	if (__predict_false((ph = pr_find_pagehead(pp, page)) == NULL)) {
a822 5
#ifdef DIAGNOSTIC
	if (((u_long)cp & (pp->pr_alloc->pa_pagesz - 1)) != 0)
		panic("pool_prime_page: %s: unaligned page", pp->pr_wchan);
#endif

d829 1
d1406 39
@


1.70
log
@Make sure that equal elements always compare equal. Logic error spotted
by otto@@
ok otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.69 2008/11/24 21:36:07 art Exp $	*/
a74 1
	int			ph_pagesize;
a111 17
/*
 * XXX - quick hack. For pools with large items we want to use a special
 *       allocator. For now, instead of having the allocator figure out
 *       the allocation size from the pool (which can be done trivially
 *       with round_page(pr_itemsperpage * pr_size)) which would require
 *	 lots of changes everywhere, we just create allocators for each
 *	 size. We limit those to 128 pages.
 */
#define POOL_LARGE_MAXPAGES 128
struct pool_allocator pool_allocator_large[POOL_LARGE_MAXPAGES];
struct pool_allocator pool_allocator_large_ni[POOL_LARGE_MAXPAGES];
void	*pool_large_alloc(struct pool *, int, int *);
void	pool_large_free(struct pool *, void *);
void	*pool_large_alloc_ni(struct pool *, int, int *);
void	pool_large_free_ni(struct pool *, void *);


d123 4
a126 5
	long diff = (vaddr_t)a->ph_page - (vaddr_t)b->ph_page;
	if (diff < 0)
		return -(-diff < a->ph_pagesize);
	else if (diff > 0)
		return (diff > b->ph_pagesize);
d138 1
a138 1
pr_find_pagehead(struct pool *pp, void *v)
d142 1
a142 5
	if ((pp->pr_roflags & PR_PHINPAGE) != 0) {
		caddr_t page;

		page = (caddr_t)((vaddr_t)v & pp->pr_alloc->pa_pagemask);

a143 1
	}
d145 1
a145 13
	/*
	 * The trick we're using in the tree compare function is to compare
	 * two elements equal when they overlap. We want to return the
	 * page header that belongs to the element just before this address.
	 * We don't want this element to compare equal to the next element,
	 * so the compare function takes the pagesize from the lower element.
	 * If this header is the lower, its pagesize is zero, so it can't
	 * overlap with the next header. But if the header we're looking for
	 * is lower, we'll use its pagesize and it will overlap and return
	 * equal.
	 */
	tmp.ph_page = v;
	tmp.ph_pagesize = 0;
d211 2
a212 32
	if (palloc == NULL) {
		if (size > PAGE_SIZE) {
			int psize;

			/*
			 * XXX - should take align into account as well.
			 */
			if (size == round_page(size))
				psize = size / PAGE_SIZE;
			else
				psize = PAGE_SIZE / roundup(size % PAGE_SIZE,
				    1024);
			if (psize > POOL_LARGE_MAXPAGES)
				psize = POOL_LARGE_MAXPAGES;
			if (flags & PR_WAITOK)
				palloc = &pool_allocator_large_ni[psize-1];
			else
				palloc = &pool_allocator_large[psize-1];
			if (palloc->pa_pagesz == 0) {
				palloc->pa_pagesz = psize * PAGE_SIZE;
				if (flags & PR_WAITOK) {
					palloc->pa_alloc = pool_large_alloc_ni;
					palloc->pa_free = pool_large_free_ni;
				} else {
					palloc->pa_alloc = pool_large_alloc;
					palloc->pa_free = pool_large_free;
				}
			}
		} else {
			palloc = &pool_allocator_nointr;
		}
	}
a214 2
	}
	if (palloc->pa_pagemask == 0) {
d268 1
a268 1
	if (pp->pr_size < palloc->pa_pagesz/16 && pp->pr_size < PAGE_SIZE) {
d606 1
d621 2
d634 1
a634 1
	if (__predict_false((ph = pr_find_pagehead(pp, v)) == NULL)) {
d753 5
a763 1
	ph->ph_pagesize = pp->pr_alloc->pa_pagesz;
a1339 39
}

void *
pool_large_alloc(struct pool *pp, int flags, int *slowdown)
{
	int kfl = (flags & PR_WAITOK) ? 0 : UVM_KMF_NOWAIT;
	vaddr_t va;
	int s;

	s = splvm();
	va = uvm_km_kmemalloc(kmem_map, NULL, pp->pr_alloc->pa_pagesz, kfl);
	splx(s);

	return ((void *)va);
}

void
pool_large_free(struct pool *pp, void *v)
{
	int s;

	s = splvm();
	uvm_km_free(kmem_map, (vaddr_t)v, pp->pr_alloc->pa_pagesz);
	splx(s);
}

void *
pool_large_alloc_ni(struct pool *pp, int flags, int *slowdown)
{
	int kfl = (flags & PR_WAITOK) ? 0 : UVM_KMF_NOWAIT;

	return ((void *)uvm_km_kmemalloc(kernel_map, NULL,
	    pp->pr_alloc->pa_pagesz, kfl));
}

void
pool_large_free_ni(struct pool *pp, void *v)
{
	uvm_km_free(kernel_map, (vaddr_t)v, pp->pr_alloc->pa_pagesz);
@


1.69
log
@Protect kmem_map allocations with splvm.
This should make dlg happy.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.68 2008/11/24 17:42:34 art Exp $	*/
d143 3
a145 1
		return -((a->ph_pagesize + diff) > 0);
d147 1
a147 1
		return (diff > b->ph_pagesize);
@


1.68
log
@Allow allocations larger than PAGE_SIZE from pools.

This is solved by special allocators and an obfuscated compare function
for the page header splay tree and some other minor adjustments.

At this moment, the allocator will be picked automagically by pool_init
and you can get a kernel_map allocator if you specify PR_WAITOK in flags
(XXX), default is kmem_map. This will be changed in the future once the
allocator code is slightly reworked. But people want to use it now.

"nag nag nag nag" dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.67 2008/11/22 17:31:52 deraadt Exp $	*/
d1405 2
d1408 5
a1412 2
	return ((void *)uvm_km_kmemalloc(kmem_map, NULL,
	    pp->pr_alloc->pa_pagesz, kfl));
d1418 3
d1422 1
@


1.67
log
@Do deadbeef-style protection in pools too, by default, even though it it
is a lot slower.  Before release this should be backed out, but for now
we need everyone to run with this and start finding the use-after-free
style bugs this exposes. original version from tedu
ok everyone in the room
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.65 2008/10/31 17:15:30 deraadt Exp $	*/
d75 1
d113 17
d141 3
a143 4
	if (a->ph_page < b->ph_page)
		return (-1);
	else if (a->ph_page > b->ph_page)
		return (1);
d145 1
a145 1
		return (0);
d155 1
a155 1
pr_find_pagehead(struct pool *pp, caddr_t page)
d159 5
a163 1
	if ((pp->pr_roflags & PR_PHINPAGE) != 0)
d165 1
d167 13
a179 1
	tmp.ph_page = page;
d245 32
a276 2
	if (palloc == NULL)
		palloc = &pool_allocator_nointr;
d279 2
d334 1
a334 1
	if (pp->pr_size < palloc->pa_pagesz/16) {
a671 1
	caddr_t page;
a685 2
	page = (caddr_t)((vaddr_t)v & pp->pr_alloc->pa_pagemask);

d697 1
a697 1
	if (__predict_false((ph = pr_find_pagehead(pp, page)) == NULL)) {
a815 5
#ifdef DIAGNOSTIC
	if (((u_long)cp & (pp->pr_alloc->pa_pagesz - 1)) != 0)
		panic("pool_prime_page: %s: unaligned page", pp->pr_wchan);
#endif

d822 1
d1399 30
@


1.66
log
@accidental commit ... backout
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.64 2008/10/24 00:08:43 tedu Exp $	*/
d79 1
a79 1
	int pi_magic;
d155 1
a155 1
     struct pool_pagelist *pq)
d229 1
a229 1
		      (u_long)size);
d317 1
a317 1
	TAILQ_INSERT_TAIL(&pool_head, pp, pr_poollist);
d396 3
d444 1
a444 1
			      &pp->pr_hardlimit_ratecap))
d518 12
a529 4
	if (__predict_false(pi->pi_magic != PI_MAGIC)) {
		panic("pool_do_get(%s): free list modified: magic=%x; page %p;"
		       " item addr %p",
			pp->pr_wchan, pi->pi_magic, ph->ph_page, pi);
d607 6
d643 3
a645 9
#endif
#ifdef DEBUG
	{
		int i, *ip = v;

		for (i = 0; i < pp->pr_size / sizeof(int); i++) {
			*ip++ = PI_MAGIC;
		}
	}
d749 3
d798 3
d1123 2
d1134 3
d1143 3
a1145 4
		printf("pool(%p:%s): page inconsistency: page %p;"
		       " at page head addr %p (p %p)\n", pp,
			pp->pr_wchan, ph->ph_page,
			ph, page);
d1157 5
a1161 6
			printf("pool(%s): free list modified: magic=%x;"
			       " page %p; item ordinal %d;"
			       " addr %p (p %p)\n",
				pp->pr_wchan, pi->pi_magic, ph->ph_page,
				n, pi, page);
			panic("pool");
d1163 11
d1183 2
a1184 3
		       " item ordinal %d; addr %p (p %p)\n", pp,
			pp->pr_wchan, ph->ph_page,
			n, pi, page);
d1196 6
a1201 18
	LIST_FOREACH(ph, &pp->pr_emptypages, ph_pagelist) {
		r = pool_chk_page(pp, label, ph);
		if (r) {
			goto out;
		}
	}
	LIST_FOREACH(ph, &pp->pr_fullpages, ph_pagelist) {
		r = pool_chk_page(pp, label, ph);
		if (r) {
			goto out;
		}
	}
	LIST_FOREACH(ph, &pp->pr_partpages, ph_pagelist) {
		r = pool_chk_page(pp, label, ph);
		if (r) {
			goto out;
		}
	}
a1202 1
out:
@


1.65
log
@kern_sysctl.c
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.63 2008/10/23 23:54:02 tedu Exp $	*/
d155 1
a155 1
    struct pool_pagelist *pq)
d229 1
a229 1
		    (u_long)size);
d317 1
a317 1
	TAILQ_INSERT_HEAD(&pool_head, pp, pr_poollist);
a395 3
#ifdef DIAGNOSTIC
	int i, *ip;
#endif
d441 1
a441 1
		    &pp->pr_hardlimit_ratecap))
d515 4
a518 12
	if (__predict_false(pi->pi_magic != PI_MAGIC))
		panic("pool_do_get(%s): free list modified: "
		    "page %p; item addr %p; offset 0x%x=0x%x",
		    pp->pr_wchan, ph->ph_page, pi, 0, pi->pi_magic);
	for (ip = (int *)pi, i = sizeof(*pi) / sizeof(int);
	    i < pp->pr_size / sizeof(int); i++) {
		if (ip[i] != PI_MAGIC) {
			panic("pool_do_get(%s): free list modified: "
			    "page %p; item addr %p; offset 0x%x=0x%x",
			    pp->pr_wchan, ph->ph_page, pi,
			    i * sizeof(int), ip[i]);
		}
a595 6
#ifdef DIAGNOSTIC
	int i, *ip;
#endif

	if (v == NULL)
		panic("pool_put of NULL");
d626 9
a634 2
	for (ip = v, i = 0; i < pp->pr_size / sizeof(int); i++)
		ip[i] = PI_MAGIC;
a783 8
		{
			int i, *ip = (int *)pi;
	
			for (i = sizeof(*pi)/sizeof(int);
			    i < pp->pr_size / sizeof(int); i++) {
				ip[i] = PI_MAGIC;
			}
		}
a1105 2

		pool_chk(pp, pp->pr_wchan);
a1114 3
#ifdef DIAGNOSTIC
	int i, *ip;
#endif
a1116 1
	printf("checking page %x\n", page);
d1121 4
a1124 3
		printf("pool(%p:%s): page inconsistency: page %p; "
		    "at page head addr %p (p %p)\n",
		    pp, pp->pr_wchan, ph->ph_page, ph, page);
d1136 6
a1141 5
			printf("pool(%s): free list modified: "
			    "page %p; item ordinal %d; addr %p "
			    "(p %p); offset 0x%x=0x%x\n",
			    pp->pr_wchan, ph->ph_page, n, pi, page,
			    0, pi->pi_magic);
a1142 12
		for (ip = (int *)pi, i = sizeof(*pi) / sizeof(int);
		    i < pp->pr_size / sizeof(int); i++) {
			if (ip[i] != PI_MAGIC) {
				printf("pool(%s): free list modified: "
				    "page %p; item ordinal %d; addr %p "
				    "(p %p); offset 0x%x=0x%x\n",
				    pp->pr_wchan, ph->ph_page, n, pi,
				    page,
				    i * sizeof(int), ip[i]);
			}
		}

d1152 3
a1154 2
		    " item ordinal %d; addr %p (p %p)\n", pp,
		    pp->pr_wchan, ph->ph_page, n, pi, page);
d1166 18
a1183 6
	LIST_FOREACH(ph, &pp->pr_emptypages, ph_pagelist)
		r += pool_chk_page(pp, label, ph);
	LIST_FOREACH(ph, &pp->pr_fullpages, ph_pagelist)
		r += pool_chk_page(pp, label, ph);
	LIST_FOREACH(ph, &pp->pr_partpages, ph_pagelist)
		r += pool_chk_page(pp, label, ph);
d1185 1
@


1.64
log
@yet again i prove unable to commit what i really wanted.  spotted by deraadt
@
text
@d155 1
a155 1
     struct pool_pagelist *pq)
d229 1
a229 1
		      (u_long)size);
d317 1
a317 1
	TAILQ_INSERT_TAIL(&pool_head, pp, pr_poollist);
d396 3
d444 1
a444 1
			      &pp->pr_hardlimit_ratecap))
d518 12
a529 4
	if (__predict_false(pi->pi_magic != PI_MAGIC)) {
		panic("pool_do_get(%s): free list modified: magic=%x; page %p;"
		       " item addr %p",
			pp->pr_wchan, pi->pi_magic, ph->ph_page, pi);
d607 6
d643 2
a644 9
#endif
#ifdef DEBUG
	{
		int i, *ip = v;

		for (i = 0; i < pp->pr_size / sizeof(int); i++) {
			*ip++ = PI_MAGIC;
		}
	}
d794 8
d1124 2
d1135 3
d1140 1
d1145 3
a1147 4
		printf("pool(%p:%s): page inconsistency: page %p;"
		       " at page head addr %p (p %p)\n", pp,
			pp->pr_wchan, ph->ph_page,
			ph, page);
d1159 5
a1163 6
			printf("pool(%s): free list modified: magic=%x;"
			       " page %p; item ordinal %d;"
			       " addr %p (p %p)\n",
				pp->pr_wchan, pi->pi_magic, ph->ph_page,
				n, pi, page);
			panic("pool");
d1165 12
d1186 2
a1187 3
		       " item ordinal %d; addr %p (p %p)\n", pp,
			pp->pr_wchan, ph->ph_page,
			n, pi, page);
d1199 6
a1204 18
	LIST_FOREACH(ph, &pp->pr_emptypages, ph_pagelist) {
		r = pool_chk_page(pp, label, ph);
		if (r) {
			goto out;
		}
	}
	LIST_FOREACH(ph, &pp->pr_fullpages, ph_pagelist) {
		r = pool_chk_page(pp, label, ph);
		if (r) {
			goto out;
		}
	}
	LIST_FOREACH(ph, &pp->pr_partpages, ph_pagelist) {
		r = pool_chk_page(pp, label, ph);
		if (r) {
			goto out;
		}
	}
a1205 1
out:
@


1.63
log
@a better fix for the "uvm_km thread runs out of memory" problem.

add a new arg to the backend so it can tell pool to slow down.  when we get
this flag, yield *after* putting the page in the pool's free list.  whatever
we do, don't let the thread sleep.

this makes things better by still letting the thread run when a huge pf
request comes in, but without artificially increasing pressure on the backend
by eating pages without feeding them forward.

ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.62 2008/06/26 05:42:20 ray Exp $	*/
a87 1
	int pi_fillstart;
a519 13
	{
		int i, *ip = v;

		for (i = 0; i < pp->pr_size / sizeof(int); i++) {
			if (++ip < &pi->pi_fillstart)
				continue;
			if (*ip != PI_MAGIC) {
				panic("pool_do_get(%s): free list modified: magic=%x; page %p;"
	       			" item addr %p",
					pp->pr_wchan, *ip, ph->ph_page, pi);
			}
		}
	}
d625 3
a634 1
	pi->pi_magic = PI_MAGIC;
d637 1
a637 1
	TAILQ_INSERT_TAIL(&ph->ph_itemlist, pi, pi_list);
a782 7
		{
			int i, *ip = (int *)pi;
	
			for (i = 0; i < pp->pr_size / sizeof(int); i++) {
				*ip++ = PI_MAGIC;
			}
		}
@


1.62
log
@First pass at removing clauses 3 and 4 from NetBSD licenses.

Not sure what's more surprising: how long it took for NetBSD to
catch up to the rest of the BSDs (including UCB), or the amount of
code that NetBSD has claimed for itself without attributing to the
actual authors.

OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.61 2008/06/14 03:56:41 art Exp $	*/
d88 1
d110 1
a110 1
void	*pool_allocator_alloc(struct pool *, int);
d396 1
d467 1
a467 1
		v = pool_allocator_alloc(pp, flags);
d495 6
d521 13
a638 3
	pi->pi_magic = PI_MAGIC;
#endif
#ifdef DEBUG
d646 1
d649 1
a649 1
	TAILQ_INSERT_HEAD(&ph->ph_itemlist, pi, pi_list);
d710 1
d716 1
a716 1
		cp = pool_allocator_alloc(pp, PR_NOWAIT);
d795 7
d829 1
d835 1
a835 1
		cp = pool_allocator_alloc(pp, PR_NOWAIT);
d1280 1
a1280 3
void	*pool_page_alloc_oldnointr(struct pool *, int);
void	pool_page_free_oldnointr(struct pool *, void *);
void	*pool_page_alloc(struct pool *, int);
d1307 1
a1307 1
pool_allocator_alloc(struct pool *pp, int flags)
d1314 1
a1314 1
	v = pp->pr_alloc->pa_alloc(pp, flags);
d1330 1
a1330 1
pool_page_alloc(struct pool *pp, int flags)
d1334 1
a1334 1
	return (uvm_km_getpage(waitok));
@


1.61
log
@oldnointr pool allocator is no longer used or necessary.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.60 2008/05/16 17:21:36 thib Exp $	*/
a19 7
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the NetBSD
 *	Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
@


1.60
log
@unsigned -> u_int and warnmess -> warnmsg
for pool_sethardlimit.

prodded by and ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.59 2008/05/06 20:57:19 thib Exp $	*/
d1264 4
a1267 6
/* previous nointr.  handles large allocations safely */
struct pool_allocator pool_allocator_oldnointr = {
	pool_page_alloc_oldnointr, pool_page_free_oldnointr, 0,
};
/* safe for interrupts, name preserved for compat
 * this is the default allocator */
a1322 19
}

void *
pool_page_alloc_oldnointr(struct pool *pp, int flags)
{
	boolean_t waitok = (flags & PR_WAITOK) ? TRUE : FALSE;

	splassert(IPL_NONE);

	return ((void *)uvm_km_alloc_poolpage1(kernel_map, uvm.kernel_object,
	    waitok));
}

void
pool_page_free_oldnointr(struct pool *pp, void *v)
{
	splassert(IPL_NONE);

	uvm_km_free_poolpage1(kernel_map, (vaddr_t)v);
@


1.59
log
@Add a PR_ZERO flag for pools, to compliment the M_ZERO
malloc flag, does the same thing.
use it in a few places.

OK tedu@@, "then go ahead. and don't forget the manpage (-:" miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.58 2007/12/11 15:04:58 tedu Exp $	*/
d871 1
a871 1
pool_sethardlimit(struct pool *pp, unsigned n, const char *warnmess, int ratecap)
d881 1
a881 1
	pp->pr_hardlimit_warning = warnmess;
@


1.58
log
@remove an overlooked simple_lock everybody likes to point out to me.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.57 2007/12/11 15:04:01 tedu Exp $	*/
d388 1
a388 1
	if (v)
d390 3
@


1.57
log
@release the pool mutex if we may sleep in the backend
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.56 2007/12/09 00:24:04 tedu Exp $	*/
a870 2

	simple_lock(&pp->pr_slock);
@


1.56
log
@big patch to simplify pool code.

remove pool_cache code.  it was barely used, and quite complex.  it's
silly to have both a "fast" and "faster" allocation interface.  provide
a ctor/dtor interface, and convert the few cache users to use it.  no
caching at this time.

use mutexes to protect pools.  they should be initialized with pool_setipl
if the pool may be used in an interrupt context, without existing spl
protection.

ok art deraadt thib
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.55 2007/08/16 15:18:54 art Exp $	*/
d1291 2
d1294 7
a1300 1
	return (pp->pr_alloc->pa_alloc(pp, flags));
@


1.55
log
@I don't really know what I was thinking when I wrote this. Not only does the
"array" index start at 1, the code also abused index 0 to detect that we
were doing a KERN_POOL_NPOOLS.

Just look at *name == KERN_POOL_NPOOLS instead of using index == 0 for that.

deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.54 2007/05/28 23:46:28 tedu Exp $	*/
a46 1
#include <sys/lock.h>
a52 7
/*
 * XXX - for now.
 */
#ifdef LOCKDEBUG
#define simple_lock_freecheck(a, s) do { /* nothing */ } while (0)
#define simple_lock_only_held(lkp, str) do { /* nothing */ } while (0)
#endif
d71 1
a71 4
static struct pool phpool;

/* This spin lock protects both pool_head */
struct simplelock pool_head_slock;
d106 7
a112 49
/*
 * Pool cache management.
 *
 * Pool caches provide a way for constructed objects to be cached by the
 * pool subsystem.  This can lead to performance improvements by avoiding
 * needless object construction/destruction; it is deferred until absolutely
 * necessary.
 *
 * Caches are grouped into cache groups.  Each cache group references
 * up to 16 constructed objects.  When a cache allocates an object
 * from the pool, it calls the object's constructor and places it into
 * a cache group.  When a cache group frees an object back to the pool,
 * it first calls the object's destructor.  This allows the object to
 * persist in constructed form while freed to the cache.
 *
 * Multiple caches may exist for each pool.  This allows a single
 * object type to have multiple constructed forms.  The pool references
 * each cache, so that when a pool is drained by the pagedaemon, it can
 * drain each individual cache as well.  Each time a cache is drained,
 * the most idle cache group is freed to the pool in its entirety.
 *
 * Pool caches are layed on top of pools.  By layering them, we can avoid
 * the complexity of cache management for pools which would not benefit
 * from it.
 */

/* The cache group pool. */
static struct pool pcgpool;

/* The pool cache group. */
#define	PCG_NOBJECTS		16
struct pool_cache_group {
	TAILQ_ENTRY(pool_cache_group)
		pcg_list;	/* link in the pool cache's group list */
	u_int	pcg_avail;	/* # available objects */
				/* pointers to the objects */
	void	*pcg_objects[PCG_NOBJECTS];
};

void	pool_cache_reclaim(struct pool_cache *);
void	pool_cache_do_invalidate(struct pool_cache *, int,
    void (*)(struct pool *, void *));

int	pool_catchup(struct pool *);
void	pool_prime_page(struct pool *, caddr_t, struct pool_item_header *);
void	pool_update_curpage(struct pool *);
void	pool_do_put(struct pool *, void *);
void	pr_rmpage(struct pool *, struct pool_item_header *,
    struct pool_pagelist *);
d114 1
d117 1
a117 1
void	pool_allocator_free(struct pool *, void *);
d120 3
a122 2
void pool_print_pagelist(struct pool_pagelist *, int (*)(const char *, ...));
void pool_print1(struct pool *, const char *, int (*)(const char *, ...));
d125 1
a125 118

/*
 * Pool log entry. An array of these is allocated in pool_init().
 */
struct pool_log {
	const char	*pl_file;
	long		pl_line;
	int		pl_action;
#define	PRLOG_GET	1
#define	PRLOG_PUT	2
	void		*pl_addr;
};

/* Number of entries in pool log buffers */
#ifndef POOL_LOGSIZE
#define	POOL_LOGSIZE	10
#endif

int pool_logsize = POOL_LOGSIZE;

#ifdef POOL_DIAGNOSTIC
static __inline void
pr_log(struct pool *pp, void *v, int action, const char *file, long line)
{
	int n = pp->pr_curlogentry;
	struct pool_log *pl;

	if ((pp->pr_roflags & PR_LOGGING) == 0)
		return;

	/*
	 * Fill in the current entry. Wrap around and overwrite
	 * the oldest entry if necessary.
	 */
	pl = &pp->pr_log[n];
	pl->pl_file = file;
	pl->pl_line = line;
	pl->pl_action = action;
	pl->pl_addr = v;
	if (++n >= pp->pr_logsize)
		n = 0;
	pp->pr_curlogentry = n;
}

static void
pr_printlog(struct pool *pp, struct pool_item *pi,
    int (*pr)(const char *, ...))
{
	int i = pp->pr_logsize;
	int n = pp->pr_curlogentry;

	if ((pp->pr_roflags & PR_LOGGING) == 0)
		return;

	/*
	 * Print all entries in this pool's log.
	 */
	while (i-- > 0) {
		struct pool_log *pl = &pp->pr_log[n];
		if (pl->pl_action != 0) {
			if (pi == NULL || pi == pl->pl_addr) {
				(*pr)("\tlog entry %d:\n", i);
				(*pr)("\t\taction = %s, addr = %p\n",
				    pl->pl_action == PRLOG_GET ? "get" : "put",
				    pl->pl_addr);
				(*pr)("\t\tfile: %s at line %lu\n",
				    pl->pl_file, pl->pl_line);
			}
		}
		if (++n >= pp->pr_logsize)
			n = 0;
	}
}

static __inline void
pr_enter(struct pool *pp, const char *file, long line)
{

	if (__predict_false(pp->pr_entered_file != NULL)) {
		printf("pool %s: reentrancy at file %s line %ld\n",
		    pp->pr_wchan, file, line);
		printf("         previous entry at file %s line %ld\n",
		    pp->pr_entered_file, pp->pr_entered_line);
		panic("pr_enter");
	}

	pp->pr_entered_file = file;
	pp->pr_entered_line = line;
}

static __inline void
pr_leave(struct pool *pp)
{

	if (__predict_false(pp->pr_entered_file == NULL)) {
		printf("pool %s not entered?\n", pp->pr_wchan);
		panic("pr_leave");
	}

	pp->pr_entered_file = NULL;
	pp->pr_entered_line = 0;
}

static __inline void
pr_enter_check(struct pool *pp, int (*pr)(const char *, ...))
{

	if (pp->pr_entered_file != NULL)
		(*pr)("\n\tcurrently entered from file %s line %ld\n",
		    pp->pr_entered_file, pp->pr_entered_line);
}
#else
#define	pr_log(pp, v, action, file, line)
#define	pr_printlog(pp, pi, pr)
#define	pr_enter(pp, file, line)
#define	pr_leave(pp)
#define	pr_enter_check(pp, pr)
#endif /* POOL_DIAGNOSTIC */
a163 1
	int s;
d184 2
d190 1
a190 3
		if ((pp->pr_roflags & PR_PHINPAGE) == 0) {
			SPLAY_REMOVE(phtree, &pp->pr_phtree, ph);
			s = splhigh();
a191 2
			splx(s);
		}
a210 8
#ifdef POOL_DIAGNOSTIC
	/*
	 * Always log if POOL_DIAGNOSTIC is defined.
	 */
	if (pool_logsize != 0)
		flags |= PR_LOGGING;
#endif

d220 2
a221 7
	if ((palloc->pa_flags & PA_INITIALIZED) == 0) {
		if (palloc->pa_pagesz == 0)
			palloc->pa_pagesz = PAGE_SIZE;

		TAILQ_INIT(&palloc->pa_list);

		simple_lock_init(&palloc->pa_slock);
a223 1
		palloc->pa_flags |= PA_INITIALIZED;
a244 1
	TAILQ_INIT(&pp->pr_cachelist);
a313 16
#ifdef POOL_DIAGNOSTIC
	if (flags & PR_LOGGING) {
		if (kmem_map == NULL ||
		    (pp->pr_log = malloc(pool_logsize * sizeof(struct pool_log),
		     M_TEMP, M_NOWAIT)) == NULL)
			pp->pr_roflags &= ~PR_LOGGING;
		pp->pr_curlogentry = 0;
		pp->pr_logsize = pool_logsize;
	}
#endif

	pp->pr_entered_file = NULL;
	pp->pr_entered_line = 0;

	simple_lock_init(&pp->pr_slock);

d315 1
a316 5
	/*
	 * Initialize private page header pool and cache magazine pool if we
	 * haven't done so yet.
	 * XXX LOCKING.
	 */
d320 1
a320 2
		pool_init(&pcgpool, sizeof(struct pool_cache_group), 0, 0,
		    0, "pcgpool", NULL);
a322 2
	simple_lock_init(&pool_head_slock);

a323 1
	simple_lock(&pool_head_slock);
a324 6
	simple_unlock(&pool_head_slock);

	/* Insert into the list of pools using this allocator. */
	simple_lock(&palloc->pa_slock);
	TAILQ_INSERT_TAIL(&palloc->pa_list, pp, pr_alloc_list);
	simple_unlock(&palloc->pa_slock);
a326 1
#ifdef DIAGNOSTIC
d331 1
a332 1
#endif
a340 10
	struct pool_cache *pc;

	/* Locking order: pool_allocator -> pool */
	simple_lock(&pp->pr_alloc->pa_slock);
	TAILQ_REMOVE(&pp->pr_alloc->pa_list, pp, pr_alloc_list);
	simple_unlock(&pp->pr_alloc->pa_slock);

	/* Destroy all caches for this pool. */
	while ((pc = TAILQ_FIRST(&pp->pr_cachelist)) != NULL)
		pool_cache_destroy(pc);
d343 2
a344 5
	if (pp->pr_nout != 0) {
		pr_printlog(pp, NULL, printf);
		panic("pool_destroy: pool busy: still out: %u",
		    pp->pr_nout);
	}
a353 1
	simple_lock(&pool_head_slock);
a354 6
	simple_unlock(&pool_head_slock);

#ifdef POOL_DIAGNOSTIC
	if ((pp->pr_roflags & PR_LOGGING) != 0)
		free(pp->pr_log, M_TEMP);
#endif
d357 1
a357 1
static struct pool_item_header *
a360 3
	int s;

	LOCK_ASSERT(simple_lock_held(&pp->pr_slock) == 0);
d363 1
a363 1
		ph = (struct pool_item_header *) (storage + pp->pr_phoffset);
a364 1
		s = splhigh();
a365 1
		splx(s);
a374 3
#ifdef POOL_DIAGNOSTIC
_pool_get(struct pool *pp, int flags, const char *file, long line)
#else
d376 19
a394 1
#endif
a404 8
	if (__predict_false(curproc == NULL && /* doing_shutdown == 0 && XXX*/
			    (flags & PR_WAITOK) != 0))
		panic("pool_get: %s:must have NOWAIT", pp->pr_wchan);

#ifdef LOCKDEBUG
	if (flags & PR_WAITOK)
		simple_lock_only_held(NULL, "pool_get(PR_WAITOK)");
#endif
d418 1
a418 4
	simple_lock(&pp->pr_slock);
	pr_enter(pp, file, line);

 startover:
d425 2
a426 5
	if (__predict_false(pp->pr_nout > pp->pr_hardlimit)) {
		pr_leave(pp);
		simple_unlock(&pp->pr_slock);
		panic("pool_get: %s: crossed hard limit", pp->pr_wchan);
	}
d435 1
a435 3
			pr_leave(pp);
			ltsleep(pp, PSWP, pp->pr_wchan, 0, &pp->pr_slock);
			pr_enter(pp, file, line);
a447 3

		pr_leave(pp);
		simple_unlock(&pp->pr_slock);
d460 1
a460 2
			simple_unlock(&pp->pr_slock);
			printf("pool_get: %s: curpage NULL, nitems %u\n",
d462 1
a462 1
			panic("pool_get: nitems inconsistent");
a467 2
		 * Release the pool lock, as the back-end page allocator
		 * may block.
a468 2
		pr_leave(pp);
		simple_unlock(&pp->pr_slock);
a471 2
		simple_lock(&pp->pr_slock);
		pr_enter(pp, file, line);
a476 9
			/*
			 * We were unable to allocate a page or item
			 * header, but we released the lock during
			 * allocation, so perhaps items were freed
			 * back to the pool.  Check for this case.
			 */
			if (pp->pr_curpage != NULL)
				goto startover;

a478 2
				pr_leave(pp);
				simple_unlock(&pp->pr_slock);
d489 1
a489 4
			/* PA_WANTED is already set on the allocator. */
			pr_leave(pp);
			ltsleep(pp, PSWP, pp->pr_wchan, 0, &pp->pr_slock);
			pr_enter(pp, file, line);
d501 1
a501 3
		pr_leave(pp);
		simple_unlock(&pp->pr_slock);
		panic("pool_get: %s: page empty", pp->pr_wchan);
d505 1
a505 3
		pr_leave(pp);
		simple_unlock(&pp->pr_slock);
		printf("pool_get: %s: items on itemlist, nitems %u\n",
d507 1
a507 1
		panic("pool_get: nitems inconsistent");
a510 4
#ifdef POOL_DIAGNOSTIC
	pr_log(pp, v, PRLOG_GET, file, line);
#endif

d513 1
a513 2
		pr_printlog(pp, pi, printf);
		panic("pool_get(%s): free list modified: magic=%x; page %p;"
d528 1
a528 1
			panic("pool_get: nidle inconsistent");
d543 1
a543 3
			pr_leave(pp);
			simple_unlock(&pp->pr_slock);
			panic("pool_get: %s: nmissing inconsistent",
a555 2
	pp->pr_nget++;

d567 2
d570 12
a581 3
	pr_leave(pp);
	simple_unlock(&pp->pr_slock);
	return (v);
d585 1
a585 1
 * Internal version of pool_put().  Pool is already locked/entered.
a600 2
	LOCK_ASSERT(simple_lock_held(&pp->pr_slock));

d610 1
a610 1
		panic("pool_put");
d615 1
a615 2
		pr_printlog(pp, NULL, printf);
		panic("pool_put: %s: page header missing", pp->pr_wchan);
a617 7
#ifdef LOCKDEBUG
	/*
	 * Check if we're freeing a locked simple lock.
	 */
	simple_lock_freecheck((caddr_t)pi, ((caddr_t)pi) + pp->pr_size);
#endif

a635 1
	pp->pr_nput++;
d664 1
a664 2
		if (pp->pr_nidle > pp->pr_maxpages ||
		    (pp->pr_alloc->pa_flags & PA_WANT) != 0) {
a686 36
 * Return resource to the pool; must be called at appropriate spl level
 */
#ifdef POOL_DIAGNOSTIC
void
_pool_put(struct pool *pp, void *v, const char *file, long line)
{

	simple_lock(&pp->pr_slock);
	pr_enter(pp, file, line);

	pr_log(pp, v, PRLOG_PUT, file, line);

	pool_do_put(pp, v);

	pr_leave(pp);
	simple_unlock(&pp->pr_slock);
}
#undef pool_put
#endif /* POOL_DIAGNOSTIC */

void
pool_put(struct pool *pp, void *v)
{

	simple_lock(&pp->pr_slock);

	pool_do_put(pp, v);

	simple_unlock(&pp->pr_slock);
}

#ifdef POOL_DIAGNOSTIC
#define		pool_put(h, v)	_pool_put((h), (v), __FILE__, __LINE__)
#endif

/*
d696 1
a696 2
	simple_lock(&pp->pr_slock);

a699 1
		simple_unlock(&pp->pr_slock);
a702 2
		simple_lock(&pp->pr_slock);

d717 1
a717 1
	simple_unlock(&pp->pr_slock);
d798 1
a798 4
 * Note 1, we never wait for memory here, we let the caller decide what to do.
 *
 * Note 2, we must be called with the pool already locked, and we return
 * with it locked.
a809 3
		 *
		 * XXX: We never wait, so should we bother unlocking
		 * the pool descriptor?
a810 1
		simple_unlock(&pp->pr_slock);
a813 1
		simple_lock(&pp->pr_slock);
a840 2
	simple_lock(&pp->pr_slock);

d846 1
d855 1
a855 2

	simple_unlock(&pp->pr_slock);
a861 2
	simple_lock(&pp->pr_slock);

a864 2

	simple_unlock(&pp->pr_slock);
d886 1
a886 2
	 * In-line version of pool_sethiwat(), because we don't want to
	 * release the lock.
d892 1
a892 3
 done:
	simple_unlock(&pp->pr_slock);

d896 8
a909 3
#ifdef POOL_DIAGNOSTIC
_pool_reclaim(struct pool *pp, const char *file, long line)
#else
a910 1
#endif
a912 1
	struct pool_cache *pc;
a913 5
	int s;

	if (simple_lock_try(&pp->pr_slock) == 0)
		return (0);
	pr_enter(pp, file, line);
d917 1
a917 6
	/*
	 * Reclaim items from the pool's caches.
	 */
	TAILQ_FOREACH(pc, &pp->pr_cachelist, pc_poollist)
		pool_cache_reclaim(pc);

d937 1
a938 2
	pr_leave(pp);
	simple_unlock(&pp->pr_slock);
d944 1
a944 1
		if (pp->pr_roflags & PR_PHINPAGE) {
a945 3
		}
		SPLAY_REMOVE(phtree, &pp->pr_phtree, ph);
		s = splhigh();
a946 1
		splx(s);
a962 9
	int s;

	s = splvm();
	if (simple_lock_try(&pp->pr_slock) == 0) {
		pr("pool %s is locked; try again later\n",
		    pp->pr_wchan);
		splx(s);
		return;
	}
a963 2
	simple_unlock(&pp->pr_slock);
	splx(s);
d992 1
a992 3
	struct pool_cache *pc;
	struct pool_cache_group *pcg;
	int i, print_log = 0, print_pagelist = 0, print_cache = 0;
a995 2
		if (c == 'l')
			print_log = 1;
a997 2
		if (c == 'c')
			print_cache = 1;
d1016 1
a1016 1
		goto skip_pagelist;
a1031 29

skip_pagelist:
	if (print_log == 0)
		goto skip_log;

	(*pr)("\n");
	if ((pp->pr_roflags & PR_LOGGING) == 0)
		(*pr)("\tno log\n");
	else
		pr_printlog(pp, NULL, pr);

skip_log:
	if (print_cache == 0)
		goto skip_cache;

	TAILQ_FOREACH(pc, &pp->pr_cachelist, pc_poollist) {
		(*pr)("\tcache %p: allocfrom %p freeto %p\n", pc,
		    pc->pc_allocfrom, pc->pc_freeto);
		(*pr)("\t    hits %lu misses %lu ngroups %lu nitems %lu\n",
		    pc->pc_hits, pc->pc_misses, pc->pc_ngroups, pc->pc_nitems);
		TAILQ_FOREACH(pcg, &pc->pc_grouplist, pcg_list) {
			(*pr)("\t\tgroup %p: avail %d\n", pcg, pcg->pcg_avail);
			for (i = 0; i < PCG_NOBJECTS; i++)
				(*pr)("\t\t\t%p\n", pcg->pcg_objects[i]);
		}
	}

skip_cache:
	pr_enter_check(pp, pr);
a1162 1
	simple_lock(&pp->pr_slock);
a1182 1
	simple_unlock(&pp->pr_slock);
a1187 282
 * pool_cache_init:
 *
 *	Initialize a pool cache.
 *
 *	NOTE: If the pool must be protected from interrupts, we expect
 *	to be called at the appropriate interrupt priority level.
 */
void
pool_cache_init(struct pool_cache *pc, struct pool *pp,
    int (*ctor)(void *, void *, int),
    void (*dtor)(void *, void *),
    void *arg)
{

	TAILQ_INIT(&pc->pc_grouplist);
	simple_lock_init(&pc->pc_slock);

	pc->pc_allocfrom = NULL;
	pc->pc_freeto = NULL;
	pc->pc_pool = pp;

	pc->pc_ctor = ctor;
	pc->pc_dtor = dtor;
	pc->pc_arg  = arg;

	pc->pc_hits   = 0;
	pc->pc_misses = 0;

	pc->pc_ngroups = 0;

	pc->pc_nitems = 0;

	simple_lock(&pp->pr_slock);
	TAILQ_INSERT_TAIL(&pp->pr_cachelist, pc, pc_poollist);
	simple_unlock(&pp->pr_slock);
}

/*
 * pool_cache_destroy:
 *
 *	Destroy a pool cache.
 */
void
pool_cache_destroy(struct pool_cache *pc)
{
	struct pool *pp = pc->pc_pool;

	/* First, invalidate the entire cache. */
	pool_cache_invalidate(pc);

	/* ...and remove it from the pool's cache list. */
	simple_lock(&pp->pr_slock);
	TAILQ_REMOVE(&pp->pr_cachelist, pc, pc_poollist);
	simple_unlock(&pp->pr_slock);
}

static __inline void *
pcg_get(struct pool_cache_group *pcg)
{
	void *object;
	u_int idx;

	KASSERT(pcg->pcg_avail <= PCG_NOBJECTS);
	KASSERT(pcg->pcg_avail != 0);
	idx = --pcg->pcg_avail;

	KASSERT(pcg->pcg_objects[idx] != NULL);
	object = pcg->pcg_objects[idx];
	pcg->pcg_objects[idx] = NULL;

	return (object);
}

static __inline void
pcg_put(struct pool_cache_group *pcg, void *object)
{
	u_int idx;

	KASSERT(pcg->pcg_avail < PCG_NOBJECTS);
	idx = pcg->pcg_avail++;

	KASSERT(pcg->pcg_objects[idx] == NULL);
	pcg->pcg_objects[idx] = object;
}

/*
 * pool_cache_get:
 *
 *	Get an object from a pool cache.
 */
void *
pool_cache_get(struct pool_cache *pc, int flags)
{
	struct pool_cache_group *pcg;
	void *object;

#ifdef LOCKDEBUG
	if (flags & PR_WAITOK)
		simple_lock_only_held(NULL, "pool_cache_get(PR_WAITOK)");
#endif

	simple_lock(&pc->pc_slock);

	if ((pcg = pc->pc_allocfrom) == NULL) {
		TAILQ_FOREACH(pcg, &pc->pc_grouplist, pcg_list) {
			if (pcg->pcg_avail != 0) {
				pc->pc_allocfrom = pcg;
				goto have_group;
			}
		}

		/*
		 * No groups with any available objects.  Allocate
		 * a new object, construct it, and return it to
		 * the caller.  We will allocate a group, if necessary,
		 * when the object is freed back to the cache.
		 */
		pc->pc_misses++;
		simple_unlock(&pc->pc_slock);
		object = pool_get(pc->pc_pool, flags);
		if (object != NULL && pc->pc_ctor != NULL) {
			if ((*pc->pc_ctor)(pc->pc_arg, object, flags) != 0) {
				pool_put(pc->pc_pool, object);
				return (NULL);
			}
		}
		return (object);
	}

 have_group:
	pc->pc_hits++;
	pc->pc_nitems--;
	object = pcg_get(pcg);

	if (pcg->pcg_avail == 0)
		pc->pc_allocfrom = NULL;

	simple_unlock(&pc->pc_slock);

	return (object);
}

/*
 * pool_cache_put:
 *
 *	Put an object back to the pool cache.
 */
void
pool_cache_put(struct pool_cache *pc, void *object)
{
	struct pool_cache_group *pcg;
	int s;

	simple_lock(&pc->pc_slock);

	if ((pcg = pc->pc_freeto) == NULL) {
		TAILQ_FOREACH(pcg, &pc->pc_grouplist, pcg_list) {
			if (pcg->pcg_avail != PCG_NOBJECTS) {
				pc->pc_freeto = pcg;
				goto have_group;
			}
		}

		/*
		 * No empty groups to free the object to.  Attempt to
		 * allocate one.
		 */
		simple_unlock(&pc->pc_slock);
		s = splvm();
		pcg = pool_get(&pcgpool, PR_NOWAIT);
		splx(s);
		if (pcg != NULL) {
			memset(pcg, 0, sizeof(*pcg));
			simple_lock(&pc->pc_slock);
			pc->pc_ngroups++;
			TAILQ_INSERT_TAIL(&pc->pc_grouplist, pcg, pcg_list);
			if (pc->pc_freeto == NULL)
				pc->pc_freeto = pcg;
			goto have_group;
		}

		/*
		 * Unable to allocate a cache group; destruct the object
		 * and free it back to the pool.
		 */
		pool_cache_destruct_object(pc, object);
		return;
	}

 have_group:
	pc->pc_nitems++;
	pcg_put(pcg, object);

	if (pcg->pcg_avail == PCG_NOBJECTS)
		pc->pc_freeto = NULL;

	simple_unlock(&pc->pc_slock);
}

/*
 * pool_cache_destruct_object:
 *
 *	Force destruction of an object and its release back into
 *	the pool.
 */
void
pool_cache_destruct_object(struct pool_cache *pc, void *object)
{

	if (pc->pc_dtor != NULL)
		(*pc->pc_dtor)(pc->pc_arg, object);
	pool_put(pc->pc_pool, object);
}

/*
 * pool_cache_do_invalidate:
 *
 *	This internal function implements pool_cache_invalidate() and
 *	pool_cache_reclaim().
 */
void
pool_cache_do_invalidate(struct pool_cache *pc, int free_groups,
    void (*putit)(struct pool *, void *))
{
	struct pool_cache_group *pcg, *npcg;
	void *object;
	int s;

	for (pcg = TAILQ_FIRST(&pc->pc_grouplist); pcg != NULL;
	     pcg = npcg) {
		npcg = TAILQ_NEXT(pcg, pcg_list);
		while (pcg->pcg_avail != 0) {
			pc->pc_nitems--;
			object = pcg_get(pcg);
			if (pcg->pcg_avail == 0 && pc->pc_allocfrom == pcg)
				pc->pc_allocfrom = NULL;
			if (pc->pc_dtor != NULL)
				(*pc->pc_dtor)(pc->pc_arg, object);
			(*putit)(pc->pc_pool, object);
		}
		if (free_groups) {
			pc->pc_ngroups--;
			TAILQ_REMOVE(&pc->pc_grouplist, pcg, pcg_list);
			if (pc->pc_freeto == pcg)
				pc->pc_freeto = NULL;
			s = splvm();
			pool_put(&pcgpool, pcg);
			splx(s);
		}
	}
}

/*
 * pool_cache_invalidate:
 *
 *	Invalidate a pool cache (destruct and release all of the
 *	cached objects).
 */
void
pool_cache_invalidate(struct pool_cache *pc)
{

	simple_lock(&pc->pc_slock);
	pool_cache_do_invalidate(pc, 0, pool_put);
	simple_unlock(&pc->pc_slock);
}

/*
 * pool_cache_reclaim:
 *
 *	Reclaim a pool cache for pool_reclaim().
 */
void
pool_cache_reclaim(struct pool_cache *pc)
{

	simple_lock(&pc->pc_slock);
	pool_cache_do_invalidate(pc, 1, pool_do_put);
	simple_unlock(&pc->pc_slock);
}

/*
a1222 1
	simple_lock(&pool_head_slock);
a1231 1
	simple_unlock(&pool_head_slock);
a1298 1
	int s;
a1300 20

	s = splvm();
	simple_lock(&pa->pa_slock);
	if ((pa->pa_flags & PA_WANT) == 0) {
		simple_unlock(&pa->pa_slock);
		splx(s);
		return;
	}

	TAILQ_FOREACH(pp, &pa->pa_list, pr_alloc_list) {
		simple_lock(&pp->pr_slock);
		if ((pp->pr_flags & PR_WANTED) != 0) {
			pp->pr_flags &= ~PR_WANTED;
			wakeup(pp);
		}
		simple_unlock(&pp->pr_slock);
	}
	pa->pa_flags &= ~PA_WANT;
	simple_unlock(&pa->pa_slock);
	splx(s);
@


1.54
log
@some remnants of the timestamping code i missed
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.53 2007/05/28 19:18:45 tedu Exp $	*/
d1916 1
a1916 1
	if (lookfor != 0 && foundpool == NULL)
@


1.53
log
@remove time from pool header.  it slows us down quite a bit, and it's
probably a better idea to just let reclaim have the emptypages.  we can
still use the partial pages.
this lets dlg sling many many more packets
ok dlg henning miod pedro ryan
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.52 2007/05/28 17:55:56 tedu Exp $	*/
a80 3
/* # of seconds to retain page after last use */
int pool_inactive_time = 10;

a1247 1
	struct timeval curtime;
a1261 2

	microuptime(&curtime);
@


1.52
log
@add a pool_setipl function, which allows setting an appropriate ipl
for splassert inside pool_get and pool_put (DIAGNOSTIC only)
ok miod pedro thib
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.51 2007/04/23 09:27:59 art Exp $	*/
a95 1
	struct timeval		ph_time;	/* last referenced */
a952 8

			/*
			 * Update the timestamp on the page.  A page must
			 * be idle for some period of time before it can
			 * be reclaimed by the pagedaemon.  This minimizes
			 * ping-pong'ing for memory.
			 */
			microuptime(&ph->ph_time);
a1070 1
	memset(&ph->ph_time, 0, sizeof(ph->ph_time));
a1252 1
	struct timeval diff;
a1276 3
		timersub(&curtime, &ph->ph_time, &diff);
		if (diff.tv_sec < pool_inactive_time)
			continue;
d1342 2
a1343 4
		(*pr)("\t\tpage %p, nmissing %d, time %lu,%lu\n",
		    ph->ph_page, ph->ph_nmissing,
		    (u_long)ph->ph_time.tv_sec,
		    (u_long)ph->ph_time.tv_usec);
@


1.51
log
@Clean up an obsolete allocator.
miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.50 2007/04/12 21:47:45 miod Exp $	*/
d520 2
d547 8
d634 2
d879 3
@


1.50
log
@Allow machine-dependant overrides for the ``deadbeef'' sentinel values,
and make sure that nothing can ever be mapped at theses addresses.

Only i386 overrides the default for now.

From mickey@@, ok art@@ miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.49 2007/04/11 12:10:42 art Exp $	*/
a1946 2
void	*pool_page_alloc_kmem(struct pool *, int);
void	pool_page_free_kmem(struct pool *, void *);
a1951 4
/* old default allocator, interrupt safe */
struct pool_allocator pool_allocator_kmem = {
	pool_page_alloc_kmem, pool_page_free_kmem, 0,
};
a2025 15
}

void *
pool_page_alloc_kmem(struct pool *pp, int flags)
{
	boolean_t waitok = (flags & PR_WAITOK) ? TRUE : FALSE;

	return ((void *)uvm_km_alloc_poolpage1(kmem_map, NULL, waitok));
}

void
pool_page_free_kmem(struct pool *pp, void *v)
{

	uvm_km_free_poolpage1(kmem_map, (vaddr_t)v);
@


1.49
log
@Instead of managing pages for intrsafe maps in special objects (aka.
kmem_object) just so that we can remove them, just use pmap_extract
to get the pages to free and simplify a lot of code to not deal with
the list of intrsafe maps, intrsafe objects, etc.

miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.48 2006/11/17 11:50:09 jmc Exp $	*/
d103 3
d107 1
@


1.48
log
@typos from bret lambert;
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.47 2006/05/20 18:29:23 mickey Exp $	*/
d2035 1
a2035 2
	return ((void *)uvm_km_alloc_poolpage1(kmem_map, uvmexp.kmem_object,
	    waitok));
@


1.47
log
@add show all pools command listing all pools as vmstat -m does; miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.46 2006/05/07 20:06:50 tedu Exp $	*/
d112 1
a112 1
 * Every pool get a unique serial number assigned to it. If this counter
d542 1
a542 1
 * De-commision a pool resource.
d1874 1
a1874 1
 * kern.pool.name.<pool#> - the name for pool#.[6~
@


1.46
log
@remove drain hooks from pool.
1.  drain hooks and lists of allocators make the code complicated
2.  the only hooks in the system are the mbuf reclaim routines
3.  if reclaim is actually able to put a meaningful amount of memory back
in the system, i think something else is dicked up.  ie, if reclaiming
your ip fragment buffers makes the difference thrashing swap and not,
your system is in a load of trouble.
4.  it's a scary amount of code running with very weird spl requirements
and i'd say it's pretty much totally untested.  raise your hand if your
router is running at the edge of swap.
5.  the reclaim stuff goes back to when mbufs lived in a tiny vm_map and
you could run out of va.  that's very unlikely (like impossible) now.
ok/tested pedro krw sturm
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.45 2004/07/29 09:18:17 mickey Exp $	*/
d1304 4
d1431 72
@


1.45
log
@proper condition for freeing a page and fix a comment appropriately; art@@ tedu@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.44 2004/07/20 23:47:08 art Exp $	*/
d84 1
a84 4
/* Next candidate for drainage (see pool_drain()) */
static struct pool	*drainpp;

/* This spin lock protects both pool_head and drainpp. */
a449 2
	pp->pr_drain_hook = NULL;
	pp->pr_drain_hook_arg = NULL;
a575 3
	if (drainpp == pp) {
		drainpp = NULL;
	}
a583 12
void
pool_set_drain_hook(struct pool *pp, void (*fn)(void *, int), void *arg)
{
	/* XXX no locking -- must be used just after pool_init() */
#ifdef DIAGNOSTIC
	if (pp->pr_drain_hook != NULL)
		panic("pool_set_drain_hook(%s): already set", pp->pr_wchan);
#endif
	pp->pr_drain_hook = fn;
	pp->pr_drain_hook_arg = arg;
}

a657 15
		if (pp->pr_drain_hook != NULL) {
			/*
			 * Since the drain hook is going to free things
			 * back to the pool, unlock, call hook, re-lock
			 * and check hardlimit condition again.
			 */
			pr_leave(pp);
			simple_unlock(&pp->pr_slock);
			(*pp->pr_drain_hook)(pp->pr_drain_hook_arg, flags);
			simple_lock(&pp->pr_slock);
			pr_enter(pp, file, line);
			if (pp->pr_nout < pp->pr_hardlimit)
				goto startover;
		}

d912 1
a912 1
		wakeup((caddr_t)pp);
a1246 7
	if (pp->pr_drain_hook != NULL) {
		/*
		 * The drain hook must be called with the pool unlocked.
		 */
		(*pp->pr_drain_hook)(pp->pr_drain_hook_arg, PR_NOWAIT);
	}

a1302 27

/*
 * Drain pools, one at a time.
 *
 * Note, we must never be called from an interrupt context.
 */
void
pool_drain(void *arg)
{
	struct pool *pp;
	int s;

	pp = NULL;
	s = splvm();
	simple_lock(&pool_head_slock);
	if (drainpp == NULL) {
		drainpp = TAILQ_FIRST(&pool_head);
	}
	if (drainpp) {
		pp = drainpp;
		drainpp = TAILQ_NEXT(pp, pr_poollist);
	}
	simple_unlock(&pool_head_slock);
	pool_reclaim(pp);
	splx(s);
}

a1865 1
 * and any additional draining that might be needed.
d1904 1
a1904 1
pool_allocator_alloc(struct pool *org, int flags)
a1905 4
	struct pool_allocator *pa = org->pr_alloc;
	int freed;
	void *res;
	int s;
d1907 1
a1907 24
	do {
		if ((res = (*pa->pa_alloc)(org, flags)) != NULL)
			return (res);
		if ((flags & PR_WAITOK) == 0) {
			/*
			 * We only run the drain hook here if PR_NOWAIT.
			 * In other cases the hook will be run in
			 * pool_reclaim.
			 */
			if (org->pr_drain_hook != NULL) {
				(*org->pr_drain_hook)(org->pr_drain_hook_arg,
				    flags);
				if ((res = (*pa->pa_alloc)(org, flags)) != NULL)
					return (res);
			}
			break;
		}
		s = splvm();
		simple_lock(&pa->pa_slock);
		freed = pool_allocator_drain(pa, org, 1);
		simple_unlock(&pa->pa_slock);
		splx(s);
	} while (freed);
	return (NULL);
a1936 47
}

/*
 * Drain all pools, except 'org', that use this allocator.
 *
 * Must be called at appropriate spl level and with the allocator locked.
 *
 * We do this to reclaim va space. pa_alloc is responsible
 * for waiting for physical memory.
 * XXX - we risk looping forever if start if someone calls
 *  pool_destroy on 'start'. But there is no other way to
 *  have potentially sleeping pool_reclaim, non-sleeping
 *  locks on pool_allocator and some stirring of drained
 *  pools in the allocator.
 * XXX - maybe we should use pool_head_slock for locking
 *  the allocators?
 */
int
pool_allocator_drain(struct pool_allocator *pa, struct pool *org, int need)
{
	struct pool *pp, *start;
	int freed;

	freed = 0;

	pp = start = TAILQ_FIRST(&pa->pa_list);
	do {
		TAILQ_REMOVE(&pa->pa_list, pp, pr_alloc_list);
		TAILQ_INSERT_TAIL(&pa->pa_list, pp, pr_alloc_list);
		if (pp == org)
			continue;
		simple_unlock(&pa->pa_slock);
		freed = pool_reclaim(pp);
		simple_lock(&pa->pa_slock);
	} while ((pp = TAILQ_FIRST(&pa->pa_list)) != start && (freed < need));

	if (!freed) {
		/*
		 * We set PA_WANT here, the caller will most likely
		 * sleep waiting for pages (if not, this won't hurt
		 * that much) and there is no way to set this in the
		 * caller without violating locking order.
		 */
		pa->pa_flags |= PA_WANT;
	}

	return (freed);
@


1.44
log
@ifdef DDB a few functions only used (or usable) from DDB.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.43 2004/06/24 19:35:24 tholo Exp $	*/
d964 1
a964 1
		if (pp->pr_npages > pp->pr_maxpages ||
@


1.43
log
@This moves access to wall and uptime variables in MI code,
encapsulating all such access into wall-defined functions
that makes sure locking is done as needed.

It also cleans up some uses of wall time vs. uptime some
places, but there is sure to be more of these needed as
well, particularily in MD code.  Also, many current calls
to microtime() should probably be changed to getmicrotime(),
or to the {,get}microuptime() versions.

ok art@@ deraadt@@ aaron@@ matthieu@@ beck@@ sturm@@ millert@@ others
"Oh, that is not your problem!" from miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.42 2004/06/13 21:49:26 niklas Exp $	*/
d174 1
d177 1
d1372 1
d1579 1
@


1.42
log
@debranch SMP, have fun
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a882 1
	int s;
d975 1
a975 3
			s = splclock();
			ph->ph_time = mono_time;
			splx(s);
d1299 1
a1299 3
	s = splclock();
	curtime = mono_time;
	splx(s);
@


1.41
log
@rearrange the allocators we provide for general use.
the new one remains the default and _nointr.
_kmem is restored to its former position, and _oldnointr is
introduced.
this is to allow some pool users who don't like the new allocator
to continue working.  testing/ok beck@@ cedric@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.40 2004/05/27 04:55:27 tedu Exp $	*/
a56 1
#define SIMPLELOCK_INITIALIZER { SLOCK_UNLOCKED }
d88 1
a88 1
struct simplelock pool_head_slock = SIMPLELOCK_INITIALIZER;
d531 2
d2066 3
a2068 3
		simple_unlock(&pa->pa_list);
		freed = pool_reclaim(pp)
		simple_lock(&pa->pa_list);
@


1.40
log
@change uvm_km_getpage to take waitok argument and sleep if appropriate.
change both the nointr and default pool allocators to using uvm_km_getpage.
change pools to default to a maxpages value of 8, so they hoard less memory.
change mbuf pools to use default pool allocator.
pools are now more efficient, use less of kmem_map, and a bit faster.
tested mcbride, deraadt, pedro, drahn, miod to work everywhere
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.39 2003/11/18 06:08:18 tedu Exp $	*/
d400 1
a400 1
		palloc = &pool_allocator_kmem;
a1935 5
 *
 * We provide two standard allocators.
 *  pool_alloc_kmem - the default used when no allocator is specified.
 *  pool_alloc_nointr - used for pools that will not be accessed in
 *   interrupt context.
d1937 4
a1942 2
void	*pool_page_alloc_nointr(struct pool *, int);
void	pool_page_free_nointr(struct pool *, void *);
d1944 1
d1946 5
a1950 1
	pool_page_alloc, pool_page_free, 0,
d1952 2
d1955 1
a1955 1
	pool_page_alloc_nointr, pool_page_free_nointr, 0,
d2099 17
a2115 1
pool_page_alloc_nointr(struct pool *pp, int flags)
d2121 2
a2122 1
	return (uvm_km_getpage(waitok));
d2126 1
a2126 1
pool_page_free_nointr(struct pool *pp, void *v)
d2130 1
a2130 1
	uvm_km_putpage(v);
@


1.39
log
@faster pools.  split pagelist into full, partial, and empty so we find what
we're looking for.  change small page_header hash table to a splay tree.
from Chuck Silvers.
tested by brad grange henning mcbride naddy otto
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.38 2002/12/20 07:48:00 art Exp $	*/
d437 1
a437 1
	pp->pr_maxpages = UINT_MAX;
d2084 1
a2084 2
	return ((void *)uvm_km_alloc_poolpage1(kmem_map, uvmexp.kmem_object,
	    waitok));
d2091 1
a2091 1
	uvm_km_free_poolpage1(kmem_map, (vaddr_t)v);
d2101 1
a2101 2
	return ((void *)uvm_km_alloc_poolpage1(kernel_map, uvm.kernel_object,
	    waitok));
d2109 1
a2109 1
	uvm_km_free_poolpage1(kernel_map, (vaddr_t)v);
@


1.38
log
@ - Clean up the defines in pool.h
 - Allow a pool to be initialized with PR_DEBUG which will cause it to
   allocate with malloc_debug.
 - sprinkle some splassert.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.37 2002/12/11 06:20:31 art Exp $	*/
d66 8
a73 6
 * Memory is allocated in pages which are split into pieces according
 * to the pool item size. Each page is kept on a list headed by `pr_pagelist'
 * in the pool structure and the individual pool items are on a linked list
 * headed by `ph_itemlist' in each page header. The memory for building
 * the page list is either taken from the allocated pages themselves (for
 * small pool items) or taken from an internal pool of page headers (`phpool').
d93 1
a93 1
	TAILQ_ENTRY(pool_item_header)
d96 2
a97 2
	LIST_ENTRY(pool_item_header)
				ph_hashlist;	/* Off-page page headers */
a101 1
TAILQ_HEAD(pool_pagelist,pool_item_header);
a111 3
#define	PR_HASH_INDEX(pp,addr) \
	(((u_long)(addr) >> (pp)->pr_alloc->pa_pageshift) & (PR_HASHTABSIZE - 1))

d166 1
d170 1
d175 1
d178 1
d297 14
d317 1
a317 1
	struct pool_item_header *ph;
d322 3
a324 7
	for (ph = LIST_FIRST(&pp->pr_hashtab[PR_HASH_INDEX(pp, page)]);
	     ph != NULL;
	     ph = LIST_NEXT(ph, ph_hashlist)) {
		if (ph->ph_page == page)
			return (ph);
	}
	return (NULL);
d354 1
a354 1
	TAILQ_REMOVE(&pp->pr_pagelist, ph, ph_pagelist);
d356 1
a356 1
		TAILQ_INSERT_HEAD(pq, ph, ph_pagelist);
d360 1
a360 1
			LIST_REMOVE(ph, ph_hashlist);
d369 1
a369 12
	if (pp->pr_curpage == ph) {
		/*
		 * Find a new non-empty page header, if any.
		 * Start search from the page head, to increase the
		 * chance for "high water" pages to be freed.
		 */
		TAILQ_FOREACH(ph, &pp->pr_pagelist, ph_pagelist)
			if (TAILQ_FIRST(&ph->ph_itemlist) != NULL)
				break;

		pp->pr_curpage = ph;
	}
d382 1
a382 1
	int off, slack, i;
d429 3
a431 1
	TAILQ_INIT(&pp->pr_pagelist);
d474 1
a474 3
		for (i = 0; i < PR_HASHTABSIZE; i++) {
			LIST_INIT(&pp->pr_hashtab[i]);
		}
d570 1
a570 1
	while ((ph = TAILQ_FIRST(&pp->pr_pagelist)) != NULL)
d572 2
d601 1
a601 1
static __inline struct pool_item_header *
a786 1

d827 7
d836 1
a836 1
	if (TAILQ_FIRST(&ph->ph_itemlist) == NULL) {
d846 2
a847 9
		 * Find a new non-empty page header, if any.
		 * Start search from the page head, to increase
		 * the chance for "high water" pages to be freed.
		 *
		 * Migrate empty pages to the end of the list.  This
		 * will speed the update of curpage as pages become
		 * idle.  Empty pages intermingled with idle pages
		 * is no big deal.  As soon as a page becomes un-empty,
		 * it will move back to the head of the list.
d849 3
a851 7
		TAILQ_REMOVE(&pp->pr_pagelist, ph, ph_pagelist);
		TAILQ_INSERT_TAIL(&pp->pr_pagelist, ph, ph_pagelist);
		TAILQ_FOREACH(ph, &pp->pr_pagelist, ph_pagelist)
			if (TAILQ_FIRST(&ph->ph_itemlist) != NULL)
				break;

		pp->pr_curpage = ph;
d950 1
a950 1
	 * If this page is now complete, do one of two things:
d952 2
a953 2
	 *	(1) If we have more pages than the page high water
	 *	    mark, free the page back to the system.
d955 4
a958 7
	 *	(2) Move it to the end of the page list, so that
	 *	    we minimize our chances of fragmenting the
	 *	    pool.  Idle pages migrate to the end (along with
	 *	    completely empty pages, so that we find un-empty
	 *	    pages more quickly when we update curpage) of the
	 *	    list so they can be more easily swept up by
	 *	    the pagedaemon when pages are scarce.
d966 2
a967 2
			TAILQ_REMOVE(&pp->pr_pagelist, ph, ph_pagelist);
			TAILQ_INSERT_TAIL(&pp->pr_pagelist, ph, ph_pagelist);
a977 14

			/*
			 * Update the current page pointer.  Just look for
			 * the first page with any free items.
			 *
			 * XXX: Maybe we want an option to look for the
			 * page with the fewest available items, to minimize
			 * fragmentation?
			 */
			TAILQ_FOREACH(ph, &pp->pr_pagelist, ph_pagelist)
				if (TAILQ_FIRST(&ph->ph_itemlist) != NULL)
					break;

			pp->pr_curpage = ph;
d979 1
d981 1
d983 4
a986 4
	 * If the page has just become un-empty, move it to the head of
	 * the list, and make it the current page.  The next allocation
	 * will get the item from this page, instead of further fragmenting
	 * the pool.
d989 2
a990 2
		TAILQ_REMOVE(&pp->pr_pagelist, ph, ph_pagelist);
		TAILQ_INSERT_HEAD(&pp->pr_pagelist, ph, ph_pagelist);
a1088 4
	if ((pp->pr_roflags & PR_PHINPAGE) == 0)
		LIST_INSERT_HEAD(&pp->pr_hashtab[PR_HASH_INDEX(pp, cp)],
		    ph, ph_hashlist);

d1092 1
a1092 1
	TAILQ_INSERT_HEAD(&pp->pr_pagelist, ph, ph_pagelist);
d1097 2
d1146 1
a1146 1
 * is used to catch up nitmes with the low water mark.
d1186 10
d1279 1
d1293 1
a1293 1
	TAILQ_INIT(&pq);
d1305 2
a1306 2
	for (ph = TAILQ_FIRST(&pp->pr_pagelist); ph != NULL; ph = phnext) {
		phnext = TAILQ_NEXT(ph, ph_pagelist);
d1312 4
a1315 5
		if (ph->ph_nmissing == 0) {
			struct timeval diff;
			timersub(&curtime, &ph->ph_time, &diff);
			if (diff.tv_sec < pool_inactive_time)
				continue;
d1317 7
a1323 7
			/*
			 * If freeing this page would put us below
			 * the low water mark, stop now.
			 */
			if ((pp->pr_nitems - pp->pr_itemsperpage) <
			    pp->pr_minitems)
				break;
d1325 1
a1325 2
			pr_rmpage(pp, ph, &pq);
		}
d1330 1
a1330 1
	if (TAILQ_EMPTY(&pq))
d1332 2
a1333 2
	while ((ph = TAILQ_FIRST(&pq)) != NULL) {
		TAILQ_REMOVE(&pq, ph, ph_pagelist);
d1338 1
a1338 1
		LIST_REMOVE(ph, ph_hashlist);
d1395 24
a1423 3
#ifdef DIAGNOSTIC
	struct pool_item *pi;
#endif
d1454 10
a1463 16
	if ((ph = TAILQ_FIRST(&pp->pr_pagelist)) != NULL)
		(*pr)("\n\tpage list:\n");
	for (; ph != NULL; ph = TAILQ_NEXT(ph, ph_pagelist)) {
		(*pr)("\t\tpage %p, nmissing %d, time %lu,%lu\n",
		    ph->ph_page, ph->ph_nmissing,
		    (u_long)ph->ph_time.tv_sec,
		    (u_long)ph->ph_time.tv_usec);
#ifdef DIAGNOSTIC
		TAILQ_FOREACH(pi, &ph->ph_itemlist, pi_list) {
			if (pi->pi_magic != PI_MAGIC) {
				(*pr)("\t\t\titem %p, magic 0x%x\n",
				    pi, pi->pi_magic);
			}
		}
#endif
	}
d1469 1
a1469 2
 skip_pagelist:

d1479 1
a1479 2
 skip_log:

d1495 1
a1495 2
 skip_cache:

d1500 1
a1500 1
pool_chk(struct pool *pp, const char *label)
d1502 15
a1516 2
	struct pool_item_header *ph;
	int r = 0;
d1518 3
a1520 1
	simple_lock(&pp->pr_slock);
d1522 2
a1523 8
	TAILQ_FOREACH(ph, &pp->pr_pagelist, ph_pagelist) {
		struct pool_item *pi;
		int n;
		caddr_t page;

		page = (caddr_t)((vaddr_t)ph & pp->pr_alloc->pa_pagemask);
		if (page != ph->ph_page &&
		    (pp->pr_roflags & PR_PHINPAGE) != 0) {
d1526 6
a1531 6
			printf("pool(%p:%s): page inconsistency: page %p;"
			       " at page head addr %p (p %p)\n", pp,
				pp->pr_wchan, ph->ph_page,
				ph, page);
			r++;
			goto out;
d1533 5
d1539 10
a1548 3
		for (pi = TAILQ_FIRST(&ph->ph_itemlist), n = 0;
		     pi != NULL;
		     pi = TAILQ_NEXT(pi,pi_list), n++) {
d1550 5
a1554 15
#ifdef DIAGNOSTIC
			if (pi->pi_magic != PI_MAGIC) {
				if (label != NULL)
					printf("%s: ", label);
				printf("pool(%s): free list modified: magic=%x;"
				       " page %p; item ordinal %d;"
				       " addr %p (p %p)\n",
					pp->pr_wchan, pi->pi_magic, ph->ph_page,
					n, pi, page);
				panic("pool");
			}
#endif
			page = (caddr_t)((vaddr_t)pi & pp->pr_alloc->pa_pagemask);
			if (page == ph->ph_page)
				continue;
d1556 16
a1571 7
			if (label != NULL)
				printf("%s: ", label);
			printf("pool(%p:%s): page inconsistency: page %p;"
			       " item ordinal %d; addr %p (p %p)\n", pp,
				pp->pr_wchan, ph->ph_page,
				n, pi, page);
			r++;
d1575 1
@


1.37
log
@ - Call uvm_km_alloc_poolpage1 directly, no need to go through the macro.
 - uvm_km_alloc_poolpage1 has its own spl protection, no need to add
   additional layer around it.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.36 2002/10/27 21:31:56 art Exp $	*/
d391 4
d632 2
d644 11
d886 7
d1143 2
d2077 2
d2086 2
@


1.36
log
@Reduce diff to NetBSD.
One relevant change: round up pool element size to the alignment.
VS: ----------------------------------------------------------------------
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.35 2002/10/14 20:09:41 art Exp $	*/
a2033 2
	void *ret;
	int s;
d2035 2
a2036 4
	s = splvm();
	ret = (void *)uvm_km_alloc_poolpage(waitok);
	splx(s);
	return (ret);
a2041 1
	int s;
d2043 1
a2043 3
	s = splvm();
	uvm_km_free_poolpage((vaddr_t)v);
	splx(s);
@


1.35
log
@- Do not try to drain other pools if PR_NOWAIT and the allocator can't
 give us pages. PR_NOWAIT most likely means "hey, we're coming from an
 interrupt, don't mess with stuff that doesn't have proper protection".

- pool_allocator_free is called in too many places so I don't feel
 comfortable without that added protection from splvm (and besides,
 pool_allocator_free is rarely called anyway, so the extra spl will be
 unnoticeable). It shouldn't matter when fiddling with those flags, but
 you never know.

- Remove a wakeup without a matching tsleep. It's a left-over from
 some other code path that I've been investigating when reworking the
 pool a while ago and it should have been removed before that commit.

deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.34 2002/10/13 18:26:12 krw Exp $	*/
d162 2
a163 2
void pool_cache_reclaim(struct pool_cache *);
void pool_cache_do_invalidate(struct pool_cache *, int,
d166 4
a169 3
int pool_catchup(struct pool *);
void pool_prime_page(struct pool *, caddr_t, struct pool_item_header *);
void pr_rmpage(struct pool *, struct pool_item_header *,
a170 1
void pool_do_put(struct pool *, void *);
d172 2
a173 2
void *pool_allocator_alloc(struct pool *, int);
void pool_allocator_free(struct pool *, void *);
d401 1
a401 1
		
d414 1
a414 1
	size = ALIGN(size);
d461 2
a462 2
		pp->pr_phoffset = off =
			palloc->pa_pagesz - ALIGN(sizeof(struct pool_item_header));
d527 1
a527 1
	/* Insert into the list of all pools. */
d547 1
a547 3
	/*
	 * Locking order: pool_allocator -> pool
	 */
d564 1
a564 1
	/* Remove all pages. */
d583 1
a583 1
pool_set_drain_hook(struct pool *pp, void (*fn)(void *, int), void *fnarg)
d585 1
a585 3
	/*
	 * XXX - no locking, must be called just after pool_init.
	 */
d591 1
a591 1
	pp->pr_drain_hook_arg = fnarg;
d630 1
a630 1
		panic("pool_get: must have NOWAIT");
d657 3
a659 3
			 * Since the drain hook is likely to free memory
			 * to this pool unlock, call hook, relock and check
			 * hardlimit condition again.
d661 1
d665 1
d753 1
a753 1
			/* PA_WANTED is already set on the allocator */
d944 1
a944 1
		    (pp->pr_alloc->pa_flags & PA_WANT)) {
d1032 1
a1032 1
	int newpages, error = 0;
a1045 1
			error = ENOMEM;
d1143 1
a1143 3
 * Note 2, this doesn't work with static pools.
 *
 * Note 3, we must be called with the pool already locked, and we return
a1180 1
	int error;
d1190 1
a1190 1
	if (POOL_NEEDS_CATCHUP(pp) && (error = pool_catchup(pp) != 0)) {
d1272 1
a1272 1
		return 0;
d1314 2
a1315 3
	if (TAILQ_EMPTY(&pq)) {
		return 0;
	}
d1328 1
a1328 1
	return 1;
@


1.34
log
@Remove more '\n's from panic() statements.  From Chris Kuethe.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.33 2002/10/12 01:09:45 krw Exp $	*/
d1950 1
d1965 1
d1969 1
d1973 1
a1984 1
	wakeup(pa);
d1987 1
@


1.33
log
@Remove more '\n's from panic() statements. Both trailing and leading.

Diff generated by Chris Kuethe.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.32 2002/07/23 15:31:36 art Exp $	*/
d793 1
a793 1
		       " item addr %p\n",
@


1.32
log
@Fix yet another braino.

Just because the pool allocates from intrsafe memory doesn't mean that the
pool has to be protected by splvm. We can have an intrsafe pools at splbio
or splsoftnet.

pool_page_alloc and pool_page_free must du their own splvm protection.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.31 2002/07/23 15:26:48 art Exp $	*/
d561 1
a561 1
		panic("pool_destroy: pool busy: still out: %u\n",
d711 1
a711 1
			panic("pool_get: nitems inconsistent\n");
d781 1
a781 1
		panic("pool_get: nitems inconsistent\n");
@


1.32.2.1
log
@Pull in patch from current:
Fix (art):
- Do not try to drain other pools if PR_NOWAIT and the allocator can't
 give us pages. PR_NOWAIT most likely means "hey, we're coming from an
 interrupt, don't mess with stuff that doesn't have proper protection".

- pool_allocator_free is called in too many places so I don't feel
 comfortable without that added protection from splvm (and besides,
 pool_allocator_free is rarely called anyway, so the extra spl will be
 unnoticeable). It shouldn't matter when fiddling with those flags, but
 you never know.

- Remove a wakeup without a matching tsleep. It's a left-over from
 some other code path that I've been investigating when reworking the
 pool a while ago and it should have been removed before that commit.

deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.32 2002/07/23 15:31:36 art Exp $	*/
a1949 1
			break;
a1963 1
	int s;
a1966 1
	s = splvm();
a1969 1
		splx(s);
d1981 1
a1983 1
	splx(s);
@


1.31
log
@Please tell me what crack I've been smoking when I did that.
When trying the drain hook just in pool_allocator_alloc, don't leak memory
when the drain succeeds and don't avoid draining other pools if this
pool doesn't have a drain hook.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.30 2002/06/09 00:15:33 niklas Exp $	*/
d2037 2
d2040 4
a2043 1
	return ((void *)uvm_km_alloc_poolpage(waitok));
d2049 3
d2053 1
@


1.30
log
@different magics for malloc and pool, art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.29 2002/03/14 00:07:57 art Exp $	*/
d1944 6
a1949 6
			if (org->pr_drain_hook == NULL)
				break;
			(*org->pr_drain_hook)(org->pr_drain_hook_arg, flags);
			if ((res = (*pa->pa_alloc)(org, flags)) != NULL)
				continue;
			break;
@


1.29
log
@Whoops. missing simple_unlock.
From thorpej@@netbsd.org
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.28 2002/02/25 04:53:16 dhartmei Exp $	*/
d106 1
a106 1
#define	PI_MAGIC 0xdeadbeef
@


1.28
log
@Make pool_sethardlimit() check that it doesn't decrease the limit below
the current size of the pool. ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.27 2002/02/23 02:52:56 art Exp $	*/
d1979 1
@


1.27
log
@Move out draining of the allocator to an own function and
let other parts of the kernel call it.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.26 2002/02/23 00:05:14 art Exp $	*/
d1220 2
a1221 2
void
pool_sethardlimit(struct pool *pp, int n, const char *warnmess, int ratecap)
d1223 1
d1227 5
d1242 2
a1243 2
	pp->pr_maxpages = (n == 0)
		? 0
d1246 1
d1248 2
@


1.26
log
@If the allocator has the PA_WANT flag set, return whole pages immediately.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.25 2002/02/23 00:03:14 art Exp $	*/
d1914 2
d1922 1
a1922 2
	struct pool *pp, *start;
	int s, freed;
d1924 1
a1941 15

		/*
		 * Drain all pools, except 'org', that use this allocator.
		 * We do this to reclaim va space. pa_alloc is responsible
		 * for waiting for physical memory.
		 * XXX - we risk looping forever if start if someone calls
		 *  pool_destroy on 'start'. But there is no other way to
		 *  have potentially sleeping pool_reclaim, non-sleeping
		 *  locks on pool_allocator and some stirring of drained
		 *  pools in the allocator.
		 * XXX - maybe we should use pool_head_slock for locking
		 *  the allocators?
		 */
		freed = 0;

d1944 1
a1944 20
		pp = start = TAILQ_FIRST(&pa->pa_list);
		do {
			TAILQ_REMOVE(&pa->pa_list, pp, pr_alloc_list);
			TAILQ_INSERT_TAIL(&pa->pa_list, pp, pr_alloc_list);
			if (pp == org)
				continue;
			simple_unlock(&pa->pa_list);
			freed = pool_reclaim(pp)
			simple_lock(&pa->pa_list);
		} while ((pp = TAILQ_FIRST(&pa->pa_list)) != start && !freed);

		if (!freed) {
			/*
			 * We set PA_WANT here, the caller will most likely
			 * sleep waiting for pages (if not, this won't hurt
			 * that much) and there is no way to set this in the
			 * caller without violating locking order.
			 */
			pa->pa_flags |= PA_WANT;
		}
d1971 1
d1974 47
@


1.25
log
@Fix up some functions. Don't have static all over the place
and don't static inline big functions that are called multiple
times and are not time critical.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.24 2002/01/29 00:14:23 miod Exp $	*/
d945 2
a946 1
		if (pp->pr_npages > pp->pr_maxpages) {
@


1.24
log
@Honor the printf-like function argument in pool_printit(), instead of
using printf(). Makes ddb sessions more fruitful.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.23 2002/01/28 03:23:52 art Exp $	*/
d162 9
a170 5
static void	pool_cache_reclaim(struct pool_cache *);

static int	pool_catchup(struct pool *);
static void	pool_prime_page(struct pool *, caddr_t,
		    struct pool_item_header *);
d175 1
a175 2
static void pool_print1(struct pool *, const char *,
	int (*)(const char *, ...));
d318 1
a318 1
static __inline void
d863 1
a863 1
static void
d1070 1
a1070 1
static void
d1150 1
a1150 1
static int
d1375 1
a1375 1
static void
d1750 1
a1750 1
static void
d1802 1
a1802 1
static void
@


1.23
log
@GC PR_STATIC and PR_MALLOCOK.
PR_MALLOC wasn't used at all in the code
and PR_STATIC was missing pieces and should be solved with allocators.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.22 2002/01/25 15:50:22 art Exp $	*/
d1362 1
a1362 1
		printf("pool %s is locked; try again later\n",
d1367 1
a1367 1
	pool_print1(pp, modif, printf);
@


1.22
log
@Add a drain hook to each pool. This hook is called in three cases.
1. When a pool hit the hard limit. Just before bailing out/sleeping.
2. When an allocator fails to allocate memory (with PR_NOWAIT).
3. Just before trying to reclaim some page in pool_reclaim.

The function called form the hook should try to free some items to the
pool if possible.

Convert m_reclaim hooks that were embedded in MCLGET, MGET and MGETHDR
into a pool drain hook (making the code much cleaner).
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.21 2002/01/23 01:44:20 art Exp $	*/
d563 3
a565 4
	/* Remove all pages */
	if ((pp->pr_roflags & PR_STATIC) == 0)
		while ((ph = TAILQ_FIRST(&pp->pr_pagelist)) != NULL)
			pr_rmpage(pp, ph, NULL);
a628 6
	if (__predict_false((pp->pr_roflags & PR_STATIC) &&
			    (flags & PR_MALLOCOK))) {
		pr_printlog(pp, NULL, printf);
		panic("pool_get: static");
	}

a1153 12
	if (pp->pr_roflags & PR_STATIC) {
		/*
		 * We dropped below the low water mark, and this is not a
		 * good thing.  Log a warning.
		 *
		 * XXX: rate-limit this?
		 */
		printf("WARNING: static pool `%s' dropped below low water "
		    "mark\n", pp->pr_wchan);
		return (0);
	}

a1255 3

	if (pp->pr_roflags & PR_STATIC)
		return 0;
@


1.21
log
@Kill PR_FREEHEADER, not used anymore and confusing.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.20 2002/01/23 00:39:47 art Exp $	*/
d442 2
d582 14
d663 13
d1279 7
d1289 1
d1945 11
a1955 1
		if ((flags & PR_WAITOK) == 0)
d1957 1
d1968 2
@


1.20
log
@Pool deals fairly well with physical memory shortage, but it doesn't deal
well (not at all) with shortages of the vm_map where the pages are mapped
(usually kmem_map).

Try to deal with it:
 - group all information the backend allocator for a pool in a separate
   struct. The pool will only have a pointer to that struct.
 - change the pool_init API to reflect that.
 - link all pools allocating from the same allocator on a linked list.
 - Since an allocator is responsible to wait for physical memory it will
   only fail (waitok) when it runs out of its backing vm_map, carefully
   drain pools using the same allocator so that va space is freed.
   (see comments in code for caveats and details).
 - change pool_reclaim to return if it actually succeeded to free some
   memory, use that information to make draining easier and more efficient.
 - get rid of PR_URGENT, noone uses it.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.19 2002/01/10 18:56:03 art Exp $	*/
a577 3

	if (pp->pr_roflags & PR_FREEHEADER)
		free(pp, M_POOL);
@


1.19
log
@Fix a locking error in pool_drain. Misc cleanup.
From NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d112 1
a112 1
	(((u_long)(addr) >> (pp)->pr_pageshift) & (PR_HASHTABSIZE - 1))
d167 3
a169 2
static void	*pool_page_alloc(unsigned long, int, int);
static void	pool_page_free(void *, unsigned long, int);
d343 1
a343 1
		(*pp->pr_free)(ph->ph_page, pp->pr_pagesz, pp->pr_mtype);
d376 1
a376 4
    const char *wchan, size_t pagesz,
    void *(*alloc)(unsigned long, int, int),
    void (*release)(void *, unsigned long, int),
    int mtype)
d391 13
a403 14
	if (!powerof2(pagesz))
		panic("pool_init: page size invalid (%lx)\n", (u_long)pagesz);

	if (alloc == NULL && release == NULL) {
		alloc = pool_page_alloc;
		release = pool_page_free;
		pagesz = PAGE_SIZE;	/* Rounds to PAGE_SIZE anyhow. */
	} else if ((alloc != NULL && release != NULL) == 0) {
		/* If you specifiy one, must specify both. */
		panic("pool_init: must specify alloc and release together");
	}
			
	if (pagesz == 0)
		pagesz = PAGE_SIZE;
d412 2
a413 1
	if (size > pagesz)
d416 1
d433 1
a433 6
	pp->pr_mtype = mtype;
	pp->pr_alloc = alloc;
	pp->pr_free = release;
	pp->pr_pagesz = pagesz;
	pp->pr_pagemask = ~(pagesz - 1);
	pp->pr_pageshift = ffs(pagesz) - 1;
d453 1
a453 1
	if (pp->pr_size < pagesz/16) {
d457 1
a457 1
			pagesz - ALIGN(sizeof(struct pool_item_header));
d461 1
a461 1
		off = pagesz;
d517 1
a517 1
		    0, "phpool", 0, 0, 0, 0);
d519 1
a519 1
		    0, "pcgpool", 0, 0, 0, 0);
d526 5
d542 7
a669 3
		if (flags & PR_URGENT)
			panic("pool_get: urgent");

d700 1
a700 1
		v = (*pp->pr_alloc)(pp->pr_pagesz, flags, pp->pr_mtype);
d708 1
a708 1
				(*pp->pr_free)(v, pp->pr_pagesz, pp->pr_mtype);
a718 3
			if (flags & PR_URGENT)
				panic("pool_get: urgent");

a728 5
			 * XXX: we actually want to wait just until
			 * the page allocator has memory again. Depending
			 * on this pool's usage, we might get stuck here
			 * for a long time.
			 *
d733 1
d851 1
a851 1
	page = (caddr_t)((u_long)v & pp->pr_pagemask);
d1019 1
a1019 1
		cp = (*pp->pr_alloc)(pp->pr_pagesz, PR_NOWAIT, pp->pr_mtype);
d1027 1
a1027 1
				(*pp->pr_free)(cp, pp->pr_pagesz, pp->pr_mtype);
d1057 2
a1058 1
	if (((u_long)cp & (pp->pr_pagesz - 1)) != 0)
d1060 1
d1155 1
a1155 1
		cp = (*pp->pr_alloc)(pp->pr_pagesz, PR_NOWAIT, pp->pr_mtype);
d1161 1
a1161 1
				(*pp->pr_free)(cp, pp->pr_pagesz, pp->pr_mtype);
a1232 39
 * Default page allocator.
 */
static void *
pool_page_alloc(unsigned long sz, int flags, int mtype)
{
	boolean_t waitok = (flags & PR_WAITOK) ? TRUE : FALSE;

	return ((void *)uvm_km_alloc_poolpage(waitok));
}

static void
pool_page_free(void *v, unsigned long sz, int mtype)
{

	uvm_km_free_poolpage((vaddr_t)v);
}

/*
 * Alternate pool page allocator for pools that know they will
 * never be accessed in interrupt context.
 */
void *
pool_page_alloc_nointr(unsigned long sz, int flags, int mtype)
{
	boolean_t waitok = (flags & PR_WAITOK) ? TRUE : FALSE;

	return ((void *)uvm_km_alloc_poolpage1(kernel_map, uvm.kernel_object,
	    waitok));
}

void
pool_page_free_nointr(void *v, unsigned long sz, int mtype)
{

	uvm_km_free_poolpage1(kernel_map, (vaddr_t)v);
}


/*
d1234 2
d1237 1
a1237 1
void
d1251 1
a1251 1
		return;
d1254 1
a1254 1
		return;
d1296 1
a1296 1
		return;
d1300 1
a1300 1
		(*pp->pr_free)(ph->ph_page, pp->pr_pagesz, pp->pr_mtype);
d1309 2
a1339 1

d1385 1
a1385 2
	(*pr)("\tpagesz %u, mtype %d\n", pp->pr_pagesz, pp->pr_mtype);
	(*pr)("\talloc %p, release %p\n", pp->pr_alloc, pp->pr_free);
d1466 1
a1466 1
		page = (caddr_t)((u_long)ph & pp->pr_pagemask);
d1495 1
a1495 1
			page = (caddr_t)((u_long)pi & pp->pr_pagemask);
d1862 143
@


1.18
log
@- unbreak POOL_DIAGNOSTIC.
- use ltsleep instead of simple_unlock ; tsleep
@
text
@d1 2
a2 2
/*	$OpenBSD: subr_pool.c,v 1.17 2002/01/10 14:21:34 art Exp $	*/
/*	$NetBSD: subr_pool.c,v 1.59 2001/06/05 18:51:04 thorpej Exp $	*/
d100 1
d275 1
a275 1
static __inline__ void
d315 2
a316 1
pr_rmpage(struct pool *pp, struct pool_item_header *ph)
d318 1
d336 1
a336 1
	 * Unlink a page from the pool and release it.
d339 11
a349 1
	(*pp->pr_free)(ph->ph_page, pp->pr_pagesz, pp->pr_mtype);
a352 8
	if ((pp->pr_roflags & PR_PHINPAGE) == 0) {
		int s;
		LIST_REMOVE(ph, ph_hashlist);
		s = splhigh();
		pool_put(&phpool, ph);
		splx(s);
	}

d557 2
a558 2
		while ((ph = pp->pr_pagelist.tqh_first) != NULL)
			pr_rmpage(pp, ph);
d563 3
a565 2
	/* XXX Only clear this if we were drainpp? */
	drainpp = NULL;
d621 3
d625 2
d765 1
d767 1
d769 1
d771 1
d853 2
d928 1
a928 1
			pr_rmpage(pp, ph);
d1248 1
d1286 1
d1295 1
d1328 1
a1328 1
			pr_rmpage(pp, ph);
d1334 14
d1362 1
d1365 8
a1372 7

	if (drainpp == NULL && (drainpp = TAILQ_FIRST(&pool_head)) == NULL)
		goto out;

	pp = drainpp;
	drainpp = TAILQ_NEXT(pp, pr_poollist);

a1373 3

 out:
	simple_unlock(&pool_head_slock);
@


1.17
log
@No more need for local define of LOCK_ASSERT.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.16 2002/01/10 14:19:30 art Exp $	*/
a109 1

d193 1
a193 1
pr_log(struct pool *pp, void *a, int action, const char *file, long line)
d640 1
a640 3
			simple_unlock(&pp->pr_slock);
			tsleep((caddr_t)pp, PSWP, (char *)pp->pr_wchan, 0);
			simple_lock(&pp->pr_slock);
d728 1
a728 3
			simple_unlock(&pp->pr_slock);
			tsleep((caddr_t)pp, PSWP, (char *)pp->pr_wchan, 0);
			simple_lock(&pp->pr_slock);
d734 1
a735 1
		pool_prime_page(pp, v, ph);
@


1.16
log
@Protect the pool cache magazine pool with splvm.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.15 2002/01/10 14:16:53 art Exp $	*/
a61 1
#define LOCK_ASSERT(x) /* nothing */
@


1.15
log
@Convert some for-loops into TAILQ_FOREACH.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.14 2001/11/06 19:53:20 miod Exp $	*/
d1678 1
d1695 1
d1697 1
d1753 1
d1772 1
d1774 1
@


1.14
log
@Replace inclusion of <vm/foo.h> with the correct <uvm/bar.h> when necessary.
(Look ma, I might have broken the tree)
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.13 2001/09/19 20:50:58 mickey Exp $	*/
d356 1
a356 2
		for (ph = TAILQ_FIRST(&pp->pr_pagelist); ph != NULL;
		     ph = TAILQ_NEXT(ph, ph_pagelist))
d807 1
a807 2
		for (ph = TAILQ_FIRST(&pp->pr_pagelist); ph != NULL;
		     ph = TAILQ_NEXT(ph, ph_pagelist))
d940 1
a940 2
			for (ph = TAILQ_FIRST(&pp->pr_pagelist); ph != NULL;
			     ph = TAILQ_NEXT(ph, ph_pagelist))
d1286 1
a1286 2
	for (pc = TAILQ_FIRST(&pp->pr_cachelist); pc != NULL;
	     pc = TAILQ_NEXT(pc, pc_poollist))
d1419 1
a1419 2
		for (pi = TAILQ_FIRST(&ph->ph_itemlist); pi != NULL;
		     pi = TAILQ_NEXT(pi, pi_list)) {
d1448 1
a1448 2
	for (pc = TAILQ_FIRST(&pp->pr_cachelist); pc != NULL;
	     pc = TAILQ_NEXT(pc, pc_poollist)) {
d1453 1
a1453 2
		for (pcg = TAILQ_FIRST(&pc->pc_grouplist); pcg != NULL;
		     pcg = TAILQ_NEXT(pcg, pcg_list)) {
d1473 1
a1473 3
	for (ph = TAILQ_FIRST(&pp->pr_pagelist); ph != NULL;
	     ph = TAILQ_NEXT(ph, ph_pagelist)) {

d1631 1
a1631 2
		for (pcg = TAILQ_FIRST(&pc->pc_grouplist); pcg != NULL;
		     pcg = TAILQ_NEXT(pcg, pcg_list)) {
d1682 1
a1682 2
		for (pcg = TAILQ_FIRST(&pc->pc_grouplist); pcg != NULL;
		     pcg = TAILQ_NEXT(pcg, pcg_list)) {
@


1.14.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 2
a2 2
/*	$OpenBSD: subr_pool.c,v 1.24 2002/01/29 00:14:23 miod Exp $	*/
/*	$NetBSD: subr_pool.c,v 1.61 2001/09/26 07:14:56 chs Exp $	*/
d62 1
a100 1
TAILQ_HEAD(pool_pagelist,pool_item_header);
d111 1
d113 1
a113 1
	(((u_long)(addr) >> (pp)->pr_alloc->pa_pageshift) & (PR_HASHTABSIZE - 1))
d168 2
a169 3

void *pool_allocator_alloc(struct pool *, int);
void pool_allocator_free(struct pool *, void *);
d195 1
a195 1
pr_log(struct pool *pp, void *v, int action, const char *file, long line)
d276 1
a276 1
static __inline void
d316 1
a316 2
pr_rmpage(struct pool *pp, struct pool_item_header *ph,
     struct pool_pagelist *pq)
a317 1
	int s;
d335 1
a335 1
	 * Unlink a page from the pool and release it (or queue it for release).
d338 1
a338 11
	if (pq) {
		TAILQ_INSERT_HEAD(pq, ph, ph_pagelist);
	} else {
		pool_allocator_free(pp, ph->ph_page);
		if ((pp->pr_roflags & PR_PHINPAGE) == 0) {
			LIST_REMOVE(ph, ph_hashlist);
			s = splhigh();
			pool_put(&phpool, ph);
			splx(s);
		}
	}
d342 8
d356 2
a357 1
		TAILQ_FOREACH(ph, &pp->pr_pagelist, ph_pagelist)
d373 4
a376 1
    const char *wchan, struct pool_allocator *palloc)
d391 14
a404 13
	if (palloc == NULL)
		palloc = &pool_allocator_kmem;
	if ((palloc->pa_flags & PA_INITIALIZED) == 0) {
		if (palloc->pa_pagesz == 0)
			palloc->pa_pagesz = PAGE_SIZE;

		TAILQ_INIT(&palloc->pa_list);
		
		simple_lock_init(&palloc->pa_slock);
		palloc->pa_pagemask = ~(palloc->pa_pagesz - 1);
		palloc->pa_pageshift = ffs(palloc->pa_pagesz) - 1;
		palloc->pa_flags |= PA_INITIALIZED;
	}
d413 1
a413 2
#ifdef DIAGNOSTIC
	if (size > palloc->pa_pagesz)
a415 1
#endif
d432 6
a437 1
	pp->pr_alloc = palloc;
a445 2
	pp->pr_drain_hook = NULL;
	pp->pr_drain_hook_arg = NULL;
d457 1
a457 1
	if (pp->pr_size < palloc->pa_pagesz/16) {
d461 1
a461 1
			palloc->pa_pagesz - ALIGN(sizeof(struct pool_item_header));
d465 1
a465 1
		off = palloc->pa_pagesz;
d521 1
a521 1
		    0, "phpool", NULL);
d523 1
a523 1
		    0, "pcgpool", NULL);
a529 5

	/* Insert into the list of pools using this allocator. */
	simple_lock(&palloc->pa_slock);
	TAILQ_INSERT_TAIL(&palloc->pa_list, pp, pr_alloc_list);
	simple_unlock(&palloc->pa_slock);
a540 7
	/*
	 * Locking order: pool_allocator -> pool
	 */
	simple_lock(&pp->pr_alloc->pa_slock);
	TAILQ_REMOVE(&pp->pr_alloc->pa_list, pp, pr_alloc_list);
	simple_unlock(&pp->pr_alloc->pa_slock);

d553 4
a556 3
	/* Remove all pages. */
	while ((ph = TAILQ_FIRST(&pp->pr_pagelist)) != NULL)
		pr_rmpage(pp, ph, NULL);
d561 2
a562 3
	if (drainpp == pp) {
		drainpp = NULL;
	}
a568 1
}
d570 2
a571 12
void
pool_set_drain_hook(struct pool *pp, void (*fn)(void *, int), void *fnarg)
{
	/*
	 * XXX - no locking, must be called just after pool_init.
	 */
#ifdef DIAGNOSTIC
	if (pp->pr_drain_hook != NULL)
		panic("pool_set_drain_hook(%s): already set", pp->pr_wchan);
#endif
	pp->pr_drain_hook = fn;
	pp->pr_drain_hook_arg = fnarg;
d608 6
a617 3
#ifdef LOCKDEBUG
	if (flags & PR_WAITOK)
		simple_lock_only_held(NULL, "pool_get(PR_WAITOK)");
a618 2
#endif /* DIAGNOSTIC */

a635 13
		if (pp->pr_drain_hook != NULL) {
			/*
			 * Since the drain hook is likely to free memory
			 * to this pool unlock, call hook, relock and check
			 * hardlimit condition again.
			 */
			simple_unlock(&pp->pr_slock);
			(*pp->pr_drain_hook)(pp->pr_drain_hook_arg, flags);
			simple_lock(&pp->pr_slock);
			if (pp->pr_nout < pp->pr_hardlimit)
				goto startover;
		}

d643 3
a645 1
			ltsleep(pp, PSWP, pp->pr_wchan, 0, &pp->pr_slock);
d658 3
d691 1
a691 1
		v = pool_allocator_alloc(pp, flags);
d699 1
a699 1
				pool_allocator_free(pp, v);
d710 3
d723 5
a731 1
			/* PA_WANTED is already set on the allocator */
d733 3
a735 1
			ltsleep(pp, PSWP, pp->pr_wchan, 0, &pp->pr_slock);
d741 1
a742 1
		pp->pr_npagealloc++;
a760 1
#endif
a761 1
#ifdef POOL_DIAGNOSTIC
a762 1
#endif
a763 1
#ifdef DIAGNOSTIC
d808 2
a809 1
		TAILQ_FOREACH(ph, &pp->pr_pagelist, ph_pagelist)
d846 1
a846 3
	LOCK_ASSERT(simple_lock_held(&pp->pr_slock));

	page = (caddr_t)((vaddr_t)v & pp->pr_alloc->pa_pagemask);
d919 1
a919 1
			pr_rmpage(pp, ph, NULL);
d942 2
a943 1
			TAILQ_FOREACH(ph, &pp->pr_pagelist, ph_pagelist)
d1015 1
a1015 1
		cp = pool_allocator_alloc(pp, PR_NOWAIT);
d1023 1
a1023 1
				pool_allocator_free(pp, cp);
d1053 1
a1053 2
#ifdef DIAGNOSTIC
	if (((u_long)cp & (pp->pr_alloc->pa_pagesz - 1)) != 0)
a1054 1
#endif
d1129 12
d1149 1
a1149 1
		cp = pool_allocator_alloc(pp, PR_NOWAIT);
d1155 1
a1155 1
				pool_allocator_free(pp, cp);
d1227 38
a1265 2
 *
 * Returns non-zero if any pages have been reclaimed.
d1267 1
a1267 1
int
a1276 1
	struct pool_pagelist pq;
d1279 2
a1280 6
	if (pp->pr_drain_hook != NULL) {
		/*
		 * The drain hook must be called with the pool unlocked.
		 */
		(*pp->pr_drain_hook)(pp->pr_drain_hook_arg, PR_NOWAIT);
	}
d1283 1
a1283 1
		return 0;
a1285 2
	TAILQ_INIT(&pq);

d1289 2
a1290 1
	TAILQ_FOREACH(pc, &pp->pr_cachelist, pc_poollist)
d1318 1
a1318 1
			pr_rmpage(pp, ph, &pq);
a1323 16
	if (TAILQ_EMPTY(&pq)) {
		return 0;
	}
	while ((ph = TAILQ_FIRST(&pq)) != NULL) {
		TAILQ_REMOVE(&pq, ph, ph_pagelist);
		pool_allocator_free(pp, ph->ph_page);
		if (pp->pr_roflags & PR_PHINPAGE) {
			continue;
		}
		LIST_REMOVE(ph, ph_hashlist);
		s = splhigh();
		pool_put(&phpool, ph);
		splx(s);
	}

	return 1;
a1337 1
	pp = NULL;
d1340 10
a1349 7
	if (drainpp == NULL) {
		drainpp = TAILQ_FIRST(&pool_head);
	}
	if (drainpp) {
		pp = drainpp;
		drainpp = TAILQ_NEXT(pp, pr_poollist);
	}
a1350 1
	pool_reclaim(pp);
d1354 1
d1365 1
a1365 1
		pr("pool %s is locked; try again later\n",
d1370 1
a1370 1
	pool_print1(pp, modif, pr);
d1400 2
a1401 1
	(*pr)("\talloc %p\n", pp->pr_alloc);
d1423 2
a1424 1
		TAILQ_FOREACH(pi, &ph->ph_itemlist, pi_list) {
d1453 2
a1454 1
	TAILQ_FOREACH(pc, &pp->pr_cachelist, pc_poollist) {
d1459 2
a1460 1
		TAILQ_FOREACH(pcg, &pc->pc_grouplist, pcg_list) {
d1480 3
a1482 1
	TAILQ_FOREACH(ph, &pp->pr_pagelist, ph_pagelist) {
d1487 1
a1487 1
		page = (caddr_t)((vaddr_t)ph & pp->pr_alloc->pa_pagemask);
d1516 1
a1516 1
			page = (caddr_t)((vaddr_t)pi & pp->pr_alloc->pa_pagemask);
d1640 2
a1641 1
		TAILQ_FOREACH(pcg, &pc->pc_grouplist, pcg_list) {
a1687 1
	int s;
d1692 2
a1693 1
		TAILQ_FOREACH(pcg, &pc->pc_grouplist, pcg_list) {
a1704 1
		s = splvm();
a1705 1
		splx(s);
a1760 1
	int s;
a1778 1
			s = splvm();
a1779 1
			splx(s);
a1878 156
}

/*
 * Pool backend allocators.
 *
 * Each pool has a backend allocator that handles allocation, deallocation
 * and any additional draining that might be needed.
 *
 * We provide two standard allocators.
 *  pool_alloc_kmem - the default used when no allocator is specified.
 *  pool_alloc_nointr - used for pools that will not be accessed in
 *   interrupt context.
 */
void	*pool_page_alloc(struct pool *, int);
void	pool_page_free(struct pool *, void *);
void	*pool_page_alloc_nointr(struct pool *, int);
void	pool_page_free_nointr(struct pool *, void *);

struct pool_allocator pool_allocator_kmem = {
	pool_page_alloc, pool_page_free, 0,
};
struct pool_allocator pool_allocator_nointr = {
	pool_page_alloc_nointr, pool_page_free_nointr, 0,
};

/*
 * XXX - we have at least three different resources for the same allocation
 *  and each resource can be depleted. First we have the ready elements in
 *  the pool. Then we have the resource (typically a vm_map) for this
 *  allocator, then we have physical memory. Waiting for any of these can
 *  be unnecessary when any other is freed, but the kernel doesn't support
 *  sleeping on multiple addresses, so we have to fake. The caller sleeps on
 *  the pool (so that we can be awakened when an item is returned to the pool),
 *  but we set PA_WANT on the allocator. When a page is returned to
 *  the allocator and PA_WANT is set pool_allocator_free will wakeup all
 *  sleeping pools belonging to this allocator. (XXX - thundering herd).
 */

void *
pool_allocator_alloc(struct pool *org, int flags)
{
	struct pool_allocator *pa = org->pr_alloc;
	struct pool *pp, *start;
	int s, freed;
	void *res;

	do {
		if ((res = (*pa->pa_alloc)(org, flags)) != NULL)
			return (res);
		if ((flags & PR_WAITOK) == 0) {
			/*
			 * We only run the drain hook here if PR_NOWAIT.
			 * In other cases the hook will be run in
			 * pool_reclaim.
			 */
			if (org->pr_drain_hook == NULL)
				break;
			(*org->pr_drain_hook)(org->pr_drain_hook_arg, flags);
			if ((res = (*pa->pa_alloc)(org, flags)) != NULL)
				continue;
			break;
		}

		/*
		 * Drain all pools, except 'org', that use this allocator.
		 * We do this to reclaim va space. pa_alloc is responsible
		 * for waiting for physical memory.
		 * XXX - we risk looping forever if start if someone calls
		 *  pool_destroy on 'start'. But there is no other way to
		 *  have potentially sleeping pool_reclaim, non-sleeping
		 *  locks on pool_allocator and some stirring of drained
		 *  pools in the allocator.
		 * XXX - maybe we should use pool_head_slock for locking
		 *  the allocators?
		 */
		freed = 0;

		s = splvm();
		simple_lock(&pa->pa_slock);
		pp = start = TAILQ_FIRST(&pa->pa_list);
		do {
			TAILQ_REMOVE(&pa->pa_list, pp, pr_alloc_list);
			TAILQ_INSERT_TAIL(&pa->pa_list, pp, pr_alloc_list);
			if (pp == org)
				continue;
			simple_unlock(&pa->pa_list);
			freed = pool_reclaim(pp)
			simple_lock(&pa->pa_list);
		} while ((pp = TAILQ_FIRST(&pa->pa_list)) != start && !freed);

		if (!freed) {
			/*
			 * We set PA_WANT here, the caller will most likely
			 * sleep waiting for pages (if not, this won't hurt
			 * that much) and there is no way to set this in the
			 * caller without violating locking order.
			 */
			pa->pa_flags |= PA_WANT;
		}
		simple_unlock(&pa->pa_slock);
		splx(s);
	} while (freed);
	return (NULL);
}

void
pool_allocator_free(struct pool *pp, void *v)
{
	struct pool_allocator *pa = pp->pr_alloc;

	(*pa->pa_free)(pp, v);

	simple_lock(&pa->pa_slock);
	if ((pa->pa_flags & PA_WANT) == 0) {
		simple_unlock(&pa->pa_slock);
		return;
	}

	TAILQ_FOREACH(pp, &pa->pa_list, pr_alloc_list) {
		simple_lock(&pp->pr_slock);
		if ((pp->pr_flags & PR_WANTED) != 0) {
			pp->pr_flags &= ~PR_WANTED;
			wakeup(pp);
		}
	}
	pa->pa_flags &= ~PA_WANT;
	simple_unlock(&pa->pa_slock);
}

void *
pool_page_alloc(struct pool *pp, int flags)
{
	boolean_t waitok = (flags & PR_WAITOK) ? TRUE : FALSE;

	return ((void *)uvm_km_alloc_poolpage(waitok));
}

void
pool_page_free(struct pool *pp, void *v)
{
	uvm_km_free_poolpage((vaddr_t)v);
}

void *
pool_page_alloc_nointr(struct pool *pp, int flags)
{
	boolean_t waitok = (flags & PR_WAITOK) ? TRUE : FALSE;

	return ((void *)uvm_km_alloc_poolpage1(kernel_map, uvm.kernel_object,
	    waitok));
}

void
pool_page_free_nointr(struct pool *pp, void *v)
{
	uvm_km_free_poolpage1(kernel_map, (vaddr_t)v);
@


1.14.2.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.14.2.1 2002/01/31 22:55:41 niklas Exp $	*/
d106 1
a106 1
#define	PI_MAGIC 0xdeafbeef
d162 5
a166 9
void pool_cache_reclaim(struct pool_cache *);
void pool_cache_do_invalidate(struct pool_cache *, int,
    void (*)(struct pool *, void *));

int pool_catchup(struct pool *);
void pool_prime_page(struct pool *, caddr_t, struct pool_item_header *);
void pr_rmpage(struct pool *, struct pool_item_header *,
    struct pool_pagelist *);
void pool_do_put(struct pool *, void *);
d171 2
a172 1
void pool_print1(struct pool *, const char *, int (*)(const char *, ...));
d315 1
a315 1
void
d860 1
a860 1
void
d942 1
a942 2
		if (pp->pr_npages > pp->pr_maxpages ||
		    (pp->pr_alloc->pa_flags & PA_WANT)) {
d1067 1
a1067 1
void
d1147 1
a1147 1
int
d1216 2
a1217 2
int
pool_sethardlimit(struct pool *pp, unsigned n, const char *warnmess, int ratecap)
a1218 1
	int error = 0;
a1221 5
	if (n < pp->pr_nout) {
		error = EINVAL;
		goto done;
	}

d1232 2
a1233 2
	pp->pr_maxpages = (n == 0 || n == UINT_MAX)
		? n
a1235 1
 done:
a1236 2

	return (error);
d1372 1
a1372 1
void
d1747 1
a1747 1
void
d1799 1
a1799 1
void
a1909 2
 *  We also wake up the allocator in case someone without a pool (malloc)
 *  is sleeping waiting for this allocator.
d1916 2
a1917 1
	int freed;
a1918 1
	int s;
d1936 15
d1953 20
a1972 1
		freed = pool_allocator_drain(pa, org, 1);
a1997 1
		simple_unlock(&pp->pr_slock);
a1998 1
	wakeup(pa);
a2000 47
}

/*
 * Drain all pools, except 'org', that use this allocator.
 *
 * Must be called at appropriate spl level and with the allocator locked.
 *
 * We do this to reclaim va space. pa_alloc is responsible
 * for waiting for physical memory.
 * XXX - we risk looping forever if start if someone calls
 *  pool_destroy on 'start'. But there is no other way to
 *  have potentially sleeping pool_reclaim, non-sleeping
 *  locks on pool_allocator and some stirring of drained
 *  pools in the allocator.
 * XXX - maybe we should use pool_head_slock for locking
 *  the allocators?
 */
int
pool_allocator_drain(struct pool_allocator *pa, struct pool *org, int need)
{
	struct pool *pp, *start;
	int freed;

	freed = 0;

	pp = start = TAILQ_FIRST(&pa->pa_list);
	do {
		TAILQ_REMOVE(&pa->pa_list, pp, pr_alloc_list);
		TAILQ_INSERT_TAIL(&pa->pa_list, pp, pr_alloc_list);
		if (pp == org)
			continue;
		simple_unlock(&pa->pa_list);
		freed = pool_reclaim(pp)
		simple_lock(&pa->pa_list);
	} while ((pp = TAILQ_FIRST(&pa->pa_list)) != start && (freed < need));

	if (!freed) {
		/*
		 * We set PA_WANT here, the caller will most likely
		 * sleep waiting for pages (if not, this won't hurt
		 * that much) and there is no way to set this in the
		 * caller without violating locking order.
		 */
		pa->pa_flags |= PA_WANT;
	}

	return (freed);
@


1.14.2.3
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.14.2.2 2002/06/11 03:29:40 art Exp $	*/
d162 2
a163 2
void	pool_cache_reclaim(struct pool_cache *);
void	pool_cache_do_invalidate(struct pool_cache *, int,
d166 3
a168 4
int	pool_catchup(struct pool *);
void	pool_prime_page(struct pool *, caddr_t, struct pool_item_header *);
void	pool_do_put(struct pool *, void *);
void	pr_rmpage(struct pool *, struct pool_item_header *,
d170 1
d172 2
a173 2
void	*pool_allocator_alloc(struct pool *, int);
void	pool_allocator_free(struct pool *, void *);
d401 1
a401 1

d414 1
a414 1
	size = roundup(size, align);
d461 2
a462 2
		pp->pr_phoffset = off = palloc->pa_pagesz -
		    ALIGN(sizeof(struct pool_item_header));
d527 1
a527 1
	/* Insert this into the list of all pools. */
d547 3
a549 1
	/* Locking order: pool_allocator -> pool */
d561 1
a561 1
		panic("pool_destroy: pool busy: still out: %u",
d566 1
a566 1
	/* Remove all pages */
d585 1
a585 1
pool_set_drain_hook(struct pool *pp, void (*fn)(void *, int), void *arg)
d587 3
a589 1
	/* XXX no locking -- must be used just after pool_init() */
d595 1
a595 1
	pp->pr_drain_hook_arg = arg;
d634 1
a634 1
		panic("pool_get: %s:must have NOWAIT", pp->pr_wchan);
d661 3
a663 3
			 * Since the drain hook is going to free things
			 * back to the pool, unlock, call hook, re-lock
			 * and check hardlimit condition again.
a664 1
			pr_leave(pp);
a667 1
			pr_enter(pp, file, line);
d711 1
a711 1
			panic("pool_get: nitems inconsistent");
d755 1
a755 1
			/* PA_WANTED is already set on the allocator. */
d781 1
a781 1
		panic("pool_get: nitems inconsistent");
d793 1
a793 1
		       " item addr %p",
d946 1
a946 1
		    (pp->pr_alloc->pa_flags & PA_WANT) != 0) {
d1034 1
a1034 1
	int newpages;
d1048 1
d1146 3
a1148 1
 * Note 2, we must be called with the pool already locked, and we return
d1186 1
d1196 1
a1196 1
	if (POOL_NEEDS_CATCHUP(pp) && pool_catchup(pp) != 0) {
d1278 1
a1278 1
		return (0);
d1320 3
a1322 2
	if (TAILQ_EMPTY(&pq))
		return (0);
d1335 1
a1335 1
	return (1);
d1944 5
a1948 6
			if (org->pr_drain_hook != NULL) {
				(*org->pr_drain_hook)(org->pr_drain_hook_arg,
				    flags);
				if ((res = (*pa->pa_alloc)(org, flags)) != NULL)
					return (res);
			}
a1963 1
	int s;
a1966 1
	s = splvm();
a1969 1
		splx(s);
d1981 1
a1983 1
	splx(s);
a2036 2
	void *ret;
	int s;
d2038 1
a2038 4
	s = splvm();
	ret = (void *)uvm_km_alloc_poolpage(waitok);
	splx(s);
	return (ret);
a2043 3
	int s;

	s = splvm();
a2044 1
	splx(s);
@


1.14.2.4
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a390 4
#ifdef MALLOC_DEBUG
	if ((flags & PR_DEBUG) && (ioff != 0 || align != 0))
		flags &= ~PR_DEBUG;
#endif
a627 2
	if ((flags & PR_WAITOK) != 0)
		splassert(IPL_NONE);
a637 11
#ifdef MALLOC_DEBUG
	if (pp->pr_roflags & PR_DEBUG) {
		void *addr;

		addr = NULL;
		debug_malloc(pp->pr_size, M_DEBUG,
		    (flags & PR_WAITOK) ? M_WAITOK : M_NOWAIT, &addr);
		return (addr);
	}
#endif

a868 7
#ifdef MALLOC_DEBUG
	if (pp->pr_roflags & PR_DEBUG) {
		debug_free(v, M_DEBUG);
		return;
	}
#endif

a1118 2
		KASSERT(((((vaddr_t)pi) + ioff) & (align - 1)) == 0);

d2034 2
d2037 4
a2040 2
	return ((void *)uvm_km_alloc_poolpage1(kmem_map, NULL,
	    waitok));
d2046 1
d2048 3
a2050 1
	uvm_km_free_poolpage1(kmem_map, (vaddr_t)v);
a2057 2
	splassert(IPL_NONE);

a2064 2
	splassert(IPL_NONE);

@


1.13
log
@merge vm/vm_kern.h into uvm/uvm_extern.h; art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.12 2001/08/07 21:02:22 art Exp $	*/
a51 1
#include <vm/vm.h>
@


1.12
log
@We now have predict_{true,false}
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.11 2001/07/15 11:03:05 assar Exp $	*/
a52 2
#include <vm/vm_kern.h>

@


1.11
log
@(define simple_lock_only_held): add dummy nop macro
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.10 2001/06/27 04:49:46 art Exp $	*/
a59 2
#define __predict_false(X) ((X) != 0)
#define __predict_true(X) ((X) != 0)
@


1.10
log
@remove old vm
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.9 2001/06/24 17:06:14 miod Exp $	*/
d65 1
@


1.9
log
@No need to define splvm() here anymore.
art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.8 2001/06/24 16:00:47 art Exp $	*/
a54 1
#if defined(UVM)
a55 1
#endif
a1235 1
#if defined(UVM)
a1238 11
#else
	boolean_t waitok = (flags & PR_WAITOK) ? TRUE : FALSE;
	int s;
	vaddr_t va;

	s = splimp();
	va = kmem_malloc(kmem_map, PAGE_SIZE, waitok);
	splx(s);

	return ((void *)va);
#endif
a1243 1
#if defined(UVM)
a1244 7
#else
	int s;

	s = splimp();
	kmem_free(kmem_map, (vaddr_t)v, PAGE_SIZE);
	splx(s);
#endif
a1253 1
#if defined(UVM)
a1257 3
#else
	return pool_page_alloc(sz, flags, mtype);
#endif
a1263 1
#if defined(UVM)
a1264 3
#else
	pool_page_free(v, sz, mtype);
#endif
@


1.8
log
@Add a sysctl for getting pool information out of the kernel.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.7 2001/06/23 17:15:46 art Exp $	*/
a63 1
#define splvm splimp
@


1.7
log
@ooops. remove unfinished work in progress.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.6 2001/06/23 16:13:01 art Exp $	*/
d126 6
d453 3
d1848 6
a1853 1
#ifdef notyet
d1857 1
a1857 1
	struct pool *pp;
d1859 34
a1892 1
	int s;
d1894 2
a1895 2
	if (namelen != 0)
		return (ENOTDIR);
d1897 2
a1898 2
	s = splimp();
	simple_lock(&pool_head_slock);
d1900 14
a1913 1
	for (pp = pool_head; pp != NULL; pp = TAILQ_NEXT(pp, pr_poollist))
a1914 1
#endif@


1.6
log
@Bring in a bunch of improvements from NetBSD.

 - pool_cache similar to the slab allocator in Solaris.
 - clean up locking a bit.
 - Don't pass __LINE__ and __FILE__ to pool_get and pool_put unless
   POOL_DIAGNOSTIC is defined.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.5 2001/03/21 23:24:51 art Exp $	*/
d1839 1
a1842 1
#ifdef notyet
a1853 3
#else
	return EOPNOTSUPP;
#endif
d1855 1
@


1.5
log
@Ok, I'm breaking my promise. I promised to not do anything with the
old vm system and I hoped that it would make people help me to switch all
archs to uvm. But that didn't help.

Fix pool to work with the old vm system (not optimal, ha!).
@
text
@d1 2
a2 2
/*	$OpenBSD: subr_pool.c,v 1.4 2001/02/19 11:34:12 art Exp $	*/
/*	$NetBSD: subr_pool.c,v 1.37 2000/06/10 18:44:44 sommerfeld Exp $	*/
d5 1
a5 1
 * Copyright (c) 1997, 1999 The NetBSD Foundation, Inc.
d50 1
d63 2
d69 1
d122 2
d125 40
d166 5
a170 7
static struct pool_item_header
		*pr_find_pagehead __P((struct pool *, caddr_t));
static void	pr_rmpage __P((struct pool *, struct pool_item_header *));
static int	pool_catchup __P((struct pool *));
static void	pool_prime_page __P((struct pool *, caddr_t));
static void	*pool_page_alloc __P((unsigned long, int, int));
static void	pool_page_free __P((void *, unsigned long, int));
d172 2
a173 2
static void pool_print1 __P((struct pool *, const char *,
	int (*)(const char *, ...)));
d176 1
a176 1
 * Pool log entry. An array of these is allocated in pool_create().
d194 3
a196 16
#ifdef DIAGNOSTIC
static void	pr_log __P((struct pool *, void *, int, const char *, long));
static void	pr_printlog __P((struct pool *, struct pool_item *,
		    int (*)(const char *, ...)));
static void	pr_enter __P((struct pool *, const char *, long));
static void	pr_leave __P((struct pool *));
static void	pr_enter_check __P((struct pool *,
		    int (*)(const char *, ...)));

static __inline__ void
pr_log(pp, v, action, file, line)
	struct pool	*pp;
	void		*v;
	int		action;
	const char	*file;
	long		line;
d219 2
a220 4
pr_printlog(pp, pi, pr)
	struct pool *pp;
	struct pool_item *pi;
	int (*pr) __P((const char *, ...));
d248 2
a249 5
static __inline__ void
pr_enter(pp, file, line)
	struct pool *pp;
	const char *file;
	long line;
d264 2
a265 3
static __inline__ void
pr_leave(pp)
	struct pool *pp;
d278 1
a278 3
pr_enter_check(pp, pr)
	struct pool *pp;
	int (*pr) __P((const char *, ...));
d291 1
a291 1
#endif /* DIAGNOSTIC */
d296 2
a297 4
static __inline__ struct pool_item_header *
pr_find_pagehead(pp, page)
	struct pool *pp;
	caddr_t page;
d316 2
a317 4
static __inline__ void
pr_rmpage(pp, ph)
	struct pool *pp;
	struct pool_item_header *ph;
a366 36
 * Allocate and initialize a pool.
 */
struct pool *
pool_create(size, align, ioff, nitems, wchan, pagesz, alloc, release, mtype)
	size_t	size;
	u_int	align;
	u_int	ioff;
	int	nitems;
	const char *wchan;
	size_t	pagesz;
	void	*(*alloc) __P((unsigned long, int, int));
	void	(*release) __P((void *, unsigned long, int));
	int	mtype;
{
	struct pool *pp;
	int flags;

	pp = (struct pool *)malloc(sizeof(*pp), M_POOL, M_NOWAIT);
	if (pp == NULL)
		return (NULL);

	flags = PR_FREEHEADER;
	pool_init(pp, size, align, ioff, flags, wchan, pagesz,
		  alloc, release, mtype);

	if (nitems != 0) {
		if (pool_prime(pp, nitems, NULL) != 0) {
			pool_destroy(pp);
			return (NULL);
		}
	}

	return (pp);
}

/*
d373 5
a377 11
pool_init(pp, size, align, ioff, flags, wchan, pagesz, alloc, release, mtype)
	struct pool	*pp;
	size_t		size;
	u_int		align;
	u_int		ioff;
	int		flags;
	const char	*wchan;
	size_t		pagesz;
	void		*(*alloc) __P((unsigned long, int, int));
	void		(*release) __P((void *, unsigned long, int));
	int		mtype;
d414 1
a414 1
	if (size >= pagesz)
d422 1
d478 1
d496 1
d505 1
d513 2
a514 1
	 * Initialize private page header pool if we haven't done so yet.
d519 3
a521 1
			  0, "phpool", 0, 0, 0, 0);
d534 1
a534 2
pool_destroy(pp)
	struct pool *pp;
d537 5
d563 1
d566 1
d572 18
d595 5
a599 5
_pool_get(pp, flags, file, line)
	struct pool *pp;
	int flags;
	const char *file;
	long line;
a600 1
	void *v;
d603 1
a610 1
#endif
d616 1
a672 2
		void *v;

d690 2
d695 4
a698 1
		if (v == NULL) {
d700 4
a703 4
			 * We were unable to allocate a page, but
			 * we released the lock during allocation,
			 * so perhaps items were freed back to the
			 * pool.  Check for this case.
d740 1
a740 1
		pool_prime_page(pp, v);
d759 1
a759 1
#endif
a761 1
#ifdef DIAGNOSTIC
d820 1
a820 1
	if (pp->pr_nitems < pp->pr_minitems && pool_catchup(pp) != 0) {
d834 1
a834 1
 * Return resource to the pool; must be called at appropriate spl level
d836 2
a837 6
void
_pool_put(pp, v, file, line)
	struct pool *pp;
	void *v;
	const char *file;
	long line;
a845 3
	simple_lock(&pp->pr_slock);
	pr_enter(pp, file, line);

a853 2
	pr_log(pp, v, PRLOG_PUT, file, line);

a895 2
		pr_leave(pp);
		simple_unlock(&pp->pr_slock);
d959 16
d978 3
d982 9
d993 4
d1001 1
a1001 4
pool_prime(pp, n, storage)
	struct pool *pp;
	int n;
	caddr_t storage;
d1003 1
d1005 1
a1005 7
	int newnitems, newpages;

#ifdef DIAGNOSTIC
	if (__predict_false(storage && !(pp->pr_roflags & PR_STATIC)))
		panic("pool_prime: static");
	/* !storage && static caught below */
#endif
d1009 1
a1009 4
	newnitems = pp->pr_minitems + n;
	newpages =
		roundup(newnitems, pp->pr_itemsperpage) / pp->pr_itemsperpage
		- pp->pr_minpages;
d1012 5
a1016 8
		if (pp->pr_roflags & PR_STATIC) {
			cp = storage;
			storage += pp->pr_pagesz;
		} else {
			simple_unlock(&pp->pr_slock);
			cp = (*pp->pr_alloc)(pp->pr_pagesz, 0, pp->pr_mtype);
			simple_lock(&pp->pr_slock);
		}
d1018 5
a1022 3
		if (cp == NULL) {
			simple_unlock(&pp->pr_slock);
			return (ENOMEM);
d1025 1
a1026 1
		pool_prime_page(pp, cp);
a1029 2
	pp->pr_minitems = newnitems;

d1043 1
a1043 3
pool_prime_page(pp, storage)
	struct pool *pp;
	caddr_t storage;
a1045 1
	struct pool_item_header *ph;
d1049 1
a1049 1
	int s, n;
d1054 1
a1054 6
	if ((pp->pr_roflags & PR_PHINPAGE) != 0) {
		ph = (struct pool_item_header *)(cp + pp->pr_phoffset);
	} else {
		s = splhigh();
		ph = pool_get(&phpool, PR_URGENT);
		splx(s);
d1056 1
a1056 2
				 ph, ph_hashlist);
	}
d1110 2
a1111 3
 * Like pool_prime(), except this is used by pool_get() when nitems
 * drops below the low water mark.  This is used to catch up nitmes
 * with the low water mark.
d1121 1
a1121 2
pool_catchup(pp)
	struct pool *pp;
d1123 1
d1139 1
a1139 1
	while (pp->pr_nitems < pp->pr_minitems) {
d1147 3
a1149 1
		cp = (*pp->pr_alloc)(pp->pr_pagesz, 0, pp->pr_mtype);
d1151 3
a1153 1
		if (__predict_false(cp == NULL)) {
d1157 1
a1158 1
		pool_prime_page(pp, cp);
d1165 1
a1165 3
pool_setlowat(pp, n)
	pool_handle_t	pp;
	int n;
d1177 1
a1177 1
	if ((error = pool_catchup(pp)) != 0) {
d1189 1
a1189 3
pool_sethiwat(pp, n)
	pool_handle_t	pp;
	int n;
d1202 1
a1202 5
pool_sethardlimit(pp, n, warnmess, ratecap)
	pool_handle_t pp;
	int n;
	const char *warnmess;
	int ratecap;
d1228 1
a1228 4
pool_page_alloc(sz, flags, mtype)
	unsigned long sz;
	int flags;
	int mtype;
d1248 1
a1248 4
pool_page_free(v, sz, mtype)
	void *v;
	unsigned long sz;
	int mtype;
d1266 1
a1266 4
pool_page_alloc_nointr(sz, flags, mtype)
	unsigned long sz;
	int flags;
	int mtype;
d1279 1
a1279 4
pool_page_free_nointr(v, sz, mtype)
	void *v;
	unsigned long sz;
	int mtype;
d1294 5
a1298 4
_pool_reclaim(pp, file, line)
	pool_handle_t pp;
	const char *file;
	long line;
d1301 1
d1312 7
d1359 1
a1359 2
pool_drain(arg)
	void *arg;
d1364 1
a1364 1
	s = splimp();
d1385 1
a1385 3
pool_print(pp, modif)
	struct pool *pp;
	const char *modif;
d1389 1
a1389 1
	s = splimp();
a1400 33
void
pool_printit(pp, modif, pr)
	struct pool *pp;
	const char *modif;
	int (*pr) __P((const char *, ...));
{
	int didlock = 0;

	if (pp == NULL) {
		(*pr)("Must specify a pool to print.\n");
		return;
	}

	/*
	 * Called from DDB; interrupts should be blocked, and all
	 * other processors should be paused.  We can skip locking
	 * the pool in this case.
	 *
	 * We do a simple_lock_try() just to print the lock
	 * status, however.
	 */

	if (simple_lock_try(&pp->pr_slock) == 0)
		(*pr)("WARNING: pool %s is locked\n", pp->pr_wchan);
	else
		didlock = 1;

	pool_print1(pp, modif, pr);

	if (didlock)
		simple_unlock(&pp->pr_slock);
}

d1402 1
a1402 4
pool_print1(pp, modif, pr)
	struct pool *pp;
	const char *modif;
	int (*pr) __P((const char *, ...));
d1405 2
d1410 1
a1410 1
	int print_log = 0, print_pagelist = 0;
d1418 2
d1476 19
d1499 1
a1499 3
pool_chk(pp, label)
	struct pool *pp;
	char *label;
d1559 298
@


1.4
log
@Fix LOCKDEBUG compile.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.3 2000/12/05 16:43:41 art Exp $	*/
d54 1
d56 1
d1230 1
d1234 11
d1253 4
d1258 4
a1261 1
	uvm_km_free_poolpage((vaddr_t)v);
d1274 1
d1279 3
d1291 1
d1293 3
@


1.3
log
@Bring in fresh pool from NetBSD. Some improvements and fixes.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d61 3
@


1.2
log
@Pool will not work with old vm and is only used by uvm. Remove non-uvm code.
@
text
@d1 2
a2 2
/*	$OpenBSD: subr_pool.c,v 1.1 1999/02/26 03:13:30 art Exp $	*/
/*	$NetBSD: subr_pool.c,v 1.17 1998/12/27 21:13:43 thorpej Exp $	*/
d5 1
a5 1
 * Copyright (c) 1997 The NetBSD Foundation, Inc.
d9 2
a10 1
 * by Paul Kranenburg.
d49 1
d57 6
a70 1
 * 
d74 1
a74 1
TAILQ_HEAD(,pool) pool_head = { NULL, &(pool_head).tqh_first };
d83 4
a86 1
static struct pool	*drainpp = NULL;
a102 1
#define PI_MAGIC 0xdeadbeef
d104 1
d110 1
a110 1
#define PR_HASH_INDEX(pp,addr) \
d118 2
a119 1
static int	pool_prime_page __P((struct pool *, caddr_t));
d123 2
a125 1
#ifdef POOL_DIAGNOSTIC
d133 2
a134 2
#define PRLOG_GET	1
#define PRLOG_PUT	2
d145 1
d147 6
a152 1
static void	pr_printlog __P((struct pool *));
d165 1
a165 1
	if ((pp->pr_flags & PR_LOGGING) == 0)
d183 1
a183 1
pr_printlog(pp)
d185 2
d191 1
a191 1
	if ((pp->pr_flags & PR_LOGGING) == 0)
a193 2
	pool_print(pp, "printlog");

d200 8
a207 6
			printf("log entry %d:\n", i);
			printf("\taction = %s, addr = %p\n",
				pl->pl_action == PRLOG_GET ? "get" : "put",
				pl->pl_addr);
			printf("\tfile: %s at line %lu\n",
				pl->pl_file, pl->pl_line);
d213 44
d258 6
a263 4
#define pr_log(pp, v, action, file, line)
#define pr_printlog(pp)
#endif

d275 1
a275 1
	if ((pp->pr_flags & PR_PHINPAGE) != 0)
d303 2
d309 2
d319 2
a320 1
	if ((pp->pr_flags & PR_PHINPAGE) == 0) {
d322 1
d324 1
d351 1
a351 1
	char	*wchan;
a364 5
#ifdef POOL_DIAGNOSTIC
	if (pool_logsize != 0)
		flags |= PR_LOGGING;
#endif

d391 1
a391 1
	char		*wchan;
d399 8
d410 1
a410 1
	if (!powerof2(pagesz) || pagesz > PAGE_SIZE)
d431 5
a438 1
	TAILQ_INSERT_TAIL(&pool_head, pp, pr_poollist);
d445 3
a447 2
	pp->pr_flags = flags;
	pp->pr_size = ALIGN(size);
d456 8
d474 1
a474 1
		pp->pr_flags |= PR_PHINPAGE;
d512 5
a516 6
#ifdef POOL_DIAGNOSTIC
	if ((flags & PR_LOGGING) != 0) {
		pp->pr_log = malloc(pool_logsize * sizeof(struct pool_log),
				    M_TEMP, M_NOWAIT);
		if (pp->pr_log == NULL)
			pp->pr_flags &= ~PR_LOGGING;
a519 1
#endif
d521 4
a524 2
	simple_lock_init(&pp->pr_lock);
	lockinit(&pp->pr_resourcelock, PSWP, wchan, 0, 0);
d528 1
d535 4
a538 1
	return;
d551 4
a554 4
	if (pp->pr_nget - pp->pr_nput != 0) {
		pr_printlog(pp);
		panic("pool_destroy: pool busy: still out: %lu\n",
		      pp->pr_nget - pp->pr_nput);
d559 1
a559 1
	if ((pp->pr_flags & PR_STATIC) == 0)
d564 1
d566 1
d568 1
d570 1
a570 2
#ifdef POOL_DIAGNOSTIC
	if ((pp->pr_flags & PR_LOGGING) != 0)
a571 1
#endif
d573 1
a573 1
	if (pp->pr_flags & PR_FREEHEADER)
a580 1
#ifdef POOL_DIAGNOSTIC
a586 6
#else
void *
pool_get(pp, flags)
	struct pool *pp;
	int flags;
#endif
d593 3
a595 2
	if ((pp->pr_flags & PR_STATIC) && (flags & PR_MALLOCOK)) {
		pr_printlog(pp);
d600 2
a601 2
	simple_lock(&pp->pr_lock);
	if (curproc == NULL && (flags & PR_WAITOK) != 0)
d604 49
d659 1
a659 1
	while ((ph = pp->pr_curpage) == NULL) {
a660 2
		int lkflags = LK_EXCLUSIVE | LK_INTERLOCK |
			      ((flags & PR_WAITOK) == 0 ? LK_NOWAIT : 0);
d662 8
a669 4
		/* Get long-term lock on pool */
		if (lockmgr(&pp->pr_resourcelock, lkflags, &pp->pr_lock,
			    curproc /*XXX*/) != 0)
			return (NULL);
d671 10
a680 3
		/* Check if pool became non-empty while we slept */
		if ((ph = pp->pr_curpage) != NULL)
			goto again;
a681 2
		/* Call the page back-end allocator for more memory */
		v = (*pp->pr_alloc)(pp->pr_pagesz, flags, pp->pr_mtype);
d683 9
d694 1
d697 2
a698 2
				lockmgr(&pp->pr_resourcelock, LK_RELEASE, NULL,
					curproc/*XXX*/);
d704 1
d709 3
d714 6
a719 5
			lockmgr(&pp->pr_resourcelock, LK_RELEASE, NULL,
				curproc/*XXX*/);
			tsleep((caddr_t)pp, PSWP, pp->pr_wchan, 0);
			simple_lock(&pp->pr_lock);
			continue;
d726 2
a727 4
again:
		/* Re-acquire pool interlock */
		simple_lock(&pp->pr_lock);
		lockmgr(&pp->pr_resourcelock, LK_RELEASE, NULL, curproc/*XXX*/);
d730 3
a732 1
	if ((v = pi = TAILQ_FIRST(&ph->ph_itemlist)) == NULL)
d734 10
a743 1

d747 2
a748 2
	if (pi->pi_magic != PI_MAGIC) {
		pr_printlog(pp);
d759 2
d763 1
a763 1
		if (pp->pr_nidle == 0)
d770 8
d783 5
a787 2
		 * First, move the now empty page to the head of
		 * the page list.
d790 3
a792 2
		TAILQ_INSERT_HEAD(&pp->pr_pagelist, ph, ph_pagelist);
		while ((ph = TAILQ_NEXT(ph, ph_pagelist)) != NULL)
d800 15
a814 1
	simple_unlock(&pp->pr_lock);
a820 1
#ifdef POOL_DIAGNOSTIC
a826 6
#else
void
pool_put(pp, v)
	struct pool *pp;
	void *v;
#endif
d831 1
d835 10
a844 1
	simple_lock(&pp->pr_lock);
d848 2
a849 2
	if ((ph = pr_find_pagehead(pp, page)) == NULL) {
		pr_printlog(pp);
d853 7
d866 10
d879 2
d890 2
a892 1
		simple_unlock(&pp->pr_lock);
d897 12
a908 2
	 * If this page is now complete, move it to the end of the pagelist.
	 * If this page has just become un-empty, move it the head.
a912 3
#if 0
			timeout(pool_drain, 0, pool_inactive_time*hz);
#else
a913 1
#endif
a916 1
			ph->ph_time = time;
d918 18
a935 1
			/* XXX - update curpage */
d944 14
a958 1
	simple_unlock(&pp->pr_lock);
d974 1
a974 1
	if (storage && !(pp->pr_flags & PR_STATIC))
d979 2
a980 1
	(void)lockmgr(&pp->pr_resourcelock, LK_EXCLUSIVE, NULL, curproc/*XXX*/);
d983 1
a983 1
		roundup(pp->pr_itemsperpage,newnitems) / pp->pr_itemsperpage
d987 1
a987 2

		if (pp->pr_flags & PR_STATIC) {
d991 1
d993 1
d997 1
a997 2
			(void)lockmgr(&pp->pr_resourcelock, LK_RELEASE, NULL,
				      curproc/*XXX*/);
d1001 1
d1011 1
a1011 1
	(void)lockmgr(&pp->pr_resourcelock, LK_RELEASE, NULL, curproc/*XXX*/);
d1017 2
d1020 1
a1020 1
int
d1030 1
a1030 1
	int n;
d1032 2
a1033 1
	simple_lock(&pp->pr_lock);
d1035 1
a1035 1
	if ((pp->pr_flags & PR_PHINPAGE) != 0) {
d1038 1
d1040 1
d1052 1
a1052 1
	ph->ph_time.tv_sec = ph->ph_time.tv_usec = 0;
d1073 1
d1094 1
d1096 50
a1145 2
	simple_unlock(&pp->pr_lock);
	return (0);
d1153 3
a1156 1
	(void)lockmgr(&pp->pr_resourcelock, LK_EXCLUSIVE, NULL, curproc/*XXX*/);
d1160 12
a1171 2
		: roundup(pp->pr_itemsperpage,n) / pp->pr_itemsperpage;
	(void)lockmgr(&pp->pr_resourcelock, LK_RELEASE, NULL, curproc/*XXX*/);
d1180 2
a1181 1
	(void)lockmgr(&pp->pr_resourcelock, LK_EXCLUSIVE, NULL, curproc/*XXX*/);
d1184 3
a1186 2
		: roundup(pp->pr_itemsperpage,n) / pp->pr_itemsperpage;
	(void)lockmgr(&pp->pr_resourcelock, LK_RELEASE, NULL, curproc/*XXX*/);
d1189 26
a1251 3
	/*
	 * With UVM, we can use the kernel_map.
	 */
d1271 1
a1271 1
pool_reclaim (pp)
d1273 2
d1277 2
a1278 1
	struct timeval curtime = time;
d1280 1
a1280 1
	if (pp->pr_flags & PR_STATIC)
d1283 1
a1283 1
	if (simple_lock_try(&pp->pr_lock) == 0)
d1285 5
d1303 9
d1316 2
a1317 1
	simple_unlock(&pp->pr_lock);
d1323 2
d1331 1
a1331 1
	int s = splimp();
d1333 5
a1337 5
	/* XXX:lock pool head */
	if (drainpp == NULL && (drainpp = TAILQ_FIRST(&pool_head)) == NULL) {
		splx(s);
		return;
	}
a1340 1
	/* XXX:unlock pool head */
d1343 3
a1349 1
#if defined(POOL_DIAGNOSTIC) || defined(DEBUG)
d1354 1
a1354 1
pool_print(pp, label)
d1356 54
a1409 1
	char *label;
d1411 65
d1477 1
a1477 2
	if (label != NULL)
		printf("%s: ", label);
d1479 1
a1479 13
	printf("pool %s: nalloc %lu nfree %lu npagealloc %lu npagefree %lu\n"
	       "         npages %u minitems %u itemsperpage %u itemoffset %u\n"
	       "         nidle %lu\n",
		pp->pr_wchan,
		pp->pr_nget,
		pp->pr_nput,
		pp->pr_npagealloc,
		pp->pr_npagefree,
		pp->pr_npages,
		pp->pr_minitems,
		pp->pr_itemsperpage,
		pp->pr_itemoffset,
		pp->pr_nidle);
d1490 1
a1490 1
	simple_lock(&pp->pr_lock);
d1500 2
a1501 1
		if (page != ph->ph_page && (pp->pr_flags & PR_PHINPAGE) != 0) {
d1543 1
a1543 1
	simple_unlock(&pp->pr_lock);
a1545 1
#endif /* POOL_DIAGNOSTIC || DEBUG */
@


1.2.2.1
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 2
a2 2
/*	$OpenBSD: subr_pool.c,v 1.5 2001/03/21 23:24:51 art Exp $	*/
/*	$NetBSD: subr_pool.c,v 1.37 2000/06/10 18:44:44 sommerfeld Exp $	*/
d5 1
a5 1
 * Copyright (c) 1997, 1999 The NetBSD Foundation, Inc.
d9 1
a9 2
 * by Paul Kranenburg; by Jason R. Thorpe of the Numerical Aerospace
 * Simulation Facility, NASA Ames Research Center.
a47 1
#include <sys/syslog.h>
a51 1
#if defined(UVM)
a52 10
#endif

/*
 * XXX - for now.
 */
#define __predict_false(X) ((X) != 0)
#define SIMPLELOCK_INITIALIZER { SLOCK_UNLOCKED }
#ifdef LOCKDEBUG
#define simple_lock_freecheck(a, s) do { /* nothing */ } while (0)
#endif
d63 1
d67 1
a67 1
TAILQ_HEAD(,pool) pool_head = TAILQ_HEAD_INITIALIZER(pool_head);
d76 1
a76 4
static struct pool	*drainpp;

/* This spin lock protects both pool_head and drainpp. */
struct simplelock pool_head_slock = SIMPLELOCK_INITIALIZER;
d93 1
a94 1
#define	PI_MAGIC 0xdeadbeef
d100 1
a100 1
#define	PR_HASH_INDEX(pp,addr) \
d108 1
a108 2
static int	pool_catchup __P((struct pool *));
static void	pool_prime_page __P((struct pool *, caddr_t));
a111 2
static void pool_print1 __P((struct pool *, const char *,
	int (*)(const char *, ...)));
d113 1
d121 2
a122 2
#define	PRLOG_GET	1
#define	PRLOG_PUT	2
a132 1
#ifdef DIAGNOSTIC
d134 1
a134 6
static void	pr_printlog __P((struct pool *, struct pool_item *,
		    int (*)(const char *, ...)));
static void	pr_enter __P((struct pool *, const char *, long));
static void	pr_leave __P((struct pool *));
static void	pr_enter_check __P((struct pool *,
		    int (*)(const char *, ...)));
d147 1
a147 1
	if ((pp->pr_roflags & PR_LOGGING) == 0)
d165 1
a165 1
pr_printlog(pp, pi, pr)
a166 2
	struct pool_item *pi;
	int (*pr) __P((const char *, ...));
d171 1
a171 1
	if ((pp->pr_roflags & PR_LOGGING) == 0)
d174 2
d182 6
a187 8
			if (pi == NULL || pi == pl->pl_addr) {
				(*pr)("\tlog entry %d:\n", i);
				(*pr)("\t\taction = %s, addr = %p\n",
				    pl->pl_action == PRLOG_GET ? "get" : "put",
				    pl->pl_addr);
				(*pr)("\t\tfile: %s at line %lu\n",
				    pl->pl_file, pl->pl_line);
			}
d193 4
a197 50
static __inline__ void
pr_enter(pp, file, line)
	struct pool *pp;
	const char *file;
	long line;
{

	if (__predict_false(pp->pr_entered_file != NULL)) {
		printf("pool %s: reentrancy at file %s line %ld\n",
		    pp->pr_wchan, file, line);
		printf("         previous entry at file %s line %ld\n",
		    pp->pr_entered_file, pp->pr_entered_line);
		panic("pr_enter");
	}

	pp->pr_entered_file = file;
	pp->pr_entered_line = line;
}

static __inline__ void
pr_leave(pp)
	struct pool *pp;
{

	if (__predict_false(pp->pr_entered_file == NULL)) {
		printf("pool %s not entered?\n", pp->pr_wchan);
		panic("pr_leave");
	}

	pp->pr_entered_file = NULL;
	pp->pr_entered_line = 0;
}

static __inline__ void
pr_enter_check(pp, pr)
	struct pool *pp;
	int (*pr) __P((const char *, ...));
{

	if (pp->pr_entered_file != NULL)
		(*pr)("\n\tcurrently entered from file %s line %ld\n",
		    pp->pr_entered_file, pp->pr_entered_line);
}
#else
#define	pr_log(pp, v, action, file, line)
#define	pr_printlog(pp, pi, pr)
#define	pr_enter(pp, file, line)
#define	pr_leave(pp)
#define	pr_enter_check(pp, pr)
#endif /* DIAGNOSTIC */
d209 1
a209 1
	if ((pp->pr_roflags & PR_PHINPAGE) != 0)
a236 2
		if (pp->pr_nitems < pp->pr_itemsperpage)
			panic("pr_rmpage: nitems inconsistent");
a240 2
	pp->pr_nitems -= pp->pr_itemsperpage;

d249 1
a249 2
	if ((pp->pr_roflags & PR_PHINPAGE) == 0) {
		int s;
a250 1
		s = splhigh();
a251 1
		splx(s);
d278 1
a278 1
	const char *wchan;
d292 5
d323 1
a323 1
	const char	*wchan;
a330 8
#ifdef POOL_DIAGNOSTIC
	/*
	 * Always log if POOL_DIAGNOSTIC is defined.
	 */
	if (pool_logsize != 0)
		flags |= PR_LOGGING;
#endif

d334 1
a334 1
	if (!powerof2(pagesz))
a354 5
	size = ALIGN(size);
	if (size >= pagesz)
		panic("pool_init: pool item size (%lu) too large",
		      (u_long)size);

d358 1
d365 2
a366 3
	pp->pr_roflags = flags;
	pp->pr_flags = 0;
	pp->pr_size = size;
a374 8
	pp->pr_nitems = 0;
	pp->pr_nout = 0;
	pp->pr_hardlimit = UINT_MAX;
	pp->pr_hardlimit_warning = NULL;
	pp->pr_hardlimit_ratecap.tv_sec = 0;
	pp->pr_hardlimit_ratecap.tv_usec = 0;
	pp->pr_hardlimit_warning_last.tv_sec = 0;
	pp->pr_hardlimit_warning_last.tv_usec = 0;
d385 1
a385 1
		pp->pr_roflags |= PR_PHINPAGE;
d423 6
a428 5
	if (flags & PR_LOGGING) {
		if (kmem_map == NULL ||
		    (pp->pr_log = malloc(pool_logsize * sizeof(struct pool_log),
		     M_TEMP, M_NOWAIT)) == NULL)
			pp->pr_roflags &= ~PR_LOGGING;
d432 1
d434 2
a435 4
	pp->pr_entered_file = NULL;
	pp->pr_entered_line = 0;

	simple_lock_init(&pp->pr_slock);
a438 1
	 * XXX LOCKING.
d445 1
a445 4
	/* Insert into the list of all pools. */
	simple_lock(&pool_head_slock);
	TAILQ_INSERT_TAIL(&pool_head, pp, pr_poollist);
	simple_unlock(&pool_head_slock);
d458 4
a461 4
	if (pp->pr_nout != 0) {
		pr_printlog(pp, NULL, printf);
		panic("pool_destroy: pool busy: still out: %u\n",
		    pp->pr_nout);
d466 1
a466 1
	if ((pp->pr_roflags & PR_STATIC) == 0)
a470 1
	simple_lock(&pool_head_slock);
a471 1
	/* XXX Only clear this if we were drainpp? */
a472 1
	simple_unlock(&pool_head_slock);
d474 2
a475 1
	if ((pp->pr_roflags & PR_LOGGING) != 0)
d477 1
d479 1
a479 1
	if (pp->pr_roflags & PR_FREEHEADER)
d487 1
d494 6
d506 2
a507 3
	if (__predict_false((pp->pr_roflags & PR_STATIC) &&
			    (flags & PR_MALLOCOK))) {
		pr_printlog(pp, NULL, printf);
d512 2
a513 2
	if (__predict_false(curproc == NULL && /* doing_shutdown == 0 && XXX*/
			    (flags & PR_WAITOK) != 0))
a515 49
	simple_lock(&pp->pr_slock);
	pr_enter(pp, file, line);

 startover:
	/*
	 * Check to see if we've reached the hard limit.  If we have,
	 * and we can wait, then wait until an item has been returned to
	 * the pool.
	 */
#ifdef DIAGNOSTIC
	if (__predict_false(pp->pr_nout > pp->pr_hardlimit)) {
		pr_leave(pp);
		simple_unlock(&pp->pr_slock);
		panic("pool_get: %s: crossed hard limit", pp->pr_wchan);
	}
#endif
	if (__predict_false(pp->pr_nout == pp->pr_hardlimit)) {
		if ((flags & PR_WAITOK) && !(flags & PR_LIMITFAIL)) {
			/*
			 * XXX: A warning isn't logged in this case.  Should
			 * it be?
			 */
			pp->pr_flags |= PR_WANTED;
			pr_leave(pp);
			simple_unlock(&pp->pr_slock);
			tsleep((caddr_t)pp, PSWP, (char *)pp->pr_wchan, 0);
			simple_lock(&pp->pr_slock);
			pr_enter(pp, file, line);
			goto startover;
		}

		/*
		 * Log a message that the hard limit has been hit.
		 */
		if (pp->pr_hardlimit_warning != NULL &&
		    ratecheck(&pp->pr_hardlimit_warning_last,
			      &pp->pr_hardlimit_ratecap))
			log(LOG_ERR, "%s\n", pp->pr_hardlimit_warning);

		if (flags & PR_URGENT)
			panic("pool_get: urgent");

		pp->pr_nfail++;

		pr_leave(pp);
		simple_unlock(&pp->pr_slock);
		return (NULL);
	}

d522 1
a522 1
	if ((ph = pp->pr_curpage) == NULL) {
d524 2
d527 8
a534 8
#ifdef DIAGNOSTIC
		if (pp->pr_nitems != 0) {
			simple_unlock(&pp->pr_slock);
			printf("pool_get: %s: curpage NULL, nitems %u\n",
			    pp->pr_wchan, pp->pr_nitems);
			panic("pool_get: nitems inconsistent\n");
		}
#endif
d536 1
a536 7
		/*
		 * Call the back-end page allocator for more memory.
		 * Release the pool lock, as the back-end page allocator
		 * may block.
		 */
		pr_leave(pp);
		simple_unlock(&pp->pr_slock);
a537 3
		simple_lock(&pp->pr_slock);
		pr_enter(pp, file, line);

a538 9
			/*
			 * We were unable to allocate a page, but
			 * we released the lock during allocation,
			 * so perhaps items were freed back to the
			 * pool.  Check for this case.
			 */
			if (pp->pr_curpage != NULL)
				goto startover;

a540 1

d543 2
a544 2
				pr_leave(pp);
				simple_unlock(&pp->pr_slock);
a549 1
			 *
a553 3
			 *
			 * XXX: maybe we should wake up once a second and
			 * try again?
d556 5
a560 6
			pr_leave(pp);
			simple_unlock(&pp->pr_slock);
			tsleep((caddr_t)pp, PSWP, (char *)pp->pr_wchan, 0);
			simple_lock(&pp->pr_slock);
			pr_enter(pp, file, line);
			goto startover;
d567 4
a570 2
		/* Start the allocation process over. */
		goto startover;
d573 1
a573 3
	if (__predict_false((v = pi = TAILQ_FIRST(&ph->ph_itemlist)) == NULL)) {
		pr_leave(pp);
		simple_unlock(&pp->pr_slock);
d575 1
a575 10
	}
#ifdef DIAGNOSTIC
	if (__predict_false(pp->pr_nitems == 0)) {
		pr_leave(pp);
		simple_unlock(&pp->pr_slock);
		printf("pool_get: %s: items on itemlist, nitems %u\n",
		    pp->pr_wchan, pp->pr_nitems);
		panic("pool_get: nitems inconsistent\n");
	}
#endif
d579 2
a580 2
	if (__predict_false(pi->pi_magic != PI_MAGIC)) {
		pr_printlog(pp, pi, printf);
a590 2
	pp->pr_nitems--;
	pp->pr_nout++;
d593 1
a593 1
		if (__predict_false(pp->pr_nidle == 0))
a599 8
#ifdef DIAGNOSTIC
		if (__predict_false(ph->ph_nmissing != pp->pr_itemsperpage)) {
			pr_leave(pp);
			simple_unlock(&pp->pr_slock);
			panic("pool_get: %s: nmissing inconsistent",
			    pp->pr_wchan);
		}
#endif
d605 2
a606 5
		 * Migrate empty pages to the end of the list.  This
		 * will speed the update of curpage as pages become
		 * idle.  Empty pages intermingled with idle pages
		 * is no big deal.  As soon as a page becomes un-empty,
		 * it will move back to the head of the list.
d609 2
a610 3
		TAILQ_INSERT_TAIL(&pp->pr_pagelist, ph, ph_pagelist);
		for (ph = TAILQ_FIRST(&pp->pr_pagelist); ph != NULL;
		     ph = TAILQ_NEXT(ph, ph_pagelist))
d618 1
a618 15

	/*
	 * If we have a low water mark and we are now below that low
	 * water mark, add more items to the pool.
	 */
	if (pp->pr_nitems < pp->pr_minitems && pool_catchup(pp) != 0) {
		/*
		 * XXX: Should we log a warning?  Should we set up a timeout
		 * to try again in a second or so?  The latter could break
		 * a caller's assumptions about interrupt protection, etc.
		 */
	}

	pr_leave(pp);
	simple_unlock(&pp->pr_slock);
d625 1
d632 6
a641 1
	int s;
d645 1
a645 10
	simple_lock(&pp->pr_slock);
	pr_enter(pp, file, line);

#ifdef DIAGNOSTIC
	if (__predict_false(pp->pr_nout == 0)) {
		printf("pool %s: putting with none out\n",
		    pp->pr_wchan);
		panic("pool_put");
	}
#endif
d649 2
a650 2
	if (__predict_false((ph = pr_find_pagehead(pp, page)) == NULL)) {
		pr_printlog(pp, NULL, printf);
a653 7
#ifdef LOCKDEBUG
	/*
	 * Check if we're freeing a locked simple lock.
	 */
	simple_lock_freecheck((caddr_t)pi, ((caddr_t)pi) + pp->pr_size);
#endif

a659 10
#ifdef DEBUG
	{
		int i, *ip = v;

		for (i = 0; i < pp->pr_size / sizeof(int); i++) {
			*ip++ = PI_MAGIC;
		}
	}
#endif

a662 2
	pp->pr_nitems++;
	pp->pr_nout--;
a671 2
		pr_leave(pp);
		simple_unlock(&pp->pr_slock);
d673 1
d678 2
a679 12
	 * If this page is now complete, do one of two things:
	 *
	 *	(1) If we have more pages than the page high water
	 *	    mark, free the page back to the system.
	 *
	 *	(2) Move it to the end of the page list, so that
	 *	    we minimize our chances of fragmenting the
	 *	    pool.  Idle pages migrate to the end (along with
	 *	    completely empty pages, so that we find un-empty
	 *	    pages more quickly when we update curpage) of the
	 *	    list so they can be more easily swept up by
	 *	    the pagedaemon when pages are scarce.
d684 3
d688 1
d692 1
d694 1
a694 18
			/*
			 * Update the timestamp on the page.  A page must
			 * be idle for some period of time before it can
			 * be reclaimed by the pagedaemon.  This minimizes
			 * ping-pong'ing for memory.
			 */
			s = splclock();
			ph->ph_time = mono_time;
			splx(s);

			/*
			 * Update the current page pointer.  Just look for
			 * the first page with any free items.
			 *
			 * XXX: Maybe we want an option to look for the
			 * page with the fewest available items, to minimize
			 * fragmentation?
			 */
a702 14
	/*
	 * If the page has just become un-empty, move it to the head of
	 * the list, and make it the current page.  The next allocation
	 * will get the item from this page, instead of further fragmenting
	 * the pool.
	 */
	else if (ph->ph_nmissing == (pp->pr_itemsperpage - 1)) {
		TAILQ_REMOVE(&pp->pr_pagelist, ph, ph_pagelist);
		TAILQ_INSERT_HEAD(&pp->pr_pagelist, ph, ph_pagelist);
		pp->pr_curpage = ph;
	}

	pr_leave(pp);
	simple_unlock(&pp->pr_slock);
d704 1
d720 1
a720 1
	if (__predict_false(storage && !(pp->pr_roflags & PR_STATIC)))
d725 1
a725 2
	simple_lock(&pp->pr_slock);

d728 1
a728 1
		roundup(newnitems, pp->pr_itemsperpage) / pp->pr_itemsperpage
d732 2
a733 1
		if (pp->pr_roflags & PR_STATIC) {
a736 1
			simple_unlock(&pp->pr_slock);
a737 1
			simple_lock(&pp->pr_slock);
d741 2
a742 1
			simple_unlock(&pp->pr_slock);
a745 1
		pp->pr_npagealloc++;
d755 1
a755 1
	simple_unlock(&pp->pr_slock);
a760 2
 *
 * Note, we must be called with the pool descriptor LOCKED.
d762 1
a762 1
static void
d772 1
a772 1
	int s, n;
d774 1
a774 2
	if (((u_long)cp & (pp->pr_pagesz - 1)) != 0)
		panic("pool_prime_page: %s: unaligned page", pp->pr_wchan);
d776 1
a776 1
	if ((pp->pr_roflags & PR_PHINPAGE) != 0) {
a778 1
		s = splhigh();
a779 1
		splx(s);
d791 1
a791 1
	memset(&ph->ph_time, 0, sizeof(ph->ph_time));
a811 1
	pp->pr_nitems += n;
a831 1
}
d833 2
a834 50
/*
 * Like pool_prime(), except this is used by pool_get() when nitems
 * drops below the low water mark.  This is used to catch up nitmes
 * with the low water mark.
 *
 * Note 1, we never wait for memory here, we let the caller decide what to do.
 *
 * Note 2, this doesn't work with static pools.
 *
 * Note 3, we must be called with the pool already locked, and we return
 * with it locked.
 */
static int
pool_catchup(pp)
	struct pool *pp;
{
	caddr_t cp;
	int error = 0;

	if (pp->pr_roflags & PR_STATIC) {
		/*
		 * We dropped below the low water mark, and this is not a
		 * good thing.  Log a warning.
		 *
		 * XXX: rate-limit this?
		 */
		printf("WARNING: static pool `%s' dropped below low water "
		    "mark\n", pp->pr_wchan);
		return (0);
	}

	while (pp->pr_nitems < pp->pr_minitems) {
		/*
		 * Call the page back-end allocator for more memory.
		 *
		 * XXX: We never wait, so should we bother unlocking
		 * the pool descriptor?
		 */
		simple_unlock(&pp->pr_slock);
		cp = (*pp->pr_alloc)(pp->pr_pagesz, 0, pp->pr_mtype);
		simple_lock(&pp->pr_slock);
		if (__predict_false(cp == NULL)) {
			error = ENOMEM;
			break;
		}
		pp->pr_npagealloc++;
		pool_prime_page(pp, cp);
	}

	return (error);
a841 3
	int error;

	simple_lock(&pp->pr_slock);
d843 1
d847 2
a848 12
		: roundup(n, pp->pr_itemsperpage) / pp->pr_itemsperpage;

	/* Make sure we're caught up with the newly-set low water mark. */
	if ((error = pool_catchup(pp)) != 0) {
		/*
		 * XXX: Should we log a warning?  Should we set up a timeout
		 * to try again in a second or so?  The latter could break
		 * a caller's assumptions about interrupt protection, etc.
		 */
	}

	simple_unlock(&pp->pr_slock);
d857 1
a857 2
	simple_lock(&pp->pr_slock);

d860 2
a861 3
		: roundup(n, pp->pr_itemsperpage) / pp->pr_itemsperpage;

	simple_unlock(&pp->pr_slock);
a863 26
void
pool_sethardlimit(pp, n, warnmess, ratecap)
	pool_handle_t pp;
	int n;
	const char *warnmess;
	int ratecap;
{

	simple_lock(&pp->pr_slock);

	pp->pr_hardlimit = n;
	pp->pr_hardlimit_warning = warnmess;
	pp->pr_hardlimit_ratecap.tv_sec = ratecap;
	pp->pr_hardlimit_warning_last.tv_sec = 0;
	pp->pr_hardlimit_warning_last.tv_usec = 0;

	/*
	 * In-line version of pool_sethiwat(), because we don't want to
	 * release the lock.
	 */
	pp->pr_maxpages = (n == 0)
		? 0
		: roundup(n, pp->pr_itemsperpage) / pp->pr_itemsperpage;

	simple_unlock(&pp->pr_slock);
}
a873 1
#if defined(UVM)
a876 11
#else
	boolean_t waitok = (flags & PR_WAITOK) ? TRUE : FALSE;
	int s;
	vaddr_t va;

	s = splimp();
	va = kmem_malloc(kmem_map, PAGE_SIZE, waitok);
	splx(s);

	return ((void *)va);
#endif
d885 1
a885 1
#if defined(UVM)
a886 7
#else
	int s;

	s = splimp();
	kmem_free(kmem_map, (vaddr_t)v, PAGE_SIZE);
	splx(s);
#endif
a898 1
#if defined(UVM)
d901 3
a905 3
#else
	return pool_page_alloc(sz, flags, mtype);
#endif
a914 1
#if defined(UVM)
a915 3
#else
	pool_page_free(v, sz, mtype);
#endif
d923 1
a923 1
_pool_reclaim(pp, file, line)
a924 2
	const char *file;
	long line;
d927 1
a927 2
	struct timeval curtime;
	int s;
d929 1
a929 1
	if (pp->pr_roflags & PR_STATIC)
d932 1
a932 1
	if (simple_lock_try(&pp->pr_slock) == 0)
a933 5
	pr_enter(pp, file, line);

	s = splclock();
	curtime = mono_time;
	splx(s);
a946 9

			/*
			 * If freeing this page would put us below
			 * the low water mark, stop now.
			 */
			if ((pp->pr_nitems - pp->pr_itemsperpage) <
			    pp->pr_minitems)
				break;

d951 1
a951 2
	pr_leave(pp);
	simple_unlock(&pp->pr_slock);
a956 2
 *
 * Note, we must never be called from an interrupt context.
d963 1
a963 1
	int s;
d965 5
a969 5
	s = splimp();
	simple_lock(&pool_head_slock);

	if (drainpp == NULL && (drainpp = TAILQ_FIRST(&pool_head)) == NULL)
		goto out;
d973 1
a975 3

 out:
	simple_unlock(&pool_head_slock);
d980 1
d985 1
a985 1
pool_print(pp, modif)
d987 1
a987 1
	const char *modif;
a988 118
	int s;

	s = splimp();
	if (simple_lock_try(&pp->pr_slock) == 0) {
		printf("pool %s is locked; try again later\n",
		    pp->pr_wchan);
		splx(s);
		return;
	}
	pool_print1(pp, modif, printf);
	simple_unlock(&pp->pr_slock);
	splx(s);
}

void
pool_printit(pp, modif, pr)
	struct pool *pp;
	const char *modif;
	int (*pr) __P((const char *, ...));
{
	int didlock = 0;

	if (pp == NULL) {
		(*pr)("Must specify a pool to print.\n");
		return;
	}

	/*
	 * Called from DDB; interrupts should be blocked, and all
	 * other processors should be paused.  We can skip locking
	 * the pool in this case.
	 *
	 * We do a simple_lock_try() just to print the lock
	 * status, however.
	 */

	if (simple_lock_try(&pp->pr_slock) == 0)
		(*pr)("WARNING: pool %s is locked\n", pp->pr_wchan);
	else
		didlock = 1;

	pool_print1(pp, modif, pr);

	if (didlock)
		simple_unlock(&pp->pr_slock);
}

static void
pool_print1(pp, modif, pr)
	struct pool *pp;
	const char *modif;
	int (*pr) __P((const char *, ...));
{
	struct pool_item_header *ph;
#ifdef DIAGNOSTIC
	struct pool_item *pi;
#endif
	int print_log = 0, print_pagelist = 0;
	char c;

	while ((c = *modif++) != '\0') {
		if (c == 'l')
			print_log = 1;
		if (c == 'p')
			print_pagelist = 1;
		modif++;
	}

	(*pr)("POOL %s: size %u, align %u, ioff %u, roflags 0x%08x\n",
	    pp->pr_wchan, pp->pr_size, pp->pr_align, pp->pr_itemoffset,
	    pp->pr_roflags);
	(*pr)("\tpagesz %u, mtype %d\n", pp->pr_pagesz, pp->pr_mtype);
	(*pr)("\talloc %p, release %p\n", pp->pr_alloc, pp->pr_free);
	(*pr)("\tminitems %u, minpages %u, maxpages %u, npages %u\n",
	    pp->pr_minitems, pp->pr_minpages, pp->pr_maxpages, pp->pr_npages);
	(*pr)("\titemsperpage %u, nitems %u, nout %u, hardlimit %u\n",
	    pp->pr_itemsperpage, pp->pr_nitems, pp->pr_nout, pp->pr_hardlimit);

	(*pr)("\n\tnget %lu, nfail %lu, nput %lu\n",
	    pp->pr_nget, pp->pr_nfail, pp->pr_nput);
	(*pr)("\tnpagealloc %lu, npagefree %lu, hiwat %u, nidle %lu\n",
	    pp->pr_npagealloc, pp->pr_npagefree, pp->pr_hiwat, pp->pr_nidle);

	if (print_pagelist == 0)
		goto skip_pagelist;

	if ((ph = TAILQ_FIRST(&pp->pr_pagelist)) != NULL)
		(*pr)("\n\tpage list:\n");
	for (; ph != NULL; ph = TAILQ_NEXT(ph, ph_pagelist)) {
		(*pr)("\t\tpage %p, nmissing %d, time %lu,%lu\n",
		    ph->ph_page, ph->ph_nmissing,
		    (u_long)ph->ph_time.tv_sec,
		    (u_long)ph->ph_time.tv_usec);
#ifdef DIAGNOSTIC
		for (pi = TAILQ_FIRST(&ph->ph_itemlist); pi != NULL;
		     pi = TAILQ_NEXT(pi, pi_list)) {
			if (pi->pi_magic != PI_MAGIC) {
				(*pr)("\t\t\titem %p, magic 0x%x\n",
				    pi, pi->pi_magic);
			}
		}
#endif
	}
	if (pp->pr_curpage == NULL)
		(*pr)("\tno current page\n");
	else
		(*pr)("\tcurpage %p\n", pp->pr_curpage->ph_page);

 skip_pagelist:

	if (print_log == 0)
		goto skip_log;

	(*pr)("\n");
	if ((pp->pr_roflags & PR_LOGGING) == 0)
		(*pr)("\tno log\n");
	else
		pr_printlog(pp, NULL, pr);
d990 2
a991 1
 skip_log:
d993 13
a1005 1
	pr_enter_check(pp, pr);
d1016 1
a1016 1
	simple_lock(&pp->pr_slock);
d1026 1
a1026 2
		if (page != ph->ph_page &&
		    (pp->pr_roflags & PR_PHINPAGE) != 0) {
d1068 1
a1068 1
	simple_unlock(&pp->pr_slock);
d1071 1
@


1.2.2.2
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: subr_pool.c,v 1.59 2001/06/05 18:51:04 thorpej Exp $	*/
d5 1
a5 1
 * Copyright (c) 1997, 1999, 2000 The NetBSD Foundation, Inc.
a49 1
#include <sys/sysctl.h>
d54 1
d56 1
a61 1
#define __predict_true(X) ((X) != 0)
a65 1
#define LOCK_ASSERT(x) /* nothing */
a117 2
#define	POOL_NEEDS_CATCHUP(pp)						\
	((pp)->pr_nitems < (pp)->pr_minitems)
a118 5
/*
 * Every pool get a unique serial number assigned to it. If this counter
 * wraps, we're screwed, but we shouldn't create so many pools anyway.
 */
unsigned int pool_serial;
d120 7
a126 25
/*
 * Pool cache management.
 *
 * Pool caches provide a way for constructed objects to be cached by the
 * pool subsystem.  This can lead to performance improvements by avoiding
 * needless object construction/destruction; it is deferred until absolutely
 * necessary.
 *
 * Caches are grouped into cache groups.  Each cache group references
 * up to 16 constructed objects.  When a cache allocates an object
 * from the pool, it calls the object's constructor and places it into
 * a cache group.  When a cache group frees an object back to the pool,
 * it first calls the object's destructor.  This allows the object to
 * persist in constructed form while freed to the cache.
 *
 * Multiple caches may exist for each pool.  This allows a single
 * object type to have multiple constructed forms.  The pool references
 * each cache, so that when a pool is drained by the pagedaemon, it can
 * drain each individual cache as well.  Each time a cache is drained,
 * the most idle cache group is freed to the pool in its entirety.
 *
 * Pool caches are layed on top of pools.  By layering them, we can avoid
 * the complexity of cache management for pools which would not benefit
 * from it.
 */
d128 2
a129 23
/* The cache group pool. */
static struct pool pcgpool;

/* The pool cache group. */
#define	PCG_NOBJECTS		16
struct pool_cache_group {
	TAILQ_ENTRY(pool_cache_group)
		pcg_list;	/* link in the pool cache's group list */
	u_int	pcg_avail;	/* # available objects */
				/* pointers to the objects */
	void	*pcg_objects[PCG_NOBJECTS];
};

static void	pool_cache_reclaim(struct pool_cache *);

static int	pool_catchup(struct pool *);
static void	pool_prime_page(struct pool *, caddr_t,
		    struct pool_item_header *);
static void	*pool_page_alloc(unsigned long, int, int);
static void	pool_page_free(void *, unsigned long, int);

static void pool_print1(struct pool *, const char *,
	int (*)(const char *, ...));
d132 1
a132 1
 * Pool log entry. An array of these is allocated in pool_init().
d150 16
a165 3
#ifdef POOL_DIAGNOSTIC
static __inline void
pr_log(struct pool *pp, void *a, int action, const char *file, long line)
d188 4
a191 2
pr_printlog(struct pool *pp, struct pool_item *pi,
    int (*pr)(const char *, ...))
d219 5
a223 2
static __inline void
pr_enter(struct pool *pp, const char *file, long line)
d238 3
a240 2
static __inline void
pr_leave(struct pool *pp)
d253 3
a255 1
pr_enter_check(struct pool *pp, int (*pr)(const char *, ...))
d268 1
a268 1
#endif /* POOL_DIAGNOSTIC */
d273 4
a276 2
static __inline struct pool_item_header *
pr_find_pagehead(struct pool *pp, caddr_t page)
d295 4
a298 2
static __inline void
pr_rmpage(struct pool *pp, struct pool_item_header *ph)
d348 36
d390 11
a400 5
pool_init(struct pool *pp, size_t size, u_int align, u_int ioff, int flags,
    const char *wchan, size_t pagesz,
    void *(*alloc)(unsigned long, int, int),
    void (*release)(void *, unsigned long, int),
    int mtype)
d437 1
a437 1
	if (size > pagesz)
a444 1
	TAILQ_INIT(&pp->pr_cachelist);
a468 3
	pp->pr_serial = ++pool_serial;
	if (pool_serial == 0)
		panic("pool_init: too much uptime");
a499 1
	KASSERT(pp->pr_itemsperpage != 0);
a516 1
#ifdef POOL_DIAGNOSTIC
a524 1
#endif
d532 1
a532 2
	 * Initialize private page header pool and cache magazine pool if we
	 * haven't done so yet.
d537 1
a537 3
		    0, "phpool", 0, 0, 0, 0);
		pool_init(&pcgpool, sizeof(struct pool_cache_group), 0, 0,
		    0, "pcgpool", 0, 0, 0, 0);
d550 2
a551 1
pool_destroy(struct pool *pp)
a553 5
	struct pool_cache *pc;

	/* Destroy all caches for this pool. */
	while ((pc = TAILQ_FIRST(&pp->pr_cachelist)) != NULL)
		pool_cache_destroy(pc);
a574 1
#ifdef POOL_DIAGNOSTIC
a576 1
#endif
a581 18
static __inline struct pool_item_header *
pool_alloc_item_header(struct pool *pp, caddr_t storage, int flags)
{
	struct pool_item_header *ph;
	int s;

	LOCK_ASSERT(simple_lock_held(&pp->pr_slock) == 0);

	if ((pp->pr_roflags & PR_PHINPAGE) != 0)
		ph = (struct pool_item_header *) (storage + pp->pr_phoffset);
	else {
		s = splhigh();
		ph = pool_get(&phpool, flags);
		splx(s);
	}

	return (ph);
}
d587 5
a591 5
#ifdef POOL_DIAGNOSTIC
_pool_get(struct pool *pp, int flags, const char *file, long line)
#else
pool_get(struct pool *pp, int flags)
#endif
d593 1
a595 1
	void *v;
d603 1
a608 1
#endif
d665 2
a683 2
		if (__predict_true(v != NULL))
			ph = pool_alloc_item_header(pp, v, flags);
d687 1
a687 4
		if (__predict_false(v == NULL || ph == NULL)) {
			if (v != NULL)
				(*pp->pr_free)(v, pp->pr_pagesz, pp->pr_mtype);

d689 4
a692 4
			 * We were unable to allocate a page or item
			 * header, but we released the lock during
			 * allocation, so perhaps items were freed
			 * back to the pool.  Check for this case.
d729 1
a729 1
		pool_prime_page(pp, v, ph);
d748 1
a748 1

d751 1
d810 1
a810 1
	if (POOL_NEEDS_CATCHUP(pp) && pool_catchup(pp) != 0) {
d824 1
a824 1
 * Internal version of pool_put().  Pool is already locked/entered.
d826 6
a831 2
static void
pool_do_put(struct pool *pp, void *v)
d840 3
d851 2
d895 2
a959 16
}

/*
 * Return resource to the pool; must be called at appropriate spl level
 */
#ifdef POOL_DIAGNOSTIC
void
_pool_put(struct pool *pp, void *v, const char *file, long line)
{

	simple_lock(&pp->pr_slock);
	pr_enter(pp, file, line);

	pr_log(pp, v, PRLOG_PUT, file, line);

	pool_do_put(pp, v);
a962 11
}
#undef pool_put
#endif /* POOL_DIAGNOSTIC */

void
pool_put(struct pool *pp, void *v)
{

	simple_lock(&pp->pr_slock);

	pool_do_put(pp, v);
a963 1
	simple_unlock(&pp->pr_slock);
a965 4
#ifdef POOL_DIAGNOSTIC
#define		pool_put(h, v)	_pool_put((h), (v), __FILE__, __LINE__)
#endif

d970 4
a973 1
pool_prime(struct pool *pp, int n)
a974 1
	struct pool_item_header *ph;
d976 7
a982 1
	int newpages, error = 0;
d986 4
a989 1
	newpages = roundup(n, pp->pr_itemsperpage) / pp->pr_itemsperpage;
d992 8
a999 5
		simple_unlock(&pp->pr_slock);
		cp = (*pp->pr_alloc)(pp->pr_pagesz, PR_NOWAIT, pp->pr_mtype);
		if (__predict_true(cp != NULL))
			ph = pool_alloc_item_header(pp, cp, PR_NOWAIT);
		simple_lock(&pp->pr_slock);
d1001 3
a1003 5
		if (__predict_false(cp == NULL || ph == NULL)) {
			error = ENOMEM;
			if (cp != NULL)
				(*pp->pr_free)(cp, pp->pr_pagesz, pp->pr_mtype);
			break;
a1005 1
		pool_prime_page(pp, cp, ph);
d1007 1
d1011 2
d1026 3
a1028 1
pool_prime_page(struct pool *pp, caddr_t storage, struct pool_item_header *ph)
d1031 1
d1035 1
a1035 1
	int n;
d1040 6
a1045 1
	if ((pp->pr_roflags & PR_PHINPAGE) == 0)
d1047 2
a1048 1
		    ph, ph_hashlist);
d1102 3
a1104 2
 * Used by pool_get() when nitems drops below the low water mark.  This
 * is used to catch up nitmes with the low water mark.
d1114 2
a1115 1
pool_catchup(struct pool *pp)
a1116 1
	struct pool_item_header *ph;
d1132 1
a1132 1
	while (POOL_NEEDS_CATCHUP(pp)) {
d1140 1
a1140 3
		cp = (*pp->pr_alloc)(pp->pr_pagesz, PR_NOWAIT, pp->pr_mtype);
		if (__predict_true(cp != NULL))
			ph = pool_alloc_item_header(pp, cp, PR_NOWAIT);
d1142 1
a1142 3
		if (__predict_false(cp == NULL || ph == NULL)) {
			if (cp != NULL)
				(*pp->pr_free)(cp, pp->pr_pagesz, pp->pr_mtype);
a1145 1
		pool_prime_page(pp, cp, ph);
d1147 1
d1154 3
a1156 1
pool_setlowat(struct pool *pp, int n)
d1168 1
a1168 1
	if (POOL_NEEDS_CATCHUP(pp) && (error = pool_catchup(pp) != 0)) {
d1180 3
a1182 1
pool_sethiwat(struct pool *pp, int n)
d1195 5
a1199 1
pool_sethardlimit(struct pool *pp, int n, const char *warnmess, int ratecap)
d1225 4
a1228 1
pool_page_alloc(unsigned long sz, int flags, int mtype)
d1230 1
d1234 11
d1248 4
a1251 1
pool_page_free(void *v, unsigned long sz, int mtype)
d1253 1
d1255 7
d1269 4
a1272 1
pool_page_alloc_nointr(unsigned long sz, int flags, int mtype)
d1274 1
d1279 3
d1285 4
a1288 1
pool_page_free_nointr(void *v, unsigned long sz, int mtype)
d1291 1
d1293 3
d1303 4
a1306 5
#ifdef POOL_DIAGNOSTIC
_pool_reclaim(struct pool *pp, const char *file, long line)
#else
pool_reclaim(struct pool *pp)
#endif
a1308 1
	struct pool_cache *pc;
a1318 7
	/*
	 * Reclaim items from the pool's caches.
	 */
	for (pc = TAILQ_FIRST(&pp->pr_cachelist); pc != NULL;
	     pc = TAILQ_NEXT(pc, pc_poollist))
		pool_cache_reclaim(pc);

d1359 2
a1360 1
pool_drain(void *arg)
d1365 1
a1365 1
	s = splvm();
d1386 3
a1388 1
pool_printit(struct pool *pp, const char *modif, int (*pr)(const char *, ...))
d1392 1
a1392 1
	s = splvm();
d1404 33
d1438 4
a1441 1
pool_print1(struct pool *pp, const char *modif, int (*pr)(const char *, ...))
a1443 2
	struct pool_cache *pc;
	struct pool_cache_group *pcg;
d1447 1
a1447 1
	int i, print_log = 0, print_pagelist = 0, print_cache = 0;
a1454 2
		if (c == 'c')
			print_cache = 1;
a1510 19
	if (print_cache == 0)
		goto skip_cache;

	for (pc = TAILQ_FIRST(&pp->pr_cachelist); pc != NULL;
	     pc = TAILQ_NEXT(pc, pc_poollist)) {
		(*pr)("\tcache %p: allocfrom %p freeto %p\n", pc,
		    pc->pc_allocfrom, pc->pc_freeto);
		(*pr)("\t    hits %lu misses %lu ngroups %lu nitems %lu\n",
		    pc->pc_hits, pc->pc_misses, pc->pc_ngroups, pc->pc_nitems);
		for (pcg = TAILQ_FIRST(&pc->pc_grouplist); pcg != NULL;
		     pcg = TAILQ_NEXT(pcg, pcg_list)) {
			(*pr)("\t\tgroup %p: avail %d\n", pcg, pcg->pcg_avail);
			for (i = 0; i < PCG_NOBJECTS; i++)
				(*pr)("\t\t\t%p\n", pcg->pcg_objects[i]);
		}
	}

 skip_cache:

d1515 3
a1517 1
pool_chk(struct pool *pp, const char *label)
a1576 346
}

/*
 * pool_cache_init:
 *
 *	Initialize a pool cache.
 *
 *	NOTE: If the pool must be protected from interrupts, we expect
 *	to be called at the appropriate interrupt priority level.
 */
void
pool_cache_init(struct pool_cache *pc, struct pool *pp,
    int (*ctor)(void *, void *, int),
    void (*dtor)(void *, void *),
    void *arg)
{

	TAILQ_INIT(&pc->pc_grouplist);
	simple_lock_init(&pc->pc_slock);

	pc->pc_allocfrom = NULL;
	pc->pc_freeto = NULL;
	pc->pc_pool = pp;

	pc->pc_ctor = ctor;
	pc->pc_dtor = dtor;
	pc->pc_arg  = arg;

	pc->pc_hits   = 0;
	pc->pc_misses = 0;

	pc->pc_ngroups = 0;

	pc->pc_nitems = 0;

	simple_lock(&pp->pr_slock);
	TAILQ_INSERT_TAIL(&pp->pr_cachelist, pc, pc_poollist);
	simple_unlock(&pp->pr_slock);
}

/*
 * pool_cache_destroy:
 *
 *	Destroy a pool cache.
 */
void
pool_cache_destroy(struct pool_cache *pc)
{
	struct pool *pp = pc->pc_pool;

	/* First, invalidate the entire cache. */
	pool_cache_invalidate(pc);

	/* ...and remove it from the pool's cache list. */
	simple_lock(&pp->pr_slock);
	TAILQ_REMOVE(&pp->pr_cachelist, pc, pc_poollist);
	simple_unlock(&pp->pr_slock);
}

static __inline void *
pcg_get(struct pool_cache_group *pcg)
{
	void *object;
	u_int idx;

	KASSERT(pcg->pcg_avail <= PCG_NOBJECTS);
	KASSERT(pcg->pcg_avail != 0);
	idx = --pcg->pcg_avail;

	KASSERT(pcg->pcg_objects[idx] != NULL);
	object = pcg->pcg_objects[idx];
	pcg->pcg_objects[idx] = NULL;

	return (object);
}

static __inline void
pcg_put(struct pool_cache_group *pcg, void *object)
{
	u_int idx;

	KASSERT(pcg->pcg_avail < PCG_NOBJECTS);
	idx = pcg->pcg_avail++;

	KASSERT(pcg->pcg_objects[idx] == NULL);
	pcg->pcg_objects[idx] = object;
}

/*
 * pool_cache_get:
 *
 *	Get an object from a pool cache.
 */
void *
pool_cache_get(struct pool_cache *pc, int flags)
{
	struct pool_cache_group *pcg;
	void *object;

#ifdef LOCKDEBUG
	if (flags & PR_WAITOK)
		simple_lock_only_held(NULL, "pool_cache_get(PR_WAITOK)");
#endif

	simple_lock(&pc->pc_slock);

	if ((pcg = pc->pc_allocfrom) == NULL) {
		for (pcg = TAILQ_FIRST(&pc->pc_grouplist); pcg != NULL;
		     pcg = TAILQ_NEXT(pcg, pcg_list)) {
			if (pcg->pcg_avail != 0) {
				pc->pc_allocfrom = pcg;
				goto have_group;
			}
		}

		/*
		 * No groups with any available objects.  Allocate
		 * a new object, construct it, and return it to
		 * the caller.  We will allocate a group, if necessary,
		 * when the object is freed back to the cache.
		 */
		pc->pc_misses++;
		simple_unlock(&pc->pc_slock);
		object = pool_get(pc->pc_pool, flags);
		if (object != NULL && pc->pc_ctor != NULL) {
			if ((*pc->pc_ctor)(pc->pc_arg, object, flags) != 0) {
				pool_put(pc->pc_pool, object);
				return (NULL);
			}
		}
		return (object);
	}

 have_group:
	pc->pc_hits++;
	pc->pc_nitems--;
	object = pcg_get(pcg);

	if (pcg->pcg_avail == 0)
		pc->pc_allocfrom = NULL;

	simple_unlock(&pc->pc_slock);

	return (object);
}

/*
 * pool_cache_put:
 *
 *	Put an object back to the pool cache.
 */
void
pool_cache_put(struct pool_cache *pc, void *object)
{
	struct pool_cache_group *pcg;

	simple_lock(&pc->pc_slock);

	if ((pcg = pc->pc_freeto) == NULL) {
		for (pcg = TAILQ_FIRST(&pc->pc_grouplist); pcg != NULL;
		     pcg = TAILQ_NEXT(pcg, pcg_list)) {
			if (pcg->pcg_avail != PCG_NOBJECTS) {
				pc->pc_freeto = pcg;
				goto have_group;
			}
		}

		/*
		 * No empty groups to free the object to.  Attempt to
		 * allocate one.
		 */
		simple_unlock(&pc->pc_slock);
		pcg = pool_get(&pcgpool, PR_NOWAIT);
		if (pcg != NULL) {
			memset(pcg, 0, sizeof(*pcg));
			simple_lock(&pc->pc_slock);
			pc->pc_ngroups++;
			TAILQ_INSERT_TAIL(&pc->pc_grouplist, pcg, pcg_list);
			if (pc->pc_freeto == NULL)
				pc->pc_freeto = pcg;
			goto have_group;
		}

		/*
		 * Unable to allocate a cache group; destruct the object
		 * and free it back to the pool.
		 */
		pool_cache_destruct_object(pc, object);
		return;
	}

 have_group:
	pc->pc_nitems++;
	pcg_put(pcg, object);

	if (pcg->pcg_avail == PCG_NOBJECTS)
		pc->pc_freeto = NULL;

	simple_unlock(&pc->pc_slock);
}

/*
 * pool_cache_destruct_object:
 *
 *	Force destruction of an object and its release back into
 *	the pool.
 */
void
pool_cache_destruct_object(struct pool_cache *pc, void *object)
{

	if (pc->pc_dtor != NULL)
		(*pc->pc_dtor)(pc->pc_arg, object);
	pool_put(pc->pc_pool, object);
}

/*
 * pool_cache_do_invalidate:
 *
 *	This internal function implements pool_cache_invalidate() and
 *	pool_cache_reclaim().
 */
static void
pool_cache_do_invalidate(struct pool_cache *pc, int free_groups,
    void (*putit)(struct pool *, void *))
{
	struct pool_cache_group *pcg, *npcg;
	void *object;

	for (pcg = TAILQ_FIRST(&pc->pc_grouplist); pcg != NULL;
	     pcg = npcg) {
		npcg = TAILQ_NEXT(pcg, pcg_list);
		while (pcg->pcg_avail != 0) {
			pc->pc_nitems--;
			object = pcg_get(pcg);
			if (pcg->pcg_avail == 0 && pc->pc_allocfrom == pcg)
				pc->pc_allocfrom = NULL;
			if (pc->pc_dtor != NULL)
				(*pc->pc_dtor)(pc->pc_arg, object);
			(*putit)(pc->pc_pool, object);
		}
		if (free_groups) {
			pc->pc_ngroups--;
			TAILQ_REMOVE(&pc->pc_grouplist, pcg, pcg_list);
			if (pc->pc_freeto == pcg)
				pc->pc_freeto = NULL;
			pool_put(&pcgpool, pcg);
		}
	}
}

/*
 * pool_cache_invalidate:
 *
 *	Invalidate a pool cache (destruct and release all of the
 *	cached objects).
 */
void
pool_cache_invalidate(struct pool_cache *pc)
{

	simple_lock(&pc->pc_slock);
	pool_cache_do_invalidate(pc, 0, pool_put);
	simple_unlock(&pc->pc_slock);
}

/*
 * pool_cache_reclaim:
 *
 *	Reclaim a pool cache for pool_reclaim().
 */
static void
pool_cache_reclaim(struct pool_cache *pc)
{

	simple_lock(&pc->pc_slock);
	pool_cache_do_invalidate(pc, 1, pool_do_put);
	simple_unlock(&pc->pc_slock);
}

/*
 * We have three different sysctls.
 * kern.pool.npools - the number of pools.
 * kern.pool.pool.<pool#> - the pool struct for the pool#.
 * kern.pool.name.<pool#> - the name for pool#.[6~
 */
int
sysctl_dopool(int *name, u_int namelen, char *where, size_t *sizep)
{
	struct pool *pp, *foundpool = NULL;
	size_t buflen = where != NULL ? *sizep : 0;
	int npools = 0, s;
	unsigned int lookfor;
	size_t len;

	switch (*name) {
	case KERN_POOL_NPOOLS:
		if (namelen != 1 || buflen != sizeof(int))
			return (EINVAL);
		lookfor = 0;
		break;
	case KERN_POOL_NAME:
		if (namelen != 2 || buflen < 1)
			return (EINVAL);
		lookfor = name[1];
		break;
	case KERN_POOL_POOL:
		if (namelen != 2 || buflen != sizeof(struct pool))
			return (EINVAL);
		lookfor = name[1];
		break;
	default:
		return (EINVAL);
	}

	s = splvm();
	simple_lock(&pool_head_slock);

	TAILQ_FOREACH(pp, &pool_head, pr_poollist) {
		npools++;
		if (lookfor == pp->pr_serial) {
			foundpool = pp;
			break;
		}
	}

	simple_unlock(&pool_head_slock);
	splx(s);

	if (lookfor != 0 && foundpool == NULL)
		return (ENOENT);

	switch (*name) {
	case KERN_POOL_NPOOLS:
		return copyout(&npools, where, buflen);
	case KERN_POOL_NAME:
		len = strlen(foundpool->pr_wchan) + 1;
		if (*sizep < len)
			return (ENOMEM);
		*sizep = len;
		return copyout(foundpool->pr_wchan, where, len);
	case KERN_POOL_POOL:
		return copyout(foundpool, where, buflen);
	}
	/* NOTREACHED */
	return (0); /* XXX - Stupid gcc */
@


1.2.2.3
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.2.2.2 2001/07/04 10:48:31 niklas Exp $	*/
d53 2
d60 2
a64 1
#define simple_lock_only_held(lkp, str) do { /* nothing */ } while (0)
@


1.2.2.4
log
@merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d52 1
@


1.2.2.5
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$NetBSD: subr_pool.c,v 1.61 2001/09/26 07:14:56 chs Exp $	*/
d62 1
a100 1
TAILQ_HEAD(pool_pagelist,pool_item_header);
d111 1
d113 1
a113 1
	(((u_long)(addr) >> (pp)->pr_alloc->pa_pageshift) & (PR_HASHTABSIZE - 1))
d163 1
a163 9
void pool_cache_reclaim(struct pool_cache *);
void pool_cache_do_invalidate(struct pool_cache *, int,
    void (*)(struct pool *, void *));

int pool_catchup(struct pool *);
void pool_prime_page(struct pool *, caddr_t, struct pool_item_header *);
void pr_rmpage(struct pool *, struct pool_item_header *,
    struct pool_pagelist *);
void pool_do_put(struct pool *, void *);
d165 5
a169 2
void *pool_allocator_alloc(struct pool *, int);
void pool_allocator_free(struct pool *, void *);
d171 2
a172 1
void pool_print1(struct pool *, const char *, int (*)(const char *, ...));
d195 1
a195 1
pr_log(struct pool *pp, void *v, int action, const char *file, long line)
d276 1
a276 1
static __inline void
d315 2
a316 3
void
pr_rmpage(struct pool *pp, struct pool_item_header *ph,
     struct pool_pagelist *pq)
a317 1
	int s;
d335 1
a335 1
	 * Unlink a page from the pool and release it (or queue it for release).
d338 1
a338 11
	if (pq) {
		TAILQ_INSERT_HEAD(pq, ph, ph_pagelist);
	} else {
		pool_allocator_free(pp, ph->ph_page);
		if ((pp->pr_roflags & PR_PHINPAGE) == 0) {
			LIST_REMOVE(ph, ph_hashlist);
			s = splhigh();
			pool_put(&phpool, ph);
			splx(s);
		}
	}
d342 8
d356 2
a357 1
		TAILQ_FOREACH(ph, &pp->pr_pagelist, ph_pagelist)
d373 4
a376 1
    const char *wchan, struct pool_allocator *palloc)
d391 14
a404 13
	if (palloc == NULL)
		palloc = &pool_allocator_kmem;
	if ((palloc->pa_flags & PA_INITIALIZED) == 0) {
		if (palloc->pa_pagesz == 0)
			palloc->pa_pagesz = PAGE_SIZE;

		TAILQ_INIT(&palloc->pa_list);
		
		simple_lock_init(&palloc->pa_slock);
		palloc->pa_pagemask = ~(palloc->pa_pagesz - 1);
		palloc->pa_pageshift = ffs(palloc->pa_pagesz) - 1;
		palloc->pa_flags |= PA_INITIALIZED;
	}
d413 1
a413 2
#ifdef DIAGNOSTIC
	if (size > palloc->pa_pagesz)
a415 1
#endif
d432 6
a437 1
	pp->pr_alloc = palloc;
a445 2
	pp->pr_drain_hook = NULL;
	pp->pr_drain_hook_arg = NULL;
d457 1
a457 1
	if (pp->pr_size < palloc->pa_pagesz/16) {
d461 1
a461 1
			palloc->pa_pagesz - ALIGN(sizeof(struct pool_item_header));
d465 1
a465 1
		off = palloc->pa_pagesz;
d521 1
a521 1
		    0, "phpool", NULL);
d523 1
a523 1
		    0, "pcgpool", NULL);
a529 5

	/* Insert into the list of pools using this allocator. */
	simple_lock(&palloc->pa_slock);
	TAILQ_INSERT_TAIL(&palloc->pa_list, pp, pr_alloc_list);
	simple_unlock(&palloc->pa_slock);
a540 7
	/*
	 * Locking order: pool_allocator -> pool
	 */
	simple_lock(&pp->pr_alloc->pa_slock);
	TAILQ_REMOVE(&pp->pr_alloc->pa_list, pp, pr_alloc_list);
	simple_unlock(&pp->pr_alloc->pa_slock);

d553 4
a556 3
	/* Remove all pages. */
	while ((ph = TAILQ_FIRST(&pp->pr_pagelist)) != NULL)
		pr_rmpage(pp, ph, NULL);
d561 2
a562 3
	if (drainpp == pp) {
		drainpp = NULL;
	}
a568 1
}
d570 2
a571 12
void
pool_set_drain_hook(struct pool *pp, void (*fn)(void *, int), void *fnarg)
{
	/*
	 * XXX - no locking, must be called just after pool_init.
	 */
#ifdef DIAGNOSTIC
	if (pp->pr_drain_hook != NULL)
		panic("pool_set_drain_hook(%s): already set", pp->pr_wchan);
#endif
	pp->pr_drain_hook = fn;
	pp->pr_drain_hook_arg = fnarg;
d608 6
a617 3
#ifdef LOCKDEBUG
	if (flags & PR_WAITOK)
		simple_lock_only_held(NULL, "pool_get(PR_WAITOK)");
a618 2
#endif /* DIAGNOSTIC */

a635 13
		if (pp->pr_drain_hook != NULL) {
			/*
			 * Since the drain hook is likely to free memory
			 * to this pool unlock, call hook, relock and check
			 * hardlimit condition again.
			 */
			simple_unlock(&pp->pr_slock);
			(*pp->pr_drain_hook)(pp->pr_drain_hook_arg, flags);
			simple_lock(&pp->pr_slock);
			if (pp->pr_nout < pp->pr_hardlimit)
				goto startover;
		}

d643 3
a645 1
			ltsleep(pp, PSWP, pp->pr_wchan, 0, &pp->pr_slock);
d658 3
d691 1
a691 1
		v = pool_allocator_alloc(pp, flags);
d699 1
a699 1
				pool_allocator_free(pp, v);
d710 3
d723 5
a731 1
			/* PA_WANTED is already set on the allocator */
d733 3
a735 1
			ltsleep(pp, PSWP, pp->pr_wchan, 0, &pp->pr_slock);
d741 1
a742 1
		pp->pr_npagealloc++;
a760 1
#endif
a761 1
#ifdef POOL_DIAGNOSTIC
a762 1
#endif
a763 1
#ifdef DIAGNOSTIC
d808 2
a809 1
		TAILQ_FOREACH(ph, &pp->pr_pagelist, ph_pagelist)
d838 1
a838 1
void
d846 1
a846 3
	LOCK_ASSERT(simple_lock_held(&pp->pr_slock));

	page = (caddr_t)((vaddr_t)v & pp->pr_alloc->pa_pagemask);
d918 2
a919 3
		if (pp->pr_npages > pp->pr_maxpages ||
		    (pp->pr_alloc->pa_flags & PA_WANT)) {
			pr_rmpage(pp, ph, NULL);
d942 2
a943 1
			TAILQ_FOREACH(ph, &pp->pr_pagelist, ph_pagelist)
d1015 1
a1015 1
		cp = pool_allocator_alloc(pp, PR_NOWAIT);
d1023 1
a1023 1
				pool_allocator_free(pp, cp);
d1044 1
a1044 1
void
d1053 1
a1053 2
#ifdef DIAGNOSTIC
	if (((u_long)cp & (pp->pr_alloc->pa_pagesz - 1)) != 0)
a1054 1
#endif
d1122 1
a1122 1
int
d1129 12
d1149 1
a1149 1
		cp = pool_allocator_alloc(pp, PR_NOWAIT);
d1155 1
a1155 1
				pool_allocator_free(pp, cp);
d1203 2
a1204 2
int
pool_sethardlimit(struct pool *pp, unsigned n, const char *warnmess, int ratecap)
a1205 1
	int error = 0;
a1208 5
	if (n < pp->pr_nout) {
		error = EINVAL;
		goto done;
	}

d1219 2
a1220 2
	pp->pr_maxpages = (n == 0 || n == UINT_MAX)
		? n
a1222 1
 done:
d1224 9
d1234 27
a1260 1
	return (error);
d1263 1
a1265 2
 *
 * Returns non-zero if any pages have been reclaimed.
d1267 1
a1267 1
int
a1276 1
	struct pool_pagelist pq;
d1279 2
a1280 6
	if (pp->pr_drain_hook != NULL) {
		/*
		 * The drain hook must be called with the pool unlocked.
		 */
		(*pp->pr_drain_hook)(pp->pr_drain_hook_arg, PR_NOWAIT);
	}
d1283 1
a1283 1
		return 0;
a1285 2
	TAILQ_INIT(&pq);

d1289 2
a1290 1
	TAILQ_FOREACH(pc, &pp->pr_cachelist, pc_poollist)
d1318 1
a1318 1
			pr_rmpage(pp, ph, &pq);
a1323 16
	if (TAILQ_EMPTY(&pq)) {
		return 0;
	}
	while ((ph = TAILQ_FIRST(&pq)) != NULL) {
		TAILQ_REMOVE(&pq, ph, ph_pagelist);
		pool_allocator_free(pp, ph->ph_page);
		if (pp->pr_roflags & PR_PHINPAGE) {
			continue;
		}
		LIST_REMOVE(ph, ph_hashlist);
		s = splhigh();
		pool_put(&phpool, ph);
		splx(s);
	}

	return 1;
a1337 1
	pp = NULL;
d1340 10
a1349 7
	if (drainpp == NULL) {
		drainpp = TAILQ_FIRST(&pool_head);
	}
	if (drainpp) {
		pp = drainpp;
		drainpp = TAILQ_NEXT(pp, pr_poollist);
	}
a1350 1
	pool_reclaim(pp);
d1354 1
d1365 1
a1365 1
		pr("pool %s is locked; try again later\n",
d1370 1
a1370 1
	pool_print1(pp, modif, pr);
d1375 1
a1375 1
void
d1400 2
a1401 1
	(*pr)("\talloc %p\n", pp->pr_alloc);
d1423 2
a1424 1
		TAILQ_FOREACH(pi, &ph->ph_itemlist, pi_list) {
d1453 2
a1454 1
	TAILQ_FOREACH(pc, &pp->pr_cachelist, pc_poollist) {
d1459 2
a1460 1
		TAILQ_FOREACH(pcg, &pc->pc_grouplist, pcg_list) {
d1480 3
a1482 1
	TAILQ_FOREACH(ph, &pp->pr_pagelist, ph_pagelist) {
d1487 1
a1487 1
		page = (caddr_t)((vaddr_t)ph & pp->pr_alloc->pa_pagemask);
d1516 1
a1516 1
			page = (caddr_t)((vaddr_t)pi & pp->pr_alloc->pa_pagemask);
d1640 2
a1641 1
		TAILQ_FOREACH(pcg, &pc->pc_grouplist, pcg_list) {
a1687 1
	int s;
d1692 2
a1693 1
		TAILQ_FOREACH(pcg, &pc->pc_grouplist, pcg_list) {
a1704 1
		s = splvm();
a1705 1
		splx(s);
d1755 1
a1755 1
void
a1760 1
	int s;
a1778 1
			s = splvm();
a1779 1
			splx(s);
d1804 1
a1804 1
void
a1878 172
}

/*
 * Pool backend allocators.
 *
 * Each pool has a backend allocator that handles allocation, deallocation
 * and any additional draining that might be needed.
 *
 * We provide two standard allocators.
 *  pool_alloc_kmem - the default used when no allocator is specified.
 *  pool_alloc_nointr - used for pools that will not be accessed in
 *   interrupt context.
 */
void	*pool_page_alloc(struct pool *, int);
void	pool_page_free(struct pool *, void *);
void	*pool_page_alloc_nointr(struct pool *, int);
void	pool_page_free_nointr(struct pool *, void *);

struct pool_allocator pool_allocator_kmem = {
	pool_page_alloc, pool_page_free, 0,
};
struct pool_allocator pool_allocator_nointr = {
	pool_page_alloc_nointr, pool_page_free_nointr, 0,
};

/*
 * XXX - we have at least three different resources for the same allocation
 *  and each resource can be depleted. First we have the ready elements in
 *  the pool. Then we have the resource (typically a vm_map) for this
 *  allocator, then we have physical memory. Waiting for any of these can
 *  be unnecessary when any other is freed, but the kernel doesn't support
 *  sleeping on multiple addresses, so we have to fake. The caller sleeps on
 *  the pool (so that we can be awakened when an item is returned to the pool),
 *  but we set PA_WANT on the allocator. When a page is returned to
 *  the allocator and PA_WANT is set pool_allocator_free will wakeup all
 *  sleeping pools belonging to this allocator. (XXX - thundering herd).
 *  We also wake up the allocator in case someone without a pool (malloc)
 *  is sleeping waiting for this allocator.
 */

void *
pool_allocator_alloc(struct pool *org, int flags)
{
	struct pool_allocator *pa = org->pr_alloc;
	int freed;
	void *res;
	int s;

	do {
		if ((res = (*pa->pa_alloc)(org, flags)) != NULL)
			return (res);
		if ((flags & PR_WAITOK) == 0) {
			/*
			 * We only run the drain hook here if PR_NOWAIT.
			 * In other cases the hook will be run in
			 * pool_reclaim.
			 */
			if (org->pr_drain_hook == NULL)
				break;
			(*org->pr_drain_hook)(org->pr_drain_hook_arg, flags);
			if ((res = (*pa->pa_alloc)(org, flags)) != NULL)
				continue;
			break;
		}
		s = splvm();
		simple_lock(&pa->pa_slock);
		freed = pool_allocator_drain(pa, org, 1);
		simple_unlock(&pa->pa_slock);
		splx(s);
	} while (freed);
	return (NULL);
}

void
pool_allocator_free(struct pool *pp, void *v)
{
	struct pool_allocator *pa = pp->pr_alloc;

	(*pa->pa_free)(pp, v);

	simple_lock(&pa->pa_slock);
	if ((pa->pa_flags & PA_WANT) == 0) {
		simple_unlock(&pa->pa_slock);
		return;
	}

	TAILQ_FOREACH(pp, &pa->pa_list, pr_alloc_list) {
		simple_lock(&pp->pr_slock);
		if ((pp->pr_flags & PR_WANTED) != 0) {
			pp->pr_flags &= ~PR_WANTED;
			wakeup(pp);
		}
	}
	wakeup(pa);
	pa->pa_flags &= ~PA_WANT;
	simple_unlock(&pa->pa_slock);
}

/*
 * Drain all pools, except 'org', that use this allocator.
 *
 * Must be called at appropriate spl level and with the allocator locked.
 *
 * We do this to reclaim va space. pa_alloc is responsible
 * for waiting for physical memory.
 * XXX - we risk looping forever if start if someone calls
 *  pool_destroy on 'start'. But there is no other way to
 *  have potentially sleeping pool_reclaim, non-sleeping
 *  locks on pool_allocator and some stirring of drained
 *  pools in the allocator.
 * XXX - maybe we should use pool_head_slock for locking
 *  the allocators?
 */
int
pool_allocator_drain(struct pool_allocator *pa, struct pool *org, int need)
{
	struct pool *pp, *start;
	int freed;

	freed = 0;

	pp = start = TAILQ_FIRST(&pa->pa_list);
	do {
		TAILQ_REMOVE(&pa->pa_list, pp, pr_alloc_list);
		TAILQ_INSERT_TAIL(&pa->pa_list, pp, pr_alloc_list);
		if (pp == org)
			continue;
		simple_unlock(&pa->pa_list);
		freed = pool_reclaim(pp)
		simple_lock(&pa->pa_list);
	} while ((pp = TAILQ_FIRST(&pa->pa_list)) != start && (freed < need));

	if (!freed) {
		/*
		 * We set PA_WANT here, the caller will most likely
		 * sleep waiting for pages (if not, this won't hurt
		 * that much) and there is no way to set this in the
		 * caller without violating locking order.
		 */
		pa->pa_flags |= PA_WANT;
	}

	return (freed);
}

void *
pool_page_alloc(struct pool *pp, int flags)
{
	boolean_t waitok = (flags & PR_WAITOK) ? TRUE : FALSE;

	return ((void *)uvm_km_alloc_poolpage(waitok));
}

void
pool_page_free(struct pool *pp, void *v)
{
	uvm_km_free_poolpage((vaddr_t)v);
}

void *
pool_page_alloc_nointr(struct pool *pp, int flags)
{
	boolean_t waitok = (flags & PR_WAITOK) ? TRUE : FALSE;

	return ((void *)uvm_km_alloc_poolpage1(kernel_map, uvm.kernel_object,
	    waitok));
}

void
pool_page_free_nointr(struct pool *pp, void *v)
{
	uvm_km_free_poolpage1(kernel_map, (vaddr_t)v);
@


1.2.2.6
log
@Merge in -current from about a week ago
@
text
@a1978 1
		simple_unlock(&pp->pr_slock);
@


1.2.2.7
log
@Sync the SMP branch with 3.3
@
text
@d106 1
a106 1
#define	PI_MAGIC 0xdeafbeef
d162 2
a163 2
void	pool_cache_reclaim(struct pool_cache *);
void	pool_cache_do_invalidate(struct pool_cache *, int,
d166 3
a168 4
int	pool_catchup(struct pool *);
void	pool_prime_page(struct pool *, caddr_t, struct pool_item_header *);
void	pool_do_put(struct pool *, void *);
void	pr_rmpage(struct pool *, struct pool_item_header *,
d170 1
d172 2
a173 2
void	*pool_allocator_alloc(struct pool *, int);
void	pool_allocator_free(struct pool *, void *);
a390 4
#ifdef MALLOC_DEBUG
	if ((flags & PR_DEBUG) && (ioff != 0 || align != 0))
		flags &= ~PR_DEBUG;
#endif
d401 1
a401 1

d414 1
a414 1
	size = roundup(size, align);
d461 2
a462 2
		pp->pr_phoffset = off = palloc->pa_pagesz -
		    ALIGN(sizeof(struct pool_item_header));
d527 1
a527 1
	/* Insert this into the list of all pools. */
d547 3
a549 1
	/* Locking order: pool_allocator -> pool */
d561 1
a561 1
		panic("pool_destroy: pool busy: still out: %u",
d566 1
a566 1
	/* Remove all pages */
d585 1
a585 1
pool_set_drain_hook(struct pool *pp, void (*fn)(void *, int), void *arg)
d587 3
a589 1
	/* XXX no locking -- must be used just after pool_init() */
d595 1
a595 1
	pp->pr_drain_hook_arg = arg;
a631 2
	if ((flags & PR_WAITOK) != 0)
		splassert(IPL_NONE);
d634 1
a634 1
		panic("pool_get: %s:must have NOWAIT", pp->pr_wchan);
a641 11
#ifdef MALLOC_DEBUG
	if (pp->pr_roflags & PR_DEBUG) {
		void *addr;

		addr = NULL;
		debug_malloc(pp->pr_size, M_DEBUG,
		    (flags & PR_WAITOK) ? M_WAITOK : M_NOWAIT, &addr);
		return (addr);
	}
#endif

d661 3
a663 3
			 * Since the drain hook is going to free things
			 * back to the pool, unlock, call hook, re-lock
			 * and check hardlimit condition again.
a664 1
			pr_leave(pp);
a667 1
			pr_enter(pp, file, line);
d711 1
a711 1
			panic("pool_get: nitems inconsistent");
d755 1
a755 1
			/* PA_WANTED is already set on the allocator. */
d781 1
a781 1
		panic("pool_get: nitems inconsistent");
d793 1
a793 1
		       " item addr %p",
a870 7
#ifdef MALLOC_DEBUG
	if (pp->pr_roflags & PR_DEBUG) {
		debug_free(v, M_DEBUG);
		return;
	}
#endif

d946 1
a946 1
		    (pp->pr_alloc->pa_flags & PA_WANT) != 0) {
d1034 1
a1034 1
	int newpages;
d1048 1
a1121 2
		KASSERT(((((vaddr_t)pi) + ioff) & (align - 1)) == 0);

d1146 3
a1148 1
 * Note 2, we must be called with the pool already locked, and we return
d1186 1
d1196 1
a1196 1
	if (POOL_NEEDS_CATCHUP(pp) && pool_catchup(pp) != 0) {
d1278 1
a1278 1
		return (0);
d1320 3
a1322 2
	if (TAILQ_EMPTY(&pq))
		return (0);
d1335 1
a1335 1
	return (1);
d1944 5
a1948 6
			if (org->pr_drain_hook != NULL) {
				(*org->pr_drain_hook)(org->pr_drain_hook_arg,
				    flags);
				if ((res = (*pa->pa_alloc)(org, flags)) != NULL)
					return (res);
			}
a1963 1
	int s;
a1966 1
	s = splvm();
a1969 1
		splx(s);
d1981 1
a1983 1
	splx(s);
d2038 1
a2038 2
	return ((void *)uvm_km_alloc_poolpage1(kmem_map, uvmexp.kmem_object,
	    waitok));
d2044 1
a2044 2

	uvm_km_free_poolpage1(kmem_map, (vaddr_t)v);
a2051 2
	splassert(IPL_NONE);

a2058 2
	splassert(IPL_NONE);

@


1.2.2.8
log
@Biglock!  Most of the logic
comes from NetBSD.
Also a lot of fixes, enough to get a dual cpu machine actually run MP for a
very short while (we are just talking about seconds) before starving out one
of the cpus.  More coming very soon.
@
text
@d1 1
a1 1
/*	$OpenBSD: subr_pool.c,v 1.2.2.7 2003/03/28 00:41:27 niklas Exp $	*/
d57 1
d87 1
a87 1
struct simplelock pool_head_slock;
a530 2
	simple_lock_init(&pool_head_slock);

d2038 3
a2040 3
		simple_unlock(&pa->pa_slock);
		freed = pool_reclaim(pp);
		simple_lock(&pa->pa_slock);
@


1.2.2.9
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d65 6
a70 8
 * Memory is allocated in pages which are split into pieces according to
 * the pool item size. Each page is kept on one of three lists in the
 * pool structure: `pr_emptypages', `pr_fullpages' and `pr_partpages',
 * for empty, full and partially-full pages respectively. The individual
 * pool items are on a linked list headed by `ph_itemlist' in each page
 * header. The memory for building the page list is either taken from
 * the allocated pages themselves (for small pool items) or taken from
 * an internal pool of page headers (`phpool').
d90 1
a90 1
	LIST_ENTRY(pool_item_header)
d93 2
a94 2
	SPLAY_ENTRY(pool_item_header)
				ph_node;	/* Off-page page headers */
d99 1
d110 3
a166 1
void	pool_update_curpage(struct pool *);
a169 1
int	pool_chk_page(struct pool *, const char *, struct pool_item_header *);
a173 1
void pool_print_pagelist(struct pool_pagelist *, int (*)(const char *, ...));
a175 1

a293 14
static __inline int
phtree_compare(struct pool_item_header *a, struct pool_item_header *b)
{
	if (a->ph_page < b->ph_page)
		return (-1);
	else if (a->ph_page > b->ph_page)
		return (1);
	else
		return (0);
}

SPLAY_PROTOTYPE(phtree, pool_item_header, ph_node, phtree_compare);
SPLAY_GENERATE(phtree, pool_item_header, ph_node, phtree_compare);

d300 1
a300 1
	struct pool_item_header *ph, tmp;
d305 7
a311 3
	tmp.ph_page = page;
	ph = SPLAY_FIND(phtree, &pp->pr_phtree, &tmp);
	return ph;
d341 1
a341 1
	LIST_REMOVE(ph, ph_pagelist);
d343 1
a343 1
		LIST_INSERT_HEAD(pq, ph, ph_pagelist);
d347 1
a347 1
			SPLAY_REMOVE(phtree, &pp->pr_phtree, ph);
d356 12
a367 1
	pool_update_curpage(pp);
d380 1
a380 1
	int off, slack;
d427 1
a427 3
	LIST_INIT(&pp->pr_emptypages);
	LIST_INIT(&pp->pr_fullpages);
	LIST_INIT(&pp->pr_partpages);
d470 3
a472 1
		SPLAY_INIT(&pp->pr_phtree);
d570 1
a570 1
	while ((ph = LIST_FIRST(&pp->pr_emptypages)) != NULL)
a571 2
	KASSERT(LIST_EMPTY(&pp->pr_fullpages));
	KASSERT(LIST_EMPTY(&pp->pr_partpages));
d599 1
a599 1
static struct pool_item_header *
d785 1
a825 7

		/*
		 * This page was previously empty.  Move it to the list of
		 * partially-full pages.  This page is already curpage.
		 */
		LIST_REMOVE(ph, ph_pagelist);
		LIST_INSERT_HEAD(&pp->pr_partpages, ph, ph_pagelist);
d828 1
a828 1
	if (TAILQ_EMPTY(&ph->ph_itemlist)) {
d838 9
a846 2
		 * This page is now full.  Move it to the full list
		 * and select a new current page.
d848 7
a854 3
		LIST_REMOVE(ph, ph_pagelist);
		LIST_INSERT_HEAD(&pp->pr_fullpages, ph, ph_pagelist);
		pool_update_curpage(pp);
d953 1
a953 1
	 * If this page is now empty, do one of two things:
d955 2
a956 2
	 *	(1) If we have more pages than the page high water mark,
	 *	    free the page back to the system.
d958 7
a964 4
	 *	(2) Otherwise, move the page to the empty page list.
	 *
	 * Either way, select a new current page (so we use a partially-full
	 * page if one is available).
d972 2
a973 2
			LIST_REMOVE(ph, ph_pagelist);
			LIST_INSERT_HEAD(&pp->pr_emptypages, ph, ph_pagelist);
d984 14
a998 1
		pool_update_curpage(pp);
a999 1

d1001 4
a1004 4
	 * If the page was previously completely full, move it to the
	 * partially-full list and make it the current page.  The next
	 * allocation will get the item from this page, instead of
	 * further fragmenting the pool.
d1007 2
a1008 2
		LIST_REMOVE(ph, ph_pagelist);
		LIST_INSERT_HEAD(&pp->pr_partpages, ph, ph_pagelist);
d1107 4
d1114 1
a1114 1
	LIST_INSERT_HEAD(&pp->pr_emptypages, ph, ph_pagelist);
a1118 2
	if ((pp->pr_roflags & PR_PHINPAGE) == 0)
		SPLAY_INSERT(phtree, &pp->pr_phtree, ph);
d1166 1
a1166 1
 * is used to catch up pr_nitems with the low water mark.
a1205 10
pool_update_curpage(struct pool *pp)
{

	pp->pr_curpage = LIST_FIRST(&pp->pr_partpages);
	if (pp->pr_curpage == NULL) {
		pp->pr_curpage = LIST_FIRST(&pp->pr_emptypages);
	}
}

void
a1288 1
	struct timeval diff;
d1302 1
a1302 1
	LIST_INIT(&pq);
d1314 2
a1315 2
	for (ph = LIST_FIRST(&pp->pr_emptypages); ph != NULL; ph = phnext) {
		phnext = LIST_NEXT(ph, ph_pagelist);
d1321 5
a1325 4
		KASSERT(ph->ph_nmissing == 0);
		timersub(&curtime, &ph->ph_time, &diff);
		if (diff.tv_sec < pool_inactive_time)
			continue;
d1327 7
a1333 7
		/*
		 * If freeing this page would put us below
		 * the low water mark, stop now.
		 */
		if ((pp->pr_nitems - pp->pr_itemsperpage) <
		    pp->pr_minitems)
			break;
d1335 2
a1336 1
		pr_rmpage(pp, ph, &pq);
d1341 1
a1341 1
	if (LIST_EMPTY(&pq))
d1343 2
a1344 2
	while ((ph = LIST_FIRST(&pq)) != NULL) {
		LIST_REMOVE(ph, ph_pagelist);
d1349 1
a1349 1
		SPLAY_REMOVE(phtree, &pp->pr_phtree, ph);
d1406 1
a1406 1
pool_print_pagelist(struct pool_pagelist *pl, int (*pr)(const char *, ...))
d1409 2
a1413 23

	LIST_FOREACH(ph, pl, ph_pagelist) {
		(*pr)("\t\tpage %p, nmissing %d, time %lu,%lu\n",
		    ph->ph_page, ph->ph_nmissing,
		    (u_long)ph->ph_time.tv_sec,
		    (u_long)ph->ph_time.tv_usec);
#ifdef DIAGNOSTIC
		TAILQ_FOREACH(pi, &ph->ph_itemlist, pi_list) {
			if (pi->pi_magic != PI_MAGIC) {
				(*pr)("\t\t\titem %p, magic 0x%x\n",
				    pi, pi->pi_magic);
			}
		}
#endif
	}
}

void
pool_print1(struct pool *pp, const char *modif, int (*pr)(const char *, ...))
{
	struct pool_item_header *ph;
	struct pool_cache *pc;
	struct pool_cache_group *pcg;
d1444 16
a1459 10
	if ((ph = LIST_FIRST(&pp->pr_emptypages)) != NULL)
		(*pr)("\n\tempty page list:\n");
	pool_print_pagelist(&pp->pr_emptypages, pr);
	if ((ph = LIST_FIRST(&pp->pr_fullpages)) != NULL)
		(*pr)("\n\tfull page list:\n");
	pool_print_pagelist(&pp->pr_fullpages, pr);
	if ((ph = LIST_FIRST(&pp->pr_partpages)) != NULL)
		(*pr)("\n\tpartial-page list:\n");
	pool_print_pagelist(&pp->pr_partpages, pr);

d1465 2
a1466 1
skip_pagelist:
d1476 2
a1477 1
skip_log:
d1493 2
a1494 1
skip_cache:
d1499 1
a1499 1
pool_chk_page(struct pool *pp, const char *label, struct pool_item_header *ph)
d1501 2
a1502 3
	struct pool_item *pi;
	caddr_t page;
	int n;
d1504 1
a1504 11
	page = (caddr_t)((u_long)ph & pp->pr_alloc->pa_pagemask);
	if (page != ph->ph_page &&
	    (pp->pr_roflags & PR_PHINPAGE) != 0) {
		if (label != NULL)
			printf("%s: ", label);
		printf("pool(%p:%s): page inconsistency: page %p;"
		       " at page head addr %p (p %p)\n", pp,
			pp->pr_wchan, ph->ph_page,
			ph, page);
		return 1;
	}
d1506 8
a1513 6
	for (pi = TAILQ_FIRST(&ph->ph_itemlist), n = 0;
	     pi != NULL;
	     pi = TAILQ_NEXT(pi,pi_list), n++) {

#ifdef DIAGNOSTIC
		if (pi->pi_magic != PI_MAGIC) {
d1516 6
a1521 6
			printf("pool(%s): free list modified: magic=%x;"
			       " page %p; item ordinal %d;"
			       " addr %p (p %p)\n",
				pp->pr_wchan, pi->pi_magic, ph->ph_page,
				n, pi, page);
			panic("pool");
a1522 5
#endif
		page =
		    (caddr_t)((u_long)pi & pp->pr_alloc->pa_pagemask);
		if (page == ph->ph_page)
			continue;
d1524 3
a1526 10
		if (label != NULL)
			printf("%s: ", label);
		printf("pool(%p:%s): page inconsistency: page %p;"
		       " item ordinal %d; addr %p (p %p)\n", pp,
			pp->pr_wchan, ph->ph_page,
			n, pi, page);
		return 1;
	}
	return 0;
}
d1528 15
a1542 5
int
pool_chk(struct pool *pp, const char *label)
{
	struct pool_item_header *ph;
	int r = 0;
d1544 7
a1550 10
	simple_lock(&pp->pr_slock);
	LIST_FOREACH(ph, &pp->pr_emptypages, ph_pagelist) {
		r = pool_chk_page(pp, label, ph);
		if (r) {
			goto out;
		}
	}
	LIST_FOREACH(ph, &pp->pr_fullpages, ph_pagelist) {
		r = pool_chk_page(pp, label, ph);
		if (r) {
a1553 7
	LIST_FOREACH(ph, &pp->pr_partpages, ph_pagelist) {
		r = pool_chk_page(pp, label, ph);
		if (r) {
			goto out;
		}
	}

@


1.2.2.10
log
@Merge with the trunk
@
text
@d399 1
a399 1
		palloc = &pool_allocator_nointr;
d436 1
a436 1
	pp->pr_maxpages = 8;
d1937 5
a1942 4
void	*pool_page_alloc_kmem(struct pool *, int);
void	pool_page_free_kmem(struct pool *, void *);
void	*pool_page_alloc_oldnointr(struct pool *, int);
void	pool_page_free_oldnointr(struct pool *, void *);
d1945 2
a1947 1
/* old default allocator, interrupt safe */
d1949 1
a1949 5
	pool_page_alloc_kmem, pool_page_free_kmem, 0,
};
/* previous nointr.  handles large allocations safely */
struct pool_allocator pool_allocator_oldnointr = {
	pool_page_alloc_oldnointr, pool_page_free_oldnointr, 0,
a1950 2
/* safe for interrupts, name preserved for compat
 * this is the default allocator */
d1952 1
a1952 1
	pool_page_alloc, pool_page_free, 0,
a2084 15
	return (uvm_km_getpage(waitok));
}

void
pool_page_free(struct pool *pp, void *v)
{

	uvm_km_putpage(v);
}

void *
pool_page_alloc_kmem(struct pool *pp, int flags)
{
	boolean_t waitok = (flags & PR_WAITOK) ? TRUE : FALSE;

d2090 1
a2090 1
pool_page_free_kmem(struct pool *pp, void *v)
d2097 1
a2097 1
pool_page_alloc_oldnointr(struct pool *pp, int flags)
d2108 1
a2108 1
pool_page_free_oldnointr(struct pool *pp, void *v)
@


1.1
log
@pool allocator from NetBSD (stays until uvm is ready)
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a51 1
#if defined(UVM)
a52 1
#endif  
a875 1
#if defined(UVM)
a876 3
#else
	return ((void *)kmem_alloc_poolpage(waitok));
#endif
a885 1
#if defined(UVM)
a886 3
#else
	kmem_free_poolpage((vaddr_t)v);
#endif  
a898 1
#if defined(UVM)
a905 6
#else
	/*
	 * Can't do anything so cool with Mach VM.
	 */
	return (pool_page_alloc(sz, flags, mtype));
#endif
a914 1
#if defined(UVM)
a915 3
#else
	pool_page_free(v, sz, mtype);
#endif  
@

