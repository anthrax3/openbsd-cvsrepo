head	1.141;
access;
symbols
	OPENBSD_6_2:1.141.0.4
	OPENBSD_6_2_BASE:1.141
	OPENBSD_6_1:1.138.0.4
	OPENBSD_6_1_BASE:1.138
	OPENBSD_6_0:1.133.0.2
	OPENBSD_6_0_BASE:1.133
	OPENBSD_5_9:1.128.0.2
	OPENBSD_5_9_BASE:1.128
	OPENBSD_5_8:1.121.0.4
	OPENBSD_5_8_BASE:1.121
	OPENBSD_5_7:1.118.0.2
	OPENBSD_5_7_BASE:1.118
	OPENBSD_5_6:1.116.0.4
	OPENBSD_5_6_BASE:1.116
	OPENBSD_5_5:1.114.0.4
	OPENBSD_5_5_BASE:1.114
	OPENBSD_5_4:1.106.0.2
	OPENBSD_5_4_BASE:1.106
	OPENBSD_5_3:1.104.0.2
	OPENBSD_5_3_BASE:1.104
	OPENBSD_5_2:1.103.0.2
	OPENBSD_5_2_BASE:1.103
	OPENBSD_5_1_BASE:1.99
	OPENBSD_5_1:1.99.0.2
	OPENBSD_5_0:1.97.0.2
	OPENBSD_5_0_BASE:1.97
	OPENBSD_4_9:1.96.0.2
	OPENBSD_4_9_BASE:1.96
	OPENBSD_4_8:1.95.0.2
	OPENBSD_4_8_BASE:1.95
	OPENBSD_4_7:1.93.0.2
	OPENBSD_4_7_BASE:1.93
	OPENBSD_4_6:1.91.0.4
	OPENBSD_4_6_BASE:1.91
	OPENBSD_4_5:1.87.0.2
	OPENBSD_4_5_BASE:1.87
	OPENBSD_4_4:1.83.0.4
	OPENBSD_4_4_BASE:1.83
	OPENBSD_4_3:1.83.0.2
	OPENBSD_4_3_BASE:1.83
	OPENBSD_4_2:1.80.0.2
	OPENBSD_4_2_BASE:1.80
	OPENBSD_4_1:1.75.0.2
	OPENBSD_4_1_BASE:1.75
	OPENBSD_4_0:1.73.0.4
	OPENBSD_4_0_BASE:1.73
	OPENBSD_3_9:1.73.0.2
	OPENBSD_3_9_BASE:1.73
	OPENBSD_3_8:1.64.0.2
	OPENBSD_3_8_BASE:1.64
	OPENBSD_3_7:1.61.0.4
	OPENBSD_3_7_BASE:1.61
	OPENBSD_3_6:1.61.0.2
	OPENBSD_3_6_BASE:1.61
	SMP_SYNC_A:1.55
	SMP_SYNC_B:1.55
	OPENBSD_3_5:1.54.0.2
	OPENBSD_3_5_BASE:1.54
	OPENBSD_3_4:1.48.0.2
	OPENBSD_3_4_BASE:1.48
	UBC_SYNC_A:1.47
	OPENBSD_3_3:1.47.0.2
	OPENBSD_3_3_BASE:1.47
	OPENBSD_3_2:1.45.0.2
	OPENBSD_3_2_BASE:1.45
	OPENBSD_3_1:1.42.0.2
	OPENBSD_3_1_BASE:1.42
	UBC_SYNC_B:1.46
	UBC:1.40.0.2
	UBC_BASE:1.40
	OPENBSD_3_0:1.38.0.2
	OPENBSD_3_0_BASE:1.38
	OPENBSD_2_9_BASE:1.33
	OPENBSD_2_9:1.33.0.2
	OPENBSD_2_8:1.28.0.2
	OPENBSD_2_8_BASE:1.28
	OPENBSD_2_7:1.24.0.2
	OPENBSD_2_7_BASE:1.24
	SMP:1.17.0.4
	SMP_BASE:1.17
	kame_19991208:1.17
	OPENBSD_2_6:1.17.0.2
	OPENBSD_2_6_BASE:1.17
	OPENBSD_2_5:1.14.0.2
	OPENBSD_2_5_BASE:1.14
	OPENBSD_2_4:1.12.0.4
	OPENBSD_2_4_BASE:1.12
	OPENBSD_2_3:1.12.0.2
	OPENBSD_2_3_BASE:1.12
	OPENBSD_2_2:1.9.0.2
	OPENBSD_2_2_BASE:1.9
	OPENBSD_2_1:1.6.0.2
	OPENBSD_2_1_BASE:1.6
	OPENBSD_2_0:1.4.0.2
	OPENBSD_2_0_BASE:1.4
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.141
date	2017.05.18.07.08.45;	author mpi;	state Exp;
branches;
next	1.140;
commitid	vi7IEyuJht0YHSq1;

1.140
date	2017.04.20.13.57.30;	author visa;	state Exp;
branches;
next	1.139;
commitid	RHJVP52IiQkInZzu;

1.139
date	2017.04.20.13.33.00;	author visa;	state Exp;
branches;
next	1.138;
commitid	wskxDpGV1KoZOcPL;

1.138
date	2017.01.31.12.16.20;	author mpi;	state Exp;
branches;
next	1.137;
commitid	iS7YuY5nvwVZYxyO;

1.137
date	2017.01.25.21.59.41;	author mpi;	state Exp;
branches;
next	1.136;
commitid	ItvKRgru62mnNdnN;

1.136
date	2017.01.21.05.42.03;	author guenther;	state Exp;
branches;
next	1.135;
commitid	CHRb0fCqa8XxUAMH;

1.135
date	2016.09.13.08.32.44;	author mpi;	state Exp;
branches;
next	1.134;
commitid	JUinkWUa5er75UzF;

1.134
date	2016.09.03.15.06.06;	author akfaew;	state Exp;
branches;
next	1.133;
commitid	4ydDdCaAL6N9vw7r;

1.133
date	2016.07.06.15.53.01;	author tedu;	state Exp;
branches;
next	1.132;
commitid	dZWS1TCbA5Wj2uVG;

1.132
date	2016.07.04.16.12.52;	author tedu;	state Exp;
branches;
next	1.131;
commitid	wrYN6yb5c3wOztJi;

1.131
date	2016.03.29.02.43.47;	author jsg;	state Exp;
branches;
next	1.130;
commitid	uGJyX1SoZqd2sL7Z;

1.130
date	2016.03.28.20.49.58;	author kettenis;	state Exp;
branches;
next	1.129;
commitid	uhHVZ2hCVC24VwS6;

1.129
date	2016.03.09.13.38.50;	author mpi;	state Exp;
branches;
next	1.128;
commitid	THpTza9IRRtZUKne;

1.128
date	2016.02.01.23.34.31;	author dlg;	state Exp;
branches
	1.128.2.1;
next	1.127;
commitid	4bLpG4UOG46MeR8k;

1.127
date	2016.01.15.11.42.47;	author dlg;	state Exp;
branches;
next	1.126;
commitid	aeqCAVqgLW41uUjC;

1.126
date	2015.11.23.10.56.20;	author mpi;	state Exp;
branches;
next	1.125;
commitid	cBdJiiWQBFMz5dcw;

1.125
date	2015.09.28.21.02.12;	author deraadt;	state Exp;
branches;
next	1.124;
commitid	i99nzVPOp9LCHId9;

1.124
date	2015.09.28.18.36.36;	author deraadt;	state Exp;
branches;
next	1.123;
commitid	sjQx9dlBaND52EhB;

1.123
date	2015.09.11.19.13.22;	author dlg;	state Exp;
branches;
next	1.122;
commitid	G2Lp7EfhVooz9MiS;

1.122
date	2015.09.07.15.38.45;	author guenther;	state Exp;
branches;
next	1.121;
commitid	P4QdNgT9HBzXxQ5B;

1.121
date	2015.05.12.09.30.35;	author mikeb;	state Exp;
branches
	1.121.4.1;
next	1.120;
commitid	A2ab5qVexJuqZwB3;

1.120
date	2015.05.07.18.30.27;	author mikeb;	state Exp;
branches;
next	1.119;
commitid	7lHukxuSzK2Z8DTk;

1.119
date	2015.03.14.03.38.50;	author jsg;	state Exp;
branches;
next	1.118;
commitid	p4LJxGKbi0BU2cG6;

1.118
date	2015.02.10.03.40.18;	author blambert;	state Exp;
branches;
next	1.117;
commitid	sIvMhGOLKExPWjQT;

1.117
date	2015.02.09.03.15.41;	author dlg;	state Exp;
branches;
next	1.116;
commitid	jVd0KngVszV2FEfg;

1.116
date	2014.07.08.17.19.25;	author deraadt;	state Exp;
branches;
next	1.115;
commitid	EF98ch02VpFassUi;

1.115
date	2014.03.22.06.05.45;	author guenther;	state Exp;
branches;
next	1.114;

1.114
date	2014.01.23.01.48.44;	author guenther;	state Exp;
branches;
next	1.113;

1.113
date	2014.01.23.00.33.36;	author guenther;	state Exp;
branches;
next	1.112;

1.112
date	2013.12.24.01.11.04;	author dlg;	state Exp;
branches;
next	1.111;

1.111
date	2013.11.25.15.24.18;	author tedu;	state Exp;
branches;
next	1.110;

1.110
date	2013.11.18.23.09.46;	author tedu;	state Exp;
branches;
next	1.109;

1.109
date	2013.11.09.06.52.15;	author guenther;	state Exp;
branches;
next	1.108;

1.108
date	2013.09.14.01.35.01;	author guenther;	state Exp;
branches;
next	1.107;

1.107
date	2013.08.13.05.52.23;	author guenther;	state Exp;
branches;
next	1.106;

1.106
date	2013.06.01.20.47.40;	author tedu;	state Exp;
branches;
next	1.105;

1.105
date	2013.04.06.03.44.34;	author tedu;	state Exp;
branches;
next	1.104;

1.104
date	2012.08.21.19.51.58;	author haesbaert;	state Exp;
branches;
next	1.103;

1.103
date	2012.07.10.16.56.28;	author haesbaert;	state Exp;
branches;
next	1.102;

1.102
date	2012.04.10.11.33.58;	author guenther;	state Exp;
branches;
next	1.101;

1.101
date	2012.03.23.15.51.26;	author guenther;	state Exp;
branches;
next	1.100;

1.100
date	2012.03.19.09.05.39;	author guenther;	state Exp;
branches;
next	1.99;

1.99
date	2012.01.17.02.34.18;	author guenther;	state Exp;
branches;
next	1.98;

1.98
date	2011.12.11.19.42.28;	author guenther;	state Exp;
branches;
next	1.97;

1.97
date	2011.07.07.18.00.33;	author guenther;	state Exp;
branches;
next	1.96;

1.96
date	2011.01.25.18.42.45;	author stsp;	state Exp;
branches;
next	1.95;

1.95
date	2010.06.29.00.28.14;	author tedu;	state Exp;
branches;
next	1.94;

1.94
date	2010.06.10.17.54.12;	author deraadt;	state Exp;
branches;
next	1.93;

1.93
date	2009.12.27.04.59.43;	author guenther;	state Exp;
branches;
next	1.92;

1.92
date	2009.11.27.19.45.53;	author guenther;	state Exp;
branches;
next	1.91;

1.91
date	2009.06.04.04.26.54;	author beck;	state Exp;
branches;
next	1.90;

1.90
date	2009.06.02.23.05.31;	author guenther;	state Exp;
branches;
next	1.89;

1.89
date	2009.04.14.09.13.25;	author art;	state Exp;
branches;
next	1.88;

1.88
date	2009.03.23.13.25.11;	author art;	state Exp;
branches;
next	1.87;

1.87
date	2008.09.10.12.30.40;	author blambert;	state Exp;
branches;
next	1.86;

1.86
date	2008.09.05.14.38.15;	author oga;	state Exp;
branches;
next	1.85;

1.85
date	2008.09.05.14.17.50;	author art;	state Exp;
branches;
next	1.84;

1.84
date	2008.09.05.14.11.57;	author oga;	state Exp;
branches;
next	1.83;

1.83
date	2007.11.30.16.44.44;	author oga;	state Exp;
branches;
next	1.82;

1.82
date	2007.11.28.20.07.36;	author oga;	state Exp;
branches;
next	1.81;

1.81
date	2007.10.10.15.53.53;	author art;	state Exp;
branches;
next	1.80;

1.80
date	2007.05.16.17.27.30;	author art;	state Exp;
branches;
next	1.79;

1.79
date	2007.04.03.08.05.43;	author art;	state Exp;
branches;
next	1.78;

1.78
date	2007.03.21.09.09.52;	author art;	state Exp;
branches;
next	1.77;

1.77
date	2007.03.18.10.46.51;	author art;	state Exp;
branches;
next	1.76;

1.76
date	2007.03.15.10.22.30;	author art;	state Exp;
branches;
next	1.75;

1.75
date	2006.11.29.12.24.17;	author miod;	state Exp;
branches;
next	1.74;

1.74
date	2006.10.21.02.18.00;	author tedu;	state Exp;
branches;
next	1.73;

1.73
date	2005.12.30.04.02.17;	author tedu;	state Exp;
branches;
next	1.72;

1.72
date	2005.12.22.06.55.03;	author tedu;	state Exp;
branches;
next	1.71;

1.71
date	2005.12.14.06.54.38;	author tedu;	state Exp;
branches;
next	1.70;

1.70
date	2005.12.14.04.03.51;	author tedu;	state Exp;
branches;
next	1.69;

1.69
date	2005.12.13.07.34.38;	author tedu;	state Exp;
branches;
next	1.68;

1.68
date	2005.12.13.06.03.54;	author tedu;	state Exp;
branches;
next	1.67;

1.67
date	2005.12.03.18.09.08;	author tedu;	state Exp;
branches;
next	1.66;

1.66
date	2005.11.28.00.14.29;	author jsg;	state Exp;
branches;
next	1.65;

1.65
date	2005.11.15.22.15.18;	author pedro;	state Exp;
branches;
next	1.64;

1.64
date	2005.06.17.22.33.34;	author niklas;	state Exp;
branches;
next	1.63;

1.63
date	2005.05.29.03.20.41;	author deraadt;	state Exp;
branches;
next	1.62;

1.62
date	2005.05.25.23.17.47;	author niklas;	state Exp;
branches;
next	1.61;

1.61
date	2004.07.29.06.25.45;	author tedu;	state Exp;
branches;
next	1.60;

1.60
date	2004.07.25.20.50.51;	author tedu;	state Exp;
branches;
next	1.59;

1.59
date	2004.06.24.19.35.24;	author tholo;	state Exp;
branches;
next	1.58;

1.58
date	2004.06.21.23.50.36;	author tholo;	state Exp;
branches;
next	1.57;

1.57
date	2004.06.20.03.00.16;	author art;	state Exp;
branches;
next	1.56;

1.56
date	2004.06.13.21.49.26;	author niklas;	state Exp;
branches;
next	1.55;

1.55
date	2004.06.09.20.18.28;	author art;	state Exp;
branches;
next	1.54;

1.54
date	2004.01.26.01.27.02;	author deraadt;	state Exp;
branches;
next	1.53;

1.53
date	2003.12.23.09.37.57;	author deraadt;	state Exp;
branches;
next	1.52;

1.52
date	2003.12.23.00.15.14;	author mickey;	state Exp;
branches;
next	1.51;

1.51
date	2003.12.19.04.51.44;	author millert;	state Exp;
branches;
next	1.50;

1.50
date	2003.12.15.22.03.41;	author millert;	state Exp;
branches;
next	1.49;

1.49
date	2003.12.15.09.00.55;	author deraadt;	state Exp;
branches;
next	1.48;

1.48
date	2003.06.02.23.28.06;	author millert;	state Exp;
branches;
next	1.47;

1.47
date	2003.03.15.00.08.41;	author deraadt;	state Exp;
branches;
next	1.46;

1.46
date	2002.10.15.20.17.22;	author art;	state Exp;
branches;
next	1.45;

1.45
date	2002.07.24.17.58.49;	author mickey;	state Exp;
branches;
next	1.44;

1.44
date	2002.07.03.21.19.08;	author miod;	state Exp;
branches;
next	1.43;

1.43
date	2002.06.11.05.04.34;	author art;	state Exp;
branches;
next	1.42;

1.42
date	2002.03.14.01.27.04;	author millert;	state Exp;
branches;
next	1.41;

1.41
date	2002.03.08.07.25.29;	author mickey;	state Exp;
branches;
next	1.40;

1.40
date	2001.11.11.22.30.56;	author art;	state Exp;
branches
	1.40.2.1;
next	1.39;

1.39
date	2001.11.06.19.53.20;	author miod;	state Exp;
branches;
next	1.38;

1.38
date	2001.09.13.14.41.50;	author art;	state Exp;
branches;
next	1.37;

1.37
date	2001.08.07.22.57.15;	author art;	state Exp;
branches;
next	1.36;

1.36
date	2001.06.27.04.49.45;	author art;	state Exp;
branches;
next	1.35;

1.35
date	2001.06.24.20.55.16;	author mickey;	state Exp;
branches;
next	1.34;

1.34
date	2001.05.26.04.08.57;	author art;	state Exp;
branches;
next	1.33;

1.33
date	2001.03.25.18.09.17;	author csapuntz;	state Exp;
branches;
next	1.32;

1.32
date	2001.03.15.21.18.30;	author art;	state Exp;
branches;
next	1.31;

1.31
date	2001.02.27.09.07.53;	author csapuntz;	state Exp;
branches;
next	1.30;

1.30
date	2001.02.19.16.33.20;	author art;	state Exp;
branches;
next	1.29;

1.29
date	2000.11.10.18.15.47;	author art;	state Exp;
branches;
next	1.28;

1.28
date	2000.08.03.14.36.36;	author mickey;	state Exp;
branches;
next	1.27;

1.27
date	2000.07.06.07.00.04;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2000.06.27.18.13.23;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2000.06.05.11.02.51;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2000.04.19.09.58.20;	author art;	state Exp;
branches;
next	1.23;

1.23
date	2000.03.23.16.54.43;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2000.03.23.14.44.37;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2000.03.23.10.13.58;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2000.03.03.16.49.25;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2000.03.03.11.46.09;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2000.03.03.11.31.43;	author art;	state Exp;
branches;
next	1.17;

1.17
date	99.09.05.22.20.45;	author tholo;	state Exp;
branches
	1.17.4.1;
next	1.16;

1.16
date	99.08.15.00.07.44;	author pjanzen;	state Exp;
branches;
next	1.15;

1.15
date	99.04.21.01.21.48;	author alex;	state Exp;
branches;
next	1.14;

1.14
date	99.02.26.05.10.40;	author art;	state Exp;
branches;
next	1.13;

1.13
date	98.11.15.16.45.16;	author art;	state Exp;
branches;
next	1.12;

1.12
date	98.02.03.19.06.25;	author deraadt;	state Exp;
branches;
next	1.11;

1.11
date	97.11.06.05.58.19;	author csapuntz;	state Exp;
branches;
next	1.10;

1.10
date	97.11.04.21.09.49;	author chuck;	state Exp;
branches;
next	1.9;

1.9
date	97.10.06.20.19.57;	author deraadt;	state Exp;
branches;
next	1.8;

1.8
date	97.10.06.15.12.23;	author csapuntz;	state Exp;
branches;
next	1.7;

1.7
date	97.07.28.09.13.17;	author deraadt;	state Exp;
branches;
next	1.6;

1.6
date	97.01.19.03.56.46;	author briggs;	state Exp;
branches;
next	1.5;

1.5
date	96.11.23.23.19.51;	author kstailey;	state Exp;
branches;
next	1.4;

1.4
date	96.05.02.13.12.15;	author deraadt;	state Exp;
branches;
next	1.3;

1.3
date	96.04.21.22.27.08;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	96.03.03.17.19.55;	author niklas;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.52.44;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.52.44;	author deraadt;	state Exp;
branches;
next	;

1.17.4.1
date	2000.03.24.09.09.24;	author niklas;	state Exp;
branches;
next	1.17.4.2;

1.17.4.2
date	2001.05.14.22.32.41;	author niklas;	state Exp;
branches;
next	1.17.4.3;

1.17.4.3
date	2001.07.04.10.48.27;	author niklas;	state Exp;
branches;
next	1.17.4.4;

1.17.4.4
date	2001.10.31.03.26.29;	author nate;	state Exp;
branches;
next	1.17.4.5;

1.17.4.5
date	2001.11.13.23.04.23;	author niklas;	state Exp;
branches;
next	1.17.4.6;

1.17.4.6
date	2002.03.28.11.43.04;	author niklas;	state Exp;
branches;
next	1.17.4.7;

1.17.4.7
date	2003.03.28.00.41.26;	author niklas;	state Exp;
branches;
next	1.17.4.8;

1.17.4.8
date	2003.05.15.04.08.02;	author niklas;	state Exp;
branches;
next	1.17.4.9;

1.17.4.9
date	2003.05.15.16.45.54;	author niklas;	state Exp;
branches;
next	1.17.4.10;

1.17.4.10
date	2003.05.18.18.16.45;	author niklas;	state Exp;
branches;
next	1.17.4.11;

1.17.4.11
date	2003.06.07.11.03.40;	author ho;	state Exp;
branches;
next	1.17.4.12;

1.17.4.12
date	2004.02.19.10.56.37;	author niklas;	state Exp;
branches;
next	1.17.4.13;

1.17.4.13
date	2004.04.21.09.36.12;	author niklas;	state Exp;
branches;
next	1.17.4.14;

1.17.4.14
date	2004.06.05.17.19.55;	author niklas;	state Exp;
branches;
next	1.17.4.15;

1.17.4.15
date	2004.06.06.21.46.12;	author tedu;	state Exp;
branches;
next	1.17.4.16;

1.17.4.16
date	2004.06.10.11.40.33;	author niklas;	state Exp;
branches;
next	1.17.4.17;

1.17.4.17
date	2004.06.10.18.21.59;	author art;	state Exp;
branches;
next	;

1.40.2.1
date	2002.06.11.03.29.40;	author art;	state Exp;
branches;
next	1.40.2.2;

1.40.2.2
date	2002.10.29.00.36.44;	author art;	state Exp;
branches;
next	1.40.2.3;

1.40.2.3
date	2003.05.19.22.31.10;	author tedu;	state Exp;
branches;
next	;

1.121.4.1
date	2016.07.14.02.46.11;	author tedu;	state Exp;
branches;
next	;
commitid	kzYtCgZfgZewqezu;

1.128.2.1
date	2016.07.14.02.45.23;	author tedu;	state Exp;
branches;
next	;
commitid	iy5C78vcnVfjlmst;


desc
@@


1.141
log
@Do not panic if we find ourself on the sleep queue while being SONPROC.

If the rwlock passed to rwsleep(9) is contented, the CPU will call wakeup()
between sleep_setup() and sleep_finish().  At this moment curproc is on the
sleep queue but marked as SONPROC.  Avoid panicing in this case.

Problem reported by sthen@@

ok kettenis@@, visa@@
@
text
@/*	$OpenBSD: kern_synch.c,v 1.140 2017/04/20 13:57:30 visa Exp $	*/
/*	$NetBSD: kern_synch.c,v 1.37 1996/04/22 01:38:37 christos Exp $	*/

/*
 * Copyright (c) 1982, 1986, 1990, 1991, 1993
 *	The Regents of the University of California.  All rights reserved.
 * (c) UNIX System Laboratories, Inc.
 * All or some portions of this file are derived from material licensed
 * to the University of California by American Telephone and Telegraph
 * Co. or Unix System Laboratories, Inc. and are reproduced herein with
 * the permission of UNIX System Laboratories, Inc.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)kern_synch.c	8.6 (Berkeley) 1/21/94
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/kernel.h>
#include <sys/signalvar.h>
#include <sys/resourcevar.h>
#include <sys/sched.h>
#include <sys/timeout.h>
#include <sys/mount.h>
#include <sys/syscallargs.h>
#include <sys/pool.h>
#include <sys/refcnt.h>
#include <sys/atomic.h>
#include <sys/witness.h>
#include <ddb/db_output.h>

#include <machine/spinlock.h>

#ifdef KTRACE
#include <sys/ktrace.h>
#endif

int	thrsleep(struct proc *, struct sys___thrsleep_args *);
int	thrsleep_unlock(void *);

/*
 * We're only looking at 7 bits of the address; everything is
 * aligned to 4, lots of things are aligned to greater powers
 * of 2.  Shift right by 8, i.e. drop the bottom 256 worth.
 */
#define TABLESIZE	128
#define LOOKUP(x)	(((long)(x) >> 8) & (TABLESIZE - 1))
TAILQ_HEAD(slpque,proc) slpque[TABLESIZE];

void
sleep_queue_init(void)
{
	int i;

	for (i = 0; i < TABLESIZE; i++)
		TAILQ_INIT(&slpque[i]);
}


/*
 * During autoconfiguration or after a panic, a sleep will simply
 * lower the priority briefly to allow interrupts, then return.
 * The priority to be used (safepri) is machine-dependent, thus this
 * value is initialized and maintained in the machine-dependent layers.
 * This priority will typically be 0, or the lowest priority
 * that is safe for use on the interrupt stack; it can be made
 * higher to block network software interrupts after panics.
 */
extern int safepri;

/*
 * General sleep call.  Suspends the current process until a wakeup is
 * performed on the specified identifier.  The process will then be made
 * runnable with the specified priority.  Sleeps at most timo/hz seconds
 * (0 means no timeout).  If pri includes PCATCH flag, signals are checked
 * before and after sleeping, else signals are not checked.  Returns 0 if
 * awakened, EWOULDBLOCK if the timeout expires.  If PCATCH is set and a
 * signal needs to be delivered, ERESTART is returned if the current system
 * call should be restarted if possible, and EINTR is returned if the system
 * call should be interrupted by the signal (return EINTR).
 */
int
tsleep(const volatile void *ident, int priority, const char *wmesg, int timo)
{
	struct sleep_state sls;
	int error, error1;
#ifdef MULTIPROCESSOR
	int hold_count;
#endif

	KASSERT((priority & ~(PRIMASK | PCATCH)) == 0);

#ifdef MULTIPROCESSOR
	KASSERT(timo || __mp_lock_held(&kernel_lock));
#endif

#ifdef DDB
	if (cold == 2)
		db_stack_dump();
#endif
	if (cold || panicstr) {
		int s;
		/*
		 * After a panic, or during autoconfiguration,
		 * just give interrupts a chance, then just return;
		 * don't run any other procs or panic below,
		 * in case this is the idle process and already asleep.
		 */
		s = splhigh();
		splx(safepri);
#ifdef MULTIPROCESSOR
		if (__mp_lock_held(&kernel_lock)) {
			hold_count = __mp_release_all(&kernel_lock);
			__mp_acquire_count(&kernel_lock, hold_count);
		}
#endif
		splx(s);
		return (0);
	}

	sleep_setup(&sls, ident, priority, wmesg);
	sleep_setup_timeout(&sls, timo);
	sleep_setup_signal(&sls, priority);

	sleep_finish(&sls, 1);
	error1 = sleep_finish_timeout(&sls);
	error = sleep_finish_signal(&sls);

	/* Signal errors are higher priority than timeouts. */
	if (error == 0 && error1 != 0)
		error = error1;

	return (error);
}

/*
 * Same as tsleep, but if we have a mutex provided, then once we've
 * entered the sleep queue we drop the mutex. After sleeping we re-lock.
 */
int
msleep(const volatile void *ident, struct mutex *mtx, int priority,
    const char *wmesg, int timo)
{
	struct sleep_state sls;
	int error, error1, spl;
#ifdef MULTIPROCESSOR
	int hold_count;
#endif
	WITNESS_SAVE_DECL(lock_fl);

	KASSERT((priority & ~(PRIMASK | PCATCH | PNORELOCK)) == 0);
	KASSERT(mtx != NULL);

	if (cold || panicstr) {
		/*
		 * After a panic, or during autoconfiguration,
		 * just give interrupts a chance, then just return;
		 * don't run any other procs or panic below,
		 * in case this is the idle process and already asleep.
		 */
		spl = MUTEX_OLDIPL(mtx);
		MUTEX_OLDIPL(mtx) = safepri;
		mtx_leave(mtx);
#ifdef MULTIPROCESSOR
		if (__mp_lock_held(&kernel_lock)) {
			hold_count = __mp_release_all(&kernel_lock);
			__mp_acquire_count(&kernel_lock, hold_count);
		}
#endif
		if ((priority & PNORELOCK) == 0) {
			mtx_enter(mtx);
			MUTEX_OLDIPL(mtx) = spl;
		} else
			splx(spl);
		return (0);
	}

	sleep_setup(&sls, ident, priority, wmesg);
	sleep_setup_timeout(&sls, timo);
	sleep_setup_signal(&sls, priority);

	WITNESS_SAVE(MUTEX_LOCK_OBJECT(mtx), lock_fl);

	/* XXX - We need to make sure that the mutex doesn't
	 * unblock splsched. This can be made a bit more
	 * correct when the sched_lock is a mutex.
	 */
	spl = MUTEX_OLDIPL(mtx);
	MUTEX_OLDIPL(mtx) = splsched();
	mtx_leave(mtx);

	sleep_finish(&sls, 1);
	error1 = sleep_finish_timeout(&sls);
	error = sleep_finish_signal(&sls);

	if ((priority & PNORELOCK) == 0) {
		mtx_enter(mtx);
		MUTEX_OLDIPL(mtx) = spl; /* put the ipl back */
		WITNESS_RESTORE(MUTEX_LOCK_OBJECT(mtx), lock_fl);
	} else
		splx(spl);

	/* Signal errors are higher priority than timeouts. */
	if (error == 0 && error1 != 0)
		error = error1;

	return (error);
}

/*
 * Same as tsleep, but if we have a rwlock provided, then once we've
 * entered the sleep queue we drop the it. After sleeping we re-lock.
 */
int
rwsleep(const volatile void *ident, struct rwlock *wl, int priority,
    const char *wmesg, int timo)
{
	struct sleep_state sls;
	int error, error1;
	WITNESS_SAVE_DECL(lock_fl);

	KASSERT((priority & ~(PRIMASK | PCATCH | PNORELOCK)) == 0);
	rw_assert_wrlock(wl);

	sleep_setup(&sls, ident, priority, wmesg);
	sleep_setup_timeout(&sls, timo);
	sleep_setup_signal(&sls, priority);

	WITNESS_SAVE(&wl->rwl_lock_obj, lock_fl);

	rw_exit_write(wl);

	sleep_finish(&sls, 1);
	error1 = sleep_finish_timeout(&sls);
	error = sleep_finish_signal(&sls);

	if ((priority & PNORELOCK) == 0) {
		rw_enter_write(wl);
		WITNESS_RESTORE(&wl->rwl_lock_obj, lock_fl);
	}

	/* Signal errors are higher priority than timeouts. */
	if (error == 0 && error1 != 0)
		error = error1;

	return (error);
}

void
sleep_setup(struct sleep_state *sls, const volatile void *ident, int prio,
    const char *wmesg)
{
	struct proc *p = curproc;

#ifdef DIAGNOSTIC
	if (p->p_flag & P_CANTSLEEP)
		panic("sleep: %s failed insomnia", p->p_p->ps_comm);
	if (ident == NULL)
		panic("tsleep: no ident");
	if (p->p_stat != SONPROC)
		panic("tsleep: not SONPROC");
#endif

	sls->sls_catch = 0;
	sls->sls_do_sleep = 1;
	sls->sls_sig = 1;

	SCHED_LOCK(sls->sls_s);

	p->p_wchan = ident;
	p->p_wmesg = wmesg;
	p->p_slptime = 0;
	p->p_priority = prio & PRIMASK;
	TAILQ_INSERT_TAIL(&slpque[LOOKUP(ident)], p, p_runq);
}

void
sleep_finish(struct sleep_state *sls, int do_sleep)
{
	struct proc *p = curproc;

	if (sls->sls_do_sleep && do_sleep) {
		p->p_stat = SSLEEP;
		p->p_ru.ru_nvcsw++;
		SCHED_ASSERT_LOCKED();
		mi_switch();
	} else if (!do_sleep) {
		unsleep(p);
	}

#ifdef DIAGNOSTIC
	if (p->p_stat != SONPROC)
		panic("sleep_finish !SONPROC");
#endif

	p->p_cpu->ci_schedstate.spc_curpriority = p->p_usrpri;
	SCHED_UNLOCK(sls->sls_s);

	/*
	 * Even though this belongs to the signal handling part of sleep,
	 * we need to clear it before the ktrace.
	 */
	atomic_clearbits_int(&p->p_flag, P_SINTR);
}

void
sleep_setup_timeout(struct sleep_state *sls, int timo)
{
	if (timo)
		timeout_add(&curproc->p_sleep_to, timo);
}

int
sleep_finish_timeout(struct sleep_state *sls)
{
	struct proc *p = curproc;

	if (p->p_flag & P_TIMEOUT) {
		atomic_clearbits_int(&p->p_flag, P_TIMEOUT);
		return (EWOULDBLOCK);
	} else
		timeout_del(&p->p_sleep_to);

	return (0);
}

void
sleep_setup_signal(struct sleep_state *sls, int prio)
{
	struct proc *p = curproc;

	if ((sls->sls_catch = (prio & PCATCH)) == 0)
		return;

	/*
	 * We put ourselves on the sleep queue and start our timeout
	 * before calling CURSIG, as we could stop there, and a wakeup
	 * or a SIGCONT (or both) could occur while we were stopped.
	 * A SIGCONT would cause us to be marked as SSLEEP
	 * without resuming us, thus we must be ready for sleep
	 * when CURSIG is called.  If the wakeup happens while we're
	 * stopped, p->p_wchan will be 0 upon return from CURSIG.
	 */
	atomic_setbits_int(&p->p_flag, P_SINTR);
	if (p->p_p->ps_single != NULL || (sls->sls_sig = CURSIG(p)) != 0) {
		if (p->p_wchan)
			unsleep(p);
		p->p_stat = SONPROC;
		sls->sls_do_sleep = 0;
	} else if (p->p_wchan == 0) {
		sls->sls_catch = 0;
		sls->sls_do_sleep = 0;
	}
}

int
sleep_finish_signal(struct sleep_state *sls)
{
	struct proc *p = curproc;
	int error;

	if (sls->sls_catch != 0) {
		if ((error = single_thread_check(p, 1)))
			return (error);
		if (sls->sls_sig != 0 || (sls->sls_sig = CURSIG(p)) != 0) {
			if (p->p_p->ps_sigacts->ps_sigintr &
			    sigmask(sls->sls_sig))
				return (EINTR);
			return (ERESTART);
		}
	}

	return (0);
}

/*
 * Implement timeout for tsleep.
 * If process hasn't been awakened (wchan non-zero),
 * set timeout flag and undo the sleep.  If proc
 * is stopped, just unsleep so it will remain stopped.
 */
void
endtsleep(void *arg)
{
	struct proc *p = arg;
	int s;

	SCHED_LOCK(s);
	if (p->p_wchan) {
		if (p->p_stat == SSLEEP)
			setrunnable(p);
		else
			unsleep(p);
		atomic_setbits_int(&p->p_flag, P_TIMEOUT);
	}
	SCHED_UNLOCK(s);
}

/*
 * Remove a process from its wait queue
 */
void
unsleep(struct proc *p)
{
	SCHED_ASSERT_LOCKED();

	if (p->p_wchan) {
		TAILQ_REMOVE(&slpque[LOOKUP(p->p_wchan)], p, p_runq);
		p->p_wchan = NULL;
	}
}

/*
 * Make a number of processes sleeping on the specified identifier runnable.
 */
void
wakeup_n(const volatile void *ident, int n)
{
	struct slpque *qp;
	struct proc *p;
	struct proc *pnext;
	int s;

	SCHED_LOCK(s);
	qp = &slpque[LOOKUP(ident)];
	for (p = TAILQ_FIRST(qp); p != NULL && n != 0; p = pnext) {
		pnext = TAILQ_NEXT(p, p_runq);
#ifdef DIAGNOSTIC
		/*
		 * If the rwlock passed to rwsleep() is contended, the
		 * CPU will end up calling wakeup() between sleep_setup()
		 * and sleep_finish().
		 */
		if (p == curproc) {
			KASSERT(p->p_stat == SONPROC);
			continue;
		}
		if (p->p_stat != SSLEEP && p->p_stat != SSTOP)
			panic("wakeup: p_stat is %d", (int)p->p_stat);
#endif
		if (p->p_wchan == ident) {
			--n;
			p->p_wchan = 0;
			TAILQ_REMOVE(qp, p, p_runq);
			if (p->p_stat == SSLEEP)
				setrunnable(p);
		}
	}
	SCHED_UNLOCK(s);
}

/*
 * Make all processes sleeping on the specified identifier runnable.
 */
void
wakeup(const volatile void *chan)
{
	wakeup_n(chan, -1);
}

int
sys_sched_yield(struct proc *p, void *v, register_t *retval)
{
	struct proc *q;
	int s;

	SCHED_LOCK(s);
	/*
	 * If one of the threads of a multi-threaded process called
	 * sched_yield(2), drop its priority to ensure its siblings
	 * can make some progress.
	 */
	p->p_priority = p->p_usrpri;
	TAILQ_FOREACH(q, &p->p_p->ps_threads, p_thr_link)
		p->p_priority = max(p->p_priority, q->p_priority);
	p->p_stat = SRUN;
	setrunqueue(p);
	p->p_ru.ru_nvcsw++;
	mi_switch();
	SCHED_UNLOCK(s);

	return (0);
}

int
thrsleep_unlock(void *lock)
{
	static _atomic_lock_t unlocked = _ATOMIC_LOCK_UNLOCKED;
	_atomic_lock_t *atomiclock = lock;

	if (!lock)
		return 0;

	return copyout(&unlocked, atomiclock, sizeof(unlocked));
}

static int globalsleepaddr;

int
thrsleep(struct proc *p, struct sys___thrsleep_args *v)
{
	struct sys___thrsleep_args /* {
		syscallarg(const volatile void *) ident;
		syscallarg(clockid_t) clock_id;
		syscallarg(const struct timespec *) tp;
		syscallarg(void *) lock;
		syscallarg(const int *) abort;
	} */ *uap = v;
	long ident = (long)SCARG(uap, ident);
	struct timespec *tsp = (struct timespec *)SCARG(uap, tp);
	void *lock = SCARG(uap, lock);
	uint64_t to_ticks = 0;
	int abort, error;
	clockid_t clock_id = SCARG(uap, clock_id);

	if (ident == 0)
		return (EINVAL);
	if (tsp != NULL) {
		struct timespec now;

		if ((error = clock_gettime(p, clock_id, &now)))
			return (error);
#ifdef KTRACE
		if (KTRPOINT(p, KTR_STRUCT))
			ktrabstimespec(p, tsp);
#endif

		if (timespeccmp(tsp, &now, <)) {
			/* already passed: still do the unlock */
			if ((error = thrsleep_unlock(lock)))
				return (error);
			return (EWOULDBLOCK);
		}

		timespecsub(tsp, &now, tsp);
		to_ticks = (uint64_t)hz * tsp->tv_sec +
		    (tsp->tv_nsec + tick * 1000 - 1) / (tick * 1000) + 1;
		if (to_ticks > INT_MAX)
			to_ticks = INT_MAX;
	}

	p->p_thrslpid = ident;

	if ((error = thrsleep_unlock(lock)))
		goto out;

	if (SCARG(uap, abort) != NULL) {
		if ((error = copyin(SCARG(uap, abort), &abort,
		    sizeof(abort))) != 0)
			goto out;
		if (abort) {
			error = EINTR;
			goto out;
		}
	}

	if (p->p_thrslpid == 0)
		error = 0;
	else {
		void *sleepaddr = &p->p_thrslpid;
		if (ident == -1)
			sleepaddr = &globalsleepaddr;
		error = tsleep(sleepaddr, PUSER | PCATCH, "thrsleep",
		    (int)to_ticks);
	}

out:
	p->p_thrslpid = 0;

	if (error == ERESTART)
		error = EINTR;

	return (error);

}

int
sys___thrsleep(struct proc *p, void *v, register_t *retval)
{
	struct sys___thrsleep_args /* {
		syscallarg(const volatile void *) ident;
		syscallarg(clockid_t) clock_id;
		syscallarg(struct timespec *) tp;
		syscallarg(void *) lock;
		syscallarg(const int *) abort;
	} */ *uap = v;
	struct timespec ts;
	int error;

	if (SCARG(uap, tp) != NULL) {
		if ((error = copyin(SCARG(uap, tp), &ts, sizeof(ts)))) {
			*retval = error;
			return (0);
		}
		SCARG(uap, tp) = &ts;
	}

	*retval = thrsleep(p, uap);
	return (0);
}

int
sys___thrwakeup(struct proc *p, void *v, register_t *retval)
{
	struct sys___thrwakeup_args /* {
		syscallarg(const volatile void *) ident;
		syscallarg(int) n;
	} */ *uap = v;
	long ident = (long)SCARG(uap, ident);
	int n = SCARG(uap, n);
	struct proc *q;
	int found = 0;

	if (ident == 0)
		*retval = EINVAL;
	else if (ident == -1)
		wakeup(&globalsleepaddr);
	else {
		TAILQ_FOREACH(q, &p->p_p->ps_threads, p_thr_link) {
			if (q->p_thrslpid == ident) {
				wakeup_one(&q->p_thrslpid);
				q->p_thrslpid = 0;
				if (++found == n)
					break;
			}
		}
		*retval = found ? 0 : ESRCH;
	}

	return (0);
}

void
refcnt_init(struct refcnt *r)
{
	r->refs = 1;
}

void
refcnt_take(struct refcnt *r)
{
#ifdef DIAGNOSTIC
	u_int refcnt;

	refcnt = atomic_inc_int_nv(&r->refs);
	KASSERT(refcnt != 0);
#else
	atomic_inc_int(&r->refs);
#endif
}

int
refcnt_rele(struct refcnt *r)
{
	u_int refcnt;

	refcnt = atomic_dec_int_nv(&r->refs);
	KASSERT(refcnt != ~0);

	return (refcnt == 0);
}

void
refcnt_rele_wake(struct refcnt *r)
{
	if (refcnt_rele(r))
		wakeup_one(r);
}

void
refcnt_finalize(struct refcnt *r, const char *wmesg)
{
	struct sleep_state sls;
	u_int refcnt;

	refcnt = atomic_dec_int_nv(&r->refs);
	while (refcnt) {
		sleep_setup(&sls, r, PWAIT, wmesg);
		refcnt = r->refs;
		sleep_finish(&sls, refcnt);
	}
}
@


1.140
log
@Hook up mutex(9) to witness(4).
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.139 2017/04/20 13:33:00 visa Exp $	*/
d453 9
@


1.139
log
@Hook up rwlock(9) to witness(4).

Loosely based on a diff from Christian Ludwig
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.138 2017/01/31 12:16:20 mpi Exp $	*/
d173 1
d206 2
d223 1
@


1.138
log
@Remove the inifioctl hack, checking for an unheld NET_LOCK() in
tsleep(9) & friends seem to only produce false positives and cannot
be easily disabled.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.136 2017/01/21 05:42:03 guenther Exp $	*/
d53 1
d240 1
d249 2
d257 1
a257 1
	if ((priority & PNORELOCK) == 0)
d259 2
@


1.137
log
@Introduce a hack to remove false-positives when looking for memory
allocation that can sleep while holding the NET_LOCK().

To be removed once we're confident the remaining code paths are safe.

Discussed with deraadt@@
@
text
@a113 5
#if 1
	extern int inifioctl;
	if (!inifioctl)
		NET_ASSERT_UNLOCKED();
#endif
a172 5
#if 1
	extern int inifioctl;
	if (!inifioctl)
		NET_ASSERT_UNLOCKED();
#endif
@


1.136
log
@p_comm is the process's command and isn't per thread, so move it from
struct proc to struct process.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.135 2016/09/13 08:32:44 mpi Exp $	*/
d114 5
d178 5
@


1.135
log
@Introduce rwsleep(9), an equivalent to msleep(9) but for code protected
by a write lock.

ok guenther@@, vgross@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.134 2016/09/03 15:06:06 akfaew Exp $	*/
d271 1
a271 1
		panic("sleep: %s failed insomnia", p->p_comm);
@


1.134
log
@Remove ticket lock support from thrsleep. It's unused.

OK guenther@@ mpi@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.133 2016/07/06 15:53:01 tedu Exp $	*/
d221 34
@


1.133
log
@fix several places where calculating ticks could overflow.
it's not enough to assign to an unsigned type because if the arithmetic
overflows the compiler may decide to do anything. so change all the
long long casts to uint64_t so that we start with the right type.
reported by Tim Newsham of NCC.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.132 2016/07/04 16:12:52 tedu Exp $	*/
d62 1
a62 1
int	thrsleep_unlock(void *, int);
d457 1
a457 1
thrsleep_unlock(void *lock, int lockflags)
a460 3
	uint32_t *ticket = lock;
	uint32_t ticketvalue;
	int error;
d463 1
a463 1
		return (0);
d465 1
a465 9
	if (lockflags) {
		if ((error = copyin(ticket, &ticketvalue, sizeof(ticketvalue))))
			return (error);
		ticketvalue++;
		error = copyout(&ticketvalue, ticket, sizeof(ticketvalue));
	} else {
		error = copyout(&unlocked, atomiclock, sizeof(unlocked));
	}
	return (error);
d485 1
a485 2
	clockid_t clock_id = SCARG(uap, clock_id) & 0x7;
	int lockflags = SCARG(uap, clock_id) & 0x8;
d501 1
a501 1
			if ((error = thrsleep_unlock(lock, lockflags)))
d515 1
a515 1
	if ((error = thrsleep_unlock(lock, lockflags))) {
a516 1
	}
@


1.132
log
@switch calculuated thrsleep timeout to unsigned to prevent overflow
into negative values, which later causes a panic.
reported by Tim Newsham at NCC.
ok guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.131 2016/03/29 02:43:47 jsg Exp $	*/
d494 1
a494 1
	unsigned long long to_ticks = 0;
d519 1
a519 1
		to_ticks = (long long)hz * tsp->tv_sec +
@


1.131
log
@add back $OpenBSD$
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d494 1
a494 1
	long long to_ticks = 0;
@


1.130
log
@Make sure that a thread that calls sched_yield(2) ends up on the run queue
behind all other threads in the process by temporarily lowering its priority.
This isn't optimal but it is the easiest way to guarantee that we make
progress when we're waiting on an other thread to release a lock.  This
results in significant improvements for processes that suffer from lock
contention, most notably firefox.  Unfortunately this means that sched_yield(2)
needs to grab the kernel lock again.

All the hard work was done by mpi@@, based on observations of the behaviour
of the BFS scheduler diff by Michal Mazurek.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$openbsd: kern_synch.c,v 1.129 2016/03/09 13:38:50 mpi Exp $	*/
@


1.129
log
@Correct some comments and definitions, from Michal Mazurek.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.128 2016/02/01 23:34:31 dlg Exp $	*/
d435 18
a452 1
	yield();
@


1.128
log
@add a DIAGNOSTIC for refcnt_take overflow.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.127 2016/01/15 11:42:47 dlg Exp $	*/
d62 1
a62 1

a438 1
int thrsleep_unlock(void *, int);
@


1.128.2.1
log
@backport timeout overflow fixes:
fix several places where calculating ticks could overflow.
it's not enough to assign to an unsigned type because if the arithmetic
overflows the compiler may decide to do anything. so change all the
long long casts to uint64_t so that we start with the right type.
reported by Tim Newsham of NCC.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.128 2016/02/01 23:34:31 dlg Exp $	*/
d478 1
a478 1
	uint64_t to_ticks = 0;
d503 1
a503 1
		to_ticks = (uint64_t)hz * tsp->tv_sec +
@


1.127
log
@KASSERT on refcnt underflow.

ok mpi@@ bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.126 2015/11/23 10:56:20 mpi Exp $	*/
d610 6
d617 1
@


1.126
log
@Do not include <sys/atomic.h> inside <sys/refcnt.h>.

Prevent lazy developers, like David and I, to use atomic operations
without including <sys/atomic.h>.

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.125 2015/09/28 21:02:12 deraadt Exp $	*/
d616 6
a621 1
	return (atomic_dec_int_nv(&r->refs) == 0);
@


1.125
log
@satisfy RAMDISK by placing cold == 2 case inside #ifdef DDB
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.124 2015/09/28 18:36:36 deraadt Exp $	*/
d52 1
d599 18
@


1.124
log
@In low-level suspend routines, set cold=2.  In tsleep(), use this to
spit out a ddb trace to console.  This should allow us to find suspend
or resume routines which break the rules.  It depends on the console
output function being non-sleeping.... but that's another codepath which
should try to be safe when cold is set.
ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.123 2015/09/11 19:13:22 dlg Exp $	*/
d119 1
d122 1
@


1.123
log
@introduce a wrapper around reference counts called refcnt.

its basically atomic inc/dec, but it includes magical sleep code
in refcnt_finalise that is better written once than many times.
refcnt_finalise sleeps until all references are released and does
so with sleep_setup and sleep_finalize, which is fairly subtle.

putting this in now so i we can get on with work in the stack, a
proper discussion about visibility and how available intrinsics
should be in the kernel can happen after next week.

with help from guenther@@
ok guenther@@ deraadt@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.122 2015/09/07 15:38:45 guenther Exp $	*/
d52 1
d119 2
@


1.122
log
@Delete ktracing of context switches: it's unused, and not particularly useful,
and doing VOP_WRITE() from inside tsleep/msleep makes the locking too
complicated, making it harder to move forward on MP changes.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.121 2015/05/12 09:30:35 mikeb Exp $	*/
d51 1
d593 21
@


1.121
log
@Drop and reacquire the kernel lock in the vfs_shutdown and "cold"
portions of msleep and tsleep to give interrupts a chance to run
on other CPUs.

Tweak and OK kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.120 2015/05/07 18:30:27 mikeb Exp $	*/
a236 5
#ifdef KTRACE
	if (KTRPOINT(p, KTR_CSW))
		ktrcsw(p, 1, 0);
#endif

a276 5

#ifdef KTRACE
	if (KTRPOINT(p, KTR_CSW))
		ktrcsw(p, 0, 0);
#endif
@


1.121.4.1
log
@backport timeout overflow fixes:
fix several places where calculating ticks could overflow.
it's not enough to assign to an unsigned type because if the arithmetic
overflows the compiler may decide to do anything. so change all the
long long casts to uint64_t so that we start with the right type.
reported by Tim Newsham of NCC.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.121 2015/05/12 09:30:35 mikeb Exp $	*/
d481 1
a481 1
	uint64_t to_ticks = 0;
d506 1
a506 1
		to_ticks = (uint64_t)hz * tsp->tv_sec +
@


1.120
log
@msleep(9) must prevent kernel from attempting a context switch
during autoconf and after panics.

Tweak and OK guenther, OK miod
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.119 2015/03/14 03:38:50 jsg Exp $	*/
d107 3
d127 6
d162 3
d179 6
@


1.119
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.118 2015/02/10 03:40:18 blambert Exp $	*/
d144 1
a144 1
 * Same as tsleep, but if we have a mutex provided, then once we've 
d157 18
d180 1
a180 1
	 * unblock splsched. This can be made a bit more 
@


1.118
log
@assert that we hold the scheduler lock in unsleep()

ok guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.117 2015/02/09 03:15:41 dlg Exp $	*/
a43 1
#include <sys/buf.h>
@


1.117
log
@we want to defer work traditionally (in openbsd) handled in an
interrupt context to a taskq running in a thread. however, there
is a concern that if we do that then we allow accidental use of
sleeping APIs in this work, which will make it harder to move the
work back to interrupts in the future.

guenther and kettenis came up with the idea of marking a proc with
CANTSLEEP which the sleep paths can check and panic on.

this builds on that so you create taskqs that run with CANTSLEEP
set except when they need to sleep for more tasks to run.

the taskq_create api is changed to take a flags argument so users
can specify CANTSLEEP. MPSAFE is also passed via this flags field
now.  this means archs that defined IPL_MPSAFE to 0 can now create
mpsafe taskqs too.

lots of discussion at s2k15
ok guenther@@ miod@@ mpi@@ tedu@@ pelikan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.116 2014/07/08 17:19:25 deraadt Exp $	*/
d353 2
@


1.116
log
@decouple struct uvmexp into a new file, so that uvm_extern.h and sysctl.h
don't need to be married.
ok guenther miod beck jsing kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.115 2014/03/22 06:05:45 guenther Exp $	*/
d194 2
@


1.115
log
@Move p_sigacts from struct proc to struct process.

testing help mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.114 2014/01/23 01:48:44 guenther Exp $	*/
a46 1
#include <uvm/uvm_extern.h>
@


1.114
log
@msleep() with a NULL mtx argument is a programming error.

ok matthew@@ phessler@@ dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.113 2014/01/23 00:33:36 guenther Exp $	*/
d313 2
a314 1
			if (p->p_sigacts->ps_sigintr & sigmask(sls->sls_sig))
@


1.113
log
@Waiting on a condition without a lock is an error: you need *some* lock
to guarantee there isn't a window in which you can lose a wakeup.  The
exception for tsleep() is when it's just being used to sleep for a period
of time, so permit that too.

ok jsing@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.112 2013/12/24 01:11:04 dlg Exp $	*/
d157 1
d163 7
a169 9
	if (mtx) {
		/* XXX - We need to make sure that the mutex doesn't
		 * unblock splsched. This can be made a bit more 
		 * correct when the sched_lock is a mutex.
		 */
		spl = MUTEX_OLDIPL(mtx);
		MUTEX_OLDIPL(mtx) = splsched();
		mtx_leave(mtx);
	}
d175 6
a180 7
	if (mtx) {
		if ((priority & PNORELOCK) == 0) {
			mtx_enter(mtx);
			MUTEX_OLDIPL(mtx) = spl; /* put the ipl back */
		} else
			splx(spl);
	}
@


1.112
log
@get rid of if (timeout_pending()) timeout_del(). this is racy. any
conditionals you did on timeout_pending can now be done on timeout_del
now that it returns what it did.

ok and a very good fix from kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.111 2013/11/25 15:24:18 tedu Exp $	*/
d111 4
@


1.111
log
@rename magicnumber to globalsleepaddr
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.110 2013/11/18 23:09:46 tedu Exp $	*/
d266 1
a266 1
	} else if (timeout_pending(&p->p_sleep_to)) {
a267 1
	}
@


1.110
log
@hack in a global rendezvous for interprocess semaphores to use
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.109 2013/11/09 06:52:15 guenther Exp $	*/
d426 1
a426 1
static int magicnumber;
d493 1
a493 1
			sleepaddr = &magicnumber;
d548 1
a548 1
		wakeup(&magicnumber);
@


1.109
log
@Add KASSERT()s to tsleep() and msleep() to verify that bogus flags
aren't being passed to them.  Fix UVM_WAIT() to not pass PNORELOCK to
tsleep(), as that flag only does something with msleep().

ok beck@@ dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.108 2013/09/14 01:35:01 guenther Exp $	*/
d426 1
d490 5
a494 2
	else
		error = tsleep(&p->p_thrslpid, PUSER | PCATCH, "thrsleep",
d496 1
d547 2
@


1.108
log
@Snapshots for all archs have been built, so remove the T32 code
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.107 2013/08/13 05:52:23 guenther Exp $	*/
d110 2
d151 2
@


1.107
log
@Switch time_t, ino_t, clock_t, and struct kevent's ident and data
members to 64bit types.  Assign new syscall numbers for (almost
all) the syscalls that involve the affected types, including anything
with time_t, timeval, itimerval, timespec, rusage, dirent, stat,
or kevent arguments.  Add a d_off member to struct dirent and replace
getdirentries() with getdents(), thus immensely simplifying and
accelerating telldir/seekdir.  Build perl with -DBIG_TIME.

Bump the major on every single base library: the compat bits included
here are only good enough to make the transition; the T32 compat
option will be burned as soon as we've reached the new world are
are happy with the snapshots for all architectures.

DANGER: ABI incompatibility.  Updating to this kernel requires extra
work or you won't be able to login: install a snapshot instead.

Much assistance in fixing userland issues from deraadt@@ and tedu@@
and build assistance from todd@@ and otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.106 2013/06/01 20:47:40 tedu Exp $	*/
a522 29

#ifdef T32
int
t32_sys___thrsleep(struct proc *p, void *v, register_t *retval)
{
	struct t32_sys___thrsleep_args /* {
		syscallarg(const volatile void *) ident;
		syscallarg(clockid_t) clock_id;
		syscallarg(struct timespec32 *) tp;
		syscallarg(void *) lock;
		syscallarg(const int *) abort;
	} */ *uap = v;
	struct timespec32 ts32;
	struct timespec ts;
	int error;

	if (SCARG(uap, tp) != NULL) {
		if ((error = copyin(SCARG(uap, tp), &ts32, sizeof(ts32)))) {
			*retval = error;
			return (0);
		}
		TIMESPEC_FROM_32(&ts, &ts32);
		SCARG(uap, tp) = (void *)&ts;
	}

	*retval = thrsleep(p, (struct sys___thrsleep_args *)uap);
	return (0);
}
#endif
@


1.106
log
@cleanup and consolidate the spinlock_lock (what a name!) code.
it's now atomic_lock to better reflect its usage, and librthread now
features a new spinlock that's really a ticket lock.
thrlseep can handle both types of lock via a flag in the clock arg.
(temp back compat hack)
remove some old stuff that's accumulated along the way and no longer used.
some feedback from dlg, who is concerned with all things ticket lock.
(you need to boot a new kernel before installing librthread)
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.105 2013/04/06 03:44:34 tedu Exp $	*/
d60 2
d424 1
a424 1
sys___thrsleep(struct proc *p, void *v, register_t *retval)
d429 1
a429 1
		syscallarg(struct timespec *) tp;
d434 1
d441 4
a444 6
	if (ident == 0) {
		*retval = EINVAL;
		return (0);
	}
	if (SCARG(uap, tp) != NULL) {
		struct timespec now, ats;
d446 2
a447 5
		if ((error = copyin(SCARG(uap, tp), &ats, sizeof(ats))) ||
		    (error = clock_gettime(p, clock_id, &now))) {
			*retval = error;
			return (0);
		}
d450 1
a450 1
			ktrabstimespec(p, &ats);
d453 1
a453 1
		if (timespeccmp(&ats, &now, <)) {
d455 3
a457 6
			if ((error = thrsleep_unlock(lock, lockflags))) {
				*retval = error;
				return (0);
			}
			*retval = EWOULDBLOCK;
			return (0);
d460 3
a462 3
		timespecsub(&ats, &now, &ats);
		to_ticks = (long long)hz * ats.tv_sec +
		    (ats.tv_nsec + tick * 1000 - 1) / (tick * 1000) + 1;
d495 26
a520 1
	*retval = error;
d522 1
d524 26
d551 1
@


1.105
log
@rthreads are always enabled. remove the sysctl.
ok deraadt guenther kettenis matthew
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.104 2012/08/21 19:51:58 haesbaert Exp $	*/
d396 25
d432 1
a432 2
	_spinlock_lock_t *lock = SCARG(uap, lock);
	static _spinlock_lock_t unlocked = _SPINLOCK_UNLOCKED;
d435 2
d446 1
a446 1
		    (error = clock_gettime(p, SCARG(uap, clock_id), &now))) {
d457 3
a459 6
			if (lock) {
				if ((error = copyout(&unlocked, lock,
				    sizeof(unlocked))) != 0) {
					*retval = error;
					return (0);
				}
d474 2
a475 3
	if (lock) {
		if ((error = copyout(&unlocked, lock, sizeof(unlocked))) != 0)
			goto out;
@


1.104
log
@Stop "inlining" setrunnable() we already had two bugs because of it.
This also makes sure we call cpu_unidle() on the correct cpu, since the
inlining order was wrong and could call it on the old cpu.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.103 2012/07/10 16:56:28 haesbaert Exp $	*/
a411 4
	if (!rthreads_enabled) {
		*retval = ENOTSUP;
		return (0);
	}
d495 1
a495 3
	if (!rthreads_enabled)
		*retval = ENOTSUP;
	else if (ident == 0)
@


1.103
log
@We should only call need_resched() if the priority is lower than the
priority of the current running process.

In amd64 a call to need_resched() sends an IPI to the other cpu.

This fixes aja@@ problem where he would move the mouse and see 60000
IPIs being sent.

Thanks to mikeb@@ for bringing that subject up tuesday.
Actually found this after inquiring guenther@@ about some changes in
mi_switch().

ok guenther@@ aja@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.102 2012/04/10 11:33:58 guenther Exp $	*/
d373 2
a374 14
			if (p->p_stat == SSLEEP) {
				/* OPTIMIZED EXPANSION OF setrunnable(p); */
				if (p->p_slptime > 1)
					updatepri(p);
				p->p_slptime = 0;
				p->p_stat = SRUN;
				p->p_cpu = sched_choosecpu(p);
				setrunqueue(p);
				if (p->p_priority <
				    p->p_cpu->ci_schedstate.spc_curpriority)
					need_resched(p->p_cpu);
				/* END INLINE EXPANSION */

			}
@


1.102
log
@When converting the timeout to ticks, both round up and add one to account
for the tick that we're already in the middle of.

noted and tested by aja; ok kurt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.101 2012/03/23 15:51:26 guenther Exp $	*/
d381 3
a383 1
				need_resched(p->p_cpu);
@


1.101
log
@Make rusage totals, itimers, and profile settings per-process instead
of per-rthread.  Handling of per-thread tick and runtime counters
inspired by how FreeBSD does it.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.100 2012/03/19 09:05:39 guenther Exp $	*/
d458 1
a458 1
		    ats.tv_nsec / (tick * 1000);
a460 2
		if (to_ticks == 0)
			to_ticks = 1;
@


1.100
log
@Add tracing and dumping of "pointer to struct" syscall arguments for
structs timespec, timeval, sigaction, and rlimit.

ok otto@@ jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.99 2012/01/17 02:34:18 guenther Exp $	*/
d218 1
a218 1
		p->p_stats->p_ru.ru_nvcsw++;
@


1.99
log
@Reimplement mutexes, condvars, and rwlocks to eliminate bugs,
particularly the "consume the signal you just sent" hang, and putting
the wait queues in userspace.

Do cancellation handling in pthread_cond_*wait(), pthread_join(),
and sem_wait().

Add __ prefix to thr{sleep,wakeup,exit,sigdivert}() syscalls; add
'abort" argument to thrsleep to close cancellation race; make
thr{sleep,wakeup} return errno values via *retval to avoid touching
userspace errno.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.98 2011/12/11 19:42:28 guenther Exp $	*/
d438 4
@


1.98
log
@Suspend other rthreads before dumping core or execing; make them exit
when exec succeeds.

ok jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.97 2011/07/07 18:00:33 guenther Exp $	*/
d407 1
a407 1
sys_thrsleep(struct proc *p, void *v, register_t *revtal)
d409 2
a410 2
	struct sys_thrsleep_args /* {
		syscallarg(void *) ident;
d414 1
d420 1
a420 1
	int error;
d422 8
a429 2
	if (!rthreads_enabled)
		return (ENOTSUP);
d433 5
a437 3
		if ((error = copyin(SCARG(uap, tp), &ats, sizeof(ats))) != 0 ||
		    (error = clock_gettime(p, SCARG(uap, clock_id), &now)) != 0)
			return (error);
d443 4
a446 2
				    sizeof(unlocked))) != 0)
					return (error);
d448 2
a449 1
			return (EWOULDBLOCK);
d465 11
a475 1
			return (error);
d477 9
a485 2
	error = tsleep(&p->p_thrslpid, PUSER | PCATCH, "thrsleep",
	    (int)to_ticks);
d490 2
a491 1
	return (error);
d496 1
a496 1
sys_thrwakeup(struct proc *p, void *v, register_t *retval)
d498 2
a499 2
	struct sys_thrwakeup_args /* {
		syscallarg(void *) ident;
d508 11
a518 7
		return (ENOTSUP);
	TAILQ_FOREACH(q, &p->p_p->ps_threads, p_thr_link) {
		if (q->p_thrslpid == ident) {
			wakeup_one(&q->p_thrslpid);
			q->p_thrslpid = 0;
			if (++found == n)
				return (0);
d520 1
a521 2
	if (!found)
		return (ESRCH);
@


1.97
log
@Functions used in files other than where they are defined should be
declared in .h files, not in each .c.  Apply that rule to endtsleep(),
scheduler_start(), updatepri(), and realitexpire()

ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.96 2011/01/25 18:42:45 stsp Exp $	*/
d285 1
a285 1
	if ((sls->sls_sig = CURSIG(p)) != 0) {
d300 1
d303 2
@


1.96
log
@Don't ignore copyout() return value in sys_thrsleep().
Spotted by miod some time ago.
ok miod guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.95 2010/06/29 00:28:14 tedu Exp $	*/
a59 2
void updatepri(struct proc *);
void endtsleep(void *);
@


1.95
log
@Eliminate RTHREADS kernel option in favor of a sysctl.  The actual status
(not done) hasn't changed, but now it's less work to test things.
ok art deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.94 2010/06/10 17:54:12 deraadt Exp $	*/
d431 5
a435 2
			if (lock)
				copyout(&unlocked, lock, sizeof(unlocked));
d450 4
a453 2
	if (lock)
		copyout(&unlocked, lock, sizeof(unlocked));
@


1.94
log
@Declare safepri at the MD level on each platform, so that the kern_synch.c
does not have to deal with it as a common.  Some platforms may be missed
by this commit... if you spot one, fix it the same way.
ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.93 2009/12/27 04:59:43 guenther Exp $	*/
a404 2
#ifdef RTHREADS

d420 2
d471 2
a485 1
#endif
@


1.93
log
@Correct previous commit: match the errno return by thrsleep() in
the already-timed-out case to be the same (EWOULDBLOCK) as when it
times out after sleeping
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.92 2009/11/27 19:45:53 guenther Exp $	*/
d91 1
a91 1
int safepri;
@


1.92
log
@Convert thrsleep() to an absolute timeout with clockid to eliminate a
race condition and prep for later support of pthread_condattr_setclock()

"get it in" deraadt@@, tedu@@, cheers by others
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.91 2009/06/04 04:26:54 beck Exp $	*/
d433 1
a433 1
			return (ETIMEDOUT);
@


1.91
log
@unfuck msleep - fixed by art and ariane after much horror and teeth gnashing
over why the processes were being woken up at splvm after the page daemon
ran - and probably also had the page daemon running at splvm after the first
pass through the loop.
ok art@@ weingart@@ oga@@ ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.90 2009/06/02 23:05:31 guenther Exp $	*/
d410 6
a415 1
	struct sys_thrsleep_args *uap = v;
a416 1
	int timo = SCARG(uap, timeout);
d418 2
a419 1
	_spinlock_lock_t unlocked = _SPINLOCK_UNLOCKED;
d422 23
d449 2
a450 7
	if (hz > 1000)
		timo = timo * (hz / 1000);
	else
		timo = timo / (1000 / hz);
	if (timo < 0)
		timo = 0;
	error = tsleep(&p->p_thrslpid, PUSER | PCATCH, "thrsleep", timo);
d462 4
a465 1
	struct sys_thrwakeup_args *uap = v;
@


1.90
log
@Change the wait-channel type to 'const volatile void *', eliminating
the need for casts when calling tsleep(), msleep(), and wakeup().

"I guess so" oga@@  "it's masturbation" art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.89 2009/04/14 09:13:25 art Exp $	*/
d168 6
a173 3
	if (mtx && (priority & PNORELOCK) == 0) {
		mtx_enter(mtx);
		MUTEX_OLDIPL(mtx) = spl; /* put the ipl back */
@


1.89
log
@Some tweaks to the cpu affinity code.
 - Split up choosing of cpu between fork and "normal" cases. Fork is
   very different and should be treated as such.
 - Instead of implicitly choosing a cpu in setrunqueue, do it outside
   where it actually makes sense.
 - Just because a cpu is marked as idle doesn't mean it will be soon.
   There could be a thundering herd effect if we call wakeup from an
   interrupt handler, so subtract cpus with queued processes when
   deciding which cpu is actually idle.
 - some simplifications allowed by the above.

kettenis@@ ok (except one bugfix that was not in the intial diff)
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.88 2009/03/23 13:25:11 art Exp $	*/
d105 1
a105 1
tsleep(void *ident, int priority, const char *wmesg, int timo)
d144 2
a145 1
msleep(void *ident, struct mutex *mtx,  int priority, const char *wmesg, int timo)
d180 2
a181 1
sleep_setup(struct sleep_state *sls, void *ident, int prio, const char *wmesg)
d350 1
a350 1
wakeup_n(void *ident, int n)
d390 1
a390 1
wakeup(void *chan)
@


1.88
log
@Processor affinity for processes.
 - Split up run queues so that every cpu has one.
 - Make setrunqueue choose the cpu where we want to make this process
   runnable (this should be refined and less brutal in the future).
 - When choosing the cpu where we want to run, make some kind of educated
   guess where it will be best to run (very naive right now).
Other:
 - Set operations for sets of cpus.
 - load average calculations per cpu.
 - sched_is_idle() -> curcpu_is_idle()

tested, debugged and prodded by many@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.87 2008/09/10 12:30:40 blambert Exp $	*/
d373 1
a373 10

				/*
				 * Since curpriority is a user priority,
				 * p->p_priority is always better than
				 * curpriority on the last CPU on
				 * which it ran.
				 *
				 * XXXSMP See affinity comment in
				 * resched_proc().
				 */
@


1.87
log
@There's no need to fully traverse the wakeup queue when waking a specific
process sleeping on a unique address (wakeup -> wakeup_one)

ok guenther@@, tedu@@, art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.86 2008/09/05 14:38:15 oga Exp $	*/
a383 1
				KASSERT(p->p_cpu != NULL);
@


1.86
log
@Back out previous. Art realised a problem with it.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.85 2008/09/05 14:17:50 art Exp $	*/
d449 1
a449 1
	
d452 1
a452 1
			wakeup(&q->p_thrslpid);
@


1.85
log
@Don't overwrite the old ipl in msleep if PNORELOCK was set.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.84 2008/09/05 14:11:57 oga Exp $	*/
d159 1
a159 1
		MUTEX_OLDIPL(mtx) = IPL_SCHED;
@


1.84
log
@When munging the WANTIPL of the mutex to prevent undoing the sched_lock,
use the constant for IPL_SCHED, and not splsched(), which doesn't do what
we want.

ok art@@. Tested by Paul de Weerd.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.83 2007/11/30 16:44:44 oga Exp $	*/
d167 1
a167 1
	if (mtx && (priority & PNORELOCK) == 0)
d169 2
a170 2

	MUTEX_OLDIPL(mtx) = spl; /* put the ipl back else it breaks things */
@


1.83
log
@Fix msleep.

Since mutexes mess around with spl levels, and the sched-lock isn't a
mutex, we need to make sure to fix the IPL when msleep does the locking.


ok art.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.82 2007/11/28 20:07:36 oga Exp $	*/
d159 1
a159 1
		MUTEX_OLDIPL(mtx) = splsched();
@


1.82
log
@Add msleep. This is identical to tsleep but it takes a mutex as a
parameter. The mutex is unlocked just before sleep and relocked after
unless P_NORELOCK is in flags, in which case it is left unlocked.

ok art@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.81 2007/10/10 15:53:53 art Exp $	*/
d147 1
a147 1
	int error, error1;
d158 1
d170 1
@


1.81
log
@Make context switching much more MI:
 - Move the functionality of choosing a process from cpu_switch into
   a much simpler function: cpu_switchto. Instead of having the locore
   code walk the run queues, let the MI code choose the process we
   want to run and only implement the context switching itself in MD
   code.
 - Let MD context switching run without worrying about spls or locks.
 - Instead of having the idle loop implemented with special contexts
   in MD code, implement one idle proc for each cpu. make the idle
   loop MI with MD hooks.
 - Change the proc lists from the old style vax queues to TAILQs.
 - Change the sleep queue from vax queues to TAILQs. This makes
   wakeup() go from O(n^2) to O(n)

there will be some MD fallout, but it will be fixed shortly.
There's also a few cleanups to be done after this.

deraadt@@, kettenis@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.80 2007/05/16 17:27:30 art Exp $	*/
d131 37
@


1.80
log
@The world of __HAVEs and __HAVE_NOTs is reducing. All architectures
have cpu_info now, so kill the option.

eyeballed by jsg@@ and grange@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.79 2007/04/03 08:05:43 art Exp $	*/
d70 11
a80 4
struct slpque {
	struct proc *sq_head;
	struct proc **sq_tailp;
} slpque[TABLESIZE];
a142 1
	struct slpque *qp;
a148 2
	if (p->p_back != NULL)
		panic("tsleep: p_back not NULL");
d166 1
a166 6
	qp = &slpque[LOOKUP(ident)];
	if (qp->sq_head == 0)
		qp->sq_head = p;
	else
		*qp->sq_tailp = p;
	*(qp->sq_tailp = &p->p_forw) = NULL;
d181 2
d184 2
a185 2
		if (p->p_stat != SONPROC)
			panic("sleep_finish !SONPROC");
a186 1
	}
a298 3
	struct slpque *qp;
	struct proc **hp;

d300 2
a301 7
		hp = &(qp = &slpque[LOOKUP(p->p_wchan)])->sq_head;
		while (*hp != p)
			hp = &(*hp)->p_forw;
		*hp = p->p_forw;
		if (qp->sq_tailp == &p->p_forw)
			qp->sq_tailp = hp;
		p->p_wchan = 0;
d312 2
a313 1
	struct proc *p, **q;
d318 2
a319 2
restart:
	for (q = &qp->sq_head; (p = *q) != NULL; ) {
a320 2
		if (p->p_back)
			panic("wakeup: p_back not NULL");
d327 1
a327 3
			*q = p->p_forw;
			if (qp->sq_tailp == &p->p_forw)
				qp->sq_tailp = q;
a348 4
				if (n != 0)
					goto restart;
				else
					break;
d350 1
a350 2
		} else
			q = &p->p_forw;
@


1.79
log
@Start moving state that is shared among threads in a process into
a new struct. Instead of doing a huge rename and deal with the fallout
for weeks, like other projects that need no mention, we will slowly and
carefully move things out of struct proc into a new struct process.

 - Create struct process and the infrastructure to create and remove them.
 - Move threads in a process into struct process.

deraadt@@, tedu@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.78 2007/03/21 09:09:52 art Exp $	*/
a187 1
#ifdef __HAVE_CPUINFO
a188 3
#else
	curpriority = p->p_usrpri;
#endif
a355 1
#ifdef __HAVE_CPUINFO
a357 3
#else
				need_resched(NULL);
#endif
@


1.78
log
@Split tsleep into pieces. Instead of doing everything in a large "shove
everything into it" function, there are now 6 stages of tsleep with
an on-stack (remember that kernel stacks are not swappable now?)
structure that keeps track of the state.

This way we first setup the sleep, setup the events that might break the
sleep, finish the sleep (actually sleeping) and then take care of the
events that could wake us up.

In the future this will make it easier to implement functionality like:
setup sleep, release lock or check some condition, finish sleep, in a
race-free way and without duplicating or complicating the tsleep function
too much.

miod@@, millert@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.77 2007/03/18 10:46:51 art Exp $	*/
d435 1
a435 8
	/* have to check the parent, it's not in the thread list */
	if (p->p_thrparent->p_thrslpid == ident) {
		wakeup(&p->p_thrparent->p_thrslpid);
		p->p_thrparent->p_thrslpid = 0;
		if (++found == n)
			return (0);
	}
	LIST_FOREACH(q, &p->p_thrparent->p_thrchildren, p_thrsib) {
@


1.77
log
@Don't restart thrsleep after a signal. After a signal happened and we
weren't on the sleep queues, the condition we were sleeping on might
have changed, so we need to go back to userland and recheck that condition.

This fixes the majority of lockups and and hanging threads in rthreads
since it fixes a race in the semaphore code.

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.76 2007/03/15 10:22:30 art Exp $	*/
a95 5
 *
 * The interlock is held until the scheduler_slock (XXX) is held.  The
 * interlock will be locked before returning back to the caller
 * unless the PNORELOCK flag is specified, in which case the
 * interlock will always be unlocked upon return.
d98 1
a98 2
ltsleep(void *ident, int priority, const char *wmesg, int timo,
    volatile struct simplelock *interlock)
d100 2
a101 5
	struct proc *p = curproc;
	struct slpque *qp;
	int s, sig;
	int catch = priority & PCATCH;
	int relock = (priority & PNORELOCK) == 0;
d104 1
a113 2
		if (interlock != NULL && relock == 0)
			simple_unlock(interlock);
d117 14
a130 4
#ifdef KTRACE
	if (KTRPOINT(p, KTR_CSW))
		ktrcsw(p, 1, 0);
#endif
d132 5
a136 1
	SCHED_LOCK(s);
d147 11
d161 1
a161 1
	p->p_priority = priority & PRIMASK;
d167 28
a194 3
	*(qp->sq_tailp = &p->p_forw) = 0;
	if (timo)
		timeout_add(&p->p_sleep_to, timo);
d196 2
a197 7
	 * We can now release the interlock; the scheduler_slock
	 * is held, so a thread can't get in to do wakeup() before
	 * we do the switch.
	 *
	 * XXX We leave the code block here, after inserting ourselves
	 * on the sleep queue, because we might want a more clever
	 * data structure for the sleep queues at some point.
d199 37
a235 2
	if (interlock != NULL)
		simple_unlock(interlock);
d246 11
a256 22
	if (catch) {
		atomic_setbits_int(&p->p_flag, P_SINTR);
		if ((sig = CURSIG(p)) != 0) {
			if (p->p_wchan)
				unsleep(p);
			p->p_stat = SONPROC;
			goto resume;
		}
		if (p->p_wchan == 0) {
			catch = 0;
			goto resume;
		}
	} else
		sig = 0;
	p->p_stat = SSLEEP;
	p->p_stats->p_ru.ru_nvcsw++;
	SCHED_ASSERT_LOCKED();
	mi_switch();
#ifdef	DDB
	/* handy breakpoint location after process "wakes" */
	__asm(".globl bpendtsleep\nbpendtsleep:");
#endif
d258 4
a261 2
resume:
	SCHED_UNLOCK(s);
d263 5
a267 16
#ifdef __HAVE_CPUINFO
	p->p_cpu->ci_schedstate.spc_curpriority = p->p_usrpri;
#else
	curpriority = p->p_usrpri;
#endif
	atomic_clearbits_int(&p->p_flag, P_SINTR);
	if (p->p_flag & P_TIMEOUT) {
		atomic_clearbits_int(&p->p_flag, P_TIMEOUT);
		if (sig == 0) {
#ifdef KTRACE
			if (KTRPOINT(p, KTR_CSW))
				ktrcsw(p, 0, 0);
#endif
			if (interlock != NULL && relock)
				simple_lock(interlock);
			return (EWOULDBLOCK);
a268 12
	} else if (timo)
		timeout_del(&p->p_sleep_to);
	if (catch && (sig != 0 || (sig = CURSIG(p)) != 0)) {
#ifdef KTRACE
		if (KTRPOINT(p, KTR_CSW))
			ktrcsw(p, 0, 0);
#endif
		if (interlock != NULL && relock)
			simple_lock(interlock);
		if (p->p_sigacts->ps_sigintr & sigmask(sig))
			return (EINTR);
		return (ERESTART);
a269 4
#ifdef KTRACE
	if (KTRPOINT(p, KTR_CSW))
		ktrcsw(p, 0, 0);
#endif
a270 2
	if (interlock != NULL && relock)
		simple_lock(interlock);
d283 1
a283 1
	struct proc *p;
a285 1
	p = (struct proc *)arg;
a304 2
#if 0
	int s;
a305 6
	/*
	 * XXX we cannot do recursive SCHED_LOCKing yet.  All callers lock
	 * anyhow.
	 */
	SCHED_LOCK(s);
#endif
a314 3
#if 0
	SCHED_UNLOCK(s);
#endif
@


1.76
log
@Since p_flag is often manipulated in interrupts and without biglock
it's a good idea to use atomic.h operations on it. This mechanic
change updates all bit operations on p_flag to atomic_{set,clear}bits_int.

Only exception is that P_OWEUPC is set by MI code before calling
need_proftick and it's automatically cleared by ADDUPC. There's
no reason for MD handling of that flag since everyone handles it the
same way.

kettenis@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.75 2006/11/29 12:24:17 miod Exp $	*/
d398 3
@


1.75
log
@Kernel stack can be swapped. This means that stuff that's on the stack
should never be referenced outside the context of the process to which
this stack belongs unless we do the PHOLD/PRELE dance. Loads of code
doesn't follow the rules here. Instead of trying to track down all
offenders and fix this hairy situation, it makes much more sense
to not swap kernel stacks.

From art@@, tested by many some time ago.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.74 2006/10/21 02:18:00 tedu Exp $	*/
d177 1
a177 1
		p->p_flag |= P_SINTR;
d207 1
a207 1
	p->p_flag &= ~P_SINTR;
d209 1
a209 1
		p->p_flag &= ~P_TIMEOUT;
d261 1
a261 1
		p->p_flag |= P_TIMEOUT;
@


1.74
log
@tbert sent me a diff to change some 0 to NULL
i got carried away and deleted a whole bunch of useless casts
this is C, not C++.  ok md5
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.73 2005/12/30 04:02:17 tedu Exp $	*/
d339 1
a339 2
				if ((p->p_flag & P_INMEM) != 0) {
					setrunqueue(p);
d341 2
a342 2
					KASSERT(p->p_cpu != NULL);
					need_resched(p->p_cpu);
d344 1
a344 1
					need_resched(NULL);
a345 3
				} else {
					wakeup(&proc0);
				}
@


1.73
log
@change thrwakeup to take an argument which specifies how many threads
to wakeup.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.72 2005/12/22 06:55:03 tedu Exp $	*/
d345 1
a345 1
					need_resched(0);
d348 1
a348 1
					wakeup((caddr_t)&proc0);
@


1.72
log
@fix memory leak conditions in thrsleep and significantly simplify
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.71 2005/12/14 06:54:38 tedu Exp $	*/
d412 1
d419 3
a421 1
		found = 1;
d427 2
a428 1
			found = 1;
@


1.71
log
@timeout code is not so happy with the negative values
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.70 2005/12/14 04:03:51 tedu Exp $	*/
a380 2
struct pool sleeper_pool;

a388 2

	struct twaitnode *n, *n2;
d391 1
a391 13
	n = pool_get(&sleeper_pool, PR_WAITOK);
	n->t_ident = ident;
	/* we may have slept */
	LIST_FOREACH(n2, &p->p_thrparent->p_sleepers, t_next) {
		if (n2->t_ident == ident)
			break;
	}
	if (n2) {
		pool_put(&sleeper_pool, n);
		n = n2;
	} else {
		LIST_INSERT_HEAD(&p->p_thrparent->p_sleepers, n, t_next);
	}
d401 1
a401 1
	error = tsleep(n, PUSER | PCATCH, "thrsleep", timo);
d412 2
a413 1
	struct twaitnode *n;
d415 10
a424 4
	LIST_FOREACH(n, &p->p_thrparent->p_sleepers, t_next) {
		if (n->t_ident == ident) {
			LIST_REMOVE(n, t_next);
			break;
d427 1
a427 1
	if (!n)
a428 3
	wakeup(n);
	pool_put(&sleeper_pool, n);
	yield();
@


1.70
log
@change wait message for thrsleep to "thrsleep"
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.69 2005/12/13 07:34:38 tedu Exp $	*/
d415 2
@


1.69
log
@stupid me got the cast backwards
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.68 2005/12/13 06:03:54 tedu Exp $	*/
d415 1
a415 1
	error = tsleep(n, PUSER | PCATCH, "sys_tsleep", timo);
@


1.68
log
@thrsleep and thrwakeup, cast syscall arg from void * to long.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.67 2005/12/03 18:09:08 tedu Exp $	*/
d425 1
a425 1
	long ident = (void *)SCARG(uap, ident);
@


1.67
log
@kernel support for threaded processes (rthreads).
uses rfork(RFTHREAD) to create threads, which are presently processes
that are a little more tightly bound together.  several new syscalls
added to facilitate a userland thread library.
all conditional on RTHREADS, currently disabled.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.66 2005/11/28 00:14:29 jsg Exp $	*/
d387 1
a387 1
	long ident = SCARG(uap, ident);
d425 1
a425 1
	long ident = SCARG(uap, ident);
@


1.66
log
@ansi/deregister.
'go for it' deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.65 2005/11/15 22:15:18 pedro Exp $	*/
d50 5
d135 6
a140 2
	if (ident == NULL || p->p_stat != SONPROC || p->p_back != NULL)
		panic("tsleep");
d312 4
a315 2
		if (p->p_back || (p->p_stat != SSLEEP && p->p_stat != SSTOP))
			panic("wakeup");
d371 72
@


1.65
log
@Match comments with reality
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.64 2005/06/17 22:33:34 niklas Exp $	*/
d98 2
a99 5
ltsleep(ident, priority, wmesg, timo, interlock)
	void *ident;
	int priority, timo;
	const char *wmesg;
	volatile struct simplelock *interlock;
d240 1
a240 2
endtsleep(arg)
	void *arg;
d261 1
a261 2
unsleep(p)
	register struct proc *p;
d263 2
a264 2
	register struct slpque *qp;
	register struct proc **hp;
d292 1
a292 3
wakeup_n(ident, n)
	void *ident;
	int n;
d356 1
a356 2
wakeup(chan)
	void *chan;
@


1.64
log
@A second approach at fixing the telnet localhost & problem
(but I tend to call it ssh localhost & now when telnetd is
history).  This is more localized patch, but leaves us with
a recursive lock for protecting scheduling and signal state.
Better care is taken to actually be symmetric over mi_switch.
Also, the dolock cruft in psignal can go with this solution.
Better test runs by more people for longer time has been
carried out compared to the c2k5 patch.

Long term the current mess with interruptible sleep, the
default action on stop signals and wakeup interactions need
to be revisited.  ok deraadt@@, art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.63 2005/05/29 03:20:41 deraadt Exp $	*/
d4 1
a4 1
/*-
a57 1

d294 1
a294 1
 * Make all processes sleeping on the specified identifier runnable.
d359 3
@


1.63
log
@sched work by niklas and art backed out; causes panics
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.61 2004/07/29 06:25:45 tedu Exp $	*/
a176 1
			SCHED_UNLOCK(s);
a180 1
			SCHED_UNLOCK(s);
d194 2
a195 6
	SCHED_ASSERT_UNLOCKED();
	/*
	 * Note! this splx belongs to the SCHED_LOCK(s) above, mi_switch
	 * releases the scheduler lock, but does not lower the spl.
	 */
	splx(s);
a196 1
resume:
@


1.62
log
@This patch is mortly art's work and was done *a year* ago.  Art wants to thank
everyone for the prompt review and ok of this work ;-)  Yeah, that includes me
too, or maybe especially me.  I am sorry.

Change the sched_lock to a mutex. This fixes, among other things, the infamous
"telnet localhost &" problem.  The real bug in that case was that the sched_lock
which is by design a non-recursive lock, was recursively acquired, and not
enough releases made us hold the lock in the idle loop, blocking scheduling
on the other processors.  Some of the other processors would hold the biglock though,
which made it impossible for cpu 0 to enter the kernel...  A nice deadlock.
Let me just say debugging this for days just to realize that it was all fixed
in an old diff noone ever ok'd was somewhat of an anti-climax.

This diff also changes splsched to be correct for all our architectures.
@
text
@a147 3

	p->p_stat = SSLEEP;

a149 1

a172 1
		SCHED_UNLOCK(s);	/* XXX - must unlock for CURSIG */
a173 1
			SCHED_LOCK(s);
a179 1
		SCHED_LOCK(s);
d187 1
d190 1
a190 1
	mi_switch(s);
d196 7
d273 2
a274 1
unsleep(struct proc *p)
d276 4
a279 4
	struct slpque *qp;
	struct proc **hp;

	SCHED_ASSERT_LOCKED();
d281 6
d296 1
a296 19
}

void
wakeup(void *ident)
{
	int s;

	SCHED_LOCK(s);
	sched_wakeup(ident);
	SCHED_UNLOCK(s);
}

void
wakeup_n(void *ident, int n)
{
	int s;

	SCHED_LOCK(s);
	sched_wakeup_n(ident, n);
d298 1
d305 3
a307 1
sched_wakeup_n(void *ident, int n)
d311 1
d313 1
a313 2
	SCHED_ASSERT_LOCKED();

d352 1
a352 1
					sched_wakeup((caddr_t)&proc0);
d364 8
@


1.61
log
@put the scheduler in its own file.  reduces clutter, and logically separates
"put this process to sleep" and "find a process to run" operations.
no functional change.  ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.60 2004/07/25 20:50:51 tedu Exp $	*/
d148 3
d153 1
d177 1
d179 1
d186 1
a193 1
	p->p_stat = SSLEEP;
d196 1
a196 1
	mi_switch();
a201 7
	SCHED_ASSERT_UNLOCKED();
	/*
	 * Note! this splx belongs to the SCHED_LOCK(s) above, mi_switch
	 * releases the scheduler lock, but does not lower the spl.
	 */
	splx(s);

d272 1
a272 2
unsleep(p)
	register struct proc *p;
d274 4
a277 4
	register struct slpque *qp;
	register struct proc **hp;
#if 0
	int s;
a278 6
	/*
	 * XXX we cannot do recursive SCHED_LOCKing yet.  All callers lock
	 * anyhow.
	 */
	SCHED_LOCK(s);
#endif
d288 19
a306 1
#if 0
a307 1
#endif
d314 1
a314 3
wakeup_n(ident, n)
	void *ident;
	int n;
a317 1
	int s;
d319 2
a320 1
	SCHED_LOCK(s);
d359 1
a359 1
					wakeup((caddr_t)&proc0);
a370 8
	SCHED_UNLOCK(s);
}

void
wakeup(chan)
	void *chan;
{
	wakeup_n(chan, -1);
@


1.60
log
@move db_show_all_procs to kern_proc.c, proc_printit goes in DDB too.
shuffle functions around so that scheduler is all together.
no real functional changes. ok art@@ testing miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.59 2004/06/24 19:35:24 tholo Exp $	*/
a54 23
#include <machine/cpu.h>

#ifndef __HAVE_CPUINFO
u_char	curpriority;		/* usrpri of curproc */
#endif
int	lbolt;			/* once a second sleep address */
#ifdef __HAVE_CPUINFO
int	rrticks_init;		/* # of hardclock ticks per roundrobin() */
#endif

int whichqs;			/* Bit mask summary of non-empty Q's. */
struct prochd qs[NQS];

struct SIMPLELOCK sched_lock;

void scheduler_start(void);

#ifdef __HAVE_CPUINFO
void roundrobin(struct cpu_info *);
#else
void roundrobin(void *);
#endif
void schedcpu(void *);
a56 609

void
scheduler_start()
{
#ifndef __HAVE_CPUINFO
	static struct timeout roundrobin_to;
#endif
	static struct timeout schedcpu_to;

	/*
	 * We avoid polluting the global namespace by keeping the scheduler
	 * timeouts static in this function.
	 * We setup the timeouts here and kick schedcpu and roundrobin once to
	 * make them do their job.
	 */

#ifndef __HAVE_CPUINFO
	timeout_set(&roundrobin_to, roundrobin, &roundrobin_to);
#endif
	timeout_set(&schedcpu_to, schedcpu, &schedcpu_to);

#ifdef __HAVE_CPUINFO
	rrticks_init = hz / 10;
#else
	roundrobin(&roundrobin_to);
#endif
	schedcpu(&schedcpu_to);
}

/*
 * Force switch among equal priority processes every 100ms.
 */
/* ARGSUSED */
#ifdef __HAVE_CPUINFO
void
roundrobin(struct cpu_info *ci)
{
	struct schedstate_percpu *spc = &ci->ci_schedstate;
	int s;

	spc->spc_rrticks = rrticks_init;

	if (curproc != NULL) {
		s = splstatclock();
		if (spc->spc_schedflags & SPCF_SEENRR) {
			/*
			 * The process has already been through a roundrobin
			 * without switching and may be hogging the CPU.
			 * Indicate that the process should yield.
			 */
			spc->spc_schedflags |= SPCF_SHOULDYIELD;
		} else {
			spc->spc_schedflags |= SPCF_SEENRR;
		}
		splx(s);
	}

	need_resched(curcpu());
}
#else
void
roundrobin(void *arg)
{
	struct timeout *to = (struct timeout *)arg;
	struct proc *p = curproc;
	int s;

	if (p != NULL) {
		s = splstatclock();
		if (p->p_schedflags & PSCHED_SEENRR) {
			/*
			 * The process has already been through a roundrobin
			 * without switching and may be hogging the CPU.
			 * Indicate that the process should yield.
			 */
			p->p_schedflags |= PSCHED_SHOULDYIELD;
		} else {
			p->p_schedflags |= PSCHED_SEENRR;
		}
		splx(s);
	}

	need_resched(0);
	timeout_add(to, hz / 10);
}
#endif

/*
 * Constants for digital decay and forget:
 *	90% of (p_estcpu) usage in 5 * loadav time
 *	95% of (p_pctcpu) usage in 60 seconds (load insensitive)
 *          Note that, as ps(1) mentions, this can let percentages
 *          total over 100% (I've seen 137.9% for 3 processes).
 *
 * Note that hardclock updates p_estcpu and p_cpticks independently.
 *
 * We wish to decay away 90% of p_estcpu in (5 * loadavg) seconds.
 * That is, the system wants to compute a value of decay such
 * that the following for loop:
 * 	for (i = 0; i < (5 * loadavg); i++)
 * 		p_estcpu *= decay;
 * will compute
 * 	p_estcpu *= 0.1;
 * for all values of loadavg:
 *
 * Mathematically this loop can be expressed by saying:
 * 	decay ** (5 * loadavg) ~= .1
 *
 * The system computes decay as:
 * 	decay = (2 * loadavg) / (2 * loadavg + 1)
 *
 * We wish to prove that the system's computation of decay
 * will always fulfill the equation:
 * 	decay ** (5 * loadavg) ~= .1
 *
 * If we compute b as:
 * 	b = 2 * loadavg
 * then
 * 	decay = b / (b + 1)
 *
 * We now need to prove two things:
 *	1) Given factor ** (5 * loadavg) ~= .1, prove factor == b/(b+1)
 *	2) Given b/(b+1) ** power ~= .1, prove power == (5 * loadavg)
 *	
 * Facts:
 *         For x close to zero, exp(x) =~ 1 + x, since
 *              exp(x) = 0! + x**1/1! + x**2/2! + ... .
 *              therefore exp(-1/b) =~ 1 - (1/b) = (b-1)/b.
 *         For x close to zero, ln(1+x) =~ x, since
 *              ln(1+x) = x - x**2/2 + x**3/3 - ...     -1 < x < 1
 *              therefore ln(b/(b+1)) = ln(1 - 1/(b+1)) =~ -1/(b+1).
 *         ln(.1) =~ -2.30
 *
 * Proof of (1):
 *    Solve (factor)**(power) =~ .1 given power (5*loadav):
 *	solving for factor,
 *      ln(factor) =~ (-2.30/5*loadav), or
 *      factor =~ exp(-1/((5/2.30)*loadav)) =~ exp(-1/(2*loadav)) =
 *          exp(-1/b) =~ (b-1)/b =~ b/(b+1).                    QED
 *
 * Proof of (2):
 *    Solve (factor)**(power) =~ .1 given factor == (b/(b+1)):
 *	solving for power,
 *      power*ln(b/(b+1)) =~ -2.30, or
 *      power =~ 2.3 * (b + 1) = 4.6*loadav + 2.3 =~ 5*loadav.  QED
 *
 * Actual power values for the implemented algorithm are as follows:
 *      loadav: 1       2       3       4
 *      power:  5.68    10.32   14.94   19.55
 */

/* calculations for digital decay to forget 90% of usage in 5*loadav sec */
#define	loadfactor(loadav)	(2 * (loadav))
#define	decay_cpu(loadfac, cpu)	(((loadfac) * (cpu)) / ((loadfac) + FSCALE))

/* decay 95% of `p_pctcpu' in 60 seconds; see CCPU_SHIFT before changing */
fixpt_t	ccpu = 0.95122942450071400909 * FSCALE;		/* exp(-1/20) */

/*
 * If `ccpu' is not equal to `exp(-1/20)' and you still want to use the
 * faster/more-accurate formula, you'll have to estimate CCPU_SHIFT below
 * and possibly adjust FSHIFT in "param.h" so that (FSHIFT >= CCPU_SHIFT).
 *
 * To estimate CCPU_SHIFT for exp(-1/20), the following formula was used:
 *	1 - exp(-1/20) ~= 0.0487 ~= 0.0488 == 1 (fixed pt, *11* bits).
 *
 * If you dont want to bother with the faster/more-accurate formula, you
 * can set CCPU_SHIFT to (FSHIFT + 1) which will use a slower/less-accurate
 * (more general) method of calculating the %age of CPU used by a process.
 */
#define	CCPU_SHIFT	11

/*
 * Recompute process priorities, every hz ticks.
 */
/* ARGSUSED */
void
schedcpu(arg)
	void *arg;
{
	struct timeout *to = (struct timeout *)arg;
	fixpt_t loadfac = loadfactor(averunnable.ldavg[0]);
	struct proc *p;
	int s;
	unsigned int newcpu;
	int phz;

	/*
	 * If we have a statistics clock, use that to calculate CPU
	 * time, otherwise revert to using the profiling clock (which,
	 * in turn, defaults to hz if there is no separate profiling
	 * clock available)
	 */
	phz = stathz ? stathz : profhz;
	KASSERT(phz);

	for (p = LIST_FIRST(&allproc); p != 0; p = LIST_NEXT(p, p_list)) {
		/*
		 * Increment time in/out of memory and sleep time
		 * (if sleeping).  We ignore overflow; with 16-bit int's
		 * (remember them?) overflow takes 45 days.
		 */
		p->p_swtime++;
		if (p->p_stat == SSLEEP || p->p_stat == SSTOP)
			p->p_slptime++;
		p->p_pctcpu = (p->p_pctcpu * ccpu) >> FSHIFT;
		/*
		 * If the process has slept the entire second,
		 * stop recalculating its priority until it wakes up.
		 */
		if (p->p_slptime > 1)
			continue;
		s = splstatclock();	/* prevent state changes */
		/*
		 * p_pctcpu is only for ps.
		 */
#if	(FSHIFT >= CCPU_SHIFT)
		p->p_pctcpu += (phz == 100)?
			((fixpt_t) p->p_cpticks) << (FSHIFT - CCPU_SHIFT):
                	100 * (((fixpt_t) p->p_cpticks)
				<< (FSHIFT - CCPU_SHIFT)) / phz;
#else
		p->p_pctcpu += ((FSCALE - ccpu) *
			(p->p_cpticks * FSCALE / phz)) >> FSHIFT;
#endif
		p->p_cpticks = 0;
		newcpu = (u_int) decay_cpu(loadfac, p->p_estcpu);
		p->p_estcpu = newcpu;
		splx(s);
		SCHED_LOCK(s);
		resetpriority(p);
		if (p->p_priority >= PUSER) {
			if ((p != curproc) &&
			    p->p_stat == SRUN &&
			    (p->p_flag & P_INMEM) &&
			    (p->p_priority / PPQ) != (p->p_usrpri / PPQ)) {
				remrunqueue(p);
				p->p_priority = p->p_usrpri;
				setrunqueue(p);
			} else
				p->p_priority = p->p_usrpri;
		}
		SCHED_UNLOCK(s);
	}
	uvm_meter();
	wakeup((caddr_t)&lbolt);
	timeout_add(to, hz);
}

/*
 * Recalculate the priority of a process after it has slept for a while.
 * For all load averages >= 1 and max p_estcpu of 255, sleeping for at
 * least six times the loadfactor will decay p_estcpu to zero.
 */
void
updatepri(p)
	register struct proc *p;
{
	register unsigned int newcpu = p->p_estcpu;
	register fixpt_t loadfac = loadfactor(averunnable.ldavg[0]);

	SCHED_ASSERT_LOCKED();

	if (p->p_slptime > 5 * loadfac)
		p->p_estcpu = 0;
	else {
		p->p_slptime--;	/* the first time was done in schedcpu */
		while (newcpu && --p->p_slptime)
			newcpu = (int) decay_cpu(loadfac, newcpu);
		p->p_estcpu = newcpu;
	}
	resetpriority(p);
}

#if defined(MULTIPROCESSOR) || defined(LOCKDEBUG)
void
sched_unlock_idle(void)
{
	SIMPLE_UNLOCK(&sched_lock);
}

void
sched_lock_idle(void)
{
	SIMPLE_LOCK(&sched_lock);
}
#endif /* MULTIPROCESSOR || LOCKDEBUG */

/*
 * General yield call.  Puts the current process back on its run queue and
 * performs a voluntary context switch.
 */
void
yield()
{
	struct proc *p = curproc;
	int s;

	SCHED_LOCK(s);
	p->p_priority = p->p_usrpri;
	setrunqueue(p);
	p->p_stats->p_ru.ru_nvcsw++;
	mi_switch();
	SCHED_ASSERT_UNLOCKED();
	splx(s);
}

/*
 * General preemption call.  Puts the current process back on its run queue
 * and performs an involuntary context switch.  If a process is supplied,
 * we switch to that process.  Otherwise, we use the normal process selection
 * criteria.
 */
void
preempt(newp)
	struct proc *newp;
{
	struct proc *p = curproc;
	int s;

	/*
	 * XXX Switching to a specific process is not supported yet.
	 */
	if (newp != NULL)
		panic("preempt: cpu_preempt not yet implemented");

	SCHED_LOCK(s);
	p->p_priority = p->p_usrpri;
	p->p_stat = SRUN;
	setrunqueue(p);
	p->p_stats->p_ru.ru_nivcsw++;
	mi_switch();
	SCHED_ASSERT_UNLOCKED();
	splx(s);
}


/*
 * Must be called at splstatclock() or higher.
 */
void
mi_switch()
{
	struct proc *p = curproc;	/* XXX */
	struct rlimit *rlim;
	struct timeval tv;
#if defined(MULTIPROCESSOR)
	int hold_count;
#endif
#ifdef __HAVE_CPUINFO
	struct schedstate_percpu *spc = &p->p_cpu->ci_schedstate;
#endif

	SCHED_ASSERT_LOCKED();

#if defined(MULTIPROCESSOR)
	/*
	 * Release the kernel_lock, as we are about to yield the CPU.
	 * The scheduler lock is still held until cpu_switch()
	 * selects a new process and removes it from the run queue.
	 */
	if (p->p_flag & P_BIGLOCK)
#ifdef notyet
		hold_count = spinlock_release_all(&kernel_lock);
#else
		hold_count = __mp_release_all(&kernel_lock);
#endif
#endif

	/*
	 * Compute the amount of time during which the current
	 * process was running, and add that to its total so far.
	 * XXX - use microuptime here to avoid strangeness.
	 */
	microuptime(&tv);
#ifdef __HAVE_CPUINFO
	if (timercmp(&tv, &spc->spc_runtime, <)) {
#if 0
		printf("uptime is not monotonic! "
		    "tv=%lu.%06lu, runtime=%lu.%06lu\n",
		    tv.tv_sec, tv.tv_usec, spc->spc_runtime.tv_sec,
		    spc->spc_runtime.tv_usec);
#endif
	} else {
		timersub(&tv, &spc->spc_runtime, &tv);
		timeradd(&p->p_rtime, &tv, &p->p_rtime);
	}
#else
	if (timercmp(&tv, &runtime, <)) {
#if 0
		printf("uptime is not monotonic! "
		    "tv=%lu.%06lu, runtime=%lu.%06lu\n",
		    tv.tv_sec, tv.tv_usec, runtime.tv_sec, runtime.tv_usec);
#endif
	} else {
		timersub(&tv, &runtime, &tv);
		timeradd(&p->p_rtime, &tv, &p->p_rtime);
	}
#endif

	/*
	 * Check if the process exceeds its cpu resource allocation.
	 * If over max, kill it.
	 */
	rlim = &p->p_rlimit[RLIMIT_CPU];
	if ((rlim_t)p->p_rtime.tv_sec >= rlim->rlim_cur) {
		if ((rlim_t)p->p_rtime.tv_sec >= rlim->rlim_max) {
			psignal(p, SIGKILL);
		} else {
			psignal(p, SIGXCPU);
			if (rlim->rlim_cur < rlim->rlim_max)
				rlim->rlim_cur += 5;
		}
	}

	/*
	 * Process is about to yield the CPU; clear the appropriate
	 * scheduling flags.
	 */
#ifdef __HAVE_CPUINFO
	spc->spc_schedflags &= ~SPCF_SWITCHCLEAR;
#else
	p->p_schedflags &= ~PSCHED_SWITCHCLEAR;
#endif

	/*
	 * Pick a new current process and record its start time.
	 */
	uvmexp.swtch++;
	cpu_switch(p);

	/*
	 * Make sure that MD code released the scheduler lock before
	 * resuming us.
	 */
	SCHED_ASSERT_UNLOCKED();

	/*
	 * We're running again; record our new start time.  We might
	 * be running on a new CPU now, so don't use the cache'd
	 * schedstate_percpu pointer.
	 */
#ifdef __HAVE_CPUINFO
	KDASSERT(p->p_cpu != NULL);
	KDASSERT(p->p_cpu == curcpu());
	microuptime(&p->p_cpu->ci_schedstate.spc_runtime);
#else
	microuptime(&runtime);
#endif

#if defined(MULTIPROCESSOR)
	/*
	 * Reacquire the kernel_lock now.  We do this after we've
	 * released the scheduler lock to avoid deadlock, and before
	 * we reacquire the interlock.
	 */
	if (p->p_flag & P_BIGLOCK)
#ifdef notyet
		spinlock_acquire_count(&kernel_lock, hold_count);
#else
		__mp_acquire_count(&kernel_lock, hold_count);
#endif
#endif
}

/*
 * Initialize the (doubly-linked) run queues
 * to be empty.
 */
void
rqinit()
{
	register int i;

	for (i = 0; i < NQS; i++)
		qs[i].ph_link = qs[i].ph_rlink = (struct proc *)&qs[i];
	SIMPLE_LOCK_INIT(&sched_lock);
}

static __inline void
resched_proc(struct proc *p, u_char pri)
{
#ifdef __HAVE_CPUINFO
	struct cpu_info *ci;
#endif

	/*
	 * XXXSMP
	 * Since p->p_cpu persists across a context switch,
	 * this gives us *very weak* processor affinity, in
	 * that we notify the CPU on which the process last
	 * ran that it should try to switch.
	 *
	 * This does not guarantee that the process will run on
	 * that processor next, because another processor might
	 * grab it the next time it performs a context switch.
	 *
	 * This also does not handle the case where its last
	 * CPU is running a higher-priority process, but every
	 * other CPU is running a lower-priority process.  There
	 * are ways to handle this situation, but they're not
	 * currently very pretty, and we also need to weigh the
	 * cost of moving a process from one CPU to another.
	 *
	 * XXXSMP
	 * There is also the issue of locking the other CPU's
	 * sched state, which we currently do not do.
	 */
#ifdef __HAVE_CPUINFO
	ci = (p->p_cpu != NULL) ? p->p_cpu : curcpu();
	if (pri < ci->ci_schedstate.spc_curpriority)
		need_resched(ci);
#else
	if (pri < curpriority)
		need_resched(0);
#endif
}

/*
 * Change process state to be runnable,
 * placing it on the run queue if it is in memory,
 * and awakening the swapper if it isn't in memory.
 */
void
setrunnable(p)
	register struct proc *p;
{
	SCHED_ASSERT_LOCKED();

	switch (p->p_stat) {
	case 0:
	case SRUN:
	case SONPROC:
	case SZOMB:
	case SDEAD:
	default:
		panic("setrunnable");
	case SSTOP:
		/*
		 * If we're being traced (possibly because someone attached us
		 * while we were stopped), check for a signal from the debugger.
		 */
		if ((p->p_flag & P_TRACED) != 0 && p->p_xstat != 0)
			p->p_siglist |= sigmask(p->p_xstat);
	case SSLEEP:
		unsleep(p);		/* e.g. when sending signals */
		break;
	case SIDL:
		break;
	}
	p->p_stat = SRUN;
	if (p->p_flag & P_INMEM)
		setrunqueue(p);
	if (p->p_slptime > 1)
		updatepri(p);
	p->p_slptime = 0;
	if ((p->p_flag & P_INMEM) == 0)
		wakeup((caddr_t)&proc0);
	else
		resched_proc(p, p->p_priority);
}

/*
 * Compute the priority of a process when running in user mode.
 * Arrange to reschedule if the resulting priority is better
 * than that of the current process.
 */
void
resetpriority(p)
	register struct proc *p;
{
	register unsigned int newpriority;

	SCHED_ASSERT_LOCKED();

	newpriority = PUSER + p->p_estcpu + NICE_WEIGHT * (p->p_nice - NZERO);
	newpriority = min(newpriority, MAXPRI);
	p->p_usrpri = newpriority;
	resched_proc(p, p->p_usrpri);
}

/*
 * We adjust the priority of the current process.  The priority of a process
 * gets worse as it accumulates CPU time.  The cpu usage estimator (p_estcpu)
 * is increased here.  The formula for computing priorities (in kern_synch.c)
 * will compute a different value each time p_estcpu increases. This can
 * cause a switch, but unless the priority crosses a PPQ boundary the actual
 * queue will not change.  The cpu usage estimator ramps up quite quickly
 * when the process is running (linearly), and decays away exponentially, at
 * a rate which is proportionally slower when the system is busy.  The basic
 * principle is that the system will 90% forget that the process used a lot
 * of CPU time in 5 * loadav seconds.  This causes the system to favor
 * processes which haven't run much recently, and to round-robin among other
 * processes.
 */

void
schedclock(p)
	struct proc *p;
{
	int s;

	p->p_estcpu = ESTCPULIM(p->p_estcpu + 1);
	SCHED_LOCK(s);
	resetpriority(p);
	SCHED_UNLOCK(s);
	if (p->p_priority >= PUSER)
		p->p_priority = p->p_usrpri;
}
@


1.59
log
@This moves access to wall and uptime variables in MI code,
encapsulating all such access into wall-defined functions
that makes sure locking is done as needed.

It also cleans up some uses of wall time vs. uptime some
places, but there is sure to be more of these needed as
well, particularily in MD code.  Also, many current calls
to microtime() should probably be changed to getmicrotime(),
or to the {,get}microuptime() versions.

ok art@@ deraadt@@ aaron@@ matthieu@@ beck@@ sturm@@ millert@@ others
"Oh, that is not your problem!" from miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.58 2004/06/21 23:50:36 tholo Exp $	*/
d354 6
a359 11
/*
 * We're only looking at 7 bits of the address; everything is
 * aligned to 4, lots of things are aligned to greater powers
 * of 2.  Shift right by 8, i.e. drop the bottom 256 worth.
 */
#define TABLESIZE	128
#define LOOKUP(x)	(((long)(x) >> 8) & (TABLESIZE - 1))
struct slpque {
	struct proc *sq_head;
	struct proc **sq_tailp;
} slpque[TABLESIZE];
d361 6
a366 10
/*
 * During autoconfiguration or after a panic, a sleep will simply
 * lower the priority briefly to allow interrupts, then return.
 * The priority to be used (safepri) is machine-dependent, thus this
 * value is initialized and maintained in the machine-dependent layers.
 * This priority will typically be 0, or the lowest priority
 * that is safe for use on the interrupt stack; it can be made
 * higher to block network software interrupts after panics.
 */
int safepri;
d369 2
a370 14
 * General sleep call.  Suspends the current process until a wakeup is
 * performed on the specified identifier.  The process will then be made
 * runnable with the specified priority.  Sleeps at most timo/hz seconds
 * (0 means no timeout).  If pri includes PCATCH flag, signals are checked
 * before and after sleeping, else signals are not checked.  Returns 0 if
 * awakened, EWOULDBLOCK if the timeout expires.  If PCATCH is set and a
 * signal needs to be delivered, ERESTART is returned if the current system
 * call should be restarted if possible, and EINTR is returned if the system
 * call should be interrupted by the signal (return EINTR).
 *
 * The interlock is held until the scheduler_slock (XXX) is held.  The
 * interlock will be locked before returning back to the caller
 * unless the PNORELOCK flag is specified, in which case the
 * interlock will always be unlocked upon return.
d372 2
a373 6
int
ltsleep(ident, priority, wmesg, timo, interlock)
	void *ident;
	int priority, timo;
	const char *wmesg;
	volatile struct simplelock *interlock;
d376 1
a376 24
	struct slpque *qp;
	int s, sig;
	int catch = priority & PCATCH;
	int relock = (priority & PNORELOCK) == 0;

	if (cold || panicstr) {
		/*
		 * After a panic, or during autoconfiguration,
		 * just give interrupts a chance, then just return;
		 * don't run any other procs or panic below,
		 * in case this is the idle process and already asleep.
		 */
		s = splhigh();
		splx(safepri);
		splx(s);
		if (interlock != NULL && relock == 0)
			simple_unlock(interlock);
		return (0);
	}

#ifdef KTRACE
	if (KTRPOINT(p, KTR_CSW))
		ktrcsw(p, 1, 0);
#endif
d379 2
a380 56

#ifdef DIAGNOSTIC
	if (ident == NULL || p->p_stat != SONPROC || p->p_back != NULL)
		panic("tsleep");
#endif

	p->p_wchan = ident;
	p->p_wmesg = wmesg;
	p->p_slptime = 0;
	p->p_priority = priority & PRIMASK;
	qp = &slpque[LOOKUP(ident)];
	if (qp->sq_head == 0)
		qp->sq_head = p;
	else
		*qp->sq_tailp = p;
	*(qp->sq_tailp = &p->p_forw) = 0;
	if (timo)
		timeout_add(&p->p_sleep_to, timo);
	/*
	 * We can now release the interlock; the scheduler_slock
	 * is held, so a thread can't get in to do wakeup() before
	 * we do the switch.
	 *
	 * XXX We leave the code block here, after inserting ourselves
	 * on the sleep queue, because we might want a more clever
	 * data structure for the sleep queues at some point.
	 */
	if (interlock != NULL)
		simple_unlock(interlock);

	/*
	 * We put ourselves on the sleep queue and start our timeout
	 * before calling CURSIG, as we could stop there, and a wakeup
	 * or a SIGCONT (or both) could occur while we were stopped.
	 * A SIGCONT would cause us to be marked as SSLEEP
	 * without resuming us, thus we must be ready for sleep
	 * when CURSIG is called.  If the wakeup happens while we're
	 * stopped, p->p_wchan will be 0 upon return from CURSIG.
	 */
	if (catch) {
		p->p_flag |= P_SINTR;
		if ((sig = CURSIG(p)) != 0) {
			if (p->p_wchan)
				unsleep(p);
			p->p_stat = SONPROC;
			SCHED_UNLOCK(s);
			goto resume;
		}
		if (p->p_wchan == 0) {
			catch = 0;
			SCHED_UNLOCK(s);
			goto resume;
		}
	} else
		sig = 0;
	p->p_stat = SSLEEP;
a381 1
	SCHED_ASSERT_LOCKED();
a382 5
#ifdef	DDB
	/* handy breakpoint location after process "wakes" */
	__asm(".globl bpendtsleep\nbpendtsleep:");
#endif

a383 4
	/*
	 * Note! this splx belongs to the SCHED_LOCK(s) above, mi_switch
	 * releases the scheduler lock, but does not lower the spl.
	 */
a384 40

resume:
#ifdef __HAVE_CPUINFO
	p->p_cpu->ci_schedstate.spc_curpriority = p->p_usrpri;
#else
	curpriority = p->p_usrpri;
#endif
	p->p_flag &= ~P_SINTR;
	if (p->p_flag & P_TIMEOUT) {
		p->p_flag &= ~P_TIMEOUT;
		if (sig == 0) {
#ifdef KTRACE
			if (KTRPOINT(p, KTR_CSW))
				ktrcsw(p, 0, 0);
#endif
			if (interlock != NULL && relock)
				simple_lock(interlock);
			return (EWOULDBLOCK);
		}
	} else if (timo)
		timeout_del(&p->p_sleep_to);
	if (catch && (sig != 0 || (sig = CURSIG(p)) != 0)) {
#ifdef KTRACE
		if (KTRPOINT(p, KTR_CSW))
			ktrcsw(p, 0, 0);
#endif
		if (interlock != NULL && relock)
			simple_lock(interlock);
		if (p->p_sigacts->ps_sigintr & sigmask(sig))
			return (EINTR);
		return (ERESTART);
	}
#ifdef KTRACE
	if (KTRPOINT(p, KTR_CSW))
		ktrcsw(p, 0, 0);
#endif

	if (interlock != NULL && relock)
		simple_lock(interlock);
	return (0);
d388 4
a391 4
 * Implement timeout for tsleep.
 * If process hasn't been awakened (wchan non-zero),
 * set timeout flag and undo the sleep.  If proc
 * is stopped, just unsleep so it will remain stopped.
d394 2
a395 2
endtsleep(arg)
	void *arg;
d397 1
a397 1
	struct proc *p;
d400 6
a405 1
	p = (struct proc *)arg;
d407 7
a413 8
	if (p->p_wchan) {
		if (p->p_stat == SSLEEP)
			setrunnable(p);
		else
			unsleep(p);
		p->p_flag |= P_TIMEOUT;
	}
	SCHED_UNLOCK(s);
d416 1
d418 1
a418 1
 * Remove a process from its wait queue
d421 1
a421 2
unsleep(p)
	register struct proc *p;
d423 11
a433 4
	register struct slpque *qp;
	register struct proc **hp;
#if 0
	int s;
d435 1
d437 3
a439 2
	 * XXX we cannot do recursive SCHED_LOCKing yet.  All callers lock
	 * anyhow.
d441 3
a443 179
	SCHED_LOCK(s);
#endif
	if (p->p_wchan) {
		hp = &(qp = &slpque[LOOKUP(p->p_wchan)])->sq_head;
		while (*hp != p)
			hp = &(*hp)->p_forw;
		*hp = p->p_forw;
		if (qp->sq_tailp == &p->p_forw)
			qp->sq_tailp = hp;
		p->p_wchan = 0;
	}
#if 0
	SCHED_UNLOCK(s);
#endif
}

#if defined(MULTIPROCESSOR) || defined(LOCKDEBUG)
void
sched_unlock_idle(void)
{
	SIMPLE_UNLOCK(&sched_lock);
}

void
sched_lock_idle(void)
{
	SIMPLE_LOCK(&sched_lock);
}
#endif /* MULTIPROCESSOR || LOCKDEBUG */

/*
 * Make all processes sleeping on the specified identifier runnable.
 */
void
wakeup_n(ident, n)
	void *ident;
	int n;
{
	struct slpque *qp;
	struct proc *p, **q;
	int s;

	SCHED_LOCK(s);
	qp = &slpque[LOOKUP(ident)];
restart:
	for (q = &qp->sq_head; (p = *q) != NULL; ) {
#ifdef DIAGNOSTIC
		if (p->p_back || (p->p_stat != SSLEEP && p->p_stat != SSTOP))
			panic("wakeup");
#endif
		if (p->p_wchan == ident) {
			--n;
			p->p_wchan = 0;
			*q = p->p_forw;
			if (qp->sq_tailp == &p->p_forw)
				qp->sq_tailp = q;
			if (p->p_stat == SSLEEP) {
				/* OPTIMIZED EXPANSION OF setrunnable(p); */
				if (p->p_slptime > 1)
					updatepri(p);
				p->p_slptime = 0;
				p->p_stat = SRUN;

				/*
				 * Since curpriority is a user priority,
				 * p->p_priority is always better than
				 * curpriority on the last CPU on
				 * which it ran.
				 *
				 * XXXSMP See affinity comment in
				 * resched_proc().
				 */
				if ((p->p_flag & P_INMEM) != 0) {
					setrunqueue(p);
#ifdef __HAVE_CPUINFO
					KASSERT(p->p_cpu != NULL);
					need_resched(p->p_cpu);
#else
					need_resched(0);
#endif
				} else {
					wakeup((caddr_t)&proc0);
				}
				/* END INLINE EXPANSION */

				if (n != 0)
					goto restart;
				else
					break;
			}
		} else
			q = &p->p_forw;
	}
	SCHED_UNLOCK(s);
}

void
wakeup(chan)
	void *chan;
{
	wakeup_n(chan, -1);
}

/*
 * General yield call.  Puts the current process back on its run queue and
 * performs a voluntary context switch.
 */
void
yield()
{
	struct proc *p = curproc;
	int s;

	SCHED_LOCK(s);
	p->p_priority = p->p_usrpri;
	setrunqueue(p);
	p->p_stats->p_ru.ru_nvcsw++;
	mi_switch();
	SCHED_ASSERT_UNLOCKED();
	splx(s);
}

/*
 * General preemption call.  Puts the current process back on its run queue
 * and performs an involuntary context switch.  If a process is supplied,
 * we switch to that process.  Otherwise, we use the normal process selection
 * criteria.
 */
void
preempt(newp)
	struct proc *newp;
{
	struct proc *p = curproc;
	int s;

	/*
	 * XXX Switching to a specific process is not supported yet.
	 */
	if (newp != NULL)
		panic("preempt: cpu_preempt not yet implemented");

	SCHED_LOCK(s);
	p->p_priority = p->p_usrpri;
	p->p_stat = SRUN;
	setrunqueue(p);
	p->p_stats->p_ru.ru_nivcsw++;
	mi_switch();
	SCHED_ASSERT_UNLOCKED();
	splx(s);
}


/*
 * Must be called at splstatclock() or higher.
 */
void
mi_switch()
{
	struct proc *p = curproc;	/* XXX */
	struct rlimit *rlim;
	struct timeval tv;
#if defined(MULTIPROCESSOR)
	int hold_count;
#endif
#ifdef __HAVE_CPUINFO
	struct schedstate_percpu *spc = &p->p_cpu->ci_schedstate;
#endif

	SCHED_ASSERT_LOCKED();

#if defined(MULTIPROCESSOR)
	/*
	 * Release the kernel_lock, as we are about to yield the CPU.
	 * The scheduler lock is still held until cpu_switch()
	 * selects a new process and removes it from the run queue.
	 */
	if (p->p_flag & P_BIGLOCK)
#ifdef notyet
		hold_count = spinlock_release_all(&kernel_lock);
d556 270
a825 8
	SIMPLE_LOCK_INIT(&sched_lock);
}

static __inline void
resched_proc(struct proc *p, u_char pri)
{
#ifdef __HAVE_CPUINFO
	struct cpu_info *ci;
d828 1
d830 2
a831 20
	 * XXXSMP
	 * Since p->p_cpu persists across a context switch,
	 * this gives us *very weak* processor affinity, in
	 * that we notify the CPU on which the process last
	 * ran that it should try to switch.
	 *
	 * This does not guarantee that the process will run on
	 * that processor next, because another processor might
	 * grab it the next time it performs a context switch.
	 *
	 * This also does not handle the case where its last
	 * CPU is running a higher-priority process, but every
	 * other CPU is running a lower-priority process.  There
	 * are ways to handle this situation, but they're not
	 * currently very pretty, and we also need to weigh the
	 * cost of moving a process from one CPU to another.
	 *
	 * XXXSMP
	 * There is also the issue of locking the other CPU's
	 * sched state, which we currently do not do.
d833 3
d837 1
a837 3
	ci = (p->p_cpu != NULL) ? p->p_cpu : curcpu();
	if (pri < ci->ci_schedstate.spc_curpriority)
		need_resched(ci);
d839 30
a868 2
	if (pri < curpriority)
		need_resched(0);
d870 4
d877 4
a880 3
 * Change process state to be runnable,
 * placing it on the run queue if it is in memory,
 * and awakening the swapper if it isn't in memory.
d883 2
a884 2
setrunnable(p)
	register struct proc *p;
d886 2
a887 1
	SCHED_ASSERT_LOCKED();
d889 8
a896 20
	switch (p->p_stat) {
	case 0:
	case SRUN:
	case SONPROC:
	case SZOMB:
	case SDEAD:
	default:
		panic("setrunnable");
	case SSTOP:
		/*
		 * If we're being traced (possibly because someone attached us
		 * while we were stopped), check for a signal from the debugger.
		 */
		if ((p->p_flag & P_TRACED) != 0 && p->p_xstat != 0)
			p->p_siglist |= sigmask(p->p_xstat);
	case SSLEEP:
		unsleep(p);		/* e.g. when sending signals */
		break;
	case SIDL:
		break;
d898 1
a898 10
	p->p_stat = SRUN;
	if (p->p_flag & P_INMEM)
		setrunqueue(p);
	if (p->p_slptime > 1)
		updatepri(p);
	p->p_slptime = 0;
	if ((p->p_flag & P_INMEM) == 0)
		wakeup((caddr_t)&proc0);
	else
		resched_proc(p, p->p_priority);
d902 1
a902 3
 * Compute the priority of a process when running in user mode.
 * Arrange to reschedule if the resulting priority is better
 * than that of the current process.
d905 1
a905 1
resetpriority(p)
d908 4
a911 1
	register unsigned int newpriority;
d913 18
a930 6
	SCHED_ASSERT_LOCKED();

	newpriority = PUSER + p->p_estcpu + NICE_WEIGHT * (p->p_nice - NZERO);
	newpriority = min(newpriority, MAXPRI);
	p->p_usrpri = newpriority;
	resched_proc(p, p->p_usrpri);
d934 1
a934 12
 * We adjust the priority of the current process.  The priority of a process
 * gets worse as it accumulates CPU time.  The cpu usage estimator (p_estcpu)
 * is increased here.  The formula for computing priorities (in kern_synch.c)
 * will compute a different value each time p_estcpu increases. This can
 * cause a switch, but unless the priority crosses a PPQ boundary the actual
 * queue will not change.  The cpu usage estimator ramps up quite quickly
 * when the process is running (linearly), and decays away exponentially, at
 * a rate which is proportionally slower when the system is busy.  The basic
 * principle is that the system will 90% forget that the process used a lot
 * of CPU time in 5 * loadav seconds.  This causes the system to favor
 * processes which haven't run much recently, and to round-robin among other
 * processes.
a935 1

d937 3
a939 2
schedclock(p)
	struct proc *p;
d941 2
a944 1
	p->p_estcpu = ESTCPULIM(p->p_estcpu + 1);
d946 50
a995 1
	resetpriority(p);
a996 2
	if (p->p_priority >= PUSER)
		p->p_priority = p->p_usrpri;
a998 6
#ifdef DDB
#include <machine/db_machdep.h>

#include <ddb/db_interface.h>
#include <ddb/db_output.h>

d1000 2
a1001 5
db_show_all_procs(addr, haddr, count, modif)
	db_expr_t addr;
	int haddr;
	db_expr_t count;
	char *modif;
d1003 1
a1003 74
	char *mode;
	int doingzomb = 0;
	struct proc *p, *pp;
    
	if (modif[0] == 0)
		modif[0] = 'n';			/* default == normal mode */

	mode = "mawn";
	while (*mode && *mode != modif[0])
		mode++;
	if (*mode == 0 || *mode == 'm') {
		db_printf("usage: show all procs [/a] [/n] [/w]\n");
		db_printf("\t/a == show process address info\n");
		db_printf("\t/n == show normal process info [default]\n");
		db_printf("\t/w == show process wait/emul info\n");
		return;
	}
	
	p = LIST_FIRST(&allproc);

	switch (*mode) {

	case 'a':
		db_printf("   PID  %-10s  %18s  %18s  %18s\n",
		    "COMMAND", "STRUCT PROC *", "UAREA *", "VMSPACE/VM_MAP");
		break;
	case 'n':
		db_printf("   PID  %5s  %5s  %5s  S  %10s  %-9s  %-16s\n",
		    "PPID", "PGRP", "UID", "FLAGS", "WAIT", "COMMAND");
		break;
	case 'w':
		db_printf("   PID  %-16s  %-8s  %18s  %s\n",
		    "COMMAND", "EMUL", "WAIT-CHANNEL", "WAIT-MSG");
		break;
	}

	while (p != 0) {
		pp = p->p_pptr;
		if (p->p_stat) {

			db_printf("%c%5d  ", p == curproc ? '*' : ' ',
				p->p_pid);

			switch (*mode) {

			case 'a':
				db_printf("%-10.10s  %18p  %18p  %18p\n",
				    p->p_comm, p, p->p_addr, p->p_vmspace);
				break;

			case 'n':
				db_printf("%5d  %5d  %5d  %d  %#10x  "
				    "%-9.9s  %-16s\n",
				    pp ? pp->p_pid : -1, p->p_pgrp->pg_id,
				    p->p_cred->p_ruid, p->p_stat, p->p_flag,
				    (p->p_wchan && p->p_wmesg) ?
					p->p_wmesg : "", p->p_comm);
				break;

			case 'w':
				db_printf("%-16s  %-8s  %18p  %s\n", p->p_comm,
				    p->p_emul->e_name, p->p_wchan,
				    (p->p_wchan && p->p_wmesg) ? 
					p->p_wmesg : "");
				break;

			}
		}
		p = LIST_NEXT(p, p_list);
		if (p == 0 && doingzomb == 0) {
			doingzomb = 1;
			p = LIST_FIRST(&zombproc);
		}
	}
a1004 1
#endif
@


1.58
log
@First step towards more sane time handling in the kernel -- this changes
things such that code that only need a second-resolution uptime or wall
time, and used to get that from time.tv_secs or mono_time.tv_secs now get
this from separate time_t globals time_second and time_uptime.

ok art@@ niklas@@ nordin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.57 2004/06/20 03:00:16 art Exp $	*/
d767 1
a767 1
	 * XXX -- comparing wall time with uptime here!
d769 1
a769 1
	microtime(&tv);
d773 1
a773 1
		printf("time is not monotonic! "
d785 1
a785 1
		printf("time is not monotonic! "
d840 1
a840 1
	microtime(&p->p_cpu->ci_schedstate.spc_runtime);
d842 1
a842 1
	microtime(&runtime);
@


1.57
log
@Merge error in smp merge. It's a miracle that people haven't noticed the
scheduling errors on non-i386 yet.

deraadt@@ aaron@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.56 2004/06/13 21:49:26 niklas Exp $	*/
d767 1
@


1.56
log
@debranch SMP, have fun
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d97 1
a97 1
	timeout_set(&roundrobin_to, schedcpu, &roundrobin_to);
@


1.55
log
@Merge in a piece of the SMP branch into HEAD.

Introduce the cpu_info structure, p_cpu field in struct proc and global
scheduling context and various changed code to deal with this. At the
moment no architecture uses this stuff yet, but it will allow us slow and
controlled migration to the new APIs.

All new code is ifdef:ed out.

ok deraadt@@ niklas@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.54 2004/01/26 01:27:02 deraadt Exp $	*/
d62 1
a62 1
int	rrticks_init;		/* # of harclock ticks per roundrobin */
d68 2
d72 3
a74 1
#ifndef __HAVE_CPUINFO
d92 1
a92 1
	 * We setup the timeouts here and kick roundrobin and schedcpu once to
d96 3
a98 1
	timeout_set(&roundrobin_to, roundrobin, &roundrobin_to);
d112 1
d132 1
a132 1
			 spc->spc_schedflags |= SPCF_SEENRR;
a139 1
/* ARGSUSED */
d161 2
a162 1
	need_resched();
d308 2
d322 1
a322 1
		splx(s);
d341 2
a405 5
#ifdef KTRACE
	if (KTRPOINT(p, KTR_CSW))
		ktrcsw(p, 1, 0);
#endif
	s = splhigh();
d413 1
d420 8
d429 1
a429 1
	if (ident == NULL || p->p_stat != SRUN || p->p_back)
d432 1
d471 2
a472 1
			p->p_stat = SRUN;
d477 1
d484 1
d490 8
a503 1
	splx(s);
d533 1
d553 1
a553 1
	s = splhigh();
d561 1
a561 69
	splx(s);
}

/*
 * Short-term, non-interruptable sleep.
 */
void
sleep(ident, priority)
	void *ident;
	int priority;
{
	register struct proc *p = curproc;
	register struct slpque *qp;
	register int s;

#ifdef DIAGNOSTIC
	if (priority > PZERO) {
		printf("sleep called with priority %d > PZERO, wchan: %p\n",
		    priority, ident);
		panic("old sleep");
	}
#endif
	s = splhigh();
	if (cold || panicstr) {
		/*
		 * After a panic, or during autoconfiguration,
		 * just give interrupts a chance, then just return;
		 * don't run any other procs or panic below,
		 * in case this is the idle process and already asleep.
		 */
		splx(safepri);
		splx(s);
		return;
	}
#ifdef DIAGNOSTIC
	if (ident == NULL || p->p_stat != SRUN || p->p_back)
		panic("sleep");
#endif
	p->p_wchan = ident;
	p->p_wmesg = NULL;
	p->p_slptime = 0;
	p->p_priority = priority;
	qp = &slpque[LOOKUP(ident)];
	if (qp->sq_head == 0)
		qp->sq_head = p;
	else
		*qp->sq_tailp = p;
	*(qp->sq_tailp = &p->p_forw) = 0;
	p->p_stat = SSLEEP;
	p->p_stats->p_ru.ru_nvcsw++;
#ifdef KTRACE
	if (KTRPOINT(p, KTR_CSW))
		ktrcsw(p, 1, 0);
#endif
	mi_switch();
#ifdef	DDB
	/* handy breakpoint location after process "wakes" */
	__asm(".globl bpendsleep\nbpendsleep:");
#endif
#ifdef KTRACE
	if (KTRPOINT(p, KTR_CSW))
		ktrcsw(p, 0, 0);
#endif
#ifdef __HAVE_CPUINFO
	p->p_cpu->ci_schedstate.spc_curpriority = p->p_usrpri;
#else
	curpriority = p->p_usrpri;
#endif
	splx(s);
d573 1
d576 6
a581 1
	s = splhigh();
d591 16
a606 1
	splx(s);
d608 1
d622 1
a622 1
	s = splhigh();
d646 5
a650 1
				 * curpriority.
a651 1

d655 1
d658 1
a658 1
					need_resched();
d673 1
a673 1
	splx(s);
d693 1
a693 1
	s = splstatclock();
d698 1
d721 1
a721 1
	s = splstatclock();
d723 1
d727 1
d741 3
d748 15
a762 1
	splassert(IPL_STATCLOCK);
d773 1
a773 1
		    "tv=%ld.%06ld, runtime=%ld.%06ld\n",
d778 1
a778 1
		timersub(&tv, &spc->runtime, &tv);
d785 1
a785 1
		    "tv=%ld.%06ld, runtime=%ld.%06ld\n",
d825 11
d837 2
a838 1
	/* p->p_cpu might have changed in cpu_switch() */
d843 14
d870 1
d880 22
d908 1
a908 1
		need_resched();
d921 1
a921 1
	register int s;
a922 1
	s = splhigh();
d926 1
a946 1
	splx(s);
d967 2
d994 2
d997 1
d999 1
@


1.54
log
@having the monotonic thing as DEBUG is not going to get it fixed faster, it is just going to annoy people
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.53 2003/12/23 09:37:57 deraadt Exp $	*/
d57 1
d59 1
d61 3
d70 1
d72 1
d80 1
d82 1
d95 3
d99 1
d106 27
d135 1
a135 2
roundrobin(arg)
	void *arg;
d158 1
d472 3
d476 1
d597 3
d601 1
d670 3
d674 1
d753 3
d764 13
d787 1
d808 3
d812 1
d819 5
d825 1
d841 17
d899 2
a900 2
	else if (p->p_priority < curpriority)
		need_resched();
d917 1
a917 2
	if (newpriority < curpriority)
		need_resched();
@


1.53
log
@enough is enough, driving people insane is not nice
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.52 2003/12/23 00:15:14 mickey Exp $	*/
d710 1
a710 1
#ifdef DEBUG
@


1.52
log
@print tv_usec fields correctly in reporting conmonotonic time
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.51 2003/12/19 04:51:44 millert Exp $	*/
d710 1
a710 1
#ifdef DIAGNOSTIC
@


1.51
log
@Add a check for time not flowing monotonically and just don't change
p->p_rtime in this case instead of zeroing it; based on an idea
from nordin@@.  Also add a printf about microtime() not being monotonic
for this case (from miod@@) #ifdef DIAGNOSTIC.  This version OK otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.50 2003/12/15 22:03:41 millert Exp $	*/
d711 2
a712 1
		printf("time is not monotonic! tv=%ld.%ld, runtime=%ld.%ld\n",
@


1.50
log
@Fix some sign issues that fell out from the change of rlim_t to unsigned.
Also add a check for a negative result when subtracting microtime(&now)
from runtime and simply treat this as zero.  This should *not* happen
but due to an apparent bug in microtime on dual clock machines, it does.
The microtime bug is currently being examined.
Based on a diff from miod@@ with help from otto@@; ok deraadt@@ otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.49 2003/12/15 09:00:55 deraadt Exp $	*/
d709 9
a717 4
	timersub(&tv, &runtime, &tv);
	timeradd(&p->p_rtime, &tv, &p->p_rtime);
	if (p->p_rtime.tv_sec < 0)
		p->p_rtime.tv_sec = p->p_rtime.tv_usec = 0;
@


1.49
log
@workaround a clock tick handling bug that the rlimit code just exposed.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.48 2003/06/02 23:28:06 millert Exp $	*/
a699 1
	long s, u;
a700 1
	rlim_t rs;
d709 4
a712 11
	u = p->p_rtime.tv_usec + (tv.tv_usec - runtime.tv_usec);
	s = p->p_rtime.tv_sec + (tv.tv_sec - runtime.tv_sec);
	if (u < 0) {
		u += 1000000;
		s--;
	} else if (u >= 1000000) {
		u -= 1000000;
		s++;
	}
	p->p_rtime.tv_usec = u;
	p->p_rtime.tv_sec = s;
d719 2
a720 3
	rs = (rlim_t)(unsigned)s;
	if (rs >= rlim->rlim_cur) {
		if (rs >= rlim->rlim_max) {
@


1.48
log
@Remove the advertising clause in the UCB license which Berkeley
rescinded 22 July 1999.  Proofed by myself and Theo.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.47 2003/03/15 00:08:41 deraadt Exp $	*/
d702 1
d728 3
a730 2
	if (s >= rlim->rlim_cur) {
		if (s >= rlim->rlim_max)
d732 1
a732 1
		else {
@


1.47
log
@kill 10 minute non-root suffers stuff.  noted that we still have this, by
matthieu, who noted it now that X is not running as root.  ok nordin
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.46 2002/10/15 20:17:22 art Exp $	*/
d21 1
a21 5
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.46
log
@Protect p_priority with splstatclock.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.45 2002/07/24 17:58:49 mickey Exp $	*/
d728 1
a728 2
	 * If over max, kill it.  In any case, if it has run for more
	 * than 10 minutes, reduce priority to give others a chance.
a739 5
	if (s > 10 * 60 && p->p_ucred->cr_uid && p->p_nice == NZERO) {
		p->p_nice = NZERO + 4;
		resetpriority(p);
	}

@


1.45
log
@fix header printing in show_all_procs
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.44 2002/07/03 21:19:08 miod Exp $	*/
d660 1
a661 1
	s = splstatclock();
d687 1
a688 1
	s = splstatclock();
@


1.44
log
@Change all variables definitions (int foo) in sys/sys/*.h to variable
declarations (extern int foo), and compensate in the appropriate locations.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.43 2002/06/11 05:04:34 art Exp $	*/
d898 1
a898 1
		db_printf("  PID  %-10s  %18s  %18s  %18s\n",
d902 1
a902 1
		db_printf("  PID  %5s  %5s  %5s  S  %10s  %-9s  %-16s\n",
d906 1
a906 1
		db_printf("  PID  %-16s  %-8s  %18s  %s\n",
@


1.43
log
@splassert(IPL_STATCLOCK) mi_switch
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.42 2002/03/14 01:27:04 millert Exp $	*/
d63 3
@


1.42
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.41 2002/03/08 07:25:29 mickey Exp $	*/
d699 3
a701 3
	register struct proc *p = curproc;	/* XXX */
	register struct rlimit *rlim;
	register long s, u;
d703 2
@


1.41
log
@semicolon is not always what it seems, replace w/ a \n in asm labels
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.40 2001/11/11 22:30:56 art Exp $	*/
d64 1
a64 1
void scheduler_start __P((void));
d66 4
a69 4
void roundrobin __P((void *));
void schedcpu __P((void *));
void updatepri __P((struct proc *));
void endtsleep __P((void *));
@


1.40
log
@Let ltsleep take a const wmesg.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.39 2001/11/06 19:53:20 miod Exp $	*/
d430 1
a430 1
	__asm(".globl bpendtsleep ; bpendtsleep:");
d548 1
a548 1
	__asm(".globl bpendsleep ; bpendsleep:");
@


1.40.2.1
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.40 2001/11/11 22:30:56 art Exp $	*/
d64 1
a64 1
void scheduler_start(void);
d66 4
a69 4
void roundrobin(void *);
void schedcpu(void *);
void updatepri(struct proc *);
void endtsleep(void *);
d430 1
a430 1
	__asm(".globl bpendtsleep\nbpendtsleep:");
d548 1
a548 1
	__asm(".globl bpendsleep\nbpendsleep:");
@


1.40.2.2
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.40.2.1 2002/06/11 03:29:40 art Exp $	*/
a63 3
int whichqs;			/* Bit mask summary of non-empty Q's. */
struct prochd qs[NQS];

d657 1
a658 1
	p->p_priority = p->p_usrpri;
d684 1
a685 1
	p->p_priority = p->p_usrpri;
d699 3
a701 3
	struct proc *p = curproc;	/* XXX */
	struct rlimit *rlim;
	long s, u;
a703 2
	splassert(IPL_STATCLOCK);

d893 1
a893 1
		db_printf("   PID  %-10s  %18s  %18s  %18s\n",
d897 1
a897 1
		db_printf("   PID  %5s  %5s  %5s  S  %10s  %-9s  %-16s\n",
d901 1
a901 1
		db_printf("   PID  %-16s  %-8s  %18s  %s\n",
@


1.40.2.3
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d728 2
a729 1
	 * If over max, kill it.
d741 5
@


1.39
log
@Replace inclusion of <vm/foo.h> with the correct <uvm/bar.h> when necessary.
(Look ma, I might have broken the tree)
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.38 2001/09/13 14:41:50 art Exp $	*/
d347 1
a347 1
	char *wmesg;
@


1.38
log
@Remove a comment that just doesn't make any sense.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.37 2001/08/07 22:57:15 art Exp $	*/
d51 1
a51 1
#include <vm/vm.h>
a53 2

#include <uvm/uvm_extern.h>
@


1.37
log
@Change tsleep into ltsleep.
ltsleep takes an additional argument - a simplelock and unlocks it when it's
safe to do so.

tsleep now becomes a wrapper around ltsleep.

From NetBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.36 2001/06/27 04:49:45 art Exp $	*/
a695 1
 * The machine independent parts of mi_switch().
@


1.36
log
@remove old vm
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.35 2001/06/24 20:55:16 mickey Exp $	*/
d339 5
d346 1
a346 1
tsleep(ident, priority, wmesg, timo)
d350 1
d352 5
a356 4
	register struct proc *p = curproc;
	register struct slpque *qp;
	register int s;
	int sig, catch = priority & PCATCH;
d372 2
d393 12
d445 2
d456 2
d466 2
@


1.35
log
@cold is in systm.h now
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.34 2001/05/26 04:08:57 art Exp $	*/
a54 1
#if defined(UVM)
a55 1
#endif
a277 1
#if defined(UVM)
a278 3
#else
	vmmeter();
#endif
a726 1
#if defined(UVM)
a727 3
#else
	cnt.v_swtch++;
#endif
@


1.34
log
@indentation.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.33 2001/03/25 18:09:17 csapuntz Exp $	*/
a355 1
	extern int cold;
a485 1
	extern int cold;
@


1.33
log
@

Reintroduce wakeup call
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.32 2001/03/15 21:18:30 art Exp $	*/
a785 1

@


1.32
log
@Print a '*' in front of curproc in ps in ddb.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.31 2001/02/27 09:07:53 csapuntz Exp $	*/
d621 7
@


1.31
log
@

Add wakeup_n and wakeup_one. wakeup_n will wakeup up to n sleeping processes
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.30 2001/02/19 16:33:20 art Exp $	*/
d892 2
a893 1
			db_printf("%5d  ", p->p_pid);
@


1.30
log
@When doing an assertion for phz, just do it once when we set phz,
not once per process.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.29 2000/11/10 18:15:47 art Exp $	*/
d569 1
a569 1
wakeup(ident)
d571 1
d586 1
d612 4
a615 1
				goto restart;
@


1.29
log
@Change the ktrace interface functions from taking the trace vnode to taking the
traced proc. The vnode is in the proc and all functions need the proc.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.28 2000/08/03 14:36:36 mickey Exp $	*/
d232 1
a253 1
		KASSERT(phz);
@


1.28
log
@s/principal/priciple/; from netbsd
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.27 2000/07/06 07:00:04 art Exp $	*/
d360 1
a360 1
		ktrcsw(p->p_tracep, 1, 0);
d429 1
a429 1
				ktrcsw(p->p_tracep, 0, 0);
d438 1
a438 1
			ktrcsw(p->p_tracep, 0, 0);
d446 1
a446 1
		ktrcsw(p->p_tracep, 0, 0);
d526 1
a526 1
		ktrcsw(p->p_tracep, 1, 0);
d535 1
a535 1
		ktrcsw(p->p_tracep, 0, 0);
@


1.27
log
@Typo in comment and some cleanup of roundrobin.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.26 2000/06/27 18:13:23 art Exp $	*/
d818 1
a818 1
 * principal is that the system will 90% forget that the process used a lot
@


1.26
log
@Slight optimization of wakeup.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.25 2000/06/05 11:02:51 art Exp $	*/
d84 1
a84 1
	 * We setup the timeouts here and kick rundrobin and schedcpu once to
d104 1
d107 1
a107 1
	if (curproc != NULL) {
d109 1
a109 1
		if (curproc->p_schedflags & PSCHED_SEENRR) {
d115 1
a115 1
			curproc->p_schedflags |= PSCHED_SHOULDYIELD;
d117 1
a117 1
			curproc->p_schedflags |= PSCHED_SEENRR;
@


1.25
log
@Changes to exit handling.

cpu_exit no longer frees the vmspace and u-area. This is now handled by a
separate kernel thread "reaper". This is to avoid sleeping locks in the
critical path of cpu_exit where we're not allowed to sleep.

From NetBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.24 2000/04/19 09:58:20 art Exp $	*/
d569 1
a569 1
	register void *ident;
d571 2
a572 2
	register struct slpque *qp;
	register struct proc *p, **q;
d594 1
a594 2
				if (p->p_flag & P_INMEM)
					setrunqueue(p);
d600 5
a604 1
				if ((p->p_flag & P_INMEM) == 0)
d606 1
a606 2
				else
					need_resched();
d608 1
@


1.24
log
@Remove the roundrobin_attempts hack and replace it with per-process scheduling
flags (much nicer for future smp work).
Add two generic functions yield() and preempt(). Use preepmt() in uio when
we are told to yield.
Based on my idea, code written by Jason Thorpe from NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.23 2000/03/23 16:54:43 art Exp $	*/
d757 1
@


1.23
log
@Don't reinitialize the tsleep and ITIMER_REAL timers all the time.
The function and the argument never change.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.22 2000/03/23 14:44:37 art Exp $	*/
a95 8
 * We need to keep track on how many times we call roundrobin before we
 * actually attempt a switch (that is when we call mi_switch()).
 * This is done so that some slow kernel subsystems can yield instead of
 * blocking the scheduling.
 */
int	roundrobin_attempts;

/*
d104 1
d106 14
a120 1
	roundrobin_attempts++;
d615 46
d709 7
a725 7

	/*
	 * We reset roundrobin_attempts at exit, because cpu_switch could
	 * have looped in the idle loop and the attempts would increase
	 * leading to unjust punishment of an innocent process.
	 */
	roundrobin_attempts = 0;
@


1.22
log
@use the new timeout interface for tsleep.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.21 2000/03/23 10:13:58 art Exp $	*/
d381 1
a381 2
	if (timo) {
		timeout_set(&p->p_sleep_to, endtsleep, p);
a382 1
	}
@


1.21
log
@Adapt roundrobin and schedcpu to the new timeout API.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.20 2000/03/03 16:49:25 art Exp $	*/
d381 4
a384 2
	if (timo)
		timeout(endtsleep, (void *)p, timo);
d429 1
a429 1
		untimeout(endtsleep, (void *)p);
d456 1
a456 1
	register struct proc *p;
@


1.20
log
@Keep track of the number of times we trigger a reschedule before the
context switch actually happens.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.19 2000/03/03 11:46:09 art Exp $	*/
d53 1
d68 27
a102 5
void roundrobin __P((void *));
void schedcpu __P((void *));
void updatepri __P((struct proc *));
void endtsleep __P((void *));

d111 1
d115 1
a115 1
	timeout(roundrobin, NULL, hz / 10);
d211 5
a215 4
	register fixpt_t loadfac = loadfactor(averunnable.ldavg[0]);
	register struct proc *p;
	register int s;
	register unsigned int newcpu;
d279 1
a279 1
	timeout(schedcpu, (void *)0, hz);
@


1.19
log
@Use the LIST_FIRST macro to get the head of zombproc list.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.18 2000/03/03 11:31:43 art Exp $	*/
d67 8
d90 1
d642 7
@


1.18
log
@Use LIST_ macros instead of internal field names to walk the allproc list.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.17 1999/09/05 22:20:45 tholo Exp $	*/
d819 1
a819 1
			p = zombproc.lh_first;
@


1.17
log
@Use stathz to calculate CPU time when available; fixes CPU calculation
problems when stathz runs at different speed than hz/profhz.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.16 1999/08/15 00:07:44 pjanzen Exp $	*/
d192 1
a192 1
	for (p = allproc.lh_first; p != 0; p = p->p_list.le_next) {
d767 1
a767 1
	p = allproc.lh_first;
d816 1
a816 1
		p = p->p_list.le_next;
@


1.17.4.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a52 1
#include <sys/timeout.h>
a66 2
void scheduler_start __P((void));

a71 28
void
scheduler_start()
{
	static struct timeout roundrobin_to;
	static struct timeout schedcpu_to;

	/*
	 * We avoid polluting the global namespace by keeping the scheduler
	 * timeouts static in this function.
	 * We setup the timeouts here and kick rundrobin and schedcpu once to
	 * make them do their job.
	 */

	timeout_set(&roundrobin_to, roundrobin, &roundrobin_to);
	timeout_set(&schedcpu_to, schedcpu, &schedcpu_to);

	roundrobin(&roundrobin_to);
	schedcpu(&schedcpu_to);
}

/*
 * We need to keep track on how many times we call roundrobin before we
 * actually attempt a switch (that is when we call mi_switch()).
 * This is done so that some slow kernel subsystems can yield instead of
 * blocking the scheduling.
 */
int	roundrobin_attempts;

a79 1
	struct timeout *to = (struct timeout *)arg;
d82 1
a82 2
	roundrobin_attempts++;
	timeout_add(to, hz / 10);
d178 4
a181 5
	struct timeout *to = (struct timeout *)arg;
	fixpt_t loadfac = loadfactor(averunnable.ldavg[0]);
	struct proc *p;
	int s;
	unsigned int newcpu;
d192 1
a192 1
	for (p = LIST_FIRST(&allproc); p != 0; p = LIST_NEXT(p, p_list)) {
d245 1
a245 1
	timeout_add(to, hz);
d348 1
a348 1
		timeout_add(&p->p_sleep_to, timo);
d393 1
a393 1
		timeout_del(&p->p_sleep_to);
d420 1
a420 1
	struct proc *p;
a632 7

	/*
	 * We reset roundrobin_attempts at exit, because cpu_switch could
	 * have looped in the idle loop and the attempts would increase
	 * leading to unjust punishment of an innocent process.
	 */
	roundrobin_attempts = 0;
d767 1
a767 1
	p = LIST_FIRST(&allproc);
d816 1
a816 1
		p = LIST_NEXT(p, p_list);
d819 1
a819 1
			p = LIST_FIRST(&zombproc);
@


1.17.4.2
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.33 2001/03/25 18:09:17 csapuntz Exp $	*/
d84 1
a84 1
	 * We setup the timeouts here and kick roundrobin and schedcpu once to
d96 8
a111 2
	struct proc *p = curproc;
	int s;
a112 14
	if (p != NULL) {
		s = splstatclock();
		if (p->p_schedflags & PSCHED_SEENRR) {
			/*
			 * The process has already been through a roundrobin
			 * without switching and may be hogging the CPU.
			 * Indicate that the process should yield.
			 */
			p->p_schedflags |= PSCHED_SHOULDYIELD;
		} else {
			p->p_schedflags |= PSCHED_SEENRR;
		}
		splx(s);
	}
d114 1
a224 1
	KASSERT(phz);
d246 1
d353 1
a353 1
		ktrcsw(p, 1, 0);
d422 1
a422 1
				ktrcsw(p, 0, 0);
d431 1
a431 1
			ktrcsw(p, 0, 0);
d439 1
a439 1
		ktrcsw(p, 0, 0);
d519 1
a519 1
		ktrcsw(p, 1, 0);
d528 1
a528 1
		ktrcsw(p, 0, 0);
d562 2
a563 3
wakeup_n(ident, n)
	void *ident;
	int n;
d565 2
a566 2
	struct slpque *qp;
	struct proc *p, **q;
a577 1
			--n;
d588 2
a589 1

d595 3
a597 3

				if ((p->p_flag & P_INMEM) != 0) {
					setrunqueue(p);
a598 3
				} else {
					wakeup((caddr_t)&proc0);
				}
d600 1
a600 5

				if (n != 0)
					goto restart;
				else
					break;
a607 53
void
wakeup(chan)
	void *chan;
{
	wakeup_n(chan, -1);
}

/*
 * General yield call.  Puts the current process back on its run queue and
 * performs a voluntary context switch.
 */
void
yield()
{
	struct proc *p = curproc;
	int s;

	p->p_priority = p->p_usrpri;
	s = splstatclock();
	setrunqueue(p);
	p->p_stats->p_ru.ru_nvcsw++;
	mi_switch();
	splx(s);
}

/*
 * General preemption call.  Puts the current process back on its run queue
 * and performs an involuntary context switch.  If a process is supplied,
 * we switch to that process.  Otherwise, we use the normal process selection
 * criteria.
 */
void
preempt(newp)
	struct proc *newp;
{
	struct proc *p = curproc;
	int s;

	/*
	 * XXX Switching to a specific process is not supported yet.
	 */
	if (newp != NULL)
		panic("preempt: cpu_preempt not yet implemented");

	p->p_priority = p->p_usrpri;
	s = splstatclock();
	setrunqueue(p);
	p->p_stats->p_ru.ru_nivcsw++;
	mi_switch();
	splx(s);
}


a656 7

	/*
	 * Process is about to yield the CPU; clear the appropriate
	 * scheduling flags.
	 */
	p->p_schedflags &= ~PSCHED_SWITCHCLEAR;

d667 7
a704 1
	case SDEAD:
d761 1
a761 1
 * principle is that the system will 90% forget that the process used a lot
d830 1
a830 2
			db_printf("%c%5d  ", p == curproc ? '*' : ' ',
				p->p_pid);
@


1.17.4.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.17.4.2 2001/05/14 22:32:41 niklas Exp $	*/
d55 1
d57 1
d280 1
d282 3
d356 1
d487 1
d735 1
d737 3
d786 1
@


1.17.4.4
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.17.4.3 2001/07/04 10:48:27 niklas Exp $	*/
a338 5
 *
 * The interlock is held until the scheduler_slock (XXX) is held.  The
 * interlock will be locked before returning back to the caller
 * unless the PNORELOCK flag is specified, in which case the
 * interlock will always be unlocked upon return.
d341 1
a341 1
ltsleep(ident, priority, wmesg, timo, interlock)
a344 1
	volatile struct simplelock *interlock;
d346 4
a349 5
	struct proc *p = curproc;
	struct slpque *qp;
	int s, sig;
	int catch = priority & PCATCH;
	int relock = (priority & PNORELOCK) == 0;
a364 2
		if (interlock != NULL && relock == 0)
			simple_unlock(interlock);
a383 12
	 * We can now release the interlock; the scheduler_slock
	 * is held, so a thread can't get in to do wakeup() before
	 * we do the switch.
	 *
	 * XXX We leave the code block here, after inserting ourselves
	 * on the sleep queue, because we might want a more clever
	 * data structure for the sleep queues at some point.
	 */
	if (interlock != NULL)
		simple_unlock(interlock);

	/*
a423 2
			if (interlock != NULL && relock)
				simple_lock(interlock);
a432 2
		if (interlock != NULL && relock)
			simple_lock(interlock);
a440 2
	if (interlock != NULL && relock)
		simple_lock(interlock);
d669 1
@


1.17.4.5
log
@merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d51 1
a51 1
#include <uvm/uvm_extern.h>
d55 2
d349 1
a349 1
	const char *wmesg;
@


1.17.4.6
log
@Merge in -current from about a week ago
@
text
@d64 1
a64 1
void scheduler_start(void);
d66 4
a69 4
void roundrobin(void *);
void schedcpu(void *);
void updatepri(struct proc *);
void endtsleep(void *);
d430 1
a430 1
	__asm(".globl bpendtsleep\nbpendtsleep:");
d548 1
a548 1
	__asm(".globl bpendsleep\nbpendsleep:");
@


1.17.4.7
log
@Sync the SMP branch with 3.3
@
text
@a63 3
int whichqs;			/* Bit mask summary of non-empty Q's. */
struct prochd qs[NQS];

d657 1
a658 1
	p->p_priority = p->p_usrpri;
d684 1
a685 1
	p->p_priority = p->p_usrpri;
d699 3
a701 3
	struct proc *p = curproc;	/* XXX */
	struct rlimit *rlim;
	long s, u;
a703 2
	splassert(IPL_STATCLOCK);

d723 2
a724 1
	 * If over max, kill it.
d736 5
d893 1
a893 1
		db_printf("   PID  %-10s  %18s  %18s  %18s\n",
d897 1
a897 1
		db_printf("   PID  %5s  %5s  %5s  S  %10s  %-9s  %-16s\n",
d901 1
a901 1
		db_printf("   PID  %-16s  %-8s  %18s  %s\n",
@


1.17.4.8
log
@Biglock!  Most of the logic
comes from NetBSD.
Also a lot of fixes, enough to get a dual cpu machine actually run MP for a
very short while (we are just talking about seconds) before starving out one
of the cpus.  More coming very soon.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.17.4.7 2003/03/28 00:41:26 niklas Exp $	*/
a66 2
struct simplelock sched_lock;

a264 2
		splx(s);
		SCHED_LOCK(s);
d277 1
a277 1
		SCHED_UNLOCK(s);
a295 2
	SCHED_ASSERT_LOCKED();

d359 5
a370 1
		s = splhigh();
a376 8

#ifdef KTRACE
	if (KTRPOINT(p, KTR_CSW))
		ktrcsw(p, 1, 0);
#endif

	SCHED_LOCK(s);

d378 1
a378 1
	if (ident == NULL || p->p_stat != SONPROC || p->p_back != NULL)
a380 1

d419 1
a419 2
			p->p_stat = SONPROC;
			SCHED_UNLOCK(s);
a423 1
			SCHED_UNLOCK(s);
a429 1
	SCHED_ASSERT_LOCKED();
a434 4

	SCHED_ASSERT_UNLOCKED();
	splx(s);

d437 1
a466 1

d486 1
a486 1
	SCHED_LOCK(s);
d494 1
a494 1
	SCHED_UNLOCK(s);
a508 2
	SCHED_ASSERT_LOCKED();

a569 1
#if 0
d572 1
a572 6
	/*
	 * XXX we cannot do recursive SCHED_LOCKing yet.  All callers lock
	 * anyhow.
	 */
	SCHED_LOCK(s);
#endif
d582 1
a582 11
#if 0
	SCHED_UNLOCK(s);
#endif
}

#if defined(MULTIPROCESSOR) || defined(LOCKDEBUG)
void
sched_unlock_idle(void)
{

	simple_unlock(&sched_lock);
a584 8
void
sched_lock_idle(void)
{

	simple_lock(&sched_lock);
}
#endif /* MULTIPROCESSOR || LOCKDEBUG */

d597 1
a597 1
	SCHED_LOCK(s);
d640 1
a640 1
	SCHED_UNLOCK(s);
d660 1
a660 1
	SCHED_LOCK(s);
a664 1
	SCHED_ASSERT_UNLOCKED();
d687 1
a687 1
	SCHED_LOCK(s);
a688 1
	p->p_stat = SRUN;
a691 1
	SCHED_ASSERT_UNLOCKED();
a705 5
#ifdef notyet
#if defined(MULTIPROCESSOR)
	int hold_count;
#endif
#endif
d707 1
a707 15
	SCHED_ASSERT_LOCKED();

#if defined(MULTIPROCESSOR)
	/*
	 * Release the kernel_lock, as we are about to yield the CPU.
	 * The scheduler lock is still held until cpu_switch()
	 * selects a new process and removes it from the run queue.
	 */
	if (p->p_flag & P_BIGLOCK)
#ifdef notyet
		hold_count = spinlock_release_all(&kernel_lock);
#else
		KERNEL_UNLOCK();
#endif
#endif
a752 20

	/*
	 * Make sure that MD code released the scheduler lock before
	 * resuming us.
	 */
	SCHED_ASSERT_UNLOCKED();

#if defined(MULTIPROCESSOR)
	/*
	 * Reacquire the kernel_lock now.  We do this after we've
	 * released the scheduler lock to avoid deadlock, and before
	 * we reacquire the interlock.
	 */
	if (p->p_flag & P_BIGLOCK)
#ifdef notyet
		spinlock_acquire_count(&kernel_lock, hold_count);
#else
		KERNEL_LOCK(LK_CANRECURSE|LK_EXCLUSIVE);
#endif
#endif
a765 1
	simple_lock_init(&sched_lock);
d777 1
a777 1
	SCHED_ASSERT_LOCKED();
d779 1
a782 1
	case SONPROC:
d803 1
a823 2
	SCHED_ASSERT_LOCKED();

a849 2
	int s;

a850 1
	SCHED_LOCK(s);
a851 1
	SCHED_UNLOCK(s);
@


1.17.4.9
log
@merge netbsd lockmgr better, makes us halfway through rc
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.17.4.8 2003/05/15 04:08:02 niklas Exp $	*/
d753 1
d757 1
d768 1
d770 3
d833 1
d835 3
@


1.17.4.10
log
@Go back to defining simplelocks as noops, even if MULTIPROCESSOR.  Instead use
a new real simple recursive-lock capable lock implementation for the few
necessary locks (kernel, scheduler, tlb shootdown, printf and ddb MP).
This because we cannot trust the old fine-grained locks spread out all over
our kernel, and not really tested.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.17.4.9 2003/05/15 16:45:54 niklas Exp $	*/
d67 1
a67 1
struct SIMPLELOCK sched_lock;
d617 2
a618 1
	SIMPLE_UNLOCK(&sched_lock);
d624 2
a625 1
	SIMPLE_LOCK(&sched_lock);
a765 1
#ifdef notyet
a766 3
#else
		hold_count = __mp_release_all(&kernel_lock);
#endif
a826 1
#ifdef notyet
a827 3
#else
		__mp_acquire_count(&kernel_lock, hold_count);
#endif
d842 1
a842 1
	SIMPLE_LOCK_INIT(&sched_lock);
@


1.17.4.11
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.17.4.10 2003/05/18 18:16:45 niklas Exp $	*/
d21 5
a25 1
 * 3. Neither the name of the University nor the names of its contributors
@


1.17.4.12
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d745 1
d772 8
a779 9
	if (timercmp(&tv, &runtime, <)) {
#if 0
		printf("time is not monotonic! "
		    "tv=%ld.%06ld, runtime=%ld.%06ld\n",
		    tv.tv_sec, tv.tv_usec, runtime.tv_sec, runtime.tv_usec);
#endif
	} else {
		timersub(&tv, &runtime, &tv);
		timeradd(&p->p_rtime, &tv, &p->p_rtime);
d781 2
d789 2
a790 2
	if ((rlim_t)p->p_rtime.tv_sec >= rlim->rlim_cur) {
		if ((rlim_t)p->p_rtime.tv_sec >= rlim->rlim_max) {
d792 1
a792 1
		} else {
@


1.17.4.13
log
@remove unused sleep func. def. comment on a seemingly hanging splx
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.17.4.12 2004/02/19 10:56:37 niklas Exp $	*/
a446 4
	/*
	 * Note! this splx belongs to the SCHED_LOCK(s) above, mi_switch
	 * releases the scheduler lock, but does not lower the spl.
	 */
d509 66
@


1.17.4.14
log
@Make a few scheduling globals per-cpu, mostly NetBSD code
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.17.4.13 2004/04/21 09:36:12 niklas Exp $	*/
a56 1
#ifndef MULTIPROCESSOR
a57 3
int	rrticks;		/* number of hardclock ticks left to
				   roundrobin() */
#endif
a58 1
int	rrticks_init;		/* # of hardclock ticks per roundrobin() */
d67 1
a67 1
void roundrobin(struct cpu_info *);
d75 1
d81 1
a81 1
	 * We setup the timeout here and kick schedcpu once to
d85 1
d88 1
a93 2
 *
 * XXX We badly need to unify MULTIPROCESSOR with not MULTIPROCESSOR.
d97 2
a98 1
roundrobin(struct cpu_info *ci)
d100 1
a100 3
#ifdef MULTIPROCESSOR
	struct schedstate_percpu *spc = &ci->ci_schedstate;
#else
a101 1
#endif
a103 20
#ifdef MULTIPROCESSOR
	spc->spc_rrticks = rrticks_init;

	if (curproc != NULL) {
		s = splstatclock();
		if (spc->spc_schedflags & SPCF_SEENRR) {
			/*
			 * The process has already been through a roundrobin
			 * without switching and may be hogging the CPU.
			 * Indicate that the process should yield.
			 */
			spc->spc_schedflags |= SPCF_SHOULDYIELD;
		} else {
			spc->spc_schedflags |= SPCF_SEENRR;
		}
		splx(s);
	}
#else
	rrticks = rrticks_init;

d118 2
a119 3
#endif

	need_resched(curcpu());
a453 3
#ifdef MULTIPROCESSOR
	p->p_cpu->ci_schedstate.spc_curpriority = p->p_usrpri;
#else
a454 1
#endif
d597 1
a597 5
				 * curpriority on the last CPU on
				 * which it ran.
				 *
				 * XXXSMP See affinity comment in
				 * resched_proc().
d599 1
d602 1
a602 2
					KASSERT(p->p_cpu != NULL);
					need_resched(p->p_cpu);
a685 1
	struct schedstate_percpu *spc;
a701 2

	spc = &p->p_cpu->ci_schedstate;
a708 3
#ifdef MULTIPROCESSOR
	if (timercmp(&tv, &spc->spc_runtime, <)) {
#else
d712 1
a712 1
		    "tv=%lu.%06lu, runtime=%lu.%06lu\n",
a714 1
#endif
a715 3
#ifdef MULTIPROCESSOR
		timersub(&tv, &spc->spc_runtime, &tv);
#else
a716 1
#endif
a738 3
#ifdef MULTIPROCESSOR
	spc->spc_schedflags &= ~SPCF_SWITCHCLEAR;
#else
a739 1
#endif
d746 1
a753 13
	/*
	 * We're running again; record our new start time.  We might
	 * be running on a new CPU now, so don't use the cache'd
	 * schedstate_percpu pointer.
	 */
#ifdef MULTIPROCESSOR
	KDASSERT(p->p_cpu != NULL);
	KDASSERT(p->p_cpu == curcpu());
	microtime(&p->p_cpu->ci_schedstate.spc_runtime);
#else
	microtime(&runtime);
#endif

a782 38
static __inline void
resched_proc(struct proc *p, u_char pri)
{
#ifdef MULTIPROCESSOR
	struct cpu_info *ci;
#endif

	/*
	 * XXXSMP
	 * Since p->p_cpu persists across a context switch,
	 * this gives us *very weak* processor affinity, in
	 * that we notify the CPU on which the process last
	 * ran that it should try to switch.
	 *
	 * This does not guarantee that the process will run on
	 * that processor next, because another processor might
	 * grab it the next time it performs a context switch.
	 *
	 * This also does not handle the case where its last
	 * CPU is running a higher-priority process, but every
	 * other CPU is running a lower-priority process.  There
	 * are ways to handle this situation, but they're not
	 * currently very pretty, and we also need to weigh the
	 * cost of moving a process from one CPU to another.
	 *
	 * XXXSMP
	 * There is also the issue of locking the other CPU's
	 * sched state, which we currently do not do.
	 */
#ifdef MULTIPROCESSOR
	ci = (p->p_cpu != NULL) ? p->p_cpu : curcpu();
	if (pri < ci->ci_schedstate.spc_curpriority)
#else
	if (pri < curpriority)
#endif
		need_resched(ci);
}

d823 2
a824 2
	else
		resched_proc(p, p->p_priority);
d843 2
a844 1
	resched_proc(p, p->p_usrpri);
@


1.17.4.15
log
@don't rely on curcpu() and other fixes for non-mp
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.17.4.14 2004/06/05 17:19:55 niklas Exp $	*/
a99 1
#ifdef MULTIPROCESSOR
d103 1
d105 3
d110 1
a126 3

	need_resched(curcpu());
}
a127 6
void
roundrobin(struct cpu_info *ci)
{
	struct proc *p = curproc;
	int s;

d144 1
d146 1
a146 1
	need_resched(0);
a147 1
#endif
a635 1
#ifdef MULTIPROCESSOR
a636 1
#endif
a876 1
		need_resched(ci);
a878 1
		need_resched(0);
d880 1
@


1.17.4.16
log
@sync with head, make i386 __HAVE_CPUINFO
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d57 1
a57 1
#ifndef __HAVE_CPUINFO
d59 2
a62 1
#ifdef __HAVE_CPUINFO
a63 1
#endif
a71 1
#ifdef __HAVE_CPUINFO
a72 3
#else
void roundrobin(void *);
#endif
a79 3
#ifndef __HAVE_CPUINFO
	static struct timeout roundrobin_to;
#endif
d85 1
a85 1
	 * We setup the timeouts here and kick schedcpu and roundrobin once to
a88 3
#ifndef __HAVE_CPUINFO
	timeout_set(&schedcpu_to, schedcpu, &roundrobin_to);
#endif
a90 5
#ifdef __HAVE_CPUINFO
	rrticks_init = hz / 10;
#else
	roundrobin(&roundrobin_to);
#endif
d96 2
d100 1
a100 1
#ifdef __HAVE_CPUINFO
d128 1
a128 1
roundrobin(void *arg)
a129 1
	struct timeout *to = (struct timeout *)arg;
d133 2
a150 1
	timeout_add(to, hz / 10);
d486 1
a486 1
#ifdef __HAVE_CPUINFO
d641 1
a641 1
#ifdef __HAVE_CPUINFO
d643 1
a644 3
#else
					need_resched(0);
#endif
d728 1
a728 3
#endif
#ifdef __HAVE_CPUINFO
	struct schedstate_percpu *spc = &p->p_cpu->ci_schedstate;
d745 2
d754 1
a754 1
#ifdef __HAVE_CPUINFO
a755 10
#if 0
		printf("time is not monotonic! "
		    "tv=%lu.%06lu, runtime=%lu.%06lu\n",
		    tv.tv_sec, tv.tv_usec, spc->spc_runtime.tv_sec,
		    spc->spc_runtime.tv_usec);
#endif
	} else {
		timersub(&tv, &spc->spc_runtime, &tv);
		timeradd(&p->p_rtime, &tv, &p->p_rtime);
	}
d763 1
d765 3
d769 1
a771 1
#endif
d792 1
a792 1
#ifdef __HAVE_CPUINFO
d815 1
a815 1
#ifdef __HAVE_CPUINFO
d855 1
a855 1
#ifdef __HAVE_CPUINFO
d881 1
a881 1
#ifdef __HAVE_CPUINFO
@


1.17.4.17
log
@Typo in merge.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.17.4.16 2004/06/10 11:40:33 niklas Exp $	*/
d97 1
a97 1
	timeout_set(&roundrobin_to, schedcpu, &roundrobin_to);
@


1.16
log
@Adopt NetBSD fix for scheduler problems (nice was broken).  From the NetBSD
commit messages:

Scheduler bug fixes and reorganization
* fix the ancient nice(1) bug, where nice +20 processes incorrectly
  steal 10 - 20% of the CPU, (or even more depending on load average)
* provide a new schedclock() mechanism at a new clock at schedhz, so high
  platform hz values don't cause nice +0 processes to look like they are
  niced
* change the algorithm slightly, and reorganize the code a lot
* fix percent-CPU calculation bugs, and eliminate some no-op code

=== nice bug === Correctly divide the scheduler queues between niced and
compute-bound processes. The current nice weight of two (sort of, see
`algorithm change' below) neatly divides the USRPRI queues in half; this
should have been used to clip p_estcpu, instead of UCHAR_MAX.  Besides
being the wrong amount, clipping an unsigned char to UCHAR_MAX is a no-op,
and it was done after decay_cpu() which can only _reduce_ the value.  It
has to be kept <= NICE_WEIGHT * PRIO_MAX - PPQ or processes can
scheduler-penalize themselves onto the same queue as nice +20 processes.
(Or even a higher one.)

=== New schedclock() mechanism === Some platforms should be cutting down
stathz before hitting the scheduler, since the scheduler algorithm only
works right in the vicinity of 64 Hz. Rather than prescale hz, then scale
back and forth by 4 every time p_estcpu is touched (each occurance an
abstraction violation), use p_estcpu without scaling and require schedhz
to be generated directly at the right frequency. Use a default stathz (well,
actually, profhz) / 4, so nothing changes unless a platform defines schedhz
and a new clock.
[ To do:  Define these for alpha, where hz==1024, and nice was totally broke.]

=== Algorithm change === The nice value used to be added to the
exponentially-decayed scheduler history value p_estcpu, in _addition_ to
be incorporated directly (with greater weight) into the priority calculation.
At first glance, it appears to be a pointless increase of 1/8 the nice
effect (pri = p_estcpu/4 + nice*2), but it's actually at least 3x that
because it will ramp up linearly but be decayed only exponentially, thus
converging to an additional .75 nice for a loadaverage of one. I killed
this: it makes the behavior hard to control, almost impossible to analyze,
and the effect (~~nothing at for the first second, then somewhat increased
niceness after three seconds or more, depending on load average) pointless.

=== Other bugs === hz -> profhz in the p_pctcpu = f(p_cpticks) calcuation.
Collect scheduler functionality. Try to put each abstraction in just one
place.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.15 1999/04/21 01:21:48 alex Exp $	*/
d182 9
d212 1
a212 1
		KASSERT(profhz);
d214 1
a214 1
		p->p_pctcpu += (profhz == 100)?
d217 1
a217 1
				<< (FSHIFT - CCPU_SHIFT)) / profhz;
d220 1
a220 1
			(p->p_cpticks * FSCALE / profhz)) >> FSHIFT;
@


1.15
log
@Improved ps formatting.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.14 1999/02/26 05:10:40 art Exp $	*/
d52 1
d203 1
d205 1
a205 1
		p->p_pctcpu += (hz == 100)?
d208 1
a208 1
				<< (FSHIFT - CCPU_SHIFT)) / hz;
d211 1
a211 1
			(p->p_cpticks * FSCALE / hz)) >> FSHIFT;
d214 2
a215 2
		newcpu = (u_int) decay_cpu(loadfac, p->p_estcpu) + p->p_nice;
		p->p_estcpu = min(newcpu, UCHAR_MAX);
a217 1
#define	PPQ	(128 / NQS)		/* priorities per queue */
d257 1
a257 1
		p->p_estcpu = min(newcpu, UCHAR_MAX);
d695 1
a695 1
	newpriority = PUSER + p->p_estcpu / 4 + 2 * p->p_nice;
d700 25
@


1.14
log
@uvm allocation and name changes
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.13 1998/11/15 16:45:16 art Exp $	*/
d737 1
a737 1
		db_printf("PID        %10s %18s %18s %18s\n",
d741 2
a742 2
		db_printf("PID        %10s %10s %10s S %7s %16s %7s\n",
		    "PPID", "PGRP", "UID", "FLAGS", "COMMAND", "WAIT");
d745 1
a745 1
		db_printf("PID        %16s %8s %18s %s\n",
d754 1
a754 1
			db_printf("%-10d ", p->p_pid);
d759 1
a759 1
				db_printf("%10.10s %18p %18p %18p\n",
d764 2
a765 1
				db_printf("%10d %10d %10d %d %#7x %16s %7.7s\n",
d768 2
a769 2
				    p->p_comm, (p->p_wchan && p->p_wmesg) ?
					p->p_wmesg : "");
d773 1
a773 1
				db_printf("%16s %8s %18p %s\n", p->p_comm,
@


1.13
log
@GC unnecessary declaration
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.12 1998/02/03 19:06:25 deraadt Exp $	*/
d52 5
d229 3
d233 1
d616 3
d620 1
@


1.12
log
@bad types; wileyc@@sekiya.twics.co.jp
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.11 1997/11/06 05:58:19 csapuntz Exp $	*/
a296 1
	void endtsleep __P((void *));
@


1.11
log
@Updates for VFS Lite 2 + soft update.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.10 1997/11/04 21:09:49 chuck Exp $	*/
d294 1
a294 1
	register s;
d427 1
a427 1
	register s;
@


1.10
log
@fix printf formatting of "ps" (aka "show all proc") so that lines never
overflow (always hated that).

replaced "/m" flag with:
        /a == show process address info
        /n == show normal process info [currently the default]
        /w == show process wait/emul info
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.9 1997/10/06 20:19:57 deraadt Exp $	*/
a176 1
	wakeup((caddr_t)&lbolt);
d225 1
@


1.9
log
@back out vfs lite2 till after 2.2
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.7 1997/07/28 09:13:17 deraadt Exp $	*/
d702 1
a702 1
	int map = modif[0] == 'm';
d706 14
d721 17
a737 2
	db_printf("  pid proc     addr     %s comm         wchan\n",
	    map ? "map     " : "uid  ppid  pgrp  flag stat em ");
d741 25
a765 14
			db_printf("%5d %p %p ",
			    p->p_pid, p, p->p_addr);
			if (map)
				db_printf("%p %s   ",
				    p->p_vmspace, p->p_comm);
			else
				db_printf("%3d %5d %5d  %06x  %d  %s  %s   ",
				    p->p_cred->p_ruid, pp ? pp->p_pid : -1,
				    p->p_pgrp->pg_id, p->p_flag, p->p_stat,
				    p->p_emul->e_name, p->p_comm);
			if (p->p_wchan) {
				if (p->p_wmesg)
					db_printf("%s ", p->p_wmesg);
				db_printf("%p", p->p_wchan);
a766 1
			db_printf("\n");
@


1.8
log
@VFS Lite2 Changes
@
text
@d177 1
a225 1
	wakeup((caddr_t)&lbolt);
@


1.7
log
@two unneeded variables; enami@@ba2.so-net.or.jp
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.6 1997/01/19 03:56:46 briggs Exp $	*/
a176 1
	wakeup((caddr_t)&lbolt);
d225 1
@


1.6
log
@asm -> __asm
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.5 1996/11/23 23:19:51 kstailey Exp $	*/
a225 2
	if (bclnlist != NULL)
		wakeup((caddr_t)pageproc);
@


1.5
log
@remrq -> remrunqueue
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_synch.c,v 1.3 1996/04/21 22:27:08 deraadt Exp $	*/
d361 1
a361 1
	asm(".globl bpendtsleep ; bpendtsleep:");
d474 1
a474 1
	asm(".globl bpendsleep ; bpendsleep:");
@


1.4
log
@sync syscalls, no sys/cpu.h
@
text
@d217 1
a217 1
				remrq(p);
@


1.3
log
@partial sync with netbsd 960418, more to come
@
text
@d1 2
a2 2
/*	$OpenBSD: kern_synch.c,v 1.2 1996/03/03 17:19:55 niklas Exp $	*/
/*	$NetBSD: kern_synch.c,v 1.36 1996/03/30 22:23:25 christos Exp $	*/
a54 1
#include <sys/cpu.h>
@


1.2
log
@From NetBSD: 960217 merge
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: kern_synch.c,v 1.35 1996/02/09 18:59:50 christos Exp $	*/
d715 1
a715 1
			db_printf("%5d %06x %06x ",
d718 1
a718 1
				db_printf("%06x %s   ",
d728 1
a728 1
				db_printf("%x", p->p_wchan);
@


1.1
log
@Initial revision
@
text
@d1 2
a2 1
/*	$NetBSD: kern_synch.c,v 1.33 1995/06/08 23:51:03 mycroft Exp $	*/
d51 1
a51 1
#include <sys/vmmeter.h>
d55 1
d62 5
d345 1
a345 1
		if (sig = CURSIG(p)) {
d379 1
a379 1
	if (catch && (sig != 0 || (sig = CURSIG(p)))) {
d523 1
a523 1
	for (q = &qp->sq_head; p = *q; ) {
d525 1
a525 1
		if (p->p_back || p->p_stat != SSLEEP && p->p_stat != SSTOP)
d693 5
d700 1
a700 1
	long addr;
d702 1
a702 1
	int count;
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@
