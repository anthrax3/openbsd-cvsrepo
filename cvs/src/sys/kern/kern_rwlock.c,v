head	1.27;
access;
symbols
	OPENBSD_6_1:1.27.0.10
	OPENBSD_6_1_BASE:1.27
	OPENBSD_6_0:1.27.0.6
	OPENBSD_6_0_BASE:1.27
	OPENBSD_5_9:1.27.0.2
	OPENBSD_5_9_BASE:1.27
	OPENBSD_5_8:1.27.0.4
	OPENBSD_5_8_BASE:1.27
	OPENBSD_5_7:1.25.0.2
	OPENBSD_5_7_BASE:1.25
	OPENBSD_5_6:1.22.0.4
	OPENBSD_5_6_BASE:1.22
	OPENBSD_5_5:1.21.0.4
	OPENBSD_5_5_BASE:1.21
	OPENBSD_5_4:1.20.0.2
	OPENBSD_5_4_BASE:1.20
	OPENBSD_5_3:1.17.0.8
	OPENBSD_5_3_BASE:1.17
	OPENBSD_5_2:1.17.0.6
	OPENBSD_5_2_BASE:1.17
	OPENBSD_5_1_BASE:1.17
	OPENBSD_5_1:1.17.0.4
	OPENBSD_5_0:1.17.0.2
	OPENBSD_5_0_BASE:1.17
	OPENBSD_4_9:1.16.0.2
	OPENBSD_4_9_BASE:1.16
	OPENBSD_4_8:1.15.0.4
	OPENBSD_4_8_BASE:1.15
	OPENBSD_4_7:1.15.0.2
	OPENBSD_4_7_BASE:1.15
	OPENBSD_4_6:1.13.0.12
	OPENBSD_4_6_BASE:1.13
	OPENBSD_4_5:1.13.0.8
	OPENBSD_4_5_BASE:1.13
	OPENBSD_4_4:1.13.0.6
	OPENBSD_4_4_BASE:1.13
	OPENBSD_4_3:1.13.0.4
	OPENBSD_4_3_BASE:1.13
	OPENBSD_4_2:1.13.0.2
	OPENBSD_4_2_BASE:1.13
	OPENBSD_4_1:1.9.0.2
	OPENBSD_4_1_BASE:1.9
	OPENBSD_4_0:1.8.0.2
	OPENBSD_4_0_BASE:1.8
	OPENBSD_3_9:1.5.0.2
	OPENBSD_3_9_BASE:1.5
	OPENBSD_3_8:1.3.0.6
	OPENBSD_3_8_BASE:1.3
	OPENBSD_3_7:1.3.0.4
	OPENBSD_3_7_BASE:1.3
	OPENBSD_3_6:1.3.0.2
	OPENBSD_3_6_BASE:1.3
	SMP_SYNC_A:1.2
	SMP_SYNC_B:1.2
	OPENBSD_3_5:1.2.0.4
	OPENBSD_3_5_BASE:1.2
	SMP:1.2.0.2;
locks; strict;
comment	@ * @;


1.27
date	2015.03.14.07.33.42;	author jsg;	state Exp;
branches;
next	1.26;
commitid	Pve4Y9ViiMt4ukJa;

1.26
date	2015.03.14.03.38.50;	author jsg;	state Exp;
branches;
next	1.25;
commitid	p4LJxGKbi0BU2cG6;

1.25
date	2015.02.11.00.14.11;	author dlg;	state Exp;
branches;
next	1.24;
commitid	OPUATglsyqcmeG4g;

1.24
date	2015.02.10.10.04.27;	author dlg;	state Exp;
branches;
next	1.23;
commitid	BGSlxgt9MbwlOxvC;

1.23
date	2014.09.01.03.37.10;	author guenther;	state Exp;
branches;
next	1.22;
commitid	40H2085b9mX3abB1;

1.22
date	2014.07.09.13.32.00;	author guenther;	state Exp;
branches;
next	1.21;
commitid	ZU9xQk5IchFd0Jm2;

1.21
date	2014.01.21.01.48.44;	author tedu;	state Exp;
branches;
next	1.20;

1.20
date	2013.05.06.16.37.55;	author tedu;	state Exp;
branches;
next	1.19;

1.19
date	2013.05.01.17.18.55;	author tedu;	state Exp;
branches;
next	1.18;

1.18
date	2013.05.01.17.13.05;	author tedu;	state Exp;
branches;
next	1.17;

1.17
date	2011.07.05.03.58.22;	author weingart;	state Exp;
branches;
next	1.16;

1.16
date	2010.09.24.13.21.30;	author matthew;	state Exp;
branches;
next	1.15;

1.15
date	2009.08.13.23.12.15;	author blambert;	state Exp;
branches;
next	1.14;

1.14
date	2009.08.13.21.22.29;	author blambert;	state Exp;
branches;
next	1.13;

1.13
date	2007.05.13.04.52.32;	author tedu;	state Exp;
branches;
next	1.12;

1.12
date	2007.05.04.13.21.03;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2007.05.04.12.56.15;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2007.04.04.18.01.57;	author art;	state Exp;
branches;
next	1.9;

1.9
date	2006.11.30.20.08.22;	author mk;	state Exp;
branches;
next	1.8;

1.8
date	2006.06.02.05.02.34;	author tedu;	state Exp;
branches;
next	1.7;

1.7
date	2006.05.07.20.12.41;	author tedu;	state Exp;
branches;
next	1.6;

1.6
date	2006.04.20.14.36.24;	author pedro;	state Exp;
branches;
next	1.5;

1.5
date	2006.01.06.07.05.12;	author tedu;	state Exp;
branches;
next	1.4;

1.4
date	2006.01.06.06.50.31;	author tedu;	state Exp;
branches;
next	1.3;

1.3
date	2004.07.21.12.10.20;	author art;	state Exp;
branches;
next	1.2;

1.2
date	2003.11.18.18.12.14;	author tedu;	state Exp;
branches
	1.2.2.1;
next	1.1;

1.1
date	2003.11.18.06.11.10;	author tedu;	state Exp;
branches;
next	;

1.2.2.1
date	2004.02.19.10.56.37;	author niklas;	state Exp;
branches;
next	;


desc
@@


1.27
log
@add sys/atomic.h back for membar_* needed for at least armv7
@
text
@/*	$OpenBSD: kern_rwlock.c,v 1.26 2015/03/14 03:38:50 jsg Exp $	*/

/*
 * Copyright (c) 2002, 2003 Artur Grabowski <art@@openbsd.org>
 * Copyright (c) 2011 Thordur Bjornsson <thib@@secnorth.net>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/rwlock.h>
#include <sys/limits.h>
#include <sys/atomic.h>

/* XXX - temporary measure until proc0 is properly aligned */
#define RW_PROC(p) (((long)p) & ~RWLOCK_MASK)

#ifdef MULTIPROCESSOR
#define rw_cas(p, o, n)	(atomic_cas_ulong(p, o, n) != o)
#else
static inline int
rw_cas(volatile unsigned long *p, unsigned long o, unsigned long n)
{
	if (*p != o)
		return (1);
	*p = n;

	return (0);
}
#endif

/*
 * Magic wand for lock operations. Every operation checks if certain
 * flags are set and if they aren't, it increments the lock with some
 * value (that might need some computing in a few cases). If the operation
 * fails, we need to set certain flags while waiting for the lock.
 *
 * RW_WRITE	The lock must be completely empty. We increment it with
 *		RWLOCK_WRLOCK and the proc pointer of the holder.
 *		Sets RWLOCK_WAIT|RWLOCK_WRWANT while waiting.
 * RW_READ	RWLOCK_WRLOCK|RWLOCK_WRWANT may not be set. We increment
 *		with RWLOCK_READ_INCR. RWLOCK_WAIT while waiting.
 */
static const struct rwlock_op {
	unsigned long inc;
	unsigned long check;
	unsigned long wait_set;
	long proc_mult;
	int wait_prio;
} rw_ops[] = {
	{	/* RW_WRITE */
		RWLOCK_WRLOCK,
		ULONG_MAX,
		RWLOCK_WAIT | RWLOCK_WRWANT,
		1,
		PLOCK - 4
	},
	{	/* RW_READ */
		RWLOCK_READ_INCR,
		RWLOCK_WRLOCK,
		RWLOCK_WAIT,
		0,
		PLOCK
	},
	{	/* Sparse Entry. */
		0,
	},
	{	/* RW_DOWNGRADE */
		RWLOCK_READ_INCR - RWLOCK_WRLOCK,
		0,
		0,
		-1,
		PLOCK
	},
};

void
rw_enter_read(struct rwlock *rwl)
{
	unsigned long owner = rwl->rwl_owner;

	if (__predict_false((owner & RWLOCK_WRLOCK) ||
	    rw_cas(&rwl->rwl_owner, owner, owner + RWLOCK_READ_INCR)))
		rw_enter(rwl, RW_READ);
	else
		membar_enter();
}

void
rw_enter_write(struct rwlock *rwl)
{
	struct proc *p = curproc;

	if (__predict_false(rw_cas(&rwl->rwl_owner, 0,
	    RW_PROC(p) | RWLOCK_WRLOCK)))
		rw_enter(rwl, RW_WRITE);
	else
		membar_enter();
}

void
rw_exit_read(struct rwlock *rwl)
{
	unsigned long owner = rwl->rwl_owner;

	rw_assert_rdlock(rwl);

	membar_exit();
	if (__predict_false((owner & RWLOCK_WAIT) ||
	    rw_cas(&rwl->rwl_owner, owner, owner - RWLOCK_READ_INCR)))
		rw_exit(rwl);
}

void
rw_exit_write(struct rwlock *rwl)
{
	unsigned long owner = rwl->rwl_owner;

	rw_assert_wrlock(rwl);

	membar_exit();
	if (__predict_false((owner & RWLOCK_WAIT) ||
	    rw_cas(&rwl->rwl_owner, owner, 0)))
		rw_exit(rwl);
}

#ifdef DIAGNOSTIC
/*
 * Put the diagnostic functions here to keep the main code free
 * from ifdef clutter.
 */
static void
rw_enter_diag(struct rwlock *rwl, int flags)
{
	switch (flags & RW_OPMASK) {
	case RW_WRITE:
	case RW_READ:
		if (RW_PROC(curproc) == RW_PROC(rwl->rwl_owner))
			panic("rw_enter: %s locking against myself",
			    rwl->rwl_name);
		break;
	case RW_DOWNGRADE:
		/*
		 * If we're downgrading, we must hold the write lock.
		 */
		if ((rwl->rwl_owner & RWLOCK_WRLOCK) == 0)
			panic("rw_enter: %s downgrade of non-write lock",
			    rwl->rwl_name);
		if (RW_PROC(curproc) != RW_PROC(rwl->rwl_owner))
			panic("rw_enter: %s downgrade, not holder",
			    rwl->rwl_name);
		break;

	default:
		panic("rw_enter: unknown op 0x%x", flags);
	}
}

#else
#define rw_enter_diag(r, f)
#endif

void
rw_init(struct rwlock *rwl, const char *name)
{
	rwl->rwl_owner = 0;
	rwl->rwl_name = name;
}

int
rw_enter(struct rwlock *rwl, int flags)
{
	const struct rwlock_op *op;
	struct sleep_state sls;
	unsigned long inc, o;
	int error;

	op = &rw_ops[(flags & RW_OPMASK) - 1];

	inc = op->inc + RW_PROC(curproc) * op->proc_mult;
retry:
	while (__predict_false(((o = rwl->rwl_owner) & op->check) != 0)) {
		unsigned long set = o | op->wait_set;
		int do_sleep;

		rw_enter_diag(rwl, flags);

		if (flags & RW_NOSLEEP)
			return (EBUSY);

		sleep_setup(&sls, rwl, op->wait_prio, rwl->rwl_name);
		if (flags & RW_INTR)
			sleep_setup_signal(&sls, op->wait_prio | PCATCH);

		do_sleep = !rw_cas(&rwl->rwl_owner, o, set);

		sleep_finish(&sls, do_sleep);
		if ((flags & RW_INTR) &&
		    (error = sleep_finish_signal(&sls)) != 0)
			return (error);
		if (flags & RW_SLEEPFAIL)
			return (EAGAIN);
	}

	if (__predict_false(rw_cas(&rwl->rwl_owner, o, o + inc)))
		goto retry;
	membar_enter();

	/*
	 * If old lock had RWLOCK_WAIT and RWLOCK_WRLOCK set, it means we
	 * downgraded a write lock and had possible read waiter, wake them
	 * to let them retry the lock.
	 */
	if (__predict_false((o & (RWLOCK_WRLOCK|RWLOCK_WAIT)) ==
	    (RWLOCK_WRLOCK|RWLOCK_WAIT)))
		wakeup(rwl);

	return (0);
}

void
rw_exit(struct rwlock *rwl)
{
	unsigned long owner = rwl->rwl_owner;
	int wrlock = owner & RWLOCK_WRLOCK;
	unsigned long set;

	if (wrlock)
		rw_assert_wrlock(rwl);
	else
		rw_assert_rdlock(rwl);

	membar_exit();
	do {
		owner = rwl->rwl_owner;
		if (wrlock)
			set = 0;
		else
			set = (owner - RWLOCK_READ_INCR) &
				~(RWLOCK_WAIT|RWLOCK_WRWANT);
	} while (rw_cas(&rwl->rwl_owner, owner, set));

	if (owner & RWLOCK_WAIT)
		wakeup(rwl);
}

int
rw_status(struct rwlock *rwl)
{
	if (rwl->rwl_owner & RWLOCK_WRLOCK) {
		if (RW_PROC(curproc) == RW_PROC(rwl->rwl_owner))
			return RW_WRITE;
		else
			return RW_WRITE_OTHER;
	}
	if (rwl->rwl_owner)
		return RW_READ;
	return (0);
}

#ifdef DIAGNOSTIC
void
rw_assert_wrlock(struct rwlock *rwl)
{
	if (!(rwl->rwl_owner & RWLOCK_WRLOCK))
		panic("%s: lock not held", rwl->rwl_name);

	if (RWLOCK_OWNER(rwl) != (struct proc *)RW_PROC(curproc))
		panic("%s: lock not held by this process", rwl->rwl_name);
}

void
rw_assert_rdlock(struct rwlock *rwl)
{
	if (!RWLOCK_OWNER(rwl) || (rwl->rwl_owner & RWLOCK_WRLOCK))
		panic("%s: lock not shared", rwl->rwl_name);
}

void
rw_assert_unlocked(struct rwlock *rwl)
{
	if (rwl->rwl_owner != 0L)
		panic("%s: lock held", rwl->rwl_name);
}
#endif

/* recursive rwlocks; */
void
rrw_init(struct rrwlock *rrwl, char *name)
{
	memset(rrwl, 0, sizeof(struct rrwlock));
	rw_init(&rrwl->rrwl_lock, name);
}

int
rrw_enter(struct rrwlock *rrwl, int flags)
{
	int	rv;

	if (RWLOCK_OWNER(&rrwl->rrwl_lock) ==
	    (struct proc *)RW_PROC(curproc)) {
		if (flags & RW_RECURSEFAIL)
			return (EDEADLK);
		else {
			rrwl->rrwl_wcnt++;
			return (0);
		}
	}

	rv = rw_enter(&rrwl->rrwl_lock, flags);
	if (rv == 0)
		rrwl->rrwl_wcnt = 1;

	return (rv);
}

void
rrw_exit(struct rrwlock *rrwl)
{

	if (RWLOCK_OWNER(&rrwl->rrwl_lock) ==
	    (struct proc *)RW_PROC(curproc)) {
		KASSERT(rrwl->rrwl_wcnt > 0);
		rrwl->rrwl_wcnt--;
		if (rrwl->rrwl_wcnt != 0)
			return;
	}

	rw_exit(&rrwl->rrwl_lock);
}

int
rrw_status(struct rrwlock *rrwl)
{
	return (rw_status(&rrwl->rrwl_lock));
}
@


1.26
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.25 2015/02/11 00:14:11 dlg Exp $	*/
d25 1
@


1.25
log
@make the rwlock implementation MI.

each arch used to have to provide an rw_cas operation, but now we
have the rwlock code build its own version. on smp machines it uses
atomic_cas_ulong. on uniproc machines it avoids interlocked
instructions by using straight loads and stores. this is safe because
rwlocks are only used from process context and processes are currently
not preemptible in our kernel. so alpha/ppc/etc might get a benefit.

ok miod@@ kettenis@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.24 2015/02/10 10:04:27 dlg Exp $	*/
a24 1
#include <sys/atomic.h>
@


1.24
log
@remove #ifndef handling of __HAVE_MD_RWLOCK. it's never set, and with a
reasonable compiler it isnt necessary.

ok miod@@ art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.23 2014/09/01 03:37:10 guenther Exp $	*/
a26 2
#include <machine/lock.h>

d30 14
a137 12

#ifndef rw_cas
int
rw_cas(volatile unsigned long *p, unsigned long o, unsigned long n)
{
	if (*p != o)
		return (1);
	*p = n;

	return (0);
}
#endif
@


1.23
log
@Add explicit membars, so that we can stop requiring rw_cas() to
provide the magic.

ok matthew@@ dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.22 2014/07/09 13:32:00 guenther Exp $	*/
a76 4
#ifndef __HAVE_MD_RWLOCK
/*
 * Simple cases that should be in MD code and atomic.
 */
a136 2
#endif

@


1.22
log
@Teach rw_status() and rrw_status() to return LK_EXCLOTHER if it's write
locked by a different thread.  Teach lockstatus() to return LK_EXCLUSIVE
if an exclusive lock is held by some other thread.

ok beck@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.21 2014/01/21 01:48:44 tedu Exp $	*/
d25 1
d89 2
d101 2
d112 1
d125 1
d225 1
d251 1
@


1.21
log
@bzero -> memset
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.20 2013/05/06 16:37:55 tedu Exp $	*/
d259 6
a264 2
	if (rwl->rwl_owner & RWLOCK_WRLOCK)
		return RW_WRITE;
@


1.20
log
@restore original gangster lockstatus return values for compat
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.19 2013/05/01 17:18:55 tedu Exp $	*/
d296 1
a296 1
	bzero(rrwl, sizeof(struct rrwlock));
@


1.19
log
@a few tweaks noticed by jsing
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.18 2013/05/01 17:13:05 tedu Exp $	*/
d259 5
a263 1
	return (rwl->rwl_owner != 0L);
@


1.18
log
@exorcise lockmgr. the api remains, but is now backed by recursive rwlocks.
originally by thib.
ok deraadt jsing and anyone who tested
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.17 2011/07/05 03:58:22 weingart Exp $	*/
a258 1

a335 1

@


1.17
log
@Replace expanded version of RW_PROC() with the macro.

ok oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.16 2010/09/24 13:21:30 matthew Exp $	*/
d5 1
a5 1
 * All rights reserved. 
d7 3
a9 3
 * Redistribution and use in source and binary forms, with or without 
 * modification, are permitted provided that the following conditions 
 * are met: 
d11 7
a17 15
 * 1. Redistributions of source code must retain the above copyright 
 *    notice, this list of conditions and the following disclaimer. 
 * 2. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission. 
 *
 * THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES,
 * INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY
 * AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL
 * THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 * EXEMPLARY, OR CONSEQUENTIAL  DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
 * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
 * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
 * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
 * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. 
d64 3
d189 1
a189 1
	op = &rw_ops[flags & RW_OPMASK];
d256 7
d288 52
@


1.16
log
@Add stricter asserts to DIAGNOSTIC kernels to help catch mutex and
rwlock misuse.  In particular, this commit makes the following
changes:

  1. i386 and amd64 now count the number of active mutexes so that
assertwaitok(9) can detect attempts to sleep while holding a mutex.

  2. i386 and amd64 check that we actually hold mutexes when passed to
mtx_leave().

  3. Calls to rw_exit*() now call rw_assert_{rd,wr}lock() as
appropriate.

ok krw@@, oga@@; "sounds good to me" deraadt@@; assembly bits double
checked by pirofti@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.15 2009/08/13 23:12:15 blambert Exp $	*/
d268 1
a268 1
	if (RWLOCK_OWNER(rwl) != (struct proc *)((long)curproc & ~RWLOCK_MASK))
@


1.15
log
@Remove unrelated bit from last commit which breaks at least 2 arches.

Bad blambert@@, no biscuit.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.14 2009/08/13 21:22:29 blambert Exp $	*/
d110 2
d122 2
d243 5
d261 1
d285 1
@


1.14
log
@rwlock assertion functions, currently unused

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.13 2007/05/13 04:52:32 tedu Exp $	*/
a198 7

		if (((struct proc *)RW_PROC(rwl))->p_stat == SONPROC) {
printf("%p\n", (struct proc *)RW_PROC(rwl));
			while(((struct proc *)RW_PROC(rwl))->p_stat == SONPROC)
				SPINLOCK_SPIN_HOOK;
			goto retry;
		}
@


1.13
log
@print lock name in some panic messages so we know what's up
ok krw marco pedro
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.12 2007/05/04 13:21:03 art Exp $	*/
d200 7
d257 24
@


1.12
log
@Kill a dead variable.
Pointed out by thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.11 2007/05/04 12:56:15 art Exp $	*/
d151 2
a152 1
			panic("rw_enter: locking against myself");
d159 2
a160 1
			panic("rw_enter: downgrade of non-write lock");
d162 2
a163 1
			panic("rw_enter: downgrade, not holder");
@


1.11
log
@- Rename rw_test_and_set to rw_cas, since most litterature uses the
  test_and_set name for some other operation, while cas is generally
  used for compare and set (cmpxchg in intel land, cas in sparc land).

- Make rw locks properly MP safe (provided that rw_cas is implemented
  in MD code). Most operations were MP safe except the sleep where we
  could have set the "I'm sleeping" flag before actually going to sleep
  so that the wakeup could miss us. Now, using the split tsleep,
  we first setup the sleep (put us on the sleep queues), then set
  the flag aborting the sleep if the lock has changed and then finally
  go to sleep.

miod@@ ok (and he's been prodding me for days to get this in)
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.10 2007/04/04 18:01:57 art Exp $	*/
d193 1
a193 5
		int do_sleep, prio;

		prio = op->wait_prio;
		if (flags & RW_INTR)
			prio |= PCATCH;
@


1.10
log
@Implement RW_DOWNGRADE that downgrades an exclusive lock to a shared lock
without letting any other exclusive locks in between. As opposed to upgrading
locks, this is easy and solves real problems.

deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.9 2006/11/30 20:08:22 mk Exp $	*/
d34 2
d91 1
a91 1
	    rw_test_and_set(&rwl->rwl_owner, owner, owner + RWLOCK_READ_INCR)))
d100 1
a100 1
	if (__predict_false(rw_test_and_set(&rwl->rwl_owner, 0,
d111 1
a111 1
	    rw_test_and_set(&rwl->rwl_owner, owner, owner - RWLOCK_READ_INCR)))
d121 1
a121 1
	    rw_test_and_set(&rwl->rwl_owner, owner, 0)))
d125 1
d127 1
a127 1
rw_test_and_set(volatile unsigned long *p, unsigned long o, unsigned long n)
d137 2
d183 1
d185 1
a185 1
	int error, prio;
d192 2
a193 2
		if (rw_test_and_set(&rwl->rwl_owner, o, o | op->wait_set))
			continue;
d203 10
a212 1
		if ((error = tsleep(rwl, prio, rwl->rwl_name, 0)) != 0)
d218 1
a218 1
	if (__predict_false(rw_test_and_set(&rwl->rwl_owner, o, o + inc)))
d247 1
a247 1
	} while (rw_test_and_set(&rwl->rwl_owner, owner, set));
@


1.9
log
@s/completly/completely/

ok brad
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.8 2006/06/02 05:02:34 tedu Exp $	*/
d70 7
d148 10
a173 3
/*
 * You are supposed to understand this.
 */
a183 3
	prio = op->wait_prio;
	if (flags & RW_INTR)
		prio |= PCATCH;
d189 4
d205 9
@


1.8
log
@remove upgrade/downgrade, they are useless.
rework the main entry points to just use rw_test_and_set.
rework exit paths to be more like enter paths.
add sleepfail so more lockmgr can be replaced.
some from art, ok sturm
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.7 2006/05/07 20:12:41 tedu Exp $	*/
d43 1
a43 1
 * RW_WRITE	The lock must be completly empty. We increment it with
@


1.7
log
@add a name to rwlock so that we can tell where procs are getting stuck
without breaking into ddb.  doubles the size of rwlock [1], but moving
forward this really helps.  ok/tested pedro fgsch millert krw
[1 - next person to add a field to this struct gets whipped with a wet noodle]
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.6 2006/04/20 14:36:24 pedro Exp $	*/
a47 4
 * RW_UPGRADE	There must be exactly one holder of the read lock.
 *		We increment with what's needed for RW_WRITE - RW_READ.
 *		RWLOCK_WAIT|RWLOCK_WRWANT while waiting.
 * RW_DOWNGRADE	Always doable. Increment with -RW_WRITE + RW_READ.
a69 14
	{	/* RW_UPGRADE */
		RWLOCK_WRLOCK-RWLOCK_READ_INCR,
		~(RWLOCK_READ_INCR | RWLOCK_WAIT | RWLOCK_WRWANT),
		RWLOCK_WAIT|RWLOCK_WRLOCK,
		1,
		PLOCK - 4
	},
	{	/* RW_DOWNGRADE */
		-RWLOCK_WRLOCK + RWLOCK_READ_INCR,
		0,
		0,
		-1,
		0
	}
d79 4
a82 1
	if (__predict_false(rwl->rwl_owner & RWLOCK_WRLOCK)) 
a83 2
	else
		rwl->rwl_owner += RWLOCK_READ_INCR;
d91 2
a92 1
	if (__predict_false(rwl->rwl_owner != 0))
a93 2
	else
		rwl->rwl_owner = RW_PROC(p) | RWLOCK_WRLOCK;
a99 2
	unsigned long decr = (owner & (RWLOCK_WAIT|RWLOCK_WRWANT)) |
	    RWLOCK_READ_INCR;
d101 3
a103 7
	rwl->rwl_owner -= decr;
	/*
	 * Potential MP race here. If the owner had WRWANT set we cleared
	 * it and a reader can sneak in before a writer. Do we care?
	 */
	if (__predict_false(owner & RWLOCK_WAIT))
		rw_exit_waiters(rwl, owner);
d111 3
a113 7
	rwl->rwl_owner = 0;
	/*
	 * Potential MP race here. If the owner had WRWANT set we cleared
	 * it and a reader can sneak in before a writer. Do we care?
	 */
	if (__predict_false(owner & RWLOCK_WAIT))
		rw_exit_waiters(rwl, owner);
a140 14
	case RW_UPGRADE:
		/*
		 * Since we're holding the read lock, it can't possibly
		 * be write locked.
		 */
		if (rwl->rwl_owner & RWLOCK_WRLOCK)
			panic("rw_enter: upgraded lock write locked");
		break;
	case RW_DOWNGRADE:
		/*
		 * If we're downgrading, we must hold the write lock.
		 */
		if (RW_PROC(curproc) != RW_PROC(rwl->rwl_owner))
			panic("rw_enter: not holder");
a145 7
static void
rw_exit_diag(struct rwlock *rwl, int owner)
{
	if ((owner & RWLOCK_WAIT) == 0)
		panic("rw_exit: no waiter");
}

a147 1
#define rw_exit_diag(r, o)
d169 1
a169 5
	inc = op->inc;
	if (op->proc_mult == -1)
		inc -= RW_PROC(curproc);
	else if (op->proc_mult == 1)
		inc += RW_PROC(curproc);
d180 2
d195 1
a195 1
rw_exit_waiters(struct rwlock *rwl, unsigned long owner)
d197 15
a211 3
	rw_exit_diag(rwl, owner);
	/* We wake up all waiters because we can't know how many they are. */
	wakeup(rwl);	
@


1.6
log
@much -> must
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.5 2006/01/06 07:05:12 tedu Exp $	*/
d201 1
a201 1
rw_init(struct rwlock *rwl)
d204 1
d234 1
a234 1
		if ((error = tsleep(rwl, prio, "rwlock", 0)) != 0)
@


1.5
log
@oring in PCATCH doesn't help if we don't pass the right prio to tsleep
@
text
@d1 2
a2 1
/*	$OpenBSD: kern_rwlock.c,v 1.4 2006/01/06 06:50:31 tedu Exp $	*/
d179 1
a179 1
		 * If we're downgrading, we much hold the write lock.
@


1.4
log
@check in of "rwlock.20051230" from art.
mostly cleanup and simplification, though now also supporting
upgrade and downgrade via the magic wand.
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.3 2004/07/21 12:10:20 art Exp $	*/
d232 1
a232 1
		if ((error = tsleep(rwl, op->wait_prio, "rwlock", 0)) != 0)
@


1.3
log
@I was wrong. The assymetry created by the proc argument to rw_enter_write
is horrible and doesn't add anything.

Remove it.
XXX - the fdplock macro will need a separate cleanup.

niklas@@ markus@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.2 2003/11/18 18:12:14 tedu Exp $	*/
d31 1
d34 54
a87 1
#define RW_PROC(p) (((unsigned long)p) & ~RWLOCK_MASK)
d96 4
a99 7
	while (__predict_false(rwl->rwl_owner & RWLOCK_WRLOCK)) {
		/*
		 * Not the simple case, go to slow path.
		 */
		rw_enter_wait(rwl, curproc, RW_READ);
	}
	rwl->rwl_owner += RWLOCK_READ_INCR;
d107 4
a110 7
	while (__predict_false(rwl->rwl_owner != 0)) {
		/*
		 * Not the simple case, go to slow path.
		 */
		rw_enter_wait(rwl, p, RW_WRITE);
	}
	rwl->rwl_owner = RW_PROC(p) | RWLOCK_WRLOCK;
a141 1
#endif
d143 2
a144 2
void
rw_init(struct rwlock *rwl)
d146 5
a150 1
	rwl->rwl_owner = 0;
d152 1
d154 7
a160 2
void
rw_enter_wait(struct rwlock *rwl, struct proc *p, int how)
d162 2
a163 13
	unsigned long need_wait, set_wait;
	int wait_prio;

#ifdef DIAGNOSTIC
	if (p == NULL)
		panic("rw_enter_wait: NULL proc");
#endif

	/*
	 * XXX - this function needs a lot of help to become MP safe.
	 */

	switch (how) {
d165 4
d170 2
a171 1
		 * Let writers through before obtaining read lock.
d173 2
a174 3
		need_wait = RWLOCK_WRLOCK | RWLOCK_WRWANT;
		set_wait = RWLOCK_WAIT;
		wait_prio = PLOCK;
d176 8
a183 13
	case RW_WRITE:
		need_wait = ~0UL;
		set_wait = RWLOCK_WAIT | RWLOCK_WRWANT;
		wait_prio = PLOCK - 4;
		if (RW_PROC(RWLOCK_OWNER(rwl)) == RW_PROC(p)) {
			panic("rw_enter: locking against myself");
		}
		break;
	}

	while (rwl->rwl_owner & need_wait) {
		rwl->rwl_owner |= set_wait;
		tsleep(rwl, wait_prio, "rwlock", 0);
d187 2
a188 2
void
rw_exit_waiters(struct rwlock *rwl, unsigned long owner)
a189 1
#ifdef DIAGNOSTIC
d191 1
a191 4
		panic("rw_exit_waiters: no waiter");
#endif
	/* We wake up all waiters because we can't know how many they are. */
	wakeup(rwl);	
d194 4
a197 10
#ifdef RWLOCK_TEST
#include <sys/kthread.h>

void rwlock_test(void);

void rwlock_testp1(void *);
void rwlock_testp2(void *);
void rwlock_testp3(void *);

struct rwlock rw_test = RWLOCK_INITIALIZER;
d200 1
a200 1
rwlock_test(void)
d202 1
a202 3
	kthread_create(rwlock_testp1, NULL, NULL, "rw1");
	kthread_create(rwlock_testp2, NULL, NULL, "rw2");
	kthread_create(rwlock_testp3, NULL, NULL, "rw3");
d205 5
a209 2
void
rwlock_testp1(void *a)
d211 26
a236 15
	int local;

	printf("rwlock test1 start\n");
	rw_enter_read(&rw_test);
	printf("rwlock test1 obtained\n");
	tsleep(&local, PWAIT, "rw1", 4);
	rw_exit_read(&rw_test);
	printf("rwlock test1 released\n");
	tsleep(&local, PWAIT, "rw1/2", 3);
	rw_enter_read(&rw_test);
	printf("rwlock test1 obtained\n");
	rw_exit_read(&rw_test);
	printf("rwlock test1 released\n");
	kthread_exit(0);
}
d238 2
a239 4
void
rwlock_testp2(void *a)
{
	int local;
d241 1
a241 7
	printf("rwlock test2 start\n");
	rw_enter_read(&rw_test);
	printf("rwlock test2 obtained\n");
	tsleep(&local, PWAIT, "rw2", 4);
	rw_exit_read(&rw_test);
	printf("rwlock test2 released\n");
	kthread_exit(0);
d245 1
a245 1
rwlock_testp3(void *a)
d247 3
a249 11
	int local;

	printf("rwlock test3 start\n");
	tsleep(&local, PWAIT, "rw3", 2);
	printf("rwlock test3 exited waiting\n");
	rw_enter_write(&rw_test);
	printf("rwlock test3 obtained\n");
	tsleep(&local, PWAIT, "rw3/2", 4);
	rw_exit_write(&rw_test);
	printf("rwlock test3 released\n");
	kthread_exit(0);
a250 1
#endif
@


1.2
log
@don't leave test code enabled, spotted Dries Schellekens
@
text
@d1 1
a1 1
/*	$OpenBSD: kern_rwlock.c,v 1.1 2003/11/18 06:11:10 tedu Exp $	*/
d52 1
a52 1
rw_enter_write(struct rwlock *rwl, struct proc *p)
d54 2
d213 1
a213 1
	rw_enter_write(&rw_test, curproc);
@


1.2.2.1
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
@


1.1
log
@add lightwight reader/writer locks from art@@.  we will be using these
later.  have been looked over for quite some time now.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a33 2

#define RWLOCK_TEST
@

