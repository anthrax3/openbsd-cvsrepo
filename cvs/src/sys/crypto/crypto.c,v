head	1.79;
access;
symbols
	OPENBSD_6_0:1.76.0.2
	OPENBSD_6_0_BASE:1.76
	OPENBSD_5_9:1.75.0.2
	OPENBSD_5_9_BASE:1.75
	OPENBSD_5_8:1.74.0.6
	OPENBSD_5_8_BASE:1.74
	OPENBSD_5_7:1.74.0.2
	OPENBSD_5_7_BASE:1.74
	OPENBSD_5_6:1.65.0.4
	OPENBSD_5_6_BASE:1.65
	OPENBSD_5_5:1.63.0.4
	OPENBSD_5_5_BASE:1.63
	OPENBSD_5_4:1.60.0.2
	OPENBSD_5_4_BASE:1.60
	OPENBSD_5_3:1.59.0.10
	OPENBSD_5_3_BASE:1.59
	OPENBSD_5_2:1.59.0.8
	OPENBSD_5_2_BASE:1.59
	OPENBSD_5_1_BASE:1.59
	OPENBSD_5_1:1.59.0.6
	OPENBSD_5_0:1.59.0.4
	OPENBSD_5_0_BASE:1.59
	OPENBSD_4_9:1.59.0.2
	OPENBSD_4_9_BASE:1.59
	OPENBSD_4_8:1.57.0.2
	OPENBSD_4_8_BASE:1.57
	OPENBSD_4_7:1.53.0.2
	OPENBSD_4_7_BASE:1.53
	OPENBSD_4_6:1.52.0.6
	OPENBSD_4_6_BASE:1.52
	OPENBSD_4_5:1.52.0.2
	OPENBSD_4_5_BASE:1.52
	OPENBSD_4_4:1.51.0.4
	OPENBSD_4_4_BASE:1.51
	OPENBSD_4_3:1.51.0.2
	OPENBSD_4_3_BASE:1.51
	OPENBSD_4_2:1.48.0.6
	OPENBSD_4_2_BASE:1.48
	OPENBSD_4_1:1.48.0.4
	OPENBSD_4_1_BASE:1.48
	OPENBSD_4_0:1.48.0.2
	OPENBSD_4_0_BASE:1.48
	OPENBSD_3_9:1.46.0.6
	OPENBSD_3_9_BASE:1.46
	OPENBSD_3_8:1.46.0.4
	OPENBSD_3_8_BASE:1.46
	OPENBSD_3_7:1.46.0.2
	OPENBSD_3_7_BASE:1.46
	OPENBSD_3_6:1.45.0.2
	OPENBSD_3_6_BASE:1.45
	SMP_SYNC_A:1.44
	SMP_SYNC_B:1.44
	OPENBSD_3_5:1.44.0.4
	OPENBSD_3_5_BASE:1.44
	OPENBSD_3_4:1.44.0.2
	OPENBSD_3_4_BASE:1.44
	UBC_SYNC_A:1.43
	OPENBSD_3_3:1.43.0.2
	OPENBSD_3_3_BASE:1.43
	OPENBSD_3_2:1.41.0.2
	OPENBSD_3_2_BASE:1.41
	OPENBSD_3_1:1.33.0.2
	OPENBSD_3_1_BASE:1.33
	UBC_SYNC_B:1.41
	UBC:1.30.0.2
	UBC_BASE:1.30
	OPENBSD_3_0:1.26.0.2
	OPENBSD_3_0_BASE:1.26
	OPENBSD_2_9_BASE:1.15
	OPENBSD_2_9:1.15.0.2
	OPENBSD_2_8:1.14.0.2
	OPENBSD_2_8_BASE:1.14
	OPENBSD_2_7:1.6.0.2
	OPENBSD_2_7_BASE:1.6
	SMP:1.2.0.2;
locks; strict;
comment	@ * @;


1.79
date	2017.02.07.17.25.46;	author patrick;	state Exp;
branches;
next	1.78;
commitid	dMJlqKWYCJoMV7JN;

1.78
date	2016.09.19.18.09.40;	author tedu;	state Exp;
branches;
next	1.77;
commitid	ylyllqBwTV1qZHDZ;

1.77
date	2016.09.15.02.00.17;	author dlg;	state Exp;
branches;
next	1.76;
commitid	RlO92XR575sygHqm;

1.76
date	2016.04.18.21.05.55;	author kettenis;	state Exp;
branches;
next	1.75;
commitid	E50p5lFUXDjjcPsS;

1.75
date	2015.08.28.00.03.53;	author deraadt;	state Exp;
branches;
next	1.74;
commitid	NdgfPIGUgJxQPnT7;

1.74
date	2015.02.09.03.15.41;	author dlg;	state Exp;
branches;
next	1.73;
commitid	jVd0KngVszV2FEfg;

1.73
date	2015.01.27.03.17.35;	author dlg;	state Exp;
branches;
next	1.72;
commitid	MyKPm9Q3dQu92BiX;

1.72
date	2014.10.23.00.15.09;	author dlg;	state Exp;
branches;
next	1.71;
commitid	FoUTQPhc405sdCII;

1.71
date	2014.10.23.00.11.48;	author dlg;	state Exp;
branches;
next	1.70;
commitid	yGM7Ri3EJy95SOAW;

1.70
date	2014.10.23.00.10.09;	author dlg;	state Exp;
branches;
next	1.69;
commitid	uem3dXNeBvHOgApd;

1.69
date	2014.10.22.05.37.54;	author dlg;	state Exp;
branches;
next	1.68;
commitid	8Tphv0manaTTDuu9;

1.68
date	2014.10.20.00.40.33;	author dlg;	state Exp;
branches;
next	1.67;
commitid	VEstYuSPR0o9WYFv;

1.67
date	2014.09.14.14.17.23;	author jsg;	state Exp;
branches;
next	1.66;
commitid	uzzBR7hz9ncd4O6G;

1.66
date	2014.08.20.06.23.03;	author mikeb;	state Exp;
branches;
next	1.65;
commitid	3nalvU8sn6rZiZ1d;

1.65
date	2014.07.13.23.24.47;	author deraadt;	state Exp;
branches;
next	1.64;
commitid	rg2qHVAzxM4HfBBO;

1.64
date	2014.07.12.18.50.00;	author tedu;	state Exp;
branches;
next	1.63;
commitid	C8XZQyreqTUCeixA;

1.63
date	2014.01.21.05.40.32;	author mikeb;	state Exp;
branches;
next	1.62;

1.62
date	2014.01.21.05.38.49;	author mikeb;	state Exp;
branches;
next	1.61;

1.61
date	2013.10.31.10.32.38;	author mikeb;	state Exp;
branches;
next	1.60;

1.60
date	2013.03.27.16.42.05;	author tedu;	state Exp;
branches;
next	1.59;

1.59
date	2011.01.11.15.42.05;	author deraadt;	state Exp;
branches;
next	1.58;

1.58
date	2010.09.08.14.15.56;	author jsing;	state Exp;
branches;
next	1.57;

1.57
date	2010.08.08.04.10.49;	author jsing;	state Exp;
branches;
next	1.56;

1.56
date	2010.07.08.09.46.50;	author thib;	state Exp;
branches;
next	1.55;

1.55
date	2010.07.08.08.12.48;	author thib;	state Exp;
branches;
next	1.54;

1.54
date	2010.06.09.19.38.19;	author thib;	state Exp;
branches;
next	1.53;

1.53
date	2009.09.03.07.47.27;	author dlg;	state Exp;
branches;
next	1.52;

1.52
date	2008.10.30.23.55.22;	author dlg;	state Exp;
branches;
next	1.51;

1.51
date	2007.11.28.13.52.23;	author tedu;	state Exp;
branches;
next	1.50;

1.50
date	2007.11.25.15.56.16;	author tedu;	state Exp;
branches;
next	1.49;

1.49
date	2007.11.14.19.12.36;	author markus;	state Exp;
branches;
next	1.48;

1.48
date	2006.05.31.23.01.44;	author tedu;	state Exp;
branches;
next	1.47;

1.47
date	2006.03.04.21.33.39;	author brad;	state Exp;
branches;
next	1.46;

1.46
date	2004.12.21.10.07.34;	author mpf;	state Exp;
branches;
next	1.45;

1.45
date	2004.06.20.20.45.06;	author aaron;	state Exp;
branches;
next	1.44;

1.44
date	2003.06.03.15.28.06;	author beck;	state Exp;
branches;
next	1.43;

1.43
date	2003.02.19.03.41.31;	author jason;	state Exp;
branches;
next	1.42;

1.42
date	2002.11.21.19.34.25;	author jason;	state Exp;
branches;
next	1.41;

1.41
date	2002.07.17.23.52.38;	author art;	state Exp;
branches;
next	1.40;

1.40
date	2002.07.16.06.29.43;	author angelos;	state Exp;
branches;
next	1.39;

1.39
date	2002.07.16.06.12.46;	author angelos;	state Exp;
branches;
next	1.38;

1.38
date	2002.06.11.11.14.29;	author beck;	state Exp;
branches;
next	1.37;

1.37
date	2002.06.10.22.36.49;	author beck;	state Exp;
branches;
next	1.36;

1.36
date	2002.06.09.22.23.17;	author angelos;	state Exp;
branches;
next	1.35;

1.35
date	2002.04.23.22.20.47;	author deraadt;	state Exp;
branches;
next	1.34;

1.34
date	2002.04.23.19.13.04;	author deraadt;	state Exp;
branches;
next	1.33;

1.33
date	2002.03.04.21.23.39;	author deraadt;	state Exp;
branches;
next	1.32;

1.32
date	2002.01.23.01.33.07;	author art;	state Exp;
branches;
next	1.31;

1.31
date	2002.01.23.00.39.47;	author art;	state Exp;
branches;
next	1.30;

1.30
date	2001.11.13.18.54.32;	author deraadt;	state Exp;
branches
	1.30.2.1;
next	1.29;

1.29
date	2001.11.13.17.45.59;	author deraadt;	state Exp;
branches;
next	1.28;

1.28
date	2001.11.09.03.11.38;	author deraadt;	state Exp;
branches;
next	1.27;

1.27
date	2001.11.08.23.12.38;	author deraadt;	state Exp;
branches;
next	1.26;

1.26
date	2001.08.05.09.36.38;	author deraadt;	state Exp;
branches
	1.26.2.1;
next	1.25;

1.25
date	2001.06.27.05.49.33;	author angelos;	state Exp;
branches;
next	1.24;

1.24
date	2001.06.26.19.29.25;	author angelos;	state Exp;
branches;
next	1.23;

1.23
date	2001.06.25.17.52.36;	author angelos;	state Exp;
branches;
next	1.22;

1.22
date	2001.06.25.05.02.22;	author angelos;	state Exp;
branches;
next	1.21;

1.21
date	2001.06.23.21.00.48;	author angelos;	state Exp;
branches;
next	1.20;

1.20
date	2001.06.23.18.30.35;	author deraadt;	state Exp;
branches;
next	1.19;

1.19
date	2001.06.16.22.17.49;	author deraadt;	state Exp;
branches;
next	1.18;

1.18
date	2001.06.06.18.58.52;	author angelos;	state Exp;
branches;
next	1.17;

1.17
date	2001.05.13.15.39.26;	author deraadt;	state Exp;
branches;
next	1.16;

1.16
date	2001.05.05.00.31.34;	author angelos;	state Exp;
branches;
next	1.15;

1.15
date	2000.12.13.08.34.05;	author provos;	state Exp;
branches;
next	1.14;

1.14
date	2000.09.07.18.44.29;	author deraadt;	state Exp;
branches
	1.14.2.1;
next	1.13;

1.13
date	2000.08.19.13.43.23;	author nate;	state Exp;
branches;
next	1.12;

1.12
date	2000.07.03.20.38.34;	author angelos;	state Exp;
branches;
next	1.11;

1.11
date	2000.06.20.05.39.32;	author angelos;	state Exp;
branches;
next	1.10;

1.10
date	2000.06.18.08.37.10;	author angelos;	state Exp;
branches;
next	1.9;

1.9
date	2000.06.18.03.08.56;	author angelos;	state Exp;
branches;
next	1.8;

1.8
date	2000.06.18.03.07.11;	author angelos;	state Exp;
branches;
next	1.7;

1.7
date	2000.06.06.06.49.47;	author deraadt;	state Exp;
branches;
next	1.6;

1.6
date	2000.04.28.05.25.39;	author angelos;	state Exp;
branches;
next	1.5;

1.5
date	2000.04.28.05.21.45;	author deraadt;	state Exp;
branches;
next	1.4;

1.4
date	2000.04.23.05.01.53;	author angelos;	state Exp;
branches;
next	1.3;

1.3
date	2000.04.18.06.21.14;	author angelos;	state Exp;
branches;
next	1.2;

1.2
date	2000.03.19.06.53.40;	author deraadt;	state Exp;
branches
	1.2.2.1;
next	1.1;

1.1
date	2000.03.17.10.25.21;	author angelos;	state Exp;
branches;
next	;

1.2.2.1
date	2000.03.24.09.09.05;	author niklas;	state Exp;
branches;
next	1.2.2.2;

1.2.2.2
date	2001.05.14.22.06.51;	author niklas;	state Exp;
branches;
next	1.2.2.3;

1.2.2.3
date	2001.07.04.10.39.58;	author niklas;	state Exp;
branches;
next	1.2.2.4;

1.2.2.4
date	2001.10.31.03.11.47;	author nate;	state Exp;
branches;
next	1.2.2.5;

1.2.2.5
date	2001.11.13.21.05.48;	author niklas;	state Exp;
branches;
next	1.2.2.6;

1.2.2.6
date	2001.12.05.00.43.29;	author niklas;	state Exp;
branches;
next	1.2.2.7;

1.2.2.7
date	2002.03.06.02.07.09;	author niklas;	state Exp;
branches;
next	1.2.2.8;

1.2.2.8
date	2003.03.27.23.53.48;	author niklas;	state Exp;
branches;
next	1.2.2.9;

1.2.2.9
date	2003.06.07.11.02.27;	author ho;	state Exp;
branches;
next	;

1.14.2.1
date	2000.12.13.16.13.45;	author jason;	state Exp;
branches;
next	;

1.26.2.1
date	2001.12.14.21.48.07;	author jason;	state Exp;
branches;
next	;

1.30.2.1
date	2002.01.31.22.55.29;	author niklas;	state Exp;
branches;
next	1.30.2.2;

1.30.2.2
date	2002.06.11.03.28.34;	author art;	state Exp;
branches;
next	1.30.2.3;

1.30.2.3
date	2002.10.29.00.30.52;	author art;	state Exp;
branches;
next	1.30.2.4;

1.30.2.4
date	2003.05.19.21.53.13;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.79
log
@Reduce the per-packet allocation costs for crypto operations (cryptop)
by pre-allocating two cryptodesc objects and storing them in an array
instead of a linked list.  If more than two cryptodesc objects are
required use mallocarray to fetch them.  Adapt the drivers to the new
API.

This change results in one pool-get per ESP packet instead of three.
It also simplifies softraid crypto where more cryptodesc objects are
allocated than used.

From, with and ok markus@@, ok bluhm@@
"looks sane" mpi@@
@
text
@/*	$OpenBSD: crypto.c,v 1.78 2016/09/19 18:09:40 tedu Exp $	*/
/*
 * The author of this code is Angelos D. Keromytis (angelos@@cis.upenn.edu)
 *
 * This code was written by Angelos D. Keromytis in Athens, Greece, in
 * February 2000. Network Security Technologies Inc. (NSTI) kindly
 * supported the development of this code.
 *
 * Copyright (c) 2000, 2001 Angelos D. Keromytis
 *
 * Permission to use, copy, and modify this software with or without fee
 * is hereby granted, provided that this entire notice is included in
 * all source code copies of any software which is or includes a copy or
 * modification of this software.
 *
 * THIS SOFTWARE IS BEING PROVIDED "AS IS", WITHOUT ANY EXPRESS OR
 * IMPLIED WARRANTY. IN PARTICULAR, NONE OF THE AUTHORS MAKES ANY
 * REPRESENTATION OR WARRANTY OF ANY KIND CONCERNING THE
 * MERCHANTABILITY OF THIS SOFTWARE OR ITS FITNESS FOR ANY PARTICULAR
 * PURPOSE.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/malloc.h>
#include <sys/pool.h>

#include <crypto/cryptodev.h>

void crypto_init(void);

struct cryptocap *crypto_drivers = NULL;
int crypto_drivers_num = 0;

struct pool cryptop_pool;
struct pool cryptodesc_pool;

struct taskq *crypto_taskq;
struct taskq *crypto_taskq_mpsafe;

/*
 * Create a new session.
 */
int
crypto_newsession(u_int64_t *sid, struct cryptoini *cri, int hard)
{
	u_int32_t hid, lid, hid2 = -1;
	struct cryptocap *cpc;
	struct cryptoini *cr;
	int err, s, turn = 0;

	if (crypto_drivers == NULL)
		return EINVAL;

	s = splvm();

	/*
	 * The algorithm we use here is pretty stupid; just use the
	 * first driver that supports all the algorithms we need. Do
	 * a double-pass over all the drivers, ignoring software ones
	 * at first, to deal with cases of drivers that register after
	 * the software one(s) --- e.g., PCMCIA crypto cards.
	 *
	 * XXX We need more smarts here (in real life too, but that's
	 * XXX another story altogether).
	 */
	do {
		for (hid = 0; hid < crypto_drivers_num; hid++) {
			cpc = &crypto_drivers[hid];

			/*
			 * If it's not initialized or has remaining sessions
			 * referencing it, skip.
			 */
			if (cpc->cc_newsession == NULL ||
			    (cpc->cc_flags & CRYPTOCAP_F_CLEANUP))
				continue;

			if (cpc->cc_flags & CRYPTOCAP_F_SOFTWARE) {
				/*
				 * First round of search, ignore
				 * software drivers.
				 */
				if (turn == 0)
					continue;
			} else { /* !CRYPTOCAP_F_SOFTWARE */
				/* Second round of search, only software. */
				if (turn == 1)
					continue;
			}

			/* See if all the algorithms are supported. */
			for (cr = cri; cr; cr = cr->cri_next) {
				if (cpc->cc_alg[cr->cri_alg] == 0)
					break;
			}

			/*
			 * If even one algorithm is not supported,
			 * keep searching.
			 */
			if (cr != NULL)
				continue;

			/*
			 * If we had a previous match, see how it compares
			 * to this one. Keep "remembering" whichever is
			 * the best of the two.
			 */
			if (hid2 != -1) {
				/*
				 * Compare session numbers, pick the one
				 * with the lowest.
				 * XXX Need better metrics, this will
				 * XXX just do un-weighted round-robin.
				 */
				if (crypto_drivers[hid].cc_sessions <=
				    crypto_drivers[hid2].cc_sessions)
					hid2 = hid;
			} else {
				/*
				 * Remember this one, for future
                                 * comparisons.
				 */
				hid2 = hid;
			}
		}

		/*
		 * If we found something worth remembering, leave. The
		 * side-effect is that we will always prefer a hardware
		 * driver over the software one.
		 */
		if (hid2 != -1)
			break;

		turn++;

		/* If we only want hardware drivers, don't do second pass. */
	} while (turn <= 2 && hard == 0);

	hid = hid2;

	/*
	 * Can't do everything in one session.
	 *
	 * XXX Fix this. We need to inject a "virtual" session
	 * XXX layer right about here.
	 */

	if (hid == -1) {
		splx(s);
		return EINVAL;
	}

	/* Call the driver initialization routine. */
	lid = hid; /* Pass the driver ID. */
	err = crypto_drivers[hid].cc_newsession(&lid, cri);
	if (err == 0) {
		(*sid) = hid;
		(*sid) <<= 32;
		(*sid) |= (lid & 0xffffffff);
		crypto_drivers[hid].cc_sessions++;
	}

	splx(s);
	return err;
}

/*
 * Delete an existing session (or a reserved session on an unregistered
 * driver).
 */
int
crypto_freesession(u_int64_t sid)
{
	int err = 0, s;
	u_int32_t hid;

	if (crypto_drivers == NULL)
		return EINVAL;

	/* Determine two IDs. */
	hid = (sid >> 32) & 0xffffffff;

	if (hid >= crypto_drivers_num)
		return ENOENT;

	s = splvm();

	if (crypto_drivers[hid].cc_sessions)
		crypto_drivers[hid].cc_sessions--;

	/* Call the driver cleanup routine, if available. */
	if (crypto_drivers[hid].cc_freesession)
		err = crypto_drivers[hid].cc_freesession(sid);

	/*
	 * If this was the last session of a driver marked as invalid,
	 * make the entry available for reuse.
	 */
	if ((crypto_drivers[hid].cc_flags & CRYPTOCAP_F_CLEANUP) &&
	    crypto_drivers[hid].cc_sessions == 0)
		explicit_bzero(&crypto_drivers[hid], sizeof(struct cryptocap));

	splx(s);
	return err;
}

/*
 * Find an empty slot.
 */
int32_t
crypto_get_driverid(u_int8_t flags)
{
	struct cryptocap *newdrv;
	int i, s;
	
	s = splvm();

	if (crypto_drivers_num == 0) {
		crypto_drivers_num = CRYPTO_DRIVERS_INITIAL;
		crypto_drivers = mallocarray(crypto_drivers_num,
		    sizeof(struct cryptocap), M_CRYPTO_DATA, M_NOWAIT | M_ZERO);
		if (crypto_drivers == NULL) {
			crypto_drivers_num = 0;
			splx(s);
			return -1;
		}
	}

	for (i = 0; i < crypto_drivers_num; i++) {
		if (crypto_drivers[i].cc_process == NULL &&
		    !(crypto_drivers[i].cc_flags & CRYPTOCAP_F_CLEANUP) &&
		    crypto_drivers[i].cc_sessions == 0) {
			crypto_drivers[i].cc_sessions = 1; /* Mark */
			crypto_drivers[i].cc_flags = flags;
			splx(s);
			return i;
		}
	}

	/* Out of entries, allocate some more. */
	if (i == crypto_drivers_num) {
		if (crypto_drivers_num >= CRYPTO_DRIVERS_MAX) {
			splx(s);
			return -1;
		}

		newdrv = mallocarray(crypto_drivers_num,
		    2 * sizeof(struct cryptocap), M_CRYPTO_DATA, M_NOWAIT);
		if (newdrv == NULL) {
			splx(s);
			return -1;
		}

		memcpy(newdrv, crypto_drivers,
		    crypto_drivers_num * sizeof(struct cryptocap));
		bzero(&newdrv[crypto_drivers_num],
		    crypto_drivers_num * sizeof(struct cryptocap));

		newdrv[i].cc_sessions = 1; /* Mark */
		newdrv[i].cc_flags = flags;

		free(crypto_drivers, M_CRYPTO_DATA,
		    crypto_drivers_num * sizeof(struct cryptocap));

		crypto_drivers_num *= 2;
		crypto_drivers = newdrv;
		splx(s);
		return i;
	}

	/* Shouldn't really get here... */
	splx(s);
	return -1;
}

/*
 * Register a crypto driver. It should be called once for each algorithm
 * supported by the driver.
 */
int
crypto_register(u_int32_t driverid, int *alg,
    int (*newses)(u_int32_t *, struct cryptoini *),
    int (*freeses)(u_int64_t), int (*process)(struct cryptop *))
{
	int s, i;


	if (driverid >= crypto_drivers_num || alg == NULL ||
	    crypto_drivers == NULL)
		return EINVAL;
	
	s = splvm();

	for (i = 0; i <= CRYPTO_ALGORITHM_MAX; i++) {
		/*
		 * XXX Do some performance testing to determine
		 * placing.  We probably need an auxiliary data
		 * structure that describes relative performances.
		 */

		crypto_drivers[driverid].cc_alg[i] = alg[i];
	}


	crypto_drivers[driverid].cc_newsession = newses;
	crypto_drivers[driverid].cc_process = process;
	crypto_drivers[driverid].cc_freesession = freeses;
	crypto_drivers[driverid].cc_sessions = 0; /* Unmark */

	splx(s);

	return 0;
}

/*
 * Unregister a crypto driver. If there are pending sessions using it,
 * leave enough information around so that subsequent calls using those
 * sessions will correctly detect the driver being unregistered and reroute
 * the request.
 */
int
crypto_unregister(u_int32_t driverid, int alg)
{
	int i = CRYPTO_ALGORITHM_MAX + 1, s;
	u_int32_t ses;

	s = splvm();

	/* Sanity checks. */
	if (driverid >= crypto_drivers_num || crypto_drivers == NULL ||
	    ((alg <= 0 || alg > CRYPTO_ALGORITHM_MAX) &&
		alg != CRYPTO_ALGORITHM_MAX + 1) ||
	    crypto_drivers[driverid].cc_alg[alg] == 0) {
		splx(s);
		return EINVAL;
	}

	if (alg != CRYPTO_ALGORITHM_MAX + 1) {
		crypto_drivers[driverid].cc_alg[alg] = 0;

		/* Was this the last algorithm ? */
		for (i = 1; i <= CRYPTO_ALGORITHM_MAX; i++)
			if (crypto_drivers[driverid].cc_alg[i] != 0)
				break;
	}

	/*
	 * If a driver unregistered its last algorithm or all of them
	 * (alg == CRYPTO_ALGORITHM_MAX + 1), cleanup its entry.
	 */
	if (i == CRYPTO_ALGORITHM_MAX + 1 || alg == CRYPTO_ALGORITHM_MAX + 1) {
		ses = crypto_drivers[driverid].cc_sessions;
		bzero(&crypto_drivers[driverid], sizeof(struct cryptocap));
		if (ses != 0) {
			/*
			 * If there are pending sessions, just mark as invalid.
			 */
			crypto_drivers[driverid].cc_flags |= CRYPTOCAP_F_CLEANUP;
			crypto_drivers[driverid].cc_sessions = ses;
		}
	}
	splx(s);
	return 0;
}

/*
 * Add crypto request to a queue, to be processed by a kernel thread.
 */
int
crypto_dispatch(struct cryptop *crp)
{
	struct taskq *tq = crypto_taskq;
	int s;
	u_int32_t hid;

	s = splvm();
	hid = (crp->crp_sid >> 32) & 0xffffffff;
	if (hid < crypto_drivers_num) {
		if (crypto_drivers[hid].cc_flags & CRYPTOCAP_F_MPSAFE)
			tq = crypto_taskq_mpsafe;
	}
	splx(s);

	if (tq && !(crp->crp_flags & CRYPTO_F_NOQUEUE)) {
		task_set(&crp->crp_task, (void (*))crypto_invoke, crp);
		task_add(tq, &crp->crp_task);
	} else {
		crypto_invoke(crp);
	}

	return 0;
}

/*
 * Dispatch a crypto request to the appropriate crypto devices.
 */
int
crypto_invoke(struct cryptop *crp)
{
	u_int64_t nid;
	u_int32_t hid;
	int error;
	int s, i;

	/* Sanity checks. */
	if (crp == NULL || crp->crp_callback == NULL)
		return EINVAL;

	s = splvm();
	if (crp->crp_ndesc < 1 || crypto_drivers == NULL) {
		crp->crp_etype = EINVAL;
		crypto_done(crp);
		splx(s);
		return 0;
	}

	hid = (crp->crp_sid >> 32) & 0xffffffff;
	if (hid >= crypto_drivers_num)
		goto migrate;

	if (crypto_drivers[hid].cc_flags & CRYPTOCAP_F_CLEANUP) {
		crypto_freesession(crp->crp_sid);
		goto migrate;
	}

	if (crypto_drivers[hid].cc_process == NULL)
		goto migrate;

	crypto_drivers[hid].cc_operations++;
	crypto_drivers[hid].cc_bytes += crp->crp_ilen;

	error = crypto_drivers[hid].cc_process(crp);
	if (error) {
		if (error == ERESTART) {
			/* Unregister driver and migrate session. */
			crypto_unregister(hid, CRYPTO_ALGORITHM_MAX + 1);
			goto migrate;
		} else {
			crp->crp_etype = error;
		}
	}

	splx(s);
	return 0;

 migrate:
	/* Migrate session. */
	for (i = 0; i < crp->crp_ndesc - 1; i++)
		crp->crp_desc[i].CRD_INI.cri_next = &crp->crp_desc[i+1].CRD_INI;
	crp->crp_desc[crp->crp_ndesc].CRD_INI.cri_next = NULL;

	if (crypto_newsession(&nid, &(crp->crp_desc->CRD_INI), 0) == 0)
		crp->crp_sid = nid;

	crp->crp_etype = EAGAIN;
	crypto_done(crp);
	splx(s);
	return 0;
}

/*
 * Release a set of crypto descriptors.
 */
void
crypto_freereq(struct cryptop *crp)
{
	if (crp == NULL)
		return;

	if (crp->crp_ndescalloc > 2)
		free(crp->crp_desc, M_CRYPTO_DATA,
		    crp->crp_ndescalloc * sizeof(struct cryptodesc));
	pool_put(&cryptop_pool, crp);
}

/*
 * Acquire a set of crypto descriptors.
 */
struct cryptop *
crypto_getreq(int num)
{
	struct cryptop *crp;

	crp = pool_get(&cryptop_pool, PR_NOWAIT | PR_ZERO);
	if (crp == NULL)
		return NULL;

	crp->crp_desc = crp->crp_sdesc;
	crp->crp_ndescalloc = crp->crp_ndesc = num;

	if (num > 2) {
		crp->crp_desc = mallocarray(num, sizeof(struct cryptodesc),
		    M_CRYPTO_DATA, M_NOWAIT | M_ZERO);
		if (crp->crp_desc == NULL) {
			pool_put(&cryptop_pool, crp);
			return NULL;
		}
	}

	return crp;
}

void
crypto_init(void)
{
	crypto_taskq = taskq_create("crypto", 1, IPL_VM, 0);
	crypto_taskq_mpsafe = taskq_create("crynlk", 1, IPL_VM|IPL_MPSAFE, 0);

	pool_init(&cryptop_pool, sizeof(struct cryptop), 0, IPL_VM, 0,
	    "cryptop", NULL);
}

/*
 * Invoke the callback on behalf of the driver.
 */
void
crypto_done(struct cryptop *crp)
{
	crp->crp_flags |= CRYPTO_F_DONE;
	if (crp->crp_flags & CRYPTO_F_NOQUEUE) {
		/* not from the crypto queue, wakeup the userland process */
		crp->crp_callback(crp);
	} else {
		task_set(&crp->crp_task, (void (*))crp->crp_callback, crp);
		task_add(crypto_taskq, &crp->crp_task);
	}
}
@


1.78
log
@convert bcopy to memcpy. from david hill
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.77 2016/09/15 02:00:17 dlg Exp $	*/
a402 1
	struct cryptodesc *crd;
d406 1
a406 1
	int s;
d413 1
a413 1
	if (crp->crp_desc == NULL || crypto_drivers == NULL) {
d451 3
a453 2
	for (crd = crp->crp_desc; crd->crd_next; crd = crd->crd_next)
		crd->CRD_INI.cri_next = &(crd->crd_next->CRD_INI);
a469 2
	struct cryptodesc *crd;

d473 3
a475 5
	while ((crd = crp->crp_desc) != NULL) {
		crp->crp_desc = crd->crd_next;
		pool_put(&cryptodesc_pool, crd);
	}

a484 1
	struct cryptodesc *crd;
d486 1
a486 1
	
d491 8
a498 4
	while (num--) {
		crd = pool_get(&cryptodesc_pool, PR_NOWAIT | PR_ZERO);
		if (crd == NULL) {
			crypto_freereq(crp);
a500 3

		crd->crd_next = crp->crp_desc;
		crp->crp_desc = crd;
a513 2
	pool_init(&cryptodesc_pool, sizeof(struct cryptodesc), 0, IPL_VM, 0,
	    "cryptodesc", NULL);
@


1.77
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.76 2016/04/18 21:05:55 kettenis Exp $	*/
d257 1
a257 1
		bcopy(crypto_drivers, newdrv,
@


1.76
log
@Add a mechanism for dispatching mpsafe crypto operations.  This adds a new
CRYPTOCAP_F_MPSAFE flag that crypto implementations can set to indicate that
their cc_process() implementation can safely run without holding the kernel
lock.

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.75 2015/08/28 00:03:53 deraadt Exp $	*/
d516 4
a519 6
	pool_init(&cryptop_pool, sizeof(struct cryptop), 0, 0,
	    0, "cryptop", NULL);
	pool_setipl(&cryptop_pool, IPL_VM);
	pool_init(&cryptodesc_pool, sizeof(struct cryptodesc), 0, 0,
	    0, "cryptodesc", NULL);
	pool_setipl(&cryptodesc_pool, IPL_VM);
@


1.75
log
@fairly simple sizes for free(); ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.74 2015/02/09 03:15:41 dlg Exp $	*/
d39 1
d375 13
a387 1
	if (crypto_taskq && !(crp->crp_flags & CRYPTO_F_NOQUEUE)) {
d389 1
a389 1
		task_add(crypto_taskq, &crp->crp_task);
d514 1
@


1.74
log
@we want to defer work traditionally (in openbsd) handled in an
interrupt context to a taskq running in a thread. however, there
is a concern that if we do that then we allow accidental use of
sleeping APIs in this work, which will make it harder to move the
work back to interrupts in the future.

guenther and kettenis came up with the idea of marking a proc with
CANTSLEEP which the sleep paths can check and panic on.

this builds on that so you create taskqs that run with CANTSLEEP
set except when they need to sleep for more tasks to run.

the taskq_create api is changed to take a flags argument so users
can specify CANTSLEEP. MPSAFE is also passed via this flags field
now.  this means archs that defined IPL_MPSAFE to 0 can now create
mpsafe taskqs too.

lots of discussion at s2k15
ok guenther@@ miod@@ mpi@@ tedu@@ pelikan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.73 2015/01/27 03:17:35 dlg Exp $	*/
d263 4
a267 2

		free(crypto_drivers, M_CRYPTO_DATA, 0);
@


1.73
log
@remove the second void * argument on tasks.

when workqs were introduced, we provided a second argument so you
could pass a thing and some context to work on it in. there were
very few things that took advantage of the second argument, so when
i introduced pools i suggested removing it. since tasks were meant
to replace workqs, it was requested that we keep the second argument
to make porting from workqs to tasks easier.

now that workqs are gone, i had a look at the use of the second
argument again and found only one good use of it (vdsp(4) on sparc64
if you're interested) and a tiny handful of questionable uses. the
vast majority of tasks only used a single argument. i have since
modified all tasks that used two args to only use one, so now we
can remove the second argument.

so this is a mechanical change. all tasks only passed NULL as their
second argument, so we can just remove it.

ok krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.72 2014/10/23 00:15:09 dlg Exp $	*/
d498 1
a498 1
	crypto_taskq = taskq_create("crypto", 1, IPL_VM);
@


1.72
log
@pools lock themselves now, we just have to tell them what IPL they
will be used from.

this adds pool_setipl at IPL_VM to the crypto descriptor pools, and
removes all the splvm handling around the use of those pools.

tested by many via tech@@
ok kettenis@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.71 2014/10/23 00:11:48 dlg Exp $	*/
d373 1
a373 1
		task_set(&crp->crp_task, (void (*))crypto_invoke, crp, NULL);
d519 1
a519 2
		task_set(&crp->crp_task, (void (*))crp->crp_callback,
		    crp, NULL);
@


1.71
log
@apply only the bit of r1.69 that should have been committed:

make the crypto taskq protect things at IPL_VM instead of IPL_HIGH.

everything else in crypto.c uses splvm/IPL_VM. it seems this IPL_HIGH
came about because the hand rolled task list and thread that crypto
used to use was converted to workqs, which unconditionally used
IPL_HIGH internally. when it was converted from workqs to tasks it
blindly ported the protection workqs gave.

tested by many via tech@@ and snapshots
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.70 2014/10/23 00:10:09 dlg Exp $	*/
a455 1
	int s;
a459 2
	s = splvm();

a465 1
	splx(s);
a475 1
	int s;
a476 2
	s = splvm();

d478 1
a478 2
	if (crp == NULL) {
		splx(s);
a479 1
	}
a483 1
			splx(s);
a491 1
	splx(s);
d502 1
d505 1
@


1.70
log
@revert previous. it did more than the commit message said it did.
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.68 2014/10/20 00:40:33 dlg Exp $	*/
d509 1
a509 1
	crypto_taskq = taskq_create("crypto", 1, IPL_HIGH);
@


1.69
log
@make the crypto taskq protect things at IPL_VM instead of IPL_HIGH.

everything else in crypto.c uses splvm/IPL_VM. it seems this IPL_HIGH
came about because the hand rolled task list and thread that crypto
used to use was converted to workqs, which unconditionally used
IPL_HIGH internally. when it was converted from workqs to tasks it
blindly ported the protection workqs gave.

tested by many via tech@@ and snapshots
ok kettenis@@
@
text
@d456 1
d461 2
d469 1
d480 1
d482 2
d485 2
a486 1
	if (crp == NULL)
d488 1
d493 1
d502 1
d509 1
a509 1
	crypto_taskq = taskq_create("crypto", 1, IPL_VM);
a512 1
	pool_setipl(&cryptop_pool, IPL_VM);
a514 1
	pool_setipl(&cryptodesc_pool, IPL_VM);
@


1.68
log
@replace bzeros after allocations with M_ZERO and PR_ZERO as appropriate.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.67 2014/09/14 14:17:23 jsg Exp $	*/
a455 1
	int s;
a459 2
	s = splvm();

a465 1
	splx(s);
a475 1
	int s;
a476 2
	s = splvm();

d478 1
a478 2
	if (crp == NULL) {
		splx(s);
a479 1
	}
a483 1
			splx(s);
a491 1
	splx(s);
d498 1
a498 1
	crypto_taskq = taskq_create("crypto", 1, IPL_HIGH);
d502 1
d505 1
@


1.67
log
@remove uneeded proc.h includes
ok mpi@@ kspillner@@
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.66 2014/08/20 06:23:03 mikeb Exp $	*/
d223 1
a223 1
		    sizeof(struct cryptocap), M_CRYPTO_DATA, M_NOWAIT);
a228 3

		bzero(crypto_drivers, crypto_drivers_num *
		    sizeof(struct cryptocap));
d484 1
a484 1
	crp = pool_get(&cryptop_pool, PR_NOWAIT);
a488 1
	bzero(crp, sizeof(struct cryptop));
d491 1
a491 1
		crd = pool_get(&cryptodesc_pool, PR_NOWAIT);
a497 1
		bzero(crd, sizeof(struct cryptodesc));
@


1.66
log
@Bye bye /dev/crypto

The interface has been disabled by default for about 4 years and
currently there's not much value in having it around at all.

ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.65 2014/07/13 23:24:47 deraadt Exp $	*/
a25 1
#include <sys/proc.h>
@


1.65
log
@use mallocarray()
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.64 2014/07/12 18:50:00 tedu Exp $	*/
a284 29
crypto_kregister(u_int32_t driverid, int *kalg,
    int (*kprocess)(struct cryptkop *))
{
	int s, i;

	if (driverid >= crypto_drivers_num || kalg  == NULL ||
	    crypto_drivers == NULL)
		return EINVAL;

	s = splvm();

	for (i = 0; i <= CRK_ALGORITHM_MAX; i++) {
		/*
		 * XXX Do some performance testing to determine
		 * placing.  We probably need an auxiliary data
		 * structure that describes relative performances.
		 */

		crypto_drivers[driverid].cc_kalg[i] = kalg[i];
	}

	crypto_drivers[driverid].cc_kprocess = kprocess;

	splx(s);
	return 0;
}

/* Register a crypto driver. */
int
a385 61
int
crypto_kdispatch(struct cryptkop *krp)
{
	if (crypto_taskq) {
		task_set(&krp->krp_task, (void (*))crypto_kinvoke, krp, NULL);
		task_add(crypto_taskq, &krp->krp_task);
	} else {
		crypto_kinvoke(krp);
	}

	return 0;
}

/*
 * Dispatch an asymmetric crypto request to the appropriate crypto devices.
 */
int
crypto_kinvoke(struct cryptkop *krp)
{
	extern int cryptodevallowsoft;
	u_int32_t hid;
	int error;
	int s;

	/* Sanity checks. */
	if (krp == NULL || krp->krp_callback == NULL)
		return (EINVAL);

	s = splvm();
	for (hid = 0; hid < crypto_drivers_num; hid++) {
		if ((crypto_drivers[hid].cc_flags & CRYPTOCAP_F_SOFTWARE) &&
		    cryptodevallowsoft == 0)
			continue;
		if (crypto_drivers[hid].cc_kprocess == NULL)
			continue;
		if ((crypto_drivers[hid].cc_kalg[krp->krp_op] &
		    CRYPTO_ALG_FLAG_SUPPORTED) == 0)
			continue;
		break;
	}

	if (hid == crypto_drivers_num) {
		krp->krp_status = ENODEV;
		crypto_kdone(krp);
		splx(s);
		return (0);
	}

	krp->krp_hid = hid;

	crypto_drivers[hid].cc_koperations++;

	error = crypto_drivers[hid].cc_kprocess(krp);
	if (error) {
		krp->krp_status = error;
		crypto_kdone(krp);
	}
	splx(s);
	return (0);
}

a537 35
}

/*
 * Invoke the callback on behalf of the driver.
 */
void
crypto_kdone(struct cryptkop *krp)
{
	task_set(&krp->krp_task, (void (*))krp->krp_callback, krp, NULL);
	task_add(crypto_taskq, &krp->krp_task);
}

int
crypto_getfeat(int *featp)
{
	extern int cryptodevallowsoft, userasymcrypto;
	int hid, kalg, feat = 0;

	if (userasymcrypto == 0)
		goto out;	  
	for (hid = 0; hid < crypto_drivers_num; hid++) {
		if ((crypto_drivers[hid].cc_flags & CRYPTOCAP_F_SOFTWARE) &&
		    cryptodevallowsoft == 0) {
			continue;
		}
		if (crypto_drivers[hid].cc_kprocess == NULL)
			continue;
		for (kalg = 0; kalg <= CRK_ALGORITHM_MAX; kalg++)
			if ((crypto_drivers[hid].cc_kalg[kalg] &
			    CRYPTO_ALG_FLAG_SUPPORTED) != 0)
				feat |=  1 << kalg;
	}
out:
	*featp = feat;
	return (0);
@


1.64
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.63 2014/01/21 05:40:32 mikeb Exp $	*/
d223 1
a223 1
		crypto_drivers = malloc(crypto_drivers_num *
d253 2
a254 2
		newdrv = malloc(2 * crypto_drivers_num *
		    sizeof(struct cryptocap), M_CRYPTO_DATA, M_NOWAIT);
@


1.63
log
@cc_queued is not used for anything atm, remove it;  ok jsing, markus
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.62 2014/01/21 05:38:49 mikeb Exp $	*/
d269 1
a269 1
		free(crypto_drivers, M_CRYPTO_DATA);
@


1.62
log
@Respect CRYPTO_F_NOQUEUE flag when dispatching a crypto operation

ok jsing, markus
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.61 2013/10/31 10:32:38 mikeb Exp $	*/
a404 14
	int s;
	u_int32_t hid;

	s = splvm();
	/*
	 * Keep track of ops per driver, for coallescing purposes. If
	 * we have been given an invalid hid, we'll deal with in the
	 * crypto_invoke(), through session migration.
	 */
	hid = (crp->crp_sid >> 32) & 0xffffffff;
	if (hid < crypto_drivers_num)
		crypto_drivers[hid].cc_queued++;
	splx(s);

a502 2

	crypto_drivers[hid].cc_queued--;
@


1.61
log
@convert crypto work queue to the task_add(9) api;  ok dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.60 2013/03/27 16:42:05 tedu Exp $	*/
d419 1
a419 1
	if (crypto_taskq) {
@


1.60
log
@institute a hard cap on crypto devs instead of a useless wraparound check
ok beck
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.59 2011/01/11 15:42:05 deraadt Exp $	*/
d39 1
a39 1
struct workq *crypto_workq;
d419 3
a421 3
	if (crypto_workq) {
		workq_queue_task(crypto_workq, &crp->crp_wqt, 0,
		    (workq_fn)crypto_invoke, crp, NULL);
d432 3
a434 3
	if (crypto_workq) {
		workq_queue_task(crypto_workq, &krp->krp_wqt, 0,
		    (workq_fn)crypto_kinvoke, krp, NULL);
d621 1
a621 1
	crypto_workq = workq_create("crypto", 1, IPL_HIGH);
d640 3
a642 2
		workq_queue_task(crypto_workq, &crp->crp_wqt, 0,
		    (workq_fn)crp->crp_callback, crp, NULL);
d652 2
a653 2
	workq_queue_task(crypto_workq, &krp->krp_wqt, 0,
	    (workq_fn)krp->krp_callback, krp, NULL);
@


1.59
log
@for key material that is being being discarded, convert bzero() to
explicit_bzero() where required
ok markus mikeb
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.58 2010/09/08 14:15:56 jsing Exp $	*/
d248 1
a248 2
		/* Be careful about wrap-around. */
		if (2 * crypto_drivers_num <= crypto_drivers_num) {
@


1.58
log
@Reintroduce most crypto/crypto.c r1.55:

Move pool initialization to init_crypto and zap the crypto_pool_initialized
variable. This way we don't have to check if the pool are initialized every
time we do a crypto_getreq().

However, also perform the crypto initialisation earlier in init_main so
that the crypto pools are initialised before they are used.

ok mikeb@@ thib@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.52 2008/10/30 23:55:22 dlg Exp $	*/
d204 1
a204 1
		bzero(&crypto_drivers[hid], sizeof(struct cryptocap));
@


1.57
log
@Backout r1.55 since this breaks anything which does crypto ops prior to
init_crypto() being called from late in init_main(). In particular, this
breaks softraid crypto volumes that are assembled at boot.

No cookies for thib/mikeb!

"Back it out, right now" deraadt@@
@
text
@d31 1
a31 1
void init_crypto(void);
a37 1
int crypto_pool_initialized = 0;
a594 8
	if (crypto_pool_initialized == 0) {
		pool_init(&cryptop_pool, sizeof(struct cryptop), 0, 0,
		    0, "cryptop", NULL);
		pool_init(&cryptodesc_pool, sizeof(struct cryptodesc), 0, 0,
		    0, "cryptodesc", NULL);
		crypto_pool_initialized = 1;
	}

d620 1
a620 1
init_crypto()
d623 5
@


1.56
log
@Revert part of previous.

The splvm protection is needed after all, as we are walking the list
of registered crypto drivers and doing that unprotected is unwise.

Pointed out by kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.55 2010/07/08 08:12:48 thib Exp $	*/
d38 1
d596 8
d629 1
a629 1
init_crypto(void)
a630 4
	pool_init(&cryptop_pool, sizeof(struct cryptop), 0, 0, 0,
	    "cryptop", NULL);
	pool_init(&cryptodesc_pool, sizeof(struct cryptodesc), 0, 0, 0,
	    "cryptodesc", NULL);
@


1.55
log
@Move pool initialization to init_crypto and zap the crypto_pool_initialized
variable. This way we don't have to check if the pool are initialized every
time we do a crypto_getreq().

Move splvm lower as it isnt need all through crypto_newsession().

tiny KNF nit.

From mikeb

OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.54 2010/06/09 19:38:19 thib Exp $	*/
d55 2
d151 2
a152 1
	if (hid == -1)
d154 1
a154 2

	s = splvm();
@


1.54
log
@Remove the CRYPTO_ALGORITHM_ALL define, fixup accordingly
and make the loop invartiants <= CRYPTO_ALGORITHM_MAX
Do this also for the CRK_ALGORITHM_MAX this also fixes
the a bug that caused us to skip CRK_DH_COMPUTE_KEY.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.52 2008/10/30 23:55:22 dlg Exp $	*/
a37 1
int crypto_pool_initialized = 0;
a54 2
	s = splvm();

d149 1
a149 2
	if (hid == -1) {
		splx(s);
d151 2
a152 1
	}
a592 8
	if (crypto_pool_initialized == 0) {
		pool_init(&cryptop_pool, sizeof(struct cryptop), 0, 0,
		    0, "cryptop", NULL);
		pool_init(&cryptodesc_pool, sizeof(struct cryptodesc), 0, 0,
		    0, "cryptodesc", NULL);
		crypto_pool_initialized = 1;
	}

d618 1
a618 1
init_crypto()
d620 4
@


1.53
log
@crypto hardware (eg, hifn) establishes its interrupt handler at
IPL_NET. when the hardware finishes some work for the crypto subsystem
and therefore something in the kernel that wanted crypto done, it
calls crypto_done from that interrupt handler.

one of the things that uses crypto is ipsec. when crypto is done
for ipsec it then pushes the packet along the network stack. the
problem is that all the structures inside the network stack are
only protected at splsoftnet. we could be in the middle of modifications
to the pf state table or the pfsync queues when we get a hifn
interrupt and then go stomp on the same structures.

the solution is to defer the completions so they can do the right
spl protections.

this basically reverts r1.46 of src/sys/crypto/crypto.c.

found by naddy@@
@
text
@d298 1
a298 1
	for (i = 0; i < CRK_ALGORITHM_MAX; i++) {
d329 1
a329 1
	for (i = 0; i < CRYPTO_ALGORITHM_ALL; i++) {
d367 1
a367 1
		alg != CRYPTO_ALGORITHM_ALL) ||
d373 1
a373 1
	if (alg != CRYPTO_ALGORITHM_ALL) {
d384 1
a384 1
	 * (alg == CRYPTO_ALGORITHM_ALL), cleanup its entry.
d386 1
a386 1
	if (i == CRYPTO_ALGORITHM_MAX + 1 || alg == CRYPTO_ALGORITHM_ALL) {
d537 1
a537 1
			crypto_unregister(hid, CRYPTO_ALGORITHM_ALL);
d675 1
a675 1
		for (kalg = 0; kalg < CRK_ALGORITHM_MAX; kalg++)
@


1.52
log
@reintroduce mutexes to workqs for locking.

tested by many on many archs including several alpha test.

ok tedu@@ go for it deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.50 2007/11/25 15:56:16 tedu Exp $	*/
a27 1
#include <sys/workq.h>
d422 1
a422 1
		workq_add_task(crypto_workq, 0,
a433 1

d435 1
a435 1
		workq_add_task(crypto_workq, 0,
d641 7
a647 1
	crp->crp_callback(crp);
d656 2
a657 1
	krp->krp_callback(krp);
@


1.51
log
@finish conversion to workq.  remove list remnants, and put spl in the right
places.  handle the no workq case here.  ok deraadt
@
text
@d633 1
a633 1
	crypto_workq = workq_create("crypto", 1);
@


1.50
log
@convert crypto thread to workq.  add WQ_DIRECTOK flag to workq.
combined, this lets us use crypto before the thread is running
and therefore cryptoraid can attach nice and early.
ok/testing deraadt mbalmer marco
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.49 2007/11/14 19:12:36 markus Exp $	*/
d420 1
d422 6
a427 2
	crp->crp_next = NULL;
	workq_add_task(crypto_workq, WQ_DIRECTOK, (workq_fn)crypto_invoke, crp, NULL);
a428 1
	splx(s);
a434 3
	int s;
	
	s = splvm();
d436 6
a441 2
	krp->krp_next = NULL;
	workq_add_task(crypto_workq, WQ_DIRECTOK, (workq_fn)crypto_kinvoke, krp, NULL);
a442 1
	splx(s);
d455 1
d461 1
d477 1
d490 1
d504 1
d510 1
d514 1
d546 1
d559 1
@


1.49
log
@do not call crypto_done() on errors, since the drivers already do this.
otherwise we call the callback twice; fixes panics on crypto errors as
seen on reboot; ok hshoexer
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.48 2006/05/31 23:01:44 tedu Exp $	*/
d28 2
d32 2
d41 1
a41 5
struct cryptop *crp_req_queue = NULL;
struct cryptop **crp_req_queue_tail = NULL;

struct cryptkop *krp_req_queue = NULL;
struct cryptkop **krp_req_queue_tail = NULL;
d422 3
a424 10
	if (crp_req_queue == NULL) {
		crp_req_queue = crp;
		crp_req_queue_tail = &(crp->crp_next);
		splx(s);
		wakeup(&crp_req_queue); /* Shared wait channel. */
	} else {
		*crp_req_queue_tail = crp;
		crp_req_queue_tail = &(crp->crp_next);
		splx(s);
	}
d436 3
a438 10
	if (krp_req_queue == NULL) {
		krp_req_queue = krp;
		krp_req_queue_tail = &(krp->krp_next);
		splx(s);
		wakeup(&crp_req_queue); /* Shared wait channel. */
	} else {
		*krp_req_queue_tail = krp;
		krp_req_queue_tail = &(krp->krp_next);
		splx(s);
	}
a616 3
/*
 * Crypto thread, runs as a kernel thread to process crypto requests.
 */
d618 1
a618 1
crypto_thread(void)
d620 1
a620 25
	struct cryptop *crp;
	struct cryptkop *krp;
	int s;

	s = splvm();

	for (;;) {
		crp = crp_req_queue;
		krp = krp_req_queue;
		if (crp == NULL && krp == NULL) {
			(void)tsleep(&crp_req_queue, PLOCK, "crypto_wait", 0);
			continue;
		}

		if (crp) {
			/* Remove from the queue. */
			crp_req_queue = crp->crp_next;
			crypto_invoke(crp);
		}
		if (krp) {
			/* Remove from the queue. */
			krp_req_queue = krp->krp_next;
			crypto_kinvoke(krp);
		}
	}
@


1.48
log
@remove some silly casts.  put spl calls after all declarations.
put one splx in a better spot.  make a variable size MALLOC use malloc.
remove null test after malloc(M_WAITOK).
add PR_NOWAIT flag to pool_get instead of 0.  change callbacks to correct type.
ok brad deraadt markus mickey
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.47 2006/03/04 21:33:39 brad Exp $	*/
a545 1
			crypto_done(crp);
@


1.47
log
@splimp -> splvm

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.46 2004/12/21 10:07:34 mpf Exp $	*/
d219 3
a221 1
	int i, s = splvm();
d228 1
a229 1
			crypto_drivers_num = 0;
d360 1
a360 1
	int i = CRYPTO_ALGORITHM_MAX + 1, s = splvm();
d363 2
d408 1
a408 1
	int s = splvm();
d411 1
d426 1
a426 1
		wakeup((caddr_t) &crp_req_queue); /* Shared wait channel. */
d438 3
a440 1
	int s = splvm();
d447 1
a447 1
		wakeup((caddr_t) &crp_req_queue); /* Shared wait channel. */
d596 3
a598 1
	int s = splvm();
d608 1
a608 1
	crp = pool_get(&cryptop_pool, 0);
d616 1
a616 1
		crd = pool_get(&cryptodesc_pool, 0);
d648 1
a648 1
			(void) tsleep(&crp_req_queue, PLOCK, "crypto_wait", 0);
@


1.46
log
@Don't use crypto thread for callbacks.
This primarily improves IPsec performance when using crypto accelerators.
With help from markus@@, tested by wvdputte@@.

ok deraadt@@, markus@@
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.45 2004/06/20 20:45:06 aaron Exp $	*/
d57 1
a57 1
	s = splimp();
d191 1
a191 1
	s = splimp();
d219 1
a219 1
	int i, s = splimp();
d295 1
a295 1
	s = splimp();
d326 1
a326 1
	s = splimp();
d358 1
a358 1
	int i = CRYPTO_ALGORITHM_MAX + 1, s = splimp();
d404 1
a404 1
	int s = splimp();
d433 1
a433 1
	int s = splimp();
d570 1
a570 1
	s = splimp();
d589 1
a589 1
	int s = splimp();
d633 1
a633 1
	s = splimp();
@


1.45
log
@In crypto_thread(), always save return value from splimp().  We were only
storing it once on kernel startup.  Scary.  "holy crap" --deraadt.  art@@ ok

Unclear if this was actually a problem in practice, but this doesn't hurt.
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.44 2003/06/03 15:28:06 beck Exp $	*/
a39 3
struct cryptop *crp_ret_queue = NULL;
struct cryptop **crp_ret_queue_tail = NULL;

a42 3
struct cryptkop *krp_ret_queue = NULL;
struct cryptkop **krp_ret_queue_tail = NULL;

d629 2
a630 2
	struct cryptop *crp, *crpt;
	struct cryptkop *krp, *krpt;
d638 1
a638 4
		crpt = crp_ret_queue;
		krpt = krp_ret_queue;
		if (crp == NULL && krp == NULL &&
		    crpt == NULL && krpt == NULL) {
a652 18
		if (crpt) {
			/* Remove from the queue. */
			crp_ret_queue = crpt->crp_next;
			splx(s);
			crpt->crp_callback(crpt);
			s = splimp();
		}
		if (krpt) {
			/* Remove from the queue. */
			krp_ret_queue = krpt->krp_next;
			/*
			 * Cheat. For public key ops, we know that
			 * all that's done is a wakeup() for the
			 * userland process, so don't bother to
			 * change the processor priority.
			 */
			krpt->krp_callback(krpt);
		}
d662 2
a663 23
	int s;

	if (crp->crp_flags & CRYPTO_F_NOQUEUE) {
		/* not from the crypto queue, wakeup the userland
		 * process 
		 */
		crp->crp_flags |= CRYPTO_F_DONE;
		crp->crp_callback(crp);
	} else {
		s = splimp();
		crp->crp_flags |= CRYPTO_F_DONE;
		crp->crp_next = NULL;
		if (crp_ret_queue == NULL) {
			crp_ret_queue = crp;
			crp_ret_queue_tail = &(crp->crp_next);
			splx(s);
			wakeup((caddr_t) &crp_req_queue); /* Shared wait channel. */
		} else {
			*crp_ret_queue_tail = crp;
			crp_ret_queue_tail = &(crp->crp_next);
			splx(s);
		}
	}
d672 1
a672 13
	int s = splimp();

	krp->krp_next = NULL;
	if (krp_ret_queue == NULL) {
		krp_ret_queue = krp;
		krp_ret_queue_tail = &(krp->krp_next);
		splx(s);
		wakeup((caddr_t) &crp_req_queue); /* Shared wait channel. */
	} else {
		*krp_ret_queue_tail = krp;
		krp_ret_queue_tail = &(krp->krp_next);
		splx(s);
	}
@


1.44
log
@Fastpath for userland crypto requests. This change makes userland
crypto requests attempt to call the crypto driver directly to process
crypto layer requests, as opposed to queueing them in the kernel
crypto thread. If we can't use the crypto devices (i.e. they're busy)
we fall back to queueing the request up in the crypto thread as
before. This does allow for faster performance in some cases (smaller
requests, how small seems to be dependent on the card/cpu combination)
where context switching is a major issue in performance.
ok deraadt@@ jason@@
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.43 2003/02/19 03:41:31 jason Exp $	*/
d667 1
a667 1
			splimp();
@


1.43
log
@Copy the ENTIRE table into the supported algorithms (how the hell did this
work before?!)
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.42 2002/11/21 19:34:25 jason Exp $	*/
d689 1
a689 1
	int s = splimp();
d691 6
a696 6
	crp->crp_next = NULL;
	if (crp_ret_queue == NULL) {
		crp_ret_queue = crp;
		crp_ret_queue_tail = &(crp->crp_next);
		splx(s);
		wakeup((caddr_t) &crp_req_queue); /* Shared wait channel. */
d698 13
a710 3
		*crp_ret_queue_tail = crp;
		crp_ret_queue_tail = &(crp->crp_next);
		splx(s);
@


1.42
log
@From Angelos:
- simplistic load balancing across multiple cards
- simplified registration process
- a few style nits.
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.41 2002/07/17 23:52:38 art Exp $	*/
d334 1
a334 1
	for (i = 0; i < CRYPTO_ALGORITHM_MAX; i++) {
@


1.41
log
@I don't know why this breaks things for me when sshd starts on sparc64.
But after wasting the whole day trying to just locate the problem I don't care.
Back out since this wasn't tested and showed to anyone else.
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.38 2002/06/11 11:14:29 beck Exp $	*/
d40 3
d46 3
d55 2
d58 1
a58 2
	u_int32_t hid, lid;
	int err, s;
d67 4
a70 1
	 * first driver that supports all the algorithms we need.
d75 61
a136 1
	for (hid = 0; hid < crypto_drivers_num; hid++) {
d138 3
a140 2
		 * If it's not initialized or has remaining sessions
		 * referencing it, skip.
d142 2
a143 3
		if (crypto_drivers[hid].cc_newsession == NULL ||
		    (crypto_drivers[hid].cc_flags & CRYPTOCAP_F_CLEANUP))
			continue;
d145 1
a145 4
		/* Hardware requested -- ignore software drivers. */
		if (hard &&
		    (crypto_drivers[hid].cc_flags & CRYPTOCAP_F_SOFTWARE))
			continue;
d147 2
a148 4
		/* See if all the algorithms are supported. */
		for (cr = cri; cr; cr = cr->cri_next)
			if (crypto_drivers[hid].cc_alg[cr->cri_alg] == 0)
				break;
d150 1
a150 4
		/* Ok, all algorithms are supported. */
		if (cr == NULL)
			break;
	}
d155 2
a156 2
	 * XXX Fix this. We need to inject a "virtual" session layer right
	 * XXX about here.
d159 1
a159 1
	if (hid == crypto_drivers_num) {
d292 1
a292 1
crypto_kregister(u_int32_t driverid, int kalg, u_int32_t flags,
d295 1
a295 1
	int s;
d297 2
a298 2
	if (driverid >= crypto_drivers_num || kalg < 0 ||
	    kalg > CRK_ALGORITHM_MAX || crypto_drivers == NULL)
d303 6
a308 5
	/*
	 * XXX Do some performance testing to determine placing.
	 * XXX We probably need an auxiliary data structure that describes
	 * XXX relative performances.
	 */
d310 2
a311 2
	crypto_drivers[driverid].cc_kalg[kalg] =
	    flags | CRYPTO_ALG_FLAG_SUPPORTED;
d313 1
a313 2
	if (crypto_drivers[driverid].cc_kprocess == NULL)
		crypto_drivers[driverid].cc_kprocess = kprocess;
d319 1
a319 4
/*
 * Register a crypto driver. It should be called once for each algorithm
 * supported by the driver.
 */
d321 1
a321 2
crypto_register(u_int32_t driverid, int alg, u_int16_t maxoplen,
    u_int32_t flags,
d325 2
a326 1
	int s;
d328 2
a329 2
	if (driverid >= crypto_drivers_num || alg <= 0 ||
	    alg > CRYPTO_ALGORITHM_MAX || crypto_drivers == NULL)
d331 1
a331 1

d334 6
a339 5
	/*
	 * XXX Do some performance testing to determine placing.
	 * XXX We probably need an auxiliary data structure that describes
	 * XXX relative performances.
	 */
d341 2
a342 2
	crypto_drivers[driverid].cc_alg[alg] =
	    flags | CRYPTO_ALG_FLAG_SUPPORTED;
a343 1
	crypto_drivers[driverid].cc_max_op_len[alg] = maxoplen;
d345 4
a348 6
	if (crypto_drivers[driverid].cc_process == NULL) {
		crypto_drivers[driverid].cc_newsession = newses;
		crypto_drivers[driverid].cc_process = process;
		crypto_drivers[driverid].cc_freesession = freeses;
		crypto_drivers[driverid].cc_sessions = 0; /* Unmark */
	}
d351 1
d364 1
a364 1
	int i, s = splimp();
d367 4
a370 3
	/* Sanity checks */
	if (driverid >= crypto_drivers_num || alg <= 0 ||
	    alg > CRYPTO_ALGORITHM_MAX || crypto_drivers == NULL ||
d376 2
a377 2
	crypto_drivers[driverid].cc_alg[alg] = 0;
	crypto_drivers[driverid].cc_max_op_len[alg] = 0;
d379 5
a383 4
	/* Was this the last algorithm ? */
	for (i = 1; i <= CRYPTO_ALGORITHM_MAX; i++)
		if (crypto_drivers[driverid].cc_alg[i] != 0)
			break;
d385 5
a389 1
	if (i == CRYPTO_ALGORITHM_MAX + 1) {
d411 10
d422 1
d427 1
a427 1
		wakeup((caddr_t) &crp_req_queue);
d441 1
d446 1
a446 1
		wakeup((caddr_t) &crp_req_queue);	/* shared wait channel */
d456 1
a456 1
 * Dispatch an assymetric crypto request to the appropriate crypto devices.
d480 1
d486 1
d488 3
d521 2
a522 4
	if (hid >= crypto_drivers_num) {
		/* Migrate session. */
		for (crd = crp->crp_desc; crd->crd_next; crd = crd->crd_next)
			crd->CRD_INI.cri_next = &(crd->crd_next->CRD_INI);
d524 1
a524 2
		if (crypto_newsession(&nid, &(crp->crp_desc->CRD_INI), 0) == 0)
			crp->crp_sid = nid;
d526 3
a528 3
		crp->crp_etype = EAGAIN;
		crypto_done(crp);
		return 0;
d531 2
a532 7
	if (crypto_drivers[hid].cc_flags & CRYPTOCAP_F_CLEANUP)
		crypto_freesession(crp->crp_sid);

	if (crypto_drivers[hid].cc_process == NULL) {
		/* Migrate session. */
		for (crd = crp->crp_desc; crd->crd_next; crd = crd->crd_next)
			crd->CRD_INI.cri_next = &(crd->crd_next->CRD_INI);
d534 2
a535 7
		if (crypto_newsession(&nid, &(crp->crp_desc->CRD_INI), 0) == 0)
			crp->crp_sid = nid;

		crp->crp_etype = EAGAIN;
		crypto_done(crp);
		return 0;
	}
d539 8
a546 2
		crp->crp_etype = error;
		crypto_done(crp);
d548 13
d635 2
a636 2
	struct cryptop *crp;
	struct cryptkop *krp;
d644 4
a647 1
		if (crp == NULL && krp == NULL) {
d662 18
d689 13
a701 1
	crp->crp_callback(crp);
d710 13
a722 1
	krp->krp_callback(krp);
@


1.40
log
@Double-pass over drivers, first hardware only, then software (if we
are interested in software).
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.39 2002/07/16 06:12:46 angelos Exp $	*/
a48 1
	struct cryptocap *cpc;
a49 1
	int err, s, turn = 0;
d51 1
d60 1
a60 4
	 * first driver that supports all the algorithms we need. Do
	 * a double-pass over all the drivers, ignoring software ones
	 * at first, to deal with cases of drivers that register after
	 * the software one(s) --- e.g., PCMCIA crypto cards.
a64 3
	do {
		for (hid = 0; hid < crypto_drivers_num; hid++) {
			cpc = &crypto_drivers[hid];
d66 13
a78 25
			/*
			 * If it's not initialized or has remaining sessions
			 * referencing it, skip.
			 */
			if (cpc->cc_newsession == NULL ||
			    (cpc->cc_flags & CRYPTOCAP_F_CLEANUP))
				continue;

			if (cpc->cc_flags & CRYPTOCAP_F_SOFTWARE) {
				/*
				 * First round of search, ignore
				 * software drivers.
				 */
				if (turn == 0)
					continue;
			} else { /* !CRYPTOCAP_F_SOFTWARE */
				/* Second round of search, only software. */
				if (turn == 1)
					continue;
			}

			/* See if all the algorithms are supported. */
			for (cr = cri; cr; cr = cr->cri_next)
				if (cpc->cc_alg[cr->cri_alg] == 0)
					break;
d80 3
a82 2
			/* Ok, all algorithms are supported. */
			if (cr == NULL)
a83 1
		}
d85 4
a88 4
		turn++;

		/* If we only want hardware drivers, don't do second pass. */
	} while (turn <= 1 && hard != 0);
d93 2
a94 2
	 * XXX Fix this. We need to inject a "virtual" session
	 * XXX layer right about here.
d306 1
a306 1
	int i = CRYPTO_ALGORITHM_MAX + 1, s = splimp();
d310 2
a311 3
	if (driverid >= crypto_drivers_num || crypto_drivers == NULL ||
	    ((alg <= 0 || alg > CRYPTO_ALGORITHM_MAX) &&
		alg != CRYPTO_ALGORITHM_ALL) ||
d317 7
a323 9
	if (alg != CRYPTO_ALGORITHM_ALL) {
		crypto_drivers[driverid].cc_alg[alg] = 0;
		crypto_drivers[driverid].cc_max_op_len[alg] = 0;

		/* Was this the last algorithm ? */
		for (i = 1; i <= CRYPTO_ALGORITHM_MAX; i++)
			if (crypto_drivers[driverid].cc_alg[i] != 0)
				break;
	}
d325 1
a325 5
	/*
	 * If a driver unregistered its last algorithm or all of them
	 * (alg == CRYPTO_ALGORITHM_ALL), cleanup its entry.
	 */
	if (i == CRYPTO_ALGORITHM_MAX + 1 || alg == CRYPTO_ALGORITHM_ALL) {
d380 1
a380 1
 * Dispatch an asymmetric crypto request to the appropriate crypto devices.
d440 12
a451 2
	if (hid >= crypto_drivers_num)
		goto migrate;
d456 12
a467 2
	if (crypto_drivers[hid].cc_process == NULL)
		goto migrate;
d471 2
a472 8
		if (error == ERESTART) {
			/* Unregister driver and migrate session. */
			crypto_unregister(hid, CRYPTO_ALGORITHM_ALL);
			goto migrate;
		} else {
			crp->crp_etype = error;
			crypto_done(crp);
		}
a473 13

	return 0;

 migrate:
	/* Migrate session. */
	for (crd = crp->crp_desc; crd->crd_next; crd = crd->crd_next)
		crd->CRD_INI.cri_next = &(crd->crd_next->CRD_INI);

	if (crypto_newsession(&nid, &(crp->crp_desc->CRD_INI), 0) == 0)
		crp->crp_sid = nid;

	crp->crp_etype = EAGAIN;
	crypto_done(crp);
@


1.39
log
@Fix a typo, cleanup on session migration code in crypto_invoke(), and
add a convention that if the driver returns ERESTART as an error
message of its process method, the crypto framework will unregister
the driver and migrate all its sessions. After discussion with Sam
Leffler and Jason Wright.
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.38 2002/06/11 11:14:29 beck Exp $	*/
d49 1
d51 1
a52 1
	int err, s;
d61 4
a64 1
	 * first driver that supports all the algorithms we need.
d69 29
d99 2
a100 17
	for (hid = 0; hid < crypto_drivers_num; hid++) {
		/*
		 * If it's not initialized or has remaining sessions
		 * referencing it, skip.
		 */
		if (crypto_drivers[hid].cc_newsession == NULL ||
		    (crypto_drivers[hid].cc_flags & CRYPTOCAP_F_CLEANUP))
			continue;

		/* Hardware requested -- ignore software drivers. */
		if (hard &&
		    (crypto_drivers[hid].cc_flags & CRYPTOCAP_F_SOFTWARE))
			continue;

		/* See if all the algorithms are supported. */
		for (cr = cri; cr; cr = cr->cri_next)
			if (crypto_drivers[hid].cc_alg[cr->cri_alg] == 0)
d102 1
d104 4
a107 4
		/* Ok, all algorithms are supported. */
		if (cr == NULL)
			break;
	}
d112 2
a113 2
	 * XXX Fix this. We need to inject a "virtual" session layer right
	 * XXX about here.
@


1.38
log
@kernel changes to make asymmetric crypto work in userland
- modify getfeat to return something more useful to us on devices
  (like lofn and everything else until jason fixes it) that can't
  do rsa stuff, etc and can only do mod_exp..
- error handling fixes so we correctly fail to software when we can't
  deal with a particular key size
- add sysctl kern.userasymcrypto to turn on/off userland asymmetric crypto
  via /dev/crypto - 1 == on, 0 == off, default is off
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.37 2002/06/10 22:36:49 beck Exp $	*/
d306 1
a306 1
	int i, s = splimp();
d310 3
a312 2
	if (driverid >= crypto_drivers_num || alg <= 0 ||
	    alg > CRYPTO_ALGORITHM_MAX || crypto_drivers == NULL ||
d318 9
a326 2
	crypto_drivers[driverid].cc_alg[alg] = 0;
	crypto_drivers[driverid].cc_max_op_len[alg] = 0;
d328 5
a332 6
	/* Was this the last algorithm ? */
	for (i = 1; i <= CRYPTO_ALGORITHM_MAX; i++)
		if (crypto_drivers[driverid].cc_alg[i] != 0)
			break;

	if (i == CRYPTO_ALGORITHM_MAX + 1) {
d387 1
a387 1
 * Dispatch an assymetric crypto request to the appropriate crypto devices.
d447 2
a448 12
	if (hid >= crypto_drivers_num) {
		/* Migrate session. */
		for (crd = crp->crp_desc; crd->crd_next; crd = crd->crd_next)
			crd->CRD_INI.cri_next = &(crd->crd_next->CRD_INI);

		if (crypto_newsession(&nid, &(crp->crp_desc->CRD_INI), 0) == 0)
			crp->crp_sid = nid;

		crp->crp_etype = EAGAIN;
		crypto_done(crp);
		return 0;
	}
d453 2
a454 12
	if (crypto_drivers[hid].cc_process == NULL) {
		/* Migrate session. */
		for (crd = crp->crp_desc; crd->crd_next; crd = crd->crd_next)
			crd->CRD_INI.cri_next = &(crd->crd_next->CRD_INI);

		if (crypto_newsession(&nid, &(crp->crp_desc->CRD_INI), 0) == 0)
			crp->crp_sid = nid;

		crp->crp_etype = EAGAIN;
		crypto_done(crp);
		return 0;
	}
d458 8
a465 2
		crp->crp_etype = error;
		crypto_done(crp);
d467 13
@


1.37
log
@fix ivory tower greek fix. ok angelos@@
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.36 2002/06/09 22:23:17 angelos Exp $	*/
d596 1
a596 2
	int kalgs[CRK_ALGORITHM_MAX];
	extern int cryptodevallowsoft;
d599 2
a600 2
	memset(kalgs, 0, sizeof(kalgs));

d610 2
a611 2
			    CRYPTO_ALG_FLAG_SUPPORTED) != 0) 
				kalgs[kalg] = 1;
d613 1
a613 7

	if (kalgs[CRK_MOD_EXP] && kalgs[CRK_MOD_EXP_CRT])
		feat |= CRSFEAT_RSA;
	if (kalgs[CRK_DSA_VERIFY] && kalgs[CRK_DSA_SIGN])
		feat |= CRSFEAT_DSA;
	if (kalgs[CRK_DH_COMPUTE_KEY] && kalgs[CRK_MOD_EXP])
		feat |= CRSFEAT_DH;
@


1.36
log
@Don't use an int for the flags, when the structure uses
u_int8_t. Also, make sure the logic is correct (bad theo!)
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.35 2002/04/23 22:20:47 deraadt Exp $	*/
d600 2
d604 1
a604 1
		    cryptodevallowsoft == 0)
d606 1
d611 1
a611 1
			    CRYPTO_ALG_FLAG_SUPPORTED) != 0)
a612 2
			else
				kalgs[kalg] = 0;
@


1.35
log
@initial hack at a CIOCSYMFEAT ioctl
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.34 2002/04/23 19:13:04 deraadt Exp $	*/
d608 1
a608 1
			    CRYPTO_ALG_FLAG_SUPPORTED) == 0)
d610 2
@


1.34
log
@driver queueing & callback code for keying operations
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.33 2002/03/04 21:23:39 deraadt Exp $	*/
d591 29
@


1.33
log
@crypto_check_alg() is not needed
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.33 2002/03/04 21:09:45 deraadt Exp $	*/
d14 1
a14 1
 * modification of this software. 
d40 3
d230 32
d281 1
a281 1
	crypto_drivers[driverid].cc_alg[alg] = 
d361 57
d427 1
d469 5
a473 1
	crypto_drivers[hid].cc_process(crp);
d549 1
d556 2
a557 1
		if (crp == NULL) {
d562 10
a571 3
		/* Remove from the queue. */
		crp_req_queue = crp->crp_next;
		crypto_invoke(crp);
d582 9
@


1.32
log
@It looks like there has been one crack smoking and a few cut and pastes.
PR_FREEHEADER should not be set in pool_init by the caller. It shouldn't
be set in pool_init at all. Besides, it's going away soon anyway.
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.31 2002/01/23 00:39:47 art Exp $	*/
a475 31
}

/*
 * Return SYMMETRIC or PUBLIC_KEY, depending on the algorithm type.
 */
int
crypto_check_alg(struct cryptoini *cri)
{
	switch (cri->cri_alg)
	{
	case CRYPTO_DES_CBC:
	case CRYPTO_3DES_CBC:
	case CRYPTO_BLF_CBC:
	case CRYPTO_CAST_CBC:
	case CRYPTO_SKIPJACK_CBC:
	case CRYPTO_RIJNDAEL128_CBC:
	case CRYPTO_ARC4:
		return SYMMETRIC;
	case CRYPTO_DH_SEND:
	case CRYPTO_DH_RECEIVE:
	case CRYPTO_RSA_ENCRYPT:
	case CRYPTO_RSA_DECRYPT:
	case CRYPTO_DSA_SIGN:
	case CRYPTO_DSA_VERIFY:
		return PUBLIC_KEY;
	}

#ifdef DIAGNOSTIC
	panic("crypto_check_alg: unknown algorithm %d", cri->cri_alg);
#endif
	return -1;
@


1.31
log
@Pool deals fairly well with physical memory shortage, but it doesn't deal
well (not at all) with shortages of the vm_map where the pages are mapped
(usually kmem_map).

Try to deal with it:
 - group all information the backend allocator for a pool in a separate
   struct. The pool will only have a pointer to that struct.
 - change the pool_init API to reflect that.
 - link all pools allocating from the same allocator on a linked list.
 - Since an allocator is responsible to wait for physical memory it will
   only fail (waitok) when it runs out of its backing vm_map, carefully
   drain pools using the same allocator so that va space is freed.
   (see comments in code for caveats and details).
 - change pool_reclaim to return if it actually succeeded to free some
   memory, use that information to make draining easier and more efficient.
 - get rid of PR_URGENT, noone uses it.
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.30 2001/11/13 18:54:32 deraadt Exp $	*/
d415 1
a415 1
		    PR_FREEHEADER, "cryptop", NULL);
d417 1
a417 1
		    PR_FREEHEADER, "cryptodesc", NULL);
@


1.30
log
@and for the case where it allocates a bunch at a time, also make sure the
software flag gets set.
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.29 2001/11/13 17:45:59 deraadt Exp $	*/
d415 1
a415 1
		    PR_FREEHEADER, "cryptop", 0, NULL, NULL, M_CRYPTO_OPS);
d417 1
a417 1
		    PR_FREEHEADER, "cryptodesc", 0, NULL, NULL, M_CRYPTO_OPS);
@


1.30.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.32 2002/01/23 01:33:07 art Exp $	*/
d415 1
a415 1
		    0, "cryptop", NULL);
d417 1
a417 1
		    0, "cryptodesc", NULL);
@


1.30.2.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.30.2.1 2002/01/31 22:55:29 niklas Exp $	*/
d14 1
a14 1
 * modification of this software.
a39 3
struct cryptkop *krp_req_queue = NULL;
struct cryptkop **krp_req_queue_tail = NULL;

a226 32
crypto_kregister(u_int32_t driverid, int kalg, u_int32_t flags,
    int (*kprocess)(struct cryptkop *))
{
	int s;

	if (driverid >= crypto_drivers_num || kalg < 0 ||
	    kalg > CRK_ALGORITHM_MAX || crypto_drivers == NULL)
		return EINVAL;

	s = splimp();

	/*
	 * XXX Do some performance testing to determine placing.
	 * XXX We probably need an auxiliary data structure that describes
	 * XXX relative performances.
	 */

	crypto_drivers[driverid].cc_kalg[kalg] =
	    flags | CRYPTO_ALG_FLAG_SUPPORTED;

	if (crypto_drivers[driverid].cc_kprocess == NULL)
		crypto_drivers[driverid].cc_kprocess = kprocess;

	splx(s);
	return 0;
}

/*
 * Register a crypto driver. It should be called once for each algorithm
 * supported by the driver.
 */
int
d246 1
a246 1
	crypto_drivers[driverid].cc_alg[alg] =
a325 57
int
crypto_kdispatch(struct cryptkop *krp)
{
	int s = splimp();

	if (krp_req_queue == NULL) {
		krp_req_queue = krp;
		krp_req_queue_tail = &(krp->krp_next);
		splx(s);
		wakeup((caddr_t) &crp_req_queue);	/* shared wait channel */
	} else {
		*krp_req_queue_tail = krp;
		krp_req_queue_tail = &(krp->krp_next);
		splx(s);
	}
	return 0;
}

/*
 * Dispatch an assymetric crypto request to the appropriate crypto devices.
 */
int
crypto_kinvoke(struct cryptkop *krp)
{
	extern int cryptodevallowsoft;
	u_int32_t hid;
	int error;

	/* Sanity checks. */
	if (krp == NULL || krp->krp_callback == NULL)
		return (EINVAL);

	for (hid = 0; hid < crypto_drivers_num; hid++) {
		if ((crypto_drivers[hid].cc_flags & CRYPTOCAP_F_SOFTWARE) &&
		    cryptodevallowsoft == 0)
			continue;
		if (crypto_drivers[hid].cc_kprocess == NULL)
			continue;
		if ((crypto_drivers[hid].cc_kalg[krp->krp_op] &
		    CRYPTO_ALG_FLAG_SUPPORTED) == 0)
			continue;
		break;
	}
	if (hid == crypto_drivers_num) {
		krp->krp_status = ENODEV;
		crypto_kdone(krp);
		return (0);
	}
	krp->krp_hid = hid;
	error = crypto_drivers[hid].cc_kprocess(krp);
	if (error) {
		krp->krp_status = error;
		crypto_kdone(krp);
	}
	return (0);
}

a334 1
	int error;
d376 1
a376 5
	error = crypto_drivers[hid].cc_process(crp);
	if (error) {
		crp->crp_etype = error;
		crypto_done(crp);
	}
a451 1
	struct cryptkop *krp;
d458 1
a458 2
		krp = krp_req_queue;
		if (crp == NULL && krp == NULL) {
d463 3
a465 10
		if (crp) {
			/* Remove from the queue. */
			crp_req_queue = crp->crp_next;
			crypto_invoke(crp);
		}
		if (krp) {
			/* Remove from the queue. */
			krp_req_queue = krp->krp_next;
			crypto_kinvoke(krp);
		}
d479 1
a479 1
 * Invoke the callback on behalf of the driver.
a480 6
void
crypto_kdone(struct cryptkop *krp)
{
	krp->krp_callback(krp);
}

d482 1
a482 1
crypto_getfeat(int *featp)
d484 23
a506 24
	int kalgs[CRK_ALGORITHM_MAX];
	extern int cryptodevallowsoft;
	int hid, kalg, feat = 0;

	for (hid = 0; hid < crypto_drivers_num; hid++) {
		if ((crypto_drivers[hid].cc_flags & CRYPTOCAP_F_SOFTWARE) &&
		    cryptodevallowsoft == 0)
			continue;
		if (crypto_drivers[hid].cc_kprocess == NULL)
			continue;
		for (kalg = 0; kalg < CRK_ALGORITHM_MAX; kalg++)
			if ((crypto_drivers[hid].cc_kalg[kalg] &
			    CRYPTO_ALG_FLAG_SUPPORTED) == 0)
				kalgs[kalg] = 1;
	}

	if (kalgs[CRK_MOD_EXP] && kalgs[CRK_MOD_EXP_CRT])
		feat |= CRSFEAT_RSA;
	if (kalgs[CRK_DSA_VERIFY] && kalgs[CRK_DSA_SIGN])
		feat |= CRSFEAT_DSA;
	if (kalgs[CRK_DH_COMPUTE_KEY] && kalgs[CRK_MOD_EXP])
		feat |= CRSFEAT_DH;
	*featp = feat;
	return (0);
@


1.30.2.3
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.30.2.2 2002/06/11 03:28:34 art Exp $	*/
d596 2
a597 1
	extern int cryptodevallowsoft, userasymcrypto;
a599 2
	if (userasymcrypto == 0)
		goto out;	  
d602 1
a602 1
		    cryptodevallowsoft == 0) {
a603 1
		}
d608 2
a609 2
			    CRYPTO_ALG_FLAG_SUPPORTED) != 0)
				feat |=  1 << kalg;
d611 7
a617 1
out:
@


1.30.2.4
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a39 3
struct cryptop *crp_ret_queue = NULL;
struct cryptop **crp_ret_queue_tail = NULL;

a42 3
struct cryptkop *krp_ret_queue = NULL;
struct cryptkop **krp_ret_queue_tail = NULL;

a48 2
	u_int32_t hid, lid, hid2 = -1;
	struct cryptocap *cpc;
d50 2
a51 1
	int err, s, turn = 0;
d60 1
a60 4
	 * first driver that supports all the algorithms we need. Do
	 * a double-pass over all the drivers, ignoring software ones
	 * at first, to deal with cases of drivers that register after
	 * the software one(s) --- e.g., PCMCIA crypto cards.
a64 61
	do {
		for (hid = 0; hid < crypto_drivers_num; hid++) {
			cpc = &crypto_drivers[hid];

			/*
			 * If it's not initialized or has remaining sessions
			 * referencing it, skip.
			 */
			if (cpc->cc_newsession == NULL ||
			    (cpc->cc_flags & CRYPTOCAP_F_CLEANUP))
				continue;

			if (cpc->cc_flags & CRYPTOCAP_F_SOFTWARE) {
				/*
				 * First round of search, ignore
				 * software drivers.
				 */
				if (turn == 0)
					continue;
			} else { /* !CRYPTOCAP_F_SOFTWARE */
				/* Second round of search, only software. */
				if (turn == 1)
					continue;
			}

			/* See if all the algorithms are supported. */
			for (cr = cri; cr; cr = cr->cri_next) {
				if (cpc->cc_alg[cr->cri_alg] == 0)
					break;
			}

			/*
			 * If even one algorithm is not supported,
			 * keep searching.
			 */
			if (cr != NULL)
				continue;

			/*
			 * If we had a previous match, see how it compares
			 * to this one. Keep "remembering" whichever is
			 * the best of the two.
			 */
			if (hid2 != -1) {
				/*
				 * Compare session numbers, pick the one
				 * with the lowest.
				 * XXX Need better metrics, this will
				 * XXX just do un-weighted round-robin.
				 */
				if (crypto_drivers[hid].cc_sessions <=
				    crypto_drivers[hid2].cc_sessions)
					hid2 = hid;
			} else {
				/*
				 * Remember this one, for future
                                 * comparisons.
				 */
				hid2 = hid;
			}
		}
d66 1
d68 2
a69 3
		 * If we found something worth remembering, leave. The
		 * side-effect is that we will always prefer a hardware
		 * driver over the software one.
d71 3
a73 2
		if (hid2 != -1)
			break;
d75 4
a78 1
		turn++;
d80 4
a83 2
		/* If we only want hardware drivers, don't do second pass. */
	} while (turn <= 2 && hard == 0);
d85 4
a88 1
	hid = hid2;
d93 2
a94 2
	 * XXX Fix this. We need to inject a "virtual" session
	 * XXX layer right about here.
d97 1
a97 1
	if (hid == -1) {
d230 1
a230 1
crypto_kregister(u_int32_t driverid, int *kalg,
d233 1
a233 1
	int s, i;
d235 2
a236 2
	if (driverid >= crypto_drivers_num || kalg  == NULL ||
	    crypto_drivers == NULL)
d241 5
a245 6
	for (i = 0; i < CRK_ALGORITHM_MAX; i++) {
		/*
		 * XXX Do some performance testing to determine
		 * placing.  We probably need an auxiliary data
		 * structure that describes relative performances.
		 */
d247 2
a248 2
		crypto_drivers[driverid].cc_kalg[i] = kalg[i];
	}
d250 2
a251 1
	crypto_drivers[driverid].cc_kprocess = kprocess;
d257 4
a260 1
/* Register a crypto driver. */
d262 2
a263 1
crypto_register(u_int32_t driverid, int *alg,
d267 1
a267 1
	int s, i;
d269 3
a272 4
	if (driverid >= crypto_drivers_num || alg == NULL ||
	    crypto_drivers == NULL)
		return EINVAL;
	
d275 5
a279 6
	for (i = 0; i < CRYPTO_ALGORITHM_ALL; i++) {
		/*
		 * XXX Do some performance testing to determine
		 * placing.  We probably need an auxiliary data
		 * structure that describes relative performances.
		 */
d281 2
a282 2
		crypto_drivers[driverid].cc_alg[i] = alg[i];
	}
d284 1
d286 6
a291 4
	crypto_drivers[driverid].cc_newsession = newses;
	crypto_drivers[driverid].cc_process = process;
	crypto_drivers[driverid].cc_freesession = freeses;
	crypto_drivers[driverid].cc_sessions = 0; /* Unmark */
a293 1

d306 1
a306 1
	int i = CRYPTO_ALGORITHM_MAX + 1, s = splimp();
d309 3
a311 4
	/* Sanity checks. */
	if (driverid >= crypto_drivers_num || crypto_drivers == NULL ||
	    ((alg <= 0 || alg > CRYPTO_ALGORITHM_MAX) &&
		alg != CRYPTO_ALGORITHM_ALL) ||
d317 2
a318 2
	if (alg != CRYPTO_ALGORITHM_ALL) {
		crypto_drivers[driverid].cc_alg[alg] = 0;
d320 4
a323 5
		/* Was this the last algorithm ? */
		for (i = 1; i <= CRYPTO_ALGORITHM_MAX; i++)
			if (crypto_drivers[driverid].cc_alg[i] != 0)
				break;
	}
d325 1
a325 5
	/*
	 * If a driver unregistered its last algorithm or all of them
	 * (alg == CRYPTO_ALGORITHM_ALL), cleanup its entry.
	 */
	if (i == CRYPTO_ALGORITHM_MAX + 1 || alg == CRYPTO_ALGORITHM_ALL) {
a346 10
	u_int32_t hid;

	/*
	 * Keep track of ops per driver, for coallescing purposes. If
	 * we have been given an invalid hid, we'll deal with in the
	 * crypto_invoke(), through session migration.
	 */
	hid = (crp->crp_sid >> 32) & 0xffffffff;
	if (hid < crypto_drivers_num)
		crypto_drivers[hid].cc_queued++;
a347 1
	crp->crp_next = NULL;
d352 1
a352 1
		wakeup((caddr_t) &crp_req_queue); /* Shared wait channel. */
a365 1
	krp->krp_next = NULL;
d370 1
a370 1
		wakeup((caddr_t) &crp_req_queue); /* Shared wait channel. */
d380 1
a380 1
 * Dispatch an asymmetric crypto request to the appropriate crypto devices.
a403 1

a408 1

a409 3

	crypto_drivers[hid].cc_koperations++;

d440 4
a443 2
	if (hid >= crypto_drivers_num)
		goto migrate;
d445 2
a446 1
	crypto_drivers[hid].cc_queued--;
d448 6
a453 1
	if (crypto_drivers[hid].cc_flags & CRYPTOCAP_F_CLEANUP) {
a454 2
		goto migrate;
	}
d456 4
a459 2
	if (crypto_drivers[hid].cc_process == NULL)
		goto migrate;
d461 7
a467 2
	crypto_drivers[hid].cc_operations++;
	crypto_drivers[hid].cc_bytes += crp->crp_ilen;
d471 2
a472 8
		if (error == ERESTART) {
			/* Unregister driver and migrate session. */
			crypto_unregister(hid, CRYPTO_ALGORITHM_ALL);
			goto migrate;
		} else {
			crp->crp_etype = error;
			crypto_done(crp);
		}
a473 13

	return 0;

 migrate:
	/* Migrate session. */
	for (crd = crp->crp_desc; crd->crd_next; crd = crd->crd_next)
		crd->CRD_INI.cri_next = &(crd->crd_next->CRD_INI);

	if (crypto_newsession(&nid, &(crp->crp_desc->CRD_INI), 0) == 0)
		crp->crp_sid = nid;

	crp->crp_etype = EAGAIN;
	crypto_done(crp);
d548 2
a549 2
	struct cryptop *crp, *crpt;
	struct cryptkop *krp, *krpt;
d557 1
a557 4
		crpt = crp_ret_queue;
		krpt = krp_ret_queue;
		if (crp == NULL && krp == NULL &&
		    crpt == NULL && krpt == NULL) {
a571 18
		if (crpt) {
			/* Remove from the queue. */
			crp_ret_queue = crpt->crp_next;
			splx(s);
			crpt->crp_callback(crpt);
			splimp();
		}
		if (krpt) {
			/* Remove from the queue. */
			krp_ret_queue = krpt->krp_next;
			/*
			 * Cheat. For public key ops, we know that
			 * all that's done is a wakeup() for the
			 * userland process, so don't bother to
			 * change the processor priority.
			 */
			krpt->krp_callback(krpt);
		}
d581 1
a581 13
	int s = splimp();

	crp->crp_next = NULL;
	if (crp_ret_queue == NULL) {
		crp_ret_queue = crp;
		crp_ret_queue_tail = &(crp->crp_next);
		splx(s);
		wakeup((caddr_t) &crp_req_queue); /* Shared wait channel. */
	} else {
		*crp_ret_queue_tail = crp;
		crp_ret_queue_tail = &(crp->crp_next);
		splx(s);
	}
d590 1
a590 13
	int s = splimp();

	krp->krp_next = NULL;
	if (krp_ret_queue == NULL) {
		krp_ret_queue = krp;
		krp_ret_queue_tail = &(krp->krp_next);
		splx(s);
		wakeup((caddr_t) &crp_req_queue); /* Shared wait channel. */
	} else {
		*krp_ret_queue_tail = krp;
		krp_ret_queue_tail = &(krp->krp_next);
		splx(s);
	}
@


1.29
log
@incorrect check
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.28 2001/11/09 03:11:38 deraadt Exp $	*/
d181 1
@


1.28
log
@be way more sure that software cannot be used
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.27 2001/11/08 23:12:38 deraadt Exp $	*/
a69 3
			continue;

		if (crypto_drivers[hid].cc_sessions == 0)
@


1.27
log
@indent
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.26 2001/08/05 09:36:38 deraadt Exp $	*/
d72 3
d160 1
a160 1
crypto_get_driverid(void)
d210 1
@


1.26
log
@put in tags for ARC4 to please ben, who now has no excuses
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.25 2001/06/27 05:49:33 angelos Exp $	*/
d80 1
a80 1
			break;
@


1.26.2.1
log
@Pull in patch from current:
Fix (deraadt):
be way more sure that software cannot be used
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.26 2001/08/05 09:36:38 deraadt Exp $	*/
d157 1
a157 1
crypto_get_driverid(u_int8_t flags)
a180 1
			crypto_drivers[i].cc_flags = flags;
a206 1
		newdrv[i].cc_flags = flags;
@


1.25
log
@KNF
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.24 2001/06/26 19:29:25 angelos Exp $	*/
d490 1
@


1.24
log
@Remove space.
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.23 2001/06/25 17:52:36 angelos Exp $	*/
d65 2
a66 2
		 * If it's not initialized or has remaining sessions referencing
		 * it, skip.
d72 1
a72 1
		/* hardware requested -- ignore software drivers */
d77 1
a77 1
		/* See if all the algorithms are supported */
d82 1
a82 1
		/* Ok, all algorithms are supported */
d84 1
a84 1
		break;
d99 2
a100 2
	/* Call the driver initialization routine */
	lid = hid; /* Pass the driver ID */
d126 1
a126 1
	/* Determine two IDs */
d137 1
a137 1
	/* Call the driver cleanup routine, if available */
d186 1
a186 1
	/* Out of entries, allocate some more */
d188 1
a188 1
		/* Be careful about wrap-around */
d194 2
a195 2
		newdrv = malloc(2 * crypto_drivers_num * sizeof(struct cryptocap),
		    M_CRYPTO_DATA, M_NOWAIT);
d292 3
a294 1
			/* If there are pending sessions, just mark as invalid */
d334 1
a334 1
	/* Sanity checks */
d346 1
a346 1
		/* Migrate session */
d362 1
a362 1
		/* Migrate session */
d461 1
a461 1
		/* Remove from the queue */
@


1.23
log
@Add crypto_check_alg(), from jgarfiel@@seas.upenn.edu
@
text
@d1 1
a1 2
/*	$OpenBSD: crypto.c,v 1.22 2001/06/25 05:02:22 angelos Exp $	*/

@


1.22
log
@Update copyright; you can use this with or without fee (unless your
name is Theo Deraadt)
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.21 2001/06/23 21:00:48 angelos Exp $	*/
d282 1
d473 30
@


1.21
log
@New prototype for crypto_register(), to take into account maximum key
length (for PK operations) and various flags.

Structures for public key operations (DH, RSA, DSA). A lot of this
work was done by jgarfiel@@seas.upenn.edu
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.20 2001/06/23 18:30:35 deraadt Exp $	*/
d10 1
a10 1
 * Copyright (c) 2000 Angelos D. Keromytis
d12 1
a12 1
 * Permission to use, copy, and modify this software without fee
@


1.20
log
@merge crypto/crypto{dev,}.h to crypto/cryptodev.h, to avoid name conflicts inside OpenSSL codebase
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.19 2001/06/16 22:17:49 deraadt Exp $	*/
d226 2
a227 1
crypto_register(u_int32_t driverid, int alg,
d245 4
a248 1
	crypto_drivers[driverid].cc_alg[alg] = 1;
@


1.19
log
@KNF
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.18 2001/06/06 18:58:52 angelos Exp $	*/
d29 1
a29 1
#include <crypto/crypto.h>
@


1.18
log
@Use pool(9) for some of the structures, and splimp/splx to protect
from ourselves. Should solve PR 1850.
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.17 2001/05/13 15:39:26 deraadt Exp $	*/
d47 8
a54 16
    struct cryptoini *cr;
    u_int32_t hid, lid;
    int err, s;

    if (crypto_drivers == NULL)
      return EINVAL;

    s = splimp();

    /*
     * The algorithm we use here is pretty stupid; just use the
     * first driver that supports all the algorithms we need.
     *
     * XXX We need more smarts here (in real life too, but that's
     * XXX another story altogether).
     */
a55 2
    for (hid = 0; hid < crypto_drivers_num; hid++)
    {
d57 30
a86 28
         * If it's not initialized or has remaining sessions referencing
         * it, skip.
         */
	if ((crypto_drivers[hid].cc_newsession == NULL) ||
	    (crypto_drivers[hid].cc_flags & CRYPTOCAP_F_CLEANUP))
	  continue;

	/* hardware requested -- ignore software drivers */
	if (hard &&
	    (crypto_drivers[hid].cc_flags & CRYPTOCAP_F_SOFTWARE))
	  continue;

	/* See if all the algorithms are supported */
	for (cr = cri; cr; cr = cr->cri_next)
	  if (crypto_drivers[hid].cc_alg[cr->cri_alg] == 0)
	    break;

	/* Ok, all algorithms are supported */
	if (cr == NULL)
	  break;
    }

    /*
     * Can't do everything in one session.
     *
     * XXX Fix this. We need to inject a "virtual" session layer right
     * XXX about here.
     */
d88 11
a98 5
    if (hid == crypto_drivers_num)
    {
	splx(s);
	return EINVAL;
    }
d100 9
a108 10
    /* Call the driver initialization routine */
    lid = hid; /* Pass the driver ID */
    err = crypto_drivers[hid].cc_newsession(&lid, cri);
    if (err == 0)
    {
	(*sid) = hid;
	(*sid) <<= 32;
	(*sid) |= (lid & 0xffffffff);
        crypto_drivers[hid].cc_sessions++;
    }
d110 2
a111 2
    splx(s);
    return err;
d121 2
a122 2
    int err = 0, s;
    u_int32_t hid;
d124 2
a125 2
    if (crypto_drivers == NULL)
      return EINVAL;
d127 2
a128 2
    /* Determine two IDs */
    hid = (sid >> 32) & 0xffffffff;
d130 2
a131 2
    if (hid >= crypto_drivers_num)
      return ENOENT;
d133 1
a133 1
    s = splimp();
d135 2
a136 2
    if (crypto_drivers[hid].cc_sessions)
      crypto_drivers[hid].cc_sessions--;
d138 3
a140 3
    /* Call the driver cleanup routine, if available */
    if (crypto_drivers[hid].cc_freesession)
      err = crypto_drivers[hid].cc_freesession(sid);
d142 7
a148 7
    /*
     * If this was the last session of a driver marked as invalid, make
     * the entry available for reuse.
     */
    if ((crypto_drivers[hid].cc_flags & CRYPTOCAP_F_CLEANUP) &&
	(crypto_drivers[hid].cc_sessions == 0))
      bzero(&crypto_drivers[hid], sizeof(struct cryptocap));
d150 2
a151 2
    splx(s);
    return err;
d160 12
a171 2
    struct cryptocap *newdrv;
    int i, s = splimp();
d173 13
a185 47
    if (crypto_drivers_num == 0)
    {
	crypto_drivers_num = CRYPTO_DRIVERS_INITIAL;
	crypto_drivers = malloc(crypto_drivers_num * sizeof(struct cryptocap),
				M_CRYPTO_DATA, M_NOWAIT);
	if (crypto_drivers == NULL)
	{
	    splx(s);
	    crypto_drivers_num = 0;
	    return -1;
	}

	bzero(crypto_drivers, crypto_drivers_num * sizeof(struct cryptocap));
    }

    for (i = 0; i < crypto_drivers_num; i++)
      if ((crypto_drivers[i].cc_process == NULL) &&
	  !(crypto_drivers[i].cc_flags & CRYPTOCAP_F_CLEANUP) &&
	  (crypto_drivers[i].cc_sessions == 0))
      {
	  crypto_drivers[i].cc_sessions = 1; /* Mark */
	  splx(s);
	  return i;
      }

    /* Out of entries, allocate some more */
    if (i == crypto_drivers_num)
    {
	/* Be careful about wrap-around */
	if (2 * crypto_drivers_num <= crypto_drivers_num)
	{
	    splx(s);
	    return -1;
	}

	newdrv = malloc(2 * crypto_drivers_num * sizeof(struct cryptocap),
			M_CRYPTO_DATA, M_NOWAIT);
	if (newdrv == NULL)
	{
	    splx(s);
	    return -1;
	}

        bcopy(crypto_drivers, newdrv,
	      crypto_drivers_num * sizeof(struct cryptocap));
	bzero(&newdrv[crypto_drivers_num],
	      crypto_drivers_num * sizeof(struct cryptocap));
d187 28
a214 2
	newdrv[i].cc_sessions = 1; /* Mark */
	crypto_drivers_num *= 2;
d216 1
a216 2
	free(crypto_drivers, M_CRYPTO_DATA);
	crypto_drivers = newdrv;
d218 1
a218 6
	return i;
    }

    /* Shouldn't really get here... */
    splx(s);
    return -1;
d230 5
a234 1
    int s;
d236 1
a236 21
    if ((driverid >= crypto_drivers_num) || (alg <= 0) ||
	(alg > CRYPTO_ALGORITHM_MAX) || (crypto_drivers == NULL))
      return EINVAL;

    s = splimp();

    /*
     * XXX Do some performance testing to determine placing.
     * XXX We probably need an auxiliary data structure that describes
     * XXX relative performances.
     */

    crypto_drivers[driverid].cc_alg[alg] = 1;

    if (crypto_drivers[driverid].cc_process == NULL)
    {
	crypto_drivers[driverid].cc_newsession = newses;
	crypto_drivers[driverid].cc_process = process;
	crypto_drivers[driverid].cc_freesession = freeses;
	crypto_drivers[driverid].cc_sessions = 0; /* Unmark */
    }
d238 17
a254 2
    splx(s);
    return 0;
d266 2
a267 2
    int i, s = splimp();
    u_int32_t ses;
d269 7
a275 8
    /* Sanity checks */
    if ((driverid >= crypto_drivers_num) || (alg <= 0) ||
        (alg > CRYPTO_ALGORITHM_MAX) || (crypto_drivers == NULL) ||
	(crypto_drivers[driverid].cc_alg[alg] == 0))
    {
	splx(s);
	return EINVAL;
    }
d277 1
a277 1
    crypto_drivers[driverid].cc_alg[alg] = 0;
d279 13
a291 15
    /* Was this the last algorithm ? */
    for (i = 1; i <= CRYPTO_ALGORITHM_MAX; i++)
      if (crypto_drivers[driverid].cc_alg[i] != 0)
	break;

    if (i == CRYPTO_ALGORITHM_MAX + 1) 
    {
	ses = crypto_drivers[driverid].cc_sessions;
        bzero(&crypto_drivers[driverid], sizeof(struct cryptocap));

        if (ses != 0)
	{
            /* If there are pending sessions, just mark as invalid */
            crypto_drivers[driverid].cc_flags |= CRYPTOCAP_F_CLEANUP;
            crypto_drivers[driverid].cc_sessions = ses;
d293 2
a294 4
    }

    splx(s);
    return 0;
d303 1
a303 1
    int s = splimp();
d305 11
a315 14
    if (crp_req_queue == NULL) {
	crp_req_queue = crp;
	crp_req_queue_tail = &(crp->crp_next);
	splx(s);

	wakeup((caddr_t) &crp_req_queue);
    }
    else
    {
	*crp_req_queue_tail = crp;
	crp_req_queue_tail = &(crp->crp_next);
	splx(s);
    }
    return 0;
d324 13
a336 14
    struct cryptodesc *crd;
    u_int64_t nid;
    u_int32_t hid;

    /* Sanity checks */
    if ((crp == NULL) || (crp->crp_callback == NULL))
      return EINVAL;

    if ((crp->crp_desc == NULL) || (crypto_drivers == NULL))
    {
	crp->crp_etype = EINVAL;
	crypto_done(crp);
	return 0;
    }
d338 13
a350 1
    hid = (crp->crp_sid >> 32) & 0xffffffff;
d352 2
a353 5
    if (hid >= crypto_drivers_num)
    {
	/* Migrate session */
	for (crd = crp->crp_desc; crd->crd_next; crd = crd->crd_next)
	  crd->CRD_INI.cri_next = &(crd->crd_next->CRD_INI);
d355 12
a366 2
	if (crypto_newsession(&nid, &(crp->crp_desc->CRD_INI), 0) == 0)
	  crp->crp_sid = nid;
d368 1
a368 2
	crp->crp_etype = EAGAIN;
	crypto_done(crp);
a369 21
    }

    if (crypto_drivers[hid].cc_flags & CRYPTOCAP_F_CLEANUP)
      crypto_freesession(crp->crp_sid);

    if (crypto_drivers[hid].cc_process == NULL)
    {
	/* Migrate session */
	for (crd = crp->crp_desc; crd->crd_next; crd = crd->crd_next)
	  crd->CRD_INI.cri_next = &(crd->crd_next->CRD_INI);

	if (crypto_newsession(&nid, &(crp->crp_desc->CRD_INI), 0) == 0)
	  crp->crp_sid = nid;

	crp->crp_etype = EAGAIN;
	crypto_done(crp);
	return 0;
    }

    crypto_drivers[hid].cc_process(crp);
    return 0;
d378 2
a379 2
    struct cryptodesc *crd;
    int s;
d381 2
a382 2
    if (crp == NULL)
      return;
d384 1
a384 1
    s = splimp();
d386 4
a389 5
    while ((crd = crp->crp_desc) != NULL)
    {
	crp->crp_desc = crd->crd_next;
	pool_put(&cryptodesc_pool, crd);
    }
d391 2
a392 2
    pool_put(&cryptop_pool, crp);
    splx(s);
d401 11
a411 20
    struct cryptodesc *crd;
    struct cryptop *crp;
    int s = splimp();

    if (crypto_pool_initialized == 0)
    {
	pool_init(&cryptop_pool, sizeof(struct cryptop), 0, 0, PR_FREEHEADER,
		  "cryptop", 0, NULL, NULL, M_CRYPTO_OPS);

	pool_init(&cryptodesc_pool, sizeof(struct cryptodesc), 0, 0,
		  PR_FREEHEADER, "cryptodesc", 0, NULL, NULL, M_CRYPTO_OPS);
	crypto_pool_initialized = 1;
    }

    crp = pool_get(&cryptop_pool, 0);
    if (crp == NULL)
    {
	splx(s);
	return NULL;
    }
d413 6
a418 1
    bzero(crp, sizeof(struct cryptop));
d420 12
a431 14
    while (num--)
    {
	crd = pool_get(&cryptodesc_pool, 0);
	if (crd == NULL)
	{
	    splx(s);
	    crypto_freereq(crp);
	    return NULL;
	}

	bzero(crd, sizeof(struct cryptodesc));
	crd->crd_next = crp->crp_desc;
	crp->crp_desc = crd;
    }
d433 2
a434 2
    splx(s);
    return crp;
d443 2
a444 2
    struct cryptop *crp;
    int s;
d446 1
a446 1
    s = splimp();
d448 10
a457 7
    for (;;)
    {
	crp = crp_req_queue;
	if (crp == NULL) /* No work to do */
	{
	    (void) tsleep(&crp_req_queue, PLOCK, "crypto_wait", 0);
	    continue;
a458 6

	/* Remove from the queue */
	crp_req_queue = crp->crp_next;

	crypto_invoke(crp);
    }
d467 1
a467 1
    crp->crp_callback(crp);
@


1.17
log
@initial cut at /dev/crypto support.  takes original mbuf "try, and discard
if we fail" semantics and extends to two varients of data movement: mbuf,
or an iovec style block.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d27 2
a28 10
#include <sys/mbuf.h>
#include <sys/sysctl.h>
#include <sys/errno.h>
#include <sys/md5k.h>
#include <dev/rndvar.h>
#include <crypto/sha1.h>
#include <crypto/rmd160.h>
#include <crypto/cast.h>
#include <crypto/skipjack.h>
#include <crypto/blf.h>
a29 1
#include <crypto/xform.h>
d34 3
a36 5
struct cryptop *cryptop_queue = NULL;
struct cryptodesc *cryptodesc_queue = NULL;

int crypto_queue_num = 0;
int crypto_queue_max = CRYPTO_MAX_CACHED;
d49 1
a49 1
    int err;
d54 2
d97 4
a100 1
      return EINVAL;
d113 1
d124 1
a125 1
    int err = 0;
d136 2
d153 1
d164 1
a164 1
    int i;
d173 1
d185 5
a189 1
	return i;
d196 4
a199 1
	  return -1;
d204 4
a207 1
	  return -1;
d213 2
d216 4
d224 1
d237 2
d243 2
d258 1
d261 1
d274 1
a275 1
    int i;
d281 4
a284 1
      return EINVAL;
d306 1
d316 1
a316 1
    int s = splhigh();
d321 2
d324 3
a326 1
    } else {
d329 1
a330 1
    splx(s);
d404 1
a404 1
    s = splhigh();
d409 1
a409 18

	if (crypto_queue_num + 1 > crypto_queue_max)
	  FREE(crd, M_CRYPTO_OPS);
	else
	{
	    crd->crd_next = cryptodesc_queue;
	    cryptodesc_queue = crd;
	    crypto_queue_num++;
	}
    }

    if (crypto_queue_num + 1 > crypto_queue_max)
      FREE(crp, M_CRYPTO_OPS);
    else
    {
        crp->crp_next = cryptop_queue;
        cryptop_queue = crp;
        crypto_queue_num++;
d412 1
d424 1
a424 1
    int s = splhigh();
d426 1
a426 1
    if (cryptop_queue == NULL)
d428 6
a433 7
        MALLOC(crp, struct cryptop *, sizeof(struct cryptop), M_CRYPTO_OPS,
	       M_NOWAIT);
        if (crp == NULL)
        {
            splx(s);
            return NULL;
        }
d435 3
a437 1
    else
d439 2
a440 3
	crp = cryptop_queue;
	cryptop_queue = crp->crp_next;
        crypto_queue_num--;
d447 2
a448 12
        if (cryptodesc_queue == NULL)
	{
	    MALLOC(crd, struct cryptodesc *, sizeof(struct cryptodesc),
		   M_CRYPTO_OPS, M_NOWAIT);
	    if (crd == NULL)
	    {
                splx(s);
		crypto_freereq(crp);
	        return NULL;
	    }
	}
	else
d450 3
a452 3
	    crd = cryptodesc_queue;
	    cryptodesc_queue = crd->crd_next;
	    crypto_queue_num--;
d473 1
a473 1
    s = splhigh();
a485 1
	splx(s);
a487 2

	s = splhigh();
@


1.16
log
@Use the M_CRYPTO_DATA and M_CRYPTO_OPS malloc types.
@
text
@d56 1
a56 1
crypto_newsession(u_int64_t *sid, struct cryptoini *cri)
d83 5
d333 1
a333 1
	if (crypto_newsession(&nid, &(crp->crp_desc->CRD_INI)) == 0)
d350 1
a350 1
	if (crypto_newsession(&nid, &(crp->crp_desc->CRD_INI)) == 0)
@


1.15
log
@make the 31-bit code work on 32-bit machines.
@
text
@d167 1
a167 1
				M_XDATA, M_NOWAIT);
d191 1
a191 1
			M_XDATA, M_NOWAIT);
d376 1
a376 1
	  FREE(crd, M_XDATA);
d386 1
a386 1
      FREE(crp, M_XDATA);
d409 1
a409 1
        MALLOC(crp, struct cryptop *, sizeof(struct cryptop), M_XDATA,
d431 1
a431 1
		   M_XDATA, M_NOWAIT);
@


1.14
log
@avoid excessive wakeup().  we think this is safe...
@
text
@d109 1
a109 1
	(*sid) <<= 31;
d131 1
a131 1
    hid = (sid >> 31) & 0xffffffff;
d320 1
a320 1
    hid = (crp->crp_sid >> 31) & 0xffffffff;
@


1.14.2.1
log
@Pull in patch from current:
Fix (provos):
make the 31-bit code work on 32-bit machines.
@
text
@d109 1
a109 1
	(*sid) <<= 32;
d131 1
a131 1
    hid = (sid >> 32) & 0xffffffff;
d320 1
a320 1
    hid = (crp->crp_sid >> 32) & 0xffffffff;
@


1.13
log
@MALLOC/FREE -> malloc/free ok art@@ angelos@@
@
text
@d287 8
a294 7
    if (crp_req_queue == NULL)
      crp_req_queue = crp;
    else
      *crp_req_queue_tail = crp;

    crp_req_queue_tail = &(crp->crp_next);
    wakeup((caddr_t) &crp_req_queue);
a295 1

@


1.12
log
@Fix tail queue leakage (zzlevo@@dd.chalmers.se)
@
text
@d166 2
a167 3
	MALLOC(crypto_drivers, struct cryptocap *, 
	       crypto_drivers_num * sizeof(struct cryptocap), M_XDATA,
	       M_NOWAIT);
d190 2
a191 3
	MALLOC(newdrv, struct cryptocap *,
	       2 * crypto_drivers_num * sizeof(struct cryptocap),
	       M_XDATA, M_NOWAIT);
@


1.11
log
@crypto_done(), all it does for now is invoke the callback.
@
text
@d292 1
a292 1
      crp->crp_next = NULL;
a478 5
	if (crp_req_queue)
	  crp_req_queue_tail = &crp_req_queue->crp_next;
        else
          crp_req_queue_tail = NULL;

@


1.10
log
@Add Rijndael (128-bit blocksize) in the software crypto driver.

Hacking at OpenBSD Crypto 2000 :-)
@
text
@d318 1
a318 1
	crp->crp_callback(crp);
d334 1
a334 1
	crp->crp_callback(crp);
d351 1
a351 1
	crp->crp_callback(crp);
d490 9
@


1.9
log
@Move prototype to include file.
@
text
@d168 1
a168 1
	       M_DONTWAIT);
d193 1
a193 1
	       M_XDATA, M_DONTWAIT);
d412 1
a412 1
	       M_DONTWAIT);
d433 1
a433 1
		   M_XDATA, M_DONTWAIT);
@


1.8
log
@Crypto kernel thread interface; requests are enqueued and processed by
a kernel thread. This allows a much cleaner interface with respect to
spl levels.
@
text
@a51 2
int crypto_invoke(struct cryptop *);

@


1.7
log
@OpenBSD tags
@
text
@d45 1
d49 5
d167 1
a167 1
        crypto_drivers_num = CRYPTO_DRIVERS_INITIAL;
d284 20
d307 1
a307 1
crypto_dispatch(struct cryptop *crp)
d368 1
d373 2
d397 2
d409 1
d416 4
a419 1
          return NULL;
d438 1
d455 1
d457 35
@


1.6
log
@crypto_dispatch() only returns an error if the argument it was
provided was NULL or no callback was specified.
@
text
@d1 2
@


1.5
log
@avoid using void * when we are talking about pointers
@
text
@d292 2
a293 1
	return crp->crp_callback(crp);
d308 2
a309 1
	return crp->crp_callback(crp);
d325 2
a326 1
	return crp->crp_callback(crp);
d329 2
a330 1
    return crypto_drivers[hid].cc_process(crp);
@


1.4
log
@Change the type of freesession to take u_int64_t as argument.
@
text
@d208 3
a210 2
crypto_register(u_int32_t driverid, int alg, void *newses, void *freeses,
		void *process)
d226 3
a228 6
	crypto_drivers[driverid].cc_newsession =
			(int (*) (u_int32_t *, struct cryptoini *)) newses;
	crypto_drivers[driverid].cc_process =
			(int (*) (struct cryptop *)) process;
	crypto_drivers[driverid].cc_freesession =
			(int (*) (u_int64_t)) freeses;
@


1.3
log
@Add a few newlines for readability.
@
text
@d118 1
a118 1
    u_int32_t hid, lid;
a125 1
    lid = sid & 0xffffffff;
d135 1
a135 1
      err = crypto_drivers[hid].cc_freesession(lid);
d230 1
a230 1
			(int (*) (u_int32_t)) freeses;
@


1.2
log
@split crypto driver front-end from software crypto engine
@
text
@d305 1
d321 1
@


1.2.2.1
log
@Sync with -current
@
text
@@


1.2.2.2
log
@merge in approximately 2.9 into SMP branch
@
text
@a0 2
/*	$OpenBSD$	*/

a42 1

a45 3
struct cryptop *crp_req_queue = NULL;
struct cryptop **crp_req_queue_tail = NULL;

d103 1
a103 1
	(*sid) <<= 32;
d118 1
a118 1
    u_int32_t hid;
d125 2
a126 1
    hid = (sid >> 32) & 0xffffffff;
d136 1
a136 1
      err = crypto_drivers[hid].cc_freesession(sid);
d160 4
a163 3
	crypto_drivers_num = CRYPTO_DRIVERS_INITIAL;
	crypto_drivers = malloc(crypto_drivers_num * sizeof(struct cryptocap),
				M_XDATA, M_NOWAIT);
d186 3
a188 2
	newdrv = malloc(2 * crypto_drivers_num * sizeof(struct cryptocap),
			M_XDATA, M_NOWAIT);
d209 2
a210 3
crypto_register(u_int32_t driverid, int alg,
    int (*newses)(u_int32_t *, struct cryptoini *),
    int (*freeses)(u_int64_t), int (*process)(struct cryptop *))
d226 6
a231 3
	crypto_drivers[driverid].cc_newsession = newses;
	crypto_drivers[driverid].cc_process = process;
	crypto_drivers[driverid].cc_freesession = freeses;
d279 1
a279 1
 * Add crypto request to a queue, to be processed by a kernel thread.
a283 20
    int s = splhigh();

    if (crp_req_queue == NULL) {
	crp_req_queue = crp;
	crp_req_queue_tail = &(crp->crp_next);
	wakeup((caddr_t) &crp_req_queue);
    } else {
	*crp_req_queue_tail = crp;
	crp_req_queue_tail = &(crp->crp_next);
    }
    splx(s);
    return 0;
}

/*
 * Dispatch a crypto request to the appropriate crypto devices.
 */
int
crypto_invoke(struct cryptop *crp)
{
d295 1
a295 2
	crypto_done(crp);
	return 0;
d298 1
a298 1
    hid = (crp->crp_sid >> 32) & 0xffffffff;
a304 1

d309 1
a309 2
	crypto_done(crp);
	return 0;
a319 1

d324 1
a324 2
	crypto_done(crp);
	return 0;
d327 1
a327 2
    crypto_drivers[hid].cc_process(crp);
    return 0;
a336 1
    int s;
a340 2
    s = splhigh();

a362 2

    splx(s);
a372 1
    int s = splhigh();
d377 1
a377 1
	       M_NOWAIT);
d379 1
a379 4
        {
            splx(s);
            return NULL;
        }
d395 1
a395 1
		   M_XDATA, M_NOWAIT);
a397 1
                splx(s);
a413 1
    splx(s);
a414 39
}

/*
 * Crypto thread, runs as a kernel thread to process crypto requests.
 */
void
crypto_thread(void)
{
    struct cryptop *crp;
    int s;

    s = splhigh();

    for (;;)
    {
	crp = crp_req_queue;
	if (crp == NULL) /* No work to do */
	{
	    (void) tsleep(&crp_req_queue, PLOCK, "crypto_wait", 0);
	    continue;
	}

	/* Remove from the queue */
	crp_req_queue = crp->crp_next;
	splx(s);

	crypto_invoke(crp);

	s = splhigh();
    }
}

/*
 * Invoke the callback on behalf of the driver.
 */
void
crypto_done(struct cryptop *crp)
{
    crp->crp_callback(crp);
@


1.2.2.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d2 1
d10 1
a10 1
 * Copyright (c) 2000, 2001 Angelos D. Keromytis
d12 1
a12 1
 * Permission to use, copy, and modify this software with or without fee
d27 12
a38 3
#include <sys/proc.h>
#include <sys/pool.h>
#include <crypto/cryptodev.h>
d43 5
a47 3
struct pool cryptop_pool;
struct pool cryptodesc_pool;
int crypto_pool_initialized = 0;
d56 1
a56 1
crypto_newsession(u_int64_t *sid, struct cryptoini *cri, int hard)
d58 14
a71 8
	struct cryptoini *cr;
	u_int32_t hid, lid;
	int err, s;

	if (crypto_drivers == NULL)
		return EINVAL;

	s = splimp();
d73 2
d76 37
a112 30
	 * The algorithm we use here is pretty stupid; just use the
	 * first driver that supports all the algorithms we need.
	 *
	 * XXX We need more smarts here (in real life too, but that's
	 * XXX another story altogether).
	 */

	for (hid = 0; hid < crypto_drivers_num; hid++) {
		/*
		 * If it's not initialized or has remaining sessions
		 * referencing it, skip.
		 */
		if (crypto_drivers[hid].cc_newsession == NULL ||
		    (crypto_drivers[hid].cc_flags & CRYPTOCAP_F_CLEANUP))
			continue;

		/* Hardware requested -- ignore software drivers. */
		if (hard &&
		    (crypto_drivers[hid].cc_flags & CRYPTOCAP_F_SOFTWARE))
			continue;

		/* See if all the algorithms are supported. */
		for (cr = cri; cr; cr = cr->cri_next)
			if (crypto_drivers[hid].cc_alg[cr->cri_alg] == 0)
			break;

		/* Ok, all algorithms are supported. */
		if (cr == NULL)
			break;
	}
d114 1
a114 24
	/*
	 * Can't do everything in one session.
	 *
	 * XXX Fix this. We need to inject a "virtual" session layer right
	 * XXX about here.
	 */

	if (hid == crypto_drivers_num) {
		splx(s);
		return EINVAL;
	}

	/* Call the driver initialization routine. */
	lid = hid; /* Pass the driver ID. */
	err = crypto_drivers[hid].cc_newsession(&lid, cri);
	if (err == 0) {
		(*sid) = hid;
		(*sid) <<= 32;
		(*sid) |= (lid & 0xffffffff);
		crypto_drivers[hid].cc_sessions++;
	}

	splx(s);
	return err;
d124 2
a125 8
	int err = 0, s;
	u_int32_t hid;

	if (crypto_drivers == NULL)
		return EINVAL;

	/* Determine two IDs. */
	hid = (sid >> 32) & 0xffffffff;
d127 2
a128 2
	if (hid >= crypto_drivers_num)
		return ENOENT;
d130 2
a131 1
	s = splimp();
d133 17
a149 2
	if (crypto_drivers[hid].cc_sessions)
		crypto_drivers[hid].cc_sessions--;
d151 1
a151 14
	/* Call the driver cleanup routine, if available. */
	if (crypto_drivers[hid].cc_freesession)
		err = crypto_drivers[hid].cc_freesession(sid);

	/*
	 * If this was the last session of a driver marked as invalid,
	 * make the entry available for reuse.
	 */
	if ((crypto_drivers[hid].cc_flags & CRYPTOCAP_F_CLEANUP) &&
	    crypto_drivers[hid].cc_sessions == 0)
		bzero(&crypto_drivers[hid], sizeof(struct cryptocap));

	splx(s);
	return err;
d160 2
a161 2
	struct cryptocap *newdrv;
	int i, s = splimp();
d163 9
a171 12
	if (crypto_drivers_num == 0) {
		crypto_drivers_num = CRYPTO_DRIVERS_INITIAL;
		crypto_drivers = malloc(crypto_drivers_num *
		    sizeof(struct cryptocap), M_CRYPTO_DATA, M_NOWAIT);
		if (crypto_drivers == NULL) {
			splx(s);
			crypto_drivers_num = 0;
			return -1;
		}

		bzero(crypto_drivers, crypto_drivers_num *
		    sizeof(struct cryptocap));
d174 2
a175 9
	for (i = 0; i < crypto_drivers_num; i++) {
		if (crypto_drivers[i].cc_process == NULL &&
		    !(crypto_drivers[i].cc_flags & CRYPTOCAP_F_CLEANUP) &&
		    crypto_drivers[i].cc_sessions == 0) {
			crypto_drivers[i].cc_sessions = 1; /* Mark */
			splx(s);
			return i;
		}
	}
d177 25
a201 28
	/* Out of entries, allocate some more. */
	if (i == crypto_drivers_num) {
		/* Be careful about wrap-around. */
		if (2 * crypto_drivers_num <= crypto_drivers_num) {
			splx(s);
			return -1;
		}

		newdrv = malloc(2 * crypto_drivers_num *
		    sizeof(struct cryptocap), M_CRYPTO_DATA, M_NOWAIT);
		if (newdrv == NULL) {
			splx(s);
			return -1;
		}

		bcopy(crypto_drivers, newdrv,
		    crypto_drivers_num * sizeof(struct cryptocap));
		bzero(&newdrv[crypto_drivers_num],
		    crypto_drivers_num * sizeof(struct cryptocap));

		newdrv[i].cc_sessions = 1; /* Mark */
		crypto_drivers_num *= 2;

		free(crypto_drivers, M_CRYPTO_DATA);
		crypto_drivers = newdrv;
		splx(s);
		return i;
	}
d203 2
a204 3
	/* Shouldn't really get here... */
	splx(s);
	return -1;
d212 1
a212 2
crypto_register(u_int32_t driverid, int alg, u_int16_t maxoplen,
    u_int32_t flags,
d216 18
a233 25
	int s;

	if (driverid >= crypto_drivers_num || alg <= 0 ||
	    alg > CRYPTO_ALGORITHM_MAX || crypto_drivers == NULL)
		return EINVAL;

	s = splimp();

	/*
	 * XXX Do some performance testing to determine placing.
	 * XXX We probably need an auxiliary data structure that describes
	 * XXX relative performances.
	 */

	crypto_drivers[driverid].cc_alg[alg] = 
	    flags | CRYPTO_ALG_FLAG_SUPPORTED;

	crypto_drivers[driverid].cc_max_op_len[alg] = maxoplen;

	if (crypto_drivers[driverid].cc_process == NULL) {
		crypto_drivers[driverid].cc_newsession = newses;
		crypto_drivers[driverid].cc_process = process;
		crypto_drivers[driverid].cc_freesession = freeses;
		crypto_drivers[driverid].cc_sessions = 0; /* Unmark */
	}
d235 1
a235 2
	splx(s);
	return 0;
d247 20
a266 2
	int i, s = splimp();
	u_int32_t ses;
d268 5
a272 6
	/* Sanity checks */
	if (driverid >= crypto_drivers_num || alg <= 0 ||
	    alg > CRYPTO_ALGORITHM_MAX || crypto_drivers == NULL ||
	    crypto_drivers[driverid].cc_alg[alg] == 0) {
		splx(s);
		return EINVAL;
d274 1
d276 1
a276 21
	crypto_drivers[driverid].cc_alg[alg] = 0;
	crypto_drivers[driverid].cc_max_op_len[alg] = 0;

	/* Was this the last algorithm ? */
	for (i = 1; i <= CRYPTO_ALGORITHM_MAX; i++)
		if (crypto_drivers[driverid].cc_alg[i] != 0)
			break;

	if (i == CRYPTO_ALGORITHM_MAX + 1) {
		ses = crypto_drivers[driverid].cc_sessions;
		bzero(&crypto_drivers[driverid], sizeof(struct cryptocap));
		if (ses != 0) {
			/*
			 * If there are pending sessions, just mark as invalid.
			 */
			crypto_drivers[driverid].cc_flags |= CRYPTOCAP_F_CLEANUP;
			crypto_drivers[driverid].cc_sessions = ses;
		}
	}
	splx(s);
	return 0;
d285 1
a285 1
	int s = splimp();
d287 10
a296 11
	if (crp_req_queue == NULL) {
		crp_req_queue = crp;
		crp_req_queue_tail = &(crp->crp_next);
		splx(s);
		wakeup((caddr_t) &crp_req_queue);
	} else {
		*crp_req_queue_tail = crp;
		crp_req_queue_tail = &(crp->crp_next);
		splx(s);
	}
	return 0;
d305 30
a334 13
	struct cryptodesc *crd;
	u_int64_t nid;
	u_int32_t hid;

	/* Sanity checks. */
	if (crp == NULL || crp->crp_callback == NULL)
		return EINVAL;

	if (crp->crp_desc == NULL || crypto_drivers == NULL) {
		crp->crp_etype = EINVAL;
		crypto_done(crp);
		return 0;
	}
d336 2
a337 13
	hid = (crp->crp_sid >> 32) & 0xffffffff;
	if (hid >= crypto_drivers_num) {
		/* Migrate session. */
		for (crd = crp->crp_desc; crd->crd_next; crd = crd->crd_next)
			crd->CRD_INI.cri_next = &(crd->crd_next->CRD_INI);

		if (crypto_newsession(&nid, &(crp->crp_desc->CRD_INI), 0) == 0)
			crp->crp_sid = nid;

		crp->crp_etype = EAGAIN;
		crypto_done(crp);
		return 0;
	}
d339 5
a343 2
	if (crypto_drivers[hid].cc_flags & CRYPTOCAP_F_CLEANUP)
		crypto_freesession(crp->crp_sid);
d345 2
a346 12
	if (crypto_drivers[hid].cc_process == NULL) {
		/* Migrate session. */
		for (crd = crp->crp_desc; crd->crd_next; crd = crd->crd_next)
			crd->CRD_INI.cri_next = &(crd->crd_next->CRD_INI);

		if (crypto_newsession(&nid, &(crp->crp_desc->CRD_INI), 0) == 0)
			crp->crp_sid = nid;

		crp->crp_etype = EAGAIN;
		crypto_done(crp);
		return 0;
	}
d348 2
a349 1
	crypto_drivers[hid].cc_process(crp);
d351 4
d363 5
a367 2
	struct cryptodesc *crd;
	int s;
d369 1
a369 2
	if (crp == NULL)
		return;
d371 3
a373 1
	s = splimp();
d375 18
a392 4
	while ((crd = crp->crp_desc) != NULL) {
		crp->crp_desc = crd->crd_next;
		pool_put(&cryptodesc_pool, crd);
	}
d394 1
a394 2
	pool_put(&cryptop_pool, crp);
	splx(s);
d403 35
a437 10
	struct cryptodesc *crd;
	struct cryptop *crp;
	int s = splimp();

	if (crypto_pool_initialized == 0) {
		pool_init(&cryptop_pool, sizeof(struct cryptop), 0, 0,
		    PR_FREEHEADER, "cryptop", 0, NULL, NULL, M_CRYPTO_OPS);
		pool_init(&cryptodesc_pool, sizeof(struct cryptodesc), 0, 0,
		    PR_FREEHEADER, "cryptodesc", 0, NULL, NULL, M_CRYPTO_OPS);
		crypto_pool_initialized = 1;
d439 5
a443 5

	crp = pool_get(&cryptop_pool, 0);
	if (crp == NULL) {
		splx(s);
		return NULL;
a444 1
	bzero(crp, sizeof(struct cryptop));
d446 4
a449 12
	while (num--) {
		crd = pool_get(&cryptodesc_pool, 0);
		if (crd == NULL) {
			splx(s);
			crypto_freereq(crp);
			return NULL;
		}

		bzero(crd, sizeof(struct cryptodesc));
		crd->crd_next = crp->crp_desc;
		crp->crp_desc = crd;
	}
d451 2
a452 2
	splx(s);
	return crp;
d461 2
a462 2
	struct cryptop *crp;
	int s;
d464 1
a464 1
	s = splimp();
d466 7
a472 10
	for (;;) {
		crp = crp_req_queue;
		if (crp == NULL) {
			(void) tsleep(&crp_req_queue, PLOCK, "crypto_wait", 0);
			continue;
		}

		/* Remove from the queue. */
		crp_req_queue = crp->crp_next;
		crypto_invoke(crp);
d474 9
d491 1
a491 31
	crp->crp_callback(crp);
}

/*
 * Return SYMMETRIC or PUBLIC_KEY, depending on the algorithm type.
 */
int
crypto_check_alg(struct cryptoini *cri)
{
	switch (cri->cri_alg)
	{
	case CRYPTO_DES_CBC:
	case CRYPTO_3DES_CBC:
	case CRYPTO_BLF_CBC:
	case CRYPTO_CAST_CBC:
	case CRYPTO_SKIPJACK_CBC:
	case CRYPTO_RIJNDAEL128_CBC:
		return SYMMETRIC;
	case CRYPTO_DH_SEND:
	case CRYPTO_DH_RECEIVE:
	case CRYPTO_RSA_ENCRYPT:
	case CRYPTO_RSA_DECRYPT:
	case CRYPTO_DSA_SIGN:
	case CRYPTO_DSA_VERIFY:
		return PUBLIC_KEY;
	}

#ifdef DIAGNOSTIC
	panic("crypto_check_alg: unknown algorithm %d", cri->cri_alg);
#endif
	return -1;
@


1.2.2.4
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.2.2.3 2001/07/04 10:39:58 niklas Exp $	*/
a489 1
	case CRYPTO_ARC4:
@


1.2.2.5
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a71 3
		if (crypto_drivers[hid].cc_sessions == 0)
			continue;

d80 1
a80 1
				break;
d157 1
a157 1
crypto_get_driverid(u_int8_t flags)
a206 1
		newdrv[i].cc_flags = flags;
@


1.2.2.6
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.2.2.5 2001/11/13 21:05:48 niklas Exp $	*/
d72 3
a183 1
			crypto_drivers[i].cc_flags = flags;
@


1.2.2.7
log
@Merge in trunk
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d415 1
a415 1
		    0, "cryptop", NULL);
d417 1
a417 1
		    0, "cryptodesc", NULL);
d476 31
@


1.2.2.8
log
@Sync the SMP branch with 3.3
@
text
@d14 1
a14 1
 * modification of this software.
a39 9
struct cryptop *crp_ret_queue = NULL;
struct cryptop **crp_ret_queue_tail = NULL;

struct cryptkop *krp_req_queue = NULL;
struct cryptkop **krp_req_queue_tail = NULL;

struct cryptkop *krp_ret_queue = NULL;
struct cryptkop **krp_ret_queue_tail = NULL;

a45 2
	u_int32_t hid, lid, hid2 = -1;
	struct cryptocap *cpc;
d47 2
a48 1
	int err, s, turn = 0;
d57 1
a57 4
	 * first driver that supports all the algorithms we need. Do
	 * a double-pass over all the drivers, ignoring software ones
	 * at first, to deal with cases of drivers that register after
	 * the software one(s) --- e.g., PCMCIA crypto cards.
a61 61
	do {
		for (hid = 0; hid < crypto_drivers_num; hid++) {
			cpc = &crypto_drivers[hid];

			/*
			 * If it's not initialized or has remaining sessions
			 * referencing it, skip.
			 */
			if (cpc->cc_newsession == NULL ||
			    (cpc->cc_flags & CRYPTOCAP_F_CLEANUP))
				continue;

			if (cpc->cc_flags & CRYPTOCAP_F_SOFTWARE) {
				/*
				 * First round of search, ignore
				 * software drivers.
				 */
				if (turn == 0)
					continue;
			} else { /* !CRYPTOCAP_F_SOFTWARE */
				/* Second round of search, only software. */
				if (turn == 1)
					continue;
			}

			/* See if all the algorithms are supported. */
			for (cr = cri; cr; cr = cr->cri_next) {
				if (cpc->cc_alg[cr->cri_alg] == 0)
					break;
			}

			/*
			 * If even one algorithm is not supported,
			 * keep searching.
			 */
			if (cr != NULL)
				continue;

			/*
			 * If we had a previous match, see how it compares
			 * to this one. Keep "remembering" whichever is
			 * the best of the two.
			 */
			if (hid2 != -1) {
				/*
				 * Compare session numbers, pick the one
				 * with the lowest.
				 * XXX Need better metrics, this will
				 * XXX just do un-weighted round-robin.
				 */
				if (crypto_drivers[hid].cc_sessions <=
				    crypto_drivers[hid2].cc_sessions)
					hid2 = hid;
			} else {
				/*
				 * Remember this one, for future
                                 * comparisons.
				 */
				hid2 = hid;
			}
		}
d63 1
d65 2
a66 3
		 * If we found something worth remembering, leave. The
		 * side-effect is that we will always prefer a hardware
		 * driver over the software one.
d68 3
a70 2
		if (hid2 != -1)
			break;
d72 4
a75 1
		turn++;
d77 4
a80 2
		/* If we only want hardware drivers, don't do second pass. */
	} while (turn <= 2 && hard == 0);
d82 4
a85 1
	hid = hid2;
d90 2
a91 2
	 * XXX Fix this. We need to inject a "virtual" session
	 * XXX layer right about here.
d94 1
a94 1
	if (hid == -1) {
d227 4
a230 2
crypto_kregister(u_int32_t driverid, int *kalg,
    int (*kprocess)(struct cryptkop *))
d232 1
a232 1
	int s, i;
d234 2
a235 2
	if (driverid >= crypto_drivers_num || kalg  == NULL ||
	    crypto_drivers == NULL)
d240 5
a244 6
	for (i = 0; i < CRK_ALGORITHM_MAX; i++) {
		/*
		 * XXX Do some performance testing to determine
		 * placing.  We probably need an auxiliary data
		 * structure that describes relative performances.
		 */
d246 2
a247 2
		crypto_drivers[driverid].cc_kalg[i] = kalg[i];
	}
d249 1
a249 1
	crypto_drivers[driverid].cc_kprocess = kprocess;
d251 5
a255 27
	splx(s);
	return 0;
}

/* Register a crypto driver. */
int
crypto_register(u_int32_t driverid, int *alg,
    int (*newses)(u_int32_t *, struct cryptoini *),
    int (*freeses)(u_int64_t), int (*process)(struct cryptop *))
{
	int s, i;


	if (driverid >= crypto_drivers_num || alg == NULL ||
	    crypto_drivers == NULL)
		return EINVAL;
	
	s = splimp();

	for (i = 0; i < CRYPTO_ALGORITHM_ALL; i++) {
		/*
		 * XXX Do some performance testing to determine
		 * placing.  We probably need an auxiliary data
		 * structure that describes relative performances.
		 */

		crypto_drivers[driverid].cc_alg[i] = alg[i];
a257 6

	crypto_drivers[driverid].cc_newsession = newses;
	crypto_drivers[driverid].cc_process = process;
	crypto_drivers[driverid].cc_freesession = freeses;
	crypto_drivers[driverid].cc_sessions = 0; /* Unmark */

a258 1

d271 1
a271 1
	int i = CRYPTO_ALGORITHM_MAX + 1, s = splimp();
d274 3
a276 4
	/* Sanity checks. */
	if (driverid >= crypto_drivers_num || crypto_drivers == NULL ||
	    ((alg <= 0 || alg > CRYPTO_ALGORITHM_MAX) &&
		alg != CRYPTO_ALGORITHM_ALL) ||
d282 2
a283 2
	if (alg != CRYPTO_ALGORITHM_ALL) {
		crypto_drivers[driverid].cc_alg[alg] = 0;
d285 4
a288 5
		/* Was this the last algorithm ? */
		for (i = 1; i <= CRYPTO_ALGORITHM_MAX; i++)
			if (crypto_drivers[driverid].cc_alg[i] != 0)
				break;
	}
d290 1
a290 5
	/*
	 * If a driver unregistered its last algorithm or all of them
	 * (alg == CRYPTO_ALGORITHM_ALL), cleanup its entry.
	 */
	if (i == CRYPTO_ALGORITHM_MAX + 1 || alg == CRYPTO_ALGORITHM_ALL) {
a311 1
	u_int32_t hid;
a312 10
	/*
	 * Keep track of ops per driver, for coallescing purposes. If
	 * we have been given an invalid hid, we'll deal with in the
	 * crypto_invoke(), through session migration.
	 */
	hid = (crp->crp_sid >> 32) & 0xffffffff;
	if (hid < crypto_drivers_num)
		crypto_drivers[hid].cc_queued++;

	crp->crp_next = NULL;
d317 1
a317 1
		wakeup((caddr_t) &crp_req_queue); /* Shared wait channel. */
a325 63
int
crypto_kdispatch(struct cryptkop *krp)
{
	int s = splimp();

	krp->krp_next = NULL;
	if (krp_req_queue == NULL) {
		krp_req_queue = krp;
		krp_req_queue_tail = &(krp->krp_next);
		splx(s);
		wakeup((caddr_t) &crp_req_queue); /* Shared wait channel. */
	} else {
		*krp_req_queue_tail = krp;
		krp_req_queue_tail = &(krp->krp_next);
		splx(s);
	}
	return 0;
}

/*
 * Dispatch an asymmetric crypto request to the appropriate crypto devices.
 */
int
crypto_kinvoke(struct cryptkop *krp)
{
	extern int cryptodevallowsoft;
	u_int32_t hid;
	int error;

	/* Sanity checks. */
	if (krp == NULL || krp->krp_callback == NULL)
		return (EINVAL);

	for (hid = 0; hid < crypto_drivers_num; hid++) {
		if ((crypto_drivers[hid].cc_flags & CRYPTOCAP_F_SOFTWARE) &&
		    cryptodevallowsoft == 0)
			continue;
		if (crypto_drivers[hid].cc_kprocess == NULL)
			continue;
		if ((crypto_drivers[hid].cc_kalg[krp->krp_op] &
		    CRYPTO_ALG_FLAG_SUPPORTED) == 0)
			continue;
		break;
	}

	if (hid == crypto_drivers_num) {
		krp->krp_status = ENODEV;
		crypto_kdone(krp);
		return (0);
	}

	krp->krp_hid = hid;

	crypto_drivers[hid].cc_koperations++;

	error = crypto_drivers[hid].cc_kprocess(krp);
	if (error) {
		krp->krp_status = error;
		crypto_kdone(krp);
	}
	return (0);
}

a334 1
	int error;
d347 7
a353 2
	if (hid >= crypto_drivers_num)
		goto migrate;
d355 4
a358 1
	crypto_drivers[hid].cc_queued--;
d360 1
a360 1
	if (crypto_drivers[hid].cc_flags & CRYPTOCAP_F_CLEANUP) {
a361 2
		goto migrate;
	}
d363 4
a366 2
	if (crypto_drivers[hid].cc_process == NULL)
		goto migrate;
d368 2
a369 2
	crypto_drivers[hid].cc_operations++;
	crypto_drivers[hid].cc_bytes += crp->crp_ilen;
d371 3
a373 10
	error = crypto_drivers[hid].cc_process(crp);
	if (error) {
		if (error == ERESTART) {
			/* Unregister driver and migrate session. */
			crypto_unregister(hid, CRYPTO_ALGORITHM_ALL);
			goto migrate;
		} else {
			crp->crp_etype = error;
			crypto_done(crp);
		}
d376 1
a376 12
	return 0;

 migrate:
	/* Migrate session. */
	for (crd = crp->crp_desc; crd->crd_next; crd = crd->crd_next)
		crd->CRD_INI.cri_next = &(crd->crd_next->CRD_INI);

	if (crypto_newsession(&nid, &(crp->crp_desc->CRD_INI), 0) == 0)
		crp->crp_sid = nid;

	crp->crp_etype = EAGAIN;
	crypto_done(crp);
d451 1
a451 2
	struct cryptop *crp, *crpt;
	struct cryptkop *krp, *krpt;
d458 1
a458 5
		krp = krp_req_queue;
		crpt = crp_ret_queue;
		krpt = krp_ret_queue;
		if (crp == NULL && krp == NULL &&
		    crpt == NULL && krpt == NULL) {
d463 3
a465 28
		if (crp) {
			/* Remove from the queue. */
			crp_req_queue = crp->crp_next;
			crypto_invoke(crp);
		}
		if (krp) {
			/* Remove from the queue. */
			krp_req_queue = krp->krp_next;
			crypto_kinvoke(krp);
		}
		if (crpt) {
			/* Remove from the queue. */
			crp_ret_queue = crpt->crp_next;
			splx(s);
			crpt->crp_callback(crpt);
			splimp();
		}
		if (krpt) {
			/* Remove from the queue. */
			krp_ret_queue = krpt->krp_next;
			/*
			 * Cheat. For public key ops, we know that
			 * all that's done is a wakeup() for the
			 * userland process, so don't bother to
			 * change the processor priority.
			 */
			krpt->krp_callback(krpt);
		}
d475 1
a475 59
	int s = splimp();

	crp->crp_next = NULL;
	if (crp_ret_queue == NULL) {
		crp_ret_queue = crp;
		crp_ret_queue_tail = &(crp->crp_next);
		splx(s);
		wakeup((caddr_t) &crp_req_queue); /* Shared wait channel. */
	} else {
		*crp_ret_queue_tail = crp;
		crp_ret_queue_tail = &(crp->crp_next);
		splx(s);
	}
}

/*
 * Invoke the callback on behalf of the driver.
 */
void
crypto_kdone(struct cryptkop *krp)
{
	int s = splimp();

	krp->krp_next = NULL;
	if (krp_ret_queue == NULL) {
		krp_ret_queue = krp;
		krp_ret_queue_tail = &(krp->krp_next);
		splx(s);
		wakeup((caddr_t) &crp_req_queue); /* Shared wait channel. */
	} else {
		*krp_ret_queue_tail = krp;
		krp_ret_queue_tail = &(krp->krp_next);
		splx(s);
	}
}

int
crypto_getfeat(int *featp)
{
	extern int cryptodevallowsoft, userasymcrypto;
	int hid, kalg, feat = 0;

	if (userasymcrypto == 0)
		goto out;	  
	for (hid = 0; hid < crypto_drivers_num; hid++) {
		if ((crypto_drivers[hid].cc_flags & CRYPTOCAP_F_SOFTWARE) &&
		    cryptodevallowsoft == 0) {
			continue;
		}
		if (crypto_drivers[hid].cc_kprocess == NULL)
			continue;
		for (kalg = 0; kalg < CRK_ALGORITHM_MAX; kalg++)
			if ((crypto_drivers[hid].cc_kalg[kalg] &
			    CRYPTO_ALG_FLAG_SUPPORTED) != 0)
				feat |=  1 << kalg;
	}
out:
	*featp = feat;
	return (0);
@


1.2.2.9
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: crypto.c,v 1.2.2.8 2003/03/27 23:53:48 niklas Exp $	*/
d689 1
a689 1
	int s;
d691 6
a696 6
	if (crp->crp_flags & CRYPTO_F_NOQUEUE) {
		/* not from the crypto queue, wakeup the userland
		 * process 
		 */
		crp->crp_flags |= CRYPTO_F_DONE;
		crp->crp_callback(crp);
d698 3
a700 13
		s = splimp();
		crp->crp_flags |= CRYPTO_F_DONE;
		crp->crp_next = NULL;
		if (crp_ret_queue == NULL) {
			crp_ret_queue = crp;
			crp_ret_queue_tail = &(crp->crp_next);
			splx(s);
			wakeup((caddr_t) &crp_req_queue); /* Shared wait channel. */
		} else {
			*crp_ret_queue_tail = crp;
			crp_ret_queue_tail = &(crp->crp_next);
			splx(s);
		}
@


1.1
log
@Cryptographic services framework, and software "device driver". The
idea is to support various cryptographic hardware accelerators (which
may be (detachable) cards, secondary/tertiary/etc processors,
software crypto, etc). Supports session migration between crypto
devices. What it doesn't (yet) support:
 - multiple instances of the same algorithm used in the same session
 - use of multiple crypto drivers in the same session
 - asymmetric crypto

No support for a userland device yet.

IPsec code path modified to allow for asynchronous cryptography
(callbacks used in both input and output processing). Some unrelated
code simplification done in the process (especially for AH).

Development of this code kindly supported by Network Security
Technologies (NSTI). The code was writen mostly in Greece, and is
being committed from Montreal.
@
text
@a40 4
struct swcr_data **swcr_sessions = NULL;
u_int32_t swcr_sesnum = 0;
int32_t swcr_id = -1;

a45 20
u_int8_t hmac_ipad_buffer[64] = {
    0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 
    0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 
    0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 
    0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 
    0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 
    0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 
    0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 
    0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 0x36, 0x36 };

u_int8_t hmac_opad_buffer[64] = {
    0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C,
    0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C,
    0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C,
    0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C,
    0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C,
    0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C,
    0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C,
    0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C, 0x5C };

a414 701
}

/*
 * Apply a symmetric encryption/decryption algorithm.
 */
int
swcr_encdec(struct cryptodesc *crd, struct swcr_data *sw, caddr_t buf,
	    int outtype)
{
    unsigned char iv[EALG_MAX_BLOCK_LEN], blk[EALG_MAX_BLOCK_LEN], *idat;
    unsigned char *ivp, piv[EALG_MAX_BLOCK_LEN];
    struct enc_xform *exf;
    int i, k, j, blks;
    struct mbuf *m;

    exf = sw->sw_exf;
    blks = exf->blocksize;

    /* Check for non-padded data */
    if (crd->crd_len % blks)
      return EINVAL;

    if (outtype == CRYPTO_BUF_CONTIG)
    {
	if (crd->crd_flags & CRD_F_ENCRYPT)
	{
	    /* Inject IV */
	    if (crd->crd_flags & CRD_F_HALFIV)
	    {
		/* "Cook" half-IV */
		for (k = 0; k < blks / 2; k++)
		  sw->sw_iv[(blks / 2) + k] = ~sw->sw_iv[k];

	        bcopy(sw->sw_iv, buf + crd->crd_inject, blks / 2);
	    }
	    else
	      bcopy(sw->sw_iv, buf + crd->crd_inject, blks);

	    for (i = crd->crd_skip;
		 i < crd->crd_skip + crd->crd_len;
		 i += blks)
	    {
		/* XOR with the IV/previous block, as appropriate. */
		if (i == crd->crd_skip)
		  for (k = 0; k < blks; k++)
		    buf[i + k] ^= sw->sw_iv[k];
		else
		  for (k = 0; k < blks; k++)
		    buf[i + k] ^= buf[i + k - blks];

		exf->encrypt(sw->sw_kschedule, buf + i);
	    }

	    /* Keep the last block */
	    bcopy(buf + crd->crd_len - blks, sw->sw_iv, blks);
	}
	else /* Decrypt */
	{
	    /* Copy the IV off the buffer */
	    bcopy(buf + crd->crd_inject, sw->sw_iv, blks);

	    /* "Cook" half-IV */
	    if (crd->crd_flags & CRD_F_HALFIV)
	      for (k = 0; k < blks / 2; k++)
		sw->sw_iv[(blks / 2) + k] = ~sw->sw_iv[k];

	    /*
	     * Start at the end, so we don't need to keep the encrypted
	     * block as the IV for the next block.
	     */
	    for (i = crd->crd_skip + crd->crd_len - blks;
		 i >= crd->crd_skip;
		 i -= blks)
	    {
		exf->decrypt(sw->sw_kschedule, buf + i);

		/* XOR with the IV/previous block, as appropriate */
		if (i == crd->crd_skip)
		  for (k = 0; k < blks; k++)
		    buf[i + k] ^= sw->sw_iv[k];
		else
		  for (k = 0; k < blks; k++)
		    buf[i + k] ^= buf[i + k - blks];
	    }
	}

	return 0; /* Done with contiguous buffer encryption/decryption */
    }
    else /* mbuf */
    {
	m = (struct mbuf *) buf;

	/* Initialize the IV */
	if (crd->crd_flags & CRD_F_ENCRYPT)
	{
	    bcopy(sw->sw_iv, iv, blks);

	    /* "Cook" half-IV */
	    if (crd->crd_flags & CRD_F_HALFIV)
	      for (k = 0; k < blks / 2; k++)
	        iv[(blks / 2) + k] = ~iv[k];

	    /* Inject IV */
	    m_copyback(m, crd->crd_inject, blks, iv);
	}
	else
	{
	    m_copydata(m, crd->crd_inject, blks, iv); /* Get IV off mbuf */

	    /* "Cook" half-IV */
	    if (crd->crd_flags & CRD_F_HALFIV)
	      for (k = 0; k < blks / 2; k++)
	        iv[(blks / 2) + k] = ~iv[k];
	}

	ivp = iv;

	/* Find beginning of data */
	m = m_getptr(m, crd->crd_skip, &k);
	if (m == NULL)
	  return EINVAL;

	i = crd->crd_len;

	while (i > 0)
	{
	    /*
	     * If there's insufficient data at the end of an mbuf, we have
	     * to do some copying.
	     */
	    if ((m->m_len < k + blks) && (m->m_len != k))
	    {
		m_copydata(m, k, blks, blk);

		/* Actual encryption/decryption */
		if (crd->crd_flags & CRD_F_ENCRYPT)
		{
		    /* XOR with previous block */
		    for (j = 0; j < blks; j++)
		      blk[j] ^= ivp[j];

		    exf->encrypt(sw->sw_kschedule, blk);

		    /* Keep encrypted block for XOR'ing with next block */
		    bcopy(blk, iv, blks);
		    ivp = iv;
		}
		else /* decrypt */
		{
		    /* Keep encrypted block for XOR'ing with next block */
		    if (ivp == iv)
		      bcopy(blk, piv, blks);
		    else
		      bcopy(blk, iv, blks);

		    exf->decrypt(sw->sw_kschedule, blk);

		    /* XOR with previous block */
		    for (j = 0; j < blks; j++)
		      blk[j] ^= ivp[j];

		    if (ivp == iv)
		      bcopy(piv, iv, blks);
		    else
		      ivp = iv;
		}

		/* Copy back decrypted block */
		m_copyback(m, k, blks, blk);

		/* Advance pointer */
		m = m_getptr(m, k + blks, &k);
		if (m == NULL)
		  return EINVAL;

		i -= blks;

		/* Could be done... */
		if (i == 0)
		  break;
	    }

	    /* Skip possibly empty mbufs */
	    if (k == m->m_len)
	    {
		for (m = m->m_next; m && m->m_len == 0; m = m->m_next)
		  ;

		k = 0;
	    }

	    /* Sanity check */
	    if (m == NULL)
	      return EINVAL;

	    /*
	     * Warning: idat may point to garbage here, but we only use it
	     * in the while() loop, only if there are indeed enough data.
	     */
	    idat = mtod(m, unsigned char *) + k;

	    while ((m->m_len >= k + blks) && (i > 0))
	    {
		if (crd->crd_flags & CRD_F_ENCRYPT)
		{
		    /* XOR with previous block/IV */
		    for (j = 0; j < blks; j++)
		      idat[j] ^= ivp[j];

		    exf->encrypt(sw->sw_kschedule, idat);
		    ivp = idat;
		}
		else /* decrypt */
		{
		    /*
		     * Keep encrypted block to be used in next block's
		     * processing.
		     */
		    if (ivp == iv)
		      bcopy(idat, piv, blks);
		    else
		      bcopy(idat, iv, blks);

		    exf->decrypt(sw->sw_kschedule, idat);

		    /* XOR with previous block/IV */
		    for (j = 0; j < blks; j++)
		      idat[j] ^= ivp[j];

		    if (ivp == iv)
		      bcopy(piv, iv, blks);
		    else
		      ivp = iv;
		}

		idat += blks;
		k += blks;
		i -= blks;
	    }
	}

	/* Keep the last block */
	if (crd->crd_flags & CRD_F_ENCRYPT)
	  bcopy(ivp, sw->sw_iv, blks);

	return 0; /* Done with mbuf encryption/decryption */
    }

    /* Unreachable */
    return EINVAL;
}

/*
 * Compute keyed-hash authenticator.
 */
int
swcr_authcompute(struct cryptodesc *crd, struct swcr_data *sw,
		 caddr_t buf, int outtype)
{
    unsigned char aalg[AALG_MAX_RESULT_LEN];
    struct auth_hash *axf;
    union authctx ctx;
    int err;

    if (sw->sw_ictx == 0)
      return EINVAL;

    axf = sw->sw_axf;

    bcopy(sw->sw_ictx, &ctx, axf->ctxsize);

    if (outtype == CRYPTO_BUF_CONTIG)
    {
	axf->Update(&ctx, buf + crd->crd_skip, crd->crd_len);
	axf->Final(aalg, &ctx);
    }
    else
    {
	err = m_apply((struct mbuf *) buf, crd->crd_skip,
		      crd->crd_len,
		      (int (*)(caddr_t, caddr_t, unsigned int)) axf->Update,
		      (caddr_t) &ctx);
	if (err)
	  return err;

	axf->Final(aalg, &ctx);
    }

    /* HMAC processing */
    switch (sw->sw_alg)
    {
	case CRYPTO_MD5_HMAC96:
	case CRYPTO_SHA1_HMAC96:
	case CRYPTO_RIPEMD160_HMAC96:
	    if (sw->sw_octx == NULL)
	      return EINVAL;

	    bcopy(sw->sw_octx, &ctx, axf->ctxsize);
	    axf->Update(&ctx, aalg, axf->hashsize);
	    axf->Final(aalg, &ctx);
	    break;
    }

    /* Inject the authentication data */
    if (outtype == CRYPTO_BUF_CONTIG)
      bcopy(aalg, buf + crd->crd_inject, axf->authsize);
    else
      m_copyback((struct mbuf *) buf, crd->crd_inject, axf->authsize, aalg);

    return 0;
}

/*
 * Generate a new software session.
 */
int
swcr_newsession(u_int32_t *sid, struct cryptoini *cri)
{
    struct swcr_data **swd;
    struct auth_hash *axf;
    struct enc_xform *txf;
    u_int32_t i;
    int k;

    if ((sid == NULL) || (cri == NULL))
      return EINVAL;

    if (swcr_sessions)
      for (i = 1; i < swcr_sesnum; i++)
	if (swcr_sessions[i] == NULL)
	  break;

    if ((swcr_sessions == NULL) || (i == swcr_sesnum))
    {
	if (swcr_sessions == NULL)
	{
	    i = 1; /* We leave swcr_sessions[0] empty */
	    swcr_sesnum = CRYPTO_SW_SESSIONS;
	}
	else
	  swcr_sesnum *= 2;

	MALLOC(swd, struct swcr_data **,
	       swcr_sesnum * sizeof(struct swcr_data *), M_XDATA, M_DONTWAIT);
	if (swd == NULL)
	{
	    /* Reset session number */
	    if (swcr_sesnum == CRYPTO_SW_SESSIONS)
	      swcr_sesnum = 0;
	    else
	      swcr_sesnum /= 2;

	    return ENOBUFS;
	}

	bzero(swd, swcr_sesnum * sizeof(struct swcr_data *));

	/* Copy existing sessions */
	if (swcr_sessions)
	{
	    bcopy(swcr_sessions, swd,
		  (swcr_sesnum / 2) * sizeof(struct swcr_data *));
	    FREE(swcr_sessions, M_XDATA);
	}

	swcr_sessions = swd;
    }

    swd = &swcr_sessions[i];
    *sid = i;

    while (cri)
    {
	MALLOC(*swd, struct swcr_data *, sizeof(struct swcr_data), M_XDATA,
	       M_DONTWAIT);
	if (*swd == NULL)
	{
	    swcr_freesession(i);
	    return ENOBUFS;
	}

	bzero(*swd, sizeof(struct swcr_data));

	switch (cri->cri_alg)
	{
	    case CRYPTO_DES_CBC:
		txf = &enc_xform_des;
		goto enccommon;

	    case CRYPTO_3DES_CBC:
		txf = &enc_xform_3des;
		goto enccommon;

	    case CRYPTO_BLF_CBC:
		txf = &enc_xform_blf;
		goto enccommon;

	    case CRYPTO_CAST_CBC:
		txf = &enc_xform_cast5;
		goto enccommon;

	    case CRYPTO_SKIPJACK_CBC:
		txf = &enc_xform_skipjack;

	enccommon:
		txf->setkey(&((*swd)->sw_kschedule), cri->cri_key,
			    cri->cri_klen / 8);
		MALLOC((*swd)->sw_iv, u_int8_t *, txf->blocksize, M_XDATA,
		       M_DONTWAIT);
		if ((*swd)->sw_iv == NULL)
		{
		    swcr_freesession(i);
		    return ENOBUFS;
		}

		(*swd)->sw_exf = txf;

		get_random_bytes((*swd)->sw_iv, txf->blocksize);
		break;

	    case CRYPTO_MD5_HMAC96:
		axf = &auth_hash_hmac_md5_96;
		goto authcommon;

	    case CRYPTO_SHA1_HMAC96:
		axf = &auth_hash_hmac_sha1_96;
		goto authcommon;
		
	    case CRYPTO_RIPEMD160_HMAC96:
		axf = &auth_hash_hmac_ripemd_160_96;

	authcommon:
		MALLOC((*swd)->sw_ictx, u_int8_t *, axf->ctxsize, M_XDATA,
		       M_DONTWAIT);
		if ((*swd)->sw_ictx == NULL)
		{
		    swcr_freesession(i);
		    return ENOBUFS;
		}

		MALLOC((*swd)->sw_octx, u_int8_t *, axf->ctxsize, M_XDATA,
		       M_DONTWAIT);
		if ((*swd)->sw_octx == NULL)
		{
		    swcr_freesession(i);
		    return ENOBUFS;
		}

		for (k = 0; k < cri->cri_klen / 8; k++)
		  cri->cri_key[k] ^= HMAC_IPAD_VAL;

		axf->Init((*swd)->sw_ictx);
		axf->Update((*swd)->sw_ictx, cri->cri_key,
			    cri->cri_klen / 8);
		axf->Update((*swd)->sw_ictx, hmac_ipad_buffer,
			    HMAC_BLOCK_LEN - (cri->cri_klen / 8));

		for (k = 0; k < cri->cri_klen / 8; k++)
		  cri->cri_key[k] ^= (HMAC_IPAD_VAL ^ HMAC_OPAD_VAL);

		axf->Init((*swd)->sw_octx);
		axf->Update((*swd)->sw_octx, cri->cri_key,
			    cri->cri_klen / 8);
		axf->Update((*swd)->sw_octx, hmac_opad_buffer,
			    HMAC_BLOCK_LEN - (cri->cri_klen / 8));

		for (k = 0; k < cri->cri_klen / 8; k++)
		  cri->cri_key[k] ^= HMAC_OPAD_VAL;

		(*swd)->sw_axf = axf;
		break;

	    case CRYPTO_MD5_KPDK:
		axf = &auth_hash_key_md5;
		goto auth2common;

	    case CRYPTO_SHA1_KPDK:
		axf = &auth_hash_key_sha1;

	auth2common:
		MALLOC((*swd)->sw_ictx, u_int8_t *, axf->ctxsize, M_XDATA,
		       M_DONTWAIT);
		if ((*swd)->sw_ictx == NULL)
		{
		    swcr_freesession(i);
		    return ENOBUFS;
		}

		axf->Init((*swd)->sw_ictx);
		axf->Update((*swd)->sw_ictx, cri->cri_key,
			    cri->cri_klen / 8);
		axf->Final(NULL, (*swd)->sw_ictx);

		(*swd)->sw_axf = axf;
		break;

	    default:
		swcr_freesession(i);
		return EINVAL;
	}

	(*swd)->sw_alg = cri->cri_alg;
	cri = cri->cri_next;
	swd = &((*swd)->sw_next);
    }

    return 0;
}

/*
 * Free a session.
 */
int
swcr_freesession(u_int32_t sid)
{
    struct swcr_data *swd;
    struct enc_xform *txf;
    struct auth_hash *axf;

    if ((sid > swcr_sesnum) || (swcr_sessions == NULL) ||
	(swcr_sessions[sid] == NULL))
      return EINVAL;

    /* Silently accept and return */
    if (sid == 0)
      return 0;

    while ((swd = swcr_sessions[sid]) != NULL)
    {
        swcr_sessions[sid] = swd->sw_next;

	switch (swd->sw_alg)
	{
	    case CRYPTO_DES_CBC:
	    case CRYPTO_3DES_CBC:
	    case CRYPTO_BLF_CBC:
	    case CRYPTO_CAST_CBC:
	    case CRYPTO_SKIPJACK_CBC:
		txf = swd->sw_exf;

		if (swd->sw_kschedule)
		  txf->zerokey(&(swd->sw_kschedule));

		if (swd->sw_iv)
		  FREE(swd->sw_iv, M_XDATA);
		break;

	    case CRYPTO_MD5_HMAC96:
	    case CRYPTO_SHA1_HMAC96:
	    case CRYPTO_RIPEMD160_HMAC96:
	    case CRYPTO_MD5_KPDK:
	    case CRYPTO_SHA1_KPDK:
		axf = swd->sw_axf;

		if (swd->sw_ictx)
		{
		    bzero(swd->sw_ictx, axf->ctxsize);
		    FREE(swd->sw_ictx, M_XDATA);
		}

		if (swd->sw_octx)
		{
		    bzero(swd->sw_octx, axf->ctxsize);
		    FREE(swd->sw_octx, M_XDATA);
		}
		break;
	}

	FREE(swd, M_XDATA);
    }

    return 0;
}

/*
 * Process a software request.
 */
int
swcr_process(struct cryptop *crp)
{
    struct cryptodesc *crd;
    struct swcr_data *sw;
    u_int32_t lid;
    u_int64_t nid;
    int type;

    /* Some simple sanity checks */
    if ((crp == NULL) || (crp->crp_callback == NULL))
      return EINVAL;

    if ((crp->crp_desc == NULL) || (crp->crp_buf == NULL))
    {
	crp->crp_etype = EINVAL;
	goto done;
    }

    lid = crp->crp_sid & 0xffffffff;
    if ((lid >= swcr_sesnum) || (lid == 0) || (swcr_sessions[lid] == NULL))
    {
	crp->crp_etype = ENOENT;
	goto done;
    }

    if (crp->crp_flags & CRYPTO_F_IMBUF)
      type = CRYPTO_BUF_MBUF;
    else
      type = CRYPTO_BUF_CONTIG;

    /* Go through crypto descriptors, processing as we go */
    for (crd = crp->crp_desc; crd; crd = crd->crd_next)
    {
	/*
	 * Find the crypto context.
	 *
	 * XXX Note that the logic here prevents us from having
	 * XXX the same algorithm multiple times in a session
	 * XXX (or rather, we can but it won't give us the right
	 * XXX results). To do that, we'd need some way of differentiating
	 * XXX between the various instances of an algorithm (so we can
	 * XXX locate the correct crypto context).
	 */
	for (sw = swcr_sessions[lid];
	     sw && sw->sw_alg != crd->crd_alg;
	     sw = sw->sw_next)
	  ;

	/* No such context ? */
	if (sw == NULL)
	{
	    crp->crp_etype = EINVAL;
	    goto done;
	}

	switch (sw->sw_alg)
	{
	    case CRYPTO_DES_CBC:
	    case CRYPTO_3DES_CBC:
	    case CRYPTO_BLF_CBC:
	    case CRYPTO_CAST_CBC:
	    case CRYPTO_SKIPJACK_CBC:
		if ((crp->crp_etype = swcr_encdec(crd, sw, crp->crp_buf,
						  type)) != 0)
		  goto done;
		break;

	    case CRYPTO_MD5_HMAC96:
	    case CRYPTO_SHA1_HMAC96:
	    case CRYPTO_RIPEMD160_HMAC96:
	    case CRYPTO_MD5_KPDK:
	    case CRYPTO_SHA1_KPDK:
		if ((crp->crp_etype = swcr_authcompute(crd, sw, crp->crp_buf,
						       type)) != 0)
		  goto done;
		break;

	    default:  /* Unknown/unsupported algorithm */
		crp->crp_etype = EINVAL;
		goto done;
	}
    }

 done:
    if (crp->crp_etype == ENOENT)
    {
	crypto_freesession(crp->crp_sid); /* Just in case */

	/* Migrate session */
	for (crd = crp->crp_desc; crd->crd_next; crd = crd->crd_next)
	  crd->CRD_INI.cri_next = &(crd->crd_next->CRD_INI);
	if (crypto_newsession(&nid, &(crp->crp_desc->CRD_INI)) == 0)
	  crp->crp_sid = nid;
    }

    return crp->crp_callback(crp);
}

/*
 * Initialize the driver, called from the kernel main().
 */
void
swcr_init(void)
{
    swcr_id = crypto_get_driverid();
    if (swcr_id >= 0)
    {
	crypto_register(swcr_id, CRYPTO_DES_CBC, swcr_newsession,
                        swcr_freesession, swcr_process);
        crypto_register(swcr_id, CRYPTO_3DES_CBC, NULL, NULL, NULL);
        crypto_register(swcr_id, CRYPTO_BLF_CBC, NULL, NULL, NULL);
        crypto_register(swcr_id, CRYPTO_CAST_CBC, NULL, NULL, NULL);
        crypto_register(swcr_id, CRYPTO_SKIPJACK_CBC, NULL, NULL, NULL);
        crypto_register(swcr_id, CRYPTO_MD5_HMAC96, NULL, NULL, NULL);
        crypto_register(swcr_id, CRYPTO_SHA1_HMAC96, NULL, NULL, NULL);
        crypto_register(swcr_id, CRYPTO_RIPEMD160_HMAC96, NULL, NULL, NULL);
        crypto_register(swcr_id, CRYPTO_MD5_KPDK, NULL, NULL, NULL);
        crypto_register(swcr_id, CRYPTO_SHA1_KPDK, NULL, NULL, NULL);
	return;
    }

    /* This should never happen */
    panic("Software crypto device cannot initialize!");
@

