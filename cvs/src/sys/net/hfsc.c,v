head	1.43;
access;
symbols
	OPENBSD_6_1:1.36.0.4
	OPENBSD_6_1_BASE:1.36
	OPENBSD_6_0:1.32.0.6
	OPENBSD_6_0_BASE:1.32
	OPENBSD_5_9:1.32.0.2
	OPENBSD_5_9_BASE:1.32
	OPENBSD_5_8:1.21.0.4
	OPENBSD_5_8_BASE:1.21
	OPENBSD_5_7:1.16.0.2
	OPENBSD_5_7_BASE:1.16
	OPENBSD_5_6:1.11.0.4
	OPENBSD_5_6_BASE:1.11
	OPENBSD_5_5:1.8.0.4
	OPENBSD_5_5_BASE:1.8;
locks; strict;
comment	@ * @;


1.43
date	2017.07.19.13.41.20;	author mikeb;	state Exp;
branches;
next	1.42;
commitid	KkgsGqtXtcjADWs5;

1.42
date	2017.07.19.12.54.09;	author mikeb;	state Exp;
branches;
next	1.41;
commitid	CZn1MxxlivKEreGD;

1.41
date	2017.06.28.18.24.02;	author mikeb;	state Exp;
branches;
next	1.40;
commitid	FqbGHiUNgQGtga0J;

1.40
date	2017.06.12.23.20.10;	author dlg;	state Exp;
branches;
next	1.39;
commitid	TJE9zfnVfUwtHsi8;

1.39
date	2017.05.08.11.30.53;	author mikeb;	state Exp;
branches;
next	1.38;
commitid	mlqf74D2T9SMJCfZ;

1.38
date	2017.05.02.12.27.37;	author mikeb;	state Exp;
branches;
next	1.37;
commitid	rOAHa272RNs62JfW;

1.37
date	2017.04.26.15.50.59;	author mikeb;	state Exp;
branches;
next	1.36;
commitid	8pGFYJ6mUEgndXbS;

1.36
date	2017.03.07.01.29.53;	author dlg;	state Exp;
branches;
next	1.35;
commitid	So70CRieBADFeXD4;

1.35
date	2017.01.24.03.57.35;	author dlg;	state Exp;
branches;
next	1.34;
commitid	PERtGPXCvlLRRBr8;

1.34
date	2017.01.22.04.48.23;	author dlg;	state Exp;
branches;
next	1.33;
commitid	mGzVB6IJ6qMfPA8g;

1.33
date	2016.09.15.02.00.18;	author dlg;	state Exp;
branches;
next	1.32;
commitid	RlO92XR575sygHqm;

1.32
date	2015.11.21.01.08.49;	author dlg;	state Exp;
branches;
next	1.31;
commitid	NZSRW1Bv4cWMAzsm;

1.31
date	2015.11.20.03.35.23;	author dlg;	state Exp;
branches;
next	1.30;
commitid	eYnPulzvLjDImPCa;

1.30
date	2015.11.09.01.06.31;	author dlg;	state Exp;
branches;
next	1.29;
commitid	mmu5QhvHibHMvv4f;

1.29
date	2015.10.23.02.29.24;	author dlg;	state Exp;
branches;
next	1.28;
commitid	uAl9JXVWjHjATaO4;

1.28
date	2015.10.23.02.08.37;	author dlg;	state Exp;
branches;
next	1.27;
commitid	VEfcfFsJB7lP0Pow;

1.27
date	2015.10.23.01.53.02;	author dlg;	state Exp;
branches;
next	1.26;
commitid	HLxJV9Wol85NIYCb;

1.26
date	2015.10.23.01.32.10;	author dlg;	state Exp;
branches;
next	1.25;
commitid	mlr0PM4t3qpD0prd;

1.25
date	2015.10.23.01.02.46;	author dlg;	state Exp;
branches;
next	1.24;
commitid	mriVVmm2c7YvvUsv;

1.24
date	2015.09.30.22.57.47;	author dlg;	state Exp;
branches;
next	1.23;
commitid	gZ2Xfzd4tyAFpaFm;

1.23
date	2015.09.30.11.36.20;	author dlg;	state Exp;
branches;
next	1.22;
commitid	shgH1LXPCebxWgto;

1.22
date	2015.09.27.05.23.50;	author dlg;	state Exp;
branches;
next	1.21;
commitid	EOMGnvAfDUkwTLM2;

1.21
date	2015.04.18.11.12.33;	author dlg;	state Exp;
branches;
next	1.20;
commitid	Shwl2woW8WFZbW45;

1.20
date	2015.04.12.14.09.40;	author dlg;	state Exp;
branches;
next	1.19;
commitid	wEd8A80zi4gbVh6k;

1.19
date	2015.04.12.12.22.26;	author dlg;	state Exp;
branches;
next	1.18;
commitid	UXOnqZxbu92eKHyQ;

1.18
date	2015.04.12.09.58.46;	author dlg;	state Exp;
branches;
next	1.17;
commitid	FhB7HPYrCRsWAEGg;

1.17
date	2015.04.11.13.00.12;	author dlg;	state Exp;
branches;
next	1.16;
commitid	uT17hi9tQuyDqgaw;

1.16
date	2015.03.03.11.14.00;	author henning;	state Exp;
branches;
next	1.15;
commitid	KbuSg22LDnxbLBoJ;

1.15
date	2015.02.08.03.16.16;	author henning;	state Exp;
branches;
next	1.14;
commitid	m2cqTb7pkhjO04nx;

1.14
date	2015.02.06.06.37.24;	author henning;	state Exp;
branches;
next	1.13;
commitid	yLDqsSedEMEWLWvy;

1.13
date	2014.12.09.07.05.06;	author doug;	state Exp;
branches;
next	1.12;
commitid	zM5ckwX4kwwmipG0;

1.12
date	2014.12.05.15.50.04;	author mpi;	state Exp;
branches;
next	1.11;
commitid	t9FBKDfc4VDxpEy2;

1.11
date	2014.07.12.18.44.22;	author tedu;	state Exp;
branches;
next	1.10;
commitid	B4dZSbxas1X1IpXI;

1.10
date	2014.06.30.12.47.23;	author pelikan;	state Exp;
branches;
next	1.9;
commitid	VTk9dgSoWyQcfMt5;

1.9
date	2014.04.19.15.58.12;	author henning;	state Exp;
branches;
next	1.8;

1.8
date	2014.01.27.15.41.06;	author pelikan;	state Exp;
branches;
next	1.7;

1.7
date	2014.01.03.19.58.54;	author pelikan;	state Exp;
branches;
next	1.6;

1.6
date	2014.01.03.12.48.58;	author pelikan;	state Exp;
branches;
next	1.5;

1.5
date	2014.01.01.17.46.43;	author pelikan;	state Exp;
branches;
next	1.4;

1.4
date	2013.11.01.23.00.02;	author pelikan;	state Exp;
branches;
next	1.3;

1.3
date	2013.10.31.13.19.17;	author pelikan;	state Exp;
branches;
next	1.2;

1.2
date	2013.10.31.08.52.44;	author pelikan;	state Exp;
branches;
next	1.1;

1.1
date	2013.10.12.11.39.17;	author henning;	state Exp;
branches;
next	;


desc
@@


1.43
log
@Fix the test condition weakened a recent change
@
text
@/*	$OpenBSD: hfsc.c,v 1.42 2017/07/19 12:54:09 mikeb Exp $	*/

/*
 * Copyright (c) 2012-2013 Henning Brauer <henning@@openbsd.org>
 * Copyright (c) 1997-1999 Carnegie Mellon University. All Rights Reserved.
 *
 * Permission to use, copy, modify, and distribute this software and
 * its documentation is hereby granted (including for commercial or
 * for-profit use), provided that both the copyright notice and this
 * permission notice appear in all copies of the software, derivative
 * works, or modified versions, and any portions thereof.
 *
 * THIS SOFTWARE IS EXPERIMENTAL AND IS KNOWN TO HAVE BUGS, SOME OF
 * WHICH MAY HAVE SERIOUS CONSEQUENCES.  CARNEGIE MELLON PROVIDES THIS
 * SOFTWARE IN ITS ``AS IS'' CONDITION, AND ANY EXPRESS OR IMPLIED
 * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED.  IN NO EVENT SHALL CARNEGIE MELLON UNIVERSITY BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT
 * OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
 * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
 * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
 * USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
 * DAMAGE.
 *
 * Carnegie Mellon encourages (but does not require) users of this
 * software to return any improvements or extensions that they make,
 * and to grant Carnegie Mellon the rights to redistribute these
 * changes without encumbrance.
 */
/*
 * H-FSC is described in Proceedings of SIGCOMM'97,
 * "A Hierarchical Fair Service Curve Algorithm for Link-Sharing,
 * Real-Time and Priority Service"
 * by Ion Stoica, Hui Zhang, and T. S. Eugene Ng.
 *
 * Oleg Cherevko <olwi@@aq.ml.com.ua> added the upperlimit for link-sharing.
 * when a class has an upperlimit, the fit-time is computed from the
 * upperlimit service curve.  the link-sharing scheduler does not schedule
 * a class whose fit-time exceeds the current time.
 */

#include <sys/param.h>
#include <sys/malloc.h>
#include <sys/pool.h>
#include <sys/mbuf.h>
#include <sys/socket.h>
#include <sys/systm.h>
#include <sys/errno.h>
#include <sys/queue.h>
#include <sys/kernel.h>
#include <sys/timeout.h>

#include <net/if.h>
#include <net/if_var.h>
#include <netinet/in.h>

#include <net/pfvar.h>
#include <net/hfsc.h>

/*
 * kernel internal service curve representation
 *	coordinates are given by 64 bit unsigned integers.
 *	x-axis: unit is clock count.  for the intel x86 architecture,
 *		the raw Pentium TSC (Timestamp Counter) value is used.
 *		virtual time is also calculated in this time scale.
 *	y-axis: unit is byte.
 *
 *	the service curve parameters are converted to the internal
 *	representation.
 *	the slope values are scaled to avoid overflow.
 *	the inverse slope values as well as the y-projection of the 1st
 *	segment are kept in order to to avoid 64-bit divide operations
 *	that are expensive on 32-bit architectures.
 *
 *  note: Intel Pentium TSC never wraps around in several thousands of years.
 *	x-axis doesn't wrap around for 1089 years with 1GHz clock.
 *      y-axis doesn't wrap around for 4358 years with 1Gbps bandwidth.
 */

/* kernel internal representation of a service curve */
struct hfsc_internal_sc {
	u_int64_t	sm1;	/* scaled slope of the 1st segment */
	u_int64_t	ism1;	/* scaled inverse-slope of the 1st segment */
	u_int64_t	dx;	/* the x-projection of the 1st segment */
	u_int64_t	dy;	/* the y-projection of the 1st segment */
	u_int64_t	sm2;	/* scaled slope of the 2nd segment */
	u_int64_t	ism2;	/* scaled inverse-slope of the 2nd segment */
};

/* runtime service curve */
struct hfsc_runtime_sc {
	u_int64_t	x;	/* current starting position on x-axis */
	u_int64_t	y;	/* current starting position on x-axis */
	u_int64_t	sm1;	/* scaled slope of the 1st segment */
	u_int64_t	ism1;	/* scaled inverse-slope of the 1st segment */
	u_int64_t	dx;	/* the x-projection of the 1st segment */
	u_int64_t	dy;	/* the y-projection of the 1st segment */
	u_int64_t	sm2;	/* scaled slope of the 2nd segment */
	u_int64_t	ism2;	/* scaled inverse-slope of the 2nd segment */
};

struct hfsc_classq {
	struct mbuf_list q;	 /* Queue of packets */
	int		 qlimit; /* Queue limit */
};

/* for TAILQ based ellist and actlist implementation */
struct hfsc_class;
TAILQ_HEAD(hfsc_eligible, hfsc_class);
TAILQ_HEAD(hfsc_active, hfsc_class);
#define	hfsc_actlist_last(s)		TAILQ_LAST(s, hfsc_active)

struct hfsc_class {
	u_int		cl_id;		/* class id (just for debug) */
	u_int32_t	cl_handle;	/* class handle */
	int		cl_flags;	/* misc flags */

	struct hfsc_class *cl_parent;	/* parent class */
	struct hfsc_class *cl_siblings;	/* sibling classes */
	struct hfsc_class *cl_children;	/* child classes */

	struct hfsc_classq cl_q;	/* class queue structure */

	const struct pfq_ops *cl_qops;	/* queue manager */
	void		*cl_qdata;	/* queue manager data */
	void		*cl_cookie;	/* queue manager cookie */

	u_int64_t	cl_total;	/* total work in bytes */
	u_int64_t	cl_cumul;	/* cumulative work in bytes
					   done by real-time criteria */
	u_int64_t	cl_d;		/* deadline */
	u_int64_t	cl_e;		/* eligible time */
	u_int64_t	cl_vt;		/* virtual time */
	u_int64_t	cl_f;		/* time when this class will fit for
					   link-sharing, max(myf, cfmin) */
	u_int64_t	cl_myf;		/* my fit-time (as calculated from this
					   class's own upperlimit curve) */
	u_int64_t	cl_myfadj;	/* my fit-time adjustment
					   (to cancel history dependence) */
	u_int64_t	cl_cfmin;	/* earliest children's fit-time (used
					   with cl_myf to obtain cl_f) */
	u_int64_t	cl_cvtmin;	/* minimal virtual time among the
					   children fit for link-sharing
					   (monotonic within a period) */
	u_int64_t	cl_vtadj;	/* intra-period cumulative vt
					   adjustment */
	u_int64_t	cl_vtoff;	/* inter-period cumulative vt offset */
	u_int64_t	cl_cvtmax;	/* max child's vt in the last period */

	u_int64_t	cl_initvt;	/* init virtual time (for debugging) */

	struct hfsc_internal_sc *cl_rsc; /* internal real-time service curve */
	struct hfsc_internal_sc *cl_fsc; /* internal fair service curve */
	struct hfsc_internal_sc *cl_usc; /* internal upperlimit service curve */
	struct hfsc_runtime_sc   cl_deadline; /* deadline curve */
	struct hfsc_runtime_sc   cl_eligible; /* eligible curve */
	struct hfsc_runtime_sc   cl_virtual;  /* virtual curve */
	struct hfsc_runtime_sc   cl_ulimit;   /* upperlimit curve */

	u_int		cl_vtperiod;	/* vt period sequence no */
	u_int		cl_parentperiod;  /* parent's vt period seqno */
	int		cl_nactive;	/* number of active children */
	struct hfsc_active	cl_actc; /* active children list */

	TAILQ_ENTRY(hfsc_class) cl_actlist; /* active children list entry */
	TAILQ_ENTRY(hfsc_class) cl_ellist; /* eligible list entry */

	struct {
		struct hfsc_pktcntr xmit_cnt;
		struct hfsc_pktcntr drop_cnt;
		u_int period;
	} cl_stats;
};

/*
 * hfsc interface state
 */
struct hfsc_if {
	struct hfsc_if		*hif_next;	/* interface state list */
	struct hfsc_class	*hif_rootclass;		/* root class */
	struct hfsc_class	*hif_defaultclass;	/* default class */
	struct hfsc_class	**hif_class_tbl;

	u_int64_t		hif_microtime;	/* time at deq_begin */

	u_int	hif_allocated;			/* # of slots in hif_class_tbl */
	u_int	hif_classes;			/* # of classes in the tree */
	u_int	hif_classid;			/* class id sequence number */

	struct hfsc_eligible hif_eligible;	/* eligible list */
	struct timeout hif_defer;	/* for queues that weren't ready */
};

/*
 * function prototypes
 */
struct hfsc_class	*hfsc_class_create(struct hfsc_if *,
			    struct hfsc_sc *, struct hfsc_sc *,
			    struct hfsc_sc *, struct hfsc_class *, int,
			    int, int);
int			 hfsc_class_destroy(struct hfsc_if *,
			    struct hfsc_class *);
struct hfsc_class	*hfsc_nextclass(struct hfsc_class *);

void		 hfsc_cl_purge(struct hfsc_if *, struct hfsc_class *,
		     struct mbuf_list *);

void		 hfsc_deferred(void *);
void		 hfsc_update_cfmin(struct hfsc_class *);
void		 hfsc_set_active(struct hfsc_if *, struct hfsc_class *, int);
void		 hfsc_set_passive(struct hfsc_if *, struct hfsc_class *);
void		 hfsc_init_ed(struct hfsc_if *, struct hfsc_class *, int);
void		 hfsc_update_ed(struct hfsc_if *, struct hfsc_class *, int);
void		 hfsc_update_d(struct hfsc_class *, int);
void		 hfsc_init_vf(struct hfsc_class *, int);
void		 hfsc_update_vf(struct hfsc_class *, int, u_int64_t);
void		 hfsc_ellist_insert(struct hfsc_if *, struct hfsc_class *);
void		 hfsc_ellist_remove(struct hfsc_if *, struct hfsc_class *);
void		 hfsc_ellist_update(struct hfsc_if *, struct hfsc_class *);
struct hfsc_class	*hfsc_ellist_get_mindl(struct hfsc_if *, u_int64_t);
void		 hfsc_actlist_insert(struct hfsc_class *);
void		 hfsc_actlist_remove(struct hfsc_class *);
void		 hfsc_actlist_update(struct hfsc_class *);

struct hfsc_class	*hfsc_actlist_firstfit(struct hfsc_class *,
				    u_int64_t);

static __inline u_int64_t	seg_x2y(u_int64_t, u_int64_t);
static __inline u_int64_t	seg_y2x(u_int64_t, u_int64_t);
static __inline u_int64_t	m2sm(u_int);
static __inline u_int64_t	m2ism(u_int);
static __inline u_int64_t	d2dx(u_int);
static __inline u_int		sm2m(u_int64_t);
static __inline u_int		dx2d(u_int64_t);

void		hfsc_sc2isc(struct hfsc_sc *, struct hfsc_internal_sc *);
void		hfsc_rtsc_init(struct hfsc_runtime_sc *,
		    struct hfsc_internal_sc *, u_int64_t, u_int64_t);
u_int64_t	hfsc_rtsc_y2x(struct hfsc_runtime_sc *, u_int64_t);
u_int64_t	hfsc_rtsc_x2y(struct hfsc_runtime_sc *, u_int64_t);
void		hfsc_rtsc_min(struct hfsc_runtime_sc *,
		    struct hfsc_internal_sc *, u_int64_t, u_int64_t);

void		hfsc_getclstats(struct hfsc_class_stats *, struct hfsc_class *);
struct hfsc_class	*hfsc_clh2cph(struct hfsc_if *, u_int32_t);

#define	HFSC_CLK_SHIFT		8
#define	HFSC_FREQ		(1000000 << HFSC_CLK_SHIFT)
#define	HFSC_CLK_PER_TICK	(HFSC_FREQ / hz)
#define	HFSC_HT_INFINITY	0xffffffffffffffffLL /* infinite time value */

struct pool	hfsc_class_pl, hfsc_internal_sc_pl;

/*
 * ifqueue glue.
 */

unsigned int	 hfsc_idx(unsigned int, const struct mbuf *);
struct mbuf	*hfsc_enq(struct ifqueue *, struct mbuf *);
struct mbuf	*hfsc_deq_begin(struct ifqueue *, void **);
void		 hfsc_deq_commit(struct ifqueue *, struct mbuf *, void *);
void		 hfsc_purge(struct ifqueue *, struct mbuf_list *);
void		*hfsc_alloc(unsigned int, void *);
void		 hfsc_free(unsigned int, void *);

const struct ifq_ops hfsc_ops = {
	hfsc_idx,
	hfsc_enq,
	hfsc_deq_begin,
	hfsc_deq_commit,
	hfsc_purge,
	hfsc_alloc,
	hfsc_free,
};

const struct ifq_ops * const ifq_hfsc_ops = &hfsc_ops;

/*
 * pf queue glue.
 */

void		*hfsc_pf_alloc(struct ifnet *);
int		 hfsc_pf_addqueue(void *, struct pf_queuespec *);
void		 hfsc_pf_free(void *);
int		 hfsc_pf_qstats(struct pf_queuespec *, void *, int *);
unsigned int	 hfsc_pf_qlength(void *);
struct mbuf *	 hfsc_pf_enqueue(void *, struct mbuf *);
struct mbuf *	 hfsc_pf_deq_begin(void *, void **, struct mbuf_list *);
void		 hfsc_pf_deq_commit(void *, struct mbuf *, void *);
void		 hfsc_pf_purge(void *, struct mbuf_list *);

const struct pfq_ops hfsc_pf_ops = {
	hfsc_pf_alloc,
	hfsc_pf_addqueue,
	hfsc_pf_free,
	hfsc_pf_qstats,
	hfsc_pf_qlength,
	hfsc_pf_enqueue,
	hfsc_pf_deq_begin,
	hfsc_pf_deq_commit,
	hfsc_pf_purge
};

const struct pfq_ops * const pfq_hfsc_ops = &hfsc_pf_ops;

/*
 * shortcuts for repeated use
 */
static inline unsigned int
hfsc_class_qlength(struct hfsc_class *cl)
{
	/* Only leaf classes have a queue */
	if (cl->cl_qops != NULL)
		return cl->cl_qops->pfq_qlength(cl->cl_qdata);
	return 0;
}

static inline struct mbuf *
hfsc_class_enqueue(struct hfsc_class *cl, struct mbuf *m)
{
	return cl->cl_qops->pfq_enqueue(cl->cl_qdata, m);
}

static inline struct mbuf *
hfsc_class_deq_begin(struct hfsc_class *cl, struct mbuf_list *ml)
{
	return cl->cl_qops->pfq_deq_begin(cl->cl_qdata, &cl->cl_cookie, ml);
}

static inline void
hfsc_class_deq_commit(struct hfsc_class *cl, struct mbuf *m)
{
	return cl->cl_qops->pfq_deq_commit(cl->cl_qdata, m, cl->cl_cookie);
}

static inline void
hfsc_class_purge(struct hfsc_class *cl, struct mbuf_list *ml)
{
	/* Only leaf classes have a queue */
	if (cl->cl_qops != NULL)
		return cl->cl_qops->pfq_purge(cl->cl_qdata, ml);
}

u_int64_t
hfsc_microuptime(void)
{
	struct timeval tv;

	microuptime(&tv);
	return (((u_int64_t)(tv.tv_sec) * 1000000 + tv.tv_usec) <<
	    HFSC_CLK_SHIFT);
}

static inline u_int
hfsc_more_slots(u_int current)
{
	u_int want = current * 2;

	return (want > HFSC_MAX_CLASSES ? HFSC_MAX_CLASSES : want);
}

static void
hfsc_grow_class_tbl(struct hfsc_if *hif, u_int howmany)
{
	struct hfsc_class **newtbl, **old;
	size_t oldlen = sizeof(void *) * hif->hif_allocated;

	newtbl = mallocarray(howmany, sizeof(void *), M_DEVBUF,
	    M_WAITOK | M_ZERO);
	old = hif->hif_class_tbl;

	memcpy(newtbl, old, oldlen);
	hif->hif_class_tbl = newtbl;
	hif->hif_allocated = howmany;

	free(old, M_DEVBUF, oldlen);
}

void
hfsc_initialize(void)
{
	pool_init(&hfsc_class_pl, sizeof(struct hfsc_class), 0,
	    IPL_NONE, PR_WAITOK, "hfscclass", NULL);
	pool_init(&hfsc_internal_sc_pl, sizeof(struct hfsc_internal_sc), 0,
	    IPL_NONE, PR_WAITOK, "hfscintsc", NULL);
}

void *
hfsc_pf_alloc(struct ifnet *ifp)
{
	struct hfsc_if *hif;

	KASSERT(ifp != NULL);

	hif = malloc(sizeof(*hif), M_DEVBUF, M_WAITOK | M_ZERO);
	TAILQ_INIT(&hif->hif_eligible);
	hif->hif_class_tbl = mallocarray(HFSC_DEFAULT_CLASSES, sizeof(void *),
	    M_DEVBUF, M_WAITOK | M_ZERO);
	hif->hif_allocated = HFSC_DEFAULT_CLASSES;

	timeout_set(&hif->hif_defer, hfsc_deferred, ifp);

	return (hif);
}

int
hfsc_pf_addqueue(void *arg, struct pf_queuespec *q)
{
	struct hfsc_if *hif = arg;
	struct hfsc_class *cl, *parent;
	struct hfsc_sc rtsc, lssc, ulsc;
	int error = 0;

	KASSERT(hif != NULL);

	if (q->parent_qid == 0 && hif->hif_rootclass == NULL) {
		parent = hfsc_class_create(hif, NULL, NULL, NULL, NULL,
		    0, 0, HFSC_ROOT_CLASS | q->qid);
		if (parent == NULL)
			return (EINVAL);
	} else if ((parent = hfsc_clh2cph(hif, q->parent_qid)) == NULL)
		return (EINVAL);

	if (q->qid == 0)
		return (EINVAL);

	if (hfsc_clh2cph(hif, q->qid) != NULL)
		return (EBUSY);

	rtsc.m1 = q->realtime.m1.absolute;
	rtsc.d  = q->realtime.d;
	rtsc.m2 = q->realtime.m2.absolute;
	lssc.m1 = q->linkshare.m1.absolute;
	lssc.d  = q->linkshare.d;
	lssc.m2 = q->linkshare.m2.absolute;
	ulsc.m1 = q->upperlimit.m1.absolute;
	ulsc.d  = q->upperlimit.d;
	ulsc.m2 = q->upperlimit.m2.absolute;

	/* Compatibility with older pfctl, return an EINVAL after 6.2 */
	if (rtsc.m1 == 0 && rtsc.m2 == 0 && lssc.m1 == 0 &&
	    lssc.m2 == 0 && ulsc.m1 == 0 && ulsc.m2 == 0 &&
	    q->parent_qid == 0 && strncmp(q->qname, "_root_", 6) == 0) {
		hfsc_class_destroy(hif, parent);
		parent = NULL;
	}

	cl = hfsc_class_create(hif, &rtsc, &lssc, &ulsc,
	    parent, q->qlimit, q->flags, q->qid);
	if (cl == NULL)
		return (ENOMEM);

	/* Compatibility with older pfctl, remove after 6.2 */
	if (parent == NULL)
		return (0);

	/* Attach a queue manager if specified */
	cl->cl_qops = pf_queue_manager(q);
	/* Realtime class cannot be used with an external queue manager */
	if (cl->cl_qops == NULL || cl->cl_rsc != NULL) {
		cl->cl_qops = pfq_hfsc_ops;
		cl->cl_qdata = &cl->cl_q;
	} else {
		cl->cl_qdata = cl->cl_qops->pfq_alloc(q->kif->pfik_ifp);
		if (cl->cl_qdata == NULL) {
			cl->cl_qops = NULL;
			hfsc_class_destroy(hif, cl);
			return (ENOMEM);
		}
		error = cl->cl_qops->pfq_addqueue(cl->cl_qdata, q);
		if (error) {
			cl->cl_qops->pfq_free(cl->cl_qdata);
			cl->cl_qops = NULL;
			hfsc_class_destroy(hif, cl);
			return (error);
		}
	}

	KASSERT(cl->cl_qops != NULL);
	KASSERT(cl->cl_qdata != NULL);

	return (0);
}

int
hfsc_pf_qstats(struct pf_queuespec *q, void *ubuf, int *nbytes)
{
	struct ifnet *ifp = q->kif->pfik_ifp;
	struct hfsc_if *hif;
	struct hfsc_class *cl;
	struct hfsc_class_stats stats;
	int error = 0;

	if (ifp == NULL)
		return (EBADF);

	if (*nbytes < sizeof(stats))
		return (EINVAL);

	hif = ifq_q_enter(&ifp->if_snd, ifq_hfsc_ops);
	if (hif == NULL)
		return (EBADF);

	if ((cl = hfsc_clh2cph(hif, q->qid)) == NULL) {
		ifq_q_leave(&ifp->if_snd, hif);
		return (EINVAL);
	}

	hfsc_getclstats(&stats, cl);
	ifq_q_leave(&ifp->if_snd, hif);

	if ((error = copyout((caddr_t)&stats, ubuf, sizeof(stats))) != 0)
		return (error);

	*nbytes = sizeof(stats);
	return (0);
}

void
hfsc_pf_free(void *arg)
{
	hfsc_free(0, arg);
}

unsigned int
hfsc_pf_qlength(void *arg)
{
	struct hfsc_classq *cq = arg;

	return ml_len(&cq->q);
}

struct mbuf *
hfsc_pf_enqueue(void *arg, struct mbuf *m)
{
	struct hfsc_classq *cq = arg;

	if (ml_len(&cq->q) >= cq->qlimit)
		return (m);

	ml_enqueue(&cq->q, m);
	m->m_pkthdr.pf.prio = IFQ_MAXPRIO;
	return (NULL);
}

struct mbuf *
hfsc_pf_deq_begin(void *arg, void **cookiep, struct mbuf_list *free_ml)
{
	struct hfsc_classq *cq = arg;

	return MBUF_LIST_FIRST(&cq->q);
}

void
hfsc_pf_deq_commit(void *arg, struct mbuf *m, void *cookie)
{
	struct hfsc_classq *cq = arg;

	ml_dequeue(&cq->q);
}

void
hfsc_pf_purge(void *arg, struct mbuf_list *ml)
{
	struct hfsc_classq *cq = arg;

	ml_enlist(ml, &cq->q);
}

unsigned int
hfsc_idx(unsigned int nqueues, const struct mbuf *m)
{
	/*
	 * hfsc can only function on a single ifq and the stack understands
	 * this. when the first ifq on an interface is switched to hfsc,
	 * this gets used to map all mbufs to the first and only ifq that
	 * is set up for hfsc.
	 */
	return (0);
}

void *
hfsc_alloc(unsigned int idx, void *q)
{
	struct hfsc_if *hif = q;
	KASSERT(idx == 0); /* when hfsc is enabled we only use the first ifq */
	KASSERT(hif != NULL);

	timeout_add(&hif->hif_defer, 1);
	return (hif);
}

void
hfsc_free(unsigned int idx, void *q)
{
	struct hfsc_if *hif = q;
	struct hfsc_class *cl;
	int i, restart;

	KERNEL_ASSERT_LOCKED();
	KASSERT(idx == 0); /* when hfsc is enabled we only use the first ifq */

	timeout_del(&hif->hif_defer);

	do {
		restart = 0;
		for (i = 0; i < hif->hif_allocated; i++) {
			cl = hif->hif_class_tbl[i];
			if (hfsc_class_destroy(hif, cl) == EBUSY)
				restart++;
		}
	} while (restart > 0);

	free(hif->hif_class_tbl, M_DEVBUF, hif->hif_allocated * sizeof(void *));
	free(hif, M_DEVBUF, sizeof(*hif));
}

void
hfsc_purge(struct ifqueue *ifq, struct mbuf_list *ml)
{
	struct hfsc_if		*hif = ifq->ifq_q;
	struct hfsc_class	*cl;

	for (cl = hif->hif_rootclass; cl != NULL; cl = hfsc_nextclass(cl))
		hfsc_cl_purge(hif, cl, ml);
}

struct hfsc_class *
hfsc_class_create(struct hfsc_if *hif, struct hfsc_sc *rsc,
    struct hfsc_sc *fsc, struct hfsc_sc *usc, struct hfsc_class *parent,
    int qlimit, int flags, int qid)
{
	struct hfsc_class *cl, *p;
	int i, s;

	if (qlimit == 0)
		qlimit = HFSC_DEFAULT_QLIMIT;

	if (hif->hif_classes >= hif->hif_allocated) {
		u_int newslots = hfsc_more_slots(hif->hif_allocated);

		if (newslots == hif->hif_allocated)
			return (NULL);
		hfsc_grow_class_tbl(hif, newslots);
	}

	cl = pool_get(&hfsc_class_pl, PR_WAITOK | PR_ZERO);
	TAILQ_INIT(&cl->cl_actc);

	ml_init(&cl->cl_q.q);
	cl->cl_q.qlimit = qlimit;
	cl->cl_flags = flags;

	if (rsc != NULL && (rsc->m1 != 0 || rsc->m2 != 0)) {
		cl->cl_rsc = pool_get(&hfsc_internal_sc_pl, PR_WAITOK);
		hfsc_sc2isc(rsc, cl->cl_rsc);
		hfsc_rtsc_init(&cl->cl_deadline, cl->cl_rsc, 0, 0);
		hfsc_rtsc_init(&cl->cl_eligible, cl->cl_rsc, 0, 0);
	}
	if (fsc != NULL && (fsc->m1 != 0 || fsc->m2 != 0)) {
		cl->cl_fsc = pool_get(&hfsc_internal_sc_pl, PR_WAITOK);
		hfsc_sc2isc(fsc, cl->cl_fsc);
		hfsc_rtsc_init(&cl->cl_virtual, cl->cl_fsc, 0, 0);
	}
	if (usc != NULL && (usc->m1 != 0 || usc->m2 != 0)) {
		cl->cl_usc = pool_get(&hfsc_internal_sc_pl, PR_WAITOK);
		hfsc_sc2isc(usc, cl->cl_usc);
		hfsc_rtsc_init(&cl->cl_ulimit, cl->cl_usc, 0, 0);
	}

	cl->cl_id = hif->hif_classid++;
	cl->cl_handle = qid;
	cl->cl_parent = parent;

	s = splnet();
	hif->hif_classes++;

	/*
	 * find a free slot in the class table.  if the slot matching
	 * the lower bits of qid is free, use this slot.  otherwise,
	 * use the first free slot.
	 */
	i = qid % hif->hif_allocated;
	if (hif->hif_class_tbl[i] == NULL)
		hif->hif_class_tbl[i] = cl;
	else {
		for (i = 0; i < hif->hif_allocated; i++)
			if (hif->hif_class_tbl[i] == NULL) {
				hif->hif_class_tbl[i] = cl;
				break;
			}
		if (i == hif->hif_allocated) {
			splx(s);
			goto err_ret;
		}
	}

	if (flags & HFSC_DEFAULTCLASS)
		hif->hif_defaultclass = cl;

	if (parent == NULL)
		hif->hif_rootclass = cl;
	else {
		/* add this class to the children list of the parent */
		if ((p = parent->cl_children) == NULL)
			parent->cl_children = cl;
		else {
			while (p->cl_siblings != NULL)
				p = p->cl_siblings;
			p->cl_siblings = cl;
		}
	}
	splx(s);

	return (cl);

err_ret:
	if (cl->cl_fsc != NULL)
		pool_put(&hfsc_internal_sc_pl, cl->cl_fsc);
	if (cl->cl_rsc != NULL)
		pool_put(&hfsc_internal_sc_pl, cl->cl_rsc);
	if (cl->cl_usc != NULL)
		pool_put(&hfsc_internal_sc_pl, cl->cl_usc);
	pool_put(&hfsc_class_pl, cl);
	return (NULL);
}

int
hfsc_class_destroy(struct hfsc_if *hif, struct hfsc_class *cl)
{
	int i, s;

	if (cl == NULL)
		return (0);

	if (cl->cl_children != NULL)
		return (EBUSY);

	s = splnet();
	KASSERT(hfsc_class_qlength(cl) == 0);

	if (cl->cl_parent != NULL) {
		struct hfsc_class *p = cl->cl_parent->cl_children;

		if (p == cl)
			cl->cl_parent->cl_children = cl->cl_siblings;
		else do {
			if (p->cl_siblings == cl) {
				p->cl_siblings = cl->cl_siblings;
				break;
			}
		} while ((p = p->cl_siblings) != NULL);
	}

	for (i = 0; i < hif->hif_allocated; i++)
		if (hif->hif_class_tbl[i] == cl) {
			hif->hif_class_tbl[i] = NULL;
			break;
		}

	hif->hif_classes--;
	splx(s);

	KASSERT(TAILQ_EMPTY(&cl->cl_actc));

	if (cl == hif->hif_rootclass)
		hif->hif_rootclass = NULL;
	if (cl == hif->hif_defaultclass)
		hif->hif_defaultclass = NULL;

	/* Free external queue manager resources */
	if (cl->cl_qops && cl->cl_qops != pfq_hfsc_ops)
		cl->cl_qops->pfq_free(cl->cl_qdata);

	if (cl->cl_usc != NULL)
		pool_put(&hfsc_internal_sc_pl, cl->cl_usc);
	if (cl->cl_fsc != NULL)
		pool_put(&hfsc_internal_sc_pl, cl->cl_fsc);
	if (cl->cl_rsc != NULL)
		pool_put(&hfsc_internal_sc_pl, cl->cl_rsc);
	pool_put(&hfsc_class_pl, cl);

	return (0);
}

/*
 * hfsc_nextclass returns the next class in the tree.
 *   usage:
 *	for (cl = hif->hif_rootclass; cl != NULL; cl = hfsc_nextclass(cl))
 *		do_something;
 */
struct hfsc_class *
hfsc_nextclass(struct hfsc_class *cl)
{
	if (cl->cl_children != NULL)
		cl = cl->cl_children;
	else if (cl->cl_siblings != NULL)
		cl = cl->cl_siblings;
	else {
		while ((cl = cl->cl_parent) != NULL)
			if (cl->cl_siblings) {
				cl = cl->cl_siblings;
				break;
			}
	}

	return (cl);
}

struct mbuf *
hfsc_enq(struct ifqueue *ifq, struct mbuf *m)
{
	struct hfsc_if *hif = ifq->ifq_q;
	struct hfsc_class *cl;
	struct mbuf *dm;

	if ((cl = hfsc_clh2cph(hif, m->m_pkthdr.pf.qid)) == NULL ||
	    cl->cl_children != NULL) {
		cl = hif->hif_defaultclass;
		if (cl == NULL)
			return (m);
	}

	dm = hfsc_class_enqueue(cl, m);

	/* successfully queued. */
	if (dm != m && hfsc_class_qlength(cl) == 1)
		hfsc_set_active(hif, cl, m->m_pkthdr.len);

	/* drop occurred. */
	if (dm != NULL)
		PKTCNTR_INC(&cl->cl_stats.drop_cnt, dm->m_pkthdr.len);

	return (dm);
}

struct mbuf *
hfsc_deq_begin(struct ifqueue *ifq, void **cookiep)
{
	struct mbuf_list free_ml = MBUF_LIST_INITIALIZER();
	struct hfsc_if *hif = ifq->ifq_q;
	struct hfsc_class *cl, *tcl;
	struct mbuf *m;
	u_int64_t cur_time;

	cur_time = hfsc_microuptime();

	/*
	 * if there are eligible classes, use real-time criteria.
	 * find the class with the minimum deadline among
	 * the eligible classes.
	 */
	cl = hfsc_ellist_get_mindl(hif, cur_time);
	if (cl == NULL) {
		/*
		 * use link-sharing criteria
		 * get the class with the minimum vt in the hierarchy
		 */
		cl = NULL;
		tcl = hif->hif_rootclass;

		while (tcl != NULL && tcl->cl_children != NULL) {
			tcl = hfsc_actlist_firstfit(tcl, cur_time);
			if (tcl == NULL)
				continue;

			/*
			 * update parent's cl_cvtmin.
			 * don't update if the new vt is smaller.
			 */
			if (tcl->cl_parent->cl_cvtmin < tcl->cl_vt)
				tcl->cl_parent->cl_cvtmin = tcl->cl_vt;

			cl = tcl;
		}
		/* XXX HRTIMER plan hfsc_deferred precisely here. */
		if (cl == NULL)
			return (NULL);
	}

	m = hfsc_class_deq_begin(cl, &free_ml);
	ifq_mfreeml(ifq, &free_ml);
	if (m == NULL) {
		/* the class becomes passive */
		hfsc_set_passive(hif, cl);
		return (NULL);
	}

	hif->hif_microtime = cur_time;
	*cookiep = cl;
	return (m);
}

void
hfsc_deq_commit(struct ifqueue *ifq, struct mbuf *m, void *cookie)
{
	struct mbuf_list free_ml = MBUF_LIST_INITIALIZER();
	struct hfsc_if *hif = ifq->ifq_q;
	struct hfsc_class *cl = cookie;
	struct mbuf *m0;
	int next_len, realtime = 0;
	u_int64_t cur_time = hif->hif_microtime;

	/* check if the class was scheduled by real-time criteria */
	if (cl->cl_rsc != NULL)
		realtime = (cl->cl_e <= cur_time);

	hfsc_class_deq_commit(cl, m);

	PKTCNTR_INC(&cl->cl_stats.xmit_cnt, m->m_pkthdr.len);

	hfsc_update_vf(cl, m->m_pkthdr.len, cur_time);
	if (realtime)
		cl->cl_cumul += m->m_pkthdr.len;

	if (hfsc_class_qlength(cl) > 0) {
		/*
		 * Realtime queue needs to look into the future and make
		 * calculations based on that. This is the reason it can't
		 * be used with an external queue manager.
		 */
		if (cl->cl_rsc != NULL) {
			/* update ed */
			m0 = hfsc_class_deq_begin(cl, &free_ml);
			ifq_mfreeml(ifq, &free_ml);
			next_len = m0->m_pkthdr.len;

			if (realtime)
				hfsc_update_ed(hif, cl, next_len);
			else
				hfsc_update_d(cl, next_len);
		}
	} else {
		/* the class becomes passive */
		hfsc_set_passive(hif, cl);
	}
}

void
hfsc_deferred(void *arg)
{
	struct ifnet *ifp = arg;
	struct ifqueue *ifq = &ifp->if_snd;
	struct hfsc_if *hif;

	KERNEL_ASSERT_LOCKED();
	KASSERT(HFSC_ENABLED(ifq));

	if (!ifq_empty(ifq))
		ifq_start(ifq);

	hif = ifq->ifq_q;

	/* XXX HRTIMER nearest virtual/fit time is likely less than 1/HZ. */
	timeout_add(&hif->hif_defer, 1);
}

void
hfsc_cl_purge(struct hfsc_if *hif, struct hfsc_class *cl, struct mbuf_list *ml)
{
	hfsc_class_purge(cl, ml);
	hfsc_update_vf(cl, 0, 0);	/* remove cl from the actlist */
	hfsc_set_passive(hif, cl);
}

void
hfsc_set_active(struct hfsc_if *hif, struct hfsc_class *cl, int len)
{
	if (cl->cl_rsc != NULL)
		hfsc_init_ed(hif, cl, len);
	if (cl->cl_fsc != NULL)
		hfsc_init_vf(cl, len);

	cl->cl_stats.period++;
}

void
hfsc_set_passive(struct hfsc_if *hif, struct hfsc_class *cl)
{
	if (cl->cl_rsc != NULL)
		hfsc_ellist_remove(hif, cl);

	/*
	 * actlist is handled in hfsc_update_vf() so that hfsc_update_vf(cl, 0,
	 * 0) needs to be called explicitly to remove a class from actlist
	 */
}

void
hfsc_init_ed(struct hfsc_if *hif, struct hfsc_class *cl, int next_len)
{
	u_int64_t cur_time;

	cur_time = hfsc_microuptime();

	/* update the deadline curve */
	hfsc_rtsc_min(&cl->cl_deadline, cl->cl_rsc, cur_time, cl->cl_cumul);

	/*
	 * update the eligible curve.
	 * for concave, it is equal to the deadline curve.
	 * for convex, it is a linear curve with slope m2.
	 */
	cl->cl_eligible = cl->cl_deadline;
	if (cl->cl_rsc->sm1 <= cl->cl_rsc->sm2) {
		cl->cl_eligible.dx = 0;
		cl->cl_eligible.dy = 0;
	}

	/* compute e and d */
	cl->cl_e = hfsc_rtsc_y2x(&cl->cl_eligible, cl->cl_cumul);
	cl->cl_d = hfsc_rtsc_y2x(&cl->cl_deadline, cl->cl_cumul + next_len);

	hfsc_ellist_insert(hif, cl);
}

void
hfsc_update_ed(struct hfsc_if *hif, struct hfsc_class *cl, int next_len)
{
	cl->cl_e = hfsc_rtsc_y2x(&cl->cl_eligible, cl->cl_cumul);
	cl->cl_d = hfsc_rtsc_y2x(&cl->cl_deadline, cl->cl_cumul + next_len);

	hfsc_ellist_update(hif, cl);
}

void
hfsc_update_d(struct hfsc_class *cl, int next_len)
{
	cl->cl_d = hfsc_rtsc_y2x(&cl->cl_deadline, cl->cl_cumul + next_len);
}

void
hfsc_init_vf(struct hfsc_class *cl, int len)
{
	struct hfsc_class *max_cl, *p;
	u_int64_t vt, f, cur_time;
	int go_active;

	cur_time = 0;
	go_active = 1;
	for ( ; cl->cl_parent != NULL; cl = cl->cl_parent) {
		if (go_active && cl->cl_nactive++ == 0)
			go_active = 1;
		else
			go_active = 0;

		if (go_active) {
			max_cl = TAILQ_LAST(&cl->cl_parent->cl_actc,
			    hfsc_active);
			if (max_cl != NULL) {
				/*
				 * set vt to the average of the min and max
				 * classes.  if the parent's period didn't
				 * change, don't decrease vt of the class.
				 */
				vt = max_cl->cl_vt;
				if (cl->cl_parent->cl_cvtmin != 0)
					vt = (cl->cl_parent->cl_cvtmin + vt)/2;

				if (cl->cl_parent->cl_vtperiod !=
				    cl->cl_parentperiod || vt > cl->cl_vt)
					cl->cl_vt = vt;
			} else {
				/*
				 * first child for a new parent backlog period.
				 * add parent's cvtmax to vtoff of children
				 * to make a new vt (vtoff + vt) larger than
				 * the vt in the last period for all children.
				 */
				vt = cl->cl_parent->cl_cvtmax;
				for (p = cl->cl_parent->cl_children; p != NULL;
				     p = p->cl_siblings)
					p->cl_vtoff += vt;
				cl->cl_vt = 0;
				cl->cl_parent->cl_cvtmax = 0;
				cl->cl_parent->cl_cvtmin = 0;
			}
			cl->cl_initvt = cl->cl_vt;

			/* update the virtual curve */
			vt = cl->cl_vt + cl->cl_vtoff;
			hfsc_rtsc_min(&cl->cl_virtual, cl->cl_fsc, vt,
			    cl->cl_total);
			if (cl->cl_virtual.x == vt) {
				cl->cl_virtual.x -= cl->cl_vtoff;
				cl->cl_vtoff = 0;
			}
			cl->cl_vtadj = 0;

			cl->cl_vtperiod++;  /* increment vt period */
			cl->cl_parentperiod = cl->cl_parent->cl_vtperiod;
			if (cl->cl_parent->cl_nactive == 0)
				cl->cl_parentperiod++;
			cl->cl_f = 0;

			hfsc_actlist_insert(cl);

			if (cl->cl_usc != NULL) {
				/* class has upper limit curve */
				if (cur_time == 0)
					cur_time = hfsc_microuptime();

				/* update the ulimit curve */
				hfsc_rtsc_min(&cl->cl_ulimit, cl->cl_usc, cur_time,
				    cl->cl_total);
				/* compute myf */
				cl->cl_myf = hfsc_rtsc_y2x(&cl->cl_ulimit,
				    cl->cl_total);
				cl->cl_myfadj = 0;
			}
		}

		if (cl->cl_myf > cl->cl_cfmin)
			f = cl->cl_myf;
		else
			f = cl->cl_cfmin;
		if (f != cl->cl_f) {
			cl->cl_f = f;
			hfsc_update_cfmin(cl->cl_parent);
		}
	}
}

void
hfsc_update_vf(struct hfsc_class *cl, int len, u_int64_t cur_time)
{
	u_int64_t f, myf_bound, delta;
	int go_passive = 0;

	if (hfsc_class_qlength(cl) == 0)
		go_passive = 1;

	for (; cl->cl_parent != NULL; cl = cl->cl_parent) {
		cl->cl_total += len;

		if (cl->cl_fsc == NULL || cl->cl_nactive == 0)
			continue;

		if (go_passive && --cl->cl_nactive == 0)
			go_passive = 1;
		else
			go_passive = 0;

		if (go_passive) {
			/* no more active child, going passive */

			/* update cvtmax of the parent class */
			if (cl->cl_vt > cl->cl_parent->cl_cvtmax)
				cl->cl_parent->cl_cvtmax = cl->cl_vt;

			/* remove this class from the vt list */
			hfsc_actlist_remove(cl);

			hfsc_update_cfmin(cl->cl_parent);

			continue;
		}

		/*
		 * update vt and f
		 */
		cl->cl_vt = hfsc_rtsc_y2x(&cl->cl_virtual, cl->cl_total)
		    - cl->cl_vtoff + cl->cl_vtadj;

		/*
		 * if vt of the class is smaller than cvtmin,
		 * the class was skipped in the past due to non-fit.
		 * if so, we need to adjust vtadj.
		 */
		if (cl->cl_vt < cl->cl_parent->cl_cvtmin) {
			cl->cl_vtadj += cl->cl_parent->cl_cvtmin - cl->cl_vt;
			cl->cl_vt = cl->cl_parent->cl_cvtmin;
		}

		/* update the vt list */
		hfsc_actlist_update(cl);

		if (cl->cl_usc != NULL) {
			cl->cl_myf = cl->cl_myfadj +
			    hfsc_rtsc_y2x(&cl->cl_ulimit, cl->cl_total);

			/*
			 * if myf lags behind by more than one clock tick
			 * from the current time, adjust myfadj to prevent
			 * a rate-limited class from going greedy.
			 * in a steady state under rate-limiting, myf
			 * fluctuates within one clock tick.
			 */
			myf_bound = cur_time - HFSC_CLK_PER_TICK;
			if (cl->cl_myf < myf_bound) {
				delta = cur_time - cl->cl_myf;
				cl->cl_myfadj += delta;
				cl->cl_myf += delta;
			}
		}

		/* cl_f is max(cl_myf, cl_cfmin) */
		if (cl->cl_myf > cl->cl_cfmin)
			f = cl->cl_myf;
		else
			f = cl->cl_cfmin;
		if (f != cl->cl_f) {
			cl->cl_f = f;
			hfsc_update_cfmin(cl->cl_parent);
		}
	}
}

void
hfsc_update_cfmin(struct hfsc_class *cl)
{
	struct hfsc_class *p;
	u_int64_t cfmin;

	if (TAILQ_EMPTY(&cl->cl_actc)) {
		cl->cl_cfmin = 0;
		return;
	}
	cfmin = HFSC_HT_INFINITY;
	TAILQ_FOREACH(p, &cl->cl_actc, cl_actlist) {
		if (p->cl_f == 0) {
			cl->cl_cfmin = 0;
			return;
		}
		if (p->cl_f < cfmin)
			cfmin = p->cl_f;
	}
	cl->cl_cfmin = cfmin;
}

/*
 * eligible list holds backlogged classes being sorted by their eligible times.
 * there is one eligible list per interface.
 */
void
hfsc_ellist_insert(struct hfsc_if *hif, struct hfsc_class *cl)
{
	struct hfsc_class *p;

	/* check the last entry first */
	if ((p = TAILQ_LAST(&hif->hif_eligible, hfsc_eligible)) == NULL ||
	    p->cl_e <= cl->cl_e) {
		TAILQ_INSERT_TAIL(&hif->hif_eligible, cl, cl_ellist);
		return;
	}

	TAILQ_FOREACH(p, &hif->hif_eligible, cl_ellist) {
		if (cl->cl_e < p->cl_e) {
			TAILQ_INSERT_BEFORE(p, cl, cl_ellist);
			return;
		}
	}
}

void
hfsc_ellist_remove(struct hfsc_if *hif, struct hfsc_class *cl)
{
	TAILQ_REMOVE(&hif->hif_eligible, cl, cl_ellist);
}

void
hfsc_ellist_update(struct hfsc_if *hif, struct hfsc_class *cl)
{
	struct hfsc_class *p, *last;

	/*
	 * the eligible time of a class increases monotonically.
	 * if the next entry has a larger eligible time, nothing to do.
	 */
	p = TAILQ_NEXT(cl, cl_ellist);
	if (p == NULL || cl->cl_e <= p->cl_e)
		return;

	/* check the last entry */
	last = TAILQ_LAST(&hif->hif_eligible, hfsc_eligible);
	if (last->cl_e <= cl->cl_e) {
		TAILQ_REMOVE(&hif->hif_eligible, cl, cl_ellist);
		TAILQ_INSERT_TAIL(&hif->hif_eligible, cl, cl_ellist);
		return;
	}

	/*
	 * the new position must be between the next entry
	 * and the last entry
	 */
	while ((p = TAILQ_NEXT(p, cl_ellist)) != NULL) {
		if (cl->cl_e < p->cl_e) {
			TAILQ_REMOVE(&hif->hif_eligible, cl, cl_ellist);
			TAILQ_INSERT_BEFORE(p, cl, cl_ellist);
			return;
		}
	}
}

/* find the class with the minimum deadline among the eligible classes */
struct hfsc_class *
hfsc_ellist_get_mindl(struct hfsc_if *hif, u_int64_t cur_time)
{
	struct hfsc_class *p, *cl = NULL;

	TAILQ_FOREACH(p, &hif->hif_eligible, cl_ellist) {
		if (p->cl_e > cur_time)
			break;
		if (cl == NULL || p->cl_d < cl->cl_d)
			cl = p;
	}
	return (cl);
}

/*
 * active children list holds backlogged child classes being sorted
 * by their virtual time.
 * each intermediate class has one active children list.
 */
void
hfsc_actlist_insert(struct hfsc_class *cl)
{
	struct hfsc_class *p;

	/* check the last entry first */
	if ((p = TAILQ_LAST(&cl->cl_parent->cl_actc, hfsc_active)) == NULL
	    || p->cl_vt <= cl->cl_vt) {
		TAILQ_INSERT_TAIL(&cl->cl_parent->cl_actc, cl, cl_actlist);
		return;
	}

	TAILQ_FOREACH(p, &cl->cl_parent->cl_actc, cl_actlist) {
		if (cl->cl_vt < p->cl_vt) {
			TAILQ_INSERT_BEFORE(p, cl, cl_actlist);
			return;
		}
	}
}

void
hfsc_actlist_remove(struct hfsc_class *cl)
{
	TAILQ_REMOVE(&cl->cl_parent->cl_actc, cl, cl_actlist);
}

void
hfsc_actlist_update(struct hfsc_class *cl)
{
	struct hfsc_class *p, *last;

	/*
	 * the virtual time of a class increases monotonically during its
	 * backlogged period.
	 * if the next entry has a larger virtual time, nothing to do.
	 */
	p = TAILQ_NEXT(cl, cl_actlist);
	if (p == NULL || cl->cl_vt < p->cl_vt)
		return;

	/* check the last entry */
	last = TAILQ_LAST(&cl->cl_parent->cl_actc, hfsc_active);
	if (last->cl_vt <= cl->cl_vt) {
		TAILQ_REMOVE(&cl->cl_parent->cl_actc, cl, cl_actlist);
		TAILQ_INSERT_TAIL(&cl->cl_parent->cl_actc, cl, cl_actlist);
		return;
	}

	/*
	 * the new position must be between the next entry
	 * and the last entry
	 */
	while ((p = TAILQ_NEXT(p, cl_actlist)) != NULL) {
		if (cl->cl_vt < p->cl_vt) {
			TAILQ_REMOVE(&cl->cl_parent->cl_actc, cl, cl_actlist);
			TAILQ_INSERT_BEFORE(p, cl, cl_actlist);
			return;
		}
	}
}

struct hfsc_class *
hfsc_actlist_firstfit(struct hfsc_class *cl, u_int64_t cur_time)
{
	struct hfsc_class *p;

	TAILQ_FOREACH(p, &cl->cl_actc, cl_actlist)
		if (p->cl_f <= cur_time)
			return (p);

	return (NULL);
}

/*
 * service curve support functions
 *
 *  external service curve parameters
 *	m: bits/sec
 *	d: msec
 *  internal service curve parameters
 *	sm: (bytes/tsc_interval) << SM_SHIFT
 *	ism: (tsc_count/byte) << ISM_SHIFT
 *	dx: tsc_count
 *
 * SM_SHIFT and ISM_SHIFT are scaled in order to keep effective digits.
 * we should be able to handle 100K-1Gbps linkspeed with 200Hz-1GHz CPU
 * speed.  SM_SHIFT and ISM_SHIFT are selected to have at least 3 effective
 * digits in decimal using the following table.
 *
 *  bits/sec    100Kbps     1Mbps     10Mbps     100Mbps    1Gbps
 *  ----------+-------------------------------------------------------
 *  bytes/nsec  12.5e-6    125e-6     1250e-6    12500e-6   125000e-6
 *  sm(500MHz)  25.0e-6    250e-6     2500e-6    25000e-6   250000e-6
 *  sm(200MHz)  62.5e-6    625e-6     6250e-6    62500e-6   625000e-6
 *
 *  nsec/byte   80000      8000       800        80         8
 *  ism(500MHz) 40000      4000       400        40         4
 *  ism(200MHz) 16000      1600       160        16         1.6
 */
#define	SM_SHIFT	24
#define	ISM_SHIFT	10

#define	SM_MASK		((1LL << SM_SHIFT) - 1)
#define	ISM_MASK	((1LL << ISM_SHIFT) - 1)

static __inline u_int64_t
seg_x2y(u_int64_t x, u_int64_t sm)
{
	u_int64_t y;

	/*
	 * compute
	 *	y = x * sm >> SM_SHIFT
	 * but divide it for the upper and lower bits to avoid overflow
	 */
	y = (x >> SM_SHIFT) * sm + (((x & SM_MASK) * sm) >> SM_SHIFT);
	return (y);
}

static __inline u_int64_t
seg_y2x(u_int64_t y, u_int64_t ism)
{
	u_int64_t x;

	if (y == 0)
		x = 0;
	else if (ism == HFSC_HT_INFINITY)
		x = HFSC_HT_INFINITY;
	else {
		x = (y >> ISM_SHIFT) * ism
		    + (((y & ISM_MASK) * ism) >> ISM_SHIFT);
	}
	return (x);
}

static __inline u_int64_t
m2sm(u_int m)
{
	u_int64_t sm;

	sm = ((u_int64_t)m << SM_SHIFT) / 8 / HFSC_FREQ;
	return (sm);
}

static __inline u_int64_t
m2ism(u_int m)
{
	u_int64_t ism;

	if (m == 0)
		ism = HFSC_HT_INFINITY;
	else
		ism = ((u_int64_t)HFSC_FREQ << ISM_SHIFT) * 8 / m;
	return (ism);
}

static __inline u_int64_t
d2dx(u_int d)
{
	u_int64_t dx;

	dx = ((u_int64_t)d * HFSC_FREQ) / 1000;
	return (dx);
}

static __inline u_int
sm2m(u_int64_t sm)
{
	u_int64_t m;

	m = (sm * 8 * HFSC_FREQ) >> SM_SHIFT;
	return ((u_int)m);
}

static __inline u_int
dx2d(u_int64_t dx)
{
	u_int64_t d;

	d = dx * 1000 / HFSC_FREQ;
	return ((u_int)d);
}

void
hfsc_sc2isc(struct hfsc_sc *sc, struct hfsc_internal_sc *isc)
{
	isc->sm1 = m2sm(sc->m1);
	isc->ism1 = m2ism(sc->m1);
	isc->dx = d2dx(sc->d);
	isc->dy = seg_x2y(isc->dx, isc->sm1);
	isc->sm2 = m2sm(sc->m2);
	isc->ism2 = m2ism(sc->m2);
}

/*
 * initialize the runtime service curve with the given internal
 * service curve starting at (x, y).
 */
void
hfsc_rtsc_init(struct hfsc_runtime_sc *rtsc, struct hfsc_internal_sc * isc,
    u_int64_t x, u_int64_t y)
{
	rtsc->x =	x;
	rtsc->y =	y;
	rtsc->sm1 =	isc->sm1;
	rtsc->ism1 =	isc->ism1;
	rtsc->dx =	isc->dx;
	rtsc->dy =	isc->dy;
	rtsc->sm2 =	isc->sm2;
	rtsc->ism2 =	isc->ism2;
}

/*
 * calculate the y-projection of the runtime service curve by the
 * given x-projection value
 */
u_int64_t
hfsc_rtsc_y2x(struct hfsc_runtime_sc *rtsc, u_int64_t y)
{
	u_int64_t x;

	if (y < rtsc->y)
		x = rtsc->x;
	else if (y <= rtsc->y + rtsc->dy) {
		/* x belongs to the 1st segment */
		if (rtsc->dy == 0)
			x = rtsc->x + rtsc->dx;
		else
			x = rtsc->x + seg_y2x(y - rtsc->y, rtsc->ism1);
	} else {
		/* x belongs to the 2nd segment */
		x = rtsc->x + rtsc->dx
		    + seg_y2x(y - rtsc->y - rtsc->dy, rtsc->ism2);
	}
	return (x);
}

u_int64_t
hfsc_rtsc_x2y(struct hfsc_runtime_sc *rtsc, u_int64_t x)
{
	u_int64_t y;

	if (x <= rtsc->x)
		y = rtsc->y;
	else if (x <= rtsc->x + rtsc->dx)
		/* y belongs to the 1st segment */
		y = rtsc->y + seg_x2y(x - rtsc->x, rtsc->sm1);
	else
		/* y belongs to the 2nd segment */
		y = rtsc->y + rtsc->dy
		    + seg_x2y(x - rtsc->x - rtsc->dx, rtsc->sm2);
	return (y);
}

/*
 * update the runtime service curve by taking the minimum of the current
 * runtime service curve and the service curve starting at (x, y).
 */
void
hfsc_rtsc_min(struct hfsc_runtime_sc *rtsc, struct hfsc_internal_sc *isc,
    u_int64_t x, u_int64_t y)
{
	u_int64_t y1, y2, dx, dy;

	if (isc->sm1 <= isc->sm2) {
		/* service curve is convex */
		y1 = hfsc_rtsc_x2y(rtsc, x);
		if (y1 < y)
			/* the current rtsc is smaller */
			return;
		rtsc->x = x;
		rtsc->y = y;
		return;
	}

	/*
	 * service curve is concave
	 * compute the two y values of the current rtsc
	 *	y1: at x
	 *	y2: at (x + dx)
	 */
	y1 = hfsc_rtsc_x2y(rtsc, x);
	if (y1 <= y) {
		/* rtsc is below isc, no change to rtsc */
		return;
	}

	y2 = hfsc_rtsc_x2y(rtsc, x + isc->dx);
	if (y2 >= y + isc->dy) {
		/* rtsc is above isc, replace rtsc by isc */
		rtsc->x = x;
		rtsc->y = y;
		rtsc->dx = isc->dx;
		rtsc->dy = isc->dy;
		return;
	}

	/*
	 * the two curves intersect
	 * compute the offsets (dx, dy) using the reverse
	 * function of seg_x2y()
	 *	seg_x2y(dx, sm1) == seg_x2y(dx, sm2) + (y1 - y)
	 */
	dx = ((y1 - y) << SM_SHIFT) / (isc->sm1 - isc->sm2);
	/*
	 * check if (x, y1) belongs to the 1st segment of rtsc.
	 * if so, add the offset.
	 */
	if (rtsc->x + rtsc->dx > x)
		dx += rtsc->x + rtsc->dx - x;
	dy = seg_x2y(dx, isc->sm1);

	rtsc->x = x;
	rtsc->y = y;
	rtsc->dx = dx;
	rtsc->dy = dy;
	return;
}

void
hfsc_getclstats(struct hfsc_class_stats *sp, struct hfsc_class *cl)
{
	sp->class_id = cl->cl_id;
	sp->class_handle = cl->cl_handle;

	if (cl->cl_rsc != NULL) {
		sp->rsc.m1 = sm2m(cl->cl_rsc->sm1);
		sp->rsc.d = dx2d(cl->cl_rsc->dx);
		sp->rsc.m2 = sm2m(cl->cl_rsc->sm2);
	} else {
		sp->rsc.m1 = 0;
		sp->rsc.d = 0;
		sp->rsc.m2 = 0;
	}
	if (cl->cl_fsc != NULL) {
		sp->fsc.m1 = sm2m(cl->cl_fsc->sm1);
		sp->fsc.d = dx2d(cl->cl_fsc->dx);
		sp->fsc.m2 = sm2m(cl->cl_fsc->sm2);
	} else {
		sp->fsc.m1 = 0;
		sp->fsc.d = 0;
		sp->fsc.m2 = 0;
	}
	if (cl->cl_usc != NULL) {
		sp->usc.m1 = sm2m(cl->cl_usc->sm1);
		sp->usc.d = dx2d(cl->cl_usc->dx);
		sp->usc.m2 = sm2m(cl->cl_usc->sm2);
	} else {
		sp->usc.m1 = 0;
		sp->usc.d = 0;
		sp->usc.m2 = 0;
	}

	sp->total = cl->cl_total;
	sp->cumul = cl->cl_cumul;

	sp->d = cl->cl_d;
	sp->e = cl->cl_e;
	sp->vt = cl->cl_vt;
	sp->f = cl->cl_f;

	sp->initvt = cl->cl_initvt;
	sp->vtperiod = cl->cl_vtperiod;
	sp->parentperiod = cl->cl_parentperiod;
	sp->nactive = cl->cl_nactive;
	sp->vtoff = cl->cl_vtoff;
	sp->cvtmax = cl->cl_cvtmax;
	sp->myf = cl->cl_myf;
	sp->cfmin = cl->cl_cfmin;
	sp->cvtmin = cl->cl_cvtmin;
	sp->myfadj = cl->cl_myfadj;
	sp->vtadj = cl->cl_vtadj;

	sp->cur_time = hfsc_microuptime();
	sp->machclk_freq = HFSC_FREQ;

	sp->qlength = hfsc_class_qlength(cl);
	sp->qlimit = cl->cl_q.qlimit;
	sp->xmit_cnt = cl->cl_stats.xmit_cnt;
	sp->drop_cnt = cl->cl_stats.drop_cnt;
	sp->period = cl->cl_stats.period;

	sp->qtype = 0;
}

/* convert a class handle to the corresponding class pointer */
struct hfsc_class *
hfsc_clh2cph(struct hfsc_if *hif, u_int32_t chandle)
{
	int i;
	struct hfsc_class *cl;

	if (chandle == 0)
		return (NULL);
	/*
	 * first, try the slot corresponding to the lower bits of the handle.
	 * if it does not match, do the linear table search.
	 */
	i = chandle % hif->hif_allocated;
	if ((cl = hif->hif_class_tbl[i]) != NULL && cl->cl_handle == chandle)
		return (cl);
	for (i = 0; i < hif->hif_allocated; i++)
		if ((cl = hif->hif_class_tbl[i]) != NULL &&
		    cl->cl_handle == chandle)
			return (cl);
	return (NULL);
}
@


1.42
log
@Factor out internal FIFO queue manager

HFSC internal queue becomes accessible via pf queueing ops. It will
also select an alternative queue manager based on the queue spec.

Discussed with and OK henning@@ at d2k17 as a part of a larger diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.41 2017/06/28 18:24:02 mikeb Exp $	*/
d775 1
a775 1
	if (cl->cl_qops != pfq_hfsc_ops)
@


1.41
log
@hfsc.c should depend on pf instead of inet

Discussed with and OK henning@@ at d2k17 as a part of a larger diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.40 2017/06/12 23:20:10 dlg Exp $	*/
d126 4
a129 2
/*	struct red	*cl_red;*/	/* RED state */
	struct altq_pktattr *cl_pktattr; /* saved header used by ECN */
d289 5
d299 6
a304 1
	hfsc_pf_qstats
d309 38
d415 1
d456 29
d525 44
a568 1
	struct hfsc_if *hif = arg;
d570 1
a570 1
	hfsc_free(0, hif);
d743 1
a743 1
	KASSERT(ml_empty(&cl->cl_q.q));
d774 4
d818 1
a824 1
		cl->cl_pktattr = NULL;
d827 1
a827 8
	if (ml_len(&cl->cl_q.q) >= cl->cl_q.qlimit) {
		/* drop occurred.  mbuf needs to be freed */
		PKTCNTR_INC(&cl->cl_stats.drop_cnt, m->m_pkthdr.len);
		return (m);
	}

	ml_enqueue(&cl->cl_q.q, m);
	m->m_pkthdr.pf.prio = IFQ_MAXPRIO;
d830 1
a830 1
	if (ml_len(&cl->cl_q.q) == 1)
d833 5
a837 1
	return (NULL);
d843 1
d884 7
a890 2
	m = MBUF_LIST_FIRST(&cl->cl_q.q);
	KASSERT(m != NULL);
d900 1
d911 1
a911 2
	m0 = ml_dequeue(&cl->cl_q.q);
	KASSERT(m == m0);
d919 6
a924 1
	if (ml_len(&cl->cl_q.q) > 0) {
d927 2
a928 1
			m0 = MBUF_LIST_FIRST(&cl->cl_q.q);
d964 1
a964 10
	struct mbuf *m;

	if (ml_empty(&cl->cl_q.q))
		return;

	MBUF_LIST_FOREACH(&cl->cl_q.q, m)
		PKTCNTR_INC(&cl->cl_stats.drop_cnt, m->m_pkthdr.len);

	ml_enlist(ml, &cl->cl_q.q);

d1131 1
a1131 1
	int go_passive;
d1133 2
a1134 1
	go_passive = ml_empty(&cl->cl_q.q);
d1694 1
a1694 1
	sp->qlength = ml_len(&cl->cl_q.q);
@


1.40
log
@have the timeout call if qstart via the serialiser isntead of directly

hfsc schedules a timeout to keep traffic moving if somethings has been
delayed an no other tx activity has occurred. that timeout was calling
(*ifp->if_qstart)(ifq) rather than ifq_start. the latter prevents
concurrent calls to if_qstart.

without this change bjorn ketelaars on misc@@ was experiencing weird
pauses in traffic and lockups because the tx ring was corrupted
because re_start was run concurrently, once from the stack and once
from hfsc.

thanks to bjorn ketelaars for debugging, and mikeb@@ for most of the
legwork in diagnosing the problem.
ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.39 2017/05/08 11:30:53 mikeb Exp $	*/
a62 3
/* need to provide dummies for hfsc-less kernels to reduce the if.h horror */
#include "pf.h"
#if NPF > 0
a1596 1
#endif
@


1.39
log
@Add a compatibility shim for older pfctl binaries

Found by and input from dlg@@, OK sthen, tedu, henning
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.38 2017/05/02 12:27:37 mikeb Exp $	*/
d820 1
a820 1
		(*ifp->if_qstart)(ifq);
@


1.38
log
@Provide pluggable queueing interface for pf

By hiding H-FSC behind pfq_ops structure similar to the ifq_ops,
we provide a possibility to plug alternative queueing interfaces
for use in pf.  This reduces amount of H-FSC specific code in the
pf ioctl handler

While here, change the the order of elements in hfsc_class_stats
to provide some compatibility between queue stat structures of
different traffic conditioners.

No objections from henning@@, ok sthen@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.37 2017/04/26 15:50:59 mikeb Exp $	*/
d394 8
@


1.37
log
@Perform H-FSC root queue allocation in the kernel

Since only leaf queues can have packets assigned to them,
H-FSC requires the user specified root queue to have a
parent.  To simplify userland tools and the configuration
interface, the kernel can be leveraged to set it up.

ok henning
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.36 2017/03/07 01:29:53 dlg Exp $	*/
d282 18
d344 1
a344 1
struct hfsc_if *
d363 1
a363 1
hfsc_pf_addqueue(struct hfsc_if *hif, struct pf_queuespec *q)
d365 1
d438 1
a438 1
hfsc_pf_free(struct hfsc_if *hif)
d440 2
@


1.36
log
@deprecate ifq_enqueue_try, and let backends drop arbitrary mbufs.

mikeb@@ wants priq to be able to drop lower priority packets if the
current one is high. because ifq avoids freeing an mbuf while an
ifq mutex is held, he needs a way for a backend to return an arbitrary
mbuf to drop rather than signal that the current one needs to be
dropped.

this lets the backends return the mbuf to be dropped, which may or
may not be the current one.

to support this ifq_enqueue_try has to be dropped because it can
only signal about the current mbuf. nothing uses it (except
ifq_enqueue), so we can get rid of it. it wasnt even documented.

this diff includes some tweaks by mikeb@@ around the statistics
gathered in ifq_enqueue when an mbuf is dropped.
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.35 2017/01/24 03:57:35 dlg Exp $	*/
d352 6
a357 4
	if (q->parent_qid == HFSC_NULLCLASS_HANDLE &&
	    hif->hif_rootclass == NULL)
		parent = NULL;
	else if ((parent = hfsc_clh2cph(hif, q->parent_qid)) == NULL)
d451 2
a452 1
	int i;
d459 8
a466 4
	i = hif->hif_allocated;
	do
		hfsc_class_destroy(hif, hif->hif_class_tbl[--i]);
	while (i > 0);
@


1.35
log
@add support for multiple transmit ifqueues per network interface.

an ifq to transmit a packet is picked by the current traffic
conditioner (ie, priq or hfsc) by providing an index into an array
of ifqs. by default interfaces get a single ifq but can ask for
more using if_attach_queues().

the vast majority of our drivers still think there's a 1:1 mapping
between interfaces and transmit queues, so their if_start routines
take an ifnet pointer instead of a pointer to the ifqueue struct.
instead of changing all the drivers in the tree, drivers can opt
into using an if_qstart routine and setting the IFXF_MPSAFE flag.
the stack provides a compatability wrapper from the new if_qstart
handler to the previous if_start handlers if IFXF_MPSAFE isnt set.

enabling hfsc on an interface configures it to transmit everything
through the first ifq. any other ifqs are left configured as priq,
but unused, when hfsc is enabled.

getting this in now so everyone can kick the tyres.

ok mpi@@ visa@@ (who provided some tweaks for cnmac).
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.33 2016/09/15 02:00:18 dlg Exp $	*/
d263 1
a263 1
int		 hfsc_enq(struct ifqueue *, struct mbuf *);
d653 1
a653 1
int
d663 1
a663 1
			return (ENOBUFS);
d670 1
a670 1
		return (ENOBUFS);
d680 1
a680 1
	return (0);
@


1.34
log
@white space fixes. no functional change.
@
text
@d262 1
a262 2
void		*hfsc_alloc(void *);
void		 hfsc_free(void *);
d267 2
d271 1
a271 2
	hfsc_alloc,
	hfsc_free,
d276 2
d419 13
a431 1
	hfsc_free(hif);
d435 1
a435 1
hfsc_alloc(void *q)
d438 1
d446 1
a446 1
hfsc_free(void *q)
d452 1
d777 1
a778 1
	int s;
d781 1
a781 1
	KASSERT(HFSC_ENABLED(&ifp->if_snd));
d783 2
a784 4
	s = splnet();
	if (!IFQ_IS_EMPTY(&ifp->if_snd))
		if_start(ifp);
	splx(s);
d786 1
a786 1
	hif = ifp->if_snd.ifq_q;
@


1.33
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.32 2015/11/21 01:08:49 dlg Exp $	*/
d270 6
a275 6
        hfsc_alloc,
        hfsc_free,
        hfsc_enq,
        hfsc_deq_begin,
        hfsc_deq_commit,
        hfsc_purge,
@


1.32
log
@simplify ifq_deq_rollback by only having it unlock.

hfsc needed a rollback ifqop to requeue the mbuf because it used
ml_dequeue in the begin op. now it uses MBUF_LIST_FIRST to get a
ref to the first mbuf in deq_begin.

now the disciplines dont need a rollback op, so ifq_deq_rollback
can be simplified to just releasing the mutex.

based on a discussion with kenjiro cho
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.31 2015/11/20 03:35:23 dlg Exp $	*/
d318 4
a321 6
	pool_init(&hfsc_class_pl, sizeof(struct hfsc_class), 0, 0, PR_WAITOK,
	    "hfscclass", NULL);
	pool_setipl(&hfsc_class_pl, IPL_NONE);
	pool_init(&hfsc_internal_sc_pl, sizeof(struct hfsc_internal_sc), 0, 0,
	    PR_WAITOK, "hfscintsc", NULL);
	pool_setipl(&hfsc_internal_sc_pl, IPL_NONE);
@


1.31
log
@shuffle struct ifqueue so in flight mbufs are protected by a mutex.

the code is refactored so the IFQ macros call newly implemented ifq
functions. the ifq code is split so each discipline (priq and hfsc
in our case) is an opaque set of operations that the common ifq
code can call. the common code does the locking, accounting (ifq_len
manipulation), and freeing of the mbuf if the disciplines enqueue
function rejects it. theyre kind of like bufqs in the block layer
with their fifo and nscan disciplines.

the new api also supports atomic switching of disciplines at runtime.
the hfsc setup in pf_ioctl.c has been tweaked to build a complete
hfsc_if structure which it attaches to the send queue in a single
operation, rather than attaching to the interface up front and
building up a list of queues.

the send queue is now mutexed, which raises the expectation that
packets can be enqueued or purged on one cpu while another cpu is
dequeueing them in a driver for transmission. a lot of drivers use
IFQ_POLL to peek at an mbuf and attempt to fit it on the ring before
committing to it with a later IFQ_DEQUEUE operation. if the mbuf
gets freed in between the POLL and DEQUEUE operations, fireworks
will ensue.

to avoid this, the ifq api introduces ifq_deq_begin, ifq_deq_rollback,
and ifq_deq_commit. ifq_deq_begin allows a driver to take the ifq
mutex and get a reference to the mbuf they wish to try and tx. if
there's space, they can ifq_deq_commit it to remove the mbuf and
release the mutex. if there's no space, ifq_deq_rollback simply
releases the mutex. this api was developed to make updating the
drivers using IFQ_POLL easy, instead of having to do significant
semantic changes to avoid POLL that we cannot test on all the
hardware.

the common code has been tested pretty hard, and all the driver
modifications are straightforward except for de(4). if that breaks
it can be dealt with later.

ok mpi@@ jmatthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.29 2015/10/23 02:29:24 dlg Exp $	*/
a266 1
void		 hfsc_deq_rollback(struct ifqueue *, struct mbuf *, void *);
a274 1
        hfsc_deq_rollback,
d712 1
a712 1
	m = ml_dequeue(&cl->cl_q.q);
d725 1
d733 3
d745 2
a746 1
			next_len = cl->cl_q.q.ml_head->m_pkthdr.len;
a756 8
}

void
hfsc_deq_rollback(struct ifqueue *ifq, struct mbuf *m, void *cookie)
{
	struct hfsc_class *cl = cookie;

	ml_requeue(&cl->cl_q.q, m);
@


1.30
log
@kenjiro cho points out that requeue is hard to support on queue
disciplines

while i was simply concerned with the safety of the mbuf, requeue
is weird when it comes to how statistics are supposed to be handled
and ultimately isnt worth it.

this removes hfsc_requeue.
@
text
@a183 1
	struct ifqueue		*hif_ifq;	/* backpointer to ifq */
d187 2
a188 1
	struct hfsc_class	*hif_pollcache;	/* cache for poll operation */
d209 2
a210 3
struct mbuf	*hfsc_cl_dequeue(struct hfsc_class *);
struct mbuf	*hfsc_cl_poll(struct hfsc_class *);
void		 hfsc_cl_purge(struct hfsc_if *, struct hfsc_class *);
d258 24
d322 1
d325 1
d328 2
a329 2
int
hfsc_attach(struct ifnet *ifp)
d333 1
a333 2
	if (ifp == NULL || ifp->if_snd.ifq_hfsc != NULL)
		return (0);
d335 1
a335 1
	hif = malloc(sizeof(struct hfsc_if), M_DEVBUF, M_WAITOK | M_ZERO);
a340 3
	hif->hif_ifq = &ifp->if_snd;
	ifp->if_snd.ifq_hfsc = hif;

a341 2
	/* XXX HRTIMER don't schedule it yet, only when some packets wait. */
	timeout_add(&hif->hif_defer, 1);
d343 1
a343 1
	return (0);
d347 1
a347 1
hfsc_detach(struct ifnet *ifp)
a348 19
	struct hfsc_if *hif;

	if (ifp == NULL)
		return (0);

	hif = ifp->if_snd.ifq_hfsc;
	timeout_del(&hif->hif_defer);
	ifp->if_snd.ifq_hfsc = NULL;

	free(hif->hif_class_tbl, M_DEVBUF, hif->hif_allocated * sizeof(void *));
	free(hif, M_DEVBUF, sizeof(struct hfsc_if));

	return (0);
}

int
hfsc_addqueue(struct pf_queuespec *q)
{
	struct hfsc_if *hif;
d352 1
a352 5
	if (q->kif->pfik_ifp == NULL)
		return (0);

	if ((hif = q->kif->pfik_ifp->if_snd.ifq_hfsc) == NULL)
		return (EINVAL);
d385 1
a385 1
hfsc_delqueue(struct pf_queuespec *q)
d387 1
d390 2
d393 2
a394 2
	if (q->kif->pfik_ifp == NULL)
		return (0);
d396 1
a396 1
	if ((hif = q->kif->pfik_ifp->if_snd.ifq_hfsc) == NULL)
d399 6
a404 1
	if ((cl = hfsc_clh2cph(hif, q->qid)) == NULL)
d406 4
d411 11
a421 1
	return (hfsc_class_destroy(hif, cl));
d424 2
a425 2
int
hfsc_qstats(struct pf_queuespec *q, void *ubuf, int *nbytes)
d427 2
a428 4
	struct hfsc_if *hif;
	struct hfsc_class *cl;
	struct hfsc_class_stats stats;
	int error = 0;
d430 3
a432 2
	if (q->kif->pfik_ifp == NULL)
		return (EBADF);
d434 5
a438 2
	if ((hif = q->kif->pfik_ifp->if_snd.ifq_hfsc) == NULL)
		return (EBADF);
d440 1
a440 2
	if ((cl = hfsc_clh2cph(hif, q->qid)) == NULL)
		return (EINVAL);
d442 1
a442 2
	if (*nbytes < sizeof(stats))
		return (EINVAL);
d444 4
a447 1
	hfsc_getclstats(&stats, cl);
d449 2
a450 4
	if ((error = copyout((caddr_t)&stats, ubuf, sizeof(stats))) != 0)
		return (error);
	*nbytes = sizeof(stats);
	return (0);
d454 1
a454 1
hfsc_purge(struct ifqueue *ifq)
d456 1
a456 1
	struct hfsc_if		*hif = ifq->ifq_hfsc;
d460 1
a460 3
		if (ml_len(&cl->cl_q.q) > 0)
			hfsc_cl_purge(hif, cl);
	hif->hif_ifq->ifq_len = 0;
d575 1
a575 3

	if (ml_len(&cl->cl_q.q) > 0)
		hfsc_cl_purge(hif, cl);
d642 1
a642 1
hfsc_enqueue(struct ifqueue *ifq, struct mbuf *m)
d644 1
a644 1
	struct hfsc_if	*hif = ifq->ifq_hfsc;
d656 1
a656 1
		/* drop. mbuf needs to be freed */
d660 1
a661 1
	ifq->ifq_len++;
d672 1
a672 1
hfsc_dequeue(struct ifqueue *ifq, int remove)
d674 1
a674 1
	struct hfsc_if *hif = ifq->ifq_hfsc;
a676 1
	int next_len, realtime = 0;
a678 3
	if (IFQ_LEN(ifq) == 0)
		return (NULL);

d681 7
a687 7
	if (remove && hif->hif_pollcache != NULL) {
		cl = hif->hif_pollcache;
		hif->hif_pollcache = NULL;
		/* check if the class was scheduled by real-time criteria */
		if (cl->cl_rsc != NULL)
			realtime = (cl->cl_e <= cur_time);
	} else {
d689 2
a690 3
		 * if there are eligible classes, use real-time criteria.
		 * find the class with the minimum deadline among
		 * the eligible classes.
d692 8
a699 3
		if ((cl = hfsc_ellist_get_mindl(hif, cur_time)) != NULL) {
			realtime = 1;
		} else {
d701 2
a702 2
			 * use link-sharing criteria
			 * get the class with the minimum vt in the hierarchy
d704 2
a705 2
			cl = NULL;
			tcl = hif->hif_rootclass;
d707 6
a712 4
			while (tcl != NULL && tcl->cl_children != NULL) {
				tcl = hfsc_actlist_firstfit(tcl, cur_time);
				if (tcl == NULL)
					continue;
d714 2
a715 6
				/*
				 * update parent's cl_cvtmin.
				 * don't update if the new vt is smaller.
				 */
				if (tcl->cl_parent->cl_cvtmin < tcl->cl_vt)
					tcl->cl_parent->cl_cvtmin = tcl->cl_vt;
d717 4
a720 6
				cl = tcl;
			}
			/* XXX HRTIMER plan hfsc_deferred precisely here. */
			if (cl == NULL)
				return (NULL);
		}
d722 7
a728 6
		if (!remove) {
			hif->hif_pollcache = cl;
			m = hfsc_cl_poll(cl);
			return (m);
		}
	}
d730 3
a732 2
	if ((m = hfsc_cl_dequeue(cl)) == NULL)
		panic("hfsc_dequeue");
a733 1
	ifq->ifq_len--;
d754 1
d756 6
a761 1
	return (m);
d768 1
d771 3
d775 1
a775 1
	if (HFSC_ENABLED(&ifp->if_snd) && !IFQ_IS_EMPTY(&ifp->if_snd))
d779 2
d782 1
a782 14
	timeout_add(&ifp->if_snd.ifq_hfsc->hif_defer, 1);
}

struct mbuf *
hfsc_cl_dequeue(struct hfsc_class *cl)
{
	return (ml_dequeue(&cl->cl_q.q));
}

struct mbuf *
hfsc_cl_poll(struct hfsc_class *cl)
{
	/* XXX */
	return (cl->cl_q.q.ml_head);
d786 1
a786 1
hfsc_cl_purge(struct hfsc_if *hif, struct hfsc_class *cl)
d793 1
a793 1
	while ((m = hfsc_cl_dequeue(cl)) != NULL) {
d795 2
a796 3
		m_freem(m);
		hif->hif_ifq->ifq_len--;
	}
a1556 21

#else /* NPF > 0 */

void
hfsc_purge(struct ifqueue *q)
{
	panic("hfsc_purge called on hfsc-less kernel");
}

int
hfsc_enqueue(struct ifqueue *q, struct mbuf *m)
{
	panic("hfsc_enqueue called on hfsc-less kernel");
}

struct mbuf *
hfsc_dequeue(struct ifqueue *q, int i)
{
	panic("hfsc_enqueue called on hfsc-less kernel");
}

@


1.29
log
@pass the right sizes to free.
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.28 2015/10/23 02:08:37 dlg Exp $	*/
a208 3
int		 hfsc_queue(struct ifqueue *, struct mbuf *,
		     int (*)(struct hfsc_class *, struct mbuf *));
int		 hfsc_cl_enqueue(struct hfsc_class *, struct mbuf *);
a209 1
int		 hfsc_cl_requeue(struct hfsc_class *, struct mbuf *);
a628 13
	return (hfsc_queue(ifq, m, hfsc_cl_enqueue));
}

void
hfsc_requeue(struct ifqueue *ifq, struct mbuf *m)
{
	(void)hfsc_queue(ifq, m, hfsc_cl_requeue);
}

int
hfsc_queue(struct ifqueue *ifq, struct mbuf *m,
    int (*queue)(struct hfsc_class *, struct mbuf *))
{
d640 2
a641 2
	if ((*queue)(cl, m) != 0) {
		/* drop occurred.  mbuf needs to be freed */
d645 1
a758 19
}

int
hfsc_cl_enqueue(struct hfsc_class *cl, struct mbuf *m)
{
	if (ml_len(&cl->cl_q.q) >= cl->cl_q.qlimit)
		return (-1);

	ml_enqueue(&cl->cl_q.q, m);

	return (0);
}

int
hfsc_cl_requeue(struct hfsc_class *cl, struct mbuf *m)
{
	ml_requeue(&cl->cl_q.q, m);

	return (0);
@


1.28
log
@inline the hfsc_active TAILQ.

make cl_actc in hfsc_class a TAILQ rather than a pointer to a TAILQ
that gets allocated seaprately.
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.27 2015/10/23 01:53:02 dlg Exp $	*/
d285 1
d291 1
a291 1
	memcpy(newtbl, old, hif->hif_allocated * sizeof(void *));
d295 1
a295 1
	free(old, M_DEVBUF, 0);
a310 3
	size_t tblsize;

	tblsize = HFSC_DEFAULT_CLASSES * sizeof(void *);
d317 2
a318 1
	hif->hif_class_tbl = malloc(tblsize, M_DEVBUF, M_WAITOK | M_ZERO);
d343 2
a344 2
	free(hif->hif_class_tbl, M_DEVBUF, 0);
	free(hif, M_DEVBUF, 0);
@


1.27
log
@inline the hif_eligible TAILQ.

make hif_eligible in hfsc_if a TAILQ rather than a pointer to a
TAILQ that gets allocated separately.

"look ma, i saved 4 or 8 bytes"
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.26 2015/10/23 01:32:10 dlg Exp $	*/
d116 1
a116 3
typedef TAILQ_HEAD(hfsc_active, hfsc_class) hfsc_actlist_t;
typedef TAILQ_ENTRY(hfsc_class) hfsc_actentry_t;
#define	hfsc_actlist_first(s)		TAILQ_FIRST(s)
d167 1
a167 1
	hfsc_actlist_t	*cl_actc;	/* active children list */
d169 1
a169 1
	hfsc_actentry_t	cl_actlist;	/* active children list entry */
a229 2
hfsc_actlist_t	*hfsc_actlist_alloc(void);
void		 hfsc_actlist_destroy(hfsc_actlist_t *);
d471 1
a471 1
	cl->cl_actc = hfsc_actlist_alloc();
a540 2
	if (cl->cl_actc != NULL)
		hfsc_actlist_destroy(cl->cl_actc);
d589 1
a589 1
	hfsc_actlist_destroy(cl->cl_actc);
d910 2
a911 1
			max_cl = hfsc_actlist_last(cl->cl_parent->cl_actc);
d1076 1
a1076 1
	if (TAILQ_EMPTY(cl->cl_actc)) {
d1081 1
a1081 1
	TAILQ_FOREACH(p, cl->cl_actc, cl_actlist) {
a1175 16
hfsc_actlist_t *
hfsc_actlist_alloc(void)
{
	hfsc_actlist_t *head;

	head = malloc(sizeof(hfsc_actlist_t), M_DEVBUF, M_WAITOK);
	TAILQ_INIT(head);
	return (head);
}

void
hfsc_actlist_destroy(hfsc_actlist_t *head)
{
	free(head, M_DEVBUF, 0);
}

d1182 1
a1182 1
	if ((p = TAILQ_LAST(cl->cl_parent->cl_actc, hfsc_active)) == NULL
d1184 1
a1184 1
		TAILQ_INSERT_TAIL(cl->cl_parent->cl_actc, cl, cl_actlist);
d1188 1
a1188 1
	TAILQ_FOREACH(p, cl->cl_parent->cl_actc, cl_actlist) {
d1199 1
a1199 1
	TAILQ_REMOVE(cl->cl_parent->cl_actc, cl, cl_actlist);
d1217 1
a1217 1
	last = TAILQ_LAST(cl->cl_parent->cl_actc, hfsc_active);
d1219 2
a1220 2
		TAILQ_REMOVE(cl->cl_parent->cl_actc, cl, cl_actlist);
		TAILQ_INSERT_TAIL(cl->cl_parent->cl_actc, cl, cl_actlist);
d1230 1
a1230 1
			TAILQ_REMOVE(cl->cl_parent->cl_actc, cl, cl_actlist);
d1242 1
a1242 1
	TAILQ_FOREACH(p, cl->cl_actc, cl_actlist)
@


1.26
log
@counting packets in hif_packets in hfsc_if is redundant.

the ifqueue struct has the same information, and hif_packets is never
read separately. trim it.
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.25 2015/10/23 01:02:46 dlg Exp $	*/
d115 1
a115 2
typedef TAILQ_HEAD(hfsc_eligible, hfsc_class) hfsc_ellist_t;
typedef TAILQ_ENTRY(hfsc_class) hfsc_elentry_t;
a117 1
#define	hfsc_ellist_first(s)		TAILQ_FIRST(s)
d172 1
a172 1
	hfsc_elentry_t	cl_ellist;	/* eligible list entry */
d196 1
a196 1
	hfsc_ellist_t *hif_eligible;			/* eligible list */
d228 4
a231 6
hfsc_ellist_t	*hfsc_ellist_alloc(void);
void		 hfsc_ellist_destroy(hfsc_ellist_t *);
void		 hfsc_ellist_insert(hfsc_ellist_t *, struct hfsc_class *);
void		 hfsc_ellist_remove(hfsc_ellist_t *, struct hfsc_class *);
void		 hfsc_ellist_update(hfsc_ellist_t *, struct hfsc_class *);
struct hfsc_class	*hfsc_ellist_get_mindl(hfsc_ellist_t *, u_int64_t);
d322 1
a322 1
	hif->hif_eligible = hfsc_ellist_alloc();
a347 1
	hfsc_ellist_destroy(hif->hif_eligible);
d705 1
a705 2
		if ((cl = hfsc_ellist_get_mindl(hif->hif_eligible, cur_time)) !=
		    NULL) {
d849 1
a849 1
		hfsc_ellist_remove(hif->hif_eligible, cl);
d882 1
a882 1
	hfsc_ellist_insert(hif->hif_eligible, cl);
d891 1
a891 1
	hfsc_ellist_update(hif->hif_eligible, cl);
a1100 16
hfsc_ellist_t *
hfsc_ellist_alloc(void)
{
	hfsc_ellist_t *head;

	head = malloc(sizeof(hfsc_ellist_t), M_DEVBUF, M_WAITOK);
	TAILQ_INIT(head);
	return (head);
}

void
hfsc_ellist_destroy(hfsc_ellist_t *head)
{
	free(head, M_DEVBUF, 0);
}

d1102 1
a1102 1
hfsc_ellist_insert(hfsc_ellist_t *head, struct hfsc_class *cl)
d1107 1
a1107 1
	if ((p = TAILQ_LAST(head, hfsc_eligible)) == NULL ||
d1109 1
a1109 1
		TAILQ_INSERT_TAIL(head, cl, cl_ellist);
d1113 1
a1113 1
	TAILQ_FOREACH(p, head, cl_ellist) {
d1122 1
a1122 1
hfsc_ellist_remove(hfsc_ellist_t *head, struct hfsc_class *cl)
d1124 1
a1124 1
	TAILQ_REMOVE(head, cl, cl_ellist);
d1128 1
a1128 1
hfsc_ellist_update(hfsc_ellist_t *head, struct hfsc_class *cl)
d1141 1
a1141 1
	last = TAILQ_LAST(head, hfsc_eligible);
d1143 2
a1144 2
		TAILQ_REMOVE(head, cl, cl_ellist);
		TAILQ_INSERT_TAIL(head, cl, cl_ellist);
d1154 1
a1154 1
			TAILQ_REMOVE(head, cl, cl_ellist);
d1163 1
a1163 1
hfsc_ellist_get_mindl(hfsc_ellist_t *head, u_int64_t cur_time)
d1167 1
a1167 1
	TAILQ_FOREACH(p, head, cl_ellist) {
@


1.25
log
@remove the pointer from hfsc_class structs back to hfsc_if.

you get to hfsc_class via a hfsc_if, so just pass the hfsc_if around
on the stack when we need it rather than following the pointer back.

most of this change is passing the hif on the stack.

ok mpi@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.24 2015/09/30 22:57:47 dlg Exp $	*/
a195 1
	u_int	hif_packets;			/* # of packets in the tree */
a674 1
	hif->hif_packets++;
d693 1
a693 1
	if (hif->hif_packets == 0)
a749 1
	hif->hif_packets--;
a832 1
		hif->hif_packets--;
@


1.24
log
@rename the internal functions that do ml_foo ops on classes to hfsc_cl_foo.

this avoids confusion with the public functions (hfsc_enqueue,
hfsc_dequeue, etc), and maps almost 1:1 to the mbuf list ops they
now use.

ok mpi@@ henning@@ mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.23 2015/09/30 11:36:20 dlg Exp $	*/
a125 1
	struct hfsc_if	*cl_hif;	/* back pointer to struct hfsc_if */
d210 2
a211 1
int			 hfsc_class_destroy(struct hfsc_class *);
d220 1
a220 1
void		 hfsc_cl_purge(struct hfsc_class *);
d224 4
a227 4
void		 hfsc_set_active(struct hfsc_class *, int);
void		 hfsc_set_passive(struct hfsc_class *);
void		 hfsc_init_ed(struct hfsc_class *, int);
void		 hfsc_update_ed(struct hfsc_class *, int);
d233 3
a235 3
void		 hfsc_ellist_insert(struct hfsc_class *);
void		 hfsc_ellist_remove(struct hfsc_class *);
void		 hfsc_ellist_update(struct hfsc_class *);
d418 1
a418 1
	return (hfsc_class_destroy(cl));
d457 1
a457 1
			hfsc_cl_purge(cl);
a505 1
	cl->cl_hif = hif;
d564 1
a564 1
hfsc_class_destroy(struct hfsc_class *cl)
d577 1
a577 1
		hfsc_cl_purge(cl);
d592 3
a594 3
	for (i = 0; i < cl->cl_hif->hif_allocated; i++)
		if (cl->cl_hif->hif_class_tbl[i] == cl) {
			cl->cl_hif->hif_class_tbl[i] = NULL;
d598 1
a598 1
	cl->cl_hif->hif_classes--;
d603 4
a606 4
	if (cl == cl->cl_hif->hif_rootclass)
		cl->cl_hif->hif_rootclass = NULL;
	if (cl == cl->cl_hif->hif_defaultclass)
		cl->cl_hif->hif_defaultclass = NULL;
d676 1
a676 1
	cl->cl_hif->hif_packets++;
d681 1
a681 1
		hfsc_set_active(cl, m->m_pkthdr.len);
d752 1
a752 1
	cl->cl_hif->hif_packets--;
d766 1
a766 1
				hfsc_update_ed(cl, next_len);
d772 1
a772 1
		hfsc_set_passive(cl);
d826 1
a826 1
hfsc_cl_purge(struct hfsc_class *cl)
d836 2
a837 2
		cl->cl_hif->hif_packets--;
		cl->cl_hif->hif_ifq->ifq_len--;
d841 1
a841 1
	hfsc_set_passive(cl);
d845 1
a845 1
hfsc_set_active(struct hfsc_class *cl, int len)
d848 1
a848 1
		hfsc_init_ed(cl, len);
d856 1
a856 1
hfsc_set_passive(struct hfsc_class *cl)
d859 1
a859 1
		hfsc_ellist_remove(cl);
d868 1
a868 1
hfsc_init_ed(struct hfsc_class *cl, int next_len)
d892 1
a892 1
	hfsc_ellist_insert(cl);
d896 1
a896 1
hfsc_update_ed(struct hfsc_class *cl, int next_len)
d901 1
a901 1
	hfsc_ellist_update(cl);
d1128 1
a1128 1
hfsc_ellist_insert(struct hfsc_class *cl)
a1129 1
	struct hfsc_if *hif = cl->cl_hif;
d1133 1
a1133 1
	if ((p = TAILQ_LAST(hif->hif_eligible, hfsc_eligible)) == NULL ||
d1135 1
a1135 1
		TAILQ_INSERT_TAIL(hif->hif_eligible, cl, cl_ellist);
d1139 1
a1139 1
	TAILQ_FOREACH(p, hif->hif_eligible, cl_ellist) {
d1148 1
a1148 1
hfsc_ellist_remove(struct hfsc_class *cl)
d1150 1
a1150 3
	struct hfsc_if	*hif = cl->cl_hif;

	TAILQ_REMOVE(hif->hif_eligible, cl, cl_ellist);
d1154 1
a1154 1
hfsc_ellist_update(struct hfsc_class *cl)
a1155 1
	struct hfsc_if *hif = cl->cl_hif;
d1167 1
a1167 1
	last = TAILQ_LAST(hif->hif_eligible, hfsc_eligible);
d1169 2
a1170 2
		TAILQ_REMOVE(hif->hif_eligible, cl, cl_ellist);
		TAILQ_INSERT_TAIL(hif->hif_eligible, cl, cl_ellist);
d1180 1
a1180 1
			TAILQ_REMOVE(hif->hif_eligible, cl, cl_ellist);
@


1.23
log
@provide a hfsc_requeue()

this will allow packets to taken off an interfaces send queue, and
requeued if space didnt exist on the hardware.

the internal names are a bit ugly, i want to change them in the
next commit.

ok henning@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.22 2015/09/27 05:23:50 dlg Exp $	*/
d216 6
a221 5
int		 hfsc_addq(struct hfsc_class *, struct mbuf *);
int		 hfsc_prependq(struct hfsc_class *, struct mbuf *);
struct mbuf	*hfsc_getq(struct hfsc_class *);
struct mbuf	*hfsc_pollq(struct hfsc_class *);
void		 hfsc_purgeq(struct hfsc_class *);
d457 1
a457 1
			hfsc_purgeq(cl);
d578 1
a578 1
		hfsc_purgeq(cl);
d647 1
a647 1
	return (hfsc_queue(ifq, m, hfsc_addq));
d653 1
a653 1
	(void)hfsc_queue(ifq, m, hfsc_prependq);
d745 1
a745 1
			m = hfsc_pollq(cl);
d750 1
a750 1
	if ((m = hfsc_getq(cl)) == NULL)
d795 1
a795 1
hfsc_addq(struct hfsc_class *cl, struct mbuf *m)
d806 1
a806 1
hfsc_prependq(struct hfsc_class *cl, struct mbuf *m)
d814 1
a814 1
hfsc_getq(struct hfsc_class *cl)
d820 1
a820 1
hfsc_pollq(struct hfsc_class *cl)
d827 1
a827 1
hfsc_purgeq(struct hfsc_class *cl)
d834 1
a834 1
	while ((m = hfsc_getq(cl)) != NULL) {
@


1.22
log
@pull the m_freem calls out of hfsc_enqueue by having IFQ_ENQUEUE free
the mbuf in both the hfsc and priq error paths.

ok mikeb@@ mpi@@ claudio@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.21 2015/04/18 11:12:33 dlg Exp $	*/
d214 2
d217 1
d646 13
d670 1
a670 1
	if (hfsc_addq(cl, m) != 0) {
d800 8
@


1.21
log
@replace the hand rolled lists of mbufs in hfsc_classq with an
mbuf_list.

hfsc lists are very clever because they manage a fifo with a single
pointer by abusing the m_next pointer of the tail mbuf to point to
the head. clever but hard to read.

mbuf_lists are slightly bigger because they explicitely track the
head mbuf, but i got us that space back by inlining hfsc_classq
into hfsc_class and removing the unnecessary classq field.

ok henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.20 2015/04/12 14:09:40 dlg Exp $	*/
d649 1
a649 2
		if (cl == NULL) {
			m_freem(m);
a650 1
		}
a656 1
		m_freem(m);
@


1.20
log
@hfsc_classq has a type member which is never set or read, except to report
its value to userland which will always be 0.

drop the member. lie to userland.

ok henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.19 2015/04/12 12:22:26 dlg Exp $	*/
d109 2
a110 3
	struct mbuf	*tail;	 /* Tail of packet queue */
	int		 qlen;	 /* Queue length (in number of packets) */
	int		 qlimit; /* Queue limit (in number of packets*) */
d452 1
a452 1
		if (cl->cl_q.qlen > 0)
d465 3
d479 1
a479 2
	if (qlimit == 0)
		qlimit = HFSC_DEFAULT_QLIMIT;
a480 1
	cl->cl_q.qlen = 0;
d573 1
a573 1
	if (cl->cl_q.qlen > 0)
d667 1
a667 1
	if (cl->cl_q.qlen == 1)
d747 1
a747 1
	if (cl->cl_q.qlen > 0) {
d750 1
a750 1
			next_len = cl->cl_q.tail->m_nextpkt->m_pkthdr.len;
d783 1
a783 3
	struct mbuf *m0;

	if (cl->cl_q.qlen >= cl->cl_q.qlimit)
d786 1
a786 8
	if ((m0 = cl->cl_q.tail) != NULL)
		m->m_nextpkt = m0->m_nextpkt;
	else
		m0 = m;

	m0->m_nextpkt = m;
	cl->cl_q.tail = m;
	cl->cl_q.qlen++;
d794 1
a794 11
	struct mbuf	*m, *m0;

	if ((m = cl->cl_q.tail) == NULL)
		return (NULL);
	if ((m0 = m->m_nextpkt) != m)
		m->m_nextpkt = m0->m_nextpkt;
	else 
		cl->cl_q.tail = NULL;
	cl->cl_q.qlen--;
	m0->m_nextpkt = NULL;
	return (m0);
d800 2
a801 3
	if (!cl->cl_q.tail)
		return (NULL);
	return (cl->cl_q.tail->m_nextpkt);
d809 1
a809 1
	if (cl->cl_q.qlen == 0)
d986 1
a986 1
	go_passive = (cl->cl_q.qlen == 0);
d1582 1
a1582 1
	sp->qlength = cl->cl_q.qlen;
@


1.19
log
@there's a 1:1 correlation between hfsc_class instances and hfsc_classq
instances, so maintaining separate pools for them and pointing
between them is overhead.

this drops the hfsc_classq pool and inlines it into hfsc_class.

ok henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.18 2015/04/12 09:58:46 dlg Exp $	*/
a111 1
	int		 qtype;	 /* Queue type */
d1608 1
a1608 1
	sp->qtype = cl->cl_q.qtype;
@


1.18
log
@pull structs and macros that are only used in hfsc.c out of the header
and into the .c file.

ok henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.17 2015/04/11 13:00:12 dlg Exp $	*/
d135 1
a135 1
	struct hfsc_classq *cl_q;	/* class queue structure */
d268 1
a268 1
struct pool	hfsc_class_pl, hfsc_classq_pl, hfsc_internal_sc_pl;
a308 2
	pool_init(&hfsc_classq_pl, sizeof(struct hfsc_classq), 0, 0, PR_WAITOK,
	    "hfscclassq", NULL);
d454 1
a454 1
		if (cl->cl_q->qlen > 0)
a475 1
	cl->cl_q = pool_get(&hfsc_classq_pl, PR_WAITOK | PR_ZERO);
d480 2
a481 2
	cl->cl_q->qlimit = qlimit;
	cl->cl_q->qlen = 0;
a556 2
	if (cl->cl_q != NULL)
		pool_put(&hfsc_classq_pl, cl->cl_q);
d574 1
a574 1
	if (cl->cl_q->qlen > 0)
a611 1
	pool_put(&hfsc_classq_pl, cl->cl_q);
d668 1
a668 1
	if (cl->cl_q->qlen == 1)
d748 1
a748 1
	if (cl->cl_q->qlen > 0) {
d751 1
a751 1
			next_len = cl->cl_q->tail->m_nextpkt->m_pkthdr.len;
d786 1
a786 1
	if (cl->cl_q->qlen >= cl->cl_q->qlimit)
d789 1
a789 1
	if ((m0 = cl->cl_q->tail) != NULL)
d795 2
a796 2
	cl->cl_q->tail = m;
	cl->cl_q->qlen++;
d806 1
a806 1
	if ((m = cl->cl_q->tail) == NULL)
d811 2
a812 2
		cl->cl_q->tail = NULL;
	cl->cl_q->qlen--;
d820 1
a820 1
	if (!cl->cl_q->tail)
d822 1
a822 1
	return (cl->cl_q->tail->m_nextpkt);
d830 1
a830 1
	if (cl->cl_q->qlen == 0)
d1007 1
a1007 1
	go_passive = (cl->cl_q->qlen == 0);
d1603 2
a1604 2
	sp->qlength = cl->cl_q->qlen;
	sp->qlimit = cl->cl_q->qlimit;
d1609 1
a1609 1
	sp->qtype = cl->cl_q->qtype;
@


1.17
log
@the hfsc pools are only used in hfsc.c, so move the init of them
there instead of pf_ioctl.c.

ok henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.16 2015/03/03 11:14:00 henning Exp $	*/
d54 1
d67 140
d263 3
@


1.16
log
@make sure hfsc_attach, hfsc_detach, hfsc_addqueue, hfsc_delqueue and
hfsc_qstats cope with a NULL ifp. Can happen when refering to nonexistant
interfaces from pf.conf. Problem noticed and fix tested by Kevin Chadwick
<ma1l1ists at yahoo.co.uk>, ok phessler benno
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.15 2015/02/08 03:16:16 henning Exp $	*/
d124 2
d158 11
@


1.15
log
@remove the dead code inside #if(def) RED_NOTYET
if we ever do sth like RED again it almost certainly won't be RED; the code
isn't even in a state to be useful as documentation/hint, and even if it
was it could be retrieved from the attic.
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.12 2014/12/05 15:50:04 mpi Exp $	*/
d166 1
a166 1
	if (ifp->if_snd.ifq_hfsc != NULL)
d187 4
a190 1
	struct hfsc_if *hif = ifp->if_snd.ifq_hfsc;
d192 1
d210 3
d252 3
d271 3
@


1.14
log
@using IFQ_INC_LEN and IFQ_DEC_LEN for len++/len-- is a bit excessive,
especially given that these are the only users of these macros.
ok claudio
@
text
@a315 30
#ifdef RED_NOTYET
	if (flags & HFSC_RED) {
		int red_flags, red_pkttime;
		u_int m2;

		m2 = 0;
		if (rsc != NULL && rsc->m2 > m2)
			m2 = rsc->m2;
		if (fsc != NULL && fsc->m2 > m2)
			m2 = fsc->m2;
		if (usc != NULL && usc->m2 > m2)
			m2 = usc->m2;

		red_flags = 0;
		if (flags & HFSC_ECN)
			red_flags |= REDF_ECN;
		if (m2 < 8)
			red_pkttime = 1000 * 1000 * 1000; /* 1 sec */
		else
			red_pkttime = (int64_t)hif->hif_ifq->altq_ifp->if_mtu
				* 1000 * 1000 * 1000 / (m2 / 8);
		if (flags & HFSC_RED) {
			cl->cl_red = red_alloc(0, 0,
			    qlimit(cl->cl_q) * 10/100,
			    qlimit(cl->cl_q) * 30/100,
			    red_flags, red_pkttime);
			qtype(cl->cl_q) = Q_RED;
		}
	}
#endif /* RED_NOTYET */
a383 6
#if RED_NOTYET
	if (cl->cl_red != NULL) {
		if (q_is_red(cl->cl_q))
			red_destroy(cl->cl_red);
	}
#endif
a435 7
#if RED_NOTYET
	if (cl->cl_red != NULL) {
		if (q_is_red(cl->cl_q))
			red_destroy(cl->cl_red);
	}
#endif

d573 1
a573 7
#if RED_NOTYET
	if (q_is_red(cl->cl_q))
		m= red_getq(cl->cl_red, cl->cl_q);
	else
#endif
		m = hfsc_getq(cl);
	if (m == NULL)
d621 1
a621 4
#if RED_NOTYET
	if (q_is_red(cl->cl_q))
		return red_addq(cl->cl_red, cl->cl_q, m, cl->cl_pktattr);
#endif
a1445 4
#ifdef RED_NOTYET
	if (q_is_red(cl->cl_q))
		red_getstats(cl->cl_red, &sp->red[0]);
#endif
@


1.13
log
@More malloc() -> mallocarray() in the kernel.

ok deraadt@@ tedu@@
@
text
@d542 1
a542 1
	IFQ_INC_LEN(ifq);
d626 1
a626 1
	IFQ_DEC_LEN(ifq);
d725 1
a725 1
		IFQ_DEC_LEN(cl->cl_hif->hif_ifq);
@


1.12
log
@Explicitly include <net/if_var.h> instead of pulling it in <net/if.h>.

ok mikeb@@, krw@@, bluhm@@, tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.11 2014/07/12 18:44:22 tedu Exp $	*/
d147 2
a148 1
	newtbl = malloc(howmany * sizeof(void *), M_DEVBUF, M_WAITOK | M_ZERO);
@


1.11
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.10 2014/06/30 12:47:23 pelikan Exp $	*/
d56 1
@


1.10
log
@Attach HFSC after it's been completely initialized.

This fixes the "integer divide fault trap" bug caused by the second malloc
skipping a beat and leaving the amount of allocated queues equal to zero.

tested by me, ok henning mikeb
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.9 2014/04/19 15:58:12 henning Exp $	*/
d153 1
a153 1
	free(old, M_DEVBUF);
d191 2
a192 2
	free(hif->hif_class_tbl, M_DEVBUF);
	free(hif, M_DEVBUF);
d1010 1
a1010 1
	free(head, M_DEVBUF);
d1110 1
a1110 1
	free(head, M_DEVBUF);
@


1.9
log
@now that if_snd is a proper ifqueue, this cast dies
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.7 2014/01/03 19:58:54 pelikan Exp $	*/
d167 1
a167 1
	hif = malloc(sizeof(struct hfsc_if), M_DEVBUF, M_WAITOK|M_ZERO);
d169 3
a173 3

	hif->hif_class_tbl = malloc(tblsize, M_DEVBUF, M_WAITOK | M_ZERO);
	hif->hif_allocated = HFSC_DEFAULT_CLASSES;
@


1.8
log
@Make the amount of classes limited by 64k, with automatic table growing.

The limit is because in places, queue IDs are being stored as u_int16_t's.

tested by me, discussed at length with (and ok) claudio henning
@
text
@d169 1
a169 1
	hif->hif_ifq = (struct ifqueue *)&ifp->if_snd; /* XXX cast temp */
@


1.7
log
@Purging a queue requires it to be non-empty, not empty.

ok millert
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.6 2014/01/03 12:48:58 pelikan Exp $	*/
d133 23
d160 3
d171 4
d191 1
d297 7
a303 2
	if (hif->hif_classes >= HFSC_MAX_CLASSES)
		return (NULL);
d375 1
a375 1
	i = qid % HFSC_MAX_CLASSES;
d379 1
a379 1
		for (i = 0; i < HFSC_MAX_CLASSES; i++)
d384 1
a384 1
		if (i == HFSC_MAX_CLASSES) {
d459 1
a459 1
	for (i = 0; i < HFSC_MAX_CLASSES; i++)
d1515 1
a1515 1
	i = chandle % HFSC_MAX_CLASSES;
d1518 1
a1518 1
	for (i = 0; i < HFSC_MAX_CLASSES; i++)
@


1.6
log
@Switch frequently allocated structs from malloc(M_DEVBUF) to separate pools.

ok henning, "looks fine" mikeb, input from guenther.
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.5 2014/01/01 17:46:43 pelikan Exp $	*/
d680 1
a680 1
	if (cl->cl_q->qlen > 0)
@


1.5
log
@Destroy the eligible list when detaching from an interface.

ok henning
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.4 2013/11/01 23:00:02 pelikan Exp $	*/
d47 1
d269 2
a270 3
	cl = malloc(sizeof(struct hfsc_class), M_DEVBUF, M_WAITOK|M_ZERO);
	cl->cl_q = malloc(sizeof(struct hfsc_classq), M_DEVBUF,
	    M_WAITOK|M_ZERO);
d310 1
a310 2
		cl->cl_rsc = malloc(sizeof(struct hfsc_internal_sc), M_DEVBUF,
		    M_WAITOK);
d316 1
a316 2
		cl->cl_fsc = malloc(sizeof(struct hfsc_internal_sc), M_DEVBUF,
		    M_WAITOK);
d321 1
a321 2
		cl->cl_usc = malloc(sizeof(struct hfsc_internal_sc), M_DEVBUF,
		    M_WAITOK);
d383 1
a383 1
		free(cl->cl_fsc, M_DEVBUF);
d385 1
a385 1
		free(cl->cl_rsc, M_DEVBUF);
d387 1
a387 1
		free(cl->cl_usc, M_DEVBUF);
d389 2
a390 2
		free(cl->cl_q, M_DEVBUF);
	free(cl, M_DEVBUF);
d447 1
a447 1
		free(cl->cl_usc, M_DEVBUF);
d449 1
a449 1
		free(cl->cl_fsc, M_DEVBUF);
d451 3
a453 3
		free(cl->cl_rsc, M_DEVBUF);
	free(cl->cl_q, M_DEVBUF);
	free(cl, M_DEVBUF);
@


1.4
log
@push the queues every 1/HZ using timeout(9)

This is a modified version of oldtbr_timeout() with a timeout for each
HFSC enabled interface.  We can now safely include <sys/timeout.h> in
net/hfsc.h without breaking the build.
tested by naddy, ok henning claudio deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.3 2013/10/31 13:19:17 pelikan Exp $	*/
d154 3
a156 2
	timeout_del(&ifp->if_snd.ifq_hfsc->hif_defer);
	free(ifp->if_snd.ifq_hfsc, M_DEVBUF);
d158 3
@


1.3
log
@revert previous, net/if.h exports bad things to userspace.
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.2 2013/10/31 08:52:44 pelikan Exp $	*/
d77 1
d144 3
d154 1
d565 1
d610 15
@


1.2
log
@push the queues every 1/HZ using timeout(9)

This is a modified version of oldtbr_timeout() with a timeout for each
HFSC enabled interface.
ok henning claudio
@
text
@d1 1
a1 1
/*	$OpenBSD: hfsc.c,v 1.1 2013/10/12 11:39:17 henning Exp $	*/
a76 1
void		 hfsc_deferred(void *);
a142 3
	timeout_set(&hif->hif_defer, hfsc_deferred, ifp);
	/* XXX HRTIMER don't schedule it yet, only when some packets wait. */
	timeout_add(&hif->hif_defer, 1);
a149 1
	timeout_del(&ifp->if_snd.ifq_hfsc->hif_defer);
a559 1
			/* XXX HRTIMER plan hfsc_deferred precisely here. */
a603 15
}

void
hfsc_deferred(void *arg)
{
	struct ifnet *ifp = arg;
	int s;

	s = splnet();
	if (HFSC_ENABLED(&ifp->if_snd) && !IFQ_IS_EMPTY(&ifp->if_snd))
		if_start(ifp);
	splx(s);

	/* XXX HRTIMER nearest virtual/fit time is likely less than 1/HZ. */
	timeout_add(&ifp->if_snd.ifq_hfsc->hif_defer, 1);
@


1.1
log
@standalone hfsc implementation with new interface to the consumers, for
the new bandwidth shaping subsystem. looked over & tested by many,
ok phessler sthen
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d77 1
d144 3
d154 1
d565 1
d610 15
@

