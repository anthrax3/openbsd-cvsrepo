head	1.76;
access;
symbols
	OPENBSD_6_1:1.75.0.4
	OPENBSD_6_1_BASE:1.75
	OPENBSD_6_0:1.61.0.4
	OPENBSD_6_0_BASE:1.61
	OPENBSD_5_9:1.60.0.2
	OPENBSD_5_9_BASE:1.60
	OPENBSD_5_8:1.55.0.4
	OPENBSD_5_8_BASE:1.55
	OPENBSD_5_7:1.49.0.2
	OPENBSD_5_7_BASE:1.49
	OPENBSD_5_6:1.45.0.4
	OPENBSD_5_6_BASE:1.45
	OPENBSD_5_5:1.40.0.4
	OPENBSD_5_5_BASE:1.40
	OPENBSD_5_4:1.32.0.2
	OPENBSD_5_4_BASE:1.32
	OPENBSD_5_3:1.24.0.2
	OPENBSD_5_3_BASE:1.24
	OPENBSD_5_2:1.20.0.2
	OPENBSD_5_2_BASE:1.20
	OPENBSD_5_1_BASE:1.19
	OPENBSD_5_1:1.19.0.2
	OPENBSD_5_0:1.17.0.2
	OPENBSD_5_0_BASE:1.17
	OPENBSD_4_9:1.14.0.4
	OPENBSD_4_9_BASE:1.14
	OPENBSD_4_8:1.14.0.2
	OPENBSD_4_8_BASE:1.14
	OPENBSD_4_7:1.12.0.2
	OPENBSD_4_7_BASE:1.12
	OPENBSD_4_6:1.11.0.4
	OPENBSD_4_6_BASE:1.11
	OPENBSD_4_5:1.10.0.2
	OPENBSD_4_5_BASE:1.10;
locks; strict;
comment	@ * @;


1.76
date	2017.05.15.12.26.00;	author mpi;	state Exp;
branches;
next	1.75;
commitid	WMZaI3vIHNC1J8ol;

1.75
date	2017.03.17.17.19.16;	author mpi;	state Exp;
branches;
next	1.74;
commitid	CxqvXOMqotM60GAI;

1.74
date	2017.02.16.10.15.12;	author mpi;	state Exp;
branches;
next	1.73;
commitid	NChVcK48FdZeU9j9;

1.73
date	2017.02.07.15.34.47;	author mpi;	state Exp;
branches;
next	1.72;
commitid	DIuaScNqLdgVcwao;

1.72
date	2017.01.25.06.15.50;	author mpi;	state Exp;
branches;
next	1.71;
commitid	X7Hk1efefaYrWlw3;

1.71
date	2017.01.24.10.08.30;	author krw;	state Exp;
branches;
next	1.70;
commitid	6c6qq5OdS4VVnyVM;

1.70
date	2017.01.23.11.37.29;	author mpi;	state Exp;
branches;
next	1.69;
commitid	F6oNrr9LCLUSAxgA;

1.69
date	2017.01.20.00.51.56;	author mpi;	state Exp;
branches;
next	1.68;
commitid	ASQosbhukUA3DGlR;

1.68
date	2017.01.18.08.48.06;	author florian;	state Exp;
branches;
next	1.67;
commitid	kjUva4gCPw3Ri9MT;

1.67
date	2017.01.18.08.47.00;	author florian;	state Exp;
branches;
next	1.66;
commitid	O8KN6T8oc1FzSWss;

1.66
date	2017.01.03.10.50.56;	author mpi;	state Exp;
branches;
next	1.65;
commitid	yCRXGgCedRtvLXfC;

1.65
date	2016.12.29.12.12.43;	author mpi;	state Exp;
branches;
next	1.64;
commitid	RhxGXGNe4WuNtTZs;

1.64
date	2016.12.21.12.28.49;	author mikeb;	state Exp;
branches;
next	1.63;
commitid	jqAX3uI7GdvIW70T;

1.63
date	2016.12.20.15.07.32;	author mpi;	state Exp;
branches;
next	1.62;
commitid	pKq8r2ycbWxnZX23;

1.62
date	2016.10.04.13.54.32;	author mpi;	state Exp;
branches;
next	1.61;
commitid	QtKIaSOt6xUiLgI5;

1.61
date	2016.04.29.08.55.03;	author krw;	state Exp;
branches;
next	1.60;
commitid	OXPWeCN3XhMKFRzT;

1.60
date	2015.10.03.10.44.23;	author florian;	state Exp;
branches;
next	1.59;
commitid	ZIIAgj3OLK4duGNy;

1.59
date	2015.09.12.22.07.47;	author florian;	state Exp;
branches;
next	1.58;
commitid	BifXRit4Cdg91FFX;

1.58
date	2015.09.09.16.02.31;	author florian;	state Exp;
branches;
next	1.57;
commitid	NtZVLZcghgWnmOsP;

1.57
date	2015.09.04.20.28.12;	author florian;	state Exp;
branches;
next	1.56;
commitid	bp86KRdCwnMUyWH1;

1.56
date	2015.09.04.08.17.06;	author mpi;	state Exp;
branches;
next	1.55;
commitid	Yy9LfcltEuawOlyO;

1.55
date	2015.07.21.03.03.10;	author florian;	state Exp;
branches;
next	1.54;
commitid	TmAZtklCMxpcxo9P;

1.54
date	2015.07.21.03.00.20;	author florian;	state Exp;
branches;
next	1.53;
commitid	LXPHRmcbxvztd335;

1.53
date	2015.07.20.23.15.54;	author florian;	state Exp;
branches;
next	1.52;
commitid	E5C8fGTRyuqmAfVd;

1.52
date	2015.07.16.18.36.59;	author florian;	state Exp;
branches;
next	1.51;
commitid	6lFU2sVLrgWm5rDq;

1.51
date	2015.06.16.11.09.39;	author mpi;	state Exp;
branches;
next	1.50;
commitid	h7z8lokZ0dFyuWpg;

1.50
date	2015.06.07.12.02.28;	author jsg;	state Exp;
branches;
next	1.49;
commitid	st7eUqjf7vKTD48M;

1.49
date	2014.12.19.17.14.39;	author tedu;	state Exp;
branches;
next	1.48;
commitid	zhW8jJrfVCoAthrR;

1.48
date	2014.11.20.14.51.42;	author krw;	state Exp;
branches;
next	1.47;
commitid	dOUqRDzYiPQXkCLL;

1.47
date	2014.11.18.02.37.31;	author tedu;	state Exp;
branches;
next	1.46;
commitid	Z1vcFtHO8wRH0yRt;

1.46
date	2014.08.13.09.46.23;	author blambert;	state Exp;
branches;
next	1.45;
commitid	dWK70fA38KC9bTHY;

1.45
date	2014.07.22.11.06.09;	author mpi;	state Exp;
branches;
next	1.44;
commitid	DQakU8LLWV6Iwx84;

1.44
date	2014.07.12.18.44.22;	author tedu;	state Exp;
branches;
next	1.43;
commitid	B4dZSbxas1X1IpXI;

1.43
date	2014.04.21.12.22.25;	author henning;	state Exp;
branches;
next	1.42;

1.42
date	2014.04.14.09.06.42;	author mpi;	state Exp;
branches;
next	1.41;

1.41
date	2014.03.29.11.26.03;	author florian;	state Exp;
branches;
next	1.40;

1.40
date	2014.01.24.09.48.37;	author henning;	state Exp;
branches;
next	1.39;

1.39
date	2014.01.21.21.27.14;	author benno;	state Exp;
branches;
next	1.38;

1.38
date	2013.11.01.14.34.27;	author florian;	state Exp;
branches;
next	1.37;

1.37
date	2013.10.19.10.49.31;	author henning;	state Exp;
branches;
next	1.36;

1.36
date	2013.10.17.16.27.41;	author bluhm;	state Exp;
branches;
next	1.35;

1.35
date	2013.09.13.14.30.47;	author florian;	state Exp;
branches;
next	1.34;

1.34
date	2013.08.13.08.44.05;	author florian;	state Exp;
branches;
next	1.33;

1.33
date	2013.08.10.18.33.21;	author florian;	state Exp;
branches;
next	1.32;

1.32
date	2013.07.05.17.14.27;	author florian;	state Exp;
branches
	1.32.2.1;
next	1.31;

1.31
date	2013.05.31.22.46.47;	author florian;	state Exp;
branches;
next	1.30;

1.30
date	2013.05.30.20.20.58;	author benno;	state Exp;
branches;
next	1.29;

1.29
date	2013.05.03.15.33.47;	author florian;	state Exp;
branches;
next	1.28;

1.28
date	2013.04.10.08.50.59;	author mpi;	state Exp;
branches;
next	1.27;

1.27
date	2013.03.28.23.10.05;	author tedu;	state Exp;
branches;
next	1.26;

1.26
date	2013.03.28.16.45.16;	author tedu;	state Exp;
branches;
next	1.25;

1.25
date	2013.03.26.13.19.25;	author mpi;	state Exp;
branches;
next	1.24;

1.24
date	2013.02.05.11.58.39;	author florian;	state Exp;
branches
	1.24.2.1;
next	1.23;

1.23
date	2013.01.16.09.53.19;	author dlg;	state Exp;
branches;
next	1.22;

1.22
date	2012.11.08.18.06.49;	author gsoares;	state Exp;
branches;
next	1.21;

1.21
date	2012.10.30.12.09.05;	author florian;	state Exp;
branches;
next	1.20;

1.20
date	2012.04.11.17.42.53;	author mikeb;	state Exp;
branches;
next	1.19;

1.19
date	2012.02.02.12.34.37;	author benno;	state Exp;
branches;
next	1.18;

1.18
date	2011.11.25.12.52.10;	author dlg;	state Exp;
branches;
next	1.17;

1.17
date	2011.07.09.04.11.15;	author dhill;	state Exp;
branches;
next	1.16;

1.16
date	2011.07.06.02.42.28;	author henning;	state Exp;
branches;
next	1.15;

1.15
date	2011.04.05.18.01.21;	author henning;	state Exp;
branches;
next	1.14;

1.14
date	2010.07.02.02.40.16;	author blambert;	state Exp;
branches;
next	1.13;

1.13
date	2010.04.20.22.05.43;	author tedu;	state Exp;
branches;
next	1.12;

1.12
date	2010.01.12.02.47.07;	author claudio;	state Exp;
branches;
next	1.11;

1.11
date	2009.06.17.06.35.30;	author gollo;	state Exp;
branches;
next	1.10;

1.10
date	2009.02.27.11.09.36;	author gollo;	state Exp;
branches;
next	1.9;

1.9
date	2009.01.03.21.47.32;	author gollo;	state Exp;
branches;
next	1.8;

1.8
date	2008.11.26.18.01.43;	author dlg;	state Exp;
branches;
next	1.7;

1.7
date	2008.10.28.15.51.27;	author gollo;	state Exp;
branches;
next	1.6;

1.6
date	2008.10.21.11.01.29;	author gollo;	state Exp;
branches;
next	1.5;

1.5
date	2008.09.17.22.18.00;	author gollo;	state Exp;
branches;
next	1.4;

1.4
date	2008.09.17.20.25.41;	author gollo;	state Exp;
branches;
next	1.3;

1.3
date	2008.09.16.15.48.12;	author gollo;	state Exp;
branches;
next	1.2;

1.2
date	2008.09.16.13.58.55;	author gollo;	state Exp;
branches;
next	1.1;

1.1
date	2008.09.09.13.56.39;	author henning;	state Exp;
branches;
next	;

1.24.2.1
date	2013.11.08.13.41.58;	author william;	state Exp;
branches;
next	;

1.32.2.1
date	2013.11.08.13.44.54;	author william;	state Exp;
branches;
next	;


desc
@@


1.76
log
@Enable the NET_LOCK(), take 3.

Recursions are still marked as XXXSMP.

ok deraadt@@, bluhm@@
@
text
@/*	$OpenBSD: if_pflow.c,v 1.75 2017/03/17 17:19:16 mpi Exp $	*/

/*
 * Copyright (c) 2011 Florian Obser <florian@@narrans.de>
 * Copyright (c) 2011 Sebastian Benoit <benoit-lists@@fb12.de>
 * Copyright (c) 2008 Henning Brauer <henning@@openbsd.org>
 * Copyright (c) 2008 Joerg Goltermann <jg@@osn.de>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF MIND, USE, DATA OR PROFITS, WHETHER IN
 * AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT
 * OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include <sys/types.h>
#include <sys/malloc.h>
#include <sys/param.h>
#include <sys/systm.h>
#include <sys/mbuf.h>
#include <sys/socket.h>
#include <sys/timeout.h>
#include <sys/ioctl.h>
#include <sys/kernel.h>
#include <sys/socket.h>
#include <sys/socketvar.h>
#include <sys/sysctl.h>

#include <net/if.h>
#include <net/if_types.h>
#include <net/bpf.h>
#include <net/route.h>
#include <netinet/in.h>
#include <netinet/if_ether.h>
#include <netinet/tcp.h>

#include <netinet/ip.h>
#include <netinet/ip_var.h>
#include <netinet/udp.h>
#include <netinet/udp_var.h>
#include <netinet/in_pcb.h>

#include <net/pfvar.h>
#include <net/if_pflow.h>

#include "bpfilter.h"
#include "pflow.h"

#define PFLOW_MINMTU	\
    (sizeof(struct pflow_header) + sizeof(struct pflow_flow))

#ifdef PFLOWDEBUG
#define DPRINTF(x)	do { printf x ; } while (0)
#else
#define DPRINTF(x)
#endif

SLIST_HEAD(, pflow_softc) pflowif_list;
struct pflowstats	 pflowstats;

void	pflowattach(int);
int	pflow_output(struct ifnet *ifp, struct mbuf *m, struct sockaddr *dst,
	struct rtentry *rt);
int	pflow_clone_create(struct if_clone *, int);
int	pflow_clone_destroy(struct ifnet *);
int	pflow_set(struct pflow_softc *, struct pflowreq *);
void	pflow_init_timeouts(struct pflow_softc *);
int	pflow_calc_mtu(struct pflow_softc *, int, int);
void	pflow_setmtu(struct pflow_softc *, int);
int	pflowvalidsockaddr(const struct sockaddr *, int);
int	pflowioctl(struct ifnet *, u_long, caddr_t);

struct mbuf	*pflow_get_mbuf(struct pflow_softc *, u_int16_t);
void	pflow_flush(struct pflow_softc *);
int	pflow_sendout_v5(struct pflow_softc *);
int	pflow_sendout_ipfix(struct pflow_softc *, sa_family_t);
int	pflow_sendout_ipfix_tmpl(struct pflow_softc *);
int	pflow_sendout_mbuf(struct pflow_softc *, struct mbuf *);
void	pflow_timeout(void *);
void	pflow_timeout6(void *);
void	pflow_timeout_tmpl(void *);
void	copy_flow_data(struct pflow_flow *, struct pflow_flow *,
	struct pf_state *, struct pf_state_key *, int, int);
void	copy_flow_ipfix_4_data(struct pflow_ipfix_flow4 *,
	struct pflow_ipfix_flow4 *, struct pf_state *, struct pf_state_key *,
	struct pflow_softc *, int, int);
void	copy_flow_ipfix_6_data(struct pflow_ipfix_flow6 *,
	struct pflow_ipfix_flow6 *, struct pf_state *, struct pf_state_key *,
	struct pflow_softc *, int, int);
int	pflow_pack_flow(struct pf_state *, struct pf_state_key *,
	struct pflow_softc *);
int	pflow_pack_flow_ipfix(struct pf_state *, struct pf_state_key *,
	struct pflow_softc *);
int	export_pflow_if(struct pf_state*, struct pf_state_key *,
	struct pflow_softc *);
int	copy_flow_to_m(struct pflow_flow *flow, struct pflow_softc *sc);
int	copy_flow_ipfix_4_to_m(struct pflow_ipfix_flow4 *flow,
	struct pflow_softc *sc);
int	copy_flow_ipfix_6_to_m(struct pflow_ipfix_flow6 *flow,
	struct pflow_softc *sc);

struct if_clone	pflow_cloner =
    IF_CLONE_INITIALIZER("pflow", pflow_clone_create,
    pflow_clone_destroy);

void
pflowattach(int npflow)
{
	SLIST_INIT(&pflowif_list);
	if_clone_attach(&pflow_cloner);
}

int
pflow_output(struct ifnet *ifp, struct mbuf *m, struct sockaddr *dst,
	struct rtentry *rt)
{
	m_freem(m);	/* drop packet */
	return (EAFNOSUPPORT);
}

int
pflow_clone_create(struct if_clone *ifc, int unit)
{
	struct ifnet		*ifp;
	struct pflow_softc	*pflowif;

	if ((pflowif = malloc(sizeof(*pflowif),
	    M_DEVBUF, M_NOWAIT|M_ZERO)) == NULL)
		return (ENOMEM);

	MGET(pflowif->send_nam, M_WAIT, MT_SONAME);

	pflowif->sc_version = PFLOW_PROTO_DEFAULT;

	/* ipfix template init */
	bzero(&pflowif->sc_tmpl_ipfix,sizeof(pflowif->sc_tmpl_ipfix));
	pflowif->sc_tmpl_ipfix.set_header.set_id =
	    htons(PFLOW_IPFIX_TMPL_SET_ID);
	pflowif->sc_tmpl_ipfix.set_header.set_length =
	    htons(sizeof(struct pflow_ipfix_tmpl));

	/* ipfix IPv4 template */
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.h.tmpl_id =
	    htons(PFLOW_IPFIX_TMPL_IPV4_ID);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.h.field_count
	    = htons(PFLOW_IPFIX_TMPL_IPV4_FIELD_COUNT);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.src_ip.field_id =
	    htons(PFIX_IE_sourceIPv4Address);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.src_ip.len = htons(4);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.dest_ip.field_id =
	    htons(PFIX_IE_destinationIPv4Address);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.dest_ip.len = htons(4);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.if_index_in.field_id =
	    htons(PFIX_IE_ingressInterface);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.if_index_in.len = htons(4);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.if_index_out.field_id =
	    htons(PFIX_IE_egressInterface);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.if_index_out.len = htons(4);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.packets.field_id =
	    htons(PFIX_IE_packetDeltaCount);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.packets.len = htons(8);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.octets.field_id =
	    htons(PFIX_IE_octetDeltaCount);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.octets.len = htons(8);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.start.field_id =
	    htons(PFIX_IE_flowStartMilliseconds);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.start.len = htons(8);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.finish.field_id =
	    htons(PFIX_IE_flowEndMilliseconds);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.finish.len = htons(8);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.src_port.field_id =
	    htons(PFIX_IE_sourceTransportPort);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.src_port.len = htons(2);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.dest_port.field_id =
	    htons(PFIX_IE_destinationTransportPort);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.dest_port.len = htons(2);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.tos.field_id =
	    htons(PFIX_IE_ipClassOfService);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.tos.len = htons(1);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.protocol.field_id =
	    htons(PFIX_IE_protocolIdentifier);
	pflowif->sc_tmpl_ipfix.ipv4_tmpl.protocol.len = htons(1);

	/* ipfix IPv6 template */
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.h.tmpl_id =
	    htons(PFLOW_IPFIX_TMPL_IPV6_ID);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.h.field_count =
	    htons(PFLOW_IPFIX_TMPL_IPV6_FIELD_COUNT);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.src_ip.field_id =
	    htons(PFIX_IE_sourceIPv6Address);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.src_ip.len = htons(16);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.dest_ip.field_id =
	    htons(PFIX_IE_destinationIPv6Address);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.dest_ip.len = htons(16);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.if_index_in.field_id =
	    htons(PFIX_IE_ingressInterface);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.if_index_in.len = htons(4);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.if_index_out.field_id =
	    htons(PFIX_IE_egressInterface);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.if_index_out.len = htons(4);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.packets.field_id =
	    htons(PFIX_IE_packetDeltaCount);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.packets.len = htons(8);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.octets.field_id =
	    htons(PFIX_IE_octetDeltaCount);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.octets.len = htons(8);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.start.field_id =
	    htons(PFIX_IE_flowStartMilliseconds);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.start.len = htons(8);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.finish.field_id =
	    htons(PFIX_IE_flowEndMilliseconds);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.finish.len = htons(8);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.src_port.field_id =
	    htons(PFIX_IE_sourceTransportPort);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.src_port.len = htons(2);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.dest_port.field_id =
	    htons(PFIX_IE_destinationTransportPort);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.dest_port.len = htons(2);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.tos.field_id =
	    htons(PFIX_IE_ipClassOfService);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.tos.len = htons(1);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.protocol.field_id =
	    htons(PFIX_IE_protocolIdentifier);
	pflowif->sc_tmpl_ipfix.ipv6_tmpl.protocol.len = htons(1);

	ifp = &pflowif->sc_if;
	snprintf(ifp->if_xname, sizeof ifp->if_xname, "pflow%d", unit);
	ifp->if_softc = pflowif;
	ifp->if_ioctl = pflowioctl;
	ifp->if_output = pflow_output;
	ifp->if_start = NULL;
	ifp->if_xflags = IFXF_CLONED;
	ifp->if_type = IFT_PFLOW;
	IFQ_SET_MAXLEN(&ifp->if_snd, IFQ_MAXLEN);
	ifp->if_hdrlen = PFLOW_HDRLEN;
	ifp->if_flags = IFF_UP;
	ifp->if_flags &= ~IFF_RUNNING;	/* not running, need receiver */
	pflow_setmtu(pflowif, ETHERMTU);
	pflow_init_timeouts(pflowif);
	if_attach(ifp);
	if_alloc_sadl(ifp);

	/* Insert into list of pflows */
	SLIST_INSERT_HEAD(&pflowif_list, pflowif, sc_next);
	return (0);
}

int
pflow_clone_destroy(struct ifnet *ifp)
{
	struct pflow_softc	*sc = ifp->if_softc;
	int			 s, error;

	error = 0;

	s = splnet();
	if (timeout_initialized(&sc->sc_tmo))
		timeout_del(&sc->sc_tmo);
	if (timeout_initialized(&sc->sc_tmo6))
		timeout_del(&sc->sc_tmo6);
	if (timeout_initialized(&sc->sc_tmo_tmpl))
		timeout_del(&sc->sc_tmo_tmpl);
	pflow_flush(sc);
	m_freem(sc->send_nam);
	if (sc->so != NULL) {
		error = soclose(sc->so);
		sc->so = NULL;
	}
	if (sc->sc_flowdst != NULL)
		free(sc->sc_flowdst, M_DEVBUF, sc->sc_flowdst->sa_len);
	if (sc->sc_flowsrc != NULL)
		free(sc->sc_flowsrc, M_DEVBUF, sc->sc_flowsrc->sa_len);
	if_detach(ifp);
	SLIST_REMOVE(&pflowif_list, sc, pflow_softc, sc_next);
	free(sc, M_DEVBUF, sizeof(*sc));
	splx(s);
	return (error);
}

int
pflowvalidsockaddr(const struct sockaddr *sa, int ignore_port)
{
	struct sockaddr_in6	*sin6;
	struct sockaddr_in	*sin;

	if (sa == NULL)
		return (0);
	switch(sa->sa_family) {
	case AF_INET:
		sin = (struct sockaddr_in*) sa;
		return (sin->sin_addr.s_addr != INADDR_ANY &&
		    (ignore_port || sin->sin_port != 0));
	case AF_INET6:
		sin6 = (struct sockaddr_in6*) sa;
		return (!IN6_IS_ADDR_UNSPECIFIED(&sin6->sin6_addr) &&
		    (ignore_port || sin6->sin6_port != 0));
	default:
		return (0);
	}
}

int
pflow_set(struct pflow_softc *sc, struct pflowreq *pflowr)
{
	struct proc		*p = curproc;
	struct socket		*so;
	struct sockaddr		*sa;
	int			 error = 0;

	if (pflowr->addrmask & PFLOW_MASK_VERSION) {
		switch(pflowr->version) {
		case PFLOW_PROTO_5:
		case PFLOW_PROTO_10:
			break;
		default:
			return(EINVAL);
		}
	}

	pflow_flush(sc);

	if (pflowr->addrmask & PFLOW_MASK_DSTIP) {
		if (sc->sc_flowdst != NULL &&
		    sc->sc_flowdst->sa_family != pflowr->flowdst.ss_family) {
			free(sc->sc_flowdst, M_DEVBUF, sc->sc_flowdst->sa_len);
			sc->sc_flowdst = NULL;
			if (sc->so != NULL) {
				soclose(sc->so);
				sc->so = NULL;
			}
		}

		switch (pflowr->flowdst.ss_family) {
		case AF_INET:
			if (sc->sc_flowdst == NULL) {
				if ((sc->sc_flowdst = malloc(
				    sizeof(struct sockaddr_in),
				    M_DEVBUF,  M_NOWAIT)) == NULL)
					return (ENOMEM);
			}
			memcpy(sc->sc_flowdst, &pflowr->flowdst,
			    sizeof(struct sockaddr_in));
			sc->sc_flowdst->sa_len = sizeof(struct
			    sockaddr_in);
			break;
		case AF_INET6:
			if (sc->sc_flowdst == NULL) {
				if ((sc->sc_flowdst = malloc(
				    sizeof(struct sockaddr_in6),
				    M_DEVBUF, M_NOWAIT)) == NULL)
					return (ENOMEM);
			}
			memcpy(sc->sc_flowdst, &pflowr->flowdst,
			    sizeof(struct sockaddr_in6));
			sc->sc_flowdst->sa_len = sizeof(struct
			    sockaddr_in6);
			break;
		default:
			break;
		}

		if (sc->sc_flowdst != NULL) {
			sc->send_nam->m_len = sc->sc_flowdst->sa_len;
			sa = mtod(sc->send_nam, struct sockaddr *);
			memcpy(sa, sc->sc_flowdst, sc->sc_flowdst->sa_len);
		}
	}

	if (pflowr->addrmask & PFLOW_MASK_SRCIP) {
		if (sc->sc_flowsrc != NULL)
			free(sc->sc_flowsrc, M_DEVBUF, sc->sc_flowsrc->sa_len);
		sc->sc_flowsrc = NULL;
		if (sc->so != NULL) {
			soclose(sc->so);
			sc->so = NULL;
		}
		switch(pflowr->flowsrc.ss_family) {
		case AF_INET:
			if ((sc->sc_flowsrc = malloc(
			    sizeof(struct sockaddr_in),
			    M_DEVBUF, M_NOWAIT)) == NULL)
				return (ENOMEM);
			memcpy(sc->sc_flowsrc, &pflowr->flowsrc,
			    sizeof(struct sockaddr_in));
			sc->sc_flowsrc->sa_len = sizeof(struct
			    sockaddr_in);
			break;
		case AF_INET6:
			if ((sc->sc_flowsrc = malloc(
			    sizeof(struct sockaddr_in6),
			    M_DEVBUF, M_NOWAIT)) == NULL)
				return (ENOMEM);
			memcpy(sc->sc_flowsrc, &pflowr->flowsrc,
			    sizeof(struct sockaddr_in6));
			sc->sc_flowsrc->sa_len = sizeof(struct
			    sockaddr_in6);
			break;
		default:
			break;
		}
	}

	if (sc->so == NULL) {
		if (pflowvalidsockaddr(sc->sc_flowdst, 0)) {
			error = socreate(sc->sc_flowdst->sa_family,
			    &so, SOCK_DGRAM, 0);
			if (error)
				return (error);
			if (pflowvalidsockaddr(sc->sc_flowsrc, 1)) {
				struct mbuf *m;

				MGET(m, M_WAIT, MT_SONAME);
				m->m_len = sc->sc_flowsrc->sa_len;
				sa = mtod(m, struct sockaddr *);
				memcpy(sa, sc->sc_flowsrc,
				    sc->sc_flowsrc->sa_len);

				error = sobind(so, m, p);
				m_freem(m);
				if (error) {
					soclose(so);
					return (error);
				}
			}
			sc->so = so;
		}
	} else if (!pflowvalidsockaddr(sc->sc_flowdst, 0)) {
		soclose(sc->so);
		sc->so = NULL;
	}

	/* error check is above */
	if (pflowr->addrmask & PFLOW_MASK_VERSION)
		sc->sc_version = pflowr->version;

	pflow_setmtu(sc, ETHERMTU);
	pflow_init_timeouts(sc);

	return (0);
}

int
pflowioctl(struct ifnet *ifp, u_long cmd, caddr_t data)
{
	struct proc		*p = curproc;
	struct pflow_softc	*sc = ifp->if_softc;
	struct ifreq		*ifr = (struct ifreq *)data;
	struct pflowreq		 pflowr;
	int			 s, error;

	switch (cmd) {
	case SIOCSIFADDR:
	case SIOCSIFDSTADDR:
	case SIOCSIFFLAGS:
		if ((ifp->if_flags & IFF_UP) && sc->so != NULL) {
			ifp->if_flags |= IFF_RUNNING;
			sc->sc_gcounter=pflowstats.pflow_flows;
			/* send templates on startup */
			if (sc->sc_version == PFLOW_PROTO_10) {
				/* XXXSMP breaks atomicity */
				rw_exit_write(&netlock);
				s = splnet();
				pflow_sendout_ipfix_tmpl(sc);
				splx(s);
				rw_enter_write(&netlock);
			}
		} else
			ifp->if_flags &= ~IFF_RUNNING;
		break;
	case SIOCSIFMTU:
		if (ifr->ifr_mtu < PFLOW_MINMTU)
			return (EINVAL);
		if (ifr->ifr_mtu > MCLBYTES)
			ifr->ifr_mtu = MCLBYTES;
		s = splnet();
		if (ifr->ifr_mtu < ifp->if_mtu)
			pflow_flush(sc);
		pflow_setmtu(sc, ifr->ifr_mtu);
		splx(s);
		break;

	case SIOCGETPFLOW:
		bzero(&pflowr, sizeof(pflowr));

		if (sc->sc_flowsrc != NULL)
			memcpy(&pflowr.flowsrc, sc->sc_flowsrc,
			    sc->sc_flowsrc->sa_len);
		if (sc->sc_flowdst != NULL)
			memcpy(&pflowr.flowdst, sc->sc_flowdst,
			    sc->sc_flowdst->sa_len);
		pflowr.version = sc->sc_version;

		if ((error = copyout(&pflowr, ifr->ifr_data,
		    sizeof(pflowr))))
			return (error);
		break;

	case SIOCSETPFLOW:
		if ((error = suser(p, 0)) != 0)
			return (error);
		if ((error = copyin(ifr->ifr_data, &pflowr,
		    sizeof(pflowr))))
			return (error);

		/* XXXSMP breaks atomicity */
		rw_exit_write(&netlock);
		s = splnet();
		error = pflow_set(sc, &pflowr);
		splx(s);
		if (error != 0)
			return (error);

		if ((ifp->if_flags & IFF_UP) && sc->so != NULL) {
			ifp->if_flags |= IFF_RUNNING;
			sc->sc_gcounter=pflowstats.pflow_flows;
			if (sc->sc_version == PFLOW_PROTO_10) {
				s = splnet();
				pflow_sendout_ipfix_tmpl(sc);
				splx(s);
			}
		} else
			ifp->if_flags &= ~IFF_RUNNING;

		rw_enter_write(&netlock);
		break;

	default:
		return (ENOTTY);
	}
	return (0);
}

void
pflow_init_timeouts(struct pflow_softc *sc)
{
	switch (sc->sc_version) {
	case PFLOW_PROTO_5:
		if (timeout_initialized(&sc->sc_tmo6))
			timeout_del(&sc->sc_tmo6);
		if (timeout_initialized(&sc->sc_tmo_tmpl))
			timeout_del(&sc->sc_tmo_tmpl);
		if (!timeout_initialized(&sc->sc_tmo))
			timeout_set_proc(&sc->sc_tmo, pflow_timeout, sc);
		break;
	case PFLOW_PROTO_10:
		if (!timeout_initialized(&sc->sc_tmo_tmpl))
			timeout_set_proc(&sc->sc_tmo_tmpl, pflow_timeout_tmpl,
			    sc);
		if (!timeout_initialized(&sc->sc_tmo))
			timeout_set_proc(&sc->sc_tmo, pflow_timeout, sc);
		if (!timeout_initialized(&sc->sc_tmo6))
			timeout_set_proc(&sc->sc_tmo6, pflow_timeout6, sc);

		timeout_add_sec(&sc->sc_tmo_tmpl, PFLOW_TMPL_TIMEOUT);
		break;
	default: /* NOTREACHED */
		break;
	}
}

int
pflow_calc_mtu(struct pflow_softc *sc, int mtu, int hdrsz)
{

	sc->sc_maxcount4 = (mtu - hdrsz -
	    sizeof(struct udpiphdr)) / sizeof(struct pflow_ipfix_flow4);
	sc->sc_maxcount6 = (mtu - hdrsz -
	    sizeof(struct udpiphdr)) / sizeof(struct pflow_ipfix_flow6);
	if (sc->sc_maxcount4 > PFLOW_MAXFLOWS)
		sc->sc_maxcount4 = PFLOW_MAXFLOWS;
	if (sc->sc_maxcount6 > PFLOW_MAXFLOWS)
		sc->sc_maxcount6 = PFLOW_MAXFLOWS;
	return (hdrsz + sizeof(struct udpiphdr) +
	    MIN(sc->sc_maxcount4 * sizeof(struct pflow_ipfix_flow4),
	    sc->sc_maxcount6 * sizeof(struct pflow_ipfix_flow6)));
}

void
pflow_setmtu(struct pflow_softc *sc, int mtu_req)
{
	int	mtu;

	if (sc->sc_pflow_ifp && sc->sc_pflow_ifp->if_mtu < mtu_req)
		mtu = sc->sc_pflow_ifp->if_mtu;
	else
		mtu = mtu_req;

	switch (sc->sc_version) {
	case PFLOW_PROTO_5:
		sc->sc_maxcount = (mtu - sizeof(struct pflow_header) -
		    sizeof(struct udpiphdr)) / sizeof(struct pflow_flow);
		if (sc->sc_maxcount > PFLOW_MAXFLOWS)
		    sc->sc_maxcount = PFLOW_MAXFLOWS;
		sc->sc_if.if_mtu = sizeof(struct pflow_header) +
		    sizeof(struct udpiphdr) +
		    sc->sc_maxcount * sizeof(struct pflow_flow);
		break;
	case PFLOW_PROTO_10:
		sc->sc_if.if_mtu =
		    pflow_calc_mtu(sc, mtu, sizeof(struct pflow_v10_header));
		break;
	default: /* NOTREACHED */
		break;
	}
}

struct mbuf *
pflow_get_mbuf(struct pflow_softc *sc, u_int16_t set_id)
{
	struct pflow_set_header	 set_hdr;
	struct pflow_header	 h;
	struct mbuf		*m;

	MGETHDR(m, M_DONTWAIT, MT_DATA);
	if (m == NULL) {
		pflowstats.pflow_onomem++;
		return (NULL);
	}

	MCLGET(m, M_DONTWAIT);
	if ((m->m_flags & M_EXT) == 0) {
		m_free(m);
		pflowstats.pflow_onomem++;
		return (NULL);
	}

	m->m_len = m->m_pkthdr.len = 0;
	m->m_pkthdr.ph_ifidx = 0;

	if (sc == NULL)		/* get only a new empty mbuf */
		return (m);

	if (sc->sc_version == PFLOW_PROTO_5) {
		/* populate pflow_header */
		h.reserved1 = 0;
		h.reserved2 = 0;
		h.count = 0;
		h.version = htons(PFLOW_PROTO_5);
		h.flow_sequence = htonl(sc->sc_gcounter);
		h.engine_type = PFLOW_ENGINE_TYPE;
		h.engine_id = PFLOW_ENGINE_ID;
		m_copyback(m, 0, PFLOW_HDRLEN, &h, M_NOWAIT);

		sc->sc_count = 0;
		timeout_add_sec(&sc->sc_tmo, PFLOW_TIMEOUT);
	} else {
		/* populate pflow_set_header */
		set_hdr.set_length = 0;
		set_hdr.set_id = htons(set_id);
		m_copyback(m, 0, PFLOW_SET_HDRLEN, &set_hdr, M_NOWAIT);
	}

	return (m);
}

void
copy_flow_data(struct pflow_flow *flow1, struct pflow_flow *flow2,
    struct pf_state *st, struct pf_state_key *sk, int src, int dst)
{
	flow1->src_ip = flow2->dest_ip = sk->addr[src].v4.s_addr;
	flow1->src_port = flow2->dest_port = sk->port[src];
	flow1->dest_ip = flow2->src_ip = sk->addr[dst].v4.s_addr;
	flow1->dest_port = flow2->src_port = sk->port[dst];

	flow1->dest_as = flow2->src_as =
	    flow1->src_as = flow2->dest_as = 0;
	flow1->if_index_in = htons(st->if_index_in);
	flow1->if_index_out = htons(st->if_index_out);
	flow2->if_index_in = htons(st->if_index_out);
	flow2->if_index_out = htons(st->if_index_in);
	flow1->dest_mask = flow2->src_mask =
	    flow1->src_mask = flow2->dest_mask = 0;

	flow1->flow_packets = htonl(st->packets[0]);
	flow2->flow_packets = htonl(st->packets[1]);
	flow1->flow_octets = htonl(st->bytes[0]);
	flow2->flow_octets = htonl(st->bytes[1]);

	/*
	 * Pretend the flow was created or expired when the machine came up
	 * when creation is in the future of the last time a package was seen
	 * or was created / expired before this machine came up due to pfsync.
	 */
	flow1->flow_start = flow2->flow_start = st->creation < 0 ||
	    st->creation > st->expire ? htonl(0) : htonl(st->creation * 1000);
	flow1->flow_finish = flow2->flow_finish = st->expire < 0 ? htonl(0) :
	    htonl(st->expire * 1000);
	flow1->tcp_flags = flow2->tcp_flags = 0;
	flow1->protocol = flow2->protocol = sk->proto;
	flow1->tos = flow2->tos = st->rule.ptr->tos;
}

void
copy_flow_ipfix_4_data(struct pflow_ipfix_flow4 *flow1,
    struct pflow_ipfix_flow4 *flow2, struct pf_state *st,
    struct pf_state_key *sk, struct pflow_softc *sc, int src, int dst)
{
	flow1->src_ip = flow2->dest_ip = sk->addr[src].v4.s_addr;
	flow1->src_port = flow2->dest_port = sk->port[src];
	flow1->dest_ip = flow2->src_ip = sk->addr[dst].v4.s_addr;
	flow1->dest_port = flow2->src_port = sk->port[dst];

	flow1->if_index_in = htonl(st->if_index_in);
	flow1->if_index_out = htonl(st->if_index_out);
	flow2->if_index_in = htonl(st->if_index_out);
	flow2->if_index_out = htonl(st->if_index_in);

	flow1->flow_packets = htobe64(st->packets[0]);
	flow2->flow_packets = htobe64(st->packets[1]);
	flow1->flow_octets = htobe64(st->bytes[0]);
	flow2->flow_octets = htobe64(st->bytes[1]);

	/*
	 * Pretend the flow was created when the machine came up when creation
	 * is in the future of the last time a package was seen due to pfsync.
	 */
	if (st->creation > st->expire)
		flow1->flow_start = flow2->flow_start = htobe64((time_second -
		    time_uptime)*1000);
	else
		flow1->flow_start = flow2->flow_start = htobe64((time_second -
		    (time_uptime - st->creation))*1000);
	flow1->flow_finish = flow2->flow_finish = htobe64((time_second -
	    (time_uptime - st->expire))*1000);

	flow1->protocol = flow2->protocol = sk->proto;
	flow1->tos = flow2->tos = st->rule.ptr->tos;
}

void
copy_flow_ipfix_6_data(struct pflow_ipfix_flow6 *flow1,
    struct pflow_ipfix_flow6 *flow2, struct pf_state *st,
    struct pf_state_key *sk, struct pflow_softc *sc, int src, int dst)
{
	bcopy(&sk->addr[src].v6, &flow1->src_ip, sizeof(flow1->src_ip));
	bcopy(&sk->addr[src].v6, &flow2->dest_ip, sizeof(flow2->dest_ip));
	flow1->src_port = flow2->dest_port = sk->port[src];
	bcopy(&sk->addr[dst].v6, &flow1->dest_ip, sizeof(flow1->dest_ip));
	bcopy(&sk->addr[dst].v6, &flow2->src_ip, sizeof(flow2->src_ip));
	flow1->dest_port = flow2->src_port = sk->port[dst];

	flow1->if_index_in = htonl(st->if_index_in);
	flow1->if_index_out = htonl(st->if_index_out);
	flow2->if_index_in = htonl(st->if_index_out);
	flow2->if_index_out = htonl(st->if_index_in);

	flow1->flow_packets = htobe64(st->packets[0]);
	flow2->flow_packets = htobe64(st->packets[1]);
	flow1->flow_octets = htobe64(st->bytes[0]);
	flow2->flow_octets = htobe64(st->bytes[1]);

	/*
	 * Pretend the flow was created when the machine came up when creation
	 * is in the future of the last time a package was seen due to pfsync.
	 */
	if (st->creation > st->expire)
		flow1->flow_start = flow2->flow_start = htobe64((time_second -
		    time_uptime)*1000);
	else
		flow1->flow_start = flow2->flow_start = htobe64((time_second -
		    (time_uptime - st->creation))*1000);
	flow1->flow_finish = flow2->flow_finish = htobe64((time_second -
	    (time_uptime - st->expire))*1000);

	flow1->protocol = flow2->protocol = sk->proto;
	flow1->tos = flow2->tos = st->rule.ptr->tos;
}

int
export_pflow(struct pf_state *st)
{
	struct pflow_softc	*sc = NULL;
	struct pf_state_key	*sk;

	sk = st->key[st->direction == PF_IN ? PF_SK_WIRE : PF_SK_STACK];

	SLIST_FOREACH(sc, &pflowif_list, sc_next) {
		switch (sc->sc_version) {
		case PFLOW_PROTO_5:
			if( sk->af == AF_INET )
				export_pflow_if(st, sk, sc);
			break;
		case PFLOW_PROTO_10:
			if( sk->af == AF_INET || sk->af == AF_INET6 )
				export_pflow_if(st, sk, sc);
			break;
		default: /* NOTREACHED */
			break;
		}
	}

	return (0);
}

int
export_pflow_if(struct pf_state *st, struct pf_state_key *sk,
    struct pflow_softc *sc)
{
	struct pf_state		 pfs_copy;
	struct ifnet		*ifp = &sc->sc_if;
	u_int64_t		 bytes[2];
	int			 ret = 0;

	if (!(ifp->if_flags & IFF_RUNNING))
		return (0);

	if (sc->sc_version == PFLOW_PROTO_10)
		return (pflow_pack_flow_ipfix(st, sk, sc));

	/* PFLOW_PROTO_5 */
	if ((st->bytes[0] < (u_int64_t)PFLOW_MAXBYTES)
	    && (st->bytes[1] < (u_int64_t)PFLOW_MAXBYTES))
		return (pflow_pack_flow(st, sk, sc));

	/* flow > PFLOW_MAXBYTES need special handling */
	bcopy(st, &pfs_copy, sizeof(pfs_copy));
	bytes[0] = pfs_copy.bytes[0];
	bytes[1] = pfs_copy.bytes[1];

	while (bytes[0] > PFLOW_MAXBYTES) {
		pfs_copy.bytes[0] = PFLOW_MAXBYTES;
		pfs_copy.bytes[1] = 0;

		if ((ret = pflow_pack_flow(&pfs_copy, sk, sc)) != 0)
			return (ret);
		if ((bytes[0] - PFLOW_MAXBYTES) > 0)
			bytes[0] -= PFLOW_MAXBYTES;
	}

	while (bytes[1] > (u_int64_t)PFLOW_MAXBYTES) {
		pfs_copy.bytes[1] = PFLOW_MAXBYTES;
		pfs_copy.bytes[0] = 0;

		if ((ret = pflow_pack_flow(&pfs_copy, sk, sc)) != 0)
			return (ret);
		if ((bytes[1] - PFLOW_MAXBYTES) > 0)
			bytes[1] -= PFLOW_MAXBYTES;
	}

	pfs_copy.bytes[0] = bytes[0];
	pfs_copy.bytes[1] = bytes[1];

	return (pflow_pack_flow(&pfs_copy, sk, sc));
}

int
copy_flow_to_m(struct pflow_flow *flow, struct pflow_softc *sc)
{
	int		s, ret = 0;

	s = splnet();
	if (sc->sc_mbuf == NULL) {
		if ((sc->sc_mbuf = pflow_get_mbuf(sc, 0)) == NULL) {
			splx(s);
			return (ENOBUFS);
		}
	}
	m_copyback(sc->sc_mbuf, PFLOW_HDRLEN +
	    (sc->sc_count * sizeof(struct pflow_flow)),
	    sizeof(struct pflow_flow), flow, M_NOWAIT);

	if (pflowstats.pflow_flows == sc->sc_gcounter)
		pflowstats.pflow_flows++;
	sc->sc_gcounter++;
	sc->sc_count++;

	if (sc->sc_count >= sc->sc_maxcount)
		ret = pflow_sendout_v5(sc);

	splx(s);
	return(ret);
}

int
copy_flow_ipfix_4_to_m(struct pflow_ipfix_flow4 *flow, struct pflow_softc *sc)
{
	int		s, ret = 0;

	s = splnet();
	if (sc->sc_mbuf == NULL) {
		if ((sc->sc_mbuf =
		    pflow_get_mbuf(sc, PFLOW_IPFIX_TMPL_IPV4_ID)) == NULL) {
			splx(s);
			return (ENOBUFS);
		}
		sc->sc_count4 = 0;
		timeout_add_sec(&sc->sc_tmo, PFLOW_TIMEOUT);
	}
	m_copyback(sc->sc_mbuf, PFLOW_SET_HDRLEN +
	    (sc->sc_count4 * sizeof(struct pflow_ipfix_flow4)),
	    sizeof(struct pflow_ipfix_flow4), flow, M_NOWAIT);

	if (pflowstats.pflow_flows == sc->sc_gcounter)
		pflowstats.pflow_flows++;
	sc->sc_gcounter++;
	sc->sc_count4++;

	if (sc->sc_count4 >= sc->sc_maxcount4)
		ret = pflow_sendout_ipfix(sc, AF_INET);
	splx(s);
	return(ret);
}

int
copy_flow_ipfix_6_to_m(struct pflow_ipfix_flow6 *flow, struct pflow_softc *sc)
{
	int		s, ret = 0;

	s = splnet();
	if (sc->sc_mbuf6 == NULL) {
		if ((sc->sc_mbuf6 =
		    pflow_get_mbuf(sc, PFLOW_IPFIX_TMPL_IPV6_ID)) == NULL) {
			splx(s);
			return (ENOBUFS);
		}
		sc->sc_count6 = 0;
		timeout_add_sec(&sc->sc_tmo6, PFLOW_TIMEOUT);
	}
	m_copyback(sc->sc_mbuf6, PFLOW_SET_HDRLEN +
	    (sc->sc_count6 * sizeof(struct pflow_ipfix_flow6)),
	    sizeof(struct pflow_ipfix_flow6), flow, M_NOWAIT);

	if (pflowstats.pflow_flows == sc->sc_gcounter)
		pflowstats.pflow_flows++;
	sc->sc_gcounter++;
	sc->sc_count6++;

	if (sc->sc_count6 >= sc->sc_maxcount6)
		ret = pflow_sendout_ipfix(sc, AF_INET6);

	splx(s);
	return(ret);
}

int
pflow_pack_flow(struct pf_state *st, struct pf_state_key *sk,
    struct pflow_softc *sc)
{
	struct pflow_flow	 flow1;
	struct pflow_flow	 flow2;
	int			 ret = 0;

	bzero(&flow1, sizeof(flow1));
	bzero(&flow2, sizeof(flow2));

	if (st->direction == PF_OUT)
		copy_flow_data(&flow1, &flow2, st, sk, 1, 0);
	else
		copy_flow_data(&flow1, &flow2, st, sk, 0, 1);

	if (st->bytes[0] != 0) /* first flow from state */
		ret = copy_flow_to_m(&flow1, sc);

	if (st->bytes[1] != 0) /* second flow from state */
		ret = copy_flow_to_m(&flow2, sc);

	return (ret);
}

int
pflow_pack_flow_ipfix(struct pf_state *st, struct pf_state_key *sk,
    struct pflow_softc *sc)
{
	struct pflow_ipfix_flow4	 flow4_1, flow4_2;
	struct pflow_ipfix_flow6	 flow6_1, flow6_2;
	int				 ret = 0;
	if (sk->af == AF_INET) {
		bzero(&flow4_1, sizeof(flow4_1));
		bzero(&flow4_2, sizeof(flow4_2));

		if (st->direction == PF_OUT)
			copy_flow_ipfix_4_data(&flow4_1, &flow4_2, st, sk, sc,
			    1, 0);
		else
			copy_flow_ipfix_4_data(&flow4_1, &flow4_2, st, sk, sc,
			    0, 1);

		if (st->bytes[0] != 0) /* first flow from state */
			ret = copy_flow_ipfix_4_to_m(&flow4_1, sc);

		if (st->bytes[1] != 0) /* second flow from state */
			ret = copy_flow_ipfix_4_to_m(&flow4_2, sc);
	} else if (sk->af == AF_INET6) {
		bzero(&flow6_1, sizeof(flow6_1));
		bzero(&flow6_2, sizeof(flow6_2));

		if (st->direction == PF_OUT)
			copy_flow_ipfix_6_data(&flow6_1, &flow6_2, st, sk, sc,
			    1, 0);
		else
			copy_flow_ipfix_6_data(&flow6_1, &flow6_2, st, sk, sc,
			    0, 1);

		if (st->bytes[0] != 0) /* first flow from state */
			ret = copy_flow_ipfix_6_to_m(&flow6_1, sc);

		if (st->bytes[1] != 0) /* second flow from state */
			ret = copy_flow_ipfix_6_to_m(&flow6_2, sc);
	}
	return (ret);
}

void
pflow_timeout(void *v)
{
	struct pflow_softc	*sc = v;
	int			 s;

	s = splnet();
	switch (sc->sc_version) {
	case PFLOW_PROTO_5:
		pflow_sendout_v5(sc);
		break;
	case PFLOW_PROTO_10:
		pflow_sendout_ipfix(sc, AF_INET);
		break;
	default: /* NOTREACHED */
		break;
	}
	splx(s);
}

void
pflow_timeout6(void *v)
{
	struct pflow_softc	*sc = v;
	int			 s;

	s = splnet();
	pflow_sendout_ipfix(sc, AF_INET6);
	splx(s);
}

void
pflow_timeout_tmpl(void *v)
{
	struct pflow_softc	*sc = v;
	int			 s;

	s = splnet();
	pflow_sendout_ipfix_tmpl(sc);
	splx(s);
}

/* This must be called in splnet() */
void
pflow_flush(struct pflow_softc *sc)
{
	switch (sc->sc_version) {
	case PFLOW_PROTO_5:
		pflow_sendout_v5(sc);
		break;
	case PFLOW_PROTO_10:
		pflow_sendout_ipfix(sc, AF_INET);
		pflow_sendout_ipfix(sc, AF_INET6);
		break;
	default: /* NOTREACHED */
		break;
	}
}


/* This must be called in splnet() */
int
pflow_sendout_v5(struct pflow_softc *sc)
{
	struct mbuf		*m = sc->sc_mbuf;
	struct pflow_header	*h;
	struct ifnet		*ifp = &sc->sc_if;
	struct timespec		tv;

	timeout_del(&sc->sc_tmo);

	if (m == NULL)
		return (0);

	sc->sc_mbuf = NULL;
	if (!(ifp->if_flags & IFF_RUNNING)) {
		m_freem(m);
		return (0);
	}

	pflowstats.pflow_packets++;
	h = mtod(m, struct pflow_header *);
	h->count = htons(sc->sc_count);

	/* populate pflow_header */
	h->uptime_ms = htonl(time_uptime * 1000);

	getnanotime(&tv);
	h->time_sec = htonl(tv.tv_sec);			/* XXX 2038 */
	h->time_nanosec = htonl(tv.tv_nsec);

	return (pflow_sendout_mbuf(sc, m));
}

/* This must be called in splnet() */
int
pflow_sendout_ipfix(struct pflow_softc *sc, sa_family_t af)
{
	struct mbuf			*m;
	struct pflow_v10_header		*h10;
	struct pflow_set_header		*set_hdr;
	struct ifnet			*ifp = &sc->sc_if;
	u_int32_t			 count;
	int				 set_length;

	switch (af) {
	case AF_INET:
		m = sc->sc_mbuf;
		timeout_del(&sc->sc_tmo);
		if (m == NULL)
			return (0);
		sc->sc_mbuf = NULL;
		count = sc->sc_count4;
		set_length = sizeof(struct pflow_set_header)
		    + sc->sc_count4 * sizeof(struct pflow_ipfix_flow4);
		break;
	case AF_INET6:
		m = sc->sc_mbuf6;
		timeout_del(&sc->sc_tmo6);
		if (m == NULL)
			return (0);
		sc->sc_mbuf6 = NULL;
		count = sc->sc_count6;
		set_length = sizeof(struct pflow_set_header)
		    + sc->sc_count6 * sizeof(struct pflow_ipfix_flow6);
		break;
	default:
		unhandled_af(af);
	}

	if (!(ifp->if_flags & IFF_RUNNING)) {
		m_freem(m);
		return (0);
	}

	pflowstats.pflow_packets++;
	set_hdr = mtod(m, struct pflow_set_header *);
	set_hdr->set_length = htons(set_length);

	/* populate pflow_header */
	M_PREPEND(m, sizeof(struct pflow_v10_header), M_DONTWAIT);
	if (m == NULL) {
		pflowstats.pflow_onomem++;
		return (ENOBUFS);
	}
	h10 = mtod(m, struct pflow_v10_header *);
	h10->version = htons(PFLOW_PROTO_10);
	h10->length = htons(PFLOW_IPFIX_HDRLEN + set_length);
	h10->time_sec = htonl(time_second);		/* XXX 2038 */
	h10->flow_sequence = htonl(sc->sc_sequence);
	sc->sc_sequence += count;
	h10->observation_dom = htonl(PFLOW_ENGINE_TYPE);
	return (pflow_sendout_mbuf(sc, m));
}

/* This must be called in splnet() */
int
pflow_sendout_ipfix_tmpl(struct pflow_softc *sc)
{
	struct mbuf			*m;
	struct pflow_v10_header		*h10;
	struct ifnet			*ifp = &sc->sc_if;

	timeout_del(&sc->sc_tmo_tmpl);

	if (!(ifp->if_flags & IFF_RUNNING)) {
		return (0);
	}
	m = pflow_get_mbuf(NULL, 0);
	if (m == NULL)
		return (0);
	if (m_copyback(m, 0, sizeof(struct pflow_ipfix_tmpl),
	    &sc->sc_tmpl_ipfix, M_NOWAIT)) {
		m_freem(m);
		return (0);
	}
	pflowstats.pflow_packets++;

	/* populate pflow_header */
	M_PREPEND(m, sizeof(struct pflow_v10_header), M_DONTWAIT);
	if (m == NULL) {
		pflowstats.pflow_onomem++;
		return (ENOBUFS);
	}
	h10 = mtod(m, struct pflow_v10_header *);
	h10->version = htons(PFLOW_PROTO_10);
	h10->length = htons(PFLOW_IPFIX_HDRLEN + sizeof(struct
	    pflow_ipfix_tmpl));
	h10->time_sec = htonl(time_second);		/* XXX 2038 */
	h10->flow_sequence = htonl(sc->sc_sequence);
	h10->observation_dom = htonl(PFLOW_ENGINE_TYPE);

	timeout_add_sec(&sc->sc_tmo_tmpl, PFLOW_TMPL_TIMEOUT);
	return (pflow_sendout_mbuf(sc, m));
}

int
pflow_sendout_mbuf(struct pflow_softc *sc, struct mbuf *m)
{
	sc->sc_if.if_opackets++;
	sc->sc_if.if_obytes += m->m_pkthdr.len;

	if (sc->so == NULL) {
		m_freem(m);
		return (EINVAL);
	}
	return (sosend(sc->so, sc->send_nam, NULL, m, NULL, 0));
}

int
pflow_sysctl(int *name, u_int namelen, void *oldp, size_t *oldlenp,
    void *newp, size_t newlen)
{
	if (namelen != 1)
		return (ENOTDIR);

	switch (name[0]) {
	case NET_PFLOW_STATS:
		if (newp != NULL)
			return (EPERM);
		return (sysctl_struct(oldp, oldlenp, newp, newlen,
		    &pflowstats, sizeof(pflowstats)));
	default:
		return (EOPNOTSUPP);
	}
	return (0);
}
@


1.75
log
@Revert the NET_LOCK() and bring back pf's contention lock for release.

For the moment the NET_LOCK() is always taken by threads running under
KERNEL_LOCK().  That means it doesn't buy us anything except a possible
deadlock that we did not spot.  So make sure this doesn't happen, we'll
have plenty of time in the next release cycle to stress test it.

ok visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.74 2017/02/16 10:15:12 mpi Exp $	*/
d466 2
d471 1
d511 2
d530 1
@


1.74
log
@Revert "Release the NET_LOCK() before entering per-driver ioctl() routine".

This is most likely to be the cause of the deadlock seen by port builders
since it's the only changed that happened after a2k17.

Instead bring back pirofti@@ original hack to release the NET_LOCK() inside
iwm(4) and iwn(4).

This fixes some splassert reported by bluhm@@

Deadlock reported by naddy@@ and rpe@@ and ajacoutot@@ confirmed the deadlock
has been introduced post a2k17.

Tested by and ok tb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.73 2017/02/07 15:34:47 mpi Exp $	*/
a465 2
				/* XXXSMP breaks atomicity */
				rw_exit_write(&netlock);
a468 1
				rw_enter_write(&netlock);
a507 2
		/* XXXSMP breaks atomicity */
		rw_exit_write(&netlock);
a524 1
		rw_enter_write(&netlock);
@


1.73
log
@No longer need to unlock the netlock since the upper layer is doing it.

Found by Hrvoje Popovski.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.72 2017/01/25 06:15:50 mpi Exp $	*/
d511 2
d530 1
@


1.72
log
@Enable the NET_LOCK(), take 2.

Recursions are currently known and marked a XXXSMP.

Please report any assert to bugs@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.71 2017/01/24 10:08:30 krw Exp $	*/
a510 2
		/* XXXSMP breaks atomicity */
		rw_exit_write(&netlock);
a527 1
		rw_enter_write(&netlock);
@


1.71
log
@A space here, a space there. Soon we're talking real whitespace
rectification.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.70 2017/01/23 11:37:29 mpi Exp $	*/
d466 2
d471 1
d511 2
d530 1
@


1.70
log
@Flag pseudo-interfaces as such in order to call add_net_randomness()
only once per packet.

Fix a regression introduced when if_input() started to be called by
every pseudo-driver.

ok claudio@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.69 2017/01/20 00:51:56 mpi Exp $	*/
d595 1
a595 1
		    sizeof(struct udpiphdr) + 
d599 1
a599 1
		sc->sc_if.if_mtu = 
@


1.69
log
@No need to handle SIOCAIFADDR in drivers, it's never passed down to
them.

ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.68 2017/01/18 08:48:06 florian Exp $	*/
d238 1
@


1.68
log
@Allow changing of sender ip/port without switching address family.
With this regress tests pass again.
OK benno
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.67 2017/01/18 08:47:00 florian Exp $	*/
a457 1
	case SIOCAIFADDR:
@


1.67
log
@Allow changing of receiver ip/port without switching address family.
OK benno
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.66 2017/01/03 10:50:56 mpi Exp $	*/
d375 1
a375 2
		if (sc->sc_flowsrc != NULL &&
		    sc->sc_flowsrc->sa_family != pflowr->flowsrc.ss_family) {
d377 1
a377 33
			sc->sc_flowsrc = NULL;
			if (sc->so != NULL) {
				soclose(sc->so);
				sc->so = NULL;
			}
		}

		if (sc->sc_flowsrc == NULL) {
			switch(pflowr->flowsrc.ss_family) {
			case AF_INET:
				if ((sc->sc_flowsrc = malloc(
				    sizeof(struct sockaddr_in),
				    M_DEVBUF, M_NOWAIT)) == NULL)
					return (ENOMEM);
				memcpy(sc->sc_flowsrc, &pflowr->flowsrc,
				    sizeof(struct sockaddr_in));
				sc->sc_flowsrc->sa_len = sizeof(struct
				    sockaddr_in);
				break;
			case AF_INET6:
				if ((sc->sc_flowsrc = malloc(
				    sizeof(struct sockaddr_in6),
				    M_DEVBUF, M_NOWAIT)) == NULL)
					return (ENOMEM);
				memcpy(sc->sc_flowsrc, &pflowr->flowsrc,
				    sizeof(struct sockaddr_in6));
				sc->sc_flowsrc->sa_len = sizeof(struct
				    sockaddr_in6);
				break;
			default:
				break;
			}
		}
d381 24
@


1.66
log
@Move the logic for SIOCSETPFLOW in a helper function to help with
upcoming locking.

ok visa@@, bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.65 2016/12/29 12:12:43 mpi Exp $	*/
d338 3
a340 3
		if (sc->sc_flowdst == NULL) {
			switch (pflowr->flowdst.ss_family) {
			case AF_INET:
d345 8
a352 6
				memcpy(sc->sc_flowdst, &pflowr->flowdst,
				    sizeof(struct sockaddr_in));
				sc->sc_flowdst->sa_len = sizeof(struct
				    sockaddr_in);
				break;
			case AF_INET6:
a356 7
				memcpy(sc->sc_flowdst, &pflowr->flowdst,
				    sizeof(struct sockaddr_in6));
				sc->sc_flowdst->sa_len = sizeof(struct
				    sockaddr_in6);
				break;
			default:
				break;
d358 7
d366 1
@


1.65
log
@Change NET_LOCK()/NET_UNLOCK() to be simple wrappers around
splsoftnet()/splx() until the known issues are fixed.

In other words, stop using a rwlock since it creates a deadlock when
chrome is used.

Issue reported by Dimitris Papastamos and kettenis@@

ok visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.64 2016/12/21 12:28:49 mikeb Exp $	*/
d72 1
d306 147
a459 3
	struct socket		*so;
	struct sockaddr		*sa;
	struct mbuf		*m;
a512 9
		if (pflowr.addrmask & PFLOW_MASK_VERSION) {
			switch(pflowr.version) {
			case PFLOW_PROTO_5:
			case PFLOW_PROTO_10:
				break;
			default:
				return(EINVAL);
			}
		}
d515 1
a515 141
		pflow_flush(sc);

		if (pflowr.addrmask & PFLOW_MASK_DSTIP) {
			if (sc->sc_flowdst != NULL &&
			    sc->sc_flowdst->sa_family !=
			    pflowr.flowdst.ss_family) {
				free(sc->sc_flowdst, M_DEVBUF,
				    sc->sc_flowdst->sa_len);
				sc->sc_flowdst = NULL;
				if (sc->so != NULL) {
					soclose(sc->so);
					sc->so = NULL;
				}
			}

			if (sc->sc_flowdst == NULL) {
				switch(pflowr.flowdst.ss_family) {
				case AF_INET:
					if ((sc->sc_flowdst = malloc(
					    sizeof(struct sockaddr_in),
					    M_DEVBUF,  M_NOWAIT)) == NULL) {
						splx(s);
						return (ENOMEM);
					}
					memcpy(sc->sc_flowdst, &pflowr.flowdst,
					    sizeof(struct sockaddr_in));
					sc->sc_flowdst->sa_len = sizeof(struct
					    sockaddr_in);
					break;
				case AF_INET6:
					if ((sc->sc_flowdst = malloc(
					    sizeof(struct sockaddr_in6),
					    M_DEVBUF, M_NOWAIT)) == NULL) {
						splx(s);
						return (ENOMEM);
					}
					memcpy(sc->sc_flowdst, &pflowr.flowdst,
					    sizeof(struct sockaddr_in6));
					sc->sc_flowdst->sa_len = sizeof(struct
					    sockaddr_in6);
					break;
				default:
					break;
				}
			}
			if (sc->sc_flowdst != NULL) {
				sc->send_nam->m_len = sc->sc_flowdst->sa_len;
				sa = mtod(sc->send_nam, struct sockaddr *);
				memcpy(sa, sc->sc_flowdst,
				    sc->sc_flowdst->sa_len);
			}
		}

		if (pflowr.addrmask & PFLOW_MASK_SRCIP) {
			if (sc->sc_flowsrc != NULL &&
			    sc->sc_flowsrc->sa_family !=
			    pflowr.flowsrc.ss_family) {
				free(sc->sc_flowsrc, M_DEVBUF,
				    sc->sc_flowsrc->sa_len);
				sc->sc_flowsrc = NULL;
				if (sc->so != NULL) {
					soclose(sc->so);
					sc->so = NULL;
				}
			}

			if (sc->sc_flowsrc == NULL) {
				switch(pflowr.flowsrc.ss_family) {
				case AF_INET:
					if ((sc->sc_flowsrc = malloc(
					    sizeof(struct sockaddr_in),
					    M_DEVBUF, M_NOWAIT)) == NULL) {
						splx(s);
						return (ENOMEM);
					}
					memcpy(sc->sc_flowsrc, &pflowr.flowsrc,
					    sizeof(struct sockaddr_in));
					sc->sc_flowsrc->sa_len = sizeof(struct
					    sockaddr_in);
					break;
				case AF_INET6:
					if ((sc->sc_flowsrc = malloc(
					    sizeof(struct sockaddr_in6),
					    M_DEVBUF, M_NOWAIT)) == NULL) {
						splx(s);
						return (ENOMEM);
					}
					memcpy(sc->sc_flowsrc, &pflowr.flowsrc,
					    sizeof(struct sockaddr_in6));
					sc->sc_flowsrc->sa_len = sizeof(struct
					    sockaddr_in6);
					break;
				default:
					break;
				}
			}
			if (sc->so != NULL) {
				soclose(sc->so);
				sc->so = NULL;
			}
		}

		if (sc->so == NULL) {
			if (pflowvalidsockaddr(sc->sc_flowdst, 0)) {
				error = socreate(sc->sc_flowdst->sa_family,
				    &so, SOCK_DGRAM, 0);
				if (error) {
					splx(s);
					return (error);
				}
				if (pflowvalidsockaddr(sc->sc_flowsrc, 1)) {
					MGET(m, M_WAIT, MT_SONAME);
					m->m_len = sc->sc_flowsrc->sa_len;
					sa = mtod(m, struct sockaddr *);
					memcpy(sa, sc->sc_flowsrc,
					    sc->sc_flowsrc->sa_len);

					error = sobind(so, m, p);
					m_freem(m);
					if (error) {
						soclose(so);
						splx(s);
						return (error);
					}
				}
				sc->so = so;
			}
		} else {
			if (!pflowvalidsockaddr(sc->sc_flowdst, 0)) {
				soclose(sc->so);
				sc->so = NULL;
			}
		}

		/* error check is above */
		if (pflowr.addrmask & PFLOW_MASK_VERSION)
			sc->sc_version = pflowr.version;

		pflow_setmtu(sc, ETHERMTU);
		pflow_init_timeouts(sc);

d517 2
@


1.64
log
@Remove the netlock workaround since if_detach is doing it for us now.

ok mpi, bluhm
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.63 2016/12/20 15:07:32 mpi Exp $	*/
a377 2
		/* XXXSMP breaks atomicity */
		rw_exit_write(&netlock);
a400 1
						rw_enter_write(&netlock);
a412 1
						rw_enter_write(&netlock);
a451 1
						rw_enter_write(&netlock);
a463 1
						rw_enter_write(&netlock);
a486 1
					rw_enter_write(&netlock);
a500 1
						rw_enter_write(&netlock);
a532 1
		rw_enter_write(&netlock);
@


1.63
log
@Release the NET_LOCK() before calling any socket function since it is
not recursive.

This is temporary until all recursions are found and can be addressed
in a correct way.

With inputs from bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.62 2016/10/04 13:54:32 mpi Exp $	*/
a269 2
		/* XXXSMP breaks atomicity */
		rw_exit_write(&netlock);
a270 1
		rw_enter_write(&netlock);
@


1.62
log
@Convert timeouts that need a process context to timeout_set_proc(9).

The current reason is that rtalloc_mpath(9) inside ip_output() might
end up inserting a RTF_CLONED route and that require a write lock.

ok kettenis@@, bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.61 2016/04/29 08:55:03 krw Exp $	*/
d270 2
d273 1
d381 2
d406 1
d419 1
d459 1
d472 1
d496 1
d511 1
d544 1
@


1.61
log
@Make if_output() return EAFNOSUPPORT instead of just dropping packets
and pretending the output succeeded. Packets are still dropped!

Idea from jsg@@ following same change to bridge(4). ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.60 2015/10/03 10:44:23 florian Exp $	*/
d551 1
a551 1
			timeout_set(&sc->sc_tmo, pflow_timeout, sc);
d555 2
a556 1
			timeout_set(&sc->sc_tmo_tmpl, pflow_timeout_tmpl, sc);
d558 1
a558 1
			timeout_set(&sc->sc_tmo, pflow_timeout, sc);
d560 1
a560 1
			timeout_set(&sc->sc_tmo6, pflow_timeout6, sc);
@


1.60
log
@IPv6 transport for pflow data.
Input deraadt@@
Bug fix & OK benno@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.59 2015/09/12 22:07:47 florian Exp $	*/
d68 2
d119 8
d235 1
a235 1
	ifp->if_output = NULL;
@


1.59
log
@Call socreate(9) only when we have a destination ip and port.
Call sobind(9) only when we have a source ip.
With this we can treat sc->so != NULL as a flag if the interface
is in state IFF_RUNNING.
Input & OK bluhm@@, OK benno@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.58 2015/09/09 16:02:31 florian Exp $	*/
d73 1
a120 1
	struct sockaddr_in	*sin;
d127 1
a127 8
	sin = mtod(pflowif->send_nam, struct sockaddr_in *);
	memset(sin, 0 , sizeof(*sin));
	sin->sin_len = pflowif->send_nam->m_len = sizeof (struct sockaddr_in);
	sin->sin_family = AF_INET;

	pflowif->sc_receiver_ip.s_addr = INADDR_ANY;
	pflowif->sc_receiver_port = 0;
	pflowif->sc_sender_ip.s_addr = INADDR_ANY;
d263 4
d275 21
d303 1
a303 1
	struct sockaddr_in	*sin;
d339 6
a344 3
		pflowr.sender_ip = sc->sc_sender_ip;
		pflowr.receiver_ip = sc->sc_receiver_ip;
		pflowr.receiver_port = sc->sc_receiver_port;
d372 48
a419 8
			sc->sc_receiver_ip.s_addr = pflowr.receiver_ip.s_addr;
			sin = mtod(sc->send_nam, struct sockaddr_in *);
			sin->sin_addr.s_addr = sc->sc_receiver_ip.s_addr;
		}
		if (pflowr.addrmask & PFLOW_MASK_DSTPRT) {
			sc->sc_receiver_port = pflowr.receiver_port;
			sin = mtod(sc->send_nam, struct sockaddr_in *);
			sin->sin_port = pflowr.receiver_port;
d421 1
d423 42
a464 1
			sc->sc_sender_ip.s_addr = pflowr.sender_ip.s_addr;
d472 3
a474 3
			if (sc->sc_receiver_ip.s_addr != INADDR_ANY &&
			    sc->sc_receiver_port != 0) {
				error = socreate(AF_INET, &so, SOCK_DGRAM, 0);
d479 1
a479 1
				if (sc->sc_sender_ip.s_addr != INADDR_ANY) {
d481 4
a484 8
					sin = mtod(m, struct sockaddr_in *);
					memset(sin, 0 , sizeof(*sin));
					sin->sin_len = m->m_len = sizeof
					    (struct sockaddr_in);
					sin->sin_family = AF_INET;
					sin->sin_addr.s_addr =
					    pflowr.sender_ip.s_addr;
					sin->sin_port = 0;
d497 1
a497 2
			if (sc->sc_receiver_ip.s_addr == INADDR_ANY ||
			    sc->sc_receiver_port == 0) {
@


1.58
log
@size for free()
OK semarie@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.57 2015/09/04 20:28:12 florian Exp $	*/
a119 1
	struct socket		*so;
a120 20
	struct mbuf		*m;
	int			 error;

	error = socreate(AF_INET, &so, SOCK_DGRAM, 0);
	if (error)
		return (error);

	MGET(m, M_WAIT, MT_SONAME);
	sin = mtod(m, struct sockaddr_in *);
	memset(sin, 0 , sizeof(*sin));
	sin->sin_len = m->m_len = sizeof (struct sockaddr_in);
	sin->sin_family = AF_INET;
	sin->sin_addr.s_addr = INADDR_ANY;
	sin->sin_port = htons(0);
	error = sobind(so, m, curproc);
	m_freem(m);
	if (error) {
		soclose(so);
		return (error);
	}
d123 1
a123 2
	    M_DEVBUF, M_NOWAIT|M_ZERO)) == NULL) {
		soclose(so);
a124 3
	}

	pflowif->so = so;
a130 2
	sin->sin_addr.s_addr = INADDR_ANY;
	sin->sin_port = 0;
d294 1
a294 3
		if ((ifp->if_flags & IFF_UP) &&
		    sc->sc_receiver_ip.s_addr != INADDR_ANY &&
		    sc->sc_receiver_port != 0 && sc->so != NULL) {
d361 35
a395 4
			error = socreate(AF_INET, &so, SOCK_DGRAM, 0);
			if (error) {
				splx(s);
				return (error);
d397 5
a401 15
			
			MGET(m, M_WAIT, MT_SONAME);
			sin = mtod(m, struct sockaddr_in *);
			memset(sin, 0 , sizeof(*sin));
			sin->sin_len = m->m_len = sizeof (struct sockaddr_in);
			sin->sin_family = AF_INET;
			sin->sin_addr.s_addr = pflowr.sender_ip.s_addr;
			sin->sin_port = 0;

			error = sobind(so, m, p);
			m_freem(m);
			if (error) {
				soclose(so);
				splx(s);
				return (error);
a402 4

			sc->sc_sender_ip.s_addr = pflowr.sender_ip.s_addr;
			soclose(sc->so);
			sc->so = so;
d414 1
a414 3
		if ((ifp->if_flags & IFF_UP) &&
		    sc->sc_receiver_ip.s_addr != INADDR_ANY &&
		    sc->sc_receiver_port != 0 && sc->so != NULL) {
@


1.57
log
@pflow_flush() still needs sc->send_nam; free it later.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.56 2015/09/04 08:17:06 mpi Exp $	*/
d299 1
a299 1
	free(sc, M_DEVBUF, 0);
@


1.56
log
@Fix an mbuf use-after-fruit in pflow_clone_create().

Issue reported by semarie@@ on bugs@@ who also isolated the
use-after-fruit to pflow(4) using dlg@@'s tracing mbuf diff.

Inputs from and ok florian@@, semarie@@, benno@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.55 2015/07/21 03:03:10 florian Exp $	*/
a284 1
	m_freem(sc->send_nam);
d292 1
@


1.55
log
@We don't do 'ARGSUSED' anymore
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.54 2015/07/21 03:00:20 florian Exp $	*/
d154 1
a154 1
	sin->sin_len = m->m_len = sizeof (struct sockaddr_in);
@


1.54
log
@use curproc instead of proc0
pointed out by and OK bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.53 2015/07/20 23:15:54 florian Exp $	*/
a303 1
/* ARGSUSED */
@


1.53
log
@Use the kernel socket interface (sosend(9) etc) instead of shoving
packets directly into the network stack with ip_output().
The locking is intentionally left as is and will be improved in
another commit.
Input / OK bluhm@@, OK benno@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.52 2015/07/16 18:36:59 florian Exp $	*/
a107 2
extern struct proc proc0;

d136 1
a136 1
	error = sobind(so, m, &proc0);
d405 1
a405 1
			error = sobind(so, m, &proc0);
@


1.52
log
@add missing malloc check
OK benno@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.51 2015/06/16 11:09:39 mpi Exp $	*/
d31 2
a72 2
int	pflowoutput(struct ifnet *, struct mbuf *, struct sockaddr *,
	    struct rtentry *);
a73 1
void	pflowstart(struct ifnet *);
a95 1
int	pflow_get_dynport(void);
d108 2
d122 22
d146 2
a147 1
	    M_DEVBUF, M_NOWAIT|M_ZERO)) == NULL)
d149 11
a160 7
	if ((pflowif->sc_imo.imo_membership = malloc(
	    (sizeof(struct in_multi *) * IP_MIN_MEMBERSHIPS), M_IPMOPTS,
	    M_WAITOK|M_ZERO)) == NULL) {
		free(pflowif, M_DEVBUF, 0);
		return (ENOMEM);
	}
	pflowif->sc_imo.imo_max_memberships = IP_MIN_MEMBERSHIPS;
a163 1
	pflowif->sc_sender_port = pflow_get_dynport();
d261 2
a262 2
	ifp->if_output = pflowoutput;
	ifp->if_start = pflowstart;
a272 4
#if NBPFILTER > 0
	bpfattach(&pflowif->sc_if.if_bpf, ifp, DLT_RAW, 0);
#endif

d282 3
a284 1
	int			 s;
d287 1
d295 4
a300 1
	free(sc->sc_imo.imo_membership, M_IPMOPTS, 0);
d303 1
a303 30
	return (0);
}

/*
 * Start output on the pflow interface.
 */
void
pflowstart(struct ifnet *ifp)
{
	struct mbuf	*m;
	int		 s;

	for (;;) {
		s = splnet();
		IF_DROP(&ifp->if_snd);
		IF_DEQUEUE(&ifp->if_snd, m);
		splx(s);

		if (m == NULL)
			return;
		m_freem(m);
	}
}

int
pflowoutput(struct ifnet *ifp, struct mbuf *m, struct sockaddr *dst,
	struct rtentry *rt)
{
	m_freem(m);
	return (0);
d314 3
d326 1
a326 2
		    sc->sc_receiver_port != 0 &&
		    sc->sc_sender_port != 0) {
d378 1
a379 1

d382 1
a382 1
		if (pflowr.addrmask & PFLOW_MASK_DSTIP)
d384 4
a387 1
		if (pflowr.addrmask & PFLOW_MASK_DSTPRT)
d389 26
a414 1
		if (pflowr.addrmask & PFLOW_MASK_SRCIP)
d416 4
d431 1
a431 2
		    sc->sc_receiver_port != 0 &&
		    sc->sc_sender_port != 0) {
a1117 40
	struct udpiphdr	*ui;
	u_int16_t	 len = m->m_pkthdr.len;
#if NBPFILTER > 0
	struct ifnet	*ifp = &sc->sc_if;
#endif
	struct ip	*ip;
	int		 err;

	/* UDP Header*/
	M_PREPEND(m, sizeof(struct udpiphdr), M_DONTWAIT);
	if (m == NULL) {
		pflowstats.pflow_onomem++;
		return (ENOBUFS);
	}

	ui = mtod(m, struct udpiphdr *);
	ui->ui_pr = IPPROTO_UDP;
	ui->ui_src = sc->sc_sender_ip;
	ui->ui_sport = sc->sc_sender_port;
	ui->ui_dst = sc->sc_receiver_ip;
	ui->ui_dport = sc->sc_receiver_port;
	ui->ui_ulen = htons(sizeof(struct udphdr) + len);
	ui->ui_sum = 0;
	m->m_pkthdr.csum_flags |= M_UDP_CSUM_OUT;
	m->m_pkthdr.ph_rtableid = sc->sc_if.if_rdomain;

	ip = (struct ip *)ui;
	ip->ip_v = IPVERSION;
	ip->ip_hl = sizeof(struct ip) >> 2;
	ip->ip_id = htons(ip_randomid());
	ip->ip_off = htons(IP_DF);
	ip->ip_tos = IPTOS_LOWDELAY;
	ip->ip_ttl = IPDEFTTL;
	ip->ip_len = htons(sizeof(struct udpiphdr) + len);

#if NBPFILTER > 0
	if (ifp->if_bpf)
		bpf_mtap(ifp->if_bpf, m, BPF_DIRECTION_OUT);
#endif

d1121 3
a1123 4
	if ((err = ip_output(m, NULL, NULL, IP_RAWOUTPUT, &sc->sc_imo, NULL,
	    0))) {
		pflowstats.pflow_oerrors++;
		sc->sc_if.if_oerrors++;
d1125 1
a1125 24
	return (err);
}

int
pflow_get_dynport(void)
{
	u_int16_t	tmp, low, high, cut;

	low = ipport_hifirstauto;     /* sysctl */
	high = ipport_hilastauto;

	cut = arc4random_uniform(1 + high - low) + low;

	for (tmp = cut; tmp <= high; ++(tmp)) {
		if (!in_baddynamic(tmp, IPPROTO_UDP))
			return (htons(tmp));
	}

	for (tmp = cut - 1; tmp >= low; --(tmp)) {
		if (!in_baddynamic(tmp, IPPROTO_UDP))
			return (htons(tmp));
	}

	return (htons(ipport_hilastauto)); /* XXX */
@


1.51
log
@Store a unique ID, an interface index, rather than a pointer to the
receiving interface in the packet header of every mbuf.

The interface pointer should now be retrieved when necessary with
if_get().  If a NULL pointer is returned by if_get(), the interface
has probably been destroy/removed and the mbuf should be freed.

Such mechanism will simplify garbage collection of mbufs and limit
problems with dangling ifp pointers.

Tested by jmatthew@@ and krw@@, discussed with many.

ok mikeb@@, bluhm@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.50 2015/06/07 12:02:28 jsg Exp $	*/
d127 1
a127 1
	pflowif->sc_imo.imo_membership = malloc(
d129 4
a132 1
	    M_WAITOK|M_ZERO);
@


1.50
log
@Introduce unhandled_af() for cases where code conditionally does
something based on an address family and later assumes one of the paths
was taken.  This was initially just calls to panic until guenther
suggested a function to reduce the amount of strings needed.

This reduces the amount of noise with static analysers and acts
as a sanity check.

ok guenther@@ bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.49 2014/12/19 17:14:39 tedu Exp $	*/
d509 1
a509 1
	m->m_pkthdr.rcvif = NULL;
@


1.49
log
@unifdef INET in net code as a precursor to removing the pretend option.
long live the one true internet.
ok henning mikeb
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.48 2014/11/20 14:51:42 krw Exp $	*/
d1010 2
a1011 2
	default: /* NOTREACHED */
		break;
@


1.48
log
@Yet more #include de-duplication.

ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.47 2014/11/18 02:37:31 tedu Exp $	*/
a40 1
#ifdef INET
a45 1
#endif /* INET */
@


1.47
log
@move arc4random prototype to systm.h. more appropriate for most code
to include that than rdnvar.h. ok deraadt dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.46 2014/08/13 09:46:23 blambert Exp $	*/
a41 1
#include <netinet/in.h>
@


1.46
log
@Bring IPFIX sequence numbers in line with the RFC; original
diff from benno@@.

ok benno@@, florian@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.45 2014/07/22 11:06:09 mpi Exp $	*/
a31 1
#include <dev/rndvar.h>
@


1.45
log
@Fewer <netinet/in_systm.h> !
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.44 2014/07/12 18:44:22 tedu Exp $	*/
d990 1
d1000 1
d1010 1
d1037 2
a1038 1
	h10->flow_sequence = htonl(sc->sc_gcounter);
d1077 1
a1077 1
	h10->flow_sequence = htonl(sc->sc_gcounter);
@


1.44
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.43 2014/04/21 12:22:25 henning Exp $	*/
a43 1
#include <netinet/in_systm.h>
@


1.43
log
@ip_output() using varargs always struck me as bizarre, esp since it's only
ever used to pass on uint32 (for ipsec). stop that madness and just pass
the uint32, 0 in all cases but the two that pass the ipsec flowinfo.
ok deraadt reyk guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.41 2014/03/29 11:26:03 florian Exp $	*/
d274 2
a275 2
	free(sc->sc_imo.imo_membership, M_IPMOPTS);
	free(sc, M_DEVBUF);
@


1.42
log
@"struct pkthdr" holds a routing table ID, not a routing domain one.
Avoid the confusion by using an appropriate name for the variable.

Note that since routing domain IDs are a subset of the set of routing
table IDs, the following idiom is correct:

	rtableid = rdomain

But to get the routing domain ID corresponding to a given routing table
ID, you must call rtable_l2(9).

claudio@@ likes it, ok mikeb@@
@
text
@d1127 2
a1128 1
	if ((err = ip_output(m, NULL, NULL, IP_RAWOUTPUT, &sc->sc_imo, NULL))) {
@


1.41
log
@Kill pflowproto 9, it's unfixable post 2038, a better, standardized
option is pflowproto 10. Also it duplicates a lot of code from
pflowproto 10 and will get in the way in the future.
OK benno@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.40 2014/01/24 09:48:37 henning Exp $	*/
d1108 1
a1108 1
	m->m_pkthdr.rdomain = sc->sc_if.if_rdomain;
@


1.40
log
@computing the ip csum just before the bpf mtap and only if there is a
consumer just to please tcpdump is stupid and not done anywhere else.
kill with fire. ok benno
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.38 2013/11/01 14:34:27 florian Exp $	*/
a83 1
int	pflow_sendout_v9(struct pflow_softc *, sa_family_t);
a84 1
int	pflow_sendout_v9_tmpl(struct pflow_softc *);
a91 3
void	copy_flow_v9_4_data(struct pflow_v9_flow4 *, struct pflow_v9_flow4 *,
	struct pf_state *, struct pf_state_key *, struct pflow_softc *, int,
	int);
a94 3
void	copy_flow_v9_6_data(struct pflow_v9_flow6 *, struct pflow_v9_flow6 *,
	struct pf_state *, struct pf_state_key *, struct pflow_softc *, int,
	int);
a99 2
int	pflow_pack_flow_v9(struct pf_state *, struct pf_state_key *,
	struct pflow_softc *);
a105 2
int	copy_flow_v9_4_to_m(struct pflow_v9_flow4 *flow, struct pflow_softc
	*sc);
a107 2
int	copy_flow_v9_6_to_m(struct pflow_v9_flow6 *flow, struct pflow_softc
	*sc);
a141 88
	/* v9 template init */
	bzero(&pflowif->sc_tmpl_v9,sizeof(pflowif->sc_tmpl_v9));
	pflowif->sc_tmpl_v9.set_header.set_id = htons(PFLOW_V9_TMPL_SET_ID);
	pflowif->sc_tmpl_v9.set_header.set_length =
	    htons(sizeof(struct pflow_v9_tmpl));

	/* v9 IPv4 template */
	pflowif->sc_tmpl_v9.ipv4_tmpl.h.tmpl_id = htons(PFLOW_V9_TMPL_IPV4_ID);
	pflowif->sc_tmpl_v9.ipv4_tmpl.h.field_count
	    = htons(PFLOW_V9_TMPL_IPV4_FIELD_COUNT);
	pflowif->sc_tmpl_v9.ipv4_tmpl.src_ip.field_id =
	    htons(PFIX_IE_sourceIPv4Address);
	pflowif->sc_tmpl_v9.ipv4_tmpl.src_ip.len = htons(4);
	pflowif->sc_tmpl_v9.ipv4_tmpl.dest_ip.field_id =
	    htons(PFIX_IE_destinationIPv4Address);
	pflowif->sc_tmpl_v9.ipv4_tmpl.dest_ip.len = htons(4);
	pflowif->sc_tmpl_v9.ipv4_tmpl.if_index_in.field_id =
	    htons(PFIX_IE_ingressInterface);
	pflowif->sc_tmpl_v9.ipv4_tmpl.if_index_in.len = htons(4);
	pflowif->sc_tmpl_v9.ipv4_tmpl.if_index_out.field_id =
	    htons(PFIX_IE_egressInterface);
	pflowif->sc_tmpl_v9.ipv4_tmpl.if_index_out.len = htons(4);
	pflowif->sc_tmpl_v9.ipv4_tmpl.packets.field_id =
	    htons(PFIX_IE_packetDeltaCount);
	pflowif->sc_tmpl_v9.ipv4_tmpl.packets.len = htons(8);
	pflowif->sc_tmpl_v9.ipv4_tmpl.octets.field_id =
	    htons(PFIX_IE_octetDeltaCount);
	pflowif->sc_tmpl_v9.ipv4_tmpl.octets.len = htons(8);
	pflowif->sc_tmpl_v9.ipv4_tmpl.start.field_id =
	    htons(PFIX_IE_flowStartSysUpTime);
	pflowif->sc_tmpl_v9.ipv4_tmpl.start.len = htons(4);
	pflowif->sc_tmpl_v9.ipv4_tmpl.finish.field_id =
	    htons(PFIX_IE_flowEndSysUpTime);
	pflowif->sc_tmpl_v9.ipv4_tmpl.finish.len = htons(4);
	pflowif->sc_tmpl_v9.ipv4_tmpl.src_port.field_id =
	    htons(PFIX_IE_sourceTransportPort);
	pflowif->sc_tmpl_v9.ipv4_tmpl.src_port.len = htons(2);
	pflowif->sc_tmpl_v9.ipv4_tmpl.dest_port.field_id =
	    htons(PFIX_IE_destinationTransportPort);
	pflowif->sc_tmpl_v9.ipv4_tmpl.dest_port.len = htons(2);
	pflowif->sc_tmpl_v9.ipv4_tmpl.tos.field_id =
	    htons(PFIX_IE_ipClassOfService);
	pflowif->sc_tmpl_v9.ipv4_tmpl.tos.len = htons(1);
	pflowif->sc_tmpl_v9.ipv4_tmpl.protocol.field_id =
	    htons(PFIX_IE_protocolIdentifier);
	pflowif->sc_tmpl_v9.ipv4_tmpl.protocol.len = htons(1);

	/* v9 IPv6 template */
	pflowif->sc_tmpl_v9.ipv6_tmpl.h.tmpl_id = htons(PFLOW_V9_TMPL_IPV6_ID);
	pflowif->sc_tmpl_v9.ipv6_tmpl.h.field_count =
	    htons(PFLOW_V9_TMPL_IPV6_FIELD_COUNT);
	pflowif->sc_tmpl_v9.ipv6_tmpl.src_ip.field_id =
	    htons(PFIX_IE_sourceIPv6Address);
	pflowif->sc_tmpl_v9.ipv6_tmpl.src_ip.len = htons(16);
	pflowif->sc_tmpl_v9.ipv6_tmpl.dest_ip.field_id =
	    htons(PFIX_IE_destinationIPv6Address);
	pflowif->sc_tmpl_v9.ipv6_tmpl.dest_ip.len = htons(16);
	pflowif->sc_tmpl_v9.ipv6_tmpl.if_index_in.field_id =
	    htons(PFIX_IE_ingressInterface);
	pflowif->sc_tmpl_v9.ipv6_tmpl.if_index_in.len = htons(4);
	pflowif->sc_tmpl_v9.ipv6_tmpl.if_index_out.field_id =
	    htons(PFIX_IE_egressInterface);
	pflowif->sc_tmpl_v9.ipv6_tmpl.if_index_out.len = htons(4);
	pflowif->sc_tmpl_v9.ipv6_tmpl.packets.field_id =
	    htons(PFIX_IE_packetDeltaCount);
	pflowif->sc_tmpl_v9.ipv6_tmpl.packets.len = htons(8);
	pflowif->sc_tmpl_v9.ipv6_tmpl.octets.field_id =
	    htons(PFIX_IE_octetDeltaCount);
	pflowif->sc_tmpl_v9.ipv6_tmpl.octets.len = htons(8);
	pflowif->sc_tmpl_v9.ipv6_tmpl.start.field_id =
	    htons(PFIX_IE_flowStartSysUpTime);
	pflowif->sc_tmpl_v9.ipv6_tmpl.start.len = htons(4);
	pflowif->sc_tmpl_v9.ipv6_tmpl.finish.field_id =
	    htons(PFIX_IE_flowEndSysUpTime);
	pflowif->sc_tmpl_v9.ipv6_tmpl.finish.len = htons(4);
	pflowif->sc_tmpl_v9.ipv6_tmpl.src_port.field_id =
	    htons(PFIX_IE_sourceTransportPort);
	pflowif->sc_tmpl_v9.ipv6_tmpl.src_port.len = htons(2);
	pflowif->sc_tmpl_v9.ipv6_tmpl.dest_port.field_id =
	    htons(PFIX_IE_destinationTransportPort);
	pflowif->sc_tmpl_v9.ipv6_tmpl.dest_port.len = htons(2);
	pflowif->sc_tmpl_v9.ipv6_tmpl.tos.field_id =
	    htons(PFIX_IE_ipClassOfService);
	pflowif->sc_tmpl_v9.ipv6_tmpl.tos.len = htons(1);
	pflowif->sc_tmpl_v9.ipv6_tmpl.protocol.field_id =
	    htons(PFIX_IE_protocolIdentifier);
	pflowif->sc_tmpl_v9.ipv6_tmpl.protocol.len = htons(1);

d331 1
a331 5
			if (sc->sc_version == PFLOW_PROTO_9) {
				s = splnet();
				pflow_sendout_v9_tmpl(sc);
				splx(s);
			} else if (sc->sc_version == PFLOW_PROTO_10) {
a372 1
			case PFLOW_PROTO_9:
d404 1
a404 5
			if (sc->sc_version == PFLOW_PROTO_9) {
				s = splnet();
				pflow_sendout_v9_tmpl(sc);
				splx(s);
			} else if (sc->sc_version == PFLOW_PROTO_10) {
a431 1
	case PFLOW_PROTO_9:
d450 5
a454 11
	if (sc->sc_version == PFLOW_PROTO_9) {
		sc->sc_maxcount4 = (mtu - hdrsz -
		    sizeof(struct udpiphdr)) / sizeof(struct pflow_v9_flow4);
		sc->sc_maxcount6 = (mtu - hdrsz -
		    sizeof(struct udpiphdr)) / sizeof(struct pflow_v9_flow6);
	} else {
		sc->sc_maxcount4 = (mtu - hdrsz -
		    sizeof(struct udpiphdr)) / sizeof(struct pflow_ipfix_flow4);
		sc->sc_maxcount6 = (mtu - hdrsz -
		    sizeof(struct udpiphdr)) / sizeof(struct pflow_ipfix_flow6);
	}
d459 3
a461 8
	if (sc->sc_version == PFLOW_PROTO_9)
		return (hdrsz + sizeof(struct udpiphdr) +
		    MIN(sc->sc_maxcount4 * sizeof(struct pflow_v9_flow4),
		    sc->sc_maxcount6 * sizeof(struct pflow_v9_flow6)));
	else
		return (hdrsz + sizeof(struct udpiphdr) +
		    MIN(sc->sc_maxcount4 * sizeof(struct pflow_ipfix_flow4),
		    sc->sc_maxcount6 * sizeof(struct pflow_ipfix_flow6)));
a483 4
	case PFLOW_PROTO_9:
		sc->sc_if.if_mtu = 
		    pflow_calc_mtu(sc, mtu, sizeof(struct pflow_v9_header));
		break;
a579 74
copy_flow_v9_4_data(struct pflow_v9_flow4 *flow1, struct pflow_v9_flow4 *flow2,
    struct pf_state *st, struct pf_state_key *sk, struct pflow_softc *sc,
    int src, int dst)
{
	flow1->src_ip = flow2->dest_ip = sk->addr[src].v4.s_addr;
	flow1->src_port = flow2->dest_port = sk->port[src];
	flow1->dest_ip = flow2->src_ip = sk->addr[dst].v4.s_addr;
	flow1->dest_port = flow2->src_port = sk->port[dst];

	flow1->if_index_in = htonl(st->if_index_in);
	flow1->if_index_out = htonl(st->if_index_out);
	flow2->if_index_in = htonl(st->if_index_out);
	flow2->if_index_out = htonl(st->if_index_in);

	flow1->flow_packets = htobe64(st->packets[0]);
	flow2->flow_packets = htobe64(st->packets[1]);
	flow1->flow_octets = htobe64(st->bytes[0]);
	flow2->flow_octets = htobe64(st->bytes[1]);

	/*
	 * Pretend the flow was created or expired when the machine came
	 * up when creation is in the future of the last time a package
	 * was seen or was created / expired before this machine came up
	 * due to pfsync.
	 */
	flow1->flow_start = flow2->flow_start = st->creation < 0 ||
	    st->creation > st->expire ? htonl(0) : htonl(st->creation *
	    1000);
	flow1->flow_finish = flow2->flow_finish = st->expire < 0 ?
	    htonl(0) : htonl(st->expire * 1000);

	flow1->protocol = flow2->protocol = sk->proto;
	flow1->tos = flow2->tos = st->rule.ptr->tos;
}

void
copy_flow_v9_6_data(struct pflow_v9_flow6 *flow1, struct pflow_v9_flow6 *flow2,
    struct pf_state *st, struct pf_state_key *sk, struct pflow_softc *sc,
    int src, int dst)
{
	bcopy(&sk->addr[src].v6, &flow1->src_ip, sizeof(flow1->src_ip));
	bcopy(&sk->addr[src].v6, &flow2->dest_ip, sizeof(flow2->dest_ip));
	flow1->src_port = flow2->dest_port = sk->port[src];
	bcopy(&sk->addr[dst].v6, &flow1->dest_ip, sizeof(flow1->dest_ip));
	bcopy(&sk->addr[dst].v6, &flow2->src_ip, sizeof(flow2->src_ip));
	flow1->dest_port = flow2->src_port = sk->port[dst];

	flow1->if_index_in = htonl(st->if_index_in);
	flow1->if_index_out = htonl(st->if_index_out);
	flow2->if_index_in = htonl(st->if_index_out);
	flow2->if_index_out = htonl(st->if_index_in);

	flow1->flow_packets = htobe64(st->packets[0]);
	flow2->flow_packets = htobe64(st->packets[1]);
	flow1->flow_octets = htobe64(st->bytes[0]);
	flow2->flow_octets = htobe64(st->bytes[1]);

	/*
	 * Pretend the flow was created or expired when the machine came
	 * up when creation is in the future of the last time a package
	 * was seen or was created / expired before this machine came up
	 * due to pfsync.
	 */
	flow1->flow_start = flow2->flow_start = st->creation < 0 ||
	    st->creation > st->expire ? htonl(0) : htonl(st->creation *
	    1000);
	flow1->flow_finish = flow2->flow_finish = st->expire < 0 ?
	    htonl(0) : htonl(st->expire * 1000);

	flow1->protocol = flow2->protocol = sk->proto;
	flow1->tos = flow2->tos = st->rule.ptr->tos;
}

void
a668 2
		case PFLOW_PROTO_9:
			/* ... fall through ... */
a692 2
	if (sc->sc_version == PFLOW_PROTO_9)
		return (pflow_pack_flow_v9(st, sk, sc));
a760 61
copy_flow_v9_4_to_m(struct pflow_v9_flow4 *flow, struct pflow_softc *sc)
{
	int		s, ret = 0;

	s = splnet();
	if (sc->sc_mbuf == NULL) {
		if ((sc->sc_mbuf =
		    pflow_get_mbuf(sc, PFLOW_V9_TMPL_IPV4_ID)) == NULL) {
			splx(s);
			return (ENOBUFS);
		}
		sc->sc_count4 = 0;
		timeout_add_sec(&sc->sc_tmo, PFLOW_TIMEOUT);
	}
	m_copyback(sc->sc_mbuf, PFLOW_SET_HDRLEN +
	    (sc->sc_count4 * sizeof(struct pflow_v9_flow4)),
	    sizeof(struct pflow_v9_flow4), flow, M_NOWAIT);

	if (pflowstats.pflow_flows == sc->sc_gcounter)
		pflowstats.pflow_flows++;
	sc->sc_gcounter++;
	sc->sc_count4++;

	if (sc->sc_count4 >= sc->sc_maxcount4)
		ret = pflow_sendout_v9(sc, AF_INET);
	splx(s);
	return(ret);
}

int
copy_flow_v9_6_to_m(struct pflow_v9_flow6 *flow, struct pflow_softc *sc)
{
	int		s, ret = 0;

	s = splnet();
	if (sc->sc_mbuf6 == NULL) {
		if ((sc->sc_mbuf6 =
		    pflow_get_mbuf(sc, PFLOW_V9_TMPL_IPV6_ID)) == NULL) {
			splx(s);
			return (ENOBUFS);
		}
		sc->sc_count6 = 0;
		timeout_add_sec(&sc->sc_tmo6, PFLOW_TIMEOUT);
	}
	m_copyback(sc->sc_mbuf6, PFLOW_SET_HDRLEN +
	    (sc->sc_count6 * sizeof(struct pflow_v9_flow6)),
	    sizeof(struct pflow_v9_flow6), flow, M_NOWAIT);

	if (pflowstats.pflow_flows == sc->sc_gcounter)
		pflowstats.pflow_flows++;
	sc->sc_gcounter++;
	sc->sc_count6++;

	if (sc->sc_count6 >= sc->sc_maxcount6)
		ret = pflow_sendout_v9(sc, AF_INET6);

	splx(s);
	return(ret);
}

int
a846 43
pflow_pack_flow_v9(struct pf_state *st, struct pf_state_key *sk,
    struct pflow_softc *sc)
{
	struct pflow_v9_flow4	 flow4_1, flow4_2;
	struct pflow_v9_flow6	 flow6_1, flow6_2;
	int			 ret = 0;
	if (sk->af == AF_INET) {
		bzero(&flow4_1, sizeof(flow4_1));
		bzero(&flow4_2, sizeof(flow4_2));

		if (st->direction == PF_OUT)
			copy_flow_v9_4_data(&flow4_1, &flow4_2, st, sk, sc, 1,
			    0);
		else
			copy_flow_v9_4_data(&flow4_1, &flow4_2, st, sk, sc, 0,
			    1);

		if (st->bytes[0] != 0) /* first flow from state */
			ret = copy_flow_v9_4_to_m(&flow4_1, sc);

		if (st->bytes[1] != 0) /* second flow from state */
			ret = copy_flow_v9_4_to_m(&flow4_2, sc);
	} else if (sk->af == AF_INET6) {
		bzero(&flow6_1, sizeof(flow6_1));
		bzero(&flow6_2, sizeof(flow6_2));

		if (st->direction == PF_OUT)
			copy_flow_v9_6_data(&flow6_1, &flow6_2, st, sk, sc, 1,
			    0);
		else
			copy_flow_v9_6_data(&flow6_1, &flow6_2, st, sk, sc, 0,
			    1);

		if (st->bytes[0] != 0) /* first flow from state */
			ret = copy_flow_v9_6_to_m(&flow6_1, sc);

		if (st->bytes[1] != 0) /* second flow from state */
			ret = copy_flow_v9_6_to_m(&flow6_2, sc);
	}
	return (ret);
}

int
a899 3
	case PFLOW_PROTO_9:
		pflow_sendout_v9(sc, AF_INET);
		break;
d916 1
a916 10
	switch (sc->sc_version) {
	case PFLOW_PROTO_9:
		pflow_sendout_v9(sc, AF_INET6);
		break;
	case PFLOW_PROTO_10:
		pflow_sendout_ipfix(sc, AF_INET6);
		break;
	default: /* NOTREACHED */
		break;
	}
d927 1
a927 4
	if (sc->sc_version == PFLOW_PROTO_9)
		pflow_sendout_v9_tmpl(sc);
	else if (sc->sc_version == PFLOW_PROTO_10)
		pflow_sendout_ipfix_tmpl(sc);
a938 4
	case PFLOW_PROTO_9:
		pflow_sendout_v9(sc, AF_INET);
		pflow_sendout_v9(sc, AF_INET6);
		break;
a984 58
pflow_sendout_v9(struct pflow_softc *sc, sa_family_t af)
{
	struct mbuf			*m;
	struct pflow_v9_header		*h9;
	struct pflow_set_header		*set_hdr;
	struct ifnet			*ifp = &sc->sc_if;
	int				 set_length;

	switch (af) {
	case AF_INET:
		m = sc->sc_mbuf;
		timeout_del(&sc->sc_tmo);
		if (m == NULL)
			return (0);
		sc->sc_mbuf = NULL;
		set_length = sizeof(struct pflow_set_header)
		    + sc->sc_count4 * sizeof(struct pflow_v9_flow4);
		break;
	case AF_INET6:
		m = sc->sc_mbuf6;
		timeout_del(&sc->sc_tmo6);
		if (m == NULL)
			return (0);
		sc->sc_mbuf6 = NULL;
		set_length = sizeof(struct pflow_set_header)
		    + sc->sc_count6 * sizeof(struct pflow_v9_flow6);
		break;
	default: /* NOTREACHED */
		break;
	}

	if (!(ifp->if_flags & IFF_RUNNING)) {
		m_freem(m);
		return (0);
	}

	pflowstats.pflow_packets++;
	set_hdr = mtod(m, struct pflow_set_header *);
	set_hdr->set_length = htons(set_length);

	/* populate pflow_header */
	M_PREPEND(m, sizeof(struct pflow_v9_header), M_DONTWAIT);
	if (m == NULL) {
		pflowstats.pflow_onomem++;
		return (ENOBUFS);
	}
	h9 = mtod(m, struct pflow_v9_header *);
	h9->version = htons(PFLOW_PROTO_9);
	h9->count = htons(1);
	h9->uptime_ms = htonl(time_uptime * 1000);
	h9->time_sec = htonl(time_second);		/* XXX 2038 */
	h9->flow_sequence = htonl(sc->sc_gcounter);
	h9->observation_dom = htonl(PFLOW_ENGINE_TYPE);
	return (pflow_sendout_mbuf(sc, m));
}

/* This must be called in splnet() */
int
a1036 41
	return (pflow_sendout_mbuf(sc, m));
}

/* This must be called in splnet() */
int
pflow_sendout_v9_tmpl(struct pflow_softc *sc)
{
	struct mbuf			*m;
	struct pflow_v9_header		*h9;
	struct ifnet			*ifp = &sc->sc_if;

	timeout_del(&sc->sc_tmo_tmpl);

	if (!(ifp->if_flags & IFF_RUNNING)) {
		return (0);
	}
	m = pflow_get_mbuf(NULL, 0);
	if (m == NULL)
		return (0);
	if (m_copyback(m, 0, sizeof(struct pflow_v9_tmpl),
	    &sc->sc_tmpl_v9, M_NOWAIT)) {
		m_freem(m);
		return (0);
	}
	pflowstats.pflow_packets++;

	/* populate pflow_header */
	M_PREPEND(m, sizeof(struct pflow_v9_header), M_DONTWAIT);
	if (m == NULL) {
		pflowstats.pflow_onomem++;
		return (ENOBUFS);
	}
	h9 = mtod(m, struct pflow_v9_header *);
	h9->version = htons(PFLOW_PROTO_9);
	h9->count = htons(1);
	h9->uptime_ms = htonl(time_uptime * 1000);
	h9->time_sec = htonl(time_second);		/* XXX 2038 */
	h9->flow_sequence = htonl(sc->sc_gcounter);
	h9->observation_dom = htonl(PFLOW_ENGINE_TYPE);

	timeout_add_sec(&sc->sc_tmo_tmpl, PFLOW_TMPL_TIMEOUT);
@


1.39
log
@allow pflow(4) to determine the src IP address based on the route
table if flowsrc is not set. Now works with new udp checksum code.
From Nathanael Rensen (nathanael.openbsd AT list DOT polymorpheus DOT
com), tweak and ok florian@@
@
text
@d1547 1
a1547 2
	if (ifp->if_bpf) {
		ip->ip_sum = in_cksum(m, ip->ip_hl << 2);
a1548 1
	}
@


1.38
log
@Send pflow(4) packets in the correct rdomain.
From Anders Berggren (anders AT halon DOT se), thanks.
OK henning, benno
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.37 2013/10/19 10:49:31 henning Exp $	*/
a428 1
		    sc->sc_sender_ip.s_addr != INADDR_ANY &&
a507 1
		    sc->sc_sender_ip.s_addr != INADDR_ANY &&
@


1.37
log
@simplify: no need to muck with the pseudo hdr cksum any more
ok lteo florian
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.36 2013/10/17 16:27:41 bluhm Exp $	*/
d1537 1
@


1.36
log
@The header file netinet/in_var.h included netinet6/in6_var.h.  This
created a bunch of useless dependencies.  Remove this implicit
inclusion and do an explicit #include <netinet6/in6_var.h> when it
is needed.
OK mpi@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.35 2013/09/13 14:30:47 florian Exp $	*/
a128 3
/* from udp_usrreq.c */
extern int udpcksum;

d1535 2
a1545 12

	/*
	 * Compute the pseudo-header checksum; defer further checksumming
	 * until ip_output() or hardware (if it exists).
	 */
	if (udpcksum) {
		m->m_pkthdr.csum_flags |= M_UDP_CSUM_OUT;
		ui->ui_sum = in_cksum_phdr(ui->ui_src.s_addr,
		    ui->ui_dst.s_addr, htons(len + sizeof(struct udphdr) +
		    IPPROTO_UDP));
	} else
		ui->ui_sum = 0;	
@


1.35
log
@Don't send flows if flowsrc is not set. Packages had a broken
checksum. Since no one seems to have a use case for sending flows from
INADDR_ANY disallow this.
Pointed out by Nathanael Rensen on tech@@, thanks.
While there make the SIOCSIFFLAGS and SIOCSETPFLOW cases symmetric by
only sending templates if the interface is running.

OK benno@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.34 2013/08/13 08:44:05 florian Exp $	*/
a43 1
#include <netinet/in_var.h>
@


1.34
log
@Split pflow version 9 and version 10 to be able to send 64 bit
time values for version 10.
While there mark places which will blow up in 2038.
OK benno@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.33 2013/08/10 18:33:21 florian Exp $	*/
d154 1
a154 1
	pflowif->sc_receiver_ip.s_addr = 0;
d431 4
a434 2
		    sc->sc_receiver_ip.s_addr != 0 &&
		    sc->sc_receiver_port != 0) {
d496 1
a496 1
			sc->sc_receiver_ip = pflowr.receiver_ip;
a507 5
		if (sc->sc_version == PFLOW_PROTO_9)
			pflow_sendout_v9_tmpl(sc);
		else if (sc->sc_version == PFLOW_PROTO_10)
			pflow_sendout_ipfix_tmpl(sc);

d511 4
a514 2
		    sc->sc_receiver_ip.s_addr != 0 &&
		    sc->sc_receiver_port != 0) {
d517 9
@


1.33
log
@Cancel timeouts on pflow interface destruction.
OK benno@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.32 2013/07/05 17:14:27 florian Exp $	*/
d85 1
d87 1
d95 1
a95 1
void	copy_flow4_data(struct pflow_flow4 *, struct pflow_flow4 *,
d98 4
a101 1
void	copy_flow6_data(struct pflow_flow6 *, struct pflow_flow6 *,
d104 3
d109 2
d117 8
a124 2
int	copy_flow4_to_m(struct pflow_flow4 *flow, struct pflow_softc *sc);
int	copy_flow6_to_m(struct pflow_flow6 *flow, struct pflow_softc *sc);
d159 12
a170 12
	bzero(&pflowif->sc_tmpl,sizeof(pflowif->sc_tmpl));
	pflowif->sc_tmpl.set_header.set_id =
	    htons(pflowif->sc_version == PFLOW_PROTO_9?
	    PFLOW_V9_TMPL_SET_ID:PFLOW_V10_TMPL_SET_ID);
	pflowif->sc_tmpl.set_header.set_length =
	    htons(sizeof(struct pflow_tmpl));

	/* v9/v10 IPv4 template */
	pflowif->sc_tmpl.ipv4_tmpl.h.tmpl_id = htons(PFLOW_TMPL_IPV4_ID);
	pflowif->sc_tmpl.ipv4_tmpl.h.field_count
	    = htons(PFLOW_TMPL_IPV4_FIELD_COUNT);
	pflowif->sc_tmpl.ipv4_tmpl.src_ip.field_id =
d172 2
a173 2
	pflowif->sc_tmpl.ipv4_tmpl.src_ip.len = htons(4);
	pflowif->sc_tmpl.ipv4_tmpl.dest_ip.field_id =
d175 2
a176 2
	pflowif->sc_tmpl.ipv4_tmpl.dest_ip.len = htons(4);
	pflowif->sc_tmpl.ipv4_tmpl.if_index_in.field_id =
d178 2
a179 2
	pflowif->sc_tmpl.ipv4_tmpl.if_index_in.len = htons(4);
	pflowif->sc_tmpl.ipv4_tmpl.if_index_out.field_id =
d181 2
a182 2
	pflowif->sc_tmpl.ipv4_tmpl.if_index_out.len = htons(4);
	pflowif->sc_tmpl.ipv4_tmpl.packets.field_id =
d184 2
a185 2
	pflowif->sc_tmpl.ipv4_tmpl.packets.len = htons(8);
	pflowif->sc_tmpl.ipv4_tmpl.octets.field_id =
d187 8
a194 12
	pflowif->sc_tmpl.ipv4_tmpl.octets.len = htons(8);
	/* keep in sync with SIOCSETPFLOW */
	pflowif->sc_tmpl.ipv4_tmpl.start.field_id =
	    htons(pflowif->sc_version == PFLOW_PROTO_9?
	    PFIX_IE_flowStartSysUpTime:PFIX_IE_flowStartSeconds);
	pflowif->sc_tmpl.ipv4_tmpl.start.len = htons(4);
	/* keep in sync with SIOCSETPFLOW */
	pflowif->sc_tmpl.ipv4_tmpl.finish.field_id =
	    htons(pflowif->sc_version == PFLOW_PROTO_9?
	    PFIX_IE_flowEndSysUpTime:PFIX_IE_flowEndSeconds);
	pflowif->sc_tmpl.ipv4_tmpl.finish.len = htons(4);
	pflowif->sc_tmpl.ipv4_tmpl.src_port.field_id =
d196 2
a197 2
	pflowif->sc_tmpl.ipv4_tmpl.src_port.len = htons(2);
	pflowif->sc_tmpl.ipv4_tmpl.dest_port.field_id =
d199 2
a200 2
	pflowif->sc_tmpl.ipv4_tmpl.dest_port.len = htons(2);
	pflowif->sc_tmpl.ipv4_tmpl.tos.field_id =
d202 2
a203 2
	pflowif->sc_tmpl.ipv4_tmpl.tos.len = htons(1);
	pflowif->sc_tmpl.ipv4_tmpl.protocol.field_id =
d205 1
a205 1
	pflowif->sc_tmpl.ipv4_tmpl.protocol.len = htons(1);
d207 5
a211 5
	/* v9/v10 IPv6 template */
	pflowif->sc_tmpl.ipv6_tmpl.h.tmpl_id = htons(PFLOW_TMPL_IPV6_ID);
	pflowif->sc_tmpl.ipv6_tmpl.h.field_count =
	    htons(PFLOW_TMPL_IPV6_FIELD_COUNT);
	pflowif->sc_tmpl.ipv6_tmpl.src_ip.field_id =
d213 2
a214 2
	pflowif->sc_tmpl.ipv6_tmpl.src_ip.len = htons(16);
	pflowif->sc_tmpl.ipv6_tmpl.dest_ip.field_id =
d216 51
a266 2
	pflowif->sc_tmpl.ipv6_tmpl.dest_ip.len = htons(16);
	pflowif->sc_tmpl.ipv6_tmpl.if_index_in.field_id =
d268 2
a269 2
	pflowif->sc_tmpl.ipv6_tmpl.if_index_in.len = htons(4);
	pflowif->sc_tmpl.ipv6_tmpl.if_index_out.field_id =
d271 2
a272 2
	pflowif->sc_tmpl.ipv6_tmpl.if_index_out.len = htons(4);
	pflowif->sc_tmpl.ipv6_tmpl.packets.field_id =
d274 2
a275 2
	pflowif->sc_tmpl.ipv6_tmpl.packets.len = htons(8);
	pflowif->sc_tmpl.ipv6_tmpl.octets.field_id =
d277 8
a284 12
	pflowif->sc_tmpl.ipv6_tmpl.octets.len = htons(8);
	/* keep in sync with SIOCSETPFLOW */
	pflowif->sc_tmpl.ipv6_tmpl.start.field_id =
	    htons(pflowif->sc_version == PFLOW_PROTO_9?
	    PFIX_IE_flowStartSysUpTime:PFIX_IE_flowStartSeconds);
	pflowif->sc_tmpl.ipv6_tmpl.start.len = htons(4);
	/* keep in sync with SIOCSETPFLOW */
	pflowif->sc_tmpl.ipv6_tmpl.finish.field_id =
	    htons(pflowif->sc_version == PFLOW_PROTO_9?
	    PFIX_IE_flowEndSysUpTime:PFIX_IE_flowEndSeconds);
	pflowif->sc_tmpl.ipv6_tmpl.finish.len = htons(4);
	pflowif->sc_tmpl.ipv6_tmpl.src_port.field_id =
d286 2
a287 2
	pflowif->sc_tmpl.ipv6_tmpl.src_port.len = htons(2);
	pflowif->sc_tmpl.ipv6_tmpl.dest_port.field_id =
d289 2
a290 2
	pflowif->sc_tmpl.ipv6_tmpl.dest_port.len = htons(2);
	pflowif->sc_tmpl.ipv6_tmpl.tos.field_id =
d292 2
a293 2
	pflowif->sc_tmpl.ipv6_tmpl.tos.len = htons(1);
	pflowif->sc_tmpl.ipv6_tmpl.protocol.field_id =
d295 43
a337 1
	pflowif->sc_tmpl.ipv6_tmpl.protocol.len = htons(1);
d436 5
a440 2
			if (sc->sc_version == PFLOW_PROTO_9
			    || sc->sc_version == PFLOW_PROTO_10) {
d506 3
a508 10
		switch (sc->sc_version) {
		case PFLOW_PROTO_9:
			sc->sc_tmpl.set_header.set_id =
			    htons(PFLOW_V9_TMPL_SET_ID);
			sc->sc_tmpl.ipv4_tmpl.start.field_id =
			    sc->sc_tmpl.ipv6_tmpl.start.field_id =
			    htons(PFIX_IE_flowStartSysUpTime);
			sc->sc_tmpl.ipv4_tmpl.finish.field_id =
			    sc->sc_tmpl.ipv6_tmpl.finish.field_id =
			    htons(PFIX_IE_flowEndSysUpTime);
a509 15
			break;
		case PFLOW_PROTO_10:
			sc->sc_tmpl.set_header.set_id =
			    htons(PFLOW_V10_TMPL_SET_ID);
			sc->sc_tmpl.ipv4_tmpl.start.field_id =
			    sc->sc_tmpl.ipv6_tmpl.start.field_id =
			    htons(PFIX_IE_flowStartSeconds);
			sc->sc_tmpl.ipv4_tmpl.finish.field_id =
			    sc->sc_tmpl.ipv6_tmpl.finish.field_id =
			    htons(PFIX_IE_flowEndSeconds);
			pflow_sendout_ipfix_tmpl(sc);
			break;
		default:
			break;
		}
d560 11
a570 2
	sc->sc_maxcount4 = (mtu - hdrsz -
	    sizeof(struct udpiphdr)) / sizeof(struct pflow_flow4);
a572 2
	sc->sc_maxcount6 = (mtu - hdrsz -
	    sizeof(struct udpiphdr)) / sizeof(struct pflow_flow6);
d575 8
a582 4

	return (hdrsz + sizeof(struct udpiphdr) +
	    MIN(sc->sc_maxcount4 * sizeof(struct pflow_flow4),
	    sc->sc_maxcount6 * sizeof(struct pflow_flow6)));
d705 1
a705 1
copy_flow4_data(struct pflow_flow4 *flow1, struct pflow_flow4 *flow2,
d724 11
a734 23
	switch (sc->sc_version) {
	case PFLOW_PROTO_9:
		/*
		 * Pretend the flow was created or expired when the machine came
		 * up when creation is in the future of the last time a package
		 * was seen or was created / expired before this machine came up
		 * due to pfsync.
		 */
		flow1->flow_start = flow2->flow_start = st->creation < 0 ||
		    st->creation > st->expire ? htonl(0) : htonl(st->creation *
		    1000);
		flow1->flow_finish = flow2->flow_finish = st->expire < 0 ?
		    htonl(0) : htonl(st->expire * 1000);
		break;
	case PFLOW_PROTO_10:
		flow1->flow_start = flow2->flow_start = htonl(time_second -
		    (time_uptime - st->creation));
		flow1->flow_finish = flow2->flow_finish = htonl(time_second -
		    (time_uptime - st->expire));
		break;
	default: /* NOTREACHED */
		break;
	}
d741 1
a741 1
copy_flow6_data(struct pflow_flow6 *flow1, struct pflow_flow6 *flow2,
d762 87
a848 23
	switch (sc->sc_version) {
	case PFLOW_PROTO_9:
		/*
		 * Pretend the flow was created or expired when the machine came
		 * up when creation is in the future of the last time a package
		 * was seen or was created / expired before this machine came up
		 * due to pfsync.
		 */
		flow1->flow_start = flow2->flow_start = st->creation < 0 ||
		    st->creation > st->expire ? htonl(0) : htonl(st->creation *
		    1000);
		flow1->flow_finish = flow2->flow_finish = st->expire < 0 ?
		    htonl(0) : htonl(st->expire * 1000);
		break;
	case PFLOW_PROTO_10:
		flow1->flow_start = flow2->flow_start = htonl(time_second -
		    (time_uptime - st->creation));
		flow1->flow_finish = flow2->flow_finish = htonl(time_second -
		    (time_uptime - st->expire));
		break;
	default: /* NOTREACHED */
		break;
	}
d894 3
a896 1
	if (sc->sc_version == PFLOW_PROTO_9 || sc->sc_version == PFLOW_PROTO_10)
d964 62
a1025 1
copy_flow4_to_m(struct pflow_flow4 *flow, struct pflow_softc *sc)
d1032 1
a1032 1
		    pflow_get_mbuf(sc, PFLOW_TMPL_IPV4_ID)) == NULL) {
d1040 2
a1041 2
	    (sc->sc_count4 * sizeof(struct pflow_flow4)),
	    sizeof(struct pflow_flow4), flow, M_NOWAIT);
d1055 1
a1055 1
copy_flow6_to_m(struct pflow_flow6 *flow, struct pflow_softc *sc)
d1062 1
a1062 1
		    pflow_get_mbuf(sc, PFLOW_TMPL_IPV6_ID)) == NULL) {
d1070 2
a1071 2
	    (sc->sc_count6 * sizeof(struct pflow_flow6)),
	    sizeof(struct pflow_flow6), flow, M_NOWAIT);
d1111 43
d1157 3
a1159 3
	struct pflow_flow4	 flow4_1, flow4_2;
	struct pflow_flow6	 flow6_1, flow6_2;
	int			 ret = 0;
d1165 2
a1166 1
			copy_flow4_data(&flow4_1, &flow4_2, st, sk, sc, 1, 0);
d1168 2
a1169 1
			copy_flow4_data(&flow4_1, &flow4_2, st, sk, sc, 0, 1);
d1172 1
a1172 1
			ret = copy_flow4_to_m(&flow4_1, sc);
d1175 1
a1175 1
			ret = copy_flow4_to_m(&flow4_2, sc);
d1181 2
a1182 1
			copy_flow6_data(&flow6_1, &flow6_2, st, sk, sc, 1, 0);
d1184 2
a1185 1
			copy_flow6_data(&flow6_1, &flow6_2, st, sk, sc, 0, 1);
d1188 1
a1188 1
			ret = copy_flow6_to_m(&flow6_1, sc);
d1191 1
a1191 1
			ret = copy_flow6_to_m(&flow6_2, sc);
d1208 2
a1209 1
		/* ... fall through ... */
d1212 1
d1226 10
a1235 1
	pflow_sendout_ipfix(sc, AF_INET6);
d1246 4
a1249 1
	pflow_sendout_ipfix_tmpl(sc);
d1262 3
d1303 1
a1303 1
	h->time_sec = htonl(tv.tv_sec);
d1311 1
a1311 1
pflow_sendout_ipfix(struct pflow_softc *sc, sa_family_t af)
a1314 1
	struct pflow_v10_header		*h10;
d1326 2
d1335 2
d1349 28
d1379 5
d1385 1
a1385 1
		    + sc->sc_count4 * sizeof(struct pflow_flow4);
d1388 5
d1394 1
a1394 1
		    + sc->sc_count6 * sizeof(struct pflow_flow6);
d1399 8
d1409 43
a1451 34
	switch (sc->sc_version) {
	case PFLOW_PROTO_9:
		/* populate pflow_header */
		M_PREPEND(m, sizeof(struct pflow_v9_header), M_DONTWAIT);
		if (m == NULL) {
			pflowstats.pflow_onomem++;
			return (ENOBUFS);
		}
		h9 = mtod(m, struct pflow_v9_header *);
		h9->version = htons(PFLOW_PROTO_9);
		h9->count = htons(1);
		h9->uptime_ms = htonl(time_uptime * 1000);
		h9->time_sec = htonl(time_second);
		/* XXX correct mod 2^32 semantics? */
		h9->flow_sequence = htonl(sc->sc_gcounter);
		h9->observation_dom = htonl(PFLOW_ENGINE_TYPE);
		break;
	case PFLOW_PROTO_10:
		/* populate pflow_header */
		M_PREPEND(m, sizeof(struct pflow_v10_header), M_DONTWAIT);
		if (m == NULL) {
			pflowstats.pflow_onomem++;
			return (ENOBUFS);
		}
		h10 = mtod(m, struct pflow_v10_header *);
		h10->version = htons(PFLOW_PROTO_10);
		h10->length = htons(PFLOW_V10_HDRLEN + set_length);
		h10->time_sec = htonl(time_second);
		/* XXX correct mod 2^32 semantics? */
		h10->flow_sequence = htonl(sc->sc_gcounter);
		h10->observation_dom = htonl(PFLOW_ENGINE_TYPE);
		break;
	default: /* NOTREACHED */
		break;
d1453 9
a1469 1
	struct pflow_v9_header		*h9;
d1481 2
a1482 2
	if (m_copyback(m, 0, sizeof(struct pflow_tmpl),
	    &sc->sc_tmpl, M_NOWAIT)) {
d1487 6
a1492 35
	switch (sc->sc_version) {
	case PFLOW_PROTO_9:
		/* populate pflow_header */
		M_PREPEND(m, sizeof(struct pflow_v9_header), M_DONTWAIT);
		if (m == NULL) {
			pflowstats.pflow_onomem++;
			return (ENOBUFS);
		}
		h9 = mtod(m, struct pflow_v9_header *);
		h9->version = htons(PFLOW_PROTO_9);
		h9->count = htons(1);
	        h9->uptime_ms = htonl(time_uptime * 1000);
		h9->time_sec = htonl(time_second);
		/* XXX correct mod 2^32 semantics? */
		h9->flow_sequence = htonl(sc->sc_gcounter);
		h9->observation_dom = htonl(PFLOW_ENGINE_TYPE);
		break;
	case PFLOW_PROTO_10:
		/* populate pflow_header */
		M_PREPEND(m, sizeof(struct pflow_v10_header), M_DONTWAIT);
		if (m == NULL) {
			pflowstats.pflow_onomem++;
			return (ENOBUFS);
		}
		h10 = mtod(m, struct pflow_v10_header *);
		h10->version = htons(PFLOW_PROTO_10);
		h10->length = htons(PFLOW_V10_HDRLEN
		    + sizeof(struct pflow_tmpl));
		h10->time_sec = htonl(time_second);
		/* XXX correct mod 2^32 semantics? */
		h10->flow_sequence = htonl(sc->sc_gcounter);
		h10->observation_dom = htonl(PFLOW_ENGINE_TYPE);
		break;
	default: /* NOTREACHED */
		break;
d1494 8
@


1.32
log
@bring back pflow v10; broken in previous commit, sorry.
ok benno@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.31 2013/05/31 22:46:47 florian Exp $	*/
d272 6
@


1.32.2.1
log
@MFC:

This is http://ftp.openbsd.org/pub/OpenBSD/patches/5.4/common/001_pflow.patch

"A crash can happen on pflow(4) interface destruction."

ok sthen
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.32 2013/07/05 17:14:27 florian Exp $	*/
a271 6
	if (timeout_initialized(&sc->sc_tmo))
		timeout_del(&sc->sc_tmo);
	if (timeout_initialized(&sc->sc_tmo6))
		timeout_del(&sc->sc_tmo6);
	if (timeout_initialized(&sc->sc_tmo_tmpl))
		timeout_del(&sc->sc_tmo_tmpl);
@


1.31
log
@export the original aka untranslated address for af-to in pflow
inspired by benno@@'s previous diff for nat-to
tests/ok benno@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.30 2013/05/30 20:20:58 benno Exp $	*/
d721 1
@


1.30
log
@export the original aka untranslated address in pflow
ok florian@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.29 2013/05/03 15:33:47 florian Exp $	*/
d92 1
a92 1
	struct pf_state *, int, int);
d94 2
a95 1
	struct pf_state *, struct pflow_softc *, int, int);
d97 6
a102 3
	struct pf_state *, struct pflow_softc *, int, int);
int	pflow_pack_flow(struct pf_state *, struct pflow_softc *);
int	pflow_pack_flow_ipfix(struct pf_state *, struct pflow_softc *);
d104 2
a105 1
int	export_pflow_if(struct pf_state*, struct pflow_softc *);
d572 1
a572 1
    struct pf_state *st, int src, int dst)
a573 3
	struct pf_state_key	*sk = st->key[st->direction == PF_IN ?
					PF_SK_WIRE : PF_SK_STACK];

d609 2
a610 1
    struct pf_state *st, struct pflow_softc *sc, int src, int dst)
a611 3
	struct pf_state_key	*sk = st->key[st->direction == PF_IN ?
					PF_SK_WIRE : PF_SK_STACK];

d657 2
a658 1
    struct pf_state *st, struct pflow_softc *sc, int src, int dst)
a659 3
	struct pf_state_key	*sk = st->key[st->direction == PF_IN ?
					PF_SK_WIRE : PF_SK_STACK];

d709 3
a711 1
	struct pf_state_key	*sk = st->key[PF_SK_WIRE];
d717 1
a717 1
				export_pflow_if(st, sc);
a720 1
		case PFLOW_PROTO_10:
d722 1
a722 1
				export_pflow_if(st, sc);
d733 2
a734 1
export_pflow_if(struct pf_state *st, struct pflow_softc *sc)
d745 1
a745 1
		return (pflow_pack_flow_ipfix(st, sc));
d750 1
a750 1
		return (pflow_pack_flow(st, sc));
d761 1
a761 1
		if ((ret = pflow_pack_flow(&pfs_copy, sc)) != 0)
d771 1
a771 1
		if ((ret = pflow_pack_flow(&pfs_copy, sc)) != 0)
d780 1
a780 1
	return (pflow_pack_flow(&pfs_copy, sc));
d873 2
a874 1
pflow_pack_flow(struct pf_state *st, struct pflow_softc *sc)
d884 1
a884 1
		copy_flow_data(&flow1, &flow2, st, 1, 0);
d886 1
a886 1
		copy_flow_data(&flow1, &flow2, st, 0, 1);
d898 2
a899 1
pflow_pack_flow_ipfix(struct pf_state *st, struct pflow_softc *sc)
a900 1
	struct pf_state_key	*sk = st->key[PF_SK_WIRE];
d909 1
a909 1
			copy_flow4_data(&flow4_1, &flow4_2, st, sc, 1, 0);
d911 1
a911 1
			copy_flow4_data(&flow4_1, &flow4_2, st, sc, 0, 1);
d923 1
a923 1
			copy_flow6_data(&flow6_1, &flow6_2, st, sc, 1, 0);
d925 1
a925 1
			copy_flow6_data(&flow6_1, &flow6_2, st, sc, 0, 1);
@


1.29
log
@Export ingress/egress interface index in pflow(4).
Report that this is needed for some netflow collector and tests by
Chris Ivancic & Colin Ligertwood.

OK mikeb@@, benno@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.28 2013/04/10 08:50:59 mpi Exp $	*/
d569 2
a570 1
	struct pf_state_key	*sk = st->key[PF_SK_WIRE];
d609 2
a610 1
	struct pf_state_key	*sk = st->key[PF_SK_WIRE];
d659 3
a661 1
	struct pf_state_key	*sk = st->key[PF_SK_WIRE];
@


1.28
log
@Remove various external variable declaration from sources files and
move them to the corresponding header with an appropriate comment if
necessary.

ok guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.27 2013/03/28 23:10:05 tedu Exp $	*/
d155 6
d200 6
d578 4
a581 2
	flow1->if_index_out = flow2->if_index_in =
	    flow1->if_index_in = flow2->if_index_out = 0;
d615 5
d664 5
@


1.27
log
@code that calls timeout functions should include timeout.h
slipped by on i386, but the zaurus doesn't automagically pick it up.
spotted by patrick
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.26 2013/03/28 16:45:16 tedu Exp $	*/
a107 4

/* from in_pcb.c */
extern int ipport_hifirstauto;
extern int ipport_hilastauto;
@


1.26
log
@no need for a lot of code to include proc.h
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.25 2013/03/26 13:19:25 mpi Exp $	*/
d28 1
@


1.25
log
@Remove various read-only *maxlen variables and use IFQ_MAXLEN directly.

ok beck@@, mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.24 2013/02/05 11:58:39 florian Exp $	*/
a29 1
#include <sys/proc.h>
@


1.24
log
@netflow v10 omitted the sysUpTime flow set header field from
v9. Without it it's not possible to find out at what time a flow
started/ended with only flowStartSysUpTime/flowEndSysUpTime. Fix this
by using flowStartSeconds/flowEndSeconds for v10.
Problem reported by Chris Ivancic and Colin Ligertwood, analyzed by
benno@@
Tested by benno@@ against nprobe (which doesn't care that much one way
or the other)
Tested by Chris Ivancic against solarwinds collector.
OK benno@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.23 2013/01/16 09:53:19 dlg Exp $	*/
d234 1
a234 1
	IFQ_SET_MAXLEN(&ifp->if_snd, ifqmaxlen);
@


1.24.2.1
log
@MFC:

This is http://ftp.openbsd.org/pub/OpenBSD/patches/5.3/common/008_pflow.patch

"A crash can happen on pflow(4) interface destruction."

ok sthen
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.24 2013/02/05 11:58:39 florian Exp $	*/
a258 6
	if (timeout_initialized(&sc->sc_tmo))
		timeout_del(&sc->sc_tmo);
	if (timeout_initialized(&sc->sc_tmo6))
		timeout_del(&sc->sc_tmo6);
	if (timeout_initialized(&sc->sc_tmo_tmpl))
		timeout_del(&sc->sc_tmo_tmpl);
@


1.23
log
@switch from using softclock ticks to getnanotime when putting time on the
wire for v5 packets.

ok (and lots of gentle prodding from) florian@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.22 2012/11/08 18:06:49 gsoares Exp $	*/
d94 1
a94 1
	struct pf_state *, int, int);
d96 1
a96 1
	struct pf_state *, int, int);
d165 1
d167 2
a168 1
	    htons(PFIX_IE_flowStartSysUpTime);
d170 1
d172 2
a173 1
	    htons(PFIX_IE_flowEndSysUpTime);
d204 1
d206 2
a207 1
	    htons(PFIX_IE_flowStartSysUpTime);
d209 1
d211 2
a212 1
	    htons(PFIX_IE_flowEndSysUpTime);
d385 13
a397 2
		if (sc->sc_version == PFLOW_PROTO_9
		    || sc->sc_version == PFLOW_PROTO_10) {
d399 7
a405 2
			    htons(sc->sc_version == PFLOW_PROTO_9?
				PFLOW_V9_TMPL_SET_ID:PFLOW_V10_TMPL_SET_ID);
d407 3
d596 1
a596 1
    struct pf_state *st, int src, int dst)
d610 23
a632 9
	/*
	 * Pretend the flow was created or expired when the machine came up
	 * when creation is in the future of the last time a package was seen
	 * or was created / expired before this machine came up due to pfsync.
	 */
	flow1->flow_start = flow2->flow_start = st->creation < 0 ||
	    st->creation > st->expire ? htonl(0) : htonl(st->creation * 1000);
	flow1->flow_finish = flow2->flow_finish = st->expire < 0 ? htonl(0) :
	    htonl(st->expire * 1000);
d640 1
a640 1
    struct pf_state *st, int src, int dst)
d655 23
a677 9
	/*
	 * Pretend the flow was created or expired when the machine came up
	 * when creation is in the future of the last time a package was seen
	 * or was created / expired before this machine came up due to pfsync.
	 */
	flow1->flow_start = flow2->flow_start = st->creation < 0 ||
	    st->creation > st->expire ? htonl(0) : htonl(st->creation * 1000);
	flow1->flow_finish = flow2->flow_finish = st->expire < 0 ? htonl(0) :
	    htonl(st->expire * 1000);
d884 1
a884 1
			copy_flow4_data(&flow4_1, &flow4_2, st, 1, 0);
d886 1
a886 1
			copy_flow4_data(&flow4_1, &flow4_2, st, 0, 1);
d898 1
a898 1
			copy_flow6_data(&flow6_1, &flow6_2, st, 1, 0);
d900 1
a900 1
			copy_flow6_data(&flow6_1, &flow6_2, st, 0, 1);
@


1.22
log
@wrap bpfilter portion with #if NBPFILTER > 0.
fix kernel builds without bpfilter.

OK sthen@@ mikeb@@ deraadt@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.21 2012/10/30 12:09:05 florian Exp $	*/
a115 3
/* from kern/kern_clock.c; incremented each clock tick. */
extern int ticks;

d925 1
d944 4
a947 2
	h->time_sec = htonl(time_second);
	h->time_nanosec = htonl(ticks);
@


1.21
log
@Use time_uptime for expiration values as time_second can be skewed at
runtime while time_uptime is monotonic. Prevent underflows in
pfsync(4) and pflow(4) by using signed variables.  pfsync(4) problem
pointed out by camield.

Diff originally by dlg, frag and pflow bits by me.

feedback dlg
man page tweak jmc

Various versions of the pflow bits tested by Hrvoje Popovski
(hrvoje AT srce DOT hr), thanks!

ok benno, henning, dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.20 2012/04/11 17:42:53 mikeb Exp $	*/
d1109 1
d1111 1
@


1.20
log
@fix all the suser calls which pass an incorrect p_acflag argument;
figured out by and ok guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.19 2012/02/02 12:34:37 benno Exp $	*/
d556 9
a564 6
	flow1->flow_start = flow2->flow_start =
	    htonl(st->creation * 1000);
	flow1->flow_finish = flow2->flow_finish =
	    htonl((time_uptime - (st->rule.ptr->timeout[st->timeout] ?
	    st->rule.ptr->timeout[st->timeout] :
	    pf_default_rule.timeout[st->timeout])) * 1000);
d586 9
a594 6
	flow1->flow_start = flow2->flow_start =
	    htonl(st->creation * 1000);
	flow1->flow_finish = flow2->flow_finish =
	    htonl((time_uptime - (st->rule.ptr->timeout[st->timeout] ?
	    st->rule.ptr->timeout[st->timeout] :
	    pf_default_rule.timeout[st->timeout])) * 1000);
d617 9
a625 6
	flow1->flow_start = flow2->flow_start =
	    htonl(st->creation * 1000);
	flow1->flow_finish = flow2->flow_finish =
	    htonl((time_uptime - (st->rule.ptr->timeout[st->timeout] ?
	    st->rule.ptr->timeout[st->timeout] :
	    pf_default_rule.timeout[st->timeout])) * 1000);
@


1.19
log
@add netflow v9/ipfix support to pflow(4).
large parts written by Florian Obser (florian -at- narrans -dot- de).
feedback from sperreault@@ gollo@@ sthen@@
ok from gollo@@ dlg@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.18 2011/11/25 12:52:10 dlg Exp $	*/
d348 1
a348 1
		if ((error = suser(p, p->p_acflag)) != 0)
@


1.18
log
@use time_uptime to set state creation values as time_second can be
skewed at runtime by things like date(1) and ntpd. time_uptime is
monotonic and therefore more useful to compare against.

ok deraadt@@ mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.17 2011/07/09 04:11:15 dhill Exp $	*/
d4 2
d74 2
d82 5
a86 2
struct mbuf *pflow_get_mbuf(struct pflow_softc *);
int	pflow_sendout(struct pflow_softc *);
d89 2
d93 4
d98 1
d102 2
a135 3
	pflowif->sc_sender_ip.s_addr = INADDR_ANY;
	pflowif->sc_sender_port = pflow_get_dynport();

d144 78
d234 1
a234 1
	timeout_set(&pflowif->sc_tmo, pflow_timeout, pflowif);
d254 1
a254 1
	pflow_sendout(sc);
d312 7
d329 1
a329 1
			pflow_sendout(sc);
d340 1
d353 11
d365 1
a365 3
		s = splnet();
		pflow_sendout(sc);
		splx(s);
d373 16
d407 45
d461 21
a481 7
	sc->sc_maxcount = (mtu - sizeof(struct pflow_header) -
	    sizeof (struct udpiphdr)) / sizeof(struct pflow_flow);
	if (sc->sc_maxcount > PFLOW_MAXFLOWS)
	    sc->sc_maxcount = PFLOW_MAXFLOWS;
	sc->sc_if.if_mtu = sizeof(struct pflow_header) +
	    sizeof (struct udpiphdr) + 
	    sc->sc_maxcount * sizeof(struct pflow_flow);
d485 1
a485 1
pflow_get_mbuf(struct pflow_softc *sc)
d487 1
d507 22
a528 9
	/* populate pflow_header */
	h.reserved1 = 0;
	h.reserved2 = 0;
	h.count = 0;
	h.version = htons(PFLOW_VERSION);
	h.flow_sequence = htonl(sc->sc_gcounter);
	h.engine_type = PFLOW_ENGINE_TYPE;
	h.engine_id = PFLOW_ENGINE_ID;
	m_copyback(m, 0, PFLOW_HDRLEN, &h, M_NOWAIT);
a529 2
	sc->sc_count = 0;
	timeout_add_sec(&sc->sc_tmo, PFLOW_TIMEOUT);
d567 55
a627 3
	if (sk->af != AF_INET)
		return (0);

d629 14
a642 1
		export_pflow_if(st, sc);
d659 4
d705 1
a705 1
		if ((sc->sc_mbuf = pflow_get_mbuf(sc)) == NULL) {
d711 2
a712 2
	    (sc->sc_count * sizeof (struct pflow_flow)),
	    sizeof (struct pflow_flow), flow, M_NOWAIT);
d720 62
a781 1
		ret = pflow_sendout(sc);
d811 39
d857 33
a889 1
	pflow_sendout(sc);
d894 19
d914 1
a914 1
pflow_sendout(struct pflow_softc *sc)
d943 152
d1117 1
a1117 1
	ui->ui_ulen = htons(sizeof (struct udphdr) + len);
d1126 1
a1126 1
	ip->ip_len = htons(sizeof (struct udpiphdr) + len);
@


1.17
log
@honor the net.inet.udp.checksum setting.

ok claudio henning yasuoka
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.16 2011/07/06 02:42:28 henning Exp $	*/
d362 1
a362 1
	    htonl((st->creation - (time_second - time_uptime)) * 1000);
@


1.16
log
@cosnistently use IFQ_SET_MAXLEN, surfaced in a discussion with + ok bluhm
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.15 2011/04/05 18:01:21 henning Exp $	*/
d97 3
d565 7
a571 4
	m->m_pkthdr.csum_flags |= M_UDP_CSUM_OUT;
	ui->ui_sum = in_cksum_phdr(ui->ui_src.s_addr,
	    ui->ui_dst.s_addr, htons(len + sizeof(struct udphdr) +
	    IPPROTO_UDP));
@


1.15
log
@mechanic rename M_{TCP|UDP}V4_CSUM_OUT -> M_{TCP|UDP}_CSUM_OUT
ok claudio krw
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.14 2010/07/02 02:40:16 blambert Exp $	*/
d135 1
a135 1
	ifp->if_snd.ifq_maxlen = ifqmaxlen;
@


1.14
log
@m_copyback can fail to allocate memory, but is a void fucntion so gymnastics
are required to detect that.

Change the function to take a wait argument (used in nfs server, but
M_NOWAIT everywhere else for now) and to return an error

ok claudio@@ henning@@ krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.13 2010/04/20 22:05:43 tedu Exp $	*/
d562 1
a562 1
	m->m_pkthdr.csum_flags |= M_UDPV4_CSUM_OUT;
@


1.13
log
@remove proc.h include from uvm_map.h.  This has far reaching effects, as
sysctl.h was reliant on this particular include, and many drivers included
sysctl.h unnecessarily.  remove sysctl.h or add proc.h as needed.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.12 2010/01/12 02:47:07 claudio Exp $	*/
d328 1
a328 1
	m_copyback(m, 0, PFLOW_HDRLEN, &h);
d445 1
a445 1
	    sizeof (struct pflow_flow), flow);
@


1.12
log
@Remove bpfdetach() call right in front of the if_detach() call since
bpfdetach() will be called in if_detach(). Diff by Gleydson Soares
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.11 2009/06/17 06:35:30 gollo Exp $	*/
d23 1
d28 1
@


1.11
log
@fix flow data values: first and last time, found by f-kons at yandex ru

OK: sthen@@, henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.10 2009/02/27 11:09:36 gollo Exp $	*/
a158 3
#if NBPFILTER > 0
	bpfdetach(ifp);
#endif
@


1.10
log
@fix mbuf problems and simplify code, well spotted and input by
Alexander Sabourenkov. mbuf logic is based on claudio's recommendation

Tested by Alexander Sabourenkov

OK: henning@@, claudio@@
Theo: "In please..."
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.9 2009/01/03 21:47:32 gollo Exp $	*/
d359 6
a364 2
	flow1->flow_start = flow2->flow_start = htonl(st->creation * 1000);
	flow1->flow_finish = flow2->flow_finish = htonl(time_second * 1000);
@


1.9
log
@sync flow sequence ids on all used pflow interfaces.

OK henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.8 2008/11/26 18:01:43 dlg Exp $	*/
d76 1
a76 1
struct mbuf *pflow_get_mbuf(struct pflow_softc *, void **);
d85 1
d290 2
a291 2
	sc->sc_maxcount = (mtu - sizeof(struct pflow_header)) /
	    sizeof(struct pflow_flow);
d295 1
d300 1
a300 1
pflow_get_mbuf(struct pflow_softc *sc, void **sp)
d302 2
a303 3
	struct pflow_header	*h;
	struct mbuf		*m, *top = NULL, **mp = &top;
	int			len, totlen;
d307 1
a307 1
		sc->sc_if.if_oerrors++;
d311 5
a315 29
	len = MHLEN;
	totlen = (sc->sc_maxcount * sizeof(struct pflow_flow)) +
	    sizeof(struct pflow_header);

	while (totlen > 0) {
		if (top) {
			MGET(m, M_DONTWAIT, MT_DATA);
			if (m == 0) {
				m_freem(top);
				sc->sc_if.if_oerrors++;
				return (NULL);
			}
			len = MLEN;
		}
		if (totlen >= MINCLSIZE) {
			MCLGET(m, M_DONTWAIT);
			if (m->m_flags & M_EXT)
				len = MCLBYTES;
			else {
				m_free(m);
				m_freem(top);
				sc->sc_if.if_oerrors++;
				return (NULL);
			}
		}
		m->m_len = len = min(totlen, len);
		totlen -= len;
		*mp = m;
		mp = &m->m_next;
d318 2
a319 2
	top->m_pkthdr.rcvif = NULL;
	top->m_len = m->m_pkthdr.len = sizeof(struct pflow_header);
d322 8
a329 8
	h = mtod(top, struct pflow_header *);
	h->reserved1 = 0;
	h->reserved2 = 0;
	h->count = 0;
	h->version = htons(PFLOW_VERSION);
	h->flow_sequence = htonl(sc->sc_gcounter);
	h->engine_type = PFLOW_ENGINE_TYPE;
	h->engine_id = PFLOW_ENGINE_ID;
a331 1
	*sp = (void *)((char *)h + PFLOW_HDRLEN);
d333 1
a333 1
	return (top);
d342 4
a345 6
	flow1->src_ip = flow2->dest_ip =
	    st->key[PF_SK_WIRE]->addr[src].v4.s_addr;
	flow1->src_port = flow2->dest_port = st->key[PF_SK_WIRE]->port[src];
	flow1->dest_ip = flow2->src_ip =
	    st->key[PF_SK_WIRE]->addr[dst].v4.s_addr;
	flow1->dest_port = flow2->src_port = st->key[PF_SK_WIRE]->port[dst];
d370 4
d386 1
a386 1
	struct ifnet		*ifp = NULL;
a389 1
	ifp = &sc->sc_if;
d429 1
a429 1
pflow_pack_flow(struct pf_state *st, struct pflow_softc *sc)
d431 1
a431 7
	struct pflow_flow	*flow1 = NULL;
	struct pflow_flow	 flow2;
	struct pf_state_key	*sk = st->key[PF_SK_WIRE];
	int			 s, ret = 0;

	if (sk->af != AF_INET)
		return (0);
a433 1

d435 1
a435 2
		if ((sc->sc_mbuf = pflow_get_mbuf(sc,
		    (void **)&sc->sc_flowp.s)) == NULL) {
d437 1
a437 1
			return (ENOMEM);
d440 3
d449 15
a463 4
	flow1 = sc->sc_flowp.s++;
	sc->sc_mbuf->m_pkthdr.len =
	    sc->sc_mbuf->m_len += sizeof(struct pflow_flow);
	bzero(flow1, sizeof(*flow1));
d467 1
a467 1
		copy_flow_data(flow1, &flow2, st, 1, 0);
d469 1
a469 1
		copy_flow_data(flow1, &flow2, st, 0, 1);
d471 2
a472 25
	if (st->bytes[0] != 0) { /* first flow from state */
		if (sc->sc_count >= sc->sc_maxcount)
			ret = pflow_sendout(sc);

		if (st->bytes[1] != 0) {
			/* one more flow, second part from state */
			if (sc->sc_mbuf == NULL) {
				if ((sc->sc_mbuf = pflow_get_mbuf(sc,
				    (void **)&sc->sc_flowp.s)) == NULL) {
					splx(s);
					return (ENOMEM);
				}
			}

			if (pflowstats.pflow_flows == sc->sc_gcounter)
				pflowstats.pflow_flows++;
			sc->sc_gcounter++;
			sc->sc_count++;

			flow1 = sc->sc_flowp.s++;
			sc->sc_mbuf->m_pkthdr.len =
			    sc->sc_mbuf->m_len += sizeof(struct pflow_flow);
			bzero(flow1, sizeof(*flow1));
		}
	}
d474 2
a475 5
	if (st->bytes[1] != 0) { /* second flow from state */
		bcopy(&flow2, flow1, sizeof(*flow1));
		if (sc->sc_count >= sc->sc_maxcount)
			ret = pflow_sendout(sc);
	}
a476 1
	splx(s);
a504 1
	sc->sc_flowp.s = NULL;
d529 1
d535 1
a535 1
		return (0);
d571 4
a574 1
	if (ip_output(m, NULL, NULL, IP_RAWOUTPUT, &sc->sc_imo, NULL))
d576 3
a578 1
	return (0);
@


1.8
log
@dont have bpf.h expose the kernel ticks variable wherever it is includeing.

it is very confusing like this.

ok deraadt@@ canacar@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.7 2008/10/28 15:51:27 gollo Exp $	*/
d215 1
a215 1
		    sc->sc_receiver_port != 0)
d217 2
a218 1
		else
d265 1
a265 1
		    sc->sc_receiver_port != 0)
d267 2
a268 1
		else
d472 2
a473 1
	pflowstats.pflow_flows++;
d502 2
a503 1
			pflowstats.pflow_flows++;
@


1.7
log
@add support for multiple pflow(4) interfaces

OK: claudio@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.6 2008/10/21 11:01:29 gollo Exp $	*/
d93 3
@


1.6
log
@add bpf/tcpdump support to pflow(4)

ok canacar@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.5 2008/09/17 22:18:00 gollo Exp $	*/
d64 1
a64 1
struct pflow_softc	*pflowif = NULL;
d82 1
a82 1
int	pflow_pack_flow(struct pf_state *);
d84 1
d97 1
d104 2
a105 4
	struct ifnet	*ifp;

	if (unit != 0)
		return (EINVAL);
d142 2
d150 2
a151 3
	struct pflow_softc *sc = ifp->if_softc;

	timeout_del(&sc->sc_tmo);
d153 2
d159 4
a162 3
	free(pflowif->sc_imo.imo_membership, M_IPMOPTS);
	free(pflowif, M_DEVBUF);
	pflowif = NULL;
d248 3
a250 6
		if ((ifp->if_flags & IFF_UP) && sc->sc_receiver_ip.s_addr != 0
		    && sc->sc_receiver_port != 0) {
			s = splnet();
			pflow_sendout(sc);
			splx(s);
		}
d390 12
a402 1
	struct pflow_softc	*sc = pflowif;
a406 3
	if (sc == NULL)
		return (0);

d408 1
a408 1
	if (!(ifp->if_flags & IFF_UP))
d413 1
a413 1
		return pflow_pack_flow(st);
d424 1
a424 1
		if ((ret = pflow_pack_flow(&pfs_copy)) != 0)
d434 1
a434 1
		if ((ret = pflow_pack_flow(&pfs_copy)) != 0)
d443 1
a443 1
	return (pflow_pack_flow(&pfs_copy));
d447 1
a447 1
pflow_pack_flow(struct pf_state *st)
a448 1
	struct pflow_softc	*sc = pflowif;
@


1.5
log
@Solve m_free problem with a not correctly configured pflow interface
leading to a kernel crash reported in PR5930

OK claudio@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.4 2008/09/17 20:25:41 gollo Exp $	*/
d138 4
a525 1
#if NBPFILTER > 0
a526 1
#endif
a548 5
#if NBPFILTER > 0
	if (ifp->if_bpf)
		bpf_mtap(ifp->if_bpf, m, BPF_DIRECTION_OUT);
#endif

d556 3
a558 1
	int		 len = m->m_pkthdr.len;
a568 1
	ui->ui_len = htons((u_int16_t) len + sizeof (struct udphdr));
d573 1
a573 1
	ui->ui_ulen = ui->ui_len;
d575 8
a582 7
	((struct ip *)ui)->ip_v = IPVERSION;
	((struct ip *)ui)->ip_hl = sizeof(struct ip) >> 2;
	((struct ip *)ui)->ip_id = htons(ip_randomid());
	((struct ip *)ui)->ip_off = htons(IP_DF);
	((struct ip *)ui)->ip_tos = IPTOS_LOWDELAY;
	((struct ip *)ui)->ip_ttl = IPDEFTTL;
	((struct ip *)ui)->ip_len = htons(sizeof (struct udpiphdr) + len);
d590 9
a598 2
	    ui->ui_dst.s_addr, htons((u_int16_t)len +
	    sizeof(struct udphdr) + IPPROTO_UDP));
@


1.4
log
@fix whitespaces

ok henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.3 2008/09/16 15:48:12 gollo Exp $	*/
d520 1
a520 1
	struct mbuf		*m;
d528 1
a528 1
	if (sc->sc_mbuf == NULL)
d531 2
a532 2
	pflowstats.pflow_packets++;

d538 1
a538 3
	m = sc->sc_mbuf;
	sc->sc_mbuf = NULL;
	sc->sc_flowp.s = NULL;
@


1.3
log
@netstat statistics for pflow(4) via pseudo family

ok cluadio@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.2 2008/09/16 13:58:55 gollo Exp $	*/
d588 2
a589 2
 	 * Compute the pseudo-header checksum; defer further checksumming
 	 * until ip_output() or hardware (if it exists).
@


1.2
log
@fix whitespace/tab typos

ok henning@@ claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_pflow.c,v 1.1 2008/09/09 13:56:39 henning Exp $	*/
d622 19
@


1.1
log
@welcome pflow(4), a netflow v5 compatible flow export interface.
flows export data gathered from pf states.
initial implementation by Joerg Goltermann <jg@@osn.de>, guidance and many
changes by me. 'put it in' theo
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d588 3
a590 3
  	 * Compute the pseudo-header checksum; defer further checksumming
  	 * until ip_output() or hardware (if it exists).
  	 */
@

