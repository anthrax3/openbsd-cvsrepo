head	1.63;
access;
symbols
	OPENBSD_6_1:1.58.0.4
	OPENBSD_6_1_BASE:1.58
	OPENBSD_6_0:1.50.0.4
	OPENBSD_6_0_BASE:1.50
	OPENBSD_5_9:1.39.0.2
	OPENBSD_5_9_BASE:1.39
	OPENBSD_5_8:1.1.0.4
	OPENBSD_5_8_BASE:1.1;
locks; strict;
comment	@ * @;


1.63
date	2017.09.05.11.15.39;	author mpi;	state Exp;
branches;
next	1.62;
commitid	WJ8dXwv0h5Tfqjv6;

1.62
date	2017.09.05.10.56.04;	author mpi;	state Exp;
branches;
next	1.61;
commitid	0vtFcd5sX2B78S2p;

1.61
date	2017.07.30.18.18.08;	author florian;	state Exp;
branches;
next	1.60;
commitid	y8TT4YTxTvaF1bH8;

1.60
date	2017.07.30.18.16.14;	author florian;	state Exp;
branches;
next	1.59;
commitid	z0X2vKcxAWZuAxov;

1.59
date	2017.05.11.14.03.19;	author mpi;	state Exp;
branches;
next	1.58;
commitid	MN7RbcuFGMzEGcY9;

1.58
date	2017.02.28.09.50.13;	author mpi;	state Exp;
branches;
next	1.57;
commitid	15ZiiVy1UrhNDMlv;

1.57
date	2017.01.24.10.08.30;	author krw;	state Exp;
branches;
next	1.56;
commitid	6c6qq5OdS4VVnyVM;

1.56
date	2016.11.20.11.46.45;	author mpi;	state Exp;
branches;
next	1.55;
commitid	BDxTsoC7qPCCxoxV;

1.55
date	2016.11.20.11.40.58;	author mpi;	state Exp;
branches;
next	1.54;
commitid	3UWH2O8lhF8HTUbi;

1.54
date	2016.11.14.10.32.46;	author mpi;	state Exp;
branches;
next	1.53;
commitid	to0Je2i4V2FtpmS1;

1.53
date	2016.11.14.08.54.19;	author mpi;	state Exp;
branches;
next	1.52;
commitid	SPJDraPFwXOZQLLT;

1.52
date	2016.09.07.09.36.49;	author mpi;	state Exp;
branches;
next	1.51;
commitid	maggOul7Gx8kGBXs;

1.51
date	2016.08.30.07.42.57;	author jmatthew;	state Exp;
branches;
next	1.50;
commitid	wt6F4HnTFVjJdyuV;

1.50
date	2016.07.19.10.51.44;	author mpi;	state Exp;
branches;
next	1.49;
commitid	yRNJr6JjtSzwhCMC;

1.49
date	2016.07.04.08.11.48;	author mpi;	state Exp;
branches;
next	1.48;
commitid	cJAFZjtalegoa3C8;

1.48
date	2016.06.22.06.32.32;	author dlg;	state Exp;
branches;
next	1.47;
commitid	ZyonIGZ7QrSF1GGx;

1.47
date	2016.06.14.04.42.02;	author jmatthew;	state Exp;
branches;
next	1.46;
commitid	s3pvPwhE6w34MZJL;

1.46
date	2016.06.07.01.31.54;	author tedu;	state Exp;
branches;
next	1.45;
commitid	37V3kDRiYRccnYvB;

1.45
date	2016.06.01.09.46.19;	author dlg;	state Exp;
branches;
next	1.44;
commitid	nJMw0AroEyWNkbZj;

1.44
date	2016.06.01.06.31.52;	author dlg;	state Exp;
branches;
next	1.43;
commitid	ZsmrEqqwCvrvnlnk;

1.43
date	2016.06.01.06.19.06;	author dlg;	state Exp;
branches;
next	1.42;
commitid	f3s29nGcU8u2IWqh;

1.42
date	2016.05.18.03.46.03;	author dlg;	state Exp;
branches;
next	1.41;
commitid	q5zkugIMulsP5tHa;

1.41
date	2016.05.02.22.15.49;	author jmatthew;	state Exp;
branches;
next	1.40;
commitid	Qe5v796bHPmQNEGf;

1.40
date	2016.04.13.08.04.14;	author mpi;	state Exp;
branches;
next	1.39;
commitid	no0HF32SpYAdh1Nd;

1.39
date	2016.02.24.22.41.53;	author mpi;	state Exp;
branches;
next	1.38;
commitid	eJx9U9f0eROcBU02;

1.38
date	2016.01.18.18.27.12;	author mpi;	state Exp;
branches;
next	1.37;
commitid	KBXvsgMO1460dNbP;

1.37
date	2016.01.18.15.38.52;	author mpi;	state Exp;
branches;
next	1.36;
commitid	wTiB8mlj2Llq0CHS;

1.36
date	2015.12.21.10.51.55;	author mpi;	state Exp;
branches;
next	1.35;
commitid	GLrMQPKEtr2rJZmH;

1.35
date	2015.12.16.13.19.14;	author mpi;	state Exp;
branches;
next	1.34;
commitid	hYQKZbzoQ031Vmwi;

1.34
date	2015.12.15.13.08.50;	author mpi;	state Exp;
branches;
next	1.33;
commitid	FvQNKISmE6wcoxFO;

1.33
date	2015.12.04.13.42.48;	author mpi;	state Exp;
branches;
next	1.32;
commitid	3gDu3mmUYYoaLpwn;

1.32
date	2015.12.03.21.57.59;	author mpi;	state Exp;
branches;
next	1.31;
commitid	nmbu8xP0zmz2PPN0;

1.31
date	2015.12.02.16.49.58;	author bluhm;	state Exp;
branches;
next	1.30;
commitid	SB6GJECP4v4xqpvF;

1.30
date	2015.12.02.11.09.01;	author mpi;	state Exp;
branches;
next	1.29;
commitid	q7sXSVgSK7Jdk6sA;

1.29
date	2015.12.02.09.17.47;	author mpi;	state Exp;
branches;
next	1.28;
commitid	qJCvM1BEJpKmZS7r;

1.28
date	2015.11.29.16.02.18;	author mpi;	state Exp;
branches;
next	1.27;
commitid	naczucYAeKE2Fq3l;

1.27
date	2015.11.27.12.13.22;	author mpi;	state Exp;
branches;
next	1.26;
commitid	zm84IHWJ1XZcahhH;

1.26
date	2015.11.27.11.52.44;	author mpi;	state Exp;
branches;
next	1.25;
commitid	b6yEC4HO3KJwk5bP;

1.25
date	2015.11.24.12.06.30;	author mpi;	state Exp;
branches;
next	1.24;
commitid	EtocpAAodAId7blH;

1.24
date	2015.11.10.10.23.27;	author mpi;	state Exp;
branches;
next	1.23;
commitid	nXhvmQzJFCbK3ydu;

1.23
date	2015.11.09.12.15.29;	author mpi;	state Exp;
branches;
next	1.22;
commitid	3AJpxBztuBu0IZI5;

1.22
date	2015.11.06.17.55.55;	author mpi;	state Exp;
branches;
next	1.21;
commitid	rF3eCQCnoqyAKLl5;

1.21
date	2015.11.06.17.44.45;	author mpi;	state Exp;
branches;
next	1.20;
commitid	ovggDYXsChxiUYyD;

1.20
date	2015.11.06.15.26.44;	author mpi;	state Exp;
branches;
next	1.19;
commitid	ySdqL2roS1M5HSBj;

1.19
date	2015.11.04.10.11.45;	author mpi;	state Exp;
branches;
next	1.18;
commitid	gKZYBMPSu3S22SGg;

1.18
date	2015.11.04.09.50.21;	author mpi;	state Exp;
branches;
next	1.17;
commitid	vubJzb8H8Cr58s7n;

1.17
date	2015.11.04.09.48.09;	author mpi;	state Exp;
branches;
next	1.16;
commitid	3A8tIMIcEEWayEz4;

1.16
date	2015.11.02.14.40.09;	author mpi;	state Exp;
branches;
next	1.15;
commitid	RcfM7kXINWo9mOSk;

1.15
date	2015.10.25.14.48.51;	author mpi;	state Exp;
branches;
next	1.14;
commitid	K8LYM6fzMmJYhMbi;

1.14
date	2015.10.22.17.19.38;	author mpi;	state Exp;
branches;
next	1.13;
commitid	AaBRgz5QXsDW1oOO;

1.13
date	2015.10.21.08.47.01;	author mpi;	state Exp;
branches;
next	1.12;
commitid	QLDDX3rXClLqZwFq;

1.12
date	2015.10.14.10.09.30;	author mpi;	state Exp;
branches;
next	1.11;
commitid	PpW3O2bSAUrWDZcl;

1.11
date	2015.10.07.11.39.49;	author mpi;	state Exp;
branches;
next	1.10;
commitid	fODLQsgZWtPvPooq;

1.10
date	2015.10.07.10.50.35;	author mpi;	state Exp;
branches;
next	1.9;
commitid	hPrd2a6fiZdlN2yP;

1.9
date	2015.10.07.08.43.36;	author mpi;	state Exp;
branches;
next	1.8;
commitid	iugr8QHoKuxcwPS9;

1.8
date	2015.09.28.08.47.53;	author mpi;	state Exp;
branches;
next	1.7;
commitid	KqoLx6DuN3v4W4Zw;

1.7
date	2015.09.28.08.36.24;	author mpi;	state Exp;
branches;
next	1.6;
commitid	wT9nmVgN3CsJH8u1;

1.6
date	2015.09.12.09.22.29;	author mpi;	state Exp;
branches;
next	1.5;
commitid	GAR4lJSojgv3R9c8;

1.5
date	2015.09.11.16.58.00;	author mpi;	state Exp;
branches;
next	1.4;
commitid	aN3TroyZ9FVMndjv;

1.4
date	2015.09.04.08.43.39;	author mpi;	state Exp;
branches;
next	1.3;
commitid	qAevExm24QrBjVNL;

1.3
date	2015.08.20.12.51.10;	author mpi;	state Exp;
branches;
next	1.2;
commitid	kbd3EInNuY47D3oQ;

1.2
date	2015.08.20.12.39.43;	author mpi;	state Exp;
branches;
next	1.1;
commitid	9IUogeilRo73xUDL;

1.1
date	2015.07.18.15.51.16;	author mpi;	state Exp;
branches;
next	;
commitid	lwQKRpFyNEr7kjoF;


desc
@@


1.63
log
@Simplify rtable_mpath_insert().

ok jmatthew@@
@
text
@/*	$OpenBSD: rtable.c,v 1.62 2017/09/05 10:56:04 mpi Exp $ */

/*
 * Copyright (c) 2014-2016 Martin Pieuchot
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#ifndef _KERNEL
#include "kern_compat.h"
#else
#include <sys/param.h>
#include <sys/systm.h>
#include <sys/socket.h>
#include <sys/malloc.h>
#include <sys/queue.h>
#include <sys/domain.h>
#include <sys/srp.h>
#endif

#include <net/rtable.h>
#include <net/route.h>

/*
 * Structures used by rtable_get() to retrieve the corresponding
 * routing table for a given pair of ``af'' and ``rtableid''.
 *
 * Note that once allocated routing table heads are never freed.
 * This way we do not need to reference count them.
 *
 *	afmap		    rtmap/dommp
 *   -----------          ---------     -----
 *   |   0     |--------> | 0 | 0 | ... | 0 |	Array mapping rtableid (=index)
 *   -----------          ---------     -----   to rdomain/loopback (=value).
 *   | AF_INET |.
 *   ----------- `.       .---------.     .---------.
 *       ...	   `----> | rtable0 | ... | rtableN |	Array of pointers for
 *   -----------          '---------'     '---------'	IPv4 routing tables
 *   | AF_MPLS |					indexed by ``rtableid''.
 *   -----------
 */
struct srp	  *afmap;
uint8_t		   af2idx[AF_MAX+1];	/* To only allocate supported AF */
uint8_t		   af2idx_max;

/* Array of routing table pointers. */
struct rtmap {
	unsigned int	   limit;
	void		 **tbl;
};

/*
 * Array of rtableid -> rdomain mapping.
 *
 * Only used for the first index as describbed above.
 */
struct dommp {
	unsigned int	   limit;
	/*
	 * Array to get the routing domain and loopback interface related to
	 * a routing table. Format:
	 *
	 * 8 unused bits | 16 bits for loopback index | 8 bits for rdomain
	 */
	unsigned int	  *value;
};

unsigned int	   rtmap_limit = 0;

void		   rtmap_init(void);
void		   rtmap_grow(unsigned int, sa_family_t);
void		   rtmap_dtor(void *, void *);

struct srp_gc	   rtmap_gc = SRP_GC_INITIALIZER(rtmap_dtor, NULL);

void		   rtable_init_backend(unsigned int);
void		  *rtable_alloc(unsigned int, unsigned int, unsigned int);
void		  *rtable_get(unsigned int, sa_family_t);

void
rtmap_init(void)
{
	struct domain	*dp;
	int		 i;

	/* Start with a single table for every domain that requires it. */
	for (i = 0; (dp = domains[i]) != NULL; i++) {
		if (dp->dom_rtoffset == 0)
			continue;

		rtmap_grow(1, dp->dom_family);
	}

	/* Initialize the rtableid->rdomain mapping table. */
	rtmap_grow(1, 0);

	rtmap_limit = 1;
}

/*
 * Grow the size of the array of routing table for AF ``af'' to ``nlimit''.
 */
void
rtmap_grow(unsigned int nlimit, sa_family_t af)
{
	struct rtmap	*map, *nmap;
	int		 i;

	KERNEL_ASSERT_LOCKED();

	KASSERT(nlimit > rtmap_limit);

	nmap = malloc(sizeof(*nmap), M_RTABLE, M_WAITOK);
	nmap->limit = nlimit;
	nmap->tbl = mallocarray(nlimit, sizeof(*nmap[0].tbl), M_RTABLE,
	    M_WAITOK|M_ZERO);

	map = srp_get_locked(&afmap[af2idx[af]]);
	if (map != NULL) {
		KASSERT(map->limit == rtmap_limit);

		for (i = 0; i < map->limit; i++)
			nmap->tbl[i] = map->tbl[i];
	}

	srp_update_locked(&rtmap_gc, &afmap[af2idx[af]], nmap);
}

void
rtmap_dtor(void *null, void *xmap)
{
	struct rtmap	*map = xmap;

	/*
	 * doesnt need to be serialized since this is the last reference
	 * to this map. there's nothing to race against.
	 */
	free(map->tbl, M_RTABLE, map->limit * sizeof(*map[0].tbl));
	free(map, M_RTABLE, sizeof(*map));
}

void
rtable_init(void)
{
	struct domain	*dp;
	unsigned int	 keylen = 0;
	int		 i;

	KASSERT(sizeof(struct rtmap) == sizeof(struct dommp));

	/* We use index 0 for the rtable/rdomain map. */
	af2idx_max = 1;
	memset(af2idx, 0, sizeof(af2idx));

	/*
	 * Compute the maximum supported key length in case the routing
	 * table backend needs it.
	 */
	for (i = 0; (dp = domains[i]) != NULL; i++) {
		if (dp->dom_rtoffset == 0)
			continue;

		af2idx[dp->dom_family] = af2idx_max++;
		if (dp->dom_rtkeylen > keylen)
			keylen = dp->dom_rtkeylen;

	}
	rtable_init_backend(keylen);

	/*
	 * Allocate AF-to-id table now that we now how many AFs this
	 * kernel supports.
	 */
	afmap = mallocarray(af2idx_max + 1, sizeof(*afmap), M_RTABLE,
	    M_WAITOK|M_ZERO);

	rtmap_init();

	if (rtable_add(0) != 0)
		panic("unable to create default routing table");
}

int
rtable_add(unsigned int id)
{
	struct domain	*dp;
	void		*tbl;
	struct rtmap	*map;
	struct dommp	*dmm;
	sa_family_t	 af;
	unsigned int	 off, alen;
	int		 i;

	KERNEL_ASSERT_LOCKED();

	if (id > RT_TABLEID_MAX)
		return (EINVAL);

	if (rtable_exists(id))
		return (EEXIST);

	for (i = 0; (dp = domains[i]) != NULL; i++) {
		if (dp->dom_rtoffset == 0)
			continue;

		af = dp->dom_family;
		off = dp->dom_rtoffset;
		alen = dp->dom_maxplen;

		if (id >= rtmap_limit)
			rtmap_grow(id + 1, af);

		tbl = rtable_alloc(id, alen, off);
		if (tbl == NULL)
			return (ENOMEM);

		map = srp_get_locked(&afmap[af2idx[af]]);
		map->tbl[id] = tbl;
	}

	/* Reflect possible growth. */
	if (id >= rtmap_limit) {
		rtmap_grow(id + 1, 0);
		rtmap_limit = id + 1;
	}

	/* Use main rtable/rdomain by default. */
	dmm = srp_get_locked(&afmap[0]);
	dmm->value[id] = 0;

	return (0);
}

void *
rtable_get(unsigned int rtableid, sa_family_t af)
{
	struct rtmap	*map;
	void		*tbl = NULL;
	struct srp_ref	 sr;

	if (af >= nitems(af2idx) || af2idx[af] == 0)
		return (NULL);

	map = srp_enter(&sr, &afmap[af2idx[af]]);
	if (rtableid < map->limit)
		tbl = map->tbl[rtableid];
	srp_leave(&sr);

	return (tbl);
}

int
rtable_exists(unsigned int rtableid)
{
	struct domain	*dp;
	void		*tbl;
	int		 i;

	for (i = 0; (dp = domains[i]) != NULL; i++) {
		if (dp->dom_rtoffset == 0)
			continue;

		tbl = rtable_get(rtableid, dp->dom_family);
		if (tbl != NULL)
			return (1);
	}

	return (0);
}

unsigned int
rtable_l2(unsigned int rtableid)
{
	struct dommp	*dmm;
	unsigned int	 rdomain = 0;
	struct srp_ref	 sr;

	dmm = srp_enter(&sr, &afmap[0]);
	if (rtableid < dmm->limit)
		rdomain = (dmm->value[rtableid] & RT_TABLEID_MASK);
	srp_leave(&sr);

	return (rdomain);
}

unsigned int
rtable_loindex(unsigned int rtableid)
{
	struct dommp	*dmm;
	unsigned int	 loifidx = 0;
	struct srp_ref	 sr;

	dmm = srp_enter(&sr, &afmap[0]);
	if (rtableid < dmm->limit)
		loifidx = (dmm->value[rtableid] >> RT_TABLEID_BITS);
	srp_leave(&sr);

	return (loifidx);
}

void
rtable_l2set(unsigned int rtableid, unsigned int rdomain, unsigned int loifidx)
{
	struct dommp	*dmm;
	unsigned int	 value;

	KERNEL_ASSERT_LOCKED();

	if (!rtable_exists(rtableid) || !rtable_exists(rdomain))
		return;

	value = (rdomain & RT_TABLEID_MASK) | (loifidx << RT_TABLEID_BITS);

	dmm = srp_get_locked(&afmap[0]);
	dmm->value[rtableid] = value;
}


static inline uint8_t	*satoaddr(struct art_root *, struct sockaddr *);

int	an_match(struct art_node *, struct sockaddr *, int);
void	rtentry_ref(void *, void *);
void	rtentry_unref(void *, void *);

void	rtable_mpath_insert(struct art_node *, struct rtentry *);

struct srpl_rc rt_rc = SRPL_RC_INITIALIZER(rtentry_ref, rtentry_unref, NULL);

void
rtable_init_backend(unsigned int keylen)
{
	art_init();
}

void *
rtable_alloc(unsigned int rtableid, unsigned int alen, unsigned int off)
{
	return (art_alloc(rtableid, alen, off));
}

struct rtentry *
rtable_lookup(unsigned int rtableid, struct sockaddr *dst,
    struct sockaddr *mask, struct sockaddr *gateway, uint8_t prio)
{
	struct art_root			*ar;
	struct art_node			*an;
	struct rtentry			*rt = NULL;
	struct srp_ref			 sr, nsr;
	uint8_t				*addr;
	int				 plen;

	ar = rtable_get(rtableid, dst->sa_family);
	if (ar == NULL)
		return (NULL);

	addr = satoaddr(ar, dst);

	/* No need for a perfect match. */
	if (mask == NULL) {
		an = art_match(ar, addr, &nsr);
		if (an == NULL)
			goto out;
	} else {
		plen = rtable_satoplen(dst->sa_family, mask);
		if (plen == -1)
			return (NULL);

		an = art_lookup(ar, addr, plen, &nsr);

		/* Make sure we've got a perfect match. */
		if (!an_match(an, dst, plen))
			goto out;
	}

	SRPL_FOREACH(rt, &sr, &an->an_rtlist, rt_next) {
		if (prio != RTP_ANY &&
		    (rt->rt_priority & RTP_MASK) != (prio & RTP_MASK))
			continue;

		if (gateway == NULL)
			break;

		if (rt->rt_gateway->sa_len == gateway->sa_len &&
		    memcmp(rt->rt_gateway, gateway, gateway->sa_len) == 0)
			break;
	}
	if (rt != NULL)
		rtref(rt);

	SRPL_LEAVE(&sr);
out:
	srp_leave(&nsr);

	return (rt);
}

struct rtentry *
rtable_match(unsigned int rtableid, struct sockaddr *dst, uint32_t *src)
{
	struct art_root			*ar;
	struct art_node			*an;
	struct rtentry			*rt = NULL;
	struct srp_ref			 sr, nsr;
	uint8_t				*addr;
	int				 hash;

	ar = rtable_get(rtableid, dst->sa_family);
	if (ar == NULL)
		return (NULL);

	addr = satoaddr(ar, dst);

	an = art_match(ar, addr, &nsr);
	if (an == NULL)
		goto out;

	rt = SRPL_FIRST(&sr, &an->an_rtlist);
	rtref(rt);
	SRPL_LEAVE(&sr);

	/* Gateway selection by Hash-Threshold (RFC 2992) */
	if ((hash = rt_hash(rt, dst, src)) != -1) {
		struct rtentry		*mrt;
		int			 threshold, npaths = 0;

		KASSERT(hash <= 0xffff);

		SRPL_FOREACH(mrt, &sr, &an->an_rtlist, rt_next) {
			/* Only count nexthops with the same priority. */
			if (mrt->rt_priority == rt->rt_priority)
				npaths++;
		}
		SRPL_LEAVE(&sr);

		threshold = (0xffff / npaths) + 1;

		/*
		 * we have no protection against concurrent modification of the
		 * route list attached to the node, so we won't necessarily
		 * have the same number of routes.  for most modifications,
		 * we'll pick a route that we wouldn't have if we only saw the
		 * list before or after the change.  if we were going to use
		 * the last available route, but it got removed, we'll hit
		 * the end of the list and then pick the first route.
		 */

		mrt = SRPL_FIRST(&sr, &an->an_rtlist);
		while (hash > threshold && mrt != NULL) {
			if (mrt->rt_priority == rt->rt_priority)
				hash -= threshold;
			mrt = SRPL_FOLLOW(&sr, mrt, rt_next);
		}

		if (mrt != NULL) {
			rtref(mrt);
			rtfree(rt);
			rt = mrt;
		}
		SRPL_LEAVE(&sr);
	}
out:
	srp_leave(&nsr);
	return (rt);
}

int
rtable_insert(unsigned int rtableid, struct sockaddr *dst,
    struct sockaddr *mask, struct sockaddr *gateway, uint8_t prio,
    struct rtentry *rt)
{
	struct rtentry			*mrt;
	struct srp_ref			 sr;
	struct art_root			*ar;
	struct art_node			*an, *prev;
	uint8_t				*addr;
	int				 plen;
	unsigned int			 rt_flags;
	int				 error = 0;

	ar = rtable_get(rtableid, dst->sa_family);
	if (ar == NULL)
		return (EAFNOSUPPORT);

	addr = satoaddr(ar, dst);
	plen = rtable_satoplen(dst->sa_family, mask);
	if (plen == -1)
		return (EINVAL);

	rtref(rt); /* guarantee rtfree won't do anything during insert */
	rw_enter_write(&ar->ar_lock);

	/* Do not permit exactly the same dst/mask/gw pair. */
	an = art_lookup(ar, addr, plen, &sr);
	srp_leave(&sr); /* an can't go away while we have the lock */
	if (an_match(an, dst, plen)) {
		struct rtentry  *mrt;
		int		 mpathok = ISSET(rt->rt_flags, RTF_MPATH);

		SRPL_FOREACH_LOCKED(mrt, &an->an_rtlist, rt_next) {
			if (prio != RTP_ANY &&
			    (mrt->rt_priority & RTP_MASK) != (prio & RTP_MASK))
				continue;

			if (!mpathok ||
			    (mrt->rt_gateway->sa_len == gateway->sa_len &&
			    !memcmp(mrt->rt_gateway, gateway, gateway->sa_len))){
				error = EEXIST;
				goto leave;
			}
		}
	}

	an = art_get(dst, plen);
	if (an == NULL) {
		error = ENOBUFS;
		goto leave;
	}

	/* prepare for immediate operation if insert succeeds */
	rt_flags = rt->rt_flags;
	rt->rt_flags &= ~RTF_MPATH;
	rt->rt_dest = dst;
	rt->rt_plen = plen;
	SRPL_INSERT_HEAD_LOCKED(&rt_rc, &an->an_rtlist, rt, rt_next);

	prev = art_insert(ar, an, addr, plen);
	if (prev != an) {
		SRPL_REMOVE_LOCKED(&rt_rc, &an->an_rtlist, rt, rtentry,
		    rt_next);
		rt->rt_flags = rt_flags;
		art_put(an);

		if (prev == NULL) {
			error = ESRCH;
			goto leave;
		}

		an = prev;

		mrt = SRPL_FIRST_LOCKED(&an->an_rtlist);
		KASSERT(mrt != NULL);
		KASSERT((rt->rt_flags & RTF_MPATH) || mrt->rt_priority != prio);

		/*
		 * An ART node with the same destination/netmask already
		 * exists, MPATH conflict must have been already checked.
		 */
		if (rt->rt_flags & RTF_MPATH) {
			/*
			 * Only keep the RTF_MPATH flag if two routes have
			 * the same gateway.
			 */
			rt->rt_flags &= ~RTF_MPATH;
			SRPL_FOREACH_LOCKED(mrt, &an->an_rtlist, rt_next) {
				if (mrt->rt_priority == prio) {
					mrt->rt_flags |= RTF_MPATH;
					rt->rt_flags |= RTF_MPATH;
				}
			}
		}

		/* Put newly inserted entry at the right place. */
		rtable_mpath_insert(an, rt);
	}
leave:
	rw_exit_write(&ar->ar_lock);
	rtfree(rt);
	return (error);
}

int
rtable_delete(unsigned int rtableid, struct sockaddr *dst,
    struct sockaddr *mask, struct rtentry *rt)
{
	struct art_root			*ar;
	struct art_node			*an;
	struct srp_ref			 sr;
	uint8_t				*addr;
	int				 plen;
	struct rtentry			*mrt;
	int				 npaths = 0;
	int				 error = 0;

	ar = rtable_get(rtableid, dst->sa_family);
	if (ar == NULL)
		return (EAFNOSUPPORT);

	addr = satoaddr(ar, dst);
	plen = rtable_satoplen(dst->sa_family, mask);

	rtref(rt); /* guarantee rtfree won't do anything under ar_lock */
	rw_enter_write(&ar->ar_lock);
	an = art_lookup(ar, addr, plen, &sr);
	srp_leave(&sr); /* an can't go away while we have the lock */

	/* Make sure we've got a perfect match. */
	if (!an_match(an, dst, plen)) {
		error = ESRCH;
		goto leave;
	}

	/*
	 * If other multipath route entries are still attached to
	 * this ART node we only have to unlink it.
	 */
	SRPL_FOREACH_LOCKED(mrt, &an->an_rtlist, rt_next)
		npaths++;

	if (npaths > 1) {
		KASSERT(rt->rt_refcnt >= 1);
		SRPL_REMOVE_LOCKED(&rt_rc, &an->an_rtlist, rt, rtentry,
		    rt_next);

		mrt = SRPL_FIRST_LOCKED(&an->an_rtlist);
		if (npaths == 2)
			mrt->rt_flags &= ~RTF_MPATH;

		goto leave;
	}

	if (art_delete(ar, an, addr, plen) == NULL)
		panic("art_delete failed to find node %p", an);

	KASSERT(rt->rt_refcnt >= 1);
	SRPL_REMOVE_LOCKED(&rt_rc, &an->an_rtlist, rt, rtentry, rt_next);
	art_put(an);

leave:
	rw_exit_write(&ar->ar_lock);
	rtfree(rt);

	return (error);
}

struct rtable_walk_cookie {
	int		(*rwc_func)(struct rtentry *, void *, unsigned int);
	void		 *rwc_arg;
	unsigned int	  rwc_rid;
};

/*
 * Helper for rtable_walk to keep the ART code free from any "struct rtentry".
 */
int
rtable_walk_helper(struct art_node *an, void *xrwc)
{
	struct srp_ref			 sr;
	struct rtable_walk_cookie	*rwc = xrwc;
	struct rtentry			*rt;
	int				 error = 0;

	SRPL_FOREACH(rt, &sr, &an->an_rtlist, rt_next) {
		if ((error = (*rwc->rwc_func)(rt, rwc->rwc_arg, rwc->rwc_rid)))
			break;
	}
	SRPL_LEAVE(&sr);

	return (error);
}

int
rtable_walk(unsigned int rtableid, sa_family_t af,
    int (*func)(struct rtentry *, void *, unsigned int), void *arg)
{
	struct art_root			*ar;
	struct rtable_walk_cookie	 rwc;
	int				 error;

	ar = rtable_get(rtableid, af);
	if (ar == NULL)
		return (EAFNOSUPPORT);

	rwc.rwc_func = func;
	rwc.rwc_arg = arg;
	rwc.rwc_rid = rtableid;

	while ((error = art_walk(ar, rtable_walk_helper, &rwc)) == EAGAIN)
		continue;

	return (error);
}

struct rtentry *
rtable_iterate(struct rtentry *rt0)
{
	struct rtentry *rt = NULL;
	struct srp_ref sr;

	rt = SRPL_NEXT(&sr, rt0, rt_next);
	if (rt != NULL)
		rtref(rt);
	SRPL_LEAVE(&sr);
	rtfree(rt0);
	return (rt);
}

int
rtable_mpath_capable(unsigned int rtableid, sa_family_t af)
{
	return (1);
}

int
rtable_mpath_reprio(unsigned int rtableid, struct sockaddr *dst,
    struct sockaddr *mask, uint8_t prio, struct rtentry *rt)
{
	struct art_root			*ar;
	struct art_node			*an;
	struct srp_ref			 sr;
	uint8_t				*addr;
	int				 plen;
	int				 error = 0;

	ar = rtable_get(rtableid, dst->sa_family);
	if (ar == NULL)
		return (EAFNOSUPPORT);

	addr = satoaddr(ar, dst);
	plen = rtable_satoplen(dst->sa_family, mask);

	rw_enter_write(&ar->ar_lock);
	an = art_lookup(ar, addr, plen, &sr);
	srp_leave(&sr); /* an can't go away while we have the lock */

	/* Make sure we've got a perfect match. */
	if (!an_match(an, dst, plen)) {
		error = ESRCH;
	} else if (SRPL_FIRST_LOCKED(&an->an_rtlist) == rt &&
		SRPL_NEXT_LOCKED(rt, rt_next) == NULL) {
		/*
		 * If there's only one entry on the list do not go
		 * through an insert/remove cycle.  This is done to
		 * guarantee that ``an->an_rtlist''  is never empty
		 * when a node is in the tree.
		 */
		rt->rt_priority = prio;
	} else {
		rtref(rt); /* keep rt alive in between remove and insert */
		SRPL_REMOVE_LOCKED(&rt_rc, &an->an_rtlist,
		    rt, rtentry, rt_next);
		rt->rt_priority = prio;
		rtable_mpath_insert(an, rt);
		rtfree(rt);
		error = EAGAIN;
	}
	rw_exit_write(&ar->ar_lock);

	return (error);
}

void
rtable_mpath_insert(struct art_node *an, struct rtentry *rt)
{
	struct rtentry			*mrt, *prt = NULL;
	uint8_t				 prio = rt->rt_priority;

	if ((mrt = SRPL_FIRST_LOCKED(&an->an_rtlist)) == NULL) {
		SRPL_INSERT_HEAD_LOCKED(&rt_rc, &an->an_rtlist, rt, rt_next);
		return;
	}

	/* Iterate until we find the route to be placed after ``rt''. */
	while (mrt->rt_priority <= prio && SRPL_NEXT_LOCKED(mrt, rt_next)) {
		prt = mrt;
		mrt = SRPL_NEXT_LOCKED(mrt, rt_next);
	}

	if (mrt->rt_priority <= prio) {
		SRPL_INSERT_AFTER_LOCKED(&rt_rc, mrt, rt, rt_next);
	} else if (prt != NULL) {
		SRPL_INSERT_AFTER_LOCKED(&rt_rc, prt, rt, rt_next);
	} else {
		SRPL_INSERT_HEAD_LOCKED(&rt_rc, &an->an_rtlist, rt, rt_next);
	}
}

/*
 * Returns 1 if ``an'' perfectly matches (``dst'', ``plen''), 0 otherwise.
 */
int
an_match(struct art_node *an, struct sockaddr *dst, int plen)
{
	struct rtentry			*rt;
	struct srp_ref			 sr;
	int				 match;

	if (an == NULL || an->an_plen != plen)
		return (0);

	rt = SRPL_FIRST(&sr, &an->an_rtlist);
	match = (memcmp(rt->rt_dest, dst, dst->sa_len) == 0);
	SRPL_LEAVE(&sr);

	return (match);
}

void
rtentry_ref(void *null, void *xrt)
{
	struct rtentry *rt = xrt;

	rtref(rt);
}

void
rtentry_unref(void *null, void *xrt)
{
	struct rtentry *rt = xrt;

	rtfree(rt);
}

/*
 * Return a pointer to the address (key).  This is an heritage from the
 * BSD radix tree needed to skip the non-address fields from the flavor
 * of "struct sockaddr" used by this routing table.
 */
static inline uint8_t *
satoaddr(struct art_root *at, struct sockaddr *sa)
{
	return (((uint8_t *)sa) + at->ar_off);
}

/*
 * Return the prefix length of a mask.
 */
int
rtable_satoplen(sa_family_t af, struct sockaddr *mask)
{
	struct domain	*dp;
	uint8_t		*ap, *ep;
	int		 mlen, plen = 0;
	int		 i;

	for (i = 0; (dp = domains[i]) != NULL; i++) {
		if (dp->dom_rtoffset == 0)
			continue;

		if (af == dp->dom_family)
			break;
	}
	if (dp == NULL)
		return (-1);

	/* Host route */
	if (mask == NULL)
		return (dp->dom_maxplen);

	mlen = mask->sa_len;

	/* Default route */
	if (mlen == 0)
		return (0);

	ap = (uint8_t *)((uint8_t *)mask) + dp->dom_rtoffset;
	ep = (uint8_t *)((uint8_t *)mask) + mlen;
	if (ap > ep)
		return (-1);

	if (ap == ep)
		return (0);

	/* "Beauty" adapted from sbin/route/show.c ... */
	while (ap < ep) {
		switch (*ap) {
		case 0xff:
			plen += 8;
			ap++;
			break;
		case 0xfe:
			plen += 7;
			ap++;
			goto out;
		case 0xfc:
			plen += 6;
			ap++;
			goto out;
		case 0xf8:
			plen += 5;
			ap++;
			goto out;
		case 0xf0:
			plen += 4;
			ap++;
			goto out;
		case 0xe0:
			plen += 3;
			ap++;
			goto out;
		case 0xc0:
			plen += 2;
			ap++;
			goto out;
		case 0x80:
			plen += 1;
			ap++;
			goto out;
		case 0x00:
			goto out;
		default:
			/* Non contiguous mask. */
			return (-1);
		}

	}

out:
#ifdef DIAGNOSTIC
	for (; ap < ep; ap++) {
		if (*ap != 0x00)
			return (-1);
	}
#endif /* DIAGNOSTIC */

	return (plen);
}
@


1.62
log
@Restart the iteration when a multipath list is re-ordered to make sure
no entry are missed.

While here do not re-ordered or send messages for route entries that are
already in the expected state.

Make rttest30 pass.

ok gerhard@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.61 2017/07/30 18:18:08 florian Exp $ */
d767 10
a776 10
	if ((mrt = SRPL_FIRST_LOCKED(&an->an_rtlist)) != NULL) {
		/*
		 * Select the order of the MPATH routes.
		 */
		while (SRPL_NEXT_LOCKED(mrt, rt_next) != NULL) {
			if (mrt->rt_priority > prio)
				break;
			prt = mrt;
			mrt = SRPL_NEXT_LOCKED(mrt, rt_next);
		}
d778 4
a781 15
		if (mrt->rt_priority > prio) {
			/*
			 * ``rt'' has a higher (smaller) priority than
			 * ``mrt'' so put it before in the list.
			 */
			if (prt != NULL) {
				SRPL_INSERT_AFTER_LOCKED(&rt_rc, prt, rt,
				    rt_next);
			} else {
				SRPL_INSERT_HEAD_LOCKED(&rt_rc, &an->an_rtlist,
				    rt, rt_next);
			}
		} else {
			SRPL_INSERT_AFTER_LOCKED(&rt_rc, mrt, rt, rt_next);
		}
@


1.61
log
@Enable mpath support in the Allotment Routing Table (ART) on the ramdisk.
OK mpi
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.60 2017/07/30 18:16:14 florian Exp $ */
d754 1
@


1.60
log
@Switch installer to Allotment Routing Table (ART).
Prompted by a bugreport by naddy that IPv6 autoconfiguration is broken
in the installer.
OK mpi, "go for it" deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.59 2017/05/11 14:03:19 mpi Exp $ */
a334 1
#ifndef SMALL_KERNEL
a335 1
#endif
a384 3
#ifdef SMALL_KERNEL
	rt = SRPL_FIRST(&sr, &an->an_rtlist);
#else
a396 1
#endif /* SMALL_KERNEL */
a414 1
#ifndef SMALL_KERNEL
a415 1
#endif /* SMALL_KERNEL */
a430 1
#ifndef SMALL_KERNEL
a470 1
#endif /* SMALL_KERNEL */
a480 1
#ifndef SMALL_KERNEL
a482 1
#endif /* SMALL_KERNEL */
a501 1
#ifndef SMALL_KERNEL
a521 1
#endif /* SMALL_KERNEL */
a547 1
#ifndef SMALL_KERNEL
a573 3
#else
		error = EEXIST;
#endif /* SMALL_KERNEL */
a589 1
#ifndef SMALL_KERNEL
a591 1
#endif /* SMALL_KERNEL */
a611 1
#ifndef SMALL_KERNEL
a629 1
#endif /* SMALL_KERNEL */
a696 1
#ifndef SMALL_KERNEL
a702 1
#endif /* SMALL_KERNEL */
a706 1
#ifndef SMALL_KERNEL
a795 1
#endif /* SMALL_KERNEL */
@


1.59
log
@No need to go through a remove/insert cycle when there's a single route
entry on the multipath list.

Fix a NULL dereference triggered by a CPU doing a lookup when another one
is updating the priorities of some routes.  By not doing a remove/insert
we ensure that ``an_rtlist'' is never empty and do not need a conditional
in the fast path.

Problem reported by and ok markus@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.58 2017/02/28 09:50:13 mpi Exp $ */
a327 151
#ifndef ART
void
rtable_init_backend(unsigned int keylen)
{
	rn_init(keylen); /* initialize all zeroes, all ones, mask table */
}

void *
rtable_alloc(unsigned int rtableid, unsigned int alen, unsigned int off)
{
	struct radix_node_head *rnh = NULL;

	if (rn_inithead((void **)&rnh, off)) {
		rnh->rnh_rtableid = rtableid;
	}

	return (rnh);
}

struct rtentry *
rtable_lookup(unsigned int rtableid, struct sockaddr *dst,
    struct sockaddr *mask, struct sockaddr *gateway, uint8_t prio)
{
	struct radix_node_head	*rnh;
	struct radix_node	*rn;
	struct rtentry		*rt;

	rnh = rtable_get(rtableid, dst->sa_family);
	if (rnh == NULL)
		return (NULL);

	rn = rn_lookup(dst, mask, rnh);
	if (rn == NULL || (rn->rn_flags & RNF_ROOT) != 0)
		return (NULL);

	rt = ((struct rtentry *)rn);

	rtref(rt);
	return (rt);
}

struct rtentry *
rtable_match(unsigned int rtableid, struct sockaddr *dst, uint32_t *src)
{
	struct radix_node_head	*rnh;
	struct radix_node	*rn;
	struct rtentry		*rt = NULL;

	rnh = rtable_get(rtableid, dst->sa_family);
	if (rnh == NULL)
		return (NULL);

	KERNEL_LOCK();
	rn = rn_match(dst, rnh);
	if (rn == NULL || (rn->rn_flags & RNF_ROOT) != 0)
		goto out;

	rt = ((struct rtentry *)rn);
	rtref(rt);
out:
	KERNEL_UNLOCK();
	return (rt);
}

int
rtable_insert(unsigned int rtableid, struct sockaddr *dst,
    struct sockaddr *mask, struct sockaddr *gateway, uint8_t prio,
    struct rtentry *rt)
{
	struct radix_node_head	*rnh;
	struct radix_node	*rn = (struct radix_node *)rt;

	rnh = rtable_get(rtableid, dst->sa_family);
	if (rnh == NULL)
		return (EAFNOSUPPORT);

	rn = rn_addroute(dst, mask, rnh, rn, prio);
	if (rn == NULL)
		return (ESRCH);

	rt = ((struct rtentry *)rn);
	rtref(rt);

	return (0);
}

int
rtable_delete(unsigned int rtableid, struct sockaddr *dst,
    struct sockaddr *mask, struct rtentry *rt)
{
	struct radix_node_head	*rnh;
	struct radix_node	*rn = (struct radix_node *)rt;

	rnh = rtable_get(rtableid, dst->sa_family);
	if (rnh == NULL)
		return (EAFNOSUPPORT);

	rn = rn_delete(dst, mask, rnh, rn);
	if (rn == NULL)
		return (ESRCH);

	if (rn->rn_flags & (RNF_ACTIVE | RNF_ROOT))
		panic("active node flags=%x", rn->rn_flags);

	rt = ((struct rtentry *)rn);
	rtfree(rt);

	return (0);
}

int
rtable_walk(unsigned int rtableid, sa_family_t af,
    int (*func)(struct rtentry *, void *, unsigned int), void *arg)
{
	struct radix_node_head	*rnh;
	int (*f)(struct radix_node *, void *, unsigned int) = (void *)func;
	int error;

	rnh = rtable_get(rtableid, af);
	if (rnh == NULL)
		return (EAFNOSUPPORT);

	while ((error = rn_walktree(rnh, f, arg)) == EAGAIN)
		continue;

	return (error);
}

struct rtentry *
rtable_iterate(struct rtentry *rt0)
{
	rtfree(rt0);
	return (NULL);
}

#ifndef SMALL_KERNEL
int
rtable_mpath_capable(unsigned int rtableid, sa_family_t af)
{
	return (0);
}

int
rtable_mpath_reprio(unsigned int rtableid, struct sockaddr *dst,
    struct sockaddr *mask, uint8_t prio, struct rtentry *rt)
{
	return (0);
}
#endif /* SMALL_KERNEL */

#else /* ART */
a868 1
#endif /* ART */
@


1.58
log
@Prevent a MP race in rtable_lookup().

If an ART node is linked to multiple route entries, in the MPATH case,
it is not safe to dereference ``an_dst''.  This non-refcounted pointer
can be changed at any time by another CPU.

So get rid of the pointer and use the first destination of a route entry
when comparing sockaddrs.

This allows us so remove a pointer from 'struct art_node' and save 5Mb of
memory in an IPv4 fullfeed.

ok jmatthew@@, claudio@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.57 2017/01/24 10:08:30 krw Exp $ */
d912 1
a912 1
	if (!an_match(an, dst, plen))
d914 10
a923 1
	else {
@


1.57
log
@A space here, a space there. Soon we're talking real whitespace
rectification.
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.56 2016/11/20 11:46:45 mpi Exp $ */
d482 1
d534 1
a534 2
		if (an == NULL || an->an_plen != plen ||
		    memcmp(an->an_dst, dst, dst->sa_len))
d669 1
a669 2
	if (an != NULL && an->an_plen == plen &&
	    !memcmp(an->an_dst, dst, dst->sa_len)) {
d778 1
a778 2
	if (an == NULL || an->an_plen != plen ||
	    memcmp(an->an_dst, dst, dst->sa_len)) {
a796 1
		an->an_dst = mrt->rt_dest;
d912 1
a912 2
	if (an == NULL || an->an_plen != plen ||
	    memcmp(an->an_dst, dst, dst->sa_len))
d964 20
@


1.56
log
@Make rtable_iterate(9) mpsafe by using the new SRPL_NEXT(9).

ok dlg@@, jmatthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.55 2016/11/20 11:40:58 mpi Exp $ */
d49 1
a49 1
 *   | AF_MPLS |            			 	indexed by ``rtableid''.
d944 1
a944 1
		    	prt = mrt;
@


1.55
log
@Rename SRPL_ENTER() to SRPL_FIRST() and SRPL_NEXT() to SRPL_FOLLOW().

This allows us to introduce SRPL_NEXT() that can be used to start
iterating on an arbitrary member of an srp list, hence without calling
SRPL_ENTER().

ok dlg@@, jmatthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.54 2016/11/14 10:32:46 mpi Exp $ */
d872 1
d874 1
a874 1
	struct rtentry *rt;
d876 1
a876 3
	KERNEL_ASSERT_LOCKED();

	rt = SRPL_NEXT_LOCKED(rt0, rt_next);
d879 2
a881 1

a882 4
#else
	rtfree(rt0);
	return (NULL);
#endif /* SMALL_KERNEL */
@


1.54
log
@Automatically create a default lo(4) interface per rdomain.

In order to stop abusing lo0 for all rdomains, a new loopback interface
will be created every time a rdomain is created.  The unit number will
be the same as the rdomain, i.e. lo1 will be attached to rdomain 1.

If this loopback interface is already in use it wont be possible to create
the corresponding rdomain.

In order to know which lo(4) interface is attached to a rdomain, its index
is stored in the rtable/rdomain map.

This is a long overdue since the introduction of rtable/rdomain.  It also
fixes a recent regression due to resetting the rdomain of an incoming
packet reported by semarie@@, Andreas Bartelt and Nils Frohberg.

ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.53 2016/11/14 08:54:19 mpi Exp $ */
d539 1
a539 1
	rt = SRPL_ENTER(&sr, &an->an_rtlist);
d586 1
a586 1
	rt = SRPL_ENTER(&sr, &an->an_rtlist);
d617 1
a617 1
		mrt = SRPL_ENTER(&sr, &an->an_rtlist);
d621 1
a621 1
			mrt = SRPL_NEXT(&sr, mrt, rt_next);
@


1.53
log
@Remove radix_mpath dragons.

This code insn't used since ART is the default.

ok vgross@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.52 2016/09/07 09:36:49 mpi Exp $ */
d4 1
a4 1
 * Copyright (c) 2014-2015 Martin Pieuchot
d44 1
a44 1
 *   -----------          ---------     -----   to rdomain (=value).
d62 5
a66 1
/* Array of rtableid -> rdomain mapping. */
d69 7
a75 1
	unsigned int	  *dom;
d159 2
d188 3
d239 1
a239 1
	dmm->dom[id] = 0;
d290 1
a290 1
		rdomain = dmm->dom[rtableid];
d296 15
d312 1
a312 1
rtable_l2set(unsigned int rtableid, unsigned int rdomain)
d315 1
d322 2
d325 1
a325 1
	dmm->dom[rtableid] = rdomain;
@


1.52
log
@Rename rtable_mpath_next() into rtable_iterate() and make it do a proper
reference count.

rtable_iterate() frees the passed ``rt'' and returns the next one on the
multipath list or NULL if there's none.

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.51 2016/08/30 07:42:57 jmatthew Exp $ */
a307 3
#ifndef SMALL_KERNEL
		rnh->rnh_multipath = 1;
#endif /* SMALL_KERNEL */
a331 8
#ifndef SMALL_KERNEL
	if (rnh->rnh_multipath) {
		rt = rt_mpath_matchgate(rt, gateway, prio);
		if (rt == NULL)
			return (NULL);
	}
#endif /* !SMALL_KERNEL */

a341 3
#ifndef SMALL_KERNEL
	int			 hash;
#endif /* SMALL_KERNEL */
a353 30

#ifndef SMALL_KERNEL
	/* Gateway selection by Hash-Threshold (RFC 2992) */
	if ((hash = rt_hash(rt, dst, src)) != -1) {
		struct rtentry		*mrt = rt;
		int			 threshold, npaths = 1;

		KASSERT(hash <= 0xffff);

		rtref(rt);

		while ((mrt = rtable_iterate(mrt)) != NULL)
			npaths++;

		threshold = (0xffff / npaths) + 1;

		mrt = rt;
		while (hash > threshold && mrt != NULL) {
			/* stay within the multipath routes */
			mrt = rtable_iterate(mrt);
			hash -= threshold;
		}

		/* if gw selection fails, use the first match (default) */
		if (mrt != NULL) {
			rtfree(rt);
			rt = mrt;
		}
	}
#endif /* SMALL_KERNEL */
a370 10
#ifndef SMALL_KERNEL
	if (rnh->rnh_multipath) {
		/* Do not permit exactly the same dst/mask/gw pair. */
		if (rt_mpath_conflict(rnh, dst, mask, gateway, prio,
	    	    ISSET(rt->rt_flags, RTF_MPATH))) {
			return (EEXIST);
		}
	}
#endif /* SMALL_KERNEL */

a425 11
#ifndef SMALL_KERNEL
	struct radix_node *rn = (struct radix_node *)rt0;
	struct rtentry *rt;

	rt = (struct rtentry *)rn_mpath_next(rn, RMP_MODE_ACTIVE);
	if (rt != NULL)
		rtref(rt);
	rtfree(rt0);

	return (rt);
#else
a427 1
#endif /* SMALL_KERNEL */
d434 1
a434 9
	struct radix_node_head	*rnh;
	int mpath;

	rnh = rtable_get(rtableid, af);
	if (rnh == NULL)
		return (0);

	mpath = rnh->rnh_multipath;
	return (mpath);
a440 4
	struct radix_node	*rn = (struct radix_node *)rt;

	rn_mpath_reprio(rn, prio);

@


1.51
log
@use a per-table rwlock to serialize ART updates and walks, rather than
taking the kernel lock.

ok mpi@@ dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.50 2016/07/19 10:51:44 mpi Exp $ */
d377 3
a379 1
		while ((mrt = rtable_mpath_next(mrt)) != NULL)
d387 1
a387 1
			mrt = rtable_mpath_next(mrt);
a392 1
			rtref(mrt);
d477 19
a520 8

struct rtentry *
rtable_mpath_next(struct rtentry *rt)
{
	struct radix_node *rn = (struct radix_node *)rt;

	return ((struct rtentry *)rn_mpath_next(rn, RMP_MODE_ACTIVE));
}
d914 20
a1014 7
}

struct rtentry *
rtable_mpath_next(struct rtentry *rt)
{
	KERNEL_ASSERT_LOCKED();
	return (SRPL_NEXT_LOCKED(rt, rt_next));
@


1.50
log
@Revert use of the _SAFE version of SRPL_FOREACH() now that the offending
function has been fixed.

Functions passed to rtable_walk() must return EAGAIN if they delete an
entry from the tree, no matter if it is a leaf or not.
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.49 2016/07/04 08:11:48 mpi Exp $ */
d518 4
a685 2
	KERNEL_ASSERT_LOCKED();

d696 1
a771 2
		SRPL_INSERT_HEAD_LOCKED(&rt_rc, &an->an_rtlist, rt, rt_next);

d773 1
a773 1
		rtable_mpath_reprio(rtableid, dst, mask, rt->rt_priority, rt);
d779 1
d797 1
d806 2
a807 2
	KERNEL_ASSERT_LOCKED();

d813 4
a816 2
	    memcmp(an->an_dst, dst, dst->sa_len))
		return (ESRCH);
d835 2
a836 1
		return (0);
d841 1
a841 1
		return (ESRCH);
d845 1
d847 5
a851 2
	art_put(an);
	return (0);
d918 1
a918 1
	struct rtentry			*mrt, *prt = NULL;
d927 1
a927 2
	KERNEL_ASSERT_LOCKED();

d934 13
a946 1
		return (ESRCH);
d948 5
a952 3
	rtref(rt); /* keep rt alive in between remove and add */
	SRPL_REMOVE_LOCKED(&rt_rc, &an->an_rtlist, rt, rtentry, rt_next);
	rt->rt_priority = prio;
a982 3
	rtfree(rt);

	return (0);
@


1.49
log
@Use the _SAFE_ version of SRPL_FOREACH() in rtable_walk_helper() to
prevent an off-by-one when removing entries from the mpath list.

Fix a regression introduced by the refactoring needed to serialize
rtable_walk() with create/delete.

ok jca@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.48 2016/06/22 06:32:32 dlg Exp $ */
d856 1
d858 1
a858 1
	struct rtentry			*rt, *nrt;
d861 2
a862 7
	KERNEL_ASSERT_LOCKED();

	SRPL_FOREACH_SAFE_LOCKED(rt, &an->an_rtlist, rt_next, nrt) {
		KERNEL_UNLOCK();
		error = (*rwc->rwc_func)(rt, rwc->rwc_arg, rwc->rwc_rid);
		KERNEL_LOCK();
		if (error)
d865 1
@


1.48
log
@rework art_walk so it will behave in an mpsafe world.

art_walk now explicitly takes the same lock used to serialise change
made via rtable_insert and _delete, so it can safely adjust the
refcnts on tables while it recurses into them. they need to still
exist when returning out of the recursion.

it uses srps to access nodes and drops the lock before calling the
callback function. this is because some callbacks sleep (eg, copyout
in the sysctl code that dumps an rtable to userland), which you
shouldnt hold a lock accross. other callbacks attempt to modify
the rtable (eg, marking routes as down when then interface theyre
on goes down), which tries to take the lock again, which probably
wont work in the future.

ok jmatthew@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.47 2016/06/14 04:42:02 jmatthew Exp $ */
a855 1
	struct srp_ref			 sr;
d857 1
a857 1
	struct rtentry			*rt;
d860 7
a866 2
	SRPL_FOREACH(rt, &sr, &an->an_rtlist, rt_next) {
		if ((error = (*rwc->rwc_func)(rt, rwc->rwc_arg, rwc->rwc_rid)))
a868 1
	SRPL_LEAVE(&sr);
@


1.47
log
@Convert the links between art data structures used during lookups into srps.
art_lookup and art_match now return an active srp_ref, which the caller must
leave when it's done with the returned route (if any).  This allows lookups
to be done without holding any locks.

The art_table and art_node garbage collectors are still responsible for
freeing items removed from the routing table, so they now use srp_finalize
to wait out any active references, and updates are done using srp_swap
operations.

ok dlg@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.46 2016/06/07 01:31:54 tedu Exp $ */
d856 1
d858 1
a858 1
	struct rtentry			*rt, *nrt;
d861 1
a861 3
	KERNEL_ASSERT_LOCKED();

	SRPL_FOREACH_SAFE_LOCKED(rt, &an->an_rtlist, rt_next, nrt) {
d865 1
@


1.46
log
@per trending style, add continue to empty loops.
ok mglocker
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.45 2016/06/01 09:46:19 dlg Exp $ */
d538 2
a539 2
	struct rtentry			*rt;
	struct srp_ref			 sr;
d551 1
a551 1
		an = art_match(ar, addr);
d553 1
a553 1
			return (NULL);
d559 2
a560 1
		an = art_lookup(ar, addr, plen);
d564 1
a564 1
			return (NULL);
a581 4
	if (rt == NULL) {
		SRPL_LEAVE(&sr);
		return (NULL);
	}
d583 2
a585 1
	rtref(rt);
d587 2
d599 1
a599 1
	struct srp_ref			 sr;
d611 1
a611 2
	KERNEL_LOCK();
	an = art_match(ar, addr);
d636 10
d662 1
a662 1
	KERNEL_UNLOCK();
d673 1
d680 1
d693 2
d697 2
a698 1
	an = art_lookup(ar, addr, plen);
d701 1
a701 1
	    	struct rtentry	*mrt;
d712 2
a713 1
			    	return (EEXIST);
d720 4
a723 2
	if (an == NULL)
		return (ENOBUFS);
d738 5
a742 3
 
		if (prev == NULL)
			return ESRCH;
d774 1
a774 1
		return (EEXIST);
d777 3
a779 2

	return (0);
d788 1
a795 2
	KERNEL_ASSERT_LOCKED();

d803 5
a807 1
	an = art_lookup(ar, addr, plen);
d822 1
a822 1
		KASSERT(rt->rt_refcnt >= 2);
d837 1
a837 1
	KASSERT(rt->rt_refcnt >= 2);
d905 1
d917 5
a921 1
	an = art_lookup(ar, addr, plen);
a925 2

	KERNEL_ASSERT_LOCKED();
@


1.45
log
@shuffle the code in rtable_insert so it inserts a populated art_node.

this makes the node usable as soon as it is in the tree, rather
than after it inserts the rtentry on the node.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.44 2016/06/01 06:31:52 dlg Exp $ */
d471 1
a471 1
		;	/* nothing */
d864 1
a864 1
		; /* nothing */
@


1.44
log
@rtref and rtfree around moving the rt in rtable_mpath_reprio so the list
operations cant drop the refcount to 0.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.43 2016/06/01 06:19:06 dlg Exp $ */
d669 1
d708 7
d716 4
a719 1
	if (prev == NULL) {
d721 3
a723 2
		return (ESRCH);
	}
a724 4
	if (prev == an) {
		rt->rt_flags &= ~RTF_MPATH;
	} else {
		art_put(an);
d749 5
a758 10
	rt->rt_dest = dst;
	rt->rt_plen = plen;
	rtref(rt);
	SRPL_INSERT_HEAD_LOCKED(&rt_rc, &an->an_rtlist, rt, rt_next);

#ifndef SMALL_KERNEL
	/* Put newly inserted entry at the right place. */
	rtable_mpath_reprio(rtableid, dst, mask, rt->rt_priority, rt);
#endif /* SMALL_KERNEL */

a801 1
		rtfree(rt);
a815 1
	rtfree(rt);
@


1.43
log
@move all the art_node initialisation to art_get in art.c

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.42 2016/05/18 03:46:03 dlg Exp $ */
d900 1
d933 1
@


1.42
log
@rework the srp api so it takes an srp_ref struct that the caller provides.

the srp_ref struct is used to track the location of the callers
hazard pointer so later calls to srp_follow and srp_enter already
know what to clear. this in turn means most of the caveats around
using srps go away. specifically, you can now:

- switch cpus while holding an srp ref
  - ie, you can sleep while holding an srp ref
- you can take and release srp refs in any order

the original intent was to simplify use of the api when dealing
with complicated data structures. the caller now no longer has to
track the location of the srp a value was fetched from, the srp_ref
effectively does that for you.

srp lists have been refactored to use srp_refs instead of srpl_iter
structs.

this is in preparation of using srps inside the ART code. ART is a
complicated data structure, and lookups require overlapping holds
of srp references.

ok mpi@@ jmatthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.41 2016/05/02 22:15:49 jmatthew Exp $ */
a705 2

	SRPL_INIT(&an->an_rtlist);
@


1.41
log
@Simplify life for routing table implementations by requiring that rtable_walk
callbacks return EAGAIN if they modify the routing table.  While we're here,
simplify life for rtable_walk callers by moving the loop that restarts the
walk on EAGAIN into rtable_walk itself.

Flushing cloned routes on interface state changes becomes a bit more
inefficient, but this can be improved later.

ok mpi@@ dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.40 2016/04/13 08:04:14 mpi Exp $ */
d234 1
d239 1
a239 1
	map = srp_enter(&afmap[af2idx[af]]);
d242 1
a242 1
	srp_leave(&afmap[af2idx[af]], map);
d271 1
d273 1
a273 1
	dmm = srp_enter(&afmap[0]);
d276 1
a276 1
	srp_leave(&afmap[0], dmm);
d539 1
a539 1
	struct srpl_iter		 i;
d567 1
a567 1
	rt = SRPL_ENTER(&an->an_rtlist, &i);
d569 1
a569 1
	SRPL_FOREACH(rt, &an->an_rtlist, &i, rt_next) {
d582 1
a582 1
		SRPL_LEAVE(&i, rt);
d588 1
a588 1
	SRPL_LEAVE(&i, rt);
d599 1
a599 1
	struct srpl_iter		 i;
d616 1
a616 1
	rt = SRPL_ENTER(&an->an_rtlist, &i);
d618 1
a618 1
	SRPL_LEAVE(&i, rt);
a623 1
		struct srpl_iter	 i;
d628 1
a628 1
		SRPL_FOREACH(mrt, &an->an_rtlist, &i, rt_next) {
d633 1
a633 1
		SRPL_LEAVE(&i, mrt);
d637 1
a637 1
		mrt = SRPL_ENTER(&an->an_rtlist, &i);
d641 1
a641 1
			mrt = SRPL_NEXT(&i, mrt, rt_next);
d649 1
a649 1
		SRPL_LEAVE(&i, mrt);
@


1.40
log
@Keep all pools in the same place.

ok jmatthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.39 2016/02/24 22:41:53 mpi Exp $ */
d462 1
d468 4
a471 1
	return (rn_walktree(rnh, f, arg));
d853 1
d863 4
a866 1
	return (art_walk(ar, rtable_walk_helper, &rwc));
@


1.39
log
@Fix ECMP routing by passing the correct destination address to the
hash routine.

Bug reported and fix analysed by Jean-Daniel Dupas <jddupas AT xooloo DOT net>

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.38 2016/01/18 18:27:12 mpi Exp $ */
a25 1
#include <sys/pool.h>
a506 2
struct pool		 an_pool;	/* pool for ART node structures */

a517 1
	pool_init(&an_pool, sizeof(struct art_node), 0, 0, 0, "art_node", NULL);
d698 1
a698 1
	an = pool_get(&an_pool, PR_NOWAIT | PR_ZERO);
a701 2
	an->an_dst = dst;
	an->an_plen = plen;
d706 1
a706 1
		pool_put(&an_pool, an);
d713 1
a713 1
		pool_put(&an_pool, an);
d813 1
a813 1
	pool_put(&an_pool, an);
@


1.38
log
@Pass the address length to art_alloc() and remove the hack abusing the
offset of the address in the sockaddr to initialize the stride lengths.
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.37 2016/01/18 15:38:52 mpi Exp $ */
d370 1
a370 1
	if ((hash = rt_hash(rt, src)) != -1) {
d620 1
a620 1
	if ((hash = rt_hash(rt, src)) != -1) {
@


1.37
log
@Stop storing a backpointer to the corresponding ART node in each route
entry.

This pointer hasn't been used for some time and without it no external
reference count is needed to turn art_lookup() mpsafe.
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.36 2015/12/21 10:51:55 mpi Exp $ */
d78 1
a78 1
void		  *rtable_alloc(unsigned int, sa_family_t, unsigned int);
d187 1
a187 1
	unsigned int	 off;
d204 1
d209 1
a209 1
		tbl = rtable_alloc(id, af, off);
d302 1
a302 1
rtable_alloc(unsigned int rtableid, sa_family_t af, unsigned int off)
d525 1
a525 1
rtable_alloc(unsigned int rtableid, sa_family_t af, unsigned int off)
d527 1
a527 1
	return (art_alloc(rtableid, off));
@


1.36
log
@Pass the destination and mask to rtable_mpath_reprio() in order to not
use ``rt_node'' with ART.
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.35 2015/12/16 13:19:14 mpi Exp $ */
a747 1
	rt->rt_node = an;
d749 1
a788 2
	KASSERT(an == rt->rt_node);

a797 1
		rt->rt_node = NULL;
a813 1
	rt->rt_node = NULL;
a894 2

	KASSERT(an == rt->rt_node);
@


1.35
log
@Merge rtable_mpath_select() into rtable_match().

This allow us to get rid of one more "rt_node" usage with ART.

ok jmatthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.34 2015/12/15 13:08:50 mpi Exp $ */
d485 3
a487 2
void
rtable_mpath_reprio(struct rtentry *rt, uint8_t newprio)
d491 3
a493 1
	rn_mpath_reprio(rn, newprio);
d755 1
a755 1
	rtable_mpath_reprio(rt, rt->rt_priority);
d877 3
a879 2
void
rtable_mpath_reprio(struct rtentry *rt, uint8_t prio)
d881 4
a884 1
	struct art_node			*an = rt->rt_node;
d887 15
d936 2
@


1.34
log
@Do not panic when trying to delete an non-existing route with ART.

Reported by bluhm@@, ok jmatthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.33 2015/12/04 13:42:48 mpi Exp $ */
a76 1
struct rtentry	  *rtable_mpath_select(struct rtentry *, uint32_t *);
d351 3
d368 26
a393 1
	rt = rtable_mpath_select(rt, src);
a484 35
/* Gateway selection by Hash-Threshold (RFC 2992) */
struct rtentry *
rtable_mpath_select(struct rtentry *rt, uint32_t *src)
{
	struct rtentry *mrt = rt;
	int npaths, threshold, hash;

	if ((hash = rt_hash(rt, src)) == -1)
		return (rt);

	KASSERT(hash <= 0xffff);

	npaths = 1;
	while ((mrt = rtable_mpath_next(mrt)) != NULL)
		npaths++;

	threshold = (0xffff / npaths) + 1;

	mrt = rt;
	while (hash > threshold && mrt != NULL) {
		/* stay within the multipath routes */
		mrt = rtable_mpath_next(mrt);
		hash -= threshold;
	}

	/* if gw selection fails, use the first match (default) */
	if (mrt != NULL) {
		rtref(mrt);
		rtfree(rt);
		rt = mrt;
	}

	return (rt);
}

d595 3
d615 31
a645 1
	rt = rtable_mpath_select(rt, src);
a871 41
}

/* Gateway selection by Hash-Threshold (RFC 2992) */
struct rtentry *
rtable_mpath_select(struct rtentry *rt, uint32_t *src)
{
	struct art_node			*an = rt->rt_node;
	struct rtentry			*mrt;
	struct srpl_iter		 i;
	int				 npaths, threshold, hash;

	if ((hash = rt_hash(rt, src)) == -1)
		return (rt);

	KASSERT(hash <= 0xffff);

	npaths = 0;
	SRPL_FOREACH(mrt, &an->an_rtlist, &i, rt_next) {
		/* Only count nexthops with the same priority. */
		if (mrt->rt_priority == rt->rt_priority)
			npaths++;
	}
	SRPL_LEAVE(&i, mrt);

	threshold = (0xffff / npaths) + 1;

	mrt = SRPL_ENTER(&an->an_rtlist, &i);
	while (hash > threshold && mrt != NULL) {
		if (mrt->rt_priority == rt->rt_priority)
			hash -= threshold;
		mrt = SRPL_NEXT(&i, mrt, rt_next);
	}

	if (mrt != NULL) {
		rtref(mrt);
		rtfree(rt);
		rt = mrt;
	}
	SRPL_LEAVE(&i, mrt);

	return (rt);
@


1.33
log
@Move the KERNEL_LOCK from rt_match() to rtable_match().

ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.32 2015/12/03 21:57:59 mpi Exp $ */
d738 1
a738 1
	struct art_node			*an = rt->rt_node;
d744 1
d748 16
a784 14

	ar = rtable_get(rtableid, dst->sa_family);
	if (ar == NULL)
		return (EAFNOSUPPORT);

#ifdef DIAGNOSTIC
	if (memcmp(dst, an->an_dst, dst->sa_len))
		panic("destination do not match");
	if (mask && an->an_plen != rtable_satoplen(dst->sa_family, mask))
		panic("mask do not match");
#endif /* DIAGNOSTIC */

	addr = satoaddr(ar, an->an_dst);
	plen = an->an_plen;
@


1.32
log
@Get rid of rt_mask() and stop allocating a "struct sockaddr" for every
route entry in ART.

rt_plen() now represents the prefix length of a route entry and should
be used instead.

For now use a "struct sockaddr_in6" to represent the mask when needed,
this should be then replaced by the prefix length and RTA_NETMASK only
used for compatibility with userland.

ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.31 2015/12/02 16:49:58 bluhm Exp $ */
d351 1
a351 1
	struct rtentry		*rt;
d357 1
d360 1
a360 1
		return (NULL);
d368 2
a369 1

d600 1
a600 1
	struct rtentry			*rt;
d609 2
d613 1
a613 1
		return (NULL);
d622 2
a623 1

@


1.31
log
@rtable_delete() does not use its prio parameter, so delete it.
OK mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.30 2015/12/02 11:09:01 mpi Exp $ */
a511 1
static inline int	 satoplen(struct art_root *, struct sockaddr *);
d555 1
a555 1
		plen = satoplen(ar, mask);
d642 1
a642 1
	plen = satoplen(ar, mask);
a667 15
	/*
	 * XXX Allocating a sockaddr for the mask per node wastes a lot
	 * of memory, thankfully we'll get rid of that when rt_mask()
	 * will be no more.
	 */
	if (mask != NULL) {
		struct sockaddr		*msk;

		msk = malloc(dst->sa_len, M_RTABLE, M_NOWAIT | M_ZERO);
		if (msk == NULL)
			return (ENOMEM);
		memcpy(msk, mask, dst->sa_len);
		rt->rt_mask = msk;
	}

a677 2
		free(rt->rt_mask, M_RTABLE, 0);
		rt->rt_mask = NULL;
a749 2
		free(rt->rt_mask, M_RTABLE, 0);
		rt->rt_mask = NULL;
d771 1
a771 1
	if (mask != NULL && an->an_plen != satoplen(ar, mask))
a780 5
	/*
	 * XXX Is it safe to free the mask now?  Are we sure rt_mask()
	 * is only used when entries are in the table?
	 */
	free(rt->rt_mask, M_RTABLE, 0);
a781 1
	rt->rt_mask = NULL;
d958 1
d963 2
a964 2
static inline int
satoplen(struct art_root *ar, struct sockaddr *mask)
d966 14
a979 2
	uint8_t				*ap, *ep;
	int				 skip, mlen, plen = 0;
d983 1
a983 1
		return (ar->ar_alen);
d991 1
a991 3
	skip = ar->ar_off;

	ap = (uint8_t *)((uint8_t *)mask) + skip;
a1052 1
#endif /* ART */
@


1.30
log
@Respect priorities when inserting routes to the same destination in ART.
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.29 2015/12/02 09:17:47 mpi Exp $ */
d405 1
a405 1
    struct sockaddr *mask, uint8_t prio, struct rtentry *rt)
d748 1
a748 1
    struct sockaddr *mask, uint8_t prio, struct rtentry *rt)
@


1.29
log
@Move multipath Hash-Threshold selection mechanism inside rtable_match().

This will helps for unlocking the routing table and will prevent further
mistake by keeping the multipath logic inside the rtable_* API.

ok dlg@@, claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.28 2015/11/29 16:02:18 mpi Exp $ */
d595 1
a595 1
rtable_match(unsigned int rtableid, struct sockaddr *dst)
d912 1
a912 1
	struct rtentry			*mrt, *prt;
d931 11
a941 2
			/* prt -> rt -> mrt */
			SRPL_INSERT_AFTER_LOCKED(&rt_rc, prt, rt, rt_next);
a942 1
			/* prt -> mrt -> rt */
@


1.28
log
@Convert the simple list of multipath route entries used by ART kernels
to a SRP list.

This turns the rtable_* layer mpsafe.  We now only need to protect the
ART implementation itself.

Note that route(8) regress tests will now fail due to a supplementary
reference taken by the SRPL_INIT(9) API.

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.27 2015/11/27 12:13:22 mpi Exp $ */
d77 1
d347 1
a347 1
rtable_match(unsigned int rtableid, struct sockaddr *dst)
d364 4
d458 1
a458 1
rtable_mpath_select(struct rtentry *rt, uint32_t hash)
d461 6
a466 1
	int npaths, threshold;
d616 4
d869 1
a869 1
rtable_mpath_select(struct rtentry *rt, uint32_t hash)
d874 6
a879 1
	int				 npaths, threshold;
@


1.27
log
@Document that routing table heads are never freed as suggested by dlg@@
and kill rtable_put() because we're not going to use it.

The overhead of keeping a "struct art_root/radix_node_head" around is
very small compared to the added complexity needed to reference count
such structures.
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.26 2015/11/27 11:52:44 mpi Exp $ */
d500 1
a500 1
struct pool		an_pool;	/* pool for ART node structures */
d505 5
d530 1
d558 1
a558 1
	rt = SLIST_FIRST(&an->an_rtlist);
d560 1
a560 1
	SLIST_FOREACH(rt, &an->an_rtlist, rt_next) {
d572 2
a573 1
	if (rt == NULL)
d575 1
d579 1
d590 1
d602 1
a602 1
	rt = SLIST_FIRST(&an->an_rtlist);
d604 1
d622 2
d641 1
a641 1
		SLIST_FOREACH(mrt, &an->an_rtlist, rt_next) {
d676 1
d693 1
a693 1
		mrt = SLIST_FIRST(&an->an_rtlist);
d707 1
a707 1
			SLIST_FOREACH(mrt, &an->an_rtlist, rt_next) {
d722 1
a722 1
	SLIST_INSERT_HEAD(&an->an_rtlist, rt, rt_next);
d744 2
d750 1
a750 1
	SLIST_FOREACH(mrt, &an->an_rtlist, rt_next)
d757 3
a759 2
		SLIST_REMOVE(&an->an_rtlist, rt, rtentry, rt_next);
		KASSERT(rt->rt_refcnt >= 1);
d762 1
a762 1
		mrt = SLIST_FIRST(&an->an_rtlist);
d794 2
a795 2
	SLIST_REMOVE(&an->an_rtlist, rt, rtentry, rt_next);
	KASSERT(rt->rt_refcnt >= 1);
d818 3
a820 1
	SLIST_FOREACH_SAFE(rt, &an->an_rtlist, rt_next, nrt) {
d859 1
d863 1
a863 1
	SLIST_FOREACH(mrt, &an->an_rtlist, rt_next) {
d868 1
d872 1
a872 1
	mrt = SLIST_FIRST(&an->an_rtlist);
d876 1
a876 1
		mrt = SLIST_NEXT(mrt, rt_next);
d884 1
d895 3
a897 1
	SLIST_REMOVE(&an->an_rtlist, rt, rtentry, rt_next);
d900 1
a900 1
	if ((mrt = SLIST_FIRST(&an->an_rtlist)) != NULL) {
d904 1
a904 1
		while (SLIST_NEXT(mrt, rt_next) != NULL) {
d908 1
a908 1
			mrt = SLIST_NEXT(mrt, rt_next);
d913 1
a913 1
			SLIST_INSERT_AFTER(prt, rt, rt_next);
d916 1
a916 1
			SLIST_INSERT_AFTER(mrt, rt, rt_next);
d919 1
a919 1
		SLIST_INSERT_HEAD(&an->an_rtlist, rt, rt_next);
d926 2
a927 1
	return (SLIST_NEXT(rt, rt_next));
d930 16
@


1.26
log
@Protect the growth of the routing table arrays used by rtable_get()
with SRPs.

This is a simplified version of the dynamically sizeable array of
pointers used by if_get() because routing table heads are never
freed.

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.25 2015/11/24 12:06:30 mpi Exp $ */
d29 1
d36 5
a40 1
 * Per AF array.
a78 1
void		   rtable_free(unsigned int, sa_family_t);
a79 1
void		   rtable_put(void *);
a228 14
void
rtable_del(unsigned int id)
{
	struct domain	*dp;
	int		 i;

	for (i = 0; (dp = domains[i]) != NULL; i++) {
		if (dp->dom_rtoffset == 0)
			continue;

		rtable_free(id, dp->dom_family);
	}
}

d251 1
a251 1
	int		 i, exist = 0;
d259 1
a259 5
			exist = 1;
		rtable_put(tbl);

		if (exist)
			break;
d262 1
a262 1
	return (exist);
a314 10
void
rtable_free(unsigned int rtableid, sa_family_t af)
{
}

void
rtable_put(void *tbl)
{
}

d321 1
a321 1
	struct rtentry		*rt = NULL;
d329 1
a329 1
		goto out;
d337 1
a337 1
			goto out;
a341 3

out:
	rtable_put(rnh);
d350 1
a350 1
	struct rtentry		*rt = NULL;
d358 1
a358 1
		goto out;
a362 2
out:
	rtable_put(rnh);
a372 1
	int			 error = 0;
d383 1
a383 2
			error = EEXIST;
			goto out;
d386 1
a386 1
#endif
d389 2
a390 4
	if (rn == NULL) {
		error = ESRCH;
		goto out;
	}
d395 1
a395 3
out:
	rtable_put(rnh);
	return (error);
a403 1
	int			 error = 0;
d410 2
a411 4
	if (rn == NULL) {
		error = ESRCH;
		goto out;
	}
d419 1
a419 3
out:
	rtable_put(rnh);
	return (error);
a427 1
	int error;
d433 1
a433 4
	error = rn_walktree(rnh, f, arg);

	rtable_put(rnh);
	return (error);
a447 2

	rtable_put(rnh);
a517 20
void
rtable_free(unsigned int rtableid, sa_family_t af)
{
	struct art_root			*ar = NULL;
	struct rtmap 			*map;

	KERNEL_ASSERT_LOCKED();

	map = srp_get_locked(&afmap[af2idx[af]]);
	if (rtableid < map->limit) {
		ar = map->tbl[rtableid];
		KASSERT(ar->ar_root == NULL);
	}
}

void
rtable_put(void *tbl)
{
}

d524 1
a524 1
	struct rtentry			*rt = NULL;
d538 1
a538 1
			goto out;
d542 1
a542 1
			goto out;
d548 1
a548 1
			goto out;
d567 1
a567 1
		goto out;
a571 2
out:
	rtable_put(ar);
d580 1
a580 1
	struct rtentry			*rt = NULL;
d590 1
a590 1
		goto out;
a594 2
out:
	rtable_put(ar);
d605 1
a605 1
#endif
d609 1
a609 1
	int				 plen, error = 0;
d617 2
a618 4
	if (plen == -1) {
		error = EINVAL;
		goto out;
	}
d636 1
a636 2
			    	error = EEXIST;
			    	goto out;
d640 1
a640 1
#endif
d651 2
a652 4
		if (msk == NULL) {
			error = ENOMEM;
			goto out;
		}
d658 2
a659 4
	if (an == NULL) {
		error = ENOBUFS;
		goto out;
	}
d669 1
a669 2
		error = ESRCH;
		goto out;
d701 1
a701 2
		error = EEXIST;
		goto out;
d715 1
a715 3
out:
	rtable_put(ar);
	return (error);
d725 1
a725 1
	int				 plen, error = 0;
d762 1
a762 1
#endif
d767 2
a768 4
	if (art_delete(ar, an, addr, plen) == NULL) {
		error = ESRCH;
		goto out;
	}
d782 1
a782 3
out:
	rtable_put(ar);
	return (error);
a814 1
	int				 error;
d824 1
a824 4
	error = art_walk(ar, rtable_walk_helper, &rwc);

	rtable_put(ar);
	return (error);
d996 1
a996 1
#endif
@


1.25
log
@Provide art_free(), a method to release unused routing table heads.

While here initialize pools in art_init().
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.24 2015/11/10 10:23:27 mpi Exp $ */
d34 15
d52 4
a55 3
union rtmap {
	void		**tbl;
	unsigned int	 *dom;
d58 13
a70 2
union rtmap	  *rtmap;		/* Array of per domain routing table */
unsigned int	   rtables_id_max;
d74 1
a74 2
void		   rtable_free(unsigned int);
void		   rtable_grow(unsigned int, sa_family_t);
d79 62
d156 4
a159 2
		if (dp->dom_rtoffset)
			af2idx[dp->dom_family] = af2idx_max++;
a163 10

	rtables_id_max = 0;
	rtmap = mallocarray(af2idx_max + 1, sizeof(*rtmap), M_RTABLE, M_WAITOK);

	/* Start with a single table for every domain that requires it. */
	for (i = 0; i < af2idx_max + 1; i++) {
		rtmap[i].tbl = mallocarray(1, sizeof(rtmap[0].tbl),
		    M_RTABLE, M_WAITOK|M_ZERO);
	}

a164 1
}
d166 6
a171 20
void
rtable_grow(unsigned int id, sa_family_t af)
{
	void		**tbl, **ntbl;
	int		  i;

	KASSERT(id > rtables_id_max);

	KERNEL_ASSERT_LOCKED();

	tbl = rtmap[af2idx[af]].tbl;
	ntbl = mallocarray(id + 1, sizeof(rtmap[0].tbl), M_RTABLE, M_WAITOK);

	for (i = 0; i < rtables_id_max + 1; i++)
		ntbl[i] = tbl[i];

	while (i < id + 1) {
		ntbl[i] = NULL;
		i++;
	}
d173 1
a173 2
	rtmap[af2idx[af]].tbl = ntbl;
	free(tbl, M_RTABLE, (rtables_id_max + 1) * sizeof(rtmap[0].tbl));
d179 9
a187 5
	struct domain	 *dp;
	void		 *rtbl;
	sa_family_t	  af;
	unsigned int	  off;
	int		  i;
d202 2
a203 2
		if (id > rtables_id_max)
			rtable_grow(id, af);
d205 2
a206 2
		rtbl = rtable_alloc(id, af, off);
		if (rtbl == NULL)
d209 2
a210 1
		rtmap[af2idx[af]].tbl[id] = rtbl;
d214 3
a216 3
	if (id > rtables_id_max) {
		rtable_grow(id, 0);
		rtables_id_max = id;
d220 2
a221 2
	rtmap[0].dom[id] = 0;

d229 2
a230 6
	struct domain	 *dp;
	sa_family_t	  af;
	int		  i;

	if (id > rtables_id_max || !rtable_exists(id))
		return;
d236 1
a236 4
		af = dp->dom_family;

		rtable_free(id);
		rtmap[af2idx[af]].tbl[id] = NULL;
d243 2
a244 2
	if (af >= nitems(af2idx) || rtableid > rtables_id_max)
		return (NULL);
d246 1
a246 1
	if (af2idx[af] == 0 || rtmap[af2idx[af]].tbl == NULL)
d249 4
a252 2
	return (rtmap[af2idx[af]].tbl[rtableid]);
}
d254 1
a254 3
void
rtable_put(void *tbl)
{
a263 3
	if (rtableid > rtables_id_max)
		return (0);

d283 7
a289 2
	if (rtableid > rtables_id_max)
		return (0);
d291 1
a291 1
	return (rtmap[0].dom[rtableid]);
d295 1
a295 1
rtable_l2set(unsigned int rtableid, unsigned int parent)
d297 5
a301 1
	if (!rtable_exists(rtableid) || !rtable_exists(parent))
d304 2
a305 1
	rtmap[0].dom[rtableid] = parent;
d331 6
a336 1
rtable_free(unsigned int rtableid)
d566 16
a581 1
rtable_free(unsigned int rtableid)
@


1.24
log
@Allocate ART table's heap independently from the structure and use
pool(9) to not waste most of the memory allocated.

This reduces the memory overhead of our ART routing table from 80M
to 70M compared to the existing radix-tree when loading ~550K IPv4
routes.

ART can now be used for huge tables without exhausting malloc(9)'s
limit.

claudio@@ agrees with the direction, inputs from and ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.23 2015/11/09 12:15:29 mpi Exp $ */
d481 1
@


1.23
log
@Do not leave dangling pointers in the ART tree in case of memory
exhaustion.

Reported by benno@@ and found thanks to his bgpd(8) test setup.
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.22 2015/11/06 17:55:55 mpi Exp $ */
d481 1
a481 1
	pool_init(&an_pool, sizeof(struct art_node), 0, 0, 0, "art node", NULL);
@


1.22
log
@Rename rt_mpath_next() into rtable_mpath_next() and provide an
implementation for ART based on the singly-linked list of route
entries.
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.21 2015/11/06 17:44:45 mpi Exp $ */
d626 17
d654 2
a696 19

	/*
	 * XXX Allocating a sockaddr for the mask per node wastes a lot
	 * of memory, thankfully we'll get rid of that when rt_mask()
	 * will be no more.
	 */
	if (mask != NULL) {
		struct sockaddr		*msk;

		msk = malloc(dst->sa_len, M_RTABLE, M_NOWAIT | M_ZERO);
		if (msk == NULL) {
			pool_put(&an_pool, an);
			error = ENOMEM;
			goto out;
		}
		memcpy(msk, mask, dst->sa_len);
		rt->rt_mask = msk;
	}

@


1.21
log
@Use a SLIST instead of a LIST for MPATH route entries with ART.
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.20 2015/11/06 15:26:44 mpi Exp $ */
d432 1
a432 1
	while ((mrt = rt_mpath_next(mrt)) != NULL)
d440 1
a440 1
		mrt = rt_mpath_next(mrt);
d461 8
d897 6
@


1.20
log
@In ART separate the MPATH delete case to properly recover if art_delete()
does not find a matching node.

This currently never happens because we always do a route lookup before
calling rtable_delete().  Yes this is odd & due to the way multipath is
implemented in the radix tree.
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.19 2015/11/04 10:11:45 mpi Exp $ */
d521 1
a521 1
	rt = LIST_FIRST(&an->an_rtlist);
d523 1
a523 1
	LIST_FOREACH(rt, &an->an_rtlist, rt_next) {
d563 1
a563 1
	rt = LIST_FIRST(&an->an_rtlist);
d603 1
a603 1
		LIST_FOREACH(mrt, &an->an_rtlist, rt_next) {
d641 1
a641 2
		mrt = LIST_FIRST(&an->an_rtlist);

d655 1
a655 1
			LIST_FOREACH(mrt, &an->an_rtlist, rt_next) {
d690 1
a690 1
	LIST_INSERT_HEAD(&an->an_rtlist, rt, rt_next);
d718 1
a718 1
	LIST_FOREACH(mrt, &an->an_rtlist, rt_next)
d725 1
a725 1
		LIST_REMOVE(rt, rt_next);
d729 1
a729 1
		mrt = LIST_FIRST(&an->an_rtlist);
d763 1
a763 1
	LIST_REMOVE(rt, rt_next);
d789 1
a789 1
	LIST_FOREACH_SAFE(rt, &an->an_rtlist, rt_next, nrt) {
d835 1
a835 1
	LIST_FOREACH(mrt, &an->an_rtlist, rt_next) {
d843 1
a843 1
	mrt = LIST_FIRST(&an->an_rtlist);
d847 1
a847 1
		mrt = LIST_NEXT(mrt, rt_next);
d863 1
a863 1
	struct rtentry			*mrt;
d865 1
a865 1
	LIST_REMOVE(rt, rt_next);
d868 1
a868 1
	if ((mrt = LIST_FIRST(&an->an_rtlist)) != NULL) {
d872 1
a872 1
		while (LIST_NEXT(mrt, rt_next) != NULL) {
d875 2
a876 1
			mrt = LIST_NEXT(mrt, rt_next);
d879 7
a885 4
		if (mrt->rt_priority > prio)
			LIST_INSERT_BEFORE(mrt, rt, rt_next);
		else
			LIST_INSERT_AFTER(mrt, rt, rt_next);
d887 1
a887 1
		LIST_INSERT_HEAD(&an->an_rtlist, rt, rt_next);
@


1.19
log
@Initialize the correct variable in ART's rtable_match().
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.18 2015/11/04 09:50:21 mpi Exp $ */
d710 27
a736 1
	int				 plen;
d749 8
a761 2

	/* Remove rt <-> ART glue. */
a767 17
#ifndef SMALL_KERNEL
	if ((rt = LIST_FIRST(&an->an_rtlist)) != NULL) {
		an->an_dst = rt->rt_dest;
		if (LIST_NEXT(rt, rt_next) == NULL)
			rt->rt_flags &= ~RTF_MPATH;
		rtable_put(ar);
		return (0);
	}
#endif /* SMALL_KERNEL */

	addr = satoaddr(ar, an->an_dst);
	plen = an->an_plen;

	/* XXX should return ESRCH and recover properly. */
	if (art_delete(ar, an, addr, plen) == NULL)
		panic("unable to delete art note");

d769 1
d771 1
a771 1
	return (0);
@


1.18
log
@Some tweaks to build the rtable API and backends in userland.

Needed by the regression tests.
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.17 2015/11/04 09:48:09 mpi Exp $ */
a548 1
	struct rtentry			*rt;
d550 2
a551 1
	struct art_node			*an = NULL;
@


1.17
log
@Call rtable_put(), a stub for now, before leaving a function that called
rtable_get().
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.16 2015/11/02 14:40:09 mpi Exp $ */
d19 3
d29 1
@


1.16
log
@Merge rtable_mpath_match() into rtable_lookup().

ok bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.15 2015/10/25 14:48:51 mpi Exp $ */
d46 1
d186 5
d195 2
a196 1
	int		 i;
d205 7
a211 2
		if (rtable_get(rtableid, dp->dom_family) != NULL)
			return (1);
d214 1
a214 1
	return (0);
d268 1
a268 1
	struct rtentry		*rt;
d276 1
a276 1
		return (NULL);
d284 1
a284 1
			return (NULL);
d290 2
d300 1
a300 1
	struct rtentry		*rt;
d308 1
a308 1
		return (NULL);
d313 2
d325 1
d335 4
a338 2
	    	    ISSET(rt->rt_flags, RTF_MPATH)))
			return (EEXIST);
d343 4
a346 2
	if (rn == NULL)
		return (ESRCH);
d351 3
a353 1
	return (0);
d362 1
d369 4
a372 2
	if (rn == NULL)
		return (ESRCH);
d380 3
a382 1
	return (0);
d391 1
d397 4
a400 1
	return (rn_walktree(rnh, f, arg));
d408 1
d414 4
a417 1
	return (rnh->rnh_multipath);
d489 1
a489 1
	struct rtentry			*rt;
d502 2
d507 1
a507 1
			return (NULL);
d513 1
a513 1
			return (NULL);
a515 3
	if (an == NULL)
		return (NULL);

d532 1
a532 1
		return (NULL);
d537 2
d547 1
a547 1
	struct art_node			*an;
d557 1
a557 1
		return (NULL);
d562 2
d578 1
a578 1
	int				 plen;
d586 4
a589 2
	if (plen == -1)
		return (EINVAL);
d604 6
a609 6
			if (!mpathok)
				return (EEXIST);

			if (mrt->rt_gateway->sa_len == gateway->sa_len &&
			    !memcmp(mrt->rt_gateway, gateway, gateway->sa_len))
				return (EEXIST);
d615 4
a618 2
	if (an == NULL)
		return (ENOBUFS);
d626 2
a627 1
		return (ESRCH);
d660 2
a661 1
		return (EEXIST);
d679 2
a680 1
			return (ENOMEM);
d694 3
a696 1
	return (0);
d713 4
a716 8
	if (memcmp(dst, an->an_dst, dst->sa_len)) {
		printf("%s: destination do not match\n", __func__);
		return (EINVAL);
	}
	if (mask != NULL && an->an_plen != satoplen(ar, mask)) {
		printf("%s: mask do not match\n", __func__);
		return (EINVAL);
	}
d737 1
d745 1
d747 1
a747 1
		return (ESRCH);
d750 1
a750 1

d784 1
d794 4
a797 1
	return art_walk(ar, rtable_walk_helper, &rwc);
@


1.15
log
@Merge rtable_mpath_conflict() into rtable_insert().

ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.14 2015/10/22 17:19:38 mpi Exp $ */
d252 1
a252 1
    struct sockaddr *mask)
d267 9
a383 20
struct rtentry *
rtable_mpath_match(unsigned int rtableid, struct rtentry *rt0,
    struct sockaddr *gateway, uint8_t prio)
{
	struct radix_node_head	*rnh;
	struct rtentry		*rt;

	rnh = rtable_get(rtableid, rt_key(rt0)->sa_family);
	if (rnh == NULL || rnh->rnh_multipath == 0)
		return (rt0);

	rt = rt_mpath_matchgate(rt0, gateway, prio);

	if (rt != NULL)
		rtref(rt);
	rtfree(rt0);

	return (rt);
}

d449 1
a449 1
    struct sockaddr *mask)
d481 1
d483 17
a754 27
}

struct rtentry *
rtable_mpath_match(unsigned int rtableid, struct rtentry *rt0,
    struct sockaddr *gateway, uint8_t prio)
{
	struct art_node			*an = rt0->rt_node;
	struct rtentry			*rt;

	LIST_FOREACH(rt, &an->an_rtlist, rt_next) {
		if (prio != RTP_ANY &&
		    (rt->rt_priority & RTP_MASK) != (prio & RTP_MASK))
			continue;

		if (gateway == NULL)
			break;

		if (rt->rt_gateway->sa_len == gateway->sa_len &&
		    memcmp(rt->rt_gateway, gateway, gateway->sa_len) == 0)
			break;
	}

	if (rt != NULL)
		rtref(rt);
	rtfree(rt0);

	return (rt);
@


1.14
log
@Use only one refcounting mechanism for route entries.

ok bluhm@@, dlg@@, claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.13 2015/10/21 08:47:01 mpi Exp $ */
d295 2
a296 1
    struct sockaddr *mask, uint8_t prio, struct rtentry *rt)
d305 9
a394 16
int
rtable_mpath_conflict(unsigned int rtableid, struct sockaddr *dst,
    struct sockaddr *mask, struct sockaddr *gateway, uint8_t prio, int mpathok)
{
	struct radix_node_head	*rnh;

	rnh = rtable_get(rtableid, dst->sa_family);
	if (rnh == NULL)
		return (EAFNOSUPPORT);

	if (rnh->rnh_multipath == 0)
		return (0);

	return (rt_mpath_conflict(rnh, dst, mask, gateway, prio, mpathok));
}

d523 2
a524 1
    struct sockaddr *mask, uint8_t prio, struct rtentry *rt)
d543 23
a774 42
}

int
rtable_mpath_conflict(unsigned int rtableid, struct sockaddr *dst,
    struct sockaddr *mask, struct sockaddr *gateway, uint8_t prio, int mpathok)
{
	struct art_root			*ar;
	struct art_node			*an;
	struct rtentry			*rt;
	uint8_t				*addr;
	int				 plen;

	ar = rtable_get(rtableid, dst->sa_family);
	if (ar == NULL)
		return (EAFNOSUPPORT);

	addr = satoaddr(ar, dst);
	plen = satoplen(ar, mask);
	if (plen == -1)
		return (EINVAL);

	an = art_lookup(ar, addr, plen);
	/* Make sure we've got a perfect match. */
	if (an == NULL || an->an_plen != plen ||
	    memcmp(an->an_dst, dst, dst->sa_len))
		return (0);

	LIST_FOREACH(rt, &an->an_rtlist, rt_next) {
		if (prio != RTP_ANY &&
		    (rt->rt_priority & RTP_MASK) != (prio & RTP_MASK))
			continue;

		if (!mpathok)
			return (EEXIST);

		if (rt->rt_gateway->sa_len == gateway->sa_len &&
		    memcmp(rt->rt_gateway, gateway, gateway->sa_len) == 0)
			return (EEXIST);
	}


	return (0);
@


1.13
log
@Return the correct error code when a table already exists.
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.12 2015/10/14 10:09:30 mpi Exp $ */
d332 3
d660 2
a661 1
	KASSERT(rt->rt_refcnt >= 0);
@


1.12
log
@Rewrite the logic around the dymanic array of routing tables to help
turning rtable_get(9) MP-safe.

Use only one per-AF array, as suggested by claudio@@, pointing to an
array of pointers to the routing table heads.

Routing tables are now allocated/initialized per-AF.  This will let
us allocate routing table on-demand instead of always having an
AF_INET, AF_MPLS and AF_INET table as soon as a new rtableID is used.

This also get rid of the "void ***" madness.

ok dlg@@, jmatthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.11 2015/10/07 11:39:49 mpi Exp $ */
d116 1
a116 1
	if (id > RT_TABLEID_MAX || rtable_exists(id))
d118 3
@


1.11
log
@Make rtable_get() private to ensure it won't be used outside of
net/rtable.c.  This will ease the introduction of rtable_put().

Routing tables are mapped to a tuple (idx, af) so the public API
should as much as possible require these two keys.

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.10 2015/10/07 10:50:35 mpi Exp $ */
d31 1
a31 1
uint8_t		   af2idx_max = 1;	/* Must have NULL at index 0 */
d33 7
a39 3
void		***rtables;		/* Array of routing tables */
unsigned int	   rtables_id_max = 0;
unsigned int	  *rtables2dom;		/* rtable to domain lookup table */
d42 3
a44 1
int		   rtable_attach(unsigned int, sa_family_t, int);
a45 1
void		   rtable_set(unsigned int, sa_family_t, void *);
d54 2
d67 10
d82 25
d110 5
a114 3
	struct domain	*dp;
	void		*p, *q;
	int		 i, rv = 0;
d116 1
a116 1
	if (id > RT_TABLEID_MAX)
d119 12
a130 3
	if (id == 0 || id > rtables_id_max) {
		if ((p = mallocarray(id + 1, sizeof(void *), M_RTABLE,
		    M_NOWAIT|M_ZERO)) == NULL)
d133 6
a138 17
		if ((q = mallocarray(id + 1, sizeof(unsigned int), M_RTABLE,
		    M_NOWAIT|M_ZERO)) == NULL) {
			free(p, M_RTABLE, (id + 1) * sizeof(void *));
			return (ENOMEM);
		}
		if (rtables) {
			memcpy(p, rtables, (rtables_id_max+1) * sizeof(void *));
			free(rtables, M_RTABLE,
			    (rtables_id_max+1) * sizeof(void *));

			memcpy(q, rtables2dom,
			    (rtables_id_max+1) * sizeof(unsigned int));
			free(rtables2dom, M_RTABLE,
			    (rtables_id_max+1) * sizeof(unsigned int));
		}
		rtables = p;
		rtables2dom = q;
d142 3
a144 2
	if (rtables[id] != NULL)	/* already exists */
		return (EEXIST);
d146 12
a157 5
	rtables2dom[id] = 0;	/* use main table/domain by default */
	rtables[id] = mallocarray(af2idx_max + 1, sizeof(void *), M_RTABLE,
	    M_NOWAIT|M_ZERO);
	if (rtables[id] == NULL)
		return (ENOMEM);
a158 1
	/* Per domain initialization. */
d162 5
a166 1
		rv |= rtable_attach(id, dp->dom_family, dp->dom_rtoffset);
d168 1
a169 2
	return (rv);
}
d173 1
a173 1
	if (rtableid > rtables_id_max)
a174 2
	return (rtables[rtableid] ? rtables[rtableid][af2idx[af]] : NULL);
}
d176 2
a177 5
void
rtable_set(unsigned int rtableid, sa_family_t af, void *p)
{
	if (rtableid > rtables_id_max)
		return;
d179 1
a179 2
	if (rtables[rtableid])
		rtables[rtableid][af2idx[af]] = p;
d185 3
d191 7
a197 2
	if (rtables[rtableid] == NULL)
		return (0);
d199 1
a199 1
	return (1);
d208 1
a208 1
	return (rtables2dom[rtableid]);
d217 1
a217 1
	rtables2dom[rtableid] = parent;
d227 2
a228 2
int
rtable_attach(unsigned int rtableid, sa_family_t af, int off)
d230 1
a230 6
	struct radix_node_head *rnh;
	int rv = 1;

	rnh = rtable_get(rtableid, af);
	if (rnh != NULL)
		return (EEXIST);
a236 1
		rv = 0;
d239 2
a240 1
	rtable_set(rtableid, af, rnh);
d242 3
a244 1
	return (rv);
d447 2
a448 2
int
rtable_attach(unsigned int rtableid, sa_family_t af, int off)
d450 2
a451 1
	struct art_root			*ar;
d453 3
a455 11
	ar = rtable_get(rtableid, af);
	if (ar != NULL)
		return (EEXIST);

	ar = art_attach(rtableid, off);
	if (ar == NULL)
		return (ENOMEM);

	rtable_set(rtableid, af, ar);

	return (0);
@


1.10
log
@Initialize the routing table before domains.

The routing table is not an optional component of the network stack
and initializing it inside the "routing domain" requires some ugly
introspection in the domain interface.

This put the rtable* layer at the same level of the if* level.  These
two subsystem are organized around the two global data structure used
in the network stack:

- the global &ifnet list, to be used in process context only, and
- the routing table which can be read in interrupt context.

This change makes the rtable_* layer domain-aware and extends the
"struct domain" such that INET, INET6 and MPLS can specify the length
of the binary key used in lookups.  This allows us to keep, or move
towards, AF-free route and rtable layers.

While here stop the madness and pass the size of the maximum key length
in *byte* to rn_inithead0().

ok claudio@@, mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.9 2015/10/07 08:43:36 mpi Exp $ */
d39 1
@


1.9
log
@Move the reference counting of a newly created route entry inside
rtable_insert().

inputs and ok bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.8 2015/09/28 08:47:53 mpi Exp $ */
d25 1
d30 135
a165 1

d167 1
a167 1
rtable_init(unsigned int keylen)
d173 1
a173 1
rtable_attach(void **head, int off)
d175 2
a176 1
	int rv;
d178 3
a180 1
	rv = rn_inithead(head, off);
d182 1
a183 2
	if (rv == 1) {
		struct radix_node_head *rnh = (struct radix_node_head *)*head;
d185 3
d189 2
a190 1
#endif /* SMALL_KERNEL */
a280 13
rtable_setid(void **p, unsigned int rtableid, sa_family_t af)
{
	struct radix_node_head **rnh = (struct radix_node_head **)p;

	if (rnh == NULL || rnh[af] == NULL)
		return (EINVAL);

	rnh[af]->rnh_rtableid = rtableid;

	return (0);
}

int
d285 1
a285 1
	int (*f)(struct radix_node *, void *, u_int) = (void *)func;
d390 1
a390 1
rtable_init(unsigned int keylen)
d396 1
a396 1
rtable_attach(void **head, int off)
d398 13
a410 1
	return (art_attach(head, off));
a626 13

	return (0);
}

int
rtable_setid(void **p, unsigned int rtableid, sa_family_t af)
{
	struct art_root			**ar = (struct art_root **)p;

	if (ar == NULL || ar[af] == NULL)
		return (EINVAL);

	ar[af]->ar_rtableid = rtableid;
@


1.8
log
@Use the radix-tree API instead of function pointers.
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.7 2015/09/28 08:36:24 mpi Exp $ */
d112 3
d424 1
@


1.7
log
@Factors ou the route hashing code to implement Equal-Cost Multi-Path
for ART.

While here sync the two remaining mix() macros.

ok chris@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.6 2015/09/12 09:22:29 mpi Exp $ */
d66 1
a66 1
	rn = rnh->rnh_lookup(dst, mask, rnh);
d87 1
a87 1
	rn = rnh->rnh_matchaddr(dst, rnh);
d108 1
a108 1
	rn = rnh->rnh_addaddr(dst, mask, rnh, rn, prio);
d126 1
a126 1
	rn = rnh->rnh_deladdr(dst, mask, rnh, rn);
d160 1
a160 1
	return (*rnh->rnh_walktree)(rnh, f, arg);
@


1.6
log
@Use rtref(9) in rtable_match() before returning a route entry.

ok bluhm@@, claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.5 2015/09/11 16:58:00 mpi Exp $ */
d42 2
d45 5
a49 4
	rv = rn_mpath_inithead(head, off);
#else
	rv = rn_inithead(head, off);
#endif
d212 1
d214 1
a214 1
rtable_mpath_select(struct rtentry *rt, uint32_t *src)
d216 24
a239 1
	return (rn_mpath_select(rt, src));
d421 2
d424 2
a425 17
	if ((mrt = LIST_FIRST(&an->an_rtlist)) != NULL) {
		/*
		 * Select the order of the MPATH routes.
		 */
		while (LIST_NEXT(mrt, rt_next) != NULL) {
			if (mrt->rt_priority > prio)
				break;
			mrt = LIST_NEXT(mrt, rt_next);
		}

		if (mrt->rt_priority > prio)
			LIST_INSERT_BEFORE(mrt, rt, rt_next);
		else
			LIST_INSERT_AFTER(mrt, rt, rt_next);

		return (0);
	}
a427 1
	LIST_INSERT_HEAD(&an->an_rtlist, rt, rt_next);
d618 1
d620 1
a620 1
rtable_mpath_select(struct rtentry *rt, uint32_t *src)
d623 24
d648 1
a648 4
	/*
	 * XXX consider using ``src'' (8
	 */
	return (LIST_FIRST(&an->an_rtlist));
d652 1
a652 1
rtable_mpath_reprio(struct rtentry *rt, uint8_t newprio)
d654 23
a676 1
	/* XXX */
@


1.5
log
@Introduce rtref(9) use it in rtable_lookup() before returning a route
entry.

ok bluhm@@, claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.4 2015/09/04 08:43:39 mpi Exp $ */
d78 1
d88 4
a91 1
	return ((struct rtentry *)rn);
d286 1
d300 4
a303 1
	return (LIST_FIRST(&an->an_rtlist));
@


1.4
log
@Make every subsystem using a radix tree call rn_init() and pass the
length of the key as argument.

This way every consumer of the radix tree has a chance to explicitly
initialize the shared data structures and no longer rely on another
subsystem to do the initialization.

As a bonus ``dom_maxrtkey'' is no longer used an die.

ART kernels should now be fully usable because pf(4) and IPSEC properly
initialized the radix tree.

ok chris@@, reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.3 2015/08/20 12:51:10 mpi Exp $ */
d57 1
d67 4
a70 1
	return ((struct rtentry *)rn);
d170 1
a170 1
rtable_mpath_match(unsigned int rtableid, struct rtentry *rt,
d174 1
d176 1
a176 1
	rnh = rtable_get(rtableid, rt_key(rt)->sa_family);
d178 3
a180 1
		return (rt);
d182 3
a184 1
	rt = rt_mpath_matchgate(rt, gateway, prio);
d245 1
d273 4
a276 1
	return (LIST_FIRST(&an->an_rtlist));
d541 1
a541 1
			return (rt);
d547 4
@


1.3
log
@Make ART internals free of 'struct sockaddr'.

Keep route entry/BSD compatibility goos in the rtable layer.  The way
addresses and masks (prefix-lengths) are encoded is really tied to the
radix-tree implementation.

Since we decided to no longer support non-contiguous masks, we could get
rid of some extra "sockaddr" allocations and reduce the memory grows
related to the use of a multibit-trie.
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.2 2015/08/20 12:39:43 mpi Exp $ */
d32 1
a32 1
rtable_init(void)
d34 1
a34 1
	rn_init();
d219 1
a219 1
rtable_init(void)
@


1.2
log
@Import an alternative routing table backend based on Yoichi Hariguchi's
ART implementation.

ART (Allotment Routing Table) is a multibit-trie algorithm invented by
D. Knuth while reviewing Yoichi's SMART [0] (Smart Multi-Array Routing
Table) paper.

This implementation, unlike the one from the KAME project, supports
variable stride lengths which makes it easier to adapt the consumed
memory/speed trade-off.  It also let you use a bigger first-level
table, what other algorithms such as POPTRIE [1] need to implement
separately.

Adaptation to the OpenBSD kernel has been done with two different data
structures.  ART nodes and route entries are managed separately which
makes the algorithm implementation free of any MULTIPATH logic.

This implementation does not include Path Compression.

[0] http://www.hariguchi.org/art/smart.pdf
[1] http://conferences.sigcomm.org/sigcomm/2015/pdf/papers/p57.pdf

ok dlg@@, reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: rtable.c,v 1.1 2015/07/18 15:51:16 mpi Exp $ */
d215 2
a216 1
static inline int	satoplen(struct art_root *, struct sockaddr *);
d236 1
d243 2
d247 1
a247 1
		an = art_match(ar, dst);
d252 6
a257 1
		an = art_lookup(ar, dst, plen);
d271 1
d277 2
a278 1
	an = art_match(ar, dst);
d294 2
a295 1
	int				 plen = 0;
d301 1
d313 1
a313 1
	prev = art_insert(ar, an);
d403 2
d442 4
a445 1
	if (art_delete(ar, an) == NULL)
d545 1
d552 1
d556 5
a560 2
	an = art_lookup(ar, dst, plen);
	if (an == NULL)
d597 11
@


1.1
log
@Abstract the routing table internals behind an rtable_* API.

Code abusing the radix internals for the routing table should now
includes <net/rtable.h> and only deal with "struct rtentry".

Code using a radix tree for another purpose can still include
<net/radix.h>.

Inputs from and ok claudio@@, mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
d22 3
d29 2
d209 445
d655 4
@

