head	1.27;
access;
symbols
	OPENBSD_6_0:1.22.0.4
	OPENBSD_6_0_BASE:1.22
	OPENBSD_5_9:1.12.0.2
	OPENBSD_5_9_BASE:1.12;
locks; strict;
comment	@ * @;


1.27
date	2017.02.28.09.50.13;	author mpi;	state Exp;
branches;
next	1.26;
commitid	15ZiiVy1UrhNDMlv;

1.26
date	2017.01.24.10.08.30;	author krw;	state Exp;
branches;
next	1.25;
commitid	6c6qq5OdS4VVnyVM;

1.25
date	2017.01.23.01.02.11;	author claudio;	state Exp;
branches;
next	1.24;
commitid	2WFWmxdDK0ywflVw;

1.24
date	2016.09.15.02.00.18;	author dlg;	state Exp;
branches;
next	1.23;
commitid	RlO92XR575sygHqm;

1.23
date	2016.08.30.07.42.57;	author jmatthew;	state Exp;
branches;
next	1.22;
commitid	wt6F4HnTFVjJdyuV;

1.22
date	2016.07.19.10.51.44;	author mpi;	state Exp;
branches;
next	1.21;
commitid	yRNJr6JjtSzwhCMC;

1.21
date	2016.07.04.08.11.48;	author mpi;	state Exp;
branches;
next	1.20;
commitid	cJAFZjtalegoa3C8;

1.20
date	2016.06.22.06.32.32;	author dlg;	state Exp;
branches;
next	1.19;
commitid	ZyonIGZ7QrSF1GGx;

1.19
date	2016.06.14.04.42.02;	author jmatthew;	state Exp;
branches;
next	1.18;
commitid	s3pvPwhE6w34MZJL;

1.18
date	2016.06.03.03.59.43;	author dlg;	state Exp;
branches;
next	1.17;
commitid	Xzrk9iYAqOSXhbNS;

1.17
date	2016.06.02.00.39.22;	author dlg;	state Exp;
branches;
next	1.16;
commitid	BLhTwGjSeWDZrKAA;

1.16
date	2016.06.02.00.34.13;	author dlg;	state Exp;
branches;
next	1.15;
commitid	en2KxUogGexSFGkF;

1.15
date	2016.06.01.06.19.06;	author dlg;	state Exp;
branches;
next	1.14;
commitid	f3s29nGcU8u2IWqh;

1.14
date	2016.04.13.08.04.14;	author mpi;	state Exp;
branches;
next	1.13;
commitid	no0HF32SpYAdh1Nd;

1.13
date	2016.04.12.06.40.44;	author mpi;	state Exp;
branches;
next	1.12;
commitid	fQD26XAfjWrjB95M;

1.12
date	2016.01.18.18.27.11;	author mpi;	state Exp;
branches;
next	1.11;
commitid	KBXvsgMO1460dNbP;

1.11
date	2015.12.04.14.15.27;	author mpi;	state Exp;
branches;
next	1.10;
commitid	Tmqp3Fwt0FRfLqfh;

1.10
date	2015.11.24.12.48.20;	author dlg;	state Exp;
branches;
next	1.9;
commitid	wmpiw2DUJTQTsJt8;

1.9
date	2015.11.24.12.06.30;	author mpi;	state Exp;
branches;
next	1.8;
commitid	EtocpAAodAId7blH;

1.8
date	2015.11.12.14.29.04;	author mpi;	state Exp;
branches;
next	1.7;
commitid	omXY7948Aj01SEWE;

1.7
date	2015.11.10.10.23.27;	author mpi;	state Exp;
branches;
next	1.6;
commitid	nXhvmQzJFCbK3ydu;

1.6
date	2015.11.04.09.50.21;	author mpi;	state Exp;
branches;
next	1.5;
commitid	vubJzb8H8Cr58s7n;

1.5
date	2015.10.14.10.09.30;	author mpi;	state Exp;
branches;
next	1.4;
commitid	PpW3O2bSAUrWDZcl;

1.4
date	2015.10.07.10.50.35;	author mpi;	state Exp;
branches;
next	1.3;
commitid	hPrd2a6fiZdlN2yP;

1.3
date	2015.08.20.12.51.10;	author mpi;	state Exp;
branches;
next	1.2;
commitid	kbd3EInNuY47D3oQ;

1.2
date	2015.08.20.12.41.54;	author mpi;	state Exp;
branches;
next	1.1;
commitid	i21JURiWWTs5Fwa0;

1.1
date	2015.08.20.12.39.43;	author mpi;	state Exp;
branches;
next	;
commitid	9IUogeilRo73xUDL;


desc
@@


1.27
log
@Prevent a MP race in rtable_lookup().

If an ART node is linked to multiple route entries, in the MPATH case,
it is not safe to dereference ``an_dst''.  This non-refcounted pointer
can be changed at any time by another CPU.

So get rid of the pointer and use the first destination of a route entry
when comparing sockaddrs.

This allows us so remove a pointer from 'struct art_node' and save 5Mb of
memory in an IPv4 fullfeed.

ok jmatthew@@, claudio@@, dlg@@
@
text
@/*	$OpenBSD: art.c,v 1.26 2017/01/24 10:08:30 krw Exp $ */

/*
 * Copyright (c) 2015 Martin Pieuchot
 * Copyright (c) 2001 Yoichi Hariguchi
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

/*
 * Allotment Routing Table (ART).
 *
 * Yoichi Hariguchi paper can be found at:
 *	http://www.hariguchi.org/art/art.pdf
 */

#ifndef _KERNEL
#include "kern_compat.h"
#else
#include <sys/param.h>
#include <sys/systm.h>
#include <sys/malloc.h>
#include <sys/pool.h>
#include <sys/task.h>
#include <sys/socket.h>
#endif

#include <net/art.h>

#define ISLEAF(e)	(((unsigned long)(e) & 1) == 0)
#define SUBTABLE(e)	((struct art_table *)((unsigned long)(e) & ~1))
#define ASNODE(t)	((struct art_node *)((unsigned long)(t) | 1))

/*
 * Allotment Table.
 */
struct art_table {
	struct art_table	*at_parent;	/* Parent table */
	uint32_t		 at_index;	/* Index in the parent table */
	uint32_t		 at_minfringe;	/* Index that fringe begins */
	uint32_t		 at_level;	/* Level of the table */
	uint8_t			 at_bits;	/* Stride length of the table */
	uint8_t			 at_offset;	/* Sum of parents' stride len */

	/*
	 * Items stored in the heap are pointers to nodes, in the leaf
	 * case, or tables otherwise.  One exception is index 0 which
	 * is a route counter.
	 */
	union {
		struct srp		 node;
		unsigned long		 count;
	} *at_heap;				/* Array of 2^(slen+1) items */
};
#define	at_refcnt	at_heap[0].count/* Refcounter (1 per different route) */
#define	at_default	at_heap[1].node	/* Default route (was in parent heap) */

/* Heap size for an ART table of stride length ``slen''. */
#define AT_HEAPSIZE(slen)	((1 << ((slen) + 1)) * sizeof(void *))

int			 art_bindex(struct art_table *, uint8_t *, int);
void			 art_allot(struct art_table *at, int, struct art_node *,
			     struct art_node *);
struct art_table	*art_table_get(struct art_root *, struct art_table *,
			     int);
struct art_table	*art_table_put(struct art_root *, struct art_table *);
struct art_node		*art_table_insert(struct art_root *, struct art_table *,
			     int, struct art_node *);
struct art_node		*art_table_delete(struct art_root *, struct art_table *,
			     int, struct art_node *);
struct art_table	*art_table_ref(struct art_root *, struct art_table *);
int			 art_table_free(struct art_root *, struct art_table *);
int			 art_table_walk(struct art_root *, struct art_table *,
			     int (*f)(struct art_node *, void *), void *);
int			 art_walk_apply(struct art_root *,
			     struct art_node *, struct art_node *,
			     int (*f)(struct art_node *, void *), void *);
void			 art_table_gc(void *);
void			 art_gc(void *);

struct pool		an_pool, at_pool, at_heap_4_pool, at_heap_8_pool;

struct art_table	*art_table_gc_list = NULL;
struct mutex		 art_table_gc_mtx = MUTEX_INITIALIZER(IPL_SOFTNET);
struct task		 art_table_gc_task =
			     TASK_INITIALIZER(art_table_gc, NULL);

struct art_node		*art_node_gc_list = NULL;
struct mutex		 art_node_gc_mtx = MUTEX_INITIALIZER(IPL_SOFTNET);
struct task		 art_node_gc_task = TASK_INITIALIZER(art_gc, NULL);

void
art_init(void)
{
	pool_init(&an_pool, sizeof(struct art_node), 0, IPL_SOFTNET, 0,
	    "art_node", NULL);
	pool_init(&at_pool, sizeof(struct art_table), 0, IPL_SOFTNET, 0,
	    "art_table", NULL);
	pool_init(&at_heap_4_pool, AT_HEAPSIZE(4), 0, IPL_SOFTNET, 0,
	    "art_heap4", NULL);
	pool_init(&at_heap_8_pool, AT_HEAPSIZE(8), 0, IPL_SOFTNET, 0,
	    "art_heap8", &pool_allocator_single);
}

/*
 * Per routing table initialization API function.
 */
struct art_root *
art_alloc(unsigned int rtableid, unsigned int alen, unsigned int off)
{
	struct art_root		*ar;
	int			 i;

	ar = malloc(sizeof(*ar), M_RTABLE, M_NOWAIT|M_ZERO);
	if (ar == NULL)
		return (NULL);

	switch (alen) {
	case 32:
		ar->ar_alen = 32;
		ar->ar_nlvl = 7;
		ar->ar_bits[0] = 8;
		for (i = 1; i < ar->ar_nlvl; i++)
			ar->ar_bits[i] = 4;
		break;
	case 128:
		ar->ar_alen = 128;
		ar->ar_nlvl = 32;
		for (i = 0; i < ar->ar_nlvl; i++)
			ar->ar_bits[i] = 4;
		break;
	default:
		printf("%s: incorrect address length %u\n", __func__, alen);
		free(ar, M_RTABLE, sizeof(*ar));
		return (NULL);
	}

	ar->ar_off = off;
	ar->ar_rtableid = rtableid;
	rw_init(&ar->ar_lock, "art");

	return (ar);
}

/*
 * Return 1 if ``old'' and ``new`` are identical, 0 otherwise.
 */
static inline int
art_check_duplicate(struct art_root *ar, struct art_node *old,
    struct art_node *new)
{
	if (old == NULL)
		return (0);

	if (old->an_plen == new->an_plen)
		return (1);

	return (0);
}

/*
 * Return the base index of the part of ``addr'' and ``plen''
 * corresponding to the range covered by the table ``at''.
 *
 * In other words, this function take the multi-level (complete)
 * address ``addr'' and prefix length ``plen'' and return the
 * single level base index for the table ``at''.
 *
 * For example with an address size of 32bit divided into four
 * 8bit-long tables, there's a maximum of 4 base indexes if the
 * prefix length is > 24.
 */
int
art_bindex(struct art_table *at, uint8_t *addr, int plen)
{
	uint8_t			boff, bend;
	uint32_t		k;

	if (plen < at->at_offset || plen > (at->at_offset + at->at_bits))
		return (-1);

	/*
	 * We are only interested in the part of the prefix length
	 * corresponding to the range of this table.
	 */
	plen -= at->at_offset;

	/*
	 * Jump to the first byte of the address containing bits
	 * covered by this table.
	 */
	addr += (at->at_offset / 8);

	/* ``at'' covers the bit range between ``boff'' & ``bend''. */
	boff = (at->at_offset % 8);
	bend = (at->at_bits + boff);

	KASSERT(bend <= 32);

	if (bend > 24) {
		k = (addr[0] & ((1 << (8 - boff)) - 1)) << (bend - 8);
		k |= addr[1] << (bend - 16);
		k |= addr[2] << (bend - 24);
		k |= addr[3] >> (32 - bend);
	} else if (bend > 16) {
		k = (addr[0] & ((1 << (8 - boff)) - 1)) << (bend - 8);
		k |= addr[1] << (bend - 16);
		k |= addr[2] >> (24 - bend);
	} else if (bend > 8) {
		k = (addr[0] & ((1 << (8 - boff)) - 1)) << (bend - 8);
		k |= addr[1] >> (16 - bend);
	} else {
		k = (addr[0] >> (8 - bend)) & ((1 << at->at_bits) - 1);
	}

	/*
	 * Single level base index formula:
	 */
	return ((k >> (at->at_bits - plen)) + (1 << plen));
}

/*
 * Single level lookup function.
 *
 * Return the fringe index of the part of ``addr''
 * corresponding to the range covered by the table ``at''.
 */
static inline int
art_findex(struct art_table *at, uint8_t *addr)
{
	return art_bindex(at, addr, (at->at_offset + at->at_bits));
}

/*
 * (Non-perfect) lookup API function.
 *
 * Return the best existing match for a destination.
 */
struct art_node *
art_match(struct art_root *ar, void *addr, struct srp_ref *nsr)
{
	struct srp_ref		dsr, ndsr;
	void			*entry;
	struct art_table	*at;
	struct art_node		*dflt, *ndflt;
	int			j;

	entry = srp_enter(nsr, &ar->ar_root);
	at = entry;

	if (at == NULL)
		goto done;

	/*
	 * Remember the default route of each table we visit in case
	 * we do not find a better matching route.
	 */
	dflt = srp_enter(&dsr, &at->at_default);

	/*
	 * Iterate until we find a leaf.
	 */
	while (1) {
		/* Do a single level route lookup. */
		j = art_findex(at, addr);
		entry = srp_follow(nsr, &at->at_heap[j].node);

		/* If this is a leaf (NULL is a leaf) we're done. */
		if (ISLEAF(entry))
			break;

		at = SUBTABLE(entry);

		ndflt = srp_enter(&ndsr, &at->at_default);
		if (ndflt != NULL) {
			srp_leave(&dsr);
			dsr = ndsr;
			dflt = ndflt;
		} else
			srp_leave(&ndsr);
	}

	if (entry == NULL) {
		srp_leave(nsr);
		*nsr = dsr;
		KASSERT(ISLEAF(dflt));
		return (dflt);
	}

	srp_leave(&dsr);
done:
	KASSERT(ISLEAF(entry));
	return (entry);
}

/*
 * Perfect lookup API function.
 *
 * Return a perfect match for a destination/prefix-length pair or NULL if
 * it does not exist.
 */
struct art_node *
art_lookup(struct art_root *ar, void *addr, int plen, struct srp_ref *nsr)
{
	void			*entry;
	struct art_table	*at;
	int			 i, j;

	KASSERT(plen >= 0 && plen <= ar->ar_alen);

	entry = srp_enter(nsr, &ar->ar_root);
	at = entry;

	if (at == NULL)
		goto done;

	/* Default route */
	if (plen == 0) {
		entry = srp_follow(nsr, &at->at_default);
		goto done;
	}

	/*
	 * If the prefix length is smaller than the sum of
	 * the stride length at this level the entry must
	 * be in the current table.
	 */
	while (plen > (at->at_offset + at->at_bits)) {
		/* Do a single level route lookup. */
		j = art_findex(at, addr);
		entry = srp_follow(nsr, &at->at_heap[j].node);

		/* A leaf is a match, but not a perfect one, or NULL */
		if (ISLEAF(entry))
			return (NULL);

		at = SUBTABLE(entry);
	}

	i = art_bindex(at, addr, plen);
	if (i == -1)
		return (NULL);

	entry = srp_follow(nsr, &at->at_heap[i].node);
	if (!ISLEAF(entry))
		entry = srp_follow(nsr, &SUBTABLE(entry)->at_default);

done:
	KASSERT(ISLEAF(entry));
	return (entry);
}


/*
 * Insertion API function.
 *
 * Insert the given node or return an existing one if a node with the
 * same destination/mask pair is already present.
 */
struct art_node *
art_insert(struct art_root *ar, struct art_node *an, void *addr, int plen)
{
	struct art_table	*at, *child;
	struct art_node		*node;
	int			 i, j;

	rw_assert_wrlock(&ar->ar_lock);
	KASSERT(plen >= 0 && plen <= ar->ar_alen);

	at = srp_get_locked(&ar->ar_root);
	if (at == NULL) {
		at = art_table_get(ar, NULL, -1);
		if (at == NULL)
			return (NULL);

		srp_swap_locked(&ar->ar_root, at);
	}

	/* Default route */
	if (plen == 0) {
		node = srp_get_locked(&at->at_default);
		if (node != NULL)
			return (node);

		art_table_ref(ar, at);
		srp_swap_locked(&at->at_default, an);
		return (an);
	}

	/*
	 * If the prefix length is smaller than the sum of
	 * the stride length at this level the entry must
	 * be in the current table.
	 */
	while (plen > (at->at_offset + at->at_bits)) {
		/* Do a single level route lookup. */
		j = art_findex(at, addr);
		node = srp_get_locked(&at->at_heap[j].node);

		/*
		 * If the node corresponding to the fringe index is
		 * a leaf we need to allocate a subtable.  The route
		 * entry of this node will then become the default
		 * route of the subtable.
		 */
		if (ISLEAF(node)) {
			child = art_table_get(ar, at, j);
			if (child == NULL)
				return (NULL);

			art_table_ref(ar, at);
			srp_swap_locked(&at->at_heap[j].node, ASNODE(child));
			at = child;
		} else
			at = SUBTABLE(node);
	}

	i = art_bindex(at, addr, plen);
	if (i == -1)
		return (NULL);

	return (art_table_insert(ar, at, i, an));
}

/*
 * Single level insertion.
 */
struct art_node *
art_table_insert(struct art_root *ar, struct art_table *at, int i,
    struct art_node *an)
{
	struct art_node	*prev, *node;

	node = srp_get_locked(&at->at_heap[i].node);
	if (!ISLEAF(node))
		prev = srp_get_locked(&SUBTABLE(node)->at_default);
	else
		prev = node;

	if (art_check_duplicate(ar, prev, an))
		return (prev);

	art_table_ref(ar, at);

	/*
	 * If the index `i' of the route that we are inserting is not
	 * a fringe index, we need to allot this new route pointer to
	 * all the corresponding fringe indices.
	 */
	if (i < at->at_minfringe)
		art_allot(at, i, prev, an);
	else if (!ISLEAF(node))
		srp_swap_locked(&SUBTABLE(node)->at_default, an);
	else
		srp_swap_locked(&at->at_heap[i].node, an);

	return (an);
}


/*
 * Deletion API function.
 */
struct art_node *
art_delete(struct art_root *ar, struct art_node *an, void *addr, int plen)
{
	struct art_table	*at;
	struct art_node		*node;
	int			 i, j;

	rw_assert_wrlock(&ar->ar_lock);
	KASSERT(plen >= 0 && plen <= ar->ar_alen);

	at = srp_get_locked(&ar->ar_root);
	if (at == NULL)
		return (NULL);

	/* Default route */
	if (plen == 0) {
		node = srp_get_locked(&at->at_default);
		srp_swap_locked(&at->at_default, NULL);
		art_table_free(ar, at);
		return (node);
	}

	/*
	 * If the prefix length is smaller than the sum of
	 * the stride length at this level the entry must
	 * be in the current table.
	 */
	while (plen > (at->at_offset + at->at_bits)) {
		/* Do a single level route lookup. */
		j = art_findex(at, addr);
		node = srp_get_locked(&at->at_heap[j].node);

		/* If this is a leaf, there is no route to delete. */
		if (ISLEAF(node))
			return (NULL);

		at = SUBTABLE(node);
	}

	i = art_bindex(at, addr, plen);
	if (i == -1)
		return (NULL);

	return (art_table_delete(ar, at, i, an));
}

/*
 * Single level deletion.
 */
struct art_node *
art_table_delete(struct art_root *ar, struct art_table *at, int i,
    struct art_node *an)
{
	struct art_node		*next, *node;

#ifdef DIAGNOSTIC
	struct art_node		*prev;
#endif

	node = srp_get_locked(&at->at_heap[i].node);
#ifdef DIAGNOSTIC
	if (!ISLEAF(node))
		prev = srp_get_locked(&SUBTABLE(node)->at_default);
	else
		prev = node;

	KASSERT(prev == an);
#endif

	/* Get the next most specific route for the index `i'. */
	if ((i >> 1) > 1)
		next = srp_get_locked(&at->at_heap[i >> 1].node);
	else
		next = NULL;

	/*
	 * If the index `i' of the route that we are removing is not
	 * a fringe index, we need to allot the next most specific
	 * route pointer to all the corresponding fringe indices.
	 */
	if (i < at->at_minfringe)
		art_allot(at, i, an, next);
	else if (!ISLEAF(node))
		srp_swap_locked(&SUBTABLE(node)->at_default, next);
	else
		srp_swap_locked(&at->at_heap[i].node, next);

	/* We have removed an entry from this table. */
	art_table_free(ar, at);

	return (an);
}

struct art_table *
art_table_ref(struct art_root *ar, struct art_table *at)
{
	at->at_refcnt++;
	return (at);
}

static inline int
art_table_rele(struct art_table *at)
{
	if (at == NULL)
		return (0);

	return (--at->at_refcnt == 0);
}

int
art_table_free(struct art_root *ar, struct art_table *at)
{
	if (art_table_rele(at)) {
		/*
		 * Garbage collect this table and all its parents
		 * that are empty.
		 */
		do {
			at = art_table_put(ar, at);
		} while (art_table_rele(at));

		return (1);
	}

	return (0);
}

/*
 * Iteration API function.
 */
int
art_walk(struct art_root *ar, int (*f)(struct art_node *, void *), void *arg)
{
	struct srp_ref		 sr;
	struct art_table	*at;
	struct art_node		*node;
	int			 error = 0;

	rw_enter_write(&ar->ar_lock);
	at = srp_get_locked(&ar->ar_root);
	if (at != NULL) {
		art_table_ref(ar, at);

		/*
		 * The default route should be processed here because the root
		 * table does not have a parent.
		 */
		node = srp_enter(&sr, &at->at_default);
		error = art_walk_apply(ar, node, NULL, f, arg);
		srp_leave(&sr);

		if (error == 0)
			error = art_table_walk(ar, at, f, arg);

		art_table_free(ar, at);
	}
	rw_exit_write(&ar->ar_lock);

	return (error);
}

int
art_table_walk(struct art_root *ar, struct art_table *at,
    int (*f)(struct art_node *, void *), void *arg)
{
	struct srp_ref		 sr;
	struct art_node		*node, *next;
	struct art_table	*nat;
	int			 i, j, error = 0;
	uint32_t		 maxfringe = (at->at_minfringe << 1);

	/*
	 * Iterate non-fringe nodes in ``natural'' order.
	 */
	for (j = 1; j < at->at_minfringe; j += 2) {
		/*
		 * The default route (index 1) is processed by the
		 * parent table (where it belongs) otherwise it could
		 * be processed more than once.
		 */
		for (i = max(j, 2); i < at->at_minfringe; i <<= 1) {
			next = srp_get_locked(&at->at_heap[i >> 1].node);

			node = srp_enter(&sr, &at->at_heap[i].node);
			error = art_walk_apply(ar, node, next, f, arg);
			srp_leave(&sr);

			if (error != 0)
				return (error);
		}
	}

	/*
	 * Iterate fringe nodes.
	 */
	for (i = at->at_minfringe; i < maxfringe; i++) {
		next = srp_get_locked(&at->at_heap[i >> 1].node);

		node = srp_enter(&sr, &at->at_heap[i].node);
		if (!ISLEAF(node)) {
			nat = art_table_ref(ar, SUBTABLE(node));
			node = srp_follow(&sr, &nat->at_default);
		} else
			nat = NULL;

		error = art_walk_apply(ar, node, next, f, arg);
		srp_leave(&sr);

		if (error != 0) {
			art_table_free(ar, nat);
			return (error);
		}

		if (nat != NULL) {
			error = art_table_walk(ar, nat, f, arg);
			art_table_free(ar, nat);
			if (error != 0)
				return (error);
		}
	}

	return (0);
}

int
art_walk_apply(struct art_root *ar,
    struct art_node *an, struct art_node *next,
    int (*f)(struct art_node *, void *), void *arg)
{
	int error = 0;

	if ((an != NULL) && (an != next)) {
		rw_exit_write(&ar->ar_lock);
		error = (*f)(an, arg);
		rw_enter_write(&ar->ar_lock);
	}

	return (error);
}


/*
 * Create a table and use the given index to set its default route.
 *
 * Note:  This function does not modify the root or the parent.
 */
struct art_table *
art_table_get(struct art_root *ar, struct art_table *parent, int j)
{
	struct art_table	*at;
	struct art_node		*node;
	void			*at_heap;
	uint32_t		 lvl;

	KASSERT(j != 0 && j != 1);
	KASSERT(parent != NULL || j == -1);

	if (parent != NULL)
		lvl = parent->at_level + 1;
	else
		lvl = 0;

	KASSERT(lvl < ar->ar_nlvl);

	at = pool_get(&at_pool, PR_NOWAIT|PR_ZERO);
	if (at == NULL)
		return (NULL);

	switch (AT_HEAPSIZE(ar->ar_bits[lvl])) {
	case AT_HEAPSIZE(4):
		at_heap = pool_get(&at_heap_4_pool, PR_NOWAIT|PR_ZERO);
		break;
	case AT_HEAPSIZE(8):
		at_heap = pool_get(&at_heap_8_pool, PR_NOWAIT|PR_ZERO);
		break;
	default:
		panic("incorrect stride length %u", ar->ar_bits[lvl]);
	}

	if (at_heap == NULL) {
		pool_put(&at_pool, at);
		return (NULL);
	}

	at->at_parent = parent;
	at->at_index = j;
	at->at_minfringe = (1 << ar->ar_bits[lvl]);
	at->at_level = lvl;
	at->at_bits = ar->ar_bits[lvl];
	at->at_heap = at_heap;
	at->at_refcnt = 0;

	if (parent != NULL) {
		node = srp_get_locked(&parent->at_heap[j].node);
		/* node isn't being deleted, no srp_finalize needed */
		srp_swap_locked(&at->at_default, node);
		at->at_offset = (parent->at_offset + parent->at_bits);
	}

	return (at);
}


/*
 * Delete a table and use its index to restore its parent's default route.
 *
 * Note:  Modify its parent to unlink the table from it.
 */
struct art_table *
art_table_put(struct art_root *ar, struct art_table *at)
{
	struct art_table	*parent = at->at_parent;
	struct art_node		*node;
	uint32_t		 j = at->at_index;

	KASSERT(at->at_refcnt == 0);
	KASSERT(j != 0 && j != 1);

	if (parent != NULL) {
		KASSERT(j != -1);
		KASSERT(at->at_level == parent->at_level + 1);
		KASSERT(parent->at_refcnt >= 1);

		/* Give the route back to its parent. */
		node = srp_get_locked(&at->at_default);
		srp_swap_locked(&parent->at_heap[j].node, node);
	} else {
		KASSERT(j == -1);
		KASSERT(at->at_level == 0);
		srp_swap_locked(&ar->ar_root, NULL);
	}

	mtx_enter(&art_table_gc_mtx);
	at->at_parent = art_table_gc_list;
	art_table_gc_list = at;
	mtx_leave(&art_table_gc_mtx);

	task_add(systqmp, &art_table_gc_task);

	return (parent);
}

void
art_table_gc(void *null)
{
	struct art_table *at, *next;

	mtx_enter(&art_table_gc_mtx);
	at = art_table_gc_list;
	art_table_gc_list = NULL;
	mtx_leave(&art_table_gc_mtx);

	while (at != NULL) {
		next = at->at_parent;

		if (at->at_level == 0)
			srp_finalize(at, "arttfini");
		else
			srp_finalize(ASNODE(at), "arttfini");

		switch (AT_HEAPSIZE(at->at_bits)) {
		case AT_HEAPSIZE(4):
			pool_put(&at_heap_4_pool, at->at_heap);
			break;
		case AT_HEAPSIZE(8):
			pool_put(&at_heap_8_pool, at->at_heap);
			break;
		default:
			panic("incorrect stride length %u", at->at_bits);
		}

		pool_put(&at_pool, at);

		at = next;
	}
}

/*
 * Substitute a node by another in the subtree whose root index is given.
 *
 * This function iterates on the table ``at'' at index ``i'' until no
 * more ``old'' node can be replaced by ``new''.
 *
 * This function was originally written by Don Knuth in CWEB. The
 * complicated ``goto''s are the result of expansion of the two
 * following recursions:
 *
 *	art_allot(at, i, old, new)
 *	{
 *		int k = i;
 *		if (at->at_heap[k] == old)
 *			at->at_heap[k] = new;
 *		if (k >= at->at_minfringe)
 *			 return;
 *		k <<= 1;
 *		art_allot(at, k, old, new);
 *		k++;
 *		art_allot(at, k, old, new);
 *	}
 */
void
art_allot(struct art_table *at, int i, struct art_node *old,
    struct art_node *new)
{
	struct art_node		*node, *dflt;
	int			 k = i;

	KASSERT(i < at->at_minfringe);

again:
	k <<= 1;
	if (k < at->at_minfringe)
		goto nonfringe;

	/* Change fringe nodes. */
	while (1) {
		node = srp_get_locked(&at->at_heap[k].node);
		if (!ISLEAF(node)) {
			dflt = srp_get_locked(&SUBTABLE(node)->at_default);
			if (dflt == old) {
				srp_swap_locked(&SUBTABLE(node)->at_default,
				    new);
			}
		} else if (node == old) {
			srp_swap_locked(&at->at_heap[k].node, new);
		}
		if (k % 2)
			goto moveup;
		k++;
	}

nonfringe:
	node = srp_get_locked(&at->at_heap[k].node);
	if (node == old)
		goto again;
moveon:
	if (k % 2)
		goto moveup;
	k++;
	goto nonfringe;
moveup:
	k >>= 1;
	srp_swap_locked(&at->at_heap[k].node, new);

	/* Change non-fringe node. */
	if (k != i)
		goto moveon;
}

struct art_node *
art_get(void *dst, uint8_t plen)
{
	struct art_node		*an;

	an = pool_get(&an_pool, PR_NOWAIT | PR_ZERO);
	if (an == NULL)
		return (NULL);

	an->an_plen = plen;
	SRPL_INIT(&an->an_rtlist);

	return (an);
}

void
art_put(struct art_node *an)
{
	KASSERT(SRPL_EMPTY_LOCKED(&an->an_rtlist));

	mtx_enter(&art_node_gc_mtx);
	an->an_gc = art_node_gc_list;
	art_node_gc_list = an;
	mtx_leave(&art_node_gc_mtx);

	task_add(systqmp, &art_node_gc_task);
}

void
art_gc(void *null)
{
	struct art_node		*an, *next;

	mtx_enter(&art_node_gc_mtx);
	an = art_node_gc_list;
	art_node_gc_list = NULL;
	mtx_leave(&art_node_gc_mtx);

	while (an != NULL) {
		next = an->an_gc;

		srp_finalize(an, "artnfini");

		pool_put(&an_pool, an);

		an = next;
	}
}
@


1.26
log
@A space here, a space there. Soon we're talking real whitespace
rectification.
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.25 2017/01/23 01:02:11 claudio Exp $ */
a705 1
		/* this assumes an->an_dst is not used by f */
a931 1
	an->an_dst = dst;
@


1.25
log
@Make the art interface a bit more generic by not depending on sockaddr
in the functions. This way it can be used for other trees as well.
OK mpi@@ phessler@@
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.24 2016/09/15 02:00:18 dlg Exp $ */
d187 1
a187 1
	uint32_t 		k;
@


1.24
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.23 2016/08/30 07:42:57 jmatthew Exp $ */
d250 1
a250 1
art_match(struct art_root *ar, uint8_t *addr, struct srp_ref *nsr)
d313 1
a313 1
art_lookup(struct art_root *ar, uint8_t *addr, int plen, struct srp_ref *nsr)
d371 1
a371 1
art_insert(struct art_root *ar, struct art_node *an, uint8_t *addr, int plen)
d475 1
a475 1
art_delete(struct art_root *ar, struct art_node *an, uint8_t *addr, int plen)
d925 1
a925 1
art_get(struct sockaddr *dst, uint8_t plen)
@


1.23
log
@use a per-table rwlock to serialize ART updates and walks, rather than
taking the kernel lock.

ok mpi@@ dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.22 2016/07/19 10:51:44 mpi Exp $ */
d105 8
a112 13
	pool_init(&an_pool, sizeof(struct art_node), 0, 0, 0, "art_node", NULL);
	pool_setipl(&an_pool, IPL_SOFTNET);

	pool_init(&at_pool, sizeof(struct art_table), 0, 0, 0, "art_table",
	    NULL);
	pool_setipl(&at_pool, IPL_SOFTNET);

	pool_init(&at_heap_4_pool, AT_HEAPSIZE(4), 0, 0, 0, "art_heap4", NULL);
	pool_setipl(&at_heap_4_pool, IPL_SOFTNET);

	pool_init(&at_heap_8_pool, AT_HEAPSIZE(8), 0, 0, 0, "art_heap8",
	    &pool_allocator_single);
	pool_setipl(&at_heap_8_pool, IPL_SOFTNET);
@


1.22
log
@Revert use of the _SAFE version of SRPL_FOREACH() now that the offending
function has been fixed.

Functions passed to rtable_walk() must return EAGAIN if they delete an
entry from the tree, no matter if it is a leaf or not.
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.21 2016/07/04 08:11:48 mpi Exp $ */
d155 1
d382 1
a382 1
	KERNEL_ASSERT_LOCKED();
d486 1
a486 1
	KERNEL_ASSERT_LOCKED();
d617 1
a617 1
	KERNEL_LOCK();
d635 1
a635 1
	KERNEL_UNLOCK();
d712 1
a712 1
		KERNEL_UNLOCK();
d714 1
a714 1
		KERNEL_LOCK();
@


1.21
log
@Use the _SAFE_ version of SRPL_FOREACH() in rtable_walk_helper() to
prevent an off-by-one when removing entries from the mpath list.

Fix a regression introduced by the refactoring needed to serialize
rtable_walk() with create/delete.

ok jca@@
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.20 2016/06/22 06:32:32 dlg Exp $ */
a708 2
	KERNEL_ASSERT_LOCKED();

d711 1
d713 1
@


1.20
log
@rework art_walk so it will behave in an mpsafe world.

art_walk now explicitly takes the same lock used to serialise change
made via rtable_insert and _delete, so it can safely adjust the
refcnts on tables while it recurses into them. they need to still
exist when returning out of the recursion.

it uses srps to access nodes and drops the lock before calling the
callback function. this is because some callbacks sleep (eg, copyout
in the sysctl code that dumps an rtable to userland), which you
shouldnt hold a lock accross. other callbacks attempt to modify
the rtable (eg, marking routes as down when then interface theyre
on goes down), which tries to take the lock again, which probably
wont work in the future.

ok jmatthew@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.19 2016/06/14 04:42:02 jmatthew Exp $ */
d709 2
a712 1
		KERNEL_UNLOCK();
a713 1
		KERNEL_LOCK();
@


1.19
log
@Convert the links between art data structures used during lookups into srps.
art_lookup and art_match now return an active srp_ref, which the caller must
leave when it's done with the returned route (if any).  This allows lookups
to be done without holding any locks.

The art_table and art_node garbage collectors are still responsible for
freeing items removed from the routing table, so they now use srp_finalize
to wait out any active references, and updates are done using srp_swap
operations.

ok dlg@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.18 2016/06/03 03:59:43 dlg Exp $ */
d81 1
a81 1
void			 art_table_ref(struct art_root *, struct art_table *);
d85 3
d571 1
a571 1
void
d575 1
d611 1
d614 6
a619 1
	int			 error;
d621 7
a627 1
	KERNEL_ASSERT_LOCKED();
d629 2
a630 3
	at = srp_get_locked(&ar->ar_root);
	if (at == NULL)
		return (0);
d632 1
a632 9
	/*
	 * The default route should be processed here because the root
	 * table does not have a parent.
	 */
	node = srp_get_locked(&at->at_default);
	if (node != NULL) {
		error = (*f)(node, arg);
		if (error)
			return (error);
d634 1
d636 1
a636 1
	return (art_table_walk(ar, at, f, arg));
d643 3
a645 2
	struct art_node		*next, *an = NULL;
	struct art_node		*node;
a648 3
	/* Prevent this table to be freed while we're manipulating it. */
	art_table_ref(ar, at);

d660 7
a666 6
			an = srp_get_locked(&at->at_heap[i].node);
			if ((an != NULL) && (an != next)) {
				error = (*f)(an, arg);
				if (error)
					goto out;
			}
d675 21
a695 10
		node = srp_get_locked(&at->at_heap[i].node);
		if (!ISLEAF(node))
			an = srp_get_locked(&SUBTABLE(node)->at_default);
		else
			an = node;

		if ((an != NULL) && (an != next)) {
			error = (*f)(an, arg);
			if (error)
				goto out;
d697 1
d699 9
a707 2
		if (ISLEAF(node))
			continue;
d709 5
a713 3
		error = art_table_walk(ar, SUBTABLE(node), f, arg);
		if (error)
			break;
a715 2
out:
	art_table_free(ar, at);
@


1.18
log
@defer the freeing of art tables and nodes to a task.

this will allow us to sleep in srp_finalize before freeing the
memory.

the defer is done by putting the tables and nodes on a list which
is serviced by a task. the task removes all the entries from the
list and pool_puts them.

the art_tables gc code uses at_parent as its list entry, and the
art_node gc code uses a union with the an_dst pointer. both at_parent
and an_dst are only used when theyre active as part of an art data
structure, and are not used in lookups. once the art is done with
them we can reuse these pointers safely.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.17 2016/06/02 00:39:22 dlg Exp $ */
d40 2
a41 2
#define ISLEAF(e)	(((unsigned long)(e).node & 1) == 0)
#define SUBTABLE(e)	(((struct art_table *)((unsigned long)(e).child & ~1)))
d61 1
a61 2
		struct art_node		*node;
		struct art_table	*child;
d251 1
a251 1
art_match(struct art_root *ar, uint8_t *addr)
d253 2
d256 5
a260 2
	struct art_node		*dflt = NULL;
	int			 j;
a261 1
	at = ar->ar_root;
d263 7
a269 1
		return (NULL);
a274 7
		/*
		 * Rember the default route of this table in case
		 * we do not find a better matching route.
		 */
		if (at->at_default != NULL)
			dflt = at->at_default;

d277 1
d279 2
a280 2
		/* If this is a leaf we're done. */
		if (ISLEAF(at->at_heap[j]))
d283 9
a291 1
		at = SUBTABLE(at->at_heap[j]);
d294 6
a299 2
	if (at->at_heap[j].node != NULL)
		return (at->at_heap[j].node);
d301 4
a304 1
	return (dflt);
d314 1
a314 1
art_lookup(struct art_root *ar, uint8_t *addr, int plen)
d316 1
a317 1
	struct art_node		*an;
d322 3
a324 1
	at = ar->ar_root;
d326 1
a326 1
		return (NULL);
d329 4
a332 2
	if (plen == 0)
		return (at->at_default);
d342 1
d344 2
a345 2
		/* A leaf is a match, but not a perfect one. */
		if (ISLEAF(at->at_heap[j]))
d348 1
a348 1
		at = SUBTABLE(at->at_heap[j]);
d355 7
a361 6
	if (!ISLEAF(at->at_heap[i]))
		an = SUBTABLE(at->at_heap[i])->at_default;
	else
		an = at->at_heap[i].node;

	return (an);
d374 2
a375 1
	struct art_table	*at;
d378 1
d381 1
a381 1
	at = ar->ar_root;
d387 1
a387 1
		ar->ar_root = at;
d392 3
a394 2
		if (at->at_default != NULL)
			return (at->at_default);
d397 1
a397 1
		at->at_default = an;
d409 1
d417 1
a417 3
		if (ISLEAF(at->at_heap[j])) {
			struct art_table  *child;

d423 4
a426 4
			at->at_heap[j].node = ASNODE(child);
		}

		at = SUBTABLE(at->at_heap[j]);
d443 1
a443 1
	struct art_node	*prev;
d445 3
a447 2
	if (!ISLEAF(at->at_heap[i]))
		prev = SUBTABLE(at->at_heap[i])->at_default;
d449 1
a449 1
		prev = at->at_heap[i].node;
d463 2
a464 2
	else if (!ISLEAF(at->at_heap[i]))
		SUBTABLE(at->at_heap[i])->at_default = an;
d466 1
a466 1
		at->at_heap[i].node = an;
d479 1
a479 1
	struct art_node		*dflt;
d482 1
d485 1
a485 1
	at = ar->ar_root;
d491 2
a492 2
		dflt = at->at_default;
		at->at_default = NULL;
d494 1
a494 1
		return (dflt);
d505 1
d508 1
a508 1
		if (ISLEAF(at->at_heap[j]))
d511 1
a511 1
		at = SUBTABLE(at->at_heap[j]);
d526 1
a526 1
    struct art_node *node)
d528 1
a528 1
	struct art_node		*next;
d532 1
d534 4
a537 2
	if (!ISLEAF(at->at_heap[i]))
		prev = SUBTABLE(at->at_heap[i])->at_default;
d539 1
a539 1
		prev = at->at_heap[i].node;
d541 1
a541 1
	KASSERT(prev == node);
d546 1
a546 1
		next = at->at_heap[i >> 1].node;
d556 3
a558 3
		art_allot(at, i, node, next);
	else if (!ISLEAF(at->at_heap[i]))
		SUBTABLE(at->at_heap[i])->at_default = next;
d560 1
a560 1
		at->at_heap[i].node = next;
d565 1
a565 1
	return (node);
d574 9
d586 1
a586 1
	if (--at->at_refcnt == 0) {
d593 1
a593 1
		} while (at != NULL && --at->at_refcnt == 0);
d608 1
d611 3
a613 1
	at = ar->ar_root;
d621 3
a623 2
	if (at->at_default != NULL) {
		error = (*f)(at->at_default, arg);
d636 1
d653 2
a654 2
			next = at->at_heap[i >> 1].node;
			an = at->at_heap[i].node;
d667 4
a670 3
		next = at->at_heap[i >> 1].node;
		if (!ISLEAF(at->at_heap[i]))
			an = SUBTABLE(at->at_heap[i])->at_default;
d672 2
a673 1
			an = at->at_heap[i].node;
d680 1
a680 1
		if (ISLEAF(at->at_heap[i]))
d683 1
a683 1
		error = art_table_walk(ar, SUBTABLE(at->at_heap[i]), f, arg);
d703 1
d746 3
a748 1
		at->at_default = parent->at_heap[j].node;
d765 1
a765 1
	uint32_t		 lvl = at->at_level;
d768 1
a769 1
	KASSERT(parent != NULL || j == -1);
d772 2
a773 1
		KASSERT(lvl == parent->at_level + 1);
d777 2
a778 1
		parent->at_heap[j].node = at->at_default;
d780 3
a782 1
		ar->ar_root = NULL;
d808 5
d857 2
a858 1
	int			k = i;
d869 6
a874 3
		if (!ISLEAF(at->at_heap[k])) {
			if (SUBTABLE(at->at_heap[k])->at_default == old) {
				SUBTABLE(at->at_heap[k])->at_default = new;
d876 2
a877 2
		} else if (at->at_heap[k].node == old) {
			at->at_heap[k].node = new;
d885 2
a886 1
	if (at->at_heap[k].node == old)
d895 1
a895 1
	at->at_heap[k].node = new;
d943 2
@


1.17
log
@pool_setipl at IPL_SOFTNET for all the art structures.
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.16 2016/06/02 00:34:13 dlg Exp $ */
d34 1
d86 2
d91 9
d730 4
a733 10
	switch (AT_HEAPSIZE(at->at_bits)) {
	case AT_HEAPSIZE(4):
		pool_put(&at_heap_4_pool, at->at_heap);
		break;
	case AT_HEAPSIZE(8):
		pool_put(&at_heap_8_pool, at->at_heap);
		break;
	default:
		panic("incorrect stride length %u", at->at_bits);
	}
d735 1
a735 1
	pool_put(&at_pool, at);
d740 30
d856 27
a882 1
	pool_put(&an_pool, an);
@


1.16
log
@always clean up the heap in art_table_delete, even for the last at_refcnt

in the future a table may also be referenced by a cpu reading it
with srp as well as the art rtable, so try and make sure it is
always usable.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.15 2016/06/01 06:19:06 dlg Exp $ */
d92 2
d96 2
d99 2
d103 1
@


1.15
log
@move all the art_node initialisation to art_get in art.c

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.14 2016/04/13 08:04:14 mpi Exp $ */
a492 4
	/* We are removing an entry from this table. */
	if (art_table_free(ar, at))
		return (node);

d510 3
@


1.14
log
@Keep all pools in the same place.

ok jmatthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.13 2016/04/12 06:40:44 mpi Exp $ */
d806 1
@


1.13
log
@Remove unneeded art_free().

Reported by and ok jmatthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.12 2016/01/18 18:27:11 mpi Exp $ */
d86 1
a86 1
struct pool		at_pool, at_heap_4_pool, at_heap_8_pool;
d91 1
d793 21
@


1.12
log
@Pass the address length to art_alloc() and remove the hack abusing the
offset of the address in the sockaddr to initialize the stride lengths.
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.11 2015/12/04 14:15:27 mpi Exp $ */
d127 1
a127 1
		art_free(ar);
a134 7
}

void
art_free(struct art_root *ar)
{
	KASSERT(ar->ar_root == NULL);
	free(ar, M_RTABLE, sizeof(*ar));
@


1.11
log
@Reduce the stride length of the tables by two and use a single page
allocator for the 4K heap.

In this configuration a fullfeed BGP server for v4 and v6 consumes
10M more than with the radix tree.

This double the depth of the tree and makes the lookup slower.  But
the ratio speed/memory can be adjusted in the future, for now we are
interested in a lock-free route lookup.

Tested by and ok benno@@
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.10 2015/11/24 12:48:20 dlg Exp $ */
d102 1
a102 1
art_alloc(unsigned int rtableid, int off)
d111 2
a112 3
	/* XXX using the offset is a hack. */
	switch (off) {
	case 4: /* AF_INET && AF_MPLS */
d119 1
a119 2
#ifdef INET6
	case 8: /* AF_INET6 */
a124 1
#endif /* INET6 */
d126 1
a126 1
		printf("%s: unknown offset %d\n", __func__, off);
@


1.10
log
@in art_insert, if at_default on the first table is set then return the
existing route rather than overwrite it.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.9 2015/11/24 12:06:30 mpi Exp $ */
d86 1
a86 1
struct pool		at_pool, at_heap_8_pool, at_heap_16_pool;
d93 3
a95 3
	pool_init(&at_heap_8_pool, AT_HEAPSIZE(8), 0, 0, 0, "art_heap8", NULL);
	pool_init(&at_heap_16_pool, AT_HEAPSIZE(16), 0, 0, 0, "art_heap16",
	    NULL);
d115 4
a118 4
		ar->ar_nlvl = 3;
		ar->ar_bits[0] = 16;
		ar->ar_bits[1] = 8;
		ar->ar_bits[2] = 8;
d123 1
a123 1
		ar->ar_nlvl = 16;
d125 1
a125 1
			ar->ar_bits[i] = 8;
d664 3
a669 3
	case AT_HEAPSIZE(16):
		at_heap = pool_get(&at_heap_16_pool, PR_NOWAIT|PR_ZERO);
		break;
d721 4
a724 1
	switch (AT_HEAPSIZE(ar->ar_bits[lvl])) {
a727 3
	case AT_HEAPSIZE(16):
		pool_put(&at_heap_16_pool, at->at_heap);
		break;
d729 1
a729 1
		panic("incorrect stride length %u", ar->ar_bits[lvl]);
@


1.9
log
@Provide art_free(), a method to release unused routing table heads.

While here initialize pools in art_init().
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.8 2015/11/12 14:29:04 mpi Exp $ */
d356 3
@


1.8
log
@Allocate root tables on demand an free them like any other table.

With this change we no longer waste some precious Kb for unused
routing tables like the AF_MPLS one or those with rtableid != 0.

This will also simplify the SRP dance during lookups.
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.7 2015/11/10 10:23:27 mpi Exp $ */
d86 11
a96 4
int			at_init = 0;
struct pool		at_pool;
struct pool		at_heap_8_pool;
struct pool		at_heap_16_pool;
a106 10
	if (!at_init) {
		at_init = 1;
		pool_init(&at_pool, sizeof(struct art_table), 0, 0, 0,
		    "art_table", NULL);
		pool_init(&at_heap_8_pool, AT_HEAPSIZE(8), 0, 0, 0,
		    "art_heap8", NULL);
		pool_init(&at_heap_16_pool, AT_HEAPSIZE(16), 0, 0, 0,
		    "art_heap16", NULL);
	}

d130 1
a130 1
		free(ar, M_RTABLE, sizeof(*ar));
d138 7
@


1.7
log
@Allocate ART table's heap independently from the structure and use
pool(9) to not waste most of the memory allocated.

This reduces the memory overhead of our ART routing table from 80M
to 70M compared to the existing radix-tree when loading ~550K IPv4
routes.

ART can now be used for huge tables without exhausting malloc(9)'s
limit.

claudio@@ agrees with the direction, inputs from and ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.6 2015/11/04 09:50:21 mpi Exp $ */
d83 1
a83 1
int			 art_table_walk(struct art_table *,
a137 6

	ar->ar_root = art_table_get(ar, NULL, -1);
	if (ar->ar_root == NULL) {
		free(ar, M_RTABLE, sizeof(*ar));
		return (NULL);
	}
d240 1
a240 1
	struct art_table	*at = ar->ar_root;
d244 4
d284 1
a284 1
	struct art_table	*at = ar->ar_root;
d290 4
d336 1
a336 1
	struct art_table	*at = ar->ar_root;
d341 9
d352 1
a352 3
		if (art_check_duplicate(ar, at->at_default, an))
			return (at->at_default);

d379 1
d434 1
a434 1
	struct art_table	*at = ar->ar_root;
d440 4
a446 2
		if (!art_check_duplicate(ar, dflt, an))
			return (NULL);
d448 1
d529 1
a529 3
	at->at_refcnt--;

	if (at->at_refcnt == 0) {
d536 1
a536 1
		} while (at->at_refcnt == 0);
d550 1
a550 1
	struct art_table	*at = ar->ar_root;
d553 4
d567 1
a567 1
	return (art_table_walk(at, f, arg));
d571 1
a571 1
art_table_walk(struct art_table *at,
d578 3
d582 1
a582 1
	 * Iterate non-fringe nodes in the ``natural'' order.
d596 1
a596 1
					return (error);
d613 1
a613 1
				return (error);
d619 1
a619 1
		error = art_table_walk(SUBTABLE(at->at_heap[i]), f, arg);
d624 2
d678 1
a678 2

	art_table_ref(ar, at);
d702 1
a702 2
	KASSERT(parent != NULL);
	KASSERT(lvl == parent->at_level + 1);
d704 9
a712 3
	/* Give the route back to its parent. */
	parent->at_heap[j].node = at->at_default;
	art_table_free(ar, parent);
@


1.6
log
@Some tweaks to build the rtable API and backends in userland.

Needed by the regression tests.
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.5 2015/10/14 10:09:30 mpi Exp $ */
d33 1
d68 3
d86 5
d100 10
d619 1
a619 1
	size_t			 size;
d632 1
a632 2
	size = sizeof(*at) + (1 << (ar->ar_bits[lvl] + 1)) * sizeof(void *);
	at = malloc(size, M_RTABLE, M_NOWAIT|M_ZERO);
d636 16
d657 1
a657 1
	at->at_heap = (void *)(at + 1);
a678 1
	size_t			 size;
d690 12
a701 2
	size = sizeof(*at) + (1 << (ar->ar_bits[lvl] + 1)) * sizeof(void *);
	free(at, M_RTABLE, size);
@


1.5
log
@Rewrite the logic around the dymanic array of routing tables to help
turning rtable_get(9) MP-safe.

Use only one per-AF array, as suggested by claudio@@, pointing to an
array of pointers to the routing table heads.

Routing tables are now allocated/initialized per-AF.  This will let
us allocate routing table on-demand instead of always having an
AF_INET, AF_MPLS and AF_INET table as soon as a new rtableID is used.

This also get rid of the "void ***" madness.

ok dlg@@, jmatthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.4 2015/10/07 10:50:35 mpi Exp $ */
d27 3
d34 1
@


1.4
log
@Initialize the routing table before domains.

The routing table is not an optional component of the network stack
and initializing it inside the "routing domain" requires some ugly
introspection in the domain interface.

This put the rtable* layer at the same level of the if* level.  These
two subsystem are organized around the two global data structure used
in the network stack:

- the global &ifnet list, to be used in process context only, and
- the routing table which can be read in interrupt context.

This change makes the rtable_* layer domain-aware and extends the
"struct domain" such that INET, INET6 and MPLS can specify the length
of the binary key used in lookups.  This allows us to keep, or move
towards, AF-free route and rtable layers.

While here stop the madness and pass the size of the maximum key length
in *byte* to rn_inithead0().

ok claudio@@, mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.3 2015/08/20 12:51:10 mpi Exp $ */
d82 1
a82 1
art_attach(unsigned int rtableid, int off)
@


1.3
log
@Make ART internals free of 'struct sockaddr'.

Keep route entry/BSD compatibility goos in the rtable layer.  The way
addresses and masks (prefix-lengths) are encoded is really tied to the
radix-tree implementation.

Since we decided to no longer support non-contiguous masks, we could get
rid of some extra "sockaddr" allocations and reduce the memory grows
related to the use of a multibit-trie.
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.2 2015/08/20 12:41:54 mpi Exp $ */
d81 2
a82 2
int
art_attach(void **head, int off)
a86 2
	if (*head)
		return (1);
d89 1
a89 1
		return (0);
d93 1
a93 1
	case 32: /* AF_INET && AF_MPLS */
d101 1
a101 1
	case 64: /* AF_INET6 */
d111 1
a111 1
		return (0);
d114 1
a114 1
	ar->ar_off = off / 8;
d119 1
a119 1
		return (0);
d121 1
d123 1
a123 2
	*head = ar;
	return (1);
@


1.2
log
@In an email dated 11 Feb 2015, Yoichi Hariguchi accepted to re-license
his reference ART implementation from a BSD 4-clause to ISC.

Thanks a lot to him!
@
text
@d1 1
a1 1
/*	$OpenBSD: art.c,v 1.1 2015/08/20 12:39:43 mpi Exp $ */
a128 11
 * Return a pointer to the address (key).  This is an heritage from the
 * BSD radix tree needed to skip the non-address fields from the flavor
 * of "struct sockaddr" used by this routing table.
 */
static inline uint8_t *
art_satoaddr(struct art_root *at, struct sockaddr *sa)
{
	return (((uint8_t *)sa) + at->ar_off);
}

/*
d135 1
a135 1
	if (old == NULL || old->an_plen != new->an_plen)
d138 1
a138 1
	if (memcmp(old->an_dst, new->an_dst, new->an_dst->sa_len) == 0)
d223 1
a223 1
art_match(struct art_root *ar, struct sockaddr *dst)
a226 1
	uint8_t			*addr;
a228 2
	addr = art_satoaddr(ar, dst);

d263 1
a263 1
art_lookup(struct art_root *ar, struct sockaddr *dst, int plen)
a266 1
	uint8_t			*addr;
a270 2
	addr = art_satoaddr(ar, dst);

a299 5
	/* Make sure we've got a perfect match. */
	if (an == NULL || an->an_plen != plen ||
	    memcmp(an->an_dst, dst, dst->sa_len))
		return (NULL);

d311 1
a311 1
art_insert(struct art_root *ar, struct art_node *an)
d314 1
a314 5
	uint8_t			*addr;
	int			 i, j, plen;

	addr = art_satoaddr(ar, an->an_dst);
	plen = an->an_plen;
d401 1
a401 1
art_delete(struct art_root *ar, struct art_node *an)
d405 1
a405 5
	uint8_t			*addr;
	int			 i, j, plen;

	addr = art_satoaddr(ar, an->an_dst);
	plen = an->an_plen;
@


1.1
log
@Import an alternative routing table backend based on Yoichi Hariguchi's
ART implementation.

ART (Allotment Routing Table) is a multibit-trie algorithm invented by
D. Knuth while reviewing Yoichi's SMART [0] (Smart Multi-Array Routing
Table) paper.

This implementation, unlike the one from the KAME project, supports
variable stride lengths which makes it easier to adapt the consumed
memory/speed trade-off.  It also let you use a bigger first-level
table, what other algorithms such as POPTRIE [1] need to implement
separately.

Adaptation to the OpenBSD kernel has been done with two different data
structures.  ART nodes and route entries are managed separately which
makes the algorithm implementation free of any MULTIPATH logic.

This implementation does not include Path Compression.

[0] http://www.hariguchi.org/art/smart.pdf
[1] http://conferences.sigcomm.org/sigcomm/2015/pdf/papers/p57.pdf

ok dlg@@, reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
d5 1
a17 34
 */
/*
 * Copyright (c) 2001 Yoichi Hariguchi. All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed by Yoichi Hariguchi.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED.  IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT,
 * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
 * STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
 * IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 *
 * THE AUTHOR DOES NOT GUARANTEE THAT THIS SOFTWARE DOES NOT INFRINGE
 * ANY OTHERS' INTELLECTUAL PROPERTIES. IN NO EVENT SHALL THE AUTHOR
 * BE LIABLE FOR ANY INFRINGEMENT OF ANY OTHERS' INTELLECTUAL
 * PROPERTIES.
@

