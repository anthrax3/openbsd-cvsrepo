head	1.206;
access;
symbols
	OPENBSD_6_1:1.202.0.4
	OPENBSD_6_1_BASE:1.202
	OPENBSD_6_0:1.188.0.4
	OPENBSD_6_0_BASE:1.188
	OPENBSD_5_9:1.183.0.2
	OPENBSD_5_9_BASE:1.183
	OPENBSD_5_8:1.180.0.4
	OPENBSD_5_8_BASE:1.180
	OPENBSD_5_7:1.174.0.2
	OPENBSD_5_7_BASE:1.174
	OPENBSD_5_6:1.167.0.4
	OPENBSD_5_6_BASE:1.167
	OPENBSD_5_5:1.164.0.4
	OPENBSD_5_5_BASE:1.164
	OPENBSD_5_4:1.160.0.2
	OPENBSD_5_4_BASE:1.160
	OPENBSD_5_3:1.157.0.2
	OPENBSD_5_3_BASE:1.157
	OPENBSD_5_2:1.154.0.2
	OPENBSD_5_2_BASE:1.154
	OPENBSD_5_1_BASE:1.153
	OPENBSD_5_1:1.153.0.2
	OPENBSD_5_0:1.140.0.2
	OPENBSD_5_0_BASE:1.140
	OPENBSD_4_9:1.128.0.2
	OPENBSD_4_9_BASE:1.128
	OPENBSD_4_8:1.123.0.2
	OPENBSD_4_8_BASE:1.123
	OPENBSD_4_7:1.121.0.2
	OPENBSD_4_7_BASE:1.121
	OPENBSD_4_6:1.118.0.4
	OPENBSD_4_6_BASE:1.118
	OPENBSD_4_5:1.115.0.2
	OPENBSD_4_5_BASE:1.115
	OPENBSD_4_4:1.113.0.2
	OPENBSD_4_4_BASE:1.113
	OPENBSD_4_3:1.111.0.2
	OPENBSD_4_3_BASE:1.111
	OPENBSD_4_2:1.109.0.2
	OPENBSD_4_2_BASE:1.109
	OPENBSD_4_1:1.107.0.4
	OPENBSD_4_1_BASE:1.107
	OPENBSD_4_0:1.107.0.2
	OPENBSD_4_0_BASE:1.107
	OPENBSD_3_9:1.104.0.2
	OPENBSD_3_9_BASE:1.104
	OPENBSD_3_8:1.102.0.2
	OPENBSD_3_8_BASE:1.102
	OPENBSD_3_7:1.97.0.2
	OPENBSD_3_7_BASE:1.97
	OPENBSD_3_6:1.96.0.2
	OPENBSD_3_6_BASE:1.96
	SMP_SYNC_A:1.88
	SMP_SYNC_B:1.87
	OPENBSD_3_5:1.80.0.2
	OPENBSD_3_5_BASE:1.80
	OPENBSD_3_4:1.75.0.2
	OPENBSD_3_4_BASE:1.75
	UBC_SYNC_A:1.59
	OPENBSD_3_3:1.55.0.2
	OPENBSD_3_3_BASE:1.55
	OPENBSD_3_2:1.35.0.2
	OPENBSD_3_2_BASE:1.35
	OPENBSD_3_1:1.21.0.2
	OPENBSD_3_1_BASE:1.21
	UBC_SYNC_B:1.37
	UBC:1.16.0.2
	UBC_BASE:1.16
	SMP:1.14.0.4
	OPENBSD_3_0:1.14.0.2
	OPENBSD_3_0_BASE:1.14;
locks; strict;
comment	@ * @;


1.206
date	2017.06.19.17.58.49;	author bluhm;	state Exp;
branches;
next	1.205;
commitid	o1URxEJIRKTXxlGk;

1.205
date	2017.06.05.22.18.28;	author sashan;	state Exp;
branches;
next	1.204;
commitid	Z0eVTR2Okj5SwW0j;

1.204
date	2017.05.15.12.26.00;	author mpi;	state Exp;
branches;
next	1.203;
commitid	WMZaI3vIHNC1J8ol;

1.203
date	2017.04.23.11.37.11;	author sthen;	state Exp;
branches;
next	1.202;
commitid	rW62UNLR6lCCiL79;

1.202
date	2017.03.17.17.19.16;	author mpi;	state Exp;
branches;
next	1.201;
commitid	CxqvXOMqotM60GAI;

1.201
date	2017.01.30.17.41.34;	author benno;	state Exp;
branches;
next	1.200;
commitid	jxfz0zamCXSbKEUx;

1.200
date	2016.12.29.13.01.48;	author bluhm;	state Exp;
branches;
next	1.199;
commitid	niqDyzVo4a11cYNV;

1.199
date	2016.12.29.00.26.48;	author bluhm;	state Exp;
branches;
next	1.198;
commitid	W0RTGVTzp2EECRUa;

1.198
date	2016.12.28.23.58.20;	author bluhm;	state Exp;
branches;
next	1.197;
commitid	NQrhR96WXu4YKIJG;

1.197
date	2016.11.22.19.29.54;	author procter;	state Exp;
branches;
next	1.196;
commitid	Wwno1dT3ILCyVgb1;

1.196
date	2016.11.21.17.52.20;	author bluhm;	state Exp;
branches;
next	1.195;
commitid	sS4aga0Tx6KeuXu9;

1.195
date	2016.10.26.21.07.22;	author bluhm;	state Exp;
branches;
next	1.194;
commitid	aaKAr0kv3QWNHoVo;

1.194
date	2016.09.27.04.57.17;	author dlg;	state Exp;
branches;
next	1.193;
commitid	irzdR7hwk1GHVaEu;

1.193
date	2016.09.27.02.51.12;	author dlg;	state Exp;
branches;
next	1.192;
commitid	bZuzILta8BoFCDiT;

1.192
date	2016.09.15.02.00.18;	author dlg;	state Exp;
branches;
next	1.191;
commitid	RlO92XR575sygHqm;

1.191
date	2016.09.02.10.19.49;	author dlg;	state Exp;
branches;
next	1.190;
commitid	BjJg5d9vjutHaOpJ;

1.190
date	2016.08.24.09.41.12;	author mpi;	state Exp;
branches;
next	1.189;
commitid	0Qtt2cJVj3irHniv;

1.189
date	2016.08.17.03.24.12;	author procter;	state Exp;
branches;
next	1.188;
commitid	NB7EODatRadIbwDM;

1.188
date	2016.06.15.11.49.34;	author mpi;	state Exp;
branches;
next	1.187;
commitid	qWegq9wDcxofLjIV;

1.187
date	2016.06.15.11.36.06;	author mikeb;	state Exp;
branches;
next	1.186;
commitid	M8Mky6ZXisJyQBHU;

1.186
date	2016.05.31.07.35.36;	author mpi;	state Exp;
branches;
next	1.185;
commitid	3vLf0g1Bo7ekw00g;

1.185
date	2016.05.28.12.04.33;	author sthen;	state Exp;
branches;
next	1.184;
commitid	WWfGhYWt5IExEXFX;

1.184
date	2016.05.24.05.02.34;	author mpi;	state Exp;
branches;
next	1.183;
commitid	GM30whPuIGiYzUbB;

1.183
date	2015.11.24.13.37.16;	author mpi;	state Exp;
branches;
next	1.182;
commitid	djjKhPvMtRdFfuFJ;

1.182
date	2015.09.10.08.28.31;	author mpi;	state Exp;
branches;
next	1.181;
commitid	HnG66cy5an8UA4Iu;

1.181
date	2015.08.19.21.22.41;	author sashan;	state Exp;
branches;
next	1.180;
commitid	YGKggp2X8s4irKok;

1.180
date	2015.07.19.01.58.19;	author sashan;	state Exp;
branches;
next	1.179;
commitid	c3fH8E2IvSIMTGY0;

1.179
date	2015.07.18.15.19.44;	author sashan;	state Exp;
branches;
next	1.178;
commitid	FkfdBMmgICAjEgne;

1.178
date	2015.05.05.23.27.47;	author chris;	state Exp;
branches;
next	1.177;
commitid	dBWvsF15j2Dh4JgT;

1.177
date	2015.04.29.18.05.56;	author bluhm;	state Exp;
branches;
next	1.176;
commitid	cDadL3pwVDCJr5Kx;

1.176
date	2015.04.17.16.42.50;	author bluhm;	state Exp;
branches;
next	1.175;
commitid	2CSuarXE0MZ4LqTU;

1.175
date	2015.03.14.03.38.51;	author jsg;	state Exp;
branches;
next	1.174;
commitid	p4LJxGKbi0BU2cG6;

1.174
date	2015.02.08.01.29.19;	author henning;	state Exp;
branches;
next	1.173;
commitid	11tF1ZuFDUfAm3Oa;

1.173
date	2015.01.24.00.29.06;	author deraadt;	state Exp;
branches;
next	1.172;
commitid	VK3ncyiP3NS1N4Sy;

1.172
date	2014.12.19.17.14.40;	author tedu;	state Exp;
branches;
next	1.171;
commitid	zhW8jJrfVCoAthrR;

1.171
date	2014.12.05.15.50.04;	author mpi;	state Exp;
branches;
next	1.170;
commitid	t9FBKDfc4VDxpEy2;

1.170
date	2014.11.18.02.37.31;	author tedu;	state Exp;
branches;
next	1.169;
commitid	Z1vcFtHO8wRH0yRt;

1.169
date	2014.10.10.16.20.03;	author sthen;	state Exp;
branches;
next	1.168;
commitid	pgLMNmr4A5OrplXA;

1.168
date	2014.09.08.06.24.13;	author jsg;	state Exp;
branches;
next	1.167;
commitid	ZqXwxwmeo3l29NOg;

1.167
date	2014.07.22.11.06.10;	author mpi;	state Exp;
branches;
next	1.166;
commitid	DQakU8LLWV6Iwx84;

1.166
date	2014.07.13.17.41.04;	author bluhm;	state Exp;
branches;
next	1.165;
commitid	LDCw3R00JB4D0Ctu;

1.165
date	2014.03.27.12.07.48;	author jca;	state Exp;
branches;
next	1.164;

1.164
date	2014.01.22.04.34.25;	author henning;	state Exp;
branches;
next	1.163;

1.163
date	2013.11.16.00.36.01;	author chl;	state Exp;
branches;
next	1.162;

1.162
date	2013.10.17.16.27.42;	author bluhm;	state Exp;
branches;
next	1.161;

1.161
date	2013.10.01.20.15.57;	author sf;	state Exp;
branches;
next	1.160;

1.160
date	2013.07.23.22.47.10;	author bluhm;	state Exp;
branches;
next	1.159;

1.159
date	2013.06.26.09.12.39;	author henning;	state Exp;
branches;
next	1.158;

1.158
date	2013.06.17.19.50.06;	author bluhm;	state Exp;
branches;
next	1.157;

1.157
date	2012.11.06.12.32.41;	author henning;	state Exp;
branches;
next	1.156;

1.156
date	2012.11.01.07.55.56;	author henning;	state Exp;
branches;
next	1.155;

1.155
date	2012.10.30.12.09.05;	author florian;	state Exp;
branches;
next	1.154;

1.154
date	2012.05.12.13.08.48;	author mpf;	state Exp;
branches;
next	1.153;

1.153
date	2012.02.03.01.57.51;	author bluhm;	state Exp;
branches;
next	1.152;

1.152
date	2012.01.26.20.16.06;	author bluhm;	state Exp;
branches;
next	1.151;

1.151
date	2012.01.23.18.37.20;	author bluhm;	state Exp;
branches;
next	1.150;

1.150
date	2012.01.15.22.55.35;	author bluhm;	state Exp;
branches;
next	1.149;

1.149
date	2012.01.13.11.24.35;	author bluhm;	state Exp;
branches;
next	1.148;

1.148
date	2012.01.03.17.06.38;	author bluhm;	state Exp;
branches;
next	1.147;

1.147
date	2011.11.25.12.52.10;	author dlg;	state Exp;
branches;
next	1.146;

1.146
date	2011.09.28.17.15.45;	author bluhm;	state Exp;
branches;
next	1.145;

1.145
date	2011.09.22.14.57.12;	author bluhm;	state Exp;
branches;
next	1.144;

1.144
date	2011.09.21.19.07.30;	author bluhm;	state Exp;
branches;
next	1.143;

1.143
date	2011.09.20.10.51.18;	author bluhm;	state Exp;
branches;
next	1.142;

1.142
date	2011.09.19.12.51.52;	author bluhm;	state Exp;
branches;
next	1.141;

1.141
date	2011.09.18.11.17.57;	author miod;	state Exp;
branches;
next	1.140;

1.140
date	2011.07.18.21.03.10;	author mikeb;	state Exp;
branches;
next	1.139;

1.139
date	2011.07.07.20.46.36;	author bluhm;	state Exp;
branches;
next	1.138;

1.138
date	2011.07.05.22.00.04;	author bluhm;	state Exp;
branches;
next	1.137;

1.137
date	2011.07.05.19.53.43;	author mikeb;	state Exp;
branches;
next	1.136;

1.136
date	2011.07.03.18.08.02;	author claudio;	state Exp;
branches;
next	1.135;

1.135
date	2011.06.21.08.59.47;	author bluhm;	state Exp;
branches;
next	1.134;

1.134
date	2011.06.20.19.03.41;	author claudio;	state Exp;
branches;
next	1.133;

1.133
date	2011.05.24.14.01.52;	author claudio;	state Exp;
branches;
next	1.132;

1.132
date	2011.04.23.10.00.36;	author bluhm;	state Exp;
branches;
next	1.131;

1.131
date	2011.04.04.14.14.53;	author henning;	state Exp;
branches;
next	1.130;

1.130
date	2011.03.24.20.09.44;	author bluhm;	state Exp;
branches;
next	1.129;

1.129
date	2011.03.23.18.34.17;	author bluhm;	state Exp;
branches;
next	1.128;

1.128
date	2011.02.01.16.10.31;	author bluhm;	state Exp;
branches;
next	1.127;

1.127
date	2011.01.20.15.03.03;	author bluhm;	state Exp;
branches;
next	1.126;

1.126
date	2011.01.19.11.39.57;	author bluhm;	state Exp;
branches;
next	1.125;

1.125
date	2011.01.06.14.01.36;	author bluhm;	state Exp;
branches;
next	1.124;

1.124
date	2010.12.31.12.26.57;	author bluhm;	state Exp;
branches;
next	1.123;

1.123
date	2010.07.08.19.30.16;	author sthen;	state Exp;
branches;
next	1.122;

1.122
date	2010.07.02.02.40.16;	author blambert;	state Exp;
branches;
next	1.121;

1.121
date	2010.01.18.23.52.46;	author mcbride;	state Exp;
branches;
next	1.120;

1.120
date	2009.09.01.15.51.06;	author jsing;	state Exp;
branches;
next	1.119;

1.119
date	2009.07.21.14.48.08;	author henning;	state Exp;
branches;
next	1.118;

1.118
date	2009.06.25.09.30.28;	author sthen;	state Exp;
branches;
next	1.117;

1.117
date	2009.04.07.13.26.23;	author henning;	state Exp;
branches;
next	1.116;

1.116
date	2009.04.06.12.05.55;	author henning;	state Exp;
branches;
next	1.115;

1.115
date	2009.01.31.20.06.55;	author henning;	state Exp;
branches;
next	1.114;

1.114
date	2009.01.29.14.11.45;	author henning;	state Exp;
branches;
next	1.113;

1.113
date	2008.05.07.07.07.29;	author markus;	state Exp;
branches;
next	1.112;

1.112
date	2008.05.07.06.23.30;	author markus;	state Exp;
branches;
next	1.111;

1.111
date	2007.12.30.10.32.24;	author mglocker;	state Exp;
branches;
next	1.110;

1.110
date	2007.12.30.00.16.39;	author mglocker;	state Exp;
branches;
next	1.109;

1.109
date	2007.05.28.17.16.39;	author henning;	state Exp;
branches;
next	1.108;

1.108
date	2007.05.26.00.36.03;	author krw;	state Exp;
branches;
next	1.107;

1.107
date	2006.04.16.00.59.52;	author pascoe;	state Exp;
branches;
next	1.106;

1.106
date	2006.03.25.20.55.24;	author dhartmei;	state Exp;
branches;
next	1.105;

1.105
date	2006.03.14.11.09.42;	author djm;	state Exp;
branches;
next	1.104;

1.104
date	2006.01.18.22.03.21;	author dhartmei;	state Exp;
branches
	1.104.2.1;
next	1.103;

1.103
date	2005.10.17.08.43.35;	author henning;	state Exp;
branches;
next	1.102;

1.102
date	2005.08.06.12.11.09;	author pascoe;	state Exp;
branches
	1.102.2.1;
next	1.101;

1.101
date	2005.06.13.20.17.25;	author henning;	state Exp;
branches;
next	1.100;

1.100
date	2005.05.27.17.22.41;	author dhartmei;	state Exp;
branches;
next	1.99;

1.99
date	2005.05.22.16.22.41;	author dhartmei;	state Exp;
branches;
next	1.98;

1.98
date	2005.05.21.21.03.57;	author henning;	state Exp;
branches;
next	1.97;

1.97
date	2004.09.21.16.59.12;	author aaron;	state Exp;
branches
	1.97.2.1;
next	1.96;

1.96
date	2004.07.17.00.17.27;	author frantzen;	state Exp;
branches;
next	1.95;

1.95
date	2004.07.11.15.54.21;	author itojun;	state Exp;
branches;
next	1.94;

1.94
date	2004.07.05.00.15.20;	author henning;	state Exp;
branches;
next	1.93;

1.93
date	2004.07.03.05.57.12;	author itojun;	state Exp;
branches;
next	1.92;

1.92
date	2004.06.25.11.04.03;	author itojun;	state Exp;
branches;
next	1.91;

1.91
date	2004.06.25.00.42.58;	author itojun;	state Exp;
branches;
next	1.90;

1.90
date	2004.06.24.19.35.25;	author tholo;	state Exp;
branches;
next	1.89;

1.89
date	2004.06.21.23.50.36;	author tholo;	state Exp;
branches;
next	1.88;

1.88
date	2004.06.10.14.22.54;	author dhartmei;	state Exp;
branches;
next	1.87;

1.87
date	2004.05.11.07.34.11;	author dhartmei;	state Exp;
branches;
next	1.86;

1.86
date	2004.05.09.00.16.38;	author dhartmei;	state Exp;
branches;
next	1.85;

1.85
date	2004.05.05.23.16.03;	author frantzen;	state Exp;
branches;
next	1.84;

1.84
date	2004.04.28.02.43.09;	author pb;	state Exp;
branches;
next	1.83;

1.83
date	2004.04.27.18.28.07;	author frantzen;	state Exp;
branches;
next	1.82;

1.82
date	2004.04.26.02.03.38;	author mcbride;	state Exp;
branches;
next	1.81;

1.81
date	2004.04.24.19.14.48;	author frantzen;	state Exp;
branches;
next	1.80;

1.80
date	2004.03.09.21.44.41;	author mcbride;	state Exp;
branches
	1.80.2.1;
next	1.79;

1.79
date	2004.02.10.18.49.10;	author henning;	state Exp;
branches;
next	1.78;

1.78
date	2004.01.16.21.15.42;	author mcbride;	state Exp;
branches;
next	1.77;

1.77
date	2003.12.31.11.18.25;	author cedric;	state Exp;
branches;
next	1.76;

1.76
date	2003.12.18.20.13.23;	author dhartmei;	state Exp;
branches;
next	1.75;

1.75
date	2003.08.29.01.49.08;	author dhartmei;	state Exp;
branches
	1.75.2.1;
next	1.74;

1.74
date	2003.08.22.21.50.34;	author david;	state Exp;
branches;
next	1.73;

1.73
date	2003.08.22.15.19.23;	author henning;	state Exp;
branches;
next	1.72;

1.72
date	2003.08.21.19.12.08;	author frantzen;	state Exp;
branches;
next	1.71;

1.71
date	2003.08.14.19.00.12;	author jason;	state Exp;
branches;
next	1.70;

1.70
date	2003.07.17.16.25.52;	author frantzen;	state Exp;
branches;
next	1.69;

1.69
date	2003.07.12.09.33.32;	author dhartmei;	state Exp;
branches;
next	1.68;

1.68
date	2003.07.10.05.50.10;	author itojun;	state Exp;
branches;
next	1.67;

1.67
date	2003.07.10.04.20.59;	author itojun;	state Exp;
branches;
next	1.66;

1.66
date	2003.07.09.22.11.08;	author itojun;	state Exp;
branches;
next	1.65;

1.65
date	2003.07.09.22.09.20;	author itojun;	state Exp;
branches;
next	1.64;

1.64
date	2003.07.09.22.03.16;	author itojun;	state Exp;
branches;
next	1.63;

1.63
date	2003.07.09.07.18.50;	author dhartmei;	state Exp;
branches;
next	1.62;

1.62
date	2003.07.01.00.28.52;	author itojun;	state Exp;
branches;
next	1.61;

1.61
date	2003.06.29.23.37.12;	author itojun;	state Exp;
branches;
next	1.60;

1.60
date	2003.06.28.07.27.20;	author itojun;	state Exp;
branches;
next	1.59;

1.59
date	2003.05.14.23.46.45;	author frantzen;	state Exp;
branches;
next	1.58;

1.58
date	2003.05.14.08.42.00;	author canacar;	state Exp;
branches;
next	1.57;

1.57
date	2003.05.11.20.44.03;	author frantzen;	state Exp;
branches;
next	1.56;

1.56
date	2003.04.05.20.20.58;	author cedric;	state Exp;
branches;
next	1.55;

1.55
date	2003.02.18.08.05.15;	author camield;	state Exp;
branches
	1.55.2.1;
next	1.54;

1.54
date	2003.02.12.20.43.36;	author dhartmei;	state Exp;
branches;
next	1.53;

1.53
date	2003.02.08.20.13.20;	author dhartmei;	state Exp;
branches;
next	1.52;

1.52
date	2003.01.25.19.47.05;	author dhartmei;	state Exp;
branches;
next	1.51;

1.51
date	2003.01.09.15.58.35;	author dhartmei;	state Exp;
branches;
next	1.50;

1.50
date	2003.01.07.00.21.07;	author dhartmei;	state Exp;
branches;
next	1.49;

1.49
date	2003.01.05.22.14.23;	author dhartmei;	state Exp;
branches;
next	1.48;

1.48
date	2003.01.04.17.40.51;	author dhartmei;	state Exp;
branches;
next	1.47;

1.47
date	2003.01.03.19.31.43;	author deraadt;	state Exp;
branches;
next	1.46;

1.46
date	2003.01.01.16.07.45;	author henning;	state Exp;
branches;
next	1.45;

1.45
date	2003.01.01.04.26.19;	author dhartmei;	state Exp;
branches;
next	1.44;

1.44
date	2002.12.31.19.18.41;	author mcbride;	state Exp;
branches;
next	1.43;

1.43
date	2002.12.18.19.17.07;	author henning;	state Exp;
branches;
next	1.42;

1.42
date	2002.12.18.16.28.40;	author dhartmei;	state Exp;
branches;
next	1.41;

1.41
date	2002.12.17.12.30.13;	author mcbride;	state Exp;
branches;
next	1.40;

1.40
date	2002.12.06.00.47.32;	author dhartmei;	state Exp;
branches;
next	1.39;

1.39
date	2002.11.23.05.16.58;	author mcbride;	state Exp;
branches;
next	1.38;

1.38
date	2002.10.29.19.51.04;	author mickey;	state Exp;
branches;
next	1.37;

1.37
date	2002.10.22.12.23.35;	author mcbride;	state Exp;
branches;
next	1.36;

1.36
date	2002.10.07.14.53.00;	author dhartmei;	state Exp;
branches;
next	1.35;

1.35
date	2002.06.28.00.08.23;	author deraadt;	state Exp;
branches
	1.35.2.1;
next	1.34;

1.34
date	2002.06.11.18.03.24;	author frantzen;	state Exp;
branches;
next	1.33;

1.33
date	2002.06.11.03.22.04;	author dhartmei;	state Exp;
branches;
next	1.32;

1.32
date	2002.06.11.02.27.19;	author frantzen;	state Exp;
branches;
next	1.31;

1.31
date	2002.06.10.17.05.11;	author dhartmei;	state Exp;
branches;
next	1.30;

1.30
date	2002.06.08.08.09.11;	author frantzen;	state Exp;
branches;
next	1.29;

1.29
date	2002.06.07.21.14.02;	author frantzen;	state Exp;
branches;
next	1.28;

1.28
date	2002.05.21.08.42.35;	author espie;	state Exp;
branches;
next	1.27;

1.27
date	2002.05.19.22.31.28;	author deraadt;	state Exp;
branches;
next	1.26;

1.26
date	2002.05.09.21.58.12;	author jasoni;	state Exp;
branches;
next	1.25;

1.25
date	2002.05.06.15.49.54;	author jasoni;	state Exp;
branches;
next	1.24;

1.24
date	2002.04.24.18.10.25;	author dhartmei;	state Exp;
branches;
next	1.23;

1.23
date	2002.04.20.18.26.03;	author dhartmei;	state Exp;
branches;
next	1.22;

1.22
date	2002.04.20.10.13.57;	author fgsch;	state Exp;
branches;
next	1.21;

1.21
date	2002.03.27.18.16.21;	author mickey;	state Exp;
branches;
next	1.20;

1.20
date	2002.02.26.07.25.33;	author dhartmei;	state Exp;
branches;
next	1.19;

1.19
date	2002.02.25.00.29.07;	author dhartmei;	state Exp;
branches;
next	1.18;

1.18
date	2002.02.14.15.32.11;	author dhartmei;	state Exp;
branches;
next	1.17;

1.17
date	2002.01.23.00.39.48;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2001.12.03.22.25.06;	author dhartmei;	state Exp;
branches
	1.16.2.1;
next	1.15;

1.15
date	2001.11.06.11.48.29;	author dhartmei;	state Exp;
branches;
next	1.14;

1.14
date	2001.10.17.22.21.42;	author markus;	state Exp;
branches
	1.14.4.1;
next	1.13;

1.13
date	2001.10.07.21.34.27;	author provos;	state Exp;
branches;
next	1.12;

1.12
date	2001.09.15.16.47.07;	author dhartmei;	state Exp;
branches;
next	1.11;

1.11
date	2001.09.15.03.54.40;	author frantzen;	state Exp;
branches;
next	1.10;

1.10
date	2001.09.08.02.10.33;	author provos;	state Exp;
branches;
next	1.9;

1.9
date	2001.09.06.20.53.44;	author dhartmei;	state Exp;
branches;
next	1.8;

1.8
date	2001.09.04.08.55.37;	author dhartmei;	state Exp;
branches;
next	1.7;

1.7
date	2001.08.31.23.05.22;	author frantzen;	state Exp;
branches;
next	1.6;

1.6
date	2001.08.11.12.05.00;	author dhartmei;	state Exp;
branches;
next	1.5;

1.5
date	2001.08.02.06.59.25;	author deraadt;	state Exp;
branches;
next	1.4;

1.4
date	2001.08.01.23.07.36;	author provos;	state Exp;
branches;
next	1.3;

1.3
date	2001.07.17.22.22.14;	author provos;	state Exp;
branches;
next	1.2;

1.2
date	2001.07.17.21.54.26;	author provos;	state Exp;
branches;
next	1.1;

1.1
date	2001.07.17.20.35.26;	author provos;	state Exp;
branches;
next	;

1.14.4.1
date	2001.10.31.03.29.02;	author nate;	state Exp;
branches;
next	1.14.4.2;

1.14.4.2
date	2001.11.13.22.59.58;	author niklas;	state Exp;
branches;
next	1.14.4.3;

1.14.4.3
date	2001.12.05.01.02.40;	author niklas;	state Exp;
branches;
next	1.14.4.4;

1.14.4.4
date	2002.03.06.02.15.07;	author niklas;	state Exp;
branches;
next	1.14.4.5;

1.14.4.5
date	2003.03.28.00.41.29;	author niklas;	state Exp;
branches;
next	1.14.4.6;

1.14.4.6
date	2003.05.13.19.36.16;	author ho;	state Exp;
branches;
next	1.14.4.7;

1.14.4.7
date	2003.05.16.00.29.44;	author niklas;	state Exp;
branches;
next	1.14.4.8;

1.14.4.8
date	2004.02.19.10.57.22;	author niklas;	state Exp;
branches;
next	1.14.4.9;

1.14.4.9
date	2004.06.05.23.11.24;	author niklas;	state Exp;
branches;
next	1.14.4.10;

1.14.4.10
date	2004.06.13.08.50.17;	author niklas;	state Exp;
branches;
next	;

1.16.2.1
date	2002.01.31.22.55.44;	author niklas;	state Exp;
branches;
next	1.16.2.2;

1.16.2.2
date	2002.06.11.03.30.46;	author art;	state Exp;
branches;
next	1.16.2.3;

1.16.2.3
date	2002.10.29.00.36.46;	author art;	state Exp;
branches;
next	1.16.2.4;

1.16.2.4
date	2003.05.19.22.29.35;	author tedu;	state Exp;
branches;
next	;

1.35.2.1
date	2003.09.24.19.20.31;	author brad;	state Exp;
branches;
next	;

1.55.2.1
date	2003.09.24.19.31.19;	author brad;	state Exp;
branches;
next	;

1.75.2.1
date	2004.04.30.23.28.36;	author brad;	state Exp;
branches;
next	;

1.80.2.1
date	2004.04.30.21.46.33;	author brad;	state Exp;
branches;
next	;

1.97.2.1
date	2006.01.19.21.51.36;	author brad;	state Exp;
branches;
next	;

1.102.2.1
date	2006.01.19.21.52.53;	author brad;	state Exp;
branches;
next	1.102.2.2;

1.102.2.2
date	2006.05.02.22.08.47;	author brad;	state Exp;
branches;
next	;

1.104.2.1
date	2006.05.02.22.00.04;	author brad;	state Exp;
branches;
next	1.104.2.2;

1.104.2.2
date	2006.06.30.08.32.56;	author brad;	state Exp;
branches;
next	;


desc
@@


1.206
log
@When dealing with mbuf pointers passed down as function parameters,
bugs could easily result in use-after-free or double free.  Introduce
m_freemp() which automatically resets the pointer before freeing
it.  So we have less dangling pointers in the kernel.
OK krw@@ mpi@@ claudio@@
@
text
@/*	$OpenBSD: pf_norm.c,v 1.205 2017/06/05 22:18:28 sashan Exp $ */

/*
 * Copyright 2001 Niels Provos <provos@@citi.umich.edu>
 * Copyright 2009 Henning Brauer <henning@@openbsd.org>
 * Copyright 2011 Alexander Bluhm <bluhm@@openbsd.org>
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

#include "pflog.h"

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/mbuf.h>
#include <sys/filio.h>
#include <sys/fcntl.h>
#include <sys/socket.h>
#include <sys/kernel.h>
#include <sys/time.h>
#include <sys/pool.h>
#include <sys/syslog.h>
#include <sys/mutex.h>

#include <net/if.h>
#include <net/if_var.h>
#include <net/if_pflog.h>

#include <netinet/in.h>
#include <netinet/ip.h>
#include <netinet/ip_var.h>
#include <netinet/ip_icmp.h>
#include <netinet/tcp.h>
#include <netinet/tcp_seq.h>
#include <netinet/tcp_fsm.h>
#include <netinet/udp.h>

#ifdef INET6
#include <netinet6/in6_var.h>
#include <netinet/ip6.h>
#include <netinet6/ip6_var.h>
#include <netinet/icmp6.h>
#include <netinet6/nd6.h>
#endif /* INET6 */

#include <net/pfvar.h>
#include <net/pfvar_priv.h>

struct pf_frent {
	TAILQ_ENTRY(pf_frent) fr_next;
	struct mbuf	*fe_m;
	u_int16_t	 fe_hdrlen;	/* ipv4 header length with ip options
					   ipv6, extension, fragment header */
	u_int16_t	 fe_extoff;	/* last extension header offset or 0 */
	u_int16_t	 fe_len;	/* fragment length */
	u_int16_t	 fe_off;	/* fragment offset */
	u_int16_t	 fe_mff;	/* more fragment flag */
};

/* keep synced with struct pf_fragment, used in RB_FIND */
struct pf_fragment_cmp {
	struct pf_addr	fr_src;
	struct pf_addr	fr_dst;
	u_int32_t	fr_id;
	sa_family_t	fr_af;
	u_int8_t	fr_proto;
	u_int8_t	fr_direction;
};

struct pf_fragment {
	struct pf_addr	fr_src;		/* ip source address */
	struct pf_addr	fr_dst;		/* ip destination address */
	u_int32_t	fr_id;		/* fragment id for reassemble */
	sa_family_t	fr_af;		/* address family */
	u_int8_t	fr_proto;	/* protocol of this fragment */
	u_int8_t	fr_direction;	/* pf packet direction */

	RB_ENTRY(pf_fragment) fr_entry;
	TAILQ_ENTRY(pf_fragment) frag_next;
	TAILQ_HEAD(pf_fragq, pf_frent) fr_queue;
	int32_t		fr_timeout;
	u_int16_t	fr_maxlen;	/* maximum length of single fragment */
};

struct pf_fragment_tag {
	u_int16_t	 ft_hdrlen;	/* header length of reassembled pkt */
	u_int16_t	 ft_extoff;	/* last extension header offset or 0 */
	u_int16_t	 ft_maxlen;	/* maximum fragment payload length */
};

TAILQ_HEAD(pf_fragqueue, pf_fragment)	pf_fragqueue;

static __inline int	 pf_frag_compare(struct pf_fragment *,
			    struct pf_fragment *);
RB_HEAD(pf_frag_tree, pf_fragment)	pf_frag_tree, pf_cache_tree;
RB_PROTOTYPE(pf_frag_tree, pf_fragment, fr_entry, pf_frag_compare);
RB_GENERATE(pf_frag_tree, pf_fragment, fr_entry, pf_frag_compare);

/* Private prototypes */
void			 pf_flush_fragments(void);
void			 pf_free_fragment(struct pf_fragment *);
struct pf_fragment	*pf_find_fragment(struct pf_fragment_cmp *,
			    struct pf_frag_tree *);
struct pf_frent		*pf_create_fragment(u_short *);
struct pf_fragment	*pf_fillup_fragment(struct pf_fragment_cmp *,
			    struct pf_frent *, u_short *);
int			 pf_isfull_fragment(struct pf_fragment *);
struct mbuf		*pf_join_fragment(struct pf_fragment *);
int			 pf_reassemble(struct mbuf **, int, u_short *);
#ifdef INET6
int			 pf_reassemble6(struct mbuf **, struct ip6_frag *,
			    u_int16_t, u_int16_t, int, u_short *);
#endif /* INET6 */

/* Globals */
struct pool		 pf_frent_pl, pf_frag_pl;
struct pool		 pf_state_scrub_pl;
int			 pf_nfrents;

#ifdef WITH_PF_LOCK
struct mutex		 pf_frag_mtx;

#define PF_FRAG_LOCK_INIT()	mtx_init(&pf_frag_mtx, IPL_SOFTNET)
#define PF_FRAG_LOCK()		mtx_enter(&pf_frag_mtx)
#define PF_FRAG_UNLOCK()	mtx_leave(&pf_frag_mtx)
#else /* !WITH_PF_LOCK */
#define PF_FRAG_LOCK_INIT()	(void)(0)
#define PF_FRAG_LOCK()		(void)(0)
#define PF_FRAG_UNLOCK()	(void)(0)
#endif /* WITH_PF_LOCK */

void
pf_normalize_init(void)
{
	pool_init(&pf_frent_pl, sizeof(struct pf_frent), 0,
	    IPL_SOFTNET, 0, "pffrent", NULL);
	pool_init(&pf_frag_pl, sizeof(struct pf_fragment), 0,
	    IPL_SOFTNET, 0, "pffrag", NULL);
	pool_init(&pf_state_scrub_pl, sizeof(struct pf_state_scrub), 0,
	    IPL_SOFTNET, 0, "pfstscr", NULL);

	pool_sethiwat(&pf_frag_pl, PFFRAG_FRAG_HIWAT);
	pool_sethardlimit(&pf_frent_pl, PFFRAG_FRENT_HIWAT, NULL, 0);

	TAILQ_INIT(&pf_fragqueue);

	PF_FRAG_LOCK_INIT();
}

static __inline int
pf_frag_compare(struct pf_fragment *a, struct pf_fragment *b)
{
	int	diff;

	if ((diff = a->fr_id - b->fr_id) != 0)
		return (diff);
	if ((diff = a->fr_proto - b->fr_proto) != 0)
		return (diff);
	if ((diff = a->fr_af - b->fr_af) != 0)
		return (diff);
	if ((diff = pf_addr_compare(&a->fr_src, &b->fr_src, a->fr_af)) != 0)
		return (diff);
	if ((diff = pf_addr_compare(&a->fr_dst, &b->fr_dst, a->fr_af)) != 0)
		return (diff);

	return (0);
}

void
pf_purge_expired_fragments(void)
{
	struct pf_fragment	*frag;
	int32_t			 expire;

	PF_ASSERT_UNLOCKED();

	expire = time_uptime - pf_default_rule.timeout[PFTM_FRAG];

	PF_FRAG_LOCK();
	while ((frag = TAILQ_LAST(&pf_fragqueue, pf_fragqueue)) != NULL) {
		if (frag->fr_timeout > expire)
			break;
		DPFPRINTF(LOG_NOTICE, "expiring %d(%p)", frag->fr_id, frag);
		pf_free_fragment(frag);
	}
	PF_FRAG_UNLOCK();
}

/*
 * Try to flush old fragments to make space for new ones
 */
void
pf_flush_fragments(void)
{
	struct pf_fragment	*frag;
	int			 goal;

	goal = pf_nfrents * 9 / 10;
	DPFPRINTF(LOG_NOTICE, "trying to free > %d frents", pf_nfrents - goal);
	while (goal < pf_nfrents) {
		if ((frag = TAILQ_LAST(&pf_fragqueue, pf_fragqueue)) == NULL)
			break;
		pf_free_fragment(frag);
	}
}

/*
 * Remove a fragment from the fragment queue, free its fragment entries,
 * and free the fragment itself.
 */
void
pf_free_fragment(struct pf_fragment *frag)
{
	struct pf_frent		*frent;

	RB_REMOVE(pf_frag_tree, &pf_frag_tree, frag);
	TAILQ_REMOVE(&pf_fragqueue, frag, frag_next);

	/* Free all fragment entries */
	while ((frent = TAILQ_FIRST(&frag->fr_queue)) != NULL) {
		TAILQ_REMOVE(&frag->fr_queue, frent, fr_next);
		m_freem(frent->fe_m);
		pool_put(&pf_frent_pl, frent);
		pf_nfrents--;
	}
	pool_put(&pf_frag_pl, frag);
}

struct pf_fragment *
pf_find_fragment(struct pf_fragment_cmp *key, struct pf_frag_tree *tree)
{
	struct pf_fragment	*frag;

	frag = RB_FIND(pf_frag_tree, tree, (struct pf_fragment *)key);
	if (frag != NULL) {
		TAILQ_REMOVE(&pf_fragqueue, frag, frag_next);
		TAILQ_INSERT_HEAD(&pf_fragqueue, frag, frag_next);
	}

	return (frag);
}

struct pf_frent *
pf_create_fragment(u_short *reason)
{
	struct pf_frent	*frent;

	frent = pool_get(&pf_frent_pl, PR_NOWAIT);
	if (frent == NULL) {
		pf_flush_fragments();
		frent = pool_get(&pf_frent_pl, PR_NOWAIT);
		if (frent == NULL) {
			REASON_SET(reason, PFRES_MEMORY);
			return (NULL);
		}
	}
	pf_nfrents++;

	return (frent);
}

struct pf_fragment *
pf_fillup_fragment(struct pf_fragment_cmp *key, struct pf_frent *frent,
    u_short *reason)
{
	struct pf_frent		*after, *next, *prev;
	struct pf_fragment	*frag;
	u_int16_t		 total;

	/* No empty fragments */
	if (frent->fe_len == 0) {
		DPFPRINTF(LOG_NOTICE, "bad fragment: len 0");
		goto bad_fragment;
	}

	/* All fragments are 8 byte aligned */
	if (frent->fe_mff && (frent->fe_len & 0x7)) {
		DPFPRINTF(LOG_NOTICE, "bad fragment: mff and len %d",
		    frent->fe_len);
		goto bad_fragment;
	}

	/* Respect maximum length, IP_MAXPACKET == IPV6_MAXPACKET */
	if (frent->fe_off + frent->fe_len > IP_MAXPACKET) {
		DPFPRINTF(LOG_NOTICE, "bad fragment: max packet %d",
		    frent->fe_off + frent->fe_len);
		goto bad_fragment;
	}

	DPFPRINTF(LOG_INFO, key->fr_af == AF_INET ?
	    "reass frag %d @@ %d-%d" : "reass frag %#08x @@ %d-%d",
	    key->fr_id, frent->fe_off, frent->fe_off + frent->fe_len);

	/* Fully buffer all of the fragments in this fragment queue */
	frag = pf_find_fragment(key, &pf_frag_tree);

	/* Create a new reassembly queue for this packet */
	if (frag == NULL) {
		frag = pool_get(&pf_frag_pl, PR_NOWAIT);
		if (frag == NULL) {
			pf_flush_fragments();
			frag = pool_get(&pf_frag_pl, PR_NOWAIT);
			if (frag == NULL) {
				REASON_SET(reason, PFRES_MEMORY);
				goto drop_fragment;
			}
		}

		*(struct pf_fragment_cmp *)frag = *key;
		TAILQ_INIT(&frag->fr_queue);
		frag->fr_timeout = time_uptime;
		frag->fr_maxlen = frent->fe_len;

		RB_INSERT(pf_frag_tree, &pf_frag_tree, frag);
		TAILQ_INSERT_HEAD(&pf_fragqueue, frag, frag_next);

		/* We do not have a previous fragment */
		TAILQ_INSERT_HEAD(&frag->fr_queue, frent, fr_next);

		return (frag);
	}

	KASSERT(!TAILQ_EMPTY(&frag->fr_queue));

	/* Remember maximum fragment len for refragmentation */
	if (frent->fe_len > frag->fr_maxlen)
		frag->fr_maxlen = frent->fe_len;

	/* Maximum data we have seen already */
	total = TAILQ_LAST(&frag->fr_queue, pf_fragq)->fe_off +
	    TAILQ_LAST(&frag->fr_queue, pf_fragq)->fe_len;

	/* Non terminal fragments must have more fragments flag */
	if (frent->fe_off + frent->fe_len < total && !frent->fe_mff)
		goto free_ipv6_fragment;

	/* Check if we saw the last fragment already */
	if (!TAILQ_LAST(&frag->fr_queue, pf_fragq)->fe_mff) {
		if (frent->fe_off + frent->fe_len > total ||
		    (frent->fe_off + frent->fe_len == total && frent->fe_mff))
			goto free_ipv6_fragment;
	} else {
		if (frent->fe_off + frent->fe_len == total && !frent->fe_mff)
			goto free_ipv6_fragment;
	}

	/* Find a fragment after the current one */
	prev = NULL;
	TAILQ_FOREACH(after, &frag->fr_queue, fr_next) {
		if (after->fe_off > frent->fe_off)
			break;
		prev = after;
	}

	KASSERT(prev != NULL || after != NULL);

	if (prev != NULL && prev->fe_off + prev->fe_len > frent->fe_off) {
		u_int16_t	precut;

#ifdef INET6
		if (frag->fr_af == AF_INET6)
			goto free_fragment;
#endif /* INET6 */

		precut = prev->fe_off + prev->fe_len - frent->fe_off;
		if (precut >= frent->fe_len) {
			DPFPRINTF(LOG_NOTICE, "new frag overlapped");
			goto drop_fragment;
		}
		DPFPRINTF(LOG_NOTICE, "frag head overlap %d", precut);
		m_adj(frent->fe_m, precut);
		frent->fe_off += precut;
		frent->fe_len -= precut;
	}

	for (; after != NULL && frent->fe_off + frent->fe_len > after->fe_off;
	    after = next) {
		u_int16_t	aftercut;

#ifdef INET6
		if (frag->fr_af == AF_INET6)
			goto free_fragment;
#endif /* INET6 */

		aftercut = frent->fe_off + frent->fe_len - after->fe_off;
		if (aftercut < after->fe_len) {
			DPFPRINTF(LOG_NOTICE, "frag tail overlap %d", aftercut);
			m_adj(after->fe_m, aftercut);
			after->fe_off += aftercut;
			after->fe_len -= aftercut;
			break;
		}

		/* This fragment is completely overlapped, lose it */
		DPFPRINTF(LOG_NOTICE, "old frag overlapped");
		next = TAILQ_NEXT(after, fr_next);
		TAILQ_REMOVE(&frag->fr_queue, after, fr_next);
		m_freem(after->fe_m);
		pool_put(&pf_frent_pl, after);
		pf_nfrents--;
	}

	if (prev == NULL)
		TAILQ_INSERT_HEAD(&frag->fr_queue, frent, fr_next);
	else
		TAILQ_INSERT_AFTER(&frag->fr_queue, prev, frent, fr_next);

	return (frag);

free_ipv6_fragment:
#ifdef INET6
	if (frag->fr_af == AF_INET)
		goto bad_fragment;
free_fragment:
	/*
	 * RFC 5722, Errata 3089:  When reassembling an IPv6 datagram, if one
	 * or more its constituent fragments is determined to be an overlapping
	 * fragment, the entire datagram (and any constituent fragments) MUST
	 * be silently discarded.
	 */
	DPFPRINTF(LOG_NOTICE, "flush overlapping fragments");
	pf_free_fragment(frag);
#endif /* INET6 */
bad_fragment:
	REASON_SET(reason, PFRES_FRAG);
drop_fragment:
	pool_put(&pf_frent_pl, frent);
	pf_nfrents--;
	return (NULL);
}

int
pf_isfull_fragment(struct pf_fragment *frag)
{
	struct pf_frent		*frent, *next;
	u_int16_t		 off, total;

	KASSERT(!TAILQ_EMPTY(&frag->fr_queue));

	/* Check if we are completely reassembled */
	if (TAILQ_LAST(&frag->fr_queue, pf_fragq)->fe_mff)
		return (0);

	/* Maximum data we have seen already */
	total = TAILQ_LAST(&frag->fr_queue, pf_fragq)->fe_off +
	    TAILQ_LAST(&frag->fr_queue, pf_fragq)->fe_len;

	/* Check if we have all the data */
	off = 0;
	for (frent = TAILQ_FIRST(&frag->fr_queue); frent; frent = next) {
		next = TAILQ_NEXT(frent, fr_next);
		off += frent->fe_len;
		if (off < total && (next == NULL || next->fe_off != off)) {
			DPFPRINTF(LOG_NOTICE,
			    "missing fragment at %d, next %d, total %d",
			    off, next == NULL ? -1 : next->fe_off, total);
			return (0);
		}
	}
	DPFPRINTF(LOG_INFO, "%d < %d?", off, total);
	if (off < total)
		return (0);
	KASSERT(off == total);

	return (1);
}

struct mbuf *
pf_join_fragment(struct pf_fragment *frag)
{
	struct mbuf		*m, *m2;
	struct pf_frent		*frent;

	frent = TAILQ_FIRST(&frag->fr_queue);
	TAILQ_REMOVE(&frag->fr_queue, frent, fr_next);

	m = frent->fe_m;
	/* Strip off any trailing bytes */
	if ((frent->fe_hdrlen + frent->fe_len) < m->m_pkthdr.len)
		m_adj(m, (frent->fe_hdrlen + frent->fe_len) - m->m_pkthdr.len);
	/* Magic from ip_input */
	m2 = m->m_next;
	m->m_next = NULL;
	m_cat(m, m2);
	pool_put(&pf_frent_pl, frent);
	pf_nfrents--;

	while ((frent = TAILQ_FIRST(&frag->fr_queue)) != NULL) {
		TAILQ_REMOVE(&frag->fr_queue, frent, fr_next);
		m2 = frent->fe_m;
		/* Strip off ip header */
		m_adj(m2, frent->fe_hdrlen);
		/* Strip off any trailing bytes */
		if (frent->fe_len < m2->m_pkthdr.len)
			m_adj(m2, frent->fe_len - m2->m_pkthdr.len);
		pool_put(&pf_frent_pl, frent);
		pf_nfrents--;
		m_cat(m, m2);
	}

	/* Remove from fragment queue */
	pf_free_fragment(frag);

	return (m);
}

int
pf_reassemble(struct mbuf **m0, int dir, u_short *reason)
{
	struct mbuf		*m = *m0;
	struct ip		*ip = mtod(m, struct ip *);
	struct pf_frent		*frent;
	struct pf_fragment	*frag;
	struct pf_fragment_cmp	 key;
	u_int16_t		 total, hdrlen;

	/* Get an entry for the fragment queue */
	if ((frent = pf_create_fragment(reason)) == NULL)
		return (PF_DROP);

	frent->fe_m = m;
	frent->fe_hdrlen = ip->ip_hl << 2;
	frent->fe_extoff = 0;
	frent->fe_len = ntohs(ip->ip_len) - (ip->ip_hl << 2);
	frent->fe_off = (ntohs(ip->ip_off) & IP_OFFMASK) << 3;
	frent->fe_mff = ntohs(ip->ip_off) & IP_MF;

	key.fr_src.v4 = ip->ip_src;
	key.fr_dst.v4 = ip->ip_dst;
	key.fr_af = AF_INET;
	key.fr_proto = ip->ip_p;
	key.fr_id = ip->ip_id;
	key.fr_direction = dir;

	PF_FRAG_LOCK();
	if ((frag = pf_fillup_fragment(&key, frent, reason)) == NULL) {
		PF_FRAG_UNLOCK();
		return (PF_DROP);
	}

	/* The mbuf is part of the fragment entry, no direct free or access */
	m = *m0 = NULL;

	if (!pf_isfull_fragment(frag)) {
		PF_FRAG_UNLOCK();
		return (PF_PASS);  /* drop because *m0 is NULL, no error */
	}

	/* We have all the data */
	frent = TAILQ_FIRST(&frag->fr_queue);
	KASSERT(frent != NULL);
	total = TAILQ_LAST(&frag->fr_queue, pf_fragq)->fe_off +
	    TAILQ_LAST(&frag->fr_queue, pf_fragq)->fe_len;
	hdrlen = frent->fe_hdrlen;
	m = *m0 = pf_join_fragment(frag);
	frag = NULL;

	if (m->m_flags & M_PKTHDR) {
		int plen = 0;
		for (m = *m0; m; m = m->m_next)
			plen += m->m_len;
		m = *m0;
		m->m_pkthdr.len = plen;
	}

	ip = mtod(m, struct ip *);
	ip->ip_len = htons(hdrlen + total);
	ip->ip_off &= ~(IP_MF|IP_OFFMASK);

	if (hdrlen + total > IP_MAXPACKET) {
		PF_FRAG_UNLOCK();
		DPFPRINTF(LOG_NOTICE, "drop: too big: %d", total);
		ip->ip_len = 0;
		REASON_SET(reason, PFRES_SHORT);
		/* PF_DROP requires a valid mbuf *m0 in pf_test() */
		return (PF_DROP);
	}

	PF_FRAG_UNLOCK();
	DPFPRINTF(LOG_INFO, "complete: %p(%d)", m, ntohs(ip->ip_len));
	return (PF_PASS);
}

#ifdef INET6
int
pf_reassemble6(struct mbuf **m0, struct ip6_frag *fraghdr,
    u_int16_t hdrlen, u_int16_t extoff, int dir, u_short *reason)
{
	struct mbuf		*m = *m0;
	struct ip6_hdr		*ip6 = mtod(m, struct ip6_hdr *);
	struct m_tag		*mtag;
	struct pf_fragment_tag	*ftag;
	struct pf_frent		*frent;
	struct pf_fragment	*frag;
	struct pf_fragment_cmp	 key;
	int			 off;
	u_int16_t		 total, maxlen;
	u_int8_t		 proto;

	/* Get an entry for the fragment queue */
	if ((frent = pf_create_fragment(reason)) == NULL)
		return (PF_DROP);

	frent->fe_m = m;
	frent->fe_hdrlen = hdrlen;
	frent->fe_extoff = extoff;
	frent->fe_len = sizeof(struct ip6_hdr) + ntohs(ip6->ip6_plen) - hdrlen;
	frent->fe_off = ntohs(fraghdr->ip6f_offlg & IP6F_OFF_MASK);
	frent->fe_mff = fraghdr->ip6f_offlg & IP6F_MORE_FRAG;

	key.fr_src.v6 = ip6->ip6_src;
	key.fr_dst.v6 = ip6->ip6_dst;
	key.fr_af = AF_INET6;
	/* Only the first fragment's protocol is relevant */
	key.fr_proto = 0;
	key.fr_id = fraghdr->ip6f_ident;
	key.fr_direction = dir;

	PF_FRAG_LOCK();
	if ((frag = pf_fillup_fragment(&key, frent, reason)) == NULL) {
		PF_FRAG_UNLOCK();
		return (PF_DROP);
	}

	/* The mbuf is part of the fragment entry, no direct free or access */
	m = *m0 = NULL;

	if (!pf_isfull_fragment(frag)) {
		PF_FRAG_UNLOCK();
		return (PF_PASS);  /* drop because *m0 is NULL, no error */
	}

	/* We have all the data */
	extoff = frent->fe_extoff;
	maxlen = frag->fr_maxlen;
	frent = TAILQ_FIRST(&frag->fr_queue);
	KASSERT(frent != NULL);
	total = TAILQ_LAST(&frag->fr_queue, pf_fragq)->fe_off +
	    TAILQ_LAST(&frag->fr_queue, pf_fragq)->fe_len;
	hdrlen = frent->fe_hdrlen - sizeof(struct ip6_frag);
	m = *m0 = pf_join_fragment(frag);
	frag = NULL;

	/* Take protocol from first fragment header */
	if ((m = m_getptr(m, hdrlen + offsetof(struct ip6_frag, ip6f_nxt),
	    &off)) == NULL)
		panic("%s: short frag mbuf chain", __func__);
	proto = *(mtod(m, caddr_t) + off);
	m = *m0;

	/* Delete frag6 header */
	if (frag6_deletefraghdr(m, hdrlen) != 0)
		goto fail;

	if (m->m_flags & M_PKTHDR) {
		int plen = 0;
		for (m = *m0; m; m = m->m_next)
			plen += m->m_len;
		m = *m0;
		m->m_pkthdr.len = plen;
	}

	if ((mtag = m_tag_get(PACKET_TAG_PF_REASSEMBLED, sizeof(struct
	    pf_fragment_tag), M_NOWAIT)) == NULL)
		goto fail;
	ftag = (struct pf_fragment_tag *)(mtag + 1);
	ftag->ft_hdrlen = hdrlen;
	ftag->ft_extoff = extoff;
	ftag->ft_maxlen = maxlen;
	m_tag_prepend(m, mtag);

	ip6 = mtod(m, struct ip6_hdr *);
	ip6->ip6_plen = htons(hdrlen - sizeof(struct ip6_hdr) + total);
	if (extoff) {
		/* Write protocol into next field of last extension header */
		if ((m = m_getptr(m, extoff + offsetof(struct ip6_ext,
		    ip6e_nxt), &off)) == NULL)
			panic("%s: short ext mbuf chain", __func__);
		*(mtod(m, caddr_t) + off) = proto;
		m = *m0;
	} else
		ip6->ip6_nxt = proto;

	if (hdrlen - sizeof(struct ip6_hdr) + total > IPV6_MAXPACKET) {
		PF_FRAG_UNLOCK();
		DPFPRINTF(LOG_NOTICE, "drop: too big: %d", total);
		ip6->ip6_plen = 0;
		REASON_SET(reason, PFRES_SHORT);
		/* PF_DROP requires a valid mbuf *m0 in pf_test6() */
		return (PF_DROP);
	}
	PF_FRAG_UNLOCK();

	DPFPRINTF(LOG_INFO, "complete: %p(%d)", m, ntohs(ip6->ip6_plen));
	return (PF_PASS);

fail:
	PF_FRAG_UNLOCK();
	REASON_SET(reason, PFRES_MEMORY);
	/* PF_DROP requires a valid mbuf *m0 in pf_test6(), will free later */
	return (PF_DROP);
}

int
pf_refragment6(struct mbuf **m0, struct m_tag *mtag, struct sockaddr_in6 *dst,
    struct ifnet *ifp, struct rtentry *rt)
{
	struct mbuf		*m = *m0, *t;
	struct pf_fragment_tag	*ftag = (struct pf_fragment_tag *)(mtag + 1);
	u_int32_t		 mtu;
	u_int16_t		 hdrlen, extoff, maxlen;
	u_int8_t		 proto;
	int			 error, action;

	hdrlen = ftag->ft_hdrlen;
	extoff = ftag->ft_extoff;
	maxlen = ftag->ft_maxlen;
	m_tag_delete(m, mtag);
	mtag = NULL;
	ftag = NULL;

	/* Checksum must be calculated for the whole packet */
	in6_proto_cksum_out(m, NULL);

	if (extoff) {
		int off;

		/* Use protocol from next field of last extension header */
		if ((m = m_getptr(m, extoff + offsetof(struct ip6_ext,
		    ip6e_nxt), &off)) == NULL)
			panic("%s: short ext mbuf chain", __func__);
		proto = *(mtod(m, caddr_t) + off);
		*(mtod(m, caddr_t) + off) = IPPROTO_FRAGMENT;
		m = *m0;
	} else {
		struct ip6_hdr *hdr;

		hdr = mtod(m, struct ip6_hdr *);
		proto = hdr->ip6_nxt;
		hdr->ip6_nxt = IPPROTO_FRAGMENT;
	}

	/*
	 * Maxlen may be less than 8 iff there was only a single
	 * fragment.  As it was fragmented before, add a fragment
	 * header also for a single fragment.  If total or maxlen
	 * is less than 8, ip6_fragment() will return EMSGSIZE and
	 * we drop the packet.
	 */
	mtu = hdrlen + sizeof(struct ip6_frag) + maxlen;
	error = ip6_fragment(m, hdrlen, proto, mtu);

	m = (*m0)->m_nextpkt;
	(*m0)->m_nextpkt = NULL;
	if (error == 0) {
		/* The first mbuf contains the unfragmented packet */
		m_freemp(m0);
		action = PF_PASS;
	} else {
		/* Drop expects an mbuf to free */
		DPFPRINTF(LOG_NOTICE, "refragment error %d", error);
		action = PF_DROP;
	}

	for (t = m; m; m = t) {
		t = m->m_nextpkt;
		m->m_nextpkt = NULL;
		m->m_pkthdr.pf.flags |= PF_TAG_REFRAGMENTED;
		if (error == 0) {
			if (ifp == NULL) {
				ip6_forward(m, NULL, 0);
			} else if ((u_long)m->m_pkthdr.len <= ifp->if_mtu) {
				ifp->if_output(ifp, m, sin6tosa(dst), rt);
			} else {
				icmp6_error(m, ICMP6_PACKET_TOO_BIG, 0,
				    ifp->if_mtu);
			}
		} else {
			m_freem(m);
		}
	}

	return (action);
}
#endif /* INET6 */

int
pf_normalize_ip(struct pf_pdesc *pd, u_short *reason)
{
	struct ip	*h = mtod(pd->m, struct ip *);
	u_int16_t	 fragoff = (ntohs(h->ip_off) & IP_OFFMASK) << 3;
	u_int16_t	 mff = (ntohs(h->ip_off) & IP_MF);

	if (!fragoff && !mff)
		goto no_fragment;

	/* Clear IP_DF if we're in no-df mode */
	if (pf_status.reass & PF_REASS_NODF && h->ip_off & htons(IP_DF))
		h->ip_off &= htons(~IP_DF);

	/* We're dealing with a fragment now. Don't allow fragments
	 * with IP_DF to enter the cache. If the flag was cleared by
	 * no-df above, fine. Otherwise drop it.
	 */
	if (h->ip_off & htons(IP_DF)) {
		DPFPRINTF(LOG_NOTICE, "bad fragment: IP_DF");
		REASON_SET(reason, PFRES_FRAG);
		return (PF_DROP);
	}

	if (!pf_status.reass)
		return (PF_PASS);	/* no reassembly */

	/* Returns PF_DROP or m is NULL or completely reassembled mbuf */
	if (pf_reassemble(&pd->m, pd->dir, reason) != PF_PASS)
		return (PF_DROP);
	if (pd->m == NULL)
		return (PF_PASS);  /* packet has been reassembled, no error */

	h = mtod(pd->m, struct ip *);

no_fragment:
	/* At this point, only IP_DF is allowed in ip_off */
	if (h->ip_off & ~htons(IP_DF))
		h->ip_off &= htons(IP_DF);

	return (PF_PASS);
}

#ifdef INET6
int
pf_normalize_ip6(struct pf_pdesc *pd, u_short *reason)
{
	struct ip6_frag		 frag;

	if (pd->fragoff == 0)
		goto no_fragment;

	if (!pf_pull_hdr(pd->m, pd->fragoff, &frag, sizeof(frag), NULL, reason,
	    AF_INET6))
		return (PF_DROP);

	if (!pf_status.reass)
		return (PF_PASS);	/* no reassembly */

	/* Returns PF_DROP or m is NULL or completely reassembled mbuf */
	if (pf_reassemble6(&pd->m, &frag, pd->fragoff + sizeof(frag),
	    pd->extoff, pd->dir, reason) != PF_PASS)
		return (PF_DROP);
	if (pd->m == NULL)
		return (PF_PASS);  /* packet has been reassembled, no error */

no_fragment:
	return (PF_PASS);
}
#endif /* INET6 */

int
pf_normalize_tcp(struct pf_pdesc *pd)
{
	struct tcphdr	*th = &pd->hdr.tcp;
	u_short		 reason;
	u_int8_t	 flags;
	u_int		 rewrite = 0;

	flags = th->th_flags;
	if (flags & TH_SYN) {
		/* Illegal packet */
		if (flags & TH_RST)
			goto tcp_drop;

		if (flags & TH_FIN)	/* XXX why clear instead of drop? */
			flags &= ~TH_FIN;
	} else {
		/* Illegal packet */
		if (!(flags & (TH_ACK|TH_RST)))
			goto tcp_drop;
	}

	if (!(flags & TH_ACK)) {
		/* These flags are only valid if ACK is set */
		if (flags & (TH_FIN|TH_PUSH|TH_URG))
			goto tcp_drop;
	}

	/* If flags changed, or reserved data set, then adjust */
	if (flags != th->th_flags || th->th_x2 != 0) {
		/* hack: set 4-bit th_x2 = 0 */
		u_int8_t *th_off = (u_int8_t*)(&th->th_ack+1);
		pf_patch_8(pd, th_off, th->th_off << 4, PF_HI);

		pf_patch_8(pd, &th->th_flags, flags, PF_LO);
		rewrite = 1;
	}

	/* Remove urgent pointer, if TH_URG is not set */
	if (!(flags & TH_URG) && th->th_urp) {
		pf_patch_16(pd, &th->th_urp, 0);
		rewrite = 1;
	}

	/* copy back packet headers if we sanitized */
	if (rewrite) {
		m_copyback(pd->m, pd->off, sizeof(*th), th, M_NOWAIT);
	}

	return (PF_PASS);

tcp_drop:
	REASON_SET(&reason, PFRES_NORM);
	return (PF_DROP);
}

int
pf_normalize_tcp_init(struct pf_pdesc *pd, struct pf_state_peer *src)
{
	struct tcphdr	*th = &pd->hdr.tcp;
	u_int32_t	 tsval, tsecr;
	u_int8_t	 hdr[60];
	u_int8_t	*opt;

	KASSERT(src->scrub == NULL);

	src->scrub = pool_get(&pf_state_scrub_pl, PR_NOWAIT);
	if (src->scrub == NULL)
		return (1);
	bzero(src->scrub, sizeof(*src->scrub));

	switch (pd->af) {
	case AF_INET: {
		struct ip *h = mtod(pd->m, struct ip *);
		src->scrub->pfss_ttl = h->ip_ttl;
		break;
	}
#ifdef INET6
	case AF_INET6: {
		struct ip6_hdr *h = mtod(pd->m, struct ip6_hdr *);
		src->scrub->pfss_ttl = h->ip6_hlim;
		break;
	}
#endif /* INET6 */
	default:
		unhandled_af(pd->af);
	}

	/*
	 * All normalizations below are only begun if we see the start of
	 * the connections.  They must all set an enabled bit in pfss_flags
	 */
	if ((th->th_flags & TH_SYN) == 0)
		return (0);

	if (th->th_off > (sizeof(struct tcphdr) >> 2) && src->scrub &&
	    pf_pull_hdr(pd->m, pd->off, hdr, th->th_off << 2, NULL, NULL,
	    pd->af)) {
		/* Diddle with TCP options */
		int	hlen;

		opt = hdr + sizeof(struct tcphdr);
		hlen = (th->th_off << 2) - sizeof(struct tcphdr);
		while (hlen >= TCPOLEN_TIMESTAMP) {
			switch (*opt) {
			case TCPOPT_EOL:	/* FALLTHROUGH */
			case TCPOPT_NOP:
				opt++;
				hlen--;
				break;
			case TCPOPT_TIMESTAMP:
				if (opt[1] >= TCPOLEN_TIMESTAMP) {
					src->scrub->pfss_flags |=
					    PFSS_TIMESTAMP;
					src->scrub->pfss_ts_mod = arc4random();

					/* note PFSS_PAWS not set yet */
					memcpy(&tsval, &opt[2],
					    sizeof(u_int32_t));
					memcpy(&tsecr, &opt[6],
					    sizeof(u_int32_t));
					src->scrub->pfss_tsval0 = ntohl(tsval);
					src->scrub->pfss_tsval = ntohl(tsval);
					src->scrub->pfss_tsecr = ntohl(tsecr);
					getmicrouptime(&src->scrub->pfss_last);
				}
				/* FALLTHROUGH */
			default:
				hlen -= MAX(opt[1], 2);
				opt += MAX(opt[1], 2);
				break;
			}
		}
	}

	return (0);
}

void
pf_normalize_tcp_cleanup(struct pf_state *state)
{
	if (state->src.scrub)
		pool_put(&pf_state_scrub_pl, state->src.scrub);
	if (state->dst.scrub)
		pool_put(&pf_state_scrub_pl, state->dst.scrub);

	/* Someday... flush the TCP segment reassembly descriptors. */
}

int
pf_normalize_tcp_stateful(struct pf_pdesc *pd, u_short *reason,
    struct pf_state *state, struct pf_state_peer *src,
    struct pf_state_peer *dst, int *writeback)
{
	struct tcphdr	*th = &pd->hdr.tcp;
	struct timeval	 uptime;
	u_int32_t	 tsval, tsecr;
	u_int		 tsval_from_last;
	u_int8_t	 hdr[60];
	u_int8_t	*opts, *opt;
	int		 copyback = 0;
	int		 got_ts = 0;

	KASSERT(src->scrub || dst->scrub);

	/*
	 * Enforce the minimum TTL seen for this connection.  Negate a common
	 * technique to evade an intrusion detection system and confuse
	 * firewall state code.
	 */
	switch (pd->af) {
	case AF_INET:
		if (src->scrub) {
			struct ip *h = mtod(pd->m, struct ip *);
			if (h->ip_ttl > src->scrub->pfss_ttl)
				src->scrub->pfss_ttl = h->ip_ttl;
			h->ip_ttl = src->scrub->pfss_ttl;
		}
		break;
#ifdef INET6
	case AF_INET6:
		if (src->scrub) {
			struct ip6_hdr *h = mtod(pd->m, struct ip6_hdr *);
			if (h->ip6_hlim > src->scrub->pfss_ttl)
				src->scrub->pfss_ttl = h->ip6_hlim;
			h->ip6_hlim = src->scrub->pfss_ttl;
		}
		break;
#endif /* INET6 */
	default:
		unhandled_af(pd->af);
	}

	if (th->th_off > (sizeof(struct tcphdr) >> 2) &&
	    ((src->scrub && (src->scrub->pfss_flags & PFSS_TIMESTAMP)) ||
	    (dst->scrub && (dst->scrub->pfss_flags & PFSS_TIMESTAMP))) &&
	    pf_pull_hdr(pd->m, pd->off, hdr, th->th_off << 2, NULL, NULL,
	    pd->af)) {
		/* Diddle with TCP options */
		int hlen;
		opt = opts = hdr + sizeof(struct tcphdr);
		hlen = (th->th_off << 2) - sizeof(struct tcphdr);
		while (hlen >= TCPOLEN_TIMESTAMP) {
			switch (*opt) {
			case TCPOPT_EOL:	/* FALLTHROUGH */
			case TCPOPT_NOP:
				opt++;
				hlen--;
				break;
			case TCPOPT_TIMESTAMP:
				/* Modulate the timestamps.  Can be used for
				 * NAT detection, OS uptime determination or
				 * reboot detection.
				 */

				if (got_ts) {
					/* Huh?  Multiple timestamps!? */
					if (pf_status.debug >= LOG_NOTICE) {
						log(LOG_NOTICE,
						    "pf: %s: multiple TS??",
						    __func__);
						pf_print_state(state);
						addlog("\n");
					}
					REASON_SET(reason, PFRES_TS);
					return (PF_DROP);
				}
				if (opt[1] >= TCPOLEN_TIMESTAMP) {
					u_int8_t *ts = opt + 2;
					u_int8_t *tsr = opt + 6;

					memcpy(&tsval, ts, sizeof(u_int32_t));
					memcpy(&tsecr, tsr, sizeof(u_int32_t));

					/* modulate TS */
					if (tsval && src->scrub &&
					    (src->scrub->pfss_flags &
					    PFSS_TIMESTAMP)) {
						/* tsval used further on */
						tsval = ntohl(tsval);
						pf_patch_32_unaligned(pd, ts,
						    htonl(tsval +
						    src->scrub->pfss_ts_mod),
						    PF_ALGNMNT(ts - opts));
						copyback = 1;
					}

					/* modulate TS reply if any (!0) */
					if (tsecr && dst->scrub &&
					    (dst->scrub->pfss_flags &
					    PFSS_TIMESTAMP)) {
						/* tsecr used further on */
						tsecr = ntohl(tsecr)
						    - dst->scrub->pfss_ts_mod;
						pf_patch_32_unaligned(pd, tsr,
						    htonl(tsecr),
						    PF_ALGNMNT(tsr - opts));
						copyback = 1;
					}
					got_ts = 1;
				}
				/* FALLTHROUGH */
			default:
				hlen -= MAX(opt[1], 2);
				opt += MAX(opt[1], 2);
				break;
			}
		}
		if (copyback) {
			/* Copyback the options, caller copys back header */
			*writeback = 1;
			m_copyback(pd->m, pd->off + sizeof(struct tcphdr),
			    (th->th_off << 2) - sizeof(struct tcphdr), hdr +
			    sizeof(struct tcphdr), M_NOWAIT);
		}
	}


	/*
	 * Must invalidate PAWS checks on connections idle for too long.
	 * The fastest allowed timestamp clock is 1ms.  That turns out to
	 * be about 24 days before it wraps.  XXX Right now our lowerbound
	 * TS echo check only works for the first 12 days of a connection
	 * when the TS has exhausted half its 32bit space
	 */
#define TS_MAX_IDLE	(24*24*60*60)
#define TS_MAX_CONN	(12*24*60*60)	/* XXX remove when better tsecr check */

	getmicrouptime(&uptime);
	if (src->scrub && (src->scrub->pfss_flags & PFSS_PAWS) &&
	    (uptime.tv_sec - src->scrub->pfss_last.tv_sec > TS_MAX_IDLE ||
	    time_uptime - state->creation > TS_MAX_CONN))  {
		if (pf_status.debug >= LOG_NOTICE) {
			log(LOG_NOTICE, "pf: src idled out of PAWS ");
			pf_print_state(state);
			addlog("\n");
		}
		src->scrub->pfss_flags =
		    (src->scrub->pfss_flags & ~PFSS_PAWS) | PFSS_PAWS_IDLED;
	}
	if (dst->scrub && (dst->scrub->pfss_flags & PFSS_PAWS) &&
	    uptime.tv_sec - dst->scrub->pfss_last.tv_sec > TS_MAX_IDLE) {
		if (pf_status.debug >= LOG_NOTICE) {
			log(LOG_NOTICE, "pf: dst idled out of PAWS ");
			pf_print_state(state);
			addlog("\n");
		}
		dst->scrub->pfss_flags =
		    (dst->scrub->pfss_flags & ~PFSS_PAWS) | PFSS_PAWS_IDLED;
	}

	if (got_ts && src->scrub && dst->scrub &&
	    (src->scrub->pfss_flags & PFSS_PAWS) &&
	    (dst->scrub->pfss_flags & PFSS_PAWS)) {
		/* Validate that the timestamps are "in-window".
		 * RFC1323 describes TCP Timestamp options that allow
		 * measurement of RTT (round trip time) and PAWS
		 * (protection against wrapped sequence numbers).  PAWS
		 * gives us a set of rules for rejecting packets on
		 * long fat pipes (packets that were somehow delayed
		 * in transit longer than the time it took to send the
		 * full TCP sequence space of 4Gb).  We can use these
		 * rules and infer a few others that will let us treat
		 * the 32bit timestamp and the 32bit echoed timestamp
		 * as sequence numbers to prevent a blind attacker from
		 * inserting packets into a connection.
		 *
		 * RFC1323 tells us:
		 *  - The timestamp on this packet must be greater than
		 *    or equal to the last value echoed by the other
		 *    endpoint.  The RFC says those will be discarded
		 *    since it is a dup that has already been acked.
		 *    This gives us a lowerbound on the timestamp.
		 *        timestamp >= other last echoed timestamp
		 *  - The timestamp will be less than or equal to
		 *    the last timestamp plus the time between the
		 *    last packet and now.  The RFC defines the max
		 *    clock rate as 1ms.  We will allow clocks to be
		 *    up to 10% fast and will allow a total difference
		 *    or 30 seconds due to a route change.  And this
		 *    gives us an upperbound on the timestamp.
		 *        timestamp <= last timestamp + max ticks
		 *    We have to be careful here.  Windows will send an
		 *    initial timestamp of zero and then initialize it
		 *    to a random value after the 3whs; presumably to
		 *    avoid a DoS by having to call an expensive RNG
		 *    during a SYN flood.  Proof MS has at least one
		 *    good security geek.
		 *
		 *  - The TCP timestamp option must also echo the other
		 *    endpoints timestamp.  The timestamp echoed is the
		 *    one carried on the earliest unacknowledged segment
		 *    on the left edge of the sequence window.  The RFC
		 *    states that the host will reject any echoed
		 *    timestamps that were larger than any ever sent.
		 *    This gives us an upperbound on the TS echo.
		 *        tescr <= largest_tsval
		 *  - The lowerbound on the TS echo is a little more
		 *    tricky to determine.  The other endpoint's echoed
		 *    values will not decrease.  But there may be
		 *    network conditions that re-order packets and
		 *    cause our view of them to decrease.  For now the
		 *    only lowerbound we can safely determine is that
		 *    the TS echo will never be less than the original
		 *    TS.  XXX There is probably a better lowerbound.
		 *    Remove TS_MAX_CONN with better lowerbound check.
		 *        tescr >= other original TS
		 *
		 * It is also important to note that the fastest
		 * timestamp clock of 1ms will wrap its 32bit space in
		 * 24 days.  So we just disable TS checking after 24
		 * days of idle time.  We actually must use a 12d
		 * connection limit until we can come up with a better
		 * lowerbound to the TS echo check.
		 */
		struct timeval	delta_ts;
		int		ts_fudge;

		/*
		 * PFTM_TS_DIFF is how many seconds of leeway to allow
		 * a host's timestamp.  This can happen if the previous
		 * packet got delayed in transit for much longer than
		 * this packet.
		 */
		if ((ts_fudge = state->rule.ptr->timeout[PFTM_TS_DIFF]) == 0)
			ts_fudge = pf_default_rule.timeout[PFTM_TS_DIFF];

		/* Calculate max ticks since the last timestamp */
#define TS_MAXFREQ	1100		/* RFC max TS freq of 1Khz + 10% skew */
#define TS_MICROSECS	1000000		/* microseconds per second */
		timersub(&uptime, &src->scrub->pfss_last, &delta_ts);
		tsval_from_last = (delta_ts.tv_sec + ts_fudge) * TS_MAXFREQ;
		tsval_from_last += delta_ts.tv_usec / (TS_MICROSECS/TS_MAXFREQ);

		if ((src->state >= TCPS_ESTABLISHED &&
		    dst->state >= TCPS_ESTABLISHED) &&
		    (SEQ_LT(tsval, dst->scrub->pfss_tsecr) ||
		    SEQ_GT(tsval, src->scrub->pfss_tsval + tsval_from_last) ||
		    (tsecr && (SEQ_GT(tsecr, dst->scrub->pfss_tsval) ||
		    SEQ_LT(tsecr, dst->scrub->pfss_tsval0))))) {
			/* Bad RFC1323 implementation or an insertion attack.
			 *
			 * - Solaris 2.6 and 2.7 are known to send another ACK
			 *   after the FIN,FIN|ACK,ACK closing that carries
			 *   an old timestamp.
			 */

			DPFPRINTF(LOG_NOTICE, "Timestamp failed %c%c%c%c",
			    SEQ_LT(tsval, dst->scrub->pfss_tsecr) ? '0' : ' ',
			    SEQ_GT(tsval, src->scrub->pfss_tsval +
			    tsval_from_last) ? '1' : ' ',
			    SEQ_GT(tsecr, dst->scrub->pfss_tsval) ? '2' : ' ',
			    SEQ_LT(tsecr, dst->scrub->pfss_tsval0)? '3' : ' ');
			DPFPRINTF(LOG_NOTICE, " tsval: %u  tsecr: %u  "
			    "+ticks: %u  idle: %llu.%06lus", tsval, tsecr,
			    tsval_from_last, (long long)delta_ts.tv_sec,
			    delta_ts.tv_usec);
			DPFPRINTF(LOG_NOTICE, " src->tsval: %u  tsecr: %u",
			    src->scrub->pfss_tsval, src->scrub->pfss_tsecr);
			DPFPRINTF(LOG_NOTICE, " dst->tsval: %u  tsecr: %u  "
			    "tsval0: %u", dst->scrub->pfss_tsval,
			    dst->scrub->pfss_tsecr, dst->scrub->pfss_tsval0);
			if (pf_status.debug >= LOG_NOTICE) {
				log(LOG_NOTICE, "pf: ");
				pf_print_state(state);
				pf_print_flags(th->th_flags);
				addlog("\n");
			}
			REASON_SET(reason, PFRES_TS);
			return (PF_DROP);
		}
		/* XXX I'd really like to require tsecr but it's optional */
	} else if (!got_ts && (th->th_flags & TH_RST) == 0 &&
	    ((src->state == TCPS_ESTABLISHED && dst->state == TCPS_ESTABLISHED)
	    || pd->p_len > 0 || (th->th_flags & TH_SYN)) &&
	    src->scrub && dst->scrub &&
	    (src->scrub->pfss_flags & PFSS_PAWS) &&
	    (dst->scrub->pfss_flags & PFSS_PAWS)) {
		/* Didn't send a timestamp.  Timestamps aren't really useful
		 * when:
		 *  - connection opening or closing (often not even sent).
		 *    but we must not let an attacker to put a FIN on a
		 *    data packet to sneak it through our ESTABLISHED check.
		 *  - on a TCP reset.  RFC suggests not even looking at TS.
		 *  - on an empty ACK.  The TS will not be echoed so it will
		 *    probably not help keep the RTT calculation in sync and
		 *    there isn't as much danger when the sequence numbers
		 *    got wrapped.  So some stacks don't include TS on empty
		 *    ACKs :-(
		 *
		 * To minimize the disruption to mostly RFC1323 conformant
		 * stacks, we will only require timestamps on data packets.
		 *
		 * And what do ya know, we cannot require timestamps on data
		 * packets.  There appear to be devices that do legitimate
		 * TCP connection hijacking.  There are HTTP devices that allow
		 * a 3whs (with timestamps) and then buffer the HTTP request.
		 * If the intermediate device has the HTTP response cache, it
		 * will spoof the response but not bother timestamping its
		 * packets.  So we can look for the presence of a timestamp in
		 * the first data packet and if there, require it in all future
		 * packets.
		 */

		if (pd->p_len > 0 && (src->scrub->pfss_flags & PFSS_DATA_TS)) {
			/*
			 * Hey!  Someone tried to sneak a packet in.  Or the
			 * stack changed its RFC1323 behavior?!?!
			 */
			if (pf_status.debug >= LOG_NOTICE) {
				log(LOG_NOTICE,
				    "pf: did not receive expected RFC1323 "
				    "timestamp");
				pf_print_state(state);
				pf_print_flags(th->th_flags);
				addlog("\n");
			}
			REASON_SET(reason, PFRES_TS);
			return (PF_DROP);
		}
	}

	/*
	 * We will note if a host sends his data packets with or without
	 * timestamps.  And require all data packets to contain a timestamp
	 * if the first does.  PAWS implicitly requires that all data packets be
	 * timestamped.  But I think there are middle-man devices that hijack
	 * TCP streams immediately after the 3whs and don't timestamp their
	 * packets (seen in a WWW accelerator or cache).
	 */
	if (pd->p_len > 0 && src->scrub && (src->scrub->pfss_flags &
	    (PFSS_TIMESTAMP|PFSS_DATA_TS|PFSS_DATA_NOTS)) == PFSS_TIMESTAMP) {
		if (got_ts)
			src->scrub->pfss_flags |= PFSS_DATA_TS;
		else {
			src->scrub->pfss_flags |= PFSS_DATA_NOTS;
			if (pf_status.debug >= LOG_NOTICE && dst->scrub &&
			    (dst->scrub->pfss_flags & PFSS_TIMESTAMP)) {
				/* Don't warn if other host rejected RFC1323 */
				log(LOG_NOTICE,
				    "pf: broken RFC1323 stack did not "
				    "timestamp data packet. Disabled PAWS "
				    "security.");
				pf_print_state(state);
				pf_print_flags(th->th_flags);
				addlog("\n");
			}
		}
	}

	/*
	 * Update PAWS values
	 */
	if (got_ts && src->scrub && PFSS_TIMESTAMP == (src->scrub->pfss_flags &
	    (PFSS_PAWS_IDLED|PFSS_TIMESTAMP))) {
		getmicrouptime(&src->scrub->pfss_last);
		if (SEQ_GEQ(tsval, src->scrub->pfss_tsval) ||
		    (src->scrub->pfss_flags & PFSS_PAWS) == 0)
			src->scrub->pfss_tsval = tsval;

		if (tsecr) {
			if (SEQ_GEQ(tsecr, src->scrub->pfss_tsecr) ||
			    (src->scrub->pfss_flags & PFSS_PAWS) == 0)
				src->scrub->pfss_tsecr = tsecr;

			if ((src->scrub->pfss_flags & PFSS_PAWS) == 0 &&
			    (SEQ_LT(tsval, src->scrub->pfss_tsval0) ||
			    src->scrub->pfss_tsval0 == 0)) {
				/* tsval0 MUST be the lowest timestamp */
				src->scrub->pfss_tsval0 = tsval;
			}

			/* Only fully initialized after a TS gets echoed */
			if ((src->scrub->pfss_flags & PFSS_PAWS) == 0)
				src->scrub->pfss_flags |= PFSS_PAWS;
		}
	}

	/* I have a dream....  TCP segment reassembly.... */
	return (0);
}

int
pf_normalize_mss(struct pf_pdesc *pd, u_int16_t maxmss)
{
	struct tcphdr	*th = &pd->hdr.tcp;
	u_int16_t	 mss;
	int		 thoff;
	int		 opt, cnt, optlen = 0;
	u_int8_t	 opts[MAX_TCPOPTLEN];
	u_int8_t	*optp = opts;

	thoff = th->th_off << 2;
	cnt = thoff - sizeof(struct tcphdr);

	if (cnt <= 0 || cnt > MAX_TCPOPTLEN || !pf_pull_hdr(pd->m,
	    pd->off + sizeof(*th), opts, cnt, NULL, NULL, pd->af))
		return (0);

	for (; cnt > 0; cnt -= optlen, optp += optlen) {
		opt = optp[0];
		if (opt == TCPOPT_EOL)
			break;
		if (opt == TCPOPT_NOP)
			optlen = 1;
		else {
			if (cnt < 2)
				break;
			optlen = optp[1];
			if (optlen < 2 || optlen > cnt)
				break;
		}
		if (opt == TCPOPT_MAXSEG) {
			u_int8_t *mssp = optp + 2;
			memcpy(&mss, mssp, sizeof(mss));
			if (ntohs(mss) > maxmss) {
				size_t mssoffopts = mssp - opts;
				pf_patch_16_unaligned(pd, &mss,
				    htons(maxmss), PF_ALGNMNT(mssoffopts));
				m_copyback(pd->m,
				    pd->off + sizeof(*th) + mssoffopts,
				    sizeof(mss), &mss, M_NOWAIT);
				m_copyback(pd->m, pd->off, sizeof(*th), th,
				    M_NOWAIT);
			}
		}
	}

	return (0);
}

void
pf_scrub(struct mbuf *m, u_int16_t flags, sa_family_t af, u_int8_t min_ttl,
    u_int8_t tos)
{
	struct ip		*h = mtod(m, struct ip *);
#ifdef INET6
	struct ip6_hdr		*h6 = mtod(m, struct ip6_hdr *);
#endif	/* INET6 */

	/* Clear IP_DF if no-df was requested */
	if (flags & PFSTATE_NODF && af == AF_INET && h->ip_off & htons(IP_DF))
		h->ip_off &= htons(~IP_DF);

	/* Enforce a minimum ttl, may cause endless packet loops */
	if (min_ttl && af == AF_INET && h->ip_ttl < min_ttl)
		h->ip_ttl = min_ttl;
#ifdef INET6
	if (min_ttl && af == AF_INET6 && h6->ip6_hlim < min_ttl)
		h6->ip6_hlim = min_ttl;
#endif	/* INET6 */

	/* Enforce tos */
	if (flags & PFSTATE_SETTOS) {
		if (af == AF_INET)
			h->ip_tos = tos | (h->ip_tos & IPTOS_ECN_MASK);
#ifdef INET6
		if (af == AF_INET6) {
			/* drugs are unable to explain such idiocy */
			h6->ip6_flow &= ~htonl(0x0fc00000);
			h6->ip6_flow |= htonl(((u_int32_t)tos) << 20);
		}
#endif	/* INET6 */
	}

	/* random-id, but not for fragments */
	if (flags & PFSTATE_RANDOMID && af == AF_INET &&
	    !(h->ip_off & ~htons(IP_DF)))
		h->ip_id = htons(ip_randomid());
}
@


1.205
log
@- let's add PF_LOCK()
  to enable PF_LOCK(), you must add 'option WITH_PF_LOCK' to your kernel
  configuration. The code does not do much currently it's just the very
  small step towards MP.

O.K. henning@@, mikeb@@, mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.204 2017/05/15 12:26:00 mpi Exp $ */
d776 1
a776 2
		m_freem(*m0);
		*m0 = NULL;
@


1.204
log
@Enable the NET_LOCK(), take 3.

Recursions are still marked as XXXSMP.

ok deraadt@@, bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.203 2017/04/23 11:37:11 sthen Exp $ */
d42 1
d139 12
d165 2
d194 1
a194 1
	NET_ASSERT_LOCKED();
d197 2
d205 1
d554 3
a556 1
	if ((frag = pf_fillup_fragment(&key, frent, reason)) == NULL)
d558 1
d563 2
a564 1
	if (!pf_isfull_fragment(frag))
d566 1
d590 1
d598 1
d638 3
a640 1
	if ((frag = pf_fillup_fragment(&key, frent, reason)) == NULL)
d642 1
d647 2
a648 1
	if (!pf_isfull_fragment(frag))
d650 1
d704 1
d711 1
d717 1
@


1.203
log
@Some of the LOG_NOTICE messages from PF were seen in normal operations
with certain rulesets and excessively noisy; move them to LOG_INFO (which was
previously unused).  ok benno@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.202 2017/03/17 17:19:16 mpi Exp $ */
d178 2
@


1.202
log
@Revert the NET_LOCK() and bring back pf's contention lock for release.

For the moment the NET_LOCK() is always taken by threads running under
KERNEL_LOCK().  That means it doesn't buy us anything except a possible
deadlock that we did not spot.  So make sure this doesn't happen, we'll
have plenty of time in the next release cycle to stress test it.

ok visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.201 2017/01/30 17:41:34 benno Exp $ */
d289 1
a289 1
	DPFPRINTF(LOG_NOTICE, key->fr_af == AF_INET ?
d459 1
a459 1
	DPFPRINTF(LOG_NOTICE, "%d < %d?", off, total);
d572 1
a572 1
	DPFPRINTF(LOG_NOTICE, "complete: %p(%d)", m, ntohs(ip->ip_len));
d679 1
a679 1
	DPFPRINTF(LOG_NOTICE, "complete: %p(%d)", m, ntohs(ip6->ip6_plen));
@


1.201
log
@removes the pf_consistency_lock and protects the users with
NET_LOCK().  pfioctl() will need the NET_LOCK() anyway. So better keep
things simple until we're going to redesign PF for a MP world.
fixes the crash reported by Kaya Saman.
ok mpi@@, bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.200 2016/12/29 13:01:48 bluhm Exp $ */
a177 2

	NET_ASSERT_LOCKED();
@


1.200
log
@In pf_refragment6() use the valid route from pf_route6() instead
of calling rtalloc() again.
OK mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.199 2016/12/29 00:26:48 bluhm Exp $ */
d178 2
@


1.199
log
@Use __func__ instead of explicit function name in panic messages.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.198 2016/12/28 23:58:20 bluhm Exp $ */
d690 1
a690 1
    struct ifnet *ifp)
a693 1
	struct rtentry		*rt = NULL;
a749 9
	if (ifp != NULL) {
		rt = rtalloc(sin6tosa(dst), RT_RESOLVE,
		    m->m_pkthdr.ph_rtableid);
		if (rt == NULL) {
			ip6stat.ip6s_noroute++;
			error = -1;
		}
	}

a766 1
	rtfree(rt);
@


1.198
log
@Fix white spaces.  No binary change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.197 2016/11/22 19:29:54 procter Exp $ */
d634 1
a634 1
		panic("pf_reassemble6: short mbuf chain");
d665 1
a665 1
			panic("pf_reassemble6: short mbuf chain");
d716 1
a716 1
			panic("pf_refragment6: short mbuf chain");
@


1.197
log
@Fold union pf_headers buffer into struct pf_pdesc (enabled by pfvar_priv.h).
Prevent pf_socket_lookup() reading uninitialised header buffers on fragments.
OK blum@@ sashan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.196 2016/11/21 17:52:20 bluhm Exp $ */
d883 2
a884 2
 	if (flags != th->th_flags || th->th_x2 != 0) {
 		/* hack: set 4-bit th_x2 = 0 */
d886 1
a886 1
 		pf_patch_8(pd, th_off, th->th_off << 4, PF_HI);
d888 3
a890 3
 		pf_patch_8(pd, &th->th_flags, flags, PF_LO);
 		rewrite = 1;
 	}
d1092 1
a1092 1
						/* note: tsval used further on */
d1096 1
a1096 1
							src->scrub->pfss_ts_mod),
d1105 1
a1105 1
						/* note: tsecr used further on */
@


1.196
log
@Follow RFC 5722 more strictly when handling overlapping fragments
in pf.  Drop the whole fragment state if IPv6 fragments appear which
have invalid length or fragment-offset or more-fragment-bit.  In
IPv4 they are considered invalid and just dropped like before.
Found by Antonios Atlasis; OK sashan@@ sthen@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.195 2016/10/26 21:07:22 bluhm Exp $ */
d857 1
a857 1
	struct tcphdr	*th = pd->hdr.tcp;
d913 1
a913 1
	struct tcphdr	*th = pd->hdr.tcp;
d1008 1
a1008 1
	struct tcphdr	*th = pd->hdr.tcp;
d1399 1
a1399 1
	struct tcphdr	*th = pd->hdr.tcp;
@


1.195
log
@Put union pf_headers and struct pf_pdesc into separate header file
pfvar_priv.h.  The pf_headers had to be defined in multiple .c files
before.  In pfvar.h it would have unknown storage size, this file
is included in too many places.  The idea is to have a private pf
header that is only included in the pf part of the kernel.  For now
it contains pf_pdesc and pf_headers, it may be extended later.
discussion, input and OK henning@@ procter@@ sashan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.194 2016/09/27 04:57:17 dlg Exp $ */
d334 1
a334 1
		goto bad_fragment;
d340 1
a340 1
			goto bad_fragment;
d343 1
a343 1
			goto bad_fragment;
d409 1
d411 2
@


1.194
log
@roll back turning RB into RBT until i get better at this process.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.193 2016/09/27 02:51:12 dlg Exp $ */
d43 4
d50 1
a54 5
#include <netinet/ip_icmp.h>

#include <net/if.h>
#include <net/if_var.h>
#include <net/if_pflog.h>
d57 1
d60 1
a60 1
#include <netinet6/in6_var.h>
a61 1
#include <netinet/icmp6.h>
d65 1
@


1.193
log
@move pf from the RB macros to the RBT functions.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.192 2016/09/15 02:00:18 dlg Exp $ */
d77 1
a77 1
/* keep synced with struct pf_fragment, used in RBT_FIND */
d95 1
a95 1
	RBT_ENTRY(pf_fragment) fr_entry;
d110 5
a114 5
static __inline int	 pf_frag_compare(const struct pf_fragment *,
			    const struct pf_fragment *);
RBT_HEAD(pf_frag_tree, pf_fragment)	pf_frag_tree, pf_cache_tree;
RBT_PROTOTYPE(pf_frag_tree, pf_fragment, fr_entry, pf_frag_compare);
RBT_GENERATE(pf_frag_tree, pf_fragment, fr_entry, pf_frag_compare);
d154 1
a154 1
pf_frag_compare(const struct pf_fragment *a, const struct pf_fragment *b)
d214 1
a214 1
	RBT_REMOVE(pf_frag_tree, &pf_frag_tree, frag);
d232 1
a232 1
	frag = RBT_FIND(pf_frag_tree, tree, (struct pf_fragment *)key);
d312 1
a312 1
		RBT_INSERT(pf_frag_tree, &pf_frag_tree, frag);
@


1.192
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.191 2016/09/02 10:19:49 dlg Exp $ */
d77 1
a77 1
/* keep synced with struct pf_fragment, used in RB_FIND */
d95 1
a95 1
	RB_ENTRY(pf_fragment) fr_entry;
d110 5
a114 5
static __inline int	 pf_frag_compare(struct pf_fragment *,
			    struct pf_fragment *);
RB_HEAD(pf_frag_tree, pf_fragment)	pf_frag_tree, pf_cache_tree;
RB_PROTOTYPE(pf_frag_tree, pf_fragment, fr_entry, pf_frag_compare);
RB_GENERATE(pf_frag_tree, pf_fragment, fr_entry, pf_frag_compare);
d154 1
a154 1
pf_frag_compare(struct pf_fragment *a, struct pf_fragment *b)
d214 1
a214 1
	RB_REMOVE(pf_frag_tree, &pf_frag_tree, frag);
d232 1
a232 1
	frag = RB_FIND(pf_frag_tree, tree, (struct pf_fragment *)key);
d312 1
a312 1
		RB_INSERT(pf_frag_tree, &pf_frag_tree, frag);
@


1.191
log
@pool_setipl for pf bits

ok phessler@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.190 2016/08/24 09:41:12 mpi Exp $ */
d140 6
a145 9
	pool_init(&pf_frent_pl, sizeof(struct pf_frent), 0, 0, 0, "pffrent",
	    NULL);
	pool_setipl(&pf_frent_pl, IPL_SOFTNET);
	pool_init(&pf_frag_pl, sizeof(struct pf_fragment), 0, 0, 0, "pffrag",
	    NULL);
	pool_setipl(&pf_frag_pl, IPL_SOFTNET);
	pool_init(&pf_state_scrub_pl, sizeof(struct pf_state_scrub), 0, 0, 0,
	    "pfstscr", NULL);
	pool_setipl(&pf_state_scrub_pl, IPL_SOFTNET);
@


1.190
log
@Kill ip6_forward_rt reducing differences between v4 and v6.

A single forwarding cache is not the answer.  The answer is 42... err PF!

ok bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.189 2016/08/17 03:24:12 procter Exp $ */
d142 1
d145 1
d148 1
@


1.189
log
@Reintroduce 5.3-style checksum modification to preserve end-to-end checksums
when fiddling with packets but without the mess that motivated Henning to
remove it. Affects only this one aspect of Henning's checksum work. Also tweak
the basic algorithm and supply a correctness argument.

OK dlg@@ deraadt@@ sthen@@; no objection henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.188 2016/06/15 11:49:34 mpi Exp $ */
d762 1
a762 1
				ip6_forward(m, 0);
@


1.188
log
@Kill nd6_output(), it doesn't do anything since the resolution logic
has been moved to nd6_resolve().

ok visa@@, millert@@, florian@@, sthen@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.187 2016/06/15 11:36:06 mikeb Exp $ */
a857 4
	if (pd->csum_status == PF_CSUM_UNKNOWN)
		pf_check_proto_cksum(pd, pd->off, pd->tot_len - pd->off,
		    pd->proto, pd->af);

d879 8
a886 5
	if (flags != th->th_flags || th->th_x2 != 0) {
		th->th_flags = flags;
		th->th_x2 = 0;
		rewrite = 1;
	}
d890 1
a890 1
		th->th_urp = 0;
a895 1
		pf_cksum(pd, pd->m);
d1009 1
a1009 1
	u_int8_t	*opt;
d1050 1
a1050 1
		opt = hdr + sizeof(struct tcphdr);
d1078 7
a1084 2
					memcpy(&tsval, &opt[2],
					    sizeof(u_int32_t));
d1088 1
d1090 1
a1090 1
						pf_change_a(pd, &opt[2],
d1092 2
a1093 1
						    src->scrub->pfss_ts_mod));
d1097 1
a1097 3
					/* Modulate TS reply iff valid (!0) */
					memcpy(&tsecr, &opt[6],
					    sizeof(u_int32_t));
d1101 1
d1104 3
a1106 2
						pf_change_a(pd, &opt[6],
						    htonl(tsecr));
d1399 2
a1400 6
	u_char		 opts[MAX_TCPOPTLEN];
	u_char		*optp = opts;

	if (pd->csum_status == PF_CSUM_UNKNOWN)
		pf_check_proto_cksum(pd, pd->off, pd->tot_len - pd->off,
		    pd->proto, pd->af);
d1423 2
a1424 1
			memcpy(&mss, (optp + 2), 2);
d1426 3
a1428 1
				mss = htons(maxmss);
d1430 2
a1431 3
				    pd->off + sizeof(*th) + optp + 2 - opts,
				    2, &mss, M_NOWAIT);
				pf_cksum(pd, pd->m);
@


1.187
log
@There's no need to convert values returned by arc4random to the network
byte order.  Spotted by Gleb Smirnoff (glebius@@FreeBSD.org), thanks!

ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.186 2016/05/31 07:35:36 mpi Exp $ */
d764 1
a764 1
				nd6_output(ifp, m, dst, rt);
@


1.186
log
@Do not call nd6_output() without route entry argument.

ok sthen@@, bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.185 2016/05/28 12:04:33 sthen Exp $ */
d966 1
a966 2
					src->scrub->pfss_ts_mod =
					    htonl(arc4random());
@


1.185
log
@Backout pf.c r1.972, pf_norm.c r1.184, ok claudio

pf_test calls pf_refragment6 with dst=NULL, which is passed down to
rtable_match which attempts to dereference it.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.183 2015/11/24 13:37:16 mpi Exp $ */
d690 1
d746 10
d764 1
a764 1
				nd6_output(ifp, m, dst, NULL);
d773 1
@


1.184
log
@Do not call nd6_output() without route entry argument.

ok bluhm@@
@
text
@a689 1
	struct rtentry		*rt = NULL;
a744 10

	if (ifp == NULL) {
		rt = rtalloc(sin6tosa(dst), RT_RESOLVE,
		    m->m_pkthdr.ph_rtableid);
		if (rt == NULL) {
			ip6stat.ip6s_noroute++;
			error = -1;
		}
	}

d753 1
a753 1
				nd6_output(ifp, m, dst, rt);
a761 1
	rtfree(rt);
@


1.183
log
@No need for <net/if_types.h>

As a bonus this removes a "#if NCARP > 0", say yeah!
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.182 2015/09/10 08:28:31 mpi Exp $ */
d690 1
d746 10
d764 1
a764 1
				nd6_output(ifp, m, dst, NULL);
d773 1
@


1.182
log
@Kill two simple in6_ifstat_inc().
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.181 2015/08/19 21:22:41 sashan Exp $ */
a53 1
#include <net/if_types.h>
@


1.181
log
@PF must keep IPv6 fragment size as chosen by sender also for packets,
which are routed on behalf route-to action.

OK bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.180 2015/07/19 01:58:19 sashan Exp $ */
a755 1
				in6_ifstat_inc(ifp, ifs6_in_toobig);
@


1.180
log
@unused arguments at pf_normalize_tcp_init() and pf_refragment6()

OK deraadt.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.179 2015/07/18 15:19:44 sashan Exp $ */
d60 3
d686 2
a687 1
pf_refragment6(struct mbuf **m0, struct m_tag *mtag)
d750 11
a760 3
		if (error == 0)
			ip6_forward(m, 0);
		else
d762 1
@


1.179
log
@INET/INET6 address family check should be unified in PF

it also adds af_unhandled(), where it is currently missing.

ok mcbride@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.178 2015/05/05 23:27:47 chris Exp $ */
d683 1
a683 1
pf_refragment6(struct mbuf **m0, struct m_tag *mtag, int dir)
d886 1
a886 2
pf_normalize_tcp_init(struct pf_pdesc *pd, struct pf_state_peer *src,
    struct pf_state_peer *dst)
@


1.178
log
@Eliminate rabid semicolon
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.177 2015/04/29 18:05:56 bluhm Exp $ */
d914 2
d1020 2
d1424 1
a1424 1
#endif
d1436 1
a1436 1
#endif
d1448 1
a1448 1
#endif
@


1.177
log
@In most cases, IP fragments do not have an Ethernet padding.  So
add a condition to save a useless call to m_adj() and have a paranoid
length check in the other cases.
OK henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.176 2015/04/17 16:42:50 bluhm Exp $ */
d487 1
a487 1
		if (frent->fe_len < m2->m_pkthdr.len);
@


1.176
log
@On Ethernet packets have a minimal length, so very short packets
get padding appended to them.  This padding is not stripped off in
ip6_input() (due to support for IPv6 Jumbograms, RFC2675).  That
means PF needs to be careful when reassembling fragmented packets
to not include the padding in the reassembled packet.
from FreeBSD; via Kristof Provost; OK henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.175 2015/03/14 03:38:51 jsg Exp $ */
d472 2
a473 1
	m_adj(m, (frent->fe_hdrlen + frent->fe_len) - m->m_pkthdr.len);
d487 2
a488 1
		m_adj(m2, frent->fe_len - m2->m_pkthdr.len);
@


1.175
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.174 2015/02/08 01:29:19 henning Exp $ */
d470 3
a473 1
	m = frent->fe_m;
d485 2
@


1.174
log
@pf normalization code was in dire need of style normalization.
ok mpi pelikan
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.172 2014/12/19 17:14:40 tedu Exp $ */
a54 1
#include <net/bpf.h>
@


1.173
log
@Userland (base & ports) was adapted to always include <netinet/in.h>
before <net/pfvar.h> or <net/if_pflog.h>.  The kernel files can be
cleaned up next.  Some sockaddr_union steps make it into here as well.
ok naddy
@
text
@d167 1
d175 1
a175 2
	int32_t			 expire = time_uptime -
				    pf_default_rule.timeout[PFTM_FRAG];
d177 1
a180 1

a188 1

d196 1
a196 2
	DPFPRINTF(LOG_NOTICE, "trying to free > %d frents",
	    pf_nfrents - goal);
d198 1
a198 2
		frag = TAILQ_LAST(&pf_fragqueue, pf_fragqueue);
		if (frag == NULL)
a218 1

a222 1

d374 1
a374 2
	    after = next)
	{
a394 1

d408 1
a408 1
 free_fragment:
d418 1
a418 1
 bad_fragment:
d420 1
a420 1
 drop_fragment:
a445 1

a480 1

a537 1

a616 1

d671 1
a671 1
 fail:
a721 1

d754 3
a756 3
	struct ip		*h = mtod(pd->m, struct ip *);
	u_int16_t		 fragoff = (ntohs(h->ip_off) & IP_OFFMASK) << 3;
	u_int16_t		 mff = (ntohs(h->ip_off) & IP_MF);
d786 1
a786 1
 no_fragment:
d817 1
a817 1
 no_fragment:
d840 1
a840 1
		if (flags & TH_FIN)
d850 1
a850 1
		if ((flags & TH_FIN) || (flags & TH_PUSH) || (flags & TH_URG))
d875 1
a875 1
 tcp_drop:
a910 1

a917 1

d922 2
a923 1
		int hlen;
d995 1
a995 1
	case AF_INET: {
a1002 1
	}
d1004 1
a1004 1
	case AF_INET6: {
a1011 1
	}
d1112 2
a1113 2
		src->scrub->pfss_flags = (src->scrub->pfss_flags & ~PFSS_PAWS)
		    | PFSS_PAWS_IDLED;
d1122 2
a1123 2
		dst->scrub->pfss_flags = (dst->scrub->pfss_flags & ~PFSS_PAWS)
		    | PFSS_PAWS_IDLED;
d1190 2
a1191 3
		struct timeval delta_ts;
		int ts_fudge;

a1201 1

a1208 1

d1228 5
a1232 7
			DPFPRINTF(LOG_NOTICE,
			    " tsval: %u  tsecr: %u  +ticks: %u  "
			    "idle: %llu.%06lus",
			    tsval, tsecr, tsval_from_last,
			    (long long)delta_ts.tv_sec, delta_ts.tv_usec);
			DPFPRINTF(LOG_NOTICE,
			    " src->tsval: %u  tsecr: %u",
d1234 3
a1236 4
			DPFPRINTF(LOG_NOTICE,
			    " dst->tsval: %u  tsecr: %u  tsval0: %u",
			    dst->scrub->pfss_tsval, dst->scrub->pfss_tsecr,
			    dst->scrub->pfss_tsval0);
a1245 1

a1246 1

a1296 1

a1324 1

d1391 1
a1391 2
		switch (opt) {
		case TCPOPT_MAXSEG:
a1401 3
			break;
		default:
			break;
@


1.172
log
@unifdef INET in net code as a precursor to removing the pretend option.
long live the one true internet.
ok henning mikeb
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.171 2014/12/05 15:50:04 mpi Exp $ */
a42 6
#include <net/if.h>
#include <net/if_var.h>
#include <net/if_types.h>
#include <net/bpf.h>
#include <net/if_pflog.h>

d48 1
d51 6
@


1.171
log
@Explicitly include <net/if_var.h> instead of pulling it in <net/if.h>.

ok mikeb@@, krw@@, bluhm@@, tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.170 2014/11/18 02:37:31 tedu Exp $ */
a907 1
#ifdef INET
a912 1
#endif /* INET */
a1006 1
#ifdef INET
a1015 1
#endif /* INET */
@


1.170
log
@move arc4random prototype to systm.h. more appropriate for most code
to include that than rdnvar.h. ok deraadt dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.169 2014/10/10 16:20:03 sthen Exp $ */
d44 1
@


1.169
log
@s/lenght/length/ in comments
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.168 2014/09/08 06:24:13 jsg Exp $ */
a42 1
#include <dev/rndvar.h>
@


1.168
log
@remove uneeded route.h includes
ok miod@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.167 2014/07/22 11:06:10 mpi Exp $ */
d67 1
a67 1
	u_int16_t	 fe_hdrlen;	/* ipv4 header lenght with ip options
d101 1
a101 1
	u_int16_t	 ft_hdrlen;	/* header lenght of reassembled pkt */
@


1.167
log
@Fewer <netinet/in_systm.h> !
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.166 2014/07/13 17:41:04 bluhm Exp $ */
a46 1
#include <net/route.h>
@


1.166
log
@When reassembled IPv6 fragments are NATed or RDRed by pf, the
checksum has to be recalculated before the packet is fragmented
again.  Put a missing in6_proto_cksum_out() into pf_refragment6().
This makes run-regress-frag6 and run-regress-frag6-ext pass again.
From Matthias Pitzl; OK henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.165 2014/03/27 12:07:48 jca Exp $ */
a50 1
#include <netinet/in_systm.h>
@


1.165
log
@When enforcing TOS (Traffic Class), preserve the ECN bits, just as we do
with IPv4 packets. ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.164 2014/01/22 04:34:25 henning Exp $ */
d705 3
@


1.164
log
@one more absolutely obvious bcopy -> memcpy
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.162 2013/10/17 16:27:42 bluhm Exp $ */
d1467 1
a1467 1
			h6->ip6_flow &= ~htonl(0x0ff00000);
@


1.163
log
@Remove dead assignments and now unused variables.

Found by LLVM/Clang Static Analyzer.

ok henning@@ mikeb@@ bluhm@@
@
text
@d1420 1
a1420 1
			bcopy((caddr_t)(optp + 2), (caddr_t)&mss, 2);
@


1.162
log
@The header file netinet/in_var.h included netinet6/in6_var.h.  This
created a bunch of useless dependencies.  Remove this implicit
inclusion and do an explicit #include <netinet6/in6_var.h> when it
is needed.
OK mpi@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.161 2013/10/01 20:15:57 sf Exp $ */
a865 3
		u_int16_t	ov, nv;

		ov = *(u_int16_t *)(&th->th_ack + 1);
a867 2
		nv = *(u_int16_t *)(&th->th_ack + 1);

@


1.161
log
@Format string fixes: Cast time_t to long long

and mnt_stat.f_ctime is long long, too
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.160 2013/07/23 22:47:10 bluhm Exp $ */
a50 1
#include <netinet/in_var.h>
@


1.160
log
@Do not reset the fragment timeout each time a fragment arrives.
Start the expire counter when the queue is created by the first
fragment and drop it if the packet could not be reassembled within
60 seconds.
Reported by Antonios Atlasis; OK henning@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.159 2013/06/26 09:12:39 henning Exp $ */
d1256 3
a1258 3
			    "idle: %lus %lums",
			    tsval, tsecr, tsval_from_last, delta_ts.tv_sec,
			    delta_ts.tv_usec / 1000);
@


1.159
log
@put the cksum diff back, of course with the bug fixed where we could
under some circumstances repair broken checksums on the way.
ok ryan naddy mikeb
.
redo most of the protocol (tcp/udp/...) checksum handling
-assume we have hardware checksum offloading. stop mucking with the
 checksum in most of the stack
-stop checksum mucking in pf, just set a "needs checksumming" flag if needed
-in all output pathes, very late, if we figure out the outbound interface
 doesn't have hw cksum offloading, do the cksum in software. this especially
 makes the bridge path behave like a regular output path
-little special casing for bridge still required until the broadcast path
 loses its disgusting shortcut hacks, but at least it's in one place now
 and not all over the stack
in6_proto_cksum_out mostly written by krw@@
started at k2k11 in iceland more than 1.5 years ago - yes it took that
long, this stuff is everything but easy.
this happens to fix the infamous pf rdr bug that made us turn off proto
cksum offloading on almost all interface drivers.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.158 2013/06/17 19:50:06 bluhm Exp $ */
a239 2
		/* XXX Are we sure we want to update the timeout? */
		frag->fr_timeout = time_uptime;
@


1.158
log
@Before pulling the TCP options from the mbuf onto the stack, do an
additional length check in pf_modulate_sack() and pf_normalize_mss().
Overflow cannot happen due to the restricted values in the length
calculation.  As this is not obvious, be better safe than sorry.
OK henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.157 2012/11/06 12:32:41 henning Exp $ */
d843 4
a875 1
		th->th_sum = pf_cksum_fixup(th->th_sum, ov, nv, 0);
a880 1
		th->th_sum = pf_cksum_fixup(th->th_sum, th->th_urp, 0, 0);
d886 2
a887 1
	if (rewrite)
d889 1
d1081 1
a1081 2
						pf_change_a(&opt[2],
						    &th->th_sum,
d1083 1
a1083 2
						    src->scrub->pfss_ts_mod),
						    0);
d1095 2
a1096 3
						pf_change_a(&opt[6],
						    &th->th_sum, htonl(tsecr),
						    0);
d1402 4
a1429 2
				th->th_sum = pf_cksum_fixup(th->th_sum,
				    mss, htons(maxmss), 0);
d1434 1
a1442 2


@


1.157
log
@backout csum diff for the moment, requested by theo
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.156 2012/11/01 07:55:56 henning Exp $ */
d1404 2
a1405 2
	if (cnt > 0 && !pf_pull_hdr(pd->m, pd->off + sizeof(*th), opts, cnt,
	    NULL, NULL, pd->af))
@


1.156
log
@redo most of the protocol (tcp/udp/...) checksum handling
-assume we have hardware checksum offloading. stop mucking with the
 checksum in most of the stack
-stop checksum mucking in pf, just set a "needs checksumming" flag if needed
-in all output pathes, very late, if we figure out the outbound interface
 doesn't have hw cksum offloading, do the cksum in software. this especially
 makes the bridge path behave like a regular output path
-little special casing for bridge still required until the broadcast path
 loses its disgusting shortcut hacks, but at least it's in one place now
 and not all over the stack
in6_proto_cksum_out mostly written by krw@@
started at k2k11 in iceland more than 1.5 years ago - yes it took that
long, this stuff is everything but easy.
this happens to fix the infamous pf rdr bug that made us turn off proto
cksum offloading on almost all interface drivers.
ok camield sthen claudio, testing by many, thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.154 2012/05/12 13:08:48 mpf Exp $ */
d5 1
a5 1
 * Copyright 2009 - 2012 Henning Brauer <henning@@openbsd.org>
d872 1
d878 1
d884 1
a884 2
	if (rewrite) {
		pf_cksum(pd, pd->m);
a885 1
	}
d1078 1
d1080 2
a1081 1
						    src->scrub->pfss_ts_mod));
d1094 2
a1095 1
						    htonl(tsecr));
d1425 2
a1430 1
				pf_cksum(pd, pd->m);
d1439 2
@


1.155
log
@Use time_uptime for expiration values as time_second can be skewed at
runtime while time_uptime is monotonic. Prevent underflows in
pfsync(4) and pflow(4) by using signed variables.  pfsync(4) problem
pointed out by camield.

Diff originally by dlg, frag and pflow bits by me.

feedback dlg
man page tweak jmc

Various versions of the pflow bits tested by Hrvoje Popovski
(hrvoje AT srce DOT hr), thanks!

ok benno, henning, dlg
@
text
@d5 1
a5 1
 * Copyright 2009 Henning Brauer <henning@@openbsd.org>
a871 1
		th->th_sum = pf_cksum_fixup(th->th_sum, ov, nv, 0);
a876 1
		th->th_sum = pf_cksum_fixup(th->th_sum, th->th_urp, 0, 0);
d882 2
a883 1
	if (rewrite)
d885 1
a1077 1
						    &th->th_sum,
d1079 1
a1079 2
						    src->scrub->pfss_ts_mod),
						    0);
d1092 1
a1092 2
						    &th->th_sum, htonl(tsecr),
						    0);
a1421 2
				th->th_sum = pf_cksum_fixup(th->th_sum,
				    mss, htons(maxmss), 0);
d1426 1
a1434 2


@


1.154
log
@Ignore/preserve ECN bits on ToS matching and scrubbing.
The lower 2 bits of the tos-header are used for ECN.
 (http://tools.ietf.org/html/rfc2474#section-3)
OK henning@@, haesbaert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.153 2012/02/03 01:57:51 bluhm Exp $ */
d99 1
a99 1
	u_int32_t	fr_timeout;
d176 1
a176 1
	u_int32_t		 expire = time_second -
d241 1
a241 1
		frag->fr_timeout = time_second;
d317 1
a317 1
		frag->fr_timeout = time_second;
@


1.153
log
@The kernel did not compile without INET6.  Put some #ifdefs into
pf to fix that.
- add #ifdef INET6 in obvious places
- af translation is only possible with both INET and INET6
- interleave #endif /* INET6 */ and closing brace correctly
- it is not necessary to #ifdef function prototypes
- do not compile af translate functions at all instead of empty stub,
  then the linker will report inconsistencies
- pf_poolmask() actually takes an sa_family_t not an u_int8_t argument
No binary change for GENERIC compiled with -O2 and -UDIAGNOSTIC.
reported by Olivier Cochard-Labbe; ok mikeb@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.152 2012/01/26 20:16:06 bluhm Exp $ */
d1469 1
a1469 1
			h->ip_tos = tos;
@


1.152
log
@Clean up the pf normalization code:
- Let pf_normalize_ip() and pf_normalize_ip6() take the struct
  pf_pdesc pd as argument.
- Always check wether the mbuf got NULL after normalization to make
  the code more robust.
- Make the code structure of pf_normalize_ip6() more like
  pf_normalize_ip() to make the differences obvious.
ok henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.151 2012/01/23 18:37:20 bluhm Exp $ */
d131 1
a131 1
#endif
d366 1
d369 1
d387 1
d390 1
d418 1
d428 1
@


1.151
log
@Do not keep state when dropping overlapping IPv6 fragments in pf
and IPv6 stack.
ok sperreault@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.150 2012/01/15 22:55:35 bluhm Exp $ */
d759 1
a759 1
pf_normalize_ip(struct mbuf **m0, int dir, u_short *reason)
d761 1
a761 2
	struct mbuf		*m = *m0;
	struct ip		*h = mtod(m, struct ip *);
a764 1
	/* We will need other tests here */
d785 2
a786 2
	/* Returns PF_DROP or *m0 is NULL or completely reassembled mbuf */
	if (pf_reassemble(m0, dir, reason) != PF_PASS)
d788 1
a788 2
	m = *m0;
	if (m == NULL)
d791 1
a791 1
	h = mtod(m, struct ip *);
d803 1
a803 2
pf_normalize_ip6(struct mbuf **m0, int dir, int off, int extoff,
    u_short *reason)
a804 1
	struct mbuf		*m = *m0;
d807 2
a808 2
	if (!pf_status.reass || off == 0)
		return (PF_PASS);	/* no reassembly */
d810 2
a811 1
	if (!pf_pull_hdr(m, off, &frag, sizeof(frag), NULL, reason, AF_INET6))
a812 2
	/* offset now points to data portion */
	off += sizeof(frag);
d814 6
a819 2
	/* Returns PF_DROP or *m0 is NULL or completely reassembled mbuf */
	if (pf_reassemble6(m0, &frag, off, extoff, dir, reason) != PF_PASS)
d821 2
d824 1
@


1.150
log
@Calling pf_normalize_ip() from pf_setup_pdesc() was bad as the
latter is called from pf packet logging.  This resulted in normalization
and reassembly of bad packets to be logged.  So rearrange the code
and move the call to pf_test().
ok henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.149 2012/01/13 11:24:35 bluhm Exp $ */
d329 1
a329 9
	if (TAILQ_EMPTY(&frag->fr_queue)) {
		/*
		 * Overlapping IPv6 fragments have been detected.  Do not
		 * reassemble packet but also drop future fragments.
		 * This will be done for this ident/src/dst combination
		 * until fragment queue timeout.
		 */
		goto drop_fragment;
	}
d367 1
a367 1
			goto flush_fragentries;
d386 1
a386 1
			goto flush_fragentries;
d414 1
a414 1
 flush_fragentries:
d416 4
a419 5
	 * RFC5722:  When reassembling an IPv6 datagram, if one or
	 * more its constituent fragments is determined to be an
	 * overlapping fragment, the entire datagram (and any constituent
	 * fragments, including those not yet received) MUST be
	 * silently discarded.
d422 1
a422 7
	while ((prev = TAILQ_FIRST(&frag->fr_queue)) != NULL) {
		TAILQ_REMOVE(&frag->fr_queue, prev, fr_next);

		m_freem(prev->fe_m);
		pool_put(&pf_frent_pl, prev);
		pf_nfrents--;
	}
@


1.149
log
@Drop IPv6 packets built from overlapping fragments in pf reassembly.
The reassembly state will be dropped after timeout, all related
fragments are dropped until that.  This is conforming to RFC 5722.
- Sort pf_fragment fields while there.
- If the fr_queue is empty, we had overlapping fragments, don't add
  new ones.
- If we detect overlapping IPv6 fragments, flush the fr_queue and
  drop all fragments immediately.
- Rearrange debug output, to make clear what happens.
- An IPv4 fragment that is totaly overlapped does not inclease the
  bad fragment counter.
- Put an KASSERT into pf_isfull_fragment() to make sure that the
  fr_queue is never emtpy there.
discussed with Fernando Gont; ok henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.148 2012/01/03 17:06:38 bluhm Exp $ */
d826 3
@


1.148
log
@Instead of having two functions pf_free_fragment() and pf_remove_fragment()
doing more or less the same, merge them into one.  Just remove
fragment entries from the queue in pf_join_fragment() before they
are freed.  Then pf_remove_fragment() is not needed anymore.
ok henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.147 2011/11/25 12:52:10 dlg Exp $ */
d98 1
a100 1
	TAILQ_HEAD(pf_fragq, pf_frent) fr_queue;
d316 1
a318 1
		TAILQ_INIT(&frag->fr_queue);
d329 9
a337 1
	KASSERT(!TAILQ_EMPTY(&frag->fr_queue));
d374 3
d378 5
a382 3
		if (precut >= frent->fe_len)
			goto bad_fragment;
		DPFPRINTF(LOG_NOTICE, "overlap -%d", precut);
d393 3
a396 1
		DPFPRINTF(LOG_NOTICE, "adjust overlap %d", aftercut);
d398 1
d406 1
d422 16
d451 2
@


1.147
log
@use time_uptime to set state creation values as time_second can be
skewed at runtime by things like date(1) and ntpd. time_uptime is
monotonic and therefore more useful to compare against.

ok deraadt@@ mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.146 2011/09/28 17:15:45 bluhm Exp $ */
a117 1
void			 pf_remove_fragment(struct pf_fragment *);
d209 4
a212 2
/* Frees the fragments and all associated entries */

d218 5
a222 3
	/* Free all fragments */
	for (frent = TAILQ_FIRST(&frag->fr_queue); frent;
	    frent = TAILQ_FIRST(&frag->fr_queue)) {
d230 1
a230 1
	pf_remove_fragment(frag);
a248 10
/* Removes a fragment from the fragment queue and frees the fragment */

void
pf_remove_fragment(struct pf_fragment *frag)
{
	RB_REMOVE(pf_frag_tree, &pf_frag_tree, frag);
	TAILQ_REMOVE(&pf_fragqueue, frag, frag_next);
	pool_put(&pf_frag_pl, frag);
}

d391 2
a393 1
		TAILQ_REMOVE(&frag->fr_queue, after, fr_next);
d452 1
a452 1
	struct pf_frent		*frent, *next;
d455 1
a455 1
	next = TAILQ_NEXT(frent, fr_next);
d464 3
a466 2
	for (frent = next; frent != NULL; frent = next) {
		next = TAILQ_NEXT(frent, fr_next);
d477 1
a477 1
	pf_remove_fragment(frag);
@


1.146
log
@As requested by henning, move the mbuf pointer into struct pf_pdesc.
Also sort pd to the beginning of the functions' parameter lists for
consistency.
ok henning
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.145 2011/09/22 14:57:12 bluhm Exp $ */
d1105 1
a1105 1
	    time_second - state->creation > TS_MAX_CONN))  {
@


1.145
log
@As I have touched half of pf lines anyway, fix whitespaces now.
KNF, no binary change.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.144 2011/09/21 19:07:30 bluhm Exp $ */
d811 1
a811 1
pf_normalize_tcp(struct mbuf *m, struct pf_pdesc *pd)
d860 1
a860 1
		m_copyback(m, pd->off, sizeof(*th), th, M_NOWAIT);
d870 2
a871 2
pf_normalize_tcp_init(struct mbuf *m, struct pf_pdesc *pd,
    struct pf_state_peer *src, struct pf_state_peer *dst)
d888 1
a888 1
		struct ip *h = mtod(m, struct ip *);
d895 1
a895 1
		struct ip6_hdr *h = mtod(m, struct ip6_hdr *);
d912 2
a913 1
	    pf_pull_hdr(m, pd->off, hdr, th->th_off << 2, NULL, NULL, pd->af)) {
d966 3
a968 3
pf_normalize_tcp_stateful(struct mbuf *m, struct pf_pdesc *pd,
    u_short *reason, struct pf_state *state,
    struct pf_state_peer *src, struct pf_state_peer *dst, int *writeback)
d990 1
a990 1
			struct ip *h = mtod(m, struct ip *);
d1001 1
a1001 1
			struct ip6_hdr *h = mtod(m, struct ip6_hdr *);
d1014 2
a1015 1
	    pf_pull_hdr(m, pd->off, hdr, th->th_off << 2, NULL, NULL, pd->af)) {
d1085 1
a1085 1
			m_copyback(m, pd->off + sizeof(struct tcphdr),
d1367 1
a1367 1
pf_normalize_mss(struct mbuf *m, struct pf_pdesc *pd, u_int16_t maxmss)
d1379 1
a1379 1
	if (cnt > 0 && !pf_pull_hdr(m, pd->off + sizeof(*th), opts, cnt,
d1403 1
a1403 1
				m_copyback(m,
d1406 1
a1406 1
				m_copyback(m, pd->off, sizeof(*th), th,
@


1.144
log
@Check the protocol header length for tcp, udp, icmp, icmp6 in
pf_setup_pdesc().  It is better to check and bail out early than
to rely on pf_pull_hdr() later.
ok henning mpf
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.143 2011/09/20 10:51:18 bluhm Exp $ */
d873 4
a876 4
	struct tcphdr	 *th = pd->hdr.tcp;
	u_int32_t tsval, tsecr;
	u_int8_t hdr[60];
	u_int8_t *opt;
d969 8
a976 8
	struct tcphdr	 *th = pd->hdr.tcp;
	struct timeval uptime;
	u_int32_t tsval, tsecr;
	u_int tsval_from_last;
	u_int8_t hdr[60];
	u_int8_t *opt;
	int copyback = 0;
	int got_ts = 0;
d1131 1
a1131 1
		 * long fat pipes (packets that were somehow delayed 
@


1.143
log
@Put kif and dir into pdesc an use this instead of passing the values
around.  This is a mechanical change.  Initialize pd2 and use it
where appropriate.
ok henning on an earlier version; ok mpf
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.142 2011/09/19 12:51:52 bluhm Exp $ */
a836 4

	/* Check for illegal header length */
	if (th->th_off < (sizeof(struct tcphdr) >> 2))
		goto tcp_drop;
@


1.142
log
@Consolidate pf function parameters.  Move off and hdrlen into pdesc
and change their type from int to u_int32_t.  Do not pass struct
tcphdr *th and sa_family_t af, it is in pd anyway.  Do not use af
and pd->af intermixed, the latter makes clear where it comes from.
Do not calculate the packet length again if pd already has it.  Use
pd2.off instead of off2.
go go go go don't stop henning@@ mpf@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.141 2011/09/18 11:17:57 miod Exp $ */
d811 1
a811 1
pf_normalize_tcp(int dir, struct mbuf *m, struct pf_pdesc *pd)
@


1.141
log
@Fix various format string types to as a minimum match the width of the
variables being processed.
ok bluhm@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.140 2011/07/18 21:03:10 mikeb Exp $ */
d811 1
a811 1
pf_normalize_tcp(int dir, struct mbuf *m, int off, struct pf_pdesc *pd)
d864 1
a864 1
		m_copyback(m, off, sizeof(*th), th, M_NOWAIT);
d874 2
a875 2
pf_normalize_tcp_init(struct mbuf *m, int off, struct pf_pdesc *pd,
    struct tcphdr *th, struct pf_state_peer *src, struct pf_state_peer *dst)
d877 1
d916 1
a916 1
	    pf_pull_hdr(m, off, hdr, th->th_off << 2, NULL, NULL, pd->af)) {
d969 2
a970 2
pf_normalize_tcp_stateful(struct mbuf *m, int off, struct pf_pdesc *pd,
    u_short *reason, struct tcphdr *th, struct pf_state *state,
d973 1
d1017 1
a1017 1
	    pf_pull_hdr(m, off, hdr, th->th_off << 2, NULL, NULL, pd->af)) {
d1087 1
a1087 1
			m_copyback(m, off + sizeof(struct tcphdr),
d1369 1
a1369 1
pf_normalize_mss(struct mbuf *m, int off, struct pf_pdesc *pd, u_int16_t maxmss)
d1381 1
a1381 1
	if (cnt > 0 && !pf_pull_hdr(m, off + sizeof(*th), opts, cnt,
d1406 1
a1406 1
				    off + sizeof(*th) + optp + 2 - opts,
d1408 2
a1409 1
				m_copyback(m, off, sizeof(*th), th, M_NOWAIT);
@


1.140
log
@unbreak set-tos for ipv6;  reported by babut at yandex dot ru,
with input and ok from bluhm and claudio
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.139 2011/07/07 20:46:36 bluhm Exp $ */
d1231 1
a1231 1
			    " tsval: %lu  tsecr: %lu  +ticks: %lu  "
d1236 1
a1236 1
			    " src->tsval: %lu  tsecr: %lu",
d1239 1
a1239 1
			    " dst->tsval: %lu  tsecr: %lu  tsval0: %lu",
@


1.139
log
@There were two loops in pf_setup_pdesc() and pf_normalize_ip6()
walking over the IPv6 header chain.  Merge them into one loop,
adjust some length checks and fix IPv6 jumbo option handling.  Also
allow strange but legal IPv6 packets with plen=0 passing through
pf.  IPv6 jumbo packets still get dropped.
testing dhill@@; ok mcbride@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.138 2011/07/05 22:00:04 bluhm Exp $ */
d1447 1
a1447 1
			h6->ip6_flow &= htonl(0x0ff00000);
@


1.138
log
@Instead of passing the ip header and mbuf to pf_reassemble(), lookup
the header address in the mbuf.
ok henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.137 2011/07/05 19:53:43 mikeb Exp $ */
d791 2
a792 1
pf_normalize_ip6(struct mbuf **m0, int dir, u_short *reason)
a794 4
	struct ip6_hdr		*h = mtod(m, struct ip6_hdr *);
	struct ip6_ext		 ext;
	struct ip6_opt		 opt;
	struct ip6_opt_jumbo	 jumbo;
a795 7
	u_int32_t		 jumbolen = 0, plen;
	int			 extoff;
	int			 off;
	int			 optend;
	int			 ooff;
	u_int8_t		 proto;
	int			 terminal;
d797 2
a798 102
	/* Check for illegal packets */
	if (sizeof(struct ip6_hdr) + IPV6_MAXPACKET < m->m_pkthdr.len)
		goto drop;

	extoff = 0;
	off = sizeof(struct ip6_hdr);
	proto = h->ip6_nxt;
	terminal = 0;
	do {
		switch (proto) {
		case IPPROTO_FRAGMENT:
			goto fragment;
		case IPPROTO_AH:
		case IPPROTO_ROUTING:
		case IPPROTO_DSTOPTS:
			if (!pf_pull_hdr(m, off, &ext, sizeof(ext), NULL,
			    NULL, AF_INET6))
				goto shortpkt;
			extoff = off;
			if (proto == IPPROTO_AH)
				off += (ext.ip6e_len + 2) * 4;
			else
				off += (ext.ip6e_len + 1) * 8;
			proto = ext.ip6e_nxt;
			break;
		case IPPROTO_HOPOPTS:
			if (!pf_pull_hdr(m, off, &ext, sizeof(ext), NULL,
			    NULL, AF_INET6))
				goto shortpkt;
			extoff = off;
			optend = off + (ext.ip6e_len + 1) * 8;
			ooff = off + sizeof(ext);
			do {
				if (!pf_pull_hdr(m, ooff, &opt.ip6o_type,
				    sizeof(opt.ip6o_type), NULL, NULL,
				    AF_INET6))
					goto shortpkt;
				if (opt.ip6o_type == IP6OPT_PAD1) {
					ooff++;
					continue;
				}
				if (!pf_pull_hdr(m, ooff, &opt, sizeof(opt),
				    NULL, NULL, AF_INET6))
					goto shortpkt;
				if (ooff + sizeof(opt) + opt.ip6o_len > optend)
					goto drop;
				switch (opt.ip6o_type) {
				case IP6OPT_JUMBO:
					if (h->ip6_plen != 0)
						goto drop;
					if (!pf_pull_hdr(m, ooff, &jumbo,
					    sizeof(jumbo), NULL, NULL,
					    AF_INET6))
						goto shortpkt;
					memcpy(&jumbolen, jumbo.ip6oj_jumbo_len,
					    sizeof(jumbolen));
					jumbolen = ntohl(jumbolen);
					if (jumbolen <= IPV6_MAXPACKET)
						goto drop;
					if (sizeof(struct ip6_hdr) + jumbolen !=
					    m->m_pkthdr.len)
						goto drop;
					break;
				default:
					break;
				}
				ooff += sizeof(opt) + opt.ip6o_len;
			} while (ooff < optend);

			off = optend;
			proto = ext.ip6e_nxt;
			break;
		default:
			terminal = 1;
			break;
		}
	} while (!terminal);

	/* jumbo payload option must be present, or plen > 0 */
	plen = ntohs(h->ip6_plen);
	if (plen == 0)
		plen = jumbolen;
	if (plen == 0)
		goto drop;
	if (sizeof(struct ip6_hdr) + plen > m->m_pkthdr.len)
		goto shortpkt;

	return (PF_PASS);

 fragment:
	/* jumbo payload packets cannot be fragmented */
	plen = ntohs(h->ip6_plen);
	if (plen == 0 || jumbolen)
		goto drop;
	if (sizeof(struct ip6_hdr) + plen > m->m_pkthdr.len)
		goto shortpkt;

	if (!pf_status.reass)
		return (PF_PASS);	/* no reassembly */

	if (!pf_pull_hdr(m, off, &frag, sizeof(frag), NULL, NULL, AF_INET6))
		goto shortpkt;
a804 3
	m = *m0;
	if (m == NULL)
		return (PF_PASS);
a806 8

 shortpkt:
	REASON_SET(reason, PFRES_SHORT);
	return (PF_DROP);

 drop:
	REASON_SET(reason, PFRES_NORM);
	return (PF_DROP);
@


1.137
log
@add missing ifdefs for INET6;  diff from form, ok henning, bluhm, claudio
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.136 2011/07/03 18:08:02 claudio Exp $ */
d128 1
a128 2
int			 pf_reassemble(struct mbuf **, struct ip *, int,
			    u_short *);
d130 2
a131 3
int			 pf_reassemble6(struct mbuf **, struct ip6_hdr *,
			    struct ip6_frag *, u_int16_t, u_int16_t, int,
			    u_short *);
d488 1
a488 1
pf_reassemble(struct mbuf **m0, struct ip *ip, int dir, u_short *reason)
d491 1
d560 1
a560 1
pf_reassemble6(struct mbuf **m0, struct ip6_hdr *ip6, struct ip6_frag *fraghdr,
d564 1
d773 1
a773 1
	if (pf_reassemble(m0, h, dir, reason) != PF_PASS)
d913 1
a913 1
	if (pf_reassemble6(m0, h, &frag, off, extoff, dir, reason) != PF_PASS)
@


1.136
log
@Refactor the fragment handling in pf_setup_pdesc() so that AF_INET
and AF_INET6 are doing the fragment handling the same way. Makes
code more readable.
With and OK bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.135 2011/06/21 08:59:47 bluhm Exp $ */
d130 1
d134 1
@


1.135
log
@There is no need to handle fragmented TCP reset packets in a special
way.  Remove PFDESC_IP_REAS and pf_pdesc flags completely.
ok claudio@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.134 2011/06/20 19:03:41 claudio Exp $ */
d767 3
d901 3
@


1.134
log
@More cleanup in pf_test/pf_test6 this time mostly the fragment
handling. More to come to make the two codepathes a bit more identical.
tested by many (esp. krw@@ and sthen@@) input and OK bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.133 2011/05/24 14:01:52 claudio Exp $ */
d742 1
a742 2
pf_normalize_ip(struct mbuf **m0, int dir, u_short *reason,
    struct pf_pdesc *pd)
a780 1
	pd->flags |= PFDESC_IP_REAS;
d786 1
a786 2
pf_normalize_ip6(struct mbuf **m0, int dir, u_short *reason,
    struct pf_pdesc *pd)
a910 1
	pd->flags |= PFDESC_IP_REAS;
@


1.133
log
@Merge pf_scrub_ip() and pf_scrub_ip6() into a single function.  Call
pf_scrub with the right arugments in the rule case so that match
rules will work as expected.  As a benefit allow setting the tos
on IPv6 packets as well.
OK henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.132 2011/04/23 10:00:36 bluhm Exp $ */
d742 2
a743 2
pf_normalize_ip(struct mbuf **m0, int dir, struct pfi_kif *kif,
    u_short *reason, struct pf_pdesc *pd)
a746 1
	int			 hlen = h->ip_hl << 2;
d750 3
a752 6
	/* Check for illegal packets */
	if (hlen < (int)sizeof(struct ip))
		goto drop;

	if (hlen > ntohs(h->ip_len))
		goto drop;
a757 4
	/* We will need other tests here */
	if (!fragoff && !mff)
		goto no_fragment;

a783 4

 drop:
	REASON_SET(reason, PFRES_NORM);
	return (PF_DROP);
d788 2
a789 2
pf_normalize_ip6(struct mbuf **m0, int dir, struct pfi_kif *kif,
    u_short *reason, struct pf_pdesc *pd)
a816 1
			break;
d928 1
a928 2
pf_normalize_tcp(int dir, struct pfi_kif *kif, struct mbuf *m, int ipoff,
    int off, void *h, struct pf_pdesc *pd)
@


1.132
log
@pf_scrub_ip() does not modify the given mbuf pointer.  So don't
pass a pointer to a pointer to make the code in pf_test() clearer.
ok henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.131 2011/04/04 14:14:53 henning Exp $ */
d1551 2
a1552 1
pf_scrub_ip(struct mbuf *m, u_int16_t flags, u_int8_t min_ttl, u_int8_t tos)
d1555 3
d1560 1
a1560 1
	if (flags & PFSTATE_NODF && h->ip_off & htons(IP_DF))
d1564 1
a1564 1
	if (min_ttl && h->ip_ttl < min_ttl)
d1566 4
d1572 11
a1582 2
	if (flags & PFSTATE_SETTOS)
		h->ip_tos = tos;
d1585 2
a1586 1
	if (flags & PFSTATE_RANDOMID && !(h->ip_off & ~htons(IP_DF)))
a1588 12

#ifdef INET6
void
pf_scrub_ip6(struct mbuf *m, u_int8_t min_ttl)
{
	struct ip6_hdr		*h = mtod(m, struct ip6_hdr *);

	/* Enforce a minimum ttl, may cause endless packet loops */
	if (min_ttl && h->ip6_hlim < min_ttl)
		h->ip6_hlim = min_ttl;
}
#endif
@


1.131
log
@stop fiddling with the ip checksum here too, it is always recalculated
in all output pathes anyway.
even worse than in the rest of pf, here we ran circles to update the ip
cksum every time we flip a tiny bit in the header...
pretty sure dlg claudio ok'd it and it is damn obvious anyway
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.130 2011/03/24 20:09:44 bluhm Exp $ */
d1551 1
a1551 1
pf_scrub_ip(struct mbuf **m0, u_int16_t flags, u_int8_t min_ttl, u_int8_t tos)
a1552 1
	struct mbuf		*m = *m0;
d1574 1
a1574 1
pf_scrub_ip6(struct mbuf **m0, u_int8_t min_ttl)
a1575 1
	struct mbuf		*m = *m0;
@


1.130
log
@Reassemble IPv6 fragments in pf.  In the forward case, pf refragments
the packets with the same maximum size.  This allows the sender to
determine the optimal fragment size by Path MTU Discovery.
testing sthen@@ matthieu@@
ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.129 2011/03/23 18:34:17 bluhm Exp $ */
d759 1
a759 3
	if (pf_status.reass & PF_REASS_NODF && h->ip_off & htons(IP_DF)) {
		u_int16_t ip_off = h->ip_off;

a760 2
		h->ip_sum = pf_cksum_fixup(h->ip_sum, ip_off, h->ip_off, 0);
	}
d787 1
a787 3
	if (h->ip_off & ~htons(IP_DF)) {
		u_int16_t ip_off = h->ip_off;

a788 2
		h->ip_sum = pf_cksum_fixup(h->ip_sum, ip_off, h->ip_off, 0);
	}
d1557 1
a1557 3
	if (flags & PFSTATE_NODF && h->ip_off & htons(IP_DF)) {
		u_int16_t ip_off = h->ip_off;

a1558 2
		h->ip_sum = pf_cksum_fixup(h->ip_sum, ip_off, h->ip_off, 0);
	}
d1561 1
a1561 3
	if (min_ttl && h->ip_ttl < min_ttl) {
		u_int16_t ip_ttl = h->ip_ttl;

a1562 2
		h->ip_sum = pf_cksum_fixup(h->ip_sum, ip_ttl, h->ip_ttl, 0);
	}
d1565 1
a1565 4
	if (flags & PFSTATE_SETTOS) {
		u_int16_t	ov, nv;

		ov = *(u_int16_t *)h;
a1566 4
		nv = *(u_int16_t *)h;

		h->ip_sum = pf_cksum_fixup(h->ip_sum, ov, nv, 0);
	}
d1569 1
a1569 3
	if (flags & PFSTATE_RANDOMID && !(h->ip_off & ~htons(IP_DF))) {
		u_int16_t ip_id = h->ip_id;

a1570 2
		h->ip_sum = pf_cksum_fixup(h->ip_sum, ip_id, h->ip_id, 0);
	}
@


1.129
log
@Extract the address family independent functions from pf fragment
reassembly code.  This makes it possible to reuse them for IPv6.
ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.128 2011/02/01 16:10:31 bluhm Exp $ */
d6 1
d62 1
d130 3
d557 184
a812 1
	int			 off;
d818 2
a819 1
	u_int16_t		 fragoff = 0;
d829 1
d844 1
d855 1
d905 2
a906 1
	if (ntohs(h->ip6_plen) == 0)
a907 2
	else
		plen = ntohs(h->ip6_plen);
d916 3
a918 1
	if (ntohs(h->ip6_plen) == 0 || jumbolen)
d920 2
a921 1
	plen = ntohs(h->ip6_plen);
d925 2
a926 4
	fragoff = ntohs(frag.ip6f_offlg & IP6F_OFF_MASK);
	if (fragoff + (sizeof(struct ip6_hdr) + plen - off - sizeof(frag)) >
	    IPV6_MAXPACKET)
		goto badfrag;
d928 8
a935 2
	/* do something about it */
	/* remember to set pd->flags |= PFDESC_IP_REAS */
a943 4
	return (PF_DROP);

 badfrag:
	REASON_SET(reason, PFRES_FRAG);
@


1.128
log
@The check for invalid IPv6 fragment size in pf_normalize_ip6() was
wrong.  As an effect small valid fragmented packets got dropped and
some invalid fragmented packets were passed.  plen is the payload
lenght of the ipv6 packet without the ipv6 header.  off is relative
to the whole packet including the ipv6 header.  Add sizeof(struct
ip6_hdr) in the calculation.
ok henning@@ markus@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.127 2011/01/20 15:03:03 bluhm Exp $ */
d66 8
a73 3
	LIST_ENTRY(pf_frent) fr_next;
	struct ip *fr_ip;
	struct mbuf *fr_m;
d76 9
a84 1
#define PFFRAG_SEENLAST	0x0001		/* Seen the last fragment for this */
d87 7
a95 6
	struct in_addr	fr_src;
	struct in_addr	fr_dst;
	u_int8_t	fr_p;		/* protocol of this fragment */
	u_int8_t	fr_flags;	/* status flags */
	u_int16_t	fr_id;		/* fragment id for reassemble */
	u_int16_t	fr_max;		/* fragment data max */
d97 8
a104 1
	LIST_HEAD(pf_fragq, pf_frent) fr_queue;
a115 1
void			 pf_ip2key(struct pf_fragment *, struct ip *);
d119 9
a127 3
struct pf_fragment	*pf_find_fragment(struct ip *, struct pf_frag_tree *);
int			 pf_reassemble(struct mbuf **, struct pf_fragment **,
			    struct pf_frent *, int, u_short *);
d155 1
a155 1
	if ((diff = a->fr_id - b->fr_id))
d157 7
a163 1
	else if ((diff = a->fr_p - b->fr_p))
a164 8
	else if (a->fr_src.s_addr < b->fr_src.s_addr)
		return (-1);
	else if (a->fr_src.s_addr > b->fr_src.s_addr)
		return (1);
	else if (a->fr_dst.s_addr < b->fr_dst.s_addr)
		return (-1);
	else if (a->fr_dst.s_addr > b->fr_dst.s_addr)
		return (1);
d213 3
a215 3
	for (frent = LIST_FIRST(&frag->fr_queue); frent;
	    frent = LIST_FIRST(&frag->fr_queue)) {
		LIST_REMOVE(frent, fr_next);
d217 1
a217 1
		m_freem(frent->fr_m);
a224 9
void
pf_ip2key(struct pf_fragment *key, struct ip *ip)
{
	key->fr_p = ip->ip_p;
	key->fr_id = ip->ip_id;
	key->fr_src.s_addr = ip->ip_src.s_addr;
	key->fr_dst.s_addr = ip->ip_dst.s_addr;
}

d226 1
a226 1
pf_find_fragment(struct ip *ip, struct pf_frag_tree *tree)
a227 1
	struct pf_fragment	 key;
d230 1
a230 3
	pf_ip2key(&key, ip);

	frag = RB_FIND(pf_frag_tree, tree, &key);
d251 22
a272 4
#define FR_IP_OFF(fr)	((ntohs((fr)->fr_ip->ip_off) & IP_OFFMASK) << 3)
int
pf_reassemble(struct mbuf **m0, struct pf_fragment **frag,
    struct pf_frent *frent, int mff, u_short *reason)
d274 30
a303 12
	struct mbuf	*m = *m0, *m2;
	struct pf_frent	*frea, *next;
	struct pf_frent	*frep = NULL;
	struct ip	*ip = frent->fr_ip;
	int		 hlen = ip->ip_hl << 2;
	u_int16_t	 off = (ntohs(ip->ip_off) & IP_OFFMASK) << 3;
	u_int16_t	 ip_len = ntohs(ip->ip_len) - ip->ip_hl * 4;
	u_int16_t	 max = ip_len + off;

	/* Strip off ip header */
	m->m_data += hlen;
	m->m_len -= hlen;
d306 3
a308 3
	if (*frag == NULL) {
		*frag = pool_get(&pf_frag_pl, PR_NOWAIT);
		if (*frag == NULL) {
d310 2
a311 2
			*frag = pool_get(&pf_frag_pl, PR_NOWAIT);
			if (*frag == NULL) {
d317 4
a320 8
		(*frag)->fr_flags = 0;
		(*frag)->fr_max = 0;
		(*frag)->fr_src = frent->fr_ip->ip_src;
		(*frag)->fr_dst = frent->fr_ip->ip_dst;
		(*frag)->fr_p = frent->fr_ip->ip_p;
		(*frag)->fr_id = frent->fr_ip->ip_id;
		(*frag)->fr_timeout = time_second;
		LIST_INIT(&(*frag)->fr_queue);
d322 2
a323 2
		RB_INSERT(pf_frag_tree, &pf_frag_tree, *frag);
		TAILQ_INSERT_HEAD(&pf_fragqueue, *frag, frag_next);
d326 27
a352 2
		frep = NULL;
		goto insert;
d355 4
a358 6
	/*
	 * Find a fragment after the current one:
	 *  - off contains the real shifted offset.
	 */
	LIST_FOREACH(frea, &(*frag)->fr_queue, fr_next) {
		if (FR_IP_OFF(frea) > off)
d360 1
a360 1
		frep = frea;
d363 1
a363 1
	KASSERT(frep != NULL || frea != NULL);
d365 1
a365 4
	if (frep != NULL &&
	    FR_IP_OFF(frep) + ntohs(frep->fr_ip->ip_len) - frep->fr_ip->ip_hl *
	    4 > off)
	{
d368 2
a369 3
		precut = FR_IP_OFF(frep) + ntohs(frep->fr_ip->ip_len) -
		    frep->fr_ip->ip_hl * 4 - off;
		if (precut >= ip_len)
a370 1
		m_adj(frent->fr_m, precut);
d372 3
a374 5
		/* Enforce 8 byte boundaries */
		ip->ip_off = htons(ntohs(ip->ip_off) + (precut >> 3));
		off = (ntohs(ip->ip_off) & IP_OFFMASK) << 3;
		ip_len -= precut;
		ip->ip_len = htons(ip_len);
d377 2
a378 2
	for (; frea != NULL && ip_len + off > FR_IP_OFF(frea);
	    frea = next)
d382 1
a382 1
		aftercut = ip_len + off - FR_IP_OFF(frea);
d384 4
a387 8
		if (aftercut < ntohs(frea->fr_ip->ip_len) - frea->fr_ip->ip_hl
		    * 4)
		{
			frea->fr_ip->ip_len =
			    htons(ntohs(frea->fr_ip->ip_len) - aftercut);
			frea->fr_ip->ip_off = htons(ntohs(frea->fr_ip->ip_off) +
			    (aftercut >> 3));
			m_adj(frea->fr_m, aftercut);
d392 4
a395 4
		next = LIST_NEXT(frea, fr_next);
		m_freem(frea->fr_m);
		LIST_REMOVE(frea, fr_next);
		pool_put(&pf_frent_pl, frea);
d399 6
a404 7
 insert:
	/* Update maximum data size */
	if ((*frag)->fr_max < max)
		(*frag)->fr_max = max;
	/* This is the last segment */
	if (!mff)
		(*frag)->fr_flags |= PFFRAG_SEENLAST;
d406 7
a412 4
	if (frep == NULL)
		LIST_INSERT_HEAD(&(*frag)->fr_queue, frent, fr_next);
	else
		LIST_INSERT_AFTER(frep, frent, fr_next);
d414 5
a418 2
	/* The mbuf is part of the fragment entry, no direct free or access */
	m = *m0 = NULL;
d421 6
a426 2
	if (!((*frag)->fr_flags & PFFRAG_SEENLAST))
		return (PF_PASS);
d430 2
a431 2
	for (frep = LIST_FIRST(&(*frag)->fr_queue); frep; frep = next) {
		next = LIST_NEXT(frep, fr_next);
d433 2
a434 4
		off += ntohs(frep->fr_ip->ip_len) - frep->fr_ip->ip_hl * 4;
		if (off < (*frag)->fr_max &&
		    (next == NULL || FR_IP_OFF(next) != off))
		{
d436 3
a438 4
			    "missing fragment at %d, next %d, max %d",
			    off, next == NULL ? -1 : FR_IP_OFF(next),
			    (*frag)->fr_max);
			return (PF_PASS);
d441 7
a447 3
	DPFPRINTF(LOG_NOTICE, "%d < %d?", off, (*frag)->fr_max);
	if (off < (*frag)->fr_max)
		return (PF_PASS);
d449 8
a456 4
	/* We have all the data */
	frent = LIST_FIRST(&(*frag)->fr_queue);
	KASSERT(frent != NULL);
	next = LIST_NEXT(frent, fr_next);
d459 1
a459 2
	ip = frent->fr_ip;
	m = frent->fr_m;
d466 1
a466 1
		next = LIST_NEXT(frent, fr_next);
d468 3
a470 1
		m2 = frent->fr_m;
d476 48
a523 2
	ip->ip_src = (*frag)->fr_src;
	ip->ip_dst = (*frag)->fr_dst;
d525 2
a526 9
	/* Remove from fragment queue */
	pf_remove_fragment(*frag);
	*frag = NULL;
	*m0 = m;

	hlen = ip->ip_hl << 2;
	ip->ip_len = htons(off + hlen);
	m->m_len += hlen;
	m->m_data -= hlen;
a527 2
	/* some debugging cruft by sklower, below, will go away soon */
	/* XXX this should be done elsewhere */
d530 3
a532 2
		for (m2 = m; m2; m2 = m2->m_next)
			plen += m2->m_len;
d536 6
a541 2
	if (hlen + off > IP_MAXPACKET) {
		DPFPRINTF(LOG_NOTICE, "drop: too big: %d", off);
a549 9

 bad_fragment:
	REASON_SET(reason, PFRES_FRAG);
 drop_fragment:
	/* Oops - fail safe - drop packet */
	pool_put(&pf_frent_pl, frent);
	pf_nfrents--;
	/* PF_DROP requires a valid mbuf *m0 in pf_test(), will free later */
	return (PF_DROP);
d553 2
a554 2
pf_normalize_ip(struct mbuf **m0, int dir, struct pfi_kif *kif, u_short *reason,
    struct pf_pdesc *pd)
a556 2
	struct pf_frent		*frent;
	struct pf_fragment	*frag = NULL;
a557 1
	int			 mff = (ntohs(h->ip_off) & IP_MF);
d560 1
a560 3
	u_int16_t		 max;
	int			 ip_len;
	int			 ip_off;
d586 2
a587 32
		DPFPRINTF(LOG_NOTICE, "IP_DF");
		goto bad;
	}

	ip_len = ntohs(h->ip_len) - hlen;
	ip_off = (ntohs(h->ip_off) & IP_OFFMASK) << 3;

	/* All fragments are 8 byte aligned */
	if (mff && (ip_len & 0x7)) {
		DPFPRINTF(LOG_NOTICE, "mff and %d", ip_len);
		goto bad;
	}

	/* Respect maximum length */
	if (fragoff + ip_len > IP_MAXPACKET) {
		DPFPRINTF(LOG_NOTICE, "max packet %d", fragoff + ip_len);
		goto bad;
	}
	max = fragoff + ip_len;

	/* Fully buffer all of the fragments */
	frag = pf_find_fragment(h, &pf_frag_tree);

	/* Check if we saw the last fragment already */
	if (frag != NULL && (frag->fr_flags & PFFRAG_SEENLAST) &&
	    max > frag->fr_max)
		goto bad;

	/* Get an entry for the fragment queue */
	frent = pool_get(&pf_frent_pl, PR_NOWAIT);
	if (frent == NULL) {
		REASON_SET(reason, PFRES_MEMORY);
a589 3
	pf_nfrents++;
	frent->fr_ip = h;
	frent->fr_m = m;
d592 1
a592 3
	DPFPRINTF(LOG_NOTICE,
	    "reass frag %d @@ %d-%d", h->ip_id, fragoff, max);
	if (pf_reassemble(m0, &frag, frent, mff, reason) != PF_PASS)
a613 11
	return (PF_DROP);

 bad:
	DPFPRINTF(LOG_NOTICE, "dropping bad fragment");

	/* Free associated fragments */
	if (frag != NULL)
		pf_free_fragment(frag);

	REASON_SET(reason, PFRES_FRAG);

@


1.127
log
@The reason accounting in pf_reassemble() was not correct.  Change
pf_reassemble() to return PF_DROP or PF_PASS and *m0 is NULL or the
reassembled packet.  In case of PF_DROP, the mbuf must be valid,
e.g. for logging, and will be freed later.  In case the reassembled
packet is too big, use the reassembled mbuf for PF_DROP.
ok henning@@ markus@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.126 2011/01/19 11:39:57 bluhm Exp $ */
d679 2
a680 1
	if (fragoff + (plen - off - sizeof(frag)) > IPV6_MAXPACKET)
@


1.126
log
@Give pf_normalize_ip() the same 3 way semantics as pf_test().
- PF_DROP, the packet is bad, the mbuf still exists and must be freed.
- PF_PASS and *m0 is NULL, the packet has been processed, not an error.
- PF_PASS and *m0 is not NULL, continue with packet processing.
This fixes a potential mbuf use after free.
ok henning@@ markus@@ mpf@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.125 2011/01/06 14:01:36 bluhm Exp $ */
d100 2
a101 2
struct mbuf		*pf_reassemble(struct mbuf **, struct pf_fragment **,
			    struct pf_frent *, int);
d240 1
a240 1
struct mbuf *
d242 1
a242 1
    struct pf_frent *frent, int mff)
d263 2
a264 1
			if (*frag == NULL)
d266 1
d307 1
a307 1
			goto drop_fragment;
d356 3
d361 1
a361 1
		return (NULL);
d376 1
a376 1
			return (NULL);
d381 1
a381 1
		return (NULL);
a385 6
	if ((frent->fr_ip->ip_hl << 2) + off > IP_MAXPACKET) {
		DPFPRINTF(LOG_NOTICE, "drop: too big: %d", off);
		pf_free_fragment(*frag);
		*frag = NULL;
		return (NULL);
	}
d411 1
d427 8
d436 1
a436 1
	return (m);
d438 2
d444 2
a445 2
	m_freem(m);
	return (NULL);
d525 1
a525 1
	/* Might return a completely reassembled mbuf, or NULL */
d527 4
a530 3
	    "reass frag %d @@ %d-%d\n", h->ip_id, fragoff, max);
	*m0 = m = pf_reassemble(m0, &frag, frent, mff);

@


1.125
log
@Put htons() around ip_randomid() for pf scrub random-id to make it
consistent with the network stack.
ok mcbride@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.124 2010/12/31 12:26:57 bluhm Exp $ */
d521 1
a521 1
		return (PF_DROP);
@


1.124
log
@Remove dead code from pf_norm.c.  The fragment cache is some leftover
from fragment crop.  PFFRAG_NOBUFFER and PFFRAG_DROP are never set.
pf_cache_pl and pf_cent_pl have no pool_get.
ok henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.123 2010/07/08 19:30:16 sthen Exp $ */
d1336 1
a1336 1
		h->ip_id = ip_randomid();
@


1.123
log
@Use correct alignment for scrub max-mss. Based on a diff from deraadt.
"that looks about right even though the offset calculation is pure
horror" claudio@@, ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.122 2010/07/02 02:40:16 blambert Exp $ */
a70 6
struct pf_frcache {
	LIST_ENTRY(pf_frcache) fr_next;
	uint16_t	fr_off;
	uint16_t	fr_end;
};

a71 3
#define PFFRAG_NOBUFFER	0x0002		/* Non-buffering fragment cache */
#define PFFRAG_DROP	0x0004		/* Drop all fragments */
#define BUFFER_FRAGMENTS(fr)	(!((fr)->fr_flags & PFFRAG_NOBUFFER))
d83 1
a83 6
#define fr_queue	fr_u.fru_queue
#define fr_cache	fr_u.fru_cache
	union {
		LIST_HEAD(pf_fragq, pf_frent) fru_queue;	/* buffering */
		LIST_HEAD(pf_cacheq, pf_frcache) fru_cache;	/* non-buf */
	} fr_u;
a86 1
TAILQ_HEAD(pf_cachequeue, pf_fragment)	pf_cachequeue;
d104 1
a104 1
struct pool		 pf_frent_pl, pf_frag_pl, pf_cache_pl, pf_cent_pl;
d106 1
a106 1
int			 pf_nfrents, pf_ncache;
a114 4
	pool_init(&pf_cache_pl, sizeof(struct pf_fragment), 0, 0, 0,
	    "pffrcache", NULL);
	pool_init(&pf_cent_pl, sizeof(struct pf_frcache), 0, 0, 0, "pffrcent",
	    NULL);
a119 2
	pool_sethardlimit(&pf_cache_pl, PFFRAG_FRCACHE_HIWAT, NULL, 0);
	pool_sethardlimit(&pf_cent_pl, PFFRAG_FRCENT_HIWAT, NULL, 0);
a121 1
	TAILQ_INIT(&pf_cachequeue);
a151 10
		KASSERT(BUFFER_FRAGMENTS(frag));
		if (frag->fr_timeout > expire)
			break;

		DPFPRINTF(LOG_NOTICE, "expiring %d(%p)", frag->fr_id, frag);
		pf_free_fragment(frag);
	}

	while ((frag = TAILQ_LAST(&pf_cachequeue, pf_cachequeue)) != NULL) {
		KASSERT(!BUFFER_FRAGMENTS(frag));
a156 2
		KASSERT(TAILQ_EMPTY(&pf_cachequeue) ||
		    TAILQ_LAST(&pf_cachequeue, pf_cachequeue) != frag);
a178 11


	goal = pf_ncache * 9 / 10;
	DPFPRINTF(LOG_NOTICE, "trying to free > %d cache entries",
	    pf_ncache - goal);
	while (goal < pf_ncache) {
		frag = TAILQ_LAST(&pf_cachequeue, pf_cachequeue);
		if (frag == NULL)
			break;
		pf_free_fragment(frag);
	}
a186 1
	struct pf_frcache	*frcache;
d189 3
a191 17
	if (BUFFER_FRAGMENTS(frag)) {
		for (frent = LIST_FIRST(&frag->fr_queue); frent;
		    frent = LIST_FIRST(&frag->fr_queue)) {
			LIST_REMOVE(frent, fr_next);

			m_freem(frent->fr_m);
			pool_put(&pf_frent_pl, frent);
			pf_nfrents--;
		}
	} else {
		for (frcache = LIST_FIRST(&frag->fr_cache); frcache;
		    frcache = LIST_FIRST(&frag->fr_cache)) {
			LIST_REMOVE(frcache, fr_next);

			KASSERT(LIST_EMPTY(&frag->fr_cache) ||
			    LIST_FIRST(&frag->fr_cache)->fr_off >
			    frcache->fr_end);
d193 3
a195 3
			pool_put(&pf_cent_pl, frcache);
			pf_ncache--;
		}
d222 2
a223 7
		if (BUFFER_FRAGMENTS(frag)) {
			TAILQ_REMOVE(&pf_fragqueue, frag, frag_next);
			TAILQ_INSERT_HEAD(&pf_fragqueue, frag, frag_next);
		} else {
			TAILQ_REMOVE(&pf_cachequeue, frag, frag_next);
			TAILQ_INSERT_HEAD(&pf_cachequeue, frag, frag_next);
		}
d234 3
a236 9
	if (BUFFER_FRAGMENTS(frag)) {
		RB_REMOVE(pf_frag_tree, &pf_frag_tree, frag);
		TAILQ_REMOVE(&pf_fragqueue, frag, frag_next);
		pool_put(&pf_frag_pl, frag);
	} else {
		RB_REMOVE(pf_frag_tree, &pf_cache_tree, frag);
		TAILQ_REMOVE(&pf_cachequeue, frag, frag_next);
		pool_put(&pf_cache_pl, frag);
	}
a252 2
	KASSERT(*frag == NULL || BUFFER_FRAGMENTS(*frag));

a521 3

	if (frag != NULL && (frag->fr_flags & PFFRAG_DROP))
		goto drop;
@


1.122
log
@m_copyback can fail to allocate memory, but is a void fucntion so gymnastics
are required to detect that.

Change the function to take a wait argument (used in nfs server, but
M_NOWAIT everywhere else for now) and to return an error

ok claudio@@ henning@@ krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.121 2010/01/18 23:52:46 mcbride Exp $ */
d1326 1
a1326 1
	u_int16_t	*mss;
d1354 2
a1355 2
			mss = (u_int16_t *)(optp + 2);
			if ((ntohs(*mss)) > maxmss) {
d1357 5
a1361 4
				    *mss, htons(maxmss), 0);
				*mss = htons(maxmss);
				m_copyback(m, off + sizeof(*th),
				    thoff - sizeof(*th), opts, M_NOWAIT);
@


1.121
log
@Convert pf debug logging to using log()/addlog(), a single standardised
definition of DPFPRINTF(), and log priorities from syslog.h. Old debug
levels will still work for now, but will eventually be phased out.

discussed with henning, ok dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.120 2009/09/01 15:51:06 jsing Exp $ */
d820 1
a820 1
		m_copyback(m, off, sizeof(*th), th);
d1043 1
a1043 1
			    sizeof(struct tcphdr));
d1360 2
a1361 2
				    thoff - sizeof(*th), opts);
				m_copyback(m, off, sizeof(*th), th);
@


1.120
log
@Clear the IP_DF bit if no-df is enabled, not if it is not enabled.

Issue reported by Matthew Dempsky. Same fix suggested by fgsch@@.

ok henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.118 2009/06/25 09:30:28 sthen Exp $ */
d40 1
a117 7
#define	DPFPRINTF(x) do {				\
	if (pf_status.debug >= PF_DEBUG_MISC) {		\
		printf("%s: ", __func__);		\
		printf x ;				\
	}						\
} while(0)

d178 1
a178 1
		DPFPRINTF(("expiring %d(%p)\n", frag->fr_id, frag));
d187 1
a187 1
		DPFPRINTF(("expiring %d(%p)\n", frag->fr_id, frag));
d205 2
a206 2
	DPFPRINTF(("trying to free > %d frents\n",
	    pf_nfrents - goal));
d216 2
a217 2
	DPFPRINTF(("trying to free > %d cache entries\n",
	    pf_ncache - goal));
d380 1
a380 1
		DPFPRINTF(("overlap -%d\n", precut));
d394 1
a394 1
		DPFPRINTF(("adjust overlap %d\n", aftercut));
d440 2
a441 1
			DPFPRINTF(("missing fragment at %d, next %d, max %d\n",
d443 1
a443 1
			    (*frag)->fr_max));
d447 1
a447 1
	DPFPRINTF(("%d < %d?\n", off, (*frag)->fr_max));
d455 1
a455 1
		DPFPRINTF(("drop: too big: %d\n", off));
d500 1
a500 1
	DPFPRINTF(("complete: %p(%d)\n", m, ntohs(ip->ip_len)));
d550 1
a550 1
		DPFPRINTF(("IP_DF\n"));
d559 1
a559 1
		DPFPRINTF(("mff and %d\n", ip_len));
d565 1
a565 1
		DPFPRINTF(("max packet %d\n", fragoff + ip_len));
d589 2
a590 1
	DPFPRINTF(("reass frag %d @@ %d-%d\n", h->ip_id, fragoff, max));
d618 1
a618 1
	DPFPRINTF(("dropping bad fragment\n"));
d991 4
a994 2
					if (pf_status.debug >= PF_DEBUG_MISC) {
						DPFPRINTF(("multiple TS??"));
d996 1
a996 1
						printf("\n");
d1062 2
a1063 2
		if (pf_status.debug >= PF_DEBUG_MISC) {
			DPFPRINTF(("src idled out of PAWS\n"));
d1065 1
a1065 1
			printf("\n");
d1072 2
a1073 2
		if (pf_status.debug >= PF_DEBUG_MISC) {
			DPFPRINTF(("dst idled out of PAWS\n"));
d1075 1
a1075 1
			printf("\n");
d1180 1
a1180 1
			DPFPRINTF(("Timestamp failed %c%c%c%c\n",
d1185 4
a1188 3
			    SEQ_LT(tsecr, dst->scrub->pfss_tsval0)? '3' : ' '));
			DPFPRINTF((" tsval: %lu  tsecr: %lu  +ticks: %lu  "
			    "idle: %lus %lums\n",
d1190 10
a1199 7
			    delta_ts.tv_usec / 1000));
			DPFPRINTF((" src->tsval: %lu  tsecr: %lu\n",
			    src->scrub->pfss_tsval, src->scrub->pfss_tsecr));
			DPFPRINTF((" dst->tsval: %lu  tsecr: %lu  tsval0: %lu"
			    "\n", dst->scrub->pfss_tsval,
			    dst->scrub->pfss_tsecr, dst->scrub->pfss_tsval0));
			if (pf_status.debug >= PF_DEBUG_MISC) {
d1202 1
a1202 1
				printf("\n");
d1247 4
a1250 3
			if (pf_status.debug >= PF_DEBUG_MISC) {
				DPFPRINTF(("Did not receive expected RFC1323 "
				    "timestamp\n"));
d1253 1
a1253 1
				printf("\n");
d1275 1
a1275 1
			if (pf_status.debug >= PF_DEBUG_MISC && dst->scrub &&
d1278 2
a1279 1
				DPFPRINTF(("Broken RFC1323 stack did not "
d1281 1
a1281 1
				    "security.\n"));
d1284 1
a1284 1
				printf("\n");
@


1.119
log
@pf_scrub_ip/ip6 prototypes are already in pfvar.h
@
text
@d539 1
a539 1
	if (!(pf_status.reass & PF_REASS_NODF) && h->ip_off & htons(IP_DF)) {
@


1.118
log
@scrub_flags is a u_int8_t, but PFSTATE_SCRUB_TCP is 0x0100, so the
"reassemble tcp" state option failed to work correctly. Increasing this
to u_int16_t fixes kernel/6178. ok deraadt@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.117 2009/04/07 13:26:23 henning Exp $ */
a115 5
void			 pf_scrub_ip(struct mbuf **, u_int16_t, u_int8_t,
			    u_int8_t);
#ifdef INET6
void			 pf_scrub_ip6(struct mbuf **, u_int8_t);
#endif
@


1.117
log
@after i took everything in this fiule apart and reassembled with a lot of
new stuff asserting copyright is in order
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.116 2009/04/06 12:05:55 henning Exp $ */
d116 1
a116 1
void			 pf_scrub_ip(struct mbuf **, u_int8_t, u_int8_t,
d1376 1
a1376 1
pf_scrub_ip(struct mbuf **m0, u_int8_t flags, u_int8_t min_ttl, u_int8_t tos)
@


1.116
log
@1) scrub rules are completely gone.
2) packet reassembly: only one method remains, full reassembly. crop
and drop-ovl are gone.
.  set reassemble yes|no [no-df]
if no-df is given fragments (and only fragments!) with the df bit set
have it cleared before entering the fragment cache, and thus the
reassembled packet doesn't have df set either. it does NOT touch
non-fragmented packets.
3) regular rules can have scrub options.
.  pass scrub(no-df, min-ttl 64, max-mss 1400, set-tos lowdelay)
.  match scrub(reassemble tcp, random-id)
of course all options are optional. the individual options still do
what they used to do on scrub rules, but everything is stateful now.
4) match rules
"match" is a new action, just like pass and block are, and can be used
like they do. opposed to pass or block, they do NOT change the
pass/block state of a packet. i. e.
.  pass
.  match
passes the packet, and
.  block
.  match
blocks it.
Every time (!) a match rule matches, i. e. not only when it is the
last matching rule, the following actions are set:
-queue assignment. can be overwritten later, the last rule that set a
queue wins. note how this is different from the last matching rule
wins, if the last matching rule has no queue assignments and the
second last matching rule was a match rule with queue assignments,
these assignments are taken.
-rtable assignments. works the same as queue assignments.
-set-tos, min-ttl, max-mss, no-df, random-id, reassemble tcp, all work
like the above
-logging. every matching rule causes the packet to be logged. this
 means a single packet can get logged more than once (think multiple log
 interfaces with different receivers, like pflogd and spamlogd)
.
almost entirely hacked at n2k9 in basel, could not be committed close to
release. this really should have been multiple diffs, but splitting them
now is not feasible any more. input from mcbride and dlg, and frantzen
about the fragment handling.
speedup around 7% for the common case, the more the more scrub rules
were in use.
manpage not up to date, being worked on.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.115 2009/01/31 20:06:55 henning Exp $ */
d5 1
@


1.115
log
@unbreak ! INET6 case by sprinking #ifdef INET6
noticed by Vladimir Kirillov <proger@@uaoug.org.ua>
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.114 2009/01/29 14:11:45 henning Exp $ */
d115 1
a115 5
struct mbuf		*pf_fragcache(struct mbuf **, struct ip*,
			    struct pf_fragment **, int, int, int *);
int			 pf_normalize_tcpopt(struct pf_rule *, struct mbuf *,
			    struct tcphdr *, int, sa_family_t);
void			 pf_scrub_ip(struct mbuf **, u_int32_t, u_int8_t,
d120 1
a519 296
struct mbuf *
pf_fragcache(struct mbuf **m0, struct ip *h, struct pf_fragment **frag, int mff,
    int drop, int *nomem)
{
	struct mbuf		*m = *m0;
	struct pf_frcache	*frp, *fra, *cur = NULL;
	int			 ip_len = ntohs(h->ip_len) - (h->ip_hl << 2);
	u_int16_t		 off = ntohs(h->ip_off) << 3;
	u_int16_t		 max = ip_len + off;
	int			 hosed = 0;

	KASSERT(*frag == NULL || !BUFFER_FRAGMENTS(*frag));

	/* Create a new range queue for this packet */
	if (*frag == NULL) {
		*frag = pool_get(&pf_cache_pl, PR_NOWAIT);
		if (*frag == NULL) {
			pf_flush_fragments();
			*frag = pool_get(&pf_cache_pl, PR_NOWAIT);
			if (*frag == NULL)
				goto no_mem;
		}

		/* Get an entry for the queue */
		cur = pool_get(&pf_cent_pl, PR_NOWAIT);
		if (cur == NULL) {
			pool_put(&pf_cache_pl, *frag);
			*frag = NULL;
			goto no_mem;
		}
		pf_ncache++;

		(*frag)->fr_flags = PFFRAG_NOBUFFER;
		(*frag)->fr_max = 0;
		(*frag)->fr_src = h->ip_src;
		(*frag)->fr_dst = h->ip_dst;
		(*frag)->fr_p = h->ip_p;
		(*frag)->fr_id = h->ip_id;
		(*frag)->fr_timeout = time_second;

		cur->fr_off = off;
		cur->fr_end = max;
		LIST_INIT(&(*frag)->fr_cache);
		LIST_INSERT_HEAD(&(*frag)->fr_cache, cur, fr_next);

		RB_INSERT(pf_frag_tree, &pf_cache_tree, *frag);
		TAILQ_INSERT_HEAD(&pf_cachequeue, *frag, frag_next);

		DPFPRINTF(("fragcache[%d]: new %d-%d\n", h->ip_id, off, max));

		goto pass;
	}

	/*
	 * Find a fragment after the current one:
	 *  - off contains the real shifted offset.
	 */
	frp = NULL;
	LIST_FOREACH(fra, &(*frag)->fr_cache, fr_next) {
		if (fra->fr_off > off)
			break;
		frp = fra;
	}

	KASSERT(frp != NULL || fra != NULL);

	if (frp != NULL) {
		int	precut;

		precut = frp->fr_end - off;
		if (precut >= ip_len) {
			/* Fragment is entirely a duplicate */
			DPFPRINTF(("fragcache[%d]: dead (%d-%d) %d-%d\n",
			    h->ip_id, frp->fr_off, frp->fr_end, off, max));
			goto drop_fragment;
		}
		if (precut == 0) {
			/* They are adjacent.  Fixup cache entry */
			DPFPRINTF(("fragcache[%d]: adjacent (%d-%d) %d-%d\n",
			    h->ip_id, frp->fr_off, frp->fr_end, off, max));
			frp->fr_end = max;
		} else if (precut > 0) {
			/* The first part of this payload overlaps with a
			 * fragment that has already been passed.
			 * Need to trim off the first part of the payload.
			 * But to do so easily, we need to create another
			 * mbuf to throw the original header into.
			 */

			DPFPRINTF(("fragcache[%d]: chop %d (%d-%d) %d-%d\n",
			    h->ip_id, precut, frp->fr_off, frp->fr_end, off,
			    max));

			off += precut;
			max -= precut;
			/* Update the previous frag to encompass this one */
			frp->fr_end = max;

			if (!drop) {
				/* XXX Optimization opportunity
				 * This is a very heavy way to trim the payload.
				 * we could do it much faster by diddling mbuf
				 * internals but that would be even less legible
				 * than this mbuf magic.  For my next trick,
				 * I'll pull a rabbit out of my laptop.
				 */
				*m0 = m_copym2(m, 0, h->ip_hl << 2, M_NOWAIT);
				if (*m0 == NULL)
					goto no_mem;
				KASSERT((*m0)->m_next == NULL);
				m_adj(m, precut + (h->ip_hl << 2));
				m_cat(*m0, m);
				m = *m0;
				if (m->m_flags & M_PKTHDR) {
					int plen = 0;
					struct mbuf *t;
					for (t = m; t; t = t->m_next)
						plen += t->m_len;
					m->m_pkthdr.len = plen;
				}


				h = mtod(m, struct ip *);


				KASSERT((int)m->m_len ==
				    ntohs(h->ip_len) - precut);
				h->ip_off = htons(ntohs(h->ip_off) +
				    (precut >> 3));
				h->ip_len = htons(ntohs(h->ip_len) - precut);
			} else {
				hosed++;
			}
		} else {
			/* There is a gap between fragments */

			DPFPRINTF(("fragcache[%d]: gap %d (%d-%d) %d-%d\n",
			    h->ip_id, -precut, frp->fr_off, frp->fr_end, off,
			    max));

			cur = pool_get(&pf_cent_pl, PR_NOWAIT);
			if (cur == NULL)
				goto no_mem;
			pf_ncache++;

			cur->fr_off = off;
			cur->fr_end = max;
			LIST_INSERT_AFTER(frp, cur, fr_next);
		}
	}

	if (fra != NULL) {
		int	aftercut;
		int	merge = 0;

		aftercut = max - fra->fr_off;
		if (aftercut == 0) {
			/* Adjacent fragments */
			DPFPRINTF(("fragcache[%d]: adjacent %d-%d (%d-%d)\n",
			    h->ip_id, off, max, fra->fr_off, fra->fr_end));
			fra->fr_off = off;
			merge = 1;
		} else if (aftercut > 0) {
			/* Need to chop off the tail of this fragment */
			DPFPRINTF(("fragcache[%d]: chop %d %d-%d (%d-%d)\n",
			    h->ip_id, aftercut, off, max, fra->fr_off,
			    fra->fr_end));
			fra->fr_off = off;
			max -= aftercut;

			merge = 1;

			if (!drop) {
				m_adj(m, -aftercut);
				if (m->m_flags & M_PKTHDR) {
					int plen = 0;
					struct mbuf *t;
					for (t = m; t; t = t->m_next)
						plen += t->m_len;
					m->m_pkthdr.len = plen;
				}
				h = mtod(m, struct ip *);
				KASSERT((int)m->m_len ==
				    ntohs(h->ip_len) - aftercut);
				h->ip_len = htons(ntohs(h->ip_len) - aftercut);
			} else {
				hosed++;
			}
		} else if (frp == NULL) {
			/* There is a gap between fragments */
			DPFPRINTF(("fragcache[%d]: gap %d %d-%d (%d-%d)\n",
			    h->ip_id, -aftercut, off, max, fra->fr_off,
			    fra->fr_end));

			cur = pool_get(&pf_cent_pl, PR_NOWAIT);
			if (cur == NULL)
				goto no_mem;
			pf_ncache++;

			cur->fr_off = off;
			cur->fr_end = max;
			LIST_INSERT_BEFORE(fra, cur, fr_next);
		}


		/* Need to glue together two separate fragment descriptors */
		if (merge) {
			if (cur && fra->fr_off <= cur->fr_end) {
				/* Need to merge in a previous 'cur' */
				DPFPRINTF(("fragcache[%d]: adjacent(merge "
				    "%d-%d) %d-%d (%d-%d)\n",
				    h->ip_id, cur->fr_off, cur->fr_end, off,
				    max, fra->fr_off, fra->fr_end));
				fra->fr_off = cur->fr_off;
				LIST_REMOVE(cur, fr_next);
				pool_put(&pf_cent_pl, cur);
				pf_ncache--;
				cur = NULL;

			} else if (frp && fra->fr_off <= frp->fr_end) {
				/* Need to merge in a modified 'frp' */
				KASSERT(cur == NULL);
				DPFPRINTF(("fragcache[%d]: adjacent(merge "
				    "%d-%d) %d-%d (%d-%d)\n",
				    h->ip_id, frp->fr_off, frp->fr_end, off,
				    max, fra->fr_off, fra->fr_end));
				fra->fr_off = frp->fr_off;
				LIST_REMOVE(frp, fr_next);
				pool_put(&pf_cent_pl, frp);
				pf_ncache--;
				frp = NULL;

			}
		}
	}

	if (hosed) {
		/*
		 * We must keep tracking the overall fragment even when
		 * we're going to drop it anyway so that we know when to
		 * free the overall descriptor.  Thus we drop the frag late.
		 */
		goto drop_fragment;
	}


 pass:
	/* Update maximum data size */
	if ((*frag)->fr_max < max)
		(*frag)->fr_max = max;

	/* This is the last segment */
	if (!mff)
		(*frag)->fr_flags |= PFFRAG_SEENLAST;

	/* Check if we are completely reassembled */
	if (((*frag)->fr_flags & PFFRAG_SEENLAST) &&
	    LIST_FIRST(&(*frag)->fr_cache)->fr_off == 0 &&
	    LIST_FIRST(&(*frag)->fr_cache)->fr_end == (*frag)->fr_max) {
		/* Remove from fragment queue */
		DPFPRINTF(("fragcache[%d]: done 0-%d\n", h->ip_id,
		    (*frag)->fr_max));
		pf_free_fragment(*frag);
		*frag = NULL;
	}

	return (m);

 no_mem:
	*nomem = 1;

	/* Still need to pay attention to !IP_MF */
	if (!mff && *frag != NULL)
		(*frag)->fr_flags |= PFFRAG_SEENLAST;

	m_freem(m);
	return (NULL);

 drop_fragment:

	/* Still need to pay attention to !IP_MF */
	if (!mff && *frag != NULL)
		(*frag)->fr_flags |= PFFRAG_SEENLAST;

	if (drop) {
		/* This fragment has been deemed bad.  Don't reass */
		if (((*frag)->fr_flags & PFFRAG_DROP) == 0)
			DPFPRINTF(("fragcache[%d]: dropping overall fragment\n",
			    h->ip_id));
		(*frag)->fr_flags |= PFFRAG_DROP;
	}

	m_freem(m);
	return (NULL);
}

a524 1
	struct pf_rule		*r;
a533 33
	int			 tag = -1;

	r = TAILQ_FIRST(pf_main_ruleset.rules[PF_RULESET_SCRUB].active.ptr);
	while (r != NULL) {
		r->evaluations++;
		if (pfi_kif_match(r->kif, kif) == r->ifnot)
			r = r->skip[PF_SKIP_IFP].ptr;
		else if (r->direction && r->direction != dir)
			r = r->skip[PF_SKIP_DIR].ptr;
		else if (r->af && r->af != AF_INET)
			r = r->skip[PF_SKIP_AF].ptr;
		else if (r->proto && r->proto != h->ip_p)
			r = r->skip[PF_SKIP_PROTO].ptr;
		else if (PF_MISMATCHAW(&r->src.addr,
		    (struct pf_addr *)&h->ip_src.s_addr, AF_INET,
		    r->src.neg, kif))
			r = r->skip[PF_SKIP_SRC_ADDR].ptr;
		else if (PF_MISMATCHAW(&r->dst.addr,
		    (struct pf_addr *)&h->ip_dst.s_addr, AF_INET,
		    r->dst.neg, NULL))
			r = r->skip[PF_SKIP_DST_ADDR].ptr;
		else if (r->match_tag && !pf_match_tag(m, r, &tag))
			r = TAILQ_NEXT(r, entries);
		else
			break;
	}

	if (r == NULL || r->action == PF_NOSCRUB)
		return (PF_PASS);
	else {
		r->packets[dir == PF_OUT]++;
		r->bytes[dir == PF_OUT] += pd->tot_len;
	}
d542 2
a543 2
	/* Clear IP_DF if the rule uses the no-df option */
	if (r->rule_flag & PFRULE_NODF && h->ip_off & htons(IP_DF)) {
d579 2
a580 2
	if ((r->rule_flag & (PFRULE_FRAGCROP|PFRULE_FRAGDROP)) == 0) {
		/* Fully buffer all of the fragments */
d582 4
a585 1
		frag = pf_find_fragment(h, &pf_frag_tree);
d587 13
a599 18
		/* Check if we saw the last fragment already */
		if (frag != NULL && (frag->fr_flags & PFFRAG_SEENLAST) &&
		    max > frag->fr_max)
			goto bad;

		/* Get an entry for the fragment queue */
		frent = pool_get(&pf_frent_pl, PR_NOWAIT);
		if (frent == NULL) {
			REASON_SET(reason, PFRES_MEMORY);
			return (PF_DROP);
		}
		pf_nfrents++;
		frent->fr_ip = h;
		frent->fr_m = m;

		/* Might return a completely reassembled mbuf, or NULL */
		DPFPRINTF(("reass frag %d @@ %d-%d\n", h->ip_id, fragoff, max));
		*m0 = m = pf_reassemble(m0, &frag, frent, mff);
d601 2
a602 2
		if (m == NULL)
			return (PF_DROP);
d604 2
a605 2
		if (frag != NULL && (frag->fr_flags & PFFRAG_DROP))
			goto drop;
d607 1
a607 39
		h = mtod(m, struct ip *);
	} else {
		/* non-buffering fragment cache (drops or masks overlaps) */
		int	nomem = 0;

		if (dir == PF_OUT && m->m_pkthdr.pf.flags & PF_TAG_FRAGCACHE) {
			/*
			 * Already passed the fragment cache in the
			 * input direction.  If we continued, it would
			 * appear to be a dup and would be dropped.
			 */
			goto fragment_pass;
		}

		frag = pf_find_fragment(h, &pf_cache_tree);

		/* Check if we saw the last fragment already */
		if (frag != NULL && (frag->fr_flags & PFFRAG_SEENLAST) &&
		    max > frag->fr_max) {
			if (r->rule_flag & PFRULE_FRAGDROP)
				frag->fr_flags |= PFFRAG_DROP;
			goto bad;
		}

		*m0 = m = pf_fragcache(m0, h, &frag, mff,
		    (r->rule_flag & PFRULE_FRAGDROP) ? 1 : 0, &nomem);
		if (m == NULL) {
			if (nomem)
				goto no_mem;
			goto drop;
		}

		if (dir == PF_IN)
			m->m_pkthdr.pf.flags |= PF_TAG_FRAGCACHE;

		if (frag != NULL && (frag->fr_flags & PFFRAG_DROP))
			goto drop;
		goto fragment_pass;
	}
d618 1
a618 7
	/* not missing a return here */

 fragment_pass:
	pf_scrub_ip(&m, r->rule_flag, r->min_ttl, r->set_tos);

	if ((r->rule_flag & (PFRULE_FRAGCROP|PFRULE_FRAGDROP)) == 0)
		pd->flags |= PFDESC_IP_REAS;
a620 6
 no_mem:
	REASON_SET(reason, PFRES_MEMORY);
	if (r != NULL && r->log)
		PFLOG_PACKET(kif, h, m, AF_INET, dir, *reason, r, NULL, NULL, pd);
	return (PF_DROP);

a622 2
	if (r != NULL && r->log)
		PFLOG_PACKET(kif, h, m, AF_INET, dir, *reason, r, NULL, NULL, pd);
a632 2
	if (r != NULL && r->log)
		PFLOG_PACKET(kif, h, m, AF_INET, dir, *reason, r, NULL, NULL, pd);
a642 1
	struct pf_rule		*r;
a655 32
	r = TAILQ_FIRST(pf_main_ruleset.rules[PF_RULESET_SCRUB].active.ptr);
	while (r != NULL) {
		r->evaluations++;
		if (pfi_kif_match(r->kif, kif) == r->ifnot)
			r = r->skip[PF_SKIP_IFP].ptr;
		else if (r->direction && r->direction != dir)
			r = r->skip[PF_SKIP_DIR].ptr;
		else if (r->af && r->af != AF_INET6)
			r = r->skip[PF_SKIP_AF].ptr;
#if 0 /* header chain! */
		else if (r->proto && r->proto != h->ip6_nxt)
			r = r->skip[PF_SKIP_PROTO].ptr;
#endif
		else if (PF_MISMATCHAW(&r->src.addr,
		    (struct pf_addr *)&h->ip6_src, AF_INET6,
		    r->src.neg, kif))
			r = r->skip[PF_SKIP_SRC_ADDR].ptr;
		else if (PF_MISMATCHAW(&r->dst.addr,
		    (struct pf_addr *)&h->ip6_dst, AF_INET6,
		    r->dst.neg, NULL))
			r = r->skip[PF_SKIP_DST_ADDR].ptr;
		else
			break;
	}

	if (r == NULL || r->action == PF_NOSCRUB)
		return (PF_PASS);
	else {
		r->packets[dir == PF_OUT]++;
		r->bytes[dir == PF_OUT] += pd->tot_len;
	}

a741 2
	pf_scrub_ip6(&m, r->min_ttl);

a760 2
	if (r != NULL && r->log)
		PFLOG_PACKET(kif, h, m, AF_INET6, dir, *reason, r, NULL, NULL, pd);
a764 2
	if (r != NULL && r->log)
		PFLOG_PACKET(kif, h, m, AF_INET6, dir, *reason, r, NULL, NULL, pd);
a768 2
	if (r != NULL && r->log)
		PFLOG_PACKET(kif, h, m, AF_INET6, dir, *reason, r, NULL, NULL, pd);
a776 1
	struct pf_rule	*r, *rm = NULL;
a777 1
	int		 rewrite = 0;
d780 1
a780 44
	sa_family_t	 af = pd->af;

	r = TAILQ_FIRST(pf_main_ruleset.rules[PF_RULESET_SCRUB].active.ptr);
	while (r != NULL) {
		r->evaluations++;
		if (pfi_kif_match(r->kif, kif) == r->ifnot)
			r = r->skip[PF_SKIP_IFP].ptr;
		else if (r->direction && r->direction != dir)
			r = r->skip[PF_SKIP_DIR].ptr;
		else if (r->af && r->af != af)
			r = r->skip[PF_SKIP_AF].ptr;
		else if (r->proto && r->proto != pd->proto)
			r = r->skip[PF_SKIP_PROTO].ptr;
		else if (PF_MISMATCHAW(&r->src.addr, pd->src, af,
		    r->src.neg, kif))
			r = r->skip[PF_SKIP_SRC_ADDR].ptr;
		else if (r->src.port_op && !pf_match_port(r->src.port_op,
			    r->src.port[0], r->src.port[1], th->th_sport))
			r = r->skip[PF_SKIP_SRC_PORT].ptr;
		else if (PF_MISMATCHAW(&r->dst.addr, pd->dst, af,
		    r->dst.neg, NULL))
			r = r->skip[PF_SKIP_DST_ADDR].ptr;
		else if (r->dst.port_op && !pf_match_port(r->dst.port_op,
			    r->dst.port[0], r->dst.port[1], th->th_dport))
			r = r->skip[PF_SKIP_DST_PORT].ptr;
		else if (r->os_fingerprint != PF_OSFP_ANY && !pf_osfp_match(
			    pf_osfp_fingerprint(pd, m, off, th),
			    r->os_fingerprint))
			r = TAILQ_NEXT(r, entries);
		else {
			rm = r;
			break;
		}
	}

	if (rm == NULL || rm->action == PF_NOSCRUB)
		return (PF_PASS);
	else {
		r->packets[dir == PF_OUT]++;
		r->bytes[dir == PF_OUT] += pd->tot_len;
	}

	if (rm->rule_flag & PFRULE_REASSEMBLE_TCP)
		pd->flags |= PFDESC_TCP_NORM;
a825 4
	/* Process options */
	if (r->max_mss && pf_normalize_tcpopt(r, m, th, off, pd->af))
		rewrite = 1;

a833 2
	if (rm != NULL && r->log)
		PFLOG_PACKET(kif, h, m, AF_INET, dir, reason, r, NULL, NULL, pd);
d1323 1
a1323 2
pf_normalize_tcpopt(struct pf_rule *r, struct mbuf *m, struct tcphdr *th,
    int off, sa_family_t af)
d1325 1
a1328 1
	int		 rewrite = 0;
d1336 2
a1337 2
	    NULL, NULL, af))
		return (rewrite);
d1355 1
a1355 1
			if ((ntohs(*mss)) > r->max_mss) {
d1357 5
a1361 3
				    *mss, htons(r->max_mss), 0);
				*mss = htons(r->max_mss);
				rewrite = 1;
a1368 2
	if (rewrite)
		m_copyback(m, off + sizeof(*th), thoff - sizeof(*th), opts);
d1370 2
a1371 1
	return (rewrite);
d1375 1
a1375 1
pf_scrub_ip(struct mbuf **m0, u_int32_t flags, u_int8_t min_ttl, u_int8_t tos)
d1381 1
a1381 1
	if (flags & PFRULE_NODF && h->ip_off & htons(IP_DF)) {
d1397 1
a1397 1
	if (flags & PFRULE_SET_TOS) {
d1408 1
a1408 1
	if (flags & PFRULE_RANDOMID && !(h->ip_off & ~htons(IP_DF))) {
@


1.114
log
@move some code around in preparation for future work:
break out the code that doesn't deal with fragment reassembly and only
modifies stuff in the ip header to their own functions. pass them what they
need instead of making them get the info from a rule ptr.
ok dlg ryan
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.113 2008/05/07 07:07:29 markus Exp $ */
d121 1
d123 1
d1897 1
d1908 1
@


1.113
log
@scrub packets based on tags; ok henning
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.112 2008/05/07 06:23:30 markus Exp $ */
d119 3
a121 1

d989 1
a989 29
	/* Enforce a minimum ttl, may cause endless packet loops */
	if (r->min_ttl && h->ip_ttl < r->min_ttl) {
		u_int16_t ip_ttl = h->ip_ttl;

		h->ip_ttl = r->min_ttl;
		h->ip_sum = pf_cksum_fixup(h->ip_sum, ip_ttl, h->ip_ttl, 0);
	}

	/* Enforce tos */
	if (r->rule_flag & PFRULE_SET_TOS) {
		u_int16_t	ov, nv;

		ov = *(u_int16_t *)h;
		h->ip_tos = r->set_tos;
		nv = *(u_int16_t *)h;

		h->ip_sum = pf_cksum_fixup(h->ip_sum, ov, nv, 0);
	}

	if (r->rule_flag & PFRULE_RANDOMID) {
		u_int16_t ip_id = h->ip_id;

		h->ip_id = ip_randomid();
		h->ip_sum = pf_cksum_fixup(h->ip_sum, ip_id, h->ip_id, 0);
	}
	if ((r->rule_flag & (PFRULE_FRAGCROP|PFRULE_FRAGDROP)) == 0)
		pd->flags |= PFDESC_IP_REAS;

	return (PF_PASS);
d992 1
a992 14
	/* Enforce a minimum ttl, may cause endless packet loops */
	if (r->min_ttl && h->ip_ttl < r->min_ttl) {
		u_int16_t ip_ttl = h->ip_ttl;

		h->ip_ttl = r->min_ttl;
		h->ip_sum = pf_cksum_fixup(h->ip_sum, ip_ttl, h->ip_ttl, 0);
	}
	/* Enforce tos */
	if (r->rule_flag & PFRULE_SET_TOS) {
		u_int16_t	ov, nv;

		ov = *(u_int16_t *)h;
		h->ip_tos = r->set_tos;
		nv = *(u_int16_t *)h;
a993 2
		h->ip_sum = pf_cksum_fixup(h->ip_sum, ov, nv, 0);
	}
d1162 1
a1162 3
	/* Enforce a minimum ttl, may cause endless packet loops */
	if (r->min_ttl && h->ip6_hlim < r->min_ttl)
		h->ip6_hlim = r->min_ttl;
d1851 53
@


1.112
log
@allow setting TOS with scrub; ok mcbride, claudio
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.111 2007/12/30 10:32:24 mglocker Exp $ */
d830 1
d851 2
@


1.111
log
@In pf_normalize_tcpopt() call pf_pull_hdr() address family safe.

OK dhartmei@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.110 2007/12/30 00:16:39 mglocker Exp $ */
d992 11
d1021 10
@


1.110
log
@Make "scrub max-mss" rule work correctly;

In pf_normalize_tcpopt() pull the TCP options before processing them.
This gets the correct TCP options even if an mbuf chain was used, instead
like now pointing into an invalid mbuf data buffer.

Will close PR 5623.  Diff done together with dhartmei@@.

OK dhartmei@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.109 2007/05/28 17:16:39 henning Exp $ */
d118 1
a118 1
			    struct tcphdr *, int);
d1319 1
a1319 1
	if (r->max_mss && pf_normalize_tcpopt(r, m, th, off))
d1822 1
a1822 1
    int off)
d1835 1
a1835 1
	    NULL, NULL, AF_INET))
@


1.109
log
@double pf performance.
boring details:
pf used to use an mbuf tag to keep track of route-to etc, altq, tags,
routing table IDs, packets redirected to localhost etc. so each and every
packet going through pf got an mbuf tag. mbuf tags use malloc'd memory,
and that is knda slow.
instead, stuff the information into the mbuf header directly.
bridging soekris with just "pass" as ruleset went from 29 MBit/s to
58 MBit/s with that (before ryan's randomness fix, now it is even betterer)
thanks to chris for the test setup!
ok ryan ryan ckuethe reyk
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.108 2007/05/26 00:36:03 krw Exp $ */
d1828 2
a1829 1
	u_char		*optp;
d1833 4
a1836 1
	optp = mtod(m, caddr_t) + off + sizeof(struct tcphdr);
d1865 3
@


1.108
log
@More comment typos from Diego Casati. Including winners like funtion, allmost,
oustside, seqencer, toghether, nessissary, etc.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.107 2006/04/16 00:59:52 pascoe Exp $ */
a931 12
		/* use mtag from concatenated mbuf chain */
		pd->pf_mtag = pf_find_mtag(m);
#ifdef DIAGNOSTIC
		if (pd->pf_mtag == NULL) {
			printf("%s: pf_find_mtag returned NULL(1)\n", __func__);
			if ((pd->pf_mtag = pf_get_mtag(m)) == NULL) {
				m_freem(m);
				*m0 = NULL;
				goto no_mem;
			}
		}
#endif
d940 1
a940 1
		if (dir == PF_OUT && pd->pf_mtag->flags & PF_TAG_FRAGCACHE) {
a966 12
		/* use mtag from copied and trimmed mbuf chain */
		pd->pf_mtag = pf_find_mtag(m);
#ifdef DIAGNOSTIC
		if (pd->pf_mtag == NULL) {
			printf("%s: pf_find_mtag returned NULL(2)\n", __func__);
			if ((pd->pf_mtag = pf_get_mtag(m)) == NULL) {
				m_freem(m);
				*m0 = NULL;
				goto no_mem;
			}
		}
#endif
d968 1
a968 1
			pd->pf_mtag->flags |= PF_TAG_FRAGCACHE;
@


1.107
log
@After fragment reassembly/trimming, pf must revalidate the mbuf tag of the
altered chain.  The cached tag may have already been freed via m_cat.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.106 2006/03/25 20:55:24 dhartmei Exp $ */
d1661 1
a1661 1
		 *    the TS echo will never be less than the orginal
@


1.106
log
@fixup IP checksum when modifying IP header fields, based on a patch in
fbsd PR 93849 from Max Laier, ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.105 2006/03/14 11:09:42 djm Exp $ */
d932 12
d979 12
@


1.105
log
@implement a Unicast Reverse Path Forwarding (uRPF) check for pf(4)
which optionally verifies that a packet is received on the interface
that holds the route back to the packet's source address. This makes
it an automatic ingress filter, but only when routing is fully
symmetric.

bugfix feedback claudio@@; ok claudio@@ and dhartmei@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.104 2006/01/18 22:03:21 dhartmei Exp $ */
d869 3
a871 1
	if (r->rule_flag & PFRULE_NODF)
d873 2
d977 6
a982 1
	h->ip_off &= htons(IP_DF);
d985 3
a987 1
	if (r->min_ttl && h->ip_ttl < r->min_ttl)
d989 2
d1005 3
a1007 1
	if (r->min_ttl && h->ip_ttl < r->min_ttl)
d1009 2
@


1.104
log
@fix a bug in the fragment cache (used for 'scrub fragment crop/drop-ovl',
but not 'fragment reassemble'), which can cause some fragments to get
inserted into the cache twice, thereby violating an invariant, and panic-
ing the system subsequently. ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.103 2005/10/17 08:43:35 henning Exp $ */
d843 2
a844 1
		    (struct pf_addr *)&h->ip_src.s_addr, AF_INET, r->src.neg))
d847 2
a848 1
		    (struct pf_addr *)&h->ip_dst.s_addr, AF_INET, r->dst.neg))
d1058 2
a1059 1
		    (struct pf_addr *)&h->ip6_src, AF_INET6, r->src.neg))
d1062 2
a1063 1
		    (struct pf_addr *)&h->ip6_dst, AF_INET6, r->dst.neg))
d1225 2
a1226 1
		else if (PF_MISMATCHAW(&r->src.addr, pd->src, af, r->src.neg))
d1231 2
a1232 1
		else if (PF_MISMATCHAW(&r->dst.addr, pd->dst, af, r->dst.neg))
@


1.104.2.1
log
@MFC:
Fix by dhartmei@@

fixup IP checksum when modifying IP header fields, based on a patch in
fbsd PR 93849 from Max Laier
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.104 2006/01/18 22:03:21 dhartmei Exp $ */
d867 1
a867 3
	if (r->rule_flag & PFRULE_NODF && h->ip_off & htons(IP_DF)) {
		u_int16_t ip_off = h->ip_off;

a868 2
		h->ip_sum = pf_cksum_fixup(h->ip_sum, ip_off, h->ip_off, 0);
	}
d971 1
a971 6
	if (h->ip_off & ~htons(IP_DF)) {
		u_int16_t ip_off = h->ip_off;

		h->ip_off &= htons(IP_DF);
		h->ip_sum = pf_cksum_fixup(h->ip_sum, ip_off, h->ip_off, 0);
	}
d974 1
a974 3
	if (r->min_ttl && h->ip_ttl < r->min_ttl) {
		u_int16_t ip_ttl = h->ip_ttl;

a975 2
		h->ip_sum = pf_cksum_fixup(h->ip_sum, ip_ttl, h->ip_ttl, 0);
	}
d990 1
a990 3
	if (r->min_ttl && h->ip_ttl < r->min_ttl) {
		u_int16_t ip_ttl = h->ip_ttl;

a991 2
		h->ip_sum = pf_cksum_fixup(h->ip_sum, ip_ttl, h->ip_ttl, 0);
	}
@


1.104.2.2
log
@MFC:
Fix by pascoe@@

After fragment reassembly/trimming, pf must revalidate the mbuf tag of the
altered chain.  The cached tag may have already been freed via m_cat.

ok pascoe@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.104.2.1 2006/05/02 22:00:04 brad Exp $ */
a929 12
		/* use mtag from concatenated mbuf chain */
		pd->pf_mtag = pf_find_mtag(m);
#ifdef DIAGNOSTIC
		if (pd->pf_mtag == NULL) {
			printf("%s: pf_find_mtag returned NULL(1)\n", __func__);
			if ((pd->pf_mtag = pf_get_mtag(m)) == NULL) {
				m_freem(m);
				*m0 = NULL;
				goto no_mem;
			}
		}
#endif
a964 12
		/* use mtag from copied and trimmed mbuf chain */
		pd->pf_mtag = pf_find_mtag(m);
#ifdef DIAGNOSTIC
		if (pd->pf_mtag == NULL) {
			printf("%s: pf_find_mtag returned NULL(2)\n", __func__);
			if ((pd->pf_mtag = pf_get_mtag(m)) == NULL) {
				m_freem(m);
				*m0 = NULL;
				goto no_mem;
			}
		}
#endif
@


1.103
log
@make pf use one mbuf tag instead of 6 distinct ones. use a little struct
in the data part for the data from the previously distinct tags.
look up the tag early and carry a pointer to it around.
makes the code easier and saves some tag lookups and thus helps performance,
as proven by tests run by Schberle Dniel <Schoeberle.Daniel@@aamtech.hu>
Initially hacked up somewhere over the atlantic ocean in an A330
early testing reyk and moritz, "put it in" theo
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.102 2005/08/06 12:11:09 pascoe Exp $ */
d707 1
a707 1
		} else {
@


1.102
log
@correct some spellos
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.101 2005/06/13 20:17:25 henning Exp $ */
d934 7
a940 9
		if (dir == PF_OUT) {
			if (m_tag_find(m, PACKET_TAG_PF_FRAGCACHE, NULL) !=
			    NULL) {
				/* Already passed the fragment cache in the
				 * input direction.  If we continued, it would
				 * appear to be a dup and would be dropped.
				 */
				goto fragment_pass;
			}
d961 2
a962 2
		if (dir == PF_IN) {
			struct m_tag	*mtag;
a963 5
			mtag = m_tag_get(PACKET_TAG_PF_FRAGCACHE, 0, M_NOWAIT);
			if (mtag == NULL)
				goto no_mem;
			m_tag_prepend(m, mtag);
		}
@


1.102.2.1
log
@MFC:
Fix by dhartmei@@

fix a bug in the fragment cache (used for 'scrub fragment crop/drop-ovl',
but not 'fragment reassemble'), which can cause some fragments to get
inserted into the cache twice, thereby violating an invariant, and panic-
ing the system subsequently.

ok deraadt@@ dhartmei@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.102 2005/08/06 12:11:09 pascoe Exp $ */
d707 1
a707 1
		} else if (frp == NULL) {
@


1.102.2.2
log
@MFC:
Fix by dhartmei@@

fixup IP checksum when modifying IP header fields, based on a patch in
fbsd PR 93849 from Max Laier
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.102.2.1 2006/01/19 21:52:53 brad Exp $ */
d867 1
a867 3
	if (r->rule_flag & PFRULE_NODF && h->ip_off & htons(IP_DF)) {
		u_int16_t ip_off = h->ip_off;

a868 2
		h->ip_sum = pf_cksum_fixup(h->ip_sum, ip_off, h->ip_off, 0);
	}
d978 1
a978 6
	if (h->ip_off & ~htons(IP_DF)) {
		u_int16_t ip_off = h->ip_off;

		h->ip_off &= htons(IP_DF);
		h->ip_sum = pf_cksum_fixup(h->ip_sum, ip_off, h->ip_off, 0);
	}
d981 1
a981 3
	if (r->min_ttl && h->ip_ttl < r->min_ttl) {
		u_int16_t ip_ttl = h->ip_ttl;

a982 2
		h->ip_sum = pf_cksum_fixup(h->ip_sum, ip_ttl, h->ip_ttl, 0);
	}
d997 1
a997 3
	if (r->min_ttl && h->ip_ttl < r->min_ttl) {
		u_int16_t ip_ttl = h->ip_ttl;

a998 2
		h->ip_sum = pf_cksum_fixup(h->ip_sum, ip_ttl, h->ip_ttl, 0);
	}
@


1.101
log
@make the packet and byte counters on rules and src nodes per direction,
matches the counters on states now. also fix the counting on scrub rules
where we previously did not handle the byte counters at all.
extend pfctl -sl output to include the new seperate in/out counters
hacked on the ferry from Earls Cove to Saltery Bay
ok ryan
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.100 2005/05/27 17:22:41 dhartmei Exp $ */
d415 1
a415 1
		/* This fragment is completely overlapped, loose it */
d1749 1
a1749 1
	 * TCP streams immedietly after the 3whs and don't timestamp their
@


1.100
log
@log two pairs of uid/pid through pflog: the uid/pid of the process that
inserted the rule which causes the logging. secondly, the uid/pid of the
process in case the logged packet is delivered to/from a local socket.
a lookup of the local socket can be forced for logged packets with a new
option, 'log (user)'. make tcpdump print the additional information when
-e and -v is used. note: this changes the pflog header struct, rebuild all
dependancies. ok bob@@, henning@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.99 2005/05/22 16:22:41 dhartmei Exp $ */
d854 4
a857 2
	else
		r->packets++;
d1074 4
a1077 2
	else
		r->packets++;
d1250 4
a1253 2
	else
		r->packets++;
@


1.99
log
@honour the 'no' in 'no scrub' rules for IP normalizations. found by
mzozd at ad2u dot gr. ok henning@@, mcbride@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.98 2005/05/21 21:03:57 henning Exp $ */
d1004 1
a1004 1
		PFLOG_PACKET(kif, h, m, AF_INET, dir, *reason, r, NULL, NULL);
d1010 1
a1010 1
		PFLOG_PACKET(kif, h, m, AF_INET, dir, *reason, r, NULL, NULL);
d1022 1
a1022 1
		PFLOG_PACKET(kif, h, m, AF_INET, dir, *reason, r, NULL, NULL);
d1185 1
a1185 1
		PFLOG_PACKET(kif, h, m, AF_INET6, dir, *reason, r, NULL, NULL);
d1191 1
a1191 1
		PFLOG_PACKET(kif, h, m, AF_INET6, dir, *reason, r, NULL, NULL);
d1197 1
a1197 1
		PFLOG_PACKET(kif, h, m, AF_INET6, dir, *reason, r, NULL, NULL);
d1309 1
a1309 1
		PFLOG_PACKET(kif, h, m, AF_INET, dir, reason, r, NULL, NULL);
@


1.98
log
@clean up and rework the interface absraction code big time, rip out multiple
useless layers of indirection and make the code way cleaner overall.
this is just the start, more to come...
worked very hard on by Ryan and me in Montreal last week, on the airplane to
vancouver and yesterday here in calgary. it hurt.
ok ryan theo
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.97 2004/09/21 16:59:12 aaron Exp $ */
d852 1
a852 1
	if (r == NULL)
d1070 1
a1070 1
	if (r == NULL)
@


1.97
log
@Implement "no scrub" to allow exclusion of specific traffic from scrub rules.
First match wins, just like "no {binat,nat,rdr}".  henning@@, dhartmei@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.96 2004/07/17 00:17:27 frantzen Exp $ */
d834 1
a834 2
		if (r->kif != NULL &&
		    (r->kif != kif && r->kif != kif->pfik_parent) == !r->ifnot)
d1050 1
a1050 2
		if (r->kif != NULL &&
		    (r->kif != kif && r->kif != kif->pfik_parent) == !r->ifnot)
d1216 1
a1216 2
		if (r->kif != NULL &&
		    (r->kif != kif && r->kif != kif->pfik_parent) == !r->ifnot)
@


1.97.2.1
log
@MFC:
Fix by dhartmei@@

fix a bug in the fragment cache (used for 'scrub fragment crop/drop-ovl',
but not 'fragment reassemble'), which can cause some fragments to get
inserted into the cache twice, thereby violating an invariant, and panic-
ing the system subsequently.

ok deraadt@@ dhartmei@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.97 2004/09/21 16:59:12 aaron Exp $ */
d707 1
a707 1
		} else if (frp == NULL) {
@


1.96
log
@Repair breakage from the hackathon's time conversion.  Using the timestamp
as an extension to the sequence number got disabled because of the failing idle
limit on PAWS checks.  One more thing off my todo list.  I need an intern
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.95 2004/07/11 15:54:21 itojun Exp $ */
d1247 1
a1247 1
	if (rm == NULL)
@


1.95
log
@backout IPv6 reass-on-scrub patch (more work needs to be done).
requested by deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.90 2004/06/24 19:35:25 tholo Exp $ */
d1385 1
a1385 1
					microuptime(&src->scrub->pfss_last);
d1415 1
d1542 2
d1545 1
a1545 1
	    (time_uptime - src->scrub->pfss_last.tv_sec > TS_MAX_IDLE ||
d1556 1
a1556 1
	    time_uptime - dst->scrub->pfss_last.tv_sec > TS_MAX_IDLE) {
d1630 1
a1630 1
		struct timeval delta_ts, mtv;
d1647 1
a1647 2
		microuptime(&mtv);
		timersub(&mtv, &src->scrub->pfss_last, &delta_ts);
d1774 1
a1774 1
		microuptime(&src->scrub->pfss_last);
@


1.94
log
@KNF
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.93 2004/07/03 05:57:12 itojun Exp $ */
a58 1
#include <netinet6/ip6_var.h>
a106 19
#ifdef INET6
struct pf_fragment6 {
	TAILQ_ENTRY(pf_fragment6) next;

	/* packets received, connected by m->m_nextpkt */
	struct mbuf *m;

	/* search key */
	struct in6_addr src, dst;
	u_int32_t ident;

	struct timeout lifetime;
};

TAILQ_HEAD(pf_frag_tree6, pf_fragment6) pf_frag_tree6;

struct pool pf_fragment6_pl;
#endif

a118 7
#ifdef INET6
void			 pf_ip2key6(struct pf_fragment6 *, struct ip6_hdr *,
			    struct ip6_frag *);
struct pf_fragment6	*pf_find_fragment6(u_int32_t, struct in6_addr *,
			    struct in6_addr *);
static void		 pf_frag6_expire(void *);
#endif
a145 5
#ifdef INET6
	pool_init(&pf_fragment6_pl, sizeof(struct pf_fragment6), 0, 0, 0,
	    "pffrag6", NULL);
#endif

a1028 13
struct pf_fragment6 *
pf_find_fragment6(u_int32_t ident, struct in6_addr *src, struct in6_addr *dst)
{
	struct pf_fragment6 *p;

	TAILQ_FOREACH(p, &pf_frag_tree6, next) {
		if (p->ident == ident && IN6_ARE_ADDR_EQUAL(&p->src, src) &&
		    IN6_ARE_ADDR_EQUAL(&p->dst, dst))
			return p;
	}
	return NULL;
}

d1031 1
a1031 1
    u_short *reason, struct pf_pdesc *pd, struct mbuf **tree)
d1033 1
a1033 1
	struct mbuf		*m = *m0, *n;
d1042 1
a1045 1
	int			 protooff;
a1046 7
	int			 nxt;
	struct pf_fragment6	*frag6;

#if 1
	if (dir == PF_FORWARD)
		return (PF_PASS);
#endif
a1082 1
	protooff = offsetof(struct ip6_hdr, ip6_nxt);
a1086 8
			if (m_tag_find(m, PACKET_TAG_PF_FRAGCACHE, NULL)
			    != NULL) {
				/*
				 * the fragment have already passed the
				 * "scrub in".  no need to go to reass code
				 */
				goto frag_scrub;
			}
a1099 1
			protooff = off + offsetof(struct ip6_ext, ip6e_nxt);
a1145 1
			protooff = off + offsetof(struct ip6_ext, ip6e_nxt);
a1166 1
 frag_scrub:
d1170 2
a1171 1
	/* jumbo payload packets cannot be fragmented */
a1172 4
	if (plen == 0 || jumbolen)
		goto drop;

	m->m_nextpkt = NULL;
d1176 3
d1180 2
a1181 33
	frag6 = pf_find_fragment6(frag.ip6f_ident, &h->ip6_src, &h->ip6_dst);
	if (frag6 == NULL) {
		/* fresh fragment id/src/dst tuple */
		frag6 = pool_get(&pf_fragment6_pl, PR_NOWAIT);
		bzero(frag6, sizeof(*frag6));
		frag6->ident = frag.ip6f_ident;
		frag6->src = h->ip6_src;
		frag6->dst = h->ip6_dst;
		timeout_set(&frag6->lifetime, pf_frag6_expire, frag6);
		timeout_add(&frag6->lifetime, hz * 60);
		TAILQ_INSERT_HEAD(&pf_frag_tree6, frag6, next);
	} else
		; /* i saw this id/src/dst tuple in the past */

	/* to be passed to the caller as is */
	n = m_copym(m, 0, M_COPYALL, M_DONTWAIT);
	if (n) {
		n->m_nextpkt = frag6->m;
		frag6->m = n;
	}
	n = NULL;

	nxt = frag6_input(m0, &off, IPPROTO_FRAGMENT);
	if (nxt == IPPROTO_DONE) {
		*m0 = m = NULL;
		goto drop;
	}

	TAILQ_REMOVE(&pf_frag_tree6, frag6, next);
	*tree = frag6->m;
	timeout_del(&frag6->lifetime);
	pool_put(&pf_fragment6_pl, frag6);

a1194 1
}
d1196 5
a1200 14
static void
pf_frag6_expire(void *arg)
{
	struct pf_fragment6 *frag6 = (struct pf_fragment6 *)arg;
	struct mbuf *n;

	TAILQ_REMOVE(&pf_frag_tree6, frag6, next);

	for (; frag6->m; frag6->m = n) {
		n = frag6->m->m_nextpkt;
		frag6->m->m_nextpkt = NULL;
		m_freem(frag6->m);
	}
	pool_put(&pf_fragment6_pl, frag6);
@


1.93
log
@quick workaround until proper PF_FORWARD reass gets implemented.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.92 2004/06/25 11:04:03 itojun Exp $ */
d1487 1
a1487 1
		    			src->scrub->pfss_tsval0 = ntohl(tsval);
d1771 1
a1771 1
			        tsval_from_last) ? '1' : ' ',
d1774 2
a1775 1
			DPFPRINTF((" tsval: %lu  tsecr: %lu  +ticks: %lu  idle: %lus %lums\n",
d1780 3
a1782 3
			DPFPRINTF((" dst->tsval: %lu  tsecr: %lu  tsval0: %lu\n",
			    dst->scrub->pfss_tsval, dst->scrub->pfss_tsecr,
			    dst->scrub->pfss_tsval0));
d1832 2
a1833 1
				DPFPRINTF(("Did not receive expected RFC1323 timestamp\n"));
d1861 3
a1863 1
				DPFPRINTF(("Broken RFC1323 stack did not timestamp data packet.  Disabled PAWS security.\n"));
d1891 1
a1891 1
	    			src->scrub->pfss_tsval0 = tsval;
@


1.92
log
@correct "scrub in" behavior for IPv6.
remaining TODO:
- "forward" case kernel behavior (IPv4 too), then pfctl syntax change
- red-black tree
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.91 2004/06/25 00:42:58 itojun Exp $ */
d1094 5
@


1.91
log
@IPv6 reassembly on "scrub" directive.

caveats: (to be addressed soon)
- "scrub in" should queue fragments back into ip6intrq again, but
  somehow it does not happen - the packet is kept inside reass queue.
  need investigation
- ip6_forwarding path is not tested
- does not use red-black tree.  somehow red-black tree behaved badly
  and was not robust.  performance issue, the above one is more
  important.

good things:
- "scrub out" is perfectly ok
- i think now we can inspect upper-layer protocol fields (tcp port)
  even if ip6 packet is fragmented.
- reass queue will be cleaned up properly by timeout (60sec).  we might
  want to impose pool limit as well
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.90 2004/06/24 19:35:25 tholo Exp $ */
d1141 1
a1141 2
				terminal = 1;
				break;
d1225 1
@


1.90
log
@This moves access to wall and uptime variables in MI code,
encapsulating all such access into wall-defined functions
that makes sure locking is done as needed.

It also cleans up some uses of wall time vs. uptime some
places, but there is sure to be more of these needed as
well, particularily in MD code.  Also, many current calls
to microtime() should probably be changed to getmicrotime(),
or to the {,get}microuptime() versions.

ok art@@ deraadt@@ aaron@@ matthieu@@ beck@@ sturm@@ millert@@ others
"Oh, that is not your problem!" from miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.89 2004/06/21 23:50:36 tholo Exp $ */
d59 1
d108 19
d139 7
d173 5
d1061 13
d1076 1
a1076 1
    u_short *reason, struct pf_pdesc *pd)
d1078 1
a1078 1
	struct mbuf		*m = *m0;
a1086 1
	u_int16_t		 fragoff = 0;
d1090 1
d1092 2
d1130 1
d1135 9
d1157 1
d1204 1
d1229 3
a1231 1
	if (ntohs(h->ip6_plen) == 0 || jumbolen)
d1233 2
a1234 1
	plen = ntohs(h->ip6_plen);
a1237 3
	fragoff = ntohs(frag.ip6f_offlg & IP6F_OFF_MASK);
	if (fragoff + (plen - off - sizeof(frag)) > IPV6_MAXPACKET)
		goto badfrag;
d1239 33
a1271 2
	/* do something about it */
	/* remember to set pd->flags |= PFDESC_IP_REAS */
d1285 7
d1293 8
a1300 5
 badfrag:
	REASON_SET(reason, PFRES_FRAG);
	if (r != NULL && r->log)
		PFLOG_PACKET(kif, h, m, AF_INET6, dir, *reason, r, NULL, NULL);
	return (PF_DROP);
@


1.89
log
@First step towards more sane time handling in the kernel -- this changes
things such that code that only need a second-resolution uptime or wall
time, and used to get that from time.tv_secs or mono_time.tv_secs now get
this from separate time_t globals time_second and time_uptime.

ok art@@ niklas@@ nordin@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.88 2004/06/10 14:22:54 dhartmei Exp $ */
d1385 1
a1385 1
					src->scrub->pfss_last = mono_time;
d1627 1
a1627 1
		struct timeval delta_ts;
d1644 2
a1645 1
		timersub(&mono_time, &src->scrub->pfss_last, &delta_ts);
d1768 1
a1768 1
		src->scrub->pfss_last = mono_time;
@


1.88
log
@rename struct pf_rule_addr member 'not' to 'neg', as 'not' is a reserved
keyword in C++. ok henning@@, cedric@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.87 2004/05/11 07:34:11 dhartmei Exp $ */
d179 1
a179 1
	u_int32_t		 expire = time.tv_sec -
d290 1
a290 1
		frag->fr_timeout = time.tv_sec;
d355 1
a355 1
		(*frag)->fr_timeout = time.tv_sec;
d557 1
a557 1
		(*frag)->fr_timeout = time.tv_sec;
d1542 2
a1543 2
	    (mono_time.tv_sec - src->scrub->pfss_last.tv_sec > TS_MAX_IDLE ||
	    time.tv_sec - state->creation > TS_MAX_CONN))  {
d1553 1
a1553 1
	    mono_time.tv_sec - dst->scrub->pfss_last.tv_sec > TS_MAX_IDLE) {
@


1.87
log
@pf_cksum_fixup() was called without last argument from normalization,
also fixup checksum when random-id modifies ip_id. This would previously
lead to incorrect checksums for packets modified by scrub random-id.
From Pyun YongHyeon. ok cedric@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.86 2004/05/09 00:16:38 dhartmei Exp $ */
d844 1
a844 1
		    (struct pf_addr *)&h->ip_src.s_addr, AF_INET, r->src.not))
d847 1
a847 1
		    (struct pf_addr *)&h->ip_dst.s_addr, AF_INET, r->dst.not))
d1063 1
a1063 1
		    (struct pf_addr *)&h->ip6_src, AF_INET6, r->src.not))
d1066 1
a1066 1
		    (struct pf_addr *)&h->ip6_dst, AF_INET6, r->dst.not))
d1227 1
a1227 1
		else if (PF_MISMATCHAW(&r->src.addr, pd->src, af, r->src.not))
d1232 1
a1232 1
		else if (PF_MISMATCHAW(&r->dst.addr, pd->dst, af, r->dst.not))
@


1.86
log
@Don't dereference scrub pointer when it's NULL, fix PR 3775, from
Marc Huber. ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.85 2004/05/05 23:16:03 frantzen Exp $ */
a116 1
u_int16_t		 pf_cksum_fixup(u_int16_t, u_int16_t, u_int16_t);
d983 3
a985 1
	if (r->rule_flag & PFRULE_RANDOMID)
d987 2
d1288 1
a1288 1
		th->th_sum = pf_cksum_fixup(th->th_sum, ov, nv);
d1294 1
a1294 1
		th->th_sum = pf_cksum_fixup(th->th_sum, th->th_urp, 0);
d1826 1
a1826 1
				    *mss, htons(r->max_mss));
@


1.85
log
@Use RFC1323 PAWS timestamps as a logical extension to the conventional TCP
sequence numbers by taking advantage of the maximum 1KHz clock as an upperbound
on the timestamp.  Typically gains 10 to 18 bits of additional security against
blind data insertion attacks.  More if the TS Echo wasn't optional :-(
Enabled with:  scrub on !lo0 all reassemble tcp
ok dhartmei@@.  documentation help from jmc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.84 2004/04/28 02:43:09 pb Exp $ */
d1741 1
a1741 1
	if (pd->p_len > 0 && (src->scrub->pfss_flags &
@


1.84
log
@Dont step into INET6 code, just because af != AF_INET
Also comment #endif properly while being here

ok mcbride@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.83 2004/04/27 18:28:07 frantzen Exp $ */
d121 6
a126 2
#define	DPFPRINTF(x)	if (pf_status.debug >= PF_DEBUG_MISC) \
			    { printf("%s: ", __func__); printf x ;}
d1317 1
d1373 10
d1409 2
a1410 2
    u_short *reason, struct tcphdr *th, struct pf_state_peer *src,
    struct pf_state_peer *dst, int *writeback)
d1412 2
d1417 1
d1471 11
d1483 3
a1485 2
					u_int32_t ts_value;
					if (src->scrub &&
d1488 1
a1488 4
						memcpy(&ts_value, &opt[2],
						    sizeof(u_int32_t));
						ts_value = htonl(ntohl(ts_value)
						    + src->scrub->pfss_ts_mod);
d1490 4
a1493 1
						    &th->th_sum, ts_value, 0);
d1498 1
a1498 1
					memcpy(&ts_value, &opt[6],
d1500 1
a1500 1
					if (ts_value && dst->scrub &&
d1503 2
a1504 2
						ts_value = htonl(ntohl(ts_value)
						    - dst->scrub->pfss_ts_mod);
d1506 2
a1507 1
						    &th->th_sum, ts_value, 0);
d1510 1
d1529 258
d1790 1
@


1.83
log
@validate the sequence numbers on TCP resets are an exact match.  check is only
enabled when we're doing full frag reassembly and thus have full seq info
ok markus@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.82 2004/04/26 02:03:38 mcbride Exp $ */
d1195 1
a1195 1
#endif
@


1.82
log
@Prevent biases in arc4random() from disclosing the byte order of the firewall.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.81 2004/04/24 19:14:48 frantzen Exp $ */
d813 2
a814 1
pf_normalize_ip(struct mbuf **m0, int dir, struct pfi_kif *kif, u_short *reason)
d982 2
d991 2
a992 1

d1024 1
a1024 1
    u_short *reason)
d1174 1
@


1.81
log
@be careful about option lengths.  ok henning@@ mcbride@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.80 2004/03/09 21:44:41 mcbride Exp $ */
d1361 2
a1362 1
					src->scrub->pfss_ts_mod = arc4random();
@


1.80
log
@KNF, ok cedric@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.79 2004/02/10 18:49:10 henning Exp $ */
d1365 2
a1366 2
				hlen -= opt[1];
				opt += opt[1];
d1476 2
a1477 2
				hlen -= opt[1];
				opt += opt[1];
@


1.80.2.1
log
@MFC:
Fix by dhartmei@@

prevent an endless loop with route-to lo0, fixes PR 3736

ok deraadt@@ dhartmei@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.80 2004/03/09 21:44:41 mcbride Exp $ */
d1365 2
a1366 2
				hlen -= MAX(opt[1], 2);
				opt += MAX(opt[1], 2);
d1476 2
a1477 2
				hlen -= MAX(opt[1], 2);
				opt += MAX(opt[1], 2);
@


1.79
log
@KNF
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.78 2004/01/16 21:15:42 mcbride Exp $ */
d1341 1
a1341 1
		return 0;
@


1.78
log
@Fix IPv6 stateful tcp scrubbing by not dereferencing a null pointer.

ok dhartmei@@ frantzen@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.77 2003/12/31 11:18:25 cedric Exp $ */
d377 1
a377 1
	        4 > off)
d641 4
a644 2
				KASSERT((int)m->m_len == ntohs(h->ip_len) - precut);
				h->ip_off = htons(ntohs(h->ip_off) + (precut >> 3));
d698 2
a699 1
				KASSERT((int)m->m_len == ntohs(h->ip_len) - aftercut);
@


1.77
log
@Many improvements to the handling of interfaces in PF.

1) PF should do the right thing when unplugging/replugging or cloning/
destroying NICs.

2) Rules can be loaded in the kernel for not-yet-existing devices
(USB, PCMCIA, Cardbus). For example, it is valid to write:
"pass in on kue0" before kue USB is plugged in.

3) It is possible to write rules that apply to group of interfaces
(drivers), like "pass in on ppp all"

4) There is a new ":peer" modifier that completes the ":broadcast"
and ":network" modifiers.

5) There is a new ":0" modifier that will filter out interface aliases.
Can also be applied to DNS names to restore original PF behaviour.

6) The dynamic interface syntax (foo) has been vastly improved, and
now support multiple addresses, v4 and v6 addresses, and all userland
modifiers, like "pass in from (fxp0:network)"

7) Scrub rules now support the !if syntax.

8) States can be bound to the specific interface that created them or
to  a group of interfaces for example:

- pass all keep state (if-bound)
- pass all keep state (group-bound)
- pass all keep state (floating)

9) The default value when only keep state is given can be selected by
using the "set state-policy" statement.

10) "pfctl -ss" will now print the interface scope of the state.

This diff change the pf_state structure slighltly, so you should
recompile your userland tools (pfctl, authpf, pflogd, tcpdump...)

Tested on i386, sparc, sparc64 by Ryan
Tested on macppc, sparc64 by Daniel

ok deraadt@@ mcbride@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.76 2003/12/18 20:13:23 dhartmei Exp $ */
d1413 1
a1413 1
		if (dst->scrub) {
@


1.76
log
@TCP timestamp modulation (scrub reassemble tcp) fix from frantzen@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.75 2003/08/29 01:49:08 dhartmei Exp $ */
d810 1
a810 1
pf_normalize_ip(struct mbuf **m0, int dir, struct ifnet *ifp, u_short *reason)
d827 2
a828 1
		if (r->ifp != NULL && r->ifp != ifp)
d991 1
a991 1
		PFLOG_PACKET(ifp, h, m, AF_INET, dir, *reason, r, NULL, NULL);
d997 1
a997 1
		PFLOG_PACKET(ifp, h, m, AF_INET, dir, *reason, r, NULL, NULL);
d1009 1
a1009 1
		PFLOG_PACKET(ifp, h, m, AF_INET, dir, *reason, r, NULL, NULL);
d1016 2
a1017 1
pf_normalize_ip6(struct mbuf **m0, int dir, struct ifnet *ifp, u_short *reason)
d1037 2
a1038 1
		if (r->ifp != NULL && r->ifp != ifp)
d1172 1
a1172 1
		PFLOG_PACKET(ifp, h, m, AF_INET6, dir, *reason, r, NULL, NULL);
d1178 1
a1178 1
		PFLOG_PACKET(ifp, h, m, AF_INET6, dir, *reason, r, NULL, NULL);
d1184 1
a1184 1
		PFLOG_PACKET(ifp, h, m, AF_INET6, dir, *reason, r, NULL, NULL);
d1190 1
a1190 1
pf_normalize_tcp(int dir, struct ifnet *ifp, struct mbuf *m, int ipoff,
d1203 2
a1204 1
		if (r->ifp != NULL && r->ifp != ifp)
d1297 1
a1297 1
		PFLOG_PACKET(ifp, h, m, AF_INET, dir, reason, r, NULL, NULL);
@


1.75
log
@Fix three cases of potential accesses to free'd memory. At least one of
them could be used to panic pf with scrub rules remotely. Found by
Rob Pickering. ok frantzen@@, henning
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.74 2003/08/22 21:50:34 david Exp $ */
d1453 5
a1457 1
					if (dst->scrub &&
a1459 2
						memcpy(&ts_value, &opt[6],
						    sizeof(u_int32_t));
@


1.75.2.1
log
@MFC:
Fix by frantzen@@

be careful about option lengths

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.75 2003/08/29 01:49:08 dhartmei Exp $ */
d1358 2
a1359 2
				hlen -= MAX(opt[1], 2);
				opt += MAX(opt[1], 2);
d1467 2
a1468 2
				hlen -= MAX(opt[1], 2);
				opt += MAX(opt[1], 2);
@


1.74
log
@pf spelling police
ok dhartmei@@ jmc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.73 2003/08/22 15:19:23 henning Exp $ */
d113 1
a113 1
struct mbuf		*pf_reassemble(struct mbuf **, struct pf_fragment *,
d116 1
a116 1
			    struct pf_fragment *, int, int, int *);
d318 1
a318 1
pf_reassemble(struct mbuf **m0, struct pf_fragment *frag,
d330 1
a330 1
	KASSERT(frag == NULL || BUFFER_FRAGMENTS(frag));
d337 3
a339 3
	if (frag == NULL) {
		frag = pool_get(&pf_frag_pl, PR_NOWAIT);
		if (frag == NULL) {
d341 2
a342 2
			frag = pool_get(&pf_frag_pl, PR_NOWAIT);
			if (frag == NULL)
d346 8
a353 8
		frag->fr_flags = 0;
		frag->fr_max = 0;
		frag->fr_src = frent->fr_ip->ip_src;
		frag->fr_dst = frent->fr_ip->ip_dst;
		frag->fr_p = frent->fr_ip->ip_p;
		frag->fr_id = frent->fr_ip->ip_id;
		frag->fr_timeout = time.tv_sec;
		LIST_INIT(&frag->fr_queue);
d355 2
a356 2
		RB_INSERT(pf_frag_tree, &pf_frag_tree, frag);
		TAILQ_INSERT_HEAD(&pf_fragqueue, frag, frag_next);
d367 1
a367 1
	LIST_FOREACH(frea, &frag->fr_queue, fr_next) {
d422 2
a423 2
	if (frag->fr_max < max)
		frag->fr_max = max;
d426 1
a426 1
		frag->fr_flags |= PFFRAG_SEENLAST;
d429 1
a429 1
		LIST_INSERT_HEAD(&frag->fr_queue, frent, fr_next);
d434 1
a434 1
	if (!(frag->fr_flags & PFFRAG_SEENLAST))
d439 1
a439 1
	for (frep = LIST_FIRST(&frag->fr_queue); frep; frep = next) {
d443 1
a443 1
		if (off < frag->fr_max &&
d448 1
a448 1
			    frag->fr_max));
d452 2
a453 2
	DPFPRINTF(("%d < %d?\n", off, frag->fr_max));
	if (off < frag->fr_max)
d457 1
a457 1
	frent = LIST_FIRST(&frag->fr_queue);
d461 2
a462 1
		pf_free_fragment(frag);
d484 2
a485 2
	ip->ip_src = frag->fr_src;
	ip->ip_dst = frag->fr_dst;
d488 2
a489 1
	pf_remove_fragment(frag);
d517 1
a517 1
pf_fragcache(struct mbuf **m0, struct ip *h, struct pf_fragment *frag, int mff,
d527 1
a527 1
	KASSERT(frag == NULL || !BUFFER_FRAGMENTS(frag));
d530 3
a532 3
	if (frag == NULL) {
		frag = pool_get(&pf_cache_pl, PR_NOWAIT);
		if (frag == NULL) {
d534 2
a535 2
			frag = pool_get(&pf_cache_pl, PR_NOWAIT);
			if (frag == NULL)
d542 2
a543 1
			pool_put(&pf_cache_pl, frag);
d548 7
a554 7
		frag->fr_flags = PFFRAG_NOBUFFER;
		frag->fr_max = 0;
		frag->fr_src = h->ip_src;
		frag->fr_dst = h->ip_dst;
		frag->fr_p = h->ip_p;
		frag->fr_id = h->ip_id;
		frag->fr_timeout = time.tv_sec;
d558 2
a559 2
		LIST_INIT(&frag->fr_cache);
		LIST_INSERT_HEAD(&frag->fr_cache, cur, fr_next);
d561 2
a562 2
		RB_INSERT(pf_frag_tree, &pf_cache_tree, frag);
		TAILQ_INSERT_HEAD(&pf_cachequeue, frag, frag_next);
d574 1
a574 1
	LIST_FOREACH(fra, &frag->fr_cache, fr_next) {
d761 2
a762 2
	if (frag->fr_max < max)
		frag->fr_max = max;
d766 1
a766 1
		frag->fr_flags |= PFFRAG_SEENLAST;
d769 3
a771 3
	if ((frag->fr_flags & PFFRAG_SEENLAST) &&
	    LIST_FIRST(&frag->fr_cache)->fr_off == 0 &&
	    LIST_FIRST(&frag->fr_cache)->fr_end == frag->fr_max) {
d774 3
a776 2
		    frag->fr_max));
		pf_free_fragment(frag);
d785 2
a786 2
	if (!mff && frag)
		frag->fr_flags |= PFFRAG_SEENLAST;
d794 2
a795 2
	if (!mff && frag)
		frag->fr_flags |= PFFRAG_SEENLAST;
d799 1
a799 1
		if ((frag->fr_flags & PFFRAG_DROP) == 0)
d802 1
a802 1
		frag->fr_flags |= PFFRAG_DROP;
d912 1
a912 1
		*m0 = m = pf_reassemble(m0, frag, frent, mff);
d917 1
a917 1
		if (frag && (frag->fr_flags & PFFRAG_DROP))
d946 1
a946 1
		*m0 = m = pf_fragcache(m0, h, frag, mff,
d962 1
a962 1
		if (frag && (frag->fr_flags & PFFRAG_DROP))
@


1.73
log
@KNF
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.72 2003/08/21 19:12:08 frantzen Exp $ */
d608 1
a608 1
			/* Update the previous frag to encompas this one */
d715 1
a715 1
		/* Need to glue together two seperate fragment descriptors */
d998 1
a998 1
	/* Free assoicated fragments */
d1341 1
a1341 1
			case TCPOPT_EOL:	/* FALLTHROUH */
d1426 1
a1426 1
			case TCPOPT_EOL:	/* FALLTHROUH */
@


1.72
log
@Add Michal Zalewski's p0f v2 style passive OS fingerprinting to PF.
Exposes the source IP's operating system to the filter language.
Interesting policy decisions are now enforceable:
.	block proto tcp from any os SCO
.	block proto tcp from any os Windows to any port smtp
.	rdr ... from any os "Windows 98" to port WWW -> 127.0.0.1 port 8001
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.71 2003/08/14 19:00:12 jason Exp $ */
d884 1
a884 1
	max = fragoff + ip_len; 
@


1.71
log
@m_copyback()'s 4th arg is const void *, nuke (caddr_t) casts.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.70 2003/07/17 16:25:52 frantzen Exp $ */
d1214 4
@


1.70
log
@fix scrub frag reassembly after the stack's ip_len/ip_off flip correction
ok itojun@@ and dhartmei@@.  heckling from henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.69 2003/07/12 09:33:32 dhartmei Exp $ */
d1278 1
a1278 1
		m_copyback(m, off, sizeof(*th), (caddr_t)th);
@


1.69
log
@Prevent u_int16_t variable from overflowing and get rid of the compiler
warning. From Pyun YongHyeon. ok itojun@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.68 2003/07/10 05:50:10 itojun Exp $ */
d316 1
d326 3
a328 2
	u_int16_t	 off = ntohs(ip->ip_off);
	u_int16_t	 max = ntohs(ip->ip_len) + off;
d368 1
a368 1
		if (ntohs(frea->fr_ip->ip_off) > off)
d376 2
a377 1
	    ntohs(frep->fr_ip->ip_off) + ntohs(frep->fr_ip->ip_len) > off)
d381 3
a383 3
		precut = ntohs(frep->fr_ip->ip_off) +
		    ntohs(frep->fr_ip->ip_len) - off;
		if (precut >= ntohs(ip->ip_len))
d388 4
a391 3
		ip->ip_off = htons(ntohs(ip->ip_off) + precut);
		off = ntohs(ip->ip_off);
		ip->ip_len = htons(ntohs(ip->ip_len) - precut);
d394 1
a394 1
	for (; frea != NULL && ntohs(ip->ip_len) + off > ntohs(frea->fr_ip->ip_off);
d399 1
a399 1
		aftercut = (ntohs(ip->ip_len) + off) - ntohs(frea->fr_ip->ip_off);
d401 2
a402 1
		if (aftercut < ntohs(frea->fr_ip->ip_len))
d406 2
a407 2
			frea->fr_ip->ip_off =
			    htons(ntohs(frea->fr_ip->ip_off) + aftercut);
d442 1
a442 1
		off += ntohs(frep->fr_ip->ip_len);
d444 1
a444 1
		    (next == NULL || ntohs(next->fr_ip->ip_off) != off))
d447 1
a447 1
			    off, next == NULL ? -1 : ntohs(next->fr_ip->ip_off),
d871 1
a871 1
	ip_off = ntohs(h->ip_off) << 3;
@


1.68
log
@correct another incorrect comparison in ip6 normalization.
don't use m->m_pkthdr.len for checking, as it is not reliable
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.67 2003/07/10 04:20:59 itojun Exp $ */
a873 1
	max = fragoff + ip_len;
d875 2
a876 2
	if (max > IP_MAXPACKET) {
		DPFPRINTF(("max packet %d\n", max));
d879 1
@


1.67
log
@wrong comparison of IPv6 packetsize
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.66 2003/07/09 22:11:08 itojun Exp $ */
d1134 1
a1134 1
	if (sizeof(struct ip6_hdr) + plen < m->m_pkthdr.len)
d1146 1
d1151 1
a1151 1
	if (fragoff + (m->m_pkthdr.len - off - sizeof(frag)) > IPV6_MAXPACKET)
@


1.66
log
@check if m->m_pkthdr.len is too short
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.65 2003/07/09 22:09:20 itojun Exp $ */
d1052 1
a1052 1
	if (sizeof(struct ip6_hdr) + IPV6_MAXPACKET >= m->m_pkthdr.len)
@


1.65
log
@don't check exact ip6_plen and m->m_pkthdr.len match, as ip6_input()
does the m_adj() only after filtering.  reported by marc
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.64 2003/07/09 22:03:16 itojun Exp $ */
d1134 2
@


1.64
log
@do not flip ip_len/ip_off in netinet stack.  deraadt ok.
(please test, especially PF portion)
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.63 2003/07/09 07:18:50 dhartmei Exp $ */
d1016 1
a1016 1
	u_int32_t		 jumbolen = 0;
d1052 1
a1052 6
	if (ntohs(h->ip6_plen) == 0) {
		/* jumbo payload option must be present */
		if (sizeof(struct ip6_hdr) + IPV6_MAXPACKET >= m->m_pkthdr.len)
			goto drop;
	} else if (sizeof(struct ip6_hdr) + ntohs(h->ip6_plen) !=
	    m->m_pkthdr.len)
d1127 6
a1132 1
	if (ntohs(h->ip6_plen) == 0 && !jumbolen)
@


1.63
log
@KNF
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.62 2003/07/01 00:28:52 itojun Exp $ */
d325 2
a326 2
	u_int16_t	 off = ip->ip_off;
	u_int16_t	 max = ip->ip_len + off;
d366 1
a366 1
		if (frea->fr_ip->ip_off > off)
d373 3
a375 1
	if (frep != NULL && frep->fr_ip->ip_off + frep->fr_ip->ip_len > off) {
d378 3
a380 2
		precut = frep->fr_ip->ip_off + frep->fr_ip->ip_len - off;
		if (precut >= ip->ip_len)
d385 3
a387 2
		off = ip->ip_off += precut;
		ip->ip_len -= precut;
d390 3
a392 2
	for (; frea != NULL && ip->ip_len + off > frea->fr_ip->ip_off;
	    frea = next) {
d395 1
a395 1
		aftercut = (ip->ip_len + off) - frea->fr_ip->ip_off;
d397 6
a402 3
		if (aftercut < frea->fr_ip->ip_len) {
			frea->fr_ip->ip_len -= aftercut;
			frea->fr_ip->ip_off += aftercut;
d437 1
a437 1
		off += frep->fr_ip->ip_len;
d439 2
a440 1
		    (next == NULL || next->fr_ip->ip_off != off)) {
d442 1
a442 1
			    off, next == NULL ? -1 : next->fr_ip->ip_off,
d485 1
a485 1
	ip->ip_len = off + hlen;
d498 1
a498 1
	DPFPRINTF(("complete: %p(%d)\n", m, ip->ip_len));
d515 2
a516 2
	int			 ip_len = h->ip_len - (h->ip_hl << 2);
	u_int16_t		 off = h->ip_off << 3;
a631 1
				KASSERT((int)m->m_len == h->ip_len - precut);
d633 3
a635 2
				h->ip_off += precut >> 3;
				h->ip_len -= precut;
d688 2
a689 2
				KASSERT((int)m->m_len == h->ip_len - aftercut);
				h->ip_len -= aftercut;
d808 1
a808 1
	int			 mff = (h->ip_off & IP_MF);
d810 1
a810 1
	u_int16_t		 fragoff = (h->ip_off & IP_OFFMASK) << 3;
d845 1
a845 1
	if (hlen > h->ip_len)
d850 1
a850 1
		h->ip_off &= ~IP_DF;
d860 1
a860 1
	if (h->ip_off & IP_DF) {
d865 2
a866 2
	ip_len = h->ip_len - hlen;
	ip_off = h->ip_off << 3;
a883 2
		h->ip_len = ip_len;	/* logic need muddled off/len */
		h->ip_off = ip_off;
d960 1
a960 1
	h->ip_off &= IP_DF;
@


1.62
log
@wrap pf_normalize_ip6() by #ifdef INET6.  pointed out by Wouter Clarie
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.61 2003/06/29 23:37:12 itojun Exp $ */
d1047 2
a1048 1
		if (sizeof(struct ip6_hdr) + IPV6_MAXPACKET >= m->m_pkthdr.len)				goto drop;
@


1.61
log
@normalize IPv6 packet (no reass, but it is a start).  dhartmei & henning ok
- length, jumbo payload option
- TTL ("hoplimit" in IPv6 terminology) rewrite
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.60 2003/06/28 07:27:20 itojun Exp $ */
d997 1
d1164 1
@


1.60
log
@redundant (pfvar.h already have it)
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.59 2003/05/14 23:46:45 frantzen Exp $ */
d994 167
@


1.59
log
@- modulate TCP Timestamps so they can't be used to detect NAT and to preclude
remote uptime determination
- scrub modifier "reassemble tcp" turns on stateful TCP normalizations
ok henning@@ dhartmei@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.58 2003/05/14 08:42:00 canacar Exp $ */
a117 2
int			 pf_normalize_tcp(int, struct ifnet *, struct mbuf *,
			    int, int, void *, struct pf_pdesc *);
@


1.58
log
@Use official (from pcap people) link type for pflog.
With this change, the log header format also changes.
The new log format is extendible and allows logging
of the originating anchor and ruleset information.

ok henning@@ dhartmei@@ frantzen@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.57 2003/05/11 20:44:03 frantzen Exp $ */
d40 1
d1042 3
a1096 3
	/* Inform the state code to do stateful normalizations */
	pd->flags |= PFDESC_TCP_NORM;

d1107 1
a1107 1
pf_normalize_tcp_init(struct mbuf *m, struct pf_pdesc *pd,
d1110 3
d1118 1
d1136 38
d1189 3
a1191 2
pf_normalize_tcp_stateful(struct mbuf *m, struct pf_pdesc *pd, u_short *reason,
    struct tcphdr *th, struct pf_state_peer *src, struct pf_state_peer *dst)
d1193 4
a1198 1

d1227 61
@


1.57
log
@the start of stateful TCP scrubbing.  dynamically determine the highest TTL of
each side of the TCP connection and prevent it from being reduced
ok pb@@ dhartmei@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.56 2003/04/05 20:20:58 cedric Exp $ */
d975 1
a975 1
		PFLOG_PACKET(ifp, h, m, AF_INET, dir, *reason, r);
d981 1
a981 1
		PFLOG_PACKET(ifp, h, m, AF_INET, dir, *reason, r);
d993 1
a993 1
		PFLOG_PACKET(ifp, h, m, AF_INET, dir, *reason, r);
d1101 1
a1101 1
		PFLOG_PACKET(ifp, h, m, AF_INET, dir, reason, r);
@


1.56
log
@Replace the timeout variables by the content of the timeout
field of a new pf_default_rule structure.
ok dhartmei@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.55 2003/02/18 08:05:15 camield Exp $ */
d56 4
d127 1
d141 2
d1093 3
d1105 81
@


1.55
log
@Enforce min-ttl and random-id on inbound scrub as well as outbound.

ok dhartmei@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.54 2003/02/12 20:43:36 dhartmei Exp $ */
a123 1
extern int		 pftm_frag;	/* Fragment expire timeout */
d170 2
a171 1
	u_int32_t		 expire = time.tv_sec - pftm_frag;
@


1.55.2.1
log
@MFC:
Fix by dhartmei@@

Fix three cases of potential accesses to free'd memory. At least one of
them could be used to panic pf with scrub rules remotely. Found by
Rob Pickering.

ok millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.55 2003/02/18 08:05:15 camield Exp $ */
d108 1
a108 1
struct mbuf		*pf_reassemble(struct mbuf **, struct pf_fragment **,
d111 1
a111 1
			    struct pf_fragment **, int, int, int *);
d311 1
a311 1
pf_reassemble(struct mbuf **m0, struct pf_fragment **frag,
d322 1
a322 1
	KASSERT(*frag == NULL || BUFFER_FRAGMENTS(*frag));
d329 3
a331 3
	if (*frag == NULL) {
		*frag = pool_get(&pf_frag_pl, PR_NOWAIT);
		if (*frag == NULL) {
d333 2
a334 2
			*frag = pool_get(&pf_frag_pl, PR_NOWAIT);
			if (*frag == NULL)
d338 8
a345 8
		(*frag)->fr_flags = 0;
		(*frag)->fr_max = 0;
		(*frag)->fr_src = frent->fr_ip->ip_src;
		(*frag)->fr_dst = frent->fr_ip->ip_dst;
		(*frag)->fr_p = frent->fr_ip->ip_p;
		(*frag)->fr_id = frent->fr_ip->ip_id;
		(*frag)->fr_timeout = time.tv_sec;
		LIST_INIT(&(*frag)->fr_queue);
d347 2
a348 2
		RB_INSERT(pf_frag_tree, &pf_frag_tree, *frag);
		TAILQ_INSERT_HEAD(&pf_fragqueue, *frag, frag_next);
d359 1
a359 1
	LIST_FOREACH(frea, &(*frag)->fr_queue, fr_next) {
d403 2
a404 2
	if ((*frag)->fr_max < max)
		(*frag)->fr_max = max;
d407 1
a407 1
		(*frag)->fr_flags |= PFFRAG_SEENLAST;
d410 1
a410 1
		LIST_INSERT_HEAD(&(*frag)->fr_queue, frent, fr_next);
d415 1
a415 1
	if (!((*frag)->fr_flags & PFFRAG_SEENLAST))
d420 1
a420 1
	for (frep = LIST_FIRST(&(*frag)->fr_queue); frep; frep = next) {
d424 1
a424 1
		if (off < (*frag)->fr_max &&
d428 1
a428 1
			    (*frag)->fr_max));
d432 2
a433 2
	DPFPRINTF(("%d < %d?\n", off, (*frag)->fr_max));
	if (off < (*frag)->fr_max)
d437 1
a437 1
	frent = LIST_FIRST(&(*frag)->fr_queue);
d441 1
a441 2
		pf_free_fragment(*frag);
		*frag = NULL;
d463 2
a464 2
	ip->ip_src = (*frag)->fr_src;
	ip->ip_dst = (*frag)->fr_dst;
d467 1
a467 2
	pf_remove_fragment(*frag);
	*frag = NULL;
d495 1
a495 1
pf_fragcache(struct mbuf **m0, struct ip *h, struct pf_fragment **frag, int mff,
d505 1
a505 1
	KASSERT(*frag == NULL || !BUFFER_FRAGMENTS(*frag));
d508 3
a510 3
	if (*frag == NULL) {
		*frag = pool_get(&pf_cache_pl, PR_NOWAIT);
		if (*frag == NULL) {
d512 2
a513 2
			*frag = pool_get(&pf_cache_pl, PR_NOWAIT);
			if (*frag == NULL)
d520 1
a520 2
			pool_put(&pf_cache_pl, *frag);
			*frag = NULL;
d525 7
a531 7
		(*frag)->fr_flags = PFFRAG_NOBUFFER;
		(*frag)->fr_max = 0;
		(*frag)->fr_src = h->ip_src;
		(*frag)->fr_dst = h->ip_dst;
		(*frag)->fr_p = h->ip_p;
		(*frag)->fr_id = h->ip_id;
		(*frag)->fr_timeout = time.tv_sec;
d535 2
a536 2
		LIST_INIT(&(*frag)->fr_cache);
		LIST_INSERT_HEAD(&(*frag)->fr_cache, cur, fr_next);
d538 2
a539 2
		RB_INSERT(pf_frag_tree, &pf_cache_tree, *frag);
		TAILQ_INSERT_HEAD(&pf_cachequeue, *frag, frag_next);
d551 1
a551 1
	LIST_FOREACH(fra, &(*frag)->fr_cache, fr_next) {
d738 2
a739 2
	if ((*frag)->fr_max < max)
		(*frag)->fr_max = max;
d743 1
a743 1
		(*frag)->fr_flags |= PFFRAG_SEENLAST;
d746 3
a748 3
	if (((*frag)->fr_flags & PFFRAG_SEENLAST) &&
	    LIST_FIRST(&(*frag)->fr_cache)->fr_off == 0 &&
	    LIST_FIRST(&(*frag)->fr_cache)->fr_end == (*frag)->fr_max) {
d751 2
a752 3
		    (*frag)->fr_max));
		pf_free_fragment(*frag);
		*frag = NULL;
d761 2
a762 2
	if (!mff && *frag != NULL)
		(*frag)->fr_flags |= PFFRAG_SEENLAST;
d770 2
a771 2
	if (!mff && *frag != NULL)
		(*frag)->fr_flags |= PFFRAG_SEENLAST;
d775 1
a775 1
		if (((*frag)->fr_flags & PFFRAG_DROP) == 0)
d778 1
a778 1
		(*frag)->fr_flags |= PFFRAG_DROP;
d890 1
a890 1
		*m0 = m = pf_reassemble(m0, &frag, frent, mff);
d895 1
a895 1
		if (frag != NULL && (frag->fr_flags & PFFRAG_DROP))
d924 1
a924 1
		*m0 = m = pf_fragcache(m0, h, &frag, mff,
d940 1
a940 1
		if (frag != NULL && (frag->fr_flags & PFFRAG_DROP))
@


1.54
log
@Address the NFS problems recently discussed in various threads.

Change semantics of scrub option 'no-df' slightly: if the option is used,
it now also applies to _fragments_ with IP_DF set, not just to complete
packets. Hence, adding 'no-df' to 'scrub in all fragment reassemble'
allows to clear IP_DF from fragments, so they don't get dropped but
reassembled.

This affects several UDP protocols that used PMTU discovery, mostly
Linux' NFS implementation. In short, if you have 'scrub in all' now,
you probably want to change that to 'scrub in all no-df', unless you
want to drop fragments with IP_DF set (some people have good reasons
to do the latter, hence the non-default option).

ok frantzen@@, henning@@, cedric@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.53 2003/02/08 20:13:20 dhartmei Exp $ */
a945 3
	if (dir != PF_OUT)
		return (PF_PASS);

a958 3
	if (dir != PF_OUT)
		return (PF_PASS);

@


1.53
log
@Add scrub option 'random-id', which replaces IP IDs with random values
for outgoing packets that are not fragmented (after reassembly), to
compensate for predictable IDs generated by some hosts, and defeat
fingerprinting and NAT detection as described in the Bellovin paper
http://www.research.att.com/~smb/papers/fnat.pdf. ok theo@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.52 2003/01/25 19:47:05 dhartmei Exp $ */
d833 4
d841 4
a844 1
	/* This can not happen */
d950 1
a950 4
	if (r->rule_flag & PFRULE_NODF)
		h->ip_off = 0;
	else
		h->ip_off &= IP_DF;
@


1.52
log
@Fix a bug that potentially caused fragments to be dropped when the
overlap calculation got negative. Found by Baruch Even. ok henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.51 2003/01/09 15:58:35 dhartmei Exp $ */
d951 3
@


1.51
log
@(whitespace) KNF, re-fold -w 80
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.50 2003/01/07 00:21:07 dhartmei Exp $ */
d367 1
a367 1
	if (frep != NULL) {
d373 5
a377 8
		if (precut) {
			m_adj(frent->fr_m, precut);

			DPFPRINTF(("overlap -%d\n", precut));
			/* Enforce 8 byte boundaries */
			off = ip->ip_off += precut;
			ip->ip_len -= precut;
		}
@


1.50
log
@Remove table name hashing (pass the name in each ioctl instead), and
introduce reference counting for tables, they are now automatically
created and deleted through referencing rules. Diff partly from cedric@@.
ok mcbride@@, henning@@, cedric@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.49 2003/01/05 22:14:23 dhartmei Exp $ */
d195 1
a195 1
 *  Try to flush old fragments to make space for new ones
d206 1
a206 1
		   pf_nfrents - goal));
d217 1
a217 1
		   pf_ncache - goal));
@


1.49
log
@Move ifname from pf_addr to pf_addr_wrap, prepare pf_addr_wrap for table
name. ok henning@@, mcbride@@, cedric@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.48 2003/01/04 17:40:51 dhartmei Exp $ */
d814 2
a815 4
		else if (!PF_AZERO(&r->src.addr.v.a.mask, AF_INET) &&
		    !PF_MATCHA(r->src.not, &r->src.addr.v.a.addr,
		    &r->src.addr.v.a.mask, (struct pf_addr *)&h->ip_src.s_addr,
		    AF_INET))
d817 2
a818 4
		else if (!PF_AZERO(&r->dst.addr.v.a.mask, AF_INET) &&
		    !PF_MATCHA(r->dst.not, &r->dst.addr.v.a.addr,
		    &r->dst.addr.v.a.mask, (struct pf_addr *)&h->ip_dst.s_addr,
		    AF_INET))
d1015 1
a1015 7
		else if (r->src.addr.type == PF_ADDR_NOROUTE &&
		    pf_routable(pd->src, af))
			r = TAILQ_NEXT(r, entries);
		else if (r->src.addr.type != PF_ADDR_NOROUTE &&
		    !PF_AZERO(&r->src.addr.v.a.mask, af) &&
		    !PF_MATCHA(r->src.not, &r->src.addr.v.a.addr,
		    &r->src.addr.v.a.mask, pd->src, af))
d1020 1
a1020 7
		else if (r->dst.addr.type == PF_ADDR_NOROUTE &&
		    pf_routable(pd->dst, af))
			r = TAILQ_NEXT(r, entries);
		else if (!r->dst.addr.type != PF_ADDR_NOROUTE &&
		    !PF_AZERO(&r->dst.addr.v.a.mask, af) &&
		    !PF_MATCHA(r->dst.not, &r->dst.addr.v.a.addr,
		    &r->dst.addr.v.a.mask, pd->dst, af))
@


1.48
log
@move noroute from flag in pf_rule_addr into type in pf_addr_wrap.
ok henning@@, mcbride@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.47 2003/01/03 19:31:43 deraadt Exp $ */
d814 4
a817 3
		else if (!PF_AZERO(&r->src.addr.mask, AF_INET) &&
		    !PF_MATCHA(r->src.not, &r->src.addr.addr, &r->src.addr.mask,
		    (struct pf_addr *)&h->ip_src.s_addr, AF_INET))
d819 4
a822 3
		else if (!PF_AZERO(&r->dst.addr.mask, AF_INET) &&
		    !PF_MATCHA(r->dst.not, &r->dst.addr.addr, &r->dst.addr.mask,
		    (struct pf_addr *)&h->ip_dst.s_addr, AF_INET))
d1023 3
a1025 3
		    !PF_AZERO(&r->src.addr.mask, af) &&
		    !PF_MATCHA(r->src.not, &r->src.addr.addr, &r->src.addr.mask,
			    pd->src, af))
d1034 3
a1036 3
		    !PF_AZERO(&r->dst.addr.mask, af) &&
		    !PF_MATCHA(r->dst.not, &r->dst.addr.addr, &r->dst.addr.mask,
			    pd->dst, af))
@


1.47
log
@KNF
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.46 2003/01/01 16:07:45 henning Exp $ */
d1017 2
a1018 1
		else if (r->src.noroute && pf_routable(pd->src, af))
d1020 2
a1021 1
		else if (!r->src.noroute && !PF_AZERO(&r->src.addr.mask, af) &&
d1028 2
a1029 1
		else if (r->dst.noroute && pf_routable(pd->dst, af))
d1031 2
a1032 1
		else if (!r->dst.noroute && !PF_AZERO(&r->dst.addr.mask, af) &&
@


1.46
log
@KNF
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.45 2003/01/01 04:26:19 dhartmei Exp $ */
d829 1
a829 1
                r->packets++;
d1044 1
a1044 1
                r->packets++;
@


1.45
log
@Remove skip step for action (scrub vs. non-scrub), as scrub rules are
stored in a separate list now. Regress tests still pass after
sed "s/ a=end / /g", other skip steps are not affected.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.44 2002/12/31 19:18:41 mcbride Exp $ */
d828 1
a828 1
	else 
d1043 1
a1043 1
	else 
@


1.44
log
@Split scrub rules out from the filter rules in the kernel.
Precursor to removing rule.action from skip steps.

Also a couple of other small fixes:
- s/PF_RULESET_RULE/PF_RULESET_FILTER/
- replacement of 4 with PF_RULESET_MAX in pfvar.h struct ruleset {
- error handling in ioctl of an invalid value in rule.action
- counting evaluations and matching packets for scrub rules

ok henning@@ dhartmei@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.43 2002/12/18 19:17:07 henning Exp $ */
d806 1
a806 3
		if (r->action != PF_SCRUB)
			r = r->skip[PF_SKIP_ACTION].ptr;
		else if (r->ifp != NULL && r->ifp != ifp)
d1009 1
a1009 3
		if (r->action != PF_SCRUB)
			r = r->skip[PF_SKIP_ACTION].ptr;
		else if (r->ifp != NULL && r->ifp != ifp)
@


1.43
log
@KNF
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.42 2002/12/18 16:28:40 dhartmei Exp $ */
d803 1
a803 1
	r = TAILQ_FIRST(pf_main_ruleset.rules[PF_RULESET_RULE].active.ptr);
d805 1
d830 2
d1008 1
a1008 1
	r = TAILQ_FIRST(pf_main_ruleset.rules[PF_RULESET_RULE].active.ptr);
d1010 1
d1047 2
d1106 2
a1107 2
	if (rm != NULL && rm->log)
		PFLOG_PACKET(ifp, h, m, AF_INET, dir, reason, rm);
@


1.42
log
@Pass skip step values through ioctl interface, pfctl -vvsr shows them,
main purpose is making them regress-testable.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.41 2002/12/17 12:30:13 mcbride Exp $ */
d150 1
a150 1
	int diff;
d170 2
a171 2
	struct pf_fragment *frag;
	u_int32_t expire = time.tv_sec - pftm_frag;
d201 2
a202 2
	struct pf_fragment *frag;
	int goal;
d231 2
a232 2
	struct pf_frent *frent;
	struct pf_frcache *frcache;
d273 2
a274 2
	struct pf_fragment key;
	struct pf_fragment *frag;
d314 7
a320 7
	struct mbuf *m = *m0, *m2;
	struct pf_frent *frea, *next;
	struct pf_frent *frep = NULL;
	struct ip *ip = frent->fr_ip;
	int hlen = ip->ip_hl << 2;
	u_int16_t off = ip->ip_off;
	u_int16_t max = ip->ip_len + off;
d368 1
a368 1
		u_int16_t precut;
d385 1
a385 1
		u_int16_t aftercut;
d501 6
a506 6
	struct mbuf *m = *m0;
	struct pf_frcache *frp, *fra, *cur = NULL;
	int ip_len = h->ip_len - (h->ip_hl << 2);
	u_int16_t off = h->ip_off << 3;
	u_int16_t max = ip_len + off;
	int hosed = 0;
d563 1
a563 1
		int precut;
d646 2
a647 2
		int aftercut;
		int merge = 0;
d791 11
a801 10
	struct mbuf *m = *m0;
	struct pf_rule *r;
	struct pf_frent *frent;
	struct pf_fragment *frag = NULL;
	struct ip *h = mtod(m, struct ip *);
	int mff = (h->ip_off & IP_MF), hlen = h->ip_hl << 2;
	u_int16_t fragoff = (h->ip_off & IP_OFFMASK) << 3;
	u_int16_t max;
	int ip_len;
	int ip_off;
d898 1
a898 1
		int nomem = 0;
d930 2
a931 1
			struct m_tag *mtag;
d998 6
a1003 6
	struct pf_rule *r, *rm = NULL;
	struct tcphdr *th = pd->hdr.tcp;
	int rewrite = 0;
	u_short reason;
	u_int8_t flags;
	sa_family_t af = pd->af;
d1070 1
a1070 1
		u_int16_t ov, nv;
d1109 5
a1113 5
	u_int16_t *mss;
	int thoff;
	int opt, cnt, optlen = 0;
	int rewrite = 0;
	u_char *optp;
@


1.41
log
@Merge pf_nat/pf_binat/pf_rdr structs into pf_rule. Simplifies code, allows
skip steps on translation rules.

Also:
- Require a ticket for DIOCCHANGERULE operations to prevent races.
- Remove pf_compare_* functions from pf_ioctl.c. DIOCCHANGE* operations
  use a rule number, and comparisons happen in userland.

Testing and fixes from dhartmei@@ and frantzen@@

ok dhartmei@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.40 2002/12/06 00:47:32 dhartmei Exp $ */
d805 1
a805 1
			r = r->skip[PF_SKIP_ACTION];
d807 1
a807 1
			r = r->skip[PF_SKIP_IFP];
d809 1
a809 1
			r = r->skip[PF_SKIP_DIR];
d811 1
a811 1
			r = r->skip[PF_SKIP_AF];
d813 1
a813 1
			r = r->skip[PF_SKIP_PROTO];
d817 1
a817 1
			r = r->skip[PF_SKIP_SRC_ADDR];
d821 1
a821 1
			r = r->skip[PF_SKIP_DST_ADDR];
d1006 1
a1006 1
			r = r->skip[PF_SKIP_ACTION];
d1008 1
a1008 1
			r = r->skip[PF_SKIP_IFP];
d1010 1
a1010 1
			r = r->skip[PF_SKIP_DIR];
d1012 1
a1012 1
			r = r->skip[PF_SKIP_AF];
d1014 1
a1014 1
			r = r->skip[PF_SKIP_PROTO];
d1020 1
a1020 1
			r = r->skip[PF_SKIP_SRC_ADDR];
d1023 1
a1023 1
			r = r->skip[PF_SKIP_SRC_PORT];
d1029 1
a1029 1
			r = r->skip[PF_SKIP_DST_ADDR];
d1032 1
a1032 1
			r = r->skip[PF_SKIP_DST_PORT];
@


1.40
log
@Introduce anchors and named rule sets, allowing to load additional rule
sets with pfctl and evaluate them from the main rule set using a new type
of rule (which will support conditional evaluation soon). Makes
maintenance of sub-rulesets simpler for pfctl and daemons.

Idea and ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.39 2002/11/23 05:16:58 mcbride Exp $ */
d802 1
a802 1
	r = TAILQ_FIRST(pf_main_ruleset.rules.active.ptr);
d1003 1
a1003 1
	r = TAILQ_FIRST(pf_main_ruleset.rules.active.ptr);
@


1.39
log
@kernel code to allow multiple redirection addresses to be specified for nat
and rdr, as well as route-to, dup-to and reply-to.

Addresses can be allocated in a number of ways:
- masking out the network portion of the address and replacing it
- randomly assigning an address in the block
- hashing the source address and a key to determine the redirection address
- iterating through the addresses sequentially (this is the only allocation
  scheme which works when a list of addresses is specified)

ok dhartmei@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.38 2002/10/29 19:51:04 mickey Exp $ */
d702 2
a703 1
				DPFPRINTF(("fragcache[%d]: adjacent(merge %d-%d) %d-%d (%d-%d)\n",
d715 2
a716 1
				DPFPRINTF(("fragcache[%d]: adjacent(merge %d-%d) %d-%d (%d-%d)\n",
d802 1
a802 1
	r = TAILQ_FIRST(pf_rules_active);
d808 1
a808 1
		else if (r->direction != dir)
d1003 1
a1003 1
	r = TAILQ_FIRST(pf_rules_active);
d1009 1
a1009 1
		else if (r->direction != dir)
@


1.38
log
@keep all pflog goodies in pflog sources, avoids code duplications; okski frantzen@@ and dhartmei@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.37 2002/10/22 12:23:35 mcbride Exp $ */
d812 2
a813 2
		else if (!PF_AZERO(&r->src.mask, AF_INET) &&
		    !PF_MATCHA(r->src.not, &r->src.addr.addr, &r->src.mask,
d816 2
a817 2
		else if (!PF_AZERO(&r->dst.mask, AF_INET) &&
		    !PF_MATCHA(r->dst.not, &r->dst.addr.addr, &r->dst.mask,
d1015 2
a1016 2
		else if (!r->src.noroute && !PF_AZERO(&r->src.mask, af) &&
		    !PF_MATCHA(r->src.not, &r->src.addr.addr, &r->src.mask,
d1024 2
a1025 2
		else if (!r->dst.noroute && !PF_AZERO(&r->dst.mask, af) &&
		    !PF_MATCHA(r->dst.not, &r->dst.addr.addr, &r->dst.mask,
@


1.37
log
@Convert "int af" and "u_int8_t af" declarations and function arguments
to the more correct and descriptive "sa_family_t af"

ok dhartmei@@ henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.36 2002/10/07 14:53:00 dhartmei Exp $ */
d28 2
a57 2
#include "pflog.h"

a119 16

#if NPFLOG > 0
#define	PFLOG_PACKET(i,x,a,b,c,d,e) \
	do { \
		if (b == AF_INET) { \
			HTONS(((struct ip *)x)->ip_len); \
			HTONS(((struct ip *)x)->ip_off); \
			pflog_packet(i,a,b,c,d,e); \
			NTOHS(((struct ip *)x)->ip_len); \
			NTOHS(((struct ip *)x)->ip_off); \
		} else \
			pflog_packet(i,a,b,c,d,e); \
	} while (0)
#else
#define	PFLOG_PACKET(i,x,a,b,c,d,e)	((void)0)
#endif
@


1.36
log
@-Wsign-compare clean
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.35 2002/06/28 00:08:23 deraadt Exp $ */
d1014 2
a1015 1
	u_int8_t flags, af = pd->af;
@


1.35
log
@KNF
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.34 2002/06/11 18:03:24 frantzen Exp $ */
d636 1
a636 1
				KASSERT(m->m_len == h->ip_len - precut);
d692 1
a692 1
				KASSERT(m->m_len == h->ip_len - aftercut);
d844 1
a844 1
	if (hlen < sizeof(struct ip))
@


1.35.2.1
log
@MFC:
Fix by dhartmei@@

Fix three cases of potential accesses to free'd memory. At least one of
them could be used to panic pf with scrub rules remotely. Found by
Rob Pickering.

ok millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.35 2002/06/28 00:08:23 deraadt Exp $ */
d108 1
a108 1
struct mbuf		*pf_reassemble(struct mbuf **, struct pf_fragment **,
d111 1
a111 1
			    struct pf_fragment **, int, int, int *);
d327 1
a327 1
pf_reassemble(struct mbuf **m0, struct pf_fragment **frag,
d338 1
a338 1
	KASSERT(*frag == NULL || BUFFER_FRAGMENTS(*frag));
d345 3
a347 3
	if (*frag == NULL) {
		*frag = pool_get(&pf_frag_pl, PR_NOWAIT);
		if (*frag == NULL) {
d349 2
a350 2
			*frag = pool_get(&pf_frag_pl, PR_NOWAIT);
			if (*frag == NULL)
d354 8
a361 8
		(*frag)->fr_flags = 0;
		(*frag)->fr_max = 0;
		(*frag)->fr_src = frent->fr_ip->ip_src;
		(*frag)->fr_dst = frent->fr_ip->ip_dst;
		(*frag)->fr_p = frent->fr_ip->ip_p;
		(*frag)->fr_id = frent->fr_ip->ip_id;
		(*frag)->fr_timeout = time.tv_sec;
		LIST_INIT(&(*frag)->fr_queue);
d363 2
a364 2
		RB_INSERT(pf_frag_tree, &pf_frag_tree, *frag);
		TAILQ_INSERT_HEAD(&pf_fragqueue, *frag, frag_next);
d375 1
a375 1
	LIST_FOREACH(frea, &(*frag)->fr_queue, fr_next) {
d422 2
a423 2
	if ((*frag)->fr_max < max)
		(*frag)->fr_max = max;
d426 1
a426 1
		(*frag)->fr_flags |= PFFRAG_SEENLAST;
d429 1
a429 1
		LIST_INSERT_HEAD(&(*frag)->fr_queue, frent, fr_next);
d434 1
a434 1
	if (!((*frag)->fr_flags & PFFRAG_SEENLAST))
d439 1
a439 1
	for (frep = LIST_FIRST(&(*frag)->fr_queue); frep; frep = next) {
d443 1
a443 1
		if (off < (*frag)->fr_max &&
d447 1
a447 1
			    (*frag)->fr_max));
d451 2
a452 2
	DPFPRINTF(("%d < %d?\n", off, (*frag)->fr_max));
	if (off < (*frag)->fr_max)
d456 1
a456 1
	frent = LIST_FIRST(&(*frag)->fr_queue);
d460 1
a460 2
		pf_free_fragment(*frag);
		*frag = NULL;
d482 2
a483 2
	ip->ip_src = (*frag)->fr_src;
	ip->ip_dst = (*frag)->fr_dst;
d486 1
a486 2
	pf_remove_fragment(*frag);
	*frag = NULL;
d514 1
a514 1
pf_fragcache(struct mbuf **m0, struct ip *h, struct pf_fragment **frag, int mff,
d524 1
a524 1
	KASSERT(*frag == NULL || !BUFFER_FRAGMENTS(*frag));
d527 3
a529 3
	if (*frag == NULL) {
		*frag = pool_get(&pf_cache_pl, PR_NOWAIT);
		if (*frag == NULL) {
d531 2
a532 2
			*frag = pool_get(&pf_cache_pl, PR_NOWAIT);
			if (*frag == NULL)
d539 1
a539 1
			pool_put(&pf_cache_pl, *frag);
d544 7
a550 7
		(*frag)->fr_flags = PFFRAG_NOBUFFER;
		(*frag)->fr_max = 0;
		(*frag)->fr_src = h->ip_src;
		(*frag)->fr_dst = h->ip_dst;
		(*frag)->fr_p = h->ip_p;
		(*frag)->fr_id = h->ip_id;
		(*frag)->fr_timeout = time.tv_sec;
d554 2
a555 2
		LIST_INIT(&(*frag)->fr_cache);
		LIST_INSERT_HEAD(&(*frag)->fr_cache, cur, fr_next);
d557 2
a558 2
		RB_INSERT(pf_frag_tree, &pf_cache_tree, *frag);
		TAILQ_INSERT_HEAD(&pf_cachequeue, *frag, frag_next);
d570 1
a570 1
	LIST_FOREACH(fra, &(*frag)->fr_cache, fr_next) {
d755 2
a756 2
	if ((*frag)->fr_max < max)
		(*frag)->fr_max = max;
d760 1
a760 1
		(*frag)->fr_flags |= PFFRAG_SEENLAST;
d763 3
a765 3
	if (((*frag)->fr_flags & PFFRAG_SEENLAST) &&
	    LIST_FIRST(&(*frag)->fr_cache)->fr_off == 0 &&
	    LIST_FIRST(&(*frag)->fr_cache)->fr_end == (*frag)->fr_max) {
d768 2
a769 3
		    (*frag)->fr_max));
		pf_free_fragment(*frag);
		*frag = NULL;
d778 2
a779 2
	if (!mff && *frag != NULL)
		(*frag)->fr_flags |= PFFRAG_SEENLAST;
d787 2
a788 2
	if (!mff && *frag != NULL)
		(*frag)->fr_flags |= PFFRAG_SEENLAST;
d792 1
a792 1
		if (((*frag)->fr_flags & PFFRAG_DROP) == 0)
d795 1
a795 1
		(*frag)->fr_flags |= PFFRAG_DROP;
d900 1
a900 1
		*m0 = m = pf_reassemble(m0, &frag, frent, mff);
d905 1
a905 1
		if (frag != NULL && (frag->fr_flags & PFFRAG_DROP))
d934 1
a934 1
		*m0 = m = pf_fragcache(m0, h, &frag, mff,
d949 1
a949 1
		if (frag != NULL && (frag->fr_flags & PFFRAG_DROP))
@


1.34
log
@split the grammar of scrub(fragcache) into scrub ... 'fragment reassemble',
'fragment crop' or a new 'fragment drop-ovl' which will drop overlapping
fragments and all corresponding ones
ok kjell@@ with feedback from kjell@@ and deraadt@@.  the rest are slacking
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.33 2002/06/11 03:22:04 dhartmei Exp $ */
d163 1
a163 1
static __inline int	
d167 1
a812 1

d951 1
a951 1
 		goto fragment_pass;
d971 1
a971 1
 	if (dir != PF_OUT)
@


1.33
log
@KNF (tabs, return (x))
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.32 2002/06/11 02:27:19 frantzen Exp $ */
d72 1
d111 1
a111 1
			    struct pf_fragment *, int, int *);
d514 1
a514 1
    int *nomem)
d521 1
a603 27
			/*
			 * This is a very heavy way to trim the payload.
			 * we could do it much faster by diddling mbuf
			 * internals but that would be even less legible
			 * than this mbuf magic.  For my next trick,
			 * I'll pull a rabbit out of my laptop.
			 */
			*m0 = m_copym2(m, 0, h->ip_hl << 2, M_NOWAIT);
			if (*m0 == NULL)
				goto no_mem;
			KASSERT((*m0)->m_next == NULL);
			m_adj(m, precut + (h->ip_hl << 2));
			m_cat(*m0, m);
			m = *m0;
			if (m->m_flags & M_PKTHDR) {
				int plen = 0;
				struct mbuf *t;
				for (t = m; t; t = t->m_next)
					plen += t->m_len;
				m->m_pkthdr.len = plen;
			}


			h = mtod(m, struct ip *);

			KASSERT(m->m_len == h->ip_len - precut);

a604 2
			h->ip_off += precut >> 3;
			h->ip_len -= precut;
a605 1

d608 34
d677 1
a678 12
			m_adj(m, -aftercut);
			if (m->m_flags & M_PKTHDR) {
				int plen = 0;
				struct mbuf *t;
				for (t = m; t; t = t->m_next)
					plen += t->m_len;
				m->m_pkthdr.len = plen;
			}
			h = mtod(m, struct ip *);
			KASSERT(m->m_len == h->ip_len - aftercut);
			h->ip_len -= aftercut;
			max -= aftercut;
d681 15
d742 9
d768 1
a768 1
		pf_remove_fragment(frag);
d775 7
a781 1
	/* FALLTHROUGH */
d789 8
d876 1
a876 1
	if ((r->rule_flag & PFRULE_FRAGCACHE) == 0) {
d905 3
d910 1
a910 1
		/* non-buffering fragment cache */
d928 3
a930 1
		    max > frag->fr_max)
d932 1
d934 2
a935 1
		*m0 = m = pf_fragcache(m0, h, frag, mff, &nomem);
d949 2
@


1.32
log
@SCRUB(fragcache) to do gap tracking and overlap pruning of IPv4 fragments
without the memory overhead of the conventional defrag in SCRUB
ok dhartmei@@, idea by deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.31 2002/06/10 17:05:11 dhartmei Exp $ */
d117 2
a118 1
#define DPFPRINTF(x)	if (pf_status.debug >= PF_DEBUG_MISC) { printf("%s: ", __func__); printf x ;}
d121 11
a131 12
#define PFLOG_PACKET(i,x,a,b,c,d,e) \
        do { \
                if (b == AF_INET) { \
                        HTONS(((struct ip *)x)->ip_len); \
                        HTONS(((struct ip *)x)->ip_off); \
                        pflog_packet(i,a,b,c,d,e); \
                        NTOHS(((struct ip *)x)->ip_len); \
                        NTOHS(((struct ip *)x)->ip_off); \
                } else { \
                        pflog_packet(i,a,b,c,d,e); \
                } \
        } while (0)
d133 1
a133 1
#define		 PFLOG_PACKET(i,x,a,b,c,d,e)	((void)0)
d178 1
a178 1
	return 0;
@


1.31
log
@Don't #include <sys/malloc.h>
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.30 2002/06/08 08:09:11 frantzen Exp $ */
d64 6
d71 2
d84 6
a89 1
	LIST_HEAD(pf_fragq, pf_frent) fr_queue;
d93 1
d97 1
a97 1
RB_HEAD(pf_frag_tree, pf_fragment)	pf_frag_tree;
d106 1
a106 1
struct pf_fragment	*pf_find_fragment(struct ip *);
d109 2
d137 2
a138 2
struct pool		 pf_frent_pl, pf_frag_pl;
int			 pf_nfrents;
d148 4
d155 2
d159 1
d188 1
d195 11
d216 1
a216 1
	int goal = pf_nfrents * 9 / 10;
d218 1
a220 1

d227 11
d246 1
d249 17
a265 3
	for (frent = LIST_FIRST(&frag->fr_queue); frent;
	    frent = LIST_FIRST(&frag->fr_queue)) {
		LIST_REMOVE(frent, fr_next);
d267 3
a269 3
		m_freem(frent->fr_m);
		pool_put(&pf_frent_pl, frent);
		pf_nfrents--;
d285 1
a285 1
pf_find_fragment(struct ip *ip)
d292 1
a292 1
	frag = RB_FIND(pf_frag_tree, &pf_frag_tree, &key);
d294 1
d296 7
a302 2
		TAILQ_REMOVE(&pf_fragqueue, frag, frag_next);
		TAILQ_INSERT_HEAD(&pf_fragqueue, frag, frag_next);
d313 9
a321 3
	RB_REMOVE(pf_frag_tree, &pf_frag_tree, frag);
	TAILQ_REMOVE(&pf_fragqueue, frag, frag_next);
	pool_put(&pf_frag_pl, frag);
d336 2
d511 257
d774 1
a774 1
	struct pf_fragment *frag;
d780 3
a820 3
	/* Now we are dealing with a fragmented packet */
	frag = pf_find_fragment(h);

d827 2
a828 2
	h->ip_len -= hlen;
	h->ip_off <<= 3;
d831 2
a832 2
	if (mff && (h->ip_len & 0x7)) {
		DPFPRINTF(("mff and %d\n", h->ip_len));
d836 1
a836 1
	max = fragoff + h->ip_len;
a841 4
	/* Check if we saw the last fragment already */
	if (frag != NULL && (frag->fr_flags & PFFRAG_SEENLAST) &&
	    max > frag->fr_max)
		goto bad;
d843 13
a855 5
	/* Get an entry for the fragment queue */
	frent = pool_get(&pf_frent_pl, PR_NOWAIT);
	if (frent == NULL) {
		/* Try to clean up old fragments */
		pf_flush_fragments();
d861 26
a886 8
	}
	pf_nfrents++;
	frent->fr_ip = h;
	frent->fr_m = m;

	/* Might return a completely reassembled mbuf, or NULL */
	DPFPRINTF(("reass frag %d @@ %d-%d\n", h->ip_id, fragoff, max));
	*m0 = m = pf_reassemble(m0, frag, frent, mff);
d888 1
a888 2
	if (m == NULL)
		return (PF_DROP);
d890 21
a910 1
	h = mtod(m, struct ip *);
d927 16
@


1.30
log
@keep the count of fragments consistent when we have to do a fail safe drop
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.29 2002/06/07 21:14:02 frantzen Exp $ */
a34 1
#include <sys/malloc.h>
@


1.29
log
@switch from AVL tree's to herr Provos' red-black trees
with suggestions from provos@@
ok dhartmei@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.28 2002/05/21 08:42:35 espie Exp $ */
d432 1
@


1.28
log
@Junk gcc's deprecated __FUNCTION__. Use standard __func__ instead.
ok dhartmei@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.27 2002/05/19 22:31:28 deraadt Exp $ */
d68 1
d82 6
d89 1
a89 1
void			 pf_ip2key(struct pf_tree_key *, struct ip *);
a121 1
struct pf_tree_node	*tree_fragment;
d140 19
d169 1
a169 1
		DPFPRINTF(("expiring %p\n", frag));
d216 1
a216 1
pf_ip2key(struct pf_tree_key *key, struct ip *ip)
d218 4
a221 6
	key->proto = ip->ip_p;
	key->af = AF_INET;
	key->addr[0].addr32[0] = ip->ip_src.s_addr;
	key->addr[1].addr32[0] = ip->ip_dst.s_addr;
	key->port[0] = ip->ip_id;
	key->port[1] = 0;
d227 1
a227 1
	struct pf_tree_key key;
d232 1
a232 2
	frag = (struct pf_fragment *)pf_find_state(tree_fragment, &key);

d247 1
a247 11
	struct pf_tree_key key;

	/* XXX keep in sync with pf_ip2key */
	key.proto = frag->fr_p;
	key.af = AF_INET;
	key.addr[0].addr32[0] = frag->fr_src.s_addr;
	key.addr[1].addr32[0] = frag->fr_dst.s_addr;
	key.port[0] = frag->fr_id;
	key.port[1] = 0;

	pf_tree_remove(&tree_fragment, NULL, &key);
a248 1

a269 2
		struct pf_tree_key key;

d287 1
a287 4
		pf_ip2key(&key, frent->fr_ip);

		pf_tree_insert(&tree_fragment, NULL, &key,
		    (struct pf_state *)frag);
d531 1
a531 1
	DPFPRINTF(("reass frag %d @@ %d\n", h->ip_id, fragoff));
@


1.27
log
@KNF again
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.26 2002/05/09 21:58:12 jasoni Exp $ */
d95 1
a95 1
#define DPFPRINTF(x)	if (pf_status.debug >= PF_DEBUG_MISC) printf x
d144 1
a144 1
		DPFPRINTF((__FUNCTION__": expiring %p\n", frag));
d159 1
a159 1
	DPFPRINTF((__FUNCTION__": trying to free > %d frents\n",
d310 1
a310 1
			DPFPRINTF((__FUNCTION__": overlap -%d\n", precut));
d322 1
a322 1
		DPFPRINTF((__FUNCTION__": adjust overlap %d\n", aftercut));
d363 1
a363 2
			DPFPRINTF((__FUNCTION__
			    ": missing fragment at %d, next %d, max %d\n",
d369 1
a369 1
	DPFPRINTF((__FUNCTION__": %d < %d?\n", off, frag->fr_max));
d377 1
a377 1
		DPFPRINTF((__FUNCTION__": drop: too big: %d\n", off));
d420 1
a420 1
	DPFPRINTF((__FUNCTION__": complete: %p(%d)\n", m, ip->ip_len));
d485 1
a485 1
		DPFPRINTF((__FUNCTION__": IP_DF\n"));
d494 1
a494 1
		DPFPRINTF((__FUNCTION__": mff and %d\n", h->ip_len));
d501 1
a501 1
		DPFPRINTF((__FUNCTION__": max packet %d\n", max));
d525 1
a525 1
	DPFPRINTF((__FUNCTION__": reass frag %d @@ %d\n", h->ip_id, fragoff));
d556 1
a556 1
	DPFPRINTF((__FUNCTION__": dropping bad fragment\n"));
@


1.26
log
@Add a max-mss option to the scrub rule which will enforce a maximum mss
by lowering it to the given value.
- ok dhartmei@@, provos@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.25 2002/05/06 15:49:54 jasoni Exp $ */
d592 1
a592 1
		else if (r->src.noroute && pf_routable(pd->src, af))  
d693 1
a693 1
	
@


1.25
log
@typo in comment
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.24 2002/04/24 18:10:25 dhartmei Exp $ */
d92 2
d663 4
d678 45
@


1.24
log
@Add dynamic (in-kernel) interface name -> address translation. Instead of
using just the interface name instead of an address and reloading the rule
set whenever the interface changes its address, the interface name can be
put in parentheses, and the kernel will keep track of changes and update
rules. There is no additional cost for evaluating rules (per packet),
the cost occurs when an interface changes address (and the rules are
traversed and updated where necessary).
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.23 2002/04/20 18:26:03 dhartmei Exp $ */
d337 1
a337 1
	/* Update maxmimum data size */
@


1.23
log
@Move normalization messages from log level 'urgent' to 'misc'.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.22 2002/04/20 10:13:57 fgsch Exp $ */
d454 1
a454 1
		    !PF_MATCHA(r->src.not, &r->src.addr, &r->src.mask,
d458 1
a458 1
		    !PF_MATCHA(r->dst.not, &r->dst.addr, &r->dst.mask,
d593 1
a593 1
		    !PF_MATCHA(r->src.not, &r->src.addr, &r->src.mask,
d602 1
a602 1
		    !PF_MATCHA(r->dst.not, &r->dst.addr, &r->dst.mask,
@


1.22
log
@All calls to pool_get(9) should use PR_xx flags, not M_xx.
millert dhartmei ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.21 2002/03/27 18:16:21 mickey Exp $ */
d93 1
a93 1
#define DPFPRINTF(x)		if (pf_status.debug) printf x
@


1.21
log
@implement a "no-route" keyword.
usage semantics are analogous w/ "any", meaning is
"any ip address for which there is no route in the
current routing table", could be used in both from and to.
typical usage would be (assuming symmetrical routing):
block in from no-route to any
also doc "any" in the pf.conf.5, include in regress, etc.
tested by me on i386 and sparc.
dhartmei@@ and frantzen@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.20 2002/02/26 07:25:33 dhartmei Exp $ */
d259 1
a259 1
		frag = pool_get(&pf_frag_pl, M_NOWAIT);
d262 1
a262 1
			frag = pool_get(&pf_frag_pl, M_NOWAIT);
@


1.20
log
@Add optional pool memory hard limits, mainly as temporary solution
until pool exhaustion causes problems no more.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.19 2002/02/25 00:29:07 dhartmei Exp $ */
d590 3
a592 1
		else if (!PF_AZERO(&r->src.mask, af) &&
d599 4
a602 3
		else if (!PF_AZERO(&r->dst.mask, af) &&
			    !PF_MATCHA(r->dst.not,
			    &r->dst.addr, &r->dst.mask,
@


1.19
log
@Change timeouts from microtime() to time.tv_sec like in pf.c,
initialize fr_timeout, free frent in pf_reassemble() when it's
not inserted into a frag. ok provos@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.18 2002/02/14 15:32:11 dhartmei Exp $ */
a91 3

#define PFFRAG_FRENT_HIWAT	5000	/* Number of fragment entries */
#define PFFRAG_FRAG_HIWAT	1000	/* Number of fragmented packets */
@


1.18
log
@Add skip steps for rule action (pass/block vs. scrub) and direction
(in vs. out). This speeds up rule set evaluation considerably, because
the rules set used to be linearly traversed (even twice) when looking
for scrub rules. Ok frantzen@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.17 2002/01/23 00:39:48 art Exp $ */
d75 1
a75 1
	struct timeval	fr_timeout;
d139 1
a139 7
	struct timeval now, expire;

	microtime(&now);

	timerclear(&expire);
	expire.tv_sec = pftm_frag;
	timersub(&now, &expire, &expire);
d142 1
a142 1
		if (timercmp(&frag->fr_timeout, &expire, >))
d213 1
a213 1
		microtime(&frag->fr_timeout);
d276 1
d427 1
@


1.17
log
@Pool deals fairly well with physical memory shortage, but it doesn't deal
well (not at all) with shortages of the vm_map where the pages are mapped
(usually kmem_map).

Try to deal with it:
 - group all information the backend allocator for a pool in a separate
   struct. The pool will only have a pointer to that struct.
 - change the pool_init API to reflect that.
 - link all pools allocating from the same allocator on a linked list.
 - Since an allocator is responsible to wait for physical memory it will
   only fail (waitok) when it runs out of its backing vm_map, carefully
   drain pools using the same allocator so that va space is freed.
   (see comments in code for caveats and details).
 - change pool_reclaim to return if it actually succeeded to free some
   memory, use that information to make draining easier and more efficient.
 - get rid of PR_URGENT, noone uses it.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.16 2001/12/03 22:25:06 dhartmei Exp $ */
d448 21
a468 3
	TAILQ_FOREACH(r, pf_rules_active, entries) {
		if ((r->action == PF_SCRUB) &&
		    MATCH_TUPLE(h, r, dir, ifp, AF_INET))
d587 3
a589 5
		if (r->action != PF_SCRUB) {
			r = TAILQ_NEXT(r, entries);
			continue;
		}
		if (r->ifp != NULL && r->ifp != ifp)
d591 2
a611 4
		else if (r->direction != dir)
			r = TAILQ_NEXT(r, entries);
		else if (r->ifp != NULL && r->ifp != ifp)
			r = TAILQ_NEXT(r, entries);
@


1.16
log
@reason int -> u_short. From Mike Pechkin.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.15 2001/11/06 11:48:29 dhartmei Exp $ */
d125 1
a125 1
	    0, NULL, NULL, 0);
d127 1
a127 1
	    0, NULL, NULL, 0);
@


1.16.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.17 2002/01/23 00:39:48 art Exp $ */
d125 1
a125 1
	    NULL);
d127 1
a127 1
	    NULL);
@


1.16.2.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.16.2.1 2002/01/31 22:55:44 niklas Exp $ */
a67 1
	RB_ENTRY(pf_fragment) fr_entry;
d75 1
a75 1
	u_int32_t	fr_timeout;
a80 6
static __inline int	 pf_frag_compare(struct pf_fragment *,
			    struct pf_fragment *);
RB_HEAD(pf_frag_tree, pf_fragment)	pf_frag_tree;
RB_PROTOTYPE(pf_frag_tree, pf_fragment, fr_entry, pf_frag_compare);
RB_GENERATE(pf_frag_tree, pf_fragment, fr_entry, pf_frag_compare);

d82 1
a82 1
void			 pf_ip2key(struct pf_fragment *, struct ip *);
a91 2
int			 pf_normalize_tcpopt(struct pf_rule *, struct mbuf *,
			    struct tcphdr *, int);
d93 4
a96 1
#define DPFPRINTF(x)	if (pf_status.debug >= PF_DEBUG_MISC) { printf("%s: ", __func__); printf x ;}
d116 1
a134 19
static __inline int	
pf_frag_compare(struct pf_fragment *a, struct pf_fragment *b)
{
	int diff;
	if ((diff = a->fr_id - b->fr_id))
		return (diff);
	else if ((diff = a->fr_p - b->fr_p))
		return (diff);
	else if (a->fr_src.s_addr < b->fr_src.s_addr)
		return (-1);
	else if (a->fr_src.s_addr > b->fr_src.s_addr)
		return (1);
	else if (a->fr_dst.s_addr < b->fr_dst.s_addr)
		return (-1);
	else if (a->fr_dst.s_addr > b->fr_dst.s_addr)
		return (1);
	return 0;
}

d139 7
a145 1
	u_int32_t expire = time.tv_sec - pftm_frag;
d148 1
a148 1
		if (frag->fr_timeout > expire)
d151 1
a151 1
		DPFPRINTF(("expiring %d(%p)\n", frag->fr_id, frag));
d166 1
a166 1
	DPFPRINTF(("trying to free > %d frents\n",
d198 1
a198 1
pf_ip2key(struct pf_fragment *key, struct ip *ip)
d200 6
a205 4
	key->fr_p = ip->ip_p;
	key->fr_id = ip->ip_id;
	key->fr_src.s_addr = ip->ip_src.s_addr;
	key->fr_dst.s_addr = ip->ip_dst.s_addr;
d211 1
a211 1
	struct pf_fragment key;
d216 2
a217 1
	frag = RB_FIND(pf_frag_tree, &pf_frag_tree, &key);
d219 1
a219 1
		frag->fr_timeout = time.tv_sec;
d232 11
a242 1
	RB_REMOVE(pf_frag_tree, &pf_frag_tree, frag);
d244 1
d266 3
a268 1
		frag = pool_get(&pf_frag_pl, PR_NOWAIT);
d271 1
a271 1
			frag = pool_get(&pf_frag_pl, PR_NOWAIT);
a281 1
		frag->fr_timeout = time.tv_sec;
d284 4
a287 1
		RB_INSERT(pf_frag_tree, &pf_frag_tree, frag);
d316 1
a316 1
			DPFPRINTF(("overlap -%d\n", precut));
d328 1
a328 1
		DPFPRINTF(("adjust overlap %d\n", aftercut));
d345 1
a345 1
	/* Update maximum data size */
d369 2
a370 1
			DPFPRINTF(("missing fragment at %d, next %d, max %d\n",
d376 1
a376 1
	DPFPRINTF(("%d < %d?\n", off, frag->fr_max));
d384 1
a384 1
		DPFPRINTF(("drop: too big: %d\n", off));
d427 1
a427 1
	DPFPRINTF(("complete: %p(%d)\n", m, ip->ip_len));
a431 2
	pool_put(&pf_frent_pl, frent);
	pf_nfrents--;
d448 3
a450 21
	r = TAILQ_FIRST(pf_rules_active);
	while (r != NULL) {
		if (r->action != PF_SCRUB)
			r = r->skip[PF_SKIP_ACTION];
		else if (r->ifp != NULL && r->ifp != ifp)
			r = r->skip[PF_SKIP_IFP];
		else if (r->direction != dir)
			r = r->skip[PF_SKIP_DIR];
		else if (r->af && r->af != AF_INET)
			r = r->skip[PF_SKIP_AF];
		else if (r->proto && r->proto != h->ip_p)
			r = r->skip[PF_SKIP_PROTO];
		else if (!PF_AZERO(&r->src.mask, AF_INET) &&
		    !PF_MATCHA(r->src.not, &r->src.addr.addr, &r->src.mask,
		    (struct pf_addr *)&h->ip_src.s_addr, AF_INET))
			r = r->skip[PF_SKIP_SRC_ADDR];
		else if (!PF_AZERO(&r->dst.mask, AF_INET) &&
		    !PF_MATCHA(r->dst.not, &r->dst.addr.addr, &r->dst.mask,
		    (struct pf_addr *)&h->ip_dst.s_addr, AF_INET))
			r = r->skip[PF_SKIP_DST_ADDR];
		else
d473 1
a473 1
		DPFPRINTF(("IP_DF\n"));
d482 1
a482 1
		DPFPRINTF(("mff and %d\n", h->ip_len));
d489 1
a489 1
		DPFPRINTF(("max packet %d\n", max));
d513 1
a513 1
	DPFPRINTF(("reass frag %d @@ %d-%d\n", h->ip_id, fragoff, max));
d544 1
a544 1
	DPFPRINTF(("dropping bad fragment\n"));
d569 5
a573 3
		if (r->action != PF_SCRUB)
			r = r->skip[PF_SKIP_ACTION];
		else if (r->ifp != NULL && r->ifp != ifp)
a574 2
		else if (r->direction != dir)
			r = r->skip[PF_SKIP_DIR];
d579 2
a580 4
		else if (r->src.noroute && pf_routable(pd->src, af))
			r = TAILQ_NEXT(r, entries);
		else if (!r->src.noroute && !PF_AZERO(&r->src.mask, af) &&
		    !PF_MATCHA(r->src.not, &r->src.addr.addr, &r->src.mask,
d586 3
a588 4
		else if (r->dst.noroute && pf_routable(pd->dst, af))
			r = TAILQ_NEXT(r, entries);
		else if (!r->dst.noroute && !PF_AZERO(&r->dst.mask, af) &&
		    !PF_MATCHA(r->dst.not, &r->dst.addr.addr, &r->dst.mask,
d594 4
a650 4
	/* Process options */
	if (r->max_mss && pf_normalize_tcpopt(r, m, th, off))
		rewrite = 1;

a661 45
}

int
pf_normalize_tcpopt(struct pf_rule *r, struct mbuf *m, struct tcphdr *th,
    int off)
{
	u_int16_t *mss;
	int thoff;
	int opt, cnt, optlen = 0;
	int rewrite = 0;
	u_char *optp;

	thoff = th->th_off << 2;
	cnt = thoff - sizeof(struct tcphdr);
	optp = mtod(m, caddr_t) + off + sizeof(struct tcphdr);

	for (; cnt > 0; cnt -= optlen, optp += optlen) {
		opt = optp[0];
		if (opt == TCPOPT_EOL)
			break;
		if (opt == TCPOPT_NOP)
			optlen = 1;
		else {
			if (cnt < 2)
				break;
			optlen = optp[1];
			if (optlen < 2 || optlen > cnt)
				break;
		}
		switch (opt) {
		case TCPOPT_MAXSEG:
			mss = (u_int16_t *)(optp + 2);
			if ((ntohs(*mss)) > r->max_mss) {
				th->th_sum = pf_cksum_fixup(th->th_sum,
				    *mss, htons(r->max_mss));
				*mss = htons(r->max_mss);
				rewrite = 1;
			}
			break;
		default:
			break;
		}
	}

	return (rewrite);
@


1.16.2.3
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.16.2.2 2002/06/11 03:30:46 art Exp $ */
d35 1
a64 6
struct pf_frcache {
	LIST_ENTRY(pf_frcache) fr_next;
	uint16_t	fr_off;
	uint16_t	fr_end;
};

a65 3
#define PFFRAG_NOBUFFER	0x0002		/* Non-buffering fragment cache */
#define PFFRAG_DROP	0x0004		/* Drop all fragments */
#define BUFFER_FRAGMENTS(fr)	(!((fr)->fr_flags & PFFRAG_NOBUFFER))
d77 1
a77 6
#define fr_queue	fr_u.fru_queue
#define fr_cache	fr_u.fru_cache
	union {
		LIST_HEAD(pf_fragq, pf_frent) fru_queue;	/* buffering */
		LIST_HEAD(pf_cacheq, pf_frcache) fru_cache;	/* non-buf */
	} fr_u;
a80 1
TAILQ_HEAD(pf_cachequeue, pf_fragment)	pf_cachequeue;
d84 1
a84 1
RB_HEAD(pf_frag_tree, pf_fragment)	pf_frag_tree, pf_cache_tree;
d93 1
a93 1
struct pf_fragment	*pf_find_fragment(struct ip *, struct pf_frag_tree *);
a95 2
struct mbuf		*pf_fragcache(struct mbuf **, struct ip*,
			    struct pf_fragment *, int, int, int *);
d102 1
a102 2
#define	DPFPRINTF(x)	if (pf_status.debug >= PF_DEBUG_MISC) \
			    { printf("%s: ", __func__); printf x ;}
d105 12
a116 11
#define	PFLOG_PACKET(i,x,a,b,c,d,e) \
	do { \
		if (b == AF_INET) { \
			HTONS(((struct ip *)x)->ip_len); \
			HTONS(((struct ip *)x)->ip_off); \
			pflog_packet(i,a,b,c,d,e); \
			NTOHS(((struct ip *)x)->ip_len); \
			NTOHS(((struct ip *)x)->ip_off); \
		} else \
			pflog_packet(i,a,b,c,d,e); \
	} while (0)
d118 1
a118 1
#define	PFLOG_PACKET(i,x,a,b,c,d,e)	((void)0)
d122 2
a123 2
struct pool		 pf_frent_pl, pf_frag_pl, pf_cache_pl, pf_cent_pl;
int			 pf_nfrents, pf_ncache;
a132 4
	pool_init(&pf_cache_pl, sizeof(struct pf_fragment), 0, 0, 0,
	    "pffrcache", NULL);
	pool_init(&pf_cent_pl, sizeof(struct pf_frcache), 0, 0, 0, "pffrcent",
	    NULL);
a135 2
	pool_sethardlimit(&pf_cache_pl, PFFRAG_FRCACHE_HIWAT, NULL, 0);
	pool_sethardlimit(&pf_cent_pl, PFFRAG_FRCENT_HIWAT, NULL, 0);
a137 1
	TAILQ_INIT(&pf_cachequeue);
d140 1
a140 1
static __inline int
a143 1

d156 1
a156 1
	return (0);
a165 1
		KASSERT(BUFFER_FRAGMENTS(frag));
a171 11

	while ((frag = TAILQ_LAST(&pf_cachequeue, pf_cachequeue)) != NULL) {
		KASSERT(!BUFFER_FRAGMENTS(frag));
		if (frag->fr_timeout > expire)
			break;

		DPFPRINTF(("expiring %d(%p)\n", frag->fr_id, frag));
		pf_free_fragment(frag);
		KASSERT(TAILQ_EMPTY(&pf_cachequeue) ||
		    TAILQ_LAST(&pf_cachequeue, pf_cachequeue) != frag);
	}
d182 1
a182 1
	int goal;
a183 1
	goal = pf_nfrents * 9 / 10;
d186 1
a192 11


	goal = pf_ncache * 9 / 10;
	DPFPRINTF(("trying to free > %d cache entries\n",
		   pf_ncache - goal));
	while (goal < pf_ncache) {
		frag = TAILQ_LAST(&pf_cachequeue, pf_cachequeue);
		if (frag == NULL)
			break;
		pf_free_fragment(frag);
	}
a200 1
	struct pf_frcache *frcache;
d203 3
a205 17
	if (BUFFER_FRAGMENTS(frag)) {
		for (frent = LIST_FIRST(&frag->fr_queue); frent;
		    frent = LIST_FIRST(&frag->fr_queue)) {
			LIST_REMOVE(frent, fr_next);

			m_freem(frent->fr_m);
			pool_put(&pf_frent_pl, frent);
			pf_nfrents--;
		}
	} else {
		for (frcache = LIST_FIRST(&frag->fr_cache); frcache;
		    frcache = LIST_FIRST(&frag->fr_cache)) {
			LIST_REMOVE(frcache, fr_next);

			KASSERT(LIST_EMPTY(&frag->fr_cache) ||
			    LIST_FIRST(&frag->fr_cache)->fr_off >
			    frcache->fr_end);
d207 3
a209 3
			pool_put(&pf_cent_pl, frcache);
			pf_ncache--;
		}
d225 1
a225 1
pf_find_fragment(struct ip *ip, struct pf_frag_tree *tree)
d232 1
a232 1
	frag = RB_FIND(pf_frag_tree, tree, &key);
a233 1
		/* XXX Are we sure we want to update the timeout? */
d235 2
a236 7
		if (BUFFER_FRAGMENTS(frag)) {
			TAILQ_REMOVE(&pf_fragqueue, frag, frag_next);
			TAILQ_INSERT_HEAD(&pf_fragqueue, frag, frag_next);
		} else {
			TAILQ_REMOVE(&pf_cachequeue, frag, frag_next);
			TAILQ_INSERT_HEAD(&pf_cachequeue, frag, frag_next);
		}
d247 3
a249 9
	if (BUFFER_FRAGMENTS(frag)) {
		RB_REMOVE(pf_frag_tree, &pf_frag_tree, frag);
		TAILQ_REMOVE(&pf_fragqueue, frag, frag_next);
		pool_put(&pf_frag_pl, frag);
	} else {
		RB_REMOVE(pf_frag_tree, &pf_cache_tree, frag);
		TAILQ_REMOVE(&pf_cachequeue, frag, frag_next);
		pool_put(&pf_cache_pl, frag);
	}
a263 2
	KASSERT(frag == NULL || BUFFER_FRAGMENTS(frag));

a436 289
struct mbuf *
pf_fragcache(struct mbuf **m0, struct ip *h, struct pf_fragment *frag, int mff,
    int drop, int *nomem)
{
	struct mbuf *m = *m0;
	struct pf_frcache *frp, *fra, *cur = NULL;
	int ip_len = h->ip_len - (h->ip_hl << 2);
	u_int16_t off = h->ip_off << 3;
	u_int16_t max = ip_len + off;
	int hosed = 0;

	KASSERT(frag == NULL || !BUFFER_FRAGMENTS(frag));

	/* Create a new range queue for this packet */
	if (frag == NULL) {
		frag = pool_get(&pf_cache_pl, PR_NOWAIT);
		if (frag == NULL) {
			pf_flush_fragments();
			frag = pool_get(&pf_cache_pl, PR_NOWAIT);
			if (frag == NULL)
				goto no_mem;
		}

		/* Get an entry for the queue */
		cur = pool_get(&pf_cent_pl, PR_NOWAIT);
		if (cur == NULL) {
			pool_put(&pf_cache_pl, frag);
			goto no_mem;
		}
		pf_ncache++;

		frag->fr_flags = PFFRAG_NOBUFFER;
		frag->fr_max = 0;
		frag->fr_src = h->ip_src;
		frag->fr_dst = h->ip_dst;
		frag->fr_p = h->ip_p;
		frag->fr_id = h->ip_id;
		frag->fr_timeout = time.tv_sec;

		cur->fr_off = off;
		cur->fr_end = max;
		LIST_INIT(&frag->fr_cache);
		LIST_INSERT_HEAD(&frag->fr_cache, cur, fr_next);

		RB_INSERT(pf_frag_tree, &pf_cache_tree, frag);
		TAILQ_INSERT_HEAD(&pf_cachequeue, frag, frag_next);

		DPFPRINTF(("fragcache[%d]: new %d-%d\n", h->ip_id, off, max));

		goto pass;
	}

	/*
	 * Find a fragment after the current one:
	 *  - off contains the real shifted offset.
	 */
	frp = NULL;
	LIST_FOREACH(fra, &frag->fr_cache, fr_next) {
		if (fra->fr_off > off)
			break;
		frp = fra;
	}

	KASSERT(frp != NULL || fra != NULL);

	if (frp != NULL) {
		int precut;

		precut = frp->fr_end - off;
		if (precut >= ip_len) {
			/* Fragment is entirely a duplicate */
			DPFPRINTF(("fragcache[%d]: dead (%d-%d) %d-%d\n",
			    h->ip_id, frp->fr_off, frp->fr_end, off, max));
			goto drop_fragment;
		}
		if (precut == 0) {
			/* They are adjacent.  Fixup cache entry */
			DPFPRINTF(("fragcache[%d]: adjacent (%d-%d) %d-%d\n",
			    h->ip_id, frp->fr_off, frp->fr_end, off, max));
			frp->fr_end = max;
		} else if (precut > 0) {
			/* The first part of this payload overlaps with a
			 * fragment that has already been passed.
			 * Need to trim off the first part of the payload.
			 * But to do so easily, we need to create another
			 * mbuf to throw the original header into.
			 */

			DPFPRINTF(("fragcache[%d]: chop %d (%d-%d) %d-%d\n",
			    h->ip_id, precut, frp->fr_off, frp->fr_end, off,
			    max));

			off += precut;
			max -= precut;
			/* Update the previous frag to encompas this one */
			frp->fr_end = max;

			if (!drop) {
				/* XXX Optimization opportunity
				 * This is a very heavy way to trim the payload.
				 * we could do it much faster by diddling mbuf
				 * internals but that would be even less legible
				 * than this mbuf magic.  For my next trick,
				 * I'll pull a rabbit out of my laptop.
				 */
				*m0 = m_copym2(m, 0, h->ip_hl << 2, M_NOWAIT);
				if (*m0 == NULL)
					goto no_mem;
				KASSERT((*m0)->m_next == NULL);
				m_adj(m, precut + (h->ip_hl << 2));
				m_cat(*m0, m);
				m = *m0;
				if (m->m_flags & M_PKTHDR) {
					int plen = 0;
					struct mbuf *t;
					for (t = m; t; t = t->m_next)
						plen += t->m_len;
					m->m_pkthdr.len = plen;
				}


				h = mtod(m, struct ip *);

				KASSERT((int)m->m_len == h->ip_len - precut);

				h->ip_off += precut >> 3;
				h->ip_len -= precut;
			} else {
				hosed++;
			}
		} else {
			/* There is a gap between fragments */

			DPFPRINTF(("fragcache[%d]: gap %d (%d-%d) %d-%d\n",
			    h->ip_id, -precut, frp->fr_off, frp->fr_end, off,
			    max));

			cur = pool_get(&pf_cent_pl, PR_NOWAIT);
			if (cur == NULL)
				goto no_mem;
			pf_ncache++;

			cur->fr_off = off;
			cur->fr_end = max;
			LIST_INSERT_AFTER(frp, cur, fr_next);
		}
	}

	if (fra != NULL) {
		int aftercut;
		int merge = 0;

		aftercut = max - fra->fr_off;
		if (aftercut == 0) {
			/* Adjacent fragments */
			DPFPRINTF(("fragcache[%d]: adjacent %d-%d (%d-%d)\n",
			    h->ip_id, off, max, fra->fr_off, fra->fr_end));
			fra->fr_off = off;
			merge = 1;
		} else if (aftercut > 0) {
			/* Need to chop off the tail of this fragment */
			DPFPRINTF(("fragcache[%d]: chop %d %d-%d (%d-%d)\n",
			    h->ip_id, aftercut, off, max, fra->fr_off,
			    fra->fr_end));
			fra->fr_off = off;
			max -= aftercut;

			merge = 1;

			if (!drop) {
				m_adj(m, -aftercut);
				if (m->m_flags & M_PKTHDR) {
					int plen = 0;
					struct mbuf *t;
					for (t = m; t; t = t->m_next)
						plen += t->m_len;
					m->m_pkthdr.len = plen;
				}
				h = mtod(m, struct ip *);
				KASSERT((int)m->m_len == h->ip_len - aftercut);
				h->ip_len -= aftercut;
			} else {
				hosed++;
			}
		} else {
			/* There is a gap between fragments */
			DPFPRINTF(("fragcache[%d]: gap %d %d-%d (%d-%d)\n",
			    h->ip_id, -aftercut, off, max, fra->fr_off,
			    fra->fr_end));

			cur = pool_get(&pf_cent_pl, PR_NOWAIT);
			if (cur == NULL)
				goto no_mem;
			pf_ncache++;

			cur->fr_off = off;
			cur->fr_end = max;
			LIST_INSERT_BEFORE(fra, cur, fr_next);
		}


		/* Need to glue together two seperate fragment descriptors */
		if (merge) {
			if (cur && fra->fr_off <= cur->fr_end) {
				/* Need to merge in a previous 'cur' */
				DPFPRINTF(("fragcache[%d]: adjacent(merge %d-%d) %d-%d (%d-%d)\n",
				    h->ip_id, cur->fr_off, cur->fr_end, off,
				    max, fra->fr_off, fra->fr_end));
				fra->fr_off = cur->fr_off;
				LIST_REMOVE(cur, fr_next);
				pool_put(&pf_cent_pl, cur);
				pf_ncache--;
				cur = NULL;

			} else if (frp && fra->fr_off <= frp->fr_end) {
				/* Need to merge in a modified 'frp' */
				KASSERT(cur == NULL);
				DPFPRINTF(("fragcache[%d]: adjacent(merge %d-%d) %d-%d (%d-%d)\n",
				    h->ip_id, frp->fr_off, frp->fr_end, off,
				    max, fra->fr_off, fra->fr_end));
				fra->fr_off = frp->fr_off;
				LIST_REMOVE(frp, fr_next);
				pool_put(&pf_cent_pl, frp);
				pf_ncache--;
				frp = NULL;

			}
		}
	}

	if (hosed) {
		/*
		 * We must keep tracking the overall fragment even when
		 * we're going to drop it anyway so that we know when to
		 * free the overall descriptor.  Thus we drop the frag late.
		 */
		goto drop_fragment;
	}


 pass:
	/* Update maximum data size */
	if (frag->fr_max < max)
		frag->fr_max = max;

	/* This is the last segment */
	if (!mff)
		frag->fr_flags |= PFFRAG_SEENLAST;

	/* Check if we are completely reassembled */
	if ((frag->fr_flags & PFFRAG_SEENLAST) &&
	    LIST_FIRST(&frag->fr_cache)->fr_off == 0 &&
	    LIST_FIRST(&frag->fr_cache)->fr_end == frag->fr_max) {
		/* Remove from fragment queue */
		DPFPRINTF(("fragcache[%d]: done 0-%d\n", h->ip_id,
		    frag->fr_max));
		pf_free_fragment(frag);
	}

	return (m);

 no_mem:
	*nomem = 1;

	/* Still need to pay attention to !IP_MF */
	if (!mff && frag)
		frag->fr_flags |= PFFRAG_SEENLAST;

	m_freem(m);
	return (NULL);

 drop_fragment:

	/* Still need to pay attention to !IP_MF */
	if (!mff && frag)
		frag->fr_flags |= PFFRAG_SEENLAST;

	if (drop) {
		/* This fragment has been deemed bad.  Don't reass */
		if ((frag->fr_flags & PFFRAG_DROP) == 0)
			DPFPRINTF(("fragcache[%d]: dropping overall fragment\n",
			    h->ip_id));
		frag->fr_flags |= PFFRAG_DROP;
	}

	m_freem(m);
	return (NULL);
}

d443 1
a443 1
	struct pf_fragment *frag = NULL;
a447 2
	int ip_len;
	int ip_off;
d477 1
a477 1
	if (hlen < (int)sizeof(struct ip))
d487 3
d496 2
a497 2
	ip_len = h->ip_len - hlen;
	ip_off = h->ip_off << 3;
d500 2
a501 2
	if (mff && (ip_len & 0x7)) {
		DPFPRINTF(("mff and %d\n", ip_len));
d505 1
a505 1
	max = fragoff + ip_len;
d511 4
d516 5
a520 13
	if ((r->rule_flag & (PFRULE_FRAGCROP|PFRULE_FRAGDROP)) == 0) {
		/* Fully buffer all of the fragments */

		h->ip_len = ip_len;	/* logic need muddled off/len */
		h->ip_off = ip_off;
		frag = pf_find_fragment(h, &pf_frag_tree);

		/* Check if we saw the last fragment already */
		if (frag != NULL && (frag->fr_flags & PFFRAG_SEENLAST) &&
		    max > frag->fr_max)
			goto bad;

		/* Get an entry for the fragment queue */
d526 8
a533 7
		pf_nfrents++;
		frent->fr_ip = h;
		frent->fr_m = m;

		/* Might return a completely reassembled mbuf, or NULL */
		DPFPRINTF(("reass frag %d @@ %d-%d\n", h->ip_id, fragoff, max));
		*m0 = m = pf_reassemble(m0, frag, frent, mff);
d535 2
a536 21
		if (m == NULL)
			return (PF_DROP);

		if (frag && (frag->fr_flags & PFFRAG_DROP))
			goto drop;

		h = mtod(m, struct ip *);
	} else {
		/* non-buffering fragment cache (drops or masks overlaps) */
		int nomem = 0;

		if (dir == PF_OUT) {
			if (m_tag_find(m, PACKET_TAG_PF_FRAGCACHE, NULL) !=
			    NULL) {
				/* Already passed the fragment cache in the
				 * input direction.  If we continued, it would
				 * appear to be a dup and would be dropped.
				 */
				goto fragment_pass;
			}
		}
d538 1
a538 29
		frag = pf_find_fragment(h, &pf_cache_tree);

		/* Check if we saw the last fragment already */
		if (frag != NULL && (frag->fr_flags & PFFRAG_SEENLAST) &&
		    max > frag->fr_max) {
			if (r->rule_flag & PFRULE_FRAGDROP)
				frag->fr_flags |= PFFRAG_DROP;
			goto bad;
		}

		*m0 = m = pf_fragcache(m0, h, frag, mff,
		    (r->rule_flag & PFRULE_FRAGDROP) ? 1 : 0, &nomem);
		if (m == NULL) {
			if (nomem)
				goto no_mem;
			goto drop;
		}

		if (dir == PF_IN) {
			struct m_tag *mtag;
			mtag = m_tag_get(PACKET_TAG_PF_FRAGCACHE, 0, M_NOWAIT);
			if (mtag == NULL)
				goto no_mem;
			m_tag_prepend(m, mtag);
		}
		if (frag && (frag->fr_flags & PFFRAG_DROP))
			goto drop;
		goto fragment_pass;
	}
a555 16
 fragment_pass:
	if (dir != PF_OUT)
		return (PF_PASS);

	/* Enforce a minimum ttl, may cause endless packet loops */
	if (r->min_ttl && h->ip_ttl < r->min_ttl)
		h->ip_ttl = r->min_ttl;

	return (PF_PASS);

 no_mem:
	REASON_SET(reason, PFRES_MEMORY);
	if (r != NULL && r->log)
		PFLOG_PACKET(ifp, h, m, AF_INET, dir, *reason, r);
	return (PF_DROP);

d584 1
a584 2
	u_int8_t flags;
	sa_family_t af = pd->af;
@


1.16.2.4
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
a27 2
#include "pflog.h"

a37 1
#include <dev/rndvar.h>
d54 1
a54 3
#ifdef INET6
#include <netinet/ip6.h>
#endif /* INET6 */
d56 1
a56 1
#include <net/pfvar.h>
d121 16
a138 1
struct pool		 pf_state_scrub_pl;
d140 1
a152 2
	pool_init(&pf_state_scrub_pl, sizeof(struct pf_state_scrub), 0, 0, 0,
	    "pfstscr", NULL);
d166 1
a166 1
	int	diff;
d186 2
a187 3
	struct pf_fragment	*frag;
	u_int32_t		 expire = time.tv_sec -
				    pf_default_rule.timeout[PFTM_FRAG];
d211 1
a211 1
 * Try to flush old fragments to make space for new ones
d217 2
a218 2
	struct pf_fragment	*frag;
	int			 goal;
d222 1
a222 1
	    pf_nfrents - goal));
d233 1
a233 1
	    pf_ncache - goal));
d247 2
a248 2
	struct pf_frent		*frent;
	struct pf_frcache	*frcache;
d289 2
a290 2
	struct pf_fragment	 key;
	struct pf_fragment	*frag;
d330 7
a336 7
	struct mbuf	*m = *m0, *m2;
	struct pf_frent	*frea, *next;
	struct pf_frent	*frep = NULL;
	struct ip	*ip = frent->fr_ip;
	int		 hlen = ip->ip_hl << 2;
	u_int16_t	 off = ip->ip_off;
	u_int16_t	 max = ip->ip_len + off;
d383 2
a384 2
	if (frep != NULL && frep->fr_ip->ip_off + frep->fr_ip->ip_len > off) {
		u_int16_t	precut;
d389 8
a396 5
		m_adj(frent->fr_m, precut);
		DPFPRINTF(("overlap -%d\n", precut));
		/* Enforce 8 byte boundaries */
		off = ip->ip_off += precut;
		ip->ip_len -= precut;
d401 1
a401 1
		u_int16_t	aftercut;
d517 6
a522 6
	struct mbuf		*m = *m0;
	struct pf_frcache	*frp, *fra, *cur = NULL;
	int			 ip_len = h->ip_len - (h->ip_hl << 2);
	u_int16_t		 off = h->ip_off << 3;
	u_int16_t		 max = ip_len + off;
	int			 hosed = 0;
d579 1
a579 1
		int	precut;
d662 2
a663 2
		int	aftercut;
		int	merge = 0;
d718 1
a718 2
				DPFPRINTF(("fragcache[%d]: adjacent(merge "
				    "%d-%d) %d-%d (%d-%d)\n",
d730 1
a730 2
				DPFPRINTF(("fragcache[%d]: adjacent(merge "
				    "%d-%d) %d-%d (%d-%d)\n",
d805 10
a814 11
	struct mbuf		*m = *m0;
	struct pf_rule		*r;
	struct pf_frent		*frent;
	struct pf_fragment	*frag = NULL;
	struct ip		*h = mtod(m, struct ip *);
	int			 mff = (h->ip_off & IP_MF);
	int			 hlen = h->ip_hl << 2;
	u_int16_t		 fragoff = (h->ip_off & IP_OFFMASK) << 3;
	u_int16_t		 max;
	int			 ip_len;
	int			 ip_off;
d816 1
a816 1
	r = TAILQ_FIRST(pf_main_ruleset.rules[PF_RULESET_SCRUB].active.ptr);
d818 6
a823 5
		r->evaluations++;
		if (r->ifp != NULL && r->ifp != ifp)
			r = r->skip[PF_SKIP_IFP].ptr;
		else if (r->direction && r->direction != dir)
			r = r->skip[PF_SKIP_DIR].ptr;
d825 1
a825 1
			r = r->skip[PF_SKIP_AF].ptr;
d827 9
a835 7
			r = r->skip[PF_SKIP_PROTO].ptr;
		else if (PF_MISMATCHAW(&r->src.addr,
		    (struct pf_addr *)&h->ip_src.s_addr, AF_INET, r->src.not))
			r = r->skip[PF_SKIP_SRC_ADDR].ptr;
		else if (PF_MISMATCHAW(&r->dst.addr,
		    (struct pf_addr *)&h->ip_dst.s_addr, AF_INET, r->dst.not))
			r = r->skip[PF_SKIP_DST_ADDR].ptr;
a841 2
	else
		r->packets++;
a849 4
	/* Clear IP_DF if the rule uses the no-df option */
	if (r->rule_flag & PFRULE_NODF)
		h->ip_off &= ~IP_DF;

d854 1
a854 4
	/* We're dealing with a fragment now. Don't allow fragments
	 * with IP_DF to enter the cache. If the flag was cleared by
	 * no-df above, fine. Otherwise drop it.
	 */
d911 1
a911 1
		int	nomem = 0;
d943 1
a943 2
			struct m_tag	*mtag;

d955 3
d959 4
a962 1
	h->ip_off &= IP_DF;
a967 3
	if (r->rule_flag & PFRULE_RANDOMID)
		h->ip_id = ip_randomid();

d971 3
d983 1
a983 1
		PFLOG_PACKET(ifp, h, m, AF_INET, dir, *reason, r, NULL, NULL);
d989 1
a989 1
		PFLOG_PACKET(ifp, h, m, AF_INET, dir, *reason, r, NULL, NULL);
d1001 1
a1001 1
		PFLOG_PACKET(ifp, h, m, AF_INET, dir, *reason, r, NULL, NULL);
d1010 6
a1015 6
	struct pf_rule	*r, *rm = NULL;
	struct tcphdr	*th = pd->hdr.tcp;
	int		 rewrite = 0;
	u_short		 reason;
	u_int8_t	 flags;
	sa_family_t	 af = pd->af;
d1017 1
a1017 1
	r = TAILQ_FIRST(pf_main_ruleset.rules[PF_RULESET_SCRUB].active.ptr);
d1019 6
a1024 5
		r->evaluations++;
		if (r->ifp != NULL && r->ifp != ifp)
			r = r->skip[PF_SKIP_IFP].ptr;
		else if (r->direction && r->direction != dir)
			r = r->skip[PF_SKIP_DIR].ptr;
d1026 1
a1026 1
			r = r->skip[PF_SKIP_AF].ptr;
d1028 7
a1034 3
			r = r->skip[PF_SKIP_PROTO].ptr;
		else if (PF_MISMATCHAW(&r->src.addr, pd->src, af, r->src.not))
			r = r->skip[PF_SKIP_SRC_ADDR].ptr;
d1037 7
a1043 3
			r = r->skip[PF_SKIP_SRC_PORT].ptr;
		else if (PF_MISMATCHAW(&r->dst.addr, pd->dst, af, r->dst.not))
			r = r->skip[PF_SKIP_DST_ADDR].ptr;
d1046 1
a1046 1
			r = r->skip[PF_SKIP_DST_PORT].ptr;
a1054 5
	else
		r->packets++;

	if (rm->rule_flag & PFRULE_REASSEMBLE_TCP)
		pd->flags |= PFDESC_TCP_NORM;
d1082 1
a1082 1
		u_int16_t	ov, nv;
d1112 2
a1113 2
	if (rm != NULL && r->log)
		PFLOG_PACKET(ifp, h, m, AF_INET, dir, reason, r, NULL, NULL);
a1117 188
pf_normalize_tcp_init(struct mbuf *m, int off, struct pf_pdesc *pd,
    struct tcphdr *th, struct pf_state_peer *src, struct pf_state_peer *dst)
{
	u_int8_t hdr[60];
	u_int8_t *opt;

	KASSERT(src->scrub == NULL);

	src->scrub = pool_get(&pf_state_scrub_pl, PR_NOWAIT);
	if (src->scrub == NULL)
		return (1);
	bzero(src->scrub, sizeof(*src->scrub));

	switch (pd->af) {
#ifdef INET
	case AF_INET: {
		struct ip *h = mtod(m, struct ip *);
		src->scrub->pfss_ttl = h->ip_ttl;
		break;
	}
#endif /* INET */
#ifdef INET6
	case AF_INET6: {
		struct ip6_hdr *h = mtod(m, struct ip6_hdr *);
		src->scrub->pfss_ttl = h->ip6_hlim;
		break;
	}
#endif /* INET6 */
	}


	/*
	 * All normalizations below are only begun if we see the start of
	 * the connections.  They must all set an enabled bit in pfss_flags
	 */
	if ((th->th_flags & TH_SYN) == 0)
		return 0;


	if (th->th_off > (sizeof(struct tcphdr) >> 2) && src->scrub &&
	    pf_pull_hdr(m, off, hdr, th->th_off << 2, NULL, NULL, pd->af)) {
		/* Diddle with TCP options */
		int hlen;
		opt = hdr + sizeof(struct tcphdr);
		hlen = (th->th_off << 2) - sizeof(struct tcphdr);
		while (hlen >= TCPOLEN_TIMESTAMP) {
			switch (*opt) {
			case TCPOPT_EOL:	/* FALLTHROUH */
			case TCPOPT_NOP:
				opt++;
				hlen--;
				break;
			case TCPOPT_TIMESTAMP:
				if (opt[1] >= TCPOLEN_TIMESTAMP) {
					src->scrub->pfss_flags |=
					    PFSS_TIMESTAMP;
					src->scrub->pfss_ts_mod = arc4random();
				}
				/* FALLTHROUGH */
			default:
				hlen -= opt[1];
				opt += opt[1];
				break;
			}
		}
	}

	return (0);
}

void
pf_normalize_tcp_cleanup(struct pf_state *state)
{
	if (state->src.scrub)
		pool_put(&pf_state_scrub_pl, state->src.scrub);
	if (state->dst.scrub)
		pool_put(&pf_state_scrub_pl, state->dst.scrub);

	/* Someday... flush the TCP segment reassembly descriptors. */
}

int
pf_normalize_tcp_stateful(struct mbuf *m, int off, struct pf_pdesc *pd,
    u_short *reason, struct tcphdr *th, struct pf_state_peer *src,
    struct pf_state_peer *dst, int *writeback)
{
	u_int8_t hdr[60];
	u_int8_t *opt;
	int copyback = 0;

	KASSERT(src->scrub || dst->scrub);

	/*
	 * Enforce the minimum TTL seen for this connection.  Negate a common
	 * technique to evade an intrusion detection system and confuse
	 * firewall state code.
	 */
	switch (pd->af) {
#ifdef INET
	case AF_INET: {
		if (src->scrub) {
			struct ip *h = mtod(m, struct ip *);
			if (h->ip_ttl > src->scrub->pfss_ttl)
				src->scrub->pfss_ttl = h->ip_ttl;
			h->ip_ttl = src->scrub->pfss_ttl;
		}
		break;
	}
#endif /* INET */
#ifdef INET6
	case AF_INET6: {
		if (dst->scrub) {
			struct ip6_hdr *h = mtod(m, struct ip6_hdr *);
			if (h->ip6_hlim > src->scrub->pfss_ttl)
				src->scrub->pfss_ttl = h->ip6_hlim;
			h->ip6_hlim = src->scrub->pfss_ttl;
		}
		break;
	}
#endif /* INET6 */
	}

	if (th->th_off > (sizeof(struct tcphdr) >> 2) &&
	    ((src->scrub && (src->scrub->pfss_flags & PFSS_TIMESTAMP)) ||
	    (dst->scrub && (dst->scrub->pfss_flags & PFSS_TIMESTAMP))) &&
	    pf_pull_hdr(m, off, hdr, th->th_off << 2, NULL, NULL, pd->af)) {
		/* Diddle with TCP options */
		int hlen;
		opt = hdr + sizeof(struct tcphdr);
		hlen = (th->th_off << 2) - sizeof(struct tcphdr);
		while (hlen >= TCPOLEN_TIMESTAMP) {
			switch (*opt) {
			case TCPOPT_EOL:	/* FALLTHROUH */
			case TCPOPT_NOP:
				opt++;
				hlen--;
				break;
			case TCPOPT_TIMESTAMP:
				/* Modulate the timestamps.  Can be used for
				 * NAT detection, OS uptime determination or
				 * reboot detection.
				 */
				if (opt[1] >= TCPOLEN_TIMESTAMP) {
					u_int32_t ts_value;
					if (src->scrub &&
					    (src->scrub->pfss_flags &
					    PFSS_TIMESTAMP)) {
						memcpy(&ts_value, &opt[2],
						    sizeof(u_int32_t));
						ts_value = htonl(ntohl(ts_value)
						    + src->scrub->pfss_ts_mod);
						pf_change_a(&opt[2],
						    &th->th_sum, ts_value, 0);
						copyback = 1;
					}
					if (dst->scrub &&
					    (dst->scrub->pfss_flags &
					    PFSS_TIMESTAMP)) {
						memcpy(&ts_value, &opt[6],
						    sizeof(u_int32_t));
						ts_value = htonl(ntohl(ts_value)
						    - dst->scrub->pfss_ts_mod);
						pf_change_a(&opt[6],
						    &th->th_sum, ts_value, 0);
						copyback = 1;
					}
				}
				/* FALLTHROUGH */
			default:
				hlen -= opt[1];
				opt += opt[1];
				break;
			}
		}
		if (copyback) {
			/* Copyback the options, caller copys back header */
			*writeback = 1;
			m_copyback(m, off + sizeof(struct tcphdr),
			    (th->th_off << 2) - sizeof(struct tcphdr), hdr +
			    sizeof(struct tcphdr));
		}
	}


	/* I have a dream....  TCP segment reassembly.... */
	return (0);
}
int
d1121 5
a1125 5
	u_int16_t	*mss;
	int		 thoff;
	int		 opt, cnt, optlen = 0;
	int		 rewrite = 0;
	u_char		*optp;
@


1.15
log
@Use #defines for skip step values. From dgregor@@net.ohio-state.edu.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.14 2001/10/17 22:21:42 markus Exp $ */
d563 2
a564 1
	int rewrite = 0, reason;
@


1.14
log
@make sure we use same key for removal (AF_INET was missing), ok deraadt@@, dhartmei@@
reported buy wizz@@mniam.net
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.13 2001/10/07 21:34:27 provos Exp $ */
d573 1
a573 1
			r = r->skip[0];
d575 1
a575 1
			r = r->skip[1];
d577 1
a577 1
			r = r->skip[2];
d581 1
a581 1
			r = r->skip[3];
d584 1
a584 1
			r = r->skip[4];
d589 1
a589 1
			r = r->skip[5];
d592 1
a592 1
			r = r->skip[6];
@


1.14.4.1
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
@


1.14.4.2
log
@merge in -current
@
text
@d573 1
a573 1
			r = r->skip[PF_SKIP_IFP];
d575 1
a575 1
			r = r->skip[PF_SKIP_AF];
d577 1
a577 1
			r = r->skip[PF_SKIP_PROTO];
d581 1
a581 1
			r = r->skip[PF_SKIP_SRC_ADDR];
d584 1
a584 1
			r = r->skip[PF_SKIP_SRC_PORT];
d589 1
a589 1
			r = r->skip[PF_SKIP_DST_ADDR];
d592 1
a592 1
			r = r->skip[PF_SKIP_DST_PORT];
@


1.14.4.3
log
@Merge in -current
@
text
@d563 1
a563 2
	int rewrite = 0;
	u_short reason;
@


1.14.4.4
log
@Merge in trunk
@
text
@d75 1
a75 1
	u_int32_t	fr_timeout;
d93 3
d125 1
a125 1
	    NULL);
d127 1
a127 1
	    NULL);
d139 7
a145 1
	u_int32_t expire = time.tv_sec - pftm_frag;
d148 1
a148 1
		if (frag->fr_timeout > expire)
d219 1
a219 1
		frag->fr_timeout = time.tv_sec;
a281 1
		frag->fr_timeout = time.tv_sec;
a431 1
	pool_put(&pf_frent_pl, frent);
d448 3
a450 21
	r = TAILQ_FIRST(pf_rules_active);
	while (r != NULL) {
		if (r->action != PF_SCRUB)
			r = r->skip[PF_SKIP_ACTION];
		else if (r->ifp != NULL && r->ifp != ifp)
			r = r->skip[PF_SKIP_IFP];
		else if (r->direction != dir)
			r = r->skip[PF_SKIP_DIR];
		else if (r->af && r->af != AF_INET)
			r = r->skip[PF_SKIP_AF];
		else if (r->proto && r->proto != h->ip_p)
			r = r->skip[PF_SKIP_PROTO];
		else if (!PF_AZERO(&r->src.mask, AF_INET) &&
		    !PF_MATCHA(r->src.not, &r->src.addr, &r->src.mask,
		    (struct pf_addr *)&h->ip_src.s_addr, AF_INET))
			r = r->skip[PF_SKIP_SRC_ADDR];
		else if (!PF_AZERO(&r->dst.mask, AF_INET) &&
		    !PF_MATCHA(r->dst.not, &r->dst.addr, &r->dst.mask,
		    (struct pf_addr *)&h->ip_dst.s_addr, AF_INET))
			r = r->skip[PF_SKIP_DST_ADDR];
		else
d569 5
a573 3
		if (r->action != PF_SCRUB)
			r = r->skip[PF_SKIP_ACTION];
		else if (r->ifp != NULL && r->ifp != ifp)
a574 2
		else if (r->direction != dir)
			r = r->skip[PF_SKIP_DIR];
d594 4
@


1.14.4.5
log
@Sync the SMP branch with 3.3
@
text
@a27 2
#include "pflog.h"

d35 1
d57 2
a64 6
struct pf_frcache {
	LIST_ENTRY(pf_frcache) fr_next;
	uint16_t	fr_off;
	uint16_t	fr_end;
};

a65 3
#define PFFRAG_NOBUFFER	0x0002		/* Non-buffering fragment cache */
#define PFFRAG_DROP	0x0004		/* Drop all fragments */
#define BUFFER_FRAGMENTS(fr)	(!((fr)->fr_flags & PFFRAG_NOBUFFER))
a67 1
	RB_ENTRY(pf_fragment) fr_entry;
d76 1
a76 6
#define fr_queue	fr_u.fru_queue
#define fr_cache	fr_u.fru_cache
	union {
		LIST_HEAD(pf_fragq, pf_frent) fru_queue;	/* buffering */
		LIST_HEAD(pf_cacheq, pf_frcache) fru_cache;	/* non-buf */
	} fr_u;
a79 7
TAILQ_HEAD(pf_cachequeue, pf_fragment)	pf_cachequeue;

static __inline int	 pf_frag_compare(struct pf_fragment *,
			    struct pf_fragment *);
RB_HEAD(pf_frag_tree, pf_fragment)	pf_frag_tree, pf_cache_tree;
RB_PROTOTYPE(pf_frag_tree, pf_fragment, fr_entry, pf_frag_compare);
RB_GENERATE(pf_frag_tree, pf_fragment, fr_entry, pf_frag_compare);
d82 1
a82 1
void			 pf_ip2key(struct pf_fragment *, struct ip *);
d86 1
a86 1
struct pf_fragment	*pf_find_fragment(struct ip *, struct pf_frag_tree *);
a88 2
struct mbuf		*pf_fragcache(struct mbuf **, struct ip*,
			    struct pf_fragment *, int, int, int *);
a91 2
int			 pf_normalize_tcpopt(struct pf_rule *, struct mbuf *,
			    struct tcphdr *, int);
d93 18
a110 2
#define	DPFPRINTF(x)	if (pf_status.debug >= PF_DEBUG_MISC) \
			    { printf("%s: ", __func__); printf x ;}
d113 3
a115 2
struct pool		 pf_frent_pl, pf_frag_pl, pf_cache_pl, pf_cent_pl;
int			 pf_nfrents, pf_ncache;
a124 4
	pool_init(&pf_cache_pl, sizeof(struct pf_fragment), 0, 0, 0,
	    "pffrcache", NULL);
	pool_init(&pf_cent_pl, sizeof(struct pf_frcache), 0, 0, 0, "pffrcent",
	    NULL);
a127 2
	pool_sethardlimit(&pf_cache_pl, PFFRAG_FRCACHE_HIWAT, NULL, 0);
	pool_sethardlimit(&pf_cent_pl, PFFRAG_FRCENT_HIWAT, NULL, 0);
a129 21
	TAILQ_INIT(&pf_cachequeue);
}

static __inline int
pf_frag_compare(struct pf_fragment *a, struct pf_fragment *b)
{
	int	diff;

	if ((diff = a->fr_id - b->fr_id))
		return (diff);
	else if ((diff = a->fr_p - b->fr_p))
		return (diff);
	else if (a->fr_src.s_addr < b->fr_src.s_addr)
		return (-1);
	else if (a->fr_src.s_addr > b->fr_src.s_addr)
		return (1);
	else if (a->fr_dst.s_addr < b->fr_dst.s_addr)
		return (-1);
	else if (a->fr_dst.s_addr > b->fr_dst.s_addr)
		return (1);
	return (0);
d135 2
a136 2
	struct pf_fragment	*frag;
	u_int32_t		 expire = time.tv_sec - pftm_frag;
a138 1
		KASSERT(BUFFER_FRAGMENTS(frag));
d142 1
a142 1
		DPFPRINTF(("expiring %d(%p)\n", frag->fr_id, frag));
a144 11

	while ((frag = TAILQ_LAST(&pf_cachequeue, pf_cachequeue)) != NULL) {
		KASSERT(!BUFFER_FRAGMENTS(frag));
		if (frag->fr_timeout > expire)
			break;

		DPFPRINTF(("expiring %d(%p)\n", frag->fr_id, frag));
		pf_free_fragment(frag);
		KASSERT(TAILQ_EMPTY(&pf_cachequeue) ||
		    TAILQ_LAST(&pf_cachequeue, pf_cachequeue) != frag);
	}
d148 1
a148 1
 * Try to flush old fragments to make space for new ones
d154 5
a158 2
	struct pf_fragment	*frag;
	int			 goal;
a159 3
	goal = pf_nfrents * 9 / 10;
	DPFPRINTF(("trying to free > %d frents\n",
	    pf_nfrents - goal));
a165 11


	goal = pf_ncache * 9 / 10;
	DPFPRINTF(("trying to free > %d cache entries\n",
	    pf_ncache - goal));
	while (goal < pf_ncache) {
		frag = TAILQ_LAST(&pf_cachequeue, pf_cachequeue);
		if (frag == NULL)
			break;
		pf_free_fragment(frag);
	}
d173 1
a173 2
	struct pf_frent		*frent;
	struct pf_frcache	*frcache;
d176 3
a178 17
	if (BUFFER_FRAGMENTS(frag)) {
		for (frent = LIST_FIRST(&frag->fr_queue); frent;
		    frent = LIST_FIRST(&frag->fr_queue)) {
			LIST_REMOVE(frent, fr_next);

			m_freem(frent->fr_m);
			pool_put(&pf_frent_pl, frent);
			pf_nfrents--;
		}
	} else {
		for (frcache = LIST_FIRST(&frag->fr_cache); frcache;
		    frcache = LIST_FIRST(&frag->fr_cache)) {
			LIST_REMOVE(frcache, fr_next);

			KASSERT(LIST_EMPTY(&frag->fr_cache) ||
			    LIST_FIRST(&frag->fr_cache)->fr_off >
			    frcache->fr_end);
d180 3
a182 3
			pool_put(&pf_cent_pl, frcache);
			pf_ncache--;
		}
d189 1
a189 1
pf_ip2key(struct pf_fragment *key, struct ip *ip)
d191 6
a196 4
	key->fr_p = ip->ip_p;
	key->fr_id = ip->ip_id;
	key->fr_src.s_addr = ip->ip_src.s_addr;
	key->fr_dst.s_addr = ip->ip_dst.s_addr;
d200 1
a200 1
pf_find_fragment(struct ip *ip, struct pf_frag_tree *tree)
d202 2
a203 2
	struct pf_fragment	 key;
	struct pf_fragment	*frag;
d207 2
a208 1
	frag = RB_FIND(pf_frag_tree, tree, &key);
a209 1
		/* XXX Are we sure we want to update the timeout? */
d211 2
a212 7
		if (BUFFER_FRAGMENTS(frag)) {
			TAILQ_REMOVE(&pf_fragqueue, frag, frag_next);
			TAILQ_INSERT_HEAD(&pf_fragqueue, frag, frag_next);
		} else {
			TAILQ_REMOVE(&pf_cachequeue, frag, frag_next);
			TAILQ_INSERT_HEAD(&pf_cachequeue, frag, frag_next);
		}
d223 14
a236 9
	if (BUFFER_FRAGMENTS(frag)) {
		RB_REMOVE(pf_frag_tree, &pf_frag_tree, frag);
		TAILQ_REMOVE(&pf_fragqueue, frag, frag_next);
		pool_put(&pf_frag_pl, frag);
	} else {
		RB_REMOVE(pf_frag_tree, &pf_cache_tree, frag);
		TAILQ_REMOVE(&pf_cachequeue, frag, frag_next);
		pool_put(&pf_cache_pl, frag);
	}
d243 7
a249 9
	struct mbuf	*m = *m0, *m2;
	struct pf_frent	*frea, *next;
	struct pf_frent	*frep = NULL;
	struct ip	*ip = frent->fr_ip;
	int		 hlen = ip->ip_hl << 2;
	u_int16_t	 off = ip->ip_off;
	u_int16_t	 max = ip->ip_len + off;

	KASSERT(frag == NULL || BUFFER_FRAGMENTS(frag));
d257 3
a259 1
		frag = pool_get(&pf_frag_pl, PR_NOWAIT);
d262 1
a262 1
			frag = pool_get(&pf_frag_pl, PR_NOWAIT);
d276 4
a279 1
		RB_INSERT(pf_frag_tree, &pf_frag_tree, frag);
d299 2
a300 2
	if (frep != NULL && frep->fr_ip->ip_off + frep->fr_ip->ip_len > off) {
		u_int16_t	precut;
d305 8
a312 5
		m_adj(frent->fr_m, precut);
		DPFPRINTF(("overlap -%d\n", precut));
		/* Enforce 8 byte boundaries */
		off = ip->ip_off += precut;
		ip->ip_len -= precut;
d317 1
a317 1
		u_int16_t	aftercut;
d320 1
a320 1
		DPFPRINTF(("adjust overlap %d\n", aftercut));
d337 1
a337 1
	/* Update maximum data size */
d361 2
a362 1
			DPFPRINTF(("missing fragment at %d, next %d, max %d\n",
d368 1
a368 1
	DPFPRINTF(("%d < %d?\n", off, frag->fr_max));
d376 1
a376 1
		DPFPRINTF(("drop: too big: %d\n", off));
d419 1
a419 1
	DPFPRINTF(("complete: %p(%d)\n", m, ip->ip_len));
a424 292
	pf_nfrents--;
	m_freem(m);
	return (NULL);
}

struct mbuf *
pf_fragcache(struct mbuf **m0, struct ip *h, struct pf_fragment *frag, int mff,
    int drop, int *nomem)
{
	struct mbuf		*m = *m0;
	struct pf_frcache	*frp, *fra, *cur = NULL;
	int			 ip_len = h->ip_len - (h->ip_hl << 2);
	u_int16_t		 off = h->ip_off << 3;
	u_int16_t		 max = ip_len + off;
	int			 hosed = 0;

	KASSERT(frag == NULL || !BUFFER_FRAGMENTS(frag));

	/* Create a new range queue for this packet */
	if (frag == NULL) {
		frag = pool_get(&pf_cache_pl, PR_NOWAIT);
		if (frag == NULL) {
			pf_flush_fragments();
			frag = pool_get(&pf_cache_pl, PR_NOWAIT);
			if (frag == NULL)
				goto no_mem;
		}

		/* Get an entry for the queue */
		cur = pool_get(&pf_cent_pl, PR_NOWAIT);
		if (cur == NULL) {
			pool_put(&pf_cache_pl, frag);
			goto no_mem;
		}
		pf_ncache++;

		frag->fr_flags = PFFRAG_NOBUFFER;
		frag->fr_max = 0;
		frag->fr_src = h->ip_src;
		frag->fr_dst = h->ip_dst;
		frag->fr_p = h->ip_p;
		frag->fr_id = h->ip_id;
		frag->fr_timeout = time.tv_sec;

		cur->fr_off = off;
		cur->fr_end = max;
		LIST_INIT(&frag->fr_cache);
		LIST_INSERT_HEAD(&frag->fr_cache, cur, fr_next);

		RB_INSERT(pf_frag_tree, &pf_cache_tree, frag);
		TAILQ_INSERT_HEAD(&pf_cachequeue, frag, frag_next);

		DPFPRINTF(("fragcache[%d]: new %d-%d\n", h->ip_id, off, max));

		goto pass;
	}

	/*
	 * Find a fragment after the current one:
	 *  - off contains the real shifted offset.
	 */
	frp = NULL;
	LIST_FOREACH(fra, &frag->fr_cache, fr_next) {
		if (fra->fr_off > off)
			break;
		frp = fra;
	}

	KASSERT(frp != NULL || fra != NULL);

	if (frp != NULL) {
		int	precut;

		precut = frp->fr_end - off;
		if (precut >= ip_len) {
			/* Fragment is entirely a duplicate */
			DPFPRINTF(("fragcache[%d]: dead (%d-%d) %d-%d\n",
			    h->ip_id, frp->fr_off, frp->fr_end, off, max));
			goto drop_fragment;
		}
		if (precut == 0) {
			/* They are adjacent.  Fixup cache entry */
			DPFPRINTF(("fragcache[%d]: adjacent (%d-%d) %d-%d\n",
			    h->ip_id, frp->fr_off, frp->fr_end, off, max));
			frp->fr_end = max;
		} else if (precut > 0) {
			/* The first part of this payload overlaps with a
			 * fragment that has already been passed.
			 * Need to trim off the first part of the payload.
			 * But to do so easily, we need to create another
			 * mbuf to throw the original header into.
			 */

			DPFPRINTF(("fragcache[%d]: chop %d (%d-%d) %d-%d\n",
			    h->ip_id, precut, frp->fr_off, frp->fr_end, off,
			    max));

			off += precut;
			max -= precut;
			/* Update the previous frag to encompas this one */
			frp->fr_end = max;

			if (!drop) {
				/* XXX Optimization opportunity
				 * This is a very heavy way to trim the payload.
				 * we could do it much faster by diddling mbuf
				 * internals but that would be even less legible
				 * than this mbuf magic.  For my next trick,
				 * I'll pull a rabbit out of my laptop.
				 */
				*m0 = m_copym2(m, 0, h->ip_hl << 2, M_NOWAIT);
				if (*m0 == NULL)
					goto no_mem;
				KASSERT((*m0)->m_next == NULL);
				m_adj(m, precut + (h->ip_hl << 2));
				m_cat(*m0, m);
				m = *m0;
				if (m->m_flags & M_PKTHDR) {
					int plen = 0;
					struct mbuf *t;
					for (t = m; t; t = t->m_next)
						plen += t->m_len;
					m->m_pkthdr.len = plen;
				}


				h = mtod(m, struct ip *);

				KASSERT((int)m->m_len == h->ip_len - precut);

				h->ip_off += precut >> 3;
				h->ip_len -= precut;
			} else {
				hosed++;
			}
		} else {
			/* There is a gap between fragments */

			DPFPRINTF(("fragcache[%d]: gap %d (%d-%d) %d-%d\n",
			    h->ip_id, -precut, frp->fr_off, frp->fr_end, off,
			    max));

			cur = pool_get(&pf_cent_pl, PR_NOWAIT);
			if (cur == NULL)
				goto no_mem;
			pf_ncache++;

			cur->fr_off = off;
			cur->fr_end = max;
			LIST_INSERT_AFTER(frp, cur, fr_next);
		}
	}

	if (fra != NULL) {
		int	aftercut;
		int	merge = 0;

		aftercut = max - fra->fr_off;
		if (aftercut == 0) {
			/* Adjacent fragments */
			DPFPRINTF(("fragcache[%d]: adjacent %d-%d (%d-%d)\n",
			    h->ip_id, off, max, fra->fr_off, fra->fr_end));
			fra->fr_off = off;
			merge = 1;
		} else if (aftercut > 0) {
			/* Need to chop off the tail of this fragment */
			DPFPRINTF(("fragcache[%d]: chop %d %d-%d (%d-%d)\n",
			    h->ip_id, aftercut, off, max, fra->fr_off,
			    fra->fr_end));
			fra->fr_off = off;
			max -= aftercut;

			merge = 1;

			if (!drop) {
				m_adj(m, -aftercut);
				if (m->m_flags & M_PKTHDR) {
					int plen = 0;
					struct mbuf *t;
					for (t = m; t; t = t->m_next)
						plen += t->m_len;
					m->m_pkthdr.len = plen;
				}
				h = mtod(m, struct ip *);
				KASSERT((int)m->m_len == h->ip_len - aftercut);
				h->ip_len -= aftercut;
			} else {
				hosed++;
			}
		} else {
			/* There is a gap between fragments */
			DPFPRINTF(("fragcache[%d]: gap %d %d-%d (%d-%d)\n",
			    h->ip_id, -aftercut, off, max, fra->fr_off,
			    fra->fr_end));

			cur = pool_get(&pf_cent_pl, PR_NOWAIT);
			if (cur == NULL)
				goto no_mem;
			pf_ncache++;

			cur->fr_off = off;
			cur->fr_end = max;
			LIST_INSERT_BEFORE(fra, cur, fr_next);
		}


		/* Need to glue together two seperate fragment descriptors */
		if (merge) {
			if (cur && fra->fr_off <= cur->fr_end) {
				/* Need to merge in a previous 'cur' */
				DPFPRINTF(("fragcache[%d]: adjacent(merge "
				    "%d-%d) %d-%d (%d-%d)\n",
				    h->ip_id, cur->fr_off, cur->fr_end, off,
				    max, fra->fr_off, fra->fr_end));
				fra->fr_off = cur->fr_off;
				LIST_REMOVE(cur, fr_next);
				pool_put(&pf_cent_pl, cur);
				pf_ncache--;
				cur = NULL;

			} else if (frp && fra->fr_off <= frp->fr_end) {
				/* Need to merge in a modified 'frp' */
				KASSERT(cur == NULL);
				DPFPRINTF(("fragcache[%d]: adjacent(merge "
				    "%d-%d) %d-%d (%d-%d)\n",
				    h->ip_id, frp->fr_off, frp->fr_end, off,
				    max, fra->fr_off, fra->fr_end));
				fra->fr_off = frp->fr_off;
				LIST_REMOVE(frp, fr_next);
				pool_put(&pf_cent_pl, frp);
				pf_ncache--;
				frp = NULL;

			}
		}
	}

	if (hosed) {
		/*
		 * We must keep tracking the overall fragment even when
		 * we're going to drop it anyway so that we know when to
		 * free the overall descriptor.  Thus we drop the frag late.
		 */
		goto drop_fragment;
	}


 pass:
	/* Update maximum data size */
	if (frag->fr_max < max)
		frag->fr_max = max;

	/* This is the last segment */
	if (!mff)
		frag->fr_flags |= PFFRAG_SEENLAST;

	/* Check if we are completely reassembled */
	if ((frag->fr_flags & PFFRAG_SEENLAST) &&
	    LIST_FIRST(&frag->fr_cache)->fr_off == 0 &&
	    LIST_FIRST(&frag->fr_cache)->fr_end == frag->fr_max) {
		/* Remove from fragment queue */
		DPFPRINTF(("fragcache[%d]: done 0-%d\n", h->ip_id,
		    frag->fr_max));
		pf_free_fragment(frag);
	}

	return (m);

 no_mem:
	*nomem = 1;

	/* Still need to pay attention to !IP_MF */
	if (!mff && frag)
		frag->fr_flags |= PFFRAG_SEENLAST;

	m_freem(m);
	return (NULL);

 drop_fragment:

	/* Still need to pay attention to !IP_MF */
	if (!mff && frag)
		frag->fr_flags |= PFFRAG_SEENLAST;

	if (drop) {
		/* This fragment has been deemed bad.  Don't reass */
		if ((frag->fr_flags & PFFRAG_DROP) == 0)
			DPFPRINTF(("fragcache[%d]: dropping overall fragment\n",
			    h->ip_id));
		frag->fr_flags |= PFFRAG_DROP;
	}

d432 8
a439 11
	struct mbuf		*m = *m0;
	struct pf_rule		*r;
	struct pf_frent		*frent;
	struct pf_fragment	*frag = NULL;
	struct ip		*h = mtod(m, struct ip *);
	int			 mff = (h->ip_off & IP_MF);
	int			 hlen = h->ip_hl << 2;
	u_int16_t		 fragoff = (h->ip_off & IP_OFFMASK) << 3;
	u_int16_t		 max;
	int			 ip_len;
	int			 ip_off;
d441 1
a441 1
	r = TAILQ_FIRST(pf_main_ruleset.rules[PF_RULESET_SCRUB].active.ptr);
d443 6
a448 5
		r->evaluations++;
		if (r->ifp != NULL && r->ifp != ifp)
			r = r->skip[PF_SKIP_IFP].ptr;
		else if (r->direction && r->direction != dir)
			r = r->skip[PF_SKIP_DIR].ptr;
d450 1
a450 1
			r = r->skip[PF_SKIP_AF].ptr;
d452 9
a460 7
			r = r->skip[PF_SKIP_PROTO].ptr;
		else if (PF_MISMATCHAW(&r->src.addr,
		    (struct pf_addr *)&h->ip_src.s_addr, AF_INET, r->src.not))
			r = r->skip[PF_SKIP_SRC_ADDR].ptr;
		else if (PF_MISMATCHAW(&r->dst.addr,
		    (struct pf_addr *)&h->ip_dst.s_addr, AF_INET, r->dst.not))
			r = r->skip[PF_SKIP_DST_ADDR].ptr;
a466 2
	else
		r->packets++;
d469 1
a469 1
	if (hlen < (int)sizeof(struct ip))
a474 4
	/* Clear IP_DF if the rule uses the no-df option */
	if (r->rule_flag & PFRULE_NODF)
		h->ip_off &= ~IP_DF;

d479 4
a482 4
	/* We're dealing with a fragment now. Don't allow fragments
	 * with IP_DF to enter the cache. If the flag was cleared by
	 * no-df above, fine. Otherwise drop it.
	 */
d484 1
a484 1
		DPFPRINTF(("IP_DF\n"));
d488 2
a489 2
	ip_len = h->ip_len - hlen;
	ip_off = h->ip_off << 3;
d492 2
a493 2
	if (mff && (ip_len & 0x7)) {
		DPFPRINTF(("mff and %d\n", ip_len));
d497 1
a497 1
	max = fragoff + ip_len;
d500 1
a500 1
		DPFPRINTF(("max packet %d\n", max));
d503 4
d508 5
a512 13
	if ((r->rule_flag & (PFRULE_FRAGCROP|PFRULE_FRAGDROP)) == 0) {
		/* Fully buffer all of the fragments */

		h->ip_len = ip_len;	/* logic need muddled off/len */
		h->ip_off = ip_off;
		frag = pf_find_fragment(h, &pf_frag_tree);

		/* Check if we saw the last fragment already */
		if (frag != NULL && (frag->fr_flags & PFFRAG_SEENLAST) &&
		    max > frag->fr_max)
			goto bad;

		/* Get an entry for the fragment queue */
d518 8
a525 10
		pf_nfrents++;
		frent->fr_ip = h;
		frent->fr_m = m;

		/* Might return a completely reassembled mbuf, or NULL */
		DPFPRINTF(("reass frag %d @@ %d-%d\n", h->ip_id, fragoff, max));
		*m0 = m = pf_reassemble(m0, frag, frent, mff);

		if (m == NULL)
			return (PF_DROP);
d527 2
a528 2
		if (frag && (frag->fr_flags & PFFRAG_DROP))
			goto drop;
d530 1
a530 4
		h = mtod(m, struct ip *);
	} else {
		/* non-buffering fragment cache (drops or masks overlaps) */
		int	nomem = 0;
d532 3
a534 31
		if (dir == PF_OUT) {
			if (m_tag_find(m, PACKET_TAG_PF_FRAGCACHE, NULL) !=
			    NULL) {
				/* Already passed the fragment cache in the
				 * input direction.  If we continued, it would
				 * appear to be a dup and would be dropped.
				 */
				goto fragment_pass;
			}
		}

		frag = pf_find_fragment(h, &pf_cache_tree);

		/* Check if we saw the last fragment already */
		if (frag != NULL && (frag->fr_flags & PFFRAG_SEENLAST) &&
		    max > frag->fr_max) {
			if (r->rule_flag & PFRULE_FRAGDROP)
				frag->fr_flags |= PFFRAG_DROP;
			goto bad;
		}

		*m0 = m = pf_fragcache(m0, h, frag, mff,
		    (r->rule_flag & PFRULE_FRAGDROP) ? 1 : 0, &nomem);
		if (m == NULL) {
			if (nomem)
				goto no_mem;
			goto drop;
		}

		if (dir == PF_IN) {
			struct m_tag	*mtag;
a535 11
			mtag = m_tag_get(PACKET_TAG_PF_FRAGCACHE, 0, M_NOWAIT);
			if (mtag == NULL)
				goto no_mem;
			m_tag_prepend(m, mtag);
		}
		if (frag && (frag->fr_flags & PFFRAG_DROP))
			goto drop;
		goto fragment_pass;
	}

 no_fragment:
d537 4
a540 1
	h->ip_off &= IP_DF;
a545 3
	if (r->rule_flag & PFRULE_RANDOMID)
		h->ip_id = ip_randomid();

a547 13
 fragment_pass:
	/* Enforce a minimum ttl, may cause endless packet loops */
	if (r->min_ttl && h->ip_ttl < r->min_ttl)
		h->ip_ttl = r->min_ttl;

	return (PF_PASS);

 no_mem:
	REASON_SET(reason, PFRES_MEMORY);
	if (r != NULL && r->log)
		PFLOG_PACKET(ifp, h, m, AF_INET, dir, *reason, r);
	return (PF_DROP);

d555 1
a555 1
	DPFPRINTF(("dropping bad fragment\n"));
d572 5
a576 6
	struct pf_rule	*r, *rm = NULL;
	struct tcphdr	*th = pd->hdr.tcp;
	int		 rewrite = 0;
	u_short		 reason;
	u_int8_t	 flags;
	sa_family_t	 af = pd->af;
d578 1
a578 1
	r = TAILQ_FIRST(pf_main_ruleset.rules[PF_RULESET_SCRUB].active.ptr);
d580 6
a585 5
		r->evaluations++;
		if (r->ifp != NULL && r->ifp != ifp)
			r = r->skip[PF_SKIP_IFP].ptr;
		else if (r->direction && r->direction != dir)
			r = r->skip[PF_SKIP_DIR].ptr;
d587 1
a587 1
			r = r->skip[PF_SKIP_AF].ptr;
d589 5
a593 3
			r = r->skip[PF_SKIP_PROTO].ptr;
		else if (PF_MISMATCHAW(&r->src.addr, pd->src, af, r->src.not))
			r = r->skip[PF_SKIP_SRC_ADDR].ptr;
d596 6
a601 3
			r = r->skip[PF_SKIP_SRC_PORT].ptr;
		else if (PF_MISMATCHAW(&r->dst.addr, pd->dst, af, r->dst.not))
			r = r->skip[PF_SKIP_DST_ADDR].ptr;
d604 1
a604 1
			r = r->skip[PF_SKIP_DST_PORT].ptr;
a612 2
	else
		r->packets++;
d640 1
a640 1
		u_int16_t	ov, nv;
a657 4
	/* Process options */
	if (r->max_mss && pf_normalize_tcpopt(r, m, th, off))
		rewrite = 1;

d666 2
a667 2
	if (rm != NULL && r->log)
		PFLOG_PACKET(ifp, h, m, AF_INET, dir, reason, r);
a668 45
}

int
pf_normalize_tcpopt(struct pf_rule *r, struct mbuf *m, struct tcphdr *th,
    int off)
{
	u_int16_t	*mss;
	int		 thoff;
	int		 opt, cnt, optlen = 0;
	int		 rewrite = 0;
	u_char		*optp;

	thoff = th->th_off << 2;
	cnt = thoff - sizeof(struct tcphdr);
	optp = mtod(m, caddr_t) + off + sizeof(struct tcphdr);

	for (; cnt > 0; cnt -= optlen, optp += optlen) {
		opt = optp[0];
		if (opt == TCPOPT_EOL)
			break;
		if (opt == TCPOPT_NOP)
			optlen = 1;
		else {
			if (cnt < 2)
				break;
			optlen = optp[1];
			if (optlen < 2 || optlen > cnt)
				break;
		}
		switch (opt) {
		case TCPOPT_MAXSEG:
			mss = (u_int16_t *)(optp + 2);
			if ((ntohs(*mss)) > r->max_mss) {
				th->th_sum = pf_cksum_fixup(th->th_sum,
				    *mss, htons(r->max_mss));
				*mss = htons(r->max_mss);
				rewrite = 1;
			}
			break;
		default:
			break;
		}
	}

	return (rewrite);
@


1.14.4.6
log
@Sync the SMP branch to -current. This includes moving to ELF.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.14.4.5 2003/03/28 00:41:29 niklas Exp $ */
a55 4
#ifdef INET6
#include <netinet/ip6.h>
#endif /* INET6 */

a122 1
struct pool		 pf_state_scrub_pl;
d124 1
a136 2
	pool_init(&pf_state_scrub_pl, sizeof(struct pf_state_scrub), 0, 0, 0,
	    "pfstscr", NULL);
d171 1
a171 2
	u_int32_t		 expire = time.tv_sec -
				    pf_default_rule.timeout[PFTM_FRAG];
a1085 3
	/* Inform the state code to do stateful normalizations */
	pd->flags |= PFDESC_TCP_NORM;

a1094 81
int
pf_normalize_tcp_init(struct mbuf *m, struct pf_pdesc *pd,
    struct tcphdr *th, struct pf_state_peer *src, struct pf_state_peer *dst)
{
	KASSERT(src->scrub == NULL);

	src->scrub = pool_get(&pf_state_scrub_pl, PR_NOWAIT);
	if (src->scrub == NULL)
		return (1);

	switch (pd->af) {
#ifdef INET
	case AF_INET: {
		struct ip *h = mtod(m, struct ip *);
		src->scrub->pfss_ttl = h->ip_ttl;
		break;
	}
#endif /* INET */
#ifdef INET6
	case AF_INET6: {
		struct ip6_hdr *h = mtod(m, struct ip6_hdr *);
		src->scrub->pfss_ttl = h->ip6_hlim;
		break;
	}
#endif /* INET6 */
	}
	return (0);
}

void
pf_normalize_tcp_cleanup(struct pf_state *state)
{
	if (state->src.scrub)
		pool_put(&pf_state_scrub_pl, state->src.scrub);
	if (state->dst.scrub)
		pool_put(&pf_state_scrub_pl, state->dst.scrub);

	/* Someday... flush the TCP segment reassembly descriptors. */
}

int
pf_normalize_tcp_stateful(struct mbuf *m, struct pf_pdesc *pd, u_short *reason,
    struct tcphdr *th, struct pf_state_peer *src, struct pf_state_peer *dst)
{
	KASSERT(src->scrub || dst->scrub);


	/*
	 * Enforce the minimum TTL seen for this connection.  Negate a common
	 * technique to evade an intrusion detection system and confuse
	 * firewall state code.
	 */
	switch (pd->af) {
#ifdef INET
	case AF_INET: {
		if (src->scrub) {
			struct ip *h = mtod(m, struct ip *);
			if (h->ip_ttl > src->scrub->pfss_ttl)
				src->scrub->pfss_ttl = h->ip_ttl;
			h->ip_ttl = src->scrub->pfss_ttl;
		}
		break;
	}
#endif /* INET */
#ifdef INET6
	case AF_INET6: {
		if (dst->scrub) {
			struct ip6_hdr *h = mtod(m, struct ip6_hdr *);
			if (h->ip6_hlim > src->scrub->pfss_ttl)
				src->scrub->pfss_ttl = h->ip6_hlim;
			h->ip6_hlim = src->scrub->pfss_ttl;
		}
		break;
	}
#endif /* INET6 */
	}


	/* I have a dream....  TCP segment reassembly.... */
	return (0);
}
@


1.14.4.7
log
@merge the trunk so we will get the genfs and locking fixes
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
a39 1
#include <dev/rndvar.h>
d975 1
a975 1
		PFLOG_PACKET(ifp, h, m, AF_INET, dir, *reason, r, NULL, NULL);
d981 1
a981 1
		PFLOG_PACKET(ifp, h, m, AF_INET, dir, *reason, r, NULL, NULL);
d993 1
a993 1
		PFLOG_PACKET(ifp, h, m, AF_INET, dir, *reason, r, NULL, NULL);
a1040 3
	if (rm->rule_flag & PFRULE_REASSEMBLE_TCP)
		pd->flags |= PFDESC_TCP_NORM;

d1093 3
d1101 1
a1101 1
		PFLOG_PACKET(ifp, h, m, AF_INET, dir, reason, r, NULL, NULL);
d1106 1
a1106 1
pf_normalize_tcp_init(struct mbuf *m, int off, struct pf_pdesc *pd,
a1108 3
	u_int8_t hdr[60];
	u_int8_t *opt;

a1113 1
	bzero(src->scrub, sizeof(*src->scrub));
a1130 38


	/*
	 * All normalizations below are only begun if we see the start of
	 * the connections.  They must all set an enabled bit in pfss_flags
	 */
	if ((th->th_flags & TH_SYN) == 0)
		return 0;


	if (th->th_off > (sizeof(struct tcphdr) >> 2) && src->scrub &&
	    pf_pull_hdr(m, off, hdr, th->th_off << 2, NULL, NULL, pd->af)) {
		/* Diddle with TCP options */
		int hlen;
		opt = hdr + sizeof(struct tcphdr);
		hlen = (th->th_off << 2) - sizeof(struct tcphdr);
		while (hlen >= TCPOLEN_TIMESTAMP) {
			switch (*opt) {
			case TCPOPT_EOL:	/* FALLTHROUH */
			case TCPOPT_NOP:
				opt++;
				hlen--;
				break;
			case TCPOPT_TIMESTAMP:
				if (opt[1] >= TCPOLEN_TIMESTAMP) {
					src->scrub->pfss_flags |=
					    PFSS_TIMESTAMP;
					src->scrub->pfss_ts_mod = arc4random();
				}
				/* FALLTHROUGH */
			default:
				hlen -= opt[1];
				opt += opt[1];
				break;
			}
		}
	}

d1146 2
a1147 3
pf_normalize_tcp_stateful(struct mbuf *m, int off, struct pf_pdesc *pd,
    u_short *reason, struct tcphdr *th, struct pf_state_peer *src,
    struct pf_state_peer *dst, int *writeback)
d1149 1
a1149 3
	u_int8_t hdr[60];
	u_int8_t *opt;
	int copyback = 0;
a1150 1
	KASSERT(src->scrub || dst->scrub);
a1179 61
	}

	if (th->th_off > (sizeof(struct tcphdr) >> 2) &&
	    ((src->scrub && (src->scrub->pfss_flags & PFSS_TIMESTAMP)) ||
	    (dst->scrub && (dst->scrub->pfss_flags & PFSS_TIMESTAMP))) &&
	    pf_pull_hdr(m, off, hdr, th->th_off << 2, NULL, NULL, pd->af)) {
		/* Diddle with TCP options */
		int hlen;
		opt = hdr + sizeof(struct tcphdr);
		hlen = (th->th_off << 2) - sizeof(struct tcphdr);
		while (hlen >= TCPOLEN_TIMESTAMP) {
			switch (*opt) {
			case TCPOPT_EOL:	/* FALLTHROUH */
			case TCPOPT_NOP:
				opt++;
				hlen--;
				break;
			case TCPOPT_TIMESTAMP:
				/* Modulate the timestamps.  Can be used for
				 * NAT detection, OS uptime determination or
				 * reboot detection.
				 */
				if (opt[1] >= TCPOLEN_TIMESTAMP) {
					u_int32_t ts_value;
					if (src->scrub &&
					    (src->scrub->pfss_flags &
					    PFSS_TIMESTAMP)) {
						memcpy(&ts_value, &opt[2],
						    sizeof(u_int32_t));
						ts_value = htonl(ntohl(ts_value)
						    + src->scrub->pfss_ts_mod);
						pf_change_a(&opt[2],
						    &th->th_sum, ts_value, 0);
						copyback = 1;
					}
					if (dst->scrub &&
					    (dst->scrub->pfss_flags &
					    PFSS_TIMESTAMP)) {
						memcpy(&ts_value, &opt[6],
						    sizeof(u_int32_t));
						ts_value = htonl(ntohl(ts_value)
						    - dst->scrub->pfss_ts_mod);
						pf_change_a(&opt[6],
						    &th->th_sum, ts_value, 0);
						copyback = 1;
					}
				}
				/* FALLTHROUGH */
			default:
				hlen -= opt[1];
				opt += opt[1];
				break;
			}
		}
		if (copyback) {
			/* Copyback the options, caller copys back header */
			*writeback = 1;
			m_copyback(m, off + sizeof(struct tcphdr),
			    (th->th_off << 2) - sizeof(struct tcphdr), hdr +
			    sizeof(struct tcphdr));
		}
@


1.14.4.8
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d113 1
a113 1
struct mbuf		*pf_reassemble(struct mbuf **, struct pf_fragment **,
d116 1
a116 1
			    struct pf_fragment **, int, int, int *);
d118 2
a317 1
#define FR_IP_OFF(fr)	((ntohs((fr)->fr_ip->ip_off) & IP_OFFMASK) << 3)
d319 1
a319 1
pf_reassemble(struct mbuf **m0, struct pf_fragment **frag,
d327 2
a328 3
	u_int16_t	 off = (ntohs(ip->ip_off) & IP_OFFMASK) << 3;
	u_int16_t	 ip_len = ntohs(ip->ip_len) - ip->ip_hl * 4;
	u_int16_t	 max = ip_len + off;
d330 1
a330 1
	KASSERT(*frag == NULL || BUFFER_FRAGMENTS(*frag));
d337 3
a339 3
	if (*frag == NULL) {
		*frag = pool_get(&pf_frag_pl, PR_NOWAIT);
		if (*frag == NULL) {
d341 2
a342 2
			*frag = pool_get(&pf_frag_pl, PR_NOWAIT);
			if (*frag == NULL)
d346 8
a353 8
		(*frag)->fr_flags = 0;
		(*frag)->fr_max = 0;
		(*frag)->fr_src = frent->fr_ip->ip_src;
		(*frag)->fr_dst = frent->fr_ip->ip_dst;
		(*frag)->fr_p = frent->fr_ip->ip_p;
		(*frag)->fr_id = frent->fr_ip->ip_id;
		(*frag)->fr_timeout = time.tv_sec;
		LIST_INIT(&(*frag)->fr_queue);
d355 2
a356 2
		RB_INSERT(pf_frag_tree, &pf_frag_tree, *frag);
		TAILQ_INSERT_HEAD(&pf_fragqueue, *frag, frag_next);
d367 2
a368 2
	LIST_FOREACH(frea, &(*frag)->fr_queue, fr_next) {
		if (FR_IP_OFF(frea) > off)
d375 1
a375 4
	if (frep != NULL &&
	    FR_IP_OFF(frep) + ntohs(frep->fr_ip->ip_len) - frep->fr_ip->ip_hl *
	        4 > off)
	{
d378 2
a379 3
		precut = FR_IP_OFF(frep) + ntohs(frep->fr_ip->ip_len) -
		    frep->fr_ip->ip_hl * 4 - off;
		if (precut >= ip_len)
d384 2
a385 4
		ip->ip_off = htons(ntohs(ip->ip_off) + (precut >> 3));
		off = (ntohs(ip->ip_off) & IP_OFFMASK) << 3;
		ip_len -= precut;
		ip->ip_len = htons(ip_len);
d388 2
a389 3
	for (; frea != NULL && ip_len + off > FR_IP_OFF(frea);
	    frea = next)
	{
d392 1
a392 1
		aftercut = ip_len + off - FR_IP_OFF(frea);
d394 3
a396 7
		if (aftercut < ntohs(frea->fr_ip->ip_len) - frea->fr_ip->ip_hl
		    * 4)
		{
			frea->fr_ip->ip_len =
			    htons(ntohs(frea->fr_ip->ip_len) - aftercut);
			frea->fr_ip->ip_off = htons(ntohs(frea->fr_ip->ip_off) +
			    (aftercut >> 3));
d411 2
a412 2
	if ((*frag)->fr_max < max)
		(*frag)->fr_max = max;
d415 1
a415 1
		(*frag)->fr_flags |= PFFRAG_SEENLAST;
d418 1
a418 1
		LIST_INSERT_HEAD(&(*frag)->fr_queue, frent, fr_next);
d423 1
a423 1
	if (!((*frag)->fr_flags & PFFRAG_SEENLAST))
d428 1
a428 1
	for (frep = LIST_FIRST(&(*frag)->fr_queue); frep; frep = next) {
d431 3
a433 4
		off += ntohs(frep->fr_ip->ip_len) - frep->fr_ip->ip_hl * 4;
		if (off < (*frag)->fr_max &&
		    (next == NULL || FR_IP_OFF(next) != off))
		{
d435 2
a436 2
			    off, next == NULL ? -1 : FR_IP_OFF(next),
			    (*frag)->fr_max));
d440 2
a441 2
	DPFPRINTF(("%d < %d?\n", off, (*frag)->fr_max));
	if (off < (*frag)->fr_max)
d445 1
a445 1
	frent = LIST_FIRST(&(*frag)->fr_queue);
d449 1
a449 2
		pf_free_fragment(*frag);
		*frag = NULL;
d471 2
a472 2
	ip->ip_src = (*frag)->fr_src;
	ip->ip_dst = (*frag)->fr_dst;
d475 1
a475 2
	pf_remove_fragment(*frag);
	*frag = NULL;
d478 1
a478 1
	ip->ip_len = htons(off + hlen);
d491 1
a491 1
	DPFPRINTF(("complete: %p(%d)\n", m, ntohs(ip->ip_len)));
d503 1
a503 1
pf_fragcache(struct mbuf **m0, struct ip *h, struct pf_fragment **frag, int mff,
d508 2
a509 2
	int			 ip_len = ntohs(h->ip_len) - (h->ip_hl << 2);
	u_int16_t		 off = ntohs(h->ip_off) << 3;
d513 1
a513 1
	KASSERT(*frag == NULL || !BUFFER_FRAGMENTS(*frag));
d516 3
a518 3
	if (*frag == NULL) {
		*frag = pool_get(&pf_cache_pl, PR_NOWAIT);
		if (*frag == NULL) {
d520 2
a521 2
			*frag = pool_get(&pf_cache_pl, PR_NOWAIT);
			if (*frag == NULL)
d528 1
a528 2
			pool_put(&pf_cache_pl, *frag);
			*frag = NULL;
d533 7
a539 7
		(*frag)->fr_flags = PFFRAG_NOBUFFER;
		(*frag)->fr_max = 0;
		(*frag)->fr_src = h->ip_src;
		(*frag)->fr_dst = h->ip_dst;
		(*frag)->fr_p = h->ip_p;
		(*frag)->fr_id = h->ip_id;
		(*frag)->fr_timeout = time.tv_sec;
d543 2
a544 2
		LIST_INIT(&(*frag)->fr_cache);
		LIST_INSERT_HEAD(&(*frag)->fr_cache, cur, fr_next);
d546 2
a547 2
		RB_INSERT(pf_frag_tree, &pf_cache_tree, *frag);
		TAILQ_INSERT_HEAD(&pf_cachequeue, *frag, frag_next);
d559 1
a559 1
	LIST_FOREACH(fra, &(*frag)->fr_cache, fr_next) {
d596 1
a596 1
			/* Update the previous frag to encompass this one */
d625 1
d627 2
a628 3
				KASSERT((int)m->m_len == ntohs(h->ip_len) - precut);
				h->ip_off = htons(ntohs(h->ip_off) + (precut >> 3));
				h->ip_len = htons(ntohs(h->ip_len) - precut);
d681 2
a682 2
				KASSERT((int)m->m_len == ntohs(h->ip_len) - aftercut);
				h->ip_len = htons(ntohs(h->ip_len) - aftercut);
d703 1
a703 1
		/* Need to glue together two separate fragment descriptors */
d746 2
a747 2
	if ((*frag)->fr_max < max)
		(*frag)->fr_max = max;
d751 1
a751 1
		(*frag)->fr_flags |= PFFRAG_SEENLAST;
d754 3
a756 3
	if (((*frag)->fr_flags & PFFRAG_SEENLAST) &&
	    LIST_FIRST(&(*frag)->fr_cache)->fr_off == 0 &&
	    LIST_FIRST(&(*frag)->fr_cache)->fr_end == (*frag)->fr_max) {
d759 2
a760 3
		    (*frag)->fr_max));
		pf_free_fragment(*frag);
		*frag = NULL;
d769 2
a770 2
	if (!mff && *frag != NULL)
		(*frag)->fr_flags |= PFFRAG_SEENLAST;
d778 2
a779 2
	if (!mff && *frag != NULL)
		(*frag)->fr_flags |= PFFRAG_SEENLAST;
d783 1
a783 1
		if (((*frag)->fr_flags & PFFRAG_DROP) == 0)
d786 1
a786 1
		(*frag)->fr_flags |= PFFRAG_DROP;
d794 1
a794 1
pf_normalize_ip(struct mbuf **m0, int dir, struct pfi_kif *kif, u_short *reason)
d801 1
a801 1
	int			 mff = (ntohs(h->ip_off) & IP_MF);
d803 1
a803 1
	u_int16_t		 fragoff = (ntohs(h->ip_off) & IP_OFFMASK) << 3;
d811 1
a811 2
		if (r->kif != NULL &&
		    (r->kif != kif && r->kif != kif->pfik_parent) == !r->ifnot)
d838 1
a838 1
	if (hlen > ntohs(h->ip_len))
d843 1
a843 1
		h->ip_off &= htons(~IP_DF);
d853 1
a853 1
	if (h->ip_off & htons(IP_DF)) {
d858 2
a859 2
	ip_len = ntohs(h->ip_len) - hlen;
	ip_off = (ntohs(h->ip_off) & IP_OFFMASK) << 3;
d867 1
d869 2
a870 2
	if (fragoff + ip_len > IP_MAXPACKET) {
		DPFPRINTF(("max packet %d\n", fragoff + ip_len));
a872 1
	max = fragoff + ip_len;
d877 2
d898 1
a898 1
		*m0 = m = pf_reassemble(m0, &frag, frent, mff);
d903 1
a903 1
		if (frag != NULL && (frag->fr_flags & PFFRAG_DROP))
d932 1
a932 1
		*m0 = m = pf_fragcache(m0, h, &frag, mff,
d948 1
a948 1
		if (frag != NULL && (frag->fr_flags & PFFRAG_DROP))
d955 1
a955 1
	h->ip_off &= htons(IP_DF);
d976 1
a976 1
		PFLOG_PACKET(kif, h, m, AF_INET, dir, *reason, r, NULL, NULL);
d982 1
a982 1
		PFLOG_PACKET(kif, h, m, AF_INET, dir, *reason, r, NULL, NULL);
d988 1
a988 1
	/* Free associated fragments */
d994 1
a994 171
		PFLOG_PACKET(kif, h, m, AF_INET, dir, *reason, r, NULL, NULL);

	return (PF_DROP);
}

#ifdef INET6
int
pf_normalize_ip6(struct mbuf **m0, int dir, struct pfi_kif *kif,
    u_short *reason)
{
	struct mbuf		*m = *m0;
	struct pf_rule		*r;
	struct ip6_hdr		*h = mtod(m, struct ip6_hdr *);
	int			 off;
	struct ip6_ext		 ext;
	struct ip6_opt		 opt;
	struct ip6_opt_jumbo	 jumbo;
	struct ip6_frag		 frag;
	u_int32_t		 jumbolen = 0, plen;
	u_int16_t		 fragoff = 0;
	int			 optend;
	int			 ooff;
	u_int8_t		 proto;
	int			 terminal;

	r = TAILQ_FIRST(pf_main_ruleset.rules[PF_RULESET_SCRUB].active.ptr);
	while (r != NULL) {
		r->evaluations++;
		if (r->kif != NULL &&
		    (r->kif != kif && r->kif != kif->pfik_parent) == !r->ifnot)
			r = r->skip[PF_SKIP_IFP].ptr;
		else if (r->direction && r->direction != dir)
			r = r->skip[PF_SKIP_DIR].ptr;
		else if (r->af && r->af != AF_INET6)
			r = r->skip[PF_SKIP_AF].ptr;
#if 0 /* header chain! */
		else if (r->proto && r->proto != h->ip6_nxt)
			r = r->skip[PF_SKIP_PROTO].ptr;
#endif
		else if (PF_MISMATCHAW(&r->src.addr,
		    (struct pf_addr *)&h->ip6_src, AF_INET6, r->src.not))
			r = r->skip[PF_SKIP_SRC_ADDR].ptr;
		else if (PF_MISMATCHAW(&r->dst.addr,
		    (struct pf_addr *)&h->ip6_dst, AF_INET6, r->dst.not))
			r = r->skip[PF_SKIP_DST_ADDR].ptr;
		else
			break;
	}

	if (r == NULL)
		return (PF_PASS);
	else
		r->packets++;

	/* Check for illegal packets */
	if (sizeof(struct ip6_hdr) + IPV6_MAXPACKET < m->m_pkthdr.len)
		goto drop;

	off = sizeof(struct ip6_hdr);
	proto = h->ip6_nxt;
	terminal = 0;
	do {
		switch (proto) {
		case IPPROTO_FRAGMENT:
			goto fragment;
			break;
		case IPPROTO_AH:
		case IPPROTO_ROUTING:
		case IPPROTO_DSTOPTS:
			if (!pf_pull_hdr(m, off, &ext, sizeof(ext), NULL,
			    NULL, AF_INET6))
				goto shortpkt;
			if (proto == IPPROTO_AH)
				off += (ext.ip6e_len + 2) * 4;
			else
				off += (ext.ip6e_len + 1) * 8;
			proto = ext.ip6e_nxt;
			break;
		case IPPROTO_HOPOPTS:
			if (!pf_pull_hdr(m, off, &ext, sizeof(ext), NULL,
			    NULL, AF_INET6))
				goto shortpkt;
			optend = off + (ext.ip6e_len + 1) * 8;
			ooff = off + sizeof(ext);
			do {
				if (!pf_pull_hdr(m, ooff, &opt.ip6o_type,
				    sizeof(opt.ip6o_type), NULL, NULL,
				    AF_INET6))
					goto shortpkt;
				if (opt.ip6o_type == IP6OPT_PAD1) {
					ooff++;
					continue;
				}
				if (!pf_pull_hdr(m, ooff, &opt, sizeof(opt),
				    NULL, NULL, AF_INET6))
					goto shortpkt;
				if (ooff + sizeof(opt) + opt.ip6o_len > optend)
					goto drop;
				switch (opt.ip6o_type) {
				case IP6OPT_JUMBO:
					if (h->ip6_plen != 0)
						goto drop;
					if (!pf_pull_hdr(m, ooff, &jumbo,
					    sizeof(jumbo), NULL, NULL,
					    AF_INET6))
						goto shortpkt;
					memcpy(&jumbolen, jumbo.ip6oj_jumbo_len,
					    sizeof(jumbolen));
					jumbolen = ntohl(jumbolen);
					if (jumbolen <= IPV6_MAXPACKET)
						goto drop;
					if (sizeof(struct ip6_hdr) + jumbolen !=
					    m->m_pkthdr.len)
						goto drop;
					break;
				default:
					break;
				}
				ooff += sizeof(opt) + opt.ip6o_len;
			} while (ooff < optend);

			off = optend;
			proto = ext.ip6e_nxt;
			break;
		default:
			terminal = 1;
			break;
		}
	} while (!terminal);

	/* jumbo payload option must be present, or plen > 0 */
	if (ntohs(h->ip6_plen) == 0)
		plen = jumbolen;
	else
		plen = ntohs(h->ip6_plen);
	if (plen == 0)
		goto drop;
	if (sizeof(struct ip6_hdr) + plen > m->m_pkthdr.len)
		goto shortpkt;

	/* Enforce a minimum ttl, may cause endless packet loops */
	if (r->min_ttl && h->ip6_hlim < r->min_ttl)
		h->ip6_hlim = r->min_ttl;

	return (PF_PASS);

 fragment:
	if (ntohs(h->ip6_plen) == 0 || jumbolen)
		goto drop;
	plen = ntohs(h->ip6_plen);

	if (!pf_pull_hdr(m, off, &frag, sizeof(frag), NULL, NULL, AF_INET6))
		goto shortpkt;
	fragoff = ntohs(frag.ip6f_offlg & IP6F_OFF_MASK);
	if (fragoff + (plen - off - sizeof(frag)) > IPV6_MAXPACKET)
		goto badfrag;

	/* do something about it */
	return (PF_PASS);

 shortpkt:
	REASON_SET(reason, PFRES_SHORT);
	if (r != NULL && r->log)
		PFLOG_PACKET(kif, h, m, AF_INET6, dir, *reason, r, NULL, NULL);
	return (PF_DROP);

 drop:
	REASON_SET(reason, PFRES_NORM);
	if (r != NULL && r->log)
		PFLOG_PACKET(kif, h, m, AF_INET6, dir, *reason, r, NULL, NULL);
	return (PF_DROP);
a995 4
 badfrag:
	REASON_SET(reason, PFRES_FRAG);
	if (r != NULL && r->log)
		PFLOG_PACKET(kif, h, m, AF_INET6, dir, *reason, r, NULL, NULL);
a997 1
#endif
d1000 1
a1000 1
pf_normalize_tcp(int dir, struct pfi_kif *kif, struct mbuf *m, int ipoff,
d1013 1
a1013 2
		if (r->kif != NULL &&
		    (r->kif != kif && r->kif != kif->pfik_parent) == !r->ifnot)
a1030 4
		else if (r->os_fingerprint != PF_OSFP_ANY && !pf_osfp_match(
			    pf_osfp_fingerprint(pd, m, off, th),
			    r->os_fingerprint))
			r = TAILQ_NEXT(r, entries);
d1095 1
a1095 1
		m_copyback(m, off, sizeof(*th), th);
d1102 1
a1102 1
		PFLOG_PACKET(kif, h, m, AF_INET, dir, reason, r, NULL, NULL);
d1154 1
a1154 1
			case TCPOPT_EOL:	/* FALLTHROUGH */
d1218 1
a1218 1
		if (src->scrub) {
d1239 1
a1239 1
			case TCPOPT_EOL:	/* FALLTHROUGH */
d1262 1
a1262 5

					/* Modulate TS reply iff valid (!0) */
					memcpy(&ts_value, &opt[6],
					    sizeof(u_int32_t));
					if (ts_value && dst->scrub &&
d1265 2
@


1.14.4.9
log
@Merge with the trunk
@
text
@d117 1
d121 2
a122 6
#define	DPFPRINTF(x) do {				\
	if (pf_status.debug >= PF_DEBUG_MISC) {		\
		printf("%s: ", __func__);		\
		printf x ;				\
	}						\
} while(0)
d377 1
a377 1
	    4 > off)
d641 2
a642 4
				KASSERT((int)m->m_len ==
				    ntohs(h->ip_len) - precut);
				h->ip_off = htons(ntohs(h->ip_off) +
				    (precut >> 3));
d696 1
a696 2
				KASSERT((int)m->m_len ==
				    ntohs(h->ip_len) - aftercut);
d810 1
a810 2
pf_normalize_ip(struct mbuf **m0, int dir, struct pfi_kif *kif, u_short *reason,
    struct pf_pdesc *pd)
d976 1
a976 3
	if (r->rule_flag & PFRULE_RANDOMID) {
		u_int16_t ip_id = h->ip_id;

a977 4
		h->ip_sum = pf_cksum_fixup(h->ip_sum, ip_id, h->ip_id, 0);
	}
	if ((r->rule_flag & (PFRULE_FRAGCROP|PFRULE_FRAGDROP)) == 0)
		pd->flags |= PFDESC_IP_REAS;
d985 1
a985 2
	if ((r->rule_flag & (PFRULE_FRAGCROP|PFRULE_FRAGDROP)) == 0)
		pd->flags |= PFDESC_IP_REAS;
d1017 1
a1017 1
    u_short *reason, struct pf_pdesc *pd)
a1166 1
	/* remember to set pd->flags |= PFDESC_IP_REAS */
d1187 1
a1187 1
#endif /* INET6 */
d1273 1
a1273 1
		th->th_sum = pf_cksum_fixup(th->th_sum, ov, nv, 0);
d1279 1
a1279 1
		th->th_sum = pf_cksum_fixup(th->th_sum, th->th_urp, 0, 0);
a1304 1
	u_int32_t tsval, tsecr;
d1338 1
a1338 1
		return (0);
d1358 1
a1358 12
					src->scrub->pfss_ts_mod =
					    htonl(arc4random());

					/* note PFSS_PAWS not set yet */
					memcpy(&tsval, &opt[2],
					    sizeof(u_int32_t));
					memcpy(&tsecr, &opt[6],
					    sizeof(u_int32_t));
		    			src->scrub->pfss_tsval0 = ntohl(tsval);
					src->scrub->pfss_tsval = ntohl(tsval);
					src->scrub->pfss_tsecr = ntohl(tsecr);
					src->scrub->pfss_last = mono_time;
d1362 2
a1363 2
				hlen -= MAX(opt[1], 2);
				opt += MAX(opt[1], 2);
d1385 2
a1386 2
    u_short *reason, struct tcphdr *th, struct pf_state *state,
    struct pf_state_peer *src, struct pf_state_peer *dst, int *writeback)
a1387 2
	u_int32_t tsval, tsecr;
	u_int tsval_from_last;
a1390 1
	int got_ts = 0;
a1443 11

				if (got_ts) {
					/* Huh?  Multiple timestamps!? */
					if (pf_status.debug >= PF_DEBUG_MISC) {
						DPFPRINTF(("multiple TS??"));
						pf_print_state(state);
						printf("\n");
					}
					REASON_SET(reason, PFRES_TS);
					return (PF_DROP);
				}
d1445 2
a1446 3
					memcpy(&tsval, &opt[2],
					    sizeof(u_int32_t));
					if (tsval && src->scrub &&
d1449 4
a1452 1
						tsval = ntohl(tsval);
d1454 1
a1454 4
						    &th->th_sum,
						    htonl(tsval +
						    src->scrub->pfss_ts_mod),
						    0);
d1459 1
a1459 1
					memcpy(&tsecr, &opt[6],
d1461 1
a1461 1
					if (tsecr && dst->scrub &&
d1464 2
a1465 2
						tsecr = ntohl(tsecr)
						    - dst->scrub->pfss_ts_mod;
d1467 1
a1467 2
						    &th->th_sum, htonl(tsecr),
						    0);
a1469 1
					got_ts = 1;
d1473 2
a1474 2
				hlen -= MAX(opt[1], 2);
				opt += MAX(opt[1], 2);
a1487 258
	/*
	 * Must invalidate PAWS checks on connections idle for too long.
	 * The fastest allowed timestamp clock is 1ms.  That turns out to
	 * be about 24 days before it wraps.  XXX Right now our lowerbound
	 * TS echo check only works for the first 12 days of a connection
	 * when the TS has exhausted half its 32bit space
	 */
#define TS_MAX_IDLE	(24*24*60*60)
#define TS_MAX_CONN	(12*24*60*60)	/* XXX remove when better tsecr check */
	if (src->scrub && (src->scrub->pfss_flags & PFSS_PAWS) &&
	    (mono_time.tv_sec - src->scrub->pfss_last.tv_sec > TS_MAX_IDLE ||
	    time.tv_sec - state->creation > TS_MAX_CONN))  {
		if (pf_status.debug >= PF_DEBUG_MISC) {
			DPFPRINTF(("src idled out of PAWS\n"));
			pf_print_state(state);
			printf("\n");
		}
		src->scrub->pfss_flags = (src->scrub->pfss_flags & ~PFSS_PAWS)
		    | PFSS_PAWS_IDLED;
	}
	if (dst->scrub && (dst->scrub->pfss_flags & PFSS_PAWS) &&
	    mono_time.tv_sec - dst->scrub->pfss_last.tv_sec > TS_MAX_IDLE) {
		if (pf_status.debug >= PF_DEBUG_MISC) {
			DPFPRINTF(("dst idled out of PAWS\n"));
			pf_print_state(state);
			printf("\n");
		}
		dst->scrub->pfss_flags = (dst->scrub->pfss_flags & ~PFSS_PAWS)
		    | PFSS_PAWS_IDLED;
	}

	if (got_ts && src->scrub && dst->scrub &&
	    (src->scrub->pfss_flags & PFSS_PAWS) &&
	    (dst->scrub->pfss_flags & PFSS_PAWS)) {
		/* Validate that the timestamps are "in-window".
		 * RFC1323 describes TCP Timestamp options that allow
		 * measurement of RTT (round trip time) and PAWS
		 * (protection against wrapped sequence numbers).  PAWS
		 * gives us a set of rules for rejecting packets on
		 * long fat pipes (packets that were somehow delayed 
		 * in transit longer than the time it took to send the
		 * full TCP sequence space of 4Gb).  We can use these
		 * rules and infer a few others that will let us treat
		 * the 32bit timestamp and the 32bit echoed timestamp
		 * as sequence numbers to prevent a blind attacker from
		 * inserting packets into a connection.
		 *
		 * RFC1323 tells us:
		 *  - The timestamp on this packet must be greater than
		 *    or equal to the last value echoed by the other
		 *    endpoint.  The RFC says those will be discarded
		 *    since it is a dup that has already been acked.
		 *    This gives us a lowerbound on the timestamp.
		 *        timestamp >= other last echoed timestamp
		 *  - The timestamp will be less than or equal to
		 *    the last timestamp plus the time between the
		 *    last packet and now.  The RFC defines the max
		 *    clock rate as 1ms.  We will allow clocks to be
		 *    up to 10% fast and will allow a total difference
		 *    or 30 seconds due to a route change.  And this
		 *    gives us an upperbound on the timestamp.
		 *        timestamp <= last timestamp + max ticks
		 *    We have to be careful here.  Windows will send an
		 *    initial timestamp of zero and then initialize it
		 *    to a random value after the 3whs; presumably to
		 *    avoid a DoS by having to call an expensive RNG
		 *    during a SYN flood.  Proof MS has at least one
		 *    good security geek.
		 *
		 *  - The TCP timestamp option must also echo the other
		 *    endpoints timestamp.  The timestamp echoed is the
		 *    one carried on the earliest unacknowledged segment
		 *    on the left edge of the sequence window.  The RFC
		 *    states that the host will reject any echoed
		 *    timestamps that were larger than any ever sent.
		 *    This gives us an upperbound on the TS echo.
		 *        tescr <= largest_tsval
		 *  - The lowerbound on the TS echo is a little more
		 *    tricky to determine.  The other endpoint's echoed
		 *    values will not decrease.  But there may be
		 *    network conditions that re-order packets and
		 *    cause our view of them to decrease.  For now the
		 *    only lowerbound we can safely determine is that
		 *    the TS echo will never be less than the orginal
		 *    TS.  XXX There is probably a better lowerbound.
		 *    Remove TS_MAX_CONN with better lowerbound check.
		 *        tescr >= other original TS
		 *
		 * It is also important to note that the fastest
		 * timestamp clock of 1ms will wrap its 32bit space in
		 * 24 days.  So we just disable TS checking after 24
		 * days of idle time.  We actually must use a 12d
		 * connection limit until we can come up with a better
		 * lowerbound to the TS echo check.
		 */
		struct timeval delta_ts;
		int ts_fudge;


		/*
		 * PFTM_TS_DIFF is how many seconds of leeway to allow
		 * a host's timestamp.  This can happen if the previous
		 * packet got delayed in transit for much longer than
		 * this packet.
		 */
		if ((ts_fudge = state->rule.ptr->timeout[PFTM_TS_DIFF]) == 0)
			ts_fudge = pf_default_rule.timeout[PFTM_TS_DIFF];


		/* Calculate max ticks since the last timestamp */
#define TS_MAXFREQ	1100		/* RFC max TS freq of 1Khz + 10% skew */
#define TS_MICROSECS	1000000		/* microseconds per second */
		timersub(&mono_time, &src->scrub->pfss_last, &delta_ts);
		tsval_from_last = (delta_ts.tv_sec + ts_fudge) * TS_MAXFREQ;
		tsval_from_last += delta_ts.tv_usec / (TS_MICROSECS/TS_MAXFREQ);


		if ((src->state >= TCPS_ESTABLISHED &&
		    dst->state >= TCPS_ESTABLISHED) &&
		    (SEQ_LT(tsval, dst->scrub->pfss_tsecr) ||
		    SEQ_GT(tsval, src->scrub->pfss_tsval + tsval_from_last) ||
		    (tsecr && (SEQ_GT(tsecr, dst->scrub->pfss_tsval) ||
		    SEQ_LT(tsecr, dst->scrub->pfss_tsval0))))) {
			/* Bad RFC1323 implementation or an insertion attack.
			 *
			 * - Solaris 2.6 and 2.7 are known to send another ACK
			 *   after the FIN,FIN|ACK,ACK closing that carries
			 *   an old timestamp.
			 */

			DPFPRINTF(("Timestamp failed %c%c%c%c\n",
			    SEQ_LT(tsval, dst->scrub->pfss_tsecr) ? '0' : ' ',
			    SEQ_GT(tsval, src->scrub->pfss_tsval +
			        tsval_from_last) ? '1' : ' ',
			    SEQ_GT(tsecr, dst->scrub->pfss_tsval) ? '2' : ' ',
			    SEQ_LT(tsecr, dst->scrub->pfss_tsval0)? '3' : ' '));
			DPFPRINTF((" tsval: %lu  tsecr: %lu  +ticks: %lu  idle: %lus %lums\n",
			    tsval, tsecr, tsval_from_last, delta_ts.tv_sec,
			    delta_ts.tv_usec / 1000));
			DPFPRINTF((" src->tsval: %lu  tsecr: %lu\n",
			    src->scrub->pfss_tsval, src->scrub->pfss_tsecr));
			DPFPRINTF((" dst->tsval: %lu  tsecr: %lu  tsval0: %lu\n",
			    dst->scrub->pfss_tsval, dst->scrub->pfss_tsecr,
			    dst->scrub->pfss_tsval0));
			if (pf_status.debug >= PF_DEBUG_MISC) {
				pf_print_state(state);
				pf_print_flags(th->th_flags);
				printf("\n");
			}
			REASON_SET(reason, PFRES_TS);
			return (PF_DROP);
		}

		/* XXX I'd really like to require tsecr but it's optional */

	} else if (!got_ts && (th->th_flags & TH_RST) == 0 &&
	    ((src->state == TCPS_ESTABLISHED && dst->state == TCPS_ESTABLISHED)
	    || pd->p_len > 0 || (th->th_flags & TH_SYN)) &&
	    src->scrub && dst->scrub &&
	    (src->scrub->pfss_flags & PFSS_PAWS) &&
	    (dst->scrub->pfss_flags & PFSS_PAWS)) {
		/* Didn't send a timestamp.  Timestamps aren't really useful
		 * when:
		 *  - connection opening or closing (often not even sent).
		 *    but we must not let an attacker to put a FIN on a
		 *    data packet to sneak it through our ESTABLISHED check.
		 *  - on a TCP reset.  RFC suggests not even looking at TS.
		 *  - on an empty ACK.  The TS will not be echoed so it will
		 *    probably not help keep the RTT calculation in sync and
		 *    there isn't as much danger when the sequence numbers
		 *    got wrapped.  So some stacks don't include TS on empty
		 *    ACKs :-(
		 *
		 * To minimize the disruption to mostly RFC1323 conformant
		 * stacks, we will only require timestamps on data packets.
		 *
		 * And what do ya know, we cannot require timestamps on data
		 * packets.  There appear to be devices that do legitimate
		 * TCP connection hijacking.  There are HTTP devices that allow
		 * a 3whs (with timestamps) and then buffer the HTTP request.
		 * If the intermediate device has the HTTP response cache, it
		 * will spoof the response but not bother timestamping its
		 * packets.  So we can look for the presence of a timestamp in
		 * the first data packet and if there, require it in all future
		 * packets.
		 */

		if (pd->p_len > 0 && (src->scrub->pfss_flags & PFSS_DATA_TS)) {
			/*
			 * Hey!  Someone tried to sneak a packet in.  Or the
			 * stack changed its RFC1323 behavior?!?!
			 */
			if (pf_status.debug >= PF_DEBUG_MISC) {
				DPFPRINTF(("Did not receive expected RFC1323 timestamp\n"));
				pf_print_state(state);
				pf_print_flags(th->th_flags);
				printf("\n");
			}
			REASON_SET(reason, PFRES_TS);
			return (PF_DROP);
		}
	}


	/*
	 * We will note if a host sends his data packets with or without
	 * timestamps.  And require all data packets to contain a timestamp
	 * if the first does.  PAWS implicitly requires that all data packets be
	 * timestamped.  But I think there are middle-man devices that hijack
	 * TCP streams immedietly after the 3whs and don't timestamp their
	 * packets (seen in a WWW accelerator or cache).
	 */
	if (pd->p_len > 0 && src->scrub && (src->scrub->pfss_flags &
	    (PFSS_TIMESTAMP|PFSS_DATA_TS|PFSS_DATA_NOTS)) == PFSS_TIMESTAMP) {
		if (got_ts)
			src->scrub->pfss_flags |= PFSS_DATA_TS;
		else {
			src->scrub->pfss_flags |= PFSS_DATA_NOTS;
			if (pf_status.debug >= PF_DEBUG_MISC && dst->scrub &&
			    (dst->scrub->pfss_flags & PFSS_TIMESTAMP)) {
				/* Don't warn if other host rejected RFC1323 */
				DPFPRINTF(("Broken RFC1323 stack did not timestamp data packet.  Disabled PAWS security.\n"));
				pf_print_state(state);
				pf_print_flags(th->th_flags);
				printf("\n");
			}
		}
	}


	/*
	 * Update PAWS values
	 */
	if (got_ts && src->scrub && PFSS_TIMESTAMP == (src->scrub->pfss_flags &
	    (PFSS_PAWS_IDLED|PFSS_TIMESTAMP))) {
		src->scrub->pfss_last = mono_time;
		if (SEQ_GEQ(tsval, src->scrub->pfss_tsval) ||
		    (src->scrub->pfss_flags & PFSS_PAWS) == 0)
			src->scrub->pfss_tsval = tsval;

		if (tsecr) {
			if (SEQ_GEQ(tsecr, src->scrub->pfss_tsecr) ||
			    (src->scrub->pfss_flags & PFSS_PAWS) == 0)
				src->scrub->pfss_tsecr = tsecr;

			if ((src->scrub->pfss_flags & PFSS_PAWS) == 0 &&
			    (SEQ_LT(tsval, src->scrub->pfss_tsval0) ||
			    src->scrub->pfss_tsval0 == 0)) {
				/* tsval0 MUST be the lowest timestamp */
	    			src->scrub->pfss_tsval0 = tsval;
			}

			/* Only fully initialized after a TS gets echoed */
			if ((src->scrub->pfss_flags & PFSS_PAWS) == 0)
				src->scrub->pfss_flags |= PFSS_PAWS;
		}
	}

a1490 1

d1523 1
a1523 1
				    *mss, htons(r->max_mss), 0);
@


1.14.4.10
log
@sync to HEAD
@
text
@d844 1
a844 1
		    (struct pf_addr *)&h->ip_src.s_addr, AF_INET, r->src.neg))
d847 1
a847 1
		    (struct pf_addr *)&h->ip_dst.s_addr, AF_INET, r->dst.neg))
d1063 1
a1063 1
		    (struct pf_addr *)&h->ip6_src, AF_INET6, r->src.neg))
d1066 1
a1066 1
		    (struct pf_addr *)&h->ip6_dst, AF_INET6, r->dst.neg))
d1227 1
a1227 1
		else if (PF_MISMATCHAW(&r->src.addr, pd->src, af, r->src.neg))
d1232 1
a1232 1
		else if (PF_MISMATCHAW(&r->dst.addr, pd->dst, af, r->dst.neg))
@


1.13
log
@fixes pr/2105
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.12 2001/09/15 16:47:07 dhartmei Exp $ */
d234 1
d236 1
@


1.12
log
@Don't use m_pkthdr.rcvif in pflog_packet(), it doesn't work for outgoing
packets and is obviously invalid (and not NULL) for IPv6 packets (hence
crashed). Pass ifp down instead.

sizeof(ih) instead of sizeof(&ih) for pf_pull_hdr() from pf_test6().
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.11 2001/09/15 03:54:40 frantzen Exp $ */
d112 1
a112 1
#define		 PFLOG_PACKET(x,a,b,c,d,e)	((void)0)
@


1.11
log
@IPv6 support from Ryan McBride (mcbride@@countersiege.com)
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.10 2001/09/08 02:10:33 provos Exp $ */
d99 1
a99 1
#define PFLOG_PACKET(x,a,b,c,d,e) \
d104 1
a104 1
                        pflog_packet(a,b,c,d,e); \
d108 1
a108 1
                        pflog_packet(a,b,c,d,e); \
d538 1
a538 1
		PFLOG_PACKET(h, m, AF_INET, dir, *reason, r);
d550 1
a550 1
		PFLOG_PACKET(h, m, AF_INET, dir, *reason, r);
d657 1
a657 1
		PFLOG_PACKET(h, m, AF_INET, dir, reason, rm);
@


1.10
log
@initialize variable and more careful bounts checking; okay frantzen@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.9 2001/09/06 20:53:44 dhartmei Exp $ */
d91 1
a91 1
			    int, int, struct ip *, struct tcphdr *);
d99 12
a110 8
#define		 PFLOG_PACKET(x,a,b,c,d,e) \
		do { \
			HTONS((x)->ip_len); \
			HTONS((x)->ip_off); \
			pflog_packet(a,b,c,d,e); \
			NTOHS((x)->ip_len); \
			NTOHS((x)->ip_off); \
		} while (0)
d201 3
a203 2
	key->addr[0] = ip->ip_src;
	key->addr[1] = ip->ip_dst;
d235 2
a236 2
	key.addr[0] = frag->fr_src;
	key.addr[1] = frag->fr_dst;
d448 1
a448 1
		    MATCH_TUPLE(h, r, dir, ifp))
d557 1
a557 1
    int off, struct ip *h, struct tcphdr *th)
d560 1
d562 1
a562 1
	u_int8_t flags;
d572 1
a572 1
		else if (r->proto && r->proto != h->ip_p)
d574 1
a574 2
		else if (r->src.mask && !pf_match_addr(r->src.not,
			    r->src.addr, r->src.mask, h->ip_src.s_addr))
d576 4
a581 3
			r = r->skip[3];
		else if (r->dst.mask && !pf_match_addr(r->dst.not,
			    r->dst.addr, r->dst.mask, h->ip_dst.s_addr))
d583 5
d590 1
a590 1
			r = r->skip[5];
@


1.9
log
@Reflect skip step changes. Spotted by Ryan McBride.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.8 2001/09/04 08:55:37 dhartmei Exp $ */
d246 2
a247 1
	struct pf_frent *frep, *frea, *next;
d304 1
a304 1
		if (precut > ip->ip_len)
@


1.8
log
@#define empty PFLOG_PACKET correctly (no side effects). Closes PR2044.
From Claus Assmann.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.7 2001/08/31 23:05:22 frantzen Exp $ */
d563 1
a563 1
		if (r->proto && r->proto != h->ip_p)
d565 2
d569 1
a569 1
			r = r->skip[1];
d572 1
a572 1
			r = r->skip[2];
d575 1
a575 1
			r = r->skip[3];
d578 1
a578 1
			r = r->skip[4];
@


1.7
log
@Forgot to commit frag expire tuning before
Check for a short ip_hl.  Could have caused proto headers to overlap IP header.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.6 2001/08/11 12:05:00 dhartmei Exp $ */
d108 1
a108 1
#define		 PFLOG_PACKET
@


1.6
log
@Add support for ICMP errors referring to ICMP queries/replies. Fixes
'ICMP error message for bad proto' messages. Reported by Mark Grimes
and Steve Rumble.

Add debugging level with ioctl interface and pfctl switch. Default
is 'None'.
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.5 2001/08/02 06:59:25 deraadt Exp $ */
d115 1
a130 2
#define FRAG_EXPIRE	30

d140 1
a140 1
	expire.tv_sec = FRAG_EXPIRE;
@


1.5
log
@KNF
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.4 2001/08/01 23:07:36 provos Exp $ */
d96 1
a96 1
#define DPFPRINTF(x)		if (pf_debug) printf x
@


1.4
log
@stateless tcp normalization along the lines of the normalization paper by
handley, paxon and kreibich; okay deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.3 2001/07/17 22:22:14 provos Exp $ */
d362 4
a365 2
			DPFPRINTF((__FUNCTION__": missing fragment at %d, next %d, max %d\n",
				  off, next == NULL ? -1 : next->fr_ip->ip_off, frag->fr_max));
d527 1
a527 1
	
d567 1
a567 1
			     r->src.addr, r->src.mask, h->ip_src.s_addr))
d570 1
a570 1
			     r->src.port[0], r->src.port[1], th->th_sport))
d573 1
a573 1
			     r->dst.addr, r->dst.mask, h->ip_dst.s_addr))
d576 1
a576 1
			     r->dst.port[0], r->dst.port[1], th->th_dport))
@


1.3
log
@support min-ttl, okay dhartmei@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.2 2001/07/17 21:54:26 provos Exp $ */
d89 3
d548 97
@


1.2
log
@normalize ip_off, make IP_DF stripping optional, return rst is a flag now.
okay markus@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pf_norm.c,v 1.1 2001/07/17 20:35:26 provos Exp $ */
d519 3
@


1.1
log
@ip normalization code
@
text
@d1 1
a1 1
/*	$OpenBSD: pf.c,v 1.113 2001/07/15 23:05:04 dhartmei Exp $ */
d513 7
@

