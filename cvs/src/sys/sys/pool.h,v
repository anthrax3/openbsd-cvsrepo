head	1.69;
access;
symbols
	OPENBSD_6_1_BASE:1.69
	OPENBSD_6_0:1.59.0.2
	OPENBSD_6_0_BASE:1.59
	OPENBSD_5_9:1.58.0.2
	OPENBSD_5_9_BASE:1.58
	OPENBSD_5_8:1.57.0.6
	OPENBSD_5_8_BASE:1.57
	OPENBSD_5_7:1.57.0.2
	OPENBSD_5_7_BASE:1.57
	OPENBSD_5_6:1.47.0.4
	OPENBSD_5_6_BASE:1.47
	OPENBSD_5_5:1.44.0.4
	OPENBSD_5_5_BASE:1.44
	OPENBSD_5_4:1.43.0.2
	OPENBSD_5_4_BASE:1.43
	OPENBSD_5_3:1.42.0.2
	OPENBSD_5_3_BASE:1.42
	OPENBSD_5_2:1.41.0.6
	OPENBSD_5_2_BASE:1.41
	OPENBSD_5_1_BASE:1.41
	OPENBSD_5_1:1.41.0.4
	OPENBSD_5_0:1.41.0.2
	OPENBSD_5_0_BASE:1.41
	OPENBSD_4_9:1.36.0.2
	OPENBSD_4_9_BASE:1.36
	OPENBSD_4_8:1.35.0.2
	OPENBSD_4_8_BASE:1.35
	OPENBSD_4_7:1.33.0.2
	OPENBSD_4_7_BASE:1.33
	OPENBSD_4_6:1.31.0.6
	OPENBSD_4_6_BASE:1.31
	OPENBSD_4_5:1.31.0.2
	OPENBSD_4_5_BASE:1.31
	OPENBSD_4_4:1.29.0.2
	OPENBSD_4_4_BASE:1.29
	OPENBSD_4_3:1.25.0.2
	OPENBSD_4_3_BASE:1.25
	OPENBSD_4_2:1.24.0.2
	OPENBSD_4_2_BASE:1.24
	OPENBSD_4_1:1.20.0.2
	OPENBSD_4_1_BASE:1.20
	OPENBSD_4_0:1.19.0.2
	OPENBSD_4_0_BASE:1.19
	OPENBSD_3_9:1.18.0.8
	OPENBSD_3_9_BASE:1.18
	OPENBSD_3_8:1.18.0.6
	OPENBSD_3_8_BASE:1.18
	OPENBSD_3_7:1.18.0.4
	OPENBSD_3_7_BASE:1.18
	OPENBSD_3_6:1.18.0.2
	OPENBSD_3_6_BASE:1.18
	SMP_SYNC_A:1.16
	SMP_SYNC_B:1.16
	OPENBSD_3_5:1.15.0.2
	OPENBSD_3_5_BASE:1.15
	OPENBSD_3_4:1.14.0.4
	OPENBSD_3_4_BASE:1.14
	UBC_SYNC_A:1.14
	OPENBSD_3_3:1.14.0.2
	OPENBSD_3_3_BASE:1.14
	OPENBSD_3_2:1.12.0.4
	OPENBSD_3_2_BASE:1.12
	OPENBSD_3_1:1.12.0.2
	OPENBSD_3_1_BASE:1.12
	UBC_SYNC_B:1.12
	UBC:1.4.0.4
	UBC_BASE:1.4
	OPENBSD_3_0:1.4.0.2
	OPENBSD_3_0_BASE:1.4
	OPENBSD_2_9_BASE:1.2
	OPENBSD_2_9:1.2.0.2
	OPENBSD_2_8:1.1.0.10
	OPENBSD_2_8_BASE:1.1
	OPENBSD_2_7:1.1.0.8
	OPENBSD_2_7_BASE:1.1
	SMP:1.1.0.6
	SMP_BASE:1.1
	kame_19991208:1.1
	OPENBSD_2_6:1.1.0.4
	OPENBSD_2_6_BASE:1.1
	OPENBSD_2_5:1.1.0.2
	OPENBSD_2_5_BASE:1.1;
locks; strict;
comment	@ * @;


1.69
date	2017.02.07.05.39.17;	author dlg;	state Exp;
branches;
next	1.68;
commitid	2bHEmLpLqQ16mm8S;

1.68
date	2016.11.21.01.44.06;	author dlg;	state Exp;
branches;
next	1.67;
commitid	JIO0WWMEivOpisVL;

1.67
date	2016.11.07.23.45.27;	author dlg;	state Exp;
branches;
next	1.66;
commitid	805X7HiU6gaUaPlw;

1.66
date	2016.11.02.06.26.16;	author dlg;	state Exp;
branches;
next	1.65;
commitid	ZrUh4wW5fuEb8W9n;

1.65
date	2016.11.02.01.58.07;	author dlg;	state Exp;
branches;
next	1.64;
commitid	hsKr6e0ZYx5Tobqr;

1.64
date	2016.11.02.01.20.50;	author dlg;	state Exp;
branches;
next	1.63;
commitid	U4ALOb3m8BAwcI9m;

1.63
date	2016.09.15.02.00.16;	author dlg;	state Exp;
branches;
next	1.62;
commitid	RlO92XR575sygHqm;

1.62
date	2016.09.15.01.24.08;	author dlg;	state Exp;
branches;
next	1.61;
commitid	3DliHSP24LcOtzRL;

1.61
date	2016.09.05.09.04.31;	author dlg;	state Exp;
branches;
next	1.60;
commitid	6XQAtywm0KL8IsZY;

1.60
date	2016.09.05.07.38.32;	author dlg;	state Exp;
branches;
next	1.59;
commitid	DmvWxtez1rgiAAMH;

1.59
date	2016.04.21.04.09.28;	author mlarkin;	state Exp;
branches;
next	1.58;
commitid	Ujd8bJH9hHcKeRgM;

1.58
date	2015.09.08.21.28.36;	author kettenis;	state Exp;
branches;
next	1.57;
commitid	WSD3bUAMn8qMj0PM;

1.57
date	2015.02.10.06.16.13;	author dlg;	state Exp;
branches;
next	1.56;
commitid	ujIKFiO4a0xbEB6g;

1.56
date	2014.12.22.02.59.54;	author tedu;	state Exp;
branches;
next	1.55;
commitid	IDPPHd75YO0b2Obx;

1.55
date	2014.12.19.02.15.25;	author dlg;	state Exp;
branches;
next	1.54;
commitid	8bRoeApKJPUxFCoQ;

1.54
date	2014.11.01.23.58.07;	author tedu;	state Exp;
branches;
next	1.53;
commitid	po5B2vOrAoI2l87Z;

1.53
date	2014.09.22.01.04.58;	author dlg;	state Exp;
branches;
next	1.52;
commitid	hZsc7X90BQCBOwmw;

1.52
date	2014.09.16.03.26.08;	author dlg;	state Exp;
branches;
next	1.51;
commitid	WsnvgTKcfl8dBlWe;

1.51
date	2014.09.08.00.05.22;	author dlg;	state Exp;
branches;
next	1.50;
commitid	QsGhYaxgHWxWkPfl;

1.50
date	2014.09.08.00.00.05;	author dlg;	state Exp;
branches;
next	1.49;
commitid	yDOHf63nOnS1TEoY;

1.49
date	2014.09.04.00.36.00;	author dlg;	state Exp;
branches;
next	1.48;
commitid	YMCPXWjFrLw20rLn;

1.48
date	2014.08.27.00.22.26;	author dlg;	state Exp;
branches;
next	1.47;
commitid	qJ6y3Y7RSE51lOmV;

1.47
date	2014.07.02.00.23.36;	author dlg;	state Exp;
branches;
next	1.46;
commitid	Gj1PMUzueBry5NYj;

1.46
date	2014.07.02.00.15.27;	author dlg;	state Exp;
branches;
next	1.45;
commitid	yL3mv3WU32Wi7d4z;

1.45
date	2014.07.02.00.12.34;	author dlg;	state Exp;
branches;
next	1.44;
commitid	zNbAzxmEBZMksTSx;

1.44
date	2013.11.05.03.28.44;	author dlg;	state Exp;
branches;
next	1.43;

1.43
date	2013.03.26.16.37.45;	author tedu;	state Exp;
branches;
next	1.42;

1.42
date	2012.12.24.19.43.11;	author guenther;	state Exp;
branches;
next	1.41;

1.41
date	2011.07.05.16.36.15;	author tedu;	state Exp;
branches;
next	1.40;

1.40
date	2011.04.18.19.23.46;	author art;	state Exp;
branches;
next	1.39;

1.39
date	2011.04.06.15.52.13;	author art;	state Exp;
branches;
next	1.38;

1.38
date	2011.04.05.01.28.05;	author art;	state Exp;
branches;
next	1.37;

1.37
date	2011.04.03.22.07.37;	author ariane;	state Exp;
branches;
next	1.36;

1.36
date	2010.09.26.21.03.57;	author tedu;	state Exp;
branches;
next	1.35;

1.35
date	2010.07.13.16.47.02;	author deraadt;	state Exp;
branches;
next	1.34;

1.34
date	2010.06.27.03.03.48;	author thib;	state Exp;
branches;
next	1.33;

1.33
date	2009.08.13.13.49.20;	author thib;	state Exp;
branches;
next	1.32;

1.32
date	2009.08.09.13.41.03;	author thib;	state Exp;
branches;
next	1.31;

1.31
date	2008.12.23.06.54.12;	author dlg;	state Exp;
branches;
next	1.30;

1.30
date	2008.10.23.23.54.02;	author tedu;	state Exp;
branches;
next	1.29;

1.29
date	2008.06.26.05.42.20;	author ray;	state Exp;
branches;
next	1.28;

1.28
date	2008.06.14.03.56.41;	author art;	state Exp;
branches;
next	1.27;

1.27
date	2008.05.16.17.21.36;	author thib;	state Exp;
branches;
next	1.26;

1.26
date	2008.05.06.20.57.19;	author thib;	state Exp;
branches;
next	1.25;

1.25
date	2007.12.09.00.24.04;	author tedu;	state Exp;
branches;
next	1.24;

1.24
date	2007.05.28.17.55.56;	author tedu;	state Exp;
branches;
next	1.23;

1.23
date	2007.04.23.11.28.24;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2007.04.23.09.27.59;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2007.03.21.17.57.40;	author pedro;	state Exp;
branches;
next	1.20;

1.20
date	2006.12.23.15.00.15;	author miod;	state Exp;
branches;
next	1.19;

1.19
date	2006.05.07.20.06.50;	author tedu;	state Exp;
branches;
next	1.18;

1.18
date	2004.07.29.09.18.17;	author mickey;	state Exp;
branches;
next	1.17;

1.17
date	2004.07.20.23.47.08;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2004.06.02.22.17.22;	author tedu;	state Exp;
branches;
next	1.15;

1.15
date	2003.11.18.06.08.18;	author tedu;	state Exp;
branches;
next	1.14;

1.14
date	2002.12.20.07.48.01;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2002.11.24.18.43.33;	author pb;	state Exp;
branches;
next	1.12;

1.12
date	2002.03.09.01.11.11;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2002.02.25.04.53.16;	author dhartmei;	state Exp;
branches;
next	1.10;

1.10
date	2002.02.23.02.52.56;	author art;	state Exp;
branches;
next	1.9;

1.9
date	2002.02.23.01.12.26;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2002.01.28.03.23.52;	author art;	state Exp;
branches;
next	1.7;

1.7
date	2002.01.25.15.50.23;	author art;	state Exp;
branches;
next	1.6;

1.6
date	2002.01.23.01.44.20;	author art;	state Exp;
branches;
next	1.5;

1.5
date	2002.01.23.00.39.48;	author art;	state Exp;
branches;
next	1.4;

1.4
date	2001.06.24.16.00.46;	author art;	state Exp;
branches
	1.4.4.1;
next	1.3;

1.3
date	2001.06.23.16.13.01;	author art;	state Exp;
branches;
next	1.2;

1.2
date	2000.12.05.16.43.40;	author art;	state Exp;
branches;
next	1.1;

1.1
date	99.02.26.03.13.29;	author art;	state Exp;
branches
	1.1.6.1;
next	;

1.1.6.1
date	2001.05.14.22.45.03;	author niklas;	state Exp;
branches;
next	1.1.6.2;

1.1.6.2
date	2001.07.04.11.00.26;	author niklas;	state Exp;
branches;
next	1.1.6.3;

1.1.6.3
date	2002.03.06.02.17.13;	author niklas;	state Exp;
branches;
next	1.1.6.4;

1.1.6.4
date	2002.03.28.14.52.01;	author niklas;	state Exp;
branches;
next	1.1.6.5;

1.1.6.5
date	2003.03.28.00.41.30;	author niklas;	state Exp;
branches;
next	1.1.6.6;

1.1.6.6
date	2004.02.19.11.01.34;	author niklas;	state Exp;
branches;
next	1.1.6.7;

1.1.6.7
date	2004.06.05.23.13.09;	author niklas;	state Exp;
branches;
next	;

1.4.4.1
date	2002.01.31.22.55.49;	author niklas;	state Exp;
branches;
next	1.4.4.2;

1.4.4.2
date	2002.06.11.03.32.33;	author art;	state Exp;
branches;
next	1.4.4.3;

1.4.4.3
date	2003.05.19.22.32.19;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.69
log
@export the multi page allocator so other things can explicitly use it.
@
text
@/*	$OpenBSD: pool.h,v 1.68 2016/11/21 01:44:06 dlg Exp $	*/
/*	$NetBSD: pool.h,v 1.27 2001/06/06 22:00:17 rafal Exp $	*/

/*-
 * Copyright (c) 1997, 1998, 1999, 2000 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Paul Kranenburg; by Jason R. Thorpe of the Numerical Aerospace
 * Simulation Facility, NASA Ames Research Center.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

#ifndef _SYS_POOL_H_
#define _SYS_POOL_H_

/*
 * sysctls.
 * kern.pool.npools
 * kern.pool.name.<number>
 * kern.pool.pool.<number>
 */
#define KERN_POOL_NPOOLS	1
#define KERN_POOL_NAME		2
#define KERN_POOL_POOL		3

struct kinfo_pool {
	unsigned int	pr_size;	/* size of a pool item */
	unsigned int	pr_pgsize;	/* size of a "page" */
	unsigned int	pr_itemsperpage; /* number of items per "page" */
	unsigned int	pr_minpages;	/* same in page units */
	unsigned int	pr_maxpages;	/* maximum # of idle pages to keep */
	unsigned int	pr_hardlimit;	/* hard limit to number of allocated
					   items */

	unsigned int	pr_npages;	/* # of pages allocated */
	unsigned int	pr_nout;	/* # items currently allocated */
	unsigned int	pr_nitems;	/* # items in the pool */

	unsigned long	pr_nget;	/* # of successful requests */
	unsigned long	pr_nput;	/* # of releases */
	unsigned long	pr_nfail;	/* # of unsuccessful requests */
	unsigned long	pr_npagealloc;	/* # of pages allocated */
	unsigned long	pr_npagefree;	/* # of pages released */
	unsigned int	pr_hiwat;	/* max # of pages in pool */
	unsigned long	pr_nidle;	/* # of idle pages */
};

#if defined(_KERNEL) || defined(_LIBKVM)

#include <sys/queue.h>
#include <sys/tree.h>
#include <sys/mutex.h>

struct pool;
struct pool_request;
TAILQ_HEAD(pool_requests, pool_request);

struct pool_allocator {
	void		*(*pa_alloc)(struct pool *, int, int *);
	void		 (*pa_free)(struct pool *, void *);
	size_t		   pa_pagesz;
};

/*
 * The pa_pagesz member encodes the sizes of pages that can be
 * provided by the allocator, and whether the allocations can be
 * aligned to their size.
 *
 * Page sizes can only be powers of two. Each available page size is
 * represented by its value set as a bit. e.g., to indicate that an
 * allocator can provide 16k and 32k pages you initialise pa_pagesz
 * to (32768 | 16384).
 *
 * If the allocator can provide aligned pages the low bit in pa_pagesz
 * is set. The POOL_ALLOC_ALIGNED macro is provided as a convenience.
 *
 * If pa_pagesz is unset (i.e. 0), POOL_ALLOC_DEFAULT will be used
 * instead.
 */

#define POOL_ALLOC_ALIGNED		1UL
#define POOL_ALLOC_SIZE(_sz, _a)	((_sz) | (_a))
#define POOL_ALLOC_SIZES(_min, _max, _a) \
	((_max) | \
	(((_max) - 1) & ~((_min) - 1)) | (_a))

#define POOL_ALLOC_DEFAULT \
	POOL_ALLOC_SIZE(PAGE_SIZE, POOL_ALLOC_ALIGNED)

TAILQ_HEAD(pool_pagelist, pool_page_header);

struct pool_cache_item;
TAILQ_HEAD(pool_cache_lists, pool_cache_item);
struct cpumem;

struct pool {
	struct mutex	pr_mtx;
	SIMPLEQ_ENTRY(pool)
			pr_poollist;
	struct pool_pagelist
			pr_emptypages;	/* Empty pages */
	struct pool_pagelist
			pr_fullpages;	/* Full pages */
	struct pool_pagelist
			pr_partpages;	/* Partially-allocated pages */
	struct pool_page_header	*
			pr_curpage;
	unsigned int	pr_size;	/* Size of item */
	unsigned int	pr_minitems;	/* minimum # of items to keep */
	unsigned int	pr_minpages;	/* same in page units */
	unsigned int	pr_maxpages;	/* maximum # of idle pages to keep */
	unsigned int	pr_npages;	/* # of pages allocated */
	unsigned int	pr_itemsperpage;/* # items that fit in a page */
	unsigned int	pr_slack;	/* unused space in a page */
	unsigned int	pr_nitems;	/* number of available items in pool */
	unsigned int	pr_nout;	/* # items currently allocated */
	unsigned int	pr_hardlimit;	/* hard limit to number of allocated
					   items */
	unsigned int	pr_serial;	/* unique serial number of the pool */
	unsigned int	pr_pgsize;	/* Size of a "page" */
	vaddr_t		pr_pgmask;	/* Mask with an item to get a page */
	struct pool_allocator *
			pr_alloc;	/* backend allocator */
	const char *	pr_wchan;	/* tsleep(9) identifier */
#define PR_WAITOK	0x0001 /* M_WAITOK */
#define PR_NOWAIT	0x0002 /* M_NOWAIT */
#define PR_LIMITFAIL	0x0004 /* M_CANFAIL */
#define PR_ZERO		0x0008 /* M_ZERO */
#define PR_WANTED	0x0100

	int		pr_ipl;

	RBT_HEAD(phtree, pool_page_header)
			pr_phtree;

	struct cpumem *	pr_cache;
	unsigned long	pr_cache_magic[2];
	struct mutex	pr_cache_mtx;
	struct pool_cache_lists
			pr_cache_lists;
	u_int		pr_cache_nlist;
	u_int		pr_cache_items;
	u_int		pr_cache_contention;
	int		pr_cache_nout;

	u_int		pr_align;
	u_int		pr_maxcolors;	/* Cache coloring */
	int		pr_phoffset;	/* Offset in page of page header */

	/*
	 * Warning message to be issued, and a per-time-delta rate cap,
	 * if the hard limit is reached.
	 */
	const char	*pr_hardlimit_warning;
	struct timeval	pr_hardlimit_ratecap;
	struct timeval	pr_hardlimit_warning_last;

	/*
	 * pool item requests queue
	 */
	struct mutex	pr_requests_mtx;
	struct pool_requests
			pr_requests;
	unsigned int	pr_requesting;

	/*
	 * Instrumentation
	 */
	unsigned long	pr_nget;	/* # of successful requests */
	unsigned long	pr_nfail;	/* # of unsuccessful requests */
	unsigned long	pr_nput;	/* # of releases */
	unsigned long	pr_npagealloc;	/* # of pages allocated */
	unsigned long	pr_npagefree;	/* # of pages released */
	unsigned int	pr_hiwat;	/* max # of pages in pool */
	unsigned long	pr_nidle;	/* # of idle pages */

	/* Physical memory configuration. */
	const struct kmem_pa_mode *
			pr_crange;
};

#endif /* _KERNEL || _LIBKVM */

#ifdef _KERNEL

extern struct pool_allocator pool_allocator_single;
extern struct pool_allocator pool_allocator_multi;

struct pool_request {
	TAILQ_ENTRY(pool_request) pr_entry;
	void (*pr_handler)(void *, void *);
	void *pr_cookie;
	void *pr_item;
};

void		pool_init(struct pool *, size_t, u_int, int, int,
		    const char *, struct pool_allocator *);
void		pool_cache_init(struct pool *);
void		pool_destroy(struct pool *);
void		pool_setlowat(struct pool *, int);
void		pool_sethiwat(struct pool *, int);
int		pool_sethardlimit(struct pool *, u_int, const char *, int);
struct uvm_constraint_range; /* XXX */
void		pool_set_constraints(struct pool *,
		    const struct kmem_pa_mode *mode);

void		*pool_get(struct pool *, int) __malloc;
void		pool_request_init(struct pool_request *,
		    void (*)(void *, void *), void *);
void		pool_request(struct pool *, struct pool_request *);
void		pool_put(struct pool *, void *);
int		pool_reclaim(struct pool *);
void		pool_reclaim_all(void);
int		pool_prime(struct pool *, int);

#ifdef DDB
/*
 * Debugging and diagnostic aides.
 */
void		pool_printit(struct pool *, const char *,
		    int (*)(const char *, ...));
void		pool_walk(struct pool *, int, int (*)(const char *, ...),
		    void (*)(void *, int, int (*)(const char *, ...)));
#endif

/* the allocator for dma-able memory is a thin layer on top of pool  */
void		 dma_alloc_init(void);
void		*dma_alloc(size_t size, int flags);
void		 dma_free(void *m, size_t size);
#endif /* _KERNEL */

#endif /* _SYS_POOL_H_ */
@


1.68
log
@let pool page allocators advertise what sizes they can provide.

to keep things concise i let the multi page allocators provide
multiple sizes of pages, but this feature was implicit inside
pool_init and only usable if the caller of pool_init did not specify
a page allocator.

callers of pool_init can now suplly a page allocator that provides
multiple page sizes. pool_init will try to fit 8 items onto a page
still, but will scale its page size down until it fits into what
the allocator provides.

supported page sizes are specified as a bit field in the pa_pagesz
member of a pool_allocator. setting the low bit in that word indicates
that the pages can be aligned to their size.
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.67 2016/11/07 23:45:27 dlg Exp $	*/
d208 1
@


1.67
log
@rename some types and functions to make the code easier to read.

pool_item_header is now pool_page_header. the more useful change
is pool_list is now pool_cache_item. that's what items going into
the per cpu pool caches are cast to, and they get linked together
to make a list.

the functions operating on what is now pool_cache_items have been
renamed to make it more obvious what they manipulate.
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.66 2016/11/02 06:26:16 dlg Exp $	*/
d80 3
a82 3
	void *(*pa_alloc)(struct pool *, int, int *);
	void (*pa_free)(struct pool *, void *);
	int pa_pagesz;
d84 26
@


1.66
log
@poison the TAILQ_ENTRY in items in the per cpu pool cache.
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.65 2016/11/02 01:58:07 dlg Exp $	*/
d85 1
a85 1
TAILQ_HEAD(pool_pagelist, pool_item_header);
d87 2
a88 2
struct pool_list;
TAILQ_HEAD(pool_lists, pool_list);
d101 1
a101 1
	struct pool_item_header	*
d128 1
a128 1
	RBT_HEAD(phtree, pool_item_header)
d134 1
a134 1
	struct pool_lists
@


1.65
log
@use a TAILQ to maintain the list of item lists used by the percpu code.

it makes it more readable, and fixes a bug in pool_list_put where it
was returning the next item in the current list rather than the next
list to be freed.
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.64 2016/11/02 01:20:50 dlg Exp $	*/
d132 1
@


1.64
log
@add per cpu caches for free pool items.

this is modelled on whats described in the "Magazines and Vmem:
Extending the Slab Allocator to Many CPUs and Arbitrary Resources"
paper by Jeff Bonwick and Jonathan Adams.

the main semantic borrowed from the paper is the use of two lists
of free pool items on each cpu, and only moving one of the lists
in and out of a global depot of free lists to mitigate against a
cpu thrashing against that global depot.

unlike slabs, pools do not maintain or cache constructed items,
which allows us to use the items themselves to build the free list
rather than having to allocate arrays to point at constructed pool
items.

the per cpu caches are build on top of the cpumem api.

this has been kicked a bit by hrvoje popovski and simon mages (thank you).
im putting it in now so it is easier to work on and test.
ok jmatthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.63 2016/09/15 02:00:16 dlg Exp $	*/
d88 1
d133 2
a134 2
	struct pool_list *
			pr_cache_list;
@


1.63
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.62 2016/09/15 01:24:08 dlg Exp $	*/
d87 3
d130 9
d190 1
@


1.62
log
@move pools to using the subr_tree version of rb trees

this is half way to recovering the space used by the subr_tree code.
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.61 2016/09/05 09:04:31 dlg Exp $	*/
d176 1
a176 1
void		pool_init(struct pool *, size_t, u_int, u_int, int,
a178 1
void		pool_setipl(struct pool *, int);
@


1.61
log
@revert moving pools from tree.h to subr_tree.c rb trees.

itll go in again when i dont break userland.
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.59 2016/04/21 04:09:28 mlarkin Exp $	*/
d124 1
a124 1
	RB_HEAD(phtree, pool_item_header)
@


1.60
log
@move pool red-black trees from tree.h code to subr_tree.c code

ok tedu@@
@
text
@d124 1
a124 1
	RBT_HEAD(phtree, pool_item_header)
@


1.59
log
@
Remove some incorrect and outdated references to pool debugging from pool.h
and pool(9) manpage

ok dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.58 2015/09/08 21:28:36 kettenis Exp $	*/
d124 1
a124 1
	RB_HEAD(phtree, pool_item_header)
@


1.58
log
@Give the pool page allocator backends more sensible names.  We now have:
* pool_allocator_single: single page allocator, always interrupt safe
* pool_allocator_multi: multi-page allocator, interrupt safe
* pool_allocator_multi_ni: multi-page allocator, not interrupt-safe

ok deraadt@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.57 2015/02/10 06:16:13 dlg Exp $	*/
a120 1
#define PR_DEBUGCHK	0x1000
@


1.57
log
@reintroduce page item cache colouring.

if you're having trouble understanding what this helps, imagine
your cpus caches are a hash table. by moving the base address of
items around (colouring them), you give it more bits to hash with.
in turn that makes it less likely that you will overflow buckets
in your hash. i mean cache.

it was inadvertantly removed in my churn of this subsystem, but as
tedu has said on this issue:

> The history of pool is filled with features getting trimmed because they
> seemed unnecessary or in the way, only to later discover how important they
> were. Having slowly learned that lesson, I think our default should be "if
> bonwick says do it, we do it" until proven otherwise.

until proven otherwise we can keep the functionality, especially
as the code cost is minimal.

ok many including tedu@@ guenther@@ deraadt@@ millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.56 2014/12/22 02:59:54 tedu Exp $	*/
d168 1
a168 1
extern struct pool_allocator pool_allocator_nointr;
@


1.56
log
@remove some unused fields from pool. ok dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.55 2014/12/19 02:15:25 dlg Exp $	*/
d128 2
@


1.55
log
@replace the page LISTS with page TAILQs. this will let me pull pages from
either end of the lists cheaply.

ok kettenis@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.54 2014/11/01 23:58:07 tedu Exp $	*/
a99 1
	unsigned int	pr_align;	/* Requested alignment, must be 2^n */
a115 2
	unsigned int	pr_flags;	/* r/w flags */
	unsigned int	pr_roflags;	/* r/o flags */
@


1.54
log
@remove color support. discussed with dlg and mikeb
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.53 2014/09/22 01:04:58 dlg Exp $	*/
d85 1
a85 1
LIST_HEAD(pool_pagelist, pool_item_header);
@


1.53
log
@rework the pool code to make the locking more obvious (to me at
least). after this i am confident that pools are mpsafe, ie, can
be called without the kernel biglock being held.

the page allocation and setup code has been split into four parts:

pool_p_alloc is called without any locks held to ask the pool_allocator
backend to get a page and page header and set up the item list.

pool_p_insert is called with the pool lock held to insert the newly
minted page on the pools internal free page list and update its
internal accounting.

once the pool has finished with a page it calls the following:

pool_p_remove is called with the pool lock help to take the now
unnecessary page off the free page list and uncount it.

pool_p_free is called without the pool lock and does a bunch of
checks to verify that the items arent corrupted and have all been
returned to the page before giving it back to the pool_allocator
to be freed.

instead of pool_do_get doing all the work for pool_get, it is now
only responsible for doing a single item allocation. if for any
reason it cant get an item, it just returns NULL. pool_get is now
responsible for checking if the allocation is allowed (according
to hi watermarks etc), and for potentially sleeping waiting for
resources if required.

sleeping for resources is now built on top of pool_requests, which
are modelled on how the scsi midlayer schedules access to scsibus
resources.

the pool code now calls pool_allocator backends inside its own
calls to KERNEL_LOCK and KERNEL_UNLOCK, so users of pools dont
have to hold biglock to call pool_get or pool_put.

tested by krw@@ (who found a SMALL_KERNEL issue, thank you)
noone objected
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.52 2014/09/16 03:26:08 dlg Exp $	*/
a130 2
	int		pr_maxcolor;	/* Cache colouring */
	int		pr_curcolor;
@


1.52
log
@deprecate PR_DEBUG and MALLOC_DEBUG in pools.

poked by kspillner@@
ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.51 2014/09/08 00:05:22 dlg Exp $	*/
d76 2
d144 8
d173 7
a179 1
/* these functions are not locked */
a190 1
/* these functions are locked */
d192 3
@


1.51
log
@the PR_LOGGING flag is unused, so im cleaning it up
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.50 2014/09/08 00:00:05 dlg Exp $	*/
a121 1
#define PR_DEBUG	0x0800
@


1.50
log
@deprecate the use of the PR_PHINPAGE flag by replacing it with a test
of pr_phoffset.

ok doug@@ guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.49 2014/09/04 00:36:00 dlg Exp $	*/
a121 1
#define PR_LOGGING	0x0400
@


1.49
log
@rework how pools with large pages (>PAGE_SIZE) are implemented.

this moves the size of the pool page (not arch page) out of the
pool allocator into struct pool. this lets us create only two pools
for the automatically determined large page allocations instead of
256 of them.

while here support using slack space in large pages for the
pool_item_header by requiring km_alloc provide pool page aligned
memory.

lastly, instead of doing incorrect math to figure how how many arch
pages to use for large pool pages, just use powers of two.

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.48 2014/08/27 00:22:26 dlg Exp $	*/
a121 1
#define PR_PHINPAGE	0x0200
@


1.48
log
@deprecate the "item offset" handling. nothing uses it, so we can
cut it out of the code to simplify things.

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.47 2014/07/02 00:23:36 dlg Exp $	*/
a80 2
	int pa_pagemask;
	int pa_pageshift;
d110 2
@


1.47
log
@pools havent needed to include <sys/time.h> for 7 or 8 years.
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.46 2014/07/02 00:15:27 dlg Exp $	*/
a100 1
	unsigned int	pr_itemoffset;	/* Align this offset in item */
@


1.46
log
@formatting tweaks. no functional change
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.45 2014/07/02 00:12:34 dlg Exp $	*/
a71 1
#include <sys/time.h>
@


1.45
log
@info about pools is currently given to userland by copying each
pools struct out. however, struct pool in the kernel contains lots
of things that userland probably isnt interested in, like actual
mutexes, and probably shouldnt get easy access to, like pointers
to kernel memory via all the lists/trees.

this implements a kinfo_pool structure that has only the data that
userland needs to know about. it cuts the sysctl code over to
building it from struct pool as required and copying that out
instead, and cuts userland over to only handling kinfo_pool.

the only problem with this is vmstat, which can read kernel images
via kvm, which needs some understanding of struct pool. to cope,
the struct pool definition is guarded by if defined(_KERNEL) ||
defined(_LIBKVM) as inspired by sysctl which needs to do the same
thing sometimes. struct pool itself is generally not visible to
userland though, which is good.

matthew@@ suggested struct kinfo_pool instead of struct pool_info.
the kinfo prefix has precedent.
lots of people liked this.
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.44 2013/11/05 03:28:44 dlg Exp $	*/
d86 1
a86 1
LIST_HEAD(pool_pagelist,pool_item_header);
d98 2
a99 1
	struct pool_item_header	*pr_curpage;
d114 3
a116 2
	struct pool_allocator *pr_alloc;/* backend allocator */
	const char	*pr_wchan;	/* tsleep(9) identifier */
d129 1
a129 1
	int			pr_ipl;
d131 2
a132 1
	RB_HEAD(phtree, pool_item_header) pr_phtree;
d158 2
a159 1
	const struct kmem_pa_mode *pr_crange;
d198 3
a200 3
void			 dma_alloc_init(void);
void			*dma_alloc(size_t size, int flags);
void			 dma_free(void *m, size_t size);
@


1.44
log
@remove pool constructors and destructors. theyre called for every
get and put, so they dont save us anything by caching constructed
objects. there were no real users of them, and this api was never
documented. removing conditionals in a hot path cant be a bad idea
either.

ok deraadt@@ krw@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.43 2013/03/26 16:37:45 tedu Exp $	*/
d47 24
d158 2
d161 1
@


1.43
log
@simpleq is lighter weight and sufficient for pool's needs.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.42 2012/12/24 19:43:11 guenther Exp $	*/
a110 4
	/* constructor, destructor, and arg */
	int		(*pr_ctor)(void *, void *, int);
	void		(*pr_dtor)(void *, void *);
	void		*pr_arg;
a147 2
void		pool_set_ctordtor(struct pool *, int (*)(void *, void *, int),
		    void(*)(void *, void *), void *);
@


1.42
log
@Fix compilation with POOL_DEBUG but !DDB

ok jsing@@ krw@@ mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.41 2011/07/05 16:36:15 tedu Exp $	*/
d66 1
a66 1
	TAILQ_ENTRY(pool)
a89 1
	TAILQ_ENTRY(pool) pr_alloc_list;/* list of pools using this allocator */
@


1.41
log
@when all you have is a hammer, make it a big one.  add more checks to pool_chk
and a pool_init flag to aggressively run pool_chk.  ok art deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.40 2011/04/18 19:23:46 art Exp $	*/
a168 1
int		pool_chk(struct pool *);
@


1.40
log
@Put back the change of pool and malloc into the new km_alloc(9) api.
The problems during the hackathon were not caused by this (most likely).

prodded by deraadt@@ and beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.39 2011/04/06 15:52:13 art Exp $	*/
d102 1
d169 1
a169 1
int		pool_chk(struct pool *, const char *);
@


1.39
log
@Backout the uvm_km_getpage -> km_alloc conversion. Weird things are happening
and we aren't sure what's causing them.

shouted oks by many before I even built a kernel with the diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.38 2011/04/05 01:28:05 art Exp $	*/
d135 1
a135 2
	struct uvm_constraint_range *pr_crange;
	int		pr_pa_nsegs;
d151 1
a151 1
		    struct uvm_constraint_range *, int);
@


1.38
log
@ - Change pool constraints to use kmem_pa_mode instead of uvm_constraint_range
 - Use km_alloc for all backend allocations in pools.
 - Use km_alloc for the emergmency kentry allocations in uvm_mapent_alloc
 - Garbage collect uvm_km_getpage, uvm_km_getpage_pla and uvm_km_putpage

ariane@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.37 2011/04/03 22:07:37 ariane Exp $	*/
d135 2
a136 1
	struct kmem_pa_mode *pr_crange;
d151 2
a152 1
void		pool_set_constraints(struct pool *, struct kmem_pa_mode *mode);
@


1.37
log
@Helper functions for suspend.

Allow reclaiming pages from all pools.
Allow zeroing all pages.
Allocate the more equal pig.

mlarking@@ needs this.
Not called yet.

ok mlarkin@@, theo@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.36 2010/09/26 21:03:57 tedu Exp $	*/
d135 1
a135 2
	struct uvm_constraint_range *pr_crange;
	int		pr_pa_nsegs;
d150 1
a150 2
void		pool_set_constraints(struct pool *,
		    struct uvm_constraint_range *, int);
@


1.36
log
@unify some pool and malloc flag values.  the important bit is that all flags
have real values, no 0 values anymore.
ok deraadt kettenis krw matthew oga thib
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.35 2010/07/13 16:47:02 deraadt Exp $	*/
d160 1
@


1.35
log
@dma_alloc() and dma_free().  This is a thin shim on top of a bag of
pools, sized by powers of 2, which are constrained to dma memory.
ok matthew tedu thib
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.34 2010/06/27 03:03:48 thib Exp $	*/
d94 8
a101 9
#define PR_MALLOCOK	0x01
#define	PR_NOWAIT	0x00		/* for symmetry */
#define PR_WAITOK	0x02
#define PR_WANTED	0x04
#define PR_PHINPAGE	0x08
#define PR_LOGGING	0x10
#define PR_LIMITFAIL	0x20	/* even if waiting, fail if we hit limit */
#define PR_DEBUG	0x40
#define PR_ZERO		0x100
@


1.34
log
@uvm constraints. Add two mandatory MD symbols, uvm_md_constraints
which contains the constraints for DMA/memory allocation for each
architecture, and dma_constraints which contains the range of addresses
that are dma accessable by the system.

This is based on ariane@@'s physcontig diff, with lots of bugfixes and
additions the following additions by my self:

Introduce a new function pool_set_constraints() which sets the address
range for which we allocate pages for the pool from, this is now used
for the mbuf/mbuf cluster pools to keep them dma accessible.

The !direct archs no longer stuff pages into the kernel object in
uvm_km_getpage_pla but rather do a pmap_extract() in uvm_km_putpages.

Tested heavily by my self on i386, amd64 and sparc64. Some tests on
alpha and SGI.

"commit it" beck, art, oga, deraadt
"i like the diff" deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.29 2008/06/26 05:42:20 ray Exp $	*/
d173 5
@


1.33
log
@add a show all vnodes command, use dlg's nice pool_walk() to accomplish
this.

ok beck@@, dlg@@
@
text
@d134 4
d151 3
@


1.32
log
@Use an RB tree instead of a SPLAY tree for the page headers tree.

ok beck@@, dlg@@
@
text
@d163 2
a164 1
void		pool_walk(struct pool *, void (*)(void *));
@


1.31
log
@oops, forgot this when committing to subr_pool.c

add pool_walk as debug code.

this can be used to walk over all the items allocated with a pool and have
them examined by a function the caller provides.

with help from and ok tedu@@
@
text
@d106 1
a106 1
	SPLAY_HEAD(phtree, pool_item_header) pr_phtree;
@


1.30
log
@a better fix for the "uvm_km thread runs out of memory" problem.

add a new arg to the backend so it can tell pool to slow down.  when we get
this flag, yield *after* putting the page in the pool's free list.  whatever
we do, don't let the thread sleep.

this makes things better by still letting the thread run when a huge pf
request comes in, but without artificially increasing pressure on the backend
by eating pages without feeding them forward.

ok deraadt
@
text
@d163 1
@


1.29
log
@First pass at removing clauses 3 and 4 from NetBSD licenses.

Not sure what's more surprising: how long it took for NetBSD to
catch up to the rest of the BSDs (including UCB), or the amount of
code that NetBSD has claimed for itself without attributing to the
actual authors.

OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.28 2008/06/14 03:56:41 art Exp $	*/
d55 1
a55 1
	void *(*pa_alloc)(struct pool *, int);
@


1.28
log
@oldnointr pool allocator is no longer used or necessary.
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.27 2008/05/16 17:21:36 thib Exp $	*/
a19 7
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the NetBSD
 *	Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
@


1.27
log
@unsigned -> u_int and warnmess -> warnmsg
for pool_sethardlimit.

prodded by and ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.26 2008/05/06 20:57:19 thib Exp $	*/
a143 3
/* old nointr allocator, still needed for large allocations */
extern struct pool_allocator pool_allocator_oldnointr;

@


1.26
log
@Add a PR_ZERO flag for pools, to compliment the M_ZERO
malloc flag, does the same thing.
use it in a few places.

OK tedu@@, "then go ahead. and don't forget the manpage (-:" miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.25 2007/12/09 00:24:04 tedu Exp $	*/
d156 1
a156 1
int		pool_sethardlimit(struct pool *, unsigned, const char *, int);
@


1.25
log
@big patch to simplify pool code.

remove pool_cache code.  it was barely used, and quite complex.  it's
silly to have both a "fast" and "faster" allocation interface.  provide
a ctor/dtor interface, and convert the few cache users to use it.  no
caching at this time.

use mutexes to protect pools.  they should be initialized with pool_setipl
if the pool may be used in an interrupt context, without existing spl
protection.

ok art deraadt thib
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.24 2007/05/28 17:55:56 tedu Exp $	*/
d109 1
@


1.24
log
@add a pool_setipl function, which allows setting an appropriate ipl
for splassert inside pool_get and pool_put (DIAGNOSTIC only)
ok miod pedro thib
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.23 2007/04/23 11:28:24 art Exp $	*/
a53 1
#include <sys/lock.h>
d57 1
d59 1
a59 24
struct pool_cache {
	TAILQ_ENTRY(pool_cache)
			pc_poollist;	/* entry on pool's group list */
	TAILQ_HEAD(, pool_cache_group)
			pc_grouplist;	/* Cache group list */
	struct pool_cache_group
			*pc_allocfrom;	/* group to allocate from */
	struct pool_cache_group
			*pc_freeto;	/* group to free to */
	struct pool	*pc_pool;	/* parent pool */
	struct simplelock pc_slock;	/* mutex */

	int		(*pc_ctor)(void *, void *, int);
	void		(*pc_dtor)(void *, void *);
	void		*pc_arg;

	/* Statistics. */
	unsigned long	pc_hits;	/* cache hits */
	unsigned long	pc_misses;	/* cache misses */

	unsigned long	pc_ngroups;	/* # cache groups */

	unsigned long	pc_nitems;	/* # objects currently in cache */
};
a64 7

	/* The following fields are for internal use only */
	struct simplelock pa_slock;
	TAILQ_HEAD(,pool) pa_list;
	int pa_flags;
#define PA_INITIALIZED	0x01
#define PA_WANT	0x02			/* wakeup any sleeping pools on free */
d72 1
a81 2
	TAILQ_HEAD(,pool_cache)
			pr_cachelist;	/* Caches for this pool */
a109 10
	/*
	 * `pr_slock' protects the pool's data structures when removing
	 * items from or returning items to the pool, or when reading
	 * or updating read/write fields in the pool descriptor.
	 *
	 * We assume back-end page allocators provide their own locking
	 * scheme.  They will be called with the pool descriptor _unlocked_,
	 * since the page allocators may block.
	 */
	struct simplelock	pr_slock;
d118 4
a139 10

	/*
	 * Diagnostic aides.
	 */
	struct pool_log	*pr_log;
	int		pr_curlogentry;
	int		pr_logsize;

	const char	*pr_entered_file; /* reentrancy check */
	long		pr_entered_line;
d148 1
d151 1
a151 1
#ifdef DIAGNOSTIC
d153 5
a157 4
#else
#define pool_setipl(p, i) do { /* nothing */  } while (0)
#endif
void		pool_destroy(struct pool *);
d159 1
a162 13

#ifdef POOL_DIAGNOSTIC
/*
 * These versions do reentrancy checking.
 */
void		*_pool_get(struct pool *, int, const char *, long);
void		_pool_put(struct pool *, void *, const char *, long);
int		_pool_reclaim(struct pool *, const char *, long);
#define		pool_get(h, f)	_pool_get((h), (f), __FILE__, __LINE__)
#define		pool_put(h, v)	_pool_put((h), (v), __FILE__, __LINE__)
#define		pool_reclaim(h)	_pool_reclaim((h), __FILE__, __LINE__)
#endif /* POOL_DIAGNOSTIC */

a163 3
void		pool_setlowat(struct pool *, int);
void		pool_sethiwat(struct pool *, int);
int		pool_sethardlimit(struct pool *, unsigned, const char *, int);
a172 13

/*
 * Pool cache routines.
 */
void		pool_cache_init(struct pool_cache *, struct pool *,
		    int (*ctor)(void *, void *, int),
		    void (*dtor)(void *, void *),
		    void *);
void		pool_cache_destroy(struct pool_cache *);
void		*pool_cache_get(struct pool_cache *, int);
void		pool_cache_put(struct pool_cache *, void *);
void		pool_cache_destruct_object(struct pool_cache *, void *);
void		pool_cache_invalidate(struct pool_cache *);
@


1.23
log
@Clean up some comments.

Don't say that the nointr allocator happens to be interrupt safe since
that's something that callers shouldn't know and it just confuses people.

If you want an interrupt safe allocator you specify NULL as the last
argument to pool_init(). If you want a no interrupt allocator you use
_nointr. The fact that they happen to be the same right now is irrelevant.

pointed out by kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.22 2007/04/23 09:27:59 art Exp $	*/
d151 1
d197 5
@


1.22
log
@Clean up an obsolete allocator.
miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.21 2007/03/21 17:57:40 pedro Exp $	*/
d191 1
a191 1
/* interrupt safe (name preserved for compat) new default allocator */
a192 1
/* previous interrupt safe allocator, allocates from kmem */
@


1.21
log
@typo
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.20 2006/12/23 15:00:15 miod Exp $	*/
a193 1
extern struct pool_allocator pool_allocator_kmem;
@


1.20
log
@When compiling with gcc3, flag pool_get() as a malloc-like function, this
helps optimization.
With feedback from grange@@ and thib@@, ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.19 2006/05/07 20:06:50 tedu Exp $	*/
d67 1
a67 1
			*pc_freeto;	/* grop to free to */
@


1.19
log
@remove drain hooks from pool.
1.  drain hooks and lists of allocators make the code complicated
2.  the only hooks in the system are the mbuf reclaim routines
3.  if reclaim is actually able to put a meaningful amount of memory back
in the system, i think something else is dicked up.  ie, if reclaiming
your ip fragment buffers makes the difference thrashing swap and not,
your system is in a load of trouble.
4.  it's a scary amount of code running with very weird spl requirements
and i'd say it's pretty much totally untested.  raise your hand if your
router is running at the edge of swap.
5.  the reclaim stuff goes back to when mbufs lived in a tiny vm_map and
you could run out of va.  that's very unlikely (like impossible) now.
ok/tested pedro krw sturm
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.18 2004/07/29 09:18:17 mickey Exp $	*/
d200 1
a200 1
void		*pool_get(struct pool *, int);
@


1.18
log
@proper condition for freeing a page and fix a comment appropriately; art@@ tedu@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.17 2004/07/20 23:47:08 art Exp $	*/
a185 2
	void		(*pr_drain_hook)(void *, int);
	void		*pr_drain_hook_arg;
a195 3
int		pool_allocator_drain(struct pool_allocator *, struct pool *,
		    int);

a199 3
void		pool_set_drain_hook(struct pool *, void (*)(void *, int),
		    void *);

a219 1
void		pool_drain(void *);
@


1.17
log
@ifdef DDB a few functions only used (or usable) from DDB.
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.16 2004/06/02 22:17:22 tedu Exp $	*/
d118 1
a118 1
	unsigned int	pr_maxpages;	/* maximum # of pages to keep */
@


1.16
log
@rearrange the allocators we provide for general use.
the new one remains the default and _nointr.
_kmem is restored to its former position, and _oldnointr is
introduced.
this is to allow some pool users who don't like the new allocator
to continue working.  testing/ok beck@@ cedric@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.15 2003/11/18 06:08:18 tedu Exp $	*/
d230 1
a233 1
void		pool_print(struct pool *, const char *);
d237 1
@


1.15
log
@faster pools.  split pagelist into full, partial, and empty so we find what
we're looking for.  change small page_header hash table to a splay tree.
from Chuck Silvers.
tested by brad grange henning mcbride naddy otto
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.14 2002/12/20 07:48:01 art Exp $	*/
d191 3
a193 4
/*
 * Alternate pool page allocator, provided for pools that know they
 * will never be accessed in interrupt context.
 */
d195 1
a195 1
/* Standard pool allocator, provided here for reference. */
@


1.14
log
@ - Clean up the defines in pool.h
 - Allow a pool to be initialized with PR_DEBUG which will cause it to
   allocate with malloc_debug.
 - sprinkle some splassert.
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.13 2002/11/24 18:43:33 pb Exp $	*/
d57 1
a57 2

#define PR_HASHTABSIZE		8
d99 2
d104 6
a109 2
	TAILQ_HEAD(,pool_item_header)
			pr_pagelist;	/* Allocated pages */
d152 1
a152 2
	LIST_HEAD(,pool_item_header)		/* Off-page page headers */
			pr_hashtab[PR_HASHTABSIZE];
@


1.13
log
@there is no spoon^Wopt_pool.h

miod@@, millert@@ ok (:art: I'll fix it post 3.2.)
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.12 2002/03/09 01:11:11 art Exp $	*/
d127 8
a134 7
#define PR_MALLOCOK	1
#define	PR_NOWAIT	0		/* for symmetry */
#define PR_WAITOK	2
#define PR_WANTED	4
#define PR_PHINPAGE	64
#define PR_LOGGING	128
#define PR_LIMITFAIL	256	/* even if waiting, fail if we hit limit */
@


1.12
log
@Fix POOL_DIAGNOSTIC
From Kamil Andrusz <wizz@@mniam.net> pr/2455
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.11 2002/02/25 04:53:16 dhartmei Exp $	*/
a52 4

#if defined(_KERNEL_OPT)
#include "opt_pool.h"
#endif
@


1.11
log
@Make pool_sethardlimit() check that it doesn't decrease the limit below
the current size of the pool. ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.10 2002/02/23 02:52:56 art Exp $	*/
d218 1
a218 1
void		_pool_reclaim(struct pool *, const char *, long);
@


1.10
log
@Move out draining of the allocator to an own function and
let other parts of the kernel call it.
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.9 2002/02/23 01:12:26 art Exp $	*/
d227 1
a227 1
void		pool_sethardlimit(struct pool *, int, const char *, int);
@


1.9
log
@Get rid of __POOL_EXPOSE. The pool needs to be always exposed in the kernel
And there is no point in hiding it in userland.
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.8 2002/01/28 03:23:52 art Exp $	*/
d197 3
@


1.8
log
@GC PR_STATIC and PR_MALLOCOK.
PR_MALLOC wasn't used at all in the code
and PR_STATIC was missing pieces and should be solved with allocators.
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.7 2002/01/25 15:50:23 art Exp $	*/
a53 4
#ifdef _KERNEL
#define	__POOL_EXPOSE
#endif

a57 1
#ifdef __POOL_EXPOSE
a60 1
#endif
a63 1
#ifdef __POOL_EXPOSE
a187 1
#endif /* __POOL_EXPOSE */
@


1.7
log
@Add a drain hook to each pool. This hook is called in three cases.
1. When a pool hit the hard limit. Just before bailing out/sleeping.
2. When an allocator fails to allocate memory (with PR_NOWAIT).
3. Just before trying to reclaim some page in pool_reclaim.

The function called form the hook should try to free some items to the
pool if possible.

Convert m_reclaim hooks that were embedded in MCLGET, MGET and MGETHDR
into a pool drain hook (making the code much cleaner).
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.6 2002/01/23 01:44:20 art Exp $	*/
a141 1
#define PR_STATIC	8
@


1.6
log
@Kill PR_FREEHEADER, not used anymore and confusing.
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.5 2002/01/23 00:39:48 art Exp $	*/
d193 2
d207 2
a208 2
void		pool_init(struct pool *, size_t, u_int, u_int,
				int, const char *, struct pool_allocator *);
d210 3
@


1.5
log
@Pool deals fairly well with physical memory shortage, but it doesn't deal
well (not at all) with shortages of the vm_map where the pages are mapped
(usually kmem_map).

Try to deal with it:
 - group all information the backend allocator for a pool in a separate
   struct. The pool will only have a pointer to that struct.
 - change the pool_init API to reflect that.
 - link all pools allocating from the same allocator on a linked list.
 - Since an allocator is responsible to wait for physical memory it will
   only fail (waitok) when it runs out of its backing vm_map, carefully
   drain pools using the same allocator so that va space is freed.
   (see comments in code for caveats and details).
 - change pool_reclaim to return if it actually succeeded to free some
   memory, use that information to make draining easier and more efficient.
 - get rid of PR_URGENT, noone uses it.
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.4 2001/06/24 16:00:46 art Exp $	*/
a142 1
#define PR_FREEHEADER	16
@


1.4
log
@Add a sysctl for getting pool information out of the kernel.
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.3 2001/06/23 16:13:01 art Exp $	*/
d96 15
a125 3
	unsigned int	pr_pagesz;	/* page size, must be 2^n */
	unsigned long	pr_pagemask;	/* abbrev. of above */
	unsigned int	pr_pageshift;	/* shift corr. to above */
d133 2
a134 3
	void		*(*pr_alloc)(unsigned long, int, int);
	void		(*pr_free)(void *, unsigned long, int);
	int		pr_mtype;	/* memory allocator tag */
a143 1
#define PR_URGENT	32
d198 8
d207 1
a207 4
				 int, const char *, size_t,
				 void *(*)__P((unsigned long, int, int)),
				 void  (*)__P((void *, unsigned long, int)),
				 int);
d212 1
a212 1
void		pool_reclaim(struct pool *);
a238 7

/*
 * Alternate pool page allocator, provided for pools that know they
 * will never be accessed in interrupt context.
 */
void		*pool_page_alloc_nointr(unsigned long, int, int);
void		pool_page_free_nointr(void *, unsigned long, int);
@


1.4.4.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.8 2002/01/28 03:23:52 art Exp $	*/
a95 15
struct pool_allocator {
	void *(*pa_alloc)(struct pool *, int);
	void (*pa_free)(struct pool *, void *);
	int pa_pagesz;

	/* The following fields are for internal use only */
	struct simplelock pa_slock;
	TAILQ_HEAD(,pool) pa_list;
	int pa_flags;
#define PA_INITIALIZED	0x01
#define PA_WANT	0x02			/* wakeup any sleeping pools on free */
	int pa_pagemask;
	int pa_pageshift;
};

d111 3
d121 3
a123 2
	struct pool_allocator *pr_alloc;/* backend allocator */
	TAILQ_ENTRY(pool) pr_alloc_list;/* list of pools using this allocator */
d131 3
a183 2
	void		(*pr_drain_hook)(void *, int);
	void		*pr_drain_hook_arg;
d188 5
a192 10
/*
 * Alternate pool page allocator, provided for pools that know they
 * will never be accessed in interrupt context.
 */
extern struct pool_allocator pool_allocator_nointr;
/* Standard pool allocator, provided here for reference. */
extern struct pool_allocator pool_allocator_kmem;

void		pool_init(struct pool *, size_t, u_int, u_int, int,
		    const char *, struct pool_allocator *);
a194 3
void		pool_set_drain_hook(struct pool *, void (*)(void *, int),
		    void *);

d197 1
a197 1
int		pool_reclaim(struct pool *);
d224 7
@


1.4.4.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.4.4.1 2002/01/31 22:55:49 niklas Exp $	*/
d54 4
d62 1
d66 1
d70 1
d195 1
a205 3
int		pool_allocator_drain(struct pool_allocator *, struct pool *,
		    int);

d223 1
a223 1
int		_pool_reclaim(struct pool *, const char *, long);
d232 1
a232 1
int		pool_sethardlimit(struct pool *, unsigned, const char *, int);
@


1.4.4.3
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d54 4
d131 7
a137 8
#define PR_MALLOCOK	0x01
#define	PR_NOWAIT	0x00		/* for symmetry */
#define PR_WAITOK	0x02
#define PR_WANTED	0x04
#define PR_PHINPAGE	0x08
#define PR_LOGGING	0x10
#define PR_LIMITFAIL	0x20	/* even if waiting, fail if we hit limit */
#define PR_DEBUG	0x40
@


1.3
log
@Bring in a bunch of improvements from NetBSD.

 - pool_cache similar to the slab allocator in Solaris.
 - clean up locking a bit.
 - Don't pass __LINE__ and __FILE__ to pool_get and pool_put unless
   POOL_DIAGNOSTIC is defined.
@
text
@d1 1
a1 1
/*	$OpenBSD: pool.h,v 1.2 2000/12/05 16:43:40 art Exp $	*/
d44 10
d120 1
@


1.2
log
@Bring in fresh pool from NetBSD. Some improvements and fixes.
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: pool.h,v 1.17 2000/02/14 21:17:04 fvdl Exp $	*/
d5 1
a5 1
 * Copyright (c) 1997, 1998, 1999 The NetBSD Foundation, Inc.
d48 4
d60 25
a84 2
struct pool;
typedef struct pool *pool_handle_t;
a85 1
#ifdef __POOL_EXPOSE
d92 2
d110 2
a111 2
	void		*(*pr_alloc) __P((unsigned long, int, int));
	void		(*pr_free) __P((void *, unsigned long, int));
d177 1
a177 1
pool_handle_t	pool_create __P((size_t, u_int, u_int,
d181 6
a186 7
				 int));
void		pool_init __P((struct pool *, size_t, u_int, u_int,
				 int, const char *, size_t,
				 void *(*)__P((unsigned long, int, int)),
				 void  (*)__P((void *, unsigned long, int)),
				 int));
void		pool_destroy __P((pool_handle_t));
d188 1
d190 1
a190 1
 * These routines do reentrancy checking.
d192 3
a194 3
void		*_pool_get __P((pool_handle_t, int, const char *, long));
void		_pool_put __P((pool_handle_t, void *, const char *, long));
void		_pool_reclaim __P((pool_handle_t, const char *, long));
d198 1
d200 5
a204 6
int		pool_prime __P((pool_handle_t, int, caddr_t));
void		pool_setlowat __P((pool_handle_t, int));
void		pool_sethiwat __P((pool_handle_t, int));
void		pool_sethardlimit __P((pool_handle_t, int, const char *, int));
void		pool_reclaim __P((pool_handle_t));
void		pool_drain __P((void *));
d209 4
a212 4
void		pool_print __P((struct pool *, const char *));
void		pool_printit __P((struct pool *, const char *,
		    int (*)(const char *, ...)));
int		pool_chk __P((struct pool *, char *));
d218 15
a232 2
void		*pool_page_alloc_nointr __P((unsigned long, int, int));
void		pool_page_free_nointr __P((void *, unsigned long, int));
@


1.1
log
@pool allocator from NetBSD (stays until uvm is ready)
@
text
@d2 1
a2 1
/*	$NetBSD: pool.h,v 1.12 1998/12/27 21:13:43 thorpej Exp $	*/
d5 1
a5 1
 * Copyright (c) 1997, 1998 The NetBSD Foundation, Inc.
d9 2
a10 1
 * by Paul Kranenburg.
d44 5
d51 2
d56 5
a60 1
typedef struct pool {
d78 4
d85 3
a87 2
	char		*pr_wchan;	/* tsleep(9) identifier */
	unsigned int	pr_flags;
d97 1
d100 7
a106 5
	 * `pr_lock' protects the pool's data structures when removing
	 * items from or returning items to the pool.
	 * `pr_resourcelock' is used to serialize access to the pool's
	 * back-end page allocator. At the same time it also protects
	 * the `pr_maxpages', `pr_minpages' and `pr_minitems' fields.
d108 1
a108 2
	struct simplelock	pr_lock;
	struct lock		pr_resourcelock;
d118 8
d136 3
a138 1
#ifdef POOL_DIAGNOSTIC
a141 2
#endif
} *pool_handle_t;
d143 6
d150 1
a150 1
				 int, char *, size_t,
d155 1
a155 1
				 int, char *, size_t,
d160 4
a163 1
#ifdef POOL_DIAGNOSTIC
d166 1
d169 2
a170 4
#else
void		*pool_get __P((pool_handle_t, int));
void		pool_put __P((pool_handle_t, void *));
#endif
d174 1
a174 1
void		pool_print __P((pool_handle_t, char *));
d177 7
a183 2
#if defined(POOL_DIAGNOSTIC) || defined(DEBUG)
void		pool_print __P((struct pool *, char *));
a184 1
#endif
d192 1
@


1.1.6.1
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 2
a2 2
/*	$OpenBSD: pool.h,v 1.2 2000/12/05 16:43:40 art Exp $	*/
/*	$NetBSD: pool.h,v 1.17 2000/02/14 21:17:04 fvdl Exp $	*/
d5 1
a5 1
 * Copyright (c) 1997, 1998, 1999 The NetBSD Foundation, Inc.
d9 1
a9 2
 * by Paul Kranenburg; by Jason R. Thorpe of the Numerical Aerospace
 * Simulation Facility, NASA Ames Research Center.
a42 5
#ifdef _KERNEL
#define	__POOL_EXPOSE
#endif

#ifdef __POOL_EXPOSE
a44 2
#include <sys/time.h>
#endif
d48 1
a48 5
struct pool;
typedef struct pool *pool_handle_t;

#ifdef __POOL_EXPOSE
struct pool {
a65 4
	unsigned int	pr_nitems;	/* number of available items in pool */
	unsigned int	pr_nout;	/* # items currently allocated */
	unsigned int	pr_hardlimit;	/* hard limit to number of allocated
					   items */
d69 2
a70 3
	const char	*pr_wchan;	/* tsleep(9) identifier */
	unsigned int	pr_flags;	/* r/w flags */
	unsigned int	pr_roflags;	/* r/o flags */
a79 1
#define PR_LIMITFAIL	256	/* even if waiting, fail if we hit limit */
d82 5
a86 7
	 * `pr_slock' protects the pool's data structures when removing
	 * items from or returning items to the pool, or when reading
	 * or updating read/write fields in the pool descriptor.
	 *
	 * We assume back-end page allocators provide their own locking
	 * scheme.  They will be called with the pool descriptor _unlocked_,
	 * since the page allocators may block.
d88 2
a89 1
	struct simplelock	pr_slock;
a98 8
	 * Warning message to be issued, and a per-time-delta rate cap,
	 * if the hard limit is reached.
	 */
	const char	*pr_hardlimit_warning;
	struct timeval	pr_hardlimit_ratecap;
	struct timeval	pr_hardlimit_warning_last;

	/*
d109 1
a109 3
	/*
	 * Diagnostic aides.
	 */
d113 2
a115 6
	const char	*pr_entered_file; /* reentrancy check */
	long		pr_entered_line;
};
#endif /* __POOL_EXPOSE */

#ifdef _KERNEL
d117 1
a117 1
				 int, const char *, size_t,
d122 1
a122 1
				 int, const char *, size_t,
d127 1
a127 4

/*
 * These routines do reentrancy checking.
 */
a129 1
void		_pool_reclaim __P((pool_handle_t, const char *, long));
d132 4
a135 2
#define		pool_reclaim(h)	_pool_reclaim((h), __FILE__, __LINE__)

d139 1
a139 1
void		pool_sethardlimit __P((pool_handle_t, int, const char *, int));
d142 2
a143 7

/*
 * Debugging and diagnostic aides.
 */
void		pool_print __P((struct pool *, const char *));
void		pool_printit __P((struct pool *, const char *,
		    int (*)(const char *, ...)));
d145 1
a152 1
#endif /* _KERNEL */
@


1.1.6.2
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: pool.h,v 1.27 2001/06/06 22:00:17 rafal Exp $	*/
d5 1
a5 1
 * Copyright (c) 1997, 1998, 1999, 2000 The NetBSD Foundation, Inc.
a43 10
/*
 * sysctls.
 * kern.pool.npools
 * kern.pool.name.<number>
 * kern.pool.pool.<number>
 */
#define KERN_POOL_NPOOLS	1
#define KERN_POOL_NAME		2
#define KERN_POOL_POOL		3

a47 4
#if defined(_KERNEL_OPT)
#include "opt_pool.h"
#endif

d56 3
a59 25
struct pool_cache {
	TAILQ_ENTRY(pool_cache)
			pc_poollist;	/* entry on pool's group list */
	TAILQ_HEAD(, pool_cache_group)
			pc_grouplist;	/* Cache group list */
	struct pool_cache_group
			*pc_allocfrom;	/* group to allocate from */
	struct pool_cache_group
			*pc_freeto;	/* grop to free to */
	struct pool	*pc_pool;	/* parent pool */
	struct simplelock pc_slock;	/* mutex */

	int		(*pc_ctor)(void *, void *, int);
	void		(*pc_dtor)(void *, void *);
	void		*pc_arg;

	/* Statistics. */
	unsigned long	pc_hits;	/* cache hits */
	unsigned long	pc_misses;	/* cache misses */

	unsigned long	pc_ngroups;	/* # cache groups */

	unsigned long	pc_nitems;	/* # objects currently in cache */
};

a65 2
	TAILQ_HEAD(,pool_cache)
			pr_cachelist;	/* Caches for this pool */
d82 2
a83 3
	unsigned int	pr_serial;	/* unique serial number of the pool */
	void		*(*pr_alloc)(unsigned long, int, int);
	void		(*pr_free)(void *, unsigned long, int);
d149 6
a154 1
void		pool_init(struct pool *, size_t, u_int, u_int,
d158 2
a159 2
				 int);
void		pool_destroy(struct pool *);
a160 5
void		*pool_get(struct pool *, int);
void		pool_put(struct pool *, void *);
void		pool_reclaim(struct pool *);

#ifdef POOL_DIAGNOSTIC
d162 1
a162 1
 * These versions do reentrancy checking.
d164 3
a166 3
void		*_pool_get(struct pool *, int, const char *, long);
void		_pool_put(struct pool *, void *, const char *, long);
void		_pool_reclaim(struct pool *, const char *, long);
a169 1
#endif /* POOL_DIAGNOSTIC */
d171 6
a176 5
int		pool_prime(struct pool *, int);
void		pool_setlowat(struct pool *, int);
void		pool_sethiwat(struct pool *, int);
void		pool_sethardlimit(struct pool *, int, const char *, int);
void		pool_drain(void *);
d181 4
a184 4
void		pool_print(struct pool *, const char *);
void		pool_printit(struct pool *, const char *,
		    int (*)(const char *, ...));
int		pool_chk(struct pool *, const char *);
d190 2
a191 15
void		*pool_page_alloc_nointr(unsigned long, int, int);
void		pool_page_free_nointr(void *, unsigned long, int);

/*
 * Pool cache routines.
 */
void		pool_cache_init(struct pool_cache *, struct pool *,
		    int (*ctor)(void *, void *, int),
		    void (*dtor)(void *, void *),
		    void *);
void		pool_cache_destroy(struct pool_cache *);
void		*pool_cache_get(struct pool_cache *, int);
void		pool_cache_put(struct pool_cache *, void *);
void		pool_cache_destruct_object(struct pool_cache *, void *);
void		pool_cache_invalidate(struct pool_cache *);
@


1.1.6.3
log
@Merge in trunk
@
text
@d54 4
d62 1
d66 1
d70 1
a95 15
struct pool_allocator {
	void *(*pa_alloc)(struct pool *, int);
	void (*pa_free)(struct pool *, void *);
	int pa_pagesz;

	/* The following fields are for internal use only */
	struct simplelock pa_slock;
	TAILQ_HEAD(,pool) pa_list;
	int pa_flags;
#define PA_INITIALIZED	0x01
#define PA_WANT	0x02			/* wakeup any sleeping pools on free */
	int pa_pagemask;
	int pa_pageshift;
};

d111 3
d121 3
a123 2
	struct pool_allocator *pr_alloc;/* backend allocator */
	TAILQ_ENTRY(pool) pr_alloc_list;/* list of pools using this allocator */
d131 3
a183 2
	void		(*pr_drain_hook)(void *, int);
	void		*pr_drain_hook_arg;
d185 1
d188 5
a192 13
/*
 * Alternate pool page allocator, provided for pools that know they
 * will never be accessed in interrupt context.
 */
extern struct pool_allocator pool_allocator_nointr;
/* Standard pool allocator, provided here for reference. */
extern struct pool_allocator pool_allocator_kmem;

int		pool_allocator_drain(struct pool_allocator *, struct pool *,
		    int);

void		pool_init(struct pool *, size_t, u_int, u_int, int,
		    const char *, struct pool_allocator *);
a194 3
void		pool_set_drain_hook(struct pool *, void (*)(void *, int),
		    void *);

d197 1
a197 1
int		pool_reclaim(struct pool *);
d214 1
a214 1
int		pool_sethardlimit(struct pool *, unsigned, const char *, int);
d224 7
@


1.1.6.4
log
@Merge in -current from roughly a week ago
@
text
@d218 1
a218 1
int		_pool_reclaim(struct pool *, const char *, long);
@


1.1.6.5
log
@Sync the SMP branch with 3.3
@
text
@d54 4
d131 7
a137 8
#define PR_MALLOCOK	0x01
#define	PR_NOWAIT	0x00		/* for symmetry */
#define PR_WAITOK	0x02
#define PR_WANTED	0x04
#define PR_PHINPAGE	0x08
#define PR_LOGGING	0x10
#define PR_LIMITFAIL	0x20	/* even if waiting, fail if we hit limit */
#define PR_DEBUG	0x40
@


1.1.6.6
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d57 2
a58 1
#include <sys/tree.h>
a99 2
LIST_HEAD(pool_pagelist,pool_item_header);

d103 2
a104 6
	struct pool_pagelist
			pr_emptypages;	/* Empty pages */
	struct pool_pagelist
			pr_fullpages;	/* Full pages */
	struct pool_pagelist
			pr_partpages;	/* Partially-allocated pages */
d147 2
a148 1
	SPLAY_HEAD(phtree, pool_item_header) pr_phtree;
@


1.1.6.7
log
@Merge with the trunk
@
text
@d191 4
a194 3
/* old nointr allocator, still needed for large allocations */
extern struct pool_allocator pool_allocator_oldnointr;
/* interrupt safe (name preserved for compat) new default allocator */
d196 1
a196 1
/* previous interrupt safe allocator, allocates from kmem */
@


