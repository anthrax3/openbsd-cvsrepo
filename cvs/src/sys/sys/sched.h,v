head	1.42;
access;
symbols
	OPENBSD_6_2:1.42.0.2
	OPENBSD_6_2_BASE:1.42
	OPENBSD_6_1:1.42.0.4
	OPENBSD_6_1_BASE:1.42
	OPENBSD_6_0:1.41.0.2
	OPENBSD_6_0_BASE:1.41
	OPENBSD_5_9:1.39.0.2
	OPENBSD_5_9_BASE:1.39
	OPENBSD_5_8:1.38.0.6
	OPENBSD_5_8_BASE:1.38
	OPENBSD_5_7:1.38.0.2
	OPENBSD_5_7_BASE:1.38
	OPENBSD_5_6:1.35.0.4
	OPENBSD_5_6_BASE:1.35
	OPENBSD_5_5:1.34.0.4
	OPENBSD_5_5_BASE:1.34
	OPENBSD_5_4:1.33.0.2
	OPENBSD_5_4_BASE:1.33
	OPENBSD_5_3:1.30.0.6
	OPENBSD_5_3_BASE:1.30
	OPENBSD_5_2:1.30.0.4
	OPENBSD_5_2_BASE:1.30
	OPENBSD_5_1_BASE:1.30
	OPENBSD_5_1:1.30.0.2
	OPENBSD_5_0:1.29.0.2
	OPENBSD_5_0_BASE:1.29
	OPENBSD_4_9:1.28.0.4
	OPENBSD_4_9_BASE:1.28
	OPENBSD_4_8:1.28.0.2
	OPENBSD_4_8_BASE:1.28
	OPENBSD_4_7:1.25.0.2
	OPENBSD_4_7_BASE:1.25
	OPENBSD_4_6:1.22.0.4
	OPENBSD_4_6_BASE:1.22
	OPENBSD_4_5:1.19.0.4
	OPENBSD_4_5_BASE:1.19
	OPENBSD_4_4:1.19.0.2
	OPENBSD_4_4_BASE:1.19
	OPENBSD_4_3:1.18.0.2
	OPENBSD_4_3_BASE:1.18
	OPENBSD_4_2:1.16.0.2
	OPENBSD_4_2_BASE:1.16
	OPENBSD_4_1:1.14.0.2
	OPENBSD_4_1_BASE:1.14
	OPENBSD_4_0:1.13.0.6
	OPENBSD_4_0_BASE:1.13
	OPENBSD_3_9:1.13.0.4
	OPENBSD_3_9_BASE:1.13
	OPENBSD_3_8:1.13.0.2
	OPENBSD_3_8_BASE:1.13
	OPENBSD_3_7:1.10.0.4
	OPENBSD_3_7_BASE:1.10
	OPENBSD_3_6:1.10.0.2
	OPENBSD_3_6_BASE:1.10
	SMP_SYNC_A:1.5
	SMP_SYNC_B:1.5
	OPENBSD_3_5:1.4.0.4
	OPENBSD_3_5_BASE:1.4
	OPENBSD_3_4:1.4.0.2
	OPENBSD_3_4_BASE:1.4
	UBC_SYNC_A:1.3
	OPENBSD_3_3:1.3.0.6
	OPENBSD_3_3_BASE:1.3
	OPENBSD_3_2:1.3.0.4
	OPENBSD_3_2_BASE:1.3
	OPENBSD_3_1:1.3.0.2
	OPENBSD_3_1_BASE:1.3
	UBC_SYNC_B:1.3
	UBC:1.2.0.4
	UBC_BASE:1.2
	OPENBSD_3_0:1.2.0.2
	OPENBSD_3_0_BASE:1.2
	OPENBSD_2_9_BASE:1.1
	OPENBSD_2_9:1.1.0.10
	OPENBSD_2_8:1.1.0.8
	OPENBSD_2_8_BASE:1.1
	OPENBSD_2_7:1.1.0.6
	OPENBSD_2_7_BASE:1.1
	SMP:1.1.0.4
	SMP_BASE:1.1
	kame_19991208:1.1
	OPENBSD_2_6:1.1.0.2
	OPENBSD_2_6_BASE:1.1;
locks; strict;
comment	@ * @;


1.42
date	2017.02.14.10.31.15;	author mpi;	state Exp;
branches;
next	1.41;
commitid	PmGi4EGraGC0Z0ml;

1.41
date	2016.03.17.13.18.47;	author mpi;	state Exp;
branches;
next	1.40;
commitid	YYXwHgvcvZhstSh4;

1.40
date	2016.03.09.13.38.50;	author mpi;	state Exp;
branches;
next	1.39;
commitid	THpTza9IRRtZUKne;

1.39
date	2015.09.13.11.15.11;	author kettenis;	state Exp;
branches;
next	1.38;
commitid	FyAroUyNFmSVBNGF;

1.38
date	2015.01.11.19.34.52;	author guenther;	state Exp;
branches;
next	1.37;
commitid	mYYjUBRhGJS1wXIo;

1.37
date	2014.10.17.01.51.39;	author tedu;	state Exp;
branches;
next	1.36;
commitid	r6zxCLSQFWXeVG73;

1.36
date	2014.09.09.07.07.39;	author blambert;	state Exp;
branches;
next	1.35;
commitid	R0IvGgmM8zlXVXKS;

1.35
date	2014.03.29.18.09.31;	author guenther;	state Exp;
branches;
next	1.34;

1.34
date	2014.01.30.20.14.27;	author miod;	state Exp;
branches;
next	1.33;

1.33
date	2013.06.04.22.17.34;	author tedu;	state Exp;
branches;
next	1.32;

1.32
date	2013.06.04.22.16.23;	author tedu;	state Exp;
branches;
next	1.31;

1.31
date	2013.06.03.16.55.22;	author guenther;	state Exp;
branches;
next	1.30;

1.30
date	2011.11.16.20.50.19;	author deraadt;	state Exp;
branches;
next	1.29;

1.29
date	2011.07.07.18.00.33;	author guenther;	state Exp;
branches;
next	1.28;

1.28
date	2010.05.14.18.47.56;	author kettenis;	state Exp;
branches;
next	1.27;

1.27
date	2010.04.23.03.50.22;	author miod;	state Exp;
branches;
next	1.26;

1.26
date	2010.04.06.20.33.28;	author kettenis;	state Exp;
branches;
next	1.25;

1.25
date	2010.01.09.02.44.17;	author kettenis;	state Exp;
branches;
next	1.24;

1.24
date	2009.11.29.23.12.30;	author kettenis;	state Exp;
branches;
next	1.23;

1.23
date	2009.11.25.11.01.14;	author kettenis;	state Exp;
branches;
next	1.22;

1.22
date	2009.04.14.09.13.25;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2009.04.03.09.29.15;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2009.03.23.13.25.11;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2008.06.26.05.42.20;	author ray;	state Exp;
branches;
next	1.18;

1.18
date	2007.10.11.10.34.08;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2007.10.10.15.53.53;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2007.05.18.14.41.55;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2007.05.16.17.27.31;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2007.02.03.16.48.23;	author miod;	state Exp;
branches;
next	1.13;

1.13
date	2005.06.17.22.33.34;	author niklas;	state Exp;
branches;
next	1.12;

1.12
date	2005.05.29.03.20.42;	author deraadt;	state Exp;
branches;
next	1.11;

1.11
date	2005.05.25.23.17.47;	author niklas;	state Exp;
branches;
next	1.10;

1.10
date	2004.06.22.01.16.50;	author art;	state Exp;
branches;
next	1.9;

1.9
date	2004.06.21.23.12.14;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2004.06.20.08.25.30;	author deraadt;	state Exp;
branches;
next	1.7;

1.7
date	2004.06.20.06.47.31;	author art;	state Exp;
branches;
next	1.6;

1.6
date	2004.06.13.21.49.28;	author niklas;	state Exp;
branches;
next	1.5;

1.5
date	2004.06.09.20.18.28;	author art;	state Exp;
branches;
next	1.4;

1.4
date	2003.06.02.23.28.21;	author millert;	state Exp;
branches;
next	1.3;

1.3
date	2002.03.14.01.27.14;	author millert;	state Exp;
branches;
next	1.2;

1.2
date	2001.08.19.05.44.27;	author art;	state Exp;
branches
	1.2.4.1;
next	1.1;

1.1
date	99.08.15.00.07.50;	author pjanzen;	state Exp;
branches
	1.1.4.1;
next	;

1.1.4.1
date	2001.07.14.10.02.47;	author ho;	state Exp;
branches;
next	1.1.4.2;

1.1.4.2
date	2001.10.31.03.30.30;	author nate;	state Exp;
branches;
next	1.1.4.3;

1.1.4.3
date	2002.03.28.14.52.01;	author niklas;	state Exp;
branches;
next	1.1.4.4;

1.1.4.4
date	2003.05.15.04.08.03;	author niklas;	state Exp;
branches;
next	1.1.4.5;

1.1.4.5
date	2003.05.18.17.41.16;	author niklas;	state Exp;
branches;
next	1.1.4.6;

1.1.4.6
date	2003.06.07.11.09.08;	author ho;	state Exp;
branches;
next	1.1.4.7;

1.1.4.7
date	2004.02.21.02.49.00;	author niklas;	state Exp;
branches;
next	1.1.4.8;

1.1.4.8
date	2004.03.14.22.08.21;	author niklas;	state Exp;
branches;
next	1.1.4.9;

1.1.4.9
date	2004.06.05.17.19.55;	author niklas;	state Exp;
branches;
next	1.1.4.10;

1.1.4.10
date	2004.06.06.23.31.37;	author niklas;	state Exp;
branches;
next	1.1.4.11;

1.1.4.11
date	2004.06.06.23.36.26;	author deraadt;	state Exp;
branches;
next	1.1.4.12;

1.1.4.12
date	2004.06.10.11.40.35;	author niklas;	state Exp;
branches;
next	;

1.2.4.1
date	2002.06.11.03.32.33;	author art;	state Exp;
branches;
next	;


desc
@@


1.42
log
@Convert most of the manual checks for CPU hogging to sched_pause().

The distinction between preempt() and yield() stays as it is usueful
to know if a thread decided to yield by itself or if the kernel told
him to go away.

ok tedu@@, guenther@@
@
text
@/*	$OpenBSD: sched.h,v 1.41 2016/03/17 13:18:47 mpi Exp $	*/
/* $NetBSD: sched.h,v 1.2 1999/02/28 18:14:58 ross Exp $ */

/*-
 * Copyright (c) 1999 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Ross Harvey.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

/*-
 * Copyright (c) 1982, 1986, 1991, 1993
 *	The Regents of the University of California.  All rights reserved.
 * (c) UNIX System Laboratories, Inc.
 * All or some portions of this file are derived from material licensed
 * to the University of California by American Telephone and Telegraph
 * Co. or Unix System Laboratories, Inc. and are reproduced herein with
 * the permission of UNIX System Laboratories, Inc.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)kern_clock.c	8.5 (Berkeley) 1/21/94
 */

#ifndef	_SYS_SCHED_H_
#define	_SYS_SCHED_H_

#include <sys/queue.h>

/*
 * Posix defines a <sched.h> which may want to include <sys/sched.h>
 */

/*
 * CPU states.
 * XXX Not really scheduler state, but no other good place to put
 * it right now, and it really is per-CPU.
 */
#define CP_USER		0
#define CP_NICE		1
#define CP_SYS		2
#define CP_INTR		3
#define CP_IDLE		4
#define CPUSTATES	5

#define	SCHED_NQS	32			/* 32 run queues. */

/*
 * Per-CPU scheduler state.
 * XXX - expose to userland for now.
 */
struct schedstate_percpu {
	struct timespec spc_runtime;	/* time curproc started running */
	volatile int spc_schedflags;	/* flags; see below */
	u_int spc_schedticks;		/* ticks for schedclock() */
	u_int64_t spc_cp_time[CPUSTATES]; /* CPU state statistics */
	u_char spc_curpriority;		/* usrpri of curproc */
	int spc_rrticks;		/* ticks until roundrobin() */
	int spc_pscnt;			/* prof/stat counter */
	int spc_psdiv;			/* prof/stat divisor */	
	struct proc *spc_idleproc;	/* idle proc for this cpu */

	u_int spc_nrun;			/* procs on the run queues */
	fixpt_t spc_ldavg;		/* shortest load avg. for this cpu */

	TAILQ_HEAD(prochead, proc) spc_qs[SCHED_NQS];
	volatile uint32_t spc_whichqs;

#ifdef notyet
	struct proc *spc_reaper;	/* dead proc reaper */
#endif
	LIST_HEAD(,proc) spc_deadproc;

	volatile int spc_barrier;	/* for sched_barrier() */
};

#ifdef	_KERNEL

/* spc_flags */
#define SPCF_SEENRR             0x0001  /* process has seen roundrobin() */
#define SPCF_SHOULDYIELD        0x0002  /* process should yield the CPU */
#define SPCF_SWITCHCLEAR        (SPCF_SEENRR|SPCF_SHOULDYIELD)
#define SPCF_SHOULDHALT		0x0004	/* CPU should be vacated */
#define SPCF_HALTED		0x0008	/* CPU has been halted */

#define	SCHED_PPQ	(128 / SCHED_NQS)	/* priorities per queue */
#define NICE_WEIGHT 2			/* priorities per nice level */
#define	ESTCPULIM(e) min((e), NICE_WEIGHT * PRIO_MAX - SCHED_PPQ)

extern int schedhz;			/* ideally: 16 */
extern int rrticks_init;		/* ticks per roundrobin() */

struct proc;
void schedclock(struct proc *);
struct cpu_info;
void roundrobin(struct cpu_info *);
void scheduler_start(void);
void userret(struct proc *p);

void sched_init_cpu(struct cpu_info *);
void sched_idle(void *);
void sched_exit(struct proc *);
void mi_switch(void);
void cpu_switchto(struct proc *, struct proc *);
struct proc *sched_chooseproc(void);
struct cpu_info *sched_choosecpu(struct proc *);
struct cpu_info *sched_choosecpu_fork(struct proc *parent, int);
void cpu_idle_enter(void);
void cpu_idle_cycle(void);
void cpu_idle_leave(void);
void sched_peg_curproc(struct cpu_info *ci);
void sched_barrier(struct cpu_info *ci);

int sysctl_hwsetperf(void *, size_t *, void *, size_t);
int sysctl_hwperfpolicy(void *, size_t *, void *, size_t);

#ifdef MULTIPROCESSOR
void sched_start_secondary_cpus(void);
void sched_stop_secondary_cpus(void);
#endif

#define cpu_is_idle(ci)	((ci)->ci_schedstate.spc_whichqs == 0)

void sched_init_runqueues(void);
void setrunqueue(struct proc *);
void remrunqueue(struct proc *);

/* Inherit the parent's scheduler history */
#define scheduler_fork_hook(parent, child) do {				\
	(child)->p_estcpu = (parent)->p_estcpu;				\
} while (0)

/* Chargeback parents for the sins of their children.  */
#define scheduler_wait_hook(parent, child) do {				\
	(parent)->p_estcpu = ESTCPULIM((parent)->p_estcpu + (child)->p_estcpu);\
} while (0)

/* Allow other processes to progress */
#define	sched_pause(func) do {						\
	if (curcpu()->ci_schedstate.spc_schedflags & SPCF_SHOULDYIELD)	\
		func();							\
} while (0)

#if defined(MULTIPROCESSOR)
#include <sys/lock.h>

/*
 * XXX Instead of using struct lock for the kernel lock and thus requiring us
 * XXX to implement simplelocks, causing all sorts of fine-grained locks all
 * XXX over our tree to be activated, the sched_lock is a different kind of
 * XXX lock to avoid introducing locking protocol bugs.
 */
extern struct __mp_lock sched_lock;

#define	SCHED_ASSERT_LOCKED()						\
do {									\
	splassert(IPL_SCHED);						\
	KASSERT(__mp_lock_held(&sched_lock));				\
} while (0)
#define	SCHED_ASSERT_UNLOCKED()	KASSERT(__mp_lock_held(&sched_lock) == 0)

#define	SCHED_LOCK_INIT()	__mp_lock_init(&sched_lock)

#define	SCHED_LOCK(s)							\
do {									\
	s = splsched();							\
	__mp_lock(&sched_lock);						\
} while (/* CONSTCOND */ 0)

#define	SCHED_UNLOCK(s)							\
do {									\
	__mp_unlock(&sched_lock);					\
	splx(s);							\
} while (/* CONSTCOND */ 0)

#else /* ! MULTIPROCESSOR */

#define	SCHED_ASSERT_LOCKED()		splassert(IPL_SCHED);
#define	SCHED_ASSERT_UNLOCKED()		/* nothing */

#define	SCHED_LOCK_INIT()		/* nothing */

#define	SCHED_LOCK(s)			s = splsched()
#define	SCHED_UNLOCK(s)			splx(s)

#endif /* MULTIPROCESSOR */

#endif	/* _KERNEL */
#endif	/* _SYS_SCHED_H_ */
@


1.41
log
@Replace curcpu_is_idle() by cpu_is_idle() and use it instead of rolling
our own.

From Michal Mazurek, ok mmcc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.40 2016/03/09 13:38:50 mpi Exp $	*/
d183 1
a183 1
#define	sched_pause() do {						\
d185 1
a185 1
		yield();						\
@


1.40
log
@Correct some comments and definitions, from Michal Mazurek.
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.39 2015/09/13 11:15:11 kettenis Exp $	*/
d166 1
a166 1
#define curcpu_is_idle()	(curcpu()->ci_schedstate.spc_whichqs == 0)
@


1.39
log
@Introduce sched_barrier(9), an interface that acts as a scheduler barrier in
the sense that it guarantees that the specified CPU went through the
scheduler.  This also guarantees that interrupt handlers running on that CPU
will have finished when sched_barrier() returns.

ok miod@@, guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.38 2015/01/11 19:34:52 guenther Exp $	*/
a141 1
void updatepri(struct proc *);
@


1.38
log
@LOCKDEBUG is dead; perform the funeral rites

pointed out by Helg (xx404 (at) msn.com)
ok deraadt@@ miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.37 2014/10/17 01:51:39 tedu Exp $	*/
d117 2
d157 1
@


1.37
log
@redo the performance throttling in the kernel.
introduce a new sysctl, hw.perfpolicy, that governs the policy.
when set to anything other than manual, hw.setperf then becomes read only.
phessler was heading in this direction, but this is slightly different. :)
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.36 2014/09/09 07:07:39 blambert Exp $	*/
d186 1
a186 1
#if defined(MULTIPROCESSOR) || defined(LOCKDEBUG)
d218 1
a218 1
#else /* ! MULTIPROCESSOR || LOCKDEBUG */
d228 1
a228 1
#endif /* MULTIPROCESSOR || LOCKDEBUG */
@


1.36
log
@Make the cleaner, syncer, pagedaemon, aiodone daemons all
yield() if the cpu is marked SHOULDYIELD.

ok miod@@ tedu@@ phessler@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.35 2014/03/29 18:09:31 guenther Exp $	*/
d155 3
@


1.35
log
@It's been a quarter century: we can assume volatile is present with that name.

ok dlg@@ mpi@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.34 2014/01/30 20:14:27 miod Exp $	*/
d175 6
@


1.34
log
@On MULTIPROCESSOR kernels, have SCHED_ASSERT_LOCKED assert both the lock being
held and ipl being >= IPL_SCHED.
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.33 2013/06/04 22:17:34 tedu Exp $	*/
d98 1
a98 1
	__volatile int spc_schedflags;	/* flags; see below */
@


1.33
log
@even better now text now.
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.32 2013/06/04 22:16:23 tedu Exp $	*/
d188 5
a192 1
#define	SCHED_ASSERT_LOCKED()	KASSERT(__mp_lock_held(&sched_lock))
@


1.32
log
@sentence explaining sched_lock is not a simplelock should be a sentence.
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.31 2013/06/03 16:55:22 guenther Exp $	*/
d183 2
a184 2
 * XXX over our tree getting activated, the sched_lock is a different kind of
 * XXX lock toi avoid introducing locking protocol bugs.
@


1.31
log
@Convert some internal APIs to use timespecs instead of timevals

ok matthew@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.30 2011/11/16 20:50:19 deraadt Exp $	*/
d183 2
a184 2
 * XXX over our tree getting activated consuming both time and potentially
 * XXX introducing locking protocol bugs.
@


1.30
log
@Make userret() MI.  On architectures which jammed stuff into it in the
past, pull that code out seperately.
ok guenther miod
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.29 2011/07/07 18:00:33 guenther Exp $	*/
d97 1
a97 1
	struct timeval spc_runtime;	/* time curproc started running */
@


1.29
log
@Functions used in files other than where they are defined should be
declared in .h files, not in each .c.  Apply that rule to endtsleep(),
scheduler_start(), updatepri(), and realitexpire()

ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.28 2010/05/14 18:47:56 kettenis Exp $	*/
d141 1
@


1.28
log
@Make sure we initialize sched_lock before we try to use it.

ok miod@@, thib@@, oga@@, jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.27 2010/04/23 03:50:22 miod Exp $	*/
d139 2
@


1.27
log
@Make sure IPL_SCHED is always defined by MD headers, instead of having a MI
fallback definition in <sys/sched.h>, so that there is no hidden include
ordering requirement between <machine/intr.h> and <sys/sched.h>.
ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.26 2010/04/06 20:33:28 kettenis Exp $	*/
d188 2
d206 2
@


1.26
log
@Implement functions to take away the secondary CPUs from the scheduler and
give them back again, effectively stopping and starting these CPUs.  Use
the stop function in sys_reboot().

ok marco@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.25 2010/01/09 02:44:17 kettenis Exp $	*/
a172 4

#ifndef IPL_SCHED
#define IPL_SCHED IPL_HIGH
#endif
@


1.25
log
@Add code to stop scheduling processes on CPUs, effectively halting that CPU.
Use this to do a shutdown with only the boot processor running.  This should
avoid nasty races during shutdown.

help from art@@, ok deraadt@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.24 2009/11/29 23:12:30 kettenis Exp $	*/
d141 1
d152 5
@


1.24
log
@Backout previous commit.  There is a possible race which makes it possible
for sys_reboot() to hang forever.
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.22 2009/04/14 09:13:25 art Exp $	*/
d125 2
@


1.23
log
@Add a mechanism to stop the scheduler from scheduling processes on a
particular CPU such that it just sits and spins in the idle loop, effectively
halting that CPU.

ok deraadt@@, miod@@
@
text
@a124 2
#define SPCF_SHOULDHALT		0x0004	/* CPU should be vacated */
#define SPCF_HALTED		0x0008	/* CPU has been halted */
@


1.22
log
@Some tweaks to the cpu affinity code.
 - Split up choosing of cpu between fork and "normal" cases. Fork is
   very different and should be treated as such.
 - Instead of implicitly choosing a cpu in setrunqueue, do it outside
   where it actually makes sense.
 - Just because a cpu is marked as idle doesn't mean it will be soon.
   There could be a thundering herd effect if we call wakeup from an
   interrupt handler, so subtract cpus with queued processes when
   deciding which cpu is actually idle.
 - some simplifications allowed by the above.

kettenis@@ ok (except one bugfix that was not in the intial diff)
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.21 2009/04/03 09:29:15 art Exp $	*/
d125 2
@


1.21
log
@sched_peg_curproc_to_cpu() - function to force a proc to stay on a cpu
forever.
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.20 2009/03/23 13:25:11 art Exp $	*/
d143 2
a144 1
void sched_choosecpu(struct proc *);
@


1.20
log
@Processor affinity for processes.
 - Split up run queues so that every cpu has one.
 - Make setrunqueue choose the cpu where we want to make this process
   runnable (this should be refined and less brutal in the future).
 - When choosing the cpu where we want to run, make some kind of educated
   guess where it will be best to run (very naive right now).
Other:
 - Set operations for sets of cpus.
 - load average calculations per cpu.
 - sched_is_idle() -> curcpu_is_idle()

tested, debugged and prodded by many@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.19 2008/06/26 05:42:20 ray Exp $	*/
d147 1
@


1.19
log
@First pass at removing clauses 3 and 4 from NetBSD licenses.

Not sure what's more surprising: how long it took for NetBSD to
catch up to the rest of the BSDs (including UCB), or the amount of
code that NetBSD has claimed for itself without attributing to the
actual authors.

OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.18 2007/10/11 10:34:08 art Exp $	*/
d90 2
d106 7
d126 1
a126 3

#define	NQS	32			/* 32 run queues. */
#define	PPQ	(128 / NQS)		/* priorities per queue */
d128 1
a128 1
#define	ESTCPULIM(e) min((e), NICE_WEIGHT * PRIO_MAX - PPQ)
d143 1
d148 1
a148 2
extern volatile int sched_whichqs;
#define sched_is_idle()	(sched_whichqs == 0)
@


1.18
log
@sched_lock_idle and sched_unlock_idle are obsolete now.
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.17 2007/10/10 15:53:53 art Exp $	*/
a18 7
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *        This product includes software developed by the NetBSD
 *        Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
@


1.17
log
@Make context switching much more MI:
 - Move the functionality of choosing a process from cpu_switch into
   a much simpler function: cpu_switchto. Instead of having the locore
   code walk the run queues, let the MI code choose the process we
   want to run and only implement the context switching itself in MD
   code.
 - Let MD context switching run without worrying about spls or locks.
 - Instead of having the idle loop implemented with special contexts
   in MD code, implement one idle proc for each cpu. make the idle
   loop MI with MD hooks.
 - Change the proc lists from the old style vax queues to TAILQs.
 - Change the sleep queue from vax queues to TAILQs. This makes
   wakeup() go from O(n^2) to O(n)

there will be some MD fallout, but it will be fixed shortly.
There's also a few cleanups to be done after this.

deraadt@@, kettenis@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.16 2007/05/18 14:41:55 art Exp $	*/
a192 4


void	sched_lock_idle(void);
void	sched_unlock_idle(void);
@


1.16
log
@Instead of checking whichqs directly, add a "sched_is_idle()" macro to
sys/sched.h and use that to check if there's something to do.

kettenis@@ thib@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.15 2007/05/16 17:27:31 art Exp $	*/
d79 2
d110 5
d124 2
d138 15
a152 1
#define sched_is_idle() (whichqs == 0)
@


1.15
log
@The world of __HAVEs and __HAVE_NOTs is reducing. All architectures
have cpu_info now, so kill the option.

eyeballed by jsg@@ and grange@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.14 2007/02/03 16:48:23 miod Exp $	*/
d128 2
@


1.14
log
@Remove unused functionality from lockmgr():
- LK_EXCLUPGRADE is never used.
- LK_REENABLE is never used.
- LK_SETRECURSE is never used. Because of this, the lk_recurselevel
  field is always zero, so it can be removed to.
- the spinlock version (and LK_SPIN) is never used, since it was decided
  to use different locking structure for MP-safe protection.

Tested by many
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.13 2005/06/17 22:33:34 niklas Exp $	*/
a125 1
#ifdef __HAVE_CPUINFO
a127 1
#endif
a138 3
#if !defined(__HAVE_CPUINFO) && !defined(splsched)
#define splsched() splhigh()
#endif
@


1.13
log
@A second approach at fixing the telnet localhost & problem
(but I tend to call it ssh localhost & now when telnetd is
history).  This is more localized patch, but leaves us with
a recursive lock for protecting scheduling and signal state.
Better care is taken to actually be symmetric over mi_switch.
Also, the dolock cruft in psignal can go with this solution.
Better test runs by more people for longer time has been
carried out compared to the c2k5 patch.

Long term the current mess with interruptible sleep, the
default action on stop signals and wakeup interactions need
to be revisited.  ok deraadt@@, art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.12 2005/05/29 03:20:42 deraadt Exp $	*/
a156 21
#ifdef notyet

extern struct simplelock sched_lock;

#define	SCHED_ASSERT_LOCKED()	KASSERT(simple_lock_held(&sched_lock))
#define	SCHED_ASSERT_UNLOCKED()	KASSERT(simple_lock_held(&sched_lock) == 0)

#define	SCHED_LOCK(s)							\
do {									\
	s = splsched();							\
	simple_lock(&sched_lock);					\
} while (/* CONSTCOND */ 0)

#define	SCHED_UNLOCK(s)							\
do {									\
	simple_unlock(&sched_lock);					\
	splx(s);							\
} while (/* CONSTCOND */ 0)

#else

a173 1
#endif
@


1.12
log
@sched work by niklas and art backed out; causes panics
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.10 2004/06/22 01:16:50 art Exp $	*/
d161 2
a162 2
#define	SCHED_ASSERT_LOCKED()	LOCK_ASSERT(simple_lock_held(&sched_lock))
#define	SCHED_ASSERT_UNLOCKED()	LOCK_ASSERT(simple_lock_held(&sched_lock) == 0)
d180 2
a181 2
#define	SCHED_ASSERT_LOCKED()	LOCK_ASSERT(__mp_lock_held(&sched_lock))
#define	SCHED_ASSERT_UNLOCKED()	LOCK_ASSERT(__mp_lock_held(&sched_lock) == 0)
@


1.11
log
@This patch is mortly art's work and was done *a year* ago.  Art wants to thank
everyone for the prompt review and ok of this work ;-)  Yeah, that includes me
too, or maybe especially me.  I am sorry.

Change the sched_lock to a mutex. This fixes, among other things, the infamous
"telnet localhost &" problem.  The real bug in that case was that the sched_lock
which is by design a non-recursive lock, was recursively acquired, and not
enough releases made us hold the lock in the idle loop, blocking scheduling
on the other processors.  Some of the other processors would hold the biglock though,
which made it impossible for cpu 0 to enter the kernel...  A nice deadlock.
Let me just say debugging this for days just to realize that it was all fixed
in an old diff noone ever ok'd was somewhat of an anti-climax.

This diff also changes splsched to be correct for all our architectures.
@
text
@d148 2
a149 2
#if defined(MULTIPROCESSOR)
#include <sys/mutex.h>
d151 24
a174 1
extern struct mutex sched_mutex;
d176 1
a176 2
#define	SCHED_ASSERT_LOCKED()	MUTEX_ASSERT_LOCKED(&sched_mutex)
#define	SCHED_ASSERT_UNLOCKED()	MUTEX_ASSERT_UNLOCKED(&sched_mutex)
d178 18
a195 12
/*
 * We need this MUTEX_OLDIPL hack to be able to properly restore the old
 * ipl after all the ipl juggling in cpu_switch.
 */
#define SCHED_LOCK(s) do {						\
	mtx_enter(&sched_mutex);					\
	s = MUTEX_OLDIPL(&sched_mutex);					\
} while (0)

#define SCHED_UNLOCK(s) do {						\
	mtx_leave(&sched_mutex);					\
} while (0)
d200 1
a200 1
#else /* ! MULTIPROCESSOR */
d208 1
a208 1
#endif /* MULTIPROCESSOR */
@


1.10
log
@Switch amd64 to __HAVE_CPUINFO

deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.9 2004/06/21 23:12:14 art Exp $	*/
d148 7
a154 2
#if defined(MULTIPROCESSOR) || defined(LOCKDEBUG)
#include <sys/lock.h>
d157 7
a163 42
 * XXX Instead of using struct lock for the kernel lock and thus requiring us
 * XXX to implement simplelocks, causing all sorts of fine-grained locks all
 * XXX over our tree getting activated consuming both time and potentially
 * XXX introducing locking protocol bugs.
 */
#ifdef notyet

extern struct simplelock sched_lock;

#define	SCHED_ASSERT_LOCKED()	LOCK_ASSERT(simple_lock_held(&sched_lock))
#define	SCHED_ASSERT_UNLOCKED()	LOCK_ASSERT(simple_lock_held(&sched_lock) == 0)

#define	SCHED_LOCK(s)							\
do {									\
	s = splsched();							\
	simple_lock(&sched_lock);					\
} while (/* CONSTCOND */ 0)

#define	SCHED_UNLOCK(s)							\
do {									\
	simple_unlock(&sched_lock);					\
	splx(s);							\
} while (/* CONSTCOND */ 0)

#else

extern struct __mp_lock sched_lock;

#define	SCHED_ASSERT_LOCKED()	LOCK_ASSERT(__mp_lock_held(&sched_lock))
#define	SCHED_ASSERT_UNLOCKED()	LOCK_ASSERT(__mp_lock_held(&sched_lock) == 0)

#define	SCHED_LOCK(s)							\
do {									\
	s = splsched();							\
	__mp_lock(&sched_lock);						\
} while (/* CONSTCOND */ 0)

#define	SCHED_UNLOCK(s)							\
do {									\
	__mp_unlock(&sched_lock);					\
	splx(s);							\
} while (/* CONSTCOND */ 0)
d165 3
a167 1
#endif
d172 1
a172 1
#else /* ! MULTIPROCESSOR || LOCKDEBUG */
d180 1
a180 1
#endif /* MULTIPROCESSOR || LOCKDEBUG */
@


1.9
log
@Put back the moving of schedstate_percpu into sched.h. This time expose
it to userland so that i386 builds (other architectures didn't show the
problem).

deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.8 2004/06/20 08:25:30 deraadt Exp $	*/
d141 1
a141 1
#ifndef splsched
@


1.8
log
@nope, tree breakage in libpthread.  too tough to run a make build?
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.6 2004/06/13 21:49:28 niklas Exp $	*/
d83 27
d112 5
d124 1
a124 1
#ifdef	_SYS_PROC_H_
d127 1
a129 4
static __inline void scheduler_fork_hook(
	struct proc *parent, struct proc *child);
static __inline void scheduler_wait_hook(
	struct proc *parent, struct proc *child);
d132 3
a134 7

static __inline void
scheduler_fork_hook(parent, child)
	struct proc *parent, *child;
{
	child->p_estcpu = parent->p_estcpu;
}
d137 3
a139 10

static __inline void
scheduler_wait_hook(parent, child)
	struct proc *parent, *child;
{
	/* XXX just return if parent == init?? */

	parent->p_estcpu = ESTCPULIM(parent->p_estcpu + child->p_estcpu);
}
#endif	/* _SYS_PROC_H_ */
@


1.7
log
@Move schedstate_percpu into sched.h so that we don't have to include
proc.h in cpu.h on __HAVE_CPU_INFO architectures. cpu.h is usually included
in param.h.

This also removes the horrible kludge with ifdef SYS_PROC_H in sched.h
by simply converting the inline functions into macros.

With a few suggestions from nordin@@

deraadt@@ ok
@
text
@a82 12
/*
 * CPU states.
 * XXX Not really scheduler state, but no other good place to put
 * it right now, and it really is per-CPU.
 */
#define CP_USER		0
#define CP_NICE		1
#define CP_SYS		2
#define CP_INTR		3
#define CP_IDLE		4
#define CPUSTATES	5

a84 19
/*
 * Per-CPU scheduler state.
 */
struct schedstate_percpu {
	struct timeval spc_runtime;	/* time curproc started running */
	__volatile int spc_schedflags;	/* flags; see below */
	u_int spc_schedticks;		/* ticks for schedclock() */
	u_int64_t spc_cp_time[CPUSTATES]; /* CPU state statistics */
	u_char spc_curpriority;		/* usrpri of curproc */
	int spc_rrticks;		/* ticks until roundrobin() */
	int spc_pscnt;			/* prof/stat counter */
	int spc_psdiv;			/* prof/stat divisor */	
};

/* spc_flags */
#define SPCF_SEENRR             0x0001  /* process has seen roundrobin() */
#define SPCF_SHOULDYIELD        0x0002  /* process should yield the CPU */
#define SPCF_SWITCHCLEAR        (SPCF_SEENRR|SPCF_SHOULDYIELD)

d92 1
a92 1
struct proc;
a94 1
struct cpu_info;
d97 4
d103 7
a109 3
#define scheduler_fork_hook(parent, child) do {				\
	(child)->p_estcpu = (parent)->p_estcpu;				\
} while (0)
d112 10
a121 3
#define scheduler_wait_hook(parent, child) do {				\
	(parent)->p_estcpu = ESTCPULIM((parent)->p_estcpu + (child)->p_estcpu);\
} while (0)
@


1.6
log
@debranch SMP, have fun
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d83 12
d97 19
d123 1
a123 1
#ifdef	_SYS_PROC_H_
d126 1
a128 4
static __inline void scheduler_fork_hook(
	struct proc *parent, struct proc *child);
static __inline void scheduler_wait_hook(
	struct proc *parent, struct proc *child);
d131 3
a133 7

static __inline void
scheduler_fork_hook(parent, child)
	struct proc *parent, *child;
{
	child->p_estcpu = parent->p_estcpu;
}
d136 3
a138 10

static __inline void
scheduler_wait_hook(parent, child)
	struct proc *parent, *child;
{
	/* XXX just return if parent == init?? */

	parent->p_estcpu = ESTCPULIM(parent->p_estcpu + child->p_estcpu);
}
#endif	/* _SYS_PROC_H_ */
@


1.5
log
@Merge in a piece of the SMP branch into HEAD.

Introduce the cpu_info structure, p_cpu field in struct proc and global
scheduling context and various changed code to deal with this. At the
moment no architecture uses this stuff yet, but it will allow us slow and
controlled migration to the new APIs.

All new code is ifdef:ed out.

ok deraadt@@ niklas@@
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.4 2003/06/02 23:28:21 millert Exp $	*/
d89 2
a90 1
extern int	schedhz;			/* ideally: 16 */
d93 1
a93 1
void schedclock(struct proc *p);
d122 70
@


1.4
log
@Remove the advertising clause in the UCB license which Berkeley
rescinded 22 July 1999.  Proofed by myself and Theo.
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.3 2002/03/14 01:27:14 millert Exp $	*/
d93 3
@


1.3
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.2 2001/08/19 05:44:27 art Exp $	*/
d57 1
a57 5
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.2
log
@extern schedhz
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.1 1999/08/15 00:07:50 pjanzen Exp $	*/
d96 5
a100 5
void schedclock __P((struct proc *p));
static __inline void scheduler_fork_hook __P((
	struct proc *parent, struct proc *child));
static __inline void scheduler_wait_hook __P((
	struct proc *parent, struct proc *child));
@


1.2.4.1
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.2 2001/08/19 05:44:27 art Exp $	*/
d96 5
a100 5
void schedclock(struct proc *p);
static __inline void scheduler_fork_hook(
	struct proc *parent, struct proc *child);
static __inline void scheduler_wait_hook(
	struct proc *parent, struct proc *child);
@


1.1
log
@Adopt NetBSD fix for scheduler problems (nice was broken).  From the NetBSD
commit messages:

Scheduler bug fixes and reorganization
* fix the ancient nice(1) bug, where nice +20 processes incorrectly
  steal 10 - 20% of the CPU, (or even more depending on load average)
* provide a new schedclock() mechanism at a new clock at schedhz, so high
  platform hz values don't cause nice +0 processes to look like they are
  niced
* change the algorithm slightly, and reorganize the code a lot
* fix percent-CPU calculation bugs, and eliminate some no-op code

=== nice bug === Correctly divide the scheduler queues between niced and
compute-bound processes. The current nice weight of two (sort of, see
`algorithm change' below) neatly divides the USRPRI queues in half; this
should have been used to clip p_estcpu, instead of UCHAR_MAX.  Besides
being the wrong amount, clipping an unsigned char to UCHAR_MAX is a no-op,
and it was done after decay_cpu() which can only _reduce_ the value.  It
has to be kept <= NICE_WEIGHT * PRIO_MAX - PPQ or processes can
scheduler-penalize themselves onto the same queue as nice +20 processes.
(Or even a higher one.)

=== New schedclock() mechanism === Some platforms should be cutting down
stathz before hitting the scheduler, since the scheduler algorithm only
works right in the vicinity of 64 Hz. Rather than prescale hz, then scale
back and forth by 4 every time p_estcpu is touched (each occurance an
abstraction violation), use p_estcpu without scaling and require schedhz
to be generated directly at the right frequency. Use a default stathz (well,
actually, profhz) / 4, so nothing changes unless a platform defines schedhz
and a new clock.
[ To do:  Define these for alpha, where hz==1024, and nice was totally broke.]

=== Algorithm change === The nice value used to be added to the
exponentially-decayed scheduler history value p_estcpu, in _addition_ to
be incorporated directly (with greater weight) into the priority calculation.
At first glance, it appears to be a pointless increase of 1/8 the nice
effect (pri = p_estcpu/4 + nice*2), but it's actually at least 3x that
because it will ramp up linearly but be decayed only exponentially, thus
converging to an additional .75 nice for a loadaverage of one. I killed
this: it makes the behavior hard to control, almost impossible to analyze,
and the effect (~~nothing at for the first second, then somewhat increased
niceness after three seconds or more, depending on load average) pointless.

=== Other bugs === hz -> profhz in the p_pctcpu = f(p_cpticks) calcuation.
Collect scheduler functionality. Try to put each abstraction in just one
place.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d93 1
a93 1
/* int	schedhz;			* ideally: 16 */
@


1.1.4.1
log
@Initial import of some SMP code from NetBSD.
Not really working here yet, but there is some work in progress.
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.1 1999/08/15 00:07:50 pjanzen Exp $	*/
a121 29

/*
 * CPU states.
 * XXX Not really scheduler state, but no other good place to put
 * it right now, and it really is per-CPU.
 */
#define CP_USER         0
#define CP_NICE         1
#define CP_SYS          2
#define CP_INTR         3
#define CP_IDLE         4
#define CPUSTATES       5

/*
 * Per-CPU scheduler state.
 */
struct schedstate_percpu {
        struct timeval spc_runtime;     /* time curproc started running */
        __volatile int spc_flags;       /* flags; see below */
        u_int spc_schedticks;           /* ticks for schedclock() */
        u_int64_t spc_cp_time[CPUSTATES]; /* CPU state statistics */
        u_char spc_curpriority;         /* usrpri of curproc */
};

/* spc_flags */
#define SPCF_SEENRR             0x0001  /* process has seen roundrobin() */
#define SPCF_SHOULDYIELD        0x0002  /* process should yield the CPU */

#define SPCF_SWITCHCLEAR        (SPCF_SEENRR|SPCF_SHOULDYIELD)
@


1.1.4.2
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.1.4.1 2001/07/14 10:02:47 ho Exp $	*/
d93 1
a93 1
extern int	schedhz;			/* ideally: 16 */
@


1.1.4.3
log
@Merge in -current from roughly a week ago
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d96 5
a100 5
void schedclock(struct proc *p);
static __inline void scheduler_fork_hook(
	struct proc *parent, struct proc *child);
static __inline void scheduler_wait_hook(
	struct proc *parent, struct proc *child);
@


1.1.4.4
log
@Biglock!  Most of the logic
comes from NetBSD.
Also a lot of fixes, enough to get a dual cpu machine actually run MP for a
very short while (we are just talking about seconds) before starving out one
of the cpus.  More coming very soon.
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.1.4.3 2002/03/28 14:52:01 niklas Exp $	*/
a150 34

#if defined(MULTIPROCESSOR) || defined(LOCKDEBUG)
#include <sys/lock.h>

extern struct simplelock sched_lock;

#define	SCHED_ASSERT_LOCKED()	LOCK_ASSERT(simple_lock_held(&sched_lock))
#define	SCHED_ASSERT_UNLOCKED()	LOCK_ASSERT(simple_lock_held(&sched_lock) == 0)

#define	SCHED_LOCK(s)							\
do {									\
	s = splsched();							\
	simple_lock(&sched_lock);					\
} while (/* CONSTCOND */ 0)

#define	SCHED_UNLOCK(s)							\
do {									\
	simple_unlock(&sched_lock);					\
	splx(s);							\
} while (/* CONSTCOND */ 0)

void	sched_lock_idle(void);
void	sched_unlock_idle(void);

#else /* ! MULTIPROCESSOR || LOCKDEBUG */

#define	SCHED_ASSERT_LOCKED()		splassert(IPL_SCHED);
#define	SCHED_ASSERT_UNLOCKED()		/* nothing */

#define	SCHED_LOCK(s)			s = splsched()
#define	SCHED_UNLOCK(s)			splx(s)

#endif /* MULTIPROCESSOR || LOCKDEBUG */

@


1.1.4.5
log
@Go back to defining simplelocks as noops, even if MULTIPROCESSOR.  Instead use
a new real simple recursive-lock capable lock implementation for the few
necessary locks (kernel, scheduler, tlb shootdown, printf and ddb MP).
This because we cannot trust the old fine-grained locks spread out all over
our kernel, and not really tested.
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.1.4.4 2003/05/15 04:08:03 niklas Exp $	*/
a154 8
/*
 * XXX Instead of using struct lock for the kernel lock and thus requiring us
 * XXX to implement simplelocks, causing all sorts of fine-grained locks all
 * XXX over our tree getting activated consuming both time and potentially
 * XXX introducing locking protocol bugs.
 */
#ifdef notyet

a170 21

#else

extern struct __mp_lock sched_lock;

#define	SCHED_ASSERT_LOCKED()	LOCK_ASSERT(__mp_lock_held(&sched_lock))
#define	SCHED_ASSERT_UNLOCKED()	LOCK_ASSERT(__mp_lock_held(&sched_lock) == 0)

#define	SCHED_LOCK(s)							\
do {									\
	s = splsched();							\
	__mp_lock(&sched_lock);						\
} while (/* CONSTCOND */ 0)

#define	SCHED_UNLOCK(s)							\
do {									\
	__mp_unlock(&sched_lock);					\
	splx(s);							\
} while (/* CONSTCOND */ 0)

#endif
@


1.1.4.6
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.1.4.5 2003/05/18 17:41:16 niklas Exp $	*/
d57 5
a61 1
 * 3. Neither the name of the University nor the names of its contributors
@


1.1.4.7
log
@export some defns to non-_KERNEL land
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.1.4.6 2003/06/07 11:09:08 ho Exp $	*/
a82 29
/*
 * CPU states.
 * XXX Not really scheduler state, but no other good place to put
 * it right now, and it really is per-CPU.
 */
#define CP_USER         0
#define CP_NICE         1
#define CP_SYS          2
#define CP_INTR         3
#define CP_IDLE         4
#define CPUSTATES       5

/*
 * Per-CPU scheduler state.
 */
struct schedstate_percpu {
        struct timeval spc_runtime;     /* time curproc started running */
        __volatile int spc_flags;       /* flags; see below */
        u_int spc_schedticks;           /* ticks for schedclock() */
        u_int64_t spc_cp_time[CPUSTATES]; /* CPU state statistics */
        u_char spc_curpriority;         /* usrpri of curproc */
};

/* spc_flags */
#define SPCF_SEENRR             0x0001  /* process has seen roundrobin() */
#define SPCF_SHOULDYIELD        0x0002  /* process should yield the CPU */

#define SPCF_SWITCHCLEAR        (SPCF_SEENRR|SPCF_SHOULDYIELD)

d119 28
@


1.1.4.8
log
@Some merged code from NetBSD, more to come
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.1.4.7 2004/02/21 02:49:00 niklas Exp $	*/
d147 1
@


1.1.4.9
log
@Make a few scheduling globals per-cpu, mostly NetBSD code
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.1.4.8 2004/03/14 22:08:21 niklas Exp $	*/
d100 1
a100 1
        __volatile int spc_schedflags;  /* flags; see below */
a103 3
	int spc_rrticks;		/* ticks until roundrobin() */
	int spc_pscnt;			/* prof/stat counter */
	int spc_psdiv;			/* prof/stat divisor */	
d118 1
a118 2
extern int schedhz;			/* ideally: 16 */
extern int rrticks_init;		/* ticks per roundrobin() */
d121 1
a121 3
void schedclock(struct proc *);
void roundrobin(struct cpu_info *);

@


1.1.4.10
log
@splsched and IPL_SCHED defaults
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.1.4.9 2004/06/05 17:19:55 niklas Exp $	*/
a152 7

#ifndef splsched
#define splsched() splhigh()
#endif
#ifndef IPL_SCHED
#define IPL_SCHED IPL_HIGH
#endif
@


1.1.4.11
log
@sched.h and proc.h contain a weird dependency; but sparc64 needs an
actual defn of schedstate_percpu, so put it in proc.h instead
@
text
@d1 1
a1 1
/*	$OpenBSD: sched.h,v 1.1.4.10 2004/06/06 23:31:37 niklas Exp $	*/
d82 26
@


1.1.4.12
log
@sync with head, make i386 __HAVE_CPUINFO
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d83 6
a99 1
#ifdef __HAVE_CPUINFO
d101 1
a101 1
#endif
@


