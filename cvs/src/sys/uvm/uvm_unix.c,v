head	1.64;
access;
symbols
	OPENBSD_6_1_BASE:1.64
	OPENBSD_6_0:1.58.0.2
	OPENBSD_6_0_BASE:1.58
	OPENBSD_5_9:1.56.0.2
	OPENBSD_5_9_BASE:1.56
	OPENBSD_5_8:1.56.0.4
	OPENBSD_5_8_BASE:1.56
	OPENBSD_5_7:1.55.0.2
	OPENBSD_5_7_BASE:1.55
	OPENBSD_5_6:1.50.0.4
	OPENBSD_5_6_BASE:1.50
	OPENBSD_5_5:1.47.0.8
	OPENBSD_5_5_BASE:1.47
	OPENBSD_5_4:1.47.0.4
	OPENBSD_5_4_BASE:1.47
	OPENBSD_5_3:1.47.0.2
	OPENBSD_5_3_BASE:1.47
	OPENBSD_5_2:1.43.0.2
	OPENBSD_5_2_BASE:1.43
	OPENBSD_5_1_BASE:1.42
	OPENBSD_5_1:1.42.0.4
	OPENBSD_5_0:1.42.0.2
	OPENBSD_5_0_BASE:1.42
	OPENBSD_4_9:1.40.0.6
	OPENBSD_4_9_BASE:1.40
	OPENBSD_4_8:1.40.0.4
	OPENBSD_4_8_BASE:1.40
	OPENBSD_4_7:1.40.0.2
	OPENBSD_4_7_BASE:1.40
	OPENBSD_4_6:1.39.0.4
	OPENBSD_4_6_BASE:1.39
	OPENBSD_4_5:1.34.0.6
	OPENBSD_4_5_BASE:1.34
	OPENBSD_4_4:1.34.0.4
	OPENBSD_4_4_BASE:1.34
	OPENBSD_4_3:1.34.0.2
	OPENBSD_4_3_BASE:1.34
	OPENBSD_4_2:1.28.0.2
	OPENBSD_4_2_BASE:1.28
	OPENBSD_4_1:1.27.0.8
	OPENBSD_4_1_BASE:1.27
	OPENBSD_4_0:1.27.0.6
	OPENBSD_4_0_BASE:1.27
	OPENBSD_3_9:1.27.0.4
	OPENBSD_3_9_BASE:1.27
	OPENBSD_3_8:1.27.0.2
	OPENBSD_3_8_BASE:1.27
	OPENBSD_3_7:1.26.0.10
	OPENBSD_3_7_BASE:1.26
	OPENBSD_3_6:1.26.0.8
	OPENBSD_3_6_BASE:1.26
	SMP_SYNC_A:1.26
	SMP_SYNC_B:1.26
	OPENBSD_3_5:1.26.0.6
	OPENBSD_3_5_BASE:1.26
	OPENBSD_3_4:1.26.0.4
	OPENBSD_3_4_BASE:1.26
	UBC_SYNC_A:1.26
	OPENBSD_3_3:1.26.0.2
	OPENBSD_3_3_BASE:1.26
	OPENBSD_3_2:1.21.0.2
	OPENBSD_3_2_BASE:1.21
	OPENBSD_3_1:1.20.0.2
	OPENBSD_3_1_BASE:1.20
	UBC_SYNC_B:1.21
	UBC:1.19.0.2
	UBC_BASE:1.19
	OPENBSD_3_0:1.13.0.2
	OPENBSD_3_0_BASE:1.13
	OPENBSD_2_9_BASE:1.8
	OPENBSD_2_9:1.8.0.2
	OPENBSD_2_8:1.7.0.2
	OPENBSD_2_8_BASE:1.7
	OPENBSD_2_7:1.5.0.2
	OPENBSD_2_7_BASE:1.5
	SMP:1.4.0.4
	SMP_BASE:1.4
	kame_19991208:1.4
	OPENBSD_2_6:1.4.0.2
	OPENBSD_2_6_BASE:1.4
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.64
date	2017.03.09.20.27.41;	author guenther;	state Exp;
branches;
next	1.63;
commitid	bB4dS4VdD1KzeCgE;

1.63
date	2017.03.05.00.55.01;	author guenther;	state Exp;
branches;
next	1.62;
commitid	4bQhVfYBhrAaUZP3;

1.62
date	2017.03.05.00.45.31;	author guenther;	state Exp;
branches;
next	1.61;
commitid	UrlMJQwoMZBUiRfc;

1.61
date	2017.02.02.06.23.58;	author guenther;	state Exp;
branches;
next	1.60;
commitid	b5n3n5cF5Af6TyP0;

1.60
date	2016.09.16.01.09.53;	author dlg;	state Exp;
branches;
next	1.59;
commitid	S1LT7BcQMYzBQOe8;

1.59
date	2016.08.12.22.46.02;	author kettenis;	state Exp;
branches;
next	1.58;
commitid	bzXvsSTgX9f9N0QZ;

1.58
date	2016.04.04.16.34.16;	author stefan;	state Exp;
branches;
next	1.57;
commitid	mErYIUO2MMXVvZFw;

1.57
date	2016.03.15.18.16.47;	author stefan;	state Exp;
branches;
next	1.56;
commitid	0lbesUe6MrY9vB2t;

1.56
date	2015.05.05.02.13.46;	author guenther;	state Exp;
branches;
next	1.55;
commitid	dNPv28CJI5BxtRGW;

1.55
date	2015.02.09.09.39.09;	author miod;	state Exp;
branches;
next	1.54;
commitid	Dny5EZ91sz8fb6Ea;

1.54
date	2014.12.17.06.58.11;	author guenther;	state Exp;
branches;
next	1.53;
commitid	DImukoCWyTxwdbuh;

1.53
date	2014.12.15.02.24.23;	author guenther;	state Exp;
branches;
next	1.52;
commitid	ZxaujiOM0aYQRjFY;

1.52
date	2014.11.17.03.15.58;	author deraadt;	state Exp;
branches;
next	1.51;
commitid	IGQzaP0ysLESh25Y;

1.51
date	2014.11.16.12.31.01;	author deraadt;	state Exp;
branches;
next	1.50;
commitid	yv0ECmCdICvq576h;

1.50
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.49;
commitid	7NtJNW9udCOFtDNM;

1.49
date	2014.04.13.23.14.15;	author tedu;	state Exp;
branches;
next	1.48;

1.48
date	2014.03.26.05.23.42;	author guenther;	state Exp;
branches;
next	1.47;

1.47
date	2013.01.16.21.47.08;	author deraadt;	state Exp;
branches;
next	1.46;

1.46
date	2013.01.16.00.24.33;	author deraadt;	state Exp;
branches;
next	1.45;

1.45
date	2013.01.15.02.03.38;	author deraadt;	state Exp;
branches;
next	1.44;

1.44
date	2013.01.15.01.34.27;	author deraadt;	state Exp;
branches;
next	1.43;

1.43
date	2012.03.09.13.01.29;	author ariane;	state Exp;
branches;
next	1.42;

1.42
date	2011.06.06.17.10.23;	author ariane;	state Exp;
branches;
next	1.41;

1.41
date	2011.05.24.15.27.36;	author ariane;	state Exp;
branches;
next	1.40;

1.40
date	2009.11.24.10.35.56;	author otto;	state Exp;
branches;
next	1.39;

1.39
date	2009.06.17.22.19.12;	author kettenis;	state Exp;
branches;
next	1.38;

1.38
date	2009.06.17.00.13.59;	author oga;	state Exp;
branches;
next	1.37;

1.37
date	2009.06.06.21.25.21;	author deraadt;	state Exp;
branches;
next	1.36;

1.36
date	2009.03.20.15.19.04;	author oga;	state Exp;
branches;
next	1.35;

1.35
date	2009.03.05.19.52.24;	author kettenis;	state Exp;
branches;
next	1.34;

1.34
date	2008.03.02.20.29.20;	author kettenis;	state Exp;
branches;
next	1.33;

1.33
date	2008.02.27.21.46.34;	author kettenis;	state Exp;
branches;
next	1.32;

1.32
date	2008.01.05.00.36.13;	author miod;	state Exp;
branches;
next	1.31;

1.31
date	2007.12.16.14.32.02;	author kettenis;	state Exp;
branches;
next	1.30;

1.30
date	2007.09.15.10.10.37;	author martin;	state Exp;
branches;
next	1.29;

1.29
date	2007.09.01.15.14.44;	author martin;	state Exp;
branches;
next	1.28;

1.28
date	2007.04.11.12.51.51;	author miod;	state Exp;
branches;
next	1.27;

1.27
date	2005.07.26.07.11.55;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2003.03.04.18.24.05;	author mickey;	state Exp;
branches;
next	1.25;

1.25
date	2003.02.18.18.50.54;	author mickey;	state Exp;
branches;
next	1.24;

1.24
date	2002.12.20.03.01.00;	author mickey;	state Exp;
branches;
next	1.23;

1.23
date	2002.12.19.00.57.07;	author mickey;	state Exp;
branches;
next	1.22;

1.22
date	2002.10.29.18.30.21;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2002.08.23.11.26.57;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2001.12.19.08.58.07;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2001.11.28.19.28.15;	author art;	state Exp;
branches
	1.19.2.1;
next	1.18;

1.18
date	2001.11.28.13.47.40;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2001.11.07.02.55.51;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2001.11.07.01.18.01;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2001.11.06.13.36.52;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2001.11.06.01.35.04;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2001.08.11.10.57.22;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2001.08.06.14.03.05;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2001.07.25.14.47.59;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2001.05.05.21.26.46;	author art;	state Exp;
branches;
next	1.9;

1.9
date	2001.05.05.20.57.03;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2001.01.29.02.07.50;	author niklas;	state Exp;
branches;
next	1.7;

1.7
date	2000.07.04.10.34.36;	author art;	state Exp;
branches;
next	1.6;

1.6
date	2000.06.30.01.07.49;	author art;	state Exp;
branches;
next	1.5;

1.5
date	2000.03.15.15.50.21;	author art;	state Exp;
branches
	1.5.2.1;
next	1.4;

1.4
date	99.08.23.08.13.25;	author art;	state Exp;
branches
	1.4.4.1;
next	1.3;

1.3
date	99.07.20.11.10.54;	author mickey;	state Exp;
branches;
next	1.2;

1.2
date	99.02.26.05.32.08;	author art;	state Exp;
branches;
next	1.1;

1.1
date	99.02.26.01.30.18;	author art;	state Exp;
branches;
next	;

1.4.4.1
date	2000.03.24.09.09.52;	author niklas;	state Exp;
branches;
next	1.4.4.2;

1.4.4.2
date	2001.05.14.22.47.49;	author niklas;	state Exp;
branches;
next	1.4.4.3;

1.4.4.3
date	2001.07.04.11.01.10;	author niklas;	state Exp;
branches;
next	1.4.4.4;

1.4.4.4
date	2001.10.31.03.32.14;	author nate;	state Exp;
branches;
next	1.4.4.5;

1.4.4.5
date	2001.11.13.23.02.31;	author niklas;	state Exp;
branches;
next	1.4.4.6;

1.4.4.6
date	2001.12.05.01.23.58;	author niklas;	state Exp;
branches;
next	1.4.4.7;

1.4.4.7
date	2002.03.06.02.17.14;	author niklas;	state Exp;
branches;
next	1.4.4.8;

1.4.4.8
date	2003.03.28.00.08.48;	author niklas;	state Exp;
branches;
next	;

1.5.2.1
date	2000.07.04.18.46.25;	author jason;	state Exp;
branches;
next	1.5.2.2;

1.5.2.2
date	2000.10.10.16.08.27;	author jason;	state Exp;
branches;
next	;

1.19.2.1
date	2002.02.02.03.28.27;	author art;	state Exp;
branches;
next	1.19.2.2;

1.19.2.2
date	2002.10.29.00.36.50;	author art;	state Exp;
branches;
next	1.19.2.3;

1.19.2.3
date	2002.10.29.02.12.53;	author art;	state Exp;
branches;
next	1.19.2.4;

1.19.2.4
date	2003.05.19.22.41.30;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.64
log
@Don't take the vmmap lock when dumping core: it's not actually necessary
and it creates a lock-order-reversal with inode locks

ok stefan@@
@
text
@/*	$OpenBSD: uvm_unix.c,v 1.63 2017/03/05 00:55:01 guenther Exp $	*/
/*	$NetBSD: uvm_unix.c,v 1.18 2000/09/13 15:00:25 thorpej Exp $	*/

/*
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * Copyright (c) 1991, 1993 The Regents of the University of California.  
 * Copyright (c) 1988 University of Utah.
 *
 * All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * the Systems Programming Group of the University of Utah Computer
 * Science Department.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 * from: Utah $Hdr: vm_unix.c 1.1 89/11/07$
 *      @@(#)vm_unix.c   8.1 (Berkeley) 6/11/93
 * from: Id: uvm_unix.c,v 1.1.2.2 1997/08/25 18:52:30 chuck Exp
 */

/*
 * uvm_unix.c: traditional sbrk/grow interface to vm.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/resourcevar.h>
#include <sys/vnode.h>

#include <sys/mount.h>
#include <sys/syscallargs.h>

#include <uvm/uvm.h>

/*
 * sys_obreak: set break
 */

int
sys_obreak(struct proc *p, void *v, register_t *retval)
{
	struct sys_obreak_args /* {
		syscallarg(char *) nsize;
	} */ *uap = v;
	struct vmspace *vm = p->p_vmspace;
	vaddr_t new, old, base;
	int error;

	base = (vaddr_t)vm->vm_daddr;
	new = round_page((vaddr_t)SCARG(uap, nsize));
	if (new < base || (new - base) > p->p_rlimit[RLIMIT_DATA].rlim_cur)
		return (ENOMEM);

	old = round_page(base + ptoa(vm->vm_dsize));

	if (new == old)
		return (0);

	/* grow or shrink? */
	if (new > old) {
		error = uvm_map(&vm->vm_map, &old, new - old, NULL,
		    UVM_UNKNOWN_OFFSET, 0,
		    UVM_MAPFLAG(PROT_READ | PROT_WRITE,
		    PROT_READ | PROT_WRITE | PROT_EXEC, MAP_INHERIT_COPY,
		    MADV_NORMAL, UVM_FLAG_FIXED|UVM_FLAG_COPYONW));
		if (error) {
			uprintf("sbrk: grow %ld failed, error = %d\n",
			    new - old, error);
			return (ENOMEM);
		}
		vm->vm_dsize += atop(new - old);
	} else {
		uvm_deallocate(&vm->vm_map, new, old - new);
		vm->vm_dsize -= atop(old - new);
	}

	return (0);
}

/*
 * uvm_grow: enlarge the "stack segment" to include sp.
 */
void
uvm_grow(struct proc *p, vaddr_t sp)
{
	struct vmspace *vm = p->p_vmspace;
	int si;

	/* For user defined stacks (from sendsig). */
	if (sp < (vaddr_t)vm->vm_maxsaddr)
		return;

	/* For common case of already allocated (from trap). */
#ifdef MACHINE_STACK_GROWS_UP
	if (sp < (vaddr_t)vm->vm_maxsaddr + ptoa(vm->vm_ssize))
#else
	if (sp >= (vaddr_t)vm->vm_minsaddr - ptoa(vm->vm_ssize))
#endif
		return;

	/* Really need to check vs limit and increment stack size if ok. */
#ifdef MACHINE_STACK_GROWS_UP
	si = atop(sp - (vaddr_t)vm->vm_maxsaddr) - vm->vm_ssize + 1;
#else
	si = atop((vaddr_t)vm->vm_minsaddr - sp) - vm->vm_ssize;
#endif
	if (vm->vm_ssize + si <= atop(p->p_rlimit[RLIMIT_STACK].rlim_cur))
		vm->vm_ssize += si;
}

#ifndef SMALL_KERNEL

#define WALK_CHUNK	32
/*
 * Not all the pages in an amap may be present.  When dumping core,
 * we don't want to force all the pages to be present: it's a waste
 * of time and memory when we already know what they contain (zeros)
 * and the ELF format at least can adequately represent them as a
 * segment with memory size larger than its file size.
 *
 * So, we walk the amap with calls to amap_lookups() and scan the
 * resulting pointers to find ranges of zero or more present pages
 * followed by at least one absent page or the end of the amap.
 * When then pass that range to the walk callback with 'start'
 * pointing to the start of the present range, 'realend' pointing
 * to the first absent page (or the end of the entry), and 'end'
 * pointing to the page page the last absent page (or the end of
 * the entry).
 *
 * Note that if the first page of the amap is empty then the callback
 * must be invoked with 'start' == 'realend' so it can present that
 * first range of absent pages.
 */
int
uvm_coredump_walk_amap(struct vm_map_entry *entry, int *nsegmentp,
    uvm_coredump_walk_cb *walk, void *cookie)
{
	struct vm_anon *anons[WALK_CHUNK];
	vaddr_t pos, start, realend, end, entry_end;
	vm_prot_t prot;
	int nsegment, absent, npages, i, error;

	prot = entry->protection;
	nsegment = *nsegmentp;
	start = entry->start;
	entry_end = MIN(entry->end, VM_MAXUSER_ADDRESS);

	absent = 0;
	for (pos = start; pos < entry_end; pos += npages << PAGE_SHIFT) {
		npages = (entry_end - pos) >> PAGE_SHIFT;
		if (npages > WALK_CHUNK)
			npages = WALK_CHUNK;
		amap_lookups(&entry->aref, pos - entry->start, anons, npages);
		for (i = 0; i < npages; i++) {
			if ((anons[i] == NULL) == absent)
				continue;
			if (!absent) {
				/* going from present to absent: set realend */
				realend = pos + (i << PAGE_SHIFT);
				absent = 1;
				continue;
			}

			/* going from absent to present: invoke callback */
			end = pos + (i << PAGE_SHIFT);
			if (start != end) {
				error = (*walk)(start, realend, end, prot,
				    nsegment, cookie);
				if (error)
					return error;
				nsegment++;
			}
			start = realend = end;
			absent = 0;
		}
	}

	if (!absent)
		realend = entry_end;
	error = (*walk)(start, realend, entry_end, prot, nsegment, cookie);
	*nsegmentp = nsegment + 1;
	return error;
}

/*
 * Common logic for whether a map entry should be included in a coredump
 */
static inline int
uvm_should_coredump(struct proc *p, struct vm_map_entry *entry)
{
	if (!(entry->protection & PROT_WRITE) &&
	    entry->aref.ar_amap == NULL &&
	    entry->start != p->p_p->ps_sigcode)
		return 0;

	/*
	 * Skip ranges marked as unreadable, as uiomove(UIO_USERSPACE)
	 * will fail on them.  Maybe this really should be a test of
	 * entry->max_protection, but doing
	 *	uvm_map_extract(UVM_EXTRACT_FIXPROT)
	 * on each such page would suck.
	 */
	if ((entry->protection & PROT_READ) == 0)
		return 0;

	/* Don't dump mmaped devices. */
	if (entry->object.uvm_obj != NULL &&
	    UVM_OBJ_IS_DEVICE(entry->object.uvm_obj))
		return 0;

	if (entry->start >= VM_MAXUSER_ADDRESS)
		return 0;

	return 1;
}


/* do nothing callback for uvm_coredump_walk_amap() */
static int
noop(vaddr_t start, vaddr_t realend, vaddr_t end, vm_prot_t prot,
    int nsegment, void *cookie)
{
	return 0;
}

/*
 * Walk the VA space for a process to identify what to write to
 * a coredump.  First the number of contiguous ranges is counted,
 * then the 'setup' callback is invoked to prepare for actually
 * recording the ranges, then the VA is walked again, invoking
 * the 'walk' callback for each range.  The number of ranges walked
 * is guaranteed to match the count seen by the 'setup' callback.
 */

int
uvm_coredump_walkmap(struct proc *p, uvm_coredump_setup_cb *setup,
    uvm_coredump_walk_cb *walk, void *cookie)
{
	struct vmspace *vm = p->p_vmspace;
	struct vm_map *map = &vm->vm_map;
	struct vm_map_entry *entry;
	vaddr_t end;
	int refed_amaps = 0;
	int nsegment, error;

	/*
	 * Walk the map once to count the segments.  If an amap is
	 * referenced more than once than take *another* reference
	 * and treat the amap as exactly one segment instead of
	 * checking page presence inside it.  On the second pass
	 * we'll recognize which amaps we did that for by the ref
	 * count being >1...and decrement it then.
	 */
	nsegment = 0;
	RBT_FOREACH(entry, uvm_map_addr, &map->addr) {
		/* should never happen for a user process */
		if (UVM_ET_ISSUBMAP(entry)) {
			panic("%s: user process with submap?", __func__);
		}

		if (! uvm_should_coredump(p, entry))
			continue;

		if (entry->aref.ar_amap != NULL) {
			if (entry->aref.ar_amap->am_ref == 1) {
				uvm_coredump_walk_amap(entry, &nsegment,
				    &noop, cookie);
				continue;
			}

			/*
			 * Multiple refs currently, so take another and
			 * treat it as a single segment
			 */
			entry->aref.ar_amap->am_ref++;
			refed_amaps++;
		}

		nsegment++;
	}

	/*
	 * Okay, we have a count in nsegment.  Prepare to
	 * walk it again, then invoke the setup callback. 
	 */
	entry = RBT_MIN(uvm_map_addr, &map->addr);
	error = (*setup)(nsegment, cookie);
	if (error)
		goto cleanup;

	/*
	 * Setup went okay, so do the second walk, invoking the walk
	 * callback on the counted segments and cleaning up references
	 * as we go.
	 */
	nsegment = 0;
	for (; entry != NULL; entry = RBT_NEXT(uvm_map_addr, entry)) {
		if (! uvm_should_coredump(p, entry))
			continue;

		if (entry->aref.ar_amap != NULL &&
		    entry->aref.ar_amap->am_ref == 1) {
			error = uvm_coredump_walk_amap(entry, &nsegment,
			    walk, cookie);
			if (error)
				break;
			continue;
		}

		end = entry->end;
		if (end > VM_MAXUSER_ADDRESS)
			end = VM_MAXUSER_ADDRESS;

		error = (*walk)(entry->start, end, end, entry->protection,
		    nsegment, cookie);
		if (error)
			break;
		nsegment++;

		if (entry->aref.ar_amap != NULL &&
		    entry->aref.ar_amap->am_ref > 1) {
			/* multiple refs, so we need to drop one */
			entry->aref.ar_amap->am_ref--;
			refed_amaps--;
		}
	}

	if (error) {
cleanup:
		/* clean up the extra references from where we left off */
		if (refed_amaps > 0) {
			for (; entry != NULL;
			    entry = RBT_NEXT(uvm_map_addr, entry)) {
				if (entry->aref.ar_amap == NULL ||
				    entry->aref.ar_amap->am_ref == 1)
					continue;
				if (! uvm_should_coredump(p, entry))
					continue;
				entry->aref.ar_amap->am_ref--;
				if (refed_amaps-- == 0)
					break;
			}
		}
	}

	return error;
}

#endif	/* !SMALL_KERNEL */
@


1.63
log
@Handle unshared amaps in uvm_coredump_walkmap() such that untouched pages
don't get written out to the core file but rather are represented via
segments which have memory size greater than their file size.  This shrinks
core files and eliminates a case where core dumping fails with EFAULT.
This can still happen in the shared amap case.

Based on a problem report from (and testing by) semarie@@
ok stefan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.62 2017/03/05 00:45:31 guenther Exp $	*/
a278 1
	vm_map_lock_read(map);
a368 1
	vm_map_unlock_read(map);
@


1.62
log
@Generating a coredump requires walking the map twice; change
uvm_coredump_walkmap() to do both with a callback in between
so it can hold locks/change state across the two.

ok stefan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.61 2017/02/02 06:23:58 guenther Exp $	*/
d137 72
d241 9
d267 1
d271 6
a276 1
	 * Walk the map once to count the segments.
d289 15
d308 2
a309 1
	 * Okay, we have a count in nsegment; invoke the setup callback.
d311 1
d318 2
a319 1
	 * callback on the counted segments.
d322 1
a322 1
	RBT_FOREACH(entry, uvm_map_addr, &map->addr) {
d326 9
d344 7
d353 1
d355 15
@


1.61
log
@When dumping core, skip pages marked as unreadable instead of aborting
the dump.

tracked down with help from semarie@@
ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.60 2016/09/16 01:09:53 dlg Exp $	*/
d138 1
a138 2
 * Walk the VA space for a process, invoking 'func' on each present range
 * that should be included in a coredump.
d140 38
d179 2
a180 3
uvm_coredump_walkmap(struct proc *p, void *iocookie,
    int (*func)(struct proc *, void *, struct uvm_coredump_state *),
    void *cookie)
a181 1
	struct uvm_coredump_state state;
d185 2
a186 2
	vaddr_t top;
	int error;
d188 5
a193 4
		state.cookie = cookie;
		state.prot = entry->protection;
		state.flags = 0;

d199 1
a199 3
		if (!(entry->protection & PROT_WRITE) &&
		    entry->aref.ar_amap == NULL &&
		    entry->start != p->p_p->ps_sigcode)
d202 2
a203 9
		/*
		 * Skip pages marked as unreadable, as uiomove(UIO_USERSPACE)
		 * will fail on them.  Maybe this really should be a test of
		 * entry->max_protection, but doing
		 *	uvm_map_extract(UVM_EXTRACT_FIXPROT)
		 * when dumping such a mapping would suck.
		 */
		if ((entry->protection & PROT_READ) == 0)
			continue;
d205 14
a218 3
		/* Don't dump mmaped devices. */
		if (entry->object.uvm_obj != NULL &&
		    UVM_OBJ_IS_DEVICE(entry->object.uvm_obj))
d221 3
a223 3
		state.start = entry->start;
		state.realend = entry->end;
		state.end = entry->end;
d225 2
a226 30
		if (state.start >= VM_MAXUSER_ADDRESS)
			continue;

		if (state.end > VM_MAXUSER_ADDRESS)
			state.end = VM_MAXUSER_ADDRESS;

#ifdef MACHINE_STACK_GROWS_UP
		if ((vaddr_t)vm->vm_maxsaddr <= state.start &&
		    state.start < ((vaddr_t)vm->vm_maxsaddr + MAXSSIZ)) {
			top = round_page((vaddr_t)vm->vm_maxsaddr +
			    ptoa(vm->vm_ssize));
			if (state.end > top)
				state.end = top;

			if (state.start >= state.end)
				continue;
#else
		if (state.start >= (vaddr_t)vm->vm_maxsaddr) {
			top = trunc_page((vaddr_t)vm->vm_minsaddr -
			    ptoa(vm->vm_ssize));
			if (state.start < top)
				state.start = top;

			if (state.start >= state.end)
				continue;
#endif
			state.flags |= UVM_COREDUMP_STACK;
		}

		error = (*func)(p, iocookie, &state);
d228 2
a229 1
			return (error);
d232 4
a235 1
	return (0);
@


1.60
log
@move the uvm_map_addr RB tree from RB macros to the RBT functions

this tree is interesting because it uses all the red black tree
features, specifically the augment callback thats called on tree
topology changes, and it poisons and checks entries as theyre removed
from and inserted back into the tree respectively.

ok stefan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.59 2016/08/12 22:46:02 kettenis Exp $	*/
d166 10
@


1.59
log
@Include map entries that have an amap associated with them in the coredump.
This fixes coredumps of processes that use relro to make part of their
writable address space read-only.

ok guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.58 2016/04/04 16:34:16 stefan Exp $	*/
d153 1
a153 1
	RB_FOREACH(entry, uvm_map_addr, &map->addr) {
@


1.58
log
@UVM_FLAG_AMAPPAD has no effect anymore, nuke it.

This flag caused amaps to be allocated with additional spare slots, to
make extending them cheaper. However, the kernel never extends amaps,
so allocating spare slots is pointless. Also UVM_FLAG_AMAPPAD only
has an effect in combination with UVM_FLAG_OVERLAY. The only function
that used both flags was sys_obreak, but that function had the use of
UVM_FLAG_OVERLAY removed recently.

While there, kill the unused prototypes amap_flags and amap_refs.
They're defined as macros already.

ok mlarkin@@ kettenis@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.57 2016/03/15 18:16:47 stefan Exp $	*/
d164 1
@


1.57
log
@Allocate amap slots for a virtual memory range reserved with sbrk lazily.

This avoids wasting kernel memory if the user process does not make
use of the allocated memory.

Testing by sthen@@ and tobiasu@@, thanks!

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.56 2015/05/05 02:13:46 guenther Exp $	*/
d89 1
a89 2
		    MADV_NORMAL, UVM_FLAG_AMAPPAD|UVM_FLAG_FIXED|
		    UVM_FLAG_COPYONW));
@


1.56
log
@emul_native is only used for kernel threads which can't dump core, so
delete coredump_trad(), uvm_coredump(), cpu_coredump(), struct md_coredump,
and various #includes that are superfluous.

This leaves compat_linux processes without a coredump callback.  If that
ability is desired, someone should update it to use coredump_elf32() and
verify the results...

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.55 2015/02/09 09:39:09 miod Exp $	*/
d90 1
a90 1
		    UVM_FLAG_OVERLAY|UVM_FLAG_COPYONW));
@


1.55
log
@Stop using USRSTACK as the edge of the stack, but rather use the vmspace
vm_minsaddr or vm_maxsaddr, depending upon the direction the stack goes in.

This should have no effect on the existing behaviourrr.

ok kettenis@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.54 2014/12/17 06:58:11 guenther Exp $	*/
a52 1
#include <sys/core.h>
d139 2
a140 1
 * uvm_coredump: dump core!
a141 111

int
uvm_coredump(struct proc *p, struct vnode *vp, struct ucred *cred,
    struct core *chdr)
{
	struct vmspace *vm = p->p_vmspace;
	vm_map_t map = &vm->vm_map;
	vm_map_entry_t entry, safe;
	vaddr_t start, end, top;
	struct coreseg cseg;
	off_t offset, coffset;
	int csize, chunk, flag, error = 0;

	offset = chdr->c_hdrsize + chdr->c_seghdrsize + chdr->c_cpusize;

	RB_FOREACH_SAFE(entry, uvm_map_addr, &map->addr, safe) {
		/* should never happen for a user process */
		if (UVM_ET_ISSUBMAP(entry)) {
			panic("uvm_coredump: user process with submap?");
		}

		if (!(entry->protection & PROT_WRITE) &&
		    entry->start != p->p_p->ps_sigcode)
			continue;

		/* Don't dump mmaped devices. */
		if (entry->object.uvm_obj != NULL &&
		    UVM_OBJ_IS_DEVICE(entry->object.uvm_obj))
			continue;

		start = entry->start;
		end = entry->end;

		if (start >= VM_MAXUSER_ADDRESS)
			continue;

		if (end > VM_MAXUSER_ADDRESS)
			end = VM_MAXUSER_ADDRESS;

#ifdef MACHINE_STACK_GROWS_UP
		if ((vaddr_t)vm->vm_maxsaddr <= start &&
		    start < ((vaddr_t)vm->vm_maxsaddr + MAXSSIZ)) {
			top = round_page((vaddr_t)vm->vm_maxsaddr +
			    ptoa(vm->vm_ssize));
			if (end > top)
				end = top;

			if (start >= end)
				continue;
#else
		if (start >= (vaddr_t)vm->vm_maxsaddr) {
			top = trunc_page((vaddr_t)vm->vm_minsaddr -
			    ptoa(vm->vm_ssize));
			if (start < top)
				start = top;

			if (start >= end)
				continue;
#endif
			flag = CORE_STACK;
		} else
			flag = CORE_DATA;

		/* Set up a new core file segment. */
		CORE_SETMAGIC(cseg, CORESEGMAGIC, CORE_GETMID(*chdr), flag);
		cseg.c_addr = start;
		cseg.c_size = end - start;

		error = vn_rdwr(UIO_WRITE, vp,
		    (caddr_t)&cseg, chdr->c_seghdrsize,
		    offset, UIO_SYSSPACE, IO_UNIT, cred, NULL, p);
		/*
		 * We might get an EFAULT on objects mapped beyond
		 * EOF. Ignore the error.
		 */
		if (error && error != EFAULT)
			break;

		offset += chdr->c_seghdrsize;

		coffset = 0;
		csize = (int)cseg.c_size;
		do {
			if (p->p_siglist & sigmask(SIGKILL))
				return (EINTR);

			/* Rest of the loop sleeps with lock held, so... */
			yield();

			chunk = MIN(csize, MAXPHYS);
			error = vn_rdwr(UIO_WRITE, vp,
			    (caddr_t)(u_long)cseg.c_addr + coffset,
			    chunk, offset + coffset, UIO_USERSPACE,
			    IO_UNIT, cred, NULL, p);
			if (error)
				return (error);

			coffset += chunk;
			csize -= chunk;
		} while (csize > 0);
		offset += cseg.c_size;

		/* Discard the memory */
		uvm_unmap(map, cseg.c_addr, cseg.c_addr + cseg.c_size);

		chdr->c_nseg++;
	}

	return (error);
}

d161 1
a161 1
			panic("uvm_coredump: user process with submap?");
@


1.54
log
@Prefer MADV_* over POSIX_MADV_* in kernel for consistency: the latter
doesn't have all the values and therefore can't be used everywhere.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.53 2014/12/15 02:24:23 guenther Exp $	*/
d121 1
a121 1
	if (sp < USRSTACK + ptoa(vm->vm_ssize))
d123 1
a123 1
	if (sp >= USRSTACK - ptoa(vm->vm_ssize))
d129 1
a129 1
	si = atop(sp - USRSTACK) - vm->vm_ssize + 1;
d131 1
a131 1
	si = atop(USRSTACK - sp) - vm->vm_ssize;
d182 4
a185 2
		if (USRSTACK <= start && start < (USRSTACK + MAXSSIZ)) {
			top = round_page(USRSTACK + ptoa(vm->vm_ssize));
d193 2
a194 1
			top = trunc_page(USRSTACK - ptoa(vm->vm_ssize));
d295 4
a298 3
		if (USRSTACK <= state.start &&
		    state.start < (USRSTACK + MAXSSIZ)) {
			top = round_page(USRSTACK + ptoa(vm->vm_ssize));
d306 2
a307 1
			top = trunc_page(USRSTACK - ptoa(vm->vm_ssize));
@


1.53
log
@Use MAP_INHERIT_* for the 'inh' argument to the UMV_MAPFLAG() macro,
eliminating the must-be-kept-in-sync UVM_INH_* macros

ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.52 2014/11/17 03:15:58 deraadt Exp $	*/
d90 1
a90 1
		    POSIX_MADV_NORMAL, UVM_FLAG_AMAPPAD|UVM_FLAG_FIXED|
@


1.52
log
@instead of PROT_MASK, use PROT_READ | PROT_WRITE | PROT_EXEC to
show the maxprot available in obreak mappings.  (the default remains
PROT_READ | PROT_WRITE, so don't be afraid).
ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.51 2014/11/16 12:31:01 deraadt Exp $	*/
d89 1
a89 1
		    PROT_READ | PROT_WRITE | PROT_EXEC, UVM_INH_COPY,
@


1.51
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.50 2014/07/11 16:35:40 jsg Exp $	*/
d88 2
a89 1
		    UVM_MAPFLAG(PROT_READ | PROT_WRITE, PROT_MASK, UVM_INH_COPY,
@


1.50
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.49 2014/04/13 23:14:15 tedu Exp $	*/
d88 2
a89 2
		    UVM_MAPFLAG(UVM_PROT_RW, UVM_PROT_RWX, UVM_INH_COPY,
		    UVM_ADV_NORMAL, UVM_FLAG_AMAPPAD|UVM_FLAG_FIXED|
d162 1
a162 1
		if (!(entry->protection & VM_PROT_WRITE) &&
d271 1
a271 1
		if (!(entry->protection & VM_PROT_WRITE) &&
@


1.49
log
@compress code by turning four line comments into one line comments.
emphatic ok usual suspects, grudging ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.48 2014/03/26 05:23:42 guenther Exp $	*/
d23 1
a23 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed by Charles D. Cranor,
 *	Washington University, the University of California, Berkeley and 
 *	its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.48
log
@Move p_emul and p_sigcode from proc to process.
Tweak the handling of ktrace EMUL when changing ktracing: only
generate one per process (not one per thread) and pass the correct
proc pointer down to the VFS layer.  Permit generating of NAMI and
CSW records inside ktrace(2) itself.

ok deraadt@@ millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.47 2013/01/16 21:47:08 deraadt Exp $	*/
d89 1
a89 3
	/*
	 * grow or shrink?
	 */
a112 1

d119 1
a119 3
	/*
	 * For user defined stacks (from sendsig).
	 */
d123 1
a123 3
	/*
	 * For common case of already allocated (from trap).
	 */
d131 1
a131 3
	/*
	 * Really need to check vs limit and increment stack size if ok.
	 */
d171 1
a171 3
		/*
		 * Don't dump mmaped devices.
		 */
d206 1
a206 3
		/*
		 * Set up a new core file segment.
		 */
d280 1
a280 3
		/*
		 * Don't dump mmaped devices.
		 */
@


1.47
log
@in uvm_coredump, use RB_FOREACH_SAFE because we are torturing the map
inside the loop.  Fixes a.out coredumps for miod, solution from guenther.
ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.46 2013/01/16 00:24:33 deraadt Exp $	*/
d177 1
a177 1
		    entry->start != p->p_sigcode)
d290 1
a290 1
		    entry->start != p->p_sigcode)
@


1.46
log
@oops, one IO_NODELOCKED left behind in the a.out coredumper
ok guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.45 2013/01/15 02:03:38 deraadt Exp $	*/
d162 1
a162 1
	vm_map_entry_t entry;
d170 1
a170 1
	RB_FOREACH(entry, uvm_map_addr, &map->addr) {
@


1.45
log
@Allow SIGKILL to terminate coredumping processes.  Semantics decided
with kettenis guenther and beck.
ok guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.44 2013/01/15 01:34:27 deraadt Exp $	*/
d226 1
a226 2
		    offset, UIO_SYSSPACE,
		    IO_NODELOCKED|IO_UNIT, cred, NULL, p);
@


1.44
log
@Slice & dice coredump write requests into MAXPHYS blocks, and
yield between operations.  Re-grab the vnode every operation,
so that multiple coredumps can be saved at the same time.
ok guenther beck etc
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.43 2012/03/09 13:01:29 ariane Exp $	*/
d240 3
@


1.43
log
@New vmmap implementation.

no oks (it is really a pain to review properly)
extensively tested, I'm confident it'll be stable
'now is the time' from several icb inhabitants

Diff provides:
- ability to specify different allocators for different regions/maps
- a simpler implementation of the current allocator
- currently in compatibility mode: it will generate similar addresses
  as the old allocator
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.42 2011/06/06 17:10:23 ariane Exp $	*/
d165 2
a166 2
	off_t offset;
	int flag, error = 0;
d236 18
a253 7
		error = vn_rdwr(UIO_WRITE, vp,
		    (caddr_t)(u_long)cseg.c_addr, (int)cseg.c_size,
		    offset, UIO_USERSPACE,
		    IO_NODELOCKED|IO_UNIT, cred, NULL, p);
		if (error)
			break;
		
d255 4
@


1.42
log
@Backout vmmap in order to repair virtual address selection algorithms
outside the tree.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.41 2011/05/24 15:27:36 ariane Exp $	*/
d170 1
a170 3
	for (entry = map->header.next; entry != &map->header;
	    entry = entry->next) {

d262 1
a262 3
	for (entry = map->header.next; entry != &map->header;
	    entry = entry->next) {

@


1.41
log
@Reimplement uvm/uvm_map.

vmmap is designed to perform address space randomized allocations,
without letting fragmentation of the address space go through the roof.

Some highlights:
- kernel address space randomization
- proper implementation of guardpages
- roughly 10% system time reduction during kernel build

Tested by alot of people on tech@@ and developers.
Theo's machines are still happy.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.40 2009/11/24 10:35:56 otto Exp $	*/
d170 3
a172 1
	RB_FOREACH(entry, uvm_map_addr, &map->addr) {
d264 3
a266 1
	RB_FOREACH(entry, uvm_map_addr, &map->addr) {
@


1.40
log
@Proper check for wrapping high address and setting the break below the
base of data; with nicm@@ ok miod@@ guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.39 2009/06/17 22:19:12 kettenis Exp $	*/
d170 1
a170 3
	for (entry = map->header.next; entry != &map->header;
	    entry = entry->next) {

d262 1
a262 3
	for (entry = map->header.next; entry != &map->header;
	    entry = entry->next) {

@


1.39
log
@Recommit part of miod's no-coredumps-on-ramdisks diff that got lost in the
big uvm backout mess.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.38 2009/06/17 00:13:59 oga Exp $	*/
d76 1
a76 1
	vaddr_t new, old;
d79 1
a79 1
	old = (vaddr_t)vm->vm_daddr;
d81 1
a81 1
	if ((new - old) > p->p_rlimit[RLIMIT_DATA].rlim_cur)
d84 1
a84 1
	old = round_page(old + ptoa(vm->vm_dsize));
@


1.38
log
@date based reversion of uvm to the 4th May.

More backouts in line with previous ones, this appears to bring us back to a
stable condition.

A machine forced to 64mb of ram cycled 10GB through swap with this diff
and is still running as I type this. Other tests by ariane@@ and thib@@
also seem to show that it's alright.

ok deraadt@@, thib@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.36 2009/03/20 15:19:04 oga Exp $	*/
d150 2
d325 2
@


1.37
log
@We need miod's no-coredumps-on-ramdisks diff, because we have grown the
media just a wee bit too much.
@
text
@a149 2
#ifndef SMALL_KERNEL

a322 2

#endif	/* !SMALL_KERNEL */
@


1.36
log
@While working on some stuff in uvm I've gotten REALLY sick of reading
K&R function declarations, so switch them all over to ansi-style, in
accordance with the prophesy.

"go for it" art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.35 2009/03/05 19:52:24 kettenis Exp $	*/
d150 2
d325 2
@


1.35
log
@Make ELF platforms generate ELF core dumps.  Somewhat based on code from
NetBSD.

ok kurt@@, drahn@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.34 2008/03/02 20:29:20 kettenis Exp $	*/
d70 1
a70 4
sys_obreak(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
d117 1
a117 3
uvm_grow(p, sp)
	struct proc *p;
	vaddr_t sp;
d155 2
a156 5
uvm_coredump(p, vp, cred, chdr)
	struct proc *p;
	struct vnode *vp;
	struct ucred *cred;
	struct core *chdr;
@


1.34
log
@Include sigcode in core dumps.  This makes it possible to backtrace
through signal handlers with gdb.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.33 2008/02/27 21:46:34 kettenis Exp $	*/
d258 73
@


1.33
log
@Don't dump duplicate data in stack segments of core dumps when the stack
memory map is fragmented.  Avoids ridiculously large core dumps.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.32 2008/01/05 00:36:13 miod Exp $	*/
d184 2
a185 1
		if (!(entry->protection & VM_PROT_WRITE))
@


1.32
log
@Mark vadvise(2) as obsolete and remove its implementation, so instead of
returning EINVAL, you'll get ENOSYS. No serious code has used this system
call in at least fifteen years.

The libc stub will be removed at the next major crank time.

ok henning@@ deraadt@@ krw@@ toby@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.31 2007/12/16 14:32:02 kettenis Exp $	*/
d169 1
a169 1
	vaddr_t start, end;
d205 4
a208 1
			end = round_page(USRSTACK + ptoa(vm->vm_ssize));
a210 1
			start = USRSTACK;
d213 3
a215 1
			start = trunc_page(USRSTACK - ptoa(vm->vm_ssize));
@


1.31
log
@Correctly calculate stack increment for MACHINE_STACK_GROWS_UP; fixes problems
where core dumps on hppa were missing the last stack page.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.30 2007/09/15 10:10:37 martin Exp $	*/
a152 20
}

/*
 * sys_oadvise: old advice system call
 */

/* ARGSUSED */
int
sys_ovadvise(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{
#if 0
	struct sys_ovadvise_args /* {
		syscallarg(int) anom;
	} */ *uap = v;
#endif

	return (EINVAL);
@


1.30
log
@replace ctob and btoc with ptoa and atop respectively

help and ok miod@@ thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.29 2007/09/01 15:14:44 martin Exp $	*/
d147 1
a147 1
	si = atop(sp - USRSTACK) - vm->vm_ssize;
@


1.29
log
@replace the machine dependant bytes-to-clicks macro by the MI ptoa()
version for i386

more architectures and ctob() replacement is being worked on

prodded by and ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.28 2007/04/11 12:51:51 miod Exp $	*/
d147 1
a147 1
	si = btoc(sp - USRSTACK) - vm->vm_ssize;
d149 1
a149 1
	si = btoc(USRSTACK - sp) - vm->vm_ssize;
d151 1
a151 1
	if (vm->vm_ssize + si <= btoc(p->p_rlimit[RLIMIT_STACK].rlim_cur))
@


1.28
log
@The return value of uvm_grow() (and previously, grow()) has not been used
in 15 years, make it a void function.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.27 2005/07/26 07:11:55 art Exp $	*/
d137 1
a137 1
	if (sp < USRSTACK + ctob(vm->vm_ssize))
d139 1
a139 1
	if (sp >= USRSTACK - ctob(vm->vm_ssize))
d225 1
a225 1
			end = round_page(USRSTACK + ctob(vm->vm_ssize));
d231 1
a231 1
			start = trunc_page(USRSTACK - ctob(vm->vm_ssize));
@


1.27
log
@ - Make a UVM_OBJ_IS_DEVICE macro.
 - Use it to skip device mappings while dumping core.
 - Ignore EFAULT errors while dumping core since they can happen
   even for valid mappings. Just skip that part of the core file and
   let it get automagically zero-filled.

This fixes the broken X core dumps that people have been seeing and also
fixes some other potential problems that could prevent core dumps (mmaps
beyond EOF, etc.).

tedu@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.26 2003/03/04 18:24:05 mickey Exp $	*/
d119 1
a119 1
int
d131 1
a131 1
		return (0);
d141 1
a141 1
		return (1);
d149 1
a149 1
	si = btoc(USRSTACK-sp) - vm->vm_ssize;
d151 2
a152 4
	if (vm->vm_ssize + si > btoc(p->p_rlimit[RLIMIT_STACK].rlim_cur))
		return (0);
	vm->vm_ssize += si;
	return (1);
@


1.26
log
@do not treat map entries above the stack as stack; only for the grow-ups
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.25 2003/02/18 18:50:54 mickey Exp $	*/
d209 7
d253 5
a257 1
		if (error)
@


1.25
log
@fix the way stack is written into the core file on the upward growing stack machines. the other case is not affected. miod@@ deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.24 2002/12/20 03:01:00 mickey Exp $	*/
d219 1
a219 1
		if (start >= USRSTACK) {
@


1.24
log
@user-defined stacks check is the same for grownups as for growndowns
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.23 2002/12/19 00:57:07 mickey Exp $	*/
d191 1
a191 1
	vaddr_t start, end, maxstack;
a196 1
	maxstack = trunc_page(USRSTACK - ctob(vm->vm_ssize));
d220 3
d227 3
a231 2
			if (start >= end)
				continue;
@


1.23
log
@simplify stack grownups (growndowns are not touched)
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.22 2002/10/29 18:30:21 art Exp $	*/
a129 3
#ifdef MACHINE_STACK_GROWS_UP
	if (sp > (vaddr_t)vm->vm_minsaddr)
#else
a130 1
#endif
@


1.22
log
@Since memory deallocation can't fail, remove the error return from
uvm_unmap, uvm_deallocate and a few other functions.
Simplifies some code and reduces diff to the UBC branch.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.21 2002/08/23 11:26:57 art Exp $	*/
d131 1
a131 1
	if (sp > (vaddr_t)vm->vm_maxsaddr)
@


1.21
log
@map the heap without PROT_EXEC.
deraadt@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.20 2001/12/19 08:58:07 art Exp $	*/
d80 1
a80 2
	ssize_t diff;
	int rv;
a87 1
	diff = new - old;
d89 1
a89 1
	if (diff == 0)
d95 4
a98 3
	if (diff > 0) {
		rv = uvm_map(&vm->vm_map, &old, diff, NULL, UVM_UNKNOWN_OFFSET,
		    0, UVM_MAPFLAG(UVM_PROT_RW, UVM_PROT_RWX, UVM_INH_COPY,
d101 4
a104 3
		if (rv == KERN_SUCCESS) {
			vm->vm_dsize += atop(diff);
			return (0);
d106 1
d108 2
a109 5
		rv = uvm_deallocate(&vm->vm_map, new, -diff);
		if (rv == KERN_SUCCESS) {
			vm->vm_dsize -= atop(-diff);
			return (0);
		}
d112 1
a112 4
	uprintf("sbrk: %s %ld failed, return = %d\n",
	    diff > 0 ? "grow" : "shrink",
	    (long)(diff > 0 ? diff : -diff), rv);
	return (ENOMEM);
@


1.20
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.17 2001/11/07 02:55:51 art Exp $	*/
d99 1
a99 1
		    0, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL, UVM_INH_COPY,
@


1.19
log
@Sync in more uvm from NetBSD. Mostly just cosmetic stuff.
Contains also support for page coloring.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_unix.c,v 1.18 2001/11/28 13:47:40 art Exp $	*/
/*	$NetBSD: uvm_unix.c,v 1.24 2001/06/06 21:28:51 mrg Exp $	*/
d6 1
a6 1
 * Copyright (c) 1991, 1993 The Regents of the University of California.
d26 1
a26 1
 *	Washington University, the University of California, Berkeley and
d80 2
a81 1
	int error;
d85 1
a85 1
	if ((new - old) > p->p_rlimit[RLIMIT_DATA].rlim_cur && new > old)
d89 1
d91 1
a91 1
	if (new == old)
d97 3
a99 4
	if (new > old) {
		error = uvm_map(&vm->vm_map, &old, new - old, NULL,
		    UVM_UNKNOWN_OFFSET, 0,
		    UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL, UVM_INH_COPY,
d102 3
a104 4
		if (error) {
			uprintf("sbrk: grow %ld failed, error = %d\n",
				new - old, error);
			return error;
a105 1
		vm->vm_dsize += atop(new - old);
d107 5
a111 2
		uvm_deallocate(&vm->vm_map, new, old - new);
		vm->vm_dsize -= atop(old - new);
d114 4
a117 1
	return (0);
d198 2
a199 2
	struct vm_map *map = &vm->vm_map;
	struct vm_map_entry *entry;
@


1.19.2.1
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_unix.c,v 1.19 2001/11/28 19:28:15 art Exp $	*/
/*	$NetBSD: uvm_unix.c,v 1.26 2001/12/08 00:35:34 thorpej Exp $	*/
d111 2
a112 1
	return 0;
d257 1
a257 1

d264 1
@


1.19.2.2
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.19.2.1 2002/02/02 03:28:27 art Exp $	*/
d95 4
a98 3
	if (diff > 0) {
		rv = uvm_map(&vm->vm_map, &old, diff, NULL, UVM_UNKNOWN_OFFSET,
		    0, UVM_MAPFLAG(UVM_PROT_RW, UVM_PROT_RWX, UVM_INH_COPY,
@


1.19.2.3
log
@Fix merge botches.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.19.2.2 2002/10/29 00:36:50 art Exp $	*/
d95 3
a97 4
	if (new > old) {
		error = uvm_map(&vm->vm_map, &old, new - old, NULL,
		    UVM_UNKNOWN_OFFSET, 0,
		    UVM_MAPFLAG(UVM_PROT_RW, UVM_PROT_RWX, UVM_INH_COPY,
d102 2
a103 2
			    new - old, error);
			return (error);
@


1.19.2.4
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d111 1
a111 1
	return (0);
d129 3
d133 1
d194 1
a194 1
	vaddr_t start, end;
d200 1
d223 1
a223 4
		if (USRSTACK <= start && start < (USRSTACK + MAXSSIZ)) {
			end = round_page(USRSTACK + ctob(vm->vm_ssize));
			if (start >= end)
				continue;
d228 2
a229 1

a231 2
#endif
			flag = CORE_STACK;
@


1.18
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_unix.c,v 1.17 2001/11/07 02:55:51 art Exp $	*/
/*	$NetBSD: uvm_unix.c,v 1.20 2001/03/19 02:25:33 simonb Exp $	*/
d6 1
a6 1
 * Copyright (c) 1991, 1993 The Regents of the University of California.  
d26 1
a26 1
 *	Washington University, the University of California, Berkeley and 
a79 1
	ssize_t diff;
d84 1
a84 1
	if ((new - old) > p->p_rlimit[RLIMIT_DATA].rlim_cur)
a87 1
	diff = new - old;
d89 1
a89 1
	if (diff == 0)
d95 2
a96 2
	if (diff > 0) {
		error = uvm_map(&vm->vm_map, &old, diff, NULL,
d103 1
a103 1
				(long)diff, error);
d106 1
a106 1
		vm->vm_dsize += atop(diff);
d108 2
a109 2
		uvm_deallocate(&vm->vm_map, new, -diff);
		vm->vm_dsize -= atop(-diff);
d193 2
a194 2
	vm_map_t map = &vm->vm_map;
	vm_map_entry_t entry;
@


1.17
log
@Another sync of uvm to NetBSD. Just minor fiddling, no major changes.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_unix.c,v 1.16 2001/11/07 01:18:01 art Exp $	*/
/*	$NetBSD: uvm_unix.c,v 1.18 2000/09/13 15:00:25 thorpej Exp $	*/
d81 1
a81 1
	int rv;
d98 3
a100 2
		rv = uvm_map(&vm->vm_map, &old, diff, NULL, UVM_UNKNOWN_OFFSET,
		    0, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL, UVM_INH_COPY,
d103 4
a106 3
		if (rv == KERN_SUCCESS) {
			vm->vm_dsize += atop(diff);
			return (0);
d108 1
d110 2
a111 5
		rv = uvm_deallocate(&vm->vm_map, new, -diff);
		if (rv == KERN_SUCCESS) {
			vm->vm_dsize -= atop(-diff);
			return (0);
		}
d114 1
a114 4
	uprintf("sbrk: %s %ld failed, return = %d\n",
	    diff > 0 ? "grow" : "shrink",
	    (long)(diff > 0 ? diff : -diff), rv);
	return (ENOMEM);
@


1.16
log
@Add an alignment argument to uvm_map that specifies an alignment hint
for the virtual address.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.15 2001/11/06 13:36:52 art Exp $	*/
d86 1
a86 1
		return(ENOMEM);
d115 3
a117 3
		diff > 0 ? "grow" : "shrink",
		(long)(diff > 0 ? diff : -diff), rv);
	return(ENOMEM);
@


1.15
log
@More sync to NetBSD.
 - Use malloc/free instead of MALLOC/FREE for variable sized allocations.
 - Move the memory inheritance code to sys/mman.h and rename from VM_* to MAP_*
 - various cleanups and simplifications.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_unix.c,v 1.14 2001/11/06 01:35:04 art Exp $	*/
/*	$NetBSD: uvm_unix.c,v 1.17 2000/09/07 05:01:43 chs Exp $	*/
d99 1
a99 1
		    UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL, UVM_INH_COPY,
@


1.14
log
@Move the last content from vm/ to uvm/
The only thing left in vm/ are just dumb wrappers.
vm/vm.h includes uvm/uvm_extern.h
vm/pmap.h includes uvm/uvm_pmap.h
vm/vm_page.h includes uvm/uvm_page.h
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_unix.c,v 1.13 2001/08/11 10:57:22 art Exp $	*/
/*	$NetBSD: uvm_unix.c,v 1.13 2000/06/27 17:29:36 mrg Exp $	*/
d80 1
a81 1
	long diff;
d200 1
a200 1
	vaddr_t start, end;
d206 1
d257 1
a257 1
		    (caddr_t)cseg.c_addr, (int)cseg.c_size,
@


1.13
log
@Various random fixes from NetBSD.
Including support for zeroing pages in the idle loop (not enabled yet).
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_unix.c,v 1.12 2001/08/06 14:03:05 art Exp $	*/
/*	$NetBSD: uvm_unix.c,v 1.12 2000/03/30 12:31:50 augustss Exp $	*/
a62 1
#include <vm/vm.h>
a63 1

@


1.12
log
@Add a new type voff_t (right now it's typedefed as off_t) used for offsets
into objects.

Gives the possibilty to mmap beyond the size of vaddr_t.

From NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_unix.c,v 1.11 2001/07/25 14:47:59 art Exp $	*/
/*	$NetBSD: uvm_unix.c,v 1.11 2000/03/26 20:54:47 kleink Exp $	*/
d131 2
a132 2
	register struct vmspace *vm = p->p_vmspace;
	register int si;
d199 3
a201 3
	register struct vmspace *vm = p->p_vmspace;
	register vm_map_t map = &vm->vm_map;
	register vm_map_entry_t entry;
@


1.11
log
@Some updates to UVM from NetBSD. Nothing really critical, just a sync.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_unix.c,v 1.10 2001/05/05 21:26:46 art Exp $	*/
/*	$NetBSD: uvm_unix.c,v 1.10 1999/12/30 16:09:47 eeh Exp $	*/
@


1.10
log
@Remove the (vaddr_t) casts inside the round_page and trunc_page macros.
We might want to use them on types that are bigger than vaddr_t.

Fix all callers that pass pointers without casts.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_unix.c,v 1.9 2001/05/05 20:57:03 art Exp $	*/
/*	$NetBSD: uvm_unix.c,v 1.8 1999/03/25 18:48:56 mrg Exp $	*/
@


1.9
log
@Get rid of CLSIZE and all related stuff.
CLSIZE -> 1
CLBYTES -> PAGE_SIZE
OLOFSET -> PAGE_MASK
etc.
At the same time some archs needed some cleaning in vmparam.h so that
goes in at the same time.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.8 2001/01/29 02:07:50 niklas Exp $	*/
d86 1
a86 1
	new = round_page(SCARG(uap, nsize));
@


1.8
log
@$OpenBSD$
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.8 1999/03/25 18:48:56 mrg Exp $	*/
d158 1
a158 1
	si = clrnd(btoc(sp - USRSTACK) - vm->vm_ssize);
d160 1
a160 1
	si = clrnd(btoc(USRSTACK-sp) - vm->vm_ssize);
@


1.7
log
@Fix the latest fix.
Only change vm_dsize if the allocation succeeded.
From Jason Thorpe.
@
text
@d1 1
@


1.6
log
@Don't cast to int when checking if we have exceeded our rlimit in sbrk.
Plus misc cleanup.
@
text
@d86 1
a86 1
	if ((rlim_t)(new - old) > p->p_rlimit[RLIMIT_DATA].rlim_cur)
d102 5
a106 1
		    UVM_FLAG_OVERLAY|UVM_FLAG_COPYONW)); 
d109 4
d115 4
a118 9
	vm->vm_dsize += atop(diff);

	if (rv != KERN_SUCCESS) {
		uprintf("sbrk: %s failed, return = %d\n",
			diff > 0 ? "grow" : "shrink", rv);
		return(ENOMEM);
	}

	return (0);
@


1.5
log
@Fix the NetBSD id strings.
@
text
@d79 1
a79 1
	register struct vmspace *vm = p->p_vmspace;
d82 1
a82 1
	register int diff;
d86 1
a86 1
	if ((int)(new - old) > p->p_rlimit[RLIMIT_DATA].rlim_cur)
d89 1
a89 1
	old = round_page(old + ctob(vm->vm_dsize));
d92 3
a97 1

a98 1

d103 3
d107 1
a107 15
		if (rv != KERN_SUCCESS) {
			uprintf("sbrk: grow failed, return = %d\n", rv);
			return(ENOMEM);
		}
		vm->vm_dsize += btoc(diff);

	} else if (diff < 0) {

		diff = -diff;
		rv = uvm_deallocate(&vm->vm_map, new, diff);
		if (rv != KERN_SUCCESS) {
			uprintf("sbrk: shrink failed, return = %d\n", rv);
			return(ENOMEM);
		}
		vm->vm_dsize -= btoc(diff);
d109 4
d114 2
a115 1
	return(0);
@


1.5.2.1
log
@Pull in patch from current:
Fix (art):
Don't cast to int when checking if we have exceeded our rlimit in sbrk.
Plus misc cleanup.
@
text
@d79 1
a79 1
	struct vmspace *vm = p->p_vmspace;
d82 1
a82 1
	long diff;
d86 1
a86 1
	if ((rlim_t)(new - old) > p->p_rlimit[RLIMIT_DATA].rlim_cur)
d89 1
a89 1
	old = round_page(old + ptoa(vm->vm_dsize));
a91 3
	if (diff == 0)
		return (0);

d95 1
d97 1
a101 3
	} else {
		rv = uvm_deallocate(&vm->vm_map, new, -diff);
	}
d103 15
a117 1
	vm->vm_dsize += atop(diff);
a118 4
	if (rv != KERN_SUCCESS) {
		uprintf("sbrk: %s failed, return = %d\n",
			diff > 0 ? "grow" : "shrink", rv);
		return(ENOMEM);
d120 1
a120 2

	return (0);
@


1.5.2.2
log
@Pull in patch from current:
Fix (art):
Fix the latest fix.
Only change vm_dsize if the allocation succeeded.
From Jason Thorpe.
@
text
@d86 1
a86 1
	if ((new - old) > p->p_rlimit[RLIMIT_DATA].rlim_cur)
d102 1
a102 5
		    UVM_FLAG_OVERLAY|UVM_FLAG_COPYONW));
		if (rv == KERN_SUCCESS) {
			vm->vm_dsize += atop(diff);
			return (0);
		}
a104 4
		if (rv == KERN_SUCCESS) {
			vm->vm_dsize -= atop(-diff);
			return (0);
		}
d107 9
a115 4
	uprintf("sbrk: %s %ld failed, return = %d\n",
		diff > 0 ? "grow" : "shrink",
		(long)(diff > 0 ? diff : -diff), rv);
	return(ENOMEM);
@


1.4
log
@sync with NetBSD from 1999.05.24 (there is a reason for this date)
 Mostly cleanups, but also a few improvements to pagedaemon for better
 handling of low memory and/or low swap conditions.
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_unix.c,v 1.7 1998/10/11 23:18:21 chuck Exp $	*/
@


1.4.4.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_unix.c,v 1.8 1999/03/25 18:48:56 mrg Exp $	*/
@


1.4.4.2
log
@merge in approximately 2.9 into SMP branch
@
text
@a0 1
/*	$OpenBSD: uvm_unix.c,v 1.8 2001/01/29 02:07:50 niklas Exp $	*/
d79 1
a79 1
	struct vmspace *vm = p->p_vmspace;
d82 1
a82 1
	long diff;
d86 1
a86 1
	if ((new - old) > p->p_rlimit[RLIMIT_DATA].rlim_cur)
d89 1
a89 1
	old = round_page(old + ptoa(vm->vm_dsize));
a91 3
	if (diff == 0)
		return (0);

d95 1
d97 1
d101 5
a105 4
		    UVM_FLAG_OVERLAY|UVM_FLAG_COPYONW));
		if (rv == KERN_SUCCESS) {
			vm->vm_dsize += atop(diff);
			return (0);
d107 9
a115 5
	} else {
		rv = uvm_deallocate(&vm->vm_map, new, -diff);
		if (rv == KERN_SUCCESS) {
			vm->vm_dsize -= atop(-diff);
			return (0);
d117 2
d120 1
a120 5

	uprintf("sbrk: %s %ld failed, return = %d\n",
		diff > 0 ? "grow" : "shrink",
		(long)(diff > 0 ? diff : -diff), rv);
	return(ENOMEM);
@


1.4.4.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_unix.c,v 1.4.4.2 2001/05/14 22:47:49 niklas Exp $	*/
d86 1
a86 1
	new = round_page((vaddr_t)SCARG(uap, nsize));
d158 1
a158 1
	si = btoc(sp - USRSTACK) - vm->vm_ssize;
d160 1
a160 1
	si = btoc(USRSTACK-sp) - vm->vm_ssize;
@


1.4.4.4
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_unix.c,v 1.12 2000/03/30 12:31:50 augustss Exp $	*/
d131 2
a132 2
	struct vmspace *vm = p->p_vmspace;
	int si;
d199 3
a201 3
	struct vmspace *vm = p->p_vmspace;
	vm_map_t map = &vm->vm_map;
	vm_map_entry_t entry;
@


1.4.4.5
log
@merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_unix.c,v 1.18 2000/09/13 15:00:25 thorpej Exp $	*/
d63 1
d66 1
a81 1
	ssize_t diff;
d83 1
d88 1
a88 1
		return (ENOMEM);
d101 1
a101 1
		    0, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL, UVM_INH_COPY,
d117 3
a119 3
	    diff > 0 ? "grow" : "shrink",
	    (long)(diff > 0 ? diff : -diff), rv);
	return (ENOMEM);
d202 1
a202 1
	vaddr_t start, end, maxstack;
a207 1
	maxstack = trunc_page(USRSTACK - ctob(vm->vm_ssize));
d258 1
a258 1
		    (caddr_t)(u_long)cseg.c_addr, (int)cseg.c_size,
@


1.4.4.6
log
@Merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_unix.c,v 1.24 2001/06/06 21:28:51 mrg Exp $	*/
d6 1
a6 1
 * Copyright (c) 1991, 1993 The Regents of the University of California.
d26 1
a26 1
 *	Washington University, the University of California, Berkeley and
d80 2
a81 1
	int error;
d85 1
a85 1
	if ((new - old) > p->p_rlimit[RLIMIT_DATA].rlim_cur && new > old)
d89 1
d91 1
a91 1
	if (new == old)
d97 3
a99 4
	if (new > old) {
		error = uvm_map(&vm->vm_map, &old, new - old, NULL,
		    UVM_UNKNOWN_OFFSET, 0,
		    UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL, UVM_INH_COPY,
d102 3
a104 4
		if (error) {
			uprintf("sbrk: grow %ld failed, error = %d\n",
				new - old, error);
			return error;
a105 1
		vm->vm_dsize += atop(new - old);
d107 5
a111 2
		uvm_deallocate(&vm->vm_map, new, old - new);
		vm->vm_dsize -= atop(old - new);
d114 4
a117 1
	return (0);
d198 2
a199 2
	struct vm_map *map = &vm->vm_map;
	struct vm_map_entry *entry;
@


1.4.4.7
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_unix.c,v 1.18 2000/09/13 15:00:25 thorpej Exp $	*/
d6 1
a6 1
 * Copyright (c) 1991, 1993 The Regents of the University of California.  
d26 1
a26 1
 *	Washington University, the University of California, Berkeley and 
d80 1
a80 2
	ssize_t diff;
	int rv;
d84 1
a84 1
	if ((new - old) > p->p_rlimit[RLIMIT_DATA].rlim_cur)
a87 1
	diff = new - old;
d89 1
a89 1
	if (diff == 0)
d95 4
a98 3
	if (diff > 0) {
		rv = uvm_map(&vm->vm_map, &old, diff, NULL, UVM_UNKNOWN_OFFSET,
		    0, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL, UVM_INH_COPY,
d101 4
a104 3
		if (rv == KERN_SUCCESS) {
			vm->vm_dsize += atop(diff);
			return (0);
d106 1
d108 2
a109 5
		rv = uvm_deallocate(&vm->vm_map, new, -diff);
		if (rv == KERN_SUCCESS) {
			vm->vm_dsize -= atop(-diff);
			return (0);
		}
d112 1
a112 4
	uprintf("sbrk: %s %ld failed, return = %d\n",
	    diff > 0 ? "grow" : "shrink",
	    (long)(diff > 0 ? diff : -diff), rv);
	return (ENOMEM);
d193 2
a194 2
	vm_map_t map = &vm->vm_map;
	vm_map_entry_t entry;
@


1.4.4.8
log
@Sync the SMP branch with 3.3
@
text
@d80 2
a81 1
	int error;
d89 1
d91 1
a91 1
	if (new == old)
d97 3
a99 4
	if (new > old) {
		error = uvm_map(&vm->vm_map, &old, new - old, NULL,
		    UVM_UNKNOWN_OFFSET, 0,
		    UVM_MAPFLAG(UVM_PROT_RW, UVM_PROT_RWX, UVM_INH_COPY,
d102 3
a104 4
		if (error) {
			uprintf("sbrk: grow %ld failed, error = %d\n",
			    new - old, error);
			return (ENOMEM);
a105 1
		vm->vm_dsize += atop(new - old);
d107 5
a111 2
		uvm_deallocate(&vm->vm_map, new, old - new);
		vm->vm_dsize -= atop(old - new);
d114 4
a117 1
	return (0);
d135 3
d139 1
d200 1
a200 1
	vaddr_t start, end;
d206 1
d229 1
a229 4
		if (USRSTACK <= start && start < (USRSTACK + MAXSSIZ)) {
			end = round_page(USRSTACK + ctob(vm->vm_ssize));
			if (start >= end)
				continue;
d234 2
a235 1

a237 2
#endif
			flag = CORE_STACK;
@


1.3
log
@accomodations for backward growing stack architectures; art@@ k
@
text
@a0 1
/*	$OpenBSD: uvm_unix.c,v 1.2 1999/02/26 05:32:08 art Exp $	*/
a2 4
/*
 * XXXCDC: "ROUGH DRAFT" QUALITY UVM PRE-RELEASE FILE!   
 *         >>>USE AT YOUR OWN RISK, WORK IS NOT FINISHED<<<
 */
@


1.2
log
@add OpenBSD tags
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d143 3
d147 1
d153 3
d157 1
d163 3
d167 1
d235 4
d240 2
a242 1
			start = trunc_page(USRSTACK - ctob(vm->vm_ssize));
@


1.1
log
@Import of uvm from NetBSD. Some local changes, some code disabled
@
text
@d1 1
@

