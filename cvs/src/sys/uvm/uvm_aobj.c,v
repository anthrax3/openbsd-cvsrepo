head	1.85;
access;
symbols
	OPENBSD_6_0:1.81.0.2
	OPENBSD_6_0_BASE:1.81
	OPENBSD_5_9:1.80.0.2
	OPENBSD_5_9_BASE:1.80
	OPENBSD_5_8:1.79.0.4
	OPENBSD_5_8_BASE:1.79
	OPENBSD_5_7:1.78.0.2
	OPENBSD_5_7_BASE:1.78
	OPENBSD_5_6:1.67.0.4
	OPENBSD_5_6_BASE:1.67
	OPENBSD_5_5:1.60.0.4
	OPENBSD_5_5_BASE:1.60
	OPENBSD_5_4:1.58.0.2
	OPENBSD_5_4_BASE:1.58
	OPENBSD_5_3:1.55.0.4
	OPENBSD_5_3_BASE:1.55
	OPENBSD_5_2:1.55.0.2
	OPENBSD_5_2_BASE:1.55
	OPENBSD_5_1_BASE:1.54
	OPENBSD_5_1:1.54.0.4
	OPENBSD_5_0:1.54.0.2
	OPENBSD_5_0_BASE:1.54
	OPENBSD_4_9:1.51.0.4
	OPENBSD_4_9_BASE:1.51
	OPENBSD_4_8:1.51.0.2
	OPENBSD_4_8_BASE:1.51
	OPENBSD_4_7:1.47.0.2
	OPENBSD_4_7_BASE:1.47
	OPENBSD_4_6:1.45.0.4
	OPENBSD_4_6_BASE:1.45
	OPENBSD_4_5:1.35.0.6
	OPENBSD_4_5_BASE:1.35
	OPENBSD_4_4:1.35.0.4
	OPENBSD_4_4_BASE:1.35
	OPENBSD_4_3:1.35.0.2
	OPENBSD_4_3_BASE:1.35
	OPENBSD_4_2:1.34.0.2
	OPENBSD_4_2_BASE:1.34
	OPENBSD_4_1:1.31.0.4
	OPENBSD_4_1_BASE:1.31
	OPENBSD_4_0:1.31.0.2
	OPENBSD_4_0_BASE:1.31
	OPENBSD_3_9:1.29.0.2
	OPENBSD_3_9_BASE:1.29
	OPENBSD_3_8:1.27.0.4
	OPENBSD_3_8_BASE:1.27
	OPENBSD_3_7:1.27.0.2
	OPENBSD_3_7_BASE:1.27
	OPENBSD_3_6:1.26.0.12
	OPENBSD_3_6_BASE:1.26
	SMP_SYNC_A:1.26
	SMP_SYNC_B:1.26
	OPENBSD_3_5:1.26.0.10
	OPENBSD_3_5_BASE:1.26
	OPENBSD_3_4:1.26.0.8
	OPENBSD_3_4_BASE:1.26
	UBC_SYNC_A:1.26
	OPENBSD_3_3:1.26.0.6
	OPENBSD_3_3_BASE:1.26
	OPENBSD_3_2:1.26.0.4
	OPENBSD_3_2_BASE:1.26
	OPENBSD_3_1:1.26.0.2
	OPENBSD_3_1_BASE:1.26
	UBC_SYNC_B:1.26
	UBC:1.23.0.2
	UBC_BASE:1.23
	OPENBSD_3_0:1.15.0.2
	OPENBSD_3_0_BASE:1.15
	OPENBSD_2_9_BASE:1.9
	OPENBSD_2_9:1.9.0.2
	OPENBSD_2_8:1.6.0.2
	OPENBSD_2_8_BASE:1.6
	OPENBSD_2_7:1.5.0.2
	OPENBSD_2_7_BASE:1.5
	SMP:1.4.0.4
	SMP_BASE:1.4
	kame_19991208:1.4
	OPENBSD_2_6:1.4.0.2
	OPENBSD_2_6_BASE:1.4
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.85
date	2017.01.31.17.08.51;	author dhill;	state Exp;
branches;
next	1.84;
commitid	2SHmhHhbpU2SoS0k;

1.84
date	2016.09.24.18.40.29;	author tedu;	state Exp;
branches;
next	1.83;
commitid	Wmpas8UTc6Zb1Abn;

1.83
date	2016.09.16.02.35.42;	author dlg;	state Exp;
branches;
next	1.82;
commitid	Fei4687v68qad1tP;

1.82
date	2016.09.15.02.00.18;	author dlg;	state Exp;
branches;
next	1.81;
commitid	RlO92XR575sygHqm;

1.81
date	2016.06.17.10.48.25;	author dlg;	state Exp;
branches;
next	1.80;
commitid	3jj6c8J8OJvfGS9o;

1.80
date	2015.08.21.16.04.35;	author visa;	state Exp;
branches;
next	1.79;
commitid	gglpDr80UKmkkP9A;

1.79
date	2015.05.07.01.55.44;	author jsg;	state Exp;
branches;
next	1.78;
commitid	KhO2CJgSFKm4Q3Hj;

1.78
date	2015.02.08.02.17.08;	author deraadt;	state Exp;
branches;
next	1.77;
commitid	EHHLDpKiLok9N5jM;

1.77
date	2015.02.06.10.58.35;	author deraadt;	state Exp;
branches;
next	1.76;
commitid	QOLWofatH4nZnDlg;

1.76
date	2014.12.23.04.56.47;	author tedu;	state Exp;
branches;
next	1.75;
commitid	Vcm2YxE1LuHhCHfd;

1.75
date	2014.12.18.23.59.28;	author tedu;	state Exp;
branches;
next	1.74;
commitid	LWyhtzv7lzm4NWIM;

1.74
date	2014.12.17.19.42.15;	author tedu;	state Exp;
branches;
next	1.73;
commitid	G4ldVK4QwvfU3tRp;

1.73
date	2014.12.09.07.16.41;	author doug;	state Exp;
branches;
next	1.72;
commitid	tPDMRisjAolmdVN1;

1.72
date	2014.11.21.07.18.44;	author tedu;	state Exp;
branches;
next	1.71;
commitid	o6qWbqLODaVEY9Pv;

1.71
date	2014.11.18.23.55.01;	author krw;	state Exp;
branches;
next	1.70;
commitid	Qb045HZ5OhQfU69H;

1.70
date	2014.11.16.12.31.00;	author deraadt;	state Exp;
branches;
next	1.69;
commitid	yv0ECmCdICvq576h;

1.69
date	2014.09.14.14.17.27;	author jsg;	state Exp;
branches;
next	1.68;
commitid	uzzBR7hz9ncd4O6G;

1.68
date	2014.09.08.19.42.57;	author kettenis;	state Exp;
branches;
next	1.67;
commitid	sDczPJd0iVYHJjS0;

1.67
date	2014.07.12.18.44.01;	author tedu;	state Exp;
branches;
next	1.66;
commitid	bDGgAR6yEQVcVl5u;

1.66
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.65;
commitid	7NtJNW9udCOFtDNM;

1.65
date	2014.07.03.11.38.46;	author kettenis;	state Exp;
branches;
next	1.64;
commitid	jjIkzZBsrJ6Rab4f;

1.64
date	2014.05.08.20.08.50;	author kettenis;	state Exp;
branches;
next	1.63;

1.63
date	2014.04.30.19.25.14;	author kettenis;	state Exp;
branches;
next	1.62;

1.62
date	2014.04.30.16.07.31;	author kettenis;	state Exp;
branches;
next	1.61;

1.61
date	2014.04.13.23.14.15;	author tedu;	state Exp;
branches;
next	1.60;

1.60
date	2013.12.09.08.24.29;	author espie;	state Exp;
branches;
next	1.59;

1.59
date	2013.12.08.21.16.34;	author espie;	state Exp;
branches;
next	1.58;

1.58
date	2013.05.30.16.39.26;	author tedu;	state Exp;
branches;
next	1.57;

1.57
date	2013.05.30.16.29.46;	author tedu;	state Exp;
branches;
next	1.56;

1.56
date	2013.05.30.15.17.59;	author tedu;	state Exp;
branches;
next	1.55;

1.55
date	2012.04.12.14.59.26;	author ariane;	state Exp;
branches;
next	1.54;

1.54
date	2011.07.03.18.34.14;	author oga;	state Exp;
branches;
next	1.53;

1.53
date	2011.05.10.21.48.17;	author oga;	state Exp;
branches;
next	1.52;

1.52
date	2011.05.07.15.31.25;	author oga;	state Exp;
branches;
next	1.51;

1.51
date	2010.07.02.02.08.53;	author syuu;	state Exp;
branches;
next	1.50;

1.50
date	2010.04.30.21.56.39;	author oga;	state Exp;
branches;
next	1.49;

1.49
date	2010.04.30.20.50.53;	author oga;	state Exp;
branches;
next	1.48;

1.48
date	2010.04.25.23.02.22;	author oga;	state Exp;
branches;
next	1.47;

1.47
date	2009.08.06.15.28.14;	author oga;	state Exp;
branches;
next	1.46;

1.46
date	2009.07.22.21.05.37;	author oga;	state Exp;
branches;
next	1.45;

1.45
date	2009.06.16.23.54.57;	author oga;	state Exp;
branches;
next	1.44;

1.44
date	2009.06.16.17.14.14;	author oga;	state Exp;
branches;
next	1.43;

1.43
date	2009.06.16.00.11.29;	author oga;	state Exp;
branches;
next	1.42;

1.42
date	2009.06.06.17.46.44;	author art;	state Exp;
branches;
next	1.41;

1.41
date	2009.06.02.23.00.19;	author oga;	state Exp;
branches;
next	1.40;

1.40
date	2009.06.01.19.54.02;	author oga;	state Exp;
branches;
next	1.39;

1.39
date	2009.05.08.13.50.15;	author ariane;	state Exp;
branches;
next	1.38;

1.38
date	2009.05.05.05.12.17;	author oga;	state Exp;
branches;
next	1.37;

1.37
date	2009.05.02.12.54.42;	author oga;	state Exp;
branches;
next	1.36;

1.36
date	2009.03.20.15.19.04;	author oga;	state Exp;
branches;
next	1.35;

1.35
date	2007.09.07.15.00.20;	author art;	state Exp;
branches;
next	1.34;

1.34
date	2007.04.13.18.57.49;	author art;	state Exp;
branches;
next	1.33;

1.33
date	2007.04.12.18.59.55;	author art;	state Exp;
branches;
next	1.32;

1.32
date	2007.04.04.17.44.45;	author art;	state Exp;
branches;
next	1.31;

1.31
date	2006.07.31.11.51.29;	author mickey;	state Exp;
branches;
next	1.30;

1.30
date	2006.07.26.23.15.55;	author mickey;	state Exp;
branches;
next	1.29;

1.29
date	2005.11.15.21.09.46;	author miod;	state Exp;
branches;
next	1.28;

1.28
date	2005.10.27.18.05.16;	author otto;	state Exp;
branches;
next	1.27;

1.27
date	2004.12.26.21.22.14;	author miod;	state Exp;
branches;
next	1.26;

1.26
date	2002.03.14.01.27.18;	author millert;	state Exp;
branches;
next	1.25;

1.25
date	2002.01.23.00.39.48;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2001.12.19.08.58.07;	author art;	state Exp;
branches;
next	1.23;

1.23
date	2001.11.28.19.28.14;	author art;	state Exp;
branches
	1.23.2.1;
next	1.22;

1.22
date	2001.11.28.13.47.39;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2001.11.27.05.27.12;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2001.11.11.01.16.56;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2001.11.07.02.55.50;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2001.11.06.13.36.52;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2001.11.06.01.35.04;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2001.11.05.22.14.54;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2001.09.11.20.05.26;	author miod;	state Exp;
branches;
next	1.14;

1.14
date	2001.08.11.10.57.22;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2001.08.06.14.03.04;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2001.07.26.19.37.13;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2001.07.18.10.47.05;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2001.06.23.19.24.33;	author smart;	state Exp;
branches;
next	1.9;

1.9
date	2001.03.22.03.05.54;	author smart;	state Exp;
branches;
next	1.8;

1.8
date	2001.03.08.15.21.36;	author smart;	state Exp;
branches;
next	1.7;

1.7
date	2001.01.29.02.07.43;	author niklas;	state Exp;
branches;
next	1.6;

1.6
date	2000.09.07.20.15.28;	author art;	state Exp;
branches;
next	1.5;

1.5
date	2000.03.16.22.11.04;	author art;	state Exp;
branches;
next	1.4;

1.4
date	99.08.23.08.13.23;	author art;	state Exp;
branches
	1.4.4.1;
next	1.3;

1.3
date	99.04.28.09.28.18;	author art;	state Exp;
branches;
next	1.2;

1.2
date	99.02.26.05.32.06;	author art;	state Exp;
branches;
next	1.1;

1.1
date	99.02.26.01.30.11;	author art;	state Exp;
branches;
next	;

1.4.4.1
date	2000.03.24.09.09.48;	author niklas;	state Exp;
branches;
next	1.4.4.2;

1.4.4.2
date	2001.05.14.22.47.44;	author niklas;	state Exp;
branches;
next	1.4.4.3;

1.4.4.3
date	2001.07.04.11.01.01;	author niklas;	state Exp;
branches;
next	1.4.4.4;

1.4.4.4
date	2001.10.31.03.32.14;	author nate;	state Exp;
branches;
next	1.4.4.5;

1.4.4.5
date	2001.11.13.23.02.31;	author niklas;	state Exp;
branches;
next	1.4.4.6;

1.4.4.6
date	2001.12.05.01.19.55;	author niklas;	state Exp;
branches;
next	1.4.4.7;

1.4.4.7
date	2002.03.06.02.17.14;	author niklas;	state Exp;
branches;
next	1.4.4.8;

1.4.4.8
date	2002.03.28.14.54.26;	author niklas;	state Exp;
branches;
next	;

1.23.2.1
date	2002.01.31.22.55.50;	author niklas;	state Exp;
branches;
next	1.23.2.2;

1.23.2.2
date	2002.02.02.03.28.26;	author art;	state Exp;
branches;
next	1.23.2.3;

1.23.2.3
date	2002.06.11.03.33.03;	author art;	state Exp;
branches;
next	1.23.2.4;

1.23.2.4
date	2002.11.04.18.02.32;	author art;	state Exp;
branches;
next	1.23.2.5;

1.23.2.5
date	2002.11.04.20.10.59;	author art;	state Exp;
branches;
next	;


desc
@@


1.85
log
@Sprinkle some free sizes in uvm/

ok stefan@@ visa@@
@
text
@/*	$OpenBSD: uvm_aobj.c,v 1.84 2016/09/24 18:40:29 tedu Exp $	*/
/*	$NetBSD: uvm_aobj.c,v 1.39 2001/02/18 21:19:08 chs Exp $	*/

/*
 * Copyright (c) 1998 Chuck Silvers, Charles D. Cranor and
 *                    Washington University.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 * from: Id: uvm_aobj.c,v 1.1.2.5 1998/02/06 05:14:38 chs Exp
 */
/*
 * uvm_aobj.c: anonymous memory uvm_object pager
 *
 * author: Chuck Silvers <chuq@@chuq.com>
 * started: Jan-1998
 *
 * - design mostly from Chuck Cranor
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/malloc.h>
#include <sys/kernel.h>
#include <sys/pool.h>
#include <sys/stdint.h>
#include <sys/atomic.h>

#include <uvm/uvm.h>

/*
 * an aobj manages anonymous-memory backed uvm_objects.   in addition
 * to keeping the list of resident pages, it also keeps a list of
 * allocated swap blocks.  depending on the size of the aobj this list
 * of allocated swap blocks is either stored in an array (small objects)
 * or in a hash table (large objects).
 */

/*
 * local structures
 */

/*
 * for hash tables, we break the address space of the aobj into blocks
 * of UAO_SWHASH_CLUSTER_SIZE pages.   we require the cluster size to
 * be a power of two.
 */
#define UAO_SWHASH_CLUSTER_SHIFT 4
#define UAO_SWHASH_CLUSTER_SIZE (1 << UAO_SWHASH_CLUSTER_SHIFT)

/* get the "tag" for this page index */
#define UAO_SWHASH_ELT_TAG(PAGEIDX) \
	((PAGEIDX) >> UAO_SWHASH_CLUSTER_SHIFT)

/* given an ELT and a page index, find the swap slot */
#define UAO_SWHASH_ELT_PAGESLOT_IDX(PAGEIDX) \
	((PAGEIDX) & (UAO_SWHASH_CLUSTER_SIZE - 1))
#define UAO_SWHASH_ELT_PAGESLOT(ELT, PAGEIDX) \
	((ELT)->slots[(PAGEIDX) & (UAO_SWHASH_CLUSTER_SIZE - 1)])

/* given an ELT, return its pageidx base */
#define UAO_SWHASH_ELT_PAGEIDX_BASE(ELT) \
	((ELT)->tag << UAO_SWHASH_CLUSTER_SHIFT)

/*
 * the swhash hash function
 */
#define UAO_SWHASH_HASH(AOBJ, PAGEIDX) \
	(&(AOBJ)->u_swhash[(((PAGEIDX) >> UAO_SWHASH_CLUSTER_SHIFT) \
			    & (AOBJ)->u_swhashmask)])

/*
 * the swhash threshold determines if we will use an array or a
 * hash table to store the list of allocated swap blocks.
 */

#define UAO_SWHASH_THRESHOLD (UAO_SWHASH_CLUSTER_SIZE * 4)

/*
 * the number of buckets in a swhash, with an upper bound
 */
#define UAO_SWHASH_MAXBUCKETS 256
#define UAO_SWHASH_BUCKETS(pages) \
	(min((pages) >> UAO_SWHASH_CLUSTER_SHIFT, UAO_SWHASH_MAXBUCKETS))


/*
 * uao_swhash_elt: when a hash table is being used, this structure defines
 * the format of an entry in the bucket list.
 */
struct uao_swhash_elt {
	LIST_ENTRY(uao_swhash_elt) list;	/* the hash list */
	voff_t tag;				/* our 'tag' */
	int count;				/* our number of active slots */
	int slots[UAO_SWHASH_CLUSTER_SIZE];	/* the slots */
};

/*
 * uao_swhash: the swap hash table structure
 */
LIST_HEAD(uao_swhash, uao_swhash_elt);

/*
 * uao_swhash_elt_pool: pool of uao_swhash_elt structures
 */
struct pool uao_swhash_elt_pool;

/*
 * uvm_aobj: the actual anon-backed uvm_object
 *
 * => the uvm_object is at the top of the structure, this allows
 *   (struct uvm_aobj *) == (struct uvm_object *)
 * => only one of u_swslots and u_swhash is used in any given aobj
 */
struct uvm_aobj {
	struct uvm_object u_obj; /* has: pgops, memt, #pages, #refs */
	int u_pages;		 /* number of pages in entire object */
	int u_flags;		 /* the flags (see uvm_aobj.h) */
	/*
	 * Either an array or hashtable (array of bucket heads) of
	 * offset -> swapslot mappings for the aobj.
	 */
#define u_swslots	u_swap.slot_array 
#define u_swhash	u_swap.slot_hash
	union swslots {
		int			*slot_array;
		struct uao_swhash	*slot_hash;
	} u_swap;
	u_long u_swhashmask;		/* mask for hashtable */
	LIST_ENTRY(uvm_aobj) u_list;	/* global list of aobjs */
};

/*
 * uvm_aobj_pool: pool of uvm_aobj structures
 */
struct pool uvm_aobj_pool;

/*
 * local functions
 */
static struct uao_swhash_elt	*uao_find_swhash_elt(struct uvm_aobj *, int,
				     boolean_t);
static int			 uao_find_swslot(struct uvm_aobj *, int);
static boolean_t		 uao_flush(struct uvm_object *, voff_t,
				     voff_t, int);
static void			 uao_free(struct uvm_aobj *);
static int			 uao_get(struct uvm_object *, voff_t,
				     vm_page_t *, int *, int, vm_prot_t,
				     int, int);
static boolean_t		 uao_pagein(struct uvm_aobj *, int, int);
static boolean_t		 uao_pagein_page(struct uvm_aobj *, int);

void	uao_dropswap_range(struct uvm_object *, voff_t, voff_t);
void	uao_shrink_flush(struct uvm_object *, int, int);
int	uao_shrink_hash(struct uvm_object *, int);
int	uao_shrink_array(struct uvm_object *, int);
int	uao_shrink_convert(struct uvm_object *, int);

int	uao_grow_hash(struct uvm_object *, int);
int	uao_grow_array(struct uvm_object *, int);
int	uao_grow_convert(struct uvm_object *, int);

/*
 * aobj_pager
 * 
 * note that some functions (e.g. put) are handled elsewhere
 */
struct uvm_pagerops aobj_pager = {
	NULL,			/* init */
	uao_reference,		/* reference */
	uao_detach,		/* detach */
	NULL,			/* fault */
	uao_flush,		/* flush */
	uao_get,		/* get */
};

/*
 * uao_list: global list of active aobjs, locked by uao_list_lock
 *
 * Lock ordering: generally the locking order is object lock, then list lock.
 * in the case of swap off we have to iterate over the list, and thus the
 * ordering is reversed. In that case we must use trylocking to prevent
 * deadlock.
 */
static LIST_HEAD(aobjlist, uvm_aobj) uao_list = LIST_HEAD_INITIALIZER(uao_list);
static struct mutex uao_list_lock = MUTEX_INITIALIZER(IPL_NONE);


/*
 * functions
 */
/*
 * hash table/array related functions
 */
/*
 * uao_find_swhash_elt: find (or create) a hash table entry for a page
 * offset.
 */
static struct uao_swhash_elt *
uao_find_swhash_elt(struct uvm_aobj *aobj, int pageidx, boolean_t create)
{
	struct uao_swhash *swhash;
	struct uao_swhash_elt *elt;
	voff_t page_tag;

	swhash = UAO_SWHASH_HASH(aobj, pageidx); /* first hash to get bucket */
	page_tag = UAO_SWHASH_ELT_TAG(pageidx);	/* tag to search for */

	/* now search the bucket for the requested tag */
	LIST_FOREACH(elt, swhash, list) {
		if (elt->tag == page_tag)
			return(elt);
	}

	/* fail now if we are not allowed to create a new entry in the bucket */
	if (!create)
		return NULL;

	/* allocate a new entry for the bucket and init/insert it in */
	elt = pool_get(&uao_swhash_elt_pool, PR_NOWAIT | PR_ZERO);
	/*
	 * XXX We cannot sleep here as the hash table might disappear
	 * from under our feet.  And we run the risk of deadlocking
	 * the pagedeamon.  In fact this code will only be called by
	 * the pagedaemon and allocation will only fail if we
	 * exhausted the pagedeamon reserve.  In that case we're
	 * doomed anyway, so panic.
	 */
	if (elt == NULL)
		panic("%s: can't allocate entry", __func__);
	LIST_INSERT_HEAD(swhash, elt, list);
	elt->tag = page_tag;

	return(elt);
}

/*
 * uao_find_swslot: find the swap slot number for an aobj/pageidx
 */
__inline static int
uao_find_swslot(struct uvm_aobj *aobj, int pageidx)
{

	/* if noswap flag is set, then we never return a slot */
	if (aobj->u_flags & UAO_FLAG_NOSWAP)
		return(0);

	/* if hashing, look in hash table.  */
	if (aobj->u_pages > UAO_SWHASH_THRESHOLD) {
		struct uao_swhash_elt *elt =
		    uao_find_swhash_elt(aobj, pageidx, FALSE);

		if (elt)
			return(UAO_SWHASH_ELT_PAGESLOT(elt, pageidx));
		else
			return(0);
	}

	/* otherwise, look in the array */
	return(aobj->u_swslots[pageidx]);
}

/*
 * uao_set_swslot: set the swap slot for a page in an aobj.
 *
 * => setting a slot to zero frees the slot
 */
int
uao_set_swslot(struct uvm_object *uobj, int pageidx, int slot)
{
	struct uvm_aobj *aobj = (struct uvm_aobj *)uobj;
	int oldslot;

	/* if noswap flag is set, then we can't set a slot */
	if (aobj->u_flags & UAO_FLAG_NOSWAP) {
		if (slot == 0)
			return(0);		/* a clear is ok */

		/* but a set is not */
		printf("uao_set_swslot: uobj = %p\n", uobj);
		panic("uao_set_swslot: attempt to set a slot"
		    " on a NOSWAP object");
	}

	/* are we using a hash table?  if so, add it in the hash.  */
	if (aobj->u_pages > UAO_SWHASH_THRESHOLD) {
		/*
		 * Avoid allocating an entry just to free it again if
		 * the page had not swap slot in the first place, and
		 * we are freeing.
		 */
		struct uao_swhash_elt *elt =
		    uao_find_swhash_elt(aobj, pageidx, slot ? TRUE : FALSE);
		if (elt == NULL) {
			KASSERT(slot == 0);
			return (0);
		}

		oldslot = UAO_SWHASH_ELT_PAGESLOT(elt, pageidx);
		UAO_SWHASH_ELT_PAGESLOT(elt, pageidx) = slot;

		/*
		 * now adjust the elt's reference counter and free it if we've
		 * dropped it to zero.
		 */
		/* an allocation? */
		if (slot) {
			if (oldslot == 0)
				elt->count++;
		} else {		/* freeing slot ... */
			if (oldslot)	/* to be safe */
				elt->count--;

			if (elt->count == 0) {
				LIST_REMOVE(elt, list);
				pool_put(&uao_swhash_elt_pool, elt);
			}
		}
	} else { 
		/* we are using an array */
		oldslot = aobj->u_swslots[pageidx];
		aobj->u_swslots[pageidx] = slot;
	}
	return (oldslot);
}
/*
 * end of hash/array functions
 */

/*
 * uao_free: free all resources held by an aobj, and then free the aobj
 *
 * => the aobj should be dead
 */
static void
uao_free(struct uvm_aobj *aobj)
{

	if (aobj->u_pages > UAO_SWHASH_THRESHOLD) {
		int i, hashbuckets = aobj->u_swhashmask + 1;

		/*
		 * free the swslots from each hash bucket,
		 * then the hash bucket, and finally the hash table itself.
		 */
		for (i = 0; i < hashbuckets; i++) {
			struct uao_swhash_elt *elt, *next;

			for (elt = LIST_FIRST(&aobj->u_swhash[i]);
			     elt != NULL;
			     elt = next) {
				int j;

				for (j = 0; j < UAO_SWHASH_CLUSTER_SIZE; j++) {
					int slot = elt->slots[j];

					if (slot == 0) {
						continue;
					}
					uvm_swap_free(slot, 1);
					/*
					 * this page is no longer
					 * only in swap.
					 */
					uvmexp.swpgonly--;
				}

				next = LIST_NEXT(elt, list);
				pool_put(&uao_swhash_elt_pool, elt);
			}
		}

		hashfree(aobj->u_swhash, UAO_SWHASH_BUCKETS(aobj->u_pages), M_UVMAOBJ);
	} else {
		int i;

		/* free the array */
		for (i = 0; i < aobj->u_pages; i++) {
			int slot = aobj->u_swslots[i];

			if (slot) {
				uvm_swap_free(slot, 1);
				/* this page is no longer only in swap. */
				uvmexp.swpgonly--;
			}
		}
		free(aobj->u_swslots, M_UVMAOBJ, aobj->u_pages * sizeof(int));
	}

	/* finally free the aobj itself */
	pool_put(&uvm_aobj_pool, aobj);
}

/*
 * pager functions
 */

/*
 * Shrink an aobj to a given number of pages. The procedure is always the same:
 * assess the necessity of data structure conversion (hash to array), secure
 * resources, flush pages and drop swap slots.
 *
 */

void
uao_shrink_flush(struct uvm_object *uobj, int startpg, int endpg)
{
	KASSERT(startpg < endpg);
	KASSERT(uobj->uo_refs == 1);
	uao_flush(uobj, (voff_t)startpg << PAGE_SHIFT,
	    (voff_t)endpg << PAGE_SHIFT, PGO_FREE);
	uao_dropswap_range(uobj, startpg, endpg);
}

int
uao_shrink_hash(struct uvm_object *uobj, int pages)
{
	struct uvm_aobj *aobj = (struct uvm_aobj *)uobj;
	struct uao_swhash *new_swhash;
	struct uao_swhash_elt *elt;
	unsigned long new_hashmask;
	int i;

	KASSERT(aobj->u_pages > UAO_SWHASH_THRESHOLD);

	/*
	 * If the size of the hash table doesn't change, all we need to do is
	 * to adjust the page count.
	 */
	if (UAO_SWHASH_BUCKETS(aobj->u_pages) == UAO_SWHASH_BUCKETS(pages)) {
		uao_shrink_flush(uobj, pages, aobj->u_pages);
		aobj->u_pages = pages;
		return 0;
	}

	new_swhash = hashinit(UAO_SWHASH_BUCKETS(pages), M_UVMAOBJ,
	    M_WAITOK | M_CANFAIL, &new_hashmask);
	if (new_swhash == NULL)
		return ENOMEM;

	uao_shrink_flush(uobj, pages, aobj->u_pages);

	/*
	 * Even though the hash table size is changing, the hash of the buckets
	 * we are interested in copying should not change.
	 */
	for (i = 0; i < UAO_SWHASH_BUCKETS(aobj->u_pages); i++) {
		while (LIST_EMPTY(&aobj->u_swhash[i]) == 0) {
			elt = LIST_FIRST(&aobj->u_swhash[i]);
			LIST_REMOVE(elt, list);
			LIST_INSERT_HEAD(&new_swhash[i], elt, list);
		}
	}

	hashfree(aobj->u_swhash, UAO_SWHASH_BUCKETS(aobj->u_pages), M_UVMAOBJ);

	aobj->u_swhash = new_swhash;
	aobj->u_pages = pages;
	aobj->u_swhashmask = new_hashmask;

	return 0;
}

int
uao_shrink_convert(struct uvm_object *uobj, int pages)
{
	struct uvm_aobj *aobj = (struct uvm_aobj *)uobj;
	struct uao_swhash_elt *elt;
	int i, *new_swslots;

	new_swslots = mallocarray(pages, sizeof(int), M_UVMAOBJ,
	    M_WAITOK | M_CANFAIL | M_ZERO);
	if (new_swslots == NULL)
		return ENOMEM;

	uao_shrink_flush(uobj, pages, aobj->u_pages);

	/* Convert swap slots from hash to array.  */
	for (i = 0; i < pages; i++) {
		elt = uao_find_swhash_elt(aobj, i, FALSE);
		if (elt != NULL) {
			new_swslots[i] = UAO_SWHASH_ELT_PAGESLOT(elt, i);
			if (new_swslots[i] != 0)
				elt->count--;
			if (elt->count == 0) {
				LIST_REMOVE(elt, list);
				pool_put(&uao_swhash_elt_pool, elt);
			}
		}
	}

	hashfree(aobj->u_swhash, UAO_SWHASH_BUCKETS(aobj->u_pages), M_UVMAOBJ);

	aobj->u_swslots = new_swslots;
	aobj->u_pages = pages;

	return 0;
}

int
uao_shrink_array(struct uvm_object *uobj, int pages)
{
	struct uvm_aobj *aobj = (struct uvm_aobj *)uobj;
	int i, *new_swslots;

	new_swslots = mallocarray(pages, sizeof(int), M_UVMAOBJ,
	    M_WAITOK | M_CANFAIL | M_ZERO);
	if (new_swslots == NULL)
		return ENOMEM;

	uao_shrink_flush(uobj, pages, aobj->u_pages);

	for (i = 0; i < pages; i++)
		new_swslots[i] = aobj->u_swslots[i];

	free(aobj->u_swslots, M_UVMAOBJ, aobj->u_pages * sizeof(int));

	aobj->u_swslots = new_swslots;
	aobj->u_pages = pages;

	return 0;
}

int
uao_shrink(struct uvm_object *uobj, int pages)
{
	struct uvm_aobj *aobj = (struct uvm_aobj *)uobj;

	KASSERT(pages < aobj->u_pages);

	/*
	 * Distinguish between three possible cases:
	 * 1. aobj uses hash and must be converted to array.
	 * 2. aobj uses array and array size needs to be adjusted.
	 * 3. aobj uses hash and hash size needs to be adjusted.
	 */
	if (pages > UAO_SWHASH_THRESHOLD)
		return uao_shrink_hash(uobj, pages);	/* case 3 */
	else if (aobj->u_pages > UAO_SWHASH_THRESHOLD)
		return uao_shrink_convert(uobj, pages);	/* case 1 */
	else
		return uao_shrink_array(uobj, pages);	/* case 2 */
}

/*
 * Grow an aobj to a given number of pages. Right now we only adjust the swap
 * slots. We could additionally handle page allocation directly, so that they
 * don't happen through uvm_fault(). That would allow us to use another
 * mechanism for the swap slots other than malloc(). It is thus mandatory that
 * the caller of these functions does not allow faults to happen in case of
 * growth error.
 */
int
uao_grow_array(struct uvm_object *uobj, int pages)
{
	struct uvm_aobj *aobj = (struct uvm_aobj *)uobj;
	int i, *new_swslots;

	KASSERT(aobj->u_pages <= UAO_SWHASH_THRESHOLD);

	new_swslots = mallocarray(pages, sizeof(int), M_UVMAOBJ,
	    M_WAITOK | M_CANFAIL | M_ZERO);
	if (new_swslots == NULL)
		return ENOMEM;

	for (i = 0; i < aobj->u_pages; i++)
		new_swslots[i] = aobj->u_swslots[i];

	free(aobj->u_swslots, M_UVMAOBJ, aobj->u_pages * sizeof(int));

	aobj->u_swslots = new_swslots;
	aobj->u_pages = pages;

	return 0;
}

int
uao_grow_hash(struct uvm_object *uobj, int pages)
{
	struct uvm_aobj *aobj = (struct uvm_aobj *)uobj;
	struct uao_swhash *new_swhash;
	struct uao_swhash_elt *elt;
	unsigned long new_hashmask;
	int i;

	KASSERT(pages > UAO_SWHASH_THRESHOLD);

	/*
	 * If the size of the hash table doesn't change, all we need to do is
	 * to adjust the page count.
	 */
	if (UAO_SWHASH_BUCKETS(aobj->u_pages) == UAO_SWHASH_BUCKETS(pages)) {
		aobj->u_pages = pages;
		return 0;
	}

	KASSERT(UAO_SWHASH_BUCKETS(aobj->u_pages) < UAO_SWHASH_BUCKETS(pages));

	new_swhash = hashinit(UAO_SWHASH_BUCKETS(pages), M_UVMAOBJ,
	    M_WAITOK | M_CANFAIL, &new_hashmask);
	if (new_swhash == NULL)
		return ENOMEM;

	for (i = 0; i < UAO_SWHASH_BUCKETS(aobj->u_pages); i++) {
		while (LIST_EMPTY(&aobj->u_swhash[i]) == 0) {
			elt = LIST_FIRST(&aobj->u_swhash[i]);
			LIST_REMOVE(elt, list);
			LIST_INSERT_HEAD(&new_swhash[i], elt, list);
		}
	}

	hashfree(aobj->u_swhash, UAO_SWHASH_BUCKETS(aobj->u_pages), M_UVMAOBJ);

	aobj->u_swhash = new_swhash;
	aobj->u_pages = pages;
	aobj->u_swhashmask = new_hashmask;

	return 0;
}

int
uao_grow_convert(struct uvm_object *uobj, int pages)
{
	struct uvm_aobj *aobj = (struct uvm_aobj *)uobj;
	struct uao_swhash *new_swhash;
	struct uao_swhash_elt *elt;
	unsigned long new_hashmask;
	int i, *old_swslots;

	new_swhash = hashinit(UAO_SWHASH_BUCKETS(pages), M_UVMAOBJ,
	    M_WAITOK | M_CANFAIL, &new_hashmask);
	if (new_swhash == NULL)
		return ENOMEM;

	/* Set these now, so we can use uao_find_swhash_elt(). */
	old_swslots = aobj->u_swslots;
	aobj->u_swhash = new_swhash;		
	aobj->u_swhashmask = new_hashmask;

	for (i = 0; i < aobj->u_pages; i++) {
		if (old_swslots[i] != 0) {
			elt = uao_find_swhash_elt(aobj, i, TRUE);
			elt->count++;
			UAO_SWHASH_ELT_PAGESLOT(elt, i) = old_swslots[i];
		}
	}

	free(old_swslots, M_UVMAOBJ, aobj->u_pages * sizeof(int));
	aobj->u_pages = pages;

	return 0;
}

int
uao_grow(struct uvm_object *uobj, int pages)
{
	struct uvm_aobj *aobj = (struct uvm_aobj *)uobj;

	KASSERT(pages > aobj->u_pages);

	/*
	 * Distinguish between three possible cases:
	 * 1. aobj uses hash and hash size needs to be adjusted.
	 * 2. aobj uses array and array size needs to be adjusted.
	 * 3. aobj uses array and must be converted to hash.
	 */
	if (pages <= UAO_SWHASH_THRESHOLD)
		return uao_grow_array(uobj, pages);	/* case 2 */
	else if (aobj->u_pages > UAO_SWHASH_THRESHOLD)
		return uao_grow_hash(uobj, pages);	/* case 1 */
	else
		return uao_grow_convert(uobj, pages);
}

/*
 * uao_create: create an aobj of the given size and return its uvm_object.
 *
 * => for normal use, flags are zero or UAO_FLAG_CANFAIL.
 * => for the kernel object, the flags are:
 *	UAO_FLAG_KERNOBJ - allocate the kernel object (can only happen once)
 *	UAO_FLAG_KERNSWAP - enable swapping of kernel object ("           ")
 */
struct uvm_object *
uao_create(vsize_t size, int flags)
{
	static struct uvm_aobj kernel_object_store; /* home of kernel_object */
	static int kobj_alloced = 0;			/* not allocated yet */
	int pages = round_page(size) >> PAGE_SHIFT;
	int refs = UVM_OBJ_KERN;
	int mflags;
	struct uvm_aobj *aobj;

	/* malloc a new aobj unless we are asked for the kernel object */
	if (flags & UAO_FLAG_KERNOBJ) {		/* want kernel object? */
		if (kobj_alloced)
			panic("uao_create: kernel object already allocated");

		aobj = &kernel_object_store;
		aobj->u_pages = pages;
		aobj->u_flags = UAO_FLAG_NOSWAP;	/* no swap to start */
		/* we are special, we never die */
		kobj_alloced = UAO_FLAG_KERNOBJ;
	} else if (flags & UAO_FLAG_KERNSWAP) {
		aobj = &kernel_object_store;
		if (kobj_alloced != UAO_FLAG_KERNOBJ)
		    panic("uao_create: asked to enable swap on kernel object");
		kobj_alloced = UAO_FLAG_KERNSWAP;
	} else {	/* normal object */
		aobj = pool_get(&uvm_aobj_pool, PR_WAITOK);
		aobj->u_pages = pages;
		aobj->u_flags = 0;		/* normal object */
		refs = 1;			/* normal object so 1 ref */
	}

	/* allocate hash/array if necessary */
 	if (flags == 0 || (flags & (UAO_FLAG_KERNSWAP | UAO_FLAG_CANFAIL))) {
		if (flags)
			mflags = M_NOWAIT;
		else
			mflags = M_WAITOK;

		/* allocate hash table or array depending on object size */
		if (aobj->u_pages > UAO_SWHASH_THRESHOLD) {
			aobj->u_swhash = hashinit(UAO_SWHASH_BUCKETS(pages),
			    M_UVMAOBJ, mflags, &aobj->u_swhashmask);
			if (aobj->u_swhash == NULL) {
				if (flags & UAO_FLAG_CANFAIL) {
					pool_put(&uvm_aobj_pool, aobj);
					return (NULL);
				}
				panic("uao_create: hashinit swhash failed");
			}
		} else {
			aobj->u_swslots = mallocarray(pages, sizeof(int),
			    M_UVMAOBJ, mflags|M_ZERO);
			if (aobj->u_swslots == NULL) {
				if (flags & UAO_FLAG_CANFAIL) {
					pool_put(&uvm_aobj_pool, aobj);
					return (NULL);
				}
				panic("uao_create: malloc swslots failed");
			}
		}

		if (flags & UAO_FLAG_KERNSWAP) {
			aobj->u_flags &= ~UAO_FLAG_NOSWAP; /* clear noswap */
			return(&aobj->u_obj);
			/* done! */
		}
	}

	uvm_objinit(&aobj->u_obj, &aobj_pager, refs);

	/* now that aobj is ready, add it to the global list */
	mtx_enter(&uao_list_lock);
	LIST_INSERT_HEAD(&uao_list, aobj, u_list);
	mtx_leave(&uao_list_lock);

	return(&aobj->u_obj);
}



/*
 * uao_init: set up aobj pager subsystem
 *
 * => called at boot time from uvm_pager_init()
 */
void
uao_init(void)
{
	static int uao_initialized;

	if (uao_initialized)
		return;
	uao_initialized = TRUE;

	/*
	 * NOTE: Pages for this pool must not come from a pageable
	 * kernel map!
	 */
	pool_init(&uao_swhash_elt_pool, sizeof(struct uao_swhash_elt), 0,
	    IPL_NONE, PR_WAITOK, "uaoeltpl", NULL);
	pool_init(&uvm_aobj_pool, sizeof(struct uvm_aobj), 0,
	    IPL_NONE, PR_WAITOK, "aobjpl", NULL);
}

/*
 * uao_reference: add a ref to an aobj
 */
void
uao_reference(struct uvm_object *uobj)
{
	uao_reference_locked(uobj);
}

/*
 * uao_reference_locked: add a ref to an aobj
 */
void
uao_reference_locked(struct uvm_object *uobj)
{

	/* kernel_object already has plenty of references, leave it alone. */
	if (UVM_OBJ_IS_KERN_OBJECT(uobj))
		return;

	uobj->uo_refs++;		/* bump! */
}


/*
 * uao_detach: drop a reference to an aobj
 */
void
uao_detach(struct uvm_object *uobj)
{
	uao_detach_locked(uobj);
}


/*
 * uao_detach_locked: drop a reference to an aobj
 *
 * => aobj may freed upon return.
 */
void
uao_detach_locked(struct uvm_object *uobj)
{
	struct uvm_aobj *aobj = (struct uvm_aobj *)uobj;
	struct vm_page *pg;

	/* detaching from kernel_object is a noop. */
	if (UVM_OBJ_IS_KERN_OBJECT(uobj)) {
		return;
	}

	uobj->uo_refs--;				/* drop ref! */
	if (uobj->uo_refs) {				/* still more refs? */
		return;
	}

	/* remove the aobj from the global list. */
	mtx_enter(&uao_list_lock);
	LIST_REMOVE(aobj, u_list);
	mtx_leave(&uao_list_lock);

	/*
	 * Free all pages left in the object. If they're busy, wait
	 * for them to become available before we kill it.
	 * Release swap resources then free the page.
 	 */
	uvm_lock_pageq();
	while((pg = RBT_ROOT(uvm_objtree, &uobj->memt)) != NULL) {
		if (pg->pg_flags & PG_BUSY) {
			atomic_setbits_int(&pg->pg_flags, PG_WANTED);
			uvm_unlock_pageq();
			UVM_WAIT(pg, 0, "uao_det", 0);
			uvm_lock_pageq();
			continue;
		}
		pmap_page_protect(pg, PROT_NONE);
		uao_dropswap(&aobj->u_obj, pg->offset >> PAGE_SHIFT);
		uvm_pagefree(pg);
	}
	uvm_unlock_pageq();

	/* finally, free the rest. */
	uao_free(aobj);
}

/*
 * uao_flush: "flush" pages out of a uvm object
 *
 * => if PGO_CLEANIT is not set, then we will not block.
 * => if PGO_ALLPAGE is set, then all pages in the object are valid targets
 *	for flushing.
 * => NOTE: we are allowed to lock the page queues, so the caller
 *	must not be holding the lock on them [e.g. pagedaemon had
 *	better not call us with the queues locked]
 * => we return TRUE unless we encountered some sort of I/O error
 *	XXXJRT currently never happens, as we never directly initiate
 *	XXXJRT I/O
 */
boolean_t
uao_flush(struct uvm_object *uobj, voff_t start, voff_t stop, int flags)
{
	struct uvm_aobj *aobj = (struct uvm_aobj *) uobj;
	struct vm_page *pp;
	voff_t curoff;

	if (flags & PGO_ALLPAGES) {
		start = 0;
		stop = (voff_t)aobj->u_pages << PAGE_SHIFT;
	} else {
		start = trunc_page(start);
		stop = round_page(stop);
		if (stop > ((voff_t)aobj->u_pages << PAGE_SHIFT)) {
			printf("uao_flush: strange, got an out of range "
			    "flush (fixed)\n");
			stop = (voff_t)aobj->u_pages << PAGE_SHIFT;
		}
	}

	/*
	 * Don't need to do any work here if we're not freeing
	 * or deactivating pages.
	 */
	if ((flags & (PGO_DEACTIVATE|PGO_FREE)) == 0)
		return (TRUE);

	curoff = start;
	for (;;) {
		if (curoff < stop) {
			pp = uvm_pagelookup(uobj, curoff);
			curoff += PAGE_SIZE;
			if (pp == NULL)
				continue;
		} else {
			break;
		}

		/* Make sure page is unbusy, else wait for it. */
		if (pp->pg_flags & PG_BUSY) {
			atomic_setbits_int(&pp->pg_flags, PG_WANTED);
			UVM_WAIT(pp, 0, "uaoflsh", 0);
			curoff -= PAGE_SIZE;
			continue;
		}

		switch (flags & (PGO_CLEANIT|PGO_FREE|PGO_DEACTIVATE)) {
		/*
		 * XXX In these first 3 cases, we always just
		 * XXX deactivate the page.  We may want to
		 * XXX handle the different cases more specifically
		 * XXX in the future.
		 */
		case PGO_CLEANIT|PGO_FREE:
			/* FALLTHROUGH */
		case PGO_CLEANIT|PGO_DEACTIVATE:
			/* FALLTHROUGH */
		case PGO_DEACTIVATE:
 deactivate_it:
			/* skip the page if it's wired */
			if (pp->wire_count != 0)
				continue;

			uvm_lock_pageq();
			/* zap all mappings for the page. */
			pmap_page_protect(pp, PROT_NONE);

			/* ...and deactivate the page. */
			uvm_pagedeactivate(pp);
			uvm_unlock_pageq();

			continue;
		case PGO_FREE:
			/*
			 * If there are multiple references to
			 * the object, just deactivate the page.
			 */
			if (uobj->uo_refs > 1)
				goto deactivate_it;

			/* XXX skip the page if it's wired */
			if (pp->wire_count != 0)
				continue;

			/* zap all mappings for the page. */
			pmap_page_protect(pp, PROT_NONE);

			uao_dropswap(uobj, pp->offset >> PAGE_SHIFT);
			uvm_lock_pageq();
			uvm_pagefree(pp);
			uvm_unlock_pageq();

			continue;
		default:
			panic("uao_flush: weird flags");
		}
	}

	return (TRUE);
}

/*
 * uao_get: fetch me a page
 *
 * we have three cases:
 * 1: page is resident     -> just return the page.
 * 2: page is zero-fill    -> allocate a new page and zero it.
 * 3: page is swapped out  -> fetch the page from swap.
 *
 * cases 1 and 2 can be handled with PGO_LOCKED, case 3 cannot.
 * so, if the "center" page hits case 3 (or any page, with PGO_ALLPAGES),
 * then we will need to return VM_PAGER_UNLOCK.
 *
 * => flags: PGO_ALLPAGES: get all of the pages
 *           PGO_LOCKED: fault data structures are locked
 * => NOTE: offset is the offset of pps[0], _NOT_ pps[centeridx]
 * => NOTE: caller must check for released pages!!
 */
static int
uao_get(struct uvm_object *uobj, voff_t offset, struct vm_page **pps,
    int *npagesp, int centeridx, vm_prot_t access_type, int advice, int flags)
{
	struct uvm_aobj *aobj = (struct uvm_aobj *)uobj;
	voff_t current_offset;
	vm_page_t ptmp;
	int lcv, gotpages, maxpages, swslot, rv, pageidx;
	boolean_t done;

	/* get number of pages */
	maxpages = *npagesp;

	/* step 1: handled the case where fault data structures are locked. */
	if (flags & PGO_LOCKED) {
		/* step 1a: get pages that are already resident. */

		done = TRUE;	/* be optimistic */
		gotpages = 0;	/* # of pages we got so far */

		for (lcv = 0, current_offset = offset ; lcv < maxpages ;
		    lcv++, current_offset += PAGE_SIZE) {
			/* do we care about this page?  if not, skip it */
			if (pps[lcv] == PGO_DONTCARE)
				continue;

			ptmp = uvm_pagelookup(uobj, current_offset);

			/*
 			 * if page is new, attempt to allocate the page,
			 * zero-fill'd.
 			 */
			if (ptmp == NULL && uao_find_swslot(aobj,
			    current_offset >> PAGE_SHIFT) == 0) {
				ptmp = uvm_pagealloc(uobj, current_offset,
				    NULL, UVM_PGA_ZERO);
				if (ptmp) {
					/* new page */
					atomic_clearbits_int(&ptmp->pg_flags,
					    PG_BUSY|PG_FAKE);
					atomic_setbits_int(&ptmp->pg_flags,
					    PQ_AOBJ);
					UVM_PAGE_OWN(ptmp, NULL);
				}
			}

			/* to be useful must get a non-busy page */
			if (ptmp == NULL ||
			    (ptmp->pg_flags & PG_BUSY) != 0) {
				if (lcv == centeridx ||
				    (flags & PGO_ALLPAGES) != 0)
					/* need to do a wait or I/O! */
					done = FALSE;	
				continue;
			}

			/*
			 * useful page: busy it and plug it in our
			 * result array
			 */
			/* caller must un-busy this page */
			atomic_setbits_int(&ptmp->pg_flags, PG_BUSY);
			UVM_PAGE_OWN(ptmp, "uao_get1");
			pps[lcv] = ptmp;
			gotpages++;

		}

		/*
 		 * step 1b: now we've either done everything needed or we
		 * to unlock and do some waiting or I/O.
 		 */
		*npagesp = gotpages;
		if (done)
			/* bingo! */
			return(VM_PAGER_OK);	
		else
			/* EEK!   Need to unlock and I/O */
			return(VM_PAGER_UNLOCK);
	}

	/*
 	 * step 2: get non-resident or busy pages.
 	 * data structures are unlocked.
 	 */
	for (lcv = 0, current_offset = offset ; lcv < maxpages ;
	    lcv++, current_offset += PAGE_SIZE) {
		/*
		 * - skip over pages we've already gotten or don't want
		 * - skip over pages we don't _have_ to get
		 */
		if (pps[lcv] != NULL ||
		    (lcv != centeridx && (flags & PGO_ALLPAGES) == 0))
			continue;

		pageidx = current_offset >> PAGE_SHIFT;

		/*
 		 * we have yet to locate the current page (pps[lcv]).   we
		 * first look for a page that is already at the current offset.
		 * if we find a page, we check to see if it is busy or
		 * released.  if that is the case, then we sleep on the page
		 * until it is no longer busy or released and repeat the lookup.
		 * if the page we found is neither busy nor released, then we
		 * busy it (so we own it) and plug it into pps[lcv].   this
		 * 'break's the following while loop and indicates we are
		 * ready to move on to the next page in the "lcv" loop above.
 		 *
 		 * if we exit the while loop with pps[lcv] still set to NULL,
		 * then it means that we allocated a new busy/fake/clean page
		 * ptmp in the object and we need to do I/O to fill in the data.
 		 */

		/* top of "pps" while loop */
		while (pps[lcv] == NULL) {
			/* look for a resident page */
			ptmp = uvm_pagelookup(uobj, current_offset);

			/* not resident?   allocate one now (if we can) */
			if (ptmp == NULL) {

				ptmp = uvm_pagealloc(uobj, current_offset,
				    NULL, 0);

				/* out of RAM? */
				if (ptmp == NULL) {
					uvm_wait("uao_getpage");
					/* goto top of pps while loop */
					continue;	
				}

				/*
				 * safe with PQ's unlocked: because we just
				 * alloc'd the page
				 */
				atomic_setbits_int(&ptmp->pg_flags, PQ_AOBJ);

				/* 
				 * got new page ready for I/O.  break pps while
				 * loop.  pps[lcv] is still NULL.
				 */
				break;
			}

			/* page is there, see if we need to wait on it */
			if ((ptmp->pg_flags & PG_BUSY) != 0) {
				atomic_setbits_int(&ptmp->pg_flags, PG_WANTED);
				UVM_WAIT(ptmp, FALSE, "uao_get", 0);
				continue;	/* goto top of pps while loop */
			}
			
			/* 
 			 * if we get here then the page has become resident and
			 * unbusy between steps 1 and 2.  we busy it now (so we
			 * own it) and set pps[lcv] (so that we exit the while
			 * loop).
 			 */
			/* we own it, caller must un-busy */
			atomic_setbits_int(&ptmp->pg_flags, PG_BUSY);
			UVM_PAGE_OWN(ptmp, "uao_get2");
			pps[lcv] = ptmp;
		}

		/*
 		 * if we own the valid page at the correct offset, pps[lcv] will
 		 * point to it.   nothing more to do except go to the next page.
 		 */
		if (pps[lcv])
			continue;			/* next lcv */

		/*
 		 * we have a "fake/busy/clean" page that we just allocated.  
 		 * do the needed "i/o", either reading from swap or zeroing.
 		 */
		swslot = uao_find_swslot(aobj, pageidx);

		/* just zero the page if there's nothing in swap.  */
		if (swslot == 0) {
			/* page hasn't existed before, just zero it. */
			uvm_pagezero(ptmp);
		} else {
			/* page in the swapped-out page. */
			rv = uvm_swap_get(ptmp, swslot, PGO_SYNCIO);

			/* I/O done.  check for errors. */
			if (rv != VM_PAGER_OK) {
				/*
				 * remove the swap slot from the aobj
				 * and mark the aobj as having no real slot.
				 * don't free the swap slot, thus preventing
				 * it from being used again.
				 */
				swslot = uao_set_swslot(&aobj->u_obj, pageidx,
							SWSLOT_BAD);
				uvm_swap_markbad(swslot, 1);

				if (ptmp->pg_flags & PG_WANTED)
					wakeup(ptmp);
				atomic_clearbits_int(&ptmp->pg_flags,
				    PG_WANTED|PG_BUSY);
				UVM_PAGE_OWN(ptmp, NULL);
				uvm_lock_pageq();
				uvm_pagefree(ptmp);
				uvm_unlock_pageq();

				return (rv);
			}
		}

		/* 
 		 * we got the page!   clear the fake flag (indicates valid
		 * data now in page) and plug into our result array.   note
		 * that page is still busy.   
 		 *
 		 * it is the callers job to:
 		 * => check if the page is released
 		 * => unbusy the page
 		 * => activate the page
 		 */

		/* data is valid ... */
		atomic_clearbits_int(&ptmp->pg_flags, PG_FAKE);
		pmap_clear_modify(ptmp);		/* ... and clean */
		pps[lcv] = ptmp;

	}	/* lcv loop */

	return(VM_PAGER_OK);
}

/*
 * uao_dropswap:  release any swap resources from this aobj page.
 */
int
uao_dropswap(struct uvm_object *uobj, int pageidx)
{
	int slot;

	slot = uao_set_swslot(uobj, pageidx, 0);
	if (slot) {
		uvm_swap_free(slot, 1);
	}
	return (slot);
}

/*
 * page in every page in every aobj that is paged-out to a range of swslots.
 * 
 * => returns TRUE if pagein was aborted due to lack of memory.
 */
boolean_t
uao_swap_off(int startslot, int endslot)
{
	struct uvm_aobj *aobj, *nextaobj, *prevaobj = NULL;

	/* walk the list of all aobjs. */
	mtx_enter(&uao_list_lock);

	for (aobj = LIST_FIRST(&uao_list);
	     aobj != NULL;
	     aobj = nextaobj) {
		boolean_t rv;

		/*
		 * add a ref to the aobj so it doesn't disappear
		 * while we're working.
		 */
		uao_reference_locked(&aobj->u_obj);

		/*
		 * now it's safe to unlock the uao list.
		 * note that lock interleaving is alright with IPL_NONE mutexes.
		 */
		mtx_leave(&uao_list_lock);

		if (prevaobj) {
			uao_detach_locked(&prevaobj->u_obj);
			prevaobj = NULL;
		}

		/*
		 * page in any pages in the swslot range.
		 * if there's an error, abort and return the error.
		 */
		rv = uao_pagein(aobj, startslot, endslot);
		if (rv) {
			uao_detach_locked(&aobj->u_obj);
			return rv;
		}

		/*
		 * we're done with this aobj.
		 * relock the list and drop our ref on the aobj.
		 */
		mtx_enter(&uao_list_lock);
		nextaobj = LIST_NEXT(aobj, u_list);
		/*
		 * prevaobj means that we have an object that we need
		 * to drop a reference for. We can't just drop it now with
		 * the list locked since that could cause lock recursion in
		 * the case where we reduce the refcount to 0. It will be
		 * released the next time we drop the list lock.
		 */
		prevaobj = aobj;
	}

	/* done with traversal, unlock the list */
	mtx_leave(&uao_list_lock);
	if (prevaobj) {
		uao_detach_locked(&prevaobj->u_obj);
	}
	return FALSE;
}

/*
 * page in any pages from aobj in the given range.
 *
 * => returns TRUE if pagein was aborted due to lack of memory.
 */
static boolean_t
uao_pagein(struct uvm_aobj *aobj, int startslot, int endslot)
{
	boolean_t rv;

	if (aobj->u_pages > UAO_SWHASH_THRESHOLD) {
		struct uao_swhash_elt *elt;
		int bucket;

restart:
		for (bucket = aobj->u_swhashmask; bucket >= 0; bucket--) {
			for (elt = LIST_FIRST(&aobj->u_swhash[bucket]);
			     elt != NULL;
			     elt = LIST_NEXT(elt, list)) {
				int i;

				for (i = 0; i < UAO_SWHASH_CLUSTER_SIZE; i++) {
					int slot = elt->slots[i];

					/* if slot isn't in range, skip it. */
					if (slot < startslot || 
					    slot >= endslot) {
						continue;
					}

					/*
					 * process the page,
					 * the start over on this object
					 * since the swhash elt
					 * may have been freed.
					 */
					rv = uao_pagein_page(aobj,
					  UAO_SWHASH_ELT_PAGEIDX_BASE(elt) + i);
					if (rv) {
						return rv;
					}
					goto restart;
				}
			}
		}
	} else {
		int i;

		for (i = 0; i < aobj->u_pages; i++) {
			int slot = aobj->u_swslots[i];

			/* if the slot isn't in range, skip it */
			if (slot < startslot || slot >= endslot) {
				continue;
			}

			/* process the page.  */
			rv = uao_pagein_page(aobj, i);
			if (rv) {
				return rv;
			}
		}
	}

	return FALSE;
}

/*
 * page in a page from an aobj.  used for swap_off.
 * returns TRUE if pagein was aborted due to lack of memory.
 */
static boolean_t
uao_pagein_page(struct uvm_aobj *aobj, int pageidx)
{
	struct vm_page *pg;
	int rv, slot, npages;

	pg = NULL;
	npages = 1;
	rv = uao_get(&aobj->u_obj, (voff_t)pageidx << PAGE_SHIFT,
	    &pg, &npages, 0, PROT_READ | PROT_WRITE, 0, 0);

	switch (rv) {
	case VM_PAGER_OK:
		break;

	case VM_PAGER_ERROR:
	case VM_PAGER_REFAULT:
		/*
		 * nothing more to do on errors.
		 * VM_PAGER_REFAULT can only mean that the anon was freed,
		 * so again there's nothing to do.
		 */
		return FALSE;
	}

	/*
	 * ok, we've got the page now.
	 * mark it as dirty, clear its swslot and un-busy it.
	 */
	slot = uao_set_swslot(&aobj->u_obj, pageidx, 0);
	uvm_swap_free(slot, 1);
	atomic_clearbits_int(&pg->pg_flags, PG_BUSY|PG_CLEAN|PG_FAKE);
	UVM_PAGE_OWN(pg, NULL);

	/* deactivate the page (to put it on a page queue). */
	pmap_clear_reference(pg);
	uvm_lock_pageq();
	uvm_pagedeactivate(pg);
	uvm_unlock_pageq();

	return FALSE;
}

/*
 * XXX pedro: Once we are comfortable enough with this function, we can adapt
 * uao_free() to use it.
 *
 * uao_dropswap_range: drop swapslots in the range.
 *
 * => aobj must be locked and is returned locked.
 * => start is inclusive.  end is exclusive.
 */
void
uao_dropswap_range(struct uvm_object *uobj, voff_t start, voff_t end)
{
	struct uvm_aobj *aobj = (struct uvm_aobj *)uobj;
	int swpgonlydelta = 0;

	/* KASSERT(mutex_owned(uobj->vmobjlock)); */

	if (end == 0) {
		end = INT64_MAX;
	}

	if (aobj->u_pages > UAO_SWHASH_THRESHOLD) {
		int i, hashbuckets = aobj->u_swhashmask + 1;
		voff_t taghi;
		voff_t taglo;

		taglo = UAO_SWHASH_ELT_TAG(start);
		taghi = UAO_SWHASH_ELT_TAG(end);

		for (i = 0; i < hashbuckets; i++) {
			struct uao_swhash_elt *elt, *next;

			for (elt = LIST_FIRST(&aobj->u_swhash[i]);
			     elt != NULL;
			     elt = next) {
				int startidx, endidx;
				int j;

				next = LIST_NEXT(elt, list);

				if (elt->tag < taglo || taghi < elt->tag) {
					continue;
				}

				if (elt->tag == taglo) {
					startidx =
					    UAO_SWHASH_ELT_PAGESLOT_IDX(start);
				} else {
					startidx = 0;
				}

				if (elt->tag == taghi) {
					endidx =
					    UAO_SWHASH_ELT_PAGESLOT_IDX(end);
				} else {
					endidx = UAO_SWHASH_CLUSTER_SIZE;
				}

				for (j = startidx; j < endidx; j++) {
					int slot = elt->slots[j];

					KASSERT(uvm_pagelookup(&aobj->u_obj,
					    (voff_t)(UAO_SWHASH_ELT_PAGEIDX_BASE(elt)
					    + j) << PAGE_SHIFT) == NULL);

					if (slot > 0) {
						uvm_swap_free(slot, 1);
						swpgonlydelta++;
						KASSERT(elt->count > 0);
						elt->slots[j] = 0;
						elt->count--;
					}
				}

				if (elt->count == 0) {
					LIST_REMOVE(elt, list);
					pool_put(&uao_swhash_elt_pool, elt);
				}
			}
		}
	} else {
		int i;

		if (aobj->u_pages < end) {
			end = aobj->u_pages;
		}
		for (i = start; i < end; i++) {
			int slot = aobj->u_swslots[i];

			if (slot > 0) {
				uvm_swap_free(slot, 1);
				swpgonlydelta++;
			}
		}
	}

	/*
	 * adjust the counter of pages only in swap for all
	 * the swap slots we've freed.
	 */
	if (swpgonlydelta > 0) {
		KASSERT(uvmexp.swpgonly >= swpgonlydelta);
		uvmexp.swpgonly -= swpgonlydelta;
	}
}
@


1.84
log
@use hashfree for aobj hashes. from Mathieu -
ok guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.83 2016/09/16 02:35:42 dlg Exp $	*/
d406 1
a406 1
		free(aobj->u_swslots, M_UVMAOBJ, 0);
d535 1
a535 1
	free(aobj->u_swslots, M_UVMAOBJ, 0);
d588 1
a588 1
	free(aobj->u_swslots, M_UVMAOBJ, 0);
d667 1
a667 1
	free(old_swslots, M_UVMAOBJ, 0);
@


1.83
log
@move the vm_page struct from being stored in RB macro trees to RBT functions

vm_page structs go into three trees, uvm_objtree, uvm_pmr_addr, and
uvm_pmr_size. all these have been moved to RBT code.

this should give us a decent chunk of code space back.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.82 2016/09/15 02:00:18 dlg Exp $	*/
d391 2
a392 1
		free(aobj->u_swhash, M_UVMAOBJ, 0);
d474 1
a474 1
	free(aobj->u_swhash, M_UVMAOBJ, 0);
d511 1
a511 1
	free(aobj->u_swhash, M_UVMAOBJ, 0);
d631 1
a631 1
	free(aobj->u_swhash, M_UVMAOBJ, 0);
@


1.82
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.81 2016/06/17 10:48:25 dlg Exp $	*/
d872 1
a872 1
	while((pg = RB_ROOT(&uobj->memt)) != NULL) {
@


1.81
log
@pool_setipl on all uvm pools.

ok kettenis@@ visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.80 2015/08/21 16:04:35 visa Exp $	*/
d800 4
a803 7
	pool_init(&uao_swhash_elt_pool, sizeof(struct uao_swhash_elt),
	    0, 0, PR_WAITOK, "uaoeltpl", NULL);
	pool_setipl(&uao_swhash_elt_pool, IPL_NONE);

	pool_init(&uvm_aobj_pool, sizeof(struct uvm_aobj), 0, 0, PR_WAITOK,
	    "aobjpl", NULL);
	pool_setipl(&uvm_aobj_pool, IPL_NONE);
@


1.80
log
@Remove the unused loan_count field and the related uvm logic. Most of
the page loaning code is already in the Attic.

ok kettenis@@, beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.79 2015/05/07 01:55:44 jsg Exp $	*/
d802 1
d806 1
@


1.79
log
@fix indentation
ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.78 2015/02/08 02:17:08 deraadt Exp $	*/
d963 2
a964 3
			/* skip the page if it's loaned or wired */
			if (pp->loan_count != 0 ||
			    pp->wire_count != 0)
d984 2
a985 3
			/* XXX skip the page if it's loaned or wired */
			if (pp->loan_count != 0 ||
			    pp->wire_count != 0)
@


1.78
log
@Something is subtly wrong with this.  On ramdisks, processes run out of
mappable memory (direct or via execve), perhaps because of the address
allocator behind maps and the way wiring counts work?
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.76 2014/12/23 04:56:47 tedu Exp $	*/
d1077 1
a1077 1
					continue;
@


1.77
log
@Clear PQ_AOBJ before calling uvm_pagefree(), clearing up one false XXX
comment (one is fixed, one is deleted).
ok kettenis beck
@
text
@a882 1
		atomic_clearbits_int(&pg->pg_flags, PQ_AOBJ);
a993 1
			atomic_clearbits_int(&pp->pg_flags, PQ_AOBJ);
@


1.76
log
@convert pool_init nointr to waitok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.75 2014/12/18 23:59:28 tedu Exp $	*/
d883 1
d995 1
@


1.75
log
@remove two useless and unused hash penalty defines
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.74 2014/12/17 19:42:15 tedu Exp $	*/
d801 1
a801 1
	    0, 0, 0, "uaoeltpl", &pool_allocator_nointr);
d803 2
a804 2
	pool_init(&uvm_aobj_pool, sizeof(struct uvm_aobj), 0, 0, 0,
	    "aobjpl", &pool_allocator_nointr);
@


1.74
log
@remove lock.h from uvm_extern.h. another holdover from the simpletonlock
era. fix uvm including c files to include lock.h or atomic.h as necessary.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.73 2014/12/09 07:16:41 doug Exp $	*/
a903 3

#define	UAO_HASH_PENALTY 4	/* XXX: a guess */

@


1.73
log
@Sprinkle in a little more mallocarray().

ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.72 2014/11/21 07:18:44 tedu Exp $	*/
d46 1
@


1.72
log
@somebody permitted simple_locks back into uvm. they will be referred to
the bureau of party loyalty for reeducation.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.71 2014/11/18 23:55:01 krw Exp $	*/
d488 1
a488 1
	new_swslots = malloc(pages * sizeof(int), M_UVMAOBJ,
d523 1
a523 1
	new_swslots = malloc(pages * sizeof(int), M_UVMAOBJ,
d578 1
a578 1
	new_swslots = malloc(pages * sizeof(int), M_UVMAOBJ,
d751 1
a751 1
			aobj->u_swslots = malloc(pages * sizeof(int),
@


1.71
log
@Nuke yet more obvious #include duplications.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.70 2014/11/16 12:31:00 deraadt Exp $	*/
a1554 1
		simple_lock(&uvm.swap_data_lock);
a1556 1
		simple_unlock(&uvm.swap_data_lock);
@


1.70
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.69 2014/09/14 14:17:27 jsg Exp $	*/
a44 1
#include <sys/kernel.h>
@


1.69
log
@remove uneeded proc.h includes
ok mpi@@ kspillner@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.68 2014/09/08 19:42:57 kettenis Exp $	*/
d881 1
a881 1
		pmap_page_protect(pg, VM_PROT_NONE);
d973 1
a973 1
			pmap_page_protect(pp, VM_PROT_NONE);
d994 1
a994 1
			pmap_page_protect(pp, VM_PROT_NONE);
d1421 1
a1421 1
		     &pg, &npages, 0, VM_PROT_READ|VM_PROT_WRITE, 0, 0);
@


1.68
log
@Don't sleep on allocation of hash table entries.  Should fix crashes people
have been seeing with tmpfs.  Based on a similar fix from Bitrig by
Owain Ainsworth.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.67 2014/07/12 18:44:01 tedu Exp $	*/
a41 1
#include <sys/proc.h>
@


1.67
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.66 2014/07/11 16:35:40 jsg Exp $	*/
d240 11
a250 1
	elt = pool_get(&uao_swhash_elt_pool, PR_WAITOK | PR_ZERO);
@


1.66
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.65 2014/07/03 11:38:46 kettenis Exp $	*/
d382 1
a382 1
		free(aobj->u_swhash, M_UVMAOBJ);
d396 1
a396 1
		free(aobj->u_swslots, M_UVMAOBJ);
d464 1
a464 1
	free(aobj->u_swhash, M_UVMAOBJ);
d501 1
a501 1
	free(aobj->u_swhash, M_UVMAOBJ);
d525 1
a525 1
	free(aobj->u_swslots, M_UVMAOBJ);
d578 1
a578 1
	free(aobj->u_swslots, M_UVMAOBJ);
d621 1
a621 1
	free(aobj->u_swhash, M_UVMAOBJ);
d657 1
a657 1
	free(old_swslots, M_UVMAOBJ);
@


1.65
log
@It is important that we don't release the kernel lock between issuing a
wakeup and clearing the PG_BUSY and PG_WANTED flags, so try to keep those
bits as close together and defenitely avoid calling random code in between.

ok guenther@@, tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.64 2014/05/08 20:08:50 kettenis Exp $	*/
a16 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed by Charles D. Cranor and
 *      Washington University.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
@


1.64
log
@Fix some potential integer overflows caused by converting a page number into
an offset/size/address by shifting by PAGE_SHIFT.  Make uvm_objwrire/unwire
use voff_t instead of off_t.  The former is the right type here even if it is
equivalent to the latter.

Inspired by a somewhat similar changes in Bitrig.

ok deraadt@@, guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.63 2014/04/30 19:25:14 kettenis Exp $	*/
a1209 3
				if (ptmp->pg_flags & PG_WANTED)
					wakeup(ptmp);

d1220 2
@


1.63
log
@Make sure we flush discarded pages even if the number of hash buckets doesn't
change.  From Pedro Martelletto via bitrig.

ok beck@@, krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.62 2014/04/30 16:07:31 kettenis Exp $	*/
d425 2
a426 1
	uao_flush(uobj, startpg << PAGE_SHIFT, endpg << PAGE_SHIFT, PGO_FREE);
d913 1
a913 1
		stop = aobj->u_pages << PAGE_SHIFT;
d917 1
a917 1
		if (stop > (aobj->u_pages << PAGE_SHIFT)) {
d920 1
a920 1
			stop = aobj->u_pages << PAGE_SHIFT;
d1418 1
a1418 1
	rv = uao_get(&aobj->u_obj, pageidx << PAGE_SHIFT,
d1515 1
a1515 1
					    (UAO_SWHASH_ELT_PAGEIDX_BASE(elt)
@


1.62
log
@Assigning list pointers doesn't really work with doubly linked lists.  Use
a remove-and-insert-all-items approach for now and remove the comments that
suggest manipulating list pointers.  Pointed out by Pedro Martelletto.

ok beck@@, krw@@, mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.61 2014/04/13 23:14:15 tedu Exp $	*/
d445 1
@


1.61
log
@compress code by turning four line comments into one line comments.
emphatic ok usual suspects, grudging ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.60 2013/12/09 08:24:29 espie Exp $	*/
d434 1
d460 7
a466 2
	for (i = 0; i < UAO_SWHASH_BUCKETS(aobj->u_pages); i++)
		LIST_FIRST(&new_swhash[i]) = LIST_FIRST(&aobj->u_swhash[i]);
a617 1
		/* XXX pedro: shouldn't copying the list pointers be enough? */
@


1.60
log
@forgot to say: aobj grow/shrink tweaks are mostly from Pedro Martelletto,
like the rest of tmpfs
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.59 2013/12/08 21:16:34 espie Exp $	*/
a73 1

a116 1

a126 1

a131 1

a140 1

a161 1

a166 1

a193 1

a210 1

a217 1

a220 1

a224 1

d235 1
a235 3
	/*
	 * now search the bucket for the requested tag
	 */
d245 1
a245 4

	/*
	 * allocate a new entry for the bucket and init/insert it in
	 */
d260 1
a260 4
	/*
	 * if noswap flag is set, then we never return a slot
	 */

d264 1
a264 4
	/*
	 * if hashing, look in hash table.
	 */

d275 1
a275 3
	/* 
	 * otherwise, look in the array
	 */
d290 1
a290 4
	/*
	 * if noswap flag is set, then we can't set a slot
	 */

a291 1

d297 2
a298 1
	    panic("uao_set_swslot: attempt to set a slot on a NOSWAP object");
d301 1
a301 4
	/*
	 * are we using a hash table?  if so, add it in the hash.
	 */

a302 1

a307 1

a321 1

a341 1

a376 1

d392 1
a392 4
		/*
		 * free the array
		 */

a397 1

d405 1
a405 3
	/*
	 * finally free the aobj itself
	 */
a442 1

a458 1

d485 1
a485 4
	/*
	 * Convert swap slots from hash to array.
	 */

a543 1

a559 1

a598 1

d643 1
a643 4
	/*
	 * Set these now, so we can use uao_find_swhash_elt().
	 */

a674 1

d701 1
a701 3
	/*
	 * malloc a new aobj unless we are asked for the kernel object
	 */
d723 1
a723 3
	/*
 	 * allocate hash/array if necessary
 	 */
d762 1
a762 3
	/*
 	 * now that aobj is ready, add it to the global list
 	 */
a766 3
	/*
 	 * done!
 	 */
d813 1
a813 4
	/*
 	 * kernel_object already has plenty of references, leave it alone.
 	 */

d842 1
a842 3
	/*
 	 * detaching from kernel_object is a noop.
 	 */
d852 1
a852 3
	/*
 	 * remove the aobj from the global list.
 	 */
d877 1
a877 3
	/*
 	 * finally, free the rest.
 	 */
a969 1

a991 1

d1027 1
a1027 3
	/*
 	 * get number of pages
 	 */
d1030 1
a1030 4
	/*
 	 * step 1: handled the case where fault data structures are locked.
 	 */

d1032 1
a1032 3
		/*
 		 * step 1a: get pages that are already resident.
		 */
d1063 1
a1063 3
			/*
			 * to be useful must get a non-busy page
			 */
d1083 1
a1083 1
		}	/* "for" lcv loop */
a1088 1

a1101 1

a1103 1

a1107 1

d1193 1
a1193 3
		/*
 		 * just zero the page if there's nothing in swap.
 		 */
d1195 1
a1195 3
			/*
			 * page hasn't existed before, just zero it.
			 */
d1198 1
a1198 3
			/*
			 * page in the swapped-out page.
			 */
d1201 2
a1202 5
			/*
			 * I/O done.  check for errors.
			 */
			if (rv != VM_PAGER_OK)
			{
a1250 1

a1262 1

d1273 1
a1273 4
	/*
	 * walk the list of all aobjs.
	 */

d1324 1
a1324 3
	/*
	 * done with traversal, unlock the list
	 */
a1331 1

d1356 1
a1356 3
					/*
					 * if the slot isn't in range, skip it.
					 */
d1383 1
a1383 3
			/*
			 * if the slot isn't in range, skip it
			 */
d1388 1
a1388 3
			/*
			 * process the page.
			 */
a1425 1

d1437 1
a1437 3
	/*
	 * deactivate the page (to put it on a page queue).
	 */
a1454 1

a1545 1

@


1.59
log
@allow aobj to shrink grow, for tmpfs support
A few tweaks asked by kettenis@@
(code doesn't affect systems without mounted tmpfs at all)

okay beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.58 2013/05/30 16:39:26 tedu Exp $	*/
@


1.58
log
@UVM_UNLOCK_AND_WAIT no longer unlocks, so rename it to UVM_WAIT.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.57 2013/05/30 16:29:46 tedu Exp $	*/
d53 1
d83 2
a104 2
#define UAO_USES_SWHASH(AOBJ) \
	((AOBJ)->u_pages > UAO_SWHASH_THRESHOLD)	/* use hash? */
d110 2
a111 3
#define UAO_SWHASH_BUCKETS(AOBJ) \
	(min((AOBJ)->u_pages >> UAO_SWHASH_CLUSTER_SHIFT, \
	     UAO_SWHASH_MAXBUCKETS))
d186 10
d288 1
a288 1
	if (UAO_USES_SWHASH(aobj)) {
d333 1
a333 1
	if (UAO_USES_SWHASH(aobj)) {
d390 1
a390 1
	if (UAO_USES_SWHASH(aobj)) {
d456 282
d740 1
a740 1
 * => for normal use, flags are always zero
d752 1
d782 5
a786 3
	if (flags == 0 || (flags & UAO_FLAG_KERNSWAP) != 0) {
		int mflags = (flags & UAO_FLAG_KERNSWAP) != 0 ?
		    M_NOWAIT : M_WAITOK;
d789 2
a790 2
		if (UAO_USES_SWHASH(aobj)) {
			aobj->u_swhash = hashinit(UAO_SWHASH_BUCKETS(aobj),
d792 5
a796 1
			if (aobj->u_swhash == NULL)
d798 1
d802 5
a806 1
			if (aobj->u_swslots == NULL)
d808 1
d811 1
a811 1
		if (flags) {
d1446 1
a1446 1
	if (UAO_USES_SWHASH(aobj)) {
d1557 110
@


1.57
log
@remove lots of comments about locking per beck's request
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.56 2013/05/30 15:17:59 tedu Exp $	*/
d632 1
a632 2
			UVM_UNLOCK_AND_WAIT(pg, &uobj->vmobjlock, 0,
			    "uao_det", 0);
d705 1
a705 2
			UVM_UNLOCK_AND_WAIT(pp, &uobj->vmobjlock, 0,
			    "uaoflsh", 0);
d946 1
a946 2
				UVM_UNLOCK_AND_WAIT(ptmp, &uobj->vmobjlock,
				    FALSE, "uao_get", 0);
@


1.56
log
@remove simple_locks from uvm code. ok beck deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.55 2012/04/12 14:59:26 ariane Exp $	*/
d147 1
a147 1
	struct uvm_object u_obj; /* has: lock, pgops, memt, #pages, #refs */
a224 2
 *
 * => the object should be locked by the caller
a261 2
 *
 * => object must be locked by caller 
a297 1
 * => object must be locked by caller
a487 3
 	 *
 	 * note: in the KERNSWAP case no need to worry about locking since
 	 * we are still booting we should be the only thread around.
a556 3
 *
 * => aobj must be unlocked
 * => just lock it and call the locked version
d565 1
a565 6
 * uao_reference_locked: add a ref to an aobj that is already locked
 *
 * => aobj must be locked
 * this needs to be separate from the normal routine
 * since sometimes we need to add a reference to an aobj when
 * it's already locked.
a583 3
 *
 * => aobj must be unlocked
 * => just lock it and call the locked version
d595 1
a595 4
 * => aobj must be locked, and is unlocked (or freed) upon return.
 * this needs to be separate from the normal routine
 * since sometimes we need to detach from an aobj when
 * it's already locked.
d652 1
a652 8
 * => object should be locked by caller.  we may _unlock_ the object
 *	if (and only if) we need to clean a page (PGO_CLEANIT).
 *	XXXJRT Currently, however, we don't.  In the case of cleaning
 *	XXXJRT a page, we simply just deactivate it.  Should probably
 *	XXXJRT handle this better, in the future (although "flushing"
 *	XXXJRT anonymous memory isn't terribly important).
 * => if PGO_CLEANIT is not set, then we will neither unlock the object
 *	or block.
a691 1
	/* locked: uobj */
a782 2
 * => prefer map unlocked (not required)
 * => object must be locked!  we will _unlock_ it before starting any I/O.
d809 2
a810 4
 		 * step 1a: get pages that are already resident.   only do
		 * this if the data structures are locked (i.e. the first
		 * time through).
 		 */
d854 1
a854 1
			 * useful page: busy/lock it and plug it in our
d881 1
a881 1
 	 * object is locked.   data structures are unlocked.
a988 1
			 * unlock object for i/o, relock when done.
a1038 4
	/*
 	 * finally, unlock object and return.
 	 */

a1043 2
 * 
 * => aobj must be locked or have a reference count of 0.
a1061 1
 * => nothing should be locked.
a1136 1
 * => aobj must be locked and is returned locked.
a1209 2
 *
 * => aobj must be locked and is returned locked.
a1218 1
	/* locked: aobj */
a1220 1
	/* unlocked: aobj */
@


1.55
log
@Remove dead UBC code
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.54 2011/07/03 18:34:14 oga Exp $	*/
a384 2
	simple_unlock(&aobj->u_obj.vmobjlock);

a411 1
					simple_lock(&uvm.swap_data_lock);
a412 1
					simple_unlock(&uvm.swap_data_lock);
a433 1
				simple_lock(&uvm.swap_data_lock);
a434 1
				simple_unlock(&uvm.swap_data_lock);
a571 1
	simple_lock(&uobj->vmobjlock);
a572 1
	simple_unlock(&uobj->vmobjlock);
a606 1
	simple_lock(&uobj->vmobjlock);
a628 1
		simple_unlock(&uobj->vmobjlock);
a633 1
		simple_unlock(&uobj->vmobjlock);
a655 1
			simple_lock(&uobj->vmobjlock);
a737 1
			simple_lock(&uobj->vmobjlock);
a960 1
					simple_unlock(&uobj->vmobjlock);
a961 1
					simple_lock(&uobj->vmobjlock);
a983 1
				simple_lock(&uobj->vmobjlock);
a1024 1
			simple_unlock(&uobj->vmobjlock);
a1025 1
			simple_lock(&uobj->vmobjlock);
a1051 1
				simple_unlock(&uobj->vmobjlock);
a1077 1
	simple_unlock(&uobj->vmobjlock);
a1114 1
restart:
a1122 15
		 * try to get the object lock,
		 * start all over if we fail.
		 * most of the time we'll get the aobj lock,
		 * so this should be a rare case.
		 */
		if (!simple_lock_try(&aobj->u_obj.vmobjlock)) {
			mtx_leave(&uao_list_lock);
			if (prevaobj) {
				uao_detach_locked(&prevaobj->u_obj);
				prevaobj = NULL;
			}
			goto restart;
		}

		/*
a1267 5

	/*
	 * relock and finish up.
	 */
	simple_lock(&aobj->u_obj.vmobjlock);
@


1.54
log
@Rip out and burn support for UVM_HIST.

The vm hackers don't use it, don't maintain it and have to look at it all the
time. About time this 800 lines of code hit /dev/null.

``never liked it'' tedu@@. ariane@@ was very happy when i told her i wrote
this diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.53 2011/05/10 21:48:17 oga Exp $	*/
a1337 3
#ifndef UBC
	pmap_page_protect(pg, VM_PROT_NONE);
#endif
@


1.53
log
@Don't leak swapslots when doing a uvm_km_pgremove and a page is in swap only.

Before we were only calling uao_dropswap() if there was a page, maning
that if the buffer was swapped out then we would leak the slot.

Quite rare because only pipebuffers should swap from the kernel object,
but i've seen panics that implied this had happened (alpha.p for example).

ok thib@@ after a lot of discussion and checking the semantics.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.52 2011/05/07 15:31:25 oga Exp $	*/
a308 3
	UVMHIST_FUNC("uao_set_swslot"); UVMHIST_CALLED(pdhist);
	UVMHIST_LOG(pdhist, "aobj %p pageidx %ld slot %ld",
	    aobj, pageidx, slot, 0);
a593 1
	UVMHIST_FUNC("uao_reference"); UVMHIST_CALLED(maphist);
a602 2
	UVMHIST_LOG(maphist, "<- done (uobj=%p, ref = %ld)", 
		    uobj, uobj->uo_refs,0,0);
a632 1
	UVMHIST_FUNC("uao_detach"); UVMHIST_CALLED(maphist);
a641 1
	UVMHIST_LOG(maphist,"  (uobj=%p)  ref=%ld", uobj,uobj->uo_refs,0,0);
a644 1
		UVMHIST_LOG(maphist, "<- done (rc>0)", 0,0,0,0);
a711 1
	UVMHIST_FUNC("uao_flush"); UVMHIST_CALLED(maphist);
a725 3
	UVMHIST_LOG(maphist, " flush start=0x%lx, stop=0x%lx, flags=0x%lx",
	    (u_long)start, (u_long)stop, flags, 0);

d730 1
a730 3
	if ((flags & (PGO_DEACTIVATE|PGO_FREE)) == 0) {
		UVMHIST_LOG(maphist,
		    "<- done (no work to do)",0,0,0,0);
a731 1
	}
a810 2
	UVMHIST_LOG(maphist,
	    "<- done, rv=TRUE",0,0,0,0);
a841 4
	UVMHIST_FUNC("uao_get"); UVMHIST_CALLED(pdhist);

	UVMHIST_LOG(pdhist, "aobj=%p offset=%ld, flags=%ld",
		    aobj, (u_long)offset, flags,0);
a916 2
		UVMHIST_LOG(pdhist, "<- done (done=%ld)", done, 0,0,0);

a974 2
					UVMHIST_LOG(pdhist,
					    "sleeping, ptmp == NULL\n",0,0,0,0);
a996 3
				UVMHIST_LOG(pdhist,
				    "sleeping, ptmp->flags 0x%lx\n",
				    ptmp->pg_flags,0,0,0);
a1036 3
			UVMHIST_LOG(pdhist, "pagein from swslot %ld",
			     swslot, 0,0,0);

a1049 2
				UVMHIST_LOG(pdhist, "<- done (error=%ld)",
				    rv,0,0,0);
a1097 1
	UVMHIST_LOG(pdhist, "<- done (OK)",0,0,0,0);
@


1.52
log
@Uvm Anonymous Objects depending on size either use an array or a
hashtable to keep the list of swap slots in use in. Only one of these
will be in use at any one tmie, so shave some bytes and make it a union.

ok thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.51 2010/07/02 02:08:53 syuu Exp $	*/
d1142 1
a1142 1
void
d1151 1
@


1.51
log
@fix corrupt UVMHIST_LOG
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.50 2010/04/30 21:56:39 oga Exp $	*/
d150 10
a159 6
	int *u_swslots;		 /* array of offset->swapslot mappings */
				 /*
				  * hashtable of offset->swapslot mappings
				  * (u_swhash is an array of bucket heads)
				  */
	struct uao_swhash *u_swhash;
@


1.50
log
@Right now, if anything internal changes with a uvm object, diverse
places in the tree need to be touched to update the object
initialisation with respect to that.

So, make a function (uvm_initobj) that takes the refcount, object and
pager ops and does this initialisation for us. This should save on
maintainance in the future.

looked good to fgs@@. Tedu complained about the British spelling but OKed
it anyway.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.49 2010/04/30 20:50:53 oga Exp $	*/
d824 1
a824 1
	    "<- done, rv=%ld",retval,0,0,0);
@


1.49
log
@Prevent a possible case of lock recursion in swapoff.

If when we have successfully swapped an aobj back in, then we release our
reference count, and that reference is the last reference, we will free the
the aobj and recursively lock the list lock.

Fix this by keeping track of the last object we had a reference on, and
releasing the refcount the next time we unlock the list lock.

Put a couple of comments in explaining lock ordering in this file.

noticed by, discussed with and ok guenther@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.48 2010/04/25 23:02:22 oga Exp $	*/
d469 1
a482 1
		aobj->u_obj.uo_refs = UVM_OBJ_KERN;
d493 1
a493 1
		aobj->u_obj.uo_refs = 1;	/* start with 1 reference */
d526 1
a526 7
	/*
 	 * init aobj fields
 	 */
	simple_lock_init(&aobj->u_obj.vmobjlock);
	aobj->u_obj.pgops = &aobj_pager;
	RB_INIT(&aobj->u_obj.memt);
	aobj->u_obj.uo_npages = 0;
@


1.48
log
@Switch uao_list_lock protecting the uao list (for create, destroy and
most importantly swapoff) over to a mutex. No idea how many times i've
written this diff in the past.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.47 2009/08/06 15:28:14 oga Exp $	*/
d199 5
d1165 1
a1165 1
	struct uvm_aobj *aobj, *nextaobj;
d1187 4
d1206 5
d1227 8
a1234 1
		uao_detach_locked(&aobj->u_obj);
d1241 3
@


1.47
log
@reintroduce the uvm_tree commit.

Now instead of the global object hashtable, we have a per object tree.

Testing shows no performance difference and a slight code shrink. OTOH when
locking is more fine grained this should be faster due to lock contention on
uvm.hashlock.

ok thib@@, art@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.46 2009/07/22 21:05:37 oga Exp $	*/
d201 2
a202 2
static LIST_HEAD(aobjlist, uvm_aobj) uao_list;
static simple_lock_data_t uao_list_lock;
d532 1
a532 1
	simple_lock(&uao_list_lock);
d534 1
a534 1
	simple_unlock(&uao_list_lock);
a557 3
	LIST_INIT(&uao_list);
	simple_lock_init(&uao_list_lock);

d657 1
a657 1
	simple_lock(&uao_list_lock);
d659 1
a659 1
	simple_unlock(&uao_list_lock);
d1167 1
a1167 1
	simple_lock(&uao_list_lock);
d1181 1
a1181 1
			simple_unlock(&uao_list_lock);
d1193 1
d1195 1
a1195 1
		simple_unlock(&uao_list_lock);
d1211 1
a1211 1
		simple_lock(&uao_list_lock);
d1219 1
a1219 1
	simple_unlock(&uao_list_lock);
@


1.46
log
@Put the PG_RELEASED changes diff back in.

This has has been tested very very thoroughly on all archs we have
excepting 88k and 68k. Please see cvs log for the individual commit
messages.

ok beck@@, thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.45 2009/06/16 23:54:57 oga Exp $	*/
d147 1
a147 1
	struct uvm_object u_obj; /* has: lock, pgops, memq, #pages, #refs */
d526 1
a526 1
	TAILQ_INIT(&aobj->u_obj.memq);
d670 1
a670 1
	while((pg = TAILQ_FIRST(&uobj->memq)) != NULL) {
a704 5
 * => NOTE: we rely on the fact that the object's memq is a TAILQ and
 *	that new pages are inserted on the tail end of the list.  thus,
 *	we can make a complete pass through the object in one go by starting
 *	at the head and working towards the tail (new pages are put in
 *	front of us).
@


1.45
log
@date based reversion of uvm to the 4th May.

We still have no idea why this stops the crashes. but it does.

a machine forced to 64mb of ram cycled 10GB through swap with this diff
and is still running as I type this. Other tests by ariane@@ and thib@@
also seem to show that it's alright.

ok deraadt@@, thib@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.37 2009/05/02 12:54:42 oga Exp $	*/
d142 1
a142 1
 *   (struct uvm_device *) == (struct uvm_object *)
a178 2
static boolean_t		 uao_releasepg(struct vm_page *,
				     struct vm_page **);
a194 4
	NULL,			/* put (done by pagedaemon) */
	NULL,			/* cluster */
	NULL,			/* mk_pcluster */
	uao_releasepg		/* releasepg */
d562 1
a562 1
	 * NOTE: Pages fror this pool must not come from a pageable
d638 1
a638 2
	struct vm_page *pg, *next;
	boolean_t busybody;
d665 3
a667 2
 	 * free all the pages that aren't PG_BUSY,
	 * mark for release any that are.
d669 2
a670 3
	busybody = FALSE;
	for (pg = TAILQ_FIRST(&uobj->memq); pg != NULL; pg = next) {
		next = TAILQ_NEXT(pg, listq);
d672 6
a677 2
			atomic_setbits_int(&pg->pg_flags, PG_RELEASED);
			busybody = TRUE;
a679 2

		/* zap the mappings, free the swap slot, free the page */
a681 1
		uvm_lock_pageq();
a682 11
		uvm_unlock_pageq();
	}

	/*
 	 * if we found any busy pages, we're done for now.
 	 * mark the aobj for death, releasepg will finish up for us.
 	 */
	if (busybody) {
		aobj->u_flags |= UAO_FLAG_KILLME;
		simple_unlock(&aobj->u_obj.vmobjlock);
		return;
d684 1
a715 29
 *
 * comment on "cleaning" object and PG_BUSY pages:
 *	this routine is holding the lock on the object.  the only time
 *	that is can run into a PG_BUSY page that it does not own is if
 *	some other process has started I/O on the page (e.g. either
 *	a pagein or a pageout).  if the PG_BUSY page is being paged
 *	in, then it can not be dirty (!PG_CLEAN) because no one has
 *	had a change to modify it yet.  if the PG_BUSY page is being
 *	paged out then it means that someone else has already started
 *	cleaning the page for us (how nice!).  in this case, if we
 *	have syncio specified, then after we make our pass through the
 *	object we need to wait for the other PG_BUSY pages to clear
 *	off (i.e. we need to do an iosync).  also note that once a
 *	page is PG_BUSY is must stary in its object until it is un-busyed.
 *	XXXJRT We never actually do this, as we are "flushing" anonymous
 *	XXXJRT memory, which doesn't have persistent backing store.
 *
 * note on page traversal:
 *	we can traverse the pages in an object either by going down the
 *	linked list in "uobj->memq", or we can go over the address range
 *	by page doing hash table lookups for each address.  depending
 *	on how many pages are in the object it may be cheaper to do one
 *	or the other.  we set "by_list" to true if we are using memq.
 *	if the cost of a hash lookup was equal to the cost of the list
 *	traversal we could compare the number of pages in the start->stop
 *	range to the total number of pages in the object.  however, it
 *	seems that a hash table lookup is more expensive than the linked
 *	list traversal, so we multiply the number of pages in the
 *	start->stop range by a penalty which we define below.
d724 1
a724 2
	struct vm_page *pp, *ppnext;
	boolean_t retval, by_list;
a727 4
	curoff = 0;	/* XXX: shut up gcc */

	retval = TRUE;	/* default to success */

a730 1
		by_list = TRUE;		/* always go by the list */
a738 2
		by_list = (uobj->uo_npages <=
		    ((stop - start) >> PAGE_SHIFT) * UAO_HASH_PENALTY);
d741 2
a742 3
	UVMHIST_LOG(maphist,
	    " flush start=0x%lx, stop=0x%lx, by_list=%ld, flags=0x%lx",
	    (u_long)start, (u_long)stop, by_list, flags);
d751 1
a751 1
		return (retval);
d754 7
a760 24
	/*
	 * now do it.  note: we must update ppnext in the body of loop or we
	 * will get stuck.  we need to use ppnext because we may free "pp"
	 * before doing the next loop.
	 */

	if (by_list) {
		pp = TAILQ_FIRST(&uobj->memq);
	} else {
		curoff = start;
		pp = uvm_pagelookup(uobj, curoff);
	}

	ppnext = NULL;	/* XXX: shut up gcc */
	uvm_lock_pageq();	/* page queues locked */

	/* locked: both page queues and uobj */
	for ( ; (by_list && pp != NULL) ||
	    (!by_list && curoff < stop) ; pp = ppnext) {
		if (by_list) {
			ppnext = TAILQ_NEXT(pp, listq);

			/* range check */
			if (pp->offset < start || pp->offset >= stop)
d763 2
a764 3
			curoff += PAGE_SIZE;
			if (curoff < stop)
				ppnext = uvm_pagelookup(uobj, curoff);
d766 8
a773 3
			/* null check */
			if (pp == NULL)
				continue;
d775 1
a775 1
		
d784 1
d786 1
d794 1
a794 4
#ifdef UBC
			/* ...and deactivate the page. */
			pmap_clear_reference(pp);
#else
a798 1
#endif
d800 1
a816 8
			/*
			 * mark the page as released if its busy.
			 */
			if (pp->pg_flags & PG_BUSY) {
				atomic_setbits_int(&pp->pg_flags, PG_RELEASED);
				continue;
			}

d821 1
d823 1
a831 2
	uvm_unlock_pageq();

d834 1
a834 1
	return (retval);
d916 1
a916 1
			 * to be useful must get a non-busy, non-released page
d919 1
a919 1
			    (ptmp->pg_flags & (PG_BUSY|PG_RELEASED)) != 0) {
d1026 1
a1026 1
			if ((ptmp->pg_flags & (PG_BUSY|PG_RELEASED)) != 0) {
d1065 1
a1065 2
		if (swslot == 0)
		{
a1141 59
 * uao_releasepg: handle released page in an aobj
 * 
 * => "pg" is a PG_BUSY [caller owns it], PG_RELEASED page that we need
 *      to dispose of.
 * => caller must handle PG_WANTED case
 * => called with page's object locked, pageq's unlocked
 * => returns TRUE if page's object is still alive, FALSE if we
 *      killed the page's object.    if we return TRUE, then we
 *      return with the object locked.
 * => if (nextpgp != NULL) => we return the next page on the queue, and return
 *                              with the page queues locked [for pagedaemon]
 * => if (nextpgp == NULL) => we return with page queues unlocked [normal case]
 * => we kill the aobj if it is not referenced and we are suppose to
 *      kill it ("KILLME").
 */
static boolean_t
uao_releasepg(struct vm_page *pg, struct vm_page **nextpgp /* OUT */)
{
	struct uvm_aobj *aobj = (struct uvm_aobj *) pg->uobject;

	KASSERT(pg->pg_flags & PG_RELEASED);

	/*
 	 * dispose of the page [caller handles PG_WANTED] and swap slot.
 	 */
	pmap_page_protect(pg, VM_PROT_NONE);
	uao_dropswap(&aobj->u_obj, pg->offset >> PAGE_SHIFT);
	uvm_lock_pageq();
	if (nextpgp)
		*nextpgp = TAILQ_NEXT(pg, pageq); /* next page for daemon */
	uvm_pagefree(pg);
	if (!nextpgp)
		uvm_unlock_pageq();		/* keep locked for daemon */

	/*
 	 * if we're not killing the object, we're done.
 	 */
	if ((aobj->u_flags & UAO_FLAG_KILLME) == 0)
		return TRUE;
	KASSERT(aobj->u_obj.uo_refs == 0);

	/*
 	 * if there are still pages in the object, we're done for now.
 	 */
	if (aobj->u_obj.uo_npages != 0)
		return TRUE;

	KASSERT(TAILQ_EMPTY(&aobj->u_obj.memq));

	/*
 	 * finally, free the rest.
 	 */
	uao_free(aobj);

	return FALSE;
}


/*
a1342 1
	KASSERT((pg->pg_flags & PG_RELEASED) == 0);
@


1.44
log
@Backout all the PG_RELEASED changes.

This is for the same reason as the earlier backouts, to avoid the bug
either added or exposed sometime around c2k9. This *should* be the last
one.

prompted by deraadt@@

ok ariane@@
@
text
@d170 13
a182 10
struct uao_swhash_elt	*uao_find_swhash_elt(struct uvm_aobj *, int,
			     boolean_t);
static int		 uao_find_swslot(struct uvm_aobj *, int);
boolean_t		 uao_flush(struct uvm_object *, voff_t, voff_t, int);
void			 uao_free(struct uvm_aobj *);
int			 uao_get(struct uvm_object *, voff_t, vm_page_t *,
			     int *, int, vm_prot_t, int, int);
boolean_t		 uao_releasepg(struct vm_page *, struct vm_page **);
boolean_t		 uao_pagein(struct uvm_aobj *, int, int);
boolean_t		 uao_pagein_page(struct uvm_aobj *, int);
d226 1
a226 1
struct uao_swhash_elt *
d264 1
a264 1
static __inline int
d381 1
a381 1
void
d932 1
a932 1
int
d1234 1
a1234 1
boolean_t
d1373 1
a1373 1
boolean_t
d1447 1
a1447 1
boolean_t
@


1.43
log
@Backout all changes to uvm after pmemrange (which will be backed out
separately).

a change at or just before the hackathon has either exposed or added a
very very nasty memory corruption bug that is giving us hell right now.
So in the interest of kernel stability these diffs are being backed out
until such a time as that corruption bug has been found and squashed,
then the ones that are proven good may slowly return.

a quick hitlist of the main commits this backs out:

mine:
uvm_objwire
the lock change in uvm_swap.c
using trees for uvm objects instead of the hash
removing the pgo_releasepg callback.

art@@'s:
putting pmap_page_protect(VM_PROT_NONE) in uvm_pagedeactivate() since
all callers called that just prior anyway.

ok beck@@, ariane@@.

prompted by deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.39 2009/05/08 13:50:15 ariane Exp $	*/
d142 1
a142 1
 *   (struct uvm_aobj *) == (struct uvm_object *)
d565 1
a565 1
	 * NOTE: Pages for this pool must not come from a pageable
d641 2
a642 1
	struct vm_page *pg;
d669 2
a670 3
	 * Free all pages left in the object. If they're busy, wait
	 * for them to become available before we kill it.
	 * Release swap resources then free the page.
d672 3
a674 2
	uvm_lock_pageq();
	while((pg = TAILQ_FIRST(&uobj->memq)) != NULL) {
d676 2
a677 6
			atomic_setbits_int(&pg->pg_flags, PG_WANTED);
			uvm_unlock_pageq();
			UVM_UNLOCK_AND_WAIT(pg, &uobj->vmobjlock, 0,
			    "uao_det", 0);
			simple_lock(&uobj->vmobjlock);
			uvm_lock_pageq();
d680 2
d684 1
d686 11
a697 1
	uvm_unlock_pageq();
d729 29
d766 2
a767 1
	struct vm_page *pp;
d771 4
d778 1
d787 2
d791 3
a793 2
	UVMHIST_LOG(maphist, " flush start=0x%lx, stop=0x%lx, flags=0x%lx",
	    (u_long)start, (u_long)stop, flags, 0);
d802 14
a815 1
		return (TRUE);
d818 13
a830 5
	/* locked: uobj */
	curoff = start;
	for (;;) {
		if (curoff < stop) {
			pp = uvm_pagelookup(uobj, curoff);
d832 4
a837 2
		} else {
			break;
d839 1
a839 11

		/* Make sure page is unbusy, else wait for it. */
		if (pp->pg_flags & PG_BUSY) {
			atomic_setbits_int(&pp->pg_flags, PG_WANTED);
			UVM_UNLOCK_AND_WAIT(pp, &uobj->vmobjlock, 0,
			    "uaoflsh", 0);
			simple_lock(&uobj->vmobjlock);
			curoff -= PAGE_SIZE;
			continue;
		}

a847 1
			/* FALLTHROUGH */
a848 1
			/* FALLTHROUGH */
d856 4
a859 1
			uvm_lock_pageq();
d864 1
a865 1
			uvm_unlock_pageq();
d882 8
a893 1
			uvm_lock_pageq();
a894 1
			uvm_unlock_pageq();
d903 2
d907 1
a907 1
	return (TRUE);
d989 1
a989 1
			 * to be useful must get a non-busy page
d992 1
a992 1
			    (ptmp->pg_flags & PG_BUSY) != 0) {
d1099 1
a1099 1
			if ((ptmp->pg_flags & PG_BUSY) != 0) {
d1138 2
a1139 1
		if (swslot == 0) {
d1250 21
a1270 1
	return TRUE;
d1476 1
@


1.42
log
@Since all callers of uvm_pagedeactivate did pmap_page_protect(.., VM_PROT_NONE)
just move that into uvm_pagedeactivate.

oga@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.41 2009/06/02 23:00:19 oga Exp $	*/
d177 1
d194 4
d529 1
a529 1
	RB_INIT(&aobj->u_obj.memt);
d673 1
a673 1
	while((pg = RB_ROOT(&uobj->memt)) != NULL) {
d798 2
d801 1
a801 1
			/* Deactivate the page. */
d1145 39
d1399 3
@


1.41
log
@Instead of the global hash table with the terrible hashfunction and a
global lock, switch the uvm object pages to being kept in a per-object
RB_TREE. Right now this is approximately the same speed, but cleaner.
When biglock usage is reduced this will improve concurrency due to lock
contention..

ok beck@@ art@@. Thanks to jasper for the speed testing.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.40 2009/06/01 19:54:02 oga Exp $	*/
a792 2
			/* zap all mappings for the page. */
			pmap_page_protect(pp, VM_PROT_NONE);
d794 1
a794 1
			/* ...and deactivate the page. */
a1352 3
#ifndef UBC
	pmap_page_protect(pg, VM_PROT_NONE);
#endif
@


1.40
log
@Since we've now cleared up a lot of the PG_RELEASED setting, remove the
pgo_releasepg() hook and just free the page the "normal" way in the one
place we'll ever see PG_RELEASED and should care (uvm_page_unbusy,
called in aiodoned).

ok art@@, beck@@, thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.39 2009/05/08 13:50:15 ariane Exp $	*/
d524 1
a524 1
	TAILQ_INIT(&aobj->u_obj.memq);
d668 1
a668 1
	while((pg = TAILQ_FIRST(&uobj->memq)) != NULL) {
@


1.39
log
@Remove static qualifier of functions that are not inline.
Makes trace in ddb useful.

ok oga
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.38 2009/05/05 05:12:17 oga Exp $	*/
a176 1
boolean_t		 uao_releasepg(struct vm_page *, struct vm_page **);
a192 4
	NULL,			/* put (done by pagedaemon) */
	NULL,			/* cluster */
	NULL,			/* mk_pcluster */
	uao_releasepg		/* releasepg */
a1137 39

/*
 * uao_releasepg: handle released page in an aobj
 * 
 * => "pg" is a PG_BUSY [caller owns it], PG_RELEASED page that we need
 *      to dispose of.
 * => caller must handle PG_WANTED case
 * => called with page's object locked, pageq's unlocked
 * => returns TRUE if page's object is still alive, FALSE if we
 *      killed the page's object.    if we return TRUE, then we
 *      return with the object locked.
 * => if (nextpgp != NULL) => we return the next page on the queue, and return
 *                              with the page queues locked [for pagedaemon]
 * => if (nextpgp == NULL) => we return with page queues unlocked [normal case]
 * => we kill the aobj if it is not referenced and we are suppose to
 *      kill it ("KILLME").
 */
boolean_t
uao_releasepg(struct vm_page *pg, struct vm_page **nextpgp /* OUT */)
{
	struct uvm_aobj *aobj = (struct uvm_aobj *) pg->uobject;

	KASSERT(pg->pg_flags & PG_RELEASED);

	/*
 	 * dispose of the page [caller handles PG_WANTED] and swap slot.
 	 */
	pmap_page_protect(pg, VM_PROT_NONE);
	uao_dropswap(&aobj->u_obj, pg->offset >> PAGE_SHIFT);
	uvm_lock_pageq();
	if (nextpgp)
		*nextpgp = TAILQ_NEXT(pg, pageq); /* next page for daemon */
	uvm_pagefree(pg);
	if (!nextpgp)
		uvm_unlock_pageq();		/* keep locked for daemon */

	return TRUE;
}

@


1.38
log
@The first step in cleaning up the use of PG_RELEASED for uvm objects.

Now, the PG_ RELEASED flag currently has two (maybe three) uses. The
valid one is for use with async io where we want to free the page after
we've paged it out.  The other ones are "oh i'd like to free this, but
someone else is playing with it". It's simpler to just sleep on the
damned page instead and stop the fiddling.

First step does uao's: in uao_detach, sleep on the object and free it
when we're clean, instead of setting a flag so it's freed after. In
uao_flush, do the same. Change the interation over the object in flush
so that we don't have to add marker pages or other such voodoo to the
list when we sleep (netbsd did that when they had a similar diff), just
use the hash always. We can now change uao_releasepg() to just free the
page, and not bother with the KILLME stuff. When the other objects are
fixed this hook will vanish.

Much discussion with art@@ over the idea, and ariane@@ over this specific
diff. As mentioned, this one is based loosely on a similar idea in
netbsd.

Been in my tree for a while, survived many make builds, etc, and forcing
paging using ariane's evil program.

ok ariane@@, beck@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.37 2009/05/02 12:54:42 oga Exp $	*/
d170 10
a179 13
static struct uao_swhash_elt	*uao_find_swhash_elt(struct uvm_aobj *, int,
				     boolean_t);
static int			 uao_find_swslot(struct uvm_aobj *, int);
static boolean_t		 uao_flush(struct uvm_object *, voff_t,
				     voff_t, int);
static void			 uao_free(struct uvm_aobj *);
static int			 uao_get(struct uvm_object *, voff_t,
				     vm_page_t *, int *, int, vm_prot_t,
				     int, int);
static boolean_t		 uao_releasepg(struct vm_page *,
				     struct vm_page **);
static boolean_t		 uao_pagein(struct uvm_aobj *, int, int);
static boolean_t		 uao_pagein_page(struct uvm_aobj *, int);
d223 1
a223 1
static struct uao_swhash_elt *
d261 1
a261 1
__inline static int
d378 1
a378 1
static void
d859 1
a859 1
static int
d1160 1
a1160 1
static boolean_t
d1279 1
a1279 1
static boolean_t
d1353 1
a1353 1
static boolean_t
@


1.37
log
@a few more memset changes.

two cases of pool_get() + memset(0) -> pool_get(,,,PR_ZERO)
1.5 cases of global variables are already zeroed, so don't zero them.

ok ariane@@, comments on stuff i'd missed from blambert@@ and cnst@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.36 2009/03/20 15:19:04 oga Exp $	*/
d142 1
a142 1
 *   (struct uvm_device *) == (struct uvm_object *)
d568 1
a568 1
	 * NOTE: Pages fror this pool must not come from a pageable
d644 1
a644 2
	struct vm_page *pg, *next;
	boolean_t busybody;
d671 3
a673 2
 	 * free all the pages that aren't PG_BUSY,
	 * mark for release any that are.
d675 2
a676 3
	busybody = FALSE;
	for (pg = TAILQ_FIRST(&uobj->memq); pg != NULL; pg = next) {
		next = TAILQ_NEXT(pg, listq);
d678 6
a683 2
			atomic_setbits_int(&pg->pg_flags, PG_RELEASED);
			busybody = TRUE;
a685 2

		/* zap the mappings, free the swap slot, free the page */
a687 1
		uvm_lock_pageq();
a688 11
		uvm_unlock_pageq();
	}

	/*
 	 * if we found any busy pages, we're done for now.
 	 * mark the aobj for death, releasepg will finish up for us.
 	 */
	if (busybody) {
		aobj->u_flags |= UAO_FLAG_KILLME;
		simple_unlock(&aobj->u_obj.vmobjlock);
		return;
d690 1
a721 29
 *
 * comment on "cleaning" object and PG_BUSY pages:
 *	this routine is holding the lock on the object.  the only time
 *	that is can run into a PG_BUSY page that it does not own is if
 *	some other process has started I/O on the page (e.g. either
 *	a pagein or a pageout).  if the PG_BUSY page is being paged
 *	in, then it can not be dirty (!PG_CLEAN) because no one has
 *	had a change to modify it yet.  if the PG_BUSY page is being
 *	paged out then it means that someone else has already started
 *	cleaning the page for us (how nice!).  in this case, if we
 *	have syncio specified, then after we make our pass through the
 *	object we need to wait for the other PG_BUSY pages to clear
 *	off (i.e. we need to do an iosync).  also note that once a
 *	page is PG_BUSY is must stary in its object until it is un-busyed.
 *	XXXJRT We never actually do this, as we are "flushing" anonymous
 *	XXXJRT memory, which doesn't have persistent backing store.
 *
 * note on page traversal:
 *	we can traverse the pages in an object either by going down the
 *	linked list in "uobj->memq", or we can go over the address range
 *	by page doing hash table lookups for each address.  depending
 *	on how many pages are in the object it may be cheaper to do one
 *	or the other.  we set "by_list" to true if we are using memq.
 *	if the cost of a hash lookup was equal to the cost of the list
 *	traversal we could compare the number of pages in the start->stop
 *	range to the total number of pages in the object.  however, it
 *	seems that a hash table lookup is more expensive than the linked
 *	list traversal, so we multiply the number of pages in the
 *	start->stop range by a penalty which we define below.
d730 1
a730 2
	struct vm_page *pp, *ppnext;
	boolean_t retval, by_list;
a733 4
	curoff = 0;	/* XXX: shut up gcc */

	retval = TRUE;	/* default to success */

a736 1
		by_list = TRUE;		/* always go by the list */
a744 2
		by_list = (uobj->uo_npages <=
		    ((stop - start) >> PAGE_SHIFT) * UAO_HASH_PENALTY);
d747 2
a748 3
	UVMHIST_LOG(maphist,
	    " flush start=0x%lx, stop=0x%lx, by_list=%ld, flags=0x%lx",
	    (u_long)start, (u_long)stop, by_list, flags);
d757 1
a757 1
		return (retval);
d760 7
a766 24
	/*
	 * now do it.  note: we must update ppnext in the body of loop or we
	 * will get stuck.  we need to use ppnext because we may free "pp"
	 * before doing the next loop.
	 */

	if (by_list) {
		pp = TAILQ_FIRST(&uobj->memq);
	} else {
		curoff = start;
		pp = uvm_pagelookup(uobj, curoff);
	}

	ppnext = NULL;	/* XXX: shut up gcc */
	uvm_lock_pageq();	/* page queues locked */

	/* locked: both page queues and uobj */
	for ( ; (by_list && pp != NULL) ||
	    (!by_list && curoff < stop) ; pp = ppnext) {
		if (by_list) {
			ppnext = TAILQ_NEXT(pp, listq);

			/* range check */
			if (pp->offset < start || pp->offset >= stop)
d769 2
a770 3
			curoff += PAGE_SIZE;
			if (curoff < stop)
				ppnext = uvm_pagelookup(uobj, curoff);
d772 8
a779 3
			/* null check */
			if (pp == NULL)
				continue;
d781 1
a781 1
		
d790 1
d792 1
d800 1
a800 4
#ifdef UBC
			/* ...and deactivate the page. */
			pmap_clear_reference(pp);
#else
a804 1
#endif
d806 1
a822 8
			/*
			 * mark the page as released if its busy.
			 */
			if (pp->pg_flags & PG_BUSY) {
				atomic_setbits_int(&pp->pg_flags, PG_RELEASED);
				continue;
			}

d827 1
d829 1
a837 2
	uvm_unlock_pageq();

d840 1
a840 1
	return (retval);
d922 1
a922 1
			 * to be useful must get a non-busy, non-released page
d925 1
a925 1
			    (ptmp->pg_flags & (PG_BUSY|PG_RELEASED)) != 0) {
d1032 1
a1032 1
			if ((ptmp->pg_flags & (PG_BUSY|PG_RELEASED)) != 0) {
d1071 1
a1071 2
		if (swslot == 0)
		{
d1182 1
a1182 21
	/*
 	 * if we're not killing the object, we're done.
 	 */
	if ((aobj->u_flags & UAO_FLAG_KILLME) == 0)
		return TRUE;
	KASSERT(aobj->u_obj.uo_refs == 0);

	/*
 	 * if there are still pages in the object, we're done for now.
 	 */
	if (aobj->u_obj.uo_npages != 0)
		return TRUE;

	KASSERT(TAILQ_EMPTY(&aobj->u_obj.memq));

	/*
 	 * finally, free the rest.
 	 */
	uao_free(aobj);

	return FALSE;
a1387 1
	KASSERT((pg->pg_flags & PG_RELEASED) == 0);
@


1.36
log
@While working on some stuff in uvm I've gotten REALLY sick of reading
K&R function declarations, so switch them all over to ansi-style, in
accordance with the prophesy.

"go for it" art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.35 2007/09/07 15:00:20 art Exp $	*/
d252 1
a252 1
	elt = pool_get(&uao_swhash_elt_pool, PR_WAITOK);
a254 2
	elt->count = 0;
	memset(elt->slots, 0, sizeof(elt->slots));
@


1.35
log
@Use M_ZERO in a few more places to shave bytes from the kernel.

eyeballed and ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.34 2007/04/13 18:57:49 art Exp $	*/
d170 2
a171 2
static struct uao_swhash_elt	*uao_find_swhash_elt(struct uvm_aobj *,
							  int, boolean_t);
d173 2
a174 2
static boolean_t		 uao_flush(struct uvm_object *,
						voff_t, voff_t, int);
d177 2
a178 2
					      vm_page_t *, int *, int,
					      vm_prot_t, int, int);
d180 1
a180 1
						    struct vm_page **);
d227 1
a227 4
uao_find_swhash_elt(aobj, pageidx, create)
	struct uvm_aobj *aobj;
	int pageidx;
	boolean_t create;
d267 1
a267 3
uao_find_swslot(aobj, pageidx)
	struct uvm_aobj *aobj;
	int pageidx;
d304 1
a304 3
uao_set_swslot(uobj, pageidx, slot)
	struct uvm_object *uobj;
	int pageidx, slot;
d384 1
a384 2
uao_free(aobj)
	struct uvm_aobj *aobj;
d467 1
a467 3
uao_create(size, flags)
	vsize_t size;
	int flags;
d558 1
a558 1
uao_init()
d587 1
a587 2
uao_reference(uobj)
	struct uvm_object *uobj;
d603 1
a603 2
uao_reference_locked(uobj)
	struct uvm_object *uobj;
d627 1
a627 2
uao_detach(uobj)
	struct uvm_object *uobj;
d643 1
a643 2
uao_detach_locked(uobj)
	struct uvm_object *uobj;
d768 1
a768 4
uao_flush(uobj, start, stop, flags)
	struct uvm_object *uobj;
	voff_t start, stop;
	int flags;
d935 2
a936 7
uao_get(uobj, offset, pps, npagesp, centeridx, access_type, advice, flags)
	struct uvm_object *uobj;
	voff_t offset;
	struct vm_page **pps;
	int *npagesp;
	int centeridx, advice, flags;
	vm_prot_t access_type;
d1237 1
a1237 3
uao_releasepg(pg, nextpgp)
	struct vm_page *pg;
	struct vm_page **nextpgp;	/* OUT */
d1286 1
a1286 3
uao_dropswap(uobj, pageidx)
	struct uvm_object *uobj;
	int pageidx;
d1304 1
a1304 2
uao_swap_off(startslot, endslot)
	int startslot, endslot;
d1376 1
a1376 3
uao_pagein(aobj, startslot, endslot)
	struct uvm_aobj *aobj;
	int startslot, endslot;
d1450 1
a1450 3
uao_pagein_page(aobj, pageidx)
	struct uvm_aobj *aobj;
	int pageidx;
@


1.34
log
@While splitting flags and pqflags might have been a good idea in theory
to separate locking, on most modern machines this is not enough
since operations on short types touch other short types that share the
same word in memory.

Merge pg_flags and pqflags again and now use atomic operations to change
the flags. Also bump wire_count to an int and pg_version might go
int as well, just for alignment.

tested by many, many. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.33 2007/04/12 18:59:55 art Exp $	*/
d527 1
a527 1
			    M_UVMAOBJ, mflags);
a529 1
			memset(aobj->u_swslots, 0, pages * sizeof(int));
@


1.33
log
@Unbreak compile with option UVMHIST after pg_flags changes.
from mickey
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.32 2007/04/04 17:44:45 art Exp $	*/
d696 1
a696 1
			pg->pg_flags |= PG_RELEASED;
d909 1
a909 1
				pp->pg_flags |= PG_RELEASED;
d1008 4
a1011 2
					ptmp->pg_flags &= ~(PG_BUSY|PG_FAKE);
					ptmp->pqflags |= PQ_AOBJ;
d1033 1
a1033 1
			ptmp->pg_flags |= PG_BUSY;	
d1117 1
a1117 1
				ptmp->pqflags |= PQ_AOBJ;
d1128 1
a1128 1
				ptmp->pg_flags |= PG_WANTED;
d1145 1
a1145 1
			ptmp->pg_flags |= PG_BUSY;
d1204 2
a1205 1
				ptmp->pg_flags &= ~(PG_WANTED|PG_BUSY);
d1227 2
a1228 1
		ptmp->pg_flags &= ~PG_FAKE;		/* data is valid ... */
d1521 1
a1521 1
	pg->pg_flags &= ~(PG_BUSY|PG_CLEAN|PG_FAKE);
@


1.32
log
@Mechanically rename the "flags" and "version" fields in struct vm_page
to "pg_flags" and "pg_version", so that they are a bit easier to work with.
Whoever uses generic names like this for a popular struct obviously doesn't
read much code.

Most architectures compile and there are no functionality changes.

deraadt@@ ok ("if something fails to compile, we fix that by hand")
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.31 2006/07/31 11:51:29 mickey Exp $	*/
d1129 1
a1129 1
				    ptmp->flags,0,0,0);
@


1.31
log
@fix uvmhist #2: args are always u_long so fix missing %d and %x and no %ll; no change for normal code
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.30 2006/07/26 23:15:55 mickey Exp $	*/
d695 2
a696 2
		if (pg->flags & PG_BUSY) {
			pg->flags |= PG_RELEASED;
d908 2
a909 2
			if (pp->flags & PG_BUSY) {
				pp->flags |= PG_RELEASED;
d1008 1
a1008 1
					ptmp->flags &= ~(PG_BUSY|PG_FAKE);
d1018 1
a1018 1
			    (ptmp->flags & (PG_BUSY|PG_RELEASED)) != 0) {
d1031 1
a1031 1
			ptmp->flags |= PG_BUSY;	
d1125 2
a1126 2
			if ((ptmp->flags & (PG_BUSY|PG_RELEASED)) != 0) {
				ptmp->flags |= PG_WANTED;
d1143 1
a1143 1
			ptmp->flags |= PG_BUSY;
d1189 1
a1189 1
				if (ptmp->flags & PG_WANTED)
d1202 1
a1202 1
				ptmp->flags &= ~(PG_WANTED|PG_BUSY);
d1224 1
a1224 1
		ptmp->flags &= ~PG_FAKE;		/* data is valid ... */
d1262 1
a1262 1
	KASSERT(pg->flags & PG_RELEASED);
d1509 1
a1509 1
	KASSERT((pg->flags & PG_RELEASED) == 0);
d1517 1
a1517 1
	pg->flags &= ~(PG_BUSY|PG_CLEAN|PG_FAKE);
@


1.30
log
@fix fmts for UVMHIST_LOG() entries making it more useful on 64bit archs; miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.29 2005/11/15 21:09:46 miod Exp $	*/
d316 1
a316 1
	UVMHIST_LOG(pdhist, "aobj %p pageidx %d slot %d",
d628 1
a628 1
	UVMHIST_LOG(maphist, "<- done (uobj=%p, ref = %d)", 
d673 1
a673 1
	UVMHIST_LOG(maphist,"  (uobj=%p)  ref=%d", uobj,uobj->uo_refs,0,0);
d815 2
a816 2
	    " flush start=0x%llx, stop=0x%llx, by_list=%d, flags=0x%x",
	    start, stop, by_list, flags);
d929 1
a929 1
	    "<- done, rv=%d",retval,0,0,0);
d968 2
a969 2
	UVMHIST_LOG(pdhist, "aobj=%p offset=%lld, flags=%d",
		    aobj, offset, flags,0);
d1043 1
a1043 1
		UVMHIST_LOG(pdhist, "<- done (done=%d)", done, 0,0,0);
d1128 1
a1128 1
				    "sleeping, ptmp->flags 0x%x\n",
d1171 1
a1171 1
			UVMHIST_LOG(pdhist, "pagein from swslot %d",
d1187 1
a1187 1
				UVMHIST_LOG(pdhist, "<- done (error=%d)",
@


1.29
log
@Only two `h' in threshold.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.28 2005/10/27 18:05:16 otto Exp $	*/
d628 1
a628 1
	UVMHIST_LOG(maphist, "<- done (uobj=0x%x, ref = %d)", 
d673 1
a673 1
	UVMHIST_LOG(maphist,"  (uobj=0x%x)  ref=%d", uobj,uobj->uo_refs,0,0);
d815 1
a815 1
	    " flush start=0x%lx, stop=0x%x, by_list=%d, flags=0x%x",
d968 1
a968 1
	UVMHIST_LOG(pdhist, "aobj=%p offset=%d, flags=%d",
@


1.28
log
@Following a next pointer of an element deleted from a list is bad
practise. Depending on the list implementation, this might or might
not work; so make it use a safe idiom. ok pedro@@ millert@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.27 2004/12/26 21:22:14 miod Exp $	*/
d97 1
a97 1
 * the swhash threshhold determines if we will use an array or a
@


1.27
log
@Use list and queue macros where applicable to make the code easier to read;
no change in compiler assembly output.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.26 2002/03/14 01:27:18 millert Exp $	*/
d661 1
a661 1
	struct vm_page *pg;
d693 2
a694 3
	for (pg = TAILQ_FIRST(&uobj->memq);
	     pg != NULL;
	     pg = TAILQ_NEXT(pg, listq)) {
@


1.26
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.25 2002/01/23 00:39:48 art Exp $	*/
d836 1
a836 1
		pp = uobj->memq.tqh_first;
@


1.25
log
@Pool deals fairly well with physical memory shortage, but it doesn't deal
well (not at all) with shortages of the vm_map where the pages are mapped
(usually kmem_map).

Try to deal with it:
 - group all information the backend allocator for a pool in a separate
   struct. The pool will only have a pointer to that struct.
 - change the pool_init API to reflect that.
 - link all pools allocating from the same allocator on a linked list.
 - Since an allocator is responsible to wait for physical memory it will
   only fail (waitok) when it runs out of its backing vm_map, carefully
   drain pools using the same allocator so that va space is freed.
   (see comments in code for caveats and details).
 - change pool_reclaim to return if it actually succeeded to free some
   memory, use that information to make draining easier and more efficient.
 - get rid of PR_URGENT, noone uses it.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.24 2001/12/19 08:58:07 art Exp $	*/
d170 7
a176 7
static struct uao_swhash_elt	*uao_find_swhash_elt __P((struct uvm_aobj *,
							  int, boolean_t));
static int			 uao_find_swslot __P((struct uvm_aobj *, int));
static boolean_t		 uao_flush __P((struct uvm_object *,
						voff_t, voff_t, int));
static void			 uao_free __P((struct uvm_aobj *));
static int			 uao_get __P((struct uvm_object *, voff_t,
d178 5
a182 5
					      vm_prot_t, int, int));
static boolean_t		 uao_releasepg __P((struct vm_page *,
						    struct vm_page **));
static boolean_t		 uao_pagein __P((struct uvm_aobj *, int, int));
static boolean_t		 uao_pagein_page __P((struct uvm_aobj *, int));
@


1.24
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.20 2001/11/11 01:16:56 art Exp $	*/
d585 1
a585 1
	    0, 0, 0, "uaoeltpl", 0, NULL, NULL, M_UVMAOBJ);
d588 1
a588 2
	    "aobjpl", 0,
	    pool_page_alloc_nointr, pool_page_free_nointr, M_UVMAOBJ);
@


1.23
log
@Sync in more uvm from NetBSD. Mostly just cosmetic stuff.
Contains also support for page coloring.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_aobj.c,v 1.22 2001/11/28 13:47:39 art Exp $	*/
/*	$NetBSD: uvm_aobj.c,v 1.45 2001/06/23 20:52:03 chs Exp $	*/
d177 1
a177 1
					      struct vm_page **, int *, int,
d186 1
a186 1
 *
d208 1
a208 1
static struct simplelock uao_list_lock;
d236 2
a237 2
	swhash = UAO_SWHASH_HASH(aobj, pageidx);
	page_tag = UAO_SWHASH_ELT_TAG(pageidx);
a241 1

d243 2
a244 3
		if (elt->tag == page_tag) {
			return elt;
		}
d246 3
a248 1
	if (!create) {
d250 1
a250 1
	}
d255 1
a255 5

	elt = pool_get(&uao_swhash_elt_pool, PR_NOWAIT);
	if (elt == NULL) {
		return NULL;
	}
d260 2
a261 1
	return elt;
d267 1
a267 1
 * => object must be locked by caller
d296 1
a296 1
	/*
a306 2
 * => we return the old slot number, or -1 if we failed to allocate
 *    memory to record the new slot number
a313 1
	struct uao_swhash_elt *elt;
d345 2
a346 1
		elt = uao_find_swhash_elt(aobj, pageidx, slot ? TRUE : FALSE);
d348 2
a349 1
			return slot ? -1 : 0;
d364 2
a365 2
		} else {
			if (oldslot)
d373 1
a373 1
	} else {
d629 1
a629 1
	UVMHIST_LOG(maphist, "<- done (uobj=0x%x, ref = %d)",
d662 1
a662 1
	struct vm_page *pg, *nextpg;
d694 3
a696 2
	for (pg = TAILQ_FIRST(&uobj->memq); pg != NULL; pg = nextpg) {
		nextpg = TAILQ_NEXT(pg, listq);
d864 1
a864 1

d881 1
d884 6
d945 1
a945 1
 * then we will need to return EBUSY.
d965 1
a965 1
	struct vm_page *ptmp;
d1024 1
a1024 1
					done = FALSE;
d1033 1
a1033 1
			ptmp->flags |= PG_BUSY;
d1050 1
a1050 1
			return(0);
d1053 1
a1053 1
			return(EBUSY);
d1110 1
a1110 1
					continue;
d1119 1
a1119 1
				/*
d1137 2
a1138 2

			/*
d1158 1
a1158 1
 		 * we have a "fake/busy/clean" page that we just allocated.
d1187 1
a1187 1
			if (rv != 0)
d1202 1
a1202 3
				if (swslot != -1) {
					uvm_swap_markbad(swslot, 1);
				}
d1215 1
a1215 1
		/*
d1218 1
a1218 1
		 * that page is still busy.
d1238 1
a1238 1
	return(0);
d1243 1
a1243 1
 *
d1304 1
a1304 1
 *
d1324 1
a1324 1
 *
d1425 1
a1425 1
					if (slot < startslot ||
d1498 1
a1498 1
	case 0:
d1501 2
a1502 2
	case EIO:
	case ERESTART:
d1505 1
a1505 1
		 * ERESTART can only mean that the anon was freed,
d1526 3
@


1.23.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.25 2002/01/23 00:39:48 art Exp $	*/
d589 1
a589 1
	    0, 0, 0, "uaoeltpl", &pool_allocator_nointr);
d592 2
a593 1
	    "aobjpl", &pool_allocator_nointr);
@


1.23.2.2
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_aobj.c,v 1.23.2.1 2002/01/31 22:55:50 niklas Exp $	*/
/*	$NetBSD: uvm_aobj.c,v 1.49 2001/11/10 07:36:59 lukem Exp $	*/
a91 1

a107 1

d110 1
a110 1
	(MIN((AOBJ)->u_pages >> UAO_SWHASH_CLUSTER_SHIFT, \
d142 1
a142 1
 *   (struct uvm_aobj *) == (struct uvm_object *)
d170 13
a182 9
static struct uao_swhash_elt *uao_find_swhash_elt
    __P((struct uvm_aobj *, int, boolean_t));

static void	uao_free __P((struct uvm_aobj *));
static int	uao_get __P((struct uvm_object *, voff_t, struct vm_page **,
		    int *, int, vm_prot_t, int, int));
static boolean_t uao_put __P((struct uvm_object *, voff_t, voff_t, int));
static boolean_t uao_pagein __P((struct uvm_aobj *, int, int));
static boolean_t uao_pagein_page __P((struct uvm_aobj *, int));
d195 1
d197 4
a200 1
	uao_put,		/* flush */
d210 1
d272 3
a274 4

int
uao_find_swslot(uobj, pageidx)
	struct uvm_object *uobj;
a276 2
	struct uvm_aobj *aobj = (struct uvm_aobj *)uobj;
	struct uao_swhash_elt *elt;
d290 3
a292 1
		elt = uao_find_swhash_elt(aobj, pageidx, FALSE);
a301 1

a312 1

d326 1
a326 1
	 * if noswap flag is set, then we can't set a non-zero slot.
d330 1
d332 1
a332 1
			return(0);
d334 1
d336 1
a336 1
		panic("uao_set_swslot: NOSWAP object");
d351 1
a351 1
		elt = uao_find_swhash_elt(aobj, pageidx, slot != 0);
d364 1
a393 1

a397 1
	int swpgonlydelta = 0;
d400 1
a407 1

d423 8
a430 1
					swpgonlydelta++;
d450 5
a454 1
				swpgonlydelta++;
a462 1

a463 12

	/*
	 * adjust the counter of pages only in swap for all
	 * the swap slots we've freed.
	 */

	if (swpgonlydelta > 0) {
		simple_lock(&uvm.swap_data_lock);
		KASSERT(uvmexp.swpgonly >= swpgonlydelta);
		uvmexp.swpgonly -= swpgonlydelta;
		simple_unlock(&uvm.swap_data_lock);
	}
a477 1

d483 2
a484 2
	static struct uvm_aobj kernel_object_store;
	static int kobj_alloced = 0;
d491 3
a494 2
	if (flags & UAO_FLAG_KERNOBJ) {
		KASSERT(!kobj_alloced);
d497 2
a498 1
		aobj->u_flags = UAO_FLAG_NOSWAP;
a501 1
		KASSERT(kobj_alloced == UAO_FLAG_KERNOBJ);
d503 2
d506 1
a506 1
	} else {
d509 2
a510 2
		aobj->u_flags = 0;
		aobj->u_obj.uo_refs = 1;
a518 1

d540 1
a546 1

a554 1

d558 4
a571 1

d573 1
a573 1
uao_init(void)
d580 1
a587 1

d590 1
a600 1

a617 1

d631 1
a631 1
	uobj->uo_refs++;
d636 1
a642 1

d651 1
a659 1

d665 2
a666 1
	struct vm_page *pg;
a671 1

d678 2
a679 2
	uobj->uo_refs--;
	if (uobj->uo_refs) {
a687 1

d693 2
a694 4
 	 * free all the pages left in the aobj.  for each page,
	 * when the page is no longer busy (and thus after any disk i/o that
	 * it's involved in is complete), release any swap resources and
	 * free the page itself.
d696 3
a698 4

	uvm_lock_pageq();
	while ((pg = TAILQ_FIRST(&uobj->memq)) != NULL) {
		pmap_page_protect(pg, VM_PROT_NONE);
d700 2
a701 6
			pg->flags |= PG_WANTED;
			uvm_unlock_pageq();
			UVM_UNLOCK_AND_WAIT(pg, &uobj->vmobjlock, FALSE,
			    "uao_det", 0);
			simple_lock(&uobj->vmobjlock);
			uvm_lock_pageq();
d704 3
d708 1
d710 1
a711 1
	uvm_unlock_pageq();
d714 2
a715 1
 	 * finally, free the aobj itself.
d717 5
d723 3
d730 1
a730 1
 * uao_put: flush pages out of a uvm object
d754 16
d784 4
a787 2
int
uao_put(uobj, start, stop, flags)
d792 3
a794 3
	struct uvm_aobj *aobj = (struct uvm_aobj *)uobj;
	struct vm_page *pg, *nextpg;
	boolean_t by_list;
d796 5
a800 1
	UVMHIST_FUNC("uao_put"); UVMHIST_CALLED(maphist);
a801 1
	curoff = 0;
d815 1
a815 1
		    ((stop - start) >> PAGE_SHIFT) * UVM_PAGE_HASH_PENALTY);
d817 1
a825 1

d827 3
a829 2
		simple_unlock(&uobj->vmobjlock);
		return 0;
d833 2
a834 2
	 * now do it.  note: we must update nextpg in the body of loop or we
	 * will get stuck.  we need to use nextpg because we may free "pg"
d839 1
a839 1
		pg = TAILQ_FIRST(&uobj->memq);
d842 1
a842 1
		pg = uvm_pagelookup(uobj, curoff);
d845 2
a846 2
	nextpg = NULL;
	uvm_lock_pageq();
d849 2
a850 2
	for ( ; (by_list && pg != NULL) ||
	    (!by_list && curoff < stop) ; pg = nextpg) {
d852 4
a855 2
			nextpg = TAILQ_NEXT(pg, listq);
			if (pg->offset < start || pg->offset >= stop)
d860 4
a863 2
				nextpg = uvm_pagelookup(uobj, curoff);
			if (pg == NULL)
d866 1
a867 1

a873 1

d879 2
a880 1
			if (pg->loan_count != 0 || pg->wire_count != 0)
d884 3
a886 2
			pmap_clear_reference(pg);
			uvm_pagedeactivate(pg);
a889 1

a893 1

d898 2
a899 1
			if (pg->loan_count != 0 || pg->wire_count != 0)
d903 1
a903 2
			 * wait if the page is busy, then free the swap slot
			 * and the page.
d905 10
a915 11
			pmap_page_protect(pg, VM_PROT_NONE);
			while (pg->flags & PG_BUSY) {
				pg->flags |= PG_WANTED;
				uvm_unlock_pageq();
				UVM_UNLOCK_AND_WAIT(pg, &uobj->vmobjlock, 0,
				    "uao_put", 0);
				simple_lock(&uobj->vmobjlock);
				uvm_lock_pageq();
			}
			uao_dropswap(uobj, pg->offset >> PAGE_SHIFT);
			uvm_pagefree(pg);
d917 3
d922 1
d924 4
a927 2
	simple_unlock(&uobj->vmobjlock);
	return 0;
a948 1

d961 1
a961 1
	int lcv, gotpages, maxpages, swslot, error, pageidx;
a970 1

a977 1

d986 1
d992 1
d999 1
a999 2

			if (ptmp == NULL && uao_find_swslot(&aobj->u_obj,
d1005 1
a1005 1
					ptmp->flags &= ~(PG_FAKE);
d1007 1
a1007 1
					goto gotpage;
d1012 1
a1012 1
			 * to be useful must get a non-busy page
d1014 2
a1015 2

			if (ptmp == NULL || (ptmp->flags & PG_BUSY) != 0) {
a1026 1

a1029 1
gotpage:
d1032 2
a1033 1
		}
d1041 1
d1044 2
a1045 1
			return 0;
d1047 2
a1048 1
			return EBUSY;
d1104 1
a1111 1

a1117 1

d1122 1
a1122 1
			if ((ptmp->flags & PG_BUSY) != 0) {
d1130 1
a1130 1
				continue;
a1138 1

a1148 1

d1156 1
a1156 2

		swslot = uao_find_swslot(&aobj->u_obj, pageidx);
d1161 2
a1162 3

		if (swslot == 0) {

a1165 1

a1174 1

d1176 1
a1176 1
			error = uvm_swap_get(ptmp, swslot, PGO_SYNCIO);
d1182 2
a1183 2

			if (error != 0) {
d1185 1
a1185 1
				    error,0,0,0);
a1194 1

d1201 2
d1206 1
d1208 1
a1208 1
				return error;
d1223 2
a1224 1
		ptmp->flags &= ~PG_FAKE;
d1226 2
a1227 1
	}
d1235 1
a1235 1
	return 0;
d1239 61
d1318 1
a1324 1

a1329 1
	boolean_t rv;
d1337 1
d1341 1
d1344 2
a1345 1
		 * try to get the object lock, start all over if we fail.
a1348 1

a1357 1

a1362 1

a1368 1

a1378 1

a1421 1

a1432 1

a1450 1

a1457 1

a1473 1

d1492 1
a1493 1
	simple_lock(&aobj->u_obj.vmobjlock);
a1499 1

d1505 1
a1506 1
		return FALSE;
d1508 1
a1513 1

d1520 1
a1520 1
	 * deactivate the page (to make sure it's on a page queue).
d1522 1
a1522 1

d1526 1
@


1.23.2.3
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.23.2.2 2002/02/02 03:28:26 art Exp $	*/
d173 1
a173 1
   (struct uvm_aobj *, int, boolean_t);
d175 6
a180 6
static void	uao_free(struct uvm_aobj *);
static int	uao_get(struct uvm_object *, voff_t, struct vm_page **,
		    int *, int, vm_prot_t, int, int);
static boolean_t uao_put(struct uvm_object *, voff_t, voff_t, int);
static boolean_t uao_pagein(struct uvm_aobj *, int, int);
static boolean_t uao_pagein_page(struct uvm_aobj *, int);
@


1.23.2.4
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_aobj.c,v 1.23.2.3 2002/06/11 03:33:03 art Exp $	*/
/*	$NetBSD: uvm_aobj.c,v 1.51 2002/05/09 07:04:23 enami Exp $	*/
d765 1
a765 1
	struct vm_page *pg, *nextpg, curmp, endmp;
a800 12
	 * Initialize the marker pages.  See the comment in
	 * genfs_putpages() also.
	 */

	curmp.uobject = uobj;
	curmp.offset = (voff_t)-1;
	curmp.flags = PG_BUSY;
	endmp.uobject = uobj;
	endmp.offset = (voff_t)-1;
	endmp.flags = PG_BUSY;

	/*
d802 2
a803 2
	 * will get stuck.  we need to use nextpg if we'll traverse the list
	 * because we may free "pg" before doing the next loop.
d807 1
a807 3
		TAILQ_INSERT_TAIL(&uobj->memq, &endmp, listq);
		nextpg = TAILQ_FIRST(&uobj->memq);
		PHOLD(curproc);
d810 1
d813 1
d817 2
a818 1
	for (;;) {
a819 3
			pg = nextpg;
			if (pg == &endmp)
				break;
d824 3
a826 5
			if (curoff < stop) {
				pg = uvm_pagelookup(uobj, curoff);
				curoff += PAGE_SIZE;
			} else
				break;
d867 2
a868 2
			 * wait and try again if the page is busy.
			 * otherwise free the swap slot and the page.
d872 1
a872 4
			if (pg->flags & PG_BUSY) {
				if (by_list) {
					TAILQ_INSERT_BEFORE(pg, &curmp, listq);
				}
a878 7
				if (by_list) {
					nextpg = TAILQ_NEXT(&curmp, listq);
					TAILQ_REMOVE(&uobj->memq, &curmp,
					    listq);
				} else
					curoff -= PAGE_SIZE;
				continue;
a886 4
	if (by_list) {
		TAILQ_REMOVE(&uobj->memq, &endmp, listq);
		PRELE(curproc);
	}
@


1.23.2.5
log
@Some minor cleanups to reduce diff to netbsd.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.23.2.4 2002/11/04 18:02:32 art Exp $	*/
d583 1
a583 1
	    0, 0, 0, "uaoeltpl", NULL);
@


1.22
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_aobj.c,v 1.21 2001/11/27 05:27:12 art Exp $	*/
/*	$NetBSD: uvm_aobj.c,v 1.40 2001/03/10 22:46:47 chs Exp $	*/
d177 1
a177 1
					      vm_page_t *, int *, int,
d186 1
a186 1
 * 
d208 1
a208 1
static simple_lock_data_t uao_list_lock;
d236 2
a237 2
	swhash = UAO_SWHASH_HASH(aobj, pageidx); /* first hash to get bucket */
	page_tag = UAO_SWHASH_ELT_TAG(pageidx);	/* tag to search for */
d242 1
d244 3
a246 2
		if (elt->tag == page_tag)
			return(elt);
d248 1
a248 3

	/* fail now if we are not allowed to create a new entry in the bucket */
	if (!create)
d250 1
a250 1

d255 5
a259 1
	elt = pool_get(&uao_swhash_elt_pool, PR_WAITOK);
d264 1
a264 2

	return(elt);
d270 1
a270 1
 * => object must be locked by caller 
d299 1
a299 1
	/* 
d310 2
d319 1
d351 1
a351 2
		struct uao_swhash_elt *elt =
		    uao_find_swhash_elt(aobj, pageidx, slot ? TRUE : FALSE);
d353 1
a353 2
			KASSERT(slot == 0);
			return (0);
d368 2
a369 2
		} else {		/* freeing slot ... */
			if (oldslot)	/* to be safe */
d377 1
a377 1
	} else { 
d633 1
a633 1
	UVMHIST_LOG(maphist, "<- done (uobj=0x%x, ref = %d)", 
d666 1
a666 1
	struct vm_page *pg;
d698 2
a699 3
	for (pg = TAILQ_FIRST(&uobj->memq);
	     pg != NULL;
	     pg = TAILQ_NEXT(pg, listq)) {
d867 1
a867 1
		
d961 1
a961 1
	vm_page_t ptmp;
d1020 1
a1020 1
					done = FALSE;	
d1029 1
a1029 1
			ptmp->flags |= PG_BUSY;	
d1046 1
a1046 1
			return(0);	
d1106 1
a1106 1
					continue;	
d1115 1
a1115 1
				/* 
d1133 2
a1134 2
			
			/* 
d1154 1
a1154 1
 		 * we have a "fake/busy/clean" page that we just allocated.  
d1198 3
a1200 1
				uvm_swap_markbad(swslot, 1);
d1213 1
a1213 1
		/* 
d1216 1
a1216 1
		 * that page is still busy.   
d1241 1
a1241 1
 * 
d1302 1
a1302 1
 * 
d1322 1
a1322 1
 * 
d1423 1
a1423 1
					if (slot < startslot || 
@


1.21
log
@Merge in the unified buffer cache code as found in NetBSD 2001/03/10. The
code is written mostly by Chuck Silvers <chuq@@chuq.com>/<chs@@netbsd.org>.

Tested for the past few weeks by many developers, should be in a pretty stable
state, but will require optimizations and additional cleanups.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_aobj.c,v 1.20 2001/11/11 01:16:56 art Exp $	*/
/*	$NetBSD: uvm_aobj.c,v 1.39 2001/02/18 21:19:08 chs Exp $	*/
d938 1
a938 1
 * then we will need to return VM_PAGER_UNLOCK.
d1043 1
a1043 1
			return(VM_PAGER_OK);	
d1046 1
a1046 1
			return(VM_PAGER_UNLOCK);
d1180 1
a1180 1
			if (rv != VM_PAGER_OK)
d1231 1
a1231 1
	return(VM_PAGER_OK);
d1491 1
a1491 1
	case VM_PAGER_OK:
d1494 2
a1495 2
	case VM_PAGER_ERROR:
	case VM_PAGER_REFAULT:
d1498 1
a1498 1
		 * VM_PAGER_REFAULT can only mean that the anon was freed,
@


1.20
log
@Sync in more stuff from NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.19 2001/11/07 02:55:50 art Exp $	*/
a880 1
#ifdef UBC
a882 6
#else
			/* zap all mappings for the page. */
			pmap_page_protect(pp, VM_PROT_NONE);

			/* ...and deactivate the page. */
#endif
a1518 3
#ifndef UBC
	pmap_page_protect(pg, VM_PROT_NONE);
#endif
@


1.19
log
@Another sync of uvm to NetBSD. Just minor fiddling, no major changes.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_aobj.c,v 1.18 2001/11/06 13:36:52 art Exp $	*/
/*	$NetBSD: uvm_aobj.c,v 1.37 2000/11/25 06:27:59 chs Exp $	*/
d338 1
d344 1
d348 1
a348 4
#ifdef DIAGNOSTIC
			if (slot)
				panic("uao_set_swslot: didn't create elt");
#endif
d881 4
d889 1
a925 3
#ifdef DIAGNOSTIC
		panic("uao_flush: unreachable code");
#endif
d1264 1
a1264 4
#ifdef DIAGNOSTIC
	if ((pg->flags & PG_RELEASED) == 0)
		panic("uao_releasepg: page not released!");
#endif
d1291 1
a1291 4
#ifdef DIAGNOSTIC
	if (TAILQ_FIRST(&aobj->u_obj.memq))
		panic("uvn_releasepg: pages in object with npages == 0");
#endif
a1509 4
#ifdef DIAGNOSTIC
	default:
		panic("uao_pagein_page: uao_get -> %d\n", rv);
#endif
d1511 1
a1511 9

#ifdef DIAGNOSTIC
	/*
	 * this should never happen, since we have a reference on the aobj.
	 */
	if (pg->flags & PG_RELEASED) {
		panic("uao_pagein_page: found PG_RELEASED page?\n");
	}
#endif
d1526 1
d1528 1
@


1.18
log
@More sync to NetBSD.
 - Use malloc/free instead of MALLOC/FREE for variable sized allocations.
 - Move the memory inheritance code to sys/mman.h and rename from VM_* to MAP_*
 - various cleanups and simplifications.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_aobj.c,v 1.17 2001/11/06 01:35:04 art Exp $	*/
/*	$NetBSD: uvm_aobj.c,v 1.34 2000/08/02 20:23:23 thorpej Exp $	*/
d50 1
a183 2


a196 1
	NULL,			/* asyncget */
a199 1
	NULL,			/* aiodone */
d242 1
a242 1
	for (elt = swhash->lh_first; elt != NULL; elt = elt->list.le_next) {
a373 1

d416 4
a419 2
					if (slot) {
						uvm_swap_free(slot, 1);
d421 7
a427 8
						/*
						 * this page is no longer
						 * only in swap.
						 */
						simple_lock(&uvm.swap_data_lock);
						uvmexp.swpgonly--;
						simple_unlock(&uvm.swap_data_lock);
					}
d851 1
a851 1
			ppnext = pp->listq.tqe_next;
d971 1
a971 1
	
d1250 1
a1250 1
 * => if (nextpgp != NULL) => we return pageq.tqe_next here, and return
d1275 1
a1275 1
		*nextpgp = pg->pageq.tqe_next;	/* next page for daemon */
d1285 1
a1285 5

#ifdef DIAGNOSTIC
	if (aobj->u_obj.uo_refs)
		panic("uvm_km_releasepg: kill flag set on referenced object!");
#endif
a1488 1
	UVMHIST_FUNC("uao_pagein_page");  UVMHIST_CALLED(pdhist);
@


1.17
log
@Move the last content from vm/ to uvm/
The only thing left in vm/ are just dumb wrappers.
vm/vm.h includes uvm/uvm_extern.h
vm/pmap.h includes uvm/uvm_pmap.h
vm/vm_page.h includes uvm/uvm_page.h
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_aobj.c,v 1.16 2001/11/05 22:14:54 art Exp $	*/
/*	$NetBSD: uvm_aobj.c,v 1.33 2000/06/27 17:29:19 mrg Exp $	*/
d437 1
a437 1
		FREE(aobj->u_swhash, M_UVMAOBJ);
d457 1
a457 1
		FREE(aobj->u_swslots, M_UVMAOBJ);
d530 1
a530 1
			MALLOC(aobj->u_swslots, int *, pages * sizeof(int),
@


1.16
log
@Minor sync to NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_aobj.c,v 1.15 2001/09/11 20:05:26 miod Exp $	*/
/*	$NetBSD: uvm_aobj.c,v 1.32 2000/06/26 14:21:17 mrg Exp $	*/
a51 2

#include <vm/vm.h>
@


1.15
log
@Don't include <vm/vm_kern.h> if you don't need foo_map.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_aobj.c,v 1.14 2001/08/11 10:57:22 art Exp $	*/
/*	$NetBSD: uvm_aobj.c,v 1.31 2000/05/19 04:34:45 thorpej Exp $	*/
a53 1
#include <vm/vm_page.h>
@


1.14
log
@Various random fixes from NetBSD.
Including support for zeroing pages in the idle loop (not enabled yet).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.13 2001/08/06 14:03:04 art Exp $	*/
a54 1
#include <vm/vm_kern.h>
@


1.13
log
@Add a new type voff_t (right now it's typedefed as off_t) used for offsets
into objects.

Gives the possibilty to mmap beyond the size of vaddr_t.

From NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_aobj.c,v 1.12 2001/07/26 19:37:13 art Exp $	*/
/*	$NetBSD: uvm_aobj.c,v 1.28 2000/03/26 20:54:46 kleink Exp $	*/
a205 1
	uvm_shareprot,		/* shareprot */
d300 1
a300 1
			return(NULL);
d1007 2
a1008 2
 			 * if page is new, attempt to allocate the page, then
			 * zero-fill it.
d1013 1
a1013 1
				    NULL, 0);
a1018 1
					uvm_pagezero(ptmp);
@


1.12
log
@Add support for disabling swap devices (swapctl -d).
Improve error handling on I/O errors to swap.
From NetBSD
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_aobj.c,v 1.11 2001/07/18 10:47:05 art Exp $	*/
/*	$NetBSD: uvm_aobj.c,v 1.27 2000/01/11 06:57:49 chs Exp $	*/
d124 1
a124 1
	vaddr_t tag;				/* our 'tag' */
d175 3
a177 5
static int			 uao_find_swslot __P((struct uvm_aobj *, 
						      int));
static boolean_t		 uao_flush __P((struct uvm_object *, 
						vaddr_t, vaddr_t, 
						int));
d179 2
a180 2
static int			 uao_get __P((struct uvm_object *, vaddr_t,
					      vm_page_t *, int *, int, 
d182 1
a182 1
static boolean_t		 uao_releasepg __P((struct vm_page *, 
d242 1
a242 1
	int page_tag;
d796 1
a796 1
	vaddr_t start, stop;
d802 1
a802 1
	vaddr_t curoff;
d964 1
a964 1
	vaddr_t offset;
d971 1
a971 1
	vaddr_t current_offset;
@


1.11
log
@Get rid of the PMAP_NEW option by making it mandatory for all archs.
The archs that didn't have a proper PMAP_NEW now have a dummy implementation
with wrappers around the old functions.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_aobj.c,v 1.10 2001/06/23 19:24:33 smart Exp $	*/
/*	$NetBSD: uvm_aobj.c,v 1.26 1999/09/12 01:17:34 chs Exp $	*/
d51 1
d124 1
a124 1
	vaddr_t tag;			/* our 'tag' */
a172 1
static void			 uao_init __P((void));
d186 2
d198 1
a198 1
	uao_init,		/* init */
d407 2
d419 3
a421 2
			for (elt = aobj->u_swhash[i].lh_first; elt != NULL;
			    elt = next) {
d424 1
a424 2
				for (j = 0; j < UAO_SWHASH_CLUSTER_SIZE; j++)
				{
d440 1
a440 1
				next = elt->list.le_next;
d452 1
a452 2
		for (i = 0; i < aobj->u_pages; i++)
		{
d490 1
a490 1
	static struct uvm_aobj kernel_object_store;	/* home of kernel_object */
d496 2
a497 2
 	* malloc a new aobj unless we are asked for the kernel object
 	*/
a501 6
		/*
		 * XXXTHORPEJ: Need to call this now, so the pool gets
		 * initialized!
		 */
		uao_init();

d531 1
a531 1
			if (UAO_USES_SWHASH(aobj)) {
a560 2
 	 * XXXCHS: uao_init hasn't been called'd in the KERNOBJ case,
	 * do we really need the kernel object on this list anyway?
d579 1
a579 1
static void
d606 2
a607 1
 * => aobj must be unlocked (we will lock it)
d613 17
a638 1
	simple_lock(&uobj->vmobjlock);
d641 1
a641 2
	uobj, uobj->uo_refs,0,0);
	simple_unlock(&uobj->vmobjlock);
d644 1
d648 2
a649 1
 * => aobj must be unlocked, we will lock it
d655 17
d680 2
a681 1
	if (UVM_OBJ_IS_KERN_OBJECT(uobj))
d683 1
a683 2

	simple_lock(&uobj->vmobjlock);
d701 2
a702 1
 	 * free all the pages that aren't PG_BUSY, mark for release any that are.
a703 1

d705 3
a707 2
	for (pg = uobj->memq.tqh_first ; pg != NULL ; pg = pg->listq.tqe_next) {

d975 1
a975 1
	int lcv, gotpages, maxpages, swslot, rv;
d979 2
a980 1
	UVMHIST_LOG(pdhist, "aobj=%p offset=%d, flags=%d", aobj, offset, flags,0);
a984 1

a991 1

d1073 1
d1078 1
d1083 2
d1171 1
a1171 1
		swslot = uao_find_swslot(aobj, current_offset >> PAGE_SHIFT);
d1182 1
a1182 3
		}
		else
		{
a1201 1
					/* object lock still held */
d1203 11
d1219 1
d1267 2
a1268 1
static boolean_t uao_releasepg(pg, nextpgp)
d1278 1
a1278 1
	
d1289 1
a1289 1
		uvm_unlock_pageq();			/* keep locked for daemon */
d1309 1
a1309 1
	if (aobj->u_obj.memq.tqh_first)
d1321 1
d1339 226
@


1.10
log
@Sync with NetBSD 19990911 (just before PMAP_NEW was required)
  - thread_sleep_msg() -> uvm_sleep()
  - initialize reference count lock in uvm_anon_{init,add}()
  - add uao_flush()
  - replace boolean 'islocked' with 'lockflags'
  - in uvm_fault() change FALSE to TRUE to in 'wide' fault handling
  - get rid of uvm_km_get()
  - various bug fixes
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_aobj.c,v 1.9 2001/03/22 03:05:54 smart Exp $	*/
/*	$NetBSD: uvm_aobj.c,v 1.25 1999/08/21 02:19:05 thorpej Exp $	*/
d684 1
a684 1
		pmap_page_protect(PMAP_PGARG(pg), VM_PROT_NONE);
d862 1
a862 2
			pmap_page_protect(PMAP_PGARG(pp),
			    VM_PROT_NONE);
d891 1
a891 2
			pmap_page_protect(PMAP_PGARG(pp),
			    VM_PROT_NONE);
d1194 1
a1194 1
		pmap_clear_modify(PMAP_PGARG(ptmp));	/* ... and clean */
d1238 1
a1238 1
	pmap_page_protect(PMAP_PGARG(pg), VM_PROT_NONE);
@


1.9
log
@Sync style, typo, and comments a little closer to NetBSD.  art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_aobj.c,v 1.8 2001/03/08 15:21:36 smart Exp $	*/
/*	$NetBSD: uvm_aobj.c,v 1.21 1999/07/07 05:32:26 thorpej Exp $	*/
d708 52
a759 1
 * uao_flush: uh, yea, sure it's flushed.  really!
d761 3
d765 1
a765 1
uao_flush(uobj, start, end, flags)
d767 1
a767 1
	vaddr_t start, end;
d770 29
d801 9
a809 2
 	 * anonymous memory doesn't "flush"
 	 */
d811 3
a813 7
 	 * XXX
	 * Deal with:
	 *
	 *	PGO_DEACTIVATE	for sequential access, via uvm_fault(), and
	 *			for MADV_DONTNEED
	 *
	 *	PGO_FREE	for MADV_FREE and MSINVALIDATE
d815 98
a912 1
	return TRUE;
@


1.8
log
@Replace thread_wakeup() with wakeup().  It is defined in vm_extern.h as a
wrapper, so this removes a dependence on the old VM system.  From NetBSD.
art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_aobj.c,v 1.7 2001/01/29 02:07:43 niklas Exp $	*/
/*	$NetBSD: uvm_aobj.c,v 1.20 1999/05/25 00:09:00 thorpej Exp $	*/
d722 7
a728 3
 	 * deal with PGO_DEACTIVATE (for madvise(MADV_SEQUENTIAL))
 	 * and PGO_FREE (for msync(MSINVALIDATE))
 	 */
d927 2
a928 2
				UVM_UNLOCK_AND_WAIT(ptmp, &uobj->vmobjlock, 0,
				    "uao_get", 0);
@


1.7
log
@$OpenBSD$
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_aobj.c,v 1.20 1999/05/25 00:09:00 thorpej Exp $	*/
d986 1
a986 1
					thread_wakeup(ptmp);
@


1.6
log
@Convert bzero to memset(X, 0..) and bcopy to memcpy.
This is to match (make diffs smaller) the code in NetBSD.
new gcc inlines those functions, so this could also be a performance win.
@
text
@d1 1
@


1.5
log
@Bring in some new UVM code from NetBSD (not current).

 - Introduce a new type of map that are interrupt safe and never allow faults
   in them. mb_map and kmem_map are made intrsafe.
 - Add "access protection" to uvm_vslock (to be passed down to uvm_fault and
   later to pmap_enter).
 - madvise(2) now works.
 - various cleanups.
@
text
@d266 1
a266 1
	bzero(elt->slots, sizeof(elt->slots));
d543 1
a543 1
			bzero(aobj->u_swslots, pages * sizeof(int));
@


1.4
log
@sync with NetBSD from 1999.05.24 (there is a reason for this date)
 Mostly cleanups, but also a few improvements to pagedaemon for better
 handling of low memory and/or low swap conditions.
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_aobj.c,v 1.18 1999/03/26 17:34:15 chs Exp $	*/
d622 1
a622 1
	if (uobj->uo_refs == UVM_OBJ_KERN)
d649 1
a649 1
	if (uobj->uo_refs == UVM_OBJ_KERN)
@


1.4.4.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_aobj.c,v 1.20 1999/05/25 00:09:00 thorpej Exp $	*/
d622 1
a622 1
	if (UVM_OBJ_IS_KERN_OBJECT(uobj))
d649 1
a649 1
	if (UVM_OBJ_IS_KERN_OBJECT(uobj))
@


1.4.4.2
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_aobj.c,v 1.9 2001/03/22 03:05:54 smart Exp $	*/
/*	$NetBSD: uvm_aobj.c,v 1.21 1999/07/07 05:32:26 thorpej Exp $	*/
d266 1
a266 1
	memset(elt->slots, 0, sizeof(elt->slots));
d543 1
a543 1
			memset(aobj->u_swslots, 0, pages * sizeof(int));
d721 3
a723 7
	 * Deal with:
	 *
	 *	PGO_DEACTIVATE	for sequential access, via uvm_fault(), and
	 *			for MADV_DONTNEED
	 *
	 *	PGO_FREE	for MADV_FREE and MSINVALIDATE
	 */
d922 2
a923 2
				UVM_UNLOCK_AND_WAIT(ptmp, &uobj->vmobjlock,
				    FALSE, "uao_get", 0);
d985 1
a985 1
					wakeup(ptmp);
@


1.4.4.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_aobj.c,v 1.25 1999/08/21 02:19:05 thorpej Exp $	*/
d708 1
a708 52
 * uao_flush: "flush" pages out of a uvm object
 *
 * => object should be locked by caller.  we may _unlock_ the object
 *	if (and only if) we need to clean a page (PGO_CLEANIT).
 *	XXXJRT Currently, however, we don't.  In the case of cleaning
 *	XXXJRT a page, we simply just deactivate it.  Should probably
 *	XXXJRT handle this better, in the future (although "flushing"
 *	XXXJRT anonymous memory isn't terribly important).
 * => if PGO_CLEANIT is not set, then we will neither unlock the object
 *	or block.
 * => if PGO_ALLPAGE is set, then all pages in the object are valid targets
 *	for flushing.
 * => NOTE: we rely on the fact that the object's memq is a TAILQ and
 *	that new pages are inserted on the tail end of the list.  thus,
 *	we can make a complete pass through the object in one go by starting
 *	at the head and working towards the tail (new pages are put in
 *	front of us).
 * => NOTE: we are allowed to lock the page queues, so the caller
 *	must not be holding the lock on them [e.g. pagedaemon had
 *	better not call us with the queues locked]
 * => we return TRUE unless we encountered some sort of I/O error
 *	XXXJRT currently never happens, as we never directly initiate
 *	XXXJRT I/O
 *
 * comment on "cleaning" object and PG_BUSY pages:
 *	this routine is holding the lock on the object.  the only time
 *	that is can run into a PG_BUSY page that it does not own is if
 *	some other process has started I/O on the page (e.g. either
 *	a pagein or a pageout).  if the PG_BUSY page is being paged
 *	in, then it can not be dirty (!PG_CLEAN) because no one has
 *	had a change to modify it yet.  if the PG_BUSY page is being
 *	paged out then it means that someone else has already started
 *	cleaning the page for us (how nice!).  in this case, if we
 *	have syncio specified, then after we make our pass through the
 *	object we need to wait for the other PG_BUSY pages to clear
 *	off (i.e. we need to do an iosync).  also note that once a
 *	page is PG_BUSY is must stary in its object until it is un-busyed.
 *	XXXJRT We never actually do this, as we are "flushing" anonymous
 *	XXXJRT memory, which doesn't have persistent backing store.
 *
 * note on page traversal:
 *	we can traverse the pages in an object either by going down the
 *	linked list in "uobj->memq", or we can go over the address range
 *	by page doing hash table lookups for each address.  depending
 *	on how many pages are in the object it may be cheaper to do one
 *	or the other.  we set "by_list" to true if we are using memq.
 *	if the cost of a hash lookup was equal to the cost of the list
 *	traversal we could compare the number of pages in the start->stop
 *	range to the total number of pages in the object.  however, it
 *	seems that a hash table lookup is more expensive than the linked
 *	list traversal, so we multiply the number of pages in the
 *	start->stop range by a penalty which we define below.
a709 3

#define	UAO_HASH_PENALTY 4	/* XXX: a guess */

d711 1
a711 1
uao_flush(uobj, start, stop, flags)
d713 1
a713 1
	vaddr_t start, stop;
a715 29
	struct uvm_aobj *aobj = (struct uvm_aobj *) uobj;
	struct vm_page *pp, *ppnext;
	boolean_t retval, by_list;
	vaddr_t curoff;
	UVMHIST_FUNC("uao_flush"); UVMHIST_CALLED(maphist);

	curoff = 0;	/* XXX: shut up gcc */

	retval = TRUE;	/* default to success */

	if (flags & PGO_ALLPAGES) {
		start = 0;
		stop = aobj->u_pages << PAGE_SHIFT;
		by_list = TRUE;		/* always go by the list */
	} else {
		start = trunc_page(start);
		stop = round_page(stop);
		if (stop > (aobj->u_pages << PAGE_SHIFT)) {
			printf("uao_flush: strange, got an out of range "
			    "flush (fixed)\n");
			stop = aobj->u_pages << PAGE_SHIFT;
		}
		by_list = (uobj->uo_npages <=
		    ((stop - start) >> PAGE_SHIFT) * UAO_HASH_PENALTY);
	}

	UVMHIST_LOG(maphist,
	    " flush start=0x%lx, stop=0x%x, by_list=%d, flags=0x%x",
	    start, stop, by_list, flags);
d718 2
a719 9
	 * Don't need to do any work here if we're not freeing
	 * or deactivating pages.
	 */
	if ((flags & (PGO_DEACTIVATE|PGO_FREE)) == 0) {
		UVMHIST_LOG(maphist,
		    "<- done (no work to do)",0,0,0,0);
		return (retval);
	}

d721 7
a727 3
	 * now do it.  note: we must update ppnext in the body of loop or we
	 * will get stuck.  we need to use ppnext because we may free "pp"
	 * before doing the next loop.
d729 1
a729 98

	if (by_list) {
		pp = uobj->memq.tqh_first;
	} else {
		curoff = start;
		pp = uvm_pagelookup(uobj, curoff);
	}

	ppnext = NULL;	/* XXX: shut up gcc */
	uvm_lock_pageq();	/* page queues locked */

	/* locked: both page queues and uobj */
	for ( ; (by_list && pp != NULL) ||
	    (!by_list && curoff < stop) ; pp = ppnext) {
		if (by_list) {
			ppnext = pp->listq.tqe_next;

			/* range check */
			if (pp->offset < start || pp->offset >= stop)
				continue;
		} else {
			curoff += PAGE_SIZE;
			if (curoff < stop)
				ppnext = uvm_pagelookup(uobj, curoff);

			/* null check */
			if (pp == NULL)
				continue;
		}
		
		switch (flags & (PGO_CLEANIT|PGO_FREE|PGO_DEACTIVATE)) {
		/*
		 * XXX In these first 3 cases, we always just
		 * XXX deactivate the page.  We may want to
		 * XXX handle the different cases more specifically
		 * XXX in the future.
		 */
		case PGO_CLEANIT|PGO_FREE:
		case PGO_CLEANIT|PGO_DEACTIVATE:
		case PGO_DEACTIVATE:
 deactivate_it:
			/* skip the page if it's loaned or wired */
			if (pp->loan_count != 0 ||
			    pp->wire_count != 0)
				continue;

			/* zap all mappings for the page. */
			pmap_page_protect(PMAP_PGARG(pp),
			    VM_PROT_NONE);

			/* ...and deactivate the page. */
			uvm_pagedeactivate(pp);

			continue;

		case PGO_FREE:
			/*
			 * If there are multiple references to
			 * the object, just deactivate the page.
			 */
			if (uobj->uo_refs > 1)
				goto deactivate_it;

			/* XXX skip the page if it's loaned or wired */
			if (pp->loan_count != 0 ||
			    pp->wire_count != 0)
				continue;

			/*
			 * mark the page as released if its busy.
			 */
			if (pp->flags & PG_BUSY) {
				pp->flags |= PG_RELEASED;
				continue;
			}

			/* zap all mappings for the page. */
			pmap_page_protect(PMAP_PGARG(pp),
			    VM_PROT_NONE);

			uao_dropswap(uobj, pp->offset >> PAGE_SHIFT);
			uvm_pagefree(pp);

			continue;

		default:
			panic("uao_flush: weird flags");
		}
#ifdef DIAGNOSTIC
		panic("uao_flush: unreachable code");
#endif
	}

	uvm_unlock_pageq();

	UVMHIST_LOG(maphist,
	    "<- done, rv=%d",retval,0,0,0);
	return (retval);
@


1.4.4.4
log
@Sync the SMP branch to something just after 3.0
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_aobj.c,v 1.31 2000/05/19 04:34:45 thorpej Exp $	*/
a50 1
#include <sys/kernel.h>
d54 1
d123 1
a123 1
	voff_t tag;				/* our 'tag' */
d172 1
d175 5
a179 3
static int			 uao_find_swslot __P((struct uvm_aobj *, int));
static boolean_t		 uao_flush __P((struct uvm_object *,
						voff_t, voff_t, int));
d181 2
a182 2
static int			 uao_get __P((struct uvm_object *, voff_t,
					      vm_page_t *, int *, int,
d184 1
a184 1
static boolean_t		 uao_releasepg __P((struct vm_page *,
a185 2
static boolean_t		 uao_pagein __P((struct uvm_aobj *, int, int));
static boolean_t		 uao_pagein_page __P((struct uvm_aobj *, int));
d196 1
a196 1
	NULL,			/* init */
d206 1
d242 1
a242 1
	voff_t page_tag;
d301 1
a301 1
			return(0);
a404 2
	simple_unlock(&aobj->u_obj.vmobjlock);

d415 2
a416 3
			for (elt = LIST_FIRST(&aobj->u_swhash[i]);
			     elt != NULL;
			     elt = next) {
d419 2
a420 1
				for (j = 0; j < UAO_SWHASH_CLUSTER_SIZE; j++) {
d436 1
a436 1
				next = LIST_NEXT(elt, list);
d448 2
a449 1
		for (i = 0; i < aobj->u_pages; i++) {
d487 1
a487 1
	static struct uvm_aobj kernel_object_store; /* home of kernel_object */
d493 2
a494 2
	 * malloc a new aobj unless we are asked for the kernel object
	 */
d499 6
d534 1
a534 1
		if (UAO_USES_SWHASH(aobj)) {
d564 2
d584 1
a584 1
void
d611 1
a611 2
 * => aobj must be unlocked
 * => just lock it and call the locked version
a616 17
	simple_lock(&uobj->vmobjlock);
	uao_reference_locked(uobj);
	simple_unlock(&uobj->vmobjlock);
}

/*
 * uao_reference_locked: add a ref to an aobj that is already locked
 *
 * => aobj must be locked
 * this needs to be separate from the normal routine
 * since sometimes we need to add a reference to an aobj when
 * it's already locked.
 */
void
uao_reference_locked(uobj)
	struct uvm_object *uobj;
{
d626 1
d629 2
a630 1
		    uobj, uobj->uo_refs,0,0);
a632 1

d636 1
a636 2
 * => aobj must be unlocked
 * => just lock it and call the locked version
a641 17
	simple_lock(&uobj->vmobjlock);
	uao_detach_locked(uobj);
}


/*
 * uao_detach_locked: drop a reference to an aobj
 *
 * => aobj must be locked, and is unlocked (or freed) upon return.
 * this needs to be separate from the normal routine
 * since sometimes we need to detach from an aobj when
 * it's already locked.
 */
void
uao_detach_locked(uobj)
	struct uvm_object *uobj;
{
d650 1
a650 2
	if (UVM_OBJ_IS_KERN_OBJECT(uobj)) {
		simple_unlock(&uobj->vmobjlock);
d652 2
a653 1
	}
d671 1
a671 2
 	 * free all the pages that aren't PG_BUSY,
	 * mark for release any that are.
d673 1
d675 2
a676 3
	for (pg = TAILQ_FIRST(&uobj->memq);
	     pg != NULL;
	     pg = TAILQ_NEXT(pg, listq)) {
d684 1
a684 1
		pmap_page_protect(pg, VM_PROT_NONE);
d767 1
a767 1
	voff_t start, stop;
d773 1
a773 1
	voff_t curoff;
d862 2
a863 1
			pmap_page_protect(pp, VM_PROT_NONE);
d892 2
a893 1
			pmap_page_protect(pp, VM_PROT_NONE);
d937 1
a937 1
	voff_t offset;
d944 1
a944 1
	voff_t current_offset;
d946 1
a946 1
	int lcv, gotpages, maxpages, swslot, rv, pageidx;
d950 1
a950 2
	UVMHIST_LOG(pdhist, "aobj=%p offset=%d, flags=%d",
		    aobj, offset, flags,0);
d955 1
d963 1
d982 2
a983 2
 			 * if page is new, attempt to allocate the page,
			 * zero-fill'd.
d988 1
a988 1
				    NULL, UVM_PGA_ZERO);
d994 1
a1044 1

a1048 1

a1052 2
		pageidx = current_offset >> PAGE_SHIFT;

d1139 1
a1139 1
		swslot = uao_find_swslot(aobj, pageidx);
d1150 3
a1152 1
		} else {
d1172 1
a1173 11

				/*
				 * remove the swap slot from the aobj
				 * and mark the aobj as having no real slot.
				 * don't free the swap slot, thus preventing
				 * it from being used again.
				 */
				swslot = uao_set_swslot(&aobj->u_obj, pageidx,
							SWSLOT_BAD);
				uvm_swap_markbad(swslot, 1);

a1178 1

d1196 1
a1196 1
		pmap_clear_modify(ptmp);		/* ... and clean */
d1226 1
a1226 2
static boolean_t
uao_releasepg(pg, nextpgp)
d1236 1
a1236 1

d1240 1
a1240 1
	pmap_page_protect(pg, VM_PROT_NONE);
d1247 1
a1247 1
		uvm_unlock_pageq();		/* keep locked for daemon */
d1267 1
a1267 1
	if (TAILQ_FIRST(&aobj->u_obj.memq))
a1278 1

a1295 226
}


/*
 * page in every page in every aobj that is paged-out to a range of swslots.
 * 
 * => nothing should be locked.
 * => returns TRUE if pagein was aborted due to lack of memory.
 */
boolean_t
uao_swap_off(startslot, endslot)
	int startslot, endslot;
{
	struct uvm_aobj *aobj, *nextaobj;

	/*
	 * walk the list of all aobjs.
	 */

restart:
	simple_lock(&uao_list_lock);

	for (aobj = LIST_FIRST(&uao_list);
	     aobj != NULL;
	     aobj = nextaobj) {
		boolean_t rv;

		/*
		 * try to get the object lock,
		 * start all over if we fail.
		 * most of the time we'll get the aobj lock,
		 * so this should be a rare case.
		 */
		if (!simple_lock_try(&aobj->u_obj.vmobjlock)) {
			simple_unlock(&uao_list_lock);
			goto restart;
		}

		/*
		 * add a ref to the aobj so it doesn't disappear
		 * while we're working.
		 */
		uao_reference_locked(&aobj->u_obj);

		/*
		 * now it's safe to unlock the uao list.
		 */
		simple_unlock(&uao_list_lock);

		/*
		 * page in any pages in the swslot range.
		 * if there's an error, abort and return the error.
		 */
		rv = uao_pagein(aobj, startslot, endslot);
		if (rv) {
			uao_detach_locked(&aobj->u_obj);
			return rv;
		}

		/*
		 * we're done with this aobj.
		 * relock the list and drop our ref on the aobj.
		 */
		simple_lock(&uao_list_lock);
		nextaobj = LIST_NEXT(aobj, u_list);
		uao_detach_locked(&aobj->u_obj);
	}

	/*
	 * done with traversal, unlock the list
	 */
	simple_unlock(&uao_list_lock);
	return FALSE;
}


/*
 * page in any pages from aobj in the given range.
 *
 * => aobj must be locked and is returned locked.
 * => returns TRUE if pagein was aborted due to lack of memory.
 */
static boolean_t
uao_pagein(aobj, startslot, endslot)
	struct uvm_aobj *aobj;
	int startslot, endslot;
{
	boolean_t rv;

	if (UAO_USES_SWHASH(aobj)) {
		struct uao_swhash_elt *elt;
		int bucket;

restart:
		for (bucket = aobj->u_swhashmask; bucket >= 0; bucket--) {
			for (elt = LIST_FIRST(&aobj->u_swhash[bucket]);
			     elt != NULL;
			     elt = LIST_NEXT(elt, list)) {
				int i;

				for (i = 0; i < UAO_SWHASH_CLUSTER_SIZE; i++) {
					int slot = elt->slots[i];

					/*
					 * if the slot isn't in range, skip it.
					 */
					if (slot < startslot || 
					    slot >= endslot) {
						continue;
					}

					/*
					 * process the page,
					 * the start over on this object
					 * since the swhash elt
					 * may have been freed.
					 */
					rv = uao_pagein_page(aobj,
					  UAO_SWHASH_ELT_PAGEIDX_BASE(elt) + i);
					if (rv) {
						return rv;
					}
					goto restart;
				}
			}
		}
	} else {
		int i;

		for (i = 0; i < aobj->u_pages; i++) {
			int slot = aobj->u_swslots[i];

			/*
			 * if the slot isn't in range, skip it
			 */
			if (slot < startslot || slot >= endslot) {
				continue;
			}

			/*
			 * process the page.
			 */
			rv = uao_pagein_page(aobj, i);
			if (rv) {
				return rv;
			}
		}
	}

	return FALSE;
}

/*
 * page in a page from an aobj.  used for swap_off.
 * returns TRUE if pagein was aborted due to lack of memory.
 *
 * => aobj must be locked and is returned locked.
 */
static boolean_t
uao_pagein_page(aobj, pageidx)
	struct uvm_aobj *aobj;
	int pageidx;
{
	struct vm_page *pg;
	int rv, slot, npages;
	UVMHIST_FUNC("uao_pagein_page");  UVMHIST_CALLED(pdhist);

	pg = NULL;
	npages = 1;
	/* locked: aobj */
	rv = uao_get(&aobj->u_obj, pageidx << PAGE_SHIFT,
		     &pg, &npages, 0, VM_PROT_READ|VM_PROT_WRITE, 0, 0);
	/* unlocked: aobj */

	/*
	 * relock and finish up.
	 */
	simple_lock(&aobj->u_obj.vmobjlock);

	switch (rv) {
	case VM_PAGER_OK:
		break;

	case VM_PAGER_ERROR:
	case VM_PAGER_REFAULT:
		/*
		 * nothing more to do on errors.
		 * VM_PAGER_REFAULT can only mean that the anon was freed,
		 * so again there's nothing to do.
		 */
		return FALSE;

#ifdef DIAGNOSTIC
	default:
		panic("uao_pagein_page: uao_get -> %d\n", rv);
#endif
	}

#ifdef DIAGNOSTIC
	/*
	 * this should never happen, since we have a reference on the aobj.
	 */
	if (pg->flags & PG_RELEASED) {
		panic("uao_pagein_page: found PG_RELEASED page?\n");
	}
#endif

	/*
	 * ok, we've got the page now.
	 * mark it as dirty, clear its swslot and un-busy it.
	 */
	slot = uao_set_swslot(&aobj->u_obj, pageidx, 0);
	uvm_swap_free(slot, 1);
	pg->flags &= ~(PG_BUSY|PG_CLEAN|PG_FAKE);
	UVM_PAGE_OWN(pg, NULL);

	/*
	 * deactivate the page (to put it on a page queue).
	 */
	pmap_clear_reference(pg);
	pmap_page_protect(pg, VM_PROT_NONE);
	uvm_lock_pageq();
	uvm_pagedeactivate(pg);
	uvm_unlock_pageq();

	return FALSE;
@


1.4.4.5
log
@merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_aobj.c,v 1.39 2001/02/18 21:19:08 chs Exp $	*/
a49 1
#include <sys/kernel.h>
d53 3
d186 2
d201 1
d205 1
d248 1
a248 1
	LIST_FOREACH(elt, swhash, list) {
a343 1

a348 1

d352 4
a355 1
			KASSERT(slot == 0);
d380 1
d423 10
a432 2
					if (slot == 0) {
						continue;
a433 9
					uvm_swap_free(slot, 1);

					/*
					 * this page is no longer
					 * only in swap.
					 */
					simple_lock(&uvm.swap_data_lock);
					uvmexp.swpgonly--;
					simple_unlock(&uvm.swap_data_lock);
d440 1
a440 1
		free(aobj->u_swhash, M_UVMAOBJ);
d460 1
a460 1
		free(aobj->u_swslots, M_UVMAOBJ);
d533 1
a533 1
			aobj->u_swslots = malloc(pages * sizeof(int),
d857 1
a857 1
			ppnext = TAILQ_NEXT(pp, listq);
a887 4
#ifdef UBC
			/* ...and deactivate the page. */
			pmap_clear_reference(pp);
#else
a891 1
#endif
d928 3
d977 1
a977 1

d1256 1
a1256 1
 * => if (nextpgp != NULL) => we return the next page on the queue, and return
d1269 4
a1272 1
	KASSERT(pg->flags & PG_RELEASED);
d1281 1
a1281 1
		*nextpgp = TAILQ_NEXT(pg, pageq); /* next page for daemon */
d1291 5
a1295 1
	KASSERT(aobj->u_obj.uo_refs == 0);
d1303 4
a1306 1
	KASSERT(TAILQ_EMPTY(&aobj->u_obj.memq));
d1499 1
d1526 4
d1531 9
a1539 1
	KASSERT((pg->flags & PG_RELEASED) == 0);
a1553 1
#ifndef UBC
a1554 1
#endif
@


1.4.4.6
log
@Merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_aobj.c,v 1.45 2001/06/23 20:52:03 chs Exp $	*/
d177 1
a177 1
					      struct vm_page **, int *, int,
d186 1
a186 1
 *
d208 1
a208 1
static struct simplelock uao_list_lock;
d236 2
a237 2
	swhash = UAO_SWHASH_HASH(aobj, pageidx);
	page_tag = UAO_SWHASH_ELT_TAG(pageidx);
a241 1

d243 2
a244 3
		if (elt->tag == page_tag) {
			return elt;
		}
d246 3
a248 1
	if (!create) {
d250 1
a250 1
	}
d255 1
a255 5

	elt = pool_get(&uao_swhash_elt_pool, PR_NOWAIT);
	if (elt == NULL) {
		return NULL;
	}
d260 2
a261 1
	return elt;
d267 1
a267 1
 * => object must be locked by caller
d296 1
a296 1
	/*
a306 2
 * => we return the old slot number, or -1 if we failed to allocate
 *    memory to record the new slot number
a313 1
	struct uao_swhash_elt *elt;
d345 2
a346 1
		elt = uao_find_swhash_elt(aobj, pageidx, slot ? TRUE : FALSE);
d348 2
a349 1
			return slot ? -1 : 0;
d364 2
a365 2
		} else {
			if (oldslot)
d373 1
a373 1
	} else {
d629 1
a629 1
	UVMHIST_LOG(maphist, "<- done (uobj=0x%x, ref = %d)",
d662 1
a662 1
	struct vm_page *pg, *nextpg;
d694 3
a696 2
	for (pg = TAILQ_FIRST(&uobj->memq); pg != NULL; pg = nextpg) {
		nextpg = TAILQ_NEXT(pg, listq);
d864 1
a864 1

d881 1
d884 6
d945 1
a945 1
 * then we will need to return EBUSY.
d965 1
a965 1
	struct vm_page *ptmp;
d1024 1
a1024 1
					done = FALSE;
d1033 1
a1033 1
			ptmp->flags |= PG_BUSY;
d1050 1
a1050 1
			return(0);
d1053 1
a1053 1
			return(EBUSY);
d1110 1
a1110 1
					continue;
d1119 1
a1119 1
				/*
d1137 2
a1138 2

			/*
d1158 1
a1158 1
 		 * we have a "fake/busy/clean" page that we just allocated.
d1187 1
a1187 1
			if (rv != 0)
d1202 1
a1202 3
				if (swslot != -1) {
					uvm_swap_markbad(swslot, 1);
				}
d1215 1
a1215 1
		/*
d1218 1
a1218 1
		 * that page is still busy.
d1238 1
a1238 1
	return(0);
d1243 1
a1243 1
 *
d1304 1
a1304 1
 *
d1324 1
a1324 1
 *
d1425 1
a1425 1
					if (slot < startslot ||
d1498 1
a1498 1
	case 0:
d1501 2
a1502 2
	case EIO:
	case ERESTART:
d1505 1
a1505 1
		 * ERESTART can only mean that the anon was freed,
d1526 3
@


1.4.4.7
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_aobj.c,v 1.39 2001/02/18 21:19:08 chs Exp $	*/
d177 1
a177 1
					      vm_page_t *, int *, int,
d186 1
a186 1
 * 
d208 1
a208 1
static simple_lock_data_t uao_list_lock;
d236 2
a237 2
	swhash = UAO_SWHASH_HASH(aobj, pageidx); /* first hash to get bucket */
	page_tag = UAO_SWHASH_ELT_TAG(pageidx);	/* tag to search for */
d242 1
d244 3
a246 2
		if (elt->tag == page_tag)
			return(elt);
d248 1
a248 3

	/* fail now if we are not allowed to create a new entry in the bucket */
	if (!create)
d250 1
a250 1

d255 5
a259 1
	elt = pool_get(&uao_swhash_elt_pool, PR_WAITOK);
d264 1
a264 2

	return(elt);
d270 1
a270 1
 * => object must be locked by caller 
d299 1
a299 1
	/* 
d310 2
d319 1
d351 1
a351 2
		struct uao_swhash_elt *elt =
		    uao_find_swhash_elt(aobj, pageidx, slot ? TRUE : FALSE);
d353 1
a353 2
			KASSERT(slot == 0);
			return (0);
d368 2
a369 2
		} else {		/* freeing slot ... */
			if (oldslot)	/* to be safe */
d377 1
a377 1
	} else { 
d589 1
a589 1
	    0, 0, 0, "uaoeltpl", &pool_allocator_nointr);
d592 2
a593 1
	    "aobjpl", &pool_allocator_nointr);
d633 1
a633 1
	UVMHIST_LOG(maphist, "<- done (uobj=0x%x, ref = %d)", 
d666 1
a666 1
	struct vm_page *pg;
d698 2
a699 3
	for (pg = TAILQ_FIRST(&uobj->memq);
	     pg != NULL;
	     pg = TAILQ_NEXT(pg, listq)) {
d867 1
a867 1
		
a883 1
#ifdef UBC
a885 6
#else
			/* zap all mappings for the page. */
			pmap_page_protect(pp, VM_PROT_NONE);

			/* ...and deactivate the page. */
#endif
d941 1
a941 1
 * then we will need to return VM_PAGER_UNLOCK.
d961 1
a961 1
	vm_page_t ptmp;
d1020 1
a1020 1
					done = FALSE;	
d1029 1
a1029 1
			ptmp->flags |= PG_BUSY;	
d1046 1
a1046 1
			return(VM_PAGER_OK);	
d1049 1
a1049 1
			return(VM_PAGER_UNLOCK);
d1106 1
a1106 1
					continue;	
d1115 1
a1115 1
				/* 
d1133 2
a1134 2
			
			/* 
d1154 1
a1154 1
 		 * we have a "fake/busy/clean" page that we just allocated.  
d1183 1
a1183 1
			if (rv != VM_PAGER_OK)
d1198 3
a1200 1
				uvm_swap_markbad(swslot, 1);
d1213 1
a1213 1
		/* 
d1216 1
a1216 1
		 * that page is still busy.   
d1236 1
a1236 1
	return(VM_PAGER_OK);
d1241 1
a1241 1
 * 
d1302 1
a1302 1
 * 
d1322 1
a1322 1
 * 
d1423 1
a1423 1
					if (slot < startslot || 
d1496 1
a1496 1
	case VM_PAGER_OK:
d1499 2
a1500 2
	case VM_PAGER_ERROR:
	case VM_PAGER_REFAULT:
d1503 1
a1503 1
		 * VM_PAGER_REFAULT can only mean that the anon was freed,
a1523 3
#ifndef UBC
	pmap_page_protect(pg, VM_PROT_NONE);
#endif
@


1.4.4.8
log
@Merge in -current from roughly a week ago
@
text
@d170 7
a176 7
static struct uao_swhash_elt	*uao_find_swhash_elt(struct uvm_aobj *,
							  int, boolean_t);
static int			 uao_find_swslot(struct uvm_aobj *, int);
static boolean_t		 uao_flush(struct uvm_object *,
						voff_t, voff_t, int);
static void			 uao_free(struct uvm_aobj *);
static int			 uao_get(struct uvm_object *, voff_t,
d178 5
a182 5
					      vm_prot_t, int, int);
static boolean_t		 uao_releasepg(struct vm_page *,
						    struct vm_page **);
static boolean_t		 uao_pagein(struct uvm_aobj *, int, int);
static boolean_t		 uao_pagein_page(struct uvm_aobj *, int);
@


1.3
log
@zap the newhashinit hack.
Add an extra flag to hashinit telling if it should wait in malloc.
update all calls to hashinit.
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_aobj.c,v 1.2 1999/02/26 05:32:06 art Exp $	*/
/*	$NetBSD: uvm_aobj.c,v 1.15 1998/10/18 23:49:59 chs Exp $	*/
a3 4
 * XXXCDC: "ROUGH DRAFT" QUALITY UVM PRE-RELEASE FILE!   
 *	   >>>USE AT YOUR OWN RISK, WORK IS NOT FINISHED<<<
 */
/*
a195 1
	NULL,			/* attach */
d422 1
a422 1
					if (slot)
d424 9
d451 1
a451 1
			if (slot)
d453 6
a674 1
		int swslot;
a681 1

d684 1
a684 7

		swslot = uao_set_swslot(&aobj->u_obj,
					pg->offset >> PAGE_SHIFT, 0);
		if (swslot) {
			uvm_swap_free(swslot, 1);
		}

d800 1
a800 1
				    NULL);
d890 1
a890 1
				    NULL);	/* alloc */
a1042 1
	int slot;
d1053 1
a1053 3
	slot = uao_set_swslot(&aobj->u_obj, pg->offset >> PAGE_SHIFT, 0);
	if (slot)
		uvm_swap_free(slot, 1);
d1089 19
@


1.2
log
@add OpenBSD tags
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d525 1
a525 1
			aobj->u_swhash = newhashinit(UAO_SWHASH_BUCKETS(aobj),
@


1.1
log
@Import of uvm from NetBSD. Some local changes, some code disabled
@
text
@d1 1
@

