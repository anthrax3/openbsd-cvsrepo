head	1.14;
access;
symbols
	OPENBSD_6_1:1.14.0.4
	OPENBSD_6_1_BASE:1.14
	OPENBSD_6_0:1.12.0.8
	OPENBSD_6_0_BASE:1.12
	OPENBSD_5_9:1.12.0.4
	OPENBSD_5_9_BASE:1.12
	OPENBSD_5_8:1.12.0.6
	OPENBSD_5_8_BASE:1.12
	OPENBSD_5_7:1.12.0.2
	OPENBSD_5_7_BASE:1.12
	OPENBSD_5_6:1.11.0.16
	OPENBSD_5_6_BASE:1.11
	OPENBSD_5_5:1.11.0.14
	OPENBSD_5_5_BASE:1.11
	OPENBSD_5_4:1.11.0.10
	OPENBSD_5_4_BASE:1.11
	OPENBSD_5_3:1.11.0.8
	OPENBSD_5_3_BASE:1.11
	OPENBSD_5_2:1.11.0.6
	OPENBSD_5_2_BASE:1.11
	OPENBSD_5_1_BASE:1.11
	OPENBSD_5_1:1.11.0.4
	OPENBSD_5_0:1.11.0.2
	OPENBSD_5_0_BASE:1.11
	OPENBSD_4_9:1.5.0.4
	OPENBSD_4_9_BASE:1.5
	OPENBSD_4_8:1.5.0.2
	OPENBSD_4_8_BASE:1.5;
locks; strict;
comment	@ * @;


1.14
date	2016.09.16.02.47.09;	author dlg;	state Exp;
branches;
next	1.13;
commitid	er2YnxHqqQ4UcS7k;

1.13
date	2016.09.16.02.35.42;	author dlg;	state Exp;
branches;
next	1.12;
commitid	Fei4687v68qad1tP;

1.12
date	2015.02.05.23.51.06;	author mpi;	state Exp;
branches;
next	1.11;
commitid	ofjqNcubSq8nv7rW;

1.11
date	2011.07.08.18.25.56;	author ariane;	state Exp;
branches;
next	1.10;

1.10
date	2011.07.08.18.20.10;	author ariane;	state Exp;
branches;
next	1.9;

1.9
date	2011.07.08.18.15.44;	author ariane;	state Exp;
branches;
next	1.8;

1.8
date	2011.07.06.19.50.38;	author beck;	state Exp;
branches;
next	1.7;

1.7
date	2011.04.03.22.07.37;	author ariane;	state Exp;
branches;
next	1.6;

1.6
date	2011.04.03.14.16.42;	author guenther;	state Exp;
branches;
next	1.5;

1.5
date	2010.04.22.19.02.55;	author oga;	state Exp;
branches;
next	1.4;

1.4
date	2009.06.16.16.42.41;	author ariane;	state dead;
branches;
next	1.3;

1.3
date	2009.06.14.02.20.23;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	2009.06.10.20.36.40;	author ariane;	state Exp;
branches;
next	1.1;

1.1
date	2009.06.01.17.42.33;	author ariane;	state Exp;
branches;
next	;


desc
@@


1.14
log
@move uvm_pmemrange_addr from RB macros to RBT functions
@
text
@/*	$OpenBSD: uvm_pmemrange.h,v 1.13 2016/09/16 02:35:42 dlg Exp $	*/

/*
 * Copyright (c) 2009 Ariane van der Steldt <ariane@@stack.nl>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

/*
 * uvm_pmemrange.h: describe and manage free physical memory.
 */

#ifndef _UVM_UVM_PMEMRANGE_H_
#define _UVM_UVM_PMEMRANGE_H_

RBT_HEAD(uvm_pmr_addr, vm_page);
RBT_HEAD(uvm_pmr_size, vm_page);

/*
 * Page types available:
 * - DIRTY: this page may contain random data.
 * - ZERO: this page has been zeroed.
 */
#define UVM_PMR_MEMTYPE_DIRTY	0
#define UVM_PMR_MEMTYPE_ZERO	1
#define UVM_PMR_MEMTYPE_MAX	2

/*
 * An address range of memory.
 */
struct uvm_pmemrange {
	struct	uvm_pmr_addr addr;	/* Free page chunks, sorted by addr. */
	struct	uvm_pmr_size size[UVM_PMR_MEMTYPE_MAX];
					/* Free page chunks, sorted by size. */
	TAILQ_HEAD(, vm_page) single[UVM_PMR_MEMTYPE_MAX];
					/* single page regions (uses pageq) */

	paddr_t	low;			/* Start of address range (pgno). */
	paddr_t	high;			/* End +1 (pgno). */
	int	use;			/* Use counter. */
	psize_t	nsegs;			/* Current range count. */

	TAILQ_ENTRY(uvm_pmemrange) pmr_use;
					/* pmr, sorted by use */
	RBT_ENTRY(uvm_pmemrange) pmr_addr;
					/* pmr, sorted by address */
};

/*
 * Description of failing memory allocation.
 *
 * Two ways new pages can become available:
 * [1] page daemon drops them (we notice because they are freed)
 * [2] a process calls free
 *
 * The buffer cache and page daemon can decide that they don't have the
 * ability to make pages available in the requested range. In that case,
 * the FAIL bit will be set.
 * XXX There's a possibility that a page is no longer on the queues but
 * XXX has not yet been freed, or that a page was busy.
 * XXX Also, wired pages are not considered for paging, so they could
 * XXX cause a failure that may be recoverable.
 */
struct uvm_pmalloc {
	TAILQ_ENTRY(uvm_pmalloc) pmq;

	/*
	 * Allocation request parameters.
	 */
	struct uvm_constraint_range pm_constraint;
	psize_t	pm_size;

	/*
	 * State flags.
	 */
	int	pm_flags;
};

/*
 * uvm_pmalloc flags.
 */
#define UVM_PMA_LINKED	0x01	/* uvm_pmalloc is on list */
#define UVM_PMA_BUSY	0x02	/* entry is busy with fpageq unlocked */
#define UVM_PMA_FAIL	0x10	/* page daemon cannot free pages */
#define UVM_PMA_FREED	0x20	/* at least one page in the range was freed */

RBT_HEAD(uvm_pmemrange_addr, uvm_pmemrange);
TAILQ_HEAD(uvm_pmemrange_use, uvm_pmemrange);

/*
 * pmr control structure. Contained in uvm.pmr_control.
 */
struct uvm_pmr_control {
	struct	uvm_pmemrange_addr addr;
	struct	uvm_pmemrange_use use;

	/* Only changed while fpageq is locked. */
	TAILQ_HEAD(, uvm_pmalloc) allocs;
};

void	uvm_pmr_freepages(struct vm_page *, psize_t);
void	uvm_pmr_freepageq(struct pglist *);
int	uvm_pmr_getpages(psize_t, paddr_t, paddr_t, paddr_t, paddr_t,
	    int, int, struct pglist *);
void	uvm_pmr_init(void);
int	uvm_wait_pla(paddr_t, paddr_t, paddr_t, int);
void	uvm_wakeup_pla(paddr_t, psize_t);

#if defined(DDB) || defined(DEBUG)
int	uvm_pmr_isfree(struct vm_page *pg);
#endif

/*
 * Internal tree logic.
 */

int	uvm_pmr_addr_cmp(const struct vm_page *, const struct vm_page *);
int	uvm_pmr_size_cmp(const struct vm_page *, const struct vm_page *);

RBT_PROTOTYPE(uvm_pmr_addr, vm_page, objt, uvm_pmr_addr_cmp);
RBT_PROTOTYPE(uvm_pmr_size, vm_page, objt, uvm_pmr_size_cmp);
RBT_PROTOTYPE(uvm_pmemrange_addr, uvm_pmemrange, pmr_addr,
    uvm_pmemrange_addr_cmp);

struct vm_page		*uvm_pmr_insert_addr(struct uvm_pmemrange *,
			    struct vm_page *, int);
void			 uvm_pmr_insert_size(struct uvm_pmemrange *,
			    struct vm_page *);
struct vm_page		*uvm_pmr_insert(struct uvm_pmemrange *,
			    struct vm_page *, int);
void			 uvm_pmr_remove_addr(struct uvm_pmemrange *,
			    struct vm_page *);
void			 uvm_pmr_remove_size(struct uvm_pmemrange *,
			    struct vm_page *);
void			 uvm_pmr_remove(struct uvm_pmemrange *,
			    struct vm_page *);
struct vm_page		*uvm_pmr_extract_range(struct uvm_pmemrange *,
			    struct vm_page *, paddr_t, paddr_t,
			    struct pglist *);

#endif /* _UVM_UVM_PMEMRANGE_H_ */
@


1.13
log
@move the vm_page struct from being stored in RB macro trees to RBT functions

vm_page structs go into three trees, uvm_objtree, uvm_pmr_addr, and
uvm_pmr_size. all these have been moved to RBT code.

this should give us a decent chunk of code space back.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pmemrange.h,v 1.12 2015/02/05 23:51:06 mpi Exp $	*/
d55 1
a55 1
	RB_ENTRY(uvm_pmemrange) pmr_addr;
d97 1
a97 1
RB_HEAD(uvm_pmemrange_addr, uvm_pmemrange);
d132 1
a132 1
RB_PROTOTYPE(uvm_pmemrange_addr, uvm_pmemrange, pmr_addr,
@


1.12
log
@Remove some unneeded <uvm/uvm_extern.h> inclusions.

ok deraadt@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pmemrange.h,v 1.11 2011/07/08 18:25:56 ariane Exp $	*/
d26 2
a27 2
RB_HEAD(uvm_pmr_addr, vm_page);
RB_HEAD(uvm_pmr_size, vm_page);
d127 2
a128 2
int	uvm_pmr_addr_cmp(struct vm_page *, struct vm_page *);
int	uvm_pmr_size_cmp(struct vm_page *, struct vm_page *);
d130 2
a131 2
RB_PROTOTYPE(uvm_pmr_addr, vm_page, objt, uvm_pmr_addr_cmp);
RB_PROTOTYPE(uvm_pmr_size, vm_page, objt, uvm_pmr_size_cmp);
@


1.11
log
@Move uvm_pmr_alloc_pig to kern/subr_hibernate.c

No callers, no functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pmemrange.h,v 1.10 2011/07/08 18:20:10 ariane Exp $	*/
a24 3

#include <uvm/uvm_extern.h>
#include <uvm/uvm_page.h>
@


1.10
log
@Move uvm_pmr_zero_everything() to subr_hibernate.

This function will probably die before ever being called
from the in-tree code, since hibernate will move to RLE encoding.

No functional change, function had no callers.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pmemrange.h,v 1.9 2011/07/08 18:15:44 ariane Exp $	*/
a124 4

#ifndef SMALL_KERNEL
int	uvm_pmr_alloc_pig(paddr_t*, psize_t*);
#endif /* SMALL_KERNEL */
@


1.9
log
@Expose pmemrange internal functions via pmemrange.h.
This is so I can move the pig allocator to subr_hibernate.

No functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pmemrange.h,v 1.8 2011/07/06 19:50:38 beck Exp $	*/
a126 1
void	uvm_pmr_zero_everything(void);
@


1.8
log
@
uvm changes for buffer cache improvements.
1) Make the pagedaemon aware of the memory ranges and size of allocations
where memory is being requested, and pass this information on to
bufbackoff(), which will later (not yet) be used to ensure that the
buffer cache gets out of the way in the right area of memory.

Note that this commit does not yet make it *do* that - as currently
the buffer cache is all in dma-able memory and it will simply back
off.

2) Add uvm_pagerealloc_multi - to be used by the buffer cache code
for reallocating pages to particular regions.

much of this work by ariane, with smatterings of me, art,and oga

ok oga@@, thib@@, ariane@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pmemrange.h,v 1.7 2011/04/03 22:07:37 ariane Exp $	*/
d130 28
@


1.7
log
@Helper functions for suspend.

Allow reclaiming pages from all pools.
Allow zeroing all pages.
Allocate the more equal pig.

mlarking@@ needs this.
Not called yet.

ok mlarkin@@, theo@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pmemrange.h,v 1.6 2011/04/03 14:16:42 guenther Exp $	*/
d62 38
d109 3
d115 1
a115 1
void	uvm_pmr_freepageq(struct pglist *pgl);
d119 2
@


1.6
log
@Fix an #ifdef: uvm_pmr_isfree() is also used when DEBUG is defined
Pointed out by Fred Crowson.  ok ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pmemrange.h,v 1.5 2010/04/22 19:02:55 oga Exp $	*/
d82 5
@


1.5
log
@Committing on behalf or ariane@@.

recommit pmemrange:
        physmem allocator: change the view of free memory from single
        free pages to free ranges.  Classify memory based on region with
        associated use-counter (which is used to construct a priority
        list of where to allocate memory).

	Based on code from tedu@@, help from many.

Useable now that bugs have been found and fixed in most architecture's
pmap.c

ok by everyone who has done a pmap or uvm commit in the last year.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pmemrange.h,v 1.2 2009/06/10 20:36:40 ariane Exp $	*/
d79 1
a79 1
#ifdef DDB
@


1.4
log
@Backout pmemrange (which to most people is more well known as physmem
allocator).

"i can't see any obvious problems" oga
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pmemrange.h,v 1.3 2009/06/14 02:20:23 deraadt Exp $	*/
d37 1
a37 1
#define UVM_PMR_MEMTYPE_DIRTY	1
d54 1
a54 1
	int	nsegs;			/* Current range count. */
@


1.3
log
@back out the free vs dirty fix, because pmemrange will go soon too
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pmemrange.h,v 1.1 2009/06/01 17:42:33 ariane Exp $	*/
@


1.2
log
@Fix: Clean and dirty pages had the memtype ids and the desparate case failed
to function properly in the fast-allocation path (should not have triggered).

ok: oga, deraadt
@
text
@d37 1
a37 1
#define UVM_PMR_MEMTYPE_DIRTY	0
@


1.1
log
@physmem allocator: change the view of free memory from single free pages
to free ranges.
Classify memory based on region with associated use-counter (which is used
to construct a priority list of where to allocate memory).

Based on code from tedu@@, help from many.
Ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d37 1
a37 1
#define UVM_PMR_MEMTYPE_DIRTY	1
@

