head	1.98;
access;
symbols
	OPENBSD_6_2:1.98.0.2
	OPENBSD_6_2_BASE:1.98
	OPENBSD_6_1:1.95.0.4
	OPENBSD_6_1_BASE:1.95
	OPENBSD_6_0:1.92.0.2
	OPENBSD_6_0_BASE:1.92
	OPENBSD_5_9:1.91.0.2
	OPENBSD_5_9_BASE:1.91
	OPENBSD_5_8:1.90.0.4
	OPENBSD_5_8_BASE:1.90
	OPENBSD_5_7:1.88.0.2
	OPENBSD_5_7_BASE:1.88
	OPENBSD_5_6:1.84.0.4
	OPENBSD_5_6_BASE:1.84
	OPENBSD_5_5:1.79.0.6
	OPENBSD_5_5_BASE:1.79
	OPENBSD_5_4:1.79.0.2
	OPENBSD_5_4_BASE:1.79
	OPENBSD_5_3:1.76.0.8
	OPENBSD_5_3_BASE:1.76
	OPENBSD_5_2:1.76.0.6
	OPENBSD_5_2_BASE:1.76
	OPENBSD_5_1_BASE:1.76
	OPENBSD_5_1:1.76.0.4
	OPENBSD_5_0:1.76.0.2
	OPENBSD_5_0_BASE:1.76
	OPENBSD_4_9:1.71.0.4
	OPENBSD_4_9_BASE:1.71
	OPENBSD_4_8:1.71.0.2
	OPENBSD_4_8_BASE:1.71
	OPENBSD_4_7:1.69.0.2
	OPENBSD_4_7_BASE:1.69
	OPENBSD_4_6:1.66.0.4
	OPENBSD_4_6_BASE:1.66
	OPENBSD_4_5:1.52.0.2
	OPENBSD_4_5_BASE:1.52
	OPENBSD_4_4:1.51.0.4
	OPENBSD_4_4_BASE:1.51
	OPENBSD_4_3:1.51.0.2
	OPENBSD_4_3_BASE:1.51
	OPENBSD_4_2:1.49.0.2
	OPENBSD_4_2_BASE:1.49
	OPENBSD_4_1:1.43.0.4
	OPENBSD_4_1_BASE:1.43
	OPENBSD_4_0:1.43.0.2
	OPENBSD_4_0_BASE:1.43
	OPENBSD_3_9:1.40.0.2
	OPENBSD_3_9_BASE:1.40
	OPENBSD_3_8:1.39.0.4
	OPENBSD_3_8_BASE:1.39
	OPENBSD_3_7:1.39.0.2
	OPENBSD_3_7_BASE:1.39
	OPENBSD_3_6:1.38.0.2
	OPENBSD_3_6_BASE:1.38
	SMP_SYNC_A:1.38
	SMP_SYNC_B:1.38
	OPENBSD_3_5:1.37.0.4
	OPENBSD_3_5_BASE:1.37
	OPENBSD_3_4:1.37.0.2
	OPENBSD_3_4_BASE:1.37
	UBC_SYNC_A:1.36
	OPENBSD_3_3:1.36.0.2
	OPENBSD_3_3_BASE:1.36
	OPENBSD_3_2:1.35.0.2
	OPENBSD_3_2_BASE:1.35
	OPENBSD_3_1:1.34.0.2
	OPENBSD_3_1_BASE:1.34
	UBC_SYNC_B:1.35
	UBC:1.31.0.2
	UBC_BASE:1.31
	OPENBSD_3_0:1.20.0.2
	OPENBSD_3_0_BASE:1.20
	OPENBSD_2_9_BASE:1.12
	OPENBSD_2_9:1.12.0.2
	OPENBSD_2_8:1.9.0.2
	OPENBSD_2_8_BASE:1.9
	OPENBSD_2_7:1.7.0.2
	OPENBSD_2_7_BASE:1.7
	SMP:1.6.0.2
	SMP_BASE:1.6
	kame_19991208:1.5
	OPENBSD_2_6:1.4.0.2
	OPENBSD_2_6_BASE:1.4
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.98
date	2017.08.12.20.27.28;	author mpi;	state Exp;
branches;
next	1.97;
commitid	ex07ODV5fGEkSmvx;

1.97
date	2017.05.15.12.26.00;	author mpi;	state Exp;
branches;
next	1.96;
commitid	WMZaI3vIHNC1J8ol;

1.96
date	2017.05.03.02.43.15;	author guenther;	state Exp;
branches;
next	1.95;
commitid	S7eZ8sPkyAJQebP2;

1.95
date	2017.03.17.17.19.16;	author mpi;	state Exp;
branches;
next	1.94;
commitid	CxqvXOMqotM60GAI;

1.94
date	2017.01.25.06.15.51;	author mpi;	state Exp;
branches;
next	1.93;
commitid	X7Hk1efefaYrWlw3;

1.93
date	2016.09.16.02.35.42;	author dlg;	state Exp;
branches;
next	1.92;
commitid	Fei4687v68qad1tP;

1.92
date	2016.03.19.12.04.16;	author natano;	state Exp;
branches;
next	1.91;
commitid	gAjwyca5TfuoJAhn;

1.91
date	2015.08.27.18.59.58;	author deraadt;	state Exp;
branches;
next	1.90;
commitid	UbU0eIOfeXtedk8Q;

1.90
date	2015.05.07.01.55.44;	author jsg;	state Exp;
branches;
next	1.89;
commitid	KhO2CJgSFKm4Q3Hj;

1.89
date	2015.03.14.03.38.53;	author jsg;	state Exp;
branches;
next	1.88;
commitid	p4LJxGKbi0BU2cG6;

1.88
date	2014.12.18.23.59.28;	author tedu;	state Exp;
branches;
next	1.87;
commitid	LWyhtzv7lzm4NWIM;

1.87
date	2014.12.17.19.42.15;	author tedu;	state Exp;
branches;
next	1.86;
commitid	G4ldVK4QwvfU3tRp;

1.86
date	2014.12.16.18.30.04;	author tedu;	state Exp;
branches;
next	1.85;
commitid	P6Av4XGqOi3rFasL;

1.85
date	2014.11.16.12.31.01;	author deraadt;	state Exp;
branches;
next	1.84;
commitid	yv0ECmCdICvq576h;

1.84
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.83;
commitid	7NtJNW9udCOFtDNM;

1.83
date	2014.07.02.06.09.49;	author matthew;	state Exp;
branches;
next	1.82;
commitid	mswsoyQHeu5M87iU;

1.82
date	2014.05.08.20.08.50;	author kettenis;	state Exp;
branches;
next	1.81;

1.81
date	2014.04.13.23.14.15;	author tedu;	state Exp;
branches;
next	1.80;

1.80
date	2014.04.08.18.48.41;	author beck;	state Exp;
branches;
next	1.79;

1.79
date	2013.05.30.16.39.26;	author tedu;	state Exp;
branches;
next	1.78;

1.78
date	2013.05.30.16.29.46;	author tedu;	state Exp;
branches;
next	1.77;

1.77
date	2013.05.30.15.17.59;	author tedu;	state Exp;
branches;
next	1.76;

1.76
date	2011.07.04.20.35.35;	author deraadt;	state Exp;
branches;
next	1.75;

1.75
date	2011.07.04.00.15.47;	author oga;	state Exp;
branches;
next	1.74;

1.74
date	2011.07.03.18.34.14;	author oga;	state Exp;
branches;
next	1.73;

1.73
date	2011.07.02.15.52.25;	author thib;	state Exp;
branches;
next	1.72;

1.72
date	2011.06.16.23.00.38;	author oga;	state Exp;
branches;
next	1.71;

1.71
date	2010.05.18.04.41.14;	author dlg;	state Exp;
branches;
next	1.70;

1.70
date	2010.04.30.21.56.39;	author oga;	state Exp;
branches;
next	1.69;

1.69
date	2009.08.06.15.28.14;	author oga;	state Exp;
branches;
next	1.68;

1.68
date	2009.07.22.21.05.37;	author oga;	state Exp;
branches;
next	1.67;

1.67
date	2009.07.09.22.29.56;	author thib;	state Exp;
branches;
next	1.66;

1.66
date	2009.06.17.00.13.59;	author oga;	state Exp;
branches;
next	1.65;

1.65
date	2009.06.16.17.14.15;	author oga;	state Exp;
branches;
next	1.64;

1.64
date	2009.06.16.16.42.41;	author ariane;	state Exp;
branches;
next	1.63;

1.63
date	2009.06.16.00.11.29;	author oga;	state Exp;
branches;
next	1.62;

1.62
date	2009.06.06.17.46.44;	author art;	state Exp;
branches;
next	1.61;

1.61
date	2009.06.02.23.00.19;	author oga;	state Exp;
branches;
next	1.60;

1.60
date	2009.06.01.19.54.02;	author oga;	state Exp;
branches;
next	1.59;

1.59
date	2009.06.01.17.42.33;	author ariane;	state Exp;
branches;
next	1.58;

1.58
date	2009.05.23.14.06.37;	author oga;	state Exp;
branches;
next	1.57;

1.57
date	2009.04.14.20.13.14;	author oga;	state Exp;
branches;
next	1.56;

1.56
date	2009.04.14.20.12.05;	author oga;	state Exp;
branches;
next	1.55;

1.55
date	2009.04.13.22.17.54;	author oga;	state Exp;
branches;
next	1.54;

1.54
date	2009.04.05.18.11.03;	author oga;	state Exp;
branches;
next	1.53;

1.53
date	2009.03.20.15.19.04;	author oga;	state Exp;
branches;
next	1.52;

1.52
date	2008.10.06.18.17.29;	author deraadt;	state Exp;
branches;
next	1.51;

1.51
date	2007.09.17.20.29.55;	author thib;	state Exp;
branches;
next	1.50;

1.50
date	2007.08.31.08.38.08;	author thib;	state Exp;
branches;
next	1.49;

1.49
date	2007.06.05.00.38.24;	author deraadt;	state Exp;
branches;
next	1.48;

1.48
date	2007.05.29.21.06.34;	author thib;	state Exp;
branches;
next	1.47;

1.47
date	2007.04.14.23.04.28;	author tedu;	state Exp;
branches;
next	1.46;

1.46
date	2007.04.13.18.57.49;	author art;	state Exp;
branches;
next	1.45;

1.45
date	2007.04.04.17.44.45;	author art;	state Exp;
branches;
next	1.44;

1.44
date	2007.03.25.13.14.41;	author pedro;	state Exp;
branches;
next	1.43;

1.43
date	2006.07.31.11.51.29;	author mickey;	state Exp;
branches;
next	1.42;

1.42
date	2006.07.26.23.15.55;	author mickey;	state Exp;
branches;
next	1.41;

1.41
date	2006.06.10.13.37.02;	author pedro;	state Exp;
branches;
next	1.40;

1.40
date	2005.11.19.02.18.02;	author pedro;	state Exp;
branches;
next	1.39;

1.39
date	2004.12.26.21.22.14;	author miod;	state Exp;
branches;
next	1.38;

1.38
date	2004.06.06.13.44.07;	author grange;	state Exp;
branches;
next	1.37;

1.37
date	2003.06.12.05.55.05;	author henric;	state Exp;
branches;
next	1.36;

1.36
date	2002.11.19.18.34.41;	author jason;	state Exp;
branches;
next	1.35;

1.35
date	2002.09.11.23.16.44;	author mickey;	state Exp;
branches;
next	1.34;

1.34
date	2002.03.17.18.28.48;	author art;	state Exp;
branches;
next	1.33;

1.33
date	2002.03.14.01.27.19;	author millert;	state Exp;
branches;
next	1.32;

1.32
date	2001.12.19.08.58.07;	author art;	state Exp;
branches;
next	1.31;

1.31
date	2001.12.10.02.19.34;	author art;	state Exp;
branches
	1.31.2.1;
next	1.30;

1.30
date	2001.12.06.12.43.20;	author art;	state Exp;
branches;
next	1.29;

1.29
date	2001.12.04.23.22.42;	author art;	state Exp;
branches;
next	1.28;

1.28
date	2001.12.02.23.37.52;	author art;	state Exp;
branches;
next	1.27;

1.27
date	2001.11.28.19.28.15;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2001.11.28.13.47.40;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2001.11.27.05.27.12;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2001.11.10.18.42.32;	author art;	state Exp;
branches;
next	1.23;

1.23
date	2001.11.07.02.55.51;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2001.11.06.01.35.04;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2001.11.05.22.14.54;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2001.09.11.20.05.26;	author miod;	state Exp;
branches;
next	1.19;

1.19
date	2001.09.05.19.22.23;	author deraadt;	state Exp;
branches;
next	1.18;

1.18
date	2001.08.11.10.57.22;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2001.08.06.14.03.05;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2001.07.31.13.34.46;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2001.07.18.14.29.35;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2001.07.18.10.47.05;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2001.06.23.19.24.34;	author smart;	state Exp;
branches;
next	1.12;

1.12
date	2001.03.22.03.05.57;	author smart;	state Exp;
branches;
next	1.11;

1.11
date	2001.03.08.15.21.38;	author smart;	state Exp;
branches;
next	1.10;

1.10
date	2001.01.29.02.07.50;	author niklas;	state Exp;
branches;
next	1.9;

1.9
date	2000.09.07.20.15.29;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2000.07.20.16.34.33;	author art;	state Exp;
branches;
next	1.7;

1.7
date	2000.03.15.15.50.22;	author art;	state Exp;
branches;
next	1.6;

1.6
date	2000.01.05.08.48.01;	author art;	state Exp;
branches
	1.6.2.1;
next	1.5;

1.5
date	99.12.06.08.50.26;	author art;	state Exp;
branches;
next	1.4;

1.4
date	99.08.23.08.13.25;	author art;	state Exp;
branches;
next	1.3;

1.3
date	99.07.08.00.54.29;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	99.02.26.05.32.08;	author art;	state Exp;
branches;
next	1.1;

1.1
date	99.02.26.01.30.18;	author art;	state Exp;
branches;
next	;

1.6.2.1
date	2000.03.24.09.09.52;	author niklas;	state Exp;
branches;
next	1.6.2.2;

1.6.2.2
date	2001.05.14.22.47.49;	author niklas;	state Exp;
branches;
next	1.6.2.3;

1.6.2.3
date	2001.07.04.11.01.11;	author niklas;	state Exp;
branches;
next	1.6.2.4;

1.6.2.4
date	2001.10.31.03.32.14;	author nate;	state Exp;
branches;
next	1.6.2.5;

1.6.2.5
date	2001.11.13.23.02.31;	author niklas;	state Exp;
branches;
next	1.6.2.6;

1.6.2.6
date	2001.12.05.01.23.58;	author niklas;	state Exp;
branches;
next	1.6.2.7;

1.6.2.7
date	2002.03.06.02.17.14;	author niklas;	state Exp;
branches;
next	1.6.2.8;

1.6.2.8
date	2002.03.28.14.54.27;	author niklas;	state Exp;
branches;
next	1.6.2.9;

1.6.2.9
date	2003.03.28.00.08.48;	author niklas;	state Exp;
branches;
next	1.6.2.10;

1.6.2.10
date	2004.02.19.11.01.45;	author niklas;	state Exp;
branches;
next	1.6.2.11;

1.6.2.11
date	2004.06.07.20.41.41;	author niklas;	state Exp;
branches;
next	;

1.31.2.1
date	2002.02.02.03.28.27;	author art;	state Exp;
branches;
next	1.31.2.2;

1.31.2.2
date	2002.06.11.03.33.04;	author art;	state Exp;
branches;
next	1.31.2.3;

1.31.2.3
date	2002.10.29.00.36.50;	author art;	state Exp;
branches;
next	1.31.2.4;

1.31.2.4
date	2002.11.04.18.02.33;	author art;	state Exp;
branches;
next	1.31.2.5;

1.31.2.5
date	2004.02.21.00.20.22;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.98
log
@Use the NET_LOCK() macro instead of handrolling it.

Tested by Hrvoje Popovski.
@
text
@/*	$OpenBSD: uvm_vnode.c,v 1.97 2017/05/15 12:26:00 mpi Exp $	*/
/*	$NetBSD: uvm_vnode.c,v 1.36 2000/11/24 20:34:01 chs Exp $	*/

/*
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * Copyright (c) 1991, 1993
 *      The Regents of the University of California.
 * Copyright (c) 1990 University of Utah.
 *
 * All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * the Systems Programming Group of the University of Utah Computer
 * Science Department.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *      @@(#)vnode_pager.c       8.8 (Berkeley) 2/13/94
 * from: Id: uvm_vnode.c,v 1.1.2.26 1998/02/02 20:38:07 chuck Exp
 */

/*
 * uvm_vnode.c: the vnode pager.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/malloc.h>
#include <sys/vnode.h>
#include <sys/lock.h>
#include <sys/disklabel.h>
#include <sys/fcntl.h>
#include <sys/conf.h>
#include <sys/rwlock.h>
#include <sys/dkio.h>
#include <sys/specdev.h>

#include <uvm/uvm.h>
#include <uvm/uvm_vnode.h>

/*
 * private global data structure
 *
 * we keep a list of writeable active vnode-backed VM objects for sync op.
 * we keep a simpleq of vnodes that are currently being sync'd.
 */

LIST_HEAD(uvn_list_struct, uvm_vnode);
struct uvn_list_struct uvn_wlist;	/* writeable uvns */

SIMPLEQ_HEAD(uvn_sq_struct, uvm_vnode);
struct uvn_sq_struct uvn_sync_q;		/* sync'ing uvns */
struct rwlock uvn_sync_lock;			/* locks sync operation */

/*
 * functions
 */
void		 uvn_cluster(struct uvm_object *, voff_t, voff_t *, voff_t *);
void		 uvn_detach(struct uvm_object *);
boolean_t	 uvn_flush(struct uvm_object *, voff_t, voff_t, int);
int		 uvn_get(struct uvm_object *, voff_t, vm_page_t *, int *, int,
		     vm_prot_t, int, int);
void		 uvn_init(void);
int		 uvn_io(struct uvm_vnode *, vm_page_t *, int, int, int);
int		 uvn_put(struct uvm_object *, vm_page_t *, int, boolean_t);
void		 uvn_reference(struct uvm_object *);

/*
 * master pager structure
 */
struct uvm_pagerops uvm_vnodeops = {
	uvn_init,
	uvn_reference,
	uvn_detach,
	NULL,			/* no specialized fault routine required */
	uvn_flush,
	uvn_get,
	uvn_put,
	uvn_cluster,
	uvm_mk_pcluster, /* use generic version of this: see uvm_pager.c */
};

/*
 * the ops!
 */
/*
 * uvn_init
 *
 * init pager private data structures.
 */
void
uvn_init(void)
{

	LIST_INIT(&uvn_wlist);
	/* note: uvn_sync_q init'd in uvm_vnp_sync() */
	rw_init_flags(&uvn_sync_lock, "uvnsync", RWL_IS_VNODE);
}

/*
 * uvn_attach
 *
 * attach a vnode structure to a VM object.  if the vnode is already
 * attached, then just bump the reference count by one and return the
 * VM object.   if not already attached, attach and return the new VM obj.
 * the "accessprot" tells the max access the attaching thread wants to
 * our pages.
 *
 * => in fact, nothing should be locked so that we can sleep here.
 * => note that uvm_object is first thing in vnode structure, so their
 *    pointers are equiv.
 */
struct uvm_object *
uvn_attach(struct vnode *vp, vm_prot_t accessprot)
{
	struct uvm_vnode *uvn = vp->v_uvm;
	struct vattr vattr;
	int oldflags, result;
	struct partinfo pi;
	u_quad_t used_vnode_size = 0;

	/* first get a lock on the uvn. */
	while (uvn->u_flags & UVM_VNODE_BLOCKED) {
		uvn->u_flags |= UVM_VNODE_WANTED;
		UVM_WAIT(uvn, FALSE, "uvn_attach", 0);
	}

	/* if we're mapping a BLK device, make sure it is a disk. */
	if (vp->v_type == VBLK && bdevsw[major(vp->v_rdev)].d_type != D_DISK) {
		return(NULL);
	}

	/*
	 * now uvn must not be in a blocked state.
	 * first check to see if it is already active, in which case
	 * we can bump the reference count, check to see if we need to
	 * add it to the writeable list, and then return.
	 */
	if (uvn->u_flags & UVM_VNODE_VALID) {	/* already active? */

		/* regain vref if we were persisting */
		if (uvn->u_obj.uo_refs == 0) {
			vref(vp);
		}
		uvn->u_obj.uo_refs++;		/* bump uvn ref! */

		/* check for new writeable uvn */
		if ((accessprot & PROT_WRITE) != 0 &&
		    (uvn->u_flags & UVM_VNODE_WRITEABLE) == 0) {
			LIST_INSERT_HEAD(&uvn_wlist, uvn, u_wlist);
			/* we are now on wlist! */
			uvn->u_flags |= UVM_VNODE_WRITEABLE;
		}

		return (&uvn->u_obj);
	}

	/*
	 * need to call VOP_GETATTR() to get the attributes, but that could
	 * block (due to I/O), so we want to unlock the object before calling.
	 * however, we want to keep anyone else from playing with the object
	 * while it is unlocked.   to do this we set UVM_VNODE_ALOCK which
	 * prevents anyone from attaching to the vnode until we are done with
	 * it.
	 */
	uvn->u_flags = UVM_VNODE_ALOCK;

	if (vp->v_type == VBLK) {
		/*
		 * We could implement this as a specfs getattr call, but:
		 *
		 *	(1) VOP_GETATTR() would get the file system
		 *	    vnode operation, not the specfs operation.
		 *
		 *	(2) All we want is the size, anyhow.
		 */
		result = (*bdevsw[major(vp->v_rdev)].d_ioctl)(vp->v_rdev,
		    DIOCGPART, (caddr_t)&pi, FREAD, curproc);
		if (result == 0) {
			/* XXX should remember blocksize */
			used_vnode_size = (u_quad_t)pi.disklab->d_secsize *
			    (u_quad_t)DL_GETPSIZE(pi.part);
		}
	} else {
		result = VOP_GETATTR(vp, &vattr, curproc->p_ucred, curproc);
		if (result == 0)
			used_vnode_size = vattr.va_size;
	}

	if (result != 0) {
		if (uvn->u_flags & UVM_VNODE_WANTED)
			wakeup(uvn);
		uvn->u_flags = 0;
		return(NULL);
	}

	/*
	 * make sure that the newsize fits within a vaddr_t
	 * XXX: need to revise addressing data types
	 */
#ifdef DEBUG
	if (vp->v_type == VBLK)
		printf("used_vnode_size = %llu\n", (long long)used_vnode_size);
#endif

	/* now set up the uvn. */
	uvm_objinit(&uvn->u_obj, &uvm_vnodeops, 1);
	oldflags = uvn->u_flags;
	uvn->u_flags = UVM_VNODE_VALID|UVM_VNODE_CANPERSIST;
	uvn->u_nio = 0;
	uvn->u_size = used_vnode_size;

	/* if write access, we need to add it to the wlist */
	if (accessprot & PROT_WRITE) {
		LIST_INSERT_HEAD(&uvn_wlist, uvn, u_wlist);
		uvn->u_flags |= UVM_VNODE_WRITEABLE;	/* we are on wlist! */
	}

	/*
	 * add a reference to the vnode.   this reference will stay as long
	 * as there is a valid mapping of the vnode.   dropped when the
	 * reference count goes to zero [and we either free or persist].
	 */
	vref(vp);
	if (oldflags & UVM_VNODE_WANTED)
		wakeup(uvn);

	return(&uvn->u_obj);
}


/*
 * uvn_reference
 *
 * duplicate a reference to a VM object.  Note that the reference
 * count must already be at least one (the passed in reference) so
 * there is no chance of the uvn being killed out here.
 *
 * => caller must be using the same accessprot as was used at attach time
 */


void
uvn_reference(struct uvm_object *uobj)
{
#ifdef DEBUG
	struct uvm_vnode *uvn = (struct uvm_vnode *) uobj;
#endif

#ifdef DEBUG
	if ((uvn->u_flags & UVM_VNODE_VALID) == 0) {
		printf("uvn_reference: ref=%d, flags=0x%x\n", uvn->u_flags,
		    uobj->uo_refs);
		panic("uvn_reference: invalid state");
	}
#endif
	uobj->uo_refs++;
}

/*
 * uvn_detach
 *
 * remove a reference to a VM object.
 *
 * => caller must call with map locked.
 * => this starts the detach process, but doesn't have to finish it
 *    (async i/o could still be pending).
 */
void
uvn_detach(struct uvm_object *uobj)
{
	struct uvm_vnode *uvn;
	struct vnode *vp;
	int oldflags;


	uobj->uo_refs--;			/* drop ref! */
	if (uobj->uo_refs) {			/* still more refs */
		return;
	}

	/* get other pointers ... */
	uvn = (struct uvm_vnode *) uobj;
	vp = uvn->u_vnode;

	/*
	 * clear VTEXT flag now that there are no mappings left (VTEXT is used
	 * to keep an active text file from being overwritten).
	 */
	vp->v_flag &= ~VTEXT;

	/*
	 * we just dropped the last reference to the uvn.   see if we can
	 * let it "stick around".
	 */
	if (uvn->u_flags & UVM_VNODE_CANPERSIST) {
		/* won't block */
		uvn_flush(uobj, 0, 0, PGO_DEACTIVATE|PGO_ALLPAGES);
		vrele(vp);			/* drop vnode reference */
		return;
	}

	/* its a goner! */
	uvn->u_flags |= UVM_VNODE_DYING;

	/*
	 * even though we may unlock in flush, no one can gain a reference
	 * to us until we clear the "dying" flag [because it blocks
	 * attaches].  we will not do that until after we've disposed of all
	 * the pages with uvn_flush().  note that before the flush the only
	 * pages that could be marked PG_BUSY are ones that are in async
	 * pageout by the daemon.  (there can't be any pending "get"'s
	 * because there are no references to the object).
	 */
	(void) uvn_flush(uobj, 0, 0, PGO_CLEANIT|PGO_FREE|PGO_ALLPAGES);

	/*
	 * given the structure of this pager, the above flush request will
	 * create the following state: all the pages that were in the object
	 * have either been free'd or they are marked PG_BUSY and in the 
	 * middle of an async io. If we still have pages we set the "relkill"
	 * state, so that in the case the vnode gets terminated we know 
	 * to leave it alone. Otherwise we'll kill the vnode when it's empty.
	 */
	uvn->u_flags |= UVM_VNODE_RELKILL;
	/* wait on any outstanding io */
	while (uobj->uo_npages && uvn->u_flags & UVM_VNODE_RELKILL) {
		uvn->u_flags |= UVM_VNODE_IOSYNC;
		UVM_WAIT(&uvn->u_nio, FALSE, "uvn_term", 0);
	}

	if ((uvn->u_flags & UVM_VNODE_RELKILL) == 0)
		return;

	/*
	 * kill object now.   note that we can't be on the sync q because
	 * all references are gone.
	 */
	if (uvn->u_flags & UVM_VNODE_WRITEABLE) {
		LIST_REMOVE(uvn, u_wlist);
	}
	KASSERT(RBT_EMPTY(uvm_objtree, &uobj->memt));
	oldflags = uvn->u_flags;
	uvn->u_flags = 0;

	/* wake up any sleepers */
	if (oldflags & UVM_VNODE_WANTED)
		wakeup(uvn);

	/* drop our reference to the vnode. */
	vrele(vp);

	return;
}

/*
 * uvm_vnp_terminate: external hook to clear out a vnode's VM
 *
 * called in two cases:
 *  [1] when a persisting vnode vm object (i.e. one with a zero reference
 *      count) needs to be freed so that a vnode can be reused.  this
 *      happens under "getnewvnode" in vfs_subr.c.   if the vnode from
 *      the free list is still attached (i.e. not VBAD) then vgone is
 *	called.   as part of the vgone trace this should get called to
 *	free the vm object.   this is the common case.
 *  [2] when a filesystem is being unmounted by force (MNT_FORCE,
 *	"umount -f") the vgone() function is called on active vnodes
 *	on the mounted file systems to kill their data (the vnodes become
 *	"dead" ones [see src/sys/miscfs/deadfs/...]).  that results in a
 *	call here (even if the uvn is still in use -- i.e. has a non-zero
 *	reference count).  this case happens at "umount -f" and during a
 *	"reboot/halt" operation.
 *
 * => the caller must XLOCK and VOP_LOCK the vnode before calling us
 *	[protects us from getting a vnode that is already in the DYING
 *	 state...]
 * => in case [2] the uvn is still alive after this call, but all I/O
 *	ops will fail (due to the backing vnode now being "dead").  this
 *	will prob. kill any process using the uvn due to pgo_get failing.
 */
void
uvm_vnp_terminate(struct vnode *vp)
{
	struct uvm_vnode *uvn = vp->v_uvm;
	int oldflags;

	/* check if it is valid */
	if ((uvn->u_flags & UVM_VNODE_VALID) == 0) {
		return;
	}

	/*
	 * must be a valid uvn that is not already dying (because XLOCK
	 * protects us from that).   the uvn can't in the ALOCK state
	 * because it is valid, and uvn's that are in the ALOCK state haven't
	 * been marked valid yet.
	 */
#ifdef DEBUG
	/*
	 * debug check: are we yanking the vnode out from under our uvn?
	 */
	if (uvn->u_obj.uo_refs) {
		printf("uvm_vnp_terminate(%p): terminating active vnode "
		    "(refs=%d)\n", uvn, uvn->u_obj.uo_refs);
	}
#endif

	/*
	 * it is possible that the uvn was detached and is in the relkill
	 * state [i.e. waiting for async i/o to finish].
	 * we take over the vnode now and cancel the relkill.
	 * we want to know when the i/o is done so we can recycle right
	 * away.   note that a uvn can only be in the RELKILL state if it
	 * has a zero reference count.
	 */
	if (uvn->u_flags & UVM_VNODE_RELKILL)
		uvn->u_flags &= ~UVM_VNODE_RELKILL;	/* cancel RELKILL */

	/*
	 * block the uvn by setting the dying flag, and then flush the
	 * pages.
	 *
	 * also, note that we tell I/O that we are already VOP_LOCK'd so
	 * that uvn_io doesn't attempt to VOP_LOCK again.
	 *
	 * XXXCDC: setting VNISLOCKED on an active uvn which is being terminated
	 *	due to a forceful unmount might not be a good idea.  maybe we
	 *	need a way to pass in this info to uvn_flush through a
	 *	pager-defined PGO_ constant [currently there are none].
	 */
	uvn->u_flags |= UVM_VNODE_DYING|UVM_VNODE_VNISLOCKED;

	(void) uvn_flush(&uvn->u_obj, 0, 0, PGO_CLEANIT|PGO_FREE|PGO_ALLPAGES);

	/*
	 * as we just did a flush we expect all the pages to be gone or in
	 * the process of going.  sleep to wait for the rest to go [via iosync].
	 */
	while (uvn->u_obj.uo_npages) {
#ifdef DEBUG
		struct vm_page *pp;
		RBT_FOREACH(pp, uvm_objtree, &uvn->u_obj.memt) {
			if ((pp->pg_flags & PG_BUSY) == 0)
				panic("uvm_vnp_terminate: detected unbusy pg");
		}
		if (uvn->u_nio == 0)
			panic("uvm_vnp_terminate: no I/O to wait for?");
		printf("uvm_vnp_terminate: waiting for I/O to fin.\n");
		/*
		 * XXXCDC: this is unlikely to happen without async i/o so we
		 * put a printf in just to keep an eye on it.
		 */
#endif
		uvn->u_flags |= UVM_VNODE_IOSYNC;
		UVM_WAIT(&uvn->u_nio, FALSE, "uvn_term", 0);
	}

	/*
	 * done.   now we free the uvn if its reference count is zero
	 * (true if we are zapping a persisting uvn).   however, if we are
	 * terminating a uvn with active mappings we let it live ... future
	 * calls down to the vnode layer will fail.
	 */
	oldflags = uvn->u_flags;
	if (uvn->u_obj.uo_refs) {
		/*
		 * uvn must live on it is dead-vnode state until all references
		 * are gone.   restore flags.    clear CANPERSIST state.
		 */
		uvn->u_flags &= ~(UVM_VNODE_DYING|UVM_VNODE_VNISLOCKED|
		      UVM_VNODE_WANTED|UVM_VNODE_CANPERSIST);
	} else {
		/*
		 * free the uvn now.   note that the vref reference is already
		 * gone [it is dropped when we enter the persist state].
		 */
		if (uvn->u_flags & UVM_VNODE_IOSYNCWANTED)
			panic("uvm_vnp_terminate: io sync wanted bit set");

		if (uvn->u_flags & UVM_VNODE_WRITEABLE) {
			LIST_REMOVE(uvn, u_wlist);
		}
		uvn->u_flags = 0;	/* uvn is history, clear all bits */
	}

	if (oldflags & UVM_VNODE_WANTED)
		wakeup(uvn);
}

/*
 * NOTE: currently we have to use VOP_READ/VOP_WRITE because they go
 * through the buffer cache and allow I/O in any size.  These VOPs use
 * synchronous i/o.  [vs. VOP_STRATEGY which can be async, but doesn't
 * go through the buffer cache or allow I/O sizes larger than a
 * block].  we will eventually want to change this.
 *
 * issues to consider:
 *   uvm provides the uvm_aiodesc structure for async i/o management.
 * there are two tailq's in the uvm. structure... one for pending async
 * i/o and one for "done" async i/o.   to do an async i/o one puts
 * an aiodesc on the "pending" list (protected by splbio()), starts the
 * i/o and returns VM_PAGER_PEND.    when the i/o is done, we expect
 * some sort of "i/o done" function to be called (at splbio(), interrupt
 * time).   this function should remove the aiodesc from the pending list
 * and place it on the "done" list and wakeup the daemon.   the daemon
 * will run at normal spl() and will remove all items from the "done"
 * list and call the "aiodone" hook for each done request (see uvm_pager.c).
 * [in the old vm code, this was done by calling the "put" routine with
 * null arguments which made the code harder to read and understand because
 * you had one function ("put") doing two things.]
 *
 * so the current pager needs:
 *   int uvn_aiodone(struct uvm_aiodesc *)
 *
 * => return 0 (aio finished, free it). otherwise requeue for later collection.
 * => called with pageq's locked by the daemon.
 *
 * general outline:
 * - drop "u_nio" (this req is done!)
 * - if (object->iosync && u_naio == 0) { wakeup &uvn->u_naio }
 * - get "page" structures (atop?).
 * - handle "wanted" pages
 * dont forget to look at "object" wanted flag in all cases.
 */

/*
 * uvn_flush: flush pages out of a uvm object.
 *
 * => if PGO_CLEANIT is set, we may block (due to I/O).   thus, a caller
 *	might want to unlock higher level resources (e.g. vm_map)
 *	before calling flush.
 * => if PGO_CLEANIT is not set, then we will not block
 * => if PGO_ALLPAGE is set, then all pages in the object are valid targets
 *	for flushing.
 * => NOTE: we are allowed to lock the page queues, so the caller
 *	must not be holding the lock on them [e.g. pagedaemon had
 *	better not call us with the queues locked]
 * => we return TRUE unless we encountered some sort of I/O error
 *
 * comment on "cleaning" object and PG_BUSY pages:
 *	this routine is holding the lock on the object.   the only time
 *	that it can run into a PG_BUSY page that it does not own is if
 *	some other process has started I/O on the page (e.g. either
 *	a pagein, or a pageout).    if the PG_BUSY page is being paged
 *	in, then it can not be dirty (!PG_CLEAN) because no one has
 *	had a chance to modify it yet.    if the PG_BUSY page is being
 *	paged out then it means that someone else has already started
 *	cleaning the page for us (how nice!).    in this case, if we
 *	have syncio specified, then after we make our pass through the
 *	object we need to wait for the other PG_BUSY pages to clear
 *	off (i.e. we need to do an iosync).   also note that once a
 *	page is PG_BUSY it must stay in its object until it is un-busyed.
 */
boolean_t
uvn_flush(struct uvm_object *uobj, voff_t start, voff_t stop, int flags)
{
	struct uvm_vnode *uvn = (struct uvm_vnode *) uobj;
	struct vm_page *pp, *ptmp;
	struct vm_page *pps[MAXBSIZE >> PAGE_SHIFT], **ppsp;
	int npages, result, lcv;
	boolean_t retval, need_iosync, needs_clean;
	voff_t curoff;

	/* get init vals and determine how we are going to traverse object */
	need_iosync = FALSE;
	retval = TRUE;		/* return value */
	if (flags & PGO_ALLPAGES) {
		start = 0;
		stop = round_page(uvn->u_size);
	} else {
		start = trunc_page(start);
		stop = MIN(round_page(stop), round_page(uvn->u_size));
	}

	/*
	 * PG_CLEANCHK: this bit is used by the pgo_mk_pcluster function as
	 * a _hint_ as to how up to date the PG_CLEAN bit is.   if the hint
	 * is wrong it will only prevent us from clustering... it won't break
	 * anything.   we clear all PG_CLEANCHK bits here, and pgo_mk_pcluster
	 * will set them as it syncs PG_CLEAN.   This is only an issue if we
	 * are looking at non-inactive pages (because inactive page's PG_CLEAN
	 * bit is always up to date since there are no mappings).
	 * [borrowed PG_CLEANCHK idea from FreeBSD VM]
	 */
	if ((flags & PGO_CLEANIT) != 0) {
		KASSERT(uobj->pgops->pgo_mk_pcluster != 0);
		for (curoff = start ; curoff < stop; curoff += PAGE_SIZE) {
			if ((pp = uvm_pagelookup(uobj, curoff)) != NULL)
				atomic_clearbits_int(&pp->pg_flags,
				    PG_CLEANCHK);
		}
	}

	ppsp = NULL;		/* XXX: shut up gcc */
	uvm_lock_pageq();
	/* locked: both page queues */
	for (curoff = start; curoff < stop; curoff += PAGE_SIZE) {
		if ((pp = uvm_pagelookup(uobj, curoff)) == NULL)
			continue;
		/*
		 * handle case where we do not need to clean page (either
		 * because we are not clean or because page is not dirty or
		 * is busy):
		 *
		 * NOTE: we are allowed to deactivate a non-wired active
		 * PG_BUSY page, but once a PG_BUSY page is on the inactive
		 * queue it must stay put until it is !PG_BUSY (so as not to
		 * confuse pagedaemon).
		 */
		if ((flags & PGO_CLEANIT) == 0 || (pp->pg_flags & PG_BUSY) != 0) {
			needs_clean = FALSE;
			if ((pp->pg_flags & PG_BUSY) != 0 &&
			    (flags & (PGO_CLEANIT|PGO_SYNCIO)) ==
			             (PGO_CLEANIT|PGO_SYNCIO))
				need_iosync = TRUE;
		} else {
			/*
			 * freeing: nuke all mappings so we can sync
			 * PG_CLEAN bit with no race
			 */
			if ((pp->pg_flags & PG_CLEAN) != 0 &&
			    (flags & PGO_FREE) != 0 &&
			    (pp->pg_flags & PQ_ACTIVE) != 0)
				pmap_page_protect(pp, PROT_NONE);
			if ((pp->pg_flags & PG_CLEAN) != 0 &&
			    pmap_is_modified(pp))
				atomic_clearbits_int(&pp->pg_flags, PG_CLEAN);
			atomic_setbits_int(&pp->pg_flags, PG_CLEANCHK);

			needs_clean = ((pp->pg_flags & PG_CLEAN) == 0);
		}

		/* if we don't need a clean, deactivate/free pages then cont. */
		if (!needs_clean) {
			if (flags & PGO_DEACTIVATE) {
				if (pp->wire_count == 0) {
					pmap_page_protect(pp, PROT_NONE);
					uvm_pagedeactivate(pp);
				}
			} else if (flags & PGO_FREE) {
				if (pp->pg_flags & PG_BUSY) {
					atomic_setbits_int(&pp->pg_flags,
					    PG_WANTED);
					uvm_unlock_pageq();
					UVM_WAIT(pp, 0, "uvn_flsh", 0);
					uvm_lock_pageq();
					curoff -= PAGE_SIZE;
					continue;
				} else {
					pmap_page_protect(pp, PROT_NONE);
					/* removed page from object */
					uvm_pagefree(pp);
				}
			}
			continue;
		}

		/*
		 * pp points to a page in the object that we are
		 * working on.  if it is !PG_CLEAN,!PG_BUSY and we asked
		 * for cleaning (PGO_CLEANIT).  we clean it now.
		 *
		 * let uvm_pager_put attempted a clustered page out.
		 * note: locked: page queues.
		 */
		atomic_setbits_int(&pp->pg_flags, PG_BUSY);
		UVM_PAGE_OWN(pp, "uvn_flush");
		pmap_page_protect(pp, PROT_READ);
		/* if we're async, free the page in aiodoned */
		if ((flags & (PGO_FREE|PGO_SYNCIO)) == PGO_FREE)
			atomic_setbits_int(&pp->pg_flags, PG_RELEASED);
ReTry:
		ppsp = pps;
		npages = sizeof(pps) / sizeof(struct vm_page *);

		result = uvm_pager_put(uobj, pp, &ppsp, &npages,
			   flags | PGO_DOACTCLUST, start, stop);

		/*
		 * if we did an async I/O it is remotely possible for the
		 * async i/o to complete and the page "pp" be freed or what
		 * not before we get a chance to relock the object. Therefore,
		 * we only touch it when it won't be freed, RELEASED took care
		 * of the rest.
		 */
		uvm_lock_pageq();

		/*
		 * VM_PAGER_AGAIN: given the structure of this pager, this
		 * can only happen when we are doing async I/O and can't
		 * map the pages into kernel memory (pager_map) due to lack
		 * of vm space.   if this happens we drop back to sync I/O.
		 */
		if (result == VM_PAGER_AGAIN) {
			/*
			 * it is unlikely, but page could have been released
			 * we ignore this now and retry the I/O.
			 * we will detect and
			 * handle the released page after the syncio I/O
			 * completes.
			 */
#ifdef DIAGNOSTIC
			if (flags & PGO_SYNCIO)
	panic("uvn_flush: PGO_SYNCIO return 'try again' error (impossible)");
#endif
			flags |= PGO_SYNCIO;
			if (flags & PGO_FREE)
				atomic_clearbits_int(&pp->pg_flags,
				    PG_RELEASED);

			goto ReTry;
		}

		/*
		 * the cleaning operation is now done.   finish up.  note that
		 * on error (!OK, !PEND) uvm_pager_put drops the cluster for us.
		 * if success (OK, PEND) then uvm_pager_put returns the cluster
		 * to us in ppsp/npages.
		 */
		/*
		 * for pending async i/o if we are not deactivating
		 * we can move on to the next page. aiodoned deals with
		 * the freeing case for us.
		 */
		if (result == VM_PAGER_PEND && (flags & PGO_DEACTIVATE) == 0)
			continue;

		/*
		 * need to look at each page of the I/O operation, and do what
		 * we gotta do.
		 */
		for (lcv = 0 ; lcv < npages; lcv++) {
			ptmp = ppsp[lcv];
			/*
			 * verify the page didn't get moved
			 */
			if (result == VM_PAGER_PEND && ptmp->uobject != uobj)
				continue;

			/*
			 * unbusy the page if I/O is done.   note that for
			 * pending I/O it is possible that the I/O op
			 * finished
			 * (in which case the page is no longer busy).
			 */
			if (result != VM_PAGER_PEND) {
				if (ptmp->pg_flags & PG_WANTED)
					wakeup(ptmp);

				atomic_clearbits_int(&ptmp->pg_flags,
				    PG_WANTED|PG_BUSY);
				UVM_PAGE_OWN(ptmp, NULL);
				atomic_setbits_int(&ptmp->pg_flags,
				    PG_CLEAN|PG_CLEANCHK);
				if ((flags & PGO_FREE) == 0)
					pmap_clear_modify(ptmp);
			}

			/* dispose of page */
			if (flags & PGO_DEACTIVATE) {
				if (ptmp->wire_count == 0) {
					pmap_page_protect(ptmp, PROT_NONE);
					uvm_pagedeactivate(ptmp);
				}
			} else if (flags & PGO_FREE &&
			    result != VM_PAGER_PEND) {
				if (result != VM_PAGER_OK) {
					printf("uvn_flush: obj=%p, "
					   "offset=0x%llx.  error "
					   "during pageout.\n",
					    pp->uobject,
					    (long long)pp->offset);
					printf("uvn_flush: WARNING: "
					    "changes to page may be "
					    "lost!\n");
					retval = FALSE;
				}
				pmap_page_protect(ptmp, PROT_NONE);
				uvm_pagefree(ptmp);
			}

		}		/* end of "lcv" for loop */

	}		/* end of "pp" for loop */

	/* done with pagequeues: unlock */
	uvm_unlock_pageq();

	/* now wait for all I/O if required. */
	if (need_iosync) {
		while (uvn->u_nio != 0) {
			uvn->u_flags |= UVM_VNODE_IOSYNC;
			UVM_WAIT(&uvn->u_nio, FALSE, "uvn_flush", 0);
		}
		if (uvn->u_flags & UVM_VNODE_IOSYNCWANTED)
			wakeup(&uvn->u_flags);
		uvn->u_flags &= ~(UVM_VNODE_IOSYNC|UVM_VNODE_IOSYNCWANTED);
	}

	return(retval);
}

/*
 * uvn_cluster
 *
 * we are about to do I/O in an object at offset.   this function is called
 * to establish a range of offsets around "offset" in which we can cluster
 * I/O.
 */

void
uvn_cluster(struct uvm_object *uobj, voff_t offset, voff_t *loffset,
    voff_t *hoffset)
{
	struct uvm_vnode *uvn = (struct uvm_vnode *) uobj;
	*loffset = offset;

	if (*loffset >= uvn->u_size)
		panic("uvn_cluster: offset out of range");

	/*
	 * XXX: old pager claims we could use VOP_BMAP to get maxcontig value.
	 */
	*hoffset = *loffset + MAXBSIZE;
	if (*hoffset > round_page(uvn->u_size))	/* past end? */
		*hoffset = round_page(uvn->u_size);

	return;
}

/*
 * uvn_put: flush page data to backing store.
 *
 * => prefer map unlocked (not required)
 * => flags: PGO_SYNCIO -- use sync. I/O
 * => note: caller must set PG_CLEAN and pmap_clear_modify (if needed)
 * => XXX: currently we use VOP_READ/VOP_WRITE which are only sync.
 *	[thus we never do async i/o!  see iodone comment]
 */
int
uvn_put(struct uvm_object *uobj, struct vm_page **pps, int npages, int flags)
{
	int retval;

	retval = uvn_io((struct uvm_vnode*)uobj, pps, npages, flags, UIO_WRITE);

	return(retval);
}

/*
 * uvn_get: get pages (synchronously) from backing store
 *
 * => prefer map unlocked (not required)
 * => flags: PGO_ALLPAGES: get all of the pages
 *           PGO_LOCKED: fault data structures are locked
 * => NOTE: offset is the offset of pps[0], _NOT_ pps[centeridx]
 * => NOTE: caller must check for released pages!!
 */
int
uvn_get(struct uvm_object *uobj, voff_t offset, struct vm_page **pps,
    int *npagesp, int centeridx, vm_prot_t access_type, int advice, int flags)
{
	voff_t current_offset;
	struct vm_page *ptmp;
	int lcv, result, gotpages;
	boolean_t done;

	/* step 1: handled the case where fault data structures are locked. */
	if (flags & PGO_LOCKED) {
		/*
		 * gotpages is the current number of pages we've gotten (which
		 * we pass back up to caller via *npagesp.
		 */
		gotpages = 0;

		/*
		 * step 1a: get pages that are already resident.   only do this
		 * if the data structures are locked (i.e. the first time
		 * through).
		 */
		done = TRUE;	/* be optimistic */

		for (lcv = 0, current_offset = offset ; lcv < *npagesp ;
		    lcv++, current_offset += PAGE_SIZE) {

			/* do we care about this page?  if not, skip it */
			if (pps[lcv] == PGO_DONTCARE)
				continue;

			/* lookup page */
			ptmp = uvm_pagelookup(uobj, current_offset);

			/* to be useful must get a non-busy, non-released pg */
			if (ptmp == NULL ||
			    (ptmp->pg_flags & PG_BUSY) != 0) {
				if (lcv == centeridx || (flags & PGO_ALLPAGES)
				    != 0)
					done = FALSE;	/* need to do a wait or I/O! */
				continue;
			}

			/*
			 * useful page: busy it and plug it in our
			 * result array
			 */
			atomic_setbits_int(&ptmp->pg_flags, PG_BUSY);
			UVM_PAGE_OWN(ptmp, "uvn_get1");
			pps[lcv] = ptmp;
			gotpages++;

		}

		/*
		 * XXX: given the "advice", should we consider async read-ahead?
		 * XXX: fault current does deactive of pages behind us.  is
		 * this good (other callers might now).
		 */
		/*
		 * XXX: read-ahead currently handled by buffer cache (bread)
		 * level.
		 * XXX: no async i/o available.
		 * XXX: so we don't do anything now.
		 */

		/*
		 * step 1c: now we've either done everything needed or we to
		 * unlock and do some waiting or I/O.
		 */

		*npagesp = gotpages;		/* let caller know */
		if (done)
			return(VM_PAGER_OK);		/* bingo! */
		else
			return(VM_PAGER_UNLOCK);
	}

	/*
	 * step 2: get non-resident or busy pages.
	 * data structures are unlocked.
	 *
	 * XXX: because we can't do async I/O at this level we get things
	 * page at a time (otherwise we'd chunk).   the VOP_READ() will do
	 * async-read-ahead for us at a lower level.
	 */
	for (lcv = 0, current_offset = offset;
			 lcv < *npagesp ; lcv++, current_offset += PAGE_SIZE) {

		/* skip over pages we've already gotten or don't want */
		/* skip over pages we don't _have_ to get */
		if (pps[lcv] != NULL || (lcv != centeridx &&
		    (flags & PGO_ALLPAGES) == 0))
			continue;

		/*
		 * we have yet to locate the current page (pps[lcv]).   we first
		 * look for a page that is already at the current offset.   if
		 * we fine a page, we check to see if it is busy or released.
		 * if that is the case, then we sleep on the page until it is
		 * no longer busy or released and repeat the lookup.    if the
		 * page we found is neither busy nor released, then we busy it
		 * (so we own it) and plug it into pps[lcv].   this breaks the
		 * following while loop and indicates we are ready to move on
		 * to the next page in the "lcv" loop above.
		 *
		 * if we exit the while loop with pps[lcv] still set to NULL,
		 * then it means that we allocated a new busy/fake/clean page
		 * ptmp in the object and we need to do I/O to fill in the data.
		 */
		while (pps[lcv] == NULL) {	/* top of "pps" while loop */
			/* look for a current page */
			ptmp = uvm_pagelookup(uobj, current_offset);

			/* nope?   allocate one now (if we can) */
			if (ptmp == NULL) {
				ptmp = uvm_pagealloc(uobj, current_offset,
				    NULL, 0);

				/* out of RAM? */
				if (ptmp == NULL) {
					uvm_wait("uvn_getpage");

					/* goto top of pps while loop */
					continue;
				}

				/*
				 * got new page ready for I/O.  break pps
				 * while loop.  pps[lcv] is still NULL.
				 */
				break;
			}

			/* page is there, see if we need to wait on it */
			if ((ptmp->pg_flags & PG_BUSY) != 0) {
				atomic_setbits_int(&ptmp->pg_flags, PG_WANTED);
				UVM_WAIT(ptmp, FALSE, "uvn_get", 0);
				continue;	/* goto top of pps while loop */
			}

			/*
			 * if we get here then the page has become resident
			 * and unbusy between steps 1 and 2.  we busy it
			 * now (so we own it) and set pps[lcv] (so that we
			 * exit the while loop).
			 */
			atomic_setbits_int(&ptmp->pg_flags, PG_BUSY);
			UVM_PAGE_OWN(ptmp, "uvn_get2");
			pps[lcv] = ptmp;
		}

		/*
		 * if we own the a valid page at the correct offset, pps[lcv]
		 * will point to it.   nothing more to do except go to the
		 * next page.
		 */
		if (pps[lcv])
			continue;			/* next lcv */

		/*
		 * we have a "fake/busy/clean" page that we just allocated.  do
		 * I/O to fill it with valid data.
		 */
		result = uvn_io((struct uvm_vnode *) uobj, &ptmp, 1,
		    PGO_SYNCIO, UIO_READ);

		/*
		 * I/O done.  because we used syncio the result can not be
		 * PEND or AGAIN.
		 */
		if (result != VM_PAGER_OK) {
			if (ptmp->pg_flags & PG_WANTED)
				wakeup(ptmp);

			atomic_clearbits_int(&ptmp->pg_flags,
			    PG_WANTED|PG_BUSY);
			UVM_PAGE_OWN(ptmp, NULL);
			uvm_lock_pageq();
			uvm_pagefree(ptmp);
			uvm_unlock_pageq();
			return(result);
		}

		/*
		 * we got the page!   clear the fake flag (indicates valid
		 * data now in page) and plug into our result array.   note
		 * that page is still busy.
		 *
		 * it is the callers job to:
		 * => check if the page is released
		 * => unbusy the page
		 * => activate the page
		 */

		/* data is valid ... */
		atomic_clearbits_int(&ptmp->pg_flags, PG_FAKE);
		pmap_clear_modify(ptmp);		/* ... and clean */
		pps[lcv] = ptmp;

	}

	return (VM_PAGER_OK);
}

/*
 * uvn_io: do I/O to a vnode
 *
 * => prefer map unlocked (not required)
 * => flags: PGO_SYNCIO -- use sync. I/O
 * => XXX: currently we use VOP_READ/VOP_WRITE which are only sync.
 *	[thus we never do async i/o!  see iodone comment]
 */

int
uvn_io(struct uvm_vnode *uvn, vm_page_t *pps, int npages, int flags, int rw)
{
	struct vnode *vn;
	struct uio uio;
	struct iovec iov;
	vaddr_t kva;
	off_t file_offset;
	int waitf, result, mapinflags;
	size_t got, wanted;

	/* init values */
	waitf = (flags & PGO_SYNCIO) ? M_WAITOK : M_NOWAIT;
	vn = uvn->u_vnode;
	file_offset = pps[0]->offset;

	/* check for sync'ing I/O. */
	while (uvn->u_flags & UVM_VNODE_IOSYNC) {
		if (waitf == M_NOWAIT) {
			return(VM_PAGER_AGAIN);
		}
		uvn->u_flags |= UVM_VNODE_IOSYNCWANTED;
		UVM_WAIT(&uvn->u_flags, FALSE, "uvn_iosync", 0);
	}

	/* check size */
	if (file_offset >= uvn->u_size) {
		return(VM_PAGER_BAD);
	}

	/* first try and map the pages in (without waiting) */
	mapinflags = (rw == UIO_READ) ?
	    UVMPAGER_MAPIN_READ : UVMPAGER_MAPIN_WRITE;

	kva = uvm_pagermapin(pps, npages, mapinflags);
	if (kva == 0 && waitf == M_NOWAIT) {
		return(VM_PAGER_AGAIN);
	}

	/*
	 * ok, now bump u_nio up.   at this point we are done with uvn
	 * and can unlock it.   if we still don't have a kva, try again
	 * (this time with sleep ok).
	 */
	uvn->u_nio++;			/* we have an I/O in progress! */
	if (kva == 0)
		kva = uvm_pagermapin(pps, npages,
		    mapinflags | UVMPAGER_MAPIN_WAITOK);

	/*
	 * ok, mapped in.  our pages are PG_BUSY so they are not going to
	 * get touched (so we can look at "offset" without having to lock
	 * the object).  set up for I/O.
	 */
	/* fill out uio/iov */
	iov.iov_base = (caddr_t) kva;
	wanted = (size_t)npages << PAGE_SHIFT;
	if (file_offset + wanted > uvn->u_size)
		wanted = uvn->u_size - file_offset;	/* XXX: needed? */
	iov.iov_len = wanted;
	uio.uio_iov = &iov;
	uio.uio_iovcnt = 1;
	uio.uio_offset = file_offset;
	uio.uio_segflg = UIO_SYSSPACE;
	uio.uio_rw = rw;
	uio.uio_resid = wanted;
	uio.uio_procp = curproc;

	/* do the I/O!  (XXX: curproc?) */
	/*
	 * This process may already have this vnode locked, if we faulted in
	 * copyin() or copyout() on a region backed by this vnode
	 * while doing I/O to the vnode.  If this is the case, don't
	 * panic.. instead, return the error to the user.
	 *
	 * XXX this is a stopgap to prevent a panic.
	 * Ideally, this kind of operation *should* work.
	 */
	result = 0;
	if ((uvn->u_flags & UVM_VNODE_VNISLOCKED) == 0)
		result = vn_lock(vn, LK_EXCLUSIVE | LK_RECURSEFAIL, curproc);

	if (result == 0) {
		int netlocked = (rw_status(&netlock) == RW_WRITE);

		/*
		 * This process may already have the NET_LOCK(), if we
		 * faulted in copyin() or copyout() in the network stack.
		 */
		if (netlocked)
			NET_UNLOCK();

		/* NOTE: vnode now locked! */
		if (rw == UIO_READ)
			result = VOP_READ(vn, &uio, 0, curproc->p_ucred);
		else
			result = VOP_WRITE(vn, &uio,
			    (flags & PGO_PDFREECLUST) ? IO_NOCACHE : 0,
			    curproc->p_ucred);

		if (netlocked)
			NET_LOCK();

		if ((uvn->u_flags & UVM_VNODE_VNISLOCKED) == 0)
			VOP_UNLOCK(vn, curproc);
	}

	/* NOTE: vnode now unlocked (unless vnislocked) */
	/*
	 * result == unix style errno (0 == OK!)
	 *
	 * zero out rest of buffer (if needed)
	 */
	if (result == 0) {
		got = wanted - uio.uio_resid;

		if (wanted && got == 0) {
			result = EIO;		/* XXX: error? */
		} else if (got < PAGE_SIZE * npages && rw == UIO_READ) {
			memset((void *) (kva + got), 0,
			       ((size_t)npages << PAGE_SHIFT) - got);
		}
	}

	/* now remove pager mapping */
	uvm_pagermapout(kva, npages);

	/* now clean up the object (i.e. drop I/O count) */
	uvn->u_nio--;			/* I/O DONE! */
	if ((uvn->u_flags & UVM_VNODE_IOSYNC) != 0 && uvn->u_nio == 0) {
		wakeup(&uvn->u_nio);
	}

	if (result == 0)
		return(VM_PAGER_OK);
	else
		return(VM_PAGER_ERROR);
}

/*
 * uvm_vnp_uncache: disable "persisting" in a vnode... when last reference
 * is gone we will kill the object (flushing dirty pages back to the vnode
 * if needed).
 *
 * => returns TRUE if there was no uvm_object attached or if there was
 *	one and we killed it [i.e. if there is no active uvn]
 * => called with the vnode VOP_LOCK'd [we will unlock it for I/O, if
 *	needed]
 *
 * => XXX: given that we now kill uvn's when a vnode is recycled (without
 *	having to hold a reference on the vnode) and given a working
 *	uvm_vnp_sync(), how does that effect the need for this function?
 *      [XXXCDC: seems like it can die?]
 *
 * => XXX: this function should DIE once we merge the VM and buffer
 *	cache.
 *
 * research shows that this is called in the following places:
 * ext2fs_truncate, ffs_truncate, detrunc[msdosfs]: called when vnode
 *	changes sizes
 * ext2fs_write, WRITE [ufs_readwrite], msdosfs_write: called when we
 *	are written to
 * ex2fs_chmod, ufs_chmod: called if VTEXT vnode and the sticky bit
 *	is off
 * ffs_realloccg: when we can't extend the current block and have
 *	to allocate a new one we call this [XXX: why?]
 * nfsrv_rename, rename_files: called when the target filename is there
 *	and we want to remove it
 * nfsrv_remove, sys_unlink: called on file we are removing
 * nfsrv_access: if VTEXT and we want WRITE access and we don't uncache
 *	then return "text busy"
 * nfs_open: seems to uncache any file opened with nfs
 * vn_writechk: if VTEXT vnode and can't uncache return "text busy"
 */

int
uvm_vnp_uncache(struct vnode *vp)
{
	struct uvm_vnode *uvn = vp->v_uvm;

	/* lock uvn part of the vnode and check if we need to do anything */

	if ((uvn->u_flags & UVM_VNODE_VALID) == 0 ||
			(uvn->u_flags & UVM_VNODE_BLOCKED) != 0) {
		return(TRUE);
	}

	/*
	 * we have a valid, non-blocked uvn.   clear persist flag.
	 * if uvn is currently active we can return now.
	 */
	uvn->u_flags &= ~UVM_VNODE_CANPERSIST;
	if (uvn->u_obj.uo_refs) {
		return(FALSE);
	}

	/*
	 * uvn is currently persisting!   we have to gain a reference to
	 * it so that we can call uvn_detach to kill the uvn.
	 */
	vref(vp);			/* seems ok, even with VOP_LOCK */
	uvn->u_obj.uo_refs++;		/* value is now 1 */

#ifdef VFSLCKDEBUG
	/*
	 * carry over sanity check from old vnode pager: the vnode should
	 * be VOP_LOCK'd, and we confirm it here.
	 */
	if ((vp->v_flag & VLOCKSWORK) && !VOP_ISLOCKED(vp))
		panic("uvm_vnp_uncache: vnode not locked!");
#endif

	/*
	 * now drop our reference to the vnode.   if we have the sole
	 * reference to the vnode then this will cause it to die [as we
	 * just cleared the persist flag].   we have to unlock the vnode
	 * while we are doing this as it may trigger I/O.
	 *
	 * XXX: it might be possible for uvn to get reclaimed while we are
	 * unlocked causing us to return TRUE when we should not.   we ignore
	 * this as a false-positive return value doesn't hurt us.
	 */
	VOP_UNLOCK(vp, curproc);
	uvn_detach(&uvn->u_obj);
	vn_lock(vp, LK_EXCLUSIVE | LK_RETRY, curproc);

	return(TRUE);
}

/*
 * uvm_vnp_setsize: grow or shrink a vnode uvn
 *
 * grow   => just update size value
 * shrink => toss un-needed pages
 *
 * => we assume that the caller has a reference of some sort to the
 *	vnode in question so that it will not be yanked out from under
 *	us.
 *
 * called from:
 *  => truncate fns (ext2fs_truncate, ffs_truncate, detrunc[msdos])
 *  => "write" fns (ext2fs_write, WRITE [ufs/ufs], msdosfs_write, nfs_write)
 *  => ffs_balloc [XXX: why? doesn't WRITE handle?]
 *  => NFS: nfs_loadattrcache, nfs_getattrcache, nfs_setattr
 *  => union fs: union_newsize
 */

void
uvm_vnp_setsize(struct vnode *vp, off_t newsize)
{
	struct uvm_vnode *uvn = vp->v_uvm;

	/* lock uvn and check for valid object, and if valid: do it! */
	if (uvn->u_flags & UVM_VNODE_VALID) {

		/*
		 * now check if the size has changed: if we shrink we had better
		 * toss some pages...
		 */

		if (uvn->u_size > newsize) {
			(void)uvn_flush(&uvn->u_obj, newsize,
			    uvn->u_size, PGO_FREE);
		}
		uvn->u_size = newsize;
	}
}

/*
 * uvm_vnp_sync: flush all dirty VM pages back to their backing vnodes.
 *
 * => called from sys_sync with no VM structures locked
 * => only one process can do a sync at a time (because the uvn
 *    structure only has one queue for sync'ing).  we ensure this
 *    by holding the uvn_sync_lock while the sync is in progress.
 *    other processes attempting a sync will sleep on this lock
 *    until we are done.
 */
void
uvm_vnp_sync(struct mount *mp)
{
	struct uvm_vnode *uvn;
	struct vnode *vp;

	/*
	 * step 1: ensure we are only ones using the uvn_sync_q by locking
	 * our lock...
	 */
	rw_enter_write(&uvn_sync_lock);

	/*
	 * step 2: build up a simpleq of uvns of interest based on the
	 * write list.   we gain a reference to uvns of interest. 
	 */
	SIMPLEQ_INIT(&uvn_sync_q);
	LIST_FOREACH(uvn, &uvn_wlist, u_wlist) {
		vp = uvn->u_vnode;
		if (mp && vp->v_mount != mp)
			continue;

		/*
		 * If the vnode is "blocked" it means it must be dying, which
		 * in turn means its in the process of being flushed out so
		 * we can safely skip it.
		 *
		 * note that uvn must already be valid because we found it on
		 * the wlist (this also means it can't be ALOCK'd).
		 */
		if ((uvn->u_flags & UVM_VNODE_BLOCKED) != 0)
			continue;

		/*
		 * gain reference.   watch out for persisting uvns (need to
		 * regain vnode REF).
		 */
		if (uvn->u_obj.uo_refs == 0)
			vref(vp);
		uvn->u_obj.uo_refs++;

		SIMPLEQ_INSERT_HEAD(&uvn_sync_q, uvn, u_syncq);
	}

	/* step 3: we now have a list of uvn's that may need cleaning. */
	SIMPLEQ_FOREACH(uvn, &uvn_sync_q, u_syncq) {
#ifdef DEBUG
		if (uvn->u_flags & UVM_VNODE_DYING) {
			printf("uvm_vnp_sync: dying vnode on sync list\n");
		}
#endif
		uvn_flush(&uvn->u_obj, 0, 0, PGO_CLEANIT|PGO_ALLPAGES|PGO_DOACTCLUST);

		/*
		 * if we have the only reference and we just cleaned the uvn,
		 * then we can pull it out of the UVM_VNODE_WRITEABLE state
		 * thus allowing us to avoid thinking about flushing it again
		 * on later sync ops.
		 */
		if (uvn->u_obj.uo_refs == 1 &&
		    (uvn->u_flags & UVM_VNODE_WRITEABLE)) {
			LIST_REMOVE(uvn, u_wlist);
			uvn->u_flags &= ~UVM_VNODE_WRITEABLE;
		}

		/* now drop our reference to the uvn */
		uvn_detach(&uvn->u_obj);
	}

	rw_exit_write(&uvn_sync_lock);
}
@


1.97
log
@Enable the NET_LOCK(), take 3.

Recursions are still marked as XXXSMP.

ok deraadt@@, bluhm@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.96 2017/05/03 02:43:15 guenther Exp $	*/
d1186 1
a1186 1
			rw_exit_write(&netlock);
d1197 1
a1197 1
			rw_enter_write(&netlock);
@


1.96
log
@Mark uvm_sync_lock as vnode'ish for witness purposes, as it is taken
between mount locks and inode locks, which may been recorded in either order

ok visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.95 2017/03/17 17:19:16 mpi Exp $	*/
d1179 9
d1195 3
@


1.95
log
@Revert the NET_LOCK() and bring back pf's contention lock for release.

For the moment the NET_LOCK() is always taken by threads running under
KERNEL_LOCK().  That means it doesn't buy us anything except a possible
deadlock that we did not spot.  So make sure this doesn't happen, we'll
have plenty of time in the next release cycle to stress test it.

ok visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.94 2017/01/25 06:15:51 mpi Exp $	*/
d120 1
a120 1
	rw_init(&uvn_sync_lock, "uvnsync");
@


1.94
log
@Enable the NET_LOCK(), take 2.

Recursions are currently known and marked a XXXSMP.

Please report any assert to bugs@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.93 2016/09/16 02:35:42 dlg Exp $	*/
a1178 9
		int netlocked = (rw_status(&netlock) == RW_WRITE);

		/*
		 * This process may already have the NET_LOCK(), if we
		 * faulted in copyin() or copyout() in the network stack.
		 */
		if (netlocked)
			rw_exit_write(&netlock);

a1185 3

		if (netlocked)
			rw_enter_write(&netlock);
@


1.93
log
@move the vm_page struct from being stored in RB macro trees to RBT functions

vm_page structs go into three trees, uvm_objtree, uvm_pmr_addr, and
uvm_pmr_size. all these have been moved to RBT code.

this should give us a decent chunk of code space back.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.92 2016/03/19 12:04:16 natano Exp $	*/
d1179 9
d1195 3
@


1.92
log
@Remove the unused flags argument from VOP_UNLOCK().

torture tested on amd64, i386 and macppc
ok beck mpi stefan
"the change looks right" deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.91 2015/08/27 18:59:58 deraadt Exp $	*/
d365 1
a365 1
	KASSERT(RB_EMPTY(&uobj->memt));
d465 1
a465 1
		RB_FOREACH(pp, uvm_objtree, &uvn->u_obj.memt) {
@


1.91
log
@delete a comment about gcc -Wuninitialized
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.90 2015/05/07 01:55:44 jsg Exp $	*/
d1188 1
a1188 1
			VOP_UNLOCK(vn, 0, curproc);
d1306 1
a1306 1
	VOP_UNLOCK(vp, 0, curproc);
@


1.90
log
@fix indentation
ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.89 2015/03/14 03:38:53 jsg Exp $	*/
d143 1
a143 3
	u_quad_t used_vnode_size;

	used_vnode_size = (u_quad_t)0;	/* XXX gcc -Wuninitialized */
@


1.89
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.88 2014/12/18 23:59:28 tedu Exp $	*/
d923 1
a923 1
				done = FALSE;	/* need to do a wait or I/O! */
@


1.88
log
@remove two useless and unused hash penalty defines
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.87 2014/12/17 19:42:15 tedu Exp $	*/
a54 1
#include <sys/ioctl.h>
@


1.87
log
@remove lock.h from uvm_extern.h. another holdover from the simpletonlock
era. fix uvm including c files to include lock.h or atomic.h as necessary.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.86 2014/12/16 18:30:04 tedu Exp $	*/
a579 2
#define UVN_HASH_PENALTY 4	/* XXX: a guess */

@


1.86
log
@primary change: move uvm_vnode out of vnode, keeping only a pointer.
objective: vnode.h doesn't include uvm_extern.h anymore.
followup changes: include uvm_extern.h or lock.h where necessary.
ok and help from deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.85 2014/11/16 12:31:01 deraadt Exp $	*/
d53 1
@


1.85
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.84 2014/07/11 16:35:40 jsg Exp $	*/
d139 1
a139 1
	struct uvm_vnode *uvn = &vp->v_uvm;
d309 1
a309 1
	vp = (struct vnode *) uobj;
d409 1
a409 1
	struct uvm_vnode *uvn = &vp->v_uvm;
d1113 1
a1113 1
	vn = (struct vnode *) uvn;
d1263 1
a1263 1
boolean_t
d1266 1
a1266 1
	struct uvm_vnode *uvn = &vp->v_uvm;
d1336 1
a1336 1
uvm_vnp_setsize(struct vnode *vp, voff_t newsize)
d1338 1
a1338 1
	struct uvm_vnode *uvn = &vp->v_uvm;
d1384 1
a1384 1
		vp = (struct vnode *)uvn;
@


1.84
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.83 2014/07/02 06:09:49 matthew Exp $	*/
d173 1
a173 1
		if ((accessprot & VM_PROT_WRITE) != 0 &&
d239 1
a239 1
	if (accessprot & VM_PROT_WRITE) {
d651 1
a651 1
				pmap_page_protect(pp, VM_PROT_NONE);
d664 1
a664 1
					pmap_page_protect(pp, VM_PROT_NONE);
d677 1
a677 1
					pmap_page_protect(pp, VM_PROT_NONE);
d695 1
a695 1
		pmap_page_protect(pp, VM_PROT_READ);
d789 1
a789 1
					pmap_page_protect(ptmp, VM_PROT_NONE);
d805 1
a805 1
				pmap_page_protect(ptmp, VM_PROT_NONE);
@


1.83
log
@Use real parameter types for u{dv,vn}_attach() instead of void *

ok guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.82 2014/05/08 20:08:50 kettenis Exp $	*/
d24 1
a24 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed by Charles D. Cranor,
 *	Washington University, the University of California, Berkeley and
 *	its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.82
log
@Fix some potential integer overflows caused by converting a page number into
an offset/size/address by shifting by PAGE_SHIFT.  Make uvm_objwrire/unwire
use voff_t instead of off_t.  The former is the right type here even if it is
equivalent to the latter.

Inspired by a somewhat similar changes in Bitrig.

ok deraadt@@, guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.81 2014/04/13 23:14:15 tedu Exp $	*/
d142 1
a142 1
uvn_attach(void *arg, vm_prot_t accessprot)
a143 1
	struct vnode *vp = arg;
@


1.81
log
@compress code by turning four line comments into one line comments.
emphatic ok usual suspects, grudging ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.80 2014/04/08 18:48:41 beck Exp $	*/
d1162 1
a1162 1
	wanted = npages << PAGE_SHIFT;
d1214 1
a1214 1
			       (npages << PAGE_SHIFT) - got);
@


1.80
log
@add IO_NOCACHE flag to vop_write arguments, which in turn sets B_NOCACHE
on the written buffers. Use the flag for writes from the page daemon to
ensure that we free buffers written out by the page daemon rather than
caching them.
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.79 2013/05/30 16:39:26 tedu Exp $	*/
a85 1

a98 1

a113 1

a118 1

a140 1

d153 1
a153 3
	/*
	 * first get a lock on the uvn.
	 */
d159 1
a159 3
	/*
	 * if we're mapping a BLK device, make sure it is a disk.
	 */
d237 1
a237 3
	/*
	 * now set up the uvn.
	 */
d313 1
a313 4
	/*
	 * get other pointers ...
	 */

a326 1

d334 1
a334 4
	/*
	 * its a goner!
	 */

a345 1

a355 1

d381 1
a381 3
	/*
	 * drop our reference to the vnode.
	 */
a411 1

d418 1
a418 3
	/*
	 * check if it is valid
	 */
a428 1

a446 1

a469 1

a494 1

a496 1

a500 1

a502 1

a503 1

a556 1

a584 1

d597 1
a597 4
	/*
	 * get init vals and determine how we are going to traverse object
	 */

a617 1

a632 1

a642 1

d666 1
a666 3
		/*
		 * if we don't need a clean... deactivate/free pages then cont.
		 */
a698 1

a726 1

a752 1

a764 1

a766 1

a778 1

d792 1
a792 4
			/*
			 * dispose of page
			 */

d819 1
a819 3
	/*
	 * done with pagequeues: unlock
	 */
d822 1
a822 3
	/*
	 * now wait for all I/O if required.
	 */
a872 1

a882 1

a891 1

d901 1
a901 4
	/*
	 * step 1: handled the case where fault data structures are locked.
	 */

a902 1

a906 1

a913 1

d944 1
a944 1
		}	/* "for" lcv loop */
a977 1

a1001 1

a1002 1

a1007 1

a1048 1

a1055 1

a1062 1

d1092 1
a1092 1
	}	/* lcv loop */
d1117 1
a1117 4
	/*
	 * init values
	 */

d1122 1
a1122 4
	/*
	 * check for sync'ing I/O.
	 */

d1131 1
a1131 4
	/*
	 * check size
	 */

d1136 1
a1136 4
	/*
	 * first try and map the pages in (without waiting)
	 */

a1149 1

d1160 1
a1160 5

	/*
	 * fill out uio/iov
	 */

d1174 1
a1174 4
	/*
	 * do the I/O!  (XXX: curproc?)
	 */

a1201 1

a1206 1

d1218 1
a1218 3
	/*
	 * now remove pager mapping
	 */
d1221 1
a1221 4
	/*
	 * now clean up the object (i.e. drop I/O count)
	 */

a1226 3
	/*
	 * done!
	 */
d1274 1
a1274 3
	/*
	 * lock uvn part of the vnode and check to see if we need to do anything
	 */
a1284 1

a1293 1

a1319 4
	/*
	 * and return...
	 */

d1346 1
a1346 3
	/*
	 * lock uvn and check for valid object, and if valid: do it!
	 */
a1359 5

	/*
	 * done
	 */
	return;
a1389 1

a1403 1

@


1.79
log
@UVM_UNLOCK_AND_WAIT no longer unlocks, so rename it to UVM_WAIT.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.78 2013/05/30 16:29:46 tedu Exp $	*/
d1285 3
a1287 1
			result = VOP_WRITE(vn, &uio, 0, curproc->p_ucred);
@


1.78
log
@remove lots of comments about locking per beck's request
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.77 2013/05/30 15:17:59 tedu Exp $	*/
d163 1
a163 2
		UVM_UNLOCK_AND_WAIT(uvn, &uvn->u_obj.vmobjlock, FALSE,
		    "uvn_attach", 0);
d380 1
a380 2
		UVM_UNLOCK_AND_WAIT(&uvn->u_nio, &uvn->u_obj.vmobjlock, FALSE,
		    "uvn_term",0);
d514 1
a514 2
		UVM_UNLOCK_AND_WAIT(&uvn->u_nio, &uvn->u_obj.vmobjlock, FALSE,
		    "uvn_term",0);
d721 1
a721 2
					UVM_UNLOCK_AND_WAIT(pp,
					    &uobj->vmobjlock, 0, "uvn_flsh", 0);
d882 1
a882 2
			UVM_UNLOCK_AND_WAIT(&uvn->u_nio, &uvn->u_obj.vmobjlock,
			  FALSE, "uvn_flush",0);
d1098 1
a1098 2
				UVM_UNLOCK_AND_WAIT(ptmp,
				    &uobj->vmobjlock, FALSE, "uvn_get",0);
d1206 1
a1206 2
		UVM_UNLOCK_AND_WAIT(&uvn->u_flags, &uvn->u_obj.vmobjlock,
			FALSE, "uvn_iosync",0);
@


1.77
log
@remove simple_locks from uvm code. ok beck deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.76 2011/07/04 20:35:35 deraadt Exp $	*/
a140 1
 * => caller must _not_ already be holding the lock on the uvm_object.
d175 1
a175 1
	 * now we have lock and uvn must not be in a blocked state.
a195 1
		/* unlock and return */
a230 2
	/* relock object */

d280 1
a280 1
 * there is no chance of the uvn being killed or locked out here.
a281 1
 * => caller must call with object unlocked.
d308 1
a308 1
 * => caller must call with object unlocked and map locked.
d444 1
a444 1
	 * lock object and check if it is valid
d481 1
a481 2
	 * pages.  (note that flush may unlock object while doing I/O, but
	 * it will re-lock it before it returns control here).
d554 1
a554 3
		wakeup(uvn);		/* object lock still held */


a585 1
 * - "try" to lock object.   if fail, just return (will try again later)
a596 3
 * => object should be locked by caller.   we may _unlock_ the object
 *	if (and only if) we need to clean a page (PGO_CLEANIT).
 *	we return with the object locked.
d600 1
a600 2
 * => if PGO_CLEANIT is not set, then we will neither unlock the object
 *	or block.
d670 2
a671 2
	uvm_lock_pageq();	/* page queues locked */
	/* locked: both page queues and uobj */
d739 1
a739 1
		 * pp points to a page in the locked object that we are
d744 1
a744 1
		 * note: locked: uobj and page queues.
a756 1
		/* locked: page queues, uobj */
a758 1
		/* unlocked: page queues, uobj */
a766 2

		/* relock! */
d779 2
a780 2
			 * while we had the object lock dropped.   we ignore
			 * this now and retry the I/O.  we will detect and
d820 1
a820 2
			 * verify the page didn't get moved while obj was
			 * unlocked
d828 2
a829 2
			 * finished before we relocked the object (in
			 * which case the page is no longer busy).
a833 1
					/* still holding object lock */
a893 1
	/* return, with object locked! */
a902 2
 *
 * - currently doesn't matter if obj locked or not.
a928 1
 * => object must be locked!   we will _unlock_ it before starting I/O.
a939 1
	/* note: object locked */
a940 1
	/* note: object unlocked */
a949 1
 * => object must be locked!  we will _unlock_ it before starting any I/O.
d1006 1
a1006 1
			 * useful page: busy/lock it and plug it in our
a1036 1
			/* EEK!   Need to unlock and I/O */
d1042 1
a1042 1
	 * object is locked.   data structures are unlocked.
d1130 1
a1130 2
		 * I/O to fill it with valid data.  note that object must be
		 * locked going into uvn_io, but will be unlocked afterwards.
d1137 2
a1138 3
		 * I/O done.   object is unlocked (by uvn_io).   because we used
		 * syncio the result can not be PEND or AGAIN.   we must relock
		 * and check for errors.
a1140 1
		/* lock object.   check for errors.   */
a1142 1
				/* object lock still held */
a1171 4
	/*
	 * finally, unlock object and return.
	 */

a1178 1
 * => object must be locked!   we will _unlock_ it before starting I/O.
a1242 1
	/* NOTE: object now unlocked */
a1288 1

a1325 2
	/* NOTE: object now locked! */

a1329 1
	/* NOTE: object now unlocked! */
a1333 1

@


1.76
log
@move the specfs code to a place people can see it; ok guenther thib krw
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.75 2011/07/04 00:15:47 oga Exp $	*/
a161 1
	simple_lock(&uvn->u_obj.vmobjlock);
a165 1
		simple_lock(&uvn->u_obj.vmobjlock);
a171 1
		simple_unlock(&uvn->u_obj.vmobjlock); /* drop lock */
a197 1
		simple_unlock(&uvn->u_obj.vmobjlock);
a209 2
	simple_unlock(&uvn->u_obj.vmobjlock); /* drop lock in case we sleep */
		/* XXX: curproc? */
a233 1
	simple_lock(&uvn->u_obj.vmobjlock);
a238 1
		simple_unlock(&uvn->u_obj.vmobjlock); /* drop lock */
a271 1
	simple_unlock(&uvn->u_obj.vmobjlock);
a297 1
	simple_lock(&uobj->vmobjlock);
a305 1
	simple_unlock(&uobj->vmobjlock);
a323 1
	simple_lock(&uobj->vmobjlock);
a326 1
		simple_unlock(&uobj->vmobjlock);
a350 1
		simple_unlock(&uobj->vmobjlock);
a387 1
		simple_lock(&uvn->u_obj.vmobjlock);
a402 1
	simple_unlock(&uobj->vmobjlock);
a450 1
	simple_lock(&uvn->u_obj.vmobjlock);
a451 1
		simple_unlock(&uvn->u_obj.vmobjlock);
a523 1
		simple_lock(&uvn->u_obj.vmobjlock);
a561 1
	simple_unlock(&uvn->u_obj.vmobjlock);
a738 1
					simple_lock(&uobj->vmobjlock);
a783 1
		simple_lock(&uobj->vmobjlock);
a906 1
			simple_lock(&uvn->u_obj.vmobjlock);
a1113 1
					simple_unlock(&uobj->vmobjlock);
a1114 1
					simple_lock(&uobj->vmobjlock);
a1131 1
				simple_lock(&uobj->vmobjlock);
a1170 1
		simple_lock(&uobj->vmobjlock);
a1181 1
			simple_unlock(&uobj->vmobjlock);
a1206 1
	simple_unlock(&uobj->vmobjlock);
a1244 1
			simple_unlock(&uvn->u_obj.vmobjlock);
a1249 1
		simple_lock(&uvn->u_obj.vmobjlock);
a1256 1
		simple_unlock(&uvn->u_obj.vmobjlock);
a1268 1
		simple_unlock(&uvn->u_obj.vmobjlock);
a1278 1
	simple_unlock(&uvn->u_obj.vmobjlock);
a1363 1
	simple_lock(&uvn->u_obj.vmobjlock);
a1369 1
	simple_unlock(&uvn->u_obj.vmobjlock);
a1426 1
	simple_lock(&uvn->u_obj.vmobjlock);
a1428 1
		simple_unlock(&uvn->u_obj.vmobjlock);
a1438 1
		simple_unlock(&uvn->u_obj.vmobjlock);
a1448 1
	simple_unlock(&uvn->u_obj.vmobjlock);
a1505 1
	simple_lock(&uvn->u_obj.vmobjlock);
a1518 1
	simple_unlock(&uvn->u_obj.vmobjlock);
@


1.75
log
@Replace inadvertantly removed line.

now we can free vnodes again.

ok gcc@@, jetpack@@, beck@@, art@@.

(the results of this were hilarious)
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.74 2011/07/03 18:34:14 oga Exp $	*/
d64 1
a64 2

#include <miscfs/specfs/specdev.h>
@


1.74
log
@Rip out and burn support for UVM_HIST.

The vm hackers don't use it, don't maintain it and have to look at it all the
time. About time this 800 lines of code hit /dev/null.

``never liked it'' tedu@@. ariane@@ was very happy when i told her i wrote
this diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.73 2011/07/02 15:52:25 thib Exp $	*/
d338 1
@


1.73
log
@rename VFSDEBUG to VFLCKDEBUG;

prompted by tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.72 2011/06/16 23:00:38 oga Exp $	*/
a156 3
	UVMHIST_FUNC("uvn_attach"); UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist, "(vn=%p)", arg,0,0,0);
a165 1
		UVMHIST_LOG(maphist, "  SLEEPING on blocked vn",0,0,0,0);
a168 1
		UVMHIST_LOG(maphist,"  WOKE UP",0,0,0,0);
a175 1
		UVMHIST_LOG(maphist,"<- done (VBLK not D_DISK!)", 0,0,0,0);
a189 2
			UVMHIST_LOG(maphist," vref (reclaim persisting vnode)",
			    0,0,0,0);
a202 2
		UVMHIST_LOG(maphist,"<- done, refcnt=%ld", uvn->u_obj.uo_refs,
		    0, 0, 0);
a247 1
		UVMHIST_LOG(maphist,"<- done (VOP_GETATTR FAILED!)", 0,0,0,0);
a284 1
	UVMHIST_LOG(maphist,"<- done/vref, ret %p", &uvn->u_obj,0,0,0);
a306 1
	UVMHIST_FUNC("uvn_reference"); UVMHIST_CALLED(maphist);
a316 2
	UVMHIST_LOG(maphist, "<- done (uobj=%p, ref = %ld)",
	    uobj, uobj->uo_refs,0,0);
a334 1
	UVMHIST_FUNC("uvn_detach"); UVMHIST_CALLED(maphist);
a337 2
	UVMHIST_LOG(maphist,"  (uobj=%p)  ref=%ld", uobj,uobj->uo_refs,0,0);
	uobj->uo_refs--;			/* drop ref! */
a339 1
		UVMHIST_LOG(maphist, "<- done (rc>0)", 0,0,0,0);
a365 1
		UVMHIST_LOG(maphist,"<- done/vrele!  (persist)", 0,0,0,0);
a372 2
	UVMHIST_LOG(maphist,"  its a goner (flushing)!", 0,0,0,0);

a386 2
	UVMHIST_LOG(maphist,"  its a goner (done flush)!", 0,0,0,0);

a427 1
	UVMHIST_LOG(maphist,"<- done (vrele) final", 0,0,0,0);
a462 1
	UVMHIST_FUNC("uvm_vnp_terminate"); UVMHIST_CALLED(maphist);
a467 2
	UVMHIST_LOG(maphist, "  vp=%p, ref=%ld, flag=0x%lx", vp,
	    uvn->u_obj.uo_refs, uvn->u_flags, 0);
a469 1
		UVMHIST_LOG(maphist, "<- done (not active)", 0, 0, 0, 0);
a581 1
	UVMHIST_LOG(maphist, "<- done", 0, 0, 0, 0);
a666 1
	UVMHIST_FUNC("uvn_flush"); UVMHIST_CALLED(maphist);
a681 4
	UVMHIST_LOG(maphist,
	    " flush start=0x%lx, stop=0x%lx, flags=0x%lx",
	    (u_long)start, (u_long)stop, flags, 0);

a924 2

		UVMHIST_LOG(maphist,"  <<DOING IOSYNC>>",0,0,0,0);
a936 1
	UVMHIST_LOG(maphist,"<- done (retval=0x%lx)",retval,0,0,0);
a1012 2
	UVMHIST_FUNC("uvn_get"); UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist, "flags=%ld", flags,0,0,0);
a1258 3
	UVMHIST_FUNC("uvn_io"); UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist, "rw=%ld", rw,0,0,0);
a1274 1
			UVMHIST_LOG(maphist,"<- try again (iosync)",0,0,0,0);
d1288 2
a1289 3
			simple_unlock(&uvn->u_obj.vmobjlock);
			UVMHIST_LOG(maphist,"<- BAD (size check)",0,0,0,0);
			return(VM_PAGER_BAD);
a1301 1
		UVMHIST_LOG(maphist,"<- mapin failed (try again)",0,0,0,0);
a1344 2
	UVMHIST_LOG(maphist, "calling VOP",0,0,0,0);

a1371 2
	UVMHIST_LOG(maphist, "done calling VOP",0,0,0,0);

a1411 1
	UVMHIST_LOG(maphist, "<- done (result %ld)", result,0,0,0);
@


1.72
log
@Use the current page not he first page when working out whether to
deactivate pages after syncing.

While here, don't check flags for PQ_INACTIVE (this is the only place
outside uvm_page.c where this is done) because pagedeactivate does this
already.

First part from Christian Ehrhart, second from me.

Both ok ariane@@.

I meant to commit this about a week ago, but accidentally commited to my
local cvs mirror then forgot about it.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.71 2010/05/18 04:41:14 dlg Exp $	*/
d1541 1
a1541 1
#ifdef VFSDEBUG
d1548 1
a1548 1
#endif	/* VFSDEBUG */
@


1.71
log
@dont let sys/ioctl.h imply that you get the ioctls in dkio.h. this
gets rid of #include <sys/dkio.h> in sys/ioctl.h and adds #include
<sys/dkio.h> to the places that actually want and use the disk
ioctls.

this became an issue when krw@@'s X build failed when he was testing
a change to dkio.h.
tested by krw@@
help from and ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.70 2010/04/30 21:56:39 oga Exp $	*/
d783 1
a783 2
				if ((pp->pg_flags & PQ_INACTIVE) == 0 &&
				    pp->wire_count == 0) {
d926 1
a926 2
				if ((pp->pg_flags & PQ_INACTIVE) == 0 &&
				    pp->wire_count == 0) {
@


1.70
log
@Right now, if anything internal changes with a uvm object, diverse
places in the tree need to be touched to update the object
initialisation with respect to that.

So, make a function (uvm_initobj) that takes the refcount, object and
pager ops and does this initialisation for us. This should save on
maintainance in the future.

looked good to fgs@@. Tedu complained about the British spelling but OKed
it anyway.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.69 2009/08/06 15:28:14 oga Exp $	*/
d63 1
@


1.69
log
@reintroduce the uvm_tree commit.

Now instead of the global object hashtable, we have a per object tree.

Testing shows no performance difference and a slight code shrink. OTOH when
locking is more fine grained this should be faster due to lock contention on
uvm.hashlock.

ok thib@@, art@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.68 2009/07/22 21:05:37 oga Exp $	*/
d273 1
a273 4
	uvn->u_obj.pgops = &uvm_vnodeops;
	RB_INIT(&uvn->u_obj.memt);
	uvn->u_obj.uo_npages = 0;
	uvn->u_obj.uo_refs = 1;			/* just us... */
@


1.68
log
@Put the PG_RELEASED changes diff back in.

This has has been tested very very thoroughly on all archs we have
excepting 88k and 68k. Please see cvs log for the individual commit
messages.

ok beck@@, thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.67 2009/07/09 22:29:56 thib Exp $	*/
d274 1
a274 1
	TAILQ_INIT(&uvn->u_obj.memq);
d441 1
a441 5
#ifdef DIAGNOSTIC
	if (!TAILQ_EMPTY(&uobj->memq))
		panic("uvn_deref: vnode VM object still has pages afer "
		    "syncio/free flush");
#endif
d558 1
a558 1
		TAILQ_FOREACH(pp, &uvn->u_obj.memq, listq) {
a667 5
 * => NOTE: we rely on the fact that the object's memq is a TAILQ and
 *	that new pages are inserted on the tail end of the list.   thus,
 *	we can make a complete pass through the object in one go by starting
 *	at the head and working towards the tail (new pages are put in
 *	front of us).
a685 13
 *
 * note on page traversal:
 *	we can traverse the pages in an object either by going down the
 *	linked list in "uobj->memq", or we can go over the address range
 *	by page doing hash table lookups for each address.    depending
 *	on how many pages are in the object it may be cheaper to do one
 *	or the other.   we set "by_list" to true if we are using memq.
 *	if the cost of a hash lookup was equal to the cost of the list
 *	traversal we could compare the number of pages in the start->stop
 *	range to the total number of pages in the object.   however, it
 *	seems that a hash table lookup is more expensive than the linked
 *	list traversal, so we multiply the number of pages in the
 *	start->stop range by a penalty which we define below.
@


1.67
log
@Remove the VREF() macro and replaces all instances with a call to verf(),
which is exactly what the macro does.

Macro's that are nothing more then:
#define FUNCTION(arg) function(arg)
are almost always pointless and should go away.

OK blambert@@
Agreed by many.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.57 2009/04/14 20:13:14 oga Exp $	*/
a95 1
boolean_t	 uvn_releasepg(struct vm_page *, struct vm_page **);
a110 1
	uvn_releasepg,
d416 4
a419 7
	 * have either been free'd or they are marked PG_BUSY|PG_RELEASED.
	 * the PG_BUSY bit was set either by us or the daemon for async I/O.
	 * in either case, if we have pages left we can't kill the object
	 * yet because i/o is pending.  in this case we set the "relkill"
	 * flag which will cause pgo_releasepg to kill the object once all
	 * the I/O's are done [pgo_releasepg will be called from the aiodone
	 * routine or from the page daemon].
d422 10
a431 13
	if (uobj->uo_npages) {		/* I/O pending.  iodone will free */
#ifdef DEBUG
		/*
		 * XXXCDC: very unlikely to happen until we have async i/o
		 * so print a little info message in case it does.
		 */
		printf("uvn_detach: vn %p has pages left after flush - "
		    "relkill mode\n", uobj);
#endif
		uvn->u_flags |= UVM_VNODE_RELKILL;
		simple_unlock(&uobj->vmobjlock);
		UVMHIST_LOG(maphist,"<- done! (releasepg will kill obj)", 0, 0,
		    0, 0);
a432 1
	}
a483 2
 * => unlike uvn_detach, this function must not return until all the
 *	uvn's pages are disposed of.
d527 2
a528 2
	 * state [i.e. waiting for async i/o to finish so that releasepg can
	 * kill object].  we take over the vnode now and cancel the relkill.
a621 66
 * uvn_releasepg: handled a released page in a uvn
 *
 * => "pg" is a PG_BUSY [caller owns it], PG_RELEASED page that we need
 *	to dispose of.
 * => caller must handled PG_WANTED case
 * => called with page's object locked, pageq's unlocked
 * => returns TRUE if page's object is still alive, FALSE if we
 *	killed the page's object.    if we return TRUE, then we
 *	return with the object locked.
 * => if (nextpgp != NULL) => we return pageq.tqe_next here, and return
 *				with the page queues locked [for pagedaemon]
 * => if (nextpgp == NULL) => we return with page queues unlocked [normal case]
 * => we kill the uvn if it is not referenced and we are suppose to
 *	kill it ("relkill").
 */

boolean_t
uvn_releasepg(struct vm_page *pg, struct vm_page **nextpgp /* OUT */)
{
	struct uvm_vnode *uvn = (struct uvm_vnode *) pg->uobject;
	struct vnode *vp = (struct vnode *)uvn;
#ifdef DIAGNOSTIC
	if ((pg->pg_flags & PG_RELEASED) == 0)
		panic("uvn_releasepg: page not released!");
#endif

	/*
	 * dispose of the page [caller handles PG_WANTED]
	 */
	pmap_page_protect(pg, VM_PROT_NONE);
	uvm_lock_pageq();
	if (nextpgp)
		*nextpgp = TAILQ_NEXT(pg, pageq); /* next page for daemon */
	uvm_pagefree(pg);
	if (!nextpgp)
		uvm_unlock_pageq();

	/*
	 * now see if we need to kill the object
	 */
	if (uvn->u_flags & UVM_VNODE_RELKILL) {
		if (uvn->u_obj.uo_refs)
			panic("uvn_releasepg: kill flag set on referenced "
			    "object!");
		if (uvn->u_obj.uo_npages == 0) {
			if (uvn->u_flags & UVM_VNODE_WRITEABLE) {
				LIST_REMOVE(uvn, u_wlist);
			}
#ifdef DIAGNOSTIC
			if (!TAILQ_EMPTY(&uvn->u_obj.memq))
	panic("uvn_releasepg: pages in object with npages == 0");
#endif
			if (uvn->u_flags & UVM_VNODE_WANTED)
				/* still holding object lock */
				wakeup(uvn);

			uvn->u_flags = 0;		/* DEAD! */
			simple_unlock(&uvn->u_obj.vmobjlock);
			vrele(vp);
			return (FALSE);
		}
	}
	return (TRUE);
}

/*
a654 2
 * - handle "released" pages [using pgo_releasepg]
 *   >>> pgo_releasepg may kill the object
d716 1
a716 1
	struct vm_page *pp, *ppnext, *ptmp;
d719 1
a719 1
	boolean_t retval, need_iosync, by_list, needs_clean, all;
a720 1
	u_short pp_version;
a722 1
	curoff = 0;	/* XXX: shut up gcc */
d730 2
a731 2
		all = TRUE;
		by_list = TRUE;		/* always go by the list */
d734 1
a734 9
		stop = round_page(stop);
#ifdef DEBUG
		if (stop > round_page(uvn->u_size))
			printf("uvn_flush: strange, got an out of range "
			    "flush (fixed)\n");
#endif
		all = FALSE;
		by_list = (uobj->uo_npages <=
		    ((stop - start) >> PAGE_SHIFT) * UVN_HASH_PENALTY);
d738 2
a739 2
	    " flush start=0x%lx, stop=0x%lx, by_list=%ld, flags=0x%lx",
	    (u_long)start, (u_long)stop, by_list, flags);
d752 4
a755 7
	if ((flags & PGO_CLEANIT) != 0 &&
	    uobj->pgops->pgo_mk_pcluster != NULL) {
		if (by_list) {
			TAILQ_FOREACH(pp, &uobj->memq, listq) {
				if (!all &&
				    (pp->offset < start || pp->offset >= stop))
					continue;
a757 10
			}

		} else {   /* by hash */
			for (curoff = start ; curoff < stop;
			    curoff += PAGE_SIZE) {
				pp = uvm_pagelookup(uobj, curoff);
				if (pp)
					atomic_clearbits_int(&pp->pg_flags,
					    PG_CLEANCHK);
			}
a760 14
	/*
	 * now do it.   note: we must update ppnext in body of loop or we
	 * will get stuck.  we need to use ppnext because we may free "pp"
	 * before doing the next loop.
	 */

	if (by_list) {
		pp = TAILQ_FIRST(&uobj->memq);
	} else {
		curoff = start;
		pp = uvm_pagelookup(uobj, curoff);
	}

	ppnext = NULL;	/* XXX: shut up gcc */
a762 1

d764 3
a766 29
	for ( ; (by_list && pp != NULL) ||
	  (!by_list && curoff < stop) ; pp = ppnext) {

		if (by_list) {

			/*
			 * range check
			 */

			if (!all &&
			    (pp->offset < start || pp->offset >= stop)) {
				ppnext = TAILQ_NEXT(pp, listq);
				continue;
			}

		} else {

			/*
			 * null check
			 */

			curoff += PAGE_SIZE;
			if (pp == NULL) {
				if (curoff < stop)
					ppnext = uvm_pagelookup(uobj, curoff);
				continue;
			}

		}
d803 1
a803 1
		 * if we don't need a clean... load ppnext and dispose of pp
a805 9
			/* load ppnext */
			if (by_list)
				ppnext = TAILQ_NEXT(pp, listq);
			else {
				if (curoff < stop)
					ppnext = uvm_pagelookup(uobj, curoff);
			}

			/* now dispose of pp */
a811 1

a813 1
					/* release busy pages */
d815 8
a822 1
					    PG_RELEASED);
a828 1
			/* ppnext is valid so we can continue... */
d844 3
a846 1
		pp_version = pp->pg_version;
d857 5
a861 5
		 * at this point nothing is locked.   if we did an async I/O
		 * it is remotely possible for the async i/o to complete and
		 * the page "pp" be freed or what not before we get a chance
		 * to relock the object.   in order to detect this, we have
		 * saved the version number of the page in "pp_version".
d870 1
a870 1
		 * can only happen when  we are doing async I/O and can't
d888 4
d903 3
a905 2
		 * for pending async i/o if we are not deactivating/freeing
		 * we can move on to the next page.
d907 2
a908 23

		if (result == VM_PAGER_PEND) {

			if ((flags & (PGO_DEACTIVATE|PGO_FREE)) == 0) {
				/*
				 * no per-page ops: refresh ppnext and continue
				 */
				if (by_list) {
					if (pp->pg_version == pp_version)
						ppnext = TAILQ_NEXT(pp, listq);
					else
						/* reset */
						ppnext = TAILQ_FIRST(&uobj->memq);
				} else {
					if (curoff < stop)
						ppnext = uvm_pagelookup(uobj,
						    curoff);
				}
				continue;
			}

			/* need to do anything here? */
		}
d911 2
a912 5
		 * need to look at each page of the I/O operation.  we defer
		 * processing "pp" until the last trip through this "for" loop
		 * so that we can load "ppnext" for the main loop after we
		 * play with the cluster pages [thus the "npages + 1" in the
		 * loop below].
d915 2
a916 25
		for (lcv = 0 ; lcv < npages + 1 ; lcv++) {

			/*
			 * handle ppnext for outside loop, and saving pp
			 * until the end.
			 */
			if (lcv < npages) {
				if (ppsp[lcv] == pp)
					continue; /* skip pp until the end */
				ptmp = ppsp[lcv];
			} else {
				ptmp = pp;

				/* set up next page for outer loop */
				if (by_list) {
					if (pp->pg_version == pp_version)
						ppnext = TAILQ_NEXT(pp, listq);
					else
						/* reset */
						ppnext = TAILQ_FIRST(&uobj->memq);
				} else {
					if (curoff < stop)
					ppnext = uvm_pagelookup(uobj, curoff);
				}
			}
d940 4
a943 19
				if (ptmp->pg_flags & PG_RELEASED) {

					/*
					 * pgo_releasepg needs to grab the
					 * pageq lock itself.
					 */
					uvm_unlock_pageq();
					if (!uvn_releasepg(ptmp, NULL))
						return (TRUE);

					uvm_lock_pageq();	/* relock */
					continue;		/* next page */

				} else {
					atomic_setbits_int(&ptmp->pg_flags,
					    PG_CLEAN|PG_CLEANCHK);
					if ((flags & PGO_FREE) == 0)
						pmap_clear_modify(ptmp);
				}
d956 12
a967 22

			} else if (flags & PGO_FREE) {
				if (result == VM_PAGER_PEND) {
					if ((ptmp->pg_flags & PG_BUSY) != 0)
						/* signal for i/o done */
						atomic_setbits_int(
						    &ptmp->pg_flags,
						    PG_RELEASED);
				} else {
					if (result != VM_PAGER_OK) {
						printf("uvn_flush: obj=%p, "
						   "offset=0x%llx.  error "
						   "during pageout.\n",
						    pp->uobject,
						    (long long)pp->offset);
						printf("uvn_flush: WARNING: "
						    "changes to page may be "
						    "lost!\n");
						retval = FALSE;
					}
					pmap_page_protect(ptmp, VM_PROT_NONE);
					uvm_pagefree(ptmp);
d969 2
d1113 1
a1113 1
			    (ptmp->pg_flags & (PG_BUSY|PG_RELEASED)) != 0) {
d1219 1
a1219 1
			if ((ptmp->pg_flags & (PG_BUSY|PG_RELEASED)) != 0) {
@


1.66
log
@date based reversion of uvm to the 4th May.

More backouts in line with previous ones, this appears to bring us back to a
stable condition.

A machine forced to 64mb of ram cycled 10GB through swap with this diff
and is still running as I type this. Other tests by ariane@@ and thib@@
also seem to show that it's alright.

ok deraadt@@, thib@@, ariane@@
@
text
@d194 1
a194 1
		/* regain VREF if we were persisting */
d196 2
a197 2
			VREF(vp);
			UVMHIST_LOG(maphist," VREF (reclaim persisting vnode)",
d295 1
a295 1
	VREF(vp);
d300 1
a300 1
	UVMHIST_LOG(maphist,"<- done/VREF, ret %p", &uvn->u_obj,0,0,0);
d612 1
a612 1
		 * free the uvn now.   note that the VREF reference is already
d1774 1
a1774 1
	VREF(vp);			/* seems ok, even with VOP_LOCK */
d1906 1
a1906 1
			VREF(vp);
@


1.65
log
@Backout all the PG_RELEASED changes.

This is for the same reason as the earlier backouts, to avoid the bug
either added or exposed sometime around c2k9. This *should* be the last
one.

prompted by deraadt@@

ok ariane@@
@
text
@@


1.64
log
@Backout pmemrange (which to most people is more well known as physmem
allocator).

"i can't see any obvious problems" oga
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.58 2009/05/23 14:06:37 oga Exp $	*/
d418 7
a424 4
	 * have either been free'd or they are marked PG_BUSY and in the 
	 * middle of an async io. If we still have pages we set the "relkill"
	 * state, so that in the case the vnode gets terminated we know 
	 * to leave it alone. Otherwise we'll kill the vnode when it's empty.
d427 14
a440 7
	uvn->u_flags |= UVM_VNODE_RELKILL;
	/* wait on any outstanding io */
	while (uobj->uo_npages && uvn->u_flags & UVM_VNODE_RELKILL) {
		uvn->u_flags |= UVM_VNODE_IOSYNC;
		UVM_UNLOCK_AND_WAIT(&uvn->u_nio, &uvn->u_obj.vmobjlock, FALSE,
		    "uvn_term",0);
		simple_lock(&uvn->u_obj.vmobjlock);
a442 3
	if ((uvn->u_flags & UVM_VNODE_RELKILL) == 0)
		return;

d493 2
d652 6
a657 1
	KASSERT(pg->pg_flags & PG_RELEASED);
d669 26
d795 1
a795 1
	struct vm_page *pp, *ptmp;
d798 1
a798 1
	boolean_t retval, need_iosync, needs_clean;
d800 1
d803 1
d811 2
a812 2
		start = 0;
		stop = round_page(uvn->u_size);
d815 9
a823 1
		stop = MIN(round_page(stop), round_page(uvn->u_size));
d827 2
a828 2
	    " flush start=0x%lx, stop=0x%lx, flags=0x%lx",
	    (u_long)start, (u_long)stop, flags, 0);
d841 7
a847 4
	if ((flags & PGO_CLEANIT) != 0) {
		KASSERT(uobj->pgops->pgo_mk_pcluster != 0);
		for (curoff = start ; curoff < stop; curoff += PAGE_SIZE) {
			if ((pp = uvm_pagelookup(uobj, curoff)) != NULL)
d850 10
d863 14
d879 1
d881 29
a909 3
	for (curoff = start; curoff < stop; curoff += PAGE_SIZE) {
		if ((pp = uvm_pagelookup(uobj, curoff)) == NULL)
			continue;
d946 1
a946 1
		 * if we don't need a clean... deactivate/free pages then cont.
d949 9
d964 1
d967 1
d969 1
a969 8
					    PG_WANTED);
					uvm_unlock_pageq();
					UVM_UNLOCK_AND_WAIT(pp,
					    &uobj->vmobjlock, 0, "uvn_flsh", 0);
					simple_lock(&uobj->vmobjlock);
					uvm_lock_pageq();
					curoff -= PAGE_SIZE;
					continue;
d976 1
d992 1
a992 3
		/* if we're async, free the page in aiodoned */
		if ((flags & (PGO_FREE|PGO_SYNCIO)) == PGO_FREE)
			atomic_setbits_int(&pp->pg_flags, PG_RELEASED);
d1003 5
a1007 5
		 * if we did an async I/O it is remotely possible for the
		 * async i/o to complete and the page "pp" be freed or what
		 * not before we get a chance to relock the object. Therefore,
		 * we only touch it when it won't be freed, RELEASED took care
		 * of the rest.
d1016 1
a1016 1
		 * can only happen when we are doing async I/O and can't
a1033 4
			if (flags & PGO_FREE)
				atomic_clearbits_int(&pp->pg_flags,
				    PG_RELEASED);

d1045 2
a1046 3
		 * for pending async i/o if we are not deactivating
		 * we can move on to the next page. aiodoned deals with
		 * the freeing case for us.
d1048 23
a1070 2
		if (result == VM_PAGER_PEND && (flags & PGO_DEACTIVATE) == 0)
			continue;
d1073 5
a1077 2
		 * need to look at each page of the I/O operation, and do what
		 * we gotta do.
d1080 25
a1104 2
		for (lcv = 0 ; lcv < npages; lcv++) {
			ptmp = ppsp[lcv];
d1128 19
a1146 4
				atomic_setbits_int(&ptmp->pg_flags,
				    PG_CLEAN|PG_CLEANCHK);
				if ((flags & PGO_FREE) == 0)
					pmap_clear_modify(ptmp);
d1159 22
a1180 12
			} else if (flags & PGO_FREE &&
			    result != VM_PAGER_PEND) {
				if (result != VM_PAGER_OK) {
					printf("uvn_flush: obj=%p, "
					   "offset=0x%llx.  error "
					   "during pageout.\n",
					    pp->uobject,
					    (long long)pp->offset);
					printf("uvn_flush: WARNING: "
					    "changes to page may be "
					    "lost!\n");
					retval = FALSE;
a1181 2
				pmap_page_protect(ptmp, VM_PROT_NONE);
				uvm_pagefree(ptmp);
d1324 1
a1324 1
			    (ptmp->pg_flags & PG_BUSY) != 0) {
d1430 1
a1430 1
			if ((ptmp->pg_flags & PG_BUSY) != 0) {
@


1.63
log
@Backout all changes to uvm after pmemrange (which will be backed out
separately).

a change at or just before the hackathon has either exposed or added a
very very nasty memory corruption bug that is giving us hell right now.
So in the interest of kernel stability these diffs are being backed out
until such a time as that corruption bug has been found and squashed,
then the ones that are proven good may slowly return.

a quick hitlist of the main commits this backs out:

mine:
uvm_objwire
the lock change in uvm_swap.c
using trees for uvm objects instead of the hash
removing the pgo_releasepg callback.

art@@'s:
putting pmap_page_protect(VM_PROT_NONE) in uvm_pagedeactivate() since
all callers called that just prior anyway.

ok beck@@, ariane@@.

prompted by deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.59 2009/06/01 17:42:33 ariane Exp $	*/
d564 1
a564 1
		TAILQ_FOREACH(pp, &uvn->u_obj.memq, fq.queues.listq) {
@


1.62
log
@Since all callers of uvm_pagedeactivate did pmap_page_protect(.., VM_PROT_NONE)
just move that into uvm_pagedeactivate.

oga@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.61 2009/06/02 23:00:19 oga Exp $	*/
d96 1
d112 1
d276 1
a276 1
	RB_INIT(&uvn->u_obj.memt);
d443 5
a447 1
	KASSERT(RB_EMPTY(&uobj->memt));
d529 2
a530 2
	 * state [i.e. waiting for async i/o to finish].
	 * we take over the vnode now and cancel the relkill.
d564 1
a564 1
		RB_FOREACH(pp, uobj_pgs, &uvn->u_obj.memt) {
d624 35
d692 2
d848 1
d992 1
@


1.61
log
@Instead of the global hash table with the terrible hashfunction and a
global lock, switch the uvm object pages to being kept in a per-object
RB_TREE. Right now this is approximately the same speed, but cleaner.
When biglock usage is reduced this will improve concurrency due to lock
contention..

ok beck@@ art@@. Thanks to jasper for the speed testing.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.60 2009/06/01 19:54:02 oga Exp $	*/
a804 1
					pmap_page_protect(pp, VM_PROT_NONE);
a947 1
					pmap_page_protect(ptmp, VM_PROT_NONE);
@


1.60
log
@Since we've now cleared up a lot of the PG_RELEASED setting, remove the
pgo_releasepg() hook and just free the page the "normal" way in the one
place we'll ever see PG_RELEASED and should care (uvm_page_unbusy,
called in aiodoned).

ok art@@, beck@@, thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.59 2009/06/01 17:42:33 ariane Exp $	*/
d274 1
a274 1
	TAILQ_INIT(&uvn->u_obj.memq);
d441 1
a441 5
#ifdef DIAGNOSTIC
	if (!TAILQ_EMPTY(&uobj->memq))
		panic("uvn_deref: vnode VM object still has pages afer "
		    "syncio/free flush");
#endif
d558 1
a558 1
		TAILQ_FOREACH(pp, &uvn->u_obj.memq, fq.queues.listq) {
@


1.59
log
@physmem allocator: change the view of free memory from single free pages
to free ranges.
Classify memory based on region with associated use-counter (which is used
to construct a priority list of where to allocate memory).

Based on code from tedu@@, help from many.
Ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.58 2009/05/23 14:06:37 oga Exp $	*/
a95 1
boolean_t	 uvn_releasepg(struct vm_page *, struct vm_page **);
a110 1
	uvn_releasepg,
d527 2
a528 2
	 * state [i.e. waiting for async i/o to finish so that releasepg can
	 * kill object].  we take over the vnode now and cancel the relkill.
a621 35
 * uvn_releasepg: handled a released page in a uvn
 *
 * => "pg" is a PG_BUSY [caller owns it], PG_RELEASED page that we need
 *	to dispose of.
 * => caller must handled PG_WANTED case
 * => called with page's object locked, pageq's unlocked
 * => returns TRUE if page's object is still alive, FALSE if we
 *	killed the page's object.    if we return TRUE, then we
 *	return with the object locked.
 * => if (nextpgp != NULL) => we return pageq.tqe_next here, and return
 *				with the page queues locked [for pagedaemon]
 * => if (nextpgp == NULL) => we return with page queues unlocked [normal case]
 * => we kill the uvn if it is not referenced and we are suppose to
 *	kill it ("relkill").
 */

boolean_t
uvn_releasepg(struct vm_page *pg, struct vm_page **nextpgp /* OUT */)
{
	KASSERT(pg->pg_flags & PG_RELEASED);

	/*
	 * dispose of the page [caller handles PG_WANTED]
	 */
	pmap_page_protect(pg, VM_PROT_NONE);
	uvm_lock_pageq();
	if (nextpgp)
		*nextpgp = TAILQ_NEXT(pg, pageq); /* next page for daemon */
	uvm_pagefree(pg);
	if (!nextpgp)
		uvm_unlock_pageq();
	return (TRUE);
}

/*
a654 2
 * - handle "released" pages [using pgo_releasepg]
 *   >>> pgo_releasepg may kill the object
@


1.58
log
@More PG_RELEASED cleaning.

similar to the aobj.c changes, this one does vnodes. Vnodes are more
complex because they actaully have to sync to backing store. So firstly,
convert sync to sleep instead of setting released.

Now, for  backing store, in the PGO_FREE case, if we set PG_RELEASED
*before* an async io, (nothing else will see it, the page is busy), then
we can ignore the page after the io is done. We could do something
similar for PGO_DEACTIVATE too, but that is another change. On error we
just clear the released flag, nothing else sets it for uobj pages other
than aiodoned.

ok thib@@, beck@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.57 2009/04/14 20:13:14 oga Exp $	*/
d564 1
a564 1
		TAILQ_FOREACH(pp, &uvn->u_obj.memq, listq) {
@


1.57
log
@Oops, this comment change should not have been commited. What it
addresses is another diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.56 2009/04/14 20:12:05 oga Exp $	*/
d418 4
a421 7
	 * have either been free'd or they are marked PG_BUSY|PG_RELEASED.
	 * the PG_BUSY bit was set either by us or the daemon for async I/O.
	 * in either case, if we have pages left we can't kill the object
	 * yet because i/o is pending.  in this case we set the "relkill"
	 * flag which will cause pgo_releasepg to kill the object once all
	 * the I/O's are done [pgo_releasepg will be called from the aiodone
	 * routine or from the page daemon].
d424 10
a433 13
	if (uobj->uo_npages) {		/* I/O pending.  iodone will free */
#ifdef DEBUG
		/*
		 * XXXCDC: very unlikely to happen until we have async i/o
		 * so print a little info message in case it does.
		 */
		printf("uvn_detach: vn %p has pages left after flush - "
		    "relkill mode\n", uobj);
#endif
		uvn->u_flags |= UVM_VNODE_RELKILL;
		simple_unlock(&uobj->vmobjlock);
		UVMHIST_LOG(maphist,"<- done! (releasepg will kill obj)", 0, 0,
		    0, 0);
a434 1
	}
a485 2
 * => unlike uvn_detach, this function must not return until all the
 *	uvn's pages are disposed of.
d643 1
a643 6
	struct uvm_vnode *uvn = (struct uvm_vnode *) pg->uobject;
	struct vnode *vp = (struct vnode *)uvn;
#ifdef DIAGNOSTIC
	if ((pg->pg_flags & PG_RELEASED) == 0)
		panic("uvn_releasepg: page not released!");
#endif
a654 26

	/*
	 * now see if we need to kill the object
	 */
	if (uvn->u_flags & UVM_VNODE_RELKILL) {
		if (uvn->u_obj.uo_refs)
			panic("uvn_releasepg: kill flag set on referenced "
			    "object!");
		if (uvn->u_obj.uo_npages == 0) {
			if (uvn->u_flags & UVM_VNODE_WRITEABLE) {
				LIST_REMOVE(uvn, u_wlist);
			}
#ifdef DIAGNOSTIC
			if (!TAILQ_EMPTY(&uvn->u_obj.memq))
	panic("uvn_releasepg: pages in object with npages == 0");
#endif
			if (uvn->u_flags & UVM_VNODE_WANTED)
				/* still holding object lock */
				wakeup(uvn);

			uvn->u_flags = 0;		/* DEAD! */
			simple_unlock(&uvn->u_obj.vmobjlock);
			vrele(vp);
			return (FALSE);
		}
	}
d755 1
a755 1
	struct vm_page *pp, *ppnext, *ptmp;
d758 1
a758 1
	boolean_t retval, need_iosync, by_list, needs_clean, all;
a759 1
	u_short pp_version;
a761 1
	curoff = 0;	/* XXX: shut up gcc */
d769 2
a770 2
		all = TRUE;
		by_list = TRUE;		/* always go by the list */
d773 1
a773 9
		stop = round_page(stop);
#ifdef DEBUG
		if (stop > round_page(uvn->u_size))
			printf("uvn_flush: strange, got an out of range "
			    "flush (fixed)\n");
#endif
		all = FALSE;
		by_list = (uobj->uo_npages <=
		    ((stop - start) >> PAGE_SHIFT) * UVN_HASH_PENALTY);
d777 2
a778 2
	    " flush start=0x%lx, stop=0x%lx, by_list=%ld, flags=0x%lx",
	    (u_long)start, (u_long)stop, by_list, flags);
d791 4
a794 7
	if ((flags & PGO_CLEANIT) != 0 &&
	    uobj->pgops->pgo_mk_pcluster != NULL) {
		if (by_list) {
			TAILQ_FOREACH(pp, &uobj->memq, listq) {
				if (!all &&
				    (pp->offset < start || pp->offset >= stop))
					continue;
a796 10
			}

		} else {   /* by hash */
			for (curoff = start ; curoff < stop;
			    curoff += PAGE_SIZE) {
				pp = uvm_pagelookup(uobj, curoff);
				if (pp)
					atomic_clearbits_int(&pp->pg_flags,
					    PG_CLEANCHK);
			}
a799 14
	/*
	 * now do it.   note: we must update ppnext in body of loop or we
	 * will get stuck.  we need to use ppnext because we may free "pp"
	 * before doing the next loop.
	 */

	if (by_list) {
		pp = TAILQ_FIRST(&uobj->memq);
	} else {
		curoff = start;
		pp = uvm_pagelookup(uobj, curoff);
	}

	ppnext = NULL;	/* XXX: shut up gcc */
a801 1

d803 3
a805 29
	for ( ; (by_list && pp != NULL) ||
	  (!by_list && curoff < stop) ; pp = ppnext) {

		if (by_list) {

			/*
			 * range check
			 */

			if (!all &&
			    (pp->offset < start || pp->offset >= stop)) {
				ppnext = TAILQ_NEXT(pp, listq);
				continue;
			}

		} else {

			/*
			 * null check
			 */

			curoff += PAGE_SIZE;
			if (pp == NULL) {
				if (curoff < stop)
					ppnext = uvm_pagelookup(uobj, curoff);
				continue;
			}

		}
d842 1
a842 1
		 * if we don't need a clean... load ppnext and dispose of pp
a844 9
			/* load ppnext */
			if (by_list)
				ppnext = TAILQ_NEXT(pp, listq);
			else {
				if (curoff < stop)
					ppnext = uvm_pagelookup(uobj, curoff);
			}

			/* now dispose of pp */
a850 1

a852 1
					/* release busy pages */
d854 8
a861 1
					    PG_RELEASED);
a867 1
			/* ppnext is valid so we can continue... */
d883 3
a885 1
		pp_version = pp->pg_version;
d896 5
a900 5
		 * at this point nothing is locked.   if we did an async I/O
		 * it is remotely possible for the async i/o to complete and
		 * the page "pp" be freed or what not before we get a chance
		 * to relock the object.   in order to detect this, we have
		 * saved the version number of the page in "pp_version".
d909 1
a909 1
		 * can only happen when  we are doing async I/O and can't
d927 4
d942 3
a944 2
		 * for pending async i/o if we are not deactivating/freeing
		 * we can move on to the next page.
d946 2
a947 23

		if (result == VM_PAGER_PEND) {

			if ((flags & (PGO_DEACTIVATE|PGO_FREE)) == 0) {
				/*
				 * no per-page ops: refresh ppnext and continue
				 */
				if (by_list) {
					if (pp->pg_version == pp_version)
						ppnext = TAILQ_NEXT(pp, listq);
					else
						/* reset */
						ppnext = TAILQ_FIRST(&uobj->memq);
				} else {
					if (curoff < stop)
						ppnext = uvm_pagelookup(uobj,
						    curoff);
				}
				continue;
			}

			/* need to do anything here? */
		}
d950 2
a951 5
		 * need to look at each page of the I/O operation.  we defer
		 * processing "pp" until the last trip through this "for" loop
		 * so that we can load "ppnext" for the main loop after we
		 * play with the cluster pages [thus the "npages + 1" in the
		 * loop below].
d954 2
a955 25
		for (lcv = 0 ; lcv < npages + 1 ; lcv++) {

			/*
			 * handle ppnext for outside loop, and saving pp
			 * until the end.
			 */
			if (lcv < npages) {
				if (ppsp[lcv] == pp)
					continue; /* skip pp until the end */
				ptmp = ppsp[lcv];
			} else {
				ptmp = pp;

				/* set up next page for outer loop */
				if (by_list) {
					if (pp->pg_version == pp_version)
						ppnext = TAILQ_NEXT(pp, listq);
					else
						/* reset */
						ppnext = TAILQ_FIRST(&uobj->memq);
				} else {
					if (curoff < stop)
					ppnext = uvm_pagelookup(uobj, curoff);
				}
			}
d979 4
a982 19
				if (ptmp->pg_flags & PG_RELEASED) {

					/*
					 * pgo_releasepg needs to grab the
					 * pageq lock itself.
					 */
					uvm_unlock_pageq();
					if (!uvn_releasepg(ptmp, NULL))
						return (TRUE);

					uvm_lock_pageq();	/* relock */
					continue;		/* next page */

				} else {
					atomic_setbits_int(&ptmp->pg_flags,
					    PG_CLEAN|PG_CLEANCHK);
					if ((flags & PGO_FREE) == 0)
						pmap_clear_modify(ptmp);
				}
d995 12
a1006 22

			} else if (flags & PGO_FREE) {
				if (result == VM_PAGER_PEND) {
					if ((ptmp->pg_flags & PG_BUSY) != 0)
						/* signal for i/o done */
						atomic_setbits_int(
						    &ptmp->pg_flags,
						    PG_RELEASED);
				} else {
					if (result != VM_PAGER_OK) {
						printf("uvn_flush: obj=%p, "
						   "offset=0x%llx.  error "
						   "during pageout.\n",
						    pp->uobject,
						    (long long)pp->offset);
						printf("uvn_flush: WARNING: "
						    "changes to page may be "
						    "lost!\n");
						retval = FALSE;
					}
					pmap_page_protect(ptmp, VM_PROT_NONE);
					uvm_pagefree(ptmp);
d1008 2
d1152 1
a1152 1
			    (ptmp->pg_flags & (PG_BUSY|PG_RELEASED)) != 0) {
d1258 1
a1258 1
			if ((ptmp->pg_flags & (PG_BUSY|PG_RELEASED)) != 0) {
@


1.56
log
@The use of uvm.pagedaemon_lock is incredibly inconsistent. only a
fraction of the wakeups and sleeps involved here actually grab that
lock. The remainder, on the other hand, always have the fpageq_lock
locked.

So, make this locking correct by switching the other users over to
fpageq_lock, too.

This would probably be better off being a semaphore, but for now at
least it's correct.

"ok, unless you want to implement semaphores" art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.55 2009/04/13 22:17:54 oga Exp $	*/
a1166 4
				/*
				 * XXX if ! busy, io is already done. shouldn't
				 * XXX we free the pages ourselves?
				 */
@


1.55
log
@Convert the page queue lock to a mutex instead of a simplelock.

Fix up the one case of lock recursion (which blatantly ignored the
comment right above it saying that we don't need to lock). The rest of
the lock usage has been checked and appears to be correct.

ok ariane@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.54 2009/04/05 18:11:03 oga Exp $	*/
d1167 4
@


1.54
log
@In the unlikely even that we do the final unref on a uvm_vnode object
while there's io pending (async io makes that possible, but not often
hit), then we'll be waiting for the pgo_releasepg hook to free the
object when all of our pages disappear.

However, uvn_releasepg, while it does everything else that unreferencing
the object would do, it neglects to do the final vrele() on the vnode.
So in this rare situation we'd end up with the vnode waiting around
until it was forcibly recycled. Fix this by adding in the missing vrele().

ok thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.53 2009/03/20 15:19:04 oga Exp $	*/
d1130 4
a1133 1
					/* pgo_releasepg wants this */
@


1.53
log
@While working on some stuff in uvm I've gotten REALLY sick of reading
K&R function declarations, so switch them all over to ansi-style, in
accordance with the prophesy.

"go for it" art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.52 2008/10/06 18:17:29 deraadt Exp $	*/
d653 1
d691 1
@


1.52
log
@uvn_attach message is purely diagnostic, not needed
no ok's from anyone because they are all slacking
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.51 2007/09/17 20:29:55 thib Exp $	*/
d87 10
a96 16
void		   uvn_cluster(struct uvm_object *, voff_t,
					   voff_t *, voff_t *);
void                uvn_detach(struct uvm_object *);
boolean_t           uvn_flush(struct uvm_object *, voff_t,
					 voff_t, int);
int                 uvn_get(struct uvm_object *, voff_t,
					vm_page_t *, int *, int,
					vm_prot_t, int, int);
void		   uvn_init(void);
int		   uvn_io(struct uvm_vnode *, vm_page_t *,
				      int, int, int);
int		   uvn_put(struct uvm_object *, vm_page_t *,
					int, boolean_t);
void                uvn_reference(struct uvm_object *);
boolean_t	   uvn_releasepg(struct vm_page *,
					      struct vm_page **);
d150 1
a150 3
uvn_attach(arg, accessprot)
	void *arg;
	vm_prot_t accessprot;
d318 1
a318 2
uvn_reference(uobj)
	struct uvm_object *uobj;
d349 1
a349 2
uvn_detach(uobj)
	struct uvm_object *uobj;
d501 1
a501 2
uvm_vnp_terminate(vp)
	struct vnode *vp;
d650 1
a650 3
uvn_releasepg(pg, nextpgp)
	struct vm_page *pg;
	struct vm_page **nextpgp;	/* OUT */
d790 1
a790 4
uvn_flush(uobj, start, stop, flags)
	struct uvm_object *uobj;
	voff_t start, stop;
	int flags;
d1221 2
a1222 4
uvn_cluster(uobj, offset, loffset, hoffset)
	struct uvm_object *uobj;
	voff_t offset;
	voff_t *loffset, *hoffset; /* OUT */
d1252 1
a1252 4
uvn_put(uobj, pps, npages, flags)
	struct uvm_object *uobj;
	struct vm_page **pps;
	int npages, flags;
d1276 2
a1277 7
uvn_get(uobj, offset, pps, npagesp, centeridx, access_type, advice, flags)
	struct uvm_object *uobj;
	voff_t offset;
	struct vm_page **pps;		/* IN/OUT */
	int *npagesp;			/* IN (OUT if PGO_LOCKED) */
	int centeridx, advice, flags;
	vm_prot_t access_type;
d1522 1
a1522 4
uvn_io(uvn, pps, npages, flags, rw)
	struct uvm_vnode *uvn;
	vm_page_t *pps;
	int npages, flags, rw;
d1822 1
a1822 3
uvm_vnp_setsize(vp, newsize)
	struct vnode *vp;
	voff_t newsize;
@


1.51
log
@instead of inspecting the vnode op's to figure out if
vnode locking actually works, just check the VLOCKSWORK
flag. Also, change this ifdef DEBUG to VFSDEBUG since
VLOCKSWORK is only ever set if VFSDEBUG is defined.

ok/input miod@@, art@@ (earlier diff)
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.50 2007/08/31 08:38:08 thib Exp $	*/
a176 2
		printf("uvn_attach: blocked at %p flags 0x%x\n",
		    uvn, uvn->u_flags);
@


1.50
log
@simplify uvm_vnp_sync() by removing some simplelock goo.
some comment cleanup and a touch of KNF.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.49 2007/06/05 00:38:24 deraadt Exp $	*/
d1769 1
a1769 2
uvm_vnp_uncache(vp)
	struct vnode *vp;
d1804 1
a1804 2

#ifdef DEBUG
d1809 3
a1811 24
	if (!VOP_ISLOCKED(vp)) {
		boolean_t is_ok_anyway = FALSE;
#if defined(NFSCLIENT)
		extern int (**nfsv2_vnodeop_p)(void *);
		extern int (**spec_nfsv2nodeop_p)(void *);
#if defined(FIFO)
		extern int (**fifo_nfsv2nodeop_p)(void *);
#endif	/* defined(FIFO) */

		/* vnode is NOT VOP_LOCKed: some vnode types _never_ lock */
		if (vp->v_op == nfsv2_vnodeop_p ||
		    vp->v_op == spec_nfsv2nodeop_p) {
			is_ok_anyway = TRUE;
		}
#if defined(FIFO)
		if (vp->v_op == fifo_nfsv2nodeop_p) {
			is_ok_anyway = TRUE;
		}
#endif	/* defined(FIFO) */
#endif	/* defined(NFSSERVER) || defined(NFSCLIENT) */
		if (!is_ok_anyway)
			panic("uvm_vnp_uncache: vnode not locked!");
	}
#endif	/* DEBUG */
@


1.49
log
@use six new macros to access & store the 48-bit disklabel fields related
to size.  tested on almost all machines, double checked by miod and krw
next comes the type handling surrounding these values
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.48 2007/05/29 21:06:34 thib Exp $	*/
a1916 1

d1918 1
a1918 2
uvm_vnp_sync(mp)
	struct mount *mp;
a1921 1
	boolean_t got_lock;
d1936 1
a1936 1
		vp = (struct vnode *) uvn;
a1939 6
		/* attempt to gain reference */
		while ((got_lock = simple_lock_try(&uvn->u_obj.vmobjlock)) ==
								FALSE &&
				(uvn->u_flags & UVM_VNODE_BLOCKED) == 0)
			/* spin */;

d1941 3
a1943 9
		 * we will exit the loop if either if the following are true:
		 *  - we got the lock [always true if NCPU == 1]
		 *  - we failed to get the lock but noticed the vnode was
		 *	"blocked" -- in this case the vnode must be a dying
		 *	vnode, and since dying vnodes are in the process of
		 *	being flushed out, we can safely skip this one
		 *
		 * we want to skip over the vnode if we did not get the lock,
		 * or if the vnode is already dying (due to the above logic).
d1948 3
a1950 5
		if (!got_lock || (uvn->u_flags & UVM_VNODE_BLOCKED) != 0) {
			if (got_lock)
				simple_unlock(&uvn->u_obj.vmobjlock);
			continue;		/* skip it */
		}
a1958 1
		simple_unlock(&uvn->u_obj.vmobjlock);
a1959 3
		/*
		 * got it!
		 */
d1963 1
a1963 5
	/*
	 * step 3: we now have a list of uvn's that may need cleaning.
	 * we are holding the uvn_sync_lock.
	 */

a1964 1
		simple_lock(&uvn->u_obj.vmobjlock);
d1970 1
a1970 2
		uvn_flush(&uvn->u_obj, 0, 0,
		    PGO_CLEANIT|PGO_ALLPAGES|PGO_DOACTCLUST);
a1983 2
		simple_unlock(&uvn->u_obj.vmobjlock);

a1987 3
	/*
	 * done!  release sync lock
	 */
@


1.48
log
@Kill some simplelock goo that annoyed me while reading.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.47 2007/04/14 23:04:28 tedu Exp $	*/
d253 1
a253 1
			    (u_quad_t)pi.part->p_size;
@


1.47
log
@remove static
ok beck miod pedro thib
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.46 2007/04/13 18:57:49 art Exp $	*/
a77 1
simple_lock_data_t uvn_wl_lock;		/* locks uvn_wlist */
d132 1
a132 1
uvn_init()
a135 1
	simple_lock_init(&uvn_wl_lock);
a214 1
			simple_lock(&uvn_wl_lock);
a215 1
			simple_unlock(&uvn_wl_lock);
a295 1
		simple_lock(&uvn_wl_lock);
a296 1
		simple_unlock(&uvn_wl_lock);
a459 1
		simple_lock(&uvn_wl_lock);		/* protect uvn_wlist */
a460 1
		simple_unlock(&uvn_wl_lock);
a631 1
			simple_lock(&uvn_wl_lock);
a632 1
			simple_unlock(&uvn_wl_lock);
a692 1
				simple_lock(&uvn_wl_lock);
a693 1
				simple_unlock(&uvn_wl_lock);
d1934 1
a1934 3
	 * write list.   we gain a reference to uvns of interest.  must
	 * be careful about locking uvn's since we will be holding uvn_wl_lock
	 * in the body of the loop.
a1936 1
	simple_lock(&uvn_wl_lock);
a1982 1
	simple_unlock(&uvn_wl_lock);
d1986 1
a1986 2
	 * we are holding the uvn_sync_lock, but have dropped the uvn_wl_lock
	 * (so we can now safely lock uvn's again).
@


1.46
log
@While splitting flags and pqflags might have been a good idea in theory
to separate locking, on most modern machines this is not enough
since operations on short types touch other short types that share the
same word in memory.

Merge pg_flags and pqflags again and now use atomic operations to change
the flags. Also bump wire_count to an int and pg_version might go
int as well, just for alignment.

tested by many, many. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.45 2007/04/04 17:44:45 art Exp $	*/
d77 2
a78 2
static struct uvn_list_struct uvn_wlist;	/* writeable uvns */
static simple_lock_data_t uvn_wl_lock;		/* locks uvn_wlist */
d81 1
a81 1
static struct uvn_sq_struct uvn_sync_q;		/* sync'ing uvns */
d88 1
a88 1
static void		   uvn_cluster(struct uvm_object *, voff_t,
d90 2
a91 2
static void                uvn_detach(struct uvm_object *);
static boolean_t           uvn_flush(struct uvm_object *, voff_t,
d93 1
a93 1
static int                 uvn_get(struct uvm_object *, voff_t,
d96 2
a97 2
static void		   uvn_init(void);
static int		   uvn_io(struct uvm_vnode *, vm_page_t *,
d99 1
a99 1
static int		   uvn_put(struct uvm_object *, vm_page_t *,
d101 2
a102 2
static void                uvn_reference(struct uvm_object *);
static boolean_t	   uvn_releasepg(struct vm_page *,
d132 1
a132 1
static void
d333 1
a333 1
static void
d365 1
a365 1
static void
d816 1
a816 1
static boolean_t
d1250 1
a1250 1
static void
d1283 1
a1283 1
static int
d1310 1
a1310 1
static int
d1561 1
a1561 1
static int
@


1.45
log
@Mechanically rename the "flags" and "version" fields in struct vm_page
to "pg_flags" and "pg_version", so that they are a bit easier to work with.
Whoever uses generic names like this for a popular struct obviously doesn't
read much code.

Most architectures compile and there are no functionality changes.

deraadt@@ ok ("if something fails to compile, we fix that by hand")
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.44 2007/03/25 13:14:41 pedro Exp $	*/
d876 2
a877 1
				pp->pg_flags &= ~PG_CLEANCHK;
d885 2
a886 1
					pp->pg_flags &= ~PG_CLEANCHK;
d963 1
a963 1
			    (pp->pqflags & PQ_ACTIVE) != 0)
d967 2
a968 2
				pp->pg_flags &= ~(PG_CLEAN);
			pp->pg_flags |= PG_CLEANCHK;	/* update "hint" */
d987 1
a987 1
				if ((pp->pqflags & PQ_INACTIVE) == 0 &&
d996 2
a997 1
					pp->pg_flags |= PG_RELEASED;
d1017 1
a1017 1
		pp->pg_flags |= PG_BUSY;	/* we 'own' page now */
d1153 2
a1154 1
				ptmp->pg_flags &= ~(PG_WANTED|PG_BUSY);
d1167 2
a1168 1
					ptmp->pg_flags |= (PG_CLEAN|PG_CLEANCHK);
d1179 1
a1179 1
				if ((pp->pqflags & PQ_INACTIVE) == 0 &&
d1189 3
a1191 1
						ptmp->pg_flags |= PG_RELEASED;
d1370 1
a1370 1
			ptmp->pg_flags |= PG_BUSY;	/* loan up to caller */
d1466 1
a1466 1
				ptmp->pg_flags |= PG_WANTED;
d1479 1
a1479 1
			ptmp->pg_flags |= PG_BUSY;
d1515 2
a1516 1
			ptmp->pg_flags &= ~(PG_WANTED|PG_BUSY);
d1536 2
a1537 1
		ptmp->pg_flags &= ~PG_FAKE;		/* data is valid ... */
@


1.44
log
@Remove references to KERN_SUCCESS, okay miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.43 2006/07/31 11:51:29 mickey Exp $	*/
d595 1
a595 1
			if ((pp->flags & PG_BUSY) == 0)
d679 1
a679 1
	if ((pg->flags & PG_RELEASED) == 0)
d876 1
a876 1
				pp->flags &= ~PG_CLEANCHK;
d884 1
a884 1
					pp->flags &= ~PG_CLEANCHK;
d948 1
a948 1
		if ((flags & PGO_CLEANIT) == 0 || (pp->flags & PG_BUSY) != 0) {
d950 1
a950 1
			if ((pp->flags & PG_BUSY) != 0 &&
d959 1
a959 1
			if ((pp->flags & PG_CLEAN) != 0 &&
d963 1
a963 1
			if ((pp->flags & PG_CLEAN) != 0 &&
d965 2
a966 2
				pp->flags &= ~(PG_CLEAN);
			pp->flags |= PG_CLEANCHK;	/* update "hint" */
d968 1
a968 1
			needs_clean = ((pp->flags & PG_CLEAN) == 0);
d992 1
a992 1
				if (pp->flags & PG_BUSY) {
d994 1
a994 1
					pp->flags |= PG_RELEASED;
d1014 1
a1014 1
		pp->flags |= PG_BUSY;	/* we 'own' page now */
d1017 1
a1017 1
		pp_version = pp->version;
d1081 1
a1081 1
					if (pp->version == pp_version)
d1120 1
a1120 1
					if (pp->version == pp_version)
d1146 1
a1146 1
				if (ptmp->flags & PG_WANTED)
d1150 1
a1150 1
				ptmp->flags &= ~(PG_WANTED|PG_BUSY);
d1152 1
a1152 1
				if (ptmp->flags & PG_RELEASED) {
d1163 1
a1163 1
					ptmp->flags |= (PG_CLEAN|PG_CLEANCHK);
d1182 1
a1182 1
					if ((ptmp->flags & PG_BUSY) != 0)
d1184 1
a1184 1
						ptmp->flags |= PG_RELEASED;
d1352 1
a1352 1
			    (ptmp->flags & (PG_BUSY|PG_RELEASED)) != 0) {
d1363 1
a1363 1
			ptmp->flags |= PG_BUSY;		/* loan up to caller */
d1458 2
a1459 2
			if ((ptmp->flags & (PG_BUSY|PG_RELEASED)) != 0) {
				ptmp->flags |= PG_WANTED;
d1472 1
a1472 1
			ptmp->flags |= PG_BUSY;
d1504 1
a1504 1
			if (ptmp->flags & PG_WANTED)
d1508 1
a1508 1
			ptmp->flags &= ~(PG_WANTED|PG_BUSY);
d1528 1
a1528 1
		ptmp->flags &= ~PG_FAKE;		/* data is valid ... */
@


1.43
log
@fix uvmhist #2: args are always u_long so fix missing %d and %x and no %ll; no change for normal code
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.42 2006/07/26 23:15:55 mickey Exp $	*/
d748 1
a748 2
 * => return KERN_SUCCESS (aio finished, free it).  otherwise requeue for
 *	later collection.
@


1.42
log
@fix fmts for UVMHIST_LOG() entries making it more useful on 64bit archs; miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.41 2006/06/10 13:37:02 pedro Exp $	*/
d226 1
a226 1
		UVMHIST_LOG(maphist,"<- done, refcnt=%d", uvn->u_obj.uo_refs,
d351 1
a351 1
	UVMHIST_LOG(maphist, "<- done (uobj=%p, ref = %d)",
d376 1
a376 1
	UVMHIST_LOG(maphist,"  (uobj=%p)  ref=%d", uobj,uobj->uo_refs,0,0);
d532 1
a532 1
	UVMHIST_LOG(maphist, "  vp=%p, ref=%d, flag=0x%x", vp,
d856 2
a857 2
	    " flush start=0x%llx, stop=0x%llx, by_list=%d, flags=0x%x",
	    start, stop, by_list, flags);
d1230 1
a1230 1
	UVMHIST_LOG(maphist,"<- done (retval=0x%x)",retval,0,0,0);
d1318 1
a1318 1
	UVMHIST_LOG(maphist, "flags=%d", flags,0,0,0);
d1568 1
a1568 1
	UVMHIST_LOG(maphist, "rw=%d", rw,0,0,0);
d1729 1
a1729 1
	UVMHIST_LOG(maphist, "<- done (result %d)", result,0,0,0);
@


1.41
log
@Make uvn_sync_lock a rwlock, okay thib@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.40 2005/11/19 02:18:02 pedro Exp $	*/
d170 1
a170 1
	UVMHIST_LOG(maphist, "(vn=0x%x)", arg,0,0,0);
d316 1
a316 1
	UVMHIST_LOG(maphist,"<- done/VREF, ret 0x%x", &uvn->u_obj,0,0,0);
d351 2
a352 2
	UVMHIST_LOG(maphist, "<- done (uobj=0x%x, ref = %d)",
	uobj, uobj->uo_refs,0,0);
d376 1
a376 1
	UVMHIST_LOG(maphist,"  (uobj=0x%x)  ref=%d", uobj,uobj->uo_refs,0,0);
d532 1
a532 1
	UVMHIST_LOG(maphist, "  vp=0x%x, ref=%d, flag=0x%x", vp,
d856 1
a856 1
	    " flush start=0x%x, stop=0x%x, by_list=%d, flags=0x%x",
@


1.40
log
@Remove unnecessary lockmgr() archaism that was costing too much in terms
of panics and bugfixes. Access curproc directly, do not expect a process
pointer as an argument. Should fix many "process context required" bugs.
Incentive and okay millert@@, okay marc@@. Various testing, thanks.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.39 2004/12/26 21:22:14 miod Exp $	*/
d62 1
d82 1
a82 1
lock_data_t uvn_sync_lock;			/* locks sync operation */
d139 1
a139 1
	lockinit(&uvn_sync_lock, PVM, "uvnsync", 0, 0);
d1934 1
a1934 1
	lockmgr(&uvn_sync_lock, LK_EXCLUSIVE, NULL);
d2029 1
a2029 1
	lockmgr(&uvn_sync_lock, LK_RELEASE, NULL);
@


1.39
log
@Use list and queue macros where applicable to make the code easier to read;
no change in compiler assembly output.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.38 2004/06/06 13:44:07 grange Exp $	*/
d1933 1
a1933 1
	lockmgr(&uvn_sync_lock, LK_EXCLUSIVE, NULL, curproc);
d2028 1
a2028 1
	lockmgr(&uvn_sync_lock, LK_RELEASE, (void *)0, curproc);
@


1.38
log
@In printf %p doesn't need extra ``0x''
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.37 2003/06/12 05:55:05 henric Exp $	*/
d470 1
a470 1
	if (uobj->memq.tqh_first != NULL)
d593 1
a593 2
		for (pp = uvn->u_obj.memq.tqh_first ; pp != NULL ;
		     pp = pp->listq.tqe_next) {
d688 1
a688 1
		*nextpgp = pg->pageq.tqe_next;	/* next page for daemon */
d707 1
a707 1
			if (uvn->u_obj.memq.tqh_first)
d872 1
a872 2
			for (pp = uobj->memq.tqh_first ; pp != NULL ;
			    pp = pp->listq.tqe_next) {
d896 1
a896 1
		pp = uobj->memq.tqh_first;
d918 1
a918 1
				ppnext = pp->listq.tqe_next;
d977 1
a977 1
				ppnext = pp->listq.tqe_next;
d1082 1
a1082 1
						ppnext = pp->listq.tqe_next;
d1085 1
a1085 1
						ppnext = uobj->memq.tqh_first;
d1121 1
a1121 1
						ppnext = pp->listq.tqe_next;
d1124 1
a1124 1
						ppnext = uobj->memq.tqh_first;
d1943 1
a1943 2
	for (uvn = uvn_wlist.lh_first ; uvn != NULL ;
	    uvn = uvn->u_wlist.le_next) {
@


1.37
log
@Fix compile with NFS but not FIFO.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.36 2002/11/19 18:34:41 jason Exp $	*/
d178 1
a178 1
		printf("uvn_attach: blocked at 0x%p flags 0x%x\n",
@


1.36
log
@Use queue.h macros
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.35 2002/09/11 23:16:44 mickey Exp $	*/
d1821 1
d1823 1
d1830 1
d1834 1
@


1.35
log
@kill annoying trailing spaces (in hope it fixes /0 for me)
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.34 2002/03/17 18:28:48 art Exp $	*/
d1996 1
a1996 1
	for (uvn = uvn_sync_q.sqh_first ; uvn ; uvn = uvn->u_syncq.sqe_next) {
@


1.34
log
@Don't set LK_RETRY to vn_lock with LK_RECURSEFAIL.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.33 2002/03/14 01:27:19 millert Exp $	*/
d7 1
a7 1
 *      The Regents of the University of California.  
d27 1
a27 1
 *	Washington University, the University of California, Berkeley and 
d90 1
a90 1
static boolean_t           uvn_flush(struct uvm_object *, voff_t, 
d93 1
a93 1
					vm_page_t *, int *, int, 
d101 1
a101 1
static boolean_t	   uvn_releasepg(struct vm_page *, 
d214 1
a214 1
		if ((accessprot & VM_PROT_WRITE) != 0 && 
d228 1
a228 1
	} 
d265 1
a265 1
	simple_lock(&uvn->u_obj.vmobjlock); 
d324 1
a324 1
 * count must already be at least one (the passed in reference) so 
d327 1
a327 1
 * => caller must call with object unlocked.  
d350 1
a350 1
	UVMHIST_LOG(maphist, "<- done (uobj=0x%x, ref = %d)", 
d446 1
a446 1
		/* 
d477 1
a477 1
	
d501 1
a501 1
 *  [2] when a filesystem is being unmounted by force (MNT_FORCE, 
d553 1
a553 1
	} 
d555 1
a555 1
	
d564 1
a564 1
	
d586 1
a586 1
	 * as we just did a flush we expect all the pages to be gone or in 
d593 1
a593 1
		for (pp = uvn->u_obj.memq.tqh_first ; pp != NULL ; 
d601 2
a602 2
		/* 
		 * XXXCDC: this is unlikely to happen without async i/o so we 
d607 1
a607 1
		UVM_UNLOCK_AND_WAIT(&uvn->u_nio, &uvn->u_obj.vmobjlock, FALSE, 
d623 1
a623 1
		 * uvn must live on it is dead-vnode state until all references 
d629 1
a629 1
	
d682 1
a682 1
	
d743 1
a743 1
 * you had one function ("put") doing two things.]  
d745 1
a745 1
 * so the current pager needs: 
d795 1
a795 1
 *	cleaning the page for us (how nice!).    in this case, if we 
d797 1
a797 1
 *	object we need to wait for the other PG_BUSY pages to clear 
d805 1
a805 1
 *	on how many pages are in the object it may be cheaper to do one 
d811 1
a811 1
 *	list traversal, so we multiply the number of pages in the 
d851 1
a851 1
		by_list = (uobj->uo_npages <= 
d904 1
a904 1
	ppnext = NULL;	/* XXX: shut up gcc */ 
d909 1
a909 1
	for ( ; (by_list && pp != NULL) || 
d943 1
a943 1
		 * 
d961 1
a961 1
			if ((pp->flags & PG_CLEAN) != 0 && 
d1025 1
a1025 1
		result = uvm_pager_put(uobj, pp, &ppsp, &npages, 
d1031 2
a1032 2
		 * it is remotely possible for the async i/o to complete and 
		 * the page "pp" be freed or what not before we get a chance 
d1042 1
a1042 1
		 * VM_PAGER_AGAIN: given the structure of this pager, this 
d1049 1
a1049 1
			/* 
d1100 2
a1101 2
		 * need to look at each page of the I/O operation.  we defer 
		 * processing "pp" until the last trip through this "for" loop 
d1103 1
a1103 1
		 * play with the cluster pages [thus the "npages + 1" in the 
d1170 1
a1170 1
	  
d1221 1
a1221 1
			UVM_UNLOCK_AND_WAIT(&uvn->u_nio, &uvn->u_obj.vmobjlock, 
d1304 1
a1304 1
 
d1374 1
a1374 1
		 * XXX: fault current does deactive of pages behind us.  is 
d1377 1
a1377 1
		/* 
d1402 1
a1402 1
	 * page at a time (otherwise we'd chunk).   the VOP_READ() will do 
d1406 1
a1406 1
	for (lcv = 0, current_offset = offset ; 
d1408 1
a1408 1
		
d1432 1
a1432 1
			
d1449 1
a1449 1
					continue;	
d1452 1
a1452 1
				/* 
d1456 1
a1456 1
				break;		
d1467 2
a1468 2
			
			/* 
d1519 1
a1519 1
		/* 
d1522 1
a1522 1
		 * that page is still busy.   
d1570 1
a1570 1
	
d1578 1
a1578 1
	
d1582 1
a1582 1
	
d1584 1
a1584 1
		if (waitf == M_NOWAIT) { 
d1590 1
a1590 1
		UVM_UNLOCK_AND_WAIT(&uvn->u_flags, &uvn->u_obj.vmobjlock, 
d1624 1
a1624 1
	
d1641 1
a1641 1
	
d1685 1
a1685 1
	
d1711 1
a1711 1
		
d1752 1
a1752 1
 * => XXX: this function should DIE once we merge the VM and buffer 
d1762 1
a1762 1
 * ffs_realloccg: when we can't extend the current block and have 
d1784 1
a1784 1
	if ((uvn->u_flags & UVM_VNODE_VALID) == 0 || 
d1838 1
a1838 1
	 * now drop our reference to the vnode.   if we have the sole 
d1840 1
a1840 1
	 * just cleared the persist flag].   we have to unlock the vnode 
d1850 1
a1850 1
	
d1854 1
a1854 1
	
d1864 1
a1864 1
 * => we assume that the caller has a reference of some sort to the 
d1934 2
a1935 2
	 * step 2: build up a simpleq of uvns of interest based on the 
	 * write list.   we gain a reference to uvns of interest.  must 
d1950 1
a1950 1
		    						FALSE && 
d1952 1
a1952 1
			/* spin */ ;
d1958 1
a1958 1
		 * 	"blocked" -- in this case the vnode must be a dying
d1973 1
a1973 1
		
@


1.33
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.32 2001/12/19 08:58:07 art Exp $	*/
d1672 1
a1672 1
		result = vn_lock(vn, LK_EXCLUSIVE | LK_RETRY | LK_RECURSEFAIL, curproc);
@


1.32
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.24 2001/11/10 18:42:32 art Exp $	*/
d87 6
a92 6
static void		   uvn_cluster __P((struct uvm_object *, voff_t,
					   voff_t *, voff_t *));
static void                uvn_detach __P((struct uvm_object *));
static boolean_t           uvn_flush __P((struct uvm_object *, voff_t, 
					 voff_t, int));
static int                 uvn_get __P((struct uvm_object *, voff_t,
d94 9
a102 9
					vm_prot_t, int, int));
static void		   uvn_init __P((void));
static int		   uvn_io __P((struct uvm_vnode *, vm_page_t *,
				      int, int, int));
static int		   uvn_put __P((struct uvm_object *, vm_page_t *,
					int, boolean_t));
static void                uvn_reference __P((struct uvm_object *));
static boolean_t	   uvn_releasepg __P((struct vm_page *, 
					      struct vm_page **));
d1819 3
a1821 3
		extern int (**nfsv2_vnodeop_p) __P((void *));
		extern int (**spec_nfsv2nodeop_p) __P((void *));
		extern int (**fifo_nfsv2nodeop_p) __P((void *));
@


1.31
log
@Merge in struct uvm_vnode into struct vnode.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_vnode.c,v 1.30 2001/12/06 12:43:20 art Exp $	*/
/*	$NetBSD: uvm_vnode.c,v 1.51 2001/08/17 05:53:02 chs Exp $	*/
d7 1
a7 1
 *      The Regents of the University of California.
d27 1
a27 1
 *	Washington University, the University of California, Berkeley and
a54 1
#include <sys/kernel.h>
a61 2
#include <sys/pool.h>
#include <sys/mount.h>
d66 16
d87 16
a102 15
static void		uvn_cluster __P((struct uvm_object *, voff_t, voff_t *,
					 voff_t *));
static void		uvn_detach __P((struct uvm_object *));
static int		uvn_findpage __P((struct uvm_object *, voff_t,
					  struct vm_page **, int));
boolean_t		uvn_flush __P((struct uvm_object *, voff_t, voff_t,
				       int));
int			uvn_get __P((struct uvm_object *, voff_t,
				     struct vm_page **, int *, int, vm_prot_t,
				     int, int));
int			uvn_put __P((struct uvm_object *, struct vm_page **,
				     int, boolean_t));
static void		uvn_reference __P((struct uvm_object *));
static boolean_t	uvn_releasepg __P((struct vm_page *,
					   struct vm_page **));
d109 1
a109 1
	NULL,
d112 1
a112 1
	NULL,
d117 1
a117 1
	uvm_mk_pcluster,
d126 16
d162 1
a162 1
	struct uvm_object *uobj = &vp->v_uobj;
d164 1
a164 1
	int result;
d166 1
a166 1
	voff_t used_vnode_size;
d170 2
a171 1
	used_vnode_size = (voff_t)0;
d176 5
a180 3
	simple_lock(uobj->vmobjlock);
	while (vp->v_flag & VXLOCK) {
		vp->v_flag |= VXWANT;
d182 1
a182 1
		UVM_UNLOCK_AND_WAIT(vp, &uobj->vmobjlock, FALSE,
d184 1
a184 1
		simple_lock(&uobj->vmobjlock);
d192 1
a192 1
		simple_unlock(&uobj->vmobjlock);
a195 1
	KASSERT(vp->v_type == VREG || vp->v_type == VBLK);
d198 4
a201 2
	 * set up our idea of the size
	 * if this hasn't been done already.
d203 9
a211 1
	if (vp->v_size == VSIZENOTSET) {
d213 27
a239 2
	vp->v_flag |= VXLOCK;
	simple_unlock(&uobj->vmobjlock); /* drop lock in case we sleep */
d241 1
d255 2
a256 2
			used_vnode_size = (voff_t)pi.disklab->d_secsize *
			    (voff_t)pi.part->p_size;
d265 1
a265 5
	simple_lock(&uobj->vmobjlock);

	if (vp->v_flag & VXWANT)
		wakeup(vp);
	vp->v_flag &= ~(VXLOCK|VXWANT);
d268 4
a271 1
		simple_unlock(&uobj->vmobjlock); /* drop lock */
a274 1
	vp->v_size = used_vnode_size;
d276 27
d305 12
a316 5
	/* unlock and return */
	simple_unlock(&uobj->vmobjlock);
	UVMHIST_LOG(maphist,"<- done, refcnt=%d", uvn->u_obj.uo_refs,
	    0, 0, 0);
	return (uobj);
d324 1
a324 1
 * count must already be at least one (the passed in reference) so
d327 1
a327 1
 * => caller must call with object unlocked.
d336 17
a352 1
	VREF((struct vnode *)uobj);
d361 2
d368 285
a652 1
	vrele((struct vnode *)uobj);
d665 1
a665 1
 * => if (nextpgp != NULL) => we return the next page on the queue, and return
d677 6
a682 2
	KASSERT(pg->flags & PG_RELEASED);

d689 1
a689 1
		*nextpgp = TAILQ_NEXT(pg, pageq);
d694 26
d724 6
d731 1
d734 2
a735 2
 * a buf on the "pending" list (protected by splbio()), starts the
 * i/o and returns 0.    when the i/o is done, we expect
d737 1
a737 1
 * time).   this function should remove the buf from the pending list
d740 7
a746 1
 * list and call the iodone hook for each done request (see uvm_pager.c).
a766 1
 * => "stop == 0" means flush all pages at or after "start".
d768 1
a768 2
 *	if (and only if) we need to clean a page (PGO_CLEANIT), or
 *	if PGO_SYNCIO is set and there are pages busy.
d770 6
a775 6
 * => if PGO_CLEANIT or PGO_SYNCIO is set, we may block (due to I/O).
 *	thus, a caller might want to unlock higher level resources
 *	(e.g. vm_map) before calling flush.
 * => if neither PGO_CLEANIT nor PGO_SYNCIO is set, then we will neither
 *	unlock the object nor block.
 * => if PGO_ALLPAGES is set, then all pages in the object are valid targets
d795 1
a795 1
 *	cleaning the page for us (how nice!).    in this case, if we
d797 1
a797 1
 *	object we need to wait for the other PG_BUSY pages to clear
d805 1
a805 1
 *	on how many pages are in the object it may be cheaper to do one
d811 1
a811 1
 *	list traversal, so we multiply the number of pages in the
d817 1
a817 1
boolean_t
d823 1
a823 1
	struct vnode *vp = (struct vnode *)uobj;
d825 1
a825 2
	struct vm_page *pps[256], **ppsp;
	int s;
d827 1
a827 2
	boolean_t retval, need_iosync, by_list, needs_clean, all, wasclean;
	boolean_t async = (flags & PGO_SYNCIO) == 0;
a830 22
	UVMHIST_LOG(maphist, "uobj %p start 0x%x stop 0x%x flags 0x%x",
		    uobj, start, stop, flags);
	KASSERT(flags & (PGO_CLEANIT|PGO_FREE|PGO_DEACTIVATE));

	if (uobj->uo_npages == 0) {
		s = splbio();
		if (LIST_FIRST(&vp->v_dirtyblkhd) == NULL &&
		    (vp->v_bioflag & VBIOONSYNCLIST)) {
			vp->v_bioflag &= ~VBIOONSYNCLIST;
			LIST_REMOVE(vp, v_synclist);
		}
		splx(s);
		return TRUE;
	}

#ifdef DIAGNOSTIC
	if (vp->v_size == VSIZENOTSET) {
		printf("uvn_flush: size not set vp %p\n", vp);
		vprint("uvn_flush VSIZENOTSET", vp);
		flags |= PGO_ALLPAGES;
	}
#endif
d832 1
a836 4
	if (stop == 0) {
		stop = trunc_page(LLONG_MAX);
	}
	curoff = 0;
d838 1
a838 2
	retval = TRUE;
	wasclean = TRUE;
d841 1
a841 1
		by_list = TRUE;
d845 5
d851 1
a851 1
		by_list = (uobj->uo_npages <=
d873 2
a874 1
			TAILQ_FOREACH(pp, &uobj->memq, listq) {
d898 1
a898 1
		pp = TAILQ_FIRST(&uobj->memq);
d904 3
a906 3
	ppnext = NULL;
	ppsp = NULL;
	uvm_lock_pageq();
d909 3
a911 2
	for ( ; (by_list && pp != NULL) ||
		      (!by_list && curoff < stop) ; pp = ppnext) {
d913 5
d920 1
a920 1
				ppnext = TAILQ_NEXT(pp, listq);
d923 1
d925 5
d936 1
d943 1
a943 1
		 *
d952 3
a954 1
			if (!async)
a956 1

d961 1
a961 1
			if ((pp->flags & PG_CLEAN) != 0 &&
d963 1
a963 2
			    /* XXX ACTIVE|INACTIVE test unnecessary? */
			    (pp->pqflags & (PQ_ACTIVE|PQ_INACTIVE)) != 0)
d968 2
a969 1
			pp->flags |= PG_CLEANCHK;
d977 1
d979 1
a979 1
				ppnext = TAILQ_NEXT(pp, listq);
d985 1
a987 1
				    (pp->flags & PG_BUSY) == 0 &&
d989 1
a989 1
					pmap_clear_reference(pp);
d995 1
d999 1
a1015 1
		wasclean = FALSE;
d1020 1
d1025 2
a1026 2
		result = uvm_pager_put(uobj, pp, &ppsp, &npages,
				       flags | PGO_DOACTCLUST, start, stop);
d1031 2
a1032 2
		 * it is remotely possible for the async i/o to complete and
		 * the page "pp" be freed or what not before we get a chance
d1042 27
a1068 4
		 * the cleaning operation is now done.  finish up.  note that
		 * on error uvm_pager_put drops the cluster for us.
		 * on success uvm_pager_put returns the cluster to us in
		 * ppsp/npages.
d1076 1
a1076 2
		if (result == 0 && async &&
		    (flags & (PGO_DEACTIVATE|PGO_FREE)) == 0) {
d1078 16
a1093 11
			/*
			 * no per-page ops: refresh ppnext and continue
			 */
			if (by_list) {
				if (pp->version == pp_version)
					ppnext = TAILQ_NEXT(pp, listq);
				else
					ppnext = TAILQ_FIRST(&uobj->memq);
			} else {
				if (curoff < stop)
					ppnext = uvm_pagelookup(uobj, curoff);
d1095 2
a1096 1
			continue;
d1100 2
a1101 2
		 * need to look at each page of the I/O operation.  we defer
		 * processing "pp" until the last trip through this "for" loop
d1103 1
a1103 1
		 * play with the cluster pages [thus the "npages + 1" in the
d1123 1
a1123 1
						ppnext = TAILQ_NEXT(pp, listq);
d1125 2
a1126 2
						ppnext = TAILQ_FIRST(
						    &uobj->memq);
d1129 1
a1129 2
						ppnext = uvm_pagelookup(uobj,
						    curoff);
d1134 1
a1134 1
			 * verify the page wasn't moved while obj was
d1137 1
a1137 1
			if (result == 0 && async && ptmp->uobject != uobj)
d1142 1
a1142 1
			 * async I/O it is possible that the I/O op
d1147 2
a1148 2
			if (result != 0 || !async) {
				if (ptmp->flags & PG_WANTED) {
d1151 1
a1151 1
				}
d1155 2
d1158 1
a1158 4
					if (!uvn_releasepg(ptmp, NULL)) {
						UVMHIST_LOG(maphist,
							    "released %p",
							    ptmp, 0,0,0);
d1160 4
a1163 3
					}
					uvm_lock_pageq();
					continue;
d1165 3
a1167 9
					if ((flags & PGO_WEAK) == 0 &&
					    !(result == EIO &&
					      curproc == uvm.pagedaemon_proc)) {
						ptmp->flags |=
							(PG_CLEAN|PG_CLEANCHK);
						if ((flags & PGO_FREE) == 0) {
							pmap_clear_modify(ptmp);
						}
					}
d1170 1
a1170 1

a1176 1
				    (pp->flags & PG_BUSY) == 0 &&
d1178 1
a1178 1
					pmap_clear_reference(ptmp);
d1181 1
d1183 1
a1183 1
				if (result == 0 && async) {
d1188 1
a1188 1
					if (result != 0) {
d1190 2
a1191 1
						   "offset=0x%llx.  error %d\n",
d1193 1
a1193 2
						    (long long)pp->offset,
						    result);
d1203 1
d1205 1
d1208 3
d1212 4
a1215 8
	s = splbio();
	if ((flags & PGO_CLEANIT) && all && wasclean &&
	    LIST_FIRST(&vp->v_dirtyblkhd) == NULL &&
	    (vp->v_bioflag & VBIOONSYNCLIST)) {
		vp->v_bioflag &= ~VBIOONSYNCLIST;
		LIST_REMOVE(vp, v_synclist);
	}
	splx(s);
d1217 1
d1219 5
a1223 16

		/*
		 * XXX this doesn't use the new two-flag scheme,
		 * but to use that, all i/o initiators will have to change.
		 */

		s = splbio();
		while (vp->v_numoutput != 0) {
			UVMHIST_LOG(ubchist, "waiting for vp %p num %d",
				    vp, vp->v_numoutput,0,0);

	                vp->v_bioflag |= VBIOWAIT;
			UVM_UNLOCK_AND_WAIT(&vp->v_numoutput,
					    &uobj->vmobjlock,
					    FALSE, "uvn_flush",0);
			simple_lock(&uobj->vmobjlock);
d1225 3
a1227 1
		splx(s);
d1251 5
a1255 1
	struct vnode *vp = (struct vnode *)uobj;
d1257 8
a1264 2
	*loffset = offset;
	*hoffset = MIN(offset + MAXBSIZE, round_page(vp->v_size));
d1270 1
d1274 2
d1278 1
a1278 1
int
d1284 5
a1288 2
	struct vnode *vp = (struct vnode *)uobj;
	int error;
d1290 1
a1290 2
	error = VOP_PUTPAGES(vp, pps, npages, flags, NULL);
	return error;
d1304 2
a1305 2

int
d1311 1
a1311 1
	int centeridx;
a1312 1
	int advice, flags;
d1314 55
a1368 17
	struct vnode *vp = (struct vnode *)uobj;
	struct proc *p = curproc;
	int error;
	UVMHIST_FUNC("uvn_get"); UVMHIST_CALLED(ubchist);

	UVMHIST_LOG(ubchist, "vp %p off 0x%x", vp, (int)offset, 0,0);
	error = vn_lock(vp, LK_EXCLUSIVE|LK_RECURSEFAIL|LK_NOWAIT, p);
	if (error) {
		if (error == EBUSY)
			return EAGAIN;
		return error;
	}
	error = VOP_GETPAGES(vp, offset, pps, npagesp, centeridx,
		     access_type, advice, flags);
	VOP_UNLOCK(vp, LK_RELEASE, p);
	return error;
}
d1370 1
d1372 11
a1382 6
/*
 * uvn_findpages:
 * return the page for the uobj and offset requested, allocating if needed.
 * => uobj must be locked.
 * => returned page will be BUSY.
 */
d1384 4
a1387 9
void
uvn_findpages(uobj, offset, npagesp, pps, flags)
	struct uvm_object *uobj;
	voff_t offset;
	int *npagesp;
	struct vm_page **pps;
	int flags;
{
	int i, rv, npages;
d1389 6
a1394 4
	rv = 0;
	npages = *npagesp;
	for (i = 0; i < npages; i++, offset += PAGE_SIZE) {
		rv += uvn_findpage(uobj, offset, &pps[i], flags);
a1395 2
	*npagesp = rv;
}
d1397 60
a1456 25
static int
uvn_findpage(uobj, offset, pgp, flags)
	struct uvm_object *uobj;
	voff_t offset;
	struct vm_page **pgp;
	int flags;
{
	struct vm_page *pg;
	int s;
	UVMHIST_FUNC("uvn_findpage"); UVMHIST_CALLED(ubchist);
	UVMHIST_LOG(ubchist, "vp %p off 0x%lx", uobj, offset,0,0);

	if (*pgp != NULL) {
		UVMHIST_LOG(ubchist, "dontcare", 0,0,0,0);
		return 0;
	}
	for (;;) {
		/* look for an existing page */
		pg = uvm_pagelookup(uobj, offset);

		/* nope?   allocate one now */
		if (pg == NULL) {
			if (flags & UFP_NOALLOC) {
				UVMHIST_LOG(ubchist, "noalloc", 0,0,0,0);
				return 0;
d1458 6
a1463 8
			pg = uvm_pagealloc(uobj, offset, NULL, 0);
			if (pg == NULL) {
				if (flags & UFP_NOWAIT) {
					UVMHIST_LOG(ubchist, "nowait",0,0,0,0);
					return 0;
				}
				simple_unlock(&uobj->vmobjlock);
				uvm_wait("uvn_fp1");
d1465 1
a1465 1
				continue;
d1467 50
a1516 13
			if (UVM_OBJ_IS_VTEXT(uobj)) {
				uvmexp.vtextpages++;
			} else {
				uvmexp.vnodepages++;
			}
			s = splbio();
			vhold((struct vnode *)uobj);
			splx(s);
			UVMHIST_LOG(ubchist, "alloced",0,0,0,0);
			break;
		} else if (flags & UFP_NOCACHE) {
			UVMHIST_LOG(ubchist, "nocache",0,0,0,0);
			return 0;
d1519 69
a1587 11
		/* page is there, see if we need to wait on it */
		if ((pg->flags & (PG_BUSY|PG_RELEASED)) != 0) {
			if (flags & UFP_NOWAIT) {
				UVMHIST_LOG(ubchist, "nowait",0,0,0,0);
				return 0;
			}
			pg->flags |= PG_WANTED;
			UVM_UNLOCK_AND_WAIT(pg, &uobj->vmobjlock, 0,
					    "uvn_fp2", 0);
			simple_lock(&uobj->vmobjlock);
			continue;
d1589 106
d1696 8
a1703 4
		/* skip PG_RDONLY pages if requested */
		if ((flags & UFP_NORDONLY) && (pg->flags & PG_RDONLY)) {
			UVMHIST_LOG(ubchist, "nordonly",0,0,0,0);
			return 0;
d1705 10
d1716 6
a1721 5
		/* mark the page BUSY and we're done. */
		pg->flags |= PG_BUSY;
		UVM_PAGE_OWN(pg, "uvn_findpage");
		UVMHIST_LOG(ubchist, "found",0,0,0,0);
		break;
d1723 133
a1855 2
	*pgp = pg;
	return 1;
d1864 1
a1864 1
 * => we assume that the caller has a reference of some sort to the
d1881 7
a1887 3
	struct uvm_object *uobj = &vp->v_uobj;
	voff_t pgend = round_page(newsize);
	UVMHIST_FUNC("uvm_vnp_setsize"); UVMHIST_CALLED(ubchist);
d1889 4
a1892 1
	simple_lock(&uobj->vmobjlock);
d1894 7
a1900 1
	UVMHIST_LOG(ubchist, "old 0x%x new 0x%x", vp->v_size, newsize, 0,0);
d1903 1
a1903 2
	 * now check if the size has changed: if we shrink we had better
	 * toss some pages...
d1905 1
a1905 6

	if (vp->v_size > pgend && vp->v_size != VSIZENOTSET) {
		(void) uvn_flush(uobj, pgend, 0, PGO_FREE);
	}
	vp->v_size = newsize;
	simple_unlock(&uobj->vmobjlock);
d1909 8
a1916 1
 * uvm_vnp_zerorange:  set a range of bytes in a file to zero.
d1920 4
a1923 1
uvm_vnp_zerorange(vp, off, len)
d1925 98
a2022 4
	off_t off;
	size_t len;
{
        void *win;
d2024 4
a2027 14
        /*
         * XXXUBC invent kzero() and use it
         */

        while (len) {
                vsize_t bytelen = len;

                win = ubc_alloc(&vp->v_uobj, off, &bytelen, UBC_WRITE);
                memset(win, 0, bytelen);
                ubc_release(win, 0);

                off += bytelen;
                len -= bytelen;
        }
@


1.31.2.1
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_vnode.c,v 1.55 2001/11/10 07:37:01 lukem Exp $	*/
d74 15
a88 7
void	uvn_detach __P((struct uvm_object *));
int	uvn_get __P((struct uvm_object *, voff_t, struct vm_page **, int *, int,
	    vm_prot_t, int, int));
int	uvn_put __P((struct uvm_object *, voff_t, voff_t, int));
void	uvn_reference __P((struct uvm_object *));

int	uvn_findpage __P((struct uvm_object *, voff_t, struct vm_page **, int));
d99 1
d102 3
d143 1
a143 1
	 * first get a lock on the uobj.
d145 1
a145 2

	simple_lock(&uobj->vmobjlock);
d149 1
a149 1
		UVM_UNLOCK_AND_WAIT(uobj, &uobj->vmobjlock, FALSE,
a170 1

d199 1
a199 1
	if (vp->v_flag & VXWANT) {
a200 1
	}
d204 1
a204 1
		simple_unlock(&uobj->vmobjlock);
d212 1
d214 1
a214 1
	UVMHIST_LOG(maphist,"<- done, refcnt=%d", vp->v_usecount,
d216 1
a216 1
	return uobj;
d231 2
a232 1
void
a238 1

d246 1
a246 2

void
d254 552
d808 1
a808 1
 * => object must be locked on entry!   VOP_PUTPAGES must unlock it.
d814 1
a814 1
uvn_put(uobj, offlo, offhi, flags)
d816 2
a817 3
	voff_t offlo;
	voff_t offhi;
	int flags;
d822 1
a822 3
	LOCK_ASSERT(simple_lock_held(&vp->v_interlock));
	error = VOP_PUTPAGES(vp, offlo, offhi, flags);
	LOCK_ASSERT(!simple_lock_held(&vp->v_interlock));
d871 1
a871 1
 * => returned pages will be BUSY.
d875 1
a875 1
uvn_findpages(uobj, offset, npagesp, pgs, flags)
d879 1
a879 1
	struct vm_page **pgs;
d882 1
a882 1
	int i, count, npages, rv;
d884 1
a884 1
	count = 0;
d886 2
a887 16
	if (flags & UFP_BACKWARD) {
		for (i = npages - 1; i >= 0; i--, offset -= PAGE_SIZE) {
			rv = uvn_findpage(uobj, offset, &pgs[i], flags);
			if (flags & UFP_DIRTYONLY && rv == 0) {
				break;
			}
			count++;
		}
	} else {
		for (i = 0; i < npages; i++, offset += PAGE_SIZE) {
			rv = uvn_findpage(uobj, offset, &pgs[i], flags);
			if (flags & UFP_DIRTYONLY && rv == 0) {
				break;
			}
			count++;
		}
d889 1
a889 1
	*npagesp = count;
d892 1
a892 1
int
a899 1
	boolean_t dirty;
d912 1
a912 1
		/* nope?  allocate one now */
d937 1
a937 1
			UVMHIST_LOG(ubchist, "alloced %p", pg,0,0,0);
d945 1
a945 1
		if ((pg->flags & PG_BUSY) != 0) {
a962 10
		/* stop on clean pages if requested */
		if (flags & UFP_DIRTYONLY) {
			dirty = pmap_clear_modify(pg) ||
				(pg->flags & PG_CLEAN) == 0;
			pg->flags |= PG_CLEAN;
			if (!dirty) {
				return 0;
			}
		}

d966 1
a966 1
		UVMHIST_LOG(ubchist, "found %p", pg,0,0,0);
d974 1
a974 1
 * uvm_vnp_setsize: grow or shrink a vnode uobj
d982 7
d1001 2
a1002 2
	UVMHIST_LOG(ubchist, "vp %p old 0x%x new 0x%x",
	    vp, vp->v_size, newsize, 0);
d1010 1
a1010 3
		(void) uvn_put(uobj, pgend, 0, PGO_FREE);
	} else {
		simple_unlock(&uobj->vmobjlock);
d1013 1
@


1.31.2.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.31.2.1 2002/02/02 03:28:27 art Exp $	*/
d74 5
a78 5
void	uvn_detach(struct uvm_object *);
int	uvn_get(struct uvm_object *, voff_t, struct vm_page **, int *, int,
	    vm_prot_t, int, int);
int	uvn_put(struct uvm_object *, voff_t, voff_t, int);
void	uvn_reference(struct uvm_object *);
d80 1
a80 1
int	uvn_findpage(struct uvm_object *, voff_t, struct vm_page **, int);
@


1.31.2.3
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.31.2.2 2002/06/11 03:33:04 art Exp $	*/
d267 1
@


1.31.2.4
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_vnode.c,v 1.31.2.3 2002/10/29 00:36:50 art Exp $	*/
/*	$NetBSD: uvm_vnode.c,v 1.59 2002/09/06 13:24:14 gehenna Exp $	*/
d315 1
a315 1
int
d323 1
a323 1
	int i, count, found, npages, rv;
d325 1
a325 1
	count = found = 0;
d330 3
a332 5
			if (rv == 0) {
				if (flags & UFP_DIRTYONLY)
					break;
			} else
				found++;
d338 3
a340 5
			if (rv == 0) {
				if (flags & UFP_DIRTYONLY)
					break;
			} else
				found++;
a344 1
	return (found);
d386 1
a386 1
				uvmexp.execpages++;
d388 1
a388 1
				uvmexp.filepages++;
a406 1
			UVMHIST_LOG(ubchist, "wait %p", pg,0,0,0);
a424 1
				UVMHIST_LOG(ubchist, "dirtonly", 0,0,0,0);
d469 1
a469 1
		(void) uvn_put(uobj, pgend, 0, PGO_FREE | PGO_SYNCIO);
@


1.31.2.5
log
@add VEXECMAP.  also make sure to modify filepages count only in the not
execpages case in uvm_pageremove().
this actually appears to solve the swap freak out problems.  sitting on it for
a long time, never checked if it worked.  sigh.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.31.2.4 2002/11/04 18:02:33 art Exp $	*/
d389 5
@


1.30
log
@Keep track of how many pages a vnode hold with vhold and vholdrele
so that we can get back the old behavior where a vnode with cached data
is less likely to be recycled than a vnode without cached data.

XXX - This is a brute-force solution - we do it where uvmexp.vnodepages
 are changed, I am not really sure it is correct but people have been
 very happy with the diff so far and want this in the tree.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.29 2001/12/04 23:22:42 art Exp $	*/
a68 1
#include <uvm/uvm_vnode.h>
d132 1
a132 1
	struct uvm_vnode *uvn = &vp->v_uvm;
d145 3
a147 3
	simple_lock(&uvn->u_obj.vmobjlock);
	while (uvn->u_flags & VXLOCK) {
		uvn->u_flags |= VXWANT;
d149 1
a149 1
		UVM_UNLOCK_AND_WAIT(uvn, &uvn->u_obj.vmobjlock, FALSE,
d151 1
a151 1
		simple_lock(&uvn->u_obj.vmobjlock);
d159 1
a159 1
		simple_unlock(&uvn->u_obj.vmobjlock);
d169 1
a169 1
	if (uvn->u_size == VSIZENOTSET) {
d171 2
a172 2
	uvn->u_flags |= VXLOCK;
	simple_unlock(&uvn->u_obj.vmobjlock); /* drop lock in case we sleep */
d197 1
a197 1
	simple_lock(&uvn->u_obj.vmobjlock);
d199 3
a201 3
	if (uvn->u_flags & VXWANT)
		wakeup(uvn);
	uvn->u_flags &= ~(VXLOCK|VXWANT);
d204 1
a204 1
		simple_unlock(&uvn->u_obj.vmobjlock); /* drop lock */
d208 1
a208 1
	uvn->u_size = used_vnode_size;
d213 1
a213 1
	simple_unlock(&uvn->u_obj.vmobjlock);
d216 1
a216 1
	return (&uvn->u_obj);
a379 1
	struct uvm_vnode *uvn = (struct uvm_vnode *)uobj;
d406 2
a407 2
	if (uvn->u_size == VSIZENOTSET) {
		printf("uvn_flush: size not set vp %p\n", uvn);
d771 1
a771 1
					    &uvn->u_obj.vmobjlock,
d773 1
a773 1
			simple_lock(&uvn->u_obj.vmobjlock);
d799 1
a799 1
	struct uvm_vnode *uvn = (struct uvm_vnode *)uobj;
d802 1
a802 1
	*hoffset = MIN(offset + MAXBSIZE, round_page(uvn->u_size));
d996 1
a996 1
	struct uvm_vnode *uvn = &vp->v_uvm;
d1000 1
a1000 1
	simple_lock(&uvn->u_obj.vmobjlock);
d1002 1
a1002 1
	UVMHIST_LOG(ubchist, "old 0x%x new 0x%x", uvn->u_size, newsize, 0,0);
d1009 2
a1010 2
	if (uvn->u_size > pgend && uvn->u_size != VSIZENOTSET) {
		(void) uvn_flush(&uvn->u_obj, pgend, 0, PGO_FREE);
d1012 2
a1013 2
	uvn->u_size = newsize;
	simple_unlock(&uvn->u_obj.vmobjlock);
d1035 1
a1035 1
                win = ubc_alloc(&vp->v_uvm.u_obj, off, &bytelen, UBC_WRITE);
@


1.29
log
@Yet another sync to NetBSD uvm.
Today we add a pmap argument to pmap_update() and allocate map entries for
kernel_map from kmem_map instead of using the static entries. This should
get rid of MAX_KMAPENT panics. Also some uvm_loan problems are fixed.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.28 2001/12/02 23:37:52 art Exp $	*/
d902 1
d936 3
@


1.28
log
@VOP_GETPAGES expects the vnode locked. Make it so.
Note that VOP_PUTPAGES has the same problems, but the fix will be more
complicated.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_vnode.c,v 1.27 2001/11/28 19:28:15 art Exp $	*/
/*	$NetBSD: uvm_vnode.c,v 1.50 2001/05/26 21:27:21 chs Exp $	*/
d164 1
a164 6

#ifdef DIAGNOSTIC
	if (vp->v_type != VREG) {
		panic("uvn_attach: vp %p not VREG", vp);
	}
#endif
@


1.27
log
@Sync in more uvm from NetBSD. Mostly just cosmetic stuff.
Contains also support for page coloring.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.26 2001/11/28 13:47:40 art Exp $	*/
d82 1
a82 1
static int		uvn_get __P((struct uvm_object *, voff_t,
d85 1
a85 1
static int		uvn_put __P((struct uvm_object *, struct vm_page **,
d820 1
a820 1
static int
d845 1
a845 1
static int
d856 1
d861 6
d868 2
a869 1
			     access_type, advice, flags);
@


1.26
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_vnode.c,v 1.25 2001/11/27 05:27:12 art Exp $	*/
/*	$NetBSD: uvm_vnode.c,v 1.48 2001/03/10 22:46:51 chs Exp $	*/
d7 1
a7 1
 *      The Regents of the University of California.  
d27 1
a27 1
 *	Washington University, the University of California, Berkeley and 
d82 5
a86 4
static int		uvn_get __P((struct uvm_object *, voff_t, vm_page_t *,
				     int *, int, vm_prot_t, int, int));
static int		uvn_put __P((struct uvm_object *, vm_page_t *, int,
				     boolean_t));
d230 1
a230 1
 * count must already be at least one (the passed in reference) so 
d233 1
a233 1
 * => caller must call with object unlocked.  
d282 1
a282 1
	
d358 1
a358 1
 *	cleaning the page for us (how nice!).    in this case, if we 
d360 1
a360 1
 *	object we need to wait for the other PG_BUSY pages to clear 
d368 1
a368 1
 *	on how many pages are in the object it may be cheaper to do one 
d374 1
a374 1
 *	list traversal, so we multiply the number of pages in the 
d438 1
a438 1
		by_list = (uobj->uo_npages <= 
d495 1
a495 1
	for ( ; (by_list && pp != NULL) || 
d516 1
a516 1
		 * 
d533 1
a533 1
			if ((pp->flags & PG_CLEAN) != 0 && 
d594 1
a594 1
		result = uvm_pager_put(uobj, pp, &ppsp, &npages, 
d600 2
a601 2
		 * it is remotely possible for the async i/o to complete and 
		 * the page "pp" be freed or what not before we get a chance 
d641 2
a642 2
		 * need to look at each page of the I/O operation.  we defer 
		 * processing "pp" until the last trip through this "for" loop 
d644 1
a644 1
		 * play with the cluster pages [thus the "npages + 1" in the 
d718 1
a718 1
	  
d778 1
a778 1
					    &uvn->u_obj.vmobjlock, 
d844 1
a844 1
 
d951 1
a951 1
			
d974 1
a974 1
 * => we assume that the caller has a reference of some sort to the 
@


1.25
log
@Merge in the unified buffer cache code as found in NetBSD 2001/03/10. The
code is written mostly by Chuck Silvers <chuq@@chuq.com>/<chs@@netbsd.org>.

Tested for the past few weeks by many developers, should be in a pretty stable
state, but will require optimizations and additional cleanups.
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_vnode.c,v 1.47 2001/03/09 01:02:13 chs Exp $	*/
a249 2
 * => this starts the detach process, but doesn't have to finish it
 *    (async i/o could still be pending).
d301 1
a301 1
 * i/o and returns VM_PAGER_PEND.    when the i/o is done, we expect
d392 1
d524 1
a524 1
			if (flags & PGO_SYNCIO)
a588 1
ReTry:
d610 4
a613 28
		 * VM_PAGER_AGAIN: given the structure of this pager, this 
		 * can only happen when  we are doing async I/O and can't
		 * map the pages into kernel memory (pager_map) due to lack
		 * of vm space.   if this happens we drop back to sync I/O.
		 */

		if (result == VM_PAGER_AGAIN) {

			/*
			 * it is unlikely, but page could have been released
			 * while we had the object lock dropped.   we ignore
			 * this now and retry the I/O.  we will detect and
			 * handle the released page after the syncio I/O
			 * completes.
			 */
#ifdef DIAGNOSTIC
			if (flags & PGO_SYNCIO)
	panic("uvn_flush: PGO_SYNCIO return 'try again' error (impossible)");
#endif
			flags |= PGO_SYNCIO;
			goto ReTry;
		}

		/*
		 * the cleaning operation is now done.   finish up.  note that
		 * on error (!OK, !PEND) uvm_pager_put drops the cluster for us.
		 * if success (OK, PEND) then uvm_pager_put returns the cluster
		 * to us in ppsp/npages.
d621 1
a621 1
		if (result == VM_PAGER_PEND &&
d678 1
a678 1
			if (result == VM_PAGER_PEND && ptmp->uobject != uobj)
d683 1
a683 1
			 * pending I/O it is possible that the I/O op
d688 1
a688 1
			if (result != VM_PAGER_PEND) {
d707 1
a707 1
					    !(result == VM_PAGER_ERROR &&
d730 1
a730 1
				if (result == VM_PAGER_PEND) {
d735 1
a735 1
					if (result != VM_PAGER_OK) {
d829 1
a829 1
	return uvm_errno2vmerror(error);
d861 1
a861 1
	return uvm_errno2vmerror(error);
@


1.24
log
@Merge in some parts of the ubc work that has been done in NetBSD that are not
UBC, but prerequsites for it.

- Create a daemon that processes async I/O (swap and paging in the future)
  requests that need processing in process context and that were processed
  in the pagedaemon before.
- Convert some ugly ifdef DIAGNOSTIC code to less intrusive KASSERTs.
- misc other cleanups.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_vnode.c,v 1.23 2001/11/07 02:55:51 art Exp $	*/
/*	$NetBSD: uvm_vnode.c,v 1.36 2000/11/24 20:34:01 chs Exp $	*/
d55 1
d63 2
a71 15
 * private global data structure
 *
 * we keep a list of writeable active vnode-backed VM objects for sync op.
 * we keep a simpleq of vnodes that are currently being sync'd.
 */

LIST_HEAD(uvn_list_struct, uvm_vnode);
static struct uvn_list_struct uvn_wlist;	/* writeable uvns */
static simple_lock_data_t uvn_wl_lock;		/* locks uvn_wlist */

SIMPLEQ_HEAD(uvn_sq_struct, uvm_vnode);
static struct uvn_sq_struct uvn_sync_q;		/* sync'ing uvns */
lock_data_t uvn_sync_lock;			/* locks sync operation */

/*
d75 14
a88 16
static void		   uvn_cluster __P((struct uvm_object *, voff_t,
					   voff_t *, voff_t *));
static void                uvn_detach __P((struct uvm_object *));
static boolean_t           uvn_flush __P((struct uvm_object *, voff_t, 
					 voff_t, int));
static int                 uvn_get __P((struct uvm_object *, voff_t,
					vm_page_t *, int *, int, 
					vm_prot_t, int, int));
static void		   uvn_init __P((void));
static int		   uvn_io __P((struct uvm_vnode *, vm_page_t *,
				      int, int, int));
static int		   uvn_put __P((struct uvm_object *, vm_page_t *,
					int, boolean_t));
static void                uvn_reference __P((struct uvm_object *));
static boolean_t	   uvn_releasepg __P((struct vm_page *, 
					      struct vm_page **));
d95 1
a95 1
	uvn_init,
d98 1
a98 1
	NULL,			/* no specialized fault routine required */
d103 1
a103 1
	uvm_mk_pcluster, /* use generic version of this: see uvm_pager.c */
a111 16
 * uvn_init
 *
 * init pager private data structures.
 */

static void
uvn_init()
{

	LIST_INIT(&uvn_wlist);
	simple_lock_init(&uvn_wl_lock);
	/* note: uvn_sync_q init'd in uvm_vnp_sync() */
	lockinit(&uvn_sync_lock, PVM, "uvnsync", 0, 0);
}

/*
d134 1
a134 1
	int oldflags, result;
d136 1
a136 1
	u_quad_t used_vnode_size;
d140 1
a140 2

	used_vnode_size = (u_quad_t)0;	/* XXX gcc -Wuninitialized */
d146 2
a147 4
	while (uvn->u_flags & UVM_VNODE_BLOCKED) {
		printf("uvn_attach: blocked at 0x%p flags 0x%x\n",
		    uvn, uvn->u_flags);
		uvn->u_flags |= UVM_VNODE_WANTED;
d159 1
a159 1
		simple_unlock(&uvn->u_obj.vmobjlock); /* drop lock */
d164 6
d171 2
a172 4
	 * now we have lock and uvn must not be in a blocked state.
	 * first check to see if it is already active, in which case
	 * we can bump the reference count, check to see if we need to
	 * add it to the writeable list, and then return.
d174 1
a174 26
	if (uvn->u_flags & UVM_VNODE_VALID) {	/* already active? */

		/* regain VREF if we were persisting */
		if (uvn->u_obj.uo_refs == 0) {
			VREF(vp);
			UVMHIST_LOG(maphist," VREF (reclaim persisting vnode)",
			    0,0,0,0);
		}
		uvn->u_obj.uo_refs++;		/* bump uvn ref! */

		/* check for new writeable uvn */
		if ((accessprot & VM_PROT_WRITE) != 0 && 
		    (uvn->u_flags & UVM_VNODE_WRITEABLE) == 0) {
			simple_lock(&uvn_wl_lock);
			LIST_INSERT_HEAD(&uvn_wlist, uvn, u_wlist);
			simple_unlock(&uvn_wl_lock);
			/* we are now on wlist! */
			uvn->u_flags |= UVM_VNODE_WRITEABLE;
		}

		/* unlock and return */
		simple_unlock(&uvn->u_obj.vmobjlock);
		UVMHIST_LOG(maphist,"<- done, refcnt=%d", uvn->u_obj.uo_refs,
		    0, 0, 0);
		return (&uvn->u_obj);
	} 
d176 1
a176 9
	/*
	 * need to call VOP_GETATTR() to get the attributes, but that could
	 * block (due to I/O), so we want to unlock the object before calling.
	 * however, we want to keep anyone else from playing with the object
	 * while it is unlocked.   to do this we set UVM_VNODE_ALOCK which
	 * prevents anyone from attaching to the vnode until we are done with
	 * it.
	 */
	uvn->u_flags = UVM_VNODE_ALOCK;
a178 1

d192 2
a193 2
			used_vnode_size = (u_quad_t)pi.disklab->d_secsize *
			    (u_quad_t)pi.part->p_size;
d202 5
a206 1
	simple_lock(&uvn->u_obj.vmobjlock); 
a208 3
		if (uvn->u_flags & UVM_VNODE_WANTED)
			wakeup(uvn);
		uvn->u_flags = 0;
a212 20

	/*
	 * make sure that the newsize fits within a vaddr_t
	 * XXX: need to revise addressing data types
	 */
#ifdef DEBUG
	if (vp->v_type == VBLK)
		printf("used_vnode_size = %llu\n", (long long)used_vnode_size);
#endif

	/*
	 * now set up the uvn.
	 */
	uvn->u_obj.pgops = &uvm_vnodeops;
	TAILQ_INIT(&uvn->u_obj.memq);
	uvn->u_obj.uo_npages = 0;
	uvn->u_obj.uo_refs = 1;			/* just us... */
	oldflags = uvn->u_flags;
	uvn->u_flags = UVM_VNODE_VALID|UVM_VNODE_CANPERSIST;
	uvn->u_nio = 0;
a214 6
	/* if write access, we need to add it to the wlist */
	if (accessprot & VM_PROT_WRITE) {
		simple_lock(&uvn_wl_lock);
		LIST_INSERT_HEAD(&uvn_wlist, uvn, u_wlist);
		simple_unlock(&uvn_wl_lock);
		uvn->u_flags |= UVM_VNODE_WRITEABLE;	/* we are on wlist! */
d217 1
a217 6
	/*
	 * add a reference to the vnode.   this reference will stay as long
	 * as there is a valid mapping of the vnode.   dropped when the
	 * reference count goes to zero [and we either free or persist].
	 */
	VREF(vp);
d219 3
a221 5
	if (oldflags & UVM_VNODE_WANTED)
		wakeup(uvn);

	UVMHIST_LOG(maphist,"<- done/VREF, ret 0x%x", &uvn->u_obj,0,0,0);
	return(&uvn->u_obj);
d241 1
a241 17
#ifdef DEBUG
	struct uvm_vnode *uvn = (struct uvm_vnode *) uobj;
#endif
	UVMHIST_FUNC("uvn_reference"); UVMHIST_CALLED(maphist);

	simple_lock(&uobj->vmobjlock);
#ifdef DEBUG
	if ((uvn->u_flags & UVM_VNODE_VALID) == 0) {
		printf("uvn_reference: ref=%d, flags=0x%x\n", uvn->u_flags,
		    uobj->uo_refs);
		panic("uvn_reference: invalid state");
	}
#endif
	uobj->uo_refs++;
	UVMHIST_LOG(maphist, "<- done (uobj=0x%x, ref = %d)", 
	uobj, uobj->uo_refs,0,0);
	simple_unlock(&uobj->vmobjlock);
d257 1
a257 285
	struct uvm_vnode *uvn;
	struct vnode *vp;
	int oldflags;
	UVMHIST_FUNC("uvn_detach"); UVMHIST_CALLED(maphist);

	simple_lock(&uobj->vmobjlock);

	UVMHIST_LOG(maphist,"  (uobj=0x%x)  ref=%d", uobj,uobj->uo_refs,0,0);
	uobj->uo_refs--;			/* drop ref! */
	if (uobj->uo_refs) {			/* still more refs */
		simple_unlock(&uobj->vmobjlock);
		UVMHIST_LOG(maphist, "<- done (rc>0)", 0,0,0,0);
		return;
	}

	/*
	 * get other pointers ...
	 */

	uvn = (struct uvm_vnode *) uobj;
	vp = (struct vnode *) uobj;

	/*
	 * clear VTEXT flag now that there are no mappings left (VTEXT is used
	 * to keep an active text file from being overwritten).
	 */
	vp->v_flag &= ~VTEXT;

	/*
	 * we just dropped the last reference to the uvn.   see if we can
	 * let it "stick around".
	 */

	if (uvn->u_flags & UVM_VNODE_CANPERSIST) {
		/* won't block */
		uvn_flush(uobj, 0, 0, PGO_DEACTIVATE|PGO_ALLPAGES);
		simple_unlock(&uobj->vmobjlock);
		vrele(vp);			/* drop vnode reference */
		UVMHIST_LOG(maphist,"<- done/vrele!  (persist)", 0,0,0,0);
		return;
	}

	/*
	 * its a goner!
	 */

	UVMHIST_LOG(maphist,"  its a goner (flushing)!", 0,0,0,0);

	uvn->u_flags |= UVM_VNODE_DYING;

	/*
	 * even though we may unlock in flush, no one can gain a reference
	 * to us until we clear the "dying" flag [because it blocks
	 * attaches].  we will not do that until after we've disposed of all
	 * the pages with uvn_flush().  note that before the flush the only
	 * pages that could be marked PG_BUSY are ones that are in async
	 * pageout by the daemon.  (there can't be any pending "get"'s
	 * because there are no references to the object).
	 */

	(void) uvn_flush(uobj, 0, 0, PGO_CLEANIT|PGO_FREE|PGO_ALLPAGES);

	UVMHIST_LOG(maphist,"  its a goner (done flush)!", 0,0,0,0);

	/*
	 * given the structure of this pager, the above flush request will
	 * create the following state: all the pages that were in the object
	 * have either been free'd or they are marked PG_BUSY|PG_RELEASED.
	 * the PG_BUSY bit was set either by us or the daemon for async I/O.
	 * in either case, if we have pages left we can't kill the object
	 * yet because i/o is pending.  in this case we set the "relkill"
	 * flag which will cause pgo_releasepg to kill the object once all
	 * the I/O's are done [pgo_releasepg will be called from the aiodone
	 * routine or from the page daemon].
	 */

	if (uobj->uo_npages) {		/* I/O pending.  iodone will free */
#ifdef DEBUG
		/* 
		 * XXXCDC: very unlikely to happen until we have async i/o
		 * so print a little info message in case it does.
		 */
		printf("uvn_detach: vn %p has pages left after flush - "
		    "relkill mode\n", uobj);
#endif
		uvn->u_flags |= UVM_VNODE_RELKILL;
		simple_unlock(&uobj->vmobjlock);
		UVMHIST_LOG(maphist,"<- done! (releasepg will kill obj)", 0, 0,
		    0, 0);
		return;
	}

	/*
	 * kill object now.   note that we can't be on the sync q because
	 * all references are gone.
	 */
	if (uvn->u_flags & UVM_VNODE_WRITEABLE) {
		simple_lock(&uvn_wl_lock);		/* protect uvn_wlist */
		LIST_REMOVE(uvn, u_wlist);
		simple_unlock(&uvn_wl_lock);
	}
#ifdef DIAGNOSTIC
	if (uobj->memq.tqh_first != NULL)
		panic("uvn_deref: vnode VM object still has pages afer "
		    "syncio/free flush");
#endif
	oldflags = uvn->u_flags;
	uvn->u_flags = 0;
	simple_unlock(&uobj->vmobjlock);
	
	/* wake up any sleepers */
	if (oldflags & UVM_VNODE_WANTED)
		wakeup(uvn);

	/*
	 * drop our reference to the vnode.
	 */
	vrele(vp);
	UVMHIST_LOG(maphist,"<- done (vrele) final", 0,0,0,0);

	return;
}

/*
 * uvm_vnp_terminate: external hook to clear out a vnode's VM
 *
 * called in two cases:
 *  [1] when a persisting vnode vm object (i.e. one with a zero reference
 *      count) needs to be freed so that a vnode can be reused.  this
 *      happens under "getnewvnode" in vfs_subr.c.   if the vnode from
 *      the free list is still attached (i.e. not VBAD) then vgone is
 *	called.   as part of the vgone trace this should get called to
 *	free the vm object.   this is the common case.
 *  [2] when a filesystem is being unmounted by force (MNT_FORCE, 
 *	"umount -f") the vgone() function is called on active vnodes
 *	on the mounted file systems to kill their data (the vnodes become
 *	"dead" ones [see src/sys/miscfs/deadfs/...]).  that results in a
 *	call here (even if the uvn is still in use -- i.e. has a non-zero
 *	reference count).  this case happens at "umount -f" and during a
 *	"reboot/halt" operation.
 *
 * => the caller must XLOCK and VOP_LOCK the vnode before calling us
 *	[protects us from getting a vnode that is already in the DYING
 *	 state...]
 * => unlike uvn_detach, this function must not return until all the
 *	uvn's pages are disposed of.
 * => in case [2] the uvn is still alive after this call, but all I/O
 *	ops will fail (due to the backing vnode now being "dead").  this
 *	will prob. kill any process using the uvn due to pgo_get failing.
 */

void
uvm_vnp_terminate(vp)
	struct vnode *vp;
{
	struct uvm_vnode *uvn = &vp->v_uvm;
	int oldflags;
	UVMHIST_FUNC("uvm_vnp_terminate"); UVMHIST_CALLED(maphist);

	/*
	 * lock object and check if it is valid
	 */
	simple_lock(&uvn->u_obj.vmobjlock);
	UVMHIST_LOG(maphist, "  vp=0x%x, ref=%d, flag=0x%x", vp,
	    uvn->u_obj.uo_refs, uvn->u_flags, 0);
	if ((uvn->u_flags & UVM_VNODE_VALID) == 0) {
		simple_unlock(&uvn->u_obj.vmobjlock);
		UVMHIST_LOG(maphist, "<- done (not active)", 0, 0, 0, 0);
		return;
	}

	/*
	 * must be a valid uvn that is not already dying (because XLOCK
	 * protects us from that).   the uvn can't in the ALOCK state
	 * because it is valid, and uvn's that are in the ALOCK state haven't
	 * been marked valid yet.
	 */

#ifdef DEBUG
	/*
	 * debug check: are we yanking the vnode out from under our uvn?
	 */
	if (uvn->u_obj.uo_refs) {
		printf("uvm_vnp_terminate(%p): terminating active vnode "
		    "(refs=%d)\n", uvn, uvn->u_obj.uo_refs);
	} 
#endif
	
	/*
	 * it is possible that the uvn was detached and is in the relkill
	 * state [i.e. waiting for async i/o to finish so that releasepg can
	 * kill object].  we take over the vnode now and cancel the relkill.
	 * we want to know when the i/o is done so we can recycle right
	 * away.   note that a uvn can only be in the RELKILL state if it
	 * has a zero reference count.
	 */
	
	if (uvn->u_flags & UVM_VNODE_RELKILL)
		uvn->u_flags &= ~UVM_VNODE_RELKILL;	/* cancel RELKILL */

	/*
	 * block the uvn by setting the dying flag, and then flush the
	 * pages.  (note that flush may unlock object while doing I/O, but
	 * it will re-lock it before it returns control here).
	 *
	 * also, note that we tell I/O that we are already VOP_LOCK'd so
	 * that uvn_io doesn't attempt to VOP_LOCK again.
	 *
	 * XXXCDC: setting VNISLOCKED on an active uvn which is being terminated
	 *	due to a forceful unmount might not be a good idea.  maybe we
	 *	need a way to pass in this info to uvn_flush through a
	 *	pager-defined PGO_ constant [currently there are none].
	 */
	uvn->u_flags |= UVM_VNODE_DYING|UVM_VNODE_VNISLOCKED;

	(void) uvn_flush(&uvn->u_obj, 0, 0, PGO_CLEANIT|PGO_FREE|PGO_ALLPAGES);

	/*
	 * as we just did a flush we expect all the pages to be gone or in 
	 * the process of going.  sleep to wait for the rest to go [via iosync].
	 */

	while (uvn->u_obj.uo_npages) {
#ifdef DEBUG
		struct vm_page *pp;
		for (pp = uvn->u_obj.memq.tqh_first ; pp != NULL ; 
		     pp = pp->listq.tqe_next) {
			if ((pp->flags & PG_BUSY) == 0)
				panic("uvm_vnp_terminate: detected unbusy pg");
		}
		if (uvn->u_nio == 0)
			panic("uvm_vnp_terminate: no I/O to wait for?");
		printf("uvm_vnp_terminate: waiting for I/O to fin.\n");
		/* 
		 * XXXCDC: this is unlikely to happen without async i/o so we 
		 * put a printf in just to keep an eye on it.
		 */
#endif
		uvn->u_flags |= UVM_VNODE_IOSYNC;
		UVM_UNLOCK_AND_WAIT(&uvn->u_nio, &uvn->u_obj.vmobjlock, FALSE, 
		    "uvn_term",0);
		simple_lock(&uvn->u_obj.vmobjlock);
	}

	/*
	 * done.   now we free the uvn if its reference count is zero
	 * (true if we are zapping a persisting uvn).   however, if we are
	 * terminating a uvn with active mappings we let it live ... future
	 * calls down to the vnode layer will fail.
	 */

	oldflags = uvn->u_flags;
	if (uvn->u_obj.uo_refs) {

		/*
		 * uvn must live on it is dead-vnode state until all references 
		 * are gone.   restore flags.    clear CANPERSIST state.
		 */

		uvn->u_flags &= ~(UVM_VNODE_DYING|UVM_VNODE_VNISLOCKED|
		      UVM_VNODE_WANTED|UVM_VNODE_CANPERSIST);
	
	} else {

		/*
		 * free the uvn now.   note that the VREF reference is already
		 * gone [it is dropped when we enter the persist state].
		 */
		if (uvn->u_flags & UVM_VNODE_IOSYNCWANTED)
			panic("uvm_vnp_terminate: io sync wanted bit set");

		if (uvn->u_flags & UVM_VNODE_WRITEABLE) {
			simple_lock(&uvn_wl_lock);
			LIST_REMOVE(uvn, u_wlist);
			simple_unlock(&uvn_wl_lock);
		}
		uvn->u_flags = 0;	/* uvn is history, clear all bits */
	}

	if (oldflags & UVM_VNODE_WANTED)
		wakeup(uvn);		/* object lock still held */

	simple_unlock(&uvn->u_obj.vmobjlock);
	UVMHIST_LOG(maphist, "<- done", 0, 0, 0, 0);

d270 1
a270 1
 * => if (nextpgp != NULL) => we return pageq.tqe_next here, and return
d282 1
a282 5
	struct uvm_vnode *uvn = (struct uvm_vnode *) pg->uobject;
#ifdef DIAGNOSTIC
	if ((pg->flags & PG_RELEASED) == 0)
		panic("uvn_releasepg: page not released!");
#endif
d290 1
a290 1
		*nextpgp = pg->pageq.tqe_next;	/* next page for daemon */
a294 26
	/*
	 * now see if we need to kill the object
	 */
	if (uvn->u_flags & UVM_VNODE_RELKILL) {
		if (uvn->u_obj.uo_refs)
			panic("uvn_releasepg: kill flag set on referenced "
			    "object!");
		if (uvn->u_obj.uo_npages == 0) {
			if (uvn->u_flags & UVM_VNODE_WRITEABLE) {
				simple_lock(&uvn_wl_lock);
				LIST_REMOVE(uvn, u_wlist);
				simple_unlock(&uvn_wl_lock);
			}
#ifdef DIAGNOSTIC
			if (uvn->u_obj.memq.tqh_first)
	panic("uvn_releasepg: pages in object with npages == 0");
#endif
			if (uvn->u_flags & UVM_VNODE_WANTED)
				/* still holding object lock */
				wakeup(uvn);

			uvn->u_flags = 0;		/* DEAD! */
			simple_unlock(&uvn->u_obj.vmobjlock);
			return (FALSE);
		}
	}
a298 6
 * NOTE: currently we have to use VOP_READ/VOP_WRITE because they go
 * through the buffer cache and allow I/O in any size.  These VOPs use
 * synchronous i/o.  [vs. VOP_STRATEGY which can be async, but doesn't
 * go through the buffer cache or allow I/O sizes larger than a
 * block].  we will eventually want to change this.
 *
a299 1
 *   uvm provides the uvm_aiodesc structure for async i/o management.
d302 1
a302 1
 * an aiodesc on the "pending" list (protected by splbio()), starts the
d305 1
a305 1
 * time).   this function should remove the aiodesc from the pending list
d308 1
a308 7
 * list and call the "aiodone" hook for each done request (see uvm_pager.c).
 * [in the old vm code, this was done by calling the "put" routine with
 * null arguments which made the code harder to read and understand because
 * you had one function ("put") doing two things.]  
 *
 * so the current pager needs: 
 *   int uvn_aiodone(struct uvm_aiodesc *)
d329 1
d331 2
a332 1
 *	if (and only if) we need to clean a page (PGO_CLEANIT).
d334 6
a339 6
 * => if PGO_CLEANIT is set, we may block (due to I/O).   thus, a caller
 *	might want to unlock higher level resources (e.g. vm_map)
 *	before calling flush.
 * => if PGO_CLEANIT is not set, then we will neither unlock the object
 *	or block.
 * => if PGO_ALLPAGE is set, then all pages in the object are valid targets
d381 1
a381 1
static boolean_t
d387 2
a388 1
	struct uvm_vnode *uvn = (struct uvm_vnode *) uobj;
d390 2
a391 1
	struct vm_page *pps[MAXBSIZE >> PAGE_SHIFT], **ppsp;
d393 1
a393 1
	boolean_t retval, need_iosync, by_list, needs_clean, all;
d397 22
a419 1
	curoff = 0;	/* XXX: shut up gcc */
d424 4
d429 2
a430 1
	retval = TRUE;		/* return value */
d433 1
a433 1
		by_list = TRUE;		/* always go by the list */
a436 5
#ifdef DEBUG
		if (stop > round_page(uvn->u_size))
			printf("uvn_flush: strange, got an out of range "
			    "flush (fixed)\n");
#endif
d460 1
a460 2
			for (pp = uobj->memq.tqh_first ; pp != NULL ;
			    pp = pp->listq.tqe_next) {
d484 1
a484 1
		pp = uobj->memq.tqh_first;
d490 3
a492 3
	ppnext = NULL;	/* XXX: shut up gcc */ 
	ppsp = NULL;		/* XXX: shut up gcc */
	uvm_lock_pageq();	/* page queues locked */
d496 1
a496 2
	  (!by_list && curoff < stop) ; pp = ppnext) {

a497 5

			/*
			 * range check
			 */

d500 1
a500 1
				ppnext = pp->listq.tqe_next;
a502 1

a503 5

			/*
			 * null check
			 */

a509 1

d525 1
a525 3
			if ((pp->flags & PG_BUSY) != 0 &&
			    (flags & (PGO_CLEANIT|PGO_SYNCIO)) ==
			             (PGO_CLEANIT|PGO_SYNCIO))
d528 1
d535 2
a536 1
			    (pp->pqflags & PQ_ACTIVE) != 0)
d541 1
a541 2
			pp->flags |= PG_CLEANCHK;	/* update "hint" */

a548 1
			/* load ppnext */
d550 1
a550 1
				ppnext = pp->listq.tqe_next;
a555 1
			/* now dispose of pp */
d558 1
d560 1
a560 1
					pmap_page_protect(pp, VM_PROT_NONE);
a565 1
					/* release busy pages */
a568 1
					/* removed page from object */
d585 1
d596 1
a596 1
			   flags | PGO_DOACTCLUST, start, stop);
d619 2
a620 1
			/* 
d647 2
a648 1
		if (result == VM_PAGER_PEND) {
d650 11
a660 16
			if ((flags & (PGO_DEACTIVATE|PGO_FREE)) == 0) {
				/*
				 * no per-page ops: refresh ppnext and continue
				 */
				if (by_list) {
					if (pp->version == pp_version)
						ppnext = pp->listq.tqe_next;
					else
						/* reset */
						ppnext = uobj->memq.tqh_first;
				} else {
					if (curoff < stop)
						ppnext = uvm_pagelookup(uobj,
						    curoff);
				}
				continue;
d662 1
a662 2

			/* need to do anything here? */
d689 1
a689 1
						ppnext = pp->listq.tqe_next;
d691 2
a692 2
						/* reset */
						ppnext = uobj->memq.tqh_first;
d695 2
a696 1
					ppnext = uvm_pagelookup(uobj, curoff);
d701 1
a701 1
			 * verify the page didn't get moved while obj was
d715 1
a715 1
				if (ptmp->flags & PG_WANTED)
d718 1
a718 1

a721 2

					/* pgo_releasepg wants this */
d723 4
a726 1
					if (!uvn_releasepg(ptmp, NULL))
d728 3
a730 4

					uvm_lock_pageq();	/* relock */
					continue;		/* next page */

d732 9
a740 3
					ptmp->flags |= (PG_CLEAN|PG_CLEANCHK);
					if ((flags & PGO_FREE) == 0)
						pmap_clear_modify(ptmp);
d750 1
d752 1
a752 1
					pmap_page_protect(ptmp, VM_PROT_NONE);
a754 1

d763 1
a763 2
						   "offset=0x%llx.  error "
						   "during pageout.\n",
d765 2
a766 1
						    (long long)pp->offset);
a775 1

a776 1

a778 3
	/*
	 * done with pagequeues: unlock
	 */
d780 10
d791 4
a794 4
	/*
	 * now wait for all I/O if required.
	 */
	if (need_iosync) {
d796 9
a804 5
		UVMHIST_LOG(maphist,"  <<DOING IOSYNC>>",0,0,0,0);
		while (uvn->u_nio != 0) {
			uvn->u_flags |= UVM_VNODE_IOSYNC;
			UVM_UNLOCK_AND_WAIT(&uvn->u_nio, &uvn->u_obj.vmobjlock, 
			  FALSE, "uvn_flush",0);
d807 1
a807 3
		if (uvn->u_flags & UVM_VNODE_IOSYNCWANTED)
			wakeup(&uvn->u_flags);
		uvn->u_flags &= ~(UVM_VNODE_IOSYNC|UVM_VNODE_IOSYNCWANTED);
d831 2
a832 1
	struct uvm_vnode *uvn = (struct uvm_vnode *) uobj;
d834 1
a834 12

	if (*loffset >= uvn->u_size)
		panic("uvn_cluster: offset out of range");

	/*
	 * XXX: old pager claims we could use VOP_BMAP to get maxcontig value.
	 */
	*hoffset = *loffset + MAXBSIZE;
	if (*hoffset > round_page(uvn->u_size))	/* past end? */
		*hoffset = round_page(uvn->u_size);

	return;
a839 1
 * => prefer map unlocked (not required)
a842 2
 * => XXX: currently we use VOP_READ/VOP_WRITE which are only sync.
 *	[thus we never do async i/o!  see iodone comment]
d851 2
a852 1
	int retval;
d854 2
a855 5
	/* note: object locked */
	retval = uvn_io((struct uvm_vnode*)uobj, pps, npages, flags, UIO_WRITE);
	/* note: object unlocked */

	return(retval);
d876 1
a876 1
	int centeridx, advice, flags;
d878 1
d880 9
a888 6
	voff_t current_offset;
	struct vm_page *ptmp;
	int lcv, result, gotpages;
	boolean_t done;
	UVMHIST_FUNC("uvn_get"); UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist, "flags=%d", flags,0,0,0);
a889 3
	/*
	 * step 1: handled the case where fault data structures are locked.
	 */
d891 6
a896 1
	if (flags & PGO_LOCKED) {
d898 9
a906 4
		/*
		 * gotpages is the current number of pages we've gotten (which
		 * we pass back up to caller via *npagesp.
		 */
d908 7
a914 1
		gotpages = 0;
d916 10
a925 10
		/*
		 * step 1a: get pages that are already resident.   only do this
		 * if the data structures are locked (i.e. the first time
		 * through).
		 */

		done = TRUE;	/* be optimistic */

		for (lcv = 0, current_offset = offset ; lcv < *npagesp ;
		    lcv++, current_offset += PAGE_SIZE) {
d927 13
a939 14
			/* do we care about this page?  if not, skip it */
			if (pps[lcv] == PGO_DONTCARE)
				continue;

			/* lookup page */
			ptmp = uvm_pagelookup(uobj, current_offset);

			/* to be useful must get a non-busy, non-released pg */
			if (ptmp == NULL ||
			    (ptmp->flags & (PG_BUSY|PG_RELEASED)) != 0) {
				if (lcv == centeridx || (flags & PGO_ALLPAGES)
				    != 0)
				done = FALSE;	/* need to do a wait or I/O! */
				continue;
d941 5
a945 90

			/*
			 * useful page: busy/lock it and plug it in our
			 * result array
			 */
			ptmp->flags |= PG_BUSY;		/* loan up to caller */
			UVM_PAGE_OWN(ptmp, "uvn_get1");
			pps[lcv] = ptmp;
			gotpages++;

		}	/* "for" lcv loop */

		/*
		 * XXX: given the "advice", should we consider async read-ahead?
		 * XXX: fault current does deactive of pages behind us.  is 
		 * this good (other callers might now).
		 */
		/* 
		 * XXX: read-ahead currently handled by buffer cache (bread)
		 * level.
		 * XXX: no async i/o available.
		 * XXX: so we don't do anything now.
		 */

		/*
		 * step 1c: now we've either done everything needed or we to
		 * unlock and do some waiting or I/O.
		 */

		*npagesp = gotpages;		/* let caller know */
		if (done)
			return(VM_PAGER_OK);		/* bingo! */
		else
			/* EEK!   Need to unlock and I/O */
			return(VM_PAGER_UNLOCK);
	}

	/*
	 * step 2: get non-resident or busy pages.
	 * object is locked.   data structures are unlocked.
	 *
	 * XXX: because we can't do async I/O at this level we get things
	 * page at a time (otherwise we'd chunk).   the VOP_READ() will do 
	 * async-read-ahead for us at a lower level.
	 */

	for (lcv = 0, current_offset = offset ; 
			 lcv < *npagesp ; lcv++, current_offset += PAGE_SIZE) {
		
		/* skip over pages we've already gotten or don't want */
		/* skip over pages we don't _have_ to get */
		if (pps[lcv] != NULL || (lcv != centeridx &&
		    (flags & PGO_ALLPAGES) == 0))
			continue;

		/*
		 * we have yet to locate the current page (pps[lcv]).   we first
		 * look for a page that is already at the current offset.   if
		 * we fine a page, we check to see if it is busy or released.
		 * if that is the case, then we sleep on the page until it is
		 * no longer busy or released and repeat the lookup.    if the
		 * page we found is neither busy nor released, then we busy it
		 * (so we own it) and plug it into pps[lcv].   this breaks the
		 * following while loop and indicates we are ready to move on
		 * to the next page in the "lcv" loop above.
		 *
		 * if we exit the while loop with pps[lcv] still set to NULL,
		 * then it means that we allocated a new busy/fake/clean page
		 * ptmp in the object and we need to do I/O to fill in the data.
		 */

		while (pps[lcv] == NULL) {	/* top of "pps" while loop */
			
			/* look for a current page */
			ptmp = uvm_pagelookup(uobj, current_offset);

			/* nope?   allocate one now (if we can) */
			if (ptmp == NULL) {

				ptmp = uvm_pagealloc(uobj, current_offset,
				    NULL, 0);

				/* out of RAM? */
				if (ptmp == NULL) {
					simple_unlock(&uobj->vmobjlock);
					uvm_wait("uvn_getpage");
					simple_lock(&uobj->vmobjlock);

					/* goto top of pps while loop */
					continue;	
d947 4
a950 6

				/* 
				 * got new page ready for I/O.  break pps
				 * while loop.  pps[lcv] is still NULL.
				 */
				break;		
d952 4
a955 8

			/* page is there, see if we need to wait on it */
			if ((ptmp->flags & (PG_BUSY|PG_RELEASED)) != 0) {
				ptmp->flags |= PG_WANTED;
				UVM_UNLOCK_AND_WAIT(ptmp,
				    &uobj->vmobjlock, FALSE, "uvn_get",0);
				simple_lock(&uobj->vmobjlock);
				continue;	/* goto top of pps while loop */
d957 5
a961 10
			
			/* 
			 * if we get here then the page has become resident
			 * and unbusy between steps 1 and 2.  we busy it
			 * now (so we own it) and set pps[lcv] (so that we
			 * exit the while loop).
			 */
			ptmp->flags |= PG_BUSY;
			UVM_PAGE_OWN(ptmp, "uvn_get2");
			pps[lcv] = ptmp;
d964 11
a974 38
		/*
		 * if we own the a valid page at the correct offset, pps[lcv]
		 * will point to it.   nothing more to do except go to the
		 * next page.
		 */

		if (pps[lcv])
			continue;			/* next lcv */

		/*
		 * we have a "fake/busy/clean" page that we just allocated.  do
		 * I/O to fill it with valid data.  note that object must be
		 * locked going into uvn_io, but will be unlocked afterwards.
		 */

		result = uvn_io((struct uvm_vnode *) uobj, &ptmp, 1,
		    PGO_SYNCIO, UIO_READ);

		/*
		 * I/O done.   object is unlocked (by uvn_io).   because we used
		 * syncio the result can not be PEND or AGAIN.   we must relock
		 * and check for errors.
		 */

		/* lock object.   check for errors.   */
		simple_lock(&uobj->vmobjlock);
		if (result != VM_PAGER_OK) {
			if (ptmp->flags & PG_WANTED)
				/* object lock still held */
				wakeup(ptmp);

			ptmp->flags &= ~(PG_WANTED|PG_BUSY);
			UVM_PAGE_OWN(ptmp, NULL);
			uvm_lock_pageq();
			uvm_pagefree(ptmp);
			uvm_unlock_pageq();
			simple_unlock(&uobj->vmobjlock);
			return(result);
d976 5
a980 186

		/* 
		 * we got the page!   clear the fake flag (indicates valid
		 * data now in page) and plug into our result array.   note
		 * that page is still busy.   
		 *
		 * it is the callers job to:
		 * => check if the page is released
		 * => unbusy the page
		 * => activate the page
		 */

		ptmp->flags &= ~PG_FAKE;		/* data is valid ... */
		pmap_clear_modify(ptmp);		/* ... and clean */
		pps[lcv] = ptmp;

	}	/* lcv loop */

	/*
	 * finally, unlock object and return.
	 */

	simple_unlock(&uobj->vmobjlock);
	return (VM_PAGER_OK);
}

/*
 * uvn_io: do I/O to a vnode
 *
 * => prefer map unlocked (not required)
 * => object must be locked!   we will _unlock_ it before starting I/O.
 * => flags: PGO_SYNCIO -- use sync. I/O
 * => XXX: currently we use VOP_READ/VOP_WRITE which are only sync.
 *	[thus we never do async i/o!  see iodone comment]
 */

static int
uvn_io(uvn, pps, npages, flags, rw)
	struct uvm_vnode *uvn;
	vm_page_t *pps;
	int npages, flags, rw;
{
	struct vnode *vn;
	struct uio uio;
	struct iovec iov;
	vaddr_t kva;
	off_t file_offset;
	int waitf, result, mapinflags;
	size_t got, wanted;
	UVMHIST_FUNC("uvn_io"); UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist, "rw=%d", rw,0,0,0);
	
	/*
	 * init values
	 */

	waitf = (flags & PGO_SYNCIO) ? M_WAITOK : M_NOWAIT;
	vn = (struct vnode *) uvn;
	file_offset = pps[0]->offset;
	
	/*
	 * check for sync'ing I/O.
	 */
	
	while (uvn->u_flags & UVM_VNODE_IOSYNC) {
		if (waitf == M_NOWAIT) { 
			simple_unlock(&uvn->u_obj.vmobjlock);
			UVMHIST_LOG(maphist,"<- try again (iosync)",0,0,0,0);
			return(VM_PAGER_AGAIN);
		}
		uvn->u_flags |= UVM_VNODE_IOSYNCWANTED;
		UVM_UNLOCK_AND_WAIT(&uvn->u_flags, &uvn->u_obj.vmobjlock, 
			FALSE, "uvn_iosync",0);
		simple_lock(&uvn->u_obj.vmobjlock);
	}

	/*
	 * check size
	 */

	if (file_offset >= uvn->u_size) {
			simple_unlock(&uvn->u_obj.vmobjlock);
			UVMHIST_LOG(maphist,"<- BAD (size check)",0,0,0,0);
			return(VM_PAGER_BAD);
	}

	/*
	 * first try and map the pages in (without waiting)
	 */

	mapinflags = (rw == UIO_READ) ?
	    UVMPAGER_MAPIN_READ : UVMPAGER_MAPIN_WRITE;

	kva = uvm_pagermapin(pps, npages, mapinflags);
	if (kva == 0 && waitf == M_NOWAIT) {
		simple_unlock(&uvn->u_obj.vmobjlock);
		UVMHIST_LOG(maphist,"<- mapin failed (try again)",0,0,0,0);
		return(VM_PAGER_AGAIN);
	}

	/*
	 * ok, now bump u_nio up.   at this point we are done with uvn
	 * and can unlock it.   if we still don't have a kva, try again
	 * (this time with sleep ok).
	 */
	
	uvn->u_nio++;			/* we have an I/O in progress! */
	simple_unlock(&uvn->u_obj.vmobjlock);
	/* NOTE: object now unlocked */
	if (kva == 0)
		kva = uvm_pagermapin(pps, npages,
		    mapinflags | UVMPAGER_MAPIN_WAITOK);

	/*
	 * ok, mapped in.  our pages are PG_BUSY so they are not going to
	 * get touched (so we can look at "offset" without having to lock
	 * the object).  set up for I/O.
	 */

	/*
	 * fill out uio/iov
	 */
	
	iov.iov_base = (caddr_t) kva;
	wanted = npages << PAGE_SHIFT;
	if (file_offset + wanted > uvn->u_size)
		wanted = uvn->u_size - file_offset;	/* XXX: needed? */
	iov.iov_len = wanted;
	uio.uio_iov = &iov;
	uio.uio_iovcnt = 1;
	uio.uio_offset = file_offset;
	uio.uio_segflg = UIO_SYSSPACE;
	uio.uio_rw = rw;
	uio.uio_resid = wanted;
	uio.uio_procp = curproc;

	/*
	 * do the I/O!  (XXX: curproc?)
	 */

	UVMHIST_LOG(maphist, "calling VOP",0,0,0,0);

	/*
	 * This process may already have this vnode locked, if we faulted in
	 * copyin() or copyout() on a region backed by this vnode
	 * while doing I/O to the vnode.  If this is the case, don't
	 * panic.. instead, return the error to the user.
	 *
	 * XXX this is a stopgap to prevent a panic.
	 * Ideally, this kind of operation *should* work.
	 */
	result = 0;
	if ((uvn->u_flags & UVM_VNODE_VNISLOCKED) == 0)
		result = vn_lock(vn, LK_EXCLUSIVE | LK_RETRY | LK_RECURSEFAIL, curproc);

	if (result == 0) {
		/* NOTE: vnode now locked! */

		if (rw == UIO_READ)
			result = VOP_READ(vn, &uio, 0, curproc->p_ucred);
		else
			result = VOP_WRITE(vn, &uio, 0, curproc->p_ucred);

		if ((uvn->u_flags & UVM_VNODE_VNISLOCKED) == 0)
			VOP_UNLOCK(vn, 0, curproc);
	}
	
	/* NOTE: vnode now unlocked (unless vnislocked) */

	UVMHIST_LOG(maphist, "done calling VOP",0,0,0,0);

	/*
	 * result == unix style errno (0 == OK!)
	 *
	 * zero out rest of buffer (if needed)
	 */

	if (result == 0) {
		got = wanted - uio.uio_resid;

		if (wanted && got == 0) {
			result = EIO;		/* XXX: error? */
		} else if (got < PAGE_SIZE * npages && rw == UIO_READ) {
			memset((void *) (kva + got), 0,
			       (npages << PAGE_SHIFT) - got);
a981 10
	}

	/*
	 * now remove pager mapping
	 */
	uvm_pagermapout(kva, npages);
		
	/*
	 * now clean up the object (i.e. drop I/O count)
	 */
d983 5
a987 6
	simple_lock(&uvn->u_obj.vmobjlock);
	/* NOTE: object now locked! */

	uvn->u_nio--;			/* I/O DONE! */
	if ((uvn->u_flags & UVM_VNODE_IOSYNC) != 0 && uvn->u_nio == 0) {
		wakeup(&uvn->u_nio);
d989 2
a990 133
	simple_unlock(&uvn->u_obj.vmobjlock);
	/* NOTE: object now unlocked! */

	/*
	 * done!
	 */

	UVMHIST_LOG(maphist, "<- done (result %d)", result,0,0,0);
	if (result == 0)
		return(VM_PAGER_OK);
	else
		return(VM_PAGER_ERROR);
}

/*
 * uvm_vnp_uncache: disable "persisting" in a vnode... when last reference
 * is gone we will kill the object (flushing dirty pages back to the vnode
 * if needed).
 *
 * => returns TRUE if there was no uvm_object attached or if there was
 *	one and we killed it [i.e. if there is no active uvn]
 * => called with the vnode VOP_LOCK'd [we will unlock it for I/O, if
 *	needed]
 *
 * => XXX: given that we now kill uvn's when a vnode is recycled (without
 *	having to hold a reference on the vnode) and given a working
 *	uvm_vnp_sync(), how does that effect the need for this function?
 *      [XXXCDC: seems like it can die?]
 *
 * => XXX: this function should DIE once we merge the VM and buffer 
 *	cache.
 *
 * research shows that this is called in the following places:
 * ext2fs_truncate, ffs_truncate, detrunc[msdosfs]: called when vnode
 *	changes sizes
 * ext2fs_write, WRITE [ufs_readwrite], msdosfs_write: called when we
 *	are written to
 * ex2fs_chmod, ufs_chmod: called if VTEXT vnode and the sticky bit
 *	is off
 * ffs_realloccg: when we can't extend the current block and have 
 *	to allocate a new one we call this [XXX: why?]
 * nfsrv_rename, rename_files: called when the target filename is there
 *	and we want to remove it
 * nfsrv_remove, sys_unlink: called on file we are removing
 * nfsrv_access: if VTEXT and we want WRITE access and we don't uncache
 *	then return "text busy"
 * nfs_open: seems to uncache any file opened with nfs
 * vn_writechk: if VTEXT vnode and can't uncache return "text busy"
 */

boolean_t
uvm_vnp_uncache(vp)
	struct vnode *vp;
{
	struct uvm_vnode *uvn = &vp->v_uvm;

	/*
	 * lock uvn part of the vnode and check to see if we need to do anything
	 */

	simple_lock(&uvn->u_obj.vmobjlock);
	if ((uvn->u_flags & UVM_VNODE_VALID) == 0 || 
			(uvn->u_flags & UVM_VNODE_BLOCKED) != 0) {
		simple_unlock(&uvn->u_obj.vmobjlock);
		return(TRUE);
	}

	/*
	 * we have a valid, non-blocked uvn.   clear persist flag.
	 * if uvn is currently active we can return now.
	 */

	uvn->u_flags &= ~UVM_VNODE_CANPERSIST;
	if (uvn->u_obj.uo_refs) {
		simple_unlock(&uvn->u_obj.vmobjlock);
		return(FALSE);
	}

	/*
	 * uvn is currently persisting!   we have to gain a reference to
	 * it so that we can call uvn_detach to kill the uvn.
	 */

	VREF(vp);			/* seems ok, even with VOP_LOCK */
	uvn->u_obj.uo_refs++;		/* value is now 1 */
	simple_unlock(&uvn->u_obj.vmobjlock);


#ifdef DEBUG
	/*
	 * carry over sanity check from old vnode pager: the vnode should
	 * be VOP_LOCK'd, and we confirm it here.
	 */
	if (!VOP_ISLOCKED(vp)) {
		boolean_t is_ok_anyway = FALSE;
#if defined(NFSCLIENT)
		extern int (**nfsv2_vnodeop_p) __P((void *));
		extern int (**spec_nfsv2nodeop_p) __P((void *));
		extern int (**fifo_nfsv2nodeop_p) __P((void *));

		/* vnode is NOT VOP_LOCKed: some vnode types _never_ lock */
		if (vp->v_op == nfsv2_vnodeop_p ||
		    vp->v_op == spec_nfsv2nodeop_p) {
			is_ok_anyway = TRUE;
		}
		if (vp->v_op == fifo_nfsv2nodeop_p) {
			is_ok_anyway = TRUE;
		}
#endif	/* defined(NFSSERVER) || defined(NFSCLIENT) */
		if (!is_ok_anyway)
			panic("uvm_vnp_uncache: vnode not locked!");
	}
#endif	/* DEBUG */

	/*
	 * now drop our reference to the vnode.   if we have the sole 
	 * reference to the vnode then this will cause it to die [as we
	 * just cleared the persist flag].   we have to unlock the vnode 
	 * while we are doing this as it may trigger I/O.
	 *
	 * XXX: it might be possible for uvn to get reclaimed while we are
	 * unlocked causing us to return TRUE when we should not.   we ignore
	 * this as a false-positive return value doesn't hurt us.
	 */
	VOP_UNLOCK(vp, 0, curproc);
	uvn_detach(&uvn->u_obj);
	vn_lock(vp, LK_EXCLUSIVE | LK_RETRY, curproc);
	
	/*
	 * and return...
	 */
	
	return(TRUE);
d1017 6
d1025 2
a1026 1
	 * lock uvn and check for valid object, and if valid: do it!
a1027 7
	simple_lock(&uvn->u_obj.vmobjlock);
	if (uvn->u_flags & UVM_VNODE_VALID) {

		/*
		 * now check if the size has changed: if we shrink we had better
		 * toss some pages...
		 */
d1029 2
a1030 5
		if (uvn->u_size > newsize) {
			(void)uvn_flush(&uvn->u_obj, newsize,
			    uvn->u_size, PGO_FREE);
		}
		uvn->u_size = newsize;
d1032 1
a1033 5

	/*
	 * done
	 */
	return;
d1037 1
a1037 8
 * uvm_vnp_sync: flush all dirty VM pages back to their backing vnodes.
 *
 * => called from sys_sync with no VM structures locked
 * => only one process can do a sync at a time (because the uvn
 *    structure only has one queue for sync'ing).  we ensure this
 *    by holding the uvn_sync_lock while the sync is in progress.
 *    other processes attempting a sync will sleep on this lock
 *    until we are done.
d1041 4
a1044 2
uvm_vnp_sync(mp)
	struct mount *mp;
d1046 1
a1046 24
	struct uvm_vnode *uvn;
	struct vnode *vp;
	boolean_t got_lock;

	/*
	 * step 1: ensure we are only ones using the uvn_sync_q by locking
	 * our lock...
	 */
	lockmgr(&uvn_sync_lock, LK_EXCLUSIVE, NULL, curproc);

	/*
	 * step 2: build up a simpleq of uvns of interest based on the 
	 * write list.   we gain a reference to uvns of interest.  must 
	 * be careful about locking uvn's since we will be holding uvn_wl_lock
	 * in the body of the loop.
	 */
	SIMPLEQ_INIT(&uvn_sync_q);
	simple_lock(&uvn_wl_lock);
	for (uvn = uvn_wlist.lh_first ; uvn != NULL ;
	    uvn = uvn->u_wlist.le_next) {

		vp = (struct vnode *) uvn;
		if (mp && vp->v_mount != mp)
			continue;
d1048 14
a1061 80
		/* attempt to gain reference */
		while ((got_lock = simple_lock_try(&uvn->u_obj.vmobjlock)) ==
		    						FALSE && 
				(uvn->u_flags & UVM_VNODE_BLOCKED) == 0)
			/* spin */ ;

		/*
		 * we will exit the loop if either if the following are true:
		 *  - we got the lock [always true if NCPU == 1]
		 *  - we failed to get the lock but noticed the vnode was
		 * 	"blocked" -- in this case the vnode must be a dying
		 *	vnode, and since dying vnodes are in the process of
		 *	being flushed out, we can safely skip this one
		 *
		 * we want to skip over the vnode if we did not get the lock,
		 * or if the vnode is already dying (due to the above logic).
		 *
		 * note that uvn must already be valid because we found it on
		 * the wlist (this also means it can't be ALOCK'd).
		 */
		if (!got_lock || (uvn->u_flags & UVM_VNODE_BLOCKED) != 0) {
			if (got_lock)
				simple_unlock(&uvn->u_obj.vmobjlock);
			continue;		/* skip it */
		}
		
		/*
		 * gain reference.   watch out for persisting uvns (need to
		 * regain vnode REF).
		 */
		if (uvn->u_obj.uo_refs == 0)
			VREF(vp);
		uvn->u_obj.uo_refs++;
		simple_unlock(&uvn->u_obj.vmobjlock);

		/*
		 * got it!
		 */
		SIMPLEQ_INSERT_HEAD(&uvn_sync_q, uvn, u_syncq);
	}
	simple_unlock(&uvn_wl_lock);

	/*
	 * step 3: we now have a list of uvn's that may need cleaning.
	 * we are holding the uvn_sync_lock, but have dropped the uvn_wl_lock
	 * (so we can now safely lock uvn's again).
	 */

	for (uvn = uvn_sync_q.sqh_first ; uvn ; uvn = uvn->u_syncq.sqe_next) {
		simple_lock(&uvn->u_obj.vmobjlock);
#ifdef DEBUG
		if (uvn->u_flags & UVM_VNODE_DYING) {
			printf("uvm_vnp_sync: dying vnode on sync list\n");
		}
#endif
		uvn_flush(&uvn->u_obj, 0, 0,
		    PGO_CLEANIT|PGO_ALLPAGES|PGO_DOACTCLUST);

		/*
		 * if we have the only reference and we just cleaned the uvn,
		 * then we can pull it out of the UVM_VNODE_WRITEABLE state
		 * thus allowing us to avoid thinking about flushing it again
		 * on later sync ops.
		 */
		if (uvn->u_obj.uo_refs == 1 &&
		    (uvn->u_flags & UVM_VNODE_WRITEABLE)) {
			LIST_REMOVE(uvn, u_wlist);
			uvn->u_flags &= ~UVM_VNODE_WRITEABLE;
		}

		simple_unlock(&uvn->u_obj.vmobjlock);

		/* now drop our reference to the uvn */
		uvn_detach(&uvn->u_obj);
	}

	/*
	 * done!  release sync lock
	 */
	lockmgr(&uvn_sync_lock, LK_RELEASE, (void *)0, curproc);
@


1.23
log
@Another sync of uvm to NetBSD. Just minor fiddling, no major changes.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.22 2001/11/06 01:35:04 art Exp $	*/
d1612 1
a1612 1
	kva = uvm_pagermapin(pps, npages, NULL, mapinflags);
d1629 1
a1629 1
		kva = uvm_pagermapin(pps, npages, NULL,
@


1.22
log
@Move the last content from vm/ to uvm/
The only thing left in vm/ are just dumb wrappers.
vm/vm.h includes uvm/uvm_extern.h
vm/pmap.h includes uvm/uvm_pmap.h
vm/vm_page.h includes uvm/uvm_page.h
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_vnode.c,v 1.21 2001/11/05 22:14:54 art Exp $	*/
/*	$NetBSD: uvm_vnode.c,v 1.35 2000/06/27 17:29:37 mrg Exp $	*/
a86 2
static int		   uvn_asyncget __P((struct uvm_object *, voff_t,
					    int));
a114 1
	uvn_asyncget,
a117 1
	NULL,		 /* AIO-DONE function (not until we have asyncio) */
a1544 22
 * uvn_asyncget: start async I/O to bring pages into ram
 *
 * => caller must lock object(???XXX: see if this is best)
 * => could be called from uvn_get or a madvise() fault-ahead.
 * => if it fails, it doesn't matter.
 */

static int
uvn_asyncget(uobj, offset, npages)
	struct uvm_object *uobj;
	voff_t offset;
	int npages;
{

	/*
	 * XXXCDC: we can't do async I/O yet
	 */
	printf("uvn_asyncget called\n");
	return (KERN_SUCCESS);
}

/*
d1672 1
a1672 1
		result = vn_lock(vn, LK_EXCLUSIVE | LK_RETRY, curproc /*XXX*/);
d1683 1
a1683 1
			VOP_UNLOCK(vn, 0, curproc /*XXX*/);
d1847 1
a1847 1
	VOP_UNLOCK(vp, 0, curproc /*XXX*/);
d1849 1
a1849 1
	vn_lock(vp, LK_EXCLUSIVE | LK_RETRY, curproc/*XXX*/);
d1931 1
a1931 1
	lockmgr(&uvn_sync_lock, LK_EXCLUSIVE, (void *)0, curproc /*XXX*/);
d2027 1
a2027 1
	lockmgr(&uvn_sync_lock, LK_RELEASE, (void *)0, curproc /*XXX*/);
@


1.21
log
@Minor sync to NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_vnode.c,v 1.20 2001/09/11 20:05:26 miod Exp $	*/
/*	$NetBSD: uvm_vnode.c,v 1.34 2000/06/26 14:21:19 mrg Exp $	*/
a63 2

#include <vm/vm.h>
@


1.20
log
@Don't include <vm/vm_kern.h> if you don't need foo_map.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_vnode.c,v 1.19 2001/09/05 19:22:23 deraadt Exp $	*/
/*	$NetBSD: uvm_vnode.c,v 1.33 2000/05/19 03:45:05 thorpej Exp $	*/
a65 1
#include <vm/vm_page.h>
@


1.19
log
@use %ll instead of %q
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.18 2001/08/11 10:57:22 art Exp $	*/
a66 1
#include <vm/vm_kern.h>
@


1.18
log
@Various random fixes from NetBSD.
Including support for zeroing pages in the idle loop (not enabled yet).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.17 2001/08/06 14:03:05 art Exp $	*/
d290 1
a290 1
		printf("used_vnode_size = %qu\n", (long long)used_vnode_size);
@


1.17
log
@Add a new type voff_t (right now it's typedefed as off_t) used for offsets
into objects.

Gives the possibilty to mmap beyond the size of vaddr_t.

From NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_vnode.c,v 1.16 2001/07/31 13:34:46 art Exp $	*/
/*	$NetBSD: uvm_vnode.c,v 1.30 2000/03/26 20:54:47 kleink Exp $	*/
a92 1
struct uvm_object 	  *uvn_attach __P((void *, vm_prot_t));
a124 1
	uvm_shareprot,	 /* !NULL: allow us in share maps */
d1595 1
a1595 1
	int waitf, result;
d1639 5
a1643 2
	kva = uvm_pagermapin(pps, npages, NULL, M_NOWAIT);
	if (kva == NULL && waitf == M_NOWAIT) {
d1658 3
a1660 3
	if (kva == NULL) {
		kva = uvm_pagermapin(pps, npages, NULL, M_WAITOK);
	}
@


1.16
log
@minor sync to NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_vnode.c,v 1.15 2001/07/18 14:29:35 art Exp $	*/
/*	$NetBSD: uvm_vnode.c,v 1.29 2000/03/13 23:52:42 soren Exp $	*/
d91 1
a91 1
static int		   uvn_asyncget __P((struct uvm_object *, vaddr_t,
d94 2
a95 2
static void		   uvn_cluster __P((struct uvm_object *, vaddr_t,
					   vaddr_t *, vaddr_t *));
d97 3
a99 3
static boolean_t           uvn_flush __P((struct uvm_object *, vaddr_t, 
					 vaddr_t, int));
static int                 uvn_get __P((struct uvm_object *, vaddr_t,
d188 2
a293 7
	if (used_vnode_size > (vaddr_t) -PAGE_SIZE) {
#ifdef DEBUG
		printf("uvn_attach: vn %p size truncated %qx->%x\n", vp,
		    (long long)used_vnode_size, -PAGE_SIZE);
#endif    
		used_vnode_size = (vaddr_t) -PAGE_SIZE;
	}
d830 1
a830 1
	vaddr_t start, stop;
d837 2
a838 2
	boolean_t retval, need_iosync, by_list, needs_clean;
	vaddr_t curoff;
d850 1
a850 2
		start = 0;
		stop = round_page(uvn->u_size);
d860 1
d885 2
a886 1
				if (pp->offset < start || pp->offset >= stop)
d928 2
a929 1
			if (pp->offset < start || pp->offset >= stop) {
d1200 1
a1200 1
						   "offset=0x%lx.  error "
d1202 2
a1203 1
						    pp->uobject, pp->offset);
d1258 2
a1259 2
	vaddr_t offset;
	vaddr_t *loffset, *hoffset; /* OUT */
d1318 1
a1318 1
	vaddr_t offset;
d1324 1
a1324 1
	vaddr_t current_offset;
d1565 1
a1565 1
	vaddr_t offset;
d1595 4
a1598 2
	vaddr_t kva, file_offset;
	int waitf, result, got, wanted;
d1908 1
a1908 1
	u_quad_t newsize;
a1918 14
		 * make sure that the newsize fits within a vaddr_t
		 * XXX: need to revise addressing data types
		 */

		if (newsize > (vaddr_t) -PAGE_SIZE) {
#ifdef DEBUG
			printf("uvm_vnp_setsize: vn %p size truncated "
			       "%qx->%lx\n", vp, (long long)newsize,
			       (vaddr_t)-PAGE_SIZE);
#endif
			newsize = (vaddr_t)-PAGE_SIZE;
		}

		/*
d1924 1
a1924 1
			(void)uvn_flush(&uvn->u_obj, (vaddr_t) newsize,
d1927 1
a1927 1
		uvn->u_size = (vaddr_t)newsize;
@


1.15
log
@Change some annoying DIAGNOSTIC printfs to DEBUG.
From NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_vnode.c,v 1.14 2001/07/18 10:47:05 art Exp $	*/
/*	$NetBSD: uvm_vnode.c,v 1.27 1999/10/19 16:04:45 chs Exp $	*/
a1633 3
#ifdef DEBUG
			printf("uvn_io: note: size check fired\n");
#endif
@


1.14
log
@Get rid of the PMAP_NEW option by making it mandatory for all archs.
The archs that didn't have a proper PMAP_NEW now have a dummy implementation
with wrappers around the old functions.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_vnode.c,v 1.13 2001/06/23 19:24:34 smart Exp $	*/
/*	$NetBSD: uvm_vnode.c,v 1.26 1999/09/12 01:17:42 chs Exp $	*/
d351 1
a351 1
#ifdef DIAGNOSTIC
d357 1
a357 1
#ifdef DIAGNOSTIC
d460 1
a460 1
#ifdef DIAGNOSTIC
d606 1
a606 1
#ifdef DIAGNOSTIC
d861 1
d865 1
a865 1

d2044 1
a2044 1
#ifdef DIAGNOSTIC
@


1.13
log
@Sync with NetBSD 19990911 (just before PMAP_NEW was required)
  - thread_sleep_msg() -> uvm_sleep()
  - initialize reference count lock in uvm_anon_{init,add}()
  - add uao_flush()
  - replace boolean 'islocked' with 'lockflags'
  - in uvm_fault() change FALSE to TRUE to in 'wide' fault handling
  - get rid of uvm_km_get()
  - various bug fixes
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_vnode.c,v 1.12 2001/03/22 03:05:57 smart Exp $	*/
/*	$NetBSD: uvm_vnode.c,v 1.25 1999/07/22 22:58:39 thorpej Exp $	*/
d701 1
a701 1
	pmap_page_protect(PMAP_PGARG(pg), VM_PROT_NONE);
d976 1
a976 1
				pmap_page_protect(PMAP_PGARG(pp), VM_PROT_NONE);
d978 1
a978 1
			    pmap_is_modified(PMAP_PGARG(pp)))
d1001 1
a1001 2
					pmap_page_protect(PMAP_PGARG(pp),
					    VM_PROT_NONE);
d1010 1
a1010 2
					pmap_page_protect(PMAP_PGARG(pp),
					    VM_PROT_NONE);
d1030 1
a1030 1
		pmap_page_protect(PMAP_PGARG(pp), VM_PROT_READ);
d1179 1
a1179 2
						pmap_clear_modify(
						    PMAP_PGARG(ptmp));
d1190 1
a1190 2
					pmap_page_protect(PMAP_PGARG(ptmp),
					    VM_PROT_NONE);
d1210 1
a1210 2
					pmap_page_protect(PMAP_PGARG(ptmp),
					    VM_PROT_NONE);
d1542 1
a1542 1
		pmap_clear_modify(PMAP_PGARG(ptmp));	/* ... and clean */
@


1.12
log
@Sync style, typo, and comments a little closer to NetBSD.  art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_vnode.c,v 1.11 2001/03/08 15:21:38 smart Exp $	*/
/*	$NetBSD: uvm_vnode.c,v 1.23 1999/04/11 04:04:11 chs Exp $	*/
@


1.11
log
@Replace thread_wakeup() with wakeup().  It is defined in vm_extern.h as a
wrapper, so this removes a dependence on the old VM system.  From NetBSD.
art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.10 2001/01/29 02:07:50 niklas Exp $	*/
d556 1
a556 1
	 * protects us from that).   the uvn can't in the the ALOCK state
d1479 1
a1479 1
				    &uobj->vmobjlock, 0, "uvn_get",0);
d1739 1
a1739 1
			      (npages << PAGE_SHIFT) - got);
@


1.10
log
@$OpenBSD$
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_vnode.c,v 1.23 1999/04/11 04:04:11 chs Exp $	*/
d1164 1
a1164 1
					thread_wakeup(ptmp);
d1524 1
a1524 1
				thread_wakeup(ptmp);
@


1.9
log
@Convert bzero to memset(X, 0..) and bcopy to memcpy.
This is to match (make diffs smaller) the code in NetBSD.
new gcc inlines those functions, so this could also be a performance win.
@
text
@d1 1
@


1.8
log
@set uio_procp correctly in uvn_io.
@
text
@d1737 1
a1737 1
			bzero((void *) (kva + got),
@


1.7
log
@Fix the NetBSD id strings.
@
text
@d1688 1
a1688 1
	uio.uio_procp = NULL;
@


1.6
log
@nfs vnodeops are only defined with NFSCLIENT, not NFSSERVER.
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_vnode.c,v 1.22 1999/03/25 18:48:56 mrg Exp $	*/
@


1.6.2.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_vnode.c,v 1.23 1999/04/11 04:04:11 chs Exp $	*/
@


1.6.2.2
log
@merge in approximately 2.9 into SMP branch
@
text
@a0 1
/*	$OpenBSD: uvm_vnode.c,v 1.12 2001/03/22 03:05:57 smart Exp $	*/
d555 1
a555 1
	 * protects us from that).   the uvn can't in the ALOCK state
d1163 1
a1163 1
					wakeup(ptmp);
d1478 1
a1478 1
				    &uobj->vmobjlock, FALSE, "uvn_get",0);
d1523 1
a1523 1
				wakeup(ptmp);
d1688 1
a1688 1
	uio.uio_procp = curproc;
d1737 2
a1738 2
			memset((void *) (kva + got), 0,
			       (npages << PAGE_SHIFT) - got);
@


1.6.2.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_vnode.c,v 1.25 1999/07/22 22:58:39 thorpej Exp $	*/
@


1.6.2.4
log
@Sync the SMP branch to something just after 3.0
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_vnode.c,v 1.33 2000/05/19 03:45:05 thorpej Exp $	*/
d67 1
d91 1
a91 1
static int		   uvn_asyncget __P((struct uvm_object *, voff_t,
d93 3
a95 2
static void		   uvn_cluster __P((struct uvm_object *, voff_t,
					   voff_t *, voff_t *));
d97 3
a99 3
static boolean_t           uvn_flush __P((struct uvm_object *, voff_t, 
					 voff_t, int));
static int                 uvn_get __P((struct uvm_object *, voff_t,
d126 1
a187 2
		printf("uvn_attach: blocked at 0x%p flags 0x%x\n",
		    uvn, uvn->u_flags);
d290 1
a290 1
		printf("used_vnode_size = %llu\n", (long long)used_vnode_size);
d292 7
d351 1
a351 1
#ifdef DEBUG
d357 1
a357 1
#ifdef DEBUG
d460 1
a460 1
#ifdef DEBUG
d606 1
a606 1
#ifdef DEBUG
d701 1
a701 1
	pmap_page_protect(pg, VM_PROT_NONE);
d835 1
a835 1
	voff_t start, stop;
d842 2
a843 2
	boolean_t retval, need_iosync, by_list, needs_clean, all;
	voff_t curoff;
d855 2
a856 1
		all = TRUE;
a860 1
#ifdef DEBUG
d864 1
a864 2
#endif
		all = FALSE;
d889 1
a889 2
				if (!all &&
				    (pp->offset < start || pp->offset >= stop))
d931 1
a931 2
			if (!all &&
			    (pp->offset < start || pp->offset >= stop)) {
d976 1
a976 1
				pmap_page_protect(pp, VM_PROT_NONE);
d978 1
a978 1
			    pmap_is_modified(pp))
d1001 2
a1002 1
					pmap_page_protect(pp, VM_PROT_NONE);
d1011 2
a1012 1
					pmap_page_protect(pp, VM_PROT_NONE);
d1032 1
a1032 1
		pmap_page_protect(pp, VM_PROT_READ);
d1181 2
a1182 1
						pmap_clear_modify(ptmp);
d1193 2
a1194 1
					pmap_page_protect(ptmp, VM_PROT_NONE);
d1206 1
a1206 1
						   "offset=0x%llx.  error "
d1208 1
a1208 2
						    pp->uobject,
						    (long long)pp->offset);
d1214 2
a1215 1
					pmap_page_protect(ptmp, VM_PROT_NONE);
d1264 2
a1265 2
	voff_t offset;
	voff_t *loffset, *hoffset; /* OUT */
d1324 1
a1324 1
	voff_t offset;
d1330 1
a1330 1
	voff_t current_offset;
d1547 1
a1547 1
		pmap_clear_modify(ptmp);		/* ... and clean */
d1571 1
a1571 1
	voff_t offset;
d1601 2
a1602 4
	vaddr_t kva;
	off_t file_offset;
	int waitf, result, mapinflags;
	size_t got, wanted;
d1638 3
d1648 2
a1649 5
	mapinflags = (rw == UIO_READ) ?
	    UVMPAGER_MAPIN_READ : UVMPAGER_MAPIN_WRITE;

	kva = uvm_pagermapin(pps, npages, NULL, mapinflags);
	if (kva == 0 && waitf == M_NOWAIT) {
d1664 3
a1666 3
	if (kva == 0)
		kva = uvm_pagermapin(pps, npages, NULL,
		    mapinflags | UVMPAGER_MAPIN_WAITOK);
d1915 1
a1915 1
	voff_t newsize;
d1926 14
d1945 1
a1945 1
			(void)uvn_flush(&uvn->u_obj, newsize,
d1948 1
a1948 1
		uvn->u_size = newsize;
d2048 1
a2048 1
#ifdef DEBUG
@


1.6.2.5
log
@merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_vnode.c,v 1.36 2000/11/24 20:34:01 chs Exp $	*/
d65 3
d90 2
d120 1
d124 1
d1552 22
d1641 1
a1641 1
	kva = uvm_pagermapin(pps, npages, mapinflags);
d1658 1
a1658 1
		kva = uvm_pagermapin(pps, npages,
d1701 1
a1701 1
		result = vn_lock(vn, LK_EXCLUSIVE | LK_RETRY | LK_RECURSEFAIL, curproc);
d1712 1
a1712 1
			VOP_UNLOCK(vn, 0, curproc);
d1876 1
a1876 1
	VOP_UNLOCK(vp, 0, curproc);
d1878 1
a1878 1
	vn_lock(vp, LK_EXCLUSIVE | LK_RETRY, curproc);
d1960 1
a1960 1
	lockmgr(&uvn_sync_lock, LK_EXCLUSIVE, NULL, curproc);
d2056 1
a2056 1
	lockmgr(&uvn_sync_lock, LK_RELEASE, (void *)0, curproc);
@


1.6.2.6
log
@Merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_vnode.c,v 1.50 2001/05/26 21:27:21 chs Exp $	*/
d7 1
a7 1
 *      The Regents of the University of California.
d27 1
a27 1
 *	Washington University, the University of California, Berkeley and
a54 1
#include <sys/kernel.h>
a61 2
#include <sys/pool.h>
#include <sys/mount.h>
d69 15
d87 16
a102 15
static void		uvn_cluster __P((struct uvm_object *, voff_t, voff_t *,
					 voff_t *));
static void		uvn_detach __P((struct uvm_object *));
static int		uvn_findpage __P((struct uvm_object *, voff_t,
					  struct vm_page **, int));
boolean_t		uvn_flush __P((struct uvm_object *, voff_t, voff_t,
				       int));
int			uvn_get __P((struct uvm_object *, voff_t,
				     struct vm_page **, int *, int, vm_prot_t,
				     int, int));
int			uvn_put __P((struct uvm_object *, struct vm_page **,
				     int, boolean_t));
static void		uvn_reference __P((struct uvm_object *));
static boolean_t	uvn_releasepg __P((struct vm_page *,
					   struct vm_page **));
d109 1
a109 1
	NULL,
d112 1
a112 1
	NULL,
d117 1
a117 1
	uvm_mk_pcluster,
d126 16
d164 1
a164 1
	int result;
d166 1
a166 1
	voff_t used_vnode_size;
d170 2
a171 1
	used_vnode_size = (voff_t)0;
d177 4
a180 2
	while (uvn->u_flags & VXLOCK) {
		uvn->u_flags |= VXWANT;
d192 1
a192 1
		simple_unlock(&uvn->u_obj.vmobjlock);
d197 32
a228 5
#ifdef DIAGNOSTIC
	if (vp->v_type != VREG) {
		panic("uvn_attach: vp %p not VREG", vp);
	}
#endif
d231 6
a236 2
	 * set up our idea of the size
	 * if this hasn't been done already.
d238 1
a238 3
	if (uvn->u_size == VSIZENOTSET) {

	uvn->u_flags |= VXLOCK;
d241 1
d255 2
a256 2
			used_vnode_size = (voff_t)pi.disklab->d_secsize *
			    (voff_t)pi.part->p_size;
d265 1
a265 5
	simple_lock(&uvn->u_obj.vmobjlock);

	if (uvn->u_flags & VXWANT)
		wakeup(uvn);
	uvn->u_flags &= ~(VXLOCK|VXWANT);
d268 3
d275 20
d297 6
d305 6
a310 1
	/* unlock and return */
d312 5
a316 3
	UVMHIST_LOG(maphist,"<- done, refcnt=%d", uvn->u_obj.uo_refs,
	    0, 0, 0);
	return (&uvn->u_obj);
d324 1
a324 1
 * count must already be at least one (the passed in reference) so
d327 1
a327 1
 * => caller must call with object unlocked.
d336 17
a352 1
	VREF((struct vnode *)uobj);
d361 2
d368 285
a652 1
	vrele((struct vnode *)uobj);
d665 1
a665 1
 * => if (nextpgp != NULL) => we return the next page on the queue, and return
d677 6
a682 2
	KASSERT(pg->flags & PG_RELEASED);

d689 1
a689 1
		*nextpgp = TAILQ_NEXT(pg, pageq);
d694 26
d724 6
d731 1
d734 2
a735 2
 * a buf on the "pending" list (protected by splbio()), starts the
 * i/o and returns 0.    when the i/o is done, we expect
d737 1
a737 1
 * time).   this function should remove the buf from the pending list
d740 7
a746 1
 * list and call the iodone hook for each done request (see uvm_pager.c).
a766 1
 * => "stop == 0" means flush all pages at or after "start".
d768 1
a768 2
 *	if (and only if) we need to clean a page (PGO_CLEANIT), or
 *	if PGO_SYNCIO is set and there are pages busy.
d770 6
a775 6
 * => if PGO_CLEANIT or PGO_SYNCIO is set, we may block (due to I/O).
 *	thus, a caller might want to unlock higher level resources
 *	(e.g. vm_map) before calling flush.
 * => if neither PGO_CLEANIT nor PGO_SYNCIO is set, then we will neither
 *	unlock the object nor block.
 * => if PGO_ALLPAGES is set, then all pages in the object are valid targets
d795 1
a795 1
 *	cleaning the page for us (how nice!).    in this case, if we
d797 1
a797 1
 *	object we need to wait for the other PG_BUSY pages to clear
d805 1
a805 1
 *	on how many pages are in the object it may be cheaper to do one
d811 1
a811 1
 *	list traversal, so we multiply the number of pages in the
d817 1
a817 1
boolean_t
d823 1
a823 2
	struct uvm_vnode *uvn = (struct uvm_vnode *)uobj;
	struct vnode *vp = (struct vnode *)uobj;
d825 1
a825 2
	struct vm_page *pps[256], **ppsp;
	int s;
d827 1
a827 2
	boolean_t retval, need_iosync, by_list, needs_clean, all, wasclean;
	boolean_t async = (flags & PGO_SYNCIO) == 0;
a830 22
	UVMHIST_LOG(maphist, "uobj %p start 0x%x stop 0x%x flags 0x%x",
		    uobj, start, stop, flags);
	KASSERT(flags & (PGO_CLEANIT|PGO_FREE|PGO_DEACTIVATE));

	if (uobj->uo_npages == 0) {
		s = splbio();
		if (LIST_FIRST(&vp->v_dirtyblkhd) == NULL &&
		    (vp->v_bioflag & VBIOONSYNCLIST)) {
			vp->v_bioflag &= ~VBIOONSYNCLIST;
			LIST_REMOVE(vp, v_synclist);
		}
		splx(s);
		return TRUE;
	}

#ifdef DIAGNOSTIC
	if (uvn->u_size == VSIZENOTSET) {
		printf("uvn_flush: size not set vp %p\n", uvn);
		vprint("uvn_flush VSIZENOTSET", vp);
		flags |= PGO_ALLPAGES;
	}
#endif
d832 1
a836 4
	if (stop == 0) {
		stop = trunc_page(LLONG_MAX);
	}
	curoff = 0;
d838 1
a838 2
	retval = TRUE;
	wasclean = TRUE;
d841 1
a841 1
		by_list = TRUE;
d845 5
d851 1
a851 1
		by_list = (uobj->uo_npages <=
d873 2
a874 1
			TAILQ_FOREACH(pp, &uobj->memq, listq) {
d898 1
a898 1
		pp = TAILQ_FIRST(&uobj->memq);
d904 3
a906 3
	ppnext = NULL;
	ppsp = NULL;
	uvm_lock_pageq();
d909 3
a911 2
	for ( ; (by_list && pp != NULL) ||
		      (!by_list && curoff < stop) ; pp = ppnext) {
d913 5
d920 1
a920 1
				ppnext = TAILQ_NEXT(pp, listq);
d923 1
d925 5
d936 1
d943 1
a943 1
		 *
d952 3
a954 1
			if (!async)
a956 1

d961 1
a961 1
			if ((pp->flags & PG_CLEAN) != 0 &&
d963 1
a963 2
			    /* XXX ACTIVE|INACTIVE test unnecessary? */
			    (pp->pqflags & (PQ_ACTIVE|PQ_INACTIVE)) != 0)
d968 2
a969 1
			pp->flags |= PG_CLEANCHK;
d977 1
d979 1
a979 1
				ppnext = TAILQ_NEXT(pp, listq);
d985 1
a987 1
				    (pp->flags & PG_BUSY) == 0 &&
d989 1
a989 1
					pmap_clear_reference(pp);
d995 1
d999 1
a1015 1
		wasclean = FALSE;
d1020 1
d1025 2
a1026 2
		result = uvm_pager_put(uobj, pp, &ppsp, &npages,
				       flags | PGO_DOACTCLUST, start, stop);
d1031 2
a1032 2
		 * it is remotely possible for the async i/o to complete and
		 * the page "pp" be freed or what not before we get a chance
d1042 27
a1068 4
		 * the cleaning operation is now done.  finish up.  note that
		 * on error uvm_pager_put drops the cluster for us.
		 * on success uvm_pager_put returns the cluster to us in
		 * ppsp/npages.
d1076 1
a1076 2
		if (result == 0 && async &&
		    (flags & (PGO_DEACTIVATE|PGO_FREE)) == 0) {
d1078 16
a1093 11
			/*
			 * no per-page ops: refresh ppnext and continue
			 */
			if (by_list) {
				if (pp->version == pp_version)
					ppnext = TAILQ_NEXT(pp, listq);
				else
					ppnext = TAILQ_FIRST(&uobj->memq);
			} else {
				if (curoff < stop)
					ppnext = uvm_pagelookup(uobj, curoff);
d1095 2
a1096 1
			continue;
d1100 2
a1101 2
		 * need to look at each page of the I/O operation.  we defer
		 * processing "pp" until the last trip through this "for" loop
d1103 1
a1103 1
		 * play with the cluster pages [thus the "npages + 1" in the
d1123 1
a1123 1
						ppnext = TAILQ_NEXT(pp, listq);
d1125 2
a1126 2
						ppnext = TAILQ_FIRST(
						    &uobj->memq);
d1129 1
a1129 2
						ppnext = uvm_pagelookup(uobj,
						    curoff);
d1134 1
a1134 1
			 * verify the page wasn't moved while obj was
d1137 1
a1137 1
			if (result == 0 && async && ptmp->uobject != uobj)
d1142 1
a1142 1
			 * async I/O it is possible that the I/O op
d1147 2
a1148 2
			if (result != 0 || !async) {
				if (ptmp->flags & PG_WANTED) {
d1151 1
a1151 1
				}
d1155 2
d1158 1
a1158 4
					if (!uvn_releasepg(ptmp, NULL)) {
						UVMHIST_LOG(maphist,
							    "released %p",
							    ptmp, 0,0,0);
d1160 4
a1163 3
					}
					uvm_lock_pageq();
					continue;
d1165 3
a1167 9
					if ((flags & PGO_WEAK) == 0 &&
					    !(result == EIO &&
					      curproc == uvm.pagedaemon_proc)) {
						ptmp->flags |=
							(PG_CLEAN|PG_CLEANCHK);
						if ((flags & PGO_FREE) == 0) {
							pmap_clear_modify(ptmp);
						}
					}
d1170 1
a1170 1

a1176 1
				    (pp->flags & PG_BUSY) == 0 &&
d1178 1
a1178 1
					pmap_clear_reference(ptmp);
d1181 1
d1183 1
a1183 1
				if (result == 0 && async) {
d1188 1
a1188 1
					if (result != 0) {
d1190 2
a1191 1
						   "offset=0x%llx.  error %d\n",
d1193 1
a1193 2
						    (long long)pp->offset,
						    result);
d1203 1
d1205 1
d1208 3
d1212 4
a1215 8
	s = splbio();
	if ((flags & PGO_CLEANIT) && all && wasclean &&
	    LIST_FIRST(&vp->v_dirtyblkhd) == NULL &&
	    (vp->v_bioflag & VBIOONSYNCLIST)) {
		vp->v_bioflag &= ~VBIOONSYNCLIST;
		LIST_REMOVE(vp, v_synclist);
	}
	splx(s);
d1217 1
d1219 4
a1222 15

		/*
		 * XXX this doesn't use the new two-flag scheme,
		 * but to use that, all i/o initiators will have to change.
		 */

		s = splbio();
		while (vp->v_numoutput != 0) {
			UVMHIST_LOG(ubchist, "waiting for vp %p num %d",
				    vp, vp->v_numoutput,0,0);

	                vp->v_bioflag |= VBIOWAIT;
			UVM_UNLOCK_AND_WAIT(&vp->v_numoutput,
					    &uvn->u_obj.vmobjlock,
					    FALSE, "uvn_flush",0);
d1225 3
a1227 1
		splx(s);
d1251 5
a1255 1
	struct uvm_vnode *uvn = (struct uvm_vnode *)uobj;
d1257 8
a1264 2
	*loffset = offset;
	*hoffset = MIN(offset + MAXBSIZE, round_page(uvn->u_size));
d1270 1
d1274 2
d1278 1
a1278 1
int
d1284 5
a1288 2
	struct vnode *vp = (struct vnode *)uobj;
	int error;
d1290 1
a1290 2
	error = VOP_PUTPAGES(vp, pps, npages, flags, NULL);
	return error;
d1304 2
a1305 2

int
d1311 1
a1311 1
	int centeridx;
a1312 1
	int advice, flags;
d1314 55
a1368 17
	struct vnode *vp = (struct vnode *)uobj;
	struct proc *p = curproc;
	int error;
	UVMHIST_FUNC("uvn_get"); UVMHIST_CALLED(ubchist);

	UVMHIST_LOG(ubchist, "vp %p off 0x%x", vp, (int)offset, 0,0);
	error = vn_lock(vp, LK_EXCLUSIVE|LK_RECURSEFAIL|LK_NOWAIT, p);
	if (error) {
		if (error == EBUSY)
			return EAGAIN;
		return error;
	}
	error = VOP_GETPAGES(vp, offset, pps, npagesp, centeridx,
		     access_type, advice, flags);
	VOP_UNLOCK(vp, LK_RELEASE, p);
	return error;
}
d1370 1
d1372 11
a1382 6
/*
 * uvn_findpages:
 * return the page for the uobj and offset requested, allocating if needed.
 * => uobj must be locked.
 * => returned page will be BUSY.
 */
d1384 4
a1387 9
void
uvn_findpages(uobj, offset, npagesp, pps, flags)
	struct uvm_object *uobj;
	voff_t offset;
	int *npagesp;
	struct vm_page **pps;
	int flags;
{
	int i, rv, npages;
d1389 6
a1394 4
	rv = 0;
	npages = *npagesp;
	for (i = 0; i < npages; i++, offset += PAGE_SIZE) {
		rv += uvn_findpage(uobj, offset, &pps[i], flags);
a1395 2
	*npagesp = rv;
}
d1397 54
a1450 10
static int
uvn_findpage(uobj, offset, pgp, flags)
	struct uvm_object *uobj;
	voff_t offset;
	struct vm_page **pgp;
	int flags;
{
	struct vm_page *pg;
	UVMHIST_FUNC("uvn_findpage"); UVMHIST_CALLED(ubchist);
	UVMHIST_LOG(ubchist, "vp %p off 0x%lx", uobj, offset,0,0);
d1452 5
a1456 13
	if (*pgp != NULL) {
		UVMHIST_LOG(ubchist, "dontcare", 0,0,0,0);
		return 0;
	}
	for (;;) {
		/* look for an existing page */
		pg = uvm_pagelookup(uobj, offset);

		/* nope?   allocate one now */
		if (pg == NULL) {
			if (flags & UFP_NOALLOC) {
				UVMHIST_LOG(ubchist, "noalloc", 0,0,0,0);
				return 0;
d1458 6
a1463 8
			pg = uvm_pagealloc(uobj, offset, NULL, 0);
			if (pg == NULL) {
				if (flags & UFP_NOWAIT) {
					UVMHIST_LOG(ubchist, "nowait",0,0,0,0);
					return 0;
				}
				simple_unlock(&uobj->vmobjlock);
				uvm_wait("uvn_fp1");
d1465 1
a1465 1
				continue;
d1467 10
a1476 10
			if (UVM_OBJ_IS_VTEXT(uobj)) {
				uvmexp.vtextpages++;
			} else {
				uvmexp.vnodepages++;
			}
			UVMHIST_LOG(ubchist, "alloced",0,0,0,0);
			break;
		} else if (flags & UFP_NOCACHE) {
			UVMHIST_LOG(ubchist, "nocache",0,0,0,0);
			return 0;
d1479 38
a1516 11
		/* page is there, see if we need to wait on it */
		if ((pg->flags & (PG_BUSY|PG_RELEASED)) != 0) {
			if (flags & UFP_NOWAIT) {
				UVMHIST_LOG(ubchist, "nowait",0,0,0,0);
				return 0;
			}
			pg->flags |= PG_WANTED;
			UVM_UNLOCK_AND_WAIT(pg, &uobj->vmobjlock, 0,
					    "uvn_fp2", 0);
			simple_lock(&uobj->vmobjlock);
			continue;
d1519 69
a1587 4
		/* skip PG_RDONLY pages if requested */
		if ((flags & UFP_NORDONLY) && (pg->flags & PG_RDONLY)) {
			UVMHIST_LOG(ubchist, "nordonly",0,0,0,0);
			return 0;
d1589 9
d1599 4
a1602 5
		/* mark the page BUSY and we're done. */
		pg->flags |= PG_BUSY;
		UVM_PAGE_OWN(pg, "uvn_findpage");
		UVMHIST_LOG(ubchist, "found",0,0,0,0);
		break;
d1604 252
a1855 2
	*pgp = pg;
	return 1;
d1864 1
a1864 1
 * => we assume that the caller has a reference of some sort to the
a1881 2
	voff_t pgend = round_page(newsize);
	UVMHIST_FUNC("uvm_vnp_setsize"); UVMHIST_CALLED(ubchist);
d1883 3
d1887 6
d1894 7
a1900 1
	UVMHIST_LOG(ubchist, "old 0x%x new 0x%x", uvn->u_size, newsize, 0,0);
d1903 1
a1903 2
	 * now check if the size has changed: if we shrink we had better
	 * toss some pages...
d1905 1
a1905 6

	if (uvn->u_size > pgend && uvn->u_size != VSIZENOTSET) {
		(void) uvn_flush(&uvn->u_obj, pgend, 0, PGO_FREE);
	}
	uvn->u_size = newsize;
	simple_unlock(&uvn->u_obj.vmobjlock);
d1909 8
a1916 1
 * uvm_vnp_zerorange:  set a range of bytes in a file to zero.
d1920 4
a1923 1
uvm_vnp_zerorange(vp, off, len)
d1925 98
a2022 4
	off_t off;
	size_t len;
{
        void *win;
d2024 4
a2027 14
        /*
         * XXXUBC invent kzero() and use it
         */

        while (len) {
                vsize_t bytelen = len;

                win = ubc_alloc(&vp->v_uvm.u_obj, off, &bytelen, UBC_WRITE);
                memset(win, 0, bytelen);
                ubc_release(win, 0);

                off += bytelen;
                len -= bytelen;
        }
@


1.6.2.7
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_vnode.c,v 1.36 2000/11/24 20:34:01 chs Exp $	*/
d7 1
a7 1
 *      The Regents of the University of California.  
d27 1
a27 1
 *	Washington University, the University of California, Berkeley and 
d55 1
d63 2
a71 15
 * private global data structure
 *
 * we keep a list of writeable active vnode-backed VM objects for sync op.
 * we keep a simpleq of vnodes that are currently being sync'd.
 */

LIST_HEAD(uvn_list_struct, uvm_vnode);
static struct uvn_list_struct uvn_wlist;	/* writeable uvns */
static simple_lock_data_t uvn_wl_lock;		/* locks uvn_wlist */

SIMPLEQ_HEAD(uvn_sq_struct, uvm_vnode);
static struct uvn_sq_struct uvn_sync_q;		/* sync'ing uvns */
lock_data_t uvn_sync_lock;			/* locks sync operation */

/*
d75 15
a89 16
static void		   uvn_cluster __P((struct uvm_object *, voff_t,
					   voff_t *, voff_t *));
static void                uvn_detach __P((struct uvm_object *));
static boolean_t           uvn_flush __P((struct uvm_object *, voff_t, 
					 voff_t, int));
static int                 uvn_get __P((struct uvm_object *, voff_t,
					vm_page_t *, int *, int, 
					vm_prot_t, int, int));
static void		   uvn_init __P((void));
static int		   uvn_io __P((struct uvm_vnode *, vm_page_t *,
				      int, int, int));
static int		   uvn_put __P((struct uvm_object *, vm_page_t *,
					int, boolean_t));
static void                uvn_reference __P((struct uvm_object *));
static boolean_t	   uvn_releasepg __P((struct vm_page *, 
					      struct vm_page **));
d96 1
a96 1
	uvn_init,
d99 1
a99 1
	NULL,			/* no specialized fault routine required */
d104 1
a104 1
	uvm_mk_pcluster, /* use generic version of this: see uvm_pager.c */
a112 16
 * uvn_init
 *
 * init pager private data structures.
 */

static void
uvn_init()
{

	LIST_INIT(&uvn_wlist);
	simple_lock_init(&uvn_wl_lock);
	/* note: uvn_sync_q init'd in uvm_vnp_sync() */
	lockinit(&uvn_sync_lock, PVM, "uvnsync", 0, 0);
}

/*
d135 1
a135 1
	int oldflags, result;
d137 1
a137 1
	u_quad_t used_vnode_size;
d141 1
a141 2

	used_vnode_size = (u_quad_t)0;	/* XXX gcc -Wuninitialized */
d147 2
a148 4
	while (uvn->u_flags & UVM_VNODE_BLOCKED) {
		printf("uvn_attach: blocked at 0x%p flags 0x%x\n",
		    uvn, uvn->u_flags);
		uvn->u_flags |= UVM_VNODE_WANTED;
d160 1
a160 1
		simple_unlock(&uvn->u_obj.vmobjlock); /* drop lock */
d165 6
d172 2
a173 4
	 * now we have lock and uvn must not be in a blocked state.
	 * first check to see if it is already active, in which case
	 * we can bump the reference count, check to see if we need to
	 * add it to the writeable list, and then return.
d175 1
a175 9
	if (uvn->u_flags & UVM_VNODE_VALID) {	/* already active? */

		/* regain VREF if we were persisting */
		if (uvn->u_obj.uo_refs == 0) {
			VREF(vp);
			UVMHIST_LOG(maphist," VREF (reclaim persisting vnode)",
			    0,0,0,0);
		}
		uvn->u_obj.uo_refs++;		/* bump uvn ref! */
d177 1
a177 26
		/* check for new writeable uvn */
		if ((accessprot & VM_PROT_WRITE) != 0 && 
		    (uvn->u_flags & UVM_VNODE_WRITEABLE) == 0) {
			simple_lock(&uvn_wl_lock);
			LIST_INSERT_HEAD(&uvn_wlist, uvn, u_wlist);
			simple_unlock(&uvn_wl_lock);
			/* we are now on wlist! */
			uvn->u_flags |= UVM_VNODE_WRITEABLE;
		}

		/* unlock and return */
		simple_unlock(&uvn->u_obj.vmobjlock);
		UVMHIST_LOG(maphist,"<- done, refcnt=%d", uvn->u_obj.uo_refs,
		    0, 0, 0);
		return (&uvn->u_obj);
	} 

	/*
	 * need to call VOP_GETATTR() to get the attributes, but that could
	 * block (due to I/O), so we want to unlock the object before calling.
	 * however, we want to keep anyone else from playing with the object
	 * while it is unlocked.   to do this we set UVM_VNODE_ALOCK which
	 * prevents anyone from attaching to the vnode until we are done with
	 * it.
	 */
	uvn->u_flags = UVM_VNODE_ALOCK;
a179 1

d193 2
a194 2
			used_vnode_size = (u_quad_t)pi.disklab->d_secsize *
			    (u_quad_t)pi.part->p_size;
d203 5
a207 1
	simple_lock(&uvn->u_obj.vmobjlock); 
a209 3
		if (uvn->u_flags & UVM_VNODE_WANTED)
			wakeup(uvn);
		uvn->u_flags = 0;
a213 20

	/*
	 * make sure that the newsize fits within a vaddr_t
	 * XXX: need to revise addressing data types
	 */
#ifdef DEBUG
	if (vp->v_type == VBLK)
		printf("used_vnode_size = %llu\n", (long long)used_vnode_size);
#endif

	/*
	 * now set up the uvn.
	 */
	uvn->u_obj.pgops = &uvm_vnodeops;
	TAILQ_INIT(&uvn->u_obj.memq);
	uvn->u_obj.uo_npages = 0;
	uvn->u_obj.uo_refs = 1;			/* just us... */
	oldflags = uvn->u_flags;
	uvn->u_flags = UVM_VNODE_VALID|UVM_VNODE_CANPERSIST;
	uvn->u_nio = 0;
a215 6
	/* if write access, we need to add it to the wlist */
	if (accessprot & VM_PROT_WRITE) {
		simple_lock(&uvn_wl_lock);
		LIST_INSERT_HEAD(&uvn_wlist, uvn, u_wlist);
		simple_unlock(&uvn_wl_lock);
		uvn->u_flags |= UVM_VNODE_WRITEABLE;	/* we are on wlist! */
d218 1
a218 6
	/*
	 * add a reference to the vnode.   this reference will stay as long
	 * as there is a valid mapping of the vnode.   dropped when the
	 * reference count goes to zero [and we either free or persist].
	 */
	VREF(vp);
d220 3
a222 5
	if (oldflags & UVM_VNODE_WANTED)
		wakeup(uvn);

	UVMHIST_LOG(maphist,"<- done/VREF, ret 0x%x", &uvn->u_obj,0,0,0);
	return(&uvn->u_obj);
d230 1
a230 1
 * count must already be at least one (the passed in reference) so 
d233 1
a233 1
 * => caller must call with object unlocked.  
d242 1
a242 17
#ifdef DEBUG
	struct uvm_vnode *uvn = (struct uvm_vnode *) uobj;
#endif
	UVMHIST_FUNC("uvn_reference"); UVMHIST_CALLED(maphist);

	simple_lock(&uobj->vmobjlock);
#ifdef DEBUG
	if ((uvn->u_flags & UVM_VNODE_VALID) == 0) {
		printf("uvn_reference: ref=%d, flags=0x%x\n", uvn->u_flags,
		    uobj->uo_refs);
		panic("uvn_reference: invalid state");
	}
#endif
	uobj->uo_refs++;
	UVMHIST_LOG(maphist, "<- done (uobj=0x%x, ref = %d)", 
	uobj, uobj->uo_refs,0,0);
	simple_unlock(&uobj->vmobjlock);
a250 2
 * => this starts the detach process, but doesn't have to finish it
 *    (async i/o could still be pending).
d256 1
a256 285
	struct uvm_vnode *uvn;
	struct vnode *vp;
	int oldflags;
	UVMHIST_FUNC("uvn_detach"); UVMHIST_CALLED(maphist);

	simple_lock(&uobj->vmobjlock);

	UVMHIST_LOG(maphist,"  (uobj=0x%x)  ref=%d", uobj,uobj->uo_refs,0,0);
	uobj->uo_refs--;			/* drop ref! */
	if (uobj->uo_refs) {			/* still more refs */
		simple_unlock(&uobj->vmobjlock);
		UVMHIST_LOG(maphist, "<- done (rc>0)", 0,0,0,0);
		return;
	}

	/*
	 * get other pointers ...
	 */

	uvn = (struct uvm_vnode *) uobj;
	vp = (struct vnode *) uobj;

	/*
	 * clear VTEXT flag now that there are no mappings left (VTEXT is used
	 * to keep an active text file from being overwritten).
	 */
	vp->v_flag &= ~VTEXT;

	/*
	 * we just dropped the last reference to the uvn.   see if we can
	 * let it "stick around".
	 */

	if (uvn->u_flags & UVM_VNODE_CANPERSIST) {
		/* won't block */
		uvn_flush(uobj, 0, 0, PGO_DEACTIVATE|PGO_ALLPAGES);
		simple_unlock(&uobj->vmobjlock);
		vrele(vp);			/* drop vnode reference */
		UVMHIST_LOG(maphist,"<- done/vrele!  (persist)", 0,0,0,0);
		return;
	}

	/*
	 * its a goner!
	 */

	UVMHIST_LOG(maphist,"  its a goner (flushing)!", 0,0,0,0);

	uvn->u_flags |= UVM_VNODE_DYING;

	/*
	 * even though we may unlock in flush, no one can gain a reference
	 * to us until we clear the "dying" flag [because it blocks
	 * attaches].  we will not do that until after we've disposed of all
	 * the pages with uvn_flush().  note that before the flush the only
	 * pages that could be marked PG_BUSY are ones that are in async
	 * pageout by the daemon.  (there can't be any pending "get"'s
	 * because there are no references to the object).
	 */

	(void) uvn_flush(uobj, 0, 0, PGO_CLEANIT|PGO_FREE|PGO_ALLPAGES);

	UVMHIST_LOG(maphist,"  its a goner (done flush)!", 0,0,0,0);

	/*
	 * given the structure of this pager, the above flush request will
	 * create the following state: all the pages that were in the object
	 * have either been free'd or they are marked PG_BUSY|PG_RELEASED.
	 * the PG_BUSY bit was set either by us or the daemon for async I/O.
	 * in either case, if we have pages left we can't kill the object
	 * yet because i/o is pending.  in this case we set the "relkill"
	 * flag which will cause pgo_releasepg to kill the object once all
	 * the I/O's are done [pgo_releasepg will be called from the aiodone
	 * routine or from the page daemon].
	 */

	if (uobj->uo_npages) {		/* I/O pending.  iodone will free */
#ifdef DEBUG
		/* 
		 * XXXCDC: very unlikely to happen until we have async i/o
		 * so print a little info message in case it does.
		 */
		printf("uvn_detach: vn %p has pages left after flush - "
		    "relkill mode\n", uobj);
#endif
		uvn->u_flags |= UVM_VNODE_RELKILL;
		simple_unlock(&uobj->vmobjlock);
		UVMHIST_LOG(maphist,"<- done! (releasepg will kill obj)", 0, 0,
		    0, 0);
		return;
	}

	/*
	 * kill object now.   note that we can't be on the sync q because
	 * all references are gone.
	 */
	if (uvn->u_flags & UVM_VNODE_WRITEABLE) {
		simple_lock(&uvn_wl_lock);		/* protect uvn_wlist */
		LIST_REMOVE(uvn, u_wlist);
		simple_unlock(&uvn_wl_lock);
	}
#ifdef DIAGNOSTIC
	if (uobj->memq.tqh_first != NULL)
		panic("uvn_deref: vnode VM object still has pages afer "
		    "syncio/free flush");
#endif
	oldflags = uvn->u_flags;
	uvn->u_flags = 0;
	simple_unlock(&uobj->vmobjlock);
	
	/* wake up any sleepers */
	if (oldflags & UVM_VNODE_WANTED)
		wakeup(uvn);

	/*
	 * drop our reference to the vnode.
	 */
	vrele(vp);
	UVMHIST_LOG(maphist,"<- done (vrele) final", 0,0,0,0);

	return;
}

/*
 * uvm_vnp_terminate: external hook to clear out a vnode's VM
 *
 * called in two cases:
 *  [1] when a persisting vnode vm object (i.e. one with a zero reference
 *      count) needs to be freed so that a vnode can be reused.  this
 *      happens under "getnewvnode" in vfs_subr.c.   if the vnode from
 *      the free list is still attached (i.e. not VBAD) then vgone is
 *	called.   as part of the vgone trace this should get called to
 *	free the vm object.   this is the common case.
 *  [2] when a filesystem is being unmounted by force (MNT_FORCE, 
 *	"umount -f") the vgone() function is called on active vnodes
 *	on the mounted file systems to kill their data (the vnodes become
 *	"dead" ones [see src/sys/miscfs/deadfs/...]).  that results in a
 *	call here (even if the uvn is still in use -- i.e. has a non-zero
 *	reference count).  this case happens at "umount -f" and during a
 *	"reboot/halt" operation.
 *
 * => the caller must XLOCK and VOP_LOCK the vnode before calling us
 *	[protects us from getting a vnode that is already in the DYING
 *	 state...]
 * => unlike uvn_detach, this function must not return until all the
 *	uvn's pages are disposed of.
 * => in case [2] the uvn is still alive after this call, but all I/O
 *	ops will fail (due to the backing vnode now being "dead").  this
 *	will prob. kill any process using the uvn due to pgo_get failing.
 */

void
uvm_vnp_terminate(vp)
	struct vnode *vp;
{
	struct uvm_vnode *uvn = &vp->v_uvm;
	int oldflags;
	UVMHIST_FUNC("uvm_vnp_terminate"); UVMHIST_CALLED(maphist);

	/*
	 * lock object and check if it is valid
	 */
	simple_lock(&uvn->u_obj.vmobjlock);
	UVMHIST_LOG(maphist, "  vp=0x%x, ref=%d, flag=0x%x", vp,
	    uvn->u_obj.uo_refs, uvn->u_flags, 0);
	if ((uvn->u_flags & UVM_VNODE_VALID) == 0) {
		simple_unlock(&uvn->u_obj.vmobjlock);
		UVMHIST_LOG(maphist, "<- done (not active)", 0, 0, 0, 0);
		return;
	}

	/*
	 * must be a valid uvn that is not already dying (because XLOCK
	 * protects us from that).   the uvn can't in the ALOCK state
	 * because it is valid, and uvn's that are in the ALOCK state haven't
	 * been marked valid yet.
	 */

#ifdef DEBUG
	/*
	 * debug check: are we yanking the vnode out from under our uvn?
	 */
	if (uvn->u_obj.uo_refs) {
		printf("uvm_vnp_terminate(%p): terminating active vnode "
		    "(refs=%d)\n", uvn, uvn->u_obj.uo_refs);
	} 
#endif
	
	/*
	 * it is possible that the uvn was detached and is in the relkill
	 * state [i.e. waiting for async i/o to finish so that releasepg can
	 * kill object].  we take over the vnode now and cancel the relkill.
	 * we want to know when the i/o is done so we can recycle right
	 * away.   note that a uvn can only be in the RELKILL state if it
	 * has a zero reference count.
	 */
	
	if (uvn->u_flags & UVM_VNODE_RELKILL)
		uvn->u_flags &= ~UVM_VNODE_RELKILL;	/* cancel RELKILL */

	/*
	 * block the uvn by setting the dying flag, and then flush the
	 * pages.  (note that flush may unlock object while doing I/O, but
	 * it will re-lock it before it returns control here).
	 *
	 * also, note that we tell I/O that we are already VOP_LOCK'd so
	 * that uvn_io doesn't attempt to VOP_LOCK again.
	 *
	 * XXXCDC: setting VNISLOCKED on an active uvn which is being terminated
	 *	due to a forceful unmount might not be a good idea.  maybe we
	 *	need a way to pass in this info to uvn_flush through a
	 *	pager-defined PGO_ constant [currently there are none].
	 */
	uvn->u_flags |= UVM_VNODE_DYING|UVM_VNODE_VNISLOCKED;

	(void) uvn_flush(&uvn->u_obj, 0, 0, PGO_CLEANIT|PGO_FREE|PGO_ALLPAGES);

	/*
	 * as we just did a flush we expect all the pages to be gone or in 
	 * the process of going.  sleep to wait for the rest to go [via iosync].
	 */

	while (uvn->u_obj.uo_npages) {
#ifdef DEBUG
		struct vm_page *pp;
		for (pp = uvn->u_obj.memq.tqh_first ; pp != NULL ; 
		     pp = pp->listq.tqe_next) {
			if ((pp->flags & PG_BUSY) == 0)
				panic("uvm_vnp_terminate: detected unbusy pg");
		}
		if (uvn->u_nio == 0)
			panic("uvm_vnp_terminate: no I/O to wait for?");
		printf("uvm_vnp_terminate: waiting for I/O to fin.\n");
		/* 
		 * XXXCDC: this is unlikely to happen without async i/o so we 
		 * put a printf in just to keep an eye on it.
		 */
#endif
		uvn->u_flags |= UVM_VNODE_IOSYNC;
		UVM_UNLOCK_AND_WAIT(&uvn->u_nio, &uvn->u_obj.vmobjlock, FALSE, 
		    "uvn_term",0);
		simple_lock(&uvn->u_obj.vmobjlock);
	}

	/*
	 * done.   now we free the uvn if its reference count is zero
	 * (true if we are zapping a persisting uvn).   however, if we are
	 * terminating a uvn with active mappings we let it live ... future
	 * calls down to the vnode layer will fail.
	 */

	oldflags = uvn->u_flags;
	if (uvn->u_obj.uo_refs) {

		/*
		 * uvn must live on it is dead-vnode state until all references 
		 * are gone.   restore flags.    clear CANPERSIST state.
		 */

		uvn->u_flags &= ~(UVM_VNODE_DYING|UVM_VNODE_VNISLOCKED|
		      UVM_VNODE_WANTED|UVM_VNODE_CANPERSIST);
	
	} else {

		/*
		 * free the uvn now.   note that the VREF reference is already
		 * gone [it is dropped when we enter the persist state].
		 */
		if (uvn->u_flags & UVM_VNODE_IOSYNCWANTED)
			panic("uvm_vnp_terminate: io sync wanted bit set");

		if (uvn->u_flags & UVM_VNODE_WRITEABLE) {
			simple_lock(&uvn_wl_lock);
			LIST_REMOVE(uvn, u_wlist);
			simple_unlock(&uvn_wl_lock);
		}
		uvn->u_flags = 0;	/* uvn is history, clear all bits */
	}

	if (oldflags & UVM_VNODE_WANTED)
		wakeup(uvn);		/* object lock still held */

	simple_unlock(&uvn->u_obj.vmobjlock);
	UVMHIST_LOG(maphist, "<- done", 0, 0, 0, 0);

d269 1
a269 1
 * => if (nextpgp != NULL) => we return pageq.tqe_next here, and return
d281 2
a282 6
	struct uvm_vnode *uvn = (struct uvm_vnode *) pg->uobject;
#ifdef DIAGNOSTIC
	if ((pg->flags & PG_RELEASED) == 0)
		panic("uvn_releasepg: page not released!");
#endif
	
d289 1
a289 1
		*nextpgp = pg->pageq.tqe_next;	/* next page for daemon */
a293 26
	/*
	 * now see if we need to kill the object
	 */
	if (uvn->u_flags & UVM_VNODE_RELKILL) {
		if (uvn->u_obj.uo_refs)
			panic("uvn_releasepg: kill flag set on referenced "
			    "object!");
		if (uvn->u_obj.uo_npages == 0) {
			if (uvn->u_flags & UVM_VNODE_WRITEABLE) {
				simple_lock(&uvn_wl_lock);
				LIST_REMOVE(uvn, u_wlist);
				simple_unlock(&uvn_wl_lock);
			}
#ifdef DIAGNOSTIC
			if (uvn->u_obj.memq.tqh_first)
	panic("uvn_releasepg: pages in object with npages == 0");
#endif
			if (uvn->u_flags & UVM_VNODE_WANTED)
				/* still holding object lock */
				wakeup(uvn);

			uvn->u_flags = 0;		/* DEAD! */
			simple_unlock(&uvn->u_obj.vmobjlock);
			return (FALSE);
		}
	}
a297 6
 * NOTE: currently we have to use VOP_READ/VOP_WRITE because they go
 * through the buffer cache and allow I/O in any size.  These VOPs use
 * synchronous i/o.  [vs. VOP_STRATEGY which can be async, but doesn't
 * go through the buffer cache or allow I/O sizes larger than a
 * block].  we will eventually want to change this.
 *
a298 1
 *   uvm provides the uvm_aiodesc structure for async i/o management.
d301 2
a302 2
 * an aiodesc on the "pending" list (protected by splbio()), starts the
 * i/o and returns VM_PAGER_PEND.    when the i/o is done, we expect
d304 1
a304 1
 * time).   this function should remove the aiodesc from the pending list
d307 1
a307 7
 * list and call the "aiodone" hook for each done request (see uvm_pager.c).
 * [in the old vm code, this was done by calling the "put" routine with
 * null arguments which made the code harder to read and understand because
 * you had one function ("put") doing two things.]  
 *
 * so the current pager needs: 
 *   int uvn_aiodone(struct uvm_aiodesc *)
d328 1
d330 2
a331 1
 *	if (and only if) we need to clean a page (PGO_CLEANIT).
d333 6
a338 6
 * => if PGO_CLEANIT is set, we may block (due to I/O).   thus, a caller
 *	might want to unlock higher level resources (e.g. vm_map)
 *	before calling flush.
 * => if PGO_CLEANIT is not set, then we will neither unlock the object
 *	or block.
 * => if PGO_ALLPAGE is set, then all pages in the object are valid targets
d358 1
a358 1
 *	cleaning the page for us (how nice!).    in this case, if we 
d360 1
a360 1
 *	object we need to wait for the other PG_BUSY pages to clear 
d368 1
a368 1
 *	on how many pages are in the object it may be cheaper to do one 
d374 1
a374 1
 *	list traversal, so we multiply the number of pages in the 
d380 1
a380 1
static boolean_t
d386 2
a387 1
	struct uvm_vnode *uvn = (struct uvm_vnode *) uobj;
d389 2
a390 1
	struct vm_page *pps[MAXBSIZE >> PAGE_SHIFT], **ppsp;
d392 2
a393 1
	boolean_t retval, need_iosync, by_list, needs_clean, all;
d397 22
a419 1
	curoff = 0;	/* XXX: shut up gcc */
d424 4
d429 2
a430 1
	retval = TRUE;		/* return value */
d433 1
a433 1
		by_list = TRUE;		/* always go by the list */
a436 5
#ifdef DEBUG
		if (stop > round_page(uvn->u_size))
			printf("uvn_flush: strange, got an out of range "
			    "flush (fixed)\n");
#endif
d438 1
a438 1
		by_list = (uobj->uo_npages <= 
d460 1
a460 2
			for (pp = uobj->memq.tqh_first ; pp != NULL ;
			    pp = pp->listq.tqe_next) {
d484 1
a484 1
		pp = uobj->memq.tqh_first;
d490 3
a492 3
	ppnext = NULL;	/* XXX: shut up gcc */ 
	ppsp = NULL;		/* XXX: shut up gcc */
	uvm_lock_pageq();	/* page queues locked */
d495 2
a496 3
	for ( ; (by_list && pp != NULL) || 
	  (!by_list && curoff < stop) ; pp = ppnext) {

a497 5

			/*
			 * range check
			 */

d500 1
a500 1
				ppnext = pp->listq.tqe_next;
a502 1

a503 5

			/*
			 * null check
			 */

a509 1

d516 1
a516 1
		 * 
d525 1
a525 3
			if ((pp->flags & PG_BUSY) != 0 &&
			    (flags & (PGO_CLEANIT|PGO_SYNCIO)) ==
			             (PGO_CLEANIT|PGO_SYNCIO))
d528 1
d533 1
a533 1
			if ((pp->flags & PG_CLEAN) != 0 && 
d535 2
a536 1
			    (pp->pqflags & PQ_ACTIVE) != 0)
d541 1
a541 2
			pp->flags |= PG_CLEANCHK;	/* update "hint" */

a548 1
			/* load ppnext */
d550 1
a550 1
				ppnext = pp->listq.tqe_next;
a555 1
			/* now dispose of pp */
d558 1
d560 1
a560 1
					pmap_page_protect(pp, VM_PROT_NONE);
a565 1
					/* release busy pages */
a568 1
					/* removed page from object */
d585 1
a589 1
ReTry:
d594 2
a595 2
		result = uvm_pager_put(uobj, pp, &ppsp, &npages, 
			   flags | PGO_DOACTCLUST, start, stop);
d600 2
a601 2
		 * it is remotely possible for the async i/o to complete and 
		 * the page "pp" be freed or what not before we get a chance 
d611 4
a614 27
		 * VM_PAGER_AGAIN: given the structure of this pager, this 
		 * can only happen when  we are doing async I/O and can't
		 * map the pages into kernel memory (pager_map) due to lack
		 * of vm space.   if this happens we drop back to sync I/O.
		 */

		if (result == VM_PAGER_AGAIN) {
			/* 
			 * it is unlikely, but page could have been released
			 * while we had the object lock dropped.   we ignore
			 * this now and retry the I/O.  we will detect and
			 * handle the released page after the syncio I/O
			 * completes.
			 */
#ifdef DIAGNOSTIC
			if (flags & PGO_SYNCIO)
	panic("uvn_flush: PGO_SYNCIO return 'try again' error (impossible)");
#endif
			flags |= PGO_SYNCIO;
			goto ReTry;
		}

		/*
		 * the cleaning operation is now done.   finish up.  note that
		 * on error (!OK, !PEND) uvm_pager_put drops the cluster for us.
		 * if success (OK, PEND) then uvm_pager_put returns the cluster
		 * to us in ppsp/npages.
d622 2
a623 1
		if (result == VM_PAGER_PEND) {
d625 11
a635 16
			if ((flags & (PGO_DEACTIVATE|PGO_FREE)) == 0) {
				/*
				 * no per-page ops: refresh ppnext and continue
				 */
				if (by_list) {
					if (pp->version == pp_version)
						ppnext = pp->listq.tqe_next;
					else
						/* reset */
						ppnext = uobj->memq.tqh_first;
				} else {
					if (curoff < stop)
						ppnext = uvm_pagelookup(uobj,
						    curoff);
				}
				continue;
d637 1
a637 2

			/* need to do anything here? */
d641 2
a642 2
		 * need to look at each page of the I/O operation.  we defer 
		 * processing "pp" until the last trip through this "for" loop 
d644 1
a644 1
		 * play with the cluster pages [thus the "npages + 1" in the 
d664 1
a664 1
						ppnext = pp->listq.tqe_next;
d666 2
a667 2
						/* reset */
						ppnext = uobj->memq.tqh_first;
d670 2
a671 1
					ppnext = uvm_pagelookup(uobj, curoff);
d676 1
a676 1
			 * verify the page didn't get moved while obj was
d679 1
a679 1
			if (result == VM_PAGER_PEND && ptmp->uobject != uobj)
d684 1
a684 1
			 * pending I/O it is possible that the I/O op
d689 2
a690 2
			if (result != VM_PAGER_PEND) {
				if (ptmp->flags & PG_WANTED)
d693 1
a693 1

a696 2

					/* pgo_releasepg wants this */
d698 4
a701 1
					if (!uvn_releasepg(ptmp, NULL))
d703 3
a705 4

					uvm_lock_pageq();	/* relock */
					continue;		/* next page */

d707 9
a715 3
					ptmp->flags |= (PG_CLEAN|PG_CLEANCHK);
					if ((flags & PGO_FREE) == 0)
						pmap_clear_modify(ptmp);
d718 1
a718 1
	  
d725 1
d727 1
a727 1
					pmap_page_protect(ptmp, VM_PROT_NONE);
a729 1

d731 1
a731 1
				if (result == VM_PAGER_PEND) {
d736 1
a736 1
					if (result != VM_PAGER_OK) {
d738 1
a738 2
						   "offset=0x%llx.  error "
						   "during pageout.\n",
d740 2
a741 1
						    (long long)pp->offset);
a750 1

a751 1

a753 3
	/*
	 * done with pagequeues: unlock
	 */
d755 10
d766 4
a769 4
	/*
	 * now wait for all I/O if required.
	 */
	if (need_iosync) {
d771 9
a779 5
		UVMHIST_LOG(maphist,"  <<DOING IOSYNC>>",0,0,0,0);
		while (uvn->u_nio != 0) {
			uvn->u_flags |= UVM_VNODE_IOSYNC;
			UVM_UNLOCK_AND_WAIT(&uvn->u_nio, &uvn->u_obj.vmobjlock, 
			  FALSE, "uvn_flush",0);
d782 1
a782 3
		if (uvn->u_flags & UVM_VNODE_IOSYNCWANTED)
			wakeup(&uvn->u_flags);
		uvn->u_flags &= ~(UVM_VNODE_IOSYNC|UVM_VNODE_IOSYNCWANTED);
d806 2
a807 1
	struct uvm_vnode *uvn = (struct uvm_vnode *) uobj;
d809 1
a809 12

	if (*loffset >= uvn->u_size)
		panic("uvn_cluster: offset out of range");

	/*
	 * XXX: old pager claims we could use VOP_BMAP to get maxcontig value.
	 */
	*hoffset = *loffset + MAXBSIZE;
	if (*hoffset > round_page(uvn->u_size))	/* past end? */
		*hoffset = round_page(uvn->u_size);

	return;
a814 1
 * => prefer map unlocked (not required)
a817 2
 * => XXX: currently we use VOP_READ/VOP_WRITE which are only sync.
 *	[thus we never do async i/o!  see iodone comment]
d820 1
a820 1
static int
d826 2
a827 5
	int retval;

	/* note: object locked */
	retval = uvn_io((struct uvm_vnode*)uobj, pps, npages, flags, UIO_WRITE);
	/* note: object unlocked */
d829 2
a830 1
	return(retval);
d844 2
a845 2
 
static int
d851 1
a851 1
	int centeridx, advice, flags;
d853 1
d855 17
a871 6
	voff_t current_offset;
	struct vm_page *ptmp;
	int lcv, result, gotpages;
	boolean_t done;
	UVMHIST_FUNC("uvn_get"); UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist, "flags=%d", flags,0,0,0);
a872 3
	/*
	 * step 1: handled the case where fault data structures are locked.
	 */
d874 6
a879 1
	if (flags & PGO_LOCKED) {
d881 9
a889 4
		/*
		 * gotpages is the current number of pages we've gotten (which
		 * we pass back up to caller via *npagesp.
		 */
d891 7
a897 1
		gotpages = 0;
d899 10
a908 17
		/*
		 * step 1a: get pages that are already resident.   only do this
		 * if the data structures are locked (i.e. the first time
		 * through).
		 */

		done = TRUE;	/* be optimistic */

		for (lcv = 0, current_offset = offset ; lcv < *npagesp ;
		    lcv++, current_offset += PAGE_SIZE) {

			/* do we care about this page?  if not, skip it */
			if (pps[lcv] == PGO_DONTCARE)
				continue;

			/* lookup page */
			ptmp = uvm_pagelookup(uobj, current_offset);
d910 13
a922 7
			/* to be useful must get a non-busy, non-released pg */
			if (ptmp == NULL ||
			    (ptmp->flags & (PG_BUSY|PG_RELEASED)) != 0) {
				if (lcv == centeridx || (flags & PGO_ALLPAGES)
				    != 0)
				done = FALSE;	/* need to do a wait or I/O! */
				continue;
d924 5
a928 90

			/*
			 * useful page: busy/lock it and plug it in our
			 * result array
			 */
			ptmp->flags |= PG_BUSY;		/* loan up to caller */
			UVM_PAGE_OWN(ptmp, "uvn_get1");
			pps[lcv] = ptmp;
			gotpages++;

		}	/* "for" lcv loop */

		/*
		 * XXX: given the "advice", should we consider async read-ahead?
		 * XXX: fault current does deactive of pages behind us.  is 
		 * this good (other callers might now).
		 */
		/* 
		 * XXX: read-ahead currently handled by buffer cache (bread)
		 * level.
		 * XXX: no async i/o available.
		 * XXX: so we don't do anything now.
		 */

		/*
		 * step 1c: now we've either done everything needed or we to
		 * unlock and do some waiting or I/O.
		 */

		*npagesp = gotpages;		/* let caller know */
		if (done)
			return(VM_PAGER_OK);		/* bingo! */
		else
			/* EEK!   Need to unlock and I/O */
			return(VM_PAGER_UNLOCK);
	}

	/*
	 * step 2: get non-resident or busy pages.
	 * object is locked.   data structures are unlocked.
	 *
	 * XXX: because we can't do async I/O at this level we get things
	 * page at a time (otherwise we'd chunk).   the VOP_READ() will do 
	 * async-read-ahead for us at a lower level.
	 */

	for (lcv = 0, current_offset = offset ; 
			 lcv < *npagesp ; lcv++, current_offset += PAGE_SIZE) {
		
		/* skip over pages we've already gotten or don't want */
		/* skip over pages we don't _have_ to get */
		if (pps[lcv] != NULL || (lcv != centeridx &&
		    (flags & PGO_ALLPAGES) == 0))
			continue;

		/*
		 * we have yet to locate the current page (pps[lcv]).   we first
		 * look for a page that is already at the current offset.   if
		 * we fine a page, we check to see if it is busy or released.
		 * if that is the case, then we sleep on the page until it is
		 * no longer busy or released and repeat the lookup.    if the
		 * page we found is neither busy nor released, then we busy it
		 * (so we own it) and plug it into pps[lcv].   this breaks the
		 * following while loop and indicates we are ready to move on
		 * to the next page in the "lcv" loop above.
		 *
		 * if we exit the while loop with pps[lcv] still set to NULL,
		 * then it means that we allocated a new busy/fake/clean page
		 * ptmp in the object and we need to do I/O to fill in the data.
		 */

		while (pps[lcv] == NULL) {	/* top of "pps" while loop */
			
			/* look for a current page */
			ptmp = uvm_pagelookup(uobj, current_offset);

			/* nope?   allocate one now (if we can) */
			if (ptmp == NULL) {

				ptmp = uvm_pagealloc(uobj, current_offset,
				    NULL, 0);

				/* out of RAM? */
				if (ptmp == NULL) {
					simple_unlock(&uobj->vmobjlock);
					uvm_wait("uvn_getpage");
					simple_lock(&uobj->vmobjlock);

					/* goto top of pps while loop */
					continue;	
d930 4
a933 6

				/* 
				 * got new page ready for I/O.  break pps
				 * while loop.  pps[lcv] is still NULL.
				 */
				break;		
d935 4
a938 8

			/* page is there, see if we need to wait on it */
			if ((ptmp->flags & (PG_BUSY|PG_RELEASED)) != 0) {
				ptmp->flags |= PG_WANTED;
				UVM_UNLOCK_AND_WAIT(ptmp,
				    &uobj->vmobjlock, FALSE, "uvn_get",0);
				simple_lock(&uobj->vmobjlock);
				continue;	/* goto top of pps while loop */
d940 5
a944 10
			
			/* 
			 * if we get here then the page has become resident
			 * and unbusy between steps 1 and 2.  we busy it
			 * now (so we own it) and set pps[lcv] (so that we
			 * exit the while loop).
			 */
			ptmp->flags |= PG_BUSY;
			UVM_PAGE_OWN(ptmp, "uvn_get2");
			pps[lcv] = ptmp;
d947 11
a957 38
		/*
		 * if we own the a valid page at the correct offset, pps[lcv]
		 * will point to it.   nothing more to do except go to the
		 * next page.
		 */

		if (pps[lcv])
			continue;			/* next lcv */

		/*
		 * we have a "fake/busy/clean" page that we just allocated.  do
		 * I/O to fill it with valid data.  note that object must be
		 * locked going into uvn_io, but will be unlocked afterwards.
		 */

		result = uvn_io((struct uvm_vnode *) uobj, &ptmp, 1,
		    PGO_SYNCIO, UIO_READ);

		/*
		 * I/O done.   object is unlocked (by uvn_io).   because we used
		 * syncio the result can not be PEND or AGAIN.   we must relock
		 * and check for errors.
		 */

		/* lock object.   check for errors.   */
		simple_lock(&uobj->vmobjlock);
		if (result != VM_PAGER_OK) {
			if (ptmp->flags & PG_WANTED)
				/* object lock still held */
				wakeup(ptmp);

			ptmp->flags &= ~(PG_WANTED|PG_BUSY);
			UVM_PAGE_OWN(ptmp, NULL);
			uvm_lock_pageq();
			uvm_pagefree(ptmp);
			uvm_unlock_pageq();
			simple_unlock(&uobj->vmobjlock);
			return(result);
d960 4
a963 69
		/* 
		 * we got the page!   clear the fake flag (indicates valid
		 * data now in page) and plug into our result array.   note
		 * that page is still busy.   
		 *
		 * it is the callers job to:
		 * => check if the page is released
		 * => unbusy the page
		 * => activate the page
		 */

		ptmp->flags &= ~PG_FAKE;		/* data is valid ... */
		pmap_clear_modify(ptmp);		/* ... and clean */
		pps[lcv] = ptmp;

	}	/* lcv loop */

	/*
	 * finally, unlock object and return.
	 */

	simple_unlock(&uobj->vmobjlock);
	return (VM_PAGER_OK);
}

/*
 * uvn_io: do I/O to a vnode
 *
 * => prefer map unlocked (not required)
 * => object must be locked!   we will _unlock_ it before starting I/O.
 * => flags: PGO_SYNCIO -- use sync. I/O
 * => XXX: currently we use VOP_READ/VOP_WRITE which are only sync.
 *	[thus we never do async i/o!  see iodone comment]
 */

static int
uvn_io(uvn, pps, npages, flags, rw)
	struct uvm_vnode *uvn;
	vm_page_t *pps;
	int npages, flags, rw;
{
	struct vnode *vn;
	struct uio uio;
	struct iovec iov;
	vaddr_t kva;
	off_t file_offset;
	int waitf, result, mapinflags;
	size_t got, wanted;
	UVMHIST_FUNC("uvn_io"); UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist, "rw=%d", rw,0,0,0);
	
	/*
	 * init values
	 */

	waitf = (flags & PGO_SYNCIO) ? M_WAITOK : M_NOWAIT;
	vn = (struct vnode *) uvn;
	file_offset = pps[0]->offset;
	
	/*
	 * check for sync'ing I/O.
	 */
	
	while (uvn->u_flags & UVM_VNODE_IOSYNC) {
		if (waitf == M_NOWAIT) { 
			simple_unlock(&uvn->u_obj.vmobjlock);
			UVMHIST_LOG(maphist,"<- try again (iosync)",0,0,0,0);
			return(VM_PAGER_AGAIN);
a964 9
		uvn->u_flags |= UVM_VNODE_IOSYNCWANTED;
		UVM_UNLOCK_AND_WAIT(&uvn->u_flags, &uvn->u_obj.vmobjlock, 
			FALSE, "uvn_iosync",0);
		simple_lock(&uvn->u_obj.vmobjlock);
	}

	/*
	 * check size
	 */
d966 5
a970 4
	if (file_offset >= uvn->u_size) {
			simple_unlock(&uvn->u_obj.vmobjlock);
			UVMHIST_LOG(maphist,"<- BAD (size check)",0,0,0,0);
			return(VM_PAGER_BAD);
d972 2
a973 252

	/*
	 * first try and map the pages in (without waiting)
	 */

	mapinflags = (rw == UIO_READ) ?
	    UVMPAGER_MAPIN_READ : UVMPAGER_MAPIN_WRITE;

	kva = uvm_pagermapin(pps, npages, mapinflags);
	if (kva == 0 && waitf == M_NOWAIT) {
		simple_unlock(&uvn->u_obj.vmobjlock);
		UVMHIST_LOG(maphist,"<- mapin failed (try again)",0,0,0,0);
		return(VM_PAGER_AGAIN);
	}

	/*
	 * ok, now bump u_nio up.   at this point we are done with uvn
	 * and can unlock it.   if we still don't have a kva, try again
	 * (this time with sleep ok).
	 */
	
	uvn->u_nio++;			/* we have an I/O in progress! */
	simple_unlock(&uvn->u_obj.vmobjlock);
	/* NOTE: object now unlocked */
	if (kva == 0)
		kva = uvm_pagermapin(pps, npages,
		    mapinflags | UVMPAGER_MAPIN_WAITOK);

	/*
	 * ok, mapped in.  our pages are PG_BUSY so they are not going to
	 * get touched (so we can look at "offset" without having to lock
	 * the object).  set up for I/O.
	 */

	/*
	 * fill out uio/iov
	 */
	
	iov.iov_base = (caddr_t) kva;
	wanted = npages << PAGE_SHIFT;
	if (file_offset + wanted > uvn->u_size)
		wanted = uvn->u_size - file_offset;	/* XXX: needed? */
	iov.iov_len = wanted;
	uio.uio_iov = &iov;
	uio.uio_iovcnt = 1;
	uio.uio_offset = file_offset;
	uio.uio_segflg = UIO_SYSSPACE;
	uio.uio_rw = rw;
	uio.uio_resid = wanted;
	uio.uio_procp = curproc;

	/*
	 * do the I/O!  (XXX: curproc?)
	 */

	UVMHIST_LOG(maphist, "calling VOP",0,0,0,0);

	/*
	 * This process may already have this vnode locked, if we faulted in
	 * copyin() or copyout() on a region backed by this vnode
	 * while doing I/O to the vnode.  If this is the case, don't
	 * panic.. instead, return the error to the user.
	 *
	 * XXX this is a stopgap to prevent a panic.
	 * Ideally, this kind of operation *should* work.
	 */
	result = 0;
	if ((uvn->u_flags & UVM_VNODE_VNISLOCKED) == 0)
		result = vn_lock(vn, LK_EXCLUSIVE | LK_RETRY | LK_RECURSEFAIL, curproc);

	if (result == 0) {
		/* NOTE: vnode now locked! */

		if (rw == UIO_READ)
			result = VOP_READ(vn, &uio, 0, curproc->p_ucred);
		else
			result = VOP_WRITE(vn, &uio, 0, curproc->p_ucred);

		if ((uvn->u_flags & UVM_VNODE_VNISLOCKED) == 0)
			VOP_UNLOCK(vn, 0, curproc);
	}
	
	/* NOTE: vnode now unlocked (unless vnislocked) */

	UVMHIST_LOG(maphist, "done calling VOP",0,0,0,0);

	/*
	 * result == unix style errno (0 == OK!)
	 *
	 * zero out rest of buffer (if needed)
	 */

	if (result == 0) {
		got = wanted - uio.uio_resid;

		if (wanted && got == 0) {
			result = EIO;		/* XXX: error? */
		} else if (got < PAGE_SIZE * npages && rw == UIO_READ) {
			memset((void *) (kva + got), 0,
			       (npages << PAGE_SHIFT) - got);
		}
	}

	/*
	 * now remove pager mapping
	 */
	uvm_pagermapout(kva, npages);
		
	/*
	 * now clean up the object (i.e. drop I/O count)
	 */

	simple_lock(&uvn->u_obj.vmobjlock);
	/* NOTE: object now locked! */

	uvn->u_nio--;			/* I/O DONE! */
	if ((uvn->u_flags & UVM_VNODE_IOSYNC) != 0 && uvn->u_nio == 0) {
		wakeup(&uvn->u_nio);
	}
	simple_unlock(&uvn->u_obj.vmobjlock);
	/* NOTE: object now unlocked! */

	/*
	 * done!
	 */

	UVMHIST_LOG(maphist, "<- done (result %d)", result,0,0,0);
	if (result == 0)
		return(VM_PAGER_OK);
	else
		return(VM_PAGER_ERROR);
}

/*
 * uvm_vnp_uncache: disable "persisting" in a vnode... when last reference
 * is gone we will kill the object (flushing dirty pages back to the vnode
 * if needed).
 *
 * => returns TRUE if there was no uvm_object attached or if there was
 *	one and we killed it [i.e. if there is no active uvn]
 * => called with the vnode VOP_LOCK'd [we will unlock it for I/O, if
 *	needed]
 *
 * => XXX: given that we now kill uvn's when a vnode is recycled (without
 *	having to hold a reference on the vnode) and given a working
 *	uvm_vnp_sync(), how does that effect the need for this function?
 *      [XXXCDC: seems like it can die?]
 *
 * => XXX: this function should DIE once we merge the VM and buffer 
 *	cache.
 *
 * research shows that this is called in the following places:
 * ext2fs_truncate, ffs_truncate, detrunc[msdosfs]: called when vnode
 *	changes sizes
 * ext2fs_write, WRITE [ufs_readwrite], msdosfs_write: called when we
 *	are written to
 * ex2fs_chmod, ufs_chmod: called if VTEXT vnode and the sticky bit
 *	is off
 * ffs_realloccg: when we can't extend the current block and have 
 *	to allocate a new one we call this [XXX: why?]
 * nfsrv_rename, rename_files: called when the target filename is there
 *	and we want to remove it
 * nfsrv_remove, sys_unlink: called on file we are removing
 * nfsrv_access: if VTEXT and we want WRITE access and we don't uncache
 *	then return "text busy"
 * nfs_open: seems to uncache any file opened with nfs
 * vn_writechk: if VTEXT vnode and can't uncache return "text busy"
 */

boolean_t
uvm_vnp_uncache(vp)
	struct vnode *vp;
{
	struct uvm_vnode *uvn = &vp->v_uvm;

	/*
	 * lock uvn part of the vnode and check to see if we need to do anything
	 */

	simple_lock(&uvn->u_obj.vmobjlock);
	if ((uvn->u_flags & UVM_VNODE_VALID) == 0 || 
			(uvn->u_flags & UVM_VNODE_BLOCKED) != 0) {
		simple_unlock(&uvn->u_obj.vmobjlock);
		return(TRUE);
	}

	/*
	 * we have a valid, non-blocked uvn.   clear persist flag.
	 * if uvn is currently active we can return now.
	 */

	uvn->u_flags &= ~UVM_VNODE_CANPERSIST;
	if (uvn->u_obj.uo_refs) {
		simple_unlock(&uvn->u_obj.vmobjlock);
		return(FALSE);
	}

	/*
	 * uvn is currently persisting!   we have to gain a reference to
	 * it so that we can call uvn_detach to kill the uvn.
	 */

	VREF(vp);			/* seems ok, even with VOP_LOCK */
	uvn->u_obj.uo_refs++;		/* value is now 1 */
	simple_unlock(&uvn->u_obj.vmobjlock);


#ifdef DEBUG
	/*
	 * carry over sanity check from old vnode pager: the vnode should
	 * be VOP_LOCK'd, and we confirm it here.
	 */
	if (!VOP_ISLOCKED(vp)) {
		boolean_t is_ok_anyway = FALSE;
#if defined(NFSCLIENT)
		extern int (**nfsv2_vnodeop_p) __P((void *));
		extern int (**spec_nfsv2nodeop_p) __P((void *));
		extern int (**fifo_nfsv2nodeop_p) __P((void *));

		/* vnode is NOT VOP_LOCKed: some vnode types _never_ lock */
		if (vp->v_op == nfsv2_vnodeop_p ||
		    vp->v_op == spec_nfsv2nodeop_p) {
			is_ok_anyway = TRUE;
		}
		if (vp->v_op == fifo_nfsv2nodeop_p) {
			is_ok_anyway = TRUE;
		}
#endif	/* defined(NFSSERVER) || defined(NFSCLIENT) */
		if (!is_ok_anyway)
			panic("uvm_vnp_uncache: vnode not locked!");
	}
#endif	/* DEBUG */

	/*
	 * now drop our reference to the vnode.   if we have the sole 
	 * reference to the vnode then this will cause it to die [as we
	 * just cleared the persist flag].   we have to unlock the vnode 
	 * while we are doing this as it may trigger I/O.
	 *
	 * XXX: it might be possible for uvn to get reclaimed while we are
	 * unlocked causing us to return TRUE when we should not.   we ignore
	 * this as a false-positive return value doesn't hurt us.
	 */
	VOP_UNLOCK(vp, 0, curproc);
	uvn_detach(&uvn->u_obj);
	vn_lock(vp, LK_EXCLUSIVE | LK_RETRY, curproc);
	
	/*
	 * and return...
	 */
	
	return(TRUE);
d982 1
a982 1
 * => we assume that the caller has a reference of some sort to the 
d1000 6
d1008 2
a1009 1
	 * lock uvn and check for valid object, and if valid: do it!
a1010 2
	simple_lock(&uvn->u_obj.vmobjlock);
	if (uvn->u_flags & UVM_VNODE_VALID) {
d1012 2
a1013 10
		/*
		 * now check if the size has changed: if we shrink we had better
		 * toss some pages...
		 */

		if (uvn->u_size > newsize) {
			(void)uvn_flush(&uvn->u_obj, newsize,
			    uvn->u_size, PGO_FREE);
		}
		uvn->u_size = newsize;
d1015 1
a1016 5

	/*
	 * done
	 */
	return;
d1020 1
a1020 8
 * uvm_vnp_sync: flush all dirty VM pages back to their backing vnodes.
 *
 * => called from sys_sync with no VM structures locked
 * => only one process can do a sync at a time (because the uvn
 *    structure only has one queue for sync'ing).  we ensure this
 *    by holding the uvn_sync_lock while the sync is in progress.
 *    other processes attempting a sync will sleep on this lock
 *    until we are done.
d1024 4
a1027 2
uvm_vnp_sync(mp)
	struct mount *mp;
d1029 1
a1029 30
	struct uvm_vnode *uvn;
	struct vnode *vp;
	boolean_t got_lock;

	/*
	 * step 1: ensure we are only ones using the uvn_sync_q by locking
	 * our lock...
	 */
	lockmgr(&uvn_sync_lock, LK_EXCLUSIVE, NULL, curproc);

	/*
	 * step 2: build up a simpleq of uvns of interest based on the 
	 * write list.   we gain a reference to uvns of interest.  must 
	 * be careful about locking uvn's since we will be holding uvn_wl_lock
	 * in the body of the loop.
	 */
	SIMPLEQ_INIT(&uvn_sync_q);
	simple_lock(&uvn_wl_lock);
	for (uvn = uvn_wlist.lh_first ; uvn != NULL ;
	    uvn = uvn->u_wlist.le_next) {

		vp = (struct vnode *) uvn;
		if (mp && vp->v_mount != mp)
			continue;

		/* attempt to gain reference */
		while ((got_lock = simple_lock_try(&uvn->u_obj.vmobjlock)) ==
		    						FALSE && 
				(uvn->u_flags & UVM_VNODE_BLOCKED) == 0)
			/* spin */ ;
d1031 14
a1044 74
		/*
		 * we will exit the loop if either if the following are true:
		 *  - we got the lock [always true if NCPU == 1]
		 *  - we failed to get the lock but noticed the vnode was
		 * 	"blocked" -- in this case the vnode must be a dying
		 *	vnode, and since dying vnodes are in the process of
		 *	being flushed out, we can safely skip this one
		 *
		 * we want to skip over the vnode if we did not get the lock,
		 * or if the vnode is already dying (due to the above logic).
		 *
		 * note that uvn must already be valid because we found it on
		 * the wlist (this also means it can't be ALOCK'd).
		 */
		if (!got_lock || (uvn->u_flags & UVM_VNODE_BLOCKED) != 0) {
			if (got_lock)
				simple_unlock(&uvn->u_obj.vmobjlock);
			continue;		/* skip it */
		}
		
		/*
		 * gain reference.   watch out for persisting uvns (need to
		 * regain vnode REF).
		 */
		if (uvn->u_obj.uo_refs == 0)
			VREF(vp);
		uvn->u_obj.uo_refs++;
		simple_unlock(&uvn->u_obj.vmobjlock);

		/*
		 * got it!
		 */
		SIMPLEQ_INSERT_HEAD(&uvn_sync_q, uvn, u_syncq);
	}
	simple_unlock(&uvn_wl_lock);

	/*
	 * step 3: we now have a list of uvn's that may need cleaning.
	 * we are holding the uvn_sync_lock, but have dropped the uvn_wl_lock
	 * (so we can now safely lock uvn's again).
	 */

	for (uvn = uvn_sync_q.sqh_first ; uvn ; uvn = uvn->u_syncq.sqe_next) {
		simple_lock(&uvn->u_obj.vmobjlock);
#ifdef DEBUG
		if (uvn->u_flags & UVM_VNODE_DYING) {
			printf("uvm_vnp_sync: dying vnode on sync list\n");
		}
#endif
		uvn_flush(&uvn->u_obj, 0, 0,
		    PGO_CLEANIT|PGO_ALLPAGES|PGO_DOACTCLUST);

		/*
		 * if we have the only reference and we just cleaned the uvn,
		 * then we can pull it out of the UVM_VNODE_WRITEABLE state
		 * thus allowing us to avoid thinking about flushing it again
		 * on later sync ops.
		 */
		if (uvn->u_obj.uo_refs == 1 &&
		    (uvn->u_flags & UVM_VNODE_WRITEABLE)) {
			LIST_REMOVE(uvn, u_wlist);
			uvn->u_flags &= ~UVM_VNODE_WRITEABLE;
		}

		simple_unlock(&uvn->u_obj.vmobjlock);

		/* now drop our reference to the uvn */
		uvn_detach(&uvn->u_obj);
	}

	/*
	 * done!  release sync lock
	 */
	lockmgr(&uvn_sync_lock, LK_RELEASE, (void *)0, curproc);
@


1.6.2.8
log
@Merge in -current from roughly a week ago
@
text
@d87 6
a92 6
static void		   uvn_cluster(struct uvm_object *, voff_t,
					   voff_t *, voff_t *);
static void                uvn_detach(struct uvm_object *);
static boolean_t           uvn_flush(struct uvm_object *, voff_t, 
					 voff_t, int);
static int                 uvn_get(struct uvm_object *, voff_t,
d94 9
a102 9
					vm_prot_t, int, int);
static void		   uvn_init(void);
static int		   uvn_io(struct uvm_vnode *, vm_page_t *,
				      int, int, int);
static int		   uvn_put(struct uvm_object *, vm_page_t *,
					int, boolean_t);
static void                uvn_reference(struct uvm_object *);
static boolean_t	   uvn_releasepg(struct vm_page *, 
					      struct vm_page **);
d1672 1
a1672 1
		result = vn_lock(vn, LK_EXCLUSIVE | LK_RECURSEFAIL, curproc);
d1819 3
a1821 3
		extern int (**nfsv2_vnodeop_p)(void *);
		extern int (**spec_nfsv2nodeop_p)(void *);
		extern int (**fifo_nfsv2nodeop_p)(void *);
@


1.6.2.9
log
@Sync the SMP branch with 3.3
@
text
@d7 1
a7 1
 *      The Regents of the University of California.
d27 1
a27 1
 *	Washington University, the University of California, Berkeley and
d90 1
a90 1
static boolean_t           uvn_flush(struct uvm_object *, voff_t,
d93 1
a93 1
					vm_page_t *, int *, int,
d101 1
a101 1
static boolean_t	   uvn_releasepg(struct vm_page *,
d214 1
a214 1
		if ((accessprot & VM_PROT_WRITE) != 0 &&
d228 1
a228 1
	}
d265 1
a265 1
	simple_lock(&uvn->u_obj.vmobjlock);
d324 1
a324 1
 * count must already be at least one (the passed in reference) so
d327 1
a327 1
 * => caller must call with object unlocked.
d350 1
a350 1
	UVMHIST_LOG(maphist, "<- done (uobj=0x%x, ref = %d)",
d446 1
a446 1
		/*
d477 1
a477 1

d501 1
a501 1
 *  [2] when a filesystem is being unmounted by force (MNT_FORCE,
d553 1
a553 1
	}
d555 1
a555 1

d564 1
a564 1

d586 1
a586 1
	 * as we just did a flush we expect all the pages to be gone or in
d593 1
a593 1
		for (pp = uvn->u_obj.memq.tqh_first ; pp != NULL ;
d601 2
a602 2
		/*
		 * XXXCDC: this is unlikely to happen without async i/o so we
d607 1
a607 1
		UVM_UNLOCK_AND_WAIT(&uvn->u_nio, &uvn->u_obj.vmobjlock, FALSE,
d623 1
a623 1
		 * uvn must live on it is dead-vnode state until all references
d629 1
a629 1

d682 1
a682 1

d743 1
a743 1
 * you had one function ("put") doing two things.]
d745 1
a745 1
 * so the current pager needs:
d795 1
a795 1
 *	cleaning the page for us (how nice!).    in this case, if we
d797 1
a797 1
 *	object we need to wait for the other PG_BUSY pages to clear
d805 1
a805 1
 *	on how many pages are in the object it may be cheaper to do one
d811 1
a811 1
 *	list traversal, so we multiply the number of pages in the
d851 1
a851 1
		by_list = (uobj->uo_npages <=
d904 1
a904 1
	ppnext = NULL;	/* XXX: shut up gcc */
d909 1
a909 1
	for ( ; (by_list && pp != NULL) ||
d943 1
a943 1
		 *
d961 1
a961 1
			if ((pp->flags & PG_CLEAN) != 0 &&
d1025 1
a1025 1
		result = uvm_pager_put(uobj, pp, &ppsp, &npages,
d1031 2
a1032 2
		 * it is remotely possible for the async i/o to complete and
		 * the page "pp" be freed or what not before we get a chance
d1042 1
a1042 1
		 * VM_PAGER_AGAIN: given the structure of this pager, this
d1049 1
a1049 1
			/*
d1100 2
a1101 2
		 * need to look at each page of the I/O operation.  we defer
		 * processing "pp" until the last trip through this "for" loop
d1103 1
a1103 1
		 * play with the cluster pages [thus the "npages + 1" in the
d1170 1
a1170 1

d1221 1
a1221 1
			UVM_UNLOCK_AND_WAIT(&uvn->u_nio, &uvn->u_obj.vmobjlock,
d1304 1
a1304 1

d1374 1
a1374 1
		 * XXX: fault current does deactive of pages behind us.  is
d1377 1
a1377 1
		/*
d1402 1
a1402 1
	 * page at a time (otherwise we'd chunk).   the VOP_READ() will do
d1406 1
a1406 1
	for (lcv = 0, current_offset = offset;
d1408 1
a1408 1

d1432 1
a1432 1

d1449 1
a1449 1
					continue;
d1452 1
a1452 1
				/*
d1456 1
a1456 1
				break;
d1467 2
a1468 2

			/*
d1519 1
a1519 1
		/*
d1522 1
a1522 1
		 * that page is still busy.
d1570 1
a1570 1

d1578 1
a1578 1

d1582 1
a1582 1

d1584 1
a1584 1
		if (waitf == M_NOWAIT) {
d1590 1
a1590 1
		UVM_UNLOCK_AND_WAIT(&uvn->u_flags, &uvn->u_obj.vmobjlock,
d1624 1
a1624 1

d1641 1
a1641 1

d1685 1
a1685 1

d1711 1
a1711 1

d1752 1
a1752 1
 * => XXX: this function should DIE once we merge the VM and buffer
d1762 1
a1762 1
 * ffs_realloccg: when we can't extend the current block and have
d1784 1
a1784 1
	if ((uvn->u_flags & UVM_VNODE_VALID) == 0 ||
d1838 1
a1838 1
	 * now drop our reference to the vnode.   if we have the sole
d1840 1
a1840 1
	 * just cleared the persist flag].   we have to unlock the vnode
d1850 1
a1850 1

d1854 1
a1854 1

d1864 1
a1864 1
 * => we assume that the caller has a reference of some sort to the
d1934 2
a1935 2
	 * step 2: build up a simpleq of uvns of interest based on the
	 * write list.   we gain a reference to uvns of interest.  must
d1950 1
a1950 1
								FALSE &&
d1952 1
a1952 1
			/* spin */;
d1958 1
a1958 1
		 *	"blocked" -- in this case the vnode must be a dying
d1973 1
a1973 1

d1996 1
a1996 1
	SIMPLEQ_FOREACH(uvn, &uvn_sync_q, u_syncq) {
@


1.6.2.10
log
@Merge of current from two weeks agointo the SMP branch
@
text
@a1820 1
#if defined(FIFO)
a1821 1
#endif	/* defined(FIFO) */
a1827 1
#if defined(FIFO)
a1830 1
#endif	/* defined(FIFO) */
@


1.6.2.11
log
@sync to head
@
text
@d178 1
a178 1
		printf("uvn_attach: blocked at %p flags 0x%x\n",
@


1.5
log
@Put the "size check fired" behind ifdef DEBUG, it can happen in some
legitimate cases.
@
text
@d1853 1
a1853 1
#if defined(NFSSERVER) || defined(NFSCLIENT)
@


1.4
log
@sync with NetBSD from 1999.05.24 (there is a reason for this date)
 Mostly cleanups, but also a few improvements to pagedaemon for better
 handling of low memory and/or low swap conditions.
@
text
@d1637 1
a1637 1
#ifdef DIAGNOSTIC
@


1.3
log
@NFS -> NFSSERVER|NFSCLIENT
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_vnode.c,v 1.2 1999/02/26 05:32:08 art Exp $	*/
/*	$NetBSD: uvm_vnode.c,v 1.18 1999/01/29 12:56:17 bouyer Exp $	*/
a3 4
 * XXXCDC: "ROUGH DRAFT" QUALITY UVM PRE-RELEASE FILE!   
 *         >>>USE AT YOUR OWN RISK, WORK IS NOT FINISHED<<<
 */
/*
a115 1
	uvn_attach,
d289 1
a289 1
		printf("used_vnode_size = %qu\n", used_vnode_size);
d294 1
a294 1
		    used_vnode_size, -PAGE_SIZE);
d1455 1
a1455 1
				    NULL);	/* alloc */
d1696 10
d1707 4
a1710 2
		vn_lock(vn, LK_EXCLUSIVE | LK_RETRY, curproc /*XXX*/);
	/* NOTE: vnode now locked! */
d1712 4
a1715 4
	if (rw == UIO_READ)
		result = VOP_READ(vn, &uio, 0, curproc->p_ucred);
	else
		result = VOP_WRITE(vn, &uio, 0, curproc->p_ucred);
d1717 4
a1720 2
	if ((uvn->u_flags & UVM_VNODE_VNISLOCKED) == 0)
		VOP_UNLOCK(vn, 0, curproc /*XXX*/);
d1932 2
a1933 1
			    "%qx->%lx\n", vp, newsize, (vaddr_t)-PAGE_SIZE);
@


1.2
log
@add OpenBSD tags
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d1845 1
a1845 1
#ifdef NFS
d1858 1
a1858 1
#endif	/* NFS */
@


1.1
log
@Import of uvm from NetBSD. Some local changes, some code disabled
@
text
@d1 1
@

