head	1.141;
access;
symbols
	OPENBSD_6_1:1.141.0.4
	OPENBSD_6_1_BASE:1.141
	OPENBSD_6_0:1.139.0.2
	OPENBSD_6_0_BASE:1.139
	OPENBSD_5_9:1.137.0.2
	OPENBSD_5_9_BASE:1.137
	OPENBSD_5_8:1.133.0.4
	OPENBSD_5_8_BASE:1.133
	OPENBSD_5_7:1.131.0.2
	OPENBSD_5_7_BASE:1.131
	OPENBSD_5_6:1.119.0.4
	OPENBSD_5_6_BASE:1.119
	OPENBSD_5_5:1.110.0.4
	OPENBSD_5_5_BASE:1.110
	OPENBSD_5_4:1.109.0.2
	OPENBSD_5_4_BASE:1.109
	OPENBSD_5_3:1.104.0.4
	OPENBSD_5_3_BASE:1.104
	OPENBSD_5_2:1.104.0.2
	OPENBSD_5_2_BASE:1.104
	OPENBSD_5_1_BASE:1.103
	OPENBSD_5_1:1.103.0.4
	OPENBSD_5_0:1.103.0.2
	OPENBSD_5_0_BASE:1.103
	OPENBSD_4_9:1.89.0.4
	OPENBSD_4_9_BASE:1.89
	OPENBSD_4_8:1.89.0.2
	OPENBSD_4_8_BASE:1.89
	OPENBSD_4_7:1.83.0.2
	OPENBSD_4_7_BASE:1.83
	OPENBSD_4_6:1.81.0.4
	OPENBSD_4_6_BASE:1.81
	OPENBSD_4_5:1.73.0.2
	OPENBSD_4_5_BASE:1.73
	OPENBSD_4_4:1.70.0.2
	OPENBSD_4_4_BASE:1.70
	OPENBSD_4_3:1.68.0.2
	OPENBSD_4_3_BASE:1.68
	OPENBSD_4_2:1.66.0.2
	OPENBSD_4_2_BASE:1.66
	OPENBSD_4_1:1.63.0.2
	OPENBSD_4_1_BASE:1.63
	OPENBSD_4_0:1.59.0.4
	OPENBSD_4_0_BASE:1.59
	OPENBSD_3_9:1.59.0.2
	OPENBSD_3_9_BASE:1.59
	OPENBSD_3_8:1.56.0.2
	OPENBSD_3_8_BASE:1.56
	OPENBSD_3_7:1.54.0.2
	OPENBSD_3_7_BASE:1.54
	OPENBSD_3_6:1.53.0.2
	OPENBSD_3_6_BASE:1.53
	SMP_SYNC_A:1.53
	SMP_SYNC_B:1.53
	OPENBSD_3_5:1.50.0.4
	OPENBSD_3_5_BASE:1.50
	OPENBSD_3_4:1.50.0.2
	OPENBSD_3_4_BASE:1.50
	UBC_SYNC_A:1.48
	OPENBSD_3_3:1.46.0.2
	OPENBSD_3_3_BASE:1.46
	OPENBSD_3_2:1.45.0.4
	OPENBSD_3_2_BASE:1.45
	OPENBSD_3_1:1.45.0.2
	OPENBSD_3_1_BASE:1.45
	UBC_SYNC_B:1.45
	UBC:1.39.0.2
	UBC_BASE:1.39
	OPENBSD_3_0:1.24.0.2
	OPENBSD_3_0_BASE:1.24
	OPENBSD_2_9_BASE:1.10
	OPENBSD_2_9:1.10.0.2
	OPENBSD_2_8:1.6.0.4
	OPENBSD_2_8_BASE:1.6
	OPENBSD_2_7:1.6.0.2
	OPENBSD_2_7_BASE:1.6
	SMP:1.5.0.4
	SMP_BASE:1.5
	kame_19991208:1.5
	OPENBSD_2_6:1.5.0.2
	OPENBSD_2_6_BASE:1.5
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.141
date	2017.03.05.00.45.31;	author guenther;	state Exp;
branches;
next	1.140;
commitid	UrlMJQwoMZBUiRfc;

1.140
date	2017.02.12.04.55.08;	author guenther;	state Exp;
branches;
next	1.139;
commitid	1rXx7AiXIWFC9gYa;

1.139
date	2016.06.05.08.35.57;	author stefan;	state Exp;
branches;
next	1.138;
commitid	019qqu3X3G1dG85T;

1.138
date	2016.04.04.16.34.16;	author stefan;	state Exp;
branches;
next	1.137;
commitid	mErYIUO2MMXVvZFw;

1.137
date	2015.12.02.09.50.46;	author blambert;	state Exp;
branches;
next	1.136;
commitid	KOyOSZd1vs52bAFl;

1.136
date	2015.11.11.15.59.33;	author mmcc;	state Exp;
branches;
next	1.135;
commitid	lKxbFcynv6SZSYfh;

1.135
date	2015.09.28.18.33.42;	author tedu;	state Exp;
branches;
next	1.134;
commitid	q0NP3po8xG1zGb8N;

1.134
date	2015.09.09.14.52.12;	author miod;	state Exp;
branches;
next	1.133;
commitid	Wbcnl6am8WQAyF85;

1.133
date	2015.07.19.21.21.14;	author beck;	state Exp;
branches;
next	1.132;
commitid	5wyRonomeQfnkJsF;

1.132
date	2015.05.05.02.13.46;	author guenther;	state Exp;
branches;
next	1.131;
commitid	dNPv28CJI5BxtRGW;

1.131
date	2015.02.07.08.21.24;	author miod;	state Exp;
branches;
next	1.130;
commitid	3WN6O42yLCDNeEYy;

1.130
date	2015.02.06.11.41.55;	author beck;	state Exp;
branches;
next	1.129;
commitid	Th5rFTtqeDivvpYH;

1.129
date	2015.02.05.23.51.06;	author mpi;	state Exp;
branches;
next	1.128;
commitid	ofjqNcubSq8nv7rW;

1.128
date	2014.12.17.19.42.15;	author tedu;	state Exp;
branches;
next	1.127;
commitid	G4ldVK4QwvfU3tRp;

1.127
date	2014.12.16.18.30.04;	author tedu;	state Exp;
branches;
next	1.126;
commitid	P6Av4XGqOi3rFasL;

1.126
date	2014.12.15.02.24.23;	author guenther;	state Exp;
branches;
next	1.125;
commitid	ZxaujiOM0aYQRjFY;

1.125
date	2014.12.05.04.12.48;	author uebayasi;	state Exp;
branches;
next	1.124;
commitid	lSMz5LkZVlVJKmFM;

1.124
date	2014.11.21.06.40.40;	author deraadt;	state Exp;
branches;
next	1.123;
commitid	IQ6xOpCCx2Y0mI3R;

1.123
date	2014.11.16.12.31.00;	author deraadt;	state Exp;
branches;
next	1.122;
commitid	yv0ECmCdICvq576h;

1.122
date	2014.11.15.21.42.07;	author deraadt;	state Exp;
branches;
next	1.121;
commitid	z1SZqolHTO8KQt1F;

1.121
date	2014.10.03.18.06.47;	author kettenis;	state Exp;
branches;
next	1.120;
commitid	3JKupIsvM8vNecnK;

1.120
date	2014.10.03.17.41.00;	author kettenis;	state Exp;
branches;
next	1.119;
commitid	h8HwsnqXqpePzqXu;

1.119
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.118;
commitid	7NtJNW9udCOFtDNM;

1.118
date	2014.07.08.17.19.26;	author deraadt;	state Exp;
branches;
next	1.117;
commitid	EF98ch02VpFassUi;

1.117
date	2014.07.02.06.09.49;	author matthew;	state Exp;
branches;
next	1.116;
commitid	mswsoyQHeu5M87iU;

1.116
date	2014.06.13.01.48.52;	author matthew;	state Exp;
branches;
next	1.115;
commitid	ZIoDqOtYqi740uHv;

1.115
date	2014.05.15.03.52.25;	author guenther;	state Exp;
branches;
next	1.114;

1.114
date	2014.05.06.11.50.14;	author mpi;	state Exp;
branches;
next	1.113;

1.113
date	2014.05.03.22.44.36;	author guenther;	state Exp;
branches;
next	1.112;

1.112
date	2014.04.03.21.40.10;	author tedu;	state Exp;
branches;
next	1.111;

1.111
date	2014.03.28.17.57.11;	author mpi;	state Exp;
branches;
next	1.110;

1.110
date	2014.01.30.18.16.41;	author miod;	state Exp;
branches;
next	1.109;

1.109
date	2013.07.09.15.37.43;	author beck;	state Exp;
branches;
next	1.108;

1.108
date	2013.06.11.19.01.20;	author beck;	state Exp;
branches;
next	1.107;

1.107
date	2013.05.23.01.42.59;	author tedu;	state Exp;
branches;
next	1.106;

1.106
date	2013.05.14.20.33.01;	author miod;	state Exp;
branches;
next	1.105;

1.105
date	2013.05.14.20.15.25;	author miod;	state Exp;
branches;
next	1.104;

1.104
date	2012.03.09.13.01.29;	author ariane;	state Exp;
branches;
next	1.103;

1.103
date	2011.07.08.00.10.59;	author tedu;	state Exp;
branches;
next	1.102;

1.102
date	2011.07.07.20.52.50;	author oga;	state Exp;
branches;
next	1.101;

1.101
date	2011.07.06.19.50.38;	author beck;	state Exp;
branches;
next	1.100;

1.100
date	2011.07.03.17.42.51;	author oga;	state Exp;
branches;
next	1.99;

1.99
date	2011.06.23.21.42.05;	author ariane;	state Exp;
branches;
next	1.98;

1.98
date	2011.06.06.17.10.23;	author ariane;	state Exp;
branches;
next	1.97;

1.97
date	2011.05.30.22.25.24;	author oga;	state Exp;
branches;
next	1.96;

1.96
date	2011.05.24.15.27.36;	author ariane;	state Exp;
branches;
next	1.95;

1.95
date	2011.04.18.19.23.46;	author art;	state Exp;
branches;
next	1.94;

1.94
date	2011.04.06.15.52.13;	author art;	state Exp;
branches;
next	1.93;

1.93
date	2011.04.05.01.28.05;	author art;	state Exp;
branches;
next	1.92;

1.92
date	2011.04.04.11.56.12;	author art;	state Exp;
branches;
next	1.91;

1.91
date	2011.04.04.11.24.45;	author art;	state Exp;
branches;
next	1.90;

1.90
date	2011.04.02.16.47.17;	author beck;	state Exp;
branches;
next	1.89;

1.89
date	2010.07.02.01.25.06;	author art;	state Exp;
branches;
next	1.88;

1.88
date	2010.07.01.21.27.39;	author art;	state Exp;
branches;
next	1.87;

1.87
date	2010.06.27.03.03.49;	author thib;	state Exp;
branches;
next	1.86;

1.86
date	2010.06.09.08.26.21;	author thib;	state Exp;
branches;
next	1.85;

1.85
date	2010.04.22.19.02.55;	author oga;	state Exp;
branches;
next	1.84;

1.84
date	2010.03.24.00.36.04;	author oga;	state Exp;
branches;
next	1.83;

1.83
date	2010.02.12.01.35.14;	author tedu;	state Exp;
branches;
next	1.82;

1.82
date	2009.08.11.18.43.33;	author blambert;	state Exp;
branches;
next	1.81;

1.81
date	2009.06.16.23.54.58;	author oga;	state Exp;
branches;
next	1.80;

1.80
date	2009.06.16.16.42.41;	author ariane;	state Exp;
branches;
next	1.79;

1.79
date	2009.06.14.03.04.08;	author deraadt;	state Exp;
branches;
next	1.78;

1.78
date	2009.06.07.02.01.54;	author oga;	state Exp;
branches;
next	1.77;

1.77
date	2009.06.01.17.42.33;	author ariane;	state Exp;
branches;
next	1.76;

1.76
date	2009.04.20.00.30.18;	author oga;	state Exp;
branches;
next	1.75;

1.75
date	2009.04.14.16.01.04;	author oga;	state Exp;
branches;
next	1.74;

1.74
date	2009.03.05.19.52.24;	author kettenis;	state Exp;
branches;
next	1.73;

1.73
date	2009.02.11.11.09.36;	author mikeb;	state Exp;
branches;
next	1.72;

1.72
date	2008.11.04.21.37.06;	author deraadt;	state Exp;
branches;
next	1.71;

1.71
date	2008.10.23.23.54.02;	author tedu;	state Exp;
branches;
next	1.70;

1.70
date	2008.06.09.20.30.23;	author miod;	state Exp;
branches;
next	1.69;

1.69
date	2008.04.09.16.58.11;	author deraadt;	state Exp;
branches;
next	1.68;

1.68
date	2007.11.29.00.26.41;	author tedu;	state Exp;
branches;
next	1.67;

1.67
date	2007.09.10.18.49.45;	author miod;	state Exp;
branches;
next	1.66;

1.66
date	2007.05.27.20.59.26;	author miod;	state Exp;
branches;
next	1.65;

1.65
date	2007.04.11.12.51.51;	author miod;	state Exp;
branches;
next	1.64;

1.64
date	2007.04.11.12.10.42;	author art;	state Exp;
branches;
next	1.63;

1.63
date	2006.11.29.12.39.50;	author miod;	state Exp;
branches;
next	1.62;

1.62
date	2006.11.29.12.26.14;	author miod;	state Exp;
branches;
next	1.61;

1.61
date	2006.11.29.12.24.18;	author miod;	state Exp;
branches;
next	1.60;

1.60
date	2006.11.29.12.17.33;	author miod;	state Exp;
branches;
next	1.59;

1.59
date	2005.11.04.21.48.07;	author miod;	state Exp;
branches;
next	1.58;

1.58
date	2005.09.28.00.24.03;	author pedro;	state Exp;
branches;
next	1.57;

1.57
date	2005.09.12.23.05.06;	author miod;	state Exp;
branches;
next	1.56;

1.56
date	2005.05.24.21.11.47;	author tedu;	state Exp;
branches;
next	1.55;

1.55
date	2005.04.21.04.39.34;	author mickey;	state Exp;
branches;
next	1.54;

1.54
date	2004.12.30.08.28.39;	author niklas;	state Exp;
branches;
next	1.53;

1.53
date	2004.05.27.04.55.28;	author tedu;	state Exp;
branches;
next	1.52;

1.52
date	2004.04.28.02.20.58;	author markus;	state Exp;
branches;
next	1.51;

1.51
date	2004.04.19.22.52.33;	author tedu;	state Exp;
branches;
next	1.50;

1.50
date	2003.08.10.00.04.50;	author miod;	state Exp;
branches;
next	1.49;

1.49
date	2003.06.02.23.28.24;	author millert;	state Exp;
branches;
next	1.48;

1.48
date	2003.05.01.09.29.02;	author jmc;	state Exp;
branches;
next	1.47;

1.47
date	2003.03.28.10.16.29;	author jmc;	state Exp;
branches;
next	1.46;

1.46
date	2002.10.29.18.30.21;	author art;	state Exp;
branches;
next	1.45;

1.45
date	2002.03.14.03.16.13;	author millert;	state Exp;
branches;
next	1.44;

1.44
date	2002.03.14.01.27.18;	author millert;	state Exp;
branches;
next	1.43;

1.43
date	2002.02.28.18.50.26;	author provos;	state Exp;
branches;
next	1.42;

1.42
date	2002.02.25.05.38.50;	author provos;	state Exp;
branches;
next	1.41;

1.41
date	2002.02.25.00.20.45;	author provos;	state Exp;
branches;
next	1.40;

1.40
date	2001.12.19.08.58.07;	author art;	state Exp;
branches;
next	1.39;

1.39
date	2001.12.06.23.01.07;	author niklas;	state Exp;
branches
	1.39.2.1;
next	1.38;

1.38
date	2001.12.04.23.22.42;	author art;	state Exp;
branches;
next	1.37;

1.37
date	2001.11.30.17.24.19;	author art;	state Exp;
branches;
next	1.36;

1.36
date	2001.11.28.19.28.14;	author art;	state Exp;
branches;
next	1.35;

1.35
date	2001.11.28.13.47.39;	author art;	state Exp;
branches;
next	1.34;

1.34
date	2001.11.27.05.27.12;	author art;	state Exp;
branches;
next	1.33;

1.33
date	2001.11.12.01.26.09;	author art;	state Exp;
branches;
next	1.32;

1.32
date	2001.11.10.18.42.31;	author art;	state Exp;
branches;
next	1.31;

1.31
date	2001.11.09.03.32.23;	author art;	state Exp;
branches;
next	1.30;

1.30
date	2001.11.07.02.55.50;	author art;	state Exp;
branches;
next	1.29;

1.29
date	2001.11.07.01.18.01;	author art;	state Exp;
branches;
next	1.28;

1.28
date	2001.11.06.18.41.10;	author art;	state Exp;
branches;
next	1.27;

1.27
date	2001.11.06.13.36.52;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2001.11.06.01.35.04;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2001.11.06.00.24.55;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2001.09.19.20.50.59;	author mickey;	state Exp;
branches;
next	1.23;

1.23
date	2001.08.12.22.41.15;	author mickey;	state Exp;
branches;
next	1.22;

1.22
date	2001.08.12.21.00.14;	author mickey;	state Exp;
branches;
next	1.21;

1.21
date	2001.08.12.20.18.30;	author mickey;	state Exp;
branches;
next	1.20;

1.20
date	2001.08.12.05.18.41;	author mickey;	state Exp;
branches;
next	1.19;

1.19
date	2001.08.11.10.57.22;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2001.08.08.02.36.59;	author millert;	state Exp;
branches;
next	1.17;

1.17
date	2001.08.06.14.03.04;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2001.08.02.11.06.38;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2001.07.26.19.37.13;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2001.07.25.14.47.59;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2001.06.23.19.24.33;	author smart;	state Exp;
branches;
next	1.12;

1.12
date	2001.05.10.07.59.06;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2001.05.07.16.08.40;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2001.03.09.14.20.50;	author art;	state Exp;
branches;
next	1.9;

1.9
date	2001.03.09.05.34.38;	author smart;	state Exp;
branches;
next	1.8;

1.8
date	2001.01.29.02.07.43;	author niklas;	state Exp;
branches;
next	1.7;

1.7
date	2000.12.06.17.19.01;	author deraadt;	state Exp;
branches;
next	1.6;

1.6
date	2000.03.16.22.11.04;	author art;	state Exp;
branches;
next	1.5;

1.5
date	99.08.23.08.13.23;	author art;	state Exp;
branches
	1.5.4.1;
next	1.4;

1.4
date	99.08.17.10.32.19;	author niklas;	state Exp;
branches;
next	1.3;

1.3
date	99.07.23.14.47.06;	author ho;	state Exp;
branches;
next	1.2;

1.2
date	99.02.26.05.32.06;	author art;	state Exp;
branches;
next	1.1;

1.1
date	99.02.26.01.30.12;	author art;	state Exp;
branches;
next	;

1.5.4.1
date	2000.03.24.09.09.48;	author niklas;	state Exp;
branches;
next	1.5.4.2;

1.5.4.2
date	2001.05.14.22.47.45;	author niklas;	state Exp;
branches;
next	1.5.4.3;

1.5.4.3
date	2001.07.04.11.01.01;	author niklas;	state Exp;
branches;
next	1.5.4.4;

1.5.4.4
date	2001.10.31.03.32.14;	author nate;	state Exp;
branches;
next	1.5.4.5;

1.5.4.5
date	2001.11.13.23.02.31;	author niklas;	state Exp;
branches;
next	1.5.4.6;

1.5.4.6
date	2001.12.05.01.23.58;	author niklas;	state Exp;
branches;
next	1.5.4.7;

1.5.4.7
date	2002.03.06.02.17.14;	author niklas;	state Exp;
branches;
next	1.5.4.8;

1.5.4.8
date	2002.03.28.14.54.26;	author niklas;	state Exp;
branches;
next	1.5.4.9;

1.5.4.9
date	2003.03.28.00.08.48;	author niklas;	state Exp;
branches;
next	1.5.4.10;

1.5.4.10
date	2003.05.13.19.36.57;	author ho;	state Exp;
branches;
next	1.5.4.11;

1.5.4.11
date	2003.06.07.11.09.09;	author ho;	state Exp;
branches;
next	1.5.4.12;

1.5.4.12
date	2004.02.19.11.01.43;	author niklas;	state Exp;
branches;
next	1.5.4.13;

1.5.4.13
date	2004.06.05.23.13.12;	author niklas;	state Exp;
branches;
next	;

1.39.2.1
date	2002.02.02.03.28.26;	author art;	state Exp;
branches;
next	1.39.2.2;

1.39.2.2
date	2002.06.11.03.33.03;	author art;	state Exp;
branches;
next	1.39.2.3;

1.39.2.3
date	2002.11.04.18.02.32;	author art;	state Exp;
branches;
next	1.39.2.4;

1.39.2.4
date	2003.05.19.22.41.29;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.141
log
@Generating a coredump requires walking the map twice; change
uvm_coredump_walkmap() to do both with a callback in between
so it can hold locks/change state across the two.

ok stefan@@
@
text
@/*	$OpenBSD: uvm_extern.h,v 1.140 2017/02/12 04:55:08 guenther Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.57 2001/03/09 01:02:12 chs Exp $	*/

/*
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

/*-
 * Copyright (c) 1991, 1992, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)vm_extern.h	8.5 (Berkeley) 5/3/95
 */

#ifndef _UVM_UVM_EXTERN_H_
#define _UVM_UVM_EXTERN_H_

typedef int vm_fault_t;

typedef int vm_inherit_t;	/* XXX: inheritance codes */
typedef off_t voff_t;		/* XXX: offset within a uvm_object */

union vm_map_object;
typedef union vm_map_object vm_map_object_t;

struct vm_map_entry;
typedef struct vm_map_entry *vm_map_entry_t;

struct vm_map;
typedef struct vm_map *vm_map_t;

struct vm_page;
typedef struct vm_page  *vm_page_t;

/*
 * Bit assignments assigned by UVM_MAPFLAG() and extracted by
 * UVM_{PROTECTION,INHERIT,MAXPROTECTION,ADVICE}():
 * bits 0-2	protection
 *  bit 3	 unused
 * bits 4-5	inheritance
 *  bits 6-7	 unused
 * bits 8-10	max protection
 *  bit 11	 unused
 * bits 12-14	advice
 *  bit 15	 unused
 * bits 16-N	flags
 */

/* protections bits */
#define PROT_MASK	(PROT_READ | PROT_WRITE | PROT_EXEC)

/* inherit codes */
#define MAP_INHERIT_MASK	0x3	/* inherit mask */

typedef int		vm_prot_t;

#define MADV_MASK	0x7	/* mask */

/* mapping flags */
#define UVM_FLAG_FIXED   0x0010000 /* find space */
#define UVM_FLAG_OVERLAY 0x0020000 /* establish overlay */
#define UVM_FLAG_NOMERGE 0x0040000 /* don't merge map entries */
#define UVM_FLAG_COPYONW 0x0080000 /* set copy_on_write flag */
#define UVM_FLAG_TRYLOCK 0x0100000 /* fail if we can not lock map */
#define UVM_FLAG_HOLE    0x0200000 /* no backend */
#define UVM_FLAG_QUERY   0x0400000 /* do everything, except actual execution */
#define UVM_FLAG_NOFAULT 0x0800000 /* don't fault */
#define UVM_FLAG_UNMAP   0x1000000 /* unmap to make space */


/* macros to extract info */
#define UVM_PROTECTION(X)	((X) & PROT_MASK)
#define UVM_INHERIT(X)		(((X) >> 4) & MAP_INHERIT_MASK)
#define UVM_MAXPROTECTION(X)	(((X) >> 8) & PROT_MASK)
#define UVM_ADVICE(X)		(((X) >> 12) & MADV_MASK)

#define UVM_MAPFLAG(prot, maxprot, inh, advice, flags) \
	((prot) | ((maxprot) << 8) | ((inh) << 4) | ((advice) << 12) | (flags))

/* magic offset value */
#define UVM_UNKNOWN_OFFSET ((voff_t) -1)
				/* offset not known(obj) or don't care(!obj) */

/*
 * the following defines are for uvm_km_kmemalloc's flags
 */
#define UVM_KMF_NOWAIT	0x1			/* matches M_NOWAIT */
#define UVM_KMF_VALLOC	0x2			/* allocate VA only */
#define UVM_KMF_CANFAIL	0x4			/* caller handles failure */
#define UVM_KMF_ZERO	0x08			/* zero pages */
#define UVM_KMF_TRYLOCK	UVM_FLAG_TRYLOCK	/* try locking only */

/*
 * flags for uvm_pagealloc()
 */
#define UVM_PGA_USERESERVE	0x0001	/* ok to use reserve pages */
#define	UVM_PGA_ZERO		0x0002	/* returned page must be zeroed */

/*
 * flags for uvm_pglistalloc()
 */
#define UVM_PLA_WAITOK		0x0001	/* may sleep */
#define UVM_PLA_NOWAIT		0x0002	/* can't sleep (need one of the two) */
#define UVM_PLA_ZERO		0x0004	/* zero all pages before returning */
#define UVM_PLA_TRYCONTIG	0x0008	/* try to allocate contig physmem */
#define UVM_PLA_FAILOK		0x0010	/* caller can handle failure */

/*
 * lockflags that control the locking behavior of various functions.
 */
#define	UVM_LK_ENTER	0x00000001	/* map locked on entry */
#define	UVM_LK_EXIT	0x00000002	/* leave map locked on exit */

/*
 * flags to uvm_page_physload.
 */
#define	PHYSLOAD_DEVICE	0x01	/* don't add to the page queue */

#include <sys/queue.h>
#include <sys/tree.h>
#include <sys/mman.h>

#ifdef _KERNEL
struct buf;
struct mount;
struct pglist;
struct vmspace;
struct pmap;
#endif

#include <uvm/uvm_param.h>

#include <uvm/uvm_pmap.h>
#include <uvm/uvm_object.h>
#include <uvm/uvm_page.h>
#include <uvm/uvm_map.h>

#ifdef _KERNEL
#include <uvm/uvm_fault.h>
#include <uvm/uvm_pager.h>
#endif

/*
 * Shareable process virtual address space.
 * May eventually be merged with vm_map.
 * Several fields are temporary (text, data stuff).
 */
struct vmspace {
	struct	vm_map vm_map;	/* VM address map */
	int	vm_refcnt;	/* number of references */
	caddr_t	vm_shm;		/* SYS5 shared memory private data XXX */
/* we copy from vm_startcopy to the end of the structure on fork */
#define vm_startcopy vm_rssize
	segsz_t vm_rssize; 	/* current resident set size in pages */
	segsz_t vm_swrss;	/* resident set size before last swap */
	segsz_t vm_tsize;	/* text size (pages) XXX */
	segsz_t vm_dsize;	/* data size (pages) XXX */
	segsz_t vm_dused;	/* data segment length (pages) XXX */
	segsz_t vm_ssize;	/* stack size (pages) */
	caddr_t	vm_taddr;	/* user virtual address of text XXX */
	caddr_t	vm_daddr;	/* user virtual address of data XXX */
	caddr_t vm_maxsaddr;	/* user VA at max stack growth */
	caddr_t vm_minsaddr;	/* user VA at top of stack */
};

/*
 * uvm_constraint_range's:
 * MD code is allowed to setup constraint ranges for memory allocators, the
 * primary use for this is to keep allocation for certain memory consumers
 * such as mbuf pools withing address ranges that are reachable by devices
 * that perform DMA.
 *
 * It is also to discourge memory allocations from being satisfied from ranges
 * such as the ISA memory range, if they can be satisfied with allocation
 * from other ranges.
 *
 * the MD ranges are defined in arch/ARCH/ARCH/machdep.c
 */
struct uvm_constraint_range {
	paddr_t	ucr_low;
	paddr_t ucr_high;
};

#ifdef _KERNEL

#include <uvm/uvmexp.h>
extern struct uvmexp uvmexp;

/* Constraint ranges, set by MD code. */
extern struct uvm_constraint_range  isa_constraint;
extern struct uvm_constraint_range  dma_constraint;
extern struct uvm_constraint_range  no_constraint;
extern struct uvm_constraint_range *uvm_md_constraints[];

extern struct pool *uvm_aiobuf_pool;

/*
 * the various kernel maps, owned by MD code
 */
extern struct vm_map *exec_map;
extern struct vm_map *kernel_map;
extern struct vm_map *kmem_map;
extern struct vm_map *phys_map;

/* base of kernel virtual memory */
extern vaddr_t vm_min_kernel_address;

/* zalloc zeros memory, alloc does not */
#define uvm_km_zalloc(MAP,SIZE) uvm_km_alloc1(MAP,SIZE,0,TRUE)
#define uvm_km_alloc(MAP,SIZE)  uvm_km_alloc1(MAP,SIZE,0,FALSE)

#define vm_resident_count(vm) (pmap_resident_count((vm)->vm_map.pmap))

void			vmapbuf(struct buf *, vsize_t);
void			vunmapbuf(struct buf *, vsize_t);
struct uvm_object	*uao_create(vsize_t, int);
void			uao_detach(struct uvm_object *);
void			uao_detach_locked(struct uvm_object *);
void			uao_reference(struct uvm_object *);
void			uao_reference_locked(struct uvm_object *);
int			uvm_fault(vm_map_t, vaddr_t, vm_fault_t, vm_prot_t);

#if defined(KGDB)
void			uvm_chgkprot(caddr_t, size_t, int);
#endif
vaddr_t			uvm_uarea_alloc(void);
void			uvm_uarea_free(struct proc *);
void			uvm_exit(struct process *);
void			uvm_init_limits(struct proc *);
boolean_t		uvm_kernacc(caddr_t, size_t, int);

int			uvm_vslock(struct proc *, caddr_t, size_t,
			    vm_prot_t);
void			uvm_vsunlock(struct proc *, caddr_t, size_t);
int			uvm_vslock_device(struct proc *, void *, size_t,
			    vm_prot_t, void **);
void			uvm_vsunlock_device(struct proc *, void *, size_t,
			    void *);
void			uvm_pause(void);
void			uvm_init(void);	
int			uvm_io(vm_map_t, struct uio *, int);

#define	UVM_IO_FIXPROT	0x01

vaddr_t			uvm_km_alloc1(vm_map_t, vsize_t, vsize_t, boolean_t);
void			uvm_km_free(vm_map_t, vaddr_t, vsize_t);
void			uvm_km_free_wakeup(vm_map_t, vaddr_t, vsize_t);
vaddr_t			uvm_km_kmemalloc_pla(struct vm_map *,
			    struct uvm_object *, vsize_t, vsize_t, int,
			    paddr_t, paddr_t, paddr_t, paddr_t, int);
#define uvm_km_kmemalloc(map, obj, sz, flags)				\
	uvm_km_kmemalloc_pla(map, obj, sz, 0, flags, 0, (paddr_t)-1, 0, 0, 0)
vaddr_t			uvm_km_valloc(vm_map_t, vsize_t);
vaddr_t			uvm_km_valloc_try(vm_map_t, vsize_t);
vaddr_t			uvm_km_valloc_wait(vm_map_t, vsize_t);
vaddr_t			uvm_km_valloc_align(struct vm_map *, vsize_t,
			    vsize_t, int);
vaddr_t			uvm_km_valloc_prefer_wait(vm_map_t, vsize_t, voff_t);
struct vm_map		*uvm_km_suballoc(vm_map_t, vaddr_t *, vaddr_t *,
			    vsize_t, int, boolean_t, vm_map_t);
/*
 * Allocation mode for virtual space.
 *
 *  kv_map - pointer to the pointer to the map we're allocating from.
 *  kv_align - alignment.
 *  kv_wait - wait for free space in the map if it's full. The default
 *   allocators don't wait since running out of space in kernel_map and
 *   kmem_map is usually fatal. Special maps like exec_map are specifically
 *   limited, so waiting for space in them is necessary.
 *  kv_singlepage - use the single page allocator.
 *  kv_executable - map the physical pages with PROT_EXEC.
 */
struct kmem_va_mode {
	struct vm_map **kv_map;
	vsize_t kv_align;
	char kv_wait;
	char kv_singlepage;
};

/*
 * Allocation mode for physical pages.
 *
 *  kp_constraint - allocation constraint for physical pages.
 *  kp_object - if the pages should be allocated from an object.
 *  kp_align - physical alignment of the first page in the allocation.
 *  kp_boundary - boundary that the physical addresses can't cross if
 *   the allocation is contiguous.
 *  kp_nomem - don't allocate any backing pages.
 *  kp_maxseg - maximal amount of contiguous segments.
 *  kp_zero - zero the returned memory.
 *  kp_pageable - allocate pageable memory.
 */
struct kmem_pa_mode {
	struct uvm_constraint_range *kp_constraint;
	struct uvm_object **kp_object;
	paddr_t kp_align;
	paddr_t kp_boundary;
	int kp_maxseg;
	char kp_nomem;
	char kp_zero;
	char kp_pageable;
};

/*
 * Dynamic allocation parameters. Stuff that changes too often or too much
 * to create separate va and pa modes for.
 *
 * kd_waitok - is it ok to sleep?
 * kd_trylock - don't sleep on map locks.
 * kd_prefer - offset to feed to PMAP_PREFER
 * kd_slowdown - special parameter for the singlepage va allocator
 *  that tells the caller to sleep if possible to let the singlepage
 *  allocator catch up.
 */
struct kmem_dyn_mode {
	voff_t kd_prefer;
	int *kd_slowdown;
	char kd_waitok;
	char kd_trylock;
};

#define KMEM_DYN_INITIALIZER { UVM_UNKNOWN_OFFSET, NULL, 0, 0 }

/*
 * Notice that for kv_ waiting has a different meaning. It's only supposed
 * to be used for very space constrained maps where waiting is a way
 * to throttle some other operation.
 * The exception is kv_page which needs to wait relatively often.
 * All kv_ except kv_intrsafe will potentially sleep.
 */
extern const struct kmem_va_mode kv_any;
extern const struct kmem_va_mode kv_intrsafe;
extern const struct kmem_va_mode kv_page;

extern const struct kmem_pa_mode kp_dirty;
extern const struct kmem_pa_mode kp_zero;
extern const struct kmem_pa_mode kp_dma;
extern const struct kmem_pa_mode kp_dma_contig;
extern const struct kmem_pa_mode kp_dma_zero;
extern const struct kmem_pa_mode kp_pageable;
extern const struct kmem_pa_mode kp_none;

extern const struct kmem_dyn_mode kd_waitok;
extern const struct kmem_dyn_mode kd_nowait;
extern const struct kmem_dyn_mode kd_trylock;

void			*km_alloc(size_t, const struct kmem_va_mode *,
			    const struct kmem_pa_mode *,
			    const struct kmem_dyn_mode *);
void			km_free(void *, size_t, const struct kmem_va_mode *,
			    const struct kmem_pa_mode *);
int			uvm_map(vm_map_t, vaddr_t *, vsize_t,
			    struct uvm_object *, voff_t, vsize_t, unsigned int);
int			uvm_mapanon(vm_map_t, vaddr_t *, vsize_t, vsize_t, unsigned int);
int			uvm_map_pageable(vm_map_t, vaddr_t, 
			    vaddr_t, boolean_t, int);
int			uvm_map_pageable_all(vm_map_t, int, vsize_t);
boolean_t		uvm_map_checkprot(vm_map_t, vaddr_t,
			    vaddr_t, vm_prot_t);
int			uvm_map_protect(vm_map_t, vaddr_t, 
			    vaddr_t, vm_prot_t, boolean_t);
struct vmspace		*uvmspace_alloc(vaddr_t, vaddr_t,
			    boolean_t, boolean_t);
void			uvmspace_init(struct vmspace *, struct pmap *,
			    vaddr_t, vaddr_t, boolean_t, boolean_t);
void			uvmspace_exec(struct proc *, vaddr_t, vaddr_t);
struct vmspace		*uvmspace_fork(struct process *);
void			uvmspace_free(struct vmspace *);
struct vmspace		*uvmspace_share(struct process *);
int			uvm_share(vm_map_t, vaddr_t, vm_prot_t,
			    vm_map_t, vaddr_t, vsize_t);
void			uvm_meter(void);
int			uvm_sysctl(int *, u_int, void *, size_t *, 
			    void *, size_t, struct proc *);
struct vm_page		*uvm_pagealloc(struct uvm_object *,
			    voff_t, struct vm_anon *, int);
vaddr_t			uvm_pagealloc_contig(vaddr_t, vaddr_t,
			    vaddr_t, vaddr_t);
int			uvm_pagealloc_multi(struct uvm_object *, voff_t,
    			    vsize_t, int);
void			uvm_pagerealloc(struct vm_page *, 
			    struct uvm_object *, voff_t);
int			uvm_pagerealloc_multi(struct uvm_object *, voff_t,
			    vsize_t, int, struct uvm_constraint_range *);
/* Actually, uvm_page_physload takes PF#s which need their own type */
void			uvm_page_physload(paddr_t, paddr_t, paddr_t,
			    paddr_t, int);
void			uvm_setpagesize(void);
void			uvm_shutdown(void);
void			uvm_aio_biodone(struct buf *);
void			uvm_aio_aiodone(struct buf *);
void			uvm_pageout(void *);
void			uvm_aiodone_daemon(void *);
void			uvm_wait(const char *);
int			uvm_pglistalloc(psize_t, paddr_t, paddr_t,
			    paddr_t, paddr_t, struct pglist *, int, int);
void			uvm_pglistfree(struct pglist *);
void			uvm_pmr_use_inc(paddr_t, paddr_t);
void			uvm_swap_init(void);
typedef int		uvm_coredump_setup_cb(int _nsegment, void *_cookie);
typedef int		uvm_coredump_walk_cb(vaddr_t _start, vaddr_t _realend,
			    vaddr_t _end, vm_prot_t _prot, int _nsegment,
			    void *_cookie);
int			uvm_coredump_walkmap(struct proc *_p,
			    uvm_coredump_setup_cb *_setup,
			    uvm_coredump_walk_cb *_walk, void *_cookie);
void			uvm_grow(struct proc *, vaddr_t);
void			uvm_deallocate(vm_map_t, vaddr_t, vsize_t);
struct uvm_object	*uvn_attach(struct vnode *, vm_prot_t);
void			uvm_pagezero_thread(void *);
void			kmeminit_nkmempages(void);
void			kmeminit(void);
extern u_int		nkmempages;

struct process;
struct kinfo_vmentry;
int			fill_vmmap(struct process *, struct kinfo_vmentry *,
			    size_t *);

#endif /* _KERNEL */

#endif /* _UVM_UVM_EXTERN_H_ */
@


1.140
log
@Split up fork1():
 - FORK_THREAD handling is a totally separate function, thread_fork(),
   that is only used by sys___tfork() and which loses the flags, func,
   arg, and newprocp parameters and gains tcb parameter to guarantee
   the new thread's TCB is set before the creating thread returns
 - fork1() loses its stack and tidptr parameters
Common bits factor out:
 - struct proc allocation and initialization moves to thread_new()
 - maxthread handling moves to fork_check_maxthread()
 - setting the new thread running moves to fork_thread_start()
The MD cpu_fork() function swaps its unused stacksize parameter for
a tcb parameter.

luna88k testing by aoyama@@, alpha testing by dlg@@
ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.139 2016/06/05 08:35:57 stefan Exp $	*/
a242 14
 * used to keep state while iterating over the map for a core dump.
 */
struct uvm_coredump_state {
	void *cookie;		/* opaque for the caller */
	vaddr_t start;		/* start of region */
	vaddr_t realend;	/* real end of region */
	vaddr_t end;		/* virtual end of region */
	vm_prot_t prot;		/* protection of region */
	int flags;		/* flags; see below */
};

#define	UVM_COREDUMP_STACK	0x01	/* region is user stack */

/*
d445 7
a451 3
int			uvm_coredump_walkmap(struct proc *,
			    void *, int (*)(struct proc *, void *,
			    struct uvm_coredump_state *), void *);
@


1.139
log
@Add uvm_share() to share a memory range between two address spaces

Its primary use is to make guest VM memory accessible to the host
(e.g. vmd(8)). That will later allow us to remove the readpage and
writepage ioctls from vmm(4), and use ordinary loads and stores instead.

"looks good to me" kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.138 2016/04/04 16:34:16 stefan Exp $	*/
a274 2
void			cpu_fork(struct proc *, struct proc *, void *,
			    size_t, void (*)(void *), void *);
@


1.138
log
@UVM_FLAG_AMAPPAD has no effect anymore, nuke it.

This flag caused amaps to be allocated with additional spare slots, to
make extending them cheaper. However, the kernel never extends amaps,
so allocating spare slots is pointless. Also UVM_FLAG_AMAPPAD only
has an effect in combination with UVM_FLAG_OVERLAY. The only function
that used both flags was sys_obreak, but that function had the use of
UVM_FLAG_OVERLAY removed recently.

While there, kill the unused prototypes amap_flags and amap_refs.
They're defined as macros already.

ok mlarkin@@ kettenis@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.137 2015/12/02 09:50:46 blambert Exp $	*/
d431 2
@


1.137
log
@remove declaration for nonexistant function

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.136 2015/11/11 15:59:33 mmcc Exp $	*/
d109 5
a113 6
#define UVM_FLAG_AMAPPAD 0x0100000 /* for bss: pad amap to reduce malloc() */
#define UVM_FLAG_TRYLOCK 0x0200000 /* fail if we can not lock map */
#define UVM_FLAG_HOLE    0x0400000 /* no backend */
#define UVM_FLAG_QUERY   0x0800000 /* do everything, except actual execution */
#define UVM_FLAG_NOFAULT 0x1000000 /* don't fault */
#define UVM_FLAG_UNMAP   0x2000000 /* unmap to make space */
@


1.136
log
@Remove the superfluous typedef uvm_flag_t (unsigned int). Also, fix an
associated mistake in the uvm manpage.

Suggested by and ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.135 2015/09/28 18:33:42 tedu Exp $	*/
a449 1
void			uvm_aio_biodone1(struct buf *);
@


1.135
log
@add a flag to indicate to uvm_map that it should unmap to make space.
this pulls all the relevant operations under the same map locking, and
relieves calling code from responsibility.
ok kettenis matthew
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.134 2015/09/09 14:52:12 miod Exp $	*/
a62 1
typedef unsigned int  uvm_flag_t;
d415 2
a416 2
			    struct uvm_object *, voff_t, vsize_t, uvm_flag_t);
int			uvm_mapanon(vm_map_t, vaddr_t *, vsize_t, vsize_t, uvm_flag_t);
@


1.134
log
@All our pmap implementations provide pmap_resident_count(), so remove
#ifndef pmap_resident_count code paths.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.133 2015/07/19 21:21:14 beck Exp $	*/
d115 2
@


1.133
log
@Change uvm_page[re]alloc_multi to actually use the flags passed in, and return
a value so that they may be called with UVM_PLA_NOWAIT
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.132 2015/05/05 02:13:46 guenther Exp $	*/
a270 1
#ifdef	pmap_resident_count
a271 3
#else
#define vm_resident_count(vm) ((vm)->vm_rssize)
#endif
@


1.132
log
@emul_native is only used for kernel threads which can't dump core, so
delete coredump_trad(), uvm_coredump(), cpu_coredump(), struct md_coredump,
and various #includes that are superfluous.

This leaves compat_linux processes without a coredump callback.  If that
ability is desired, someone should update it to use coredump_elf32() and
verify the results...

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.131 2015/02/07 08:21:24 miod Exp $	*/
d442 1
a442 1
void			uvm_pagealloc_multi(struct uvm_object *, voff_t,
d446 1
a446 1
void			uvm_pagerealloc_multi(struct uvm_object *, voff_t,
@


1.131
log
@Introduce VM_KERNEL_SPACE_SIZE as a replacement for
(VM_MAX_KERNEL_ADDRESS - VM_MIN_KERNEL_ADDRESS). This will allow these to no
longer be constants in the future.

ok guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.130 2015/02/06 11:41:55 beck Exp $	*/
a169 1
struct core;
a463 2
int			uvm_coredump(struct proc *, struct vnode *, 
			    struct ucred *, struct core *);
@


1.130
log
@-Split out uvm_mmap and uvm_map into a version for anon's and a version
for everything else.
-Adapt the anon version to be callable without the biglock held.
Done by tedu@@, kettenis@@ and me.. pounded on a bunch.

This does not yet make mmap a NOLOCK call, but permits it to be so.
ok tedu@@, kettenis@@, guenther@@ jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.129 2015/02/05 23:51:06 mpi Exp $	*/
d265 2
@


1.129
log
@Remove some unneeded <uvm/uvm_extern.h> inclusions.

ok deraadt@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.128 2014/12/17 19:42:15 tedu Exp $	*/
d418 1
a436 3
int			uvm_mmap(vm_map_t, vaddr_t *, vsize_t,
			    vm_prot_t, vm_prot_t, int, 
			    caddr_t, voff_t, vsize_t, struct proc *);
@


1.128
log
@remove lock.h from uvm_extern.h. another holdover from the simpletonlock
era. fix uvm including c files to include lock.h or atomic.h as necessary.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.127 2014/12/16 18:30:04 tedu Exp $	*/
a26 2
 *
 * from: Id: uvm_extern.h,v 1.1.2.21 1998/02/07 01:16:53 chs Exp
@


1.127
log
@primary change: move uvm_vnode out of vnode, keeping only a pointer.
objective: vnode.h doesn't include uvm_extern.h anymore.
followup changes: include uvm_extern.h or lock.h where necessary.
ok and help from deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.126 2014/12/15 02:24:23 guenther Exp $	*/
a167 1
#include <sys/lock.h>
@


1.126
log
@Use MAP_INHERIT_* for the 'inh' argument to the UMV_MAPFLAG() macro,
eliminating the must-be-kept-in-sync UVM_INH_* macros

ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.125 2014/12/05 04:12:48 uebayasi Exp $	*/
a474 4
void			uvm_vnp_setsize(struct vnode *, voff_t);
void			uvm_vnp_sync(struct mount *);
void 			uvm_vnp_terminate(struct vnode *);
boolean_t		uvm_vnp_uncache(struct vnode *);
@


1.125
log
@Introduce a new sysctl to retrieve VM map entries

This adds a new sysctl KERN_PROC_VMMAP, which returns an array of VM map
entries of a specified process.  This prevents debuggers from iterating
vm_map_entry RB tree via kvm(3).

The name KERN_PROC_VMMAP and struct kinfo_vmentry are chosen from the same
function in FreeBSD.  struct kinfo_vmentry is revised to reduce size, because
OpenBSD does not keep track of filepaths.  The semantic is also changed to
return max buffer size as a hint, and start iteration at the specified base
address.

Much valuable input from deraadt@@, guenther@@, tedu@@

OK tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d83 14
a98 1
/* 0x08: not used */
d101 1
a101 9
#define UVM_INH_MASK	0x30	/* inherit mask */
#define UVM_INH_SHARE	0x00	/* "share" */
#define UVM_INH_COPY	0x10	/* "copy" */
#define UVM_INH_NONE	0x20	/* "none" */
#define UVM_INH_ZERO	0x30	/* "zero" */

/* 0x40, 0x80: not used */
/* bits 0x700: max protection, 0x800: not used */
/* bits 0x7000: advice, 0x8000: not used */
d105 1
a105 1
#define UVM_ADV_MASK	0x7	/* mask */
d120 1
a120 1
#define UVM_INHERIT(X)		(((X) & UVM_INH_MASK) >> 4)
d122 1
a122 1
#define UVM_ADVICE(X)		(((X) >> 12) & UVM_ADV_MASK)
d125 1
a125 1
	(((maxprot) << 8) | (prot) | (inh) | ((advice) << 12) | (flags))
@


1.124
log
@Kill kv_executable flag.  We no longer allow requests for PROT_EXEC
mappings via this interface (nothing uses it, in any case)
ok uebayasi tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.123 2014/11/16 12:31:00 deraadt Exp $	*/
d479 5
@


1.123
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.122 2014/11/15 21:42:07 deraadt Exp $	*/
a339 1
	char kv_executable;
@


1.122
log
@repair operator precedence in UVM_MAPFLAG() macro; ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.121 2014/10/03 18:06:47 kettenis Exp $	*/
d84 1
a84 16
#define UVM_PROT_MASK	0x07	/* protection mask */
#define UVM_PROT_NONE	0x00	/* protection none */
#define UVM_PROT_ALL	0x07	/* everything */
#define UVM_PROT_READ	0x01	/* read */
#define UVM_PROT_WRITE  0x02	/* write */
#define UVM_PROT_EXEC	0x04	/* exec */

/* protection short codes */
#define UVM_PROT_R	0x01	/* read */
#define UVM_PROT_W	0x02	/* write */
#define UVM_PROT_RW	0x03    /* read-write */
#define UVM_PROT_X	0x04	/* exec */
#define UVM_PROT_RX	0x05	/* read-exec */
#define UVM_PROT_WX	0x06	/* write-exec */
#define UVM_PROT_RWX	0x07	/* read-write-exec */

a94 1

a95 1

a99 31
/*
 *	Protection values, defined as bits within the vm_prot_t type
 *
 *   These are funky definitions from old CMU VM and are kept
 *   for compatibility reasons, one day they are going to die,
 *   just like everybody else.
 */

#define	VM_PROT_NONE	((vm_prot_t) 0x00)

#define VM_PROT_READ	((vm_prot_t) 0x01)	/* read permission */
#define VM_PROT_WRITE	((vm_prot_t) 0x02)	/* write permission */
#define VM_PROT_EXECUTE	((vm_prot_t) 0x04)	/* execute permission */

/*
 *	The default protection for newly-created virtual memory
 */

#define VM_PROT_DEFAULT	(VM_PROT_READ|VM_PROT_WRITE|VM_PROT_EXECUTE)

/*
 *	The maximum privileges possible, for parameter checking.
 */

#define VM_PROT_ALL	(VM_PROT_READ|VM_PROT_WRITE|VM_PROT_EXECUTE)

/* advice: matches MADV_* from sys/mman.h */
#define UVM_ADV_NORMAL	0x0	/* 'normal' */
#define UVM_ADV_RANDOM	0x1	/* 'random' */
#define UVM_ADV_SEQUENTIAL 0x2	/* 'sequential' */
/* 0x3: will need, 0x4: dontneed */
d114 1
a114 1
#define UVM_PROTECTION(X)	((X) & UVM_PROT_MASK)
d116 1
a116 1
#define UVM_MAXPROTECTION(X)	(((X) >> 8) & UVM_PROT_MASK)
d164 1
@


1.121
log
@Introduce a thread for zeroing pages without holding the kernel lock.  This
way we can do some useful kernel lock in parallel with other things and create
a reservoir of zeroed pages ready for use elsewhere.  This should reduce
latency.  The thread runs at the absolutel lowest priority such that we don't
keep other kernel threads or userland from doing useful work.

Can be easily disabled by disabling the kthread_create(9) call in main().
Which perhaps we should do for non-MP kernels.

ok deraadt@@, tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.120 2014/10/03 17:41:00 kettenis Exp $	*/
d167 2
a168 2
#define UVM_MAPFLAG(PROT,MAXPROT,INH,ADVICE,FLAGS) \
	((MAXPROT << 8)|(PROT)|(INH)|((ADVICE) << 12)|(FLAGS))
@


1.120
log
@Introduce __MAP_NOFAULT, a mmap(2) flag that makes sure a mapping will not
cause a SIGSEGV or SIGBUS when a mapped file gets truncated.  Access to
pages that are not backed by a file on such a mapping will be replaced by
zero-filled anonymous pages.  Makes passing file descriptors of mapped files
usable without having to play tricks with signal handlers.

"steal your mmap flag" deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.119 2014/07/11 16:35:40 jsg Exp $	*/
d523 1
@


1.119
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.118 2014/07/08 17:19:26 deraadt Exp $	*/
d151 9
a159 8
#define UVM_FLAG_FIXED   0x010000 /* find space */
#define UVM_FLAG_OVERLAY 0x020000 /* establish overlay */
#define UVM_FLAG_NOMERGE 0x040000 /* don't merge map entries */
#define UVM_FLAG_COPYONW 0x080000 /* set copy_on_write flag */
#define UVM_FLAG_AMAPPAD 0x100000 /* for bss: pad amap to reduce malloc() */
#define UVM_FLAG_TRYLOCK 0x200000 /* fail if we can not lock map */
#define	UVM_FLAG_HOLE    0x400000 /* no backend */
#define UVM_FLAG_QUERY   0x800000 /* do everything, except actual execution */
@


1.118
log
@decouple struct uvmexp into a new file, so that uvm_extern.h and sysctl.h
don't need to be married.
ok guenther miod beck jsing kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.117 2014/07/02 06:09:49 matthew Exp $	*/
a4 1
 *
a15 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed by Charles D. Cranor and
 *      Washington University.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
@


1.117
log
@Use real parameter types for u{dv,vn}_attach() instead of void *

ok guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.116 2014/06/13 01:48:52 matthew Exp $	*/
a71 15
/*
 * uvm_extern.h: this file defines the external interface to the VM system.
 *
 * this should be the only file included by non-VM parts of the kernel
 * which need access to VM services.   if you want to know the interface
 * to the MI VM layer without knowing the details, this is the file to
 * learn.
 *
 * NOTE: vm system calls are prototyped in syscallargs.h
 */

/*
 * typedefs, necessary for standard UVM headers.
 */

a89 8
/*
 * defines
 */

/*
 * the following defines are for uvm_map and functions which call it.
 */

d211 1
a211 1
 * flags to uvm_physload.
d215 3
a217 3
/*
 * structures
 */
d219 1
a223 7
struct proc;
struct ucred;
struct uio;
struct uvm_object;
struct vm_anon;
struct vm_aref;
struct vm_map;
a225 137
struct vnode;
struct pool;

/*
 * uvm_constraint_range's:
 * MD code is allowed to setup constraint ranges for memory allocators, the
 * primary use for this is to keep allocation for certain memory consumers
 * such as mbuf pools withing address ranges that are reachable by devices
 * that perform DMA.
 *
 * It is also to discourge memory allocations from being satisfied from ranges
 * such as the ISA memory range, if they can be satisfied with allocation
 * from other ranges.
 *
 * the MD ranges are defined in arch/ARCH/ARCH/machdep.c
 */
struct uvm_constraint_range {
	paddr_t	ucr_low;
	paddr_t ucr_high;
};

extern struct pool *uvm_aiobuf_pool;

/*
 * uvmexp: global data structures that are exported to parts of the kernel
 * other than the vm system.
 */

struct uvmexp {
	/* vm_page constants */
	int pagesize;   /* size of a page (PAGE_SIZE): must be power of 2 */
	int pagemask;   /* page mask */
	int pageshift;  /* page shift */

	/* vm_page counters */
	int npages;     /* number of pages we manage */
	int free;       /* number of free pages */
	int active;     /* number of active pages */
	int inactive;   /* number of pages that we free'd but may want back */
	int paging;	/* number of pages in the process of being paged out */
	int wired;      /* number of wired pages */

	int zeropages;		/* number of zero'd pages */
	int reserve_pagedaemon; /* number of pages reserved for pagedaemon */
	int reserve_kernel;	/* number of pages reserved for kernel */
	int anonpages;		/* number of pages used by anon pagers */
	int vnodepages;		/* number of pages used by vnode page cache */
	int vtextpages;		/* number of pages used by vtext vnodes */

	/* pageout params */
	int freemin;    /* min number of free pages */
	int freetarg;   /* target number of free pages */
	int inactarg;   /* target number of inactive pages */
	int wiredmax;   /* max number of wired pages */
	int anonmin;	/* min threshold for anon pages */
	int vtextmin;	/* min threshold for vtext pages */
	int vnodemin;	/* min threshold for vnode pages */
	int anonminpct;	/* min percent anon pages */
	int vtextminpct;/* min percent vtext pages */
	int vnodeminpct;/* min percent vnode pages */

	/* swap */
	int nswapdev;	/* number of configured swap devices in system */
	int swpages;	/* number of PAGE_SIZE'ed swap pages */
	int swpginuse;	/* number of swap pages in use */
	int swpgonly;	/* number of swap pages in use, not also in RAM */
	int nswget;	/* number of times fault calls uvm_swap_get() */
	int nanon;	/* number total of anon's in system */
	int nanonneeded;/* number of anons currently needed */
	int nfreeanon;	/* number of free anon's */

	/* stat counters */
	int faults;		/* page fault count */
	int traps;		/* trap count */
	int intrs;		/* interrupt count */
	int swtch;		/* context switch count */
	int softs;		/* software interrupt count */
	int syscalls;		/* system calls */
	int pageins;		/* pagein operation count */
				/* pageouts are in pdpageouts below */
	int obsolete_swapins;	/* swapins */
	int obsolete_swapouts;	/* swapouts */
	int pgswapin;		/* pages swapped in */
	int pgswapout;		/* pages swapped out */
	int forks;  		/* forks */
	int forks_ppwait;	/* forks where parent waits */
	int forks_sharevm;	/* forks where vmspace is shared */
	int pga_zerohit;	/* pagealloc where zero wanted and zero
				   was available */
	int pga_zeromiss;	/* pagealloc where zero wanted and zero
				   not available */
	int zeroaborts;		/* number of times page zeroing was
				   aborted */

	/* fault subcounters */
	int fltnoram;	/* number of times fault was out of ram */
	int fltnoanon;	/* number of times fault was out of anons */
	int fltpgwait;	/* number of times fault had to wait on a page */
	int fltpgrele;	/* number of times fault found a released page */
	int fltrelck;	/* number of times fault relock called */
	int fltrelckok;	/* number of times fault relock is a success */
	int fltanget;	/* number of times fault gets anon page */
	int fltanretry;	/* number of times fault retrys an anon get */
	int fltamcopy;	/* number of times fault clears "needs copy" */
	int fltnamap;	/* number of times fault maps a neighbor anon page */
	int fltnomap;	/* number of times fault maps a neighbor obj page */
	int fltlget;	/* number of times fault does a locked pgo_get */
	int fltget;	/* number of times fault does an unlocked get */
	int flt_anon;	/* number of times fault anon (case 1a) */
	int flt_acow;	/* number of times fault anon cow (case 1b) */
	int flt_obj;	/* number of times fault is on object page (2a) */
	int flt_prcopy;	/* number of times fault promotes with copy (2b) */
	int flt_przero;	/* number of times fault promotes with zerofill (2b) */

	/* daemon counters */
	int pdwoke;	/* number of times daemon woke up */
	int pdrevs;	/* number of times daemon rev'd clock hand */
	int pdswout;	/* number of times daemon called for swapout */
	int pdfreed;	/* number of pages daemon freed since boot */
	int pdscans;	/* number of pages daemon scanned since boot */
	int pdanscan;	/* number of anonymous pages scanned by daemon */
	int pdobscan;	/* number of object pages scanned by daemon */
	int pdreact;	/* number of pages daemon reactivated since boot */
	int pdbusy;	/* number of times daemon found a busy page */
	int pdpageouts;	/* number of times daemon started a pageout */
	int pdpending;	/* number of times daemon got a pending pagout */
	int pddeact;	/* number of pages daemon deactivates */
	int pdreanon;	/* anon pages reactivated due to min threshold */
	int pdrevnode;	/* vnode pages reactivated due to min threshold */
	int pdrevtext;	/* vtext pages reactivated due to min threshold */

	int fpswtch;	/* FPU context switches */
	int kmapent;	/* number of kernel map entries */
};

#ifdef _KERNEL
extern struct uvmexp uvmexp;
d228 1
a228 8
/*
 * Finally, bring in standard UVM headers.
 */
#include <sys/vmmeter.h>
#include <sys/queue.h>
#include <sys/tree.h>
#include <sys/lock.h>
#include <sys/mutex.h>
a229 1
#include <uvm/uvm_param.h>
d234 2
d238 1
a238 6

/* Constraint ranges, set by MD code. */
extern struct uvm_constraint_range  isa_constraint;
extern struct uvm_constraint_range  dma_constraint;
extern struct uvm_constraint_range  no_constraint;
extern struct uvm_constraint_range *uvm_md_constraints[];
d263 18
d283 11
a316 4
/*
 * macros
 */

a320 2
#endif /* _KERNEL */

d327 4
a330 19
/* XXX clean up later */
struct buf;
struct loadavg;
struct proc;
struct pmap;
struct vmspace;
struct mount;
struct vnode;
struct core;

#ifdef _KERNEL

/* vm_machdep.c */
void		vmapbuf(struct buf *, vsize_t);
void		vunmapbuf(struct buf *, vsize_t);
void		cpu_fork(struct proc *, struct proc *, void *, size_t,
		    void (*)(void *), void *);

/* uvm_aobj.c */
d336 1
a337 6
/* uvm_fault.c */
int			uvm_fault(vm_map_t, vaddr_t, 
				vm_fault_t, vm_prot_t);
				/* handle a page fault */

/* uvm_glue.c */
a349 1

a354 3


/* uvm_init.c */
a355 3
				/* init the uvm system */

/* uvm_io.c */
a359 1
/* uvm_km.c */
d371 5
a375 8
vaddr_t			uvm_km_valloc_align(struct vm_map *, vsize_t, vsize_t, int);
vaddr_t			uvm_km_valloc_prefer_wait(vm_map_t, vsize_t,
					voff_t);

struct vm_map		*uvm_km_suballoc(vm_map_t, vaddr_t *,
				vaddr_t *, vsize_t, int,
				boolean_t, vm_map_t);

d463 5
a467 7

void *km_alloc(size_t, const struct kmem_va_mode *, const struct kmem_pa_mode *,
    const struct kmem_dyn_mode *);
void km_free(void *, size_t, const struct kmem_va_mode *,
    const struct kmem_pa_mode *);

/* uvm_map.c */
d469 1
a469 2
				struct uvm_object *, voff_t, vsize_t,
				uvm_flag_t);
d471 1
a471 1
				vaddr_t, boolean_t, int);
d474 1
a474 1
				vaddr_t, vm_prot_t);
d476 1
a476 1
				vaddr_t, vm_prot_t, boolean_t);
d478 1
a478 1
				boolean_t, boolean_t);
d480 1
a480 1
				vaddr_t, vaddr_t, boolean_t, boolean_t);
a484 3


/* uvm_meter.c */
d487 1
a487 3
				void *, size_t, struct proc *);

/* uvm_mmap.c */
d489 2
a490 4
				vm_prot_t, vm_prot_t, int, 
				caddr_t, voff_t, vsize_t, struct proc *);

/* uvm_page.c */
d492 1
a492 1
				voff_t, struct vm_anon *, int);
d494 1
a494 1
				vaddr_t, vaddr_t);
d498 1
a498 1
					     struct uvm_object *, voff_t);
a505 2

/* uvm_pager.c */
a508 2

/* uvm_pdaemon.c */
d512 2
a513 5

/* uvm_pglist.c */
int			uvm_pglistalloc(psize_t, paddr_t,
				paddr_t, paddr_t, paddr_t,
				struct pglist *, int, int); 
a514 2

/* uvm_pmemrange.c */
a515 2

/* uvm_swap.c */
a516 2

/* uvm_unix.c */
d518 1
a518 1
				struct ucred *, struct core *);
d520 2
a521 3
			    void *,
			    int (*)(struct proc *, void *,
				    struct uvm_coredump_state *), void *);
a522 2

/* uvm_user.c */
a523 2

/* uvm_vnode.c */
a526 1
				/* terminate a uvm/uvn object */
a528 2

/* kern_malloc.c */
@


1.116
log
@Add support for MAP_INHERIT_ZERO.

This provides a way for a process to designate pages in its address
space that should be replaced by fresh, zero-initialized anonymous
memory in forked child processes, rather than being copied or shared.

ok jmc, kettenis, tedu, deraadt; positive feedback from many more
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.115 2014/05/15 03:52:25 guenther Exp $	*/
d742 1
a742 1
struct uvm_object	*uvn_attach(void *, vm_prot_t);
@


1.115
log
@Move from struct proc to process the reference-count-holding pointers
to the process's vmspace and filedescs.  struct proc continues to
keep copies of the pointers, copying them on fork, clearing them
on exit, and (for vmspace) refreshing on exec.
Also, make uvm_swapout_threads() thread aware, eliminating p_swtime
in kernel.

particular testing by ajacoutot@@ and sebastia@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.114 2014/05/06 11:50:14 mpi Exp $	*/
d137 1
a137 1
#define UVM_INH_DONATE	0x30	/* "donate" << not used */
@


1.114
log
@Include <sys/vmmeter.h> directly instead of relying on it being
pulled by <uvm/uvm_extern.h> and turn uvm_total() into a private
function.

The preferred way to get memory stats is through the VM_UVMEXP
sysctl(3) since VM_METER is just a wrapper on top of it.  In the
kernel, use `uvmexp' directly instead of uvm_total().

This change does not remove <sys/vmmeter.h> from <uvm/uvm_extern.h>
to give some more time to port maintainers to fix their ports.

ok guenther@@ as part of a larger diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.113 2014/05/03 22:44:36 guenther Exp $	*/
d516 2
a517 3
void			uvm_fork(struct proc *, struct proc *, boolean_t,
			    void *, size_t, void (*)(void *), void *);
void			uvm_exit(struct proc *);
d670 1
a670 1
struct vmspace		*uvmspace_fork(struct vmspace *);
d672 1
a672 1
void			uvmspace_share(struct proc *, struct proc *);
@


1.113
log
@Move the u-area allocation and pmap-magic logic to its own function
uvm_uarea_alloc()

function name from NetBSD; arm testing by miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.112 2014/04/03 21:40:10 tedu Exp $	*/
a486 1
struct vmtotal;
a679 1
void			uvm_total(struct vmtotal *);
@


1.112
log
@add a uvm_yield function and use it in the reaper path to prevent the
reaper from hogging the cpu. it will do the kernel lock twiddle trick to
allow other CPUs a chance to run, and also checks if the reaper has been
running for an entire timeslice and should be preempted.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.111 2014/03/28 17:57:11 mpi Exp $	*/
d516 1
@


1.111
log
@Reduce uvm include madness.  Use <uvm/uvm_extern.h> instead of
<uvm/uvm.h> if possible and remove double inclusions.

ok beck@@, mlarkin@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.110 2014/01/30 18:16:41 miod Exp $	*/
d530 1
@


1.110
log
@Move declaration of struct vm_page_md from <machine/vmparam.h> to
<machine/pmap.h> where it belongs, and compensate in <uvm/uvm_extern.h>
by including <uvm/uvm_pmap.h> before <uvm/uvm_page.h>. Tested on all
MACHINE_ARCH but amd64 and i386 (and hppa64).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.109 2013/07/09 15:37:43 beck Exp $	*/
a399 1
#include <uvm/uvm_param.h>
d402 2
d405 1
d410 6
@


1.109
log
@back out the cache flipper temporarily to work out of tree.
will come back soon.
ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.107 2013/05/23 01:42:59 tedu Exp $	*/
d403 1
a404 1
#include <uvm/uvm_pmap.h>
@


1.108
log
@High memory page flipping for the buffer cache.

This change splits the buffer cache free lists into lists of dma reachable
buffers and high memory buffers based on the ranges returned by pmemrange.
Buffers move from dma to high memory as they age, but are flipped to dma
reachable memory if IO is needed to/from and high mem buffer. The total
amount of buffers  allocated is now bufcachepercent of both the dma and
the high memory region.

This change allows the use of large buffer caches on amd64 using more than
4 GB of memory

ok tedu@@ krw@@ - testing by many.
@
text
@d683 1
a683 1
int			uvm_pagealloc_multi(struct uvm_object *, voff_t,
d687 1
a687 1
int			uvm_pagerealloc_multi(struct uvm_object *, voff_t,
@


1.107
log
@the simplelock is a lie
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.106 2013/05/14 20:33:01 miod Exp $	*/
d683 1
a683 1
void			uvm_pagealloc_multi(struct uvm_object *, voff_t,
d687 1
a687 1
void			uvm_pagerealloc_multi(struct uvm_object *, voff_t,
@


1.106
log
@restore ABI compatibility; guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.105 2013/05/14 20:15:25 miod Exp $	*/
a256 1
struct simplelock;
@


1.105
log
@Remove `swapin' and `swapout' from uvm statistics, since we don't swap out
u areas since quite a few years now.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.104 2012/03/09 13:01:29 ariane Exp $	*/
d336 2
@


1.104
log
@New vmmap implementation.

no oks (it is really a pain to review properly)
extensively tested, I'm confident it'll be stable
'now is the time' from several icb inhabitants

Diff provides:
- ability to specify different allocators for different regions/maps
- a simpler implementation of the current allocator
- currently in compatibility mode: it will generate similar addresses
  as the old allocator
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.103 2011/07/08 00:10:59 tedu Exp $	*/
a335 2
	int swapins;		/* swapins */
	int swapouts;		/* swapouts */
@


1.103
log
@some machines don't boot with the previous uvm reserve enforcement diff.
back it out.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.102 2011/07/07 20:52:50 oga Exp $	*/
d188 1
d648 1
a648 2
#define	uvm_map(_m, _a, _sz, _u, _f, _al, _fl) uvm_map_p(_m, _a, _sz, _u, _f, _al, _fl, 0)
int			uvm_map_p(vm_map_t, vaddr_t *, vsize_t,
d650 1
a650 1
				uvm_flag_t, struct proc *);
@


1.102
log
@Move the uvm reserve enforcement from uvm_pagealloc to pmemrange.

More and more things are allocating outside of uvm_pagealloc these days making
it easy for something like the buffer cache to eat your last page with no
repercussions (other than a hung machine, of course).

ok ariane@@ also ok ariane@@ again after I spotted and fixed a possible underflow
problem in the calculation.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.101 2011/07/06 19:50:38 beck Exp $	*/
a224 1
#define UVM_PLA_USERESERVE	0x0020	/* ok to use reserve pages */
@


1.101
log
@
uvm changes for buffer cache improvements.
1) Make the pagedaemon aware of the memory ranges and size of allocations
where memory is being requested, and pass this information on to
bufbackoff(), which will later (not yet) be used to ensure that the
buffer cache gets out of the way in the right area of memory.

Note that this commit does not yet make it *do* that - as currently
the buffer cache is all in dma-able memory and it will simply back
off.

2) Add uvm_pagerealloc_multi - to be used by the buffer cache code
for reallocating pages to particular regions.

much of this work by ariane, with smatterings of me, art,and oga

ok oga@@, thib@@, ariane@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.100 2011/07/03 17:42:51 oga Exp $	*/
d225 1
@


1.100
log
@The UVM_STRAT defines have been unused since pmemrange was commited.

They may die now.

``kill it'' thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.99 2011/06/23 21:42:05 ariane Exp $	*/
d224 1
d258 18
d688 2
@


1.99
log
@Make mbufs and dma_alloc be contig allocations.
Requested by dlg@@

ok oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.98 2011/06/06 17:10:23 ariane Exp $	*/
a209 7

/*
 * the following defines the strategies for uvm_pagealloc()
 */
#define	UVM_PGA_STRAT_NORMAL	0	/* high -> low free list walk */
#define	UVM_PGA_STRAT_ONLY	1	/* only specified free list */
#define	UVM_PGA_STRAT_FALLBACK	2	/* ONLY falls back on NORMAL */
@


1.98
log
@Backout vmmap in order to repair virtual address selection algorithms
outside the tree.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.97 2011/05/30 22:25:24 oga Exp $	*/
d619 1
@


1.97
log
@Remove the freelist member from vm_physseg

The new world order of pmemrange makes this data completely redundant
(being dealt with by the pmemrange constraints instead). Remove all code
that messes with the freelist.

While touching every caller of uvm_page_physload() anyway, add the flags
argument to all callers (all but one is 0 and that one already used
PHYSLOAD_DEVICE) and remove the macro magic to allow callers to continue
without it.

Should shrink the code a bit, as well.

matthew@@ pointed out some mistakes i'd made.
``freelist death, I like. Ok.' ariane@@
`I agree with the general direction, go ahead and i'll fix any fallout
shortly'' miod@@ (68k 88k and vax i could not check would build)
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.96 2011/05/24 15:27:36 ariane Exp $	*/
a187 1
#define UVM_FLAG_QUERY   0x800000 /* do everything, except actual execution */
d634 2
a635 1
int			uvm_map(vm_map_t, vaddr_t *, vsize_t,
d637 1
a637 1
				uvm_flag_t);
@


1.96
log
@Reimplement uvm/uvm_map.

vmmap is designed to perform address space randomized allocations,
without letting fragmentation of the address space go through the roof.

Some highlights:
- kernel address space randomization
- proper implementation of guardpages
- roughly 10% system time reduction during kernel build

Tested by alot of people on tech@@ and developers.
Theo's machines are still happy.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.95 2011/04/18 19:23:46 art Exp $	*/
d676 2
a677 4
void			uvm_page_physload_flags(paddr_t, paddr_t, paddr_t,
			    paddr_t, int, int);
#define uvm_page_physload(s, e, as, ae, fl)	\
	uvm_page_physload_flags(s, e, as, ae, fl, 0)
@


1.95
log
@Put back the change of pool and malloc into the new km_alloc(9) api.
The problems during the hackathon were not caused by this (most likely).

prodded by deraadt@@ and beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.94 2011/04/06 15:52:13 art Exp $	*/
d188 1
d635 1
a635 2
#define	uvm_map(_m, _a, _sz, _u, _f, _al, _fl) uvm_map_p(_m, _a, _sz, _u, _f, _al, _fl, 0)
int			uvm_map_p(vm_map_t, vaddr_t *, vsize_t,
d637 1
a637 1
				uvm_flag_t, struct proc *);
@


1.94
log
@Backout the uvm_km_getpage -> km_alloc conversion. Weird things are happening
and we aren't sure what's causing them.

shouted oks by many before I even built a kernel with the diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.93 2011/04/05 01:28:05 art Exp $	*/
a535 8
void			*uvm_km_getpage_pla(boolean_t, int *, paddr_t, paddr_t,
			    paddr_t, paddr_t);
/* Wrapper around old function prototype. */
#define uvm_km_getpage(waitok, slowdown)				\
	uvm_km_getpage_pla(((waitok) ? 0 : UVM_KMF_NOWAIT), (slowdown),	\
	    (paddr_t)0, (paddr_t)-1, 0, 0)

void			uvm_km_putpage(void *);
d612 20
a631 19
extern struct kmem_va_mode kv_any;
extern struct kmem_va_mode kv_intrsafe;
extern struct kmem_va_mode kv_page;

extern struct kmem_pa_mode kp_dirty;
extern struct kmem_pa_mode kp_zero;
extern struct kmem_pa_mode kp_dma;
extern struct kmem_pa_mode kp_dma_zero;
extern struct kmem_pa_mode kp_pageable;
extern struct kmem_pa_mode kp_none;

extern struct kmem_dyn_mode kd_waitok;
extern struct kmem_dyn_mode kd_nowait;
extern struct kmem_dyn_mode kd_trylock;


void *km_alloc(size_t, struct kmem_va_mode *, struct kmem_pa_mode *,
    struct kmem_dyn_mode *);
void km_free(void *, size_t, struct kmem_va_mode *, struct kmem_pa_mode *);
@


1.93
log
@ - Change pool constraints to use kmem_pa_mode instead of uvm_constraint_range
 - Use km_alloc for all backend allocations in pools.
 - Use km_alloc for the emergmency kentry allocations in uvm_mapent_alloc
 - Garbage collect uvm_km_getpage, uvm_km_getpage_pla and uvm_km_putpage

ariane@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.92 2011/04/04 11:56:12 art Exp $	*/
d536 8
@


1.92
log
@Some minor fixes:
- Clarify a comment.
- Change all the flags to chars from ints to make the structs smaller.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.91 2011/04/04 11:24:45 art Exp $	*/
a535 8
void			*uvm_km_getpage_pla(boolean_t, int *, paddr_t, paddr_t,
			    paddr_t, paddr_t);
/* Wrapper around old function prototype. */
#define uvm_km_getpage(waitok, slowdown)				\
	uvm_km_getpage_pla(((waitok) ? 0 : UVM_KMF_NOWAIT), (slowdown),	\
	    (paddr_t)0, (paddr_t)-1, 0, 0)

void			uvm_km_putpage(void *);
@


1.91
log
@New unified allocator of kernel memory.

We've reached the point where we have a dozen allocators that all do more
or less the same thing, but slightly different, with slightly different
semantics, slightly different default behaviors and default behaviors that
most callers don't expect or want. A random sample on the last general
hackathon showed that no one could explain what all the different allocators
did. And every time someone needed to do something slightly different a
new allocator was written.

Unify everything. One single function to allocate multiples of PAGE_SIZE
kernel memory. Four arguments: size, how va is allocated, how pa is allocated
and misc parameters. Same parameters are passed to the free function so that
we don't need to guess what's going on.

Functions are currently unused, we'll do one thing at a time to avoid a
giant commit.

looked at by lots of people, deraadt@@ and beck@@ are yelling at me to commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.90 2011/04/02 16:47:17 beck Exp $	*/
d564 3
a566 3
	int kv_wait;
	int kv_singlepage;
	int kv_executable;
a586 1
	int kp_nomem;
d588 3
a590 2
	int kp_zero;
	int kp_pageable;
a604 2
	int kd_waitok;
	int kd_trylock;
d607 2
d611 1
a611 1
#define KMEM_DYN_INITIALIZER { 0, 0, UVM_UNKNOWN_OFFSET, NULL }
@


1.90
log
@Constrain the buffer cache to use only the dma reachable region of memory.
With this change bufcachepercent will be the percentage of dma reachable
memory that the buffer cache will attempt to use.
ok deraadt@@ thib@@ oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.89 2010/07/02 01:25:06 art Exp $	*/
a529 3
struct vm_map		*uvm_km_suballoc(vm_map_t, vaddr_t *,
				vaddr_t *, vsize_t, int,
				boolean_t, vm_map_t);
d544 95
@


1.89
log
@add an align argument to uvm_km_kmemalloc_pla.

Use uvm_km_kmemalloc_pla with the dma constraint to allocate kernel stacks.

Yes, that means DMA is possible to kernel stacks, but only until we've fixed
all the scary drivers.

deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.88 2010/07/01 21:27:39 art Exp $	*/
d586 2
@


1.88
log
@Implement vs{,un}lock_device and use it for physio.

Just like normal vs{,un}lock, but in case the pages we get are not dma
accessible, we bounce them, if they are dma acessible, the functions behave
exactly like normal vslock. The plan for the future is to have fault_wire
allocate dma acessible pages so that we don't need to bounce (especially
in cases where the same buffer is reused for physio over and over again),
but for now, keep it as simple as possible.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.87 2010/06/27 03:03:49 thib Exp $	*/
d524 1
a524 2
void			uvm_km_free_wakeup(vm_map_t, vaddr_t,
						vsize_t);
d526 2
a527 2
			    struct uvm_object *, vsize_t, int, paddr_t,
			    paddr_t, paddr_t, paddr_t, int);
d529 1
a529 1
	uvm_km_kmemalloc_pla(map, obj, sz, flags, 0, (paddr_t)-1, 0, 0, 0)
@


1.87
log
@uvm constraints. Add two mandatory MD symbols, uvm_md_constraints
which contains the constraints for DMA/memory allocation for each
architecture, and dma_constraints which contains the range of addresses
that are dma accessable by the system.

This is based on ariane@@'s physcontig diff, with lots of bugfixes and
additions the following additions by my self:

Introduce a new function pool_set_constraints() which sets the address
range for which we allocate pages for the pool from, this is now used
for the mbuf/mbuf cluster pools to keep them dma accessible.

The !direct archs no longer stuff pages into the kernel object in
uvm_km_getpage_pla but rather do a pmap_extract() in uvm_km_putpages.

Tested heavily by my self on i386, amd64 and sparc64. Some tests on
alpha and SGI.

"commit it" beck, art, oga, deraadt
"i like the diff" deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.85 2010/04/22 19:02:55 oga Exp $	*/
d505 5
@


1.86
log
@Move the prototype for uvm_wait() to uvm_extern.h and remove
uvm_pdaemon.h has it was only holding that one prototype.

OK art@@, oga@@, miod@@, deraadt@@
@
text
@d208 1
d521 5
a525 2
vaddr_t			uvm_km_kmemalloc(vm_map_t, struct uvm_object *,
						vsize_t, int);
d535 7
a541 1
void			*uvm_km_getpage(boolean_t, int *);
@


1.85
log
@Committing on behalf or ariane@@.

recommit pmemrange:
        physmem allocator: change the view of free memory from single
        free pages to free ranges.  Classify memory based on region with
        associated use-counter (which is used to construct a priority
        list of where to allocate memory).

	Based on code from tedu@@, help from many.

Useable now that bugs have been found and fixed in most architecture's
pmap.c

ok by everyone who has done a pmap or uvm commit in the last year.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.84 2010/03/24 00:36:04 oga Exp $	*/
d590 1
@


1.84
log
@Bring back PHYSLOAD_DEVICE for uvm_page_physload.

ok kettenis@@ beck@@ (tentatively) and ariane@@. deraadt asked for it to be
commited now.

original commit message:

	extend uvm_page_physload to have the ability to add "device" pages to
	the system.

	This is needed in the case where you need managed pages so you can
	handle faulting and pmap_page_protect() on said pages when you manage
	memory in such regions (i'm looking at you, graphics cards).

	these pages are flagged PG_DEV, and shall never be on the freelists,
	assert this. behaviour remains unchanged in the non-device case,
	specifically for all archs currently in the tree we panic if called
	after bootstrap.

	ok art@@ kettenis@@, beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.83 2010/02/12 01:35:14 tedu Exp $	*/
d211 1
a211 1
 * the following defines the strategies for uvm_pagealloc_strat()
d218 1
a218 1
 * flags for uvm_pagealloc_strat()
d229 1
d568 2
a569 5
struct vm_page		*uvm_pagealloc_strat(struct uvm_object *,
				voff_t, struct vm_anon *, int, int, int);
#define	uvm_pagealloc(obj, off, anon, flags) \
	    uvm_pagealloc_strat((obj), (off), (anon), (flags), \
				UVM_PGA_STRAT_NORMAL, 0)
d596 3
@


1.83
log
@introduce a uvm_km_valloc_try function that won't get a lower level lock
for use by the uvm pseg code.  this is the path of least resistance until
we sort out how many of these functions we really need.  problem found by mikeb
ok kettenis oga
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.82 2009/08/11 18:43:33 blambert Exp $	*/
d237 5
d577 4
a580 2
void			uvm_page_physload(paddr_t, paddr_t,
					       paddr_t, paddr_t, int);
@


1.82
log
@uvm_scheduler() sounds important, but ``while(1) tsleep()'' is kinda lame

inline the loop in the one place it exists, and remove it from uvm

adjust a comment mentioning it accordingly

originally inspired by a diff fixing a comment from oga@@

ok art@@ beck@@ miod@@ oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.81 2009/06/16 23:54:58 oga Exp $	*/
d520 1
a520 1
vaddr_t			uvm_km_valloc_align(vm_map_t, vsize_t, vsize_t);
d522 1
@


1.81
log
@date based reversion of uvm to the 4th May.

We still have no idea why this stops the crashes. but it does.

a machine forced to 64mb of ram cycled 10GB through swap with this diff
and is still running as I type this. Other tests by ariane@@ and thib@@
also seem to show that it's alright.

ok deraadt@@, thib@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.76 2009/04/20 00:30:18 oga Exp $	*/
a493 1
__dead void		uvm_scheduler(void);
@


1.80
log
@Backout pmemrange (which to most people is more well known as physmem
allocator).

"i can't see any obvious problems" oga
@
text
@@


1.79
log
@backout:
> extend uvm_page_physload to have the ability to add "device" pages to the
> system.
since it was overlayed over a system that we warned would go "in to be
tested, but may be pulled out".  oga, you just made me spend 20 minutes
of time I should not have had to spend doing this.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.77 2009/06/01 17:42:33 ariane Exp $	*/
d224 1
a224 1
 * flags for uvm_pglistalloc() and uvm_pmr_getpages()
a228 1
#define UVM_PLA_TRY_CONTIG	0x0008	/* try to allocate a contig range */
a590 4

/* uvm_pmemrange.c */

void			uvm_pmr_use_inc(paddr_t, paddr_t);
@


1.78
log
@extend uvm_page_physload to have the ability to add "device" pages to the
system.

This is needed in the case where you need managed pages so you can
handle faulting and pmap_page_protect() on said pages when you manage
memory in such regions (i'm looking at you, graphics cards).

these pages are flagged PG_DEV, and shall never be on the freelists,
assert this. behaviour remains unchanged in the non-device case,
specifically for all archs currently in the tree we panic if called
after bootstrap.

ok art@@, kettenis@@, ariane@@, beck@@.
@
text
@a237 5
 * flags to uvm_physload.
 */
#define	PHYSLOAD_DEVICE	0x01	/* don't add to the page queue */

/*
d573 2
a574 4
void			uvm_page_physload_flags(paddr_t, paddr_t, paddr_t,
			    paddr_t, int, int);
#define uvm_page_physload(s, e, as, ae, fl)	\
	uvm_page_physload_flags(s, e, as, ae, fl, 0)
@


1.77
log
@physmem allocator: change the view of free memory from single free pages
to free ranges.
Classify memory based on region with associated use-counter (which is used
to construct a priority list of where to allocate memory).

Based on code from tedu@@, help from many.
Ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.76 2009/04/20 00:30:18 oga Exp $	*/
d238 5
d578 4
a581 2
void			uvm_page_physload(paddr_t, paddr_t,
					       paddr_t, paddr_t, int);
@


1.76
log
@add the UVM_PLA_ZERO flag for uvm_pglistalloc to make it return zeroed
pages.

"go for it" miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.75 2009/04/14 16:01:04 oga Exp $	*/
d224 1
a224 1
 * flags for uvm_pglistalloc()
d229 1
d592 4
@


1.75
log
@Convert the waitok field of uvm_pglistalloc to "flags", more will be added soon.

For the possibility of sleeping, the first two flags are UVM_PLA_WAITOK
and UVM_PLA_NOWAIT. It is an error not to show intention, so assert that
one of the two is provided. Switch over every caller in the tree to
using the appropriate flag.

ok art@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.74 2009/03/05 19:52:24 kettenis Exp $	*/
d228 1
@


1.74
log
@Make ELF platforms generate ELF core dumps.  Somewhat based on code from
NetBSD.

ok kurt@@, drahn@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.73 2009/02/11 11:09:36 mikeb Exp $	*/
d222 6
@


1.73
log
@Remove uvm_km_alloc_poolpage1 as it serves no particular purpose
now and valid for __HAVE_PMAP_DIRECT archs only, though implements
both code paths.

Put it's code directly into the uvm_km_getpage for PMAP_DIRECT archs.

No functional change.

ok tedu, art
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.72 2008/11/04 21:37:06 deraadt Exp $	*/
d409 14
d591 4
@


1.72
log
@uvmspace_unshare() is never used; ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.71 2008/10/23 23:54:02 tedu Exp $	*/
a503 4
vaddr_t			uvm_km_alloc_poolpage1(vm_map_t,
				struct uvm_object *, boolean_t);
void			uvm_km_free_poolpage1(vm_map_t, vaddr_t);

@


1.71
log
@a better fix for the "uvm_km thread runs out of memory" problem.

add a new arg to the backend so it can tell pool to slow down.  when we get
this flag, yield *after* putting the page in the pool's free list.  whatever
we do, don't let the thread sleep.

this makes things better by still letting the thread run when a huge pf
request comes in, but without artificially increasing pressure on the backend
by eating pages without feeding them forward.

ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.70 2008/06/09 20:30:23 miod Exp $	*/
a530 1
void			uvmspace_unshare(struct proc *);
@


1.70
log
@Define a new flag, UVM_FLAG_HOLE, for uvm_map to create a vm_map_entry of
a new etype, UVM_ET_HOLE, meaning it has no backend.

UVM_ET_HOLE entries (which should be created as UVM_PROT_NONE and with
UVM_FLAG_NOMERGE and UVM_FLAG_HOLE) are skipped in uvm_unmap_remove(), so
that pmap_{k,}remove() is not called on the entry.

This is intended to save time, and behave better, on pmaps with MMU holes
at process exit time.

ok art@@, kettenis@@ provided feedback as well.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.69 2008/04/09 16:58:11 deraadt Exp $	*/
d508 1
a508 1
void			*uvm_km_getpage(boolean_t);
@


1.69
log
@Add new stub uvm_shutdown() and call it from the right place in MD boot()
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.69 2008/04/09 16:50:30 deraadt Exp $	*/
d187 1
@


1.68
log
@use a working mutex for the freepage list. ok art deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.67 2007/09/10 18:49:45 miod Exp $	*/
d558 1
@


1.67
log
@Introduce a md pmap hook, pmap_remove_holes(), which is supposed to mark
the holes a MMU may have from a given vm_map. This will be automagically
invoked for newly created vmspaces.

On platforms with MMU holes (e.g. sun4, sun4c and vax), this prevents
mmap(2) hints which would end up being in the hole to be accepted as valid,
causing unexpected signals when the process tries to access the hole
(since pmap can not fill the hole anyway).

Unfortunately, the logic mmap() uses to pick a valid address for anonymous
mappings needs work, as it will only try to find an address higher than the
hint, which causes all mmap() with a hint in the hole to fail on vax. This
will be improved later.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.66 2007/05/27 20:59:26 miod Exp $	*/
d375 1
@


1.66
log
@pagemove() is no longer used.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.65 2007/04/11 12:51:51 miod Exp $	*/
d522 1
a522 1
				boolean_t);
d524 1
a524 1
				vaddr_t, vaddr_t, boolean_t);
@


1.65
log
@The return value of uvm_grow() (and previously, grow()) has not been used
in 15 years, make it a void function.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.64 2007/04/11 12:10:42 art Exp $	*/
a446 1
void		pagemove(caddr_t, caddr_t, size_t);
@


1.64
log
@Instead of managing pages for intrsafe maps in special objects (aka.
kmem_object) just so that we can remove them, just use pmap_extract
to get the pages to free and simplify a lot of code to not deal with
the list of intrsafe maps, intrsafe objects, etc.

miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.63 2006/11/29 12:39:50 miod Exp $	*/
d580 1
a580 1
int			uvm_grow(struct proc *, vaddr_t);
@


1.63
log
@We don't use mb_map anymore since a long time already. Remove it.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.62 2006/11/29 12:26:14 miod Exp $	*/
a358 3
	/* kernel memory objects: managed by uvm_km_kmemalloc() only! */
	struct uvm_object *kmem_object;

a506 3
#define	uvm_km_alloc_poolpage(waitok)	uvm_km_alloc_poolpage1(kmem_map, \
						uvmexp.kmem_object, (waitok))
#define	uvm_km_free_poolpage(addr)	uvm_km_free_poolpage1(kmem_map, (addr))
@


1.62
log
@Remove cpu_swapin() and cpu_swapout(), they are no longer necessary (except
for cpu_swapin() on hppa* which is kept).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.61 2006/11/29 12:24:18 miod Exp $	*/
a414 1
extern struct vm_map *mb_map;
@


1.61
log
@Kernel stack can be swapped. This means that stuff that's on the stack
should never be referenced outside the context of the process to which
this stack belongs unless we do the PHOLD/PRELE dance. Loads of code
doesn't follow the rules here. Instead of trying to track down all
offenders and fix this hairy situation, it makes much more sense
to not swap kernel stacks.

From art@@, tested by many some time ago.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.60 2006/11/29 12:17:33 miod Exp $	*/
a451 6
#ifndef	cpu_swapin
void		cpu_swapin(struct proc *);
#endif
#ifndef	cpu_swapout
void		cpu_swapout(struct proc *);
#endif
@


1.60
log
@Add an alignment parameter to uvm_km_alloc1(), and change all callers to
pass zero; this will be used shortly. From art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.59 2005/11/04 21:48:07 miod Exp $	*/
d483 1
a483 1
void			uvm_swapin(struct proc *);
@


1.59
log
@Add an extra flags argument to uvm_io(), to specify whether we want to fix
the protection of the memory mapping we're doing I/O on, or if we want to
leave them as they are. This should only be necessary for breakpoint
insertion in code, so we'll only use it for ptrace requests.

Initially from art@@ after discussion with kettenis@@ millert@@ and I,
tested by many.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.58 2005/09/28 00:24:03 pedro Exp $	*/
d424 2
a425 2
#define uvm_km_zalloc(MAP,SIZE) uvm_km_alloc1(MAP,SIZE,TRUE)
#define uvm_km_alloc(MAP,SIZE)  uvm_km_alloc1(MAP,SIZE,FALSE)
d499 1
a499 1
vaddr_t			uvm_km_alloc1(vm_map_t, vsize_t, boolean_t);
@


1.58
log
@- when we run out of static kernel map entries, grab a fresh page using
  the uvm_km_page allocator and use it instead of calling panic()
- add a counter to uvmexp so we can keep track of how many map entries
  we have in use

idea from tedu@@, long ago, okay deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.57 2005/09/12 23:05:06 miod Exp $	*/
d494 3
a496 1
int			uvm_io(vm_map_t, struct uio *);
@


1.57
log
@Change the NKMEMPAGES range to 4-64MB for 32bit arches, and 8-128MB for 64bit
arches; except on sparc where the range is 4-8 for !sun4m and 4-64 for sun4m,
selected at runtime.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.56 2005/05/24 21:11:47 tedu Exp $	*/
d363 1
@


1.56
log
@add a new field to vm_space and use it to track the number of anon
pages a process uses.  this is now the userland "data size" value.
ok art deraadt tdeval.  thanks testers.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.55 2005/04/21 04:39:34 mickey Exp $	*/
d606 1
a606 1
extern int		nkmempages;
@


1.55
log
@count fpu lazy context switches; deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.54 2004/12/30 08:28:39 niklas Exp $	*/
d398 1
d521 2
a522 1
int			uvm_map(vm_map_t, vaddr_t *, vsize_t,
d524 1
a524 1
				uvm_flag_t);
d552 1
a552 1
				caddr_t, voff_t, vsize_t);
@


1.54
log
@Import M_CANFAIL support from NetBSD, removes a nasty panic during low-mem scenarios, instead generating an ENOMEM backfeed, ok tedu@@, prodded by many
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.53 2004/05/27 04:55:28 tedu Exp $	*/
d361 2
@


1.53
log
@change uvm_km_getpage to take waitok argument and sleep if appropriate.
change both the nointr and default pool allocators to using uvm_km_getpage.
change pools to default to a maxpages value of 8, so they hoard less memory.
change mbuf pools to use default pool allocator.
pools are now more efficient, use less of kmem_map, and a bit faster.
tested mcbride, deraadt, pedro, drahn, miod to work everywhere
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.52 2004/04/28 02:20:58 markus Exp $	*/
d206 1
@


1.52
log
@remove mb_object*; ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.51 2004/04/19 22:52:33 tedu Exp $	*/
d513 1
a513 1
void			*uvm_km_getpage(void);
@


1.51
log
@introduce a new km_page allocator that gets pages from kernel_map using
an interrupt safe thread.
use this as the new backend for mbpool and mclpool, eliminating the mb_map.
introduce a sysctl kern.maxclusters which controls the limit of clusters
allocated.
testing by many people, works everywhere but m68k.  ok deraadt@@

this essentially deprecates the NMBCLUSTERS option, don't use it.
this should reduce pressure on the kmem_map and the uvm reserve of static
map entries.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.50 2003/08/10 00:04:50 miod Exp $	*/
a359 1
	struct uvm_object *mb_object;
@


1.50
log
@Remove uvm_useracc(): misleading, gives a false sentiment of security, and
eventually not used anymore. Conforming to art@@'s evil plans.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.49 2003/06/02 23:28:24 millert Exp $	*/
d514 2
@


1.49
log
@Remove the advertising clause in the UCB license which Berkeley
rescinded 22 July 1999.  Proofed by myself and Theo.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.48 2003/05/01 09:29:02 jmc Exp $	*/
a479 1
boolean_t		uvm_useracc(caddr_t, size_t, int);
@


1.48
log
@typos;
ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.47 2003/03/28 10:16:29 jmc Exp $	*/
d50 1
a50 5
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.47
log
@zero'd -> zeroed;

ok art@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.46 2002/10/29 18:30:21 art Exp $	*/
d350 1
a350 1
	int pdscans;	/* number of pages daemon scaned since boot */
@


1.46
log
@Since memory deallocation can't fail, remove the error return from
uvm_unmap, uvm_deallocate and a few other functions.
Simplifies some code and reduces diff to the UBC branch.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.45 2002/03/14 03:16:13 millert Exp $	*/
d223 1
a223 1
#define	UVM_PGA_ZERO		0x0002	/* returned page must be zero'd */
@


1.45
log
@Final __P removal plus some cosmetic fixups
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.44 2002/03/14 01:27:18 millert Exp $	*/
d592 1
a592 1
int			uvm_deallocate(vm_map_t, vaddr_t, vsize_t);
@


1.44
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.43 2002/02/28 18:50:26 provos Exp $	*/
d458 2
a459 2
void		cpu_fork __P((struct proc *, struct proc *, void *, size_t,
			void (*)(void *), void *));
d477 2
a478 2
void			uvm_fork __P((struct proc *, struct proc *, boolean_t,
			    void *, size_t, void (*)(void *), void *));
@


1.43
log
@use red-black tree for lookup_entry.  the red-black tree case for
map_findspace is still broken on alpha.  this will make debugging easier.
okay millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.41 2002/02/25 00:20:45 provos Exp $	*/
d449 3
a451 3
void		vmapbuf __P((struct buf *, vsize_t));
void		vunmapbuf __P((struct buf *, vsize_t));
void		pagemove __P((caddr_t, caddr_t, size_t));
d453 1
a453 1
void		cpu_swapin __P((struct proc *));
d456 1
a456 1
void		cpu_swapout __P((struct proc *));
d462 5
a466 5
struct uvm_object	*uao_create __P((vsize_t, int));
void			uao_detach __P((struct uvm_object *));
void			uao_detach_locked __P((struct uvm_object *));
void			uao_reference __P((struct uvm_object *));
void			uao_reference_locked __P((struct uvm_object *));
d469 2
a470 2
int			uvm_fault __P((vm_map_t, vaddr_t, 
				vm_fault_t, vm_prot_t));
d475 1
a475 1
void			uvm_chgkprot __P((caddr_t, size_t, int));
d479 9
a487 9
void			uvm_exit __P((struct proc *));
void			uvm_init_limits __P((struct proc *));
boolean_t		uvm_kernacc __P((caddr_t, size_t, int));
__dead void		uvm_scheduler __P((void));
void			uvm_swapin __P((struct proc *));
boolean_t		uvm_useracc __P((caddr_t, size_t, int));
int			uvm_vslock __P((struct proc *, caddr_t, size_t,
			    vm_prot_t));
void			uvm_vsunlock __P((struct proc *, caddr_t, size_t));
d491 1
a491 1
void			uvm_init __P((void));	
d495 1
a495 1
int			uvm_io __P((vm_map_t, struct uio *));
d498 7
a504 7
vaddr_t			uvm_km_alloc1 __P((vm_map_t, vsize_t, boolean_t));
void			uvm_km_free __P((vm_map_t, vaddr_t, vsize_t));
void			uvm_km_free_wakeup __P((vm_map_t, vaddr_t,
						vsize_t));
vaddr_t			uvm_km_kmemalloc __P((vm_map_t, struct uvm_object *,
						vsize_t, int));
struct vm_map		*uvm_km_suballoc __P((vm_map_t, vaddr_t *,
d506 9
a514 9
				boolean_t, vm_map_t));
vaddr_t			uvm_km_valloc __P((vm_map_t, vsize_t));
vaddr_t			uvm_km_valloc_align __P((vm_map_t, vsize_t, vsize_t));
vaddr_t			uvm_km_valloc_wait __P((vm_map_t, vsize_t));
vaddr_t			uvm_km_valloc_prefer_wait __P((vm_map_t, vsize_t,
					voff_t));
vaddr_t			uvm_km_alloc_poolpage1 __P((vm_map_t,
				struct uvm_object *, boolean_t));
void			uvm_km_free_poolpage1 __P((vm_map_t, vaddr_t));
d521 1
a521 1
int			uvm_map __P((vm_map_t, vaddr_t *, vsize_t,
d523 17
a539 17
				uvm_flag_t));
int			uvm_map_pageable __P((vm_map_t, vaddr_t, 
				vaddr_t, boolean_t, int));
int			uvm_map_pageable_all __P((vm_map_t, int, vsize_t));
boolean_t		uvm_map_checkprot __P((vm_map_t, vaddr_t,
				vaddr_t, vm_prot_t));
int			uvm_map_protect __P((vm_map_t, vaddr_t, 
				vaddr_t, vm_prot_t, boolean_t));
struct vmspace		*uvmspace_alloc __P((vaddr_t, vaddr_t,
				boolean_t));
void			uvmspace_init __P((struct vmspace *, struct pmap *,
				vaddr_t, vaddr_t, boolean_t));
void			uvmspace_exec __P((struct proc *, vaddr_t, vaddr_t));
struct vmspace		*uvmspace_fork __P((struct vmspace *));
void			uvmspace_free __P((struct vmspace *));
void			uvmspace_share __P((struct proc *, struct proc *));
void			uvmspace_unshare __P((struct proc *));
d543 4
a546 4
void			uvm_meter __P((void));
int			uvm_sysctl __P((int *, u_int, void *, size_t *, 
				void *, size_t, struct proc *));
void			uvm_total __P((struct vmtotal *));
d549 1
a549 1
int			uvm_mmap __P((vm_map_t, vaddr_t *, vsize_t,
d551 1
a551 1
				caddr_t, voff_t, vsize_t));
d554 2
a555 2
struct vm_page		*uvm_pagealloc_strat __P((struct uvm_object *,
				voff_t, struct vm_anon *, int, int, int));
d559 4
a562 4
vaddr_t			uvm_pagealloc_contig __P((vaddr_t, vaddr_t,
				vaddr_t, vaddr_t));
void			uvm_pagerealloc __P((struct vm_page *, 
					     struct uvm_object *, voff_t));
d564 3
a566 3
void			uvm_page_physload __P((paddr_t, paddr_t,
					       paddr_t, paddr_t, int));
void			uvm_setpagesize __P((void));
d569 3
a571 3
void			uvm_aio_biodone1 __P((struct buf *));
void			uvm_aio_biodone __P((struct buf *));
void			uvm_aio_aiodone __P((struct buf *));
d574 2
a575 2
void			uvm_pageout __P((void *));
void			uvm_aiodone_daemon __P((void *));
d578 1
a578 1
int			uvm_pglistalloc __P((psize_t, paddr_t,
d580 2
a581 2
				struct pglist *, int, int)); 
void			uvm_pglistfree __P((struct pglist *));
d584 1
a584 1
void			uvm_swap_init __P((void));
d587 3
a589 3
int			uvm_coredump __P((struct proc *, struct vnode *, 
				struct ucred *, struct core *));
int			uvm_grow __P((struct proc *, vaddr_t));
d592 1
a592 1
int			uvm_deallocate __P((vm_map_t, vaddr_t, vsize_t));
d595 3
a597 3
void			uvm_vnp_setsize __P((struct vnode *, voff_t));
void			uvm_vnp_sync __P((struct mount *));
void 			uvm_vnp_terminate __P((struct vnode *));
d599 2
a600 2
boolean_t		uvm_vnp_uncache __P((struct vnode *));
struct uvm_object	*uvn_attach __P((void *, vm_prot_t));
d603 2
a604 2
void			kmeminit_nkmempages __P((void));
void			kmeminit __P((void));
@


1.42
log
@back out red-black tree. they are very fast but alpha UVM is broken and
the tree triggers the bug, PMAP_PREFER case was broken also.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.40 2001/12/19 08:58:07 art Exp $	*/
d376 1
@


1.41
log
@use a red-black tree to find entries in the vm_map. augment the red-black
tree to find free space between entries.  speeds up memory allocation,
etc...
@
text
@a375 1
#include <sys/tree.h>
@


1.40
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.33 2001/11/12 01:26:09 art Exp $	*/
d376 1
@


1.39
log
@remove unnecessary newline
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.38 2001/12/04 23:22:42 art Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.66 2001/08/16 01:37:50 chs Exp $	*/
d91 1
a91 1
typedef unsigned int uvm_flag_t;
d97 12
a225 15
 * the following defines are for ubc_alloc's flags
 */
#define UBC_READ	0
#define UBC_WRITE	1

/*
 * flags for uvn_findpages().
 */
#define UFP_ALL		0x0
#define UFP_NOWAIT	0x1
#define UFP_NOALLOC	0x2
#define UFP_NOCACHE	0x4
#define UFP_NORDONLY	0x8

/*
a250 3
struct vm_map_entry;
struct vm_map;
struct vm_page;
a272 3
	int ncolors;	/* number of page color buckets: must be p-o-2 */
	int colormask;	/* color bucket mask */

d322 2
a323 3
	int zeroaborts;		/* number of times page zeroing was aborted */
	int colorhit;		/* pagealloc where we got optimal color */
	int colormiss;		/* pagealloc where we didn't */
d395 1
a395 1
	segsz_t vm_rssize;	/* current resident set size in pages */
d417 1
d428 1
d430 3
a466 7
/* uvm_bio.c */
void			ubc_init __P((void));
void *			ubc_alloc __P((struct uvm_object *, voff_t, vsize_t *,
				       int));
void			ubc_release __P((void *, vsize_t));
void			ubc_flush __P((struct uvm_object *, voff_t, voff_t));

d468 2
a469 2
int			uvm_fault __P((struct vm_map *, vaddr_t, vm_fault_t,
				       vm_prot_t));
d490 1
a490 1
void			uvm_init __P((void));
d494 1
a494 1
int			uvm_io __P((struct vm_map *, struct uio *));
d497 21
a517 24
vaddr_t			uvm_km_alloc1 __P((struct vm_map *, vsize_t,
			    boolean_t));
void			uvm_km_free __P((struct vm_map *, vaddr_t, vsize_t));
void			uvm_km_free_wakeup __P((struct vm_map *, vaddr_t,
			    vsize_t));
vaddr_t			uvm_km_kmemalloc __P((struct vm_map *, struct
			    uvm_object *, vsize_t, int));
struct vm_map		*uvm_km_suballoc __P((struct vm_map *, vaddr_t *,
			    vaddr_t *, vsize_t, int, boolean_t,
			    struct vm_map *));
vaddr_t			uvm_km_valloc __P((struct vm_map *, vsize_t));
vaddr_t			uvm_km_valloc_align __P((struct vm_map *, vsize_t,
			    vsize_t));
vaddr_t			uvm_km_valloc_wait __P((struct vm_map *, vsize_t));
vaddr_t			uvm_km_valloc_prefer_wait __P((struct vm_map *, vsize_t,
			    voff_t));
vaddr_t			uvm_km_alloc_poolpage1 __P((struct vm_map *,
			    struct uvm_object *, boolean_t));
void			uvm_km_free_poolpage1 __P((struct vm_map *, vaddr_t));

#define	uvm_km_alloc_poolpage(waitok) \
	uvm_km_alloc_poolpage1(kmem_map, uvmexp.kmem_object, (waitok))
#define	uvm_km_free_poolpage(addr) \
	uvm_km_free_poolpage1(kmem_map, (addr))
d520 1
a520 1
int			uvm_map __P((struct vm_map *, vaddr_t *, vsize_t,
d523 1
a523 1
int			uvm_map_pageable __P((struct vm_map *, vaddr_t,
d525 2
a526 3
int			uvm_map_pageable_all __P((struct vm_map *, int,
				vsize_t));
boolean_t		uvm_map_checkprot __P((struct vm_map *, vaddr_t,
d528 1
a528 1
int			uvm_map_protect __P((struct vm_map *, vaddr_t,
d530 2
a531 1
struct vmspace		*uvmspace_alloc __P((vaddr_t, vaddr_t));
d533 1
a533 1
				vaddr_t, vaddr_t));
d543 1
a543 1
int			uvm_sysctl __P((int *, u_int, void *, size_t *,
d548 3
a550 3
int			uvm_mmap __P((struct vm_map *, vaddr_t *, vsize_t,
				vm_prot_t, vm_prot_t, int,
				void *, voff_t, vsize_t));
d558 3
a560 1
void			uvm_pagerealloc __P((struct vm_page *,
d579 1
a579 1
				struct pglist *, int, int));
d586 1
a586 1
int			uvm_coredump __P((struct proc *, struct vnode *,
d591 1
a591 1
void			uvm_deallocate __P((struct vm_map *, vaddr_t, vsize_t));
d596 3
a599 4
void			uvn_findpages __P((struct uvm_object *, voff_t,
					   int *, struct vm_page **, int));
void			uvm_vnp_zerorange __P((struct vnode *, off_t, size_t));
void			uvm_vnp_asyncget __P((struct vnode *, off_t, size_t));
@


1.39.2.1
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.39 2001/12/06 23:01:07 niklas Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.68 2001/12/08 00:35:33 thorpej Exp $	*/
d189 1
a189 1
/* magic offset value: offset not known(obj) or don't care(!obj) */
d191 1
d216 2
a217 3
#define UBC_READ	0x01
#define UBC_WRITE	0x02
#define UBC_FAULTBUSY	0x04
d222 5
a226 7
#define UFP_ALL		0x00
#define UFP_NOWAIT	0x01
#define UFP_NOALLOC	0x02
#define UFP_NOCACHE	0x04
#define UFP_NORDONLY	0x08
#define UFP_DIRTYONLY	0x10
#define UFP_BACKWARD	0x20
d371 4
d439 1
d476 1
a476 1
void			ubc_release __P((void *, int));
d503 1
d529 3
a531 3
#define	uvm_km_alloc_poolpage(waitok)					\
	uvm_km_alloc_poolpage1(kmem_map, NULL, (waitok))
#define	uvm_km_free_poolpage(addr)					\
d576 2
a577 2
void			uvm_page_physload __P((paddr_t, paddr_t, paddr_t,
			    paddr_t, int));
d590 3
a592 2
int			uvm_pglistalloc __P((psize_t, paddr_t, paddr_t,
			    paddr_t, paddr_t, struct pglist *, int, int));
d613 1
@


1.39.2.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.39.2.1 2002/02/02 03:28:26 art Exp $	*/
a383 1
#include <sys/tree.h>
d450 3
a452 3
void		vmapbuf(struct buf *, vsize_t);
void		vunmapbuf(struct buf *, vsize_t);
void		pagemove(caddr_t, caddr_t, size_t);
d454 1
a454 1
void		cpu_swapin(struct proc *);
d457 1
a457 1
void		cpu_swapout(struct proc *);
d459 2
a460 2
void		cpu_fork(struct proc *, struct proc *, void *, size_t,
		    void (*)(void *), void *);
d463 5
a467 5
struct uvm_object	*uao_create(vsize_t, int);
void			uao_detach(struct uvm_object *);
void			uao_detach_locked(struct uvm_object *);
void			uao_reference(struct uvm_object *);
void			uao_reference_locked(struct uvm_object *);
d470 5
a474 5
void			ubc_init(void);
void *			ubc_alloc(struct uvm_object *, voff_t, vsize_t *,
				       int);
void			ubc_release(void *, int);
void			ubc_flush(struct uvm_object *, voff_t, voff_t);
d477 2
a478 2
int			uvm_fault(struct vm_map *, vaddr_t, vm_fault_t,
				       vm_prot_t);
d483 1
a483 1
void			uvm_chgkprot(caddr_t, size_t, int);
d485 11
a495 11
void			uvm_fork(struct proc *, struct proc *, boolean_t,
			    void *, size_t, void (*)(void *), void *);
void			uvm_exit(struct proc *);
void			uvm_init_limits(struct proc *);
boolean_t		uvm_kernacc(caddr_t, size_t, int);
__dead void		uvm_scheduler(void);
void			uvm_swapin(struct proc *);
boolean_t		uvm_useracc(caddr_t, size_t, int);
int			uvm_vslock(struct proc *, caddr_t, size_t,
			    vm_prot_t);
void			uvm_vsunlock(struct proc *, caddr_t, size_t);
d499 1
a499 1
void			uvm_init(void);
d502 1
a502 1
int			uvm_io(struct vm_map *, struct uio *);
d505 8
a512 8
vaddr_t			uvm_km_alloc1(struct vm_map *, vsize_t,
			    boolean_t);
void			uvm_km_free(struct vm_map *, vaddr_t, vsize_t);
void			uvm_km_free_wakeup(struct vm_map *, vaddr_t,
			    vsize_t);
vaddr_t			uvm_km_kmemalloc(struct vm_map *, struct
			    uvm_object *, vsize_t, int);
struct vm_map		*uvm_km_suballoc(struct vm_map *, vaddr_t *,
d514 10
a523 10
			    struct vm_map *);
vaddr_t			uvm_km_valloc(struct vm_map *, vsize_t);
vaddr_t			uvm_km_valloc_align(struct vm_map *, vsize_t,
			    vsize_t);
vaddr_t			uvm_km_valloc_wait(struct vm_map *, vsize_t);
vaddr_t			uvm_km_valloc_prefer_wait(struct vm_map *, vsize_t,
			    voff_t);
vaddr_t			uvm_km_alloc_poolpage1(struct vm_map *,
			    struct uvm_object *, boolean_t);
void			uvm_km_free_poolpage1(struct vm_map *, vaddr_t);
d531 1
a531 1
int			uvm_map(struct vm_map *, vaddr_t *, vsize_t,
d533 17
a549 17
				uvm_flag_t);
int			uvm_map_pageable(struct vm_map *, vaddr_t,
				vaddr_t, boolean_t, int);
int			uvm_map_pageable_all(struct vm_map *, int,
				vsize_t);
boolean_t		uvm_map_checkprot(struct vm_map *, vaddr_t,
				vaddr_t, vm_prot_t);
int			uvm_map_protect(struct vm_map *, vaddr_t,
				vaddr_t, vm_prot_t, boolean_t);
struct vmspace		*uvmspace_alloc(vaddr_t, vaddr_t);
void			uvmspace_init(struct vmspace *, struct pmap *,
				vaddr_t, vaddr_t);
void			uvmspace_exec(struct proc *, vaddr_t, vaddr_t);
struct vmspace		*uvmspace_fork(struct vmspace *);
void			uvmspace_free(struct vmspace *);
void			uvmspace_share(struct proc *, struct proc *);
void			uvmspace_unshare(struct proc *);
d553 4
a556 4
void			uvm_meter(void);
int			uvm_sysctl(int *, u_int, void *, size_t *,
				void *, size_t, struct proc *);
void			uvm_total(struct vmtotal *);
d559 1
a559 1
int			uvm_mmap(struct vm_map *, vaddr_t *, vsize_t,
d561 1
a561 1
				void *, voff_t, vsize_t);
d564 2
a565 2
struct vm_page		*uvm_pagealloc_strat(struct uvm_object *,
				voff_t, struct vm_anon *, int, int, int);
d569 2
a570 2
void			uvm_pagerealloc(struct vm_page *,
					     struct uvm_object *, voff_t);
d572 3
a574 3
void			uvm_page_physload(paddr_t, paddr_t, paddr_t,
			    paddr_t, int);
void			uvm_setpagesize(void);
d577 3
a579 3
void			uvm_aio_biodone1(struct buf *);
void			uvm_aio_biodone(struct buf *);
void			uvm_aio_aiodone(struct buf *);
d582 2
a583 2
void			uvm_pageout(void *);
void			uvm_aiodone_daemon(void *);
d586 3
a588 3
int			uvm_pglistalloc(psize_t, paddr_t, paddr_t,
			    paddr_t, paddr_t, struct pglist *, int, int);
void			uvm_pglistfree(struct pglist *);
d591 1
a591 1
void			uvm_swap_init(void);
d594 3
a596 3
int			uvm_coredump(struct proc *, struct vnode *,
				struct ucred *, struct core *);
int			uvm_grow(struct proc *, vaddr_t);
d599 1
a599 1
void			uvm_deallocate(struct vm_map *, vaddr_t, vsize_t);
d602 6
a607 6
void			uvm_vnp_setsize(struct vnode *, voff_t);
void			uvm_vnp_sync(struct mount *);
struct uvm_object	*uvn_attach(void *, vm_prot_t);
void			uvn_findpages(struct uvm_object *, voff_t,
					   int *, struct vm_page **, int);
void			uvm_vnp_zerorange(struct vnode *, off_t, size_t);
d610 2
a611 2
void			kmeminit_nkmempages(void);
void			kmeminit(void);
@


1.39.2.3
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.39.2.2 2002/06/11 03:33:03 art Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.73 2002/09/22 07:20:31 chs Exp $	*/
a196 1
#define UVM_KMF_CANFAIL	0x4			/* caller handles failure */
d287 3
a289 3
	int anonpages;		/* number of pages used by anon mappings */
	int filepages;		/* number of pages used by cached file data */
	int execpages;		/* number of pages used by cached exec date */
d297 2
a298 2
	int execmin;	/* min threshold for executable pages */
	int filemin;	/* min threshold for file pages */
d300 2
a301 8
	int execminpct;	/* min percent executable pages */
	int fileminpct;	/* min percent file pages */
	int anonmax;	/* max threshold for anon pages */
	int execmax;	/* max threshold for executable pages */
	int filemax;	/* max threshold for file pages */
	int anonmaxpct;	/* max percent anon pages */
	int execmaxpct;	/* max percent executable pages */
	int filemaxpct;	/* max percent file pages */
d370 3
a372 3
	int pdreanon;	/* anon pages reactivated due to thresholds */
	int pdrefile;	/* file pages reactivated due to thresholds */
	int pdreexec;	/* executable pages reactivated due to thresholds */
a417 14
 * used to keep state while iterating over the map for a core dump.
 */
struct uvm_coredump_state {
	void *cookie;		/* opaque for the caller */
	vaddr_t start;		/* start of region */
	vaddr_t end;		/* end of region */
	vm_prot_t prot;		/* protection of region */
	int flags;		/* flags; see below */
};

#define	UVM_COREDUMP_STACK	0x01	/* region is user stack */
#define	UVM_COREDUMP_NODUMP	0x02	/* don't actually dump this region */

/*
a485 5
int			uvm_coredump_walkmap __P((struct proc *,
			    struct vnode *, struct ucred *,
			    int (*)(struct proc *, struct vnode *,
				    struct ucred *,
				    struct uvm_coredump_state *), void *));
a492 2
vaddr_t			uvm_uarea_alloc(void);
void			uvm_uarea_free(vaddr_t);
d606 1
a606 1
int			uvn_findpages(struct uvm_object *, voff_t,
@


1.39.2.4
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d211 1
a211 1
#define	UVM_PGA_ZERO		0x0002	/* returned page must be zeroed */
d369 1
a369 1
	int pdscans;	/* number of pages daemon scanned since boot */
@


1.38
log
@Yet another sync to NetBSD uvm.
Today we add a pmap argument to pmap_update() and allocate map entries for
kernel_map from kmem_map instead of using the static entries. This should
get rid of MAX_KMAPENT panics. Also some uvm_loan problems are fixed.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.37 2001/11/30 17:24:19 art Exp $	*/
d331 1
a331 2
	int zeroaborts;		/* number of times page zeroing was
				   aborted */
@


1.37
log
@Kill uvm_pagealloc_contig. The two drivers that still used it should have
been converted to bus_dma ages ago, but since noone haven't bothered to do that
I haven't bothered to do more than to test that the kernel still builds
with those changes.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.36 2001/11/28 19:28:14 art Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.65 2001/06/02 18:09:26 chs Exp $	*/
d547 1
a547 2
struct vmspace		*uvmspace_alloc __P((vaddr_t, vaddr_t,
				boolean_t));
d549 1
a549 1
				vaddr_t, vaddr_t, boolean_t));
@


1.36
log
@Sync in more uvm from NetBSD. Mostly just cosmetic stuff.
Contains also support for page coloring.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.35 2001/11/28 13:47:39 art Exp $	*/
a574 2
vaddr_t			uvm_pagealloc_contig __P((vaddr_t, vaddr_t,
				vaddr_t, vaddr_t));
@


1.35
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.34 2001/11/27 05:27:12 art Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.58 2001/03/15 06:10:56 chs Exp $	*/
d91 1
a91 1
typedef unsigned int  uvm_flag_t;
a96 12
union vm_map_object;
typedef union vm_map_object vm_map_object_t;

struct vm_map_entry;
typedef struct vm_map_entry *vm_map_entry_t;

struct vm_map;
typedef struct vm_map *vm_map_t;

struct vm_page;
typedef struct vm_page  *vm_page_t;

d254 3
d279 3
d333 2
d406 1
a406 1
	segsz_t vm_rssize; 	/* current resident set size in pages */
a427 1

a437 1
#ifdef	pmap_resident_count
a438 3
#else
#define vm_resident_count(vm) ((vm)->vm_rssize)
#endif
d481 1
a481 1
int			uvm_fault __P((vm_map_t, vaddr_t, vm_fault_t,
d503 1
a503 1
void			uvm_init __P((void));	
d507 1
a507 1
int			uvm_io __P((vm_map_t, struct uio *));
d510 24
a533 21
vaddr_t			uvm_km_alloc1 __P((vm_map_t, vsize_t, boolean_t));
void			uvm_km_free __P((vm_map_t, vaddr_t, vsize_t));
void			uvm_km_free_wakeup __P((vm_map_t, vaddr_t,
						vsize_t));
vaddr_t			uvm_km_kmemalloc __P((vm_map_t, struct uvm_object *,
						vsize_t, int));
struct vm_map		*uvm_km_suballoc __P((vm_map_t, vaddr_t *,
				vaddr_t *, vsize_t, int,
				boolean_t, vm_map_t));
vaddr_t			uvm_km_valloc __P((vm_map_t, vsize_t));
vaddr_t			uvm_km_valloc_align __P((vm_map_t, vsize_t, vsize_t));
vaddr_t			uvm_km_valloc_wait __P((vm_map_t, vsize_t));
vaddr_t			uvm_km_valloc_prefer_wait __P((vm_map_t, vsize_t,
					voff_t));
vaddr_t			uvm_km_alloc_poolpage1 __P((vm_map_t,
				struct uvm_object *, boolean_t));
void			uvm_km_free_poolpage1 __P((vm_map_t, vaddr_t));

#define	uvm_km_alloc_poolpage(waitok)	uvm_km_alloc_poolpage1(kmem_map, \
						uvmexp.kmem_object, (waitok))
#define	uvm_km_free_poolpage(addr)	uvm_km_free_poolpage1(kmem_map, (addr))
d536 1
a536 1
int			uvm_map __P((vm_map_t, vaddr_t *, vsize_t,
d539 1
a539 1
int			uvm_map_pageable __P((vm_map_t, vaddr_t, 
d541 3
a543 2
int			uvm_map_pageable_all __P((vm_map_t, int, vsize_t));
boolean_t		uvm_map_checkprot __P((vm_map_t, vaddr_t,
d545 1
a545 1
int			uvm_map_protect __P((vm_map_t, vaddr_t, 
d560 1
a560 1
int			uvm_sysctl __P((int *, u_int, void *, size_t *, 
d565 2
a566 2
int			uvm_mmap __P((vm_map_t, vaddr_t *, vsize_t,
				vm_prot_t, vm_prot_t, int, 
d577 1
a577 1
void			uvm_pagerealloc __P((struct vm_page *, 
d596 1
a596 1
				struct pglist *, int, int)); 
d603 1
a603 1
int			uvm_coredump __P((struct proc *, struct vnode *, 
d608 1
a608 1
void			uvm_deallocate __P((vm_map_t, vaddr_t, vsize_t));
@


1.34
log
@Merge in the unified buffer cache code as found in NetBSD 2001/03/10. The
code is written mostly by Chuck Silvers <chuq@@chuq.com>/<chs@@netbsd.org>.

Tested for the past few weeks by many developers, should be in a pretty stable
state, but will require optimizations and additional cleanups.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.33 2001/11/12 01:26:09 art Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.57 2001/03/09 01:02:12 chs Exp $	*/
d572 1
a572 1
				caddr_t, voff_t, vsize_t));
d613 1
a613 1
int			uvm_deallocate __P((vm_map_t, vaddr_t, vsize_t));
@


1.33
log
@Bring in more changes from NetBSD. Mostly pagedaemon improvements.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.32 2001/11/10 18:42:31 art Exp $	*/
d226 15
d482 7
d490 2
a491 2
int			uvm_fault __P((vm_map_t, vaddr_t, 
				vm_fault_t, vm_prot_t));
a617 3
void 			uvm_vnp_terminate __P((struct vnode *));
				/* terminate a uvm/uvn object */
boolean_t		uvm_vnp_uncache __P((struct vnode *));
d619 4
@


1.32
log
@Merge in some parts of the ubc work that has been done in NetBSD that are not
UBC, but prerequsites for it.

- Create a daemon that processes async I/O (swap and paging in the future)
  requests that need processing in process context and that were processed
  in the pagedaemon before.
- Convert some ugly ifdef DIAGNOSTIC code to less intrusive KASSERTs.
- misc other cleanups.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.31 2001/11/09 03:32:23 art Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.52 2000/11/27 04:36:40 nisimura Exp $	*/
d272 2
a273 1
	int zeropages;	/* number of zero'd pages */
d275 4
a278 1
	int reserve_kernel; /* number of pages reserved for kernel */
d285 6
a294 1
	int swpguniq;	/* number of swap pages in use, not also in RAM */
d358 4
a361 1
	
d534 1
a534 1
void			uvmspace_exec __P((struct proc *));
@


1.31
log
@minor sync to NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.30 2001/11/07 02:55:50 art Exp $	*/
d249 1
d252 2
d555 5
d562 1
@


1.30
log
@Another sync of uvm to NetBSD. Just minor fiddling, no major changes.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.29 2001/11/07 01:18:01 art Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.51 2000/09/28 19:05:06 eeh Exp $	*/
d492 1
@


1.29
log
@Add an alignment argument to uvm_map that specifies an alignment hint
for the virtual address.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.28 2001/11/06 18:41:10 art Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.49 2000/09/13 15:00:25 thorpej Exp $	*/
d310 2
d388 1
d432 13
d584 1
a584 1
void		swstrategy __P((struct buf *));
a585 14
/* Machine dependent portion */
void		vmapbuf __P((struct buf *, vsize_t));
void		vunmapbuf __P((struct buf *, vsize_t));
void		pagemove __P((caddr_t, caddr_t, size_t));
void		cpu_fork __P((struct proc *, struct proc *, void *, size_t,
			void (*)(void *), void *));
#ifndef	cpu_swapin
void		cpu_swapin __P((struct proc *));
#endif
#ifndef	cpu_swapout
void		cpu_swapout __P((struct proc *));
#endif

#endif /* _KERNEL */
@


1.28
log
@Let fork1, uvm_fork, and cpu_fork take a function/argument pair as argument,
instead of doing fork1, cpu_set_kpc. This lets us retire cpu_set_kpc and
avoid a multiprocessor race.

This commit breaks vax because it doesn't look like any other arch, someone
working on vax might want to look at this and try to adapt the code to be
more like the rest of the world.

Idea and uvm parts from NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.27 2001/11/06 13:36:52 art Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.48 2000/08/12 22:41:55 thorpej Exp $	*/
d489 2
a490 1
				struct uvm_object *, voff_t, uvm_flag_t));
@


1.27
log
@More sync to NetBSD.
 - Use malloc/free instead of MALLOC/FREE for variable sized allocations.
 - Move the memory inheritance code to sys/mman.h and rename from VM_* to MAP_*
 - various cleanups and simplifications.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.26 2001/11/06 01:35:04 art Exp $	*/
a428 11
/* vm_machdep.c */
void		vmapbuf __P((struct buf *, vsize_t));
void		vunmapbuf __P((struct buf *, vsize_t));
void		pagemove __P((caddr_t, caddr_t, size_t));
#ifndef	cpu_swapin
void		cpu_swapin __P((struct proc *));
#endif
#ifndef	cpu_swapout
void		cpu_swapout __P((struct proc *));
#endif

d446 1
a446 1
			    void *, size_t));
d573 2
a574 1
void		cpu_fork __P((struct proc *, struct proc *, void *, size_t));
@


1.26
log
@Move the last content from vm/ to uvm/
The only thing left in vm/ are just dumb wrappers.
vm/vm.h includes uvm/uvm_extern.h
vm/pmap.h includes uvm/uvm_pmap.h
vm/vm_page.h includes uvm/uvm_page.h
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.25 2001/11/06 00:24:55 art Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.45 2000/06/27 16:16:43 mrg Exp $	*/
a176 11
/*
 *	Enumeration of valid values for vm_inherit_t.
 */

#define	VM_INHERIT_SHARE	((vm_inherit_t)0)	/* share with child */
#define	VM_INHERIT_COPY		((vm_inherit_t)1)	/* copy into child */
#define	VM_INHERIT_NONE		((vm_inherit_t)2)	/* absent from child */
#define	VM_INHERIT_DONATE_COPY	((vm_inherit_t)3)	/* copy and delete */

#define VM_INHERIT_DEFAULT	VM_INHERIT_COPY

d488 2
d546 1
a546 1
void			uvm_pageout __P((void));
@


1.25
log
@sync to NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.24 2001/09/19 20:50:59 mickey Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.43 2000/06/26 14:21:17 mrg Exp $	*/
d38 35
d88 22
d362 38
a409 1
extern struct uvmexp uvmexp;
d421 5
a425 3
/*
 * typedefs 
 */
d427 10
a436 2
typedef unsigned int  uvm_flag_t;
typedef int vm_fault_t;
d439 11
@


1.24
log
@merge vm/vm_kern.h into uvm/uvm_extern.h; art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.23 2001/08/12 22:41:15 mickey Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.42 2000/06/08 05:52:34 thorpej Exp $	*/
@


1.23
log
@merge vm_map.h into uvm_map.h, kinda matches netbsd's approach
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.22 2001/08/12 21:00:14 mickey Exp $	*/
d199 1
d305 9
@


1.22
log
@vm_inherit.h no more
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.21 2001/08/12 20:18:30 mickey Exp $	*/
d198 1
@


1.21
log
@vm_extern.h no more
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.20 2001/08/12 05:18:41 mickey Exp $	*/
d119 11
@


1.20
log
@vm/vm_prot.h no more
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.20 2001/08/12 04:57:19 mickey Exp $	*/
d178 1
d450 14
a464 1

@


1.19
log
@Various random fixes from NetBSD.
Including support for zeroing pages in the idle loop (not enabled yet).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.18 2001/08/08 02:36:59 millert Exp $	*/
d91 29
@


1.18
log
@nuke __attribute__((__noreturn__)) when used in conjunction with __dead
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.17 2001/08/06 14:03:04 art Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.38 2000/03/26 20:54:46 kleink Exp $	*/
d136 2
a137 1
#define UVM_PGA_USERESERVE		0x0001
d180 1
d217 4
a299 2
void			uvm_sleep __P((void *, struct simplelock *, boolean_t,
			    const char *, int));
@


1.17
log
@Add a new type voff_t (right now it's typedefed as off_t) used for offsets
into objects.

Gives the possibilty to mmap beyond the size of vaddr_t.

From NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.16 2001/08/02 11:06:38 art Exp $	*/
d301 1
a301 1
__dead void		uvm_scheduler __P((void)) __attribute__((__noreturn__));
@


1.16
log
@Sysctl for finding out how many pages there are in kmem_map.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.15 2001/07/26 19:37:13 art Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.37 2000/02/11 19:22:54 thorpej Exp $	*/
d116 1
a116 1
#define UVM_UNKNOWN_OFFSET ((vaddr_t) -1)
d338 1
a338 1
				struct uvm_object *, vaddr_t, uvm_flag_t));
d366 1
a366 1
				caddr_t, vaddr_t, vsize_t));
d370 1
a370 1
				vaddr_t, struct vm_anon *, int, int, int));
d377 1
a377 1
					     struct uvm_object *, vaddr_t));
d404 1
a404 1
void			uvm_vnp_setsize __P((struct vnode *, u_quad_t));
@


1.15
log
@Add support for disabling swap devices (swapctl -d).
Improve error handling on I/O errors to swap.
From NetBSD
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.14 2001/07/25 14:47:59 art Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.36 2000/01/11 06:57:49 chs Exp $	*/
d410 5
@


1.14
log
@Some updates to UVM from NetBSD. Nothing really critical, just a sync.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.13 2001/06/23 19:24:33 smart Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.35 1999/12/30 16:09:47 eeh Exp $	*/
d281 1
d283 1
@


1.13
log
@Sync with NetBSD 19990911 (just before PMAP_NEW was required)
  - thread_sleep_msg() -> uvm_sleep()
  - initialize reference count lock in uvm_anon_{init,add}()
  - add uao_flush()
  - replace boolean 'islocked' with 'lockflags'
  - in uvm_fault() change FALSE to TRUE to in 'wide' fault handling
  - get rid of uvm_km_get()
  - various bug fixes
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.12 2001/05/10 07:59:06 art Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.34 1999/07/22 22:58:38 thorpej Exp $	*/
d338 1
a338 1
				vaddr_t, boolean_t, boolean_t));
d377 2
a378 2
void			uvm_page_physload __P((vaddr_t, vaddr_t,
					       vaddr_t, vaddr_t, int));
@


1.12
log
@Some locking protocol fixes and better enforcement of wiring limits.

From NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.11 2001/05/07 16:08:40 art Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.32 1999/07/02 23:20:58 thorpej Exp $	*/
d139 6
d159 1
d292 2
d372 1
a372 1
vaddr_t			uvm_pagealloc_contig __P((vaddr_t, vaddr_t, 
@


1.11
log
@Few fixes from NetBSD.
 - make sure that vsunlock doesn't unwire mlocked memory.
 - fix locking in uvm_useracc.
 - Return the error uvm_fault_wire in uvm_vslock (will be used soon).
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.10 2001/03/09 14:20:50 art Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.29 1999/06/17 15:47:22 thorpej Exp $	*/
d184 1
d189 1
d329 1
a329 1
				vaddr_t, boolean_t));
d355 1
a355 1
				caddr_t, vaddr_t));
@


1.10
log
@More syncing to NetBSD.

Implements mincore(2), mlockall(2) and munlockall(2). mlockall and munlockall
are disabled for the moment.

The rest is mostly cosmetic.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.9 2001/03/09 05:34:38 smart Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.28 1999/06/15 23:27:47 thorpej Exp $	*/
d291 1
a291 1
void			uvm_vslock __P((struct proc *, caddr_t, size_t,
@


1.9
log
@Protect protypes, certain macros, and inlines from userland.  Checked userland
with a 'make build'.  From NetBSD.  art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_extern.h,v 1.8 2001/01/29 02:07:43 niklas Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.27 1999/05/26 19:16:36 thorpej Exp $	*/
d328 1
@


1.8
log
@$OpenBSD$
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.27 1999/05/26 19:16:36 thorpej Exp $	*/
d246 1
d258 2
d267 2
d397 2
a399 1

@


1.7
log
@use __x__ formats for __attribute__ arguments; guenther@@gac.edu
@
text
@d1 1
@


1.6
log
@Bring in some new UVM code from NetBSD (not current).

 - Introduce a new type of map that are interrupt safe and never allow faults
   in them. mb_map and kmem_map are made intrsafe.
 - Add "access protection" to uvm_vslock (to be passed down to uvm_fault and
   later to pmap_enter).
 - madvise(2) now works.
 - various cleanups.
@
text
@d282 1
a282 1
__dead void		uvm_scheduler __P((void)) __attribute__((noreturn));
@


1.5
log
@sync with NetBSD from 1999.05.24 (there is a reason for this date)
 Mostly cleanups, but also a few improvements to pagedaemon for better
 handling of low memory and/or low swap conditions.
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_extern.h,v 1.24 1999/04/11 04:04:11 chs Exp $	*/
d285 2
a286 1
void			uvm_vslock __P((struct proc *, caddr_t, size_t));
d305 1
a305 1
				vaddr_t *, vsize_t, boolean_t,
@


1.5.4.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_extern.h,v 1.27 1999/05/26 19:16:36 thorpej Exp $	*/
d285 1
a285 2
void			uvm_vslock __P((struct proc *, caddr_t, size_t,
			    vm_prot_t));
d304 1
a304 1
				vaddr_t *, vsize_t, int,
@


1.5.4.2
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_extern.h,v 1.10 2001/03/09 14:20:50 art Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.28 1999/06/15 23:27:47 thorpej Exp $	*/
a244 1
#ifdef _KERNEL
a255 2
#endif /* _KERNEL */

a262 2
#ifdef _KERNEL

d282 1
a282 1
__dead void		uvm_scheduler __P((void)) __attribute__((__noreturn__));
a321 1
int			uvm_map_pageable_all __P((vm_map_t, int, vsize_t));
d391 1
a391 1
#endif /* _KERNEL */
a392 1
#endif /* _UVM_UVM_EXTERN_H_ */
@


1.5.4.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_extern.h,v 1.34 1999/07/22 22:58:38 thorpej Exp $	*/
a138 6
 * lockflags that control the locking behavior of various functions.
 */
#define	UVM_LK_ENTER	0x00000001	/* map locked on entry */
#define	UVM_LK_EXIT	0x00000002	/* leave map locked on exit */

/*
a152 1
struct simplelock;
a183 1
	int swpguniq;	/* number of swap pages in use, not also in RAM */
a187 1
	int nanonneeded;/* number of anons currently needed */
a282 2
void			uvm_sleep __P((void *, struct simplelock *, boolean_t,
			    const char *, int));
d291 1
a291 1
int			uvm_vslock __P((struct proc *, caddr_t, size_t,
d327 1
a327 1
				vaddr_t, boolean_t, boolean_t));
d353 1
a353 1
				caddr_t, vaddr_t, vsize_t));
d361 1
a361 1
vaddr_t			uvm_pagealloc_contig __P((vaddr_t, vaddr_t,
@


1.5.4.4
log
@Sync the SMP branch to something just after 3.0
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_extern.h,v 1.42 2000/06/08 05:52:34 thorpej Exp $	*/
a90 40

typedef int		vm_prot_t;

/*
 *	Protection values, defined as bits within the vm_prot_t type
 *
 *   These are funky definitions from old CMU VM and are kept
 *   for compatibility reasons, one day they are going to die,
 *   just like everybody else.
 */

#define	VM_PROT_NONE	((vm_prot_t) 0x00)

#define VM_PROT_READ	((vm_prot_t) 0x01)	/* read permission */
#define VM_PROT_WRITE	((vm_prot_t) 0x02)	/* write permission */
#define VM_PROT_EXECUTE	((vm_prot_t) 0x04)	/* execute permission */

/*
 *	The default protection for newly-created virtual memory
 */

#define VM_PROT_DEFAULT	(VM_PROT_READ|VM_PROT_WRITE|VM_PROT_EXECUTE)

/*
 *	The maximum privileges possible, for parameter checking.
 */

#define VM_PROT_ALL	(VM_PROT_READ|VM_PROT_WRITE|VM_PROT_EXECUTE)

/*
 *	Enumeration of valid values for vm_inherit_t.
 */

#define	VM_INHERIT_SHARE	((vm_inherit_t)0)	/* share with child */
#define	VM_INHERIT_COPY		((vm_inherit_t)1)	/* copy into child */
#define	VM_INHERIT_NONE		((vm_inherit_t)2)	/* absent from child */
#define	VM_INHERIT_DONATE_COPY	((vm_inherit_t)3)	/* copy and delete */

#define VM_INHERIT_DEFAULT	VM_INHERIT_COPY

d116 1
a116 1
#define UVM_UNKNOWN_OFFSET ((voff_t) -1)
d136 1
a136 2
#define UVM_PGA_USERESERVE	0x0001	/* ok to use reserve pages */
#define	UVM_PGA_ZERO		0x0002	/* returned page must be zero'd */
a147 1
struct buf;
a155 2
struct vm_aref;
struct vm_map;
a178 1
	int zeropages;	/* number of zero'd pages */
a214 4
	int pga_zerohit;	/* pagealloc where zero wanted and zero
				   was available */
	int pga_zeromiss;	/* pagealloc where zero wanted and zero
				   not available */
a256 9
/*
 * the various kernel maps, owned by MD code
 */
extern struct vm_map *exec_map;
extern struct vm_map *kernel_map;
extern struct vm_map *kmem_map;
extern struct vm_map *mb_map;
extern struct vm_map *phys_map;

a280 1
void			uao_detach_locked __P((struct uvm_object *));
a281 1
void			uao_reference_locked __P((struct uvm_object *));
d292 2
d299 1
a299 1
__dead void		uvm_scheduler __P((void));
d336 1
a336 1
				struct uvm_object *, voff_t, uvm_flag_t));
d338 1
a338 1
				vaddr_t, boolean_t, int));
d364 1
a364 1
				caddr_t, voff_t, vsize_t));
d368 1
a368 1
				voff_t, struct vm_anon *, int, int, int));
d375 1
a375 1
					     struct uvm_object *, voff_t));
d377 2
a378 2
void			uvm_page_physload __P((paddr_t, paddr_t,
					       paddr_t, paddr_t, int));
d402 1
a402 1
void			uvm_vnp_setsize __P((struct vnode *, voff_t));
d409 1
a409 18
/* kern_malloc.c */
void			kmeminit_nkmempages __P((void));
void			kmeminit __P((void));
extern int		nkmempages;

void		swstrategy __P((struct buf *));

/* Machine dependent portion */
void		vmapbuf __P((struct buf *, vsize_t));
void		vunmapbuf __P((struct buf *, vsize_t));
void		pagemove __P((caddr_t, caddr_t, size_t));
void		cpu_fork __P((struct proc *, struct proc *, void *, size_t));
#ifndef	cpu_swapin
void		cpu_swapin __P((struct proc *));
#endif
#ifndef	cpu_swapout
void		cpu_swapout __P((struct proc *));
#endif
a410 1
#endif /* _KERNEL */
@


1.5.4.5
log
@merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_extern.h,v 1.57 2001/03/09 01:02:12 chs Exp $	*/
a37 35
/*-
 * Copyright (c) 1991, 1992, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)vm_extern.h	8.5 (Berkeley) 5/3/95
 */

a52 22
 * typedefs, necessary for standard UVM headers.
 */

typedef unsigned int  uvm_flag_t;
typedef int vm_fault_t;

typedef int vm_inherit_t;	/* XXX: inheritance codes */
typedef off_t voff_t;		/* XXX: offset within a uvm_object */

union vm_map_object;
typedef union vm_map_object vm_map_object_t;

struct vm_map_entry;
typedef struct vm_map_entry *vm_map_entry_t;

struct vm_map;
typedef struct vm_map *vm_map_t;

struct vm_page;
typedef struct vm_page  *vm_page_t;

/*
d120 11
a202 1
struct pool;
a204 2
extern struct pool *uvm_aiobuf_pool;

d223 1
a223 2

	int zeropages;		/* number of zero'd pages */
d225 1
a225 4
	int reserve_kernel;	/* number of pages reserved for kernel */
	int anonpages;		/* number of pages used by anon pagers */
	int vnodepages;		/* number of pages used by vnode page cache */
	int vtextpages;		/* number of pages used by vtext vnodes */
a231 6
	int anonmin;	/* min threshold for anon pages */
	int vtextmin;	/* min threshold for vtext pages */
	int vnodemin;	/* min threshold for vnode pages */
	int anonminpct;	/* min percent anon pages */
	int vtextminpct;/* min percent vtext pages */
	int vnodeminpct;/* min percent vnode pages */
d236 1
a263 2
	int zeroaborts;		/* number of times page zeroing was
				   aborted */
d298 1
a298 4
	int pdreanon;	/* anon pages reactivated due to min threshold */
	int pdrevnode;	/* vnode pages reactivated due to min threshold */
	int pdrevtext;	/* vtext pages reactivated due to min threshold */

a304 39
extern struct uvmexp uvmexp;
#endif

/*
 * Finally, bring in standard UVM headers.
 */
#include <sys/vmmeter.h>
#include <sys/queue.h>
#include <uvm/uvm_param.h>
#include <sys/lock.h>
#include <uvm/uvm_page.h>
#include <uvm/uvm_pmap.h>
#include <uvm/uvm_map.h>
#include <uvm/uvm_fault.h>
#include <uvm/uvm_pager.h>

/*
 * Shareable process virtual address space.
 * May eventually be merged with vm_map.
 * Several fields are temporary (text, data stuff).
 */
struct vmspace {
	struct	vm_map vm_map;	/* VM address map */
	int	vm_refcnt;	/* number of references */
	caddr_t	vm_shm;		/* SYS5 shared memory private data XXX */
/* we copy from vm_startcopy to the end of the structure on fork */
#define vm_startcopy vm_rssize
	segsz_t vm_rssize; 	/* current resident set size in pages */
	segsz_t vm_swrss;	/* resident set size before last swap */
	segsz_t vm_tsize;	/* text size (pages) XXX */
	segsz_t vm_dsize;	/* data size (pages) XXX */
	segsz_t vm_ssize;	/* stack size (pages) */
	caddr_t	vm_taddr;	/* user virtual address of text XXX */
	caddr_t	vm_daddr;	/* user virtual address of data XXX */
	caddr_t vm_maxsaddr;	/* user VA at max stack growth */
	caddr_t vm_minsaddr;	/* user VA at top of stack */
};

#ifdef _KERNEL
d315 1
d327 3
a329 5
#ifdef	pmap_resident_count
#define vm_resident_count(vm) (pmap_resident_count((vm)->vm_map.pmap))
#else
#define vm_resident_count(vm) ((vm)->vm_rssize)
#endif
d331 2
a332 10
/* XXX clean up later */
struct buf;
struct loadavg;
struct proc;
struct pmap;
struct vmspace;
struct vmtotal;
struct mount;
struct vnode;
struct core;
a335 13
/* vm_machdep.c */
void		vmapbuf __P((struct buf *, vsize_t));
void		vunmapbuf __P((struct buf *, vsize_t));
void		pagemove __P((caddr_t, caddr_t, size_t));
#ifndef	cpu_swapin
void		cpu_swapin __P((struct proc *));
#endif
#ifndef	cpu_swapout
void		cpu_swapout __P((struct proc *));
#endif
void		cpu_fork __P((struct proc *, struct proc *, void *, size_t,
			void (*)(void *), void *));

d353 1
a353 1
			    void *, size_t, void (*)(void *), void *));
a382 1
vaddr_t			uvm_km_valloc_align __P((vm_map_t, vsize_t, vsize_t));
a383 2
vaddr_t			uvm_km_valloc_prefer_wait __P((vm_map_t, vsize_t,
					voff_t));
d394 1
a394 2
				struct uvm_object *, voff_t, vsize_t,
				uvm_flag_t));
d406 1
a406 1
void			uvmspace_exec __P((struct proc *, vaddr_t, vaddr_t));
a438 5
/* uvm_pager.c */
void			uvm_aio_biodone1 __P((struct buf *));
void			uvm_aio_biodone __P((struct buf *));
void			uvm_aio_aiodone __P((struct buf *));

d440 1
a440 2
void			uvm_pageout __P((void *));
void			uvm_aiodone_daemon __P((void *));
d472 14
a486 1

@


1.5.4.6
log
@Merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_extern.h,v 1.65 2001/06/02 18:09:26 chs Exp $	*/
d91 1
a91 1
typedef unsigned int uvm_flag_t;
d97 12
a225 15
 * the following defines are for ubc_alloc's flags
 */
#define UBC_READ	0
#define UBC_WRITE	1

/*
 * flags for uvn_findpages().
 */
#define UFP_ALL		0x0
#define UFP_NOWAIT	0x1
#define UFP_NOALLOC	0x2
#define UFP_NOCACHE	0x4
#define UFP_NORDONLY	0x8

/*
a250 3
struct vm_map_entry;
struct vm_map;
struct vm_page;
a272 3
	int ncolors;	/* number of page color buckets: must be p-o-2 */
	int colormask;	/* color bucket mask */

a323 2
	int colorhit;		/* pagealloc where we got optimal color */
	int colormiss;		/* pagealloc where we didn't */
d395 1
a395 1
	segsz_t vm_rssize;	/* current resident set size in pages */
d417 1
d428 1
d430 3
a466 7
/* uvm_bio.c */
void			ubc_init __P((void));
void *			ubc_alloc __P((struct uvm_object *, voff_t, vsize_t *,
				       int));
void			ubc_release __P((void *, vsize_t));
void			ubc_flush __P((struct uvm_object *, voff_t, voff_t));

d468 2
a469 2
int			uvm_fault __P((struct vm_map *, vaddr_t, vm_fault_t,
				       vm_prot_t));
d490 1
a490 1
void			uvm_init __P((void));
d494 1
a494 1
int			uvm_io __P((struct vm_map *, struct uio *));
d497 21
a517 24
vaddr_t			uvm_km_alloc1 __P((struct vm_map *, vsize_t,
			    boolean_t));
void			uvm_km_free __P((struct vm_map *, vaddr_t, vsize_t));
void			uvm_km_free_wakeup __P((struct vm_map *, vaddr_t,
			    vsize_t));
vaddr_t			uvm_km_kmemalloc __P((struct vm_map *, struct
			    uvm_object *, vsize_t, int));
struct vm_map		*uvm_km_suballoc __P((struct vm_map *, vaddr_t *,
			    vaddr_t *, vsize_t, int, boolean_t,
			    struct vm_map *));
vaddr_t			uvm_km_valloc __P((struct vm_map *, vsize_t));
vaddr_t			uvm_km_valloc_align __P((struct vm_map *, vsize_t,
			    vsize_t));
vaddr_t			uvm_km_valloc_wait __P((struct vm_map *, vsize_t));
vaddr_t			uvm_km_valloc_prefer_wait __P((struct vm_map *, vsize_t,
			    voff_t));
vaddr_t			uvm_km_alloc_poolpage1 __P((struct vm_map *,
			    struct uvm_object *, boolean_t));
void			uvm_km_free_poolpage1 __P((struct vm_map *, vaddr_t));

#define	uvm_km_alloc_poolpage(waitok) \
	uvm_km_alloc_poolpage1(kmem_map, uvmexp.kmem_object, (waitok))
#define	uvm_km_free_poolpage(addr) \
	uvm_km_free_poolpage1(kmem_map, (addr))
d520 1
a520 1
int			uvm_map __P((struct vm_map *, vaddr_t *, vsize_t,
d523 1
a523 1
int			uvm_map_pageable __P((struct vm_map *, vaddr_t,
d525 2
a526 3
int			uvm_map_pageable_all __P((struct vm_map *, int,
				vsize_t));
boolean_t		uvm_map_checkprot __P((struct vm_map *, vaddr_t,
d528 1
a528 1
int			uvm_map_protect __P((struct vm_map *, vaddr_t,
d543 1
a543 1
int			uvm_sysctl __P((int *, u_int, void *, size_t *,
d548 3
a550 3
int			uvm_mmap __P((struct vm_map *, vaddr_t *, vsize_t,
				vm_prot_t, vm_prot_t, int,
				void *, voff_t, vsize_t));
d558 3
a560 1
void			uvm_pagerealloc __P((struct vm_page *,
d579 1
a579 1
				struct pglist *, int, int));
d586 1
a586 1
int			uvm_coredump __P((struct proc *, struct vnode *,
d591 1
a591 1
void			uvm_deallocate __P((struct vm_map *, vaddr_t, vsize_t));
d596 3
a599 4
void			uvn_findpages __P((struct uvm_object *, voff_t,
					   int *, struct vm_page **, int));
void			uvm_vnp_zerorange __P((struct vnode *, off_t, size_t));
void			uvm_vnp_asyncget __P((struct vnode *, off_t, size_t));
@


1.5.4.7
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_extern.h,v 1.57 2001/03/09 01:02:12 chs Exp $	*/
d91 1
a91 1
typedef unsigned int  uvm_flag_t;
a96 12
union vm_map_object;
typedef union vm_map_object vm_map_object_t;

struct vm_map_entry;
typedef struct vm_map_entry *vm_map_entry_t;

struct vm_map;
typedef struct vm_map *vm_map_t;

struct vm_page;
typedef struct vm_page  *vm_page_t;

d214 15
d254 3
d279 3
d333 2
a386 1
#include <sys/tree.h>
d406 1
a406 1
	segsz_t vm_rssize; 	/* current resident set size in pages */
a427 1

a437 1
#ifdef	pmap_resident_count
a438 3
#else
#define vm_resident_count(vm) ((vm)->vm_rssize)
#endif
d473 7
d481 2
a482 2
int			uvm_fault __P((vm_map_t, vaddr_t, 
				vm_fault_t, vm_prot_t));
d503 1
a503 1
void			uvm_init __P((void));	
d507 1
a507 1
int			uvm_io __P((vm_map_t, struct uio *));
d510 24
a533 21
vaddr_t			uvm_km_alloc1 __P((vm_map_t, vsize_t, boolean_t));
void			uvm_km_free __P((vm_map_t, vaddr_t, vsize_t));
void			uvm_km_free_wakeup __P((vm_map_t, vaddr_t,
						vsize_t));
vaddr_t			uvm_km_kmemalloc __P((vm_map_t, struct uvm_object *,
						vsize_t, int));
struct vm_map		*uvm_km_suballoc __P((vm_map_t, vaddr_t *,
				vaddr_t *, vsize_t, int,
				boolean_t, vm_map_t));
vaddr_t			uvm_km_valloc __P((vm_map_t, vsize_t));
vaddr_t			uvm_km_valloc_align __P((vm_map_t, vsize_t, vsize_t));
vaddr_t			uvm_km_valloc_wait __P((vm_map_t, vsize_t));
vaddr_t			uvm_km_valloc_prefer_wait __P((vm_map_t, vsize_t,
					voff_t));
vaddr_t			uvm_km_alloc_poolpage1 __P((vm_map_t,
				struct uvm_object *, boolean_t));
void			uvm_km_free_poolpage1 __P((vm_map_t, vaddr_t));

#define	uvm_km_alloc_poolpage(waitok)	uvm_km_alloc_poolpage1(kmem_map, \
						uvmexp.kmem_object, (waitok))
#define	uvm_km_free_poolpage(addr)	uvm_km_free_poolpage1(kmem_map, (addr))
d536 1
a536 1
int			uvm_map __P((vm_map_t, vaddr_t *, vsize_t,
d539 1
a539 1
int			uvm_map_pageable __P((vm_map_t, vaddr_t, 
d541 3
a543 2
int			uvm_map_pageable_all __P((vm_map_t, int, vsize_t));
boolean_t		uvm_map_checkprot __P((vm_map_t, vaddr_t,
d545 1
a545 1
int			uvm_map_protect __P((vm_map_t, vaddr_t, 
d560 1
a560 1
int			uvm_sysctl __P((int *, u_int, void *, size_t *, 
d565 3
a567 3
int			uvm_mmap __P((vm_map_t, vaddr_t *, vsize_t,
				vm_prot_t, vm_prot_t, int, 
				caddr_t, voff_t, vsize_t));
d575 1
a575 3
vaddr_t			uvm_pagealloc_contig __P((vaddr_t, vaddr_t,
				vaddr_t, vaddr_t));
void			uvm_pagerealloc __P((struct vm_page *, 
d594 1
a594 1
				struct pglist *, int, int)); 
d601 1
a601 1
int			uvm_coredump __P((struct proc *, struct vnode *, 
d606 1
a606 1
int			uvm_deallocate __P((vm_map_t, vaddr_t, vsize_t));
a610 3
void 			uvm_vnp_terminate __P((struct vnode *));
				/* terminate a uvm/uvn object */
boolean_t		uvm_vnp_uncache __P((struct vnode *));
d612 4
@


1.5.4.8
log
@Merge in -current from roughly a week ago
@
text
@d449 3
a451 3
void		vmapbuf(struct buf *, vsize_t);
void		vunmapbuf(struct buf *, vsize_t);
void		pagemove(caddr_t, caddr_t, size_t);
d453 1
a453 1
void		cpu_swapin(struct proc *);
d456 1
a456 1
void		cpu_swapout(struct proc *);
d458 2
a459 2
void		cpu_fork(struct proc *, struct proc *, void *, size_t,
		    void (*)(void *), void *);
d462 5
a466 5
struct uvm_object	*uao_create(vsize_t, int);
void			uao_detach(struct uvm_object *);
void			uao_detach_locked(struct uvm_object *);
void			uao_reference(struct uvm_object *);
void			uao_reference_locked(struct uvm_object *);
d469 2
a470 2
int			uvm_fault(vm_map_t, vaddr_t, 
				vm_fault_t, vm_prot_t);
d475 1
a475 1
void			uvm_chgkprot(caddr_t, size_t, int);
d477 11
a487 11
void			uvm_fork(struct proc *, struct proc *, boolean_t,
			    void *, size_t, void (*)(void *), void *);
void			uvm_exit(struct proc *);
void			uvm_init_limits(struct proc *);
boolean_t		uvm_kernacc(caddr_t, size_t, int);
__dead void		uvm_scheduler(void);
void			uvm_swapin(struct proc *);
boolean_t		uvm_useracc(caddr_t, size_t, int);
int			uvm_vslock(struct proc *, caddr_t, size_t,
			    vm_prot_t);
void			uvm_vsunlock(struct proc *, caddr_t, size_t);
d491 1
a491 1
void			uvm_init(void);	
d495 1
a495 1
int			uvm_io(vm_map_t, struct uio *);
d498 7
a504 7
vaddr_t			uvm_km_alloc1(vm_map_t, vsize_t, boolean_t);
void			uvm_km_free(vm_map_t, vaddr_t, vsize_t);
void			uvm_km_free_wakeup(vm_map_t, vaddr_t,
						vsize_t);
vaddr_t			uvm_km_kmemalloc(vm_map_t, struct uvm_object *,
						vsize_t, int);
struct vm_map		*uvm_km_suballoc(vm_map_t, vaddr_t *,
d506 9
a514 9
				boolean_t, vm_map_t);
vaddr_t			uvm_km_valloc(vm_map_t, vsize_t);
vaddr_t			uvm_km_valloc_align(vm_map_t, vsize_t, vsize_t);
vaddr_t			uvm_km_valloc_wait(vm_map_t, vsize_t);
vaddr_t			uvm_km_valloc_prefer_wait(vm_map_t, vsize_t,
					voff_t);
vaddr_t			uvm_km_alloc_poolpage1(vm_map_t,
				struct uvm_object *, boolean_t);
void			uvm_km_free_poolpage1(vm_map_t, vaddr_t);
d521 1
a521 1
int			uvm_map(vm_map_t, vaddr_t *, vsize_t,
d523 17
a539 17
				uvm_flag_t);
int			uvm_map_pageable(vm_map_t, vaddr_t, 
				vaddr_t, boolean_t, int);
int			uvm_map_pageable_all(vm_map_t, int, vsize_t);
boolean_t		uvm_map_checkprot(vm_map_t, vaddr_t,
				vaddr_t, vm_prot_t);
int			uvm_map_protect(vm_map_t, vaddr_t, 
				vaddr_t, vm_prot_t, boolean_t);
struct vmspace		*uvmspace_alloc(vaddr_t, vaddr_t,
				boolean_t);
void			uvmspace_init(struct vmspace *, struct pmap *,
				vaddr_t, vaddr_t, boolean_t);
void			uvmspace_exec(struct proc *, vaddr_t, vaddr_t);
struct vmspace		*uvmspace_fork(struct vmspace *);
void			uvmspace_free(struct vmspace *);
void			uvmspace_share(struct proc *, struct proc *);
void			uvmspace_unshare(struct proc *);
d543 4
a546 4
void			uvm_meter(void);
int			uvm_sysctl(int *, u_int, void *, size_t *, 
				void *, size_t, struct proc *);
void			uvm_total(struct vmtotal *);
d549 1
a549 1
int			uvm_mmap(vm_map_t, vaddr_t *, vsize_t,
d551 1
a551 1
				caddr_t, voff_t, vsize_t);
d554 2
a555 2
struct vm_page		*uvm_pagealloc_strat(struct uvm_object *,
				voff_t, struct vm_anon *, int, int, int);
d559 4
a562 4
vaddr_t			uvm_pagealloc_contig(vaddr_t, vaddr_t,
				vaddr_t, vaddr_t);
void			uvm_pagerealloc(struct vm_page *, 
					     struct uvm_object *, voff_t);
d564 3
a566 3
void			uvm_page_physload(paddr_t, paddr_t,
					       paddr_t, paddr_t, int);
void			uvm_setpagesize(void);
d569 3
a571 3
void			uvm_aio_biodone1(struct buf *);
void			uvm_aio_biodone(struct buf *);
void			uvm_aio_aiodone(struct buf *);
d574 2
a575 2
void			uvm_pageout(void *);
void			uvm_aiodone_daemon(void *);
d578 1
a578 1
int			uvm_pglistalloc(psize_t, paddr_t,
d580 2
a581 2
				struct pglist *, int, int); 
void			uvm_pglistfree(struct pglist *);
d584 1
a584 1
void			uvm_swap_init(void);
d587 3
a589 3
int			uvm_coredump(struct proc *, struct vnode *, 
				struct ucred *, struct core *);
int			uvm_grow(struct proc *, vaddr_t);
d592 1
a592 1
int			uvm_deallocate(vm_map_t, vaddr_t, vsize_t);
d595 3
a597 3
void			uvm_vnp_setsize(struct vnode *, voff_t);
void			uvm_vnp_sync(struct mount *);
void 			uvm_vnp_terminate(struct vnode *);
d599 2
a600 2
boolean_t		uvm_vnp_uncache(struct vnode *);
struct uvm_object	*uvn_attach(void *, vm_prot_t);
d603 2
a604 2
void			kmeminit_nkmempages(void);
void			kmeminit(void);
@


1.5.4.9
log
@Sync the SMP branch with 3.3
@
text
@d592 1
a592 1
void			uvm_deallocate(vm_map_t, vaddr_t, vsize_t);
@


1.5.4.10
log
@Sync the SMP branch to -current. This includes moving to ELF.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.5.4.9 2003/03/28 00:08:48 niklas Exp $	*/
d223 1
a223 1
#define	UVM_PGA_ZERO		0x0002	/* returned page must be zeroed */
d350 1
a350 1
	int pdscans;	/* number of pages daemon scanned since boot */
@


1.5.4.11
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.5.4.10 2003/05/13 19:36:57 ho Exp $	*/
d50 5
a54 1
 * 3. Neither the name of the University nor the names of its contributors
@


1.5.4.12
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d480 1
@


1.5.4.13
log
@Merge with the trunk
@
text
@d360 1
a513 2
void			*uvm_km_getpage(boolean_t);
void			uvm_km_putpage(void *);
@


1.4
log
@New cpu_fork API to take a stack in which you point the child's stackpointer
to, at the bottom or the top, depending on your architecture's stack growth
direction.  This is in preparation for Linux' clone(2) emulation.
port maintainers, please check that I did the work right.
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_extern.h,v 1.3 1999/07/23 14:47:06 ho Exp $	*/
/*	$NetBSD: uvm_extern.h,v 1.21 1998/09/08 23:44:21 thorpej Exp $	*/
a3 4
 * XXXCDC: "ROUGH DRAFT" QUALITY UVM PRE-RELEASE FILE!   
 *	   >>>USE AT YOUR OWN RISK, WORK IS NOT FINISHED<<<
 */
/*
a120 1

d133 5
d184 1
d278 1
a278 1
				void *, size_t));
d349 4
a352 3
				vaddr_t, struct vm_anon *, int, int));
#define	uvm_pagealloc(obj, off, anon) \
	    uvm_pagealloc_strat((obj), (off), (anon), UVM_PGA_STRAT_NORMAL, 0)
@


1.3
log
@Add uvm_pagealloc_contig
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_extern.h,v 1.2 1999/02/26 05:32:06 art Exp $	*/
d277 2
a278 1
void			uvm_fork __P((struct proc *, struct proc *, boolean_t));
@


1.2
log
@add OpenBSD tags
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d351 2
@


1.1
log
@Import of uvm from NetBSD. Some local changes, some code disabled
@
text
@d1 1
@

