head	1.63;
access;
symbols
	OPENBSD_6_1_BASE:1.63
	OPENBSD_6_0:1.61.0.2
	OPENBSD_6_0_BASE:1.61
	OPENBSD_5_9:1.60.0.2
	OPENBSD_5_9_BASE:1.60
	OPENBSD_5_8:1.58.0.4
	OPENBSD_5_8_BASE:1.58
	OPENBSD_5_7:1.57.0.2
	OPENBSD_5_7_BASE:1.57
	OPENBSD_5_6:1.54.0.4
	OPENBSD_5_6_BASE:1.54
	OPENBSD_5_5:1.52.0.4
	OPENBSD_5_5_BASE:1.52
	OPENBSD_5_4:1.50.0.2
	OPENBSD_5_4_BASE:1.50
	OPENBSD_5_3:1.48.0.8
	OPENBSD_5_3_BASE:1.48
	OPENBSD_5_2:1.48.0.6
	OPENBSD_5_2_BASE:1.48
	OPENBSD_5_1_BASE:1.48
	OPENBSD_5_1:1.48.0.4
	OPENBSD_5_0:1.48.0.2
	OPENBSD_5_0_BASE:1.48
	OPENBSD_4_9:1.44.0.4
	OPENBSD_4_9_BASE:1.44
	OPENBSD_4_8:1.44.0.2
	OPENBSD_4_8_BASE:1.44
	OPENBSD_4_7:1.40.0.2
	OPENBSD_4_7_BASE:1.40
	OPENBSD_4_6:1.39.0.4
	OPENBSD_4_6_BASE:1.39
	OPENBSD_4_5:1.27.0.2
	OPENBSD_4_5_BASE:1.27
	OPENBSD_4_4:1.26.0.4
	OPENBSD_4_4_BASE:1.26
	OPENBSD_4_3:1.26.0.2
	OPENBSD_4_3_BASE:1.26
	OPENBSD_4_2:1.25.0.2
	OPENBSD_4_2_BASE:1.25
	OPENBSD_4_1:1.22.0.4
	OPENBSD_4_1_BASE:1.22
	OPENBSD_4_0:1.22.0.2
	OPENBSD_4_0_BASE:1.22
	OPENBSD_3_9:1.21.0.10
	OPENBSD_3_9_BASE:1.21
	OPENBSD_3_8:1.21.0.8
	OPENBSD_3_8_BASE:1.21
	OPENBSD_3_7:1.21.0.6
	OPENBSD_3_7_BASE:1.21
	OPENBSD_3_6:1.21.0.4
	OPENBSD_3_6_BASE:1.21
	SMP_SYNC_A:1.21
	SMP_SYNC_B:1.21
	OPENBSD_3_5:1.21.0.2
	OPENBSD_3_5_BASE:1.21
	OPENBSD_3_4:1.20.0.6
	OPENBSD_3_4_BASE:1.20
	UBC_SYNC_A:1.20
	OPENBSD_3_3:1.20.0.4
	OPENBSD_3_3_BASE:1.20
	OPENBSD_3_2:1.20.0.2
	OPENBSD_3_2_BASE:1.20
	OPENBSD_3_1:1.18.0.2
	OPENBSD_3_1_BASE:1.18
	UBC_SYNC_B:1.20
	UBC:1.16.0.2
	UBC_BASE:1.16
	OPENBSD_3_0:1.9.0.2
	OPENBSD_3_0_BASE:1.9
	OPENBSD_2_9_BASE:1.7
	OPENBSD_2_9:1.7.0.2
	OPENBSD_2_8:1.4.0.4
	OPENBSD_2_8_BASE:1.4
	OPENBSD_2_7:1.4.0.2
	OPENBSD_2_7_BASE:1.4
	SMP:1.3.0.4
	SMP_BASE:1.3
	kame_19991208:1.3
	OPENBSD_2_6:1.3.0.2
	OPENBSD_2_6_BASE:1.3
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.63
date	2016.11.07.00.26.33;	author guenther;	state Exp;
branches;
next	1.62;
commitid	W7ztnDZwvjCaeQTS;

1.62
date	2016.09.16.02.35.42;	author dlg;	state Exp;
branches;
next	1.61;
commitid	Fei4687v68qad1tP;

1.61
date	2016.03.09.16.45.43;	author deraadt;	state Exp;
branches;
next	1.60;
commitid	RjqAE3tSh8ngurus;

1.60
date	2015.10.08.15.58.38;	author kettenis;	state Exp;
branches;
next	1.59;
commitid	wmnQpRj0rGJ31TDS;

1.59
date	2015.08.21.16.04.35;	author visa;	state Exp;
branches;
next	1.58;
commitid	gglpDr80UKmkkP9A;

1.58
date	2015.04.22.03.48.52;	author dlg;	state Exp;
branches;
next	1.57;
commitid	JZgBzixlVpkNz9RB;

1.57
date	2015.02.07.01.46.27;	author kettenis;	state Exp;
branches;
next	1.56;
commitid	7b6QyxnUYRNhvNHU;

1.56
date	2015.02.05.23.51.06;	author mpi;	state Exp;
branches;
next	1.55;
commitid	ofjqNcubSq8nv7rW;

1.55
date	2014.10.03.18.06.47;	author kettenis;	state Exp;
branches;
next	1.54;
commitid	3JKupIsvM8vNecnK;

1.54
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.53;
commitid	7NtJNW9udCOFtDNM;

1.53
date	2014.03.21.21.39.36;	author miod;	state Exp;
branches;
next	1.52;

1.52
date	2014.01.23.22.06.30;	author miod;	state Exp;
branches;
next	1.51;

1.51
date	2014.01.01.22.15.18;	author miod;	state Exp;
branches;
next	1.50;

1.50
date	2013.05.30.16.29.46;	author tedu;	state Exp;
branches;
next	1.49;

1.49
date	2013.05.30.15.17.59;	author tedu;	state Exp;
branches;
next	1.48;

1.48
date	2011.05.30.22.25.24;	author oga;	state Exp;
branches;
next	1.47;

1.47
date	2011.05.10.21.38.04;	author oga;	state Exp;
branches;
next	1.46;

1.46
date	2011.05.07.15.27.01;	author oga;	state Exp;
branches;
next	1.45;

1.45
date	2011.04.02.12.38.37;	author ariane;	state Exp;
branches;
next	1.44;

1.44
date	2010.06.29.21.25.16;	author thib;	state Exp;
branches;
next	1.43;

1.43
date	2010.06.27.03.03.49;	author thib;	state Exp;
branches;
next	1.42;

1.42
date	2010.04.22.19.02.55;	author oga;	state Exp;
branches;
next	1.41;

1.41
date	2010.03.24.00.36.04;	author oga;	state Exp;
branches;
next	1.40;

1.40
date	2009.08.06.15.28.14;	author oga;	state Exp;
branches;
next	1.39;

1.39
date	2009.06.17.00.13.59;	author oga;	state Exp;
branches;
next	1.38;

1.38
date	2009.06.16.16.42.41;	author ariane;	state Exp;
branches;
next	1.37;

1.37
date	2009.06.16.00.11.29;	author oga;	state Exp;
branches;
next	1.36;

1.36
date	2009.06.14.03.04.08;	author deraadt;	state Exp;
branches;
next	1.35;

1.35
date	2009.06.07.02.01.54;	author oga;	state Exp;
branches;
next	1.34;

1.34
date	2009.06.02.23.00.19;	author oga;	state Exp;
branches;
next	1.33;

1.33
date	2009.06.01.17.42.33;	author ariane;	state Exp;
branches;
next	1.32;

1.32
date	2009.04.28.16.06.07;	author miod;	state Exp;
branches;
next	1.31;

1.31
date	2009.04.13.22.17.54;	author oga;	state Exp;
branches;
next	1.30;

1.30
date	2009.04.06.17.03.51;	author oga;	state Exp;
branches;
next	1.29;

1.29
date	2009.03.25.20.00.18;	author oga;	state Exp;
branches;
next	1.28;

1.28
date	2009.03.24.16.29.42;	author oga;	state Exp;
branches;
next	1.27;

1.27
date	2009.01.20.20.20.51;	author ariane;	state Exp;
branches;
next	1.26;

1.26
date	2007.12.18.11.05.52;	author thib;	state Exp;
branches;
next	1.25;

1.25
date	2007.04.18.18.51.37;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2007.04.13.18.57.49;	author art;	state Exp;
branches;
next	1.23;

1.23
date	2007.04.04.17.44.45;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2006.06.16.23.05.23;	author miod;	state Exp;
branches;
next	1.21;

1.21
date	2003.11.08.19.17.28;	author jmc;	state Exp;
branches;
next	1.20;

1.20
date	2002.07.20.22.19.35;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2002.06.11.09.45.16;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2002.03.14.01.27.18;	author millert;	state Exp;
branches;
next	1.17;

1.17
date	2001.12.19.08.58.07;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2001.12.04.23.22.42;	author art;	state Exp;
branches
	1.16.2.1;
next	1.15;

1.15
date	2001.11.30.17.37.43;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2001.11.28.19.28.15;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2001.11.12.01.26.09;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2001.11.10.18.42.31;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2001.11.07.02.55.50;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2001.11.06.01.35.04;	author art;	state Exp;
branches;
next	1.9;

1.9
date	2001.08.11.10.57.22;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2001.08.06.14.03.05;	author art;	state Exp;
branches;
next	1.7;

1.7
date	2001.03.22.03.05.56;	author smart;	state Exp;
branches;
next	1.6;

1.6
date	2001.03.09.05.34.38;	author smart;	state Exp;
branches;
next	1.5;

1.5
date	2001.01.29.02.07.47;	author niklas;	state Exp;
branches;
next	1.4;

1.4
date	2000.03.16.22.11.04;	author art;	state Exp;
branches;
next	1.3;

1.3
date	99.08.23.08.13.24;	author art;	state Exp;
branches
	1.3.4.1;
next	1.2;

1.2
date	99.02.26.05.32.07;	author art;	state Exp;
branches;
next	1.1;

1.1
date	99.02.26.01.30.17;	author art;	state Exp;
branches;
next	;

1.3.4.1
date	2000.03.24.09.09.51;	author niklas;	state Exp;
branches;
next	1.3.4.2;

1.3.4.2
date	2001.05.14.22.47.47;	author niklas;	state Exp;
branches;
next	1.3.4.3;

1.3.4.3
date	2001.10.31.03.32.14;	author nate;	state Exp;
branches;
next	1.3.4.4;

1.3.4.4
date	2001.11.13.23.02.31;	author niklas;	state Exp;
branches;
next	1.3.4.5;

1.3.4.5
date	2001.12.05.01.23.58;	author niklas;	state Exp;
branches;
next	1.3.4.6;

1.3.4.6
date	2002.03.06.02.17.14;	author niklas;	state Exp;
branches;
next	1.3.4.7;

1.3.4.7
date	2002.03.28.14.54.27;	author niklas;	state Exp;
branches;
next	1.3.4.8;

1.3.4.8
date	2003.03.28.00.08.48;	author niklas;	state Exp;
branches;
next	1.3.4.9;

1.3.4.9
date	2004.02.19.11.01.44;	author niklas;	state Exp;
branches;
next	;

1.16.2.1
date	2002.02.02.03.28.27;	author art;	state Exp;
branches;
next	1.16.2.2;

1.16.2.2
date	2002.06.11.03.33.04;	author art;	state Exp;
branches;
next	1.16.2.3;

1.16.2.3
date	2002.10.29.00.36.50;	author art;	state Exp;
branches;
next	;


desc
@@


1.63
log
@Split PID from TID, giving processes a PID unrelated to the TID of their
initial thread

ok jsing@@ kettenis@@
@
text
@/*	$OpenBSD: uvm_page.h,v 1.62 2016/09/16 02:35:42 dlg Exp $	*/
/*	$NetBSD: uvm_page.h,v 1.19 2000/12/28 08:24:55 chs Exp $	*/

/* 
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
 *
 * All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * The Mach Operating System project at Carnegie-Mellon University.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)vm_page.h   7.3 (Berkeley) 4/21/91
 * from: Id: uvm_page.h,v 1.1.2.6 1998/02/04 02:31:42 chuck Exp
 *
 *
 * Copyright (c) 1987, 1990 Carnegie-Mellon University.
 * All rights reserved.
 * 
 * Permission to use, copy, modify and distribute this software and
 * its documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
 * FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
 * 
 * Carnegie Mellon requests users of this software to return to
 *
 *  Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
 *  School of Computer Science
 *  Carnegie Mellon University
 *  Pittsburgh PA 15213-3890
 *
 * any improvements or extensions that they make and grant Carnegie the
 * rights to redistribute these changes.
 */

#ifndef _UVM_UVM_PAGE_H_
#define _UVM_UVM_PAGE_H_

/*
 * uvm_page.h
 */

/*
 *	Resident memory system definitions.
 */

/*
 *	Management of resident (logical) pages.
 *
 *	A small structure is kept for each resident
 *	page, indexed by page number.  Each structure
 *	contains a list used for manipulating pages, and
 *	a tree structure for in object/offset lookups
 *
 *	In addition, the structure contains the object
 *	and offset to which this page belongs (for pageout),
 *	and sundry status bits.
 *
 *	Fields in this structure are possibly locked by the lock on the page
 *	queues (P).
 */

TAILQ_HEAD(pglist, vm_page);

struct vm_page {
	TAILQ_ENTRY(vm_page)	pageq;		/* queue info for FIFO
						 * queue or free list (P) */
	RBT_ENTRY(vm_page)	objt;		/* object tree */

	struct vm_anon		*uanon;		/* anon (P) */
	struct uvm_object	*uobject;	/* object (P) */
	voff_t			offset;		/* offset into object (P) */

	u_int			pg_flags;	/* object flags [P] */

	u_int			pg_version;	/* version count */
	u_int			wire_count;	/* wired down map refs [P] */

	paddr_t			phys_addr;	/* physical address of page */
	psize_t			fpgsz;		/* free page range size */

	struct vm_page_md	mdpage;		/* pmap-specific data */

#if defined(UVM_PAGE_TRKOWN)
	/* debugging fields to track page ownership */
	pid_t			owner;		/* thread that set PG_BUSY */
	char			*owner_tag;	/* why it was set busy */
#endif
};

/*
 * These are the flags defined for vm_page.
 *
 * Note: PG_FILLED and PG_DIRTY are added for the filesystems.
 */

/*
 * locking rules:
 *   PQ_ ==> lock by page queue lock 
 *   PQ_FREE is locked by free queue lock and is mutex with all other PQs
 *   pg_flags may only be changed using the atomic operations.
 *
 * PG_ZERO is used to indicate that a page has been pre-zero'd.  This flag
 * is only set when the page is on no queues, and is cleared when the page
 * is placed on the free list.
 */

#define	PG_BUSY		0x00000001	/* page is locked */
#define	PG_WANTED	0x00000002	/* someone is waiting for page */
#define	PG_TABLED	0x00000004	/* page is in VP table  */
#define	PG_CLEAN	0x00000008	/* page has not been modified */
#define PG_CLEANCHK	0x00000010	/* clean bit has been checked */
#define PG_RELEASED	0x00000020	/* page released while paging */
#define	PG_FAKE		0x00000040	/* page is not yet initialized */
#define PG_RDONLY	0x00000080	/* page must be mapped read-only */
#define PG_ZERO		0x00000100	/* page is pre-zero'd */
#define PG_DEV		0x00000200	/* page is in device space, lay off */

#define PG_PAGER1	0x00001000	/* pager-specific flag */
#define PG_MASK		0x0000ffff

#define PQ_FREE		0x00010000	/* page is on free list */
#define PQ_INACTIVE	0x00020000	/* page is in inactive list */
#define PQ_ACTIVE	0x00040000	/* page is in active list */
#define PQ_ANON		0x00100000	/* page is part of an anon, rather
					   than an uvm_object */
#define PQ_AOBJ		0x00200000	/* page is part of an anonymous
					   uvm_object */
#define PQ_SWAPBACKED	(PQ_ANON|PQ_AOBJ)
#define	PQ_ENCRYPT	0x00400000	/* page needs {en,de}cryption */
#define PQ_MASK		0x00ff0000

#define PG_PMAP0	0x01000000	/* Used by some pmaps. */
#define PG_PMAP1	0x02000000	/* Used by some pmaps. */
#define PG_PMAP2	0x04000000	/* Used by some pmaps. */
#define PG_PMAP3	0x08000000	/* Used by some pmaps. */
#define PG_PMAP4	0x10000000	/* Used by some pmaps. */
#define PG_PMAP5	0x20000000	/* Used by some pmaps. */
#define PG_PMAPMASK	0x3f000000

/*
 * physical memory layout structure
 *
 * MD vmparam.h must #define:
 *   VM_PHYSEG_MAX = max number of physical memory segments we support
 *		   (if this is "1" then we revert to a "contig" case)
 *   VM_PHYSSEG_STRAT: memory sort/search options (for VM_PHYSEG_MAX > 1)
 * 	- VM_PSTRAT_RANDOM:   linear search (random order)
 *	- VM_PSTRAT_BSEARCH:  binary search (sorted by address)
 *	- VM_PSTRAT_BIGFIRST: linear search (sorted by largest segment first)
 *      - others?
 *   XXXCDC: eventually we should purge all left-over global variables...
 */
#define VM_PSTRAT_RANDOM	1
#define VM_PSTRAT_BSEARCH	2
#define VM_PSTRAT_BIGFIRST	3

/*
 * vm_physmemseg: describes one segment of physical memory
 */
struct vm_physseg {
	paddr_t	start;			/* PF# of first page in segment */
	paddr_t	end;			/* (PF# of last page in segment) + 1 */
	paddr_t	avail_start;		/* PF# of first free page in segment */
	paddr_t	avail_end;		/* (PF# of last free page in segment) +1  */
	struct	vm_page *pgs;		/* vm_page structures (from start) */
	struct	vm_page *lastpg;	/* vm_page structure for end */
};

#ifdef _KERNEL

/*
 * globals
 */

extern boolean_t vm_page_zero_enable;

/*
 * physical memory config is stored in vm_physmem.
 */

extern struct vm_physseg vm_physmem[VM_PHYSSEG_MAX];
extern int vm_nphysseg;

/*
 * prototypes: the following prototypes define the interface to pages
 */

void		uvm_page_init(vaddr_t *, vaddr_t *);
#if defined(UVM_PAGE_TRKOWN)
void		uvm_page_own(struct vm_page *, char *);
#endif
#if !defined(PMAP_STEAL_MEMORY)
boolean_t	uvm_page_physget(paddr_t *);
#endif

void		uvm_pageactivate(struct vm_page *);
vaddr_t		uvm_pageboot_alloc(vsize_t);
void		uvm_pagecopy(struct vm_page *, struct vm_page *);
void		uvm_pagedeactivate(struct vm_page *);
void		uvm_pagefree(struct vm_page *);
void		uvm_page_unbusy(struct vm_page **, int);
struct vm_page	*uvm_pagelookup(struct uvm_object *, voff_t);
void		uvm_pageunwire(struct vm_page *);
void		uvm_pagewait(struct vm_page *, int);
void		uvm_pagewake(struct vm_page *);
void		uvm_pagewire(struct vm_page *);
void		uvm_pagezero(struct vm_page *);
void		uvm_pagealloc_pg(struct vm_page *, struct uvm_object *,
		    voff_t, struct vm_anon *);

struct uvm_constraint_range; /* XXX move to uvm_extern.h? */
psize_t		uvm_pagecount(struct uvm_constraint_range*);

#if  VM_PHYSSEG_MAX == 1
/*
 * Inline functions for archs where function calls are expensive.
 */
/*
 * vm_physseg_find: find vm_physseg structure that belongs to a PA
 */
static __inline int
vm_physseg_find(paddr_t pframe, int *offp)
{
	/* 'contig' case */
	if (pframe >= vm_physmem[0].start && pframe < vm_physmem[0].end) {
		if (offp)
			*offp = pframe - vm_physmem[0].start;
		return(0);
	}
	return(-1);
}

/*
 * PHYS_TO_VM_PAGE: find vm_page for a PA.   used by MI code to get vm_pages
 * back from an I/O mapping (ugh!).   used in some MD code as well.
 */
static __inline struct vm_page *
PHYS_TO_VM_PAGE(paddr_t pa)
{
	paddr_t pf = atop(pa);
	int	off;
	int	psi;

	psi = vm_physseg_find(pf, &off);

	return ((psi == -1) ? NULL : &vm_physmem[psi].pgs[off]);
}
#else
/* if VM_PHYSSEG_MAX > 1 they're not inline, they're in uvm_page.c. */
struct vm_page	*PHYS_TO_VM_PAGE(paddr_t);
int		vm_physseg_find(paddr_t, int *);
#endif

/*
 * macros
 */

#define uvm_lock_pageq()	mtx_enter(&uvm.pageqlock)
#define uvm_unlock_pageq()	mtx_leave(&uvm.pageqlock)
#define uvm_lock_fpageq()	mtx_enter(&uvm.fpageqlock)
#define uvm_unlock_fpageq()	mtx_leave(&uvm.fpageqlock)

#define	UVM_PAGEZERO_TARGET	(uvmexp.free / 8)

#define VM_PAGE_TO_PHYS(entry)	((entry)->phys_addr)

#define VM_PAGE_IS_FREE(entry)  ((entry)->pg_flags & PQ_FREE)

#define	PADDR_IS_DMA_REACHABLE(paddr)	\
	(dma_constraint.ucr_low <= paddr && dma_constraint.ucr_high > paddr)

#endif /* _KERNEL */

#endif /* _UVM_UVM_PAGE_H_ */
@


1.62
log
@move the vm_page struct from being stored in RB macro trees to RBT functions

vm_page structs go into three trees, uvm_objtree, uvm_pmr_addr, and
uvm_pmr_size. all these have been moved to RBT code.

this should give us a decent chunk of code space back.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.61 2016/03/09 16:45:43 deraadt Exp $	*/
d115 1
a115 1
	pid_t			owner;		/* proc that set PG_BUSY */
@


1.61
log
@remove vaxisms
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.60 2015/10/08 15:58:38 kettenis Exp $	*/
d97 1
a97 1
	RB_ENTRY(vm_page)	objt;		/* object tree */
@


1.60
log
@Lock the page queues by turning uvm_lock_pageq() and uvm_unlock_pageq() into
mtx_enter() and mtx_leave() operations.  Not 100% this won't blow up but
there is only one way to find out, and we need this to make progress on
further unlocking uvm.

prodded by deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.59 2015/08/21 16:04:35 visa Exp $	*/
d246 1
a246 1
 * Inline functions for archs like the vax where function calls are expensive.
@


1.59
log
@Remove the unused loan_count field and the related uvm logic. Most of
the page loaning code is already in the Attic.

ok kettenis@@, beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.58 2015/04/22 03:48:52 dlg Exp $	*/
d288 2
a289 2
#define uvm_lock_pageq()	/* lock */
#define uvm_unlock_pageq()	/* unlock */
@


1.58
log
@having macros provide semicolons is dangerous.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.57 2015/02/07 01:46:27 kettenis Exp $	*/
a107 3
	u_int			loan_count;	/* number of active loans
						 * to read: [P]
						 * to modify: [P] */
@


1.57
log
@Tedu the old idle page zeroing code.

ok tedu@@, guenther@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.56 2015/02/05 23:51:06 mpi Exp $	*/
d293 2
a294 2
#define uvm_lock_fpageq()	mtx_enter(&uvm.fpageqlock);
#define uvm_unlock_fpageq()	mtx_leave(&uvm.fpageqlock);
@


1.56
log
@Remove some unneeded <uvm/uvm_extern.h> inclusions.

ok deraadt@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.55 2014/10/03 18:06:47 kettenis Exp $	*/
a227 1
void		uvm_pageidlezero(void);
@


1.55
log
@Introduce a thread for zeroing pages without holding the kernel lock.  This
way we can do some useful kernel lock in parallel with other things and create
a reservoir of zeroed pages ready for use elsewhere.  This should reduce
latency.  The thread runs at the absolutel lowest priority such that we don't
keep other kernel threads or userland from doing useful work.

Can be easily disabled by disabling the kthread_create(9) call in main().
Which perhaps we should do for non-MP kernels.

ok deraadt@@, tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.54 2014/07/11 16:35:40 jsg Exp $	*/
a90 2

#include <uvm/uvm_extern.h>
@


1.54
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.53 2014/03/21 21:39:36 miod Exp $	*/
d299 1
a299 1
#define	UVM_PAGEZERO_TARGET	(uvmexp.free)
@


1.53
log
@Allow for two more pmap-specific bits in vm_page pg_flags. Define
PG_PMAPMASK as all the possible pmap-specific bits (similar to the other
PG_fooMASK) to make sure MI code does not need to be updated, the next time
more bits are allocated to greedy pmaps.

No functional change, soon to be used by the (greedy) mips64 pmap.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.52 2014/01/23 22:06:30 miod Exp $	*/
d21 1
a21 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by Charles D. Cranor,
 *      Washington University, the University of California, Berkeley and 
 *      its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.52
log
@unifdef -D__HAVE_VM_PAGE_MD - no functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.51 2014/01/01 22:15:18 miod Exp $	*/
d176 3
@


1.51
log
@Remove __HAVE_PMAP_PHYSSEG support, nothing uses it anymore.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.50 2013/05/30 16:29:46 tedu Exp $	*/
a120 1
#ifdef __HAVE_VM_PAGE_MD
d122 1
a122 1
#endif
@


1.50
log
@remove lots of comments about locking per beck's request
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.49 2013/05/30 15:17:59 tedu Exp $	*/
a204 3
#ifdef __HAVE_PMAP_PHYSSEG
	struct	pmap_physseg pmseg;	/* pmap specific (MD) data */
#endif
@


1.49
log
@remove simple_locks from uvm code. ok beck deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.48 2011/05/30 22:25:24 oga Exp $	*/
d93 2
a94 3
 *	Fields in this structure are locked either by the lock on the
 *	object that the page belongs to (O) or by the lock on the page
 *	queues (P) [or both].
d104 1
a104 1
	RB_ENTRY(vm_page)	objt;		/* object tree (O)*/
d106 3
a108 3
	struct vm_anon		*uanon;		/* anon (O,P) */
	struct uvm_object	*uobject;	/* object (O,P) */
	voff_t			offset;		/* offset into object (O,P) */
d110 1
a110 1
	u_int			pg_flags;	/* object flags [O or P] */
d112 1
a112 1
	u_int			pg_version;	/* version count [O] */
d116 2
a117 2
						 * to read: [O or P]
						 * to modify: [O _and_ P] */
a138 1
 *   PG_ ==> locked by object lock
@


1.48
log
@Remove the freelist member from vm_physseg

The new world order of pmemrange makes this data completely redundant
(being dealt with by the pmemrange constraints instead). Remove all code
that messes with the freelist.

While touching every caller of uvm_page_physload() anyway, add the flags
argument to all callers (all but one is 0 and that one already used
PHYSLOAD_DEVICE) and remove the macro magic to allow callers to continue
without it.

Should shrink the code a bit, as well.

matthew@@ pointed out some mistakes i'd made.
``freelist death, I like. Ok.' ariane@@
`I agree with the general direction, go ahead and i'll fix any fallout
shortly'' miod@@ (68k 88k and vax i could not check would build)
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.47 2011/05/10 21:38:04 oga Exp $	*/
d302 2
a303 2
#define uvm_lock_pageq()	simple_lock(&uvm.pageqlock)
#define uvm_unlock_pageq()	simple_unlock(&uvm.pageqlock)
@


1.47
log
@Kill vm_page_lookup_freelist.

it belongs to a world order that isn't here anymore. More importantly it
has been unused for a fair while now.

ok thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.46 2011/05/07 15:27:01 oga Exp $	*/
a204 1
	int	free_list;		/* which free list they belong on */
@


1.46
log
@So long, uvm_pglist.h

This header defined three thing. two of which are unused throughout the tree,
the final one was the definition of the pagq head type, move that to uvm_page.h
and nuke the header

ok thib@@. Thanks to krw@@ for testing the hppa build for me.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.45 2011/04/02 12:38:37 ariane Exp $	*/
a257 2

int		uvm_page_lookup_freelist(struct vm_page *);
@


1.45
log
@Count the number of physical pages within a memory range.
Bob needs this.

ok art@@ bob@@ thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.44 2010/06/29 21:25:16 thib Exp $	*/
d99 2
a100 1
#include <uvm/uvm_pglist.h>
@


1.44
log
@Add PADDR_IS_DMA_REACHABLE macro so art stops whining
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.43 2010/06/27 03:03:49 thib Exp $	*/
d254 3
@


1.43
log
@uvm constraints. Add two mandatory MD symbols, uvm_md_constraints
which contains the constraints for DMA/memory allocation for each
architecture, and dma_constraints which contains the range of addresses
that are dma accessable by the system.

This is based on ariane@@'s physcontig diff, with lots of bugfixes and
additions the following additions by my self:

Introduce a new function pool_set_constraints() which sets the address
range for which we allocate pages for the pool from, this is now used
for the mbuf/mbuf cluster pools to keep them dma accessible.

The !direct archs no longer stuff pages into the kernel object in
uvm_km_getpage_pla but rather do a pmap_extract() in uvm_km_putpages.

Tested heavily by my self on i386, amd64 and sparc64. Some tests on
alpha and SGI.

"commit it" beck, art, oga, deraadt
"i like the diff" deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.41 2010/03/24 00:36:04 oga Exp $	*/
d311 3
@


1.42
log
@Committing on behalf or ariane@@.

recommit pmemrange:
        physmem allocator: change the view of free memory from single
        free pages to free ranges.  Classify memory based on region with
        associated use-counter (which is used to construct a priority
        list of where to allocate memory).

	Based on code from tedu@@, help from many.

Useable now that bugs have been found and fixed in most architecture's
pmap.c

ok by everyone who has done a pmap or uvm commit in the last year.
@
text
@d252 2
@


1.41
log
@Bring back PHYSLOAD_DEVICE for uvm_page_physload.

ok kettenis@@ beck@@ (tentatively) and ariane@@. deraadt asked for it to be
commited now.

original commit message:

	extend uvm_page_physload to have the ability to add "device" pages to
	the system.

	This is needed in the case where you need managed pages so you can
	handle faulting and pmap_page_protect() on said pages when you manage
	memory in such regions (i'm looking at you, graphics cards).

	these pages are flagged PG_DEV, and shall never be on the freelists,
	assert this. behaviour remains unchanged in the non-device case,
	specifically for all archs currently in the tree we panic if called
	after bootstrap.

	ok art@@ kettenis@@, beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.40 2009/08/06 15:28:14 oga Exp $	*/
d119 1
@


1.40
log
@reintroduce the uvm_tree commit.

Now instead of the global object hashtable, we have a per object tree.

Testing shows no performance difference and a slight code shrink. OTOH when
locking is more fine grained this should be faster due to lock contention on
uvm.hashlock.

ok thib@@, art@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.39 2009/06/17 00:13:59 oga Exp $	*/
d157 1
@


1.39
log
@date based reversion of uvm to the 4th May.

More backouts in line with previous ones, this appears to bring us back to a
stable condition.

A machine forced to 64mb of ram cycled 10GB through swap with this diff
and is still running as I type this. Other tests by ariane@@ and thib@@
also seem to show that it's alright.

ok deraadt@@, thib@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.32 2009/04/28 16:06:07 miod Exp $	*/
d86 2
a87 10
 *	is an element of several lists:
 *
 *		A hash table bucket used to quickly
 *		perform object/offset lookups
 *
 *		A list of all pages for a given object,
 *		so they can be quickly deactivated at
 *		time of deallocation.
 *
 *		An ordered list of pages due for pageout.
d104 1
a104 2
	TAILQ_ENTRY(vm_page)	hashq;		/* hash table links (O)*/
	TAILQ_ENTRY(vm_page)	listq;		/* pages in same object (O)*/
a235 1
void		uvm_page_rehash(void);
a300 3

#define uvm_pagehash(obj,off) \
	(((unsigned long)obj+(unsigned long)atop(off)) & uvm.page_hashmask)
@


1.38
log
@Backout pmemrange (which to most people is more well known as physmem
allocator).

"i can't see any obvious problems" oga
@
text
@@


1.37
log
@Backout all changes to uvm after pmemrange (which will be backed out
separately).

a change at or just before the hackathon has either exposed or added a
very very nasty memory corruption bug that is giving us hell right now.
So in the interest of kernel stability these diffs are being backed out
until such a time as that corruption bug has been found and squashed,
then the ones that are proven good may slowly return.

a quick hitlist of the main commits this backs out:

mine:
uvm_objwire
the lock change in uvm_swap.c
using trees for uvm objects instead of the hash
removing the pgo_releasepg callback.

art@@'s:
putting pmap_page_protect(VM_PROT_NONE) in uvm_pagedeactivate() since
all callers called that just prior anyway.

ok beck@@, ariane@@.

prompted by deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.33 2009/06/01 17:42:33 ariane Exp $	*/
a108 12
union vm_page_fq {
	struct {
		TAILQ_ENTRY(vm_page)	hashq;	/* hash table links (O)*/
		TAILQ_ENTRY(vm_page)	listq;	/* pages in same object (O)*/
	}	queues;

	struct {
		RB_ENTRY(vm_page)	tree;	/* Free chunks, addr/size */
		psize_t			pages;
	}	free;
};

a109 1
	union vm_page_fq	fq;		/* free and queue management */
d112 2
@


1.36
log
@backout:
> extend uvm_page_physload to have the ability to add "device" pages to the
> system.
since it was overlayed over a system that we warned would go "in to be
tested, but may be pulled out".  oga, you just made me spend 20 minutes
of time I should not have had to spend doing this.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.34 2009/06/02 23:00:19 oga Exp $	*/
d111 1
a111 1
		RB_ENTRY(vm_page)	tree;	/* hash table links (O)*/
d125 1
d256 1
@


1.35
log
@extend uvm_page_physload to have the ability to add "device" pages to the
system.

This is needed in the case where you need managed pages so you can
handle faulting and pmap_page_protect() on said pages when you manage
memory in such regions (i'm looking at you, graphics cards).

these pages are flagged PG_DEV, and shall never be on the freelists,
assert this. behaviour remains unchanged in the non-device case,
specifically for all archs currently in the tree we panic if called
after bootstrap.

ok art@@, kettenis@@, ariane@@, beck@@.
@
text
@a175 1
#define PG_DEV		0x00000200	/* page is in device space, lay off */
@


1.34
log
@Instead of the global hash table with the terrible hashfunction and a
global lock, switch the uvm object pages to being kept in a per-object
RB_TREE. Right now this is approximately the same speed, but cleaner.
When biglock usage is reduced this will improve concurrency due to lock
contention..

ok beck@@ art@@. Thanks to jasper for the speed testing.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.33 2009/06/01 17:42:33 ariane Exp $	*/
d176 1
@


1.33
log
@physmem allocator: change the view of free memory from single free pages
to free ranges.
Classify memory based on region with associated use-counter (which is used
to construct a priority list of where to allocate memory).

Based on code from tedu@@, help from many.
Ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.32 2009/04/28 16:06:07 miod Exp $	*/
d111 1
a111 1
		TAILQ_ENTRY(vm_page)	hashq;	/* hash table links (O)*/
a124 1

a254 1
void		uvm_page_rehash(void);
@


1.32
log
@Revert pageqlock back from a mutex to a simple_lock, as it needs to be
recursive in some cases (mostly involving swapping). A proper fix is in
the works, but this will unbreak kernels for now.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.31 2009/04/13 22:17:54 oga Exp $	*/
d109 12
d122 1
a124 2
	TAILQ_ENTRY(vm_page)	hashq;		/* hash table links (O)*/
	TAILQ_ENTRY(vm_page)	listq;		/* pages in same object (O)*/
@


1.31
log
@Convert the page queue lock to a mutex instead of a simplelock.

Fix up the one case of lock recursion (which blatantly ignored the
comment right above it saying that we don't need to lock). The rest of
the lock usage has been checked and appears to be correct.

ok ariane@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.30 2009/04/06 17:03:51 oga Exp $	*/
d307 2
a308 2
#define uvm_lock_pageq()	mtx_enter(&uvm.pageqlock)
#define uvm_unlock_pageq()	mtx_leave(&uvm.pageqlock)
@


1.30
log
@In the case where VM_PHYSSEG_MAX == 1 make vm_physseg_find and
PHYS_TO_VM_PAGE inline again. This should stop function call overhead
killing the vax and other slow archs while keeping the benefit for the
faster platforms.

suggested by miod. ok miod@@, toby@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.29 2009/03/25 20:00:18 oga Exp $	*/
d307 2
a308 2
#define uvm_lock_pageq()	simple_lock(&uvm.pageqlock)
#define uvm_unlock_pageq()	simple_unlock(&uvm.pageqlock)
@


1.29
log
@Move all of the pseudo-inline functions in uvm into C files.

By pseudo-inline, I mean that if a certain macro was defined, they would
be inlined. However, no architecture defines that, and none has for a
very very long time. Therefore mainly this just makes the code a damned
sight easier to read. Some k&r -> ansi declarations while I'm in there.

"just commit it" art@@. ok weingart@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.28 2009/03/24 16:29:42 oga Exp $	*/
d263 36
d301 1
@


1.28
log
@vm_physseg_find and VM_PAGE_TO_PHYS are both called many times in your
average arch port. They are also inline. This does not help, de-inline them.

shaves about 1k on i386 and amd64 bsd.mp. Probably similar amounts of
most architectures.

"no issue" beck@@ "Nuke nuke nuke... make them functions" weingart@@ "this
is good" art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.27 2009/01/20 20:20:51 ariane Exp $	*/
a234 10
 * handle inline options
 */

#ifdef UVM_PAGE_INLINE
#define PAGE_INLINE static __inline
#else 
#define PAGE_INLINE /* nothing */
#endif /* UVM_PAGE_INLINE */

/*
d238 1
a238 1
void uvm_page_init(vaddr_t *, vaddr_t *);
d240 1
a240 1
void uvm_page_own(struct vm_page *, char *);
d243 1
a243 1
boolean_t uvm_page_physget(paddr_t *);
d245 2
a246 5
void uvm_page_rehash(void);
void uvm_pageidlezero(void);

PAGE_INLINE int uvm_lock_fpageq(void);
PAGE_INLINE void uvm_unlock_fpageq(int);
d248 12
a259 12
PAGE_INLINE void uvm_pageactivate(struct vm_page *);
vaddr_t uvm_pageboot_alloc(vsize_t);
PAGE_INLINE void uvm_pagecopy(struct vm_page *, struct vm_page *);
PAGE_INLINE void uvm_pagedeactivate(struct vm_page *);
void uvm_pagefree(struct vm_page *);
void uvm_page_unbusy(struct vm_page **, int);
PAGE_INLINE struct vm_page *uvm_pagelookup(struct uvm_object *, voff_t);
PAGE_INLINE void uvm_pageunwire(struct vm_page *);
PAGE_INLINE void uvm_pagewait(struct vm_page *, int);
PAGE_INLINE void uvm_pagewake(struct vm_page *);
PAGE_INLINE void uvm_pagewire(struct vm_page *);
PAGE_INLINE void uvm_pagezero(struct vm_page *);
d261 1
a261 1
PAGE_INLINE int uvm_page_lookup_freelist(struct vm_page *);
d264 1
a264 1
int	vm_physseg_find(paddr_t, int *);
@


1.27
log
@Variables were never used, never implemented.

Ok miod, toby
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.26 2007/12/18 11:05:52 thib Exp $	*/
d276 2
a277 2
static struct vm_page *PHYS_TO_VM_PAGE(paddr_t);
static int vm_physseg_find(paddr_t, int *);
a293 96

/*
 * when VM_PHYSSEG_MAX is 1, we can simplify these functions
 */

/*
 * vm_physseg_find: find vm_physseg structure that belongs to a PA
 */
static __inline int
vm_physseg_find(pframe, offp)
	paddr_t pframe;
	int	*offp;
{
#if VM_PHYSSEG_MAX == 1

	/* 'contig' case */
	if (pframe >= vm_physmem[0].start && pframe < vm_physmem[0].end) {
		if (offp)
			*offp = pframe - vm_physmem[0].start;
		return(0);
	}
	return(-1);

#elif (VM_PHYSSEG_STRAT == VM_PSTRAT_BSEARCH)
	/* binary search for it */
	int	start, len, try;

	/*
	 * if try is too large (thus target is less than than try) we reduce
	 * the length to trunc(len/2) [i.e. everything smaller than "try"]
	 *
	 * if the try is too small (thus target is greater than try) then
	 * we set the new start to be (try + 1).   this means we need to
	 * reduce the length to (round(len/2) - 1).
	 *
	 * note "adjust" below which takes advantage of the fact that
	 *  (round(len/2) - 1) == trunc((len - 1) / 2)
	 * for any value of len we may have
	 */

	for (start = 0, len = vm_nphysseg ; len != 0 ; len = len / 2) {
		try = start + (len / 2);	/* try in the middle */

		/* start past our try? */
		if (pframe >= vm_physmem[try].start) {
			/* was try correct? */
			if (pframe < vm_physmem[try].end) {
				if (offp)
					*offp = pframe - vm_physmem[try].start;
				return(try);            /* got it */
			}
			start = try + 1;	/* next time, start here */
			len--;			/* "adjust" */
		} else {
			/*
			 * pframe before try, just reduce length of
			 * region, done in "for" loop
			 */
		}
	}
	return(-1);

#else
	/* linear search for it */
	int	lcv;

	for (lcv = 0; lcv < vm_nphysseg; lcv++) {
		if (pframe >= vm_physmem[lcv].start &&
		    pframe < vm_physmem[lcv].end) {
			if (offp)
				*offp = pframe - vm_physmem[lcv].start;
			return(lcv);		   /* got it */
		}
	}
	return(-1);

#endif
}

/*
 * PHYS_TO_VM_PAGE: find vm_page for a PA.   used by MI code to get vm_pages
 * back from an I/O mapping (ugh!).   used in some MD code as well.
 */
static __inline struct vm_page *
PHYS_TO_VM_PAGE(pa)
	paddr_t pa;
{
	paddr_t pf = atop(pa);
	int	off;
	int	psi;

	psi = vm_physseg_find(pf, &off);
	if (psi != -1)
		return(&vm_physmem[psi].pgs[off]);
	return(NULL);
}
@


1.26
log
@Turn the uvm_{lock/unlock}_fpageq() inlines into
macros that just expand into the mutex functions
to keep the abstraction, do assorted cleanup.

ok miod@@,art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.25 2007/04/18 18:51:37 art Exp $	*/
a225 20

/*
 *	Each pageable resident page falls into one of three lists:
 *
 *	free	
 *		Available for allocation now.
 *	inactive
 *		Not referenced in any map, but still has an
 *		object/offset-page mapping, and may be dirty.
 *		This is the list of pages that should be
 *		paged out next.
 *	active
 *		A list of pages which have been placed in
 *		at least one physical map.  This list is
 *		ordered, in LRU-like fashion.
 */

extern struct pglist	vm_page_queue_free;	/* memory free queue */
extern struct pglist	vm_page_queue_active;	/* active memory queue */
extern struct pglist	vm_page_queue_inactive;	/* inactive memory queue */
@


1.25
log
@Reserve a few pg_flags for pmaps that might want to use them.
i386 will use them soon and miod wants to work on other pmaps in
parallell.

miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.24 2007/04/13 18:57:49 art Exp $	*/
d305 2
@


1.24
log
@While splitting flags and pqflags might have been a good idea in theory
to separate locking, on most modern machines this is not enough
since operations on short types touch other short types that share the
same word in memory.

Merge pg_flags and pqflags again and now use atomic operations to change
the flags. Also bump wire_count to an int and pg_version might go
int as well, just for alignment.

tested by many, many. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.23 2007/04/04 17:44:45 art Exp $	*/
d179 6
a184 1
#define PQ_MASK		0xffff0000
@


1.23
log
@Mechanically rename the "flags" and "version" fields in struct vm_page
to "pg_flags" and "pg_version", so that they are a bit easier to work with.
Whoever uses generic names like this for a popular struct obviously doesn't
read much code.

Most architectures compile and there are no functionality changes.

deraadt@@ ok ("if something fails to compile, we fix that by hand")
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.22 2006/06/16 23:05:23 miod Exp $	*/
a105 12
/*
 * locking note: the mach version of this data structure had bit
 * fields for the flags, and the bit fields were divided into two
 * items (depending on who locked what).  some time, in BSD, the bit
 * fields were dumped and all the flags were lumped into one short.
 * that is fine for a single threaded uniprocessor OS, but bad if you
 * want to actual make use of locking (simple_lock's).  so, we've
 * separated things back out again.
 *
 * note the page structure has no lock of its own.
 */

d119 5
a123 4
	u_short			pg_flags;	/* object flags [O] */
	u_short			pg_version;	/* version count [O] */
	u_short			wire_count;	/* wired down map refs [P] */
	u_short			pqflags;	/* page queue flags [P] */
d150 1
d157 17
a173 16
#define	PG_BUSY		0x0001		/* page is locked */
#define	PG_WANTED	0x0002		/* someone is waiting for page */
#define	PG_TABLED	0x0004		/* page is in VP table  */
#define	PG_CLEAN	0x0008		/* page has not been modified */
#define PG_CLEANCHK	0x0010		/* clean bit has been checked */
#define PG_RELEASED	0x0020		/* page released while paging */
#define	PG_FAKE		0x0040		/* page is not yet initialized */
#define PG_RDONLY	0x0080		/* page must be mapped read-only */
#define PG_ZERO		0x0100		/* page is pre-zero'd */

#define PG_PAGER1	0x1000		/* pager-specific flag */

#define PQ_FREE		0x0001		/* page is on free list */
#define PQ_INACTIVE	0x0002		/* page is in inactive list */
#define PQ_ACTIVE	0x0004		/* page is in active list */
#define PQ_ANON		0x0010		/* page is part of an anon, rather
d175 1
a175 1
#define PQ_AOBJ		0x0020		/* page is part of an anonymous
d178 2
a179 1
#define	PQ_ENCRYPT	0x0040		/* page needs {en,de}cryption */
d404 1
a404 1
#define VM_PAGE_IS_FREE(entry)  ((entry)->pqflags & PQ_FREE)
@


1.22
log
@IS_VM_PHYSADDR is no longer used.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.21 2003/11/08 19:17:28 jmc Exp $	*/
d131 2
a132 2
	u_short			flags;		/* object flags [O] */
	u_short			version;	/* version count [O] */
@


1.21
log
@typos from Jonathon Gray;
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.20 2002/07/20 22:19:35 art Exp $	*/
a392 7


/*
 * IS_VM_PHYSADDR: only used my mips/pmax/pica trap/pmap.
 */

#define IS_VM_PHYSADDR(PA) (vm_physseg_find(atop(PA), NULL) != -1)
@


1.20
log
@Only add a pmap_physseg if MD code defines __HAVE_PMAP_PHYSSEG.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.19 2002/06/11 09:45:16 art Exp $	*/
d113 1
a113 1
 * seperated things back out again.
@


1.19
log
@Allow MD code to define __HAVE_VM_PAGE_MD to add own members into struct vm_page.
From NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.18 2002/03/14 01:27:18 millert Exp $	*/
d217 1
d219 1
@


1.18
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.17 2001/12/19 08:58:07 art Exp $	*/
d139 4
@


1.17
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.13 2001/11/12 01:26:09 art Exp $	*/
d265 1
a265 1
void uvm_page_init __P((vaddr_t *, vaddr_t *));
d267 1
a267 1
void uvm_page_own __P((struct vm_page *, char *));
d270 1
a270 1
boolean_t uvm_page_physget __P((paddr_t *));
d272 2
a273 2
void uvm_page_rehash __P((void));
void uvm_pageidlezero __P((void));
d275 2
a276 2
PAGE_INLINE int uvm_lock_fpageq __P((void));
PAGE_INLINE void uvm_unlock_fpageq __P((int));
d278 12
a289 12
PAGE_INLINE void uvm_pageactivate __P((struct vm_page *));
vaddr_t uvm_pageboot_alloc __P((vsize_t));
PAGE_INLINE void uvm_pagecopy __P((struct vm_page *, struct vm_page *));
PAGE_INLINE void uvm_pagedeactivate __P((struct vm_page *));
void uvm_pagefree __P((struct vm_page *));
void uvm_page_unbusy __P((struct vm_page **, int));
PAGE_INLINE struct vm_page *uvm_pagelookup __P((struct uvm_object *, voff_t));
PAGE_INLINE void uvm_pageunwire __P((struct vm_page *));
PAGE_INLINE void uvm_pagewait __P((struct vm_page *, int));
PAGE_INLINE void uvm_pagewake __P((struct vm_page *));
PAGE_INLINE void uvm_pagewire __P((struct vm_page *));
PAGE_INLINE void uvm_pagezero __P((struct vm_page *));
d291 1
a291 1
PAGE_INLINE int uvm_page_lookup_freelist __P((struct vm_page *));
d293 2
a294 2
static struct vm_page *PHYS_TO_VM_PAGE __P((paddr_t));
static int vm_physseg_find __P((paddr_t, int *));
@


1.16
log
@Yet another sync to NetBSD uvm.
Today we add a pmap argument to pmap_update() and allocate map entries for
kernel_map from kmem_map instead of using the static entries. This should
get rid of MAX_KMAPENT panics. Also some uvm_loan problems are fixed.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.h,v 1.15 2001/11/30 17:37:43 art Exp $	*/
/*	$NetBSD: uvm_page.h,v 1.30 2001/07/25 23:05:04 thorpej Exp $	*/
d4 1
a4 1
/*
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and
d48 1
a48 1
 *
d54 3
a56 3
 *
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
d58 1
a58 1
 *
d113 1
a113 1
 * separated things back out again.
d131 4
a134 7
	u_int			flags:      16,	/* object flags [O] */
				version:    16;	/* version count [O] */

	u_int			wire_count: 16,	/* wired down map refs [P] */
				pqflags:    8,	/* page queue flags [P] */
				       :    8;

a138 5

#ifdef __HAVE_VM_PAGE_MD
	struct vm_page_md	mdpage;		/* pmap-specific data */
#endif

d148 2
d155 1
a155 1
 *   PQ_ ==> lock by page queue lock
d175 4
a178 4
#define PQ_FREE		0x01		/* page is on free list */
#define PQ_INACTIVE	0x02		/* page is in inactive list */
#define PQ_ACTIVE	0x04		/* page is in active list */
#define PQ_ANON		0x10		/* page is part of an anon, rather
d180 1
a180 1
#define PQ_AOBJ		0x20		/* page is part of an anonymous
a212 1
#ifdef __HAVE_PMAP_PHYSSEG
a213 1
#endif
d227 1
a227 1
 *	free
d257 1
a257 1
#else
a272 1
void uvm_page_recolor __P((int));
a308 6

/*
 * Compute the page color bucket for a given page.
 */
#define	VM_PGCOLOR_BUCKET(pg) \
	(atop(VM_PAGE_TO_PHYS((pg))) & uvmexp.colormask)
@


1.16.2.1
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.h,v 1.16 2001/12/04 23:22:42 art Exp $	*/
/*	$NetBSD: uvm_page.h,v 1.31 2001/09/15 20:36:46 chs Exp $	*/
d130 9
a138 2
	uint16_t		flags;		/* object flags [O] */
	uint16_t		loan_count;	/* number of active loans
a140 2
	uint16_t		wire_count;	/* wired down map refs [P] */
	uint16_t		pqflags;	/* page queue flags [P] */
d173 2
a174 2
#define	PG_PAGEOUT	0x0010		/* page to be freed for pagedaemon */
#define PG_RELEASED	0x0020		/* page to be freed when unbusied */
d176 2
a177 2
#define	PG_RDONLY	0x0080		/* page must be mapped read-only */
#define	PG_ZERO		0x0100		/* page is pre-zero'd */
a290 1
PAGE_INLINE void uvm_pagedequeue __P((struct vm_page *));
a307 2

#define UVM_PAGE_HASH_PENALTY	4	/* XXX: a guess */
@


1.16.2.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.16.2.1 2002/02/02 03:28:27 art Exp $	*/
d268 1
a268 1
void uvm_page_init(vaddr_t *, vaddr_t *);
d270 1
a270 1
void uvm_page_own(struct vm_page *, char *);
d273 1
a273 1
boolean_t uvm_page_physget(paddr_t *);
d275 20
a294 20
void uvm_page_rehash(void);
void uvm_page_recolor(int);
void uvm_pageidlezero(void);

PAGE_INLINE int uvm_lock_fpageq(void);
PAGE_INLINE void uvm_unlock_fpageq(int);

PAGE_INLINE void uvm_pageactivate(struct vm_page *);
vaddr_t uvm_pageboot_alloc(vsize_t);
PAGE_INLINE void uvm_pagecopy(struct vm_page *, struct vm_page *);
PAGE_INLINE void uvm_pagedeactivate(struct vm_page *);
PAGE_INLINE void uvm_pagedequeue(struct vm_page *);
void uvm_pagefree(struct vm_page *);
void uvm_page_unbusy(struct vm_page **, int);
PAGE_INLINE struct vm_page *uvm_pagelookup(struct uvm_object *, voff_t);
PAGE_INLINE void uvm_pageunwire(struct vm_page *);
PAGE_INLINE void uvm_pagewait(struct vm_page *, int);
PAGE_INLINE void uvm_pagewake(struct vm_page *);
PAGE_INLINE void uvm_pagewire(struct vm_page *);
PAGE_INLINE void uvm_pagezero(struct vm_page *);
d296 1
a296 1
PAGE_INLINE int uvm_page_lookup_freelist(struct vm_page *);
d298 2
a299 2
static struct vm_page *PHYS_TO_VM_PAGE(paddr_t);
static int vm_physseg_find(paddr_t, int *);
@


1.16.2.3
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.16.2.2 2002/06/11 03:33:04 art Exp $	*/
d141 1
@


1.15
log
@Now that pmaps can have vm_page_md, make pmap_physseg optional.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.h,v 1.14 2001/11/28 19:28:15 art Exp $	*/
/*	$NetBSD: uvm_page.h,v 1.27 2001/06/28 00:26:38 thorpej Exp $	*/
d113 1
a113 1
 * seperated things back out again.
@


1.14
log
@Sync in more uvm from NetBSD. Mostly just cosmetic stuff.
Contains also support for page coloring.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.13 2001/11/12 01:26:09 art Exp $	*/
d219 1
a219 1
/* #ifdef __HAVE_PMAP_PHYSSEG XXX */
d221 1
a221 1
/* #endif */
@


1.13
log
@Bring in more changes from NetBSD. Mostly pagedaemon improvements.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.h,v 1.12 2001/11/10 18:42:31 art Exp $	*/
/*	$NetBSD: uvm_page.h,v 1.19 2000/12/28 08:24:55 chs Exp $	*/
d4 1
a4 1
/* 
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and 
d48 1
a48 1
 * 
d54 3
a56 3
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
d58 1
a58 1
 * 
d131 7
a137 4
	u_short			flags;		/* object flags [O] */
	u_short			version;	/* version count [O] */
	u_short			wire_count;	/* wired down map refs [P] */
	u_short			pqflags;	/* page queue flags [P] */
d142 5
a155 2
 *
 * Note: PG_FILLED and PG_DIRTY are added for the filesystems.
d161 1
a161 1
 *   PQ_ ==> lock by page queue lock 
d181 4
a184 4
#define PQ_FREE		0x0001		/* page is on free list */
#define PQ_INACTIVE	0x0002		/* page is in inactive list */
#define PQ_ACTIVE	0x0004		/* page is in active list */
#define PQ_ANON		0x0010		/* page is part of an anon, rather
d186 1
a186 1
#define PQ_AOBJ		0x0020		/* page is part of an anonymous
d219 1
d221 1
d235 1
a235 1
 *	free	
d265 1
a265 1
#else 
d281 1
d318 6
@


1.12
log
@Merge in some parts of the ubc work that has been done in NetBSD that are not
UBC, but prerequsites for it.

- Create a daemon that processes async I/O (swap and paging in the future)
  requests that need processing in process context and that were processed
  in the pagedaemon before.
- Convert some ugly ifdef DIAGNOSTIC code to less intrusive KASSERTs.
- misc other cleanups.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.h,v 1.11 2001/11/07 02:55:50 art Exp $	*/
/*	$NetBSD: uvm_page.h,v 1.18 2000/11/27 08:40:05 chs Exp $	*/
a413 59

extern
simple_lock_data_t	vm_page_queue_lock;	/* lock on active and inactive
						   page queues */
extern						/* lock on free page queue */
simple_lock_data_t	vm_page_queue_free_lock;

#define PAGE_ASSERT_WAIT(m, interruptible)	{ \
				(m)->flags |= PG_WANTED; \
				assert_wait((m), (interruptible)); \
			}

#define PAGE_WAKEUP(m)	{ \
				(m)->flags &= ~PG_BUSY; \
				if ((m)->flags & PG_WANTED) { \
					(m)->flags &= ~PG_WANTED; \
					wakeup((m)); \
				} \
			}

#define	vm_page_lock_queues()	simple_lock(&vm_page_queue_lock)
#define	vm_page_unlock_queues()	simple_unlock(&vm_page_queue_lock)

#define vm_page_set_modified(m)	{ (m)->flags &= ~PG_CLEAN; }

#define	VM_PAGE_INIT(mem, obj, offset) { \
	(mem)->flags = PG_BUSY | PG_CLEAN | PG_FAKE; \
	if (obj) \
		vm_page_insert((mem), (obj), (offset)); \
	else \
		(mem)->object = NULL; \
	(mem)->wire_count = 0; \
}

#if VM_PAGE_DEBUG

/*
 * VM_PAGE_CHECK: debugging check of a vm_page structure
 */
static __inline void
VM_PAGE_CHECK(mem)
	struct vm_page *mem;
{
	int lcv;

	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++) {
		if ((unsigned int) mem >= (unsigned int) vm_physmem[lcv].pgs &&
		    (unsigned int) mem <= (unsigned int) vm_physmem[lcv].lastpg)
			break;
	}
	if (lcv == vm_nphysseg ||
	    (mem->flags & (PG_ACTIVE|PG_INACTIVE)) == (PG_ACTIVE|PG_INACTIVE))
		panic("vm_page_check: not valid!"); 
	return;
}

#else /* VM_PAGE_DEBUG */
#define	VM_PAGE_CHECK(mem)
#endif /* VM_PAGE_DEBUG */
@


1.11
log
@Another sync of uvm to NetBSD. Just minor fiddling, no major changes.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.h,v 1.10 2001/11/06 01:35:04 art Exp $	*/
/*	$NetBSD: uvm_page.h,v 1.17 2000/10/03 20:50:49 mrg Exp $	*/
d122 17
a138 17
  TAILQ_ENTRY(vm_page)	pageq;		/* queue info for FIFO
					 * queue or free list (P) */
  TAILQ_ENTRY(vm_page)	hashq;		/* hash table links (O)*/
  TAILQ_ENTRY(vm_page)	listq;		/* pages in same object (O)*/

  struct vm_anon	*uanon;		/* anon (O,P) */
  struct uvm_object	*uobject;	/* object (O,P) */
  voff_t		offset;		/* offset into object (O,P) */

  u_short		flags;		/* object flags [O] */
  u_short		version;	/* version count [O] */
  u_short		wire_count;	/* wired down map refs [P] */
  u_short 		pqflags;	/* page queue flags [P] */
  u_int			loan_count;	/* number of active loans
					 * to read: [O or P]
					 * to modify: [O _and_ P] */
  paddr_t		phys_addr;	/* physical address of page */
d140 3
a142 3
  /* debugging fields to track page ownership */
  pid_t			owner;		/* proc that set PG_BUSY */
  char			*owner_tag;	/* why it was set busy */
a160 2
 *
 * possible deadwood: PG_FAULTING, PQ_LAUNDRY
d162 4
d167 7
a173 10
#define	PG_BUSY		0x0010		/* page is in transit  */
#define	PG_WANTED	0x0020		/* someone is waiting for page */
#define	PG_TABLED	0x0040		/* page is in VP table  */
#define	PG_ZERO		0x0100		/* page is pre-zero'd */
#define	PG_FAKE		0x0200		/* page is placeholder for pagein */
#define	PG_FILLED	0x0400		/* client flag to set when filled */
#define	PG_DIRTY	0x0800		/* client flag to set when dirty */
#define PG_RELEASED	0x1000		/* page released while paging */
#define	PG_FAULTING	0x2000		/* page is being faulted in */
#define PG_CLEANCHK	0x4000		/* clean bit has been checked */
a177 1
#define PQ_LAUNDRY	0x0008		/* page is being cleaned now */
d240 3
a242 6
extern
struct pglist	vm_page_queue_free;	/* memory free queue */
extern
struct pglist	vm_page_queue_active;	/* active memory queue */
extern
struct pglist	vm_page_queue_inactive;	/* inactive memory queue */
d283 1
a284 2
void uvm_pageremove __P((struct vm_page *));
/* uvm_pagerename: not needed */
@


1.10
log
@Move the last content from vm/ to uvm/
The only thing left in vm/ are just dumb wrappers.
vm/vm.h includes uvm/uvm_extern.h
vm/pmap.h includes uvm/uvm_pmap.h
vm/vm_page.h includes uvm/uvm_page.h
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.h,v 1.9 2001/08/11 10:57:22 art Exp $	*/
/*	$NetBSD: uvm_page.h,v 1.16 2000/06/27 09:00:14 mrg Exp $	*/
d198 1
a198 2
 *   XXXCDC: eventually we should remove contig and old non-contig cases
 *   and purge all left-over global variables...
@


1.9
log
@Various random fixes from NetBSD.
Including support for zeroing pages in the idle loop (not enabled yet).
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.h,v 1.8 2001/08/06 14:03:05 art Exp $	*/
/*	$NetBSD: uvm_page.h,v 1.15 2000/04/24 17:12:01 thorpej Exp $	*/
d77 142
d228 13
a240 1
 * macros
d243 6
a248 2
#define uvm_lock_pageq()	simple_lock(&uvm.pageqlock)
#define uvm_unlock_pageq()	simple_unlock(&uvm.pageqlock)
d250 3
a252 2
#define uvm_pagehash(obj,off) \
	(((unsigned long)obj+(unsigned long)atop(off)) & uvm.page_hashmask)
d254 2
a255 1
#define	UVM_PAGEZERO_TARGET	(uvmexp.free)
d299 181
@


1.8
log
@Add a new type voff_t (right now it's typedefed as off_t) used for offsets
into objects.

Gives the possibilty to mmap beyond the size of vaddr_t.

From NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.h,v 1.7 2001/03/22 03:05:56 smart Exp $	*/
/*	$NetBSD: uvm_page.h,v 1.14 2000/03/26 20:54:47 kleink Exp $	*/
d80 6
d95 2
d119 1
@


1.7
log
@Sync style, typo, and comments a little closer to NetBSD.  art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.h,v 1.6 2001/03/09 05:34:38 smart Exp $	*/
/*	$NetBSD: uvm_page.h,v 1.13 1999/06/21 17:25:12 thorpej Exp $	*/
d120 1
a120 1
PAGE_INLINE struct vm_page *uvm_pagelookup __P((struct uvm_object *, vaddr_t));
@


1.6
log
@Protect protypes, certain macros, and inlines from userland.  Checked userland
with a 'make build'.  From NetBSD.  art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.h,v 1.5 2001/01/29 02:07:47 niklas Exp $	*/
d120 1
a120 2
PAGE_INLINE struct vm_page *uvm_pagelookup 
					__P((struct uvm_object *, vaddr_t));
@


1.5
log
@$OpenBSD$
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.h,v 1.12 1999/05/24 19:10:57 thorpej Exp $	*/
/*	$NetBSD: uvm_page.h,v 1.12 1999/05/24 19:10:57 thorpej Exp $	*/
d77 2
d131 2
@


1.4
log
@Bring in some new UVM code from NetBSD (not current).

 - Introduce a new type of map that are interrupt safe and never allow faults
   in them. mb_map and kmem_map are made intrsafe.
 - Add "access protection" to uvm_vslock (to be passed down to uvm_fault and
   later to pmap_enter).
 - madvise(2) now works.
 - various cleanups.
@
text
@d1 1
@


1.3
log
@sync with NetBSD from 1999.05.24 (there is a reason for this date)
 Mostly cleanups, but also a few improvements to pagedaemon for better
 handling of low memory and/or low swap conditions.
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_page.h,v 1.10 1998/08/13 02:11:02 eeh Exp $	*/
a81 2
#define uvm_lock_fpageq()	simple_lock(&uvm.fpageqlock)
#define uvm_unlock_fpageq()	simple_unlock(&uvm.fpageqlock)
d108 3
@


1.3.4.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_page.h,v 1.12 1999/05/24 19:10:57 thorpej Exp $	*/
d82 2
a109 3

PAGE_INLINE int uvm_lock_fpageq __P((void));
PAGE_INLINE void uvm_unlock_fpageq __P((int));
@


1.3.4.2
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_page.h,v 1.7 2001/03/22 03:05:56 smart Exp $	*/
/*	$NetBSD: uvm_page.h,v 1.13 1999/06/21 17:25:12 thorpej Exp $	*/
a75 2
#ifdef _KERNEL

d117 2
a118 1
PAGE_INLINE struct vm_page *uvm_pagelookup __P((struct uvm_object *, vaddr_t));
a127 2

#endif /* _KERNEL */
@


1.3.4.3
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_page.h,v 1.15 2000/04/24 17:12:01 thorpej Exp $	*/
a79 6
 * globals
 */

extern boolean_t vm_page_zero_enable;

/*
a88 2
#define	UVM_PAGEZERO_TARGET	(uvmexp.free)

a110 1
void uvm_pageidlezero __P((void));
d120 1
a120 1
PAGE_INLINE struct vm_page *uvm_pagelookup __P((struct uvm_object *, voff_t));
@


1.3.4.4
log
@merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_page.h,v 1.19 2000/12/28 08:24:55 chs Exp $	*/
a76 139
/*
 *	Resident memory system definitions.
 */

/*
 *	Management of resident (logical) pages.
 *
 *	A small structure is kept for each resident
 *	page, indexed by page number.  Each structure
 *	is an element of several lists:
 *
 *		A hash table bucket used to quickly
 *		perform object/offset lookups
 *
 *		A list of all pages for a given object,
 *		so they can be quickly deactivated at
 *		time of deallocation.
 *
 *		An ordered list of pages due for pageout.
 *
 *	In addition, the structure contains the object
 *	and offset to which this page belongs (for pageout),
 *	and sundry status bits.
 *
 *	Fields in this structure are locked either by the lock on the
 *	object that the page belongs to (O) or by the lock on the page
 *	queues (P) [or both].
 */

/*
 * locking note: the mach version of this data structure had bit
 * fields for the flags, and the bit fields were divided into two
 * items (depending on who locked what).  some time, in BSD, the bit
 * fields were dumped and all the flags were lumped into one short.
 * that is fine for a single threaded uniprocessor OS, but bad if you
 * want to actual make use of locking (simple_lock's).  so, we've
 * seperated things back out again.
 *
 * note the page structure has no lock of its own.
 */

#include <uvm/uvm_extern.h>
#include <uvm/uvm_pglist.h>

struct vm_page {
	TAILQ_ENTRY(vm_page)	pageq;		/* queue info for FIFO
						 * queue or free list (P) */
	TAILQ_ENTRY(vm_page)	hashq;		/* hash table links (O)*/
	TAILQ_ENTRY(vm_page)	listq;		/* pages in same object (O)*/

	struct vm_anon		*uanon;		/* anon (O,P) */
	struct uvm_object	*uobject;	/* object (O,P) */
	voff_t			offset;		/* offset into object (O,P) */

	u_short			flags;		/* object flags [O] */
	u_short			version;	/* version count [O] */
	u_short			wire_count;	/* wired down map refs [P] */
	u_short			pqflags;	/* page queue flags [P] */
	u_int			loan_count;	/* number of active loans
						 * to read: [O or P]
						 * to modify: [O _and_ P] */
	paddr_t			phys_addr;	/* physical address of page */
#if defined(UVM_PAGE_TRKOWN)
	/* debugging fields to track page ownership */
	pid_t			owner;		/* proc that set PG_BUSY */
	char			*owner_tag;	/* why it was set busy */
#endif
};

/*
 * These are the flags defined for vm_page.
 *
 * Note: PG_FILLED and PG_DIRTY are added for the filesystems.
 */

/*
 * locking rules:
 *   PG_ ==> locked by object lock
 *   PQ_ ==> lock by page queue lock 
 *   PQ_FREE is locked by free queue lock and is mutex with all other PQs
 *
 * PG_ZERO is used to indicate that a page has been pre-zero'd.  This flag
 * is only set when the page is on no queues, and is cleared when the page
 * is placed on the free list.
 */

#define	PG_BUSY		0x0001		/* page is locked */
#define	PG_WANTED	0x0002		/* someone is waiting for page */
#define	PG_TABLED	0x0004		/* page is in VP table  */
#define	PG_CLEAN	0x0008		/* page has not been modified */
#define PG_CLEANCHK	0x0010		/* clean bit has been checked */
#define PG_RELEASED	0x0020		/* page released while paging */
#define	PG_FAKE		0x0040		/* page is not yet initialized */
#define PG_RDONLY	0x0080		/* page must be mapped read-only */
#define PG_ZERO		0x0100		/* page is pre-zero'd */

#define PG_PAGER1	0x1000		/* pager-specific flag */

#define PQ_FREE		0x0001		/* page is on free list */
#define PQ_INACTIVE	0x0002		/* page is in inactive list */
#define PQ_ACTIVE	0x0004		/* page is in active list */
#define PQ_ANON		0x0010		/* page is part of an anon, rather
					   than an uvm_object */
#define PQ_AOBJ		0x0020		/* page is part of an anonymous
					   uvm_object */
#define PQ_SWAPBACKED	(PQ_ANON|PQ_AOBJ)
#define	PQ_ENCRYPT	0x0040		/* page needs {en,de}cryption */

/*
 * physical memory layout structure
 *
 * MD vmparam.h must #define:
 *   VM_PHYSEG_MAX = max number of physical memory segments we support
 *		   (if this is "1" then we revert to a "contig" case)
 *   VM_PHYSSEG_STRAT: memory sort/search options (for VM_PHYSEG_MAX > 1)
 * 	- VM_PSTRAT_RANDOM:   linear search (random order)
 *	- VM_PSTRAT_BSEARCH:  binary search (sorted by address)
 *	- VM_PSTRAT_BIGFIRST: linear search (sorted by largest segment first)
 *      - others?
 *   XXXCDC: eventually we should purge all left-over global variables...
 */
#define VM_PSTRAT_RANDOM	1
#define VM_PSTRAT_BSEARCH	2
#define VM_PSTRAT_BIGFIRST	3

/*
 * vm_physmemseg: describes one segment of physical memory
 */
struct vm_physseg {
	paddr_t	start;			/* PF# of first page in segment */
	paddr_t	end;			/* (PF# of last page in segment) + 1 */
	paddr_t	avail_start;		/* PF# of first free page in segment */
	paddr_t	avail_end;		/* (PF# of last free page in segment) +1  */
	int	free_list;		/* which free list they belong on */
	struct	vm_page *pgs;		/* vm_page structures (from start) */
	struct	vm_page *lastpg;	/* vm_page structure for end */
	struct	pmap_physseg pmseg;	/* pmap specific (MD) data */
};

d86 1
a86 13
 *	Each pageable resident page falls into one of three lists:
 *
 *	free	
 *		Available for allocation now.
 *	inactive
 *		Not referenced in any map, but still has an
 *		object/offset-page mapping, and may be dirty.
 *		This is the list of pages that should be
 *		paged out next.
 *	active
 *		A list of pages which have been placed in
 *		at least one physical map.  This list is
 *		ordered, in LRU-like fashion.
d89 2
a90 3
extern struct pglist	vm_page_queue_free;	/* memory free queue */
extern struct pglist	vm_page_queue_active;	/* active memory queue */
extern struct pglist	vm_page_queue_inactive;	/* inactive memory queue */
d92 2
a93 3
/*
 * physical memory config is stored in vm_physmem.
 */
d95 1
a95 2
extern struct vm_physseg vm_physmem[VM_PHYSSEG_MAX];
extern int vm_nphysseg;
a128 1
void uvm_page_unbusy __P((struct vm_page **, int));
d130 2
a138 122

static struct vm_page *PHYS_TO_VM_PAGE __P((paddr_t));
static int vm_physseg_find __P((paddr_t, int *));

/*
 * macros
 */

#define uvm_lock_pageq()	simple_lock(&uvm.pageqlock)
#define uvm_unlock_pageq()	simple_unlock(&uvm.pageqlock)

#define uvm_pagehash(obj,off) \
	(((unsigned long)obj+(unsigned long)atop(off)) & uvm.page_hashmask)

#define	UVM_PAGEZERO_TARGET	(uvmexp.free)

#define VM_PAGE_TO_PHYS(entry)	((entry)->phys_addr)

/*
 * when VM_PHYSSEG_MAX is 1, we can simplify these functions
 */

/*
 * vm_physseg_find: find vm_physseg structure that belongs to a PA
 */
static __inline int
vm_physseg_find(pframe, offp)
	paddr_t pframe;
	int	*offp;
{
#if VM_PHYSSEG_MAX == 1

	/* 'contig' case */
	if (pframe >= vm_physmem[0].start && pframe < vm_physmem[0].end) {
		if (offp)
			*offp = pframe - vm_physmem[0].start;
		return(0);
	}
	return(-1);

#elif (VM_PHYSSEG_STRAT == VM_PSTRAT_BSEARCH)
	/* binary search for it */
	int	start, len, try;

	/*
	 * if try is too large (thus target is less than than try) we reduce
	 * the length to trunc(len/2) [i.e. everything smaller than "try"]
	 *
	 * if the try is too small (thus target is greater than try) then
	 * we set the new start to be (try + 1).   this means we need to
	 * reduce the length to (round(len/2) - 1).
	 *
	 * note "adjust" below which takes advantage of the fact that
	 *  (round(len/2) - 1) == trunc((len - 1) / 2)
	 * for any value of len we may have
	 */

	for (start = 0, len = vm_nphysseg ; len != 0 ; len = len / 2) {
		try = start + (len / 2);	/* try in the middle */

		/* start past our try? */
		if (pframe >= vm_physmem[try].start) {
			/* was try correct? */
			if (pframe < vm_physmem[try].end) {
				if (offp)
					*offp = pframe - vm_physmem[try].start;
				return(try);            /* got it */
			}
			start = try + 1;	/* next time, start here */
			len--;			/* "adjust" */
		} else {
			/*
			 * pframe before try, just reduce length of
			 * region, done in "for" loop
			 */
		}
	}
	return(-1);

#else
	/* linear search for it */
	int	lcv;

	for (lcv = 0; lcv < vm_nphysseg; lcv++) {
		if (pframe >= vm_physmem[lcv].start &&
		    pframe < vm_physmem[lcv].end) {
			if (offp)
				*offp = pframe - vm_physmem[lcv].start;
			return(lcv);		   /* got it */
		}
	}
	return(-1);

#endif
}


/*
 * IS_VM_PHYSADDR: only used my mips/pmax/pica trap/pmap.
 */

#define IS_VM_PHYSADDR(PA) (vm_physseg_find(atop(PA), NULL) != -1)

/*
 * PHYS_TO_VM_PAGE: find vm_page for a PA.   used by MI code to get vm_pages
 * back from an I/O mapping (ugh!).   used in some MD code as well.
 */
static __inline struct vm_page *
PHYS_TO_VM_PAGE(pa)
	paddr_t pa;
{
	paddr_t pf = atop(pa);
	int	off;
	int	psi;

	psi = vm_physseg_find(pf, &off);
	if (psi != -1)
		return(&vm_physmem[psi].pgs[off]);
	return(NULL);
}

#define VM_PAGE_IS_FREE(entry)  ((entry)->pqflags & PQ_FREE)
@


1.3.4.5
log
@Merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_page.h,v 1.27 2001/06/28 00:26:38 thorpej Exp $	*/
d4 1
a4 1
/*
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and
d48 1
a48 1
 *
d54 3
a56 3
 *
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
d58 1
a58 1
 *
d131 4
a134 7
	u_int			flags:      16,	/* object flags [O] */
				version:    16;	/* version count [O] */

	u_int			wire_count: 16,	/* wired down map refs [P] */
				pqflags:    8,	/* page queue flags [P] */
				       :    8;

a138 5

#ifdef __HAVE_VM_PAGE_MD
	struct vm_page_md	mdpage;		/* pmap-specific data */
#endif

d148 2
d155 1
a155 1
 *   PQ_ ==> lock by page queue lock
d175 4
a178 4
#define PQ_FREE		0x01		/* page is on free list */
#define PQ_INACTIVE	0x02		/* page is in inactive list */
#define PQ_ACTIVE	0x04		/* page is in active list */
#define PQ_ANON		0x10		/* page is part of an anon, rather
d180 1
a180 1
#define PQ_AOBJ		0x20		/* page is part of an anonymous
a212 1
#ifdef __HAVE_PMAP_PHYSSEG
a213 1
#endif
d227 1
a227 1
 *	free
d257 1
a257 1
#else
a272 1
void uvm_page_recolor __P((int));
a308 6

/*
 * Compute the page color bucket for a given page.
 */
#define	VM_PGCOLOR_BUCKET(pg) \
	(atop(VM_PAGE_TO_PHYS((pg))) & uvmexp.colormask)
@


1.3.4.6
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_page.h,v 1.19 2000/12/28 08:24:55 chs Exp $	*/
d4 1
a4 1
/* 
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and 
d48 1
a48 1
 * 
d54 3
a56 3
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
d58 1
a58 1
 * 
d131 7
a137 4
	u_short			flags;		/* object flags [O] */
	u_short			version;	/* version count [O] */
	u_short			wire_count;	/* wired down map refs [P] */
	u_short			pqflags;	/* page queue flags [P] */
d142 5
a155 2
 *
 * Note: PG_FILLED and PG_DIRTY are added for the filesystems.
d161 1
a161 1
 *   PQ_ ==> lock by page queue lock 
d181 4
a184 4
#define PQ_FREE		0x0001		/* page is on free list */
#define PQ_INACTIVE	0x0002		/* page is in inactive list */
#define PQ_ACTIVE	0x0004		/* page is in active list */
#define PQ_ANON		0x0010		/* page is part of an anon, rather
d186 1
a186 1
#define PQ_AOBJ		0x0020		/* page is part of an anonymous
d219 1
d221 1
d235 1
a235 1
 *	free	
d265 1
a265 1
#else 
d281 1
d318 6
@


1.3.4.7
log
@Merge in -current from roughly a week ago
@
text
@d265 1
a265 1
void uvm_page_init(vaddr_t *, vaddr_t *);
d267 1
a267 1
void uvm_page_own(struct vm_page *, char *);
d270 1
a270 1
boolean_t uvm_page_physget(paddr_t *);
d272 2
a273 2
void uvm_page_rehash(void);
void uvm_pageidlezero(void);
d275 2
a276 2
PAGE_INLINE int uvm_lock_fpageq(void);
PAGE_INLINE void uvm_unlock_fpageq(int);
d278 12
a289 12
PAGE_INLINE void uvm_pageactivate(struct vm_page *);
vaddr_t uvm_pageboot_alloc(vsize_t);
PAGE_INLINE void uvm_pagecopy(struct vm_page *, struct vm_page *);
PAGE_INLINE void uvm_pagedeactivate(struct vm_page *);
void uvm_pagefree(struct vm_page *);
void uvm_page_unbusy(struct vm_page **, int);
PAGE_INLINE struct vm_page *uvm_pagelookup(struct uvm_object *, voff_t);
PAGE_INLINE void uvm_pageunwire(struct vm_page *);
PAGE_INLINE void uvm_pagewait(struct vm_page *, int);
PAGE_INLINE void uvm_pagewake(struct vm_page *);
PAGE_INLINE void uvm_pagewire(struct vm_page *);
PAGE_INLINE void uvm_pagezero(struct vm_page *);
d291 1
a291 1
PAGE_INLINE int uvm_page_lookup_freelist(struct vm_page *);
d293 2
a294 2
static struct vm_page *PHYS_TO_VM_PAGE(paddr_t);
static int vm_physseg_find(paddr_t, int *);
@


1.3.4.8
log
@Sync the SMP branch with 3.3
@
text
@a138 4

#ifdef __HAVE_VM_PAGE_MD
	struct vm_page_md	mdpage;		/* pmap-specific data */
#endif
a212 1
#ifdef __HAVE_PMAP_PHYSSEG
a213 1
#endif
@


1.3.4.9
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d113 1
a113 1
 * separated things back out again.
@


1.2
log
@add OpenBSD tags
@
text
@a0 1
/*	$OpenBSD$	*/
a2 4
/*
 * XXXCDC: "ROUGH DRAFT" QUALITY UVM PRE-RELEASE FILE!
 *         >>>USE AT YOUR OWN RISK, WORK IS NOT FINISHED<<<
 */
@


1.1
log
@Import of uvm from NetBSD. Some local changes, some code disabled
@
text
@d1 1
@

