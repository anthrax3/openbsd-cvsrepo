head	1.79;
access;
symbols
	OPENBSD_6_0:1.75.0.2
	OPENBSD_6_0_BASE:1.75
	OPENBSD_5_9:1.59.0.2
	OPENBSD_5_9_BASE:1.59
	OPENBSD_5_8:1.58.0.6
	OPENBSD_5_8_BASE:1.58
	OPENBSD_5_7:1.58.0.2
	OPENBSD_5_7_BASE:1.58
	OPENBSD_5_6:1.53.0.4
	OPENBSD_5_6_BASE:1.53
	OPENBSD_5_5:1.50.0.6
	OPENBSD_5_5_BASE:1.50
	OPENBSD_5_4:1.50.0.2
	OPENBSD_5_4_BASE:1.50
	OPENBSD_5_3:1.46.0.8
	OPENBSD_5_3_BASE:1.46
	OPENBSD_5_2:1.46.0.6
	OPENBSD_5_2_BASE:1.46
	OPENBSD_5_1_BASE:1.46
	OPENBSD_5_1:1.46.0.4
	OPENBSD_5_0:1.46.0.2
	OPENBSD_5_0_BASE:1.46
	OPENBSD_4_9:1.45.0.4
	OPENBSD_4_9_BASE:1.45
	OPENBSD_4_8:1.45.0.2
	OPENBSD_4_8_BASE:1.45
	OPENBSD_4_7:1.44.0.2
	OPENBSD_4_7_BASE:1.44
	OPENBSD_4_6:1.44.0.4
	OPENBSD_4_6_BASE:1.44
	OPENBSD_4_5:1.43.0.2
	OPENBSD_4_5_BASE:1.43
	OPENBSD_4_4:1.40.0.4
	OPENBSD_4_4_BASE:1.40
	OPENBSD_4_3:1.40.0.2
	OPENBSD_4_3_BASE:1.40
	OPENBSD_4_2:1.39.0.2
	OPENBSD_4_2_BASE:1.39
	OPENBSD_4_1:1.33.0.4
	OPENBSD_4_1_BASE:1.33
	OPENBSD_4_0:1.33.0.2
	OPENBSD_4_0_BASE:1.33
	OPENBSD_3_9:1.29.0.2
	OPENBSD_3_9_BASE:1.29
	OPENBSD_3_8:1.28.0.4
	OPENBSD_3_8_BASE:1.28
	OPENBSD_3_7:1.28.0.2
	OPENBSD_3_7_BASE:1.28
	OPENBSD_3_6:1.27.0.10
	OPENBSD_3_6_BASE:1.27
	SMP_SYNC_A:1.27
	SMP_SYNC_B:1.27
	OPENBSD_3_5:1.27.0.8
	OPENBSD_3_5_BASE:1.27
	OPENBSD_3_4:1.27.0.6
	OPENBSD_3_4_BASE:1.27
	UBC_SYNC_A:1.27
	OPENBSD_3_3:1.27.0.4
	OPENBSD_3_3_BASE:1.27
	OPENBSD_3_2:1.27.0.2
	OPENBSD_3_2_BASE:1.27
	OPENBSD_3_1:1.26.0.2
	OPENBSD_3_1_BASE:1.26
	UBC_SYNC_B:1.27
	UBC:1.17.0.2
	UBC_BASE:1.17
	OPENBSD_3_0:1.10.0.2
	OPENBSD_3_0_BASE:1.10
	OPENBSD_2_9_BASE:1.8
	OPENBSD_2_9:1.8.0.2
	OPENBSD_2_8:1.5.0.2
	OPENBSD_2_8_BASE:1.5
	OPENBSD_2_7:1.4.0.2
	OPENBSD_2_7_BASE:1.4
	SMP:1.3.0.4
	SMP_BASE:1.3
	kame_19991208:1.3
	OPENBSD_2_6:1.3.0.2
	OPENBSD_2_6_BASE:1.3
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.79
date	2017.01.31.17.08.51;	author dhill;	state Exp;
branches;
next	1.78;
commitid	2SHmhHhbpU2SoS0k;

1.78
date	2016.10.08.16.19.44;	author stefan;	state Exp;
branches;
next	1.77;
commitid	9MLuCGl47JnnzNrF;

1.77
date	2016.09.15.02.00.18;	author dlg;	state Exp;
branches;
next	1.76;
commitid	RlO92XR575sygHqm;

1.76
date	2016.07.27.14.48.56;	author tedu;	state Exp;
branches;
next	1.75;
commitid	h4c6nAYowS3uyZUb;

1.75
date	2016.07.14.16.23.49;	author stefan;	state Exp;
branches
	1.75.2.1;
next	1.74;
commitid	jExKr9PwpqKu6Ffq;

1.74
date	2016.07.11.08.38.49;	author stefan;	state Exp;
branches;
next	1.73;
commitid	MAmTeIWnr6Hg0W6Q;

1.73
date	2016.07.09.17.13.05;	author stefan;	state Exp;
branches;
next	1.72;
commitid	ScgoSlrJVDHjg3GT;

1.72
date	2016.06.17.10.48.25;	author dlg;	state Exp;
branches;
next	1.71;
commitid	3jj6c8J8OJvfGS9o;

1.71
date	2016.05.26.13.37.26;	author stefan;	state Exp;
branches;
next	1.70;
commitid	eiMXKHK3UupAUyDE;

1.70
date	2016.05.22.22.52.01;	author guenther;	state Exp;
branches;
next	1.69;
commitid	KhyebnIkY686CCxA;

1.69
date	2016.05.22.16.18.26;	author stefan;	state Exp;
branches;
next	1.68;
commitid	biGCNWraZJf92vP1;

1.68
date	2016.05.08.16.29.57;	author stefan;	state Exp;
branches;
next	1.67;
commitid	szK6LyawtrqhVtcp;

1.67
date	2016.05.08.11.52.32;	author stefan;	state Exp;
branches;
next	1.66;
commitid	hUj20vPhiD6DQNDL;

1.66
date	2016.04.16.18.39.31;	author stefan;	state Exp;
branches;
next	1.65;
commitid	4OGLhCEcSCff5pGJ;

1.65
date	2016.04.12.16.47.33;	author stefan;	state Exp;
branches;
next	1.64;
commitid	jaTkU9MflTDv2wJN;

1.64
date	2016.04.04.16.34.16;	author stefan;	state Exp;
branches;
next	1.63;
commitid	mErYIUO2MMXVvZFw;

1.63
date	2016.03.27.09.51.37;	author stefan;	state Exp;
branches;
next	1.62;
commitid	vrFSc4NQkLUJ1a8U;

1.62
date	2016.03.16.16.53.43;	author stefan;	state Exp;
branches;
next	1.61;
commitid	Bl8n3YcZ0WuegxoE;

1.61
date	2016.03.15.18.16.21;	author stefan;	state Exp;
branches;
next	1.60;
commitid	CoWpF2dLmDJAmOxQ;

1.60
date	2016.03.06.14.47.07;	author stefan;	state Exp;
branches;
next	1.59;
commitid	j1TTEtBJGEcXIrnP;

1.59
date	2015.08.21.16.04.35;	author visa;	state Exp;
branches
	1.59.2.1;
next	1.58;
commitid	gglpDr80UKmkkP9A;

1.58
date	2014.12.23.04.56.47;	author tedu;	state Exp;
branches
	1.58.6.1;
next	1.57;
commitid	Vcm2YxE1LuHhCHfd;

1.57
date	2014.12.17.19.42.15;	author tedu;	state Exp;
branches;
next	1.56;
commitid	G4ldVK4QwvfU3tRp;

1.56
date	2014.12.09.07.16.41;	author doug;	state Exp;
branches;
next	1.55;
commitid	tPDMRisjAolmdVN1;

1.55
date	2014.11.16.12.31.00;	author deraadt;	state Exp;
branches;
next	1.54;
commitid	yv0ECmCdICvq576h;

1.54
date	2014.09.14.14.17.27;	author jsg;	state Exp;
branches;
next	1.53;
commitid	uzzBR7hz9ncd4O6G;

1.53
date	2014.07.12.18.44.01;	author tedu;	state Exp;
branches;
next	1.52;
commitid	bDGgAR6yEQVcVl5u;

1.52
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.51;
commitid	7NtJNW9udCOFtDNM;

1.51
date	2014.04.13.23.14.15;	author tedu;	state Exp;
branches;
next	1.50;

1.50
date	2013.05.30.16.39.26;	author tedu;	state Exp;
branches;
next	1.49;

1.49
date	2013.05.30.16.29.46;	author tedu;	state Exp;
branches;
next	1.48;

1.48
date	2013.05.30.15.17.59;	author tedu;	state Exp;
branches;
next	1.47;

1.47
date	2013.05.23.01.42.59;	author tedu;	state Exp;
branches;
next	1.46;

1.46
date	2011.07.03.18.34.14;	author oga;	state Exp;
branches;
next	1.45;

1.45
date	2010.07.03.03.04.55;	author tedu;	state Exp;
branches;
next	1.44;

1.44
date	2009.03.25.20.00.18;	author oga;	state Exp;
branches;
next	1.43;

1.43
date	2008.10.08.08.41.18;	author art;	state Exp;
branches;
next	1.42;

1.42
date	2008.09.12.08.55.41;	author otto;	state Exp;
branches;
next	1.41;

1.41
date	2008.08.26.15.39.27;	author kettenis;	state Exp;
branches;
next	1.40;

1.40
date	2007.09.07.15.00.20;	author art;	state Exp;
branches;
next	1.39;

1.39
date	2007.06.18.21.51.15;	author pedro;	state Exp;
branches;
next	1.38;

1.38
date	2007.06.01.20.10.04;	author tedu;	state Exp;
branches;
next	1.37;

1.37
date	2007.05.31.21.20.30;	author thib;	state Exp;
branches;
next	1.36;

1.36
date	2007.04.27.16.38.13;	author art;	state Exp;
branches;
next	1.35;

1.35
date	2007.04.13.18.57.49;	author art;	state Exp;
branches;
next	1.34;

1.34
date	2007.04.04.17.44.45;	author art;	state Exp;
branches;
next	1.33;

1.33
date	2006.07.31.11.51.29;	author mickey;	state Exp;
branches;
next	1.32;

1.32
date	2006.07.26.23.15.55;	author mickey;	state Exp;
branches;
next	1.31;

1.31
date	2006.07.13.22.51.26;	author deraadt;	state Exp;
branches;
next	1.30;

1.30
date	2006.06.21.16.20.05;	author mickey;	state Exp;
branches;
next	1.29;

1.29
date	2005.12.10.11.45.43;	author miod;	state Exp;
branches;
next	1.28;

1.28
date	2004.12.30.08.28.39;	author niklas;	state Exp;
branches;
next	1.27;

1.27
date	2002.05.09.14.14.18;	author provos;	state Exp;
branches;
next	1.26;

1.26
date	2002.03.14.01.27.18;	author millert;	state Exp;
branches;
next	1.25;

1.25
date	2002.02.28.16.32.03;	author provos;	state Exp;
branches;
next	1.24;

1.24
date	2002.02.20.20.16.23;	author provos;	state Exp;
branches;
next	1.23;

1.23
date	2002.02.19.17.57.34;	author provos;	state Exp;
branches;
next	1.22;

1.22
date	2002.02.12.18.36.53;	author provos;	state Exp;
branches;
next	1.21;

1.21
date	2002.01.23.00.39.48;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2002.01.15.20.09.56;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2002.01.02.22.23.25;	author miod;	state Exp;
branches;
next	1.18;

1.18
date	2001.12.19.08.58.07;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2001.12.04.23.22.42;	author art;	state Exp;
branches
	1.17.2.1;
next	1.16;

1.16
date	2001.11.28.19.28.14;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2001.11.11.01.16.56;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2001.11.07.02.55.50;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2001.11.06.13.36.52;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2001.11.06.01.35.04;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2001.11.05.22.14.54;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2001.09.11.20.05.25;	author miod;	state Exp;
branches;
next	1.9;

1.9
date	2001.07.18.10.47.05;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2001.03.15.11.48.17;	author art;	state Exp;
branches;
next	1.7;

1.7
date	2001.01.29.02.07.42;	author niklas;	state Exp;
branches;
next	1.6;

1.6
date	2000.11.10.15.33.11;	author provos;	state Exp;
branches;
next	1.5;

1.5
date	2000.09.07.20.15.28;	author art;	state Exp;
branches;
next	1.4;

1.4
date	2000.03.15.15.50.18;	author art;	state Exp;
branches;
next	1.3;

1.3
date	99.08.23.08.13.22;	author art;	state Exp;
branches
	1.3.4.1;
next	1.2;

1.2
date	99.02.26.05.32.06;	author art;	state Exp;
branches;
next	1.1;

1.1
date	99.02.26.01.30.10;	author art;	state Exp;
branches;
next	;

1.3.4.1
date	2000.03.24.09.09.47;	author niklas;	state Exp;
branches;
next	1.3.4.2;

1.3.4.2
date	2001.05.14.22.47.44;	author niklas;	state Exp;
branches;
next	1.3.4.3;

1.3.4.3
date	2001.10.31.03.32.14;	author nate;	state Exp;
branches;
next	1.3.4.4;

1.3.4.4
date	2001.11.13.23.02.31;	author niklas;	state Exp;
branches;
next	1.3.4.5;

1.3.4.5
date	2001.12.05.01.19.55;	author niklas;	state Exp;
branches;
next	1.3.4.6;

1.3.4.6
date	2002.03.06.02.17.14;	author niklas;	state Exp;
branches;
next	1.3.4.7;

1.3.4.7
date	2002.03.28.14.54.26;	author niklas;	state Exp;
branches;
next	1.3.4.8;

1.3.4.8
date	2003.03.28.00.08.48;	author niklas;	state Exp;
branches;
next	;

1.17.2.1
date	2002.01.31.22.55.50;	author niklas;	state Exp;
branches;
next	1.17.2.2;

1.17.2.2
date	2002.02.02.03.28.26;	author art;	state Exp;
branches;
next	1.17.2.3;

1.17.2.3
date	2002.06.11.03.33.03;	author art;	state Exp;
branches;
next	1.17.2.4;

1.17.2.4
date	2002.11.04.18.02.32;	author art;	state Exp;
branches;
next	1.17.2.5;

1.17.2.5
date	2002.11.04.20.10.59;	author art;	state Exp;
branches;
next	;

1.58.6.1
date	2016.07.14.17.27.02;	author stefan;	state Exp;
branches;
next	;
commitid	eQDCoOmd8mXmpicG;

1.59.2.1
date	2016.07.14.17.26.16;	author stefan;	state Exp;
branches;
next	;
commitid	Z6m3DnhTR8lgEhAD;

1.75.2.1
date	2016.10.08.16.20.44;	author stefan;	state Exp;
branches;
next	;
commitid	XAVZwzprLEFkIUWO;


desc
@@


1.79
log
@Sprinkle some free sizes in uvm/

ok stefan@@ visa@@
@
text
@/*	$OpenBSD: uvm_amap.c,v 1.78 2016/10/08 16:19:44 stefan Exp $	*/
/*	$NetBSD: uvm_amap.c,v 1.27 2000/11/25 06:27:59 chs Exp $	*/

/*
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * uvm_amap.c: amap operations
 *
 * this file contains functions that perform operations on amaps.  see
 * uvm_amap.h for a brief explanation of the role of amaps in uvm.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/malloc.h>
#include <sys/kernel.h>
#include <sys/pool.h>
#include <sys/atomic.h>

#include <uvm/uvm.h>
#include <uvm/uvm_swap.h>

/*
 * pools for allocation of vm_amap structures.  note that in order to
 * avoid an endless loop, the amap pool's allocator cannot allocate
 * memory from an amap (it currently goes through the kernel uobj, so
 * we are ok).
 */

struct pool uvm_amap_pool;
struct pool uvm_small_amap_pool[UVM_AMAP_CHUNK];
struct pool uvm_amap_chunk_pool;

LIST_HEAD(, vm_amap) amap_list;

static char amap_small_pool_names[UVM_AMAP_CHUNK][9];

/*
 * local functions
 */

static struct vm_amap *amap_alloc1(int, int, int);
static __inline void amap_list_insert(struct vm_amap *);
static __inline void amap_list_remove(struct vm_amap *);   

struct vm_amap_chunk *amap_chunk_get(struct vm_amap *, int, int, int);
void amap_chunk_free(struct vm_amap *, struct vm_amap_chunk *);
void amap_wiperange_chunk(struct vm_amap *, struct vm_amap_chunk *, int, int);

static __inline void
amap_list_insert(struct vm_amap *amap)
{
	LIST_INSERT_HEAD(&amap_list, amap, am_list);
}

static __inline void
amap_list_remove(struct vm_amap *amap)
{ 
	LIST_REMOVE(amap, am_list);
}

/*
 * amap_chunk_get: lookup a chunk for slot. if create is non-zero,
 * the chunk is created if it does not yet exist.
 *
 * => returns the chunk on success or NULL on error
 */
struct vm_amap_chunk *
amap_chunk_get(struct vm_amap *amap, int slot, int create, int waitf)
{
	int bucket = UVM_AMAP_BUCKET(amap, slot);
	int baseslot = AMAP_BASE_SLOT(slot);
	int n;
	struct vm_amap_chunk *chunk, *newchunk, *pchunk = NULL;

	if (UVM_AMAP_SMALL(amap))
		return &amap->am_small;

	for (chunk = amap->am_buckets[bucket]; chunk != NULL;
	    chunk = TAILQ_NEXT(chunk, ac_list)) {
		if (UVM_AMAP_BUCKET(amap, chunk->ac_baseslot) != bucket)
			break;
		if (chunk->ac_baseslot == baseslot)
			return chunk;
		pchunk = chunk;
	}
	if (!create)
		return NULL;

	if (amap->am_nslot - baseslot >= UVM_AMAP_CHUNK)
		n = UVM_AMAP_CHUNK;
	else
		n = amap->am_nslot - baseslot;

	newchunk = pool_get(&uvm_amap_chunk_pool, waitf | PR_ZERO);
	if (newchunk == NULL)
		return NULL;

	if (pchunk == NULL) {
		TAILQ_INSERT_TAIL(&amap->am_chunks, newchunk, ac_list);
		KASSERT(amap->am_buckets[bucket] == NULL);
		amap->am_buckets[bucket] = newchunk;
	} else
		TAILQ_INSERT_AFTER(&amap->am_chunks, pchunk, newchunk,
		    ac_list);

	amap->am_ncused++;
	newchunk->ac_baseslot = baseslot;
	newchunk->ac_nslot = n;
	return newchunk;
}

void
amap_chunk_free(struct vm_amap *amap, struct vm_amap_chunk *chunk)
{
	int bucket = UVM_AMAP_BUCKET(amap, chunk->ac_baseslot);
	struct vm_amap_chunk *nchunk;

	if (UVM_AMAP_SMALL(amap))
		return;

	nchunk = TAILQ_NEXT(chunk, ac_list);
	TAILQ_REMOVE(&amap->am_chunks, chunk, ac_list);
	if (amap->am_buckets[bucket] == chunk) {
		if (nchunk != NULL &&
		    UVM_AMAP_BUCKET(amap, nchunk->ac_baseslot) == bucket)
			amap->am_buckets[bucket] = nchunk;
		else
			amap->am_buckets[bucket] = NULL;

	}
	pool_put(&uvm_amap_chunk_pool, chunk);
	amap->am_ncused--;
}

#ifdef UVM_AMAP_PPREF
/*
 * what is ppref?   ppref is an _optional_ amap feature which is used
 * to keep track of reference counts on a per-page basis.  it is enabled
 * when UVM_AMAP_PPREF is defined.
 *
 * when enabled, an array of ints is allocated for the pprefs.  this
 * array is allocated only when a partial reference is added to the
 * map (either by unmapping part of the amap, or gaining a reference
 * to only a part of an amap).  if the malloc of the array fails
 * (M_NOWAIT), then we set the array pointer to PPREF_NONE to indicate
 * that we tried to do ppref's but couldn't alloc the array so just
 * give up (after all, this is an optional feature!).
 *
 * the array is divided into page sized "chunks."   for chunks of length 1,
 * the chunk reference count plus one is stored in that chunk's slot.
 * for chunks of length > 1 the first slot contains (the reference count
 * plus one) * -1.    [the negative value indicates that the length is
 * greater than one.]   the second slot of the chunk contains the length
 * of the chunk.   here is an example:
 *
 * actual REFS:  2  2  2  2  3  1  1  0  0  0  4  4  0  1  1  1
 *       ppref: -3  4  x  x  4 -2  2 -1  3  x -5  2  1 -2  3  x
 *              <----------><-><----><-------><----><-><------->
 * (x = don't care)
 *
 * this allows us to allow one int to contain the ref count for the whole
 * chunk.    note that the "plus one" part is needed because a reference
 * count of zero is neither positive or negative (need a way to tell
 * if we've got one zero or a bunch of them).
 * 
 * here are some in-line functions to help us.
 */

static __inline void pp_getreflen(int *, int, int *, int *);
static __inline void pp_setreflen(int *, int, int, int);

/*
 * pp_getreflen: get the reference and length for a specific offset
 */
static __inline void
pp_getreflen(int *ppref, int offset, int *refp, int *lenp)
{

	if (ppref[offset] > 0) {		/* chunk size must be 1 */
		*refp = ppref[offset] - 1;	/* don't forget to adjust */
		*lenp = 1;
	} else {
		*refp = (ppref[offset] * -1) - 1;
		*lenp = ppref[offset+1];
	}
}

/*
 * pp_setreflen: set the reference and length for a specific offset
 */
static __inline void
pp_setreflen(int *ppref, int offset, int ref, int len)
{
	if (len == 1) {
		ppref[offset] = ref + 1;
	} else {
		ppref[offset] = (ref + 1) * -1;
		ppref[offset+1] = len;
	}
}
#endif

/*
 * amap_init: called at boot time to init global amap data structures
 */

void
amap_init(void)
{
	int i;
	size_t size;

	/* Initialize the vm_amap pool. */
	pool_init(&uvm_amap_pool, sizeof(struct vm_amap),
	    0, IPL_NONE, PR_WAITOK, "amappl", NULL);
	pool_sethiwat(&uvm_amap_pool, 4096);

	/* initialize small amap pools */
	for (i = 0; i < nitems(uvm_small_amap_pool); i++) {
		snprintf(amap_small_pool_names[i],
		    sizeof(amap_small_pool_names[0]), "amappl%d", i + 1);
		size = offsetof(struct vm_amap, am_small.ac_anon) +
		    (i + 1) * sizeof(struct vm_anon *);
		pool_init(&uvm_small_amap_pool[i], size, 0,
		    IPL_NONE, 0, amap_small_pool_names[i], NULL);
	}

	pool_init(&uvm_amap_chunk_pool, sizeof(struct vm_amap_chunk) +
	    UVM_AMAP_CHUNK * sizeof(struct vm_anon *),
	    0, IPL_NONE, 0, "amapchunkpl", NULL);
	pool_sethiwat(&uvm_amap_chunk_pool, 4096);
}

/*
 * amap_alloc1: internal function that allocates an amap, but does not
 *	init the overlay.
 */
static inline struct vm_amap *
amap_alloc1(int slots, int waitf, int lazyalloc)
{
	struct vm_amap *amap;
	struct vm_amap_chunk *chunk, *tmp;
	int chunks, log_chunks, chunkperbucket = 1, hashshift = 0;
	int buckets, i, n;
	int pwaitf = (waitf & M_WAITOK) ? PR_WAITOK : PR_NOWAIT;

	KASSERT(slots > 0);

	/*
	 * Cast to unsigned so that rounding up cannot cause integer overflow
	 * if slots is large.
	 */
	chunks = roundup((unsigned int)slots, UVM_AMAP_CHUNK) / UVM_AMAP_CHUNK;

	if (lazyalloc) {
		/*
		 * Basically, the amap is a hash map where the number of
		 * buckets is fixed. We select the number of buckets using the
		 * following strategy:
		 *
		 * 1. The maximal number of entries to search in a bucket upon
		 * a collision should be less than or equal to
		 * log2(slots / UVM_AMAP_CHUNK). This is the worst-case number
		 * of lookups we would have if we could chunk the amap. The
		 * log2(n) comes from the fact that amaps are chunked by
		 * splitting up their vm_map_entries and organizing those
		 * in a binary search tree.
		 *
		 * 2. The maximal number of entries in a bucket must be a
		 * power of two.
		 *
		 * The maximal number of entries per bucket is used to hash
		 * a slot to a bucket.
		 *
		 * In the future, this strategy could be refined to make it
		 * even harder/impossible that the total amount of KVA needed
		 * for the hash buckets of all amaps to exceed the maximal
		 * amount of KVA memory reserved for amaps.
		 */
		for (log_chunks = 1; (chunks >> log_chunks) > 0; log_chunks++)
			continue;

		chunkperbucket = 1 << hashshift;
		while (chunkperbucket + 1 < log_chunks) {
			hashshift++;
			chunkperbucket = 1 << hashshift;
		}
	}

	if (slots > UVM_AMAP_CHUNK)
		amap = pool_get(&uvm_amap_pool, pwaitf);
	else
		amap = pool_get(&uvm_small_amap_pool[slots - 1],
		    pwaitf | PR_ZERO);
	if (amap == NULL)
		return(NULL);

	amap->am_ref = 1;
	amap->am_flags = 0;
#ifdef UVM_AMAP_PPREF
	amap->am_ppref = NULL;
#endif
	amap->am_nslot = slots;
	amap->am_nused = 0;

	if (UVM_AMAP_SMALL(amap)) {
		amap->am_small.ac_nslot = slots;
		return (amap);
	}

	amap->am_ncused = 0;
	TAILQ_INIT(&amap->am_chunks);
	amap->am_hashshift = hashshift;
	amap->am_buckets = NULL;

	buckets = howmany(chunks, chunkperbucket);
	amap->am_buckets = mallocarray(buckets, sizeof(*amap->am_buckets),
	    M_UVMAMAP, waitf | (lazyalloc ? M_ZERO : 0));
	if (amap->am_buckets == NULL)
		goto fail1;

	if (!lazyalloc) {
		for (i = 0; i < buckets; i++) {
			if (i == buckets - 1) {
				n = slots % UVM_AMAP_CHUNK;
				if (n == 0)
					n = UVM_AMAP_CHUNK;
			} else
				n = UVM_AMAP_CHUNK;

			chunk = pool_get(&uvm_amap_chunk_pool,
			    PR_ZERO | pwaitf);
			if (chunk == NULL)
				goto fail1;

			amap->am_buckets[i] = chunk;
			amap->am_ncused++;
			chunk->ac_baseslot = i * UVM_AMAP_CHUNK;
			chunk->ac_nslot = n;
			TAILQ_INSERT_TAIL(&amap->am_chunks, chunk, ac_list);
		}
	}

	return(amap);

fail1:
	free(amap->am_buckets, M_UVMAMAP, buckets * sizeof(*amap->am_buckets));
	TAILQ_FOREACH_SAFE(chunk, &amap->am_chunks, ac_list, tmp)
		pool_put(&uvm_amap_chunk_pool, chunk);
	pool_put(&uvm_amap_pool, amap);
	return (NULL);
}

/*
 * amap_alloc: allocate an amap to manage "sz" bytes of anonymous VM
 *
 * => caller should ensure sz is a multiple of PAGE_SIZE
 * => reference count to new amap is set to one
 */
struct vm_amap *
amap_alloc(vaddr_t sz, int waitf, int lazyalloc)
{
	struct vm_amap *amap;
	size_t slots;

	AMAP_B2SLOT(slots, sz);		/* load slots */
	if (slots > INT_MAX)
		return (NULL);

	amap = amap_alloc1(slots, waitf, lazyalloc);
	if (amap)
		amap_list_insert(amap);

	return(amap);
}


/*
 * amap_free: free an amap
 *
 * => the amap should have a zero reference count and be empty
 */
void
amap_free(struct vm_amap *amap)
{
	struct vm_amap_chunk *chunk, *tmp;

	KASSERT(amap->am_ref == 0 && amap->am_nused == 0);
	KASSERT((amap->am_flags & AMAP_SWAPOFF) == 0);

#ifdef UVM_AMAP_PPREF
	if (amap->am_ppref && amap->am_ppref != PPREF_NONE)
		free(amap->am_ppref, M_UVMAMAP, amap->am_nslot * sizeof(int));
#endif

	if (UVM_AMAP_SMALL(amap))
		pool_put(&uvm_small_amap_pool[amap->am_nslot - 1], amap);
	else {
		TAILQ_FOREACH_SAFE(chunk, &amap->am_chunks, ac_list, tmp)
		    pool_put(&uvm_amap_chunk_pool, chunk);
		free(amap->am_buckets, M_UVMAMAP, 0);
		pool_put(&uvm_amap_pool, amap);
	}
}

/*
 * amap_wipeout: wipeout all anon's in an amap; then free the amap!
 *
 * => called from amap_unref when the final reference to an amap is
 *	discarded (i.e. when reference count == 1)
 */

void
amap_wipeout(struct vm_amap *amap)
{
	int slot;
	struct vm_anon *anon;
	struct vm_amap_chunk *chunk;

	KASSERT(amap->am_ref == 0);

	if (__predict_false((amap->am_flags & AMAP_SWAPOFF) != 0)) {
		/* amap_swap_off will call us again. */
		return;
	}
	amap_list_remove(amap);

	AMAP_CHUNK_FOREACH(chunk, amap) {
		int i, refs, map = chunk->ac_usedmap;

		for (i = ffs(map); i != 0; i = ffs(map)) {
			slot = i - 1;
			map ^= 1 << slot;
			anon = chunk->ac_anon[slot];

			if (anon == NULL || anon->an_ref == 0)
				panic("amap_wipeout: corrupt amap");

			refs = --anon->an_ref;
			if (refs == 0) {
				/*
				 * we had the last reference to a vm_anon.
				 * free it.
				 */
				uvm_anfree(anon);
			}
		}
	}

	/* now we free the map */
	amap->am_ref = 0;	/* ... was one */
	amap->am_nused = 0;
	amap_free(amap);	/* will free amap */
}

/*
 * amap_copy: ensure that a map entry's "needs_copy" flag is false
 *	by copying the amap if necessary.
 * 
 * => an entry with a null amap pointer will get a new (blank) one.
 * => if canchunk is true, then we may clip the entry into a chunk
 * => "startva" and "endva" are used only if canchunk is true.  they are
 *     used to limit chunking (e.g. if you have a large space that you
 *     know you are going to need to allocate amaps for, there is no point
 *     in allowing that to be chunked)
 */

void
amap_copy(struct vm_map *map, struct vm_map_entry *entry, int waitf,
    boolean_t canchunk, vaddr_t startva, vaddr_t endva)
{
	struct vm_amap *amap, *srcamap;
	int slots, lcv, lazyalloc = 0;
	vaddr_t chunksize;
	int i, j, k, n, srcslot;
	struct vm_amap_chunk *chunk = NULL, *srcchunk = NULL;

	/* is there a map to copy?   if not, create one from scratch. */
	if (entry->aref.ar_amap == NULL) {
		/*
		 * check to see if we have a large amap that we can
		 * chunk.  we align startva/endva to chunk-sized
		 * boundaries and then clip to them.
		 *
		 * if we cannot chunk the amap, allocate it in a way
		 * that makes it grow or shrink dynamically with
		 * the number of slots.
		 */
		if (atop(entry->end - entry->start) >= UVM_AMAP_LARGE) {
			if (canchunk) {
				/* convert slots to bytes */
				chunksize = UVM_AMAP_CHUNK << PAGE_SHIFT;
				startva = (startva / chunksize) * chunksize;
				endva = roundup(endva, chunksize);
				UVM_MAP_CLIP_START(map, entry, startva);
				/* watch out for endva wrap-around! */
				if (endva >= startva)
					UVM_MAP_CLIP_END(map, entry, endva);
			} else
				lazyalloc = 1;
		}

		entry->aref.ar_pageoff = 0;
		entry->aref.ar_amap = amap_alloc(entry->end - entry->start,
		    waitf, lazyalloc);
		if (entry->aref.ar_amap != NULL)
			entry->etype &= ~UVM_ET_NEEDSCOPY;
		return;
	}

	/*
	 * first check and see if we are the only map entry
	 * referencing the amap we currently have.  if so, then we can
	 * just take it over rather than copying it.  the value can only
	 * be one if we have the only reference to the amap
	 */
	if (entry->aref.ar_amap->am_ref == 1) {
		entry->etype &= ~UVM_ET_NEEDSCOPY;
		return;
	}

	/* looks like we need to copy the map. */
	AMAP_B2SLOT(slots, entry->end - entry->start);
	if (!UVM_AMAP_SMALL(entry->aref.ar_amap) &&
	    entry->aref.ar_amap->am_hashshift != 0)
		lazyalloc = 1;
	amap = amap_alloc1(slots, waitf, lazyalloc);
	if (amap == NULL)
		return;
	srcamap = entry->aref.ar_amap;

	/*
	 * need to double check reference count now.  the reference count
	 * could have changed while we were in malloc.  if the reference count
	 * dropped down to one we take over the old map rather than
	 * copying the amap.
	 */
	if (srcamap->am_ref == 1) {		/* take it over? */
		entry->etype &= ~UVM_ET_NEEDSCOPY;
		amap->am_ref--;		/* drop final reference to map */
		amap_free(amap);	/* dispose of new (unused) amap */
		return;
	}

	/* we must copy it now. */
	for (lcv = 0; lcv < slots; lcv += n) {
		srcslot = entry->aref.ar_pageoff + lcv;
		i = UVM_AMAP_SLOTIDX(lcv);
		j = UVM_AMAP_SLOTIDX(srcslot);
		n = UVM_AMAP_CHUNK;
		if (i > j)
			n -= i;
		else
			n -= j;
		if (lcv + n > slots)
			n = slots - lcv;

		srcchunk = amap_chunk_get(srcamap, srcslot, 0, PR_NOWAIT);
		if (srcchunk == NULL)
			continue;

		chunk = amap_chunk_get(amap, lcv, 1, PR_NOWAIT);
		if (chunk == NULL) {
			amap->am_ref = 0;
			amap_wipeout(amap);
			return;
		}

		for (k = 0; k < n; i++, j++, k++) {
			chunk->ac_anon[i] = srcchunk->ac_anon[j];
			if (chunk->ac_anon[i] == NULL)
				continue;

			chunk->ac_usedmap |= (1 << i);
			chunk->ac_anon[i]->an_ref++;
			amap->am_nused++;
		}
	}

	/*
	 * drop our reference to the old amap (srcamap).
	 * we know that the reference count on srcamap is greater than
	 * one (we checked above), so there is no way we could drop
	 * the count to zero.  [and no need to worry about freeing it]
	 */
	srcamap->am_ref--;
	if (srcamap->am_ref == 1 && (srcamap->am_flags & AMAP_SHARED) != 0)
		srcamap->am_flags &= ~AMAP_SHARED;   /* clear shared flag */
#ifdef UVM_AMAP_PPREF
	if (srcamap->am_ppref && srcamap->am_ppref != PPREF_NONE) {
		amap_pp_adjref(srcamap, entry->aref.ar_pageoff, 
		    (entry->end - entry->start) >> PAGE_SHIFT, -1);
	}
#endif

	/* install new amap. */
	entry->aref.ar_pageoff = 0;
	entry->aref.ar_amap = amap;
	entry->etype &= ~UVM_ET_NEEDSCOPY;

	amap_list_insert(amap);
}

/*
 * amap_cow_now: resolve all copy-on-write faults in an amap now for fork(2)
 *
 *	called during fork(2) when the parent process has a wired map
 *	entry.   in that case we want to avoid write-protecting pages
 *	in the parent's map (e.g. like what you'd do for a COW page)
 *	so we resolve the COW here.
 *
 * => assume parent's entry was wired, thus all pages are resident.
 * => caller passes child's map/entry in to us
 * => XXXCDC: out of memory should cause fork to fail, but there is
 *	currently no easy way to do this (needs fix)
 */

void
amap_cow_now(struct vm_map *map, struct vm_map_entry *entry)
{
	struct vm_amap *amap = entry->aref.ar_amap;
	int slot;
	struct vm_anon *anon, *nanon;
	struct vm_page *pg, *npg;
	struct vm_amap_chunk *chunk;

	/*
	 * note that if we wait, we must ReStart the "lcv" for loop because
	 * some other process could reorder the anon's in the
	 * am_anon[] array on us.
	 */
ReStart:
	AMAP_CHUNK_FOREACH(chunk, amap) {
		int i, map = chunk->ac_usedmap;

		for (i = ffs(map); i != 0; i = ffs(map)) {
			slot = i - 1;
			map ^= 1 << slot;
			anon = chunk->ac_anon[slot];
			pg = anon->an_page;

			/* page must be resident since parent is wired */
			if (pg == NULL)
				panic("amap_cow_now: non-resident wired page"
				    " in anon %p", anon);

			/*
			 * if the anon ref count is one, we are safe (the child
			 * has exclusive access to the page).
			 */
			if (anon->an_ref <= 1)
				continue;

			/*
			 * if the page is busy then we have to wait for
			 * it and then restart.
			 */
			if (pg->pg_flags & PG_BUSY) {
				atomic_setbits_int(&pg->pg_flags, PG_WANTED);
				UVM_WAIT(pg, FALSE, "cownow", 0);
				goto ReStart;
			}

			/* ok, time to do a copy-on-write to a new anon */
			nanon = uvm_analloc();
			if (nanon) {
				npg = uvm_pagealloc(NULL, 0, nanon, 0);
			} else
				npg = NULL;	/* XXX: quiet gcc warning */

			if (nanon == NULL || npg == NULL) {
				/* out of memory */
				/*
				 * XXXCDC: we should cause fork to fail, but
				 * we can't ...
				 */
				if (nanon) {
					uvm_anfree(nanon);
				}
				uvm_wait("cownowpage");
				goto ReStart;
			}

			/*
			 * got it... now we can copy the data and replace anon
			 * with our new one...
			 */
			uvm_pagecopy(pg, npg);		/* old -> new */
			anon->an_ref--;			/* can't drop to zero */
			chunk->ac_anon[slot] = nanon;	/* replace */

			/*
			 * drop PG_BUSY on new page ... since we have had its
			 * owner locked the whole time it can't be
			 * PG_RELEASED | PG_WANTED.
			 */
			atomic_clearbits_int(&npg->pg_flags, PG_BUSY|PG_FAKE);
			UVM_PAGE_OWN(npg, NULL);
			uvm_lock_pageq();
			uvm_pageactivate(npg);
			uvm_unlock_pageq();
		}
	}
}

/*
 * amap_splitref: split a single reference into two separate references
 *
 * => called from uvm_map's clip routines
 */
void
amap_splitref(struct vm_aref *origref, struct vm_aref *splitref, vaddr_t offset)
{
	int leftslots;

	AMAP_B2SLOT(leftslots, offset);
	if (leftslots == 0)
		panic("amap_splitref: split at zero offset");

	/* now: we have a valid am_mapped array. */
	if (origref->ar_amap->am_nslot - origref->ar_pageoff - leftslots <= 0)
		panic("amap_splitref: map size check failed");

#ifdef UVM_AMAP_PPREF
        /* establish ppref before we add a duplicate reference to the amap */
	if (origref->ar_amap->am_ppref == NULL)
		amap_pp_establish(origref->ar_amap);
#endif

	splitref->ar_amap = origref->ar_amap;
	splitref->ar_amap->am_ref++;		/* not a share reference */
	splitref->ar_pageoff = origref->ar_pageoff + leftslots;
}

#ifdef UVM_AMAP_PPREF

/*
 * amap_pp_establish: add a ppref array to an amap, if possible
 */
void
amap_pp_establish(struct vm_amap *amap)
{

	amap->am_ppref = mallocarray(amap->am_nslot, sizeof(int),
	    M_UVMAMAP, M_NOWAIT|M_ZERO);

	/* if we fail then we just won't use ppref for this amap */
	if (amap->am_ppref == NULL) {
		amap->am_ppref = PPREF_NONE;	/* not using it */
		return;
	}

	/* init ppref */
	pp_setreflen(amap->am_ppref, 0, amap->am_ref, amap->am_nslot);
}

/*
 * amap_pp_adjref: adjust reference count to a part of an amap using the
 * per-page reference count array.
 *
 * => caller must check that ppref != PPREF_NONE before calling
 */
void
amap_pp_adjref(struct vm_amap *amap, int curslot, vsize_t slotlen, int adjval)
{
 	int stopslot, *ppref, lcv, prevlcv;
 	int ref, len, prevref, prevlen;

	stopslot = curslot + slotlen;
	ppref = amap->am_ppref;
 	prevlcv = 0;

	/*
 	 * first advance to the correct place in the ppref array,
 	 * fragment if needed.
	 */
	for (lcv = 0 ; lcv < curslot ; lcv += len) {
		pp_getreflen(ppref, lcv, &ref, &len);
		if (lcv + len > curslot) {     /* goes past start? */
			pp_setreflen(ppref, lcv, ref, curslot - lcv);
			pp_setreflen(ppref, curslot, ref, len - (curslot -lcv));
			len = curslot - lcv;   /* new length of entry @@ lcv */
		}
		prevlcv = lcv;
	}
	if (lcv != 0)
		pp_getreflen(ppref, prevlcv, &prevref, &prevlen);
	else {
		/* Ensure that the "prevref == ref" test below always
		 * fails, since we're starting from the beginning of
		 * the ppref array; that is, there is no previous
		 * chunk.  
		 */
		prevref = -1;
		prevlen = 0;
	}

	/*
	 * now adjust reference counts in range.  merge the first
	 * changed entry with the last unchanged entry if possible.
	 */
	if (lcv != curslot)
		panic("amap_pp_adjref: overshot target");

	for (/* lcv already set */; lcv < stopslot ; lcv += len) {
		pp_getreflen(ppref, lcv, &ref, &len);
		if (lcv + len > stopslot) {     /* goes past end? */
			pp_setreflen(ppref, lcv, ref, stopslot - lcv);
			pp_setreflen(ppref, stopslot, ref,
			    len - (stopslot - lcv));
			len = stopslot - lcv;
		}
		ref += adjval;
		if (ref < 0)
			panic("amap_pp_adjref: negative reference count");
		if (lcv == prevlcv + prevlen && ref == prevref) {
			pp_setreflen(ppref, prevlcv, ref, prevlen + len);
		} else {
			pp_setreflen(ppref, lcv, ref, len);
		}
		if (ref == 0)
			amap_wiperange(amap, lcv, len);
	}

}

void
amap_wiperange_chunk(struct vm_amap *amap, struct vm_amap_chunk *chunk,
    int slotoff, int slots)
{
	int curslot, i, map;
	int startbase, endbase;
	struct vm_anon *anon;

	startbase = AMAP_BASE_SLOT(slotoff);
	endbase = AMAP_BASE_SLOT(slotoff + slots - 1);

	map = chunk->ac_usedmap;
	if (startbase == chunk->ac_baseslot)
		map &= ~((1 << (slotoff - startbase)) - 1);
	if (endbase == chunk->ac_baseslot)
		map &= (1 << (slotoff + slots - endbase)) - 1;

	for (i = ffs(map); i != 0; i = ffs(map)) {
		int refs;

		curslot = i - 1;
		map ^= 1 << curslot;
		chunk->ac_usedmap ^= 1 << curslot;
		anon = chunk->ac_anon[curslot];

		/* remove it from the amap */
		chunk->ac_anon[curslot] = NULL;

		amap->am_nused--;

		/* drop anon reference count */
		refs = --anon->an_ref;
		if (refs == 0) {
			/*
			 * we just eliminated the last reference to an
			 * anon.  free it.
			 */
			uvm_anfree(anon);
		}
	}
}

/*
 * amap_wiperange: wipe out a range of an amap
 * [different from amap_wipeout because the amap is kept intact]
 */
void
amap_wiperange(struct vm_amap *amap, int slotoff, int slots)
{
	int bucket, startbucket, endbucket;
	struct vm_amap_chunk *chunk, *nchunk;

	startbucket = UVM_AMAP_BUCKET(amap, slotoff);
	endbucket = UVM_AMAP_BUCKET(amap, slotoff + slots - 1);

	/*
	 * we can either traverse the amap by am_chunks or by am_buckets
	 * depending on which is cheaper.    decide now.
	 */
	if (UVM_AMAP_SMALL(amap))
		amap_wiperange_chunk(amap, &amap->am_small, slotoff, slots);
	else if (endbucket + 1 - startbucket >= amap->am_ncused) {
		TAILQ_FOREACH_SAFE(chunk, &amap->am_chunks, ac_list, nchunk) {
			if (chunk->ac_baseslot + chunk->ac_nslot <= slotoff)
				continue;
			if (chunk->ac_baseslot >= slotoff + slots)
				continue;

			amap_wiperange_chunk(amap, chunk, slotoff, slots);
			if (chunk->ac_usedmap == 0)
				amap_chunk_free(amap, chunk);
		}
	} else {
		for (bucket = startbucket; bucket <= endbucket; bucket++) {
			for (chunk = amap->am_buckets[bucket]; chunk != NULL;
			    chunk = nchunk) {
				nchunk = TAILQ_NEXT(chunk, ac_list);

				if (UVM_AMAP_BUCKET(amap, chunk->ac_baseslot) !=
				    bucket)
					break;
				if (chunk->ac_baseslot + chunk->ac_nslot <=
				    slotoff)
					continue;
				if (chunk->ac_baseslot >= slotoff + slots)
					continue;

				amap_wiperange_chunk(amap, chunk, slotoff,
				    slots);
				if (chunk->ac_usedmap == 0)
					amap_chunk_free(amap, chunk);
			}
		}
	}
}

#endif

/*
 * amap_swap_off: pagein anonymous pages in amaps and drop swap slots.
 *
 * => note that we don't always traverse all anons.
 *    eg. amaps being wiped out, released anons.
 * => return TRUE if failed.
 */

boolean_t
amap_swap_off(int startslot, int endslot)
{
	struct vm_amap *am;
	struct vm_amap *am_next;
	boolean_t rv = FALSE;

	for (am = LIST_FIRST(&amap_list); am != NULL && !rv; am = am_next) {
		int i, map;
		struct vm_amap_chunk *chunk;

again:
		AMAP_CHUNK_FOREACH(chunk, am) {
			map = chunk->ac_usedmap;

			for (i = ffs(map); i != 0; i = ffs(map)) {
				int swslot;
				int slot = i - 1;
				struct vm_anon *anon;

				map ^= 1 << slot;
				anon = chunk->ac_anon[slot];

				swslot = anon->an_swslot;
				if (swslot < startslot || endslot <= swslot) {
					continue;
				}

				am->am_flags |= AMAP_SWAPOFF;

				rv = uvm_anon_pagein(anon);

				am->am_flags &= ~AMAP_SWAPOFF;
				if (rv || amap_refs(am) == 0)
					goto nextamap;
				goto again;
			}
		}

nextamap:
		am_next = LIST_NEXT(am, am_list);
		if (amap_refs(am) == 0)
			amap_wipeout(am);
	}

	return rv;
}

/*
 * amap_lookup: look up a page in an amap
 */
struct vm_anon *
amap_lookup(struct vm_aref *aref, vaddr_t offset)
{
	int slot;
	struct vm_amap *amap = aref->ar_amap;
	struct vm_amap_chunk *chunk;

	AMAP_B2SLOT(slot, offset);
	slot += aref->ar_pageoff;

	if (slot >= amap->am_nslot)
		panic("amap_lookup: offset out of range");

	chunk = amap_chunk_get(amap, slot, 0, PR_NOWAIT);
	if (chunk == NULL)
		return NULL;

	return chunk->ac_anon[UVM_AMAP_SLOTIDX(slot)];
}

/*
 * amap_lookups: look up a range of pages in an amap
 *
 * => XXXCDC: this interface is biased toward array-based amaps.  fix.
 */
void
amap_lookups(struct vm_aref *aref, vaddr_t offset,
    struct vm_anon **anons, int npages)
{
	int i, lcv, n, slot;
	struct vm_amap *amap = aref->ar_amap;
	struct vm_amap_chunk *chunk = NULL;

	AMAP_B2SLOT(slot, offset);
	slot += aref->ar_pageoff;

	if ((slot + (npages - 1)) >= amap->am_nslot)
		panic("amap_lookups: offset out of range");

	for (i = 0, lcv = slot; lcv < slot + npages; i += n, lcv += n) {
		n = UVM_AMAP_CHUNK - UVM_AMAP_SLOTIDX(lcv);
		if (lcv + n > slot + npages)
			n = slot + npages - lcv;

		chunk = amap_chunk_get(amap, lcv, 0, PR_NOWAIT);
		if (chunk == NULL)
			memset(&anons[i], 0, n * sizeof(*anons));
		else
			memcpy(&anons[i],
			    &chunk->ac_anon[UVM_AMAP_SLOTIDX(lcv)],
			    n * sizeof(*anons));
	}
}

/*
 * amap_populate: ensure that the amap can store an anon for the page at
 * offset. This function can sleep until memory to store the anon is
 * available.
 */
void
amap_populate(struct vm_aref *aref, vaddr_t offset)
{
	int slot;
	struct vm_amap *amap = aref->ar_amap;
	struct vm_amap_chunk *chunk;

	AMAP_B2SLOT(slot, offset);
	slot += aref->ar_pageoff;

	if (slot >= amap->am_nslot)
		panic("amap_populate: offset out of range");

	chunk = amap_chunk_get(amap, slot, 1, PR_WAITOK);
	KASSERT(chunk != NULL);
}

/*
 * amap_add: add (or replace) a page to an amap
 *
 * => returns 0 if adding the page was successful or 1 when not.
 */
int
amap_add(struct vm_aref *aref, vaddr_t offset, struct vm_anon *anon,
    boolean_t replace)
{
	int slot;
	struct vm_amap *amap = aref->ar_amap;
	struct vm_amap_chunk *chunk;

	AMAP_B2SLOT(slot, offset);
	slot += aref->ar_pageoff;

	if (slot >= amap->am_nslot)
		panic("amap_add: offset out of range");
	chunk = amap_chunk_get(amap, slot, 1, PR_NOWAIT);
	if (chunk == NULL)
		return 1;

	slot = UVM_AMAP_SLOTIDX(slot);
	if (replace) {
		if (chunk->ac_anon[slot] == NULL)
			panic("amap_add: replacing null anon");
		if (chunk->ac_anon[slot]->an_page != NULL &&
		    (amap->am_flags & AMAP_SHARED) != 0) {
			pmap_page_protect(chunk->ac_anon[slot]->an_page,
			    PROT_NONE);
			/*
			 * XXX: suppose page is supposed to be wired somewhere?
			 */
		}
	} else {   /* !replace */
		if (chunk->ac_anon[slot] != NULL)
			panic("amap_add: slot in use");

		chunk->ac_usedmap |= 1 << slot;
		amap->am_nused++;
	}
	chunk->ac_anon[slot] = anon;

	return 0;
}

/*
 * amap_unadd: remove a page from an amap
 */
void
amap_unadd(struct vm_aref *aref, vaddr_t offset)
{
	int slot;
	struct vm_amap *amap = aref->ar_amap;
	struct vm_amap_chunk *chunk;

	AMAP_B2SLOT(slot, offset);
	slot += aref->ar_pageoff;

	if (slot >= amap->am_nslot)
		panic("amap_unadd: offset out of range");
	chunk = amap_chunk_get(amap, slot, 0, PR_NOWAIT);
	if (chunk == NULL)
		panic("amap_unadd: chunk for slot %d not present", slot);

	slot = UVM_AMAP_SLOTIDX(slot);
	if (chunk->ac_anon[slot] == NULL)
		panic("amap_unadd: nothing there");

	chunk->ac_anon[slot] = NULL;
	chunk->ac_usedmap &= ~(1 << slot);
	amap->am_nused--;

	if (chunk->ac_usedmap == 0)
		amap_chunk_free(amap, chunk);
}

/*
 * amap_ref: gain a reference to an amap
 *
 * => "offset" and "len" are in units of pages
 * => called at fork time to gain the child's reference
 */
void
amap_ref(struct vm_amap *amap, vaddr_t offset, vsize_t len, int flags)
{

	amap->am_ref++;
	if (flags & AMAP_SHARED)
		amap->am_flags |= AMAP_SHARED;
#ifdef UVM_AMAP_PPREF
	if (amap->am_ppref == NULL && (flags & AMAP_REFALL) == 0 &&
	    len != amap->am_nslot)
		amap_pp_establish(amap);
	if (amap->am_ppref && amap->am_ppref != PPREF_NONE) {
		if (flags & AMAP_REFALL)
			amap_pp_adjref(amap, 0, amap->am_nslot, 1);
		else
			amap_pp_adjref(amap, offset, len, 1);
	}
#endif
}

/*
 * amap_unref: remove a reference to an amap
 *
 * => caller must remove all pmap-level references to this amap before
 *	dropping the reference
 * => called from uvm_unmap_detach [only]  ... note that entry is no
 *	longer part of a map
 */
void
amap_unref(struct vm_amap *amap, vaddr_t offset, vsize_t len, boolean_t all)
{

	/* if we are the last reference, free the amap and return. */
	if (amap->am_ref-- == 1) {
		amap_wipeout(amap);	/* drops final ref and frees */
		return;
	}

	/* otherwise just drop the reference count(s) */
	if (amap->am_ref == 1 && (amap->am_flags & AMAP_SHARED) != 0)
		amap->am_flags &= ~AMAP_SHARED;	/* clear shared flag */
#ifdef UVM_AMAP_PPREF
	if (amap->am_ppref == NULL && all == 0 && len != amap->am_nslot)
		amap_pp_establish(amap);
	if (amap->am_ppref && amap->am_ppref != PPREF_NONE) {
		if (all)
			amap_pp_adjref(amap, 0, amap->am_nslot, -1);
		else
			amap_pp_adjref(amap, offset, len, -1);
	}
#endif
}
@


1.78
log
@Prevent infinite loops for amap allocations with >= 2^17 slots

This was caused by an integer overflow in a loop. mlarkin@@
noticed the hang when trying to run a vmm(4) guest with lots of RAM.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.77 2016/09/15 02:00:18 dlg Exp $	*/
d371 1
a371 1
	free(amap->am_buckets, M_UVMAMAP, 0);
d417 1
a417 1
		free(amap->am_ppref, M_UVMAMAP, 0);
@


1.77
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.76 2016/07/27 14:48:56 tedu Exp $	*/
d267 1
a267 1
	int chunks, chunkperbucket = 1, hashshift = 0;
d304 3
d308 1
a308 1
		while ((1 << chunkperbucket) * 2 <= chunks) {
@


1.76
log
@check flags with mask instead of equality, in case we decide to mix
another flag in at some point. ok stefan
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.75 2016/07/14 16:23:49 stefan Exp $	*/
d238 2
a239 3
	pool_init(&uvm_amap_pool, sizeof(struct vm_amap), 0, 0, PR_WAITOK,
	    "amappl", NULL);
	pool_setipl(&uvm_amap_pool, IPL_NONE);
d248 2
a249 3
		pool_init(&uvm_small_amap_pool[i], size, 0, 0, 0,
		    amap_small_pool_names[i], NULL);
		pool_setipl(&uvm_small_amap_pool[i], IPL_NONE);
d252 3
a254 5
	pool_init(&uvm_amap_chunk_pool,
	    sizeof(struct vm_amap_chunk) +
	    UVM_AMAP_CHUNK * sizeof(struct vm_anon *), 0, 0, 0,
	    "amapchunkpl", NULL);
	pool_setipl(&uvm_amap_chunk_pool, IPL_NONE);
@


1.75
log
@Make sure that amap slot calculation does not overflow

This prevents from too small amaps being allocated by
forcing the allocation of a large number of slots.

Based on an analysis from Jesse Hertz and Tim Newsham.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.74 2016/07/11 08:38:49 stefan Exp $	*/
d273 1
a273 1
	int pwaitf = (waitf == M_WAITOK) ? PR_WAITOK : PR_NOWAIT;
@


1.75.2.1
log
@Backport r1.78 from -current:
Prevent infinite loops for amap allocations with >= 2^17 slots

This was caused by an integer overflow in a loop. mlarkin@@
noticed the hang when trying to run a vmm(4) guest with lots of RAM.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.75 2016/07/14 16:23:49 stefan Exp $	*/
d271 1
a271 1
	int chunks, log_chunks, chunkperbucket = 1, hashshift = 0;
a307 3
		for (log_chunks = 1; (chunks >> log_chunks) > 0; log_chunks++)
			continue;

d309 1
a309 1
		while (chunkperbucket + 1 < log_chunks) {
@


1.74
log
@Make sure variables are used initialized in amap_wiperange

Uninitialized variables used in an if/else could cause a slower
codepath to be taken, but the end effect of both paths is the same.

Found by jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.73 2016/07/09 17:13:05 stefan Exp $	*/
d275 7
a281 1
	chunks = roundup(slots, UVM_AMAP_CHUNK) / UVM_AMAP_CHUNK;
a322 2
	KASSERT(slots > 0);

d389 1
a389 1
	int slots;
d392 2
@


1.73
log
@Fix bugs introduced with the amap rework

- The number of slots must be initialized in the chunk of a small amap,
  otherwise unmapping() part of a mmap()'d range would delay freeing
  of vm_anons for small amaps
- If the first chunk of a bucket is freed, check if the next chunk in
  the list has to become the new first chunk
- Use a separate loop for each type of traversal (small amap, by bucket
  by list) in amap_wiperange(). This makes the code easier to follow and
  also fixes a bug where too many chunks were wiped out when traversing
  by list

However, the last two bugs should happen only when turning a previously
private mapping into a shared one, then forking, and then having
both processes unmap a part of the mapping.

snap and ports build tested by krw@@, review by kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.72 2016/06/17 10:48:25 dlg Exp $	*/
d898 3
a918 3
		startbucket = UVM_AMAP_BUCKET(amap, slotoff);
		endbucket = UVM_AMAP_BUCKET(amap, slotoff + slots - 1);

@


1.72
log
@pool_setipl on all uvm pools.

ok kettenis@@ visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.71 2016/05/26 13:37:26 stefan Exp $	*/
d70 2
d140 1
d145 1
d147 8
a154 2
	if (amap->am_buckets[bucket] == chunk)
		amap->am_buckets[bucket] = NULL;
d327 2
a328 1
	if (UVM_AMAP_SMALL(amap))
d330 1
d846 42
a894 1
	int curslot, i, map, bybucket;
a895 2
	int startbase, endbase;
	struct vm_anon *anon;
d902 4
a905 26
	startbucket = UVM_AMAP_BUCKET(amap, slotoff);
	endbucket = UVM_AMAP_BUCKET(amap, slotoff + slots - 1);

	if (UVM_AMAP_SMALL(amap)) {
		bybucket = FALSE;
		chunk = &amap->am_small;
	} else if (endbucket + 1 - startbucket >= amap->am_ncused) {
		bybucket = FALSE;
		chunk = TAILQ_FIRST(&amap->am_chunks);
	} else {
		bybucket = TRUE;
		bucket = startbucket;
		chunk = amap->am_buckets[bucket];
	}

	startbase = AMAP_BASE_SLOT(slotoff);
	endbase = AMAP_BASE_SLOT(slotoff + slots - 1);
	for (;;) {
		if (chunk == NULL || (bybucket &&
		    UVM_AMAP_BUCKET(amap, chunk->ac_baseslot) != bucket)) {
			if (!bybucket || bucket >= endbucket)
				break;
			bucket++;
			chunk = amap->am_buckets[bucket];
			continue;
		} else if (!bybucket) {
d907 1
a907 1
				goto next;
d909 5
a913 1
				goto next;
d915 3
d919 18
a936 27
		map = chunk->ac_usedmap;
		if (startbase == chunk->ac_baseslot)
			map &= ~((1 << (slotoff - startbase)) - 1);
		if (endbase == chunk->ac_baseslot)
			map &= (1 << (slotoff + slots - endbase)) - 1;

		for (i = ffs(map); i != 0; i = ffs(map)) {
			int refs;

			curslot = i - 1;
			map ^= 1 << curslot;
			chunk->ac_usedmap ^= 1 << curslot;
			anon = chunk->ac_anon[curslot];

			/* remove it from the amap */
			chunk->ac_anon[curslot] = NULL;

			amap->am_nused--;

			/* drop anon reference count */
			refs = --anon->an_ref;
			if (refs == 0) {
				/*
				 * we just eliminated the last reference to an
				 * anon.  free it.
				 */
				uvm_anfree(anon);
a938 6

next:
		nchunk = TAILQ_NEXT(chunk, ac_list);
		if (chunk->ac_usedmap == 0)
			amap_chunk_free(amap, chunk);
		chunk = nchunk;
@


1.71
log
@Make amaps use less kernel memory (2nd try)

The original diff would crash at least i386 and powerpc, as spotted by
guenther@@ The reason was an incorrect use of sizeof in amap_lookups().

Confirmation that powerpc works by mpi@@ and mglocker@@

"throw it in" deraadt@@

Original commit message:

This is achieved by grouping amap slots into chunks that are allocated
on-demand by pool(9). Endless "fltamapcopy" loops because of kmem
shortage should be solved now. The kmem savings are also important to later
enable vmm(4) to use larged shared memory mappings for guest VM RAM.

This adapts libkvm also because the amap structure layout has changed.

Testing and fix of libkvm glitch in initial diff by tb@@
Feedback and "time to get this in" kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.70 2016/05/22 22:52:01 guenther Exp $	*/
d230 1
d241 1
d248 1
@


1.70
log
@Revert previous: breaks i386 and powerpc, probably all non-PMAP_DIRECT archs
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.68 2016/05/08 16:29:57 stefan Exp $	*/
d47 1
a47 1
 * pool for allocation of vm_map structures.  note that in order to
d54 2
a55 3

/* Pools for amap slots for the most common amap slot sizes */
struct pool uvm_amap_slot_pools[UVM_AMAP_CHUNK];
d59 1
a59 3
static char amap_slot_pool_names[UVM_AMAP_CHUNK][13];

#define MALLOC_SLOT_UNIT (2 * sizeof(int) + sizeof(struct vm_anon *))
d69 2
d83 66
d225 1
d232 8
a239 6
	for (i = 0; i < nitems(uvm_amap_slot_pools); i++) {
		snprintf(amap_slot_pool_names[i],
		    sizeof(amap_slot_pool_names[0]), "amapslotpl%d", i + 1);
		pool_init(&uvm_amap_slot_pools[i], (i + 1) * MALLOC_SLOT_UNIT,
		    0, 0, PR_WAITOK, amap_slot_pool_names[i], NULL);
		pool_sethiwat(&uvm_amap_slot_pools[i], 4096);
d241 6
d257 6
d264 37
a300 2
	amap = pool_get(&uvm_amap_pool, (waitf == M_WAITOK) ? PR_WAITOK
	    : PR_NOWAIT);
d314 2
a315 7
	if (slots > UVM_AMAP_CHUNK)
		amap->am_slots = malloc(slots * MALLOC_SLOT_UNIT,
		    M_UVMAMAP, waitf);
	else
		amap->am_slots = pool_get(
		    &uvm_amap_slot_pools[slots - 1],
		    (waitf == M_WAITOK) ? PR_WAITOK : PR_NOWAIT);
d317 9
a325 1
	if (amap->am_slots == NULL)
d328 21
a348 4
	amap->am_bckptr = (int *)(((char *)amap->am_slots) + slots *
	    sizeof(int));
	amap->am_anon = (struct vm_anon **)(((char *)amap->am_bckptr) +
	    slots * sizeof(int));
d353 3
d375 1
a375 3
	if (amap) {
		memset(amap->am_anon, 0,
		    amap->am_nslot * sizeof(struct vm_anon *));
a376 1
	}
d390 1
a394 6
	if (amap->am_nslot > UVM_AMAP_CHUNK)
		free(amap->am_slots, M_UVMAMAP, 0);
	else
		pool_put(&uvm_amap_slot_pools[amap->am_nslot - 1],
		    amap->am_slots);

a398 1
	pool_put(&uvm_amap_pool, amap);
d400 8
d420 1
a420 1
	int lcv, slot;
d422 1
d432 2
a433 2
	for (lcv = 0 ; lcv < amap->am_nused ; lcv++) {
		int refs;
d435 4
a438 2
		slot = amap->am_slots[lcv];
		anon = amap->am_anon[slot];
d440 2
a441 2
		if (anon == NULL || anon->an_ref == 0)
			panic("amap_wipeout: corrupt amap");
d443 8
a450 4
		refs = --anon->an_ref;
		if (refs == 0) {
			/* we had the last reference to a vm_anon. free it. */
			uvm_anfree(anon);
d479 2
d528 3
d550 14
a563 4
	for (lcv = 0 ; lcv < slots; lcv++) {
		amap->am_anon[lcv] =
		    srcamap->am_anon[entry->aref.ar_pageoff + lcv];
		if (amap->am_anon[lcv] == NULL)
d565 17
a581 4
		amap->am_anon[lcv]->an_ref++;
		amap->am_bckptr[lcv] = amap->am_nused;
		amap->am_slots[amap->am_nused] = lcv;
		amap->am_nused++;
a582 1
	KASSERT(lcv == amap->am_nslot);
d626 1
a626 1
	int lcv, slot;
d629 1
d637 20
a656 10
	for (lcv = 0 ; lcv < amap->am_nused ; lcv++) {
		/* get the page */
		slot = amap->am_slots[lcv];
		anon = amap->am_anon[slot];
		pg = anon->an_page;

		/* page must be resident since parent is wired */
		if (pg == NULL)
			panic("amap_cow_now: non-resident wired page"
			    " in anon %p", anon);
a657 5
		/*
		 * if the anon ref count is one, we are safe (the child has
		 * exclusive access to the page).
		 */
		if (anon->an_ref > 1) {
d687 1
a687 1
	
d694 1
a694 1
			amap->am_anon[slot] = nanon;	/* replace */
d838 3
a840 1
	int byanon, lcv, stop, curslot, ptr, slotend;
d842 1
d845 2
a846 2
	 * we can either traverse the amap by am_anon or by am_slots depending
	 * on which is cheaper.    decide now.
d848 9
a856 4
	if (slots < amap->am_nused) {
		byanon = TRUE;
		lcv = slotoff;
		stop = slotoff + slots;
d858 3
a860 4
		byanon = FALSE;
		lcv = 0;
		stop = amap->am_nused;
		slotend = slotoff + slots;
d863 44
a906 12
	while (lcv < stop) {
		int refs;

  		if (byanon) {
			curslot = lcv++;	/* lcv advances here */
			if (amap->am_anon[curslot] == NULL)
				continue;
		} else {
			curslot = amap->am_slots[lcv];
			if (curslot < slotoff || curslot >= slotend) {
				lcv++;		/* lcv advances here */
				continue;
a907 1
			stop--;	/* drop stop, since anon will be removed */
a908 1
		anon = amap->am_anon[curslot];
d910 5
a914 20
		/* remove it from the amap */
		amap->am_anon[curslot] = NULL;
		ptr = amap->am_bckptr[curslot];
		if (ptr != (amap->am_nused - 1)) {
			amap->am_slots[ptr] =
			    amap->am_slots[amap->am_nused - 1];
			amap->am_bckptr[amap->am_slots[ptr]] =
			    ptr;    /* back ptr. */
		}
		amap->am_nused--;

		/* drop anon reference count */
		refs = --anon->an_ref;
		if (refs == 0) {
			/*
			 * we just eliminated the last reference to an anon.
			 * free it.
			 */
			uvm_anfree(anon);
		}
d936 19
a954 1
		int i;
d956 1
a956 4
		for (i = 0; i < am->am_nused; i++) {
			int slot;
			int swslot;
			struct vm_anon *anon;
d958 1
a958 2
			slot = am->am_slots[i];
			anon = am->am_anon[slot];
d960 4
a963 3
			swslot = anon->an_swslot;
			if (swslot < startslot || endslot <= swslot) {
				continue;
a964 9

			am->am_flags |= AMAP_SWAPOFF;

			rv = uvm_anon_pagein(anon);

			am->am_flags &= ~AMAP_SWAPOFF;
			if (rv || amap_refs(am) == 0)
				break;
			i = 0;
d967 1
d984 1
d992 5
a996 1
	return(amap->am_anon[slot]);
d1008 1
a1008 1
	int slot;
d1010 1
d1018 13
a1030 3
	memcpy(anons, &amap->am_anon[slot], npages * sizeof(struct vm_anon *));

	return;
d1041 12
d1066 1
d1073 3
d1077 1
d1079 1
a1079 1
		if (amap->am_anon[slot] == NULL)
d1081 1
a1081 1
		if (amap->am_anon[slot]->an_page != NULL && 
d1083 1
a1083 1
			pmap_page_protect(amap->am_anon[slot]->an_page,
d1090 1
a1090 1
		if (amap->am_anon[slot] != NULL)
d1093 1
a1093 2
		amap->am_bckptr[slot] = amap->am_nused;
		amap->am_slots[amap->am_nused] = slot;
d1096 1
a1096 1
	amap->am_anon[slot] = anon;
d1107 1
a1107 1
	int ptr, slot;
d1109 1
d1116 3
d1120 2
a1121 1
	if (amap->am_anon[slot] == NULL)
d1124 3
a1126 2
	amap->am_anon[slot] = NULL;
	ptr = amap->am_bckptr[slot];
d1128 2
a1129 5
	if (ptr != (amap->am_nused - 1)) {	/* swap to keep slots contig? */
		amap->am_slots[ptr] = amap->am_slots[amap->am_nused - 1];
		amap->am_bckptr[amap->am_slots[ptr]] = ptr;	/* back link */
	}
	amap->am_nused--;
@


1.69
log
@Make amaps use less kernel memory

This is achieved by grouping amap slots into chunks that are allocated
on-demand by pool(9). Endless "fltamapcopy" loops because of kmem
shortage should be solved now. The kmem savings are also important to later
enable vmm(4) to use larged shared memory mappings for guest VM RAM.

This adapts libkvm also because the amap structure layout has changed.

Testing and fix of libkvm glitch in initial diff by tb@@
Feedback and "time to get this in" kettenis@@
@
text
@d47 1
a47 1
 * pools for allocation of vm_amap structures.  note that in order to
d54 3
a56 2
struct pool uvm_small_amap_pool[UVM_AMAP_CHUNK];
struct pool uvm_amap_chunk_pool;
d60 1
a60 1
static char amap_small_pool_names[UVM_AMAP_CHUNK][9];
a71 2
struct vm_amap_chunk *amap_chunk_get(struct vm_amap *, int, int, int);

a83 66
/*
 * amap_chunk_get: lookup a chunk for slot. if create is non-zero,
 * the chunk is created if it does not yet exist.
 *
 * => returns the chunk on success or NULL on error
 */
struct vm_amap_chunk *
amap_chunk_get(struct vm_amap *amap, int slot, int create, int waitf)
{
	int bucket = UVM_AMAP_BUCKET(amap, slot);
	int baseslot = AMAP_BASE_SLOT(slot);
	int n;
	struct vm_amap_chunk *chunk, *newchunk, *pchunk = NULL;

	if (UVM_AMAP_SMALL(amap))
		return &amap->am_small;

	for (chunk = amap->am_buckets[bucket]; chunk != NULL;
	    chunk = TAILQ_NEXT(chunk, ac_list)) {
		if (UVM_AMAP_BUCKET(amap, chunk->ac_baseslot) != bucket)
			break;
		if (chunk->ac_baseslot == baseslot)
			return chunk;
		pchunk = chunk;
	}
	if (!create)
		return NULL;

	if (amap->am_nslot - baseslot >= UVM_AMAP_CHUNK)
		n = UVM_AMAP_CHUNK;
	else
		n = amap->am_nslot - baseslot;

	newchunk = pool_get(&uvm_amap_chunk_pool, waitf | PR_ZERO);
	if (newchunk == NULL)
		return NULL;

	if (pchunk == NULL) {
		TAILQ_INSERT_TAIL(&amap->am_chunks, newchunk, ac_list);
		KASSERT(amap->am_buckets[bucket] == NULL);
		amap->am_buckets[bucket] = newchunk;
	} else
		TAILQ_INSERT_AFTER(&amap->am_chunks, pchunk, newchunk,
		    ac_list);

	amap->am_ncused++;
	newchunk->ac_baseslot = baseslot;
	newchunk->ac_nslot = n;
	return newchunk;
}

void
amap_chunk_free(struct vm_amap *amap, struct vm_amap_chunk *chunk)
{
	int bucket = UVM_AMAP_BUCKET(amap, chunk->ac_baseslot);

	if (UVM_AMAP_SMALL(amap))
		return;

	TAILQ_REMOVE(&amap->am_chunks, chunk, ac_list);
	if (amap->am_buckets[bucket] == chunk)
		amap->am_buckets[bucket] = NULL;
	pool_put(&uvm_amap_chunk_pool, chunk);
	amap->am_ncused--;
}

a159 1
	size_t size;
d166 6
a171 8
	/* initialize small amap pools */
	for (i = 0; i < nitems(uvm_small_amap_pool); i++) {
		snprintf(amap_small_pool_names[i],
		    sizeof(amap_small_pool_names[0]), "amappl%d", i + 1);
		size = offsetof(struct vm_amap, am_small.ac_anon) +
		    (i + 1) * sizeof(struct vm_anon *);
		pool_init(&uvm_small_amap_pool[i], size, 0, 0, 0,
		    amap_small_pool_names[i], NULL);
a172 6

	pool_init(&uvm_amap_chunk_pool,
	    sizeof(struct vm_amap_chunk) +
	    UVM_AMAP_CHUNK * sizeof(struct vm_anon *), 0, 0, 0,
	    "amapchunkpl", NULL);
	pool_sethiwat(&uvm_amap_chunk_pool, 4096);
a182 4
	struct vm_amap_chunk *chunk, *tmp;
	int chunks, chunkperbucket = 1, hashshift = 0;
	int buckets, i, n;
	int pwaitf = (waitf == M_WAITOK) ? PR_WAITOK : PR_NOWAIT;
d184 2
a185 39
	chunks = roundup(slots, UVM_AMAP_CHUNK) / UVM_AMAP_CHUNK;

	if (lazyalloc) {
		/*
		 * Basically, the amap is a hash map where the number of
		 * buckets is fixed. We select the number of buckets using the
		 * following strategy:
		 *
		 * 1. The maximal number of entries to search in a bucket upon
		 * a collision should be less than or equal to
		 * log2(slots / UVM_AMAP_CHUNK). This is the worst-case number
		 * of lookups we would have if we could chunk the amap. The
		 * log2(n) comes from the fact that amaps are chunked by
		 * splitting up their vm_map_entries and organizing those
		 * in a binary search tree.
		 *
		 * 2. The maximal number of entries in a bucket must be a
		 * power of two.
		 *
		 * The maximal number of entries per bucket is used to hash
		 * a slot to a bucket.
		 *
		 * In the future, this strategy could be refined to make it
		 * even harder/impossible that the total amount of KVA needed
		 * for the hash buckets of all amaps to exceed the maximal
		 * amount of KVA memory reserved for amaps.
		 */
		chunkperbucket = 1 << hashshift;
		while ((1 << chunkperbucket) * 2 <= chunks) {
			hashshift++;
			chunkperbucket = 1 << hashshift;
		}
	}

	if (slots > UVM_AMAP_CHUNK)
		amap = pool_get(&uvm_amap_pool, pwaitf);
	else
		amap = pool_get(&uvm_small_amap_pool[slots - 1],
		    pwaitf | PR_ZERO);
d199 7
a205 2
	if (UVM_AMAP_SMALL(amap))
		return (amap);
d207 1
a207 9
	amap->am_ncused = 0;
	TAILQ_INIT(&amap->am_chunks);
	amap->am_hashshift = hashshift;
	amap->am_buckets = NULL;

	buckets = howmany(chunks, chunkperbucket);
	amap->am_buckets = mallocarray(buckets, sizeof(amap->am_buckets),
	    M_UVMAMAP, waitf | (lazyalloc ? M_ZERO : 0));
	if (amap->am_buckets == NULL)
d210 4
a213 21
	if (!lazyalloc) {
		for (i = 0; i < buckets; i++) {
			if (i == buckets - 1) {
				n = slots % UVM_AMAP_CHUNK;
				if (n == 0)
					n = UVM_AMAP_CHUNK;
			} else
				n = UVM_AMAP_CHUNK;

			chunk = pool_get(&uvm_amap_chunk_pool,
			    PR_ZERO | pwaitf);
			if (chunk == NULL)
				goto fail1;

			amap->am_buckets[i] = chunk;
			amap->am_ncused++;
			chunk->ac_baseslot = i * UVM_AMAP_CHUNK;
			chunk->ac_nslot = n;
			TAILQ_INSERT_TAIL(&amap->am_chunks, chunk, ac_list);
		}
	}
a217 3
	free(amap->am_buckets, M_UVMAMAP, 0);
	TAILQ_FOREACH_SAFE(chunk, &amap->am_chunks, ac_list, tmp)
		pool_put(&uvm_amap_chunk_pool, chunk);
d237 3
a239 1
	if (amap)
d241 1
a254 1
	struct vm_amap_chunk *chunk, *tmp;
d259 6
d269 1
a270 8
	if (UVM_AMAP_SMALL(amap))
		pool_put(&uvm_small_amap_pool[amap->am_nslot - 1], amap);
	else {
		TAILQ_FOREACH_SAFE(chunk, &amap->am_chunks, ac_list, tmp)
		    pool_put(&uvm_amap_chunk_pool, chunk);
		free(amap->am_buckets, M_UVMAMAP, 0);
		pool_put(&uvm_amap_pool, amap);
	}
d283 1
a283 1
	int slot;
a284 1
	struct vm_amap_chunk *chunk;
d294 2
a295 2
	AMAP_CHUNK_FOREACH(chunk, amap) {
		int i, refs, map = chunk->ac_usedmap;
d297 2
a298 4
		for (i = ffs(map); i != 0; i = ffs(map)) {
			slot = i - 1;
			map ^= 1 << slot;
			anon = chunk->ac_anon[slot];
d300 2
a301 2
			if (anon == NULL || anon->an_ref == 0)
				panic("amap_wipeout: corrupt amap");
d303 4
a306 8
			refs = --anon->an_ref;
			if (refs == 0) {
				/*
				 * we had the last reference to a vm_anon.
				 * free it.
				 */
				uvm_anfree(anon);
			}
a334 2
	int i, j, k, n, srcslot;
	struct vm_amap_chunk *chunk = NULL, *srcchunk = NULL;
a381 3
	if (!UVM_AMAP_SMALL(entry->aref.ar_amap) &&
	    entry->aref.ar_amap->am_hashshift != 0)
		lazyalloc = 1;
d401 4
a404 14
	for (lcv = 0; lcv < slots; lcv += n) {
		srcslot = entry->aref.ar_pageoff + lcv;
		i = UVM_AMAP_SLOTIDX(lcv);
		j = UVM_AMAP_SLOTIDX(srcslot);
		n = UVM_AMAP_CHUNK;
		if (i > j)
			n -= i;
		else
			n -= j;
		if (lcv + n > slots)
			n = slots - lcv;

		srcchunk = amap_chunk_get(srcamap, srcslot, 0, PR_NOWAIT);
		if (srcchunk == NULL)
d406 4
a409 17

		chunk = amap_chunk_get(amap, lcv, 1, PR_NOWAIT);
		if (chunk == NULL) {
			amap->am_ref = 0;
			amap_wipeout(amap);
			return;
		}

		for (k = 0; k < n; i++, j++, k++) {
			chunk->ac_anon[i] = srcchunk->ac_anon[j];
			if (chunk->ac_anon[i] == NULL)
				continue;

			chunk->ac_usedmap |= (1 << i);
			chunk->ac_anon[i]->an_ref++;
			amap->am_nused++;
		}
d411 1
d455 1
a455 1
	int slot;
a457 1
	struct vm_amap_chunk *chunk;
d465 10
a474 20
	AMAP_CHUNK_FOREACH(chunk, amap) {
		int i, map = chunk->ac_usedmap;

		for (i = ffs(map); i != 0; i = ffs(map)) {
			slot = i - 1;
			map ^= 1 << slot;
			anon = chunk->ac_anon[slot];
			pg = anon->an_page;

			/* page must be resident since parent is wired */
			if (pg == NULL)
				panic("amap_cow_now: non-resident wired page"
				    " in anon %p", anon);

			/*
			 * if the anon ref count is one, we are safe (the child
			 * has exclusive access to the page).
			 */
			if (anon->an_ref <= 1)
				continue;
d476 5
d510 1
a510 1

d517 1
a517 1
			chunk->ac_anon[slot] = nanon;	/* replace */
d661 1
a661 3
	int curslot, i, map, bybucket;
	int bucket, startbucket, endbucket;
	int startbase, endbase;
a662 1
	struct vm_amap_chunk *chunk, *nchunk;
d665 2
a666 2
	 * we can either traverse the amap by am_chunks or by am_buckets
	 * depending on which is cheaper.    decide now.
d668 4
a671 9
	startbucket = UVM_AMAP_BUCKET(amap, slotoff);
	endbucket = UVM_AMAP_BUCKET(amap, slotoff + slots - 1);

	if (UVM_AMAP_SMALL(amap)) {
		bybucket = FALSE;
		chunk = &amap->am_small;
	} else if (endbucket + 1 - startbucket >= amap->am_ncused) {
		bybucket = FALSE;
		chunk = TAILQ_FIRST(&amap->am_chunks);
d673 4
a676 3
		bybucket = TRUE;
		bucket = startbucket;
		chunk = amap->am_buckets[bucket];
d679 14
a692 15
	startbase = AMAP_BASE_SLOT(slotoff);
	endbase = AMAP_BASE_SLOT(slotoff + slots - 1);
	for (;;) {
		if (chunk == NULL || (bybucket &&
		    UVM_AMAP_BUCKET(amap, chunk->ac_baseslot) != bucket)) {
			if (!bybucket || bucket >= endbucket)
				break;
			bucket++;
			chunk = amap->am_buckets[bucket];
			continue;
		} else if (!bybucket) {
			if (chunk->ac_baseslot + chunk->ac_nslot <= slotoff)
				goto next;
			if (chunk->ac_baseslot >= slotoff + slots)
				goto next;
d694 1
d696 19
a714 28
		map = chunk->ac_usedmap;
		if (startbase == chunk->ac_baseslot)
			map &= ~((1 << (slotoff - startbase)) - 1);
		if (endbase == chunk->ac_baseslot)
			map &= (1 << (slotoff + slots - endbase)) - 1;

		for (i = ffs(map); i != 0; i = ffs(map)) {
			int refs;

			curslot = i - 1;
			map ^= 1 << curslot;
			chunk->ac_usedmap ^= 1 << curslot;
			anon = chunk->ac_anon[curslot];

			/* remove it from the amap */
			chunk->ac_anon[curslot] = NULL;

			amap->am_nused--;

			/* drop anon reference count */
			refs = --anon->an_ref;
			if (refs == 0) {
				/*
				 * we just eliminated the last reference to an
				 * anon.  free it.
				 */
				uvm_anfree(anon);
			}
a715 6

next:
		nchunk = TAILQ_NEXT(chunk, ac_list);
		if (chunk->ac_usedmap == 0)
			amap_chunk_free(amap, chunk);
		chunk = nchunk;
d737 9
a745 2
		int i, map;
		struct vm_amap_chunk *chunk;
d747 4
a750 16
again:
		AMAP_CHUNK_FOREACH(chunk, am) {
			map = chunk->ac_usedmap;

			for (i = ffs(map); i != 0; i = ffs(map)) {
				int swslot;
				int slot = i - 1;
				struct vm_anon *anon;

				map ^= 1 << slot;
				anon = chunk->ac_anon[slot];

				swslot = anon->an_swslot;
				if (swslot < startslot || endslot <= swslot) {
					continue;
				}
d752 1
a752 1
				am->am_flags |= AMAP_SWAPOFF;
d754 1
a754 1
				rv = uvm_anon_pagein(anon);
d756 4
a759 5
				am->am_flags &= ~AMAP_SWAPOFF;
				if (rv || amap_refs(am) == 0)
					goto nextamap;
				goto again;
			}
a761 1
nextamap:
a777 1
	struct vm_amap_chunk *chunk;
d785 1
a785 5
	chunk = amap_chunk_get(amap, slot, 0, PR_NOWAIT);
	if (chunk == NULL)
		return NULL;

	return chunk->ac_anon[UVM_AMAP_SLOTIDX(slot)];
d797 1
a797 1
	int i, lcv, n, slot;
a798 1
	struct vm_amap_chunk *chunk = NULL;
d806 3
a808 13
	for (i = 0, lcv = slot; lcv < slot + npages; i += n, lcv += n) {
		n = UVM_AMAP_CHUNK - UVM_AMAP_SLOTIDX(lcv);
		if (lcv + n > slot + npages)
			n = slot + npages - lcv;

		chunk = amap_chunk_get(amap, lcv, 0, PR_NOWAIT);
		if (chunk == NULL)
			memset(&anons[i], 0, n * sizeof(*anons[i]));
		else
			memcpy(&anons[i],
			    &chunk->ac_anon[UVM_AMAP_SLOTIDX(lcv)],
			    n * sizeof(*anons[i]));
	}
a818 12
	int slot;
	struct vm_amap *amap = aref->ar_amap;
	struct vm_amap_chunk *chunk;

	AMAP_B2SLOT(slot, offset);
	slot += aref->ar_pageoff;

	if (slot >= amap->am_nslot)
		panic("amap_populate: offset out of range");

	chunk = amap_chunk_get(amap, slot, 1, PR_WAITOK);
	KASSERT(chunk != NULL);
a831 1
	struct vm_amap_chunk *chunk;
a837 3
	chunk = amap_chunk_get(amap, slot, 1, PR_NOWAIT);
	if (chunk == NULL)
		return 1;
a838 1
	slot = UVM_AMAP_SLOTIDX(slot);
d840 1
a840 1
		if (chunk->ac_anon[slot] == NULL)
d842 1
a842 1
		if (chunk->ac_anon[slot]->an_page != NULL &&
d844 1
a844 1
			pmap_page_protect(chunk->ac_anon[slot]->an_page,
d851 1
a851 1
		if (chunk->ac_anon[slot] != NULL)
d854 2
a855 1
		chunk->ac_usedmap |= 1 << slot;
d858 1
a858 1
	chunk->ac_anon[slot] = anon;
d869 1
a869 1
	int slot;
a870 1
	struct vm_amap_chunk *chunk;
a876 3
	chunk = amap_chunk_get(amap, slot, 0, PR_NOWAIT);
	if (chunk == NULL)
		panic("amap_unadd: chunk for slot %d not present", slot);
d878 1
a878 2
	slot = UVM_AMAP_SLOTIDX(slot);
	if (chunk->ac_anon[slot] == NULL)
d881 7
a887 2
	chunk->ac_anon[slot] = NULL;
	chunk->ac_usedmap &= ~(1 << slot);
a888 3

	if (chunk->ac_usedmap == 0)
		amap_chunk_free(amap, chunk);
@


1.68
log
@Additional parameter for amap_alloc().

It is supposed to control whether an amap should allocate memory
to store anon pointers lazily or upfront. Needed for upcoming amap
changes.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.67 2016/05/08 11:52:32 stefan Exp $	*/
d47 1
a47 1
 * pool for allocation of vm_map structures.  note that in order to
d54 2
a55 3

/* Pools for amap slots for the most common amap slot sizes */
struct pool uvm_amap_slot_pools[UVM_AMAP_CHUNK];
d59 1
a59 1
static char amap_slot_pool_names[UVM_AMAP_CHUNK][13];
d71 2
d85 66
d227 1
d234 8
a241 6
	for (i = 0; i < nitems(uvm_amap_slot_pools); i++) {
		snprintf(amap_slot_pool_names[i],
		    sizeof(amap_slot_pool_names[0]), "amapslotpl%d", i + 1);
		pool_init(&uvm_amap_slot_pools[i], (i + 1) * MALLOC_SLOT_UNIT,
		    0, 0, PR_WAITOK, amap_slot_pool_names[i], NULL);
		pool_sethiwat(&uvm_amap_slot_pools[i], 4096);
d243 6
d259 4
d264 39
a302 2
	amap = pool_get(&uvm_amap_pool, (waitf == M_WAITOK) ? PR_WAITOK
	    : PR_NOWAIT);
d316 2
a317 7
	if (slots > UVM_AMAP_CHUNK)
		amap->am_slots = malloc(slots * MALLOC_SLOT_UNIT,
		    M_UVMAMAP, waitf);
	else
		amap->am_slots = pool_get(
		    &uvm_amap_slot_pools[slots - 1],
		    (waitf == M_WAITOK) ? PR_WAITOK : PR_NOWAIT);
d319 9
a327 1
	if (amap->am_slots == NULL)
d330 21
a350 4
	amap->am_bckptr = (int *)(((char *)amap->am_slots) + slots *
	    sizeof(int));
	amap->am_anon = (struct vm_anon **)(((char *)amap->am_bckptr) +
	    slots * sizeof(int));
d355 3
d377 1
a377 3
	if (amap) {
		memset(amap->am_anon, 0,
		    amap->am_nslot * sizeof(struct vm_anon *));
a378 1
	}
d392 1
a396 6
	if (amap->am_nslot > UVM_AMAP_CHUNK)
		free(amap->am_slots, M_UVMAMAP, 0);
	else
		pool_put(&uvm_amap_slot_pools[amap->am_nslot - 1],
		    amap->am_slots);

a400 1
	pool_put(&uvm_amap_pool, amap);
d402 8
d422 1
a422 1
	int lcv, slot;
d424 1
d434 2
a435 2
	for (lcv = 0 ; lcv < amap->am_nused ; lcv++) {
		int refs;
d437 4
a440 2
		slot = amap->am_slots[lcv];
		anon = amap->am_anon[slot];
d442 2
a443 2
		if (anon == NULL || anon->an_ref == 0)
			panic("amap_wipeout: corrupt amap");
d445 8
a452 4
		refs = --anon->an_ref;
		if (refs == 0) {
			/* we had the last reference to a vm_anon. free it. */
			uvm_anfree(anon);
d481 2
d530 3
d552 14
a565 4
	for (lcv = 0 ; lcv < slots; lcv++) {
		amap->am_anon[lcv] =
		    srcamap->am_anon[entry->aref.ar_pageoff + lcv];
		if (amap->am_anon[lcv] == NULL)
d567 17
a583 4
		amap->am_anon[lcv]->an_ref++;
		amap->am_bckptr[lcv] = amap->am_nused;
		amap->am_slots[amap->am_nused] = lcv;
		amap->am_nused++;
a584 1
	KASSERT(lcv == amap->am_nslot);
d628 1
a628 1
	int lcv, slot;
d631 1
d639 20
a658 10
	for (lcv = 0 ; lcv < amap->am_nused ; lcv++) {
		/* get the page */
		slot = amap->am_slots[lcv];
		anon = amap->am_anon[slot];
		pg = anon->an_page;

		/* page must be resident since parent is wired */
		if (pg == NULL)
			panic("amap_cow_now: non-resident wired page"
			    " in anon %p", anon);
a659 5
		/*
		 * if the anon ref count is one, we are safe (the child has
		 * exclusive access to the page).
		 */
		if (anon->an_ref > 1) {
d689 1
a689 1
	
d696 1
a696 1
			amap->am_anon[slot] = nanon;	/* replace */
d840 3
a842 1
	int byanon, lcv, stop, curslot, ptr, slotend;
d844 1
d847 2
a848 2
	 * we can either traverse the amap by am_anon or by am_slots depending
	 * on which is cheaper.    decide now.
d850 9
a858 4
	if (slots < amap->am_nused) {
		byanon = TRUE;
		lcv = slotoff;
		stop = slotoff + slots;
d860 3
a862 4
		byanon = FALSE;
		lcv = 0;
		stop = amap->am_nused;
		slotend = slotoff + slots;
d865 44
a908 12
	while (lcv < stop) {
		int refs;

  		if (byanon) {
			curslot = lcv++;	/* lcv advances here */
			if (amap->am_anon[curslot] == NULL)
				continue;
		} else {
			curslot = amap->am_slots[lcv];
			if (curslot < slotoff || curslot >= slotend) {
				lcv++;		/* lcv advances here */
				continue;
a909 1
			stop--;	/* drop stop, since anon will be removed */
a910 1
		anon = amap->am_anon[curslot];
d912 5
a916 20
		/* remove it from the amap */
		amap->am_anon[curslot] = NULL;
		ptr = amap->am_bckptr[curslot];
		if (ptr != (amap->am_nused - 1)) {
			amap->am_slots[ptr] =
			    amap->am_slots[amap->am_nused - 1];
			amap->am_bckptr[amap->am_slots[ptr]] =
			    ptr;    /* back ptr. */
		}
		amap->am_nused--;

		/* drop anon reference count */
		refs = --anon->an_ref;
		if (refs == 0) {
			/*
			 * we just eliminated the last reference to an anon.
			 * free it.
			 */
			uvm_anfree(anon);
		}
d938 19
a956 1
		int i;
d958 1
a958 4
		for (i = 0; i < am->am_nused; i++) {
			int slot;
			int swslot;
			struct vm_anon *anon;
d960 1
a960 2
			slot = am->am_slots[i];
			anon = am->am_anon[slot];
d962 4
a965 3
			swslot = anon->an_swslot;
			if (swslot < startslot || endslot <= swslot) {
				continue;
a966 9

			am->am_flags |= AMAP_SWAPOFF;

			rv = uvm_anon_pagein(anon);

			am->am_flags &= ~AMAP_SWAPOFF;
			if (rv || amap_refs(am) == 0)
				break;
			i = 0;
d969 1
d986 1
d994 5
a998 1
	return(amap->am_anon[slot]);
d1010 1
a1010 1
	int slot;
d1012 1
d1020 13
a1032 3
	memcpy(anons, &amap->am_anon[slot], npages * sizeof(struct vm_anon *));

	return;
d1043 12
d1068 1
d1075 3
d1079 1
d1081 1
a1081 1
		if (amap->am_anon[slot] == NULL)
d1083 1
a1083 1
		if (amap->am_anon[slot]->an_page != NULL && 
d1085 1
a1085 1
			pmap_page_protect(amap->am_anon[slot]->an_page,
d1092 1
a1092 1
		if (amap->am_anon[slot] != NULL)
d1095 1
a1095 2
		amap->am_bckptr[slot] = amap->am_nused;
		amap->am_slots[amap->am_nused] = slot;
d1098 1
a1098 1
	amap->am_anon[slot] = anon;
d1109 1
a1109 1
	int ptr, slot;
d1111 1
d1118 3
d1122 2
a1123 1
	if (amap->am_anon[slot] == NULL)
d1126 3
a1128 2
	amap->am_anon[slot] = NULL;
	ptr = amap->am_bckptr[slot];
d1130 2
a1131 5
	if (ptr != (amap->am_nused - 1)) {	/* swap to keep slots contig? */
		amap->am_slots[ptr] = amap->am_slots[amap->am_nused - 1];
		amap->am_bckptr[amap->am_slots[ptr]] = ptr;	/* back link */
	}
	amap->am_nused--;
@


1.67
log
@Wait for RAM in uvm_fault when allocating uvm structures fails

Only fail hard when running out of swap space also, as suggested by
kettenis@@

While there, let amap_add() return a success status and handle
amap_add() errors in uvm_fault() similar to other out of RAM situations.
These bits are needed for further amap reorganization diffs.

lots of feedback and ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.66 2016/04/16 18:39:31 stefan Exp $	*/
d68 1
a68 1
static struct vm_amap *amap_alloc1(int, int);
d180 1
a180 1
amap_alloc1(int slots, int waitf)
d229 1
a229 1
amap_alloc(vaddr_t sz, int waitf)
d236 1
a236 1
	amap = amap_alloc1(slots, waitf);
d333 1
a333 1
	int slots, lcv;
d342 4
d347 12
a358 10
		if (canchunk && atop(entry->end - entry->start) >=
		    UVM_AMAP_LARGE) {
			/* convert slots to bytes */
			chunksize = UVM_AMAP_CHUNK << PAGE_SHIFT;
			startva = (startva / chunksize) * chunksize;
			endva = roundup(endva, chunksize);
			UVM_MAP_CLIP_START(map, entry, startva);
			/* watch out for endva wrap-around! */
			if (endva >= startva)
				UVM_MAP_CLIP_END(map, entry, endva);
d363 1
a363 1
		    waitf);
d382 1
a382 1
	amap = amap_alloc1(slots, waitf);
@


1.66
log
@Remove am_maxslot from amap.

am_maxslot represents the total number of slots an amap can be extended
to. Since we do not extend amaps, this field as well as rounding the
number of slots to the next malloc bucket is not useful.

This also removes the corresponding output from procmap(1).

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.65 2016/04/12 16:47:33 stefan Exp $	*/
d806 10
d818 1
a818 1
 * => returns an "offset" which is meaningful to amap_unadd().
d820 1
a820 1
void
d853 2
@


1.65
log
@Simplify amap traversal in amap_swap_off.

There's no need to insert marker elements to find the next item in the
amap list. The next amap can be determined by looking at the currently
examined amap.

Care must be taken to get the next element before the current amap is
possibly deleted, and after all the current amap's pages were read in
from swap (because the page-in may sleep and remove items from the amap
list).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.64 2016/04/04 16:34:16 stefan Exp $	*/
a182 1
	int totalslots;
d189 1
a189 6
	totalslots = slots;
	KASSERT(totalslots > 0);

	if (totalslots > UVM_AMAP_CHUNK)
		totalslots = malloc_roundup(totalslots * MALLOC_SLOT_UNIT) /
		    MALLOC_SLOT_UNIT;
a195 1
	amap->am_maxslot = totalslots;
d199 2
a200 2
	if (totalslots > UVM_AMAP_CHUNK)
		amap->am_slots = malloc(totalslots * MALLOC_SLOT_UNIT,
d204 1
a204 1
		    &uvm_amap_slot_pools[totalslots - 1],
d210 1
a210 1
	amap->am_bckptr = (int *)(((char *)amap->am_slots) + totalslots *
d213 1
a213 1
	    totalslots * sizeof(int));
d239 1
a239 1
		    amap->am_maxslot * sizeof(struct vm_anon *));
d259 1
a259 1
	if (amap->am_maxslot > UVM_AMAP_CHUNK)
d262 1
a262 1
		pool_put(&uvm_amap_slot_pools[amap->am_maxslot - 1],
d405 1
a405 2
	memset(&amap->am_anon[lcv], 0,
	    (amap->am_maxslot - lcv) * sizeof(struct vm_anon *));
d565 1
a565 1
	amap->am_ppref = mallocarray(amap->am_maxslot, sizeof(int),
@


1.64
log
@UVM_FLAG_AMAPPAD has no effect anymore, nuke it.

This flag caused amaps to be allocated with additional spare slots, to
make extending them cheaper. However, the kernel never extends amaps,
so allocating spare slots is pointless. Also UVM_FLAG_AMAPPAD only
has an effect in combination with UVM_FLAG_OVERLAY. The only function
that used both flags was sys_obreak, but that function had the use of
UVM_FLAG_OVERLAY removed recently.

While there, kill the unused prototypes amap_flags and amap_refs.
They're defined as macros already.

ok mlarkin@@ kettenis@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.63 2016/03/27 09:51:37 stefan Exp $	*/
a735 2
	struct vm_amap marker_prev;
	struct vm_amap marker_next;
a737 5
#if defined(DIAGNOSTIC)
	memset(&marker_prev, 0, sizeof(marker_prev));
	memset(&marker_next, 0, sizeof(marker_next));
#endif /* defined(DIAGNOSTIC) */

a740 3
		LIST_INSERT_BEFORE(am, &marker_prev, am_list);
		LIST_INSERT_AFTER(am, &marker_next, am_list);

d759 1
a759 3
			if (amap_refs(am) == 0) {
				amap_wipeout(am);
				am = NULL;
a760 4
			}
			if (rv) {
				break;
			}
d764 3
a766 6
		KASSERT(LIST_NEXT(&marker_prev, am_list) == &marker_next ||
		    LIST_NEXT(LIST_NEXT(&marker_prev, am_list), am_list) ==
		    &marker_next);
		am_next = LIST_NEXT(&marker_next, am_list);
		LIST_REMOVE(&marker_prev, am_list);
		LIST_REMOVE(&marker_next, am_list);
@


1.63
log
@amap_extend is never called, remove it.

In the code, this function is called when vm_map_entries are merged.
However, only kernel map entries are merged, and these do not use amaps.
Therefore amap_extend() is never called at runtime.

ok millert@@, KASSERT suggestion and ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.62 2016/03/16 16:53:43 stefan Exp $	*/
d68 1
a68 1
static struct vm_amap *amap_alloc1(int, int, int);
d180 1
a180 1
amap_alloc1(int slots, int padslots, int waitf)
d190 1
a190 1
	totalslots = slots + padslots;
d236 1
a236 1
amap_alloc(vaddr_t sz, vaddr_t padsz, int waitf)
d239 1
a239 1
	int slots, padslots;
a241 1
	AMAP_B2SLOT(padslots, padsz);
d243 1
a243 1
	amap = amap_alloc1(slots, padslots, waitf);
d363 1
a363 1
		entry->aref.ar_amap = amap_alloc(entry->end - entry->start, 0,
d383 1
a383 1
	amap = amap_alloc1(slots, 0, waitf);
@


1.62
log
@Remove redundant check.

The compiler is also smart enough to recognize that this is redundant.
The resulting code on amd64 is basically equivalent (slightly different
register allocation and instruction scheduling).

ok mpi@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.61 2016/03/15 18:16:21 stefan Exp $	*/
a278 168
}

/*
 * amap_extend: extend the size of an amap (if needed)
 *
 * => called from uvm_map when we want to extend an amap to cover
 *    a new mapping (rather than allocate a new one)
 * => to safely extend an amap it should have a reference count of
 *    one (thus it can't be shared)
 * => XXXCDC: support padding at this level?
 */
int
amap_extend(struct vm_map_entry *entry, vsize_t addsize)
{
	struct vm_amap *amap = entry->aref.ar_amap;
	int slotoff = entry->aref.ar_pageoff;
	int slotmapped, slotadd, slotneed, slotalloc;
#ifdef UVM_AMAP_PPREF
	int *newppref, *oldppref;
#endif
	u_int *newsl, *newbck, *oldsl, *oldbck;
	struct vm_anon **newover, **oldover;
	int slotadded;

	/*
	 * first, determine how many slots we need in the amap.  don't
	 * forget that ar_pageoff could be non-zero: this means that
	 * there are some unused slots before us in the amap.
	 */
	AMAP_B2SLOT(slotmapped, entry->end - entry->start); /* slots mapped */
	AMAP_B2SLOT(slotadd, addsize);			/* slots to add */
	slotneed = slotoff + slotmapped + slotadd;

	/*
	 * case 1: we already have enough slots in the map and thus
	 * only need to bump the reference counts on the slots we are
	 * adding.
	 */
	if (amap->am_nslot >= slotneed) {
#ifdef UVM_AMAP_PPREF
		if (amap->am_ppref && amap->am_ppref != PPREF_NONE) {
			amap_pp_adjref(amap, slotoff + slotmapped, slotadd, 1);
		}
#endif
		return (0);
	}

	/*
	 * case 2: we pre-allocated slots for use and we just need to
	 * bump nslot up to take account for these slots.
	 */
	if (amap->am_maxslot >= slotneed) {
#ifdef UVM_AMAP_PPREF
		if (amap->am_ppref && amap->am_ppref != PPREF_NONE) {
			if ((slotoff + slotmapped) < amap->am_nslot)
				amap_pp_adjref(amap, slotoff + slotmapped, 
				    (amap->am_nslot - (slotoff + slotmapped)),
				    1);
			pp_setreflen(amap->am_ppref, amap->am_nslot, 1, 
			   slotneed - amap->am_nslot);
		}
#endif
		amap->am_nslot = slotneed;
		/*
		 * no need to zero am_anon since that was done at
		 * alloc time and we never shrink an allocation.
		 */
		return (0);
	}

	/*
	 * case 3: we need to malloc a new amap and copy all the amap
	 * data over from old amap to the new one.
	 *
	 * XXXCDC: could we take advantage of a kernel realloc()?  
	 */
	if (slotneed >= UVM_AMAP_LARGE)
		return E2BIG;

	if (slotneed > UVM_AMAP_CHUNK)
		slotalloc = malloc_roundup(slotneed * MALLOC_SLOT_UNIT) /
		    MALLOC_SLOT_UNIT;
	else
		slotalloc = slotneed;

#ifdef UVM_AMAP_PPREF
	newppref = NULL;
	if (amap->am_ppref && amap->am_ppref != PPREF_NONE) {
		newppref = mallocarray(slotalloc, sizeof(int), M_UVMAMAP,
		    M_WAITOK | M_CANFAIL);
		if (newppref == NULL) {
			/* give up if malloc fails */
			free(amap->am_ppref, M_UVMAMAP, 0);
			amap->am_ppref = PPREF_NONE;
		}
	}
#endif
	if (slotneed > UVM_AMAP_CHUNK)
		newsl = malloc(slotalloc * MALLOC_SLOT_UNIT, M_UVMAMAP,
		    M_WAITOK | M_CANFAIL);
	else
		newsl = pool_get(&uvm_amap_slot_pools[slotalloc - 1],
		    PR_WAITOK | PR_LIMITFAIL);
	if (newsl == NULL) {
#ifdef UVM_AMAP_PPREF
		if (newppref != NULL) {
			free(newppref, M_UVMAMAP, 0);
		}
#endif
		return (ENOMEM);
	}
	newbck = (int *)(((char *)newsl) + slotalloc * sizeof(int));
	newover = (struct vm_anon **)(((char *)newbck) + slotalloc *
	    sizeof(int));
	KASSERT(amap->am_maxslot < slotneed);

	/* now copy everything over to new malloc'd areas... */
	slotadded = slotalloc - amap->am_nslot;

	/* do am_slots */
	oldsl = amap->am_slots;
	memcpy(newsl, oldsl, sizeof(int) * amap->am_nused);
	amap->am_slots = newsl;

	/* do am_anon */
	oldover = amap->am_anon;
	memcpy(newover, oldover, sizeof(struct vm_anon *) * amap->am_nslot);
	memset(newover + amap->am_nslot, 0, sizeof(struct vm_anon *) *
	    slotadded);
	amap->am_anon = newover;

	/* do am_bckptr */
	oldbck = amap->am_bckptr;
	memcpy(newbck, oldbck, sizeof(int) * amap->am_nslot);
	memset(newbck + amap->am_nslot, 0, sizeof(int) * slotadded); /* XXX: needed? */
	amap->am_bckptr = newbck;

#ifdef UVM_AMAP_PPREF
	/* do ppref */
	oldppref = amap->am_ppref;
	if (newppref) {
		memcpy(newppref, oldppref, sizeof(int) * amap->am_nslot);
		memset(newppref + amap->am_nslot, 0, sizeof(int) * slotadded);
		amap->am_ppref = newppref;
		if ((slotoff + slotmapped) < amap->am_nslot)
			amap_pp_adjref(amap, slotoff + slotmapped, 
			    (amap->am_nslot - (slotoff + slotmapped)), 1);
		pp_setreflen(newppref, amap->am_nslot, 1,
		    slotneed - amap->am_nslot);
	}
#endif

	/* free */
	if (amap->am_maxslot > UVM_AMAP_CHUNK)
		free(oldsl, M_UVMAMAP, 0);
	else
		pool_put(&uvm_amap_slot_pools[amap->am_maxslot - 1],
		    oldsl);

	/* and update master values */
	amap->am_nslot = slotneed;
	amap->am_maxslot = slotalloc;

#ifdef UVM_AMAP_PPREF
	if (oldppref && oldppref != PPREF_NONE)
		free(oldppref, M_UVMAMAP, 0);
#endif
	return (0);
@


1.61
log
@For amaps with only a few slots, allocate the slots via pool(9)

This saves some memory compared to using malloc, because there's
no roundup to the next bucket size. And it reduces kmem pressure
at least for some architectures (e.g. amd64).

Testing by sthen@@ and tobiasu@@, thanks!

ok sthen@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.60 2016/03/06 14:47:07 stefan Exp $	*/
a919 4
		if (am->am_nused <= 0) {
			goto next;
		}

a948 1
next:
@


1.60
log
@Remove unused amap_share_protect().

ok mpi@@ visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.59 2015/08/21 16:04:35 visa Exp $	*/
d55 3
d60 2
d159 2
d165 8
d190 7
a196 2
	totalslots = malloc_roundup((slots + padslots) * MALLOC_SLOT_UNIT) /
	    MALLOC_SLOT_UNIT;
d206 8
a213 2
	amap->am_slots = malloc(totalslots * MALLOC_SLOT_UNIT, M_UVMAMAP,
	    waitf);
d267 6
a272 1
	free(amap->am_slots, M_UVMAMAP, 0);
d358 6
a363 2
	slotalloc = malloc_roundup(slotneed * MALLOC_SLOT_UNIT) /
	    MALLOC_SLOT_UNIT;
d376 6
a381 2
	newsl = malloc(slotalloc * MALLOC_SLOT_UNIT, M_UVMAMAP,
	    M_WAITOK | M_CANFAIL);
d431 8
a438 1
	/* update master values */
a441 2
	/* and free */
	free(oldsl, M_UVMAMAP, 0);
@


1.59
log
@Remove the unused loan_count field and the related uvm logic. Most of
the page loaning code is already in the Attic.

ok kettenis@@, beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.58 2014/12/23 04:56:47 tedu Exp $	*/
a402 43
}

/*
 * amap_share_protect: change protection of anons in a shared amap
 *
 * for shared amaps, given the current data structure layout, it is
 * not possible for us to directly locate all maps referencing the
 * shared anon (to change the protection).  in order to protect data
 * in shared maps we use pmap_page_protect().  [this is useful for IPC
 * mechanisms like map entry passing that may want to write-protect
 * all mappings of a shared amap.]  we traverse am_anon or am_slots
 * depending on the current state of the amap.
 */
void
amap_share_protect(struct vm_map_entry *entry, vm_prot_t prot)
{
	struct vm_amap *amap = entry->aref.ar_amap;
	int slots, lcv, slot, stop;

	AMAP_B2SLOT(slots, (entry->end - entry->start));
	stop = entry->aref.ar_pageoff + slots;

	if (slots < amap->am_nused) {
		/* cheaper to traverse am_anon */
		for (lcv = entry->aref.ar_pageoff ; lcv < stop ; lcv++) {
			if (amap->am_anon[lcv] == NULL)
				continue;
			if (amap->am_anon[lcv]->an_page != NULL)
				pmap_page_protect(amap->am_anon[lcv]->an_page,
						  prot);
		}
		return;
	}

	/* cheaper to traverse am_slots */
	for (lcv = 0 ; lcv < amap->am_nused ; lcv++) {
		slot = amap->am_slots[lcv];
		if (slot < entry->aref.ar_pageoff || slot >= stop)
			continue;
		if (amap->am_anon[slot]->an_page != NULL)
			pmap_page_protect(amap->am_anon[slot]->an_page, prot);
	}
	return;
@


1.59.2.1
log
@backport r1.75:
Make sure that amap slot calculation does not overflow

This prevents too small amaps from being allocated by
forcing the allocation of a large number of slots.

Based on an analysis from Jesse Hertz and Tim Newsham.

This is a different patch for 5.9 that addresses the same
issue as r1.75 of uvm/uvm_amap.c. It also makes sure that
vmm(4) cannot make such large amap allocation requests.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.59 2015/08/21 16:04:35 visa Exp $	*/
a37 1
#include <sys/stdint.h>
d168 1
a168 1
	size_t totalslots = (size_t)slots + padslots;
d175 1
a175 10
	/*
	 * Make sure that totalslots * MALLOC_SLOT_UNIT is within
	 * a size_t, AND: since malloc_roundup may round its argument
	 * to a multiple of the PAGE_SIZE, make sure that malloc_roundup
	 * cannot wrap totalslots * MALLOC_SLOT_UNIT to zero.
	 */
	if (totalslots >= (trunc_page(SIZE_MAX) / MALLOC_SLOT_UNIT))
		return (NULL);

	totalslots = malloc_roundup(totalslots * MALLOC_SLOT_UNIT) /
a176 5
	if (totalslots > INT_MAX)
		return (NULL);

	KASSERT(totalslots > 0);

d186 1
a186 1
	amap->am_slots = mallocarray(totalslots, MALLOC_SLOT_UNIT, M_UVMAMAP,
d213 1
a213 1
	size_t slots, padslots;
a216 4

	/* Ensure that slots + padslots <= INT_MAX */
	if (slots > INT_MAX || padslots > INT_MAX - slots)
		return (NULL);
@


1.58
log
@convert pool_init nointr to waitok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.57 2014/12/17 19:42:15 tedu Exp $	*/
a615 2
 * => assume pages that are loaned out (loan_count) are already mapped
 *	read-only in all maps, and thus no need for us to worry about them
d647 2
a648 7
		 * if the anon ref count is one and the page is not loaned,
		 * then we are safe (the child has exclusive access to the
		 * page).  if the page is loaned, then it must already be
		 * mapped read-only.
		 *
		 * we only need to get involved when these are not true.
		 * [note: if loan_count == 0, then the anon must own the page]
d650 1
a650 1
		if (anon->an_ref > 1 && pg->loan_count == 0) {
@


1.58.6.1
log
@backport r1.75:
Make sure that amap slot calculation does not overflow

This prevents too small amaps from being allocated by
forcing the allocation of a large number of slots.

Based on an analysis from Jesse Hertz and Tim Newsham.

This is a different patch for 5.8 that addresses the same
issue as r1.75
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.58 2014/12/23 04:56:47 tedu Exp $	*/
a37 1
#include <sys/stdint.h>
d168 1
a168 1
	size_t totalslots = (size_t)slots + padslots;
d175 1
a175 10
	/*
	 * Make sure that totalslots * MALLOC_SLOT_UNIT is within
	 * a size_t, AND: since malloc_roundup may round its argument
	 * to a multiple of the PAGE_SIZE, make sure that malloc_roundup
	 * cannot wrap totalslots * MALLOC_SLOT_UNIT to zero.
	 */
	if (totalslots >= (trunc_page(SIZE_MAX) / MALLOC_SLOT_UNIT))
		return (NULL);

	totalslots = malloc_roundup(totalslots * MALLOC_SLOT_UNIT) /
a176 5
	if (totalslots > INT_MAX)
		return (NULL);

	KASSERT(totalslots > 0);

d186 1
a186 1
	amap->am_slots = mallocarray(totalslots, MALLOC_SLOT_UNIT, M_UVMAMAP,
d213 1
a213 1
	size_t slots, padslots;
a216 4

	/* Ensure that slots + padslots <= INT_MAX */
	if (slots > INT_MAX || padslots > INT_MAX - slots)
		return (NULL);
@


1.57
log
@remove lock.h from uvm_extern.h. another holdover from the simpletonlock
era. fix uvm including c files to include lock.h or atomic.h as necessary.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.56 2014/12/09 07:16:41 doug Exp $	*/
d155 2
a156 2
	pool_init(&uvm_amap_pool, sizeof(struct vm_amap), 0, 0, 0,
	    "amappl", &pool_allocator_nointr);
@


1.56
log
@Sprinkle in a little more mallocarray().

ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.55 2014/11/16 12:31:00 deraadt Exp $	*/
d41 1
@


1.55
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.54 2014/09/14 14:17:27 jsg Exp $	*/
d331 1
a331 1
		newppref = malloc(slotalloc *sizeof(int), M_UVMAMAP,
d747 1
a747 1
	amap->am_ppref = malloc(sizeof(int) * amap->am_maxslot,
@


1.54
log
@remove uneeded proc.h includes
ok mpi@@ kspillner@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.53 2014/07/12 18:44:01 tedu Exp $	*/
d1035 1
a1035 1
			    VM_PROT_NONE);
@


1.53
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.52 2014/07/11 16:35:40 jsg Exp $	*/
a37 1
#include <sys/proc.h>
@


1.52
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.51 2014/04/13 23:14:15 tedu Exp $	*/
d241 1
a241 1
	free(amap->am_slots, M_UVMAMAP);
d244 1
a244 1
		free(amap->am_ppref, M_UVMAMAP);
d336 1
a336 1
			free(amap->am_ppref, M_UVMAMAP);
d346 1
a346 1
			free(newppref, M_UVMAMAP);
d397 1
a397 1
	free(oldsl, M_UVMAMAP);
d400 1
a400 1
		free(oldppref, M_UVMAMAP);
@


1.51
log
@compress code by turning four line comments into one line comments.
emphatic ok usual suspects, grudging ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.50 2013/05/30 16:39:26 tedu Exp $	*/
a4 1
 *
a15 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed by Charles D. Cranor and
 *      Washington University.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
@


1.50
log
@UVM_UNLOCK_AND_WAIT no longer unlocks, so rename it to UVM_WAIT.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.49 2013/05/30 16:29:46 tedu Exp $	*/
d38 1
a38 3
 */

/*
d161 1
a161 3
	/*
	 * Initialize the vm_amap pool.
	 */
a215 1

a283 1

a292 1

a305 1

a317 1

a330 1

d363 1
a363 4
	/*
	 * now copy everything over to new malloc'd areas...
	 */

d471 1
a471 3
		/*
		 * amap_swap_off will call us again.
		 */
d487 1
a487 3
			/*
			 * we had the last reference to a vm_anon. free it.
			 */
d492 1
a492 4
	/*
	 * now we free the map
	 */

d518 1
a518 4
	/*
	 * is there a map to copy?   if not, create one from scratch.
	 */

a519 1

a524 1

a550 1

d556 1
a556 4
	/*
	 * looks like we need to copy the map.
	 */

a568 1

d576 1
a576 4
	/*
	 * we must copy it now.
	 */

a595 1

d606 1
a606 4
	/*
	 * install new amap.
	 */

d645 1
a645 5

		/*
		 * get the page
		 */

d650 1
a650 4
		/*
		 * page must be resident since parent is wired
		 */

d652 2
a653 2
		    panic("amap_cow_now: non-resident wired page in anon %p",
			anon);
a663 1

a664 1

d675 1
a675 3
			/*
			 * ok, time to do a copy-on-write to a new anon
			 */
d714 1
a714 6

		/*
		 * done with this anon, next ...!
		 */

	}	/* end of 'for' loop */
d731 1
a731 4
	/*
	 * now: we have a valid am_mapped array.
	 */

d736 1
a736 3
        /*
	 * establish ppref before we add a duplicate reference to the amap
	 */
d758 1
a758 3
	/*
	 * if we fail then we just won't use ppref for this amap
	 */
d764 1
a764 3
	/*
	 * init ppref
	 */
a787 1

a812 1

a851 1

d880 1
a880 3
		/*
		 * remove it from the amap
		 */
d891 1
a891 3
		/*
		 * drop anon reference count
		 */
a1037 1

d1125 1
a1125 4
	/*
	 * if we are the last reference, free the amap and return.
	 */

d1131 1
a1131 3
	/*
	 * otherwise just drop the reference count(s)
	 */
@


1.49
log
@remove lots of comments about locking per beck's request
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.48 2013/05/30 15:17:59 tedu Exp $	*/
d717 1
a717 2
				UVM_UNLOCK_AND_WAIT(pg, &anon->an_lock, FALSE,
				    "cownow", 0);
@


1.48
log
@remove simple_locks from uvm code. ok beck deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.47 2013/05/23 01:42:59 tedu Exp $	*/
a126 2
 *
 * => ppref's amap must be locked
a142 2
 *
 * => ppref's amap must be locked
a173 2
 *
 * => lock on returned amap is init'd
a218 1
 * => new amap is returned unlocked
a243 1
 * => the amap must be locked
a266 1
 * => amap should be unlocked (we will lock it)
a434 2
 *
 * => entry's map and amap must be locked by the caller
a472 1
 * => the amap should be locked (by the caller)
d515 1
a515 1
	amap_free(amap);	/* will unlock and free amap */
a522 2
 * => the map that the map entry belongs to must be locked by caller.
 * => the amap currently attached to "entry" (if any) must be unlocked.
d573 2
a574 5
	 * just take it over rather than copying it.  note that we are
	 * reading am_ref with the amap unlocked... the value can only
	 * be one if we have the only reference to the amap (via our
	 * locked map).  if we are greater than one we fall through to
	 * the next case (where we double check the value).
d593 2
a594 3
	 * need to double check reference count now that we've got the
	 * src amap locked down.  the reference count could have
	 * changed while we were in malloc.  if the reference count
d624 1
a624 1
	 * drop our reference to the old amap (srcamap) and unlock.
a661 1
 * => assume both parent and child vm_map's are locked
a662 4
 * => if we run out of memory we will unlock the amap and sleep _with_ the
 *	parent and child vm_map's locked(!).    we have to do this since
 *	we are in the middle of a fork(2) and we can't let the parent
 *	map change until we are done copying all the map entries.
a664 1
 * => page queues must be unlocked (we may lock them)
d676 3
a678 3
	 * note that if we unlock the amap then we must ReStart the "lcv" for
	 * loop because some other process could reorder the anon's in the
	 * am_anon[] array on us while the lock is dropped.
d712 1
a712 1
			 * if the page is busy then we have to unlock, wait for
d753 1
a753 1
			 * drop PG_BUSY on new page ... since we have had it's
a774 2
 * => origref's map should be locked
 * => origref->ar_amap should be unlocked (we will lock)
d786 1
a786 1
	 * now: amap is locked and we have a valid am_mapped array.
a808 2
 *
 * => amap locked by caller
a834 1
 * => map and amap locked by caller
a905 2
 *
 * => both map and amap must be locked by caller.
a977 1
 * => called with swap_syscall_lock held.
a1049 2
 *
 * => amap should be locked by caller.
a1068 1
 * => amap should be locked by caller.
a1091 3
 * => caller must lock amap.   
 * => if (replace) caller must lock anon because we might have to call
 *	pmap_page_protect on the anon's page.
a1131 2
 *
 * => caller must lock amap
a1160 1
 * => amap must not be locked (we will lock)
d1190 1
a1190 2
 *	longer part of a map and thus has no need for locking
 * => amap must be unlocked (we will lock it).
d1202 1
a1202 1
		return;			/* no need to unlock */
@


1.47
log
@the simplelock is a lie
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.46 2011/07/03 18:34:14 oga Exp $	*/
a511 2
		simple_lock(&anon->an_lock); /* lock anon */

a512 1
		simple_unlock(&anon->an_lock);
a632 1
		simple_lock(&amap->am_anon[lcv]->an_lock);
a633 1
		simple_unlock(&amap->am_anon[lcv]->an_lock);
a712 1
		simple_lock(&anon->an_lock);
a761 1
					simple_lock(&nanon->an_lock);
a763 1
				simple_unlock(&anon->an_lock);
a787 1
		simple_unlock(&anon->an_lock);
a992 1
		simple_lock(&anon->an_lock);
a993 1
		simple_unlock(&anon->an_lock);
a1045 1
			simple_lock(&anon->an_lock);
a1048 1
				simple_unlock(&anon->an_lock);
@


1.46
log
@Rip out and burn support for UVM_HIST.

The vm hackers don't use it, don't maintain it and have to look at it all the
time. About time this 800 lines of code hit /dev/null.

``never liked it'' tedu@@. ariane@@ was very happy when i told her i wrote
this diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.45 2010/07/03 03:04:55 tedu Exp $	*/
d56 1
a56 2
 * pool for allocation of vm_map structures.  note that the pool has
 * its own simplelock for its protection.  also note that in order to
d251 1
a251 1
 * => the amap must be locked (mainly for simplelock accounting)
@


1.45
log
@explicitly specify flags to malloc and pool_get instead of relying on 0.
This is more clear, and as thib pointed out, the default in softraid was
wrong.  ok thib.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.44 2009/03/25 20:00:18 oga Exp $	*/
a233 1
	UVMHIST_FUNC("amap_alloc"); UVMHIST_CALLED(maphist);
a244 1
	UVMHIST_LOG(maphist,"<- done, amap = %p, sz=%lu", amap, sz, 0, 0);
a257 1
	UVMHIST_FUNC("amap_free"); UVMHIST_CALLED(maphist);
a268 1
	UVMHIST_LOG(maphist,"<- done, freed amap = %p", amap, 0, 0, 0);
a292 3
	UVMHIST_FUNC("amap_extend"); UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist, "  (entry=%p, addsize=%lu)", entry, addsize, 0, 0);
a315 2
		UVMHIST_LOG(maphist,"<- done (case 1), amap = %p, sltneed=%ld", 
		    amap, slotneed, 0, 0);
a340 2
		UVMHIST_LOG(maphist,"<- done (case 2), amap = %p, slotneed=%ld",
		    amap, slotneed, 0, 0);
a431 2
	UVMHIST_LOG(maphist,"<- done (case 3), amap = %p, slotneed=%ld", 
	    amap, slotneed, 0, 0);
a492 2
	UVMHIST_FUNC("amap_wipeout"); UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist,"(amap=%p)", amap, 0,0,0);
a514 3
		UVMHIST_LOG(maphist,"  processing anon %p, ref=%ld", anon, 
		    anon->an_ref, 0, 0);

a531 1
	UVMHIST_LOG(maphist,"<- done!", 0,0,0,0);
a554 3
	UVMHIST_FUNC("amap_copy"); UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist, "  (map=%p, entry=%p, waitf=%ld)",
		    map, entry, waitf, 0);
a573 3
			UVMHIST_LOG(maphist, "  chunk amap ==> clip "
			    "0x%lx->0x%lx to 0x%lx->0x%lx",
			    entry->start, entry->end, startva, endva);
a579 2
		UVMHIST_LOG(maphist, "<- done [creating new amap 0x%lx->0x%lx]",
		    entry->start, entry->end, 0, 0);
a599 2
		UVMHIST_LOG(maphist, "<- done [ref cnt = 1, took it over]",
		    0, 0, 0, 0);
a606 2
	UVMHIST_LOG(maphist,"  amap=%p, ref=%ld, must copy it", 
	    entry->aref.ar_amap, entry->aref.ar_amap->am_ref, 0, 0);
d609 1
a609 2
	if (amap == NULL) {
		UVMHIST_LOG(maphist, "  amap_alloc1 failed", 0,0,0,0);
a610 1
	}
a631 1
	UVMHIST_LOG(maphist, "  copying amap now",0, 0, 0, 0);
a672 5

	/*
	 * done!
	 */
	UVMHIST_LOG(maphist, "<- done",0, 0, 0, 0);
a1103 1
	UVMHIST_FUNC("amap_lookup"); UVMHIST_CALLED(maphist);
a1110 2
	UVMHIST_LOG(maphist, "<- done (amap=%p, offset=0x%lx, result=%p)",
	    amap, offset, amap->am_anon[slot], 0);
a1125 1
	UVMHIST_FUNC("amap_lookups"); UVMHIST_CALLED(maphist);
a1129 3
	UVMHIST_LOG(maphist, "  slot=%ld, npages=%ld, nslot=%ld", slot, npages,
		amap->am_nslot, 0);

a1134 1
	UVMHIST_LOG(maphist, "<- done", 0, 0, 0, 0);
a1151 1
	UVMHIST_FUNC("amap_add"); UVMHIST_CALLED(maphist);
a1179 3
	UVMHIST_LOG(maphist,
	    "<- done (amap=%p, offset=0x%lx, anon=%p, rep=%ld)",
	    amap, offset, anon, replace);
a1191 1
	UVMHIST_FUNC("amap_unadd"); UVMHIST_CALLED(maphist);
a1209 1
	UVMHIST_LOG(maphist, "<- done (amap=%p, slot=%ld)", amap, slot,0, 0);
a1221 1
	UVMHIST_FUNC("amap_ref"); UVMHIST_CALLED(maphist);
a1236 1
	UVMHIST_LOG(maphist,"<- done!  amap=%p", amap, 0, 0, 0);
a1250 4
	UVMHIST_FUNC("amap_unref"); UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist,"  amap=%p  refs=%ld, nused=%ld",
	    amap, amap->am_ref, amap->am_nused, 0);
a1257 1
		UVMHIST_LOG(maphist,"<- done (was last ref)!", 0, 0, 0, 0);
a1275 2

	UVMHIST_LOG(maphist,"<- done!", 0, 0, 0, 0);
@


1.44
log
@Move all of the pseudo-inline functions in uvm into C files.

By pseudo-inline, I mean that if a certain macro was defined, they would
be inlined. However, no architecture defines that, and none has for a
very very long time. Therefore mainly this just makes the code a damned
sight easier to read. Some k&r -> ansi declarations while I'm in there.

"just commit it" art@@. ok weingart@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.43 2008/10/08 08:41:18 art Exp $	*/
d188 2
a189 1
	amap = pool_get(&uvm_amap_pool, (waitf == M_WAITOK) ? PR_WAITOK : 0);
@


1.43
log
@Don't extend amaps beyond what their supposed maximum. This code path is
not taken anymore, but it doesn't hurt to be correct.

from NetBSD, through mickey in pr 5812
prodded by otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.42 2008/09/12 08:55:41 otto Exp $	*/
a44 2
#undef UVM_AMAP_INLINE		/* enable/disable amap inlines */

a51 1
#define UVM_AMAP_C		/* ensure disabled inlines are in */
d1130 207
@


1.42
log
@less waste for amaps in the common case:
allocate a single malloc chunk instead of three and allocate a single
slot for a single page instead of four slots. ok miod@@ tedu@@ @@deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.41 2008/08/26 15:39:27 kettenis Exp $	*/
d363 3
@


1.41
log
@Plug potential memory leak.

"looks sane to me" otto@@, ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.40 2007/09/07 15:00:20 art Exp $	*/
d70 2
d195 2
a196 2
	totalslots = malloc_roundup((slots + padslots) * sizeof(int)) /
	    sizeof(int);
d206 1
a206 1
	amap->am_slots = malloc(totalslots * sizeof(int), M_UVMAMAP,
d211 4
a214 8
	amap->am_bckptr = malloc(totalslots * sizeof(int), M_UVMAMAP, waitf);
	if (amap->am_bckptr == NULL)
		goto fail2;

	amap->am_anon = malloc(totalslots * sizeof(struct vm_anon *),
	    M_UVMAMAP, waitf);
	if (amap->am_anon == NULL)
		goto fail3;
a217 4
fail3:
	free(amap->am_bckptr, M_UVMAMAP);
fail2:
	free(amap->am_slots, M_UVMAMAP);
a267 2
	free(amap->am_bckptr, M_UVMAMAP);
	free(amap->am_anon, M_UVMAMAP);
d364 2
a365 1
	slotalloc = malloc_roundup(slotneed * sizeof(int)) / sizeof(int);
d378 1
a378 5
	newsl = malloc(slotalloc * sizeof(int), M_UVMAMAP,
	    M_WAITOK | M_CANFAIL);
	newbck = malloc(slotalloc * sizeof(int), M_UVMAMAP,
	    M_WAITOK | M_CANFAIL);
	newover = malloc(slotalloc * sizeof(struct vm_anon *), M_UVMAMAP,
d380 1
a380 10
	if (newsl == NULL || newbck == NULL || newover == NULL) {
		if (newsl != NULL) {
			free(newsl, M_UVMAMAP);
		}
		if (newbck != NULL) {
			free(newbck, M_UVMAMAP);
		}
		if (newover != NULL) {
			free(newover, M_UVMAMAP);
		}
d388 3
a437 2
	free(oldbck, M_UVMAMAP);
	free(oldover, M_UVMAMAP);
@


1.40
log
@Use M_ZERO in a few more places to shave bytes from the kernel.

eyeballed and ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.39 2007/06/18 21:51:15 pedro Exp $	*/
d401 5
@


1.39
log
@Bring back Mickey's UVM anon change. Testing by thib@@, beck@@ and
ckuethe@@ for a while. Okay beck@@, "it is good timing" deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.38 2007/06/01 20:10:04 tedu Exp $	*/
d905 1
a905 1
	    M_UVMAMAP, M_NOWAIT);
a917 1
	memset(amap->am_ppref, 0, sizeof(int) * amap->am_maxslot);
@


1.38
log
@set hiwat mark for some of the more popular pools to reduce bouncing
ok art bob
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.37 2007/05/31 21:20:30 thib Exp $	*/
d68 2
d75 14
d248 1
a248 1
	if (amap)
d251 2
d271 1
d488 2
a489 2
			if (amap->am_anon[lcv]->u.an_page != NULL)
				pmap_page_protect(amap->am_anon[lcv]->u.an_page,
d500 2
a501 2
		if (amap->am_anon[slot]->u.an_page != NULL)
			pmap_page_protect(amap->am_anon[slot]->u.an_page, prot);
d509 1
a509 1
 * => called from amap_unref when the final reference to an amap is 
d522 10
d538 1
a538 1
		if (anon == NULL || anon->an_ref == 0) 
d719 2
d772 1
a772 1
		pg = anon->u.an_page;
d1070 76
@


1.37
log
@zap the vm_amap am_l simplelock, and amap_{lock/unlock} macros for
simple_{lock/unlock}.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.36 2007/04/27 16:38:13 art Exp $	*/
d158 1
@


1.36
log
@Some ANSI function prototypes and misc cleanups.

only binary change is the line numbers to asserts.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.35 2007/04/13 18:57:49 art Exp $	*/
a177 1
	simple_lock_init(&amap->am_l);
a259 1
	amap_unlock(amap);	/* mainly for lock debugging */
a296 1
	amap_lock(amap);					/* lock! */
a312 1
		amap_unlock(amap);
a334 1
		amap_unlock(amap);
a351 1
	amap_unlock(amap);	/* unlock in case we sleep in malloc */
a382 1
	amap_lock(amap);			/* re-lock! */
a427 3
	/* unlock */
	amap_unlock(amap);

a627 1
	amap_lock(srcamap);
a640 1
		amap_unlock(srcamap);
a680 2
	amap_unlock(srcamap);

a730 2
	amap_lock(amap);

a767 1
				amap_unlock(amap);
a792 1
				amap_unlock(amap);
a822 2

	amap_unlock(amap);
a841 5
	 * lock the amap
	 */
	amap_lock(origref->ar_amap);

	/*
a858 2

	amap_unlock(origref->ar_amap);
@


1.35
log
@While splitting flags and pqflags might have been a good idea in theory
to separate locking, on most modern machines this is not enough
since operations on short types touch other short types that share the
same word in memory.

Merge pg_flags and pqflags again and now use atomic operations to change
the flags. Also bump wire_count to an int and pg_version might go
int as well, just for alignment.

tested by many, many. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.34 2007/04/04 17:44:45 art Exp $	*/
d117 1
a117 2
pp_getreflen(ppref, offset, refp, lenp)
	int *ppref, offset, *refp, *lenp;
d135 1
a135 2
pp_setreflen(ppref, offset, ref, len)
	int *ppref, offset, ref, len;
d151 1
a151 2
amap_init()

d167 1
a167 2
amap_alloc1(slots, padslots, waitf)
	int slots, padslots, waitf;
d222 1
a222 3
amap_alloc(sz, padsz, waitf)
	vaddr_t sz, padsz;
	int waitf;
d248 1
a248 2
amap_free(amap)
	struct vm_amap *amap;
d278 1
a278 3
amap_extend(entry, addsize)
	vm_map_entry_t entry;
	vsize_t addsize;
d465 1
a465 3
amap_share_protect(entry, prot)
	vm_map_entry_t entry;
	vm_prot_t prot;
d505 1
a505 2
amap_wipeout(amap)
	struct vm_amap *amap;
d561 2
a562 6
amap_copy(map, entry, waitf, canchunk, startva, endva)
	vm_map_t map;
	vm_map_entry_t entry;
	int waitf;
	boolean_t canchunk;
	vaddr_t startva, endva;
d732 1
a732 3
amap_cow_now(map, entry)
	struct vm_map *map;
	struct vm_map_entry *entry;
d853 1
a853 3
amap_splitref(origref, splitref, offset)
	struct vm_aref *origref, *splitref;
	vaddr_t offset;
d896 1
a896 2
amap_pp_establish(amap)
	struct vm_amap *amap;
a914 1
	return;
d925 1
a925 5
amap_pp_adjref(amap, curslot, slotlen, adjval)
	struct vm_amap *amap;
	int curslot;
	vsize_t slotlen;
	int adjval;
d997 1
a997 3
amap_wiperange(amap, slotoff, slots)
	struct vm_amap *amap;
	int slotoff, slots;
@


1.34
log
@Mechanically rename the "flags" and "version" fields in struct vm_page
to "pg_flags" and "pg_version", so that they are a bit easier to work with.
Whoever uses generic names like this for a popular struct obviously doesn't
read much code.

Most architectures compile and there are no functionality changes.

deraadt@@ ok ("if something fails to compile, we fix that by hand")
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.33 2006/07/31 11:51:29 mickey Exp $	*/
d801 1
a801 1
				pg->pg_flags |= PG_WANTED;
d846 1
a846 1
			npg->pg_flags &= ~(PG_BUSY|PG_FAKE);
@


1.33
log
@fix uvmhist #2: args are always u_long so fix missing %d and %x and no %ll; no change for normal code
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.32 2006/07/26 23:15:55 mickey Exp $	*/
d800 2
a801 2
			if (pg->flags & PG_BUSY) {
				pg->flags |= PG_WANTED;
d846 1
a846 1
			npg->flags &= ~(PG_BUSY|PG_FAKE);
@


1.32
log
@fix fmts for UVMHIST_LOG() entries making it more useful on 64bit archs; miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.31 2006/07/13 22:51:26 deraadt Exp $	*/
d326 1
a326 1
		UVMHIST_LOG(maphist,"<- done (case 1), amap = %p, sltneed=%d", 
d354 1
a354 1
		UVMHIST_LOG(maphist,"<- done (case 2), amap = %p, slotneed=%d",
d455 1
a455 1
	UVMHIST_LOG(maphist,"<- done (case 3), amap = %p, slotneed=%d", 
d535 1
a535 1
		UVMHIST_LOG(maphist,"  processing anon %p, ref=%d", anon, 
d584 1
a584 1
	UVMHIST_LOG(maphist, "  (map=%p, entry=%p, waitf=%d)",
d645 1
a645 1
	UVMHIST_LOG(maphist,"  amap=%p, ref=%d, must copy it", 
@


1.31
log
@Back out the anon change.  Apparently it was tested by a few, but most of
us did not see it or get a chance to test it before it was commited. It
broke cvs, in the ami driver, making it not succeed at seeing it's devices.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.29 2005/12/10 11:45:43 miod Exp $	*/
d242 1
a242 1
	UVMHIST_LOG(maphist,"<- done, amap = 0x%x, sz=%d", amap, sz, 0, 0);
d271 1
a271 1
	UVMHIST_LOG(maphist,"<- done, freed amap = 0x%x", amap, 0, 0, 0);
d300 1
a300 1
	UVMHIST_LOG(maphist, "  (entry=0x%x, addsize=0x%x)", entry,addsize,0,0);
d326 1
a326 1
		UVMHIST_LOG(maphist,"<- done (case 1), amap = 0x%x, sltneed=%d", 
d354 2
a355 2
		UVMHIST_LOG(maphist,"<- done (case 2), amap = 0x%x, "
		    "slotneed=%d", amap, slotneed, 0, 0);
d455 1
a455 1
	UVMHIST_LOG(maphist,"<- done (case 3), amap = 0x%x, slotneed=%d", 
d522 1
a522 1
	UVMHIST_LOG(maphist,"(amap=0x%x)", amap, 0,0,0);
d535 1
a535 1
		UVMHIST_LOG(maphist,"  processing anon 0x%x, ref=%d", anon, 
d605 3
a607 3
			UVMHIST_LOG(maphist, "  chunk amap ==> clip 0x%x->0x%x"
			    "to 0x%x->0x%x", entry->start, entry->end, startva,
			    endva);
d614 2
a615 2
		UVMHIST_LOG(maphist, "<- done [creating new amap 0x%x->0x%x]", 
		entry->start, entry->end, 0, 0);
@


1.30
log
@from netbsd: make anons dynamically allocated from pool.
this results in lesse kva waste due to static preallocation of those
for every phys page and also every swap page.
tested by beck krw miod
@
text
@a67 3
struct simplelock amap_list_lock;
LIST_HEAD(, vm_amap) amap_list;

a72 20
static __inline void amap_list_insert(struct vm_amap *);
static __inline void amap_list_remove(struct vm_amap *);   

static __inline void
amap_list_insert(struct vm_amap *amap)
{

	simple_lock(&amap_list_lock);
	LIST_INSERT_HEAD(&amap_list, amap, am_list);
	simple_unlock(&amap_list_lock);
}

static __inline void
amap_list_remove(struct vm_amap *amap)
{ 

	simple_lock(&amap_list_lock);
	LIST_REMOVE(amap, am_list);
	simple_unlock(&amap_list_lock);
}
a155 3

	simple_lock_init(&amap_list_lock);

d238 1
a238 1
	if (amap) {
a241 3
		amap_list_insert(amap);
	}

a259 1
	KASSERT((amap->am_flags & AMAP_SWAPOFF) == 0);
d489 2
a490 2
			if (amap->am_anon[lcv]->an_page != NULL)
				pmap_page_protect(amap->am_anon[lcv]->an_page,
d501 2
a502 2
		if (amap->am_anon[slot]->an_page != NULL)
			pmap_page_protect(amap->am_anon[slot]->an_page, prot);
d510 1
a510 1
 * => called from amap_unref when the final reference to an amap is
a523 11
	KASSERT(amap->am_ref == 0);

	if (__predict_false((amap->am_flags & AMAP_SWAPOFF) != 0)) {
		/*
		 * amap_swap_off will call us again.
		 */
		amap_unlock(amap);
		return;
	}
	amap_list_remove(amap);

d530 1
a530 1
		if (anon == NULL || anon->an_ref == 0)
a710 2
	amap_list_insert(amap);

d774 1
a774 1
		pg = anon->an_page;
a1092 107

/*
 * amap_swap_off: pagein anonymous pages in amaps and drop swap slots.
 *
 * => called with swap_syscall_lock held.
 * => note that we don't always traverse all anons.
 *    eg. amaps being wiped out, released anons.
 * => return TRUE if failed.
 */

boolean_t
amap_swap_off(int startslot, int endslot)
{
	struct vm_amap *am;
	struct vm_amap *am_next;
	struct vm_amap marker_prev;
	struct vm_amap marker_next;
	struct proc *p = curproc;
	boolean_t rv = FALSE;

#if defined(DIAGNOSTIC)
	memset(&marker_prev, 0, sizeof(marker_prev));
	memset(&marker_next, 0, sizeof(marker_next));
#endif /* defined(DIAGNOSTIC) */

	PHOLD(p);
	simple_lock(&amap_list_lock);
	for (am = LIST_FIRST(&amap_list); am != NULL && !rv; am = am_next) {
		int i;

		LIST_INSERT_BEFORE(am, &marker_prev, am_list);
		LIST_INSERT_AFTER(am, &marker_next, am_list);

		if (!amap_lock_try(am)) {
			simple_unlock(&amap_list_lock);
			preempt(NULL);
			simple_lock(&amap_list_lock);
			am_next = LIST_NEXT(&marker_prev, am_list);
			if (am_next == &marker_next) {
				am_next = LIST_NEXT(am_next, am_list);
			} else {
				KASSERT(LIST_NEXT(am_next, am_list) ==
				    &marker_next);
			}
			LIST_REMOVE(&marker_prev, am_list);
			LIST_REMOVE(&marker_next, am_list);
			continue;
		}

		simple_unlock(&amap_list_lock);

		if (am->am_nused <= 0) {
			amap_unlock(am);
			goto next;
		}

		for (i = 0; i < am->am_nused; i++) {
			int slot;
			int swslot;
			struct vm_anon *anon;

			slot = am->am_slots[i];
			anon = am->am_anon[slot];
			simple_lock(&anon->an_lock);

			swslot = anon->an_swslot;
			if (swslot < startslot || endslot <= swslot) {
				simple_unlock(&anon->an_lock);
				continue;
			}

			am->am_flags |= AMAP_SWAPOFF;
			amap_unlock(am);

			rv = uvm_anon_pagein(anon);

			amap_lock(am);
			am->am_flags &= ~AMAP_SWAPOFF;
			if (amap_refs(am) == 0) {
				amap_wipeout(am);
				am = NULL;
				break;
			}
			if (rv) {
				break;
			}
			i = 0;
		}

		if (am) {
			amap_unlock(am);
		}
		
next:
		simple_lock(&amap_list_lock);
		KASSERT(LIST_NEXT(&marker_prev, am_list) == &marker_next ||
		    LIST_NEXT(LIST_NEXT(&marker_prev, am_list), am_list) ==
		    &marker_next);
		am_next = LIST_NEXT(&marker_next, am_list);
		LIST_REMOVE(&marker_prev, am_list);
		LIST_REMOVE(&marker_next, am_list);
	}
	simple_unlock(&amap_list_lock);
	PRELE(p);

	return rv;
}
@


1.29
log
@{en,re}trys -> {en,re}tries; eyeballed by jmc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.28 2004/12/30 08:28:39 niklas Exp $	*/
d68 3
d76 20
d179 3
d264 1
a264 1
	if (amap)
d268 3
d289 1
d519 2
a520 2
			if (amap->am_anon[lcv]->u.an_page != NULL)
				pmap_page_protect(amap->am_anon[lcv]->u.an_page,
d531 2
a532 2
		if (amap->am_anon[slot]->u.an_page != NULL)
			pmap_page_protect(amap->am_anon[slot]->u.an_page, prot);
d540 1
a540 1
 * => called from amap_unref when the final reference to an amap is 
d554 11
d571 1
a571 1
		if (anon == NULL || anon->an_ref == 0) 
d752 2
d817 1
a817 1
		pg = anon->u.an_page;
d1136 107
@


1.28
log
@Import M_CANFAIL support from NetBSD, removes a nasty panic during low-mem scenarios, instead generating an ENOMEM backfeed, ok tedu@@, prodded by many
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.27 2002/05/09 14:14:18 provos Exp $	*/
d741 1
a741 1
 *	map change until we are done copying all the map entrys.
@


1.27
log
@from netbsd; okay art@@

revision 1.43
date: 2002/03/28 06:06:29;  author: nathanw;  state: Exp;  lines: +13 -3
In amap_pp_adjref(), avoid incorrectly merging the first two chunks in
a ppref array when the range being adjusted includes the beginning of
the array.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.26 2002/03/14 01:27:18 millert Exp $	*/
a281 1
 * => XXXCDC: needs a waitflag or failure return value?
d284 1
a284 1
void
a308 1

d328 1
a328 1
		return;				/* done! */
d335 1
d349 1
d354 3
a356 3
		UVMHIST_LOG(maphist,"<- done (case 2), amap = 0x%x, slotneed=%d", 
		    amap, slotneed, 0, 0);
		return;
d371 2
a372 1
		newppref = malloc(slotalloc *sizeof(int), M_UVMAMAP, M_NOWAIT);
d380 18
a397 4
	newsl = malloc(slotalloc * sizeof(int), M_UVMAMAP, M_WAITOK);
	newbck = malloc(slotalloc * sizeof(int), M_UVMAMAP, M_WAITOK);
	newover = malloc(slotalloc * sizeof(struct vm_anon *),
	    M_UVMAMAP, M_WAITOK);
d415 2
a416 1
	memset(newover + amap->am_nslot, 0, sizeof(struct vm_anon *) * slotadded);
d457 1
@


1.26
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.25 2002/02/28 16:32:03 provos Exp $	*/
d957 11
a967 1
	pp_getreflen(ppref, prevlcv, &prevref, &prevlen);
@


1.25
log
@setting prevlcv to 0 doesnt require additional checks. from chs@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.24 2002/02/20 20:16:23 provos Exp $	*/
d72 1
a72 1
static struct vm_amap *amap_alloc1 __P((int, int, int));
d108 2
a109 2
static __inline void pp_getreflen __P((int *, int, int *, int *));
static __inline void pp_setreflen __P((int *, int, int, int));
@


1.24
log
@merging ppref could cause negative back reference, fix from niklas@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.23 2002/02/19 17:57:34 provos Exp $	*/
d941 1
a941 1
 	prevlcv = -1;
d957 1
a957 2
	if (prevlcv >= 0)
		pp_getreflen(ppref, prevlcv, &prevref, &prevlen);
d978 1
a978 2
		if (prevlcv >= 0 && lcv == prevlcv + prevlen &&
		    ref == prevref) {
@


1.23
log
@amap_pp_adjref could fragment ppref array; fix by merging adjacent chunks;
from Chuck Silvers; okay art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.22 2002/02/12 18:36:53 provos Exp $	*/
d957 2
a958 1
	pp_getreflen(ppref, prevlcv, &prevref, &prevlen);
d979 2
a980 1
		if (lcv == prevlcv + prevlen && ref == prevref) {
@


1.22
log
@amap_extend is very expensive, allocate more memory to avoid bcopy the next
time around; my fix was very similiar to netbsd, take netbsd fix; okay art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.21 2002/01/23 00:39:48 art Exp $	*/
d936 2
a937 6
	int stopslot, *ppref, lcv;
	int ref, len;

	/*
	 * get init values
	 */
d941 1
d944 2
a945 2
	 * first advance to the correct place in the ppref array, fragment
	 * if needed.
d955 1
d957 1
d960 2
a961 1
	 * now adjust reference counts in range (make sure we dont overshoot)
d975 1
a975 1
		ref = ref + adjval;    /* ADJUST! */
d978 5
a982 1
		pp_setreflen(ppref, lcv, ref, len);
@


1.21
log
@Pool deals fairly well with physical memory shortage, but it doesn't deal
well (not at all) with shortages of the vm_map where the pages are mapped
(usually kmem_map).

Try to deal with it:
 - group all information the backend allocator for a pool in a separate
   struct. The pool will only have a pointer to that struct.
 - change the pool_init API to reflect that.
 - link all pools allocating from the same allocator on a linked list.
 - Since an allocator is responsible to wait for physical memory it will
   only fail (waitok) when it runs out of its backing vm_map, carefully
   drain pools using the same allocator so that va space is freed.
   (see comments in code for caveats and details).
 - change pool_reclaim to return if it actually succeeded to free some
   memory, use that information to make draining easier and more efficient.
 - get rid of PR_URGENT, noone uses it.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.20 2002/01/15 20:09:56 art Exp $	*/
d174 1
a174 1
	int totalslots = slots + padslots;
d180 2
d239 2
a240 1
		memset(amap->am_anon, 0, (slots + padslots) * sizeof(struct vm_anon *));
d292 1
a292 1
	int slotmapped, slotadd, slotneed;
d367 1
d371 1
a371 1
		newppref = malloc(slotneed * sizeof(int), M_UVMAMAP, M_NOWAIT);
d379 3
a381 3
	newsl = malloc(slotneed * sizeof(int), M_UVMAMAP, M_WAITOK);
	newbck = malloc(slotneed * sizeof(int), M_UVMAMAP, M_WAITOK);
	newover = malloc(slotneed * sizeof(struct vm_anon *),
d390 1
a390 1
	slotadded = slotneed - amap->am_nslot;
d419 2
a420 1
		pp_setreflen(newppref, amap->am_nslot, 1, slotadded);
d426 1
a426 1
	amap->am_maxslot = slotneed;
d672 2
@


1.20
log
@A fix to amap_wiperange from Chuck Cranor that has been in my tree for
a while. With his words:
"    here is a fix for a bug in amap_wiperange  (was posted on netbsd's
tech-kern list).   i screwed up the loop control :("
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.19 2002/01/02 22:23:25 miod Exp $	*/
d160 1
a160 2
	    "amappl", 0, pool_page_alloc_nointr, pool_page_free_nointr, 
	    M_UVMAMAP);
@


1.19
log
@Back out a few more uvm changes, especially wrt swap usage.
This unbreaks m68k m88k sparc and perhaps others, which eventually froze
when hitting swap.
Tested by various people on various platforms.
ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.14 2001/11/07 02:55:50 art Exp $	*/
d990 1
a990 1
	int byanon, lcv, stop, curslot, ptr;
d1006 1
d1009 1
a1009 5
	/*
	 * ok, now do it!
	 */

	for (; lcv < stop; lcv++) {
d1012 3
a1014 5
		/*
		 * verify the anon is ok.
		 */
		if (byanon) {
			if (amap->am_anon[lcv] == NULL)
a1015 1
			curslot = lcv;
d1018 2
a1019 1
			if (curslot < slotoff || curslot >= stop)
d1021 2
@


1.18
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_amap.c,v 1.15 2001/11/11 01:16:56 art Exp $	*/
/*	$NetBSD: uvm_amap.c,v 1.30 2001/02/18 21:19:09 chs Exp $	*/
a257 1
	LOCK_ASSERT(simple_lock_held(&amap->am_l));
a459 2
	LOCK_ASSERT(simple_lock_held(&amap->am_l));

a502 2
	LOCK_ASSERT(simple_lock_held(&amap->am_l));

a789 1
				/* nanon is locked! */
d801 1
a801 2
					nanon->an_ref--;
					simple_unlock(&nanon->an_lock);
a827 1
			simple_unlock(&nanon->an_lock);
@


1.17
log
@Yet another sync to NetBSD uvm.
Today we add a pmap argument to pmap_update() and allocate map entries for
kernel_map from kmem_map instead of using the static entries. This should
get rid of MAX_KMAPENT panics. Also some uvm_loan problems are fixed.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_amap.c,v 1.16 2001/11/28 19:28:14 art Exp $	*/
/*	$NetBSD: uvm_amap.c,v 1.33 2001/07/22 13:34:12 wiz Exp $	*/
d104 1
a104 1
 *
d160 1
a160 1
	    "amappl", 0, pool_page_alloc_nointr, pool_page_free_nointr,
d286 1
a286 1
	struct vm_map_entry *entry;
d327 1
a327 1
		UVMHIST_LOG(maphist,"<- done (case 1), amap = 0x%x, sltneed=%d",
d340 1
a340 1
				amap_pp_adjref(amap, slotoff + slotmapped,
d343 1
a343 1
			pp_setreflen(amap->am_ppref, amap->am_nslot, 1,
d353 1
a353 1
		UVMHIST_LOG(maphist,"<- done (case 2), amap = 0x%x, slotneed=%d",
d362 1
a362 1
	 * XXXCDC: could we take advantage of a kernel realloc()?
d415 1
a415 1
			amap_pp_adjref(amap, slotoff + slotmapped,
d436 1
a436 1
	UVMHIST_LOG(maphist,"<- done (case 3), amap = 0x%x, slotneed=%d",
d455 1
a455 1
	struct vm_map_entry *entry;
d492 1
a492 1
 * => called from amap_unref when the final reference to an amap is
d514 1
a514 1
		if (anon == NULL || anon->an_ref == 0)
d519 1
a519 1
		UVMHIST_LOG(maphist,"  processing anon 0x%x, ref=%d", anon,
d545 1
a545 1
 *
d558 2
a559 2
	struct vm_map *map;
	struct vm_map_entry *entry;
d598 1
a598 1
		UVMHIST_LOG(maphist, "<- done [creating new amap 0x%x->0x%x]",
d629 1
a629 1
	UVMHIST_LOG(maphist,"  amap=%p, ref=%d, must copy it",
d686 1
a686 1
		amap_pp_adjref(srcamap, entry->aref.ar_pageoff,
d816 1
a816 1

@


1.17.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.21 2002/01/23 00:39:48 art Exp $	*/
d160 2
a161 1
	    "amappl", &pool_allocator_nointr);
d258 1
d461 2
d506 2
d795 1
d807 2
a808 1
					simple_lock(&nanon->an_lock);
d835 1
d998 1
a998 1
	int byanon, lcv, stop, curslot, ptr, slotend;
a1013 1
		slotend = slotoff + slots;
d1016 5
a1020 1
	while (lcv < stop) {
d1023 5
a1027 3
  		if (byanon) {
			curslot = lcv++;	/* lcv advances here */
			if (amap->am_anon[curslot] == NULL)
d1029 1
d1032 1
a1032 2
			if (curslot < slotoff || curslot >= slotend) {
				lcv++;		/* lcv advances here */
a1033 2
			}
			stop--;	/* drop stop, since anon will be removed */
@


1.17.2.2
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_amap.c,v 1.17.2.1 2002/01/31 22:55:50 niklas Exp $	*/
/*	$NetBSD: uvm_amap.c,v 1.40 2001/12/05 01:33:09 enami Exp $	*/
d153 2
a154 1
amap_init(void)
a155 1

a158 1

d174 1
a174 1
	int totalslots;
a179 2
	totalslots = malloc_roundup((slots + padslots) * sizeof(int)) /
	    sizeof(int);
d232 1
a232 1
	AMAP_B2SLOT(slots, sz);
d237 1
a237 2
		memset(amap->am_anon, 0,
		    amap->am_maxslot * sizeof(struct vm_anon *));
d247 1
a247 1
 * => the amap must be unlocked
d257 1
a257 1
	LOCK_ASSERT(!simple_lock_held(&amap->am_l));
d265 1
d267 1
d289 1
a289 1
	int slotmapped, slotadd, slotneed, slotadded, slotalloc;
d293 1
a293 1
	int *newsl, *newbck, *oldsl, *oldbck;
d295 1
a363 1
	slotalloc = malloc_roundup(slotneed * sizeof(int)) / sizeof(int);
d367 1
a367 2
		newppref = malloc(slotalloc * sizeof(int), M_UVMAMAP,
		    M_NOWAIT);
d375 3
a377 3
	newsl = malloc(slotalloc * sizeof(int), M_UVMAMAP, M_WAITOK);
	newbck = malloc(slotalloc * sizeof(int), M_UVMAMAP, M_WAITOK);
	newover = malloc(slotalloc * sizeof(struct vm_anon *),
d386 1
a386 1
	slotadded = slotalloc - amap->am_nslot;
d402 1
d415 1
a415 2
		pp_setreflen(newppref, amap->am_nslot, 1,
		    slotneed - amap->am_nslot);
d421 1
a421 1
	amap->am_maxslot = slotalloc;
d423 1
d425 2
a458 2
	LOCK_ASSERT(simple_lock_held(&amap->am_l));

d482 1
a501 1
	amap_unlock(amap);
d511 2
a512 1
		simple_lock(&anon->an_lock);
d515 1
a518 1

a521 1

a523 14

		/*
		 * XXX
		 * releasing the swap space held by an N anons is an O(N^2)
		 * operation because of the implementation of extents.
		 * if there are many anons, tearing down an exiting process'
		 * address space can take many seconds, which causes very
		 * annoying pauses.  we yield here to give other processes
		 * a chance to run.  this should be removed once the performance
		 * of swap space management is improved.
		 */

		if (curproc->p_schedflags & PSCHED_SHOULDYIELD)
			preempt(NULL);
a644 1
		amap_unlock(amap);
a666 2
	memset(&amap->am_anon[lcv], 0,
	    (amap->am_maxslot - lcv) * sizeof(struct vm_anon *));
d694 4
a737 1

a788 1
				/* nanon is locked! */
d800 1
a800 2
					nanon->an_ref--;
					simple_unlock(&nanon->an_lock);
a812 1

a821 1

a826 1
			simple_unlock(&nanon->an_lock);
d828 1
d830 6
a835 1
	}
d857 3
d895 1
a901 1

d906 4
d932 4
d1011 1
a1011 1
		if (byanon) {
a1027 1

a1040 1

a1044 1

a1048 1

@


1.17.2.3
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.17.2.2 2002/02/02 03:28:26 art Exp $	*/
d72 1
a72 1
static struct vm_amap *amap_alloc1(int, int, int);
d108 2
a109 2
static __inline void pp_getreflen(int *, int, int *, int *);
static __inline void pp_setreflen(int *, int, int, int);
d937 2
a938 2
 	int stopslot, *ppref, lcv, prevlcv;
 	int ref, len, prevref, prevlen;
a941 1
 	prevlcv = 0;
d944 2
a945 2
 	 * first advance to the correct place in the ppref array,
 	 * fragment if needed.
a954 12
		prevlcv = lcv;
	}
	if (lcv != 0)
		pp_getreflen(ppref, prevlcv, &prevref, &prevlen);
	else {
		/* Ensure that the "prevref == ref" test below always
		 * fails, since we're starting from the beginning of
		 * the ppref array; that is, there is no previous
		 * chunk.  
		 */
		prevref = -1;
		prevlen = 0;
d958 1
a958 2
	 * now adjust reference counts in range.  merge the first
	 * changed entry with the last unchanged entry if possible.
d972 1
a972 1
		ref += adjval;
d975 1
a975 5
		if (lcv == prevlcv + prevlen && ref == prevref) {
			pp_setreflen(ppref, prevlcv, ref, prevlen + len);
		} else {
			pp_setreflen(ppref, lcv, ref, len);
		}
@


1.17.2.4
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_amap.c,v 1.17.2.3 2002/06/11 03:33:03 art Exp $	*/
/*	$NetBSD: uvm_amap.c,v 1.45 2002/09/15 16:54:27 chs Exp $	*/
d281 2
d284 1
a284 1
int
d307 2
a308 1
	amap_lock(amap);
d328 1
a328 1
		return 0;
a334 1

a347 1

d352 3
a354 4

		UVMHIST_LOG(maphist,"<- done (case 2), amap = 0x%x, "
		    "slotneed=%d", amap, slotneed, 0, 0);
		return 0;
d370 1
a370 1
		    M_WAITOK);
d372 1
d378 5
a382 19
	newsl = malloc(slotalloc * sizeof(int), M_UVMAMAP,
	    M_WAITOK);
	newbck = malloc(slotalloc * sizeof(int), M_UVMAMAP,
	    M_WAITOK);
	newover = malloc(slotalloc * sizeof(struct vm_anon *), M_UVMAMAP,
	    M_WAITOK);
	if (newsl == NULL || newbck == NULL || newover == NULL) {
		if (newsl != NULL) {
			free(newsl, M_UVMAMAP);
		}
		if (newbck != NULL) {
			free(newbck, M_UVMAMAP);
		}
		if (newover != NULL) {
			free(newover, M_UVMAMAP);
		}
		return ENOMEM;
	}
	amap_lock(amap);
d399 1
a399 2
	memset(newover + amap->am_nslot, 0, sizeof(struct vm_anon *) *
	    slotadded);
a435 1
	return 0;
d842 2
a846 2
			npg->flags &= ~(PG_BUSY|PG_FAKE);
			UVM_PAGE_OWN(npg, NULL);
@


1.17.2.5
log
@Some minor cleanups to reduce diff to netbsd.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.17.2.4 2002/11/04 18:02:32 art Exp $	*/
d952 2
a953 2
	int stopslot, *ppref, lcv, prevlcv;
	int ref, len, prevref, prevlen;
d957 1
a957 1
	prevlcv = 0;
d960 2
a961 2
	 * first advance to the correct place in the ppref array,
	 * fragment if needed.
@


1.16
log
@Sync in more uvm from NetBSD. Mostly just cosmetic stuff.
Contains also support for page coloring.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_amap.c,v 1.15 2001/11/11 01:16:56 art Exp $	*/
/*	$NetBSD: uvm_amap.c,v 1.32 2001/06/02 18:09:25 chs Exp $	*/
@


1.15
log
@Sync in more stuff from NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_amap.c,v 1.14 2001/11/07 02:55:50 art Exp $	*/
/*	$NetBSD: uvm_amap.c,v 1.30 2001/02/18 21:19:09 chs Exp $	*/
d104 1
a104 1
 * 
d160 1
a160 1
	    "amappl", 0, pool_page_alloc_nointr, pool_page_free_nointr, 
d286 1
a286 1
	vm_map_entry_t entry;
d327 1
a327 1
		UVMHIST_LOG(maphist,"<- done (case 1), amap = 0x%x, sltneed=%d", 
d340 1
a340 1
				amap_pp_adjref(amap, slotoff + slotmapped, 
d343 1
a343 1
			pp_setreflen(amap->am_ppref, amap->am_nslot, 1, 
d353 1
a353 1
		UVMHIST_LOG(maphist,"<- done (case 2), amap = 0x%x, slotneed=%d", 
d362 1
a362 1
	 * XXXCDC: could we take advantage of a kernel realloc()?  
d415 1
a415 1
			amap_pp_adjref(amap, slotoff + slotmapped, 
d436 1
a436 1
	UVMHIST_LOG(maphist,"<- done (case 3), amap = 0x%x, slotneed=%d", 
d455 1
a455 1
	vm_map_entry_t entry;
d492 1
a492 1
 * => called from amap_unref when the final reference to an amap is 
d514 1
a514 1
		if (anon == NULL || anon->an_ref == 0) 
d519 1
a519 1
		UVMHIST_LOG(maphist,"  processing anon 0x%x, ref=%d", anon, 
d545 1
a545 1
 * 
d558 2
a559 2
	vm_map_t map;
	vm_map_entry_t entry;
d598 1
a598 1
		UVMHIST_LOG(maphist, "<- done [creating new amap 0x%x->0x%x]", 
d629 1
a629 1
	UVMHIST_LOG(maphist,"  amap=%p, ref=%d, must copy it", 
d686 1
a686 1
		amap_pp_adjref(srcamap, entry->aref.ar_pageoff, 
d816 1
a816 1
	
@


1.14
log
@Another sync of uvm to NetBSD. Just minor fiddling, no major changes.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_amap.c,v 1.13 2001/11/06 13:36:52 art Exp $	*/
/*	$NetBSD: uvm_amap.c,v 1.27 2000/11/25 06:27:59 chs Exp $	*/
d257 2
a258 4
#ifdef DIAGNOSTIC
	if (amap->am_ref || amap->am_nused)
		panic("amap_free");
#endif
d382 1
a382 5

#ifdef DIAGNOSTIC
	if (amap->am_maxslot >= slotneed)
		panic("amap_extend: amap changed during malloc");
#endif
d461 2
d506 2
d794 2
a795 1
			if (nanon)
d797 1
a797 1
			else
d807 2
a808 1
					simple_lock(&nanon->an_lock);
d835 1
@


1.13
log
@More sync to NetBSD.
 - Use malloc/free instead of MALLOC/FREE for variable sized allocations.
 - Move the memory inheritance code to sys/mman.h and rename from VM_* to MAP_*
 - various cleanups and simplifications.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_amap.c,v 1.12 2001/11/06 01:35:04 art Exp $	*/
/*	$NetBSD: uvm_amap.c,v 1.26 2000/08/03 00:47:02 thorpej Exp $	*/
d51 1
d325 1
a325 1
			amap_pp_adjref(amap, slotoff + slotmapped, addsize, 1);
d343 2
a344 2
				    (amap->am_nslot - (slotoff + slotmapped)) <<
				    PAGE_SHIFT, 1);
d422 1
a422 2
			    (amap->am_nslot - (slotoff + slotmapped)) <<
			    PAGE_SHIFT, 1);
d570 2
a571 1
	UVMHIST_LOG(maphist, "  (map=%p, entry=%p, waitf=%d)", map, entry, waitf, 0);
d689 1
a689 1
		    entry->end - entry->start, -1);
d807 2
a808 1
				if (nanon)
d810 1
a859 1
	UVMHIST_FUNC("amap_splitref"); UVMHIST_CALLED(maphist);
d931 1
a931 1
amap_pp_adjref(amap, curslot, bytelen, adjval)
d934 1
a934 1
	vsize_t bytelen;
d937 1
a937 1
	int slots, stopslot, *ppref, lcv;
d944 1
a944 2
	AMAP_B2SLOT(slots, bytelen);
	stopslot = curslot + slots;
a998 1
	UVMHIST_FUNC("amap_wiperange"); UVMHIST_CALLED(maphist);
@


1.12
log
@Move the last content from vm/ to uvm/
The only thing left in vm/ are just dumb wrappers.
vm/vm.h includes uvm/uvm_extern.h
vm/pmap.h includes uvm/uvm_pmap.h
vm/vm_page.h includes uvm/uvm_page.h
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_amap.c,v 1.11 2001/11/05 22:14:54 art Exp $	*/
/*	$NetBSD: uvm_amap.c,v 1.24 2000/06/27 17:29:17 mrg Exp $	*/
d261 3
a263 3
	FREE(amap->am_slots, M_UVMAMAP);
	FREE(amap->am_bckptr, M_UVMAMAP);
	FREE(amap->am_anon, M_UVMAMAP);
d266 1
a266 1
		FREE(amap->am_ppref, M_UVMAMAP);
d370 1
a370 2
		MALLOC(newppref, int *, slotneed * sizeof(int), M_UVMAMAP,
		    M_NOWAIT);
d373 2
a374 2
			FREE(amap->am_ppref, M_UVMAMAP);
			    amap->am_ppref = PPREF_NONE;
d378 4
a381 4
	MALLOC(newsl, int *, slotneed * sizeof(int), M_UVMAMAP, M_WAITOK);
	MALLOC(newbck, int *, slotneed * sizeof(int), M_UVMAMAP, M_WAITOK);
	MALLOC(newover, struct vm_anon **, slotneed * sizeof(struct vm_anon *),
						   M_UVMAMAP, M_WAITOK);
d435 3
a437 3
	FREE(oldsl, M_UVMAMAP);
	FREE(oldbck, M_UVMAMAP);
	FREE(oldover, M_UVMAMAP);
d440 1
a440 1
		FREE(oldppref, M_UVMAMAP);
d841 1
a841 1
	return;
d902 1
a902 1
	MALLOC(amap->am_ppref, int *, sizeof(int) * amap->am_maxslot,
@


1.11
log
@Minor sync to NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_amap.c,v 1.10 2001/09/11 20:05:25 miod Exp $	*/
/*	$NetBSD: uvm_amap.c,v 1.23 2000/06/26 14:21:16 mrg Exp $	*/
a51 2

#include <vm/vm.h>
@


1.10
log
@Don't include <vm/vm_kern.h> if you don't need foo_map.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_amap.c,v 1.9 2001/07/18 10:47:05 art Exp $	*/
/*	$NetBSD: uvm_amap.c,v 1.22 1999/09/12 01:17:33 chs Exp $	*/
a53 1
#include <vm/vm_page.h>
@


1.9
log
@Get rid of the PMAP_NEW option by making it mandatory for all archs.
The archs that didn't have a proper PMAP_NEW now have a dummy implementation
with wrappers around the old functions.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.c,v 1.8 2001/03/15 11:48:17 art Exp $	*/
a54 1
#include <vm/vm_kern.h>
@


1.8
log
@Bring in revision 1.21 from NetBSD.

Be more careful in amap_alloc1 when handling failed allocations. We could have
incorrectly returned success when the first or second of three allocations
failed.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_amap.c,v 1.7 2001/01/29 02:07:42 niklas Exp $	*/
/*	$NetBSD: uvm_amap.c,v 1.21 1999/07/06 02:15:53 cgd Exp $	*/
d481 2
a482 3
				pmap_page_protect(
				    PMAP_PGARG(amap->am_anon[lcv]->u.an_page),
				prot);
d493 1
a493 2
			pmap_page_protect(
			    PMAP_PGARG(amap->am_anon[slot]->u.an_page), prot);
@


1.7
log
@$OpenBSD$
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_amap.c,v 1.20 1999/04/11 04:04:11 chs Exp $	*/
/*	$NetBSD: uvm_amap.c,v 1.20 1999/04/11 04:04:11 chs Exp $	*/
a192 11
	MALLOC(amap->am_slots,  int *, totalslots * sizeof(int), M_UVMAMAP, waitf);
	if (amap->am_slots) {
		MALLOC(amap->am_bckptr, int *, totalslots * sizeof(int), M_UVMAMAP, waitf);
		if (amap->am_bckptr) {
			MALLOC(amap->am_anon, struct vm_anon **, 
			    totalslots * sizeof(struct vm_anon *), M_UVMAMAP, waitf);
		}
	}

	if (amap->am_anon)
		return(amap);
d194 21
a214 5
	if (amap->am_slots) {
		FREE(amap->am_slots, M_UVMAMAP);
		if (amap->am_bckptr)
			FREE(amap->am_bckptr, M_UVMAMAP);
	}
@


1.6
log
@seperate -> separate, okay aaron@@
@
text
@d1 1
@


1.5
log
@Convert bzero to memset(X, 0..) and bcopy to memcpy.
This is to match (make diffs smaller) the code in NetBSD.
new gcc inlines those functions, so this could also be a performance win.
@
text
@d846 1
a846 1
 * amap_splitref: split a single reference into two seperate references
@


1.4
log
@Fix the NetBSD id strings.
@
text
@d235 1
a235 1
		bzero(amap->am_anon, (slots + padslots) * sizeof(struct vm_anon *));
d396 1
a396 1
	bcopy(oldsl, newsl, sizeof(int) * amap->am_nused);
d401 2
a402 2
	bcopy(oldover, newover, sizeof(struct vm_anon *) * amap->am_nslot);
	bzero(newover + amap->am_nslot, sizeof(struct vm_anon *) * slotadded);
d407 2
a408 2
	bcopy(oldbck, newbck, sizeof(int) * amap->am_nslot);
	bzero(newbck + amap->am_nslot, sizeof(int) * slotadded); /* XXX: needed? */
d415 2
a416 2
		bcopy(oldppref, newppref, sizeof(int) * amap->am_nslot);
		bzero(newppref + amap->am_nslot, sizeof(int) * slotadded);
d917 1
a917 1
	bzero(amap->am_ppref, sizeof(int) * amap->am_maxslot);
@


1.3
log
@sync with NetBSD from 1999.05.24 (there is a reason for this date)
 Mostly cleanups, but also a few improvements to pagedaemon for better
 handling of low memory and/or low swap conditions.
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_amap.c,v 1.2 1999/02/26 05:32:06 art Exp $	*/
/*	$NetBSD: uvm_amap.c,v 1.19 1999/01/28 14:46:27 chuck Exp $	*/
@


1.3.4.1
log
@Sync with -current
@
text
@d1 2
a2 1
/*	$NetBSD: uvm_amap.c,v 1.20 1999/04/11 04:04:11 chs Exp $	*/
@


1.3.4.2
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_amap.c,v 1.8 2001/03/15 11:48:17 art Exp $	*/
/*	$NetBSD: uvm_amap.c,v 1.21 1999/07/06 02:15:53 cgd Exp $	*/
d192 8
d201 2
a202 13
	amap->am_slots = malloc(totalslots * sizeof(int), M_UVMAMAP,
	    waitf);
	if (amap->am_slots == NULL)
		goto fail1;

	amap->am_bckptr = malloc(totalslots * sizeof(int), M_UVMAMAP, waitf);
	if (amap->am_bckptr == NULL)
		goto fail2;

	amap->am_anon = malloc(totalslots * sizeof(struct vm_anon *),
	    M_UVMAMAP, waitf);
	if (amap->am_anon == NULL)
		goto fail3;
d204 5
a208 7
	return(amap);

fail3:
	free(amap->am_bckptr, M_UVMAMAP);
fail2:
	free(amap->am_slots, M_UVMAMAP);
fail1:
d235 1
a235 1
		memset(amap->am_anon, 0, (slots + padslots) * sizeof(struct vm_anon *));
d396 1
a396 1
	memcpy(newsl, oldsl, sizeof(int) * amap->am_nused);
d401 2
a402 2
	memcpy(newover, oldover, sizeof(struct vm_anon *) * amap->am_nslot);
	memset(newover + amap->am_nslot, 0, sizeof(struct vm_anon *) * slotadded);
d407 2
a408 2
	memcpy(newbck, oldbck, sizeof(int) * amap->am_nslot);
	memset(newbck + amap->am_nslot, 0, sizeof(int) * slotadded); /* XXX: needed? */
d415 2
a416 2
		memcpy(newppref, oldppref, sizeof(int) * amap->am_nslot);
		memset(newppref + amap->am_nslot, 0, sizeof(int) * slotadded);
d846 1
a846 1
 * amap_splitref: split a single reference into two separate references
d917 1
a917 1
	memset(amap->am_ppref, 0, sizeof(int) * amap->am_maxslot);
@


1.3.4.3
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_amap.c,v 1.3.4.2 2001/05/14 22:47:44 niklas Exp $	*/
/*	$NetBSD: uvm_amap.c,v 1.22 1999/09/12 01:17:33 chs Exp $	*/
d55 1
d481 3
a483 2
				pmap_page_protect(amap->am_anon[lcv]->u.an_page,
						  prot);
d494 2
a495 1
			pmap_page_protect(amap->am_anon[slot]->u.an_page, prot);
@


1.3.4.4
log
@merge in -current
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_amap.c,v 1.30 2001/02/18 21:19:09 chs Exp $	*/
a50 1
#include <sys/kernel.h>
d53 3
d259 4
a262 2
	KASSERT(amap->am_ref == 0 && amap->am_nused == 0);
	LOCK_ASSERT(simple_lock_held(&amap->am_l));
d264 3
a266 3
	free(amap->am_slots, M_UVMAMAP);
	free(amap->am_bckptr, M_UVMAMAP);
	free(amap->am_anon, M_UVMAMAP);
d269 1
a269 1
		free(amap->am_ppref, M_UVMAMAP);
d327 1
a327 1
			amap_pp_adjref(amap, slotoff + slotmapped, slotadd, 1);
d345 2
a346 2
				    (amap->am_nslot - (slotoff + slotmapped)),
				    1);
d373 2
a374 1
		newppref = malloc(slotneed * sizeof(int), M_UVMAMAP, M_NOWAIT);
d377 2
a378 2
			free(amap->am_ppref, M_UVMAMAP);
			amap->am_ppref = PPREF_NONE;
d382 4
a385 4
	newsl = malloc(slotneed * sizeof(int), M_UVMAMAP, M_WAITOK);
	newbck = malloc(slotneed * sizeof(int), M_UVMAMAP, M_WAITOK);
	newover = malloc(slotneed * sizeof(struct vm_anon *),
	    M_UVMAMAP, M_WAITOK);
d387 5
a391 1
	KASSERT(amap->am_maxslot < slotneed);
d425 2
a426 1
			    (amap->am_nslot - (slotoff + slotmapped)), 1);
d439 3
a441 3
	free(oldsl, M_UVMAMAP);
	free(oldbck, M_UVMAMAP);
	free(oldover, M_UVMAMAP);
d444 1
a444 1
		free(oldppref, M_UVMAMAP);
a470 2
	LOCK_ASSERT(simple_lock_held(&amap->am_l));

a513 2
	LOCK_ASSERT(simple_lock_held(&amap->am_l));

d574 1
a574 2
	UVMHIST_LOG(maphist, "  (map=%p, entry=%p, waitf=%d)",
		    map, entry, waitf, 0);
d692 1
a692 1
		    (entry->end - entry->start) >> PAGE_SHIFT, -1);
d799 1
a799 2
			if (nanon) {
				/* nanon is locked! */
d801 1
a801 1
			} else
d810 1
a810 3
				if (nanon) {
					nanon->an_ref--;
					simple_unlock(&nanon->an_lock);
a811 1
				}
a835 1
			simple_unlock(&nanon->an_lock);
d845 1
a845 1
	amap_unlock(amap);
d861 1
d906 1
a906 1
	amap->am_ppref = malloc(sizeof(int) * amap->am_maxslot,
d933 1
a933 1
amap_pp_adjref(amap, curslot, slotlen, adjval)
d936 1
a936 1
	vsize_t slotlen;
d939 1
a939 1
	int stopslot, *ppref, lcv;
d946 2
a947 1
	stopslot = curslot + slotlen;
d1002 1
@


1.3.4.5
log
@Merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_amap.c,v 1.32 2001/06/02 18:09:25 chs Exp $	*/
d104 1
a104 1
 *
d160 1
a160 1
	    "amappl", 0, pool_page_alloc_nointr, pool_page_free_nointr,
d286 1
a286 1
	struct vm_map_entry *entry;
d327 1
a327 1
		UVMHIST_LOG(maphist,"<- done (case 1), amap = 0x%x, sltneed=%d",
d340 1
a340 1
				amap_pp_adjref(amap, slotoff + slotmapped,
d343 1
a343 1
			pp_setreflen(amap->am_ppref, amap->am_nslot, 1,
d353 1
a353 1
		UVMHIST_LOG(maphist,"<- done (case 2), amap = 0x%x, slotneed=%d",
d362 1
a362 1
	 * XXXCDC: could we take advantage of a kernel realloc()?
d415 1
a415 1
			amap_pp_adjref(amap, slotoff + slotmapped,
d436 1
a436 1
	UVMHIST_LOG(maphist,"<- done (case 3), amap = 0x%x, slotneed=%d",
d455 1
a455 1
	struct vm_map_entry *entry;
d492 1
a492 1
 * => called from amap_unref when the final reference to an amap is
d514 1
a514 1
		if (anon == NULL || anon->an_ref == 0)
d519 1
a519 1
		UVMHIST_LOG(maphist,"  processing anon 0x%x, ref=%d", anon,
d545 1
a545 1
 *
d558 2
a559 2
	struct vm_map *map;
	struct vm_map_entry *entry;
d598 1
a598 1
		UVMHIST_LOG(maphist, "<- done [creating new amap 0x%x->0x%x]",
d629 1
a629 1
	UVMHIST_LOG(maphist,"  amap=%p, ref=%d, must copy it",
d686 1
a686 1
		amap_pp_adjref(srcamap, entry->aref.ar_pageoff,
d816 1
a816 1

@


1.3.4.6
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_amap.c,v 1.27 2000/11/25 06:27:59 chs Exp $	*/
d104 1
a104 1
 * 
d160 2
a161 1
	    "amappl", &pool_allocator_nointr);
d175 1
a175 1
	int totalslots;
a180 2
	totalslots = malloc_roundup((slots + padslots) * sizeof(int)) /
	    sizeof(int);
d238 1
a238 2
		memset(amap->am_anon, 0,
		    amap->am_maxslot * sizeof(struct vm_anon *));
d258 1
d286 1
a286 1
	vm_map_entry_t entry;
d291 1
a291 1
	int slotmapped, slotadd, slotneed, slotalloc;
d327 1
a327 1
		UVMHIST_LOG(maphist,"<- done (case 1), amap = 0x%x, sltneed=%d", 
d340 1
a340 1
				amap_pp_adjref(amap, slotoff + slotmapped, 
d343 1
a343 1
			pp_setreflen(amap->am_ppref, amap->am_nslot, 1, 
d353 1
a353 1
		UVMHIST_LOG(maphist,"<- done (case 2), amap = 0x%x, slotneed=%d", 
d362 1
a362 1
	 * XXXCDC: could we take advantage of a kernel realloc()?  
a365 1
	slotalloc = malloc_roundup(slotneed * sizeof(int)) / sizeof(int);
d369 1
a369 1
		newppref = malloc(slotalloc *sizeof(int), M_UVMAMAP, M_NOWAIT);
d377 3
a379 3
	newsl = malloc(slotalloc * sizeof(int), M_UVMAMAP, M_WAITOK);
	newbck = malloc(slotalloc * sizeof(int), M_UVMAMAP, M_WAITOK);
	newover = malloc(slotalloc * sizeof(struct vm_anon *),
d388 1
a388 1
	slotadded = slotalloc - amap->am_nslot;
d415 1
a415 1
			amap_pp_adjref(amap, slotoff + slotmapped, 
d417 1
a417 2
		pp_setreflen(newppref, amap->am_nslot, 1,
		    slotneed - amap->am_nslot);
d423 1
a423 1
	amap->am_maxslot = slotalloc;
d436 1
a436 1
	UVMHIST_LOG(maphist,"<- done (case 3), amap = 0x%x, slotneed=%d", 
d455 1
a455 1
	vm_map_entry_t entry;
d461 2
d492 1
a492 1
 * => called from amap_unref when the final reference to an amap is 
d506 2
d514 1
a514 1
		if (anon == NULL || anon->an_ref == 0) 
d519 1
a519 1
		UVMHIST_LOG(maphist,"  processing anon 0x%x, ref=%d", anon, 
d545 1
a545 1
 * 
d558 2
a559 2
	vm_map_t map;
	vm_map_entry_t entry;
d598 1
a598 1
		UVMHIST_LOG(maphist, "<- done [creating new amap 0x%x->0x%x]", 
d629 1
a629 1
	UVMHIST_LOG(maphist,"  amap=%p, ref=%d, must copy it", 
a672 2
	memset(&amap->am_anon[lcv], 0,
	    (amap->am_maxslot - lcv) * sizeof(struct vm_anon *));
d686 1
a686 1
		amap_pp_adjref(srcamap, entry->aref.ar_pageoff, 
d795 1
d807 2
a808 1
					simple_lock(&nanon->an_lock);
d816 1
a816 1
	
d835 1
d938 6
a943 2
 	int stopslot, *ppref, lcv, prevlcv;
 	int ref, len, prevref, prevlen;
a946 1
 	prevlcv = 0;
d949 2
a950 2
 	 * first advance to the correct place in the ppref array,
 	 * fragment if needed.
a959 1
		prevlcv = lcv;
a960 1
	pp_getreflen(ppref, prevlcv, &prevref, &prevlen);
d963 1
a963 2
	 * now adjust reference counts in range.  merge the first
	 * changed entry with the last unchanged entry if possible.
d977 1
a977 1
		ref += adjval;
d980 1
a980 5
		if (lcv == prevlcv + prevlen && ref == prevref) {
			pp_setreflen(ppref, prevlcv, ref, prevlen + len);
		} else {
			pp_setreflen(ppref, lcv, ref, len);
		}
d998 1
a998 1
	int byanon, lcv, stop, curslot, ptr, slotend;
a1013 1
		slotend = slotoff + slots;
d1016 5
a1020 1
	while (lcv < stop) {
d1023 5
a1027 3
  		if (byanon) {
			curslot = lcv++;	/* lcv advances here */
			if (amap->am_anon[curslot] == NULL)
d1029 1
d1032 1
a1032 2
			if (curslot < slotoff || curslot >= slotend) {
				lcv++;		/* lcv advances here */
a1033 2
			}
			stop--;	/* drop stop, since anon will be removed */
@


1.3.4.7
log
@Merge in -current from roughly a week ago
@
text
@d72 1
a72 1
static struct vm_amap *amap_alloc1(int, int, int);
d108 2
a109 2
static __inline void pp_getreflen(int *, int, int *, int *);
static __inline void pp_setreflen(int *, int, int, int);
@


1.3.4.8
log
@Sync the SMP branch with 3.3
@
text
@d957 1
a957 11
	if (lcv != 0)
		pp_getreflen(ppref, prevlcv, &prevref, &prevlen);
	else {
		/* Ensure that the "prevref == ref" test below always
		 * fails, since we're starting from the beginning of
		 * the ppref array; that is, there is no previous
		 * chunk.  
		 */
		prevref = -1;
		prevlen = 0;
	}
@


1.2
log
@add OpenBSD tags
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d798 1
a798 1
				npg = uvm_pagealloc(NULL, 0, nanon);
@


1.1
log
@Import of uvm from NetBSD. Some local changes, some code disabled
@
text
@d1 1
@

