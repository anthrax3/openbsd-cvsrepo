head	1.146;
access;
symbols
	OPENBSD_6_1_BASE:1.146
	OPENBSD_6_0:1.144.0.4
	OPENBSD_6_0_BASE:1.144
	OPENBSD_5_9:1.144.0.2
	OPENBSD_5_9_BASE:1.144
	OPENBSD_5_8:1.140.0.4
	OPENBSD_5_8_BASE:1.140
	OPENBSD_5_7:1.136.0.2
	OPENBSD_5_7_BASE:1.136
	OPENBSD_5_6:1.131.0.4
	OPENBSD_5_6_BASE:1.131
	OPENBSD_5_5:1.129.0.4
	OPENBSD_5_5_BASE:1.129
	OPENBSD_5_4:1.128.0.2
	OPENBSD_5_4_BASE:1.128
	OPENBSD_5_3:1.121.0.2
	OPENBSD_5_3_BASE:1.121
	OPENBSD_5_2:1.114.0.6
	OPENBSD_5_2_BASE:1.114
	OPENBSD_5_1_BASE:1.114
	OPENBSD_5_1:1.114.0.4
	OPENBSD_5_0:1.114.0.2
	OPENBSD_5_0_BASE:1.114
	OPENBSD_4_9:1.102.0.4
	OPENBSD_4_9_BASE:1.102
	OPENBSD_4_8:1.102.0.2
	OPENBSD_4_8_BASE:1.102
	OPENBSD_4_7:1.97.0.2
	OPENBSD_4_7_BASE:1.97
	OPENBSD_4_6:1.91.0.4
	OPENBSD_4_6_BASE:1.91
	OPENBSD_4_5:1.67.0.4
	OPENBSD_4_5_BASE:1.67
	OPENBSD_4_4:1.67.0.2
	OPENBSD_4_4_BASE:1.67
	OPENBSD_4_3:1.64.0.2
	OPENBSD_4_3_BASE:1.64
	OPENBSD_4_2:1.61.0.2
	OPENBSD_4_2_BASE:1.61
	OPENBSD_4_1:1.56.0.4
	OPENBSD_4_1_BASE:1.56
	OPENBSD_4_0:1.56.0.2
	OPENBSD_4_0_BASE:1.56
	OPENBSD_3_9:1.51.0.2
	OPENBSD_3_9_BASE:1.51
	OPENBSD_3_8:1.50.0.4
	OPENBSD_3_8_BASE:1.50
	OPENBSD_3_7:1.50.0.2
	OPENBSD_3_7_BASE:1.50
	OPENBSD_3_6:1.49.0.4
	OPENBSD_3_6_BASE:1.49
	SMP_SYNC_A:1.49
	SMP_SYNC_B:1.49
	OPENBSD_3_5:1.49.0.2
	OPENBSD_3_5_BASE:1.49
	OPENBSD_3_4:1.48.0.2
	OPENBSD_3_4_BASE:1.48
	UBC_SYNC_A:1.47
	OPENBSD_3_3:1.46.0.2
	OPENBSD_3_3_BASE:1.46
	OPENBSD_3_2:1.45.0.2
	OPENBSD_3_2_BASE:1.45
	OPENBSD_3_1:1.42.0.2
	OPENBSD_3_1_BASE:1.42
	UBC_SYNC_B:1.46
	UBC:1.38.0.2
	UBC_BASE:1.38
	OPENBSD_3_0:1.24.0.2
	OPENBSD_3_0_BASE:1.24
	OPENBSD_2_9_BASE:1.16
	OPENBSD_2_9:1.16.0.2
	OPENBSD_2_8:1.9.0.2
	OPENBSD_2_8_BASE:1.9
	OPENBSD_2_7:1.8.0.2
	OPENBSD_2_7_BASE:1.8
	SMP:1.6.0.4
	SMP_BASE:1.6
	kame_19991208:1.6
	OPENBSD_2_6:1.6.0.2
	OPENBSD_2_6_BASE:1.6
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.146
date	2016.11.07.00.26.33;	author guenther;	state Exp;
branches;
next	1.145;
commitid	W7ztnDZwvjCaeQTS;

1.145
date	2016.09.16.02.35.42;	author dlg;	state Exp;
branches;
next	1.144;
commitid	Fei4687v68qad1tP;

1.144
date	2015.10.30.16.47.01;	author miod;	state Exp;
branches;
next	1.143;
commitid	msvqRfn4BKnAwShv;

1.143
date	2015.10.08.15.58.38;	author kettenis;	state Exp;
branches;
next	1.142;
commitid	wmnQpRj0rGJ31TDS;

1.142
date	2015.09.21.12.59.01;	author visa;	state Exp;
branches;
next	1.141;
commitid	FBjwKjapA6vrQw6Y;

1.141
date	2015.08.21.16.04.35;	author visa;	state Exp;
branches;
next	1.140;
commitid	gglpDr80UKmkkP9A;

1.140
date	2015.07.19.22.52.30;	author beck;	state Exp;
branches;
next	1.139;
commitid	o80DXUfb7lEcfPz0;

1.139
date	2015.07.19.21.21.14;	author beck;	state Exp;
branches;
next	1.138;
commitid	5wyRonomeQfnkJsF;

1.138
date	2015.04.23.09.56.23;	author dlg;	state Exp;
branches;
next	1.137;
commitid	lsqyb9bvESnxS0Z9;

1.137
date	2015.03.14.03.38.53;	author jsg;	state Exp;
branches;
next	1.136;
commitid	p4LJxGKbi0BU2cG6;

1.136
date	2015.02.28.06.11.04;	author mlarkin;	state Exp;
branches;
next	1.135;
commitid	08jqalflWCgfVHaD;

1.135
date	2015.02.08.02.17.08;	author deraadt;	state Exp;
branches;
next	1.134;
commitid	EHHLDpKiLok9N5jM;

1.134
date	2015.02.07.01.46.27;	author kettenis;	state Exp;
branches;
next	1.133;
commitid	7b6QyxnUYRNhvNHU;

1.133
date	2015.02.06.10.58.35;	author deraadt;	state Exp;
branches;
next	1.132;
commitid	QOLWofatH4nZnDlg;

1.132
date	2014.11.16.12.31.00;	author deraadt;	state Exp;
branches;
next	1.131;
commitid	yv0ECmCdICvq576h;

1.131
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.130;
commitid	7NtJNW9udCOFtDNM;

1.130
date	2014.04.13.23.14.15;	author tedu;	state Exp;
branches;
next	1.129;

1.129
date	2014.01.23.22.06.30;	author miod;	state Exp;
branches;
next	1.128;

1.128
date	2013.07.09.15.37.43;	author beck;	state Exp;
branches;
next	1.127;

1.127
date	2013.06.21.21.42.17;	author kettenis;	state Exp;
branches;
next	1.126;

1.126
date	2013.06.11.19.01.20;	author beck;	state Exp;
branches;
next	1.125;

1.125
date	2013.05.30.16.29.46;	author tedu;	state Exp;
branches;
next	1.124;

1.124
date	2013.05.30.15.17.59;	author tedu;	state Exp;
branches;
next	1.123;

1.123
date	2013.03.27.02.02.23;	author tedu;	state Exp;
branches;
next	1.122;

1.122
date	2013.03.12.21.10.11;	author deraadt;	state Exp;
branches;
next	1.121;

1.121
date	2013.03.12.21.08.04;	author deraadt;	state Exp;
branches;
next	1.120;

1.120
date	2013.03.12.21.07.02;	author deraadt;	state Exp;
branches;
next	1.119;

1.119
date	2013.03.12.20.47.16;	author beck;	state Exp;
branches;
next	1.118;

1.118
date	2013.03.06.22.26.15;	author beck;	state Exp;
branches;
next	1.117;

1.117
date	2013.03.03.22.37.58;	author miod;	state Exp;
branches;
next	1.116;

1.116
date	2013.03.02.23.07.55;	author miod;	state Exp;
branches;
next	1.115;

1.115
date	2013.02.07.17.38.12;	author beck;	state Exp;
branches;
next	1.114;

1.114
date	2011.07.08.00.10.59;	author tedu;	state Exp;
branches;
next	1.113;

1.113
date	2011.07.07.20.52.50;	author oga;	state Exp;
branches;
next	1.112;

1.112
date	2011.07.06.19.50.38;	author beck;	state Exp;
branches;
next	1.111;

1.111
date	2011.07.03.18.34.14;	author oga;	state Exp;
branches;
next	1.110;

1.110
date	2011.06.23.21.55.58;	author oga;	state Exp;
branches;
next	1.109;

1.109
date	2011.06.23.21.50.26;	author oga;	state Exp;
branches;
next	1.108;

1.108
date	2011.05.30.22.25.24;	author oga;	state Exp;
branches;
next	1.107;

1.107
date	2011.05.10.21.38.04;	author oga;	state Exp;
branches;
next	1.106;

1.106
date	2011.04.15.21.35.16;	author oga;	state Exp;
branches;
next	1.105;

1.105
date	2011.04.03.12.36.08;	author beck;	state Exp;
branches;
next	1.104;

1.104
date	2011.04.02.16.47.17;	author beck;	state Exp;
branches;
next	1.103;

1.103
date	2011.04.02.12.38.37;	author ariane;	state Exp;
branches;
next	1.102;

1.102
date	2010.08.07.03.50.02;	author krw;	state Exp;
branches;
next	1.101;

1.101
date	2010.06.27.03.03.49;	author thib;	state Exp;
branches;
next	1.100;

1.100
date	2010.04.22.19.02.55;	author oga;	state Exp;
branches;
next	1.99;

1.99
date	2010.04.20.22.05.44;	author tedu;	state Exp;
branches;
next	1.98;

1.98
date	2010.03.24.00.36.04;	author oga;	state Exp;
branches;
next	1.97;

1.97
date	2009.10.14.17.53.30;	author beck;	state Exp;
branches;
next	1.96;

1.96
date	2009.08.13.15.29.59;	author deraadt;	state Exp;
branches;
next	1.95;

1.95
date	2009.08.06.15.28.14;	author oga;	state Exp;
branches;
next	1.94;

1.94
date	2009.07.26.21.26.10;	author deraadt;	state Exp;
branches;
next	1.93;

1.93
date	2009.07.23.21.39.10;	author kettenis;	state Exp;
branches;
next	1.92;

1.92
date	2009.07.22.21.05.37;	author oga;	state Exp;
branches;
next	1.91;

1.91
date	2009.06.17.00.13.59;	author oga;	state Exp;
branches;
next	1.90;

1.90
date	2009.06.16.16.42.41;	author ariane;	state Exp;
branches;
next	1.89;

1.89
date	2009.06.16.00.11.29;	author oga;	state Exp;
branches;
next	1.88;

1.88
date	2009.06.14.03.04.08;	author deraadt;	state Exp;
branches;
next	1.87;

1.87
date	2009.06.07.02.01.54;	author oga;	state Exp;
branches;
next	1.86;

1.86
date	2009.06.06.17.46.44;	author art;	state Exp;
branches;
next	1.85;

1.85
date	2009.06.03.04.56.54;	author ariane;	state Exp;
branches;
next	1.84;

1.84
date	2009.06.02.23.00.19;	author oga;	state Exp;
branches;
next	1.83;

1.83
date	2009.06.02.19.49.08;	author ariane;	state Exp;
branches;
next	1.82;

1.82
date	2009.06.01.19.54.02;	author oga;	state Exp;
branches;
next	1.81;

1.81
date	2009.06.01.17.42.33;	author ariane;	state Exp;
branches;
next	1.80;

1.80
date	2009.05.08.15.10.35;	author ariane;	state Exp;
branches;
next	1.79;

1.79
date	2009.05.08.13.50.15;	author ariane;	state Exp;
branches;
next	1.78;

1.78
date	2009.05.04.18.08.06;	author oga;	state Exp;
branches;
next	1.77;

1.77
date	2009.05.01.20.44.22;	author oga;	state Exp;
branches;
next	1.76;

1.76
date	2009.04.28.16.06.07;	author miod;	state Exp;
branches;
next	1.75;

1.75
date	2009.04.14.20.12.05;	author oga;	state Exp;
branches;
next	1.74;

1.74
date	2009.04.13.22.17.54;	author oga;	state Exp;
branches;
next	1.73;

1.73
date	2009.04.06.17.03.51;	author oga;	state Exp;
branches;
next	1.72;

1.72
date	2009.04.06.12.02.52;	author oga;	state Exp;
branches;
next	1.71;

1.71
date	2009.03.26.13.38.45;	author oga;	state Exp;
branches;
next	1.70;

1.70
date	2009.03.25.20.00.18;	author oga;	state Exp;
branches;
next	1.69;

1.69
date	2009.03.24.16.29.42;	author oga;	state Exp;
branches;
next	1.68;

1.68
date	2009.03.23.13.25.11;	author art;	state Exp;
branches;
next	1.67;

1.67
date	2008.07.02.15.21.33;	author art;	state Exp;
branches;
next	1.66;

1.66
date	2008.04.12.20.37.35;	author miod;	state Exp;
branches;
next	1.65;

1.65
date	2008.04.09.16.58.11;	author deraadt;	state Exp;
branches;
next	1.64;

1.64
date	2008.01.04.19.26.52;	author miod;	state Exp;
branches;
next	1.63;

1.63
date	2007.12.18.11.05.52;	author thib;	state Exp;
branches;
next	1.62;

1.62
date	2007.11.29.00.26.41;	author tedu;	state Exp;
branches;
next	1.61;

1.61
date	2007.06.18.21.51.15;	author pedro;	state Exp;
branches;
next	1.60;

1.60
date	2007.05.18.14.41.55;	author art;	state Exp;
branches;
next	1.59;

1.59
date	2007.04.13.18.57.49;	author art;	state Exp;
branches;
next	1.58;

1.58
date	2007.04.11.12.10.42;	author art;	state Exp;
branches;
next	1.57;

1.57
date	2007.04.04.17.44.45;	author art;	state Exp;
branches;
next	1.56;

1.56
date	2006.07.31.11.51.29;	author mickey;	state Exp;
branches;
next	1.55;

1.55
date	2006.07.26.23.15.55;	author mickey;	state Exp;
branches;
next	1.54;

1.54
date	2006.07.13.22.51.26;	author deraadt;	state Exp;
branches;
next	1.53;

1.53
date	2006.06.21.16.20.05;	author mickey;	state Exp;
branches;
next	1.52;

1.52
date	2006.04.27.15.21.19;	author mickey;	state Exp;
branches;
next	1.51;

1.51
date	2006.01.16.13.11.05;	author mickey;	state Exp;
branches;
next	1.50;

1.50
date	2004.12.26.21.22.14;	author miod;	state Exp;
branches;
next	1.49;

1.49
date	2004.02.23.06.19.32;	author drahn;	state Exp;
branches;
next	1.48;

1.48
date	2003.06.01.00.26.09;	author miod;	state Exp;
branches;
next	1.47;

1.47
date	2003.03.29.01.13.57;	author mickey;	state Exp;
branches;
next	1.46;

1.46
date	2002.10.12.01.09.45;	author krw;	state Exp;
branches;
next	1.45;

1.45
date	2002.09.12.12.56.16;	author art;	state Exp;
branches;
next	1.44;

1.44
date	2002.09.10.18.29.44;	author art;	state Exp;
branches;
next	1.43;

1.43
date	2002.06.11.09.45.16;	author art;	state Exp;
branches;
next	1.42;

1.42
date	2002.03.14.01.27.18;	author millert;	state Exp;
branches;
next	1.41;

1.41
date	2002.01.28.11.53.48;	author art;	state Exp;
branches;
next	1.40;

1.40
date	2002.01.02.22.23.25;	author miod;	state Exp;
branches;
next	1.39;

1.39
date	2001.12.19.08.58.07;	author art;	state Exp;
branches;
next	1.38;

1.38
date	2001.12.06.12.43.20;	author art;	state Exp;
branches
	1.38.2.1;
next	1.37;

1.37
date	2001.12.04.23.22.42;	author art;	state Exp;
branches;
next	1.36;

1.36
date	2001.11.30.17.24.19;	author art;	state Exp;
branches;
next	1.35;

1.35
date	2001.11.28.19.28.15;	author art;	state Exp;
branches;
next	1.34;

1.34
date	2001.11.28.14.29.13;	author art;	state Exp;
branches;
next	1.33;

1.33
date	2001.11.28.13.47.40;	author art;	state Exp;
branches;
next	1.32;

1.32
date	2001.11.27.05.27.12;	author art;	state Exp;
branches;
next	1.31;

1.31
date	2001.11.12.01.26.09;	author art;	state Exp;
branches;
next	1.30;

1.30
date	2001.11.10.18.42.31;	author art;	state Exp;
branches;
next	1.29;

1.29
date	2001.11.07.02.55.50;	author art;	state Exp;
branches;
next	1.28;

1.28
date	2001.11.07.01.18.01;	author art;	state Exp;
branches;
next	1.27;

1.27
date	2001.11.06.13.36.52;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2001.11.06.01.35.04;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2001.11.05.22.14.54;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2001.09.19.20.50.59;	author mickey;	state Exp;
branches;
next	1.23;

1.23
date	2001.08.25.12.13.27;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2001.08.11.10.57.22;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2001.08.06.14.03.05;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2001.07.31.14.03.47;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2001.07.25.14.47.59;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2001.07.19.14.31.32;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2001.07.18.10.47.05;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2001.04.10.06.59.12;	author niklas;	state Exp;
branches;
next	1.15;

1.15
date	2001.03.22.18.05.33;	author niklas;	state Exp;
branches;
next	1.14;

1.14
date	2001.03.22.03.05.56;	author smart;	state Exp;
branches;
next	1.13;

1.13
date	2001.03.08.15.21.37;	author smart;	state Exp;
branches;
next	1.12;

1.12
date	2001.03.03.12.28.55;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2001.03.02.09.07.39;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2001.01.29.02.07.47;	author niklas;	state Exp;
branches;
next	1.9;

1.9
date	2000.09.07.20.15.28;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2000.04.25.23.10.30;	author niklas;	state Exp;
branches;
next	1.7;

1.7
date	2000.03.16.22.11.04;	author art;	state Exp;
branches;
next	1.6;

1.6
date	99.09.10.16.59.14;	author mickey;	state Exp;
branches
	1.6.4.1;
next	1.5;

1.5
date	99.09.03.18.02.22;	author art;	state Exp;
branches;
next	1.4;

1.4
date	99.08.23.08.13.24;	author art;	state Exp;
branches;
next	1.3;

1.3
date	99.07.23.14.47.06;	author ho;	state Exp;
branches;
next	1.2;

1.2
date	99.02.26.05.32.07;	author art;	state Exp;
branches;
next	1.1;

1.1
date	99.02.26.01.30.16;	author art;	state Exp;
branches;
next	;

1.6.4.1
date	2000.03.24.09.09.50;	author niklas;	state Exp;
branches;
next	1.6.4.2;

1.6.4.2
date	2001.05.14.22.47.47;	author niklas;	state Exp;
branches;
next	1.6.4.3;

1.6.4.3
date	2001.10.31.03.32.14;	author nate;	state Exp;
branches;
next	1.6.4.4;

1.6.4.4
date	2001.11.13.23.02.31;	author niklas;	state Exp;
branches;
next	1.6.4.5;

1.6.4.5
date	2001.12.05.01.23.58;	author niklas;	state Exp;
branches;
next	1.6.4.6;

1.6.4.6
date	2002.03.06.02.17.14;	author niklas;	state Exp;
branches;
next	1.6.4.7;

1.6.4.7
date	2002.03.28.14.54.26;	author niklas;	state Exp;
branches;
next	1.6.4.8;

1.6.4.8
date	2003.03.28.00.08.48;	author niklas;	state Exp;
branches;
next	1.6.4.9;

1.6.4.9
date	2003.05.13.19.36.58;	author ho;	state Exp;
branches;
next	1.6.4.10;

1.6.4.10
date	2003.06.07.11.09.09;	author ho;	state Exp;
branches;
next	1.6.4.11;

1.6.4.11
date	2004.06.05.23.13.13;	author niklas;	state Exp;
branches;
next	;

1.38.2.1
date	2002.01.31.22.55.51;	author niklas;	state Exp;
branches;
next	1.38.2.2;

1.38.2.2
date	2002.02.02.03.28.27;	author art;	state Exp;
branches;
next	1.38.2.3;

1.38.2.3
date	2002.06.11.03.33.04;	author art;	state Exp;
branches;
next	1.38.2.4;

1.38.2.4
date	2002.10.29.00.36.50;	author art;	state Exp;
branches;
next	1.38.2.5;

1.38.2.5
date	2002.11.04.18.02.33;	author art;	state Exp;
branches;
next	1.38.2.6;

1.38.2.6
date	2003.05.19.22.41.30;	author tedu;	state Exp;
branches;
next	1.38.2.7;

1.38.2.7
date	2004.02.21.00.20.22;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.146
log
@Split PID from TID, giving processes a PID unrelated to the TID of their
initial thread

ok jsing@@ kettenis@@
@
text
@/*	$OpenBSD: uvm_page.c,v 1.145 2016/09/16 02:35:42 dlg Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.44 2000/11/27 08:40:04 chs Exp $	*/

/*
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * Copyright (c) 1991, 1993, The Regents of the University of California.
 *
 * All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * The Mach Operating System project at Carnegie-Mellon University.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)vm_page.c   8.3 (Berkeley) 3/21/94
 * from: Id: uvm_page.c,v 1.1.2.18 1998/02/06 05:24:42 chs Exp
 *
 *
 * Copyright (c) 1987, 1990 Carnegie-Mellon University.
 * All rights reserved.
 *
 * Permission to use, copy, modify and distribute this software and
 * its documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 *
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
 * FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
 *
 * Carnegie Mellon requests users of this software to return to
 *
 *  Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
 *  School of Computer Science
 *  Carnegie Mellon University
 *  Pittsburgh PA 15213-3890
 *
 * any improvements or extensions that they make and grant Carnegie the
 * rights to redistribute these changes.
 */

/*
 * uvm_page.c: page ops.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/sched.h>
#include <sys/vnode.h>
#include <sys/mount.h>
#include <sys/proc.h>

#include <uvm/uvm.h>

/*
 * for object trees
 */
RBT_GENERATE(uvm_objtree, vm_page, objt, uvm_pagecmp);

int
uvm_pagecmp(const struct vm_page *a, const struct vm_page *b)
{
	return (a->offset < b->offset ? -1 : a->offset > b->offset);
}

/*
 * global vars... XXXCDC: move to uvm. structure.
 */
/*
 * physical memory config is stored in vm_physmem.
 */
struct vm_physseg vm_physmem[VM_PHYSSEG_MAX];	/* XXXCDC: uvm.physmem */
int vm_nphysseg = 0;				/* XXXCDC: uvm.nphysseg */

/*
 * Some supported CPUs in a given architecture don't support all
 * of the things necessary to do idle page zero'ing efficiently.
 * We therefore provide a way to disable it from machdep code here.
 */

/*
 * local variables
 */
/*
 * these variables record the values returned by vm_page_bootstrap,
 * for debugging purposes.  The implementation of uvm_pageboot_alloc
 * and pmap_startup here also uses them internally.
 */
static vaddr_t      virtual_space_start;
static vaddr_t      virtual_space_end;

/*
 * local prototypes
 */
static void uvm_pageinsert(struct vm_page *);
static void uvm_pageremove(struct vm_page *);

/*
 * inline functions
 */
/*
 * uvm_pageinsert: insert a page in the object
 *
 * => caller must lock page queues XXX questionable
 * => call should have already set pg's object and offset pointers
 *    and bumped the version counter
 */
__inline static void
uvm_pageinsert(struct vm_page *pg)
{
	struct vm_page	*dupe;

	KASSERT((pg->pg_flags & PG_TABLED) == 0);
	dupe = RBT_INSERT(uvm_objtree, &pg->uobject->memt, pg);
	/* not allowed to insert over another page */
	KASSERT(dupe == NULL);
	atomic_setbits_int(&pg->pg_flags, PG_TABLED);
	pg->uobject->uo_npages++;
}

/*
 * uvm_page_remove: remove page from object
 *
 * => caller must lock page queues
 */
static __inline void
uvm_pageremove(struct vm_page *pg)
{
	KASSERT(pg->pg_flags & PG_TABLED);
	RBT_REMOVE(uvm_objtree, &pg->uobject->memt, pg);

	atomic_clearbits_int(&pg->pg_flags, PG_TABLED);
	pg->uobject->uo_npages--;
	pg->uobject = NULL;
	pg->pg_version++;
}

/*
 * uvm_page_init: init the page system.   called from uvm_init().
 *
 * => we return the range of kernel virtual memory in kvm_startp/kvm_endp
 */
void
uvm_page_init(vaddr_t *kvm_startp, vaddr_t *kvm_endp)
{
	vsize_t freepages, pagecount, n;
	vm_page_t pagearray, curpg;
	int lcv, i;
	paddr_t paddr, pgno;
	struct vm_physseg *seg;

	/*
	 * init the page queues and page queue locks
	 */

	TAILQ_INIT(&uvm.page_active);
	TAILQ_INIT(&uvm.page_inactive_swp);
	TAILQ_INIT(&uvm.page_inactive_obj);
	mtx_init(&uvm.pageqlock, IPL_NONE);
	mtx_init(&uvm.fpageqlock, IPL_VM);
	uvm_pmr_init();

	/*
	 * allocate vm_page structures.
	 */

	/*
	 * sanity check:
	 * before calling this function the MD code is expected to register
	 * some free RAM with the uvm_page_physload() function.   our job
	 * now is to allocate vm_page structures for this memory.
	 */

	if (vm_nphysseg == 0)
		panic("uvm_page_bootstrap: no memory pre-allocated");

	/*
	 * first calculate the number of free pages...
	 *
	 * note that we use start/end rather than avail_start/avail_end.
	 * this allows us to allocate extra vm_page structures in case we
	 * want to return some memory to the pool after booting.
	 */

	freepages = 0;
	for (lcv = 0, seg = vm_physmem; lcv < vm_nphysseg ; lcv++, seg++)
		freepages += (seg->end - seg->start);

	/*
	 * we now know we have (PAGE_SIZE * freepages) bytes of memory we can
	 * use.   for each page of memory we use we need a vm_page structure.
	 * thus, the total number of pages we can use is the total size of
	 * the memory divided by the PAGE_SIZE plus the size of the vm_page
	 * structure.   we add one to freepages as a fudge factor to avoid
	 * truncation errors (since we can only allocate in terms of whole
	 * pages).
	 */

	pagecount = (((paddr_t)freepages + 1) << PAGE_SHIFT) /
	    (PAGE_SIZE + sizeof(struct vm_page));
	pagearray = (vm_page_t)uvm_pageboot_alloc(pagecount *
	    sizeof(struct vm_page));
	memset(pagearray, 0, pagecount * sizeof(struct vm_page));

	/* init the vm_page structures and put them in the correct place. */
	for (lcv = 0, seg = vm_physmem; lcv < vm_nphysseg ; lcv++, seg++) {
		n = seg->end - seg->start;
		if (n > pagecount) {
			panic("uvm_page_init: lost %ld page(s) in init",
			    (long)(n - pagecount));
			    /* XXXCDC: shouldn't happen? */
			/* n = pagecount; */
		}

		/* set up page array pointers */
		seg->pgs = pagearray;
		pagearray += n;
		pagecount -= n;
		seg->lastpg = seg->pgs + (n - 1);

		/* init and free vm_pages (we've already zeroed them) */
		pgno = seg->start;
		paddr = ptoa(pgno);
		for (i = 0, curpg = seg->pgs; i < n;
		    i++, curpg++, pgno++, paddr += PAGE_SIZE) {
			curpg->phys_addr = paddr;
			VM_MDPAGE_INIT(curpg);
			if (pgno >= seg->avail_start &&
			    pgno < seg->avail_end) {
				uvmexp.npages++;
			}
		}

		/* Add pages to free pool. */
		uvm_pmr_freepages(&seg->pgs[seg->avail_start - seg->start],
		    seg->avail_end - seg->avail_start);
	}

	/*
	 * pass up the values of virtual_space_start and
	 * virtual_space_end (obtained by uvm_pageboot_alloc) to the upper
	 * layers of the VM.
	 */

	*kvm_startp = round_page(virtual_space_start);
	*kvm_endp = trunc_page(virtual_space_end);

	/* init locks for kernel threads */
	mtx_init(&uvm.aiodoned_lock, IPL_BIO);

	/*
	 * init reserve thresholds
	 * XXXCDC - values may need adjusting
	 */
	uvmexp.reserve_pagedaemon = 4;
	uvmexp.reserve_kernel = 6;
	uvmexp.anonminpct = 10;
	uvmexp.vnodeminpct = 10;
	uvmexp.vtextminpct = 5;
	uvmexp.anonmin = uvmexp.anonminpct * 256 / 100;
	uvmexp.vnodemin = uvmexp.vnodeminpct * 256 / 100;
	uvmexp.vtextmin = uvmexp.vtextminpct * 256 / 100;

	uvm.page_init_done = TRUE;
}

/*
 * uvm_setpagesize: set the page size
 *
 * => sets page_shift and page_mask from uvmexp.pagesize.
 */
void
uvm_setpagesize(void)
{
	if (uvmexp.pagesize == 0)
		uvmexp.pagesize = DEFAULT_PAGE_SIZE;
	uvmexp.pagemask = uvmexp.pagesize - 1;
	if ((uvmexp.pagemask & uvmexp.pagesize) != 0)
		panic("uvm_setpagesize: page size not a power of two");
	for (uvmexp.pageshift = 0; ; uvmexp.pageshift++)
		if ((1 << uvmexp.pageshift) == uvmexp.pagesize)
			break;
}

/*
 * uvm_pageboot_alloc: steal memory from physmem for bootstrapping
 */
vaddr_t
uvm_pageboot_alloc(vsize_t size)
{
#if defined(PMAP_STEAL_MEMORY)
	vaddr_t addr;

	/*
	 * defer bootstrap allocation to MD code (it may want to allocate
	 * from a direct-mapped segment).  pmap_steal_memory should round
	 * off virtual_space_start/virtual_space_end.
	 */

	addr = pmap_steal_memory(size, &virtual_space_start,
	    &virtual_space_end);

	return(addr);

#else /* !PMAP_STEAL_MEMORY */

	static boolean_t initialized = FALSE;
	vaddr_t addr, vaddr;
	paddr_t paddr;

	/* round to page size */
	size = round_page(size);

	/* on first call to this function, initialize ourselves. */
	if (initialized == FALSE) {
		pmap_virtual_space(&virtual_space_start, &virtual_space_end);

		/* round it the way we like it */
		virtual_space_start = round_page(virtual_space_start);
		virtual_space_end = trunc_page(virtual_space_end);

		initialized = TRUE;
	}

	/* allocate virtual memory for this request */
	if (virtual_space_start == virtual_space_end ||
	    (virtual_space_end - virtual_space_start) < size)
		panic("uvm_pageboot_alloc: out of virtual space");

	addr = virtual_space_start;

#ifdef PMAP_GROWKERNEL
	/*
	 * If the kernel pmap can't map the requested space,
	 * then allocate more resources for it.
	 */
	if (uvm_maxkaddr < (addr + size)) {
		uvm_maxkaddr = pmap_growkernel(addr + size);
		if (uvm_maxkaddr < (addr + size))
			panic("uvm_pageboot_alloc: pmap_growkernel() failed");
	}
#endif

	virtual_space_start += size;

	/* allocate and mapin physical pages to back new virtual pages */
	for (vaddr = round_page(addr) ; vaddr < addr + size ;
	    vaddr += PAGE_SIZE) {
		if (!uvm_page_physget(&paddr))
			panic("uvm_pageboot_alloc: out of memory");

		/*
		 * Note this memory is no longer managed, so using
		 * pmap_kenter is safe.
		 */
		pmap_kenter_pa(vaddr, paddr, PROT_READ | PROT_WRITE);
	}
	pmap_update(pmap_kernel());
	return(addr);
#endif	/* PMAP_STEAL_MEMORY */
}

#if !defined(PMAP_STEAL_MEMORY)
/*
 * uvm_page_physget: "steal" one page from the vm_physmem structure.
 *
 * => attempt to allocate it off the end of a segment in which the "avail"
 *    values match the start/end values.   if we can't do that, then we
 *    will advance both values (making them equal, and removing some
 *    vm_page structures from the non-avail area).
 * => return false if out of memory.
 */

boolean_t
uvm_page_physget(paddr_t *paddrp)
{
	int lcv;
	struct vm_physseg *seg;

	/* pass 1: try allocating from a matching end */
#if (VM_PHYSSEG_STRAT == VM_PSTRAT_BIGFIRST) || \
	(VM_PHYSSEG_STRAT == VM_PSTRAT_BSEARCH)
	for (lcv = vm_nphysseg - 1, seg = vm_physmem + lcv; lcv >= 0;
	    lcv--, seg--)
#else
	for (lcv = 0, seg = vm_physmem; lcv < vm_nphysseg ; lcv++, seg++)
#endif
	{
		if (uvm.page_init_done == TRUE)
			panic("uvm_page_physget: called _after_ bootstrap");

		/* try from front */
		if (seg->avail_start == seg->start &&
		    seg->avail_start < seg->avail_end) {
			*paddrp = ptoa(seg->avail_start);
			seg->avail_start++;
			seg->start++;
			/* nothing left?   nuke it */
			if (seg->avail_start == seg->end) {
				if (vm_nphysseg == 1)
				    panic("uvm_page_physget: out of memory!");
				vm_nphysseg--;
				for (; lcv < vm_nphysseg; lcv++, seg++)
					/* structure copy */
					seg[0] = seg[1];
			}
			return (TRUE);
		}

		/* try from rear */
		if (seg->avail_end == seg->end &&
		    seg->avail_start < seg->avail_end) {
			*paddrp = ptoa(seg->avail_end - 1);
			seg->avail_end--;
			seg->end--;
			/* nothing left?   nuke it */
			if (seg->avail_end == seg->start) {
				if (vm_nphysseg == 1)
				    panic("uvm_page_physget: out of memory!");
				vm_nphysseg--;
				for (; lcv < vm_nphysseg ; lcv++, seg++)
					/* structure copy */
					seg[0] = seg[1];
			}
			return (TRUE);
		}
	}

	/* pass2: forget about matching ends, just allocate something */
#if (VM_PHYSSEG_STRAT == VM_PSTRAT_BIGFIRST) || \
	(VM_PHYSSEG_STRAT == VM_PSTRAT_BSEARCH)
	for (lcv = vm_nphysseg - 1, seg = vm_physmem + lcv; lcv >= 0;
	    lcv--, seg--)
#else
	for (lcv = 0, seg = vm_physmem; lcv < vm_nphysseg ; lcv++, seg++)
#endif
	{

		/* any room in this bank? */
		if (seg->avail_start >= seg->avail_end)
			continue;  /* nope */

		*paddrp = ptoa(seg->avail_start);
		seg->avail_start++;
		/* truncate! */
		seg->start = seg->avail_start;

		/* nothing left?   nuke it */
		if (seg->avail_start == seg->end) {
			if (vm_nphysseg == 1)
				panic("uvm_page_physget: out of memory!");
			vm_nphysseg--;
			for (; lcv < vm_nphysseg ; lcv++, seg++)
				/* structure copy */
				seg[0] = seg[1];
		}
		return (TRUE);
	}

	return (FALSE);        /* whoops! */
}

#endif /* PMAP_STEAL_MEMORY */

/*
 * uvm_page_physload: load physical memory into VM system
 *
 * => all args are PFs
 * => all pages in start/end get vm_page structures
 * => areas marked by avail_start/avail_end get added to the free page pool
 * => we are limited to VM_PHYSSEG_MAX physical memory segments
 */

void
uvm_page_physload(paddr_t start, paddr_t end, paddr_t avail_start,
    paddr_t avail_end, int flags)
{
	int preload, lcv;
	psize_t npages;
	struct vm_page *pgs;
	struct vm_physseg *ps, *seg;

#ifdef DIAGNOSTIC
	if (uvmexp.pagesize == 0)
		panic("uvm_page_physload: page size not set!");

	if (start >= end)
		panic("uvm_page_physload: start >= end");
#endif

	/* do we have room? */
	if (vm_nphysseg == VM_PHYSSEG_MAX) {
		printf("uvm_page_physload: unable to load physical memory "
		    "segment\n");
		printf("\t%d segments allocated, ignoring 0x%llx -> 0x%llx\n",
		    VM_PHYSSEG_MAX, (long long)start, (long long)end);
		printf("\tincrease VM_PHYSSEG_MAX\n");
		return;
	}

	/*
	 * check to see if this is a "preload" (i.e. uvm_mem_init hasn't been
	 * called yet, so malloc is not available).
	 */
	for (lcv = 0, seg = vm_physmem; lcv < vm_nphysseg; lcv++, seg++) {
		if (seg->pgs)
			break;
	}
	preload = (lcv == vm_nphysseg);

	/* if VM is already running, attempt to malloc() vm_page structures */
	if (!preload) {
		/*
		 * XXXCDC: need some sort of lockout for this case
		 * right now it is only used by devices so it should be alright.
		 */
 		paddr_t paddr;

 		npages = end - start;  /* # of pages */

		pgs = (struct vm_page *)uvm_km_zalloc(kernel_map,
		    npages * sizeof(*pgs));
		if (pgs == NULL) {
			printf("uvm_page_physload: can not malloc vm_page "
			    "structs for segment\n");
			printf("\tignoring 0x%lx -> 0x%lx\n", start, end);
			return;
		}
		/* init phys_addr and free pages, XXX uvmexp.npages */
		for (lcv = 0, paddr = ptoa(start); lcv < npages;
		    lcv++, paddr += PAGE_SIZE) {
			pgs[lcv].phys_addr = paddr;
			VM_MDPAGE_INIT(&pgs[lcv]);
			if (atop(paddr) >= avail_start &&
			    atop(paddr) < avail_end) {
				if (flags & PHYSLOAD_DEVICE) {
					atomic_setbits_int(&pgs[lcv].pg_flags,
					    PG_DEV);
					pgs[lcv].wire_count = 1;
				} else {
#if defined(VM_PHYSSEG_NOADD)
		panic("uvm_page_physload: tried to add RAM after vm_mem_init");
#endif
				}
			}
		}

		/* Add pages to free pool. */
		if ((flags & PHYSLOAD_DEVICE) == 0) {
			uvm_pmr_freepages(&pgs[avail_start - start],
			    avail_end - avail_start);
		}

		/* XXXCDC: need hook to tell pmap to rebuild pv_list, etc... */
	} else {
		/* gcc complains if these don't get init'd */
		pgs = NULL;
		npages = 0;

	}

	/* now insert us in the proper place in vm_physmem[] */
#if (VM_PHYSSEG_STRAT == VM_PSTRAT_RANDOM)
	/* random: put it at the end (easy!) */
	ps = &vm_physmem[vm_nphysseg];
#elif (VM_PHYSSEG_STRAT == VM_PSTRAT_BSEARCH)
	{
		int x;
		/* sort by address for binary search */
		for (lcv = 0, seg = vm_physmem; lcv < vm_nphysseg; lcv++, seg++)
			if (start < seg->start)
				break;
		ps = seg;
		/* move back other entries, if necessary ... */
		for (x = vm_nphysseg, seg = vm_physmem + x - 1; x > lcv;
		    x--, seg--)
			/* structure copy */
			seg[1] = seg[0];
	}
#elif (VM_PHYSSEG_STRAT == VM_PSTRAT_BIGFIRST)
	{
		int x;
		/* sort by largest segment first */
		for (lcv = 0, seg = vm_physmem; lcv < vm_nphysseg; lcv++, seg++)
			if ((end - start) >
			    (seg->end - seg->start))
				break;
		ps = &vm_physmem[lcv];
		/* move back other entries, if necessary ... */
		for (x = vm_nphysseg, seg = vm_physmem + x - 1; x > lcv;
		    x--, seg--)
			/* structure copy */
			seg[1] = seg[0];
	}
#else
	panic("uvm_page_physload: unknown physseg strategy selected!");
#endif

	ps->start = start;
	ps->end = end;
	ps->avail_start = avail_start;
	ps->avail_end = avail_end;
	if (preload) {
		ps->pgs = NULL;
	} else {
		ps->pgs = pgs;
		ps->lastpg = pgs + npages - 1;
	}
	vm_nphysseg++;

	return;
}

#ifdef DDB /* XXXCDC: TMP TMP TMP DEBUG DEBUG DEBUG */

void uvm_page_physdump(void); /* SHUT UP GCC */

/* call from DDB */
void
uvm_page_physdump(void)
{
	int lcv;
	struct vm_physseg *seg;

	printf("uvm_page_physdump: physical memory config [segs=%d of %d]:\n",
	    vm_nphysseg, VM_PHYSSEG_MAX);
	for (lcv = 0, seg = vm_physmem; lcv < vm_nphysseg ; lcv++, seg++)
		printf("0x%llx->0x%llx [0x%llx->0x%llx]\n",
		    (long long)seg->start,
		    (long long)seg->end,
		    (long long)seg->avail_start,
		    (long long)seg->avail_end);
	printf("STRATEGY = ");
	switch (VM_PHYSSEG_STRAT) {
	case VM_PSTRAT_RANDOM: printf("RANDOM\n"); break;
	case VM_PSTRAT_BSEARCH: printf("BSEARCH\n"); break;
	case VM_PSTRAT_BIGFIRST: printf("BIGFIRST\n"); break;
	default: printf("<<UNKNOWN>>!!!!\n");
	}
}
#endif

void
uvm_shutdown(void)
{
#ifdef UVM_SWAP_ENCRYPT
	uvm_swap_finicrypt_all();
#endif
}

/*
 * Perform insert of a given page in the specified anon of obj.
 * This is basically, uvm_pagealloc, but with the page already given.
 */
void
uvm_pagealloc_pg(struct vm_page *pg, struct uvm_object *obj, voff_t off,
    struct vm_anon *anon)
{
	int	flags;

	flags = PG_BUSY | PG_FAKE;
	pg->offset = off;
	pg->uobject = obj;
	pg->uanon = anon;

	if (anon) {
		anon->an_page = pg;
		flags |= PQ_ANON;
	} else if (obj)
		uvm_pageinsert(pg);
	atomic_setbits_int(&pg->pg_flags, flags);
#if defined(UVM_PAGE_TRKOWN)
	pg->owner_tag = NULL;
#endif
	UVM_PAGE_OWN(pg, "new alloc");
}

/*
 * uvm_pglistalloc: allocate a list of pages
 *
 * => allocated pages are placed at the tail of rlist.  rlist is
 *    assumed to be properly initialized by caller.
 * => returns 0 on success or errno on failure
 * => doesn't take into account clean non-busy pages on inactive list
 *	that could be used(?)
 * => params:
 *	size		the size of the allocation, rounded to page size.
 *	low		the low address of the allowed allocation range.
 *	high		the high address of the allowed allocation range.
 *	alignment	memory must be aligned to this power-of-two boundary.
 *	boundary	no segment in the allocation may cross this 
 *			power-of-two boundary (relative to zero).
 * => flags:
 *	UVM_PLA_NOWAIT	fail if allocation fails
 *	UVM_PLA_WAITOK	wait for memory to become avail
 *	UVM_PLA_ZERO	return zeroed memory
 */
int
uvm_pglistalloc(psize_t size, paddr_t low, paddr_t high, paddr_t alignment,
    paddr_t boundary, struct pglist *rlist, int nsegs, int flags)
{
	KASSERT((alignment & (alignment - 1)) == 0);
	KASSERT((boundary & (boundary - 1)) == 0);
	KASSERT(!(flags & UVM_PLA_WAITOK) ^ !(flags & UVM_PLA_NOWAIT));

	if (size == 0)
		return (EINVAL);
	size = atop(round_page(size));

	/*
	 * check to see if we need to generate some free pages waking
	 * the pagedaemon.
	 */
	if ((uvmexp.free - BUFPAGES_DEFICIT) < uvmexp.freemin ||
	    ((uvmexp.free - BUFPAGES_DEFICIT) < uvmexp.freetarg &&
	    (uvmexp.inactive + BUFPAGES_INACT) < uvmexp.inactarg))
		wakeup(&uvm.pagedaemon);

	/*
	 * XXX uvm_pglistalloc is currently only used for kernel
	 * objects. Unlike the checks in uvm_pagealloc, below, here
	 * we are always allowed to use the kernel reserve. However, we
	 * have to enforce the pagedaemon reserve here or allocations
	 * via this path could consume everything and we can't
	 * recover in the page daemon.
	 */
 again:
	if ((uvmexp.free <= uvmexp.reserve_pagedaemon + size &&
	    !((curproc == uvm.pagedaemon_proc) ||
		(curproc == syncerproc)))) {
		if (flags & UVM_PLA_WAITOK) {
			uvm_wait("uvm_pglistalloc");
			goto again;
		}
		return (ENOMEM);
	}

	if ((high & PAGE_MASK) != PAGE_MASK) {
		printf("uvm_pglistalloc: Upper boundary 0x%lx "
		    "not on pagemask.\n", (unsigned long)high);
	}

	/*
	 * Our allocations are always page granularity, so our alignment
	 * must be, too.
	 */
	if (alignment < PAGE_SIZE)
		alignment = PAGE_SIZE;

	low = atop(roundup(low, alignment));
	/*
	 * high + 1 may result in overflow, in which case high becomes 0x0,
	 * which is the 'don't care' value.
	 * The only requirement in that case is that low is also 0x0, or the
	 * low<high assert will fail.
	 */
	high = atop(high + 1);
	alignment = atop(alignment);
	if (boundary < PAGE_SIZE && boundary != 0)
		boundary = PAGE_SIZE;
	boundary = atop(boundary);

	return uvm_pmr_getpages(size, low, high, alignment, boundary, nsegs,
	    flags, rlist);
}

/*
 * uvm_pglistfree: free a list of pages
 *
 * => pages should already be unmapped
 */
void
uvm_pglistfree(struct pglist *list)
{
	uvm_pmr_freepageq(list);
}

/*
 * interface used by the buffer cache to allocate a buffer at a time.
 * The pages are allocated wired in DMA accessible memory
 */
int
uvm_pagealloc_multi(struct uvm_object *obj, voff_t off, vsize_t size,
    int flags)
{
	struct pglist    plist;
	struct vm_page  *pg;
	int              i, r;


	TAILQ_INIT(&plist);
	r = uvm_pglistalloc(size, dma_constraint.ucr_low,
	    dma_constraint.ucr_high, 0, 0, &plist, atop(round_page(size)),
	    flags);
	if (r == 0) {
		i = 0;
		while ((pg = TAILQ_FIRST(&plist)) != NULL) {
			pg->wire_count = 1;
			atomic_setbits_int(&pg->pg_flags, PG_CLEAN | PG_FAKE);
			KASSERT((pg->pg_flags & PG_DEV) == 0);
			TAILQ_REMOVE(&plist, pg, pageq);
			uvm_pagealloc_pg(pg, obj, off + ptoa(i++), NULL);
		}
	}
	return r;
}

/*
 * interface used by the buffer cache to reallocate a buffer at a time.
 * The pages are reallocated wired outside the DMA accessible region.
 *
 */
int
uvm_pagerealloc_multi(struct uvm_object *obj, voff_t off, vsize_t size,
    int flags, struct uvm_constraint_range *where)
{
	struct pglist    plist;
	struct vm_page  *pg, *tpg;
	int              i, r;
	voff_t		offset;


	TAILQ_INIT(&plist);
	if (size == 0)
		panic("size 0 uvm_pagerealloc");
	r = uvm_pglistalloc(size, where->ucr_low, where->ucr_high, 0,
	    0, &plist, atop(round_page(size)), flags);
	if (r == 0) {
		i = 0;
		while((pg = TAILQ_FIRST(&plist)) != NULL) {
			offset = off + ptoa(i++);
			tpg = uvm_pagelookup(obj, offset);
			pg->wire_count = 1;
			atomic_setbits_int(&pg->pg_flags, PG_CLEAN | PG_FAKE);
			KASSERT((pg->pg_flags & PG_DEV) == 0);
			TAILQ_REMOVE(&plist, pg, pageq);
			uvm_pagecopy(tpg, pg);
			uvm_pagefree(tpg);
			uvm_pagealloc_pg(pg, obj, offset, NULL);
		}
	}
	return r;
}

/*
 * uvm_pagealloc_strat: allocate vm_page from a particular free list.
 *
 * => return null if no pages free
 * => wake up pagedaemon if number of free pages drops below low water mark
 * => only one of obj or anon can be non-null
 * => caller must activate/deactivate page if it is not wired.
 */

struct vm_page *
uvm_pagealloc(struct uvm_object *obj, voff_t off, struct vm_anon *anon,
    int flags)
{
	struct vm_page *pg;
	struct pglist pgl;
	int pmr_flags;
	boolean_t use_reserve;

	KASSERT(obj == NULL || anon == NULL);
	KASSERT(off == trunc_page(off));

	/*
	 * check to see if we need to generate some free pages waking
	 * the pagedaemon.
	 */
	if ((uvmexp.free - BUFPAGES_DEFICIT) < uvmexp.freemin ||
	    ((uvmexp.free - BUFPAGES_DEFICIT) < uvmexp.freetarg &&
	    (uvmexp.inactive + BUFPAGES_INACT) < uvmexp.inactarg))
		wakeup(&uvm.pagedaemon);

	/*
	 * fail if any of these conditions is true:
	 * [1]  there really are no free pages, or
	 * [2]  only kernel "reserved" pages remain and
	 *        the page isn't being allocated to a kernel object.
	 * [3]  only pagedaemon "reserved" pages remain and
	 *        the requestor isn't the pagedaemon.
	 */
	use_reserve = (flags & UVM_PGA_USERESERVE) ||
		(obj && UVM_OBJ_IS_KERN_OBJECT(obj));
	if ((uvmexp.free <= uvmexp.reserve_kernel && !use_reserve) ||
	    (uvmexp.free <= uvmexp.reserve_pagedaemon &&
	     !((curproc == uvm.pagedaemon_proc) ||
	      (curproc == syncerproc))))
		goto fail;

	pmr_flags = UVM_PLA_NOWAIT;
	if (flags & UVM_PGA_ZERO)
		pmr_flags |= UVM_PLA_ZERO;
	TAILQ_INIT(&pgl);
	if (uvm_pmr_getpages(1, 0, 0, 1, 0, 1, pmr_flags, &pgl) != 0)
		goto fail;

	pg = TAILQ_FIRST(&pgl);
	KASSERT(pg != NULL && TAILQ_NEXT(pg, pageq) == NULL);

	uvm_pagealloc_pg(pg, obj, off, anon);
	KASSERT((pg->pg_flags & PG_DEV) == 0);
	if (flags & UVM_PGA_ZERO)
		atomic_clearbits_int(&pg->pg_flags, PG_CLEAN);
	else
		atomic_setbits_int(&pg->pg_flags, PG_CLEAN);

	return(pg);

fail:
	return (NULL);
}

/*
 * uvm_pagerealloc: reallocate a page from one object to another
 */

void
uvm_pagerealloc(struct vm_page *pg, struct uvm_object *newobj, voff_t newoff)
{

	/* remove it from the old object */
	if (pg->uobject) {
		uvm_pageremove(pg);
	}

	/* put it in the new object */
	if (newobj) {
		pg->uobject = newobj;
		pg->offset = newoff;
		pg->pg_version++;
		uvm_pageinsert(pg);
	}
}


/*
 * uvm_pagefree: free page
 *
 * => erase page's identity (i.e. remove from object)
 * => put page on free list
 * => caller must lock page queues
 * => assumes all valid mappings of pg are gone
 */
void
uvm_pagefree(struct vm_page *pg)
{
	u_int flags_to_clear = 0;

#ifdef DEBUG
	if (pg->uobject == (void *)0xdeadbeef &&
	    pg->uanon == (void *)0xdeadbeef) {
		panic("uvm_pagefree: freeing free page %p", pg);
	}
#endif

	KASSERT((pg->pg_flags & PG_DEV) == 0);

	/*
	 * if the page was an object page (and thus "TABLED"), remove it
	 * from the object.
	 */
	if (pg->pg_flags & PG_TABLED)
		uvm_pageremove(pg);

	/* now remove the page from the queues */
	if (pg->pg_flags & PQ_ACTIVE) {
		TAILQ_REMOVE(&uvm.page_active, pg, pageq);
		flags_to_clear |= PQ_ACTIVE;
		uvmexp.active--;
	}
	if (pg->pg_flags & PQ_INACTIVE) {
		if (pg->pg_flags & PQ_SWAPBACKED)
			TAILQ_REMOVE(&uvm.page_inactive_swp, pg, pageq);
		else
			TAILQ_REMOVE(&uvm.page_inactive_obj, pg, pageq);
		flags_to_clear |= PQ_INACTIVE;
		uvmexp.inactive--;
	}

	/* if the page was wired, unwire it now. */
	if (pg->wire_count) {
		pg->wire_count = 0;
		uvmexp.wired--;
	}
	if (pg->uanon) {
		pg->uanon->an_page = NULL;
		pg->uanon = NULL;
	}

	/* Clean page state bits. */
	flags_to_clear |= PQ_ANON|PQ_AOBJ|PQ_ENCRYPT|PG_ZERO|PG_FAKE|PG_BUSY|
	    PG_RELEASED|PG_CLEAN|PG_CLEANCHK;
	atomic_clearbits_int(&pg->pg_flags, flags_to_clear);

	/* and put on free queue */
#ifdef DEBUG
	pg->uobject = (void *)0xdeadbeef;
	pg->offset = 0xdeadbeef;
	pg->uanon = (void *)0xdeadbeef;
#endif

	uvm_pmr_freepages(pg, 1);
}

/*
 * uvm_page_unbusy: unbusy an array of pages.
 *
 * => pages must either all belong to the same object, or all belong to anons.
 * => if pages are anon-owned, anons must have 0 refcount.
 */
void
uvm_page_unbusy(struct vm_page **pgs, int npgs)
{
	struct vm_page *pg;
	struct uvm_object *uobj;
	int i;

	for (i = 0; i < npgs; i++) {
		pg = pgs[i];

		if (pg == NULL || pg == PGO_DONTCARE) {
			continue;
		}
		if (pg->pg_flags & PG_WANTED) {
			wakeup(pg);
		}
		if (pg->pg_flags & PG_RELEASED) {
			uobj = pg->uobject;
			if (uobj != NULL) {
				uvm_lock_pageq();
				pmap_page_protect(pg, PROT_NONE);
				/* XXX won't happen right now */
				if (pg->pg_flags & PQ_AOBJ)
					uao_dropswap(uobj,
					    pg->offset >> PAGE_SHIFT);
				uvm_pagefree(pg);
				uvm_unlock_pageq();
			} else {
				atomic_clearbits_int(&pg->pg_flags, PG_BUSY);
				UVM_PAGE_OWN(pg, NULL);
				uvm_anfree(pg->uanon);
			}
		} else {
			atomic_clearbits_int(&pg->pg_flags, PG_WANTED|PG_BUSY);
			UVM_PAGE_OWN(pg, NULL);
		}
	}
}

#if defined(UVM_PAGE_TRKOWN)
/*
 * uvm_page_own: set or release page ownership
 *
 * => this is a debugging function that keeps track of who sets PG_BUSY
 *	and where they do it.   it can be used to track down problems
 *	such a thread setting "PG_BUSY" and never releasing it.
 * => if "tag" is NULL then we are releasing page ownership
 */
void
uvm_page_own(struct vm_page *pg, char *tag)
{
	/* gain ownership? */
	if (tag) {
		if (pg->owner_tag) {
			printf("uvm_page_own: page %p already owned "
			    "by thread %d [%s]\n", pg,
			     pg->owner, pg->owner_tag);
			panic("uvm_page_own");
		}
		pg->owner = (curproc) ? curproc->p_tid :  (pid_t) -1;
		pg->owner_tag = tag;
		return;
	}

	/* drop ownership */
	if (pg->owner_tag == NULL) {
		printf("uvm_page_own: dropping ownership of an non-owned "
		    "page (%p)\n", pg);
		panic("uvm_page_own");
	}
	pg->owner_tag = NULL;
	return;
}
#endif

/*
 * when VM_PHYSSEG_MAX is 1, we can simplify these functions
 */

#if VM_PHYSSEG_MAX > 1
/*
 * vm_physseg_find: find vm_physseg structure that belongs to a PA
 */
int
vm_physseg_find(paddr_t pframe, int *offp)
{
	struct vm_physseg *seg;

#if (VM_PHYSSEG_STRAT == VM_PSTRAT_BSEARCH)
	/* binary search for it */
	int	start, len, try;

	/*
	 * if try is too large (thus target is less than than try) we reduce
	 * the length to trunc(len/2) [i.e. everything smaller than "try"]
	 *
	 * if the try is too small (thus target is greater than try) then
	 * we set the new start to be (try + 1).   this means we need to
	 * reduce the length to (round(len/2) - 1).
	 *
	 * note "adjust" below which takes advantage of the fact that
	 *  (round(len/2) - 1) == trunc((len - 1) / 2)
	 * for any value of len we may have
	 */

	for (start = 0, len = vm_nphysseg ; len != 0 ; len = len / 2) {
		try = start + (len / 2);	/* try in the middle */
		seg = vm_physmem + try;

		/* start past our try? */
		if (pframe >= seg->start) {
			/* was try correct? */
			if (pframe < seg->end) {
				if (offp)
					*offp = pframe - seg->start;
				return(try);            /* got it */
			}
			start = try + 1;	/* next time, start here */
			len--;			/* "adjust" */
		} else {
			/*
			 * pframe before try, just reduce length of
			 * region, done in "for" loop
			 */
		}
	}
	return(-1);

#else
	/* linear search for it */
	int	lcv;

	for (lcv = 0, seg = vm_physmem; lcv < vm_nphysseg ; lcv++, seg++) {
		if (pframe >= seg->start && pframe < seg->end) {
			if (offp)
				*offp = pframe - seg->start;
			return(lcv);		   /* got it */
		}
	}
	return(-1);

#endif
}

/*
 * PHYS_TO_VM_PAGE: find vm_page for a PA.   used by MI code to get vm_pages
 * back from an I/O mapping (ugh!).   used in some MD code as well.
 */
struct vm_page *
PHYS_TO_VM_PAGE(paddr_t pa)
{
	paddr_t pf = atop(pa);
	int	off;
	int	psi;

	psi = vm_physseg_find(pf, &off);

	return ((psi == -1) ? NULL : &vm_physmem[psi].pgs[off]);
}
#endif /* VM_PHYSSEG_MAX > 1 */

/*
 * uvm_pagelookup: look up a page
 */
struct vm_page *
uvm_pagelookup(struct uvm_object *obj, voff_t off)
{
	/* XXX if stack is too much, handroll */
	struct vm_page pg;

	pg.offset = off;
	return (RBT_FIND(uvm_objtree, &obj->memt, &pg));
}

/*
 * uvm_pagewire: wire the page, thus removing it from the daemon's grasp
 *
 * => caller must lock page queues
 */
void
uvm_pagewire(struct vm_page *pg)
{
	if (pg->wire_count == 0) {
		if (pg->pg_flags & PQ_ACTIVE) {
			TAILQ_REMOVE(&uvm.page_active, pg, pageq);
			atomic_clearbits_int(&pg->pg_flags, PQ_ACTIVE);
			uvmexp.active--;
		}
		if (pg->pg_flags & PQ_INACTIVE) {
			if (pg->pg_flags & PQ_SWAPBACKED)
				TAILQ_REMOVE(&uvm.page_inactive_swp, pg, pageq);
			else
				TAILQ_REMOVE(&uvm.page_inactive_obj, pg, pageq);
			atomic_clearbits_int(&pg->pg_flags, PQ_INACTIVE);
			uvmexp.inactive--;
		}
		uvmexp.wired++;
	}
	pg->wire_count++;
}

/*
 * uvm_pageunwire: unwire the page.
 *
 * => activate if wire count goes to zero.
 * => caller must lock page queues
 */
void
uvm_pageunwire(struct vm_page *pg)
{
	pg->wire_count--;
	if (pg->wire_count == 0) {
		TAILQ_INSERT_TAIL(&uvm.page_active, pg, pageq);
		uvmexp.active++;
		atomic_setbits_int(&pg->pg_flags, PQ_ACTIVE);
		uvmexp.wired--;
	}
}

/*
 * uvm_pagedeactivate: deactivate page -- no pmaps have access to page
 *
 * => caller must lock page queues
 * => caller must check to make sure page is not wired
 * => object that page belongs to must be locked (so we can adjust pg->flags)
 */
void
uvm_pagedeactivate(struct vm_page *pg)
{
	if (pg->pg_flags & PQ_ACTIVE) {
		TAILQ_REMOVE(&uvm.page_active, pg, pageq);
		atomic_clearbits_int(&pg->pg_flags, PQ_ACTIVE);
		uvmexp.active--;
	}
	if ((pg->pg_flags & PQ_INACTIVE) == 0) {
		KASSERT(pg->wire_count == 0);
		if (pg->pg_flags & PQ_SWAPBACKED)
			TAILQ_INSERT_TAIL(&uvm.page_inactive_swp, pg, pageq);
		else
			TAILQ_INSERT_TAIL(&uvm.page_inactive_obj, pg, pageq);
		atomic_setbits_int(&pg->pg_flags, PQ_INACTIVE);
		uvmexp.inactive++;
		pmap_clear_reference(pg);
		/*
		 * update the "clean" bit.  this isn't 100%
		 * accurate, and doesn't have to be.  we'll
		 * re-sync it after we zap all mappings when
		 * scanning the inactive list.
		 */
		if ((pg->pg_flags & PG_CLEAN) != 0 &&
		    pmap_is_modified(pg))
			atomic_clearbits_int(&pg->pg_flags, PG_CLEAN);
	}
}

/*
 * uvm_pageactivate: activate page
 *
 * => caller must lock page queues
 */
void
uvm_pageactivate(struct vm_page *pg)
{
	if (pg->pg_flags & PQ_INACTIVE) {
		if (pg->pg_flags & PQ_SWAPBACKED)
			TAILQ_REMOVE(&uvm.page_inactive_swp, pg, pageq);
		else
			TAILQ_REMOVE(&uvm.page_inactive_obj, pg, pageq);
		atomic_clearbits_int(&pg->pg_flags, PQ_INACTIVE);
		uvmexp.inactive--;
	}
	if (pg->wire_count == 0) {
		/*
		 * if page is already active, remove it from list so we
		 * can put it at tail.  if it wasn't active, then mark
		 * it active and bump active count
		 */
		if (pg->pg_flags & PQ_ACTIVE)
			TAILQ_REMOVE(&uvm.page_active, pg, pageq);
		else {
			atomic_setbits_int(&pg->pg_flags, PQ_ACTIVE);
			uvmexp.active++;
		}

		TAILQ_INSERT_TAIL(&uvm.page_active, pg, pageq);
	}
}

/*
 * uvm_pagezero: zero fill a page
 */
void
uvm_pagezero(struct vm_page *pg)
{
	atomic_clearbits_int(&pg->pg_flags, PG_CLEAN);
	pmap_zero_page(pg);
}

/*
 * uvm_pagecopy: copy a page
 */
void
uvm_pagecopy(struct vm_page *src, struct vm_page *dst)
{
	atomic_clearbits_int(&dst->pg_flags, PG_CLEAN);
	pmap_copy_page(src, dst);
}

/*
 * uvm_pagecount: count the number of physical pages in the address range.
 */
psize_t
uvm_pagecount(struct uvm_constraint_range* constraint)
{
	int lcv;
	psize_t sz;
	paddr_t low, high;
	paddr_t ps_low, ps_high;

	/* Algorithm uses page numbers. */
	low = atop(constraint->ucr_low);
	high = atop(constraint->ucr_high);

	sz = 0;
	for (lcv = 0; lcv < vm_nphysseg; lcv++) {
		ps_low = MAX(low, vm_physmem[lcv].avail_start);
		ps_high = MIN(high, vm_physmem[lcv].avail_end);
		if (ps_low < ps_high)
			sz += ps_high - ps_low;
	}
	return sz;
}
@


1.145
log
@move the vm_page struct from being stored in RB macro trees to RBT functions

vm_page structs go into three trees, uvm_objtree, uvm_pmr_addr, and
uvm_pmr_size. all these have been moved to RBT code.

this should give us a decent chunk of code space back.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.144 2015/10/30 16:47:01 miod Exp $	*/
d1080 1
a1080 1
 *	such a process setting "PG_BUSY" and never releasing it.
d1090 1
a1090 1
			    "by proc %d [%s]\n", pg,
d1094 1
a1094 1
		pg->owner = (curproc) ? curproc->p_pid :  (pid_t) -1;
@


1.144
log
@Fix two (verified to be harmless) off-by-ones in bounds checks in
uvm_page_init() (causing uvmexp.npages to be sligthly wrong if
pmap_steal_memory() has been used) and uvm_page_physload().

ok guenther@@ kettenis@@ visa@@ beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.143 2015/10/08 15:58:38 kettenis Exp $	*/
d81 1
a81 1
RB_GENERATE(uvm_objtree, vm_page, objt, uvm_pagecmp);
d84 1
a84 1
uvm_pagecmp(struct vm_page *a, struct vm_page *b)
d137 1
a137 1
	dupe = RB_INSERT(uvm_objtree, &pg->uobject->memt, pg);
d153 1
a153 1
	RB_REMOVE(uvm_objtree, &pg->uobject->memt, pg);
d1206 1
a1206 1
	return (RB_FIND(uvm_objtree, &obj->memt, &pg));
@


1.143
log
@Lock the page queues by turning uvm_lock_pageq() and uvm_unlock_pageq() into
mtx_enter() and mtx_leave() operations.  Not 100% this won't blow up but
there is only one way to find out, and we need this to make progress on
further unlocking uvm.

prodded by deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.142 2015/09/21 12:59:01 visa Exp $	*/
d252 1
a252 1
			    pgno <= seg->avail_end) {
d558 1
a558 1
			    atop(paddr) <= avail_end) {
@


1.142
log
@Drop a misleading XXX about PQ_AOBJ. Clear PQ_ANON unconditionally for
consistency with PQ_AOBJ.

Input kettenis@@, ok beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.141 2015/08/21 16:04:35 visa Exp $	*/
d182 1
@


1.141
log
@Remove the unused loan_count field and the related uvm logic. Most of
the page loaning code is already in the Attic.

ok kettenis@@, beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.140 2015/07/19 22:52:30 beck Exp $	*/
a1010 1
		flags_to_clear |= PQ_ANON;
d1014 2
a1015 3
	flags_to_clear |= PQ_AOBJ; /* XXX: find culprit */
	flags_to_clear |= PQ_ENCRYPT|PG_ZERO|PG_FAKE|PG_BUSY|PG_RELEASED|
	    PG_CLEAN|PG_CLEANCHK;
@


1.140
log
@Fix backward test that broke the cache
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.139 2015/07/19 21:21:14 beck Exp $	*/
a969 1
	int saved_loan_count = pg->loan_count;
d985 1
a985 12
	if (pg->pg_flags & PG_TABLED) {
		/*
		 * if the object page is on loan we are going to drop ownership.
		 * it is possible that an anon will take over as owner for this
		 * page later on.   the anon will want a !PG_CLEAN page so that
		 * it knows it needs to allocate swap if it wants to page the
		 * page out.
		 */

		/* in case an anon takes over */
		if (saved_loan_count)
			atomic_clearbits_int(&pg->pg_flags, PG_CLEAN);
a986 25

		/*
		 * if our page was on loan, then we just lost control over it
		 * (in fact, if it was loaned to an anon, the anon may have
		 * already taken over ownership of the page by now and thus
		 * changed the loan_count [e.g. in uvmfault_anonget()]) we just
		 * return (when the last loan is dropped, then the page can be
		 * freed by whatever was holding the last loan).
		 */
		if (saved_loan_count)
			return;
	} else if (saved_loan_count && pg->uanon) {
		/*
		 * if our page is owned by an anon and is loaned out to the
		 * kernel then we just want to drop ownership and return.
		 * the kernel must free the page when all its loans clear ...
		 * note that the kernel can't change the loan status of our
		 * page as long as we are holding PQ lock.
		 */
		atomic_clearbits_int(&pg->pg_flags, PQ_ANON);
		pg->uanon->an_page = NULL;
		pg->uanon = NULL;
		return;
	}
	KASSERT(saved_loan_count == 0);
@


1.139
log
@Change uvm_page[re]alloc_multi to actually use the flags passed in, and return
a value so that they may be called with UVM_PLA_NOWAIT
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.138 2015/04/23 09:56:23 dlg Exp $	*/
d817 1
a817 1
	if (r != 0) {
d850 1
a850 1
	if (r != 0) {
@


1.138
log
@tedu remnants of the previous attempt to implement page zeroing in
the idle thread.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.137 2015/03/14 03:38:53 jsg Exp $	*/
d804 1
a804 1
void
d810 1
a810 1
	int              i;
d814 1
a814 1
	(void) uvm_pglistalloc(size, dma_constraint.ucr_low,
d816 10
a825 8
	    UVM_PLA_WAITOK);
	i = 0;
	while ((pg = TAILQ_FIRST(&plist)) != NULL) {
		pg->wire_count = 1;
		atomic_setbits_int(&pg->pg_flags, PG_CLEAN | PG_FAKE);
		KASSERT((pg->pg_flags & PG_DEV) == 0);
		TAILQ_REMOVE(&plist, pg, pageq);
		uvm_pagealloc_pg(pg, obj, off + ptoa(i++), NULL);
d827 1
d835 1
a835 1
void
d841 1
a841 1
	int              i;
d848 15
a862 13
	(void) uvm_pglistalloc(size, where->ucr_low, where->ucr_high, 0,
	    0, &plist, atop(round_page(size)), UVM_PLA_WAITOK);
	i = 0;
	while((pg = TAILQ_FIRST(&plist)) != NULL) {
		offset = off + ptoa(i++);
		tpg = uvm_pagelookup(obj, offset);
		pg->wire_count = 1;
		atomic_setbits_int(&pg->pg_flags, PG_CLEAN | PG_FAKE);
		KASSERT((pg->pg_flags & PG_DEV) == 0);
		TAILQ_REMOVE(&plist, pg, pageq);
		uvm_pagecopy(tpg, pg);
		uvm_pagefree(tpg);
		uvm_pagealloc_pg(pg, obj, offset, NULL);
d864 1
@


1.137
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.136 2015/02/28 06:11:04 mlarkin Exp $	*/
a102 5
/*
 * XXX disabled until we can find a way to do this without causing
 * problems for either cpu caches or DMA latency.
 */
boolean_t vm_page_zero_enable = FALSE;
a151 1

a285 3
  	/* determine if we should zero pages in the idle loop. */
	uvm.page_idle_zero = vm_page_zero_enable;

a1058 3

	if (uvmexp.zeropages < UVM_PAGEZERO_TARGET)
		uvm.page_idle_zero = vm_page_zero_enable;
@


1.136
log
@
Typo in comment 'reseve' -> 'reserve'
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.135 2015/02/08 02:17:08 deraadt Exp $	*/
a71 1
#include <sys/kernel.h>
@


1.135
log
@Something is subtly wrong with this.  On ramdisks, processes run out of
mappable memory (direct or via execve), perhaps because of the address
allocator behind maps and the way wiring counts work?
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.132 2014/11/16 12:31:00 deraadt Exp $	*/
d754 1
a754 1
	 * we are always allowed to use the kernel reseve. However, we
@


1.134
log
@Tedu the old idle page zeroing code.

ok tedu@@, guenther@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.133 2015/02/06 10:58:35 deraadt Exp $	*/
d1056 1
a1056 1
	KASSERT(!(pg->pg_flags & PQ_AOBJ));
d1101 2
a1102 1
				if (pg->pg_flags & PQ_AOBJ) {
a1104 3
					atomic_clearbits_int(&pg->pg_flags,
					    PQ_AOBJ);
				}
@


1.133
log
@Clear PQ_AOBJ before calling uvm_pagefree(), clearing up one false XXX
comment (one is fixed, one is deleted).
ok kettenis beck
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.132 2014/11/16 12:31:00 deraadt Exp $	*/
a1155 79

/*
 * uvm_pageidlezero: zero free pages while the system is idle.
 *
 * => we do at least one iteration per call, if we are below the target.
 * => we loop until we either reach the target or whichqs indicates that
 *	there is a process ready to run.
 */
void
uvm_pageidlezero(void)
{
#if 0 /* disabled: need new code */
	struct vm_page *pg;
	struct pgfreelist *pgfl;
	int free_list;

	do {
		uvm_lock_fpageq();

		if (uvmexp.zeropages >= UVM_PAGEZERO_TARGET) {
			uvm.page_idle_zero = FALSE;
			uvm_unlock_fpageq();
			return;
		}

		for (free_list = 0; free_list < VM_NFREELIST; free_list++) {
			pgfl = &uvm.page_free[free_list];
			if ((pg = TAILQ_FIRST(&pgfl->pgfl_queues[
			    PGFL_UNKNOWN])) != NULL)
				break;
		}

		if (pg == NULL) {
			/*
			 * No non-zero'd pages; don't bother trying again
			 * until we know we have non-zero'd pages free.
			 */
			uvm.page_idle_zero = FALSE;
			uvm_unlock_fpageq();
			return;
		}

		TAILQ_REMOVE(&pgfl->pgfl_queues[PGFL_UNKNOWN], pg, pageq);
		uvmexp.free--;
		uvm_unlock_fpageq();

#ifdef PMAP_PAGEIDLEZERO
		if (PMAP_PAGEIDLEZERO(pg) == FALSE) {
			/*
			 * The machine-dependent code detected some
			 * reason for us to abort zeroing pages,
			 * probably because there is a process now
			 * ready to run.
			 */
			uvm_lock_fpageq();
			TAILQ_INSERT_HEAD(&pgfl->pgfl_queues[PGFL_UNKNOWN],
			    pg, pageq);
			uvmexp.free++;
			uvmexp.zeroaborts++;
			uvm_unlock_fpageq();
			return;
		}
#else
		/*
		 * XXX This will toast the cache unless the pmap_zero_page()
		 * XXX implementation does uncached access.
		 */
		pmap_zero_page(pg);
#endif
		atomic_setbits_int(&pg->pg_flags, PG_ZERO);

		uvm_lock_fpageq();
		TAILQ_INSERT_HEAD(&pgfl->pgfl_queues[PGFL_ZEROS], pg, pageq);
		uvmexp.free++;
		uvmexp.zeropages++;
		uvm_unlock_fpageq();
	} while (curcpu_is_idle());
#endif /* 0 */
}
@


1.132
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.131 2014/07/11 16:35:40 jsg Exp $	*/
d1056 1
a1056 1
	flags_to_clear |= PQ_AOBJ; /* XXX: find culprit */
d1101 1
a1101 2
				/* XXX won't happen right now */
				if (pg->pg_flags & PQ_AOBJ)
d1104 3
@


1.131
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.130 2014/04/13 23:14:15 tedu Exp $	*/
d388 1
a388 1
		pmap_kenter_pa(vaddr, paddr, VM_PROT_READ|VM_PROT_WRITE);
d1100 1
a1100 1
				pmap_page_protect(pg, VM_PROT_NONE);
@


1.130
log
@compress code by turning four line comments into one line comments.
emphatic ok usual suspects, grudging ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.129 2014/01/23 22:06:30 miod Exp $	*/
d21 1
a21 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by Charles D. Cranor,
 *      Washington University, the University of California, Berkeley and
 *      its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.129
log
@unifdef -D__HAVE_VM_PAGE_MD - no functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.128 2013/07/09 15:37:43 beck Exp $	*/
a97 1

a100 1

a108 1

a117 1

a122 1

a128 1

a134 1

a141 1

a159 1

a177 1

d239 1
a239 4
	/*
	 * init the vm_page structures and put them in the correct place.
	 */

d268 1
a268 3
		/*
		 * Add pages to free pool.
		 */
d282 1
a282 3
	/*
	 * init locks for kernel threads
	 */
d298 1
a298 4
  	/*
	 * determine if we should zero pages in the idle loop.
	 */

a300 4
	/*
	 * done!
	 */

a308 1

a324 1

d351 1
a351 3
	/*
	 * on first call to this function, initialize ourselves.
	 */
d362 1
a362 3
	/*
	 * allocate virtual memory for this request
	 */
d383 1
a383 4
	/*
	 * allocate and mapin physical pages to back new virtual pages
	 */

a385 1

d528 1
a528 3
	/*
	 * do we have room?
	 */
d548 1
a548 3
	/*
	 * if VM is already running, attempt to malloc() vm_page structures
	 */
d585 1
a585 3
		/*
		 * Add pages to free pool.
		 */
a592 1

d599 1
a599 4
	/*
	 * now insert us in the proper place in vm_physmem[]
	 */

a600 1

a602 1

a603 1

a616 1

a617 1

a631 1

a632 1

a633 1

a647 4
	/*
	 * done!
	 */

a913 1

d941 1
a941 1
 fail:
d953 1
a953 4
	/*
	 * remove it from the old object
	 */

d958 1
a958 4
	/*
	 * put it in the new object
	 */

a975 1

a994 1

a995 1

a1016 1

d1034 1
a1034 4
	/*
	 * now remove the page from the queues
	 */

d1049 1
a1049 4
	/*
	 * if the page was wired, unwire it now.
	 */

d1060 1
a1060 3
	/*
	 * Clean page state bits.
	 */
d1066 1
a1066 4
	/*
	 * and put on free queue
	 */

a1084 1

a1435 1

@


1.128
log
@back out the cache flipper temporarily to work out of tree.
will come back soon.
ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.125 2013/05/30 16:29:46 tedu Exp $	*/
a273 1
#ifdef __HAVE_VM_PAGE_MD
a274 1
#endif
a607 1
#ifdef __HAVE_VM_PAGE_MD
a608 1
#endif
@


1.127
log
@Buffer cache pages are wired but not counted as such.  Therefore we have to
set the wire count on the pages to 0 before we call uvm_pagefree() on them,
just like we do in buf_free_pages().  Otherwise the wired pages counter goes
negative.  While there, also sprinkle some KASSERTs in there that
buf_free_pages() has as well.

ok beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.126 2013/06/11 19:01:20 beck Exp $	*/
d879 1
a879 1
int
d885 1
a885 1
	int              i, r;
d889 1
a889 1
	r = uvm_pglistalloc(size, dma_constraint.ucr_low,
d891 1
a891 3
	    flags);
	if (r != 0)
		return(r);
a899 1
	return(0);
d907 1
a907 1
int
d913 1
a913 1
	int              i,r;
d916 1
d920 2
a921 4
	r = uvm_pglistalloc(size, where->ucr_low, where->ucr_high, 0,
	    0, &plist, atop(round_page(size)), flags);
	if (r != 0)
		return(r);
a925 1
		KASSERT(tpg != NULL);
a930 2
		KASSERT(tpg->wire_count == 1);
		tpg->wire_count = 0;
a933 1
	return(0);
@


1.126
log
@High memory page flipping for the buffer cache.

This change splits the buffer cache free lists into lists of dma reachable
buffers and high memory buffers based on the ranges returned by pmemrange.
Buffers move from dma to high memory as they age, but are flipped to dma
reachable memory if IO is needed to/from and high mem buffer. The total
amount of buffers  allocated is now bufcachepercent of both the dma and
the high memory region.

This change allows the use of large buffer caches on amd64 using more than
4 GB of memory

ok tedu@@ krw@@ - testing by many.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.125 2013/05/30 16:29:46 tedu Exp $	*/
a918 1

d930 1
d936 2
@


1.125
log
@remove lots of comments about locking per beck's request
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.124 2013/05/30 15:17:59 tedu Exp $	*/
d879 1
a879 1
void
d885 1
a885 1
	int              i;
d889 1
a889 1
	(void) uvm_pglistalloc(size, dma_constraint.ucr_low,
d891 3
a893 1
	    UVM_PLA_WAITOK);
d902 1
d910 1
a910 1
void
d916 1
a916 1
	int              i;
d923 4
a926 2
	(void) uvm_pglistalloc(size, where->ucr_low, where->ucr_high, 0,
	    0, &plist, atop(round_page(size)), UVM_PLA_WAITOK);
d939 1
@


1.124
log
@remove simple_locks from uvm code. ok beck deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.123 2013/03/27 02:02:23 tedu Exp $	*/
a144 1
 * => caller must lock object
a165 1
 * => caller must lock object
a940 2
 * => if obj != NULL, obj must be locked (to put in tree)
 * => if anon != NULL, anon must be locked (to put in anon)
a1007 2
 *
 * => both objects must be locked
a1039 1
 * => caller must lock owning object (either anon or uvm_object)
d1165 1
a1165 2
 * => if pages are object-owned, object must be locked.
 * => if pages are anon-owned, anons must be unlockd and have 0 refcount.
a1213 1
 * => page's object [if any] must be locked
a1409 3
 *
 * => caller should lock object to keep someone from pulling the page
 *	out from under it
a1537 3
 *
 * => if page is part of an object then the object should be locked
 *	to protect pg->flags.
a1547 3
 *
 * => if page is part of an object then the object should be locked
 *	to protect pg->flags.
@


1.123
log
@combine several atomic_clearbits calls into one. slightly faster on
machines where atomic ops aren't so simple.
ok beck deraadt miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.122 2013/03/12 21:10:11 deraadt Exp $	*/
a205 1
	simple_lock_init(&uvm.pageqlock);
@


1.122
log
@preserving main-branch topology for a perverse reason:
step 3 - re-merge 1.116 to 1.118
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.118 2013/03/06 22:26:15 beck Exp $	*/
d1056 1
d1119 1
a1119 1
		atomic_clearbits_int(&pg->pg_flags, PQ_ACTIVE);
d1127 1
a1127 1
		atomic_clearbits_int(&pg->pg_flags, PQ_INACTIVE);
d1142 1
a1142 1
		atomic_clearbits_int(&pg->pg_flags, PQ_ANON);
d1148 4
a1151 3
	atomic_clearbits_int(&pg->pg_flags, PQ_AOBJ); /* XXX: find culprit */
	atomic_clearbits_int(&pg->pg_flags, PQ_ENCRYPT|
	    PG_ZERO|PG_FAKE|PG_BUSY|PG_RELEASED|PG_CLEAN|PG_CLEANCHK);
@


1.121
log
@preserving main-branch topology for a perverse reason:
step 2 - re-merge 1.119 (the WAITOK diff)
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.119 2013/03/12 20:47:16 beck Exp $	*/
d194 1
a194 1
	vm_page_t pagearray;
d196 2
a197 1
	paddr_t paddr;
d233 2
a234 2
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++)
		freepages += (vm_physmem[lcv].end - vm_physmem[lcv].start);
d256 2
a257 2
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++) {
		n = vm_physmem[lcv].end - vm_physmem[lcv].start;
d266 1
a266 1
		vm_physmem[lcv].pgs = pagearray;
d269 1
a269 1
		vm_physmem[lcv].lastpg = vm_physmem[lcv].pgs + (n - 1);
d272 5
a276 3
		paddr = ptoa(vm_physmem[lcv].start);
		for (i = 0 ; i < n ; i++, paddr += PAGE_SIZE) {
			vm_physmem[lcv].pgs[i].phys_addr = paddr;
d278 1
a278 1
			VM_MDPAGE_INIT(&vm_physmem[lcv].pgs[i]);
d280 2
a281 2
			if (atop(paddr) >= vm_physmem[lcv].avail_start &&
			    atop(paddr) <= vm_physmem[lcv].avail_end) {
d289 2
a290 3
		uvm_pmr_freepages(&vm_physmem[lcv].pgs[
		    vm_physmem[lcv].avail_start - vm_physmem[lcv].start],
		    vm_physmem[lcv].avail_end - vm_physmem[lcv].avail_start);
d453 2
a454 1
	int lcv, x;
d459 2
a460 1
	for (lcv = vm_nphysseg - 1 ; lcv >= 0 ; lcv--)
d462 1
a462 1
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++)
a464 1

d469 5
a473 5
		if (vm_physmem[lcv].avail_start == vm_physmem[lcv].start &&
		    vm_physmem[lcv].avail_start < vm_physmem[lcv].avail_end) {
			*paddrp = ptoa(vm_physmem[lcv].avail_start);
			vm_physmem[lcv].avail_start++;
			vm_physmem[lcv].start++;
d475 1
a475 2
			if (vm_physmem[lcv].avail_start ==
			    vm_physmem[lcv].end) {
d479 1
a479 1
				for (x = lcv ; x < vm_nphysseg ; x++)
d481 1
a481 1
					vm_physmem[x] = vm_physmem[x+1];
d487 5
a491 5
		if (vm_physmem[lcv].avail_end == vm_physmem[lcv].end &&
		    vm_physmem[lcv].avail_start < vm_physmem[lcv].avail_end) {
			*paddrp = ptoa(vm_physmem[lcv].avail_end - 1);
			vm_physmem[lcv].avail_end--;
			vm_physmem[lcv].end--;
d493 1
a493 2
			if (vm_physmem[lcv].avail_end ==
			    vm_physmem[lcv].start) {
d497 1
a497 1
				for (x = lcv ; x < vm_nphysseg ; x++)
d499 1
a499 1
					vm_physmem[x] = vm_physmem[x+1];
d508 2
a509 1
	for (lcv = vm_nphysseg - 1 ; lcv >= 0 ; lcv--)
d511 1
a511 1
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++)
d516 1
a516 1
		if (vm_physmem[lcv].avail_start >= vm_physmem[lcv].avail_end)
d519 2
a520 2
		*paddrp = ptoa(vm_physmem[lcv].avail_start);
		vm_physmem[lcv].avail_start++;
d522 1
a522 1
		vm_physmem[lcv].start = vm_physmem[lcv].avail_start;
d525 1
a525 1
		if (vm_physmem[lcv].avail_start == vm_physmem[lcv].end) {
d529 1
a529 1
			for (x = lcv ; x < vm_nphysseg ; x++)
d531 1
a531 1
				vm_physmem[x] = vm_physmem[x+1];
d557 1
a557 1
	struct vm_physseg *ps;
d559 1
d565 1
d583 2
a584 2
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++) {
		if (vm_physmem[lcv].pgs)
d661 2
a662 2
		for (lcv = 0 ; lcv < vm_nphysseg ; lcv++)
			if (start < vm_physmem[lcv].start)
d664 1
a664 1
		ps = &vm_physmem[lcv];
d666 2
a667 1
		for (x = vm_nphysseg ; x > lcv ; x--)
d669 1
a669 1
			vm_physmem[x] = vm_physmem[x - 1];
d677 1
a677 1
		for (lcv = 0 ; lcv < vm_nphysseg ; lcv++)
d679 1
a679 1
			    (vm_physmem[lcv].end - vm_physmem[lcv].start))
d683 2
a684 1
		for (x = vm_nphysseg ; x > lcv ; x--)
d686 1
a686 1
			vm_physmem[x] = vm_physmem[x - 1];
d723 1
d727 1
a727 1
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++)
d729 4
a732 4
		    (long long)vm_physmem[lcv].start,
		    (long long)vm_physmem[lcv].end,
		    (long long)vm_physmem[lcv].avail_start,
		    (long long)vm_physmem[lcv].avail_end);
d808 2
d828 1
a828 1
	if ((uvmexp.free <= uvmexp.reserve_pagedaemon &&
a857 1
	size = atop(round_page(size));
a999 1
	atomic_setbits_int(&pg->pg_flags, PG_BUSY|PG_CLEAN|PG_FAKE);
d1002 2
d1341 1
d1362 1
d1365 1
a1365 1
		if (pframe >= vm_physmem[try].start) {
d1367 1
a1367 1
			if (pframe < vm_physmem[try].end) {
d1369 1
a1369 1
					*offp = pframe - vm_physmem[try].start;
d1387 2
a1388 3
	for (lcv = 0; lcv < vm_nphysseg; lcv++) {
		if (pframe >= vm_physmem[lcv].start &&
		    pframe < vm_physmem[lcv].end) {
d1390 1
a1390 1
				*offp = pframe - vm_physmem[lcv].start;
@


1.120
log
@preserving main-branch topology for a perverse reason:
step 1 - backout 1.116 to 1.119
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.115 2013/02/07 17:38:12 beck Exp $	*/
d822 1
a822 1
		if (UVM_PLA_WAITOK) {
@


1.119
log
@Fix horrible typo of mine checking for WAITOK flags, found by sthen.
This fix actually by mikeb@@, this needs thorough testing to verify
it doesn't bring up other issues in what it hid.
ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.118 2013/03/06 22:26:15 beck Exp $	*/
d194 1
a194 1
	vm_page_t pagearray, curpg;
d196 1
a196 2
	paddr_t paddr, pgno;
	struct vm_physseg *seg;
d232 2
a233 2
	for (lcv = 0, seg = vm_physmem; lcv < vm_nphysseg ; lcv++, seg++)
		freepages += (seg->end - seg->start);
d255 2
a256 2
	for (lcv = 0, seg = vm_physmem; lcv < vm_nphysseg ; lcv++, seg++) {
		n = seg->end - seg->start;
d265 1
a265 1
		seg->pgs = pagearray;
d268 1
a268 1
		seg->lastpg = seg->pgs + (n - 1);
d271 3
a273 5
		pgno = seg->start;
		paddr = ptoa(pgno);
		for (i = 0, curpg = seg->pgs; i < n;
		    i++, curpg++, pgno++, paddr += PAGE_SIZE) {
			curpg->phys_addr = paddr;
d275 1
a275 1
			VM_MDPAGE_INIT(curpg);
d277 2
a278 2
			if (pgno >= seg->avail_start &&
			    pgno <= seg->avail_end) {
d286 3
a288 2
		uvm_pmr_freepages(&seg->pgs[seg->avail_start - seg->start],
		    seg->avail_end - seg->avail_start);
d451 1
a451 2
	int lcv;
	struct vm_physseg *seg;
d456 1
a456 2
	for (lcv = vm_nphysseg - 1, seg = vm_physmem + lcv; lcv >= 0;
	    lcv--, seg--)
d458 1
a458 1
	for (lcv = 0, seg = vm_physmem; lcv < vm_nphysseg ; lcv++, seg++)
d461 1
d466 5
a470 5
		if (seg->avail_start == seg->start &&
		    seg->avail_start < seg->avail_end) {
			*paddrp = ptoa(seg->avail_start);
			seg->avail_start++;
			seg->start++;
d472 2
a473 1
			if (seg->avail_start == seg->end) {
d477 1
a477 1
				for (; lcv < vm_nphysseg; lcv++, seg++)
d479 1
a479 1
					seg[0] = seg[1];
d485 5
a489 5
		if (seg->avail_end == seg->end &&
		    seg->avail_start < seg->avail_end) {
			*paddrp = ptoa(seg->avail_end - 1);
			seg->avail_end--;
			seg->end--;
d491 2
a492 1
			if (seg->avail_end == seg->start) {
d496 1
a496 1
				for (; lcv < vm_nphysseg ; lcv++, seg++)
d498 1
a498 1
					seg[0] = seg[1];
d507 1
a507 2
	for (lcv = vm_nphysseg - 1, seg = vm_physmem + lcv; lcv >= 0;
	    lcv--, seg--)
d509 1
a509 1
	for (lcv = 0, seg = vm_physmem; lcv < vm_nphysseg ; lcv++, seg++)
d514 1
a514 1
		if (seg->avail_start >= seg->avail_end)
d517 2
a518 2
		*paddrp = ptoa(seg->avail_start);
		seg->avail_start++;
d520 1
a520 1
		seg->start = seg->avail_start;
d523 1
a523 1
		if (seg->avail_start == seg->end) {
d527 1
a527 1
			for (; lcv < vm_nphysseg ; lcv++, seg++)
d529 1
a529 1
				seg[0] = seg[1];
d555 1
a555 1
	struct vm_physseg *ps, *seg;
a556 1
#ifdef DIAGNOSTIC
a561 1
#endif
d579 2
a580 2
	for (lcv = 0, seg = vm_physmem; lcv < vm_nphysseg; lcv++, seg++) {
		if (seg->pgs)
d657 2
a658 2
		for (lcv = 0, seg = vm_physmem; lcv < vm_nphysseg; lcv++, seg++)
			if (start < seg->start)
d660 1
a660 1
		ps = seg;
d662 1
a662 2
		for (x = vm_nphysseg, seg = vm_physmem + x - 1; x > lcv;
		    x--, seg--)
d664 1
a664 1
			seg[1] = seg[0];
d672 1
a672 1
		for (lcv = 0, seg = vm_physmem; lcv < vm_nphysseg; lcv++, seg++)
d674 1
a674 1
			    (seg->end - seg->start))
d678 1
a678 2
		for (x = vm_nphysseg, seg = vm_physmem + x - 1; x > lcv;
		    x--, seg--)
d680 1
a680 1
			seg[1] = seg[0];
a716 1
	struct vm_physseg *seg;
d720 1
a720 1
	for (lcv = 0, seg = vm_physmem; lcv < vm_nphysseg ; lcv++, seg++)
d722 4
a725 4
		    (long long)seg->start,
		    (long long)seg->end,
		    (long long)seg->avail_start,
		    (long long)seg->avail_end);
a800 2
	size = atop(round_page(size));

d819 1
a819 1
	if ((uvmexp.free <= uvmexp.reserve_pagedaemon + size &&
d822 1
a822 1
		if (flags & UVM_PLA_WAITOK) {
d849 1
d992 1
a994 2
	else
		atomic_setbits_int(&pg->pg_flags, PG_CLEAN);
a1331 1
	struct vm_physseg *seg;
a1351 1
		seg = vm_physmem + try;
d1354 1
a1354 1
		if (pframe >= seg->start) {
d1356 1
a1356 1
			if (pframe < seg->end) {
d1358 1
a1358 1
					*offp = pframe - seg->start;
d1376 3
a1378 2
	for (lcv = 0, seg = vm_physmem; lcv < vm_nphysseg ; lcv++, seg++) {
		if (pframe >= seg->start && pframe < seg->end) {
d1380 1
a1380 1
				*offp = pframe - seg->start;
@


1.118
log
@Account for the size of the allocation when defending the pagedaemon reserve.
Spotted by oga@@nicotinebsd.org, with help from dhill@@. Fix by me.
ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.117 2013/03/03 22:37:58 miod Exp $	*/
d831 1
a831 1
		if (UVM_PLA_WAITOK) {
@


1.117
log
@Use local vm_physseg pointers instead of compting vm_physmem[index] gazillions
of times. No function change but makes the code a bit smaller.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.116 2013/03/02 23:07:55 miod Exp $	*/
d808 2
d828 1
a828 1
	if ((uvmexp.free <= uvmexp.reserve_pagedaemon &&
a857 1
	size = atop(round_page(size));
@


1.116
log
@Simplify uvm_pagealloc() to only need one atomic operation on the page flags
instead of two, building upon the knowledge of the state uvm_pagealloc_pg()
leaves the uvm_page in.
ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.115 2013/02/07 17:38:12 beck Exp $	*/
d194 1
a194 1
	vm_page_t pagearray;
d196 2
a197 1
	paddr_t paddr;
d233 2
a234 2
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++)
		freepages += (vm_physmem[lcv].end - vm_physmem[lcv].start);
d256 2
a257 2
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++) {
		n = vm_physmem[lcv].end - vm_physmem[lcv].start;
d266 1
a266 1
		vm_physmem[lcv].pgs = pagearray;
d269 1
a269 1
		vm_physmem[lcv].lastpg = vm_physmem[lcv].pgs + (n - 1);
d272 5
a276 3
		paddr = ptoa(vm_physmem[lcv].start);
		for (i = 0 ; i < n ; i++, paddr += PAGE_SIZE) {
			vm_physmem[lcv].pgs[i].phys_addr = paddr;
d278 1
a278 1
			VM_MDPAGE_INIT(&vm_physmem[lcv].pgs[i]);
d280 2
a281 2
			if (atop(paddr) >= vm_physmem[lcv].avail_start &&
			    atop(paddr) <= vm_physmem[lcv].avail_end) {
d289 2
a290 3
		uvm_pmr_freepages(&vm_physmem[lcv].pgs[
		    vm_physmem[lcv].avail_start - vm_physmem[lcv].start],
		    vm_physmem[lcv].avail_end - vm_physmem[lcv].avail_start);
d453 2
a454 1
	int lcv, x;
d459 2
a460 1
	for (lcv = vm_nphysseg - 1 ; lcv >= 0 ; lcv--)
d462 1
a462 1
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++)
a464 1

d469 5
a473 5
		if (vm_physmem[lcv].avail_start == vm_physmem[lcv].start &&
		    vm_physmem[lcv].avail_start < vm_physmem[lcv].avail_end) {
			*paddrp = ptoa(vm_physmem[lcv].avail_start);
			vm_physmem[lcv].avail_start++;
			vm_physmem[lcv].start++;
d475 1
a475 2
			if (vm_physmem[lcv].avail_start ==
			    vm_physmem[lcv].end) {
d479 1
a479 1
				for (x = lcv ; x < vm_nphysseg ; x++)
d481 1
a481 1
					vm_physmem[x] = vm_physmem[x+1];
d487 5
a491 5
		if (vm_physmem[lcv].avail_end == vm_physmem[lcv].end &&
		    vm_physmem[lcv].avail_start < vm_physmem[lcv].avail_end) {
			*paddrp = ptoa(vm_physmem[lcv].avail_end - 1);
			vm_physmem[lcv].avail_end--;
			vm_physmem[lcv].end--;
d493 1
a493 2
			if (vm_physmem[lcv].avail_end ==
			    vm_physmem[lcv].start) {
d497 1
a497 1
				for (x = lcv ; x < vm_nphysseg ; x++)
d499 1
a499 1
					vm_physmem[x] = vm_physmem[x+1];
d508 2
a509 1
	for (lcv = vm_nphysseg - 1 ; lcv >= 0 ; lcv--)
d511 1
a511 1
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++)
d516 1
a516 1
		if (vm_physmem[lcv].avail_start >= vm_physmem[lcv].avail_end)
d519 2
a520 2
		*paddrp = ptoa(vm_physmem[lcv].avail_start);
		vm_physmem[lcv].avail_start++;
d522 1
a522 1
		vm_physmem[lcv].start = vm_physmem[lcv].avail_start;
d525 1
a525 1
		if (vm_physmem[lcv].avail_start == vm_physmem[lcv].end) {
d529 1
a529 1
			for (x = lcv ; x < vm_nphysseg ; x++)
d531 1
a531 1
				vm_physmem[x] = vm_physmem[x+1];
d557 1
a557 1
	struct vm_physseg *ps;
d559 1
d565 1
d583 2
a584 2
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++) {
		if (vm_physmem[lcv].pgs)
d661 2
a662 2
		for (lcv = 0 ; lcv < vm_nphysseg ; lcv++)
			if (start < vm_physmem[lcv].start)
d664 1
a664 1
		ps = &vm_physmem[lcv];
d666 2
a667 1
		for (x = vm_nphysseg ; x > lcv ; x--)
d669 1
a669 1
			vm_physmem[x] = vm_physmem[x - 1];
d677 1
a677 1
		for (lcv = 0 ; lcv < vm_nphysseg ; lcv++)
d679 1
a679 1
			    (vm_physmem[lcv].end - vm_physmem[lcv].start))
d683 2
a684 1
		for (x = vm_nphysseg ; x > lcv ; x--)
d686 1
a686 1
			vm_physmem[x] = vm_physmem[x - 1];
d723 1
d727 1
a727 1
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++)
d729 4
a732 4
		    (long long)vm_physmem[lcv].start,
		    (long long)vm_physmem[lcv].end,
		    (long long)vm_physmem[lcv].avail_start,
		    (long long)vm_physmem[lcv].avail_end);
d1340 1
d1361 1
d1364 1
a1364 1
		if (pframe >= vm_physmem[try].start) {
d1366 1
a1366 1
			if (pframe < vm_physmem[try].end) {
d1368 1
a1368 1
					*offp = pframe - vm_physmem[try].start;
d1386 2
a1387 3
	for (lcv = 0; lcv < vm_nphysseg; lcv++) {
		if (pframe >= vm_physmem[lcv].start &&
		    pframe < vm_physmem[lcv].end) {
d1389 1
a1389 1
				*offp = pframe - vm_physmem[lcv].start;
@


1.115
log
@Bring back reserve enforcement and page daemon wakeup into uvm_pglistalloc,
It was removed as this function was redone to use pmemrange in mid 2010
with the result that kernel malloc and other users of this function can
consume the page daemon reserve and run us out of memory.
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.114 2011/07/08 00:10:59 tedu Exp $	*/
a991 1
	atomic_setbits_int(&pg->pg_flags, PG_BUSY|PG_CLEAN|PG_FAKE);
d994 2
@


1.114
log
@some machines don't boot with the previous uvm reserve enforcement diff.
back it out.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.113 2011/07/07 20:52:50 oga Exp $	*/
a794 1

d801 27
@


1.113
log
@Move the uvm reserve enforcement from uvm_pagealloc to pmemrange.

More and more things are allocating outside of uvm_pagealloc these days making
it easy for something like the buffer cache to eat your last page with no
repercussions (other than a hung machine, of course).

ok ariane@@ also ok ariane@@ again after I spotted and fixed a possible underflow
problem in the calculation.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.112 2011/07/06 19:50:38 beck Exp $	*/
d79 1
d923 1
d928 26
a953 1
	/* XXX these functions should share flags */
a956 2
	if (flags & UVM_PGA_USERESERVE)
		pmr_flags |= UVM_PLA_USERESERVE;
d959 1
a959 1
		return (NULL);
d970 4
a973 1
	return (pg);
@


1.112
log
@
uvm changes for buffer cache improvements.
1) Make the pagedaemon aware of the memory ranges and size of allocations
where memory is being requested, and pass this information on to
bufbackoff(), which will later (not yet) be used to ensure that the
buffer cache gets out of the way in the right area of memory.

Note that this commit does not yet make it *do* that - as currently
the buffer cache is all in dma-able memory and it will simply back
off.

2) Add uvm_pagerealloc_multi - to be used by the buffer cache code
for reallocating pages to particular regions.

much of this work by ariane, with smatterings of me, art,and oga

ok oga@@, thib@@, ariane@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.111 2011/07/03 18:34:14 oga Exp $	*/
a78 1
#include <sys/mount.h>
a921 1
	boolean_t use_reserve;
d926 1
a926 26
	/*
	 * check to see if we need to generate some free pages waking
	 * the pagedaemon.
	 */
	if ((uvmexp.free - BUFPAGES_DEFICIT) < uvmexp.freemin ||
	    ((uvmexp.free - BUFPAGES_DEFICIT) < uvmexp.freetarg &&
	    (uvmexp.inactive + BUFPAGES_INACT) < uvmexp.inactarg))
		wakeup(&uvm.pagedaemon);

	/*
	 * fail if any of these conditions is true:
	 * [1]  there really are no free pages, or
	 * [2]  only kernel "reserved" pages remain and
	 *        the page isn't being allocated to a kernel object.
	 * [3]  only pagedaemon "reserved" pages remain and
	 *        the requestor isn't the pagedaemon.
	 */

	use_reserve = (flags & UVM_PGA_USERESERVE) ||
		(obj && UVM_OBJ_IS_KERN_OBJECT(obj));
	if ((uvmexp.free <= uvmexp.reserve_kernel && !use_reserve) ||
	    (uvmexp.free <= uvmexp.reserve_pagedaemon &&
	     !((curproc == uvm.pagedaemon_proc) ||
	      (curproc == syncerproc))))
		goto fail;

d930 2
d934 1
a934 1
		goto fail;
d945 1
a945 4
	return(pg);

 fail:
	return (NULL);
@


1.111
log
@Rip out and burn support for UVM_HIST.

The vm hackers don't use it, don't maintain it and have to look at it all the
time. About time this 800 lines of code hit /dev/null.

``never liked it'' tedu@@. ariane@@ was very happy when i told her i wrote
this diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.110 2011/06/23 21:55:58 oga Exp $	*/
d849 2
a850 1
uvm_pagealloc_multi(struct uvm_object *obj, voff_t off, vsize_t size, int flags)
d868 34
@


1.110
log
@Check for the correct flag when checking to see if the page is part of an aobj.

This is no function change since aobjs never actually hit this path. (also it is
my bug from a while ago).

ok ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.109 2011/06/23 21:50:26 oga Exp $	*/
a131 5
 * History
 */
UVMHIST_DECL(pghist);

/*
a154 1
	UVMHIST_FUNC("uvm_pageinsert"); UVMHIST_CALLED(pghist);
a173 1
	UVMHIST_FUNC("uvm_pageremove"); UVMHIST_CALLED(pghist);
a196 7
#if defined(UVMHIST)
	static struct uvm_history_ent pghistbuf[100];
#endif

	UVMHIST_FUNC("uvm_page_init");
	UVMHIST_INIT_STATIC(pghist, pghistbuf);
	UVMHIST_CALLED(pghist);
a451 1
	UVMHIST_FUNC("uvm_page_physget"); UVMHIST_CALLED(pghist);
a794 1
	UVMHIST_FUNC("uvm_pglistalloc"); UVMHIST_CALLED(pghist);
a840 1
	UVMHIST_FUNC("uvm_pglistfree"); UVMHIST_CALLED(pghist);
a888 1
	UVMHIST_FUNC("uvm_pagealloc"); UVMHIST_CALLED(pghist);
a934 2
	UVMHIST_LOG(pghist, "allocated pg %p/%lx", pg,
	    (u_long)VM_PAGE_TO_PHYS(pg), 0, 0);
a937 1
	UVMHIST_LOG(pghist, "failed!", 0, 0, 0, 0);
a950 2
	UVMHIST_FUNC("uvm_pagerealloc"); UVMHIST_CALLED(pghist);

a985 1
	UVMHIST_FUNC("uvm_pagefree"); UVMHIST_CALLED(pghist);
a993 2
	UVMHIST_LOG(pghist, "freeing pg %p/%lx", pg,
	    (u_long)VM_PAGE_TO_PHYS(pg), 0, 0);
a1110 1
	UVMHIST_FUNC("uvm_page_unbusy"); UVMHIST_CALLED(pdhist);
a1121 1
			UVMHIST_LOG(pdhist, "releasing pg %p", pg,0,0,0);
a1137 1
			UVMHIST_LOG(pdhist, "unbusying pg %p", pg,0,0,0);
a1194 1
	UVMHIST_FUNC("uvm_pageidlezero"); UVMHIST_CALLED(pghist);
@


1.109
log
@Move uvm_pglistalloc and uvm_pglistfree to uvm_page.c and garbage
college uvm_pglist.c

uvm_pglistalloc and free are just thin wrappers around pmemrange these
days and don't really need their own file.

ok ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.108 2011/05/30 22:25:24 oga Exp $	*/
d1155 1
a1155 1
				if (pg->pg_flags & PQ_ANON)
@


1.108
log
@Remove the freelist member from vm_physseg

The new world order of pmemrange makes this data completely redundant
(being dealt with by the pmemrange constraints instead). Remove all code
that messes with the freelist.

While touching every caller of uvm_page_physload() anyway, add the flags
argument to all callers (all but one is 0 and that one already used
PHYSLOAD_DEVICE) and remove the macro magic to allow callers to continue
without it.

Should shrink the code a bit, as well.

matthew@@ pointed out some mistakes i'd made.
``freelist death, I like. Ok.' ariane@@
`I agree with the general direction, go ahead and i'll fix any fallout
shortly'' miod@@ (68k 88k and vax i could not check would build)
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.107 2011/05/10 21:38:04 oga Exp $	*/
d784 75
@


1.107
log
@Kill vm_page_lookup_freelist.

it belongs to a world order that isn't here anymore. More importantly it
has been unused for a fair while now.

ok thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.106 2011/04/15 21:35:16 oga Exp $	*/
d462 2
a463 5
/* subroutine: try to allocate from memory chunks on the specified freelist */
static boolean_t uvm_page_physget_freelist(paddr_t *, int);

static boolean_t
uvm_page_physget_freelist(paddr_t *paddrp, int freelist)
d466 1
a466 1
	UVMHIST_FUNC("uvm_page_physget_freelist"); UVMHIST_CALLED(pghist);
a479 3
		if (vm_physmem[lcv].free_list != freelist)
			continue;

a551 12
boolean_t
uvm_page_physget(paddr_t *paddrp)
{
	int i;
	UVMHIST_FUNC("uvm_page_physget"); UVMHIST_CALLED(pghist);

	/* try in the order of freelist preference */
	for (i = 0; i < VM_NFREELIST; i++)
		if (uvm_page_physget_freelist(paddrp, i) == TRUE)
			return (TRUE);
	return (FALSE);
}
d564 2
a565 2
uvm_page_physload_flags(paddr_t start, paddr_t end, paddr_t avail_start,
    paddr_t avail_end, int free_list, int flags)
a574 3
	if (free_list >= VM_NFREELIST || free_list < VM_FREELIST_DEFAULT)
		panic("uvm_page_physload: bad free list %d", free_list);

a713 1
	ps->free_list = free_list;
@


1.106
log
@Add a bit of paranoia to uvm_pageinsert.

At various times diffs have had debugging that checked that we don't
insert a page into the tree on top of an existing page, leaking that
page's references.  Until the recent hackathon (and introduction if
uvm_pagealloc_multi) the bufcache for example did a rb tree look up on
insert to check (under #ifdef DEBUG || 1) so instead just check it on
pageinsert every time, since RB_INSERT returns any duplicates so this
check is pretty much free.

``emphatically yes'' beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.105 2011/04/03 12:36:08 beck Exp $	*/
a1476 17
}

/*
 * uvm_page_lookup_freelist: look up the free list for the specified page
 */
int
uvm_page_lookup_freelist(struct vm_page *pg)
{
#if VM_PHYSSEG_MAX == 1
	return (vm_physmem[0].free_list);
#else
	int lcv;

	lcv = vm_physseg_find(atop(VM_PAGE_TO_PHYS(pg)), NULL);
	KASSERT(lcv != -1);
	return (vm_physmem[lcv].free_list);
#endif
@


1.105
log
@knf - trailing whitespace flense.
ok henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.104 2011/04/02 16:47:17 beck Exp $	*/
d159 1
d163 3
a165 2
	/* XXX should we check duplicates? */
	RB_INSERT(uvm_objtree, &pg->uobject->memt, pg);
@


1.104
log
@Constrain the buffer cache to use only the dma reachable region of memory.
With this change bufcachepercent will be the percentage of dma reachable
memory that the buffer cache will attempt to use.
ok deraadt@@ thib@@ oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.103 2011/04/02 12:38:37 ariane Exp $	*/
d4 1
a4 1
/* 
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and 
d48 1
a48 1
 * 
d54 3
a56 3
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
d58 1
a58 1
 * 
d162 1
a162 1
	/* XXX should we check duplicates? */	
d191 1
a191 1
 * 
d200 1
a200 1
	int lcv, i;  
d202 1
a202 1
#if defined(UVMHIST)   
d221 1
a221 1
	/* 
d234 1
a234 1
	
d236 1
a236 1
	 * first calculate the number of free pages...  
d242 1
a242 1
	 
d256 1
a256 1
	 
d262 1
a262 1
					 
d345 1
a345 1
 * 
d347 1
a347 1
 */   
d372 2
a373 2
	/* 
	 * defer bootstrap allocation to MD code (it may want to allocate 
d877 1
a877 1
	    (uvmexp.free <= uvmexp.reserve_pagedaemon && 
d978 2
a979 2
		 * it knows it needs to allocate swap if it wants to page the 
		 * page out. 
d991 2
a992 2
		 * changed the loan_count [e.g. in uvmfault_anonget()]) we just 
		 * return (when the last loan is dropped, then the page can be 
d996 1
a996 1
		if (saved_loan_count) 
d1364 1
a1364 1
 * uvm_pageunwire: unwire the page.   
@


1.103
log
@Count the number of physical pages within a memory range.
Bob needs this.

ok art@@ bob@@ thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.102 2010/08/07 03:50:02 krw Exp $	*/
d804 26
@


1.102
log
@No "\n" needed at the end of panic() strings.

Bogus chunks pointed out by matthew@@ and miod@@. No cookies for
marco@@ and jasper@@.

ok deraadt@@ miod@@ matthew@@ jasper@@ macro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.101 2010/06/27 03:03:49 thib Exp $	*/
d1466 25
@


1.101
log
@uvm constraints. Add two mandatory MD symbols, uvm_md_constraints
which contains the constraints for DMA/memory allocation for each
architecture, and dma_constraints which contains the range of addresses
that are dma accessable by the system.

This is based on ariane@@'s physcontig diff, with lots of bugfixes and
additions the following additions by my self:

Introduce a new function pool_set_constraints() which sets the address
range for which we allocate pages for the pool from, this is now used
for the mbuf/mbuf cluster pools to keep them dma accessible.

The !direct archs no longer stuff pages into the kernel object in
uvm_km_getpage_pla but rather do a pmap_extract() in uvm_km_putpages.

Tested heavily by my self on i386, amd64 and sparc64. Some tests on
alpha and SGI.

"commit it" beck, art, oga, deraadt
"i like the diff" deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.99 2010/04/20 22:05:44 tedu Exp $	*/
d270 1
a270 1
			panic("uvm_page_init: lost %ld page(s) in init\n",
@


1.100
log
@Committing on behalf or ariane@@.

recommit pmemrange:
        physmem allocator: change the view of free memory from single
        free pages to free ranges.  Classify memory based on region with
        associated use-counter (which is used to construct a priority
        list of where to allocate memory).

	Based on code from tedu@@, help from many.

Useable now that bugs have been found and fixed in most architecture's
pmap.c

ok by everyone who has done a pmap or uvm commit in the last year.
@
text
@d780 27
d866 1
a866 3
	pg->offset = off;
	pg->uobject = obj;
	pg->uanon = anon;
a870 10
	if (anon) {
		anon->an_page = pg;
		atomic_setbits_int(&pg->pg_flags, PQ_ANON);
	} else if (obj)
		uvm_pageinsert(pg);

#if defined(UVM_PAGE_TRKOWN)
	pg->owner_tag = NULL;
#endif
	UVM_PAGE_OWN(pg, "new alloc");
@


1.99
log
@remove proc.h include from uvm_map.h.  This has far reaching effects, as
sysctl.h was reliant on this particular include, and many drivers included
sysctl.h unnecessarily.  remove sysctl.h or add proc.h as needed.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.98 2010/03/24 00:36:04 oga Exp $	*/
a75 1
#include <sys/malloc.h>
a213 4
	for (lcv = 0; lcv < VM_NFREELIST; lcv++) {
		for (i = 0; i < PGFL_NQUEUES; i++)
			TAILQ_INIT(&uvm.page_free[lcv].pgfl_queues[i]);
	}
d219 1
d270 1
a270 1
			printf("uvm_page_init: lost %ld page(s) in init\n",
d272 1
a272 1
			panic("uvm_page_init");  /* XXXCDC: shouldn't happen? */
a291 2
				/* add page to free pool */
				uvm_pagefree(&vm_physmem[lcv].pgs[i]);
d294 7
a654 2
#else
					uvm_pagefree(&pgs[lcv]);
d659 9
a667 1
		/* XXXCDC: incomplete: need to update uvmexp.free, what else? */
a787 7
 * => free_list is ignored if strat == UVM_PGA_STRAT_NORMAL.
 * => policy decision: it is more important to pull a page off of the
 *	appropriate priority free list than it is to get a zero'd or
 *	unknown contents page.  This is because we live with the
 *	consequences of a bad free list decision for the entire
 *	lifetime of the page, e.g. if the page comes from memory that
 *	is slower to access.
d791 2
a792 2
uvm_pagealloc_strat(struct uvm_object *obj, voff_t off, struct vm_anon *anon,
    int flags, int strat, int free_list)
a793 1
	int lcv, try1, try2, zeroit = 0;
d795 2
a796 2
	struct pglist *freeq;
	struct pgfreelist *pgfl;
d798 1
a798 1
	UVMHIST_FUNC("uvm_pagealloc_strat"); UVMHIST_CALLED(pghist);
a802 2
	uvm_lock_fpageq();

d829 5
a833 53
#if PGFL_NQUEUES != 2
#error uvm_pagealloc_strat needs to be updated
#endif

	/*
	 * If we want a zero'd page, try the ZEROS queue first, otherwise
	 * we try the UNKNOWN queue first.
	 */
	if (flags & UVM_PGA_ZERO) {
		try1 = PGFL_ZEROS;
		try2 = PGFL_UNKNOWN;
	} else {
		try1 = PGFL_UNKNOWN;
		try2 = PGFL_ZEROS;
	}

	UVMHIST_LOG(pghist, "obj=%p off=%lx anon=%p flags=%lx",
	    obj, (u_long)off, anon, flags);
	UVMHIST_LOG(pghist, "strat=%ld free_list=%ld", strat, free_list, 0, 0);
 again:
	switch (strat) {
	case UVM_PGA_STRAT_NORMAL:
		/* Check all freelists in descending priority order. */
		for (lcv = 0; lcv < VM_NFREELIST; lcv++) {
			pgfl = &uvm.page_free[lcv];
			if ((pg = TAILQ_FIRST((freeq =
			      &pgfl->pgfl_queues[try1]))) != NULL ||
			    (pg = TAILQ_FIRST((freeq =
			      &pgfl->pgfl_queues[try2]))) != NULL)
				goto gotit;
		}

		/* No pages free! */
		goto fail;

	case UVM_PGA_STRAT_ONLY:
	case UVM_PGA_STRAT_FALLBACK:
		/* Attempt to allocate from the specified free list. */
		KASSERT(free_list >= 0 && free_list < VM_NFREELIST);
		pgfl = &uvm.page_free[free_list];
		if ((pg = TAILQ_FIRST((freeq =
		      &pgfl->pgfl_queues[try1]))) != NULL ||
		    (pg = TAILQ_FIRST((freeq =
		      &pgfl->pgfl_queues[try2]))) != NULL)
			goto gotit;

		/* Fall back, if possible. */
		if (strat == UVM_PGA_STRAT_FALLBACK) {
			strat = UVM_PGA_STRAT_NORMAL;
			goto again;
		}

		/* No pages free! */
d836 2
a837 28
	default:
		panic("uvm_pagealloc_strat: bad strat %d", strat);
		/* NOTREACHED */
	}

 gotit:
	TAILQ_REMOVE(freeq, pg, pageq);
	uvmexp.free--;

	/* update zero'd page count */
	if (pg->pg_flags & PG_ZERO)
		uvmexp.zeropages--;

	/*
	 * update allocation statistics and remember if we have to
	 * zero the page
	 */
	if (flags & UVM_PGA_ZERO) {
		if (pg->pg_flags & PG_ZERO) {
			uvmexp.pga_zerohit++;
			zeroit = 0;
		} else {
			uvmexp.pga_zeromiss++;
			zeroit = 1;
		}
	}

	uvm_unlock_fpageq();		/* unlock free page queue */
d843 3
a845 2
	pg->pg_flags = PG_BUSY|PG_CLEAN|PG_FAKE;
	pg->pg_version++;
d849 3
a851 4
	} else {
		if (obj)
			uvm_pageinsert(pg);
	}
a856 10
	if (flags & UVM_PGA_ZERO) {
		/*
		 * A zero'd page is not clean.  If we got a page not already
		 * zero'd, then we have to zero it ourselves.
		 */
		atomic_clearbits_int(&pg->pg_flags, PG_CLEAN);
		if (zeroit)
			pmap_zero_page(pg);
	}

a861 1
	uvm_unlock_fpageq();
d945 1
a945 1
		
d1000 2
a1001 3
#ifdef UBC
		uvm_pgcnt_anon--;
#endif
d1005 7
a1014 12
	atomic_clearbits_int(&pg->pg_flags, PG_ZERO);

	uvm_lock_fpageq();
#ifdef PAGEFASTRECYCLE
	TAILQ_INSERT_HEAD(&uvm.page_free[
	    uvm_page_lookup_freelist(pg)].pgfl_queues[PGFL_UNKNOWN], pg, pageq);
#else
	TAILQ_INSERT_TAIL(&uvm.page_free[
	    uvm_page_lookup_freelist(pg)].pgfl_queues[PGFL_UNKNOWN], pg, pageq);
#endif
	atomic_clearbits_int(&pg->pg_flags, PQ_MASK);
	atomic_setbits_int(&pg->pg_flags, PQ_FREE);
d1020 2
a1021 1
	uvmexp.free++;
a1024 2

	uvm_unlock_fpageq();
d1124 1
d1191 1
@


1.98
log
@Bring back PHYSLOAD_DEVICE for uvm_page_physload.

ok kettenis@@ beck@@ (tentatively) and ariane@@. deraadt asked for it to be
commited now.

original commit message:

	extend uvm_page_physload to have the ability to add "device" pages to
	the system.

	This is needed in the case where you need managed pages so you can
	handle faulting and pmap_page_protect() on said pages when you manage
	memory in such regions (i'm looking at you, graphics cards).

	these pages are flagged PG_DEV, and shall never be on the freelists,
	assert this. behaviour remains unchanged in the non-device case,
	specifically for all archs currently in the tree we panic if called
	after bootstrap.

	ok art@@ kettenis@@, beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.97 2009/10/14 17:53:30 beck Exp $	*/
d81 1
@


1.97
log
@Fix buffer cache backoff in the page daemon - deal with inactive pages to
more correctly reflect the new state of the world - that is - how many pages
can be cheaply reclaimed - which now includes clean buffer cache pages.

This change fixes situations where people would be running with a large bufcachepercent, and still notice swapping without the buffer cache backing off.

ok oga@@, testing by many on tech@@ and others. Thanks.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.96 2009/08/13 15:29:59 deraadt Exp $	*/
d578 2
a579 2
uvm_page_physload(paddr_t start, paddr_t end, paddr_t avail_start,
    paddr_t avail_end, int free_list)
d621 10
a630 8
#if defined(VM_PHYSSEG_NOADD)
		panic("uvm_page_physload: tried to add RAM after vm_mem_init");
#else
		/* XXXCDC: need some sort of lockout for this case */
		paddr_t paddr;
		npages = end - start;  /* # of pages */
		pgs = (vm_page *)uvm_km_zalloc(kernel_map,
		    sizeof(struct vm_page) * npages);
d637 3
a639 3
		/* init phys_addr and free_list, and free pages */
		for (lcv = 0, paddr = ptoa(start) ;
				 lcv < npages ; lcv++, paddr += PAGE_SIZE) {
d641 3
a643 1
			pgs[lcv].free_list = free_list;
d645 13
a657 2
			    atop(paddr) <= avail_end)
				uvm_pagefree(&pgs[lcv]);
a660 1
#endif
d918 1
a923 3
#ifdef UBC
		uvm_pgcnt_anon++;
#endif
d1011 1
@


1.96
log
@PAGEFASTRECYCLE is an option we have been using for a while to encourage
the kernel to reuse freed pages as quickly as possible, and it has been
finding bugs (some of which we have already fixed)
ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.95 2009/08/06 15:28:14 oga Exp $	*/
d797 1
a797 1
	     uvmexp.inactive < uvmexp.inactarg))
@


1.95
log
@reintroduce the uvm_tree commit.

Now instead of the global object hashtable, we have a per object tree.

Testing shows no performance difference and a slight code shrink. OTOH when
locking is more fine grained this should be faster due to lock contention on
uvm.hashlock.

ok thib@@, art@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.94 2009/07/26 21:26:10 deraadt Exp $	*/
d1086 4
d1092 1
@


1.94
log
@stop trying to fast-recycle pages for now.  a few bugs have been found and
fixed, but now it is time for a little break from the chaos.
ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.92 2009/07/22 21:05:37 oga Exp $	*/
d85 11
a131 8
 * we use a hash table with only one bucket during bootup.  we will
 * later rehash (resize) the hash table once the allocator is ready.
 * we static allocate the one bootstrap bucket below...
 */

static struct pglist uvm_bootbucket;

/*
d148 1
a148 1
 * uvm_pageinsert: insert a page in the object and the hash table
d151 1
a151 1
 * => caller must lock page queues
a158 1
	struct pglist *buck;
d162 2
a163 6
	mtx_enter(&uvm.hashlock);
	buck = &uvm.page_hash[uvm_pagehash(pg->uobject,pg->offset)];
	TAILQ_INSERT_TAIL(buck, pg, hashq);	/* put in hash */
	mtx_leave(&uvm.hashlock);

	TAILQ_INSERT_TAIL(&pg->uobject->memq, pg, listq); /* put in object */
d169 1
a169 1
 * uvm_page_remove: remove page from object and hash
a177 1
	struct pglist *buck;
d181 1
a181 13
	mtx_enter(&uvm.hashlock);
	buck = &uvm.page_hash[uvm_pagehash(pg->uobject,pg->offset)];
	TAILQ_REMOVE(buck, pg, hashq);
	mtx_leave(&uvm.hashlock);

#ifdef UBC
	if (pg->uobject->pgops == &uvm_vnodeops) {
		uvm_pgcnt_vnode--;
	}
#endif

	/* object should be locked */
	TAILQ_REMOVE(&pg->uobject->memq, pg, listq);
a223 12
	/*
	 * init the <obj,offset> => <page> hash table.  for now
	 * we just have one bucket (the bootstrap bucket).  later on we
	 * will allocate new buckets as we dynamically resize the hash table.
	 */

	uvm.page_nhash = 1;			/* 1 bucket */
	uvm.page_hashmask = 0;			/* mask for hash function */
	uvm.page_hash = &uvm_bootbucket;	/* install bootstrap bucket */
	TAILQ_INIT(uvm.page_hash);		/* init hash table */
	mtx_init(&uvm.hashlock, IPL_VM);	/* init hash table lock */

a717 3
	if (!preload)
		uvm_page_rehash();

a720 85
/*
 * uvm_page_rehash: reallocate hash table based on number of free pages.
 */

void
uvm_page_rehash(void)
{
	int freepages, lcv, bucketcount, oldcount;
	struct pglist *newbuckets, *oldbuckets;
	struct vm_page *pg;
	size_t newsize, oldsize;

	/*
	 * compute number of pages that can go in the free pool
	 */

	freepages = 0;
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++)
		freepages +=
		    (vm_physmem[lcv].avail_end - vm_physmem[lcv].avail_start);

	/*
	 * compute number of buckets needed for this number of pages
	 */

	bucketcount = 1;
	while (bucketcount < freepages)
		bucketcount = bucketcount * 2;

	/*
	 * compute the size of the current table and new table.
	 */

	oldbuckets = uvm.page_hash;
	oldcount = uvm.page_nhash;
	oldsize = round_page(sizeof(struct pglist) * oldcount);
	newsize = round_page(sizeof(struct pglist) * bucketcount);

	/*
	 * allocate the new buckets
	 */

	newbuckets = (struct pglist *) uvm_km_alloc(kernel_map, newsize);
	if (newbuckets == NULL) {
		printf("uvm_page_physrehash: WARNING: could not grow page "
		    "hash table\n");
		return;
	}
	for (lcv = 0 ; lcv < bucketcount ; lcv++)
		TAILQ_INIT(&newbuckets[lcv]);

	/*
	 * now replace the old buckets with the new ones and rehash everything
	 */

	mtx_enter(&uvm.hashlock);
	uvm.page_hash = newbuckets;
	uvm.page_nhash = bucketcount;
	uvm.page_hashmask = bucketcount - 1;  /* power of 2 */

	/* ... and rehash */
	for (lcv = 0 ; lcv < oldcount ; lcv++) {
		while ((pg = TAILQ_FIRST(&oldbuckets[lcv])) != NULL) {
			TAILQ_REMOVE(&oldbuckets[lcv], pg, hashq);
			TAILQ_INSERT_TAIL(
			  &uvm.page_hash[uvm_pagehash(pg->uobject, pg->offset)],
			  pg, hashq);
		}
	}
	mtx_leave(&uvm.hashlock);

	/*
	 * free old bucket array if is not the boot-time table
	 */

	if (oldbuckets != &uvm_bootbucket)
		uvm_km_free(kernel_map, (vaddr_t) oldbuckets, oldsize);

	/*
	 * done
	 */
	return;
}


d731 2
a732 2
	printf("rehash: physical memory config [segs=%d of %d]:\n",
				 vm_nphysseg, VM_PHYSSEG_MAX);
a745 1
	printf("number of buckets = %d\n", uvm.page_nhash);
d762 1
a762 1
 * => if obj != NULL, obj must be locked (to put in hash)
d977 1
a977 1
 * => erase page's identity (i.e. remove from hash/object)
d1362 2
a1363 5
	struct vm_page *pg;
	struct pglist *buck;

	mtx_enter(&uvm.hashlock);
	buck = &uvm.page_hash[uvm_pagehash(obj,off)];
d1365 2
a1366 7
	TAILQ_FOREACH(pg, buck, hashq) {
		if (pg->uobject == obj && pg->offset == off) {
			break;
		}
	}
	mtx_leave(&uvm.hashlock);
	return(pg);
@


1.93
log
@Insert free pages at the head of the page queues.  Should provide better
cache locality and will pave the way for the new pmemrange allocator.
Based on hints from art@@ and ariane@@.

ok ariane@@, deraadt@@, oga@@
@
text
@d1202 1
a1202 1
	TAILQ_INSERT_HEAD(&uvm.page_free[
@


1.92
log
@Put the PG_RELEASED changes diff back in.

This has has been tested very very thoroughly on all archs we have
excepting 88k and 68k. Please see cvs log for the individual commit
messages.

ok beck@@, thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.91 2009/06/17 00:13:59 oga Exp $	*/
d1202 1
a1202 1
	TAILQ_INSERT_TAIL(&uvm.page_free[
@


1.91
log
@date based reversion of uvm to the 4th May.

More backouts in line with previous ones, this appears to bring us back to a
stable condition.

A machine forced to 64mb of ram cycled 10GB through swap with this diff
and is still running as I type this. Other tests by ariane@@ and thib@@
also seem to show that it's alright.

ok deraadt@@, thib@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.77 2009/05/01 20:44:22 oga Exp $	*/
d1248 8
a1255 1
				uobj->pgops->pgo_releasepg(pg, NULL);
@


1.90
log
@Backout pmemrange (which to most people is more well known as physmem
allocator).

"i can't see any obvious problems" oga
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.80 2009/05/08 15:10:35 ariane Exp $	*/
d198 1
a198 1
	atomic_clearbits_int(&pg->pg_flags, PG_TABLED|PQ_AOBJ);
d486 1
a486 1
boolean_t uvm_page_physget_freelist(paddr_t *, int);
d488 1
a488 1
boolean_t
d914 1
a914 1
		wakeup(&uvm.pagedaemon_proc);
a1188 1
		atomic_clearbits_int(&pg->pg_flags, PQ_ANON);
a1189 1
		pg->uanon = NULL;
@


1.89
log
@Backout all changes to uvm after pmemrange (which will be backed out
separately).

a change at or just before the hackathon has either exposed or added a
very very nasty memory corruption bug that is giving us hell right now.
So in the interest of kernel stability these diffs are being backed out
until such a time as that corruption bug has been found and squashed,
then the ones that are proven good may slowly return.

a quick hitlist of the main commits this backs out:

mine:
uvm_objwire
the lock change in uvm_swap.c
using trees for uvm objects instead of the hash
removing the pgo_releasepg callback.

art@@'s:
putting pmap_page_protect(VM_PROT_NONE) in uvm_pagedeactivate() since
all callers called that just prior anyway.

ok beck@@, ariane@@.

prompted by deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.81 2009/06/01 17:42:33 ariane Exp $	*/
d162 1
a162 1
	TAILQ_INSERT_TAIL(buck, pg, fq.queues.hashq);	/* put in hash */
d165 1
a165 2
	TAILQ_INSERT_TAIL(&pg->uobject->memq, pg,
	    fq.queues.listq); /* put in object */
d186 1
a186 1
	TAILQ_REMOVE(buck, pg, fq.queues.hashq);
d196 1
a196 1
	TAILQ_REMOVE(&pg->uobject->memq, pg, fq.queues.listq);
d229 4
a237 1
	uvm_pmr_init();
d322 2
a325 5

		/* add pages to free pool */
		uvm_pmr_freepages(&vm_physmem[lcv].pgs[
		    vm_physmem[lcv].avail_start - vm_physmem[lcv].start],
		    vm_physmem[lcv].avail_end - vm_physmem[lcv].avail_start);
d814 1
a814 1
			TAILQ_REMOVE(&oldbuckets[lcv], pg, fq.queues.hashq);
d817 1
a817 1
			  pg, fq.queues.hashq);
d895 1
a895 2
	struct pglist pgl;
	int pmr_flags;
d897 2
d905 2
d933 53
a985 5
	pmr_flags = UVM_PLA_NOWAIT;
	if (flags & UVM_PGA_ZERO)
		pmr_flags |= UVM_PLA_ZERO;
	TAILQ_INIT(&pgl);
	if (uvm_pmr_getpages(1, 0, 0, 1, 0, 1, pmr_flags, &pgl) != 0)
d987 29
a1015 3
	pg = TAILQ_FIRST(&pgl);
	KASSERT(pg != NULL);
	KASSERT(TAILQ_NEXT(pg, pageq) == NULL);
d1021 1
d1037 10
d1052 1
a1102 1
	struct pglist pgl;
d1198 1
a1198 1
	 * Clean page state bits.
d1200 8
a1207 16
	atomic_clearbits_int(&pg->pg_flags,
	    PG_ZERO|PG_FAKE|PG_BUSY|PG_RELEASED|PG_CLEAN|PG_CLEANCHK);
	/*
	 * Pmap flag cleaning.
	 * XXX: Shouldn't pmap do this?
	 */
	atomic_clearbits_int(&pg->pg_flags,
	    PG_PMAP0|PG_PMAP1|PG_PMAP2|PG_PMAP3);

#if defined(DIAGNOSTIC)
	if (pg->pg_flags != 0) {
		panic("uvm_pagefree: expected page %p pg_flags to be 0\n"
		    "uvm_pagefree: instead of pg->pg_flags = %x\n",
		    VM_PAGE_TO_PHYS(pg), pg->pg_flags);
	}
#endif
d1213 1
a1213 3
	TAILQ_INIT(&pgl);
	TAILQ_INSERT_HEAD(&pgl, pg, pageq);
	uvm_pmr_freepageq(&pgl);
d1217 2
a1310 1
#if 0 /* Disabled for now. */
a1376 1
#endif /* 0 */
d1479 1
a1479 1
	TAILQ_FOREACH(pg, buck, fq.queues.hashq) {
@


1.88
log
@backout:
> extend uvm_page_physload to have the ability to add "device" pages to the
> system.
since it was overlayed over a system that we warned would go "in to be
tested, but may be pulled out".  oga, you just made me spend 20 minutes
of time I should not have had to spend doing this.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.86 2009/06/06 17:46:44 art Exp $	*/
d121 8
d145 1
a145 1
 * uvm_pageinsert: insert a page in the object
d156 1
d160 4
d165 2
a166 1
	RB_INSERT(uobj_pgs, &pg->uobject->memt, pg);
d172 1
a172 1
 * uvm_page_remove: remove page from object
d181 1
d185 11
d197 1
a197 1
	RB_REMOVE(uobj_pgs, &pg->uobject->memt, pg);
a204 8
int
uvm_pagecmp(struct vm_page *a, struct vm_page *b)
{
	return (a->offset < b->offset ? -1 : a->offset > b->offset);
}

RB_GENERATE(uobj_pgs, vm_page, fq.queues.tree, uvm_pagecmp);

d237 12
d746 3
d752 85
d862 1
d944 1
a944 3
	pg->pg_flags = PG_BUSY|PG_FAKE;
	if (!(flags & UVM_PGA_ZERO))
		atomic_setbits_int(&pg->pg_flags, PG_CLEAN);
d1005 1
a1005 1
 * => erase page's identity (i.e. remove from object)
d1113 2
a1114 2
	atomic_clearbits_int(&pg->pg_flags, PG_ZERO|PG_FAKE|PG_BUSY|
	    PG_RELEASED|PG_CLEAN|PG_CLEANCHK|PQ_ENCRYPT);
d1171 1
a1171 8
				uvm_lock_pageq();
				pmap_page_protect(pg, VM_PROT_NONE);
				/* XXX won't happen right now */
				if (pg->pg_flags & PQ_ANON)
					uao_dropswap(uobj,
					    pg->offset >> PAGE_SHIFT);
				uvm_pagefree(pg);
				uvm_unlock_pageq();
d1396 2
a1397 1
	struct vm_page find;
d1399 10
a1408 2
	find.offset = off;
	return (RB_FIND(uobj_pgs, &obj->memt, &find));
a1465 2
	pmap_page_protect(pg, VM_PROT_NONE);

@


1.87
log
@extend uvm_page_physload to have the ability to add "device" pages to the
system.

This is needed in the case where you need managed pages so you can
handle faulting and pmap_page_protect() on said pages when you manage
memory in such regions (i'm looking at you, graphics cards).

these pages are flagged PG_DEV, and shall never be on the freelists,
assert this. behaviour remains unchanged in the non-device case,
specifically for all archs currently in the tree we panic if called
after bootstrap.

ok art@@, kettenis@@, ariane@@, beck@@.
@
text
@d76 1
d576 2
a577 2
uvm_page_physload_flags(paddr_t start, paddr_t end, paddr_t avail_start,
    paddr_t avail_end, int free_list, int flags)
d619 4
a622 4
		/*
		 * XXXCDC: need some sort of lockout for this case
		 * right now it is only used by devices so it should be alright.
		 */
a623 1

d625 1
a625 2

		pgs = (struct vm_page *)uvm_km_zalloc(kernel_map,
d633 3
a635 4

		/* init phys_addr and free pages, XXX uvmexp.npages */
		for (lcv = 0, paddr = ptoa(start); lcv < npages;
		    lcv++, paddr += PAGE_SIZE) {
d637 1
a637 3
#ifdef __HAVE_VM_PAGE_MD
			VM_MDPAGE_INIT(&pgs[lcv]);
#endif
d639 2
a640 13
			    atop(paddr) <= avail_end) {
				if (flags & PHYSLOAD_DEVICE) {
					atomic_setbits_int(&pgs[lcv].pg_flags,
					    PG_DEV);
					pgs[lcv].wire_count = 1;
				} else {
#if defined(VM_PHYSSEG_NOADD)
		panic("uvm_page_physload: tried to add RAM after vm_mem_init");
#else
					uvm_pagefree(&pgs[lcv]);
#endif
				}
			}
d644 1
a824 1
	KASSERT((pg->pg_flags & PG_DEV) == 0);
d831 3
a910 1
	KASSERT((pg->pg_flags & PG_DEV) == 0);
@


1.86
log
@Since all callers of uvm_pagedeactivate did pmap_page_protect(.., VM_PROT_NONE)
just move that into uvm_pagedeactivate.

oga@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.85 2009/06/03 04:56:54 ariane Exp $	*/
a75 1
#include <sys/malloc.h>
d575 2
a576 2
uvm_page_physload(paddr_t start, paddr_t end, paddr_t avail_start,
    paddr_t avail_end, int free_list)
d618 4
a621 4
#if defined(VM_PHYSSEG_NOADD)
		panic("uvm_page_physload: tried to add RAM after vm_mem_init");
#else
		/* XXXCDC: need some sort of lockout for this case */
d623 1
d625 2
a626 1
		pgs = (vm_page *)uvm_km_zalloc(kernel_map,
d634 4
a637 3
		/* init phys_addr and free_list, and free pages */
		for (lcv = 0, paddr = ptoa(start) ;
				 lcv < npages ; lcv++, paddr += PAGE_SIZE) {
d639 3
a641 1
			pgs[lcv].free_list = free_list;
d643 13
a655 2
			    atop(paddr) <= avail_end)
				uvm_pagefree(&pgs[lcv]);
a658 1
#endif
d839 1
a845 3
#ifdef UBC
		uvm_pgcnt_anon++;
#endif
d923 1
@


1.85
log
@phys allocator fix: zeroed pages are not clean.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.84 2009/06/02 23:00:19 oga Exp $	*/
d1347 2
@


1.84
log
@Instead of the global hash table with the terrible hashfunction and a
global lock, switch the uvm object pages to being kept in a per-object
RB_TREE. Right now this is approximately the same speed, but cleaner.
When biglock usage is reduced this will improve concurrency due to lock
contention..

ok beck@@ art@@. Thanks to jasper for the speed testing.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.83 2009/06/02 19:49:08 ariane Exp $	*/
d825 3
a827 1
	pg->pg_flags = PG_BUSY|PG_CLEAN|PG_FAKE;
@


1.83
log
@Clear PQ_ENCRYPT flag on uvm_pagefree, because free pages are by definition
not encrypted.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.82 2009/06/01 19:54:02 oga Exp $	*/
a120 8
 * we use a hash table with only one bucket during bootup.  we will
 * later rehash (resize) the hash table once the allocator is ready.
 * we static allocate the one bootstrap bucket below...
 */

static struct pglist uvm_bootbucket;

/*
d137 1
a137 1
 * uvm_pageinsert: insert a page in the object and the hash table
a147 1
	struct pglist *buck;
a150 4
	mtx_enter(&uvm.hashlock);
	buck = &uvm.page_hash[uvm_pagehash(pg->uobject,pg->offset)];
	TAILQ_INSERT_TAIL(buck, pg, fq.queues.hashq);	/* put in hash */
	mtx_leave(&uvm.hashlock);
d152 1
a152 2
	TAILQ_INSERT_TAIL(&pg->uobject->memq, pg,
	    fq.queues.listq); /* put in object */
d158 1
a158 1
 * uvm_page_remove: remove page from object and hash
a166 1
	struct pglist *buck;
a169 11
	mtx_enter(&uvm.hashlock);
	buck = &uvm.page_hash[uvm_pagehash(pg->uobject,pg->offset)];
	TAILQ_REMOVE(buck, pg, fq.queues.hashq);
	mtx_leave(&uvm.hashlock);

#ifdef UBC
	if (pg->uobject->pgops == &uvm_vnodeops) {
		uvm_pgcnt_vnode--;
	}
#endif

d171 1
a171 1
	TAILQ_REMOVE(&pg->uobject->memq, pg, fq.queues.listq);
d179 8
a218 12
	/*
	 * init the <obj,offset> => <page> hash table.  for now
	 * we just have one bucket (the bootstrap bucket).  later on we
	 * will allocate new buckets as we dynamically resize the hash table.
	 */

	uvm.page_nhash = 1;			/* 1 bucket */
	uvm.page_hashmask = 0;			/* mask for hash function */
	uvm.page_hash = &uvm_bootbucket;	/* install bootstrap bucket */
	TAILQ_INIT(uvm.page_hash);		/* init hash table */
	mtx_init(&uvm.hashlock, IPL_VM);	/* init hash table lock */

a715 3
	if (!preload)
		uvm_page_rehash();

a718 85
/*
 * uvm_page_rehash: reallocate hash table based on number of free pages.
 */

void
uvm_page_rehash(void)
{
	int freepages, lcv, bucketcount, oldcount;
	struct pglist *newbuckets, *oldbuckets;
	struct vm_page *pg;
	size_t newsize, oldsize;

	/*
	 * compute number of pages that can go in the free pool
	 */

	freepages = 0;
	for (lcv = 0 ; lcv < vm_nphysseg ; lcv++)
		freepages +=
		    (vm_physmem[lcv].avail_end - vm_physmem[lcv].avail_start);

	/*
	 * compute number of buckets needed for this number of pages
	 */

	bucketcount = 1;
	while (bucketcount < freepages)
		bucketcount = bucketcount * 2;

	/*
	 * compute the size of the current table and new table.
	 */

	oldbuckets = uvm.page_hash;
	oldcount = uvm.page_nhash;
	oldsize = round_page(sizeof(struct pglist) * oldcount);
	newsize = round_page(sizeof(struct pglist) * bucketcount);

	/*
	 * allocate the new buckets
	 */

	newbuckets = (struct pglist *) uvm_km_alloc(kernel_map, newsize);
	if (newbuckets == NULL) {
		printf("uvm_page_physrehash: WARNING: could not grow page "
		    "hash table\n");
		return;
	}
	for (lcv = 0 ; lcv < bucketcount ; lcv++)
		TAILQ_INIT(&newbuckets[lcv]);

	/*
	 * now replace the old buckets with the new ones and rehash everything
	 */

	mtx_enter(&uvm.hashlock);
	uvm.page_hash = newbuckets;
	uvm.page_nhash = bucketcount;
	uvm.page_hashmask = bucketcount - 1;  /* power of 2 */

	/* ... and rehash */
	for (lcv = 0 ; lcv < oldcount ; lcv++) {
		while ((pg = TAILQ_FIRST(&oldbuckets[lcv])) != NULL) {
			TAILQ_REMOVE(&oldbuckets[lcv], pg, fq.queues.hashq);
			TAILQ_INSERT_TAIL(
			  &uvm.page_hash[uvm_pagehash(pg->uobject, pg->offset)],
			  pg, fq.queues.hashq);
		}
	}
	mtx_leave(&uvm.hashlock);

	/*
	 * free old bucket array if is not the boot-time table
	 */

	if (oldbuckets != &uvm_bootbucket)
		uvm_km_free(kernel_map, (vaddr_t) oldbuckets, oldsize);

	/*
	 * done
	 */
	return;
}


a743 1
	printf("number of buckets = %d\n", uvm.page_nhash);
d886 1
a886 1
 * => erase page's identity (i.e. remove from hash/object)
d1284 1
a1284 5
	struct vm_page *pg;
	struct pglist *buck;

	mtx_enter(&uvm.hashlock);
	buck = &uvm.page_hash[uvm_pagehash(obj,off)];
d1286 2
a1287 7
	TAILQ_FOREACH(pg, buck, fq.queues.hashq) {
		if (pg->uobject == obj && pg->offset == off) {
			break;
		}
	}
	mtx_leave(&uvm.hashlock);
	return(pg);
@


1.82
log
@Since we've now cleared up a lot of the PG_RELEASED setting, remove the
pgo_releasepg() hook and just free the page the "normal" way in the one
place we'll ever see PG_RELEASED and should care (uvm_page_unbusy,
called in aiodoned).

ok art@@, beck@@, thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.81 2009/06/01 17:42:33 ariane Exp $	*/
d1113 2
a1114 2
	atomic_clearbits_int(&pg->pg_flags,
	    PG_ZERO|PG_FAKE|PG_BUSY|PG_RELEASED|PG_CLEAN|PG_CLEANCHK);
@


1.81
log
@physmem allocator: change the view of free memory from single free pages
to free ranges.
Classify memory based on region with associated use-counter (which is used
to construct a priority list of where to allocate memory).

Based on code from tedu@@, help from many.
Ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.80 2009/05/08 15:10:35 ariane Exp $	*/
d1171 8
a1178 1
				uobj->pgops->pgo_releasepg(pg, NULL);
@


1.80
log
@Clear PQ_AOBJ at pageremove: when a page is no longer part of a uvm_object,
it is also not part of an aobj.
Clear anon flags at pagefree: page is no longer part of an anon.

ok oga
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.79 2009/05/08 13:50:15 ariane Exp $	*/
d162 1
a162 1
	TAILQ_INSERT_TAIL(buck, pg, hashq);	/* put in hash */
d165 2
a166 1
	TAILQ_INSERT_TAIL(&pg->uobject->memq, pg, listq); /* put in object */
d187 1
a187 1
	TAILQ_REMOVE(buck, pg, hashq);
d197 1
a197 1
	TAILQ_REMOVE(&pg->uobject->memq, pg, listq);
a229 4
	for (lcv = 0; lcv < VM_NFREELIST; lcv++) {
		for (i = 0; i < PGFL_NQUEUES; i++)
			TAILQ_INIT(&uvm.page_free[lcv].pgfl_queues[i]);
	}
d235 1
a319 2
				/* add page to free pool */
				uvm_pagefree(&vm_physmem[lcv].pgs[i]);
d322 5
d815 1
a815 1
			TAILQ_REMOVE(&oldbuckets[lcv], pg, hashq);
d818 1
a818 1
			  pg, hashq);
d896 2
a897 1
	int lcv, try1, try2, zeroit = 0;
a898 2
	struct pglist *freeq;
	struct pgfreelist *pgfl;
a904 2
	uvm_lock_fpageq();

d931 5
a935 53
#if PGFL_NQUEUES != 2
#error uvm_pagealloc_strat needs to be updated
#endif

	/*
	 * If we want a zero'd page, try the ZEROS queue first, otherwise
	 * we try the UNKNOWN queue first.
	 */
	if (flags & UVM_PGA_ZERO) {
		try1 = PGFL_ZEROS;
		try2 = PGFL_UNKNOWN;
	} else {
		try1 = PGFL_UNKNOWN;
		try2 = PGFL_ZEROS;
	}

	UVMHIST_LOG(pghist, "obj=%p off=%lx anon=%p flags=%lx",
	    obj, (u_long)off, anon, flags);
	UVMHIST_LOG(pghist, "strat=%ld free_list=%ld", strat, free_list, 0, 0);
 again:
	switch (strat) {
	case UVM_PGA_STRAT_NORMAL:
		/* Check all freelists in descending priority order. */
		for (lcv = 0; lcv < VM_NFREELIST; lcv++) {
			pgfl = &uvm.page_free[lcv];
			if ((pg = TAILQ_FIRST((freeq =
			      &pgfl->pgfl_queues[try1]))) != NULL ||
			    (pg = TAILQ_FIRST((freeq =
			      &pgfl->pgfl_queues[try2]))) != NULL)
				goto gotit;
		}

		/* No pages free! */
		goto fail;

	case UVM_PGA_STRAT_ONLY:
	case UVM_PGA_STRAT_FALLBACK:
		/* Attempt to allocate from the specified free list. */
		KASSERT(free_list >= 0 && free_list < VM_NFREELIST);
		pgfl = &uvm.page_free[free_list];
		if ((pg = TAILQ_FIRST((freeq =
		      &pgfl->pgfl_queues[try1]))) != NULL ||
		    (pg = TAILQ_FIRST((freeq =
		      &pgfl->pgfl_queues[try2]))) != NULL)
			goto gotit;

		/* Fall back, if possible. */
		if (strat == UVM_PGA_STRAT_FALLBACK) {
			strat = UVM_PGA_STRAT_NORMAL;
			goto again;
		}

		/* No pages free! */
d937 3
a939 29

	default:
		panic("uvm_pagealloc_strat: bad strat %d", strat);
		/* NOTREACHED */
	}

 gotit:
	TAILQ_REMOVE(freeq, pg, pageq);
	uvmexp.free--;

	/* update zero'd page count */
	if (pg->pg_flags & PG_ZERO)
		uvmexp.zeropages--;

	/*
	 * update allocation statistics and remember if we have to
	 * zero the page
	 */
	if (flags & UVM_PGA_ZERO) {
		if (pg->pg_flags & PG_ZERO) {
			uvmexp.pga_zerohit++;
			zeroit = 0;
		} else {
			uvmexp.pga_zeromiss++;
			zeroit = 1;
		}
	}

	uvm_unlock_fpageq();		/* unlock free page queue */
a944 1
	pg->pg_version++;
a959 10
	if (flags & UVM_PGA_ZERO) {
		/*
		 * A zero'd page is not clean.  If we got a page not already
		 * zero'd, then we have to zero it ourselves.
		 */
		atomic_clearbits_int(&pg->pg_flags, PG_CLEAN);
		if (zeroit)
			pmap_zero_page(pg);
	}

a964 1
	uvm_unlock_fpageq();
d1015 1
d1111 7
a1117 1
	 * and put on free queue
d1119 10
a1128 8

	atomic_clearbits_int(&pg->pg_flags, PG_ZERO);

	uvm_lock_fpageq();
	TAILQ_INSERT_TAIL(&uvm.page_free[
	    uvm_page_lookup_freelist(pg)].pgfl_queues[PGFL_UNKNOWN], pg, pageq);
	atomic_clearbits_int(&pg->pg_flags, PQ_MASK);
	atomic_setbits_int(&pg->pg_flags, PQ_FREE);
d1134 3
a1136 1
	uvmexp.free++;
a1139 2

	uvm_unlock_fpageq();
d1232 1
d1299 1
d1402 1
a1402 1
	TAILQ_FOREACH(pg, buck, hashq) {
@


1.79
log
@Remove static qualifier of functions that are not inline.
Makes trace in ddb useful.

ok oga
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.78 2009/05/04 18:08:06 oga Exp $	*/
d198 1
a198 1
	atomic_clearbits_int(&pg->pg_flags, PG_TABLED);
d1189 1
d1191 1
@


1.78
log
@Instead of keeping two ints in the uvm structure specifically just to
sleep on them (and otherwise ignore them) sleep on the pointer to the
{aiodoned,pagedaemon}_proc members, and nuke the two extra words.

"no objections" art@@, ok beck@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.77 2009/05/01 20:44:22 oga Exp $	*/
d486 1
a486 1
static boolean_t uvm_page_physget_freelist(paddr_t *, int);
d488 1
a488 1
static boolean_t
@


1.77
log
@uvm_page_alloc() + memset -> uvm_page_zalloc()

nothing uses this code yet, but might as well do it the right way.

"if you can't live without commiting this." miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.76 2009/04/28 16:06:07 miod Exp $	*/
d914 1
a914 1
		wakeup(&uvm.pagedaemon);
@


1.76
log
@Revert pageqlock back from a mutex to a simple_lock, as it needs to be
recursive in some cases (mostly involving swapping). A proper fix is in
the works, but this will unbreak kernels for now.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.75 2009/04/14 20:12:05 oga Exp $	*/
d654 1
a654 1
		pgs = (vm_page *)uvm_km_alloc(kernel_map,
d662 1
a662 2
		/* zero data, init phys_addr and free_list, and free pages */
		memset(pgs, 0, sizeof(struct vm_page) * npages);
@


1.75
log
@The use of uvm.pagedaemon_lock is incredibly inconsistent. only a
fraction of the wakeups and sleeps involved here actually grab that
lock. The remainder, on the other hand, always have the fpageq_lock
locked.

So, make this locking correct by switching the other users over to
fpageq_lock, too.

This would probably be better off being a semaphore, but for now at
least it's correct.

"ok, unless you want to implement semaphores" art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.74 2009/04/13 22:17:54 oga Exp $	*/
d236 1
a236 1
	mtx_init(&uvm.pageqlock, IPL_NONE);
@


1.74
log
@Convert the page queue lock to a mutex instead of a simplelock.

Fix up the one case of lock recursion (which blatantly ignored the
comment right above it saying that we don't need to lock). The rest of
the lock usage has been checked and appears to be correct.

ok ariane@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.73 2009/04/06 17:03:51 oga Exp $	*/
a339 2

	simple_lock_init(&uvm.pagedaemon_lock);
@


1.73
log
@In the case where VM_PHYSSEG_MAX == 1 make vm_physseg_find and
PHYS_TO_VM_PAGE inline again. This should stop function call overhead
killing the vax and other slow archs while keeping the benefit for the
faster platforms.

suggested by miod. ok miod@@, toby@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.72 2009/04/06 12:02:52 oga Exp $	*/
d236 1
a236 1
	simple_lock_init(&uvm.pageqlock);
@


1.72
log
@Instead of doing splbio(); simple_lock(&uvm.aiodoned_lock); just replace
the simple lock with a real lock - a IPL_BIO mutex. While i'm here, make
the sleeping condition one hell of a lot simpler in the aio daemon.

some ideas from and ok art@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.71 2009/03/26 13:38:45 oga Exp $	*/
d1384 1
a1390 1
#if VM_PHYSSEG_MAX == 1
d1392 1
a1392 9
	/* 'contig' case */
	if (pframe >= vm_physmem[0].start && pframe < vm_physmem[0].end) {
		if (offp)
			*offp = pframe - vm_physmem[0].start;
		return(0);
	}
	return(-1);

#elif (VM_PHYSSEG_STRAT == VM_PSTRAT_BSEARCH)
d1463 1
@


1.71
log
@Convert splvm() + simplelock(&uvm.hashlock); around the page hash table
into a IPL_VM blocking mutex, also slightly extend the locked area so
that it actually protects access to the page array (as the comment on
the lock declaration says it should).

ansify a few functions while i'm in the file.

"ok, even though you're sneaking in ansification in a diff. You dirty
you." art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.70 2009/03/25 20:00:18 oga Exp $	*/
d342 1
a342 1
	simple_lock_init(&uvm.aiodoned_lock);
@


1.70
log
@Move all of the pseudo-inline functions in uvm into C files.

By pseudo-inline, I mean that if a certain macro was defined, they would
be inlined. However, no architecture defines that, and none has for a
very very long time. Therefore mainly this just makes the code a damned
sight easier to read. Some k&r -> ansi declarations while I'm in there.

"just commit it" art@@. ok weingart@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.69 2009/03/24 16:29:42 oga Exp $	*/
a156 1
	int s;
d160 1
a161 2
	s = splvm();
	simple_lock(&uvm.hashlock);
d163 1
a163 2
	simple_unlock(&uvm.hashlock);
	splx(s);
a180 1
	int s;
d184 1
a185 2
	s = splvm();
	simple_lock(&uvm.hashlock);
d187 1
a187 2
	simple_unlock(&uvm.hashlock);
	splx(s);
d211 1
a211 2
uvm_page_init(kvm_startp, kvm_endp)
	vaddr_t *kvm_startp, *kvm_endp;
d249 1
a249 1
	simple_lock_init(&uvm.hashlock);	/* init hash table lock */
d377 1
a377 1
uvm_setpagesize()
d394 1
a394 2
uvm_pageboot_alloc(size)
	vsize_t size;
d491 1
a491 3
uvm_page_physget_freelist(paddrp, freelist)
	paddr_t *paddrp;
	int freelist;
d584 1
a584 2
uvm_page_physget(paddrp)
	paddr_t *paddrp;
d607 2
a608 3
uvm_page_physload(start, end, avail_start, avail_end, free_list)
	paddr_t start, end, avail_start, avail_end;
	int free_list;
d759 1
a759 1
uvm_page_rehash()
d761 1
a761 1
	int freepages, lcv, bucketcount, s, oldcount;
d809 1
a809 2
	s = splvm();
	simple_lock(&uvm.hashlock);
d823 1
a823 2
	simple_unlock(&uvm.hashlock);
	splx(s);
d845 1
a845 1
uvm_page_physdump()
d895 2
a896 6
uvm_pagealloc_strat(obj, off, anon, flags, strat, free_list)
	struct uvm_object *obj;
	voff_t off;
	int flags;
	struct vm_anon *anon;
	int strat, free_list;
d1067 1
a1067 4
uvm_pagerealloc(pg, newobj, newoff)
	struct vm_page *pg;
	struct uvm_object *newobj;
	voff_t newoff;
d1231 1
a1231 3
uvm_page_unbusy(pgs, npgs)
	struct vm_page **pgs;
	int npgs;
d1276 1
a1276 3
uvm_page_own(pg, tag)
	struct vm_page *pg;
	char *tag;
d1310 1
a1310 1
uvm_pageidlezero()
a1482 1
	int s;
d1484 1
a1486 2
	s = splvm();
	simple_lock(&uvm.hashlock);
d1492 1
a1492 2
	simple_unlock(&uvm.hashlock);
	splx(s);
@


1.69
log
@vm_physseg_find and VM_PAGE_TO_PHYS are both called many times in your
average arch port. They are also inline. This does not help, de-inline them.

shaves about 1k on i386 and amd64 bsd.mp. Probably similar amounts of
most architectures.

"no issue" beck@@ "Nuke nuke nuke... make them functions" weingart@@ "this
is good" art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.68 2009/03/23 13:25:11 art Exp $	*/
a73 1
#define UVM_PAGE                /* pull in uvm_page.h functions */
d1497 184
@


1.68
log
@Processor affinity for processes.
 - Split up run queues so that every cpu has one.
 - Make setrunqueue choose the cpu where we want to make this process
   runnable (this should be refined and less brutal in the future).
 - When choosing the cpu where we want to run, make some kind of educated
   guess where it will be best to run (very naive right now).
Other:
 - Set operations for sets of cpus.
 - load average calculations per cpu.
 - sched_is_idle() -> curcpu_is_idle()

tested, debugged and prodded by many@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.67 2008/07/02 15:21:33 art Exp $	*/
d1405 93
@


1.67
log
@Make the pagedaemon a bit happier.
1. When checking if the pagedaemon should be awakened and to see how
   much work it should do, consider the buffer cache deficit
   (how much pages the buffer cache can eat max vs. how much it has
   now) as pages that are not free. They are actually still usable by
   the allocator, but the presure on the pagedaemon is increased when
   we starting to chew into the memory that the buffer cache wants to
   use.
2. Remove the stupid 512kB limit of how much memory should be our
   free target. That maybe made sense on 68k, but on modern systems
   512k is just a joke. Keep it at 3% of physical memory just like
   it was meant to be.
3. When doing allocations for the pagedaemon, always let it use the
   reserve. the whole UVM_OBJ_IS_KERN_OBJECT is silly and doesn't
   work in most cases anyway. We still don't have a reserve for
   the pagedaemon in the km_page allocator, but this seems to help
   enough. (yes, there are still bad cases in that code and the comment
   is only half-true, the whole section needs a massage, but that will
   happen later, this diff only touches pagedaemon parts)

Testing by many, prodded by theo.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.66 2008/04/12 20:37:35 miod Exp $	*/
d1403 1
a1403 1
	} while (sched_is_idle());
@


1.66
log
@Prune the in-use swap encryption keys in uvm_shutdown(), per deraadt@@'s idea.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.65 2008/04/09 16:58:11 deraadt Exp $	*/
d81 1
d933 3
a935 10

#ifdef UBC
	if (uvmexp.free + uvmexp.paging < uvmexp.freemin ||
	    (uvmexp.free + uvmexp.paging < uvmexp.freetarg &&
	     uvmexp.inactive < uvmexp.inactarg)) {
		wakeup(&uvm.pagedaemon);
	}
#else
	if (uvmexp.free < uvmexp.freemin || (uvmexp.free < uvmexp.freetarg &&
	    uvmexp.inactive < uvmexp.inactarg))
a936 1
#endif
d950 3
a952 3
	    (uvmexp.free <= uvmexp.reserve_pagedaemon &&
	     !(use_reserve && (curproc == uvm.pagedaemon_proc ||
				curproc == syncerproc))))
@


1.65
log
@Add new stub uvm_shutdown() and call it from the right place in MD boot()
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.65 2008/04/09 16:50:30 deraadt Exp $	*/
d885 3
@


1.64
log
@Only compile in uvm_page_physdump() if option DDB as it's not directly callable
and supposed to be only used from within ddb.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.63 2007/12/18 11:05:52 thib Exp $	*/
d881 5
@


1.63
log
@Turn the uvm_{lock/unlock}_fpageq() inlines into
macros that just expand into the mutex functions
to keep the abstraction, do assorted cleanup.

ok miod@@,art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.62 2007/11/29 00:26:41 tedu Exp $	*/
d853 1
a853 1
#if 1 /* XXXCDC: TMP TMP TMP DEBUG DEBUG DEBUG */
@


1.62
log
@use a working mutex for the freepage list. ok art deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.61 2007/06/18 21:51:15 pedro Exp $	*/
d908 1
a908 1
	int lcv, try1, try2, s, zeroit = 0;
d917 2
a918 1
	s = uvm_lock_fpageq();
d1036 1
a1036 1
	uvm_unlock_fpageq(s);		/* unlock free page queue */
d1073 1
a1073 1
	uvm_unlock_fpageq(s);
a1126 1
	int s;
d1225 1
a1225 1
	s = uvm_lock_fpageq();
d1240 1
a1240 1
	uvm_unlock_fpageq(s);
d1339 1
a1339 1
	int free_list, s;
d1343 1
a1343 1
		s = uvm_lock_fpageq();
d1347 1
a1347 1
			uvm_unlock_fpageq(s);
d1364 1
a1364 1
			uvm_unlock_fpageq(s);
d1370 1
a1370 1
		uvm_unlock_fpageq(s);
d1380 1
a1380 1
			s = uvm_lock_fpageq();
d1385 1
a1385 1
			uvm_unlock_fpageq(s);
d1397 1
a1397 1
		s = uvm_lock_fpageq();
d1401 1
a1401 1
		uvm_unlock_fpageq(s);
@


1.61
log
@Bring back Mickey's UVM anon change. Testing by thib@@, beck@@ and
ckuethe@@ for a while. Okay beck@@, "it is good timing" deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.60 2007/05/18 14:41:55 art Exp $	*/
d244 1
a244 1
	simple_lock_init(&uvm.fpageqlock);
@


1.60
log
@Instead of checking whichqs directly, add a "sched_is_idle()" macro to
sys/sched.h and use that to check if there's something to do.

kettenis@@ thib@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.59 2007/04/13 18:57:49 art Exp $	*/
d1043 1
a1043 1
		anon->u.an_page = pg;
d1171 1
a1171 1
	} else if (saved_loan_count && (pg->pg_flags & PQ_ANON)) {
d1180 1
d1212 2
a1214 1
	if (pg->uanon) {
d1216 1
a1217 1
#endif
@


1.59
log
@While splitting flags and pqflags might have been a good idea in theory
to separate locking, on most modern machines this is not enough
since operations on short types touch other short types that share the
same word in memory.

Merge pg_flags and pqflags again and now use atomic operations to change
the flags. Also bump wire_count to an int and pg_version might go
int as well, just for alignment.

tested by many, many. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.58 2007/04/11 12:10:42 art Exp $	*/
d1400 1
a1400 1
	} while (whichqs == 0);
@


1.58
log
@Instead of managing pages for intrsafe maps in special objects (aka.
kmem_object) just so that we can remove them, just use pmap_extract
to get the pages to free and simplify a lot of code to not deal with
the list of intrsafe maps, intrsafe objects, etc.

miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.57 2007/04/04 17:44:45 art Exp $	*/
d169 1
a169 1
	pg->pg_flags |= PG_TABLED;
d204 1
a204 1
	pg->pg_flags &= ~PG_TABLED;
d1044 1
a1044 1
		pg->pqflags = PQ_ANON;
a1050 1
		pg->pqflags = 0;
d1062 1
a1062 1
		pg->pg_flags &= ~PG_CLEAN;
d1155 1
d1157 1
a1157 1
			pg->pg_flags &= ~PG_CLEAN;/* in case an anon takes over */
d1171 1
a1171 1
	} else if (saved_loan_count && (pg->pqflags & PQ_ANON)) {
d1179 1
a1179 2

		pg->pqflags &= ~PQ_ANON;
d1189 1
a1189 1
	if (pg->pqflags & PQ_ACTIVE) {
d1191 1
a1191 1
		pg->pqflags &= ~PQ_ACTIVE;
d1194 2
a1195 2
	if (pg->pqflags & PQ_INACTIVE) {
		if (pg->pqflags & PQ_SWAPBACKED)
d1199 1
a1199 1
		pg->pqflags &= ~PQ_INACTIVE;
d1221 1
a1221 1
	pg->pg_flags &= ~PG_ZERO;
d1226 2
a1227 1
	pg->pqflags = PQ_FREE;
d1274 1
a1274 1
				pg->pg_flags &= ~(PG_BUSY);
d1280 1
a1280 1
			pg->pg_flags &= ~(PG_WANTED|PG_BUSY);
d1393 1
a1393 1
		pg->pg_flags |= PG_ZERO;
@


1.57
log
@Mechanically rename the "flags" and "version" fields in struct vm_page
to "pg_flags" and "pg_version", so that they are a bit easier to work with.
Whoever uses generic names like this for a popular struct obviously doesn't
read much code.

Most architectures compile and there are no functionality changes.

deraadt@@ ok ("if something fails to compile, we fix that by hand")
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.56 2006/07/31 11:51:29 mickey Exp $	*/
d1125 1
a1125 2
uvm_pagefree(pg)
	struct vm_page *pg;
@


1.56
log
@fix uvmhist #2: args are always u_long so fix missing %d and %x and no %ll; no change for normal code
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.55 2006/07/26 23:15:55 mickey Exp $	*/
d154 1
a154 2
uvm_pageinsert(pg)
	struct vm_page *pg;
d160 1
a160 1
	KASSERT((pg->flags & PG_TABLED) == 0);
d169 1
a169 1
	pg->flags |= PG_TABLED;
d181 1
a181 2
uvm_pageremove(pg)
	struct vm_page *pg;
d187 1
a187 1
	KASSERT(pg->flags & PG_TABLED);
d204 1
a204 1
	pg->flags &= ~PG_TABLED;
d207 1
a207 1
	pg->version++;
d1018 1
a1018 1
	if (pg->flags & PG_ZERO)
d1026 1
a1026 1
		if (pg->flags & PG_ZERO) {
d1040 2
a1041 2
	pg->flags = PG_BUSY|PG_CLEAN|PG_FAKE;
	pg->version++;
d1063 1
a1063 1
		pg->flags &= ~PG_CLEAN;
d1108 1
a1108 1
		pg->version++;
d1147 1
a1147 1
	if (pg->flags & PG_TABLED) {
d1158 1
a1158 1
			pg->flags &= ~PG_CLEAN;	/* in case an anon takes over */
a1172 1

d1223 1
a1223 1
	pg->flags &= ~PG_ZERO;
d1266 1
a1266 1
		if (pg->flags & PG_WANTED) {
d1269 1
a1269 1
		if (pg->flags & PG_RELEASED) {
d1275 1
a1275 1
				pg->flags &= ~(PG_BUSY);
d1281 1
a1281 1
			pg->flags &= ~(PG_WANTED|PG_BUSY);
d1394 1
a1394 1
		pg->flags |= PG_ZERO;
@


1.55
log
@fix fmts for UVMHIST_LOG() entries making it more useful on 64bit archs; miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.54 2006/07/13 22:51:26 deraadt Exp $	*/
d971 3
a973 3
	UVMHIST_LOG(pghist, "obj=%p off=%llx anon=%p flags=%x",
	    obj, off, anon, flags);
	UVMHIST_LOG(pghist, "strat=%d free_list=%d", strat, free_list, 0, 0);
d1070 2
a1071 2
	UVMHIST_LOG(pghist, "allocated pg %p/%llx", pg,
	    (long long)VM_PAGE_TO_PHYS(pg), 0, 0);
d1141 2
a1142 2
	UVMHIST_LOG(pghist, "freeing pg %p/%llx", pg,
	    (long long)VM_PAGE_TO_PHYS(pg), 0, 0);
@


1.54
log
@Back out the anon change.  Apparently it was tested by a few, but most of
us did not see it or get a chance to test it before it was commited. It
broke cvs, in the ami driver, making it not succeed at seeing it's devices.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.52 2006/04/27 15:21:19 mickey Exp $	*/
d971 2
a972 2
	UVMHIST_LOG(pghist, "obj=%p off=%llx anon=%x flags=%x",
	    obj, off, flags, anon);
@


1.53
log
@from netbsd: make anons dynamically allocated from pool.
this results in lesse kva waste due to static preallocation of those
for every phys page and also every swap page.
tested by beck krw miod
@
text
@d1045 1
a1045 1
		anon->an_page = pg;
d1174 1
a1174 1
	} else if (saved_loan_count && pg->uanon) {
a1184 1
		pg->uanon->an_page = NULL;
d1216 1
a1217 2
		pg->uanon->an_page = NULL;
#ifdef UBC
d1219 1
a1220 1
	}
@


1.52
log
@from PAE work:
as freepages being vconverted back to byte address make sure to
perform calculations in (upcoming) larger paddr_t to avoid losing
higher bits in calculation.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.51 2006/01/16 13:11:05 mickey Exp $	*/
d1045 1
a1045 1
		anon->u.an_page = pg;
d1174 1
a1174 1
	} else if (saved_loan_count && (pg->pqflags & PQ_ANON)) {
d1185 1
d1217 2
a1219 1
	if (pg->uanon) {
d1221 1
a1222 1
#endif
@


1.51
log
@add another uvm histroy for physpage alloc/free and propagate a debugging pgfree check into pglist; no functional change for normal kernels; make histories uncommon
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.50 2004/12/26 21:22:14 miod Exp $	*/
d296 1
a296 1
	pagecount = ((freepages + 1) << PAGE_SHIFT) /
@


1.50
log
@Use list and queue macros where applicable to make the code easier to read;
no change in compiler assembly output.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.49 2004/02/23 06:19:32 drahn Exp $	*/
d129 5
d159 1
d187 1
d226 7
d506 1
d600 1
d915 1
d971 3
d1070 2
d1076 1
d1092 3
d1132 1
d1141 3
d1342 1
@


1.49
log
@sync of pmap_update() calls with NetBSD. pmap_update is defined away on
all architectures but arm, where it is needed.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.48 2003/06/01 00:26:09 miod Exp $	*/
d815 1
a815 1
		while ((pg = oldbuckets[lcv].tqh_first) != NULL) {
@


1.48
log
@Typo in panic message.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.47 2003/03/29 01:13:57 mickey Exp $	*/
d467 1
@


1.47
log
@ubchist is not a fully cooked kadaver and though use the other well formed pdhist one until ubc gaets back. art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.46 2002/10/12 01:09:45 krw Exp $	*/
d517 1
a517 1
				    panic("vum_page_physget: out of memory!");
@


1.46
log
@Remove more '\n's from panic() statements. Both trailing and leading.

Diff generated by Chris Kuethe.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.45 2002/09/12 12:56:16 art Exp $	*/
d1230 1
a1230 1
	UVMHIST_FUNC("uvm_page_unbusy"); UVMHIST_CALLED(ubchist);
d1235 1
a1235 1
		if (pg == NULL) {
d1242 1
a1242 1
			UVMHIST_LOG(ubchist, "releasing pg %p", pg,0,0,0);
d1252 1
a1252 1
			UVMHIST_LOG(ubchist, "unbusying pg %p", pg,0,0,0);
@


1.45
log
@Change the PMAP_PAGEIDLEZERO api to take the struct vm_page instead of the pa.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.44 2002/09/10 18:29:44 art Exp $	*/
d616 1
a616 1
		panic("uvm_page_physload: bad free list %d\n", free_list);
d1109 1
a1109 1
		panic("uvm_pagefree: freeing free page %p\n", pg);
@


1.44
log
@Change the pmap_zero_page and pmap_copy_page API to take the struct vm_page *
instead of the pa. Most callers already had it handy and those who didn't
only called it for managed pages and were outside time-critical code.

This will allow us to make those functions clean and fast on sparc and
sparc64 letting us to avoid unnecessary cache flushes.

deraadt@@ miod@@ drahn@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.43 2002/06/11 09:45:16 art Exp $	*/
d1343 1
a1343 1
		if (PMAP_PAGEIDLEZERO(VM_PAGE_TO_PHYS(pg)) == FALSE) {
@


1.43
log
@Allow MD code to define __HAVE_VM_PAGE_MD to add own members into struct vm_page.
From NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.42 2002/03/14 01:27:18 millert Exp $	*/
d1046 1
a1046 1
			pmap_zero_page(VM_PAGE_TO_PHYS(pg));
d1363 1
a1363 1
		pmap_zero_page(VM_PAGE_TO_PHYS(pg));
@


1.42
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.41 2002/01/28 11:53:48 art Exp $	*/
d311 3
@


1.41
log
@allocate vm pages with uvm_km_alloc (this code is ifdefed out anyway).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.40 2002/01/02 22:23:25 miod Exp $	*/
d132 2
a133 2
static void uvm_pageinsert __P((struct vm_page *));
static void uvm_pageremove __P((struct vm_page *));
d480 1
a480 1
static boolean_t uvm_page_physget_freelist __P((paddr_t *, int));
d837 1
a837 1
void uvm_page_physdump __P((void)); /* SHUT UP GCC */
@


1.40
log
@Back out a few more uvm changes, especially wrt swap usage.
This unbreaks m68k m88k sparc and perhaps others, which eventually froze
when hitting swap.
Tested by various people on various platforms.
ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.30 2001/11/10 18:42:31 art Exp $	*/
d650 2
a651 2
		pgs = malloc(sizeof(struct vm_page) * npages,
		    M_VMPAGE, M_NOWAIT);
@


1.39
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.c,v 1.31 2001/11/12 01:26:09 art Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.51 2001/03/09 01:02:12 chs Exp $	*/
d190 3
a192 4
	if (UVM_OBJ_IS_VTEXT(pg->uobject)) {
		uvmexp.vtextpages--;
	} else if (UVM_OBJ_IS_VNODE(pg->uobject)) {
		uvmexp.vnodepages--;
d194 1
a897 4

	LOCK_ASSERT(obj == NULL || simple_lock_held(&obj->vmobjlock));
	LOCK_ASSERT(anon == NULL || simple_lock_held(&anon->an_lock));

d1023 3
a1025 1
		uvmexp.anonpages++;
d1182 1
a1182 1

d1184 1
a1184 1
		uvmexp.anonpages--;
d1186 1
a1249 2
			KASSERT(pg->wire_count ||
				(pg->pqflags & (PQ_ACTIVE|PQ_INACTIVE)));
@


1.38
log
@Keep track of how many pages a vnode hold with vhold and vholdrele
so that we can get back the old behavior where a vnode with cached data
is less likely to be recycled than a vnode without cached data.

XXX - This is a brute-force solution - we do it where uvmexp.vnodepages
 are changed, I am not really sure it is correct but people have been
 very happy with the diff so far and want this in the tree.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.c,v 1.37 2001/12/04 23:22:42 art Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.66 2001/09/10 21:19:43 chris Exp $	*/
d4 1
a4 1
/*
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and
d48 1
a48 1
 *
d54 3
a56 3
 *
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
d58 1
a58 1
 *
a128 10
 * we allocate an initial number of page colors in uvm_page_init(),
 * and remember them.  We may re-color pages as cache sizes are
 * discovered during the autoconfiguration phase.  But we can never
 * free the initial set of buckets, since they are allocated using
 * uvm_pageboot_alloc().
 */

static boolean_t have_recolored_pages /* = FALSE */;

/*
d190 4
a193 8
	if (UVM_OBJ_IS_VTEXT(pg->uobject) || UVM_OBJ_IS_VNODE(pg->uobject)) {
		if (UVM_OBJ_IS_VNODE(pg->uobject))
			uvmexp.vnodepages--;
		else
			uvmexp.vtextpages--;
		s = splbio();
		vholdrele((struct vnode *)pg->uobject);
		splx(s);
a204 13
static void
uvm_page_init_buckets(struct pgfreelist *pgfl)
{
	int color, i;

	for (color = 0; color < uvmexp.ncolors; color++) {
		for (i = 0; i < PGFL_NQUEUES; i++) {
			TAILQ_INIT(&pgfl->pgfl_buckets[
			    color].pgfl_queues[i]);
		}
	}
}

d207 1
a207 1
 *
d215 3
a217 4
	vsize_t freepages, pagecount, bucketcount, n;
	struct pgflbucket *bucketarray;
	struct vm_page *pagearray;
	int lcv, i;
d221 1
a221 3
	 * init the page queues and page queue locks, except the free
	 * list; we allocate that later (with the initial vm_page
	 * structures).
d224 4
d229 2
a230 1
	TAILQ_INIT(&uvm.page_inactive);
d246 1
a246 1
	/*
d259 1
a259 1

d261 1
a261 1
	 * first calculate the number of free pages...
d267 1
a267 1

a272 8
	 * Let MD code initialize the number of colors, or default
	 * to 1 color if MD code doesn't care.
	 */
	if (uvmexp.ncolors == 0)
		uvmexp.ncolors = 1;
	uvmexp.colormask = uvmexp.ncolors - 1;

	/*
d281 1
a281 2

	bucketcount = uvmexp.ncolors * VM_NFREELIST;
d284 2
a285 12

	bucketarray = (void *) uvm_pageboot_alloc((bucketcount *
	    sizeof(struct pgflbucket)) + (pagecount *
	    sizeof(struct vm_page)));
	pagearray = (struct vm_page *)(bucketarray + bucketcount);

	for (lcv = 0; lcv < VM_NFREELIST; lcv++) {
		uvm.page_free[lcv].pgfl_buckets =
		    (bucketarray + (lcv * uvmexp.ncolors));
		uvm_page_init_buckets(&uvm.page_free[lcv]);
	}

d287 1
a287 1

a310 3
#ifdef __HAVE_VM_PAGE_MD
			VM_MDPAGE_INIT(&vm_physmem[lcv].pgs[i]);
#endif
d364 1
a364 1
 *
d366 1
a366 1
 */
d389 16
d406 1
a406 3
	vaddr_t addr;
#if !defined(PMAP_STEAL_MEMORY)
	vaddr_t vaddr;
d408 3
a410 1
#endif
a424 18
	/* round to page size */
	size = round_page(size);

#if defined(PMAP_STEAL_MEMORY)

	/*
	 * defer bootstrap allocation to MD code (it may want to allocate
	 * from a direct-mapped segment).  pmap_steal_memory should adjust
	 * virtual_space_start/virtual_space_end if necessary.
	 */

	addr = pmap_steal_memory(size, &virtual_space_start,
	    &virtual_space_end);

	return(addr);

#else /* !PMAP_STEAL_MEMORY */

a463 1
	pmap_update(pmap_kernel());
a833 70
/*
 * uvm_page_recolor: Recolor the pages if the new bucket count is
 * larger than the old one.
 */

void
uvm_page_recolor(int newncolors)
{
	struct pgflbucket *bucketarray, *oldbucketarray;
	struct pgfreelist pgfl;
	struct vm_page *pg;
	vsize_t bucketcount;
	int s, lcv, color, i, ocolors;

	if (newncolors <= uvmexp.ncolors)
		return;

	bucketcount = newncolors * VM_NFREELIST;
	bucketarray = malloc(bucketcount * sizeof(struct pgflbucket),
	    M_VMPAGE, M_NOWAIT);
	if (bucketarray == NULL) {
		printf("WARNING: unable to allocate %ld page color buckets\n",
		    (long) bucketcount);
		return;
	}

	s = uvm_lock_fpageq();

	/* Make sure we should still do this. */
	if (newncolors <= uvmexp.ncolors) {
		uvm_unlock_fpageq(s);
		free(bucketarray, M_VMPAGE);
		return;
	}

	oldbucketarray = uvm.page_free[0].pgfl_buckets;
	ocolors = uvmexp.ncolors;

	uvmexp.ncolors = newncolors;
	uvmexp.colormask = uvmexp.ncolors - 1;

	for (lcv = 0; lcv < VM_NFREELIST; lcv++) {
		pgfl.pgfl_buckets = (bucketarray + (lcv * newncolors));
		uvm_page_init_buckets(&pgfl);
		for (color = 0; color < ocolors; color++) {
			for (i = 0; i < PGFL_NQUEUES; i++) {
				while ((pg = TAILQ_FIRST(&uvm.page_free[
				    lcv].pgfl_buckets[color].pgfl_queues[i]))
				    != NULL) {
					TAILQ_REMOVE(&uvm.page_free[
					    lcv].pgfl_buckets[
					    color].pgfl_queues[i], pg, pageq);
					TAILQ_INSERT_TAIL(&pgfl.pgfl_buckets[
					    VM_PGCOLOR_BUCKET(pg)].pgfl_queues[
					    i], pg, pageq);
				}
			}
		}
		uvm.page_free[lcv].pgfl_buckets = pgfl.pgfl_buckets;
	}

	if (have_recolored_pages) {
		uvm_unlock_fpageq(s);
		free(oldbucketarray, M_VMPAGE);
		return;
	}

	have_recolored_pages = TRUE;
	uvm_unlock_fpageq(s);
}
a864 43
 * uvm_pagealloc_pgfl: helper routine for uvm_pagealloc_strat
 */

static __inline struct vm_page *
uvm_pagealloc_pgfl(struct pgfreelist *pgfl, int try1, int try2,
    unsigned int *trycolorp)
{
	struct pglist *freeq;
	struct vm_page *pg;
	int color, trycolor = *trycolorp;

	color = trycolor;
	do {
		if ((pg = TAILQ_FIRST((freeq =
		    &pgfl->pgfl_buckets[color].pgfl_queues[try1]))) != NULL)
			goto gotit;
		if ((pg = TAILQ_FIRST((freeq =
		    &pgfl->pgfl_buckets[color].pgfl_queues[try2]))) != NULL)
			goto gotit;
		color = (color + 1) & uvmexp.colormask;
	} while (color != trycolor);

	return (NULL);

 gotit:
	TAILQ_REMOVE(freeq, pg, pageq);
	uvmexp.free--;

	/* update zero'd page count */
	if (pg->flags & PG_ZERO)
		uvmexp.zeropages--;

	if (color == trycolor)
		uvmexp.colorhit++;
	else {
		uvmexp.colormiss++;
		*trycolorp = color;
	}

	return (pg);
}

/*
d890 1
a890 1
	int lcv, try1, try2, s, zeroit = 0, color;
d892 2
a904 9
	 * This implements a global round-robin page coloring
	 * algorithm.
	 *
	 * XXXJRT: Should we make the `nextcolor' per-cpu?
	 * XXXJRT: What about virtually-indexed caches?
	 */
	color = uvm.page_free_nextcolor;

	/*
d909 11
a919 1
	UVM_KICK_PDAEMON();
d959 5
a963 3
			pg = uvm_pagealloc_pgfl(&uvm.page_free[lcv],
			    try1, try2, &color);
			if (pg != NULL)
d974 5
a978 3
		pg = uvm_pagealloc_pgfl(&uvm.page_free[free_list],
		    try1, try2, &color);
		if (pg != NULL)
d996 6
a1001 5
	/*
	 * We now know which color we actually allocated from; set
	 * the next color accordingly.
	 */
	uvm.page_free_nextcolor = (color + 1) & uvmexp.colormask;
d1123 2
a1124 2
		 * it knows it needs to allocate swap if it wants to page the
		 * page out.
d1130 1
a1130 1

d1135 2
a1136 2
		 * changed the loan_count [e.g. in uvmfault_anonget()]) we just
		 * return (when the last loan is dropped, then the page can be
d1140 1
a1140 1
		if (saved_loan_count)
d1166 6
a1171 2
	} else if (pg->pqflags & PQ_INACTIVE) {
		TAILQ_REMOVE(&uvm.page_inactive, pg, pageq);
d1197 1
a1197 2
	    uvm_page_lookup_freelist(pg)].pgfl_buckets[
	    VM_PGCOLOR_BUCKET(pg)].pgfl_queues[PGFL_UNKNOWN], pg, pageq);
d1301 1
a1301 2
 * => try to complete one color bucket at a time, to reduce our impact
 *	on the CPU cache.
d1310 1
a1310 2
	int free_list, s, firstbucket;
	static int nextbucket;
d1312 2
a1313 1
	s = uvm_lock_fpageq();
d1315 2
a1316 3
	firstbucket = nextbucket;
	do {
		if (whichqs != 0) {
d1321 12
a1332 1
		if (uvmexp.zeropages >= UVM_PAGEZERO_TARGET) {
d1338 4
a1341 14
		for (free_list = 0; free_list < VM_NFREELIST; free_list++) {
			pgfl = &uvm.page_free[free_list];
			while ((pg = TAILQ_FIRST(&pgfl->pgfl_buckets[
			    nextbucket].pgfl_queues[PGFL_UNKNOWN])) != NULL) {
				if (whichqs != 0) {
					uvm_unlock_fpageq(s);
					return;
				}

				TAILQ_REMOVE(&pgfl->pgfl_buckets[
				    nextbucket].pgfl_queues[PGFL_UNKNOWN],
				    pg, pageq);
				uvmexp.free--;
				uvm_unlock_fpageq(s);
d1343 15
a1357 17
				if (PMAP_PAGEIDLEZERO(VM_PAGE_TO_PHYS(pg)) ==
				    FALSE) {
					/*
					 * The machine-dependent code detected
					 * some reason for us to abort zeroing
					 * pages, probably because there is a
					 * process now ready to run.
					 */
					s = uvm_lock_fpageq();
					TAILQ_INSERT_HEAD(&pgfl->pgfl_buckets[
					    nextbucket].pgfl_queues[
					    PGFL_UNKNOWN], pg, pageq);
					uvmexp.free++;
					uvmexp.zeroaborts++;
					uvm_unlock_fpageq(s);
					return;
				}
d1359 7
a1365 12
				pmap_zero_page(VM_PAGE_TO_PHYS(pg));
#endif /* PMAP_PAGEIDLEZERO */
				pg->flags |= PG_ZERO;

				s = uvm_lock_fpageq();
				TAILQ_INSERT_HEAD(&pgfl->pgfl_buckets[
				    nextbucket].pgfl_queues[PGFL_ZEROS],
				    pg, pageq);
				uvmexp.free++;
				uvmexp.zeropages++;
			}
		}
d1367 6
a1372 4
		nextbucket = (nextbucket + 1) & uvmexp.colormask;
	} while (nextbucket != firstbucket);

	uvm_unlock_fpageq(s);
@


1.38.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.41 2002/01/28 11:53:48 art Exp $	*/
d700 2
a701 2
		pgs = (vm_page *)uvm_km_alloc(kernel_map,
		    sizeof(struct vm_page) * npages);
d1059 4
d1182 1
a1182 3
#ifdef UBC
		uvm_pgcnt_anon++;
#endif
d1335 1
a1335 1
#ifdef UBC
d1337 1
a1337 1
		uvm_pgcnt_anon--;
a1338 1
#endif
d1403 2
@


1.38.2.2
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.c,v 1.38.2.1 2002/01/31 22:55:51 niklas Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.71 2001/11/10 07:37:00 lukem Exp $	*/
a80 1
#include <sys/proc.h>
a81 1
#define UVM_PAGE                /* pull in uvm_page.h functions */
d100 1
d163 1
a163 1
	struct uvm_object *uobj = pg->uobject;
d166 2
a167 1
	buck = &uvm.page_hash[uvm_pagehash(uobj, pg->offset)];
d169 1
a169 1
	TAILQ_INSERT_TAIL(buck, pg, hashq);
d171 1
d173 1
a173 1
	TAILQ_INSERT_TAIL(&uobj->memq, pg, listq);
d175 1
a175 1
	uobj->uo_npages++;
a189 1
	struct uvm_object *uobj = pg->uobject;
d193 2
a194 1
	buck = &uvm.page_hash[uvm_pagehash(uobj ,pg->offset)];
d198 1
d200 2
a201 2
	if (UVM_OBJ_IS_VTEXT(uobj) || UVM_OBJ_IS_VNODE(uobj)) {
		if (UVM_OBJ_IS_VNODE(uobj))
d206 1
a206 1
		vholdrele((struct vnode *)uobj);
d211 2
a212 2
	uobj->uo_npages--;
	TAILQ_REMOVE(&uobj->memq, pg, listq);
d214 1
d216 1
d319 1
a319 1
	bucketarray = (void *)uvm_pageboot_alloc((bucketcount *
d329 1
d338 6
d384 1
a384 1
	 * init various thresholds.
d396 1
a396 1
	/*
d661 1
d664 1
a670 1

a683 1

a692 1

d722 2
d726 1
d734 1
d737 1
d739 1
d752 1
d754 1
d768 1
d770 1
d772 1
d788 4
d794 2
d805 1
a805 1
	int freepages, lcv, bucketcount, oldcount;
d853 1
d869 1
d877 5
d955 29
d990 1
a990 1
    int *trycolorp)
a1058 3
	LOCK_ASSERT(obj == NULL || simple_lock_held(&obj->vmobjlock));
	LOCK_ASSERT(anon == NULL || simple_lock_held(&anon->an_lock));

a1067 1

a1151 1

a1157 1

d1167 2
a1168 1
	uvm_unlock_fpageq(s);
d1174 1
d1178 3
a1180 1
		uvmexp.anonpages++;
d1182 1
a1182 1
		if (obj) {
a1183 1
		}
d1235 1
d1240 1
d1256 1
a1256 8

	KASSERT((pg->flags & PG_PAGEOUT) == 0);
	LOCK_ASSERT(simple_lock_held(&uvm.pageqlock) ||
		    (pg->pqflags & (PQ_ACTIVE|PQ_INACTIVE)) == 0);
	LOCK_ASSERT(pg->uobject == NULL ||
		    simple_lock_held(&pg->uobject->vmobjlock));
	LOCK_ASSERT(pg->uobject != NULL || pg->uanon == NULL ||
		    simple_lock_held(&pg->uanon->an_lock));
d1266 2
a1267 1
	 * if the page is loaned, resolve the loan instead of freeing.
d1270 1
a1270 2
	if (pg->loan_count) {
		KASSERT(pg->wire_count == 0);
d1273 5
a1277 8
		 * if the page is owned by an anon then we just want to
		 * drop anon ownership.  the kernel will free the page when
		 * it is done with it.  if the page is owned by an object,
		 * remove it from the object and mark it dirty for the benefit
		 * of possible anon owners.
		 *
		 * regardless of previous ownership, wakeup any waiters,
		 * unbusy the page, and we're done.
d1280 27
a1306 14
		if (pg->pqflags & PQ_ANON) {
			pg->pqflags &= ~PQ_ANON;
			pg->uanon = NULL;
		} else if (pg->flags & PG_TABLED) {
			uvm_pageremove(pg);
			pg->flags &= ~PG_CLEAN;
		}
		if (pg->flags & PG_WANTED) {
			wakeup(pg);
		}
		pg->flags &= ~(PG_WANTED|PG_BUSY);
#ifdef UVM_PAGE_TRKOWN
		pg->owner_tag = NULL;
#endif
d1309 1
d1312 1
a1312 2
	 * remove page from its object or anon.
	 * adjust swpgonly if the page is swap-backed.
d1315 8
a1322 4
	if (pg->flags & PG_TABLED) {
		uvm_pageremove(pg);
	} else if (pg->pqflags & PQ_ANON) {
		pg->uanon->u.an_page = NULL;
a1325 6
	 * now remove the page from the queues.
	 */

	uvm_pagedequeue(pg);

	/*
d1333 3
a1335 2
	if (pg->pqflags & PQ_ANON) {
		uvmexp.anonpages--;
d1337 1
d1368 1
a1368 1
 * => if pages are anon-owned, anons must be locked.
d1377 1
d1383 1
d1392 8
a1399 2
			pg->flags &= ~PG_RELEASED;
			uvm_pagefree(pg);
a1422 2
	KASSERT((pg->flags & (PG_PAGEOUT|PG_RELEASED)) == 0);

a1442 6
	KASSERT((pg->pqflags & (PQ_ACTIVE|PQ_INACTIVE)) ||
		(pg->uanon == NULL && pg->uobject == NULL) ||
		pg->uobject == uvm.kernel_object ||
		pg->wire_count > 0 ||
		(pg->loan_count == 1 && pg->uanon == NULL) ||
		pg->loan_count > 1);
d1464 1
d1471 1
d1477 1
d1493 2
a1494 2
				if (!PMAP_PAGEIDLEZERO(VM_PAGE_TO_PHYS(pg))) {

a1500 1

d1523 1
d1526 1
@


1.38.2.3
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.38.2.2 2002/02/02 03:28:27 art Exp $	*/
d143 2
a144 2
static void uvm_pageinsert(struct vm_page *);
static void uvm_pageremove(struct vm_page *);
d519 1
a519 1
static boolean_t uvm_page_physget_freelist(paddr_t *, int);
@


1.38.2.4
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.38.2.3 2002/06/11 03:33:04 art Exp $	*/
d651 1
a651 2
		panic("uvm_page_physload: bad free list %d", free_list);

d1138 1
a1138 1
			pmap_zero_page(pg);
d1206 1
a1206 1
		panic("uvm_pagefree: freeing free page %p", pg);
d1425 1
a1425 1
				if (!PMAP_PAGEIDLEZERO(pg)) {
d1444 1
a1444 1
				pmap_zero_page(pg);
@


1.38.2.5
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.c,v 1.38.2.4 2002/10/29 00:36:50 art Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.80 2002/10/30 02:48:28 simonb Exp $	*/
a171 4
	if (UVM_OBJ_IS_AOBJ(uobj)) {
		uvmexp.anonpages++;
	}

d193 1
a193 1
	buck = &uvm.page_hash[uvm_pagehash(uobj, pg->offset)];
d200 1
a200 1
			uvmexp.filepages--;
d202 1
a202 1
			uvmexp.execpages--;
a205 2
	} else if (UVM_OBJ_IS_AOBJ(uobj)) {
		uvmexp.anonpages--;
d379 2
a380 5
	uvmexp.fileminpct = 10;
	uvmexp.execminpct = 5;
	uvmexp.anonmaxpct = 80;
	uvmexp.filemaxpct = 50;
	uvmexp.execmaxpct = 30;
d382 2
a383 5
	uvmexp.filemin = uvmexp.fileminpct * 256 / 100;
	uvmexp.execmin = uvmexp.execminpct * 256 / 100;
	uvmexp.anonmax = uvmexp.anonmaxpct * 256 / 100;
	uvmexp.filemax = uvmexp.filemaxpct * 256 / 100;
	uvmexp.execmax = uvmexp.execmaxpct * 256 / 100;
a867 5
	if (uvm.page_init_done == FALSE) {
		uvmexp.ncolors = newncolors;
		return;
	}

d1229 4
a1232 1
		if (pg->uobject != NULL) {
a1234 7
		} else if (pg->uanon != NULL) {
			if ((pg->pqflags & PQ_ANON) == 0) {
				pg->loan_count--;
			} else {
				pg->pqflags &= ~PQ_ANON;
			}
			pg->uanon = NULL;
d1239 1
a1239 1
		pg->flags &= ~(PG_WANTED|PG_BUSY|PG_RELEASED);
d1243 1
a1243 4
		if (pg->loan_count) {
			uvm_pagedequeue(pg);
			return;
		}
d1248 1
d1251 1
a1251 1
	if (pg->uobject != NULL) {
d1253 1
a1253 1
	} else if (pg->uanon != NULL) {
a1254 1
		uvmexp.anonpages--;
d1271 3
a1304 1
 * => caller must lock page queues if pages may be released.
d1358 1
a1358 1
			    pg->owner, pg->owner_tag);
d1372 1
d1374 6
a1379 6
	    (pg->uanon == NULL && pg->uobject == NULL) ||
	    pg->uobject == uvm.kernel_object ||
	    pg->wire_count > 0 ||
	    (pg->loan_count == 1 && pg->uanon == NULL) ||
	    pg->loan_count > 1);
	pg->owner_tag = NULL;
@


1.38.2.6
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d1336 1
a1336 1
	UVMHIST_FUNC("uvm_page_unbusy"); UVMHIST_CALLED(pdhist);
d1340 1
a1340 1
		if (pg == NULL || pg == PGO_DONTCARE) {
d1351 1
a1351 1
			UVMHIST_LOG(pdhist, "unbusying pg %p", pg,0,0,0);
@


1.38.2.7
log
@add VEXECMAP.  also make sure to modify filepages count only in the not
execpages case in uvm_pageremove().
this actually appears to solve the swap freak out problems.  sitting on it for
a long time, never checked if it worked.  sigh.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.38.2.6 2003/05/19 22:41:30 tedu Exp $	*/
d172 1
a172 5
	if (UVM_OBJ_IS_VTEXT(uobj)) {
		uvmexp.execpages++;
	} else if (UVM_OBJ_IS_VNODE(uobj)) {
		uvmexp.filepages++;
	} else if (UVM_OBJ_IS_AOBJ(uobj)) {
d203 3
a205 1
		if (UVM_OBJ_IS_VTEXT(uobj))
a206 2
		else
			uvmexp.filepages--;
@


1.37
log
@Yet another sync to NetBSD uvm.
Today we add a pmap argument to pmap_update() and allocate map entries for
kernel_map from kmem_map instead of using the static entries. This should
get rid of MAX_KMAPENT panics. Also some uvm_loan problems are fixed.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.36 2001/11/30 17:24:19 art Exp $	*/
d200 8
a207 4
	if (UVM_OBJ_IS_VTEXT(pg->uobject)) {
		uvmexp.vtextpages--;
	} else if (UVM_OBJ_IS_VNODE(pg->uobject)) {
		uvmexp.vnodepages--;
@


1.36
log
@Kill uvm_pagealloc_contig. The two drivers that still used it should have
been converted to bus_dma ages ago, but since noone haven't bothered to do that
I haven't bothered to do more than to test that the kernel still builds
with those changes.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.c,v 1.35 2001/11/28 19:28:15 art Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.65 2001/06/27 23:57:16 thorpej Exp $	*/
d509 1
a509 1
	pmap_update();
@


1.35
log
@Sync in more uvm from NetBSD. Mostly just cosmetic stuff.
Contains also support for page coloring.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.34 2001/11/28 14:29:13 art Exp $	*/
a1203 43
}

/* 
 * uvm_pagealloc_contig: allocate contiguous memory. 
 *
 * XXX - fix comment.
 */

vaddr_t
uvm_pagealloc_contig(size, low, high, alignment)
	vaddr_t size;
	vaddr_t low, high;
	vaddr_t alignment;
{
	struct pglist pglist; 
	struct vm_page *pg;
	vaddr_t addr, temp_addr;

	size = round_page(size);

	TAILQ_INIT(&pglist);
	if (uvm_pglistalloc(size, low, high, alignment, 0,
			    &pglist, 1, FALSE))
		return 0;
	addr = vm_map_min(kernel_map);
	if (uvm_map(kernel_map, &addr, size, NULL, UVM_UNKNOWN_OFFSET, 0,
		    UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL, UVM_INH_NONE,
				UVM_ADV_RANDOM, 0))) {
		uvm_pglistfree(&pglist);
		return 0;
	}
	temp_addr = addr;
	for (pg = TAILQ_FIRST(&pglist); pg != NULL; 
	     pg = TAILQ_NEXT(pg, pageq)) {
	        pg->uobject = uvm.kernel_object;
		pg->offset = temp_addr - vm_map_min(kernel_map);
		uvm_pageinsert(pg);
		uvm_pagewire(pg);
		pmap_kenter_pa(temp_addr, VM_PAGE_TO_PHYS(pg), 
			       VM_PROT_READ|VM_PROT_WRITE);
		temp_addr += PAGE_SIZE;
	}
	return addr;
@


1.34
log
@more sync to netbsd. some bugfixes in uvm_km_kmemalloc, lots of fixes in uvm_loan.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.c,v 1.33 2001/11/28 13:47:40 art Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.52 2001/04/22 17:22:58 thorpej Exp $	*/
d4 1
a4 1
/* 
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and 
d48 1
a48 1
 * 
d54 3
a56 3
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
d58 1
a58 1
 * 
d129 10
d215 13
d230 1
a230 1
 * 
d238 4
a241 3
	vsize_t freepages, pagecount, n;
	vm_page_t pagearray;
	int lcv, i;  
d245 3
a247 1
	 * init the page queues and page queue locks
a249 4
	for (lcv = 0; lcv < VM_NFREELIST; lcv++) {
		for (i = 0; i < PGFL_NQUEUES; i++)
			TAILQ_INIT(&uvm.page_free[lcv].pgfl_queues[i]);
	}
d251 1
a251 2
	TAILQ_INIT(&uvm.page_inactive_swp);
	TAILQ_INIT(&uvm.page_inactive_obj);
d267 1
a267 1
	/* 
d280 1
a280 1
	
d282 1
a282 1
	 * first calculate the number of free pages...  
d288 1
a288 1
	 
d294 8
d310 2
a311 1
	 
d314 12
a325 2
	pagearray = (vm_page_t)uvm_pageboot_alloc(pagecount *
	    sizeof(struct vm_page));
d327 1
a327 1
					 
d351 3
d407 1
a407 1
 * 
d409 1
a409 1
 */   
d457 2
a458 2
	/* 
	 * defer bootstrap allocation to MD code (it may want to allocate 
d509 1
d880 70
d981 43
d1049 1
a1049 1
	int lcv, try1, try2, s, zeroit = 0;
a1050 2
	struct pglist *freeq;
	struct pgfreelist *pgfl;
d1062 9
d1075 1
a1075 5
	if (uvmexp.free + uvmexp.paging < uvmexp.freemin ||
	    (uvmexp.free + uvmexp.paging < uvmexp.freetarg &&
	     uvmexp.inactive < uvmexp.inactarg)) {
		wakeup(&uvm.pagedaemon);
	}
d1115 3
a1117 5
			pgfl = &uvm.page_free[lcv];
			if ((pg = TAILQ_FIRST((freeq =
			      &pgfl->pgfl_queues[try1]))) != NULL ||
			    (pg = TAILQ_FIRST((freeq =
			      &pgfl->pgfl_queues[try2]))) != NULL)
d1128 3
a1130 5
		pgfl = &uvm.page_free[free_list];
		if ((pg = TAILQ_FIRST((freeq =
		      &pgfl->pgfl_queues[try1]))) != NULL ||
		    (pg = TAILQ_FIRST((freeq =
		      &pgfl->pgfl_queues[try2]))) != NULL)
d1148 5
a1152 6
	TAILQ_REMOVE(freeq, pg, pageq);
	uvmexp.free--;

	/* update zero'd page count */
	if (pg->flags & PG_ZERO)
		uvmexp.zeropages--;
d1317 2
a1318 2
		 * it knows it needs to allocate swap if it wants to page the 
		 * page out. 
d1324 1
a1324 1
		
d1329 2
a1330 2
		 * changed the loan_count [e.g. in uvmfault_anonget()]) we just 
		 * return (when the last loan is dropped, then the page can be 
d1334 1
a1334 1
		if (saved_loan_count) 
d1360 2
a1361 6
	}
	if (pg->pqflags & PQ_INACTIVE) {
		if (pg->pqflags & PQ_SWAPBACKED)
			TAILQ_REMOVE(&uvm.page_inactive_swp, pg, pageq);
		else
			TAILQ_REMOVE(&uvm.page_inactive_obj, pg, pageq);
d1387 2
a1388 1
	    uvm_page_lookup_freelist(pg)].pgfl_queues[PGFL_UNKNOWN], pg, pageq);
d1492 2
a1493 1
 * => we do at least one iteration per call, if we are below the target.
d1502 4
a1505 1
	int free_list, s;
d1507 1
d1509 4
a1512 1
		s = uvm_lock_fpageq();
d1522 42
a1563 13
			if ((pg = TAILQ_FIRST(&pgfl->pgfl_queues[
			    PGFL_UNKNOWN])) != NULL)
				break;
		}

		if (pg == NULL) {
			/*
			 * No non-zero'd pages; don't bother trying again
			 * until we know we have non-zero'd pages free.
			 */
			uvm.page_idle_zero = FALSE;
			uvm_unlock_fpageq(s);
			return;
d1566 2
a1567 3
		TAILQ_REMOVE(&pgfl->pgfl_queues[PGFL_UNKNOWN], pg, pageq);
		uvmexp.free--;
		uvm_unlock_fpageq(s);
d1569 1
a1569 31
#ifdef PMAP_PAGEIDLEZERO
		if (PMAP_PAGEIDLEZERO(VM_PAGE_TO_PHYS(pg)) == FALSE) {
			/*
			 * The machine-dependent code detected some
			 * reason for us to abort zeroing pages,
			 * probably because there is a process now
			 * ready to run.
			 */
			s = uvm_lock_fpageq();
			TAILQ_INSERT_HEAD(&pgfl->pgfl_queues[PGFL_UNKNOWN],
			    pg, pageq);
			uvmexp.free++;
			uvmexp.zeroaborts++;
			uvm_unlock_fpageq(s);
			return;
		}
#else
		/*
		 * XXX This will toast the cache unless the pmap_zero_page()
		 * XXX implementation does uncached access.
		 */
		pmap_zero_page(VM_PAGE_TO_PHYS(pg));
#endif
		pg->flags |= PG_ZERO;

		s = uvm_lock_fpageq();
		TAILQ_INSERT_HEAD(&pgfl->pgfl_queues[PGFL_ZEROS], pg, pageq);
		uvmexp.free++;
		uvmexp.zeropages++;
		uvm_unlock_fpageq(s);
	} while (whichqs == 0);
@


1.33
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.c,v 1.32 2001/11/27 05:27:12 art Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.51 2001/03/09 01:02:12 chs Exp $	*/
d389 1
a389 1
#if defined(PMAP_STEAL_MEMORY)
d391 2
a392 16

	/* 
	 * defer bootstrap allocation to MD code (it may want to allocate 
	 * from a direct-mapped segment).  pmap_steal_memory should round
	 * off virtual_space_start/virtual_space_end.
	 */

	addr = pmap_steal_memory(size, &virtual_space_start,
	    &virtual_space_end);

	return(addr);

#else /* !PMAP_STEAL_MEMORY */

	static boolean_t initialized = FALSE;
	vaddr_t addr, vaddr;
d394 1
a394 3

	/* round to page size */
	size = round_page(size);
d408 18
@


1.32
log
@Merge in the unified buffer cache code as found in NetBSD 2001/03/10. The
code is written mostly by Chuck Silvers <chuq@@chuq.com>/<chs@@netbsd.org>.

Tested for the past few weeks by many developers, should be in a pretty stable
state, but will require optimizations and additional cleanups.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.31 2001/11/12 01:26:09 art Exp $	*/
d1074 1
a1074 1
				UVM_ADV_RANDOM, 0)) != KERN_SUCCESS) {
@


1.31
log
@Bring in more changes from NetBSD. Mostly pagedaemon improvements.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.30 2001/11/10 18:42:31 art Exp $	*/
a908 1
#ifdef UBC
a913 5
#else
	if (uvmexp.free < uvmexp.freemin || (uvmexp.free < uvmexp.freetarg &&
	    uvmexp.inactive < uvmexp.inactarg))
		wakeup(&uvm.pagedaemon);
#endif
@


1.30
log
@Merge in some parts of the ubc work that has been done in NetBSD that are not
UBC, but prerequsites for it.

- Create a daemon that processes async I/O (swap and paging in the future)
  requests that need processing in process context and that were processed
  in the pagedaemon before.
- Convert some ugly ifdef DIAGNOSTIC code to less intrusive KASSERTs.
- misc other cleanups.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.c,v 1.29 2001/11/07 02:55:50 art Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.44 2000/11/27 08:40:04 chs Exp $	*/
d74 1
d80 1
a81 1
#define UVM_PAGE                /* pull in uvm_page.h functions */
a106 6
#ifdef UBC
u_long uvm_pgcnt_anon;
u_long uvm_pgcnt_vnode;
extern struct uvm_pagerops uvm_vnodeops;
#endif

d155 1
a155 5
#ifdef DIAGNOSTIC
	if (pg->flags & PG_TABLED)
		panic("uvm_pageinsert: already inserted");
#endif

d157 1
a157 1
	s = splimp();
d184 1
a184 1
	s = splimp();
d190 4
a193 3
#ifdef UBC
	if (pg->uobject->pgops == &uvm_vnodeops) {
		uvm_pgcnt_vnode--;
a194 1
#endif
d221 1
a221 1
	 * step 1: init the page queues and page queue locks
d223 1
d235 2
a236 2
	 * step 2: init the <obj,offset> => <page> hash table. for now
	 * we just have one bucket (the bootstrap bucket).   later on we
d247 1
a247 1
	 * step 3: allocate vm_page structures.
d289 1
a289 2
	 * step 4: init the vm_page structures and put them in the correct
	 * place...
d300 1
d321 1
a321 1
	 * step 5: pass up the values of virtual_space_start and
d330 1
a330 1
	 * step 6: init locks for kernel threads
d337 1
a337 1
	 * step 7: init reserve thresholds
d342 6
d350 1
a350 2
	 * step 8: determine if we should zero pages in the idle
	 * loop.
d352 1
d803 1
a803 1
	s = splimp();
d898 4
d1027 1
a1027 3
#ifdef UBC
		uvm_pgcnt_anon++;
#endif
d1227 1
a1227 1
#ifdef UBC
d1229 1
a1229 1
		uvm_pgcnt_anon--;
a1230 1
#endif
d1294 2
@


1.29
log
@Another sync of uvm to NetBSD. Just minor fiddling, no major changes.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.c,v 1.28 2001/11/07 01:18:01 art Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.43 2000/11/09 19:15:28 christos Exp $	*/
d78 1
d100 4
d106 6
d138 1
a138 1

a174 1

d184 1
a184 1
void __inline
d191 1
a191 8
#ifdef DIAGNOSTIC
	if ((pg->flags & (PG_FAULTING)) != 0)
		panic("uvm_pageremove: page is faulting");
#endif

	if ((pg->flags & PG_TABLED) == 0)
		return;				/* XXX: log */

d199 6
a211 1

a228 1

d249 1
a249 1
	uvm.page_hashmask = 0;		/* mask for hash function */
a301 1
	 
d327 1
d338 1
a338 1
	 * step 6: init pagedaemon lock
d342 1
a353 4
	 *
	 * XXXJRT - might consider zero'ing up to the target *now*,
	 *	    but that could take an awfully long time if you
	 *	    have a lot of memory.
a367 1
 * => XXXCDC: move global vars.
d898 3
a900 7
#ifdef DIAGNOSTIC
	/* sanity check */
	if (obj && anon)
		panic("uvm_pagealloc: obj and anon != NULL");
#endif

	s = uvm_lock_fpageq();		/* lock free page queue */
d907 7
d917 1
d971 1
a971 5
#ifdef DIAGNOSTIC
		if (free_list >= VM_NFREELIST || free_list < 0)
			panic("uvm_pagealloc_strat: bad free list %d",
			    free_list);
#endif
a1021 2
	pg->wire_count = 0;
	pg->loan_count = 0;
d1025 3
a1127 2
 
	return;
d1141 3
a1143 4
void uvm_pagefree(pg)

struct vm_page *pg;

d1148 7
d1163 1
a1163 1
		 * if the object page is on loan we are going to drop ownership.  
a1171 1

d1182 1
a1184 1

d1194 1
d1199 1
a1199 9

#ifdef DIAGNOSTIC
	if (saved_loan_count) {
		printf("uvm_pagefree: warning: freeing page with a loan "
		    "count of %d\n", saved_loan_count);
		panic("uvm_pagefree: loan count");
	}
#endif
	
d1222 1
d1227 5
d1234 1
a1234 1
	 * and put on free queue 
d1254 45
@


1.28
log
@Add an alignment argument to uvm_map that specifies an alignment hint
for the virtual address.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.c,v 1.27 2001/11/06 13:36:52 art Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.40 2000/08/02 20:25:11 thorpej Exp $	*/
d259 1
a259 1
		panic("vm_page_bootstrap: no memory pre-allocated");
d498 1
a498 1
			panic("vm_page_physget: called _after_ bootstrap");
d513 1
a513 1
				    panic("vm_page_physget: out of memory!");
d532 1
a532 1
				    panic("vm_page_physget: out of memory!");
d563 1
a563 1
				panic("vm_page_physget: out of memory!");
d625 1
d1334 15
a1348 1
		PMAP_PAGEIDLEZERO(VM_PAGE_TO_PHYS(pg));
@


1.27
log
@More sync to NetBSD.
 - Use malloc/free instead of MALLOC/FREE for variable sized allocations.
 - Move the memory inheritance code to sys/mman.h and rename from VM_* to MAP_*
 - various cleanups and simplifications.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.26 2001/11/06 01:35:04 art Exp $	*/
d1073 1
a1073 1
	if (uvm_map(kernel_map, &addr, size, NULL, UVM_UNKNOWN_OFFSET,
@


1.26
log
@Move the last content from vm/ to uvm/
The only thing left in vm/ are just dumb wrappers.
vm/vm.h includes uvm/uvm_extern.h
vm/pmap.h includes uvm/uvm_pmap.h
vm/vm_page.h includes uvm/uvm_page.h
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.c,v 1.25 2001/11/05 22:14:54 art Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.39 2000/06/27 17:29:31 mrg Exp $	*/
d648 2
a649 2
		MALLOC(pgs, struct vm_page *, sizeof(struct vm_page) * npages,
					 M_VMPAGE, M_NOWAIT);
@


1.25
log
@Minor sync to NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.c,v 1.24 2001/09/19 20:50:59 mickey Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.38 2000/06/26 14:21:18 mrg Exp $	*/
a77 2

#include <vm/vm.h>
@


1.24
log
@merge vm/vm_kern.h into uvm/uvm_extern.h; art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.c,v 1.23 2001/08/25 12:13:27 art Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.37 2000/06/09 04:43:19 soda Exp $	*/
a79 1
#include <vm/vm_page.h>
@


1.23
log
@Default to disabled zeroing of pages in the idle loop.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.22 2001/08/11 10:57:22 art Exp $	*/
a80 1
#include <vm/vm_kern.h>
@


1.22
log
@Various random fixes from NetBSD.
Including support for zeroing pages in the idle loop (not enabled yet).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.21 2001/08/06 14:03:05 art Exp $	*/
d103 1
a103 1
boolean_t vm_page_zero_enable = TRUE;
@


1.21
log
@Add a new type voff_t (right now it's typedefed as off_t) used for offsets
into objects.

Gives the possibilty to mmap beyond the size of vaddr_t.

From NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.c,v 1.20 2001/07/31 14:03:47 art Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.31 2000/03/26 20:54:47 kleink Exp $	*/
d77 1
a77 1
#include <sys/proc.h>
d98 8
d229 4
a232 2
	for (lcv = 0; lcv < VM_NFREELIST; lcv++)
	  TAILQ_INIT(&uvm.page_free[lcv]);
d347 10
d627 2
a628 2
		printf("\t%d segments allocated, ignoring 0x%lx -> 0x%lx\n",
		    VM_PHYSSEG_MAX, start, end);
d850 5
a854 3
		printf("0x%lx->0x%lx [0x%lx->0x%lx]\n", vm_physmem[lcv].start,
		    vm_physmem[lcv].end, vm_physmem[lcv].avail_start,
		    vm_physmem[lcv].avail_end);
d876 6
d892 1
a892 1
	int lcv, s;
d895 1
d932 16
d953 5
a957 2
			freeq = &uvm.page_free[lcv];
			if ((pg = freeq->tqh_first) != NULL)
d972 5
a976 2
		freeq = &uvm.page_free[free_list];
		if ((pg = freeq->tqh_first) != NULL)
d997 18
d1037 10
d1225 1
a1225 2
	if (pg->wire_count)
	{
d1234 2
d1237 2
a1238 2
	TAILQ_INSERT_TAIL(&uvm.page_free[uvm_page_lookup_freelist(pg)],
	    pg, pageq);
d1246 4
d1291 63
@


1.20
log
@Allocate page buckets from kernel_map. This should save a good
amount of kmem_map on machines with lots of physical memory.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.c,v 1.19 2001/07/25 14:47:59 art Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.30 2000/02/13 03:34:40 thorpej Exp $	*/
d859 1
a859 1
	vaddr_t off;
d1028 1
a1028 1
	vaddr_t newoff;
@


1.19
log
@Some updates to UVM from NetBSD. Nothing really critical, just a sync.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.c,v 1.18 2001/07/19 14:31:32 art Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.29 1999/12/30 16:09:47 eeh Exp $	*/
d740 1
d760 1
a760 1
	 * malloc new buckets
d763 10
a772 2
	MALLOC(newbuckets, struct pglist *, sizeof(struct pglist) * bucketcount,
					 M_VMPBUCKET, M_NOWAIT);
a786 3
	/* swap old for new ... */
	oldbuckets = uvm.page_hash;
	oldcount = uvm.page_nhash;
d808 1
a808 1
		FREE(oldbuckets, M_VMPBUCKET);
@


1.18
log
@Missed one in PMAP_NEW fix.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.c,v 1.17 2001/07/18 10:47:05 art Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.25 1999/09/12 01:17:38 chs Exp $	*/
d212 1
a212 1
	int freepages, pagecount;
d214 1
a214 1
	int lcv, n, i;  
d292 2
a293 2
			printf("uvm_page_init: lost %d page(s) in init\n",
			    n - pagecount);
d462 5
a466 2
boolean_t
uvm_page_physget(paddrp)
d468 1
d484 3
d558 13
d584 1
a584 1
	vaddr_t start, end, avail_start, avail_end;
d597 3
@


1.17
log
@Get rid of the PMAP_NEW option by making it mandatory for all archs.
The archs that didn't have a proper PMAP_NEW now have a dummy implementation
with wrappers around the old functions.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.16 2001/04/10 06:59:12 niklas Exp $	*/
a981 1
#if defined(PMAP_NEW)
a983 5
#else
		pmap_enter(pmap_kernel(), temp_addr, VM_PAGE_TO_PHYS(pg),
			   VM_PROT_READ|VM_PROT_WRITE, TRUE,
			   VM_PROT_READ|VM_PROT_WRITE);
#endif
@


1.16
log
@Fix for machines which need to enlarge the kernel address space, at least
1GB i386 machines needs this.  The fix is heavily based on Jason Thorpe's
found in NetBSD.  Here is his original commit message:

Instead of checking vm_physmem[<physseg>].pgs to determine if
uvm_page_init() has completed, add a boolean uvm.page_init_done,
and test against that.  Use this same boolean (rather than
pmap_initialized) in pmap_growkernel() to determine if we are
being called via uvm_page_init() to grow the kernel address space.

This fixes a problem on some i386 configurations where pmap_init()
itself was needing to have the kernel page table grown, and since
pmap_initialized was not yet set to TRUE, pmap_growkernel() was
choosing the wrong code path.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.c,v 1.15 2001/03/22 18:05:33 niklas Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.24 1999/07/22 22:58:38 thorpej Exp $	*/
a440 2
		/* XXX: should be wired, but some pmaps don't like that ... */
#if defined(PMAP_NEW)
a445 6
#else
		pmap_enter(pmap_kernel(), vaddr, paddr,
		    VM_PROT_READ|VM_PROT_WRITE, FALSE,
		    VM_PROT_READ|VM_PROT_WRITE);
#endif

@


1.15
log
@pastos in diagnostic strings
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.14 2001/03/22 03:05:56 smart Exp $	*/
d341 1
d485 1
a485 1
		if (vm_physmem[lcv].pgs)
@


1.14
log
@Sync style, typo, and comments a little closer to NetBSD.  art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.13 2001/03/08 15:21:37 smart Exp $	*/
d580 1
a580 1
		panic("vm_page_physload: page size not set!");
d589 1
a589 1
		printf("vm_page_physload: unable to load physical memory "
d611 1
a611 1
		panic("vm_page_physload: tried to add RAM after vm_mem_init");
d619 1
a619 1
			printf("vm_page_physload: can not malloc vm_page "
d687 1
a687 1
	panic("vm_page_physload: unknown physseg strategy selected!");
@


1.13
log
@Replace thread_wakeup() with wakeup().  It is defined in vm_extern.h as a
wrapper, so this removes a dependence on the old VM system.  From NetBSD.
art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.12 2001/03/03 12:28:55 art Exp $	*/
d112 2
a113 2
 * later rehash (resize) the hash table once malloc() is ready.
 * we static allocate the bootstrap bucket below...
d232 1
a232 1
	 * will malloc() new buckets as we dynamically resize the hash table.
d749 1
a749 1
		printf("vm_page_physrehash: WARNING: could not grow page "
d782 1
a782 1
	 * free old bucket array if we malloc'd it previously
@


1.12
log
@Allow the syncer to get pages from the pagedaemon reserve.
Otherwise we can end up in a situation where the syncer waits for pages
and the pagedaemon waits for buffers.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_page.c,v 1.11 2001/03/02 09:07:39 art Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.23 1999/05/25 01:34:13 thorpej Exp $	*/
d862 1
a862 1
		thread_wakeup(&uvm.pagedaemon);
@


1.11
log
@Reserve more pages for the pagedaemon and the kernel.
With soft updates, writing out pages to disk can cause a bunch of allocations.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.10 2001/01/29 02:07:47 niklas Exp $	*/
d877 2
a878 1
	     !(use_reserve && curproc == uvm.pagedaemon_proc)))
@


1.10
log
@$OpenBSD$
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.23 1999/05/25 01:34:13 thorpej Exp $	*/
d334 2
a335 2
	uvmexp.reserve_pagedaemon = 1;
	uvmexp.reserve_kernel = 5;
@


1.9
log
@Convert bzero to memset(X, 0..) and bcopy to memcpy.
This is to match (make diffs smaller) the code in NetBSD.
new gcc inlines those functions, so this could also be a performance win.
@
text
@d1 1
@


1.8
log
@A fix to the dreaded isadmaattach panic which hunts people playing with
large memory machines.  This time I really hope we can continue quite a bit
away over the Gig.
@
text
@d280 1
a280 1
	bzero(pagearray, pagecount * sizeof(struct vm_page));
d624 1
a624 1
		bzero(pgs, sizeof(struct vm_page) * npages);
@


1.7
log
@Bring in some new UVM code from NetBSD (not current).

 - Introduce a new type of map that are interrupt safe and never allow faults
   in them. mb_map and kmem_map are made intrsafe.
 - Add "access protection" to uvm_vslock (to be passed down to uvm_fault and
   later to pmap_enter).
 - madvise(2) now works.
 - various cleanups.
@
text
@d475 2
a476 1
#if (VM_PHYSSEG_STRAT == VM_PSTRAT_BIGFIRST)
d526 2
a527 1
#if (VM_PHYSSEG_STRAT == VM_PSTRAT_BIGFIRST)
@


1.6
log
@fixup the uvm_map() call in the uvm_pagealloc_contig() w/
right uvm_map flags values, also fix the error ondition check.
couple of spaces vs tabs in the same code spot.
art@@ ok
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_page.c,v 1.19 1999/05/20 20:07:55 thorpej Exp $	*/
d850 1
a850 3
	s = splimp();

	uvm_lock_fpageq();		/* lock free page queue */
d871 1
a871 1
		(obj && obj->uo_refs == UVM_OBJ_KERN);
d920 1
a920 2
	uvm_unlock_fpageq();		/* unlock free page queue */
	splx(s);
d945 1
a945 2
	uvm_unlock_fpageq();
	splx(s);
d1136 1
a1136 2
	s = splimp();
	uvm_lock_fpageq();
d1146 1
a1146 2
	uvm_unlock_fpageq();
	splx(s);
@


1.6.4.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_page.c,v 1.23 1999/05/25 01:34:13 thorpej Exp $	*/
d850 3
a852 1
	s = uvm_lock_fpageq();		/* lock free page queue */
d873 1
a873 1
		(obj && UVM_OBJ_IS_KERN_OBJECT(obj));
d922 2
a923 1
	uvm_unlock_fpageq(s);		/* unlock free page queue */
d948 2
a949 1
	uvm_unlock_fpageq(s);
d1140 2
a1141 1
	s = uvm_lock_fpageq();
d1151 2
a1152 1
	uvm_unlock_fpageq(s);
@


1.6.4.2
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_page.c,v 1.16 2001/04/10 06:59:12 niklas Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.24 1999/07/22 22:58:38 thorpej Exp $	*/
d111 2
a112 2
 * later rehash (resize) the hash table once the allocator is ready.
 * we static allocate the one bootstrap bucket below...
d231 1
a231 1
	 * will allocate new buckets as we dynamically resize the hash table.
d280 1
a280 1
	memset(pagearray, 0, pagecount * sizeof(struct vm_page));
d333 2
a334 2
	uvmexp.reserve_pagedaemon = 4;
	uvmexp.reserve_kernel = 6;
a339 1
	uvm.page_init_done = TRUE;
d475 1
a475 2
#if (VM_PHYSSEG_STRAT == VM_PSTRAT_BIGFIRST) || \
	(VM_PHYSSEG_STRAT == VM_PSTRAT_BSEARCH)
d482 1
a482 1
		if (uvm.page_init_done == TRUE)
d525 1
a525 2
#if (VM_PHYSSEG_STRAT == VM_PSTRAT_BIGFIRST) || \
	(VM_PHYSSEG_STRAT == VM_PSTRAT_BSEARCH)
d577 1
a577 1
		panic("uvm_page_physload: page size not set!");
d586 1
a586 1
		printf("uvm_page_physload: unable to load physical memory "
d608 1
a608 1
		panic("uvm_page_physload: tried to add RAM after vm_mem_init");
d616 1
a616 1
			printf("uvm_page_physload: can not malloc vm_page "
d622 1
a622 1
		memset(pgs, 0, sizeof(struct vm_page) * npages);
d684 1
a684 1
	panic("uvm_page_physload: unknown physseg strategy selected!");
d746 1
a746 1
		printf("uvm_page_physrehash: WARNING: could not grow page "
d779 1
a779 1
	 * free old bucket array if is not the boot-time table
d859 1
a859 1
		wakeup(&uvm.pagedaemon);
d874 1
a874 2
	     !(use_reserve && (curproc == uvm.pagedaemon_proc ||
				curproc == syncerproc))))
@


1.6.4.3
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_page.c,v 1.37 2000/06/09 04:43:19 soda Exp $	*/
d77 1
a77 1
#include <sys/sched.h>
d81 1
a97 8
 * Some supported CPUs in a given architecture don't support all
 * of the things necessary to do idle page zero'ing efficiently.
 * We therefore provide a way to disable it from machdep code here.
 */

boolean_t vm_page_zero_enable = FALSE;

/*
d212 1
a212 1
	vsize_t freepages, pagecount, n;
d214 1
a214 1
	int lcv, i;  
d221 2
a222 4
	for (lcv = 0; lcv < VM_NFREELIST; lcv++) {
		for (i = 0; i < PGFL_NQUEUES; i++)
			TAILQ_INIT(&uvm.page_free[lcv].pgfl_queues[i]);
	}
d292 2
a293 2
			printf("uvm_page_init: lost %ld page(s) in init\n",
			    (long)(n - pagecount));
a336 10
  	/*
	 * step 8: determine if we should zero pages in the idle
	 * loop.
	 *
	 * XXXJRT - might consider zero'ing up to the target *now*,
	 *	    but that could take an awfully long time if you
	 *	    have a lot of memory.
	 */
	uvm.page_idle_zero = vm_page_zero_enable;

d441 2
d448 6
d470 2
a471 5
/* subroutine: try to allocate from memory chunks on the specified freelist */
static boolean_t uvm_page_physget_freelist __P((paddr_t *, int));

static boolean_t
uvm_page_physget_freelist(paddrp, freelist)
a472 1
	int freelist;
a487 3
		if (vm_physmem[lcv].free_list != freelist)
			continue;

a558 13

boolean_t
uvm_page_physget(paddrp)
	paddr_t *paddrp;
{
	int i;

	/* try in the order of freelist preference */
	for (i = 0; i < VM_NFREELIST; i++)
		if (uvm_page_physget_freelist(paddrp, i) == TRUE)
			return (TRUE);
	return (FALSE);
}
d572 1
a572 1
	paddr_t start, end, avail_start, avail_end;
a585 3
	if (start >= end)
		panic("uvm_page_physload: start >= end");

d592 2
a593 2
		printf("\t%d segments allocated, ignoring 0x%llx -> 0x%llx\n",
		    VM_PHYSSEG_MAX, (long long)start, (long long)end);
a724 1
	size_t newsize, oldsize;
d744 1
a744 10
	 * compute the size of the current table and new table.
	 */

	oldbuckets = uvm.page_hash;
	oldcount = uvm.page_nhash;
	oldsize = round_page(sizeof(struct pglist) * oldcount);
	newsize = round_page(sizeof(struct pglist) * bucketcount);

	/*
	 * allocate the new buckets
d747 2
a748 1
	newbuckets = (struct pglist *) uvm_km_alloc(kernel_map, newsize);
d763 3
d787 1
a787 1
		uvm_km_free(kernel_map, (vaddr_t) oldbuckets, oldsize);
d809 3
a811 5
		printf("0x%llx->0x%llx [0x%llx->0x%llx]\n",
		    (long long)vm_physmem[lcv].start,
		    (long long)vm_physmem[lcv].end,
		    (long long)vm_physmem[lcv].avail_start,
		    (long long)vm_physmem[lcv].avail_end);
a832 6
 * => policy decision: it is more important to pull a page off of the
 *	appropriate priority free list than it is to get a zero'd or
 *	unknown contents page.  This is because we live with the
 *	consequences of a bad free list decision for the entire
 *	lifetime of the page, e.g. if the page comes from memory that
 *	is slower to access.
d838 1
a838 1
	voff_t off;
d843 1
a843 1
	int lcv, try1, try2, s, zeroit = 0;
a845 1
	struct pgfreelist *pgfl;
a881 16
#if PGFL_NQUEUES != 2
#error uvm_pagealloc_strat needs to be updated
#endif

	/*
	 * If we want a zero'd page, try the ZEROS queue first, otherwise
	 * we try the UNKNOWN queue first.
	 */
	if (flags & UVM_PGA_ZERO) {
		try1 = PGFL_ZEROS;
		try2 = PGFL_UNKNOWN;
	} else {
		try1 = PGFL_UNKNOWN;
		try2 = PGFL_ZEROS;
	}

d887 2
a888 5
			pgfl = &uvm.page_free[lcv];
			if ((pg = TAILQ_FIRST((freeq =
			      &pgfl->pgfl_queues[try1]))) != NULL ||
			    (pg = TAILQ_FIRST((freeq =
			      &pgfl->pgfl_queues[try2]))) != NULL)
d903 2
a904 5
		pgfl = &uvm.page_free[free_list];
		if ((pg = TAILQ_FIRST((freeq =
		      &pgfl->pgfl_queues[try1]))) != NULL ||
		    (pg = TAILQ_FIRST((freeq =
		      &pgfl->pgfl_queues[try2]))) != NULL)
a924 18
	/* update zero'd page count */
	if (pg->flags & PG_ZERO)
		uvmexp.zeropages--;

	/*
	 * update allocation statistics and remember if we have to
	 * zero the page
	 */
	if (flags & UVM_PGA_ZERO) {
		if (pg->flags & PG_ZERO) {
			uvmexp.pga_zerohit++;
			zeroit = 0;
		} else {
			uvmexp.pga_zeromiss++;
			zeroit = 1;
		}
	}

a946 10
	if (flags & UVM_PGA_ZERO) {
		/*
		 * A zero'd page is not clean.  If we got a page not already
		 * zero'd, then we have to zero it ourselves.
		 */
		pg->flags &= ~PG_CLEAN;
		if (zeroit)
			pmap_zero_page(VM_PAGE_TO_PHYS(pg));
	}

d990 1
d993 5
d1013 1
a1013 1
	voff_t newoff;
d1131 2
a1132 1
	if (pg->wire_count) {
a1140 2
	pg->flags &= ~PG_ZERO;

d1142 2
a1143 2
	TAILQ_INSERT_TAIL(&uvm.page_free[
	    uvm_page_lookup_freelist(pg)].pgfl_queues[PGFL_UNKNOWN], pg, pageq);
a1150 4

	if (uvmexp.zeropages < UVM_PAGEZERO_TARGET)
		uvm.page_idle_zero = vm_page_zero_enable;

a1191 63

/*
 * uvm_pageidlezero: zero free pages while the system is idle.
 *
 * => we do at least one iteration per call, if we are below the target.
 * => we loop until we either reach the target or whichqs indicates that
 *	there is a process ready to run.
 */
void
uvm_pageidlezero()
{
	struct vm_page *pg;
	struct pgfreelist *pgfl;
	int free_list, s;

	do {
		s = uvm_lock_fpageq();

		if (uvmexp.zeropages >= UVM_PAGEZERO_TARGET) {
			uvm.page_idle_zero = FALSE;
			uvm_unlock_fpageq(s);
			return;
		}

		for (free_list = 0; free_list < VM_NFREELIST; free_list++) {
			pgfl = &uvm.page_free[free_list];
			if ((pg = TAILQ_FIRST(&pgfl->pgfl_queues[
			    PGFL_UNKNOWN])) != NULL)
				break;
		}

		if (pg == NULL) {
			/*
			 * No non-zero'd pages; don't bother trying again
			 * until we know we have non-zero'd pages free.
			 */
			uvm.page_idle_zero = FALSE;
			uvm_unlock_fpageq(s);
			return;
		}

		TAILQ_REMOVE(&pgfl->pgfl_queues[PGFL_UNKNOWN], pg, pageq);
		uvmexp.free--;
		uvm_unlock_fpageq(s);

#ifdef PMAP_PAGEIDLEZERO
		PMAP_PAGEIDLEZERO(VM_PAGE_TO_PHYS(pg));
#else
		/*
		 * XXX This will toast the cache unless the pmap_zero_page()
		 * XXX implementation does uncached access.
		 */
		pmap_zero_page(VM_PAGE_TO_PHYS(pg));
#endif
		pg->flags |= PG_ZERO;

		s = uvm_lock_fpageq();
		TAILQ_INSERT_HEAD(&pgfl->pgfl_queues[PGFL_ZEROS], pg, pageq);
		uvmexp.free++;
		uvmexp.zeropages++;
		uvm_unlock_fpageq(s);
	} while (whichqs == 0);
}
@


1.6.4.4
log
@merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_page.c,v 1.51 2001/03/09 01:02:12 chs Exp $	*/
a73 1
#define UVM_PAGE                /* pull in uvm_page.h functions */
a77 2
#include <sys/kernel.h>
#include <sys/vnode.h>
d79 4
a101 4
/*
 * XXX disabled until we can find a way to do this without causing
 * problems for either cpu caches or DMA latency.
 */
d130 1
a130 1
static void uvm_pageremove __P((struct vm_page *));
d152 5
a156 1
	KASSERT((pg->flags & PG_TABLED) == 0);
d158 1
a158 1
	s = splvm();
d167 1
d177 1
a177 1
static __inline void
d184 8
a191 1
	KASSERT(pg->flags & PG_TABLED);
d193 1
a193 1
	s = splvm();
a198 6
	if (UVM_OBJ_IS_VTEXT(pg->uobject)) {
		uvmexp.vtextpages--;
	} else if (UVM_OBJ_IS_VNODE(pg->uobject)) {
		uvmexp.vnodepages--;
	}

d206 1
d224 1
d226 1
a226 1
	 * init the page queues and page queue locks
a227 1

d239 2
a240 2
	 * init the <obj,offset> => <page> hash table.  for now
	 * we just have one bucket (the bootstrap bucket).  later on we
d245 1
a245 1
	uvm.page_hashmask = 0;			/* mask for hash function */
d251 1
a251 1
	 * allocate vm_page structures.
d262 1
a262 1
		panic("uvm_page_bootstrap: no memory pre-allocated");
d293 2
a294 1
	 * init the vm_page structures and put them in the correct place.
d298 1
a305 1

a323 1

d325 1
a325 1
	 * pass up the values of virtual_space_start and
d334 1
a334 1
	 * init locks for kernel threads
a337 1
	simple_lock_init(&uvm.aiodoned_lock);
d340 1
a340 1
	 * init reserve thresholds
a344 6
	uvmexp.anonminpct = 10;
	uvmexp.vnodeminpct = 10;
	uvmexp.vtextminpct = 5;
	uvmexp.anonmin = uvmexp.anonminpct * 256 / 100;
	uvmexp.vnodemin = uvmexp.vnodeminpct * 256 / 100;
	uvmexp.vtextmin = uvmexp.vtextminpct * 256 / 100;
d347 6
a352 1
	 * determine if we should zero pages in the idle loop.
a353 1

d367 1
d501 1
a501 1
			panic("uvm_page_physget: called _after_ bootstrap");
d516 1
a516 1
				    panic("vum_page_physget: out of memory!");
d535 1
a535 1
				    panic("uvm_page_physget: out of memory!");
d566 1
a566 1
				panic("uvm_page_physget: out of memory!");
a627 1
		printf("\tincrease VM_PHYSSEG_MAX\n");
d651 2
a652 2
		pgs = malloc(sizeof(struct vm_page) * npages,
		    M_VMPAGE, M_NOWAIT);
d804 1
a804 1
	s = splvm();
d897 5
a901 5
	KASSERT(obj == NULL || anon == NULL);
	KASSERT(off == trunc_page(off));

	LOCK_ASSERT(obj == NULL || simple_lock_held(&obj->vmobjlock));
	LOCK_ASSERT(anon == NULL || simple_lock_held(&anon->an_lock));
d903 1
a903 1
	s = uvm_lock_fpageq();
a909 7
#ifdef UBC
	if (uvmexp.free + uvmexp.paging < uvmexp.freemin ||
	    (uvmexp.free + uvmexp.paging < uvmexp.freetarg &&
	     uvmexp.inactive < uvmexp.inactarg)) {
		wakeup(&uvm.pagedaemon);
	}
#else
a912 1
#endif
d966 5
a970 1
		KASSERT(free_list >= 0 && free_list < VM_NFREELIST);
d1021 2
a1025 1
		uvmexp.anonpages++;
d1076 1
a1076 1
	if (uvm_map(kernel_map, &addr, size, NULL, UVM_UNKNOWN_OFFSET, 0,
d1126 2
d1141 4
a1144 3
void
uvm_pagefree(pg)
	struct vm_page *pg;
a1148 7
#ifdef DEBUG
	if (pg->uobject == (void *)0xdeadbeef &&
	    pg->uanon == (void *)0xdeadbeef) {
		panic("uvm_pagefree: freeing free page %p\n", pg);
	}
#endif

d1157 1
a1157 1
		 * if the object page is on loan we are going to drop ownership.
d1166 1
a1176 1

d1179 1
a1188 1

d1193 9
a1201 1
	KASSERT(saved_loan_count == 0);
a1223 1

a1228 4
	if (pg->uanon) {
		uvmexp.anonpages--;
	}

d1230 1
a1230 1
	 * and put on free queue
a1251 47
/*
 * uvm_page_unbusy: unbusy an array of pages.
 *
 * => pages must either all belong to the same object, or all belong to anons.
 * => if pages are object-owned, object must be locked.
 * => if pages are anon-owned, anons must be unlockd and have 0 refcount.
 */

void
uvm_page_unbusy(pgs, npgs)
	struct vm_page **pgs;
	int npgs;
{
	struct vm_page *pg;
	struct uvm_object *uobj;
	int i;
	UVMHIST_FUNC("uvm_page_unbusy"); UVMHIST_CALLED(ubchist);

	for (i = 0; i < npgs; i++) {
		pg = pgs[i];

		if (pg == NULL) {
			continue;
		}
		if (pg->flags & PG_WANTED) {
			wakeup(pg);
		}
		if (pg->flags & PG_RELEASED) {
			UVMHIST_LOG(ubchist, "releasing pg %p", pg,0,0,0);
			uobj = pg->uobject;
			if (uobj != NULL) {
				uobj->pgops->pgo_releasepg(pg, NULL);
			} else {
				pg->flags &= ~(PG_BUSY);
				UVM_PAGE_OWN(pg, NULL);
				uvm_anfree(pg->uanon);
			}
		} else {
			UVMHIST_LOG(ubchist, "unbusying pg %p", pg,0,0,0);
			KASSERT(pg->wire_count ||
				(pg->pqflags & (PQ_ACTIVE|PQ_INACTIVE)));
			pg->flags &= ~(PG_WANTED|PG_BUSY);
			UVM_PAGE_OWN(pg, NULL);
		}
	}
}

d1336 1
a1336 15
		if (PMAP_PAGEIDLEZERO(VM_PAGE_TO_PHYS(pg)) == FALSE) {
			/*
			 * The machine-dependent code detected some
			 * reason for us to abort zeroing pages,
			 * probably because there is a process now
			 * ready to run.
			 */
			s = uvm_lock_fpageq();
			TAILQ_INSERT_HEAD(&pgfl->pgfl_queues[PGFL_UNKNOWN],
			    pg, pageq);
			uvmexp.free++;
			uvmexp.zeroaborts++;
			uvm_unlock_fpageq(s);
			return;
		}
@


1.6.4.5
log
@Merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_page.c,v 1.65 2001/06/27 23:57:16 thorpej Exp $	*/
d4 1
a4 1
/*
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and
d48 1
a48 1
 *
d54 3
a56 3
 *
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
d58 1
a58 1
 *
a128 10
 * we allocate an initial number of page colors in uvm_page_init(),
 * and remember them.  We may re-color pages as cache sizes are
 * discovered during the autoconfiguration phase.  But we can never
 * free the initial set of buckets, since they are allocated using
 * uvm_pageboot_alloc().
 */

static boolean_t have_recolored_pages /* = FALSE */;

/*
a204 13
static void
uvm_page_init_buckets(struct pgfreelist *pgfl)
{
	int color, i;

	for (color = 0; color < uvmexp.ncolors; color++) {
		for (i = 0; i < PGFL_NQUEUES; i++) {
			TAILQ_INIT(&pgfl->pgfl_buckets[
			    color].pgfl_queues[i]);
		}
	}
}

d207 1
a207 1
 *
d215 3
a217 4
	vsize_t freepages, pagecount, bucketcount, n;
	struct pgflbucket *bucketarray;
	struct vm_page *pagearray;
	int lcv, i;
d221 1
a221 3
	 * init the page queues and page queue locks, except the free
	 * list; we allocate that later (with the initial vm_page
	 * structures).
d224 4
d229 2
a230 1
	TAILQ_INIT(&uvm.page_inactive);
d246 1
a246 1
	/*
d259 1
a259 1

d261 1
a261 1
	 * first calculate the number of free pages...
d267 1
a267 1

a272 8
	 * Let MD code initialize the number of colors, or default
	 * to 1 color if MD code doesn't care.
	 */
	if (uvmexp.ncolors == 0)
		uvmexp.ncolors = 1;
	uvmexp.colormask = uvmexp.ncolors - 1;

	/*
d281 1
a281 2

	bucketcount = uvmexp.ncolors * VM_NFREELIST;
d284 2
a285 12

	bucketarray = (void *) uvm_pageboot_alloc((bucketcount *
	    sizeof(struct pgflbucket)) + (pagecount *
	    sizeof(struct vm_page)));
	pagearray = (struct vm_page *)(bucketarray + bucketcount);

	for (lcv = 0; lcv < VM_NFREELIST; lcv++) {
		uvm.page_free[lcv].pgfl_buckets =
		    (bucketarray + (lcv * uvmexp.ncolors));
		uvm_page_init_buckets(&uvm.page_free[lcv]);
	}

d287 1
a287 1

a310 3
#ifdef __HAVE_VM_PAGE_MD
			VM_MDPAGE_INIT(&vm_physmem[lcv].pgs[i]);
#endif
d364 1
a364 1
 *
d366 1
a366 1
 */
d389 16
d406 1
a406 3
	vaddr_t addr;
#if !defined(PMAP_STEAL_MEMORY)
	vaddr_t vaddr;
d408 3
a410 1
#endif
a424 18
	/* round to page size */
	size = round_page(size);

#if defined(PMAP_STEAL_MEMORY)

	/*
	 * defer bootstrap allocation to MD code (it may want to allocate
	 * from a direct-mapped segment).  pmap_steal_memory should adjust
	 * virtual_space_start/virtual_space_end if necessary.
	 */

	addr = pmap_steal_memory(size, &virtual_space_start,
	    &virtual_space_end);

	return(addr);

#else /* !PMAP_STEAL_MEMORY */

a463 1
	pmap_update();
a833 70
/*
 * uvm_page_recolor: Recolor the pages if the new bucket count is
 * larger than the old one.
 */

void
uvm_page_recolor(int newncolors)
{
	struct pgflbucket *bucketarray, *oldbucketarray;
	struct pgfreelist pgfl;
	struct vm_page *pg;
	vsize_t bucketcount;
	int s, lcv, color, i, ocolors;

	if (newncolors <= uvmexp.ncolors)
		return;

	bucketcount = newncolors * VM_NFREELIST;
	bucketarray = malloc(bucketcount * sizeof(struct pgflbucket),
	    M_VMPAGE, M_NOWAIT);
	if (bucketarray == NULL) {
		printf("WARNING: unable to allocate %ld page color buckets\n",
		    (long) bucketcount);
		return;
	}

	s = uvm_lock_fpageq();

	/* Make sure we should still do this. */
	if (newncolors <= uvmexp.ncolors) {
		uvm_unlock_fpageq(s);
		free(bucketarray, M_VMPAGE);
		return;
	}

	oldbucketarray = uvm.page_free[0].pgfl_buckets;
	ocolors = uvmexp.ncolors;

	uvmexp.ncolors = newncolors;
	uvmexp.colormask = uvmexp.ncolors - 1;

	for (lcv = 0; lcv < VM_NFREELIST; lcv++) {
		pgfl.pgfl_buckets = (bucketarray + (lcv * newncolors));
		uvm_page_init_buckets(&pgfl);
		for (color = 0; color < ocolors; color++) {
			for (i = 0; i < PGFL_NQUEUES; i++) {
				while ((pg = TAILQ_FIRST(&uvm.page_free[
				    lcv].pgfl_buckets[color].pgfl_queues[i]))
				    != NULL) {
					TAILQ_REMOVE(&uvm.page_free[
					    lcv].pgfl_buckets[
					    color].pgfl_queues[i], pg, pageq);
					TAILQ_INSERT_TAIL(&pgfl.pgfl_buckets[
					    VM_PGCOLOR_BUCKET(pg)].pgfl_queues[
					    i], pg, pageq);
				}
			}
		}
		uvm.page_free[lcv].pgfl_buckets = pgfl.pgfl_buckets;
	}

	if (have_recolored_pages) {
		uvm_unlock_fpageq(s);
		free(oldbucketarray, M_VMPAGE);
		return;
	}

	have_recolored_pages = TRUE;
	uvm_unlock_fpageq(s);
}
a864 43
 * uvm_pagealloc_pgfl: helper routine for uvm_pagealloc_strat
 */

static __inline struct vm_page *
uvm_pagealloc_pgfl(struct pgfreelist *pgfl, int try1, int try2,
    unsigned int *trycolorp)
{
	struct pglist *freeq;
	struct vm_page *pg;
	int color, trycolor = *trycolorp;

	color = trycolor;
	do {
		if ((pg = TAILQ_FIRST((freeq =
		    &pgfl->pgfl_buckets[color].pgfl_queues[try1]))) != NULL)
			goto gotit;
		if ((pg = TAILQ_FIRST((freeq =
		    &pgfl->pgfl_buckets[color].pgfl_queues[try2]))) != NULL)
			goto gotit;
		color = (color + 1) & uvmexp.colormask;
	} while (color != trycolor);

	return (NULL);

 gotit:
	TAILQ_REMOVE(freeq, pg, pageq);
	uvmexp.free--;

	/* update zero'd page count */
	if (pg->flags & PG_ZERO)
		uvmexp.zeropages--;

	if (color == trycolor)
		uvmexp.colorhit++;
	else {
		uvmexp.colormiss++;
		*trycolorp = color;
	}

	return (pg);
}

/*
d890 1
a890 1
	int lcv, try1, try2, s, zeroit = 0, color;
d892 2
a904 9
	 * This implements a global round-robin page coloring
	 * algorithm.
	 *
	 * XXXJRT: Should we make the `nextcolor' per-cpu?
	 * XXXJRT: What about virtually-indexed caches?
	 */
	color = uvm.page_free_nextcolor;

	/*
d909 11
a919 1
	UVM_KICK_PDAEMON();
d959 5
a963 3
			pg = uvm_pagealloc_pgfl(&uvm.page_free[lcv],
			    try1, try2, &color);
			if (pg != NULL)
d974 5
a978 3
		pg = uvm_pagealloc_pgfl(&uvm.page_free[free_list],
		    try1, try2, &color);
		if (pg != NULL)
d996 6
a1001 5
	/*
	 * We now know which color we actually allocated from; set
	 * the next color accordingly.
	 */
	uvm.page_free_nextcolor = (color + 1) & uvmexp.colormask;
d1055 43
d1166 2
a1167 2
		 * it knows it needs to allocate swap if it wants to page the
		 * page out.
d1173 1
a1173 1

d1178 2
a1179 2
		 * changed the loan_count [e.g. in uvmfault_anonget()]) we just
		 * return (when the last loan is dropped, then the page can be
d1183 1
a1183 1
		if (saved_loan_count)
d1209 6
a1214 2
	} else if (pg->pqflags & PQ_INACTIVE) {
		TAILQ_REMOVE(&uvm.page_inactive, pg, pageq);
d1240 1
a1240 2
	    uvm_page_lookup_freelist(pg)].pgfl_buckets[
	    VM_PGCOLOR_BUCKET(pg)].pgfl_queues[PGFL_UNKNOWN], pg, pageq);
d1344 1
a1344 2
 * => try to complete one color bucket at a time, to reduce our impact
 *	on the CPU cache.
d1353 1
a1353 2
	int free_list, s, firstbucket;
	static int nextbucket;
d1355 2
a1356 1
	s = uvm_lock_fpageq();
d1358 2
a1359 3
	firstbucket = nextbucket;
	do {
		if (whichqs != 0) {
d1364 12
a1375 1
		if (uvmexp.zeropages >= UVM_PAGEZERO_TARGET) {
d1381 4
a1384 14
		for (free_list = 0; free_list < VM_NFREELIST; free_list++) {
			pgfl = &uvm.page_free[free_list];
			while ((pg = TAILQ_FIRST(&pgfl->pgfl_buckets[
			    nextbucket].pgfl_queues[PGFL_UNKNOWN])) != NULL) {
				if (whichqs != 0) {
					uvm_unlock_fpageq(s);
					return;
				}

				TAILQ_REMOVE(&pgfl->pgfl_buckets[
				    nextbucket].pgfl_queues[PGFL_UNKNOWN],
				    pg, pageq);
				uvmexp.free--;
				uvm_unlock_fpageq(s);
d1386 15
a1400 17
				if (PMAP_PAGEIDLEZERO(VM_PAGE_TO_PHYS(pg)) ==
				    FALSE) {
					/*
					 * The machine-dependent code detected
					 * some reason for us to abort zeroing
					 * pages, probably because there is a
					 * process now ready to run.
					 */
					s = uvm_lock_fpageq();
					TAILQ_INSERT_HEAD(&pgfl->pgfl_buckets[
					    nextbucket].pgfl_queues[
					    PGFL_UNKNOWN], pg, pageq);
					uvmexp.free++;
					uvmexp.zeroaborts++;
					uvm_unlock_fpageq(s);
					return;
				}
d1402 7
a1408 12
				pmap_zero_page(VM_PAGE_TO_PHYS(pg));
#endif /* PMAP_PAGEIDLEZERO */
				pg->flags |= PG_ZERO;

				s = uvm_lock_fpageq();
				TAILQ_INSERT_HEAD(&pgfl->pgfl_buckets[
				    nextbucket].pgfl_queues[PGFL_ZEROS],
				    pg, pageq);
				uvmexp.free++;
				uvmexp.zeropages++;
			}
		}
d1410 6
a1415 4
		nextbucket = (nextbucket + 1) & uvmexp.colormask;
	} while (nextbucket != firstbucket);

	uvm_unlock_fpageq(s);
@


1.6.4.6
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_page.c,v 1.44 2000/11/27 08:40:04 chs Exp $	*/
d4 1
a4 1
/* 
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and 
d48 1
a48 1
 * 
d54 3
a56 3
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
d58 1
a58 1
 * 
d129 10
d200 4
a203 3
#ifdef UBC
	if (pg->uobject->pgops == &uvm_vnodeops) {
		uvm_pgcnt_vnode--;
a204 1
#endif
d215 13
d230 1
a230 1
 * 
d238 4
a241 3
	vsize_t freepages, pagecount, n;
	vm_page_t pagearray;
	int lcv, i;  
d245 3
a247 1
	 * init the page queues and page queue locks
a249 4
	for (lcv = 0; lcv < VM_NFREELIST; lcv++) {
		for (i = 0; i < PGFL_NQUEUES; i++)
			TAILQ_INIT(&uvm.page_free[lcv].pgfl_queues[i]);
	}
d251 1
a251 2
	TAILQ_INIT(&uvm.page_inactive_swp);
	TAILQ_INIT(&uvm.page_inactive_obj);
d267 1
a267 1
	/* 
d280 1
a280 1
	
d282 1
a282 1
	 * first calculate the number of free pages...  
d288 1
a288 1
	 
d294 8
d310 2
a311 1
	 
d314 12
a325 2
	pagearray = (vm_page_t)uvm_pageboot_alloc(pagecount *
	    sizeof(struct vm_page));
d327 1
a327 1
					 
d351 3
d407 1
a407 1
 * 
d409 1
a409 1
 */   
d432 1
a432 1
#if defined(PMAP_STEAL_MEMORY)
d434 2
a435 16

	/* 
	 * defer bootstrap allocation to MD code (it may want to allocate 
	 * from a direct-mapped segment).  pmap_steal_memory should round
	 * off virtual_space_start/virtual_space_end.
	 */

	addr = pmap_steal_memory(size, &virtual_space_start,
	    &virtual_space_end);

	return(addr);

#else /* !PMAP_STEAL_MEMORY */

	static boolean_t initialized = FALSE;
	vaddr_t addr, vaddr;
d437 1
a437 3

	/* round to page size */
	size = round_page(size);
d452 18
d509 1
d696 2
a697 2
		pgs = (vm_page *)uvm_km_alloc(kernel_map,
		    sizeof(struct vm_page) * npages);
d880 70
d981 43
d1049 1
a1049 1
	int lcv, try1, try2, s, zeroit = 0;
a1050 2
	struct pglist *freeq;
	struct pgfreelist *pgfl;
d1055 4
d1062 9
d1075 1
a1075 11
#ifdef UBC
	if (uvmexp.free + uvmexp.paging < uvmexp.freemin ||
	    (uvmexp.free + uvmexp.paging < uvmexp.freetarg &&
	     uvmexp.inactive < uvmexp.inactarg)) {
		wakeup(&uvm.pagedaemon);
	}
#else
	if (uvmexp.free < uvmexp.freemin || (uvmexp.free < uvmexp.freetarg &&
	    uvmexp.inactive < uvmexp.inactarg))
		wakeup(&uvm.pagedaemon);
#endif
d1115 3
a1117 5
			pgfl = &uvm.page_free[lcv];
			if ((pg = TAILQ_FIRST((freeq =
			      &pgfl->pgfl_queues[try1]))) != NULL ||
			    (pg = TAILQ_FIRST((freeq =
			      &pgfl->pgfl_queues[try2]))) != NULL)
d1128 3
a1130 5
		pgfl = &uvm.page_free[free_list];
		if ((pg = TAILQ_FIRST((freeq =
		      &pgfl->pgfl_queues[try1]))) != NULL ||
		    (pg = TAILQ_FIRST((freeq =
		      &pgfl->pgfl_queues[try2]))) != NULL)
d1148 5
a1152 6
	TAILQ_REMOVE(freeq, pg, pageq);
	uvmexp.free--;

	/* update zero'd page count */
	if (pg->flags & PG_ZERO)
		uvmexp.zeropages--;
d1178 1
a1178 3
#ifdef UBC
		uvm_pgcnt_anon++;
#endif
d1274 2
a1275 2
		 * it knows it needs to allocate swap if it wants to page the 
		 * page out. 
d1281 1
a1281 1
		
d1286 2
a1287 2
		 * changed the loan_count [e.g. in uvmfault_anonget()]) we just 
		 * return (when the last loan is dropped, then the page can be 
d1291 1
a1291 1
		if (saved_loan_count) 
d1317 2
a1318 6
	}
	if (pg->pqflags & PQ_INACTIVE) {
		if (pg->pqflags & PQ_SWAPBACKED)
			TAILQ_REMOVE(&uvm.page_inactive_swp, pg, pageq);
		else
			TAILQ_REMOVE(&uvm.page_inactive_obj, pg, pageq);
d1331 1
a1331 1
#ifdef UBC
d1333 1
a1333 1
		uvm_pgcnt_anon--;
a1334 1
#endif
d1344 2
a1345 1
	    uvm_page_lookup_freelist(pg)].pgfl_queues[PGFL_UNKNOWN], pg, pageq);
d1399 2
d1449 2
a1450 1
 * => we do at least one iteration per call, if we are below the target.
d1459 2
a1460 1
	int free_list, s;
d1462 3
d1466 4
a1469 1
		s = uvm_lock_fpageq();
d1479 42
a1520 3
			if ((pg = TAILQ_FIRST(&pgfl->pgfl_queues[
			    PGFL_UNKNOWN])) != NULL)
				break;
d1523 2
a1524 9
		if (pg == NULL) {
			/*
			 * No non-zero'd pages; don't bother trying again
			 * until we know we have non-zero'd pages free.
			 */
			uvm.page_idle_zero = FALSE;
			uvm_unlock_fpageq(s);
			return;
		}
d1526 1
a1526 35
		TAILQ_REMOVE(&pgfl->pgfl_queues[PGFL_UNKNOWN], pg, pageq);
		uvmexp.free--;
		uvm_unlock_fpageq(s);

#ifdef PMAP_PAGEIDLEZERO
		if (PMAP_PAGEIDLEZERO(VM_PAGE_TO_PHYS(pg)) == FALSE) {
			/*
			 * The machine-dependent code detected some
			 * reason for us to abort zeroing pages,
			 * probably because there is a process now
			 * ready to run.
			 */
			s = uvm_lock_fpageq();
			TAILQ_INSERT_HEAD(&pgfl->pgfl_queues[PGFL_UNKNOWN],
			    pg, pageq);
			uvmexp.free++;
			uvmexp.zeroaborts++;
			uvm_unlock_fpageq(s);
			return;
		}
#else
		/*
		 * XXX This will toast the cache unless the pmap_zero_page()
		 * XXX implementation does uncached access.
		 */
		pmap_zero_page(VM_PAGE_TO_PHYS(pg));
#endif
		pg->flags |= PG_ZERO;

		s = uvm_lock_fpageq();
		TAILQ_INSERT_HEAD(&pgfl->pgfl_queues[PGFL_ZEROS], pg, pageq);
		uvmexp.free++;
		uvmexp.zeropages++;
		uvm_unlock_fpageq(s);
	} while (whichqs == 0);
@


1.6.4.7
log
@Merge in -current from roughly a week ago
@
text
@d132 2
a133 2
static void uvm_pageinsert(struct vm_page *);
static void uvm_pageremove(struct vm_page *);
d480 1
a480 1
static boolean_t uvm_page_physget_freelist(paddr_t *, int);
d837 1
a837 1
void uvm_page_physdump(void); /* SHUT UP GCC */
@


1.6.4.8
log
@Sync the SMP branch with 3.3
@
text
@a310 3
#ifdef __HAVE_VM_PAGE_MD
			VM_MDPAGE_INIT(&vm_physmem[lcv].pgs[i]);
#endif
d613 1
a613 1
		panic("uvm_page_physload: bad free list %d", free_list);
d1043 1
a1043 1
			pmap_zero_page(pg);
d1106 1
a1106 1
		panic("uvm_pagefree: freeing free page %p", pg);
d1340 1
a1340 1
		if (PMAP_PAGEIDLEZERO(pg) == FALSE) {
d1360 1
a1360 1
		pmap_zero_page(pg);
@


1.6.4.9
log
@Sync the SMP branch to -current. This includes moving to ELF.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.6.4.8 2003/03/28 00:08:48 niklas Exp $	*/
d1230 1
a1230 1
	UVMHIST_FUNC("uvm_page_unbusy"); UVMHIST_CALLED(pdhist);
d1235 1
a1235 1
		if (pg == NULL || pg == PGO_DONTCARE) {
d1242 1
a1242 1
			UVMHIST_LOG(pdhist, "releasing pg %p", pg,0,0,0);
d1252 1
a1252 1
			UVMHIST_LOG(pdhist, "unbusying pg %p", pg,0,0,0);
@


1.6.4.10
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_page.c,v 1.6.4.9 2003/05/13 19:36:58 ho Exp $	*/
d517 1
a517 1
				    panic("uvm_page_physget: out of memory!");
@


1.6.4.11
log
@Merge with the trunk
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a466 1
	pmap_update(pmap_kernel());
@


1.5
log
@Change the pmap_enter api to pass down an argument that indicates
the access type that caused this mapping. This is to simplify pmaps
with mod/ref emulation (none for the moment) and in some cases speed
up pmap_is_{referenced,modified}.
At the same time, clean up some mappings that had too high protection.

XXX - the access type is incorrect in old vm, it's only used by uvm and MD code.
The actual use of this in pmap_enter implementations is not in this commit.
@
text
@d974 1
a974 1
	        return 0;
d976 6
a981 3
	if (uvm_map(kernel_map, &addr, size, NULL, 
		    UVM_UNKNOWN_OFFSET, TRUE) != KERN_SUCCESS)
	        addr = 0;
d988 1
a988 1
	        uvm_pagewire(pg);
@


1.4
log
@sync with NetBSD from 1999.05.24 (there is a reason for this date)
 Mostly cleanups, but also a few improvements to pagedaemon for better
 handling of low memory and/or low swap conditions.
@
text
@d441 4
d448 2
a449 1
		    VM_PROT_READ|VM_PROT_WRITE, FALSE);
d990 3
a992 2
		pmap_enter(kernel_map->pmap, temp_addr, VM_PAGE_TO_PHYS(pg),
		    UVM_PROT_READ|UVM_PROT_WRITE, TRUE);
@


1.3
log
@Add uvm_pagealloc_contig
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_page.c,v 1.2 1999/02/26 05:32:07 art Exp $	*/
/*	$NetBSD: uvm_page.c,v 1.15 1998/10/18 23:50:00 chs Exp $	*/
a2 4
/*
 * XXXCDC: "ROUGH DRAFT" QUALITY UVM PRE-RELEASE FILE!
 *         >>>USE AT YOUR OWN RISK, WORK IS NOT FINISHED<<<
 */
d386 1
d394 1
a394 2
	 * on first call to this function init ourselves.   we detect this
	 * by checking virtual_space_start/end which are in the zero'd BSS area.
d396 1
a396 2

	if (virtual_space_start == virtual_space_end) {
d402 2
d409 3
d414 13
d827 1
a827 1
uvm_pagealloc_strat(obj, off, anon, strat, free_list)
d830 1
d837 1
d867 3
a869 2
	if ((uvmexp.free <= uvmexp.reserve_kernel &&
	     !(obj && obj->uo_refs == UVM_OBJ_KERN)) ||
d871 1
a871 1
	     !(obj == uvmexp.kmem_object && curproc == uvm.pagedaemon_proc)))
@


1.2
log
@add OpenBSD tags
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d931 45
@


1.1
log
@Import of uvm from NetBSD. Some local changes, some code disabled
@
text
@d1 1
@

