head	1.230;
access;
symbols
	OPENBSD_6_1:1.229.0.4
	OPENBSD_6_1_BASE:1.229
	OPENBSD_6_0:1.217.0.2
	OPENBSD_6_0_BASE:1.217
	OPENBSD_5_9:1.205.0.2
	OPENBSD_5_9_BASE:1.205
	OPENBSD_5_8:1.192.0.4
	OPENBSD_5_8_BASE:1.192
	OPENBSD_5_7:1.187.0.2
	OPENBSD_5_7_BASE:1.187
	OPENBSD_5_6:1.173.0.4
	OPENBSD_5_6_BASE:1.173
	OPENBSD_5_5:1.164.0.4
	OPENBSD_5_5_BASE:1.164
	OPENBSD_5_4:1.162.0.2
	OPENBSD_5_4_BASE:1.162
	OPENBSD_5_3:1.159.0.2
	OPENBSD_5_3_BASE:1.159
	OPENBSD_5_2:1.157.0.2
	OPENBSD_5_2_BASE:1.157
	OPENBSD_5_1_BASE:1.147
	OPENBSD_5_1:1.147.0.2
	OPENBSD_5_0:1.145.0.2
	OPENBSD_5_0_BASE:1.145
	OPENBSD_4_9:1.131.0.2
	OPENBSD_4_9_BASE:1.131
	OPENBSD_4_8:1.127.0.2
	OPENBSD_4_8_BASE:1.127
	OPENBSD_4_7:1.123.0.2
	OPENBSD_4_7_BASE:1.123
	OPENBSD_4_6:1.118.0.4
	OPENBSD_4_6_BASE:1.118
	OPENBSD_4_5:1.108.0.2
	OPENBSD_4_5_BASE:1.108
	OPENBSD_4_4:1.103.0.2
	OPENBSD_4_4_BASE:1.103
	OPENBSD_4_3:1.99.0.2
	OPENBSD_4_3_BASE:1.99
	OPENBSD_4_2:1.97.0.2
	OPENBSD_4_2_BASE:1.97
	OPENBSD_4_1:1.82.0.4
	OPENBSD_4_1_BASE:1.82
	OPENBSD_4_0:1.82.0.2
	OPENBSD_4_0_BASE:1.82
	OPENBSD_3_9:1.77.0.2
	OPENBSD_3_9_BASE:1.77
	OPENBSD_3_8:1.72.0.2
	OPENBSD_3_8_BASE:1.72
	OPENBSD_3_7:1.70.0.2
	OPENBSD_3_7_BASE:1.70
	OPENBSD_3_6:1.69.0.2
	OPENBSD_3_6_BASE:1.69
	SMP_SYNC_A:1.67
	SMP_SYNC_B:1.67
	OPENBSD_3_5:1.65.0.2
	OPENBSD_3_5_BASE:1.65
	OPENBSD_3_4:1.62.0.2
	OPENBSD_3_4_BASE:1.62
	UBC_SYNC_A:1.59
	OPENBSD_3_3:1.56.0.2
	OPENBSD_3_3_BASE:1.56
	OPENBSD_3_2:1.52.0.2
	OPENBSD_3_2_BASE:1.52
	OPENBSD_3_1:1.45.0.2
	OPENBSD_3_1_BASE:1.45
	UBC_SYNC_B:1.53
	UBC:1.34.0.2
	UBC_BASE:1.34
	OPENBSD_3_0:1.24.0.2
	OPENBSD_3_0_BASE:1.24
	OPENBSD_2_9_BASE:1.11
	OPENBSD_2_9:1.11.0.2
	OPENBSD_2_8:1.6.0.2
	OPENBSD_2_8_BASE:1.6
	OPENBSD_2_7:1.5.0.2
	OPENBSD_2_7_BASE:1.5
	SMP:1.3.0.4
	SMP_BASE:1.3
	kame_19991208:1.3
	OPENBSD_2_6:1.3.0.2
	OPENBSD_2_6_BASE:1.3
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.230
date	2017.04.20.14.13.00;	author visa;	state Exp;
branches;
next	1.229;
commitid	GnoPKa34InShCqYl;

1.229
date	2017.02.14.10.31.15;	author mpi;	state Exp;
branches;
next	1.228;
commitid	PmGi4EGraGC0Z0ml;

1.228
date	2017.02.05.01.08.31;	author guenther;	state Exp;
branches;
next	1.227;
commitid	H63KMOswW4foDHWy;

1.227
date	2017.01.17.17.19.21;	author stefan;	state Exp;
branches;
next	1.226;
commitid	v2YvnjMySRAyIm1v;

1.226
date	2016.11.07.00.26.33;	author guenther;	state Exp;
branches;
next	1.225;
commitid	W7ztnDZwvjCaeQTS;

1.225
date	2016.09.16.02.35.42;	author dlg;	state Exp;
branches;
next	1.224;
commitid	Fei4687v68qad1tP;

1.224
date	2016.09.16.01.09.53;	author dlg;	state Exp;
branches;
next	1.223;
commitid	S1LT7BcQMYzBQOe8;

1.223
date	2016.09.15.02.00.18;	author dlg;	state Exp;
branches;
next	1.222;
commitid	RlO92XR575sygHqm;

1.222
date	2016.09.03.18.43.34;	author stefan;	state Exp;
branches;
next	1.221;
commitid	eetpnLzDQlNozO5P;

1.221
date	2016.08.31.13.13.58;	author stefan;	state Exp;
branches;
next	1.220;
commitid	20yw1FlDaXp29jQ7;

1.220
date	2016.08.11.01.17.33;	author dlg;	state Exp;
branches;
next	1.219;
commitid	3Fyaahd7uscMQePg;

1.219
date	2016.07.30.16.43.44;	author kettenis;	state Exp;
branches;
next	1.218;
commitid	tF5vQRBQc9wvVIoz;

1.218
date	2016.07.29.20.44.40;	author tedu;	state Exp;
branches;
next	1.217;
commitid	lqbEbKUJ4qof7wwE;

1.217
date	2016.06.17.10.48.25;	author dlg;	state Exp;
branches
	1.217.2.1;
next	1.216;
commitid	3jj6c8J8OJvfGS9o;

1.216
date	2016.06.13.17.14.09;	author kettenis;	state Exp;
branches;
next	1.215;
commitid	gbNHsVOnlJDzXyqh;

1.215
date	2016.06.05.08.35.57;	author stefan;	state Exp;
branches;
next	1.214;
commitid	019qqu3X3G1dG85T;

1.214
date	2016.06.03.06.47.51;	author kettenis;	state Exp;
branches;
next	1.213;
commitid	8QGd8SuPi2Wrcow4;

1.213
date	2016.05.08.16.29.57;	author stefan;	state Exp;
branches;
next	1.212;
commitid	szK6LyawtrqhVtcp;

1.212
date	2016.05.05.11.23.39;	author stefan;	state Exp;
branches;
next	1.211;
commitid	HIdJgPWsP79q5pOb;

1.211
date	2016.04.04.16.34.16;	author stefan;	state Exp;
branches;
next	1.210;
commitid	mErYIUO2MMXVvZFw;

1.210
date	2016.03.27.09.51.37;	author stefan;	state Exp;
branches;
next	1.209;
commitid	vrFSc4NQkLUJ1a8U;

1.209
date	2016.03.15.20.50.23;	author krw;	state Exp;
branches;
next	1.208;
commitid	JZR2bOwahEjnBJaG;

1.208
date	2016.03.09.16.45.43;	author deraadt;	state Exp;
branches;
next	1.207;
commitid	RjqAE3tSh8ngurus;

1.207
date	2016.03.06.08.56.16;	author stefan;	state Exp;
branches;
next	1.206;
commitid	6x8rejvlIUCAC7qx;

1.206
date	2016.03.03.12.41.30;	author naddy;	state Exp;
branches;
next	1.205;
commitid	Ykztt9UU7jxBEqeD;

1.205
date	2015.12.16.14.22.21;	author kettenis;	state Exp;
branches
	1.205.2.1;
next	1.204;
commitid	ENvVqV9HycoMAffQ;

1.204
date	2015.11.14.14.53.14;	author miod;	state Exp;
branches;
next	1.203;
commitid	F4YvjgeZAdPdFa0q;

1.203
date	2015.11.11.15.59.33;	author mmcc;	state Exp;
branches;
next	1.202;
commitid	lKxbFcynv6SZSYfh;

1.202
date	2015.10.01.20.27.51;	author kettenis;	state Exp;
branches;
next	1.201;
commitid	AXCpoJAfMpu47rXm;

1.201
date	2015.09.28.18.33.42;	author tedu;	state Exp;
branches;
next	1.200;
commitid	q0NP3po8xG1zGb8N;

1.200
date	2015.09.26.17.55.00;	author kettenis;	state Exp;
branches;
next	1.199;
commitid	Kz7km4wLjVuZXy3p;

1.199
date	2015.09.12.18.54.47;	author kettenis;	state Exp;
branches;
next	1.198;
commitid	QecTBLSiGKMxYkjT;

1.198
date	2015.09.09.23.33.37;	author kettenis;	state Exp;
branches;
next	1.197;
commitid	FRBrgGts03IJfFKV;

1.197
date	2015.09.09.14.52.12;	author miod;	state Exp;
branches;
next	1.196;
commitid	Wbcnl6am8WQAyF85;

1.196
date	2015.09.01.05.49.37;	author deraadt;	state Exp;
branches;
next	1.195;
commitid	ugj36LrBqQhH7xP4;

1.195
date	2015.08.27.21.58.15;	author kettenis;	state Exp;
branches;
next	1.194;
commitid	LM7rPNCZ71Y7XKZE;

1.194
date	2015.08.21.16.04.35;	author visa;	state Exp;
branches;
next	1.193;
commitid	gglpDr80UKmkkP9A;

1.193
date	2015.08.19.12.23.25;	author visa;	state Exp;
branches;
next	1.192;
commitid	JpzUOGqG1VBX1A6L;

1.192
date	2015.07.17.21.56.14;	author kettenis;	state Exp;
branches
	1.192.4.1;
next	1.191;
commitid	54fGtmaEmMoqowTP;

1.191
date	2015.04.23.00.49.37;	author dlg;	state Exp;
branches;
next	1.190;
commitid	b2Am3ZlpJQesGPiM;

1.190
date	2015.03.30.21.09.55;	author miod;	state Exp;
branches;
next	1.189;
commitid	NjSjxIQQRVlatmsO;

1.189
date	2015.03.30.21.08.40;	author miod;	state Exp;
branches;
next	1.188;
commitid	UIgWyHZZqklZyLUa;

1.188
date	2015.03.14.03.38.53;	author jsg;	state Exp;
branches;
next	1.187;
commitid	p4LJxGKbi0BU2cG6;

1.187
date	2015.02.19.03.06.53;	author mlarkin;	state Exp;
branches;
next	1.186;
commitid	IKzA8VsiUR9LaBbY;

1.186
date	2015.02.15.21.34.33;	author miod;	state Exp;
branches;
next	1.185;
commitid	eahBabNpxnDWKzqJ;

1.185
date	2015.02.09.07.14.38;	author kettenis;	state Exp;
branches;
next	1.184;
commitid	Qw77WWE9EADLIx8U;

1.184
date	2015.02.06.11.41.55;	author beck;	state Exp;
branches;
next	1.183;
commitid	Th5rFTtqeDivvpYH;

1.183
date	2015.02.06.09.04.34;	author tedu;	state Exp;
branches;
next	1.182;
commitid	9kMVt50b6Qp3T4s6;

1.182
date	2014.12.23.02.01.57;	author tedu;	state Exp;
branches;
next	1.181;
commitid	q8S2qkrDRKx3mqd8;

1.181
date	2014.12.05.04.12.48;	author uebayasi;	state Exp;
branches;
next	1.180;
commitid	lSMz5LkZVlVJKmFM;

1.180
date	2014.11.30.19.50.53;	author deraadt;	state Exp;
branches;
next	1.179;
commitid	IfJTnP1WH6i8FXb4;

1.179
date	2014.11.18.02.37.31;	author tedu;	state Exp;
branches;
next	1.178;
commitid	Z1vcFtHO8wRH0yRt;

1.178
date	2014.11.16.12.31.00;	author deraadt;	state Exp;
branches;
next	1.177;
commitid	yv0ECmCdICvq576h;

1.177
date	2014.11.13.00.47.44;	author tedu;	state Exp;
branches;
next	1.176;
commitid	t8zRELnW12GnYxX5;

1.176
date	2014.10.03.17.41.00;	author kettenis;	state Exp;
branches;
next	1.175;
commitid	h8HwsnqXqpePzqXu;

1.175
date	2014.08.14.17.21.38;	author miod;	state Exp;
branches;
next	1.174;
commitid	qoxfcd6xnPTAEG4M;

1.174
date	2014.08.12.04.29.05;	author miod;	state Exp;
branches;
next	1.173;
commitid	hv9j3Ft4KjtQ38Ly;

1.173
date	2014.07.13.15.33.28;	author pirofti;	state Exp;
branches;
next	1.172;
commitid	12jZnZViDIHL6faN;

1.172
date	2014.07.13.08.15.16;	author tedu;	state Exp;
branches;
next	1.171;
commitid	jYn0EICz2shfZKcJ;

1.171
date	2014.07.12.18.44.01;	author tedu;	state Exp;
branches;
next	1.170;
commitid	bDGgAR6yEQVcVl5u;

1.170
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.169;
commitid	7NtJNW9udCOFtDNM;

1.169
date	2014.06.13.01.48.52;	author matthew;	state Exp;
branches;
next	1.168;
commitid	ZIoDqOtYqi740uHv;

1.168
date	2014.05.15.03.52.25;	author guenther;	state Exp;
branches;
next	1.167;

1.167
date	2014.04.13.23.14.15;	author tedu;	state Exp;
branches;
next	1.166;

1.166
date	2014.04.10.01.40.04;	author tedu;	state Exp;
branches;
next	1.165;

1.165
date	2014.04.03.21.40.10;	author tedu;	state Exp;
branches;
next	1.164;

1.164
date	2014.01.23.22.06.30;	author miod;	state Exp;
branches;
next	1.163;

1.163
date	2013.09.21.10.01.27;	author miod;	state Exp;
branches;
next	1.162;

1.162
date	2013.05.30.15.17.59;	author tedu;	state Exp;
branches;
next	1.161;

1.161
date	2013.04.17.23.22.42;	author tedu;	state Exp;
branches;
next	1.160;

1.160
date	2013.04.17.17.46.53;	author tedu;	state Exp;
branches;
next	1.159;

1.159
date	2013.02.10.19.19.30;	author beck;	state Exp;
branches;
next	1.158;

1.158
date	2012.10.18.08.46.23;	author gerhard;	state Exp;
branches;
next	1.157;

1.157
date	2012.06.14.15.54.36;	author ariane;	state Exp;
branches;
next	1.156;

1.156
date	2012.06.14.11.57.18;	author jasper;	state Exp;
branches;
next	1.155;

1.155
date	2012.06.03.13.30.04;	author kettenis;	state Exp;
branches;
next	1.154;

1.154
date	2012.06.01.05.47.10;	author guenther;	state Exp;
branches;
next	1.153;

1.153
date	2012.04.19.12.42.03;	author ariane;	state Exp;
branches;
next	1.152;

1.152
date	2012.04.17.20.22.52;	author ariane;	state Exp;
branches;
next	1.151;

1.151
date	2012.04.11.11.23.22;	author ariane;	state Exp;
branches;
next	1.150;

1.150
date	2012.03.15.22.22.28;	author ariane;	state Exp;
branches;
next	1.149;

1.149
date	2012.03.15.17.52.28;	author ariane;	state Exp;
branches;
next	1.148;

1.148
date	2012.03.09.13.01.29;	author ariane;	state Exp;
branches;
next	1.147;

1.147
date	2011.11.24.18.47.34;	author guenther;	state Exp;
branches;
next	1.146;

1.146
date	2011.11.08.11.42.43;	author miod;	state Exp;
branches;
next	1.145;

1.145
date	2011.07.05.03.10.29;	author dhill;	state Exp;
branches;
next	1.144;

1.144
date	2011.07.03.18.36.49;	author oga;	state Exp;
branches;
next	1.143;

1.143
date	2011.07.03.18.34.14;	author oga;	state Exp;
branches;
next	1.142;

1.142
date	2011.06.30.15.51.06;	author tedu;	state Exp;
branches;
next	1.141;

1.141
date	2011.06.06.17.10.23;	author ariane;	state Exp;
branches;
next	1.140;

1.140
date	2011.06.03.16.20.29;	author deraadt;	state Exp;
branches;
next	1.139;

1.139
date	2011.06.01.22.29.25;	author ariane;	state Exp;
branches;
next	1.138;

1.138
date	2011.05.29.17.18.22;	author ariane;	state Exp;
branches;
next	1.137;

1.137
date	2011.05.29.15.18.19;	author ariane;	state Exp;
branches;
next	1.136;

1.136
date	2011.05.24.15.27.36;	author ariane;	state Exp;
branches;
next	1.135;

1.135
date	2011.04.26.23.50.21;	author ariane;	state Exp;
branches;
next	1.134;

1.134
date	2011.04.18.19.23.46;	author art;	state Exp;
branches;
next	1.133;

1.133
date	2011.04.06.15.52.13;	author art;	state Exp;
branches;
next	1.132;

1.132
date	2011.04.05.01.28.05;	author art;	state Exp;
branches;
next	1.131;

1.131
date	2010.12.24.21.49.04;	author tedu;	state Exp;
branches;
next	1.130;

1.130
date	2010.12.15.04.59.52;	author tedu;	state Exp;
branches;
next	1.129;

1.129
date	2010.12.06.20.57.19;	author miod;	state Exp;
branches;
next	1.128;

1.128
date	2010.09.29.18.04.33;	author thib;	state Exp;
branches;
next	1.127;

1.127
date	2010.06.17.16.11.20;	author miod;	state Exp;
branches;
next	1.126;

1.126
date	2010.04.26.05.48.19;	author deraadt;	state Exp;
branches;
next	1.125;

1.125
date	2010.04.23.04.49.46;	author tedu;	state Exp;
branches;
next	1.124;

1.124
date	2010.04.22.19.02.55;	author oga;	state Exp;
branches;
next	1.123;

1.123
date	2009.08.28.00.40.03;	author ariane;	state Exp;
branches;
next	1.122;

1.122
date	2009.08.24.22.45.29;	author miod;	state Exp;
branches;
next	1.121;

1.121
date	2009.08.13.20.40.13;	author ariane;	state Exp;
branches;
next	1.120;

1.120
date	2009.08.06.15.28.14;	author oga;	state Exp;
branches;
next	1.119;

1.119
date	2009.07.25.12.55.40;	author miod;	state Exp;
branches;
next	1.118;

1.118
date	2009.06.17.00.13.59;	author oga;	state Exp;
branches;
next	1.117;

1.117
date	2009.06.16.16.42.41;	author ariane;	state Exp;
branches;
next	1.116;

1.116
date	2009.06.16.00.11.29;	author oga;	state Exp;
branches;
next	1.115;

1.115
date	2009.06.14.02.53.09;	author deraadt;	state Exp;
branches;
next	1.114;

1.114
date	2009.06.09.20.07.59;	author oga;	state Exp;
branches;
next	1.113;

1.113
date	2009.06.06.17.46.44;	author art;	state Exp;
branches;
next	1.112;

1.112
date	2009.06.02.23.00.19;	author oga;	state Exp;
branches;
next	1.111;

1.111
date	2009.06.01.17.42.33;	author ariane;	state Exp;
branches;
next	1.110;

1.110
date	2009.05.02.12.54.42;	author oga;	state Exp;
branches;
next	1.109;

1.109
date	2009.03.25.20.00.18;	author oga;	state Exp;
branches;
next	1.108;

1.108
date	2008.11.10.18.11.59;	author oga;	state Exp;
branches;
next	1.107;

1.107
date	2008.11.04.21.37.06;	author deraadt;	state Exp;
branches;
next	1.106;

1.106
date	2008.10.23.23.54.02;	author tedu;	state Exp;
branches;
next	1.105;

1.105
date	2008.10.08.08.41.19;	author art;	state Exp;
branches;
next	1.104;

1.104
date	2008.09.23.13.25.46;	author art;	state Exp;
branches;
next	1.103;

1.103
date	2008.07.25.12.05.04;	author art;	state Exp;
branches;
next	1.102;

1.102
date	2008.07.25.12.02.09;	author art;	state Exp;
branches;
next	1.101;

1.101
date	2008.07.18.16.40.17;	author kurt;	state Exp;
branches;
next	1.100;

1.100
date	2008.06.09.20.30.23;	author miod;	state Exp;
branches;
next	1.99;

1.99
date	2007.09.15.10.10.37;	author martin;	state Exp;
branches;
next	1.98;

1.98
date	2007.09.10.18.49.45;	author miod;	state Exp;
branches;
next	1.97;

1.97
date	2007.07.18.17.00.20;	author art;	state Exp;
branches;
next	1.96;

1.96
date	2007.06.18.21.51.15;	author pedro;	state Exp;
branches;
next	1.95;

1.95
date	2007.06.13.13.32.26;	author art;	state Exp;
branches;
next	1.94;

1.94
date	2007.06.01.20.10.04;	author tedu;	state Exp;
branches;
next	1.93;

1.93
date	2007.05.31.21.20.30;	author thib;	state Exp;
branches;
next	1.92;

1.92
date	2007.04.27.18.01.49;	author art;	state Exp;
branches;
next	1.91;

1.91
date	2007.04.27.16.23.49;	author art;	state Exp;
branches;
next	1.90;

1.90
date	2007.04.14.14.11.13;	author art;	state Exp;
branches;
next	1.89;

1.89
date	2007.04.13.18.57.49;	author art;	state Exp;
branches;
next	1.88;

1.88
date	2007.04.12.18.59.55;	author art;	state Exp;
branches;
next	1.87;

1.87
date	2007.04.11.12.10.42;	author art;	state Exp;
branches;
next	1.86;

1.86
date	2007.04.04.17.44.45;	author art;	state Exp;
branches;
next	1.85;

1.85
date	2007.03.27.16.13.45;	author art;	state Exp;
branches;
next	1.84;

1.84
date	2007.03.26.08.43.34;	author art;	state Exp;
branches;
next	1.83;

1.83
date	2007.03.25.11.31.07;	author art;	state Exp;
branches;
next	1.82;

1.82
date	2006.07.31.11.51.29;	author mickey;	state Exp;
branches;
next	1.81;

1.81
date	2006.07.26.23.15.55;	author mickey;	state Exp;
branches;
next	1.80;

1.80
date	2006.07.13.22.51.26;	author deraadt;	state Exp;
branches;
next	1.79;

1.79
date	2006.06.21.16.20.05;	author mickey;	state Exp;
branches;
next	1.78;

1.78
date	2006.04.27.15.23.56;	author mickey;	state Exp;
branches;
next	1.77;

1.77
date	2006.01.16.13.11.05;	author mickey;	state Exp;
branches;
next	1.76;

1.76
date	2006.01.02.15.05.45;	author tom;	state Exp;
branches;
next	1.75;

1.75
date	2005.12.10.16.06.10;	author krw;	state Exp;
branches;
next	1.74;

1.74
date	2005.12.10.11.45.43;	author miod;	state Exp;
branches;
next	1.73;

1.73
date	2005.09.28.00.24.03;	author pedro;	state Exp;
branches;
next	1.72;

1.72
date	2005.06.29.06.07.32;	author deraadt;	state Exp;
branches;
next	1.71;

1.71
date	2005.05.24.21.11.47;	author tedu;	state Exp;
branches;
next	1.70;

1.70
date	2004.12.30.08.28.39;	author niklas;	state Exp;
branches;
next	1.69;

1.69
date	2004.08.06.22.39.14;	author deraadt;	state Exp;
branches;
next	1.68;

1.68
date	2004.07.21.01.02.09;	author art;	state Exp;
branches;
next	1.67;

1.67
date	2004.05.30.22.35.43;	author tedu;	state Exp;
branches;
next	1.66;

1.66
date	2004.05.03.07.14.53;	author tedu;	state Exp;
branches;
next	1.65;

1.65
date	2004.02.23.06.19.32;	author drahn;	state Exp;
branches
	1.65.2.1;
next	1.64;

1.64
date	2003.11.18.06.08.19;	author tedu;	state Exp;
branches;
next	1.63;

1.63
date	2003.10.08.22.23.56;	author tedu;	state Exp;
branches;
next	1.62;

1.62
date	2003.09.03.22.52.47;	author tedu;	state Exp;
branches
	1.62.2.1;
next	1.61;

1.61
date	2003.09.02.17.57.12;	author tedu;	state Exp;
branches;
next	1.60;

1.60
date	2003.06.29.17.31.12;	author avsm;	state Exp;
branches;
next	1.59;

1.59
date	2003.05.05.17.54.59;	author drahn;	state Exp;
branches;
next	1.58;

1.58
date	2003.04.17.03.50.54;	author drahn;	state Exp;
branches;
next	1.57;

1.57
date	2003.04.14.04.53.51;	author art;	state Exp;
branches;
next	1.56;

1.56
date	2002.12.09.02.35.21;	author art;	state Exp;
branches;
next	1.55;

1.55
date	2002.10.29.18.30.21;	author art;	state Exp;
branches;
next	1.54;

1.54
date	2002.10.29.01.26.58;	author art;	state Exp;
branches;
next	1.53;

1.53
date	2002.10.17.22.08.37;	author art;	state Exp;
branches;
next	1.52;

1.52
date	2002.09.17.13.01.20;	author mpech;	state Exp;
branches;
next	1.51;

1.51
date	2002.08.30.09.56.22;	author espie;	state Exp;
branches;
next	1.50;

1.50
date	2002.08.20.23.21.17;	author mickey;	state Exp;
branches;
next	1.49;

1.49
date	2002.07.23.15.53.45;	author art;	state Exp;
branches;
next	1.48;

1.48
date	2002.06.14.21.35.00;	author todd;	state Exp;
branches;
next	1.47;

1.47
date	2002.06.05.17.40.08;	author art;	state Exp;
branches;
next	1.46;

1.46
date	2002.04.29.06.26.51;	author pvalchev;	state Exp;
branches;
next	1.45;

1.45
date	2002.03.14.03.16.13;	author millert;	state Exp;
branches;
next	1.44;

1.44
date	2002.03.14.01.27.18;	author millert;	state Exp;
branches;
next	1.43;

1.43
date	2002.03.08.07.25.29;	author mickey;	state Exp;
branches;
next	1.42;

1.42
date	2002.03.07.01.08.57;	author provos;	state Exp;
branches;
next	1.41;

1.41
date	2002.02.28.18.50.26;	author provos;	state Exp;
branches;
next	1.40;

1.40
date	2002.02.25.05.38.50;	author provos;	state Exp;
branches;
next	1.39;

1.39
date	2002.02.25.00.20.45;	author provos;	state Exp;
branches;
next	1.38;

1.38
date	2002.02.18.10.02.20;	author art;	state Exp;
branches;
next	1.37;

1.37
date	2002.01.23.00.39.48;	author art;	state Exp;
branches;
next	1.36;

1.36
date	2002.01.02.22.23.25;	author miod;	state Exp;
branches;
next	1.35;

1.35
date	2001.12.19.08.58.07;	author art;	state Exp;
branches;
next	1.34;

1.34
date	2001.12.04.23.22.42;	author art;	state Exp;
branches
	1.34.2.1;
next	1.33;

1.33
date	2001.11.28.19.28.14;	author art;	state Exp;
branches;
next	1.32;

1.32
date	2001.11.28.13.47.39;	author art;	state Exp;
branches;
next	1.31;

1.31
date	2001.11.12.01.26.09;	author art;	state Exp;
branches;
next	1.30;

1.30
date	2001.11.09.03.32.23;	author art;	state Exp;
branches;
next	1.29;

1.29
date	2001.11.07.02.55.50;	author art;	state Exp;
branches;
next	1.28;

1.28
date	2001.11.07.01.18.01;	author art;	state Exp;
branches;
next	1.27;

1.27
date	2001.11.06.13.36.52;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2001.11.06.01.35.04;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2001.11.05.22.14.54;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2001.09.19.20.50.59;	author mickey;	state Exp;
branches;
next	1.23;

1.23
date	2001.08.11.10.57.22;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2001.08.06.14.03.04;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2001.07.18.14.38.07;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2001.07.18.10.47.05;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2001.07.17.10.55.02;	author mts;	state Exp;
branches;
next	1.18;

1.18
date	2001.07.17.10.31.08;	author mts;	state Exp;
branches;
next	1.17;

1.17
date	2001.07.11.13.57.54;	author mts;	state Exp;
branches;
next	1.16;

1.16
date	2001.06.23.19.24.33;	author smart;	state Exp;
branches;
next	1.15;

1.15
date	2001.05.10.14.51.21;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2001.05.10.07.59.06;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2001.05.07.16.08.40;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2001.05.05.23.25.55;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2001.03.22.03.05.55;	author smart;	state Exp;
branches;
next	1.10;

1.10
date	2001.03.15.10.30.57;	author art;	state Exp;
branches;
next	1.9;

1.9
date	2001.03.09.14.20.51;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2001.01.29.02.07.46;	author niklas;	state Exp;
branches;
next	1.7;

1.7
date	2000.11.10.15.33.11;	author provos;	state Exp;
branches;
next	1.6;

1.6
date	2000.09.07.20.15.28;	author art;	state Exp;
branches;
next	1.5;

1.5
date	2000.03.17.13.53.23;	author deraadt;	state Exp;
branches;
next	1.4;

1.4
date	2000.03.16.22.11.04;	author art;	state Exp;
branches;
next	1.3;

1.3
date	99.08.23.08.13.24;	author art;	state Exp;
branches
	1.3.4.1;
next	1.2;

1.2
date	99.02.26.05.32.07;	author art;	state Exp;
branches;
next	1.1;

1.1
date	99.02.26.01.30.14;	author art;	state Exp;
branches;
next	;

1.3.4.1
date	2000.03.24.09.09.50;	author niklas;	state Exp;
branches;
next	1.3.4.2;

1.3.4.2
date	2001.05.14.22.47.46;	author niklas;	state Exp;
branches;
next	1.3.4.3;

1.3.4.3
date	2001.07.04.11.01.05;	author niklas;	state Exp;
branches;
next	1.3.4.4;

1.3.4.4
date	2001.10.31.03.32.14;	author nate;	state Exp;
branches;
next	1.3.4.5;

1.3.4.5
date	2001.11.13.23.02.31;	author niklas;	state Exp;
branches;
next	1.3.4.6;

1.3.4.6
date	2001.12.05.01.23.58;	author niklas;	state Exp;
branches;
next	1.3.4.7;

1.3.4.7
date	2002.03.06.02.17.14;	author niklas;	state Exp;
branches;
next	1.3.4.8;

1.3.4.8
date	2002.03.28.14.54.26;	author niklas;	state Exp;
branches;
next	1.3.4.9;

1.3.4.9
date	2003.03.28.00.08.48;	author niklas;	state Exp;
branches;
next	1.3.4.10;

1.3.4.10
date	2003.05.13.19.36.58;	author ho;	state Exp;
branches;
next	1.3.4.11;

1.3.4.11
date	2004.02.19.11.01.44;	author niklas;	state Exp;
branches;
next	1.3.4.12;

1.3.4.12
date	2004.06.05.23.13.12;	author niklas;	state Exp;
branches;
next	;

1.34.2.1
date	2002.01.31.22.55.51;	author niklas;	state Exp;
branches;
next	1.34.2.2;

1.34.2.2
date	2002.02.02.03.28.26;	author art;	state Exp;
branches;
next	1.34.2.3;

1.34.2.3
date	2002.06.11.03.33.03;	author art;	state Exp;
branches;
next	1.34.2.4;

1.34.2.4
date	2002.10.29.00.36.50;	author art;	state Exp;
branches;
next	1.34.2.5;

1.34.2.5
date	2002.10.29.02.12.53;	author art;	state Exp;
branches;
next	1.34.2.6;

1.34.2.6
date	2002.11.04.18.02.33;	author art;	state Exp;
branches;
next	1.34.2.7;

1.34.2.7
date	2003.05.19.22.41.29;	author tedu;	state Exp;
branches;
next	1.34.2.8;

1.34.2.8
date	2004.02.21.00.20.22;	author tedu;	state Exp;
branches;
next	;

1.62.2.1
date	2004.06.01.02.54.56;	author brad;	state Exp;
branches;
next	;

1.65.2.1
date	2004.05.14.21.34.24;	author brad;	state Exp;
branches;
next	1.65.2.2;

1.65.2.2
date	2004.06.01.03.44.10;	author brad;	state Exp;
branches;
next	;

1.192.4.1
date	2016.08.01.19.19.14;	author tedu;	state Exp;
branches;
next	;
commitid	J6D0jNdClXEH5tAw;

1.205.2.1
date	2016.08.01.19.20.23;	author tedu;	state Exp;
branches;
next	;
commitid	Y8idNeRXpFJcBw81;

1.217.2.1
date	2016.08.01.19.22.55;	author tedu;	state Exp;
branches;
next	;
commitid	R4iVNwzGFUaRwtSV;


desc
@@


1.230
log
@Tweak lock inits to make the system runnable with witness(4)
on amd64 and i386.
@
text
@/*	$OpenBSD: uvm_map.c,v 1.229 2017/02/14 10:31:15 mpi Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.86 2000/11/27 08:40:03 chs Exp $	*/

/*
 * Copyright (c) 2011 Ariane van der Steldt <ariane@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 *
 * 
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
 *
 * All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * The Mach Operating System project at Carnegie-Mellon University.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)vm_map.c    8.3 (Berkeley) 1/12/94
 * from: Id: uvm_map.c,v 1.1.2.27 1998/02/07 01:16:54 chs Exp
 *
 *
 * Copyright (c) 1987, 1990 Carnegie-Mellon University.
 * All rights reserved.
 * 
 * Permission to use, copy, modify and distribute this software and
 * its documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
 * FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
 * 
 * Carnegie Mellon requests users of this software to return to
 *
 *  Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
 *  School of Computer Science
 *  Carnegie Mellon University
 *  Pittsburgh PA 15213-3890
 *
 * any improvements or extensions that they make and grant Carnegie the
 * rights to redistribute these changes.
 */

/*
 * uvm_map.c: uvm map operations
 */

/* #define DEBUG */
/* #define VMMAP_DEBUG */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/mman.h>
#include <sys/proc.h>
#include <sys/malloc.h>
#include <sys/pool.h>
#include <sys/sysctl.h>

#ifdef SYSVSHM
#include <sys/shm.h>
#endif

#include <uvm/uvm.h>

#ifdef DDB
#include <uvm/uvm_ddb.h>
#endif

#include <uvm/uvm_addr.h>


vsize_t			 uvmspace_dused(struct vm_map*, vaddr_t, vaddr_t);
int			 uvm_mapent_isjoinable(struct vm_map*,
			    struct vm_map_entry*, struct vm_map_entry*);
struct vm_map_entry	*uvm_mapent_merge(struct vm_map*, struct vm_map_entry*,
			    struct vm_map_entry*, struct uvm_map_deadq*);
struct vm_map_entry	*uvm_mapent_tryjoin(struct vm_map*,
			    struct vm_map_entry*, struct uvm_map_deadq*);
struct vm_map_entry	*uvm_map_mkentry(struct vm_map*, struct vm_map_entry*,
			    struct vm_map_entry*, vaddr_t, vsize_t, int,
			    struct uvm_map_deadq*, struct vm_map_entry*);
struct vm_map_entry	*uvm_mapent_alloc(struct vm_map*, int);
void			 uvm_mapent_free(struct vm_map_entry*);
void			 uvm_unmap_kill_entry(struct vm_map*,
			    struct vm_map_entry*);
void			 uvm_unmap_detach_intrsafe(struct uvm_map_deadq *);
void			 uvm_mapent_mkfree(struct vm_map*,
			    struct vm_map_entry*, struct vm_map_entry**,
			    struct uvm_map_deadq*, boolean_t);
void			 uvm_map_pageable_pgon(struct vm_map*,
			    struct vm_map_entry*, struct vm_map_entry*,
			    vaddr_t, vaddr_t);
int			 uvm_map_pageable_wire(struct vm_map*,
			    struct vm_map_entry*, struct vm_map_entry*,
			    vaddr_t, vaddr_t, int);
void			 uvm_map_setup_entries(struct vm_map*);
void			 uvm_map_setup_md(struct vm_map*);
void			 uvm_map_teardown(struct vm_map*);
void			 uvm_map_vmspace_update(struct vm_map*,
			    struct uvm_map_deadq*, int);
void			 uvm_map_kmem_grow(struct vm_map*,
			    struct uvm_map_deadq*, vsize_t, int);
void			 uvm_map_freelist_update_clear(struct vm_map*,
			    struct uvm_map_deadq*);
void			 uvm_map_freelist_update_refill(struct vm_map *, int);
void			 uvm_map_freelist_update(struct vm_map*,
			    struct uvm_map_deadq*, vaddr_t, vaddr_t,
			    vaddr_t, vaddr_t, int);
struct vm_map_entry	*uvm_map_fix_space(struct vm_map*, struct vm_map_entry*,
			    vaddr_t, vaddr_t, int);
int			 uvm_map_sel_limits(vaddr_t*, vaddr_t*, vsize_t, int,
			    struct vm_map_entry*, vaddr_t, vaddr_t, vaddr_t,
			    int);
int			 uvm_map_findspace(struct vm_map*,
			    struct vm_map_entry**, struct vm_map_entry**,
			    vaddr_t*, vsize_t, vaddr_t, vaddr_t, vm_prot_t,
			    vaddr_t);
vsize_t			 uvm_map_addr_augment_get(struct vm_map_entry*);
void			 uvm_map_addr_augment(struct vm_map_entry*);

/*
 * Tree management functions.
 */

static __inline void	 uvm_mapent_copy(struct vm_map_entry*,
			    struct vm_map_entry*);
static inline int	 uvm_mapentry_addrcmp(const struct vm_map_entry*,
			    const struct vm_map_entry*);
void			 uvm_mapent_free_insert(struct vm_map*,
			    struct uvm_addr_state*, struct vm_map_entry*);
void			 uvm_mapent_free_remove(struct vm_map*,
			    struct uvm_addr_state*, struct vm_map_entry*);
void			 uvm_mapent_addr_insert(struct vm_map*,
			    struct vm_map_entry*);
void			 uvm_mapent_addr_remove(struct vm_map*,
			    struct vm_map_entry*);
void			 uvm_map_splitentry(struct vm_map*,
			    struct vm_map_entry*, struct vm_map_entry*,
			    vaddr_t);
vsize_t			 uvm_map_boundary(struct vm_map*, vaddr_t, vaddr_t);
int			 uvm_mapent_bias(struct vm_map*, struct vm_map_entry*);

/*
 * uvm_vmspace_fork helper functions.
 */
struct vm_map_entry	*uvm_mapent_clone(struct vm_map*, vaddr_t, vsize_t,
			    vsize_t, vm_prot_t, vm_prot_t,
			    struct vm_map_entry*, struct uvm_map_deadq*, int,
			    int);
struct vm_map_entry	*uvm_mapent_share(struct vm_map*, vaddr_t, vsize_t,
			    vsize_t, vm_prot_t, vm_prot_t, struct vm_map*,
			    struct vm_map_entry*, struct uvm_map_deadq*);
struct vm_map_entry	*uvm_mapent_forkshared(struct vmspace*, struct vm_map*,
			    struct vm_map*, struct vm_map_entry*,
			    struct uvm_map_deadq*);
struct vm_map_entry	*uvm_mapent_forkcopy(struct vmspace*, struct vm_map*,
			    struct vm_map*, struct vm_map_entry*,
			    struct uvm_map_deadq*);
struct vm_map_entry	*uvm_mapent_forkzero(struct vmspace*, struct vm_map*,
			    struct vm_map*, struct vm_map_entry*,
			    struct uvm_map_deadq*);

/*
 * Tree validation.
 */
#ifdef VMMAP_DEBUG
void			 uvm_tree_assert(struct vm_map*, int, char*,
			    char*, int);
#define UVM_ASSERT(map, cond, file, line)				\
	uvm_tree_assert((map), (cond), #cond, (file), (line))
void			 uvm_tree_sanity(struct vm_map*, char*, int);
void			 uvm_tree_size_chk(struct vm_map*, char*, int);
void			 vmspace_validate(struct vm_map*);
#else
#define uvm_tree_sanity(_map, _file, _line)		do {} while (0)
#define uvm_tree_size_chk(_map, _file, _line)		do {} while (0)
#define vmspace_validate(_map)				do {} while (0)
#endif

/*
 * All architectures will have pmap_prefer.
 */
#ifndef PMAP_PREFER
#define PMAP_PREFER_ALIGN()	(vaddr_t)PAGE_SIZE
#define PMAP_PREFER_OFFSET(off)	0
#define PMAP_PREFER(addr, off)	(addr)
#endif


/*
 * The kernel map will initially be VM_MAP_KSIZE_INIT bytes.
 * Every time that gets cramped, we grow by at least VM_MAP_KSIZE_DELTA bytes.
 *
 * We attempt to grow by UVM_MAP_KSIZE_ALLOCMUL times the allocation size
 * each time.
 */
#define VM_MAP_KSIZE_INIT	(512 * (vaddr_t)PAGE_SIZE)
#define VM_MAP_KSIZE_DELTA	(256 * (vaddr_t)PAGE_SIZE)
#define VM_MAP_KSIZE_ALLOCMUL	4
/*
 * When selecting a random free-space block, look at most FSPACE_DELTA blocks
 * ahead.
 */
#define FSPACE_DELTA		8
/*
 * Put allocations adjecent to previous allocations when the free-space tree
 * is larger than FSPACE_COMPACT entries.
 *
 * Alignment and PMAP_PREFER may still cause the entry to not be fully
 * adjecent. Note that this strategy reduces memory fragmentation (by leaving
 * a large space before or after the allocation).
 */
#define FSPACE_COMPACT		128
/*
 * Make the address selection skip at most this many bytes from the start of
 * the free space in which the allocation takes place.
 *
 * The main idea behind a randomized address space is that an attacker cannot
 * know where to target his attack. Therefore, the location of objects must be
 * as random as possible. However, the goal is not to create the most sparse
 * map that is possible.
 * FSPACE_MAXOFF pushes the considered range in bytes down to less insane
 * sizes, thereby reducing the sparseness. The biggest randomization comes
 * from fragmentation, i.e. FSPACE_COMPACT.
 */
#define FSPACE_MAXOFF		((vaddr_t)32 * 1024 * 1024)
/*
 * Allow for small gaps in the overflow areas.
 * Gap size is in bytes and does not have to be a multiple of page-size.
 */
#define FSPACE_BIASGAP		((vaddr_t)32 * 1024)

/* auto-allocate address lower bound */
#define VMMAP_MIN_ADDR		PAGE_SIZE


#ifdef DEADBEEF0
#define UVMMAP_DEADBEEF		((unsigned long)DEADBEEF0)
#else
#define UVMMAP_DEADBEEF		((unsigned long)0xdeadd0d0)
#endif

#ifdef DEBUG
int uvm_map_printlocks = 0;

#define LPRINTF(_args)							\
	do {								\
		if (uvm_map_printlocks)					\
			printf _args;					\
	} while (0)
#else
#define LPRINTF(_args)	do {} while (0)
#endif

static struct mutex uvm_kmapent_mtx;
static struct timeval uvm_kmapent_last_warn_time;
static struct timeval uvm_kmapent_warn_rate = { 10, 0 };

const char vmmapbsy[] = "vmmapbsy";

/*
 * pool for vmspace structures.
 */
struct pool uvm_vmspace_pool;

/*
 * pool for dynamically-allocated map entries.
 */
struct pool uvm_map_entry_pool;
struct pool uvm_map_entry_kmem_pool;

/*
 * This global represents the end of the kernel virtual address
 * space. If we want to exceed this, we must grow the kernel
 * virtual address space dynamically.
 *
 * Note, this variable is locked by kernel_map's lock.
 */
vaddr_t uvm_maxkaddr;

/*
 * Locking predicate.
 */
#define UVM_MAP_REQ_WRITE(_map)						\
	do {								\
		if ((_map)->ref_count > 0) {				\
			if (((_map)->flags & VM_MAP_INTRSAFE) == 0)	\
				rw_assert_wrlock(&(_map)->lock);	\
			else						\
				MUTEX_ASSERT_LOCKED(&(_map)->mtx);	\
		}							\
	} while (0)

/*
 * Tree describing entries by address.
 *
 * Addresses are unique.
 * Entries with start == end may only exist if they are the first entry
 * (sorted by address) within a free-memory tree.
 */

static inline int
uvm_mapentry_addrcmp(const struct vm_map_entry *e1,
    const struct vm_map_entry *e2)
{
	return e1->start < e2->start ? -1 : e1->start > e2->start;
}

/*
 * Copy mapentry.
 */
static __inline void
uvm_mapent_copy(struct vm_map_entry *src, struct vm_map_entry *dst)
{
	caddr_t csrc, cdst;
	size_t sz;

	csrc = (caddr_t)src;
	cdst = (caddr_t)dst;
	csrc += offsetof(struct vm_map_entry, uvm_map_entry_start_copy);
	cdst += offsetof(struct vm_map_entry, uvm_map_entry_start_copy);

	sz = offsetof(struct vm_map_entry, uvm_map_entry_stop_copy) -
	    offsetof(struct vm_map_entry, uvm_map_entry_start_copy);
	memcpy(cdst, csrc, sz);
}

/*
 * Handle free-list insertion.
 */
void
uvm_mapent_free_insert(struct vm_map *map, struct uvm_addr_state *uaddr,
    struct vm_map_entry *entry)
{
	const struct uvm_addr_functions *fun;
#ifdef VMMAP_DEBUG
	vaddr_t min, max, bound;
#endif

#ifdef VMMAP_DEBUG
	/*
	 * Boundary check.
	 * Boundaries are folded if they go on the same free list.
	 */
	min = VMMAP_FREE_START(entry);
	max = VMMAP_FREE_END(entry);

	while (min < max) {
		bound = uvm_map_boundary(map, min, max);
		KASSERT(uvm_map_uaddr(map, min) == uaddr);
		min = bound;
	}
#endif
	KDASSERT((entry->fspace & (vaddr_t)PAGE_MASK) == 0);
	KASSERT((entry->etype & UVM_ET_FREEMAPPED) == 0);

	UVM_MAP_REQ_WRITE(map);

	/* Actual insert: forward to uaddr pointer. */
	if (uaddr != NULL) {
		fun = uaddr->uaddr_functions;
		KDASSERT(fun != NULL);
		if (fun->uaddr_free_insert != NULL)
			(*fun->uaddr_free_insert)(map, uaddr, entry);
		entry->etype |= UVM_ET_FREEMAPPED;
	}

	/* Update fspace augmentation. */
	uvm_map_addr_augment(entry);
}

/*
 * Handle free-list removal.
 */
void
uvm_mapent_free_remove(struct vm_map *map, struct uvm_addr_state *uaddr,
    struct vm_map_entry *entry)
{
	const struct uvm_addr_functions *fun;

	KASSERT((entry->etype & UVM_ET_FREEMAPPED) != 0 || uaddr == NULL);
	KASSERT(uvm_map_uaddr_e(map, entry) == uaddr);
	UVM_MAP_REQ_WRITE(map);

	if (uaddr != NULL) {
		fun = uaddr->uaddr_functions;
		if (fun->uaddr_free_remove != NULL)
			(*fun->uaddr_free_remove)(map, uaddr, entry);
		entry->etype &= ~UVM_ET_FREEMAPPED;
	}
}

/*
 * Handle address tree insertion.
 */
void
uvm_mapent_addr_insert(struct vm_map *map, struct vm_map_entry *entry)
{
	struct vm_map_entry *res;

	if (!RBT_CHECK(uvm_map_addr, entry, UVMMAP_DEADBEEF))
		panic("uvm_mapent_addr_insert: entry still in addr list");
	KDASSERT(entry->start <= entry->end);
	KDASSERT((entry->start & (vaddr_t)PAGE_MASK) == 0 &&
	    (entry->end & (vaddr_t)PAGE_MASK) == 0);

	UVM_MAP_REQ_WRITE(map);
	res = RBT_INSERT(uvm_map_addr, &map->addr, entry);
	if (res != NULL) {
		panic("uvm_mapent_addr_insert: map %p entry %p "
		    "(0x%lx-0x%lx G=0x%lx F=0x%lx) insert collision "
		    "with entry %p (0x%lx-0x%lx G=0x%lx F=0x%lx)",
		    map, entry,
		    entry->start, entry->end, entry->guard, entry->fspace,
		    res, res->start, res->end, res->guard, res->fspace);
	}
}

/*
 * Handle address tree removal.
 */
void
uvm_mapent_addr_remove(struct vm_map *map, struct vm_map_entry *entry)
{
	struct vm_map_entry *res;

	UVM_MAP_REQ_WRITE(map);
	res = RBT_REMOVE(uvm_map_addr, &map->addr, entry);
	if (res != entry)
		panic("uvm_mapent_addr_remove");
	RBT_POISON(uvm_map_addr, entry, UVMMAP_DEADBEEF);
}

/*
 * uvm_map_reference: add reference to a map
 *
 * XXX check map reference counter lock
 */
#define uvm_map_reference(_map)						\
	do {								\
		map->ref_count++;					\
	} while (0)

/*
 * Calculate the dused delta.
 */
vsize_t
uvmspace_dused(struct vm_map *map, vaddr_t min, vaddr_t max)
{
	struct vmspace *vm;
	vsize_t sz;
	vaddr_t lmax;
	vaddr_t stack_begin, stack_end; /* Position of stack. */

	KASSERT(map->flags & VM_MAP_ISVMSPACE);
	vm = (struct vmspace *)map;
	stack_begin = MIN((vaddr_t)vm->vm_maxsaddr, (vaddr_t)vm->vm_minsaddr);
	stack_end = MAX((vaddr_t)vm->vm_maxsaddr, (vaddr_t)vm->vm_minsaddr);

	sz = 0;
	while (min != max) {
		lmax = max;
		if (min < stack_begin && lmax > stack_begin)
			lmax = stack_begin;
		else if (min < stack_end && lmax > stack_end)
			lmax = stack_end;

		if (min >= stack_begin && min < stack_end) {
			/* nothing */
		} else
			sz += lmax - min;
		min = lmax;
	}

	return sz >> PAGE_SHIFT;
}

/*
 * Find the entry describing the given address.
 */
struct vm_map_entry*
uvm_map_entrybyaddr(struct uvm_map_addr *atree, vaddr_t addr)
{
	struct vm_map_entry *iter;

	iter = RBT_ROOT(uvm_map_addr, atree);
	while (iter != NULL) {
		if (iter->start > addr)
			iter = RBT_LEFT(uvm_map_addr, iter);
		else if (VMMAP_FREE_END(iter) <= addr)
			iter = RBT_RIGHT(uvm_map_addr, iter);
		else
			return iter;
	}
	return NULL;
}

/*
 * DEAD_ENTRY_PUSH(struct vm_map_deadq *deadq, struct vm_map_entry *entry)
 *
 * Push dead entries into a linked list.
 * Since the linked list abuses the address tree for storage, the entry
 * may not be linked in a map.
 *
 * *head must be initialized to NULL before the first call to this macro.
 * uvm_unmap_detach(*head, 0) will remove dead entries.
 */
static __inline void
dead_entry_push(struct uvm_map_deadq *deadq, struct vm_map_entry *entry)
{
	TAILQ_INSERT_TAIL(deadq, entry, dfree.deadq);
}
#define DEAD_ENTRY_PUSH(_headptr, _entry)				\
	dead_entry_push((_headptr), (_entry))

/*
 * Helper function for uvm_map_findspace_tree.
 *
 * Given allocation constraints and pmap constraints, finds the
 * lowest and highest address in a range that can be used for the
 * allocation.
 *
 * pmap_align and pmap_off are ignored on non-PMAP_PREFER archs.
 *
 *
 * Big chunk of math with a seasoning of dragons.
 */
int
uvm_map_sel_limits(vaddr_t *min, vaddr_t *max, vsize_t sz, int guardpg,
    struct vm_map_entry *sel, vaddr_t align,
    vaddr_t pmap_align, vaddr_t pmap_off, int bias)
{
	vaddr_t sel_min, sel_max;
#ifdef PMAP_PREFER
	vaddr_t pmap_min, pmap_max;
#endif /* PMAP_PREFER */
#ifdef DIAGNOSTIC
	int bad;
#endif /* DIAGNOSTIC */

	sel_min = VMMAP_FREE_START(sel);
	sel_max = VMMAP_FREE_END(sel) - sz - (guardpg ? PAGE_SIZE : 0);

#ifdef PMAP_PREFER

	/*
	 * There are two special cases, in which we can satisfy the align
	 * requirement and the pmap_prefer requirement.
	 * - when pmap_off == 0, we always select the largest of the two
	 * - when pmap_off % align == 0 and pmap_align > align, we simply
	 *   satisfy the pmap_align requirement and automatically
	 *   satisfy the align requirement.
	 */
	if (align > PAGE_SIZE &&
	    !(pmap_align > align && (pmap_off & (align - 1)) == 0)) {
		/*
		 * Simple case: only use align.
		 */
		sel_min = roundup(sel_min, align);
		sel_max &= ~(align - 1);

		if (sel_min > sel_max)
			return ENOMEM;

		/* Correct for bias. */
		if (sel_max - sel_min > FSPACE_BIASGAP) {
			if (bias > 0) {
				sel_min = sel_max - FSPACE_BIASGAP;
				sel_min = roundup(sel_min, align);
			} else if (bias < 0) {
				sel_max = sel_min + FSPACE_BIASGAP;
				sel_max &= ~(align - 1);
			}
		}
	} else if (pmap_align != 0) {
		/*
		 * Special case: satisfy both pmap_prefer and
		 * align argument.
		 */
		pmap_max = sel_max & ~(pmap_align - 1);
		pmap_min = sel_min;
		if (pmap_max < sel_min)
			return ENOMEM;

		/* Adjust pmap_min for BIASGAP for top-addr bias. */
		if (bias > 0 && pmap_max - pmap_min > FSPACE_BIASGAP)
			pmap_min = pmap_max - FSPACE_BIASGAP;
		/* Align pmap_min. */
		pmap_min &= ~(pmap_align - 1);
		if (pmap_min < sel_min)
			pmap_min += pmap_align;
		if (pmap_min > pmap_max)
			return ENOMEM;

		/* Adjust pmap_max for BIASGAP for bottom-addr bias. */
		if (bias < 0 && pmap_max - pmap_min > FSPACE_BIASGAP) {
			pmap_max = (pmap_min + FSPACE_BIASGAP) &
			    ~(pmap_align - 1);
		}
		if (pmap_min > pmap_max)
			return ENOMEM;

		/* Apply pmap prefer offset. */
		pmap_max |= pmap_off;
		if (pmap_max > sel_max)
			pmap_max -= pmap_align;
		pmap_min |= pmap_off;
		if (pmap_min < sel_min)
			pmap_min += pmap_align;

		/*
		 * Fixup: it's possible that pmap_min and pmap_max
		 * cross eachother. In this case, try to find one
		 * address that is allowed.
		 * (This usually happens in biased case.)
		 */
		if (pmap_min > pmap_max) {
			if (pmap_min < sel_max)
				pmap_max = pmap_min;
			else if (pmap_max > sel_min)
				pmap_min = pmap_max;
			else
				return ENOMEM;
		}

		/* Internal validation. */
		KDASSERT(pmap_min <= pmap_max);

		sel_min = pmap_min;
		sel_max = pmap_max;
	} else if (bias > 0 && sel_max - sel_min > FSPACE_BIASGAP)
		sel_min = sel_max - FSPACE_BIASGAP;
	else if (bias < 0 && sel_max - sel_min > FSPACE_BIASGAP)
		sel_max = sel_min + FSPACE_BIASGAP;

#else

	if (align > PAGE_SIZE) {
		sel_min = roundup(sel_min, align);
		sel_max &= ~(align - 1);
		if (sel_min > sel_max)
			return ENOMEM;

		if (bias != 0 && sel_max - sel_min > FSPACE_BIASGAP) {
			if (bias > 0) {
				sel_min = roundup(sel_max - FSPACE_BIASGAP,
				    align);
			} else {
				sel_max = (sel_min + FSPACE_BIASGAP) &
				    ~(align - 1);
			}
		}
	} else if (bias > 0 && sel_max - sel_min > FSPACE_BIASGAP)
		sel_min = sel_max - FSPACE_BIASGAP;
	else if (bias < 0 && sel_max - sel_min > FSPACE_BIASGAP)
		sel_max = sel_min + FSPACE_BIASGAP;

#endif

	if (sel_min > sel_max)
		return ENOMEM;

#ifdef DIAGNOSTIC
	bad = 0;
	/* Lower boundary check. */
	if (sel_min < VMMAP_FREE_START(sel)) {
		printf("sel_min: 0x%lx, but should be at least 0x%lx\n",
		    sel_min, VMMAP_FREE_START(sel));
		bad++;
	}
	/* Upper boundary check. */
	if (sel_max > VMMAP_FREE_END(sel) - sz - (guardpg ? PAGE_SIZE : 0)) {
		printf("sel_max: 0x%lx, but should be at most 0x%lx\n",
		    sel_max,
		    VMMAP_FREE_END(sel) - sz - (guardpg ? PAGE_SIZE : 0));
		bad++;
	}
	/* Lower boundary alignment. */
	if (align != 0 && (sel_min & (align - 1)) != 0) {
		printf("sel_min: 0x%lx, not aligned to 0x%lx\n",
		    sel_min, align);
		bad++;
	}
	/* Upper boundary alignment. */
	if (align != 0 && (sel_max & (align - 1)) != 0) {
		printf("sel_max: 0x%lx, not aligned to 0x%lx\n",
		    sel_max, align);
		bad++;
	}
	/* Lower boundary PMAP_PREFER check. */
	if (pmap_align != 0 && align == 0 &&
	    (sel_min & (pmap_align - 1)) != pmap_off) {
		printf("sel_min: 0x%lx, aligned to 0x%lx, expected 0x%lx\n",
		    sel_min, sel_min & (pmap_align - 1), pmap_off);
		bad++;
	}
	/* Upper boundary PMAP_PREFER check. */
	if (pmap_align != 0 && align == 0 &&
	    (sel_max & (pmap_align - 1)) != pmap_off) {
		printf("sel_max: 0x%lx, aligned to 0x%lx, expected 0x%lx\n",
		    sel_max, sel_max & (pmap_align - 1), pmap_off);
		bad++;
	}

	if (bad) {
		panic("uvm_map_sel_limits(sz = %lu, guardpg = %c, "
		    "align = 0x%lx, pmap_align = 0x%lx, pmap_off = 0x%lx, "
		    "bias = %d, "
		    "FREE_START(sel) = 0x%lx, FREE_END(sel) = 0x%lx)",
		    sz, (guardpg ? 'T' : 'F'), align, pmap_align, pmap_off,
		    bias, VMMAP_FREE_START(sel), VMMAP_FREE_END(sel));
	}
#endif /* DIAGNOSTIC */

	*min = sel_min;
	*max = sel_max;
	return 0;
}

/*
 * Test if memory starting at addr with sz bytes is free.
 *
 * Fills in *start_ptr and *end_ptr to be the first and last entry describing
 * the space.
 * If called with prefilled *start_ptr and *end_ptr, they are to be correct.
 */
int
uvm_map_isavail(struct vm_map *map, struct uvm_addr_state *uaddr,
    struct vm_map_entry **start_ptr, struct vm_map_entry **end_ptr,
    vaddr_t addr, vsize_t sz)
{
	struct uvm_addr_state *free;
	struct uvm_map_addr *atree;
	struct vm_map_entry *i, *i_end;

	if (addr + sz < addr)
		return 0;

	/*
	 * Kernel memory above uvm_maxkaddr is considered unavailable.
	 */
	if ((map->flags & VM_MAP_ISVMSPACE) == 0) {
		if (addr + sz > uvm_maxkaddr)
			return 0;
	}

	atree = &map->addr;

	/*
	 * Fill in first, last, so they point at the entries containing the
	 * first and last address of the range.
	 * Note that if they are not NULL, we don't perform the lookup.
	 */
	KDASSERT(atree != NULL && start_ptr != NULL && end_ptr != NULL);
	if (*start_ptr == NULL) {
		*start_ptr = uvm_map_entrybyaddr(atree, addr);
		if (*start_ptr == NULL)
			return 0;
	} else
		KASSERT(*start_ptr == uvm_map_entrybyaddr(atree, addr));
	if (*end_ptr == NULL) {
		if (VMMAP_FREE_END(*start_ptr) >= addr + sz)
			*end_ptr = *start_ptr;
		else {
			*end_ptr = uvm_map_entrybyaddr(atree, addr + sz - 1);
			if (*end_ptr == NULL)
				return 0;
		}
	} else
		KASSERT(*end_ptr == uvm_map_entrybyaddr(atree, addr + sz - 1));

	/* Validation. */
	KDASSERT(*start_ptr != NULL && *end_ptr != NULL);
	KDASSERT((*start_ptr)->start <= addr &&
	    VMMAP_FREE_END(*start_ptr) > addr &&
	    (*end_ptr)->start < addr + sz &&
	    VMMAP_FREE_END(*end_ptr) >= addr + sz);

	/*
	 * Check the none of the entries intersects with <addr, addr+sz>.
	 * Also, if the entry belong to uaddr_exe or uaddr_brk_stack, it is
	 * considered unavailable unless called by those allocators.
	 */
	i = *start_ptr;
	i_end = RBT_NEXT(uvm_map_addr, *end_ptr);
	for (; i != i_end;
	    i = RBT_NEXT(uvm_map_addr, i)) {
		if (i->start != i->end && i->end > addr)
			return 0;

		/*
		 * uaddr_exe and uaddr_brk_stack may only be used
		 * by these allocators and the NULL uaddr (i.e. no
		 * uaddr).
		 * Reject if this requirement is not met.
		 */
		if (uaddr != NULL) {
			free = uvm_map_uaddr_e(map, i);

			if (uaddr != free && free != NULL &&
			    (free == map->uaddr_exe ||
			     free == map->uaddr_brk_stack))
				return 0;
		}
	}

	return -1;
}

/*
 * Invoke each address selector until an address is found.
 * Will not invoke uaddr_exe.
 */
int
uvm_map_findspace(struct vm_map *map, struct vm_map_entry**first,
    struct vm_map_entry**last, vaddr_t *addr, vsize_t sz,
    vaddr_t pmap_align, vaddr_t pmap_offset, vm_prot_t prot, vaddr_t hint)
{
	struct uvm_addr_state *uaddr;
	int i;

	/*
	 * Allocation for sz bytes at any address,
	 * using the addr selectors in order.
	 */
	for (i = 0; i < nitems(map->uaddr_any); i++) {
		uaddr = map->uaddr_any[i];

		if (uvm_addr_invoke(map, uaddr, first, last,
		    addr, sz, pmap_align, pmap_offset, prot, hint) == 0)
			return 0;
	}

	/* Fall back to brk() and stack() address selectors. */
	uaddr = map->uaddr_brk_stack;
	if (uvm_addr_invoke(map, uaddr, first, last,
	    addr, sz, pmap_align, pmap_offset, prot, hint) == 0)
		return 0;

	return ENOMEM;
}

/* Calculate entry augmentation value. */
vsize_t
uvm_map_addr_augment_get(struct vm_map_entry *entry)
{
	vsize_t			 augment;
	struct vm_map_entry	*left, *right;

	augment = entry->fspace;
	if ((left = RBT_LEFT(uvm_map_addr, entry)) != NULL)
		augment = MAX(augment, left->fspace_augment);
	if ((right = RBT_RIGHT(uvm_map_addr, entry)) != NULL)
		augment = MAX(augment, right->fspace_augment);
	return augment;
}

/*
 * Update augmentation data in entry.
 */
void
uvm_map_addr_augment(struct vm_map_entry *entry)
{
	vsize_t			 augment;

	while (entry != NULL) {
		/* Calculate value for augmentation. */
		augment = uvm_map_addr_augment_get(entry);

		/*
		 * Descend update.
		 * Once we find an entry that already has the correct value,
		 * stop, since it means all its parents will use the correct
		 * value too.
		 */
		if (entry->fspace_augment == augment)
			return;
		entry->fspace_augment = augment;
		entry = RBT_PARENT(uvm_map_addr, entry);
	}
}

/*
 * uvm_mapanon: establish a valid mapping in map for an anon
 *
 * => *addr and sz must be a multiple of PAGE_SIZE.
 * => *addr is ignored, except if flags contains UVM_FLAG_FIXED.
 * => map must be unlocked.
 *
 * => align: align vaddr, must be a power-of-2.
 *    Align is only a hint and will be ignored if the alignment fails.
 */
int
uvm_mapanon(struct vm_map *map, vaddr_t *addr, vsize_t sz,
    vsize_t align, unsigned int flags)
{
	struct vm_map_entry	*first, *last, *entry, *new;
	struct uvm_map_deadq	 dead;
	vm_prot_t		 prot;
	vm_prot_t		 maxprot;
	vm_inherit_t		 inherit;
	int			 advice;
	int			 error;
	vaddr_t			 pmap_align, pmap_offset;
	vaddr_t			 hint;

	KASSERT((map->flags & VM_MAP_ISVMSPACE) == VM_MAP_ISVMSPACE);
	KASSERT(map != kernel_map);
	KASSERT((map->flags & UVM_FLAG_HOLE) == 0);

	KASSERT((map->flags & VM_MAP_INTRSAFE) == 0);
	splassert(IPL_NONE);

	/*
	 * We use pmap_align and pmap_offset as alignment and offset variables.
	 *
	 * Because the align parameter takes precedence over pmap prefer,
	 * the pmap_align will need to be set to align, with pmap_offset = 0,
	 * if pmap_prefer will not align.
	 */
	pmap_align = MAX(align, PAGE_SIZE);
	pmap_offset = 0;

	/* Decode parameters. */
	prot = UVM_PROTECTION(flags);
	maxprot = UVM_MAXPROTECTION(flags);
	advice = UVM_ADVICE(flags);
	inherit = UVM_INHERIT(flags);
	error = 0;
	hint = trunc_page(*addr);
	TAILQ_INIT(&dead);
	KASSERT((sz & (vaddr_t)PAGE_MASK) == 0);
	KASSERT((align & (align - 1)) == 0);

	/* Check protection. */
	if ((prot & maxprot) != prot)
		return EACCES;

	/*
	 * Before grabbing the lock, allocate a map entry for later
	 * use to ensure we don't wait for memory while holding the
	 * vm_map_lock.
	 */
	new = uvm_mapent_alloc(map, flags);
	if (new == NULL)
		return(ENOMEM);

	if (flags & UVM_FLAG_TRYLOCK) {
		if (vm_map_lock_try(map) == FALSE) {
			error = EFAULT;
			goto out;
		}
	} else
		vm_map_lock(map);

	first = last = NULL;
	if (flags & UVM_FLAG_FIXED) {
		/*
		 * Fixed location.
		 *
		 * Note: we ignore align, pmap_prefer.
		 * Fill in first, last and *addr.
		 */
		KASSERT((*addr & PAGE_MASK) == 0);

		/* Check that the space is available. */
		if (flags & UVM_FLAG_UNMAP)
			uvm_unmap_remove(map, *addr, *addr + sz, &dead, FALSE, TRUE);
		if (!uvm_map_isavail(map, NULL, &first, &last, *addr, sz)) {
			error = ENOMEM;
			goto unlock;
		}
	} else if (*addr != 0 && (*addr & PAGE_MASK) == 0 &&
	    (align == 0 || (*addr & (align - 1)) == 0) &&
	    uvm_map_isavail(map, NULL, &first, &last, *addr, sz)) {
		/*
		 * Address used as hint.
		 *
		 * Note: we enforce the alignment restriction,
		 * but ignore pmap_prefer.
		 */
	} else if ((prot & PROT_EXEC) != 0 && map->uaddr_exe != NULL) {
		/* Run selection algorithm for executables. */
		error = uvm_addr_invoke(map, map->uaddr_exe, &first, &last,
		    addr, sz, pmap_align, pmap_offset, prot, hint);

		if (error != 0)
			goto unlock;
	} else {
		/* Update freelists from vmspace. */
		uvm_map_vmspace_update(map, &dead, flags);

		error = uvm_map_findspace(map, &first, &last, addr, sz,
		    pmap_align, pmap_offset, prot, hint);

		if (error != 0)
			goto unlock;
	}

	/* Double-check if selected address doesn't cause overflow. */
	if (*addr + sz < *addr) {
		error = ENOMEM;
		goto unlock;
	}

	/* If we only want a query, return now. */
	if (flags & UVM_FLAG_QUERY) {
		error = 0;
		goto unlock;
	}

	/*
	 * Create new entry.
	 * first and last may be invalidated after this call.
	 */
	entry = uvm_map_mkentry(map, first, last, *addr, sz, flags, &dead,
	    new);
	if (entry == NULL) {
		error = ENOMEM;
		goto unlock;
	}
	new = NULL;
	KDASSERT(entry->start == *addr && entry->end == *addr + sz);
	entry->object.uvm_obj = NULL;
	entry->offset = 0;
	entry->protection = prot;
	entry->max_protection = maxprot;
	entry->inheritance = inherit;
	entry->wired_count = 0;
	entry->advice = advice;
	if (flags & UVM_FLAG_NOFAULT)
		entry->etype |= UVM_ET_NOFAULT;
	if (flags & UVM_FLAG_COPYONW) {
		entry->etype |= UVM_ET_COPYONWRITE;
		if ((flags & UVM_FLAG_OVERLAY) == 0)
			entry->etype |= UVM_ET_NEEDSCOPY;
	}
	if (flags & UVM_FLAG_OVERLAY) {
		KERNEL_LOCK();
		entry->aref.ar_pageoff = 0;
		entry->aref.ar_amap = amap_alloc(sz, M_WAITOK, 0);
		KERNEL_UNLOCK();
	}

	/* Update map and process statistics. */
	map->size += sz;
	((struct vmspace *)map)->vm_dused += uvmspace_dused(map, *addr, *addr + sz);

unlock:
	vm_map_unlock(map);

	/*
	 * Remove dead entries.
	 *
	 * Dead entries may be the result of merging.
	 * uvm_map_mkentry may also create dead entries, when it attempts to
	 * destroy free-space entries.
	 */
	uvm_unmap_detach(&dead, 0);
out:
	if (new)
		uvm_mapent_free(new);
	return error;
}

/*
 * uvm_map: establish a valid mapping in map
 *
 * => *addr and sz must be a multiple of PAGE_SIZE.
 * => map must be unlocked.
 * => <uobj,uoffset> value meanings (4 cases):
 *	[1] <NULL,uoffset>		== uoffset is a hint for PMAP_PREFER
 *	[2] <NULL,UVM_UNKNOWN_OFFSET>	== don't PMAP_PREFER
 *	[3] <uobj,uoffset>		== normal mapping
 *	[4] <uobj,UVM_UNKNOWN_OFFSET>	== uvm_map finds offset based on VA
 *
 *   case [4] is for kernel mappings where we don't know the offset until
 *   we've found a virtual address.   note that kernel object offsets are
 *   always relative to vm_map_min(kernel_map).
 *
 * => align: align vaddr, must be a power-of-2.
 *    Align is only a hint and will be ignored if the alignment fails.
 */
int
uvm_map(struct vm_map *map, vaddr_t *addr, vsize_t sz,
    struct uvm_object *uobj, voff_t uoffset,
    vsize_t align, unsigned int flags)
{
	struct vm_map_entry	*first, *last, *entry, *new;
	struct uvm_map_deadq	 dead;
	vm_prot_t		 prot;
	vm_prot_t		 maxprot;
	vm_inherit_t		 inherit;
	int			 advice;
	int			 error;
	vaddr_t			 pmap_align, pmap_offset;
	vaddr_t			 hint;

	if ((map->flags & VM_MAP_INTRSAFE) == 0)
		splassert(IPL_NONE);
	else
		splassert(IPL_VM);

	/*
	 * We use pmap_align and pmap_offset as alignment and offset variables.
	 *
	 * Because the align parameter takes precedence over pmap prefer,
	 * the pmap_align will need to be set to align, with pmap_offset = 0,
	 * if pmap_prefer will not align.
	 */
	if (uoffset == UVM_UNKNOWN_OFFSET) {
		pmap_align = MAX(align, PAGE_SIZE);
		pmap_offset = 0;
	} else {
		pmap_align = MAX(PMAP_PREFER_ALIGN(), PAGE_SIZE);
		pmap_offset = PMAP_PREFER_OFFSET(uoffset);

		if (align == 0 ||
		    (align <= pmap_align && (pmap_offset & (align - 1)) == 0)) {
			/* pmap_offset satisfies align, no change. */
		} else {
			/* Align takes precedence over pmap prefer. */
			pmap_align = align;
			pmap_offset = 0;
		}
	}

	/* Decode parameters. */
	prot = UVM_PROTECTION(flags);
	maxprot = UVM_MAXPROTECTION(flags);
	advice = UVM_ADVICE(flags);
	inherit = UVM_INHERIT(flags);
	error = 0;
	hint = trunc_page(*addr);
	TAILQ_INIT(&dead);
	KASSERT((sz & (vaddr_t)PAGE_MASK) == 0);
	KASSERT((align & (align - 1)) == 0);

	/* Holes are incompatible with other types of mappings. */
	if (flags & UVM_FLAG_HOLE) {
		KASSERT(uobj == NULL && (flags & UVM_FLAG_FIXED) &&
		    (flags & (UVM_FLAG_OVERLAY | UVM_FLAG_COPYONW)) == 0);
	}

	/* Unset hint for kernel_map non-fixed allocations. */
	if (!(map->flags & VM_MAP_ISVMSPACE) && !(flags & UVM_FLAG_FIXED))
		hint = 0;

	/* Check protection. */
	if ((prot & maxprot) != prot)
		return EACCES;

	if (map == kernel_map &&
	    (prot & (PROT_WRITE | PROT_EXEC)) == (PROT_WRITE | PROT_EXEC))
		panic("uvm_map: kernel map W^X violation requested");

	/*
	 * Before grabbing the lock, allocate a map entry for later
	 * use to ensure we don't wait for memory while holding the
	 * vm_map_lock.
	 */
	new = uvm_mapent_alloc(map, flags);
	if (new == NULL)
		return(ENOMEM);

	if (flags & UVM_FLAG_TRYLOCK) {
		if (vm_map_lock_try(map) == FALSE) {
			error = EFAULT;
			goto out;
		}
	} else {
		vm_map_lock(map);
	}

	first = last = NULL;
	if (flags & UVM_FLAG_FIXED) {
		/*
		 * Fixed location.
		 *
		 * Note: we ignore align, pmap_prefer.
		 * Fill in first, last and *addr.
		 */
		KASSERT((*addr & PAGE_MASK) == 0);

		/*
		 * Grow pmap to include allocated address.
		 * If the growth fails, the allocation will fail too.
		 */
		if ((map->flags & VM_MAP_ISVMSPACE) == 0 &&
		    uvm_maxkaddr < (*addr + sz)) {
			uvm_map_kmem_grow(map, &dead,
			    *addr + sz - uvm_maxkaddr, flags);
		}

		/* Check that the space is available. */
		if (flags & UVM_FLAG_UNMAP)
			uvm_unmap_remove(map, *addr, *addr + sz, &dead, FALSE, TRUE);
		if (!uvm_map_isavail(map, NULL, &first, &last, *addr, sz)) {
			error = ENOMEM;
			goto unlock;
		}
	} else if (*addr != 0 && (*addr & PAGE_MASK) == 0 &&
	    (map->flags & VM_MAP_ISVMSPACE) == VM_MAP_ISVMSPACE &&
	    (align == 0 || (*addr & (align - 1)) == 0) &&
	    uvm_map_isavail(map, NULL, &first, &last, *addr, sz)) {
		/*
		 * Address used as hint.
		 *
		 * Note: we enforce the alignment restriction,
		 * but ignore pmap_prefer.
		 */
	} else if ((prot & PROT_EXEC) != 0 && map->uaddr_exe != NULL) {
		/* Run selection algorithm for executables. */
		error = uvm_addr_invoke(map, map->uaddr_exe, &first, &last,
		    addr, sz, pmap_align, pmap_offset, prot, hint);

		/* Grow kernel memory and try again. */
		if (error != 0 && (map->flags & VM_MAP_ISVMSPACE) == 0) {
			uvm_map_kmem_grow(map, &dead, sz, flags);

			error = uvm_addr_invoke(map, map->uaddr_exe,
			    &first, &last, addr, sz,
			    pmap_align, pmap_offset, prot, hint);
		}

		if (error != 0)
			goto unlock;
	} else {
		/* Update freelists from vmspace. */
		if (map->flags & VM_MAP_ISVMSPACE)
			uvm_map_vmspace_update(map, &dead, flags);

		error = uvm_map_findspace(map, &first, &last, addr, sz,
		    pmap_align, pmap_offset, prot, hint);

		/* Grow kernel memory and try again. */
		if (error != 0 && (map->flags & VM_MAP_ISVMSPACE) == 0) {
			uvm_map_kmem_grow(map, &dead, sz, flags);

			error = uvm_map_findspace(map, &first, &last, addr, sz,
			    pmap_align, pmap_offset, prot, hint);
		}

		if (error != 0)
			goto unlock;
	}

	/* Double-check if selected address doesn't cause overflow. */
	if (*addr + sz < *addr) {
		error = ENOMEM;
		goto unlock;
	}

	KASSERT((map->flags & VM_MAP_ISVMSPACE) == VM_MAP_ISVMSPACE ||
	    uvm_maxkaddr >= *addr + sz);

	/* If we only want a query, return now. */
	if (flags & UVM_FLAG_QUERY) {
		error = 0;
		goto unlock;
	}

	if (uobj == NULL)
		uoffset = 0;
	else if (uoffset == UVM_UNKNOWN_OFFSET) {
		KASSERT(UVM_OBJ_IS_KERN_OBJECT(uobj));
		uoffset = *addr - vm_map_min(kernel_map);
	}

	/*
	 * Create new entry.
	 * first and last may be invalidated after this call.
	 */
	entry = uvm_map_mkentry(map, first, last, *addr, sz, flags, &dead,
	    new);
	if (entry == NULL) {
		error = ENOMEM;
		goto unlock;
	}
	new = NULL;
	KDASSERT(entry->start == *addr && entry->end == *addr + sz);
	entry->object.uvm_obj = uobj;
	entry->offset = uoffset;
	entry->protection = prot;
	entry->max_protection = maxprot;
	entry->inheritance = inherit;
	entry->wired_count = 0;
	entry->advice = advice;
	if (uobj)
		entry->etype |= UVM_ET_OBJ;
	else if (flags & UVM_FLAG_HOLE)
		entry->etype |= UVM_ET_HOLE;
	if (flags & UVM_FLAG_NOFAULT)
		entry->etype |= UVM_ET_NOFAULT;
	if (flags & UVM_FLAG_COPYONW) {
		entry->etype |= UVM_ET_COPYONWRITE;
		if ((flags & UVM_FLAG_OVERLAY) == 0)
			entry->etype |= UVM_ET_NEEDSCOPY;
	}
	if (flags & UVM_FLAG_OVERLAY) {
		entry->aref.ar_pageoff = 0;
		entry->aref.ar_amap = amap_alloc(sz, M_WAITOK, 0);
	}

	/* Update map and process statistics. */
	if (!(flags & UVM_FLAG_HOLE)) {
		map->size += sz;
		if ((map->flags & VM_MAP_ISVMSPACE) && uobj == NULL) {
			((struct vmspace *)map)->vm_dused +=
			    uvmspace_dused(map, *addr, *addr + sz);
		}
	}

	/*
	 * Try to merge entry.
	 *
	 * Userland allocations are kept separated most of the time.
	 * Forego the effort of merging what most of the time can't be merged
	 * and only try the merge if it concerns a kernel entry.
	 */
	if ((flags & UVM_FLAG_NOMERGE) == 0 &&
	    (map->flags & VM_MAP_ISVMSPACE) == 0)
		uvm_mapent_tryjoin(map, entry, &dead);

unlock:
	vm_map_unlock(map);

	/*
	 * Remove dead entries.
	 *
	 * Dead entries may be the result of merging.
	 * uvm_map_mkentry may also create dead entries, when it attempts to
	 * destroy free-space entries.
	 */
	if (map->flags & VM_MAP_INTRSAFE)
		uvm_unmap_detach_intrsafe(&dead);
	else
		uvm_unmap_detach(&dead, 0);
out:
	if (new)
		uvm_mapent_free(new);
	return error;
}

/*
 * True iff e1 and e2 can be joined together.
 */
int
uvm_mapent_isjoinable(struct vm_map *map, struct vm_map_entry *e1,
    struct vm_map_entry *e2)
{
	KDASSERT(e1 != NULL && e2 != NULL);

	/* Must be the same entry type and not have free memory between. */
	if (e1->etype != e2->etype || e1->end != e2->start)
		return 0;

	/* Submaps are never joined. */
	if (UVM_ET_ISSUBMAP(e1))
		return 0;

	/* Never merge wired memory. */
	if (VM_MAPENT_ISWIRED(e1) || VM_MAPENT_ISWIRED(e2))
		return 0;

	/* Protection, inheritance and advice must be equal. */
	if (e1->protection != e2->protection ||
	    e1->max_protection != e2->max_protection ||
	    e1->inheritance != e2->inheritance ||
	    e1->advice != e2->advice)
		return 0;

	/* If uvm_object: object itself and offsets within object must match. */
	if (UVM_ET_ISOBJ(e1)) {
		if (e1->object.uvm_obj != e2->object.uvm_obj)
			return 0;
		if (e1->offset + (e1->end - e1->start) != e2->offset)
			return 0;
	}

	/*
	 * Cannot join shared amaps.
	 * Note: no need to lock amap to look at refs, since we don't care
	 * about its exact value.
	 * If it is 1 (i.e. we have the only reference) it will stay there.
	 */
	if (e1->aref.ar_amap && amap_refs(e1->aref.ar_amap) != 1)
		return 0;
	if (e2->aref.ar_amap && amap_refs(e2->aref.ar_amap) != 1)
		return 0;

	/* Apprently, e1 and e2 match. */
	return 1;
}

/*
 * Join support function.
 *
 * Returns the merged entry on succes.
 * Returns NULL if the merge failed.
 */
struct vm_map_entry*
uvm_mapent_merge(struct vm_map *map, struct vm_map_entry *e1,
    struct vm_map_entry *e2, struct uvm_map_deadq *dead)
{
	struct uvm_addr_state *free;

	/*
	 * Merging is not supported for map entries that
	 * contain an amap in e1. This should never happen
	 * anyway, because only kernel entries are merged.
	 * These do not contain amaps.
	 * e2 contains no real information in its amap,
	 * so it can be erased immediately.
	 */
	KASSERT(e1->aref.ar_amap == NULL);

	/*
	 * Don't drop obj reference:
	 * uvm_unmap_detach will do this for us.
	 */
	free = uvm_map_uaddr_e(map, e1);
	uvm_mapent_free_remove(map, free, e1);

	free = uvm_map_uaddr_e(map, e2);
	uvm_mapent_free_remove(map, free, e2);
	uvm_mapent_addr_remove(map, e2);
	e1->end = e2->end;
	e1->guard = e2->guard;
	e1->fspace = e2->fspace;
	uvm_mapent_free_insert(map, free, e1);

	DEAD_ENTRY_PUSH(dead, e2);
	return e1;
}

/*
 * Attempt forward and backward joining of entry.
 *
 * Returns entry after joins.
 * We are guaranteed that the amap of entry is either non-existant or
 * has never been used.
 */
struct vm_map_entry*
uvm_mapent_tryjoin(struct vm_map *map, struct vm_map_entry *entry,
    struct uvm_map_deadq *dead)
{
	struct vm_map_entry *other;
	struct vm_map_entry *merged;

	/* Merge with previous entry. */
	other = RBT_PREV(uvm_map_addr, entry);
	if (other && uvm_mapent_isjoinable(map, other, entry)) {
		merged = uvm_mapent_merge(map, other, entry, dead);
		if (merged)
			entry = merged;
	}

	/*
	 * Merge with next entry.
	 *
	 * Because amap can only extend forward and the next entry
	 * probably contains sensible info, only perform forward merging
	 * in the absence of an amap.
	 */
	other = RBT_NEXT(uvm_map_addr, entry);
	if (other && entry->aref.ar_amap == NULL &&
	    other->aref.ar_amap == NULL &&
	    uvm_mapent_isjoinable(map, entry, other)) {
		merged = uvm_mapent_merge(map, entry, other, dead);
		if (merged)
			entry = merged;
	}

	return entry;
}

/*
 * Kill entries that are no longer in a map.
 */
void
uvm_unmap_detach(struct uvm_map_deadq *deadq, int flags)
{
	struct vm_map_entry *entry;
	int waitok = flags & UVM_PLA_WAITOK;

	if (TAILQ_EMPTY(deadq))
		return;

	KERNEL_LOCK();
	while ((entry = TAILQ_FIRST(deadq)) != NULL) {
		if (waitok)
			uvm_pause();
		/* Drop reference to amap, if we've got one. */
		if (entry->aref.ar_amap)
			amap_unref(entry->aref.ar_amap,
			    entry->aref.ar_pageoff,
			    atop(entry->end - entry->start),
			    flags & AMAP_REFALL);

		/* Drop reference to our backing object, if we've got one. */
		if (UVM_ET_ISSUBMAP(entry)) {
			/* ... unlikely to happen, but play it safe */
			uvm_map_deallocate(entry->object.sub_map);
		} else if (UVM_ET_ISOBJ(entry) &&
		    entry->object.uvm_obj->pgops->pgo_detach) {
			entry->object.uvm_obj->pgops->pgo_detach(
			    entry->object.uvm_obj);
		}

		/* Step to next. */
		TAILQ_REMOVE(deadq, entry, dfree.deadq);
		uvm_mapent_free(entry);
	}
	KERNEL_UNLOCK();
}

void
uvm_unmap_detach_intrsafe(struct uvm_map_deadq *deadq)
{
	struct vm_map_entry *entry;

	while ((entry = TAILQ_FIRST(deadq)) != NULL) {
		KASSERT(entry->aref.ar_amap == NULL);
		KASSERT(!UVM_ET_ISSUBMAP(entry));
		KASSERT(!UVM_ET_ISOBJ(entry));
		TAILQ_REMOVE(deadq, entry, dfree.deadq);
		uvm_mapent_free(entry);
	}
}

/*
 * Create and insert new entry.
 *
 * Returned entry contains new addresses and is inserted properly in the tree.
 * first and last are (probably) no longer valid.
 */
struct vm_map_entry*
uvm_map_mkentry(struct vm_map *map, struct vm_map_entry *first,
    struct vm_map_entry *last, vaddr_t addr, vsize_t sz, int flags,
    struct uvm_map_deadq *dead, struct vm_map_entry *new)
{
	struct vm_map_entry *entry, *prev;
	struct uvm_addr_state *free;
	vaddr_t min, max;	/* free space boundaries for new entry */

	KDASSERT(map != NULL);
	KDASSERT(first != NULL);
	KDASSERT(last != NULL);
	KDASSERT(dead != NULL);
	KDASSERT(sz > 0);
	KDASSERT(addr + sz > addr);
	KDASSERT(first->end <= addr && VMMAP_FREE_END(first) > addr);
	KDASSERT(last->start < addr + sz && VMMAP_FREE_END(last) >= addr + sz);
	KDASSERT(uvm_map_isavail(map, NULL, &first, &last, addr, sz));
	uvm_tree_sanity(map, __FILE__, __LINE__);

	min = addr + sz;
	max = VMMAP_FREE_END(last);

	/* Initialize new entry. */
	if (new == NULL)
		entry = uvm_mapent_alloc(map, flags);
	else
		entry = new;
	if (entry == NULL)
		return NULL;
	entry->offset = 0;
	entry->etype = 0;
	entry->wired_count = 0;
	entry->aref.ar_pageoff = 0;
	entry->aref.ar_amap = NULL;

	entry->start = addr;
	entry->end = min;
	entry->guard = 0;
	entry->fspace = 0;

	/* Reset free space in first. */
	free = uvm_map_uaddr_e(map, first);
	uvm_mapent_free_remove(map, free, first);
	first->guard = 0;
	first->fspace = 0;

	/*
	 * Remove all entries that are fully replaced.
	 * We are iterating using last in reverse order.
	 */
	for (; first != last; last = prev) {
		prev = RBT_PREV(uvm_map_addr, last);

		KDASSERT(last->start == last->end);
		free = uvm_map_uaddr_e(map, last);
		uvm_mapent_free_remove(map, free, last);
		uvm_mapent_addr_remove(map, last);
		DEAD_ENTRY_PUSH(dead, last);
	}
	/* Remove first if it is entirely inside <addr, addr+sz>.  */
	if (first->start == addr) {
		uvm_mapent_addr_remove(map, first);
		DEAD_ENTRY_PUSH(dead, first);
	} else {
		uvm_map_fix_space(map, first, VMMAP_FREE_START(first),
		    addr, flags);
	}

	/* Finally, link in entry. */
	uvm_mapent_addr_insert(map, entry);
	uvm_map_fix_space(map, entry, min, max, flags);

	uvm_tree_sanity(map, __FILE__, __LINE__);
	return entry;
}


/*
 * uvm_mapent_alloc: allocate a map entry
 */
struct vm_map_entry *
uvm_mapent_alloc(struct vm_map *map, int flags)
{
	struct vm_map_entry *me, *ne;
	int pool_flags;
	int i;

	pool_flags = PR_WAITOK;
	if (flags & UVM_FLAG_TRYLOCK)
		pool_flags = PR_NOWAIT;

	if (map->flags & VM_MAP_INTRSAFE || cold) {
		mtx_enter(&uvm_kmapent_mtx);
		if (SLIST_EMPTY(&uvm.kentry_free)) {
			ne = km_alloc(PAGE_SIZE, &kv_page, &kp_dirty,
			    &kd_nowait);
			if (ne == NULL)
				panic("uvm_mapent_alloc: cannot allocate map "
				    "entry");
			for (i = 0; i < PAGE_SIZE / sizeof(*ne); i++) {
				SLIST_INSERT_HEAD(&uvm.kentry_free,
				    &ne[i], daddrs.addr_kentry);
			}
			if (ratecheck(&uvm_kmapent_last_warn_time,
			    &uvm_kmapent_warn_rate))
				printf("uvm_mapent_alloc: out of static "
				    "map entries\n");
		}
		me = SLIST_FIRST(&uvm.kentry_free);
		SLIST_REMOVE_HEAD(&uvm.kentry_free, daddrs.addr_kentry);
		uvmexp.kmapent++;
		mtx_leave(&uvm_kmapent_mtx);
		me->flags = UVM_MAP_STATIC;
	} else if (map == kernel_map) {
		splassert(IPL_NONE);
		me = pool_get(&uvm_map_entry_kmem_pool, pool_flags);
		if (me == NULL)
			goto out;
		me->flags = UVM_MAP_KMEM;
	} else {
		splassert(IPL_NONE);
		me = pool_get(&uvm_map_entry_pool, pool_flags);
		if (me == NULL)
			goto out;
		me->flags = 0;
	}

	if (me != NULL) {
		RBT_POISON(uvm_map_addr, me, UVMMAP_DEADBEEF);
	}

out:
	return(me);
}

/*
 * uvm_mapent_free: free map entry
 *
 * => XXX: static pool for kernel map?
 */
void
uvm_mapent_free(struct vm_map_entry *me)
{
	if (me->flags & UVM_MAP_STATIC) {
		mtx_enter(&uvm_kmapent_mtx);
		SLIST_INSERT_HEAD(&uvm.kentry_free, me, daddrs.addr_kentry);
		uvmexp.kmapent--;
		mtx_leave(&uvm_kmapent_mtx);
	} else if (me->flags & UVM_MAP_KMEM) {
		splassert(IPL_NONE);
		pool_put(&uvm_map_entry_kmem_pool, me);
	} else {
		splassert(IPL_NONE);
		pool_put(&uvm_map_entry_pool, me);
	}
}

/*
 * uvm_map_lookup_entry: find map entry at or before an address.
 *
 * => map must at least be read-locked by caller
 * => entry is returned in "entry"
 * => return value is true if address is in the returned entry
 * ET_HOLE entries are considered to not contain a mapping, ergo FALSE is
 * returned for those mappings.
 */
boolean_t
uvm_map_lookup_entry(struct vm_map *map, vaddr_t address,
    struct vm_map_entry **entry)
{
	*entry = uvm_map_entrybyaddr(&map->addr, address);
	return *entry != NULL && !UVM_ET_ISHOLE(*entry) &&
	    (*entry)->start <= address && (*entry)->end > address;
}

/*
 * uvm_map_pie: return a random load address for a PIE executable
 * properly aligned.
 */
#ifndef VM_PIE_MAX_ADDR
#define VM_PIE_MAX_ADDR (VM_MAXUSER_ADDRESS / 4)
#endif

#ifndef VM_PIE_MIN_ADDR
#define VM_PIE_MIN_ADDR VM_MIN_ADDRESS
#endif

#ifndef VM_PIE_MIN_ALIGN
#define VM_PIE_MIN_ALIGN PAGE_SIZE
#endif

vaddr_t
uvm_map_pie(vaddr_t align)
{
	vaddr_t addr, space, min;

	align = MAX(align, VM_PIE_MIN_ALIGN);

	/* round up to next alignment */
	min = (VM_PIE_MIN_ADDR + align - 1) & ~(align - 1);

	if (align >= VM_PIE_MAX_ADDR || min >= VM_PIE_MAX_ADDR)
		return (align);

	space = (VM_PIE_MAX_ADDR - min) / align;
	space = MIN(space, (u_int32_t)-1);

	addr = (vaddr_t)arc4random_uniform((u_int32_t)space) * align;
	addr += min;

	return (addr);
}

void
uvm_unmap(struct vm_map *map, vaddr_t start, vaddr_t end)
{
	struct uvm_map_deadq dead;

	KASSERT((start & (vaddr_t)PAGE_MASK) == 0 &&
	    (end & (vaddr_t)PAGE_MASK) == 0);
	TAILQ_INIT(&dead);
	vm_map_lock(map);
	uvm_unmap_remove(map, start, end, &dead, FALSE, TRUE);
	vm_map_unlock(map);

	if (map->flags & VM_MAP_INTRSAFE)
		uvm_unmap_detach_intrsafe(&dead);
	else
		uvm_unmap_detach(&dead, 0);
}

/*
 * Mark entry as free.
 *
 * entry will be put on the dead list.
 * The free space will be merged into the previous or a new entry,
 * unless markfree is false.
 */
void
uvm_mapent_mkfree(struct vm_map *map, struct vm_map_entry *entry,
    struct vm_map_entry **prev_ptr, struct uvm_map_deadq *dead,
    boolean_t markfree)
{
	struct uvm_addr_state	*free;
	struct vm_map_entry	*prev;
	vaddr_t			 addr;	/* Start of freed range. */
	vaddr_t			 end;	/* End of freed range. */

	prev = *prev_ptr;
	if (prev == entry)
		*prev_ptr = prev = NULL;

	if (prev == NULL ||
	    VMMAP_FREE_END(prev) != entry->start)
		prev = RBT_PREV(uvm_map_addr, entry);

	/* Entry is describing only free memory and has nothing to drain into. */
	if (prev == NULL && entry->start == entry->end && markfree) {
		*prev_ptr = entry;
		return;
	}

	addr = entry->start;
	end = VMMAP_FREE_END(entry);
	free = uvm_map_uaddr_e(map, entry);
	uvm_mapent_free_remove(map, free, entry);
	uvm_mapent_addr_remove(map, entry);
	DEAD_ENTRY_PUSH(dead, entry);

	if (markfree) {
		if (prev) {
			free = uvm_map_uaddr_e(map, prev);
			uvm_mapent_free_remove(map, free, prev);
		}
		*prev_ptr = uvm_map_fix_space(map, prev, addr, end, 0);
	}
}

/*
 * Unwire and release referenced amap and object from map entry.
 */
void
uvm_unmap_kill_entry(struct vm_map *map, struct vm_map_entry *entry)
{
	/* Unwire removed map entry. */
	if (VM_MAPENT_ISWIRED(entry)) {
		KERNEL_LOCK();
		entry->wired_count = 0;
		uvm_fault_unwire_locked(map, entry->start, entry->end);
		KERNEL_UNLOCK();
	}

	/* Entry-type specific code. */
	if (UVM_ET_ISHOLE(entry)) {
		/* Nothing to be done for holes. */
	} else if (map->flags & VM_MAP_INTRSAFE) {
		KASSERT(vm_map_pmap(map) == pmap_kernel());
		uvm_km_pgremove_intrsafe(entry->start, entry->end);
		pmap_kremove(entry->start, entry->end - entry->start);
	} else if (UVM_ET_ISOBJ(entry) &&
	    UVM_OBJ_IS_KERN_OBJECT(entry->object.uvm_obj)) {
		KASSERT(vm_map_pmap(map) == pmap_kernel());
		/*
		 * Note: kernel object mappings are currently used in
		 * two ways:
		 *  [1] "normal" mappings of pages in the kernel object
		 *  [2] uvm_km_valloc'd allocations in which we
		 *      pmap_enter in some non-kernel-object page
		 *      (e.g. vmapbuf).
		 *
		 * for case [1], we need to remove the mapping from
		 * the pmap and then remove the page from the kernel
		 * object (because, once pages in a kernel object are
		 * unmapped they are no longer needed, unlike, say,
		 * a vnode where you might want the data to persist
		 * until flushed out of a queue).
		 *
		 * for case [2], we need to remove the mapping from
		 * the pmap.  there shouldn't be any pages at the
		 * specified offset in the kernel object [but it
		 * doesn't hurt to call uvm_km_pgremove just to be
		 * safe?]
		 *
		 * uvm_km_pgremove currently does the following:
		 *   for pages in the kernel object range:
		 *     - drops the swap slot
		 *     - uvm_pagefree the page
		 *
		 * note there is version of uvm_km_pgremove() that
		 * is used for "intrsafe" objects.
		 */
		/*
		 * remove mappings from pmap and drop the pages
		 * from the object.  offsets are always relative
		 * to vm_map_min(kernel_map).
		 */
		pmap_remove(pmap_kernel(), entry->start, entry->end);
		uvm_km_pgremove(entry->object.uvm_obj,
		    entry->start - vm_map_min(kernel_map),
		    entry->end - vm_map_min(kernel_map));

		/*
		 * null out kernel_object reference, we've just
		 * dropped it
		 */
		entry->etype &= ~UVM_ET_OBJ;
		entry->object.uvm_obj = NULL;  /* to be safe */
	} else {
		/* remove mappings the standard way. */
		pmap_remove(map->pmap, entry->start, entry->end);
	}
}

/*
 * Remove all entries from start to end.
 *
 * If remove_holes, then remove ET_HOLE entries as well.
 * If markfree, entry will be properly marked free, otherwise, no replacement
 * entry will be put in the tree (corrupting the tree).
 */
void
uvm_unmap_remove(struct vm_map *map, vaddr_t start, vaddr_t end,
    struct uvm_map_deadq *dead, boolean_t remove_holes,
    boolean_t markfree)
{
	struct vm_map_entry *prev_hint, *next, *entry;

	start = MAX(start, map->min_offset);
	end = MIN(end, map->max_offset);
	if (start >= end)
		return;

	if ((map->flags & VM_MAP_INTRSAFE) == 0)
		splassert(IPL_NONE);
	else
		splassert(IPL_VM);

	/* Find first affected entry. */
	entry = uvm_map_entrybyaddr(&map->addr, start);
	KDASSERT(entry != NULL && entry->start <= start);
	if (entry->end <= start && markfree)
		entry = RBT_NEXT(uvm_map_addr, entry);
	else
		UVM_MAP_CLIP_START(map, entry, start);

	/*
	 * Iterate entries until we reach end address.
	 * prev_hint hints where the freed space can be appended to.
	 */
	prev_hint = NULL;
	for (; entry != NULL && entry->start < end; entry = next) {
		KDASSERT(entry->start >= start);
		if (entry->end > end || !markfree)
			UVM_MAP_CLIP_END(map, entry, end);
		KDASSERT(entry->start >= start && entry->end <= end);
		next = RBT_NEXT(uvm_map_addr, entry);

		/* Don't remove holes unless asked to do so. */
		if (UVM_ET_ISHOLE(entry)) {
			if (!remove_holes) {
				prev_hint = entry;
				continue;
			}
		}

		/* Kill entry. */
		uvm_unmap_kill_entry(map, entry);

		/* Update space usage. */
		if ((map->flags & VM_MAP_ISVMSPACE) &&
		    entry->object.uvm_obj == NULL &&
		    !UVM_ET_ISHOLE(entry)) {
			((struct vmspace *)map)->vm_dused -=
			    uvmspace_dused(map, entry->start, entry->end);
		}
		if (!UVM_ET_ISHOLE(entry))
			map->size -= entry->end - entry->start;

		/* Actual removal of entry. */
		uvm_mapent_mkfree(map, entry, &prev_hint, dead, markfree);
	}

	pmap_update(vm_map_pmap(map));

#ifdef VMMAP_DEBUG
	if (markfree) {
		for (entry = uvm_map_entrybyaddr(&map->addr, start);
		    entry != NULL && entry->start < end;
		    entry = RBT_NEXT(uvm_map_addr, entry)) {
			KDASSERT(entry->end <= start ||
			    entry->start == entry->end ||
			    UVM_ET_ISHOLE(entry));
		}
	} else {
		vaddr_t a;
		for (a = start; a < end; a += PAGE_SIZE)
			KDASSERT(uvm_map_entrybyaddr(&map->addr, a) == NULL);
	}
#endif
}

/*
 * Mark all entries from first until end (exclusive) as pageable.
 *
 * Lock must be exclusive on entry and will not be touched.
 */
void
uvm_map_pageable_pgon(struct vm_map *map, struct vm_map_entry *first,
    struct vm_map_entry *end, vaddr_t start_addr, vaddr_t end_addr)
{
	struct vm_map_entry *iter;

	for (iter = first; iter != end;
	    iter = RBT_NEXT(uvm_map_addr, iter)) {
		KDASSERT(iter->start >= start_addr && iter->end <= end_addr);
		if (!VM_MAPENT_ISWIRED(iter) || UVM_ET_ISHOLE(iter))
			continue;

		iter->wired_count = 0;
		uvm_fault_unwire_locked(map, iter->start, iter->end);
	}
}

/*
 * Mark all entries from first until end (exclusive) as wired.
 *
 * Lockflags determines the lock state on return from this function.
 * Lock must be exclusive on entry.
 */
int
uvm_map_pageable_wire(struct vm_map *map, struct vm_map_entry *first,
    struct vm_map_entry *end, vaddr_t start_addr, vaddr_t end_addr,
    int lockflags)
{
	struct vm_map_entry *iter;
#ifdef DIAGNOSTIC
	unsigned int timestamp_save;
#endif
	int error;

	/*
	 * Wire pages in two passes:
	 *
	 * 1: holding the write lock, we create any anonymous maps that need
	 *    to be created.  then we clip each map entry to the region to
	 *    be wired and increment its wiring count.
	 *
	 * 2: we downgrade to a read lock, and call uvm_fault_wire to fault
	 *    in the pages for any newly wired area (wired_count == 1).
	 *
	 *    downgrading to a read lock for uvm_fault_wire avoids a possible
	 *    deadlock with another thread that may have faulted on one of
	 *    the pages to be wired (it would mark the page busy, blocking
	 *    us, then in turn block on the map lock that we hold).
	 *    because we keep the read lock on the map, the copy-on-write
	 *    status of the entries we modify here cannot change.
	 */
	for (iter = first; iter != end;
	    iter = RBT_NEXT(uvm_map_addr, iter)) {
		KDASSERT(iter->start >= start_addr && iter->end <= end_addr);
		if (UVM_ET_ISHOLE(iter) || iter->start == iter->end ||
		    iter->protection == PROT_NONE)
			continue;

		/*
		 * Perform actions of vm_map_lookup that need the write lock.
		 * - create an anonymous map for copy-on-write
		 * - anonymous map for zero-fill
		 * Skip submaps.
		 */
		if (!VM_MAPENT_ISWIRED(iter) && !UVM_ET_ISSUBMAP(iter) &&
		    UVM_ET_ISNEEDSCOPY(iter) &&
		    ((iter->protection & PROT_WRITE) ||
		    iter->object.uvm_obj == NULL)) {
			amap_copy(map, iter, M_WAITOK, TRUE,
			    iter->start, iter->end);
		}
		iter->wired_count++;
	}

	/*
	 * Pass 2.
	 */
#ifdef DIAGNOSTIC
	timestamp_save = map->timestamp;
#endif
	vm_map_busy(map);
	vm_map_downgrade(map);

	error = 0;
	for (iter = first; error == 0 && iter != end;
	    iter = RBT_NEXT(uvm_map_addr, iter)) {
		if (UVM_ET_ISHOLE(iter) || iter->start == iter->end ||
		    iter->protection == PROT_NONE)
			continue;

		error = uvm_fault_wire(map, iter->start, iter->end,
		    iter->protection);
	}

	if (error) {
		/*
		 * uvm_fault_wire failure
		 *
		 * Reacquire lock and undo our work.
		 */
		vm_map_upgrade(map);
		vm_map_unbusy(map);
#ifdef DIAGNOSTIC
		if (timestamp_save != map->timestamp)
			panic("uvm_map_pageable_wire: stale map");
#endif

		/*
		 * first is no longer needed to restart loops.
		 * Use it as iterator to unmap successful mappings.
		 */
		for (; first != iter;
		    first = RBT_NEXT(uvm_map_addr, first)) {
			if (UVM_ET_ISHOLE(first) ||
			    first->start == first->end ||
			    first->protection == PROT_NONE)
				continue;

			first->wired_count--;
			if (!VM_MAPENT_ISWIRED(first)) {
				uvm_fault_unwire_locked(map,
				    iter->start, iter->end);
			}
		}

		/* decrease counter in the rest of the entries */
		for (; iter != end;
		    iter = RBT_NEXT(uvm_map_addr, iter)) {
			if (UVM_ET_ISHOLE(iter) || iter->start == iter->end ||
			    iter->protection == PROT_NONE)
				continue;

			iter->wired_count--;
		}

		if ((lockflags & UVM_LK_EXIT) == 0)
			vm_map_unlock(map);
		return error;
	}

	/* We are currently holding a read lock. */
	if ((lockflags & UVM_LK_EXIT) == 0) {
		vm_map_unbusy(map);
		vm_map_unlock_read(map);
	} else {
		vm_map_upgrade(map);
		vm_map_unbusy(map);
#ifdef DIAGNOSTIC
		if (timestamp_save != map->timestamp)
			panic("uvm_map_pageable_wire: stale map");
#endif
	}
	return 0;
}

/*
 * uvm_map_pageable: set pageability of a range in a map.
 *
 * Flags:
 * UVM_LK_ENTER: map is already locked by caller
 * UVM_LK_EXIT:  don't unlock map on exit
 *
 * The full range must be in use (entries may not have fspace != 0).
 * UVM_ET_HOLE counts as unmapped.
 */
int
uvm_map_pageable(struct vm_map *map, vaddr_t start, vaddr_t end,
    boolean_t new_pageable, int lockflags)
{
	struct vm_map_entry *first, *last, *tmp;
	int error;

	start = trunc_page(start);
	end = round_page(end);

	if (start > end)
		return EINVAL;
	if (start == end)
		return 0;	/* nothing to do */
	if (start < map->min_offset)
		return EFAULT; /* why? see first XXX below */
	if (end > map->max_offset)
		return EINVAL; /* why? see second XXX below */

	KASSERT(map->flags & VM_MAP_PAGEABLE);
	if ((lockflags & UVM_LK_ENTER) == 0)
		vm_map_lock(map);

	/*
	 * Find first entry.
	 *
	 * Initial test on start is different, because of the different
	 * error returned. Rest is tested further down.
	 */
	first = uvm_map_entrybyaddr(&map->addr, start);
	if (first->end <= start || UVM_ET_ISHOLE(first)) {
		/*
		 * XXX if the first address is not mapped, it is EFAULT?
		 */
		error = EFAULT;
		goto out;
	}

	/* Check that the range has no holes. */
	for (last = first; last != NULL && last->start < end;
	    last = RBT_NEXT(uvm_map_addr, last)) {
		if (UVM_ET_ISHOLE(last) ||
		    (last->end < end && VMMAP_FREE_END(last) != last->end)) {
			/*
			 * XXX unmapped memory in range, why is it EINVAL
			 * instead of EFAULT?
			 */
			error = EINVAL;
			goto out;
		}
	}

	/*
	 * Last ended at the first entry after the range.
	 * Move back one step.
	 *
	 * Note that last may be NULL.
	 */
	if (last == NULL) {
		last = RBT_MAX(uvm_map_addr, &map->addr);
		if (last->end < end) {
			error = EINVAL;
			goto out;
		}
	} else {
		KASSERT(last != first);
		last = RBT_PREV(uvm_map_addr, last);
	}

	/* Wire/unwire pages here. */
	if (new_pageable) {
		/*
		 * Mark pageable.
		 * entries that are not wired are untouched.
		 */
		if (VM_MAPENT_ISWIRED(first))
			UVM_MAP_CLIP_START(map, first, start);
		/*
		 * Split last at end.
		 * Make tmp be the first entry after what is to be touched.
		 * If last is not wired, don't touch it.
		 */
		if (VM_MAPENT_ISWIRED(last)) {
			UVM_MAP_CLIP_END(map, last, end);
			tmp = RBT_NEXT(uvm_map_addr, last);
		} else
			tmp = last;

		uvm_map_pageable_pgon(map, first, tmp, start, end);
		error = 0;

out:
		if ((lockflags & UVM_LK_EXIT) == 0)
			vm_map_unlock(map);
		return error;
	} else {
		/*
		 * Mark entries wired.
		 * entries are always touched (because recovery needs this).
		 */
		if (!VM_MAPENT_ISWIRED(first))
			UVM_MAP_CLIP_START(map, first, start);
		/*
		 * Split last at end.
		 * Make tmp be the first entry after what is to be touched.
		 * If last is not wired, don't touch it.
		 */
		if (!VM_MAPENT_ISWIRED(last)) {
			UVM_MAP_CLIP_END(map, last, end);
			tmp = RBT_NEXT(uvm_map_addr, last);
		} else
			tmp = last;

		return uvm_map_pageable_wire(map, first, tmp, start, end,
		    lockflags);
	}
}

/*
 * uvm_map_pageable_all: special case of uvm_map_pageable - affects
 * all mapped regions.
 *
 * Map must not be locked.
 * If no flags are specified, all ragions are unwired.
 */
int
uvm_map_pageable_all(struct vm_map *map, int flags, vsize_t limit)
{
	vsize_t size;
	struct vm_map_entry *iter;

	KASSERT(map->flags & VM_MAP_PAGEABLE);
	vm_map_lock(map);

	if (flags == 0) {
		uvm_map_pageable_pgon(map, RBT_MIN(uvm_map_addr, &map->addr),
		    NULL, map->min_offset, map->max_offset);

		vm_map_modflags(map, 0, VM_MAP_WIREFUTURE);
		vm_map_unlock(map);
		return 0;
	}

	if (flags & MCL_FUTURE)
		vm_map_modflags(map, VM_MAP_WIREFUTURE, 0);
	if (!(flags & MCL_CURRENT)) {
		vm_map_unlock(map);
		return 0;
	}

	/*
	 * Count number of pages in all non-wired entries.
	 * If the number exceeds the limit, abort.
	 */
	size = 0;
	RBT_FOREACH(iter, uvm_map_addr, &map->addr) {
		if (VM_MAPENT_ISWIRED(iter) || UVM_ET_ISHOLE(iter))
			continue;

		size += iter->end - iter->start;
	}

	if (atop(size) + uvmexp.wired > uvmexp.wiredmax) {
		vm_map_unlock(map);
		return ENOMEM;
	}

	/* XXX non-pmap_wired_count case must be handled by caller */
#ifdef pmap_wired_count
	if (limit != 0 &&
	    size + ptoa(pmap_wired_count(vm_map_pmap(map))) > limit) {
		vm_map_unlock(map);
		return ENOMEM;
	}
#endif

	/*
	 * uvm_map_pageable_wire will release lcok
	 */
	return uvm_map_pageable_wire(map, RBT_MIN(uvm_map_addr, &map->addr),
	    NULL, map->min_offset, map->max_offset, 0);
}

/*
 * Initialize map.
 *
 * Allocates sufficient entries to describe the free memory in the map.
 */
void
uvm_map_setup(struct vm_map *map, vaddr_t min, vaddr_t max, int flags)
{
	int i;

	KASSERT((min & (vaddr_t)PAGE_MASK) == 0);
	KASSERT((max & (vaddr_t)PAGE_MASK) == 0 ||
	    (max & (vaddr_t)PAGE_MASK) == (vaddr_t)PAGE_MASK);

	/*
	 * Update parameters.
	 *
	 * This code handles (vaddr_t)-1 and other page mask ending addresses
	 * properly.
	 * We lose the top page if the full virtual address space is used.
	 */
	if (max & (vaddr_t)PAGE_MASK) {
		max += 1;
		if (max == 0) /* overflow */
			max -= PAGE_SIZE;
	}

	RBT_INIT(uvm_map_addr, &map->addr);
	map->uaddr_exe = NULL;
	for (i = 0; i < nitems(map->uaddr_any); ++i)
		map->uaddr_any[i] = NULL;
	map->uaddr_brk_stack = NULL;

	map->size = 0;
	map->ref_count = 0;
	map->min_offset = min;
	map->max_offset = max;
	map->b_start = map->b_end = 0; /* Empty brk() area by default. */
	map->s_start = map->s_end = 0; /* Empty stack area by default. */
	map->flags = flags;
	map->timestamp = 0;
	rw_init_flags(&map->lock, "vmmaplk", RWL_DUPOK);
	mtx_init(&map->mtx, IPL_VM);
	mtx_init(&map->flags_lock, IPL_VM);

	/* Configure the allocators. */
	if (flags & VM_MAP_ISVMSPACE)
		uvm_map_setup_md(map);
	else
		map->uaddr_any[3] = &uaddr_kbootstrap;

	/*
	 * Fill map entries.
	 * We do not need to write-lock the map here because only the current
	 * thread sees it right now. Initialize ref_count to 0 above to avoid
	 * bogus triggering of lock-not-held assertions.
	 */
	uvm_map_setup_entries(map);
	uvm_tree_sanity(map, __FILE__, __LINE__);
	map->ref_count = 1;
}

/*
 * Destroy the map.
 *
 * This is the inverse operation to uvm_map_setup.
 */
void
uvm_map_teardown(struct vm_map *map)
{
	struct uvm_map_deadq	 dead_entries;
	struct vm_map_entry	*entry, *tmp;
#ifdef VMMAP_DEBUG
	size_t			 numq, numt;
#endif
	int			 i;

	KERNEL_ASSERT_LOCKED();
	KERNEL_UNLOCK();
	KERNEL_ASSERT_UNLOCKED();

	KASSERT((map->flags & VM_MAP_INTRSAFE) == 0);

	/* Remove address selectors. */
	uvm_addr_destroy(map->uaddr_exe);
	map->uaddr_exe = NULL;
	for (i = 0; i < nitems(map->uaddr_any); i++) {
		uvm_addr_destroy(map->uaddr_any[i]);
		map->uaddr_any[i] = NULL;
	}
	uvm_addr_destroy(map->uaddr_brk_stack);
	map->uaddr_brk_stack = NULL;

	/*
	 * Remove entries.
	 *
	 * The following is based on graph breadth-first search.
	 *
	 * In color terms:
	 * - the dead_entries set contains all nodes that are reachable
	 *   (i.e. both the black and the grey nodes)
	 * - any entry not in dead_entries is white
	 * - any entry that appears in dead_entries before entry,
	 *   is black, the rest is grey.
	 * The set [entry, end] is also referred to as the wavefront.
	 *
	 * Since the tree is always a fully connected graph, the breadth-first
	 * search guarantees that each vmmap_entry is visited exactly once.
	 * The vm_map is broken down in linear time.
	 */
	TAILQ_INIT(&dead_entries);
	if ((entry = RBT_ROOT(uvm_map_addr, &map->addr)) != NULL)
		DEAD_ENTRY_PUSH(&dead_entries, entry);
	while (entry != NULL) {
		sched_pause(yield);
		uvm_unmap_kill_entry(map, entry);
		if ((tmp = RBT_LEFT(uvm_map_addr, entry)) != NULL)
			DEAD_ENTRY_PUSH(&dead_entries, tmp);
		if ((tmp = RBT_RIGHT(uvm_map_addr, entry)) != NULL)
			DEAD_ENTRY_PUSH(&dead_entries, tmp);
		/* Update wave-front. */
		entry = TAILQ_NEXT(entry, dfree.deadq);
	}

#ifdef VMMAP_DEBUG
	numt = numq = 0;
	RBT_FOREACH(entry, uvm_map_addr, &map->addr)
		numt++;
	TAILQ_FOREACH(entry, &dead_entries, dfree.deadq)
		numq++;
	KASSERT(numt == numq);
#endif
	uvm_unmap_detach(&dead_entries, UVM_PLA_WAITOK);

	KERNEL_LOCK();

	pmap_destroy(map->pmap);
	map->pmap = NULL;
}

/*
 * Populate map with free-memory entries.
 *
 * Map must be initialized and empty.
 */
void
uvm_map_setup_entries(struct vm_map *map)
{
	KDASSERT(RBT_EMPTY(uvm_map_addr, &map->addr));

	uvm_map_fix_space(map, NULL, map->min_offset, map->max_offset, 0);
}

/*
 * Split entry at given address.
 *
 * orig:  entry that is to be split.
 * next:  a newly allocated map entry that is not linked.
 * split: address at which the split is done.
 */
void
uvm_map_splitentry(struct vm_map *map, struct vm_map_entry *orig,
    struct vm_map_entry *next, vaddr_t split)
{
	struct uvm_addr_state *free, *free_before;
	vsize_t adj;

	if ((split & PAGE_MASK) != 0) {
		panic("uvm_map_splitentry: split address 0x%lx "
		    "not on page boundary!", split);
	}
	KDASSERT(map != NULL && orig != NULL && next != NULL);
	uvm_tree_sanity(map, __FILE__, __LINE__);
	KASSERT(orig->start < split && VMMAP_FREE_END(orig) > split);

#ifdef VMMAP_DEBUG
	KDASSERT(RBT_FIND(uvm_map_addr, &map->addr, orig) == orig);
	KDASSERT(RBT_FIND(uvm_map_addr, &map->addr, next) != next);
#endif /* VMMAP_DEBUG */

	/*
	 * Free space will change, unlink from free space tree.
	 */
	free = uvm_map_uaddr_e(map, orig);
	uvm_mapent_free_remove(map, free, orig);

	adj = split - orig->start;

	uvm_mapent_copy(orig, next);
	if (split >= orig->end) {
		next->etype = 0;
		next->offset = 0;
		next->wired_count = 0;
		next->start = next->end = split;
		next->guard = 0;
		next->fspace = VMMAP_FREE_END(orig) - split;
		next->aref.ar_amap = NULL;
		next->aref.ar_pageoff = 0;
		orig->guard = MIN(orig->guard, split - orig->end);
		orig->fspace = split - VMMAP_FREE_START(orig);
	} else {
		orig->fspace = 0;
		orig->guard = 0;
		orig->end = next->start = split;

		if (next->aref.ar_amap) {
			KERNEL_LOCK();
			amap_splitref(&orig->aref, &next->aref, adj);
			KERNEL_UNLOCK();
		}
		if (UVM_ET_ISSUBMAP(orig)) {
			uvm_map_reference(next->object.sub_map);
			next->offset += adj;
		} else if (UVM_ET_ISOBJ(orig)) {
			if (next->object.uvm_obj->pgops &&
			    next->object.uvm_obj->pgops->pgo_reference) {
				KERNEL_LOCK();
				next->object.uvm_obj->pgops->pgo_reference(
				    next->object.uvm_obj);
				KERNEL_UNLOCK();
			}
			next->offset += adj;
		}
	}

	/*
	 * Link next into address tree.
	 * Link orig and next into free-space tree.
	 *
	 * Don't insert 'next' into the addr tree until orig has been linked,
	 * in case the free-list looks at adjecent entries in the addr tree
	 * for its decisions.
	 */
	if (orig->fspace > 0)
		free_before = free;
	else
		free_before = uvm_map_uaddr_e(map, orig);
	uvm_mapent_free_insert(map, free_before, orig);
	uvm_mapent_addr_insert(map, next);
	uvm_mapent_free_insert(map, free, next);

	uvm_tree_sanity(map, __FILE__, __LINE__);
}


#ifdef VMMAP_DEBUG

void
uvm_tree_assert(struct vm_map *map, int test, char *test_str,
    char *file, int line)
{
	char* map_special;

	if (test)
		return;

	if (map == kernel_map)
		map_special = " (kernel_map)";
	else if (map == kmem_map)
		map_special = " (kmem_map)";
	else
		map_special = "";
	panic("uvm_tree_sanity %p%s (%s %d): %s", map, map_special, file,
	    line, test_str);
}

/*
 * Check that map is sane.
 */
void
uvm_tree_sanity(struct vm_map *map, char *file, int line)
{
	struct vm_map_entry	*iter;
	vaddr_t			 addr;
	vaddr_t			 min, max, bound; /* Bounds checker. */
	struct uvm_addr_state	*free;

	addr = vm_map_min(map);
	RBT_FOREACH(iter, uvm_map_addr, &map->addr) {
		/*
		 * Valid start, end.
		 * Catch overflow for end+fspace.
		 */
		UVM_ASSERT(map, iter->end >= iter->start, file, line);
		UVM_ASSERT(map, VMMAP_FREE_END(iter) >= iter->end, file, line);

		/* May not be empty. */
		UVM_ASSERT(map, iter->start < VMMAP_FREE_END(iter),
		    file, line);

		/* Addresses for entry must lie within map boundaries. */
		UVM_ASSERT(map, iter->start >= vm_map_min(map) &&
		    VMMAP_FREE_END(iter) <= vm_map_max(map), file, line);

		/* Tree may not have gaps. */
		UVM_ASSERT(map, iter->start == addr, file, line);
		addr = VMMAP_FREE_END(iter);

		/*
		 * Free space may not cross boundaries, unless the same
		 * free list is used on both sides of the border.
		 */
		min = VMMAP_FREE_START(iter);
		max = VMMAP_FREE_END(iter);

		while (min < max &&
		    (bound = uvm_map_boundary(map, min, max)) != max) {
			UVM_ASSERT(map,
			    uvm_map_uaddr(map, bound - 1) ==
			    uvm_map_uaddr(map, bound),
			    file, line);
			min = bound;
		}

		free = uvm_map_uaddr_e(map, iter);
		if (free) {
			UVM_ASSERT(map, (iter->etype & UVM_ET_FREEMAPPED) != 0,
			    file, line);
		} else {
			UVM_ASSERT(map, (iter->etype & UVM_ET_FREEMAPPED) == 0,
			    file, line);
		}
	}
	UVM_ASSERT(map, addr == vm_map_max(map), file, line);
}

void
uvm_tree_size_chk(struct vm_map *map, char *file, int line)
{
	struct vm_map_entry *iter;
	vsize_t size;

	size = 0;
	RBT_FOREACH(iter, uvm_map_addr, &map->addr) {
		if (!UVM_ET_ISHOLE(iter))
			size += iter->end - iter->start;
	}

	if (map->size != size)
		printf("map size = 0x%lx, should be 0x%lx\n", map->size, size);
	UVM_ASSERT(map, map->size == size, file, line);

	vmspace_validate(map);
}

/*
 * This function validates the statistics on vmspace.
 */
void
vmspace_validate(struct vm_map *map)
{
	struct vmspace *vm;
	struct vm_map_entry *iter;
	vaddr_t imin, imax;
	vaddr_t stack_begin, stack_end; /* Position of stack. */
	vsize_t stack, heap; /* Measured sizes. */

	if (!(map->flags & VM_MAP_ISVMSPACE))
		return;

	vm = (struct vmspace *)map;
	stack_begin = MIN((vaddr_t)vm->vm_maxsaddr, (vaddr_t)vm->vm_minsaddr);
	stack_end = MAX((vaddr_t)vm->vm_maxsaddr, (vaddr_t)vm->vm_minsaddr);

	stack = heap = 0;
	RBT_FOREACH(iter, uvm_map_addr, &map->addr) {
		imin = imax = iter->start;

		if (UVM_ET_ISHOLE(iter) || iter->object.uvm_obj != NULL)
			continue;

		/*
		 * Update stack, heap.
		 * Keep in mind that (theoretically) the entries of
		 * userspace and stack may be joined.
		 */
		while (imin != iter->end) {
			/*
			 * Set imax to the first boundary crossed between
			 * imin and stack addresses.
			 */
			imax = iter->end;
			if (imin < stack_begin && imax > stack_begin)
				imax = stack_begin;
			else if (imin < stack_end && imax > stack_end)
				imax = stack_end;

			if (imin >= stack_begin && imin < stack_end)
				stack += imax - imin;
			else
				heap += imax - imin;
			imin = imax;
		}
	}

	heap >>= PAGE_SHIFT;
	if (heap != vm->vm_dused) {
		printf("vmspace stack range: 0x%lx-0x%lx\n",
		    stack_begin, stack_end);
		panic("vmspace_validate: vmspace.vm_dused invalid, "
		    "expected %ld pgs, got %ld pgs in map %p",
		    heap, vm->vm_dused,
		    map);
	}
}

#endif /* VMMAP_DEBUG */

/*
 * uvm_map_init: init mapping system at boot time.   note that we allocate
 * and init the static pool of structs vm_map_entry for the kernel here.
 */
void
uvm_map_init(void)
{
	static struct vm_map_entry kernel_map_entry[MAX_KMAPENT];
	int lcv;

	/* now set up static pool of kernel map entries ... */
	mtx_init(&uvm_kmapent_mtx, IPL_VM);
	SLIST_INIT(&uvm.kentry_free);
	for (lcv = 0 ; lcv < MAX_KMAPENT ; lcv++) {
		SLIST_INSERT_HEAD(&uvm.kentry_free,
		    &kernel_map_entry[lcv], daddrs.addr_kentry);
	}

	/* initialize the map-related pools. */
	pool_init(&uvm_vmspace_pool, sizeof(struct vmspace), 0,
	    IPL_NONE, PR_WAITOK, "vmsppl", NULL);
	pool_init(&uvm_map_entry_pool, sizeof(struct vm_map_entry), 0,
	    IPL_VM, PR_WAITOK, "vmmpepl", NULL);
	pool_init(&uvm_map_entry_kmem_pool, sizeof(struct vm_map_entry), 0,
	    IPL_NONE, 0, "vmmpekpl", NULL);
	pool_sethiwat(&uvm_map_entry_pool, 8192);

	uvm_addr_init();
}

#if defined(DDB)

/*
 * DDB hooks
 */

/*
 * uvm_map_printit: actually prints the map
 */
void
uvm_map_printit(struct vm_map *map, boolean_t full,
    int (*pr)(const char *, ...))
{
	struct vmspace			*vm;
	struct vm_map_entry		*entry;
	struct uvm_addr_state		*free;
	int				 in_free, i;
	char				 buf[8];

	(*pr)("MAP %p: [0x%lx->0x%lx]\n", map, map->min_offset,map->max_offset);
	(*pr)("\tbrk() allocate range: 0x%lx-0x%lx\n",
	    map->b_start, map->b_end);
	(*pr)("\tstack allocate range: 0x%lx-0x%lx\n",
	    map->s_start, map->s_end);
	(*pr)("\tsz=%u, ref=%d, version=%u, flags=0x%x\n",
	    map->size, map->ref_count, map->timestamp,
	    map->flags);
	(*pr)("\tpmap=%p(resident=%d)\n", map->pmap, 
	    pmap_resident_count(map->pmap));

	/* struct vmspace handling. */
	if (map->flags & VM_MAP_ISVMSPACE) {
		vm = (struct vmspace *)map;

		(*pr)("\tvm_refcnt=%d vm_shm=%p vm_rssize=%u vm_swrss=%u\n",
		    vm->vm_refcnt, vm->vm_shm, vm->vm_rssize, vm->vm_swrss);
		(*pr)("\tvm_tsize=%u vm_dsize=%u\n",
		    vm->vm_tsize, vm->vm_dsize);
		(*pr)("\tvm_taddr=%p vm_daddr=%p\n",
		    vm->vm_taddr, vm->vm_daddr);
		(*pr)("\tvm_maxsaddr=%p vm_minsaddr=%p\n",
		    vm->vm_maxsaddr, vm->vm_minsaddr);
	}

	if (!full)
		goto print_uaddr;
	RBT_FOREACH(entry, uvm_map_addr, &map->addr) {
		(*pr)(" - %p: 0x%lx->0x%lx: obj=%p/0x%llx, amap=%p/%d\n",
		    entry, entry->start, entry->end, entry->object.uvm_obj,
		    (long long)entry->offset, entry->aref.ar_amap,
		    entry->aref.ar_pageoff);
		(*pr)("\tsubmap=%c, cow=%c, nc=%c, prot(max)=%d/%d, inh=%d, "
		    "wc=%d, adv=%d\n",
		    (entry->etype & UVM_ET_SUBMAP) ? 'T' : 'F',
		    (entry->etype & UVM_ET_COPYONWRITE) ? 'T' : 'F', 
		    (entry->etype & UVM_ET_NEEDSCOPY) ? 'T' : 'F',
		    entry->protection, entry->max_protection,
		    entry->inheritance, entry->wired_count, entry->advice);

		free = uvm_map_uaddr_e(map, entry);
		in_free = (free != NULL);
		(*pr)("\thole=%c, free=%c, guard=0x%lx, "
		    "free=0x%lx-0x%lx\n",
		    (entry->etype & UVM_ET_HOLE) ? 'T' : 'F',
		    in_free ? 'T' : 'F',
		    entry->guard,
		    VMMAP_FREE_START(entry), VMMAP_FREE_END(entry));
		(*pr)("\tfspace_augment=%lu\n", entry->fspace_augment);
		(*pr)("\tfreemapped=%c, uaddr=%p\n",
		    (entry->etype & UVM_ET_FREEMAPPED) ? 'T' : 'F', free);
		if (free) {
			(*pr)("\t\t(0x%lx-0x%lx %s)\n",
			    free->uaddr_minaddr, free->uaddr_maxaddr,
			    free->uaddr_functions->uaddr_name);
		}
	}

print_uaddr:
	uvm_addr_print(map->uaddr_exe, "exe", full, pr);
	for (i = 0; i < nitems(map->uaddr_any); i++) {
		snprintf(&buf[0], sizeof(buf), "any[%d]", i);
		uvm_addr_print(map->uaddr_any[i], &buf[0], full, pr);
	}
	uvm_addr_print(map->uaddr_brk_stack, "brk/stack", full, pr);
}

/*
 * uvm_object_printit: actually prints the object
 */
void
uvm_object_printit(uobj, full, pr)
	struct uvm_object *uobj;
	boolean_t full;
	int (*pr)(const char *, ...);
{
	struct vm_page *pg;
	int cnt = 0;

	(*pr)("OBJECT %p: pgops=%p, npages=%d, ",
	    uobj, uobj->pgops, uobj->uo_npages);
	if (UVM_OBJ_IS_KERN_OBJECT(uobj))
		(*pr)("refs=<SYSTEM>\n");
	else
		(*pr)("refs=%d\n", uobj->uo_refs);

	if (!full) {
		return;
	}
	(*pr)("  PAGES <pg,offset>:\n  ");
	RBT_FOREACH(pg, uvm_objtree, &uobj->memt) {
		(*pr)("<%p,0x%llx> ", pg, (long long)pg->offset);
		if ((cnt % 3) == 2) {
			(*pr)("\n  ");
		}
		cnt++;
	}
	if ((cnt % 3) != 2) {
		(*pr)("\n");
	}
} 

/*
 * uvm_page_printit: actually print the page
 */
static const char page_flagbits[] =
	"\20\1BUSY\2WANTED\3TABLED\4CLEAN\5CLEANCHK\6RELEASED\7FAKE\10RDONLY"
	"\11ZERO\12DEV\15PAGER1\21FREE\22INACTIVE\23ACTIVE\25ANON\26AOBJ"
	"\27ENCRYPT\31PMAP0\32PMAP1\33PMAP2\34PMAP3\35PMAP4\36PMAP5";

void
uvm_page_printit(pg, full, pr)
	struct vm_page *pg;
	boolean_t full;
	int (*pr)(const char *, ...);
{
	struct vm_page *tpg;
	struct uvm_object *uobj;
	struct pglist *pgl;

	(*pr)("PAGE %p:\n", pg);
	(*pr)("  flags=%b, vers=%d, wire_count=%d, pa=0x%llx\n",
	    pg->pg_flags, page_flagbits, pg->pg_version, pg->wire_count,
	    (long long)pg->phys_addr);
	(*pr)("  uobject=%p, uanon=%p, offset=0x%llx\n",
	    pg->uobject, pg->uanon, (long long)pg->offset);
#if defined(UVM_PAGE_TRKOWN)
	if (pg->pg_flags & PG_BUSY)
		(*pr)("  owning thread = %d, tag=%s",
		    pg->owner, pg->owner_tag);
	else
		(*pr)("  page not busy, no owner");
#else
	(*pr)("  [page ownership tracking disabled]");
#endif
	(*pr)("\tvm_page_md %p\n", &pg->mdpage);

	if (!full)
		return;

	/* cross-verify object/anon */
	if ((pg->pg_flags & PQ_FREE) == 0) {
		if (pg->pg_flags & PQ_ANON) {
			if (pg->uanon == NULL || pg->uanon->an_page != pg)
			    (*pr)("  >>> ANON DOES NOT POINT HERE <<< (%p)\n",
				(pg->uanon) ? pg->uanon->an_page : NULL);
			else
				(*pr)("  anon backpointer is OK\n");
		} else {
			uobj = pg->uobject;
			if (uobj) {
				(*pr)("  checking object list\n");
				RBT_FOREACH(tpg, uvm_objtree, &uobj->memt) {
					if (tpg == pg) {
						break;
					}
				}
				if (tpg)
					(*pr)("  page found on object list\n");
				else
					(*pr)("  >>> PAGE NOT FOUND "
					    "ON OBJECT LIST! <<<\n");
			}
		}
	}

	/* cross-verify page queue */
	if (pg->pg_flags & PQ_FREE) {
		if (uvm_pmr_isfree(pg))
			(*pr)("  page found in uvm_pmemrange\n");
		else
			(*pr)("  >>> page not found in uvm_pmemrange <<<\n");
		pgl = NULL;
	} else if (pg->pg_flags & PQ_INACTIVE) {
		pgl = (pg->pg_flags & PQ_SWAPBACKED) ?
		    &uvm.page_inactive_swp : &uvm.page_inactive_obj;
	} else if (pg->pg_flags & PQ_ACTIVE) {
		pgl = &uvm.page_active;
 	} else {
		pgl = NULL;
	}

	if (pgl) {
		(*pr)("  checking pageq list\n");
		TAILQ_FOREACH(tpg, pgl, pageq) {
			if (tpg == pg) {
				break;
			}
		}
		if (tpg)
			(*pr)("  page found on pageq list\n");
		else
			(*pr)("  >>> PAGE NOT FOUND ON PAGEQ LIST! <<<\n");
	}
}
#endif

/*
 * uvm_map_protect: change map protection
 *
 * => set_max means set max_protection.
 * => map must be unlocked.
 */
int
uvm_map_protect(struct vm_map *map, vaddr_t start, vaddr_t end,
    vm_prot_t new_prot, boolean_t set_max)
{
	struct vm_map_entry *first, *iter;
	vm_prot_t old_prot;
	vm_prot_t mask;
	int error;

	if (start > end)
		return EINVAL;
	start = MAX(start, map->min_offset);
	end = MIN(end, map->max_offset);
	if (start >= end)
		return 0;

	error = 0;
	vm_map_lock(map);

	/*
	 * Set up first and last.
	 * - first will contain first entry at or after start.
	 */
	first = uvm_map_entrybyaddr(&map->addr, start);
	KDASSERT(first != NULL);
	if (first->end < start)
		first = RBT_NEXT(uvm_map_addr, first);

	/* First, check for protection violations. */
	for (iter = first; iter != NULL && iter->start < end;
	    iter = RBT_NEXT(uvm_map_addr, iter)) {
		/* Treat memory holes as free space. */
		if (iter->start == iter->end || UVM_ET_ISHOLE(iter))
			continue;

		if (UVM_ET_ISSUBMAP(iter)) {
			error = EINVAL;
			goto out;
		}
		if ((new_prot & iter->max_protection) != new_prot) {
			error = EACCES;
			goto out;
		}
		if (map == kernel_map &&
		    (new_prot & (PROT_WRITE | PROT_EXEC)) == (PROT_WRITE | PROT_EXEC))
			panic("uvm_map_protect: kernel map W^X violation requested");
	}

	/* Fix protections.  */
	for (iter = first; iter != NULL && iter->start < end;
	    iter = RBT_NEXT(uvm_map_addr, iter)) {
		/* Treat memory holes as free space. */
		if (iter->start == iter->end || UVM_ET_ISHOLE(iter))
			continue;

		old_prot = iter->protection;

		/*
		 * Skip adapting protection iff old and new protection
		 * are equal.
		 */
		if (set_max) {
			if (old_prot == (new_prot & old_prot) &&
			    iter->max_protection == new_prot)
				continue;
		} else {
			if (old_prot == new_prot)
				continue;
		}

		UVM_MAP_CLIP_START(map, iter, start);
		UVM_MAP_CLIP_END(map, iter, end);

		if (set_max) {
			iter->max_protection = new_prot;
			iter->protection &= new_prot;
		} else
			iter->protection = new_prot;

		/*
		 * update physical map if necessary.  worry about copy-on-write
		 * here -- CHECK THIS XXX
		 */
		if (iter->protection != old_prot) {
			mask = UVM_ET_ISCOPYONWRITE(iter) ?
			    ~PROT_WRITE : PROT_MASK;

			/* update pmap */
			if ((iter->protection & mask) == PROT_NONE &&
			    VM_MAPENT_ISWIRED(iter)) {
				/*
				 * TODO(ariane) this is stupid. wired_count
				 * is 0 if not wired, otherwise anything
				 * larger than 0 (incremented once each time
				 * wire is called).
				 * Mostly to be able to undo the damage on
				 * failure. Not the actually be a wired
				 * refcounter...
				 * Originally: iter->wired_count--;
				 * (don't we have to unwire this in the pmap
				 * as well?)
				 */
				iter->wired_count = 0;
			}
			pmap_protect(map->pmap, iter->start, iter->end,
			    iter->protection & mask);
		}

		/*
		 * If the map is configured to lock any future mappings,
		 * wire this entry now if the old protection was PROT_NONE
		 * and the new protection is not PROT_NONE.
		 */
		if ((map->flags & VM_MAP_WIREFUTURE) != 0 &&
		    VM_MAPENT_ISWIRED(iter) == 0 &&
		    old_prot == PROT_NONE &&
		    new_prot != PROT_NONE) {
			if (uvm_map_pageable(map, iter->start, iter->end,
			    FALSE, UVM_LK_ENTER | UVM_LK_EXIT) != 0) {
				/*
				 * If locking the entry fails, remember the
				 * error if it's the first one.  Note we
				 * still continue setting the protection in
				 * the map, but it will return the resource
				 * storage condition regardless.
				 *
				 * XXX Ignore what the actual error is,
				 * XXX just call it a resource shortage
				 * XXX so that it doesn't get confused
				 * XXX what uvm_map_protect() itself would
				 * XXX normally return.
				 */
				error = ENOMEM;
			}
		}
	}
	pmap_update(map->pmap);

out:
	vm_map_unlock(map);
	return error;
}

/*
 * uvmspace_alloc: allocate a vmspace structure.
 *
 * - structure includes vm_map and pmap
 * - XXX: no locking on this structure
 * - refcnt set to 1, rest must be init'd by caller
 */
struct vmspace *
uvmspace_alloc(vaddr_t min, vaddr_t max, boolean_t pageable,
    boolean_t remove_holes)
{
	struct vmspace *vm;

	vm = pool_get(&uvm_vmspace_pool, PR_WAITOK | PR_ZERO);
	uvmspace_init(vm, NULL, min, max, pageable, remove_holes);
	return (vm);
}

/*
 * uvmspace_init: initialize a vmspace structure.
 *
 * - XXX: no locking on this structure
 * - refcnt set to 1, rest must be init'd by caller
 */
void
uvmspace_init(struct vmspace *vm, struct pmap *pmap, vaddr_t min, vaddr_t max,
    boolean_t pageable, boolean_t remove_holes)
{
	KASSERT(pmap == NULL || pmap == pmap_kernel());

	if (pmap)
		pmap_reference(pmap);
	else
		pmap = pmap_create();
	vm->vm_map.pmap = pmap;

	uvm_map_setup(&vm->vm_map, min, max,
	    (pageable ? VM_MAP_PAGEABLE : 0) | VM_MAP_ISVMSPACE);

	vm->vm_refcnt = 1;

	if (remove_holes)
		pmap_remove_holes(vm);
}

/*
 * uvmspace_share: share a vmspace between two processes
 *
 * - XXX: no locking on vmspace
 * - used for vfork
 */

struct vmspace *
uvmspace_share(struct process *pr)
{
	struct vmspace *vm = pr->ps_vmspace;

	vm->vm_refcnt++;
	return vm;
}

/*
 * uvmspace_exec: the process wants to exec a new program
 *
 * - XXX: no locking on vmspace
 */

void
uvmspace_exec(struct proc *p, vaddr_t start, vaddr_t end)
{
	struct process *pr = p->p_p;
	struct vmspace *nvm, *ovm = pr->ps_vmspace;
	struct vm_map *map = &ovm->vm_map;
	struct uvm_map_deadq dead_entries;

	KASSERT((start & (vaddr_t)PAGE_MASK) == 0);
	KASSERT((end & (vaddr_t)PAGE_MASK) == 0 ||
	    (end & (vaddr_t)PAGE_MASK) == (vaddr_t)PAGE_MASK);

	pmap_unuse_final(p);   /* before stack addresses go away */
	TAILQ_INIT(&dead_entries);

	/* see if more than one process is using this vmspace...  */
	if (ovm->vm_refcnt == 1) {
		/*
		 * If pr is the only process using its vmspace then
		 * we can safely recycle that vmspace for the program
		 * that is being exec'd.
		 */

#ifdef SYSVSHM
		/*
		 * SYSV SHM semantics require us to kill all segments on an exec
		 */
		if (ovm->vm_shm)
			shmexit(ovm);
#endif

		/*
		 * POSIX 1003.1b -- "lock future mappings" is revoked
		 * when a process execs another program image.
		 */
		vm_map_lock(map);
		vm_map_modflags(map, 0, VM_MAP_WIREFUTURE);

		/*
		 * now unmap the old program
		 *
		 * Instead of attempting to keep the map valid, we simply
		 * nuke all entries and ask uvm_map_setup to reinitialize
		 * the map to the new boundaries.
		 *
		 * uvm_unmap_remove will actually nuke all entries for us
		 * (as in, not replace them with free-memory entries).
		 */
		uvm_unmap_remove(map, map->min_offset, map->max_offset,
		    &dead_entries, TRUE, FALSE);

		KDASSERT(RBT_EMPTY(uvm_map_addr, &map->addr));

		/* Nuke statistics and boundaries. */
		memset(&ovm->vm_startcopy, 0,
		    (caddr_t) (ovm + 1) - (caddr_t) &ovm->vm_startcopy);


		if (end & (vaddr_t)PAGE_MASK) {
			end += 1;
			if (end == 0) /* overflow */
				end -= PAGE_SIZE;
		}

		/* Setup new boundaries and populate map with entries. */
		map->min_offset = start;
		map->max_offset = end;
		uvm_map_setup_entries(map);
		vm_map_unlock(map);

		/* but keep MMU holes unavailable */
		pmap_remove_holes(ovm);
	} else {
		/*
		 * pr's vmspace is being shared, so we can't reuse
		 * it for pr since it is still being used for others.
		 * allocate a new vmspace for pr
		 */
		nvm = uvmspace_alloc(start, end,
		    (map->flags & VM_MAP_PAGEABLE) ? TRUE : FALSE, TRUE);

		/* install new vmspace and drop our ref to the old one. */
		pmap_deactivate(p);
		p->p_vmspace = pr->ps_vmspace = nvm;
		pmap_activate(p);

		uvmspace_free(ovm);
	}

	/* Release dead entries */
	uvm_unmap_detach(&dead_entries, 0);
}

/*
 * uvmspace_free: free a vmspace data structure
 *
 * - XXX: no locking on vmspace
 */
void
uvmspace_free(struct vmspace *vm)
{
	if (--vm->vm_refcnt == 0) {
		/*
		 * lock the map, to wait out all other references to it.  delete
		 * all of the mappings and pages they hold, then call the pmap
		 * module to reclaim anything left.
		 */
#ifdef SYSVSHM
		/* Get rid of any SYSV shared memory segments. */
		if (vm->vm_shm != NULL)
			shmexit(vm);
#endif

		uvm_map_teardown(&vm->vm_map);
		pool_put(&uvm_vmspace_pool, vm);
	}
}

/*
 * uvm_share: Map the address range [srcaddr, srcaddr + sz) in
 * srcmap to the address range [dstaddr, dstaddr + sz) in
 * dstmap.
 *
 * The whole address range in srcmap must be backed by an object
 * (no holes).
 *
 * If successful, the address ranges share memory and the destination
 * address range uses the protection flags in prot.
 *
 * This routine assumes that sz is a multiple of PAGE_SIZE and
 * that dstaddr and srcaddr are page-aligned.
 */
int
uvm_share(struct vm_map *dstmap, vaddr_t dstaddr, vm_prot_t prot,
    struct vm_map *srcmap, vaddr_t srcaddr, vsize_t sz)
{
	int ret = 0;
	vaddr_t unmap_end;
	vaddr_t dstva;
	vsize_t off, len, n = sz;
	struct vm_map_entry *first = NULL, *last = NULL;
	struct vm_map_entry *src_entry, *psrc_entry = NULL;
	struct uvm_map_deadq dead;

	if (srcaddr >= srcmap->max_offset || sz > srcmap->max_offset - srcaddr)
		return EINVAL;

	TAILQ_INIT(&dead);
	vm_map_lock(dstmap);
	vm_map_lock_read(srcmap);

	if (!uvm_map_isavail(dstmap, NULL, &first, &last, dstaddr, sz)) {
		ret = ENOMEM;
		goto exit_unlock;
	}
	if (!uvm_map_lookup_entry(srcmap, srcaddr, &src_entry)) {
		ret = EINVAL;
		goto exit_unlock;
	}

	unmap_end = dstaddr;
	for (; src_entry != NULL;
	    psrc_entry = src_entry,
	    src_entry = RBT_NEXT(uvm_map_addr, src_entry)) {
		/* hole in address space, bail out */
		if (psrc_entry != NULL && psrc_entry->end != src_entry->start)
			break;
		if (src_entry->start >= srcaddr + sz)
			break;

		if (UVM_ET_ISSUBMAP(src_entry))
			panic("uvm_share: encountered a submap (illegal)");
		if (!UVM_ET_ISCOPYONWRITE(src_entry) &&
		    UVM_ET_ISNEEDSCOPY(src_entry))
			panic("uvm_share: non-copy_on_write map entries "
			    "marked needs_copy (illegal)");

		dstva = dstaddr;
		if (src_entry->start > srcaddr) {
			dstva += src_entry->start - srcaddr;
			off = 0;
		} else
			off = srcaddr - src_entry->start;

		if (n < src_entry->end - src_entry->start)
			len = n;
		else
			len = src_entry->end - src_entry->start;
		n -= len;

		if (uvm_mapent_share(dstmap, dstva, len, off, prot, prot,
		    srcmap, src_entry, &dead) == NULL)
			break;

		unmap_end = dstva + len;
		if (n == 0)
			goto exit_unlock;
	}

	ret = EINVAL;
	uvm_unmap_remove(dstmap, dstaddr, unmap_end, &dead, FALSE, TRUE);

exit_unlock:
	vm_map_unlock_read(srcmap);
	vm_map_unlock(dstmap);
	uvm_unmap_detach(&dead, 0);

	return ret;
}

/*
 * Clone map entry into other map.
 *
 * Mapping will be placed at dstaddr, for the same length.
 * Space must be available.
 * Reference counters are incremented.
 */
struct vm_map_entry *
uvm_mapent_clone(struct vm_map *dstmap, vaddr_t dstaddr, vsize_t dstlen,
    vsize_t off, vm_prot_t prot, vm_prot_t maxprot,
    struct vm_map_entry *old_entry, struct uvm_map_deadq *dead,
    int mapent_flags, int amap_share_flags)
{
	struct vm_map_entry *new_entry, *first, *last;

	KDASSERT(!UVM_ET_ISSUBMAP(old_entry));

	/* Create new entry (linked in on creation). Fill in first, last. */
	first = last = NULL;
	if (!uvm_map_isavail(dstmap, NULL, &first, &last, dstaddr, dstlen)) {
		panic("uvmspace_fork: no space in map for "
		    "entry in empty map");
	}
	new_entry = uvm_map_mkentry(dstmap, first, last,
	    dstaddr, dstlen, mapent_flags, dead, NULL);
	if (new_entry == NULL)
		return NULL;
	/* old_entry -> new_entry */
	new_entry->object = old_entry->object;
	new_entry->offset = old_entry->offset;
	new_entry->aref = old_entry->aref;
	new_entry->etype |= old_entry->etype & ~UVM_ET_FREEMAPPED;
	new_entry->protection = prot;
	new_entry->max_protection = maxprot;
	new_entry->inheritance = old_entry->inheritance;
	new_entry->advice = old_entry->advice;

	/* gain reference to object backing the map (can't be a submap). */
	if (new_entry->aref.ar_amap) {
		new_entry->aref.ar_pageoff += off >> PAGE_SHIFT;
		amap_ref(new_entry->aref.ar_amap, new_entry->aref.ar_pageoff,
		    (new_entry->end - new_entry->start) >> PAGE_SHIFT,
		    amap_share_flags);
	}

	if (UVM_ET_ISOBJ(new_entry) &&
	    new_entry->object.uvm_obj->pgops->pgo_reference) {
		new_entry->offset += off;
		new_entry->object.uvm_obj->pgops->pgo_reference
		    (new_entry->object.uvm_obj);
	}

	return new_entry;
}

struct vm_map_entry *
uvm_mapent_share(struct vm_map *dstmap, vaddr_t dstaddr, vsize_t dstlen,
    vsize_t off, vm_prot_t prot, vm_prot_t maxprot, struct vm_map *old_map,
    struct vm_map_entry *old_entry, struct uvm_map_deadq *dead)
{
	/*
	 * If old_entry refers to a copy-on-write region that has not yet been
	 * written to (needs_copy flag is set), then we need to allocate a new
	 * amap for old_entry.
	 *
	 * If we do not do this, and the process owning old_entry does a copy-on
	 * write later, old_entry and new_entry will refer to different memory
	 * regions, and the memory between the processes is no longer shared.
	 *
	 * [in other words, we need to clear needs_copy]
	 */

	if (UVM_ET_ISNEEDSCOPY(old_entry)) {
		/* get our own amap, clears needs_copy */
		amap_copy(old_map, old_entry, M_WAITOK, FALSE,
		    0, 0);
		/* XXXCDC: WAITOK??? */
	}

	return uvm_mapent_clone(dstmap, dstaddr, dstlen, off,
	    prot, maxprot, old_entry, dead, 0, AMAP_SHARED);
}

/*
 * share the mapping: this means we want the old and
 * new entries to share amaps and backing objects.
 */
struct vm_map_entry *
uvm_mapent_forkshared(struct vmspace *new_vm, struct vm_map *new_map,
    struct vm_map *old_map,
    struct vm_map_entry *old_entry, struct uvm_map_deadq *dead)
{
	struct vm_map_entry *new_entry;

	new_entry = uvm_mapent_share(new_map, old_entry->start,
	    old_entry->end - old_entry->start, 0, old_entry->protection,
	    old_entry->max_protection, old_map, old_entry, dead);

	/* 
	 * pmap_copy the mappings: this routine is optional
	 * but if it is there it will reduce the number of
	 * page faults in the new proc.
	 */
	if (!UVM_ET_ISHOLE(new_entry))
		pmap_copy(new_map->pmap, old_map->pmap, new_entry->start,
		    (new_entry->end - new_entry->start), new_entry->start);

	return (new_entry);
}

/*
 * copy-on-write the mapping (using mmap's
 * MAP_PRIVATE semantics)
 *
 * allocate new_entry, adjust reference counts.  
 * (note that new references are read-only).
 */
struct vm_map_entry *
uvm_mapent_forkcopy(struct vmspace *new_vm, struct vm_map *new_map,
    struct vm_map *old_map,
    struct vm_map_entry *old_entry, struct uvm_map_deadq *dead)
{
	struct vm_map_entry	*new_entry;
	boolean_t		 protect_child;

	new_entry = uvm_mapent_clone(new_map, old_entry->start,
	    old_entry->end - old_entry->start, 0, old_entry->protection,
	    old_entry->max_protection, old_entry, dead, 0, 0);

	new_entry->etype |=
	    (UVM_ET_COPYONWRITE|UVM_ET_NEEDSCOPY);

	/*
	 * the new entry will need an amap.  it will either
	 * need to be copied from the old entry or created
	 * from scratch (if the old entry does not have an
	 * amap).  can we defer this process until later
	 * (by setting "needs_copy") or do we need to copy
	 * the amap now?
	 *
	 * we must copy the amap now if any of the following
	 * conditions hold:
	 * 1. the old entry has an amap and that amap is
	 *    being shared.  this means that the old (parent)
	 *    process is sharing the amap with another 
	 *    process.  if we do not clear needs_copy here
	 *    we will end up in a situation where both the
	 *    parent and child process are referring to the
	 *    same amap with "needs_copy" set.  if the 
	 *    parent write-faults, the fault routine will
	 *    clear "needs_copy" in the parent by allocating
	 *    a new amap.   this is wrong because the 
	 *    parent is supposed to be sharing the old amap
	 *    and the new amap will break that.
	 *
	 * 2. if the old entry has an amap and a non-zero
	 *    wire count then we are going to have to call
	 *    amap_cow_now to avoid page faults in the 
	 *    parent process.   since amap_cow_now requires
	 *    "needs_copy" to be clear we might as well
	 *    clear it here as well.
	 *
	 */
	if (old_entry->aref.ar_amap != NULL &&
	    ((amap_flags(old_entry->aref.ar_amap) &
	    AMAP_SHARED) != 0 ||
	    VM_MAPENT_ISWIRED(old_entry))) {
		amap_copy(new_map, new_entry, M_WAITOK, FALSE,
		    0, 0);
		/* XXXCDC: M_WAITOK ... ok? */
	}

	/*
	 * if the parent's entry is wired down, then the
	 * parent process does not want page faults on
	 * access to that memory.  this means that we
	 * cannot do copy-on-write because we can't write
	 * protect the old entry.   in this case we
	 * resolve all copy-on-write faults now, using
	 * amap_cow_now.   note that we have already
	 * allocated any needed amap (above).
	 */
	if (VM_MAPENT_ISWIRED(old_entry)) {
		/* 
		 * resolve all copy-on-write faults now
		 * (note that there is nothing to do if 
		 * the old mapping does not have an amap).
		 * XXX: is it worthwhile to bother with
		 * pmap_copy in this case?
		 */
		if (old_entry->aref.ar_amap)
			amap_cow_now(new_map, new_entry);
	} else {
		if (old_entry->aref.ar_amap) {
			/*
			 * setup mappings to trigger copy-on-write faults
			 * we must write-protect the parent if it has
			 * an amap and it is not already "needs_copy"...
			 * if it is already "needs_copy" then the parent
			 * has already been write-protected by a previous
			 * fork operation.
			 *
			 * if we do not write-protect the parent, then
			 * we must be sure to write-protect the child
			 * after the pmap_copy() operation.
			 *
			 * XXX: pmap_copy should have some way of telling
			 * us that it didn't do anything so we can avoid
			 * calling pmap_protect needlessly.
			 */
			if (!UVM_ET_ISNEEDSCOPY(old_entry)) {
				if (old_entry->max_protection & PROT_WRITE) {
					pmap_protect(old_map->pmap,
					    old_entry->start,
					    old_entry->end,
					    old_entry->protection &
					    ~PROT_WRITE);
					pmap_update(old_map->pmap);
				}
				old_entry->etype |= UVM_ET_NEEDSCOPY;
			}

	  		/* parent must now be write-protected */
	  		protect_child = FALSE;
		} else {
			/*
			 * we only need to protect the child if the 
			 * parent has write access.
			 */
			if (old_entry->max_protection & PROT_WRITE)
				protect_child = TRUE;
			else
				protect_child = FALSE;
		}
		/*
		 * copy the mappings
		 * XXX: need a way to tell if this does anything
		 */
		if (!UVM_ET_ISHOLE(new_entry))
			pmap_copy(new_map->pmap, old_map->pmap,
			    new_entry->start,
			    (old_entry->end - old_entry->start),
			    old_entry->start);

		/* protect the child's mappings if necessary */
		if (protect_child) {
			pmap_protect(new_map->pmap, new_entry->start,
			    new_entry->end,
			    new_entry->protection &
			    ~PROT_WRITE);
		}
	}

	return (new_entry);
}

/*
 * zero the mapping: the new entry will be zero initialized
 */
struct vm_map_entry *
uvm_mapent_forkzero(struct vmspace *new_vm, struct vm_map *new_map,
    struct vm_map *old_map,
    struct vm_map_entry *old_entry, struct uvm_map_deadq *dead)
{
	struct vm_map_entry *new_entry;

	new_entry = uvm_mapent_clone(new_map, old_entry->start,
	    old_entry->end - old_entry->start, 0, old_entry->protection,
	    old_entry->max_protection, old_entry, dead, 0, 0);

	new_entry->etype |=
	    (UVM_ET_COPYONWRITE|UVM_ET_NEEDSCOPY);

	if (new_entry->aref.ar_amap) {
		amap_unref(new_entry->aref.ar_amap, new_entry->aref.ar_pageoff,
		    atop(new_entry->end - new_entry->start), 0);
		new_entry->aref.ar_amap = NULL;
		new_entry->aref.ar_pageoff = 0;
	}

	if (UVM_ET_ISOBJ(new_entry)) {
		if (new_entry->object.uvm_obj->pgops->pgo_detach)
			new_entry->object.uvm_obj->pgops->pgo_detach(
			    new_entry->object.uvm_obj);
		new_entry->object.uvm_obj = NULL;
		new_entry->etype &= ~UVM_ET_OBJ;
	}

	return (new_entry);
}

/*
 * uvmspace_fork: fork a process' main map
 *
 * => create a new vmspace for child process from parent.
 * => parent's map must not be locked.
 */
struct vmspace *
uvmspace_fork(struct process *pr)
{
	struct vmspace *vm1 = pr->ps_vmspace;
	struct vmspace *vm2;
	struct vm_map *old_map = &vm1->vm_map;
	struct vm_map *new_map;
	struct vm_map_entry *old_entry, *new_entry;
	struct uvm_map_deadq dead;

	vm_map_lock(old_map);

	vm2 = uvmspace_alloc(old_map->min_offset, old_map->max_offset,
	    (old_map->flags & VM_MAP_PAGEABLE) ? TRUE : FALSE, FALSE);
	memcpy(&vm2->vm_startcopy, &vm1->vm_startcopy,
	    (caddr_t) (vm1 + 1) - (caddr_t) &vm1->vm_startcopy);
	vm2->vm_dused = 0; /* Statistic managed by us. */
	new_map = &vm2->vm_map;
	vm_map_lock(new_map);

	/* go entry-by-entry */
	TAILQ_INIT(&dead);
	RBT_FOREACH(old_entry, uvm_map_addr, &old_map->addr) {
		if (old_entry->start == old_entry->end)
			continue;

		/* first, some sanity checks on the old entry */
		if (UVM_ET_ISSUBMAP(old_entry)) {
			panic("fork: encountered a submap during fork "
			    "(illegal)");
		}

		if (!UVM_ET_ISCOPYONWRITE(old_entry) &&
		    UVM_ET_ISNEEDSCOPY(old_entry)) {
			panic("fork: non-copy_on_write map entry marked "
			    "needs_copy (illegal)");
		}

		/* Apply inheritance. */
		switch (old_entry->inheritance) {
		case MAP_INHERIT_SHARE:
			new_entry = uvm_mapent_forkshared(vm2, new_map,
			    old_map, old_entry, &dead);
			break;
		case MAP_INHERIT_COPY:
			new_entry = uvm_mapent_forkcopy(vm2, new_map,
			    old_map, old_entry, &dead);
			break;
		case MAP_INHERIT_ZERO:
			new_entry = uvm_mapent_forkzero(vm2, new_map,
			    old_map, old_entry, &dead);
			break;
		default:
			continue;
		}

	 	/* Update process statistics. */
		if (!UVM_ET_ISHOLE(new_entry))
			new_map->size += new_entry->end - new_entry->start;
		if (!UVM_ET_ISOBJ(new_entry) && !UVM_ET_ISHOLE(new_entry)) {
			vm2->vm_dused += uvmspace_dused(
			    new_map, new_entry->start, new_entry->end);
		}
	}

	vm_map_unlock(old_map); 
	vm_map_unlock(new_map); 

	/*
	 * This can actually happen, if multiple entries described a
	 * space in which an entry was inherited.
	 */
	uvm_unmap_detach(&dead, 0);

#ifdef SYSVSHM
	if (vm1->vm_shm)
		shmfork(vm1, vm2);
#endif

	return vm2;    
}

/*
 * uvm_map_hint: return the beginning of the best area suitable for
 * creating a new mapping with "prot" protection.
 */
vaddr_t
uvm_map_hint(struct vmspace *vm, vm_prot_t prot, vaddr_t minaddr,
    vaddr_t maxaddr)
{
	vaddr_t addr;
	vaddr_t spacing;

#ifdef __i386__
	/*
	 * If executable skip first two pages, otherwise start
	 * after data + heap region.
	 */
	if ((prot & PROT_EXEC) != 0 &&
	    (vaddr_t)vm->vm_daddr >= I386_MAX_EXE_ADDR) {
		addr = (PAGE_SIZE*2) +
		    (arc4random() & (I386_MAX_EXE_ADDR / 2 - 1));
		return (round_page(addr));
	}
#endif

#if defined (__LP64__)
	spacing = (MIN((4UL * 1024 * 1024 * 1024), BRKSIZ) - 1);
#else
	spacing = (MIN((256 * 1024 * 1024), BRKSIZ) - 1);
#endif

	addr = (vaddr_t)vm->vm_daddr;
	/*
	 * Start malloc/mmap after the brk.
	 * If the random spacing area has been used up,
	 * the brk area becomes fair game for mmap as well.
	 */
	if (vm->vm_dused < spacing >> PAGE_SHIFT)
		addr += BRKSIZ;
	if (addr < maxaddr) {
		while (spacing > maxaddr - addr)
			spacing >>= 1;
	}
	addr += arc4random() & spacing;
	return (round_page(addr));
}

/*
 * uvm_map_submap: punch down part of a map into a submap
 *
 * => only the kernel_map is allowed to be submapped
 * => the purpose of submapping is to break up the locking granularity
 *	of a larger map
 * => the range specified must have been mapped previously with a uvm_map()
 *	call [with uobj==NULL] to create a blank map entry in the main map.
 *	[And it had better still be blank!]
 * => maps which contain submaps should never be copied or forked.
 * => to remove a submap, use uvm_unmap() on the main map 
 *	and then uvm_map_deallocate() the submap.
 * => main map must be unlocked.
 * => submap must have been init'd and have a zero reference count.
 *	[need not be locked as we don't actually reference it]
 */
int
uvm_map_submap(struct vm_map *map, vaddr_t start, vaddr_t end,
    struct vm_map *submap)
{
	struct vm_map_entry *entry;
	int result;

	if (start > map->max_offset || end > map->max_offset ||
	    start < map->min_offset || end < map->min_offset)
		return EINVAL;

	vm_map_lock(map);

	if (uvm_map_lookup_entry(map, start, &entry)) {
		UVM_MAP_CLIP_START(map, entry, start);
		UVM_MAP_CLIP_END(map, entry, end);
	} else
		entry = NULL;

	if (entry != NULL && 
	    entry->start == start && entry->end == end &&
	    entry->object.uvm_obj == NULL && entry->aref.ar_amap == NULL &&
	    !UVM_ET_ISCOPYONWRITE(entry) && !UVM_ET_ISNEEDSCOPY(entry)) {
		entry->etype |= UVM_ET_SUBMAP;
		entry->object.sub_map = submap;
		entry->offset = 0;
		uvm_map_reference(submap);
		result = 0;
	} else
		result = EINVAL;

	vm_map_unlock(map);
	return(result);
}

/*
 * uvm_map_checkprot: check protection in map
 *
 * => must allow specific protection in a fully allocated region.
 * => map mut be read or write locked by caller.
 */
boolean_t
uvm_map_checkprot(struct vm_map *map, vaddr_t start, vaddr_t end,
    vm_prot_t protection)
{
	struct vm_map_entry *entry;

	if (start < map->min_offset || end > map->max_offset || start > end)
		return FALSE;
	if (start == end)
		return TRUE;

	/*
	 * Iterate entries.
	 */
	for (entry = uvm_map_entrybyaddr(&map->addr, start);
	    entry != NULL && entry->start < end;
	    entry = RBT_NEXT(uvm_map_addr, entry)) {
		/* Fail if a hole is found. */
		if (UVM_ET_ISHOLE(entry) ||
		    (entry->end < end && entry->end != VMMAP_FREE_END(entry)))
			return FALSE;

		/* Check protection. */
		if ((entry->protection & protection) != protection)
			return FALSE;
	}
	return TRUE;
}

/*
 * uvm_map_create: create map
 */
vm_map_t
uvm_map_create(pmap_t pmap, vaddr_t min, vaddr_t max, int flags)
{
	vm_map_t map;

	map = malloc(sizeof *map, M_VMMAP, M_WAITOK);
	map->pmap = pmap;
	uvm_map_setup(map, min, max, flags);
	return (map);
}

/*
 * uvm_map_deallocate: drop reference to a map
 *
 * => caller must not lock map
 * => we will zap map if ref count goes to zero
 */
void
uvm_map_deallocate(vm_map_t map)
{
	int c;
	struct uvm_map_deadq dead;

	c = --map->ref_count;
	if (c > 0) {
		return;
	}

	/*
	 * all references gone.   unmap and free.
	 *
	 * No lock required: we are only one to access this map.
	 */
	TAILQ_INIT(&dead);
	uvm_tree_sanity(map, __FILE__, __LINE__);
	uvm_unmap_remove(map, map->min_offset, map->max_offset, &dead,
	    TRUE, FALSE);
	pmap_destroy(map->pmap);
	KASSERT(RBT_EMPTY(uvm_map_addr, &map->addr));
	free(map, M_VMMAP, sizeof *map);

	uvm_unmap_detach(&dead, 0);
}

/* 
 * uvm_map_inherit: set inheritance code for range of addrs in map.
 *
 * => map must be unlocked
 * => note that the inherit code is used during a "fork".  see fork
 *	code for details.
 */
int
uvm_map_inherit(struct vm_map *map, vaddr_t start, vaddr_t end,
    vm_inherit_t new_inheritance)
{
	struct vm_map_entry *entry;

	switch (new_inheritance) {
	case MAP_INHERIT_NONE:
	case MAP_INHERIT_COPY:
	case MAP_INHERIT_SHARE:
	case MAP_INHERIT_ZERO:
		break;
	default:
		return (EINVAL);
	}

	if (start > end)
		return EINVAL;
	start = MAX(start, map->min_offset);
	end = MIN(end, map->max_offset);
	if (start >= end)
		return 0;

	vm_map_lock(map);

	entry = uvm_map_entrybyaddr(&map->addr, start);
	if (entry->end > start)
		UVM_MAP_CLIP_START(map, entry, start);
	else
		entry = RBT_NEXT(uvm_map_addr, entry);

	while (entry != NULL && entry->start < end) {
		UVM_MAP_CLIP_END(map, entry, end);
		entry->inheritance = new_inheritance;
		entry = RBT_NEXT(uvm_map_addr, entry);
	}

	vm_map_unlock(map);
	return (0);
}

/* 
 * uvm_map_advice: set advice code for range of addrs in map.
 *
 * => map must be unlocked
 */
int
uvm_map_advice(struct vm_map *map, vaddr_t start, vaddr_t end, int new_advice)
{
	struct vm_map_entry *entry;

	switch (new_advice) {
	case MADV_NORMAL:
	case MADV_RANDOM:
	case MADV_SEQUENTIAL:
		break;
	default:
		return (EINVAL);
	}

	if (start > end)
		return EINVAL;
	start = MAX(start, map->min_offset);
	end = MIN(end, map->max_offset);
	if (start >= end)
		return 0;

	vm_map_lock(map);

	entry = uvm_map_entrybyaddr(&map->addr, start);
	if (entry != NULL && entry->end > start)
		UVM_MAP_CLIP_START(map, entry, start);
	else if (entry!= NULL)
		entry = RBT_NEXT(uvm_map_addr, entry);

	/*
	 * XXXJRT: disallow holes?
	 */
	while (entry != NULL && entry->start < end) {
		UVM_MAP_CLIP_END(map, entry, end);
		entry->advice = new_advice;
		entry = RBT_NEXT(uvm_map_addr, entry);
	}

	vm_map_unlock(map);
	return (0);
}

/*
 * uvm_map_extract: extract a mapping from a map and put it somewhere
 * in the kernel_map, setting protection to max_prot.
 *
 * => map should be unlocked (we will write lock it and kernel_map)
 * => returns 0 on success, error code otherwise
 * => start must be page aligned
 * => len must be page sized
 * => flags:
 *      UVM_EXTRACT_FIXPROT: set prot to maxprot as we go
 * Mappings are QREF's.
 */
int
uvm_map_extract(struct vm_map *srcmap, vaddr_t start, vsize_t len,
    vaddr_t *dstaddrp, int flags)
{
	struct uvm_map_deadq dead;
	struct vm_map_entry *first, *entry, *newentry, *tmp1, *tmp2;
	vaddr_t dstaddr;
	vaddr_t end;
	vaddr_t cp_start;
	vsize_t cp_len, cp_off;
	int error;

	TAILQ_INIT(&dead);
	end = start + len;

	/*
	 * Sanity check on the parameters.
	 * Also, since the mapping may not contain gaps, error out if the
	 * mapped area is not in source map.
	 */
	if ((start & (vaddr_t)PAGE_MASK) != 0 ||
	    (end & (vaddr_t)PAGE_MASK) != 0 || end < start)
		return EINVAL;
	if (start < srcmap->min_offset || end > srcmap->max_offset)
		return EINVAL;

	/* Initialize dead entries. Handle len == 0 case. */
	if (len == 0)
		return 0;

	/* Acquire lock on srcmap. */
	vm_map_lock(srcmap);

	/* Lock srcmap, lookup first and last entry in <start,len>. */
	first = uvm_map_entrybyaddr(&srcmap->addr, start);

	/* Check that the range is contiguous. */
	for (entry = first; entry != NULL && entry->end < end;
	    entry = RBT_NEXT(uvm_map_addr, entry)) {
		if (VMMAP_FREE_END(entry) != entry->end ||
		    UVM_ET_ISHOLE(entry)) {
			error = EINVAL;
			goto fail;
		}
	}
	if (entry == NULL || UVM_ET_ISHOLE(entry)) {
		error = EINVAL;
		goto fail;
	}

	/*
	 * Handle need-copy flag.
	 */
	for (entry = first; entry != NULL && entry->start < end;
	    entry = RBT_NEXT(uvm_map_addr, entry)) {
		if (UVM_ET_ISNEEDSCOPY(entry))
			amap_copy(srcmap, entry, M_NOWAIT, TRUE, start, end);
		if (UVM_ET_ISNEEDSCOPY(entry)) {
			/*
			 * amap_copy failure
			 */
			error = ENOMEM;
			goto fail;
		}
	}

	/* Lock destination map (kernel_map). */
	vm_map_lock(kernel_map);

	if (uvm_map_findspace(kernel_map, &tmp1, &tmp2, &dstaddr, len,
	    MAX(PAGE_SIZE, PMAP_PREFER_ALIGN()), PMAP_PREFER_OFFSET(start),
	    PROT_NONE, 0) != 0) {
		error = ENOMEM;
		goto fail2;
	}
	*dstaddrp = dstaddr;

	/*
	 * We now have srcmap and kernel_map locked.
	 * dstaddr contains the destination offset in dstmap.
	 */
	/* step 1: start looping through map entries, performing extraction. */
	for (entry = first; entry != NULL && entry->start < end;
	    entry = RBT_NEXT(uvm_map_addr, entry)) {
		KDASSERT(!UVM_ET_ISNEEDSCOPY(entry));
		if (UVM_ET_ISHOLE(entry))
			continue;

		/* Calculate uvm_mapent_clone parameters. */
		cp_start = entry->start;
		if (cp_start < start) {
			cp_off = start - cp_start;
			cp_start = start;
		} else
			cp_off = 0;
		cp_len = MIN(entry->end, end) - cp_start;

		newentry = uvm_mapent_clone(kernel_map,
		    cp_start - start + dstaddr, cp_len, cp_off,
		    entry->protection, entry->max_protection,
		    entry, &dead, flags, AMAP_SHARED | AMAP_REFALL);
		if (newentry == NULL) {
			error = ENOMEM;
			goto fail2_unmap;
		}
		kernel_map->size += cp_len;
		if (flags & UVM_EXTRACT_FIXPROT)
			newentry->protection = newentry->max_protection;

		/*
		 * Step 2: perform pmap copy.
		 * (Doing this in the loop saves one RB traversal.)
		 */
		pmap_copy(kernel_map->pmap, srcmap->pmap,
		    cp_start - start + dstaddr, cp_len, cp_start);
	}
	pmap_update(kernel_map->pmap);

	error = 0;

	/* Unmap copied entries on failure. */
fail2_unmap:
	if (error) {
		uvm_unmap_remove(kernel_map, dstaddr, dstaddr + len, &dead,
		    FALSE, TRUE);
	}

	/* Release maps, release dead entries. */
fail2:
	vm_map_unlock(kernel_map);

fail:
	vm_map_unlock(srcmap);

	uvm_unmap_detach(&dead, 0);

	return error;
}

/*
 * uvm_map_clean: clean out a map range
 *
 * => valid flags:
 *   if (flags & PGO_CLEANIT): dirty pages are cleaned first
 *   if (flags & PGO_SYNCIO): dirty pages are written synchronously
 *   if (flags & PGO_DEACTIVATE): any cached pages are deactivated after clean
 *   if (flags & PGO_FREE): any cached pages are freed after clean
 * => returns an error if any part of the specified range isn't mapped
 * => never a need to flush amap layer since the anonymous memory has 
 *	no permanent home, but may deactivate pages there
 * => called from sys_msync() and sys_madvise()
 * => caller must not write-lock map (read OK).
 * => we may sleep while cleaning if SYNCIO [with map read-locked]
 */

int
uvm_map_clean(struct vm_map *map, vaddr_t start, vaddr_t end, int flags)
{
	struct vm_map_entry *first, *entry;
	struct vm_amap *amap;
	struct vm_anon *anon;
	struct vm_page *pg;
	struct uvm_object *uobj;
	vaddr_t cp_start, cp_end;
	int refs;
	int error;
	boolean_t rv;

	KASSERT((flags & (PGO_FREE|PGO_DEACTIVATE)) !=
	    (PGO_FREE|PGO_DEACTIVATE));

	if (start > end || start < map->min_offset || end > map->max_offset)
		return EINVAL;

	vm_map_lock_read(map);
	first = uvm_map_entrybyaddr(&map->addr, start);

	/* Make a first pass to check for holes. */
	for (entry = first; entry != NULL && entry->start < end;
	    entry = RBT_NEXT(uvm_map_addr, entry)) {
		if (UVM_ET_ISSUBMAP(entry)) {
			vm_map_unlock_read(map);
			return EINVAL;
		}
		if (UVM_ET_ISSUBMAP(entry) ||
		    UVM_ET_ISHOLE(entry) ||
		    (entry->end < end &&
		    VMMAP_FREE_END(entry) != entry->end)) {
			vm_map_unlock_read(map);
			return EFAULT;
		}
	}

	error = 0;
	for (entry = first; entry != NULL && entry->start < end;
	    entry = RBT_NEXT(uvm_map_addr, entry)) {
		amap = entry->aref.ar_amap;	/* top layer */
		if (UVM_ET_ISOBJ(entry))
			uobj = entry->object.uvm_obj;
		else
			uobj = NULL;

		/*
		 * No amap cleaning necessary if:
		 *  - there's no amap
		 *  - we're not deactivating or freeing pages.
		 */
		if (amap == NULL || (flags & (PGO_DEACTIVATE|PGO_FREE)) == 0)
			goto flush_object;

		cp_start = MAX(entry->start, start);
		cp_end = MIN(entry->end, end);

		for (; cp_start != cp_end; cp_start += PAGE_SIZE) {
			anon = amap_lookup(&entry->aref,
			    cp_start - entry->start);
			if (anon == NULL)
				continue;

			pg = anon->an_page;
			if (pg == NULL) {
				continue;
			}
			KASSERT(pg->pg_flags & PQ_ANON);

			switch (flags & (PGO_CLEANIT|PGO_FREE|PGO_DEACTIVATE)) {
			/*
			 * XXX In these first 3 cases, we always just
			 * XXX deactivate the page.  We may want to
			 * XXX handle the different cases more
			 * XXX specifically, in the future.
			 */
			case PGO_CLEANIT|PGO_FREE:
			case PGO_CLEANIT|PGO_DEACTIVATE:
			case PGO_DEACTIVATE:
deactivate_it:
				/* skip the page if it's wired */
				if (pg->wire_count != 0)
					break;

				uvm_lock_pageq();

				KASSERT(pg->uanon == anon);

				/* zap all mappings for the page. */
				pmap_page_protect(pg, PROT_NONE);

				/* ...and deactivate the page. */
				uvm_pagedeactivate(pg);

				uvm_unlock_pageq();
				break;
			case PGO_FREE:
				/*
				 * If there are multiple references to
				 * the amap, just deactivate the page.
				 */
				if (amap_refs(amap) > 1)
					goto deactivate_it;

				/* XXX skip the page if it's wired */
				if (pg->wire_count != 0) {
					break;
				}
				amap_unadd(&entry->aref,
				    cp_start - entry->start);
				refs = --anon->an_ref;
				if (refs == 0)
					uvm_anfree(anon);
				break;
			default:
				panic("uvm_map_clean: weird flags");
			}
		}

flush_object:
		cp_start = MAX(entry->start, start);
		cp_end = MIN(entry->end, end);

		/*
		 * flush pages if we've got a valid backing object.
		 *
		 * Don't PGO_FREE if we don't have write permission
		 * and don't flush if this is a copy-on-write object
		 * since we can't know our permissions on it.
		 */
		if (uobj != NULL &&
		    ((flags & PGO_FREE) == 0 ||
		     ((entry->max_protection & PROT_WRITE) != 0 &&
		      (entry->etype & UVM_ET_COPYONWRITE) == 0))) {
			rv = uobj->pgops->pgo_flush(uobj,
			    cp_start - entry->start + entry->offset,
			    cp_end - entry->start + entry->offset, flags);

			if (rv == FALSE)
				error = EFAULT;
		}
	}

	vm_map_unlock_read(map);
	return error;
}

/*
 * UVM_MAP_CLIP_END implementation
 */
void
uvm_map_clip_end(struct vm_map *map, struct vm_map_entry *entry, vaddr_t addr)
{
	struct vm_map_entry *tmp;

	KASSERT(entry->start < addr && VMMAP_FREE_END(entry) > addr);
	tmp = uvm_mapent_alloc(map, 0);

	/* Invoke splitentry. */
	uvm_map_splitentry(map, entry, tmp, addr);
}

/*
 * UVM_MAP_CLIP_START implementation
 *
 * Clippers are required to not change the pointers to the entry they are
 * clipping on.
 * Since uvm_map_splitentry turns the original entry into the lowest
 * entry (address wise) we do a swap between the new entry and the original
 * entry, prior to calling uvm_map_splitentry.
 */
void
uvm_map_clip_start(struct vm_map *map, struct vm_map_entry *entry, vaddr_t addr)
{
	struct vm_map_entry *tmp;
	struct uvm_addr_state *free;

	/* Unlink original. */
	free = uvm_map_uaddr_e(map, entry);
	uvm_mapent_free_remove(map, free, entry);
	uvm_mapent_addr_remove(map, entry);

	/* Copy entry. */
	KASSERT(entry->start < addr && VMMAP_FREE_END(entry) > addr);
	tmp = uvm_mapent_alloc(map, 0);
	uvm_mapent_copy(entry, tmp);

	/* Put new entry in place of original entry. */
	uvm_mapent_addr_insert(map, tmp);
	uvm_mapent_free_insert(map, free, tmp);

	/* Invoke splitentry. */
	uvm_map_splitentry(map, tmp, entry, addr);
}

/*
 * Boundary fixer.
 */
static __inline vaddr_t uvm_map_boundfix(vaddr_t, vaddr_t, vaddr_t);
static __inline vaddr_t
uvm_map_boundfix(vaddr_t min, vaddr_t max, vaddr_t bound)
{
	return (min < bound && max > bound) ? bound : max;
}

/*
 * Choose free list based on address at start of free space.
 *
 * The uvm_addr_state returned contains addr and is the first of:
 * - uaddr_exe
 * - uaddr_brk_stack
 * - uaddr_any
 */
struct uvm_addr_state*
uvm_map_uaddr(struct vm_map *map, vaddr_t addr)
{
	struct uvm_addr_state *uaddr;
	int i;

	/* Special case the first page, to prevent mmap from returning 0. */
	if (addr < VMMAP_MIN_ADDR)
		return NULL;

	/* Upper bound for kernel maps at uvm_maxkaddr. */
	if ((map->flags & VM_MAP_ISVMSPACE) == 0) {
		if (addr >= uvm_maxkaddr)
			return NULL;
	}

	/* Is the address inside the exe-only map? */
	if (map->uaddr_exe != NULL && addr >= map->uaddr_exe->uaddr_minaddr &&
	    addr < map->uaddr_exe->uaddr_maxaddr)
		return map->uaddr_exe;

	/* Check if the space falls inside brk/stack area. */
	if ((addr >= map->b_start && addr < map->b_end) ||
	    (addr >= map->s_start && addr < map->s_end)) {
		if (map->uaddr_brk_stack != NULL &&
		    addr >= map->uaddr_brk_stack->uaddr_minaddr &&
		    addr < map->uaddr_brk_stack->uaddr_maxaddr) {
			return map->uaddr_brk_stack;
		} else
			return NULL;
	}

	/*
	 * Check the other selectors.
	 *
	 * These selectors are only marked as the owner, if they have insert
	 * functions.
	 */
	for (i = 0; i < nitems(map->uaddr_any); i++) {
		uaddr = map->uaddr_any[i];
		if (uaddr == NULL)
			continue;
		if (uaddr->uaddr_functions->uaddr_free_insert == NULL)
			continue;

		if (addr >= uaddr->uaddr_minaddr &&
		    addr < uaddr->uaddr_maxaddr)
			return uaddr;
	}

	return NULL;
}

/*
 * Choose free list based on address at start of free space.
 *
 * The uvm_addr_state returned contains addr and is the first of:
 * - uaddr_exe
 * - uaddr_brk_stack
 * - uaddr_any
 */
struct uvm_addr_state*
uvm_map_uaddr_e(struct vm_map *map, struct vm_map_entry *entry)
{
	return uvm_map_uaddr(map, VMMAP_FREE_START(entry));
}

/*
 * Returns the first free-memory boundary that is crossed by [min-max].
 */
vsize_t
uvm_map_boundary(struct vm_map *map, vaddr_t min, vaddr_t max)
{
	struct uvm_addr_state	*uaddr;
	int			 i;

	/* Never return first page. */
	max = uvm_map_boundfix(min, max, VMMAP_MIN_ADDR);

	/* Treat the maxkaddr special, if the map is a kernel_map. */
	if ((map->flags & VM_MAP_ISVMSPACE) == 0)
		max = uvm_map_boundfix(min, max, uvm_maxkaddr);

	/* Check for exe-only boundaries. */
	if (map->uaddr_exe != NULL) {
		max = uvm_map_boundfix(min, max, map->uaddr_exe->uaddr_minaddr);
		max = uvm_map_boundfix(min, max, map->uaddr_exe->uaddr_maxaddr);
	}

	/* Check for exe-only boundaries. */
	if (map->uaddr_brk_stack != NULL) {
		max = uvm_map_boundfix(min, max,
		    map->uaddr_brk_stack->uaddr_minaddr);
		max = uvm_map_boundfix(min, max,
		    map->uaddr_brk_stack->uaddr_maxaddr);
	}

	/* Check other boundaries. */
	for (i = 0; i < nitems(map->uaddr_any); i++) {
		uaddr = map->uaddr_any[i];
		if (uaddr != NULL) {
			max = uvm_map_boundfix(min, max, uaddr->uaddr_minaddr);
			max = uvm_map_boundfix(min, max, uaddr->uaddr_maxaddr);
		}
	}

	/* Boundaries at stack and brk() area. */
	max = uvm_map_boundfix(min, max, map->s_start);
	max = uvm_map_boundfix(min, max, map->s_end);
	max = uvm_map_boundfix(min, max, map->b_start);
	max = uvm_map_boundfix(min, max, map->b_end);

	return max;
}

/*
 * Update map allocation start and end addresses from proc vmspace.
 */
void
uvm_map_vmspace_update(struct vm_map *map,
    struct uvm_map_deadq *dead, int flags)
{
	struct vmspace *vm;
	vaddr_t b_start, b_end, s_start, s_end;

	KASSERT(map->flags & VM_MAP_ISVMSPACE);
	KASSERT(offsetof(struct vmspace, vm_map) == 0);

	/*
	 * Derive actual allocation boundaries from vmspace.
	 */
	vm = (struct vmspace *)map;
	b_start = (vaddr_t)vm->vm_daddr;
	b_end   = b_start + BRKSIZ;
	s_start = MIN((vaddr_t)vm->vm_maxsaddr, (vaddr_t)vm->vm_minsaddr);
	s_end   = MAX((vaddr_t)vm->vm_maxsaddr, (vaddr_t)vm->vm_minsaddr);
#ifdef DIAGNOSTIC
	if ((b_start & (vaddr_t)PAGE_MASK) != 0 ||
	    (b_end & (vaddr_t)PAGE_MASK) != 0 ||
	    (s_start & (vaddr_t)PAGE_MASK) != 0 ||
	    (s_end & (vaddr_t)PAGE_MASK) != 0) {
		panic("uvm_map_vmspace_update: vmspace %p invalid bounds: "
		    "b=0x%lx-0x%lx s=0x%lx-0x%lx",
		    vm, b_start, b_end, s_start, s_end);
	}
#endif

	if (__predict_true(map->b_start == b_start && map->b_end == b_end &&
	    map->s_start == s_start && map->s_end == s_end))
		return;

	uvm_map_freelist_update(map, dead, b_start, b_end,
	    s_start, s_end, flags);
}

/*
 * Grow kernel memory.
 *
 * This function is only called for kernel maps when an allocation fails.
 *
 * If the map has a gap that is large enough to accommodate alloc_sz, this
 * function will make sure map->free will include it.
 */
void
uvm_map_kmem_grow(struct vm_map *map, struct uvm_map_deadq *dead,
    vsize_t alloc_sz, int flags)
{
	vsize_t sz;
	vaddr_t end;
	struct vm_map_entry *entry;

	/* Kernel memory only. */
	KASSERT((map->flags & VM_MAP_ISVMSPACE) == 0);
	/* Destroy free list. */
	uvm_map_freelist_update_clear(map, dead);

	/* Include the guard page in the hard minimum requirement of alloc_sz. */
	if (map->flags & VM_MAP_GUARDPAGES)
		alloc_sz += PAGE_SIZE;

	/*
	 * Grow by ALLOCMUL * alloc_sz, but at least VM_MAP_KSIZE_DELTA.
	 *
	 * Don't handle the case where the multiplication overflows:
	 * if that happens, the allocation is probably too big anyway.
	 */
	sz = MAX(VM_MAP_KSIZE_ALLOCMUL * alloc_sz, VM_MAP_KSIZE_DELTA);

	/*
	 * Walk forward until a gap large enough for alloc_sz shows up.
	 *
	 * We assume the kernel map has no boundaries.
	 * uvm_maxkaddr may be zero.
	 */
	end = MAX(uvm_maxkaddr, map->min_offset);
	entry = uvm_map_entrybyaddr(&map->addr, end);
	while (entry && entry->fspace < alloc_sz)
		entry = RBT_NEXT(uvm_map_addr, entry);
	if (entry) {
		end = MAX(VMMAP_FREE_START(entry), end);
		end += MIN(sz, map->max_offset - end);
	} else
		end = map->max_offset;

	/* Reserve pmap entries. */
#ifdef PMAP_GROWKERNEL
	uvm_maxkaddr = pmap_growkernel(end);
#else
	uvm_maxkaddr = MAX(uvm_maxkaddr, end);
#endif

	/* Rebuild free list. */
	uvm_map_freelist_update_refill(map, flags);
}

/*
 * Freelist update subfunction: unlink all entries from freelists.
 */
void
uvm_map_freelist_update_clear(struct vm_map *map, struct uvm_map_deadq *dead)
{
	struct uvm_addr_state *free;
	struct vm_map_entry *entry, *prev, *next;

	prev = NULL;
	for (entry = RBT_MIN(uvm_map_addr, &map->addr); entry != NULL;
	    entry = next) {
		next = RBT_NEXT(uvm_map_addr, entry);

		free = uvm_map_uaddr_e(map, entry);
		uvm_mapent_free_remove(map, free, entry);

		if (prev != NULL && entry->start == entry->end) {
			prev->fspace += VMMAP_FREE_END(entry) - entry->end;
			uvm_mapent_addr_remove(map, entry);
			DEAD_ENTRY_PUSH(dead, entry);
		} else
			prev = entry;
	}
}

/*
 * Freelist update subfunction: refill the freelists with entries.
 */
void
uvm_map_freelist_update_refill(struct vm_map *map, int flags)
{
	struct vm_map_entry *entry;
	vaddr_t min, max;

	RBT_FOREACH(entry, uvm_map_addr, &map->addr) {
		min = VMMAP_FREE_START(entry);
		max = VMMAP_FREE_END(entry);
		entry->fspace = 0;

		entry = uvm_map_fix_space(map, entry, min, max, flags);
	}

	uvm_tree_sanity(map, __FILE__, __LINE__);
}

/*
 * Change {a,b}_{start,end} allocation ranges and associated free lists.
 */
void
uvm_map_freelist_update(struct vm_map *map, struct uvm_map_deadq *dead,
    vaddr_t b_start, vaddr_t b_end, vaddr_t s_start, vaddr_t s_end, int flags)
{
	KDASSERT(b_end >= b_start && s_end >= s_start);

	/* Clear all free lists. */
	uvm_map_freelist_update_clear(map, dead);

	/* Apply new bounds. */
	map->b_start = b_start;
	map->b_end   = b_end;
	map->s_start = s_start;
	map->s_end   = s_end;

	/* Refill free lists. */
	uvm_map_freelist_update_refill(map, flags);
}

/*
 * Assign a uvm_addr_state to the specified pointer in vm_map.
 *
 * May sleep.
 */
void
uvm_map_set_uaddr(struct vm_map *map, struct uvm_addr_state **which,
    struct uvm_addr_state *newval)
{
	struct uvm_map_deadq dead;

	/* Pointer which must be in this map. */
	KASSERT(which != NULL);
	KASSERT((void*)map <= (void*)(which) &&
	    (void*)(which) < (void*)(map + 1));

	vm_map_lock(map);
	TAILQ_INIT(&dead);
	uvm_map_freelist_update_clear(map, &dead);

	uvm_addr_destroy(*which);
	*which = newval;

	uvm_map_freelist_update_refill(map, 0);
	vm_map_unlock(map);
	uvm_unmap_detach(&dead, 0);
}

/*
 * Correct space insert.
 *
 * Entry must not be on any freelist.
 */
struct vm_map_entry*
uvm_map_fix_space(struct vm_map *map, struct vm_map_entry *entry,
    vaddr_t min, vaddr_t max, int flags)
{
	struct uvm_addr_state	*free, *entfree;
	vaddr_t			 lmax;

	KASSERT(entry == NULL || (entry->etype & UVM_ET_FREEMAPPED) == 0);
	KDASSERT(min <= max);
	KDASSERT((entry != NULL && VMMAP_FREE_END(entry) == min) ||
	    min == map->min_offset);

	/*
	 * During the function, entfree will always point at the uaddr state
	 * for entry.
	 */
	entfree = (entry == NULL ? NULL :
	    uvm_map_uaddr_e(map, entry));

	while (min != max) {
		/* Claim guard page for entry. */
		if ((map->flags & VM_MAP_GUARDPAGES) && entry != NULL &&
		    VMMAP_FREE_END(entry) == entry->end &&
		    entry->start != entry->end) {
			if (max - min == 2 * PAGE_SIZE) {
				/*
				 * If the free-space gap is exactly 2 pages,
				 * we make the guard 2 pages instead of 1.
				 * Because in a guarded map, an area needs
				 * at least 2 pages to allocate from:
				 * one page for the allocation and one for
				 * the guard.
				 */
				entry->guard = 2 * PAGE_SIZE;
				min = max;
			} else {
				entry->guard = PAGE_SIZE;
				min += PAGE_SIZE;
			}
			continue;
		}

		/*
		 * Handle the case where entry has a 2-page guard, but the
		 * space after entry is freed.
		 */
		if (entry != NULL && entry->fspace == 0 &&
		    entry->guard > PAGE_SIZE) {
			entry->guard = PAGE_SIZE;
			min = VMMAP_FREE_START(entry);
		}

		lmax = uvm_map_boundary(map, min, max);
		free = uvm_map_uaddr(map, min);

		/*
		 * Entries are merged if they point at the same uvm_free().
		 * Exception to that rule: if min == uvm_maxkaddr, a new
		 * entry is started regardless (otherwise the allocators
		 * will get confused).
		 */
		if (entry != NULL && free == entfree &&
		    !((map->flags & VM_MAP_ISVMSPACE) == 0 &&
		    min == uvm_maxkaddr)) {
			KDASSERT(VMMAP_FREE_END(entry) == min);
			entry->fspace += lmax - min;
		} else {
			/*
			 * Commit entry to free list: it'll not be added to
			 * anymore.
			 * We'll start a new entry and add to that entry
			 * instead.
			 */
			if (entry != NULL)
				uvm_mapent_free_insert(map, entfree, entry);

			/* New entry for new uaddr. */
			entry = uvm_mapent_alloc(map, flags);
			KDASSERT(entry != NULL);
			entry->end = entry->start = min;
			entry->guard = 0;
			entry->fspace = lmax - min;
			entry->object.uvm_obj = NULL;
			entry->offset = 0;
			entry->etype = 0;
			entry->protection = entry->max_protection = 0;
			entry->inheritance = 0;
			entry->wired_count = 0;
			entry->advice = 0;
			entry->aref.ar_pageoff = 0;
			entry->aref.ar_amap = NULL;
			uvm_mapent_addr_insert(map, entry);

			entfree = free;
		}

		min = lmax;
	}
	/* Finally put entry on the uaddr state. */
	if (entry != NULL)
		uvm_mapent_free_insert(map, entfree, entry);

	return entry;
}

/*
 * MQuery style of allocation.
 *
 * This allocator searches forward until sufficient space is found to map
 * the given size.
 *
 * XXX: factor in offset (via pmap_prefer) and protection?
 */
int
uvm_map_mquery(struct vm_map *map, vaddr_t *addr_p, vsize_t sz, voff_t offset,
    int flags)
{
	struct vm_map_entry *entry, *last;
	vaddr_t addr;
	vaddr_t tmp, pmap_align, pmap_offset;
	int error;

	addr = *addr_p;
	vm_map_lock_read(map);

	/* Configure pmap prefer. */
	if (offset != UVM_UNKNOWN_OFFSET) {
		pmap_align = MAX(PAGE_SIZE, PMAP_PREFER_ALIGN());
		pmap_offset = PMAP_PREFER_OFFSET(offset);
	} else {
		pmap_align = PAGE_SIZE;
		pmap_offset = 0;
	}

	/* Align address to pmap_prefer unless FLAG_FIXED is set. */
	if (!(flags & UVM_FLAG_FIXED) && offset != UVM_UNKNOWN_OFFSET) {
	  	tmp = (addr & ~(pmap_align - 1)) | pmap_offset;
		if (tmp < addr)
			tmp += pmap_align;
		addr = tmp;
	}

	/* First, check if the requested range is fully available. */
	entry = uvm_map_entrybyaddr(&map->addr, addr);
	last = NULL;
	if (uvm_map_isavail(map, NULL, &entry, &last, addr, sz)) {
		error = 0;
		goto out;
	}
	if (flags & UVM_FLAG_FIXED) {
		error = EINVAL;
		goto out;
	}

	error = ENOMEM; /* Default error from here. */

	/*
	 * At this point, the memory at <addr, sz> is not available.
	 * The reasons are:
	 * [1] it's outside the map,
	 * [2] it starts in used memory (and therefore needs to move
	 *     toward the first free page in entry),
	 * [3] it starts in free memory but bumps into used memory.
	 *
	 * Note that for case [2], the forward moving is handled by the
	 * for loop below.
	 */
	if (entry == NULL) {
		/* [1] Outside the map. */
		if (addr >= map->max_offset)
			goto out;
		else
			entry = RBT_MIN(uvm_map_addr, &map->addr);
	} else if (VMMAP_FREE_START(entry) <= addr) {
		/* [3] Bumped into used memory. */
		entry = RBT_NEXT(uvm_map_addr, entry);
	}

	/* Test if the next entry is sufficient for the allocation. */
	for (; entry != NULL;
	    entry = RBT_NEXT(uvm_map_addr, entry)) {
		if (entry->fspace == 0)
			continue;
		addr = VMMAP_FREE_START(entry);

restart:	/* Restart address checks on address change. */
		tmp = (addr & ~(pmap_align - 1)) | pmap_offset;
		if (tmp < addr)
			tmp += pmap_align;
		addr = tmp;
		if (addr >= VMMAP_FREE_END(entry))
			continue;

		/* Skip brk() allocation addresses. */
		if (addr + sz > map->b_start && addr < map->b_end) {
			if (VMMAP_FREE_END(entry) > map->b_end) {
				addr = map->b_end;
				goto restart;
			} else
				continue;
		}
		/* Skip stack allocation addresses. */
		if (addr + sz > map->s_start && addr < map->s_end) {
			if (VMMAP_FREE_END(entry) > map->s_end) {
				addr = map->s_end;
				goto restart;
			} else
				continue;
		}

		last = NULL;
		if (uvm_map_isavail(map, NULL, &entry, &last, addr, sz)) {
			error = 0;
			goto out;
		}
	}

out:
	vm_map_unlock_read(map);
	if (error == 0)
		*addr_p = addr;
	return error;
}

/*
 * Determine allocation bias.
 *
 * Returns 1 if we should bias to high addresses, -1 for a bias towards low
 * addresses, or 0 for no bias.
 * The bias mechanism is intended to avoid clashing with brk() and stack
 * areas.
 */
int
uvm_mapent_bias(struct vm_map *map, struct vm_map_entry *entry)
{
	vaddr_t start, end;

	start = VMMAP_FREE_START(entry);
	end = VMMAP_FREE_END(entry);

	/* Stay at the top of brk() area. */
	if (end >= map->b_start && start < map->b_end)
		return 1;
	/* Stay at the far end of the stack area. */
	if (end >= map->s_start && start < map->s_end) {
#ifdef MACHINE_STACK_GROWS_UP
		return 1;
#else
		return -1;
#endif
	}

	/* No bias, this area is meant for us. */
	return 0;
}


boolean_t
vm_map_lock_try_ln(struct vm_map *map, char *file, int line)
{
	boolean_t rv;

	if (map->flags & VM_MAP_INTRSAFE) {
		rv = mtx_enter_try(&map->mtx);
	} else {
		mtx_enter(&map->flags_lock);
		if (map->flags & VM_MAP_BUSY) {
			mtx_leave(&map->flags_lock);
			return (FALSE);
		}
		mtx_leave(&map->flags_lock);
		rv = (rw_enter(&map->lock, RW_WRITE|RW_NOSLEEP) == 0);
		/* check if the lock is busy and back out if we won the race */
		if (rv) {
			mtx_enter(&map->flags_lock);
			if (map->flags & VM_MAP_BUSY) {
				rw_exit(&map->lock);
				rv = FALSE;
			}
			mtx_leave(&map->flags_lock);
		}
	}

	if (rv) {
		map->timestamp++;
		LPRINTF(("map   lock: %p (at %s %d)\n", map, file, line));
		uvm_tree_sanity(map, file, line);
		uvm_tree_size_chk(map, file, line);
	}

	return (rv);
}

void
vm_map_lock_ln(struct vm_map *map, char *file, int line)
{
	if ((map->flags & VM_MAP_INTRSAFE) == 0) {
		do {
			mtx_enter(&map->flags_lock);
tryagain:
			while (map->flags & VM_MAP_BUSY) {
				map->flags |= VM_MAP_WANTLOCK;
				msleep(&map->flags, &map->flags_lock,
				    PVM, vmmapbsy, 0);
			}
			mtx_leave(&map->flags_lock);
		} while (rw_enter(&map->lock, RW_WRITE|RW_SLEEPFAIL) != 0);
		/* check if the lock is busy and back out if we won the race */
		mtx_enter(&map->flags_lock);
		if (map->flags & VM_MAP_BUSY) {
			rw_exit(&map->lock);
			goto tryagain;
		}
		mtx_leave(&map->flags_lock);
	} else {
		mtx_enter(&map->mtx);
	}

	map->timestamp++;
	LPRINTF(("map   lock: %p (at %s %d)\n", map, file, line));
	uvm_tree_sanity(map, file, line);
	uvm_tree_size_chk(map, file, line);
}

void
vm_map_lock_read_ln(struct vm_map *map, char *file, int line)
{
	if ((map->flags & VM_MAP_INTRSAFE) == 0)
		rw_enter_read(&map->lock);
	else
		mtx_enter(&map->mtx);
	LPRINTF(("map   lock: %p (at %s %d)\n", map, file, line));
	uvm_tree_sanity(map, file, line);
	uvm_tree_size_chk(map, file, line);
}

void
vm_map_unlock_ln(struct vm_map *map, char *file, int line)
{
	uvm_tree_sanity(map, file, line);
	uvm_tree_size_chk(map, file, line);
	LPRINTF(("map unlock: %p (at %s %d)\n", map, file, line));
	if ((map->flags & VM_MAP_INTRSAFE) == 0)
		rw_exit(&map->lock);
	else
		mtx_leave(&map->mtx);
}

void
vm_map_unlock_read_ln(struct vm_map *map, char *file, int line)
{
	/* XXX: RO */ uvm_tree_sanity(map, file, line);
	/* XXX: RO */ uvm_tree_size_chk(map, file, line);
	LPRINTF(("map unlock: %p (at %s %d)\n", map, file, line));
	if ((map->flags & VM_MAP_INTRSAFE) == 0)
		rw_exit_read(&map->lock);
	else
		mtx_leave(&map->mtx);
}

void
vm_map_downgrade_ln(struct vm_map *map, char *file, int line)
{
	uvm_tree_sanity(map, file, line);
	uvm_tree_size_chk(map, file, line);
	LPRINTF(("map unlock: %p (at %s %d)\n", map, file, line));
	LPRINTF(("map   lock: %p (at %s %d)\n", map, file, line));
	KASSERT((map->flags & VM_MAP_INTRSAFE) == 0);
	if ((map->flags & VM_MAP_INTRSAFE) == 0)
		rw_enter(&map->lock, RW_DOWNGRADE);
}

void
vm_map_upgrade_ln(struct vm_map *map, char *file, int line)
{
	/* XXX: RO */ uvm_tree_sanity(map, file, line);
	/* XXX: RO */ uvm_tree_size_chk(map, file, line);
	LPRINTF(("map unlock: %p (at %s %d)\n", map, file, line));
	KASSERT((map->flags & VM_MAP_INTRSAFE) == 0);
	if ((map->flags & VM_MAP_INTRSAFE) == 0) {
		rw_exit_read(&map->lock);
		rw_enter_write(&map->lock);
	}
	LPRINTF(("map   lock: %p (at %s %d)\n", map, file, line));
	uvm_tree_sanity(map, file, line);
}

void
vm_map_busy_ln(struct vm_map *map, char *file, int line)
{
	KASSERT((map->flags & VM_MAP_INTRSAFE) == 0);
	mtx_enter(&map->flags_lock);
	map->flags |= VM_MAP_BUSY;
	mtx_leave(&map->flags_lock);
}

void
vm_map_unbusy_ln(struct vm_map *map, char *file, int line)
{
	int oflags;

	KASSERT((map->flags & VM_MAP_INTRSAFE) == 0);
	mtx_enter(&map->flags_lock);
	oflags = map->flags;
	map->flags &= ~(VM_MAP_BUSY|VM_MAP_WANTLOCK);
	mtx_leave(&map->flags_lock);
	if (oflags & VM_MAP_WANTLOCK)
		wakeup(&map->flags);
}

#ifndef SMALL_KERNEL
int
uvm_map_fill_vmmap(struct vm_map *map, struct kinfo_vmentry *kve,
    size_t *lenp)
{
	struct vm_map_entry *entry;
	vaddr_t start;
	int cnt, maxcnt, error = 0;

	KASSERT(*lenp > 0);
	KASSERT((*lenp % sizeof(*kve)) == 0);
	cnt = 0;
	maxcnt = *lenp / sizeof(*kve);
	KASSERT(maxcnt > 0);

	/*
	 * Return only entries whose address is above the given base
	 * address.  This allows userland to iterate without knowing the
	 * number of entries beforehand.
	 */
	start = (vaddr_t)kve[0].kve_start;

	vm_map_lock(map);
	RBT_FOREACH(entry, uvm_map_addr, &map->addr) {
		if (cnt == maxcnt) {
			error = ENOMEM;
			break;
		}
		if (start != 0 && entry->start < start)
			continue;
		kve->kve_start = entry->start;
		kve->kve_end = entry->end;
		kve->kve_guard = entry->guard;
		kve->kve_fspace = entry->fspace;
		kve->kve_fspace_augment = entry->fspace_augment;
		kve->kve_offset = entry->offset;
		kve->kve_wired_count = entry->wired_count;
		kve->kve_etype = entry->etype;
		kve->kve_protection = entry->protection;
		kve->kve_max_protection = entry->max_protection;
		kve->kve_advice = entry->advice;
		kve->kve_inheritance = entry->inheritance;
		kve->kve_flags = entry->flags;
		kve++;
		cnt++;
	}
	vm_map_unlock(map);

	KASSERT(cnt <= maxcnt);

	*lenp = sizeof(*kve) * cnt;
	return error;
}
#endif


RBT_GENERATE_AUGMENT(uvm_map_addr, vm_map_entry, daddrs.addr_entry,
    uvm_mapentry_addrcmp, uvm_map_addr_augment);


/*
 * MD code: vmspace allocator setup.
 */

#ifdef __i386__
void
uvm_map_setup_md(struct vm_map *map)
{
	vaddr_t		min, max;

	min = map->min_offset;
	max = map->max_offset;

	/*
	 * Ensure the selectors will not try to manage page 0;
	 * it's too special.
	 */
	if (min < VMMAP_MIN_ADDR)
		min = VMMAP_MIN_ADDR;

#if 0	/* Cool stuff, not yet */
	/* Executable code is special. */
	map->uaddr_exe = uaddr_rnd_create(min, I386_MAX_EXE_ADDR);
	/* Place normal allocations beyond executable mappings. */
	map->uaddr_any[3] = uaddr_pivot_create(2 * I386_MAX_EXE_ADDR, max);
#else	/* Crappy stuff, for now */
	map->uaddr_any[0] = uaddr_rnd_create(min, max);
#endif

#ifndef SMALL_KERNEL
	map->uaddr_brk_stack = uaddr_stack_brk_create(min, max);
#endif /* !SMALL_KERNEL */
}
#elif __LP64__
void
uvm_map_setup_md(struct vm_map *map)
{
	vaddr_t		min, max;

	min = map->min_offset;
	max = map->max_offset;

	/*
	 * Ensure the selectors will not try to manage page 0;
	 * it's too special.
	 */
	if (min < VMMAP_MIN_ADDR)
		min = VMMAP_MIN_ADDR;

#if 0	/* Cool stuff, not yet */
	map->uaddr_any[3] = uaddr_pivot_create(MAX(min, 0x100000000ULL), max);
#else	/* Crappy stuff, for now */
	map->uaddr_any[0] = uaddr_rnd_create(min, max);
#endif

#ifndef SMALL_KERNEL
	map->uaddr_brk_stack = uaddr_stack_brk_create(min, max);
#endif /* !SMALL_KERNEL */
}
#else	/* non-i386, 32 bit */
void
uvm_map_setup_md(struct vm_map *map)
{
	vaddr_t		min, max;

	min = map->min_offset;
	max = map->max_offset;

	/*
	 * Ensure the selectors will not try to manage page 0;
	 * it's too special.
	 */
	if (min < VMMAP_MIN_ADDR)
		min = VMMAP_MIN_ADDR;

#if 0	/* Cool stuff, not yet */
	map->uaddr_any[3] = uaddr_pivot_create(min, max);
#else	/* Crappy stuff, for now */
	map->uaddr_any[0] = uaddr_rnd_create(min, max);
#endif

#ifndef SMALL_KERNEL
	map->uaddr_brk_stack = uaddr_stack_brk_create(min, max);
#endif /* !SMALL_KERNEL */
}
#endif
@


1.229
log
@Convert most of the manual checks for CPU hogging to sched_pause().

The distinction between preempt() and yield() stays as it is usueful
to know if a thread decided to yield by itself or if the kernel told
him to go away.

ok tedu@@, guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.228 2017/02/05 01:08:31 guenther Exp $	*/
d2410 1
a2410 1
	rw_init(&map->lock, "vmmaplk");
@


1.228
log
@Delete comment obsoleted by the rewrite in rev 1.136 (2011-05-24)
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.227 2017/01/17 17:19:21 stefan Exp $	*/
d2483 1
a2483 1
		sched_pause();
@


1.227
log
@Remove uaddr_hint allocator

The hint allocator would have to check that the
allocation does not overlap with brk, stack or text
areas. This would make the address selectors too
entagled. Just use the rnd allocator for hinted allocations
in case pivots are used. This also reduces the amount of code somewhat.

ok kettenis visa deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.226 2016/11/07 00:26:33 guenther Exp $	*/
a4165 4
	 * This may invalidate last, hence the re-initialization during the
	 * loop.
	 *
	 * Also, perform clipping of last if not UVM_EXTRACT_QREF.
@


1.226
log
@Split PID from TID, giving processes a PID unrelated to the TID of their
initial thread

ok jsing@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.225 2016/09/16 02:35:42 dlg Exp $	*/
a5293 3
	/* Hinted allocations. */
	map->uaddr_any[1] = uaddr_hint_create(min, max, 1024 * 1024 * 1024);

a5322 7
	/* Hinted allocations above 4GB */
	map->uaddr_any[0] =
	    uaddr_hint_create(0x100000000ULL, max, 1024 * 1024 * 1024);
	/* Hinted allocations below 4GB */
	map->uaddr_any[1] = uaddr_hint_create(min, 0x100000000ULL,
	    1024 * 1024 * 1024);
	/* Normal allocations, always above 4GB */
a5348 3
	/* Hinted allocations. */
	map->uaddr_any[1] = uaddr_hint_create(min, max, 1024 * 1024 * 1024);
	/* Normal allocations. */
@


1.225
log
@move the vm_page struct from being stored in RB macro trees to RBT functions

vm_page structs go into three trees, uvm_objtree, uvm_pmr_addr, and
uvm_pmr_size. all these have been moved to RBT code.

this should give us a decent chunk of code space back.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.224 2016/09/16 01:09:53 dlg Exp $	*/
d2954 1
a2954 1
		(*pr)("  owning process = %d, tag=%s",
@


1.224
log
@move the uvm_map_addr RB tree from RB macros to the RBT functions

this tree is interesting because it uses all the red black tree
features, specifically the augment callback thats called on tree
topology changes, and it poisons and checks entries as theyre removed
from and inserted back into the tree respectively.

ok stefan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.223 2016/09/15 02:00:18 dlg Exp $	*/
d2916 1
a2916 1
	RB_FOREACH(pg, uvm_objtree, &uobj->memt) {
d2978 1
a2978 1
				RB_FOREACH(tpg, uvm_objtree, &uobj->memt) {
@


1.223
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.222 2016/09/03 18:43:34 stefan Exp $	*/
d163 2
a164 2
static int		 uvm_mapentry_addrcmp(struct vm_map_entry*,
			    struct vm_map_entry*);
d274 1
a274 1
#define UVMMAP_DEADBEEF		((void*)DEADBEEF0)
d276 1
a276 1
#define UVMMAP_DEADBEEF		((void*)0xdeadd0d0)
d338 3
a340 2
static __inline int
uvm_mapentry_addrcmp(struct vm_map_entry *e1, struct vm_map_entry *e2)
d437 1
a437 3
	if (RB_LEFT(entry, daddrs.addr_entry) != UVMMAP_DEADBEEF ||
	    RB_RIGHT(entry, daddrs.addr_entry) != UVMMAP_DEADBEEF ||
	    RB_PARENT(entry, daddrs.addr_entry) != UVMMAP_DEADBEEF)
d444 1
a444 1
	res = RB_INSERT(uvm_map_addr, &map->addr, entry);
d464 1
a464 1
	res = RB_REMOVE(uvm_map_addr, &map->addr, entry);
d467 1
a467 2
	RB_LEFT(entry, daddrs.addr_entry) = RB_RIGHT(entry, daddrs.addr_entry) =
	    RB_PARENT(entry, daddrs.addr_entry) = UVMMAP_DEADBEEF;
d522 1
a522 1
	iter = RB_ROOT(atree);
d525 1
a525 1
			iter = RB_LEFT(iter, daddrs.addr_entry);
d527 1
a527 1
			iter = RB_RIGHT(iter, daddrs.addr_entry);
d821 1
a821 1
	i_end = RB_NEXT(uvm_map_addr, atree, *end_ptr);
d823 1
a823 1
	    i = RB_NEXT(uvm_map_addr, atree, i)) {
d887 1
a887 1
	if ((left = RB_LEFT(entry, daddrs.addr_entry)) != NULL)
d889 1
a889 1
	if ((right = RB_RIGHT(entry, daddrs.addr_entry)) != NULL)
d915 1
a915 1
		entry = RB_PARENT(entry, daddrs.addr_entry);
d1488 1
a1488 1
	other = RB_PREV(uvm_map_addr, &map->addr, entry);
d1502 1
a1502 1
	other = RB_NEXT(uvm_map_addr, &map->addr, entry);
d1626 1
a1626 1
		prev = RB_PREV(uvm_map_addr, &map->addr, last);
d1703 1
a1703 3
		RB_LEFT(me, daddrs.addr_entry) =
		    RB_RIGHT(me, daddrs.addr_entry) =
		    RB_PARENT(me, daddrs.addr_entry) = UVMMAP_DEADBEEF;
d1829 1
a1829 1
		prev = RB_PREV(uvm_map_addr, &map->addr, entry);
d1956 1
a1956 1
		entry = RB_NEXT(uvm_map_addr, &map->addr, entry);
d1970 1
a1970 1
		next = RB_NEXT(uvm_map_addr, &map->addr, entry);
d2003 1
a2003 1
		    entry = RB_NEXT(uvm_map_addr, &map->addr, entry)) {
d2028 1
a2028 1
	    iter = RB_NEXT(uvm_map_addr, &map->addr, iter)) {
d2073 1
a2073 1
	    iter = RB_NEXT(uvm_map_addr, &map->addr, iter)) {
d2106 1
a2106 1
	    iter = RB_NEXT(uvm_map_addr, &map->addr, iter)) {
d2133 1
a2133 1
		    first = RB_NEXT(uvm_map_addr, &map->addr, first)) {
d2148 1
a2148 1
		    iter = RB_NEXT(uvm_map_addr, &map->addr, iter)) {
d2226 1
a2226 1
	    last = RB_NEXT(uvm_map_addr, &map->addr, last)) {
d2245 1
a2245 1
		last = RB_MAX(uvm_map_addr, &map->addr);
d2252 1
a2252 1
		last = RB_PREV(uvm_map_addr, &map->addr, last);
d2270 1
a2270 1
			tmp = RB_NEXT(uvm_map_addr, &map->addr, last);
d2295 1
a2295 1
			tmp = RB_NEXT(uvm_map_addr, &map->addr, last);
d2321 1
a2321 1
		uvm_map_pageable_pgon(map, RB_MIN(uvm_map_addr, &map->addr),
d2341 1
a2341 1
	RB_FOREACH(iter, uvm_map_addr, &map->addr) {
d2365 1
a2365 1
	return uvm_map_pageable_wire(map, RB_MIN(uvm_map_addr, &map->addr),
d2396 1
a2396 1
	RB_INIT(&map->addr);
d2480 1
a2480 1
	if ((entry = RB_ROOT(&map->addr)) != NULL)
d2485 1
a2485 1
		if ((tmp = RB_LEFT(entry, daddrs.addr_entry)) != NULL)
d2487 1
a2487 1
		if ((tmp = RB_RIGHT(entry, daddrs.addr_entry)) != NULL)
d2495 1
a2495 1
	RB_FOREACH(entry, uvm_map_addr, &map->addr)
d2517 1
a2517 1
	KDASSERT(RB_EMPTY(&map->addr));
d2545 2
a2546 2
	KDASSERT(RB_FIND(uvm_map_addr, &map->addr, orig) == orig);
	KDASSERT(RB_FIND(uvm_map_addr, &map->addr, next) != next);
d2647 1
a2647 1
	RB_FOREACH(iter, uvm_map_addr, &map->addr) {
d2702 1
a2702 1
	RB_FOREACH(iter, uvm_map_addr, &map->addr) {
d2734 1
a2734 1
	RB_FOREACH(iter, uvm_map_addr, &map->addr) {
d2853 1
a2853 1
	RB_FOREACH(entry, uvm_map_addr, &map->addr) {
d3055 1
a3055 1
		first = RB_NEXT(uvm_map_addr, &map->addr, first);
d3059 1
a3059 1
	    iter = RB_NEXT(uvm_map_addr, &map->addr, iter)) {
d3079 1
a3079 1
	    iter = RB_NEXT(uvm_map_addr, &map->addr, iter)) {
d3290 1
a3290 1
		KDASSERT(RB_EMPTY(&map->addr));
d3402 1
a3402 1
	    src_entry = RB_NEXT(uvm_map_addr, &srcmap->addr, src_entry)) {
d3768 1
a3768 1
	RB_FOREACH(old_entry, uvm_map_addr, &old_map->addr) {
d3947 1
a3947 1
	    entry = RB_NEXT(uvm_map_addr, &map->addr, entry)) {
d4001 1
a4001 1
	KASSERT(RB_EMPTY(&map->addr));
d4043 1
a4043 1
		entry = RB_NEXT(uvm_map_addr, &map->addr, entry);
d4048 1
a4048 1
		entry = RB_NEXT(uvm_map_addr, &map->addr, entry);
d4087 1
a4087 1
		entry = RB_NEXT(uvm_map_addr, &map->addr, entry);
d4095 1
a4095 1
		entry = RB_NEXT(uvm_map_addr, &map->addr, entry);
d4152 1
a4152 1
	    entry = RB_NEXT(uvm_map_addr, &map->addr, entry)) {
d4172 1
a4172 1
	    entry = RB_NEXT(uvm_map_addr, &map->addr, entry)) {
d4201 1
a4201 1
	    entry = RB_NEXT(uvm_map_addr, &map->addr, entry)) {
d4297 1
a4297 1
	    entry = RB_NEXT(uvm_map_addr, &map->addr, entry)) {
d4313 1
a4313 1
	    entry = RB_NEXT(uvm_map_addr, &map->addr, entry)) {
d4684 1
a4684 1
		entry = RB_NEXT(uvm_map_addr, &map->addr, entry);
d4712 1
a4712 1
	for (entry = RB_MIN(uvm_map_addr, &map->addr); entry != NULL;
d4714 1
a4714 1
		next = RB_NEXT(uvm_map_addr, &map->addr, entry);
d4737 1
a4737 1
	RB_FOREACH(entry, uvm_map_addr, &map->addr) {
d4975 1
a4975 1
			entry = RB_MIN(uvm_map_addr, &map->addr);
d4978 1
a4978 1
		entry = RB_NEXT(uvm_map_addr, &map->addr, entry);
d4983 1
a4983 1
	    entry = RB_NEXT(uvm_map_addr, &map->addr, entry)) {
d5236 1
a5236 1
	RB_FOREACH(entry, uvm_map_addr, &map->addr) {
d5269 2
a5270 5
#undef RB_AUGMENT
#define RB_AUGMENT(x)	uvm_map_addr_augment((x))
RB_GENERATE(uvm_map_addr, vm_map_entry, daddrs.addr_entry,
    uvm_mapentry_addrcmp);
#undef RB_AUGMENT
@


1.222
log
@Only use uaddr_exe for address selection when PROT_EXEC is requested

Checking whether a memory range could be mprotect()'ed to PROT_EXEC
attempts to put every mapping into the uaddr_exe range, if it exists.
This would fill up the exe range on i386 quickly, once uaddr_exe gets
used. So only use uaddr_exe if we know PROT_EXEC is needed for sure
No change in current behavior, since uaddr_exe will only be used
with uvm pivots.

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.221 2016/08/31 13:13:58 stefan Exp $	*/
d2800 6
a2805 9
	pool_init(&uvm_vmspace_pool, sizeof(struct vmspace),
	    0, 0, PR_WAITOK, "vmsppl", NULL);
	pool_setipl(&uvm_vmspace_pool, IPL_NONE);
	pool_init(&uvm_map_entry_pool, sizeof(struct vm_map_entry),
	    0, 0, PR_WAITOK, "vmmpepl", NULL);
	pool_setipl(&uvm_map_entry_pool, IPL_VM);
	pool_init(&uvm_map_entry_kmem_pool, sizeof(struct vm_map_entry),
	    0, 0, 0, "vmmpekpl", NULL);
	pool_setipl(&uvm_map_entry_kmem_pool, IPL_NONE);
@


1.221
log
@Simplify arguments to uaddr_*_create functions

min is already clamped before invoking these functions.
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.220 2016/08/11 01:17:33 dlg Exp $	*/
d1020 1
a1020 2
	} else if ((maxprot & PROT_EXEC) != 0 &&
	    map->uaddr_exe != NULL) {
d1250 1
a1250 2
	} else if ((maxprot & PROT_EXEC) != 0 &&
	    map->uaddr_exe != NULL) {
@


1.220
log
@replace abuse of the static map entries RB_ENTRY pointers with an SLIST

free static entries are kept in a simple linked list, so use SLIST
to make this obvious. the RB_PARENT manipulations are ugly and
confusing.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.219 2016/07/30 16:43:44 kettenis Exp $	*/
d5307 1
a5307 2
	map->uaddr_any[1] = uaddr_hint_create(MAX(min, VMMAP_MIN_ADDR), max,
	    1024 * 1024 * 1024);
d5342 1
a5342 2
	map->uaddr_any[1] =
	    uaddr_hint_create(MAX(min, VMMAP_MIN_ADDR), 0x100000000ULL,
d5345 1
a5345 2
	map->uaddr_any[3] =
	    uaddr_pivot_create(MAX(min, 0x100000000ULL), max);
d5372 1
a5372 2
	map->uaddr_any[1] = uaddr_hint_create(MAX(min, VMMAP_MIN_ADDR), max,
	    1024 * 1024 * 1024);
@


1.219
log
@Check for wraparound before the "commit" phase of uvm_map() and uvm_mapanon(),
to prevent hitting assertions and/or corrupting data structures during that
phase.

ok deraadt@@, tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.218 2016/07/29 20:44:40 tedu Exp $	*/
d1672 1
a1672 2
		me = uvm.kentry_free;
		if (me == NULL) {
d1678 4
a1681 6
			for (i = 0;
			    i < PAGE_SIZE / sizeof(struct vm_map_entry) - 1;
			    i++)
				RB_LEFT(&ne[i], daddrs.addr_entry) = &ne[i + 1];
			RB_LEFT(&ne[i], daddrs.addr_entry) = NULL;
			me = ne;
d1687 2
a1688 1
		uvm.kentry_free = RB_LEFT(me, daddrs.addr_entry);
d1726 1
a1726 2
		RB_LEFT(me, daddrs.addr_entry) = uvm.kentry_free;
		uvm.kentry_free = me;
d2795 1
a2795 1
	uvm.kentry_free = NULL;
d2797 2
a2798 3
		RB_LEFT(&kernel_map_entry[lcv], daddrs.addr_entry) =
		    uvm.kentry_free;
		uvm.kentry_free = &kernel_map_entry[lcv];
@


1.218
log
@add a check that the arguments to isavail don't overflow.
callers should probably check too, but checking here won't hurt.
possible panic reported by tim newsham.
ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.217 2016/06/17 10:48:25 dlg Exp $	*/
d1039 6
d1286 6
@


1.217
log
@pool_setipl on all uvm pools.

ok kettenis@@ visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.216 2016/06/13 17:14:09 kettenis Exp $	*/
d773 3
@


1.217.2.1
log
@backport overflow checks from 1.218 and 1.219:
add a check that the arguments to isavail don't overflow.
callers should probably check too, but checking here won't hurt.
possible panic reported by tim newsham.
Check for wraparound before the "commit" phase of uvm_map() and uvm_mapanon(),
to prevent hitting assertions and/or corrupting data structures during that
phase.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.217 2016/06/17 10:48:25 dlg Exp $	*/
a773 3
	if (addr + sz < addr)
		return 0;

a1035 6
	/* Check if selected address doesn't cause overflow. */
	if (*addr + sz < *addr) {
		error = ENOMEM;
		goto unlock;
	}

a1276 6
	}

	/* Check if selected address doesn't cause overflow. */
	if (*addr + sz < *addr) {
		error = ENOMEM;
		goto unlock;
@


1.216
log
@In uvm_map(), call uvm_unmap_detach_intrsafe() if we have an interrupt-safe
map, to avoid grabbing the kernel lock when pool_get() needs to allocate
a new pool page.  Hopefully this really is the last case where we might grab
the kernel lock for interrupt-safe pools.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.215 2016/06/05 08:35:57 stefan Exp $	*/
d2793 1
d2799 1
@


1.215
log
@Add uvm_share() to share a memory range between two address spaces

Its primary use is to make guest VM memory accessible to the host
(e.g. vmd(8)). That will later allow us to remove the readpage and
writepage ioctls from vmm(4), and use ordinary loads and stores instead.

"looks good to me" kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.214 2016/06/03 06:47:51 kettenis Exp $	*/
d1360 4
a1363 1
	uvm_unmap_detach(&dead, 0);
@


1.214
log
@We should never decrease uvm_maxkaddr.  Currently this may happen if
uvm_map_kmem_grow() gets called for submaps of the kernel_map on
architectures that don't implement pmap_growkernel().  When that happens
we get the infamous "address selector returned unavailable address" panic.

ok tedu@@, mglocker@@, beck@@, stefan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.213 2016/05/08 16:29:57 stefan Exp $	*/
d183 6
a188 2
			    vsize_t, struct vm_map_entry*,
			    struct uvm_map_deadq*, int, int);
d3351 92
d3451 2
a3452 1
    vsize_t off, struct vm_map_entry *old_entry, struct uvm_map_deadq *dead,
d3474 2
a3475 2
	new_entry->protection = old_entry->protection;
	new_entry->max_protection = old_entry->max_protection;
a3496 4
/*
 * share the mapping: this means we want the old and
 * new entries to share amaps and backing objects.
 */
d3498 2
a3499 2
uvm_mapent_forkshared(struct vmspace *new_vm, struct vm_map *new_map,
    struct vm_map *old_map,
a3501 2
	struct vm_map_entry *new_entry;

d3503 9
a3511 4
	 * if the old_entry needs a new amap (due to prev fork)
	 * then we need to allocate it now so that we have
	 * something we own to share with the new_entry.   [in
	 * other words, we need to clear needs_copy]
d3517 1
a3517 1
		    0, 0); 
d3521 18
a3538 3
	new_entry = uvm_mapent_clone(new_map, old_entry->start,
	    old_entry->end - old_entry->start, 0, old_entry,
	    dead, 0, AMAP_SHARED);
d3568 2
a3569 2
	    old_entry->end - old_entry->start, 0, old_entry,
	    dead, 0, 0);
d3709 2
a3710 2
	    old_entry->end - old_entry->start, 0, old_entry,
	    dead, 0, 0);
d4210 1
@


1.213
log
@Additional parameter for amap_alloc().

It is supposed to control whether an amap should allocate memory
to store anon pointers lazily or upfront. Needed for upcoming amap
changes.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.212 2016/05/05 11:23:39 stefan Exp $	*/
d4576 1
a4576 1
	uvm_maxkaddr = end;
@


1.212
log
@Remove uvm_mapentry_freecmp which has been unused for years

Found by David Hill with clang.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.211 2016/04/04 16:34:16 stefan Exp $	*/
d1067 1
a1067 1
		entry->aref.ar_amap = amap_alloc(sz, M_WAITOK);
d1323 1
a1323 1
		entry->aref.ar_amap = amap_alloc(sz, M_WAITOK);
@


1.211
log
@UVM_FLAG_AMAPPAD has no effect anymore, nuke it.

This flag caused amaps to be allocated with additional spare slots, to
make extending them cheaper. However, the kernel never extends amaps,
so allocating spare slots is pointless. Also UVM_FLAG_AMAPPAD only
has an effect in combination with UVM_FLAG_OVERLAY. The only function
that used both flags was sys_obreak, but that function had the use of
UVM_FLAG_OVERLAY removed recently.

While there, kill the unused prototypes amap_flags and amap_refs.
They're defined as macros already.

ok mlarkin@@ kettenis@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.210 2016/03/27 09:51:37 stefan Exp $	*/
a164 2
static int		 uvm_mapentry_freecmp(struct vm_map_entry*,
			    struct vm_map_entry*);
a337 18
}

/*
 * Tree describing free memory.
 *
 * Free memory is indexed (so we can use array semantics in O(log N).
 * Free memory is ordered by size (so we can reduce fragmentation).
 *
 * The address range in the tree can be limited, having part of the
 * free memory not in the free-memory tree. Only free memory in the
 * tree will be considered during 'any address' allocations.
 */

static __inline int
uvm_mapentry_freecmp(struct vm_map_entry *e1, struct vm_map_entry *e2)
{
	int cmp = e1->fspace < e2->fspace ? -1 : e1->fspace > e2->fspace;
	return cmp ? cmp : uvm_mapentry_addrcmp(e1, e2);
@


1.210
log
@amap_extend is never called, remove it.

In the code, this function is called when vm_map_entries are merged.
However, only kernel map entries are merged, and these do not use amaps.
Therefore amap_extend() is never called at runtime.

ok millert@@, KASSERT suggestion and ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.209 2016/03/15 20:50:23 krw Exp $	*/
d1087 1
a1087 3
		entry->aref.ar_amap = amap_alloc(sz,
		    ptoa(flags & UVM_FLAG_AMAPPAD ? UVM_AMAP_CHUNK : 0),
		    M_WAITOK);
d1343 1
a1343 3
		entry->aref.ar_amap = amap_alloc(sz,
		    ptoa(flags & UVM_FLAG_AMAPPAD ? UVM_AMAP_CHUNK : 0),
		    M_WAITOK);
@


1.209
log
@'accomodate' -> 'accommodate' in comments.

Started by diff from Mical Mazurek.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.208 2016/03/09 16:45:43 deraadt Exp $	*/
d1451 4
a1454 1
	 * Amap of e1 must be extended to include e2.
d1458 1
a1458 4
	if (e1->aref.ar_amap) {
		if (amap_extend(e1, e2->end - e2->start))
			return NULL;
	}
@


1.208
log
@remove vaxisms
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.207 2016/03/06 08:56:16 stefan Exp $	*/
d4552 1
a4552 1
 * If the map has a gap that is large enough to accomodate alloc_sz, this
@


1.207
log
@Tweak uvm assertions to avoid locking in some cases.

When only one thread can access a map, there's no need
to lock it. Tweak the assertion instead of appeasing it
by acquiring a lock when it's not necessary.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.206 2016/03/03 12:41:30 naddy Exp $	*/
a3771 1
#if !defined(__vax__)
a3776 1
#endif
@


1.206
log
@Remove option USER_LDT and everything depending on it.
Remove machdep.userldt sysctl.
Remove i386_[gs]et_ldt syscall stub from libi386.
Remove i386_[gs]et_ldt regression test.

ok mlarkin@@ millert@@ guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.205 2015/12/16 14:22:21 kettenis Exp $	*/
d320 6
a325 4
		if (((_map)->flags & VM_MAP_INTRSAFE) == 0)		\
			rw_assert_wrlock(&(_map)->lock);		\
		else							\
			MUTEX_ASSERT_LOCKED(&(_map)->mtx);		\
d2414 1
a2414 1
	map->ref_count = 1;
d2433 3
a2435 2
	 * This requires a write-locked map (because of diagnostic assertions
	 * in insert code).
a2436 5
	if ((map->flags & VM_MAP_INTRSAFE) == 0) {
		if (rw_enter(&map->lock, RW_NOSLEEP|RW_WRITE) != 0)
			panic("uvm_map_setup: rw_enter failed on new map");
	} else
		mtx_enter(&map->mtx);
d2439 1
a2439 4
	if ((map->flags & VM_MAP_INTRSAFE) == 0)
		rw_exit(&map->lock);
	else
		mtx_leave(&map->mtx);
a2462 3
	if (rw_enter(&map->lock, RW_NOSLEEP | RW_WRITE) != 0)
		panic("uvm_map_teardown: rw_enter failed on free map");

a2502 2

	rw_exit(&map->lock);
@


1.205
log
@Avoid grabbing the kernel lock in uvm_unmap() if we have an interrupt-safe
map.  This removes the (hopefully) last case in which pool_put() might try
to grab the kernel lock for interrupt-safe pools.  Note that pools that are
created with the PR_WAITOK flag will still grab the kernel lock.

ok mpi@@, tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.204 2015/11/14 14:53:14 miod Exp $	*/
a3738 4
#endif

#ifdef PMAP_FORK
	pmap_fork(vm1->vm_map.pmap, vm2->vm_map.pmap);
@


1.205.2.1
log
@backport overflow checks from 1.218 and 1.219:
add a check that the arguments to isavail don't overflow.
callers should probably check too, but checking here won't hurt.
possible panic reported by tim newsham.
Check for wraparound before the "commit" phase of uvm_map() and uvm_mapanon(),
to prevent hitting assertions and/or corrupting data structures during that
phase.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.205 2015/12/16 14:22:21 kettenis Exp $	*/
a787 3
	if (addr + sz < addr)
		return 0;

a1049 6
	/* Check if selected address doesn't cause overflow. */
	if (*addr + sz < *addr) {
		error = ENOMEM;
		goto unlock;
	}

a1292 6
	}

	/* Check if selected address doesn't cause overflow. */
	if (*addr + sz < *addr) {
		error = ENOMEM;
		goto unlock;
@


1.204
log
@mutli -> multi
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.203 2015/11/11 15:59:33 mmcc Exp $	*/
d122 1
d1558 14
d1809 4
a1812 1
	uvm_unmap_detach(&dead, 0);
@


1.203
log
@Remove the superfluous typedef uvm_flag_t (unsigned int). Also, fix an
associated mistake in the uvm manpage.

Suggested by and ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.202 2015/10/01 20:27:51 kettenis Exp $	*/
d4275 1
a4275 1
				 * If there are mutliple references to
@


1.202
log
@In uvm_map_splitentry(), grab the kernel lock before calling into the amap
or pager code.  We may end up here without holding the kernel lock from
uvm_unmap().

"ja ja" tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.201 2015/09/28 18:33:42 tedu Exp $	*/
d943 1
a943 1
    vsize_t align, uvm_flag_t flags)
d1131 2
a1132 1
    struct uvm_object *uobj, voff_t uoffset, vsize_t align, uvm_flag_t flags)
@


1.201
log
@add a flag to indicate to uvm_map that it should unmap to make space.
this pulls all the relevant operations under the same map locking, and
relieves calling code from responsibility.
ok kettenis matthew
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.200 2015/09/26 17:55:00 kettenis Exp $	*/
d2576 2
a2577 1
		if (next->aref.ar_amap)
d2579 2
d2587 1
d2590 1
@


1.200
log
@Protect the list of free map entries with a mutex.  This should fix the
crashes seen by sthen@@ on i386.

ok visa@@, guenther@@, tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.199 2015/09/12 18:54:47 kettenis Exp $	*/
d1015 2
d1215 1
a1215 1
	} else
d1217 1
d1240 2
@


1.199
log
@Enable some diagnostics for interrupt-safe maps that are already happening for
normal maps.

ok beck@@, miod@@, guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.198 2015/09/09 23:33:37 kettenis Exp $	*/
d288 1
a1641 1
	int s, i;
d1643 1
d1650 1
a1650 1
		s = splvm();
d1671 1
a1671 1
		splx(s);
a1704 2
	int s;

d1706 1
a1706 1
		s = splvm();
d1710 1
a1710 1
		splx(s);
d2780 1
@


1.198
log
@Add locking for interrupt-safe maps (those that have the VM_MAP_INTRSAFE flag
set).  Since we cannot sleep in that case, use a mutex instead of an rwlock.
This is ok as the more complex code paths in the uvm code are not entered
for interrupt-safe maps as paging isn't allowed in those maps.

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.197 2015/09/09 14:52:12 miod Exp $	*/
d320 2
d2414 2
a2415 1
	}
d2420 2
@


1.197
log
@All our pmap implementations provide pmap_resident_count(), so remove
#ifndef pmap_resident_count code paths.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.196 2015/09/01 05:49:37 deraadt Exp $	*/
d2395 1
d4953 1
a4953 1
		rv = TRUE;
d5004 2
d5019 2
d5034 2
d5046 2
d5057 1
d5068 1
d5080 1
d5091 1
@


1.196
log
@size for free(), and make allocation side look similar
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.195 2015/08/27 21:58:15 kettenis Exp $	*/
a2821 1
#ifdef pmap_resident_count
a2823 4
#else
	/* XXXCDC: this should be required ... */
	(*pr)("\tpmap=%p(resident=<<NOT SUPPORTED!!!>>)\n", map->pmap);
#endif
@


1.195
log
@After more than a decade it seems safe to conclude that amap_clean works.

ok mpi@@, visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.194 2015/08/21 16:04:35 visa Exp $	*/
d3858 1
a3858 1
	vm_map_t result;
d3860 4
a3863 4
	result = malloc(sizeof(struct vm_map), M_VMMAP, M_WAITOK);
	result->pmap = pmap;
	uvm_map_setup(result, min, max, flags);
	return(result);
d3894 1
a3894 1
	free(map, M_VMMAP, 0);
@


1.194
log
@Remove the unused loan_count field and the related uvm logic. Most of
the page loaning code is already in the Attic.

ok kettenis@@, beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.193 2015/08/19 12:23:25 visa Exp $	*/
a4162 1
int	amap_clean_works = 1;	/* XXX for now, just in case... */
a4216 2
			goto flush_object;
		if (!amap_clean_works)
@


1.193
log
@Sync page_flagbits with reality.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.192 2015/07/17 21:56:14 kettenis Exp $	*/
d2943 2
a2944 2
	(*pr)("  uobject=%p, uanon=%p, offset=0x%llx loan_count=%d\n",
	    pg->uobject, pg->uanon, (long long)pg->offset, pg->loan_count);
d4235 1
d4248 2
a4249 3
				/* skip the page if it's loaned or wired */
				if (pg->loan_count != 0 ||
				    pg->wire_count != 0) {
a4250 1
				}
a4253 10
				/*
				 * skip the page if it's not actually owned
				 * by the anon (may simply be loaned to the
				 * anon).
				 */
				if ((pg->pg_flags & PQ_ANON) == 0) {
					KASSERT(pg->uobject == NULL);
					uvm_unlock_pageq();
					break;
				}
@


1.192
log
@Release the kernel lock while tearing down the uvm map in the reaper.  Speeds
up workloads that fork a lot of processes and, more importantly reduces
latency because it makes sure the reaper doesn't hold on to the kernel lock
for long periods of time.

This almost certainly breaks MP kernels on alpha, macppc, m88k and sgi;
hppa might work, but probably doesn't.

ok deraadt@@, beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.191 2015/04/23 00:49:37 dlg Exp $	*/
d2926 2
a2927 2
	"\11ZERO\15PAGER1\20FREE\21INACTIVE\22ACTIVE\24ENCRYPT\30PMAP0"
	"\31PMAP1\32PMAP2\33PMAP3";
@


1.192.4.1
log
@backport overflow checks from 1.218 and 1.219:
add a check that the arguments to isavail don't overflow.
callers should probably check too, but checking here won't hurt.
possible panic reported by tim newsham.
Check for wraparound before the "commit" phase of uvm_map() and uvm_mapanon(),
to prevent hitting assertions and/or corrupting data structures during that
phase.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.192 2015/07/17 21:56:14 kettenis Exp $	*/
a783 3
	if (addr + sz < addr)
		return 0;

a1043 6
	/* Check if selected address doesn't cause overflow. */
	if (*addr + sz < *addr) {
		error = ENOMEM;
		goto unlock;
	}

a1282 6
	}

	/* Check if selected address doesn't cause overflow. */
	if (*addr + sz < *addr) {
		error = ENOMEM;
		goto unlock;
@


1.191
log
@dont need \n on panic strings
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.190 2015/03/30 21:09:55 miod Exp $	*/
d1845 1
d1848 1
a2426 1
	int			 i, waitok = 0;
d2431 5
d2437 4
a2440 6
	if ((map->flags & VM_MAP_INTRSAFE) == 0)
		waitok = 1;
	if (waitok) {
		if (rw_enter(&map->lock, RW_NOSLEEP | RW_WRITE) != 0)
			panic("uvm_map_teardown: rw_enter failed on free map");
	}
d2473 1
a2473 2
		if (waitok)
			uvm_pause();
d2483 1
a2483 2
	if (waitok)
		rw_exit(&map->lock);
d2493 4
a2496 1
	uvm_unmap_detach(&dead_entries, waitok ? UVM_PLA_WAITOK : 0);
d3193 2
@


1.190
log
@Revert 1.173 (special-casing mips64) now that uvm_map_hint()'s return value is
constrained to a suitable range.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.189 2015/03/30 21:08:40 miod Exp $	*/
d1194 1
a1194 1
		panic("uvm_map: kernel map W^X violation requested\n");
d3062 1
a3062 1
			panic("uvm_map_protect: kernel map W^X violation requested\n");
@


1.189
log
@Extend uvm_map_hint() to get an address range as extra arguments, and make
sure it will return an address within that range.

Use this in uaddr_rnd_select() to make sure we will not attempt to pick
an address beyond what we are allowed to map.

In my trees for 9 months, blackmailed s2k15 attendees into agreeing now would
be a good time to commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.188 2015/03/14 03:38:53 jsg Exp $	*/
d3735 1
a3735 1
#if defined (__LP64__) && !defined (__mips64__)
@


1.188
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.187 2015/02/19 03:06:53 mlarkin Exp $	*/
d3716 2
a3717 1
uvm_map_hint(struct vmspace *vm, vm_prot_t prot)
d3750 4
@


1.187
log
@
Remove a lie from a comment that ratholed me for an evening.
Thanks to tedu for hinting that I may have been the victim of UVM's lies,
which pointed me in this direction.

ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.186 2015/02/15 21:34:33 miod Exp $	*/
a92 1
#include <sys/kernel.h>
@


1.186
log
@Change pmap_remove_holes() to take a vmspace instead of a map as its argument.

Use this on vax to correctly pick the end of the stack area now that the
stackgap adjustment code will no longer guarantee it is a fixed location.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.185 2015/02/09 07:14:38 kettenis Exp $	*/
a1110 1
 * => *addr is ignored, except if flags contains UVM_FLAG_FIXED.
@


1.185
log
@Make sure we actually have an entry before checking its limits.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.184 2015/02/06 11:41:55 beck Exp $	*/
d3202 1
a3202 1
		pmap_remove_holes(&vm->vm_map);
d3298 1
a3298 1
		pmap_remove_holes(map);
@


1.184
log
@-Split out uvm_mmap and uvm_map into a version for anon's and a version
for everything else.
-Adapt the anon version to be callable without the biglock held.
Done by tedu@@, kettenis@@ and me.. pounded on a bunch.

This does not yet make mmap a NOLOCK call, but permits it to be so.
ok tedu@@, kettenis@@, guenther@@ jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.183 2015/02/06 09:04:34 tedu Exp $	*/
d4178 1
a4178 1
	for (entry = first; entry->start < end;
@


1.183
log
@make vm_map_lock lock when it's supposed to. add mutex to protect flags
and then double check we didn't lose the unavoidable race.
ok beck guenther kettenis miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.182 2014/12/23 02:01:57 tedu Exp $	*/
d930 178
d1517 1
a1517 1
	int waitok;
d1519 4
a1522 1
	waitok = flags & UVM_PLA_WAITOK;
d1547 1
d1633 1
d2784 1
@


1.182
log
@convert nointr pool_init argument to pr_waitok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.181 2014/12/05 04:12:48 uebayasi Exp $	*/
d2125 1
a2125 1
		atomic_clearbits_int(&map->flags, VM_MAP_WIREFUTURE);
d2131 1
a2131 1
		atomic_setbits_int(&map->flags, VM_MAP_WIREFUTURE);
d2212 1
d4779 1
d4781 1
d4784 1
d4786 9
d4812 2
d4816 2
a4817 1
				tsleep(&map->flags, PVM, (char *)vmmapbsy, 0);
d4819 1
d4821 7
d4894 1
d4896 1
d4904 1
d4907 1
@


1.181
log
@Introduce a new sysctl to retrieve VM map entries

This adds a new sysctl KERN_PROC_VMMAP, which returns an array of VM map
entries of a specified process.  This prevents debuggers from iterating
vm_map_entry RB tree via kvm(3).

The name KERN_PROC_VMMAP and struct kinfo_vmentry are chosen from the same
function in FreeBSD.  struct kinfo_vmentry is revised to reduce size, because
OpenBSD does not keep track of filepaths.  The semantic is also changed to
return max buffer size as a hint, and start iteration at the specified base
address.

Much valuable input from deraadt@@, guenther@@, tedu@@

OK tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d2597 1
a2597 1
	    0, 0, 0, "vmsppl", &pool_allocator_nointr);
d2599 1
a2599 1
	    0, 0, 0, "vmmpepl", &pool_allocator_nointr);
@


1.180
log
@panic if the kernel requests W | X pages; ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.179 2014/11/18 02:37:31 tedu Exp $	*/
d94 1
d4883 55
@


1.179
log
@move arc4random prototype to systm.h. more appropriate for most code
to include that than rdnvar.h. ok deraadt dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.178 2014/11/16 12:31:00 deraadt Exp $	*/
d1015 4
d2876 3
@


1.178
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.177 2014/11/13 00:47:44 tedu Exp $	*/
a93 2

#include <dev/rndvar.h>
@


1.177
log
@bzero -> memset
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.176 2014/10/03 17:41:00 kettenis Exp $	*/
d1069 1
a1069 1
	} else if ((maxprot & VM_PROT_EXECUTE) != 0 &&
d1874 1
a1874 1
		    iter->protection == VM_PROT_NONE)
d1885 1
a1885 1
		    ((iter->protection & VM_PROT_WRITE) ||
d1906 1
a1906 1
		    iter->protection == VM_PROT_NONE)
d1934 1
a1934 1
			    first->protection == VM_PROT_NONE)
d1948 1
a1948 1
			    iter->protection == VM_PROT_NONE)
d2913 1
a2913 1
			    ~VM_PROT_WRITE : VM_PROT_ALL;
d2938 2
a2939 2
		 * wire this entry now if the old protection was VM_PROT_NONE
		 * and the new protection is not VM_PROT_NONE.
d2943 2
a2944 2
		    old_prot == VM_PROT_NONE &&
		    new_prot != VM_PROT_NONE) {
d3350 1
a3350 2
				if (old_entry->max_protection &
				    VM_PROT_WRITE) {
d3355 1
a3355 1
					    ~VM_PROT_WRITE);
d3368 1
a3368 1
			if (old_entry->max_protection & VM_PROT_WRITE)
d3388 1
a3388 1
			    ~VM_PROT_WRITE);
d3537 1
a3537 1
	if ((prot & VM_PROT_EXECUTE) != 0 &&
d3880 1
a3880 1
	    VM_PROT_NONE, 0) != 0) {
d4067 1
a4067 1
				pmap_page_protect(pg, VM_PROT_NONE);
d4110 1
a4110 1
		     ((entry->max_protection & VM_PROT_WRITE) != 0 &&
@


1.176
log
@Introduce __MAP_NOFAULT, a mmap(2) flag that makes sure a mapping will not
cause a SIGSEGV or SIGBUS when a mapped file gets truncated.  Access to
pages that are not backed by a file on such a mapping will be replaced by
zero-filled anonymous pages.  Makes passing file descriptors of mapped files
usable without having to play tricks with signal handlers.

"steal your mmap flag" deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.175 2014/08/14 17:21:38 miod Exp $	*/
d3090 1
a3090 1
		bzero(&ovm->vm_startcopy,
@


1.175
log
@Bring back 1.173 (reverting 1.174) - mips64 systems are still unhappy when
the hint returned is over VM_MAXUSER_ADDRESS, apparently; better be safe for
now while this is investigated further.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.174 2014/08/12 04:29:05 miod Exp $	*/
d1145 2
@


1.174
log
@Revert 1.173 now that the real cause of the octeon regression has been fixed.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.173 2014/07/13 15:33:28 pirofti Exp $	*/
d3544 1
a3544 1
#if defined (__LP64__)
@


1.173
log
@Fall back to smaller spacing on the mips64 machines.

Please spare some change for the mips64 memory-challenged machines..
Some change, Sir?

Fixes at least the octeon platform. Found the hardway on my DSR500.

Found by Boss tedu@@ and Boss deraadt@@
Okay Boss miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.172 2014/07/13 08:15:16 tedu Exp $	*/
d3544 1
a3544 1
#if defined (__LP64__) && !defined (__mips64__)
@


1.172
log
@more mmap random on 64-bit platforms. noticed in freebsd aslr patches.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.171 2014/07/12 18:44:01 tedu Exp $	*/
d3544 1
a3544 1
#ifdef __LP64__
@


1.171
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.170 2014/07/11 16:35:40 jsg Exp $	*/
d3544 3
d3548 1
@


1.170
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.169 2014/06/13 01:48:52 matthew Exp $	*/
d3688 1
a3688 1
	free(map, M_VMMAP);
@


1.169
log
@Add support for MAP_INHERIT_ZERO.

This provides a way for a process to designate pages in its address
space that should be replaced by fresh, zero-initialized anonymous
memory in forked child processes, rather than being copied or shared.

ok jmc, kettenis, tedu, deraadt; positive feedback from many more
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.168 2014/05/15 03:52:25 guenther Exp $	*/
d36 1
a36 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by Charles D. Cranor,
 *      Washington University, the University of California, Berkeley and 
 *      its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.168
log
@Move from struct proc to process the reference-count-holding pointers
to the process's vmspace and filedescs.  struct proc continues to
keep copies of the pointers, copying them on fork, clearing them
on exit, and (for vmspace) refreshing on exec.
Also, make uvm_swapout_threads() thread aware, eliminating p_swtime
in kernel.

particular testing by ajacoutot@@ and sebastia@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.167 2014/04/13 23:14:15 tedu Exp $	*/
d193 1
a193 1
void			 uvm_mapent_forkshared(struct vmspace*, struct vm_map*,
d196 4
a199 1
void			 uvm_mapent_forkcopy(struct vmspace*, struct vm_map*,
d3164 1
a3164 1
struct vm_map_entry*
d3215 1
a3215 1
void
d3249 1
a3249 7
	/* Update process statistics. */
	if (!UVM_ET_ISHOLE(new_entry))
		new_map->size += new_entry->end - new_entry->start;
	if (!UVM_ET_ISOBJ(new_entry) && !UVM_ET_ISHOLE(new_entry)) {
		new_vm->vm_dused +=
		    uvmspace_dused(new_map, new_entry->start, new_entry->end);
	}
d3259 1
a3259 1
void
d3396 33
a3428 6
	/* Update process statistics. */
	if (!UVM_ET_ISHOLE(new_entry))
		new_map->size += new_entry->end - new_entry->start;
	if (!UVM_ET_ISOBJ(new_entry) && !UVM_ET_ISHOLE(new_entry)) {
		new_vm->vm_dused +=
		    uvmspace_dused(new_map, new_entry->start, new_entry->end);
d3430 2
d3447 1
a3447 1
	struct vm_map_entry *old_entry;
d3479 11
a3489 2
		if (old_entry->inheritance == MAP_INHERIT_SHARE) {
			uvm_mapent_forkshared(vm2, new_map,
d3491 3
d3495 7
a3501 3
		if (old_entry->inheritance == MAP_INHERIT_COPY) {
			uvm_mapent_forkcopy(vm2, new_map,
			    old_map, old_entry, &dead);
d3715 1
@


1.167
log
@compress code by turning four line comments into one line comments.
emphatic ok usual suspects, grudging ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.166 2014/04/10 01:40:04 tedu Exp $	*/
d3018 1
a3018 1
 * - used for vfork and threads
d3021 2
a3022 3
void
uvmspace_share(p1, p2)
	struct proc *p1, *p2;
d3024 4
a3027 2
	p2->p_vmspace = p1->p_vmspace;
	p1->p_vmspace->vm_refcnt++;
d3039 2
a3040 1
	struct vmspace *nvm, *ovm = p->p_vmspace;
d3054 3
a3056 2
		 * if p is the only process using its vmspace then we can safely
		 * recycle that vmspace for the program that is being exec'd.
d3110 3
a3112 3
		 * p's vmspace is being shared, so we can't reuse it for p since
		 * it is still being used for others.   allocate a new vmspace
		 * for p
d3119 1
a3119 1
		p->p_vmspace = nvm;
d3415 1
a3415 1
uvmspace_fork(struct vmspace *vm1)
d3417 1
@


1.166
log
@rename waitable to waitok as suggested by kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.165 2014/04/03 21:40:10 tedu Exp $	*/
a202 1

d617 1
a617 3
		/*
		 * Correct for bias.
		 */
d821 1
a821 3
	/*
	 * Validation.
	 */
d883 1
a883 3
	/*
	 * Fall back to brk() and stack() address selectors.
	 */
d986 1
a986 3
			/*
			 * pmap_offset satisfies align, no change.
			 */
d988 1
a988 3
			/*
			 * Align takes precedence over pmap prefer.
			 */
d994 1
a994 3
	/*
	 * Decode parameters.
	 */
d1005 1
a1005 3
	/*
	 * Holes are incompatible with other types of mappings.
	 */
d1011 1
a1011 3
	/*
	 * Unset hint for kernel_map non-fixed allocations.
	 */
d1015 1
a1015 3
	/*
	 * Check protection.
	 */
d1056 1
a1056 3
		/*
		 * Check that the space is available.
		 */
d1073 1
a1073 3
		/*
		 * Run selection algorithm for executables.
		 */
d1077 1
a1077 3
		/*
		 * Grow kernel memory and try again.
		 */
d1089 1
a1089 3
		/*
		 * Update freelists from vmspace.
		 */
d1096 1
a1096 3
		/*
		 * Grow kernel memory and try again.
		 */
d1111 1
a1111 3
	/*
	 * If we only want a query, return now.
	 */
d1159 1
a1159 3
	/*
	 * Update map and process statistics.
	 */
d1205 1
a1205 3
	/*
	 * Must be the same entry type and not have free memory between.
	 */
d1209 1
a1209 3
	/*
	 * Submaps are never joined.
	 */
d1213 1
a1213 3
	/*
	 * Never merge wired memory.
	 */
d1217 1
a1217 3
	/*
	 * Protection, inheritance and advice must be equal.
	 */
d1224 1
a1224 3
	/*
	 * If uvm_object: objects itself and offsets within object must match.
	 */
d1243 1
a1243 3
	/*
	 * Apprently, e1 and e2 match.
	 */
a1272 1

d1302 1
a1302 3
	/*
	 * Merge with previous entry.
	 */
d1342 1
a1342 3
		/*
		 * Drop reference to amap, if we've got one.
		 */
d1349 1
a1349 3
		/*
		 * Drop reference to our backing object, if we've got one.
		 */
d1359 1
a1359 3
		/*
		 * Step to next.
		 */
d1394 1
a1394 3
	/*
	 * Initialize new entry.
	 */
d1412 1
a1412 3
	/*
	 * Reset free space in first.
	 */
d1431 1
a1431 3
	/*
	 * Remove first if it is entirely inside <addr, addr+sz>.
	 */
d1440 1
a1440 3
	/*
	 * Finally, link in entry.
	 */
d1630 2
a1631 3
	/*
	 * Entry is describing only free memory and has nothing to drain into.
	 */
d1659 1
a1659 3
	/*
	 * Unwire removed map entry.
	 */
d1665 1
a1665 3
	/*
	 * Entry-type specific code.
	 */
d1667 1
a1667 3
		/*
		 * Nothing to be done for holes.
		 */
a1674 1

a1703 1

d1721 1
a1721 3
		/*
		 * remove mappings the standard way.
		 */
d1750 1
a1750 3
	/*
	 * Find first affected entry.
	 */
d1781 1
a1781 3
		/*
		 * Update space usage.
		 */
d1791 1
a1791 3
		/*
		 * Actual removal of entry.
		 */
d1944 1
a1944 3
		/*
		 * decrease counter in the rest of the entries
		 */
d1959 1
a1959 3
	/*
	 * We are currently holding a read lock.
	 */
d2022 1
a2022 3
	/*
	 * Check that the range has no holes.
	 */
d2053 1
a2053 3
	/*
	 * Wire/unwire pages here.
	 */
d2210 1
a2210 3
	/*
	 * Configure the allocators.
	 */
d2253 1
a2253 3
	/*
	 * Remove address selectors.
	 */
d2451 2
a2452 3
		/*
		 * May not be empty.
		 */
d2456 1
a2456 3
		/*
		 * Addresses for entry must lie within map boundaries.
		 */
d2460 1
a2460 3
		/*
		 * Tree may not have gaps.
		 */
d2584 1
a2584 4
	/*
	 * now set up static pool of kernel map entries ...
	 */

d2592 1
a2592 3
	/*
	 * initialize the map-related pools.
	 */
d2639 1
a2639 3
	/*
	 * struct vmspace handling.
	 */
d2859 1
a2859 3
	/*
	 * First, check for protection violations.
	 */
d2876 1
a2876 3
	/*
	 * Fix protections.
	 */
d3049 1
a3049 4
	/*
	 * see if more than one process is using this vmspace...
	 */

d3086 1
a3086 3
		/*
		 * Nuke statistics and boundaries.
		 */
d3097 1
a3097 3
		/*
		 * Setup new boundaries and populate map with entries.
		 */
d3103 1
a3103 3
		/*
		 * but keep MMU holes unavailable
		 */
a3104 1

a3105 1

d3114 1
a3114 4
		/*
		 * install new vmspace and drop our ref to the old one.
		 */

d3122 1
a3122 3
	/*
	 * Release dead entries
	 */
a3130 1

d3167 1
a3167 4
	/*
	 * Create new entry (linked in on creation).
	 * Fill in first, last.
	 */
d3187 1
a3187 4
	/*
	 * gain reference to object backing the map (can't
	 * be a submap).
	 */
d3243 1
a3243 3
	/*
	 * Update process statistics.
	 */
a3304 1

a3323 1

a3324 1

a3333 1

a3335 1

d3365 1
a3365 3
	  		/*
	  		 * parent must now be write-protected
	  		 */
a3367 1

a3375 1

a3376 1

a3380 1

d3387 1
a3387 3
		/*
		 * protect the child's mappings if necessary
		 */
d3396 1
a3396 3
	/*
	 * Update process statistics.
	 */
d3430 1
a3430 4
	/*
	 * go entry-by-entry
	 */

d3436 1
a3436 3
		/*
		 * first, some sanity checks on the old entry
		 */
d3448 1
a3448 3
		/*
		 * Apply inheritance.
		 */
d3593 1
a3593 3
		/*
		 * Fail if a hole is found.
		 */
d3598 1
a3598 3
		/*
		 * Check protection.
		 */
a3640 1

a3735 1

a3777 1

d3784 1
a3784 5
	/*
	 * Initialize dead entries.
	 * Handle len == 0 case.
	 */

d3788 1
a3788 3
	/*
	 * Acquire lock on srcmap.
	 */
d3791 1
a3791 3
	/*
	 * Lock srcmap, lookup first and last entry in <start,len>.
	 */
d3794 1
a3794 3
	/*
	 * Check that the range is contiguous.
	 */
d3828 1
a3828 3
	/*
	 * Lock destination map (kernel_map).
	 */
d3843 1
a3843 4

	/*
	 * step 1: start looping through map entries, performing extraction.
	 */
d3850 1
a3850 3
		/*
		 * Calculate uvm_mapent_clone parameters.
		 */
d3881 1
a3881 3
	/*
	 * Unmap copied entries on failure.
	 */
d3888 1
a3888 3
	/*
	 * Release maps, release dead entries.
	 */
a3914 1

d3939 1
a3939 3
	/*
	 * Make a first pass to check for holes.
	 */
a4026 1

a4027 1

a4044 1

d4089 1
a4089 3
	/*
	 * Invoke splitentry.
	 */
d4320 1
a4320 3
	/*
	 * Include the guard page in the hard minimum requirement of alloc_sz.
	 */
d4480 1
a4480 3
		/*
		 * Claim guard page for entry.
		 */
d4585 1
a4585 3
	/*
	 * Configure pmap prefer.
	 */
d4594 1
a4594 3
	/*
	 * Align address to pmap_prefer unless FLAG_FIXED is set.
	 */
d4602 1
a4602 3
	/*
	 * First, check if the requested range is fully available.
	 */
a4626 1

d4638 1
a4638 3
	/*
	 * Test if the next entry is sufficient for the allocation.
	 */
a4645 1

d4653 1
a4653 3
		/*
		 * Skip brk() allocation addresses.
		 */
d4661 1
a4661 3
		/*
		 * Skip stack allocation addresses.
		 */
d4700 1
a4700 3
	/*
	 * Stay at the top of brk() area.
	 */
d4703 1
a4703 3
	/*
	 * Stay at the far end of the stack area.
	 */
d4712 1
a4712 3
	/*
	 * No bias, this area is meant for us.
	 */
a4841 1

@


1.165
log
@add a uvm_yield function and use it in the reaper path to prevent the
reaper from hogging the cpu. it will do the kernel lock twiddle trick to
allow other CPUs a chance to run, and also checks if the reaper has been
running for an entire timeslice and should be preempted.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.164 2014/01/23 22:06:30 miod Exp $	*/
d1384 1
a1384 1
	int waitable;
d1386 1
a1386 1
	waitable = flags & UVM_PLA_WAITOK;
d1388 1
a1388 1
		if (waitable)
d2329 1
a2329 1
	int			 i, waitable = 0;
d2336 2
a2337 2
		waitable = 1;
	if (waitable) {
d2375 1
a2375 1
		if (waitable)
d2386 1
a2386 1
	if (waitable)
d2397 1
a2397 1
	uvm_unmap_detach(&dead_entries, waitable ? UVM_PLA_WAITOK : 0);
@


1.164
log
@unifdef -D__HAVE_VM_PAGE_MD - no functional change.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.163 2013/09/21 10:01:27 miod Exp $	*/
d1384 1
d1386 1
d1388 2
d1397 1
a1397 1
			    flags);
d2329 1
a2329 1
	int			 i;
d2335 3
a2337 1
	if ((map->flags & VM_MAP_INTRSAFE) == 0) {
d2375 2
d2386 1
a2386 1
	if ((map->flags & VM_MAP_INTRSAFE) == 0)
d2397 1
a2397 1
	uvm_unmap_detach(&dead_entries, 0);
@


1.163
log
@Don't invoke pmap_copy() on map holes.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.162 2013/05/30 15:17:59 tedu Exp $	*/
a2857 1
#ifdef __HAVE_VM_PAGE_MD
a2858 3
#else
	(*pr)("\n");
#endif
@


1.162
log
@remove simple_locks from uvm code. ok beck deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.161 2013/04/17 23:22:42 tedu Exp $	*/
d3365 3
a3367 2
	pmap_copy(new_map->pmap, old_map->pmap, new_entry->start,
	    (new_entry->end - new_entry->start), new_entry->start);
d3520 5
a3524 4
		pmap_copy(new_map->pmap, old_map->pmap,
		    new_entry->start,
		    (old_entry->end - old_entry->start),
		    old_entry->start);
@


1.161
log
@it is better if we always start addr at something reasonable, and
then move it up. previous revision would leave addr uninitialized.
pointed out by oga at nicotinebsd.org
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.160 2013/04/17 17:46:53 tedu Exp $	*/
a493 1
		simple_lock(&map->ref_lock);				\
a494 1
		simple_unlock(&map->ref_lock);				\
a1521 1
		simple_lock(&uvm.kentry_lock);
a1541 1
		simple_unlock(&uvm.kentry_lock);
a1579 1
		simple_lock(&uvm.kentry_lock);
a1582 1
		simple_unlock(&uvm.kentry_lock);
a2291 1
	simple_lock_init(&map->ref_lock);
a2675 1
	simple_lock_init(&uvm.kentry_lock);
a3782 1
	simple_lock(&map->ref_lock);
a3783 1
	simple_unlock(&map->ref_lock);
a4161 2
			simple_lock(&anon->an_lock); /* XXX */

a4163 1
				simple_unlock(&anon->an_lock);
a4180 1
					simple_unlock(&anon->an_lock);
a4193 1
					simple_unlock(&anon->an_lock);
a4204 1
				simple_unlock(&anon->an_lock);
a4217 1
					simple_unlock(&anon->an_lock);
a4222 1
				simple_unlock(&anon->an_lock);
a4246 1
			simple_lock(&uobj->vmobjlock);
a4249 1
			simple_unlock(&uobj->vmobjlock);
@


1.160
log
@do not permanently avoid the BRKSIZ gap in the heap for mmap. after some
allocations have been made, open it up. this is a variation on a previous
change that was lost in the great uvm map rewrite. allows some platforms,
notably i386, to fully utilize their address space.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.159 2013/02/10 19:19:30 beck Exp $	*/
d3661 1
d3668 1
a3668 1
		addr = (vaddr_t)vm->vm_daddr + BRKSIZ;
@


1.159
log
@Don't wait for memory from pool while holding vm_map_lock or we can
deadlock ourselves - based on an infrequent hang caught by sthen, and
diagnosed by kettenis and me. Fix after some iterations is to simply
call uvm_map_allocate and allocate the map entry before grabbing the
lock so we don't wait while holding the lock.
ok miod@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.158 2012/10/18 08:46:23 gerhard Exp $	*/
d3644 1
d3658 10
a3667 2
	/* start malloc/mmap after the brk */
	addr = (vaddr_t)vm->vm_daddr + BRKSIZ;
d3669 1
a3669 1
	addr += arc4random() & (MIN((256 * 1024 * 1024), BRKSIZ) - 1);
@


1.158
log
@Wiring map entries with VM_PROT_NONE only waists RAM and bears no
advantages. We shouln't do this. If the protection changes later on
(and VM_MAP_WIREFUTURE was set), uvm_map_protect() will wire them.
Found by Matthias Pitzl.

ok miod@@ markus@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.157 2012/06/14 15:54:36 ariane Exp $	*/
d124 1
a124 1
			    struct uvm_map_deadq*);
d964 1
a964 1
	struct vm_map_entry	*first, *last, *entry;
d1040 9
d1050 4
a1053 2
		if (vm_map_lock_try(map) == FALSE)
			return EFAULT;
d1161 2
a1162 1
	entry = uvm_map_mkentry(map, first, last, *addr, sz, flags, &dead);
d1167 1
d1225 3
d1426 1
a1426 1
    struct uvm_map_deadq *dead)
d1449 4
a1452 1
	entry = uvm_mapent_alloc(map, flags);
d3305 1
a3305 1
	    dstaddr, dstlen, mapent_flags, dead);
@


1.157
log
@Remove uvm_km_kmem_grow printf.

It's no longer required, code is stable.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.156 2012/06/14 11:57:18 jasper Exp $	*/
d1935 2
a1936 1
		if (UVM_ET_ISHOLE(iter) || iter->start == iter->end)
d1967 2
a1968 1
		if (UVM_ET_ISHOLE(iter) || iter->start == iter->end)
d1994 3
a1996 1
			if (UVM_ET_ISHOLE(first) || first->start == first->end)
d2011 2
a2012 1
			if (UVM_ET_ISHOLE(iter) || iter->start == iter->end)
@


1.156
log
@fix typo in comment

ok ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.155 2012/06/03 13:30:04 kettenis Exp $	*/
a4525 2

	printf("uvm_km_kmem_grow: grown to 0x%lx\n", uvm_maxkaddr);
@


1.155
log
@Make sure uvm_map_extract() entesr mappings at an address that doesn't
introduce any virtual cache aliasing problems.  Fixes a regression introduced
by vmmap.

ok ariane@@, jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.154 2012/06/01 05:47:10 guenther Exp $	*/
d958 1
a958 1
 *    Align is only a hint and will be ignored if the alignemnt fails.
@


1.154
log
@Correct handling of mlock()/munlock() with len==0 to return success
instead of crashing.  Add a KASSERT() to catch other bugs that might
result in the tree iterators being reversed.

Problem observed by Tom Doherty (tomd at singlesecond.com)
ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.153 2012/04/19 12:42:03 ariane Exp $	*/
d3975 2
a3976 1
	    PAGE_SIZE, 0, VM_PROT_NONE, 0) != 0) {
@


1.153
log
@Backout misplaced optimization in vmmap.

(That means the misplaced optimization is back in.)  It broke mips and
possibly other architectures.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.152 2012/04/17 20:22:52 ariane Exp $	*/
d2057 2
d2111 2
a2112 1
	} else
d2114 1
@


1.152
log
@uvmspace_exec: Remove disfunctional "optimization".

The optimization goes through great lengths to use less optimized code
paths in place of the simple path, where the latter is actually faster.

ok tedu, guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.151 2012/04/11 11:23:22 ariane Exp $	*/
d3121 7
a3127 2
	int pageable = (p->p_vmspace->vm_map.flags & VM_MAP_PAGEABLE);
	struct vmspace *old, *new;
d3129 70
a3198 4
	/* Create new vmspace. */
	new = uvmspace_alloc(start, end, (pageable ? TRUE : FALSE),
	    TRUE);
	old = p->p_vmspace;
d3200 18
a3217 4
	pmap_unuse_final(p);   /* before stack addresses go away */
	pmap_deactivate(p);
	p->p_vmspace = new;
	pmap_activate(p);
d3219 4
a3222 2
	/* Throw away the old vmspace. */
	uvmspace_free(old);
@


1.151
log
@vmmap: speed up allocations

Reduces O(n log n) allocations to O(log n).

ok deraadt, tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.150 2012/03/15 22:22:28 ariane Exp $	*/
d3121 7
a3127 7
	struct vmspace *nvm, *ovm = p->p_vmspace;
	struct vm_map *map = &ovm->vm_map;
	struct uvm_map_deadq dead_entries;

	KASSERT((start & (vaddr_t)PAGE_MASK) == 0);
	KASSERT((end & (vaddr_t)PAGE_MASK) == 0 ||
	    (end & (vaddr_t)PAGE_MASK) == (vaddr_t)PAGE_MASK);
d3130 3
a3132 69
	TAILQ_INIT(&dead_entries);

	/*
	 * see if more than one process is using this vmspace...
	 */

	if (ovm->vm_refcnt == 1) {
		/*
		 * if p is the only process using its vmspace then we can safely
		 * recycle that vmspace for the program that is being exec'd.
		 */

#ifdef SYSVSHM
		/*
		 * SYSV SHM semantics require us to kill all segments on an exec
		 */
		if (ovm->vm_shm)
			shmexit(ovm);
#endif

		/*
		 * POSIX 1003.1b -- "lock future mappings" is revoked
		 * when a process execs another program image.
		 */
		vm_map_lock(map);
		vm_map_modflags(map, 0, VM_MAP_WIREFUTURE);

		/*
		 * now unmap the old program
		 *
		 * Instead of attempting to keep the map valid, we simply
		 * nuke all entries and ask uvm_map_setup to reinitialize
		 * the map to the new boundaries.
		 *
		 * uvm_unmap_remove will actually nuke all entries for us
		 * (as in, not replace them with free-memory entries).
		 */
		uvm_unmap_remove(map, map->min_offset, map->max_offset,
		    &dead_entries, TRUE, FALSE);

		KDASSERT(RB_EMPTY(&map->addr));

		/*
		 * Nuke statistics and boundaries.
		 */
		bzero(&ovm->vm_startcopy,
		    (caddr_t) (ovm + 1) - (caddr_t) &ovm->vm_startcopy);


		if (end & (vaddr_t)PAGE_MASK) {
			end += 1;
			if (end == 0) /* overflow */
				end -= PAGE_SIZE;
		}

		/*
		 * Setup new boundaries and populate map with entries.
		 */
		map->min_offset = start;
		map->max_offset = end;
		uvm_map_setup_entries(map);
		vm_map_unlock(map);

		/*
		 * but keep MMU holes unavailable
		 */
		pmap_remove_holes(map);

	} else {
d3134 2
a3135 23
		/*
		 * p's vmspace is being shared, so we can't reuse it for p since
		 * it is still being used for others.   allocate a new vmspace
		 * for p
		 */
		nvm = uvmspace_alloc(start, end,
		    (map->flags & VM_MAP_PAGEABLE) ? TRUE : FALSE, TRUE);

		/*
		 * install new vmspace and drop our ref to the old one.
		 */

		pmap_deactivate(p);
		p->p_vmspace = nvm;
		pmap_activate(p);

		uvmspace_free(ovm);
	}

	/*
	 * Release dead entries
	 */
	uvm_unmap_detach(&dead_entries, 0);
@


1.150
log
@Fix vmmap SMALL_KERNEL introduced bug.

The
	if (min < VMMAP_MIN_ADDR)
		min = VMMAP_MIN_ADDR;
code should have moved across when the small_kernel diff moved the
initialization from uvm_map_setup() to uvm_map_setup_md().
Prevents a nasty panic on hppa, sparc64 (and possibly other archs).

kettenis: the diff make some sense to me
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.149 2012/03/15 17:52:28 ariane Exp $	*/
d160 2
d410 10
a419 5
	fun = uaddr->uaddr_functions;
	KDASSERT(fun != NULL);
	if (fun->uaddr_free_insert != NULL)
		(*fun->uaddr_free_insert)(map, uaddr, entry);
	entry->etype |= UVM_ET_FREEMAPPED;
d431 1
a431 1
	KASSERT((entry->etype & UVM_ET_FREEMAPPED) != 0);
d435 6
a440 4
	fun = uaddr->uaddr_functions;
	if (fun->uaddr_free_remove != NULL)
		(*fun->uaddr_free_remove)(map, uaddr, entry);
	entry->etype &= ~UVM_ET_FREEMAPPED;
d901 40
d1306 1
a1306 2
	if (free)
		uvm_mapent_free_remove(map, free, e1);
d1309 1
a1309 2
	if (free)
		uvm_mapent_free_remove(map, free, e2);
d1314 1
a1314 2
	if (free)
		uvm_mapent_free_insert(map, free, e1);
d1451 1
a1451 2
	if (free)
		uvm_mapent_free_remove(map, free, first);
d1464 1
a1464 2
		if (free)
			uvm_mapent_free_remove(map, free, last);
d1686 1
a1686 2
	if (free)
		uvm_mapent_free_remove(map, free, entry);
d1693 1
a1693 2
			if (free)
				uvm_mapent_free_remove(map, free, prev);
d2417 1
a2417 2
	if (free)
		uvm_mapent_free_remove(map, free, orig);
d2465 1
a2465 2
	if (free_before)
		uvm_mapent_free_insert(map, free_before, orig);
d2467 1
a2467 2
	if (free)
		uvm_mapent_free_insert(map, free, next);
d2751 1
d4275 1
a4275 2
	if (free)
		uvm_mapent_free_remove(map, free, entry);
d4285 1
a4285 2
	if (free)
		uvm_mapent_free_insert(map, free, tmp);
d4543 1
a4543 2
		if (free)
			uvm_mapent_free_remove(map, free, entry);
d4704 1
a4704 1
			if (entry != NULL && entfree != NULL)
d4730 1
a4730 1
	if (entry != NULL && entfree != NULL)
d5023 2
d5027 1
@


1.149
log
@Reduce installmedia pressure from new vmmap.

Has less special allocators on install media (where they aren't required
anyway).
Bonus: makes the vmmap initialization code easier to read.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.148 2012/03/09 13:01:29 ariane Exp $	*/
a2230 7
	 * Ensure the selectors will not try to manage page 0;
	 * it's too special.
	 */
	if (min < VMMAP_MIN_ADDR)
		min = VMMAP_MIN_ADDR;

	/*
d5004 7
d5037 7
d5071 7
@


1.148
log
@New vmmap implementation.

no oks (it is really a pain to review properly)
extensively tested, I'm confident it'll be stable
'now is the time' from several icb inhabitants

Diff provides:
- ability to specify different allocators for different regions/maps
- a simpler implementation of the current allocator
- currently in compatibility mode: it will generate similar addresses
  as the old allocator
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.144 2011/07/03 18:36:49 oga Exp $	*/
d139 1
d2240 3
a2242 38
	if (flags & VM_MAP_ISVMSPACE) {
		/*
		 * Setup hint areas.
		 */
#if 0 /* Don't use the cool stuff yet. */
#ifdef __LP64__
		/* Hinted allocations above 4GB */
		map->uaddr_any[0] =
		    uaddr_hint_create(0x100000000ULL, max, 1024 * 1024 * 1024);
		/* Hinted allocations below 4GB */
		map->uaddr_any[1] =
		    uaddr_hint_create(MAX(min, VMMAP_MIN_ADDR), 0x100000000ULL,
		    1024 * 1024 * 1024);
#else
		map->uaddr_any[1] =
		    uaddr_hint_create(MAX(min, VMMAP_MIN_ADDR), max,
		    1024 * 1024 * 1024);
#endif

#ifdef __i386__
		map->uaddr_exe = uaddr_rnd_create(min, I386_MAX_EXE_ADDR);
		map->uaddr_any[3] = uaddr_pivot_create(2 * I386_MAX_EXE_ADDR,
		    max);
#elif defined(__LP64__)
		map->uaddr_any[3] =
		    uaddr_pivot_create(MAX(min, 0x100000000ULL), max);
#else
		map->uaddr_any[3] = uaddr_pivot_create(min, max);
#endif
#else /* Don't use the cool stuff yet. */
		/*
		 * Use the really crappy stuff at first commit.
		 * Browsers like crappy stuff.
		 */
		map->uaddr_any[0] = uaddr_rnd_create(min, max);
#endif
		map->uaddr_brk_stack = uaddr_stack_brk_create(min, max);
	} else
d4995 85
@


1.147
log
@Fix spelling and remove question
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.146 2011/11/08 11:42:43 miod Exp $	*/
d4 16
a19 1
/* 
d89 3
a106 2
#undef RB_AUGMENT
#define RB_AUGMENT(x) uvm_rb_augment(x)
d112 178
a297 1

a302 1

a305 1
#ifdef PMAP_GROWKERNEL
d308 1
a308 1
 * space.  If we want to exceed this, we must grow the kernel
a313 1
#endif
d316 1
a316 1
 * macros
d318 5
d325 1
a325 1
 * uvm_map_entry_link: insert entry into a map
d327 3
a329 1
 * => map must be locked
a330 8
#define uvm_map_entry_link(map, after_where, entry) do { \
	(map)->nentries++; \
	(entry)->prev = (after_where); \
	(entry)->next = (after_where)->next; \
	(entry)->prev->next = (entry); \
	(entry)->next->prev = (entry); \
	uvm_rb_insert(map, entry); \
} while (0)
d332 5
a336 11
/*
 * uvm_map_entry_unlink: remove entry from a map
 *
 * => map must be locked
 */
#define uvm_map_entry_unlink(map, entry) do { \
	(map)->nentries--; \
	(entry)->next->prev = (entry)->prev; \
	(entry)->prev->next = (entry)->next; \
	uvm_rb_remove(map, entry); \
} while (0)
d339 1
a339 1
 * SAVE_HINT: saves the specified entry as the hint for future lookups.
d341 2
a342 11
 * => map need not be locked (protected by hint_lock).
 */
#define SAVE_HINT(map,check,value) do { \
	simple_lock(&(map)->hint_lock); \
	if ((map)->hint == (check)) \
		(map)->hint = (value); \
	simple_unlock(&(map)->hint_lock); \
} while (0)

/*
 * VM_MAP_RANGE_CHECK: check and correct range
d344 3
a346 1
 * => map must at least be read locked
d349 6
a354 8
#define VM_MAP_RANGE_CHECK(map, start, end) do { \
	if (start < vm_map_min(map)) 		\
		start = vm_map_min(map);        \
	if (end > vm_map_max(map))              \
		end = vm_map_max(map);          \
	if (start > end)                        \
		start = end;                    \
} while (0)
d357 1
a357 1
 * local prototypes
d359 5
d365 4
a368 6
void uvm_mapent_copy(struct vm_map_entry *, struct vm_map_entry *);
void uvm_map_entry_unwire(struct vm_map *, struct vm_map_entry *);
void uvm_map_reference_amap(struct vm_map_entry *, int);
void uvm_map_unreference_amap(struct vm_map_entry *, int);
int uvm_map_spacefits(struct vm_map *, vaddr_t *, vsize_t,
    struct vm_map_entry *, voff_t, vsize_t);
d370 4
a373 2
struct vm_map_entry	*uvm_mapent_alloc(struct vm_map *, int);
void			uvm_mapent_free(struct vm_map_entry *);
a374 1
#ifdef KVA_GUARDPAGES
d376 1
a376 1
 * Number of kva guardpages in use.
d378 7
a384 1
int kva_guardpages;
d387 7
d395 5
a399 9
/*
 * Tree manipulation.
 */
void uvm_rb_insert(struct vm_map *, struct vm_map_entry *);
void uvm_rb_remove(struct vm_map *, struct vm_map_entry *);
vsize_t uvm_rb_space(struct vm_map *, struct vm_map_entry *);

#ifdef DEBUG
int _uvm_tree_sanity(struct vm_map *map, const char *name);
d401 2
a402 13
vsize_t uvm_rb_subtree_space(struct vm_map_entry *);
void uvm_rb_fixup(struct vm_map *, struct vm_map_entry *);

static __inline int
uvm_compare(struct vm_map_entry *a, struct vm_map_entry *b)
{
	if (a->start < b->start)
		return (-1);
	else if (a->start > b->start)
		return (1);
	
	return (0);
}
d404 1
d406 6
a411 4
static __inline void
uvm_rb_augment(struct vm_map_entry *entry)
{
	entry->space = uvm_rb_subtree_space(entry);
d414 6
a419 21
RB_PROTOTYPE(uvm_tree, vm_map_entry, rb_entry, uvm_compare);

RB_GENERATE(uvm_tree, vm_map_entry, rb_entry, uvm_compare);

vsize_t
uvm_rb_space(struct vm_map *map, struct vm_map_entry *entry)
{
	struct vm_map_entry *next;
	vaddr_t space;

	if ((next = entry->next) == &map->header)
		space = map->max_offset - entry->end;
	else {
		KASSERT(next);
		space = next->start - entry->end;
	}
	return (space);
}
		
vsize_t
uvm_rb_subtree_space(struct vm_map_entry *entry)
d421 1
a421 8
	vaddr_t space, tmp;

	space = entry->ownspace;
	if (RB_LEFT(entry, rb_entry)) {
		tmp = RB_LEFT(entry, rb_entry)->space;
		if (tmp > space)
			space = tmp;
	}
d423 3
a425 5
	if (RB_RIGHT(entry, rb_entry)) {
		tmp = RB_RIGHT(entry, rb_entry)->space;
		if (tmp > space)
			space = tmp;
	}
d427 4
a430 1
	return (space);
d433 3
d437 1
a437 1
uvm_rb_fixup(struct vm_map *map, struct vm_map_entry *entry)
d439 1
a439 6
	/* We need to traverse to the very top */
	do {
		entry->ownspace = uvm_rb_space(map, entry);
		entry->space = uvm_rb_subtree_space(entry);
	} while ((entry = RB_PARENT(entry, rb_entry)) != NULL);
}
d441 7
a447 5
void
uvm_rb_insert(struct vm_map *map, struct vm_map_entry *entry)
{
	vaddr_t space = uvm_rb_space(map, entry);
	struct vm_map_entry *tmp;
d449 10
a458 9
	entry->ownspace = entry->space = space;
	tmp = RB_INSERT(uvm_tree, &(map)->rbhead, entry);
#ifdef DIAGNOSTIC
	if (tmp != NULL)
		panic("uvm_rb_insert: duplicate entry?");
#endif
	uvm_rb_fixup(map, entry);
	if (entry->prev != &map->header)
		uvm_rb_fixup(map, entry->prev);
d461 3
d465 1
a465 1
uvm_rb_remove(struct vm_map *map, struct vm_map_entry *entry)
d467 1
a467 1
	struct vm_map_entry *parent;
d469 6
a474 6
	parent = RB_PARENT(entry, rb_entry);
	RB_REMOVE(uvm_tree, &(map)->rbhead, entry);
	if (entry->prev != &map->header)
		uvm_rb_fixup(map, entry->prev);
	if (parent)
		uvm_rb_fixup(map, parent);
d477 11
a487 65
#ifdef DEBUG
#define uvm_tree_sanity(x,y) _uvm_tree_sanity(x,y)
#else
#define uvm_tree_sanity(x,y)
#endif

#ifdef DEBUG
int
_uvm_tree_sanity(struct vm_map *map, const char *name)
{
	struct vm_map_entry *tmp, *trtmp;
	int n = 0, i = 1;

	RB_FOREACH(tmp, uvm_tree, &map->rbhead) {
		if (tmp->ownspace != uvm_rb_space(map, tmp)) {
			printf("%s: %d/%d ownspace %x != %x %s\n",
			    name, n + 1, map->nentries,
			    tmp->ownspace, uvm_rb_space(map, tmp),
			    tmp->next == &map->header ? "(last)" : "");
			goto error;
		}
	}
	trtmp = NULL;
	RB_FOREACH(tmp, uvm_tree, &map->rbhead) {
		if (tmp->space != uvm_rb_subtree_space(tmp)) {
			printf("%s: space %d != %d\n",
			    name, tmp->space, uvm_rb_subtree_space(tmp));
			goto error;
		}
		if (trtmp != NULL && trtmp->start >= tmp->start) {
			printf("%s: corrupt: 0x%lx >= 0x%lx\n",
			    name, trtmp->start, tmp->start);
			goto error;
		}
		n++;

	    trtmp = tmp;
	}

	if (n != map->nentries) {
		printf("%s: nentries: %d vs %d\n",
		    name, n, map->nentries);
		goto error;
	}

	for (tmp = map->header.next; tmp && tmp != &map->header;
	    tmp = tmp->next, i++) {
		trtmp = RB_FIND(uvm_tree, &map->rbhead, tmp);
		if (trtmp != tmp) {
			printf("%s: lookup: %d: %p - %p: %p\n",
			    name, i, tmp, trtmp,
			    RB_PARENT(tmp, rb_entry));
			goto error;
		}
	}

	return (0);
 error:
#ifdef	DDB
	/* handy breakpoint location for error case */
	__asm(".globl treesanity_label\ntreesanity_label:");
#endif
	return (-1);
}
#endif
d490 1
a490 1
 * uvm_mapent_alloc: allocate a map entry
d492 7
d500 4
a503 6
struct vm_map_entry *
uvm_mapent_alloc(struct vm_map *map, int flags)
{
	struct vm_map_entry *me, *ne;
	int s, i;
	int pool_flags;
d505 7
a511 3
	pool_flags = PR_WAITOK;
	if (flags & UVM_FLAG_TRYLOCK)
		pool_flags = PR_NOWAIT;
d513 5
a517 38
	if (map->flags & VM_MAP_INTRSAFE || cold) {
		s = splvm();
		simple_lock(&uvm.kentry_lock);
		me = uvm.kentry_free;
		if (me == NULL) {
			ne = km_alloc(PAGE_SIZE, &kv_page, &kp_dirty,
			    &kd_nowait);
			if (ne == NULL)
				panic("uvm_mapent_alloc: cannot allocate map "
				    "entry");
			for (i = 0;
			    i < PAGE_SIZE / sizeof(struct vm_map_entry) - 1;
			    i++)
				ne[i].next = &ne[i + 1];
			ne[i].next = NULL;
			me = ne;
			if (ratecheck(&uvm_kmapent_last_warn_time,
			    &uvm_kmapent_warn_rate))
				printf("uvm_mapent_alloc: out of static "
				    "map entries\n");
		}
		uvm.kentry_free = me->next;
		uvmexp.kmapent++;
		simple_unlock(&uvm.kentry_lock);
		splx(s);
		me->flags = UVM_MAP_STATIC;
	} else if (map == kernel_map) {
		splassert(IPL_NONE);
		me = pool_get(&uvm_map_entry_kmem_pool, pool_flags);
		if (me == NULL)
			goto out;
		me->flags = UVM_MAP_KMEM;
	} else {
		splassert(IPL_NONE);
		me = pool_get(&uvm_map_entry_pool, pool_flags);
		if (me == NULL)
			goto out;
		me->flags = 0;
d520 1
a520 2
out:
	return(me);
d524 1
a524 3
 * uvm_mapent_free: free map entry
 *
 * => XXX: static pool for kernel map?
d526 2
a527 3

void
uvm_mapent_free(struct vm_map_entry *me)
d529 1
a529 1
	int s;
d531 8
a538 14
	if (me->flags & UVM_MAP_STATIC) {
		s = splvm();
		simple_lock(&uvm.kentry_lock);
		me->next = uvm.kentry_free;
		uvm.kentry_free = me;
		uvmexp.kmapent--;
		simple_unlock(&uvm.kentry_lock);
		splx(s);
	} else if (me->flags & UVM_MAP_KMEM) {
		splassert(IPL_NONE);
		pool_put(&uvm_map_entry_kmem_pool, me);
	} else {
		splassert(IPL_NONE);
		pool_put(&uvm_map_entry_pool, me);
d540 1
d544 8
a551 1
 * uvm_mapent_copy: copy a map entry, preserving flags
d553 2
a554 3

void
uvm_mapent_copy(struct vm_map_entry *src, struct vm_map_entry *dst)
d556 1
a556 2
	memcpy(dst, src, ((char *)&src->uvm_map_entry_stop_copy) -
	    ((char *)src));
d558 2
d562 8
a569 1
 * uvm_map_entry_unwire: unwire a map entry
d571 1
a571 1
 * => map should be locked by caller
d573 4
a576 2
void
uvm_map_entry_unwire(struct vm_map *map, struct vm_map_entry *entry)
d578 97
d676 8
a683 3
	entry->wired_count = 0;
	uvm_fault_unwire_locked(map, entry->start, entry->end);
}
d685 19
d705 1
a705 9
/*
 * wrapper for calling amap_ref()
 */
void
uvm_map_reference_amap(struct vm_map_entry *entry, int flags)
{
	amap_ref(entry->aref.ar_amap, entry->aref.ar_pageoff,
	    (entry->end - entry->start) >> PAGE_SHIFT, flags);
}
d707 2
d710 55
a764 8
/*
 * wrapper for calling amap_unref() 
 */
void
uvm_map_unreference_amap(struct vm_map_entry *entry, int flags)
{
	amap_unref(entry->aref.ar_amap, entry->aref.ar_pageoff,
	    (entry->end - entry->start) >> PAGE_SHIFT, flags);
a766 1

d768 5
a772 2
 * uvm_map_init: init mapping system at boot time.   note that we allocate
 * and init the static pool of structs vm_map_entry for the kernel here.
d774 4
a777 3

void
uvm_map_init(void)
d779 3
a781 2
	static struct vm_map_entry kernel_map_entry[MAX_KMAPENT];
	int lcv;
d784 1
a784 1
	 * set up static pool of kernel map entries ...
d786 4
d791 24
a814 6
	simple_lock_init(&uvm.kentry_lock);
	uvm.kentry_free = NULL;
	for (lcv = 0 ; lcv < MAX_KMAPENT ; lcv++) {
		kernel_map_entry[lcv].next = uvm.kentry_free;
		uvm.kentry_free = &kernel_map_entry[lcv];
	}
d817 1
a817 1
	 * initialize the map-related pools.
d819 6
a824 33
	pool_init(&uvm_vmspace_pool, sizeof(struct vmspace),
	    0, 0, 0, "vmsppl", &pool_allocator_nointr);
	pool_init(&uvm_map_entry_pool, sizeof(struct vm_map_entry),
	    0, 0, 0, "vmmpepl", &pool_allocator_nointr);
	pool_init(&uvm_map_entry_kmem_pool, sizeof(struct vm_map_entry),
	    0, 0, 0, "vmmpekpl", NULL);
	pool_sethiwat(&uvm_map_entry_pool, 8192);
}

/*
 * clippers
 */

/*
 * uvm_map_clip_start: ensure that the entry begins at or after
 *	the starting address, if it doesn't we split the entry.
 * 
 * => caller should use UVM_MAP_CLIP_START macro rather than calling
 *    this directly
 * => map must be locked by caller
 */

void
uvm_map_clip_start(struct vm_map *map, struct vm_map_entry *entry,
    vaddr_t start)
{
	struct vm_map_entry *new_entry;
	vaddr_t new_adj;

	/* uvm_map_simplify_entry(map, entry); */ /* XXX */

	uvm_tree_sanity(map, "clip_start entry");

d826 3
a828 3
	 * Split off the front portion.  note that we must insert the new
	 * entry BEFORE this one, so that this entry has the specified
	 * starting address.
d830 6
d837 8
a844 2
	new_entry = uvm_mapent_alloc(map, 0);
	uvm_mapent_copy(entry, new_entry); /* entry -> new_entry */
d846 5
a850 23
	new_entry->end = start; 
	new_adj = start - new_entry->start;
	if (entry->object.uvm_obj)
		entry->offset += new_adj;	/* shift start over */

	/* Does not change order for the RB tree */
	entry->start = start;

	if (new_entry->aref.ar_amap) {
		amap_splitref(&new_entry->aref, &entry->aref, new_adj);
	}

	uvm_map_entry_link(map, entry->prev, new_entry);

	if (UVM_ET_ISSUBMAP(entry)) {
		/* ... unlikely to happen, but play it safe */
		 uvm_map_reference(new_entry->object.sub_map);
	} else {
		if (UVM_ET_ISOBJ(entry) && 
		    entry->object.uvm_obj->pgops &&
		    entry->object.uvm_obj->pgops->pgo_reference)
			entry->object.uvm_obj->pgops->pgo_reference(
			    entry->object.uvm_obj);
d853 1
a853 1
	uvm_tree_sanity(map, "clip_start leave");
d857 2
a858 6
 * uvm_map_clip_end: ensure that the entry ends at or before
 *	the ending address, if it doesn't we split the reference
 * 
 * => caller should use UVM_MAP_CLIP_END macro rather than calling
 *    this directly
 * => map must be locked by caller
d860 4
a863 3

void
uvm_map_clip_end(struct vm_map *map, struct vm_map_entry *entry, vaddr_t end)
d865 2
a866 2
	struct vm_map_entry *new_entry;
	vaddr_t new_adj; /* #bytes we move start forward */
a867 1
	uvm_tree_sanity(map, "clip_end entry");
d869 2
a870 2
	 *	Create a new entry and insert it
	 *	AFTER the specified entry
d872 2
d875 4
a878 2
	new_entry = uvm_mapent_alloc(map, 0);
	uvm_mapent_copy(entry, new_entry); /* entry -> new_entry */
d880 7
a886 9
	new_entry->start = entry->end = end;
	new_adj = end - entry->start;
	if (new_entry->object.uvm_obj)
		new_entry->offset += new_adj;

	if (entry->aref.ar_amap)
		amap_splitref(&entry->aref, &new_entry->aref, new_adj);
	
	uvm_rb_fixup(map, entry);
d888 1
a888 13
	uvm_map_entry_link(map, entry, new_entry);

	if (UVM_ET_ISSUBMAP(entry)) {
		/* ... unlikely to happen, but play it safe */
	 	uvm_map_reference(new_entry->object.sub_map);
	} else {
		if (UVM_ET_ISOBJ(entry) &&
		    entry->object.uvm_obj->pgops &&
		    entry->object.uvm_obj->pgops->pgo_reference)
			entry->object.uvm_obj->pgops->pgo_reference(
			    entry->object.uvm_obj);
	}
	uvm_tree_sanity(map, "clip_end leave");
a890 4

/*
 *   M A P   -   m a i n   e n t r y   p o i n t
 */
d892 1
a892 1
 * uvm_map: establish a valid mapping in a map
d894 3
a896 5
 * => assume startp is page aligned.
 * => assume size is a multiple of PAGE_SIZE.
 * => assume sys_mmap provides enough of a "hint" to have us skip
 *	over text/data/bss area.
 * => map must be unlocked (we will lock it)
d898 8
a905 13
 *	 [1] <NULL,uoffset> 		== uoffset is a hint for PMAP_PREFER
 *	 [2] <NULL,UVM_UNKNOWN_OFFSET>	== don't PMAP_PREFER
 *	 [3] <uobj,uoffset>		== normal mapping
 *	 [4] <uobj,UVM_UNKNOWN_OFFSET>	== uvm_map finds offset based on VA
 *	
 *    case [4] is for kernel mappings where we don't know the offset until
 *    we've found a virtual address.   note that kernel object offsets are
 *    always relative to vm_map_min(kernel_map).
 *
 * => if `align' is non-zero, we try to align the virtual address to
 *	the specified alignment.  this is only a hint; if we can't
 *	do it, the address will be unaligned.  this is provided as
 *	a mechanism for large pages.
d907 2
a908 1
 * => XXXCDC: need way to map in external amap?
a909 1

d911 12
a922 55
uvm_map_p(struct vm_map *map, vaddr_t *startp, vsize_t size,
    struct uvm_object *uobj, voff_t uoffset, vsize_t align, uvm_flag_t flags,
    struct proc *p)
{
	struct vm_map_entry *prev_entry, *new_entry;
#ifdef KVA_GUARDPAGES
	struct vm_map_entry *guard_entry;
#endif
	vm_prot_t prot = UVM_PROTECTION(flags), maxprot =
	    UVM_MAXPROTECTION(flags);
	vm_inherit_t inherit = UVM_INHERIT(flags);
	int advice = UVM_ADVICE(flags);
	int error;

	/*
	 * Holes are incompatible with other types of mappings.
	 */
	if (flags & UVM_FLAG_HOLE) {
		KASSERT(uobj == NULL && (flags & UVM_FLAG_FIXED) != 0 &&
		    (flags & (UVM_FLAG_OVERLAY | UVM_FLAG_COPYONW)) == 0);
	}

#ifdef KVA_GUARDPAGES
	if (map == kernel_map && !(flags & UVM_FLAG_FIXED)) {
		/*
		 * kva_guardstart is initialized to the start of the kernelmap
		 * and cycles through the kva space.
		 * This way we should have a long time between re-use of kva.
		 */
		static vaddr_t kva_guardstart = 0;
		if (kva_guardstart == 0) {
			kva_guardstart = vm_map_min(map);
			printf("uvm_map: kva guard pages enabled: %p\n",
			    kva_guardstart);
		}
		size += PAGE_SIZE;	/* Add guard page at the end. */
		/*
		 * Try to fully exhaust kva prior to wrap-around.
		 * (This may eat your ram!)
		 */
		if (VM_MAX_KERNEL_ADDRESS - kva_guardstart < size) {
			static int wrap_counter = 0;
			printf("uvm_map: kva guard page wrap-around %d\n",
			    ++wrap_counter);
			kva_guardstart = vm_map_min(map);
		}
		*startp = kva_guardstart;
		/*
		 * Prepare for next round.
		 */
		kva_guardstart += size;
	}
#endif

	uvm_tree_sanity(map, "map entry");
d930 12
a941 2
	 * step 0: sanity check of protection code
	 */
d943 12
a954 2
	if ((prot & maxprot) != prot) {
		return (EACCES);
d958 1
a958 1
	 * step 1: figure out where to put new VM range
d960 9
d970 6
a975 4
	if (vm_map_lock_try(map) == FALSE) {
		if (flags & UVM_FLAG_TRYLOCK)
			return (EFAULT);
		vm_map_lock(map); /* could sleep here */
a976 17
	if ((prev_entry = uvm_map_findspace(map, *startp, size, startp, 
	    uobj, uoffset, align, flags)) == NULL) {
		vm_map_unlock(map);
		return (ENOMEM);
	}

#ifdef PMAP_GROWKERNEL
	{
		/*
		 * If the kernel pmap can't map the requested space,
		 * then allocate more resources for it.
		 */
		if (map == kernel_map && !(flags & UVM_FLAG_FIXED) &&
		    uvm_maxkaddr < (*startp + size))
			uvm_maxkaddr = pmap_growkernel(*startp + size);
	}
#endif
d979 1
a979 13
	 * if uobj is null, then uoffset is either a VAC hint for PMAP_PREFER
	 * [typically from uvm_map_reserve] or it is UVM_UNKNOWN_OFFSET.   in 
	 * either case we want to zero it  before storing it in the map entry 
	 * (because it looks strange and confusing when debugging...)
	 * 
	 * if uobj is not null 
	 *   if uoffset is not UVM_UNKNOWN_OFFSET then we have a normal mapping
	 *      and we do not need to change uoffset.
	 *   if uoffset is UVM_UNKNOWN_OFFSET then we need to find the offset
	 *      now (based on the starting address of the map).   this case is
	 *      for kernel object mappings where we don't know the offset until
	 *      the virtual address is found (with uvm_map_findspace).   the
	 *      offset is the distance we are from the start of the map.
d981 2
a982 9

	if (uobj == NULL) {
		uoffset = 0;
	} else {
		if (uoffset == UVM_UNKNOWN_OFFSET) {
			KASSERT(UVM_OBJ_IS_KERN_OBJECT(uobj));
			uoffset = *startp - vm_map_min(kernel_map);
		}
	}
d985 1
a985 4
	 * step 2: try and insert in map by extending previous entry, if
	 * possible
	 * XXX: we don't try and pull back the next entry.   might be useful
	 * for a stack, but we are currently allocating our stack in advance.
d987 2
d990 5
a994 3
	if ((flags & UVM_FLAG_NOMERGE) == 0 && 
	    prev_entry->end == *startp && prev_entry != &map->header &&
	    prev_entry->object.uvm_obj == uobj) {
d996 9
a1004 3
		if (uobj && prev_entry->offset +
		    (prev_entry->end - prev_entry->start) != uoffset)
			goto step3;
d1006 9
a1014 14
		if (UVM_ET_ISSUBMAP(prev_entry))
			goto step3;

		if (prev_entry->protection != prot || 
		    prev_entry->max_protection != maxprot)
			goto step3;

		if (prev_entry->inheritance != inherit ||
		    prev_entry->advice != advice)
			goto step3;

		/* wiring status must match (new area is unwired) */
		if (VM_MAPENT_ISWIRED(prev_entry))
			goto step3; 
d1017 1
a1017 3
		 * can't extend a shared amap.  note: no need to lock amap to 
		 * look at refs since we don't care about its exact value.
		 * if it is one (i.e. we have only reference) it will stay there
d1019 3
a1021 4

		if (prev_entry->aref.ar_amap &&
		    amap_refs(prev_entry->aref.ar_amap) != 1) {
			goto step3;
d1023 17
d1042 1
a1042 2
		 * Only merge kernel mappings, but keep track
		 * of how much we skipped.
d1044 2
a1045 3
		if (map != kernel_map && map != kmem_map) {
			goto step3;
		}
d1047 3
a1049 4
		if (prev_entry->aref.ar_amap) {
			error = amap_extend(prev_entry, size);
			if (error)
				goto step3;
d1052 3
d1056 1
a1056 2
		 * drop our reference to uobj since we are extending a reference
		 * that we already have (the ref count can not drop to zero).
d1058 2
d1061 2
a1062 2
		if (uobj && uobj->pgops->pgo_detach)
			uobj->pgops->pgo_detach(uobj);
d1064 5
a1068 5
		prev_entry->end += size;
		uvm_rb_fixup(map, prev_entry);
		map->size += size;
		if (p && uobj == NULL)
			p->p_vmspace->vm_dused += atop(size);
d1070 3
a1072 1
		uvm_tree_sanity(map, "map leave 2");
d1074 3
a1076 2
		vm_map_unlock(map);
		return (0);
d1078 2
a1079 2
	}
step3:
d1082 1
a1082 1
	 * step 3: allocate new entry and link it in
d1084 4
d1089 5
a1093 9
#ifdef KVA_GUARDPAGES
	if (map == kernel_map && !(flags & UVM_FLAG_FIXED))
		size -= PAGE_SIZE;
#endif

	new_entry = uvm_mapent_alloc(map, flags);
	if (new_entry == NULL) {
		vm_map_unlock(map);
		return (ENOMEM);
a1094 9
	new_entry->start = *startp;
	new_entry->end = new_entry->start + size;
	new_entry->object.uvm_obj = uobj;
	new_entry->offset = uoffset;

	if (uobj) 
		new_entry->etype = UVM_ET_OBJ;
	else
		new_entry->etype = 0;
d1096 21
d1118 1
a1118 1
		new_entry->etype |= UVM_ET_COPYONWRITE;
d1120 1
a1120 1
			new_entry->etype |= UVM_ET_NEEDSCOPY;
a1121 8
	if (flags & UVM_FLAG_HOLE)
		new_entry->etype |= UVM_ET_HOLE;

	new_entry->protection = prot;
	new_entry->max_protection = maxprot;
	new_entry->inheritance = inherit;
	new_entry->wired_count = 0;
	new_entry->advice = advice;
d1123 4
a1126 12
		/*
		 * to_add: for BSS we overallocate a little since we
		 * are likely to extend
		 */
		vaddr_t to_add = (flags & UVM_FLAG_AMAPPAD) ? 
			UVM_AMAP_CHUNK << PAGE_SHIFT : 0;
		struct vm_amap *amap = amap_alloc(size, to_add, M_WAITOK);
		new_entry->aref.ar_pageoff = 0;
		new_entry->aref.ar_amap = amap;
	} else {
		new_entry->aref.ar_pageoff = 0;
		new_entry->aref.ar_amap = NULL;
a1128 16
	uvm_map_entry_link(map, prev_entry, new_entry);

	map->size += size;
	if (p && uobj == NULL)
		p->p_vmspace->vm_dused += atop(size);


	/*
	 *      Update the free space hint
	 */

	if ((map->first_free == prev_entry) &&
	    (prev_entry->end >= new_entry->start))
		map->first_free = new_entry;

#ifdef KVA_GUARDPAGES
d1130 1
a1130 1
	 * Create the guard entry.
d1132 5
a1136 18
	if (map == kernel_map && !(flags & UVM_FLAG_FIXED)) {
		guard_entry = uvm_mapent_alloc(map, flags);
		if (guard_entry != NULL) {
			guard_entry->start = new_entry->end;
			guard_entry->end = guard_entry->start + PAGE_SIZE;
			guard_entry->object.uvm_obj = uobj;
			guard_entry->offset = uoffset;
			guard_entry->etype = MAP_ET_KVAGUARD;
			guard_entry->protection = prot;
			guard_entry->max_protection = maxprot;
			guard_entry->inheritance = inherit;
			guard_entry->wired_count = 0;
			guard_entry->advice = advice;
			guard_entry->aref.ar_pageoff = 0;
			guard_entry->aref.ar_amap = NULL;
			uvm_map_entry_link(map, new_entry, guard_entry);
			map->size += PAGE_SIZE;
			kva_guardpages++;
a1138 1
#endif
d1140 10
a1149 1
	uvm_tree_sanity(map, "map leave");
d1151 1
d1153 10
a1162 1
	return (0);
d1166 1
a1166 5
 * uvm_map_lookup_entry: find map entry at or before an address
 *
 * => map must at least be read-locked by caller
 * => entry is returned in "entry"
 * => return value is true if address is in the returned entry
d1168 3
a1170 4

boolean_t
uvm_map_lookup_entry(struct vm_map *map, vaddr_t address,
    struct vm_map_entry **entry)
d1172 1
a1172 3
	struct vm_map_entry *cur;
	struct vm_map_entry *last;
	int			use_tree = 0;
d1175 1
a1175 2
	 * start looking either from the head of the
	 * list, or from the hint.
d1177 2
d1180 5
a1184 3
	simple_lock(&map->hint_lock);
	cur = map->hint;
	simple_unlock(&map->hint_lock);
d1186 5
a1190 2
	if (cur == &map->header)
		cur = cur->next;
d1192 8
a1199 17
	if (address >= cur->start) {
	    	/*
		 * go from hint to end of list.
		 *
		 * but first, make a quick check to see if
		 * we are already looking at the entry we
		 * want (which is usually the case).
		 * note also that we don't need to save the hint
		 * here... it is the same hint (unless we are
		 * at the header, in which case the hint didn't
		 * buy us anything anyway).
		 */
		last = &map->header;
		if ((cur != last) && (cur->end > address)) {
			*entry = cur;
			return (TRUE);
		}
d1201 8
a1208 9
		if (map->nentries > 30)
			use_tree = 1;
	} else {
	    	/*
		 * go from start to hint, *inclusively*
		 */
		last = cur->next;
		cur = map->header.next;
		use_tree = 1;
d1211 16
a1226 1
	uvm_tree_sanity(map, __func__);
d1228 11
a1238 3
	if (use_tree) {
		struct vm_map_entry *prev = &map->header;
		cur = RB_ROOT(&map->rbhead);
d1240 8
a1247 18
		/*
		 * Simple lookup in the tree.  Happens when the hint is
		 * invalid, or nentries reach a threshold.
		 */
		while (cur) {
			if (address >= cur->start) {
				if (address < cur->end) {
					*entry = cur;
					SAVE_HINT(map, map->hint, cur);
					return (TRUE);
				}
				prev = cur;
				cur = RB_RIGHT(cur, rb_entry);
			} else
				cur = RB_LEFT(cur, rb_entry);
		}
		*entry = prev;
		return (FALSE);
d1251 2
a1252 1
	 * search linearly
d1255 3
a1257 7
	while (cur != last) {
		if (cur->end > address) {
			if (address >= cur->start) {
			    	/*
				 * save this lookup for future
				 * hints, and return
				 */
d1259 9
a1267 8
				*entry = cur;
				SAVE_HINT(map, map->hint, cur);
				return (TRUE);
			}
			break;
		}
		cur = cur->next;
	}
d1269 2
a1270 3
	*entry = cur->prev;
	SAVE_HINT(map, map->hint, *entry);
	return (FALSE);
d1274 5
a1278 3
 * Checks if address pointed to by phint fits into the empty
 * space before the vm_map_entry after.  Takes alignment and
 * offset into consideration.
d1280 6
d1287 9
a1295 6
int
uvm_map_spacefits(struct vm_map *map, vaddr_t *phint, vsize_t length,
    struct vm_map_entry *after, voff_t uoffset, vsize_t align)
{
	vaddr_t hint = *phint;
	vaddr_t end;
a1296 1
#ifdef PMAP_PREFER
d1298 5
a1302 2
	 * push hint forward as needed to avoid VAC alias problems.
	 * we only do this if a valid offset is specified.
d1304 10
a1313 15
	if (uoffset != UVM_UNKNOWN_OFFSET)
		hint = PMAP_PREFER(uoffset, hint);
#endif
	if (align != 0)
		if ((hint & (align - 1)) != 0)
			hint = roundup(hint, align);
	*phint = hint;

	end = hint + length;
	if (end > map->max_offset || end < hint)
		return (FALSE);
	if (after != NULL && after != &map->header && after->start < end)
		return (FALSE);
	
	return (TRUE);
d1317 1
a1317 2
 * uvm_map_pie: return a random load address for a PIE executable
 * properly aligned.
d1319 4
d1324 9
a1332 3
#ifndef VM_PIE_MAX_ADDR
#define VM_PIE_MAX_ADDR (VM_MAXUSER_ADDRESS / 4)
#endif
d1334 11
a1344 3
#ifndef VM_PIE_MIN_ADDR
#define VM_PIE_MIN_ADDR VM_MIN_ADDRESS
#endif
d1346 7
a1352 3
#ifndef VM_PIE_MIN_ALIGN
#define VM_PIE_MIN_ALIGN PAGE_SIZE
#endif
d1354 10
a1363 2
vaddr_t
uvm_map_pie(vaddr_t align)
d1365 3
a1367 1
	vaddr_t addr, space, min;
d1369 10
a1378 1
	align = MAX(align, VM_PIE_MIN_ALIGN);
d1380 2
a1381 2
	/* round up to next alignment */
	min = (VM_PIE_MIN_ADDR + align - 1) & ~(align - 1);
d1383 11
a1393 2
	if (align >= VM_PIE_MAX_ADDR || min >= VM_PIE_MAX_ADDR)
		return (align);
d1395 4
a1398 2
	space = (VM_PIE_MAX_ADDR - min) / align;
	space = MIN(space, (u_int32_t)-1);
d1400 8
a1407 2
	addr = (vaddr_t)arc4random_uniform((u_int32_t)space) * align;
	addr += min;
d1409 6
a1414 2
	return (addr);
}
d1416 17
a1432 8
/*
 * uvm_map_hint: return the beginning of the best area suitable for
 * creating a new mapping with "prot" protection.
 */
vaddr_t
uvm_map_hint1(struct proc *p, vm_prot_t prot, int skipheap)
{
	vaddr_t addr;
a1433 1
#ifdef __i386__
d1435 1
a1435 2
	 * If executable skip first two pages, otherwise start
	 * after data + heap region.
d1437 5
a1441 15
	if ((prot & VM_PROT_EXECUTE) &&
	    ((vaddr_t)p->p_vmspace->vm_daddr >= I386_MAX_EXE_ADDR)) {
		addr = (PAGE_SIZE*2) +
		    (arc4random() & (I386_MAX_EXE_ADDR / 2 - 1));
		return (round_page(addr));
	}
#endif
	/* start malloc/mmap after the brk */
	addr = (vaddr_t)p->p_vmspace->vm_daddr;
	if (skipheap)
		addr += BRKSIZ;
#if !defined(__vax__)
	addr += arc4random() & (MIN((256 * 1024 * 1024), BRKSIZ) - 1);
#endif
	return (round_page(addr));
d1445 1
a1445 10
 * uvm_map_findspace: find "length" sized space in "map".
 *
 * => "hint" is a hint about where we want it, unless FINDSPACE_FIXED is
 *	set (in which case we insist on using "hint").
 * => "result" is VA returned
 * => uobj/uoffset are to be used to handle VAC alignment, if required
 * => if `align' is non-zero, we attempt to align to that value.
 * => caller must at least have read-locked map
 * => returns NULL on failure, or pointer to prev. map entry if success
 * => note this is a cross between the old vm_map_findspace and vm_map_find
a1446 1

d1448 1
a1448 3
uvm_map_findspace(struct vm_map *map, vaddr_t hint, vsize_t length,
    vaddr_t *result, struct uvm_object *uobj, voff_t uoffset, vsize_t align,
    int flags)
d1450 3
a1452 3
	struct vm_map_entry *entry, *next, *tmp;
	struct vm_map_entry *child, *prev = NULL;
	vaddr_t end, orig_hint;
d1454 3
a1456 2
	KASSERT((align & (align - 1)) == 0);
	KASSERT((flags & UVM_FLAG_FIXED) == 0 || align == 0);
d1458 20
a1477 12
	uvm_tree_sanity(map, "map_findspace entry");

	/*
	 * remember the original hint.  if we are aligning, then we
	 * may have to try again with no alignment constraint if
	 * we fail the first time.
	 */

	orig_hint = hint;
	if (hint < map->min_offset) {	/* check ranges ... */
		if (flags & UVM_FLAG_FIXED) {
			return(NULL);
d1479 11
a1489 14
		hint = map->min_offset;
	}
	if (hint > map->max_offset) {
		return(NULL);
	}

	/*
	 * Look for the first possible address; if there's already
	 * something at this address, we have to start after it.
	 */

	if ((flags & UVM_FLAG_FIXED) == 0 && hint == map->min_offset) {
		if ((entry = map->first_free) != &map->header) 
			hint = entry->end;
d1491 5
a1495 8
		if (uvm_map_lookup_entry(map, hint, &tmp)) {
			/* "hint" address already in use ... */
			if (flags & UVM_FLAG_FIXED) {
				return(NULL);
			}
			hint = tmp->end;
		}
		entry = tmp;
d1498 4
a1501 88
	if (flags & UVM_FLAG_FIXED) {
		end = hint + length;
		if (end > map->max_offset || end < hint) {
			goto error;
		}
		next = entry->next;
		if (next == &map->header || next->start >= end)
			goto found;
		return(NULL); /* only one shot at it ... */
	}

	/* Try to find the space in the red-black tree */

	/* Check slot before any entry */
	if (uvm_map_spacefits(map, &hint, length, entry->next, uoffset, align))
		goto found;
	
	/* If there is not enough space in the whole tree, we fail */
	tmp = RB_ROOT(&map->rbhead);
	if (tmp == NULL || tmp->space < length)
		goto error;

	/* Find an entry close to hint that has enough space */
	for (; tmp;) {
		if (tmp->end >= hint &&
		    (prev == NULL || tmp->end < prev->end)) {
			if (tmp->ownspace >= length)
				prev = tmp;
			else if ((child = RB_RIGHT(tmp, rb_entry)) != NULL &&
			    child->space >= length)
				prev = tmp;
		}
		if (tmp->end < hint)
			child = RB_RIGHT(tmp, rb_entry);
		else if (tmp->end > hint)
			child = RB_LEFT(tmp, rb_entry);
		else {
			if (tmp->ownspace >= length)
				break;
			child = RB_RIGHT(tmp, rb_entry);
		}
		if (child == NULL || child->space < length)
			break;
		tmp = child;
	}
	
	if (tmp != NULL && hint < tmp->end + tmp->ownspace) {
		/* 
		 * Check if the entry that we found satifies the
		 * space requirement
		 */
		if (hint < tmp->end)
			hint = tmp->end;
		if (uvm_map_spacefits(map, &hint, length, tmp->next, uoffset,
			align)) {
			entry = tmp;
			goto found;
		} else if (tmp->ownspace >= length)
			goto listsearch;
	}
	if (prev == NULL)
		goto error;
	
	hint = prev->end;
	if (uvm_map_spacefits(map, &hint, length, prev->next, uoffset,
		align)) {
		entry = prev;
		goto found;
	} else if (prev->ownspace >= length)
		goto listsearch;
	
	tmp = RB_RIGHT(prev, rb_entry);
	for (;;) {
		KASSERT(tmp && tmp->space >= length);
		child = RB_LEFT(tmp, rb_entry);
		if (child && child->space >= length) {
			tmp = child;
			continue;
		}
		if (tmp->ownspace >= length)
			break;
		tmp = RB_RIGHT(tmp, rb_entry);
	}
	
	hint = tmp->end;
	if (uvm_map_spacefits(map, &hint, length, tmp->next, uoffset, align)) {
		entry = tmp;
		goto found;
d1504 3
a1506 19
	/* 
	 * The tree fails to find an entry because of offset or alignment
	 * restrictions.  Search the list instead.
	 */
 listsearch:
	/*
	 * Look through the rest of the map, trying to fit a new region in
	 * the gap between existing regions, or after the very last region.
	 * note: entry->end   = base VA of current gap,
	 *	 next->start  = VA of end of current gap
	 */
	for (;; hint = (entry = next)->end) {
		/*
		 * Find the end of the proposed new region.  Be sure we didn't
		 * go beyond the end of the map, or wrap around the address;
		 * if so, we lose.  Otherwise, if this is the last entry, or
		 * if the proposed new region fits before the next entry, we
		 * win.
		 */
d1508 9
a1516 27
#ifdef PMAP_PREFER
		/*
		 * push hint forward as needed to avoid VAC alias problems.
		 * we only do this if a valid offset is specified.
		 */
		if (uoffset != UVM_UNKNOWN_OFFSET)
			hint = PMAP_PREFER(uoffset, hint);
#endif
		if (align != 0) {
			if ((hint & (align - 1)) != 0)
				hint = roundup(hint, align);
			/*
			 * XXX Should we PMAP_PREFER() here again?
			 */
		}
		end = hint + length;
		if (end > map->max_offset || end < hint) {
			goto error;
		}
		next = entry->next;
		if (next == &map->header || next->start >= end)
			break;
	}
 found:
	SAVE_HINT(map, map->hint, entry);
	*result = hint;
	return (entry);
d1518 14
a1531 4
 error:
	if (align != 0) {
		return (uvm_map_findspace(map, orig_hint,
			    length, result, uobj, uoffset, 0, flags));
a1532 1
	return (NULL);
d1536 7
a1542 1
 *   U N M A P   -   m a i n   e n t r y   p o i n t
d1544 8
d1554 2
a1555 4
 * uvm_unmap: remove mappings from a vm_map (from "start" up to "stop")
 *
 * => caller must check alignment and size 
 * => map must be unlocked (we will lock it)
d1557 14
a1570 2
void
uvm_unmap_p(vm_map_t map, vaddr_t start, vaddr_t end, struct proc *p)
d1572 1
a1572 1
	vm_map_entry_t dead_entries;
d1574 1
a1574 7
	/*
	 * work now done by helper functions.   wipe the pmap's and then
	 * detach from the dead entries...
	 */
	vm_map_lock(map);
	uvm_unmap_remove(map, start, end, &dead_entries, p, FALSE);
	vm_map_unlock(map);
d1576 2
a1577 2
	if (dead_entries != NULL)
		uvm_unmap_detach(dead_entries, 0);
d1579 2
a1580 1
}
d1582 2
d1585 2
a1586 3
/*
 *   U N M A P   -   m a i n   h e l p e r   f u n c t i o n s
 */
d1588 2
a1589 8
/*
 * uvm_unmap_remove: remove mappings from a vm_map (from "start" up to "stop")
 *
 * => caller must check alignment and size 
 * => map must be locked by caller
 * => we return a list of map entries that we've remove from the map
 *    in "entry_list"
 */
d1592 1
a1592 2
uvm_unmap_remove(struct vm_map *map, vaddr_t start, vaddr_t end,
    struct vm_map_entry **entry_list, struct proc *p, boolean_t remove_holes)
d1594 1
a1594 2
	struct vm_map_entry *entry, *first_entry, *next;
	vaddr_t len;
d1596 6
a1601 1
	VM_MAP_RANGE_CHECK(map, start, end);
d1603 2
a1604 6
	uvm_tree_sanity(map, "unmap_remove entry");

	if ((map->flags & VM_MAP_INTRSAFE) == 0)
		splassert(IPL_NONE);
	else
		splassert(IPL_VM);
d1606 24
d1631 1
a1631 1
	 * find first entry
d1633 3
a1635 9
	if (uvm_map_lookup_entry(map, start, &first_entry) == TRUE) {
		/* clip and go... */
		entry = first_entry;
		UVM_MAP_CLIP_START(map, entry, start);
		/* critical!  prevents stale hint */
		SAVE_HINT(map, entry, entry->prev);

	} else {
		entry = first_entry->next;
d1638 7
a1644 3
	/*
	 * Save the free space hint
	 */
d1646 9
a1654 2
	if (map->first_free->start >= start)
		map->first_free = entry->prev;
d1656 6
d1663 1
a1663 12
	 * note: we now re-use first_entry for a different task.  we remove
	 * a number of map entries from the map and save them in a linked
	 * list headed by "first_entry".  once we remove them from the map
	 * the caller should unlock the map and drop the references to the
	 * backing objects [c.f. uvm_unmap_detach].  the object is to
	 * separate unmapping from reference dropping.  why?
	 *   [1] the map has to be locked for unmapping
	 *   [2] the map need not be locked for reference dropping
	 *   [3] dropping references may trigger pager I/O, and if we hit
	 *       a pager that does synchronous I/O we may have to wait for it.
	 *   [4] we would like all waiting for I/O to occur with maps unlocked
	 *       so that we don't block other threads.  
d1665 4
a1668 2
	first_entry = NULL;
	*entry_list = NULL;		/* to be safe */
d1671 1
a1671 4
	 * break up the area into map entry sized regions and unmap.  note 
	 * that all mappings have to be removed before we can even consider
	 * dropping references to amaps or VM objects (otherwise we could end
	 * up with a mapping to a page on the free list which would be very bad)
d1673 11
a1683 8

	while ((entry != &map->header) && (entry->start < end)) {

		UVM_MAP_CLIP_END(map, entry, end); 
		next = entry->next;
		len = entry->end - entry->start;
		if (p && entry->object.uvm_obj == NULL)
			p->p_vmspace->vm_dused -= atop(len);
d1686 27
a1712 2
		 * unwire before removing addresses from the pmap; otherwise
		 * unwiring will put the entries back into the pmap (XXX).
a1714 3
		if (VM_MAPENT_ISWIRED(entry))
			uvm_map_entry_unwire(map, entry);

d1716 3
a1718 2
		 * special case: handle mappings to anonymous kernel objects.
		 * we want to free these pages right away...
d1720 4
a1723 71
#ifdef KVA_GUARDPAGES
		if (map == kernel_map && entry->etype & MAP_ET_KVAGUARD) {
			entry->etype &= ~MAP_ET_KVAGUARD;
			kva_guardpages--;
		} else		/* (code continues across line-break) */
#endif
		if (UVM_ET_ISHOLE(entry)) {
			if (!remove_holes) {
				entry = next;
				continue;
			}
		} else if (map->flags & VM_MAP_INTRSAFE) {
			uvm_km_pgremove_intrsafe(entry->start, entry->end);
			pmap_kremove(entry->start, len);
		} else if (UVM_ET_ISOBJ(entry) &&
		    UVM_OBJ_IS_KERN_OBJECT(entry->object.uvm_obj)) {
			KASSERT(vm_map_pmap(map) == pmap_kernel());

			/*
			 * note: kernel object mappings are currently used in
			 * two ways:
			 *  [1] "normal" mappings of pages in the kernel object
			 *  [2] uvm_km_valloc'd allocations in which we
			 *      pmap_enter in some non-kernel-object page
			 *      (e.g. vmapbuf).
			 *
			 * for case [1], we need to remove the mapping from
			 * the pmap and then remove the page from the kernel
			 * object (because, once pages in a kernel object are
			 * unmapped they are no longer needed, unlike, say,
			 * a vnode where you might want the data to persist
			 * until flushed out of a queue).
			 *
			 * for case [2], we need to remove the mapping from
			 * the pmap.  there shouldn't be any pages at the
			 * specified offset in the kernel object [but it
			 * doesn't hurt to call uvm_km_pgremove just to be
			 * safe?]
			 *
			 * uvm_km_pgremove currently does the following: 
			 *   for pages in the kernel object in range: 
			 *     - drops the swap slot
			 *     - uvm_pagefree the page
			 *
			 * note there is version of uvm_km_pgremove() that
			 * is used for "intrsafe" objects.
			 */

			/*
			 * remove mappings from pmap and drop the pages
			 * from the object.  offsets are always relative
			 * to vm_map_min(kernel_map).
			 */
			pmap_remove(pmap_kernel(), entry->start, entry->end);
			uvm_km_pgremove(entry->object.uvm_obj,
			    entry->start - vm_map_min(kernel_map),
			    entry->end - vm_map_min(kernel_map));

			/*
			 * null out kernel_object reference, we've just
			 * dropped it
			 */
			entry->etype &= ~UVM_ET_OBJ;
			entry->object.uvm_obj = NULL;	/* to be safe */

		} else {
			/*
		 	 * remove mappings the standard way.
		 	 */
			pmap_remove(map->pmap, entry->start, entry->end);
		}
d1726 2
a1727 2
		 * remove entry from map and put it on our list of entries 
		 * that we've nuked.  then go do next entry.
d1729 3
a1731 15
		/* critical! prevents stale hint */
		SAVE_HINT(map, entry, entry->prev);

		uvm_map_entry_unlink(map, entry);
		map->size -= len;
		entry->next = first_entry;
		first_entry = entry;
		entry = next;		/* next entry, please */
	}
#ifdef KVA_GUARDPAGES
	/*
	 * entry points at the map-entry after the last-removed map-entry.
	 */
	if (map == kernel_map && entry != &map->header &&
	    entry->etype & MAP_ET_KVAGUARD && entry->start == end) {
d1733 1
a1733 2
		 * Removed range is followed by guard page;
		 * remove that guard page now (or it will stay forever).
d1735 1
a1735 8
		entry->etype &= ~MAP_ET_KVAGUARD;
		kva_guardpages--;

		uvm_map_entry_unlink(map, entry);
		map->size -= len;
		entry->next = first_entry;
		first_entry = entry;
		entry = next;		/* next entry, please */
a1736 14
#endif
	/* if ((map->flags & VM_MAP_DYING) == 0) { */
		pmap_update(vm_map_pmap(map));
	/* } */


	uvm_tree_sanity(map, "unmap_remove leave");

	/*
	 * now we've cleaned up the map and are ready for the caller to drop
	 * references to the mapped objects.  
	 */

	*entry_list = first_entry;
d1740 1
a1740 1
 * uvm_unmap_detach: drop references in a chain of map entries
d1742 3
a1744 1
 * => we will free the map entries as we traverse the list.
a1745 1

d1747 3
a1749 1
uvm_unmap_detach(struct vm_map_entry *first_entry, int flags)
d1751 41
a1791 1
	struct vm_map_entry *next_entry;
d1793 2
a1794 2
	while (first_entry) {
		KASSERT(!VM_MAPENT_ISWIRED(first_entry));
d1797 1
a1797 1
		 * drop reference to amap, if we've got one
d1799 8
a1806 3

		if (first_entry->aref.ar_amap)
			uvm_map_unreference_amap(first_entry, flags);
d1809 1
a1809 1
		 * drop reference to our backing object, if we've got one
d1811 2
d1814 10
a1823 8
		if (UVM_ET_ISSUBMAP(first_entry)) {
			/* ... unlikely to happen, but play it safe */
			uvm_map_deallocate(first_entry->object.sub_map);
		} else {
			if (UVM_ET_ISOBJ(first_entry) &&
			    first_entry->object.uvm_obj->pgops->pgo_detach)
				first_entry->object.uvm_obj->pgops->
				    pgo_detach(first_entry->object.uvm_obj);
d1825 4
a1828 4

		next_entry = first_entry->next;
		uvm_mapent_free(first_entry);
		first_entry = next_entry;
d1830 1
d1834 1
a1834 5
 *   E X T R A C T I O N   F U N C T I O N S
 */

/* 
 * uvm_map_reserve: reserve space in a vm_map for future use.
d1836 1
a1836 5
 * => we reserve space in a map by putting a dummy map entry in the 
 *    map (dummy means obj=NULL, amap=NULL, prot=VM_PROT_NONE)
 * => map should be unlocked (we will write lock it)
 * => we return true if we were able to reserve space
 * => XXXCDC: should be inline?
d1838 3
a1840 4

int
uvm_map_reserve(struct vm_map *map, vsize_t size, vaddr_t offset,
    vsize_t align, vaddr_t *raddr)
d1842 1
d1844 5
a1848 3
	size = round_page(size);
	if (*raddr < vm_map_min(map))
		*raddr = vm_map_min(map);                /* hint */
d1850 3
a1852 11
	/*
	 * reserve some virtual space.
	 */

	if (uvm_map(map, raddr, size, NULL, offset, 0,
	    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE, UVM_INH_NONE,
	    UVM_ADV_RANDOM, UVM_FLAG_NOMERGE)) != 0) {
		return (FALSE);
	}     

	return (TRUE);
d1856 1
a1856 2
 * uvm_map_replace: replace a reserved (blank) area of memory with 
 * real mappings.
d1858 2
a1859 5
 * => caller must WRITE-LOCK the map 
 * => we return TRUE if replacement was a success
 * => we expect the newents chain to have nnewents entries on it and
 *    we expect newents->prev to point to the last entry on the list
 * => note newents is allowed to be NULL
a1860 1

d1862 3
a1864 2
uvm_map_replace(struct vm_map *map, vaddr_t start, vaddr_t end,
    struct vm_map_entry *newents, int nnewents)
d1866 5
a1870 3
	struct vm_map_entry *oldent, *last;

	uvm_tree_sanity(map, "map_replace entry");
d1873 21
a1893 2
	 * first find the blank map entry at the specified address
	 */
d1895 14
a1908 2
	if (!uvm_map_lookup_entry(map, start, &oldent)) {
		return(FALSE);
d1912 1
a1912 1
	 * check to make sure we have a proper blank entry
d1914 11
d1926 2
a1927 3
	if (oldent->start != start || oldent->end != end || 
	    oldent->object.uvm_obj != NULL || oldent->aref.ar_amap != NULL) {
		return (FALSE);
d1930 8
d1939 2
a1940 30
	/*
	 * sanity check the newents chain
	 */
	{
		struct vm_map_entry *tmpent = newents;
		int nent = 0;
		vaddr_t cur = start;

		while (tmpent) {
			nent++;
			if (tmpent->start < cur)
				panic("uvm_map_replace1");
			if (tmpent->start > tmpent->end || tmpent->end > end) {
		printf("tmpent->start=0x%lx, tmpent->end=0x%lx, end=0x%lx\n",
			    tmpent->start, tmpent->end, end);
				panic("uvm_map_replace2");
			}
			cur = tmpent->end;
			if (tmpent->next) {
				if (tmpent->next->prev != tmpent)
					panic("uvm_map_replace3");
			} else {
				if (newents->prev != tmpent)
					panic("uvm_map_replace4");
			}
			tmpent = tmpent->next;
		}
		if (nent != nnewents)
			panic("uvm_map_replace5");
	}
d1943 8
a1950 4
	/*
	 * map entry is a valid blank!   replace it.   (this does all the
	 * work of map entry link/unlink...).
	 */
d1952 6
a1957 2
	if (newents) {
		last = newents->prev;		/* we expect this */
d1959 7
a1965 7
		/* critical: flush stale hints out of map */
		SAVE_HINT(map, map->hint, newents);
		if (map->first_free == oldent)
			map->first_free = last;

		last->next = oldent->next;
		last->next->prev = last;
d1967 1
a1967 17
		/* Fix RB tree */
		uvm_rb_remove(map, oldent);

		newents->prev = oldent->prev;
		newents->prev->next = newents;
		map->nentries = map->nentries + (nnewents - 1);

		/* Fixup the RB tree */
		{
			int i;
			struct vm_map_entry *tmp;

			tmp = newents;
			for (i = 0; i < nnewents && tmp; i++) {
				uvm_rb_insert(map, tmp);
				tmp = tmp->next;
			}
a1968 1
	} else {
d1970 3
a1972 7
		/* critical: flush stale hints out of map */
		SAVE_HINT(map, map->hint, oldent->prev);
		if (map->first_free == oldent)
			map->first_free = oldent->prev;

		/* NULL list of new entries: just remove the old one */
		uvm_map_entry_unlink(map, oldent);
a1974 3

	uvm_tree_sanity(map, "map_replace leave");

d1976 1
a1976 1
	 * now we can free the old blank entry, unlock the map and return.
d1978 12
a1989 3

	uvm_mapent_free(oldent);
	return(TRUE);
d1993 5
a1997 2
 * uvm_map_extract: extract a mapping from a map and put it somewhere
 *	(maybe removing the old mapping)
d1999 2
a2000 12
 * => maps should be unlocked (we will write lock them)
 * => returns 0 on success, error code otherwise
 * => start must be page aligned
 * => len must be page sized
 * => flags:
 *      UVM_EXTRACT_REMOVE: remove mappings from srcmap
 *      UVM_EXTRACT_CONTIG: abort if unmapped area (advisory only)
 *      UVM_EXTRACT_QREF: for a temporary extraction do quick obj refs
 *      UVM_EXTRACT_FIXPROT: set prot to maxprot as we go
 *    >>>NOTE: if you set REMOVE, you are not allowed to use CONTIG or QREF!<<<
 *    >>>NOTE: QREF's must be unmapped via the QREF path, thus should only
 *             be used from within the kernel in a kernel level map <<<
a2001 1

d2003 2
a2004 2
uvm_map_extract(struct vm_map *srcmap, vaddr_t start, vsize_t len,
    struct vm_map *dstmap, vaddr_t *dstaddrp, int flags)
d2006 2
a2007 6
	vaddr_t dstaddr, end, newend, oldoffset, fudge, orig_fudge,
	    oldstart;
	struct vm_map_entry *chain, *endchain, *entry, *orig_entry, *newentry;
	struct vm_map_entry *deadentry, *oldentry;
	vsize_t elen;
	int nchain, error, copy_ok;
d2009 2
a2010 2
	uvm_tree_sanity(srcmap, "map_extract src enter");
	uvm_tree_sanity(dstmap, "map_extract dst enter");
d2012 6
a2017 5
	/*
	 * step 0: sanity check: start must be on a page boundary, length
	 * must be page sized.  can't ask for CONTIG/QREF if you asked for
	 * REMOVE.
	 */
d2019 3
a2021 3
	KASSERT((start & PAGE_MASK) == 0 && (len & PAGE_MASK) == 0);
	KASSERT((flags & UVM_EXTRACT_REMOVE) == 0 ||
		(flags & (UVM_EXTRACT_CONTIG|UVM_EXTRACT_QREF)) == 0);
d2024 4
a2027 1
	 * step 1: reserve space in the target map for the extracted area
d2029 8
a2036 5

	dstaddr = vm_map_min(dstmap);
	if (uvm_map_reserve(dstmap, len, start, 0, &dstaddr) == FALSE)
		return(ENOMEM);
	*dstaddrp = dstaddr;	/* pass address back to caller */
d2039 1
a2039 3
	 * step 2: setup for the extraction process loop by init'ing the 
	 * map entry chain, locking src map, and looking up the first useful
	 * entry in the map.
d2041 4
a2044 12

	end = start + len;
	newend = dstaddr + len;
	chain = endchain = NULL;
	nchain = 0;
	vm_map_lock(srcmap);

	if (uvm_map_lookup_entry(srcmap, start, &entry)) {

		/* "start" is within an entry */
		if (flags & UVM_EXTRACT_QREF) {

d2046 2
a2047 5
			 * for quick references we don't clip the entry, so
			 * the entry may map space "before" the starting
			 * virtual address... this is the "fudge" factor
			 * (which can be non-zero only the first time
			 * through the "while" loop in step 3).
d2049 2
a2050 12

			fudge = start - entry->start;
		} else {

			/*
			 * normal reference: we clip the map to fit (thus
			 * fudge is zero)
			 */

			UVM_MAP_CLIP_START(srcmap, entry, start);
			SAVE_HINT(srcmap, srcmap->hint, entry->prev);
			fudge = 0;
d2052 1
a2052 1
	} else {
d2054 9
a2062 2
		/* "start" is not within an entry ... skip to next entry */
		if (flags & UVM_EXTRACT_CONTIG) {
d2064 1
a2064 1
			goto bad;    /* definite hole here ... */
d2066 2
a2067 8

		entry = entry->next;
		fudge = 0;
	}

	/* save values from srcmap for step 6 */
	orig_entry = entry;
	orig_fudge = fudge;
d2070 1
a2070 2
	 * step 3: now start looping through the map entries, extracting
	 * as we go.
d2072 17
d2090 2
a2091 1
	while (entry->start < end && entry != &srcmap->header) {
d2093 21
a2113 3
		/* if we are not doing a quick reference, clip it */
		if ((flags & UVM_EXTRACT_QREF) == 0)
			UVM_MAP_CLIP_END(srcmap, entry, end);
d2115 4
a2118 11
		/* clear needs_copy (allow chunking) */
		if (UVM_ET_ISNEEDSCOPY(entry)) {
			if (fudge)
				oldstart = entry->start;
			else
				oldstart = 0;	/* XXX: gcc */
			amap_copy(srcmap, entry, M_NOWAIT, TRUE, start, end);
			if (UVM_ET_ISNEEDSCOPY(entry)) {  /* failed? */
				error = ENOMEM;
				goto bad;
			}
d2120 12
a2131 6
			/* amap_copy could clip (during chunk)!  update fudge */
			if (fudge) {
				fudge = fudge - (entry->start - oldstart);
				orig_fudge = fudge;
			}
		}
d2133 2
a2134 2
		/* calculate the offset of this from "start" */
		oldoffset = (entry->start + fudge) - start;
d2136 3
a2138 6
		/* allocate a new map entry */
		newentry = uvm_mapent_alloc(dstmap, flags);
		if (newentry == NULL) {
			error = ENOMEM;
			goto bad;
		}
d2140 4
a2143 33
		/* set up new map entry */
		newentry->next = NULL;
		newentry->prev = endchain;
		newentry->start = dstaddr + oldoffset;
		newentry->end =
		    newentry->start + (entry->end - (entry->start + fudge));
		if (newentry->end > newend || newentry->end < newentry->start)
			newentry->end = newend;
		newentry->object.uvm_obj = entry->object.uvm_obj;
		if (newentry->object.uvm_obj) {
			if (newentry->object.uvm_obj->pgops->pgo_reference)
				newentry->object.uvm_obj->pgops->
				    pgo_reference(newentry->object.uvm_obj);
			newentry->offset = entry->offset + fudge;
		} else {
			newentry->offset = 0;
		}
		newentry->etype = entry->etype;
		newentry->protection = (flags & UVM_EXTRACT_FIXPROT) ? 
			entry->max_protection : entry->protection; 
		newentry->max_protection = entry->max_protection;
		newentry->inheritance = entry->inheritance;
		newentry->wired_count = 0;
		newentry->aref.ar_amap = entry->aref.ar_amap;
		if (newentry->aref.ar_amap) {
			newentry->aref.ar_pageoff =
			    entry->aref.ar_pageoff + (fudge >> PAGE_SHIFT);
			uvm_map_reference_amap(newentry, AMAP_SHARED |
			    ((flags & UVM_EXTRACT_QREF) ? AMAP_REFALL : 0));
		} else {
			newentry->aref.ar_pageoff = 0;
		}
		newentry->advice = entry->advice;
d2145 5
a2149 18
		/* now link it on the chain */
		nchain++;
		if (endchain == NULL) {
			chain = endchain = newentry;
		} else {
			endchain->next = newentry;
			endchain = newentry;
		}

		/* end of 'while' loop! */
		if ((flags & UVM_EXTRACT_CONTIG) && entry->end < end && 
		    (entry->next == &srcmap->header ||
		    entry->next->start != entry->end)) {
			error = EINVAL;
			goto bad;
		}
		entry = entry->next;
		fudge = 0;
d2153 2
a2154 1
	 * step 4: close off chain (in format expected by uvm_map_replace)
d2156 4
d2161 2
a2162 2
	if (chain)
		chain->prev = endchain;
d2164 4
a2167 6
	/*
	 * step 5: attempt to lock the dest map so we can pmap_copy.
	 * note usage of copy_ok: 
	 *   1 => dstmap locked, pmap_copy ok, and we "replace" here (step 5)
	 *   0 => dstmap unlocked, NO pmap_copy, and we will "replace" in step 7
	 */
d2169 6
a2174 12
	if (srcmap == dstmap || vm_map_lock_try(dstmap) == TRUE) {
		copy_ok = 1;
		if (!uvm_map_replace(dstmap, dstaddr, dstaddr+len, chain,
		    nchain)) {
			if (srcmap != dstmap)
				vm_map_unlock(dstmap);
			error = EIO;
			goto bad;
		}
	} else {
		copy_ok = 0;
		/* replace defered until step 7 */
d2176 1
d2179 1
a2179 4
	 * step 6: traverse the srcmap a second time to do the following:
	 *  - if we got a lock on the dstmap do pmap_copy
	 *  - if UVM_EXTRACT_REMOVE remove the entries
	 * we make use of orig_entry and orig_fudge (saved in step 2)
d2181 3
d2185 9
a2193 1
	if (copy_ok || (flags & UVM_EXTRACT_REMOVE)) {
d2195 3
a2197 6
		/* purge possible stale hints from srcmap */
		if (flags & UVM_EXTRACT_REMOVE) {
			SAVE_HINT(srcmap, srcmap->hint, orig_entry->prev);
			if (srcmap->first_free->start >= start)
				srcmap->first_free = orig_entry->prev;
		}
d2199 12
a2210 3
		entry = orig_entry;
		fudge = orig_fudge;
		deadentry = NULL;	/* for UVM_EXTRACT_REMOVE */
d2212 5
a2216 9
		while (entry->start < end && entry != &srcmap->header) {
			if (copy_ok) {
				oldoffset = (entry->start + fudge) - start;
				elen = MIN(end, entry->end) -
				    (entry->start + fudge);
				pmap_copy(dstmap->pmap, srcmap->pmap,
				    dstaddr + oldoffset, elen,
				    entry->start + fudge);
			}
d2218 10
a2227 13
			/* we advance "entry" in the following if statement */
			if (flags & UVM_EXTRACT_REMOVE) {
				pmap_remove(srcmap->pmap, entry->start, 
						entry->end);
        			oldentry = entry;	/* save entry */
        			entry = entry->next;	/* advance */
				uvm_map_entry_unlink(srcmap, oldentry);
							/* add to dead list */
				oldentry->next = deadentry;
				deadentry = oldentry;
      			} else {
        			entry = entry->next;		/* advance */
			}
d2229 6
a2234 4
			/* end of 'while' loop */
			fudge = 0;
		}
		pmap_update(srcmap->pmap);
d2236 4
d2241 1
a2241 2
		 * unlock dstmap.  we will dispose of deadentry in
		 * step 7 if needed
d2243 14
d2258 20
a2277 2
		if (copy_ok && srcmap != dstmap)
			vm_map_unlock(dstmap);
d2279 33
a2312 2
	else
		deadentry = NULL; /* XXX: gcc */
d2315 1
a2315 3
	 * step 7: we are done with the source map, unlock.   if copy_ok
	 * is 0 then we have not replaced the dummy mapping in dstmap yet
	 * and we need to do so now.
d2317 5
a2321 16

	vm_map_unlock(srcmap);
	if ((flags & UVM_EXTRACT_REMOVE) && deadentry)
		uvm_unmap_detach(deadentry, 0);   /* dispose of old entries */

	/* now do the replacement if we didn't do it in step 5 */
	if (copy_ok == 0) {
		vm_map_lock(dstmap);
		error = uvm_map_replace(dstmap, dstaddr, dstaddr+len, chain,
		    nchain);
		vm_map_unlock(dstmap);

		if (error == FALSE) {
			error = EIO;
			goto bad2;
		}
d2323 2
a2324 5

	uvm_tree_sanity(srcmap, "map_extract src leave");
	uvm_tree_sanity(dstmap, "map_extract dst leave");

	return(0);
d2327 28
a2354 8
	 * bad: failure recovery
	 */
bad:
	vm_map_unlock(srcmap);
bad2:			/* src already unlocked */
	if (chain)
		uvm_unmap_detach(chain,
		    (flags & UVM_EXTRACT_QREF) ? AMAP_REFALL : 0);
d2356 2
a2357 2
	uvm_tree_sanity(srcmap, "map_extract src err leave");
	uvm_tree_sanity(dstmap, "map_extract dst err leave");
d2359 11
a2369 2
	uvm_unmap(dstmap, dstaddr, dstaddr+len);   /* ??? */
	return(error);
a2371 2
/* end of extraction functions */

d2373 1
a2373 1
 * uvm_map_submap: punch down part of a map into a submap
d2375 1
a2375 12
 * => only the kernel_map is allowed to be submapped
 * => the purpose of submapping is to break up the locking granularity
 *	of a larger map
 * => the range specified must have been mapped previously with a uvm_map()
 *	call [with uobj==NULL] to create a blank map entry in the main map.
 *	[And it had better still be blank!]
 * => maps which contain submaps should never be copied or forked.
 * => to remove a submap, use uvm_unmap() on the main map 
 *	and then uvm_map_deallocate() the submap.
 * => main map must be unlocked.
 * => submap must have been init'd and have a zero reference count.
 *	[need not be locked as we don't actually reference it]
d2377 2
a2378 4

int
uvm_map_submap(struct vm_map *map, vaddr_t start, vaddr_t end,
    struct vm_map *submap)
d2380 1
a2380 13
	struct vm_map_entry *entry;
	int result;

	vm_map_lock(map);

	VM_MAP_RANGE_CHECK(map, start, end);

	if (uvm_map_lookup_entry(map, start, &entry)) {
		UVM_MAP_CLIP_START(map, entry, start);
		UVM_MAP_CLIP_END(map, entry, end);		/* to be safe */
	} else {
		entry = NULL;
	}
d2382 1
a2382 14
	if (entry != NULL && 
	    entry->start == start && entry->end == end &&
	    entry->object.uvm_obj == NULL && entry->aref.ar_amap == NULL &&
	    !UVM_ET_ISCOPYONWRITE(entry) && !UVM_ET_ISNEEDSCOPY(entry)) {
		entry->etype |= UVM_ET_SUBMAP;
		entry->object.sub_map = submap;
		entry->offset = 0;
		uvm_map_reference(submap);
		result = 0;
	} else {
		result = EINVAL;
	}
	vm_map_unlock(map);
	return(result);
a2384 1

d2386 1
a2386 1
 * uvm_map_protect: change map protection
d2388 3
a2390 2
 * => set_max means set max_protection.
 * => map must be unlocked.
d2392 3
a2394 8

#define MASK(entry)     (UVM_ET_ISCOPYONWRITE(entry) ? \
			 ~VM_PROT_WRITE : VM_PROT_ALL)
#define max(a,b)        ((a) > (b) ? (a) : (b))

int
uvm_map_protect(struct vm_map *map, vaddr_t start, vaddr_t end,
    vm_prot_t new_prot, boolean_t set_max)
d2396 2
a2397 6
	struct vm_map_entry *current, *entry;
	int error = 0;

	vm_map_lock(map);

	VM_MAP_RANGE_CHECK(map, start, end);
d2399 34
a2432 2
	if (uvm_map_lookup_entry(map, start, &entry)) {
		UVM_MAP_CLIP_START(map, entry, start);
d2434 17
a2450 1
		entry = entry->next;
d2454 6
a2459 1
	 * make a first pass to check for protection violations.
d2461 9
d2471 5
a2475 12
	current = entry;
	while ((current != &map->header) && (current->start < end)) {
		if (UVM_ET_ISSUBMAP(current)) {
			error = EINVAL;
			goto out;
		}
		if ((new_prot & current->max_protection) != new_prot) {
			error = EACCES;
			goto out;
		}
		current = current->next;
	}
d2477 5
a2481 1
	/* go back and fix up protections (no need to clip this time). */
d2483 2
a2484 1
	current = entry;
d2486 9
a2494 2
	while ((current != &map->header) && (current->start < end)) {
		vm_prot_t old_prot;
d2496 10
a2505 1
		UVM_MAP_CLIP_END(map, current, end);
d2507 13
a2519 6
		old_prot = current->protection;
		if (set_max)
			current->protection =
			    (current->max_protection = new_prot) & old_prot;
		else
			current->protection = new_prot;
d2522 1
a2522 2
		 * update physical map if necessary.  worry about copy-on-write 
		 * here -- CHECK THIS XXX
d2524 2
d2527 5
a2531 8
		if (current->protection != old_prot) {
			/* update pmap! */
			if ((current->protection & MASK(entry)) == PROT_NONE &&
			    VM_MAPENT_ISWIRED(entry))
				current->wired_count--;
			pmap_protect(map->pmap, current->start, current->end,
			    current->protection & MASK(entry));
		}
d2534 2
a2535 3
		 * If the map is configured to lock any future mappings,
		 * wire this entry now if the old protection was VM_PROT_NONE
		 * and the new protection is not VM_PROT_NONE.
d2537 2
d2540 7
a2546 21
		if ((map->flags & VM_MAP_WIREFUTURE) != 0 &&
		    VM_MAPENT_ISWIRED(entry) == 0 &&
		    old_prot == VM_PROT_NONE &&
		    new_prot != VM_PROT_NONE) {
			if (uvm_map_pageable(map, entry->start, entry->end,
			    FALSE, UVM_LK_ENTER|UVM_LK_EXIT) != 0) {
				/*
				 * If locking the entry fails, remember the
				 * error if it's the first one.  Note we
				 * still continue setting the protection in
				 * the map, but will return the resource
				 * shortage condition regardless.
				 *
				 * XXX Ignore what the actual error is,
				 * XXX just call it a resource shortage
				 * XXX so that it doesn't get confused
				 * XXX what uvm_map_protect() itself would
				 * XXX normally return.
				 */
				error = ENOMEM;
			}
d2549 8
a2556 1
		current = current->next;
d2558 1
a2558 5
	pmap_update(map->pmap);

 out:
	vm_map_unlock(map);
	return (error);
d2561 2
a2562 14
#undef  max
#undef  MASK

/* 
 * uvm_map_inherit: set inheritance code for range of addrs in map.
 *
 * => map must be unlocked
 * => note that the inherit code is used during a "fork".  see fork
 *	code for details.
 */

int
uvm_map_inherit(struct vm_map *map, vaddr_t start, vaddr_t end,
    vm_inherit_t new_inheritance)
d2564 2
a2565 10
	struct vm_map_entry *entry;

	switch (new_inheritance) {
	case MAP_INHERIT_NONE:
	case MAP_INHERIT_COPY:
	case MAP_INHERIT_SHARE:
		break;
	default:
		return (EINVAL);
	}
d2567 4
a2570 8
	vm_map_lock(map);
	
	VM_MAP_RANGE_CHECK(map, start, end);
	
	if (uvm_map_lookup_entry(map, start, &entry)) {
		UVM_MAP_CLIP_START(map, entry, start);
	} else {
		entry = entry->next;
d2573 3
a2575 5
	while ((entry != &map->header) && (entry->start < end)) {
		UVM_MAP_CLIP_END(map, entry, end);
		entry->inheritance = new_inheritance;
		entry = entry->next;
	}
d2577 1
a2577 2
	vm_map_unlock(map);
	return (0);
d2580 2
a2581 4
/* 
 * uvm_map_advice: set advice code for range of addrs in map.
 *
 * => map must be unlocked
d2583 2
a2584 3

int
uvm_map_advice(struct vm_map *map, vaddr_t start, vaddr_t end, int new_advice)
d2586 5
a2590 1
	struct vm_map_entry *entry;
d2592 2
a2593 6
	switch (new_advice) {
	case MADV_NORMAL:
	case MADV_RANDOM:
	case MADV_SEQUENTIAL:
		/* nothing special here */
		break;
d2595 7
a2601 10
	default:
		return (EINVAL);
	}
	vm_map_lock(map);
	VM_MAP_RANGE_CHECK(map, start, end);
	if (uvm_map_lookup_entry(map, start, &entry)) {
		UVM_MAP_CLIP_START(map, entry, start);
	} else {
		entry = entry->next;
	}
d2603 2
a2604 3
	/*
	 * XXXJRT: disallow holes?
	 */
d2606 15
a2620 2
	while ((entry != &map->header) && (entry->start < end)) {
		UVM_MAP_CLIP_END(map, entry, end);
d2622 6
a2627 2
		entry->advice = new_advice;
		entry = entry->next;
d2630 9
a2638 2
	vm_map_unlock(map);
	return (0);
d2641 2
d2644 2
a2645 11
 * uvm_map_pageable: sets the pageability of a range in a map.
 *
 * => wires map entries.  should not be used for transient page locking.
 *	for that, use uvm_fault_wire()/uvm_fault_unwire() (see uvm_vslock()).
 * => regions sepcified as not pageable require lock-down (wired) memory
 *	and page tables.
 * => map must never be read-locked
 * => if islocked is TRUE, map is already write-locked
 * => we always unlock the map, since we must downgrade to a read-lock
 *	to call uvm_fault_wire()
 * => XXXCDC: check this and try and clean it up.
d2647 2
a2648 4

int
uvm_map_pageable(struct vm_map *map, vaddr_t start, vaddr_t end,
    boolean_t new_pageable, int lockflags)
d2650 2
a2651 6
	struct vm_map_entry *entry, *start_entry, *failed_entry;
	int rv;
#ifdef DIAGNOSTIC
	u_int timestamp_save;
#endif
	KASSERT(map->flags & VM_MAP_PAGEABLE);
d2653 3
a2655 2
	if ((lockflags & UVM_LK_ENTER) == 0)
		vm_map_lock(map);
d2657 7
a2663 1
	VM_MAP_RANGE_CHECK(map, start, end);
d2665 2
a2666 6
	/* 
	 * only one pageability change may take place at one time, since
	 * uvm_fault_wire assumes it will be called only once for each
	 * wiring/unwiring.  therefore, we have to make sure we're actually
	 * changing the pageability for the entire region.  we do so before
	 * making any changes.  
d2668 7
d2676 2
a2677 3
	if (uvm_map_lookup_entry(map, start, &start_entry) == FALSE) {
		if ((lockflags & UVM_LK_EXIT) == 0)
			vm_map_unlock(map);
d2679 1
a2679 3
		return (EFAULT);
	}
	entry = start_entry;
d2681 751
a3431 2
	/* 
	 * handle wiring and unwiring separately.
d3434 1
a3434 2
	if (new_pageable) {		/* unwire */
		UVM_MAP_CLIP_START(map, entry, start);
d3436 6
a3441 3
		/*
		 * unwiring.  first ensure that the range to be unwired is
		 * really wired down and that there are no holes.  
d3443 5
d3449 27
a3475 8
		while ((entry != &map->header) && (entry->start < end)) {
			if (entry->wired_count == 0 ||
			    (entry->end < end &&
			     (entry->next == &map->header ||
			      entry->next->start > entry->end))) {
				if ((lockflags & UVM_LK_EXIT) == 0)
					vm_map_unlock(map);
				return (EINVAL);
d3477 16
a3492 1
			entry = entry->next;
d3495 3
a3497 4
		/* 
		 * POSIX 1003.1b - a single munlock call unlocks a region,
		 * regardless of the number of mlock calls made on that
		 * region.
d3500 13
a3512 6
		entry = start_entry;
		while ((entry != &map->header) && (entry->start < end)) {
			UVM_MAP_CLIP_END(map, entry, end);
			if (VM_MAPENT_ISWIRED(entry))
				uvm_map_entry_unwire(map, entry);
			entry = entry->next;
a3513 3
		if ((lockflags & UVM_LK_EXIT) == 0)
			vm_map_unlock(map);
		return (0);
d3517 37
a3553 18
	 * wire case: in two passes [XXXCDC: ugly block of code here]
	 *
	 * 1: holding the write lock, we create any anonymous maps that need
	 *    to be created.  then we clip each map entry to the region to
	 *    be wired and increment its wiring count.  
	 *
	 * 2: we downgrade to a read lock, and call uvm_fault_wire to fault
	 *    in the pages for any newly wired area (wired_count == 1).
	 *
	 *    downgrading to a read lock for uvm_fault_wire avoids a possible
	 *    deadlock with another thread that may have faulted on one of
	 *    the pages to be wired (it would mark the page busy, blocking
	 *    us, then in turn block on the map lock that we hold).  because
	 *    of problems in the recursive lock package, we cannot upgrade
	 *    to a write lock in vm_map_lookup.  thus, any actions that
	 *    require the write lock must be done beforehand.  because we
	 *    keep the read lock on the map, the copy-on-write status of the
	 *    entries we modify here cannot change.
d3556 12
a3567 2
	while ((entry != &map->header) && (entry->start < end)) {
		if (VM_MAPENT_ISWIRED(entry) == 0) { /* not already wired? */
d3569 5
a3573 7
			/*
			 * perform actions of vm_map_lookup that need the
			 * write lock on the map: create an anonymous map
			 * for a copy-on-write region, or an anonymous map
			 * for a zero-fill region.  (XXXCDC: submap case
			 * ok?)
			 */
d3575 10
a3584 9
			if (!UVM_ET_ISSUBMAP(entry)) {  /* not submap */
				if (UVM_ET_ISNEEDSCOPY(entry) && 
				    ((entry->protection & VM_PROT_WRITE) ||
				     (entry->object.uvm_obj == NULL))) {
					amap_copy(map, entry, M_WAITOK, TRUE,
					    start, end); 
					/* XXXCDC: wait OK? */
				}
			}
d3586 82
d3670 48
a3717 1
		entry->wired_count++;
d3720 1
a3720 1
		 * Check for holes 
d3722 5
d3728 71
a3798 4
		if (entry->protection == VM_PROT_NONE ||
		    (entry->end < end &&
		     (entry->next == &map->header ||
		      entry->next->start > entry->end))) {
d3800 6
a3805 4
			/*
			 * found one.  amap creation actions do not need to
			 * be undone, but the wired counts need to be restored. 
			 */
d3807 1
a3807 10
			while (entry != &map->header && entry->end > start) {
				entry->wired_count--;
				entry = entry->prev;
			}
			if ((lockflags & UVM_LK_EXIT) == 0)
				vm_map_unlock(map);
			return (EINVAL);
		}
		entry = entry->next;
	}
d3809 5
a3813 3
	/*
	 * Pass 2.
	 */
d3815 4
a3818 22
#ifdef DIAGNOSTIC
	timestamp_save = map->timestamp;
#endif
	vm_map_busy(map);
	vm_map_downgrade(map);

	rv = 0;
	entry = start_entry;
	while (entry != &map->header && entry->start < end) {
		if (entry->wired_count == 1) {
			rv = uvm_fault_wire(map, entry->start, entry->end,
			    entry->protection);
			if (rv) {
				/*
				 * wiring failed.  break out of the loop.
				 * we'll clean up the map below, once we
				 * have a write lock again.
				 */
				break;
			}
		}
		entry = entry->next;
d3821 3
a3823 1
	if (rv) {        /* failed? */
d3825 9
a3833 3
		/*
		 * Get back to an exclusive (write) lock.
		 */
d3835 8
a3842 2
		vm_map_upgrade(map);
		vm_map_unbusy(map);
d3844 6
a3849 4
#ifdef DIAGNOSTIC
		if (timestamp_save != map->timestamp)
			panic("uvm_map_pageable: stale map");
#endif
d3851 1
a3851 4
		/*
		 * first drop the wiring count on all the entries
		 * which haven't actually been wired yet.
		 */
d3853 5
a3857 5
		failed_entry = entry;
		while (entry != &map->header && entry->start < end) {
			entry->wired_count--;
			entry = entry->next;
		}
d3859 3
a3861 4
		/*
		 * now, unwire all the entries that were successfully
		 * wired above.
		 */
d3863 4
a3866 24
		entry = start_entry;
		while (entry != failed_entry) {
			entry->wired_count--;
			if (VM_MAPENT_ISWIRED(entry) == 0)
				uvm_map_entry_unwire(map, entry);
			entry = entry->next;
		}
		if ((lockflags & UVM_LK_EXIT) == 0)
			vm_map_unlock(map);
		return(rv);
	}

	/* We are holding a read lock here. */
	if ((lockflags & UVM_LK_EXIT) == 0) {
		vm_map_unbusy(map);
		vm_map_unlock_read(map);
	} else {

		/*
		 * Get back to an exclusive (write) lock.
		 */

		vm_map_upgrade(map);
		vm_map_unbusy(map);
d3869 1
d3874 2
a3875 2
 * uvm_map_pageable_all: special case of uvm_map_pageable - affects
 * all mapped regions.
d3877 7
a3883 3
 * => map must not be locked.
 * => if no flags are specified, all regions are unwired.
 * => XXXJRT: has some of the same problems as uvm_map_pageable() above.
a3884 1

d3886 2
a3887 1
uvm_map_pageable_all(struct vm_map *map, int flags, vsize_t limit)
d3889 6
a3894 2
	struct vm_map_entry *entry, *failed_entry;
	vsize_t size;
a3895 3
#ifdef DIAGNOSTIC
	u_int timestamp_save;
#endif
d3897 2
a3898 3
	KASSERT(map->flags & VM_MAP_PAGEABLE);

	vm_map_lock(map);
d3901 3
a3903 1
	 * handle wiring and unwiring separately.
d3906 5
a3910 13
	if (flags == 0) {			/* unwire */
		/*
		 * POSIX 1003.1b -- munlockall unlocks all regions,
		 * regardless of how many times mlockall has been called.
		 */
		for (entry = map->header.next; entry != &map->header;
		     entry = entry->next) {
			if (VM_MAPENT_ISWIRED(entry))
				uvm_map_entry_unwire(map, entry);
		}
		vm_map_modflags(map, 0, VM_MAP_WIREFUTURE);
		vm_map_unlock(map);
		return (0);
d3912 4
a3915 4
		/*
		 * end of unwire case!
		 */
	}
d3917 2
a3918 6
	if (flags & MCL_FUTURE) {
		/*
		 * must wire all future mappings; remember this.
		 */
		vm_map_modflags(map, VM_MAP_WIREFUTURE, 0);
	}
d3920 4
a3923 7
	if ((flags & MCL_CURRENT) == 0) {
		/*
		 * no more work to do!
		 */
		vm_map_unlock(map);
		return (0);
	}
d3926 1
a3926 20
	 * wire case: in three passes [XXXCDC: ugly block of code here]
	 *
	 * 1: holding the write lock, count all pages mapped by non-wired
	 *    entries.  if this would cause us to go over our limit, we fail.
	 *
	 * 2: still holding the write lock, we create any anonymous maps that
	 *    need to be created.  then we increment its wiring count.
	 *
	 * 3: we downgrade to a read lock, and call uvm_fault_wire to fault
	 *    in the pages for any newly wired area (wired_count == 1).
	 *
	 *    downgrading to a read lock for uvm_fault_wire avoids a possible
	 *    deadlock with another thread that may have faulted on one of
	 *    the pages to be wired (it would mark the page busy, blocking
	 *    us, then in turn block on the map lock that we hold).  because
	 *    of problems in the recursive lock package, we cannot upgrade
	 *    to a write lock in vm_map_lookup.  thus, any actions that
	 *    require the write lock must be done beforehand.  because we
	 *    keep the read lock on the map, the copy-on-write status of the
	 *    entries we modify here cannot change.
d3928 1
d3930 9
a3938 5
	for (size = 0, entry = map->header.next; entry != &map->header;
	     entry = entry->next) {
		if (entry->protection != VM_PROT_NONE &&
		    VM_MAPENT_ISWIRED(entry) == 0) { /* not already wired? */
			size += entry->end - entry->start;
d3941 3
a3943 12

	if (atop(size) + uvmexp.wired > uvmexp.wiredmax) {
		vm_map_unlock(map);
		return (ENOMEM);		/* XXX overloaded */
	}

	/* XXX non-pmap_wired_count case must be handled by caller */
#ifdef pmap_wired_count
	if (limit != 0 &&
	    (size + ptoa(pmap_wired_count(vm_map_pmap(map))) > limit)) {
		vm_map_unlock(map);
		return (ENOMEM);		/* XXX overloaded */
a3944 1
#endif
d3947 5
a3951 1
	 * Pass 2.
d3953 5
a3957 6

	for (entry = map->header.next; entry != &map->header;
	     entry = entry->next) {
		if (entry->protection == VM_PROT_NONE)
			continue;
		if (VM_MAPENT_ISWIRED(entry) == 0) { /* not already wired? */
d3959 1
a3959 5
			 * perform actions of vm_map_lookup that need the
			 * write lock on the map: create an anonymous map
			 * for a copy-on-write region, or an anonymous map
			 * for a zero-fill region.  (XXXCDC: submap case
			 * ok?)
d3961 2
a3962 9
			if (!UVM_ET_ISSUBMAP(entry)) {	/* not submap */
				if (UVM_ET_ISNEEDSCOPY(entry) && 
				    ((entry->protection & VM_PROT_WRITE) ||
				     (entry->object.uvm_obj == NULL))) {
					amap_copy(map, entry, M_WAITOK, TRUE,
					    entry->start, entry->end);
					/* XXXCDC: wait OK? */
				}
			}
a3963 1
		entry->wired_count++;
d3967 1
a3967 1
	 * Pass 3.
d3969 1
d3971 4
a3974 13
#ifdef DIAGNOSTIC
	timestamp_save = map->timestamp;
#endif
	vm_map_busy(map);
	vm_map_downgrade(map);

	for (error = 0, entry = map->header.next;
	    entry != &map->header && error == 0;
	    entry = entry->next) {
		if (entry->wired_count == 1) {
			error = uvm_fault_wire(map, entry->start, entry->end,
			     entry->protection);
		}
d3976 1
d3978 4
a3981 6
	if (error) {	/* failed? */
		/*
		 * Get back an exclusive (write) lock.
		 */
		vm_map_upgrade(map);
		vm_map_unbusy(map);
d3983 8
a3990 4
#ifdef DIAGNOSTIC
		if (timestamp_save != map->timestamp)
			panic("uvm_map_pageable_all: stale map");
#endif
d3993 1
a3993 4
		 * first drop the wiring count on all the entries
		 * which haven't actually been wired yet.
		 *
		 * Skip VM_PROT_NONE entries like we did above.
d3995 14
a4008 6
		failed_entry = entry;
		for (/* nothing */; entry != &map->header;
		     entry = entry->next) {
			if (entry->protection == VM_PROT_NONE)
				continue;
			entry->wired_count--;
d4010 3
d4015 2
a4016 4
		 * now, unwire all the entries that were successfully
		 * wired above.
		 *
		 * Skip VM_PROT_NONE entries like we did above.
d4018 14
a4031 10
		for (entry = map->header.next; entry != failed_entry;
		     entry = entry->next) {
			if (entry->protection == VM_PROT_NONE)
				continue;
			entry->wired_count--;
			if (VM_MAPENT_ISWIRED(entry))
				uvm_map_entry_unwire(map, entry);
		}
		vm_map_unlock(map);
		return (error);
d4034 10
a4043 3
	/* We are holding a read lock here. */
	vm_map_unbusy(map);
	vm_map_unlock_read(map);
d4045 1
a4045 1
	return (0);
d4069 1
a4069 2
	struct vm_map_entry *current, *entry;
	struct uvm_object *uobj;
d4073 5
a4077 3
	vaddr_t offset;
	vsize_t size;
	int rv, error, refs;
d4080 4
a4083 1
		(PGO_FREE|PGO_DEACTIVATE));
d4086 1
a4086 5
	VM_MAP_RANGE_CHECK(map, start, end);
	if (uvm_map_lookup_entry(map, start, &entry) == FALSE) {
		vm_map_unlock_read(map);
		return (EFAULT);
	}
d4091 3
a4093 3

	for (current = entry; current->start < end; current = current->next) {
		if (UVM_ET_ISSUBMAP(current)) {
d4095 1
a4095 1
			return (EINVAL);
d4097 4
a4100 2
		if (end > current->end && (current->next == &map->header ||
		    current->end != current->next->start)) {
d4102 1
a4102 1
			return (EFAULT);
d4107 7
a4113 5

	for (current = entry; current->start < end; current = current->next) {
		amap = current->aref.ar_amap;	/* top layer */
		uobj = current->object.uvm_obj;	/* bottom layer */
		KASSERT(start >= current->start);
d4117 2
a4118 4
		 *
		 *	(1) There's no amap.
		 *
		 *	(2) We're not deactivating or freeing pages.
a4119 1

d4122 2
d4125 2
a4126 3
		/* XXX for now, just in case... */
		if (amap_clean_works == 0)
			goto flush_object;
d4128 3
a4130 4
		offset = start - current->start;
		size = MIN(end, current->end) - start;
		for ( ; size != 0; size -= PAGE_SIZE, offset += PAGE_SIZE) {
			anon = amap_lookup(&current->aref, offset);
d4134 1
a4134 1
			simple_lock(&anon->an_lock);
a4142 1

a4148 1

d4152 1
a4152 1
 deactivate_it:
d4157 1
a4157 1
					continue;
a4166 1

d4171 1
a4171 1
					continue;
a4174 4
#ifdef UBC
				/* ...and deactivate the page. */
				pmap_clear_reference(pg);
#else
a4178 1
#endif
d4183 1
a4183 1
				continue;
d4188 1
a4188 1
				 * If there are multiple references to
a4190 1

d4197 1
a4197 1
					continue;
d4199 2
a4200 1
				amap_unadd(&current->aref, offset);
d4205 1
a4205 1
				continue;
d4213 3
d4220 1
a4220 1
	 	 * and don't flush if this is a copy-on-write object
a4222 3

		offset = current->offset + (start - current->start);
		size = MIN(end, current->end) - start;
d4228 3
a4230 2
			rv = uobj->pgops->pgo_flush(uobj, offset,
			    offset + size, flags);
a4235 1
		start += size;
d4237 1
d4239 1
a4239 1
	return (error); 
a4241 1

d4243 1
a4243 4
 * uvm_map_checkprot: check protection in map
 *
 * => must allow specified protection in a fully allocated region.
 * => map must be read or write locked by caller.
d4245 2
a4246 51

boolean_t
uvm_map_checkprot(struct vm_map *map, vaddr_t start, vaddr_t end,
    vm_prot_t protection)
{
	struct vm_map_entry *entry;
	struct vm_map_entry *tmp_entry;

	if (!uvm_map_lookup_entry(map, start, &tmp_entry)) {
		return(FALSE);
	}
	entry = tmp_entry;
	while (start < end) {
		if (entry == &map->header) {
			return(FALSE);
		}

		/*
		 * no holes allowed
		 */

		if (start < entry->start) {
			return(FALSE);
		}

		/*
		 * check protection associated with entry
		 */

		if ((entry->protection & protection) != protection) {
			return(FALSE);
		}

		/* go to next entry */

		start = entry->end;
		entry = entry->next;
	}
	return(TRUE);
}

/*
 * uvmspace_alloc: allocate a vmspace structure.
 *
 * - structure includes vm_map and pmap
 * - XXX: no locking on this structure
 * - refcnt set to 1, rest must be init'd by caller
 */
struct vmspace *
uvmspace_alloc(vaddr_t min, vaddr_t max, boolean_t pageable,
    boolean_t remove_holes)
d4248 4
a4251 1
	struct vmspace *vm;
d4253 4
a4256 3
	vm = pool_get(&uvm_vmspace_pool, PR_WAITOK | PR_ZERO);
	uvmspace_init(vm, NULL, min, max, pageable, remove_holes);
	return (vm);
d4260 1
a4260 1
 * uvmspace_init: initialize a vmspace structure.
d4262 5
a4266 2
 * - XXX: no locking on this structure
 * - refcnt set to 1, rest must be init'd by caller
d4269 1
a4269 2
uvmspace_init(struct vmspace *vm, struct pmap *pmap, vaddr_t min, vaddr_t max,
    boolean_t pageable, boolean_t remove_holes)
d4271 2
d4274 5
a4278 1
	uvm_map_setup(&vm->vm_map, min, max, pageable ? VM_MAP_PAGEABLE : 0);
d4280 4
a4283 5
	if (pmap)
		pmap_reference(pmap);
	else
		pmap = pmap_create();
	vm->vm_map.pmap = pmap;
d4285 4
a4288 1
	vm->vm_refcnt = 1;
d4290 2
a4291 2
	if (remove_holes)
		pmap_remove_holes(&vm->vm_map);
d4295 1
a4295 4
 * uvmspace_share: share a vmspace between two processes
 *
 * - XXX: no locking on vmspace
 * - used for vfork and threads
d4297 3
a4299 3

void
uvmspace_share(struct proc *p1, struct proc *p2)
d4301 1
a4301 2
	p2->p_vmspace = p1->p_vmspace;
	p1->p_vmspace->vm_refcnt++;
d4305 1
a4305 1
 * uvmspace_exec: the process wants to exec a new program
d4307 4
a4310 1
 * - XXX: no locking on vmspace
d4312 15
d4328 4
a4331 5
void
uvmspace_exec(struct proc *p, vaddr_t start, vaddr_t end)
{
	struct vmspace *nvm, *ovm = p->p_vmspace;
	struct vm_map *map = &ovm->vm_map;
d4333 10
a4342 1
	pmap_unuse_final(p);   /* before stack addresses go away */
d4345 4
a4348 1
	 * see if more than one process is using this vmspace...
d4350 14
d4365 13
a4377 1
	if (ovm->vm_refcnt == 1) {
d4379 8
a4386 4
		/*
		 * if p is the only process using its vmspace then we can safely
		 * recycle that vmspace for the program that is being exec'd.
		 */
d4388 2
a4389 7
#ifdef SYSVSHM
		/*
		 * SYSV SHM semantics require us to kill all segments on an exec
		 */
		if (ovm->vm_shm)
			shmexit(ovm);
#endif
d4391 3
a4393 7
		/*
		 * POSIX 1003.1b -- "lock future mappings" is revoked
		 * when a process execs another program image.
		 */
		vm_map_lock(map);
		vm_map_modflags(map, 0, VM_MAP_WIREFUTURE);
		vm_map_unlock(map);
d4395 5
a4399 4
		/*
		 * now unmap the old program
		 */
		uvm_unmap(map, map->min_offset, map->max_offset);
d4401 7
a4407 4
		/*
		 * but keep MMU holes unavailable
		 */
		pmap_remove_holes(map);
d4409 8
a4416 12
		/*
		 * resize the map
		 */
		vm_map_lock(map);
		map->min_offset = start;
		uvm_tree_sanity(map, "resize enter");
		map->max_offset = end;
		if (map->header.prev != &map->header)
			uvm_rb_fixup(map, map->header.prev);
		uvm_tree_sanity(map, "resize leave");
		vm_map_unlock(map);
	
d4418 5
a4422 1
	} else {
d4424 2
a4425 7
		/*
		 * p's vmspace is being shared, so we can't reuse it for p since
		 * it is still being used for others.   allocate a new vmspace
		 * for p
		 */
		nvm = uvmspace_alloc(start, end,
			 (map->flags & VM_MAP_PAGEABLE) ? TRUE : FALSE, TRUE);
d4427 9
a4435 3
		/*
		 * install new vmspace and drop our ref to the old one.
		 */
d4437 2
a4438 3
		pmap_deactivate(p);
		p->p_vmspace = nvm;
		pmap_activate(p);
d4440 16
a4455 1
		uvmspace_free(ovm);
d4457 8
d4468 3
a4470 1
 * uvmspace_free: free a vmspace data structure
d4472 2
a4473 1
 * - XXX: no locking on vmspace
a4474 1

d4476 2
a4477 1
uvmspace_free(struct vmspace *vm)
d4479 38
a4516 1
	struct vm_map_entry *dead_entries;
d4518 5
a4522 10
	if (--vm->vm_refcnt == 0) {
		/*
		 * lock the map, to wait out all other references to it.  delete
		 * all of the mappings and pages they hold, then call the pmap
		 * module to reclaim anything left.
		 */
#ifdef SYSVSHM
		/* Get rid of any SYSV shared memory segments. */
		if (vm->vm_shm != NULL)
			shmexit(vm);
d4524 5
a4528 12
		vm_map_lock(&vm->vm_map);
		if (vm->vm_map.nentries) {
			uvm_unmap_remove(&vm->vm_map,
			    vm->vm_map.min_offset, vm->vm_map.max_offset,
			    &dead_entries, NULL, TRUE);
			if (dead_entries != NULL)
				uvm_unmap_detach(dead_entries, 0);
		}
		pmap_destroy(vm->vm_map.pmap);
		vm->vm_map.pmap = NULL;
		pool_put(&uvm_vmspace_pool, vm);
	}
d4532 1
a4532 1
 * uvm_map_create: create map
d4534 2
a4535 2
vm_map_t
uvm_map_create(pmap_t pmap, vaddr_t min, vaddr_t max, int flags)
d4537 11
a4547 1
	vm_map_t result;
d4549 7
a4555 4
	result = malloc(sizeof(struct vm_map), M_VMMAP, M_WAITOK);
	uvm_map_setup(result, min, max, flags);
	result->pmap = pmap;
	return(result);
d4559 1
a4559 3
 * uvm_map_setup: init map
 *
 * => map must not be in service yet.
d4562 1
a4562 1
uvm_map_setup(vm_map_t map, vaddr_t min, vaddr_t max, int flags)
d4564 2
d4567 4
a4570 15
	RB_INIT(&map->rbhead);
	map->header.next = map->header.prev = &map->header;
	map->nentries = 0;
	map->size = 0;
	map->ref_count = 1;
	map->min_offset = min;
	map->max_offset = max;
	map->flags = flags;
	map->first_free = &map->header;
	map->hint = &map->header;
	map->timestamp = 0;
	rw_init(&map->lock, "vmmaplk");
	simple_lock_init(&map->ref_lock);
	simple_lock_init(&map->hint_lock);
}
d4572 2
d4575 2
d4579 1
a4579 3
 * uvm_map_reference: add reference to a map
 *
 * => map need not be locked (we use ref_lock).
d4582 2
a4583 1
uvm_map_reference(vm_map_t map)
d4585 13
a4597 3
	simple_lock(&map->ref_lock);
	map->ref_count++; 
	simple_unlock(&map->ref_lock);
d4601 1
a4601 1
 * uvm_map_deallocate: drop reference to a map
d4603 1
a4603 2
 * => caller must not lock map
 * => we will zap map if ref count goes to zero
d4606 2
a4607 1
uvm_map_deallocate(vm_map_t map)
d4609 6
a4614 1
	int c;
d4616 3
a4618 6
	simple_lock(&map->ref_lock);
	c = --map->ref_count;
	simple_unlock(&map->ref_lock);
	if (c > 0) {
		return;
	}
d4620 2
a4621 3
	/*
	 * all references gone.   unmap and free.
	 */
d4623 3
a4625 3
	uvm_unmap(map, map->min_offset, map->max_offset);
	pmap_destroy(map->pmap);
	free(map, M_VMMAP);
d4629 1
a4629 4
 *   F O R K   -   m a i n   e n t r y   p o i n t
 */
/*
 * uvmspace_fork: fork a process' main map
d4631 1
a4631 2
 * => create a new vmspace for child process from parent.
 * => parent's map must not be locked.
d4633 3
a4635 3

struct vmspace *
uvmspace_fork(struct vmspace *vm1)
d4637 2
a4638 16
	struct vmspace *vm2;
	struct vm_map *old_map = &vm1->vm_map;
	struct vm_map *new_map;
	struct vm_map_entry *old_entry;
	struct vm_map_entry *new_entry;
	pmap_t          new_pmap;
	boolean_t	protect_child;

	vm_map_lock(old_map);

	vm2 = uvmspace_alloc(old_map->min_offset, old_map->max_offset,
	    (old_map->flags & VM_MAP_PAGEABLE) ? TRUE : FALSE, FALSE);
	memcpy(&vm2->vm_startcopy, &vm1->vm_startcopy,
	(caddr_t) (vm1 + 1) - (caddr_t) &vm1->vm_startcopy);
	new_map = &vm2->vm_map;		  /* XXX */
	new_pmap = new_map->pmap;
d4640 4
a4643 1
	old_entry = old_map->header.next;
d4646 2
a4647 1
	 * go entry-by-entry
d4649 2
d4652 1
a4652 2
	while (old_entry != &old_map->header) {

d4654 1
a4654 1
		 * first, some sanity checks on the old entry
d4656 17
a4672 33
		if (UVM_ET_ISSUBMAP(old_entry))
		    panic("fork: encountered a submap during fork (illegal)");

		if (!UVM_ET_ISCOPYONWRITE(old_entry) &&
			    UVM_ET_ISNEEDSCOPY(old_entry))
	panic("fork: non-copy_on_write map entry marked needs_copy (illegal)");


		switch (old_entry->inheritance) {
		case MAP_INHERIT_NONE:
			/*
			 * drop the mapping
			 */
			break;

		case MAP_INHERIT_SHARE:
			/*
			 * share the mapping: this means we want the old and
			 * new entries to share amaps and backing objects.
			 */

			/*
			 * if the old_entry needs a new amap (due to prev fork)
			 * then we need to allocate it now so that we have
			 * something we own to share with the new_entry.   [in
			 * other words, we need to clear needs_copy]
			 */

			if (UVM_ET_ISNEEDSCOPY(old_entry)) {
				/* get our own amap, clears needs_copy */
				amap_copy(old_map, old_entry, M_WAITOK, FALSE,
				    0, 0); 
				/* XXXCDC: WAITOK??? */
d4674 2
d4677 9
a4685 3
			new_entry = uvm_mapent_alloc(new_map, 0);
			/* old_entry -> new_entry */
			uvm_mapent_copy(old_entry, new_entry);
d4687 2
a4688 2
			/* new pmap has nothing wired in it */
			new_entry->wired_count = 0;
d4690 12
d4703 4
a4706 21
			 * gain reference to object backing the map (can't
			 * be a submap, already checked this case).
			 */
			if (new_entry->aref.ar_amap)
				/* share reference */
				uvm_map_reference_amap(new_entry, AMAP_SHARED);

			if (new_entry->object.uvm_obj &&
			    new_entry->object.uvm_obj->pgops->pgo_reference)
				new_entry->object.uvm_obj->
				    pgops->pgo_reference(
				        new_entry->object.uvm_obj);

			/* insert entry at end of new_map's entry list */
			uvm_map_entry_link(new_map, new_map->header.prev,
			    new_entry);

			/* 
			 * pmap_copy the mappings: this routine is optional
			 * but if it is there it will reduce the number of
			 * page faults in the new proc.
d4708 2
d4711 16
a4726 3
			pmap_copy(new_pmap, old_map->pmap, new_entry->start,
			    (old_entry->end - old_entry->start),
			    old_entry->start);
d4728 2
a4729 1
			break;
d4731 5
a4735 1
		case MAP_INHERIT_COPY:
d4737 2
a4738 7
			/*
			 * copy-on-write the mapping (using mmap's
			 * MAP_PRIVATE semantics)
			 *
			 * allocate new_entry, adjust reference counts.  
			 * (note that new references are read-only).
			 */
d4740 16
a4755 19
			new_entry = uvm_mapent_alloc(new_map, 0);
			/* old_entry -> new_entry */
			uvm_mapent_copy(old_entry, new_entry);

			if (new_entry->aref.ar_amap)
				uvm_map_reference_amap(new_entry, 0);

			if (new_entry->object.uvm_obj &&
			    new_entry->object.uvm_obj->pgops->pgo_reference)
				new_entry->object.uvm_obj->pgops->pgo_reference
				    (new_entry->object.uvm_obj);

			/* new pmap has nothing wired in it */
			new_entry->wired_count = 0;

			new_entry->etype |=
			    (UVM_ET_COPYONWRITE|UVM_ET_NEEDSCOPY);
			uvm_map_entry_link(new_map, new_map->header.prev,
			    new_entry);
d4757 2
a4758 31
			/*
			 * the new entry will need an amap.  it will either
			 * need to be copied from the old entry or created
			 * from scratch (if the old entry does not have an
			 * amap).  can we defer this process until later
			 * (by setting "needs_copy") or do we need to copy
			 * the amap now?
			 *
			 * we must copy the amap now if any of the following
			 * conditions hold:
			 * 1. the old entry has an amap and that amap is
			 *    being shared.  this means that the old (parent)
			 *    process is sharing the amap with another 
			 *    process.  if we do not clear needs_copy here
			 *    we will end up in a situation where both the
			 *    parent and child process are referring to the
			 *    same amap with "needs_copy" set.  if the 
			 *    parent write-faults, the fault routine will
			 *    clear "needs_copy" in the parent by allocating
			 *    a new amap.   this is wrong because the 
			 *    parent is supposed to be sharing the old amap
			 *    and the new amap will break that.
			 *
			 * 2. if the old entry has an amap and a non-zero
			 *    wire count then we are going to have to call
			 *    amap_cow_now to avoid page faults in the 
			 *    parent process.   since amap_cow_now requires
			 *    "needs_copy" to be clear we might as well
			 *    clear it here as well.
			 *
			 */
d4760 10
a4769 1
			if (old_entry->aref.ar_amap != NULL) {
d4771 9
a4779 9
			  if ((amap_flags(old_entry->aref.ar_amap) & 
			       AMAP_SHARED) != 0 ||
			      VM_MAPENT_ISWIRED(old_entry)) {

			    amap_copy(new_map, new_entry, M_WAITOK, FALSE,
				      0, 0);
			    /* XXXCDC: M_WAITOK ... ok? */
			  }
			}
d4781 13
a4793 10
			/*
			 * if the parent's entry is wired down, then the
			 * parent process does not want page faults on
			 * access to that memory.  this means that we
			 * cannot do copy-on-write because we can't write
			 * protect the old entry.   in this case we
			 * resolve all copy-on-write faults now, using
			 * amap_cow_now.   note that we have already
			 * allocated any needed amap (above).
			 */
d4795 1
a4795 1
			if (VM_MAPENT_ISWIRED(old_entry)) {
d4797 11
a4807 80
			  /* 
			   * resolve all copy-on-write faults now
			   * (note that there is nothing to do if 
			   * the old mapping does not have an amap).
			   * XXX: is it worthwhile to bother with pmap_copy
			   * in this case?
			   */
			  if (old_entry->aref.ar_amap)
			    amap_cow_now(new_map, new_entry);

			} else { 

			  /*
			   * setup mappings to trigger copy-on-write faults
			   * we must write-protect the parent if it has
			   * an amap and it is not already "needs_copy"...
			   * if it is already "needs_copy" then the parent
			   * has already been write-protected by a previous
			   * fork operation.
			   *
			   * if we do not write-protect the parent, then
			   * we must be sure to write-protect the child
			   * after the pmap_copy() operation.
			   *
			   * XXX: pmap_copy should have some way of telling
			   * us that it didn't do anything so we can avoid
			   * calling pmap_protect needlessly.
			   */

			  if (old_entry->aref.ar_amap) {

			    if (!UVM_ET_ISNEEDSCOPY(old_entry)) {
			      if (old_entry->max_protection & VM_PROT_WRITE) {
				pmap_protect(old_map->pmap,
					     old_entry->start,
					     old_entry->end,
					     old_entry->protection &
					     ~VM_PROT_WRITE);
			        pmap_update(old_map->pmap);

			      }
			      old_entry->etype |= UVM_ET_NEEDSCOPY;
			    }

			    /*
			     * parent must now be write-protected
			     */
			    protect_child = FALSE;
			  } else {

			    /*
			     * we only need to protect the child if the 
			     * parent has write access.
			     */
			    if (old_entry->max_protection & VM_PROT_WRITE)
			      protect_child = TRUE;
			    else
			      protect_child = FALSE;

			  }

			  /*
			   * copy the mappings
			   * XXX: need a way to tell if this does anything
			   */

			  pmap_copy(new_pmap, old_map->pmap,
				    new_entry->start,
				    (old_entry->end - old_entry->start),
				    old_entry->start);

			  /*
			   * protect the child's mappings if necessary
			   */
			  if (protect_child) {
			    pmap_protect(new_pmap, new_entry->start,
					 new_entry->end, 
					 new_entry->protection & 
					          ~VM_PROT_WRITE);
			  }
d4809 9
a4817 4
			}
			break;
		}  /* end of switch statement */
		old_entry = old_entry->next;
d4820 8
a4827 2
	new_map->size = old_map->size;
	vm_map_unlock(old_map); 
d4829 1
a4829 4
#ifdef SYSVSHM
	if (vm1->vm_shm)
		shmfork(vm1, vm2);
#endif
d4831 6
a4836 3
#ifdef PMAP_FORK
	pmap_fork(vm1->vm_map.pmap, vm2->vm_map.pmap);
#endif
d4838 20
a4857 2
	return(vm2);    
}
d4859 6
a4864 1
#if defined(DDB)
d4866 6
a4871 3
/*
 * DDB hooks
 */
d4874 6
a4879 1
 * uvm_map_printit: actually prints the map
d4881 4
d4886 2
a4887 5
void
uvm_map_printit(struct vm_map *map, boolean_t full,
    int (*pr)(const char *, ...))
{
	struct vm_map_entry *entry;
d4889 11
a4899 7
	(*pr)("MAP %p: [0x%lx->0x%lx]\n", map, map->min_offset,map->max_offset);
	(*pr)("\t#ent=%d, sz=%u, ref=%d, version=%u, flags=0x%x\n",
	    map->nentries, map->size, map->ref_count, map->timestamp,
	    map->flags);
#ifdef pmap_resident_count
	(*pr)("\tpmap=%p(resident=%d)\n", map->pmap, 
	    pmap_resident_count(map->pmap));
d4901 1
a4901 2
	/* XXXCDC: this should be required ... */
	(*pr)("\tpmap=%p(resident=<<NOT SUPPORTED!!!>>)\n", map->pmap);
a4902 16
	if (!full)
		return;
	for (entry = map->header.next; entry != &map->header;
	    entry = entry->next) {
		(*pr)(" - %p: 0x%lx->0x%lx: obj=%p/0x%llx, amap=%p/%d\n",
		    entry, entry->start, entry->end, entry->object.uvm_obj,
		    (long long)entry->offset, entry->aref.ar_amap,
		    entry->aref.ar_pageoff);
		(*pr)(
		    "\tsubmap=%c, cow=%c, nc=%c, prot(max)=%d/%d, inh=%d, "
		    "wc=%d, adv=%d\n",
		    (entry->etype & UVM_ET_SUBMAP) ? 'T' : 'F',
		    (entry->etype & UVM_ET_COPYONWRITE) ? 'T' : 'F', 
		    (entry->etype & UVM_ET_NEEDSCOPY) ? 'T' : 'F',
		    entry->protection, entry->max_protection,
		    entry->inheritance, entry->wired_count, entry->advice);
a4903 1
} 
d4905 6
a4910 3
/*
 * uvm_object_printit: actually prints the object
 */
d4912 2
a4913 3
void
uvm_object_printit(struct uvm_object *uobj, boolean_t full,
    int (*pr)(const char *, ...))
d4915 1
a4915 2
	struct vm_page *pg;
	int cnt = 0;
d4917 8
a4924 6
	(*pr)("OBJECT %p: pgops=%p, npages=%d, ",
	    uobj, uobj->pgops, uobj->uo_npages);
	if (UVM_OBJ_IS_KERN_OBJECT(uobj))
		(*pr)("refs=<SYSTEM>\n");
	else
		(*pr)("refs=%d\n", uobj->uo_refs);
d4926 5
a4930 2
	if (!full) {
		return;
d4932 14
a4945 10
	(*pr)("  PAGES <pg,offset>:\n  ");
	RB_FOREACH(pg, uvm_objtree, &uobj->memt) {
		(*pr)("<%p,0x%llx> ", pg, (long long)pg->offset);
		if ((cnt % 3) == 2) {
			(*pr)("\n  ");
		}
		cnt++;
	}
	if ((cnt % 3) != 2) {
		(*pr)("\n");
a4946 1
} 
d4948 5
a4952 3
/*
 * uvm_page_printit: actually print the page
 */
d4954 9
a4962 4
static const char page_flagbits[] =
	"\20\1BUSY\2WANTED\3TABLED\4CLEAN\5CLEANCHK\6RELEASED\7FAKE\10RDONLY"
	"\11ZERO\15PAGER1\20FREE\21INACTIVE\22ACTIVE\24ENCRYPT\30PMAP0"
	"\31PMAP1\32PMAP2\33PMAP3";
d4965 1
a4965 2
uvm_page_printit(struct vm_page *pg, boolean_t full,
    int (*pr)(const char *, ...))
d4967 6
a4972 3
	struct vm_page *tpg;
	struct uvm_object *uobj;
	struct pglist *pgl;
d4974 9
a4982 20
	(*pr)("PAGE %p:\n", pg);
	(*pr)("  flags=%b, vers=%d, wire_count=%d, pa=0x%llx\n",
	    pg->pg_flags, page_flagbits, pg->pg_version, pg->wire_count,
	    (long long)pg->phys_addr);
	(*pr)("  uobject=%p, uanon=%p, offset=0x%llx loan_count=%d\n",
	    pg->uobject, pg->uanon, (long long)pg->offset, pg->loan_count);
#if defined(UVM_PAGE_TRKOWN)
	if (pg->pg_flags & PG_BUSY)
		(*pr)("  owning process = %d, tag=%s",
		    pg->owner, pg->owner_tag);
	else
		(*pr)("  page not busy, no owner");
#else
	(*pr)("  [page ownership tracking disabled]");
#endif
#ifdef __HAVE_VM_PAGE_MD
	(*pr)("\tvm_page_md %p\n", &pg->mdpage);
#else
	(*pr)("\n");
#endif
d4984 10
a4993 2
	if (!full)
		return;
d4995 9
a5003 23
	/* cross-verify object/anon */
	if ((pg->pg_flags & PQ_FREE) == 0) {
		if (pg->pg_flags & PQ_ANON) {
			if (pg->uanon == NULL || pg->uanon->an_page != pg)
			    (*pr)("  >>> ANON DOES NOT POINT HERE <<< (%p)\n",
				(pg->uanon) ? pg->uanon->an_page : NULL);
			else
				(*pr)("  anon backpointer is OK\n");
		} else {
			uobj = pg->uobject;
			if (uobj) {
				(*pr)("  checking object list\n");
				RB_FOREACH(tpg, uvm_objtree, &uobj->memt) {
					if (tpg == pg) {
						break;
					}
				}
				if (tpg)
					(*pr)("  page found on object list\n");
				else
			(*pr)("  >>> PAGE NOT FOUND ON OBJECT LIST! <<<\n");
			}
		}
d5005 9
d5015 4
a5018 15
	/* cross-verify page queue */
	if (pg->pg_flags & PQ_FREE) {
		if (uvm_pmr_isfree(pg))
			printf("  page found in uvm_pmemrange\n");
		else
			printf("  >>> page not found in uvm_pmemrange <<<\n");
		pgl = NULL;
	} else if (pg->pg_flags & PQ_INACTIVE) {
		pgl = (pg->pg_flags & PQ_SWAPBACKED) ?
		    &uvm.page_inactive_swp : &uvm.page_inactive_obj;
	} else if (pg->pg_flags & PQ_ACTIVE) {
		pgl = &uvm.page_active;
 	} else {
		pgl = NULL;
	}
d5020 4
a5023 12
	if (pgl) {
		(*pr)("  checking pageq list\n");
		TAILQ_FOREACH(tpg, pgl, pageq) {
			if (tpg == pg) {
				break;
			}
		}
		if (tpg)
			(*pr)("  page found on pageq list\n");
		else
			(*pr)("  >>> PAGE NOT FOUND ON PAGEQ LIST! <<<\n");
	}
d5025 4
a5028 1
#endif
@


1.146
log
@In ddb `show page', print the address of the vm_page_md substruct if
defined(__HAVE_VM_PAGE_MD), for convenience.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.145 2011/07/05 03:10:29 dhill Exp $	*/
d3187 1
a3187 1
 * uvmspace_share: share a vmspace between two proceses
d3190 1
a3190 1
 * - used for vfork, threads(?)
@


1.145
log
@allow option KVA_GUARDPAGES to compile.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.144 2011/07/03 18:36:49 oga Exp $	*/
d3810 1
a3810 1
		(*pr)("  owning process = %d, tag=%s\n",
d3813 1
a3813 1
		(*pr)("  page not busy, no owner\n");
d3815 6
a3820 1
	(*pr)("  [page ownership tracking disabled]\n");
@


1.144
log
@endodoify UVM_CNT too.

``beat it'' tedu@@ the deleteotron
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.143 2011/07/03 18:34:14 oga Exp $	*/
d961 1
a961 1
		if (guard_entry != NULL {
@


1.143
log
@Rip out and burn support for UVM_HIST.

The vm hackers don't use it, don't maintain it and have to look at it all the
time. About time this 800 lines of code hit /dev/null.

``never liked it'' tedu@@. ariane@@ was very happy when i told her i wrote
this diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.142 2011/06/30 15:51:06 tedu Exp $	*/
a98 3
struct uvm_cnt uvm_map_call, map_backmerge, map_forwmerge;
struct uvm_cnt map_nousermerge;
struct uvm_cnt uvm_mlk_call, uvm_mlk_hint;
d527 1
a527 14
	 * first, init logging system.
	 */

	UVMCNT_INIT(uvm_map_call,  UVMCNT_CNT, 0,
	    "# uvm_map() successful calls", 0);
	UVMCNT_INIT(map_backmerge, UVMCNT_CNT, 0, "# uvm_map() back merges", 0);
	UVMCNT_INIT(map_forwmerge, UVMCNT_CNT, 0, "# uvm_map() missed forward",
	    0);
	UVMCNT_INIT(map_nousermerge, UVMCNT_CNT, 0, "# back merges skipped", 0);
	UVMCNT_INIT(uvm_mlk_call,  UVMCNT_CNT, 0, "# map lookup calls", 0);
	UVMCNT_INIT(uvm_mlk_hint,  UVMCNT_CNT, 0, "# map lookup hint hits", 0);

	/*
	 * now set up static pool of kernel map entries ...
a785 2
	UVMCNT_INCR(uvm_map_call);

a856 1
			UVMCNT_INCR(map_nousermerge);
a865 2
		UVMCNT_INCR(map_backmerge);

a888 10
	 * check for possible forward merge (which we don't do) and count
	 * the number of times we missed a *possible* chance to merge more 
	 */

	if ((flags & UVM_FLAG_NOMERGE) == 0 &&
	    prev_entry->next != &map->header && 
	    prev_entry->next->start == (*startp + size))
		UVMCNT_INCR(map_forwmerge);

	/*
a1014 1
	UVMCNT_INCR(uvm_mlk_call);
a1028 1
			UVMCNT_INCR(uvm_mlk_hint);
@


1.142
log
@finish ansi in uvm.  ok ariane oga
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.141 2011/06/06 17:10:23 ariane Exp $	*/
a104 6
 * Da history books
 */
UVMHIST_DECL(maphist);
UVMHIST_DECL(pdhist);

/*
a393 1
	UVMHIST_FUNC("uvm_mapent_alloc"); UVMHIST_CALLED(maphist);
a439 2
	UVMHIST_LOG(maphist, "<- new entry=%p [kentry=%ld]", me,
	    ((map->flags & VM_MAP_INTRSAFE) != 0 || map == kernel_map), 0, 0);
a452 1
	UVMHIST_FUNC("uvm_mapent_free"); UVMHIST_CALLED(maphist);
a453 2
	UVMHIST_LOG(maphist,"<- freeing map entry=%p [flags=%ld]",
		me, me->flags, 0, 0);
a526 4
#if defined(UVMHIST)
	static struct uvm_history_ent maphistbuf[100];
	static struct uvm_history_ent pdhistbuf[100];
#endif
a532 5
	UVMHIST_FUNC("uvm_map_init");
	UVMHIST_INIT_STATIC(maphist, maphistbuf);
	UVMHIST_INIT_STATIC(pdhist, pdhistbuf);
	UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist,"<starting uvm map system>", 0, 0, 0, 0);
a718 6
	UVMHIST_FUNC("uvm_map");
	UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist, "(map=%p, *startp=0x%lx, size=%ld, flags=0x%lx)",
	    map, *startp, size, flags);
	UVMHIST_LOG(maphist, "  uobj/offset %p/%ld", uobj, (u_long)uoffset,0,0);
a771 2
		UVMHIST_LOG(maphist, "<- prot. failure: prot=0x%lx, max=0x%lx",
		    prot, maxprot,0,0);
a785 1
		UVMHIST_LOG(maphist,"<- uvm_map_findspace failed!",0,0,0,0);
a885 1
		UVMHIST_LOG(maphist,"  starting back merge", 0, 0, 0, 0);
a902 1
		UVMHIST_LOG(maphist,"<- done (via backmerge)!", 0, 0, 0, 0);
a907 1
	UVMHIST_LOG(maphist,"  allocating new map entry", 0, 0, 0, 0);
a1013 1
	UVMHIST_LOG(maphist,"<- done!", 0, 0, 0, 0);
a1032 5
	UVMHIST_FUNC("uvm_map_lookup_entry");
	UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist,"(map=%p,addr=0x%lx,ent=%p)",
	    map, address, entry, 0);
a1062 2
			UVMHIST_LOG(maphist,"<- got it via hint (%p)",
			    cur, 0, 0, 0);
a1099 1
		UVMHIST_LOG(maphist,"<- failed!",0,0,0,0);
a1116 2
				UVMHIST_LOG(maphist,"<- search got it (%p)",
					cur, 0, 0, 0);
a1125 1
	UVMHIST_LOG(maphist,"<- failed!",0,0,0,0);
a1253 1

a1254 2
	UVMHIST_FUNC("uvm_map_findspace");
	UVMHIST_CALLED(maphist);
a1255 2
	UVMHIST_LOG(maphist, "(map=%p, hint=0x%lx, len=%ld, flags=0x%lx)", 
		    map, hint, length, flags);
a1269 1
			UVMHIST_LOG(maphist,"<- VA below map range",0,0,0,0);
a1274 2
		UVMHIST_LOG(maphist,"<- VA 0x%lx > range [0x%lx->0x%lx]",
				hint, map->min_offset, map->max_offset, 0);
a1289 2
				UVMHIST_LOG(maphist,"<- fixed & VA in use",
				    0, 0, 0, 0);
a1299 1
			UVMHIST_LOG(maphist,"<- failed (off end)", 0,0,0,0);
a1304 1
		UVMHIST_LOG(maphist,"<- fixed mapping failed", 0,0,0,0);
a1423 1
			UVMHIST_LOG(maphist,"<- failed (off end)", 0,0,0,0);
a1432 1
	UVMHIST_LOG(maphist,"<- got it!  (result=0x%lx)", hint, 0,0,0);
a1436 3
		UVMHIST_LOG(maphist,
		    "calling recursively, no align",
		    0,0,0,0);
a1456 4
	UVMHIST_FUNC("uvm_unmap"); UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist, "  (map=%p, start=0x%lx, end=0x%lx)",
	    map, start, end, 0);
a1468 1
	UVMHIST_LOG(maphist, "<- done", 0,0,0,0);
a1490 5
	UVMHIST_FUNC("uvm_unmap_remove");
	UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist,"(map=%p, start=0x%lx, end=0x%lx)",
	    map, start, end, 0);
a1641 2
		UVMHIST_LOG(maphist, "  removed map entry %p", entry, 0, 0,0);

a1683 1
	UVMHIST_LOG(maphist,"<- done!", 0, 0, 0, 0);
a1695 1
	UVMHIST_FUNC("uvm_unmap_detach"); UVMHIST_CALLED(maphist);
a1698 5
		UVMHIST_LOG(maphist,
		    "  detach 0x%lx: amap=%p, obj=%p, submap?=%ld", 
		    first_entry, first_entry->aref.ar_amap, 
		    first_entry->object.uvm_obj,
		    UVM_ET_ISSUBMAP(first_entry));
a1724 1
	UVMHIST_LOG(maphist, "<- done", 0,0,0,0);
a1744 4
	UVMHIST_FUNC("uvm_map_reserve"); UVMHIST_CALLED(maphist); 

	UVMHIST_LOG(maphist, "(map=%p, size=0x%lx, offset=0x%lx,addr=0x%lx)",
	      map,size,offset,raddr);
a1756 1
	    UVMHIST_LOG(maphist, "<- done (no VM)", 0,0,0,0);
a1759 1
	UVMHIST_LOG(maphist, "<- done (*raddr=0x%lx)", *raddr,0,0,0);
a1915 5
	UVMHIST_FUNC("uvm_map_extract"); UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist,"(srcmap=%p,start=0x%lx, len=0x%lx", srcmap, start,
	    len,0);
	UVMHIST_LOG(maphist," ...,dstmap=%p, flags=0x%lx)", dstmap,flags,0,0);
a1937 1
	UVMHIST_LOG(maphist, "  dstaddr=0x%lx", dstaddr,0,0,0);
a2290 3
	UVMHIST_FUNC("uvm_map_protect"); UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist,"(map=%p,start=0x%lx,end=0x%lx,new_prot=0x%lx)",
		    map, start, end, new_prot);
a2383 1
	UVMHIST_LOG(maphist, "<- done, rv=%ld",error,0,0,0);
a2402 3
	UVMHIST_FUNC("uvm_map_inherit"); UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist,"(map=%p,start=0x%lx,end=0x%lx,new_inh=0x%lx)",
	    map, start, end, new_inheritance);
a2409 1
		UVMHIST_LOG(maphist,"<- done (INVALID ARG)",0,0,0,0);
a2429 1
	UVMHIST_LOG(maphist,"<- done (OK)",0,0,0,0);
a2442 3
	UVMHIST_FUNC("uvm_map_advice"); UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist,"(map=%p,start=0x%lx,end=0x%lx,new_adv=0x%lx)",
	    map, start, end, new_advice);
a2451 1
		UVMHIST_LOG(maphist,"<- done (INVALID ARG)",0,0,0,0);
a2473 1
	UVMHIST_LOG(maphist,"<- done (OK)",0,0,0,0);
a2499 3
	UVMHIST_FUNC("uvm_map_pageable"); UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist,"(map=%p,start=0x%lx,end=0x%lx,new_pageable=0x%lx)",
		    map, start, end, new_pageable);
a2518 1
		UVMHIST_LOG(maphist,"<- done (INVALID ARG)",0,0,0,0);
a2541 2
				UVMHIST_LOG(maphist,
				    "<- done (INVALID UNWIRE ARG)",0,0,0,0);
a2561 1
		UVMHIST_LOG(maphist,"<- done (OK UNWIRE)",0,0,0,0);
a2630 1
			UVMHIST_LOG(maphist,"<- done (INVALID WIRE)",0,0,0,0);
a2702 1
		UVMHIST_LOG(maphist, "<- done (RV=%ld)", rv,0,0,0);
a2719 1
	UVMHIST_LOG(maphist,"<- done (OK WIRE)",0,0,0,0);
a2740 2
	UVMHIST_FUNC("uvm_map_pageable_all"); UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist,"(map=%p,flags=0x%lx)", map, flags, 0, 0);
a2761 1
		UVMHIST_LOG(maphist,"<- done (OK UNWIRE)",0,0,0,0);
a2779 1
		UVMHIST_LOG(maphist,"<- done (OK no wire)",0,0,0,0);
a2917 1
		UVMHIST_LOG(maphist,"<- done (RV=%ld)", error,0,0,0);
a2924 1
	UVMHIST_LOG(maphist,"<- done (OK WIRE)",0,0,0,0);
a2956 1
	UVMHIST_FUNC("uvm_map_clean"); UVMHIST_CALLED(maphist);
a2957 2
	UVMHIST_LOG(maphist,"(map=%p,start=0x%lx,end=0x%lx,flags=0x%lx)",
		    map, start, end, flags);
a3187 1
	UVMHIST_FUNC("uvmspace_alloc"); UVMHIST_CALLED(maphist);
a3190 1
	UVMHIST_LOG(maphist,"<- done (vm=%p)", vm,0,0,0);
a3203 1
	UVMHIST_FUNC("uvmspace_init"); UVMHIST_CALLED(maphist);
a3216 2

	UVMHIST_LOG(maphist,"<- done",0,0,0,0);
a3328 1
	UVMHIST_FUNC("uvmspace_free"); UVMHIST_CALLED(maphist);
a3329 1
	UVMHIST_LOG(maphist,"(vm=%p) ref=%ld", vm, vm->vm_refcnt,0,0);
a3352 1
	UVMHIST_LOG(maphist,"<- done", 0,0,0,0);
a3455 1
	UVMHIST_FUNC("uvmspace_fork"); UVMHIST_CALLED(maphist);
a3734 1
	UVMHIST_LOG(maphist,"<- done",0,0,0,0);
@


1.141
log
@Backout vmmap in order to repair virtual address selection algorithms
outside the tree.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.135 2011/04/26 23:50:21 ariane Exp $	*/
d3359 1
a3359 2
uvmspace_share(p1, p2)
	struct proc *p1, *p2;
d3926 2
a3927 4
uvm_object_printit(uobj, full, pr)
	struct uvm_object *uobj;
	boolean_t full;
	int (*pr)(const char *, ...);
d3965 2
a3966 4
uvm_page_printit(pg, full, pr)
	struct vm_page *pg;
	boolean_t full;
	int (*pr)(const char *, ...);
@


1.140
log
@randomness might be broken, but this change
a) chooses incorrect kernel memory on the macppc
b) perhaps on zaurus too, which does not make it to copyright
c) was not tested on those platforms before commit
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.138 2011/05/29 17:18:22 ariane Exp $	*/
d4 1
a4 16
/*
 * Copyright (c) 2011 Ariane van der Steldt <ariane@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 *
 * 
a73 3
/* #define DEBUG */
#define VMMAP_MIN_ADDR	PAGE_SIZE	/* auto-allocate address lower bound */

d89 2
a95 185

vsize_t			 uvmspace_dused(struct vm_map*, vaddr_t, vaddr_t);
struct vm_map_entry	*uvm_map_entrybyaddr(struct uvm_map_addr*, vaddr_t);
struct vm_map_entry	*uvm_map_findspace_entry(struct uvm_map_free*, vsize_t);
struct vm_map_entry	*uvm_map_findspace_tree(struct uvm_map_free*, vsize_t,
			    voff_t, vsize_t, int, vaddr_t*, struct vm_map*);
int			 uvm_map_isavail(struct uvm_map_addr*,
			    struct vm_map_entry**, struct vm_map_entry**,
			    vaddr_t, vsize_t);
int			 uvm_mapent_isjoinable(struct vm_map*,
			    struct vm_map_entry*, struct vm_map_entry*);
struct vm_map_entry	*uvm_mapent_merge(struct vm_map*, struct vm_map_entry*,
			    struct vm_map_entry*, struct uvm_map_deadq*);
struct vm_map_entry	*uvm_mapent_tryjoin(struct vm_map*,
			    struct vm_map_entry*, struct uvm_map_deadq*);
struct vm_map_entry	*uvm_map_mkentry(struct vm_map*, struct vm_map_entry*,
			    struct vm_map_entry*, vaddr_t, vsize_t, int,
			    struct uvm_map_deadq*);
struct vm_map_entry	*uvm_mapent_alloc(struct vm_map*, int);
void			 uvm_mapent_free(struct vm_map_entry*);
void			 uvm_mapent_mkfree(struct vm_map*,
			    struct vm_map_entry*, struct vm_map_entry**,
			    struct uvm_map_deadq*, boolean_t);
void			 uvm_map_pageable_pgon(struct vm_map*,
			    struct vm_map_entry*, struct vm_map_entry*,
			    vaddr_t, vaddr_t);
int			 uvm_map_pageable_wire(struct vm_map*,
			    struct vm_map_entry*, struct vm_map_entry*,
			    vaddr_t, vaddr_t, int);
void			 uvm_map_setup_entries(struct vm_map*);
void			 uvm_map_vmspace_update(struct vm_map*,
			    struct uvm_map_deadq*, int);
void			 uvm_map_kmem_grow(struct vm_map*,
			    struct uvm_map_deadq*, vsize_t, int);
void			 uvm_map_freelist_update_clear(struct vm_map*,
			    struct uvm_map_deadq*);
void			 uvm_map_freelist_update_refill(struct vm_map *, int);
void			 uvm_map_freelist_update(struct vm_map*,
			    struct uvm_map_deadq*, vaddr_t, vaddr_t,
			    vaddr_t, vaddr_t, int);
struct vm_map_entry	*uvm_map_fix_space(struct vm_map*, struct vm_map_entry*,
			    vaddr_t, vaddr_t, int);
int			 uvm_map_sel_limits(vaddr_t*, vaddr_t*, vsize_t, int,
			    struct vm_map_entry*, vaddr_t, vaddr_t, vaddr_t,
			    int);

/*
 * Tree management functions.
 */

static __inline void	 uvm_mapent_copy(struct vm_map_entry*,
			    struct vm_map_entry*);
static int		 uvm_mapentry_addrcmp(struct vm_map_entry*,
			    struct vm_map_entry*);
static int		 uvm_mapentry_freecmp(struct vm_map_entry*,
			    struct vm_map_entry*);
void			 uvm_mapent_free_insert(struct vm_map*,
			    struct uvm_map_free*, struct vm_map_entry*);
void			 uvm_mapent_free_remove(struct vm_map*,
			    struct uvm_map_free*, struct vm_map_entry*);
void			 uvm_mapent_addr_insert(struct vm_map*,
			    struct vm_map_entry*);
void			 uvm_mapent_addr_remove(struct vm_map*,
			    struct vm_map_entry*);
void			 uvm_map_splitentry(struct vm_map*,
			    struct vm_map_entry*, struct vm_map_entry*,
			    vaddr_t);
vsize_t			 uvm_map_boundary(struct vm_map*, vaddr_t, vaddr_t);
struct uvm_map_free	*uvm_free(struct vm_map*, vaddr_t);
int			 uvm_mapent_bias(struct vm_map*, struct vm_map_entry*);

/* Find freelist for containing addr. */
#define UVM_FREE(_map, _addr)		uvm_free((_map), (_addr))
/* Size of the free tree. */
#define uvm_mapfree_size(_free)		((_free)->treesz)

/*
 * uvm_vmspace_fork helper functions.
 */
struct vm_map_entry	*uvm_mapent_clone(struct vm_map*, vaddr_t, vsize_t,
			    vsize_t, struct vm_map_entry*,
			    struct uvm_map_deadq*, int, int);
void			 uvm_mapent_forkshared(struct vmspace*, struct vm_map*,
			    struct vm_map*, struct vm_map_entry*,
			    struct uvm_map_deadq*);
void			 uvm_mapent_forkcopy(struct vmspace*, struct vm_map*,
			    struct vm_map*, struct vm_map_entry*,
			    struct uvm_map_deadq*);

/*
 * Tree validation.
 */

#ifdef DEBUG
void			 uvm_tree_assert(struct vm_map*, int, char*,
			    char*, int);
#define UVM_ASSERT(map, cond, file, line)				\
	uvm_tree_assert((map), (cond), #cond, (file), (line))
void			 uvm_tree_sanity_free(struct vm_map*,
			    struct uvm_map_free*, char*, int);
void			 uvm_tree_sanity(struct vm_map*, char*, int);
void			 uvm_tree_size_chk(struct vm_map*, char*, int);
void			 vmspace_validate(struct vm_map*);
#else
#define uvm_tree_sanity_free(_map, _free, _file, _line)	do {} while (0)
#define uvm_tree_sanity(_map, _file, _line)		do {} while (0)
#define uvm_tree_size_chk(_map, _file, _line)		do {} while (0)
#define vmspace_validate(_map)				do {} while (0)
#endif


/*
 * The kernel map will initially be VM_MAP_KSIZE_INIT bytes.
 * Every time that gets cramped, we grow by at least VM_MAP_KSIZE_DELTA bytes.
 *
 * We attempt to grow by UVM_MAP_KSIZE_ALLOCMUL times the allocation size
 * each time.
 */
#define VM_MAP_KSIZE_INIT	(512 * PAGE_SIZE)
#define VM_MAP_KSIZE_DELTA	(256 * PAGE_SIZE)
#define VM_MAP_KSIZE_ALLOCMUL	4
/*
 * When selecting a random free-space block, look at most FSPACE_DELTA blocks
 * ahead.
 */
#define FSPACE_DELTA		8
/*
 * Put allocations adjecent to previous allocations when the free-space tree
 * is larger than FSPACE_COMPACT entries.
 *
 * Alignment and PMAP_PREFER may still cause the entry to not be fully
 * adjecent. Note that this strategy reduces memory fragmentation (by leaving
 * a large space before or after the allocation).
 */
#define FSPACE_COMPACT		128
/*
 * Make the address selection skip at most this many bytes from the start of
 * the free space in which the allocation takes place.
 *
 * The main idea behind a randomized address space is that an attacker cannot
 * know where to target his attack. Therefore, the location of objects must be
 * as random as possible. However, the goal is not to create the most sparse
 * map that is possible.
 * FSPACE_MAXOFF pushes the considered range in bytes down to less insane
 * sizes, thereby reducing the sparseness. The biggest randomization comes
 * from fragmentation, i.e. FSPACE_COMPACT.
 */
#define FSPACE_MAXOFF		((vaddr_t)32 * 1024 * 1024)
/*
 * Allow for small gaps in the overflow areas.
 * Gap size is in bytes and does not have to be a multiple of page-size.
 */
#define FSPACE_BIASGAP		((vaddr_t)32 * 1024)


#define FREE_START(_entry)	((_entry)->end + (_entry)->guard)
#define FREE_END(_entry)	((_entry)->end + (_entry)->guard +	\
				    (_entry)->fspace)

#ifdef DEADBEEF0
#define UVMMAP_DEADBEEF		((void*)DEADBEEF0)
#else
#define UVMMAP_DEADBEEF		((void*)0xdeadd0d0)
#endif

#ifdef DEBUG
int uvm_map_dprintf = 0;
int uvm_map_printlocks = 0;

#define DPRINTF(_args)							\
	do {								\
		if (uvm_map_dprintf)					\
			printf _args;					\
	} while (0)

#define LPRINTF(_args)							\
	do {								\
		if (uvm_map_printlocks)					\
			printf _args;					\
	} while (0)
#else
#define DPRINTF(_args)	do {} while (0)
#define LPRINTF(_args)	do {} while (0)
#endif

d105 6
d113 1
d119 1
d123 1
d126 1
a126 1
 * space. If we want to exceed this, we must grow the kernel
d132 1
d135 1
a135 1
 * Locking predicate.
a136 5
#define UVM_MAP_REQ_WRITE(_map)						\
	do {								\
		if (((_map)->flags & VM_MAP_INTRSAFE) == 0)		\
			rw_assert_wrlock(&(_map)->lock);		\
	} while (0)
d139 1
a139 1
 * Tree describing entries by address.
d141 1
a141 3
 * Addresses are unique.
 * Entries with start == end may only exist if they are the first entry
 * (sorted by address) within a free-memory tree.
d143 8
d152 11
a162 5
static __inline int
uvm_mapentry_addrcmp(struct vm_map_entry *e1, struct vm_map_entry *e2)
{
	return e1->start < e2->start ? -1 : e1->start > e2->start;
}
d165 1
a165 1
 * Tree describing free memory.
d167 11
a177 2
 * Free memory is indexed (so we can use array semantics in O(log N).
 * Free memory is ordered by size (so we can reduce fragmentation).
d179 36
a214 3
 * The address range in the tree can be limited, having part of the
 * free memory not in the free-memory tree. Only free memory in the
 * tree will be considered during 'any address' allocations.
d216 9
d227 1
a227 1
uvm_mapentry_freecmp(struct vm_map_entry *e1, struct vm_map_entry *e2)
d229 6
a234 2
	int cmp = e1->fspace < e2->fspace ? -1 : e1->fspace > e2->fspace;
	return cmp ? cmp : uvm_mapentry_addrcmp(e1, e2);
d237 1
a237 3
/*
 * Copy mapentry.
 */
d239 1
a239 1
uvm_mapent_copy(struct vm_map_entry *src, struct vm_map_entry *dst)
d241 2
a242 2
	caddr_t csrc, cdst;
	size_t sz;
d244 1
a244 4
	csrc = (caddr_t)src;
	cdst = (caddr_t)dst;
	csrc += offsetof(struct vm_map_entry, uvm_map_entry_start_copy);
	cdst += offsetof(struct vm_map_entry, uvm_map_entry_start_copy);
d246 1
a246 4
	sz = offsetof(struct vm_map_entry, uvm_map_entry_stop_copy) -
	    offsetof(struct vm_map_entry, uvm_map_entry_start_copy);
	memcpy(cdst, csrc, sz);
}
d248 2
a249 6
/*
 * Handle free-list insertion.
 */
void
uvm_mapent_free_insert(struct vm_map *map, struct uvm_map_free *free,
    struct vm_map_entry *entry)
d251 2
a252 4
	struct vm_map_entry *res;
#ifdef DEBUG
	vaddr_t min, max, bound;
#endif
d254 13
a266 4
	if (RB_LEFT(entry, free_entry) != UVMMAP_DEADBEEF ||
	    RB_RIGHT(entry, free_entry) != UVMMAP_DEADBEEF ||
	    RB_PARENT(entry, free_entry) != UVMMAP_DEADBEEF)
		panic("uvm_mapent_addr_insert: entry still in free list");
d268 6
a273 7
#ifdef DEBUG
	/*
	 * Boundary check.
	 * Boundaries are folded if they go on the same free list.
	 */
	min = FREE_START(entry);
	max = FREE_END(entry);
d275 4
a278 3
	while (min < max && (bound = uvm_map_boundary(map, min, max)) != max) {
		KASSERT(UVM_FREE(map, min) == free);
		min = bound;
a279 2
#endif
	KDASSERT(entry->fspace > 0 && (entry->fspace & PAGE_MASK) == 0);
d281 1
a281 5
	UVM_MAP_REQ_WRITE(map);
	res = RB_INSERT(uvm_map_free_int, &free->tree, entry);
	free->treesz++;
	if (res != NULL)
		panic("uvm_mapent_free_insert");
a283 3
/*
 * Handle free-list removal.
 */
d285 1
a285 2
uvm_mapent_free_remove(struct vm_map *map, struct uvm_map_free *free,
    struct vm_map_entry *entry)
d287 5
a291 9
	struct vm_map_entry *res;

	UVM_MAP_REQ_WRITE(map);
	res = RB_REMOVE(uvm_map_free_int, &free->tree, entry);
	free->treesz--;
	if (res != entry)
		panic("uvm_mapent_free_remove");
	RB_LEFT(entry, free_entry) = RB_RIGHT(entry, free_entry) =
	    RB_PARENT(entry, free_entry) = UVMMAP_DEADBEEF;
a293 3
/*
 * Handle address tree insertion.
 */
d295 1
a295 1
uvm_mapent_addr_insert(struct vm_map *map, struct vm_map_entry *entry)
d297 2
a298 1
	struct vm_map_entry *res;
d300 9
a308 12
	if (RB_LEFT(entry, daddrs.addr_entry) != UVMMAP_DEADBEEF ||
	    RB_RIGHT(entry, daddrs.addr_entry) != UVMMAP_DEADBEEF ||
	    RB_PARENT(entry, daddrs.addr_entry) != UVMMAP_DEADBEEF)
		panic("uvm_mapent_addr_insert: entry still in addr list");
	KDASSERT(entry->start <= entry->end);
	KDASSERT((entry->start & PAGE_MASK) == 0 &&
	    (entry->end & PAGE_MASK) == 0);

	UVM_MAP_REQ_WRITE(map);
	res = RB_INSERT(uvm_map_addr, &map->addr, entry);
	if (res != NULL)
		panic("uvm_mapent_addr_insert");
a310 3
/*
 * Handle address tree removal.
 */
d312 1
a312 1
uvm_mapent_addr_remove(struct vm_map *map, struct vm_map_entry *entry)
d314 1
a314 1
	struct vm_map_entry *res;
d316 6
a321 6
	UVM_MAP_REQ_WRITE(map);
	res = RB_REMOVE(uvm_map_addr, &map->addr, entry);
	if (res != entry)
		panic("uvm_mapent_addr_remove");
	RB_LEFT(entry, daddrs.addr_entry) = RB_RIGHT(entry, daddrs.addr_entry) =
	    RB_PARENT(entry, daddrs.addr_entry) = UVMMAP_DEADBEEF;
d324 5
a328 11
/*
 * uvm_map_reference: add reference to a map
 *
 * XXX check map reference counter lock
 */
#define uvm_map_reference(_map)						\
	do {								\
		simple_lock(&map->ref_lock);				\
		map->ref_count++;					\
		simple_unlock(&map->ref_lock);				\
	} while (0)
d330 3
a332 5
/*
 * Calculate the dused delta.
 */
vsize_t
uvmspace_dused(struct vm_map *map, vaddr_t min, vaddr_t max)
d334 2
a335 4
	struct vmspace *vm;
	vsize_t sz;
	vaddr_t lmax;
	vaddr_t stack_begin, stack_end; /* Position of stack. */
d337 41
a377 18
	KASSERT(map->flags & VM_MAP_ISVMSPACE);
	vm = (struct vmspace *)map;
	stack_begin = MIN((vaddr_t)vm->vm_maxsaddr, (vaddr_t)vm->vm_minsaddr);
	stack_end = MAX((vaddr_t)vm->vm_maxsaddr, (vaddr_t)vm->vm_minsaddr);

	sz = 0;
	while (min != max) {
		lmax = max;
		if (min < stack_begin && lmax > stack_begin)
			lmax = stack_begin;
		else if (min < stack_end && lmax > stack_end)
			lmax = stack_end;

		if (min >= stack_begin && min < stack_end) {
			/* nothing */
		} else
			sz += lmax - min;
		min = lmax;
d380 7
a386 1
	return sz >> PAGE_SHIFT;
d388 1
d391 1
a391 1
 * Find the entry describing the given address.
d393 3
a395 2
struct vm_map_entry*
uvm_map_entrybyaddr(struct uvm_map_addr *atree, vaddr_t addr)
d397 8
a404 1
	struct vm_map_entry *iter;
d406 38
a443 8
	iter = RB_ROOT(atree);
	while (iter != NULL) {
		if (iter->start > addr)
			iter = RB_LEFT(iter, daddrs.addr_entry);
		else if (FREE_END(iter) <= addr)
			iter = RB_RIGHT(iter, daddrs.addr_entry);
		else
			return iter;
d445 5
a449 1
	return NULL;
d453 3
a455 1
 * Find the first entry with at least sz bytes free.
d457 3
a459 2
struct vm_map_entry*
uvm_map_findspace_entry(struct uvm_map_free *free, vsize_t sz)
d461 2
a462 2
	struct vm_map_entry *iter;
	struct vm_map_entry *res;
d464 16
a479 9
	iter = RB_ROOT(&free->tree);
	res = NULL;

	while (iter) {
		if (iter->fspace >= sz) {
			res = iter;
			iter = RB_LEFT(iter, free_entry);
		} else
			iter = RB_RIGHT(iter, free_entry);
a480 1
	return res;
d484 1
a484 8
 * DEAD_ENTRY_PUSH(struct vm_map_entry**head, struct vm_map_entry *entry)
 *
 * Push dead entries into a linked list.
 * Since the linked list abuses the address tree for storage, the entry
 * may not be linked in a map.
 *
 * *head must be initialized to NULL before the first call to this macro.
 * uvm_unmap_detach(*head, 0) will remove dead entries.
d486 3
a488 2
static __inline void
dead_entry_push(struct uvm_map_deadq *deadq, struct vm_map_entry *entry)
d490 2
a491 1
	TAILQ_INSERT_TAIL(deadq, entry, daddrs.deadq);
a492 2
#define DEAD_ENTRY_PUSH(_headptr, _entry)				\
	dead_entry_push((_headptr), (_entry))
d495 1
a495 1
 * Helper function for uvm_map_findspace_tree.
d497 1
a497 8
 * Given allocation constraints and pmap constraints, finds the
 * lowest and highest address in a range that can be used for the
 * allocation.
 *
 * pmap_align and pmap_off are ignored on non-PMAP_PREFER archs.
 *
 *
 * Big chunk of math with a seasoning of dragons.
d499 2
a500 4
int
uvm_map_sel_limits(vaddr_t *min, vaddr_t *max, vsize_t sz, int guardpg,
    struct vm_map_entry *sel, vaddr_t align,
    vaddr_t pmap_align, vaddr_t pmap_off, int bias)
a501 7
	vaddr_t sel_min, sel_max;
#ifdef PMAP_PREFER
	vaddr_t pmap_min, pmap_max;
#endif /* PMAP_PREFER */
#ifdef DIAGNOSTIC
	int bad;
#endif /* DIAGNOSTIC */
d503 3
a505 2
	sel_min = FREE_START(sel);
	sel_max = FREE_END(sel) - sz - (guardpg ? PAGE_SIZE : 0);
a506 1
#ifdef PMAP_PREFER
d508 9
a516 81
	/*
	 * There are two special cases, in which we can satisfy the align
	 * requirement and the pmap_prefer requirement.
	 * - when pmap_off == 0, we always select the largest of the two
	 * - when pmap_off % align == 0 and pmap_align > align, we simply
	 *   satisfy the pmap_align requirement and automatically
	 *   satisfy the align requirement.
	 */
	if (align > PAGE_SIZE &&
	    !(pmap_align > align && (pmap_off & (align - 1)) == 0)) {
		/*
		 * Simple case: only use align.
		 */
		sel_min = roundup(sel_min, align);
		sel_max &= ~(align - 1);

		if (sel_min > sel_max)
			return ENOMEM;

		/*
		 * Correct for bias.
		 */
		if (sel_max - sel_min > FSPACE_BIASGAP) {
			if (bias > 0) {
				sel_min = sel_max - FSPACE_BIASGAP;
				sel_min = roundup(sel_min, align);
			} else if (bias < 0) {
				sel_max = sel_min + FSPACE_BIASGAP;
				sel_max &= ~(align - 1);
			}
		}
	} else if (pmap_align != 0) {
		/*
		 * Special case: satisfy both pmap_prefer and
		 * align argument.
		 */
		pmap_max = sel_max & ~(pmap_align - 1);
		pmap_min = sel_min;
		if (pmap_max < sel_min)
			return ENOMEM;

		/* Adjust pmap_min for BIASGAP for top-addr bias. */
		if (bias > 0 && pmap_max - pmap_min > FSPACE_BIASGAP)
			pmap_min = pmap_max - FSPACE_BIASGAP;
		/* Align pmap_min. */
		pmap_min &= ~(pmap_align - 1);
		if (pmap_min < sel_min)
			pmap_min += pmap_align;
		if (pmap_min > pmap_max)
			return ENOMEM;

		/* Adjust pmap_max for BIASGAP for bottom-addr bias. */
		if (bias < 0 && pmap_max - pmap_min > FSPACE_BIASGAP) {
			pmap_max = (pmap_min + FSPACE_BIASGAP) &
			    ~(pmap_align - 1);
		}
		if (pmap_min > pmap_max)
			return ENOMEM;

		/* Apply pmap prefer offset. */
		pmap_max |= pmap_off;
		if (pmap_max > sel_max)
			pmap_max -= pmap_align;
		pmap_min |= pmap_off;
		if (pmap_min < sel_min)
			pmap_min += pmap_align;

		/*
		 * Fixup: it's possible that pmap_min and pmap_max
		 * cross eachother. In this case, try to find one
		 * address that is allowed.
		 * (This usually happens in biased case.)
		 */
		if (pmap_min > pmap_max) {
			if (pmap_min < sel_max)
				pmap_max = pmap_min;
			else if (pmap_max > sel_min)
				pmap_min = pmap_max;
			else
				return ENOMEM;
		}
a517 2
		/* Internal validation. */
		KDASSERT(pmap_min <= pmap_max);
d519 9
a527 6
		sel_min = pmap_min;
		sel_max = pmap_max;
	} else if (bias > 0 && sel_max - sel_min > FSPACE_BIASGAP)
		sel_min = sel_max - FSPACE_BIASGAP;
	else if (bias < 0 && sel_max - sel_min > FSPACE_BIASGAP)
		sel_max = sel_min + FSPACE_BIASGAP;
a528 1
#else
d530 4
a533 19
	if (align > PAGE_SIZE) {
		sel_min = roundup(sel_min, align);
		sel_max &= ~(align - 1);
		if (sel_min > sel_max)
			return ENOMEM;

		if (bias != 0 && sel_max - sel_min > FSPACE_BIASGAP) {
			if (bias > 0) {
				sel_min = roundup(sel_max - FSPACE_BIASGAP,
				    align);
			} else {
				sel_max = (sel_min + FSPACE_BIASGAP) &
				    ~(align - 1);
			}
		}
	} else if (bias > 0 && sel_max - sel_min > FSPACE_BIASGAP)
		sel_min = sel_max - FSPACE_BIASGAP;
	else if (bias < 0 && sel_max - sel_min > FSPACE_BIASGAP)
		sel_max = sel_min + FSPACE_BIASGAP;
d535 7
d543 1
d545 3
a547 2
	if (sel_min > sel_max)
		return ENOMEM;
d549 13
a561 85
#ifdef DIAGNOSTIC
	bad = 0;
	/* Lower boundary check. */
	if (sel_min < FREE_START(sel)) {
		printf("sel_min: 0x%lx, but should be at least 0x%lx\n",
		    sel_min, FREE_START(sel));
		bad++;
	}
	/* Upper boundary check. */
	if (sel_max > FREE_END(sel) - sz - (guardpg ? PAGE_SIZE : 0)) {
		printf("sel_max: 0x%lx, but should be at most 0x%lx\n",
		    sel_max, FREE_END(sel) - sz - (guardpg ? PAGE_SIZE : 0));
		bad++;
	}
	/* Lower boundary alignment. */
	if (align != 0 && (sel_min & (align - 1)) != 0) {
		printf("sel_min: 0x%lx, not aligned to 0x%lx\n",
		    sel_min, align);
		bad++;
	}
	/* Upper boundary alignment. */
	if (align != 0 && (sel_max & (align - 1)) != 0) {
		printf("sel_max: 0x%lx, not aligned to 0x%lx\n",
		    sel_max, align);
		bad++;
	}
	/* Lower boundary PMAP_PREFER check. */
	if (pmap_align != 0 && align == 0 &&
	    (sel_min & (pmap_align - 1)) != pmap_off) {
		printf("sel_min: 0x%lx, aligned to 0x%lx, expected 0x%lx\n",
		    sel_min, sel_min & (pmap_align - 1), pmap_off);
		bad++;
	}
	/* Upper boundary PMAP_PREFER check. */
	if (pmap_align != 0 && align == 0 &&
	    (sel_max & (pmap_align - 1)) != pmap_off) {
		printf("sel_max: 0x%lx, aligned to 0x%lx, expected 0x%lx\n",
		    sel_max, sel_max & (pmap_align - 1), pmap_off);
		bad++;
	}

	if (bad) {
		panic("uvm_map_sel_limits(sz = %lu, guardpg = %c, "
		    "align = 0x%lx, pmap_align = 0x%lx, pmap_off = 0x%lx, "
		    "bias = %d, "
		    "FREE_START(sel) = 0x%lx, FREE_END(sel) = 0x%lx)",
		    sz, (guardpg ? 'T' : 'F'), align, pmap_align, pmap_off,
		    bias, FREE_START(sel), FREE_END(sel));
	}
#endif /* DIAGNOSTIC */

	*min = sel_min;
	*max = sel_max;
	return 0;
}

/*
 * Find address and free space for sz bytes.
 *
 * free: tree of free space
 * sz: size in bytes
 * align: preferred alignment
 * guardpg: if true, keep free space guards on both ends
 * addr: fill in found address
 *
 * align is a hard requirement to align to virtual addresses.
 * PMAP_PREFER is a soft requirement that is dropped if
 * no memory can be found that will be acceptable.
 *
 * align overrules PMAP_PREFER, but if both can be satisfied, the code
 * will attempt to find a range that does this.
 *
 * Returns NULL on failure.
 */
struct vm_map_entry*
uvm_map_findspace_tree(struct uvm_map_free *free, vsize_t sz, voff_t uoffset,
    vsize_t align, int guardpg, vaddr_t *addr, struct vm_map *map)
{
	struct vm_map_entry *sfe; /* Start free entry. */
	struct vm_map_entry *sel; /* Selected free entry. */
	struct vm_map_entry *search_start, *fail_start;
	size_t sel_idx, i;
	vaddr_t sel_min, sel_max, sel_addr;
	vaddr_t pmap_off, pmap_align; /* pmap_prefer variables */
	int bias;
d563 3
a565 9
#ifdef PMAP_PREFER
	/* Fix pmap prefer parameters. */
	pmap_off = 0;
	pmap_align = PMAP_PREFER_ALIGN();
	if (uoffset != UVM_UNKNOWN_OFFSET && pmap_align > PAGE_SIZE)
		pmap_off = PMAP_PREFER_OFFSET(uoffset);
	else
		pmap_align = 0;
	KDASSERT(pmap_align == 0 || pmap_off < pmap_align);
d567 5
a571 9
	if (align > PAGE_SIZE || (pmap_off != 0 && pmap_off < align)) {
		/*
		 * We're doomed.
		 *
		 * This allocation will never be able to fulfil the pmap_off
		 * requirement.
		 */
		pmap_off = 0;
		pmap_align = 0;
a572 7
#else
	pmap_off = pmap_align = 0;
#endif

	/* Set up alignment argument. */
	if (align < PAGE_SIZE)
		align = PAGE_SIZE;
d575 1
a575 1
	 * First entry that meets requirements.
d577 8
a584 3
	sfe = uvm_map_findspace_entry(free, sz + (guardpg ? PAGE_SIZE : 0));
	if (sfe == NULL)
		return NULL;
d586 12
a597 18
	/* Select the entry from which we will allocate. */
	sel_idx = arc4random_uniform(FSPACE_DELTA);
	sel = sfe;
	for (i = 0; i < sel_idx; i++) {
		sel = RB_NEXT(uvm_map_free_int, free->tree, sel);
		/*
		 * This has a slight bias at the top of the tree (largest
		 * segments) towards the smaller elements.
		 * This may be nice.
		 */
		if (sel == NULL) {
			sel_idx -= i;
			i = 0;
			sel = sfe;
		}
	}
	search_start = sel;
	fail_start = NULL;
d599 6
a604 28
#ifdef PMAP_PREFER
pmap_prefer_retry:
#endif /* PMAP_PREFER */
	while (sel != NULL) {
		bias = uvm_mapent_bias(map, sel);
		if (bias == 0 && free->treesz >= FSPACE_COMPACT)
			bias = (arc4random() & 0x1) ? 1 : -1;

		if (uvm_map_sel_limits(&sel_min, &sel_max, sz, guardpg, sel,
		    align, pmap_align, pmap_off, bias) == 0) {
			if (bias > 0)
				sel_addr = sel_max;
			else if (bias < 0)
				sel_addr = sel_min;
			else if (sel_min == sel_max)
				sel_addr = sel_min;
			else {
				/*
				 * Select a random address.
				 *
				 * Use sel_addr to limit the arc4random range.
				 */
				sel_addr = sel_max - sel_min;
				if (align <= PAGE_SIZE && pmap_align != 0)
					sel_addr += pmap_align;
				else
					sel_addr += align;
				sel_addr = MIN(sel_addr, FSPACE_MAXOFF);
d606 1
a606 14
				/*
				 * Shift down, so arc4random can deal with
				 * the number.
				 * arc4random wants a 32-bit number. Therefore,
				 * handle 64-bit overflow.
				 */
				sel_addr >>= PAGE_SHIFT;
				if (sel_addr > 0xffffffff)
					sel_addr = 0xffffffff;
				sel_addr = arc4random_uniform(sel_addr);
				/*
				 * Shift back up.
				 */
				sel_addr <<= PAGE_SHIFT;
d608 1
a608 9
				/*
				 * Cancel bits that violate our alignment.
				 *
				 * This also cancels bits that are in
				 * PAGE_MASK, because align is at least
				 * a page.
				 */
				sel_addr &= ~(align - 1);
				sel_addr &= ~(pmap_align - 1);
d610 5
a614 7
				KDASSERT(sel_addr <= sel_max - sel_min);
				/*
				 * Change sel_addr from an offset relative
				 * to sel_min, to the actual selected address.
				 */
				sel_addr += sel_min;
			}
d616 2
a617 3
			*addr = sel_addr;
			return sel;
		}
d619 4
a622 7
		/* Next entry. */
		sel_idx++;
		sel = RB_NEXT(uvm_map_free_int, &free->tree, sel);
		if (sel_idx == FSPACE_DELTA ||
		    (sel == NULL && sel_idx <= FSPACE_DELTA)) {
			if (fail_start == NULL)
				fail_start = sel;
d624 2
a625 3
			sel_idx = 0;
			sel = sfe;
		}
d627 2
a628 15
		/*
		 * sel == search_start -> we made a full loop through the
		 * first FSPACE_DELTA items and couldn't find anything.
		 *
		 * We now restart the loop, at the first entry after
		 * FSPACE_DELTA (which we stored in fail_start during
		 * the first iteration).
		 *
		 * In the case that fail_start == NULL, we will stop
		 * immediately.
		 */
		if (sel == search_start) {
			sel_idx = FSPACE_DELTA;
			sel = fail_start;
		}
d631 11
a641 14
#ifdef PMAP_PREFER
	/*
	 * If we can't satisfy pmap_prefer, we try without.
	 *
	 * We retry even in the case align is specified, since
	 * uvm_map_sel_limits() always attempts to take it into
	 * account.
	 */
	if (pmap_align != 0) {
		printf("pmap_prefer aligned allocation failed -> "
		    "going for unaligned mapping\n"); /* DEBUG, for now */
		pmap_align = 0;
		pmap_off = 0;
		goto pmap_prefer_retry;
a642 1
#endif /* PMAP_PREFER */
d644 1
a644 4
	/*
	 * Iterated everything, but nothing was good enough.
	 */
	return NULL;
d648 6
a653 5
 * Test if memory starting at addr with sz bytes is free.
 *
 * Fills in *start_ptr and *end_ptr to be the first and last entry describing
 * the space.
 * If called with prefilled *start_ptr and *end_ptr, they are to be correct.
d655 3
a657 3
int
uvm_map_isavail(struct uvm_map_addr *atree, struct vm_map_entry **start_ptr,
    struct vm_map_entry **end_ptr, vaddr_t addr, vsize_t sz)
d659 8
a666 1
	struct vm_map_entry *i, *i_end;
d668 12
a679 17
	KDASSERT(atree != NULL && start_ptr != NULL && end_ptr != NULL);
	if (*start_ptr == NULL) {
		*start_ptr = uvm_map_entrybyaddr(atree, addr);
		if (*start_ptr == NULL)
			return 0;
	} else
		KASSERT(*start_ptr == uvm_map_entrybyaddr(atree, addr));
	if (*end_ptr == NULL) {
		if (FREE_END(*start_ptr) >= addr + sz)
			*end_ptr = *start_ptr;
		else {
			*end_ptr = uvm_map_entrybyaddr(atree, addr + sz - 1);
			if (*end_ptr == NULL)
				return 0;
		}
	} else
		KASSERT(*end_ptr == uvm_map_entrybyaddr(atree, addr + sz - 1));
d681 1
a681 3
	KDASSERT(*start_ptr != NULL && *end_ptr != NULL);
	KDASSERT((*start_ptr)->start <= addr && FREE_END(*start_ptr) > addr &&
	    (*end_ptr)->start < addr + sz && FREE_END(*end_ptr) >= addr + sz);
d683 9
a691 8
	i = *start_ptr;
	if (i->end > addr)
		return 0;
	i_end = RB_NEXT(uvm_map_addr, atree, *end_ptr);
	for (i = RB_NEXT(uvm_map_addr, atree, i); i != i_end;
	    i = RB_NEXT(uvm_map_addr, atree, i)) {
		if (i->start != i->end)
			return 0;
d693 2
a695 2
	return -1;
}
d698 4
a701 1
 * uvm_map: establish a valid mapping in map
d703 5
a707 3
 * => *addr and sz must be a multiple of PAGE_SIZE.
 * => *addr is ignored, except if flags contains UVM_FLAG_FIXED.
 * => map must be unlocked.
d709 13
a721 8
 *	[1] <NULL,uoffset>		== uoffset is a hint for PMAP_PREFER
 *	[2] <NULL,UVM_UNKNOWN_OFFSET>	== don't PMAP_PREFER
 *	[3] <uobj,uoffset>		== normal mapping
 *	[4] <uobj,UVM_UNKNOWN_OFFSET>	== uvm_map finds offset based on VA
 *
 *   case [4] is for kernel mappings where we don't know the offset until
 *   we've found a virtual address.   note that kernel object offsets are
 *   always relative to vm_map_min(kernel_map).
d723 1
a723 2
 * => align: align vaddr, must be a power-of-2.
 *    Align is only a hint and will be ignored if the alignemnt fails.
d725 1
d727 61
a787 11
uvm_map(struct vm_map *map, vaddr_t *addr, vsize_t sz,
    struct uvm_object *uobj, voff_t uoffset, vsize_t align, uvm_flag_t flags)
{
	struct vm_map_entry	*first, *last, *entry;
	struct uvm_map_deadq	 dead;
	struct uvm_map_free	*free;
	vm_prot_t		 prot;
	vm_prot_t		 maxprot;
	vm_inherit_t		 inherit;
	int			 advice;
	int			 error;
d795 1
a795 1
	 * Decode parameters.
a796 8
	prot = UVM_PROTECTION(flags);
	maxprot = UVM_MAXPROTECTION(flags);
	advice = UVM_ADVICE(flags);
	inherit = UVM_INHERIT(flags);
	error = 0;
	TAILQ_INIT(&dead);
	KASSERT((sz & PAGE_MASK) == 0);
	KASSERT((align & (align - 1)) == 0);
d798 4
a801 6
	/*
	 * Holes are incompatible with other types of mappings.
	 */
	if (flags & UVM_FLAG_HOLE) {
		KASSERT(uobj == NULL && (flags & UVM_FLAG_FIXED) &&
		    (flags & (UVM_FLAG_OVERLAY | UVM_FLAG_COPYONW)) == 0);
d805 1
a805 1
	 * Check protection.
a806 2
	if ((prot & maxprot) != prot)
		return EACCES;
d808 11
a818 5
	if (flags & UVM_FLAG_TRYLOCK) {
		if (vm_map_lock_try(map) == FALSE)
			return EFAULT;
	} else
		vm_map_lock(map);
d820 2
a821 2
	first = last = NULL;
	if (flags & UVM_FLAG_FIXED) {
d823 2
a824 4
		 * Fixed location.
		 *
		 * Note: we ignore align, pmap_prefer.
		 * Fill in first, last and *addr.
d826 30
a855 4
		KASSERT((*addr & PAGE_MASK) == 0);
		if (!uvm_map_isavail(&map->addr, &first, &last, *addr, sz)) {
			error = ENOMEM;
			goto unlock;
d857 27
d885 3
a887 8
		/*
		 * Grow pmap to include allocated address.
		 * XXX not possible in kernel?
		 */
		if ((map->flags & VM_MAP_ISVMSPACE) == 0 &&
		    uvm_maxkaddr < (*addr + sz)) {
			uvm_map_kmem_grow(map, &dead,
			    *addr + sz - uvm_maxkaddr, flags);
a888 20
			/*
			 * Reload first, last, since uvm_map_kmem_grow likely
			 * moved them around.
			 */
			first = last = NULL;
			if (!uvm_map_isavail(&map->addr, &first, &last,
			    *addr, sz))
				panic("uvm_map: opened box, cat died");
		}
	} else if (*addr != 0 && (*addr & PAGE_MASK) == 0 &&
	    (map->flags & VM_MAP_ISVMSPACE) == VM_MAP_ISVMSPACE &&
	    (align == 0 || (*addr & (align - 1)) == 0) &&
	    uvm_map_isavail(&map->addr, &first, &last, *addr, sz)) {
		/*
		 * Address used as hint.
		 *
		 * Note: we enforce the alignment restriction,
		 * but ignore the pmap_prefer.
		 */
	} else {
d890 3
a892 1
		 * Update freelists from vmspace.
a893 2
		if (map->flags & VM_MAP_ISVMSPACE)
			uvm_map_vmspace_update(map, &dead, flags);
d895 4
a898 8
		/*
		 * Allocation for sz bytes at any address on the
		 * freelist.
		 */
		free = &map->free;
		first = uvm_map_findspace_tree(free, sz, uoffset, align,
		    map->flags & VM_MAP_GUARDPAGES, addr, map);
		last = NULL; /* May get set in previous test (by isavail). */
d901 2
a902 1
		 * Fall back into brk() space if the initial attempt failed.
d904 4
a907 5
		if (first == NULL) {
			if (map->flags & VM_MAP_ISVMSPACE)
				free = &map->bfree;
			else
				uvm_map_kmem_grow(map, &dead, sz, flags);
d909 4
a912 6
			first = uvm_map_findspace_tree(free, sz, uoffset, align,
			    map->flags & VM_MAP_GUARDPAGES, addr, map);
			if (first == NULL) {
				error = ENOMEM;
				goto unlock;
			}
d915 3
d919 2
a920 1
		 * Fill in last.
a921 3
		if (!uvm_map_isavail(&map->addr, &first, &last, *addr, sz))
			panic("uvm_map: findspace and isavail disagree");
	}
d923 8
a930 2
	KASSERT((map->flags & VM_MAP_ISVMSPACE) == VM_MAP_ISVMSPACE ||
	    uvm_maxkaddr >= *addr + sz);
d932 1
a932 7
	/*
	 * If we only want a query, return now.
	 */
	if (flags & UVM_FLAG_QUERY) {
		error = 0;
		goto unlock;
	}
d934 3
a936 6
	if (uobj == NULL)
		uoffset = 0;
	else if (uoffset == UVM_UNKNOWN_OFFSET) {
		KASSERT(UVM_OBJ_IS_KERN_OBJECT(uobj));
		uoffset = *addr - vm_map_min(kernel_map);
	}
a937 33
	/*
	 * Create new entry.
	 * first and last may be invalidated after this call.
	 */
	entry = uvm_map_mkentry(map, first, last, *addr, sz, flags, &dead);
	if (entry == NULL) {
		error = ENOMEM;
		goto unlock;
	}
	KDASSERT(entry->start == *addr && entry->end == *addr + sz);
	entry->object.uvm_obj = uobj;
	entry->offset = uoffset;
	entry->protection = prot;
	entry->max_protection = maxprot;
	entry->inheritance = inherit;
	entry->wired_count = 0;
	entry->advice = advice;
	if (uobj)
		entry->etype = UVM_ET_OBJ;
	else if (flags & UVM_FLAG_HOLE)
		entry->etype = UVM_ET_HOLE;
	else
		entry->etype = 0;
	if (flags & UVM_FLAG_COPYONW) {
		entry->etype |= UVM_ET_COPYONWRITE;
		if ((flags & UVM_FLAG_OVERLAY) == 0)
			entry->etype |= UVM_ET_NEEDSCOPY;
	}
	if (flags & UVM_FLAG_OVERLAY) {
		entry->aref.ar_pageoff = 0;
		entry->aref.ar_amap = amap_alloc(sz,
		    ptoa(flags & UVM_FLAG_AMAPPAD ? UVM_AMAP_CHUNK : 0),
		    M_WAITOK);
d939 2
d943 2
a944 1
	 * Update map and process statistics.
d946 5
a950 7
	if (!(flags & UVM_FLAG_HOLE))
		map->size += sz;
	if ((map->flags & VM_MAP_ISVMSPACE) && uobj == NULL &&
	    !(flags & UVM_FLAG_HOLE)) {
		((struct vmspace *)map)->vm_dused +=
		    uvmspace_dused(map, *addr, *addr + sz);
	}
d953 1
a953 4
	 * Try to merge entry.
	 *
	 * XXX: I can't think of a good reason to only merge kernel map entries,
	 * but it's what the old code did. I'll look at it later.
a954 2
	if ((flags & UVM_FLAG_NOMERGE) == 0)
		entry = uvm_mapent_tryjoin(map, entry, &dead);
d956 19
a974 2
unlock:
	vm_map_unlock(map);
d976 4
a979 4
	if (error == 0) {
		DPRINTF(("uvm_map:   0x%lx-0x%lx (query=%c)  map=%p\n",
		    *addr, *addr + sz,
		    (flags & UVM_FLAG_QUERY ? 'T' : 'F'), map));
d981 2
d984 19
a1002 10
	/*
	 * Remove dead entries.
	 *
	 * Dead entries may be the result of merging.
	 * uvm_map_mkentry may also create dead entries, when it attempts to
	 * destroy free-space entries.
	 */
	uvm_unmap_detach(&dead, 0);
	return error;
}
d1004 1
a1004 8
/*
 * True iff e1 and e2 can be joined together.
 */
int
uvm_mapent_isjoinable(struct vm_map *map, struct vm_map_entry *e1,
    struct vm_map_entry *e2)
{
	KDASSERT(e1 != NULL && e2 != NULL);
d1006 3
a1008 5
	/*
	 * Must be the same entry type and not have free memory between.
	 */
	if (e1->etype != e2->etype || e1->end != e2->start)
		return 0;
a1009 5
	/*
	 * Submaps are never joined.
	 */
	if (UVM_ET_ISSUBMAP(e1))
		return 0;
d1012 1
a1012 1
	 * Never merge wired memory.
a1013 2
	if (VM_MAPENT_ISWIRED(e1) || VM_MAPENT_ISWIRED(e2))
		return 0;
d1015 3
a1017 8
	/*
	 * Protection, inheritance and advice must be equal.
	 */
	if (e1->protection != e2->protection ||
	    e1->max_protection != e2->max_protection ||
	    e1->inheritance != e2->inheritance ||
	    e1->advice != e2->advice)
		return 0;
d1019 1
d1021 1
a1021 1
	 * If uvm_object: objects itself and offsets within object must match.
d1023 19
a1041 5
	if (UVM_ET_ISOBJ(e1)) {
		if (e1->object.uvm_obj != e2->object.uvm_obj)
			return 0;
		if (e1->offset + (e1->end - e1->start) != e2->offset)
			return 0;
d1043 1
d1045 1
a1045 10
	/*
	 * Cannot join shared amaps.
	 * Note: no need to lock amap to look at refs, since we don't care
	 * about its exact value.
	 * If it is 1 (i.e. we have the only reference) it will stay there.
	 */
	if (e1->aref.ar_amap && amap_refs(e1->aref.ar_amap) != 1)
		return 0;
	if (e2->aref.ar_amap && amap_refs(e2->aref.ar_amap) != 1)
		return 0;
d1047 3
a1049 4
	/*
	 * Apprently, e1 and e2 match.
	 */
	return 1;
d1053 1
a1053 1
 * Join support function.
d1055 3
a1057 2
 * Returns the merged entry on succes.
 * Returns NULL if the merge failed.
d1059 4
a1062 3
struct vm_map_entry*
uvm_mapent_merge(struct vm_map *map, struct vm_map_entry *e1,
    struct vm_map_entry *e2, struct uvm_map_deadq *dead)
d1064 5
a1068 1
	struct uvm_map_free *free;
d1070 2
a1071 9
	/*
	 * Amap of e1 must be extended to include e2.
	 * e2 contains no real information in its amap,
	 * so it can be erased immediately.
	 */
	if (e1->aref.ar_amap) {
		if (amap_extend(e1, e2->end - e2->start))
			return NULL;
	}
d1074 2
a1075 2
	 * Don't drop obj reference:
	 * uvm_unmap_detach will do this for us.
d1078 3
a1080 9
	free = UVM_FREE(map, FREE_START(e2));
	if (e2->fspace > 0 && free)
		uvm_mapent_free_remove(map, free, e2);
	uvm_mapent_addr_remove(map, e2);
	e1->end = e2->end;
	e1->guard = e2->guard;
	e1->fspace = e2->fspace;
	if (e1->fspace > 0 && free)
		uvm_mapent_free_insert(map, free, e1);
d1082 2
a1083 3
	DEAD_ENTRY_PUSH(dead, e2);
	return e1;
}
d1085 21
a1105 13
/*
 * Attempt forward and backward joining of entry.
 *
 * Returns entry after joins.
 * We are guaranteed that the amap of entry is either non-existant or
 * has never been used.
 */
struct vm_map_entry*
uvm_mapent_tryjoin(struct vm_map *map, struct vm_map_entry *entry,
    struct uvm_map_deadq *dead)
{
	struct vm_map_entry *other;
	struct vm_map_entry *merged;
d1107 9
a1115 9
	/*
	 * Merge with previous entry.
	 */
	other = RB_PREV(uvm_map_addr, &map->addr, entry);
	if (other && uvm_mapent_isjoinable(map, other, entry)) {
		merged = uvm_mapent_merge(map, other, entry, dead);
		DPRINTF(("prev merge: %p + %p -> %p\n", other, entry, merged));
		if (merged)
			entry = merged;
d1118 1
a1118 16
	/*
	 * Merge with next entry.
	 *
	 * Because amap can only extend forward and the next entry
	 * probably contains sensible info, only perform forward merging
	 * in the absence of an amap.
	 */
	other = RB_NEXT(uvm_map_addr, &map->addr, entry);
	if (other && entry->aref.ar_amap == NULL &&
	    other->aref.ar_amap == NULL &&
	    uvm_mapent_isjoinable(map, entry, other)) {
		merged = uvm_mapent_merge(map, entry, other, dead);
		DPRINTF(("next merge: %p + %p -> %p\n", entry, other, merged));
		if (merged)
			entry = merged;
	}
d1120 3
a1122 10
	return entry;
}

/*
 * Kill entries that are no longer in a map.
 */
void
uvm_unmap_detach(struct uvm_map_deadq *deadq, int flags)
{
	struct vm_map_entry *entry;
a1123 1
	while ((entry = TAILQ_FIRST(deadq)) != NULL) {
d1125 2
a1126 1
		 * Drop reference to amap, if we've got one.
d1128 11
a1138 16
		if (entry->aref.ar_amap)
			amap_unref(entry->aref.ar_amap,
			    entry->aref.ar_pageoff,
			    atop(entry->end - entry->start),
			    flags);

		/*
		 * Drop reference to our backing object, if we've got one.
		 */
		if (UVM_ET_ISSUBMAP(entry)) {
			/* ... unlikely to happen, but play it safe */
			uvm_map_deallocate(entry->object.sub_map);
		} else if (UVM_ET_ISOBJ(entry) &&
		    entry->object.uvm_obj->pgops->pgo_detach) {
			entry->object.uvm_obj->pgops->pgo_detach(
			    entry->object.uvm_obj);
d1140 3
a1142 6

		/*
		 * Step to next.
		 */
		TAILQ_REMOVE(deadq, entry, daddrs.deadq);
		uvm_mapent_free(entry);
a1143 26
}

/*
 * Create and insert new entry.
 *
 * Returned entry contains new addresses and is inserted properly in the tree.
 * first and last are (probably) no longer valid.
 */
struct vm_map_entry*
uvm_map_mkentry(struct vm_map *map, struct vm_map_entry *first,
    struct vm_map_entry *last, vaddr_t addr, vsize_t sz, int flags,
    struct uvm_map_deadq *dead)
{
	struct vm_map_entry *entry, *prev;
	struct uvm_map_free *free;
	vaddr_t min, max;	/* free space boundaries for new entry */

	KDASSERT(map != NULL && first != NULL && last != NULL && dead != NULL &&
	    sz > 0 && addr + sz > addr);
	KDASSERT(first->end <= addr && FREE_END(first) > addr);
	KDASSERT(last->start < addr + sz && FREE_END(last) >= addr + sz);
	KDASSERT(uvm_map_isavail(&map->addr, &first, &last, addr, sz));
	uvm_tree_sanity(map, __FILE__, __LINE__);

	min = addr + sz;
	max = FREE_END(last);
d1146 1
a1146 1
	 * Initialize new entry.
a1147 8
	entry = uvm_mapent_alloc(map, flags);
	if (entry == NULL)
		return NULL;
	entry->offset = 0;
	entry->etype = 0;
	entry->wired_count = 0;
	entry->aref.ar_pageoff = 0;
	entry->aref.ar_amap = NULL;
d1149 7
a1155 4
	entry->start = addr;
	entry->end = min;
	entry->guard = 0;
	entry->fspace = 0;
d1157 9
a1165 22
	/*
	 * Reset free space in first.
	 */
	free = UVM_FREE(map, FREE_START(first));
	if (free)
		uvm_mapent_free_remove(map, free, first);
	first->guard = 0;
	first->fspace = 0;

	/*
	 * Remove all entries that are fully replaced.
	 * We are iterating using last in reverse order.
	 */
	for (; first != last; last = prev) {
		prev = RB_PREV(uvm_map_addr, &map->addr, last);

		KDASSERT(last->start == last->end);
		free = UVM_FREE(map, FREE_START(last));
		if (free && last->fspace > 0)
			uvm_mapent_free_remove(map, free, last);
		uvm_mapent_addr_remove(map, last);
		DEAD_ENTRY_PUSH(dead, last);
a1166 14
	/*
	 * Remove first if it is entirely inside <addr, addr+sz>.
	 */
	if (first->start == addr) {
		uvm_mapent_addr_remove(map, first);
		DEAD_ENTRY_PUSH(dead, first);
	} else
		uvm_map_fix_space(map, first, FREE_START(first), addr, flags);

	/*
	 * Finally, link in entry.
	 */
	uvm_mapent_addr_insert(map, entry);
	uvm_map_fix_space(map, entry, min, max, flags);
d1168 4
a1171 2
	uvm_tree_sanity(map, __FILE__, __LINE__);
	return entry;
d1175 3
a1177 1
 * uvm_mapent_alloc: allocate a map entry
a1178 7
struct vm_map_entry *
uvm_mapent_alloc(struct vm_map *map, int flags)
{
	struct vm_map_entry *me, *ne;
	int s, i;
	int pool_flags;
	UVMHIST_FUNC("uvm_mapent_alloc"); UVMHIST_CALLED(maphist);
d1180 3
a1182 65
	pool_flags = PR_WAITOK;
	if (flags & UVM_FLAG_TRYLOCK)
		pool_flags = PR_NOWAIT;

	if (map->flags & VM_MAP_INTRSAFE || cold) {
		s = splvm();
		simple_lock(&uvm.kentry_lock);
		me = uvm.kentry_free;
		if (me == NULL) {
			ne = km_alloc(PAGE_SIZE, &kv_page, &kp_dirty,
			    &kd_nowait);
			if (ne == NULL)
				panic("uvm_mapent_alloc: cannot allocate map "
				    "entry");
			for (i = 0;
			    i < PAGE_SIZE / sizeof(struct vm_map_entry) - 1;
			    i++)
				RB_LEFT(&ne[i], daddrs.addr_entry) = &ne[i + 1];
			RB_LEFT(&ne[i], daddrs.addr_entry) = NULL;
			me = ne;
			if (ratecheck(&uvm_kmapent_last_warn_time,
			    &uvm_kmapent_warn_rate))
				printf("uvm_mapent_alloc: out of static "
				    "map entries\n");
		}
		uvm.kentry_free = RB_LEFT(me, daddrs.addr_entry);
		uvmexp.kmapent++;
		simple_unlock(&uvm.kentry_lock);
		splx(s);
		me->flags = UVM_MAP_STATIC;
	} else if (map == kernel_map) {
		splassert(IPL_NONE);
		me = pool_get(&uvm_map_entry_kmem_pool, pool_flags);
		if (me == NULL)
			goto out;
		me->flags = UVM_MAP_KMEM;
	} else {
		splassert(IPL_NONE);
		me = pool_get(&uvm_map_entry_pool, pool_flags);
		if (me == NULL)
			goto out;
		me->flags = 0;
	}

	if (me != NULL) {
		RB_LEFT(me, free_entry) = RB_RIGHT(me, free_entry) =
		    RB_PARENT(me, free_entry) = UVMMAP_DEADBEEF;
		RB_LEFT(me, daddrs.addr_entry) =
		    RB_RIGHT(me, daddrs.addr_entry) =
		    RB_PARENT(me, daddrs.addr_entry) = UVMMAP_DEADBEEF;
	}

out:
	UVMHIST_LOG(maphist, "<- new entry=%p [kentry=%ld]", me,
	    ((map->flags & VM_MAP_INTRSAFE) != 0 || map == kernel_map), 0, 0);
	return(me);
}

/*
 * uvm_mapent_free: free map entry
 *
 * => XXX: static pool for kernel map?
 */
void
uvm_mapent_free(struct vm_map_entry *me)
d1184 2
a1185 2
	int s;
	UVMHIST_FUNC("uvm_mapent_free"); UVMHIST_CALLED(maphist);
d1187 20
a1206 41
	UVMHIST_LOG(maphist,"<- freeing map entry=%p [flags=%ld]",
		me, me->flags, 0, 0);

	if (RB_LEFT(me, free_entry) != UVMMAP_DEADBEEF ||
	    RB_RIGHT(me, free_entry) != UVMMAP_DEADBEEF ||
	    RB_PARENT(me, free_entry) != UVMMAP_DEADBEEF)
		panic("uvm_mapent_free: mapent %p still in free list\n", me);

	if (me->flags & UVM_MAP_STATIC) {
		s = splvm();
		simple_lock(&uvm.kentry_lock);
		RB_LEFT(me, daddrs.addr_entry) = uvm.kentry_free;
		uvm.kentry_free = me;
		uvmexp.kmapent--;
		simple_unlock(&uvm.kentry_lock);
		splx(s);
	} else if (me->flags & UVM_MAP_KMEM) {
		splassert(IPL_NONE);
		pool_put(&uvm_map_entry_kmem_pool, me);
	} else {
		splassert(IPL_NONE);
		pool_put(&uvm_map_entry_pool, me);
	}
}

/*
 * uvm_map_lookup_entry: find map entry at or before an address.
 *
 * => map must at least be read-locked by caller
 * => entry is returned in "entry"
 * => return value is true if address is in the returned entry
 * ET_HOLE entries are considered to not contain a mapping, ergo FALSE is
 * returned for those mappings.
 */
boolean_t
uvm_map_lookup_entry(struct vm_map *map, vaddr_t address,
    struct vm_map_entry **entry)
{
	*entry = uvm_map_entrybyaddr(&map->addr, address);
	return *entry != NULL && !UVM_ET_ISHOLE(*entry) &&
	    (*entry)->start <= address && (*entry)->end > address;
d1213 1
d1248 48
a1295 2
void
uvm_unmap(struct vm_map *map, vaddr_t start, vaddr_t end)
d1297 166
a1462 1
	struct uvm_map_deadq dead;
d1464 29
a1492 5
	KASSERT((start & PAGE_MASK) == 0 && (end & PAGE_MASK) == 0);
	TAILQ_INIT(&dead);
	vm_map_lock(map);
	uvm_unmap_remove(map, start, end, &dead, FALSE, TRUE);
	vm_map_unlock(map);
d1494 9
a1502 1
	uvm_unmap_detach(&dead, 0);
d1506 5
a1510 1
 * Mark entry as free.
d1512 2
a1513 3
 * entry will be put on the dead list.
 * The free space will be merged into the previous or a new entry,
 * unless markfree is false.
d1516 8
a1523 16
uvm_mapent_mkfree(struct vm_map *map, struct vm_map_entry *entry,
    struct vm_map_entry **prev_ptr, struct uvm_map_deadq *dead,
    boolean_t markfree)
{
	struct uvm_map_free	*free;
	struct vm_map_entry	*prev;
	vaddr_t			 addr;	/* Start of freed range. */
	vaddr_t			 end;	/* End of freed range. */

	prev = *prev_ptr;
	if (prev == entry)
		*prev_ptr = prev = NULL;

	if (prev == NULL ||
	    FREE_END(prev) != entry->start)
		prev = RB_PREV(uvm_map_addr, &map->addr, entry);
d1525 2
a1526 1
	 * Entry is describing only free memory and has nothing to drain into.
d1528 3
a1530 4
	if (prev == NULL && entry->start == entry->end && markfree) {
		*prev_ptr = entry;
		return;
	}
d1532 2
a1533 7
	addr = entry->start;
	end = FREE_END(entry);
	free = UVM_FREE(map, FREE_START(entry));
	if (entry->fspace > 0 && free)
		uvm_mapent_free_remove(map, free, entry);
	uvm_mapent_addr_remove(map, entry);
	DEAD_ENTRY_PUSH(dead, entry);
d1535 1
a1535 2
	if (markfree)
		*prev_ptr = uvm_map_fix_space(map, prev, addr, end, 0);
d1538 5
d1544 1
a1544 1
 * Remove all entries from start to end.
d1546 4
a1549 3
 * If remove_holes, then remove ET_HOLE entries as well.
 * If markfree, entry will be properly marked free, otherwise, no replacement
 * entry will be put in the tree (corrupting the tree).
d1551 1
d1554 1
a1554 2
    struct uvm_map_deadq *dead, boolean_t remove_holes,
    boolean_t markfree)
d1556 9
a1564 1
	struct vm_map_entry *prev_hint, *next, *entry;
d1566 1
a1566 4
	start = MAX(start, map->min_offset);
	end = MIN(end, map->max_offset);
	if (start >= end)
		return;
d1574 1
a1574 1
	 * Find first affected entry.
d1576 3
a1578 5
	entry = uvm_map_entrybyaddr(&map->addr, start);
	KDASSERT(entry != NULL && entry->start <= start);
	if (entry->end <= start && markfree)
		entry = RB_NEXT(uvm_map_addr, &map->addr, entry);
	else
d1580 2
d1583 3
a1585 4
	DPRINTF(("uvm_unmap_p: 0x%lx-0x%lx\n"
	    "\tfirst 0x%lx-0x%lx\n",
	    start, end,
	    entry->start, entry->end));
d1588 5
a1592 13
	 * Iterate entries until we reach end address.
	 * prev_hint hints where the freed space can be appended to.
	 */
	prev_hint = NULL;
	for (; entry != NULL && entry->start < end; entry = next) {
		KDASSERT(entry->start >= start);
		if (entry->end > end || !markfree)
			UVM_MAP_CLIP_END(map, entry, end);
		KDASSERT(entry->start >= start && entry->end <= end);
		next = RB_NEXT(uvm_map_addr, &map->addr, entry);
		DPRINTF(("\tunmap 0x%lx-0x%lx used  0x%lx-0x%lx free\n",
		    entry->start, entry->end,
		    FREE_START(entry), FREE_END(entry)));
d1594 39
a1632 7
		/*
		 * Unwire removed map entry.
		 */
		if (VM_MAPENT_ISWIRED(entry)) {
			entry->wired_count = 0;
			uvm_fault_unwire_locked(map, entry->start, entry->end);
		}
d1635 2
a1636 1
		 * Entry-type specific code.
d1638 6
a1644 3
			/*
			 * Skip holes unless remove_holes.
			 */
d1646 1
a1646 1
				prev_hint = entry;
a1649 1
			KASSERT(vm_map_pmap(map) == pmap_kernel());
d1651 1
a1651 1
			pmap_kremove(entry->start, entry->end - entry->start);
d1657 1
a1657 1
			 * Note: kernel object mappings are currently used in
d1677 2
a1678 2
			 * uvm_km_pgremove currently does the following:
			 *   for pages in the kernel object range:
d1701 2
a1702 1
			entry->object.uvm_obj = NULL;  /* to be safe */
d1705 2
a1706 2
			 * remove mappings the standard way.
			 */
d1711 2
a1712 1
		 * Update space usage.
d1714 4
a1717 8
		if ((map->flags & VM_MAP_ISVMSPACE) &&
		    entry->object.uvm_obj == NULL &&
		    !UVM_ET_ISHOLE(entry)) {
			((struct vmspace *)map)->vm_dused -=
			    uvmspace_dused(map, entry->start, entry->end);
		}
		if (!UVM_ET_ISHOLE(entry))
			map->size -= entry->end - entry->start;
d1719 12
d1732 2
a1733 1
		 * Actual removal of entry.
d1735 8
a1742 1
		uvm_mapent_mkfree(map, entry, &prev_hint, dead, markfree);
d1744 4
a1748 1
	pmap_update(vm_map_pmap(map));
d1750 6
a1755 2
	DPRINTF(("uvm_unmap_p: 0x%lx-0x%lx            map=%p\n", start, end,
	    map));
d1757 2
a1758 15
#ifdef DEBUG
	if (markfree) {
		for (entry = uvm_map_entrybyaddr(&map->addr, start);
		    entry != NULL && entry->start < end;
		    entry = RB_NEXT(uvm_map_addr, &map->addr, entry)) {
			KDASSERT(entry->end <= start ||
			    entry->start == entry->end ||
			    UVM_ET_ISHOLE(entry));
		}
	} else {
		vaddr_t a;
		for (a = start; a < end; a += PAGE_SIZE)
			KDASSERT(uvm_map_entrybyaddr(&map->addr, a) == NULL);
	}
#endif
d1762 1
a1762 1
 * Mark all entries from first until end (exclusive) as pageable.
d1764 1
a1764 1
 * Lock must be exclusive on entry and will not be touched.
d1766 1
d1768 1
a1768 2
uvm_map_pageable_pgon(struct vm_map *map, struct vm_map_entry *first,
    struct vm_map_entry *end, vaddr_t start_addr, vaddr_t end_addr)
d1770 10
a1779 1
	struct vm_map_entry *iter;
d1781 20
a1800 5
	for (iter = first; iter != end;
	    iter = RB_NEXT(uvm_map_addr, &map->addr, iter)) {
		KDASSERT(iter->start >= start_addr && iter->end <= end_addr);
		if (!VM_MAPENT_ISWIRED(iter) || UVM_ET_ISHOLE(iter))
			continue;
d1802 3
a1804 2
		iter->wired_count = 0;
		uvm_fault_unwire_locked(map, iter->start, iter->end);
d1806 1
d1810 5
a1814 1
 * Mark all entries from first until end (exclusive) as wired.
d1816 5
a1820 2
 * Lockflags determines the lock state on return from this function.
 * Lock must be exclusive on entry.
d1822 1
d1824 2
a1825 3
uvm_map_pageable_wire(struct vm_map *map, struct vm_map_entry *first,
    struct vm_map_entry *end, vaddr_t start_addr, vaddr_t end_addr,
    int lockflags)
d1827 1
a1827 5
	struct vm_map_entry *iter;
#ifdef DIAGNOSTIC
	unsigned int timestamp_save;
#endif
	int error;
d1829 2
a1830 22
	/*
	 * Wire pages in two passes:
	 *
	 * 1: holding the write lock, we create any anonymous maps that need
	 *    to be created.  then we clip each map entry to the region to
	 *    be wired and increment its wiring count.
	 *
	 * 2: we downgrade to a read lock, and call uvm_fault_wire to fault
	 *    in the pages for any newly wired area (wired_count == 1).
	 *
	 *    downgrading to a read lock for uvm_fault_wire avoids a possible
	 *    deadlock with another thread that may have faulted on one of
	 *    the pages to be wired (it would mark the page busy, blocking
	 *    us, then in turn block on the map lock that we hold).
	 *    because we keep the read lock on the map, the copy-on-write
	 *    status of the entries we modify here cannot change.
	 */
	for (iter = first; iter != end;
	    iter = RB_NEXT(uvm_map_addr, &map->addr, iter)) {
		KDASSERT(iter->start >= start_addr && iter->end <= end_addr);
		if (UVM_ET_ISHOLE(iter) || iter->start == iter->end)
			continue;
d1832 3
a1834 15
		/*
		 * Perform actions of vm_map_lookup that need the write lock.
		 * - create an anonymous map for copy-on-write
		 * - anonymous map for zero-fill
		 * Skip submaps.
		 */
		if (!VM_MAPENT_ISWIRED(iter) && !UVM_ET_ISSUBMAP(iter) &&
		    UVM_ET_ISNEEDSCOPY(iter) &&
		    ((iter->protection & VM_PROT_WRITE) ||
		    iter->object.uvm_obj == NULL)) {
			amap_copy(map, iter, M_WAITOK, TRUE,
			    iter->start, iter->end);
		}
		iter->wired_count++;
	}
d1837 1
a1837 1
	 * Pass 2.
a1838 52
#ifdef DIAGNOSTIC
	timestamp_save = map->timestamp;
#endif
	vm_map_busy(map);
	vm_map_downgrade(map);

	error = 0;
	for (iter = first; error == 0 && iter != end;
	    iter = RB_NEXT(uvm_map_addr, &map->addr, iter)) {
		if (UVM_ET_ISHOLE(iter) || iter->start == iter->end)
			continue;

		error = uvm_fault_wire(map, iter->start, iter->end,
		    iter->protection);
	}

	if (error) {
		/*
		 * uvm_fault_wire failure
		 *
		 * Reacquire lock and undo our work.
		 */
		vm_map_upgrade(map);
		vm_map_unbusy(map);
#ifdef DIAGNOSTIC
		if (timestamp_save != map->timestamp)
			panic("uvm_map_pageable_wire: stale map");
#endif

		/*
		 * first is no longer needed to restart loops.
		 * Use it as iterator to unmap successful mappings.
		 */
		for (; first != iter;
		    first = RB_NEXT(uvm_map_addr, &map->addr, first)) {
			if (UVM_ET_ISHOLE(first) || first->start == first->end)
				continue;

			first->wired_count--;
			if (!VM_MAPENT_ISWIRED(first)) {
				uvm_fault_unwire_locked(map,
				    iter->start, iter->end);
			}
		}

		/*
		 * decrease counter in the rest of the entries
		 */
		for (; iter != end;
		    iter = RB_NEXT(uvm_map_addr, &map->addr, iter)) {
			if (UVM_ET_ISHOLE(iter) || iter->start == iter->end)
				continue;
d1840 6
a1845 7
			iter->wired_count--;
		}

		if ((lockflags & UVM_LK_EXIT) == 0)
			vm_map_unlock(map);
		return error;
	}
d1847 2
a1848 15
	/*
	 * We are currently holding a read lock.
	 */
	if ((lockflags & UVM_LK_EXIT) == 0) {
		vm_map_unbusy(map);
		vm_map_unlock_read(map);
	} else {
		vm_map_upgrade(map);
		vm_map_unbusy(map);
#ifdef DIAGNOSTIC
		if (timestamp_save != map->timestamp)
			panic("uvm_map_pageable_wire: stale map");
#endif
	}
	return 0;
d1852 2
a1853 5
 * uvm_map_pageable: set pageability of a range in a map.
 *
 * Flags:
 * UVM_LK_ENTER: map is already locked by caller
 * UVM_LK_EXIT:  don't unlock map on exit
d1855 5
a1859 2
 * The full range must be in use (entries may not have fspace != 0).
 * UVM_ET_HOLE counts as unmapped.
d1861 1
d1863 2
a1864 2
uvm_map_pageable(struct vm_map *map, vaddr_t start, vaddr_t end,
    boolean_t new_pageable, int lockflags)
d1866 1
a1866 2
	struct vm_map_entry *first, *last, *tmp;
	int error;
d1868 1
a1868 10
	if (start > end)
		return EINVAL;
	if (start < map->min_offset)
		return EFAULT; /* why? see first XXX below */
	if (end > map->max_offset)
		return EINVAL; /* why? see second XXX below */

	KASSERT(map->flags & VM_MAP_PAGEABLE);
	if ((lockflags & UVM_LK_ENTER) == 0)
		vm_map_lock(map);
d1871 1
a1871 4
	 * Find first entry.
	 *
	 * Initial test on start is different, because of the different
	 * error returned. Rest is tested further down.
d1873 3
a1875 7
	first = uvm_map_entrybyaddr(&map->addr, start);
	if (first->end <= start || UVM_ET_ISHOLE(first)) {
		/*
		 * XXX if the first address is not mapped, it is EFAULT?
		 */
		error = EFAULT;
		goto out;
d1879 1
a1879 1
	 * Check that the range has no holes.
d1881 4
a1884 11
	for (last = first; last != NULL && last->start < end;
	    last = RB_NEXT(uvm_map_addr, &map->addr, last)) {
		if (UVM_ET_ISHOLE(last) ||
		    (last->end < end && FREE_END(last) != last->end)) {
			/*
			 * XXX unmapped memory in range, why is it EINVAL
			 * instead of EFAULT?
			 */
			error = EINVAL;
			goto out;
		}
d1887 1
d1889 1
a1889 4
	 * Last ended at the first entry after the range.
	 * Move back one step.
	 *
	 * Note that last may be NULL.
d1891 23
a1913 5
	if (last == NULL) {
		last = RB_MAX(uvm_map_addr, &map->addr);
		if (last->end < end) {
			error = EINVAL;
			goto out;
d1915 4
a1918 2
	} else
		last = RB_PREV(uvm_map_addr, &map->addr, last);
d1921 2
a1922 1
	 * Wire/unwire pages here.
a1923 17
	if (new_pageable) {
		/*
		 * Mark pageable.
		 * entries that are not wired are untouched.
		 */
		if (VM_MAPENT_ISWIRED(first))
			UVM_MAP_CLIP_START(map, first, start);
		/*
		 * Split last at end.
		 * Make tmp be the first entry after what is to be touched.
		 * If last is not wired, don't touch it.
		 */
		if (VM_MAPENT_ISWIRED(last)) {
			UVM_MAP_CLIP_END(map, last, end);
			tmp = RB_NEXT(uvm_map_addr, &map->addr, last);
		} else
			tmp = last;
d1925 22
a1946 2
		uvm_map_pageable_pgon(map, first, tmp, start, end);
		error = 0;
d1948 6
a1953 4
out:
		if ((lockflags & UVM_LK_EXIT) == 0)
			vm_map_unlock(map);
		return error;
a1954 16
		/*
		 * Mark entries wired.
		 * entries are always touched (because recovery needs this).
		 */
		if (!VM_MAPENT_ISWIRED(first))
			UVM_MAP_CLIP_START(map, first, start);
		/*
		 * Split last at end.
		 * Make tmp be the first entry after what is to be touched.
		 * If last is not wired, don't touch it.
		 */
		if (!VM_MAPENT_ISWIRED(last)) {
			UVM_MAP_CLIP_END(map, last, end);
			tmp = RB_NEXT(uvm_map_addr, &map->addr, last);
		} else
			tmp = last;
d1956 7
a1962 2
		return uvm_map_pageable_wire(map, first, tmp, start, end,
		    lockflags);
d1964 10
d1977 2
a1978 2
 * uvm_map_pageable_all: special case of uvm_map_pageable - affects
 * all mapped regions.
d1980 12
a1991 2
 * Map must not be locked.
 * If no flags are specified, all ragions are unwired.
d1993 1
d1995 2
a1996 1
uvm_map_pageable_all(struct vm_map *map, int flags, vsize_t limit)
d1998 7
a2004 5
	vsize_t size;
	struct vm_map_entry *iter;

	KASSERT(map->flags & VM_MAP_PAGEABLE);
	vm_map_lock(map);
d2006 3
a2008 3
	if (flags == 0) {
		uvm_map_pageable_pgon(map, RB_MIN(uvm_map_addr, &map->addr),
		    NULL, map->min_offset, map->max_offset);
d2010 2
a2011 11
		atomic_clearbits_int(&map->flags, VM_MAP_WIREFUTURE);
		vm_map_unlock(map);
		return 0;
	}

	if (flags & MCL_FUTURE)
		atomic_setbits_int(&map->flags, VM_MAP_WIREFUTURE);
	if (!(flags & MCL_CURRENT)) {
		vm_map_unlock(map);
		return 0;
	}
d2014 3
a2016 2
	 * Count number of pages in all non-wired entries.
	 * If the number exceeds the limit, abort.
a2017 12
	size = 0;
	RB_FOREACH(iter, uvm_map_addr, &map->addr) {
		if (VM_MAPENT_ISWIRED(iter) || UVM_ET_ISHOLE(iter))
			continue;

		size += iter->end - iter->start;
	}

	if (atop(size) + uvmexp.wired > uvmexp.wiredmax) {
		vm_map_unlock(map);
		return ENOMEM;
	}
d2019 3
a2021 8
	/* XXX non-pmap_wired_count case must be handled by caller */
#ifdef pmap_wired_count
	if (limit != 0 &&
	    size + ptoa(pmap_wired_count(vm_map_pmap(map))) > limit) {
		vm_map_unlock(map);
		return ENOMEM;
	}
#endif
d2024 1
a2024 1
	 * uvm_map_pageable_wire will release lcok
a2025 3
	return uvm_map_pageable_wire(map, RB_MIN(uvm_map_addr, &map->addr),
	    NULL, map->min_offset, map->max_offset, 0);
}
d2027 5
a2031 10
/*
 * Initialize map.
 *
 * Allocates sufficient entries to describe the free memory in the map.
 */
void
uvm_map_setup(struct vm_map *map, vaddr_t min, vaddr_t max, int flags)
{
	KASSERT((min & PAGE_MASK) == 0);
	KASSERT((max & PAGE_MASK) == 0 || (max & PAGE_MASK) == PAGE_MASK);
d2034 4
a2037 11
	 * Update parameters.
	 *
	 * This code handles (vaddr_t)-1 and other page mask ending addresses
	 * properly.
	 * We lose the top page if the full virtual address space is used.
	 */
	if (max & PAGE_MASK) {
		max += 1;
		if (max == 0) /* overflow */
			max -= PAGE_SIZE;
	}
d2039 5
a2043 5
	RB_INIT(&map->addr);
	RB_INIT(&map->free.tree);
	map->free.treesz = 0;
	RB_INIT(&map->bfree.tree);
	map->bfree.treesz = 0;
d2045 1
a2045 10
	map->size = 0;
	map->ref_count = 1;
	map->min_offset = min;
	map->max_offset = max;
	map->b_start = map->b_end = 0; /* Empty brk() area by default. */
	map->s_start = map->s_end = 0; /* Empty stack area by default. */
	map->flags = flags;
	map->timestamp = 0;
	rw_init(&map->lock, "vmmaplk");
	simple_lock_init(&map->ref_lock);
d2047 2
a2048 14
	/*
	 * Fill map entries.
	 * This requires a write-locked map (because of diagnostic assertions
	 * in insert code).
	 */
	if ((map->flags & VM_MAP_INTRSAFE) == 0) {
		if (rw_enter(&map->lock, RW_NOSLEEP|RW_WRITE) != 0)
			panic("uvm_map_setup: rw_enter failed on new map");
	}
	uvm_map_setup_entries(map);
	uvm_tree_sanity(map, __FILE__, __LINE__);
	if ((map->flags & VM_MAP_INTRSAFE) == 0)
		rw_exit(&map->lock);
}
d2050 7
a2056 11
/*
 * Populate map with free-memory entries.
 *
 * Map must be initialized and empty.
 */
void
uvm_map_setup_entries(struct vm_map *map)
{
	KDASSERT(RB_EMPTY(&map->addr));
	KDASSERT(RB_EMPTY(&map->free.tree) && map->free.treesz == 0);
	KDASSERT(RB_EMPTY(&map->bfree.tree) && map->bfree.treesz == 0);
d2058 2
a2059 2
	uvm_map_fix_space(map, NULL, map->min_offset, map->max_offset, 0);
}
d2061 4
a2064 13
/*
 * Split entry at given address.
 *
 * orig:  entry that is to be split.
 * next:  a newly allocated map entry that is not linked.
 * split: address at which the split is done.
 */
void
uvm_map_splitentry(struct vm_map *map, struct vm_map_entry *orig,
    struct vm_map_entry *next, vaddr_t split)
{
	struct uvm_map_free *free;
	vsize_t adj;
d2066 4
a2069 29
	KDASSERT(map != NULL && orig != NULL && next != NULL);
	uvm_tree_sanity(map, __FILE__, __LINE__);
	KASSERT(orig->start < split && FREE_END(orig) > split);

	adj = split - orig->start;
	free = UVM_FREE(map, FREE_START(orig));
	KDASSERT(RB_FIND(uvm_map_addr, &map->addr, orig) == orig);
	KDASSERT(RB_FIND(uvm_map_addr, &map->addr, next) != next);
	KDASSERT(orig->fspace == 0 || free == NULL ||
	    RB_FIND(uvm_map_free_int, &free->tree, orig) == orig);

	/*
	 * Free space will change, unlink from free space tree.
	 */
	if (orig->fspace > 0 && free)
		uvm_mapent_free_remove(map, free, orig);

	uvm_mapent_copy(orig, next);
	if (split >= orig->end) {
		next->etype = 0;
		next->offset = 0;
		next->wired_count = 0;
		next->start = next->end = split;
		next->guard = 0;
		next->fspace = FREE_END(orig) - split;
		next->aref.ar_amap = NULL;
		next->aref.ar_pageoff = 0;
		orig->guard = MIN(orig->guard, split - orig->end);
		orig->fspace = split - FREE_START(orig);
d2071 5
a2075 16
		orig->fspace = 0;
		orig->guard = 0;
		orig->end = next->start = split;

		if (next->aref.ar_amap)
			amap_splitref(&orig->aref, &next->aref, adj);
		if (UVM_ET_ISSUBMAP(orig)) {
			uvm_map_reference(next->object.sub_map);
			next->offset += adj;
		} else if (UVM_ET_ISOBJ(orig)) {
			if (next->object.uvm_obj->pgops &&
			    next->object.uvm_obj->pgops->pgo_reference) {
				next->object.uvm_obj->pgops->pgo_reference(
				    next->object.uvm_obj);
			}
			next->offset += adj;
d2077 3
d2082 4
d2087 2
a2088 2
	 * Link next into address tree.
	 * Link orig and next into free-space tree.
a2089 5
	uvm_mapent_addr_insert(map, next);
	if (orig->fspace > 0 && free)
		uvm_mapent_free_insert(map, free, orig);
	if (next->fspace > 0 && free)
		uvm_mapent_free_insert(map, free, next);
d2091 1
a2091 2
	uvm_tree_sanity(map, __FILE__, __LINE__);
}
d2093 3
d2097 11
a2107 1
#ifdef DEBUG
d2109 6
a2114 5
void
uvm_tree_assert(struct vm_map *map, int test, char *test_str,
    char *file, int line)
{
	char* map_special;
d2116 2
a2117 2
	if (test)
		return;
d2119 60
a2178 9
	if (map == kernel_map)
		map_special = " (kernel_map)";
	else if (map == kmem_map)
		map_special = " (kmem_map)";
	else
		map_special = "";
	panic("uvm_tree_sanity %p%s (%s %d): %s", map, map_special, file,
	    line, test_str);
}
d2180 3
a2182 9
/*
 * Check that free space tree is sane.
 */
void
uvm_tree_sanity_free(struct vm_map *map, struct uvm_map_free *free,
    char *file, int line)
{
	struct vm_map_entry *iter;
	vsize_t space, sz;
d2184 2
a2185 4
	space = PAGE_SIZE;
	sz = 0;
	RB_FOREACH(iter, uvm_map_free_int, &free->tree) {
		sz++;
d2187 6
a2192 2
		UVM_ASSERT(map, iter->fspace >= space, file, line);
		space = iter->fspace;
d2194 12
a2205 2
		UVM_ASSERT(map, RB_FIND(uvm_map_addr, &map->addr, iter) == iter,
		    file, line);
a2206 2
	UVM_ASSERT(map, free->treesz == sz, file, line);
}
d2208 8
a2215 10
/*
 * Check that map is sane.
 */
void
uvm_tree_sanity(struct vm_map *map, char *file, int line)
{
	struct vm_map_entry *iter;
	struct uvm_map_free *free;
	vaddr_t addr;
	vaddr_t min, max, bound; /* Bounds checker. */
d2217 6
a2222 12
	addr = vm_map_min(map);
	RB_FOREACH(iter, uvm_map_addr, &map->addr) {
		/*
		 * Valid start, end.
		 * Catch overflow for end+fspace.
		 */
		UVM_ASSERT(map, iter->end >= iter->start, file, line);
		UVM_ASSERT(map, FREE_END(iter) >= iter->end, file, line);
		/*
		 * May not be empty.
		 */
		UVM_ASSERT(map, iter->start < FREE_END(iter), file, line);
d2224 3
a2226 5
		/*
		 * Addresses for entry must lie within map boundaries.
		 */
		UVM_ASSERT(map, iter->start >= vm_map_min(map) &&
		    FREE_END(iter) <= vm_map_max(map), file, line);
d2228 9
a2236 5
		/*
		 * Tree may not have gaps.
		 */
		UVM_ASSERT(map, iter->start == addr, file, line);
		addr = FREE_END(iter);
d2238 13
a2250 6
		/*
		 * Free space may not cross boundaries, unless the same
		 * free list is used on both sides of the border.
		 */
		min = FREE_START(iter);
		max = FREE_END(iter);
d2252 2
a2253 6
		while (min < max &&
		    (bound = uvm_map_boundary(map, min, max)) != max) {
			UVM_ASSERT(map,
			    UVM_FREE(map, min) == UVM_FREE(map, bound),
			    file, line);
			min = bound;
d2255 1
d2258 2
a2259 1
		 * Entries with free space must appear in the free list.
a2260 12
		free = UVM_FREE(map, FREE_START(iter));
		if (iter->fspace > 0 && free) {
			UVM_ASSERT(map,
			    RB_FIND(uvm_map_free_int, &free->tree, iter) ==
			    iter, file, line);
		}
	}
	UVM_ASSERT(map, addr == vm_map_max(map), file, line);

	uvm_tree_sanity_free(map, &map->free, file, line);
	uvm_tree_sanity_free(map, &map->bfree, file, line);
}
d2262 2
a2263 5
void
uvm_tree_size_chk(struct vm_map *map, char *file, int line)
{
	struct vm_map_entry *iter;
	vsize_t size;
a2264 4
	size = 0;
	RB_FOREACH(iter, uvm_map_addr, &map->addr) {
		if (!UVM_ET_ISHOLE(iter))
			size += iter->end - iter->start;
d2266 2
d2269 5
a2273 3
	if (map->size != size)
		printf("map size = 0x%lx, should be 0x%lx\n", map->size, size);
	UVM_ASSERT(map, map->size == size, file, line);
d2275 3
a2277 2
	vmspace_validate(map);
}
d2279 6
a2284 11
/*
 * This function validates the statistics on vmspace.
 */
void
vmspace_validate(struct vm_map *map)
{
	struct vmspace *vm;
	struct vm_map_entry *iter;
	vaddr_t imin, imax;
	vaddr_t stack_begin, stack_end; /* Position of stack. */
	vsize_t stack, heap; /* Measured sizes. */
d2286 5
a2290 2
	if (!(map->flags & VM_MAP_ISVMSPACE))
		return;
d2292 2
a2293 7
	vm = (struct vmspace *)map;
	stack_begin = MIN((vaddr_t)vm->vm_maxsaddr, (vaddr_t)vm->vm_minsaddr);
	stack_end = MAX((vaddr_t)vm->vm_maxsaddr, (vaddr_t)vm->vm_minsaddr);

	stack = heap = 0;
	RB_FOREACH(iter, uvm_map_addr, &map->addr) {
		imin = imax = iter->start;
d2295 1
a2295 2
		if (UVM_ET_ISHOLE(iter) || iter->object.uvm_obj != NULL)
			continue;
d2297 9
a2305 15
		/*
		 * Update stack, heap.
		 * Keep in mind that (theoretically) the entries of
		 * userspace and stack may be joined.
		 */
		while (imin != iter->end) {
			/*
			 * Set imax to the first boundary crossed between
			 * imin and stack addresses.
			 */
			imax = iter->end;
			if (imin < stack_begin && imax > stack_begin)
				imax = stack_begin;
			else if (imin < stack_end && imax > stack_end)
				imax = stack_end;
d2307 2
a2308 7
			if (imin >= stack_begin && imin < stack_end)
				stack += imax - imin;
			else
				heap += imax - imin;
			imin = imax;
		}
	}
d2310 2
a2311 9
	heap >>= PAGE_SHIFT;
	if (heap != vm->vm_dused) {
		printf("vmspace stack range: 0x%lx-0x%lx\n",
		    stack_begin, stack_end);
		panic("vmspace_validate: vmspace.vm_dused invalid, "
		    "expected %ld pgs, got %ld pgs in map %p",
		    heap, vm->vm_dused,
		    map);
	}
d2314 1
a2314 1
#endif /* DEBUG */
d2317 14
a2330 2
 * uvm_map_init: init mapping system at boot time.   note that we allocate
 * and init the static pool of structs vm_map_entry for the kernel here.
d2332 4
a2335 2
void
uvm_map_init(void)
d2337 2
a2338 6
	static struct vm_map_entry kernel_map_entry[MAX_KMAPENT];
#if defined(UVMHIST)
	static struct uvm_history_ent maphistbuf[100];
	static struct uvm_history_ent pdhistbuf[100];
#endif
	int lcv;
d2340 1
a2340 3
	/*
	 * first, init logging system.
	 */
d2342 1
a2342 13
	UVMHIST_FUNC("uvm_map_init");
	UVMHIST_INIT_STATIC(maphist, maphistbuf);
	UVMHIST_INIT_STATIC(pdhist, pdhistbuf);
	UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist,"<starting uvm map system>", 0, 0, 0, 0);
	UVMCNT_INIT(uvm_map_call,  UVMCNT_CNT, 0,
	    "# uvm_map() successful calls", 0);
	UVMCNT_INIT(map_backmerge, UVMCNT_CNT, 0, "# uvm_map() back merges", 0);
	UVMCNT_INIT(map_forwmerge, UVMCNT_CNT, 0, "# uvm_map() missed forward",
	    0);
	UVMCNT_INIT(map_nousermerge, UVMCNT_CNT, 0, "# back merges skipped", 0);
	UVMCNT_INIT(uvm_mlk_call,  UVMCNT_CNT, 0, "# map lookup calls", 0);
	UVMCNT_INIT(uvm_mlk_hint,  UVMCNT_CNT, 0, "# map lookup hint hits", 0);
d2344 6
a2349 3
	/*
	 * now set up static pool of kernel map entries ...
	 */
d2351 11
a2361 6
	simple_lock_init(&uvm.kentry_lock);
	uvm.kentry_free = NULL;
	for (lcv = 0 ; lcv < MAX_KMAPENT ; lcv++) {
		RB_LEFT(&kernel_map_entry[lcv], daddrs.addr_entry) =
		    uvm.kentry_free;
		uvm.kentry_free = &kernel_map_entry[lcv];
d2363 2
a2364 11

	/*
	 * initialize the map-related pools.
	 */
	pool_init(&uvm_vmspace_pool, sizeof(struct vmspace),
	    0, 0, 0, "vmsppl", &pool_allocator_nointr);
	pool_init(&uvm_map_entry_pool, sizeof(struct vm_map_entry),
	    0, 0, 0, "vmmpepl", &pool_allocator_nointr);
	pool_init(&uvm_map_entry_kmem_pool, sizeof(struct vm_map_entry),
	    0, 0, 0, "vmmpekpl", NULL);
	pool_sethiwat(&uvm_map_entry_pool, 8192);
a2366 1
#if defined(DDB)
d2369 4
a2372 1
 * DDB hooks
d2375 7
a2381 6
/*
 * uvm_map_printit: actually prints the map
 */
void
uvm_map_printit(struct vm_map *map, boolean_t full,
    int (*pr)(const char *, ...))
d2383 9
a2391 4
	struct vmspace *vm;
	struct vm_map_entry *entry;
	struct uvm_map_free *free;
	int in_free;
d2393 5
a2397 15
	(*pr)("MAP %p: [0x%lx->0x%lx]\n", map, map->min_offset,map->max_offset);
	(*pr)("\tbrk() allocate range: 0x%lx-0x%lx %ld segments\n",
	    map->b_start, map->b_end, uvm_mapfree_size(&map->bfree));
	(*pr)("\tstack allocate range: 0x%lx-0x%lx %ld segments\n",
	    map->s_start, map->s_end, uvm_mapfree_size(&map->bfree));
	(*pr)("\tsz=%u, ref=%d, version=%u, flags=0x%x\n",
	    map->size, map->ref_count, map->timestamp,
	    map->flags);
#ifdef pmap_resident_count
	(*pr)("\tpmap=%p(resident=%d)\n", map->pmap, 
	    pmap_resident_count(map->pmap));
#else
	/* XXXCDC: this should be required ... */
	(*pr)("\tpmap=%p(resident=<<NOT SUPPORTED!!!>>)\n", map->pmap);
#endif
d2400 1
a2400 1
	 * struct vmspace handling.
a2401 2
	if (map->flags & VM_MAP_ISVMSPACE) {
		vm = (struct vmspace *)map;
d2403 11
a2413 8
		(*pr)("\tvm_refcnt=%d vm_shm=%p vm_rssize=%u vm_swrss=%u\n",
		    vm->vm_refcnt, vm->vm_shm, vm->vm_rssize, vm->vm_swrss);
		(*pr)("\tvm_tsize=%u vm_dsize=%u\n",
		    vm->vm_tsize, vm->vm_dsize);
		(*pr)("\tvm_taddr=%p vm_daddr=%p\n",
		    vm->vm_taddr, vm->vm_daddr);
		(*pr)("\tvm_maxsaddr=%p vm_minsaddr=%p\n",
		    vm->vm_maxsaddr, vm->vm_minsaddr);
d2416 6
a2421 14
	if (!full)
		return;
	RB_FOREACH(entry, uvm_map_addr, &map->addr) {
		(*pr)(" - %p: 0x%lx->0x%lx: obj=%p/0x%llx, amap=%p/%d\n",
		    entry, entry->start, entry->end, entry->object.uvm_obj,
		    (long long)entry->offset, entry->aref.ar_amap,
		    entry->aref.ar_pageoff);
		(*pr)("\tsubmap=%c, cow=%c, nc=%c, prot(max)=%d/%d, inh=%d, "
		    "wc=%d, adv=%d\n",
		    (entry->etype & UVM_ET_SUBMAP) ? 'T' : 'F',
		    (entry->etype & UVM_ET_COPYONWRITE) ? 'T' : 'F', 
		    (entry->etype & UVM_ET_NEEDSCOPY) ? 'T' : 'F',
		    entry->protection, entry->max_protection,
		    entry->inheritance, entry->wired_count, entry->advice);
d2423 1
a2423 11
		free = UVM_FREE(map, FREE_START(entry));
		in_free = (free != NULL) &&
		    (RB_FIND(uvm_map_free_int, &free->tree, entry) == entry);
		(*pr)("\thole=%c, free=%c, guard=0x%lx, "
		    "free=0x%lx-0x%lx\n",
		    (entry->etype & UVM_ET_HOLE) ? 'T' : 'F',
		    in_free ? 'T' : 'F',
		    entry->guard,
		    FREE_START(entry), FREE_END(entry));
	}
} 
d2425 6
a2430 11
/*
 * uvm_object_printit: actually prints the object
 */
void
uvm_object_printit(uobj, full, pr)
	struct uvm_object *uobj;
	boolean_t full;
	int (*pr)(const char *, ...);
{
	struct vm_page *pg;
	int cnt = 0;
d2432 4
a2435 6
	(*pr)("OBJECT %p: pgops=%p, npages=%d, ",
	    uobj, uobj->pgops, uobj->uo_npages);
	if (UVM_OBJ_IS_KERN_OBJECT(uobj))
		(*pr)("refs=<SYSTEM>\n");
	else
		(*pr)("refs=%d\n", uobj->uo_refs);
d2437 7
a2443 8
	if (!full) {
		return;
	}
	(*pr)("  PAGES <pg,offset>:\n  ");
	RB_FOREACH(pg, uvm_objtree, &uobj->memt) {
		(*pr)("<%p,0x%llx> ", pg, (long long)pg->offset);
		if ((cnt % 3) == 2) {
			(*pr)("\n  ");
a2444 6
		cnt++;
	}
	if ((cnt % 3) != 2) {
		(*pr)("\n");
	}
} 
d2446 2
a2447 211
/*
 * uvm_page_printit: actually print the page
 */
static const char page_flagbits[] =
	"\20\1BUSY\2WANTED\3TABLED\4CLEAN\5CLEANCHK\6RELEASED\7FAKE\10RDONLY"
	"\11ZERO\15PAGER1\20FREE\21INACTIVE\22ACTIVE\24ENCRYPT\30PMAP0"
	"\31PMAP1\32PMAP2\33PMAP3";

void
uvm_page_printit(pg, full, pr)
	struct vm_page *pg;
	boolean_t full;
	int (*pr)(const char *, ...);
{
	struct vm_page *tpg;
	struct uvm_object *uobj;
	struct pglist *pgl;

	(*pr)("PAGE %p:\n", pg);
	(*pr)("  flags=%b, vers=%d, wire_count=%d, pa=0x%llx\n",
	    pg->pg_flags, page_flagbits, pg->pg_version, pg->wire_count,
	    (long long)pg->phys_addr);
	(*pr)("  uobject=%p, uanon=%p, offset=0x%llx loan_count=%d\n",
	    pg->uobject, pg->uanon, (long long)pg->offset, pg->loan_count);
#if defined(UVM_PAGE_TRKOWN)
	if (pg->pg_flags & PG_BUSY)
		(*pr)("  owning process = %d, tag=%s\n",
		    pg->owner, pg->owner_tag);
	else
		(*pr)("  page not busy, no owner\n");
#else
	(*pr)("  [page ownership tracking disabled]\n");
#endif

	if (!full)
		return;

	/* cross-verify object/anon */
	if ((pg->pg_flags & PQ_FREE) == 0) {
		if (pg->pg_flags & PQ_ANON) {
			if (pg->uanon == NULL || pg->uanon->an_page != pg)
			    (*pr)("  >>> ANON DOES NOT POINT HERE <<< (%p)\n",
				(pg->uanon) ? pg->uanon->an_page : NULL);
			else
				(*pr)("  anon backpointer is OK\n");
		} else {
			uobj = pg->uobject;
			if (uobj) {
				(*pr)("  checking object list\n");
				RB_FOREACH(tpg, uvm_objtree, &uobj->memt) {
					if (tpg == pg) {
						break;
					}
				}
				if (tpg)
					(*pr)("  page found on object list\n");
				else
					(*pr)("  >>> PAGE NOT FOUND "
					    "ON OBJECT LIST! <<<\n");
			}
		}
	}

	/* cross-verify page queue */
	if (pg->pg_flags & PQ_FREE) {
		if (uvm_pmr_isfree(pg))
			(*pr)("  page found in uvm_pmemrange\n");
		else
			(*pr)("  >>> page not found in uvm_pmemrange <<<\n");
		pgl = NULL;
	} else if (pg->pg_flags & PQ_INACTIVE) {
		pgl = (pg->pg_flags & PQ_SWAPBACKED) ?
		    &uvm.page_inactive_swp : &uvm.page_inactive_obj;
	} else if (pg->pg_flags & PQ_ACTIVE) {
		pgl = &uvm.page_active;
 	} else {
		pgl = NULL;
	}

	if (pgl) {
		(*pr)("  checking pageq list\n");
		TAILQ_FOREACH(tpg, pgl, pageq) {
			if (tpg == pg) {
				break;
			}
		}
		if (tpg)
			(*pr)("  page found on pageq list\n");
		else
			(*pr)("  >>> PAGE NOT FOUND ON PAGEQ LIST! <<<\n");
	}
}
#endif

/*
 * uvm_map_protect: change map protection
 *
 * => set_max means set max_protection.
 * => map must be unlocked.
 */
int
uvm_map_protect(struct vm_map *map, vaddr_t start, vaddr_t end,
    vm_prot_t new_prot, boolean_t set_max)
{
	struct vm_map_entry *first, *iter;
	vm_prot_t old_prot;
	vm_prot_t mask;
	int error;

	if (start > end)
		return EINVAL;
	start = MAX(start, map->min_offset);
	end = MIN(end, map->max_offset);
	if (start >= end)
		return 0;

	error = 0;
	vm_map_lock(map);

	/*
	 * Set up first and last.
	 * - first will contain first entry at or after start.
	 */
	first = uvm_map_entrybyaddr(&map->addr, start);
	KDASSERT(first != NULL);
	if (first->end < start)
		first = RB_NEXT(uvm_map_addr, &map->addr, first);

	/*
	 * First, check for protection violations.
	 */
	for (iter = first; iter != NULL && iter->start < end;
	    iter = RB_NEXT(uvm_map_addr, &map->addr, iter)) {
		/* Treat memory holes as free space. */
		if (iter->start == iter->end || UVM_ET_ISHOLE(iter))
			continue;

		if (UVM_ET_ISSUBMAP(iter)) {
			error = EINVAL;
			goto out;
		}
		if ((new_prot & iter->max_protection) != new_prot) {
			error = EACCES;
			goto out;
		}
	}

	/*
	 * Fix protections.
	 */
	for (iter = first; iter != NULL && iter->start < end;
	    iter = RB_NEXT(uvm_map_addr, &map->addr, iter)) {
		/* Treat memory holes as free space. */
		if (iter->start == iter->end || UVM_ET_ISHOLE(iter))
			continue;

		old_prot = iter->protection;

		/*
		 * Skip adapting protection iff old and new protection
		 * are equal.
		 */
		if (set_max) {
			if (old_prot == (new_prot & old_prot) &&
			    iter->max_protection == new_prot)
				continue;
		} else {
			if (old_prot == new_prot)
				continue;
		}

		UVM_MAP_CLIP_START(map, iter, start);
		UVM_MAP_CLIP_END(map, iter, end);

		if (set_max) {
			iter->max_protection = new_prot;
			iter->protection &= new_prot;
		} else
			iter->protection = new_prot;

		/*
		 * update physical map if necessary.  worry about copy-on-write
		 * here -- CHECK THIS XXX
		 */
		if (iter->protection != old_prot) {
			mask = UVM_ET_ISCOPYONWRITE(iter) ?
			    ~VM_PROT_WRITE : VM_PROT_ALL;

			/* update pmap */
			if ((iter->protection & mask) == PROT_NONE &&
			    VM_MAPENT_ISWIRED(iter)) {
				/*
				 * TODO(ariane) this is stupid. wired_count
				 * is 0 if not wired, otherwise anything
				 * larger than 0 (incremented once each time
				 * wire is called).
				 * Mostly to be able to undo the damage on
				 * failure. Not the actually be a wired
				 * refcounter...
				 * Originally: iter->wired_count--;
				 * (don't we have to unwire this in the pmap
				 * as well?)
				 */
				iter->wired_count = 0;
			}
			pmap_protect(map->pmap, iter->start, iter->end,
			    iter->protection & mask);
		}

		/*
		 * If the map is configured to lock any future mappings,
d2451 1
d2453 1
a2453 1
		    VM_MAPENT_ISWIRED(iter) == 0 &&
d2456 2
a2457 2
			if (uvm_map_pageable(map, iter->start, iter->end,
			    FALSE, UVM_LK_ENTER | UVM_LK_EXIT) != 0) {
d2462 2
a2463 2
				 * the map, but it will return the resource
				 * storage condition regardless.
d2474 2
d2479 1
a2479 1
out:
d2482 1
a2482 1
	return error;
d2485 5
a2489 2
/*
 * uvmspace_alloc: allocate a vmspace structure.
d2491 3
a2493 3
 * - structure includes vm_map and pmap
 * - XXX: no locking on this structure
 * - refcnt set to 1, rest must be init'd by caller
d2495 4
a2498 3
struct vmspace *
uvmspace_alloc(vaddr_t min, vaddr_t max, boolean_t pageable,
    boolean_t remove_holes)
d2500 30
a2529 2
	struct vmspace *vm;
	UVMHIST_FUNC("uvmspace_alloc"); UVMHIST_CALLED(maphist);
d2531 3
a2533 4
	vm = pool_get(&uvm_vmspace_pool, PR_WAITOK | PR_ZERO);
	uvmspace_init(vm, NULL, min, max, pageable, remove_holes);
	UVMHIST_LOG(maphist,"<- done (vm=%p)", vm,0,0,0);
	return (vm);
d2536 2
a2537 2
/*
 * uvmspace_init: initialize a vmspace structure.
d2539 1
a2539 2
 * - XXX: no locking on this structure
 * - refcnt set to 1, rest must be init'd by caller
d2541 3
a2543 3
void
uvmspace_init(struct vmspace *vm, struct pmap *pmap, vaddr_t min, vaddr_t max,
    boolean_t pageable, boolean_t remove_holes)
d2545 11
a2555 1
	UVMHIST_FUNC("uvmspace_init"); UVMHIST_CALLED(maphist);
d2557 11
a2567 5
	if (pmap)
		pmap_reference(pmap);
	else
		pmap = pmap_create();
	vm->vm_map.pmap = pmap;
d2569 3
a2571 2
	uvm_map_setup(&vm->vm_map, min, max,
	    (pageable ? VM_MAP_PAGEABLE : 0) | VM_MAP_ISVMSPACE);
d2573 2
a2574 1
	vm->vm_refcnt = 1;
d2576 3
a2578 2
	if (remove_holes)
		pmap_remove_holes(&vm->vm_map);
d2580 3
a2582 1
	UVMHIST_LOG(maphist,"<- done",0,0,0,0);
d2586 1
a2586 1
 * uvmspace_share: share a vmspace between two proceses
d2588 9
a2596 2
 * - XXX: no locking on vmspace
 * - used for vfork, threads(?)
d2599 3
a2601 3
void
uvmspace_share(p1, p2)
	struct proc *p1, *p2;
d2603 12
a2614 3
	p2->p_vmspace = p1->p_vmspace;
	p1->p_vmspace->vm_refcnt++;
}
d2616 1
a2616 5
/*
 * uvmspace_exec: the process wants to exec a new program
 *
 * - XXX: no locking on vmspace
 */
d2618 7
a2624 6
void
uvmspace_exec(struct proc *p, vaddr_t start, vaddr_t end)
{
	struct vmspace *nvm, *ovm = p->p_vmspace;
	struct vm_map *map = &ovm->vm_map;
	struct uvm_map_deadq dead_entries;
d2626 3
a2628 2
	KASSERT((start & PAGE_MASK) == 0);
	KASSERT((end & PAGE_MASK) == 0 || (end & PAGE_MASK) == PAGE_MASK);
d2630 4
a2633 2
	pmap_unuse_final(p);   /* before stack addresses go away */
	TAILQ_INIT(&dead_entries);
d2635 2
a2636 2
	/*
	 * see if more than one process is using this vmspace...
d2639 3
a2641 1
	if (ovm->vm_refcnt == 1) {
d2643 2
a2644 2
		 * if p is the only process using its vmspace then we can safely
		 * recycle that vmspace for the program that is being exec'd.
d2647 18
a2664 3
#ifdef SYSVSHM
		/*
		 * SYSV SHM semantics require us to kill all segments on an exec
d2666 59
a2724 3
		if (ovm->vm_shm)
			shmexit(ovm);
#endif
d2727 1
a2727 2
		 * POSIX 1003.1b -- "lock future mappings" is revoked
		 * when a process execs another program image.
a2728 2
		vm_map_lock(map);
		vm_map_modflags(map, 0, VM_MAP_WIREFUTURE);
d2730 9
a2738 12
		/*
		 * now unmap the old program
		 *
		 * Instead of attempting to keep the map valid, we simply
		 * nuke all entries and ask uvm_map_setup to reinitialize
		 * the map to the new boundaries.
		 *
		 * uvm_unmap_remove will actually nuke all entries for us
		 * (as in, not replace them with free-memory entries).
		 */
		uvm_unmap_remove(map, map->min_offset, map->max_offset,
		    &dead_entries, TRUE, FALSE);
d2740 11
a2750 1
		KDASSERT(RB_EMPTY(&map->addr));
d2752 3
a2754 5
		/*
		 * Nuke statistics and boundaries.
		 */
		bzero(&ovm->vm_startcopy,
		    (caddr_t) (ovm + 1) - (caddr_t) &ovm->vm_startcopy);
d2756 5
d2762 14
a2775 4
		if (end & PAGE_MASK) {
			end += 1;
			if (end == 0) /* overflow */
				end -= PAGE_SIZE;
d2777 4
d2783 1
a2783 1
		 * Setup new boundaries and populate map with entries.
d2785 8
a2792 4
		map->min_offset = start;
		map->max_offset = end;
		uvm_map_setup_entries(map);
		vm_map_unlock(map);
d2795 2
a2796 1
		 * but keep MMU holes unavailable
a2797 1
		pmap_remove_holes(map);
d2799 5
a2803 1
	} else {
d2806 2
a2807 3
		 * p's vmspace is being shared, so we can't reuse it for p since
		 * it is still being used for others.   allocate a new vmspace
		 * for p
d2809 19
a2827 2
		nvm = uvmspace_alloc(start, end,
		    (map->flags & VM_MAP_PAGEABLE) ? TRUE : FALSE, TRUE);
d2830 1
a2830 1
		 * install new vmspace and drop our ref to the old one.
d2833 2
a2834 5
		pmap_deactivate(p);
		p->p_vmspace = nvm;
		pmap_activate(p);

		uvmspace_free(ovm);
d2837 2
a2838 4
	/*
	 * Release dead entries
	 */
	uvm_unmap_detach(&dead_entries, 0);
d2842 2
a2843 1
 * uvmspace_free: free a vmspace data structure
d2845 3
a2847 1
 * - XXX: no locking on vmspace
d2850 2
a2851 2
void
uvmspace_free(struct vmspace *vm)
d2853 10
a2862 1
	struct uvm_map_deadq dead_entries;
d2864 5
a2868 1
	UVMHIST_FUNC("uvmspace_free"); UVMHIST_CALLED(maphist);
d2870 1
a2870 2
	UVMHIST_LOG(maphist,"(vm=%p) ref=%ld", vm, vm->vm_refcnt,0,0);
	if (--vm->vm_refcnt == 0) {
d2872 2
a2873 3
		 * lock the map, to wait out all other references to it.  delete
		 * all of the mappings and pages they hold, then call the pmap
		 * module to reclaim anything left.
d2875 4
a2878 11
#ifdef SYSVSHM
		/* Get rid of any SYSV shared memory segments. */
		if (vm->vm_shm != NULL)
			shmexit(vm);
#endif
		if ((vm->vm_map.flags & VM_MAP_INTRSAFE) == 0) {
			if (rw_enter(&vm->vm_map.lock, RW_NOSLEEP|RW_WRITE) !=
			    0) {
				panic("uvm_map_setup: "
				    "rw_enter failed on free map");
			}
d2880 8
a2887 12
		uvm_tree_sanity(&vm->vm_map, __FILE__, __LINE__);
		TAILQ_INIT(&dead_entries);
		uvm_unmap_remove(&vm->vm_map,
		    vm->vm_map.min_offset, vm->vm_map.max_offset,
		    &dead_entries, TRUE, FALSE);
		if ((vm->vm_map.flags & VM_MAP_INTRSAFE) == 0)
			rw_exit(&vm->vm_map.lock);
		KDASSERT(RB_EMPTY(&vm->vm_map.addr));
		uvm_unmap_detach(&dead_entries, 0);
		pmap_destroy(vm->vm_map.pmap);
		vm->vm_map.pmap = NULL;
		pool_put(&uvm_vmspace_pool, vm);
a2888 2
	UVMHIST_LOG(maphist,"<- done", 0,0,0,0);
}
d2890 5
a2894 48
/*
 * Clone map entry into other map.
 *
 * Mapping will be placed at dstaddr, for the same length.
 * Space must be available.
 * Reference counters are incremented.
 */
struct vm_map_entry*
uvm_mapent_clone(struct vm_map *dstmap, vaddr_t dstaddr, vsize_t dstlen,
    vsize_t off, struct vm_map_entry *old_entry, struct uvm_map_deadq *dead,
    int mapent_flags, int amap_share_flags)
{
	struct vm_map_entry *new_entry, *first, *last;

	KDASSERT(!UVM_ET_ISSUBMAP(old_entry));

	/*
	 * Create new entry (linked in on creation).
	 * Fill in first, last.
	 */
	first = last = NULL;
	if (!uvm_map_isavail(&dstmap->addr, &first, &last, dstaddr, dstlen)) {
		panic("uvmspace_fork: no space in map for "
		    "entry in empty map");
	}
	new_entry = uvm_map_mkentry(dstmap, first, last,
	    dstaddr, dstlen, mapent_flags, dead);
	if (new_entry == NULL)
		return NULL;
	/* old_entry -> new_entry */
	new_entry->object = old_entry->object;
	new_entry->offset = old_entry->offset;
	new_entry->aref = old_entry->aref;
	new_entry->etype = old_entry->etype;
	new_entry->protection = old_entry->protection;
	new_entry->max_protection = old_entry->max_protection;
	new_entry->inheritance = old_entry->inheritance;
	new_entry->advice = old_entry->advice;

	/*
	 * gain reference to object backing the map (can't
	 * be a submap).
	 */
	if (new_entry->aref.ar_amap) {
		new_entry->aref.ar_pageoff += off >> PAGE_SHIFT;
		amap_ref(new_entry->aref.ar_amap, new_entry->aref.ar_pageoff,
		    (new_entry->end - new_entry->start) >> PAGE_SHIFT,
		    amap_share_flags);
d2897 7
a2903 5
	if (UVM_ET_ISOBJ(new_entry) &&
	    new_entry->object.uvm_obj->pgops->pgo_reference) {
		new_entry->offset += off;
		new_entry->object.uvm_obj->pgops->pgo_reference
		    (new_entry->object.uvm_obj);
a2905 14
	return new_entry;
}

/*
 * share the mapping: this means we want the old and
 * new entries to share amaps and backing objects.
 */
void
uvm_mapent_forkshared(struct vmspace *new_vm, struct vm_map *new_map,
    struct vm_map *old_map,
    struct vm_map_entry *old_entry, struct uvm_map_deadq *dead)
{
	struct vm_map_entry *new_entry;

d2907 20
a2926 21
	 * if the old_entry needs a new amap (due to prev fork)
	 * then we need to allocate it now so that we have
	 * something we own to share with the new_entry.   [in
	 * other words, we need to clear needs_copy]
	 */

	if (UVM_ET_ISNEEDSCOPY(old_entry)) {
		/* get our own amap, clears needs_copy */
		amap_copy(old_map, old_entry, M_WAITOK, FALSE,
		    0, 0); 
		/* XXXCDC: WAITOK??? */
	}

	new_entry = uvm_mapent_clone(new_map, old_entry->start,
	    old_entry->end - old_entry->start, 0, old_entry,
	    dead, 0, AMAP_SHARED);

	/* 
	 * pmap_copy the mappings: this routine is optional
	 * but if it is there it will reduce the number of
	 * page faults in the new proc.
a2927 2
	pmap_copy(new_map->pmap, old_map->pmap, new_entry->start,
	    (new_entry->end - new_entry->start), new_entry->start);
d2929 6
a2934 8
	/*
	 * Update process statistics.
	 */
	if (!UVM_ET_ISHOLE(new_entry))
		new_map->size += new_entry->end - new_entry->start;
	if (!UVM_ET_ISOBJ(new_entry) && !UVM_ET_ISHOLE(new_entry)) {
		new_vm->vm_dused +=
		    uvmspace_dused(new_map, new_entry->start, new_entry->end);
a2935 1
}
d2937 4
a2940 53
/*
 * copy-on-write the mapping (using mmap's
 * MAP_PRIVATE semantics)
 *
 * allocate new_entry, adjust reference counts.  
 * (note that new references are read-only).
 */
void
uvm_mapent_forkcopy(struct vmspace *new_vm, struct vm_map *new_map,
    struct vm_map *old_map,
    struct vm_map_entry *old_entry, struct uvm_map_deadq *dead)
{
	struct vm_map_entry	*new_entry;
	boolean_t		 protect_child;

	new_entry = uvm_mapent_clone(new_map, old_entry->start,
	    old_entry->end - old_entry->start, 0, old_entry,
	    dead, 0, 0);

	new_entry->etype |=
	    (UVM_ET_COPYONWRITE|UVM_ET_NEEDSCOPY);

	/*
	 * the new entry will need an amap.  it will either
	 * need to be copied from the old entry or created
	 * from scratch (if the old entry does not have an
	 * amap).  can we defer this process until later
	 * (by setting "needs_copy") or do we need to copy
	 * the amap now?
	 *
	 * we must copy the amap now if any of the following
	 * conditions hold:
	 * 1. the old entry has an amap and that amap is
	 *    being shared.  this means that the old (parent)
	 *    process is sharing the amap with another 
	 *    process.  if we do not clear needs_copy here
	 *    we will end up in a situation where both the
	 *    parent and child process are referring to the
	 *    same amap with "needs_copy" set.  if the 
	 *    parent write-faults, the fault routine will
	 *    clear "needs_copy" in the parent by allocating
	 *    a new amap.   this is wrong because the 
	 *    parent is supposed to be sharing the old amap
	 *    and the new amap will break that.
	 *
	 * 2. if the old entry has an amap and a non-zero
	 *    wire count then we are going to have to call
	 *    amap_cow_now to avoid page faults in the 
	 *    parent process.   since amap_cow_now requires
	 *    "needs_copy" to be clear we might as well
	 *    clear it here as well.
	 *
	 */
d2942 6
a2947 7
	if (old_entry->aref.ar_amap != NULL &&
	    ((amap_flags(old_entry->aref.ar_amap) &
	    AMAP_SHARED) != 0 ||
	    VM_MAPENT_ISWIRED(old_entry))) {
		amap_copy(new_map, new_entry, M_WAITOK, FALSE,
		    0, 0);
		/* XXXCDC: M_WAITOK ... ok? */
d2949 1
d2952 1
a2952 8
	 * if the parent's entry is wired down, then the
	 * parent process does not want page faults on
	 * access to that memory.  this means that we
	 * cannot do copy-on-write because we can't write
	 * protect the old entry.   in this case we
	 * resolve all copy-on-write faults now, using
	 * amap_cow_now.   note that we have already
	 * allocated any needed amap (above).
d2955 5
a2959 15
	if (VM_MAPENT_ISWIRED(old_entry)) {

		/* 
		 * resolve all copy-on-write faults now
		 * (note that there is nothing to do if 
		 * the old mapping does not have an amap).
		 * XXX: is it worthwhile to bother with
		 * pmap_copy in this case?
		 */
		if (old_entry->aref.ar_amap)
			amap_cow_now(new_map, new_entry);

	} else {
		if (old_entry->aref.ar_amap) {

d2961 5
a2965 14
			 * setup mappings to trigger copy-on-write faults
			 * we must write-protect the parent if it has
			 * an amap and it is not already "needs_copy"...
			 * if it is already "needs_copy" then the parent
			 * has already been write-protected by a previous
			 * fork operation.
			 *
			 * if we do not write-protect the parent, then
			 * we must be sure to write-protect the child
			 * after the pmap_copy() operation.
			 *
			 * XXX: pmap_copy should have some way of telling
			 * us that it didn't do anything so we can avoid
			 * calling pmap_protect needlessly.
d2967 7
a2973 9
			if (!UVM_ET_ISNEEDSCOPY(old_entry)) {
				if (old_entry->max_protection &
				    VM_PROT_WRITE) {
					pmap_protect(old_map->pmap,
					    old_entry->start,
					    old_entry->end,
					    old_entry->protection &
					    ~VM_PROT_WRITE);
					pmap_update(old_map->pmap);
a2974 1
				old_entry->etype |= UVM_ET_NEEDSCOPY;
d2976 7
d2984 5
a2988 14
	  		/*
	  		 * parent must now be write-protected
	  		 */
	  		protect_child = FALSE;
		} else {

			/*
			 * we only need to protect the child if the 
			 * parent has write access.
			 */
			if (old_entry->max_protection & VM_PROT_WRITE)
				protect_child = TRUE;
			else
				protect_child = FALSE;
d2990 6
d2997 1
d2999 1
d3001 1
a3001 2
		 * copy the mappings
		 * XXX: need a way to tell if this does anything
d3003 7
d3011 13
a3023 4
		pmap_copy(new_map->pmap, old_map->pmap,
		    new_entry->start,
		    (old_entry->end - old_entry->start),
		    old_entry->start);
d3026 4
a3029 1
		 * protect the child's mappings if necessary
d3031 7
a3037 5
		if (protect_child) {
			pmap_protect(new_map->pmap, new_entry->start,
			    new_entry->end,
			    new_entry->protection &
			    ~VM_PROT_WRITE);
d3039 3
d3044 6
a3049 9
	/*
	 * Update process statistics.
	 */
	if (!UVM_ET_ISHOLE(new_entry))
		new_map->size += new_entry->end - new_entry->start;
	if (!UVM_ET_ISOBJ(new_entry) && !UVM_ET_ISHOLE(new_entry)) {
		new_vm->vm_dused +=
		    uvmspace_dused(new_map, new_entry->start, new_entry->end);
	}
d3053 1
a3053 1
 * uvmspace_fork: fork a process' main map
d3055 11
a3065 2
 * => create a new vmspace for child process from parent.
 * => parent's map must not be locked.
d3067 5
a3071 2
struct vmspace *
uvmspace_fork(struct vmspace *vm1)
d3073 9
a3081 6
	struct vmspace *vm2;
	struct vm_map *old_map = &vm1->vm_map;
	struct vm_map *new_map;
	struct vm_map_entry *old_entry;
	struct uvm_map_deadq dead;
	UVMHIST_FUNC("uvmspace_fork"); UVMHIST_CALLED(maphist);
d3083 4
a3086 1
	vm_map_lock(old_map);
d3088 6
a3093 7
	vm2 = uvmspace_alloc(old_map->min_offset, old_map->max_offset,
	    (old_map->flags & VM_MAP_PAGEABLE) ? TRUE : FALSE, FALSE);
	memcpy(&vm2->vm_startcopy, &vm1->vm_startcopy,
	    (caddr_t) (vm1 + 1) - (caddr_t) &vm1->vm_startcopy);
	vm2->vm_dused = 0; /* Statistic managed by us. */
	new_map = &vm2->vm_map;
	vm_map_lock(new_map);
d3096 1
a3096 1
	 * go entry-by-entry
d3099 18
a3116 4
	TAILQ_INIT(&dead);
	RB_FOREACH(old_entry, uvm_map_addr, &old_map->addr) {
		if (old_entry->start == old_entry->end)
			continue;
d3119 5
a3123 1
		 * first, some sanity checks on the old entry
a3124 4
		if (UVM_ET_ISSUBMAP(old_entry)) {
			panic("fork: encountered a submap during fork "
			    "(illegal)");
		}
d3126 98
a3223 4
		if (!UVM_ET_ISCOPYONWRITE(old_entry) &&
		    UVM_ET_ISNEEDSCOPY(old_entry)) {
			panic("fork: non-copy_on_write map entry marked "
			    "needs_copy (illegal)");
d3226 1
d3228 5
a3232 1
		 * Apply inheritance.
d3234 14
a3247 3
		if (old_entry->inheritance == MAP_INHERIT_SHARE) {
			uvm_mapent_forkshared(vm2, new_map,
			    old_map, old_entry, &dead);
d3249 28
a3276 3
		if (old_entry->inheritance == MAP_INHERIT_COPY) {
			uvm_mapent_forkcopy(vm2, new_map,
			    old_map, old_entry, &dead);
a3277 1
	}
d3279 7
a3285 2
	vm_map_unlock(old_map); 
	vm_map_unlock(new_map); 
d3287 3
a3289 5
	/*
	 * This can actually happen, if multiple entries described a
	 * space in which an entry was inherited.
	 */
	uvm_unmap_detach(&dead, 0);
d3291 3
a3293 4
#ifdef SYSVSHM
	if (vm1->vm_shm)
		shmfork(vm1, vm2);
#endif
d3295 1
a3295 3
#ifdef PMAP_FORK
	pmap_fork(vm1->vm_map.pmap, vm2->vm_map.pmap);
#endif
d3297 4
a3300 2
	UVMHIST_LOG(maphist,"<- done",0,0,0,0);
	return vm2;    
d3304 5
a3308 2
 * uvm_map_hint: return the beginning of the best area suitable for
 * creating a new mapping with "prot" protection.
d3310 3
a3312 2
vaddr_t
uvm_map_hint(struct proc *p, vm_prot_t prot)
d3314 2
a3315 1
	vaddr_t addr;
d3317 4
a3320 18
#ifdef __i386__
	/*
	 * If executable skip first two pages, otherwise start
	 * after data + heap region.
	 */
	if ((prot & VM_PROT_EXECUTE) &&
	    ((vaddr_t)p->p_vmspace->vm_daddr >= I386_MAX_EXE_ADDR)) {
		addr = (PAGE_SIZE*2) +
		    (arc4random() & (I386_MAX_EXE_ADDR / 2 - 1));
		return (round_page(addr));
	}
#endif
	/* start malloc/mmap after the brk */
	addr = (vaddr_t)p->p_vmspace->vm_daddr + BRKSIZ;
#if !defined(__vax__)
	addr += arc4random() & (MIN((256 * 1024 * 1024), BRKSIZ) - 1);
#endif
	return (round_page(addr));
d3324 1
a3324 1
 * uvm_map_submap: punch down part of a map into a submap
d3326 2
a3327 12
 * => only the kernel_map is allowed to be submapped
 * => the purpose of submapping is to break up the locking granularity
 *	of a larger map
 * => the range specified must have been mapped previously with a uvm_map()
 *	call [with uobj==NULL] to create a blank map entry in the main map.
 *	[And it had better still be blank!]
 * => maps which contain submaps should never be copied or forked.
 * => to remove a submap, use uvm_unmap() on the main map 
 *	and then uvm_map_deallocate() the submap.
 * => main map must be unlocked.
 * => submap must have been init'd and have a zero reference count.
 *	[need not be locked as we don't actually reference it]
d3329 3
a3331 3
int
uvm_map_submap(struct vm_map *map, vaddr_t start, vaddr_t end,
    struct vm_map *submap)
d3333 9
a3341 2
	struct vm_map_entry *entry;
	int result;
d3343 1
a3343 3
	if (start > map->max_offset || end > map->max_offset ||
	    start < map->min_offset || end < map->min_offset)
		return EINVAL;
d3345 2
a3346 1
	vm_map_lock(map);
d3348 2
a3349 5
	if (uvm_map_lookup_entry(map, start, &entry)) {
		UVM_MAP_CLIP_START(map, entry, start);
		UVM_MAP_CLIP_END(map, entry, end);
	} else
		entry = NULL;
d3351 6
a3356 11
	if (entry != NULL && 
	    entry->start == start && entry->end == end &&
	    entry->object.uvm_obj == NULL && entry->aref.ar_amap == NULL &&
	    !UVM_ET_ISCOPYONWRITE(entry) && !UVM_ET_ISNEEDSCOPY(entry)) {
		entry->etype |= UVM_ET_SUBMAP;
		entry->object.sub_map = submap;
		entry->offset = 0;
		uvm_map_reference(submap);
		result = 0;
	} else
		result = EINVAL;
d3358 6
a3363 2
	vm_map_unlock(map);
	return(result);
d3367 1
a3367 1
 * uvm_map_checkprot: check protection in map
d3369 1
a3369 2
 * => must allow specific protection in a fully allocated region.
 * => map mut be read or write locked by caller.
d3371 3
a3373 3
boolean_t
uvm_map_checkprot(struct vm_map *map, vaddr_t start, vaddr_t end,
    vm_prot_t protection)
d3375 2
a3376 1
	struct vm_map_entry *entry;
d3378 1
a3378 4
	if (start < map->min_offset || end > map->max_offset || start > end)
		return FALSE;
	if (start == end)
		return TRUE;
d3381 1
a3381 1
	 * Iterate entries.
d3383 49
a3431 3
	for (entry = uvm_map_entrybyaddr(&map->addr, start);
	    entry != NULL && entry->start < end;
	    entry = RB_NEXT(uvm_map_addr, &map->addr, entry)) {
d3433 9
a3441 1
		 * Fail if a hole is found.
a3442 3
		if (UVM_ET_ISHOLE(entry) ||
		    (entry->end < end && entry->end != FREE_END(entry)))
			return FALSE;
d3444 22
d3467 3
a3469 1
		 * Check protection.
d3471 16
a3486 2
		if ((entry->protection & protection) != protection)
			return FALSE;
d3488 1
a3488 1
	return TRUE;
d3500 1
a3501 1
	uvm_map_setup(result, min, max, flags);
d3506 40
a3554 1
	struct uvm_map_deadq dead;
a3564 2
	 *
	 * No lock required: we are only one to access this map.
d3567 1
a3567 4
	TAILQ_INIT(&dead);
	uvm_tree_sanity(map, __FILE__, __LINE__);
	uvm_unmap_remove(map, map->min_offset, map->max_offset, &dead,
	    TRUE, FALSE);
a3568 1
	KASSERT(RB_EMPTY(&map->addr));
a3569 2

	uvm_unmap_detach(&dead, 0);
d3572 5
a3576 2
/* 
 * uvm_map_inherit: set inheritance code for range of addrs in map.
d3578 2
a3579 3
 * => map must be unlocked
 * => note that the inherit code is used during a "fork".  see fork
 *	code for details.
d3581 3
a3583 3
int
uvm_map_inherit(struct vm_map *map, vaddr_t start, vaddr_t end,
    vm_inherit_t new_inheritance)
d3585 19
a3603 4
	struct vm_map_entry *entry;
	UVMHIST_FUNC("uvm_map_inherit"); UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist,"(map=%p,start=0x%lx,end=0x%lx,new_inh=0x%lx)",
	    map, start, end, new_inheritance);
d3605 3
a3607 9
	switch (new_inheritance) {
	case MAP_INHERIT_NONE:
	case MAP_INHERIT_COPY:
	case MAP_INHERIT_SHARE:
		break;
	default:
		UVMHIST_LOG(maphist,"<- done (INVALID ARG)",0,0,0,0);
		return (EINVAL);
	}
d3609 1
a3609 6
	if (start > end)
		return EINVAL;
	start = MAX(start, map->min_offset);
	end = MIN(end, map->max_offset);
	if (start >= end)
		return 0;
d3611 5
a3615 1
	vm_map_lock(map);
d3617 3
a3619 5
	entry = uvm_map_entrybyaddr(&map->addr, start);
	if (entry->end > start)
		UVM_MAP_CLIP_START(map, entry, start);
	else
		entry = RB_NEXT(uvm_map_addr, &map->addr, entry);
a3620 5
	while (entry != NULL && entry->start < end) {
		UVM_MAP_CLIP_END(map, entry, end);
		entry->inheritance = new_inheritance;
		entry = RB_NEXT(uvm_map_addr, &map->addr, entry);
	}
d3622 6
a3627 4
	vm_map_unlock(map);
	UVMHIST_LOG(maphist,"<- done (OK)",0,0,0,0);
	return (0);
}
d3629 1
a3629 136
/* 
 * uvm_map_advice: set advice code for range of addrs in map.
 *
 * => map must be unlocked
 */
int
uvm_map_advice(struct vm_map *map, vaddr_t start, vaddr_t end, int new_advice)
{
	struct vm_map_entry *entry;
	UVMHIST_FUNC("uvm_map_advice"); UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist,"(map=%p,start=0x%lx,end=0x%lx,new_adv=0x%lx)",
	    map, start, end, new_advice);

	switch (new_advice) {
	case MADV_NORMAL:
	case MADV_RANDOM:
	case MADV_SEQUENTIAL:
		break;
	default:
		UVMHIST_LOG(maphist,"<- done (INVALID ARG)",0,0,0,0);
		return (EINVAL);
	}

	if (start > end)
		return EINVAL;
	start = MAX(start, map->min_offset);
	end = MIN(end, map->max_offset);
	if (start >= end)
		return 0;

	vm_map_lock(map);

	entry = uvm_map_entrybyaddr(&map->addr, start);
	if (entry != NULL && entry->end > start)
		UVM_MAP_CLIP_START(map, entry, start);
	else if (entry!= NULL)
		entry = RB_NEXT(uvm_map_addr, &map->addr, entry);

	/*
	 * XXXJRT: disallow holes?
	 */

	while (entry != NULL && entry->start < end) {
		UVM_MAP_CLIP_END(map, entry, end);
		entry->advice = new_advice;
		entry = RB_NEXT(uvm_map_addr, &map->addr, entry);
	}

	vm_map_unlock(map);
	UVMHIST_LOG(maphist,"<- done (OK)",0,0,0,0);
	return (0);
}

/*
 * uvm_map_extract: extract a mapping from a map and put it somewhere
 * in the kernel_map, setting protection to max_prot.
 *
 * => map should be unlocked (we will write lock it and kernel_map)
 * => returns 0 on success, error code otherwise
 * => start must be page aligned
 * => len must be page sized
 * => flags:
 *      UVM_EXTRACT_FIXPROT: set prot to maxprot as we go
 * Mappings are QREF's.
 */
int
uvm_map_extract(struct vm_map *srcmap, vaddr_t start, vsize_t len,
    vaddr_t *dstaddrp, int flags)
{
	struct uvm_map_deadq dead;
	struct vm_map_entry *first, *entry, *newentry;
	vaddr_t dstaddr;
	vaddr_t end;
	vaddr_t cp_start;
	vsize_t cp_len, cp_off;
	int error;

	TAILQ_INIT(&dead);
	end = start + len;

	/*
	 * Sanity check on the parameters.
	 * Also, since the mapping may not contain gaps, error out if the
	 * mapped area is not in source map.
	 */

	if ((start & PAGE_MASK) != 0 || (end & PAGE_MASK) != 0 || end < start)
		return EINVAL;
	if (start < srcmap->min_offset || end > srcmap->max_offset)
		return EINVAL;

	/*
	 * Initialize dead entries.
	 * Handle len == 0 case.
	 */

	if (len == 0)
		return 0;

	/*
	 * Acquire lock on srcmap.
	 */
	vm_map_lock(srcmap);

	/*
	 * Lock srcmap, lookup first and last entry in <start,len>.
	 */
	first = uvm_map_entrybyaddr(&srcmap->addr, start);

	/*
	 * Check that the range is contiguous.
	 */
	for (entry = first; entry != NULL && entry->end < end;
	    entry = RB_NEXT(uvm_map_addr, &map->addr, entry)) {
		if (FREE_END(entry) != entry->end || UVM_ET_ISHOLE(entry)) {
			error = EINVAL;
			goto fail;
		}
	}
	if (entry == NULL || UVM_ET_ISHOLE(entry)) {
		error = EINVAL;
		goto fail;
	}

	/*
	 * Handle need-copy flag.
	 * This may invalidate last, hence the re-initialization during the
	 * loop.
	 *
	 * Also, perform clipping of last if not UVM_EXTRACT_QREF.
	 */
	for (entry = first; entry != NULL && entry->start < end;
	    entry = RB_NEXT(uvm_map_addr, &map->addr, entry)) {
		if (UVM_ET_ISNEEDSCOPY(entry))
			amap_copy(srcmap, entry, M_NOWAIT, TRUE, start, end);
		if (UVM_ET_ISNEEDSCOPY(entry)) {
d3631 2
a3632 1
			 * amap_copy failure
a3633 9
			error = ENOMEM;
			goto fail;
		}
	}

	/*
	 * Lock destination map (kernel_map).
	 */
	vm_map_lock(kernel_map);
a3634 186
	if (uvm_map_findspace_tree(&kernel_map->free, len, UVM_UNKNOWN_OFFSET,
	    0, kernel_map->flags & VM_MAP_GUARDPAGES, &dstaddr, kernel_map) ==
	    NULL) {
		error = ENOMEM;
		goto fail2;
	}
	*dstaddrp = dstaddr;

	/*
	 * We now have srcmap and kernel_map locked.
	 * dstaddr contains the destination offset in dstmap.
	 */

	/*
	 * step 1: start looping through map entries, performing extraction.
	 */
	for (entry = first; entry != NULL && entry->start < end;
	    entry = RB_NEXT(uvm_map_addr, &map->addr, entry)) {
		KDASSERT(!UVM_ET_ISNEEDSCOPY(entry));
		if (UVM_ET_ISHOLE(entry))
			continue;

		/*
		 * Calculate uvm_mapent_clone parameters.
		 */
		cp_start = entry->start;
		if (cp_start < start) {
			cp_off = start - cp_start;
			cp_start = start;
		} else
			cp_off = 0;
		cp_len = MIN(entry->end, end) - cp_start;

		newentry = uvm_mapent_clone(kernel_map,
		    cp_start - start + dstaddr, cp_len, cp_off,
		    entry, &dead, flags, AMAP_SHARED | AMAP_REFALL);
		if (newentry == NULL) {
			error = ENOMEM;
			goto fail2_unmap;
		}
		kernel_map->size += cp_len;
		if (flags & UVM_EXTRACT_FIXPROT)
			newentry->protection = newentry->max_protection;
	}

	/*
	 * step 2: perform pmap copy.
	 */
	for (entry = first; entry != NULL && entry->start < end;
	    entry = RB_NEXT(uvm_map_addr, &map->addr, entry)) {
		/*
		 * Calculate uvm_mapent_clone parameters (again).
		 */
		cp_start = entry->start;
		if (cp_start < start)
			cp_start = start;
		cp_len = MIN(entry->end, end) - cp_start;

		pmap_copy(kernel_map->pmap, srcmap->pmap,
		    cp_start - start + dstaddr, cp_len, cp_start);
	}
	pmap_update(kernel_map->pmap);

	error = 0;

	/*
	 * Unmap copied entries on failure.
	 */
fail2_unmap:
	if (error) {
		uvm_unmap_remove(kernel_map, dstaddr, dstaddr + len, &dead,
		    FALSE, TRUE);
	}

	/*
	 * Release maps, release dead entries.
	 */
fail2:
	vm_map_unlock(kernel_map);

fail:
	vm_map_unlock(srcmap);

	uvm_unmap_detach(&dead, 0);

	return error;
}

/*
 * uvm_map_clean: clean out a map range
 *
 * => valid flags:
 *   if (flags & PGO_CLEANIT): dirty pages are cleaned first
 *   if (flags & PGO_SYNCIO): dirty pages are written synchronously
 *   if (flags & PGO_DEACTIVATE): any cached pages are deactivated after clean
 *   if (flags & PGO_FREE): any cached pages are freed after clean
 * => returns an error if any part of the specified range isn't mapped
 * => never a need to flush amap layer since the anonymous memory has 
 *	no permanent home, but may deactivate pages there
 * => called from sys_msync() and sys_madvise()
 * => caller must not write-lock map (read OK).
 * => we may sleep while cleaning if SYNCIO [with map read-locked]
 */

int	amap_clean_works = 1;	/* XXX for now, just in case... */

int
uvm_map_clean(struct vm_map *map, vaddr_t start, vaddr_t end, int flags)
{
	struct vm_map_entry *first, *entry;
	struct vm_amap *amap;
	struct vm_anon *anon;
	struct vm_page *pg;
	struct uvm_object *uobj;
	vaddr_t cp_start, cp_end;
	int refs;
	int error;
	boolean_t rv;

	UVMHIST_FUNC("uvm_map_clean"); UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist,"(map=%p,start=0x%lx,end=0x%lx,flags=0x%lx)",
	    map, start, end, flags);
	KASSERT((flags & (PGO_FREE|PGO_DEACTIVATE)) !=
	    (PGO_FREE|PGO_DEACTIVATE));

	if (start > end || start < map->min_offset || end > map->max_offset)
		return EINVAL;

	vm_map_lock_read(map);
	first = uvm_map_entrybyaddr(&map->addr, start);

	/*
	 * Make a first pass to check for holes.
	 */
	for (entry = first; entry->start < end;
	    entry = RB_NEXT(uvm_map_addr, &map->addr, entry)) {
		if (UVM_ET_ISSUBMAP(entry)) {
			vm_map_unlock_read(map);
			return EINVAL;
		}
		if (UVM_ET_ISSUBMAP(entry) ||
		    UVM_ET_ISHOLE(entry) ||
		    (entry->end < end && FREE_END(entry) != entry->end)) {
			vm_map_unlock_read(map);
			return EFAULT;
		}
	}

	error = 0;
	for (entry = first; entry != NULL && entry->start < end;
	    entry = RB_NEXT(uvm_map_addr, &map->addr, entry)) {
		amap = entry->aref.ar_amap;	/* top layer */
		if (UVM_ET_ISOBJ(entry))
			uobj = entry->object.uvm_obj;
		else
			uobj = NULL;

		/*
		 * No amap cleaning necessary if:
		 *  - there's no amap
		 *  - we're not deactivating or freeing pages.
		 */
		if (amap == NULL || (flags & (PGO_DEACTIVATE|PGO_FREE)) == 0)
			goto flush_object;
		if (!amap_clean_works)
			goto flush_object;

		cp_start = MAX(entry->start, start);
		cp_end = MIN(entry->end, end);

		for (; cp_start != cp_end; cp_start += PAGE_SIZE) {
			anon = amap_lookup(&entry->aref,
			    cp_start - entry->start);
			if (anon == NULL)
				continue;

			simple_lock(&anon->an_lock); /* XXX */

			pg = anon->an_page;
			if (pg == NULL) {
				simple_unlock(&anon->an_lock);
				continue;
			}

			switch (flags & (PGO_CLEANIT|PGO_FREE|PGO_DEACTIVATE)) {
d3636 4
a3639 4
			 * XXX In these first 3 cases, we always just
			 * XXX deactivate the page.  We may want to
			 * XXX handle the different cases more
			 * XXX specifically, in the future.
a3640 10
			case PGO_CLEANIT|PGO_FREE:
			case PGO_CLEANIT|PGO_DEACTIVATE:
			case PGO_DEACTIVATE:
deactivate_it:
				/* skip the page if it's loaned or wired */
				if (pg->loan_count != 0 ||
				    pg->wire_count != 0) {
					simple_unlock(&anon->an_lock);
					break;
				}
d3642 5
a3646 49
				uvm_lock_pageq();

				/*
				 * skip the page if it's not actually owned
				 * by the anon (may simply be loaned to the
				 * anon).
				 */
				if ((pg->pg_flags & PQ_ANON) == 0) {
					KASSERT(pg->uobject == NULL);
					uvm_unlock_pageq();
					simple_unlock(&anon->an_lock);
					break;
				}
				KASSERT(pg->uanon == anon);

				/* zap all mappings for the page. */
				pmap_page_protect(pg, VM_PROT_NONE);

				/* ...and deactivate the page. */
				uvm_pagedeactivate(pg);

				uvm_unlock_pageq();
				simple_unlock(&anon->an_lock);
				break;

			case PGO_FREE:

				/*
				 * If there are mutliple references to
				 * the amap, just deactivate the page.
				 */
				if (amap_refs(amap) > 1)
					goto deactivate_it;

				/* XXX skip the page if it's wired */
				if (pg->wire_count != 0) {
					simple_unlock(&anon->an_lock);
					break;
				}
				amap_unadd(&entry->aref,
				    cp_start - entry->start);
				refs = --anon->an_ref;
				simple_unlock(&anon->an_lock);
				if (refs == 0)
					uvm_anfree(anon);
				break;

			default:
				panic("uvm_map_clean: weird flags");
a3647 1
		}
d3649 3
a3651 3
flush_object:
		cp_start = MAX(entry->start, start);
		cp_end = MIN(entry->end, end);
d3653 2
a3654 16
		/*
		 * flush pages if we've got a valid backing object.
		 *
		 * Don't PGO_FREE if we don't have write permission
		 * and don't flush if this is a copy-on-write object
		 * since we can't know our permissions on it.
		 */
		if (uobj != NULL &&
		    ((flags & PGO_FREE) == 0 ||
		     ((entry->max_protection & VM_PROT_WRITE) != 0 &&
		      (entry->etype & UVM_ET_COPYONWRITE) == 0))) {
			simple_lock(&uobj->vmobjlock);
			rv = uobj->pgops->pgo_flush(uobj,
			    cp_start - entry->start + entry->offset,
			    cp_end - entry->start + entry->offset, flags);
			simple_unlock(&uobj->vmobjlock);
d3656 23
a3678 185
			if (rv == FALSE)
				error = EFAULT;
		}
	}

	vm_map_unlock_read(map);
	return error;
}

/*
 * UVM_MAP_CLIP_END implementation
 */
void
uvm_map_clip_end(struct vm_map *map, struct vm_map_entry *entry, vaddr_t addr)
{
	struct vm_map_entry *tmp;

	KASSERT(entry->start < addr && FREE_END(entry) > addr);
	tmp = uvm_mapent_alloc(map, 0);

	/*
	 * Invoke splitentry.
	 */
	uvm_map_splitentry(map, entry, tmp, addr);
}

/*
 * UVM_MAP_CLIP_START implementation
 *
 * Clippers are required to not change the pointers to the entry they are
 * clipping on.
 * Since uvm_map_splitentry turns the original entry into the lowest
 * entry (address wise) we do a swap between the new entry and the original
 * entry, prior to calling uvm_map_splitentry.
 */
void
uvm_map_clip_start(struct vm_map *map, struct vm_map_entry *entry, vaddr_t addr)
{
	struct vm_map_entry *tmp;
	struct uvm_map_free *free;

	/*
	 * Copy entry.
	 */
	KASSERT(entry->start < addr && FREE_END(entry) > addr);
	tmp = uvm_mapent_alloc(map, 0);
	uvm_mapent_copy(entry, tmp);

	/*
	 * Put new entry in place of original entry.
	 */
	free = UVM_FREE(map, FREE_START(entry));
	uvm_mapent_addr_remove(map, entry);
	if (entry->fspace > 0 && free) {
		uvm_mapent_free_remove(map, free, entry);
		uvm_mapent_free_insert(map, free, tmp);
	}
	uvm_mapent_addr_insert(map, tmp);

	/*
	 * Invoke splitentry.
	 */
	uvm_map_splitentry(map, tmp, entry, addr);
}

/*
 * Boundary fixer.
 */
static __inline vaddr_t uvm_map_boundfix(vaddr_t, vaddr_t, vaddr_t);
static __inline vaddr_t
uvm_map_boundfix(vaddr_t min, vaddr_t max, vaddr_t bound)
{
	return (min < bound && max > bound) ? bound : max;
}

/*
 * Choose free list based on address at start of free space.
 */
struct uvm_map_free*
uvm_free(struct vm_map *map, vaddr_t addr)
{
	/* Special case the first page, to prevent mmap from returning 0. */
	if (addr < VMMAP_MIN_ADDR)
		return NULL;

	if ((map->flags & VM_MAP_ISVMSPACE) == 0) {
		if (addr >= uvm_maxkaddr)
			return NULL;
	} else {
		/* addr falls within brk() area. */
		if (addr >= map->b_start && addr < map->b_end)
			return &map->bfree;
		/* addr falls within stack area. */
		if (addr >= map->s_start && addr < map->s_end)
			return &map->bfree;
	}
	return &map->free;
}

/*
 * Returns the first free-memory boundary that is crossed by [min-max].
 */
vsize_t
uvm_map_boundary(struct vm_map *map, vaddr_t min, vaddr_t max)
{
	/* Treat the first page special, mmap returning 0 breaks too much. */
	max = uvm_map_boundfix(min, max, VMMAP_MIN_ADDR);

	if ((map->flags & VM_MAP_ISVMSPACE) == 0) {
		max = uvm_map_boundfix(min, max, uvm_maxkaddr);
	} else {
		max = uvm_map_boundfix(min, max, map->b_start);
		max = uvm_map_boundfix(min, max, map->b_end);
		max = uvm_map_boundfix(min, max, map->s_start);
		max = uvm_map_boundfix(min, max, map->s_end);
	}
	return max;
}

/*
 * Update map allocation start and end addresses from proc vmspace.
 */
void
uvm_map_vmspace_update(struct vm_map *map,
    struct uvm_map_deadq *dead, int flags)
{
	struct vmspace *vm;
	vaddr_t b_start, b_end, s_start, s_end;

	KASSERT(map->flags & VM_MAP_ISVMSPACE);
	KASSERT(offsetof(struct vmspace, vm_map) == 0);

	/*
	 * Derive actual allocation boundaries from vmspace.
	 */
	vm = (struct vmspace *)map;
	b_start = (vaddr_t)vm->vm_daddr;
	b_end   = b_start + BRKSIZ;
	s_start = MIN((vaddr_t)vm->vm_maxsaddr, (vaddr_t)vm->vm_minsaddr);
	s_end   = MAX((vaddr_t)vm->vm_maxsaddr, (vaddr_t)vm->vm_minsaddr);
#ifdef DIAGNOSTIC
	if ((b_start & PAGE_MASK) != 0 || (b_end & PAGE_MASK) != 0 ||
	    (s_start & PAGE_MASK) != 0 || (s_end & PAGE_MASK) != 0) {
		panic("uvm_map_vmspace_update: vmspace %p invalid bounds: "
		    "b=0x%lx-0x%lx s=0x%lx-0x%lx",
		    vm, b_start, b_end, s_start, s_end);
	}
#endif

	if (__predict_true(map->b_start == b_start && map->b_end == b_end &&
	    map->s_start == s_start && map->s_end == s_end))
		return;

	uvm_map_freelist_update(map, dead, b_start, b_end,
	    s_start, s_end, flags);
}

/*
 * Grow kernel memory.
 *
 * This function is only called for kernel maps when an allocation fails.
 *
 * If the map has a gap that is large enough to accomodate alloc_sz, this
 * function will make sure map->free will include it.
 */
void
uvm_map_kmem_grow(struct vm_map *map, struct uvm_map_deadq *dead,
    vsize_t alloc_sz, int flags)
{
	vsize_t sz;
	vaddr_t end;
	struct vm_map_entry *entry;

	/* Kernel memory only. */
	KASSERT((map->flags & VM_MAP_ISVMSPACE) == 0);
	/* Destroy free list. */
	uvm_map_freelist_update_clear(map, dead);

	/*
	 * Grow by ALLOCMUL * alloc_sz, but at least VM_MAP_KSIZE_DELTA.
	 *
	 * Don't handle the case where the multiplication overflows:
	 * if that happens, the allocation is probably too big anyway.
	 */
	sz = MAX(VM_MAP_KSIZE_ALLOCMUL * alloc_sz, VM_MAP_KSIZE_DELTA);
d3680 3
a3682 21
	/*
	 * Include the guard page in the hard minimum requirement of alloc_sz.
	 */
	if (map->flags & VM_MAP_GUARDPAGES)
		alloc_sz += PAGE_SIZE;

	/*
	 * Walk forward until a gap large enough for alloc_sz shows up.
	 *
	 * We assume the kernel map has no boundaries.
	 * uvm_maxkaddr may be zero.
	 */
	end = MAX(uvm_maxkaddr, map->min_offset);
	entry = uvm_map_entrybyaddr(&map->addr, end);
	while (entry && entry->fspace < alloc_sz)
		entry = RB_NEXT(uvm_map_addr, &map->addr, entry);
	if (entry) {
		end = MAX(FREE_START(entry), end);
		end += MIN(sz, map->max_offset - end);
	} else
		end = map->max_offset;
d3684 1
a3684 152
	/* Reserve pmap entries. */
#ifdef PMAP_GROWKERNEL
	uvm_maxkaddr = pmap_growkernel(end);
#else
	uvm_maxkaddr = end;
#endif
	/* Rebuild free list. */
	uvm_map_freelist_update_refill(map, flags);
}

/*
 * Freelist update subfunction: unlink all entries from freelists.
 */
void
uvm_map_freelist_update_clear(struct vm_map *map, struct uvm_map_deadq *dead)
{
	struct uvm_map_free *free;
	struct vm_map_entry *entry, *prev, *next;

	prev = NULL;
	for (entry = RB_MIN(uvm_map_addr, &map->addr); entry != NULL;
	    entry = next) {
		next = RB_NEXT(uvm_map_addr, &map->addr, entry);

		free = UVM_FREE(map, FREE_START(entry));
		if (entry->fspace > 0 && free)
			uvm_mapent_free_remove(map, free, entry);

		if (prev != NULL && entry->start == entry->end) {
			prev->fspace += FREE_END(entry) - entry->end;
			uvm_mapent_addr_remove(map, entry);
			DEAD_ENTRY_PUSH(dead, entry);
		} else
			prev = entry;
	}
}

/*
 * Freelist update subfunction: refill the freelists with entries.
 */
void
uvm_map_freelist_update_refill(struct vm_map *map, int flags)
{
	struct vm_map_entry *entry;
	vaddr_t min, max;

	RB_FOREACH(entry, uvm_map_addr, &map->addr) {
		min = FREE_START(entry);
		max = FREE_END(entry);
		entry->fspace = 0;

		entry = uvm_map_fix_space(map, entry, min, max, flags);
	}

	uvm_tree_sanity(map, __FILE__, __LINE__);
}

/*
 * Change {a,b}_{start,end} allocation ranges and associated free lists.
 */
void
uvm_map_freelist_update(struct vm_map *map, struct uvm_map_deadq *dead,
    vaddr_t b_start, vaddr_t b_end, vaddr_t s_start, vaddr_t s_end, int flags)
{
	KDASSERT(b_end >= b_start && s_end >= s_start);

	/* Clear all free lists. */
	uvm_map_freelist_update_clear(map, dead);

	/* Apply new bounds. */
	map->b_start = b_start;
	map->b_end   = b_end;
	map->s_start = s_start;
	map->s_end   = s_end;

	/* Refill free lists. */
	uvm_map_freelist_update_refill(map, flags);
}

/*
 * Correct space insert.
 */
struct vm_map_entry*
uvm_map_fix_space(struct vm_map *map, struct vm_map_entry *entry,
    vaddr_t min, vaddr_t max, int flags)
{
	struct uvm_map_free *free;
	vaddr_t lmax;

	KDASSERT(min <= max);
	KDASSERT((entry != NULL && FREE_END(entry) == min) ||
	    min == map->min_offset);
	while (min != max) {
		/*
		 * Claim guard page for entry.
		 */
		if ((map->flags & VM_MAP_GUARDPAGES) && entry != NULL &&
		    FREE_END(entry) == entry->end &&
		    entry->start != entry->end) {
			if (max - min == 2 * PAGE_SIZE) {
				/*
				 * If the free-space gap is exactly 2 pages,
				 * we make the guard 2 pages instead of 1.
				 * Because in a guarded map, an area needs
				 * at least 2 pages to allocate from:
				 * one page for the allocation and one for
				 * the guard.
				 */
				entry->guard = 2 * PAGE_SIZE;
				min = max;
			} else {
				entry->guard = PAGE_SIZE;
				min += PAGE_SIZE;
			}
			continue;
		}

		/*
		 * Handle the case where entry has a 2-page guard, but the
		 * space after entry is freed.
		 */
		if (entry != NULL && entry->fspace == 0 &&
		    entry->guard > PAGE_SIZE) {
			entry->guard = PAGE_SIZE;
			min = FREE_START(entry);
		}

		lmax = uvm_map_boundary(map, min, max);
		free = UVM_FREE(map, min);

		if (entry != NULL && free == UVM_FREE(map, FREE_START(entry))) {
			KDASSERT(FREE_END(entry) == min);
			if (entry->fspace > 0 && free != NULL)
				uvm_mapent_free_remove(map, free, entry);
			entry->fspace += lmax - min;
		} else {
			entry = uvm_mapent_alloc(map, flags);
			KDASSERT(entry != NULL);
			entry->end = entry->start = min;
			entry->guard = 0;
			entry->fspace = lmax - min;
			entry->object.uvm_obj = NULL;
			entry->offset = 0;
			entry->etype = 0;
			entry->protection = entry->max_protection = 0;
			entry->inheritance = 0;
			entry->wired_count = 0;
			entry->advice = 0;
			entry->aref.ar_pageoff = 0;
			entry->aref.ar_amap = NULL;
			uvm_mapent_addr_insert(map, entry);
		}
d3686 1
a3686 2
		if (free)
			uvm_mapent_free_insert(map, free, entry);
d3688 7
a3694 2
		min = lmax;
	}
d3696 19
a3714 2
	return entry;
}
d3716 31
a3746 18
/*
 * MQuery style of allocation.
 *
 * This allocator searches forward until sufficient space is found to map
 * the given size.
 *
 * XXX: factor in offset (via pmap_prefer) and protection?
 */
int
uvm_map_mquery(struct vm_map *map, vaddr_t *addr_p, vsize_t sz, voff_t offset,
    int flags)
{
	struct vm_map_entry *entry, *last;
	vaddr_t addr;
#ifdef PMAP_PREFER
	vaddr_t tmp;
#endif
	int error;
d3748 1
a3748 2
	addr = *addr_p;
	vm_map_lock_read(map);
d3750 9
a3758 4
#ifdef PMAP_PREFER
	if (!(flags & UVM_FLAG_FIXED) && offset != UVM_UNKNOWN_OFFSET)
		addr = PMAP_PREFER(offset, addr);
#endif
d3760 10
a3769 13
	/*
	 * First, check if the requested range is fully available.
	 */
	entry = uvm_map_entrybyaddr(&map->addr, addr);
	last = NULL;
	if (uvm_map_isavail(&map->addr, &entry, &last, addr, sz)) {
		error = 0;
		goto out;
	}
	if (flags & UVM_FLAG_FIXED) {
		error = EINVAL;
		goto out;
	}
d3771 1
a3771 1
	error = ENOMEM; /* Default error from here. */
d3773 80
a3852 11
	/*
	 * At this point, the memory at <addr, sz> is not available.
	 * The reasons are:
	 * [1] it's outside the map,
	 * [2] it starts in used memory (and therefore needs to move
	 *     toward the first free page in entry),
	 * [3] it starts in free memory but bumps into used memory.
	 *
	 * Note that for case [2], the forward moving is handled by the
	 * for loop below.
	 */
d3854 4
a3857 9
	if (entry == NULL) {
		/* [1] Outside the map. */
		if (addr >= map->max_offset)
			goto out;
		else
			entry = RB_MIN(uvm_map_addr, &map->addr);
	} else if (FREE_START(entry) <= addr) {
		/* [3] Bumped into used memory. */
		entry = RB_NEXT(uvm_map_addr, &map->addr, entry);
d3860 2
a3861 8
	/*
	 * Test if the next entry is sufficient for the allocation.
	 */
	for (; entry != NULL;
	    entry = RB_NEXT(uvm_map_addr, &map->addr, entry)) {
		if (entry->fspace == 0)
			continue;
		addr = FREE_START(entry);
d3863 4
a3866 1
restart:	/* Restart address checks on address change. */
d3868 2
a3869 13
#ifdef PMAP_PREFER
		if (offset != UVM_UNKNOWN_OFFSET) {
			tmp = (addr & ~(PMAP_PREFER_ALIGN() - 1)) |
			    PMAP_PREFER_OFFSET(offset);
			if (tmp < addr)
				tmp += PMAP_PREFER_ALIGN();
			if (addr >= FREE_END(entry))
				continue;
			if (addr != tmp) {
				addr = tmp;
				goto restart;
			}
		}
d3872 3
a3874 20
		/*
		 * Skip brk() allocation addresses.
		 */
		if (addr + sz > map->b_start && addr < map->b_end) {
			if (FREE_END(entry) > map->b_end) {
				addr = map->b_end;
				goto restart;
			} else
				continue;
		}
		/*
		 * Skip stack allocation addresses.
		 */
		if (addr + sz > map->s_start && addr < map->s_end) {
			if (FREE_END(entry) > map->s_end) {
				addr = map->s_end;
				goto restart;
			} else
				continue;
		}
d3876 1
a3876 6
		last = NULL;
		if (uvm_map_isavail(&map->addr, &entry, &last, addr, sz)) {
			error = 0;
			goto out;
		}
	}
d3878 3
a3880 6
out:
	vm_map_unlock_read(map);
	if (error == 0)
		*addr_p = addr;
	return error;
}
d3883 1
a3883 6
 * Determine allocation bias.
 *
 * Returns 1 if we should bias to high addresses, -1 for a bias towards low
 * addresses, or 0 for no bias.
 * The bias mechanism is intended to avoid clashing with brk() and stack
 * areas.
d3885 4
a3888 2
int
uvm_mapent_bias(struct vm_map *map, struct vm_map_entry *entry)
d3890 1
a3890 1
	vaddr_t start, end;
d3892 7
a3898 14
	start = FREE_START(entry);
	end = FREE_END(entry);

	/*
	 * Stay at the top of brk() area.
	 */
	if (end >= map->b_start && start < map->b_end)
		return 1;
	/*
	 * Stay at the far end of the stack area.
	 */
	if (end >= map->s_start && start < map->s_end) {
#ifdef MACHINE_STACK_GROWS_UP
		return 1;
d3900 2
a3901 1
		return -1;
d3903 16
d3920 1
d3922 3
a3924 5
	/*
	 * No bias, this area is meant for us.
	 */
	return 0;
}
d3926 8
d3935 6
a3940 4
boolean_t
vm_map_lock_try_ln(struct vm_map *map, char *file, int line)
{
	boolean_t rv;
d3942 8
a3949 5
	if (map->flags & VM_MAP_INTRSAFE) {
		rv = TRUE;
	} else {
		if (map->flags & VM_MAP_BUSY) {
			return (FALSE);
d3951 1
a3951 1
		rv = (rw_enter(&map->lock, RW_WRITE|RW_NOSLEEP) == 0);
d3953 2
a3954 6

	if (rv) {
		map->timestamp++;
		LPRINTF(("map   lock: %p (at %s %d)\n", map, file, line));
		uvm_tree_sanity(map, file, line);
		uvm_tree_size_chk(map, file, line);
d3956 1
d3958 3
a3960 2
	return (rv);
}
d3962 4
a3965 17
void
vm_map_lock_ln(struct vm_map *map, char *file, int line)
{
	if ((map->flags & VM_MAP_INTRSAFE) == 0) {
		do {
			while (map->flags & VM_MAP_BUSY) {
				map->flags |= VM_MAP_WANTLOCK;
				tsleep(&map->flags, PVM, (char *)vmmapbsy, 0);
			}
		} while (rw_enter(&map->lock, RW_WRITE|RW_SLEEPFAIL) != 0);
	}

	map->timestamp++;
	LPRINTF(("map   lock: %p (at %s %d)\n", map, file, line));
	uvm_tree_sanity(map, file, line);
	uvm_tree_size_chk(map, file, line);
}
d3968 4
a3971 1
vm_map_lock_read_ln(struct vm_map *map, char *file, int line)
d3973 3
a3975 6
	if ((map->flags & VM_MAP_INTRSAFE) == 0)
		rw_enter_read(&map->lock);
	LPRINTF(("map   lock: %p (at %s %d)\n", map, file, line));
	uvm_tree_sanity(map, file, line);
	uvm_tree_size_chk(map, file, line);
}
d3977 15
a3991 9
void
vm_map_unlock_ln(struct vm_map *map, char *file, int line)
{
	uvm_tree_sanity(map, file, line);
	uvm_tree_size_chk(map, file, line);
	LPRINTF(("map unlock: %p (at %s %d)\n", map, file, line));
	if ((map->flags & VM_MAP_INTRSAFE) == 0)
		rw_exit(&map->lock);
}
d3993 2
a3994 9
void
vm_map_unlock_read_ln(struct vm_map *map, char *file, int line)
{
	/* XXX: RO */ uvm_tree_sanity(map, file, line);
	/* XXX: RO */ uvm_tree_size_chk(map, file, line);
	LPRINTF(("map unlock: %p (at %s %d)\n", map, file, line));
	if ((map->flags & VM_MAP_INTRSAFE) == 0)
		rw_exit_read(&map->lock);
}
d3996 24
a4019 10
void
vm_map_downgrade_ln(struct vm_map *map, char *file, int line)
{
	uvm_tree_sanity(map, file, line);
	uvm_tree_size_chk(map, file, line);
	LPRINTF(("map unlock: %p (at %s %d)\n", map, file, line));
	LPRINTF(("map   lock: %p (at %s %d)\n", map, file, line));
	if ((map->flags & VM_MAP_INTRSAFE) == 0)
		rw_enter(&map->lock, RW_DOWNGRADE);
}
d4021 14
a4034 9
void
vm_map_upgrade_ln(struct vm_map *map, char *file, int line)
{
	/* XXX: RO */ uvm_tree_sanity(map, file, line);
	/* XXX: RO */ uvm_tree_size_chk(map, file, line);
	LPRINTF(("map unlock: %p (at %s %d)\n", map, file, line));
	if ((map->flags & VM_MAP_INTRSAFE) == 0) {
		rw_exit_read(&map->lock);
		rw_enter_write(&map->lock);
a4035 3
	LPRINTF(("map   lock: %p (at %s %d)\n", map, file, line));
	uvm_tree_sanity(map, file, line);
}
d4037 12
a4048 4
void
vm_map_busy_ln(struct vm_map *map, char *file, int line)
{
	map->flags |= VM_MAP_BUSY;
d4050 1
a4050 16

void
vm_map_unbusy_ln(struct vm_map *map, char *file, int line)
{
	int oflags;

	oflags = map->flags;
	map->flags &= ~(VM_MAP_BUSY|VM_MAP_WANTLOCK);
	if (oflags & VM_MAP_WANTLOCK)
		wakeup(&map->flags);
}


RB_GENERATE(uvm_map_addr, vm_map_entry, daddrs.addr_entry,
    uvm_mapentry_addrcmp);
RB_GENERATE(uvm_map_free_int, vm_map_entry, free_entry, uvm_mapentry_freecmp);
@


1.139
log
@Oops, I broke randomness. (Please upgrade, this is _bad_.)

sel_addr &= ~(pmap_align - 1);
with pmap_align allowed to be 0 (no PMAP_PREFER) is a bad idea.
Fix this by a conditional.

ok oga@@
@
text
@d927 1
a927 2
				if (pmap_align != 0)
					sel_addr &= ~(pmap_align - 1);
@


1.138
log
@Replace the lower bound PAGE_SIZE with VMMAP_MIN_ADDR.
This makes writing a diff that makes 64-bit unclean applications explode
a one-line diff.

ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.137 2011/05/29 15:18:19 ariane Exp $	*/
d927 2
a928 1
				sel_addr &= ~(pmap_align - 1);
@


1.137
log
@Fix parameter range clamping in vmmap routines.
The old VM_MAP_RANGE_CHECK macro was wrong and caused code to be unreadable
(argument altering macros are harmful).

Each function now treats the memory range outside the map as it would treat
free memory: if it would error on being given free memory, it'll error
in a similar fashion when the start,end parameters fall outside the map.
If it would accept free memory in its argument range, it'll silently accept
the outside-map memory too.

Confirmed to help ports build machines.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.136 2011/05/24 15:27:36 ariane Exp $	*/
d90 1
d4364 1
a4364 1
	if (addr < PAGE_SIZE)
d4388 1
a4388 1
	max = uvm_map_boundfix(min, max, PAGE_SIZE);
@


1.136
log
@Reimplement uvm/uvm_map.

vmmap is designed to perform address space randomized allocations,
without letting fragmentation of the address space go through the roof.

Some highlights:
- kernel address space randomization
- proper implementation of guardpages
- roughly 10% system time reduction during kernel build

Tested by alot of people on tech@@ and developers.
Theo's machines are still happy.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.135 2011/04/26 23:50:21 ariane Exp $	*/
a479 10
 * Clamp start and end to map boundaries.
 */
#define VM_MAP_RANGE_CHECK(_map, _start, _end)				\
	do {								\
		(_start) = MAX((_start), vm_map_min((_map)));		\
		(_end) = MIN((_end), vm_map_max((_map)));		\
		(_start) = MIN((_start), (_end));			\
	} while (0)

/*
d1788 3
a1790 2
	VM_MAP_RANGE_CHECK(map, start, end);
	if (start == end)
d2124 7
a2134 2
	VM_MAP_RANGE_CHECK(map, start, end);

d2916 5
a2920 2
	VM_MAP_RANGE_CHECK(map, start, end);
	if (start == end)
d3687 4
a3692 2
	VM_MAP_RANGE_CHECK(map, start, end);

d3830 7
a3838 2
	VM_MAP_RANGE_CHECK(map, start, end);

d3879 7
a3887 2
	VM_MAP_RANGE_CHECK(map, start, end);

d4131 3
a4134 1
	VM_MAP_RANGE_CHECK(map, start, end);
@


1.135
log
@MMU address space holes are at a fixed position (ofcourse).
Therefore set UVM_FLAG_FIXED and enforce this.

ok oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.134 2011/04/18 19:23:46 art Exp $	*/
d4 16
a19 1
/* 
d89 2
a105 2
#undef RB_AUGMENT
#define RB_AUGMENT(x) uvm_rb_augment(x)
d111 185
a304 6
 * Da history books
 */
UVMHIST_DECL(maphist);
UVMHIST_DECL(pdhist);

/*
a306 1

a311 1

a314 1
#ifdef PMAP_GROWKERNEL
d317 1
a317 1
 * space.  If we want to exceed this, we must grow the kernel
a322 1
#endif
d325 1
a325 1
 * macros
d327 5
d334 1
a334 1
 * uvm_map_entry_link: insert entry into a map
d336 3
a338 1
 * => map must be locked
a339 8
#define uvm_map_entry_link(map, after_where, entry) do { \
	(map)->nentries++; \
	(entry)->prev = (after_where); \
	(entry)->next = (after_where)->next; \
	(entry)->prev->next = (entry); \
	(entry)->next->prev = (entry); \
	uvm_rb_insert(map, entry); \
} while (0)
d341 5
a345 11
/*
 * uvm_map_entry_unlink: remove entry from a map
 *
 * => map must be locked
 */
#define uvm_map_entry_unlink(map, entry) do { \
	(map)->nentries--; \
	(entry)->next->prev = (entry)->prev; \
	(entry)->prev->next = (entry)->next; \
	uvm_rb_remove(map, entry); \
} while (0)
d348 1
a348 1
 * SAVE_HINT: saves the specified entry as the hint for future lookups.
d350 2
a351 11
 * => map need not be locked (protected by hint_lock).
 */
#define SAVE_HINT(map,check,value) do { \
	simple_lock(&(map)->hint_lock); \
	if ((map)->hint == (check)) \
		(map)->hint = (value); \
	simple_unlock(&(map)->hint_lock); \
} while (0)

/*
 * VM_MAP_RANGE_CHECK: check and correct range
d353 3
a355 1
 * => map must at least be read locked
d358 6
a363 8
#define VM_MAP_RANGE_CHECK(map, start, end) do { \
	if (start < vm_map_min(map)) 		\
		start = vm_map_min(map);        \
	if (end > vm_map_max(map))              \
		end = vm_map_max(map);          \
	if (start > end)                        \
		start = end;                    \
} while (0)
d366 1
a366 1
 * local prototypes
d368 5
d374 4
a377 6
void uvm_mapent_copy(struct vm_map_entry *, struct vm_map_entry *);
void uvm_map_entry_unwire(struct vm_map *, struct vm_map_entry *);
void uvm_map_reference_amap(struct vm_map_entry *, int);
void uvm_map_unreference_amap(struct vm_map_entry *, int);
int uvm_map_spacefits(struct vm_map *, vaddr_t *, vsize_t,
    struct vm_map_entry *, voff_t, vsize_t);
d379 4
a382 2
struct vm_map_entry	*uvm_mapent_alloc(struct vm_map *, int);
void			uvm_mapent_free(struct vm_map_entry *);
a383 1
#ifdef KVA_GUARDPAGES
d385 1
a385 1
 * Number of kva guardpages in use.
d387 7
a393 1
int kva_guardpages;
d396 4
d401 7
a407 6
/*
 * Tree manipulation.
 */
void uvm_rb_insert(struct vm_map *, struct vm_map_entry *);
void uvm_rb_remove(struct vm_map *, struct vm_map_entry *);
vsize_t uvm_rb_space(struct vm_map *, struct vm_map_entry *);
d409 4
a412 2
#ifdef DEBUG
int _uvm_tree_sanity(struct vm_map *map, const char *name);
d414 1
a414 2
vsize_t uvm_rb_subtree_space(struct vm_map_entry *);
void uvm_rb_fixup(struct vm_map *, struct vm_map_entry *);
d416 5
a420 9
static __inline int
uvm_compare(struct vm_map_entry *a, struct vm_map_entry *b)
{
	if (a->start < b->start)
		return (-1);
	else if (a->start > b->start)
		return (1);
	
	return (0);
d423 8
d432 7
a438 4
static __inline void
uvm_rb_augment(struct vm_map_entry *entry)
{
	entry->space = uvm_rb_subtree_space(entry);
d441 5
a445 6
RB_PROTOTYPE(uvm_tree, vm_map_entry, rb_entry, uvm_compare);

RB_GENERATE(uvm_tree, vm_map_entry, rb_entry, uvm_compare);

vsize_t
uvm_rb_space(struct vm_map *map, struct vm_map_entry *entry)
d447 1
a447 2
	struct vm_map_entry *next;
	vaddr_t space;
d449 7
a455 26
	if ((next = entry->next) == &map->header)
		space = map->max_offset - entry->end;
	else {
		KASSERT(next);
		space = next->start - entry->end;
	}
	return (space);
}
		
vsize_t
uvm_rb_subtree_space(struct vm_map_entry *entry)
{
	vaddr_t space, tmp;

	space = entry->ownspace;
	if (RB_LEFT(entry, rb_entry)) {
		tmp = RB_LEFT(entry, rb_entry)->space;
		if (tmp > space)
			space = tmp;
	}

	if (RB_RIGHT(entry, rb_entry)) {
		tmp = RB_RIGHT(entry, rb_entry)->space;
		if (tmp > space)
			space = tmp;
	}
d457 4
a460 1
	return (space);
d463 3
d467 1
a467 1
uvm_rb_fixup(struct vm_map *map, struct vm_map_entry *entry)
d469 8
a476 5
	/* We need to traverse to the very top */
	do {
		entry->ownspace = uvm_rb_space(map, entry);
		entry->space = uvm_rb_subtree_space(entry);
	} while ((entry = RB_PARENT(entry, rb_entry)) != NULL);
d479 9
a487 5
void
uvm_rb_insert(struct vm_map *map, struct vm_map_entry *entry)
{
	vaddr_t space = uvm_rb_space(map, entry);
	struct vm_map_entry *tmp;
d489 11
a499 10
	entry->ownspace = entry->space = space;
	tmp = RB_INSERT(uvm_tree, &(map)->rbhead, entry);
#ifdef DIAGNOSTIC
	if (tmp != NULL)
		panic("uvm_rb_insert: duplicate entry?");
#endif
	uvm_rb_fixup(map, entry);
	if (entry->prev != &map->header)
		uvm_rb_fixup(map, entry->prev);
}
d501 5
a505 2
void
uvm_rb_remove(struct vm_map *map, struct vm_map_entry *entry)
d507 4
a510 1
	struct vm_map_entry *parent;
d512 4
a515 13
	parent = RB_PARENT(entry, rb_entry);
	RB_REMOVE(uvm_tree, &(map)->rbhead, entry);
	if (entry->prev != &map->header)
		uvm_rb_fixup(map, entry->prev);
	if (parent)
		uvm_rb_fixup(map, parent);
}

#ifdef DEBUG
#define uvm_tree_sanity(x,y) _uvm_tree_sanity(x,y)
#else
#define uvm_tree_sanity(x,y)
#endif
d517 7
a523 6
#ifdef DEBUG
int
_uvm_tree_sanity(struct vm_map *map, const char *name)
{
	struct vm_map_entry *tmp, *trtmp;
	int n = 0, i = 1;
d525 5
a529 41
	RB_FOREACH(tmp, uvm_tree, &map->rbhead) {
		if (tmp->ownspace != uvm_rb_space(map, tmp)) {
			printf("%s: %d/%d ownspace %x != %x %s\n",
			    name, n + 1, map->nentries,
			    tmp->ownspace, uvm_rb_space(map, tmp),
			    tmp->next == &map->header ? "(last)" : "");
			goto error;
		}
	}
	trtmp = NULL;
	RB_FOREACH(tmp, uvm_tree, &map->rbhead) {
		if (tmp->space != uvm_rb_subtree_space(tmp)) {
			printf("%s: space %d != %d\n",
			    name, tmp->space, uvm_rb_subtree_space(tmp));
			goto error;
		}
		if (trtmp != NULL && trtmp->start >= tmp->start) {
			printf("%s: corrupt: 0x%lx >= 0x%lx\n",
			    name, trtmp->start, tmp->start);
			goto error;
		}
		n++;

	    trtmp = tmp;
	}

	if (n != map->nentries) {
		printf("%s: nentries: %d vs %d\n",
		    name, n, map->nentries);
		goto error;
	}

	for (tmp = map->header.next; tmp && tmp != &map->header;
	    tmp = tmp->next, i++) {
		trtmp = RB_FIND(uvm_tree, &map->rbhead, tmp);
		if (trtmp != tmp) {
			printf("%s: lookup: %d: %p - %p: %p\n",
			    name, i, tmp, trtmp,
			    RB_PARENT(tmp, rb_entry));
			goto error;
		}
d532 1
a532 7
	return (0);
 error:
#ifdef	DDB
	/* handy breakpoint location for error case */
	__asm(".globl treesanity_label\ntreesanity_label:");
#endif
	return (-1);
a533 1
#endif
d536 1
a536 1
 * uvm_mapent_alloc: allocate a map entry
d538 2
a539 3

struct vm_map_entry *
uvm_mapent_alloc(struct vm_map *map, int flags)
d541 1
a541 4
	struct vm_map_entry *me, *ne;
	int s, i;
	int pool_flags;
	UVMHIST_FUNC("uvm_mapent_alloc"); UVMHIST_CALLED(maphist);
d543 8
a550 42
	pool_flags = PR_WAITOK;
	if (flags & UVM_FLAG_TRYLOCK)
		pool_flags = PR_NOWAIT;

	if (map->flags & VM_MAP_INTRSAFE || cold) {
		s = splvm();
		simple_lock(&uvm.kentry_lock);
		me = uvm.kentry_free;
		if (me == NULL) {
			ne = km_alloc(PAGE_SIZE, &kv_page, &kp_dirty,
			    &kd_nowait);
			if (ne == NULL)
				panic("uvm_mapent_alloc: cannot allocate map "
				    "entry");
			for (i = 0;
			    i < PAGE_SIZE / sizeof(struct vm_map_entry) - 1;
			    i++)
				ne[i].next = &ne[i + 1];
			ne[i].next = NULL;
			me = ne;
			if (ratecheck(&uvm_kmapent_last_warn_time,
			    &uvm_kmapent_warn_rate))
				printf("uvm_mapent_alloc: out of static "
				    "map entries\n");
		}
		uvm.kentry_free = me->next;
		uvmexp.kmapent++;
		simple_unlock(&uvm.kentry_lock);
		splx(s);
		me->flags = UVM_MAP_STATIC;
	} else if (map == kernel_map) {
		splassert(IPL_NONE);
		me = pool_get(&uvm_map_entry_kmem_pool, pool_flags);
		if (me == NULL)
			goto out;
		me->flags = UVM_MAP_KMEM;
	} else {
		splassert(IPL_NONE);
		me = pool_get(&uvm_map_entry_pool, pool_flags);
		if (me == NULL)
			goto out;
		me->flags = 0;
d552 1
a552 5

out:
	UVMHIST_LOG(maphist, "<- new entry=%p [kentry=%ld]", me,
	    ((map->flags & VM_MAP_INTRSAFE) != 0 || map == kernel_map), 0, 0);
	return(me);
d556 1
a556 3
 * uvm_mapent_free: free map entry
 *
 * => XXX: static pool for kernel map?
d558 5
d564 2
a565 5
void
uvm_mapent_free(struct vm_map_entry *me)
{
	int s;
	UVMHIST_FUNC("uvm_mapent_free"); UVMHIST_CALLED(maphist);
d567 6
a572 16
	UVMHIST_LOG(maphist,"<- freeing map entry=%p [flags=%ld]",
		me, me->flags, 0, 0);
	if (me->flags & UVM_MAP_STATIC) {
		s = splvm();
		simple_lock(&uvm.kentry_lock);
		me->next = uvm.kentry_free;
		uvm.kentry_free = me;
		uvmexp.kmapent--;
		simple_unlock(&uvm.kentry_lock);
		splx(s);
	} else if (me->flags & UVM_MAP_KMEM) {
		splassert(IPL_NONE);
		pool_put(&uvm_map_entry_kmem_pool, me);
	} else {
		splassert(IPL_NONE);
		pool_put(&uvm_map_entry_pool, me);
d574 1
d578 8
a585 1
 * uvm_mapent_copy: copy a map entry, preserving flags
d587 2
a588 3

void
uvm_mapent_copy(struct vm_map_entry *src, struct vm_map_entry *dst)
d590 1
a590 2
	memcpy(dst, src, ((char *)&src->uvm_map_entry_stop_copy) -
	    ((char *)src));
d592 2
d596 5
a600 1
 * uvm_map_entry_unwire: unwire a map entry
d602 4
a605 1
 * => map should be locked by caller
d607 4
a610 2
void
uvm_map_entry_unwire(struct vm_map *map, struct vm_map_entry *entry)
d612 7
d620 2
a621 3
	entry->wired_count = 0;
	uvm_fault_unwire_locked(map, entry->start, entry->end);
}
d623 1
d625 81
a705 9
/*
 * wrapper for calling amap_ref()
 */
void
uvm_map_reference_amap(struct vm_map_entry *entry, int flags)
{
	amap_ref(entry->aref.ar_amap, entry->aref.ar_pageoff,
	    (entry->end - entry->start) >> PAGE_SHIFT, flags);
}
d707 2
d710 6
a715 9
/*
 * wrapper for calling amap_unref() 
 */
void
uvm_map_unreference_amap(struct vm_map_entry *entry, int flags)
{
	amap_unref(entry->aref.ar_amap, entry->aref.ar_pageoff,
	    (entry->end - entry->start) >> PAGE_SHIFT, flags);
}
d717 1
d719 19
a737 4
/*
 * uvm_map_init: init mapping system at boot time.   note that we allocate
 * and init the static pool of structs vm_map_entry for the kernel here.
 */
a738 7
void
uvm_map_init(void)
{
	static struct vm_map_entry kernel_map_entry[MAX_KMAPENT];
#if defined(UVMHIST)
	static struct uvm_history_ent maphistbuf[100];
	static struct uvm_history_ent pdhistbuf[100];
a739 1
	int lcv;
d741 88
a828 3
	/*
	 * first, init logging system.
	 */
d830 9
a838 13
	UVMHIST_FUNC("uvm_map_init");
	UVMHIST_INIT_STATIC(maphist, maphistbuf);
	UVMHIST_INIT_STATIC(pdhist, pdhistbuf);
	UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist,"<starting uvm map system>", 0, 0, 0, 0);
	UVMCNT_INIT(uvm_map_call,  UVMCNT_CNT, 0,
	    "# uvm_map() successful calls", 0);
	UVMCNT_INIT(map_backmerge, UVMCNT_CNT, 0, "# uvm_map() back merges", 0);
	UVMCNT_INIT(map_forwmerge, UVMCNT_CNT, 0, "# uvm_map() missed forward",
	    0);
	UVMCNT_INIT(map_nousermerge, UVMCNT_CNT, 0, "# back merges skipped", 0);
	UVMCNT_INIT(uvm_mlk_call,  UVMCNT_CNT, 0, "# map lookup calls", 0);
	UVMCNT_INIT(uvm_mlk_hint,  UVMCNT_CNT, 0, "# map lookup hint hits", 0);
d840 13
a852 3
	/*
	 * now set up static pool of kernel map entries ...
	 */
d854 3
a856 6
	simple_lock_init(&uvm.kentry_lock);
	uvm.kentry_free = NULL;
	for (lcv = 0 ; lcv < MAX_KMAPENT ; lcv++) {
		kernel_map_entry[lcv].next = uvm.kentry_free;
		uvm.kentry_free = &kernel_map_entry[lcv];
	}
d859 1
a859 1
	 * initialize the map-related pools.
d861 3
a863 8
	pool_init(&uvm_vmspace_pool, sizeof(struct vmspace),
	    0, 0, 0, "vmsppl", &pool_allocator_nointr);
	pool_init(&uvm_map_entry_pool, sizeof(struct vm_map_entry),
	    0, 0, 0, "vmmpepl", &pool_allocator_nointr);
	pool_init(&uvm_map_entry_kmem_pool, sizeof(struct vm_map_entry),
	    0, 0, 0, "vmmpekpl", NULL);
	pool_sethiwat(&uvm_map_entry_pool, 8192);
}
d865 18
a882 3
/*
 * clippers
 */
d884 28
a911 8
/*
 * uvm_map_clip_start: ensure that the entry begins at or after
 *	the starting address, if it doesn't we split the entry.
 * 
 * => caller should use UVM_MAP_CLIP_START macro rather than calling
 *    this directly
 * => map must be locked by caller
 */
d913 14
a926 6
void
uvm_map_clip_start(struct vm_map *map, struct vm_map_entry *entry,
    vaddr_t start)
{
	struct vm_map_entry *new_entry;
	vaddr_t new_adj;
d928 9
a936 1
	/* uvm_map_simplify_entry(map, entry); */ /* XXX */
d938 7
a944 1
	uvm_tree_sanity(map, "clip_start entry");
d946 3
a948 5
	/*
	 * Split off the front portion.  note that we must insert the new
	 * entry BEFORE this one, so that this entry has the specified
	 * starting address.
	 */
d950 7
a956 2
	new_entry = uvm_mapent_alloc(map, 0);
	uvm_mapent_copy(entry, new_entry); /* entry -> new_entry */
d958 3
a960 4
	new_entry->end = start; 
	new_adj = start - new_entry->start;
	if (entry->object.uvm_obj)
		entry->offset += new_adj;	/* shift start over */
d962 15
a976 5
	/* Does not change order for the RB tree */
	entry->start = start;

	if (new_entry->aref.ar_amap) {
		amap_splitref(&new_entry->aref, &entry->aref, new_adj);
d979 14
a992 11
	uvm_map_entry_link(map, entry->prev, new_entry);

	if (UVM_ET_ISSUBMAP(entry)) {
		/* ... unlikely to happen, but play it safe */
		 uvm_map_reference(new_entry->object.sub_map);
	} else {
		if (UVM_ET_ISOBJ(entry) && 
		    entry->object.uvm_obj->pgops &&
		    entry->object.uvm_obj->pgops->pgo_reference)
			entry->object.uvm_obj->pgops->pgo_reference(
			    entry->object.uvm_obj);
d994 1
d996 4
a999 1
	uvm_tree_sanity(map, "clip_start leave");
d1003 5
a1007 6
 * uvm_map_clip_end: ensure that the entry ends at or before
 *	the ending address, if it doesn't we split the reference
 * 
 * => caller should use UVM_MAP_CLIP_END macro rather than calling
 *    this directly
 * => map must be locked by caller
d1009 3
a1011 3

void
uvm_map_clip_end(struct vm_map *map, struct vm_map_entry *entry, vaddr_t end)
d1013 1
a1013 2
	struct vm_map_entry *new_entry;
	vaddr_t new_adj; /* #bytes we move start forward */
d1015 17
a1031 5
	uvm_tree_sanity(map, "clip_end entry");
	/*
	 *	Create a new entry and insert it
	 *	AFTER the specified entry
	 */
d1033 3
a1035 2
	new_entry = uvm_mapent_alloc(map, 0);
	uvm_mapent_copy(entry, new_entry); /* entry -> new_entry */
d1037 9
a1045 9
	new_entry->start = entry->end = end;
	new_adj = end - entry->start;
	if (new_entry->object.uvm_obj)
		new_entry->offset += new_adj;

	if (entry->aref.ar_amap)
		amap_splitref(&entry->aref, &new_entry->aref, new_adj);
	
	uvm_rb_fixup(map, entry);
d1047 1
a1047 13
	uvm_map_entry_link(map, entry, new_entry);

	if (UVM_ET_ISSUBMAP(entry)) {
		/* ... unlikely to happen, but play it safe */
	 	uvm_map_reference(new_entry->object.sub_map);
	} else {
		if (UVM_ET_ISOBJ(entry) &&
		    entry->object.uvm_obj->pgops &&
		    entry->object.uvm_obj->pgops->pgo_reference)
			entry->object.uvm_obj->pgops->pgo_reference(
			    entry->object.uvm_obj);
	}
	uvm_tree_sanity(map, "clip_end leave");
a1049 4

/*
 *   M A P   -   m a i n   e n t r y   p o i n t
 */
d1051 1
a1051 1
 * uvm_map: establish a valid mapping in a map
d1053 3
a1055 5
 * => assume startp is page aligned.
 * => assume size is a multiple of PAGE_SIZE.
 * => assume sys_mmap provides enough of a "hint" to have us skip
 *	over text/data/bss area.
 * => map must be unlocked (we will lock it)
d1057 8
a1064 13
 *	 [1] <NULL,uoffset> 		== uoffset is a hint for PMAP_PREFER
 *	 [2] <NULL,UVM_UNKNOWN_OFFSET>	== don't PMAP_PREFER
 *	 [3] <uobj,uoffset>		== normal mapping
 *	 [4] <uobj,UVM_UNKNOWN_OFFSET>	== uvm_map finds offset based on VA
 *	
 *    case [4] is for kernel mappings where we don't know the offset until
 *    we've found a virtual address.   note that kernel object offsets are
 *    always relative to vm_map_min(kernel_map).
 *
 * => if `align' is non-zero, we try to align the virtual address to
 *	the specified alignment.  this is only a hint; if we can't
 *	do it, the address will be unaligned.  this is provided as
 *	a mechanism for large pages.
d1066 2
a1067 1
 * => XXXCDC: need way to map in external amap?
a1068 1

d1070 11
a1080 61
uvm_map_p(struct vm_map *map, vaddr_t *startp, vsize_t size,
    struct uvm_object *uobj, voff_t uoffset, vsize_t align, uvm_flag_t flags,
    struct proc *p)
{
	struct vm_map_entry *prev_entry, *new_entry;
#ifdef KVA_GUARDPAGES
	struct vm_map_entry *guard_entry;
#endif
	vm_prot_t prot = UVM_PROTECTION(flags), maxprot =
	    UVM_MAXPROTECTION(flags);
	vm_inherit_t inherit = UVM_INHERIT(flags);
	int advice = UVM_ADVICE(flags);
	int error;
	UVMHIST_FUNC("uvm_map");
	UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist, "(map=%p, *startp=0x%lx, size=%ld, flags=0x%lx)",
	    map, *startp, size, flags);
	UVMHIST_LOG(maphist, "  uobj/offset %p/%ld", uobj, (u_long)uoffset,0,0);

	/*
	 * Holes are incompatible with other types of mappings.
	 */
	if (flags & UVM_FLAG_HOLE) {
		KASSERT(uobj == NULL && (flags & UVM_FLAG_FIXED) != 0 &&
		    (flags & (UVM_FLAG_OVERLAY | UVM_FLAG_COPYONW)) == 0);
	}

#ifdef KVA_GUARDPAGES
	if (map == kernel_map && !(flags & UVM_FLAG_FIXED)) {
		/*
		 * kva_guardstart is initialized to the start of the kernelmap
		 * and cycles through the kva space.
		 * This way we should have a long time between re-use of kva.
		 */
		static vaddr_t kva_guardstart = 0;
		if (kva_guardstart == 0) {
			kva_guardstart = vm_map_min(map);
			printf("uvm_map: kva guard pages enabled: %p\n",
			    kva_guardstart);
		}
		size += PAGE_SIZE;	/* Add guard page at the end. */
		/*
		 * Try to fully exhaust kva prior to wrap-around.
		 * (This may eat your ram!)
		 */
		if (VM_MAX_KERNEL_ADDRESS - kva_guardstart < size) {
			static int wrap_counter = 0;
			printf("uvm_map: kva guard page wrap-around %d\n",
			    ++wrap_counter);
			kva_guardstart = vm_map_min(map);
		}
		*startp = kva_guardstart;
		/*
		 * Prepare for next round.
		 */
		kva_guardstart += size;
	}
#endif

	uvm_tree_sanity(map, "map entry");
d1088 1
a1088 1
	 * step 0: sanity check of protection code
d1090 8
d1099 6
a1104 4
	if ((prot & maxprot) != prot) {
		UVMHIST_LOG(maphist, "<- prot. failure: prot=0x%lx, max=0x%lx",
		    prot, maxprot,0,0);
		return (EACCES);
d1108 1
a1108 1
	 * step 1: figure out where to put new VM range
d1110 2
d1113 5
a1117 11
	if (vm_map_lock_try(map) == FALSE) {
		if (flags & UVM_FLAG_TRYLOCK)
			return (EFAULT);
		vm_map_lock(map); /* could sleep here */
	}
	if ((prev_entry = uvm_map_findspace(map, *startp, size, startp, 
	    uobj, uoffset, align, flags)) == NULL) {
		UVMHIST_LOG(maphist,"<- uvm_map_findspace failed!",0,0,0,0);
		vm_map_unlock(map);
		return (ENOMEM);
	}
d1119 2
a1120 2
#ifdef PMAP_GROWKERNEL
	{
d1122 4
a1125 2
		 * If the kernel pmap can't map the requested space,
		 * then allocate more resources for it.
d1127 5
a1131 5
		if (map == kernel_map && !(flags & UVM_FLAG_FIXED) &&
		    uvm_maxkaddr < (*startp + size))
			uvm_maxkaddr = pmap_growkernel(*startp + size);
	}
#endif
d1133 8
a1140 1
	UVMCNT_INCR(uvm_map_call);
d1142 19
a1160 18
	/*
	 * if uobj is null, then uoffset is either a VAC hint for PMAP_PREFER
	 * [typically from uvm_map_reserve] or it is UVM_UNKNOWN_OFFSET.   in 
	 * either case we want to zero it  before storing it in the map entry 
	 * (because it looks strange and confusing when debugging...)
	 * 
	 * if uobj is not null 
	 *   if uoffset is not UVM_UNKNOWN_OFFSET then we have a normal mapping
	 *      and we do not need to change uoffset.
	 *   if uoffset is UVM_UNKNOWN_OFFSET then we need to find the offset
	 *      now (based on the starting address of the map).   this case is
	 *      for kernel object mappings where we don't know the offset until
	 *      the virtual address is found (with uvm_map_findspace).   the
	 *      offset is the distance we are from the start of the map.
	 */

	if (uobj == NULL) {
		uoffset = 0;
d1162 5
a1166 35
		if (uoffset == UVM_UNKNOWN_OFFSET) {
			KASSERT(UVM_OBJ_IS_KERN_OBJECT(uobj));
			uoffset = *startp - vm_map_min(kernel_map);
		}
	}

	/*
	 * step 2: try and insert in map by extending previous entry, if
	 * possible
	 * XXX: we don't try and pull back the next entry.   might be useful
	 * for a stack, but we are currently allocating our stack in advance.
	 */

	if ((flags & UVM_FLAG_NOMERGE) == 0 && 
	    prev_entry->end == *startp && prev_entry != &map->header &&
	    prev_entry->object.uvm_obj == uobj) {

		if (uobj && prev_entry->offset +
		    (prev_entry->end - prev_entry->start) != uoffset)
			goto step3;

		if (UVM_ET_ISSUBMAP(prev_entry))
			goto step3;

		if (prev_entry->protection != prot || 
		    prev_entry->max_protection != maxprot)
			goto step3;

		if (prev_entry->inheritance != inherit ||
		    prev_entry->advice != advice)
			goto step3;

		/* wiring status must match (new area is unwired) */
		if (VM_MAPENT_ISWIRED(prev_entry))
			goto step3; 
d1169 2
a1170 3
		 * can't extend a shared amap.  note: no need to lock amap to 
		 * look at refs since we don't care about its exact value.
		 * if it is one (i.e. we have only reference) it will stay there
d1172 4
a1175 5

		if (prev_entry->aref.ar_amap &&
		    amap_refs(prev_entry->aref.ar_amap) != 1) {
			goto step3;
		}
d1178 1
a1178 2
		 * Only merge kernel mappings, but keep track
		 * of how much we skipped.
d1180 5
a1184 4
		if (map != kernel_map && map != kmem_map) {
			UVMCNT_INCR(map_nousermerge);
			goto step3;
		}
d1186 6
a1191 4
		if (prev_entry->aref.ar_amap) {
			error = amap_extend(prev_entry, size);
			if (error)
				goto step3;
a1193 3
		UVMCNT_INCR(map_backmerge);
		UVMHIST_LOG(maphist,"  starting back merge", 0, 0, 0, 0);

d1195 1
a1195 2
		 * drop our reference to uobj since we are extending a reference
		 * that we already have (the ref count can not drop to zero).
d1197 3
d1201 2
a1202 2
		if (uobj && uobj->pgops->pgo_detach)
			uobj->pgops->pgo_detach(uobj);
d1204 7
a1210 5
		prev_entry->end += size;
		uvm_rb_fixup(map, prev_entry);
		map->size += size;
		if (p && uobj == NULL)
			p->p_vmspace->vm_dused += atop(size);
d1212 6
a1217 5
		uvm_tree_sanity(map, "map leave 2");

		UVMHIST_LOG(maphist,"<- done (via backmerge)!", 0, 0, 0, 0);
		vm_map_unlock(map);
		return (0);
d1219 33
a1252 2
step3:
	UVMHIST_LOG(maphist,"  allocating new map entry", 0, 0, 0, 0);
d1255 1
a1255 2
	 * check for possible forward merge (which we don't do) and count
	 * the number of times we missed a *possible* chance to merge more 
d1257 7
a1263 5

	if ((flags & UVM_FLAG_NOMERGE) == 0 &&
	    prev_entry->next != &map->header && 
	    prev_entry->next->start == (*startp + size))
		UVMCNT_INCR(map_forwmerge);
d1266 4
a1269 1
	 * step 3: allocate new entry and link it in
d1271 2
d1274 2
a1275 4
#ifdef KVA_GUARDPAGES
	if (map == kernel_map && !(flags & UVM_FLAG_FIXED))
		size -= PAGE_SIZE;
#endif
d1277 4
a1280 4
	new_entry = uvm_mapent_alloc(map, flags);
	if (new_entry == NULL) {
		vm_map_unlock(map);
		return (ENOMEM);
a1281 4
	new_entry->start = *startp;
	new_entry->end = new_entry->start + size;
	new_entry->object.uvm_obj = uobj;
	new_entry->offset = uoffset;
d1283 10
a1292 4
	if (uobj) 
		new_entry->etype = UVM_ET_OBJ;
	else
		new_entry->etype = 0;
d1294 8
a1301 7
	if (flags & UVM_FLAG_COPYONW) {
		new_entry->etype |= UVM_ET_COPYONWRITE;
		if ((flags & UVM_FLAG_OVERLAY) == 0)
			new_entry->etype |= UVM_ET_NEEDSCOPY;
	}
	if (flags & UVM_FLAG_HOLE)
		new_entry->etype |= UVM_ET_HOLE;
d1303 5
a1307 19
	new_entry->protection = prot;
	new_entry->max_protection = maxprot;
	new_entry->inheritance = inherit;
	new_entry->wired_count = 0;
	new_entry->advice = advice;
	if (flags & UVM_FLAG_OVERLAY) {
		/*
		 * to_add: for BSS we overallocate a little since we
		 * are likely to extend
		 */
		vaddr_t to_add = (flags & UVM_FLAG_AMAPPAD) ? 
			UVM_AMAP_CHUNK << PAGE_SHIFT : 0;
		struct vm_amap *amap = amap_alloc(size, to_add, M_WAITOK);
		new_entry->aref.ar_pageoff = 0;
		new_entry->aref.ar_amap = amap;
	} else {
		new_entry->aref.ar_pageoff = 0;
		new_entry->aref.ar_amap = NULL;
	}
d1309 5
a1313 5
	uvm_map_entry_link(map, prev_entry, new_entry);

	map->size += size;
	if (p && uobj == NULL)
		p->p_vmspace->vm_dused += atop(size);
d1315 5
d1322 1
a1322 1
	 *      Update the free space hint
d1324 5
a1329 5
	if ((map->first_free == prev_entry) &&
	    (prev_entry->end >= new_entry->start))
		map->first_free = new_entry;

#ifdef KVA_GUARDPAGES
d1331 1
a1331 1
	 * Create the guard entry.
d1333 5
a1337 19
	if (map == kernel_map && !(flags & UVM_FLAG_FIXED)) {
		guard_entry = uvm_mapent_alloc(map, flags);
		if (guard_entry != NULL {
			guard_entry->start = new_entry->end;
			guard_entry->end = guard_entry->start + PAGE_SIZE;
			guard_entry->object.uvm_obj = uobj;
			guard_entry->offset = uoffset;
			guard_entry->etype = MAP_ET_KVAGUARD;
			guard_entry->protection = prot;
			guard_entry->max_protection = maxprot;
			guard_entry->inheritance = inherit;
			guard_entry->wired_count = 0;
			guard_entry->advice = advice;
			guard_entry->aref.ar_pageoff = 0;
			guard_entry->aref.ar_amap = NULL;
			uvm_map_entry_link(map, new_entry, guard_entry);
			map->size += PAGE_SIZE;
			kva_guardpages++;
		}
a1338 1
#endif
d1340 10
a1349 1
	uvm_tree_sanity(map, "map leave");
d1351 4
a1354 3
	UVMHIST_LOG(maphist,"<- done!", 0, 0, 0, 0);
	vm_map_unlock(map);
	return (0);
d1358 1
a1358 1
 * uvm_map_lookup_entry: find map entry at or before an address
d1360 2
a1361 3
 * => map must at least be read-locked by caller
 * => entry is returned in "entry"
 * => return value is true if address is in the returned entry
d1363 3
a1365 4

boolean_t
uvm_map_lookup_entry(struct vm_map *map, vaddr_t address,
    struct vm_map_entry **entry)
d1367 1
a1367 5
	struct vm_map_entry *cur;
	struct vm_map_entry *last;
	int			use_tree = 0;
	UVMHIST_FUNC("uvm_map_lookup_entry");
	UVMHIST_CALLED(maphist);
d1369 9
a1377 2
	UVMHIST_LOG(maphist,"(map=%p,addr=0x%lx,ent=%p)",
	    map, address, entry, 0);
d1380 2
a1381 2
	 * start looking either from the head of the
	 * list, or from the hint.
d1384 9
a1392 3
	simple_lock(&map->hint_lock);
	cur = map->hint;
	simple_unlock(&map->hint_lock);
d1394 3
a1396 2
	if (cur == &map->header)
		cur = cur->next;
d1398 13
a1410 21
	UVMCNT_INCR(uvm_mlk_call);
	if (address >= cur->start) {
	    	/*
		 * go from hint to end of list.
		 *
		 * but first, make a quick check to see if
		 * we are already looking at the entry we
		 * want (which is usually the case).
		 * note also that we don't need to save the hint
		 * here... it is the same hint (unless we are
		 * at the header, in which case the hint didn't
		 * buy us anything anyway).
		 */
		last = &map->header;
		if ((cur != last) && (cur->end > address)) {
			UVMCNT_INCR(uvm_mlk_hint);
			*entry = cur;
			UVMHIST_LOG(maphist,"<- got it via hint (%p)",
			    cur, 0, 0, 0);
			return (TRUE);
		}
d1412 26
a1437 9
		if (map->nentries > 30)
			use_tree = 1;
	} else {
	    	/*
		 * go from start to hint, *inclusively*
		 */
		last = cur->next;
		cur = map->header.next;
		use_tree = 1;
d1440 10
a1449 1
	uvm_tree_sanity(map, __func__);
d1451 9
a1459 3
	if (use_tree) {
		struct vm_map_entry *prev = &map->header;
		cur = RB_ROOT(&map->rbhead);
d1462 1
a1462 2
		 * Simple lookup in the tree.  Happens when the hint is
		 * invalid, or nentries reach a threshold.
d1464 7
a1470 11
		while (cur) {
			if (address >= cur->start) {
				if (address < cur->end) {
					*entry = cur;
					SAVE_HINT(map, map->hint, cur);
					return (TRUE);
				}
				prev = cur;
				cur = RB_RIGHT(cur, rb_entry);
			} else
				cur = RB_LEFT(cur, rb_entry);
d1472 73
a1544 3
		*entry = prev;
		UVMHIST_LOG(maphist,"<- failed!",0,0,0,0);
		return (FALSE);
d1546 8
d1556 1
a1556 1
	 * search linearly
d1558 6
d1565 14
a1578 7
	while (cur != last) {
		if (cur->end > address) {
			if (address >= cur->start) {
			    	/*
				 * save this lookup for future
				 * hints, and return
				 */
d1580 20
a1599 7
				*entry = cur;
				SAVE_HINT(map, map->hint, cur);
				UVMHIST_LOG(maphist,"<- search got it (%p)",
					cur, 0, 0, 0);
				return (TRUE);
			}
			break;
d1601 25
a1625 1
		cur = cur->next;
d1628 4
a1631 4
	*entry = cur->prev;
	SAVE_HINT(map, map->hint, *entry);
	UVMHIST_LOG(maphist,"<- failed!",0,0,0,0);
	return (FALSE);
d1635 3
a1637 3
 * Checks if address pointed to by phint fits into the empty
 * space before the vm_map_entry after.  Takes alignment and
 * offset into consideration.
d1639 30
d1670 12
a1681 3
int
uvm_map_spacefits(struct vm_map *map, vaddr_t *phint, vsize_t length,
    struct vm_map_entry *after, voff_t uoffset, vsize_t align)
d1683 3
a1685 23
	vaddr_t hint = *phint;
	vaddr_t end;

#ifdef PMAP_PREFER
	/*
	 * push hint forward as needed to avoid VAC alias problems.
	 * we only do this if a valid offset is specified.
	 */
	if (uoffset != UVM_UNKNOWN_OFFSET)
		hint = PMAP_PREFER(uoffset, hint);
#endif
	if (align != 0)
		if ((hint & (align - 1)) != 0)
			hint = roundup(hint, align);
	*phint = hint;

	end = hint + length;
	if (end > map->max_offset || end < hint)
		return (FALSE);
	if (after != NULL && after != &map->header && after->start < end)
		return (FALSE);
	
	return (TRUE);
a1691 1

d1726 2
a1727 6
/*
 * uvm_map_hint: return the beginning of the best area suitable for
 * creating a new mapping with "prot" protection.
 */
vaddr_t
uvm_map_hint1(struct proc *p, vm_prot_t prot, int skipheap)
d1729 1
a1729 1
	vaddr_t addr;
d1731 5
a1735 59
#ifdef __i386__
	/*
	 * If executable skip first two pages, otherwise start
	 * after data + heap region.
	 */
	if ((prot & VM_PROT_EXECUTE) &&
	    ((vaddr_t)p->p_vmspace->vm_daddr >= I386_MAX_EXE_ADDR)) {
		addr = (PAGE_SIZE*2) +
		    (arc4random() & (I386_MAX_EXE_ADDR / 2 - 1));
		return (round_page(addr));
	}
#endif
	/* start malloc/mmap after the brk */
	addr = (vaddr_t)p->p_vmspace->vm_daddr;
	if (skipheap)
		addr += BRKSIZ;
#if !defined(__vax__)
	addr += arc4random() & (MIN((256 * 1024 * 1024), BRKSIZ) - 1);
#endif
	return (round_page(addr));
}

/*
 * uvm_map_findspace: find "length" sized space in "map".
 *
 * => "hint" is a hint about where we want it, unless FINDSPACE_FIXED is
 *	set (in which case we insist on using "hint").
 * => "result" is VA returned
 * => uobj/uoffset are to be used to handle VAC alignment, if required
 * => if `align' is non-zero, we attempt to align to that value.
 * => caller must at least have read-locked map
 * => returns NULL on failure, or pointer to prev. map entry if success
 * => note this is a cross between the old vm_map_findspace and vm_map_find
 */

struct vm_map_entry *
uvm_map_findspace(struct vm_map *map, vaddr_t hint, vsize_t length,
    vaddr_t *result, struct uvm_object *uobj, voff_t uoffset, vsize_t align,
    int flags)
{
	struct vm_map_entry *entry, *next, *tmp;
	struct vm_map_entry *child, *prev = NULL;

	vaddr_t end, orig_hint;
	UVMHIST_FUNC("uvm_map_findspace");
	UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist, "(map=%p, hint=0x%lx, len=%ld, flags=0x%lx)", 
		    map, hint, length, flags);
	KASSERT((align & (align - 1)) == 0);
	KASSERT((flags & UVM_FLAG_FIXED) == 0 || align == 0);

	uvm_tree_sanity(map, "map_findspace entry");

	/*
	 * remember the original hint.  if we are aligning, then we
	 * may have to try again with no alignment constraint if
	 * we fail the first time.
	 */
d1737 1
a1737 186
	orig_hint = hint;
	if (hint < map->min_offset) {	/* check ranges ... */
		if (flags & UVM_FLAG_FIXED) {
			UVMHIST_LOG(maphist,"<- VA below map range",0,0,0,0);
			return(NULL);
		}
		hint = map->min_offset;
	}
	if (hint > map->max_offset) {
		UVMHIST_LOG(maphist,"<- VA 0x%lx > range [0x%lx->0x%lx]",
				hint, map->min_offset, map->max_offset, 0);
		return(NULL);
	}

	/*
	 * Look for the first possible address; if there's already
	 * something at this address, we have to start after it.
	 */

	if ((flags & UVM_FLAG_FIXED) == 0 && hint == map->min_offset) {
		if ((entry = map->first_free) != &map->header) 
			hint = entry->end;
	} else {
		if (uvm_map_lookup_entry(map, hint, &tmp)) {
			/* "hint" address already in use ... */
			if (flags & UVM_FLAG_FIXED) {
				UVMHIST_LOG(maphist,"<- fixed & VA in use",
				    0, 0, 0, 0);
				return(NULL);
			}
			hint = tmp->end;
		}
		entry = tmp;
	}

	if (flags & UVM_FLAG_FIXED) {
		end = hint + length;
		if (end > map->max_offset || end < hint) {
			UVMHIST_LOG(maphist,"<- failed (off end)", 0,0,0,0);
			goto error;
		}
		next = entry->next;
		if (next == &map->header || next->start >= end)
			goto found;
		UVMHIST_LOG(maphist,"<- fixed mapping failed", 0,0,0,0);
		return(NULL); /* only one shot at it ... */
	}

	/* Try to find the space in the red-black tree */

	/* Check slot before any entry */
	if (uvm_map_spacefits(map, &hint, length, entry->next, uoffset, align))
		goto found;
	
	/* If there is not enough space in the whole tree, we fail */
	tmp = RB_ROOT(&map->rbhead);
	if (tmp == NULL || tmp->space < length)
		goto error;

	/* Find an entry close to hint that has enough space */
	for (; tmp;) {
		if (tmp->end >= hint &&
		    (prev == NULL || tmp->end < prev->end)) {
			if (tmp->ownspace >= length)
				prev = tmp;
			else if ((child = RB_RIGHT(tmp, rb_entry)) != NULL &&
			    child->space >= length)
				prev = tmp;
		}
		if (tmp->end < hint)
			child = RB_RIGHT(tmp, rb_entry);
		else if (tmp->end > hint)
			child = RB_LEFT(tmp, rb_entry);
		else {
			if (tmp->ownspace >= length)
				break;
			child = RB_RIGHT(tmp, rb_entry);
		}
		if (child == NULL || child->space < length)
			break;
		tmp = child;
	}
	
	if (tmp != NULL && hint < tmp->end + tmp->ownspace) {
		/* 
		 * Check if the entry that we found satifies the
		 * space requirement
		 */
		if (hint < tmp->end)
			hint = tmp->end;
		if (uvm_map_spacefits(map, &hint, length, tmp->next, uoffset,
			align)) {
			entry = tmp;
			goto found;
		} else if (tmp->ownspace >= length)
			goto listsearch;
	}
	if (prev == NULL)
		goto error;
	
	hint = prev->end;
	if (uvm_map_spacefits(map, &hint, length, prev->next, uoffset,
		align)) {
		entry = prev;
		goto found;
	} else if (prev->ownspace >= length)
		goto listsearch;
	
	tmp = RB_RIGHT(prev, rb_entry);
	for (;;) {
		KASSERT(tmp && tmp->space >= length);
		child = RB_LEFT(tmp, rb_entry);
		if (child && child->space >= length) {
			tmp = child;
			continue;
		}
		if (tmp->ownspace >= length)
			break;
		tmp = RB_RIGHT(tmp, rb_entry);
	}
	
	hint = tmp->end;
	if (uvm_map_spacefits(map, &hint, length, tmp->next, uoffset, align)) {
		entry = tmp;
		goto found;
	}

	/* 
	 * The tree fails to find an entry because of offset or alignment
	 * restrictions.  Search the list instead.
	 */
 listsearch:
	/*
	 * Look through the rest of the map, trying to fit a new region in
	 * the gap between existing regions, or after the very last region.
	 * note: entry->end   = base VA of current gap,
	 *	 next->start  = VA of end of current gap
	 */
	for (;; hint = (entry = next)->end) {
		/*
		 * Find the end of the proposed new region.  Be sure we didn't
		 * go beyond the end of the map, or wrap around the address;
		 * if so, we lose.  Otherwise, if this is the last entry, or
		 * if the proposed new region fits before the next entry, we
		 * win.
		 */

#ifdef PMAP_PREFER
		/*
		 * push hint forward as needed to avoid VAC alias problems.
		 * we only do this if a valid offset is specified.
		 */
		if (uoffset != UVM_UNKNOWN_OFFSET)
			hint = PMAP_PREFER(uoffset, hint);
#endif
		if (align != 0) {
			if ((hint & (align - 1)) != 0)
				hint = roundup(hint, align);
			/*
			 * XXX Should we PMAP_PREFER() here again?
			 */
		}
		end = hint + length;
		if (end > map->max_offset || end < hint) {
			UVMHIST_LOG(maphist,"<- failed (off end)", 0,0,0,0);
			goto error;
		}
		next = entry->next;
		if (next == &map->header || next->start >= end)
			break;
	}
 found:
	SAVE_HINT(map, map->hint, entry);
	*result = hint;
	UVMHIST_LOG(maphist,"<- got it!  (result=0x%lx)", hint, 0,0,0);
	return (entry);

 error:
	if (align != 0) {
		UVMHIST_LOG(maphist,
		    "calling recursively, no align",
		    0,0,0,0);
		return (uvm_map_findspace(map, orig_hint,
			    length, result, uobj, uoffset, 0, flags));
	}
	return (NULL);
d1741 1
a1741 5
 *   U N M A P   -   m a i n   e n t r y   p o i n t
 */

/*
 * uvm_unmap: remove mappings from a vm_map (from "start" up to "stop")
d1743 3
a1745 2
 * => caller must check alignment and size 
 * => map must be unlocked (we will lock it)
d1748 16
a1763 8
uvm_unmap_p(vm_map_t map, vaddr_t start, vaddr_t end, struct proc *p)
{
	vm_map_entry_t dead_entries;
	UVMHIST_FUNC("uvm_unmap"); UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist, "  (map=%p, start=0x%lx, end=0x%lx)",
	    map, start, end, 0);

d1765 1
a1765 2
	 * work now done by helper functions.   wipe the pmap's and then
	 * detach from the dead entries...
d1767 4
a1770 3
	vm_map_lock(map);
	uvm_unmap_remove(map, start, end, &dead_entries, p, FALSE);
	vm_map_unlock(map);
d1772 7
a1778 2
	if (dead_entries != NULL)
		uvm_unmap_detach(dead_entries, 0);
d1780 2
a1781 1
	UVMHIST_LOG(maphist, "<- done", 0,0,0,0);
a1783 5

/*
 *   U N M A P   -   m a i n   h e l p e r   f u n c t i o n s
 */

d1785 1
a1785 1
 * uvm_unmap_remove: remove mappings from a vm_map (from "start" up to "stop")
d1787 3
a1789 4
 * => caller must check alignment and size 
 * => map must be locked by caller
 * => we return a list of map entries that we've remove from the map
 *    in "entry_list"
a1790 1

d1793 2
a1794 1
    struct vm_map_entry **entry_list, struct proc *p, boolean_t remove_holes)
d1796 1
a1796 7
	struct vm_map_entry *entry, *first_entry, *next;
	vaddr_t len;
	UVMHIST_FUNC("uvm_unmap_remove");
	UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist,"(map=%p, start=0x%lx, end=0x%lx)",
	    map, start, end, 0);
d1799 2
a1800 2

	uvm_tree_sanity(map, "unmap_remove entry");
d1808 1
a1808 1
	 * find first entry
d1810 5
a1814 3
	if (uvm_map_lookup_entry(map, start, &first_entry) == TRUE) {
		/* clip and go... */
		entry = first_entry;
a1815 2
		/* critical!  prevents stale hint */
		SAVE_HINT(map, entry, entry->prev);
d1817 4
a1820 3
	} else {
		entry = first_entry->next;
	}
d1823 13
a1835 2
	 * Save the free space hint
	 */
d1837 2
a1838 38
	if (map->first_free->start >= start)
		map->first_free = entry->prev;

	/*
	 * note: we now re-use first_entry for a different task.  we remove
	 * a number of map entries from the map and save them in a linked
	 * list headed by "first_entry".  once we remove them from the map
	 * the caller should unlock the map and drop the references to the
	 * backing objects [c.f. uvm_unmap_detach].  the object is to
	 * separate unmapping from reference dropping.  why?
	 *   [1] the map has to be locked for unmapping
	 *   [2] the map need not be locked for reference dropping
	 *   [3] dropping references may trigger pager I/O, and if we hit
	 *       a pager that does synchronous I/O we may have to wait for it.
	 *   [4] we would like all waiting for I/O to occur with maps unlocked
	 *       so that we don't block other threads.  
	 */
	first_entry = NULL;
	*entry_list = NULL;		/* to be safe */

	/*
	 * break up the area into map entry sized regions and unmap.  note 
	 * that all mappings have to be removed before we can even consider
	 * dropping references to amaps or VM objects (otherwise we could end
	 * up with a mapping to a page on the free list which would be very bad)
	 */

	while ((entry != &map->header) && (entry->start < end)) {

		UVM_MAP_CLIP_END(map, entry, end); 
		next = entry->next;
		len = entry->end - entry->start;
		if (p && entry->object.uvm_obj == NULL)
			p->p_vmspace->vm_dused -= atop(len);

		/*
		 * unwire before removing addresses from the pmap; otherwise
		 * unwiring will put the entries back into the pmap (XXX).
d1840 4
a1843 3

		if (VM_MAPENT_ISWIRED(entry))
			uvm_map_entry_unwire(map, entry);
d1846 1
a1846 2
		 * special case: handle mappings to anonymous kernel objects.
		 * we want to free these pages right away...
a1847 6
#ifdef KVA_GUARDPAGES
		if (map == kernel_map && entry->etype & MAP_ET_KVAGUARD) {
			entry->etype &= ~MAP_ET_KVAGUARD;
			kva_guardpages--;
		} else		/* (code continues across line-break) */
#endif
d1849 3
d1853 1
a1853 1
				entry = next;
d1857 1
d1859 1
a1859 1
			pmap_kremove(entry->start, len);
d1865 1
a1865 1
			 * note: kernel object mappings are currently used in
d1885 2
a1886 2
			 * uvm_km_pgremove currently does the following: 
			 *   for pages in the kernel object in range: 
d1909 1
a1909 2
			entry->object.uvm_obj = NULL;	/* to be safe */

d1912 2
a1913 2
		 	 * remove mappings the standard way.
		 	 */
d1918 1
a1918 2
		 * remove entry from map and put it on our list of entries 
		 * that we've nuked.  then go do next entry.
d1920 8
a1927 4
		UVMHIST_LOG(maphist, "  removed map entry %p", entry, 0, 0,0);

		/* critical! prevents stale hint */
		SAVE_HINT(map, entry, entry->prev);
a1928 12
		uvm_map_entry_unlink(map, entry);
		map->size -= len;
		entry->next = first_entry;
		first_entry = entry;
		entry = next;		/* next entry, please */
	}
#ifdef KVA_GUARDPAGES
	/*
	 * entry points at the map-entry after the last-removed map-entry.
	 */
	if (map == kernel_map && entry != &map->header &&
	    entry->etype & MAP_ET_KVAGUARD && entry->start == end) {
d1930 1
a1930 2
		 * Removed range is followed by guard page;
		 * remove that guard page now (or it will stay forever).
d1932 1
a1932 8
		entry->etype &= ~MAP_ET_KVAGUARD;
		kva_guardpages--;

		uvm_map_entry_unlink(map, entry);
		map->size -= len;
		entry->next = first_entry;
		first_entry = entry;
		entry = next;		/* next entry, please */
a1933 4
#endif
	/* if ((map->flags & VM_MAP_DYING) == 0) { */
		pmap_update(vm_map_pmap(map));
	/* } */
d1935 1
d1937 2
a1938 1
	uvm_tree_sanity(map, "unmap_remove leave");
d1940 15
a1954 7
	/*
	 * now we've cleaned up the map and are ready for the caller to drop
	 * references to the mapped objects.  
	 */

	*entry_list = first_entry;
	UVMHIST_LOG(maphist,"<- done!", 0, 0, 0, 0);
d1958 1
a1958 1
 * uvm_unmap_detach: drop references in a chain of map entries
d1960 1
a1960 1
 * => we will free the map entries as we traverse the list.
a1961 1

d1963 2
a1964 1
uvm_unmap_detach(struct vm_map_entry *first_entry, int flags)
d1966 1
a1966 2
	struct vm_map_entry *next_entry;
	UVMHIST_FUNC("uvm_unmap_detach"); UVMHIST_CALLED(maphist);
d1968 5
a1972 28
	while (first_entry) {
		KASSERT(!VM_MAPENT_ISWIRED(first_entry));
		UVMHIST_LOG(maphist,
		    "  detach 0x%lx: amap=%p, obj=%p, submap?=%ld", 
		    first_entry, first_entry->aref.ar_amap, 
		    first_entry->object.uvm_obj,
		    UVM_ET_ISSUBMAP(first_entry));

		/*
		 * drop reference to amap, if we've got one
		 */

		if (first_entry->aref.ar_amap)
			uvm_map_unreference_amap(first_entry, flags);

		/*
		 * drop reference to our backing object, if we've got one
		 */

		if (UVM_ET_ISSUBMAP(first_entry)) {
			/* ... unlikely to happen, but play it safe */
			uvm_map_deallocate(first_entry->object.sub_map);
		} else {
			if (UVM_ET_ISOBJ(first_entry) &&
			    first_entry->object.uvm_obj->pgops->pgo_detach)
				first_entry->object.uvm_obj->pgops->
				    pgo_detach(first_entry->object.uvm_obj);
		}
d1974 2
a1975 3
		next_entry = first_entry->next;
		uvm_mapent_free(first_entry);
		first_entry = next_entry;
a1976 1
	UVMHIST_LOG(maphist, "<- done", 0,0,0,0);
d1980 1
a1980 5
 *   E X T R A C T I O N   F U N C T I O N S
 */

/* 
 * uvm_map_reserve: reserve space in a vm_map for future use.
d1982 2
a1983 5
 * => we reserve space in a map by putting a dummy map entry in the 
 *    map (dummy means obj=NULL, amap=NULL, prot=VM_PROT_NONE)
 * => map should be unlocked (we will write lock it)
 * => we return true if we were able to reserve space
 * => XXXCDC: should be inline?
a1984 1

d1986 3
a1988 2
uvm_map_reserve(struct vm_map *map, vsize_t size, vaddr_t offset,
    vsize_t align, vaddr_t *raddr)
d1990 5
a1994 1
	UVMHIST_FUNC("uvm_map_reserve"); UVMHIST_CALLED(maphist); 
d1996 22
a2017 2
	UVMHIST_LOG(maphist, "(map=%p, size=0x%lx, offset=0x%lx,addr=0x%lx)",
	      map,size,offset,raddr);
d2019 15
a2033 3
	size = round_page(size);
	if (*raddr < vm_map_min(map))
		*raddr = vm_map_min(map);                /* hint */
d2036 1
a2036 1
	 * reserve some virtual space.
d2038 55
d2094 4
a2097 6
	if (uvm_map(map, raddr, size, NULL, offset, 0,
	    UVM_MAPFLAG(UVM_PROT_NONE, UVM_PROT_NONE, UVM_INH_NONE,
	    UVM_ADV_RANDOM, UVM_FLAG_NOMERGE)) != 0) {
	    UVMHIST_LOG(maphist, "<- done (no VM)", 0,0,0,0);
		return (FALSE);
	}     
d2099 15
a2113 2
	UVMHIST_LOG(maphist, "<- done (*raddr=0x%lx)", *raddr,0,0,0);
	return (TRUE);
d2117 1
a2117 2
 * uvm_map_replace: replace a reserved (blank) area of memory with 
 * real mappings.
d2119 6
a2124 5
 * => caller must WRITE-LOCK the map 
 * => we return TRUE if replacement was a success
 * => we expect the newents chain to have nnewents entries on it and
 *    we expect newents->prev to point to the last entry on the list
 * => note newents is allowed to be NULL
a2125 1

d2127 2
a2128 2
uvm_map_replace(struct vm_map *map, vaddr_t start, vaddr_t end,
    struct vm_map_entry *newents, int nnewents)
d2130 6
a2135 1
	struct vm_map_entry *oldent, *last;
d2137 1
a2137 1
	uvm_tree_sanity(map, "map_replace entry");
d2140 4
a2143 1
	 * first find the blank map entry at the specified address
d2145 7
a2151 3

	if (!uvm_map_lookup_entry(map, start, &oldent)) {
		return(FALSE);
d2155 1
a2155 1
	 * check to make sure we have a proper blank entry
d2157 11
a2167 4

	if (oldent->start != start || oldent->end != end || 
	    oldent->object.uvm_obj != NULL || oldent->aref.ar_amap != NULL) {
		return (FALSE);
a2169 1
#ifdef DIAGNOSTIC
d2171 4
a2174 1
	 * sanity check the newents chain
d2176 5
a2180 23
	{
		struct vm_map_entry *tmpent = newents;
		int nent = 0;
		vaddr_t cur = start;

		while (tmpent) {
			nent++;
			if (tmpent->start < cur)
				panic("uvm_map_replace1");
			if (tmpent->start > tmpent->end || tmpent->end > end) {
		printf("tmpent->start=0x%lx, tmpent->end=0x%lx, end=0x%lx\n",
			    tmpent->start, tmpent->end, end);
				panic("uvm_map_replace2");
			}
			cur = tmpent->end;
			if (tmpent->next) {
				if (tmpent->next->prev != tmpent)
					panic("uvm_map_replace3");
			} else {
				if (newents->prev != tmpent)
					panic("uvm_map_replace4");
			}
			tmpent = tmpent->next;
d2182 2
a2183 4
		if (nent != nnewents)
			panic("uvm_map_replace5");
	}
#endif
d2186 1
a2186 2
	 * map entry is a valid blank!   replace it.   (this does all the
	 * work of map entry link/unlink...).
d2188 20
d2209 21
a2229 2
	if (newents) {
		last = newents->prev;		/* we expect this */
d2231 4
a2234 4
		/* critical: flush stale hints out of map */
		SAVE_HINT(map, map->hint, newents);
		if (map->first_free == oldent)
			map->first_free = last;
d2236 12
a2247 2
		last->next = oldent->next;
		last->next->prev = last;
d2249 2
a2250 2
		/* Fix RB tree */
		uvm_rb_remove(map, oldent);
d2252 3
a2254 3
		newents->prev = oldent->prev;
		newents->prev->next = newents;
		map->nentries = map->nentries + (nnewents - 1);
d2256 4
a2259 4
		/* Fixup the RB tree */
		{
			int i;
			struct vm_map_entry *tmp;
d2261 6
a2266 7
			tmp = newents;
			for (i = 0; i < nnewents && tmp; i++) {
				uvm_rb_insert(map, tmp);
				tmp = tmp->next;
			}
		}
	} else {
d2268 8
a2275 4
		/* critical: flush stale hints out of map */
		SAVE_HINT(map, map->hint, oldent->prev);
		if (map->first_free == oldent)
			map->first_free = oldent->prev;
d2277 1
a2277 2
		/* NULL list of new entries: just remove the old one */
		uvm_map_entry_unlink(map, oldent);
d2280 4
d2285 8
a2292 1
	uvm_tree_sanity(map, "map_replace leave");
d2295 1
a2295 1
	 * now we can free the old blank entry, unlock the map and return.
d2297 2
a2298 3

	uvm_mapent_free(oldent);
	return(TRUE);
d2302 1
a2302 2
 * uvm_map_extract: extract a mapping from a map and put it somewhere
 *	(maybe removing the old mapping)
d2304 1
a2304 12
 * => maps should be unlocked (we will write lock them)
 * => returns 0 on success, error code otherwise
 * => start must be page aligned
 * => len must be page sized
 * => flags:
 *      UVM_EXTRACT_REMOVE: remove mappings from srcmap
 *      UVM_EXTRACT_CONTIG: abort if unmapped area (advisory only)
 *      UVM_EXTRACT_QREF: for a temporary extraction do quick obj refs
 *      UVM_EXTRACT_FIXPROT: set prot to maxprot as we go
 *    >>>NOTE: if you set REMOVE, you are not allowed to use CONTIG or QREF!<<<
 *    >>>NOTE: QREF's must be unmapped via the QREF path, thus should only
 *             be used from within the kernel in a kernel level map <<<
d2306 2
a2307 4

int
uvm_map_extract(struct vm_map *srcmap, vaddr_t start, vsize_t len,
    struct vm_map *dstmap, vaddr_t *dstaddrp, int flags)
d2309 2
a2310 14
	vaddr_t dstaddr, end, newend, oldoffset, fudge, orig_fudge,
	    oldstart;
	struct vm_map_entry *chain, *endchain, *entry, *orig_entry, *newentry;
	struct vm_map_entry *deadentry, *oldentry;
	vsize_t elen;
	int nchain, error, copy_ok;
	UVMHIST_FUNC("uvm_map_extract"); UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist,"(srcmap=%p,start=0x%lx, len=0x%lx", srcmap, start,
	    len,0);
	UVMHIST_LOG(maphist," ...,dstmap=%p, flags=0x%lx)", dstmap,flags,0,0);

	uvm_tree_sanity(srcmap, "map_extract src enter");
	uvm_tree_sanity(dstmap, "map_extract dst enter");
d2313 11
a2323 4
	 * step 0: sanity check: start must be on a page boundary, length
	 * must be page sized.  can't ask for CONTIG/QREF if you asked for
	 * REMOVE.
	 */
d2325 5
a2329 3
	KASSERT((start & PAGE_MASK) == 0 && (len & PAGE_MASK) == 0);
	KASSERT((flags & UVM_EXTRACT_REMOVE) == 0 ||
		(flags & (UVM_EXTRACT_CONTIG|UVM_EXTRACT_QREF)) == 0);
d2331 10
a2340 9
	/*
	 * step 1: reserve space in the target map for the extracted area
	 */

	dstaddr = vm_map_min(dstmap);
	if (uvm_map_reserve(dstmap, len, start, 0, &dstaddr) == FALSE)
		return(ENOMEM);
	*dstaddrp = dstaddr;	/* pass address back to caller */
	UVMHIST_LOG(maphist, "  dstaddr=0x%lx", dstaddr,0,0,0);
d2343 13
a2355 4
	 * step 2: setup for the extraction process loop by init'ing the 
	 * map entry chain, locking src map, and looking up the first useful
	 * entry in the map.
	 */
d2357 11
a2367 5
	end = start + len;
	newend = dstaddr + len;
	chain = endchain = NULL;
	nchain = 0;
	vm_map_lock(srcmap);
d2369 2
a2370 1
	if (uvm_map_lookup_entry(srcmap, start, &entry)) {
d2372 13
a2384 18
		/* "start" is within an entry */
		if (flags & UVM_EXTRACT_QREF) {

			/*
			 * for quick references we don't clip the entry, so
			 * the entry may map space "before" the starting
			 * virtual address... this is the "fudge" factor
			 * (which can be non-zero only the first time
			 * through the "while" loop in step 3).
			 */

			fudge = start - entry->start;
		} else {

			/*
			 * normal reference: we clip the map to fit (thus
			 * fudge is zero)
			 */
d2386 29
a2414 4
			UVM_MAP_CLIP_START(srcmap, entry, start);
			SAVE_HINT(srcmap, srcmap->hint, entry->prev);
			fudge = 0;
		}
d2416 16
a2431 5

		/* "start" is not within an entry ... skip to next entry */
		if (flags & UVM_EXTRACT_CONTIG) {
			error = EINVAL;
			goto bad;    /* definite hole here ... */
a2432 3

		entry = entry->next;
		fudge = 0;
a2434 4
	/* save values from srcmap for step 6 */
	orig_entry = entry;
	orig_fudge = fudge;

d2436 2
a2437 2
	 * step 3: now start looping through the map entries, extracting
	 * as we go.
d2439 5
d2445 2
a2446 1
	while (entry->start < end && entry != &srcmap->header) {
a2447 3
		/* if we are not doing a quick reference, clip it */
		if ((flags & UVM_EXTRACT_QREF) == 0)
			UVM_MAP_CLIP_END(srcmap, entry, end);
d2449 1
a2449 11
		/* clear needs_copy (allow chunking) */
		if (UVM_ET_ISNEEDSCOPY(entry)) {
			if (fudge)
				oldstart = entry->start;
			else
				oldstart = 0;	/* XXX: gcc */
			amap_copy(srcmap, entry, M_NOWAIT, TRUE, start, end);
			if (UVM_ET_ISNEEDSCOPY(entry)) {  /* failed? */
				error = ENOMEM;
				goto bad;
			}
d2451 8
a2458 6
			/* amap_copy could clip (during chunk)!  update fudge */
			if (fudge) {
				fudge = fudge - (entry->start - oldstart);
				orig_fudge = fudge;
			}
		}
d2460 9
a2468 2
		/* calculate the offset of this from "start" */
		oldoffset = (entry->start + fudge) - start;
d2470 9
a2478 6
		/* allocate a new map entry */
		newentry = uvm_mapent_alloc(dstmap, flags);
		if (newentry == NULL) {
			error = ENOMEM;
			goto bad;
		}
d2480 4
a2483 33
		/* set up new map entry */
		newentry->next = NULL;
		newentry->prev = endchain;
		newentry->start = dstaddr + oldoffset;
		newentry->end =
		    newentry->start + (entry->end - (entry->start + fudge));
		if (newentry->end > newend || newentry->end < newentry->start)
			newentry->end = newend;
		newentry->object.uvm_obj = entry->object.uvm_obj;
		if (newentry->object.uvm_obj) {
			if (newentry->object.uvm_obj->pgops->pgo_reference)
				newentry->object.uvm_obj->pgops->
				    pgo_reference(newentry->object.uvm_obj);
			newentry->offset = entry->offset + fudge;
		} else {
			newentry->offset = 0;
		}
		newentry->etype = entry->etype;
		newentry->protection = (flags & UVM_EXTRACT_FIXPROT) ? 
			entry->max_protection : entry->protection; 
		newentry->max_protection = entry->max_protection;
		newentry->inheritance = entry->inheritance;
		newentry->wired_count = 0;
		newentry->aref.ar_amap = entry->aref.ar_amap;
		if (newentry->aref.ar_amap) {
			newentry->aref.ar_pageoff =
			    entry->aref.ar_pageoff + (fudge >> PAGE_SHIFT);
			uvm_map_reference_amap(newentry, AMAP_SHARED |
			    ((flags & UVM_EXTRACT_QREF) ? AMAP_REFALL : 0));
		} else {
			newentry->aref.ar_pageoff = 0;
		}
		newentry->advice = entry->advice;
d2485 2
a2486 8
		/* now link it on the chain */
		nchain++;
		if (endchain == NULL) {
			chain = endchain = newentry;
		} else {
			endchain->next = newentry;
			endchain = newentry;
		}
d2488 2
a2489 9
		/* end of 'while' loop! */
		if ((flags & UVM_EXTRACT_CONTIG) && entry->end < end && 
		    (entry->next == &srcmap->header ||
		    entry->next->start != entry->end)) {
			error = EINVAL;
			goto bad;
		}
		entry = entry->next;
		fudge = 0;
d2491 32
d2524 5
a2528 3
	/*
	 * step 4: close off chain (in format expected by uvm_map_replace)
	 */
d2530 6
a2535 2
	if (chain)
		chain->prev = endchain;
d2537 7
a2543 6
	/*
	 * step 5: attempt to lock the dest map so we can pmap_copy.
	 * note usage of copy_ok: 
	 *   1 => dstmap locked, pmap_copy ok, and we "replace" here (step 5)
	 *   0 => dstmap unlocked, NO pmap_copy, and we will "replace" in step 7
	 */
d2545 8
a2552 8
	if (srcmap == dstmap || vm_map_lock_try(dstmap) == TRUE) {
		copy_ok = 1;
		if (!uvm_map_replace(dstmap, dstaddr, dstaddr+len, chain,
		    nchain)) {
			if (srcmap != dstmap)
				vm_map_unlock(dstmap);
			error = EIO;
			goto bad;
a2553 3
	} else {
		copy_ok = 0;
		/* replace defered until step 7 */
d2555 1
d2557 15
a2571 6
	/*
	 * step 6: traverse the srcmap a second time to do the following:
	 *  - if we got a lock on the dstmap do pmap_copy
	 *  - if UVM_EXTRACT_REMOVE remove the entries
	 * we make use of orig_entry and orig_fudge (saved in step 2)
	 */
d2573 3
a2575 1
	if (copy_ok || (flags & UVM_EXTRACT_REMOVE)) {
d2577 2
a2578 6
		/* purge possible stale hints from srcmap */
		if (flags & UVM_EXTRACT_REMOVE) {
			SAVE_HINT(srcmap, srcmap->hint, orig_entry->prev);
			if (srcmap->first_free->start >= start)
				srcmap->first_free = orig_entry->prev;
		}
d2580 11
a2590 3
		entry = orig_entry;
		fudge = orig_fudge;
		deadentry = NULL;	/* for UVM_EXTRACT_REMOVE */
d2592 2
a2593 9
		while (entry->start < end && entry != &srcmap->header) {
			if (copy_ok) {
				oldoffset = (entry->start + fudge) - start;
				elen = MIN(end, entry->end) -
				    (entry->start + fudge);
				pmap_copy(dstmap->pmap, srcmap->pmap,
				    dstaddr + oldoffset, elen,
				    entry->start + fudge);
			}
d2595 7
a2601 13
			/* we advance "entry" in the following if statement */
			if (flags & UVM_EXTRACT_REMOVE) {
				pmap_remove(srcmap->pmap, entry->start, 
						entry->end);
        			oldentry = entry;	/* save entry */
        			entry = entry->next;	/* advance */
				uvm_map_entry_unlink(srcmap, oldentry);
							/* add to dead list */
				oldentry->next = deadentry;
				deadentry = oldentry;
      			} else {
        			entry = entry->next;		/* advance */
			}
d2603 2
a2604 4
			/* end of 'while' loop */
			fudge = 0;
		}
		pmap_update(srcmap->pmap);
d2607 3
a2609 2
		 * unlock dstmap.  we will dispose of deadentry in
		 * step 7 if needed
d2611 10
d2622 7
a2628 2
		if (copy_ok && srcmap != dstmap)
			vm_map_unlock(dstmap);
d2630 8
d2639 17
a2655 2
	else
		deadentry = NULL; /* XXX: gcc */
d2658 1
a2658 3
	 * step 7: we are done with the source map, unlock.   if copy_ok
	 * is 0 then we have not replaced the dummy mapping in dstmap yet
	 * and we need to do so now.
d2661 13
a2673 3
	vm_map_unlock(srcmap);
	if ((flags & UVM_EXTRACT_REMOVE) && deadentry)
		uvm_unmap_detach(deadentry, 0);   /* dispose of old entries */
d2675 3
a2677 6
	/* now do the replacement if we didn't do it in step 5 */
	if (copy_ok == 0) {
		vm_map_lock(dstmap);
		error = uvm_map_replace(dstmap, dstaddr, dstaddr+len, chain,
		    nchain);
		vm_map_unlock(dstmap);
d2679 6
a2684 4
		if (error == FALSE) {
			error = EIO;
			goto bad2;
		}
a2686 5
	uvm_tree_sanity(srcmap, "map_extract src leave");
	uvm_tree_sanity(dstmap, "map_extract dst leave");

	return(0);

d2688 1
a2688 1
	 * bad: failure recovery
d2690 7
a2696 12
bad:
	vm_map_unlock(srcmap);
bad2:			/* src already unlocked */
	if (chain)
		uvm_unmap_detach(chain,
		    (flags & UVM_EXTRACT_QREF) ? AMAP_REFALL : 0);

	uvm_tree_sanity(srcmap, "map_extract src err leave");
	uvm_tree_sanity(dstmap, "map_extract dst err leave");

	uvm_unmap(dstmap, dstaddr, dstaddr+len);   /* ??? */
	return(error);
d2699 1
a2699 1
/* end of extraction functions */
d2702 1
a2702 14
 * uvm_map_submap: punch down part of a map into a submap
 *
 * => only the kernel_map is allowed to be submapped
 * => the purpose of submapping is to break up the locking granularity
 *	of a larger map
 * => the range specified must have been mapped previously with a uvm_map()
 *	call [with uobj==NULL] to create a blank map entry in the main map.
 *	[And it had better still be blank!]
 * => maps which contain submaps should never be copied or forked.
 * => to remove a submap, use uvm_unmap() on the main map 
 *	and then uvm_map_deallocate() the submap.
 * => main map must be unlocked.
 * => submap must have been init'd and have a zero reference count.
 *	[need not be locked as we don't actually reference it]
d2705 6
a2710 3
int
uvm_map_submap(struct vm_map *map, vaddr_t start, vaddr_t end,
    struct vm_map *submap)
d2712 1
d2714 2
a2715 1
	int result;
d2717 15
a2731 1
	vm_map_lock(map);
d2733 5
a2737 1
	VM_MAP_RANGE_CHECK(map, start, end);
d2739 8
a2746 5
	if (uvm_map_lookup_entry(map, start, &entry)) {
		UVM_MAP_CLIP_START(map, entry, start);
		UVM_MAP_CLIP_END(map, entry, end);		/* to be safe */
	} else {
		entry = NULL;
d2749 24
a2772 11
	if (entry != NULL && 
	    entry->start == start && entry->end == end &&
	    entry->object.uvm_obj == NULL && entry->aref.ar_amap == NULL &&
	    !UVM_ET_ISCOPYONWRITE(entry) && !UVM_ET_ISNEEDSCOPY(entry)) {
		entry->etype |= UVM_ET_SUBMAP;
		entry->object.sub_map = submap;
		entry->offset = 0;
		uvm_map_reference(submap);
		result = 0;
	} else {
		result = EINVAL;
d2774 1
a2774 4
	vm_map_unlock(map);
	return(result);
}

d2777 1
a2777 4
 * uvm_map_protect: change map protection
 *
 * => set_max means set max_protection.
 * => map must be unlocked.
d2779 5
a2783 8

#define MASK(entry)     (UVM_ET_ISCOPYONWRITE(entry) ? \
			 ~VM_PROT_WRITE : VM_PROT_ALL)
#define max(a,b)        ((a) > (b) ? (a) : (b))

int
uvm_map_protect(struct vm_map *map, vaddr_t start, vaddr_t end,
    vm_prot_t new_prot, boolean_t set_max)
d2785 2
a2786 5
	struct vm_map_entry *current, *entry;
	int error = 0;
	UVMHIST_FUNC("uvm_map_protect"); UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist,"(map=%p,start=0x%lx,end=0x%lx,new_prot=0x%lx)",
		    map, start, end, new_prot);
d2788 6
a2793 1
	vm_map_lock(map);
d2795 2
a2796 6
	VM_MAP_RANGE_CHECK(map, start, end);

	if (uvm_map_lookup_entry(map, start, &entry)) {
		UVM_MAP_CLIP_START(map, entry, start);
	} else {
		entry = entry->next;
d2798 5
a2802 10

	/*
	 * make a first pass to check for protection violations.
	 */

	current = entry;
	while ((current != &map->header) && (current->start < end)) {
		if (UVM_ET_ISSUBMAP(current)) {
			error = EINVAL;
			goto out;
d2804 4
a2807 5
		if ((new_prot & current->max_protection) != new_prot) {
			error = EACCES;
			goto out;
		}
		current = current->next;
d2809 1
d2811 33
a2843 1
	/* go back and fix up protections (no need to clip this time). */
d2845 2
a2846 1
	current = entry;
d2848 25
a2872 2
	while ((current != &map->header) && (current->start < end)) {
		vm_prot_t old_prot;
d2874 15
a2888 1
		UVM_MAP_CLIP_END(map, current, end);
d2890 9
a2898 4
		old_prot = current->protection;
		if (set_max)
			current->protection =
			    (current->max_protection = new_prot) & old_prot;
d2900 87
a2986 1
			current->protection = new_prot;
d2989 1
a2989 1
		 * update physical map if necessary.  worry about copy-on-write 
d2992 23
a3014 8

		if (current->protection != old_prot) {
			/* update pmap! */
			if ((current->protection & MASK(entry)) == PROT_NONE &&
			    VM_MAPENT_ISWIRED(entry))
				current->wired_count--;
			pmap_protect(map->pmap, current->start, current->end,
			    current->protection & MASK(entry));
a3021 1

d3023 1
a3023 1
		    VM_MAPENT_ISWIRED(entry) == 0 &&
d3026 2
a3027 2
			if (uvm_map_pageable(map, entry->start, entry->end,
			    FALSE, UVM_LK_ENTER|UVM_LK_EXIT) != 0) {
d3032 2
a3033 2
				 * the map, but will return the resource
				 * shortage condition regardless.
a3043 2

		current = current->next;
d3047 1
a3047 1
 out:
d3050 1
a3050 1
	return (error);
d3053 2
a3054 5
#undef  max
#undef  MASK

/* 
 * uvm_map_inherit: set inheritance code for range of addrs in map.
d3056 3
a3058 3
 * => map must be unlocked
 * => note that the inherit code is used during a "fork".  see fork
 *	code for details.
d3060 3
a3062 4

int
uvm_map_inherit(struct vm_map *map, vaddr_t start, vaddr_t end,
    vm_inherit_t new_inheritance)
d3064 2
a3065 4
	struct vm_map_entry *entry;
	UVMHIST_FUNC("uvm_map_inherit"); UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist,"(map=%p,start=0x%lx,end=0x%lx,new_inh=0x%lx)",
	    map, start, end, new_inheritance);
d3067 4
a3070 29
	switch (new_inheritance) {
	case MAP_INHERIT_NONE:
	case MAP_INHERIT_COPY:
	case MAP_INHERIT_SHARE:
		break;
	default:
		UVMHIST_LOG(maphist,"<- done (INVALID ARG)",0,0,0,0);
		return (EINVAL);
	}

	vm_map_lock(map);
	
	VM_MAP_RANGE_CHECK(map, start, end);
	
	if (uvm_map_lookup_entry(map, start, &entry)) {
		UVM_MAP_CLIP_START(map, entry, start);
	} else {
		entry = entry->next;
	}

	while ((entry != &map->header) && (entry->start < end)) {
		UVM_MAP_CLIP_END(map, entry, end);
		entry->inheritance = new_inheritance;
		entry = entry->next;
	}

	vm_map_unlock(map);
	UVMHIST_LOG(maphist,"<- done (OK)",0,0,0,0);
	return (0);
d3073 2
a3074 2
/* 
 * uvm_map_advice: set advice code for range of addrs in map.
d3076 2
a3077 1
 * => map must be unlocked
d3079 5
d3085 5
a3089 7
int
uvm_map_advice(struct vm_map *map, vaddr_t start, vaddr_t end, int new_advice)
{
	struct vm_map_entry *entry;
	UVMHIST_FUNC("uvm_map_advice"); UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist,"(map=%p,start=0x%lx,end=0x%lx,new_adv=0x%lx)",
	    map, start, end, new_advice);
d3091 2
a3092 6
	switch (new_advice) {
	case MADV_NORMAL:
	case MADV_RANDOM:
	case MADV_SEQUENTIAL:
		/* nothing special here */
		break;
d3094 1
a3094 11
	default:
		UVMHIST_LOG(maphist,"<- done (INVALID ARG)",0,0,0,0);
		return (EINVAL);
	}
	vm_map_lock(map);
	VM_MAP_RANGE_CHECK(map, start, end);
	if (uvm_map_lookup_entry(map, start, &entry)) {
		UVM_MAP_CLIP_START(map, entry, start);
	} else {
		entry = entry->next;
	}
d3096 2
a3097 3
	/*
	 * XXXJRT: disallow holes?
	 */
d3099 2
a3100 2
	while ((entry != &map->header) && (entry->start < end)) {
		UVM_MAP_CLIP_END(map, entry, end);
d3102 6
a3107 3
		entry->advice = new_advice;
		entry = entry->next;
	}
d3109 6
a3114 3
	vm_map_unlock(map);
	UVMHIST_LOG(maphist,"<- done (OK)",0,0,0,0);
	return (0);
d3118 1
a3118 1
 * uvm_map_pageable: sets the pageability of a range in a map.
d3120 1
a3120 9
 * => wires map entries.  should not be used for transient page locking.
 *	for that, use uvm_fault_wire()/uvm_fault_unwire() (see uvm_vslock()).
 * => regions sepcified as not pageable require lock-down (wired) memory
 *	and page tables.
 * => map must never be read-locked
 * => if islocked is TRUE, map is already write-locked
 * => we always unlock the map, since we must downgrade to a read-lock
 *	to call uvm_fault_wire()
 * => XXXCDC: check this and try and clean it up.
d3123 2
a3124 3
int
uvm_map_pageable(struct vm_map *map, vaddr_t start, vaddr_t end,
    boolean_t new_pageable, int lockflags)
d3126 3
a3128 9
	struct vm_map_entry *entry, *start_entry, *failed_entry;
	int rv;
#ifdef DIAGNOSTIC
	u_int timestamp_save;
#endif
	UVMHIST_FUNC("uvm_map_pageable"); UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist,"(map=%p,start=0x%lx,end=0x%lx,new_pageable=0x%lx)",
		    map, start, end, new_pageable);
	KASSERT(map->flags & VM_MAP_PAGEABLE);
d3130 2
a3131 2
	if ((lockflags & UVM_LK_ENTER) == 0)
		vm_map_lock(map);
d3133 2
a3134 1
	VM_MAP_RANGE_CHECK(map, start, end);
d3136 2
a3137 6
	/* 
	 * only one pageability change may take place at one time, since
	 * uvm_fault_wire assumes it will be called only once for each
	 * wiring/unwiring.  therefore, we have to make sure we're actually
	 * changing the pageability for the entire region.  we do so before
	 * making any changes.  
d3140 5
a3144 3
	if (uvm_map_lookup_entry(map, start, &start_entry) == FALSE) {
		if ((lockflags & UVM_LK_EXIT) == 0)
			vm_map_unlock(map);
d3146 7
a3152 4
		UVMHIST_LOG(maphist,"<- done (INVALID ARG)",0,0,0,0);
		return (EFAULT);
	}
	entry = start_entry;
d3154 6
a3159 6
	/* 
	 * handle wiring and unwiring separately.
	 */

	if (new_pageable) {		/* unwire */
		UVM_MAP_CLIP_START(map, entry, start);
d3162 8
a3169 2
		 * unwiring.  first ensure that the range to be unwired is
		 * really wired down and that there are no holes.  
d3171 2
d3174 1
a3174 13
		while ((entry != &map->header) && (entry->start < end)) {
			if (entry->wired_count == 0 ||
			    (entry->end < end &&
			     (entry->next == &map->header ||
			      entry->next->start > entry->end))) {
				if ((lockflags & UVM_LK_EXIT) == 0)
					vm_map_unlock(map);
				UVMHIST_LOG(maphist,
				    "<- done (INVALID UNWIRE ARG)",0,0,0,0);
				return (EINVAL);
			}
			entry = entry->next;
		}
d3176 2
a3177 4
		/* 
		 * POSIX 1003.1b - a single munlock call unlocks a region,
		 * regardless of the number of mlock calls made on that
		 * region.
d3179 2
d3182 5
a3186 6
		entry = start_entry;
		while ((entry != &map->header) && (entry->start < end)) {
			UVM_MAP_CLIP_END(map, entry, end);
			if (VM_MAPENT_ISWIRED(entry))
				uvm_map_entry_unwire(map, entry);
			entry = entry->next;
a3187 5
		if ((lockflags & UVM_LK_EXIT) == 0)
			vm_map_unlock(map);
		UVMHIST_LOG(maphist,"<- done (OK UNWIRE)",0,0,0,0);
		return (0);
	}
d3189 7
a3195 20
	/*
	 * wire case: in two passes [XXXCDC: ugly block of code here]
	 *
	 * 1: holding the write lock, we create any anonymous maps that need
	 *    to be created.  then we clip each map entry to the region to
	 *    be wired and increment its wiring count.  
	 *
	 * 2: we downgrade to a read lock, and call uvm_fault_wire to fault
	 *    in the pages for any newly wired area (wired_count == 1).
	 *
	 *    downgrading to a read lock for uvm_fault_wire avoids a possible
	 *    deadlock with another thread that may have faulted on one of
	 *    the pages to be wired (it would mark the page busy, blocking
	 *    us, then in turn block on the map lock that we hold).  because
	 *    of problems in the recursive lock package, we cannot upgrade
	 *    to a write lock in vm_map_lookup.  thus, any actions that
	 *    require the write lock must be done beforehand.  because we
	 *    keep the read lock on the map, the copy-on-write status of the
	 *    entries we modify here cannot change.
	 */
d3197 4
a3200 2
	while ((entry != &map->header) && (entry->start < end)) {
		if (VM_MAPENT_ISWIRED(entry) == 0) { /* not already wired? */
d3202 1
a3202 7
			/*
			 * perform actions of vm_map_lookup that need the
			 * write lock on the map: create an anonymous map
			 * for a copy-on-write region, or an anonymous map
			 * for a zero-fill region.  (XXXCDC: submap case
			 * ok?)
			 */
d3204 7
a3210 13
			if (!UVM_ET_ISSUBMAP(entry)) {  /* not submap */
				if (UVM_ET_ISNEEDSCOPY(entry) && 
				    ((entry->protection & VM_PROT_WRITE) ||
				     (entry->object.uvm_obj == NULL))) {
					amap_copy(map, entry, M_WAITOK, TRUE,
					    start, end); 
					/* XXXCDC: wait OK? */
				}
			}
		}
		UVM_MAP_CLIP_START(map, entry, start);
		UVM_MAP_CLIP_END(map, entry, end);
		entry->wired_count++;
d3213 1
a3213 1
		 * Check for holes 
d3216 3
a3218 4
		if (entry->protection == VM_PROT_NONE ||
		    (entry->end < end &&
		     (entry->next == &map->header ||
		      entry->next->start > entry->end))) {
d3220 1
a3220 15
			/*
			 * found one.  amap creation actions do not need to
			 * be undone, but the wired counts need to be restored. 
			 */

			while (entry != &map->header && entry->end > start) {
				entry->wired_count--;
				entry = entry->prev;
			}
			if ((lockflags & UVM_LK_EXIT) == 0)
				vm_map_unlock(map);
			UVMHIST_LOG(maphist,"<- done (INVALID WIRE)",0,0,0,0);
			return (EINVAL);
		}
		entry = entry->next;
d3224 1
a3224 1
	 * Pass 2.
d3226 2
d3229 5
a3233 5
#ifdef DIAGNOSTIC
	timestamp_save = map->timestamp;
#endif
	vm_map_busy(map);
	vm_map_downgrade(map);
d3235 4
a3238 17
	rv = 0;
	entry = start_entry;
	while (entry != &map->header && entry->start < end) {
		if (entry->wired_count == 1) {
			rv = uvm_fault_wire(map, entry->start, entry->end,
			    entry->protection);
			if (rv) {
				/*
				 * wiring failed.  break out of the loop.
				 * we'll clean up the map below, once we
				 * have a write lock again.
				 */
				break;
			}
		}
		entry = entry->next;
	}
d3240 1
a3240 1
	if (rv) {        /* failed? */
d3242 2
d3245 3
a3247 1
		 * Get back to an exclusive (write) lock.
d3249 4
a3252 7

		vm_map_upgrade(map);
		vm_map_unbusy(map);

#ifdef DIAGNOSTIC
		if (timestamp_save != map->timestamp)
			panic("uvm_map_pageable: stale map");
d3254 6
a3259 10

		/*
		 * first drop the wiring count on all the entries
		 * which haven't actually been wired yet.
		 */

		failed_entry = entry;
		while (entry != &map->header && entry->start < end) {
			entry->wired_count--;
			entry = entry->next;
d3261 15
d3277 38
a3314 4
		/*
		 * now, unwire all the entries that were successfully
		 * wired above.
		 */
d3316 9
a3324 11
		entry = start_entry;
		while (entry != failed_entry) {
			entry->wired_count--;
			if (VM_MAPENT_ISWIRED(entry) == 0)
				uvm_map_entry_unwire(map, entry);
			entry = entry->next;
		}
		if ((lockflags & UVM_LK_EXIT) == 0)
			vm_map_unlock(map);
		UVMHIST_LOG(maphist, "<- done (RV=%ld)", rv,0,0,0);
		return(rv);
d3327 5
a3331 12
	/* We are holding a read lock here. */
	if ((lockflags & UVM_LK_EXIT) == 0) {
		vm_map_unbusy(map);
		vm_map_unlock_read(map);
	} else {

		/*
		 * Get back to an exclusive (write) lock.
		 */

		vm_map_upgrade(map);
		vm_map_unbusy(map);
d3334 1
a3334 2
	UVMHIST_LOG(maphist,"<- done (OK WIRE)",0,0,0,0);
	return (0);
d3338 2
a3339 6
 * uvm_map_pageable_all: special case of uvm_map_pageable - affects
 * all mapped regions.
 *
 * => map must not be locked.
 * => if no flags are specified, all regions are unwired.
 * => XXXJRT: has some of the same problems as uvm_map_pageable() above.
d3341 4
a3344 3

int
uvm_map_pageable_all(struct vm_map *map, int flags, vsize_t limit)
d3346 1
a3346 8
	struct vm_map_entry *entry, *failed_entry;
	vsize_t size;
	int error;
#ifdef DIAGNOSTIC
	u_int timestamp_save;
#endif
	UVMHIST_FUNC("uvm_map_pageable_all"); UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist,"(map=%p,flags=0x%lx)", map, flags, 0, 0);
d3348 17
a3364 1
	KASSERT(map->flags & VM_MAP_PAGEABLE);
d3366 7
a3372 1
	vm_map_lock(map);
d3375 1
a3375 1
	 * handle wiring and unwiring separately.
d3377 5
a3381 19

	if (flags == 0) {			/* unwire */
		/*
		 * POSIX 1003.1b -- munlockall unlocks all regions,
		 * regardless of how many times mlockall has been called.
		 */
		for (entry = map->header.next; entry != &map->header;
		     entry = entry->next) {
			if (VM_MAPENT_ISWIRED(entry))
				uvm_map_entry_unwire(map, entry);
		}
		vm_map_modflags(map, 0, VM_MAP_WIREFUTURE);
		vm_map_unlock(map);
		UVMHIST_LOG(maphist,"<- done (OK UNWIRE)",0,0,0,0);
		return (0);

		/*
		 * end of unwire case!
		 */
d3383 1
d3385 29
a3413 18
	if (flags & MCL_FUTURE) {
		/*
		 * must wire all future mappings; remember this.
		 */
		vm_map_modflags(map, VM_MAP_WIREFUTURE, 0);
	}

	if ((flags & MCL_CURRENT) == 0) {
		/*
		 * no more work to do!
		 */
		UVMHIST_LOG(maphist,"<- done (OK no wire)",0,0,0,0);
		vm_map_unlock(map);
		return (0);
	}

	/*
	 * wire case: in three passes [XXXCDC: ugly block of code here]
d3415 14
a3428 2
	 * 1: holding the write lock, count all pages mapped by non-wired
	 *    entries.  if this would cause us to go over our limit, we fail.
d3430 6
a3435 2
	 * 2: still holding the write lock, we create any anonymous maps that
	 *    need to be created.  then we increment its wiring count.
a3436 12
	 * 3: we downgrade to a read lock, and call uvm_fault_wire to fault
	 *    in the pages for any newly wired area (wired_count == 1).
	 *
	 *    downgrading to a read lock for uvm_fault_wire avoids a possible
	 *    deadlock with another thread that may have faulted on one of
	 *    the pages to be wired (it would mark the page busy, blocking
	 *    us, then in turn block on the map lock that we hold).  because
	 *    of problems in the recursive lock package, we cannot upgrade
	 *    to a write lock in vm_map_lookup.  thus, any actions that
	 *    require the write lock must be done beforehand.  because we
	 *    keep the read lock on the map, the copy-on-write status of the
	 *    entries we modify here cannot change.
d3439 7
a3445 6
	for (size = 0, entry = map->header.next; entry != &map->header;
	     entry = entry->next) {
		if (entry->protection != VM_PROT_NONE &&
		    VM_MAPENT_ISWIRED(entry) == 0) { /* not already wired? */
			size += entry->end - entry->start;
		}
d3448 12
a3459 4
	if (atop(size) + uvmexp.wired > uvmexp.wiredmax) {
		vm_map_unlock(map);
		return (ENOMEM);		/* XXX overloaded */
	}
d3461 9
a3469 8
	/* XXX non-pmap_wired_count case must be handled by caller */
#ifdef pmap_wired_count
	if (limit != 0 &&
	    (size + ptoa(pmap_wired_count(vm_map_pmap(map))) > limit)) {
		vm_map_unlock(map);
		return (ENOMEM);		/* XXX overloaded */
	}
#endif
d3471 2
a3472 3
	/*
	 * Pass 2.
	 */
a3473 5
	for (entry = map->header.next; entry != &map->header;
	     entry = entry->next) {
		if (entry->protection == VM_PROT_NONE)
			continue;
		if (VM_MAPENT_ISWIRED(entry) == 0) { /* not already wired? */
d3475 14
a3488 5
			 * perform actions of vm_map_lookup that need the
			 * write lock on the map: create an anonymous map
			 * for a copy-on-write region, or an anonymous map
			 * for a zero-fill region.  (XXXCDC: submap case
			 * ok?)
d3490 9
a3498 7
			if (!UVM_ET_ISSUBMAP(entry)) {	/* not submap */
				if (UVM_ET_ISNEEDSCOPY(entry) && 
				    ((entry->protection & VM_PROT_WRITE) ||
				     (entry->object.uvm_obj == NULL))) {
					amap_copy(map, entry, M_WAITOK, TRUE,
					    entry->start, entry->end);
					/* XXXCDC: wait OK? */
d3500 1
a3501 3
		}
		entry->wired_count++;
	}
d3503 5
a3507 3
	/*
	 * Pass 3.
	 */
d3509 8
a3516 5
#ifdef DIAGNOSTIC
	timestamp_save = map->timestamp;
#endif
	vm_map_busy(map);
	vm_map_downgrade(map);
a3517 6
	for (error = 0, entry = map->header.next;
	    entry != &map->header && error == 0;
	    entry = entry->next) {
		if (entry->wired_count == 1) {
			error = uvm_fault_wire(map, entry->start, entry->end,
			     entry->protection);
a3518 1
	}
a3519 1
	if (error) {	/* failed? */
d3521 2
a3522 1
		 * Get back an exclusive (write) lock.
a3523 2
		vm_map_upgrade(map);
		vm_map_unbusy(map);
d3525 4
a3528 4
#ifdef DIAGNOSTIC
		if (timestamp_save != map->timestamp)
			panic("uvm_map_pageable_all: stale map");
#endif
d3531 1
a3531 4
		 * first drop the wiring count on all the entries
		 * which haven't actually been wired yet.
		 *
		 * Skip VM_PROT_NONE entries like we did above.
d3533 5
a3537 6
		failed_entry = entry;
		for (/* nothing */; entry != &map->header;
		     entry = entry->next) {
			if (entry->protection == VM_PROT_NONE)
				continue;
			entry->wired_count--;
d3539 1
d3541 9
a3549 25
		/*
		 * now, unwire all the entries that were successfully
		 * wired above.
		 *
		 * Skip VM_PROT_NONE entries like we did above.
		 */
		for (entry = map->header.next; entry != failed_entry;
		     entry = entry->next) {
			if (entry->protection == VM_PROT_NONE)
				continue;
			entry->wired_count--;
			if (VM_MAPENT_ISWIRED(entry))
				uvm_map_entry_unwire(map, entry);
		}
		vm_map_unlock(map);
		UVMHIST_LOG(maphist,"<- done (RV=%ld)", error,0,0,0);
		return (error);
	}

	/* We are holding a read lock here. */
	vm_map_unbusy(map);
	vm_map_unlock_read(map);

	UVMHIST_LOG(maphist,"<- done (OK WIRE)",0,0,0,0);
	return (0);
d3553 1
a3553 1
 * uvm_map_clean: clean out a map range
d3555 2
a3556 11
 * => valid flags:
 *   if (flags & PGO_CLEANIT): dirty pages are cleaned first
 *   if (flags & PGO_SYNCIO): dirty pages are written synchronously
 *   if (flags & PGO_DEACTIVATE): any cached pages are deactivated after clean
 *   if (flags & PGO_FREE): any cached pages are freed after clean
 * => returns an error if any part of the specified range isn't mapped
 * => never a need to flush amap layer since the anonymous memory has 
 *	no permanent home, but may deactivate pages there
 * => called from sys_msync() and sys_madvise()
 * => caller must not write-lock map (read OK).
 * => we may sleep while cleaning if SYNCIO [with map read-locked]
d3558 2
a3559 5

int	amap_clean_works = 1;	/* XXX for now, just in case... */

int
uvm_map_clean(struct vm_map *map, vaddr_t start, vaddr_t end, int flags)
d3561 6
a3566 9
	struct vm_map_entry *current, *entry;
	struct uvm_object *uobj;
	struct vm_amap *amap;
	struct vm_anon *anon;
	struct vm_page *pg;
	vaddr_t offset;
	vsize_t size;
	int rv, error, refs;
	UVMHIST_FUNC("uvm_map_clean"); UVMHIST_CALLED(maphist);
d3568 1
a3568 4
	UVMHIST_LOG(maphist,"(map=%p,start=0x%lx,end=0x%lx,flags=0x%lx)",
		    map, start, end, flags);
	KASSERT((flags & (PGO_FREE|PGO_DEACTIVATE)) !=
		(PGO_FREE|PGO_DEACTIVATE));
d3570 7
a3576 6
	vm_map_lock_read(map);
	VM_MAP_RANGE_CHECK(map, start, end);
	if (uvm_map_lookup_entry(map, start, &entry) == FALSE) {
		vm_map_unlock_read(map);
		return (EFAULT);
	}
d3579 1
a3579 1
	 * Make a first pass to check for holes.
d3582 11
a3592 4
	for (current = entry; current->start < end; current = current->next) {
		if (UVM_ET_ISSUBMAP(current)) {
			vm_map_unlock_read(map);
			return (EINVAL);
d3594 5
a3598 4
		if (end > current->end && (current->next == &map->header ||
		    current->end != current->next->start)) {
			vm_map_unlock_read(map);
			return (EFAULT);
a3599 8
	}

	error = 0;

	for (current = entry; current->start < end; current = current->next) {
		amap = current->aref.ar_amap;	/* top layer */
		uobj = current->object.uvm_obj;	/* bottom layer */
		KASSERT(start >= current->start);
d3602 1
a3602 5
		 * No amap cleaning necessary if:
		 *
		 *	(1) There's no amap.
		 *
		 *	(2) We're not deactivating or freeing pages.
d3604 9
d3614 2
a3615 2
		if (amap == NULL || (flags & (PGO_DEACTIVATE|PGO_FREE)) == 0)
			goto flush_object;
d3617 5
a3621 3
		/* XXX for now, just in case... */
		if (amap_clean_works == 0)
			goto flush_object;
d3623 4
a3626 6
		offset = start - current->start;
		size = MIN(end, current->end) - start;
		for ( ; size != 0; size -= PAGE_SIZE, offset += PAGE_SIZE) {
			anon = amap_lookup(&current->aref, offset);
			if (anon == NULL)
				continue;
d3628 3
a3630 1
			simple_lock(&anon->an_lock);
d3632 3
a3634 5
			pg = anon->an_page;
			if (pg == NULL) {
				simple_unlock(&anon->an_lock);
				continue;
			}
d3636 8
a3643 1
			switch (flags & (PGO_CLEANIT|PGO_FREE|PGO_DEACTIVATE)) {
d3645 16
a3660 42
			/*
			 * XXX In these first 3 cases, we always just
			 * XXX deactivate the page.  We may want to
			 * XXX handle the different cases more
			 * XXX specifically, in the future.
			 */

			case PGO_CLEANIT|PGO_FREE:
			case PGO_CLEANIT|PGO_DEACTIVATE:
			case PGO_DEACTIVATE:
 deactivate_it:
				/* skip the page if it's loaned or wired */
				if (pg->loan_count != 0 ||
				    pg->wire_count != 0) {
					simple_unlock(&anon->an_lock);
					continue;
				}

				uvm_lock_pageq();

				/*
				 * skip the page if it's not actually owned
				 * by the anon (may simply be loaned to the
				 * anon).
				 */

				if ((pg->pg_flags & PQ_ANON) == 0) {
					KASSERT(pg->uobject == NULL);
					uvm_unlock_pageq();
					simple_unlock(&anon->an_lock);
					continue;
				}
				KASSERT(pg->uanon == anon);

#ifdef UBC
				/* ...and deactivate the page. */
				pmap_clear_reference(pg);
#else
				/* zap all mappings for the page. */
				pmap_page_protect(pg, VM_PROT_NONE);

				/* ...and deactivate the page. */
d3662 2
a3663 1
				uvm_pagedeactivate(pg);
d3665 22
a3686 3
				uvm_unlock_pageq();
				simple_unlock(&anon->an_lock);
				continue;
d3688 1
a3688 1
			case PGO_FREE:
d3690 1
a3690 4
				/*
				 * If there are multiple references to
				 * the amap, just deactivate the page.
				 */
d3692 5
a3696 2
				if (amap_refs(amap) > 1)
					goto deactivate_it;
d3698 11
a3708 11
				/* XXX skip the page if it's wired */
				if (pg->wire_count != 0) {
					simple_unlock(&anon->an_lock);
					continue;
				}
				amap_unadd(&current->aref, offset);
				refs = --anon->an_ref;
				simple_unlock(&anon->an_lock);
				if (refs == 0)
					uvm_anfree(anon);
				continue;
d3710 2
a3711 32
			default:
				panic("uvm_map_clean: weird flags");
			}
		}

flush_object:
		/*
		 * flush pages if we've got a valid backing object.
		 *
		 * Don't PGO_FREE if we don't have write permission
	 	 * and don't flush if this is a copy-on-write object
		 * since we can't know our permissions on it.
		 */

		offset = current->offset + (start - current->start);
		size = MIN(end, current->end) - start;
		if (uobj != NULL &&
		    ((flags & PGO_FREE) == 0 ||
		     ((entry->max_protection & VM_PROT_WRITE) != 0 &&
		      (entry->etype & UVM_ET_COPYONWRITE) == 0))) {
			simple_lock(&uobj->vmobjlock);
			rv = uobj->pgops->pgo_flush(uobj, offset,
			    offset + size, flags);
			simple_unlock(&uobj->vmobjlock);

			if (rv == FALSE)
				error = EFAULT;
		}
		start += size;
	}
	vm_map_unlock_read(map);
	return (error); 
a3713 1

d3717 2
a3718 2
 * => must allow specified protection in a fully allocated region.
 * => map must be read or write locked by caller.
a3719 1

a3724 1
	struct vm_map_entry *tmp_entry;
d3726 4
a3729 8
	if (!uvm_map_lookup_entry(map, start, &tmp_entry)) {
		return(FALSE);
	}
	entry = tmp_entry;
	while (start < end) {
		if (entry == &map->header) {
			return(FALSE);
		}
d3731 6
d3738 1
a3738 1
		 * no holes allowed
d3740 3
a3742 4

		if (start < entry->start) {
			return(FALSE);
		}
d3745 1
a3745 1
		 * check protection associated with entry
d3747 2
a3748 9

		if ((entry->protection & protection) != protection) {
			return(FALSE);
		}

		/* go to next entry */

		start = entry->end;
		entry = entry->next;
d3750 1
a3750 1
	return(TRUE);
d3754 1
a3754 5
 * uvmspace_alloc: allocate a vmspace structure.
 *
 * - structure includes vm_map and pmap
 * - XXX: no locking on this structure
 * - refcnt set to 1, rest must be init'd by caller
d3756 2
a3757 3
struct vmspace *
uvmspace_alloc(vaddr_t min, vaddr_t max, boolean_t pageable,
    boolean_t remove_holes)
d3759 1
a3759 2
	struct vmspace *vm;
	UVMHIST_FUNC("uvmspace_alloc"); UVMHIST_CALLED(maphist);
d3761 4
a3764 4
	vm = pool_get(&uvm_vmspace_pool, PR_WAITOK | PR_ZERO);
	uvmspace_init(vm, NULL, min, max, pageable, remove_holes);
	UVMHIST_LOG(maphist,"<- done (vm=%p)", vm,0,0,0);
	return (vm);
d3768 1
a3768 1
 * uvmspace_init: initialize a vmspace structure.
d3770 2
a3771 2
 * - XXX: no locking on this structure
 * - refcnt set to 1, rest must be init'd by caller
d3774 39
a3812 2
uvmspace_init(struct vmspace *vm, struct pmap *pmap, vaddr_t min, vaddr_t max,
    boolean_t pageable, boolean_t remove_holes)
d3814 16
a3829 1
	UVMHIST_FUNC("uvmspace_init"); UVMHIST_CALLED(maphist);
d3831 1
a3831 1
	uvm_map_setup(&vm->vm_map, min, max, pageable ? VM_MAP_PAGEABLE : 0);
d3833 3
a3835 2
	if (pmap)
		pmap_reference(pmap);
d3837 1
a3837 2
		pmap = pmap_create();
	vm->vm_map.pmap = pmap;
d3839 5
a3843 1
	vm->vm_refcnt = 1;
d3845 3
a3847 4
	if (remove_holes)
		pmap_remove_holes(&vm->vm_map);

	UVMHIST_LOG(maphist,"<- done",0,0,0,0);
d3850 2
a3851 2
/*
 * uvmspace_share: share a vmspace between two proceses
d3853 1
a3853 2
 * - XXX: no locking on vmspace
 * - used for vfork, threads(?)
d3855 31
d3887 9
a3895 6
void
uvmspace_share(p1, p2)
	struct proc *p1, *p2;
{
	p2->p_vmspace = p1->p_vmspace;
	p1->p_vmspace->vm_refcnt++;
d3899 2
a3900 1
 * uvmspace_exec: the process wants to exec a new program
d3902 7
a3908 1
 * - XXX: no locking on vmspace
d3910 14
d3925 5
a3929 5
void
uvmspace_exec(struct proc *p, vaddr_t start, vaddr_t end)
{
	struct vmspace *nvm, *ovm = p->p_vmspace;
	struct vm_map *map = &ovm->vm_map;
d3931 4
a3934 1
	pmap_unuse_final(p);   /* before stack addresses go away */
d3937 2
a3938 1
	 * see if more than one process is using this vmspace...
d3941 7
a3947 1
	if (ovm->vm_refcnt == 1) {
d3949 4
a3952 4
		/*
		 * if p is the only process using its vmspace then we can safely
		 * recycle that vmspace for the program that is being exec'd.
		 */
d3954 14
a3967 7
#ifdef SYSVSHM
		/*
		 * SYSV SHM semantics require us to kill all segments on an exec
		 */
		if (ovm->vm_shm)
			shmexit(ovm);
#endif
d3969 19
a3987 7
		/*
		 * POSIX 1003.1b -- "lock future mappings" is revoked
		 * when a process execs another program image.
		 */
		vm_map_lock(map);
		vm_map_modflags(map, 0, VM_MAP_WIREFUTURE);
		vm_map_unlock(map);
d3989 4
a3992 4
		/*
		 * now unmap the old program
		 */
		uvm_unmap(map, map->min_offset, map->max_offset);
d3994 7
a4000 4
		/*
		 * but keep MMU holes unavailable
		 */
		pmap_remove_holes(map);
d4002 4
a4005 12
		/*
		 * resize the map
		 */
		vm_map_lock(map);
		map->min_offset = start;
		uvm_tree_sanity(map, "resize enter");
		map->max_offset = end;
		if (map->header.prev != &map->header)
			uvm_rb_fixup(map, map->header.prev);
		uvm_tree_sanity(map, "resize leave");
		vm_map_unlock(map);
	
d4007 8
a4014 1
	} else {
d4017 1
a4017 3
		 * p's vmspace is being shared, so we can't reuse it for p since
		 * it is still being used for others.   allocate a new vmspace
		 * for p
d4019 19
a4037 2
		nvm = uvmspace_alloc(start, end,
			 (map->flags & VM_MAP_PAGEABLE) ? TRUE : FALSE, TRUE);
d4039 5
d4045 1
a4045 1
		 * install new vmspace and drop our ref to the old one.
d4047 9
d4057 1
a4057 3
		pmap_deactivate(p);
		p->p_vmspace = nvm;
		pmap_activate(p);
d4059 7
a4065 1
		uvmspace_free(ovm);
d4067 13
d4083 1
a4083 1
 * uvmspace_free: free a vmspace data structure
d4085 11
a4095 1
 * - XXX: no locking on vmspace
d4098 4
a4101 2
void
uvmspace_free(struct vmspace *vm)
d4103 147
a4249 2
	struct vm_map_entry *dead_entries;
	UVMHIST_FUNC("uvmspace_free"); UVMHIST_CALLED(maphist);
a4250 2
	UVMHIST_LOG(maphist,"(vm=%p) ref=%ld", vm, vm->vm_refcnt,0,0);
	if (--vm->vm_refcnt == 0) {
d4252 5
a4256 3
		 * lock the map, to wait out all other references to it.  delete
		 * all of the mappings and pages they hold, then call the pmap
		 * module to reclaim anything left.
d4258 12
a4269 12
#ifdef SYSVSHM
		/* Get rid of any SYSV shared memory segments. */
		if (vm->vm_shm != NULL)
			shmexit(vm);
#endif
		vm_map_lock(&vm->vm_map);
		if (vm->vm_map.nentries) {
			uvm_unmap_remove(&vm->vm_map,
			    vm->vm_map.min_offset, vm->vm_map.max_offset,
			    &dead_entries, NULL, TRUE);
			if (dead_entries != NULL)
				uvm_unmap_detach(dead_entries, 0);
a4270 3
		pmap_destroy(vm->vm_map.pmap);
		vm->vm_map.pmap = NULL;
		pool_put(&uvm_vmspace_pool, vm);
d4272 3
a4274 1
	UVMHIST_LOG(maphist,"<- done", 0,0,0,0);
d4278 1
a4278 1
 * uvm_map_create: create map
d4280 2
a4281 2
vm_map_t
uvm_map_create(pmap_t pmap, vaddr_t min, vaddr_t max, int flags)
d4283 4
a4286 1
	vm_map_t result;
d4288 4
a4291 4
	result = malloc(sizeof(struct vm_map), M_VMMAP, M_WAITOK);
	uvm_map_setup(result, min, max, flags);
	result->pmap = pmap;
	return(result);
d4295 1
a4295 1
 * uvm_map_setup: init map
d4297 5
a4301 1
 * => map must not be in service yet.
d4304 1
a4304 1
uvm_map_setup(vm_map_t map, vaddr_t min, vaddr_t max, int flags)
d4306 26
d4333 8
a4340 14
	RB_INIT(&map->rbhead);
	map->header.next = map->header.prev = &map->header;
	map->nentries = 0;
	map->size = 0;
	map->ref_count = 1;
	map->min_offset = min;
	map->max_offset = max;
	map->flags = flags;
	map->first_free = &map->header;
	map->hint = &map->header;
	map->timestamp = 0;
	rw_init(&map->lock, "vmmaplk");
	simple_lock_init(&map->ref_lock);
	simple_lock_init(&map->hint_lock);
d4343 9
d4353 13
d4368 1
a4368 3
 * uvm_map_reference: add reference to a map
 *
 * => map need not be locked (we use ref_lock).
d4370 2
a4371 2
void
uvm_map_reference(vm_map_t map)
d4373 12
a4384 3
	simple_lock(&map->ref_lock);
	map->ref_count++; 
	simple_unlock(&map->ref_lock);
d4388 1
a4388 4
 * uvm_map_deallocate: drop reference to a map
 *
 * => caller must not lock map
 * => we will zap map if ref count goes to zero
d4391 2
a4392 1
uvm_map_deallocate(vm_map_t map)
d4394 2
a4395 1
	int c;
d4397 2
a4398 6
	simple_lock(&map->ref_lock);
	c = --map->ref_count;
	simple_unlock(&map->ref_lock);
	if (c > 0) {
		return;
	}
d4401 1
a4401 1
	 * all references gone.   unmap and free.
d4403 17
d4421 2
a4422 3
	uvm_unmap(map, map->min_offset, map->max_offset);
	pmap_destroy(map->pmap);
	free(map, M_VMMAP);
d4426 3
a4428 4
 *   F O R K   -   m a i n   e n t r y   p o i n t
 */
/*
 * uvmspace_fork: fork a process' main map
d4430 2
a4431 2
 * => create a new vmspace for child process from parent.
 * => parent's map must not be locked.
d4433 3
a4435 3

struct vmspace *
uvmspace_fork(struct vmspace *vm1)
d4437 3
a4439 10
	struct vmspace *vm2;
	struct vm_map *old_map = &vm1->vm_map;
	struct vm_map *new_map;
	struct vm_map_entry *old_entry;
	struct vm_map_entry *new_entry;
	pmap_t          new_pmap;
	boolean_t	protect_child;
	UVMHIST_FUNC("uvmspace_fork"); UVMHIST_CALLED(maphist);

	vm_map_lock(old_map);
d4441 4
a4444 6
	vm2 = uvmspace_alloc(old_map->min_offset, old_map->max_offset,
	    (old_map->flags & VM_MAP_PAGEABLE) ? TRUE : FALSE, FALSE);
	memcpy(&vm2->vm_startcopy, &vm1->vm_startcopy,
	(caddr_t) (vm1 + 1) - (caddr_t) &vm1->vm_startcopy);
	new_map = &vm2->vm_map;		  /* XXX */
	new_pmap = new_map->pmap;
d4446 7
a4452 1
	old_entry = old_map->header.next;
d4455 1
a4455 1
	 * go entry-by-entry
d4457 2
d4460 34
a4493 1
	while (old_entry != &old_map->header) {
d4495 8
a4502 5
		/*
		 * first, some sanity checks on the old entry
		 */
		if (UVM_ET_ISSUBMAP(old_entry))
		    panic("fork: encountered a submap during fork (illegal)");
d4504 8
a4511 3
		if (!UVM_ET_ISCOPYONWRITE(old_entry) &&
			    UVM_ET_ISNEEDSCOPY(old_entry))
	panic("fork: non-copy_on_write map entry marked needs_copy (illegal)");
d4513 8
d4522 4
a4525 6
		switch (old_entry->inheritance) {
		case MAP_INHERIT_NONE:
			/*
			 * drop the mapping
			 */
			break;
d4527 2
a4528 5
		case MAP_INHERIT_SHARE:
			/*
			 * share the mapping: this means we want the old and
			 * new entries to share amaps and backing objects.
			 */
d4530 2
a4531 6
			/*
			 * if the old_entry needs a new amap (due to prev fork)
			 * then we need to allocate it now so that we have
			 * something we own to share with the new_entry.   [in
			 * other words, we need to clear needs_copy]
			 */
d4533 8
a4540 6
			if (UVM_ET_ISNEEDSCOPY(old_entry)) {
				/* get our own amap, clears needs_copy */
				amap_copy(old_map, old_entry, M_WAITOK, FALSE,
				    0, 0); 
				/* XXXCDC: WAITOK??? */
			}
d4542 2
a4543 3
			new_entry = uvm_mapent_alloc(new_map, 0);
			/* old_entry -> new_entry */
			uvm_mapent_copy(old_entry, new_entry);
d4545 5
a4549 2
			/* new pmap has nothing wired in it */
			new_entry->wired_count = 0;
d4551 3
a4553 23
			/*
			 * gain reference to object backing the map (can't
			 * be a submap, already checked this case).
			 */
			if (new_entry->aref.ar_amap)
				/* share reference */
				uvm_map_reference_amap(new_entry, AMAP_SHARED);

			if (new_entry->object.uvm_obj &&
			    new_entry->object.uvm_obj->pgops->pgo_reference)
				new_entry->object.uvm_obj->
				    pgops->pgo_reference(
				        new_entry->object.uvm_obj);

			/* insert entry at end of new_map's entry list */
			uvm_map_entry_link(new_map, new_map->header.prev,
			    new_entry);

			/* 
			 * pmap_copy the mappings: this routine is optional
			 * but if it is there it will reduce the number of
			 * page faults in the new proc.
			 */
d4555 9
a4563 3
			pmap_copy(new_pmap, old_map->pmap, new_entry->start,
			    (old_entry->end - old_entry->start),
			    old_entry->start);
d4565 27
a4591 1
			break;
d4593 35
a4627 1
		case MAP_INHERIT_COPY:
d4629 2
a4630 7
			/*
			 * copy-on-write the mapping (using mmap's
			 * MAP_PRIVATE semantics)
			 *
			 * allocate new_entry, adjust reference counts.  
			 * (note that new references are read-only).
			 */
d4632 2
a4633 19
			new_entry = uvm_mapent_alloc(new_map, 0);
			/* old_entry -> new_entry */
			uvm_mapent_copy(old_entry, new_entry);

			if (new_entry->aref.ar_amap)
				uvm_map_reference_amap(new_entry, 0);

			if (new_entry->object.uvm_obj &&
			    new_entry->object.uvm_obj->pgops->pgo_reference)
				new_entry->object.uvm_obj->pgops->pgo_reference
				    (new_entry->object.uvm_obj);

			/* new pmap has nothing wired in it */
			new_entry->wired_count = 0;

			new_entry->etype |=
			    (UVM_ET_COPYONWRITE|UVM_ET_NEEDSCOPY);
			uvm_map_entry_link(new_map, new_map->header.prev,
			    new_entry);
d4635 2
a4636 31
			/*
			 * the new entry will need an amap.  it will either
			 * need to be copied from the old entry or created
			 * from scratch (if the old entry does not have an
			 * amap).  can we defer this process until later
			 * (by setting "needs_copy") or do we need to copy
			 * the amap now?
			 *
			 * we must copy the amap now if any of the following
			 * conditions hold:
			 * 1. the old entry has an amap and that amap is
			 *    being shared.  this means that the old (parent)
			 *    process is sharing the amap with another 
			 *    process.  if we do not clear needs_copy here
			 *    we will end up in a situation where both the
			 *    parent and child process are referring to the
			 *    same amap with "needs_copy" set.  if the 
			 *    parent write-faults, the fault routine will
			 *    clear "needs_copy" in the parent by allocating
			 *    a new amap.   this is wrong because the 
			 *    parent is supposed to be sharing the old amap
			 *    and the new amap will break that.
			 *
			 * 2. if the old entry has an amap and a non-zero
			 *    wire count then we are going to have to call
			 *    amap_cow_now to avoid page faults in the 
			 *    parent process.   since amap_cow_now requires
			 *    "needs_copy" to be clear we might as well
			 *    clear it here as well.
			 *
			 */
d4638 18
a4655 1
			if (old_entry->aref.ar_amap != NULL) {
d4657 7
a4663 9
			  if ((amap_flags(old_entry->aref.ar_amap) & 
			       AMAP_SHARED) != 0 ||
			      VM_MAPENT_ISWIRED(old_entry)) {

			    amap_copy(new_map, new_entry, M_WAITOK, FALSE,
				      0, 0);
			    /* XXXCDC: M_WAITOK ... ok? */
			  }
			}
d4665 13
a4677 10
			/*
			 * if the parent's entry is wired down, then the
			 * parent process does not want page faults on
			 * access to that memory.  this means that we
			 * cannot do copy-on-write because we can't write
			 * protect the old entry.   in this case we
			 * resolve all copy-on-write faults now, using
			 * amap_cow_now.   note that we have already
			 * allocated any needed amap (above).
			 */
d4679 1
a4679 1
			if (VM_MAPENT_ISWIRED(old_entry)) {
d4681 11
a4691 80
			  /* 
			   * resolve all copy-on-write faults now
			   * (note that there is nothing to do if 
			   * the old mapping does not have an amap).
			   * XXX: is it worthwhile to bother with pmap_copy
			   * in this case?
			   */
			  if (old_entry->aref.ar_amap)
			    amap_cow_now(new_map, new_entry);

			} else { 

			  /*
			   * setup mappings to trigger copy-on-write faults
			   * we must write-protect the parent if it has
			   * an amap and it is not already "needs_copy"...
			   * if it is already "needs_copy" then the parent
			   * has already been write-protected by a previous
			   * fork operation.
			   *
			   * if we do not write-protect the parent, then
			   * we must be sure to write-protect the child
			   * after the pmap_copy() operation.
			   *
			   * XXX: pmap_copy should have some way of telling
			   * us that it didn't do anything so we can avoid
			   * calling pmap_protect needlessly.
			   */

			  if (old_entry->aref.ar_amap) {

			    if (!UVM_ET_ISNEEDSCOPY(old_entry)) {
			      if (old_entry->max_protection & VM_PROT_WRITE) {
				pmap_protect(old_map->pmap,
					     old_entry->start,
					     old_entry->end,
					     old_entry->protection &
					     ~VM_PROT_WRITE);
			        pmap_update(old_map->pmap);

			      }
			      old_entry->etype |= UVM_ET_NEEDSCOPY;
			    }

			    /*
			     * parent must now be write-protected
			     */
			    protect_child = FALSE;
			  } else {

			    /*
			     * we only need to protect the child if the 
			     * parent has write access.
			     */
			    if (old_entry->max_protection & VM_PROT_WRITE)
			      protect_child = TRUE;
			    else
			      protect_child = FALSE;

			  }

			  /*
			   * copy the mappings
			   * XXX: need a way to tell if this does anything
			   */

			  pmap_copy(new_pmap, old_map->pmap,
				    new_entry->start,
				    (old_entry->end - old_entry->start),
				    old_entry->start);

			  /*
			   * protect the child's mappings if necessary
			   */
			  if (protect_child) {
			    pmap_protect(new_pmap, new_entry->start,
					 new_entry->end, 
					 new_entry->protection & 
					          ~VM_PROT_WRITE);
			  }
d4693 9
a4701 4
			}
			break;
		}  /* end of switch statement */
		old_entry = old_entry->next;
d4704 8
a4711 2
	new_map->size = old_map->size;
	vm_map_unlock(old_map); 
d4713 1
a4713 4
#ifdef SYSVSHM
	if (vm1->vm_shm)
		shmfork(vm1, vm2);
#endif
d4715 13
a4727 2
#ifdef PMAP_FORK
	pmap_fork(vm1->vm_map.pmap, vm2->vm_map.pmap);
d4730 20
a4749 3
	UVMHIST_LOG(maphist,"<- done",0,0,0,0);
	return(vm2);    
}
d4751 6
a4756 1
#if defined(DDB)
d4758 6
a4763 3
/*
 * DDB hooks
 */
d4766 6
a4771 1
 * uvm_map_printit: actually prints the map
d4773 4
d4778 2
a4779 5
void
uvm_map_printit(struct vm_map *map, boolean_t full,
    int (*pr)(const char *, ...))
{
	struct vm_map_entry *entry;
d4781 11
a4791 7
	(*pr)("MAP %p: [0x%lx->0x%lx]\n", map, map->min_offset,map->max_offset);
	(*pr)("\t#ent=%d, sz=%u, ref=%d, version=%u, flags=0x%x\n",
	    map->nentries, map->size, map->ref_count, map->timestamp,
	    map->flags);
#ifdef pmap_resident_count
	(*pr)("\tpmap=%p(resident=%d)\n", map->pmap, 
	    pmap_resident_count(map->pmap));
d4793 1
a4793 2
	/* XXXCDC: this should be required ... */
	(*pr)("\tpmap=%p(resident=<<NOT SUPPORTED!!!>>)\n", map->pmap);
a4794 16
	if (!full)
		return;
	for (entry = map->header.next; entry != &map->header;
	    entry = entry->next) {
		(*pr)(" - %p: 0x%lx->0x%lx: obj=%p/0x%llx, amap=%p/%d\n",
		    entry, entry->start, entry->end, entry->object.uvm_obj,
		    (long long)entry->offset, entry->aref.ar_amap,
		    entry->aref.ar_pageoff);
		(*pr)(
		    "\tsubmap=%c, cow=%c, nc=%c, prot(max)=%d/%d, inh=%d, "
		    "wc=%d, adv=%d\n",
		    (entry->etype & UVM_ET_SUBMAP) ? 'T' : 'F',
		    (entry->etype & UVM_ET_COPYONWRITE) ? 'T' : 'F', 
		    (entry->etype & UVM_ET_NEEDSCOPY) ? 'T' : 'F',
		    entry->protection, entry->max_protection,
		    entry->inheritance, entry->wired_count, entry->advice);
a4795 1
} 
d4797 6
a4802 3
/*
 * uvm_object_printit: actually prints the object
 */
d4804 2
a4805 5
void
uvm_object_printit(uobj, full, pr)
	struct uvm_object *uobj;
	boolean_t full;
	int (*pr)(const char *, ...);
d4807 1
a4807 2
	struct vm_page *pg;
	int cnt = 0;
d4809 8
a4816 6
	(*pr)("OBJECT %p: pgops=%p, npages=%d, ",
	    uobj, uobj->pgops, uobj->uo_npages);
	if (UVM_OBJ_IS_KERN_OBJECT(uobj))
		(*pr)("refs=<SYSTEM>\n");
	else
		(*pr)("refs=%d\n", uobj->uo_refs);
d4818 5
a4822 2
	if (!full) {
		return;
d4824 14
a4837 10
	(*pr)("  PAGES <pg,offset>:\n  ");
	RB_FOREACH(pg, uvm_objtree, &uobj->memt) {
		(*pr)("<%p,0x%llx> ", pg, (long long)pg->offset);
		if ((cnt % 3) == 2) {
			(*pr)("\n  ");
		}
		cnt++;
	}
	if ((cnt % 3) != 2) {
		(*pr)("\n");
a4838 1
} 
d4840 5
a4844 3
/*
 * uvm_page_printit: actually print the page
 */
d4846 9
a4854 4
static const char page_flagbits[] =
	"\20\1BUSY\2WANTED\3TABLED\4CLEAN\5CLEANCHK\6RELEASED\7FAKE\10RDONLY"
	"\11ZERO\15PAGER1\20FREE\21INACTIVE\22ACTIVE\24ENCRYPT\30PMAP0"
	"\31PMAP1\32PMAP2\33PMAP3";
d4857 1
a4857 4
uvm_page_printit(pg, full, pr)
	struct vm_page *pg;
	boolean_t full;
	int (*pr)(const char *, ...);
d4859 6
a4864 3
	struct vm_page *tpg;
	struct uvm_object *uobj;
	struct pglist *pgl;
d4866 9
a4874 15
	(*pr)("PAGE %p:\n", pg);
	(*pr)("  flags=%b, vers=%d, wire_count=%d, pa=0x%llx\n",
	    pg->pg_flags, page_flagbits, pg->pg_version, pg->wire_count,
	    (long long)pg->phys_addr);
	(*pr)("  uobject=%p, uanon=%p, offset=0x%llx loan_count=%d\n",
	    pg->uobject, pg->uanon, (long long)pg->offset, pg->loan_count);
#if defined(UVM_PAGE_TRKOWN)
	if (pg->pg_flags & PG_BUSY)
		(*pr)("  owning process = %d, tag=%s\n",
		    pg->owner, pg->owner_tag);
	else
		(*pr)("  page not busy, no owner\n");
#else
	(*pr)("  [page ownership tracking disabled]\n");
#endif
d4876 10
a4885 2
	if (!full)
		return;
d4887 9
a4895 23
	/* cross-verify object/anon */
	if ((pg->pg_flags & PQ_FREE) == 0) {
		if (pg->pg_flags & PQ_ANON) {
			if (pg->uanon == NULL || pg->uanon->an_page != pg)
			    (*pr)("  >>> ANON DOES NOT POINT HERE <<< (%p)\n",
				(pg->uanon) ? pg->uanon->an_page : NULL);
			else
				(*pr)("  anon backpointer is OK\n");
		} else {
			uobj = pg->uobject;
			if (uobj) {
				(*pr)("  checking object list\n");
				RB_FOREACH(tpg, uvm_objtree, &uobj->memt) {
					if (tpg == pg) {
						break;
					}
				}
				if (tpg)
					(*pr)("  page found on object list\n");
				else
			(*pr)("  >>> PAGE NOT FOUND ON OBJECT LIST! <<<\n");
			}
		}
d4897 3
d4901 10
a4910 15
	/* cross-verify page queue */
	if (pg->pg_flags & PQ_FREE) {
		if (uvm_pmr_isfree(pg))
			printf("  page found in uvm_pmemrange\n");
		else
			printf("  >>> page not found in uvm_pmemrange <<<\n");
		pgl = NULL;
	} else if (pg->pg_flags & PQ_INACTIVE) {
		pgl = (pg->pg_flags & PQ_SWAPBACKED) ?
		    &uvm.page_inactive_swp : &uvm.page_inactive_obj;
	} else if (pg->pg_flags & PQ_ACTIVE) {
		pgl = &uvm.page_active;
 	} else {
		pgl = NULL;
	}
d4912 4
a4915 12
	if (pgl) {
		(*pr)("  checking pageq list\n");
		TAILQ_FOREACH(tpg, pgl, pageq) {
			if (tpg == pg) {
				break;
			}
		}
		if (tpg)
			(*pr)("  page found on pageq list\n");
		else
			(*pr)("  >>> PAGE NOT FOUND ON PAGEQ LIST! <<<\n");
	}
d4917 5
a4921 1
#endif
@


1.134
log
@Put back the change of pool and malloc into the new km_alloc(9) api.
The problems during the hackathon were not caused by this (most likely).

prodded by deraadt@@ and beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.133 2011/04/06 15:52:13 art Exp $	*/
d746 8
@


1.133
log
@Backout the uvm_km_getpage -> km_alloc conversion. Weird things are happening
and we aren't sure what's causing them.

shouted oks by many before I even built a kernel with the diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.132 2011/04/05 01:28:05 art Exp $	*/
d399 1
a399 1
	int slowdown, pool_flags;
d411 2
a412 1
			ne = uvm_km_getpage(0, &slowdown);
@


1.132
log
@ - Change pool constraints to use kmem_pa_mode instead of uvm_constraint_range
 - Use km_alloc for all backend allocations in pools.
 - Use km_alloc for the emergmency kentry allocations in uvm_mapent_alloc
 - Garbage collect uvm_km_getpage, uvm_km_getpage_pla and uvm_km_putpage

ariane@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.131 2010/12/24 21:49:04 tedu Exp $	*/
d399 1
a399 1
	int pool_flags;
d411 1
a411 2
			ne = km_alloc(PAGE_SIZE, &kv_page, &kp_dirty,
			    &kd_nowait);
@


1.131
log
@add a param to uvm_map_hint to not skip over the heap, and use it as a last
resort if mmap fails otherwise to enable more complete address space
utilization.  tested for a while with no ill effects.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.130 2010/12/15 04:59:52 tedu Exp $	*/
d399 1
a399 1
	int slowdown, pool_flags;
d411 2
a412 1
			ne = uvm_km_getpage(0, &slowdown);
@


1.130
log
@add a BRKSIZ define and use it for the heap gap constant, decoupling
heap gap from max data size.  nothing else changes yet.  ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.129 2010/12/06 20:57:19 miod Exp $	*/
d1244 1
a1244 1
uvm_map_hint(struct proc *p, vm_prot_t prot)
d1261 3
a1263 1
	addr = (vaddr_t)p->p_vmspace->vm_daddr + BRKSIZ;
@


1.129
log
@Change the signature of PMAP_PREFER from void PMAP_PREFER(..., vaddr_t *) to
vaddr_t PMAP_PREFER(..., vaddr_t). This allows better compiler optimization
when the function is inlined, and avoids accessing memory on architectures
when we can pass function arguments in registers.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.128 2010/09/29 18:04:33 thib Exp $	*/
a1259 4
	addr = (vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ;
#if !defined(__vax__)
	addr += arc4random() & (MIN((256 * 1024 * 1024), MAXDSIZ) - 1);
#else
d1262 2
@


1.128
log
@Band-aid to prevent uvm_mapent_alloc() sleeping with the pseg mutex held
(as this trips assertwaitok() in pool_get()).

This should get revisited soon.

"Commit it!" from many, as people like to be able to hit swap
without havoc.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.127 2010/06/17 16:11:20 miod Exp $	*/
d1184 1
a1184 1
		PMAP_PREFER(uoffset, &hint);
d1461 1
a1461 1
			PMAP_PREFER(uoffset, &hint);
@


1.127
log
@aligment -> alignment
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.126 2010/04/26 05:48:19 deraadt Exp $	*/
d202 1
a202 1
struct vm_map_entry	*uvm_mapent_alloc(struct vm_map *);
d395 1
a395 1
uvm_mapent_alloc(struct vm_map *map)
d399 1
a399 1
	int slowdown;
d402 4
d433 3
a435 1
		me = pool_get(&uvm_map_entry_kmem_pool, PR_WAITOK);
d439 3
a441 1
		me = pool_get(&uvm_map_entry_pool, PR_WAITOK);
d445 1
d615 1
a615 1
	new_entry = uvm_mapent_alloc(map);
d667 1
a667 1
	new_entry = uvm_mapent_alloc(map);
d952 5
a956 1
	new_entry = uvm_mapent_alloc(map);
d1015 18
a1032 16
		guard_entry = uvm_mapent_alloc(map);
		guard_entry->start = new_entry->end;
		guard_entry->end = guard_entry->start + PAGE_SIZE;
		guard_entry->object.uvm_obj = uobj;
		guard_entry->offset = uoffset;
		guard_entry->etype = MAP_ET_KVAGUARD;
		guard_entry->protection = prot;
		guard_entry->max_protection = maxprot;
		guard_entry->inheritance = inherit;
		guard_entry->wired_count = 0;
		guard_entry->advice = advice;
		guard_entry->aref.ar_pageoff = 0;
		guard_entry->aref.ar_amap = NULL;
		uvm_map_entry_link(map, new_entry, guard_entry);
		map->size += PAGE_SIZE;
		kva_guardpages++;
d2111 1
a2111 1
		newentry = uvm_mapent_alloc(dstmap);
d3640 1
a3640 1
			new_entry = uvm_mapent_alloc(new_map);
d3687 1
a3687 1
			new_entry = uvm_mapent_alloc(new_map);
@


1.126
log
@cut down simple locks (so simple that they don't even lock) to the point
where there is almost nothing left to them, so that we can continue getting
rid of them
ok oga
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.125 2010/04/23 04:49:46 tedu Exp $	*/
d1152 1
a1152 1
 * space before the vm_map_entry after.  Takes aligment and
@


1.125
log
@remove unnecessary temp_entry variables and hoist simple check out of a loop.
no functional change.  from Anton Maksimenkov
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.124 2010/04/22 19:02:55 oga Exp $	*/
d3911 2
a3912 2
	(*pr)("OBJECT %p: locked=%d, pgops=%p, npages=%d, ",
	    uobj, uobj->vmobjlock.lock_data, uobj->pgops, uobj->uo_npages);
@


1.124
log
@Committing on behalf or ariane@@.

recommit pmemrange:
        physmem allocator: change the view of free memory from single
        free pages to free ranges.  Classify memory based on region with
        associated use-counter (which is used to construct a priority
        list of where to allocate memory).

	Based on code from tedu@@, help from many.

Useable now that bugs have been found and fixed in most architecture's
pmap.c

ok by everyone who has done a pmap or uvm commit in the last year.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.123 2009/08/28 00:40:03 ariane Exp $	*/
d2476 1
a2476 1
	struct vm_map_entry *entry, *temp_entry;
d2495 1
a2495 2
	if (uvm_map_lookup_entry(map, start, &temp_entry)) {
		entry = temp_entry;
d2498 1
a2498 1
		entry = temp_entry->next;
d2521 1
a2521 1
	struct vm_map_entry *entry, *temp_entry;
d2526 11
d2539 1
a2539 2
	if (uvm_map_lookup_entry(map, start, &temp_entry)) {
		entry = temp_entry;
d2542 1
a2542 1
		entry = temp_entry->next;
a2551 12
		switch (new_advice) {
		case MADV_NORMAL:
		case MADV_RANDOM:
		case MADV_SEQUENTIAL:
			/* nothing special here */
			break;

		default:
			vm_map_unlock(map);
			UVMHIST_LOG(maphist,"<- done (INVALID ARG)",0,0,0,0);
			return (EINVAL);
		}
@


1.123
log
@kva_guardpages: make guard pages separate map entries
- fixes ps(1)
- fixes kva deadbeef entries
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.122 2009/08/24 22:45:29 miod Exp $	*/
d4002 5
a4006 3
		int fl = uvm_page_lookup_freelist(pg);
		pgl = &uvm.page_free[fl].pgfl_queues[((pg)->pg_flags & PG_ZERO) ?
		    PGFL_ZEROS : PGFL_UNKNOWN];
@


1.122
log
@Properly check for VM_MAX_KERNEL_ADDRESS wraparound in the guard pages code.
ok ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.121 2009/08/13 20:40:13 ariane Exp $	*/
d205 7
d722 3
d738 1
a738 1
	if (map == kernel_map) {
d808 2
a809 1
		if (map == kernel_map && uvm_maxkaddr < (*startp + size))
d938 5
d997 24
a1499 5
#ifdef KVA_GUARDPAGES
	if (map == kernel_map)
		end += PAGE_SIZE;	/* Add guardpage. */
#endif

d1614 6
d1701 20
@


1.121
log
@Enable guard pages and slow recycling of kva.
Debugging aid, hiding in "option KVA_GUARDPAGES" in config.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.120 2009/08/06 15:28:14 oga Exp $	*/
d745 1
a745 1
		if (VM_MAX_KERNEL_ADDRESS < (kva_guardstart + size)) {
@


1.120
log
@reintroduce the uvm_tree commit.

Now instead of the global object hashtable, we have a per object tree.

Testing shows no performance difference and a slight code shrink. OTOH when
locking is more fine grained this should be faster due to lock contention on
uvm.hashlock.

ok thib@@, art@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.119 2009/07/25 12:55:40 miod Exp $	*/
d727 32
d1459 6
@


1.119
log
@Add an extra argument to uvm_unmap_remove(), for the caller to tell it
whether removing holes or parts of them is allowed or not.
Only allow hole removal in uvmspace_free(), when tearing the vmspace down.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.118 2009/06/17 00:13:59 oga Exp $	*/
d3826 1
a3826 3
	for (pg = TAILQ_FIRST(&uobj->memq);
	     pg != NULL;
	     pg = TAILQ_NEXT(pg, listq), cnt++) {
d3831 1
d3888 1
a3888 1
				TAILQ_FOREACH(tpg, &uobj->memq, listq) {
@


1.118
log
@date based reversion of uvm to the 4th May.

More backouts in line with previous ones, this appears to bring us back to a
stable condition.

A machine forced to 64mb of ram cycled 10GB through swap with this diff
and is still running as I type this. Other tests by ariane@@ and thib@@
also seem to show that it's alright.

ok deraadt@@, thib@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.110 2009/05/02 12:54:42 oga Exp $	*/
d1432 1
a1432 1
	uvm_unmap_remove(map, start, end, &dead_entries, p);
d1457 1
a1457 1
    struct vm_map_entry **entry_list, struct proc *p)
d1542 4
a1545 1
			/* nothing to do! */
d3360 1
a3360 1
			    &dead_entries, NULL);
@


1.117
log
@Backout pmemrange (which to most people is more well known as physmem
allocator).

"i can't see any obvious problems" oga
@
text
@@


1.116
log
@Backout all changes to uvm after pmemrange (which will be backed out
separately).

a change at or just before the hackathon has either exposed or added a
very very nasty memory corruption bug that is giving us hell right now.
So in the interest of kernel stability these diffs are being backed out
until such a time as that corruption bug has been found and squashed,
then the ones that are proven good may slowly return.

a quick hitlist of the main commits this backs out:

mine:
uvm_objwire
the lock change in uvm_swap.c
using trees for uvm objects instead of the hash
removing the pgo_releasepg callback.

art@@'s:
putting pmap_page_protect(VM_PROT_NONE) in uvm_pagedeactivate() since
all callers called that just prior anyway.

ok beck@@, ariane@@.

prompted by deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.111 2009/06/01 17:42:33 ariane Exp $	*/
d3825 1
a3825 1
	     pg = TAILQ_NEXT(pg, fq.queues.listq), cnt++) {
d3886 1
a3886 2
				TAILQ_FOREACH(tpg, &uobj->memq,
				    fq.queues.listq) {
d3901 3
a3903 5
		if (uvm_pmr_isfree(pg))
			printf("  page found in uvm_pmemrange\n");
		else
			printf("  >>> page not found in uvm_pmemrange <<<\n");
		pgl = NULL;
@


1.115
log
@Noone else ever saw this diff except oga and ariane, so that is an
utter failure of process.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.113 2009/06/06 17:46:44 art Exp $	*/
d3061 9
a3069 1
				/* Deactivate the page. */
d3823 3
a3825 2
	RB_FOREACH(pg, uobj_pgs, &uobj->memt) {
		cnt++;
d3886 2
a3887 1
				RB_FOREACH(pg, uobj_pgs, &uobj->memt) {
@


1.114
log
@correct loop in debug code to not use an uninitialised page. it was
ass-backwards.

afaik this was found by the LLVM CLang static analyser.

ok ariane@@ a couple of days ago.
@
text
@d3837 4
a3840 2
uvm_page_printit(struct vm_page *pg, boolean_t full,
    int (*pr)(const char *, ...))
d3877 1
a3877 1
				RB_FOREACH(tpg, uobj_pgs, &uobj->memt) {
@


1.113
log
@Since all callers of uvm_pagedeactivate did pmap_page_protect(.., VM_PROT_NONE)
just move that into uvm_pagedeactivate.

oga@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.112 2009/06/02 23:00:19 oga Exp $	*/
d3837 2
a3838 4
uvm_page_printit(pg, full, pr)
	struct vm_page *pg;
	boolean_t full;
	int (*pr)(const char *, ...);
d3875 1
a3875 1
				RB_FOREACH(pg, uobj_pgs, &uobj->memt) {
@


1.112
log
@Instead of the global hash table with the terrible hashfunction and a
global lock, switch the uvm object pages to being kept in a per-object
RB_TREE. Right now this is approximately the same speed, but cleaner.
When biglock usage is reduced this will improve concurrency due to lock
contention..

ok beck@@ art@@. Thanks to jasper for the speed testing.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.111 2009/06/01 17:42:33 ariane Exp $	*/
d3061 1
a3061 9
#ifdef UBC
				/* ...and deactivate the page. */
				pmap_clear_reference(pg);
#else
				/* zap all mappings for the page. */
				pmap_page_protect(pg, VM_PROT_NONE);

				/* ...and deactivate the page. */
#endif
@


1.111
log
@physmem allocator: change the view of free memory from single free pages
to free ranges.
Classify memory based on region with associated use-counter (which is used
to construct a priority list of where to allocate memory).

Based on code from tedu@@, help from many.
Ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.110 2009/05/02 12:54:42 oga Exp $	*/
d3823 2
a3824 3
	for (pg = TAILQ_FIRST(&uobj->memq);
	     pg != NULL;
	     pg = TAILQ_NEXT(pg, fq.queues.listq), cnt++) {
d3885 1
a3885 2
				TAILQ_FOREACH(tpg, &uobj->memq,
				    fq.queues.listq) {
@


1.110
log
@a few more memset changes.

two cases of pool_get() + memset(0) -> pool_get(,,,PR_ZERO)
1.5 cases of global variables are already zeroed, so don't zero them.

ok ariane@@, comments on stuff i'd missed from blambert@@ and cnst@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.109 2009/03/25 20:00:18 oga Exp $	*/
d3825 1
a3825 1
	     pg = TAILQ_NEXT(pg, listq), cnt++) {
d3886 2
a3887 1
				TAILQ_FOREACH(tpg, &uobj->memq, listq) {
d3902 5
a3906 3
		int fl = uvm_page_lookup_freelist(pg);
		pgl = &uvm.page_free[fl].pgfl_queues[((pg)->pg_flags & PG_ZERO) ?
		    PGFL_ZEROS : PGFL_UNKNOWN];
@


1.109
log
@Move all of the pseudo-inline functions in uvm into C files.

By pseudo-inline, I mean that if a certain macro was defined, they would
be inlined. However, no architecture defines that, and none has for a
very very long time. Therefore mainly this just makes the code a damned
sight easier to read. Some k&r -> ansi declarations while I'm in there.

"just commit it" art@@. ok weingart@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.108 2008/11/10 18:11:59 oga Exp $	*/
d3194 1
a3194 1
	vm = pool_get(&uvm_vmspace_pool, PR_WAITOK);
a3210 2

	memset(vm, 0, sizeof(*vm));
@


1.108
log
@typo: be -> by in comment

``of course'' deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.107 2008/11/04 21:37:06 deraadt Exp $	*/
a87 1
#define UVM_MAP
d1410 33
d3368 81
@


1.107
log
@uvmspace_unshare() is never used; ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.106 2008/10/23 23:54:02 tedu Exp $	*/
d1080 1
a1080 1
 * Checks if address pointed to be phint fits into the empty
@


1.106
log
@a better fix for the "uvm_km thread runs out of memory" problem.

add a new arg to the backend so it can tell pool to slow down.  when we get
this flag, yield *after* putting the page in the pool's free list.  whatever
we do, don't let the thread sleep.

this makes things better by still letting the thread run when a huge pf
request comes in, but without artificially increasing pressure on the backend
by eating pages without feeding them forward.

ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.105 2008/10/08 08:41:19 art Exp $	*/
a3210 26
}

/*
 * uvmspace_unshare: ensure that process "p" has its own, unshared, vmspace
 *
 * - XXX: no locking on vmspace
 */

void
uvmspace_unshare(p)
	struct proc *p; 
{
	struct vmspace *nvm, *ovm = p->p_vmspace;

	if (ovm->vm_refcnt == 1)
		/* nothing to do: vmspace isn't shared in the first place */
		return;

	/* make a new vmspace, still holding old one */
	nvm = uvmspace_fork(ovm);

	pmap_deactivate(p);		/* unbind old vmspace */
	p->p_vmspace = nvm; 
	pmap_activate(p);		/* switch to new vmspace */

	uvmspace_free(ovm);		/* drop reference to old vmspace */
@


1.105
log
@Don't extend amaps beyond what their supposed maximum. This code path is
not taken anymore, but it doesn't hurt to be correct.

from NetBSD, through mickey in pr 5812
prodded by otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.104 2008/09/23 13:25:46 art Exp $	*/
d393 1
d401 1
a401 1
			ne = uvm_km_getpage(0);
@


1.104
log
@Do not merge userland map entries.

 Imagine lots of random small mappings (think malloc(3)) and sometimes
 one large mapping (network buffer). If we've filled up our address space
 enough, the random address picked for the large allocation is likely to
 be overlapping an existing small allocation, so we'll do a linear scan
 to find the next free address. That next free address is likely to
 be just after a small allocation. Those two map entires get merged.
 If we now allocate an amap for the merged map entry, it will be large.
 When we later free the large allocation the amap is not truncated. All
 these are design decisions that made sense for sbrk, but with random
 allocations and malloc that actually returns memory, this really hurt us.

 This is the reason why certain processes like apache and sendmail could
 eat more than 10 times as much amap memory as they needed, eventually
 hitting the malloc limit and hanging or running the machine out of
 kmem_map and crashing.

otto@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.103 2008/07/25 12:05:04 art Exp $	*/
d850 2
a851 4
			if (error) {
				vm_map_unlock(map);
				return (error);
			}
@


1.103
log
@Correct printing of the pg_flags for ddb.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.102 2008/07/25 12:02:09 art Exp $	*/
d101 1
d542 1
d836 9
@


1.102
log
@some splassert paranoia.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.101 2008/07/18 16:40:17 kurt Exp $	*/
d3747 2
a3748 3
	"\11ZERO\15PAGER1";
static const char page_pqflagbits[] =
	"\20\1FREE\2INACTIVE\3ACTIVE\4LAUNDRY\5ANON\6AOBJ";
a3758 2
	char pgbuf[128];
	char pqbuf[128];
d3761 2
a3762 4
	snprintf(pgbuf, sizeof(pgbuf), "%b", pg->pg_flags, page_flagbits);
	snprintf(pqbuf, sizeof(pqbuf), "%b", pg->pg_flags, page_pqflagbits);
	(*pr)("  flags=%s, pg_flags=%s, vers=%d, wire_count=%d, pa=0x%llx\n",
	    pgbuf, pqbuf, pg->pg_version, pg->wire_count,
@


1.101
log
@Add new uvm function called uvm_map_pie() which takes align as a
parameter and returns an aligned random load address for position
independent executables to use. This also adds three new vmparam.h
defines to specify the maximum address, minimum address and minimum
allowed alignment for uvm_map_pie() to use. The PIE address range
for i386 was carefully selected to work well within the i386 W^X
framework.

With much help and feedback from weingart@@.
okay weingart@@, miod@@, kettenis@@, drahn@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.100 2008/06/09 20:30:23 miod Exp $	*/
d729 2
d1431 2
@


1.100
log
@Define a new flag, UVM_FLAG_HOLE, for uvm_map to create a vm_map_entry of
a new etype, UVM_ET_HOLE, meaning it has no backend.

UVM_ET_HOLE entries (which should be created as UVM_PROT_NONE and with
UVM_FLAG_NOMERGE and UVM_FLAG_HOLE) are skipped in uvm_unmap_remove(), so
that pmap_{k,}remove() is not called on the entry.

This is intended to save time, and behave better, on pmaps with MMU holes
at process exit time.

ok art@@, kettenis@@ provided feedback as well.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.99 2007/09/15 10:10:37 martin Exp $	*/
d1100 39
@


1.99
log
@replace ctob and btoc with ptoa and atop respectively

help and ok miod@@ thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.98 2007/09/10 18:49:45 miod Exp $	*/
d900 2
d1456 3
a1458 1
		if (map->flags & VM_MAP_INTRSAFE) {
@


1.98
log
@Introduce a md pmap hook, pmap_remove_holes(), which is supposed to mark
the holes a MMU may have from a given vm_map. This will be automagically
invoked for newly created vmspaces.

On platforms with MMU holes (e.g. sun4, sun4c and vax), this prevents
mmap(2) hints which would end up being in the hole to be accepted as valid,
causing unexpected signals when the process tries to access the hole
(since pmap can not fill the hole anyway).

Unfortunately, the logic mmap() uses to pick a valid address for anonymous
mappings needs work, as it will only try to find an address higher than the
hint, which causes all mmap() with a hint in the hole to fail on vax. This
will be improved later.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.97 2007/07/18 17:00:20 art Exp $	*/
d858 1
a858 1
			p->p_vmspace->vm_dused += btoc(size);
d925 1
a925 1
		p->p_vmspace->vm_dused += btoc(size);
d1440 1
a1440 1
			p->p_vmspace->vm_dused -= btoc(len);
@


1.97
log
@Indentation nit.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.96 2007/06/18 21:51:15 pedro Exp $	*/
d3099 2
a3100 1
uvmspace_alloc(vaddr_t min, vaddr_t max, int pageable)
d3106 1
a3106 1
	uvmspace_init(vm, NULL, min, max, pageable);
d3118 2
a3119 5
uvmspace_init(vm, pmap, min, max, pageable)
	struct vmspace *vm;
	struct pmap *pmap;
	vaddr_t min, max;
	boolean_t pageable;
d3134 4
d3229 5
d3254 1
a3254 1
			 (map->flags & VM_MAP_PAGEABLE) ? TRUE : FALSE);
d3332 1
a3332 1
		      (old_map->flags & VM_MAP_PAGEABLE) ? TRUE : FALSE);
@


1.96
log
@Bring back Mickey's UVM anon change. Testing by thib@@, beck@@ and
ckuethe@@ for a while. Okay beck@@, "it is good timing" deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.95 2007/06/13 13:32:26 art Exp $	*/
d1924 1
a1924 1
				newentry->offset = entry->offset + fudge;
@


1.95
log
@Remove some statics and inlines (mostly to get a readable profile output).

"reads ok" dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.94 2007/06/01 20:10:04 tedu Exp $	*/
d2930 1
a2930 1
			pg = anon->u.an_page;
d3733 1
a3733 1
			if (pg->uanon == NULL || pg->uanon->u.an_page != pg)
d3735 1
a3735 1
				(pg->uanon) ? pg->uanon->u.an_page : NULL);
@


1.94
log
@set hiwat mark for some of the more popular pools to reduce bouncing
ok art bob
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.93 2007/05/31 21:20:30 thib Exp $	*/
d195 5
a199 7
static void		uvm_mapent_copy(struct vm_map_entry *,
    struct vm_map_entry *);
static void		uvm_map_entry_unwire(struct vm_map *,
    struct vm_map_entry *);
static void		uvm_map_reference_amap(struct vm_map_entry *, int);
static void		uvm_map_unreference_amap(struct vm_map_entry *, int);
int			uvm_map_spacefits(struct vm_map *, vaddr_t *, vsize_t,
d216 2
a217 1
static vsize_t uvm_rb_subtree_space(struct vm_map_entry *);
d256 1
a256 1
static vsize_t
d277 1
a277 1
static void
d469 1
a469 1
static __inline void
d481 1
a481 2

static __inline void
d493 1
a493 1
static __inline void
d504 1
a504 1
static __inline void
@


1.93
log
@zap the vm_amap am_l simplelock, and amap_{lock/unlock} macros for
simple_{lock/unlock}.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.92 2007/04/27 18:01:49 art Exp $	*/
d566 1
@


1.92
log
@For uvm_map and uvm_unmap, splassert(IPL_NONE) for non-INTRSAFE maps.

miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.91 2007/04/27 16:23:49 art Exp $	*/
a2921 1
		amap_lock(amap);
a3013 1
		amap_unlock(amap);
d3015 1
a3015 1
 flush_object:
@


1.91
log
@Some indentation cleanup.
md5@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.90 2007/04/14 14:11:13 art Exp $	*/
d728 3
d1386 3
@


1.90
log
@Clean up.
 - ansi-fy
 - use struct vm_map * and struct vm_map_entry * instead of _t types
 - fix some indentation problems.

miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.89 2007/04/13 18:57:49 art Exp $	*/
d473 2
a474 2

	memcpy(dst, src, ((char *)&src->uvm_map_entry_stop_copy) - ((char *)src));
d498 2
a499 2
    amap_ref(entry->aref.ar_amap, entry->aref.ar_pageoff,
	     (entry->end - entry->start) >> PAGE_SHIFT, flags);
d509 2
a510 2
    amap_unref(entry->aref.ar_amap, entry->aref.ar_pageoff,
	     (entry->end - entry->start) >> PAGE_SHIFT, flags);
@


1.89
log
@While splitting flags and pqflags might have been a good idea in theory
to separate locking, on most modern machines this is not enough
since operations on short types touch other short types that share the
same word in memory.

Merge pg_flags and pqflags again and now use atomic operations to change
the flags. Also bump wire_count to an int and pg_version might go
int as well, just for alignment.

tested by many, many. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.88 2007/04/12 18:59:55 art Exp $	*/
d195 8
a202 7
static vm_map_entry_t	uvm_mapent_alloc(vm_map_t);
static void		uvm_mapent_copy(vm_map_entry_t,vm_map_entry_t);
static void		uvm_mapent_free(vm_map_entry_t);
static void		uvm_map_entry_unwire(vm_map_t, vm_map_entry_t);
static void		uvm_map_reference_amap(vm_map_entry_t, int);
static void		uvm_map_unreference_amap(vm_map_entry_t, int);
int			uvm_map_spacefits(vm_map_t, vaddr_t *, vsize_t, vm_map_entry_t, voff_t, vsize_t);
d216 1
a216 1
int _uvm_tree_sanity(vm_map_t map, const char *name);
d221 1
a221 1
uvm_compare(vm_map_entry_t a, vm_map_entry_t b)
d233 1
a233 1
uvm_rb_augment(vm_map_entry_t entry)
d245 1
a245 1
	vm_map_entry_t next;
d279 1
a279 1
uvm_rb_fixup(vm_map_t map, vm_map_entry_t entry)
d292 1
a292 1
	vm_map_entry_t tmp;
d308 1
a308 1
	vm_map_entry_t parent;
d326 1
a326 1
_uvm_tree_sanity(vm_map_t map, const char *name)
d328 1
a328 1
	vm_map_entry_t tmp, trtmp;
d471 1
a471 3
uvm_mapent_copy(src, dst)
	vm_map_entry_t src;
	vm_map_entry_t dst;
d484 1
a484 3
uvm_map_entry_unwire(map, entry)
	vm_map_t map;
	vm_map_entry_t entry;
d496 1
a496 3
uvm_map_reference_amap(entry, flags)
	vm_map_entry_t entry;
	int flags;
d507 1
a507 3
uvm_map_unreference_amap(entry, flags)
	vm_map_entry_t entry;
	int flags;
d516 1
a516 1
 * and init the static pool of vm_map_entry_t's for the kernel here.
d520 1
a520 1
uvm_map_init() 
d581 3
a583 4
void uvm_map_clip_start(map, entry, start)
	vm_map_t       map;
	vm_map_entry_t entry;
	vaddr_t    start;
d585 1
a585 1
	vm_map_entry_t new_entry;
d639 1
a639 4
uvm_map_clip_end(map, entry, end)
	vm_map_t	map;
	vm_map_entry_t	entry;
	vaddr_t	end;
d641 1
a641 1
	vm_map_entry_t	new_entry;
d709 3
a711 9
uvm_map_p(map, startp, size, uobj, uoffset, align, flags, p)
	vm_map_t map;
	vaddr_t *startp;	/* IN/OUT */
	vsize_t size;
	struct uvm_object *uobj;
	voff_t uoffset;
	vsize_t align;
	uvm_flag_t flags;
	struct proc *p;
d713 1
a713 1
	vm_map_entry_t prev_entry, new_entry;
d950 2
a951 4
uvm_map_lookup_entry(map, address, entry)
	vm_map_t	map;
	vaddr_t	address;
	vm_map_entry_t		*entry;		/* OUT */
d953 2
a954 2
	vm_map_entry_t		cur;
	vm_map_entry_t		last;
d1010 1
a1010 1
		vm_map_entry_t prev = &map->header;
d1070 2
a1071 2
uvm_map_spacefits(vm_map_t map, vaddr_t *phint, vsize_t length,
    vm_map_entry_t after, voff_t uoffset, vsize_t align)
d1142 4
a1145 10
vm_map_entry_t
uvm_map_findspace(map, hint, length, result, uobj, uoffset, align, flags)
	vm_map_t map;
	vaddr_t hint;
	vsize_t length;
	vaddr_t *result; /* OUT */
	struct uvm_object *uobj;
	voff_t uoffset;
	vsize_t align;
	int flags;
d1147 2
a1148 2
	vm_map_entry_t entry, next, tmp;
	vm_map_entry_t child, prev = NULL;
d1369 2
a1370 5
uvm_unmap_remove(map, start, end, entry_list, p)
	vm_map_t map;
	vaddr_t start,end;
	vm_map_entry_t *entry_list;	/* OUT */
	struct proc *p;
d1372 1
a1372 1
	vm_map_entry_t entry, first_entry, next;
d1548 1
a1548 3
uvm_unmap_detach(first_entry, flags)
	vm_map_entry_t first_entry;
	int flags;
d1550 1
a1550 1
	vm_map_entry_t next_entry;
d1604 2
a1605 6
uvm_map_reserve(map, size, offset, align, raddr)
	vm_map_t map;
	vsize_t size;
	vaddr_t offset;	/* hint for pmap_prefer */
	vsize_t align;	/* alignment hint */
	vaddr_t *raddr;	/* IN:hint, OUT: reserved VA */
d1643 2
a1644 5
uvm_map_replace(map, start, end, newents, nnewents)
	struct vm_map *map;
	vaddr_t start, end;
	vm_map_entry_t newents;
	int nnewents;
d1646 1
a1646 1
	vm_map_entry_t oldent, last;
d1672 1
a1672 1
		vm_map_entry_t tmpent = newents;
d1726 1
a1726 1
			vm_map_entry_t tmp;
d1775 2
a1776 5
uvm_map_extract(srcmap, start, len, dstmap, dstaddrp, flags)
	vm_map_t srcmap, dstmap;
	vaddr_t start, *dstaddrp;
	vsize_t len;
	int flags;
d1780 2
a1781 2
	vm_map_entry_t chain, endchain, entry, orig_entry, newentry, deadentry;
	vm_map_entry_t oldentry;
d2114 2
a2115 3
uvm_map_submap(map, start, end, submap)
	vm_map_t map, submap;
	vaddr_t start, end;
d2117 1
a2117 1
	vm_map_entry_t entry;
d2324 1
a2324 5
uvm_map_advice(map, start, end, new_advice)
	vm_map_t map;
	vaddr_t start;
	vaddr_t end;
	int new_advice;
d2326 1
a2326 1
	vm_map_entry_t entry, temp_entry;
d2383 2
a2384 5
uvm_map_pageable(map, start, end, new_pageable, lockflags)
	vm_map_t map;
	vaddr_t start, end;
	boolean_t new_pageable;
	int lockflags;
d2386 1
a2386 1
	vm_map_entry_t entry, start_entry, failed_entry;
d2854 1
a2854 4
uvm_map_clean(map, start, end, flags)
	vm_map_t map;
	vaddr_t start, end;
	int flags;
d2856 1
a2856 1
	vm_map_entry_t current, entry;
d3049 14
a3062 16
uvm_map_checkprot(map, start, end, protection)
	vm_map_t       map;
	vaddr_t    start, end;
	vm_prot_t      protection;
{
	 vm_map_entry_t entry;
	 vm_map_entry_t tmp_entry;

	 if (!uvm_map_lookup_entry(map, start, &tmp_entry)) {
		 return(FALSE);
	 }
	 entry = tmp_entry;
	 while (start < end) {
		 if (entry == &map->header) {
			 return(FALSE);
		 }
d3068 3
a3070 3
		 if (start < entry->start) {
			 return(FALSE);
		 }
d3076 10
a3085 10
		 if ((entry->protection & protection) != protection) {
			 return(FALSE);
		 }

		 /* go to next entry */

		 start = entry->end;
		 entry = entry->next;
	 }
	 return(TRUE);
d3096 1
a3096 3
uvmspace_alloc(min, max, pageable)
	vaddr_t min, max;
	int pageable;
d3184 1
a3184 3
uvmspace_exec(p, start, end)
	struct proc *p;
	vaddr_t start, end;
d3187 1
a3187 1
	vm_map_t map = &ovm->vm_map;
d3265 1
a3265 2
uvmspace_free(vm)
	struct vmspace *vm;
d3267 1
a3267 1
	vm_map_entry_t dead_entries;
d3308 1
a3308 2
uvmspace_fork(vm1)
	struct vmspace *vm1;
d3311 4
a3314 4
	vm_map_t        old_map = &vm1->vm_map;
	vm_map_t        new_map;
	vm_map_entry_t  old_entry;
	vm_map_entry_t  new_entry;
d3612 2
a3613 4
uvm_map_printit(map, full, pr)
	vm_map_t map;
	boolean_t full;
	int (*pr)(const char *, ...);
d3615 1
a3615 1
	vm_map_entry_t entry;
@


1.88
log
@Unbreak compile with option UVMHIST after pg_flags changes.
from mickey
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.87 2007/04/11 12:10:42 art Exp $	*/
d3011 1
a3011 1
				if ((pg->pqflags & PQ_ANON) == 0) {
d3769 2
a3770 2
	snprintf(pqbuf, sizeof(pqbuf), "%b", pg->pqflags, page_pqflagbits);
	(*pr)("  flags=%s, pqflags=%s, vers=%d, wire_count=%d, pa=0x%llx\n",
d3789 2
a3790 2
	if ((pg->pqflags & PQ_FREE) == 0) {
		if (pg->pqflags & PQ_ANON) {
d3814 1
a3814 1
	if (pg->pqflags & PQ_FREE) {
d3818 2
a3819 2
	} else if (pg->pqflags & PQ_INACTIVE) {
		pgl = (pg->pqflags & PQ_SWAPBACKED) ?
d3821 1
a3821 1
	} else if (pg->pqflags & PQ_ACTIVE) {
@


1.87
log
@Instead of managing pages for intrsafe maps in special objects (aka.
kmem_object) just so that we can remove them, just use pmap_extract
to get the pages to free and simplify a lot of code to not deal with
the list of intrsafe maps, intrsafe objects, etc.

miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.86 2007/04/04 17:44:45 art Exp $	*/
d2302 1
a2302 1
	UVMHIST_LOG(maphist, "<- done, rv=%ld",rv,0,0,0);
d2871 1
a2871 1
		UVMHIST_LOG(maphist,"<- done (RV=%ld)", rv,0,0,0);
@


1.86
log
@Mechanically rename the "flags" and "version" fields in struct vm_page
to "pg_flags" and "pg_version", so that they are a bit easier to work with.
Whoever uses generic names like this for a popular struct obviously doesn't
read much code.

Most architectures compile and there are no functionality changes.

deraadt@@ ok ("if something fails to compile, we fix that by hand")
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.85 2007/03/27 16:13:45 art Exp $	*/
d1477 4
a1480 1
		if (UVM_ET_ISOBJ(entry) &&
d1519 4
a1522 12
			if (UVM_OBJ_IS_INTRSAFE_OBJECT(entry->object.uvm_obj)) {
				pmap_kremove(entry->start, len);
				uvm_km_pgremove_intrsafe(entry->object.uvm_obj,
				    entry->start - vm_map_min(kernel_map),
				    entry->end - vm_map_min(kernel_map));
			} else {
				pmap_remove(pmap_kernel(), entry->start,
				    entry->end);
				uvm_km_pgremove(entry->object.uvm_obj,
				    entry->start - vm_map_min(kernel_map),
				    entry->end - vm_map_min(kernel_map));
			}
@


1.85
log
@Clean up some return value handling now that we know that what's returned
is proper errnos.

millert@@ ok and some help
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.84 2007/03/26 08:43:34 art Exp $	*/
d3773 1
a3773 1
	snprintf(pgbuf, sizeof(pgbuf), "%b", pg->flags, page_flagbits);
d3776 1
a3776 1
	    pgbuf, pqbuf, pg->version, pg->wire_count,
d3781 1
a3781 1
	if (pg->flags & PG_BUSY)
d3821 1
a3821 1
		pgl = &uvm.page_free[fl].pgfl_queues[((pg)->flags & PG_ZERO) ?
@


1.84
log
@Rip out the KERN_ error codes.
ok otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.83 2007/03/25 11:31:07 art Exp $	*/
d2206 2
a2207 5
uvm_map_protect(map, start, end, new_prot, set_max)
	vm_map_t map;
	vaddr_t start, end;
	vm_prot_t new_prot;
	boolean_t set_max;
d2209 2
a2210 2
	vm_map_entry_t current, entry;
	int rv = 0;
d2232 1
a2232 1
			rv = EINVAL;
d2236 1
a2236 1
			rv = EACCES;
d2297 1
a2297 1
				rv = ENOMEM;
d2308 1
a2308 1
	return (rv);
d2323 2
a2324 5
uvm_map_inherit(map, start, end, new_inheritance)
	vm_map_t map;
	vaddr_t start;
	vaddr_t end;
	vm_inherit_t new_inheritance;
d2326 1
a2326 1
	vm_map_entry_t entry, temp_entry;
d2687 1
a2687 4
uvm_map_pageable_all(map, flags, limit)
	vm_map_t map;
	int flags;
	vsize_t limit;
d2689 1
a2689 1
	vm_map_entry_t entry, failed_entry;
d2691 1
a2691 1
	int rv;
d2826 3
a2828 3
	rv = 0;
	for (entry = map->header.next; entry != &map->header;
	     entry = entry->next) {
d2830 1
a2830 1
			rv = uvm_fault_wire(map, entry->start, entry->end,
a2831 8
			if (rv) {
				/*
				 * wiring failed.  break out of the loop.
				 * we'll clean up the map below, once we
				 * have a write lock again.
				 */
				break;
			}
d2835 1
a2835 1
	if (rv) {	/* failed? */
d2877 1
a2877 1
		return (rv);
@


1.83
log
@remove KERN_SUCCESS and use 0 instead.
eyeballed by miod@@ and pedro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.82 2006/07/31 11:51:29 mickey Exp $	*/
d752 1
a752 1
		return(KERN_PROTECTION_FAILURE);
d761 1
a761 1
			return(KERN_FAILURE);
d768 1
a768 1
		return (KERN_NO_SPACE);
d2187 1
a2187 1
		result = KERN_INVALID_ARGUMENT;
d2235 1
a2235 1
			rv = KERN_INVALID_ARGUMENT;
d2239 1
a2239 1
			rv = KERN_PROTECTION_FAILURE;
d2300 1
a2300 1
				rv = KERN_RESOURCE_SHORTAGE;
d2344 1
a2344 1
		return (KERN_INVALID_ARGUMENT);
d2413 1
a2413 1
			return (KERN_INVALID_ARGUMENT);
d2473 1
a2473 1
		return (KERN_INVALID_ADDRESS);
d2498 1
a2498 1
				return (KERN_INVALID_ARGUMENT);
d2589 1
a2589 1
			return (KERN_INVALID_ARGUMENT);
d2784 1
a2784 1
		return (KERN_NO_SPACE);		/* XXX overloaded */
d2792 1
a2792 1
		return (KERN_NO_SPACE);		/* XXX overloaded */
d2948 1
a2948 1
		return(KERN_INVALID_ADDRESS);
d2958 1
a2958 1
			return (KERN_INVALID_ARGUMENT);
d2963 1
a2963 1
			return (KERN_INVALID_ADDRESS);
d3105 1
a3105 1
				error = KERN_FAILURE;
@


1.82
log
@fix uvmhist #2: args are always u_long so fix missing %d and %x and no %ll; no change for normal code
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.81 2006/07/26 23:15:55 mickey Exp $	*/
d879 1
a879 1
		return (KERN_SUCCESS);
d955 1
a955 1
	return(KERN_SUCCESS);
d1661 1
a1661 1
	    UVM_ADV_RANDOM, UVM_FLAG_NOMERGE)) != KERN_SUCCESS) {
d2185 1
a2185 1
		result = KERN_SUCCESS;
d2213 1
a2213 1
	int rv = KERN_SUCCESS;
d2285 2
a2286 3
			if (uvm_map_pageable(map, entry->start,
			    entry->end, FALSE,
			    UVM_LK_ENTER|UVM_LK_EXIT) != KERN_SUCCESS) {
d2366 1
a2366 1
	return(KERN_SUCCESS);
d2421 1
a2421 1
	return (KERN_SUCCESS);
d2519 1
a2519 1
		return(KERN_SUCCESS);
d2680 1
a2680 1
	return(KERN_SUCCESS);
d2728 1
a2728 1
		return (KERN_SUCCESS);
d2748 1
a2748 1
		return (KERN_SUCCESS);
d2835 1
a2835 1
	rv = KERN_SUCCESS;
d2902 1
a2902 1
	return (KERN_SUCCESS);
d2967 1
a2967 1
	error = KERN_SUCCESS;
@


1.81
log
@fix fmts for UVMHIST_LOG() entries making it more useful on 64bit archs; miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.80 2006/07/13 22:51:26 deraadt Exp $	*/
d429 1
a429 1
	UVMHIST_LOG(maphist, "<- new entry=%p [kentry=%d]", me,
d446 1
a446 1
	UVMHIST_LOG(maphist,"<- freeing map entry=%p [flags=%d]",
d739 1
a739 1
	UVMHIST_LOG(maphist, "(map=%p, *startp=0x%lx, size=%ld, flags=0x%x)",
d741 1
a741 1
	UVMHIST_LOG(maphist, "  uobj/offset %p/%lld", uobj, uoffset,0,0);
d750 1
a750 1
		UVMHIST_LOG(maphist, "<- prot. failure:  prot=0x%x, max=0x%x", 
d1179 1
a1179 1
	UVMHIST_LOG(maphist, "(map=%p, hint=0x%lx, len=%ld, flags=0x%x)", 
d1591 1
a1591 1
		    "  detach 0x%x: amap=%p, obj=%p, submap?=%d", 
d1833 1
a1833 1
	UVMHIST_LOG(maphist," ...,dstmap=%p, flags=0x%x)", dstmap,flags,0,0);
d2215 1
a2215 1
	UVMHIST_LOG(maphist,"(map=%p,start=0x%lx,end=0x%lx,new_prot=0x%x)",
d2311 1
a2311 1
	UVMHIST_LOG(maphist, "<- done, rv=%d",rv,0,0,0);
d2335 1
a2335 1
	UVMHIST_LOG(maphist,"(map=%p,start=0x%lx,end=0x%lx,new_inh=0x%x)",
d2385 1
a2385 1
	UVMHIST_LOG(maphist,"(map=%p,start=0x%lx,end=0x%lx,new_adv=0x%x)",
d2452 1
a2452 1
	UVMHIST_LOG(maphist,"(map=%p,start=0x%lx,end=0x%lx,new_pageable=0x%x)",
d2662 1
a2662 1
		UVMHIST_LOG(maphist, "<- done (RV=%d)", rv,0,0,0);
d2706 1
a2706 1
	UVMHIST_LOG(maphist,"(map=%p,flags=0x%x)", map, flags, 0, 0);
d2894 1
a2894 1
		UVMHIST_LOG(maphist,"<- done (RV=%d)", rv,0,0,0);
d2940 1
a2940 1
	UVMHIST_LOG(maphist,"(map=%p,start=0x%lx,end=0x%lx,flags=0x%x)",
d3351 1
a3351 1
	UVMHIST_LOG(maphist,"(vm=%p) ref=%d", vm, vm->vm_refcnt,0,0);
@


1.80
log
@Back out the anon change.  Apparently it was tested by a few, but most of
us did not see it or get a chance to test it before it was commited. It
broke cvs, in the ami driver, making it not succeed at seeing it's devices.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.78 2006/04/27 15:23:56 mickey Exp $	*/
d429 1
a429 1
	UVMHIST_LOG(maphist, "<- new entry=0x%x [kentry=%d]", me,
d446 1
a446 1
	UVMHIST_LOG(maphist,"<- freeing map entry=0x%x [flags=%d]",
d739 1
a739 1
	UVMHIST_LOG(maphist, "(map=0x%x, *startp=0x%x, size=%d, flags=0x%x)",
d741 1
a741 1
	UVMHIST_LOG(maphist, "  uobj/offset 0x%x/%d", uobj, uoffset,0,0);
d751 1
a751 1
		prot, maxprot,0,0);
d978 1
a978 1
	UVMHIST_LOG(maphist,"(map=0x%x,addr=0x%x,ent=0x%x)",
d1010 1
a1010 1
			UVMHIST_LOG(maphist,"<- got it via hint (0x%x)",
d1067 1
a1067 1
				UVMHIST_LOG(maphist,"<- search got it (0x%x)",
d1179 1
a1179 1
	UVMHIST_LOG(maphist, "(map=0x%x, hint=0x%x, len=%d, flags=0x%x)", 
d1201 1
a1201 1
		UVMHIST_LOG(maphist,"<- VA 0x%x > range [0x%x->0x%x]",
d1366 1
a1366 1
	UVMHIST_LOG(maphist,"<- got it!  (result=0x%x)", hint, 0,0,0);
d1405 1
a1405 1
	UVMHIST_LOG(maphist,"(map=0x%x, start=0x%x, end=0x%x)",
d1547 1
a1547 1
		UVMHIST_LOG(maphist, "  removed map entry 0x%x", entry, 0, 0,0);
d1591 1
a1591 1
		    "  detach 0x%x: amap=0x%x, obj=0x%x, submap?=%d", 
d1648 1
a1648 1
	UVMHIST_LOG(maphist, "(map=0x%x, size=0x%x, offset=0x%x,addr=0x%x)",
d1666 1
a1666 1
	UVMHIST_LOG(maphist, "<- done (*raddr=0x%x)", *raddr,0,0,0);
d1831 1
a1831 1
	UVMHIST_LOG(maphist,"(srcmap=0x%x,start=0x%x, len=0x%x", srcmap, start,
d1833 1
a1833 1
	UVMHIST_LOG(maphist," ...,dstmap=0x%x, flags=0x%x)", dstmap,flags,0,0);
d1856 1
a1856 1
	UVMHIST_LOG(maphist, "  dstaddr=0x%x", dstaddr,0,0,0);
d2215 1
a2215 1
	UVMHIST_LOG(maphist,"(map=0x%x,start=0x%x,end=0x%x,new_prot=0x%x)",
d2335 1
a2335 1
	UVMHIST_LOG(maphist,"(map=0x%x,start=0x%x,end=0x%x,new_inh=0x%x)",
d2385 1
a2385 1
	UVMHIST_LOG(maphist,"(map=0x%x,start=0x%x,end=0x%x,new_adv=0x%x)",
d2452 1
a2452 1
	UVMHIST_LOG(maphist,"(map=0x%x,start=0x%x,end=0x%x,new_pageable=0x%x)",
d2706 1
a2706 1
	UVMHIST_LOG(maphist,"(map=0x%x,flags=0x%x)", map, flags, 0, 0);
d2940 1
a2940 1
	UVMHIST_LOG(maphist,"(map=0x%x,start=0x%x,end=0x%x,flags=0x%x)",
d3181 1
a3181 1
	UVMHIST_LOG(maphist,"<- done (vm=0x%x)", vm,0,0,0);
d3351 1
a3351 1
	UVMHIST_LOG(maphist,"(vm=0x%x) ref=%d", vm, vm->vm_refcnt,0,0);
@


1.79
log
@from netbsd: make anons dynamically allocated from pool.
this results in lesse kva waste due to static preallocation of those
for every phys page and also every swap page.
tested by beck krw miod
@
text
@d3000 1
a3000 1
			pg = anon->an_page;
d3814 1
a3814 1
			if (pg->uanon == NULL || pg->uanon->an_page != pg)
d3816 1
a3816 1
				(pg->uanon) ? pg->uanon->an_page : NULL);
@


1.78
log
@from PAE work:
as paddr_t could be a long long (soon) always cast and print as llx.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.77 2006/01/16 13:11:05 mickey Exp $	*/
d3000 1
a3000 1
			pg = anon->u.an_page;
d3814 1
a3814 1
			if (pg->uanon == NULL || pg->uanon->u.an_page != pg)
d3816 1
a3816 1
				(pg->uanon) ? pg->uanon->u.an_page : NULL);
@


1.77
log
@add another uvm histroy for physpage alloc/free and propagate a debugging pgfree check into pglist; no functional change for normal kernels; make histories uncommon
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.76 2006/01/02 15:05:45 tom Exp $	*/
d3793 3
a3795 2
	(*pr)("  flags=%s, pqflags=%s, vers=%d, wire_count=%d, pa=0x%lx\n",
	    pgbuf, pqbuf, pg->version, pg->wire_count, (long)pg->phys_addr);
@


1.76
log
@fix typo in comment

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.75 2005/12/10 16:06:10 krw Exp $	*/
d103 6
@


1.75
log
@Don't check/free new_entry here, since it can't have been set. Spotted
by lint.

"Probably a bogus cut'n paste." says moid.

ok miod@@ pedro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.74 2005/12/10 11:45:43 miod Exp $	*/
d3183 1
a3183 1
 * - refcnt set to 1, rest must me init'd by caller
@


1.74
log
@{en,re}trys -> {en,re}tries; eyeballed by jmc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.73 2005/09/28 00:24:03 pedro Exp $	*/
a847 3
				if (new_entry) {
					uvm_mapent_free(new_entry);
				}
@


1.73
log
@- when we run out of static kernel map entries, grab a fresh page using
  the uvm_km_page allocator and use it instead of calling panic()
- add a counter to uvmexp so we can keep track of how many map entries
  we have in use

idea from tedu@@, long ago, okay deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.72 2005/06/29 06:07:32 deraadt Exp $	*/
d548 1
a548 1
	 * now set up static pool of kernel map entrys ...
d1673 1
a1673 1
 * => we expect the newents chain to have nnewents entrys on it and
@


1.72
log
@on the vax, start mmap BRKSIZ (8MB) into data seg; ok tdeval
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.71 2005/05/24 21:11:47 tedu Exp $	*/
d97 3
d384 2
a385 2
	struct vm_map_entry *me;
	int s;
d392 18
a409 1
		if (me) uvm.kentry_free = me->next;
a411 5
		if (me == NULL) {
			panic("uvm_mapent_alloc: out of static map entries, "
			      "check MAX_KMAPENT (currently %d)",
			      MAX_KMAPENT);
		}
d447 1
@


1.71
log
@add a new field to vm_space and use it to track the number of anon
pages a process uses.  this is now the userland "data size" value.
ok art deraadt tdeval.  thanks testers.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.70 2004/12/30 08:28:39 niklas Exp $	*/
d1122 3
@


1.70
log
@Import M_CANFAIL support from NetBSD, removes a nasty panic during low-mem scenarios, instead generating an ENOMEM backfeed, ok tedu@@, prodded by many
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.69 2004/08/06 22:39:14 deraadt Exp $	*/
d698 1
a698 1
uvm_map(map, startp, size, uobj, uoffset, align, flags)
d706 1
d853 2
d920 3
d1372 1
a1372 1
uvm_unmap_remove(map, start, end, entry_list)
d1376 1
d1440 2
d3345 1
a3345 1
			    &dead_entries);
@


1.69
log
@rename sparc kill_user_windows() to pmap_unuse_final().  provide empty stubs
on all other architectures.  remove last architecture dependent #ifdef from
uvm code.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.68 2004/07/21 01:02:09 art Exp $	*/
d712 1
d827 10
a836 1
		/* got it! */
d845 1
a847 4

		if (prev_entry->aref.ar_amap) {
			amap_extend(prev_entry, size);
		}
@


1.68
log
@Yet another victim of inline masturbation.

Remove inline from a few functions, shrink the kernel by a few kB and
make things faster. A simple compilation on amd64 spends around 5%
less time in kernel.

Yes, it's faster without inlines, now go buy a book about modern cpu
architectures and find a chapter about the new and revolutionary thing
called "cache".

deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.67 2004/05/30 22:35:43 tedu Exp $	*/
d3231 1
a3231 4
#ifdef __sparc__
	/* XXX cgd 960926: the sparc #ifdef should be a MD hook */
	kill_user_windows(p);   /* before stack addresses go away */
#endif
@


1.67
log
@only free pages if we have write permission and they are not COW.
prevents msync/madvise funniness
from art@@ ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.66 2004/05/03 07:14:53 tedu Exp $	*/
d192 5
d198 6
a203 1
int			uvm_map_spacefits(vm_map_t, vaddr_t *, vsize_t, vm_map_entry_t, voff_t, vsize_t);
d205 1
d207 2
a208 1
static vsize_t		uvm_rb_subtree_space(vm_map_entry_t);
d232 2
a233 2
static __inline vsize_t
uvm_rb_space(vm_map_t map, vm_map_entry_t entry)
d248 1
a248 1
uvm_rb_subtree_space(vm_map_entry_t entry)
d268 1
a268 1
static __inline void
d278 2
a279 2
static __inline void
uvm_rb_insert(vm_map_t map, vm_map_entry_t entry)
d295 2
a296 2
static __inline void
uvm_rb_remove(vm_map_t map, vm_map_entry_t entry)
d314 1
d372 1
a372 4

/*
 * local inlines
 */
d378 2
a379 3
static __inline vm_map_entry_t
uvm_mapent_alloc(map)
	vm_map_t map;
d419 2
a420 3
static __inline void
uvm_mapent_free(me)
	vm_map_entry_t me;
@


1.66
log
@protecing with NONE means we must drop the wired count.
solves problems encountered by david@@ and dtucker@@ (pr3758)
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.65 2004/02/23 06:19:32 drahn Exp $	*/
d3042 4
d3050 4
a3053 1
		if (uobj != NULL) {
@


1.65
log
@sync of pmap_update() calls with NetBSD. pmap_update is defined away on
all architectures but arm, where it is needed.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.64 2003/11/18 06:08:19 tedu Exp $	*/
d2222 3
@


1.65.2.1
log
@MFC:
Fix by tedu@@

protecing with NONE means we must drop the wired count.
solves problems encountered by david@@ and dtucker@@ (pr3758)

ok deradt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.65 2004/02/23 06:19:32 drahn Exp $	*/
a2221 3
			if ((current->protection & MASK(entry)) == PROT_NONE &&
			    VM_MAPENT_ISWIRED(entry))
				current->wired_count--;
@


1.65.2.2
log
@MFC:
Fix by tedu@@

only free pages if we have write permission and they are not COW.
prevents msync/madvise funniness

ok deraadt@@ art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.65.2.1 2004/05/14 21:34:24 brad Exp $	*/
a3041 4
		 *
		 * Don't PGO_FREE if we don't have write permission
	 	 * and don't flush if this is a copy-on-write object
		 * since we can't know our permissions on it.
d3046 1
a3046 4
		if (uobj != NULL &&
		    ((flags & PGO_FREE) == 0 ||
		     ((entry->max_protection & VM_PROT_WRITE) != 0 &&
		      (entry->etype & UVM_ET_COPYONWRITE) == 0))) {
@


1.64
log
@faster pools.  split pagelist into full, partial, and empty so we find what
we're looking for.  change small page_header hash table to a splay tree.
from Chuck Silvers.
tested by brad grange henning mcbride naddy otto
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.63 2003/10/08 22:23:56 tedu Exp $	*/
d1512 4
d2034 1
d2258 1
d3565 2
@


1.63
log
@randomize return from uvm_map_hint.  the random increment is limited
to prevent fragmentation.
this has the effect of randomizing unhinted mmap()s, sysV mem, and
position of ld.so.

tested on many archs by many developers for quite some time.
use of MIN to allow m68k to play from miod@@.
vax is not included.
ok deraadt@@ miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.62 2003/09/03 22:52:47 tedu Exp $	*/
a87 1
#define RB_AUGMENT(x) uvm_rb_augment(x)
d90 2
@


1.62
log
@m68k at least doesn't like random mappings.  disable for now.
i386 exec mappings are still random.  detected by pvalchev@@.  ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.61 2003/09/02 17:57:12 tedu Exp $	*/
d1098 2
a1099 2
#if 0
	addr += arc4random() & (256 * 1024 * 1024 - 1);
@


1.62.2.1
log
@MFC:
Fix by tedu@@

only free pages if we have write permission and they are not COW.
prevents msync/madvise funniness

ok deraadt@@ art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.62 2003/09/03 22:52:47 tedu Exp $	*/
a3031 4
		 *
		 * Don't PGO_FREE if we don't have write permission
	 	 * and don't flush if this is a copy-on-write object
		 * since we can't know our permissions on it.
d3036 1
a3036 4
		if (uobj != NULL &&
		    ((flags & PGO_FREE) == 0 ||
		     ((entry->max_protection & VM_PROT_WRITE) != 0 &&
		      (entry->etype & UVM_ET_COPYONWRITE) == 0))) {
@


1.61
log
@add a random offset to uvm_map_hint.  this has the primary effect of
scattering ld.so and libraries around, although all mmaps will also
have some jitter too.  better version after some discussion with drahn
testing/ok deraadt henning marcm otto pb
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.60 2003/06/29 17:31:12 avsm Exp $	*/
d1098 1
a1098 1
#ifndef __vax__
@


1.60
log
@quell a "not a pointer" warning by using 0x%lx instead of %p for a vaddr_t
suggested by art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.59 2003/05/05 17:54:59 drahn Exp $	*/
d82 2
d1083 2
d1091 9
a1099 2
	    ((vaddr_t)p->p_vmspace->vm_daddr >= I386_MAX_EXE_ADDR))
		return (round_page(PAGE_SIZE*2));
d1101 1
a1101 1
	return (round_page((vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ));
@


1.59
log
@Move exec base to 0x1c000000, exe/data gap to 512MB. Allows better
interleave of exe/shared libs. Raise MAXDSIZ back to 1G.
This change REQUIRES a binary update on i386.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.58 2003/04/17 03:50:54 drahn Exp $	*/
d322 1
a322 1
			printf("%s: corrupt: %p >= %p\n",
@


1.58
log
@changes to support mquery with 1Gsep on i386. avoid heap on mappings.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.57 2003/04/14 04:53:51 art Exp $	*/
d1087 1
a1087 1
	    ((vaddr_t)p->p_vmspace->vm_daddr >= 0x40000000))
@


1.57
log
@There are two related changes.

The first one is an mquery(2) syscall. It's for asking the VM system
about where to map things. It will be used by ld.so, read the man page
for details.

The second change is related and is a centralization of uvm_map hint
that all callers of uvm_map calculated. This will allow us to adjust
this hint on architectures that have segments for non-exec mappings.

deraadt@@ drahn@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.56 2002/12/09 02:35:21 art Exp $	*/
d1081 9
@


1.56
log
@Two splasserts in map entry allocation.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.55 2002/10/29 18:30:21 art Exp $	*/
d1075 10
a3599 1

@


1.55
log
@Since memory deallocation can't fail, remove the error return from
uvm_unmap, uvm_deallocate and a few other functions.
Simplifies some code and reduces diff to the UBC branch.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.54 2002/10/29 01:26:58 art Exp $	*/
d424 1
d427 1
@


1.54
log
@"len = entry->end - entry->start;" then logically "entry->start + len"
should be equal to "entry->end". (len is never changed)
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.53 2002/10/17 22:08:37 art Exp $	*/
d1317 1
a1317 1
int
a1488 1
	return(KERN_SUCCESS);
d3273 1
a3273 1
			(void)uvm_unmap_remove(&vm->vm_map,
@


1.53
log
@ - name in uvm_tree_sanity must be const.
 - fix a typo in comment.
 - enable uvm_tree_sanity ifdef DEBUG
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.52 2002/09/17 13:01:20 mpech Exp $	*/
d1444 1
a1444 1
				    entry->start + len);
@


1.52
log
@int is small for returned values from uvm_rb_{space,subtree_space}, use
vsize_t instead.

art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.51 2002/08/30 09:56:22 espie Exp $	*/
d192 1
a192 1
int _uvm_tree_sanity(vm_map_t map, char *name);
d293 3
d297 1
d300 1
a300 1
_uvm_tree_sanity(vm_map_t map, char *name)
d604 1
a604 1
 *	the ending address, if it does't we split the reference
@


1.51
log
@__FUNCTION__ -> __func__ that I forgot to commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.50 2002/08/20 23:21:17 mickey Exp $	*/
d193 1
a193 1
static int		uvm_rb_subtree_space(vm_map_entry_t);
d217 1
a217 1
static __inline int
d232 1
a232 1
static int
@


1.50
log
@print as unsigned for unsigned values, when printing out a map in ddb; art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.49 2002/07/23 15:53:45 art Exp $	*/
d977 1
a977 1
	uvm_tree_sanity(map, __FUNCTION__);
@


1.49
log
@Now that sparc64 implements reverse splassert, it make sense
to add splassert(IPL_NONE) in a few strategic places.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.48 2002/06/14 21:35:00 todd Exp $	*/
d3606 1
a3606 1
	(*pr)("\t#ent=%d, sz=%d, ref=%d, version=%d, flags=0x%x\n",
@


1.48
log
@spelling; from Brian Poole <raj@@cerias.purdue.edu>
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.47 2002/06/05 17:40:08 art Exp $	*/
d383 1
d387 1
@


1.47
log
@DIAGNOSTIC check for duplicate entry into the tree.
niels@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.46 2002/04/29 06:26:51 pvalchev Exp $	*/
d3441 1
a3441 1
			 *    parent and child process are refering to the
@


1.46
log
@wierd -> weird
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.45 2002/03/14 03:16:13 millert Exp $	*/
d267 1
d270 5
a274 1
	RB_INSERT(uvm_tree, &(map)->rbhead, entry);
@


1.45
log
@Final __P removal plus some cosmetic fixups
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.44 2002/03/14 01:27:18 millert Exp $	*/
d2982 1
a2982 1
				panic("uvm_map_clean: wierd flags");
@


1.44
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.43 2002/03/08 07:25:29 mickey Exp $	*/
d429 1
a429 1
	memcpy(dst, src, ((char *)&src->uvm_map_entry_stop_copy) - ((char*)src));
@


1.43
log
@semicolon is not always what it seems, replace w/ a \n in asm labels
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.42 2002/03/07 01:08:57 provos Exp $	*/
d183 6
a188 6
static vm_map_entry_t	uvm_mapent_alloc __P((vm_map_t));
static void		uvm_mapent_copy __P((vm_map_entry_t,vm_map_entry_t));
static void		uvm_mapent_free __P((vm_map_entry_t));
static void		uvm_map_entry_unwire __P((vm_map_t, vm_map_entry_t));
static void		uvm_map_reference_amap __P((vm_map_entry_t, int));
static void		uvm_map_unreference_amap __P((vm_map_entry_t, int));
d190 1
a190 1
int			uvm_map_spacefits __P((vm_map_t, vaddr_t *, vsize_t, vm_map_entry_t, voff_t, vsize_t));
d193 1
a193 1
static int		uvm_rb_subtree_space __P((vm_map_entry_t));
d3594 1
a3594 1
	int (*pr) __P((const char *, ...));
d3636 1
a3636 1
	int (*pr) __P((const char *, ...));
d3679 1
a3679 1
	int (*pr) __P((const char *, ...));
@


1.42
log
@use an augmented red-black tree to keep track of free space in the vm_map.
uvm_tree_sanity is left as debugging help but needs to be enabled manually.
okay art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.41 2002/02/28 18:50:26 provos Exp $	*/
d343 1
a343 1
	__asm(".globl treesanity_label ; treesanity_label:");
@


1.41
log
@use red-black tree for lookup_entry.  the red-black tree case for
map_findspace is still broken on alpha.  this will make debugging easier.
okay millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.39 2002/02/25 00:20:45 provos Exp $	*/
d86 1
d190 2
d193 1
d207 6
d217 46
d266 3
d270 3
d278 3
d282 4
d296 9
d307 5
d626 2
d819 1
d1027 35
d1086 1
d1153 84
d3208 2
@


1.40
log
@back out red-black tree. they are very fast but alpha UVM is broken and
the tree triggers the bug, PMAP_PREFER case was broken also.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.38 2002/02/18 10:02:20 art Exp $	*/
a92 1

d136 1
d148 1
d189 76
d469 2
d484 2
d504 2
d526 1
d542 1
a542 1

d555 1
d610 2
d736 2
d809 2
d832 1
d872 3
d881 28
d933 1
d965 1
d975 2
d1018 13
d1051 1
a1051 2
		if ((flags & UVM_FLAG_FIXED) == 0 &&
		    uoffset != UVM_UNKNOWN_OFFSET)
d1064 1
a1064 8
			if (align != 0) {
				UVMHIST_LOG(maphist,
				    "calling recursively, no align",
				    0,0,0,0);
				return (uvm_map_findspace(map, orig_hint,
				    length, result, uobj, uoffset, 0, flags));
			}
			return (NULL);
a1068 4
		if (flags & UVM_FLAG_FIXED) {
			UVMHIST_LOG(maphist,"<- fixed mapping failed", 0,0,0,0);
			return(NULL); /* only one shot at it ... */
		}
d1070 1
d1075 10
d1116 2
d1263 2
d1391 2
a1448 1

d1458 4
d1466 11
d1489 2
d1536 3
d1816 4
d1831 4
d3000 1
d3002 1
@


1.39
log
@use a red-black tree to find entries in the vm_map. augment the red-black
tree to find free space between entries.  speeds up memory allocation,
etc...
@
text
@a85 1
#define RB_AUGMENT(x) uvm_rb_augment(x)
d93 1
a136 1
	uvm_rb_insert(map, entry); \
a147 1
	uvm_rb_remove(map, entry); \
a187 158
int			uvm_map_spacefits __P((vm_map_t, vaddr_t *, vsize_t, vm_map_entry_t, voff_t, vsize_t));

int _uvm_tree_sanity(vm_map_t map, char *name);
static int		uvm_rb_subtree_space __P((vm_map_entry_t));

static __inline int
uvm_compare(vm_map_entry_t a, vm_map_entry_t b)
{
	if (a->start < b->start)
		return (-1);
	else if (a->start > b->start)
		return (1);
	
	return (0);
}


static __inline void
uvm_rb_augment(vm_map_entry_t entry)
{
	entry->space = uvm_rb_subtree_space(entry);
}

RB_PROTOTYPE(uvm_tree, vm_map_entry, rb_entry, uvm_compare);

RB_GENERATE(uvm_tree, vm_map_entry, rb_entry, uvm_compare);

static __inline int
uvm_rb_space(vm_map_t map, vm_map_entry_t entry)
{
	vm_map_entry_t next;
	vaddr_t space;

	if ((next = entry->next) == &map->header)
		space = map->max_offset - entry->end;
	else {
		KASSERT(next);
		space = next->start - entry->end;
	}
	return (space);
}
		
static int
uvm_rb_subtree_space(vm_map_entry_t entry)
{
	vaddr_t space, tmp;

	space = entry->ownspace;
	if (RB_LEFT(entry, rb_entry)) {
		tmp = RB_LEFT(entry, rb_entry)->space;
		if (tmp > space)
			space = tmp;
	}

	if (RB_RIGHT(entry, rb_entry)) {
		tmp = RB_RIGHT(entry, rb_entry)->space;
		if (tmp > space)
			space = tmp;
	}

	return (space);
}

static __inline void
uvm_rb_fixup(vm_map_t map, vm_map_entry_t entry)
{
	/* We need to traverse to the very top */
	do {
		entry->ownspace = uvm_rb_space(map, entry);
		entry->space = uvm_rb_subtree_space(entry);
	} while ((entry = RB_PARENT(entry, rb_entry)) != NULL);
}

static __inline void
uvm_rb_insert(vm_map_t map, vm_map_entry_t entry)
{
	vaddr_t space = uvm_rb_space(map, entry);

	entry->ownspace = entry->space = space;
	RB_INSERT(uvm_tree, &(map)->rbhead, entry);
	uvm_rb_fixup(map, entry);
	if (entry->prev != &map->header)
		uvm_rb_fixup(map, entry->prev);
}

static __inline void
uvm_rb_remove(vm_map_t map, vm_map_entry_t entry)
{
	vm_map_entry_t parent;

	parent = RB_PARENT(entry, rb_entry);
	RB_REMOVE(uvm_tree, &(map)->rbhead, entry);
	if (entry->prev != &map->header)
		uvm_rb_fixup(map, entry->prev);
	if (parent)
		uvm_rb_fixup(map, parent);
}

#define uvm_tree_sanity(x,y)

int
_uvm_tree_sanity(vm_map_t map, char *name)
{
	vm_map_entry_t tmp, trtmp;
	int n = 0, i = 1;

	RB_FOREACH(tmp, uvm_tree, &map->rbhead) {
		if (tmp->ownspace != uvm_rb_space(map, tmp)) {
			printf("%s: %d/%d ownspace %x != %x %s\n",
			    name, n + 1, map->nentries,
			    tmp->ownspace, uvm_rb_space(map, tmp),
			    tmp->next == &map->header ? "(last)" : "");
			goto error;
		}
	}
	trtmp = NULL;
	RB_FOREACH(tmp, uvm_tree, &map->rbhead) {
		if (tmp->space != uvm_rb_subtree_space(tmp)) {
			printf("%s: space %d != %d\n",
			    name, tmp->space, uvm_rb_subtree_space(tmp));
			goto error;
		}
		if (trtmp != NULL && trtmp->start >= tmp->start) {
			printf("%s: corrupt: %p >= %p\n",
			    name, trtmp->start, tmp->start);
			goto error;
		}
		n++;

	    trtmp = tmp;
	}

	if (n != map->nentries) {
		printf("%s: nentries: %d vs %d\n",
		    name, n, map->nentries);
		goto error;
	}

	for (tmp = map->header.next; tmp && tmp != &map->header;
	    tmp = tmp->next, i++) {
		trtmp = RB_FIND(uvm_tree, &map->rbhead, tmp);
		if (trtmp != tmp) {
			printf("%s: lookup: %d: %p - %p: %p\n",
			    name, i, tmp, trtmp,
			    RB_PARENT(tmp, rb_entry));
			goto error;
		}
	}

	return (0);
 error:
#ifdef	DDB
	/* handy breakpoint location for error case */
	__asm(".globl treesanity_label ; treesanity_label:");
#endif
	return (-1);
}

a391 2
	uvm_tree_sanity(map, "clip_start entry");

a404 2

	/* Does not change order for the RB tree */
a422 2

	uvm_tree_sanity(map, "clip_start leave");
a442 1
	uvm_tree_sanity(map, "clip_end entry");
a457 2
	
	uvm_rb_fixup(map, entry);
a470 1
	uvm_tree_sanity(map, "clip_end leave");
a524 2
	uvm_tree_sanity(map, "map entry");

a646 1
		uvm_rb_fixup(map, prev_entry);
a648 2
		uvm_tree_sanity(map, "map leave 2");

a719 2
	uvm_tree_sanity(map, "map leave");

a740 1
	int			use_tree = 0;
a779 3

		if (map->nentries > 30)
			use_tree = 1;
a785 28
		use_tree = 1;
	}

	uvm_tree_sanity(map, __FUNCTION__);

	if (use_tree) {
		vm_map_entry_t prev = &map->header;
		cur = RB_ROOT(&map->rbhead);

		/*
		 * Simple lookup in the tree.  Happens when the hint is
		 * invalid, or nentries reach a threshold.
		 */
		while (cur) {
			if (address >= cur->start) {
				if (address < cur->end) {
					*entry = cur;
					SAVE_HINT(map, map->hint, cur);
					return (TRUE);
				}
				prev = cur;
				cur = RB_RIGHT(cur, rb_entry);
			} else
				cur = RB_LEFT(cur, rb_entry);
		}
		*entry = prev;
		UVMHIST_LOG(maphist,"<- failed!",0,0,0,0);
		return (FALSE);
a809 1

a816 34
 * Checks if address pointed to be phint fits into the empty
 * space before the vm_map_entry after.  Takes aligment and
 * offset into consideration.
 */

int
uvm_map_spacefits(vm_map_t map, vaddr_t *phint, vsize_t length,
    vm_map_entry_t after, voff_t uoffset, vsize_t align)
{
	vaddr_t hint = *phint;
	vaddr_t end;

#ifdef PMAP_PREFER
	/*
	 * push hint forward as needed to avoid VAC alias problems.
	 * we only do this if a valid offset is specified.
	 */
	if (uoffset != UVM_UNKNOWN_OFFSET)
		PMAP_PREFER(uoffset, &hint);
#endif
	if (align != 0)
		if ((hint & (align - 1)) != 0)
			hint = roundup(hint, align);
	end = hint + length;
	if (end > map->max_offset || end < hint)
		return (FALSE);
	if (after != NULL && after != &map->header && after->start < end)
		return (FALSE);
	
	*phint = hint;
	return (TRUE);
}

/*
a840 2
	vm_map_entry_t child, prev = NULL;

a849 2
	uvm_tree_sanity(map, "map_findspace entry");

a890 117
	if (flags & UVM_FLAG_FIXED) {
#ifdef PMAP_PREFER
		/*
		 * push hint forward as needed to avoid VAC alias problems.
		 * we only do this if a valid offset is specified.
		 */
		if (uoffset != UVM_UNKNOWN_OFFSET)
			PMAP_PREFER(uoffset, &hint);
#endif
		if (align != 0) {
			if ((hint & (align - 1)) != 0)
				hint = roundup(hint, align);
			/*
			 * XXX Should we PMAP_PREFER() here again?
			 */
		}
		end = hint + length;
		if (end > map->max_offset || end < hint) {
			UVMHIST_LOG(maphist,"<- failed (off end)", 0,0,0,0);
			goto error;
		}
		next = entry->next;
		if (next == &map->header || next->start >= end)
			goto found;
		UVMHIST_LOG(maphist,"<- fixed mapping failed", 0,0,0,0);
		return(NULL); /* only one shot at it ... */
	}

	/* Try to find the space in the red-black tree */

	/* Check slot before any entry */
	tmp = map->header.next;
	if (uvm_map_spacefits(map, &hint, length, tmp, uoffset, align)) {
		entry = map->first_free;
		goto found;
	}
	
	/* If there is not enough space in the whole tree, we fail */
	tmp = RB_ROOT(&map->rbhead);
	if (tmp == NULL || tmp->space < length)
		goto error;

	/* Find an entry close to hint that has enough space */
	for (; tmp;) {
		if (tmp->start > hint &&
		    (prev == NULL || tmp->start < prev->start)) {
			if (tmp->ownspace >= length)
				prev = tmp;
			else if ((child = RB_RIGHT(tmp, rb_entry)) != NULL &&
			    child->space >= length)
				prev = tmp;
		}
		if (tmp->start < hint)
			child = RB_RIGHT(tmp, rb_entry);
		else if (tmp->start > hint)
			child = RB_LEFT(tmp, rb_entry);
		else {
			if (tmp->ownspace >= length)
				break;
			child = RB_RIGHT(tmp, rb_entry);
		}
		if (child == NULL || child->space < length)
			break;
		tmp = child;
	}
	
	if (tmp != NULL) {
		/* 
		 * Check if the entry that we found satifies the
		 * space requirement
		 */
		if (hint < tmp->end)
			hint = tmp->end;
		if (uvm_map_spacefits(map, &hint, length, tmp->next, uoffset,
			align)) {
			entry = tmp;
			hint = hint;
			goto found;
		}
	}
	if (prev == NULL)
		goto error;
	
	hint = prev->end;
	if (uvm_map_spacefits(map, &hint, length, prev->next, uoffset,
		align)) {
		entry = prev;
		goto found;
	}
	
	if ((tmp = RB_RIGHT(prev, rb_entry)) == NULL ||
	    tmp->space < length)
		goto error;
	
	for (;;) {
		KASSERT(tmp && tmp->space >= length);
		child = RB_LEFT(tmp, rb_entry);
		if (child && child->space >= length) {
			tmp = child;
			continue;
		}
		if (tmp->ownspace >= length)
			break;
		tmp = RB_RIGHT(tmp, rb_entry);
	}
	
	hint = tmp->end;
	if (uvm_map_spacefits(map, &hint, length, tmp->next, uoffset, align)) {
		entry = tmp;
		goto found;
	}

	/* 
	 * The tree fails to find an entry because of offset or alignment
	 * restrictions.  Search the list instead.
	 */

d911 2
a912 1
		if (uoffset != UVM_UNKNOWN_OFFSET)
d925 8
a932 1
			goto error;
d937 4
a941 1
 found:
a945 10

 error:
	if (align != 0) {
		UVMHIST_LOG(maphist,
		    "calling recursively, no align",
		    0,0,0,0);
		return (uvm_map_findspace(map, orig_hint,
			    length, result, uobj, uoffset, 0, flags));
			}
	return (NULL);
a976 2
	uvm_tree_sanity(map, "unmap_remove entry");

a1121 2
	uvm_tree_sanity(map, "unmap_remove leave");

a1247 2
	uvm_tree_sanity(map, "map_replace entry");

d1304 1
a1313 4

		/* Fix RB tree */
		uvm_rb_remove(map, oldent);

a1317 11
		/* Fixup the RB tree */
		{
			int i;
			vm_map_entry_t tmp;

			tmp = newents;
			for (i = 0; i < nnewents && tmp; i++) {
				uvm_rb_insert(map, tmp);
				tmp = tmp->next;
			}
		}
a1329 2
	uvm_tree_sanity(map, "map_replace leave");

a1374 3
	uvm_tree_sanity(srcmap, "map_extract src enter");
	uvm_tree_sanity(dstmap, "map_extract dst enter");

a1651 4

	uvm_tree_sanity(srcmap, "map_extract src leave");
	uvm_tree_sanity(dstmap, "map_extract dst leave");

a1662 4

	uvm_tree_sanity(srcmap, "map_extract src err leave");
	uvm_tree_sanity(dstmap, "map_extract dst err leave");

a2827 1
		uvm_tree_sanity(map, "resize enter");
a2828 3
		if (map->header.prev != &map->header)
			uvm_rb_fixup(map, map->header.prev);
		uvm_tree_sanity(map, "resize leave");
@


1.38
log
@From the UBC branch and NetBSD.

We allocate map entries for the non-intrsafe kernel map (most notably
kernel_map and exec_map) from a pool that's backed by kmem_map (to avoid
deadlocking).

This should get rid of MAX_KMAPENT panics.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.37 2002/01/23 00:39:48 art Exp $	*/
d86 1
a93 1

d137 1
d149 1
d190 158
d552 2
d567 2
d587 2
d609 1
d625 2
d640 1
d695 2
d819 1
d822 2
d895 2
d918 1
d958 3
d967 28
d1019 1
d1027 34
d1085 2
d1096 2
d1139 117
d1276 1
a1276 2
		if ((flags & UVM_FLAG_FIXED) == 0 &&
		    uoffset != UVM_UNKNOWN_OFFSET)
d1289 1
a1289 8
			if (align != 0) {
				UVMHIST_LOG(maphist,
				    "calling recursively, no align",
				    0,0,0,0);
				return (uvm_map_findspace(map, orig_hint,
				    length, result, uobj, uoffset, 0, flags));
			}
			return (NULL);
a1293 4
		if (flags & UVM_FLAG_FIXED) {
			UVMHIST_LOG(maphist,"<- fixed mapping failed", 0,0,0,0);
			return(NULL); /* only one shot at it ... */
		}
d1295 1
d1300 10
d1341 2
d1488 2
d1616 2
a1673 1

d1683 4
d1691 11
d1714 2
d1761 3
d2041 4
d2056 4
d3225 1
d3227 3
@


1.37
log
@Pool deals fairly well with physical memory shortage, but it doesn't deal
well (not at all) with shortages of the vm_map where the pages are mapped
(usually kmem_map).

Try to deal with it:
 - group all information the backend allocator for a pool in a separate
   struct. The pool will only have a pointer to that struct.
 - change the pool_init API to reflect that.
 - link all pools allocating from the same allocator on a linked list.
 - Since an allocator is responsible to wait for physical memory it will
   only fail (waitok) when it runs out of its backing vm_map, carefully
   drain pools using the same allocator so that va space is freed.
   (see comments in code for caveats and details).
 - change pool_reclaim to return if it actually succeeded to free some
   memory, use that information to make draining easier and more efficient.
 - get rid of PR_URGENT, noone uses it.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.36 2002/01/02 22:23:25 miod Exp $	*/
d80 1
d109 1
a193 2
 *
 * => XXX: static pool for kernel map?
d200 1
a200 1
	vm_map_entry_t me;
d202 1
a202 2
	UVMHIST_FUNC("uvm_mapent_alloc");
	UVMHIST_CALLED(maphist);
d204 2
a205 7
	if ((map->flags & VM_MAP_INTRSAFE) == 0 &&
	    map != kernel_map && kernel_map != NULL /* XXX */) {
		me = pool_get(&uvm_map_entry_pool, PR_WAITOK);
		me->flags = 0;
		/* me can't be null, wait ok */
	} else {
		s = splvm();	/* protect kentry_free list with splvm */
d211 5
a215 2
		if (!me)
	panic("mapent_alloc: out of static map entries, check MAX_KMAPENT");
d217 6
d225 2
a226 3
	UVMHIST_LOG(maphist, "<- new entry=0x%x [kentry=%d]", 
		me, ((map->flags & VM_MAP_INTRSAFE) != 0 || map == kernel_map)
		? TRUE : FALSE, 0, 0);
d241 3
a243 3
	UVMHIST_FUNC("uvm_mapent_free");
	UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist,"<- freeing map entry=0x%x [flags=%d]", 
d245 2
a246 4
	if ((me->flags & UVM_MAP_STATIC) == 0) {
		pool_put(&uvm_map_entry_pool, me);
	} else {
		s = splvm();	/* protect kentry_free list with splvm */
d252 4
d365 2
@


1.36
log
@Back out a few more uvm changes, especially wrt swap usage.
This unbreaks m68k m88k sparc and perhaps others, which eventually froze
when hitting swap.
Tested by various people on various platforms.
ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.30 2001/11/09 03:32:23 art Exp $	*/
d358 1
a358 2
	    0, 0, 0, "vmsppl", 0,
	    pool_page_alloc_nointr, pool_page_free_nointr, M_VMMAP);
d360 1
a360 2
	    0, 0, 0, "vmmpepl", 0,
	    pool_page_alloc_nointr, pool_page_free_nointr, M_VMMAP);
@


1.35
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.31 2001/11/12 01:26:09 art Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.93 2001/02/11 01:34:23 eeh Exp $	*/
d2442 2
d2481 2
a2482 4
		if (end <= current->end) {
			break;
		}
		if (current->end != current->next->start) {
d2490 1
a2490 1
	for (current = entry; start < end; current = current->next) {
d2506 4
d2562 1
d2565 6
@


1.34
log
@Yet another sync to NetBSD uvm.
Today we add a pmap argument to pmap_update() and allocate map entries for
kernel_map from kmem_map instead of using the static entries. This should
get rid of MAX_KMAPENT panics. Also some uvm_loan problems are fixed.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.33 2001/11/28 19:28:14 art Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.105 2001/09/10 21:19:42 chris Exp $	*/
d4 1
a4 1
/*
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and
d48 1
a48 1
 *
d54 3
a56 3
 *
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
d58 1
a58 1
 *
a79 1
#include <sys/kernel.h>
a107 1
struct pool uvm_map_entry_kmem_pool;
d179 6
a184 6
static struct vm_map_entry *uvm_mapent_alloc __P((struct vm_map *));
static void uvm_mapent_copy __P((struct vm_map_entry *, struct vm_map_entry *));
static void uvm_mapent_free __P((struct vm_map_entry *));
static void uvm_map_entry_unwire __P((struct vm_map *, struct vm_map_entry *));
static void uvm_map_reference_amap __P((struct vm_map_entry *, int));
static void uvm_map_unreference_amap __P((struct vm_map_entry *, int));
d192 2
d196 1
a196 1
static __inline struct vm_map_entry *
d198 1
a198 1
	struct vm_map *map;
d200 1
a200 1
	struct vm_map_entry *me;
d202 2
a203 1
	UVMHIST_FUNC("uvm_mapent_alloc"); UVMHIST_CALLED(maphist);
d205 7
a211 2
	if (map->flags & VM_MAP_INTRSAFE || cold) {
		s = splvm();
d217 2
a218 5
		if (me == NULL) {
			panic("uvm_mapent_alloc: out of static map entries, "
			      "check MAX_KMAPENT (currently %d)",
			      MAX_KMAPENT);
		}
a219 6
	} else if (map == kernel_map) {
		me = pool_get(&uvm_map_entry_kmem_pool, PR_WAITOK);
		me->flags = UVM_MAP_KMEM;
	} else {
		me = pool_get(&uvm_map_entry_pool, PR_WAITOK);
		me->flags = 0;
d222 3
a224 2
	UVMHIST_LOG(maphist, "<- new entry=0x%x [kentry=%d]", me,
	    ((map->flags & VM_MAP_INTRSAFE) != 0 || map == kernel_map), 0, 0);
d230 2
d236 1
a236 1
	struct vm_map_entry *me;
d239 3
a241 3
	UVMHIST_FUNC("uvm_mapent_free"); UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist,"<- freeing map entry=0x%x [flags=%d]",
d243 4
a246 2
	if (me->flags & UVM_MAP_STATIC) {
		s = splvm();
a251 4
	} else if (me->flags & UVM_MAP_KMEM) {
		pool_put(&uvm_map_entry_kmem_pool, me);
	} else {
		pool_put(&uvm_map_entry_pool, me);
d261 2
a262 2
	struct vm_map_entry *src;
	struct vm_map_entry *dst;
d264 2
a265 2
	memcpy(dst, src,
	       ((char *)&src->uvm_map_entry_stop_copy) - ((char *)src));
d276 2
a277 2
	struct vm_map *map;
	struct vm_map_entry *entry;
d279 1
d290 1
a290 1
	struct vm_map_entry *entry;
d293 1
a293 1
	amap_ref(entry->aref.ar_amap, entry->aref.ar_pageoff,
d299 1
a299 1
 * wrapper for calling amap_unref()
d303 1
a303 1
	struct vm_map_entry *entry;
d306 1
a306 1
	amap_unref(entry->aref.ar_amap, entry->aref.ar_pageoff,
d313 1
a313 1
 * and init the static pool of struct vm_map_entry *'s for the kernel here.
d317 1
a317 1
uvm_map_init()
a362 2
	pool_init(&uvm_map_entry_kmem_pool, sizeof(struct vm_map_entry),
	    0, 0, 0, "vmmpekpl", 0, NULL, NULL, M_VMMAP);
d372 1
a372 1
 *
d378 4
a381 5
void
uvm_map_clip_start(map, entry, start)
	struct vm_map *map;
	struct vm_map_entry *entry;
	vaddr_t start;
d383 1
a383 1
	struct vm_map_entry *new_entry;
d397 1
a397 1
	new_entry->end = start;
d413 1
a413 1
		if (UVM_ET_ISOBJ(entry) &&
d424 1
a424 1
 *
d432 2
a433 2
	struct vm_map *map;
	struct vm_map_entry *entry;
d436 1
a436 1
	struct vm_map_entry *	new_entry;
d486 1
a486 1
 *
d501 1
a501 1
	struct vm_map *map;
d509 1
a509 1
	struct vm_map_entry *prev_entry, *new_entry;
d526 1
a526 1
		UVMHIST_LOG(maphist, "<- prot. failure:  prot=0x%x, max=0x%x",
d528 1
a528 1
		return EACCES;
d537 1
a537 1
			return EAGAIN;
d540 1
a540 1
	if ((prev_entry = uvm_map_findspace(map, *startp, size, startp,
d544 1
a544 1
		return ENOMEM;
d562 2
a563 2
	 * [typically from uvm_map_reserve] or it is UVM_UNKNOWN_OFFSET.   in
	 * either case we want to zero it  before storing it in the map entry
d565 2
a566 2
	 *
	 * if uobj is not null
d592 1
a592 1
	if ((flags & UVM_FLAG_NOMERGE) == 0 &&
d603 1
a603 1
		if (prev_entry->protection != prot ||
d613 1
a613 1
			goto step3;
d616 1
a616 1
		 * can't extend a shared amap.  note: no need to lock amap to
d647 1
a647 1
		return 0;
d655 1
a655 1
	 * the number of times we missed a *possible* chance to merge more
d659 1
a659 1
	    prev_entry->next != &map->header &&
d673 1
a673 1
	if (uobj)
d694 1
a694 1
		vaddr_t to_add = (flags & UVM_FLAG_AMAPPAD) ?
d703 1
d705 1
d718 1
a718 1
	return 0;
d731 1
a731 1
	struct vm_map *map;
d733 1
a733 1
	struct vm_map_entry **entry;		/* OUT */
d735 2
a736 2
	struct vm_map_entry *cur;
	struct vm_map_entry *last;
a756 1

a767 1

a776 1

a779 1

d825 1
a825 1
struct vm_map_entry *
d827 1
a827 1
	struct vm_map *map;
d836 1
a836 1
	struct vm_map_entry *entry, *next, *tmp;
d841 1
a841 1
	UVMHIST_LOG(maphist, "(map=0x%x, hint=0x%x, len=%d, flags=0x%x)",
d872 1
a872 1
		if ((entry = map->first_free) != &map->header)
a892 1

a893 1

a906 1

d951 1
a951 1
 * => caller must check alignment and size
d957 1
a957 1
void
d959 3
a961 3
	struct vm_map *map;
	vaddr_t start, end;
	struct vm_map_entry **entry_list;	/* OUT */
d963 1
a963 1
	struct vm_map_entry *entry, *first_entry, *next;
d965 2
a966 1
	UVMHIST_FUNC("uvm_unmap_remove"); UVMHIST_CALLED(maphist);
a975 1

d982 1
d1006 1
a1006 1
	 *       so that we don't block other threads.
a1007 1

d1012 1
a1012 1
	 * break up the area into map entry sized regions and unmap.  note
d1020 1
a1020 1
		UVM_MAP_CLIP_END(map, entry, end);
a1035 1

d1061 2
a1062 2
			 * uvm_km_pgremove currently does the following:
			 *   for pages in the kernel object in range:
a1074 1

a1091 1

a1095 1

a1098 1

d1103 1
a1103 1
		 * remove entry from map and put it on our list of entries
a1105 1

a1116 1
	pmap_update(vm_map_pmap(map));
d1120 1
a1120 1
	 * references to the mapped objects.
d1125 1
d1136 1
a1136 1
	struct vm_map_entry *first_entry;
d1139 1
a1139 1
	struct vm_map_entry *next_entry;
d1145 2
a1146 2
		    "  detach 0x%x: amap=0x%x, obj=0x%x, submap?=%d",
		    first_entry, first_entry->aref.ar_amap,
d1170 1
d1182 1
a1182 1
/*
d1185 1
a1185 1
 * => we reserve space in a map by putting a dummy map entry in the
d1194 1
a1194 1
	struct vm_map *map;
d1200 1
a1200 1
	UVMHIST_FUNC("uvm_map_reserve"); UVMHIST_CALLED(maphist);
d1215 1
a1215 1
	    UVM_ADV_RANDOM, UVM_FLAG_NOMERGE)) != 0) {
d1218 1
a1218 1
	}
d1225 1
a1225 1
 * uvm_map_replace: replace a reserved (blank) area of memory with
d1228 1
a1228 1
 * => caller must WRITE-LOCK the map
d1239 1
a1239 1
	struct vm_map_entry *newents;
d1242 1
a1242 1
	struct vm_map_entry *oldent, *last;
d1256 1
a1256 1
	if (oldent->start != start || oldent->end != end ||
a1261 1

a1264 1

d1266 1
a1266 1
		struct vm_map_entry *tmpent = newents;
d1300 2
a1301 1
		last = newents->prev;
d1354 1
a1354 1
	struct vm_map *srcmap, *dstmap;
d1361 2
a1362 2
	struct vm_map_entry *chain, *endchain, *entry, *orig_entry, *newentry,
	    *deadentry, *oldentry;
d1392 1
a1392 1
	 * step 2: setup for the extraction process loop by init'ing the
d1502 2
a1503 2
		newentry->protection = (flags & UVM_EXTRACT_FIXPROT) ?
			entry->max_protection : entry->protection;
d1528 1
a1528 1
		if ((flags & UVM_EXTRACT_CONTIG) && entry->end < end &&
d1547 1
a1547 1
	 * note usage of copy_ok:
d1598 1
a1598 1
				pmap_remove(srcmap->pmap, entry->start,
a1612 1
		pmap_update(srcmap->pmap);
a1621 2
	} else {
		deadentry = NULL;
d1623 2
d1675 1
a1675 1
 * => to remove a submap, use uvm_unmap() on the main map
d1684 1
a1684 1
	struct vm_map *map, *submap;
d1687 2
a1688 2
	struct vm_map_entry *entry;
	int error;
d1701 1
a1701 1
	if (entry != NULL &&
d1709 1
a1709 1
		error = 0;
d1711 1
a1711 1
		error = EINVAL;
d1714 1
a1714 1
	return error;
d1727 1
d1731 1
a1731 1
	struct vm_map *map;
d1736 2
a1737 2
	struct vm_map_entry *current, *entry;
	int error = 0;
d1743 1
d1745 1
d1759 1
a1759 1
			error = EINVAL;
d1763 1
a1763 1
			error = EACCES;
d1772 1
d1777 1
d1786 1
a1786 1
		 * update physical map if necessary.  worry about copy-on-write
d1808 1
a1808 2
			    UVM_LK_ENTER|UVM_LK_EXIT) != 0) {

d1813 2
a1814 2
				 * the map, but will return the error
				 * condition regardless.
d1822 1
a1822 2

				error = ENOMEM;
d1825 1
a1827 1
	pmap_update(map->pmap);
d1831 2
a1832 2
	UVMHIST_LOG(maphist, "<- done, error=%d",error,0,0,0);
	return error;
d1835 1
d1838 1
a1838 1
/*
d1848 1
a1848 1
	struct vm_map *map;
d1853 1
a1853 1
	struct vm_map_entry *entry, *temp_entry;
d1865 1
a1865 1
		return EINVAL;
d1868 2
a1869 1
	vm_map_lock(map);	
d1871 1
d1884 1
d1887 1
a1887 1
	return 0;
d1890 1
a1890 1
/*
d1898 1
a1898 1
	struct vm_map *map;
d1903 1
a1903 1
	struct vm_map_entry *entry, *temp_entry;
d1934 1
a1934 1
			return EINVAL;
d1942 1
a1942 1
	return 0;
d1961 1
a1961 1
	struct vm_map *map;
d1966 1
a1966 1
	struct vm_map_entry *entry, *start_entry, *failed_entry;
d1978 1
d1981 1
a1981 1
	/*
d1986 1
a1986 1
	 * making any changes.
d1993 2
a1994 2
		UVMHIST_LOG(maphist,"<- done (fault)",0,0,0,0);
		return EFAULT;
d1998 1
a1998 1
	/*
d2007 1
a2007 1
		 * really wired down and that there are no holes.
d2017 3
a2019 2
				UVMHIST_LOG(maphist, "<- done (INVAL)",0,0,0,0);
				return EINVAL;
d2024 1
a2024 1
		/*
d2040 1
a2040 1
		return 0;
d2048 1
a2048 1
	 *    be wired and increment its wiring count.
d2076 1
a2076 1
				if (UVM_ET_ISNEEDSCOPY(entry) &&
d2080 1
a2080 1
					    start, end);
d2090 1
a2090 1
		 * Check for holes
d2100 1
a2100 1
			 * be undone, but the wired counts need to be restored.
d2110 1
a2110 1
			return EINVAL;
a2131 1

a2136 1

d2201 1
a2201 1
	return 0;
d2215 1
a2215 1
	struct vm_map *map;
d2219 1
a2219 1
	struct vm_map_entry *entry, *failed_entry;
a2236 1

a2240 1

d2249 5
a2253 1
		return 0;
a2256 1

a2259 1

a2263 1

a2266 1

d2269 1
a2269 1
		return 0;
d2305 1
a2305 1
		return ENOMEM;
d2313 1
a2313 1
		return ENOMEM;
a2325 1

a2332 1

d2334 1
a2334 1
				if (UVM_ET_ISNEEDSCOPY(entry) &&
d2356 1
a2356 1
	rv = 0;
a2362 1

a2367 1

d2373 1
a2373 2
	if (rv) {

a2376 1

a2390 1

a2404 1

d2423 1
a2423 1
	return 0;
d2435 1
a2435 1
 * => never a need to flush amap layer since the anonymous memory has
d2444 1
a2444 1
	struct vm_map *map;
d2448 1
a2448 1
	struct vm_map_entry *current, *entry;
d2467 1
a2467 1
		return EFAULT;
d2477 1
a2477 1
			return EINVAL;
d2484 1
a2484 1
			return EFAULT;
d2488 2
a2489 1
	error = 0;
d2589 1
a2589 1
				panic("uvm_map_clean: weird flags");
d2608 1
a2608 1
				error = EIO;
d2613 1
a2613 1
	return (error);
d2626 15
a2640 15
	struct vm_map * map;
	vaddr_t start, end;
	vm_prot_t protection;
{
	struct vm_map_entry *entry;
	struct vm_map_entry *tmp_entry;

	if (!uvm_map_lookup_entry(map, start, &tmp_entry)) {
		return(FALSE);
	}
	entry = tmp_entry;
	while (start < end) {
		if (entry == &map->header) {
			return(FALSE);
		}
d2646 3
a2648 3
		if (start < entry->start) {
			return(FALSE);
		}
d2654 10
a2663 7
		if ((entry->protection & protection) != protection) {
			return(FALSE);
		}
		start = entry->end;
		entry = entry->next;
	}
	return(TRUE);
d2674 1
a2674 1
uvmspace_alloc(min, max)
d2676 1
d2682 1
a2682 1
	uvmspace_init(vm, NULL, min, max);
d2694 1
a2694 1
uvmspace_init(vm, pmap, min, max)
d2698 1
d2703 3
a2705 1
	uvm_map_setup(&vm->vm_map, min, max, VM_MAP_PAGEABLE);
d2711 1
d2739 1
a2739 1
	struct proc *p;
d2751 1
a2751 1
	p->p_vmspace = nvm;
d2769 1
a2769 1
	struct vm_map *map = &ovm->vm_map;
a2790 1

a2798 1

a2805 1

a2810 1

d2815 2
d2824 2
a2825 2

		nvm = uvmspace_alloc(start, end);
d2849 1
a2849 1
	struct vm_map_entry *dead_entries;
a2853 1

a2858 1

d2866 1
a2866 1
			uvm_unmap_remove(&vm->vm_map,
d2894 6
a2899 6
	struct vm_map *old_map = &vm1->vm_map;
	struct vm_map *new_map;
	struct vm_map_entry *old_entry;
	struct vm_map_entry *new_entry;
	pmap_t new_pmap;
	boolean_t protect_child;
d2904 2
a2905 1
	vm2 = uvmspace_alloc(old_map->min_offset, old_map->max_offset);
d2922 6
a2928 3
		KASSERT(!UVM_ET_ISSUBMAP(old_entry));
		KASSERT(UVM_ET_ISCOPYONWRITE(old_entry) ||
			!UVM_ET_ISNEEDSCOPY(old_entry));
a2931 1

a2934 1

a2937 1

d2942 1
d2953 1
a2953 1
				    0, 0);
a2967 1

d2969 1
d2982 1
a2982 1
			/*
d3000 1
a3000 1
			 * allocate new_entry, adjust reference counts.
d3036 1
a3036 1
			 *    process is sharing the amap with another
d3040 1
a3040 1
			 *    same amap with "needs_copy" set.  if the
d3043 1
a3043 1
			 *    a new amap.   this is wrong because the
d3049 1
a3049 1
			 *    amap_cow_now to avoid page faults in the
d3057 9
a3065 8
				if ((amap_flags(old_entry->aref.ar_amap) &
				     AMAP_SHARED) != 0 ||
				    VM_MAPENT_ISWIRED(old_entry)) {

					amap_copy(new_map, new_entry, M_WAITOK,
					    FALSE, 0, 0);
					/* XXXCDC: M_WAITOK ... ok? */
				}
d3081 1
a3081 1
			  /*
d3083 1
a3083 1
			   * (note that there is nothing to do if
d3091 1
a3091 1
			} else {
a3118 1
				pmap_update(old_map->pmap);
d3130 1
a3130 1
			     * we only need to protect the child if the
d3155 2
a3156 2
					 new_entry->end,
					 new_entry->protection &
a3157 1
			    pmap_update(new_pmap);
d3167 1
a3167 1
	vm_map_unlock(old_map);
d3179 1
a3179 1
	return(vm2);
d3195 1
a3195 1
	struct vm_map *map;
d3199 1
a3199 1
	struct vm_map_entry *entry;
d3224 1
a3224 1
		    (entry->etype & UVM_ET_COPYONWRITE) ? 'T' : 'F',
d3229 1
a3229 1
}
d3266 1
a3266 1
}
d3338 2
a3339 3
		int color = VM_PGCOLOR_BUCKET(pg);
		pgl = &uvm.page_free[fl].pgfl_buckets[color].pgfl_queues[
		    ((pg)->flags & PG_ZERO) ? PGFL_ZEROS : PGFL_UNKNOWN];
d3341 2
a3342 1
		pgl = &uvm.page_inactive;
@


1.34.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.37 2002/01/23 00:39:48 art Exp $	*/
d359 2
a360 1
	    0, 0, 0, "vmsppl", &pool_allocator_nointr);
d362 2
a363 1
	    0, 0, 0, "vmmpepl", &pool_allocator_nointr);
d365 1
a365 1
	    0, 0, 0, "vmmpekpl", NULL);
a2460 2
int	amap_clean_works = 1;	/* XXX for now, just in case... */

d2498 4
a2501 2
		if (end > current->end && (current->next == &map->header ||
		    current->end != current->next->start)) {
a2523 4
		/* XXX for now, just in case... */
		if (amap_clean_works == 0)
			goto flush_object;

a2575 1
#ifdef UBC
a2577 6
#else
				/* zap all mappings for the page. */
				pmap_page_protect(pg, VM_PROT_NONE);

				/* ...and deactivate the page. */
#endif
@


1.34.2.2
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.34.2.1 2002/01/31 22:55:51 niklas Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.114 2001/11/10 07:37:00 lukem Exp $	*/
a73 1
#define UVM_MAP
a80 2
#include <sys/mount.h>
#include <sys/vnode.h>
a92 1
extern struct vm_map *pager_map;
d266 2
a267 2
	memcpy(dst, src, ((char *)&src->uvm_map_entry_stop_copy) -
	   ((char *)src));
d524 1
a524 7
	 * detect a popular device driver bug.
	 */

	KASSERT(curproc != NULL || map->flags & VM_MAP_INTRSAFE);

	/*
	 * check sanity of protection code
d534 1
a534 11
	 * for pager_map, allocate the new entry first to avoid sleeping
	 * for memory while we have the map locked.
	 */

	new_entry = NULL;
	if (map == pager_map) {
		new_entry = uvm_mapent_alloc(map);
	}

	/*
	 * figure out where to put new VM range
d538 1
a538 4
		if (flags & UVM_FLAG_TRYLOCK) {
			if (new_entry) {
				uvm_mapent_free(new_entry);
			}
a539 1
		}
a545 3
		if (new_entry) {
			uvm_mapent_free(new_entry);
		}
d588 2
a589 1
	 * try and insert in map by extending previous entry, if possible.
d600 1
a600 1
			goto nomerge;
d603 1
a603 1
			goto nomerge;
d607 1
a607 1
			goto nomerge;
d611 1
a611 1
			goto nomerge;
d615 1
a615 1
			goto nomerge;
d625 1
a625 1
			goto nomerge;
a648 3
		if (new_entry) {
			uvm_mapent_free(new_entry);
		}
d650 1
d652 1
a652 2

nomerge:
d666 1
a666 1
	 * allocate new entry and link it in.
d669 1
a669 3
	if (new_entry == NULL) {
		new_entry = uvm_mapent_alloc(map);
	}
a691 1

a695 1

d709 1
a709 1
	 * Update the free space hint
d976 1
d1016 1
a1016 1
	*entry_list = NULL;
d1026 1
d1036 1
a1036 1
		if (VM_MAPENT_ISWIRED(entry)) {
a1037 2
		}
		if ((map->flags & VM_MAP_PAGEABLE) == 0) {
d1039 4
a1042 6
			/*
			 * if the map is non-pageable, any pages mapped there
			 * must be wired and entered with pmap_kenter_pa(),
			 * and we should free any such pages immediately.
			 * this is mostly used for kmem_map and mb_map.
			 */
d1044 2
a1045 4
			uvm_km_pgremove_intrsafe(entry->start, entry->end);
			pmap_kremove(entry->start, len);
		} else if (UVM_ET_ISOBJ(entry) &&
			   UVM_OBJ_IS_KERN_OBJECT(entry->object.uvm_obj)) {
d1073 3
d1084 12
a1095 5
			pmap_remove(pmap_kernel(), entry->start,
			    entry->start + len);
			uvm_km_pgremove(entry->object.uvm_obj,
			    entry->start - vm_map_min(kernel_map),
			    entry->end - vm_map_min(kernel_map));
d1103 3
a1105 2
			entry->object.uvm_obj = NULL;
		} else if (UVM_ET_ISOBJ(entry) || entry->aref.ar_amap) {
d1116 1
a1116 1
		 * that we've nuked.  then go to next entry.
d1121 1
a1121 1
		/* critical!  prevents stale hint */
d1128 1
a1128 1
		entry = next;
d1705 1
a1776 16
		/*
		 * Don't allow VM_PROT_EXECUTE to be set on entries that
		 * point to vnodes that are associated with a NOEXEC file
		 * system.
		 */
		if (UVM_ET_ISOBJ(current) &&
		    UVM_OBJ_IS_VNODE(current->object.uvm_obj)) {
			struct vnode *vp =
			    (struct vnode *) current->object.uvm_obj;

			if ((new_prot & VM_PROT_EXECUTE) != 0 &&
			    (vp->v_mount->mnt_flag & MNT_NOEXEC) != 0) {
				error = EACCES;
				goto out;
			}
		}
a1802 16

#ifdef notyet /* XXXART */
			/*
			 * If this entry points at a vnode, and the
			 * protection includes VM_PROT_EXECUTE, mark
			 * the vnode as VEXECMAP.
			 */
			if (UVM_ET_ISOBJ(current)) {
				struct uvm_object *uobj =
				    current->object.uvm_obj;

				if (UVM_OBJ_IS_VNODE(uobj) &&
				    (current->protection & VM_PROT_EXECUTE))
					vn_markexec((struct vnode *) uobj);
			}
#endif
d1878 1
a1878 1
	vm_map_lock(map);
d1883 1
a1883 1
	}  else {
d1886 1
d2459 2
d2474 1
a2474 1
	int error, refs;
d2498 2
a2499 4
		if (end <= current->end) {
			break;
		}
		if (current->end != current->next->start) {
d2522 4
d2578 1
d2581 6
d2614 3
d2630 1
a2630 1
			error = (uobj->pgops->pgo_put)(uobj, offset,
d2632 4
d2922 1
d3003 10
d3105 2
d3120 8
d3130 3
a3132 2
			  if (old_entry->aref.ar_amap &&
			      !UVM_ET_ISNEEDSCOPY(old_entry)) {
d3142 38
d3181 1
d3227 1
d3230 4
d3277 3
a3279 2
	TAILQ_FOREACH(pg, &uobj->memq, listq) {
		cnt++;
d3281 1
a3281 1
		if ((cnt % 3) == 0) {
d3285 1
a3285 1
	if ((cnt % 3) != 0) {
d3295 1
a3295 1
	"\20\1BUSY\2WANTED\3TABLED\4CLEAN\5PAGEOUT\6RELEASED\7FAKE\10RDONLY"
d3298 1
a3298 1
	"\20\1FREE\2INACTIVE\3ACTIVE\5ANON\6AOBJ";
d3315 2
a3316 2
	(*pr)("  flags=%s, pqflags=%s, wire_count=%d, pa=0x%lx\n",
	    pgbuf, pqbuf, pg->wire_count, (long)pg->phys_addr);
@


1.34.2.3
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.34.2.2 2002/02/02 03:28:26 art Exp $	*/
a73 4
/*
 * XXX - Since there is stuff that pulls in uvm.h and tree.h behind our back,
 *  we need to define those here.
 */
a74 2
#define RB_AUGMENT(x) uvm_rb_augment(x)

d89 1
d97 1
a140 1
	uvm_rb_insert(map, entry); \
a151 1
	uvm_rb_remove(map, entry); \
d185 6
a190 170
static struct vm_map_entry *uvm_mapent_alloc(struct vm_map *);
static void uvm_mapent_copy(struct vm_map_entry *, struct vm_map_entry *);
static void uvm_mapent_free(struct vm_map_entry *);
static void uvm_map_entry_unwire(struct vm_map *, struct vm_map_entry *);
static void uvm_map_reference_amap(struct vm_map_entry *, int);
static void uvm_map_unreference_amap(struct vm_map_entry *, int);

int uvm_map_spacefits(struct vm_map *, vaddr_t *, vsize_t, struct vm_map_entry *,
    voff_t, vsize_t);

int _uvm_tree_sanity(struct vm_map *map, char *name);
static int uvm_rb_subtree_space(struct vm_map_entry *);

static __inline int
uvm_compare(struct vm_map_entry *a, struct vm_map_entry *b)
{
	if (a->start < b->start)
		return (-1);
	else if (a->start > b->start)
		return (1);
	
	return (0);
}


static __inline void
uvm_rb_augment(struct vm_map_entry *entry)
{
	entry->space = uvm_rb_subtree_space(entry);
}

RB_PROTOTYPE(uvm_tree, vm_map_entry, rb_entry, uvm_compare);

RB_GENERATE(uvm_tree, vm_map_entry, rb_entry, uvm_compare);

static __inline int
uvm_rb_space(struct vm_map *map, struct vm_map_entry *entry)
{
	struct vm_map_entry *next;
	vaddr_t space;

	if ((next = entry->next) == &map->header)
		space = map->max_offset - entry->end;
	else {
		KASSERT(next);
		space = next->start - entry->end;
	}
	return (space);
}
		
static int
uvm_rb_subtree_space(struct vm_map_entry *entry)
{
	vaddr_t space, tmp;

	space = entry->ownspace;
	if (RB_LEFT(entry, rb_entry)) {
		tmp = RB_LEFT(entry, rb_entry)->space;
		if (tmp > space)
			space = tmp;
	}

	if (RB_RIGHT(entry, rb_entry)) {
		tmp = RB_RIGHT(entry, rb_entry)->space;
		if (tmp > space)
			space = tmp;
	}

	return (space);
}

static __inline void
uvm_rb_fixup(struct vm_map *map, struct vm_map_entry *entry)
{
	/* We need to traverse to the very top */
	do {
		entry->ownspace = uvm_rb_space(map, entry);
		entry->space = uvm_rb_subtree_space(entry);
	} while ((entry = RB_PARENT(entry, rb_entry)) != NULL);
}

static __inline void
uvm_rb_insert(struct vm_map *map, struct vm_map_entry *entry)
{
	vaddr_t space = uvm_rb_space(map, entry);
	struct vm_map_entry *tmp;

	entry->ownspace = entry->space = space;
	tmp = RB_INSERT(uvm_tree, &(map)->rbhead, entry);
#ifdef DIAGNOSTIC
	if (tmp != NULL)
		panic("uvm_rb_insert: duplicate entry?");
#endif
	uvm_rb_fixup(map, entry);
	if (entry->prev != &map->header)
		uvm_rb_fixup(map, entry->prev);
}

static __inline void
uvm_rb_remove(struct vm_map *map, struct vm_map_entry *entry)
{
	struct vm_map_entry *parent;

	parent = RB_PARENT(entry, rb_entry);
	RB_REMOVE(uvm_tree, &(map)->rbhead, entry);
	if (entry->prev != &map->header)
		uvm_rb_fixup(map, entry->prev);
	if (parent)
		uvm_rb_fixup(map, parent);
}

#define uvm_tree_sanity(x,y)

int
_uvm_tree_sanity(struct vm_map *map, char *name)
{
	struct vm_map_entry *tmp, *trtmp;
	int n = 0, i = 1;

	RB_FOREACH(tmp, uvm_tree, &map->rbhead) {
		if (tmp->ownspace != uvm_rb_space(map, tmp)) {
			printf("%s: %d/%d ownspace %x != %x %s\n",
			    name, n + 1, map->nentries,
			    tmp->ownspace, uvm_rb_space(map, tmp),
			    tmp->next == &map->header ? "(last)" : "");
			goto error;
		}
	}
	trtmp = NULL;
	RB_FOREACH(tmp, uvm_tree, &map->rbhead) {
		if (tmp->space != uvm_rb_subtree_space(tmp)) {
			printf("%s: space %d != %d\n",
			    name, tmp->space, uvm_rb_subtree_space(tmp));
			goto error;
		}
		if (trtmp != NULL && trtmp->start >= tmp->start) {
			printf("%s: corrupt: %p >= %p\n",
			    name, trtmp->start, tmp->start);
			goto error;
		}
		n++;

	    trtmp = tmp;
	}

	if (n != map->nentries) {
		printf("%s: nentries: %d vs %d\n",
		    name, n, map->nentries);
		goto error;
	}

	for (tmp = map->header.next; tmp && tmp != &map->header;
	    tmp = tmp->next, i++) {
		trtmp = RB_FIND(uvm_tree, &map->rbhead, tmp);
		if (trtmp != tmp) {
			printf("%s: lookup: %d: %p - %p: %p\n",
			    name, i, tmp, trtmp,
			    RB_PARENT(tmp, rb_entry));
			goto error;
		}
	}

	return (0);
 error:
#ifdef	DDB
	/* handy breakpoint location for error case */
	__asm(".globl treesanity_label\ntreesanity_label:");
#endif
	return (-1);
}
a393 2
	uvm_tree_sanity(map, "clip_start entry");

a406 2

	/* Does not change order for the RB tree */
a424 2

	uvm_tree_sanity(map, "clip_start leave");
a444 1
	uvm_tree_sanity(map, "clip_end entry");
a459 2
	
	uvm_rb_fixup(map, entry);
a472 1
	uvm_tree_sanity(map, "clip_end leave");
a526 2
	uvm_tree_sanity(map, "map entry");

a670 1
		uvm_rb_fixup(map, prev_entry);
a672 2
		uvm_tree_sanity(map, "map leave 2");

a748 2
	uvm_tree_sanity(map, "map leave");

a769 1
	int			use_tree = 0;
a810 3

		if (map->nentries > 30)
			use_tree = 1;
a818 28
		use_tree = 1;
	}

	uvm_tree_sanity(map, __FUNCTION__);

	if (use_tree) {
		struct vm_map_entry *prev = &map->header;
		cur = RB_ROOT(&map->rbhead);

		/*
		 * Simple lookup in the tree.  Happens when the hint is
		 * invalid, or nentries reach a threshold.
		 */
		while (cur) {
			if (address >= cur->start) {
				if (address < cur->end) {
					*entry = cur;
					SAVE_HINT(map, map->hint, cur);
					return (TRUE);
				}
				prev = cur;
				cur = RB_RIGHT(cur, rb_entry);
			} else
				cur = RB_LEFT(cur, rb_entry);
		}
		*entry = prev;
		UVMHIST_LOG(maphist,"<- failed!",0,0,0,0);
		return (FALSE);
a842 1

a849 35
 * Checks if address pointed to be phint fits into the empty
 * space before the vm_map_entry after.  Takes aligment and
 * offset into consideration.
 */

int
uvm_map_spacefits(struct vm_map *map, vaddr_t *phint, vsize_t length,
    struct vm_map_entry *after, voff_t uoffset, vsize_t align)
{
	vaddr_t hint = *phint;
	vaddr_t end;

#ifdef PMAP_PREFER
	/*
	 * push hint forward as needed to avoid VAC alias problems.
	 * we only do this if a valid offset is specified.
	 */
	if (uoffset != UVM_UNKNOWN_OFFSET)
		PMAP_PREFER(uoffset, &hint);
#endif
	if (align != 0)
		if ((hint & (align - 1)) != 0)
			hint = roundup(hint, align);
	*phint = hint;

	end = hint + length;
	if (end > map->max_offset || end < hint)
		return (FALSE);
	if (after != NULL && after != &map->header && after->start < end)
		return (FALSE);
	
	return (TRUE);
}

/*
a873 1
	struct vm_map_entry *child, *prev = NULL;
a882 2
	uvm_tree_sanity(map, "map_findspace entry");

a923 97
	if (flags & UVM_FLAG_FIXED) {
		end = hint + length;
		if (end > map->max_offset || end < hint) {
			UVMHIST_LOG(maphist,"<- failed (off end)", 0,0,0,0);
			goto error;
		}
		next = entry->next;
		if (next == &map->header || next->start >= end)
			goto found;
		UVMHIST_LOG(maphist,"<- fixed mapping failed", 0,0,0,0);
		return(NULL); /* only one shot at it ... */
	}

	/* Try to find the space in the red-black tree */

	/* Check slot before any entry */
	if (uvm_map_spacefits(map, &hint, length, entry->next, uoffset, align))
		goto found;
	
	/* If there is not enough space in the whole tree, we fail */
	tmp = RB_ROOT(&map->rbhead);
	if (tmp == NULL || tmp->space < length)
		goto error;

	/* Find an entry close to hint that has enough space */
	for (; tmp;) {
		if (tmp->end >= hint &&
		    (prev == NULL || tmp->end < prev->end)) {
			if (tmp->ownspace >= length)
				prev = tmp;
			else if ((child = RB_RIGHT(tmp, rb_entry)) != NULL &&
			    child->space >= length)
				prev = tmp;
		}
		if (tmp->end < hint)
			child = RB_RIGHT(tmp, rb_entry);
		else if (tmp->end > hint)
			child = RB_LEFT(tmp, rb_entry);
		else {
			if (tmp->ownspace >= length)
				break;
			child = RB_RIGHT(tmp, rb_entry);
		}
		if (child == NULL || child->space < length)
			break;
		tmp = child;
	}
	
	if (tmp != NULL && hint < tmp->end + tmp->ownspace) {
		/* 
		 * Check if the entry that we found satifies the
		 * space requirement
		 */
		if (hint < tmp->end)
			hint = tmp->end;
		if (uvm_map_spacefits(map, &hint, length, tmp->next, uoffset,
			align)) {
			entry = tmp;
			goto found;
		} else if (tmp->ownspace >= length)
			goto listsearch;
	}
	if (prev == NULL)
		goto error;
	
	hint = prev->end;
	if (uvm_map_spacefits(map, &hint, length, prev->next, uoffset,
		align)) {
		entry = prev;
		goto found;
	} else if (prev->ownspace >= length)
		goto listsearch;
	
	tmp = RB_RIGHT(prev, rb_entry);
	for (;;) {
		KASSERT(tmp && tmp->space >= length);
		child = RB_LEFT(tmp, rb_entry);
		if (child && child->space >= length) {
			tmp = child;
			continue;
		}
		if (tmp->ownspace >= length)
			break;
		tmp = RB_RIGHT(tmp, rb_entry);
	}
	
	hint = tmp->end;
	if (uvm_map_spacefits(map, &hint, length, tmp->next, uoffset, align)) {
		entry = tmp;
		goto found;
	}

	/* 
	 * The tree fails to find an entry because of offset or alignment
	 * restrictions.  Search the list instead.
	 */
 listsearch:
d946 1
d961 8
a968 1
			goto error;
d973 4
a977 1
 found:
a981 10

 error:
	if (align != 0) {
		UVMHIST_LOG(maphist,
		    "calling recursively, no align",
		    0,0,0,0);
		return (uvm_map_findspace(map, orig_hint,
			    length, result, uobj, uoffset, 0, flags));
	}
	return (NULL);
a1010 2
	uvm_tree_sanity(map, "unmap_remove entry");

a1157 2
	uvm_tree_sanity(map, "unmap_remove leave");

a1281 2
	uvm_tree_sanity(map, "map_replace entry");

d1340 1
a1340 1
		last = newents->prev;		/* we expect this */
a1348 4

		/* Fix RB tree */
		uvm_rb_remove(map, oldent);

a1352 11
		/* Fixup the RB tree */
		{
			int i;
			struct vm_map_entry *tmp;

			tmp = newents;
			for (i = 0; i < nnewents && tmp; i++) {
				uvm_rb_insert(map, tmp);
				tmp = tmp->next;
			}
		}
a1364 2
	uvm_tree_sanity(map, "map_replace leave");

a1409 3
	uvm_tree_sanity(srcmap, "map_extract src enter");
	uvm_tree_sanity(dstmap, "map_extract dst enter");

a1687 4

	uvm_tree_sanity(srcmap, "map_extract src leave");
	uvm_tree_sanity(dstmap, "map_extract dst leave");

a1698 4

	uvm_tree_sanity(srcmap, "map_extract src err leave");
	uvm_tree_sanity(dstmap, "map_extract dst err leave");

a2873 1
		uvm_tree_sanity(map, "resize enter");
a2874 3
		if (map->header.prev != &map->header)
			uvm_rb_fixup(map, map->header.prev);
		uvm_tree_sanity(map, "resize leave");
d3196 1
a3196 1
	int (*pr)(const char *, ...);
d3233 1
a3233 1
	int (*pr)(const char *, ...);
d3275 1
a3275 1
	int (*pr)(const char *, ...);
@


1.34.2.4
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.34.2.3 2002/06/11 03:33:03 art Exp $	*/
d201 2
a202 2
int _uvm_tree_sanity(vm_map_t map, const char *name);
static vsize_t		uvm_rb_subtree_space(vm_map_entry_t);
a301 3
#ifdef DEBUG
#define uvm_tree_sanity(x,y) _uvm_tree_sanity(x,y)
#else
a302 1
#endif
d305 1
a305 1
_uvm_tree_sanity(struct vm_map *map, const char *name)
a391 1
		splassert(IPL_NONE);
a394 1
		splassert(IPL_NONE);
d605 2
a606 2
 *	the ending address, if it doesn't we split the reference
 * 
d1013 1
a1013 1
	uvm_tree_sanity(map, __func__);
d3479 2
a3480 2
			 *    parent and child process are referring to the
			 *    same amap with "needs_copy" set.  if the 
d3593 1
a3593 1
	(*pr)("\t#ent=%d, sz=%u, ref=%d, version=%u, flags=0x%x\n",
@


1.34.2.5
log
@Fix merge botches.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.34.2.4 2002/10/29 00:36:50 art Exp $	*/
d201 2
a202 2
int _uvm_tree_sanity(struct vm_map *map, const char *name);
static vsize_t		uvm_rb_subtree_space(struct vm_map_entry *);
d226 1
a226 1
static __inline vsize_t
d241 1
a241 1
static vsize_t
@


1.34.2.6
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.34.2.5 2002/10/29 02:12:53 art Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.120 2002/09/22 07:21:29 chs Exp $	*/
a705 1
	int error;
d842 1
a842 10
		if (prev_entry->aref.ar_amap) {
			error = amap_extend(prev_entry, size);
			if (error) {
				vm_map_unlock(map);
				if (new_entry) {
					uvm_mapent_free(new_entry);
				}
				return error;
			}
		}
a850 1

d854 4
d1518 1
a1518 3
	if ((map->flags & VM_MAP_DYING) == 0) {
		pmap_update(vm_map_pmap(map));
	}
d1564 8
a1571 5
		KASSERT(!UVM_ET_ISSUBMAP(first_entry));
		if (UVM_ET_ISOBJ(first_entry) &&
		    first_entry->object.uvm_obj->pgops->pgo_detach) {
			(*first_entry->object.uvm_obj->pgops->pgo_detach)
				(first_entry->object.uvm_obj);
d2532 1
a2532 1
				    ((entry->max_protection & VM_PROT_WRITE) ||
d2585 1
a2585 1
			    VM_FAULT_WIREMAX, entry->max_protection);
d2796 1
a2796 1
				    ((entry->max_protection & VM_PROT_WRITE) ||
d2822 1
a2822 1
			    VM_FAULT_WIREMAX, entry->max_protection);
d2981 1
d2991 4
a2994 1
			 * In these first 3 cases, we just deactivate the page.
d3001 9
d3011 3
a3013 3
				 * skip the page if it's loaned or wired,
				 * since it shouldn't be on a paging queue
				 * at all in these cases.
d3016 2
a3017 3
				uvm_lock_pageq();
				if (pg->loan_count != 0 ||
				    pg->wire_count != 0) {
d3023 2
d3027 1
d3042 1
a3042 1
				/* skip the page if it's wired */
a3059 3
		 * note that we must always clean object pages before
		 * freeing them since otherwise we could reveal stale
		 * data from files.
d3067 1
a3067 1
			    offset + size, flags | PGO_CLEANIT);
d3252 1
d3254 1
a3306 1
	struct vm_map *map;
d3310 1
a3310 3
	if (--vm->vm_refcnt > 0) {
		return;
	}
d3312 5
a3316 4
	/*
	 * at this point, there should be no other references to the map.
	 * delete all of the mappings, then destroy the pmap.
	 */
a3317 2
	map = &vm->vm_map;
	map->flags |= VM_MAP_DYING;
d3319 15
a3333 9
	/* Get rid of any SYSV shared memory segments. */
	if (vm->vm_shm != NULL)
		shmexit(vm);
#endif
	if (map->nentries) {
		uvm_unmap_remove(map, map->min_offset, map->max_offset,
		    &dead_entries);
		if (dead_entries != NULL)
			uvm_unmap_detach(dead_entries, 0);
d3335 1
a3335 2
	pmap_destroy(map->pmap);
	pool_put(&uvm_vmspace_pool, vm);
@


1.34.2.7
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a430 1
		splassert(IPL_NONE);
a432 1
		splassert(IPL_NONE);
a1117 19
 * uvm_map_hint: return the beginning of the best area suitable for
 * creating a new mapping with "prot" protection.
 */
vaddr_t
uvm_map_hint(struct proc *p, vm_prot_t prot)
{
#ifdef __i386__
	/*
	 * If executable skip first two pages, otherwise start
	 * after data + heap region.
	 */
	if ((prot & VM_PROT_EXECUTE) &&
	    ((vaddr_t)p->p_vmspace->vm_daddr >= I386_MAX_EXE_ADDR))
		return (round_page(PAGE_SIZE*2));
#endif
	return (round_page((vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ));
}

/*
d1488 1
a1488 1
			    entry->end);
d3569 1
@


1.34.2.8
log
@add VEXECMAP.  also make sure to modify filepages count only in the not
execpages case in uvm_pageremove().
this actually appears to solve the swap freak out problems.  sitting on it for
a long time, never checked if it worked.  sigh.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.34.2.7 2003/05/19 22:41:29 tedu Exp $	*/
d2266 1
a2266 1
#if 1
@


1.33
log
@Sync in more uvm from NetBSD. Mostly just cosmetic stuff.
Contains also support for page coloring.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.32 2001/11/28 13:47:39 art Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.99 2001/06/02 18:09:26 chs Exp $	*/
d80 1
d109 1
a193 2
 *
 * => XXX: static pool for kernel map?
d202 1
a202 2
	UVMHIST_FUNC("uvm_mapent_alloc");
	UVMHIST_CALLED(maphist);
d204 2
a205 7
	if ((map->flags & VM_MAP_INTRSAFE) == 0 &&
	    map != kernel_map && kernel_map != NULL /* XXX */) {
		me = pool_get(&uvm_map_entry_pool, PR_WAITOK);
		me->flags = 0;
		/* me can't be null, wait ok */
	} else {
		s = splvm();	/* protect kentry_free list with splvm */
d211 5
a215 2
		if (!me)
	panic("mapent_alloc: out of static map entries, check MAX_KMAPENT");
d217 6
d225 2
a226 3
	UVMHIST_LOG(maphist, "<- new entry=0x%x [kentry=%d]",
		me, ((map->flags & VM_MAP_INTRSAFE) != 0 || map == kernel_map)
		? TRUE : FALSE, 0, 0);
a231 2
 *
 * => XXX: static pool for kernel map?
d239 2
a240 2
	UVMHIST_FUNC("uvm_mapent_free");
	UVMHIST_CALLED(maphist);
d243 2
a244 4
	if ((me->flags & UVM_MAP_STATIC) == 0) {
		pool_put(&uvm_map_entry_pool, me);
	} else {
		s = splvm();	/* protect kentry_free list with splvm */
d250 4
d364 2
d1132 1
a1132 1
	pmap_update();
d1628 1
a1628 1
		pmap_update();
d1840 1
a1840 1
	pmap_update();
d2607 1
a2607 1
				panic("uvm_map_clean: wierd flags");
d2689 1
a2689 1
uvmspace_alloc(min, max, pageable)
a2690 1
	int pageable;
d2696 1
a2696 1
	uvmspace_init(vm, NULL, min, max, pageable);
d2708 1
a2708 1
uvmspace_init(vm, pmap, min, max, pageable)
a2711 1
	boolean_t pageable;
d2716 1
a2716 1
	uvm_map_setup(&vm->vm_map, min, max, pageable ? VM_MAP_PAGEABLE : 0);
d2837 1
a2837 2
		nvm = uvmspace_alloc(start, end,
			 (map->flags & VM_MAP_PAGEABLE) ? TRUE : FALSE);
d2918 1
a2918 2
	vm2 = uvmspace_alloc(old_map->min_offset, old_map->max_offset,
		      (old_map->flags & VM_MAP_PAGEABLE) ? TRUE : FALSE);
d3130 1
a3130 1
				pmap_update();
d3170 1
a3170 1
			    pmap_update();
@


1.32
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.31 2001/11/12 01:26:09 art Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.94 2001/03/15 06:10:57 chs Exp $	*/
d4 1
a4 1
/* 
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and 
d48 1
a48 1
 * 
d54 3
a56 3
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
d58 1
a58 1
 * 
d179 6
a184 6
static vm_map_entry_t	uvm_mapent_alloc __P((vm_map_t));
static void		uvm_mapent_copy __P((vm_map_entry_t,vm_map_entry_t));
static void		uvm_mapent_free __P((vm_map_entry_t));
static void		uvm_map_entry_unwire __P((vm_map_t, vm_map_entry_t));
static void		uvm_map_reference_amap __P((vm_map_entry_t, int));
static void		uvm_map_unreference_amap __P((vm_map_entry_t, int));
d196 1
a196 1
static __inline vm_map_entry_t
d198 1
a198 1
	vm_map_t map;
d200 1
a200 1
	vm_map_entry_t me;
d222 1
a222 1
	UVMHIST_LOG(maphist, "<- new entry=0x%x [kentry=%d]", 
d236 1
a236 1
	vm_map_entry_t me;
d241 1
a241 1
	UVMHIST_LOG(maphist,"<- freeing map entry=0x%x [flags=%d]", 
d261 2
a262 2
	vm_map_entry_t src;
	vm_map_entry_t dst;
d264 2
a265 2

	memcpy(dst, src, ((char *)&src->uvm_map_entry_stop_copy) - ((char*)src));
d276 2
a277 2
	vm_map_t map;
	vm_map_entry_t entry;
a278 1

d289 1
a289 1
	vm_map_entry_t entry;
d292 1
a292 1
    amap_ref(entry->aref.ar_amap, entry->aref.ar_pageoff,
d298 1
a298 1
 * wrapper for calling amap_unref() 
d302 1
a302 1
	vm_map_entry_t entry;
d305 1
a305 1
    amap_unref(entry->aref.ar_amap, entry->aref.ar_pageoff,
d312 1
a312 1
 * and init the static pool of vm_map_entry_t's for the kernel here.
d316 1
a316 1
uvm_map_init() 
d371 1
a371 1
 * 
d377 5
a381 4
void uvm_map_clip_start(map, entry, start)
	vm_map_t       map;
	vm_map_entry_t entry;
	vaddr_t    start;
d383 1
a383 1
	vm_map_entry_t new_entry;
d397 1
a397 1
	new_entry->end = start; 
d413 1
a413 1
		if (UVM_ET_ISOBJ(entry) && 
d424 1
a424 1
 * 
d432 2
a433 2
	vm_map_t	map;
	vm_map_entry_t	entry;
d436 1
a436 1
	vm_map_entry_t	new_entry;
d486 1
a486 1
 *	
d501 1
a501 1
	vm_map_t map;
d509 1
a509 1
	vm_map_entry_t prev_entry, new_entry;
d526 1
a526 1
		UVMHIST_LOG(maphist, "<- prot. failure:  prot=0x%x, max=0x%x", 
d540 1
a540 1
	if ((prev_entry = uvm_map_findspace(map, *startp, size, startp, 
d562 2
a563 2
	 * [typically from uvm_map_reserve] or it is UVM_UNKNOWN_OFFSET.   in 
	 * either case we want to zero it  before storing it in the map entry 
d565 2
a566 2
	 * 
	 * if uobj is not null 
d592 1
a592 1
	if ((flags & UVM_FLAG_NOMERGE) == 0 && 
d603 1
a603 1
		if (prev_entry->protection != prot || 
d613 1
a613 1
			goto step3; 
d616 1
a616 1
		 * can't extend a shared amap.  note: no need to lock amap to 
d655 1
a655 1
	 * the number of times we missed a *possible* chance to merge more 
d659 1
a659 1
	    prev_entry->next != &map->header && 
d673 1
a673 1
	if (uobj) 
d694 1
a694 1
		vaddr_t to_add = (flags & UVM_FLAG_AMAPPAD) ? 
a702 1

a703 1

d729 1
a729 1
	vm_map_t	map;
d731 1
a731 1
	vm_map_entry_t		*entry;		/* OUT */
d733 2
a734 2
	vm_map_entry_t		cur;
	vm_map_entry_t		last;
d755 1
d767 1
d777 1
d781 1
d827 1
a827 1
vm_map_entry_t
d829 1
a829 1
	vm_map_t map;
d838 1
a838 1
	vm_map_entry_t entry, next, tmp;
d843 1
a843 1
	UVMHIST_LOG(maphist, "(map=0x%x, hint=0x%x, len=%d, flags=0x%x)", 
d874 1
a874 1
		if ((entry = map->first_free) != &map->header) 
d895 1
d897 1
d911 1
d956 1
a956 1
 * => caller must check alignment and size 
d964 3
a966 3
	vm_map_t map;
	vaddr_t start,end;
	vm_map_entry_t *entry_list;	/* OUT */
d968 1
a968 1
	vm_map_entry_t entry, first_entry, next;
d970 1
a970 2
	UVMHIST_FUNC("uvm_unmap_remove");
	UVMHIST_CALLED(maphist);
d980 1
a986 1

d1010 1
a1010 1
	 *       so that we don't block other threads.  
d1012 1
d1017 1
a1017 1
	 * break up the area into map entry sized regions and unmap.  note 
d1025 1
a1025 1
		UVM_MAP_CLIP_END(map, entry, end); 
d1041 1
d1067 2
a1068 2
			 * uvm_km_pgremove currently does the following: 
			 *   for pages in the kernel object in range: 
d1081 1
d1099 1
d1104 1
d1108 1
d1113 1
a1113 1
		 * remove entry from map and put it on our list of entries 
d1116 1
d1128 1
d1132 1
a1132 1
	 * references to the mapped objects.  
d1147 1
a1147 1
	vm_map_entry_t first_entry;
d1150 1
a1150 1
	vm_map_entry_t next_entry;
d1156 2
a1157 2
		    "  detach 0x%x: amap=0x%x, obj=0x%x, submap?=%d", 
		    first_entry, first_entry->aref.ar_amap, 
a1180 1

d1192 1
a1192 1
/* 
d1195 1
a1195 1
 * => we reserve space in a map by putting a dummy map entry in the 
d1204 1
a1204 1
	vm_map_t map;
d1210 1
a1210 1
	UVMHIST_FUNC("uvm_map_reserve"); UVMHIST_CALLED(maphist); 
d1228 1
a1228 1
	}     
d1235 1
a1235 1
 * uvm_map_replace: replace a reserved (blank) area of memory with 
d1238 1
a1238 1
 * => caller must WRITE-LOCK the map 
d1249 1
a1249 1
	vm_map_entry_t newents;
d1252 1
a1252 1
	vm_map_entry_t oldent, last;
d1266 1
a1266 1
	if (oldent->start != start || oldent->end != end || 
d1272 1
d1276 1
d1278 1
a1278 1
		vm_map_entry_t tmpent = newents;
d1312 1
a1312 2

		last = newents->prev;		/* we expect this */
d1365 1
a1365 1
	vm_map_t srcmap, dstmap;
d1372 2
a1373 2
	vm_map_entry_t chain, endchain, entry, orig_entry, newentry, deadentry;
	vm_map_entry_t oldentry;
d1403 1
a1403 1
	 * step 2: setup for the extraction process loop by init'ing the 
d1513 2
a1514 2
		newentry->protection = (flags & UVM_EXTRACT_FIXPROT) ? 
			entry->max_protection : entry->protection; 
d1539 1
a1539 1
		if ((flags & UVM_EXTRACT_CONTIG) && entry->end < end && 
d1558 1
a1558 1
	 * note usage of copy_ok: 
d1609 1
a1609 1
				pmap_remove(srcmap->pmap, entry->start, 
d1624 1
d1634 2
a1636 2
	else
		deadentry = NULL; /* XXX: gcc */
d1687 1
a1687 1
 * => to remove a submap, use uvm_unmap() on the main map 
d1696 1
a1696 1
	vm_map_t map, submap;
d1699 1
a1699 1
	vm_map_entry_t entry;
d1713 1
a1713 1
	if (entry != NULL && 
d1742 1
a1742 1
	vm_map_t map;
d1747 1
a1747 1
	vm_map_entry_t current, entry;
d1793 1
a1793 1
		 * update physical map if necessary.  worry about copy-on-write 
d1816 1
d1830 1
a1833 1

d1836 1
d1846 1
a1846 1
/* 
d1856 1
a1856 1
	vm_map_t map;
d1861 1
a1861 1
	vm_map_entry_t entry, temp_entry;
d1876 1
a1876 2
	vm_map_lock(map);
	
a1877 1
	
a1889 1

d1895 1
a1895 1
/* 
d1903 1
a1903 1
	vm_map_t map;
d1908 1
a1908 1
	vm_map_entry_t entry, temp_entry;
d1966 1
a1966 1
	vm_map_t map;
d1971 1
a1971 1
	vm_map_entry_t entry, start_entry, failed_entry;
d1985 1
a1985 1
	/* 
d1990 1
a1990 1
	 * making any changes.  
d2002 1
a2002 1
	/* 
d2011 1
a2011 1
		 * really wired down and that there are no holes.  
d2027 1
a2027 1
		/* 
d2051 1
a2051 1
	 *    be wired and increment its wiring count.  
d2079 1
a2079 1
				if (UVM_ET_ISNEEDSCOPY(entry) && 
d2083 1
a2083 1
					    start, end); 
d2093 1
a2093 1
		 * Check for holes 
d2103 1
a2103 1
			 * be undone, but the wired counts need to be restored. 
d2220 1
a2220 1
	vm_map_t map;
d2224 1
a2224 1
	vm_map_entry_t entry, failed_entry;
d2242 1
d2247 1
a2256 4

		/*
		 * end of unwire case!
		 */
d2260 1
d2264 1
d2269 1
d2273 1
d2333 1
d2341 1
d2343 1
a2343 1
				if (UVM_ET_ISNEEDSCOPY(entry) && 
d2372 1
d2378 1
d2384 2
a2385 1
	if (rv) {	/* failed? */
d2389 1
d2404 1
d2419 1
d2450 1
a2450 1
 * => never a need to flush amap layer since the anonymous memory has 
d2459 1
a2459 1
	vm_map_t map;
d2463 1
a2463 1
	vm_map_entry_t current, entry;
d2627 1
a2627 1
	return (error); 
d2640 3
a2642 3
	vm_map_t       map;
	vaddr_t    start, end;
	vm_prot_t      protection;
d2644 2
a2645 2
	vm_map_entry_t entry;
	vm_map_entry_t tmp_entry;
a2670 3

		/* go to next entry */

d2747 1
a2747 1
	struct proc *p; 
d2759 1
a2759 1
	p->p_vmspace = nvm; 
d2777 1
a2777 1
	vm_map_t map = &ovm->vm_map;
d2799 1
d2808 1
d2816 1
d2822 1
d2834 1
d2860 1
a2860 1
	vm_map_entry_t dead_entries;
d2865 1
d2871 1
d2907 6
a2912 6
	vm_map_t        old_map = &vm1->vm_map;
	vm_map_t        new_map;
	vm_map_entry_t  old_entry;
	vm_map_entry_t  new_entry;
	pmap_t          new_pmap;
	boolean_t	protect_child;
d2935 1
d2942 1
d2946 1
d2950 1
a2954 1

d2965 1
a2965 1
				    0, 0); 
d2980 1
a2981 1
				/* share reference */
d2994 1
a2994 1
			/* 
d3012 1
a3012 1
			 * allocate new_entry, adjust reference counts.  
d3048 1
a3048 1
			 *    process is sharing the amap with another 
d3052 1
a3052 1
			 *    same amap with "needs_copy" set.  if the 
d3055 1
a3055 1
			 *    a new amap.   this is wrong because the 
d3061 1
a3061 1
			 *    amap_cow_now to avoid page faults in the 
d3069 8
a3076 9

			  if ((amap_flags(old_entry->aref.ar_amap) & 
			       AMAP_SHARED) != 0 ||
			      VM_MAPENT_ISWIRED(old_entry)) {

			    amap_copy(new_map, new_entry, M_WAITOK, FALSE,
				      0, 0);
			    /* XXXCDC: M_WAITOK ... ok? */
			  }
d3092 1
a3092 1
			  /* 
d3094 1
a3094 1
			   * (note that there is nothing to do if 
d3102 1
a3102 1
			} else { 
d3130 1
d3142 1
a3142 1
			     * we only need to protect the child if the 
d3167 2
a3168 2
					 new_entry->end, 
					 new_entry->protection & 
d3170 1
d3180 1
a3180 1
	vm_map_unlock(old_map); 
d3192 1
a3192 1
	return(vm2);    
d3208 1
a3208 1
	vm_map_t map;
d3212 1
a3212 1
	vm_map_entry_t entry;
d3237 1
a3237 1
		    (entry->etype & UVM_ET_COPYONWRITE) ? 'T' : 'F', 
d3242 1
a3242 1
} 
d3279 1
a3279 1
} 
d3351 3
a3353 2
		pgl = &uvm.page_free[fl].pgfl_queues[((pg)->flags & PG_ZERO) ?
		    PGFL_ZEROS : PGFL_UNKNOWN];
d3355 1
a3355 2
		pgl = (pg->pqflags & PQ_SWAPBACKED) ?
		    &uvm.page_inactive_swp : &uvm.page_inactive_obj;
@


1.31
log
@Bring in more changes from NetBSD. Mostly pagedaemon improvements.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.30 2001/11/09 03:32:23 art Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.93 2001/02/11 01:34:23 eeh Exp $	*/
d528 1
a528 1
		return(KERN_PROTECTION_FAILURE);
d537 1
a537 1
			return(KERN_FAILURE);
d544 1
a544 1
		return (KERN_NO_SPACE);
d647 1
a647 1
		return (KERN_SUCCESS);
d718 1
a718 1
	return(KERN_SUCCESS);
d957 1
a957 1
int
a1124 1
	return(KERN_SUCCESS);
d1214 1
a1214 1
	    UVM_ADV_RANDOM, UVM_FLAG_NOMERGE)) != KERN_SUCCESS) {
d1687 1
a1687 1
	int result;
d1708 1
a1708 1
		result = KERN_SUCCESS;
d1710 1
a1710 1
		result = KERN_INVALID_ARGUMENT;
d1713 1
a1713 1
	return(result);
a1725 1
#define max(a,b)        ((a) > (b) ? (a) : (b))
d1735 1
a1735 1
	int rv = KERN_SUCCESS;
a1740 1

a1741 1

d1755 1
a1755 1
			rv = KERN_INVALID_ARGUMENT;
d1759 1
a1759 1
			rv = KERN_PROTECTION_FAILURE;
a1767 1

a1771 1

d1802 1
a1802 1
			    UVM_LK_ENTER|UVM_LK_EXIT) != KERN_SUCCESS) {
d1807 2
a1808 2
				 * the map, but will return the resource
				 * shortage condition regardless.
d1816 1
a1816 1
				rv = KERN_RESOURCE_SHORTAGE;
d1825 2
a1826 2
	UVMHIST_LOG(maphist, "<- done, rv=%d",rv,0,0,0);
	return (rv);
a1828 1
#undef  max
d1858 1
a1858 1
		return (KERN_INVALID_ARGUMENT);
d1880 1
a1880 1
	return(KERN_SUCCESS);
d1927 1
a1927 1
			return (KERN_INVALID_ARGUMENT);
d1935 1
a1935 1
	return (KERN_SUCCESS);
a1970 1

d1985 2
a1986 2
		UVMHIST_LOG(maphist,"<- done (INVALID ARG)",0,0,0,0);
		return (KERN_INVALID_ADDRESS);
d2009 2
a2010 3
				UVMHIST_LOG(maphist,
				    "<- done (INVALID UNWIRE ARG)",0,0,0,0);
				return (KERN_INVALID_ARGUMENT);
d2031 1
a2031 1
		return(KERN_SUCCESS);
d2101 1
a2101 1
			return (KERN_INVALID_ARGUMENT);
d2123 1
d2129 1
d2194 1
a2194 1
	return(KERN_SUCCESS);
d2242 1
a2242 1
		return (KERN_SUCCESS);
d2262 1
a2262 1
		return (KERN_SUCCESS);
d2298 1
a2298 1
		return (KERN_NO_SPACE);		/* XXX overloaded */
d2306 1
a2306 1
		return (KERN_NO_SPACE);		/* XXX overloaded */
d2349 1
a2349 1
	rv = KERN_SUCCESS;
d2416 1
a2416 1
	return (KERN_SUCCESS);
d2460 1
a2460 1
		return(KERN_INVALID_ADDRESS);
d2470 1
a2470 1
			return (KERN_INVALID_ARGUMENT);
d2477 1
a2477 1
			return (KERN_INVALID_ADDRESS);
d2481 1
a2481 2
	error = KERN_SUCCESS;

d2600 1
a2600 1
				error = KERN_FAILURE;
d2622 2
a2623 2
	 vm_map_entry_t entry;
	 vm_map_entry_t tmp_entry;
d2625 8
a2632 8
	 if (!uvm_map_lookup_entry(map, start, &tmp_entry)) {
		 return(FALSE);
	 }
	 entry = tmp_entry;
	 while (start < end) {
		 if (entry == &map->header) {
			 return(FALSE);
		 }
d2638 3
a2640 3
		 if (start < entry->start) {
			 return(FALSE);
		 }
d2646 10
a2655 10
		 if ((entry->protection & protection) != protection) {
			 return(FALSE);
		 }

		 /* go to next entry */

		 start = entry->end;
		 entry = entry->next;
	 }
	 return(TRUE);
a2694 1

a2695 1

a2700 1

a2803 2
	

d2853 1
a2853 1
			(void)uvm_unmap_remove(&vm->vm_map,
d2909 3
a2911 7
		if (UVM_ET_ISSUBMAP(old_entry))
		    panic("fork: encountered a submap during fork (illegal)");

		if (!UVM_ET_ISCOPYONWRITE(old_entry) &&
			    UVM_ET_ISNEEDSCOPY(old_entry))
	panic("fork: non-copy_on_write map entry marked needs_copy (illegal)");

@


1.30
log
@minor sync to NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.29 2001/11/07 02:55:50 art Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.86 2000/11/27 08:40:03 chs Exp $	*/
d95 1
d211 1
a211 1
		s = splimp();	/* protect kentry_free list with splimp */
d246 1
a246 1
		s = splimp();	/* protect kentry_free list with splimp */
d1589 1
a1589 1
				elen = min(end, entry->end) -
a2441 2
int	amap_clean_works = 1;	/* XXX for now, just in case... */

d2479 4
a2482 2
		if (end > current->end && (current->next == &map->header ||
		    current->end != current->next->start)) {
d2490 1
a2490 1
	for (current = entry; current->start < end; current = current->next) {
d2503 1
a2503 6
		if (amap == NULL ||
		    (flags & (PGO_DEACTIVATE|PGO_FREE)) == 0)
			goto flush_object;

		/* XXX for now, just in case... */
		if (amap_clean_works == 0)
a2506 1

d2508 2
a2509 5
		size = (end <= current->end ? end : current->end) -
		    start;

		for (/* nothing */; size != 0; size -= PAGE_SIZE,
		     offset += PAGE_SIZE) {
a2557 3
				/* zap all mappings for the page. */
				pmap_page_protect(pg, VM_PROT_NONE);

d2559 1
a2590 3
#ifdef DIAGNOSTIC
			panic("uvm_map_clean: unreachable code");
#endif
a2591 1

d2600 1
a2600 2
		size = (end <= current->end ? end : current->end) - start;

d2764 1
a2764 1
uvmspace_exec(p)
d2766 1
d2806 10
a2815 1
		uvm_unmap(map, VM_MIN_ADDRESS, VM_MAXUSER_ADDRESS);
d2824 1
a2824 1
		nvm = uvmspace_alloc(map->min_offset, map->max_offset, 
d2859 5
@


1.29
log
@Another sync of uvm to NetBSD. Just minor fiddling, no major changes.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.28 2001/11/07 01:18:01 art Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.85 2000/11/25 06:27:59 chs Exp $	*/
a3267 6
const char page_flagbits[] =
	"\20\4CLEAN\5BUSY\6WANTED\7TABLED\12FAKE\13FILLED\14DIRTY\15RELEASED"
	"\16FAULTING\17CLEANCHK";
const char page_pqflagbits[] =
	"\20\1FREE\2INACTIVE\3ACTIVE\4LAUNDRY\5ANON\6AOBJ";

d3272 6
d3295 2
a3296 2
	(*pr)("  uobject=%p, uanon=%p, offset=0x%lx loan_count=%d\n", 
	    pg->uobject, pg->uanon, pg->offset, pg->loan_count);
@


1.28
log
@Add an alignment argument to uvm_map that specifies an alignment hint
for the virtual address.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.27 2001/11/06 13:36:52 art Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.81 2000/09/13 15:00:25 thorpej Exp $	*/
d152 1
a152 1
#define SAVE_HINT(map,value) do { \
d154 2
a155 1
	(map)->hint = (value); \
d182 2
d283 27
d395 1
a395 1
				
d407 1
a407 1
				 
d579 1
a579 5
#ifdef DIAGNOSTIC
			if (UVM_OBJ_IS_KERN_OBJECT(uobj) == 0)
				panic("uvm_map: unknown offset with "
				    "non-kernel object");
#endif
d619 1
a619 1
		   
d624 1
a624 1
		
d796 1
a796 1
				SAVE_HINT(map, cur);
d806 1
a806 1
	SAVE_HINT(map, *entry);
d841 3
a843 8
		map, hint, length, flags);

#ifdef DIAGNOSTIC
	if ((align & (align - 1)) != 0)
		panic("uvm_map_findspace: alignment not power of 2");
	if ((flags & UVM_FLAG_FIXED) != 0 && align != 0)
		panic("uvm_map_findslace: fixed and alignment both specified");
#endif
d850 1
a851 1

d937 1
a937 1
	SAVE_HINT(map, entry);
d980 1
a980 1
		SAVE_HINT(map, entry->prev);
d1037 1
a1037 5
#ifdef DIAGNOSTIC
			if (vm_map_pmap(map) != pmap_kernel())
				panic("uvm_unmap_remove: kernel object "
				    "mapped by non-kernel map");
#endif
d1108 1
a1108 5
		/* XXX: need SAVE_HINT with three parms */
		simple_lock(&map->hint_lock);
		if (map->hint == entry)
		    map->hint = entry->prev;
		simple_unlock(&map->hint_lock);
d1134 1
a1134 1
uvm_unmap_detach(first_entry, amap_unref_flags)
d1136 1
a1136 1
	int amap_unref_flags;
d1142 1
a1142 10

#ifdef DIAGNOSTIC
		/*
		 * sanity check
		 */
		/* was part of vm_map_entry_delete() */
		if (VM_MAPENT_ISWIRED(first_entry))
			panic("unmap: still wired!");
#endif

d1154 1
a1154 1
			amap_unref(first_entry, amap_unref_flags);
d1159 1
a1159 1
		
a1169 3
		/*
		 * next entry
		 */
a1173 4

	/*
	 * done!
	 */
a1174 1
	return;
d1200 1
a1200 1
 
d1203 1
a1203 1
 
d1207 1
a1207 1
 
d1211 1
a1211 1
 
d1218 1
a1218 1
	
a1241 2
	UVMHIST_FUNC("uvm_map_replace");
	UVMHIST_CALLED(maphist);
d1246 1
a1246 1
	
d1250 1
a1250 1
	
d1303 1
a1303 1
		SAVE_HINT(map, newents);
d1316 1
a1316 1
		SAVE_HINT(map, oldent->prev);
d1365 1
a1369 1
#ifdef DIAGNOSTIC
a1374 6
	if ((start & PAGE_MASK) || (len & PAGE_MASK))
		panic("uvm_map_extract1");
	if (flags & UVM_EXTRACT_REMOVE)
		if (flags & (UVM_EXTRACT_CONTIG|UVM_EXTRACT_QREF))
			panic("uvm_map_extract2");
#endif
d1376 3
a1389 1

d1406 1
d1414 1
d1417 1
d1422 1
d1424 1
a1424 1
			SAVE_HINT(srcmap, entry->prev);
d1427 1
a1428 2
	} else {
		
d1438 1
a1442 1

d1449 1
a1449 1
		
d1465 1
d1510 1
a1510 1
			amap_ref(newentry, AMAP_SHARED |
a1536 1

a1543 1

d1550 1
a1550 1
	
a1551 1

a1559 1

a1560 1

a1562 1

a1564 1
		
d1576 1
a1576 1
			SAVE_HINT(srcmap, orig_entry->prev);
a1585 1

d1617 1
a1646 4

	/*
	 * done!
	 */
d1680 1
a1680 1
	 
a1687 1
	UVMHIST_FUNC("uvm_map_submap"); UVMHIST_CALLED(maphist);
d1692 1
a1692 1
 
d1696 1
a1696 2
	}             
	else {
a1703 4
		
		/*
		 * doit!
		 */
a1712 1

d1739 2
a1740 2
	map, start, end, new_prot);
	
d1744 1
a1744 1
	
d1774 1
a1774 1
		
d1827 1
a1827 1
	
d1874 1
a1874 1
	}  else {
a1879 1

a1880 1
		
a1907 1
	
a1908 1
	
a1934 2


a1935 1
		
d1972 2
a1973 6
	map, start, end, new_pageable);

#ifdef DIAGNOSTIC
	if ((map->flags & VM_MAP_PAGEABLE) == 0)
		panic("uvm_map_pageable: map %p not pageable", map);
#endif
d1991 1
a1991 1
	 
d2003 1
d2008 1
d2028 1
a2039 4

		/*
		 * end of unwire case!
		 */
d2065 2
a2066 1
			/* 
d2073 1
d2091 1
d2096 1
d2101 1
d2143 1
d2147 1
d2160 1
d2171 1
d2190 1
d2194 1
d2227 1
a2227 4
#ifdef DIAGNOSTIC
	if ((map->flags & VM_MAP_PAGEABLE) == 0)
		panic("uvm_map_pageable_all: map %p not pageable", map);
#endif
d2458 1
d2460 3
a2462 6
	map, start, end, flags);

#ifdef DIAGNOSTIC
	if ((flags & (PGO_FREE|PGO_DEACTIVATE)) == (PGO_FREE|PGO_DEACTIVATE))
		panic("uvm_map_clean: FREE and DEACTIVATE");
#endif
d2474 1
d2492 1
a2492 5

#ifdef DIAGNOSTIC
		if (start < current->start)
			panic("uvm_map_clean: hole");
#endif
d2501 1
d2531 1
d2538 1
d2557 1
d2559 1
a2559 6
#ifdef DIAGNOSTIC
					if (pg->uobject != NULL)
						panic("uvm_map_clean: "
						    "page anon vs. object "
						    "inconsistency");
#endif
d2564 1
a2564 6

#ifdef DIAGNOSTIC
				if (pg->uanon != anon)
					panic("uvm_map_clean: anon "
					    "inconsistency");
#endif
d2577 1
d2582 1
a2626 1

a2650 1

a2651 1
	 
d2656 1
a2656 1
		 
d2674 1
a2674 1
		 
d2757 1
a2757 1
 
d2761 1
a2761 1
 
d2970 1
a2970 1
				amap_ref(new_entry, AMAP_SHARED);
d3009 1
a3009 1
				amap_ref(new_entry, 0);
d3023 1
a3023 1
			
d3067 1
a3067 1
			
d3149 1
a3149 1
			  
a3189 13
 * uvm_map_print: print out a map 
 */

void
uvm_map_print(map, full)
	vm_map_t map;
	boolean_t full;
{

	uvm_map_printit(map, full, printf);
}

/*
d3218 2
a3219 1
		    (long long)entry->offset, entry->aref.ar_amap, entry->aref.ar_pageoff);
d3221 2
a3222 1
"\tsubmap=%c, cow=%c, nc=%c, prot(max)=%d/%d, inh=%d, wc=%d, adv=%d\n",
a3231 13
 * uvm_object_print: print out an object 
 */

void
uvm_object_print(uobj, full)
	struct uvm_object *uobj;
	boolean_t full;
{

	uvm_object_printit(uobj, full, printf);
}

/*
d3258 1
a3258 1
		(*pr)("<%p,0x%lx> ", pg, pg->offset);
a3274 13
 * uvm_page_print: print out a page
 */

void
uvm_page_print(pg, full)
	struct vm_page *pg;
	boolean_t full;
{

	uvm_page_printit(pg, full, printf);
}

/*
d3284 1
a3284 1
	struct vm_page *lcv;
d3314 1
a3314 1
			    (*pr)("  >>> ANON DOES NOT POINT HERE <<< (%p)\n", 
d3322 4
a3325 3
				for (lcv = uobj->memq.tqh_first ; lcv ;
				    lcv = lcv->listq.tqe_next) {
					if (lcv == pg) break;
d3327 1
a3327 1
				if (lcv)
d3340 2
a3341 3
	}
	else if (pg->pqflags & PQ_INACTIVE)
		pgl = (pg->pqflags & PQ_SWAPBACKED) ? 
d3343 1
a3343 1
	else if (pg->pqflags & PQ_ACTIVE)
d3345 1
a3345 1
	else
d3347 1
d3351 4
a3354 2
		for (lcv = pgl->tqh_first ; lcv ; lcv = lcv->pageq.tqe_next) {
			if (lcv == pg) break;
d3356 1
a3356 1
		if (lcv)
@


1.27
log
@More sync to NetBSD.
 - Use malloc/free instead of MALLOC/FREE for variable sized allocations.
 - Move the memory inheritance code to sys/mman.h and rename from VM_* to MAP_*
 - various cleanups and simplifications.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.26 2001/11/06 01:35:04 art Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.80 2000/08/01 00:53:11 wiz Exp $	*/
d459 6
d469 1
a469 1
uvm_map(map, startp, size, uobj, uoffset, flags)
d475 1
d510 1
a510 1
	    uobj, uoffset, flags & UVM_FLAG_FIXED)) == NULL) {
a784 1

d788 2
a789 2
 * => "hint" is a hint about where we want it, unless fixed is true
 *	(in which case we insist on using "hint").
d792 1
d799 1
a799 1
uvm_map_findspace(map, hint, length, result, uobj, uoffset, fixed)
d806 2
a807 1
	boolean_t fixed;
d810 1
a810 1
	vaddr_t end;
d814 16
a829 2
	UVMHIST_LOG(maphist, "(map=0x%x, hint=0x%x, len=%d, fixed=%d)", 
		map, hint, length, fixed);
d832 1
a832 1
		if (fixed) {
d849 1
a849 1
	if (!fixed && hint == map->min_offset) {
d855 1
a855 1
			if (fixed) {
d885 3
a887 2
		if (!fixed && uoffset != UVM_UNKNOWN_OFFSET)
		  PMAP_PREFER(uoffset, &hint);
d889 7
d899 7
d911 1
a911 1
		if (fixed) {
d1001 1
a1001 1
	
d1196 1
a1196 1
uvm_map_reserve(map, size, offset, raddr)
d1199 2
a1200 1
	vaddr_t offset;    /* hint for pmap_prefer */
d1216 1
a1216 1
	if (uvm_map(map, raddr, size, NULL, offset,
d1394 1
a1394 1
	if (uvm_map_reserve(dstmap, len, start, &dstaddr) == FALSE)
@


1.26
log
@Move the last content from vm/ to uvm/
The only thing left in vm/ are just dumb wrappers.
vm/vm.h includes uvm/uvm_extern.h
vm/pmap.h includes uvm/uvm_pmap.h
vm/vm_page.h includes uvm/uvm_page.h
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.25 2001/11/05 22:14:54 art Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.79 2000/06/27 17:29:26 mrg Exp $	*/
d1844 3
a1846 3
	case VM_INHERIT_NONE:
	case VM_INHERIT_COPY:
	case VM_INHERIT_SHARE:
d2933 1
a2933 1
		case VM_INHERIT_NONE:
d2939 1
a2939 1
		case VM_INHERIT_SHARE:
d2996 1
a2996 1
		case VM_INHERIT_COPY:
@


1.25
log
@Minor sync to NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.24 2001/09/19 20:50:59 mickey Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.78 2000/06/26 14:21:18 mrg Exp $	*/
a83 2

#include <vm/vm.h>
@


1.24
log
@merge vm/vm_kern.h into uvm/uvm_extern.h; art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.23 2001/08/11 10:57:22 art Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.77 2000/06/13 04:10:47 chs Exp $	*/
a85 1
#include <vm/vm_page.h>
@


1.23
log
@Various random fixes from NetBSD.
Including support for zeroing pages in the idle loop (not enabled yet).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.22 2001/08/06 14:03:04 art Exp $	*/
a86 1
#include <vm/vm_kern.h>
@


1.22
log
@Add a new type voff_t (right now it's typedefed as off_t) used for offsets
into objects.

Gives the possibilty to mmap beyond the size of vaddr_t.

From NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.21 2001/07/18 14:38:07 art Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.70 2000/03/26 20:54:47 kleink Exp $	*/
d670 1
d1167 1
a1167 1
	vaddr_t *raddr;	/* OUT: reserved VA */
d1567 6
a1572 4
	oldoffset = (entry->start + fudge) - start;
	elen = min(end, entry->end) - (entry->start + fudge);
	pmap_copy(dstmap->pmap, srcmap->pmap, dstaddr + oldoffset, 
		  elen, entry->start + fudge);
d1575 1
a1575 1
      /* we advance "entry" in the following if statement */
a2762 1
	int s;
a2770 1
	s = splhigh();			/* make this `atomic' */
a2773 1
	splx(s);			/* end of critical section */
a2789 1
	int s;
d2791 1
a2791 1
#ifdef sparc
a2841 1
		s = splhigh();
a2844 1
		splx(s);
d3274 2
a3275 2
	(*pr)("OBJECT %p: pgops=%p, npages=%d, ", uobj, uobj->pgops,
	    uobj->uo_npages);
d3281 3
a3283 1
	if (!full) return;
d3285 3
a3287 1
	for (pg = uobj->memq.tqh_first ; pg ; pg = pg->listq.tqe_next, cnt++) {
d3289 6
a3294 1
		if ((cnt % 3) == 2) (*pr)("\n  ");
a3295 1
	if ((cnt % 3) != 2) (*pr)("\n");
d3298 6
d3330 2
d3334 4
a3337 2
	(*pr)("  flags=0x%x, pqflags=0x%x, vers=%d, wire_count=%d, pa=0x%lx\n", 
	pg->flags, pg->pqflags, pg->version, pg->wire_count, (long)pg->phys_addr);
d3339 1
a3339 1
	pg->uobject, pg->uanon, pg->offset, pg->loan_count);
d3378 5
a3382 2
	if (pg->pqflags & PQ_FREE)
		pgl = &uvm.page_free[uvm_page_lookup_freelist(pg)];
@


1.21
log
@Correct the NetBSD tag.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.20 2001/07/18 10:47:05 art Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.69 1999/09/12 01:17:37 chs Exp $	*/
d472 1
a472 1
	vaddr_t uoffset;
d801 1
a801 1
	vaddr_t uoffset;
d3238 1
a3238 1
		(*pr)(" - %p: 0x%lx->0x%lx: obj=%p/0x%x, amap=%p/%d\n",
d3240 1
a3240 1
		    entry->offset, entry->aref.ar_amap, entry->aref.ar_pageoff);
@


1.20
log
@Get rid of the PMAP_NEW option by making it mandatory for all archs.
The archs that didn't have a proper PMAP_NEW now have a dummy implementation
with wrappers around the old functions.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.19 2001/07/17 10:55:02 mts Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.68 1999/08/21 02:19:05 thorpej Exp $	*/
@


1.19
log
@i suk. needs &'s on the locks...
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.18 2001/07/17 10:31:08 mts Exp $	*/
a1023 1
#if defined(PMAP_NEW)
a1024 4
#else
				pmap_remove(pmap_kernel(), entry->start,
				    entry->start + len);
#endif
d2569 1
a2569 2
				pmap_page_protect(PMAP_PGARG(pg),
				    VM_PROT_NONE);
a2726 1
#if defined(PMAP_NEW)
a2727 3
#else
		pmap = pmap_create(0);
#endif
@


1.18
log
@current netbsd's uvm_map uses a 3-parm SAVE_HINT to perform a test/swap
under locked conditions.  we currently use a 2-parm SAVE_HINT... to meet
the same functionality, we instead need to validate the hint is the one
CURRENTLY in the map before substituing it, and we need to do that while
the lock is retained.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.17 2001/07/11 13:57:54 mts Exp $	*/
d1063 1
a1063 1
		simple_lock(map->hint_lock);
d1066 1
a1066 1
		simple_unlock(map->hint_lock);
@


1.17
log
@need to save hint (verified from current netbsd uvm_map.c) -- need mod
to allow bpf to manage shared address space.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.16 2001/06/23 19:24:33 smart Exp $	*/
d1062 5
a1066 1
		SAVE_HINT(map, entry->prev);
@


1.16
log
@Sync with NetBSD 19990911 (just before PMAP_NEW was required)
  - thread_sleep_msg() -> uvm_sleep()
  - initialize reference count lock in uvm_anon_{init,add}()
  - add uao_flush()
  - replace boolean 'islocked' with 'lockflags'
  - in uvm_fault() change FALSE to TRUE to in 'wide' fault handling
  - get rid of uvm_km_get()
  - various bug fixes
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.15 2001/05/10 14:51:21 art Exp $	*/
d1060 4
@


1.15
log
@More sync to NetBSD.
The highlight is some more advices to madvise(2).
 o MADV_DONTNEED will deactive the pages in the given range giving a quicker
   reuse.
 o MADV_FREE will garbage-collect the pages and swap resources causing the
   next fault to either page in new pages from backing store (mapped vnode)
   or allocate new zero-fill pages (anonymous mapping).
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.14 2001/05/10 07:59:06 art Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.63 1999/07/07 21:51:35 thorpej Exp $	*/
d1720 1
d1741 4
a1744 2
		if (UVM_ET_ISSUBMAP(current))
			return (KERN_INVALID_ARGUMENT);
d1746 2
a1747 2
			vm_map_unlock(map);
			return (KERN_PROTECTION_FAILURE);
d1779 30
d1812 1
d1814 2
a1815 2
	UVMHIST_LOG(maphist, "<- done",0,0,0,0);
	return(KERN_SUCCESS);
a1861 4
	/*
	 * XXXJRT: disallow holes?
	 */

d1904 4
d1950 1
a1950 1
uvm_map_pageable(map, start, end, new_pageable, islocked)
d1953 2
a1954 1
	boolean_t new_pageable, islocked;
d1970 1
a1970 1
	if (islocked == FALSE)
d1972 1
d1984 2
a1985 1
		vm_map_unlock(map);
d2007 2
a2008 1
				vm_map_unlock(map);
d2028 2
a2029 1
		vm_map_unlock(map);
d2097 2
a2098 1
			vm_map_unlock(map);
d2166 2
a2167 1
		vm_map_unlock(map);
d2173 11
a2183 3
	vm_map_unbusy(map);
	vm_map_unlock_read(map);
	
d2375 2
d2380 3
a2382 1
		     entry = entry->next)
d2384 1
d2389 2
d2394 2
d2397 1
a2397 1
			if (VM_MAPENT_ISWIRED(entry) == 0)
d2532 1
d2577 7
d2589 1
a2589 1
				amap_unadd(&entry->aref, offset);
@


1.14
log
@Some locking protocol fixes and better enforcement of wiring limits.

From NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.13 2001/05/07 16:08:40 art Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.60 1999/07/01 20:07:05 thorpej Exp $	*/
d1827 5
a1831 1
	
a1883 59
#if 0
		case MADV_WILLNEED:
			/* activate all these pages */
			/* XXX */
			/*
			 * should invent a "weak" mode for uvm_fault()
			 * which would only do the PGO_LOCKED pgo_get().
			 */
			break;

		case MADV_DONTNEED:
			/* deactivate this page */
			/* XXX */
			/*
			 * vm_page_t p;
			 * uvm_lock_pageq();
			 * for (p in each page)
			 *	if (not_wired)
			 *		uvm_pagedeactivate(p);
			 * uvm_unlock_pageq();
			 */
			break;

		case MADV_SPACEAVAIL:
			/* 
			 * XXXMRG
			 * what is this?  i think:  "ensure that we have
			 * allocated backing-store for these pages".  this
			 * is going to require changes in the page daemon,
			 * as it will free swap space allocated to pages in
			 * core.  there's also what to do for
			 * device/file/anonymous memory..
			 */
			break;

		case MADV_GARBAGE:
			/* pages are `empty' and can be garbage collected */
			/* XXX */
			/*
			 * (perhaps MADV_FREE? check freebsd's MADV_FREE).
			 * 
			 * need to do this:
			 *	- clear all the referenced and modified bits on
			 *	  the pages,
			 *	- delete any backing store,
			 *	- mark the page as `recycable'.
			 *
			 * So, if you start paging, the pages would be thrown out
			 * and then zero-filled the next time they're used.
			 * Otherwise you'd just reuse them directly.  Once the
			 * page has been modified again, it would no longer be
			 * recyclable.  That way, malloc() can just tell the
			 * system when pages are `empty'; if memory is needed,
			 * they'll be tossed; if memory is not needed, there
			 * will be no additional overhead.
			 */
			break;
#endif

d2184 1
a2184 1
		map->flags &= ~VM_MAP_WIREFUTURE;
d2198 1
a2198 1
		map->flags |= VM_MAP_WIREFUTURE;
d2356 1
a2356 1
 * uvm_map_clean: push dirty pages off to backing store.
d2359 1
d2365 2
a2366 2
 *	no permanent home...
 * => called from sys_msync()
d2371 2
d2380 5
d2386 1
a2386 2
	struct uvm_object *object;
	vaddr_t offset;
d2391 5
d2398 1
a2398 1
	if (!uvm_map_lookup_entry(map, start, &entry)) {
d2418 2
a2419 6
	/* 
	 * add "cleanit" flag to flags (for generic flush routine).  
	 * then make a second pass, cleaning/uncaching pages from 
	 * the indicated objects as we go.  
	 */
	flags = flags | PGO_CLEANIT;
d2421 7
a2427 2
		offset = current->offset + (start - current->start);
		size = (end <= current->end ? end : current->end) - start;
d2430 5
a2434 1
		 * get object/offset.  can't be submap (checked above).
d2436 27
a2462 2
		object = current->object.uvm_obj;
		simple_lock(&object->vmobjlock);
d2464 77
a2542 2
		 * note that object is locked.
		 * XXX should we continue on an error?
d2545 11
a2555 7
		if (object && object->pgops) {
			if (!object->pgops->pgo_flush(object, offset,
			    offset+size, flags)) {
				simple_unlock(&object->vmobjlock);
				vm_map_unlock_read(map);
				return (KERN_FAILURE);
			}
a2556 1
		simple_unlock(&object->vmobjlock);
d2559 1
d2561 1
a2561 1
	return(KERN_SUCCESS); 
d2757 1
a2757 1
		map->flags &= ~VM_MAP_WIREFUTURE;
@


1.13
log
@Few fixes from NetBSD.
 - make sure that vsunlock doesn't unwire mlocked memory.
 - fix locking in uvm_useracc.
 - Return the error uvm_fault_wire in uvm_vslock (will be used soon).
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.12 2001/05/05 23:25:55 art Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.58 1999/06/17 00:24:10 thorpej Exp $	*/
a189 16
/* XXX Should not exist! */
#define	vm_map_downgrade(map)						\
	(void) lockmgr(&(map)->lock, LK_DOWNGRADE, NULL, curproc)

/* XXX Should not exist! */
#ifdef DIAGNOSTIC
#define	vm_map_upgrade(map)						\
do {									\
	if (lockmgr(&(map)->lock, LK_UPGRADE, NULL, curproc) != 0)	\
		panic("vm_map_upgrade: failed to upgrade lock");	\
} while (0)
#else
#define	vm_map_upgrade(map)						\
	(void) lockmgr(&(map)->lock, LK_UPGRADE, NULL, curproc)
#endif /* DIAGNOSTIC */

d1963 4
a1966 1
 * => map must not be locked.
d1971 1
a1971 1
uvm_map_pageable(map, start, end, new_pageable)
d1974 1
a1974 1
	boolean_t new_pageable;
d1978 3
d1990 2
a1991 1
	vm_map_lock(map);
d2124 4
d2153 6
d2187 1
d2212 3
d2343 4
d2371 6
d2403 1
@


1.12
log
@PMAP_NEW and UVM are no longer optional on i386.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.11 2001/03/22 03:05:55 smart Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.56 1999/06/16 19:34:24 thorpej Exp $	*/
a295 1
	uvm_fault_unwire(map, entry->start, entry->end);
d297 1
a2045 4
		 *
		 * Note, uvm_fault_unwire() (called via uvm_map_entry_unwire())
		 * does not lock the map, so we don't have to do anything
		 * special regarding locking here.
a2227 4
		 *
		 * Note, uvm_fault_unwire() (called via uvm_map_entry_unwire())
		 * does not lock the map, so we don't have to do anything
		 * special regarding locking here.
@


1.11
log
@Sync style, typo, and comments a little closer to NetBSD.  art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.10 2001/03/15 10:30:57 art Exp $	*/
a2700 19
#if (defined(i386) || defined(pc532)) && !defined(PMAP_NEW)
		/* 
		 * allocate zero fill area in the new vmspace's map for user
		 * page tables for ports that have old style pmaps that keep
		 * user page tables in the top part of the process' address
		 * space.
		 *
		 * XXXCDC: this should go away once all pmaps are fixed
		 */
		{ 
			vaddr_t addr = VM_MAXUSER_ADDRESS;
			if (uvm_map(&nvm->vm_map, &addr, VM_MAX_ADDRESS - addr,
			    NULL, UVM_UNKNOWN_OFFSET, UVM_MAPFLAG(UVM_PROT_ALL,
			    UVM_PROT_ALL, UVM_INH_NONE, UVM_ADV_NORMAL,
			    UVM_FLAG_FIXED|UVM_FLAG_COPYONW)) != KERN_SUCCESS)
				panic("vm_allocate of PT page area failed");
		}
#endif

a2772 10
#if (defined(i386) || defined(pc532)) && !defined(PMAP_NEW)
	/*    
	 * avoid copying any of the parent's pagetables or other per-process
	 * objects that reside in the map by marking all of them non-inheritable
	 * XXXCDC: should go away
	 */
	(void) uvm_map_inherit(old_map, VM_MAXUSER_ADDRESS, VM_MAX_ADDRESS, 
			 VM_INHERIT_NONE);
#endif

a3038 19

#if (defined(i386) || defined(pc532)) && !defined(PMAP_NEW)
	/* 
	 * allocate zero fill area in the new vmspace's map for user
	 * page tables for ports that have old style pmaps that keep
	 * user page tables in the top part of the process' address
	 * space.
	 *
	 * XXXCDC: this should go away once all pmaps are fixed
	 */
	{
		vaddr_t addr = VM_MAXUSER_ADDRESS;
		if (uvm_map(new_map, &addr, VM_MAX_ADDRESS - addr, NULL,
		    UVM_UNKNOWN_OFFSET, UVM_MAPFLAG(UVM_PROT_ALL,
		    UVM_PROT_ALL, UVM_INH_NONE, UVM_ADV_NORMAL,
		    UVM_FLAG_FIXED|UVM_FLAG_COPYONW)) != KERN_SUCCESS)
			panic("vm_allocate of PT page area failed");
	}
#endif
@


1.10
log
@Let uvm_map_extract set the lower bound on the address range itself
instead of depending on the callers to do that. (which they don't)
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.9 2001/03/09 14:20:51 art Exp $	*/
a205 1

d1762 1
a1762 1
			current = current->next;
a1786 1

d1790 1
a1791 1
		}
d1885 1
a1885 1
	
d2423 1
a2423 2
	vm_map_entry_t current;
	vm_map_entry_t entry;
@


1.9
log
@More syncing to NetBSD.

Implements mincore(2), mlockall(2) and munlockall(2). mlockall and munlockall
are disabled for the moment.

The rest is mostly cosmetic.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.8 2001/01/29 02:07:46 niklas Exp $	*/
d1372 1
a1372 1
	dstaddr = *dstaddrp;
@


1.8
log
@$OpenBSD$
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.c,v 1.53 1999/06/07 16:31:42 thorpej Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.53 1999/06/07 16:31:42 thorpej Exp $	*/
d598 2
a599 2
		/* wired_count's must match (new area is unwired) */
		if (prev_entry->wired_count)
d990 1
a990 1
		if (entry->wired_count)
d1115 1
a1115 1
		if (first_entry->wired_count)
d1977 2
d1991 1
a1991 2
	vm_map_entry_t entry, start_entry;
	vaddr_t failed = 0;
d2025 1
a2025 2
	if (new_pageable) {               /* unwire */

a2026 1

a2031 1
			
d2034 2
a2035 2
			    (entry->next == &map->header ||
			    entry->next->start > entry->end))) {
d2045 3
a2047 3
		 * now decrement the wiring count for each region.  if a region
		 * becomes completely unwired, unwire its physical pages and
		 * mappings.
a2052 1

d2056 1
a2056 3
			
			entry->wired_count--;
			if (entry->wired_count == 0)
a2057 1
			
d2077 1
a2077 1
	 *    in the pages for any newly wired area (wired_count is 1).
d2091 1
a2091 3

		if (entry->wired_count == 0) {  /* not already wired? */
			
a2098 1
			
d2100 3
a2102 8
				/*
				 * XXXCDC: protection vs. max_protection??
				 * (wirefault uses max?)
				 * XXXCDC: used to do it always if
				 * uvm_obj == NULL (wrong?)
				 */
				if ( UVM_ET_ISNEEDSCOPY(entry) && 
				    (entry->protection & VM_PROT_WRITE) != 0) {
d2108 1
a2108 1
		}     /* wired_count == 0 */
d2116 4
a2119 2
		if (entry->end < end && (entry->next == &map->header ||
			     entry->next->start > entry->end)) {
d2169 2
a2170 2
		failed = entry->start;
		while (entry != &map->header && entry->start < end)
d2172 2
d2176 2
a2177 2
		 * now, unlock the map, and unwire all the pages that
		 * were successfully wired above.
d2179 7
a2186 1
		(void) uvm_map_pageable(map, start, failed, TRUE);
d2199 206
d2682 8
d2963 1
a2963 1
			      old_entry->wired_count != 0) {
d2982 1
a2982 1
			if (old_entry->wired_count != 0) {
@


1.7
log
@seperate -> separate, okay aaron@@
@
text
@d1 1
@


1.6
log
@Convert bzero to memset(X, 0..) and bcopy to memcpy.
This is to match (make diffs smaller) the code in NetBSD.
new gcc inlines those functions, so this could also be a performance win.
@
text
@d960 1
a960 1
	 * seperate unmapping from reference dropping.  why?
d2020 1
a2020 1
	 * handle wiring and unwiring seperately.
@


1.5
log
@fix a non-DIAGNOSTIC gotcha
@
text
@d281 1
a281 1
	bcopy(src, dst, ((char *)&src->uvm_map_entry_stop_copy) - ((char*)src));
d2378 1
a2378 1
	bzero(vm, sizeof(*vm));
d2598 1
a2598 1
	bcopy(&vm1->vm_startcopy, &vm2->vm_startcopy,
@


1.4
log
@Bring in some new UVM code from NetBSD (not current).

 - Introduce a new type of map that are interrupt safe and never allow faults
   in them. mb_map and kmem_map are made intrsafe.
 - Add "access protection" to uvm_vslock (to be passed down to uvm_fault and
   later to pmap_enter).
 - madvise(2) now works.
 - various cleanups.
@
text
@d202 1
a202 1
	(void) lockmgr(&(map)->lock, LK_UPGRADE, NULL)
@


1.3
log
@sync with NetBSD from 1999.05.24 (there is a reason for this date)
 Mostly cleanups, but also a few improvements to pagedaemon for better
 handling of low memory and/or low swap conditions.
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_map.c,v 1.39 1999/05/12 19:11:23 thorpej Exp $	*/
d189 17
d221 2
a222 1
	if (map->entries_pageable) {
a225 1

d234 1
a234 1
	panic("mapent_alloc: out of kernel map entries, check MAX_KMAPENT");
d238 3
a240 2
	UVMHIST_LOG(maphist, "<- new entry=0x%x [pageable=%d]", 
		me, map->entries_pageable, 0, 0);
a241 1

d296 1
a296 1
	uvm_fault_unwire(map->pmap, entry->start, entry->end);
d563 3
a565 2
			if (uobj->uo_refs != UVM_OBJ_KERN)
	panic("uvm_map: unknown offset with non-kernel object");
d997 1
a997 2
		    entry->object.uvm_obj->uo_refs == UVM_OBJ_KERN) {

d1000 2
a1001 1
	panic("uvm_unmap_remove: kernel object mapped by non-kernel map");
d1027 1
a1027 1
			 *     - pmap_page_protect them out of all pmaps
d1030 2
a1031 12
			 * note that in case [1] the pmap_page_protect call
			 * in uvm_km_pgremove may very well be redundant
			 * because we have already removed the mappings
			 * beforehand with pmap_remove (or pmap_kremove).
			 * in the PMAP_NEW case, the pmap_page_protect call
			 * may not do anything, since PMAP_NEW allows the
			 * kernel to enter/remove kernel mappings without
			 * bothing to keep track of the mappings (e.g. via
			 * pv_entry lists).    XXX: because of this, in the
			 * future we should consider removing the
			 * pmap_page_protect from uvm_km_pgremove some time
			 * in the future.
d1035 3
a1037 1
			 * remove mappings from pmap
d1039 1
d1041 1
a1041 1
			pmap_kremove(entry->start, len);
d1043 2
a1044 2
			pmap_remove(pmap_kernel(), entry->start,
			    entry->start+len);
d1046 10
a1055 8

			/*
			 * remove pages from a kernel object (offsets are
			 * always relative to vm_map_min(kernel_map)).
			 */
			uvm_km_pgremove(entry->object.uvm_obj, 
			entry->start - vm_map_min(kernel_map),
			entry->end - vm_map_min(kernel_map));
d1858 115
d1995 5
d2049 4
a2053 3
#if 0		/* not necessary: uvm_fault_unwire does not lock */
		lock_set_recursive(&map->lock);
#endif  /* XXXCDC */
a2064 3
#if 0 /* XXXCDC: not necessary, see above */
		lock_clear_recursive(&map->lock);
#endif
d2149 2
a2150 17
	/*
	 * HACK HACK HACK HACK
	 *
	 * if we are wiring in the kernel map or a submap of it, unlock the
	 * map to avoid deadlocks.  we trust that the kernel threads are
	 * well-behaved, and therefore will not do anything destructive to
	 * this region of the map while we have it unlocked.  we cannot
	 * trust user threads to do the same.
	 *
	 * HACK HACK HACK HACK 
	 */
	if (vm_map_pmap(map) == pmap_kernel()) {
		vm_map_unlock(map);         /* trust me ... */
	} else {
		vm_map_set_recursive(&map->lock);
		lockmgr(&map->lock, LK_DOWNGRADE, (void *)0, curproc /*XXX*/);
	}
d2155 3
a2157 14
		/*
		 * if uvm_fault_wire fails for any page we need to undo what has
		 * been done.  we decrement the wiring count for those pages
		 * which have not yet been wired (now) and unwire those that
		 * have * (later).
		 *
		 * XXX this violates the locking protocol on the map, needs to
		 * be fixed.  [because we only have a read lock on map we 
		 * shouldn't be changing wired_count?]
		 */
		if (rv) {
			entry->wired_count--;
		} else if (entry->wired_count == 1) {
			rv = uvm_fault_wire(map, entry->start, entry->end);
d2159 6
a2164 2
				failed = entry->start;
				entry->wired_count--;
d2170 13
a2182 5
	if (vm_map_pmap(map) == pmap_kernel()) {
		vm_map_lock(map);     /* relock */
	} else {
		vm_map_clear_recursive(&map->lock);
	} 
d2184 4
a2187 1
	if (rv) {        /* failed? */
d2193 3
a2195 1
	vm_map_unlock(map);
d2380 1
a2380 1
	uvm_map_setup(&vm->vm_map, min, max, pageable);
d2491 1
a2491 1
			 map->entries_pageable);
d2597 1
a2597 1
		      old_map->entries_pageable);
d2926 3
a2928 2
	(*pr)("\t#ent=%d, sz=%d, ref=%d, version=%d\n",
	    map->nentries, map->size, map->ref_count, map->timestamp);
d2981 1
a2981 1
	if (uobj->uo_refs == UVM_OBJ_KERN)
@


1.3.4.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_map.c,v 1.53 1999/06/07 16:31:42 thorpej Exp $	*/
a188 17
/* XXX Should not exist! */
#define	vm_map_downgrade(map)						\
	(void) lockmgr(&(map)->lock, LK_DOWNGRADE, NULL, curproc)

/* XXX Should not exist! */
#ifdef DIAGNOSTIC
#define	vm_map_upgrade(map)						\
do {									\
	if (lockmgr(&(map)->lock, LK_UPGRADE, NULL, curproc) != 0)	\
		panic("vm_map_upgrade: failed to upgrade lock");	\
} while (0)
#else
#define	vm_map_upgrade(map)						\
	(void) lockmgr(&(map)->lock, LK_UPGRADE, NULL, curproc)
#endif /* DIAGNOSTIC */


d204 1
a204 2
	if ((map->flags & VM_MAP_INTRSAFE) == 0 &&
	    map != kernel_map && kernel_map != NULL /* XXX */) {
d208 1
d217 1
a217 1
	panic("mapent_alloc: out of static map entries, check MAX_KMAPENT");
d221 2
a222 3
	UVMHIST_LOG(maphist, "<- new entry=0x%x [kentry=%d]", 
		me, ((map->flags & VM_MAP_INTRSAFE) != 0 || map == kernel_map)
		? TRUE : FALSE, 0, 0);
d224 1
d279 1
a279 1
	uvm_fault_unwire(map, entry->start, entry->end);
d546 2
a547 3
			if (UVM_OBJ_IS_KERN_OBJECT(uobj) == 0)
				panic("uvm_map: unknown offset with "
				    "non-kernel object");
d979 2
a980 1
		    UVM_OBJ_IS_KERN_OBJECT(entry->object.uvm_obj)) {
d983 1
a983 2
				panic("uvm_unmap_remove: kernel object "
				    "mapped by non-kernel map");
d1009 1
a1009 1
			 *     - drops the swap slot
d1012 12
a1023 2
			 * note there is version of uvm_km_pgremove() that
			 * is used for "intrsafe" objects.
d1027 1
a1027 3
			 * remove mappings from pmap and drop the pages
			 * from the object.  offsets are always relative
			 * to vm_map_min(kernel_map).
a1028 1
			if (UVM_OBJ_IS_INTRSAFE_OBJECT(entry->object.uvm_obj)) {
d1030 1
a1030 1
				pmap_kremove(entry->start, len);
d1032 2
a1033 2
				pmap_remove(pmap_kernel(), entry->start,
				    entry->start + len);
d1035 8
a1042 10
				uvm_km_pgremove_intrsafe(entry->object.uvm_obj,
				    entry->start - vm_map_min(kernel_map),
				    entry->end - vm_map_min(kernel_map));
			} else {
				pmap_remove(pmap_kernel(), entry->start,
				    entry->start + len);
				uvm_km_pgremove(entry->object.uvm_obj,
				    entry->start - vm_map_min(kernel_map),
				    entry->end - vm_map_min(kernel_map));
			}
a1844 115
/* 
 * uvm_map_advice: set advice code for range of addrs in map.
 *
 * => map must be unlocked
 */

int
uvm_map_advice(map, start, end, new_advice)
	vm_map_t map;
	vaddr_t start;
	vaddr_t end;
	int new_advice;
{
	vm_map_entry_t entry, temp_entry;
	UVMHIST_FUNC("uvm_map_advice"); UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist,"(map=0x%x,start=0x%x,end=0x%x,new_adv=0x%x)",
	    map, start, end, new_advice);

	vm_map_lock(map);
	
	VM_MAP_RANGE_CHECK(map, start, end);
	
	if (uvm_map_lookup_entry(map, start, &temp_entry)) {
		entry = temp_entry;
		UVM_MAP_CLIP_START(map, entry, start);
	} else {
		entry = temp_entry->next;
	}
	
	while ((entry != &map->header) && (entry->start < end)) {
		UVM_MAP_CLIP_END(map, entry, end);

		switch (new_advice) {
		case MADV_NORMAL:
		case MADV_RANDOM:
		case MADV_SEQUENTIAL:
			/* nothing special here */
			break;

#if 0
		case MADV_WILLNEED:
			/* activate all these pages */
			/* XXX */
			/*
			 * should invent a "weak" mode for uvm_fault()
			 * which would only do the PGO_LOCKED pgo_get().
			 */
			break;

		case MADV_DONTNEED:
			/* deactivate this page */
			/* XXX */
			/*
			 * vm_page_t p;
			 * uvm_lock_pageq();
			 * for (p in each page)
			 *	if (not_wired)
			 *		uvm_pagedeactivate(p);
			 * uvm_unlock_pageq();
			 */
			break;

		case MADV_SPACEAVAIL:
			/* 
			 * XXXMRG
			 * what is this?  i think:  "ensure that we have
			 * allocated backing-store for these pages".  this
			 * is going to require changes in the page daemon,
			 * as it will free swap space allocated to pages in
			 * core.  there's also what to do for
			 * device/file/anonymous memory..
			 */
			break;

		case MADV_GARBAGE:
			/* pages are `empty' and can be garbage collected */
			/* XXX */
			/*
			 * (perhaps MADV_FREE? check freebsd's MADV_FREE).
			 * 
			 * need to do this:
			 *	- clear all the referenced and modified bits on
			 *	  the pages,
			 *	- delete any backing store,
			 *	- mark the page as `recycable'.
			 *
			 * So, if you start paging, the pages would be thrown out
			 * and then zero-filled the next time they're used.
			 * Otherwise you'd just reuse them directly.  Once the
			 * page has been modified again, it would no longer be
			 * recyclable.  That way, malloc() can just tell the
			 * system when pages are `empty'; if memory is needed,
			 * they'll be tossed; if memory is not needed, there
			 * will be no additional overhead.
			 */
			break;
#endif

		default:
			vm_map_unlock(map);
			UVMHIST_LOG(maphist,"<- done (INVALID ARG)",0,0,0,0);
			return (KERN_INVALID_ARGUMENT);
		}


		entry->advice = new_advice;
		
		entry = entry->next;
	}

	vm_map_unlock(map);
	UVMHIST_LOG(maphist,"<- done (OK)",0,0,0,0);
	return (KERN_SUCCESS);
}

a1866 5
#ifdef DIAGNOSTIC
	if ((map->flags & VM_MAP_PAGEABLE) == 0)
		panic("uvm_map_pageable: map %p not pageable", map);
#endif

a1915 4
		 *
		 * Note, uvm_fault_unwire() (called via uvm_map_entry_unwire())
		 * does not lock the map, so we don't have to do anything
		 * special regarding locking here.
d1917 3
d1931 3
d2018 17
a2034 2

	vm_map_downgrade(map);
d2039 14
a2052 3
		if (entry->wired_count == 1) {
			rv = uvm_fault_wire(map, entry->start, entry->end,
			    entry->protection);
d2054 2
a2055 6
				/*
				 * wiring failed.  break out of the loop.
				 * we'll clean up the map below, once we
				 * have a write lock again.
				 */
				break;
d2061 6
a2067 17
		/*
		 * Get back to an exclusive (write) lock.
		 */
		vm_map_upgrade(map);

		/*
		 * first drop the wiring count on all the entries
		 * which haven't actually been wired yet.
		 */
		failed = entry->start;
		while (entry != &map->header && entry->start < end)
			entry->wired_count--;

		/*
		 * now, unlock the map, and unwire all the pages that
		 * were successfully wired above.
		 */
d2073 1
a2073 3

	/* We are holding a read lock here. */
	vm_map_unlock_read(map);
d2258 1
a2258 1
	uvm_map_setup(&vm->vm_map, min, max, pageable ? VM_MAP_PAGEABLE : 0);
d2369 1
a2369 1
			 (map->flags & VM_MAP_PAGEABLE) ? TRUE : FALSE);
d2475 1
a2475 1
		      (old_map->flags & VM_MAP_PAGEABLE) ? TRUE : FALSE);
d2804 2
a2805 3
	(*pr)("\t#ent=%d, sz=%d, ref=%d, version=%d, flags=0x%x\n",
	    map->nentries, map->size, map->ref_count, map->timestamp,
	    map->flags);
d2858 1
a2858 1
	if (UVM_OBJ_IS_KERN_OBJECT(uobj))
@


1.3.4.2
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_map.c,v 1.11 2001/03/22 03:05:55 smart Exp $	*/
/*	$NetBSD: uvm_map.c,v 1.56 1999/06/16 19:34:24 thorpej Exp $	*/
d205 1
d281 1
a281 1
	memcpy(dst, src, ((char *)&src->uvm_map_entry_stop_copy) - ((char*)src));
d597 2
a598 2
		/* wiring status must match (new area is unwired) */
		if (VM_MAPENT_ISWIRED(prev_entry))
d960 1
a960 1
	 * separate unmapping from reference dropping.  why?
d989 1
a989 1
		if (VM_MAPENT_ISWIRED(entry))
d1114 1
a1114 1
		if (VM_MAPENT_ISWIRED(first_entry))
d1371 1
a1371 1
	dstaddr = vm_map_min(dstmap);
d1762 1
a1762 1
		current = current->next;
d1787 1
d1791 1
a1792 1

d1886 1
a1886 1

a1975 2
 * => wires map entries.  should not be used for transient page locking.
 *	for that, use uvm_fault_wire()/uvm_fault_unwire() (see uvm_vslock()).
d1988 2
a1989 1
	vm_map_entry_t entry, start_entry, failed_entry;
d2020 1
a2020 1
	 * handle wiring and unwiring separately.
d2023 2
a2024 1
	if (new_pageable) {		/* unwire */
d2026 1
d2032 1
d2035 2
a2036 2
			     (entry->next == &map->header ||
			      entry->next->start > entry->end))) {
d2046 3
a2048 3
		 * POSIX 1003.1b - a single munlock call unlocks a region,
		 * regardless of the number of mlock calls made on that
		 * region.
d2054 1
d2058 3
a2060 1
			if (VM_MAPENT_ISWIRED(entry))
d2062 1
d2082 1
a2082 1
	 *    in the pages for any newly wired area (wired_count == 1).
d2096 3
a2098 1
		if (VM_MAPENT_ISWIRED(entry) == 0) { /* not already wired? */
d2106 1
d2108 8
a2115 3
				if (UVM_ET_ISNEEDSCOPY(entry) && 
				    ((entry->protection & VM_PROT_WRITE) ||
				     (entry->object.uvm_obj == NULL))) {
d2121 1
a2121 1
		}
d2129 2
a2130 4
		if (entry->protection == VM_PROT_NONE ||
		    (entry->end < end &&
		     (entry->next == &map->header ||
		      entry->next->start > entry->end))) {
d2180 2
a2181 2
		failed_entry = entry;
		while (entry != &map->header && entry->start < end) {
a2182 2
			entry = entry->next;
		}
d2185 2
a2186 2
		 * now, unwire all the entries that were successfully
		 * wired above.
a2187 7
		entry = start_entry;
		while (entry != failed_entry) {
			entry->wired_count--;
			if (VM_MAPENT_ISWIRED(entry) == 0)
				uvm_map_entry_unwire(map, entry);
			entry = entry->next;
		}
d2189 1
a2201 206
 * uvm_map_pageable_all: special case of uvm_map_pageable - affects
 * all mapped regions.
 *
 * => map must not be locked.
 * => if no flags are specified, all regions are unwired.
 * => XXXJRT: has some of the same problems as uvm_map_pageable() above.
 */

int
uvm_map_pageable_all(map, flags, limit)
	vm_map_t map;
	int flags;
	vsize_t limit;
{
	vm_map_entry_t entry, failed_entry;
	vsize_t size;
	int rv;
	UVMHIST_FUNC("uvm_map_pageable_all"); UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist,"(map=0x%x,flags=0x%x)", map, flags, 0, 0);

#ifdef DIAGNOSTIC
	if ((map->flags & VM_MAP_PAGEABLE) == 0)
		panic("uvm_map_pageable_all: map %p not pageable", map);
#endif

	vm_map_lock(map);

	/*
	 * handle wiring and unwiring separately.
	 */

	if (flags == 0) {			/* unwire */
		/*
		 * POSIX 1003.1b -- munlockall unlocks all regions,
		 * regardless of how many times mlockall has been called.
		 *
		 * Note, uvm_fault_unwire() (called via uvm_map_entry_unwire())
		 * does not lock the map, so we don't have to do anything
		 * special regarding locking here.
		 */
		for (entry = map->header.next; entry != &map->header;
		     entry = entry->next) {
			if (VM_MAPENT_ISWIRED(entry))
				uvm_map_entry_unwire(map, entry);
		}
		map->flags &= ~VM_MAP_WIREFUTURE;
		vm_map_unlock(map);
		UVMHIST_LOG(maphist,"<- done (OK UNWIRE)",0,0,0,0);
		return (KERN_SUCCESS);

		/*
		 * end of unwire case!
		 */
	}

	if (flags & MCL_FUTURE) {
		/*
		 * must wire all future mappings; remember this.
		 */
		map->flags |= VM_MAP_WIREFUTURE;
	}

	if ((flags & MCL_CURRENT) == 0) {
		/*
		 * no more work to do!
		 */
		UVMHIST_LOG(maphist,"<- done (OK no wire)",0,0,0,0);
		vm_map_unlock(map);
		return (KERN_SUCCESS);
	}

	/*
	 * wire case: in three passes [XXXCDC: ugly block of code here]
	 *
	 * 1: holding the write lock, count all pages mapped by non-wired
	 *    entries.  if this would cause us to go over our limit, we fail.
	 *
	 * 2: still holding the write lock, we create any anonymous maps that
	 *    need to be created.  then we increment its wiring count.
	 *
	 * 3: we downgrade to a read lock, and call uvm_fault_wire to fault
	 *    in the pages for any newly wired area (wired_count == 1).
	 *
	 *    downgrading to a read lock for uvm_fault_wire avoids a possible
	 *    deadlock with another thread that may have faulted on one of
	 *    the pages to be wired (it would mark the page busy, blocking
	 *    us, then in turn block on the map lock that we hold).  because
	 *    of problems in the recursive lock package, we cannot upgrade
	 *    to a write lock in vm_map_lookup.  thus, any actions that
	 *    require the write lock must be done beforehand.  because we
	 *    keep the read lock on the map, the copy-on-write status of the
	 *    entries we modify here cannot change.
	 */

	for (size = 0, entry = map->header.next; entry != &map->header;
	     entry = entry->next) {
		if (entry->protection != VM_PROT_NONE &&
		    VM_MAPENT_ISWIRED(entry) == 0) { /* not already wired? */
			size += entry->end - entry->start;
		}
	}

	if (atop(size) + uvmexp.wired > uvmexp.wiredmax) {
		vm_map_unlock(map);
		return (KERN_NO_SPACE);		/* XXX overloaded */
	}

	/* XXX non-pmap_wired_count case must be handled by caller */
#ifdef pmap_wired_count
	if (limit != 0 &&
	    (size + ptoa(pmap_wired_count(vm_map_pmap(map))) > limit)) {
		vm_map_unlock(map);
		return (KERN_NO_SPACE);		/* XXX overloaded */
	}
#endif

	/*
	 * Pass 2.
	 */

	for (entry = map->header.next; entry != &map->header;
	     entry = entry->next) {
		if (entry->protection == VM_PROT_NONE)
			continue;
		if (VM_MAPENT_ISWIRED(entry) == 0) { /* not already wired? */
			/*
			 * perform actions of vm_map_lookup that need the
			 * write lock on the map: create an anonymous map
			 * for a copy-on-write region, or an anonymous map
			 * for a zero-fill region.  (XXXCDC: submap case
			 * ok?)
			 */
			if (!UVM_ET_ISSUBMAP(entry)) {	/* not submap */
				if (UVM_ET_ISNEEDSCOPY(entry) && 
				    ((entry->protection & VM_PROT_WRITE) ||
				     (entry->object.uvm_obj == NULL))) {
					amap_copy(map, entry, M_WAITOK, TRUE,
					    entry->start, entry->end);
					/* XXXCDC: wait OK? */
				}
			}
		}
		entry->wired_count++;
	}

	/*
	 * Pass 3.
	 */

	vm_map_downgrade(map);

	rv = KERN_SUCCESS;
	for (entry = map->header.next; entry != &map->header;
	     entry = entry->next) {
		if (entry->wired_count == 1) {
			rv = uvm_fault_wire(map, entry->start, entry->end,
			     entry->protection);
			if (rv) {
				/*
				 * wiring failed.  break out of the loop.
				 * we'll clean up the map below, once we
				 * have a write lock again.
				 */
				break;
			}
		}
	}

	if (rv) {	/* failed? */
		/*
		 * Get back an exclusive (write) lock.
		 */
		vm_map_upgrade(map);

		/*
		 * first drop the wiring count on all the entries
		 * which haven't actually been wired yet.
		 */
		failed_entry = entry;
		for (/* nothing */; entry != &map->header;
		     entry = entry->next)
			entry->wired_count--;

		/*
		 * now, unwire all the entries that were successfully
		 * wired above.
		 */
		for (entry = map->header.next; entry != failed_entry;
		     entry = entry->next) {
			entry->wired_count--;
			if (VM_MAPENT_ISWIRED(entry) == 0)
				uvm_map_entry_unwire(map, entry);
		}
		vm_map_unlock(map);
		UVMHIST_LOG(maphist,"<- done (RV=%d)", rv,0,0,0);
		return (rv);
	}

	/* We are holding a read lock here. */
	vm_map_unlock_read(map);

	UVMHIST_LOG(maphist,"<- done (OK WIRE)",0,0,0,0);
	return (KERN_SUCCESS);
}

/*
d2222 2
a2223 1
	vm_map_entry_t current, entry;
d2378 1
a2378 1
	memset(vm, 0, sizeof(*vm));
a2478 8
		 * POSIX 1003.1b -- "lock future mappings" is revoked
		 * when a process execs another program image.
		 */
		vm_map_lock(map);
		map->flags &= ~VM_MAP_WIREFUTURE;
		vm_map_unlock(map);

		/*
d2598 1
a2598 1
	memcpy(&vm2->vm_startcopy, &vm1->vm_startcopy,
d2752 1
a2752 1
			      VM_MAPENT_ISWIRED(old_entry)) {
d2771 1
a2771 1
			if (VM_MAPENT_ISWIRED(old_entry)) {
@


1.3.4.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_map.c,v 1.68 1999/08/21 02:19:05 thorpej Exp $	*/
d190 16
d296 1
a297 1
	uvm_fault_unwire_locked(map, entry->start, entry->end);
a1735 1
	int rv = KERN_SUCCESS;
d1756 2
a1757 4
		if (UVM_ET_ISSUBMAP(current)) {
			rv = KERN_INVALID_ARGUMENT;
			goto out;
		}
d1759 2
a1760 2
			rv = KERN_PROTECTION_FAILURE;
			goto out;
a1791 30
		/*
		 * If the map is configured to lock any future mappings,
		 * wire this entry now if the old protection was VM_PROT_NONE
		 * and the new protection is not VM_PROT_NONE.
		 */

		if ((map->flags & VM_MAP_WIREFUTURE) != 0 &&
		    VM_MAPENT_ISWIRED(entry) == 0 &&
		    old_prot == VM_PROT_NONE &&
		    new_prot != VM_PROT_NONE) {
			if (uvm_map_pageable(map, entry->start,
			    entry->end, FALSE,
			    UVM_LK_ENTER|UVM_LK_EXIT) != KERN_SUCCESS) {
				/*
				 * If locking the entry fails, remember the
				 * error if it's the first one.  Note we
				 * still continue setting the protection in
				 * the map, but will return the resource
				 * shortage condition regardless.
				 *
				 * XXX Ignore what the actual error is,
				 * XXX just call it a resource shortage
				 * XXX so that it doesn't get confused
				 * XXX what uvm_map_protect() itself would
				 * XXX normally return.
				 */
				rv = KERN_RESOURCE_SHORTAGE;
			}
		}

a1794 1
 out:
d1796 2
a1797 2
	UVMHIST_LOG(maphist, "<- done, rv=%d",rv,0,0,0);
	return (rv);
d1843 1
a1843 1

a1885 4
	/*
	 * XXXJRT: disallow holes?
	 */

d1896 59
d1979 1
a1979 4
 * => map must never be read-locked
 * => if islocked is TRUE, map is already write-locked
 * => we always unlock the map, since we must downgrade to a read-lock
 *	to call uvm_fault_wire()
d1984 1
a1984 1
uvm_map_pageable(map, start, end, new_pageable, lockflags)
a1987 1
	int lockflags;
a1990 3
#ifdef DIAGNOSTIC
	u_int timestamp_save;
#endif
d2000 1
a2000 3
	if ((lockflags & UVM_LK_ENTER) == 0)
		vm_map_lock(map);

d2012 1
a2012 2
		if ((lockflags & UVM_LK_EXIT) == 0)
			vm_map_unlock(map);
d2034 1
a2034 2
				if ((lockflags & UVM_LK_EXIT) == 0)
					vm_map_unlock(map);
d2046 4
d2058 1
a2058 2
		if ((lockflags & UVM_LK_EXIT) == 0)
			vm_map_unlock(map);
d2126 1
a2126 2
			if ((lockflags & UVM_LK_EXIT) == 0)
				vm_map_unlock(map);
a2136 4
#ifdef DIAGNOSTIC
	timestamp_save = map->timestamp;
#endif
	vm_map_busy(map);
a2161 6
		vm_map_unbusy(map);

#ifdef DIAGNOSTIC
		if (timestamp_save != map->timestamp)
			panic("uvm_map_pageable: stale map");
#endif
d2184 1
a2184 2
		if ((lockflags & UVM_LK_EXIT) == 0)
			vm_map_unlock(map);
d2190 2
a2191 11
	if ((lockflags & UVM_LK_EXIT) == 0) {
		vm_map_unbusy(map);
		vm_map_unlock_read(map);
	} else {
		/*
		 * Get back to an exclusive (write) lock.
		 */
		vm_map_upgrade(map);
		vm_map_unbusy(map);
	}

a2213 3
#ifdef DIAGNOSTIC
	u_int timestamp_save;
#endif
d2232 4
d2242 1
a2242 1
		vm_map_modflags(map, 0, VM_MAP_WIREFUTURE);
d2256 1
a2256 1
		vm_map_modflags(map, VM_MAP_WIREFUTURE, 0);
a2345 4
#ifdef DIAGNOSTIC
	timestamp_save = map->timestamp;
#endif
	vm_map_busy(map);
a2369 6
		vm_map_unbusy(map);

#ifdef DIAGNOSTIC
		if (timestamp_save != map->timestamp)
			panic("uvm_map_pageable_all: stale map");
#endif
a2373 2
		 *
		 * Skip VM_PROT_NONE entries like we did above.
d2377 1
a2377 3
		     entry = entry->next) {
			if (entry->protection == VM_PROT_NONE)
				continue;
a2378 1
		}
a2382 2
		 *
		 * Skip VM_PROT_NONE entries like we did above.
a2385 2
			if (entry->protection == VM_PROT_NONE)
				continue;
d2387 1
a2387 1
			if (VM_MAPENT_ISWIRED(entry))
a2395 1
	vm_map_unbusy(map);
d2403 1
a2403 1
 * uvm_map_clean: clean out a map range
a2405 1
 *   if (flags & PGO_CLEANIT): dirty pages are cleaned first
d2411 2
a2412 2
 *	no permanent home, but may deactivate pages there
 * => called from sys_msync() and sys_madvise()
a2416 2
int	amap_clean_works = 1;	/* XXX for now, just in case... */

d2424 2
a2425 4
	struct uvm_object *uobj;
	struct vm_amap *amap;
	struct vm_anon *anon;
	struct vm_page *pg;
a2426 2
	vsize_t size;
	int rv, error, refs;
a2430 5
#ifdef DIAGNOSTIC
	if ((flags & (PGO_FREE|PGO_DEACTIVATE)) == (PGO_FREE|PGO_DEACTIVATE))
		panic("uvm_map_clean: FREE and DEACTIVATE");
#endif

d2433 1
a2433 1
	if (uvm_map_lookup_entry(map, start, &entry) == FALSE) {
d2453 6
a2458 2
	error = KERN_SUCCESS;

d2460 2
a2461 7
		amap = current->aref.ar_amap;	/* top layer */
		uobj = current->object.uvm_obj;	/* bottom layer */

#ifdef DIAGNOSTIC
		if (start < current->start)
			panic("uvm_map_clean: hole");
#endif
d2464 1
a2464 5
		 * No amap cleaning necessary if:
		 *
		 *	(1) There's no amap.
		 *
		 *	(2) We're not deactivating or freeing pages.
d2466 2
a2467 101
		if (amap == NULL ||
		    (flags & (PGO_DEACTIVATE|PGO_FREE)) == 0)
			goto flush_object;

		/* XXX for now, just in case... */
		if (amap_clean_works == 0)
			goto flush_object;

		amap_lock(amap);

		offset = start - current->start;
		size = (end <= current->end ? end : current->end) -
		    start;

		for (/* nothing */; size != 0; size -= PAGE_SIZE,
		     offset += PAGE_SIZE) {
			anon = amap_lookup(&current->aref, offset);
			if (anon == NULL)
				continue;

			simple_lock(&anon->an_lock);

			pg = anon->u.an_page;
			if (pg == NULL) {
				simple_unlock(&anon->an_lock);
				continue;
			}

			switch (flags & (PGO_CLEANIT|PGO_FREE|PGO_DEACTIVATE)) {
			/*
			 * XXX In these first 3 cases, we always just
			 * XXX deactivate the page.  We may want to
			 * XXX handle the different cases more
			 * XXX specifically, in the future.
			 */
			case PGO_CLEANIT|PGO_FREE:
			case PGO_CLEANIT|PGO_DEACTIVATE:
			case PGO_DEACTIVATE:
 deactivate_it:
				/* skip the page if it's loaned or wired */
				if (pg->loan_count != 0 ||
				    pg->wire_count != 0) {
					simple_unlock(&anon->an_lock);
					continue;
				}

				uvm_lock_pageq();

				/*
				 * skip the page if it's not actually owned
				 * by the anon (may simply be loaned to the
				 * anon).
				 */
				if ((pg->pqflags & PQ_ANON) == 0) {
#ifdef DIAGNOSTIC
					if (pg->uobject != NULL)
						panic("uvm_map_clean: "
						    "page anon vs. object "
						    "inconsistency");
#endif
					uvm_unlock_pageq();
					simple_unlock(&anon->an_lock);
					continue;
				}

#ifdef DIAGNOSTIC
				if (pg->uanon != anon)
					panic("uvm_map_clean: anon "
					    "inconsistency");
#endif

				/* zap all mappings for the page. */
				pmap_page_protect(PMAP_PGARG(pg),
				    VM_PROT_NONE);

				/* ...and deactivate the page. */
				uvm_pagedeactivate(pg);

				uvm_unlock_pageq();
				simple_unlock(&anon->an_lock);
				continue;

			case PGO_FREE:
				/*
				 * If there are multiple references to
				 * the amap, just deactivate the page.
				 */
				if (amap_refs(amap) > 1)
					goto deactivate_it;

				/* XXX skip the page if it's wired */
				if (pg->wire_count != 0) {
					simple_unlock(&anon->an_lock);
					continue;
				}
				amap_unadd(&current->aref, offset);
				refs = --anon->an_ref;
				simple_unlock(&anon->an_lock);
				if (refs == 0)
					uvm_anfree(anon);
				continue;
a2468 11
			default:
				panic("uvm_map_clean: wierd flags");
			}
#ifdef DIAGNOSTIC
			panic("uvm_map_clean: unreachable code");
#endif
		}

		amap_unlock(amap);

 flush_object:
d2471 2
d2475 7
a2481 11
		offset = current->offset + (start - current->start);
		size = (end <= current->end ? end : current->end) - start;

		if (uobj != NULL) {
			simple_lock(&uobj->vmobjlock);
			rv = uobj->pgops->pgo_flush(uobj, offset,
			    offset + size, flags);
			simple_unlock(&uobj->vmobjlock);

			if (rv == FALSE)
				error = KERN_FAILURE;
d2483 1
a2485 1

d2487 1
a2487 1
	return (error); 
d2683 1
a2683 1
		vm_map_modflags(map, 0, VM_MAP_WIREFUTURE);
d2701 19
d2792 10
d3068 19
@


1.3.4.4
log
@Sync the SMP branch to something just after 3.0
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_map.c,v 1.77 2000/06/13 04:10:47 chs Exp $	*/
d87 1
d472 1
a472 1
	voff_t uoffset;
a669 1
		new_entry->aref.ar_pageoff = 0;
d801 1
a801 1
	voff_t uoffset;
d1024 1
d1026 4
a1059 8

		/* critical! prevents stale hint */
		/* XXX: need SAVE_HINT with three parms */
		simple_lock(&map->hint_lock);
		if (map->hint == entry)
		    map->hint = entry->prev;
		simple_unlock(&map->hint_lock);

d1163 1
a1163 1
	vaddr_t *raddr;	/* IN:hint, OUT: reserved VA */
d1563 4
a1566 6
				oldoffset = (entry->start + fudge) - start;
				elen = min(end, entry->end) -
				    (entry->start + fudge);
				pmap_copy(dstmap->pmap, srcmap->pmap,
				    dstaddr + oldoffset, elen,
				    entry->start + fudge);
d1569 1
a1569 1
			/* we advance "entry" in the following if statement */
d2566 2
a2567 1
				pmap_page_protect(pg, VM_PROT_NONE);
d2725 1
d2727 3
d2762 1
d2771 1
d2775 1
d2792 1
d2794 1
a2794 1
#ifdef __sparc__
d2845 1
d2849 1
d3240 1
a3240 1
		(*pr)(" - %p: 0x%lx->0x%lx: obj=%p/0x%llx, amap=%p/%d\n",
d3242 1
a3242 1
		    (long long)entry->offset, entry->aref.ar_amap, entry->aref.ar_pageoff);
d3279 2
a3280 2
	(*pr)("OBJECT %p: locked=%d, pgops=%p, npages=%d, ",
	    uobj, uobj->vmobjlock.lock_data, uobj->pgops, uobj->uo_npages);
d3286 1
a3286 3
	if (!full) {
		return;
	}
d3288 1
a3288 3
	for (pg = TAILQ_FIRST(&uobj->memq);
	     pg != NULL;
	     pg = TAILQ_NEXT(pg, listq), cnt++) {
d3290 1
a3290 6
		if ((cnt % 3) == 2) {
			(*pr)("\n  ");
		}
	}
	if ((cnt % 3) != 2) {
		(*pr)("\n");
d3292 1
a3294 6
const char page_flagbits[] =
	"\20\4CLEAN\5BUSY\6WANTED\7TABLED\12FAKE\13FILLED\14DIRTY\15RELEASED"
	"\16FAULTING\17CLEANCHK";
const char page_pqflagbits[] =
	"\20\1FREE\2INACTIVE\3ACTIVE\4LAUNDRY\5ANON\6AOBJ";

a3320 2
	char pgbuf[128];
	char pqbuf[128];
d3323 2
a3324 4
	snprintf(pgbuf, sizeof(pgbuf), "%b", pg->flags, page_flagbits);
	snprintf(pqbuf, sizeof(pqbuf), "%b", pg->pqflags, page_pqflagbits);
	(*pr)("  flags=%s, pqflags=%s, vers=%d, wire_count=%d, pa=0x%lx\n",
	    pgbuf, pqbuf, pg->version, pg->wire_count, (long)pg->phys_addr);
d3326 1
a3326 1
	    pg->uobject, pg->uanon, pg->offset, pg->loan_count);
d3365 2
a3366 5
	if (pg->pqflags & PQ_FREE) {
		int fl = uvm_page_lookup_freelist(pg);
		pgl = &uvm.page_free[fl].pgfl_queues[((pg)->flags & PG_ZERO) ?
		    PGFL_ZEROS : PGFL_UNKNOWN];
	}
@


1.3.4.5
log
@merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_map.c,v 1.93 2001/02/11 01:34:23 eeh Exp $	*/
d85 3
a97 1
const char vmmapbsy[] = "vmmapbsy";
d155 1
a155 1
#define SAVE_HINT(map,check,value) do { \
d157 1
a157 2
	if ((map)->hint == (check)) \
		(map)->hint = (value); \
a183 2
static void		uvm_map_reference_amap __P((vm_map_entry_t, int));
static void		uvm_map_unreference_amap __P((vm_map_entry_t, int));
d210 1
a210 1
		s = splvm();	/* protect kentry_free list with splvm */
d245 1
a245 1
		s = splvm();	/* protect kentry_free list with splvm */
a282 27

/*
 * wrapper for calling amap_ref()
 */
static __inline void
uvm_map_reference_amap(entry, flags)
	vm_map_entry_t entry;
	int flags;
{
    amap_ref(entry->aref.ar_amap, entry->aref.ar_pageoff,
	     (entry->end - entry->start) >> PAGE_SHIFT, flags);
}


/*
 * wrapper for calling amap_unref() 
 */
static __inline void
uvm_map_unreference_amap(entry, flags)
	vm_map_entry_t entry;
	int flags;
{
    amap_unref(entry->aref.ar_amap, entry->aref.ar_pageoff,
	     (entry->end - entry->start) >> PAGE_SHIFT, flags);
}


d368 1
a368 1

d380 1
a380 1

a461 6
 *
 * => if `align' is non-zero, we try to align the virtual address to
 *	the specified alignment.  this is only a hint; if we can't
 *	do it, the address will be unaligned.  this is provided as
 *	a mechanism for large pages.
 *
d466 1
a466 1
uvm_map(map, startp, size, uobj, uoffset, align, flags)
a471 1
	vsize_t align;
d506 1
a506 1
	    uobj, uoffset, align, flags)) == NULL) {
d545 5
a549 1
			KASSERT(UVM_OBJ_IS_KERN_OBJECT(uobj));
d589 1
a589 1

d594 1
a594 1

d766 1
a766 1
				SAVE_HINT(map, map->hint, cur);
d776 1
a776 1
	SAVE_HINT(map, map->hint, *entry);
d781 1
d785 2
a786 2
 * => "hint" is a hint about where we want it, unless FINDSPACE_FIXED is
 *	set (in which case we insist on using "hint").
a788 1
 * => if `align' is non-zero, we attempt to align to that value.
d795 1
a795 1
uvm_map_findspace(map, hint, length, result, uobj, uoffset, align, flags)
d802 1
a802 2
	vsize_t align;
	int flags;
d805 1
a805 1
	vaddr_t end, orig_hint;
d809 2
a810 4
	UVMHIST_LOG(maphist, "(map=0x%x, hint=0x%x, len=%d, flags=0x%x)", 
		    map, hint, length, flags);
	KASSERT((align & (align - 1)) == 0);
	KASSERT((flags & UVM_FLAG_FIXED) == 0 || align == 0);
a811 7
	/*
	 * remember the original hint.  if we are aligning, then we
	 * may have to try again with no alignment constraint if
	 * we fail the first time.
	 */

	orig_hint = hint;
d813 1
a813 1
		if (flags & UVM_FLAG_FIXED) {
d830 1
a830 1
	if ((flags & UVM_FLAG_FIXED) == 0 && hint == map->min_offset) {
d836 1
a836 1
			if (flags & UVM_FLAG_FIXED) {
d866 3
a868 11
		if ((flags & UVM_FLAG_FIXED) == 0 &&
		    uoffset != UVM_UNKNOWN_OFFSET)
			PMAP_PREFER(uoffset, &hint);
#endif
		if (align != 0) {
			if ((hint & (align - 1)) != 0)
				hint = roundup(hint, align);
			/*
			 * XXX Should we PMAP_PREFER() here again?
			 */
		}
a871 7
			if (align != 0) {
				UVMHIST_LOG(maphist,
				    "calling recursively, no align",
				    0,0,0,0);
				return (uvm_map_findspace(map, orig_hint,
				    length, result, uobj, uoffset, 0, flags));
			}
d877 1
a877 1
		if (flags & UVM_FLAG_FIXED) {
d882 1
a882 1
	SAVE_HINT(map, map->hint, entry);
d925 1
a925 1
		SAVE_HINT(map, entry, entry->prev);
d967 1
a967 1

d982 5
a986 1
			KASSERT(vm_map_pmap(map) == pmap_kernel());
d1057 5
a1061 1
		SAVE_HINT(map, entry, entry->prev);
d1087 1
a1087 1
uvm_unmap_detach(first_entry, flags)
d1089 1
a1089 1
	int flags;
d1095 10
a1104 1
		KASSERT(!VM_MAPENT_ISWIRED(first_entry));
d1116 1
a1116 1
			uvm_map_unreference_amap(first_entry, flags);
d1121 1
a1121 1

d1132 3
d1139 4
d1144 1
d1162 1
a1162 1
uvm_map_reserve(map, size, offset, align, raddr)
d1165 1
a1165 2
	vaddr_t offset;	/* hint for pmap_prefer */
	vsize_t align;	/* alignment hint */
d1169 1
a1169 1

d1172 1
a1172 1

d1176 1
a1176 1

d1180 2
a1181 2

	if (uvm_map(map, raddr, size, NULL, offset, 0,
d1187 1
a1187 1

d1211 2
d1217 1
a1217 1

d1221 1
a1221 1

d1274 1
a1274 1
		SAVE_HINT(map, map->hint, newents);
d1287 1
a1287 1
		SAVE_HINT(map, map->hint, oldent->prev);
a1335 1

d1340 1
d1346 6
a1352 3
	KASSERT((start & PAGE_MASK) == 0 && (len & PAGE_MASK) == 0);
	KASSERT((flags & UVM_EXTRACT_REMOVE) == 0 ||
		(flags & (UVM_EXTRACT_CONTIG|UVM_EXTRACT_QREF)) == 0);
d1359 1
a1359 1
	if (uvm_map_reserve(dstmap, len, start, 0, &dstaddr) == FALSE)
d1364 1
a1380 1

a1387 1

a1389 1

a1393 1

d1395 1
a1395 1
			SAVE_HINT(srcmap, srcmap->hint, entry->prev);
d1398 1
d1400 1
a1400 1

a1409 1

d1414 1
d1421 1
a1421 1

a1436 1

d1481 1
a1481 1
			uvm_map_reference_amap(newentry, AMAP_SHARED |
d1508 1
d1516 1
d1523 2
a1525 1
	if (srcmap == dstmap || vm_map_lock_try(dstmap) == TRUE) {
d1534 1
d1536 1
d1539 1
d1542 1
d1554 1
a1554 1
			SAVE_HINT(srcmap, srcmap->hint, orig_entry->prev);
d1564 1
d1567 1
a1567 1
				elen = MIN(end, entry->end) -
a1595 1

d1625 4
d1662 1
a1662 1

d1670 1
d1675 1
a1675 1

d1679 2
a1680 1
	} else {
d1688 4
d1701 1
d1728 2
a1729 2
		    map, start, end, new_prot);

d1733 1
a1733 1

d1763 1
a1763 1

d1816 1
a1816 1

d1847 3
a1849 3
	case MAP_INHERIT_NONE:
	case MAP_INHERIT_COPY:
	case MAP_INHERIT_SHARE:
d1863 1
a1863 1
	} else {
d1869 1
d1871 1
d1899 1
d1901 1
d1928 2
d1931 1
d1968 6
a1973 2
		    map, start, end, new_pageable);
	KASSERT(map->flags & VM_MAP_PAGEABLE);
d1991 1
a1991 1

a2002 1

a2006 1

a2025 1

d2037 4
d2066 1
a2066 2

			/*
a2072 1

a2089 1

a2093 1

a2097 1

a2138 1

a2141 1

a2153 1

a2163 1

a2181 1

a2184 1

d2217 4
a2220 1
	KASSERT(map->flags & VM_MAP_PAGEABLE);
d2434 2
d2451 2
d2454 4
a2457 4
	UVMHIST_LOG(maphist,"(map=0x%x,start=0x%x,end=0x%x,flags=0x%x)",
		    map, start, end, flags);
	KASSERT((flags & (PGO_FREE|PGO_DEACTIVATE)) !=
		(PGO_FREE|PGO_DEACTIVATE));
a2468 1

d2474 2
a2475 4
		if (end <= current->end) {
			break;
		}
		if (current->end != current->next->start) {
d2483 1
a2483 1
	for (current = entry; start < end; current = current->next) {
d2486 5
a2490 1
		KASSERT(start >= current->start);
d2499 3
d2503 2
a2504 1
		if (amap == NULL || (flags & (PGO_DEACTIVATE|PGO_FREE)) == 0)
d2508 1
d2510 5
a2514 2
		size = MIN(end, current->end) - start;
		for ( ; size != 0; size -= PAGE_SIZE, offset += PAGE_SIZE) {
a2527 1

a2533 1

a2551 1

d2553 6
a2558 1
					KASSERT(pg->uobject == NULL);
d2563 9
a2571 1
				KASSERT(pg->uanon == anon);
a2573 1
				pmap_clear_reference(pg);
a2580 1

a2584 1

d2603 3
d2607 1
d2616 2
a2617 1
		size = MIN(end, current->end) - start;
d2629 1
d2654 1
d2656 1
d2661 1
a2661 1

d2679 1
a2679 1

d2762 1
a2762 1

d2766 1
a2766 1

d2784 1
a2784 1
uvmspace_exec(p, start, end)
a2785 1
	vaddr_t start, end;
d2825 1
a2825 10
		uvm_unmap(map, map->min_offset, map->max_offset);

		/*
		 * resize the map
		 */
		vm_map_lock(map);
		map->min_offset = start;
		map->max_offset = end;
		vm_map_unlock(map);
	
d2834 1
a2834 1
		nvm = uvmspace_alloc(start, end,
a2868 5
#ifdef SYSVSHM
		/* Get rid of any SYSV shared memory segments. */
		if (vm->vm_shm != NULL)
			shmexit(vm);
#endif
d2936 1
a2936 1
		case MAP_INHERIT_NONE:
d2942 1
a2942 1
		case MAP_INHERIT_SHARE:
d2975 1
a2975 1
				uvm_map_reference_amap(new_entry, AMAP_SHARED);
d2999 1
a2999 1
		case MAP_INHERIT_COPY:
d3014 1
a3014 1
				uvm_map_reference_amap(new_entry, 0);
d3028 1
a3028 1

d3072 1
a3072 1

d3154 1
a3154 1

d3195 13
d3236 1
a3236 2
		    (long long)entry->offset, entry->aref.ar_amap,
		    entry->aref.ar_pageoff);
d3238 1
a3238 2
		    "\tsubmap=%c, cow=%c, nc=%c, prot(max)=%d/%d, inh=%d, "
		    "wc=%d, adv=%d\n",
d3248 13
d3287 1
a3287 1
		(*pr)("<%p,0x%llx> ", pg, (long long)pg->offset);
d3297 19
a3319 6
static const char page_flagbits[] =
	"\20\1BUSY\2WANTED\3TABLED\4CLEAN\5CLEANCHK\6RELEASED\7FAKE\10RDONLY"
	"\11ZERO\15PAGER1";
static const char page_pqflagbits[] =
	"\20\1FREE\2INACTIVE\3ACTIVE\4LAUNDRY\5ANON\6AOBJ";

d3326 1
a3326 1
	struct vm_page *tpg;
d3337 2
a3338 2
	(*pr)("  uobject=%p, uanon=%p, offset=0x%llx loan_count=%d\n",
	    pg->uobject, pg->uanon, (long long)pg->offset, pg->loan_count);
d3356 1
a3356 1
			    (*pr)("  >>> ANON DOES NOT POINT HERE <<< (%p)\n",
d3364 3
a3366 4
				TAILQ_FOREACH(tpg, &uobj->memq, listq) {
					if (tpg == pg) {
						break;
					}
d3368 1
a3368 1
				if (tpg)
d3381 3
a3383 2
	} else if (pg->pqflags & PQ_INACTIVE) {
		pgl = (pg->pqflags & PQ_SWAPBACKED) ?
d3385 1
a3385 1
	} else if (pg->pqflags & PQ_ACTIVE) {
d3387 1
a3387 1
 	} else {
a3388 1
	}
d3392 2
a3393 4
		TAILQ_FOREACH(tpg, pgl, pageq) {
			if (tpg == pg) {
				break;
			}
d3395 1
a3395 1
		if (tpg)
@


1.3.4.6
log
@Merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_map.c,v 1.99 2001/06/02 18:09:26 chs Exp $	*/
d4 1
a4 1
/*
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and
d48 1
a48 1
 *
d54 3
a56 3
 *
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
d58 1
a58 1
 *
d179 6
a184 6
static struct vm_map_entry *uvm_mapent_alloc __P((struct vm_map *));
static void uvm_mapent_copy __P((struct vm_map_entry *, struct vm_map_entry *));
static void uvm_mapent_free __P((struct vm_map_entry *));
static void uvm_map_entry_unwire __P((struct vm_map *, struct vm_map_entry *));
static void uvm_map_reference_amap __P((struct vm_map_entry *, int));
static void uvm_map_unreference_amap __P((struct vm_map_entry *, int));
d196 1
a196 1
static __inline struct vm_map_entry *
d198 1
a198 1
	struct vm_map *map;
d200 1
a200 1
	struct vm_map_entry *me;
d222 1
a222 1
	UVMHIST_LOG(maphist, "<- new entry=0x%x [kentry=%d]",
d236 1
a236 1
	struct vm_map_entry *me;
d241 1
a241 1
	UVMHIST_LOG(maphist,"<- freeing map entry=0x%x [flags=%d]",
d261 2
a262 2
	struct vm_map_entry *src;
	struct vm_map_entry *dst;
d264 2
a265 2
	memcpy(dst, src,
	       ((char *)&src->uvm_map_entry_stop_copy) - ((char *)src));
d276 2
a277 2
	struct vm_map *map;
	struct vm_map_entry *entry;
d279 1
d290 1
a290 1
	struct vm_map_entry *entry;
d293 1
a293 1
	amap_ref(entry->aref.ar_amap, entry->aref.ar_pageoff,
d299 1
a299 1
 * wrapper for calling amap_unref()
d303 1
a303 1
	struct vm_map_entry *entry;
d306 1
a306 1
	amap_unref(entry->aref.ar_amap, entry->aref.ar_pageoff,
d313 1
a313 1
 * and init the static pool of struct vm_map_entry *'s for the kernel here.
d317 1
a317 1
uvm_map_init()
d372 1
a372 1
 *
d378 4
a381 5
void
uvm_map_clip_start(map, entry, start)
	struct vm_map *map;
	struct vm_map_entry *entry;
	vaddr_t start;
d383 1
a383 1
	struct vm_map_entry *new_entry;
d397 1
a397 1
	new_entry->end = start;
d413 1
a413 1
		if (UVM_ET_ISOBJ(entry) &&
d424 1
a424 1
 *
d432 2
a433 2
	struct vm_map *map;
	struct vm_map_entry *entry;
d436 1
a436 1
	struct vm_map_entry *	new_entry;
d486 1
a486 1
 *
d501 1
a501 1
	struct vm_map *map;
d509 1
a509 1
	struct vm_map_entry *prev_entry, *new_entry;
d526 1
a526 1
		UVMHIST_LOG(maphist, "<- prot. failure:  prot=0x%x, max=0x%x",
d528 1
a528 1
		return EACCES;
d537 1
a537 1
			return EAGAIN;
d540 1
a540 1
	if ((prev_entry = uvm_map_findspace(map, *startp, size, startp,
d544 1
a544 1
		return ENOMEM;
d562 2
a563 2
	 * [typically from uvm_map_reserve] or it is UVM_UNKNOWN_OFFSET.   in
	 * either case we want to zero it  before storing it in the map entry
d565 2
a566 2
	 *
	 * if uobj is not null
d592 1
a592 1
	if ((flags & UVM_FLAG_NOMERGE) == 0 &&
d603 1
a603 1
		if (prev_entry->protection != prot ||
d613 1
a613 1
			goto step3;
d616 1
a616 1
		 * can't extend a shared amap.  note: no need to lock amap to
d647 1
a647 1
		return 0;
d655 1
a655 1
	 * the number of times we missed a *possible* chance to merge more
d659 1
a659 1
	    prev_entry->next != &map->header &&
d673 1
a673 1
	if (uobj)
d694 1
a694 1
		vaddr_t to_add = (flags & UVM_FLAG_AMAPPAD) ?
d703 1
d705 1
d718 1
a718 1
	return 0;
d731 1
a731 1
	struct vm_map *map;
d733 1
a733 1
	struct vm_map_entry **entry;		/* OUT */
d735 2
a736 2
	struct vm_map_entry *cur;
	struct vm_map_entry *last;
a756 1

a767 1

a776 1

a779 1

d825 1
a825 1
struct vm_map_entry *
d827 1
a827 1
	struct vm_map *map;
d836 1
a836 1
	struct vm_map_entry *entry, *next, *tmp;
d841 1
a841 1
	UVMHIST_LOG(maphist, "(map=0x%x, hint=0x%x, len=%d, flags=0x%x)",
d872 1
a872 1
		if ((entry = map->first_free) != &map->header)
a892 1

a893 1

a906 1

d951 1
a951 1
 * => caller must check alignment and size
d957 1
a957 1
void
d959 3
a961 3
	struct vm_map *map;
	vaddr_t start, end;
	struct vm_map_entry **entry_list;	/* OUT */
d963 1
a963 1
	struct vm_map_entry *entry, *first_entry, *next;
d965 2
a966 1
	UVMHIST_FUNC("uvm_unmap_remove"); UVMHIST_CALLED(maphist);
a975 1

d982 1
d1006 1
a1006 1
	 *       so that we don't block other threads.
a1007 1

d1012 1
a1012 1
	 * break up the area into map entry sized regions and unmap.  note
d1020 1
a1020 1
		UVM_MAP_CLIP_END(map, entry, end);
a1035 1

d1061 2
a1062 2
			 * uvm_km_pgremove currently does the following:
			 *   for pages in the kernel object in range:
a1074 1

a1091 1

a1095 1

a1098 1

d1103 1
a1103 1
		 * remove entry from map and put it on our list of entries
a1105 1

a1116 1
	pmap_update();
d1120 1
a1120 1
	 * references to the mapped objects.
d1125 1
d1136 1
a1136 1
	struct vm_map_entry *first_entry;
d1139 1
a1139 1
	struct vm_map_entry *next_entry;
d1145 2
a1146 2
		    "  detach 0x%x: amap=0x%x, obj=0x%x, submap?=%d",
		    first_entry, first_entry->aref.ar_amap,
d1170 1
d1182 1
a1182 1
/*
d1185 1
a1185 1
 * => we reserve space in a map by putting a dummy map entry in the
d1194 1
a1194 1
	struct vm_map *map;
d1200 1
a1200 1
	UVMHIST_FUNC("uvm_map_reserve"); UVMHIST_CALLED(maphist);
d1215 1
a1215 1
	    UVM_ADV_RANDOM, UVM_FLAG_NOMERGE)) != 0) {
d1218 1
a1218 1
	}
d1225 1
a1225 1
 * uvm_map_replace: replace a reserved (blank) area of memory with
d1228 1
a1228 1
 * => caller must WRITE-LOCK the map
d1239 1
a1239 1
	struct vm_map_entry *newents;
d1242 1
a1242 1
	struct vm_map_entry *oldent, *last;
d1256 1
a1256 1
	if (oldent->start != start || oldent->end != end ||
a1261 1

a1264 1

d1266 1
a1266 1
		struct vm_map_entry *tmpent = newents;
d1300 2
a1301 1
		last = newents->prev;
d1354 1
a1354 1
	struct vm_map *srcmap, *dstmap;
d1361 2
a1362 2
	struct vm_map_entry *chain, *endchain, *entry, *orig_entry, *newentry,
	    *deadentry, *oldentry;
d1392 1
a1392 1
	 * step 2: setup for the extraction process loop by init'ing the
d1502 2
a1503 2
		newentry->protection = (flags & UVM_EXTRACT_FIXPROT) ?
			entry->max_protection : entry->protection;
d1528 1
a1528 1
		if ((flags & UVM_EXTRACT_CONTIG) && entry->end < end &&
d1547 1
a1547 1
	 * note usage of copy_ok:
d1598 1
a1598 1
				pmap_remove(srcmap->pmap, entry->start,
a1612 1
		pmap_update();
a1621 2
	} else {
		deadentry = NULL;
d1623 2
d1675 1
a1675 1
 * => to remove a submap, use uvm_unmap() on the main map
d1684 1
a1684 1
	struct vm_map *map, *submap;
d1687 2
a1688 2
	struct vm_map_entry *entry;
	int error;
d1701 1
a1701 1
	if (entry != NULL &&
d1709 1
a1709 1
		error = 0;
d1711 1
a1711 1
		error = EINVAL;
d1714 1
a1714 1
	return error;
d1727 1
d1731 1
a1731 1
	struct vm_map *map;
d1736 2
a1737 2
	struct vm_map_entry *current, *entry;
	int error = 0;
d1743 1
d1745 1
d1759 1
a1759 1
			error = EINVAL;
d1763 1
a1763 1
			error = EACCES;
d1772 1
d1777 1
d1786 1
a1786 1
		 * update physical map if necessary.  worry about copy-on-write
d1808 1
a1808 2
			    UVM_LK_ENTER|UVM_LK_EXIT) != 0) {

d1813 2
a1814 2
				 * the map, but will return the error
				 * condition regardless.
d1822 1
a1822 2

				error = ENOMEM;
d1825 1
a1827 1
	pmap_update();
d1831 2
a1832 2
	UVMHIST_LOG(maphist, "<- done, error=%d",error,0,0,0);
	return error;
d1835 1
d1838 1
a1838 1
/*
d1848 1
a1848 1
	struct vm_map *map;
d1853 1
a1853 1
	struct vm_map_entry *entry, *temp_entry;
d1865 1
a1865 1
		return EINVAL;
d1868 2
a1869 1
	vm_map_lock(map);	
d1871 1
d1884 1
d1887 1
a1887 1
	return 0;
d1890 1
a1890 1
/*
d1898 1
a1898 1
	struct vm_map *map;
d1903 1
a1903 1
	struct vm_map_entry *entry, *temp_entry;
d1934 1
a1934 1
			return EINVAL;
d1942 1
a1942 1
	return 0;
d1961 1
a1961 1
	struct vm_map *map;
d1966 1
a1966 1
	struct vm_map_entry *entry, *start_entry, *failed_entry;
d1978 1
d1981 1
a1981 1
	/*
d1986 1
a1986 1
	 * making any changes.
d1993 2
a1994 2
		UVMHIST_LOG(maphist,"<- done (fault)",0,0,0,0);
		return EFAULT;
d1998 1
a1998 1
	/*
d2007 1
a2007 1
		 * really wired down and that there are no holes.
d2017 3
a2019 2
				UVMHIST_LOG(maphist, "<- done (INVAL)",0,0,0,0);
				return EINVAL;
d2024 1
a2024 1
		/*
d2040 1
a2040 1
		return 0;
d2048 1
a2048 1
	 *    be wired and increment its wiring count.
d2076 1
a2076 1
				if (UVM_ET_ISNEEDSCOPY(entry) &&
d2080 1
a2080 1
					    start, end);
d2090 1
a2090 1
		 * Check for holes
d2100 1
a2100 1
			 * be undone, but the wired counts need to be restored.
d2110 1
a2110 1
			return EINVAL;
a2131 1

a2136 1

d2201 1
a2201 1
	return 0;
d2215 1
a2215 1
	struct vm_map *map;
d2219 1
a2219 1
	struct vm_map_entry *entry, *failed_entry;
a2236 1

a2240 1

d2249 5
a2253 1
		return 0;
a2256 1

a2259 1

a2263 1

a2266 1

d2269 1
a2269 1
		return 0;
d2305 1
a2305 1
		return ENOMEM;
d2313 1
a2313 1
		return ENOMEM;
a2325 1

a2332 1

d2334 1
a2334 1
				if (UVM_ET_ISNEEDSCOPY(entry) &&
d2356 1
a2356 1
	rv = 0;
a2362 1

a2367 1

d2373 1
a2373 2
	if (rv) {

a2376 1

a2390 1

a2404 1

d2423 1
a2423 1
	return 0;
d2435 1
a2435 1
 * => never a need to flush amap layer since the anonymous memory has
d2444 1
a2444 1
	struct vm_map *map;
d2448 1
a2448 1
	struct vm_map_entry *current, *entry;
d2467 1
a2467 1
		return EFAULT;
d2477 1
a2477 1
			return EINVAL;
d2484 1
a2484 1
			return EFAULT;
d2488 2
a2489 1
	error = 0;
d2608 1
a2608 1
				error = EIO;
d2613 1
a2613 1
	return (error);
d2626 15
a2640 15
	struct vm_map * map;
	vaddr_t start, end;
	vm_prot_t protection;
{
	struct vm_map_entry *entry;
	struct vm_map_entry *tmp_entry;

	if (!uvm_map_lookup_entry(map, start, &tmp_entry)) {
		return(FALSE);
	}
	entry = tmp_entry;
	while (start < end) {
		if (entry == &map->header) {
			return(FALSE);
		}
d2646 3
a2648 3
		if (start < entry->start) {
			return(FALSE);
		}
d2654 10
a2663 7
		if ((entry->protection & protection) != protection) {
			return(FALSE);
		}
		start = entry->end;
		entry = entry->next;
	}
	return(TRUE);
d2703 1
d2705 1
d2711 1
d2739 1
a2739 1
	struct proc *p;
d2751 1
a2751 1
	p->p_vmspace = nvm;
d2769 1
a2769 1
	struct vm_map *map = &ovm->vm_map;
a2790 1

a2798 1

a2805 1

a2810 1

d2815 2
a2823 1

d2849 1
a2849 1
	struct vm_map_entry *dead_entries;
a2853 1

a2858 1

d2866 1
a2866 1
			uvm_unmap_remove(&vm->vm_map,
d2894 6
a2899 6
	struct vm_map *old_map = &vm1->vm_map;
	struct vm_map *new_map;
	struct vm_map_entry *old_entry;
	struct vm_map_entry *new_entry;
	pmap_t new_pmap;
	boolean_t protect_child;
d2922 6
a2928 3
		KASSERT(!UVM_ET_ISSUBMAP(old_entry));
		KASSERT(UVM_ET_ISCOPYONWRITE(old_entry) ||
			!UVM_ET_ISNEEDSCOPY(old_entry));
a2931 1

a2934 1

a2937 1

d2942 1
d2953 1
a2953 1
				    0, 0);
a2967 1

d2969 1
d2982 1
a2982 1
			/*
d3000 1
a3000 1
			 * allocate new_entry, adjust reference counts.
d3036 1
a3036 1
			 *    process is sharing the amap with another
d3040 1
a3040 1
			 *    same amap with "needs_copy" set.  if the
d3043 1
a3043 1
			 *    a new amap.   this is wrong because the
d3049 1
a3049 1
			 *    amap_cow_now to avoid page faults in the
d3057 9
a3065 8
				if ((amap_flags(old_entry->aref.ar_amap) &
				     AMAP_SHARED) != 0 ||
				    VM_MAPENT_ISWIRED(old_entry)) {

					amap_copy(new_map, new_entry, M_WAITOK,
					    FALSE, 0, 0);
					/* XXXCDC: M_WAITOK ... ok? */
				}
d3081 1
a3081 1
			  /*
d3083 1
a3083 1
			   * (note that there is nothing to do if
d3091 1
a3091 1
			} else {
a3118 1
				pmap_update();
d3130 1
a3130 1
			     * we only need to protect the child if the
d3155 2
a3156 2
					 new_entry->end,
					 new_entry->protection &
a3157 1
			    pmap_update();
d3167 1
a3167 1
	vm_map_unlock(old_map);
d3179 1
a3179 1
	return(vm2);
d3195 1
a3195 1
	struct vm_map *map;
d3199 1
a3199 1
	struct vm_map_entry *entry;
d3224 1
a3224 1
		    (entry->etype & UVM_ET_COPYONWRITE) ? 'T' : 'F',
d3229 1
a3229 1
}
d3266 1
a3266 1
}
d3338 2
a3339 3
		int color = VM_PGCOLOR_BUCKET(pg);
		pgl = &uvm.page_free[fl].pgfl_buckets[color].pgfl_queues[
		    ((pg)->flags & PG_ZERO) ? PGFL_ZEROS : PGFL_UNKNOWN];
d3341 2
a3342 1
		pgl = &uvm.page_inactive;
@


1.3.4.7
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_map.c,v 1.86 2000/11/27 08:40:03 chs Exp $	*/
d4 1
a4 1
/* 
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and 
d48 1
a48 1
 * 
d54 3
a56 3
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
d58 1
a58 1
 * 
a79 1
#include <sys/kernel.h>
d92 1
a107 1
struct pool uvm_map_entry_kmem_pool;
a134 1
	uvm_rb_insert(map, entry); \
a145 1
	uvm_rb_remove(map, entry); \
d179 6
a184 82
static vm_map_entry_t	uvm_mapent_alloc __P((vm_map_t));
static void		uvm_mapent_copy __P((vm_map_entry_t,vm_map_entry_t));
static void		uvm_mapent_free __P((vm_map_entry_t));
static void		uvm_map_entry_unwire __P((vm_map_t, vm_map_entry_t));
static void		uvm_map_reference_amap __P((vm_map_entry_t, int));
static void		uvm_map_unreference_amap __P((vm_map_entry_t, int));

int _uvm_tree_sanity(vm_map_t map, char *name);

static __inline int
uvm_compare(vm_map_entry_t a, vm_map_entry_t b)
{
	if (a->start < b->start)
		return (-1);
	else if (a->start > b->start)
		return (1);
	
	return (0);
}


RB_PROTOTYPE(uvm_tree, vm_map_entry, rb_entry, uvm_compare);

RB_GENERATE(uvm_tree, vm_map_entry, rb_entry, uvm_compare);

static __inline void
uvm_rb_insert(vm_map_t map, vm_map_entry_t entry)
{
	RB_INSERT(uvm_tree, &(map)->rbhead, entry);
}

static __inline void
uvm_rb_remove(vm_map_t map, vm_map_entry_t entry)
{
	RB_REMOVE(uvm_tree, &(map)->rbhead, entry);
}

#define uvm_tree_sanity(x,y)

int
_uvm_tree_sanity(vm_map_t map, char *name)
{
	vm_map_entry_t tmp, trtmp;
	int n = 0, i = 1;

	trtmp = NULL;
	RB_FOREACH(tmp, uvm_tree, &map->rbhead) {
		if (trtmp != NULL && trtmp->start >= tmp->start) {
			printf("%s: corrupt: %p >= %p\n",
			    name, trtmp->start, tmp->start);
			goto error;
		}
		n++;

	    trtmp = tmp;
	}

	if (n != map->nentries) {
		printf("%s: nentries: %d vs %d\n",
		    name, n, map->nentries);
		goto error;
	}

	for (tmp = map->header.next; tmp && tmp != &map->header;
	    tmp = tmp->next, i++) {
		trtmp = RB_FIND(uvm_tree, &map->rbhead, tmp);
		if (trtmp != tmp) {
			printf("%s: lookup: %d: %p - %p: %p\n",
			    name, i, tmp, trtmp,
			    RB_PARENT(tmp, rb_entry));
			goto error;
		}
	}

	return (0);
 error:
#ifdef	DDB
	/* handy breakpoint location for error case */
	__asm(".globl treesanity_label ; treesanity_label:");
#endif
	return (-1);
}
d192 2
d196 1
a196 1
static __inline vm_map_entry_t
d198 1
a198 1
	vm_map_t map;
d202 2
a203 1
	UVMHIST_FUNC("uvm_mapent_alloc"); UVMHIST_CALLED(maphist);
d205 7
a211 2
	if (map->flags & VM_MAP_INTRSAFE || cold) {
		s = splvm();
d217 2
a218 5
		if (me == NULL) {
			panic("uvm_mapent_alloc: out of static map entries, "
			      "check MAX_KMAPENT (currently %d)",
			      MAX_KMAPENT);
		}
a219 6
	} else if (map == kernel_map) {
		me = pool_get(&uvm_map_entry_kmem_pool, PR_WAITOK);
		me->flags = UVM_MAP_KMEM;
	} else {
		me = pool_get(&uvm_map_entry_pool, PR_WAITOK);
		me->flags = 0;
d222 3
a224 2
	UVMHIST_LOG(maphist, "<- new entry=0x%x [kentry=%d]", me,
	    ((map->flags & VM_MAP_INTRSAFE) != 0 || map == kernel_map), 0, 0);
d236 1
a236 1
	vm_map_entry_t me;
d239 2
a240 2
	UVMHIST_FUNC("uvm_mapent_free"); UVMHIST_CALLED(maphist);

d243 4
a246 2
	if (me->flags & UVM_MAP_STATIC) {
		s = splvm();
a251 4
	} else if (me->flags & UVM_MAP_KMEM) {
		pool_put(&uvm_map_entry_kmem_pool, me);
	} else {
		pool_put(&uvm_map_entry_pool, me);
d261 2
a262 2
	vm_map_entry_t src;
	vm_map_entry_t dst;
d264 2
a265 2

	memcpy(dst, src, ((char *)&src->uvm_map_entry_stop_copy) - ((char*)src));
d276 2
a277 2
	vm_map_t map;
	vm_map_entry_t entry;
a278 1

d289 1
a289 1
	vm_map_entry_t entry;
d292 1
a292 1
    amap_ref(entry->aref.ar_amap, entry->aref.ar_pageoff,
d298 1
a298 1
 * wrapper for calling amap_unref() 
d302 1
a302 1
	vm_map_entry_t entry;
d305 1
a305 1
    amap_unref(entry->aref.ar_amap, entry->aref.ar_pageoff,
d312 1
a312 1
 * and init the static pool of vm_map_entry_t's for the kernel here.
d316 1
a316 1
uvm_map_init() 
d357 2
a358 1
	    0, 0, 0, "vmsppl", &pool_allocator_nointr);
d360 2
a361 3
	    0, 0, 0, "vmmpepl", &pool_allocator_nointr);
	pool_init(&uvm_map_entry_kmem_pool, sizeof(struct vm_map_entry),
	    0, 0, 0, "vmmpekpl", NULL);
d371 1
a371 1
 * 
d377 5
a381 4
void uvm_map_clip_start(map, entry, start)
	vm_map_t       map;
	vm_map_entry_t entry;
	vaddr_t    start;
d383 1
a383 1
	vm_map_entry_t new_entry;
a387 2
	uvm_tree_sanity(map, "clip_start entry");

d397 1
a397 1
	new_entry->end = start; 
a400 2

	/* Does not change order for the RB tree */
d413 1
a413 1
		if (UVM_ET_ISOBJ(entry) && 
a418 2

	uvm_tree_sanity(map, "clip_start leave");
d424 1
a424 1
 * 
d432 2
a433 2
	vm_map_t	map;
	vm_map_entry_t	entry;
d436 1
a436 1
	vm_map_entry_t	new_entry;
a438 1
	uvm_tree_sanity(map, "clip_end entry");
d454 1
a454 1
	
a466 1
	uvm_tree_sanity(map, "clip_end leave");
d486 1
a486 1
 *	
d501 1
a501 1
	vm_map_t map;
d509 1
a509 1
	vm_map_entry_t prev_entry, new_entry;
a520 2
	uvm_tree_sanity(map, "map entry");

d526 1
a526 1
		UVMHIST_LOG(maphist, "<- prot. failure:  prot=0x%x, max=0x%x", 
d528 1
a528 1
		return(KERN_PROTECTION_FAILURE);
d537 1
a537 1
			return(KERN_FAILURE);
d540 1
a540 1
	if ((prev_entry = uvm_map_findspace(map, *startp, size, startp, 
d544 1
a544 1
		return (KERN_NO_SPACE);
d562 2
a563 2
	 * [typically from uvm_map_reserve] or it is UVM_UNKNOWN_OFFSET.   in 
	 * either case we want to zero it  before storing it in the map entry 
d565 2
a566 2
	 * 
	 * if uobj is not null 
d592 1
a592 1
	if ((flags & UVM_FLAG_NOMERGE) == 0 && 
d603 1
a603 1
		if (prev_entry->protection != prot || 
d613 1
a613 1
			goto step3; 
d616 1
a616 1
		 * can't extend a shared amap.  note: no need to lock amap to 
a644 2
		uvm_tree_sanity(map, "map leave 2");

d647 1
a647 1
		return (KERN_SUCCESS);
d655 1
a655 1
	 * the number of times we missed a *possible* chance to merge more 
d659 1
a659 1
	    prev_entry->next != &map->header && 
d673 1
a673 1
	if (uobj) 
d694 1
a694 1
		vaddr_t to_add = (flags & UVM_FLAG_AMAPPAD) ? 
a702 1

a703 1

a713 2
	uvm_tree_sanity(map, "map leave");

d716 1
a716 1
	return(KERN_SUCCESS);
d729 1
a729 1
	vm_map_t	map;
d731 1
a731 1
	vm_map_entry_t		*entry;		/* OUT */
d733 2
a734 3
	vm_map_entry_t		cur;
	vm_map_entry_t		last;
	int			use_tree = 0;
d755 1
d767 1
d776 1
a777 3
		if (map->nentries > 30)
			use_tree = 1;
	} else {
d781 1
a783 28
		use_tree = 1;
	}

	uvm_tree_sanity(map, __FUNCTION__);

	if (use_tree) {
		vm_map_entry_t prev = &map->header;
		cur = RB_ROOT(&map->rbhead);

		/*
		 * Simple lookup in the tree.  Happens when the hint is
		 * invalid, or nentries reach a threshold.
		 */
		while (cur) {
			if (address >= cur->start) {
				if (address < cur->end) {
					*entry = cur;
					SAVE_HINT(map, map->hint, cur);
					return (TRUE);
				}
				prev = cur;
				cur = RB_RIGHT(cur, rb_entry);
			} else
				cur = RB_LEFT(cur, rb_entry);
		}
		*entry = prev;
		UVMHIST_LOG(maphist,"<- failed!",0,0,0,0);
		return (FALSE);
a807 1

d827 1
a827 1
vm_map_entry_t
d829 1
a829 1
	vm_map_t map;
d838 1
a838 2
	vm_map_entry_t entry, next, tmp;

d843 1
a843 1
	UVMHIST_LOG(maphist, "(map=0x%x, hint=0x%x, len=%d, flags=0x%x)", 
a847 2
	uvm_tree_sanity(map, "map_findspace entry");

d874 1
a874 1
		if ((entry = map->first_free) != &map->header) 
a888 13
	if (flags & UVM_FLAG_FIXED) {
		end = hint + length;
		if (end > map->max_offset || end < hint) {
			UVMHIST_LOG(maphist,"<- failed (off end)", 0,0,0,0);
			goto error;
		}
		next = entry->next;
		if (next == &map->header || next->start >= end)
			goto found;
		UVMHIST_LOG(maphist,"<- fixed mapping failed", 0,0,0,0);
		return(NULL); /* only one shot at it ... */
	}

d895 1
d897 1
d911 3
a913 1
		if (uoffset != UVM_UNKNOWN_OFFSET)
d926 8
a933 1
			goto error;
d938 4
a942 1
 found:
a946 10

 error:
	if (align != 0) {
		UVMHIST_LOG(maphist,
		    "calling recursively, no align",
		    0,0,0,0);
		return (uvm_map_findspace(map, orig_hint,
			    length, result, uobj, uoffset, 0, flags));
	}
	return (NULL);
d956 1
a956 1
 * => caller must check alignment and size 
d962 1
a962 1
int
d964 3
a966 3
	vm_map_t map;
	vaddr_t start,end;
	vm_map_entry_t *entry_list;	/* OUT */
d968 1
a968 1
	vm_map_entry_t entry, first_entry, next;
d970 1
a970 2
	UVMHIST_FUNC("uvm_unmap_remove");
	UVMHIST_CALLED(maphist);
a976 2
	uvm_tree_sanity(map, "unmap_remove entry");

d980 1
a986 1

d1010 1
a1010 1
	 *       so that we don't block other threads.  
d1012 1
d1017 1
a1017 1
	 * break up the area into map entry sized regions and unmap.  note 
d1025 1
a1025 1
		UVM_MAP_CLIP_END(map, entry, end); 
d1041 1
d1067 2
a1068 2
			 * uvm_km_pgremove currently does the following: 
			 *   for pages in the kernel object in range: 
d1081 1
d1099 1
d1104 1
d1108 1
d1113 1
a1113 1
		 * remove entry from map and put it on our list of entries 
d1116 1
d1128 1
a1128 2

	uvm_tree_sanity(map, "unmap_remove leave");
d1132 1
a1132 1
	 * references to the mapped objects.  
a1136 1
	return(KERN_SUCCESS);
d1147 1
a1147 1
	vm_map_entry_t first_entry;
d1150 1
a1150 1
	vm_map_entry_t next_entry;
d1156 2
a1157 2
		    "  detach 0x%x: amap=0x%x, obj=0x%x, submap?=%d", 
		    first_entry, first_entry->aref.ar_amap, 
a1180 1

d1192 1
a1192 1
/* 
d1195 1
a1195 1
 * => we reserve space in a map by putting a dummy map entry in the 
d1204 1
a1204 1
	vm_map_t map;
d1210 1
a1210 1
	UVMHIST_FUNC("uvm_map_reserve"); UVMHIST_CALLED(maphist); 
d1225 1
a1225 1
	    UVM_ADV_RANDOM, UVM_FLAG_NOMERGE)) != KERN_SUCCESS) {
d1228 1
a1228 1
	}     
d1235 1
a1235 1
 * uvm_map_replace: replace a reserved (blank) area of memory with 
d1238 1
a1238 1
 * => caller must WRITE-LOCK the map 
d1249 1
a1249 1
	vm_map_entry_t newents;
d1252 1
a1252 3
	vm_map_entry_t oldent, last;

	uvm_tree_sanity(map, "map_replace entry");
d1266 1
a1266 1
	if (oldent->start != start || oldent->end != end || 
d1272 1
d1276 1
d1278 1
a1278 1
		vm_map_entry_t tmpent = newents;
d1312 1
a1312 1
		last = newents->prev;		/* we expect this */
a1320 4

		/* Fix RB tree */
		uvm_rb_remove(map, oldent);

a1324 11
		/* Fixup the RB tree */
		{
			int i;
			vm_map_entry_t tmp;

			tmp = newents;
			for (i = 0; i < nnewents && tmp; i++) {
				uvm_rb_insert(map, tmp);
				tmp = tmp->next;
			}
		}
a1336 2
	uvm_tree_sanity(map, "map_replace leave");

d1365 1
a1365 1
	vm_map_t srcmap, dstmap;
d1372 2
a1373 2
	vm_map_entry_t chain, endchain, entry, orig_entry, newentry, deadentry;
	vm_map_entry_t oldentry;
a1381 3
	uvm_tree_sanity(srcmap, "map_extract src enter");
	uvm_tree_sanity(dstmap, "map_extract dst enter");

d1403 1
a1403 1
	 * step 2: setup for the extraction process loop by init'ing the 
d1513 2
a1514 2
		newentry->protection = (flags & UVM_EXTRACT_FIXPROT) ? 
			entry->max_protection : entry->protection; 
d1539 1
a1539 1
		if ((flags & UVM_EXTRACT_CONTIG) && entry->end < end && 
d1558 1
a1558 1
	 * note usage of copy_ok: 
d1609 1
a1609 1
				pmap_remove(srcmap->pmap, entry->start, 
d1624 1
d1634 2
a1636 2
	else
		deadentry = NULL; /* XXX: gcc */
a1659 4

	uvm_tree_sanity(srcmap, "map_extract src leave");
	uvm_tree_sanity(dstmap, "map_extract dst leave");

a1670 4

	uvm_tree_sanity(srcmap, "map_extract src err leave");
	uvm_tree_sanity(dstmap, "map_extract dst err leave");

d1687 1
a1687 1
 * => to remove a submap, use uvm_unmap() on the main map 
d1696 1
a1696 1
	vm_map_t map, submap;
d1699 2
a1700 2
	vm_map_entry_t entry;
	int result;
d1713 1
a1713 1
	if (entry != NULL && 
d1721 1
a1721 1
		result = KERN_SUCCESS;
d1723 1
a1723 1
		result = KERN_INVALID_ARGUMENT;
d1726 1
a1726 1
	return(result);
a1738 1
#define max(a,b)        ((a) > (b) ? (a) : (b))
d1742 1
a1742 1
	vm_map_t map;
d1747 2
a1748 2
	vm_map_entry_t current, entry;
	int rv = KERN_SUCCESS;
a1753 1

a1754 1

d1768 1
a1768 1
			rv = KERN_INVALID_ARGUMENT;
d1772 1
a1772 1
			rv = KERN_PROTECTION_FAILURE;
a1780 1

a1784 1

d1793 1
a1793 1
		 * update physical map if necessary.  worry about copy-on-write 
d1815 2
a1816 1
			    UVM_LK_ENTER|UVM_LK_EXIT) != KERN_SUCCESS) {
d1821 2
a1822 2
				 * the map, but will return the resource
				 * shortage condition regardless.
d1830 2
a1831 1
				rv = KERN_RESOURCE_SHORTAGE;
a1833 1

d1836 1
d1840 2
a1841 2
	UVMHIST_LOG(maphist, "<- done, rv=%d",rv,0,0,0);
	return (rv);
a1843 1
#undef  max
d1846 1
a1846 1
/* 
d1856 1
a1856 1
	vm_map_t map;
d1861 1
a1861 1
	vm_map_entry_t entry, temp_entry;
d1873 1
a1873 1
		return (KERN_INVALID_ARGUMENT);
d1876 1
a1876 2
	vm_map_lock(map);
	
a1877 1
	
a1889 1

d1892 1
a1892 1
	return(KERN_SUCCESS);
d1895 1
a1895 1
/* 
d1903 1
a1903 1
	vm_map_t map;
d1908 1
a1908 1
	vm_map_entry_t entry, temp_entry;
d1939 1
a1939 1
			return (KERN_INVALID_ARGUMENT);
d1947 1
a1947 1
	return (KERN_SUCCESS);
d1966 1
a1966 1
	vm_map_t map;
d1971 1
a1971 1
	vm_map_entry_t entry, start_entry, failed_entry;
a1982 1

d1985 1
a1985 1
	/* 
d1990 1
a1990 1
	 * making any changes.  
d1997 2
a1998 2
		UVMHIST_LOG(maphist,"<- done (INVALID ARG)",0,0,0,0);
		return (KERN_INVALID_ADDRESS);
d2002 1
a2002 1
	/* 
d2011 1
a2011 1
		 * really wired down and that there are no holes.  
d2021 2
a2022 3
				UVMHIST_LOG(maphist,
				    "<- done (INVALID UNWIRE ARG)",0,0,0,0);
				return (KERN_INVALID_ARGUMENT);
d2027 1
a2027 1
		/* 
d2043 1
a2043 1
		return(KERN_SUCCESS);
d2051 1
a2051 1
	 *    be wired and increment its wiring count.  
d2079 1
a2079 1
				if (UVM_ET_ISNEEDSCOPY(entry) && 
d2083 1
a2083 1
					    start, end); 
d2093 1
a2093 1
		 * Check for holes 
d2103 1
a2103 1
			 * be undone, but the wired counts need to be restored. 
d2113 1
a2113 1
			return (KERN_INVALID_ARGUMENT);
d2135 1
d2141 1
d2206 1
a2206 1
	return(KERN_SUCCESS);
d2220 1
a2220 1
	vm_map_t map;
d2224 1
a2224 1
	vm_map_entry_t entry, failed_entry;
d2242 1
d2247 1
d2256 1
a2256 5
		return (KERN_SUCCESS);

		/*
		 * end of unwire case!
		 */
d2260 1
d2264 1
d2269 1
d2273 1
d2276 1
a2276 1
		return (KERN_SUCCESS);
d2312 1
a2312 1
		return (KERN_NO_SPACE);		/* XXX overloaded */
d2320 1
a2320 1
		return (KERN_NO_SPACE);		/* XXX overloaded */
d2333 1
d2341 1
d2343 1
a2343 1
				if (UVM_ET_ISNEEDSCOPY(entry) && 
d2365 1
a2365 1
	rv = KERN_SUCCESS;
d2372 1
d2378 1
d2384 2
a2385 1
	if (rv) {	/* failed? */
d2389 1
d2404 1
d2419 1
d2438 1
a2438 1
	return (KERN_SUCCESS);
d2450 1
a2450 1
 * => never a need to flush amap layer since the anonymous memory has 
a2456 2
int	amap_clean_works = 1;	/* XXX for now, just in case... */

d2459 1
a2459 1
	vm_map_t map;
d2463 1
a2463 1
	vm_map_entry_t current, entry;
d2482 1
a2482 1
		return(KERN_INVALID_ADDRESS);
d2492 4
a2495 1
			return (KERN_INVALID_ARGUMENT);
d2497 1
a2497 2
		if (end > current->end && (current->next == &map->header ||
		    current->end != current->next->start)) {
d2499 1
a2499 1
			return (KERN_INVALID_ADDRESS);
d2503 2
a2504 3
	error = KERN_SUCCESS;

	for (current = entry; current->start < end; current = current->next) {
a2519 4
		/* XXX for now, just in case... */
		if (amap_clean_works == 0)
			goto flush_object;

a2571 1
#ifdef UBC
a2573 6
#else
				/* zap all mappings for the page. */
				pmap_page_protect(pg, VM_PROT_NONE);

				/* ...and deactivate the page. */
#endif
d2622 1
a2622 1
				error = KERN_FAILURE;
d2627 1
a2627 1
	return (error); 
d2640 15
a2654 15
	vm_map_t       map;
	vaddr_t    start, end;
	vm_prot_t      protection;
{
	 vm_map_entry_t entry;
	 vm_map_entry_t tmp_entry;

	 if (!uvm_map_lookup_entry(map, start, &tmp_entry)) {
		 return(FALSE);
	 }
	 entry = tmp_entry;
	 while (start < end) {
		 if (entry == &map->header) {
			 return(FALSE);
		 }
d2660 3
a2662 3
		 if (start < entry->start) {
			 return(FALSE);
		 }
d2668 7
a2674 10
		 if ((entry->protection & protection) != protection) {
			 return(FALSE);
		 }

		 /* go to next entry */

		 start = entry->end;
		 entry = entry->next;
	 }
	 return(TRUE);
a2713 1

a2714 1

a2719 1

d2747 1
a2747 1
	struct proc *p; 
d2759 1
a2759 1
	p->p_vmspace = nvm; 
d2777 1
a2777 1
	vm_map_t map = &ovm->vm_map;
d2799 1
d2808 1
d2816 1
d2822 1
a2824 1
		uvm_tree_sanity(map, "resize enter");
a2825 1
		uvm_tree_sanity(map, "resize leave");
a2826 2
	

d2834 1
d2860 1
a2860 1
	vm_map_entry_t dead_entries;
d2865 1
d2871 1
d2879 1
a2879 1
			(void)uvm_unmap_remove(&vm->vm_map,
d2907 6
a2912 6
	vm_map_t        old_map = &vm1->vm_map;
	vm_map_t        new_map;
	vm_map_entry_t  old_entry;
	vm_map_entry_t  new_entry;
	pmap_t          new_pmap;
	boolean_t	protect_child;
a2934 6
		if (UVM_ET_ISSUBMAP(old_entry))
		    panic("fork: encountered a submap during fork (illegal)");

		if (!UVM_ET_ISCOPYONWRITE(old_entry) &&
			    UVM_ET_ISNEEDSCOPY(old_entry))
	panic("fork: non-copy_on_write map entry marked needs_copy (illegal)");
d2936 3
d2942 1
d2946 1
d2950 1
a2954 1

d2965 1
a2965 1
				    0, 0); 
d2980 1
a2981 1
				/* share reference */
d2994 1
a2994 1
			/* 
d3012 1
a3012 1
			 * allocate new_entry, adjust reference counts.  
d3048 1
a3048 1
			 *    process is sharing the amap with another 
d3052 1
a3052 1
			 *    same amap with "needs_copy" set.  if the 
d3055 1
a3055 1
			 *    a new amap.   this is wrong because the 
d3061 1
a3061 1
			 *    amap_cow_now to avoid page faults in the 
d3069 8
a3076 9

			  if ((amap_flags(old_entry->aref.ar_amap) & 
			       AMAP_SHARED) != 0 ||
			      VM_MAPENT_ISWIRED(old_entry)) {

			    amap_copy(new_map, new_entry, M_WAITOK, FALSE,
				      0, 0);
			    /* XXXCDC: M_WAITOK ... ok? */
			  }
d3092 1
a3092 1
			  /* 
d3094 1
a3094 1
			   * (note that there is nothing to do if 
d3102 1
a3102 1
			} else { 
d3130 1
d3142 1
a3142 1
			     * we only need to protect the child if the 
d3167 2
a3168 2
					 new_entry->end, 
					 new_entry->protection & 
d3170 1
d3180 1
a3180 1
	vm_map_unlock(old_map); 
d3192 1
a3192 1
	return(vm2);    
d3208 1
a3208 1
	vm_map_t map;
d3212 1
a3212 1
	vm_map_entry_t entry;
d3237 1
a3237 1
		    (entry->etype & UVM_ET_COPYONWRITE) ? 'T' : 'F', 
d3242 1
a3242 1
} 
d3279 1
a3279 1
} 
d3351 3
a3353 2
		pgl = &uvm.page_free[fl].pgfl_queues[((pg)->flags & PG_ZERO) ?
		    PGFL_ZEROS : PGFL_UNKNOWN];
d3355 1
a3355 2
		pgl = (pg->pqflags & PQ_SWAPBACKED) ?
		    &uvm.page_inactive_swp : &uvm.page_inactive_obj;
@


1.3.4.8
log
@Merge in -current from roughly a week ago
@
text
@a85 1
#define RB_AUGMENT(x) uvm_rb_augment(x)
d182 6
a187 8
static vm_map_entry_t	uvm_mapent_alloc(vm_map_t);
static void		uvm_mapent_copy(vm_map_entry_t,vm_map_entry_t);
static void		uvm_mapent_free(vm_map_entry_t);
static void		uvm_map_entry_unwire(vm_map_t, vm_map_entry_t);
static void		uvm_map_reference_amap(vm_map_entry_t, int);
static void		uvm_map_unreference_amap(vm_map_entry_t, int);

int			uvm_map_spacefits(vm_map_t, vaddr_t *, vsize_t, vm_map_entry_t, voff_t, vsize_t);
a189 1
static int		uvm_rb_subtree_space(vm_map_entry_t);
a202 6
static __inline void
uvm_rb_augment(vm_map_entry_t entry)
{
	entry->space = uvm_rb_subtree_space(entry);
}

a206 46
static __inline int
uvm_rb_space(vm_map_t map, vm_map_entry_t entry)
{
	vm_map_entry_t next;
	vaddr_t space;

	if ((next = entry->next) == &map->header)
		space = map->max_offset - entry->end;
	else {
		KASSERT(next);
		space = next->start - entry->end;
	}
	return (space);
}
		
static int
uvm_rb_subtree_space(vm_map_entry_t entry)
{
	vaddr_t space, tmp;

	space = entry->ownspace;
	if (RB_LEFT(entry, rb_entry)) {
		tmp = RB_LEFT(entry, rb_entry)->space;
		if (tmp > space)
			space = tmp;
	}

	if (RB_RIGHT(entry, rb_entry)) {
		tmp = RB_RIGHT(entry, rb_entry)->space;
		if (tmp > space)
			space = tmp;
	}

	return (space);
}

static __inline void
uvm_rb_fixup(vm_map_t map, vm_map_entry_t entry)
{
	/* We need to traverse to the very top */
	do {
		entry->ownspace = uvm_rb_space(map, entry);
		entry->space = uvm_rb_subtree_space(entry);
	} while ((entry = RB_PARENT(entry, rb_entry)) != NULL);
}

a209 3
	vaddr_t space = uvm_rb_space(map, entry);

	entry->ownspace = entry->space = space;
a210 3
	uvm_rb_fixup(map, entry);
	if (entry->prev != &map->header)
		uvm_rb_fixup(map, entry->prev);
a215 3
	vm_map_entry_t parent;

	parent = RB_PARENT(entry, rb_entry);
a216 4
	if (entry->prev != &map->header)
		uvm_rb_fixup(map, entry->prev);
	if (parent)
		uvm_rb_fixup(map, parent);
a226 9
	RB_FOREACH(tmp, uvm_tree, &map->rbhead) {
		if (tmp->ownspace != uvm_rb_space(map, tmp)) {
			printf("%s: %d/%d ownspace %x != %x %s\n",
			    name, n + 1, map->nentries,
			    tmp->ownspace, uvm_rb_space(map, tmp),
			    tmp->next == &map->header ? "(last)" : "");
			goto error;
		}
	}
a228 5
		if (tmp->space != uvm_rb_subtree_space(tmp)) {
			printf("%s: space %d != %d\n",
			    name, tmp->space, uvm_rb_subtree_space(tmp));
			goto error;
		}
d260 1
a260 1
	__asm(".globl treesanity_label\ntreesanity_label:");
d346 1
a346 1
	memcpy(dst, src, ((char *)&src->uvm_map_entry_stop_copy) - ((char *)src));
a542 2
	uvm_rb_fixup(map, entry);

a733 1
		uvm_rb_fixup(map, prev_entry);
a940 35
 * Checks if address pointed to be phint fits into the empty
 * space before the vm_map_entry after.  Takes aligment and
 * offset into consideration.
 */

int
uvm_map_spacefits(vm_map_t map, vaddr_t *phint, vsize_t length,
    vm_map_entry_t after, voff_t uoffset, vsize_t align)
{
	vaddr_t hint = *phint;
	vaddr_t end;

#ifdef PMAP_PREFER
	/*
	 * push hint forward as needed to avoid VAC alias problems.
	 * we only do this if a valid offset is specified.
	 */
	if (uoffset != UVM_UNKNOWN_OFFSET)
		PMAP_PREFER(uoffset, &hint);
#endif
	if (align != 0)
		if ((hint & (align - 1)) != 0)
			hint = roundup(hint, align);
	*phint = hint;

	end = hint + length;
	if (end > map->max_offset || end < hint)
		return (FALSE);
	if (after != NULL && after != &map->header && after->start < end)
		return (FALSE);
	
	return (TRUE);
}

/*
a964 1
	vm_map_entry_t child, prev = NULL;
a1030 84
	/* Try to find the space in the red-black tree */

	/* Check slot before any entry */
	if (uvm_map_spacefits(map, &hint, length, entry->next, uoffset, align))
		goto found;
	
	/* If there is not enough space in the whole tree, we fail */
	tmp = RB_ROOT(&map->rbhead);
	if (tmp == NULL || tmp->space < length)
		goto error;

	/* Find an entry close to hint that has enough space */
	for (; tmp;) {
		if (tmp->end >= hint &&
		    (prev == NULL || tmp->end < prev->end)) {
			if (tmp->ownspace >= length)
				prev = tmp;
			else if ((child = RB_RIGHT(tmp, rb_entry)) != NULL &&
			    child->space >= length)
				prev = tmp;
		}
		if (tmp->end < hint)
			child = RB_RIGHT(tmp, rb_entry);
		else if (tmp->end > hint)
			child = RB_LEFT(tmp, rb_entry);
		else {
			if (tmp->ownspace >= length)
				break;
			child = RB_RIGHT(tmp, rb_entry);
		}
		if (child == NULL || child->space < length)
			break;
		tmp = child;
	}
	
	if (tmp != NULL && hint < tmp->end + tmp->ownspace) {
		/* 
		 * Check if the entry that we found satifies the
		 * space requirement
		 */
		if (hint < tmp->end)
			hint = tmp->end;
		if (uvm_map_spacefits(map, &hint, length, tmp->next, uoffset,
			align)) {
			entry = tmp;
			goto found;
		} else if (tmp->ownspace >= length)
			goto listsearch;
	}
	if (prev == NULL)
		goto error;
	
	hint = prev->end;
	if (uvm_map_spacefits(map, &hint, length, prev->next, uoffset,
		align)) {
		entry = prev;
		goto found;
	} else if (prev->ownspace >= length)
		goto listsearch;
	
	tmp = RB_RIGHT(prev, rb_entry);
	for (;;) {
		KASSERT(tmp && tmp->space >= length);
		child = RB_LEFT(tmp, rb_entry);
		if (child && child->space >= length) {
			tmp = child;
			continue;
		}
		if (tmp->ownspace >= length)
			break;
		tmp = RB_RIGHT(tmp, rb_entry);
	}
	
	hint = tmp->end;
	if (uvm_map_spacefits(map, &hint, length, tmp->next, uoffset, align)) {
		entry = tmp;
		goto found;
	}

	/* 
	 * The tree fails to find an entry because of offset or alignment
	 * restrictions.  Search the list instead.
	 */
 listsearch:
a3001 2
		if (map->header.prev != &map->header)
			uvm_rb_fixup(map, map->header.prev);
d3386 1
a3386 1
	int (*pr)(const char *, ...);
d3428 1
a3428 1
	int (*pr)(const char *, ...);
d3471 1
a3471 1
	int (*pr)(const char *, ...);
@


1.3.4.9
log
@Sync the SMP branch with 3.3
@
text
@d192 2
a193 2
int _uvm_tree_sanity(vm_map_t map, const char *name);
static vsize_t		uvm_rb_subtree_space(vm_map_entry_t);
d217 1
a217 1
static __inline vsize_t
d232 1
a232 1
static vsize_t
a266 1
	vm_map_entry_t tmp;
d269 1
a269 5
	tmp = RB_INSERT(uvm_tree, &(map)->rbhead, entry);
#ifdef DIAGNOSTIC
	if (tmp != NULL)
		panic("uvm_rb_insert: duplicate entry?");
#endif
a287 3
#ifdef DEBUG
#define uvm_tree_sanity(x,y) _uvm_tree_sanity(x,y)
#else
a288 1
#endif
d291 1
a291 1
_uvm_tree_sanity(vm_map_t map, const char *name)
a377 1
		splassert(IPL_NONE);
a380 1
		splassert(IPL_NONE);
a412 1
		splassert(IPL_NONE);
a414 1
		splassert(IPL_NONE);
d593 1
a593 1
 *	the ending address, if it doesn't we split the reference
d970 1
a970 1
	uvm_tree_sanity(map, __func__);
d1306 1
a1306 1
void
d1433 1
a1433 1
				    entry->end);
d1478 1
d2982 1
a2982 1
				panic("uvm_map_clean: weird flags");
d3263 1
a3263 1
			uvm_unmap_remove(&vm->vm_map,
d3436 1
a3436 1
			 *    parent and child process are referring to the
d3599 1
a3599 1
	(*pr)("\t#ent=%d, sz=%u, ref=%d, version=%u, flags=0x%x\n",
@


1.3.4.10
log
@Sync the SMP branch to -current. This includes moving to ELF.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.c,v 1.3.4.9 2003/03/28 00:08:48 niklas Exp $	*/
a1074 19
 * uvm_map_hint: return the beginning of the best area suitable for
 * creating a new mapping with "prot" protection.
 */
vaddr_t
uvm_map_hint(struct proc *p, vm_prot_t prot)
{
#ifdef __i386__
	/*
	 * If executable skip first two pages, otherwise start
	 * after data + heap region.
	 */
	if ((prot & VM_PROT_EXECUTE) &&
	    ((vaddr_t)p->p_vmspace->vm_daddr >= I386_MAX_EXE_ADDR))
		return (round_page(PAGE_SIZE*2));
#endif
	return (round_page((vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ));
}

/*
d3590 1
@


1.3.4.11
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a81 2
#include <dev/rndvar.h>

d86 1
a88 2
#undef RB_AUGMENT
#define RB_AUGMENT(x) uvm_rb_augment(x)
d322 1
a322 1
			printf("%s: corrupt: 0x%lx >= 0x%lx\n",
a1080 2
	vaddr_t addr;

d1087 2
a1088 9
	    ((vaddr_t)p->p_vmspace->vm_daddr >= I386_MAX_EXE_ADDR)) {
		addr = (PAGE_SIZE*2) +
		    (arc4random() & (I386_MAX_EXE_ADDR / 2 - 1));
		return (round_page(addr));
	}
#endif
	addr = (vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ;
#if !defined(__vax__)
	addr += arc4random() & (MIN((256 * 1024 * 1024), MAXDSIZ) - 1);
d1090 1
a1090 1
	return (round_page(addr));
@


1.3.4.12
log
@Merge with the trunk
@
text
@a1511 4
	/* if ((map->flags & VM_MAP_DYING) == 0) { */
		pmap_update(vm_map_pmap(map));
	/* } */

a2029 1
		pmap_update(srcmap->pmap);
a2216 3
			if ((current->protection & MASK(entry)) == PROT_NONE &&
			    VM_MAPENT_ISWIRED(entry))
				current->wired_count--;
a2252 1
	pmap_update(map->pmap);
a3032 4
		 *
		 * Don't PGO_FREE if we don't have write permission
	 	 * and don't flush if this is a copy-on-write object
		 * since we can't know our permissions on it.
d3037 1
a3037 4
		if (uobj != NULL &&
		    ((flags & PGO_FREE) == 0 ||
		     ((entry->max_protection & VM_PROT_WRITE) != 0 &&
		      (entry->etype & UVM_ET_COPYONWRITE) == 0))) {
a3558 2
			        pmap_update(old_map->pmap);

@


1.2
log
@add OpenBSD tags
@
text
@d1 1
a1 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_map.c,v 1.34 1999/01/24 23:53:15 chuck Exp $	*/
a2 4
/*
 * XXXCDC: "ROUGH DRAFT" QUALITY UVM PRE-RELEASE FILE!
 *         >>>USE AT YOUR OWN RISK, WORK IS NOT FINISHED<<<
 */
a79 3
#include <sys/user.h>
#include <machine/pcb.h>

d111 11
a188 8
#undef UVM_MAP_INLINES

#ifdef UVM_MAP_INLINES
#define UVM_INLINE __inline
#else
#define UVM_INLINE
#endif

d195 1
a195 1
static UVM_INLINE vm_map_entry_t
d233 1
a233 1
static UVM_INLINE void
d258 1
a258 1
static UVM_INLINE void
d273 1
a273 1
static UVM_INLINE void
d512 1
a512 1
#if defined(PMAP_GROWKERNEL)	/* hack */
a513 3
		/* locked by kernel_map lock */
		static vaddr_t maxkaddr = 0;
		
d515 2
a516 1
		 * hack: grow kernel PTPs in advance.
d518 2
a519 4
		if (map == kernel_map && maxkaddr < (*startp + size)) {
			pmap_growkernel(*startp + size);
			maxkaddr = *startp + size;
		}
d1460 1
a1460 1
		if (newentry->end > newend)
a1708 1
 * => XXXCDC: does not work properly with share maps.  rethink.
d1711 2
a1712 2
#define MASK(entry)     ( UVM_ET_ISCOPYONWRITE(entry) ? \
	~VM_PROT_WRITE : VM_PROT_ALL)
d1744 1
a1744 1
			return(KERN_INVALID_ARGUMENT);
d1747 1
a1747 1
			return(KERN_PROTECTION_FAILURE);
a1796 1
 * => XXXCDC: currently only works in main map.  what about share map?
d1818 1
a1818 1
		return(KERN_INVALID_ARGUMENT);
d2010 1
a2010 1
			return(KERN_INVALID_ARGUMENT);
a2091 1
 * => XXX: does this handle share maps properly?
d2122 1
a2122 1
			return(KERN_INVALID_ARGUMENT);
d2127 1
a2127 1
			return(KERN_INVALID_ADDRESS);
d2310 1
a2310 2
	pmap_deactivate(p);
					/* unbind old vmspace */
d2312 1
a2312 2
	pmap_activate(p);
					/* switch to new vmspace */
d2761 4
@


1.1
log
@Import of uvm from NetBSD. Some local changes, some code disabled
@
text
@d1 1
@

