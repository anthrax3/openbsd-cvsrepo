head	1.130;
access;
symbols
	OPENBSD_6_2:1.130.0.4
	OPENBSD_6_2_BASE:1.130
	OPENBSD_6_1:1.128.0.8
	OPENBSD_6_1_BASE:1.128
	OPENBSD_6_0:1.128.0.4
	OPENBSD_6_0_BASE:1.128
	OPENBSD_5_9:1.128.0.2
	OPENBSD_5_9_BASE:1.128
	OPENBSD_5_8:1.126.0.6
	OPENBSD_5_8_BASE:1.126
	OPENBSD_5_7:1.126.0.2
	OPENBSD_5_7_BASE:1.126
	OPENBSD_5_6:1.114.0.4
	OPENBSD_5_6_BASE:1.114
	OPENBSD_5_5:1.111.0.6
	OPENBSD_5_5_BASE:1.111
	OPENBSD_5_4:1.111.0.2
	OPENBSD_5_4_BASE:1.111
	OPENBSD_5_3:1.108.0.2
	OPENBSD_5_3_BASE:1.108
	OPENBSD_5_2:1.107.0.2
	OPENBSD_5_2_BASE:1.107
	OPENBSD_5_1_BASE:1.106
	OPENBSD_5_1:1.106.0.4
	OPENBSD_5_0:1.106.0.2
	OPENBSD_5_0_BASE:1.106
	OPENBSD_4_9:1.86.0.2
	OPENBSD_4_9_BASE:1.86
	OPENBSD_4_8:1.85.0.2
	OPENBSD_4_8_BASE:1.85
	OPENBSD_4_7:1.76.0.2
	OPENBSD_4_7_BASE:1.76
	OPENBSD_4_6:1.73.0.4
	OPENBSD_4_6_BASE:1.73
	OPENBSD_4_5:1.70.0.2
	OPENBSD_4_5_BASE:1.70
	OPENBSD_4_4:1.67.0.2
	OPENBSD_4_4_BASE:1.67
	OPENBSD_4_3:1.66.0.2
	OPENBSD_4_3_BASE:1.66
	OPENBSD_4_2:1.64.0.2
	OPENBSD_4_2_BASE:1.64
	OPENBSD_4_1:1.54.0.2
	OPENBSD_4_1_BASE:1.54
	OPENBSD_4_0:1.52.0.2
	OPENBSD_4_0_BASE:1.52
	OPENBSD_3_9:1.48.0.2
	OPENBSD_3_9_BASE:1.48
	OPENBSD_3_8:1.46.0.2
	OPENBSD_3_8_BASE:1.46
	OPENBSD_3_7:1.45.0.2
	OPENBSD_3_7_BASE:1.45
	OPENBSD_3_6:1.44.0.2
	OPENBSD_3_6_BASE:1.44
	SMP_SYNC_A:1.41
	SMP_SYNC_B:1.41
	OPENBSD_3_5:1.35.0.2
	OPENBSD_3_5_BASE:1.35
	OPENBSD_3_4:1.34.0.4
	OPENBSD_3_4_BASE:1.34
	UBC_SYNC_A:1.34
	OPENBSD_3_3:1.34.0.2
	OPENBSD_3_3_BASE:1.34
	OPENBSD_3_2:1.33.0.2
	OPENBSD_3_2_BASE:1.33
	OPENBSD_3_1:1.28.0.2
	OPENBSD_3_1_BASE:1.28
	UBC_SYNC_B:1.33
	UBC:1.26.0.2
	UBC_BASE:1.26
	OPENBSD_3_0:1.15.0.2
	OPENBSD_3_0_BASE:1.15
	OPENBSD_2_9_BASE:1.9
	OPENBSD_2_9:1.9.0.2
	OPENBSD_2_8:1.6.0.2
	OPENBSD_2_8_BASE:1.6
	OPENBSD_2_7:1.5.0.2
	OPENBSD_2_7_BASE:1.5
	SMP:1.4.0.4
	SMP_BASE:1.4
	kame_19991208:1.4
	OPENBSD_2_6:1.4.0.2
	OPENBSD_2_6_BASE:1.4
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.130
date	2017.05.11.06.55.47;	author dlg;	state Exp;
branches;
next	1.129;
commitid	8BWj6CyAp3zTq15k;

1.129
date	2017.05.11.00.42.05;	author dlg;	state Exp;
branches;
next	1.128;
commitid	VXiRNm94dKo73F5w;

1.128
date	2015.09.26.17.55.00;	author kettenis;	state Exp;
branches;
next	1.127;
commitid	Kz7km4wLjVuZXy3p;

1.127
date	2015.09.17.18.04.49;	author kettenis;	state Exp;
branches;
next	1.126;
commitid	IxBfXo7EF3M1f4pD;

1.126
date	2015.02.07.08.21.24;	author miod;	state Exp;
branches
	1.126.2.1
	1.126.6.1;
next	1.125;
commitid	3WN6O42yLCDNeEYy;

1.125
date	2015.02.06.10.58.35;	author deraadt;	state Exp;
branches;
next	1.124;
commitid	QOLWofatH4nZnDlg;

1.124
date	2015.01.23.17.09.23;	author kettenis;	state Exp;
branches;
next	1.123;
commitid	aV3aFfozhOMj5ocH;

1.123
date	2014.12.17.06.58.11;	author guenther;	state Exp;
branches;
next	1.122;
commitid	DImukoCWyTxwdbuh;

1.122
date	2014.12.15.02.24.23;	author guenther;	state Exp;
branches;
next	1.121;
commitid	ZxaujiOM0aYQRjFY;

1.121
date	2014.11.27.21.40.52;	author kettenis;	state Exp;
branches;
next	1.120;
commitid	vnKr06rFhTIeNCdA;

1.120
date	2014.11.21.06.40.40;	author deraadt;	state Exp;
branches;
next	1.119;
commitid	IQ6xOpCCx2Y0mI3R;

1.119
date	2014.11.17.04.31.08;	author deraadt;	state Exp;
branches;
next	1.118;
commitid	Doc3gqByXIfnf0qT;

1.118
date	2014.11.17.04.26.53;	author deraadt;	state Exp;
branches;
next	1.117;
commitid	ciHxlJEk8K5rC3XZ;

1.117
date	2014.11.16.12.31.00;	author deraadt;	state Exp;
branches;
next	1.116;
commitid	yv0ECmCdICvq576h;

1.116
date	2014.11.13.00.47.44;	author tedu;	state Exp;
branches;
next	1.115;
commitid	t8zRELnW12GnYxX5;

1.115
date	2014.09.14.14.17.27;	author jsg;	state Exp;
branches;
next	1.114;
commitid	uzzBR7hz9ncd4O6G;

1.114
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.113;
commitid	7NtJNW9udCOFtDNM;

1.113
date	2014.06.21.21.09.25;	author guenther;	state Exp;
branches;
next	1.112;
commitid	SbcLTn6nWrhwyb4V;

1.112
date	2014.04.13.23.14.15;	author tedu;	state Exp;
branches;
next	1.111;

1.111
date	2013.05.30.18.02.04;	author tedu;	state Exp;
branches;
next	1.110;

1.110
date	2013.05.30.16.39.26;	author tedu;	state Exp;
branches;
next	1.109;

1.109
date	2013.05.30.15.17.59;	author tedu;	state Exp;
branches;
next	1.108;

1.108
date	2012.11.10.11.18.45;	author kettenis;	state Exp;
branches;
next	1.107;

1.107
date	2012.03.09.13.01.29;	author ariane;	state Exp;
branches;
next	1.106;

1.106
date	2011.07.03.18.34.14;	author oga;	state Exp;
branches;
next	1.105;

1.105
date	2011.06.23.21.54.56;	author oga;	state Exp;
branches;
next	1.104;

1.104
date	2011.06.23.21.42.05;	author ariane;	state Exp;
branches;
next	1.103;

1.103
date	2011.06.06.17.10.23;	author ariane;	state Exp;
branches;
next	1.102;

1.102
date	2011.05.24.15.27.36;	author ariane;	state Exp;
branches;
next	1.101;

1.101
date	2011.05.10.21.48.17;	author oga;	state Exp;
branches;
next	1.100;

1.100
date	2011.04.23.17.48.48;	author kettenis;	state Exp;
branches;
next	1.99;

1.99
date	2011.04.19.20.00.11;	author matthew;	state Exp;
branches;
next	1.98;

1.98
date	2011.04.19.15.59.11;	author art;	state Exp;
branches;
next	1.97;

1.97
date	2011.04.18.19.23.46;	author art;	state Exp;
branches;
next	1.96;

1.96
date	2011.04.15.23.04.19;	author deraadt;	state Exp;
branches;
next	1.95;

1.95
date	2011.04.15.21.47.24;	author oga;	state Exp;
branches;
next	1.94;

1.94
date	2011.04.07.15.30.16;	author miod;	state Exp;
branches;
next	1.93;

1.93
date	2011.04.06.15.52.13;	author art;	state Exp;
branches;
next	1.92;

1.92
date	2011.04.05.01.28.05;	author art;	state Exp;
branches;
next	1.91;

1.91
date	2011.04.04.21.16.31;	author art;	state Exp;
branches;
next	1.90;

1.90
date	2011.04.04.12.25.23;	author art;	state Exp;
branches;
next	1.89;

1.89
date	2011.04.04.12.22.59;	author art;	state Exp;
branches;
next	1.88;

1.88
date	2011.04.04.11.56.12;	author art;	state Exp;
branches;
next	1.87;

1.87
date	2011.04.04.11.24.45;	author art;	state Exp;
branches;
next	1.86;

1.86
date	2010.08.26.16.08.24;	author thib;	state Exp;
branches;
next	1.85;

1.85
date	2010.07.22.17.31.39;	author thib;	state Exp;
branches;
next	1.84;

1.84
date	2010.07.15.00.14.17;	author tedu;	state Exp;
branches;
next	1.83;

1.83
date	2010.07.02.23.12.38;	author thib;	state Exp;
branches;
next	1.82;

1.82
date	2010.07.02.01.25.06;	author art;	state Exp;
branches;
next	1.81;

1.81
date	2010.07.02.01.13.59;	author thib;	state Exp;
branches;
next	1.80;

1.80
date	2010.06.29.20.39.27;	author thib;	state Exp;
branches;
next	1.79;

1.79
date	2010.06.28.04.20.29;	author miod;	state Exp;
branches;
next	1.78;

1.78
date	2010.06.27.17.45.20;	author thib;	state Exp;
branches;
next	1.77;

1.77
date	2010.06.27.03.03.49;	author thib;	state Exp;
branches;
next	1.76;

1.76
date	2010.02.12.01.35.14;	author tedu;	state Exp;
branches;
next	1.75;

1.75
date	2009.07.25.12.55.40;	author miod;	state Exp;
branches;
next	1.74;

1.74
date	2009.07.22.21.05.37;	author oga;	state Exp;
branches;
next	1.73;

1.73
date	2009.06.17.00.13.59;	author oga;	state Exp;
branches;
next	1.72;

1.72
date	2009.06.16.17.14.15;	author oga;	state Exp;
branches;
next	1.71;

1.71
date	2009.05.05.05.27.53;	author oga;	state Exp;
branches;
next	1.70;

1.70
date	2009.02.22.19.59.01;	author miod;	state Exp;
branches;
next	1.69;

1.69
date	2009.02.11.11.09.36;	author mikeb;	state Exp;
branches;
next	1.68;

1.68
date	2008.10.23.23.54.02;	author tedu;	state Exp;
branches;
next	1.67;

1.67
date	2008.06.14.03.48.32;	author art;	state Exp;
branches;
next	1.66;

1.66
date	2007.12.15.03.42.57;	author deraadt;	state Exp;
branches;
next	1.65;

1.65
date	2007.12.11.15.05.45;	author tedu;	state Exp;
branches;
next	1.64;

1.64
date	2007.08.03.22.49.07;	author art;	state Exp;
branches;
next	1.63;

1.63
date	2007.04.29.15.46.42;	author art;	state Exp;
branches;
next	1.62;

1.62
date	2007.04.27.07.45.30;	author art;	state Exp;
branches;
next	1.61;

1.61
date	2007.04.15.12.54.08;	author art;	state Exp;
branches;
next	1.60;

1.60
date	2007.04.15.11.29.33;	author art;	state Exp;
branches;
next	1.59;

1.59
date	2007.04.15.11.23.16;	author art;	state Exp;
branches;
next	1.58;

1.58
date	2007.04.13.18.57.49;	author art;	state Exp;
branches;
next	1.57;

1.57
date	2007.04.11.12.10.42;	author art;	state Exp;
branches;
next	1.56;

1.56
date	2007.04.04.17.44.45;	author art;	state Exp;
branches;
next	1.55;

1.55
date	2007.03.25.11.31.07;	author art;	state Exp;
branches;
next	1.54;

1.54
date	2006.11.29.12.39.50;	author miod;	state Exp;
branches;
next	1.53;

1.53
date	2006.11.29.12.17.33;	author miod;	state Exp;
branches;
next	1.52;

1.52
date	2006.07.31.11.51.29;	author mickey;	state Exp;
branches;
next	1.51;

1.51
date	2006.07.26.23.15.55;	author mickey;	state Exp;
branches;
next	1.50;

1.50
date	2006.04.25.08.31.00;	author mickey;	state Exp;
branches;
next	1.49;

1.49
date	2006.03.06.19.11.03;	author mickey;	state Exp;
branches;
next	1.48;

1.48
date	2005.10.06.03.59.50;	author brad;	state Exp;
branches;
next	1.47;

1.47
date	2005.09.09.15.48.43;	author pedro;	state Exp;
branches;
next	1.46;

1.46
date	2005.05.24.21.11.47;	author tedu;	state Exp;
branches;
next	1.45;

1.45
date	2004.12.30.08.28.39;	author niklas;	state Exp;
branches;
next	1.44;

1.44
date	2004.08.24.07.16.12;	author tedu;	state Exp;
branches;
next	1.43;

1.43
date	2004.08.24.03.58.14;	author tedu;	state Exp;
branches;
next	1.42;

1.42
date	2004.07.13.14.51.29;	author tedu;	state Exp;
branches;
next	1.41;

1.41
date	2004.06.09.20.17.23;	author tedu;	state Exp;
branches;
next	1.40;

1.40
date	2004.05.31.22.53.49;	author tedu;	state Exp;
branches;
next	1.39;

1.39
date	2004.05.27.04.55.28;	author tedu;	state Exp;
branches;
next	1.38;

1.38
date	2004.04.28.02.20.58;	author markus;	state Exp;
branches;
next	1.37;

1.37
date	2004.04.20.09.39.36;	author markus;	state Exp;
branches;
next	1.36;

1.36
date	2004.04.19.22.52.33;	author tedu;	state Exp;
branches;
next	1.35;

1.35
date	2004.02.23.06.19.32;	author drahn;	state Exp;
branches;
next	1.34;

1.34
date	2002.10.29.18.30.21;	author art;	state Exp;
branches;
next	1.33;

1.33
date	2002.09.12.12.50.47;	author art;	state Exp;
branches;
next	1.32;

1.32
date	2002.05.30.06.26.10;	author deraadt;	state Exp;
branches;
next	1.31;

1.31
date	2002.04.25.04.36.43;	author mickey;	state Exp;
branches;
next	1.30;

1.30
date	2002.04.22.16.25.46;	author deraadt;	state Exp;
branches;
next	1.29;

1.29
date	2002.04.22.12.50.34;	author mickey;	state Exp;
branches;
next	1.28;

1.28
date	2002.03.06.22.05.31;	author art;	state Exp;
branches;
next	1.27;

1.27
date	2001.12.19.08.58.07;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2001.12.04.23.22.42;	author art;	state Exp;
branches
	1.26.2.1;
next	1.25;

1.25
date	2001.11.28.19.28.14;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2001.11.28.14.29.13;	author art;	state Exp;
branches;
next	1.23;

1.23
date	2001.11.28.13.47.39;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2001.11.11.01.16.56;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2001.11.09.03.32.23;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2001.11.07.02.55.50;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2001.11.07.01.18.01;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2001.11.06.13.36.52;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2001.11.06.01.35.04;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2001.11.05.22.14.54;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2001.09.19.20.50.59;	author mickey;	state Exp;
branches;
next	1.14;

1.14
date	2001.08.11.10.57.22;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2001.07.26.19.37.13;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2001.07.25.13.25.33;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2001.07.18.10.47.05;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2001.06.23.19.24.33;	author smart;	state Exp;
branches;
next	1.9;

1.9
date	2001.03.22.03.05.55;	author smart;	state Exp;
branches;
next	1.8;

1.8
date	2001.03.08.15.21.36;	author smart;	state Exp;
branches;
next	1.7;

1.7
date	2001.01.29.02.07.45;	author niklas;	state Exp;
branches;
next	1.6;

1.6
date	2000.09.07.20.15.28;	author art;	state Exp;
branches;
next	1.5;

1.5
date	2000.03.16.22.11.04;	author art;	state Exp;
branches;
next	1.4;

1.4
date	99.09.03.18.02.22;	author art;	state Exp;
branches
	1.4.4.1;
next	1.3;

1.3
date	99.08.23.08.13.23;	author art;	state Exp;
branches;
next	1.2;

1.2
date	99.02.26.05.32.07;	author art;	state Exp;
branches;
next	1.1;

1.1
date	99.02.26.01.30.13;	author art;	state Exp;
branches;
next	;

1.4.4.1
date	2000.03.24.09.09.49;	author niklas;	state Exp;
branches;
next	1.4.4.2;

1.4.4.2
date	2001.05.14.22.47.46;	author niklas;	state Exp;
branches;
next	1.4.4.3;

1.4.4.3
date	2001.07.04.11.01.05;	author niklas;	state Exp;
branches;
next	1.4.4.4;

1.4.4.4
date	2001.10.31.03.32.14;	author nate;	state Exp;
branches;
next	1.4.4.5;

1.4.4.5
date	2001.11.13.23.02.31;	author niklas;	state Exp;
branches;
next	1.4.4.6;

1.4.4.6
date	2001.12.05.01.23.58;	author niklas;	state Exp;
branches;
next	1.4.4.7;

1.4.4.7
date	2002.03.06.02.17.14;	author niklas;	state Exp;
branches;
next	1.4.4.8;

1.4.4.8
date	2002.03.28.14.54.26;	author niklas;	state Exp;
branches;
next	1.4.4.9;

1.4.4.9
date	2003.03.28.00.08.48;	author niklas;	state Exp;
branches;
next	1.4.4.10;

1.4.4.10
date	2004.06.05.23.13.12;	author niklas;	state Exp;
branches;
next	1.4.4.11;

1.4.4.11
date	2004.06.10.11.40.36;	author niklas;	state Exp;
branches;
next	;

1.26.2.1
date	2002.02.02.03.28.26;	author art;	state Exp;
branches;
next	1.26.2.2;

1.26.2.2
date	2002.06.11.03.33.03;	author art;	state Exp;
branches;
next	1.26.2.3;

1.26.2.3
date	2002.10.29.00.36.50;	author art;	state Exp;
branches;
next	1.26.2.4;

1.26.2.4
date	2002.11.04.18.02.33;	author art;	state Exp;
branches;
next	1.26.2.5;

1.26.2.5
date	2002.11.04.20.11.00;	author art;	state Exp;
branches;
next	;

1.126.2.1
date	2015.09.28.18.21.46;	author tedu;	state Exp;
branches;
next	;
commitid	XYw2twQzYNpHJWBD;

1.126.6.1
date	2015.09.28.18.22.37;	author tedu;	state Exp;
branches;
next	;
commitid	IznnOHjj0l7PNe9V;


desc
@@


1.130
log
@unbreak PMAP_DIRECT archs.

found by jmc@@
@
text
@/*	$OpenBSD: uvm_km.c,v 1.129 2017/05/11 00:42:05 dlg Exp $	*/
/*	$NetBSD: uvm_km.c,v 1.42 2001/01/14 02:10:01 thorpej Exp $	*/

/* 
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
 *
 * All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * The Mach Operating System project at Carnegie-Mellon University.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)vm_kern.c   8.3 (Berkeley) 1/12/94
 * from: Id: uvm_km.c,v 1.1.2.14 1998/02/06 05:19:27 chs Exp
 *
 *
 * Copyright (c) 1987, 1990 Carnegie-Mellon University.
 * All rights reserved.
 * 
 * Permission to use, copy, modify and distribute this software and
 * its documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
 * FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
 * 
 * Carnegie Mellon requests users of this software to return to
 *
 *  Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
 *  School of Computer Science
 *  Carnegie Mellon University
 *  Pittsburgh PA 15213-3890
 *
 * any improvements or extensions that they make and grant Carnegie the
 * rights to redistribute these changes.
 */

/*
 * uvm_km.c: handle kernel memory allocation and management
 */

/*
 * overview of kernel memory management:
 *
 * the kernel virtual address space is mapped by "kernel_map."   kernel_map
 * starts at a machine-dependent address and is VM_KERNEL_SPACE_SIZE bytes
 * large.
 *
 * the kernel_map has several "submaps."   submaps can only appear in 
 * the kernel_map (user processes can't use them).   submaps "take over"
 * the management of a sub-range of the kernel's address space.  submaps
 * are typically allocated at boot time and are never released.   kernel
 * virtual address space that is mapped by a submap is locked by the 
 * submap's lock -- not the kernel_map's lock.
 *
 * thus, the useful feature of submaps is that they allow us to break
 * up the locking and protection of the kernel address space into smaller
 * chunks.
 *
 * The VM system has several standard kernel submaps:
 *   kmem_map: Contains only wired kernel memory for malloc(9).
 *	       Note: All access to this map must be protected by splvm as
 *	       calls to malloc(9) are allowed in interrupt handlers.
 *   exec_map: Memory to hold arguments to system calls are allocated from
 *	       this map.
 *	       XXX: This is primeraly used to artificially limit the number
 *	       of concurrent processes doing an exec.
 *   phys_map: Buffers for vmapbuf (physio) are allocated from this map.
 *
 * the kernel allocates its private memory out of special uvm_objects whose
 * reference count is set to UVM_OBJ_KERN (thus indicating that the objects
 * are "special" and never die).   all kernel objects should be thought of
 * as large, fixed-sized, sparsely populated uvm_objects.   each kernel 
 * object is equal to the size of kernel virtual address space (i.e.
 * VM_KERNEL_SPACE_SIZE).
 *
 * most kernel private memory lives in kernel_object.   the only exception
 * to this is for memory that belongs to submaps that must be protected
 * by splvm(). each of these submaps manages their own pages.
 *
 * note that just because a kernel object spans the entire kernel virtual
 * address space doesn't mean that it has to be mapped into the entire space.
 * large chunks of a kernel object's space go unused either because 
 * that area of kernel VM is unmapped, or there is some other type of 
 * object mapped into that range (e.g. a vnode).    for submap's kernel
 * objects, the only part of the object that can ever be populated is the
 * offsets that are managed by the submap.
 *
 * note that the "offset" in a kernel object is always the kernel virtual
 * address minus the vm_map_min(kernel_map).
 * example:
 *   suppose kernel_map starts at 0xf8000000 and the kernel does a
 *   uvm_km_alloc(kernel_map, PAGE_SIZE) [allocate 1 wired down page in the
 *   kernel map].    if uvm_km_alloc returns virtual address 0xf8235000,
 *   then that means that the page at offset 0x235000 in kernel_object is
 *   mapped at 0xf8235000.   
 *
 * kernel objects have one other special property: when the kernel virtual
 * memory mapping them is unmapped, the backing memory in the object is
 * freed right away.   this is done with the uvm_km_pgremove() function.
 * this has to be done because there is no backing store for kernel pages
 * and no need to save them after they are no longer referenced.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/kthread.h>
#include <uvm/uvm.h>

/*
 * global data structures
 */

struct vm_map *kernel_map = NULL;

/* Unconstraint range. */
struct uvm_constraint_range	no_constraint = { 0x0, (paddr_t)-1 };

/*
 * local data structues
 */
static struct vm_map		kernel_map_store;

/*
 * uvm_km_init: init kernel maps and objects to reflect reality (i.e.
 * KVM already allocated for text, data, bss, and static data structures).
 *
 * => KVM is defined by [base.. base + VM_KERNEL_SPACE_SIZE].
 *    we assume that [base -> start] has already been allocated and that
 *    "end" is the end of the kernel image span.
 */
void
uvm_km_init(vaddr_t base, vaddr_t start, vaddr_t end)
{
	/* kernel_object: for pageable anonymous kernel memory */
	uao_init();
	uvm.kernel_object = uao_create(VM_KERNEL_SPACE_SIZE, UAO_FLAG_KERNOBJ);

	/*
	 * init the map and reserve already allocated kernel space 
	 * before installing.
	 */

	uvm_map_setup(&kernel_map_store, base, end,
#ifdef KVA_GUARDPAGES
	    VM_MAP_PAGEABLE | VM_MAP_GUARDPAGES
#else
	    VM_MAP_PAGEABLE
#endif
	    );
	kernel_map_store.pmap = pmap_kernel();
	if (base != start && uvm_map(&kernel_map_store, &base, start - base,
	    NULL, UVM_UNKNOWN_OFFSET, 0,
	    UVM_MAPFLAG(PROT_READ | PROT_WRITE, PROT_READ | PROT_WRITE,
	    MAP_INHERIT_NONE, MADV_RANDOM, UVM_FLAG_FIXED)) != 0)
		panic("uvm_km_init: could not reserve space for kernel");
	
	kernel_map = &kernel_map_store;
}

/*
 * uvm_km_suballoc: allocate a submap in the kernel map.   once a submap
 * is allocated all references to that area of VM must go through it.  this
 * allows the locking of VAs in kernel_map to be broken up into regions.
 *
 * => if `fixed' is true, *min specifies where the region described
 *      by the submap must start
 * => if submap is non NULL we use that as the submap, otherwise we
 *	alloc a new map
 */
struct vm_map *
uvm_km_suballoc(struct vm_map *map, vaddr_t *min, vaddr_t *max, vsize_t size,
    int flags, boolean_t fixed, struct vm_map *submap)
{
	int mapflags = UVM_FLAG_NOMERGE | (fixed ? UVM_FLAG_FIXED : 0);

	size = round_page(size);	/* round up to pagesize */

	/* first allocate a blank spot in the parent map */
	if (uvm_map(map, min, size, NULL, UVM_UNKNOWN_OFFSET, 0,
	    UVM_MAPFLAG(PROT_READ | PROT_WRITE, PROT_READ | PROT_WRITE,
	    MAP_INHERIT_NONE, MADV_RANDOM, mapflags)) != 0) {
	       panic("uvm_km_suballoc: unable to allocate space in parent map");
	}

	/* set VM bounds (min is filled in by uvm_map) */
	*max = *min + size;

	/* add references to pmap and create or init the submap */
	pmap_reference(vm_map_pmap(map));
	if (submap == NULL) {
		submap = uvm_map_create(vm_map_pmap(map), *min, *max, flags);
		if (submap == NULL)
			panic("uvm_km_suballoc: unable to create submap");
	} else {
		uvm_map_setup(submap, *min, *max, flags);
		submap->pmap = vm_map_pmap(map);
	}

	/* now let uvm_map_submap plug in it...  */
	if (uvm_map_submap(map, *min, *max, submap) != 0)
		panic("uvm_km_suballoc: submap allocation failed");

	return(submap);
}

/*
 * uvm_km_pgremove: remove pages from a kernel uvm_object.
 *
 * => when you unmap a part of anonymous kernel memory you want to toss
 *    the pages right away.    (this gets called from uvm_unmap_...).
 */
void
uvm_km_pgremove(struct uvm_object *uobj, vaddr_t start, vaddr_t end)
{
	struct vm_page *pp;
	voff_t curoff;
	int slot;

	KASSERT(uobj->pgops == &aobj_pager);

	for (curoff = start ; curoff < end ; curoff += PAGE_SIZE) {
		pp = uvm_pagelookup(uobj, curoff);
		if (pp && pp->pg_flags & PG_BUSY) {
			atomic_setbits_int(&pp->pg_flags, PG_WANTED);
			UVM_WAIT(pp, 0, "km_pgrm", 0);
			curoff -= PAGE_SIZE; /* loop back to us */
			continue;
		}

		/* free the swap slot, then the page */
		slot = uao_dropswap(uobj, curoff >> PAGE_SHIFT);

		if (pp != NULL) {
			uvm_lock_pageq();
			uvm_pagefree(pp);
			uvm_unlock_pageq();
		} else if (slot != 0) {
			uvmexp.swpgonly--;
		}
	}
}


/*
 * uvm_km_pgremove_intrsafe: like uvm_km_pgremove(), but for "intrsafe"
 *    objects
 *
 * => when you unmap a part of anonymous kernel memory you want to toss
 *    the pages right away.    (this gets called from uvm_unmap_...).
 * => none of the pages will ever be busy, and none of them will ever
 *    be on the active or inactive queues (because these objects are
 *    never allowed to "page").
 */
void
uvm_km_pgremove_intrsafe(vaddr_t start, vaddr_t end)
{
	struct vm_page *pg;
	vaddr_t va;
	paddr_t pa;

	for (va = start; va < end; va += PAGE_SIZE) {
		if (!pmap_extract(pmap_kernel(), va, &pa))
			continue;
		pg = PHYS_TO_VM_PAGE(pa);
		if (pg == NULL)
			panic("uvm_km_pgremove_intrsafe: no page");
		uvm_pagefree(pg);
	}
}

/*
 * uvm_km_kmemalloc: lower level kernel memory allocator for malloc()
 *
 * => we map wired memory into the specified map using the obj passed in
 * => NOTE: we can return NULL even if we can wait if there is not enough
 *	free VM space in the map... caller should be prepared to handle
 *	this case.
 * => we return KVA of memory allocated
 * => flags: NOWAIT, VALLOC - just allocate VA, TRYLOCK - fail if we can't
 *	lock the map
 * => low, high, alignment, boundary, nsegs are the corresponding parameters
 *	to uvm_pglistalloc
 * => flags: ZERO - correspond to uvm_pglistalloc flags
 */
vaddr_t
uvm_km_kmemalloc_pla(struct vm_map *map, struct uvm_object *obj, vsize_t size,
    vsize_t valign, int flags, paddr_t low, paddr_t high, paddr_t alignment,
    paddr_t boundary, int nsegs)
{
	vaddr_t kva, loopva;
	voff_t offset;
	struct vm_page *pg;
	struct pglist pgl;
	int pla_flags;

	KASSERT(vm_map_pmap(map) == pmap_kernel());
	/* UVM_KMF_VALLOC => !UVM_KMF_ZERO */
	KASSERT(!(flags & UVM_KMF_VALLOC) ||
	    !(flags & UVM_KMF_ZERO));

	/* setup for call */
	size = round_page(size);
	kva = vm_map_min(map);	/* hint */
	if (nsegs == 0)
		nsegs = atop(size);

	/* allocate some virtual space */
	if (__predict_false(uvm_map(map, &kva, size, obj, UVM_UNKNOWN_OFFSET,
	    valign, UVM_MAPFLAG(PROT_READ | PROT_WRITE, PROT_READ | PROT_WRITE,
	    MAP_INHERIT_NONE, MADV_RANDOM, (flags & UVM_KMF_TRYLOCK))) != 0)) {
		return(0);
	}

	/* if all we wanted was VA, return now */
	if (flags & UVM_KMF_VALLOC) {
		return(kva);
	}

	/* recover object offset from virtual address */
	if (obj != NULL)
		offset = kva - vm_map_min(kernel_map);
	else
		offset = 0;

	/*
	 * now allocate and map in the memory... note that we are the only ones
	 * whom should ever get a handle on this area of VM.
	 */
	TAILQ_INIT(&pgl);
	pla_flags = 0;
	KASSERT(uvmexp.swpgonly <= uvmexp.swpages);
	if ((flags & UVM_KMF_NOWAIT) ||
	    ((flags & UVM_KMF_CANFAIL) &&
	    uvmexp.swpages - uvmexp.swpgonly <= atop(size)))
		pla_flags |= UVM_PLA_NOWAIT;
	else
		pla_flags |= UVM_PLA_WAITOK;
	if (flags & UVM_KMF_ZERO)
		pla_flags |= UVM_PLA_ZERO;
	if (uvm_pglistalloc(size, low, high, alignment, boundary, &pgl, nsegs,
	    pla_flags) != 0) {
		/* Failed. */
		uvm_unmap(map, kva, kva + size);
		return (0);
	}

	loopva = kva;
	while (loopva != kva + size) {
		pg = TAILQ_FIRST(&pgl);
		TAILQ_REMOVE(&pgl, pg, pageq);
		uvm_pagealloc_pg(pg, obj, offset, NULL);
		atomic_clearbits_int(&pg->pg_flags, PG_BUSY);
		UVM_PAGE_OWN(pg, NULL);

		/*
		 * map it in: note that we call pmap_enter with the map and
		 * object unlocked in case we are kmem_map.
		 */
		if (obj == NULL) {
			pmap_kenter_pa(loopva, VM_PAGE_TO_PHYS(pg),
			    PROT_READ | PROT_WRITE);
		} else {
			pmap_enter(map->pmap, loopva, VM_PAGE_TO_PHYS(pg),
			    PROT_READ | PROT_WRITE,
			    PROT_READ | PROT_WRITE | PMAP_WIRED);
		}
		loopva += PAGE_SIZE;
		offset += PAGE_SIZE;
	}
	KASSERT(TAILQ_EMPTY(&pgl));
	pmap_update(pmap_kernel());

	return(kva);
}

/*
 * uvm_km_free: free an area of kernel memory
 */
void
uvm_km_free(struct vm_map *map, vaddr_t addr, vsize_t size)
{
	uvm_unmap(map, trunc_page(addr), round_page(addr+size));
}

/*
 * uvm_km_free_wakeup: free an area of kernel memory and wake up
 * anyone waiting for vm space.
 *
 * => XXX: "wanted" bit + unlock&wait on other end?
 */
void
uvm_km_free_wakeup(struct vm_map *map, vaddr_t addr, vsize_t size)
{
	struct uvm_map_deadq dead_entries;

	vm_map_lock(map);
	TAILQ_INIT(&dead_entries);
	uvm_unmap_remove(map, trunc_page(addr), round_page(addr+size), 
	     &dead_entries, FALSE, TRUE);
	wakeup(map);
	vm_map_unlock(map);

	uvm_unmap_detach(&dead_entries, 0);
}

/*
 * uvm_km_alloc1: allocate wired down memory in the kernel map.
 *
 * => we can sleep if needed
 */
vaddr_t
uvm_km_alloc1(struct vm_map *map, vsize_t size, vsize_t align, boolean_t zeroit)
{
	vaddr_t kva, loopva;
	voff_t offset;
	struct vm_page *pg;

	KASSERT(vm_map_pmap(map) == pmap_kernel());

	size = round_page(size);
	kva = vm_map_min(map);		/* hint */

	/* allocate some virtual space */
	if (__predict_false(uvm_map(map, &kva, size, uvm.kernel_object,
	    UVM_UNKNOWN_OFFSET, align,
	    UVM_MAPFLAG(PROT_READ | PROT_WRITE,
	    PROT_READ | PROT_WRITE | PROT_EXEC,
	    MAP_INHERIT_NONE, MADV_RANDOM, 0)) != 0)) {
		return(0);
	}

	/* recover object offset from virtual address */
	offset = kva - vm_map_min(kernel_map);

	/* now allocate the memory.  we must be careful about released pages. */
	loopva = kva;
	while (size) {
		/* allocate ram */
		pg = uvm_pagealloc(uvm.kernel_object, offset, NULL, 0);
		if (pg) {
			atomic_clearbits_int(&pg->pg_flags, PG_BUSY);
			UVM_PAGE_OWN(pg, NULL);
		}
		if (__predict_false(pg == NULL)) {
			if (curproc == uvm.pagedaemon_proc) {
				/*
				 * It is unfeasible for the page daemon to
				 * sleep for memory, so free what we have
				 * allocated and fail.
				 */
				uvm_unmap(map, kva, loopva - kva);
				return (0);
			} else {
				uvm_wait("km_alloc1w");	/* wait for memory */
				continue;
			}
		}

		/*
		 * map it in; note we're never called with an intrsafe
		 * object, so we always use regular old pmap_enter().
		 */
		pmap_enter(map->pmap, loopva, VM_PAGE_TO_PHYS(pg),
		    PROT_READ | PROT_WRITE,
		    PROT_READ | PROT_WRITE | PMAP_WIRED);

		loopva += PAGE_SIZE;
		offset += PAGE_SIZE;
		size -= PAGE_SIZE;
	}
	pmap_update(map->pmap);
	
	/*
	 * zero on request (note that "size" is now zero due to the above loop
	 * so we need to subtract kva from loopva to reconstruct the size).
	 */
	if (zeroit)
		memset((caddr_t)kva, 0, loopva - kva);

	return(kva);
}

/*
 * uvm_km_valloc: allocate zero-fill memory in the kernel's address space
 *
 * => memory is not allocated until fault time
 */

vaddr_t
uvm_km_valloc(struct vm_map *map, vsize_t size)
{
	return(uvm_km_valloc_align(map, size, 0, 0));
}

vaddr_t
uvm_km_valloc_try(struct vm_map *map, vsize_t size)
{
	return(uvm_km_valloc_align(map, size, 0, UVM_FLAG_TRYLOCK));
}

vaddr_t
uvm_km_valloc_align(struct vm_map *map, vsize_t size, vsize_t align, int flags)
{
	vaddr_t kva;

	KASSERT(vm_map_pmap(map) == pmap_kernel());

	size = round_page(size);
	kva = vm_map_min(map);		/* hint */

	/* allocate some virtual space, demand filled by kernel_object. */

	if (__predict_false(uvm_map(map, &kva, size, uvm.kernel_object,
	    UVM_UNKNOWN_OFFSET, align,
	    UVM_MAPFLAG(PROT_READ | PROT_WRITE, PROT_READ | PROT_WRITE,
	    MAP_INHERIT_NONE, MADV_RANDOM, flags)) != 0)) {
		return(0);
	}

	return(kva);
}

/*
 * uvm_km_valloc_wait: allocate zero-fill memory in the kernel's address space
 *
 * => memory is not allocated until fault time
 * => if no room in map, wait for space to free, unless requested size
 *    is larger than map (in which case we return 0)
 */
vaddr_t
uvm_km_valloc_prefer_wait(struct vm_map *map, vsize_t size, voff_t prefer)
{
	vaddr_t kva;

	KASSERT(vm_map_pmap(map) == pmap_kernel());

	size = round_page(size);
	if (size > vm_map_max(map) - vm_map_min(map))
		return(0);

	while (1) {
		kva = vm_map_min(map);		/* hint */

		/*
		 * allocate some virtual space.   will be demand filled
		 * by kernel_object.
		 */
		if (__predict_true(uvm_map(map, &kva, size, uvm.kernel_object,
		    prefer, 0,
		    UVM_MAPFLAG(PROT_READ | PROT_WRITE, PROT_READ | PROT_WRITE,
		    MAP_INHERIT_NONE, MADV_RANDOM, 0)) == 0)) {
			return(kva);
		}

		/* failed.  sleep for a while (on map) */
		tsleep(map, PVM, "vallocwait", 0);
	}
	/*NOTREACHED*/
}

vaddr_t
uvm_km_valloc_wait(struct vm_map *map, vsize_t size)
{
	return uvm_km_valloc_prefer_wait(map, size, UVM_UNKNOWN_OFFSET);
}

#if defined(__HAVE_PMAP_DIRECT)
/*
 * uvm_km_page allocator, __HAVE_PMAP_DIRECT arch
 * On architectures with machine memory direct mapped into a portion
 * of KVM, we have very little work to do.  Just get a physical page,
 * and find and return its VA.
 */
void
uvm_km_page_init(void)
{
	/* nothing */
}

void
uvm_km_page_lateinit(void)
{
	/* nothing */
}

#else
/*
 * uvm_km_page allocator, non __HAVE_PMAP_DIRECT archs
 * This is a special allocator that uses a reserve of free pages
 * to fulfill requests.  It is fast and interrupt safe, but can only
 * return page sized regions.  Its primary use is as a backend for pool.
 *
 * The memory returned is allocated from the larger kernel_map, sparing
 * pressure on the small interrupt-safe kmem_map.  It is wired, but
 * not zero filled.
 */

struct uvm_km_pages uvm_km_pages;

void uvm_km_createthread(void *);
void uvm_km_thread(void *);
struct uvm_km_free_page *uvm_km_doputpage(struct uvm_km_free_page *);

/*
 * Allocate the initial reserve, and create the thread which will
 * keep the reserve full.  For bootstrapping, we allocate more than
 * the lowat amount, because it may be a while before the thread is
 * running.
 */
void
uvm_km_page_init(void)
{
	int	lowat_min;
	int	i;
	int	len, bulk;
	vaddr_t	addr;

	mtx_init(&uvm_km_pages.mtx, IPL_VM);
	if (!uvm_km_pages.lowat) {
		/* based on physmem, calculate a good value here */
		uvm_km_pages.lowat = physmem / 256;
		lowat_min = physmem < atop(16 * 1024 * 1024) ? 32 : 128;
		if (uvm_km_pages.lowat < lowat_min)
			uvm_km_pages.lowat = lowat_min;
	}
	if (uvm_km_pages.lowat > UVM_KM_PAGES_LOWAT_MAX)
		uvm_km_pages.lowat = UVM_KM_PAGES_LOWAT_MAX;
	uvm_km_pages.hiwat = 4 * uvm_km_pages.lowat;
	if (uvm_km_pages.hiwat > UVM_KM_PAGES_HIWAT_MAX)
		uvm_km_pages.hiwat = UVM_KM_PAGES_HIWAT_MAX;

	/* Allocate all pages in as few allocations as possible. */
	len = 0;
	bulk = uvm_km_pages.hiwat;
	while (len < uvm_km_pages.hiwat && bulk > 0) {
		bulk = MIN(bulk, uvm_km_pages.hiwat - len);
		addr = vm_map_min(kernel_map);
		if (uvm_map(kernel_map, &addr, (vsize_t)bulk << PAGE_SHIFT,
		    NULL, UVM_UNKNOWN_OFFSET, 0,
		    UVM_MAPFLAG(PROT_READ | PROT_WRITE,
		    PROT_READ | PROT_WRITE, MAP_INHERIT_NONE,
		    MADV_RANDOM, UVM_KMF_TRYLOCK)) != 0) {
			bulk /= 2;
			continue;
		}

		for (i = len; i < len + bulk; i++, addr += PAGE_SIZE)
			uvm_km_pages.page[i] = addr;
		len += bulk;
	}

	uvm_km_pages.free = len;
	for (i = len; i < UVM_KM_PAGES_HIWAT_MAX; i++)
		uvm_km_pages.page[i] = 0;

	/* tone down if really high */
	if (uvm_km_pages.lowat > 512)
		uvm_km_pages.lowat = 512;
}

void
uvm_km_page_lateinit(void)
{
	kthread_create_deferred(uvm_km_createthread, NULL);
}

void
uvm_km_createthread(void *arg)
{
	kthread_create(uvm_km_thread, NULL, &uvm_km_pages.km_proc, "kmthread");
}

/*
 * Endless loop.  We grab pages in increments of 16 pages, then
 * quickly swap them into the list.  At some point we can consider
 * returning memory to the system if we have too many free pages,
 * but that's not implemented yet.
 */
void
uvm_km_thread(void *arg)
{
	vaddr_t pg[16];
	int i;
	int allocmore = 0;
	int flags;
	struct uvm_km_free_page *fp = NULL;

	KERNEL_UNLOCK();

	for (;;) {
		mtx_enter(&uvm_km_pages.mtx);
		if (uvm_km_pages.free >= uvm_km_pages.lowat &&
		    uvm_km_pages.freelist == NULL) {
			msleep(&uvm_km_pages.km_proc, &uvm_km_pages.mtx,
			    PVM, "kmalloc", 0);
		}
		allocmore = uvm_km_pages.free < uvm_km_pages.lowat;
		fp = uvm_km_pages.freelist;
		uvm_km_pages.freelist = NULL;
		uvm_km_pages.freelistlen = 0;
		mtx_leave(&uvm_km_pages.mtx);

		if (allocmore) {
			/*
			 * If there was nothing on the freelist, then we
			 * must obtain at least one page to make progress.
			 * So, only use UVM_KMF_TRYLOCK for the first page
			 * if fp != NULL
			 */
			flags = UVM_MAPFLAG(PROT_READ | PROT_WRITE,
			    PROT_READ | PROT_WRITE, MAP_INHERIT_NONE,
			    MADV_RANDOM, fp != NULL ? UVM_KMF_TRYLOCK : 0);
			memset(pg, 0, sizeof(pg));
			for (i = 0; i < nitems(pg); i++) {
				pg[i] = vm_map_min(kernel_map);
				if (uvm_map(kernel_map, &pg[i], PAGE_SIZE,
				    NULL, UVM_UNKNOWN_OFFSET, 0, flags) != 0) {
					pg[i] = 0;
					break;
				}

				/* made progress, so don't sleep for more */
				flags = UVM_MAPFLAG(PROT_READ | PROT_WRITE,
				    PROT_READ | PROT_WRITE, MAP_INHERIT_NONE,
				    MADV_RANDOM, UVM_KMF_TRYLOCK);
			}

			mtx_enter(&uvm_km_pages.mtx);
			for (i = 0; i < nitems(pg); i++) {
				if (uvm_km_pages.free ==
				    nitems(uvm_km_pages.page))
					break;
				else if (pg[i] != 0)
					uvm_km_pages.page[uvm_km_pages.free++]
					    = pg[i];
			}
			wakeup(&uvm_km_pages.free);
			mtx_leave(&uvm_km_pages.mtx);

			/* Cleanup left-over pages (if any). */
			for (; i < nitems(pg); i++) {
				if (pg[i] != 0) {
					uvm_unmap(kernel_map,
					    pg[i], pg[i] + PAGE_SIZE);
				}
			}
		}
		while (fp) {
			fp = uvm_km_doputpage(fp);
		}
	}
}

struct uvm_km_free_page *
uvm_km_doputpage(struct uvm_km_free_page *fp)
{
	vaddr_t va = (vaddr_t)fp;
	struct vm_page *pg;
	int	freeva = 1;
	struct uvm_km_free_page *nextfp = fp->next;

	pg = uvm_atopg(va);

	pmap_kremove(va, PAGE_SIZE);
	pmap_update(kernel_map->pmap);

	mtx_enter(&uvm_km_pages.mtx);
	if (uvm_km_pages.free < uvm_km_pages.hiwat) {
		uvm_km_pages.page[uvm_km_pages.free++] = va;
		freeva = 0;
	}
	mtx_leave(&uvm_km_pages.mtx);

	if (freeva)
		uvm_unmap(kernel_map, va, va + PAGE_SIZE);

	uvm_pagefree(pg);
	return (nextfp);
}
#endif	/* !__HAVE_PMAP_DIRECT */

void *
km_alloc(size_t sz, const struct kmem_va_mode *kv,
    const struct kmem_pa_mode *kp, const struct kmem_dyn_mode *kd)
{
	struct vm_map *map;
	struct vm_page *pg;
	struct pglist pgl;
	int mapflags = 0;
	vm_prot_t prot;
	paddr_t pla_align;
	int pla_flags;
	int pla_maxseg;
	vaddr_t va, sva;

	KASSERT(sz == round_page(sz));

	TAILQ_INIT(&pgl);

	if (kp->kp_nomem || kp->kp_pageable)
		goto alloc_va;

	pla_flags = kd->kd_waitok ? UVM_PLA_WAITOK : UVM_PLA_NOWAIT;
	pla_flags |= UVM_PLA_TRYCONTIG;
	if (kp->kp_zero)
		pla_flags |= UVM_PLA_ZERO;

	pla_align = kp->kp_align;
#ifdef __HAVE_PMAP_DIRECT
	if (pla_align < kv->kv_align)
		pla_align = kv->kv_align;
#endif
	pla_maxseg = kp->kp_maxseg;
	if (pla_maxseg == 0)
		pla_maxseg = sz / PAGE_SIZE;

	if (uvm_pglistalloc(sz, kp->kp_constraint->ucr_low,
	    kp->kp_constraint->ucr_high, pla_align, kp->kp_boundary,
	    &pgl, pla_maxseg, pla_flags)) {	
		return (NULL);
	}

#ifdef __HAVE_PMAP_DIRECT
	/*
	 * Only use direct mappings for single page or single segment
	 * allocations.
	 */
	if (kv->kv_singlepage || kp->kp_maxseg == 1) {
		TAILQ_FOREACH(pg, &pgl, pageq) {
			va = pmap_map_direct(pg);
			if (pg == TAILQ_FIRST(&pgl))
				sva = va;
		}
		return ((void *)sva);
	}
#endif
alloc_va:
	prot = PROT_READ | PROT_WRITE;

	if (kp->kp_pageable) {
		KASSERT(kp->kp_object);
		KASSERT(!kv->kv_singlepage);
	} else {
		KASSERT(kp->kp_object == NULL);
	}

	if (kv->kv_singlepage) {
		KASSERT(sz == PAGE_SIZE);
#ifdef __HAVE_PMAP_DIRECT
		panic("km_alloc: DIRECT single page");
#else
		mtx_enter(&uvm_km_pages.mtx);
		while (uvm_km_pages.free == 0) {
			if (kd->kd_waitok == 0) {
				mtx_leave(&uvm_km_pages.mtx);
				uvm_pglistfree(&pgl);
				return NULL;
			}
			msleep(&uvm_km_pages.free, &uvm_km_pages.mtx, PVM,
			    "getpage", 0);
		}
		va = uvm_km_pages.page[--uvm_km_pages.free];
		if (uvm_km_pages.free < uvm_km_pages.lowat &&
		    curproc != uvm_km_pages.km_proc) {
			if (kd->kd_slowdown)
				*kd->kd_slowdown = 1;
			wakeup(&uvm_km_pages.km_proc);
		}
		mtx_leave(&uvm_km_pages.mtx);
#endif
	} else {
		struct uvm_object *uobj = NULL;

		if (kd->kd_trylock)
			mapflags |= UVM_KMF_TRYLOCK;

		if (kp->kp_object)
			uobj = *kp->kp_object;
try_map:
		map = *kv->kv_map;
		va = vm_map_min(map);
		if (uvm_map(map, &va, sz, uobj, kd->kd_prefer,
		    kv->kv_align, UVM_MAPFLAG(prot, prot, MAP_INHERIT_NONE,
		    MADV_RANDOM, mapflags))) {
			if (kv->kv_wait && kd->kd_waitok) {
				tsleep(map, PVM, "km_allocva", 0);
				goto try_map;
			}
			uvm_pglistfree(&pgl);
			return (NULL);
		}
	}
	sva = va;
	TAILQ_FOREACH(pg, &pgl, pageq) {
		if (kp->kp_pageable)
			pmap_enter(pmap_kernel(), va, VM_PAGE_TO_PHYS(pg),
			    prot, prot | PMAP_WIRED);
		else
			pmap_kenter_pa(va, VM_PAGE_TO_PHYS(pg), prot);
		va += PAGE_SIZE;
	}
	pmap_update(pmap_kernel());
	return ((void *)sva);
}

void
km_free(void *v, size_t sz, const struct kmem_va_mode *kv,
    const struct kmem_pa_mode *kp)
{
	vaddr_t sva, eva, va;
	struct vm_page *pg;
	struct pglist pgl;

	sva = (vaddr_t)v;
	eva = sva + sz;

	if (kp->kp_nomem)
		goto free_va;

#ifdef __HAVE_PMAP_DIRECT
	if (kv->kv_singlepage || kp->kp_maxseg == 1) {
		TAILQ_INIT(&pgl);
		for (va = sva; va < eva; va += PAGE_SIZE) {
			pg = pmap_unmap_direct(va);
			TAILQ_INSERT_TAIL(&pgl, pg, pageq);
		}
		uvm_pglistfree(&pgl);
		return;
	}
#else
	if (kv->kv_singlepage) {
		struct uvm_km_free_page *fp = v;

		mtx_enter(&uvm_km_pages.mtx);
		fp->next = uvm_km_pages.freelist;
		uvm_km_pages.freelist = fp;
		if (uvm_km_pages.freelistlen++ > 16)
			wakeup(&uvm_km_pages.km_proc);
		mtx_leave(&uvm_km_pages.mtx);
		return;
	}
#endif

	if (kp->kp_pageable) {
		pmap_remove(pmap_kernel(), sva, eva);
		pmap_update(pmap_kernel());
	} else {
		TAILQ_INIT(&pgl);
		for (va = sva; va < eva; va += PAGE_SIZE) {
			paddr_t pa;

			if (!pmap_extract(pmap_kernel(), va, &pa))
				continue;

			pg = PHYS_TO_VM_PAGE(pa);
			if (pg == NULL) {
				panic("km_free: unmanaged page 0x%lx\n", pa);
			}
			TAILQ_INSERT_TAIL(&pgl, pg, pageq);
		}
		pmap_kremove(sva, sz);
		pmap_update(pmap_kernel());
		uvm_pglistfree(&pgl);
	}
free_va:
	uvm_unmap(*kv->kv_map, sva, eva);
	if (kv->kv_wait)
		wakeup(*kv->kv_map);
}

const struct kmem_va_mode kv_any = {
	.kv_map = &kernel_map,
};

const struct kmem_va_mode kv_intrsafe = {
	.kv_map = &kmem_map,
};

const struct kmem_va_mode kv_page = {
	.kv_singlepage = 1
};

const struct kmem_pa_mode kp_dirty = {
	.kp_constraint = &no_constraint
};

const struct kmem_pa_mode kp_dma = {
	.kp_constraint = &dma_constraint
};

const struct kmem_pa_mode kp_dma_contig = {
	.kp_constraint = &dma_constraint,
	.kp_maxseg = 1
};

const struct kmem_pa_mode kp_dma_zero = {
	.kp_constraint = &dma_constraint,
	.kp_zero = 1
};

const struct kmem_pa_mode kp_zero = {
	.kp_constraint = &no_constraint,
	.kp_zero = 1
};

const struct kmem_pa_mode kp_pageable = {
	.kp_object = &uvm.kernel_object,
	.kp_pageable = 1
/* XXX - kp_nomem, maybe, but we'll need to fix km_free. */
};

const struct kmem_pa_mode kp_none = {
	.kp_nomem = 1
};

const struct kmem_dyn_mode kd_waitok = {
	.kd_waitok = 1,
	.kd_prefer = UVM_UNKNOWN_OFFSET
};

const struct kmem_dyn_mode kd_nowait = {
	.kd_prefer = UVM_UNKNOWN_OFFSET
};

const struct kmem_dyn_mode kd_trylock = {
	.kd_trylock = 1,
	.kd_prefer = UVM_UNKNOWN_OFFSET
};
@


1.129
log
@reorder uvm init to avoid use before initialisation.

the particular use before init was in uvm_init step 6, which calls
kmeminit to set up malloc(9), which calls uvm_km_zalloc, which calls
pmap_enter, which calls pool_get, which tries to allocate a page
using km_alloc, which isnt initalised until step 9 in uvm_init.

uvm_km_page_init calls kthread_create though, which uses malloc
internally, so it cant be reordered before malloc init.

to cope with this, uvm_km_page_init is split up. it sets up the
subsystem, and is called before kmeminit. the thread init is moved
to uvm_km_page_lateinit, which is called after kmeminit in uvm_init.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.128 2015/09/26 17:55:00 kettenis Exp $	*/
d609 1
a609 1
uvm_km_page_latethread(void)
@


1.128
log
@Protect the list of free map entries with a mutex.  This should fix the
crashes seen by sthen@@ on i386.

ok visa@@, guenther@@, tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.127 2015/09/17 18:04:49 kettenis Exp $	*/
d608 6
d687 1
d689 3
@


1.127
log
@Back out rev. 1.125.  This bit was left behind (intentionally?) when the
remainder of that commit was backed out.  However,clearing the PQ_AOBJ bit
here is definitely wrong.

Our pagedaemon uses two separate lists to keep track of inactive
pages.  It uses PQ_SWAPBACKED, which really is both PQ_ANON and
PQ_AOBJ to keep track of which inactive queue a page is sitting on.
So if you twiddle PQ_AOBJ (or PQ_ANON) for an inactive page, a
subsequent uvm_pagefree(9) will remove the page from the wrong queue!
This usually goes unnoticed, but if the page happens to be the last
one on the queue, the queues get corrupted.  The damage quickly
spreads to the free page queues and almost certainly results in the
uvm_pmr_size_RB_REMOVE_COLOR() faults that people have seen
sporadically since the spring of this year.

ok visa@@, beck@@, krw@@, guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.126 2015/02/07 08:21:24 miod Exp $	*/
d705 2
@


1.126
log
@Introduce VM_KERNEL_SPACE_SIZE as a replacement for
(VM_MAX_KERNEL_ADDRESS - VM_MIN_KERNEL_ADDRESS). This will allow these to no
longer be constants in the future.

ok guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.125 2015/02/06 10:58:35 deraadt Exp $	*/
a261 1
			atomic_clearbits_int(&pp->pg_flags, PQ_AOBJ);
@


1.126.6.1
log
@backport 1.127 by kettenis, the remainder of the 1.125 reversion.
solves panics resulting from queue corruption
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.126 2015/02/07 08:21:24 miod Exp $	*/
d262 1
@


1.126.2.1
log
@backport 1.127 by kettenis, the remainder of the 1.125 reversion.
solves panics resulting from queue corruption
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.126 2015/02/07 08:21:24 miod Exp $	*/
d262 1
@


1.125
log
@Clear PQ_AOBJ before calling uvm_pagefree(), clearing up one false XXX
comment (one is fixed, one is deleted).
ok kettenis beck
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.124 2015/01/23 17:09:23 kettenis Exp $	*/
d73 2
a74 2
 * starts at VM_MIN_KERNEL_ADDRESS and goes to VM_MAX_KERNEL_ADDRESS.
 * note that VM_MIN_KERNEL_ADDRESS is equal to vm_map_min(kernel_map).
d101 2
a102 2
 * object is equal to the size of kernel virtual address space (i.e. the
 * value "VM_MAX_KERNEL_ADDRESS - VM_MIN_KERNEL_ADDRESS").
d117 1
a117 1
 * address minus the VM_MIN_KERNEL_ADDRESS (aka vm_map_min(kernel_map)).
d119 1
a119 1
 *   suppose VM_MIN_KERNEL_ADDRESS is 0xf8000000 and the kernel does a
d155 3
a157 3
 * => KVM is defined by VM_MIN_KERNEL_ADDRESS/VM_MAX_KERNEL_ADDRESS.
 *    we assume that [min -> start] has already been allocated and that
 *    "end" is the end.
d160 1
a160 1
uvm_km_init(vaddr_t start, vaddr_t end)
a161 4
	vaddr_t base = VM_MIN_KERNEL_ADDRESS;

	/* next, init kernel memory objects. */

d164 1
a164 2
	uvm.kernel_object = uao_create(VM_MAX_KERNEL_ADDRESS -
				 VM_MIN_KERNEL_ADDRESS, UAO_FLAG_KERNOBJ);
@


1.124
log
@Make km_alloc(9) use the direct map for all "phys contig" mappings requested
by the caller on architectures that implement them.  Make sure that we
physically align memory such that we meet any demands on virtual alignment
in this case.  This should reduce the overhead of mapping large pool pages
for pools that request dma'able memory.

ok deraadt@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.123 2014/12/17 06:58:11 guenther Exp $	*/
d267 1
@


1.123
log
@Prefer MADV_* over POSIX_MADV_* in kernel for consistency: the latter
doesn't have all the values and therefore can't be used everywhere.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.122 2014/12/15 02:24:23 guenther Exp $	*/
d812 1
a814 3
#ifdef __HAVE_PMAP_DIRECT
	paddr_t pa;
#endif
d829 5
d839 1
a839 1
	    kp->kp_constraint->ucr_high, kp->kp_align, kp->kp_boundary,
a844 3
	if (kv->kv_align)
		goto alloc_va;
#if 1
d846 2
a847 3
	 * For now, only do DIRECT mappings for single page
	 * allocations, until we figure out a good way to deal
	 * with contig allocations in km_free.
d849 1
a849 14
	if (!kv->kv_singlepage)
		goto alloc_va;
#endif
	/*
	 * Dubious optimization. If we got a contig segment, just map it
	 * through the direct map.
	 */
	TAILQ_FOREACH(pg, &pgl, pageq) {
		if (pg != TAILQ_FIRST(&pgl) &&
		    VM_PAGE_TO_PHYS(pg) != pa + PAGE_SIZE)
			break;
		pa = VM_PAGE_TO_PHYS(pg);
	}
	if (pg == NULL) {
d851 1
a851 2
			vaddr_t v;
			v = pmap_map_direct(pg);
d853 1
a853 1
				va = v;
d855 1
a855 1
		return ((void *)va);
d935 2
a936 2
	sva = va = (vaddr_t)v;
	eva = va + sz;
d938 1
a938 1
	if (kp->kp_nomem) {
a939 1
	}
a940 1
	if (kv->kv_singlepage) {
d942 9
a950 2
		pg = pmap_unmap_direct(va);
		uvm_pagefree(pg);
d952 1
d954 1
a960 1
#endif
d963 1
@


1.122
log
@Use MAP_INHERIT_* for the 'inh' argument to the UMV_MAPFLAG() macro,
eliminating the must-be-kept-in-sync UVM_INH_* macros

ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.121 2014/11/27 21:40:52 kettenis Exp $	*/
d187 1
a187 1
	    MAP_INHERIT_NONE, POSIX_MADV_RANDOM, UVM_FLAG_FIXED)) != 0)
d213 2
a214 3
	    UVM_MAPFLAG(PROT_READ | PROT_WRITE,
	    PROT_READ | PROT_WRITE, MAP_INHERIT_NONE,
	    POSIX_MADV_RANDOM, mapflags)) != 0) {
d342 2
a343 3
	    valign, UVM_MAPFLAG(PROT_READ | PROT_WRITE,
	    PROT_READ | PROT_WRITE, MAP_INHERIT_NONE,
	    POSIX_MADV_RANDOM, (flags & UVM_KMF_TRYLOCK))) != 0)) {
d461 1
a461 1
	    MAP_INHERIT_NONE, POSIX_MADV_RANDOM, 0)) != 0)) {
d549 1
a549 1
	    MAP_INHERIT_NONE, POSIX_MADV_RANDOM, flags)) != 0)) {
d584 1
a584 1
		    MAP_INHERIT_NONE, POSIX_MADV_RANDOM, 0)) == 0)) {
d669 1
a669 1
		    POSIX_MADV_RANDOM, UVM_KMF_TRYLOCK)) != 0) {
d732 2
a733 3
			    PROT_READ | PROT_WRITE,
			    MAP_INHERIT_NONE, POSIX_MADV_RANDOM,
			    fp != NULL ? UVM_KMF_TRYLOCK : 0);
d745 2
a746 3
				    PROT_READ | PROT_WRITE,
				    MAP_INHERIT_NONE, POSIX_MADV_RANDOM,
				    UVM_KMF_TRYLOCK);
d920 1
a920 1
		    POSIX_MADV_RANDOM, mapflags))) {
@


1.121
log
@The sti(4) driver copies its ROM into kernel memory and executes the code
in there.  It explicitly changes the mapping of that memory to RX, but this
only works if the maximum protection of the mapping includes PROT_EXEC.

ok miod@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.120 2014/11/21 06:40:40 deraadt Exp $	*/
d187 1
a187 1
	    UVM_INH_NONE, POSIX_MADV_RANDOM, UVM_FLAG_FIXED)) != 0)
d214 1
a214 1
	    PROT_READ | PROT_WRITE, UVM_INH_NONE,
d344 1
a344 1
	    PROT_READ | PROT_WRITE, UVM_INH_NONE,
d463 1
a463 1
	    UVM_INH_NONE, POSIX_MADV_RANDOM, 0)) != 0)) {
d551 1
a551 1
	    UVM_INH_NONE, POSIX_MADV_RANDOM, flags)) != 0)) {
d586 1
a586 1
		    UVM_INH_NONE, POSIX_MADV_RANDOM, 0)) == 0)) {
d670 1
a670 1
		    PROT_READ | PROT_WRITE, UVM_INH_NONE,
d735 1
a735 1
			    UVM_INH_NONE, POSIX_MADV_RANDOM,
d749 1
a749 1
				    UVM_INH_NONE, POSIX_MADV_RANDOM,
d923 1
a923 1
		    kv->kv_align, UVM_MAPFLAG(prot, prot, UVM_INH_NONE,
@


1.120
log
@Kill kv_executable flag.  We no longer allow requests for PROT_EXEC
mappings via this interface (nothing uses it, in any case)
ok uebayasi tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.119 2014/11/17 04:31:08 deraadt Exp $	*/
d461 2
a462 1
	    UVM_MAPFLAG(PROT_READ | PROT_WRITE, PROT_READ | PROT_WRITE,
@


1.119
log
@More cases of kernel map entries being created as EXEC by default; not
just the base permission but the maxprot as well.
ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.118 2014/11/17 04:26:53 deraadt Exp $	*/
d845 1
a845 1
	if (kv->kv_align || kv->kv_executable)
d877 1
a877 5
	if (kv->kv_executable) {
		prot = PROT_READ | PROT_WRITE | PROT_EXEC;
	} else {
		prot = PROT_READ | PROT_WRITE;
	}
@


1.118
log
@There is no reason for uvm_km_alloc1() to allocate kernel memory
that is executable.
ok tedu kettenis guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.117 2014/11/16 12:31:00 deraadt Exp $	*/
d185 2
a186 1
	    NULL, UVM_UNKNOWN_OFFSET, 0, UVM_MAPFLAG(PROT_MASK, PROT_MASK,
d213 2
a214 1
	    UVM_MAPFLAG(PROT_MASK, PROT_MASK, UVM_INH_NONE,
d460 2
a461 1
	    UVM_UNKNOWN_OFFSET, align, UVM_MAPFLAG(PROT_MASK, PROT_MASK,
d548 2
a549 1
	    UVM_UNKNOWN_OFFSET, align, UVM_MAPFLAG(PROT_MASK, PROT_MASK,
d583 3
a585 2
		    prefer, 0, UVM_MAPFLAG(PROT_MASK,
		    PROT_MASK, UVM_INH_NONE, POSIX_MADV_RANDOM, 0)) == 0)) {
@


1.117
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.116 2014/11/13 00:47:44 tedu Exp $	*/
d495 1
a495 1
		    PROT_READ | PROT_WRITE | PROT_EXEC,
a496 1
		/* XXX why is the above executable? */
@


1.116
log
@bzero -> memset
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.115 2014/09/14 14:17:27 jsg Exp $	*/
d185 2
a186 2
	    NULL, UVM_UNKNOWN_OFFSET, 0, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL,
	    UVM_INH_NONE, UVM_ADV_RANDOM,UVM_FLAG_FIXED)) != 0)
d212 2
a213 2
	    UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL, UVM_INH_NONE,
	    UVM_ADV_RANDOM, mapflags)) != 0) {
d341 3
a343 2
	      valign, UVM_MAPFLAG(UVM_PROT_RW, UVM_PROT_RW, UVM_INH_NONE,
			  UVM_ADV_RANDOM, (flags & UVM_KMF_TRYLOCK))) != 0)) {
d394 1
a394 1
			    UVM_PROT_RW);
d397 2
a398 2
			    UVM_PROT_RW,
			    PMAP_WIRED | VM_PROT_READ | VM_PROT_WRITE);
d458 2
a459 2
	    UVM_UNKNOWN_OFFSET, align, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL,
	    UVM_INH_NONE, UVM_ADV_RANDOM, 0)) != 0)) {
d495 3
a497 1
		    UVM_PROT_ALL, PMAP_WIRED | VM_PROT_READ | VM_PROT_WRITE);
d546 2
a547 2
	    UVM_UNKNOWN_OFFSET, align, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL,
	    UVM_INH_NONE, UVM_ADV_RANDOM, flags)) != 0)) {
d580 2
a581 2
		    prefer, 0, UVM_MAPFLAG(UVM_PROT_ALL,
		    UVM_PROT_ALL, UVM_INH_NONE, UVM_ADV_RANDOM, 0)) == 0)) {
d664 3
a666 2
		    UVM_MAPFLAG(UVM_PROT_RW, UVM_PROT_RW, UVM_INH_NONE,
		    UVM_ADV_RANDOM, UVM_KMF_TRYLOCK)) != 0) {
d728 3
a730 2
			flags = UVM_MAPFLAG(UVM_PROT_RW, UVM_PROT_RW,
			    UVM_INH_NONE, UVM_ADV_RANDOM,
d742 3
a744 2
				flags = UVM_MAPFLAG(UVM_PROT_RW, UVM_PROT_RW,
				    UVM_INH_NONE, UVM_ADV_RANDOM,
d874 1
a874 1
		prot = VM_PROT_READ | VM_PROT_WRITE | VM_PROT_EXECUTE;
d876 1
a876 1
		prot = VM_PROT_READ | VM_PROT_WRITE;
d923 1
a923 1
		    UVM_ADV_RANDOM, mapflags))) {
@


1.115
log
@remove uneeded proc.h includes
ok mpi@@ kspillner@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.114 2014/07/11 16:35:40 jsg Exp $	*/
d727 1
a727 1
			bzero(pg, sizeof(pg));
@


1.114
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.113 2014/06/21 21:09:25 guenther Exp $	*/
a133 1
#include <sys/proc.h>
@


1.113
log
@Make sure kmthread never loops without making progress: if the freelist
was empty then the first page allocation should sleep until it can get one.

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.112 2014/04/13 23:14:15 tedu Exp $	*/
d21 1
a21 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by Charles D. Cranor,
 *      Washington University, the University of California, Berkeley and 
 *      its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.112
log
@compress code by turning four line comments into one line comments.
emphatic ok usual suspects, grudging ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.111 2013/05/30 18:02:04 tedu Exp $	*/
d707 1
d724 9
d737 1
a737 4
				    NULL, UVM_UNKNOWN_OFFSET, 0,
				    UVM_MAPFLAG(UVM_PROT_RW, UVM_PROT_RW,
				    UVM_INH_NONE, UVM_ADV_RANDOM,
				    UVM_KMF_TRYLOCK)) != 0) {
d741 5
@


1.111
log
@in the brave new world of void *, we don't need caddr_t casts
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.110 2013/05/30 16:39:26 tedu Exp $	*/
a154 1

a164 1

d170 1
a170 3
	/*
	 * next, init kernel memory objects.
	 */
a194 4
	/*
	 * install!
	 */

d216 1
a216 4
	/*
	 * first allocate a blank spot in the parent map
	 */

d223 1
a223 4
	/*
	 * set VM bounds (min is filled in by uvm_map)
	 */

d226 1
a226 4
	/*
	 * add references to pmap and create or init the submap
	 */

d237 1
a237 4
	/*
	 * now let uvm_map_submap plug in it...
	 */

a291 1

a322 1

d339 1
a339 4
	/*
	 * setup for call
	 */

d345 1
a345 4
	/*
	 * allocate some virtual space
	 */

d352 1
a352 4
	/*
	 * if all we wanted was VA, return now
	 */

d357 1
a357 4
	/*
	 * recover object offset from virtual address
	 */

a396 1

a416 1

a428 1

a448 1

d461 1
a461 4
	/*
	 * allocate some virtual space
	 */

d468 1
a468 4
	/*
	 * recover object offset from virtual address
	 */

d471 1
a471 4
	/*
	 * now allocate the memory.  we must be careful about released pages.
	 */

a511 1

d546 1
a546 3
	/*
	 * allocate some virtual space.  will be demand filled by kernel_object.
	 */
a563 1

a581 1

d588 1
a588 4
		/*
		 * failed.  sleep for a while (on map)
		 */

@


1.110
log
@UVM_UNLOCK_AND_WAIT no longer unlocks, so rename it to UVM_WAIT.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.109 2013/05/30 15:17:59 tedu Exp $	*/
d644 1
a644 1
		tsleep((caddr_t)map, PVM, "vallocwait", 0);
@


1.109
log
@remove simple_locks from uvm code. ok beck deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.108 2012/11/10 11:18:45 kettenis Exp $	*/
d283 1
a283 2
			UVM_UNLOCK_AND_WAIT(pp, &uobj->vmobjlock, 0,
			    "km_pgrm", 0);
@


1.108
log
@Number of swap pages in use must be smaller than tha total number of swap
pages, so fix non-sensical comparison introduced in rev 1.77.

ok miod@@, krw@@, beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.107 2012/03/09 13:01:29 ariane Exp $	*/
a284 1
			simple_lock(&uobj->vmobjlock);
a296 1
			simple_lock(&uvm.swap_data_lock);
a297 1
			simple_unlock(&uvm.swap_data_lock);
a521 1
		simple_lock(&uvm.kernel_object->vmobjlock);
a527 1
		simple_unlock(&uvm.kernel_object->vmobjlock);
@


1.107
log
@New vmmap implementation.

no oks (it is really a pain to review properly)
extensively tested, I'm confident it'll be stable
'now is the time' from several icb inhabitants

Diff provides:
- ability to specify different allocators for different regions/maps
- a simpler implementation of the current allocator
- currently in compatibility mode: it will generate similar addresses
  as the old allocator
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.106 2011/07/03 18:34:14 oga Exp $	*/
d407 1
d410 1
a410 1
	    uvmexp.swpgonly - uvmexp.swpages <= atop(size)))
@


1.106
log
@Rip out and burn support for UVM_HIST.

The vm hackers don't use it, don't maintain it and have to look at it all the
time. About time this 800 lines of code hit /dev/null.

``never liked it'' tedu@@. ariane@@ was very happy when i told her i wrote
this diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.105 2011/06/23 21:54:56 oga Exp $	*/
a140 1

d186 7
a192 1
	uvm_map_setup(&kernel_map_store, base, end, VM_MAP_PAGEABLE);
d472 1
a472 1
	struct vm_map_entry *dead_entries;
d475 1
d477 1
a477 1
	     &dead_entries, NULL, FALSE);
d481 1
a481 2
	if (dead_entries != NULL)
		uvm_unmap_detach(dead_entries, 0);
d700 4
a703 2
	int lowat_min;
	int i;
d719 17
a735 5
	for (i = 0; i < uvm_km_pages.hiwat; i++) {
		uvm_km_pages.page[i] = (vaddr_t)uvm_km_kmemalloc(kernel_map,
		    NULL, PAGE_SIZE, UVM_KMF_NOWAIT|UVM_KMF_VALLOC);
		if (uvm_km_pages.page[i] == 0)
			break;
d737 3
a739 2
	uvm_km_pages.free = i;
	for ( ; i < UVM_KM_PAGES_HIWAT_MAX; i++)
d783 1
d785 9
a793 2
				pg[i] = (vaddr_t)uvm_km_kmemalloc(kernel_map,
				    NULL, PAGE_SIZE, UVM_KMF_VALLOC);
d795 1
a795 1
	
d801 1
a801 1
				else
d809 6
a814 2
			for (; i < nitems(pg); i++)
				uvm_km_free(kernel_map, pg[i], PAGE_SIZE);
d843 1
a843 1
		uvm_km_free(kernel_map, va, PAGE_SIZE);
@


1.105
log
@Don't bother checking for an empty queue before calling uvm_pglistfree.

It will handle an empty list just fine (there's a small optimisation
possible here to avoid grabbing the fpageqlock if no pages need freeing,
but that is definitely another diff)

ok ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.104 2011/06/23 21:42:05 ariane Exp $	*/
a270 1
	UVMHIST_FUNC("uvm_km_pgremove"); UVMHIST_CALLED(maphist);
a353 1
	UVMHIST_FUNC("uvm_km_kmemalloc"); UVMHIST_CALLED(maphist);
a354 2
	UVMHIST_LOG(maphist,"  (map=%p, obj=%p, size=0x%lx, flags=%d)",
		    map, obj, size, flags);
a375 1
		UVMHIST_LOG(maphist, "<- done (no VM)",0,0,0,0);
a383 1
		UVMHIST_LOG(maphist,"<- done valloc (kva=0x%lx)", kva,0,0,0);
a395 2
	UVMHIST_LOG(maphist, "  kva=0x%lx, offset=0x%lx", kva, offset,0,0);

a443 1
	UVMHIST_LOG(maphist,"<- done (kva=0x%lx)", kva,0,0,0);
a490 1
	UVMHIST_FUNC("uvm_km_alloc1"); UVMHIST_CALLED(maphist);
a491 1
	UVMHIST_LOG(maphist,"(map=%p, size=0x%lx)", map, size,0,0);
a503 1
		UVMHIST_LOG(maphist,"<- done (no VM)",0,0,0,0);
a511 1
	UVMHIST_LOG(maphist,"  kva=0x%lx, offset=0x%lx", kva, offset,0,0);
a562 1
	UVMHIST_LOG(maphist,"<- done (kva=0x%lx)", kva,0,0,0);
a587 1
	UVMHIST_FUNC("uvm_km_valloc"); UVMHIST_CALLED(maphist);
a588 1
	UVMHIST_LOG(maphist, "(map=%p, size=0x%lx)", map, size, 0,0);
a600 1
		UVMHIST_LOG(maphist, "<- done (no VM)", 0,0,0,0);
a603 1
	UVMHIST_LOG(maphist, "<- done (kva=0x%lx)", kva,0,0,0);
a618 1
	UVMHIST_FUNC("uvm_km_valloc_prefer_wait"); UVMHIST_CALLED(maphist);
a619 1
	UVMHIST_LOG(maphist, "(map=%p, size=0x%lx)", map, size, 0,0);
a636 1
			UVMHIST_LOG(maphist,"<- done (kva=0x%lx)", kva,0,0,0);
a643 1
		UVMHIST_LOG(maphist,"<<<sleeping>>>",0,0,0,0);
@


1.104
log
@Make mbufs and dma_alloc be contig allocations.
Requested by dlg@@

ok oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.103 2011/06/06 17:10:23 ariane Exp $	*/
d933 1
a933 2
				if (!TAILQ_EMPTY(&pgl))
					uvm_pglistfree(&pgl);
d966 1
a966 2
			if (!TAILQ_EMPTY(&pgl))
				uvm_pglistfree(&pgl);
@


1.103
log
@Backout vmmap in order to repair virtual address selection algorithms
outside the tree.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.102 2011/05/24 15:27:36 ariane Exp $	*/
d850 1
d868 4
d874 1
a874 1
	    &pgl, sz / PAGE_SIZE, pla_flags)) {	
d1061 5
@


1.102
log
@Reimplement uvm/uvm_map.

vmmap is designed to perform address space randomized allocations,
without letting fragmentation of the address space go through the roof.

Some highlights:
- kernel address space randomization
- proper implementation of guardpages
- roughly 10% system time reduction during kernel build

Tested by alot of people on tech@@ and developers.
Theo's machines are still happy.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.101 2011/05/10 21:48:17 oga Exp $	*/
d187 1
a187 7
	uvm_map_setup(&kernel_map_store, base, end,
#ifdef KVA_GUARDPAGES
	    VM_MAP_PAGEABLE | VM_MAP_GUARDPAGES
#else
	    VM_MAP_PAGEABLE
#endif
	    );
d476 1
a476 1
	struct uvm_map_deadq dead_entries;
a478 1
	TAILQ_INIT(&dead_entries);
d480 1
a480 1
	     &dead_entries, FALSE, TRUE);
d484 2
a485 1
	uvm_unmap_detach(&dead_entries, 0);
@


1.101
log
@Don't leak swapslots when doing a uvm_km_pgremove and a page is in swap only.

Before we were only calling uao_dropswap() if there was a page, maning
that if the buffer was swapped out then we would leak the slot.

Quite rare because only pipebuffers should swap from the kernel object,
but i've seen panics that implied this had happened (alpha.p for example).

ok thib@@ after a lot of discussion and checking the semantics.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.100 2011/04/23 17:48:48 kettenis Exp $	*/
d187 7
a193 1
	uvm_map_setup(&kernel_map_store, base, end, VM_MAP_PAGEABLE);
d482 1
a482 1
	struct vm_map_entry *dead_entries;
d485 1
d487 1
a487 1
	     &dead_entries, NULL, FALSE);
d491 1
a491 2
	if (dead_entries != NULL)
		uvm_unmap_detach(dead_entries, 0);
@


1.100
log
@Fix management of the list of free uvm_km_pages.  Seems art@@ lost a line
when he copied this code from uvm_km_putpage() into km_free().

Found independently by ariane@@; ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.99 2011/04/19 20:00:11 matthew Exp $	*/
d270 1
d277 1
a277 7
		if (pp == NULL)
			continue;

		UVMHIST_LOG(maphist,"  page %p, busy=%ld", pp,
		    pp->pg_flags & PG_BUSY, 0, 0);

		if (pp->pg_flags & PG_BUSY) {
d284 6
a289 8
		} else {
			/* free the swap slot... */
			uao_dropswap(uobj, curoff >> PAGE_SHIFT);

			/*
			 * ...and free the page; note it may be on the
			 * active or inactive queues.
			 */
d293 4
@


1.99
log
@Add missing call to pmap_update() in km_alloc().

ok deraadt@@, miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.98 2011/04/19 15:59:11 art Exp $	*/
d1006 1
@


1.98
log
@Free the correct pages when we failed to allocate va.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.97 2011/04/18 19:23:46 art Exp $	*/
d979 1
@


1.97
log
@Put back the change of pool and malloc into the new km_alloc(9) api.
The problems during the hackathon were not caused by this (most likely).

prodded by deraadt@@ and beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.96 2011/04/15 23:04:19 deraadt Exp $	*/
d931 2
a932 1
				uvm_pagefree(pg);
d965 2
@


1.96
log
@unused variable on !PMAP_DIRECT
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.95 2011/04/15 21:47:24 oga Exp $	*/
a813 80
#endif

void *
uvm_km_getpage_pla(int flags, int *slowdown, paddr_t low, paddr_t high,
    paddr_t alignment, paddr_t boundary)
{
	struct pglist pgl;
	int pla_flags;
	struct vm_page *pg;
	vaddr_t va;

	*slowdown = 0;
	pla_flags = (flags & UVM_KMF_NOWAIT) ? UVM_PLA_NOWAIT : UVM_PLA_WAITOK;
	if (flags & UVM_KMF_ZERO)
		pla_flags |= UVM_PLA_ZERO;
	TAILQ_INIT(&pgl);
	if (uvm_pglistalloc(PAGE_SIZE, low, high, alignment, boundary, &pgl,
	    1, pla_flags) != 0)
		return NULL;
	pg = TAILQ_FIRST(&pgl);
	KASSERT(pg != NULL && TAILQ_NEXT(pg, pageq) == NULL);
	TAILQ_REMOVE(&pgl, pg, pageq);

#ifdef __HAVE_PMAP_DIRECT
	va = pmap_map_direct(pg);
	if (__predict_false(va == 0))
		uvm_pagefree(pg);

#else	/* !__HAVE_PMAP_DIRECT */
	mtx_enter(&uvm_km_pages.mtx);
	while (uvm_km_pages.free == 0) {
		if (flags & UVM_KMF_NOWAIT) {
			mtx_leave(&uvm_km_pages.mtx);
			uvm_pagefree(pg);
			return NULL;
		}
		msleep(&uvm_km_pages.free, &uvm_km_pages.mtx, PVM, "getpage",
		    0);
	}

	va = uvm_km_pages.page[--uvm_km_pages.free];
	if (uvm_km_pages.free < uvm_km_pages.lowat &&
	    curproc != uvm_km_pages.km_proc) {
		*slowdown = 1;
		wakeup(&uvm_km_pages.km_proc);
	}
	mtx_leave(&uvm_km_pages.mtx);


	atomic_setbits_int(&pg->pg_flags, PG_FAKE);
	UVM_PAGE_OWN(pg, NULL);

	pmap_kenter_pa(va, VM_PAGE_TO_PHYS(pg), UVM_PROT_RW);
	pmap_update(kernel_map->pmap);

#endif	/* !__HAVE_PMAP_DIRECT */
	return ((void *)va);
}

void
uvm_km_putpage(void *v)
{
#ifdef __HAVE_PMAP_DIRECT
	vaddr_t va = (vaddr_t)v;
	struct vm_page *pg;

	pg = pmap_unmap_direct(va);

	uvm_pagefree(pg);
#else	/* !__HAVE_PMAP_DIRECT */
	struct uvm_km_free_page *fp = v;

	mtx_enter(&uvm_km_pages.mtx);
	fp->next = uvm_km_pages.freelist;
	uvm_km_pages.freelist = fp;
	if (uvm_km_pages.freelistlen++ > 16)
		wakeup(&uvm_km_pages.km_proc);
	mtx_leave(&uvm_km_pages.mtx);
#endif	/* !__HAVE_PMAP_DIRECT */
}
a814 1
#ifndef __HAVE_PMAP_DIRECT
d844 2
a845 2
km_alloc(size_t sz, struct kmem_va_mode *kv, struct kmem_pa_mode *kp,
    struct kmem_dyn_mode *kd)
d980 2
a981 1
km_free(void *v, size_t sz, struct kmem_va_mode *kv, struct kmem_pa_mode *kp)
d1036 1
a1036 1
struct kmem_va_mode kv_any = {
d1040 1
a1040 1
struct kmem_va_mode kv_intrsafe = {
d1044 1
a1044 1
struct kmem_va_mode kv_page = {
d1048 1
a1048 1
struct kmem_pa_mode kp_dirty = {
d1052 1
a1052 1
struct kmem_pa_mode kp_dma = {
d1056 1
a1056 1
struct kmem_pa_mode kp_dma_zero = {
d1061 1
a1061 1
struct kmem_pa_mode kp_zero = {
d1066 1
a1066 1
struct kmem_pa_mode kp_pageable = {
d1072 1
a1072 1
struct kmem_pa_mode kp_none = {
d1076 1
a1076 1
struct kmem_dyn_mode kd_waitok = {
d1081 1
a1081 1
struct kmem_dyn_mode kd_nowait = {
d1085 1
a1085 1
struct kmem_dyn_mode kd_trylock = {
@


1.95
log
@move uvm_pageratop from uvm_pager.c local to a general uvm function
(uvm_atopg) and use it in uvm_km_doputpage to replace some handrolled
code. Shrinks the kernel a trivial amount.

ok beck@@ and miod@@ (who suggested i name it uvm_atopg not uvm_atop)
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.94 2011/04/07 15:30:16 miod Exp $	*/
a901 1
	paddr_t pa;
@


1.94
log
@Do not use NULL in integer comparisons. No functional change.
ok matthew@@ tedu@@, also eyeballed by at least krw@@ oga@@ kettenis@@ jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.93 2011/04/06 15:52:13 art Exp $	*/
d905 1
a905 5
	if (!pmap_extract(pmap_kernel(), va, &pa))
		panic("lost pa");
	pg = PHYS_TO_VM_PAGE(pa);

	KASSERT(pg != NULL);
@


1.93
log
@Backout the uvm_km_getpage -> km_alloc conversion. Weird things are happening
and we aren't sure what's causing them.

shouted oks by many before I even built a kernel with the diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.92 2011/04/05 01:28:05 art Exp $	*/
d551 1
a551 1
				return (NULL);
d740 1
a740 1
		if (uvm_km_pages.page[i] == NULL)
d745 1
a745 1
		uvm_km_pages.page[i] = NULL;
@


1.92
log
@ - Change pool constraints to use kmem_pa_mode instead of uvm_constraint_range
 - Use km_alloc for all backend allocations in pools.
 - Use km_alloc for the emergmency kentry allocations in uvm_mapent_alloc
 - Garbage collect uvm_km_getpage, uvm_km_getpage_pla and uvm_km_putpage

ariane@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.91 2011/04/04 21:16:31 art Exp $	*/
d814 1
d816 80
@


1.91
log
@Few minor ninja fixes while this isn't being used anywhere in -current.
 - Change a few KASSERT(0) into proper panics.
 - Match the new behavior of single page freeing.
 - kremove pages and then free them, it's safer.

thib@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.90 2011/04/04 12:25:23 art Exp $	*/
a813 1
#endif
a814 80
void *
uvm_km_getpage_pla(int flags, int *slowdown, paddr_t low, paddr_t high,
    paddr_t alignment, paddr_t boundary)
{
	struct pglist pgl;
	int pla_flags;
	struct vm_page *pg;
	vaddr_t va;

	*slowdown = 0;
	pla_flags = (flags & UVM_KMF_NOWAIT) ? UVM_PLA_NOWAIT : UVM_PLA_WAITOK;
	if (flags & UVM_KMF_ZERO)
		pla_flags |= UVM_PLA_ZERO;
	TAILQ_INIT(&pgl);
	if (uvm_pglistalloc(PAGE_SIZE, low, high, alignment, boundary, &pgl,
	    1, pla_flags) != 0)
		return NULL;
	pg = TAILQ_FIRST(&pgl);
	KASSERT(pg != NULL && TAILQ_NEXT(pg, pageq) == NULL);
	TAILQ_REMOVE(&pgl, pg, pageq);

#ifdef __HAVE_PMAP_DIRECT
	va = pmap_map_direct(pg);
	if (__predict_false(va == 0))
		uvm_pagefree(pg);

#else	/* !__HAVE_PMAP_DIRECT */
	mtx_enter(&uvm_km_pages.mtx);
	while (uvm_km_pages.free == 0) {
		if (flags & UVM_KMF_NOWAIT) {
			mtx_leave(&uvm_km_pages.mtx);
			uvm_pagefree(pg);
			return NULL;
		}
		msleep(&uvm_km_pages.free, &uvm_km_pages.mtx, PVM, "getpage",
		    0);
	}

	va = uvm_km_pages.page[--uvm_km_pages.free];
	if (uvm_km_pages.free < uvm_km_pages.lowat &&
	    curproc != uvm_km_pages.km_proc) {
		*slowdown = 1;
		wakeup(&uvm_km_pages.km_proc);
	}
	mtx_leave(&uvm_km_pages.mtx);


	atomic_setbits_int(&pg->pg_flags, PG_FAKE);
	UVM_PAGE_OWN(pg, NULL);

	pmap_kenter_pa(va, VM_PAGE_TO_PHYS(pg), UVM_PROT_RW);
	pmap_update(kernel_map->pmap);

#endif	/* !__HAVE_PMAP_DIRECT */
	return ((void *)va);
}

void
uvm_km_putpage(void *v)
{
#ifdef __HAVE_PMAP_DIRECT
	vaddr_t va = (vaddr_t)v;
	struct vm_page *pg;

	pg = pmap_unmap_direct(va);

	uvm_pagefree(pg);
#else	/* !__HAVE_PMAP_DIRECT */
	struct uvm_km_free_page *fp = v;

	mtx_enter(&uvm_km_pages.mtx);
	fp->next = uvm_km_pages.freelist;
	uvm_km_pages.freelist = fp;
	if (uvm_km_pages.freelistlen++ > 16)
		wakeup(&uvm_km_pages.km_proc);
	mtx_leave(&uvm_km_pages.mtx);
#endif	/* !__HAVE_PMAP_DIRECT */
}

#ifndef __HAVE_PMAP_DIRECT
@


1.90
log
@Better.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.89 2011/04/04 12:22:59 art Exp $	*/
d1011 1
a1011 1
		KASSERT(0);
d1079 1
a1080 1
	if (kv->kv_singlepage) {
d1083 8
a1092 1
#endif
d1096 1
d1106 2
a1107 2
			if (!pg) {
				printf("pa: 0x%lx\n", pa);
d1109 1
a1109 2
			KASSERT(pg);
			uvm_pagefree(pg);
d1112 2
a1114 1
	pmap_update(pmap_kernel());
d1116 3
a1118 16
	if (kv->kv_singlepage) {
#ifdef __HAVE_PMAP_DIRECT
		KASSERT(0);
#else
		mtx_enter(&uvm_km_pages.mtx);
		if (uvm_km_pages.free < uvm_km_pages.hiwat)
			uvm_km_pages.page[uvm_km_pages.free++] = va;
		else    
			uvm_unmap(kernel_map, va, eva);
		mtx_leave(&uvm_km_pages.mtx);
#endif
	} else {
		uvm_unmap(*kv->kv_map, sva, eva);
		if (kv->kv_wait)
			wakeup(*kv->kv_map);
	}
@


1.89
log
@Make gcc stop whining. pointed out by ariane@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.88 2011/04/04 11:56:12 art Exp $	*/
d939 3
a941 1
	paddr_t pa = 0;
@


1.88
log
@Some minor fixes:
- Clarify a comment.
- Change all the flags to chars from ints to make the structs smaller.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.87 2011/04/04 11:24:45 art Exp $	*/
d939 1
a939 1
	paddr_t pa;
@


1.87
log
@New unified allocator of kernel memory.

We've reached the point where we have a dozen allocators that all do more
or less the same thing, but slightly different, with slightly different
semantics, slightly different default behaviors and default behaviors that
most callers don't expect or want. A random sample on the last general
hackathon showed that no one could explain what all the different allocators
did. And every time someone needed to do something slightly different a
new allocator was written.

Unify everything. One single function to allocate multiples of PAGE_SIZE
kernel memory. Four arguments: size, how va is allocated, how pa is allocated
and misc parameters. Same parameters are passed to the free function so that
we don't need to guess what's going on.

Functions are currently unused, we'll do one thing at a time to avoid a
giant commit.

looked at by lots of people, deraadt@@ and beck@@ are yelling at me to commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.86 2010/08/26 16:08:24 thib Exp $	*/
d963 6
a968 1
#if 1	/* For now, because I'm lazy in free */
@


1.86
log
@make the comment explaining the kernel submaps a bit better.

ok art@@, oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.85 2010/07/22 17:31:39 thib Exp $	*/
d928 244
@


1.85
log
@Remove the VM_KMPAGESFREE sysctl. After the pmemrange
changes it was returing a constant 0, changing to cope
with those changes makes less sense then just removing
as it provides the user with no usefull information.

sthen@@ grepped the port's tree for me and found not hits,
thanks!

OK deraadt@@, matthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.84 2010/07/15 00:14:17 tedu Exp $	*/
d92 9
a100 8
 * the vm system has several standard kernel submaps, including:
 *   kmem_map => contains only wired kernel memory for the kernel
 *		malloc.   *** access to kmem_map must be protected
 *		by splvm() because we are allowed to call malloc()
 *		at interrupt time ***
 *   pager_map => used to map "buf" structures into kernel space
 *   exec_map => used during exec to handle exec args
 *   etc...
@


1.84
log
@the uvm_km_putpage is calling into tangly uvm guts again on not pmap direct.
go back to something more like the previous design, and have the thread do
the heavy lifting.  solves vmmaplk panics.
ok deraadt oga thib
[and even simple diffs are hard to get perfect. help from mdempsky and deraadt]
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.83 2010/07/02 23:12:38 thib Exp $	*/
a677 2

int uvm_km_pages_free; /* number of pages currently on free list */
@


1.83
log
@no need to call uvm_km_free_wakup for the kernel map, uvm_km_free is
enough.

ok tedu@@, art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.82 2010/07/02 01:25:06 art Exp $	*/
d710 1
d772 2
d777 2
a778 1
		if (uvm_km_pages.free >= uvm_km_pages.lowat) {
d782 4
d788 21
a808 3
		for (i = 0; i < nitems(pg); i++) {
			pg[i] = (vaddr_t)uvm_km_kmemalloc(kernel_map, NULL,
			    PAGE_SIZE, UVM_KMF_VALLOC);
d810 2
a811 7

		mtx_enter(&uvm_km_pages.mtx);
		for (i = 0; i < nitems(pg); i++) {
			if (uvm_km_pages.free == nitems(uvm_km_pages.page))
				break;
			else
				uvm_km_pages.page[uvm_km_pages.free++] = pg[i];
a812 7

		wakeup(&uvm_km_pages.free);
		mtx_leave(&uvm_km_pages.mtx);

		/* Cleanup left-over pages (if any). */
		for (; i < nitems(pg); i++)
			uvm_km_free(kernel_map, pg[i], PAGE_SIZE);
d877 1
a880 1
#ifdef __HAVE_PMAP_DIRECT
d882 2
d885 17
d904 2
a923 1
#endif	/* !__HAVE_PMAP_DIRECT */
d926 1
d928 1
@


1.82
log
@add an align argument to uvm_km_kmemalloc_pla.

Use uvm_km_kmemalloc_pla with the dma constraint to allocate kernel stacks.

Yes, that means DMA is possible to kernel stacks, but only until we've fixed
all the scary drivers.

deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.81 2010/07/02 01:13:59 thib Exp $	*/
d798 1
a798 1
			uvm_km_free_wakeup(kernel_map, pg[i], PAGE_SIZE);
d888 1
a888 1
		uvm_km_free_wakeup(kernel_map, va, PAGE_SIZE);
@


1.81
log
@Drop the uvm_km_pages.mtx mutex in uvm_km_putpage before we free va's,
as calls to uvm_km_free_wakup can end up in uvm_mapent_alloc which tries
to grab this mutex.

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.79 2010/06/28 04:20:29 miod Exp $	*/
d349 2
a350 2
    int flags, paddr_t low, paddr_t high, paddr_t alignment, paddr_t boundary,
    int nsegs)
d380 1
a380 1
	      0, UVM_MAPFLAG(UVM_PROT_RW, UVM_PROT_RW, UVM_INH_NONE,
@


1.80
log
@Add a no_constraint uvm_constraint_range; use it in the pool code.

ok tedu@@, beck@@, oga@@
@
text
@d869 1
d881 1
a881 1
	if (uvm_km_pages.free < uvm_km_pages.hiwat)
d883 5
a887 1
	else
a888 1
	mtx_leave(&uvm_km_pages.mtx);
@


1.79
log
@Move uvm_km_pages struct declaration and watermark bounds to uvm_km.h, so
that md code can peek at it, and update m68k !__HAVE_PMAP_DIRECT setup code
to the recent uvm_km changes.
ok thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.78 2010/06/27 17:45:20 thib Exp $	*/
d148 3
@


1.78
log
@doh! Use pmap_kenter/pmap_kremove in the backend page allocator to prevent
recursion in pmap_enter as seen on zaurus.
ok art@@

also, release a the uvm_km_page.mtx before calling uvm_km_kmemalloc as we
can sleep there.
ok oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.77 2010/06/27 03:03:49 thib Exp $	*/
a701 17

#define UVM_KM_PAGES_LOWAT_MAX	(2048)
#define UVM_KM_PAGES_HIWAT_MAX	(4 * UVM_KM_PAGES_LOWAT_MAX)

struct uvm_km_pages {
	struct	mutex mtx;

	/* Low and high water mark for addresses. */
	int	lowat;
	int	hiwat;

	/* Kernel address pool. */
	int	free;
	vaddr_t	page[UVM_KM_PAGES_HIWAT_MAX];

	struct	proc *km_proc;
};
@


1.77
log
@uvm constraints. Add two mandatory MD symbols, uvm_md_constraints
which contains the constraints for DMA/memory allocation for each
architecture, and dma_constraints which contains the range of addresses
that are dma accessable by the system.

This is based on ariane@@'s physcontig diff, with lots of bugfixes and
additions the following additions by my self:

Introduce a new function pool_set_constraints() which sets the address
range for which we allocate pages for the pool from, this is now used
for the mbuf/mbuf cluster pools to keep them dma accessible.

The !direct archs no longer stuff pages into the kernel object in
uvm_km_getpage_pla but rather do a pmap_extract() in uvm_km_putpages.

Tested heavily by my self on i386, amd64 and sparc64. Some tests on
alpha and SGI.

"commit it" beck, art, oga, deraadt
"i like the diff" deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.75 2009/07/25 12:55:40 miod Exp $	*/
d792 1
d867 1
a867 2
	pmap_enter(kernel_map->pmap, va, VM_PAGE_TO_PHYS(pg), UVM_PROT_RW,
	    PMAP_WIRED | VM_PROT_READ | VM_PROT_WRITE);
d890 1
a890 1
	pmap_remove(kernel_map->pmap, va, va + PAGE_SIZE);
@


1.76
log
@introduce a uvm_km_valloc_try function that won't get a lower level lock
for use by the uvm pseg code.  this is the path of least resistance until
we sort out how many of these functions we really need.  problem found by mikeb
ok kettenis oga
@
text
@a328 1

d339 3
d345 3
a347 2
uvm_km_kmemalloc(struct vm_map *map, struct uvm_object *obj, vsize_t size,
    int flags)
d352 2
d359 3
d369 2
d407 16
d426 6
a431 19
		pg = uvm_pagealloc(obj, offset, NULL, 0);
		if (pg) {
			atomic_clearbits_int(&pg->pg_flags, PG_BUSY);
			UVM_PAGE_OWN(pg, NULL);
		}
		
		if (__predict_false(pg == NULL)) {
			if ((flags & UVM_KMF_NOWAIT) ||
			    ((flags & UVM_KMF_CANFAIL) &&
			    uvmexp.swpgonly == uvmexp.swpages)) {
				/* free everything! */
				uvm_unmap(map, kva, kva + size);
				return (0);
			} else {
				uvm_wait("km_getwait2");	/* sleep here */
				continue;
			}
		}
		
d448 1
d683 1
a683 1
 * and find and return its VA.  We use the poolpage functions for this.
a690 28
void *
uvm_km_getpage(boolean_t waitok, int *slowdown)
{
	struct vm_page *pg;
	vaddr_t va;

	*slowdown = 0;
 again:
	pg = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_USERESERVE);
	if (__predict_false(pg == NULL)) {
		if (waitok) {
			uvm_wait("plpg");
			goto again;
		} else
			return (NULL);
	}
	va = pmap_map_direct(pg);
	if (__predict_false(va == 0))
		uvm_pagefree(pg);
	return ((void *)va);
}

void
uvm_km_putpage(void *v)
{
	uvm_pagefree(pmap_unmap_direct((vaddr_t)v));
}

d703 16
a718 5
struct mutex uvm_km_mtx;
int uvm_km_pages_lowat; /* allocate more when reserve drops below this */
struct km_page {
	struct km_page *next;
} *uvm_km_pages_head;
d720 1
a720 1
struct proc *uvm_km_proc;
a733 1
	struct km_page *page;
d737 2
a738 2
	mtx_init(&uvm_km_mtx, IPL_VM);
	if (!uvm_km_pages_lowat) {
d740 1
a740 3
		uvm_km_pages_lowat = physmem / 256;
		if (uvm_km_pages_lowat > 2048)
			uvm_km_pages_lowat = 2048;
d742 2
a743 2
		if (uvm_km_pages_lowat < lowat_min)
			uvm_km_pages_lowat = lowat_min;
d745 11
a755 5

	for (i = 0; i < uvm_km_pages_lowat * 4; i++) {
		page = (void *)uvm_km_alloc(kernel_map, PAGE_SIZE);
		page->next = uvm_km_pages_head;
		uvm_km_pages_head = page;
d757 3
a759 1
	uvm_km_pages_free = i;
d762 2
a763 2
	if (uvm_km_pages_lowat > 512)
		uvm_km_pages_lowat = 512;
d771 1
a771 1
	kthread_create(uvm_km_thread, NULL, &uvm_km_proc, "kmthread");
d783 2
a784 2
	struct km_page *head, *tail, *page;
	int i, want;
d786 15
a800 8
	for (i = want = 16; ; ) {
		if (i < want || uvm_km_pages_free >= uvm_km_pages_lowat)
			tsleep(&uvm_km_pages_head, PVM, "kmalloc", 0);
		for (i = 0; i < want; i++) {
			page = (void *)uvm_km_alloc(kernel_map, PAGE_SIZE);
			if (i == 0)
				head = tail = page;
			if (page == NULL)
d802 2
a803 2
			page->next = head;
			head = page;
d805 7
a811 9
		if (head != NULL) {
			mtx_enter(&uvm_km_mtx);
			tail->next = uvm_km_pages_head;
			uvm_km_pages_head = head;
			uvm_km_pages_free += i;
			mtx_leave(&uvm_km_mtx);
		}
		if (uvm_km_pages_free)
			wakeup(&uvm_km_pages_free);
d814 1
a815 5

/*
 * Allocate one page.  We can sleep for more if the caller
 * permits it.  Wake up the thread if we've dropped below lowat.
 */
d817 2
a818 1
uvm_km_getpage(boolean_t waitok, int *slowdown)
d820 4
a823 1
	struct km_page *page = NULL;
d826 23
a848 7
	mtx_enter(&uvm_km_mtx);
	for (;;) {
		page = uvm_km_pages_head;
		if (page) {
			uvm_km_pages_head = page->next;
			uvm_km_pages_free--;
			break;
d850 2
a851 3
		if (!waitok)
			break;
		msleep(&uvm_km_pages_free, &uvm_km_mtx, PVM, "getpage", 0);
d853 6
a858 5
	mtx_leave(&uvm_km_mtx);
	if (uvm_km_pages_free < uvm_km_pages_lowat) {
		if (curproc != uvm_km_proc)
			*slowdown = 1;
		wakeup(&uvm_km_pages_head);
d860 12
a871 1
	return (page);
d877 23
a899 1
	struct km_page *page = v;
d901 1
a901 5
	mtx_enter(&uvm_km_mtx);
	page->next = uvm_km_pages_head;
	uvm_km_pages_head = page;
	uvm_km_pages_free++;
	mtx_leave(&uvm_km_mtx);
a902 1
#endif
@


1.75
log
@Add an extra argument to uvm_unmap_remove(), for the caller to tell it
whether removing holes or parts of them is allowed or not.
Only allow hole removal in uvmspace_free(), when tearing the vmspace down.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.74 2009/07/22 21:05:37 oga Exp $	*/
d574 1
a574 1
	return(uvm_km_valloc_align(map, size, 0));
d578 7
a584 1
uvm_km_valloc_align(struct vm_map *map, vsize_t size, vsize_t align)
d601 1
a601 1
	    UVM_INH_NONE, UVM_ADV_RANDOM, 0)) != 0)) {
@


1.74
log
@Put the PG_RELEASED changes diff back in.

This has has been tested very very thoroughly on all archs we have
excepting 88k and 68k. Please see cvs log for the individual commit
messages.

ok beck@@, thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.73 2009/06/17 00:13:59 oga Exp $	*/
d465 1
a465 1
			 &dead_entries, NULL);
@


1.73
log
@date based reversion of uvm to the 4th May.

More backouts in line with previous ones, this appears to bring us back to a
stable condition.

A machine forced to 64mb of ram cycled 10GB through swap with this diff
and is still running as I type this. Other tests by ariane@@ and thib@@
also seem to show that it's alright.

ok deraadt@@, thib@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.70 2009/02/22 19:59:01 miod Exp $	*/
d279 6
a284 2
			/* owner must check for this when done */
			atomic_setbits_int(&pp->pg_flags, PG_RELEASED);
a517 15
		pg = uvm_pagelookup(uvm.kernel_object, offset);

		/*
		 * if we found a page in an unallocated region, it must be
		 * released
		 */
		if (pg) {
			if ((pg->pg_flags & PG_RELEASED) == 0)
				panic("uvm_km_alloc1: non-released page");
			atomic_setbits_int(&pg->pg_flags, PG_WANTED);
			UVM_UNLOCK_AND_WAIT(pg, &uvm.kernel_object->vmobjlock,
			    FALSE, "km_alloc", 0);
			continue;   /* retry */
		}
		
@


1.72
log
@Backout all the PG_RELEASED changes.

This is for the same reason as the earlier backouts, to avoid the bug
either added or exposed sometime around c2k9. This *should* be the last
one.

prompted by deraadt@@

ok ariane@@
@
text
@@


1.71
log
@Second step of PG_RELEASED cleanup.

uvm_km deals with kernel memory which is either part of one of the
kernel maps, or the main kernel object (a uao). If on km_pgremove we hit
a busy page, just sleep on it, if so there's some async io (and that is
unlikely). we can remove the check for uvm_km_alloc1() for a released page
since now we will never end up with a removed but released page in the kernel
map (due to the other chunk and the last diff).

ok ariane@@. Diff survived several make builds, on amd64 and sparc64,
also forced paging with ariane's evil program.
@
text
@d279 2
a280 6
			atomic_setbits_int(&pp->pg_flags, PG_WANTED);
			UVM_UNLOCK_AND_WAIT(pp, &uobj->vmobjlock, 0,
			    "km_pgrm", 0);
			simple_lock(&uobj->vmobjlock);
			curoff -= PAGE_SIZE; /* loop back to us */
			continue;
d514 15
@


1.70
log
@On machines with less than 16MB of physical memory, reduce the lower bound
of uvm_km_pages.

ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.69 2009/02/11 11:09:36 mikeb Exp $	*/
d279 6
a284 2
			/* owner must check for this when done */
			atomic_setbits_int(&pp->pg_flags, PG_RELEASED);
a517 15
		pg = uvm_pagelookup(uvm.kernel_object, offset);

		/*
		 * if we found a page in an unallocated region, it must be
		 * released
		 */
		if (pg) {
			if ((pg->pg_flags & PG_RELEASED) == 0)
				panic("uvm_km_alloc1: non-released page");
			atomic_setbits_int(&pg->pg_flags, PG_WANTED);
			UVM_UNLOCK_AND_WAIT(pg, &uvm.kernel_object->vmobjlock,
			    FALSE, "km_alloc", 0);
			continue;   /* retry */
		}
		
@


1.69
log
@Remove uvm_km_alloc_poolpage1 as it serves no particular purpose
now and valid for __HAVE_PMAP_DIRECT archs only, though implements
both code paths.

Put it's code directly into the uvm_km_getpage for PMAP_DIRECT archs.

No functional change.

ok tedu, art
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.68 2008/10/23 23:54:02 tedu Exp $	*/
d743 1
d752 3
a754 2
		if (uvm_km_pages_lowat < 128)
			uvm_km_pages_lowat = 128;
@


1.68
log
@a better fix for the "uvm_km thread runs out of memory" problem.

add a new arg to the backend so it can tell pool to slow down.  when we get
this flag, yield *after* putting the page in the pool's free list.  whatever
we do, don't let the thread sleep.

this makes things better by still letting the thread run when a huge pf
request comes in, but without artificially increasing pressure on the backend
by eating pages without feeding them forward.

ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.67 2008/06/14 03:48:32 art Exp $	*/
a666 80
/*
 * uvm_km_alloc_poolpage: allocate a page for the pool allocator
 *
 * => if the pmap specifies an alternate mapping method, we use it.
 */

/* ARGSUSED */
vaddr_t
uvm_km_alloc_poolpage1(struct vm_map *map, struct uvm_object *obj,
    boolean_t waitok)
{
#if defined(__HAVE_PMAP_DIRECT)
	struct vm_page *pg;
	vaddr_t va;

 again:
	pg = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_USERESERVE);
	if (__predict_false(pg == NULL)) {
		if (waitok) {
			uvm_wait("plpg");
			goto again;
		} else
			return (0);
	}
	va = pmap_map_direct(pg);
	if (__predict_false(va == 0))
		uvm_pagefree(pg);
	return (va);
#else
	vaddr_t va;
	int s;

	/*
	 * NOTE: We may be called with a map that doesn't require splvm
	 * protection (e.g. kernel_map).  However, it does not hurt to
	 * go to splvm in this case (since unprotected maps will never be
	 * accessed in interrupt context).
	 *
	 * XXX We may want to consider changing the interface to this
	 * XXX function.
	 */

	s = splvm();
	va = uvm_km_kmemalloc(map, obj, PAGE_SIZE, waitok ? 0 : UVM_KMF_NOWAIT);
	splx(s);
	return (va);
#endif /* __HAVE_PMAP_DIRECT */
}

/*
 * uvm_km_free_poolpage: free a previously allocated pool page
 *
 * => if the pmap specifies an alternate unmapping method, we use it.
 */

/* ARGSUSED */
void
uvm_km_free_poolpage1(struct vm_map *map, vaddr_t addr)
{
#if defined(__HAVE_PMAP_DIRECT)
	uvm_pagefree(pmap_unmap_direct(addr));
#else
	int s;

	/*
	 * NOTE: We may be called with a map that doesn't require splvm
	 * protection (e.g. kernel_map).  However, it does not hurt to
	 * go to splvm in this case (since unprocted maps will never be
	 * accessed in interrupt context).
	 *
	 * XXX We may want to consider changing the interface to this
	 * XXX function.
	 */

	s = splvm();
	uvm_km_free(map, addr, PAGE_SIZE);
	splx(s);
#endif /* __HAVE_PMAP_DIRECT */
}

d685 2
d689 13
a701 1
	return ((void *)uvm_km_alloc_poolpage1(NULL, NULL, waitok));
d707 1
a707 2

	uvm_km_free_poolpage1(NULL, (vaddr_t)v);
@


1.67
log
@If we have one syscall that consumes large amounts of memory (like for
example an ioctl that loads bazillions of entries into a pf table) it
would exhaust the pool of free pages and not let uvm_km_thread catch
up until the pool was actually empty. This could be bad for non-sleeping
allocators since they can't wait for the memory while the big hog
can.

Instead of letting the syscall exhaust the pool, detect when we fall below
the low watermark, wake the thread, sleep once and let the thread
catch up. This paces the huge consumer so that the more critical consumers
never find an exhausted pool of pages.

"seems reasonable" kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.66 2007/12/15 03:42:57 deraadt Exp $	*/
d763 1
a763 1
uvm_km_getpage(boolean_t waitok)
d766 1
d795 2
d839 1
a839 1
	kthread_create(uvm_km_thread, NULL, NULL, "kmthread");
d884 1
a884 1
uvm_km_getpage(boolean_t waitok)
d888 1
d901 1
d903 2
a905 12

		/*
		 * If we're below the low watermark and are allowed to
		 * sleep, we should slow down our allocations a bit
		 * to not exhaust the reserve of pages for nosleep
		 * allocators.
		 *
		 * Just sleep once.
		 */
		if (waitok)
			msleep(&uvm_km_pages_free, &uvm_km_mtx, PPAUSE,
			    "getpg2", 0);
a906 1
	mtx_leave(&uvm_km_mtx);
@


1.66
log
@export kernel uvm_km_pages_free as vm.kmpagesfree; ok tedu, tested jsg
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.65 2007/12/11 15:05:45 tedu Exp $	*/
d897 15
a912 2
	if (uvm_km_pages_free < uvm_km_pages_lowat)
		wakeup(&uvm_km_pages_head);
@


1.65
log
@use a mutex for protection of the uvm_km list.  ok art
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.64 2007/08/03 22:49:07 art Exp $	*/
d747 2
a789 1
int uvm_km_pages_free; /* number of pages currently on free list */
@


1.64
log
@Don't let pagedaemon wait for pages here. We could trigger this easily
when we hit swap before actually fully populating the buffer cache which
would lead to deadlocks.

From pedro, tested by many, deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.63 2007/04/29 15:46:42 art Exp $	*/
d786 1
d808 1
d848 1
a848 1
	int i, s, want;
d863 1
a863 1
			s = splvm();
d867 1
a867 1
			splx(s);
a882 1
	int s;
d884 1
a884 1
	s = splvm();
d894 1
a894 1
		tsleep(&uvm_km_pages_free, PVM, "getpage", 0);
d896 1
a896 1
	splx(s);
a905 1
	int s;
d907 1
a907 1
	s = splvm();
d911 1
a911 1
	splx(s);
@


1.63
log
@Change the loop test in uvm_km_kmemalloc from '<' to '!='. Everything
is aligned just fine and in case we allocate the last piece of the
address space we don't want wrap-around to cause us to fail.

pointed out by and ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.62 2007/04/27 07:45:30 art Exp $	*/
d537 12
a548 2
			uvm_wait("km_alloc1w");	/* wait for memory */
			continue;
d550 1
a550 1
		
@


1.62
log
@Use the right size when we're backing out the allocation in
uvm_km_kmemalloc.

"should probbaly go in" millert@@, "I think it should too" deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.61 2007/04/15 12:54:08 art Exp $	*/
d395 1
a395 1
	while (loopva < kva + size) {
@


1.61
log
@One more voff_t in the right place.
miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.60 2007/04/15 11:29:33 art Exp $	*/
d317 1
a317 1
			continue;			/* panic? */
d395 1
a395 1
	while (size) {
a429 1
		size -= PAGE_SIZE;
@


1.60
log
@Use the right types for calculating the object offset.
miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.59 2007/04/15 11:23:16 art Exp $	*/
d265 1
a265 1
	vaddr_t curoff;
@


1.59
log
@Clean up prototypes, change vm_map_t to struct vm_map *.

miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.58 2007/04/13 18:57:49 art Exp $	*/
d343 1
a343 1
	vaddr_t offset;
d479 2
a480 1
	vaddr_t kva, loopva, offset;
@


1.58
log
@While splitting flags and pqflags might have been a good idea in theory
to separate locking, on most modern machines this is not enough
since operations on short types touch other short types that share the
same word in memory.

Merge pg_flags and pqflags again and now use atomic operations to change
the flags. Also bump wire_count to an int and pg_version might go
int as well, just for alignment.

tested by many, many. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.57 2007/04/11 12:10:42 art Exp $	*/
d147 1
a147 1
vm_map_t kernel_map = NULL;
d165 1
a165 2
uvm_km_init(start, end)
	vaddr_t start, end;
d208 2
a209 7
uvm_km_suballoc(map, min, max, size, flags, fixed, submap)
	struct vm_map *map;
	vaddr_t *min, *max;		/* OUT, OUT */
	vsize_t size;
	int flags;
	boolean_t fixed;
	struct vm_map *submap;
d339 2
a340 5
uvm_km_kmemalloc(map, obj, size, flags)
	vm_map_t map;
	struct uvm_object *obj;
	vsize_t size;
	int flags;
d456 1
a456 4
uvm_km_free_wakeup(map, addr, size)
	vm_map_t map;
	vaddr_t addr;
	vsize_t size;
d458 1
a458 1
	vm_map_entry_t dead_entries;
d573 1
a573 3
uvm_km_valloc(map, size)
	vm_map_t map;
	vsize_t size;
d579 1
a579 4
uvm_km_valloc_align(map, size, align)
	vm_map_t map;
	vsize_t size;
	vsize_t align;
d614 1
a614 4
uvm_km_valloc_prefer_wait(map, size, prefer)
	vm_map_t map;
	vsize_t size;
	voff_t prefer;
d652 1
a652 3
uvm_km_valloc_wait(map, size)
	vm_map_t map;
	vsize_t size;
d665 2
a666 4
uvm_km_alloc_poolpage1(map, obj, waitok)
	vm_map_t map;
	struct uvm_object *obj;
	boolean_t waitok;
d714 1
a714 3
uvm_km_free_poolpage1(map, addr)
	vm_map_t map;
	vaddr_t addr;
@


1.57
log
@Instead of managing pages for intrsafe maps in special objects (aka.
kmem_object) just so that we can remove them, just use pmap_extract
to get the pages to free and simplify a lot of code to not deal with
the list of intrsafe maps, intrsafe objects, etc.

miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.56 2007/04/04 17:44:45 art Exp $	*/
d286 1
a286 1
			pp->pg_flags |= PG_RELEASED;
d407 1
a407 1
			pg->pg_flags &= ~PG_BUSY;	/* new page */
d535 1
a535 1
			pg->pg_flags |= PG_WANTED;
d544 1
a544 1
			pg->pg_flags &= ~PG_BUSY;	/* new page */
@


1.56
log
@Mechanically rename the "flags" and "version" fields in struct vm_page
to "pg_flags" and "pg_version", so that they are a bit easier to work with.
Whoever uses generic names like this for a popular struct obviously doesn't
read much code.

Most architectures compile and there are no functionality changes.

deraadt@@ ok ("if something fails to compile, we fix that by hand")
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.55 2007/03/25 11:31:07 art Exp $	*/
d110 1
a110 2
 * by splvm().    each of these submaps has their own private kernel 
 * object (e.g. kmem_object).
a128 6
 * note that the offsets in kmem_object also follow this rule.
 * this means that the offsets for kmem_object must fall in the
 * range of [vm_map_min(kmem_object) - vm_map_min(kernel_map)] to
 * [vm_map_max(kmem_object) - vm_map_min(kernel_map)], so the offsets
 * in those objects will typically not start at zero.
 *
a148 3
struct vmi_list vmi_list;
simple_lock_data_t vmi_list_slock;

a153 8
static struct uvm_object	kmem_object_store;

/*
 * All pager operations here are NULL, but the object must have
 * a pager ops vector associated with it; various places assume
 * it to be so.
 */
static struct uvm_pagerops	km_pager;
a170 6
	 * first, initialize the interrupt-safe map list.
	 */
	LIST_INIT(&vmi_list);
	simple_lock_init(&vmi_list_slock);

	/*
a179 13
	 * kmem_object: for use by the kernel malloc().  Memory is always
	 * wired, and this object (and the kmem_map) can be accessed at
	 * interrupt time.
	 */
	simple_lock_init(&kmem_object_store.vmobjlock);
	kmem_object_store.pgops = &km_pager;
	TAILQ_INIT(&kmem_object_store.memq);
	kmem_object_store.uo_npages = 0;
	/* we are special.  we never die */
	kmem_object_store.uo_refs = UVM_OBJ_KERN_INTRSAFE; 
	uvmexp.kmem_object = &kmem_object_store;

	/*
a266 3

#define UKM_HASH_PENALTY 4      /* a guess */

d268 1
a268 3
uvm_km_pgremove(uobj, start, end)
	struct uvm_object *uobj;
	vaddr_t start, end;
d270 1
a270 2
	boolean_t by_list;
	struct vm_page *pp, *ppnext;
a274 10
	simple_lock(&uobj->vmobjlock);

	/* choose cheapest traversal */
	by_list = (uobj->uo_npages <=
	     ((end - start) >> PAGE_SHIFT) * UKM_HASH_PENALTY);
 
	if (by_list)
		goto loop_by_list;

	/* by hash */
a283 1
		/* now do the actual work */
a299 31
	simple_unlock(&uobj->vmobjlock);
	return;

loop_by_list:

	for (pp = TAILQ_FIRST(&uobj->memq); pp != NULL; pp = ppnext) {
		ppnext = TAILQ_NEXT(pp, listq);
		if (pp->offset < start || pp->offset >= end) {
			continue;
		}

		UVMHIST_LOG(maphist,"  page %p, busy=%ld", pp,
		    pp->pg_flags & PG_BUSY, 0, 0);

		if (pp->pg_flags & PG_BUSY) {
			/* owner must check for this when done */
			pp->pg_flags |= PG_RELEASED;
		} else {
			/* free the swap slot... */
			uao_dropswap(uobj, pp->offset >> PAGE_SHIFT);

			/*
			 * ...and free the page; note it may be on the
			 * active or inactive queues.
			 */
			uvm_lock_pageq();
			uvm_pagefree(pp);
			uvm_unlock_pageq();
		}
	}
	simple_unlock(&uobj->vmobjlock);
d315 1
a315 3
uvm_km_pgremove_intrsafe(uobj, start, end)
	struct uvm_object *uobj;
	vaddr_t start, end;
d317 3
a319 40
	boolean_t by_list;
	struct vm_page *pp, *ppnext;
	vaddr_t curoff;
	UVMHIST_FUNC("uvm_km_pgremove_intrsafe"); UVMHIST_CALLED(maphist);

	KASSERT(UVM_OBJ_IS_INTRSAFE_OBJECT(uobj));
	simple_lock(&uobj->vmobjlock);		/* lock object */

	/* choose cheapest traversal */
	by_list = (uobj->uo_npages <=
	     ((end - start) >> PAGE_SHIFT) * UKM_HASH_PENALTY);
 
	if (by_list)
		goto loop_by_list;

	/* by hash */

	for (curoff = start ; curoff < end ; curoff += PAGE_SIZE) {
		pp = uvm_pagelookup(uobj, curoff);
		if (pp == NULL) {
			continue;
		}

		UVMHIST_LOG(maphist,"  page %p, busy=%ld", pp,
		    pp->pg_flags & PG_BUSY, 0, 0);
		KASSERT((pp->pg_flags & PG_BUSY) == 0);
		KASSERT((pp->pqflags & PQ_ACTIVE) == 0);
		KASSERT((pp->pqflags & PQ_INACTIVE) == 0);
		uvm_pagefree(pp);
	}
	simple_unlock(&uobj->vmobjlock);
	return;

loop_by_list:

	for (pp = TAILQ_FIRST(&uobj->memq); pp != NULL; pp = ppnext) {
		ppnext = TAILQ_NEXT(pp, listq);
		if (pp->offset < start || pp->offset >= end) {
			continue;
		}
d321 7
a327 6
		UVMHIST_LOG(maphist,"  page %p, busy=%ld", pp,
		    pp->flags & PG_BUSY, 0, 0);
		KASSERT((pp->pg_flags & PG_BUSY) == 0);
		KASSERT((pp->pqflags & PQ_ACTIVE) == 0);
		KASSERT((pp->pqflags & PQ_INACTIVE) == 0);
		uvm_pagefree(pp);
a328 1
	simple_unlock(&uobj->vmobjlock);
d391 5
a395 1
	offset = kva - vm_map_min(kernel_map);
a404 1
		simple_lock(&obj->vmobjlock);
a409 1
		simple_unlock(&obj->vmobjlock);
a410 4
		/*
		 * out of memory?
		 */

d426 1
a426 3
		 * object unlocked in case we are kmem_map/kmem_object
		 * (because if pmap_enter wants to allocate out of kmem_object
		 * it will need to lock it itself!)
d429 1
a429 1
		if (UVM_OBJ_IS_INTRSAFE_OBJECT(obj)) {
d452 1
a452 4
uvm_km_free(map, addr, size)
	vm_map_t map;
	vaddr_t addr;
	vsize_t size;
@


1.55
log
@remove KERN_SUCCESS and use 0 instead.
eyeballed by miod@@ and pedro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.54 2006/11/29 12:39:50 miod Exp $	*/
d335 1
a335 1
		    pp->flags & PG_BUSY, 0, 0);
d338 1
a338 1
		if (pp->flags & PG_BUSY) {
d340 1
a340 1
			pp->flags |= PG_RELEASED;
d366 1
a366 1
		    pp->flags & PG_BUSY, 0, 0);
d368 1
a368 1
		if (pp->flags & PG_BUSY) {
d370 1
a370 1
			pp->flags |= PG_RELEASED;
d428 2
a429 2
		    pp->flags & PG_BUSY, 0, 0);
		KASSERT((pp->flags & PG_BUSY) == 0);
d447 1
a447 1
		KASSERT((pp->flags & PG_BUSY) == 0);
d528 1
a528 1
			pg->flags &= ~PG_BUSY;	/* new page */
d664 1
a664 1
			if ((pg->flags & PG_RELEASED) == 0)
d666 1
a666 1
			pg->flags |= PG_WANTED;
d675 1
a675 1
			pg->flags &= ~PG_BUSY;	/* new page */
@


1.54
log
@We don't use mb_map anymore since a long time already. Remove it.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.53 2006/11/29 12:17:33 miod Exp $	*/
d225 1
a225 1
	    UVM_INH_NONE, UVM_ADV_RANDOM,UVM_FLAG_FIXED)) != KERN_SUCCESS)
d264 1
a264 1
	    UVM_ADV_RANDOM, mapflags)) != KERN_SUCCESS) {
d292 1
a292 1
	if (uvm_map_submap(map, *min, *max, submap) != KERN_SUCCESS)
d497 1
a497 2
			  UVM_ADV_RANDOM, (flags & UVM_KMF_TRYLOCK))) 
			!= KERN_SUCCESS)) {
d637 2
a638 3
	      UVM_UNKNOWN_OFFSET, align, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL,
					      UVM_INH_NONE, UVM_ADV_RANDOM,
					      0)) != KERN_SUCCESS)) {
d744 1
a744 2
					    UVM_INH_NONE, UVM_ADV_RANDOM,
					    0)) != KERN_SUCCESS)) {
d787 1
a787 2
		    UVM_PROT_ALL, UVM_INH_NONE, UVM_ADV_RANDOM, 0))
		    == KERN_SUCCESS)) {
@


1.53
log
@Add an alignment parameter to uvm_km_alloc1(), and change all callers to
pass zero; this will be used shortly. From art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.52 2006/07/31 11:51:29 mickey Exp $	*/
a96 1
 *   mb_map => memory for large mbufs,  *** protected by splvm ***
@


1.52
log
@fix uvmhist #2: args are always u_long so fix missing %d and %x and no %ll; no change for normal code
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.51 2006/07/26 23:15:55 mickey Exp $	*/
d622 1
a622 4
uvm_km_alloc1(map, size, zeroit)
	vm_map_t map;
	vsize_t size;
	boolean_t zeroit;
d639 1
a639 1
	      UVM_UNKNOWN_OFFSET, 0, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL,
@


1.51
log
@fix fmts for UVMHIST_LOG() entries making it more useful on 64bit archs; miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.50 2006/04/25 08:31:00 mickey Exp $	*/
d335 1
a335 1
		UVMHIST_LOG(maphist,"  page %p, busy=%d", pp,
d366 1
a366 1
		UVMHIST_LOG(maphist,"  page %p, busy=%d", pp,
d428 1
a428 1
		UVMHIST_LOG(maphist,"  page %p, busy=%d", pp,
d446 1
a446 1
		UVMHIST_LOG(maphist,"  page %p, busy=%d", pp,
@


1.50
log
@limit pool backend preallocation to 2048 pages max (which only affects >2g physmem); miod@@ toby@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.49 2006/03/06 19:11:03 mickey Exp $	*/
d335 1
a335 1
		UVMHIST_LOG(maphist,"  page 0x%x, busy=%d", pp,
d366 1
a366 1
		UVMHIST_LOG(maphist,"  page 0x%x, busy=%d", pp,
d428 1
a428 1
		UVMHIST_LOG(maphist,"  page 0x%x, busy=%d", pp,
d446 1
a446 1
		UVMHIST_LOG(maphist,"  page 0x%x, busy=%d", pp,
d481 1
a481 1
	UVMHIST_LOG(maphist,"  (map=0x%x, obj=0x%x, size=0x%x, flags=%d)",
d509 1
a509 1
		UVMHIST_LOG(maphist,"<- done valloc (kva=0x%x)", kva,0,0,0);
d518 1
a518 1
	UVMHIST_LOG(maphist, "  kva=0x%x, offset=0x%x", kva, offset,0,0);
d573 1
a573 1
	UVMHIST_LOG(maphist,"<- done (kva=0x%x)", kva,0,0,0);
d631 1
a631 1
	UVMHIST_LOG(maphist,"(map=0x%x, size=0x%x)", map, size,0,0);
d654 1
a654 1
	UVMHIST_LOG(maphist,"  kva=0x%x, offset=0x%x", kva, offset,0,0);
d711 1
a711 1
	UVMHIST_LOG(maphist,"<- done (kva=0x%x)", kva,0,0,0);
d738 1
a738 1
	UVMHIST_LOG(maphist, "(map=0x%x, size=0x%x)", map, size, 0,0);
d756 1
a756 1
	UVMHIST_LOG(maphist, "<- done (kva=0x%x)", kva,0,0,0);
d777 1
a777 1
	UVMHIST_LOG(maphist, "(map=0x%x, size=0x%x)", map, size, 0,0);
d796 1
a796 1
			UVMHIST_LOG(maphist,"<- done (kva=0x%x)", kva,0,0,0);
@


1.49
log
@deal w/ uvm_km_alloc() returning null; tedu@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.48 2005/10/06 03:59:50 brad Exp $	*/
d965 2
@


1.48
log
@typo
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.47 2005/09/09 15:48:43 pedro Exp $	*/
d1001 2
a1002 2
	for (;;) {
		if (uvm_km_pages_free >= uvm_km_pages_lowat)
a1003 1
		want = 16;
d1008 2
d1013 9
a1021 6
		s = splvm();
		tail->next = uvm_km_pages_head;
		uvm_km_pages_head = head;
		uvm_km_pages_free += i;
		splx(s);
		wakeup(&uvm_km_pages_free);
@


1.47
log
@typos
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.46 2005/05/24 21:11:47 tedu Exp $	*/
d855 1
a855 1
	 * go to splvm in this case (since unprocted maps will never be
@


1.46
log
@add a new field to vm_space and use it to track the number of anon
pages a process uses.  this is now the userland "data size" value.
ok art deraadt tdeval.  thanks testers.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.45 2004/12/30 08:28:39 niklas Exp $	*/
d853 1
a853 1
	 * NOTE: We may be called with a map that doens't require splvm
d887 1
a887 1
	 * NOTE: We may be called with a map that doens't require splvm
@


1.45
log
@Import M_CANFAIL support from NetBSD, removes a nasty panic during low-mem scenarios, instead generating an ENOMEM backfeed, ok tedu@@, prodded by many
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.44 2004/08/24 07:16:12 tedu Exp $	*/
d607 1
a607 1
			 &dead_entries);
@


1.44
log
@change physmem divisor to 256.  divide by page size was wrong.  this does
what i intended all along, without contrived arithmetic screw up.
from discussions with mickey and deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.43 2004/08/24 03:58:14 tedu Exp $	*/
d540 3
a542 1
			if (flags & UVM_KMF_NOWAIT) {
d545 1
a545 1
				return(0);
@


1.43
log
@adapt uvm_km_pages_lowat to physmem.  thanks testers.  ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.42 2004/07/13 14:51:29 tedu Exp $	*/
d962 1
a962 1
		uvm_km_pages_lowat = physmem / (PAGE_SIZE / 16);
@


1.42
log
@#define __HAVE_PMAP_DIRECT and use it.  requested by art
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.41 2004/06/09 20:17:23 tedu Exp $	*/
d939 1
a939 1
int uvm_km_pages_lowat = 128; /* allocate more when reserve drops below this */
d960 6
d973 4
@


1.41
log
@rename POOLPAGE macros to pmap_map_direct
break out uvm_km_page bits for this case, no thread here
lots of testing tech@@, deraadt@@, naddy@@, mickey@@, ...
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.40 2004/05/31 22:53:49 tedu Exp $	*/
a815 6
/* Sanity; must specify both or none. */
#if (defined(pmap_map_direct) || defined(pmap_unmap_direct)) && \
    (!defined(pmap_map_direct) || !defined(pmap_unmap_direct))
#error Must specify MAP and UNMAP together.
#endif

d829 1
a829 1
#if defined(pmap_map_direct)
d864 1
a864 1
#endif /* pmap_map_direct */
d879 1
a879 1
#if defined(pmap_unmap_direct)
d897 1
a897 1
#endif /* pmap_unmap_direct */
d900 1
a900 1
#if defined(pmap_map_direct)
d902 1
a902 1
 * uvm_km_page allocator, pmap_map_direct arch
d929 1
a929 1
 * uvm_km_page allocator, non pmap_map_direct archs
@


1.40
log
@explanatory comments for the uvm_km_page functions.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.39 2004/05/27 04:55:28 tedu Exp $	*/
d817 2
a818 2
#if (defined(PMAP_MAP_POOLPAGE) || defined(PMAP_UNMAP_POOLPAGE)) && \
    (!defined(PMAP_MAP_POOLPAGE) || !defined(PMAP_UNMAP_POOLPAGE))
d835 1
a835 1
#if defined(PMAP_MAP_POOLPAGE)
d848 1
a848 1
	va = PMAP_MAP_POOLPAGE(pg);
d870 1
a870 1
#endif /* PMAP_MAP_POOLPAGE */
d885 2
a886 2
#if defined(PMAP_UNMAP_POOLPAGE)
	uvm_pagefree(PMAP_UNMAP_POOLPAGE(addr));
d903 1
a903 1
#endif /* PMAP_UNMAP_POOLPAGE */
d906 1
d908 28
a935 1
 * uvm_km_page allocator
a967 15
#if defined(PMAP_MAP_POOLPAGE)
		struct vm_page *pg;
		vaddr_t va;

		pg = uvm_pagealloc(NULL, 0, NULL, 0);
		if (__predict_false(pg == NULL))
			break;

		va = PMAP_MAP_POOLPAGE(pg);
		if (__predict_false(va == 0)) {
			uvm_pagefree(pg);
			break;
		}
		page = (void *)va;
#else
a968 1
#endif
a999 14
#if defined(PMAP_MAP_POOLPAGE)
			struct vm_page *pg;
			vaddr_t va;
	
			pg = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_USERESERVE);
			if (__predict_false(pg == NULL))
				break;
			va = PMAP_MAP_POOLPAGE(pg);
			if (__predict_false(va == 0)) {
				uvm_pagefree(pg);
				break;
			}
			page = (void *)va;
#else
a1000 1
#endif
d1056 1
@


1.39
log
@change uvm_km_getpage to take waitok argument and sleep if appropriate.
change both the nointr and default pool allocators to using uvm_km_getpage.
change pools to default to a maxpages value of 8, so they hoard less memory.
change mbuf pools to use default pool allocator.
pools are now more efficient, use less of kmem_map, and a bit faster.
tested mcbride, deraadt, pedro, drahn, miod to work everywhere
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.38 2004/04/28 02:20:58 markus Exp $	*/
d906 13
a918 2
int uvm_km_pages_lowat = 128;
int uvm_km_pages_free;
d926 6
d971 6
d1019 4
@


1.38
log
@remove mb_object*; ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.37 2004/04/20 09:39:36 markus Exp $	*/
d918 1
a918 1
	struct km_page *head, *page;
a921 1
	head = NULL;
d940 2
a941 2
		page->next = head;
		head = page;
a942 1
	uvm_km_pages_head = head;
d963 1
a963 3
		want = uvm_km_pages_lowat - uvm_km_pages_free;
		if (want < 16)
			want = 16;
d991 1
d997 1
a997 1
uvm_km_getpage(void)
d999 1
a999 1
	struct km_page *page;
d1003 10
a1012 4
	page = uvm_km_pages_head;
	if (page) {
		uvm_km_pages_head = page->next;
		uvm_km_pages_free--;
@


1.37
log
@set uvm_km_pages_lowat to 128; unbreaks mbuf allocation;
tested by jmc, brad, hshoexer
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.36 2004/04/19 22:52:33 tedu Exp $	*/
d112 1
a112 1
 * object (e.g. kmem_object, mb_object).
d131 2
a132 2
 * note that the offsets in kmem_object and mb_object also follow this
 * rule.   this means that the offsets for kmem_object must fall in the
a165 1
static struct uvm_object	mb_object_store;
a215 13

	/*
	 * mb_object: for mbuf cluster pages on platforms which use the
	 * mb_map.  Memory is always wired, and this object (and the mb_map)
	 * can be accessed at interrupt time.
	 */
	simple_lock_init(&mb_object_store.vmobjlock);
	mb_object_store.pgops = &km_pager;
	TAILQ_INIT(&mb_object_store.memq);
	mb_object_store.uo_npages = 0;
	/* we are special.  we never die */
	mb_object_store.uo_refs = UVM_OBJ_KERN_INTRSAFE; 
	uvmexp.mb_object = &mb_object_store;
@


1.36
log
@introduce a new km_page allocator that gets pages from kernel_map using
an interrupt safe thread.
use this as the new backend for mbpool and mclpool, eliminating the mb_map.
introduce a sysctl kern.maxclusters which controls the limit of clusters
allocated.
testing by many people, works everywhere but m68k.  ok deraadt@@

this essentially deprecates the NMBCLUSTERS option, don't use it.
this should reduce pressure on the kmem_map and the uvm reserve of static
map entries.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.35 2004/02/23 06:19:32 drahn Exp $	*/
d920 1
a920 1
int uvm_km_pages_lowat;
@


1.35
log
@sync of pmap_update() calls with NetBSD. pmap_update is defined away on
all architectures but arm, where it is needed.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.34 2002/10/29 18:30:21 art Exp $	*/
d147 1
d918 124
@


1.34
log
@Since memory deallocation can't fail, remove the error return from
uvm_unmap, uvm_deallocate and a few other functions.
Simplifies some code and reduces diff to the UBC branch.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.33 2002/09/12 12:50:47 art Exp $	*/
d582 2
d712 1
@


1.33
log
@Change the PMAP_{MAP,UNMAP}_POOLPAGE api to take a vm_page as argument
and return a VM_PAGE. This is to allow sparc64 to cheaply record the
VAC color for those pages.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.32 2002/05/30 06:26:10 deraadt Exp $	*/
d615 1
a615 1
	(void)uvm_unmap_remove(map, trunc_page(addr), round_page(addr+size), 
@


1.32
log
@spelling errors; moritz@@jodeit.org
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.31 2002/04/25 04:36:43 mickey Exp $	*/
d858 1
a858 1
	va = PMAP_MAP_POOLPAGE(VM_PAGE_TO_PHYS(pg));
d896 1
a896 4
	paddr_t pa;

	pa = PMAP_UNMAP_POOLPAGE(addr);
	uvm_pagefree(PHYS_TO_VM_PAGE(pa));
@


1.31
log
@do not map memory for malloc w/ execute perms.
this prevents i-cache preload on some archs,
but does not hurt on others anyway.
art looked all over all the pmaps,
miod and mickey tested it on all possible archs,
deraadt made a lesson out of it for the rest of the folks.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.30 2002/04/22 16:25:46 deraadt Exp $	*/
d114 1
a114 1
 * note that just because a kernel object spans the entire kernel virutal
d137 1
a137 1
 * kernel object have one other special property: when the kernel virtual
d231 1
a231 1
	 * init the map and reserve allready allocated kernel space 
@


1.30
log
@Uncommit.  Since this came out of the blue, without anyone else having
known of it; and since the commit message does not give the rest of us
any feeling that this was tested by anyone, this is being removed.  This
is not an area where one commits because just art agrees.  And that is
what the commit message says.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.28 2002/03/06 22:05:31 art Exp $	*/
d510 1
a510 1
	      0, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL, UVM_INH_NONE,
d572 1
a572 1
			    VM_PROT_ALL);
d575 1
a575 1
			    UVM_PROT_ALL,
@


1.29
log
@do not map malloced memory executable (do avoid possible i-cache prefetch); art@@ ok
@
text
@d510 1
a510 1
	      0, UVM_MAPFLAG(UVM_PROT_RW, UVM_PROT_RW, UVM_INH_NONE,
d572 1
a572 1
			    UVM_PROT_RW);
d575 2
a576 1
			    UVM_PROT_RW, PMAP_WIRED | UVM_PROT_RW);
@


1.28
log
@Some architectures don't allocate any memory out of the kernel_map space
before uvm_km_init (alpha). Don't uvm_map 0 space, it gives very strange
breakage on alpha.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.27 2001/12/19 08:58:07 art Exp $	*/
d510 1
a510 1
	      0, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL, UVM_INH_NONE,
d572 1
a572 1
			    VM_PROT_ALL);
d575 1
a575 2
			    UVM_PROT_ALL,
			    PMAP_WIRED | VM_PROT_READ | VM_PROT_WRITE);
@


1.27
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.22 2001/11/11 01:16:56 art Exp $	*/
d237 2
a238 2
	if (uvm_map(&kernel_map_store, &base, start - base, NULL,
	    UVM_UNKNOWN_OFFSET, 0, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL,
@


1.26
log
@Yet another sync to NetBSD uvm.
Today we add a pmap argument to pmap_update() and allocate map entries for
kernel_map from kmem_map instead of using the static entries. This should
get rid of MAX_KMAPENT panics. Also some uvm_loan problems are fixed.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_km.c,v 1.25 2001/11/28 19:28:14 art Exp $	*/
/*	$NetBSD: uvm_km.c,v 1.51 2001/09/10 21:19:42 chris Exp $	*/
d4 1
a4 1
/*
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and
d48 1
a48 1
 *
d54 3
a56 3
 *
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
d58 1
a58 1
 *
d81 1
a81 1
 * the kernel_map has several "submaps."   submaps can only appear in
d85 1
a85 1
 * virtual address space that is mapped by a submap is locked by the
d105 1
a105 1
 * as large, fixed-sized, sparsely populated uvm_objects.   each kernel
d111 1
a111 1
 * by splvm().    each of these submaps has their own private kernel
d116 2
a117 2
 * large chunks of a kernel object's space go unused either because
 * that area of kernel VM is unmapped, or there is some other type of
d129 1
a129 1
 *   mapped at 0xf8235000.
d154 4
a157 1
struct vm_map *kernel_map = NULL;
d190 6
d214 1
a214 1
	kmem_object_store.uo_refs = UVM_OBJ_KERN_INTRSAFE;
d227 1
a227 1
	mb_object_store.uo_refs = UVM_OBJ_KERN_INTRSAFE;
d231 1
a231 1
	 * init the map and reserve allready allocated kernel space
d239 1
a239 1
	    UVM_INH_NONE, UVM_ADV_RANDOM,UVM_FLAG_FIXED)) != 0)
d241 1
a241 1

d278 1
a278 1
	    UVM_ADV_RANDOM, mapflags)) != 0) {
d306 1
a306 1
	if (uvm_map_submap(map, *min, *max, submap) != 0)
d337 1
a337 1

d429 1
a429 1

d484 1
a484 1
	struct vm_map *map;
a490 1
	vsize_t loopsize;
d511 2
a512 2
			  UVM_ADV_RANDOM, (flags & UVM_KMF_TRYLOCK)))
			!= 0)) {
d539 1
a539 2
	loopsize = size;
	while (loopsize) {
d542 1
a542 1
		if (__predict_true(pg != NULL)) {
d547 1
a547 1

d562 1
a562 1

d580 1
a580 1
		loopsize -= PAGE_SIZE;
a581 3
	
       	pmap_update(pmap_kernel());
	 
d592 1
a592 1
	struct vm_map *map;
d608 1
a608 1
	struct vm_map *map;
d612 1
a612 1
	struct vm_map_entry *dead_entries;
d615 2
a616 2
	uvm_unmap_remove(map, trunc_page(addr), round_page(addr + size),
	    &dead_entries);
d619 1
d632 1
a632 1
	struct vm_map *map;
d653 1
a653 1
					      0)) != 0)) {
d686 1
a686 1

d698 1
a698 1

d710 1
a710 3

	pmap_update(map->pmap);

d731 1
a731 1
	struct vm_map *map;
d739 1
a739 1
	struct vm_map *map;
d759 1
a759 1
					    0)) != 0)) {
d778 1
a778 1
	struct vm_map *map;
d803 1
a803 1
		    == 0)) {
d820 1
a820 1
	struct vm_map *map;
d841 1
a841 1
	struct vm_map *map;
d892 1
a892 1
	struct vm_map *map;
@


1.26.2.1
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_km.c,v 1.26 2001/12/04 23:22:42 art Exp $	*/
/*	$NetBSD: uvm_km.c,v 1.55 2001/11/10 07:37:00 lukem Exp $	*/
d111 2
a112 1
 * by splvm().  pages in these submaps are not assigned to an object.
d131 6
d161 9
d196 27
a222 1
	 * init the map and reserve already allocated kernel space
d253 1
a253 1
	vaddr_t *min, *max;		/* IN/OUT, OUT */
d310 2
d317 3
a319 3
	struct vm_page *pg;
	voff_t curoff, nextoff;
	int swpgonlydelta = 0;
d325 12
a336 9
	for (curoff = start; curoff < end; curoff = nextoff) {
		nextoff = curoff + PAGE_SIZE;
		pg = uvm_pagelookup(uobj, curoff);
		if (pg != NULL && pg->flags & PG_BUSY) {
			pg->flags |= PG_WANTED;
			UVM_UNLOCK_AND_WAIT(pg, &uobj->vmobjlock, 0,
				    "km_pgrm", 0);
			simple_lock(&uobj->vmobjlock);
			nextoff = curoff;
d338 19
d358 3
d362 1
a362 3
		/*
		 * free the swap slot, then the page.
		 */
d364 4
a367 3
		if (pg == NULL &&
		    uao_find_swslot(uobj, curoff >> PAGE_SHIFT) != 0) {
			swpgonlydelta++;
d369 15
a383 2
		uao_dropswap(uobj, curoff >> PAGE_SHIFT);
		if (pg != NULL) {
d385 1
a385 1
			uvm_pagefree(pg);
a389 7

	if (swpgonlydelta > 0) {
		simple_lock(&uvm.swap_data_lock);
		KASSERT(uvmexp.swpgonly >= swpgonlydelta);
		uvmexp.swpgonly -= swpgonlydelta;
		simple_unlock(&uvm.swap_data_lock);
	}
d395 1
a395 1
 *    maps
d398 1
a398 1
 *    the pages right away.    (this is called from uvm_unmap_...).
d400 2
a401 1
 *    be on the active or inactive queues (because they have no object).
d405 2
a406 1
uvm_km_pgremove_intrsafe(start, end)
d409 3
a411 2
	struct vm_page *pg;
	paddr_t pa;
d414 15
a428 2
	for (; start < end; start += PAGE_SIZE) {
		if (!pmap_extract(pmap_kernel(), start, &pa)) {
d431 7
a437 4
		pg = PHYS_TO_VM_PAGE(pa);
		KASSERT(pg);
		KASSERT(pg->uobject == NULL && pg->uanon == NULL);
		uvm_pagefree(pg);
d439 19
d533 2
a534 4
		if (obj) {
			simple_lock(&obj->vmobjlock);
		}
		pg = uvm_pagealloc(obj, offset, NULL, UVM_PGA_USERESERVE);
d539 1
a539 3
		if (obj) {
			simple_unlock(&obj->vmobjlock);
		}
d557 4
a560 1
		 * map it in
d563 1
a563 1
		if (obj == NULL) {
d661 1
a661 1
	 * now allocate the memory.
d667 16
a682 1
		KASSERT(uvm_pagelookup(uvm.kernel_object, offset) == NULL);
d685 1
a685 1
			pg->flags &= ~PG_BUSY;
d689 2
a690 2
		if (pg == NULL) {
			uvm_wait("km_alloc1w");
d693 5
d700 1
d705 1
d715 1
d789 1
a789 1
	for (;;) {
@


1.26.2.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.26.2.1 2002/02/02 03:28:26 art Exp $	*/
d113 1
a113 1
 * note that just because a kernel object spans the entire kernel virtual
d130 1
a130 1
 * kernel objects have one other special property: when the kernel virtual
d186 2
a187 2
	if (base != start && uvm_map(&kernel_map_store, &base, start - base,
	    NULL, UVM_UNKNOWN_OFFSET, 0, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL,
d454 1
a454 1
			    UVM_PROT_RW);
d457 1
a457 1
			    UVM_PROT_RW,
@


1.26.2.3
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.26.2.2 2002/06/11 03:33:03 art Exp $	*/
d721 1
a721 1
	va = PMAP_MAP_POOLPAGE(pg);
d759 4
a762 1
	uvm_pagefree(PMAP_UNMAP_POOLPAGE(addr));
@


1.26.2.4
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_km.c,v 1.26.2.3 2002/10/29 00:36:50 art Exp $	*/
/*	$NetBSD: uvm_km.c,v 1.59 2002/10/05 17:26:06 oster Exp $	*/
d180 2
a181 2
	 * init the map and reserve any space that might already
	 * have been allocated kernel space before installing.
d186 3
a188 5
	if (start != base &&
	    uvm_map(&kernel_map_store, &base, start - base, NULL,
	    UVM_UNKNOWN_OFFSET, 0,
	    UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL, UVM_INH_NONE,
			UVM_ADV_RANDOM,UVM_FLAG_FIXED)) != 0)
d438 1
a438 3
			if ((flags & UVM_KMF_NOWAIT) ||
			    ((flags & UVM_KMF_CANFAIL) &&
			     uvmexp.swpgonly == uvmexp.swpages)) {
d441 1
a441 1
				return (0);
@


1.26.2.5
log
@Some minor cleanups to reduce diff to netbsd.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.26.2.4 2002/11/04 18:02:33 art Exp $	*/
d188 3
a190 3
		    UVM_UNKNOWN_OFFSET, 0,
		    UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL, UVM_INH_NONE,
				UVM_ADV_RANDOM,UVM_FLAG_FIXED)) != 0)
d458 1
a458 1
			    VM_PROT_READ | VM_PROT_WRITE);
d461 1
a461 1
			    VM_PROT_READ | VM_PROT_WRITE,
@


1.25
log
@Sync in more uvm from NetBSD. Mostly just cosmetic stuff.
Contains also support for page coloring.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_km.c,v 1.24 2001/11/28 14:29:13 art Exp $	*/
/*	$NetBSD: uvm_km.c,v 1.50 2001/06/26 17:55:15 thorpej Exp $	*/
d575 3
a577 1
	pmap_update();
d706 1
a706 1
	pmap_update();
@


1.24
log
@more sync to netbsd. some bugfixes in uvm_km_kmemalloc, lots of fixes in uvm_loan.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_km.c,v 1.23 2001/11/28 13:47:39 art Exp $	*/
/*	$NetBSD: uvm_km.c,v 1.45 2001/04/12 21:11:47 thorpej Exp $	*/
d4 1
a4 1
/* 
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and 
d48 1
a48 1
 * 
d54 3
a56 3
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
d58 1
a58 1
 * 
d81 1
a81 1
 * the kernel_map has several "submaps."   submaps can only appear in 
d85 1
a85 1
 * virtual address space that is mapped by a submap is locked by the 
d105 1
a105 1
 * as large, fixed-sized, sparsely populated uvm_objects.   each kernel 
d111 1
a111 1
 * by splvm().    each of these submaps has their own private kernel 
d116 2
a117 2
 * large chunks of a kernel object's space go unused either because 
 * that area of kernel VM is unmapped, or there is some other type of 
d129 1
a129 1
 *   mapped at 0xf8235000.   
d154 1
a154 4
vm_map_t kernel_map = NULL;

struct vmi_list vmi_list;
simple_lock_data_t vmi_list_slock;
a186 6
	 * first, initialize the interrupt-safe map list.
	 */
	LIST_INIT(&vmi_list);
	simple_lock_init(&vmi_list_slock);

	/*
d205 1
a205 1
	kmem_object_store.uo_refs = UVM_OBJ_KERN_INTRSAFE; 
d218 1
a218 1
	mb_object_store.uo_refs = UVM_OBJ_KERN_INTRSAFE; 
d222 1
a222 1
	 * init the map and reserve allready allocated kernel space 
d232 1
a232 1
	
d328 1
a328 1
 
d420 1
a420 1
 
d475 1
a475 1
	vm_map_t map;
d503 1
a503 1
			  UVM_ADV_RANDOM, (flags & UVM_KMF_TRYLOCK))) 
d540 1
a540 1
		
d555 1
a555 1
		
d575 1
d586 1
a586 1
	vm_map_t map;
d602 1
a602 1
	vm_map_t map;
d606 1
a606 1
	vm_map_entry_t dead_entries;
d609 1
a609 1
	uvm_unmap_remove(map, trunc_page(addr), round_page(addr + size), 
d625 1
a625 1
	vm_map_t map;
d679 1
a679 1
		
d691 1
a691 1
		
d703 3
a705 1
	
d726 1
a726 1
	vm_map_t map;
d734 1
a734 1
	vm_map_t map;
d773 1
a773 1
	vm_map_t map;
d815 1
a815 1
	vm_map_t map;
d836 1
a836 1
	vm_map_t map;
d887 1
a887 1
	vm_map_t map;
@


1.23
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_km.c,v 1.22 2001/11/11 01:16:56 art Exp $	*/
/*	$NetBSD: uvm_km.c,v 1.43 2001/03/15 06:10:57 chs Exp $	*/
d491 1
d540 2
a541 1
	while (size) {
d544 1
a544 1
		if (pg) {
d582 1
a582 1
		size -= PAGE_SIZE;
@


1.22
log
@Sync in more stuff from NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_km.c,v 1.21 2001/11/09 03:32:23 art Exp $	*/
/*	$NetBSD: uvm_km.c,v 1.42 2001/01/14 02:10:01 thorpej Exp $	*/
d239 1
a239 1
	    UVM_INH_NONE, UVM_ADV_RANDOM,UVM_FLAG_FIXED)) != KERN_SUCCESS)
d278 1
a278 1
	    UVM_ADV_RANDOM, mapflags)) != KERN_SUCCESS) {
d306 1
a306 1
	if (uvm_map_submap(map, *min, *max, submap) != KERN_SUCCESS)
d512 1
a512 1
			!= KERN_SUCCESS)) {
d615 2
a616 2
	(void)uvm_unmap_remove(map, trunc_page(addr), round_page(addr+size), 
			 &dead_entries);
a618 1

d652 1
a652 1
					      0)) != KERN_SUCCESS)) {
d758 1
a758 1
					    0)) != KERN_SUCCESS)) {
d802 1
a802 1
		    == KERN_SUCCESS)) {
@


1.21
log
@minor sync to NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_km.c,v 1.20 2001/11/07 02:55:50 art Exp $	*/
/*	$NetBSD: uvm_km.c,v 1.41 2000/11/27 04:36:40 nisimura Exp $	*/
d95 1
a95 1
 *		by splimp() because we are allowed to call malloc()
d97 1
a97 1
 *   mb_map => memory for large mbufs,  *** protected by splimp ***
d111 1
a111 1
 * by splimp().    each of these submaps has their own private kernel 
d867 1
a867 1
	 * NOTE: We may be called with a map that doens't require splimp
d869 1
a869 1
	 * go to splimp in this case (since unprocted maps will never be
d876 1
a876 1
	s = splimp();
d904 1
a904 1
	 * NOTE: We may be called with a map that doens't require splimp
d906 1
a906 1
	 * go to splimp in this case (since unprocted maps will never be
d913 1
a913 1
	s = splimp();
@


1.20
log
@Another sync of uvm to NetBSD. Just minor fiddling, no major changes.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_km.c,v 1.19 2001/11/07 01:18:01 art Exp $	*/
/*	$NetBSD: uvm_km.c,v 1.40 2000/11/24 07:07:27 chs Exp $	*/
d734 9
d757 1
a757 1
	    UVM_UNKNOWN_OFFSET, 0, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL,
@


1.19
log
@Add an alignment argument to uvm_map that specifies an alignment hint
for the virtual address.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_km.c,v 1.18 2001/11/06 13:36:52 art Exp $	*/
/*	$NetBSD: uvm_km.c,v 1.39 2000/09/13 15:00:25 thorpej Exp $	*/
d331 2
a332 6
	simple_lock(&uobj->vmobjlock);		/* lock object */

#ifdef DIAGNOSTIC
	if (__predict_false(uobj->pgops != &aobj_pager))
		panic("uvm_km_pgremove: object %p not an aobj", uobj);
#endif
a366 1
		/* done */
d373 2
a374 2
	for (pp = uobj->memq.tqh_first ; pp != NULL ; pp = ppnext) {
		ppnext = pp->listq.tqe_next;
a381 1
		/* now do the actual work */
a396 1
		/* done */
a398 1
	return;
d423 1
a425 5
#ifdef DIAGNOSTIC
	if (__predict_false(UVM_OBJ_IS_INTRSAFE_OBJECT(uobj) == 0))
		panic("uvm_km_pgremove_intrsafe: object %p not intrsafe", uobj);
#endif

d437 1
a437 1
		if (pp == NULL)
d439 1
d443 3
a445 10
#ifdef DIAGNOSTIC
		if (__predict_false(pp->flags & PG_BUSY))
			panic("uvm_km_pgremove_intrsafe: busy page");
		if (__predict_false(pp->pqflags & PQ_ACTIVE))
			panic("uvm_km_pgremove_intrsafe: active page");
		if (__predict_false(pp->pqflags & PQ_INACTIVE))
			panic("uvm_km_pgremove_intrsafe: inactive page");
#endif

		/* free the page */
d453 2
a454 2
	for (pp = uobj->memq.tqh_first ; pp != NULL ; pp = ppnext) {
		ppnext = pp->listq.tqe_next;
d461 3
a463 11

#ifdef DIAGNOSTIC
		if (__predict_false(pp->flags & PG_BUSY))
			panic("uvm_km_pgremove_intrsafe: busy page");
		if (__predict_false(pp->pqflags & PQ_ACTIVE))
			panic("uvm_km_pgremove_intrsafe: active page");
		if (__predict_false(pp->pqflags & PQ_INACTIVE))
			panic("uvm_km_pgremove_intrsafe: inactive page");
#endif

		/* free the page */
a466 1
	return;
a493 1

d495 2
a496 6
	map, obj, size, flags);
#ifdef DIAGNOSTIC
	/* sanity check */
	if (__predict_false(vm_map_pmap(map) != pmap_kernel()))
		panic("uvm_km_kmemalloc: invalid map");
#endif
d525 1
d569 1
a581 1

a595 1

d641 1
a641 5

#ifdef DIAGNOSTIC
	if (vm_map_pmap(map) != pmap_kernel())
		panic("uvm_km_alloc1");
#endif
d738 1
a738 5

#ifdef DIAGNOSTIC
	if (__predict_false(vm_map_pmap(map) != pmap_kernel()))
		panic("uvm_km_valloc");
#endif
d777 1
a777 5

#ifdef DIAGNOSTIC
	if (__predict_false(vm_map_pmap(map) != pmap_kernel()))
		panic("uvm_km_valloc_wait");
#endif
@


1.18
log
@More sync to NetBSD.
 - Use malloc/free instead of MALLOC/FREE for variable sized allocations.
 - Move the memory inheritance code to sys/mman.h and rename from VM_* to MAP_*
 - various cleanups and simplifications.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_km.c,v 1.17 2001/11/06 01:35:04 art Exp $	*/
/*	$NetBSD: uvm_km.c,v 1.38 2000/07/24 20:10:53 jeffs Exp $	*/
d238 1
a238 1
	    UVM_UNKNOWN_OFFSET, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL,
d276 1
a276 1
	if (uvm_map(map, min, size, NULL, UVM_UNKNOWN_OFFSET, 
d542 1
a542 1
	      UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL, UVM_INH_NONE,
d687 1
a687 1
	      UVM_UNKNOWN_OFFSET, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL,
d788 1
a788 1
	    UVM_UNKNOWN_OFFSET, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL,
d836 1
a836 1
		    prefer, UVM_MAPFLAG(UVM_PROT_ALL,
@


1.17
log
@Move the last content from vm/ to uvm/
The only thing left in vm/ are just dumb wrappers.
vm/vm.h includes uvm/uvm_extern.h
vm/pmap.h includes uvm/uvm_pmap.h
vm/vm_page.h includes uvm/uvm_page.h
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_km.c,v 1.16 2001/11/05 22:14:54 art Exp $	*/
/*	$NetBSD: uvm_km.c,v 1.37 2000/06/27 17:29:24 mrg Exp $	*/
d808 1
a808 1
uvm_km_valloc_wait(map, size)
d811 1
d814 1
a814 1
	UVMHIST_FUNC("uvm_km_valloc_wait"); UVMHIST_CALLED(maphist);
d836 1
a836 1
		    UVM_UNKNOWN_OFFSET, UVM_MAPFLAG(UVM_PROT_ALL,
d851 8
@


1.16
log
@Minor sync to NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_km.c,v 1.15 2001/09/19 20:50:59 mickey Exp $	*/
/*	$NetBSD: uvm_km.c,v 1.36 2000/06/26 14:21:18 mrg Exp $	*/
a146 2

#include <vm/vm.h>
@


1.15
log
@merge vm/vm_kern.h into uvm/uvm_extern.h; art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_km.c,v 1.14 2001/08/11 10:57:22 art Exp $	*/
/*	$NetBSD: uvm_km.c,v 1.35 2000/05/08 23:10:20 thorpej Exp $	*/
a148 1
#include <vm/vm_page.h>
@


1.14
log
@Various random fixes from NetBSD.
Including support for zeroing pages in the idle loop (not enabled yet).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.13 2001/07/26 19:37:13 art Exp $	*/
a149 1
#include <vm/vm_kern.h>
@


1.13
log
@Add support for disabling swap devices (swapctl -d).
Improve error handling on I/O errors to swap.
From NetBSD
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_km.c,v 1.12 2001/07/25 13:25:33 art Exp $	*/
/*	$NetBSD: uvm_km.c,v 1.34 2000/01/11 06:57:50 chs Exp $	*/
d338 1
a338 1
	if (uobj->pgops != &aobj_pager)
d438 1
a438 1
	if (UVM_OBJ_IS_INTRSAFE_OBJECT(uobj) == 0)
d459 1
a459 1
		if (pp->flags & PG_BUSY)
d461 1
a461 1
		if (pp->pqflags & PQ_ACTIVE)
d463 1
a463 1
		if (pp->pqflags & PQ_INACTIVE)
d485 1
a485 1
		if (pp->flags & PG_BUSY)
d487 1
a487 1
		if (pp->pqflags & PQ_ACTIVE)
d489 1
a489 1
		if (pp->pqflags & PQ_INACTIVE)
d530 1
a530 1
	if (vm_map_pmap(map) != pmap_kernel())
d545 1
a545 1
	if (uvm_map(map, &kva, size, obj, UVM_UNKNOWN_OFFSET,
d548 1
a548 1
			!= KERN_SUCCESS) {
d587 1
a587 1
		if (pg == NULL) {
d690 4
a693 3
	if (uvm_map(map, &kva, size, uvm.kernel_object, UVM_UNKNOWN_OFFSET,
	      UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL, UVM_INH_NONE,
			  UVM_ADV_RANDOM, 0)) != KERN_SUCCESS) {
d734 1
a734 1
		if (pg == NULL) {
d780 1
a780 1
	if (vm_map_pmap(map) != pmap_kernel())
d791 4
a794 3
	if (uvm_map(map, &kva, size, uvm.kernel_object, UVM_UNKNOWN_OFFSET,
	    UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL, UVM_INH_NONE,
	    UVM_ADV_RANDOM, 0)) != KERN_SUCCESS) {
d822 1
a822 1
	if (vm_map_pmap(map) != pmap_kernel())
d838 1
a838 1
		if (uvm_map(map, &kva, size, uvm.kernel_object,
d841 1
a841 1
		    == KERN_SUCCESS) {
d881 1
a881 1
	if (pg == NULL) {
d889 1
a889 1
	if (va == 0)
@


1.12
log
@Change the pmap_enter interface to merge access_type and the wired boolean
and arbitrary flags into one argument.

One new flag is PMAP_CANFAIL that tells pmap_enter that it can fail if there
are not enough resources to satisfy the request. If this flag is not passed,
pmap_enter should panic as it should have done before this change (XXX - many
pmaps are still not doing that).

Only i386 and alpha implement CANFAIL for now.

Includes uvm updates from NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_km.c,v 1.11 2001/07/18 10:47:05 art Exp $	*/
/*	$NetBSD: uvm_km.c,v 1.33 1999/11/13 00:24:38 thorpej Exp $	*/
d204 1
@


1.11
log
@Get rid of the PMAP_NEW option by making it mandatory for all archs.
The archs that didn't have a proper PMAP_NEW now have a dummy implementation
with wrappers around the old functions.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_km.c,v 1.10 2001/06/23 19:24:33 smart Exp $	*/
/*	$NetBSD: uvm_km.c,v 1.32 1999/09/12 01:17:36 chs Exp $	*/
d608 2
a609 1
			    UVM_PROT_ALL, TRUE, VM_PROT_READ|VM_PROT_WRITE);
d742 1
a742 1
		    UVM_PROT_ALL, TRUE, VM_PROT_READ|VM_PROT_WRITE);
@


1.10
log
@Sync with NetBSD 19990911 (just before PMAP_NEW was required)
  - thread_sleep_msg() -> uvm_sleep()
  - initialize reference count lock in uvm_anon_{init,add}()
  - add uao_flush()
  - replace boolean 'islocked' with 'lockflags'
  - in uvm_fault() change FALSE to TRUE to in 'wide' fault handling
  - get rid of uvm_km_get()
  - various bug fixes
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_km.c,v 1.9 2001/03/22 03:05:55 smart Exp $	*/
/*	$NetBSD: uvm_km.c,v 1.31 1999/07/22 22:58:38 thorpej Exp $	*/
a603 1
#if defined(PMAP_NEW)
a605 4
#else
			pmap_enter(map->pmap, loopva, VM_PAGE_TO_PHYS(pg),
			    UVM_PROT_ALL, TRUE, VM_PROT_READ|VM_PROT_WRITE);
#endif
@


1.9
log
@Sync style, typo, and comments a little closer to NetBSD.  art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_km.c,v 1.8 2001/03/08 15:21:36 smart Exp $	*/
/*	$NetBSD: uvm_km.c,v 1.27 1999/06/04 23:38:41 thorpej Exp $	*/
a163 7
 * local functions
 */

static int uvm_km_get __P((struct uvm_object *, vaddr_t, 
	vm_page_t *, int *, int, vm_prot_t, int, int));

/*
a170 10
static struct uvm_pagerops km_pager = {
	NULL,	/* init */
	NULL, /* reference */
	NULL, /* detach */
	NULL, /* fault */
	NULL, /* flush */
	uvm_km_get, /* get */
	/* ... rest are NULL */
};

d172 3
a174 6
 * uvm_km_get: pager get function for kernel objects
 *
 * => currently we do not support pageout to the swap area, so this
 *    pager is very simple.    eventually we may want an anonymous 
 *    object pager which will do paging.
 * => XXXCDC: this pager should be phased out in favor of the aobj pager
d176 1
a176 209


static int
uvm_km_get(uobj, offset, pps, npagesp, centeridx, access_type, advice, flags)
	struct uvm_object *uobj;
	vaddr_t offset;
	struct vm_page **pps;
	int *npagesp;
	int centeridx, advice, flags;
	vm_prot_t access_type;
{
	vaddr_t current_offset;
	vm_page_t ptmp;
	int lcv, gotpages, maxpages;
	boolean_t done;
	UVMHIST_FUNC("uvm_km_get"); UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist, "flags=%d", flags,0,0,0);
	
	/*
	 * get number of pages
	 */

	maxpages = *npagesp;

	/*
	 * step 1: handled the case where fault data structures are locked.
	 */

	if (flags & PGO_LOCKED) {

		/*
		 * step 1a: get pages that are already resident.   only do
		 * this if the data structures are locked (i.e. the first time
		 * through).
		 */

		done = TRUE;	/* be optimistic */
		gotpages = 0;	/* # of pages we got so far */

		for (lcv = 0, current_offset = offset ; 
		    lcv < maxpages ; lcv++, current_offset += PAGE_SIZE) {

			/* do we care about this page?  if not, skip it */
			if (pps[lcv] == PGO_DONTCARE)
				continue;

			/* lookup page */
			ptmp = uvm_pagelookup(uobj, current_offset);
			
			/* null?  attempt to allocate the page */
			if (ptmp == NULL) {
				ptmp = uvm_pagealloc(uobj, current_offset,
				    NULL, 0);
				if (ptmp) {
					/* new page */
					ptmp->flags &= ~(PG_BUSY|PG_FAKE);
					UVM_PAGE_OWN(ptmp, NULL);
					uvm_pagezero(ptmp);
				}
			}

			/*
			 * to be useful must get a non-busy, non-released page
			 */
			if (ptmp == NULL ||
			    (ptmp->flags & (PG_BUSY|PG_RELEASED)) != 0) {
				if (lcv == centeridx ||
				    (flags & PGO_ALLPAGES) != 0)
					/* need to do a wait or I/O! */
					done = FALSE;
				continue;
			}

			/*
			 * useful page: busy/lock it and plug it in our
			 * result array
			 */

			/* caller must un-busy this page */
			ptmp->flags |= PG_BUSY;	
			UVM_PAGE_OWN(ptmp, "uvm_km_get1");
			pps[lcv] = ptmp;
			gotpages++;

		}	/* "for" lcv loop */

		/*
		 * step 1b: now we've either done everything needed or we
		 * to unlock and do some waiting or I/O.
		 */

		UVMHIST_LOG(maphist, "<- done (done=%d)", done, 0,0,0);

		*npagesp = gotpages;
		if (done)
			return(VM_PAGER_OK);		/* bingo! */
		else
			return(VM_PAGER_UNLOCK);	/* EEK!   Need to
							 * unlock and I/O */
	}

	/*
	 * step 2: get non-resident or busy pages.
	 * object is locked.   data structures are unlocked.
	 */

	for (lcv = 0, current_offset = offset ; 
	    lcv < maxpages ; lcv++, current_offset += PAGE_SIZE) {
		
		/* skip over pages we've already gotten or don't want */
		/* skip over pages we don't _have_ to get */
		if (pps[lcv] != NULL ||
		    (lcv != centeridx && (flags & PGO_ALLPAGES) == 0))
			continue;

		/*
		 * we have yet to locate the current page (pps[lcv]).   we
		 * first look for a page that is already at the current offset.
		 * if we find a page, we check to see if it is busy or
		 * released.  if that is the case, then we sleep on the page
		 * until it is no longer busy or released and repeat the
		 * lookup.    if the page we found is neither busy nor
		 * released, then we busy it (so we own it) and plug it into
		 * pps[lcv].   this 'break's the following while loop and
		 * indicates we are ready to move on to the next page in the
		 * "lcv" loop above.
		 *
		 * if we exit the while loop with pps[lcv] still set to NULL,
		 * then it means that we allocated a new busy/fake/clean page
		 * ptmp in the object and we need to do I/O to fill in the
		 * data.
		 */

		while (pps[lcv] == NULL) {	/* top of "pps" while loop */
			
			/* look for a current page */
			ptmp = uvm_pagelookup(uobj, current_offset);

			/* nope?   allocate one now (if we can) */
			if (ptmp == NULL) {

				ptmp = uvm_pagealloc(uobj, current_offset,
				    NULL, 0);

				/* out of RAM? */
				if (ptmp == NULL) {
					simple_unlock(&uobj->vmobjlock);
					uvm_wait("kmgetwait1");
					simple_lock(&uobj->vmobjlock);
					/* goto top of pps while loop */
					continue;
				}

				/* 
				 * got new page ready for I/O.  break pps
				 * while loop.  pps[lcv] is still NULL.
				 */
				break;		
			}

			/* page is there, see if we need to wait on it */
			if ((ptmp->flags & (PG_BUSY|PG_RELEASED)) != 0) {
				ptmp->flags |= PG_WANTED;
				UVM_UNLOCK_AND_WAIT(ptmp,&uobj->vmobjlock,
				    FALSE, "uvn_get",0);
				simple_lock(&uobj->vmobjlock);
				continue;	/* goto top of pps while loop */
			}
			
			/* 
			 * if we get here then the page has become resident
			 * and unbusy between steps 1 and 2.  we busy it now
			 * (so we own it) and set pps[lcv] (so that we exit
			 * the while loop).  caller must un-busy.
			 */
			ptmp->flags |= PG_BUSY;
			UVM_PAGE_OWN(ptmp, "uvm_km_get2");
			pps[lcv] = ptmp;
		}

		/*
		 * if we own the a valid page at the correct offset, pps[lcv]
		 * will point to it.   nothing more to do except go to the
		 * next page.
		 */

		if (pps[lcv])
			continue;			/* next lcv */

		/*
		 * we have a "fake/busy/clean" page that we just allocated.  
		 * do the needed "i/o" (in this case that means zero it).
		 */

		uvm_pagezero(ptmp);
		ptmp->flags &= ~(PG_FAKE);
		pps[lcv] = ptmp;

	}	/* lcv loop */

	/*
	 * finally, unlock object and return.
	 */

	simple_unlock(&uobj->vmobjlock);
	UVMHIST_LOG(maphist, "<- done (OK)",0,0,0,0);
	return(VM_PAGER_OK);
}
d881 1
a881 1
	pg = uvm_pagealloc(NULL, 0, NULL, 0);
@


1.8
log
@Replace thread_wakeup() with wakeup().  It is defined in vm_extern.h as a
wrapper, so this removes a dependence on the old VM system.  From NetBSD.
art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.7 2001/01/29 02:07:45 niklas Exp $	*/
d360 2
a361 2
				UVM_UNLOCK_AND_WAIT(ptmp,&uobj->vmobjlock, 0,
				    "uvn_get",0);
d953 1
a953 1
			    0, "km_alloc", 0);
@


1.7
log
@$OpenBSD$
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_km.c,v 1.27 1999/06/04 23:38:41 thorpej Exp $	*/
d884 1
a884 1
	thread_wakeup(map);
@


1.6
log
@Convert bzero to memset(X, 0..) and bcopy to memcpy.
This is to match (make diffs smaller) the code in NetBSD.
new gcc inlines those functions, so this could also be a performance win.
@
text
@d1 1
@


1.5
log
@Bring in some new UVM code from NetBSD (not current).

 - Introduce a new type of map that are interrupt safe and never allow faults
   in them. mb_map and kmem_map are made intrsafe.
 - Add "access protection" to uvm_vslock (to be passed down to uvm_fault and
   later to pmap_enter).
 - madvise(2) now works.
 - various cleanups.
@
text
@d986 1
a986 1
		bzero((caddr_t)kva, loopva - kva);
@


1.4
log
@Change the pmap_enter api to pass down an argument that indicates
the access type that caused this mapping. This is to simplify pmaps
with mod/ref emulation (none for the moment) and in some cases speed
up pmap_is_{referenced,modified}.
At the same time, clean up some mappings that had too high protection.

XXX - the access type is incorrect in old vm, it's only used by uvm and MD code.
The actual use of this in pmap_enter implementations is not in this commit.
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_km.c,v 1.22 1999/03/26 21:58:39 mycroft Exp $	*/
d159 3
d167 2
a168 1
													 vm_page_t *, int *, int, vm_prot_t, int, int));
d421 7
a427 1
	 * first, init kernel memory objects.
d434 5
a438 1
	/* kmem_object: for malloc'd memory (wired, protected by splimp) */
d444 1
a444 1
	kmem_object_store.uo_refs = UVM_OBJ_KERN; 
d447 5
a451 1
	/* mb_object: for mbuf memory (always wired, protected by splimp) */
d457 1
a457 1
	mb_object_store.uo_refs = UVM_OBJ_KERN; 
d465 1
a465 1
	uvm_map_setup(&kernel_map_store, base, end, FALSE);
d490 1
a490 1
uvm_km_suballoc(map, min, max, size, pageable, fixed, submap)
d494 1
a494 1
	boolean_t pageable;
d524 1
a524 1
		submap = uvm_map_create(vm_map_pmap(map), *min, *max, pageable);
d528 1
a528 1
		uvm_map_setup(submap, *min, *max, pageable);
d556 1
a556 1
	boolean_t by_list, is_aobj;
d563 4
a566 2
	/* is uobj an aobj? */
	is_aobj = uobj->pgops == &aobj_pager;
d584 1
d586 1
a586 1
		if (pp->flags & PG_BUSY)
d589 3
a591 2
		else {
			pmap_page_protect(PMAP_PGARG(pp), VM_PROT_NONE);
d594 2
a595 1
			 * if this kernel object is an aobj, free the swap slot.
a596 4
			if (is_aobj) {
				uao_dropswap(uobj, curoff >> PAGE_SHIFT);
			}

a601 1

a608 1

d616 1
d618 1
a618 1
		if (pp->flags & PG_BUSY)
d621 3
a623 2
		else {
			pmap_page_protect(PMAP_PGARG(pp), VM_PROT_NONE);
d626 2
a627 1
			 * if this kernel object is an aobj, free the swap slot.
a628 4
			if (is_aobj) {
				uao_dropswap(uobj, pp->offset >> PAGE_SHIFT);
			}

d634 16
d651 70
d830 1
d832 2
a833 1
		pmap_kenter_pa(loopva, VM_PAGE_TO_PHYS(pg), VM_PROT_ALL);
d835 2
a836 2
		pmap_enter(map->pmap, loopva, VM_PAGE_TO_PHYS(pg),
		    UVM_PROT_ALL, TRUE, VM_PROT_READ | VM_PROT_WRITE);
d838 4
d972 2
a973 2
                pmap_enter(map->pmap, loopva, VM_PAGE_TO_PHYS(pg),
                    UVM_PROT_ALL, TRUE, VM_PROT_READ|VM_PROT_WRITE);
@


1.4.4.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_km.c,v 1.27 1999/06/04 23:38:41 thorpej Exp $	*/
a158 3
struct vmi_list vmi_list;
simple_lock_data_t vmi_list_slock;

d164 1
a164 2
	vm_page_t *, int *, int, vm_prot_t, int, int));

d417 1
a417 7
	 * first, initialize the interrupt-safe map list.
	 */
	LIST_INIT(&vmi_list);
	simple_lock_init(&vmi_list_slock);

	/*
	 * next, init kernel memory objects.
d424 1
a424 5
	/*
	 * kmem_object: for use by the kernel malloc().  Memory is always
	 * wired, and this object (and the kmem_map) can be accessed at
	 * interrupt time.
	 */
d430 1
a430 1
	kmem_object_store.uo_refs = UVM_OBJ_KERN_INTRSAFE; 
d433 1
a433 5
	/*
	 * mb_object: for mbuf cluster pages on platforms which use the
	 * mb_map.  Memory is always wired, and this object (and the mb_map)
	 * can be accessed at interrupt time.
	 */
d439 1
a439 1
	mb_object_store.uo_refs = UVM_OBJ_KERN_INTRSAFE; 
d447 1
a447 1
	uvm_map_setup(&kernel_map_store, base, end, VM_MAP_PAGEABLE);
d472 1
a472 1
uvm_km_suballoc(map, min, max, size, flags, fixed, submap)
d476 1
a476 1
	int flags;
d506 1
a506 1
		submap = uvm_map_create(vm_map_pmap(map), *min, *max, flags);
d510 1
a510 1
		uvm_map_setup(submap, *min, *max, flags);
d538 1
a538 1
	boolean_t by_list;
d545 2
a546 4
#ifdef DIAGNOSTIC
	if (uobj->pgops != &aobj_pager)
		panic("uvm_km_pgremove: object %p not an aobj", uobj);
#endif
a563 1

d565 1
a565 1
		if (pp->flags & PG_BUSY) {
d568 2
a569 3
		} else {
			/* free the swap slot... */
			uao_dropswap(uobj, curoff >> PAGE_SHIFT);
d572 1
a572 2
			 * ...and free the page; note it may be on the
			 * active or inactive queues.
d574 4
d583 1
d591 1
a598 1

d600 1
a600 1
		if (pp->flags & PG_BUSY) {
d603 2
a604 3
		} else {
			/* free the swap slot... */
			uao_dropswap(uobj, pp->offset >> PAGE_SHIFT);
d607 1
a607 2
			 * ...and free the page; note it may be on the
			 * active or inactive queues.
d609 4
a617 16
	}
	simple_unlock(&uobj->vmobjlock);
	return;
}


/*
 * uvm_km_pgremove_intrsafe: like uvm_km_pgremove(), but for "intrsafe"
 *    objects
 *
 * => when you unmap a part of anonymous kernel memory you want to toss
 *    the pages right away.    (this gets called from uvm_unmap_...).
 * => none of the pages will ever be busy, and none of them will ever
 *    be on the active or inactive queues (because these objects are
 *    never allowed to "page").
 */
a618 70
void
uvm_km_pgremove_intrsafe(uobj, start, end)
	struct uvm_object *uobj;
	vaddr_t start, end;
{
	boolean_t by_list;
	struct vm_page *pp, *ppnext;
	vaddr_t curoff;
	UVMHIST_FUNC("uvm_km_pgremove_intrsafe"); UVMHIST_CALLED(maphist);

	simple_lock(&uobj->vmobjlock);		/* lock object */

#ifdef DIAGNOSTIC
	if (UVM_OBJ_IS_INTRSAFE_OBJECT(uobj) == 0)
		panic("uvm_km_pgremove_intrsafe: object %p not intrsafe", uobj);
#endif

	/* choose cheapest traversal */
	by_list = (uobj->uo_npages <=
	     ((end - start) >> PAGE_SHIFT) * UKM_HASH_PENALTY);
 
	if (by_list)
		goto loop_by_list;

	/* by hash */

	for (curoff = start ; curoff < end ; curoff += PAGE_SIZE) {
		pp = uvm_pagelookup(uobj, curoff);
		if (pp == NULL)
			continue;

		UVMHIST_LOG(maphist,"  page 0x%x, busy=%d", pp,
		    pp->flags & PG_BUSY, 0, 0);
#ifdef DIAGNOSTIC
		if (pp->flags & PG_BUSY)
			panic("uvm_km_pgremove_intrsafe: busy page");
		if (pp->pqflags & PQ_ACTIVE)
			panic("uvm_km_pgremove_intrsafe: active page");
		if (pp->pqflags & PQ_INACTIVE)
			panic("uvm_km_pgremove_intrsafe: inactive page");
#endif

		/* free the page */
		uvm_pagefree(pp);
	}
	simple_unlock(&uobj->vmobjlock);
	return;

loop_by_list:

	for (pp = uobj->memq.tqh_first ; pp != NULL ; pp = ppnext) {
		ppnext = pp->listq.tqe_next;
		if (pp->offset < start || pp->offset >= end) {
			continue;
		}

		UVMHIST_LOG(maphist,"  page 0x%x, busy=%d", pp,
		    pp->flags & PG_BUSY, 0, 0);

#ifdef DIAGNOSTIC
		if (pp->flags & PG_BUSY)
			panic("uvm_km_pgremove_intrsafe: busy page");
		if (pp->pqflags & PQ_ACTIVE)
			panic("uvm_km_pgremove_intrsafe: active page");
		if (pp->pqflags & PQ_INACTIVE)
			panic("uvm_km_pgremove_intrsafe: inactive page");
#endif

		/* free the page */
		uvm_pagefree(pp);
a727 1
		if (UVM_OBJ_IS_INTRSAFE_OBJECT(obj)) {
d729 1
a729 2
			pmap_kenter_pa(loopva, VM_PAGE_TO_PHYS(pg),
			    VM_PROT_ALL);
d731 2
a732 2
			pmap_enter(map->pmap, loopva, VM_PAGE_TO_PHYS(pg),
			    UVM_PROT_ALL, TRUE, VM_PROT_READ|VM_PROT_WRITE);
a733 4
		} else {
			pmap_enter(map->pmap, loopva, VM_PAGE_TO_PHYS(pg),
			    UVM_PROT_ALL, TRUE, VM_PROT_READ|VM_PROT_WRITE);
		}
d864 2
a865 2
		pmap_enter(map->pmap, loopva, VM_PAGE_TO_PHYS(pg),
		    UVM_PROT_ALL, TRUE, VM_PROT_READ|VM_PROT_WRITE);
@


1.4.4.2
log
@merge in approximately 2.9 into SMP branch
@
text
@a0 1
/*	$OpenBSD: uvm_km.c,v 1.9 2001/03/22 03:05:55 smart Exp $	*/
d359 2
a360 2
				UVM_UNLOCK_AND_WAIT(ptmp,&uobj->vmobjlock,
				    FALSE, "uvn_get",0);
d883 1
a883 1
	wakeup(map);
d952 1
a952 1
			    FALSE, "km_alloc", 0);
d986 1
a986 1
		memset((caddr_t)kva, 0, loopva - kva);
@


1.4.4.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_km.c,v 1.31 1999/07/22 22:58:38 thorpej Exp $	*/
d164 7
d178 10
d189 6
a194 3
 * All pager operations here are NULL, but the object must have
 * a pager ops vector associated with it; various places assume
 * it to be so.
d196 209
a404 1
static struct uvm_pagerops	km_pager;
d1109 1
a1109 1
	pg = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_USERESERVE);
@


1.4.4.4
log
@Sync the SMP branch to something just after 3.0
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_km.c,v 1.35 2000/05/08 23:10:20 thorpej Exp $	*/
d150 1
a203 1
	uao_init();
d337 1
a337 1
	if (__predict_false(uobj->pgops != &aobj_pager))
d437 1
a437 1
	if (__predict_false(UVM_OBJ_IS_INTRSAFE_OBJECT(uobj) == 0))
d458 1
a458 1
		if (__predict_false(pp->flags & PG_BUSY))
d460 1
a460 1
		if (__predict_false(pp->pqflags & PQ_ACTIVE))
d462 1
a462 1
		if (__predict_false(pp->pqflags & PQ_INACTIVE))
d484 1
a484 1
		if (__predict_false(pp->flags & PG_BUSY))
d486 1
a486 1
		if (__predict_false(pp->pqflags & PQ_ACTIVE))
d488 1
a488 1
		if (__predict_false(pp->pqflags & PQ_INACTIVE))
d529 1
a529 1
	if (__predict_false(vm_map_pmap(map) != pmap_kernel()))
d544 1
a544 1
	if (__predict_false(uvm_map(map, &kva, size, obj, UVM_UNKNOWN_OFFSET,
d547 1
a547 1
			!= KERN_SUCCESS)) {
d586 1
a586 1
		if (__predict_false(pg == NULL)) {
d604 1
d607 4
d613 1
a613 2
			    UVM_PROT_ALL,
			    PMAP_WIRED | VM_PROT_READ | VM_PROT_WRITE);
d693 3
a695 4
	if (__predict_false(uvm_map(map, &kva, size, uvm.kernel_object,
	      UVM_UNKNOWN_OFFSET, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL,
					      UVM_INH_NONE, UVM_ADV_RANDOM,
					      0)) != KERN_SUCCESS)) {
d736 1
a736 1
		if (__predict_false(pg == NULL)) {
d746 1
a746 1
		    UVM_PROT_ALL, PMAP_WIRED | VM_PROT_READ | VM_PROT_WRITE);
d782 1
a782 1
	if (__predict_false(vm_map_pmap(map) != pmap_kernel()))
d793 3
a795 4
	if (__predict_false(uvm_map(map, &kva, size, uvm.kernel_object,
	    UVM_UNKNOWN_OFFSET, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL,
					    UVM_INH_NONE, UVM_ADV_RANDOM,
					    0)) != KERN_SUCCESS)) {
d823 1
a823 1
	if (__predict_false(vm_map_pmap(map) != pmap_kernel()))
d839 1
a839 1
		if (__predict_true(uvm_map(map, &kva, size, uvm.kernel_object,
d842 1
a842 1
		    == KERN_SUCCESS)) {
d882 1
a882 1
	if (__predict_false(pg == NULL)) {
d890 1
a890 1
	if (__predict_false(va == 0))
@


1.4.4.5
log
@merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_km.c,v 1.42 2001/01/14 02:10:01 thorpej Exp $	*/
d95 1
a95 1
 *		by splvm() because we are allowed to call malloc()
d97 1
a97 1
 *   mb_map => memory for large mbufs,  *** protected by splvm ***
d111 1
a111 1
 * by splvm().    each of these submaps has their own private kernel 
d148 3
d241 1
a241 1
	    UVM_UNKNOWN_OFFSET, 0, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL,
d279 1
a279 1
	if (uvm_map(map, min, size, NULL, UVM_UNKNOWN_OFFSET, 0,
d334 6
a339 2
	KASSERT(uobj->pgops == &aobj_pager);
	simple_lock(&uobj->vmobjlock);
d374 1
d381 2
a382 2
	for (pp = TAILQ_FIRST(&uobj->memq); pp != NULL; pp = ppnext) {
		ppnext = TAILQ_NEXT(pp, listq);
d390 1
d406 1
d409 1
a433 1
	KASSERT(UVM_OBJ_IS_INTRSAFE_OBJECT(uobj));
d436 5
d452 1
a452 1
		if (pp == NULL) {
a453 1
		}
d457 10
a466 3
		KASSERT((pp->flags & PG_BUSY) == 0);
		KASSERT((pp->pqflags & PQ_ACTIVE) == 0);
		KASSERT((pp->pqflags & PQ_INACTIVE) == 0);
d474 2
a475 2
	for (pp = TAILQ_FIRST(&uobj->memq); pp != NULL; pp = ppnext) {
		ppnext = TAILQ_NEXT(pp, listq);
d482 11
a492 3
		KASSERT((pp->flags & PG_BUSY) == 0);
		KASSERT((pp->pqflags & PQ_ACTIVE) == 0);
		KASSERT((pp->pqflags & PQ_INACTIVE) == 0);
d496 1
d524 1
d526 6
a531 2
		    map, obj, size, flags);
	KASSERT(vm_map_pmap(map) == pmap_kernel());
d545 1
a545 1
	      0, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL, UVM_INH_NONE,
a559 1

a602 1

d615 1
d630 1
d676 5
a680 1
	KASSERT(vm_map_pmap(map) == pmap_kernel());
d690 1
a690 1
	      UVM_UNKNOWN_OFFSET, 0, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL,
a772 9
	return(uvm_km_valloc_align(map, size, 0));
}

vaddr_t
uvm_km_valloc_align(map, size, align)
	vm_map_t map;
	vsize_t size;
	vsize_t align;
{
d777 5
a781 1
	KASSERT(vm_map_pmap(map) == pmap_kernel());
d791 1
a791 1
	    UVM_UNKNOWN_OFFSET, align, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL,
d811 1
a811 1
uvm_km_valloc_prefer_wait(map, size, prefer)
a813 1
	voff_t prefer;
d816 1
a816 1
	UVMHIST_FUNC("uvm_km_valloc_prefer_wait"); UVMHIST_CALLED(maphist);
d819 5
a823 1
	KASSERT(vm_map_pmap(map) == pmap_kernel());
d838 1
a838 1
		    prefer, 0, UVM_MAPFLAG(UVM_PROT_ALL,
a854 8
vaddr_t
uvm_km_valloc_wait(map, size)
	vm_map_t map;
	vsize_t size;
{
	return uvm_km_valloc_prefer_wait(map, size, UVM_UNKNOWN_OFFSET);
}

d896 1
a896 1
	 * NOTE: We may be called with a map that doens't require splvm
d898 1
a898 1
	 * go to splvm in this case (since unprocted maps will never be
d905 1
a905 1
	s = splvm();
d933 1
a933 1
	 * NOTE: We may be called with a map that doens't require splvm
d935 1
a935 1
	 * go to splvm in this case (since unprocted maps will never be
d942 1
a942 1
	s = splvm();
@


1.4.4.6
log
@Merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_km.c,v 1.50 2001/06/26 17:55:15 thorpej Exp $	*/
d4 1
a4 1
/*
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and
d48 1
a48 1
 *
d54 3
a56 3
 *
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
d58 1
a58 1
 *
d81 1
a81 1
 * the kernel_map has several "submaps."   submaps can only appear in
d85 1
a85 1
 * virtual address space that is mapped by a submap is locked by the
d105 1
a105 1
 * as large, fixed-sized, sparsely populated uvm_objects.   each kernel
d111 1
a111 1
 * by splvm().    each of these submaps has their own private kernel
d116 2
a117 2
 * large chunks of a kernel object's space go unused either because
 * that area of kernel VM is unmapped, or there is some other type of
d129 1
a129 1
 *   mapped at 0xf8235000.
d154 4
a157 1
struct vm_map *kernel_map = NULL;
d190 6
d214 1
a214 1
	kmem_object_store.uo_refs = UVM_OBJ_KERN_INTRSAFE;
d227 1
a227 1
	mb_object_store.uo_refs = UVM_OBJ_KERN_INTRSAFE;
d231 1
a231 1
	 * init the map and reserve allready allocated kernel space
d239 1
a239 1
	    UVM_INH_NONE, UVM_ADV_RANDOM,UVM_FLAG_FIXED)) != 0)
d241 1
a241 1

d278 1
a278 1
	    UVM_ADV_RANDOM, mapflags)) != 0) {
d306 1
a306 1
	if (uvm_map_submap(map, *min, *max, submap) != 0)
d337 1
a337 1

d429 1
a429 1

d484 1
a484 1
	struct vm_map *map;
a490 1
	vsize_t loopsize;
d511 2
a512 2
			  UVM_ADV_RANDOM, (flags & UVM_KMF_TRYLOCK)))
			!= 0)) {
d539 1
a539 2
	loopsize = size;
	while (loopsize) {
d542 1
a542 1
		if (__predict_true(pg != NULL)) {
d547 1
a547 1

d562 1
a562 1

d580 1
a580 1
		loopsize -= PAGE_SIZE;
a581 1
	pmap_update();
d592 1
a592 1
	struct vm_map *map;
d608 1
a608 1
	struct vm_map *map;
d612 1
a612 1
	struct vm_map_entry *dead_entries;
d615 2
a616 2
	uvm_unmap_remove(map, trunc_page(addr), round_page(addr + size),
	    &dead_entries);
d619 1
d632 1
a632 1
	struct vm_map *map;
d653 1
a653 1
					      0)) != 0)) {
d686 1
a686 1

d698 1
a698 1

d710 1
a710 3

	pmap_update();

d731 1
a731 1
	struct vm_map *map;
d739 1
a739 1
	struct vm_map *map;
d759 1
a759 1
					    0)) != 0)) {
d778 1
a778 1
	struct vm_map *map;
d803 1
a803 1
		    == 0)) {
d820 1
a820 1
	struct vm_map *map;
d841 1
a841 1
	struct vm_map *map;
d892 1
a892 1
	struct vm_map *map;
@


1.4.4.7
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_km.c,v 1.42 2001/01/14 02:10:01 thorpej Exp $	*/
d4 1
a4 1
/* 
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and 
d48 1
a48 1
 * 
d54 3
a56 3
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
d58 1
a58 1
 * 
d81 1
a81 1
 * the kernel_map has several "submaps."   submaps can only appear in 
d85 1
a85 1
 * virtual address space that is mapped by a submap is locked by the 
d105 1
a105 1
 * as large, fixed-sized, sparsely populated uvm_objects.   each kernel 
d111 1
a111 1
 * by splvm().    each of these submaps has their own private kernel 
d116 2
a117 2
 * large chunks of a kernel object's space go unused either because 
 * that area of kernel VM is unmapped, or there is some other type of 
d129 1
a129 1
 *   mapped at 0xf8235000.   
d154 1
a154 4
vm_map_t kernel_map = NULL;

struct vmi_list vmi_list;
simple_lock_data_t vmi_list_slock;
a186 6
	 * first, initialize the interrupt-safe map list.
	 */
	LIST_INIT(&vmi_list);
	simple_lock_init(&vmi_list_slock);

	/*
d205 1
a205 1
	kmem_object_store.uo_refs = UVM_OBJ_KERN_INTRSAFE; 
d218 1
a218 1
	mb_object_store.uo_refs = UVM_OBJ_KERN_INTRSAFE; 
d222 1
a222 1
	 * init the map and reserve allready allocated kernel space 
d230 1
a230 1
	    UVM_INH_NONE, UVM_ADV_RANDOM,UVM_FLAG_FIXED)) != KERN_SUCCESS)
d232 1
a232 1
	
d269 1
a269 1
	    UVM_ADV_RANDOM, mapflags)) != KERN_SUCCESS) {
d297 1
a297 1
	if (uvm_map_submap(map, *min, *max, submap) != KERN_SUCCESS)
d328 1
a328 1
 
d420 1
a420 1
 
d475 1
a475 1
	vm_map_t map;
d482 1
d503 2
a504 2
			  UVM_ADV_RANDOM, (flags & UVM_KMF_TRYLOCK))) 
			!= KERN_SUCCESS)) {
d531 2
a532 1
	while (size) {
d535 1
a535 1
		if (pg) {
d540 1
a540 1
		
d555 1
a555 1
		
d573 1
a573 1
		size -= PAGE_SIZE;
d575 1
d586 1
a586 1
	vm_map_t map;
d602 1
a602 1
	vm_map_t map;
d606 1
a606 1
	vm_map_entry_t dead_entries;
d609 2
a610 2
	(void)uvm_unmap_remove(map, trunc_page(addr), round_page(addr+size), 
			 &dead_entries);
a612 1

d625 1
a625 1
	vm_map_t map;
d646 1
a646 1
					      0)) != KERN_SUCCESS)) {
d679 1
a679 1
		
d691 1
a691 1
		
d703 3
a705 1
	
d726 1
a726 1
	vm_map_t map;
d734 1
a734 1
	vm_map_t map;
d754 1
a754 1
					    0)) != KERN_SUCCESS)) {
d773 1
a773 1
	vm_map_t map;
d798 1
a798 1
		    == KERN_SUCCESS)) {
d815 1
a815 1
	vm_map_t map;
d836 1
a836 1
	vm_map_t map;
d887 1
a887 1
	vm_map_t map;
@


1.4.4.8
log
@Merge in -current from roughly a week ago
@
text
@d237 2
a238 2
	if (base != start && uvm_map(&kernel_map_store, &base, start - base,
	    NULL, UVM_UNKNOWN_OFFSET, 0, UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL,
@


1.4.4.9
log
@Sync the SMP branch with 3.3
@
text
@d114 1
a114 1
 * note that just because a kernel object spans the entire kernel virtual
d137 1
a137 1
 * kernel objects have one other special property: when the kernel virtual
d231 1
a231 1
	 * init the map and reserve already allocated kernel space 
d510 1
a510 1
	      0, UVM_MAPFLAG(UVM_PROT_RW, UVM_PROT_RW, UVM_INH_NONE,
d572 1
a572 1
			    UVM_PROT_RW);
d575 1
a575 1
			    UVM_PROT_RW,
d615 1
a615 1
	uvm_unmap_remove(map, trunc_page(addr), round_page(addr+size), 
d858 1
a858 1
	va = PMAP_MAP_POOLPAGE(pg);
d896 4
a899 1
	uvm_pagefree(PMAP_UNMAP_POOLPAGE(addr));
@


1.4.4.10
log
@Merge with the trunk
@
text
@d112 1
a112 1
 * object (e.g. kmem_object).
d131 2
a132 2
 * note that the offsets in kmem_object also follow this rule.
 * this means that the offsets for kmem_object must fall in the
a146 1
#include <sys/kthread.h>
d165 1
d218 13
a581 2
	pmap_update(pmap_kernel());

a709 1
	pmap_update(map->pmap);
a913 154
}

/*
 * uvm_km_page allocator
 * This is a special allocator that uses a reserve of free pages
 * to fulfill requests.  It is fast and interrupt safe, but can only
 * return page sized regions.  Its primary use is as a backend for pool.
 *
 * The memory returned is allocated from the larger kernel_map, sparing
 * pressure on the small interrupt-safe kmem_map.  It is wired, but
 * not zero filled.
 */

int uvm_km_pages_lowat = 128; /* allocate more when reserve drops below this */
int uvm_km_pages_free; /* number of pages currently on free list */
struct km_page {
	struct km_page *next;
} *uvm_km_pages_head;

void uvm_km_createthread(void *);
void uvm_km_thread(void *);

/*
 * Allocate the initial reserve, and create the thread which will
 * keep the reserve full.  For bootstrapping, we allocate more than
 * the lowat amount, because it may be a while before the thread is
 * running.
 */
void
uvm_km_page_init(void)
{
	struct km_page *page;
	int i;


	for (i = 0; i < uvm_km_pages_lowat * 4; i++) {
#if defined(PMAP_MAP_POOLPAGE)
		struct vm_page *pg;
		vaddr_t va;

		pg = uvm_pagealloc(NULL, 0, NULL, 0);
		if (__predict_false(pg == NULL))
			break;

		va = PMAP_MAP_POOLPAGE(pg);
		if (__predict_false(va == 0)) {
			uvm_pagefree(pg);
			break;
		}
		page = (void *)va;
#else
		page = (void *)uvm_km_alloc(kernel_map, PAGE_SIZE);
#endif
		page->next = uvm_km_pages_head;
		uvm_km_pages_head = page;
	}
	uvm_km_pages_free = i;

	kthread_create_deferred(uvm_km_createthread, NULL);
}

void
uvm_km_createthread(void *arg)
{
	kthread_create(uvm_km_thread, NULL, NULL, "kmthread");
}

/*
 * Endless loop.  We grab pages in increments of 16 pages, then
 * quickly swap them into the list.  At some point we can consider
 * returning memory to the system if we have too many free pages,
 * but that's not implemented yet.
 */
void
uvm_km_thread(void *arg)
{
	struct km_page *head, *tail, *page;
	int i, s, want;

	for (;;) {
		if (uvm_km_pages_free >= uvm_km_pages_lowat)
			tsleep(&uvm_km_pages_head, PVM, "kmalloc", 0);
		want = 16;
		for (i = 0; i < want; i++) {
#if defined(PMAP_MAP_POOLPAGE)
			struct vm_page *pg;
			vaddr_t va;
	
			pg = uvm_pagealloc(NULL, 0, NULL, UVM_PGA_USERESERVE);
			if (__predict_false(pg == NULL))
				break;
			va = PMAP_MAP_POOLPAGE(pg);
			if (__predict_false(va == 0)) {
				uvm_pagefree(pg);
				break;
			}
			page = (void *)va;
#else
			page = (void *)uvm_km_alloc(kernel_map, PAGE_SIZE);
#endif
			if (i == 0)
				head = tail = page;
			page->next = head;
			head = page;
		}
		s = splvm();
		tail->next = uvm_km_pages_head;
		uvm_km_pages_head = head;
		uvm_km_pages_free += i;
		splx(s);
		wakeup(&uvm_km_pages_free);
	}
}


/*
 * Allocate one page.  We can sleep for more if the caller
 * permits it.  Wake up the thread if we've dropped below lowat.
 */
void *
uvm_km_getpage(boolean_t waitok)
{
	struct km_page *page = NULL;
	int s;

	s = splvm();
	for (;;) {
		page = uvm_km_pages_head;
		if (page) {
			uvm_km_pages_head = page->next;
			uvm_km_pages_free--;
			break;
		}
		if (!waitok)
			break;
		tsleep(&uvm_km_pages_free, PVM, "getpage", 0);
	}
	splx(s);
	if (uvm_km_pages_free < uvm_km_pages_lowat)
		wakeup(&uvm_km_pages_head);
	return (page);
}

void
uvm_km_putpage(void *v)
{
	struct km_page *page = v;
	int s;

	s = splvm();
	page->next = uvm_km_pages_head;
	uvm_km_pages_head = page;
	uvm_km_pages_free++;
	splx(s);
@


1.4.4.11
log
@sync with head, make i386 __HAVE_CPUINFO
@
text
@d817 2
a818 2
#if (defined(pmap_map_direct) || defined(pmap_unmap_direct)) && \
    (!defined(pmap_map_direct) || !defined(pmap_unmap_direct))
d835 1
a835 1
#if defined(pmap_map_direct)
d848 1
a848 1
	va = pmap_map_direct(pg);
d870 1
a870 1
#endif /* pmap_map_direct */
d885 2
a886 2
#if defined(pmap_unmap_direct)
	uvm_pagefree(pmap_unmap_direct(addr));
d903 1
a903 1
#endif /* pmap_unmap_direct */
a905 1
#if defined(pmap_map_direct)
d907 1
a907 28
 * uvm_km_page allocator, pmap_map_direct arch
 * On architectures with machine memory direct mapped into a portion
 * of KVM, we have very little work to do.  Just get a physical page,
 * and find and return its VA.  We use the poolpage functions for this.
 */
void
uvm_km_page_init(void)
{
	/* nothing */
}

void *
uvm_km_getpage(boolean_t waitok)
{

	return ((void *)uvm_km_alloc_poolpage1(NULL, NULL, waitok));
}

void
uvm_km_putpage(void *v)
{

	uvm_km_free_poolpage1(NULL, (vaddr_t)v);
}

#else
/*
 * uvm_km_page allocator, non pmap_map_direct archs
d940 15
d956 1
d988 14
d1003 1
a1058 1
#endif
@


1.3
log
@sync with NetBSD from 1999.05.24 (there is a reason for this date)
 Mostly cleanups, but also a few improvements to pagedaemon for better
 handling of low memory and/or low swap conditions.
@
text
@d732 1
a732 1
		    UVM_PROT_ALL, TRUE);
d860 7
a866 7
		/* map it in */
#if defined(PMAP_NEW)
		pmap_kenter_pa(loopva, VM_PAGE_TO_PHYS(pg), UVM_PROT_ALL);
#else
		pmap_enter(map->pmap, loopva, VM_PAGE_TO_PHYS(pg),
		    UVM_PROT_ALL, TRUE);
#endif
@


1.2
log
@add OpenBSD tags
@
text
@d1 1
a1 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_km.c,v 1.18 1998/10/18 23:49:59 chs Exp $	*/
a2 4
/*
 * XXXCDC: "ROUGH DRAFT" QUALITY UVM PRE-RELEASE FILE!
 *         >>>USE AT YOUR OWN RISK, WORK IS NOT FINISHED<<<
 */
a174 1
	NULL, /* attach */
d244 1
a244 1
				    NULL);
d334 1
a334 1
				    NULL);	/* alloc */
d575 1
a575 6
				int slot = uao_set_swslot(uobj,
							  curoff >> PAGE_SHIFT,
							  0);

				if (slot)
					uvm_swap_free(slot, 1);
d610 1
a610 5
				int slot = uao_set_swslot(uobj,
						pp->offset >> PAGE_SHIFT, 0);

				if (slot)
					uvm_swap_free(slot, 1);
d700 1
a700 1
		pg = uvm_pagealloc(obj, offset, NULL);
d849 1
a849 1
		pg = uvm_pagealloc(uvm.kernel_object, offset, NULL);
d1000 1
a1000 1
	pg = uvm_pagealloc(NULL, 0, NULL);
@


1.1
log
@Import of uvm from NetBSD. Some local changes, some code disabled
@
text
@d1 1
@

