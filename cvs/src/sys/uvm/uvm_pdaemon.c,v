head	1.78;
access;
symbols
	OPENBSD_6_2:1.78.0.2
	OPENBSD_6_2_BASE:1.78
	OPENBSD_6_1:1.78.0.4
	OPENBSD_6_1_BASE:1.78
	OPENBSD_6_0:1.77.0.4
	OPENBSD_6_0_BASE:1.77
	OPENBSD_5_9:1.77.0.2
	OPENBSD_5_9_BASE:1.77
	OPENBSD_5_8:1.75.0.6
	OPENBSD_5_8_BASE:1.75
	OPENBSD_5_7:1.75.0.2
	OPENBSD_5_7_BASE:1.75
	OPENBSD_5_6:1.71.0.4
	OPENBSD_5_6_BASE:1.71
	OPENBSD_5_5:1.66.0.4
	OPENBSD_5_5_BASE:1.66
	OPENBSD_5_4:1.64.0.2
	OPENBSD_5_4_BASE:1.64
	OPENBSD_5_3:1.62.0.2
	OPENBSD_5_3_BASE:1.62
	OPENBSD_5_2:1.59.0.6
	OPENBSD_5_2_BASE:1.59
	OPENBSD_5_1_BASE:1.59
	OPENBSD_5_1:1.59.0.4
	OPENBSD_5_0:1.59.0.2
	OPENBSD_5_0_BASE:1.59
	OPENBSD_4_9:1.56.0.2
	OPENBSD_4_9_BASE:1.56
	OPENBSD_4_8:1.55.0.4
	OPENBSD_4_8_BASE:1.55
	OPENBSD_4_7:1.55.0.2
	OPENBSD_4_7_BASE:1.55
	OPENBSD_4_6:1.51.0.4
	OPENBSD_4_6_BASE:1.51
	OPENBSD_4_5:1.36.0.2
	OPENBSD_4_5_BASE:1.36
	OPENBSD_4_4:1.35.0.2
	OPENBSD_4_4_BASE:1.35
	OPENBSD_4_3:1.34.0.2
	OPENBSD_4_3_BASE:1.34
	OPENBSD_4_2:1.33.0.2
	OPENBSD_4_2_BASE:1.33
	OPENBSD_4_1:1.30.0.4
	OPENBSD_4_1_BASE:1.30
	OPENBSD_4_0:1.30.0.2
	OPENBSD_4_0_BASE:1.30
	OPENBSD_3_9:1.25.0.16
	OPENBSD_3_9_BASE:1.25
	OPENBSD_3_8:1.25.0.14
	OPENBSD_3_8_BASE:1.25
	OPENBSD_3_7:1.25.0.12
	OPENBSD_3_7_BASE:1.25
	OPENBSD_3_6:1.25.0.10
	OPENBSD_3_6_BASE:1.25
	SMP_SYNC_A:1.25
	SMP_SYNC_B:1.25
	OPENBSD_3_5:1.25.0.8
	OPENBSD_3_5_BASE:1.25
	OPENBSD_3_4:1.25.0.6
	OPENBSD_3_4_BASE:1.25
	UBC_SYNC_A:1.25
	OPENBSD_3_3:1.25.0.4
	OPENBSD_3_3_BASE:1.25
	OPENBSD_3_2:1.25.0.2
	OPENBSD_3_2_BASE:1.25
	OPENBSD_3_1:1.24.0.2
	OPENBSD_3_1_BASE:1.24
	UBC_SYNC_B:1.25
	UBC:1.20.0.2
	UBC_BASE:1.20
	OPENBSD_3_0:1.13.0.2
	OPENBSD_3_0_BASE:1.13
	OPENBSD_2_9_BASE:1.9
	OPENBSD_2_9:1.9.0.2
	OPENBSD_2_8:1.5.0.4
	OPENBSD_2_8_BASE:1.5
	OPENBSD_2_7:1.5.0.2
	OPENBSD_2_7_BASE:1.5
	SMP:1.4.0.2
	SMP_BASE:1.4
	kame_19991208:1.3
	OPENBSD_2_6:1.3.0.2
	OPENBSD_2_6_BASE:1.3
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.78
date	2017.02.14.10.31.15;	author mpi;	state Exp;
branches;
next	1.77;
commitid	PmGi4EGraGC0Z0ml;

1.77
date	2015.10.08.15.58.38;	author kettenis;	state Exp;
branches;
next	1.76;
commitid	wmnQpRj0rGJ31TDS;

1.76
date	2015.08.21.16.04.35;	author visa;	state Exp;
branches;
next	1.75;
commitid	gglpDr80UKmkkP9A;

1.75
date	2014.12.17.19.42.15;	author tedu;	state Exp;
branches;
next	1.74;
commitid	G4ldVK4QwvfU3tRp;

1.74
date	2014.11.16.12.31.00;	author deraadt;	state Exp;
branches;
next	1.73;
commitid	yv0ECmCdICvq576h;

1.73
date	2014.09.14.14.17.27;	author jsg;	state Exp;
branches;
next	1.72;
commitid	uzzBR7hz9ncd4O6G;

1.72
date	2014.09.09.07.07.39;	author blambert;	state Exp;
branches;
next	1.71;
commitid	R0IvGgmM8zlXVXKS;

1.71
date	2014.07.12.09.02.24;	author kettenis;	state Exp;
branches;
next	1.70;
commitid	PNB6VkI4aO2bcc3c;

1.70
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.69;
commitid	7NtJNW9udCOFtDNM;

1.69
date	2014.07.08.13.10.52;	author deraadt;	state Exp;
branches;
next	1.68;
commitid	NlK85xqFeQYasUwv;

1.68
date	2014.07.08.11.38.48;	author deraadt;	state Exp;
branches;
next	1.67;
commitid	myoIhq56iLBOzmdA;

1.67
date	2014.04.13.23.14.15;	author tedu;	state Exp;
branches;
next	1.66;

1.66
date	2014.02.06.16.40.40;	author tedu;	state Exp;
branches;
next	1.65;

1.65
date	2014.02.06.16.32.40;	author tedu;	state Exp;
branches;
next	1.64;

1.64
date	2013.05.30.16.29.46;	author tedu;	state Exp;
branches;
next	1.63;

1.63
date	2013.05.30.15.17.59;	author tedu;	state Exp;
branches;
next	1.62;

1.62
date	2013.02.07.17.29.31;	author beck;	state Exp;
branches;
next	1.61;

1.61
date	2012.12.10.22.42.54;	author beck;	state Exp;
branches;
next	1.60;

1.60
date	2012.11.07.17.50.49;	author beck;	state Exp;
branches;
next	1.59;

1.59
date	2011.07.06.19.50.38;	author beck;	state Exp;
branches;
next	1.58;

1.58
date	2011.07.03.18.34.14;	author oga;	state Exp;
branches;
next	1.57;

1.57
date	2011.04.01.12.58.13;	author krw;	state Exp;
branches;
next	1.56;

1.56
date	2010.09.26.12.53.27;	author thib;	state Exp;
branches;
next	1.55;

1.55
date	2009.10.14.17.53.30;	author beck;	state Exp;
branches;
next	1.54;

1.54
date	2009.08.08.13.43.59;	author beck;	state Exp;
branches;
next	1.53;

1.53
date	2009.08.02.16.28.40;	author beck;	state Exp;
branches;
next	1.52;

1.52
date	2009.07.22.21.05.37;	author oga;	state Exp;
branches;
next	1.51;

1.51
date	2009.06.26.20.26.02;	author oga;	state Exp;
branches;
next	1.50;

1.50
date	2009.06.17.00.13.59;	author oga;	state Exp;
branches;
next	1.49;

1.49
date	2009.06.16.00.11.29;	author oga;	state Exp;
branches;
next	1.48;

1.48
date	2009.06.15.17.01.26;	author beck;	state Exp;
branches;
next	1.47;

1.47
date	2009.06.06.23.35.08;	author art;	state Exp;
branches;
next	1.46;

1.46
date	2009.06.05.04.29.14;	author beck;	state Exp;
branches;
next	1.45;

1.45
date	2009.06.01.19.54.02;	author oga;	state Exp;
branches;
next	1.44;

1.44
date	2009.05.08.13.50.15;	author ariane;	state Exp;
branches;
next	1.43;

1.43
date	2009.05.04.18.08.06;	author oga;	state Exp;
branches;
next	1.42;

1.42
date	2009.04.17.07.14.04;	author oga;	state Exp;
branches;
next	1.41;

1.41
date	2009.04.15.12.43.07;	author oga;	state Exp;
branches;
next	1.40;

1.40
date	2009.04.14.20.12.05;	author oga;	state Exp;
branches;
next	1.39;

1.39
date	2009.04.13.22.17.54;	author oga;	state Exp;
branches;
next	1.38;

1.38
date	2009.04.06.12.02.52;	author oga;	state Exp;
branches;
next	1.37;

1.37
date	2009.03.20.15.19.04;	author oga;	state Exp;
branches;
next	1.36;

1.36
date	2009.01.12.19.03.12;	author miod;	state Exp;
branches;
next	1.35;

1.35
date	2008.07.02.15.21.33;	author art;	state Exp;
branches;
next	1.34;

1.34
date	2007.12.18.11.05.52;	author thib;	state Exp;
branches;
next	1.33;

1.33
date	2007.06.18.21.51.15;	author pedro;	state Exp;
branches;
next	1.32;

1.32
date	2007.04.13.18.57.49;	author art;	state Exp;
branches;
next	1.31;

1.31
date	2007.04.04.17.44.45;	author art;	state Exp;
branches;
next	1.30;

1.30
date	2006.07.31.11.51.29;	author mickey;	state Exp;
branches;
next	1.29;

1.29
date	2006.07.13.22.51.26;	author deraadt;	state Exp;
branches;
next	1.28;

1.28
date	2006.06.21.16.20.05;	author mickey;	state Exp;
branches;
next	1.27;

1.27
date	2006.05.16.08.34.42;	author mickey;	state Exp;
branches;
next	1.26;

1.26
date	2006.05.07.20.06.50;	author tedu;	state Exp;
branches;
next	1.25;

1.25
date	2002.05.24.13.10.53;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2002.03.14.01.27.18;	author millert;	state Exp;
branches;
next	1.23;

1.23
date	2002.02.10.23.15.05;	author deraadt;	state Exp;
branches;
next	1.22;

1.22
date	2002.01.02.22.23.25;	author miod;	state Exp;
branches;
next	1.21;

1.21
date	2001.12.19.08.58.07;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2001.11.28.19.28.15;	author art;	state Exp;
branches
	1.20.2.1;
next	1.19;

1.19
date	2001.11.28.13.47.40;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2001.11.12.01.26.10;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2001.11.10.18.42.31;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2001.11.06.13.36.52;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2001.11.06.01.35.04;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2001.11.05.22.14.54;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2001.09.11.20.05.26;	author miod;	state Exp;
branches;
next	1.12;

1.12
date	2001.08.11.10.57.22;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2001.07.18.14.27.07;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2001.07.18.10.47.05;	author art;	state Exp;
branches;
next	1.9;

1.9
date	2001.03.22.03.05.56;	author smart;	state Exp;
branches;
next	1.8;

1.8
date	2001.03.08.15.21.37;	author smart;	state Exp;
branches;
next	1.7;

1.7
date	2001.01.29.02.07.48;	author niklas;	state Exp;
branches;
next	1.6;

1.6
date	2000.11.10.15.33.11;	author provos;	state Exp;
branches;
next	1.5;

1.5
date	2000.03.16.22.11.05;	author art;	state Exp;
branches;
next	1.4;

1.4
date	99.12.30.18.21.55;	author provos;	state Exp;
branches
	1.4.2.1;
next	1.3;

1.3
date	99.08.23.08.13.25;	author art;	state Exp;
branches;
next	1.2;

1.2
date	99.02.26.05.32.08;	author art;	state Exp;
branches;
next	1.1;

1.1
date	99.02.26.01.30.17;	author art;	state Exp;
branches;
next	;

1.4.2.1
date	2000.03.24.09.09.51;	author niklas;	state Exp;
branches;
next	1.4.2.2;

1.4.2.2
date	2001.05.14.22.47.47;	author niklas;	state Exp;
branches;
next	1.4.2.3;

1.4.2.3
date	2001.10.31.03.32.14;	author nate;	state Exp;
branches;
next	1.4.2.4;

1.4.2.4
date	2001.11.13.23.02.31;	author niklas;	state Exp;
branches;
next	1.4.2.5;

1.4.2.5
date	2001.12.05.01.23.58;	author niklas;	state Exp;
branches;
next	1.4.2.6;

1.4.2.6
date	2002.03.06.02.17.14;	author niklas;	state Exp;
branches;
next	1.4.2.7;

1.4.2.7
date	2002.03.28.14.54.27;	author niklas;	state Exp;
branches;
next	1.4.2.8;

1.4.2.8
date	2003.03.28.00.08.48;	author niklas;	state Exp;
branches;
next	;

1.20.2.1
date	2002.01.31.22.55.51;	author niklas;	state Exp;
branches;
next	1.20.2.2;

1.20.2.2
date	2002.02.02.03.28.27;	author art;	state Exp;
branches;
next	1.20.2.3;

1.20.2.3
date	2002.06.11.03.33.04;	author art;	state Exp;
branches;
next	1.20.2.4;

1.20.2.4
date	2002.11.04.18.02.33;	author art;	state Exp;
branches;
next	1.20.2.5;

1.20.2.5
date	2004.02.21.00.20.22;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.78
log
@Convert most of the manual checks for CPU hogging to sched_pause().

The distinction between preempt() and yield() stays as it is usueful
to know if a thread decided to yield by itself or if the kernel told
him to go away.

ok tedu@@, guenther@@
@
text
@/*	$OpenBSD: uvm_pdaemon.c,v 1.77 2015/10/08 15:58:38 kettenis Exp $	*/
/*	$NetBSD: uvm_pdaemon.c,v 1.23 2000/08/20 10:24:14 bjh21 Exp $	*/

/* 
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
 *
 * All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * The Mach Operating System project at Carnegie-Mellon University.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)vm_pageout.c        8.5 (Berkeley) 2/14/94
 * from: Id: uvm_pdaemon.c,v 1.1.2.32 1998/02/06 05:26:30 chs Exp
 *
 *
 * Copyright (c) 1987, 1990 Carnegie-Mellon University.
 * All rights reserved.
 * 
 * Permission to use, copy, modify and distribute this software and
 * its documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
 * FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
 * 
 * Carnegie Mellon requests users of this software to return to
 *
 *  Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
 *  School of Computer Science
 *  Carnegie Mellon University
 *  Pittsburgh PA 15213-3890
 *
 * any improvements or extensions that they make and grant Carnegie the
 * rights to redistribute these changes.
 */

/*
 * uvm_pdaemon.c: the page daemon
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/kernel.h>
#include <sys/pool.h>
#include <sys/buf.h>
#include <sys/mount.h>
#include <sys/atomic.h>

#ifdef HIBERNATE
#include <sys/hibernate.h>
#endif

#include <uvm/uvm.h>

/*
 * UVMPD_NUMDIRTYREACTS is how many dirty pages the pagedaemon will reactivate
 * in a pass thru the inactive list when swap is full.  the value should be
 * "small"... if it's too large we'll cycle the active pages thru the inactive
 * queue too quickly to for them to be referenced and avoid being freed.
 */

#define UVMPD_NUMDIRTYREACTS 16


/*
 * local prototypes
 */

void		uvmpd_scan(void);
boolean_t	uvmpd_scan_inactive(struct pglist *);
void		uvmpd_tune(void);
void		uvmpd_drop(struct pglist *);

/*
 * uvm_wait: wait (sleep) for the page daemon to free some pages
 *
 * => should be called with all locks released
 * => should _not_ be called by the page daemon (to avoid deadlock)
 */

void
uvm_wait(const char *wmsg)
{
	int	timo = 0;

	/* check for page daemon going to sleep (waiting for itself) */
	if (curproc == uvm.pagedaemon_proc) {
		printf("uvm_wait emergency bufbackoff\n");
		if (bufbackoff(NULL, 4) == 0)
			return;
		/*
		 * now we have a problem: the pagedaemon wants to go to
		 * sleep until it frees more memory.   but how can it
		 * free more memory if it is asleep?  that is a deadlock.
		 * we have two options:
		 *  [1] panic now
		 *  [2] put a timeout on the sleep, thus causing the
		 *      pagedaemon to only pause (rather than sleep forever)
		 *
		 * note that option [2] will only help us if we get lucky
		 * and some other process on the system breaks the deadlock
		 * by exiting or freeing memory (thus allowing the pagedaemon
		 * to continue).  for now we panic if DEBUG is defined,
		 * otherwise we hope for the best with option [2] (better
		 * yet, this should never happen in the first place!).
		 */

		printf("pagedaemon: deadlock detected!\n");
		timo = hz >> 3;		/* set timeout */
#if defined(DEBUG)
		/* DEBUG: panic so we can debug it */
		panic("pagedaemon deadlock");
#endif
	}

	uvm_lock_fpageq();
	wakeup(&uvm.pagedaemon);		/* wake the daemon! */
	msleep(&uvmexp.free, &uvm.fpageqlock, PVM | PNORELOCK, wmsg, timo);
}

/*
 * uvmpd_tune: tune paging parameters
 *
 * => called whenever memory is added to (or removed from?) the system
 * => caller must call with page queues locked
 */

void
uvmpd_tune(void)
{

	uvmexp.freemin = uvmexp.npages / 30;

	/* between 16k and 512k */
	/* XXX:  what are these values good for? */
	uvmexp.freemin = max(uvmexp.freemin, (16*1024) >> PAGE_SHIFT);
#if 0
	uvmexp.freemin = min(uvmexp.freemin, (512*1024) >> PAGE_SHIFT);
#endif

	/* Make sure there's always a user page free. */
	if (uvmexp.freemin < uvmexp.reserve_kernel + 1)
		uvmexp.freemin = uvmexp.reserve_kernel + 1;

	uvmexp.freetarg = (uvmexp.freemin * 4) / 3;
	if (uvmexp.freetarg <= uvmexp.freemin)
		uvmexp.freetarg = uvmexp.freemin + 1;

	/* uvmexp.inactarg: computed in main daemon loop */

	uvmexp.wiredmax = uvmexp.npages / 3;
}

/*
 * uvm_pageout: the main loop for the pagedaemon
 */
void
uvm_pageout(void *arg)
{
	struct uvm_constraint_range constraint;
	struct uvm_pmalloc *pma;
	int work_done;
	int npages = 0;

	/* ensure correct priority and set paging parameters... */
	uvm.pagedaemon_proc = curproc;
	(void) spl0();
	uvm_lock_pageq();
	npages = uvmexp.npages;
	uvmpd_tune();
	uvm_unlock_pageq();

	for (;;) {
		long size;
	  	work_done = 0; /* No work done this iteration. */

		uvm_lock_fpageq();

		if (TAILQ_EMPTY(&uvm.pmr_control.allocs)) {
			msleep(&uvm.pagedaemon, &uvm.fpageqlock, PVM,
			    "pgdaemon", 0);
			uvmexp.pdwoke++;
		}

		if ((pma = TAILQ_FIRST(&uvm.pmr_control.allocs)) != NULL) {
			pma->pm_flags |= UVM_PMA_BUSY;
			constraint = pma->pm_constraint;
		} else
			constraint = no_constraint;

		uvm_unlock_fpageq();

		/* now lock page queues and recompute inactive count */
		uvm_lock_pageq();
		if (npages != uvmexp.npages) {	/* check for new pages? */
			npages = uvmexp.npages;
			uvmpd_tune();
		}

		uvmexp.inactarg = (uvmexp.active + uvmexp.inactive) / 3;
		if (uvmexp.inactarg <= uvmexp.freetarg) {
			uvmexp.inactarg = uvmexp.freetarg + 1;
		}

		/* Reclaim pages from the buffer cache if possible. */
		size = 0;
		if (pma != NULL)
			size += pma->pm_size >> PAGE_SHIFT;
		if (uvmexp.free - BUFPAGES_DEFICIT < uvmexp.freetarg)
			size += uvmexp.freetarg - (uvmexp.free -
			    BUFPAGES_DEFICIT);
		uvm_unlock_pageq();
		(void) bufbackoff(&constraint, size * 2);
		uvm_lock_pageq();

		/* Scan if needed to meet our targets. */
		if (pma != NULL ||
		    ((uvmexp.free - BUFPAGES_DEFICIT) < uvmexp.freetarg) ||
		    ((uvmexp.inactive + BUFPAGES_INACT) < uvmexp.inactarg)) {
			uvmpd_scan();
			work_done = 1; /* XXX we hope... */
		}

		/*
		 * if there's any free memory to be had,
		 * wake up any waiters.
		 */
		uvm_lock_fpageq();
		if (uvmexp.free > uvmexp.reserve_kernel ||
		    uvmexp.paging == 0) {
			wakeup(&uvmexp.free);
		}

		if (pma != NULL) {
			pma->pm_flags &= ~UVM_PMA_BUSY;
			if (!work_done)
				pma->pm_flags |= UVM_PMA_FAIL;
			if (pma->pm_flags & (UVM_PMA_FAIL | UVM_PMA_FREED)) {
				pma->pm_flags &= ~UVM_PMA_LINKED;
				TAILQ_REMOVE(&uvm.pmr_control.allocs, pma,
				    pmq);
			}
			wakeup(pma);
		}
		uvm_unlock_fpageq();

		/* scan done. unlock page queues (only lock we are holding) */
		uvm_unlock_pageq();

		sched_pause(yield);
	}
	/*NOTREACHED*/
}


/*
 * uvm_aiodone_daemon:  main loop for the aiodone daemon.
 */
void
uvm_aiodone_daemon(void *arg)
{
	int s, free;
	struct buf *bp, *nbp;

	uvm.aiodoned_proc = curproc;

	for (;;) {
		/*
		 * Check for done aio structures. If we've got structures to
		 * process, do so. Otherwise sleep while avoiding races.
		 */
		mtx_enter(&uvm.aiodoned_lock);
		while ((bp = TAILQ_FIRST(&uvm.aio_done)) == NULL)
			msleep(&uvm.aiodoned, &uvm.aiodoned_lock,
			    PVM, "aiodoned", 0);
		/* Take the list for ourselves. */
		TAILQ_INIT(&uvm.aio_done);
		mtx_leave(&uvm.aiodoned_lock);

		/* process each i/o that's done. */
		free = uvmexp.free;
		while (bp != NULL) {
			if (bp->b_flags & B_PDAEMON) {
				uvmexp.paging -= bp->b_bufsize >> PAGE_SHIFT;
			}
			nbp = TAILQ_NEXT(bp, b_freelist);
			s = splbio();	/* b_iodone must by called at splbio */
			(*bp->b_iodone)(bp);
			splx(s);
			bp = nbp;

			sched_pause(yield);
		}
		uvm_lock_fpageq();
		wakeup(free <= uvmexp.reserve_kernel ? &uvm.pagedaemon :
		    &uvmexp.free);
		uvm_unlock_fpageq();
	}
}



/*
 * uvmpd_scan_inactive: scan an inactive list for pages to clean or free.
 *
 * => called with page queues locked
 * => we work on meeting our free target by converting inactive pages
 *    into free pages.
 * => we handle the building of swap-backed clusters
 * => we return TRUE if we are exiting because we met our target
 */

boolean_t
uvmpd_scan_inactive(struct pglist *pglst)
{
	boolean_t retval = FALSE;	/* assume we haven't hit target */
	int free, result;
	struct vm_page *p, *nextpg;
	struct uvm_object *uobj;
	struct vm_page *pps[MAXBSIZE >> PAGE_SHIFT], **ppsp;
	int npages;
	struct vm_page *swpps[MAXBSIZE >> PAGE_SHIFT]; 	/* XXX: see below */
	int swnpages, swcpages;				/* XXX: see below */
	int swslot;
	struct vm_anon *anon;
	boolean_t swap_backed;
	vaddr_t start;
	int dirtyreacts;

	/*
	 * note: we currently keep swap-backed pages on a separate inactive
	 * list from object-backed pages.   however, merging the two lists
	 * back together again hasn't been ruled out.   thus, we keep our
	 * swap cluster in "swpps" rather than in pps (allows us to mix
	 * clustering types in the event of a mixed inactive queue).
	 */
	/*
	 * swslot is non-zero if we are building a swap cluster.  we want
	 * to stay in the loop while we have a page to scan or we have
	 * a swap-cluster to build.
	 */
	swslot = 0;
	swnpages = swcpages = 0;
	free = 0;
	dirtyreacts = 0;

	for (p = TAILQ_FIRST(pglst); p != NULL || swslot != 0; p = nextpg) {
		/*
		 * note that p can be NULL iff we have traversed the whole
		 * list and need to do one final swap-backed clustered pageout.
		 */
		uobj = NULL;
		anon = NULL;

		if (p) {
			/*
			 * update our copy of "free" and see if we've met
			 * our target
			 */
			free = uvmexp.free - BUFPAGES_DEFICIT;

			if (free + uvmexp.paging >= uvmexp.freetarg << 2 ||
			    dirtyreacts == UVMPD_NUMDIRTYREACTS) {
				retval = TRUE;

				if (swslot == 0) {
					/* exit now if no swap-i/o pending */
					break;
				}

				/* set p to null to signal final swap i/o */
				p = NULL;
			}
		}

		if (p) {	/* if (we have a new page to consider) */
			/*
			 * we are below target and have a new page to consider.
			 */
			uvmexp.pdscans++;
			nextpg = TAILQ_NEXT(p, pageq);

			/*
			 * move referenced pages back to active queue and
			 * skip to next page (unlikely to happen since
			 * inactive pages shouldn't have any valid mappings
			 * and we cleared reference before deactivating).
			 */

			if (pmap_is_referenced(p)) {
				uvm_pageactivate(p);
				uvmexp.pdreact++;
				continue;
			}

			if (p->pg_flags & PQ_ANON) {
				anon = p->uanon;
				KASSERT(anon != NULL);
				if (p->pg_flags & PG_BUSY) {
					uvmexp.pdbusy++;
					/* someone else owns page, skip it */
					continue;
				}
				uvmexp.pdanscan++;
			} else {
				uobj = p->uobject;
				KASSERT(uobj != NULL);
				if (p->pg_flags & PG_BUSY) {
					uvmexp.pdbusy++;
					/* someone else owns page, skip it */
					continue;
				}
				uvmexp.pdobscan++;
			}

			/*
			 * we now have the page queues locked.
			 * the page is not busy.   if the page is clean we
			 * can free it now and continue.
			 */
			if (p->pg_flags & PG_CLEAN) {
				if (p->pg_flags & PQ_SWAPBACKED) {
					/* this page now lives only in swap */
					uvmexp.swpgonly++;
				}

				/* zap all mappings with pmap_page_protect... */
				pmap_page_protect(p, PROT_NONE);
				uvm_pagefree(p);
				uvmexp.pdfreed++;

				if (anon) {

					/*
					 * an anonymous page can only be clean
					 * if it has backing store assigned.
					 */

					KASSERT(anon->an_swslot != 0);

					/* remove from object */
					anon->an_page = NULL;
				}
				continue;
			}

			/*
			 * this page is dirty, skip it if we'll have met our
			 * free target when all the current pageouts complete.
			 */
			if (free + uvmexp.paging > uvmexp.freetarg << 2) {
				continue;
			}

			/*
			 * this page is dirty, but we can't page it out
			 * since all pages in swap are only in swap.
			 * reactivate it so that we eventually cycle
			 * all pages thru the inactive queue.
			 */
			KASSERT(uvmexp.swpgonly <= uvmexp.swpages);
			if ((p->pg_flags & PQ_SWAPBACKED) &&
			    uvmexp.swpgonly == uvmexp.swpages) {
				dirtyreacts++;
				uvm_pageactivate(p);
				continue;
			}

			/*
			 * if the page is swap-backed and dirty and swap space
			 * is full, free any swap allocated to the page
			 * so that other pages can be paged out.
			 */
			KASSERT(uvmexp.swpginuse <= uvmexp.swpages);
			if ((p->pg_flags & PQ_SWAPBACKED) &&
			    uvmexp.swpginuse == uvmexp.swpages) {

				if ((p->pg_flags & PQ_ANON) &&
				    p->uanon->an_swslot) {
					uvm_swap_free(p->uanon->an_swslot, 1);
					p->uanon->an_swslot = 0;
				}
				if (p->pg_flags & PQ_AOBJ) {
					uao_dropswap(p->uobject,
						     p->offset >> PAGE_SHIFT);
				}
			}

			/*
			 * the page we are looking at is dirty.   we must
			 * clean it before it can be freed.  to do this we
			 * first mark the page busy so that no one else will
			 * touch the page.   we write protect all the mappings
			 * of the page so that no one touches it while it is
			 * in I/O.
			 */

			swap_backed = ((p->pg_flags & PQ_SWAPBACKED) != 0);
			atomic_setbits_int(&p->pg_flags, PG_BUSY);
			UVM_PAGE_OWN(p, "scan_inactive");
			pmap_page_protect(p, PROT_READ);
			uvmexp.pgswapout++;

			/*
			 * for swap-backed pages we need to (re)allocate
			 * swap space.
			 */
			if (swap_backed) {
				/* free old swap slot (if any) */
				if (anon) {
					if (anon->an_swslot) {
						uvm_swap_free(anon->an_swslot,
						    1);
						anon->an_swslot = 0;
					}
				} else {
					uao_dropswap(uobj,
						     p->offset >> PAGE_SHIFT);
				}

				/* start new cluster (if necessary) */
				if (swslot == 0) {
					swnpages = MAXBSIZE >> PAGE_SHIFT;
					swslot = uvm_swap_alloc(&swnpages,
					    TRUE);
					if (swslot == 0) {
						/* no swap?  give up! */
						atomic_clearbits_int(
						    &p->pg_flags,
						    PG_BUSY);
						UVM_PAGE_OWN(p, NULL);
						continue;
					}
					swcpages = 0;	/* cluster is empty */
				}

				/* add block to cluster */
				swpps[swcpages] = p;
				if (anon)
					anon->an_swslot = swslot + swcpages;
				else
					uao_set_swslot(uobj,
					    p->offset >> PAGE_SHIFT,
					    swslot + swcpages);
				swcpages++;
			}
		} else {
			/* if p == NULL we must be doing a last swap i/o */
			swap_backed = TRUE;
		}

		/*
		 * now consider doing the pageout.
		 *
		 * for swap-backed pages, we do the pageout if we have either
		 * filled the cluster (in which case (swnpages == swcpages) or
		 * run out of pages (p == NULL).
		 *
		 * for object pages, we always do the pageout.
		 */
		if (swap_backed) {
			if (p) {	/* if we just added a page to cluster */
				/* cluster not full yet? */
				if (swcpages < swnpages)
					continue;
			}

			/* starting I/O now... set up for it */
			npages = swcpages;
			ppsp = swpps;
			/* for swap-backed pages only */
			start = (vaddr_t) swslot;

			/* if this is final pageout we could have a few
			 * extra swap blocks */
			if (swcpages < swnpages) {
				uvm_swap_free(swslot + swcpages,
				    (swnpages - swcpages));
			}
		} else {
			/* normal object pageout */
			ppsp = pps;
			npages = sizeof(pps) / sizeof(struct vm_page *);
			/* not looked at because PGO_ALLPAGES is set */
			start = 0;
		}

		/*
		 * now do the pageout.
		 *
		 * for swap_backed pages we have already built the cluster.
		 * for !swap_backed pages, uvm_pager_put will call the object's
		 * "make put cluster" function to build a cluster on our behalf.
		 *
		 * we pass the PGO_PDFREECLUST flag to uvm_pager_put to instruct
		 * it to free the cluster pages for us on a successful I/O (it
		 * always does this for un-successful I/O requests).  this
		 * allows us to do clustered pageout without having to deal
		 * with cluster pages at this level.
		 *
		 * note locking semantics of uvm_pager_put with PGO_PDFREECLUST:
		 *  IN: locked: page queues
		 * OUT: locked: 
		 *     !locked: pageqs
		 */

		uvmexp.pdpageouts++;
		result = uvm_pager_put(swap_backed ? NULL : uobj, p,
		    &ppsp, &npages, PGO_ALLPAGES|PGO_PDFREECLUST, start, 0);

		/*
		 * if we did i/o to swap, zero swslot to indicate that we are
		 * no longer building a swap-backed cluster.
		 */

		if (swap_backed)
			swslot = 0;		/* done with this cluster */

		/*
		 * first, we check for VM_PAGER_PEND which means that the
		 * async I/O is in progress and the async I/O done routine
		 * will clean up after us.   in this case we move on to the
		 * next page.
		 *
		 * there is a very remote chance that the pending async i/o can
		 * finish _before_ we get here.   if that happens, our page "p"
		 * may no longer be on the inactive queue.   so we verify this
		 * when determining the next page (starting over at the head if
		 * we've lost our inactive page).
		 */

		if (result == VM_PAGER_PEND) {
			uvmexp.paging += npages;
			uvm_lock_pageq();
			uvmexp.pdpending++;
			if (p) {
				if (p->pg_flags & PQ_INACTIVE)
					nextpg = TAILQ_NEXT(p, pageq);
				else
					nextpg = TAILQ_FIRST(pglst);
			} else {
				nextpg = NULL;
			}
			continue;
		}

		/* clean up "p" if we have one */
		if (p) {
			/*
			 * the I/O request to "p" is done and uvm_pager_put
			 * has freed any cluster pages it may have allocated
			 * during I/O.  all that is left for us to do is
			 * clean up page "p" (which is still PG_BUSY).
			 *
			 * our result could be one of the following:
			 *   VM_PAGER_OK: successful pageout
			 *
			 *   VM_PAGER_AGAIN: tmp resource shortage, we skip
			 *     to next page
			 *   VM_PAGER_{FAIL,ERROR,BAD}: an error.   we
			 *     "reactivate" page to get it out of the way (it
			 *     will eventually drift back into the inactive
			 *     queue for a retry).
			 *   VM_PAGER_UNLOCK: should never see this as it is
			 *     only valid for "get" operations
			 */

			/* relock p's object: page queues not lock yet, so
			 * no need for "try" */

#ifdef DIAGNOSTIC
			if (result == VM_PAGER_UNLOCK)
				panic("pagedaemon: pageout returned "
				    "invalid 'unlock' code");
#endif

			/* handle PG_WANTED now */
			if (p->pg_flags & PG_WANTED)
				wakeup(p);

			atomic_clearbits_int(&p->pg_flags, PG_BUSY|PG_WANTED);
			UVM_PAGE_OWN(p, NULL);

			/* released during I/O? Can only happen for anons */
			if (p->pg_flags & PG_RELEASED) {
				KASSERT(anon != NULL);
				/*
				 * remove page so we can get nextpg,
				 * also zero out anon so we don't use
				 * it after the free.
				 */
				anon->an_page = NULL;
				p->uanon = NULL;

				uvm_anfree(anon);	/* kills anon */
				pmap_page_protect(p, PROT_NONE);
				anon = NULL;
				uvm_lock_pageq();
				nextpg = TAILQ_NEXT(p, pageq);
				/* free released page */
				uvm_pagefree(p);
			} else {	/* page was not released during I/O */
				uvm_lock_pageq();
				nextpg = TAILQ_NEXT(p, pageq);
				if (result != VM_PAGER_OK) {
					/* pageout was a failure... */
					if (result != VM_PAGER_AGAIN)
						uvm_pageactivate(p);
					pmap_clear_reference(p);
					/* XXXCDC: if (swap_backed) FREE p's
					 * swap block? */
				} else {
					/* pageout was a success... */
					pmap_clear_reference(p);
					pmap_clear_modify(p);
					atomic_setbits_int(&p->pg_flags,
					    PG_CLEAN);
				}
			}

			/*
			 * drop object lock (if there is an object left).   do
			 * a safety check of nextpg to make sure it is on the
			 * inactive queue (it should be since PG_BUSY pages on
			 * the inactive queue can't be re-queued [note: not
			 * true for active queue]).
			 */

			if (nextpg && (nextpg->pg_flags & PQ_INACTIVE) == 0) {
				nextpg = TAILQ_FIRST(pglst);	/* reload! */
			}
		} else {
			/*
			 * if p is null in this loop, make sure it stays null
			 * in the next loop.
			 */
			nextpg = NULL;

			/*
			 * lock page queues here just so they're always locked
			 * at the end of the loop.
			 */
			uvm_lock_pageq();
		}
	}
	return (retval);
}

/*
 * uvmpd_scan: scan the page queues and attempt to meet our targets.
 *
 * => called with pageq's locked
 */

void
uvmpd_scan(void)
{
	int free, inactive_shortage, swap_shortage, pages_freed;
	struct vm_page *p, *nextpg;
	struct uvm_object *uobj;
	boolean_t got_it;

	uvmexp.pdrevs++;		/* counter */
	uobj = NULL;

	/*
	 * get current "free" page count
	 */
	free = uvmexp.free - BUFPAGES_DEFICIT;

#ifndef __SWAP_BROKEN
	/*
	 * swap out some processes if we are below our free target.
	 * we need to unlock the page queues for this.
	 */
	if (free < uvmexp.freetarg) {
		uvmexp.pdswout++;
		uvm_unlock_pageq();
		uvm_swapout_threads();
		uvm_lock_pageq();
	}
#endif

	/*
	 * now we want to work on meeting our targets.   first we work on our
	 * free target by converting inactive pages into free pages.  then
	 * we work on meeting our inactive target by converting active pages
	 * to inactive ones.
	 */

	/*
	 * alternate starting queue between swap and object based on the
	 * low bit of uvmexp.pdrevs (which we bump by one each call).
	 */
	got_it = FALSE;
	pages_freed = uvmexp.pdfreed;	/* XXX - int */
	if ((uvmexp.pdrevs & 1) != 0 && uvmexp.nswapdev != 0)
		got_it = uvmpd_scan_inactive(&uvm.page_inactive_swp);
	if (!got_it)
		got_it = uvmpd_scan_inactive(&uvm.page_inactive_obj);
	if (!got_it && (uvmexp.pdrevs & 1) == 0 && uvmexp.nswapdev != 0)
		(void) uvmpd_scan_inactive(&uvm.page_inactive_swp);
	pages_freed = uvmexp.pdfreed - pages_freed;

	/*
	 * we have done the scan to get free pages.   now we work on meeting
	 * our inactive target.
	 */
	inactive_shortage = uvmexp.inactarg - uvmexp.inactive - BUFPAGES_INACT;

	/*
	 * detect if we're not going to be able to page anything out
	 * until we free some swap resources from active pages.
	 */
	swap_shortage = 0;
	if (uvmexp.free < uvmexp.freetarg &&
	    uvmexp.swpginuse == uvmexp.swpages &&
	    uvmexp.swpgonly < uvmexp.swpages &&
	    pages_freed == 0) {
		swap_shortage = uvmexp.freetarg - uvmexp.free;
	}

	for (p = TAILQ_FIRST(&uvm.page_active);
	     p != NULL && (inactive_shortage > 0 || swap_shortage > 0);
	     p = nextpg) {
		nextpg = TAILQ_NEXT(p, pageq);

		/* skip this page if it's busy. */
		if (p->pg_flags & PG_BUSY)
			continue;

		if (p->pg_flags & PQ_ANON)
			KASSERT(p->uanon != NULL);
		else
			KASSERT(p->uobject != NULL);

		/*
		 * if there's a shortage of swap, free any swap allocated
		 * to this page so that other pages can be paged out.
		 */
		if (swap_shortage > 0) {
			if ((p->pg_flags & PQ_ANON) && p->uanon->an_swslot) {
				uvm_swap_free(p->uanon->an_swslot, 1);
				p->uanon->an_swslot = 0;
				atomic_clearbits_int(&p->pg_flags, PG_CLEAN);
				swap_shortage--;
			}
			if (p->pg_flags & PQ_AOBJ) {
				int slot = uao_set_swslot(p->uobject,
					p->offset >> PAGE_SHIFT, 0);
				if (slot) {
					uvm_swap_free(slot, 1);
					atomic_clearbits_int(&p->pg_flags,
					    PG_CLEAN);
					swap_shortage--;
				}
			}
		}

		/*
		 * deactivate this page if there's a shortage of
		 * inactive pages.
		 */
		if (inactive_shortage > 0) {
			pmap_page_protect(p, PROT_NONE);
			/* no need to check wire_count as pg is "active" */
			uvm_pagedeactivate(p);
			uvmexp.pddeact++;
			inactive_shortage--;
		}
	}
}

#ifdef HIBERNATE

/*
 * uvmpd_drop: drop clean pages from list
 */
void
uvmpd_drop(struct pglist *pglst)
{
	struct vm_page *p, *nextpg;

	for (p = TAILQ_FIRST(pglst); p != NULL; p = nextpg) {
		nextpg = TAILQ_NEXT(p, pageq);

		if (p->pg_flags & PQ_ANON || p->uobject == NULL)
			continue;

		if (p->pg_flags & PG_BUSY)
			continue;

		if (p->pg_flags & PG_CLEAN) {
			/*
			 * we now have the page queues locked.
			 * the page is not busy.   if the page is clean we
			 * can free it now and continue.
			 */
			if (p->pg_flags & PG_CLEAN) {
				if (p->pg_flags & PQ_SWAPBACKED) {
					/* this page now lives only in swap */
					uvmexp.swpgonly++;
				}

				/* zap all mappings with pmap_page_protect... */
				pmap_page_protect(p, PROT_NONE);
				uvm_pagefree(p);
			}
		}
	}
}

void
uvmpd_hibernate(void)
{
	uvm_lock_pageq();

	uvmpd_drop(&uvm.page_inactive_swp);
	uvmpd_drop(&uvm.page_inactive_obj);
	uvmpd_drop(&uvm.page_active);

	uvm_unlock_pageq();
}

#endif
@


1.77
log
@Lock the page queues by turning uvm_lock_pageq() and uvm_unlock_pageq() into
mtx_enter() and mtx_leave() operations.  Not 100% this won't blow up but
there is only one way to find out, and we need this to make progress on
further unlocking uvm.

prodded by deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.76 2015/08/21 16:04:35 visa Exp $	*/
d278 1
a278 1
		sched_pause();
d320 1
a320 1
			sched_pause();
@


1.76
log
@Remove the unused loan_count field and the related uvm logic. Most of
the page loaning code is already in the Attic.

ok kettenis@@, beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.75 2014/12/17 19:42:15 tedu Exp $	*/
d240 1
d242 1
@


1.75
log
@remove lock.h from uvm_extern.h. another holdover from the simpletonlock
era. fix uvm including c files to include lock.h or atomic.h as necessary.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.74 2014/11/16 12:31:00 deraadt Exp $	*/
d422 1
a422 11
			/*
			 * the only time we expect to see an ownerless page
			 * (i.e. a page with no uobject and !PQ_ANON) is if an
			 * anon has loaned a page from a uvm_object and the
			 * uvm_object has dropped the ownership.  in that
			 * case, the anon can "take over" the loaned page
			 * and make it its own.
			 */

			/* is page part of an anon or ownerless ? */
			if ((p->pg_flags & PQ_ANON) || p->uobject == NULL) {
a424 12

				/*
				 * if the page is ownerless, claim it in the
				 * name of "anon"!
				 */
				if ((p->pg_flags & PQ_ANON) == 0) {
					KASSERT(p->loan_count > 0);
					p->loan_count--;
					atomic_setbits_int(&p->pg_flags,
					    PQ_ANON);
					/* anon now owns it */
				}
d853 2
d858 1
a858 2
		/* is page anon owned or ownerless? */
		if ((p->pg_flags & PQ_ANON) || p->uobject == NULL) {
d860 2
a861 13

			/* take over the page? */
			if ((p->pg_flags & PQ_ANON) == 0) {
				KASSERT(p->loan_count > 0);
				p->loan_count--;
				atomic_setbits_int(&p->pg_flags, PQ_ANON);
			}
		}

		/* skip this page if it's busy. */
		if ((p->pg_flags & PG_BUSY) != 0) {
			continue;
		}
@


1.74
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.73 2014/09/14 14:17:27 jsg Exp $	*/
a73 1
#include <sys/vnode.h>
d75 1
@


1.73
log
@remove uneeded proc.h includes
ok mpi@@ kspillner@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.72 2014/09/09 07:07:39 blambert Exp $	*/
d476 1
a476 1
				pmap_page_protect(p, VM_PROT_NONE);
d549 1
a549 1
			pmap_page_protect(p, VM_PROT_READ);
d744 1
a744 1
				pmap_page_protect(p, VM_PROT_NONE);
d923 1
a923 1
			pmap_page_protect(p, VM_PROT_NONE);
d964 1
a964 1
				pmap_page_protect(p, VM_PROT_NONE);
@


1.72
log
@Make the cleaner, syncer, pagedaemon, aiodone daemons all
yield() if the cpu is marked SHOULDYIELD.

ok miod@@ tedu@@ phessler@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.71 2014/07/12 09:02:24 kettenis Exp $	*/
a70 1
#include <sys/proc.h>
@


1.71
log
@Add a function to drop all clean pages on the page daemon queues and call
it when we hibernate.

ok mlarkin@@, miod@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.70 2014/07/11 16:35:40 jsg Exp $	*/
d276 2
d318 2
@


1.70
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.69 2014/07/08 13:10:52 deraadt Exp $	*/
d78 4
d101 1
d928 53
@


1.69
log
@subtle rearrangement of includes
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.67 2014/04/13 23:14:15 tedu Exp $	*/
d21 1
a21 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by Charles D. Cranor,
 *      Washington University, the University of California, Berkeley and 
 *      its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.68
log
@bye bye UBC; ok beck dlg
@
text
@d75 1
a76 1
#include <sys/systm.h>
@


1.67
log
@compress code by turning four line comments into one line comments.
emphatic ok usual suspects, grudging ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.66 2014/02/06 16:40:40 tedu Exp $	*/
a690 10

#ifdef UBC
		if (result == VM_PAGER_ERROR &&
		    curproc == uvm.pagedaemon_proc) {
			uvm_lock_pageq();
			nextpg = TAILQ_NEXT(p, pageq);
			uvm_pageactivate(p);
			continue;
		}
#endif
@


1.66
log
@add some more bufbackoff calls. uvm_wait optimistically (?), uvm_wait_pla
after analysis and testing. when flushing a large mmapped file, we can
eat up all the reserve bufs, but there's a good chance there will be more
clean ones available.
ok beck kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.65 2014/02/06 16:32:40 tedu Exp $	*/
d115 1
a115 4
	/*
	 * check for page daemon going to sleep (waiting for itself)
	 */

a185 1

d194 1
a194 4
	/*
	 * ensure correct priority and set paging parameters...
	 */

a201 4
	/*
	 * main loop
	 */

d222 1
a222 4
		/*
		 * now lock page queues and recompute inactive count
		 */

d234 1
a234 3
		/*
		 * Reclaim pages from the buffer cache if possible.
		 */
d243 1
a243 3
		/*
		 * Scan if needed to meet our targets.
		 */
d274 1
a274 4
		/*
		 * scan done.  unlock page queues (the only lock we are holding)
		 */

a283 1

a292 1

d305 1
a305 4
		/*
		 * process each i/o that's done.
		 */

a359 1

a364 1

a370 1

a374 1

a378 1

a399 1

a436 1

a465 1

a495 1

a505 1

a518 1

a552 1

d554 1
a554 5

				/*
				 * free old swap slot (if any)
				 */

d566 1
a566 4
				/*
				 * start new cluster (if necessary)
				 */

d582 1
a582 4
				/*
				 * add block to cluster
				 */

a592 1

a605 1

d702 1
a702 4
		/*
		 * clean up "p" if we have one
		 */

a787 1

a791 1

a797 1

a849 1

a863 1

a869 1

d897 1
a897 4
		/*
		 * skip this page if it's busy.
		 */

a905 1

a928 1

@


1.65
log
@parenthesis to make the math right. ok beck kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.64 2013/05/30 16:29:46 tedu Exp $	*/
d120 3
@


1.64
log
@remove lots of comments about locking per beck's request
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.63 2013/05/30 15:17:59 tedu Exp $	*/
d252 2
a253 2
			size += uvmexp.freetarg - uvmexp.free -
			    BUFPAGES_DEFICIT;
@


1.63
log
@remove simple_locks from uvm code. ok beck deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.62 2013/02/07 17:29:31 beck Exp $	*/
a448 7
			 * first we attempt to lock the object that this page
			 * belongs to.  if our attempt fails we skip on to
			 * the next page (no harm done).  it is important to
			 * "try" locking the object as we are locking in the
			 * wrong order (pageq -> object) and we don't want to
			 * deadlock.
			 *
d492 1
a492 1
			 * we now have the object and the page queues locked.
d694 3
a696 5
		 *  IN: locked: uobj (if !swap_backed), page queues
		 * OUT: locked: uobj (if !swap_backed && result !=VM_PAGER_PEND)
		 *     !locked: pageqs, uobj (if swap_backed || VM_PAGER_PEND)
		 *
		 * [the bit about VM_PAGER_PEND saves us one lock-unlock pair]
a698 1
		/* locked: uobj (if !swap_backed), page queues */
a701 2
		/* locked: uobj (if !swap_backed && result != PEND) */
		/* unlocked: pageqs, object (if swap_backed ||result == PEND) */
a783 1
				/* still holding object lock */
d939 1
a939 1
			continue;	/* quick check before trying to lock */
a940 3
		/*
		 * lock the page's owner.
		 */
@


1.62
log
@make sure the page daemon considers BUFPAGES_INACT when deciding
to do work, just as is done when waking it up.
tested by me, phessler@@, espie@@, landry@@
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.61 2012/12/10 22:42:54 beck Exp $	*/
a467 4
				if (!simple_lock_try(&anon->an_lock)) {
					/* lock failed, skip this page */
					continue;
				}
a481 1
					simple_unlock(&anon->an_lock);
a489 4
				if (!simple_lock_try(&uobj->vmobjlock)) {
					/* lock failed, skip this page */
					continue;
				}
a490 1
					simple_unlock(&uobj->vmobjlock);
a506 1
					simple_lock(&uvm.swap_data_lock);
a507 1
					simple_unlock(&uvm.swap_data_lock);
a525 5
					simple_unlock(&anon->an_lock);
				} else {
					/* pagefree has already removed the
					 * page from the object */
					simple_unlock(&uobj->vmobjlock);
a535 5
				if (anon) {
					simple_unlock(&anon->an_lock);
				} else {
					simple_unlock(&uobj->vmobjlock);
				}
a550 5
				if (anon) {
					simple_unlock(&anon->an_lock);
				} else {
					simple_unlock(&uobj->vmobjlock);
				}
a625 6
						if (anon)
							simple_unlock(
							    &anon->an_lock);
						else
							simple_unlock(
							    &uobj->vmobjlock);
a661 5
				if (anon)
					simple_unlock(&anon->an_lock);
				else
					simple_unlock(&uobj->vmobjlock);

a787 8
			/* !swap_backed case: already locked... */
			if (swap_backed) {
				if (anon)
					simple_lock(&anon->an_lock);
				else
					simple_lock(&uobj->vmobjlock);
			}

a812 1
				simple_unlock(&anon->an_lock);
a846 5
			if (anon)
				simple_unlock(&anon->an_lock);
			else if (uobj)
				simple_unlock(&uobj->vmobjlock);

a959 2
			if (!simple_lock_try(&p->uanon->an_lock))
				continue;
a966 3
		} else {
			if (!simple_lock_try(&p->uobject->vmobjlock))
				continue;
a973 4
			if (p->pg_flags & PQ_ANON)
				simple_unlock(&p->uanon->an_lock);
			else
				simple_unlock(&p->uobject->vmobjlock);
a1012 4
		if (p->pg_flags & PQ_ANON)
			simple_unlock(&p->uanon->an_lock);
		else
			simple_unlock(&p->uobject->vmobjlock);
@


1.61
log
@Always back the buffer cache off on any page daemon wakeup. This avoids
a few problems noticed by phessler@@ and beck@@ where certain allocations
would repeatedly wake the page daemon even though the page daemon's targets
were met already so it didn't do any work. We can avoid this problem when
the buffer cache has pages to throw away by always doing so any time
the page daemon is woken, rather than only when we are under the free
page target.
ok phessler@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.60 2012/11/07 17:50:49 beck Exp $	*/
d984 1
a984 1
	inactive_shortage = uvmexp.inactarg - uvmexp.inactive;
@


1.60
log
@
Fix the buffer cache.

A long time ago (in vienna) the reserves for the cleaner and syncer were
removed. softdep and many things have not performed ths same ever since.
Follow on generations of buffer cache hackers assumed the exising code
was the reference and have been in frustrating state of coprophagia ever
since.

This commit

0) Brings back a (small) reserve allotment of buffer pages, and the kva to
   map them, to allow the cleaner and syncer to run even when under intense
   memory or kva pressure.
1) Fixes a lot of comments and variables to represent reality.
2) Simplifies and corrects how the buffer cache backs off down to the lowest
   level.
3) Corrects how the page daemons asks the buffer cache to back off, ensuring
   that uvmpd_scan is done to recover inactive pages in low memory situaitons
4) Adds a high water mark to the pool used to allocate struct buf's
5) Correct the cleaner and the sleep/wakeup cases in both low memory and low
   kva situations. (including accounting for the cleaner/syncer reserve)

Tested by many, with very much helpful input from deraadt, miod, tobiasu,
kettenis and others.

ok kettenis@@ deraadt@@ jj@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.59 2011/07/06 19:50:38 beck Exp $	*/
d211 1
d246 12
a257 1
		 * get pages from the buffer cache, or scan if needed
a261 20
			u_int64_t free, size;
			free = uvmexp.free - BUFPAGES_DEFICIT;
			/* start with the size of what we are asking for */
			size = pma ? pma->pm_size : 0;
			/*
			 * If we below our target, add twice the amount
			 * we are below the target to what we will ask
			 * the buffer cache to give back. We then ask
			 * the buffer cache to give us free pages by
			 * backing off.
			 */
			if (uvmexp.freetarg - free > 0)
				size += (uvmexp.freetarg - free) * 2;
			(void) bufbackoff(&constraint, size);
			/*
			 * Now we scan to ensure we meet all our targets,
			 * we will not swap at this point, unless the buffer
			 * cache could not back off enough for us to meet
			 * our free page targets.
			 */
@


1.59
log
@
uvm changes for buffer cache improvements.
1) Make the pagedaemon aware of the memory ranges and size of allocations
where memory is being requested, and pass this information on to
bufbackoff(), which will later (not yet) be used to ensure that the
buffer cache gets out of the way in the right area of memory.

Note that this commit does not yet make it *do* that - as currently
the buffer cache is all in dma-able memory and it will simply back
off.

2) Add uvm_pagerealloc_multi - to be used by the buffer cache code
for reallocating pages to particular regions.

much of this work by ariane, with smatterings of me, art,and oga

ok oga@@, thib@@, ariane@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.58 2011/07/03 18:34:14 oga Exp $	*/
d250 22
a271 7
			if (bufbackoff(&constraint,
			    (pma ? pma->pm_size : -1)) == 0)
				work_done = 1;
			else {
				uvmpd_scan();
				work_done = 1; /* we hope... */
			}
@


1.58
log
@Rip out and burn support for UVM_HIST.

The vm hackers don't use it, don't maintain it and have to look at it all the
time. About time this 800 lines of code hit /dev/null.

``never liked it'' tedu@@. ariane@@ was very happy when i told her i wrote
this diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.57 2011/04/01 12:58:13 krw Exp $	*/
a149 1

d190 3
d211 2
d214 14
a227 3
		msleep(&uvm.pagedaemon, &uvm.fpageqlock, PVM | PNORELOCK,
		    "pgdaemon", 0);
		uvmexp.pdwoke++;
d247 2
a248 1
		if (((uvmexp.free - BUFPAGES_DEFICIT) < uvmexp.freetarg) ||
d250 4
a253 1
			if (bufbackoff() == -1)
d255 2
d267 12
@


1.57
log
@Typo in comment.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.56 2010/09/26 12:53:27 thib Exp $	*/
a160 1
	UVMHIST_FUNC("uvmpd_tune"); UVMHIST_CALLED(pdhist);
a181 2
	UVMHIST_LOG(pdhist, "<- done, freemin=%ld, freetarg=%ld, wiredmax=%ld",
	      uvmexp.freemin, uvmexp.freetarg, uvmexp.wiredmax, 0);
a191 3
	UVMHIST_FUNC("uvm_pageout"); UVMHIST_CALLED(pdhist);

	UVMHIST_LOG(pdhist,"<starting uvm pagedaemon>", 0, 0, 0, 0);
a209 1
		UVMHIST_LOG(pdhist,"  <<SLEEPING>>",0,0,0,0);
a212 1
		UVMHIST_LOG(pdhist,"  <<WOKE UP>>",0,0,0,0);
a228 4
		UVMHIST_LOG(pdhist,"  free/ftarg=%ld/%ld, inact/itarg=%ld/%ld",
		    uvmexp.free, uvmexp.freetarg, uvmexp.inactive,
		    uvmexp.inactarg);

a267 1
	UVMHIST_FUNC("uvm_aiodoned"); UVMHIST_CALLED(pdhist);
a334 1
	UVMHIST_FUNC("uvmpd_scan_inactive"); UVMHIST_CALLED(pdhist);
a374 2
				UVMHIST_LOG(pdhist,"  met free target: "
					    "exit loop", 0, 0, 0, 0);
a894 1
	UVMHIST_FUNC("uvmpd_scan"); UVMHIST_CALLED(pdhist);
a910 2
		UVMHIST_LOG(pdhist,"  free %ld < target %ld: swapout", free,
		    uvmexp.freetarg, 0, 0);
a923 2
	UVMHIST_LOG(pdhist, "  starting 'free' loop",0,0,0,0);

a958 2
	UVMHIST_LOG(pdhist, "  loop 2: inactive_shortage=%ld swap_shortage=%ld",
		    inactive_shortage, swap_shortage,0,0);
@


1.56
log
@remove static so things show up in ddb.

ok miod@@, oga@@, tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.55 2009/10/14 17:53:30 beck Exp $	*/
d154 1
a154 1
 * => called when ever memory is added (or removed?) to the system
@


1.55
log
@Fix buffer cache backoff in the page daemon - deal with inactive pages to
more correctly reflect the new state of the world - that is - how many pages
can be cheaply reclaimed - which now includes clean buffer cache pages.

This change fixes situations where people would be running with a large bufcachepercent, and still notice swapping without the buffer cache backing off.

ok oga@@, testing by many on tech@@ and others. Thanks.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.54 2009/08/08 13:43:59 beck Exp $	*/
d99 3
a101 3
static void		uvmpd_scan(void);
static boolean_t	uvmpd_scan_inactive(struct pglist *);
static void		uvmpd_tune(void);
d158 1
a158 1
static void
d332 1
a332 1
static boolean_t
@


1.54
log
@fix the page daemon to back off the buffer cache correctly even in the case
where we are below the inactive page target. This fixes a problem with a large
buffer cache on low memory machines where the the page daemon would woken up,
however the buffer cache would never be backed off because we were below the
inactive page target, which could result in constant paging and basically
a livelock condition.

ok oga@@ art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.52 2009/07/22 21:05:37 oga Exp $	*/
a214 1
		int scanned = 0;
d244 3
a246 2
		if ((uvmexp.free - BUFPAGES_DEFICIT) < uvmexp.freetarg) {
			if (bufbackoff() == -1) {
a247 3
				scanned = 1;
			} else
				scanned = 0;
a248 4

		if (!scanned && (uvmexp.inactive < uvmexp.inactarg))
			uvmpd_scan();

@


1.53
log
@
Dynamic buffer cache support - a re-commit of what was backed out
after c2k9

allows buffer cache to be extended and grow/shrink dynamically

tested by many, ok oga@@, "why not just commit it" deraadt@@
@
text
@d215 1
d245 2
a246 4
		if (uvmexp.inactive < uvmexp.inactarg)
			uvmpd_scan();
		else if ((uvmexp.free - BUFPAGES_DEFICIT) < uvmexp.freetarg) {
			if (bufbackoff() == -1)
d248 3
d252 4
@


1.52
log
@Put the PG_RELEASED changes diff back in.

This has has been tested very very thoroughly on all archs we have
excepting 88k and 68k. Please see cvs log for the individual commit
messages.

ok beck@@, thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.51 2009/06/26 20:26:02 oga Exp $	*/
d242 1
a242 1
		 * scan if needed
d244 1
a244 2
		if ((uvmexp.free - BUFPAGES_DEFICIT) < uvmexp.freetarg ||
		    uvmexp.inactive < uvmexp.inactarg) {
d246 3
@


1.51
log
@Fix a use after free in the pagedaemon.

specifically, if we free a RELEASED anon, then we will first of all
remove the page from the anon, free the anon, then get the next page
relative to the anon page, then call uvm_pagefree().

The problem is that while we zero out anon->an_page, we do not zero out
pg->uanon. Now, uvm_pagefree() if pg->uanon is not NULL zeroes out some
variables in the struct for us. One of the backed out commits added more
zeroing there which would have exacerbated this use after free under
heavy paging (which was where we saw bugs). Fix this by zeroing out
pg->uanon.

I have looked for other similar cases, but have not found any as of yet.

been in snaps a while, "please do commit that" deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.50 2009/06/17 00:13:59 oga Exp $	*/
d823 1
a823 1
			/* released during I/O? */
d825 8
a832 8
				if (anon) {
					/*
					 * remove page so we can get nextpg,
					 * also zero out anon so we don't use
					 * it after the free.
					 */
					anon->an_page = NULL;
					p->uanon = NULL;
d834 8
a841 23
					simple_unlock(&anon->an_lock);
					uvm_anfree(anon);	/* kills anon */
					pmap_page_protect(p, VM_PROT_NONE);
					anon = NULL;
					uvm_lock_pageq();
					nextpg = TAILQ_NEXT(p, pageq);
					/* free released page */
					uvm_pagefree(p);

				} else {

					/*
					 * pgo_releasepg nukes the page and
					 * gets "nextpg" for us.  it returns
					 * with the page queues locked (when
					 * given nextpg ptr).
					 */

					if (!uobj->pgops->pgo_releasepg(p,
					    &nextpg))
						/* uobj died after release */
						uobj = NULL;
				}
@


1.50
log
@date based reversion of uvm to the 4th May.

More backouts in line with previous ones, this appears to bring us back to a
stable condition.

A machine forced to 64mb of ram cycled 10GB through swap with this diff
and is still running as I type this. Other tests by ariane@@ and thib@@
also seem to show that it's alright.

ok deraadt@@, thib@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.42 2009/04/17 07:14:04 oga Exp $	*/
d826 5
a830 1
					/* remove page so we can get nextpg */
d832 1
@


1.49
log
@Backout all changes to uvm after pmemrange (which will be backed out
separately).

a change at or just before the hackathon has either exposed or added a
very very nasty memory corruption bug that is giving us hell right now.
So in the interest of kernel stability these diffs are being backed out
until such a time as that corruption bug has been found and squashed,
then the ones that are proven good may slowly return.

a quick hitlist of the main commits this backs out:

mine:
uvm_objwire
the lock change in uvm_swap.c
using trees for uvm objects instead of the hash
removing the pgo_releasepg callback.

art@@'s:
putting pmap_page_protect(VM_PROT_NONE) in uvm_pagedeactivate() since
all callers called that just prior anyway.

ok beck@@, ariane@@.

prompted by deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.44 2009/05/08 13:50:15 ariane Exp $	*/
d99 3
a101 3
void		uvmpd_scan(void);
boolean_t	uvmpd_scan_inactive(struct pglist *);
void		uvmpd_tune(void);
d146 1
a146 1
	wakeup(&uvm.pagedaemon_proc);		/* wake the daemon! */
d158 1
a158 1
void
d217 1
a217 1
		msleep(&uvm.pagedaemon_proc, &uvm.fpageqlock, PVM | PNORELOCK,
d291 1
a291 1
			msleep(&uvm.aiodoned_proc, &uvm.aiodoned_lock,
d313 2
a314 2
		wakeup(free <= uvmexp.reserve_kernel ?
		    (void *)&uvm.pagedaemon_proc : (void *)&uvmexp.free);
d331 1
a331 1
boolean_t
@


1.48
log
@Back out all the buffer cache changes I committed during c2k9. This reverts three
commits:

1) The sysctl allowing bufcachepercent to be changed at boot time.
2) The change moving the buffer cache hash chains to a red-black tree
3) The dynamic buffer cache (Which depended on the earlier too).

ok on the backout from marco and todd
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.47 2009/06/06 23:35:08 art Exp $	*/
d823 1
a823 1
			/* released during I/O? Can only happen for anons */
d825 21
a845 3
				KASSERT(anon != NULL);
				/* remove page so we can get nextpg */
				anon->an_page = NULL;
d847 5
a851 8
				simple_unlock(&anon->an_lock);
				uvm_anfree(anon);	/* kills anon */
				pmap_page_protect(p, VM_PROT_NONE);
				anon = NULL;
				uvm_lock_pageq();
				nextpg = TAILQ_NEXT(p, pageq);
				/* free released page */
				uvm_pagefree(p);
d1060 1
@


1.47
log
@Somehow I missed comitting this.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.46 2009/06/05 04:29:14 beck Exp $	*/
d217 2
a218 2
		msleep(&uvm.pagedaemon_proc, &uvm.fpageqlock,
		    PVM | PNORELOCK, "pgdaemon", 0);
d242 1
a242 1
		 * get pages from the buffer cache, or scan if needed
d244 2
a245 1
		if (uvmexp.inactive < uvmexp.inactarg)
a246 3
		else if ((uvmexp.free - BUFPAGES_DEFICIT) < uvmexp.freetarg) {
			if (bufbackoff() == -1)
				uvmpd_scan();
@


1.46
log
@Dynamic buffer cache sizing.

This commit won't change the default behaviour of the system unless the
buffer cache size is increased with sysctl kern.bufcachepercent. By default
our buffer cache is 10% of memory, which with this commit is now treated
as a low water mark.  If the buffer cache size is increased, the new size
is treated as a high water mark and the buffer cache is permitted to grow
to that percentage of memory.

If the page daemon is invoked, the page daemon will ask the buffer cache
to relenquish pages. if the buffer cache has more than the low water mark it
will relenquish pages allowing them to be consumed by uvm. after a short
period the buffer cache will attempt to re-grow back to the high water mark.

This permits the use of a large buffer cache without penalizing the available
memory for other purposes.

Above the low water mark the buffer cache remains entirely subservient to
the page daemon, so if uvm requires pages, the buffer cache will abandon
them.

ok art@@ thib@@ oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.45 2009/06/01 19:54:02 oga Exp $	*/
a1046 1
			pmap_page_protect(p, VM_PROT_NONE);
@


1.45
log
@Since we've now cleared up a lot of the PG_RELEASED setting, remove the
pgo_releasepg() hook and just free the page the "normal" way in the one
place we'll ever see PG_RELEASED and should care (uvm_page_unbusy,
called in aiodoned).

ok art@@, beck@@, thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.44 2009/05/08 13:50:15 ariane Exp $	*/
d217 2
a218 2
		msleep(&uvm.pagedaemon_proc, &uvm.fpageqlock, PVM | PNORELOCK,
		    "pgdaemon", 0);
d242 1
a242 1
		 * scan if needed
d244 1
a244 2
		if ((uvmexp.free - BUFPAGES_DEFICIT) < uvmexp.freetarg ||
		    uvmexp.inactive < uvmexp.inactarg) {
d246 3
@


1.44
log
@Remove static qualifier of functions that are not inline.
Makes trace in ddb useful.

ok oga
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.43 2009/05/04 18:08:06 oga Exp $	*/
d823 1
a823 1
			/* released during I/O? */
d825 3
a827 3
				if (anon) {
					/* remove page so we can get nextpg */
					anon->an_page = NULL;
d829 8
a836 23
					simple_unlock(&anon->an_lock);
					uvm_anfree(anon);	/* kills anon */
					pmap_page_protect(p, VM_PROT_NONE);
					anon = NULL;
					uvm_lock_pageq();
					nextpg = TAILQ_NEXT(p, pageq);
					/* free released page */
					uvm_pagefree(p);

				} else {

					/*
					 * pgo_releasepg nukes the page and
					 * gets "nextpg" for us.  it returns
					 * with the page queues locked (when
					 * given nextpg ptr).
					 */

					if (!uobj->pgops->pgo_releasepg(p,
					    &nextpg))
						/* uobj died after release */
						uobj = NULL;
				}
@


1.43
log
@Instead of keeping two ints in the uvm structure specifically just to
sleep on them (and otherwise ignore them) sleep on the pointer to the
{aiodoned,pagedaemon}_proc members, and nuke the two extra words.

"no objections" art@@, ok beck@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.42 2009/04/17 07:14:04 oga Exp $	*/
d99 3
a101 3
static void		uvmpd_scan(void);
static boolean_t	uvmpd_scan_inactive(struct pglist *);
static void		uvmpd_tune(void);
d158 1
a158 1
static void
d331 1
a331 1
static boolean_t
@


1.42
log
@Another case of locking just to read uvmexp.free. Kill the locking, not
needed.

"of course" art@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.41 2009/04/15 12:43:07 oga Exp $	*/
d146 1
a146 1
	wakeup(&uvm.pagedaemon);		/* wake the daemon! */
d217 1
a217 1
		msleep(&uvm.pagedaemon, &uvm.fpageqlock, PVM | PNORELOCK,
d291 1
a291 1
			msleep(&uvm.aiodoned, &uvm.aiodoned_lock,
d313 2
a314 2
		wakeup(free <= uvmexp.reserve_kernel ? &uvm.pagedaemon :
		    &uvmexp.free);
@


1.41
log
@We don't need to grab the fpageqlock to do nothing but look at the value
of uvmexp.free.

"yeah, go for it" art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.40 2009/04/14 20:12:05 oga Exp $	*/
a927 1
	uvm_lock_fpageq();
a928 1
	uvm_unlock_fpageq();
@


1.40
log
@The use of uvm.pagedaemon_lock is incredibly inconsistent. only a
fraction of the wakeups and sleeps involved here actually grab that
lock. The remainder, on the other hand, always have the fpageq_lock
locked.

So, make this locking correct by switching the other users over to
fpageq_lock, too.

This would probably be better off being a semaphore, but for now at
least it's correct.

"ok, unless you want to implement semaphores" art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.39 2009/04/13 22:17:54 oga Exp $	*/
a383 2

			uvm_lock_fpageq();
a384 1
			uvm_unlock_fpageq();
@


1.39
log
@Convert the page queue lock to a mutex instead of a simplelock.

Fix up the one case of lock recursion (which blatantly ignored the
comment right above it saying that we don't need to lock). The rest of
the lock usage has been checked and appears to be correct.

ok ariane@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.38 2009/04/06 12:02:52 oga Exp $	*/
d113 1
a113 2
	int timo = 0;
	int s = splbio();
d145 1
a145 1
	simple_lock(&uvm.pagedaemon_lock);
d147 1
a147 4
	UVM_UNLOCK_AND_WAIT(&uvmexp.free, &uvm.pagedaemon_lock, FALSE, wmsg,
	    timo);

	splx(s);
d215 1
a215 2
		simple_lock(&uvm.pagedaemon_lock);

d217 2
a218 2
		UVM_UNLOCK_AND_WAIT(&uvm.pagedaemon,
		    &uvm.pagedaemon_lock, FALSE, "pgdaemon", 0);
d253 1
a253 1

d258 1
d312 4
a315 9
		if (free <= uvmexp.reserve_kernel) {
			uvm_lock_fpageq();
			wakeup(&uvm.pagedaemon);
			uvm_unlock_fpageq();
		} else {
			simple_lock(&uvm.pagedaemon_lock);
			wakeup(&uvmexp.free);
			simple_unlock(&uvm.pagedaemon_lock);
		}
@


1.38
log
@Instead of doing splbio(); simple_lock(&uvm.aiodoned_lock); just replace
the simple lock with a real lock - a IPL_BIO mutex. While i'm here, make
the sleeping condition one hell of a lot simpler in the aio daemon.

some ideas from and ok art@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.37 2009/03/20 15:19:04 oga Exp $	*/
a862 7

					/*
					 * lock page queues here so that they're
					 * always locked at the end of the loop.
					 */

					uvm_lock_pageq();
d896 3
a913 4
		}

		if (nextpg && (nextpg->pg_flags & PQ_INACTIVE) == 0) {
			nextpg = TAILQ_FIRST(pglst);	/* reload! */
@


1.37
log
@While working on some stuff in uvm I've gotten REALLY sick of reading
K&R function declarations, so switch them all over to ansi-style, in
accordance with the prophesy.

"go for it" art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.36 2009/01/12 19:03:12 miod Exp $	*/
d290 2
a291 3
		 * carefully attempt to go to sleep (without losing "wakeups"!).
		 * we need splbio because we want to make sure the aio_done list
		 * is totally empty before we go to sleep.
d293 7
a299 24

		s = splbio();
		simple_lock(&uvm.aiodoned_lock);
		if (TAILQ_FIRST(&uvm.aio_done) == NULL) {
			UVMHIST_LOG(pdhist,"  <<SLEEPING>>",0,0,0,0);
			UVM_UNLOCK_AND_WAIT(&uvm.aiodoned,
			    &uvm.aiodoned_lock, FALSE, "aiodoned", 0);
			UVMHIST_LOG(pdhist,"  <<WOKE UP>>",0,0,0,0);

			/* relock aiodoned_lock, still at splbio */
			simple_lock(&uvm.aiodoned_lock);
		}

		/*
		 * check for done aio structures
		 */

		bp = TAILQ_FIRST(&uvm.aio_done);
		if (bp) {
			TAILQ_INIT(&uvm.aio_done);
		}

		simple_unlock(&uvm.aiodoned_lock);
		splx(s);
@


1.36
log
@Register aiodoned_proc, although it is not used anywhere yet; PR #6034
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.35 2008/07/02 15:21:33 art Exp $	*/
d111 1
a111 2
uvm_wait(wmsg)
	const char *wmsg;
d359 1
a359 2
uvmpd_scan_inactive(pglst)
	struct pglist *pglst;
@


1.35
log
@Make the pagedaemon a bit happier.
1. When checking if the pagedaemon should be awakened and to see how
   much work it should do, consider the buffer cache deficit
   (how much pages the buffer cache can eat max vs. how much it has
   now) as pages that are not free. They are actually still usable by
   the allocator, but the presure on the pagedaemon is increased when
   we starting to chew into the memory that the buffer cache wants to
   use.
2. Remove the stupid 512kB limit of how much memory should be our
   free target. That maybe made sense on 68k, but on modern systems
   512k is just a joke. Keep it at 3% of physical memory just like
   it was meant to be.
3. When doing allocations for the pagedaemon, always let it use the
   reserve. the whole UVM_OBJ_IS_KERN_OBJECT is silly and doesn't
   work in most cases anyway. We still don't have a reserve for
   the pagedaemon in the km_page allocator, but this seems to help
   enough. (yes, there are still bad cases in that code and the comment
   is only half-true, the whole section needs a massage, but that will
   happen later, this diff only touches pagedaemon parts)

Testing by many, prodded by theo.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.34 2007/12/18 11:05:52 thib Exp $	*/
d285 2
@


1.34
log
@Turn the uvm_{lock/unlock}_fpageq() inlines into
macros that just expand into the mutex functions
to keep the abstraction, do assorted cleanup.

ok miod@@,art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.33 2007/06/18 21:51:15 pedro Exp $	*/
d81 1
d164 1
a164 1
uvmpd_tune()
d173 1
d175 1
d250 1
a250 9

#ifdef UBC
		if (uvmexp.free + uvmexp.paging < uvmexp.freetarg ||
		    uvmexp.inactive < uvmexp.inactarg ||
		    uvm_pgcnt_vnode >
		    (uvmexp.active + uvmexp.inactive + uvmexp.wired +
		     uvmexp.free) * 13 / 16) {
#else
		if (uvmexp.free < uvmexp.freetarg ||
a251 1
#endif
d413 1
a413 1
			free = uvmexp.free;
d952 1
a952 1
uvmpd_scan()
d967 1
a967 1
	free = uvmexp.free;
a981 1

d1000 1
a1000 1
	pages_freed = uvmexp.pdfreed;
@


1.33
log
@Bring back Mickey's UVM anon change. Testing by thib@@, beck@@ and
ckuethe@@ for a while. Okay beck@@, "it is good timing" deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.32 2007/04/13 18:57:49 art Exp $	*/
d340 1
a340 1
			s = uvm_lock_fpageq();
d342 1
a342 1
			uvm_unlock_fpageq(s);
d368 1
a368 1
	int s, free, result;
d418 1
a418 1
			s = uvm_lock_fpageq();
d420 1
a420 1
			uvm_unlock_fpageq(s);
d960 1
a960 1
	int s, free, inactive_shortage, swap_shortage, pages_freed;
d972 1
a972 1
	s = uvm_lock_fpageq();
d974 1
a974 1
	uvm_unlock_fpageq(s);
@


1.32
log
@While splitting flags and pqflags might have been a good idea in theory
to separate locking, on most modern machines this is not enough
since operations on short types touch other short types that share the
same word in memory.

Merge pg_flags and pqflags again and now use atomic operations to change
the flags. Also bump wire_count to an int and pg_version might go
int as well, just for alignment.

tested by many, many. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.31 2007/04/04 17:44:45 art Exp $	*/
d548 1
a548 1
					anon->u.an_page = NULL;
d863 1
a863 1
					anon->u.an_page = NULL;
@


1.31
log
@Mechanically rename the "flags" and "version" fields in struct vm_page
to "pg_flags" and "pg_version", so that they are a bit easier to work with.
Whoever uses generic names like this for a popular struct obviously doesn't
read much code.

Most architectures compile and there are no functionality changes.

deraadt@@ ok ("if something fails to compile, we fix that by hand")
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.30 2006/07/31 11:51:29 mickey Exp $	*/
d476 1
a476 1
			if ((p->pqflags & PQ_ANON) || p->uobject == NULL) {
d489 1
a489 1
				if ((p->pqflags & PQ_ANON) == 0) {
d492 2
a493 1
					p->pqflags |= PQ_ANON;
d526 1
a526 1
				if (p->pqflags & PQ_SWAPBACKED) {
d580 1
a580 1
			if ((p->pqflags & PQ_SWAPBACKED) &&
d599 1
a599 1
			if ((p->pqflags & PQ_SWAPBACKED) &&
d602 1
a602 1
				if ((p->pqflags & PQ_ANON) &&
d607 1
a607 1
				if (p->pqflags & PQ_AOBJ) {
d622 2
a623 2
			swap_backed = ((p->pqflags & PQ_SWAPBACKED) != 0);
			p->pg_flags |= PG_BUSY;		/* now we own it */
d660 3
a662 1
						p->pg_flags &= ~PG_BUSY;
d790 1
a790 1
				if (p->pqflags & PQ_INACTIVE)
d856 1
a856 1
			p->pg_flags &= ~(PG_BUSY|PG_WANTED);
d909 2
a910 1
					p->pg_flags |= PG_CLEAN;
d944 1
a944 1
		if (nextpg && (nextpg->pqflags & PQ_INACTIVE) == 0) {
d1049 1
a1049 1
		if ((p->pqflags & PQ_ANON) || p->uobject == NULL) {
d1055 1
a1055 1
			if ((p->pqflags & PQ_ANON) == 0) {
d1058 1
a1058 1
				p->pqflags |= PQ_ANON;
d1070 1
a1070 1
			if (p->pqflags & PQ_ANON)
d1083 1
a1083 1
			if ((p->pqflags & PQ_ANON) && p->uanon->an_swslot) {
d1086 1
a1086 1
				p->pg_flags &= ~PG_CLEAN;
d1089 1
a1089 1
			if (p->pqflags & PQ_AOBJ) {
d1094 2
a1095 1
					p->pg_flags &= ~PG_CLEAN;
d1113 1
a1113 1
		if (p->pqflags & PQ_ANON)
@


1.30
log
@fix uvmhist #2: args are always u_long so fix missing %d and %x and no %ll; no change for normal code
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.29 2006/07/13 22:51:26 deraadt Exp $	*/
d495 1
a495 1
				if (p->flags & PG_BUSY) {
d509 1
a509 1
				if (p->flags & PG_BUSY) {
d524 1
a524 1
			if (p->flags & PG_CLEAN) {
d622 1
a622 1
			p->flags |= PG_BUSY;		/* now we own it */
d659 1
a659 1
						p->flags &= ~PG_BUSY;
d849 1
a849 1
			if (p->flags & PG_WANTED)
d853 1
a853 1
			p->flags &= ~(PG_BUSY|PG_WANTED);
d857 1
a857 1
			if (p->flags & PG_RELEASED) {
d906 1
a906 1
					p->flags |= PG_CLEAN;
d1038 1
a1038 1
		if (p->flags & PG_BUSY)
d1065 1
a1065 1
		if ((p->flags & PG_BUSY) != 0) {
d1082 1
a1082 1
				p->flags &= ~PG_CLEAN;
d1090 1
a1090 1
					p->flags &= ~PG_CLEAN;
@


1.29
log
@Back out the anon change.  Apparently it was tested by a few, but most of
us did not see it or get a chance to test it before it was commited. It
broke cvs, in the ami driver, making it not succeed at seeing it's devices.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.27 2006/05/16 08:34:42 mickey Exp $	*/
d185 1
a185 1
	UVMHIST_LOG(pdhist, "<- done, freemin=%d, freetarg=%d, wiredmax=%d",
d240 1
a240 1
		UVMHIST_LOG(pdhist,"  free/ftarg=%d/%d, inact/itarg=%d/%d",
d979 1
a979 1
		UVMHIST_LOG(pdhist,"  free %d < target %d: swapout", free,
d1032 1
a1032 1
	UVMHIST_LOG(pdhist, "  loop 2: inactive_shortage=%d swap_shortage=%d",
@


1.28
log
@from netbsd: make anons dynamically allocated from pool.
this results in lesse kva waste due to static preallocation of those
for every phys page and also every swap page.
tested by beck krw miod
@
text
@d547 1
a547 1
					anon->an_page = NULL;
d860 1
a860 1
					anon->an_page = NULL;
@


1.27
log
@tpyo
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.26 2006/05/07 20:06:50 tedu Exp $	*/
d547 1
a547 1
					anon->u.an_page = NULL;
d860 1
a860 1
					anon->u.an_page = NULL;
@


1.26
log
@remove drain hooks from pool.
1.  drain hooks and lists of allocators make the code complicated
2.  the only hooks in the system are the mbuf reclaim routines
3.  if reclaim is actually able to put a meaningful amount of memory back
in the system, i think something else is dicked up.  ie, if reclaiming
your ip fragment buffers makes the difference thrashing swap and not,
your system is in a load of trouble.
4.  it's a scary amount of code running with very weird spl requirements
and i'd say it's pretty much totally untested.  raise your hand if your
router is running at the edge of swap.
5.  the reclaim stuff goes back to when mbufs lived in a tiny vm_map and
you could run out of va.  that's very unlikely (like impossible) now.
ok/tested pedro krw sturm
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.25 2002/05/24 13:10:53 art Exp $	*/
d85 1
a85 1
 * UVMPD_NUMDIRTYREACTS is how many dirty pages the pagedeamon will reactivate
@


1.25
log
@Make sure that b_iodone handlers are called at splbio (and splassert(IPL_BIO) in all known callers, just to make sure).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.24 2002/03/14 01:27:18 millert Exp $	*/
a223 3

		/* drain pool resources */
		pool_drain(0);
@


1.24
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.23 2002/02/10 23:15:05 deraadt Exp $	*/
d337 1
d339 1
@


1.23
log
@spelling
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.22 2002/01/02 22:23:25 miod Exp $	*/
d98 3
a100 3
static void		uvmpd_scan __P((void));
static boolean_t	uvmpd_scan_inactive __P((struct pglist *));
static void		uvmpd_tune __P((void));
@


1.22
log
@Back out a few more uvm changes, especially wrt swap usage.
This unbreaks m68k m88k sparc and perhaps others, which eventually froze
when hitting swap.
Tested by various people on various platforms.
ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.17 2001/11/10 18:42:31 art Exp $	*/
d384 1
a384 1
	 * note: we currently keep swap-backed pages on a seperate inactive
@


1.21
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pdaemon.c,v 1.18 2001/11/12 01:26:10 art Exp $	*/
/*	$NetBSD: uvm_pdaemon.c,v 1.30 2001/03/09 01:02:12 chs Exp $	*/
d167 1
a167 1
	uvmexp.freemin = uvmexp.npages / 20;
d169 1
a169 1
	/* between 16k and 256k */
d172 1
a172 1
	uvmexp.freemin = min(uvmexp.freemin, (256*1024) >> PAGE_SHIFT);
d251 1
d253 6
d260 1
d380 1
a380 1
	int dirtyreacts, t;
d449 3
a451 1
			 * skip to next page.
a460 29
			 * enforce the minimum thresholds on different
			 * types of memory usage.  if reusing the current
			 * page would reduce that type of usage below its
			 * minimum, reactivate the page instead and move
			 * on to the next page.
			 */

			t = uvmexp.active + uvmexp.inactive + uvmexp.free;
			if (p->uanon &&
			    uvmexp.anonpages <= (t * uvmexp.anonmin) >> 8) {
				uvm_pageactivate(p);
				uvmexp.pdreanon++;
				continue;
			}
			if (p->uobject && UVM_OBJ_IS_VTEXT(p->uobject) &&
			    uvmexp.vtextpages <= (t * uvmexp.vtextmin) >> 8) {
				uvm_pageactivate(p);
				uvmexp.pdrevtext++;
				continue;
			}
			if (p->uobject && UVM_OBJ_IS_VNODE(p->uobject) &&
			    !UVM_OBJ_IS_VTEXT(p->uobject) &&
			    uvmexp.vnodepages <= (t * uvmexp.vnodemin) >> 8) {
				uvm_pageactivate(p);
				uvmexp.pdrevnode++;
				continue;
			}

			/*
d521 2
a522 4
			 * the page is not busy.  remove all the permissions
			 * from the page so we can sync the modified info
			 * without any race conditions.  if the page is clean
			 * we can free it now and continue.
a524 5
			pmap_page_protect(p, VM_PROT_NONE);
			if ((p->flags & PG_CLEAN) != 0 && pmap_is_modified(p)) {
				p->flags &= ~PG_CLEAN;
			}

d533 2
d617 3
a619 1
			 * touch the page.
d625 1
d798 1
d806 1
d843 6
d1098 2
a1099 3
		 * If the page has not been referenced since the
		 * last scan, deactivate the page if there is a
		 * shortage of inactive pages.
d1102 2
a1103 2
		if (inactive_shortage > 0 &&
		    pmap_clear_reference(p) == FALSE) {
@


1.20
log
@Sync in more uvm from NetBSD. Mostly just cosmetic stuff.
Contains also support for page coloring.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pdaemon.c,v 1.19 2001/11/28 13:47:40 art Exp $	*/
/*	$NetBSD: uvm_pdaemon.c,v 1.36 2001/06/27 18:52:10 thorpej Exp $	*/
d4 1
a4 1
/*
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and
d48 1
a48 1
 *
d54 3
a56 3
 *
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
d58 1
a58 1
 *
d376 8
d698 2
a699 1
				if (anon) {
d701 2
a702 2
				} else {
					result = uao_set_swslot(uobj,
a704 8
					if (result == -1) {
						p->flags &= ~PG_BUSY;
						UVM_PAGE_OWN(p, NULL);
						simple_unlock(&uobj->vmobjlock);
						continue;
					}
				}
				swpps[swcpages] = p;
d770 4
a773 1
		 * OUT:!locked: pageqs, uobj
d780 2
a781 1
		/* unlocked: pageqs, uobj */
d792 10
a801 1
		 * if the pageout failed, reactivate the page and continue.
d804 17
a820 1
		if (result == EIO && curproc == uvm.pagedaemon_proc) {
d828 1
a828 2
		 * the pageout is in progress.  bump counters and set up
		 * for the next loop.
a830 3
		uvm_lock_pageq();
		uvmexp.paging += npages;
		uvmexp.pdpending++;
d832 76
a907 1
			if (p->pqflags & PQ_INACTIVE)
d909 28
a936 2
			else
				nextpg = TAILQ_FIRST(pglst);
d938 6
d945 11
d1018 6
a1023 1
	(void) uvmpd_scan_inactive(&uvm.page_inactive);
d1111 3
a1113 4
		 * If we're short on inactive pages, move this over
		 * to the inactive list.  The second hand will sweep
		 * it later, and if it has been referenced again, it
		 * will be moved back to active.
d1116 2
a1117 2
		if (inactive_shortage > 0) {
			pmap_clear_reference(p);
@


1.20.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.22 2002/01/02 22:23:25 miod Exp $	*/
d167 1
a167 1
	uvmexp.freemin = uvmexp.npages / 30;
d169 1
a169 1
	/* between 16k and 512k */
d172 1
a172 1
	uvmexp.freemin = min(uvmexp.freemin, (512*1024) >> PAGE_SHIFT);
a250 1
#ifdef UBC
a251 6
		    uvmexp.inactive < uvmexp.inactarg ||
		    uvm_pgcnt_vnode >
		    (uvmexp.active + uvmexp.inactive + uvmexp.wired +
		     uvmexp.free) * 13 / 16) {
#else
		if (uvmexp.free < uvmexp.freetarg ||
a252 1
#endif
d372 1
a372 1
	int dirtyreacts;
d433 1
a433 3
			 * skip to next page (unlikely to happen since
			 * inactive pages shouldn't have any valid mappings
			 * and we cleared reference before deactivating).
d443 29
d532 4
a535 2
			 * the page is not busy.   if the page is clean we
			 * can free it now and continue.
d538 5
a550 2
				/* zap all mappings with pmap_page_protect... */
				pmap_page_protect(p, VM_PROT_NONE);
d633 1
a633 3
			 * touch the page.   we write protect all the mappings
			 * of the page so that no one touches it while it is
			 * in I/O.
a638 1
			pmap_page_protect(p, VM_PROT_READ);
@


1.20.2.2
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pdaemon.c,v 1.20.2.1 2002/01/31 22:55:51 niklas Exp $	*/
/*	$NetBSD: uvm_pdaemon.c,v 1.42 2001/11/10 07:37:01 lukem Exp $	*/
d98 3
a100 3
void		uvmpd_scan __P((void));
boolean_t	uvmpd_scan_inactive __P((struct pglist *));
void		uvmpd_tune __P((void));
d120 1
a120 1
	if (curproc == uvm.pagedaemon_proc && uvmexp.paging == 0) {
d162 2
a163 2
void
uvmpd_tune(void)
d171 2
a172 2
	uvmexp.freemin = MAX(uvmexp.freemin, (16*1024) >> PAGE_SHIFT);
	uvmexp.freemin = MIN(uvmexp.freemin, (512*1024) >> PAGE_SHIFT);
d206 1
d225 3
d251 1
d253 6
d260 1
a278 6

		/*
		 * drain pool resources now that we're not holding any locks
		 */

		pool_drain(0);
d333 3
d352 2
d364 1
a364 1
boolean_t
d368 2
a369 1
	int error;
d372 5
d378 3
a380 5
	struct vm_page *swpps[MAXBSIZE >> PAGE_SHIFT];
	struct simplelock *slock;
	int swnpages, swcpages;
	int swslot;
	int dirtyreacts, t, result;
d391 1
d393 1
d395 6
d403 1
d407 2
a408 1
			 * see if we've met the free target.
d411 5
a415 2
			if (uvmexp.free + uvmexp.paging >=
			    uvmexp.freetarg << 2 ||
d419 1
a427 1
				nextpg = NULL;
d430 1
a435 1

d441 3
a443 1
			 * skip to next page.
d446 1
a446 1
			if (pmap_clear_reference(p)) {
a450 31
			anon = p->uanon;
			uobj = p->uobject;

			/*
			 * enforce the minimum thresholds on different
			 * types of memory usage.  if reusing the current
			 * page would reduce that type of usage below its
			 * minimum, reactivate the page instead and move
			 * on to the next page.
			 */

			t = uvmexp.active + uvmexp.inactive + uvmexp.free;
			if (anon &&
			    uvmexp.anonpages <= (t * uvmexp.anonmin) >> 8) {
				uvm_pageactivate(p);
				uvmexp.pdreanon++;
				continue;
			}
			if (uobj && UVM_OBJ_IS_VTEXT(uobj) &&
			    uvmexp.vtextpages <= (t * uvmexp.vtextmin) >> 8) {
				uvm_pageactivate(p);
				uvmexp.pdrevtext++;
				continue;
			}
			if (uobj && UVM_OBJ_IS_VNODE(uobj) &&
			    !UVM_OBJ_IS_VTEXT(uobj) &&
			    uvmexp.vnodepages <= (t * uvmexp.vnodemin) >> 8) {
				uvm_pageactivate(p);
				uvmexp.pdrevnode++;
				continue;
			}
d469 2
a470 1
			if ((p->pqflags & PQ_ANON) || uobj == NULL) {
d472 1
a472 2
				slock = &anon->an_lock;
				if (!simple_lock_try(slock)) {
d489 1
a489 1
					simple_unlock(slock);
d491 1
d496 1
d498 2
a499 2
				slock = &uobj->vmobjlock;
				if (!simple_lock_try(slock)) {
d503 1
a503 1
					simple_unlock(slock);
d505 1
a510 1

d513 2
a514 2
			 * if the page is not swap-backed, call the object's
			 * pager to flush and free the page.
d517 6
a522 9
			if ((p->pqflags & PQ_SWAPBACKED) == 0) {
				uvm_unlock_pageq();
				error = (uobj->pgops->pgo_put)(uobj, p->offset,
				    p->offset + PAGE_SIZE,
				    PGO_CLEANIT|PGO_FREE);
				uvm_lock_pageq();
				if (nextpg &&
				    (nextpg->flags & PQ_INACTIVE) == 0) {
					nextpg = TAILQ_FIRST(pglst);
a523 2
				continue;
			}
d525 2
a526 12
			/*
			 * the page is swap-backed.  remove all the permissions
			 * from the page so we can sync the modified info
			 * without any race conditions.  if the page is clean
			 * we can free it now and continue.
			 */

			pmap_page_protect(p, VM_PROT_NONE);
			if ((p->flags & PG_CLEAN) && pmap_clear_modify(p)) {
				p->flags &= ~(PG_CLEAN);
			}
			if (p->flags & PG_CLEAN) {
d530 6
a535 5
				/*
				 * for anons, we need to remove the page
				 * from the anon ourselves.  for aobjs,
				 * pagefree did that for us.
				 */
a536 1
				if (anon) {
d538 2
d541 5
a546 7
				simple_unlock(slock);

				/* this page is now only in swap. */
				simple_lock(&uvm.swap_data_lock);
				KASSERT(uvmexp.swpgonly < uvmexp.swpginuse);
				uvmexp.swpgonly++;
				simple_unlock(&uvm.swap_data_lock);
d555 6
a560 3
			if (uvmexp.free + uvmexp.paging >
			    uvmexp.freetarg << 2) {
				simple_unlock(slock);
d565 4
a568 17
			 * free any swap space allocated to the page since
			 * we'll have to write it again with its new data.
			 */

			if ((p->pqflags & PQ_ANON) && anon->an_swslot) {
				uvm_swap_free(anon->an_swslot, 1);
				anon->an_swslot = 0;
			} else if (p->pqflags & PQ_AOBJ) {
				uao_dropswap(uobj, p->offset >> PAGE_SHIFT);
			}

			/*
			 * if all pages in swap are only in swap,
			 * the swap space is full and we can't page out
			 * any more swap-backed pages.  reactivate this page
			 * so that we eventually cycle all pages through
			 * the inactive queue.
d572 2
a573 1
			if (uvmexp.swpgonly == uvmexp.swpages) {
d576 5
a580 1
				simple_unlock(slock);
d585 3
a587 1
			 * start new swap pageout cluster (if necessary).
d590 12
a601 6
			if (swslot == 0) {
				swnpages = MAXBSIZE >> PAGE_SHIFT;
				swslot = uvm_swap_alloc(&swnpages, TRUE);
				if (swslot == 0) {
					simple_unlock(slock);
					continue;
a602 1
				swcpages = 0;
d606 6
a611 5
			 * at this point, we're definitely going reuse this
			 * page.  mark the page busy and delayed-free.
			 * we should remove the page from the page queues
			 * so we don't ever look at it again.
			 * adjust counters and such.
d614 2
a615 1
			p->flags |= PG_BUSY;
d617 1
a617 5

			p->flags |= PG_PAGEOUT;
			uvmexp.paging++;
			uvm_pagedequeue(p);

d621 2
a622 1
			 * add the new page to the cluster.
d625 38
a662 13
			if (anon) {
				anon->an_swslot = swslot + swcpages;
				simple_unlock(slock);
			} else {
				result = uao_set_swslot(uobj,
				    p->offset >> PAGE_SHIFT, swslot + swcpages);
				if (result == -1) {
					p->flags &= ~(PG_BUSY|PG_PAGEOUT);
					UVM_PAGE_OWN(p, NULL);
					uvmexp.paging--;
					uvm_pageactivate(p);
					simple_unlock(slock);
					continue;
a663 4
				simple_unlock(slock);
			}
			swpps[swcpages] = p;
			swcpages++;
d665 3
a667 4
			/*
			 * if the cluster isn't full, look for more pages
			 * before starting the i/o.
			 */
d669 15
a683 2
			if (swcpages < swnpages) {
				continue;
d685 4
d692 7
a698 2
		 * if this is the final pageout we could have a few
		 * unused swap blocks.  if so, free them now.
d701 30
a730 2
		if (swcpages < swnpages) {
			uvm_swap_free(swslot + swcpages, (swnpages - swcpages));
d734 15
a748 1
		 * now start the pageout.
d751 1
a751 1
		uvm_unlock_pageq();
d753 3
a755 3
		error = uvm_swap_put(swslot, swpps, swcpages, 0);
		KASSERT(error == 0);
		uvm_lock_pageq();
d758 1
a758 1
		 * zero swslot to indicate that we are
d762 13
a774 1
		swslot = 0;
d781 2
d784 7
a790 2
		if (nextpg && (nextpg->pqflags & PQ_INACTIVE) == 0) {
			nextpg = TAILQ_FIRST(pglst);
d793 1
a793 1
	return (error);
d803 1
a803 1
uvmpd_scan(void)
d805 1
a805 1
	int inactive_shortage, swap_shortage, pages_freed;
d808 1
a808 1
	struct vm_anon *anon;
d811 1
a811 1
	uvmexp.pdrevs++;
d813 7
a819 1
	anon = NULL;
a821 1

d826 1
a826 2

	if (uvmexp.free < uvmexp.freetarg && uvmexp.nswapdev != 0) {
d828 2
a829 2
		UVMHIST_LOG(pdhist,"  free %d < target %d: swapout",
		    uvmexp.free, uvmexp.freetarg, 0, 0);
d851 1
d882 2
a883 3
		if (p->flags & PG_BUSY) {
			continue;
		}
d890 2
a891 3
			anon = p->uanon;
			KASSERT(anon != NULL);
			if (!simple_lock_try(&anon->an_lock)) {
a892 1
			}
d901 1
a901 2
			uobj = p->uobject;
			if (!simple_lock_try(&uobj->vmobjlock)) {
a902 1
			}
d911 1
a911 1
				simple_unlock(&anon->an_lock);
d913 1
a913 1
				simple_unlock(&uobj->vmobjlock);
d923 3
a925 3
			if ((p->pqflags & PQ_ANON) && anon->an_swslot) {
				uvm_swap_free(anon->an_swslot, 1);
				anon->an_swslot = 0;
d928 3
a930 2
			} else if (p->pqflags & PQ_AOBJ) {
				int slot = uao_set_swslot(uobj,
d941 4
a944 1
		 * if there's a shortage of inactive pages, deactivate.
d948 1
a953 5

		/*
		 * we're done with this page.
		 */

d955 1
a955 1
			simple_unlock(&anon->an_lock);
d957 1
a957 1
			simple_unlock(&uobj->vmobjlock);
@


1.20.2.3
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.20.2.2 2002/02/02 03:28:27 art Exp $	*/
d98 3
a100 3
static void		uvmpd_scan(void);
static boolean_t	uvmpd_scan_inactive(struct pglist *);
static void		uvmpd_tune(void);
a327 1
			s = splbio();	/* b_iodone must by called at splbio */
a328 1
			splx(s);
@


1.20.2.4
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pdaemon.c,v 1.20.2.3 2002/06/11 03:33:04 art Exp $	*/
/*	$NetBSD: uvm_pdaemon.c,v 1.47 2002/06/20 15:05:29 chs Exp $	*/
d85 1
a85 1
 * UVMPD_NUMDIRTYREACTS is how many dirty pages the pagedaemon will reactivate
d98 3
a100 3
void		uvmpd_scan(void);
void		uvmpd_scan_inactive(struct pglist *);
void		uvmpd_tune(void);
d355 1
a355 1
void
a367 3
	boolean_t anonunder, fileunder, execunder;
	boolean_t anonover, fileover, execover;
	boolean_t anonreact, filereact, execreact;
a378 16

	/*
	 * decide which types of pages we want to reactivate instead of freeing
	 * to keep usage within the minimum and maximum usage limits.
	 */

	t = uvmexp.active + uvmexp.inactive + uvmexp.free;
	anonunder = (uvmexp.anonpages <= (t * uvmexp.anonmin) >> 8);
	fileunder = (uvmexp.filepages <= (t * uvmexp.filemin) >> 8);
	execunder = (uvmexp.execpages <= (t * uvmexp.execmin) >> 8);
	anonover = uvmexp.anonpages > ((t * uvmexp.anonmax) >> 8);
	fileover = uvmexp.filepages > ((t * uvmexp.filemax) >> 8);
	execover = uvmexp.execpages > ((t * uvmexp.execmax) >> 8);
	anonreact = anonunder || (!anonover && (fileover || execover));
	filereact = fileunder || (!fileover && (anonover || execover));
	execreact = execunder || (!execover && (anonover || fileover));
d434 3
a436 1
			if (uobj && UVM_OBJ_IS_VTEXT(uobj) && execreact) {
d438 1
a438 1
				uvmexp.pdreexec++;
d441 2
a442 2
			if (uobj && UVM_OBJ_IS_VNODE(uobj) &&
			    !UVM_OBJ_IS_VTEXT(uobj) && filereact) {
d444 1
a444 1
				uvmexp.pdrefile++;
d447 3
a449 1
			if ((anon || UVM_OBJ_IS_AOBJ(uobj)) && anonreact) {
d451 1
a451 1
				uvmexp.pdreanon++;
d471 2
a472 13
			/* does the page belong to an object? */
			if (uobj != NULL) {
				slock = &uobj->vmobjlock;
				if (!simple_lock_try(slock)) {
					continue;
				}
				if (p->flags & PG_BUSY) {
					simple_unlock(slock);
					uvmexp.pdbusy++;
					continue;
				}
				uvmexp.pdobscan++;
			} else {
d476 1
d481 2
a482 1
				 * set PQ_ANON if it isn't set already.
d497 12
d525 1
a525 1
				    (nextpg->pqflags & PQ_INACTIVE) == 0) {
d705 1
a720 1
	struct simplelock *slock;
d760 1
a760 1
	uvmpd_scan_inactive(&uvm.page_inactive);
d796 2
a797 8

		if (p->uobject != NULL) {
			uobj = p->uobject;
			slock = &uobj->vmobjlock;
			if (!simple_lock_try(slock)) {
				continue;
			}
		} else {
d800 1
a800 2
			slock = &anon->an_lock;
			if (!simple_lock_try(slock)) {
d810 5
d822 4
a825 1
			simple_unlock(slock);
d866 4
a869 1
		simple_unlock(slock);
@


1.20.2.5
log
@add VEXECMAP.  also make sure to modify filepages count only in the not
execpages case in uvm_pageremove().
this actually appears to solve the swap freak out problems.  sitting on it for
a long time, never checked if it worked.  sigh.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.20.2.4 2002/11/04 18:02:33 art Exp $	*/
d167 1
a167 1
	uvmexp.freemin = uvmexp.npages / 20;
d169 1
a169 1
	/* between 16k and 256k */
d172 1
a172 1
	uvmexp.freemin = MIN(uvmexp.freemin, (256*1024) >> PAGE_SHIFT);
d363 1
a363 1
	struct vm_page *swpps[round_page(MAXPHYS) >> PAGE_SHIFT];
d532 1
a532 1
				(void)(uobj->pgops->pgo_put)(uobj, p->offset,
d622 1
a622 1
				swnpages = MAXPHYS >> PAGE_SHIFT;
@


1.19
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pdaemon.c,v 1.18 2001/11/12 01:26:10 art Exp $	*/
/*	$NetBSD: uvm_pdaemon.c,v 1.31 2001/03/10 22:46:50 chs Exp $	*/
d4 1
a4 1
/* 
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and 
d48 1
a48 1
 * 
d54 3
a56 3
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
d58 1
a58 1
 * 
a375 8
	 * note: we currently keep swap-backed pages on a seperate inactive
	 * list from object-backed pages.   however, merging the two lists
	 * back together again hasn't been ruled out.   thus, we keep our
	 * swap cluster in "swpps" rather than in pps (allows us to mix
	 * clustering types in the event of a mixed inactive queue).
	 */

	/*
d690 1
a690 2
				swpps[swcpages] = p;
				if (anon)
d692 2
a693 2
				else
					uao_set_swslot(uobj,
d696 8
d874 1
a874 6
	if ((uvmexp.pdrevs & 1) != 0 && uvmexp.nswapdev != 0)
		got_it = uvmpd_scan_inactive(&uvm.page_inactive_swp);
	if (!got_it)
		got_it = uvmpd_scan_inactive(&uvm.page_inactive_obj);
	if (!got_it && (uvmexp.pdrevs & 1) == 0 && uvmexp.nswapdev != 0)
		(void) uvmpd_scan_inactive(&uvm.page_inactive_swp);
d962 4
a965 3
		 * If the page has not been referenced since the
		 * last scan, deactivate the page if there is a
		 * shortage of inactive pages.
d968 2
a969 2
		if (inactive_shortage > 0 &&
		    pmap_clear_reference(p) == FALSE) {
@


1.18
log
@Bring in more changes from NetBSD. Mostly pagedaemon improvements.
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_pdaemon.c,v 1.30 2001/03/09 01:02:12 chs Exp $	*/
d770 1
a770 4
		 * OUT: locked: uobj (if !swap_backed && result !=VM_PAGER_PEND)
		 *     !locked: pageqs, uobj (if swap_backed || VM_PAGER_PEND)
		 *
		 * [the bit about VM_PAGER_PEND saves us one lock-unlock pair]
d777 1
a777 2
		/* locked: uobj (if !swap_backed && result != PEND) */
		/* unlocked: pageqs, object (if swap_backed ||result == PEND) */
d788 1
a788 10
		 * first, we check for VM_PAGER_PEND which means that the
		 * async I/O is in progress and the async I/O done routine
		 * will clean up after us.   in this case we move on to the
		 * next page.
		 *
		 * there is a very remote chance that the pending async i/o can
		 * finish _before_ we get here.   if that happens, our page "p"
		 * may no longer be on the inactive queue.   so we verify this
		 * when determining the next page (starting over at the head if
		 * we've lost our inactive page).
d791 1
a791 17
		if (result == VM_PAGER_PEND) {
			uvmexp.paging += npages;
			uvm_lock_pageq();
			uvmexp.pdpending++;
			if (p) {
				if (p->pqflags & PQ_INACTIVE)
					nextpg = TAILQ_NEXT(p, pageq);
				else
					nextpg = TAILQ_FIRST(pglst);
			} else {
				nextpg = NULL;
			}
			continue;
		}

		if (result == VM_PAGER_ERROR &&
		    curproc == uvm.pagedaemon_proc) {
d799 2
a800 1
		 * clean up "p" if we have one
d803 3
d807 1
a807 76
			/*
			 * the I/O request to "p" is done and uvm_pager_put
			 * has freed any cluster pages it may have allocated
			 * during I/O.  all that is left for us to do is
			 * clean up page "p" (which is still PG_BUSY).
			 *
			 * our result could be one of the following:
			 *   VM_PAGER_OK: successful pageout
			 *
			 *   VM_PAGER_AGAIN: tmp resource shortage, we skip
			 *     to next page
			 *   VM_PAGER_{FAIL,ERROR,BAD}: an error.   we
			 *     "reactivate" page to get it out of the way (it
			 *     will eventually drift back into the inactive
			 *     queue for a retry).
			 *   VM_PAGER_UNLOCK: should never see this as it is
			 *     only valid for "get" operations
			 */

			/* relock p's object: page queues not lock yet, so
			 * no need for "try" */

			/* !swap_backed case: already locked... */
			if (swap_backed) {
				if (anon)
					simple_lock(&anon->an_lock);
				else
					simple_lock(&uobj->vmobjlock);
			}

			/* handle PG_WANTED now */
			if (p->flags & PG_WANTED)
				/* still holding object lock */
				wakeup(p);

			p->flags &= ~(PG_BUSY|PG_WANTED);
			UVM_PAGE_OWN(p, NULL);

			/* released during I/O? */
			if (p->flags & PG_RELEASED) {
				if (anon) {
					/* remove page so we can get nextpg */
					anon->u.an_page = NULL;

					simple_unlock(&anon->an_lock);
					uvm_anfree(anon);	/* kills anon */
					pmap_page_protect(p, VM_PROT_NONE);
					anon = NULL;
					uvm_lock_pageq();
					nextpg = TAILQ_NEXT(p, pageq);
					/* free released page */
					uvm_pagefree(p);

				} else {

					/*
					 * pgo_releasepg nukes the page and
					 * gets "nextpg" for us.  it returns
					 * with the page queues locked (when
					 * given nextpg ptr).
					 */

					if (!uobj->pgops->pgo_releasepg(p,
					    &nextpg))
						/* uobj died after release */
						uobj = NULL;

					/*
					 * lock page queues here so that they're
					 * always locked at the end of the loop.
					 */

					uvm_lock_pageq();
				}
			} else {	/* page was not released during I/O */
				uvm_lock_pageq();
d809 2
a810 28
				if (result != VM_PAGER_OK) {
					/* pageout was a failure... */
					if (result != VM_PAGER_AGAIN)
						uvm_pageactivate(p);
					pmap_clear_reference(p);
					/* XXXCDC: if (swap_backed) FREE p's
					 * swap block? */
				} else {
					/* pageout was a success... */
					pmap_clear_reference(p);
					pmap_clear_modify(p);
					p->flags |= PG_CLEAN;
				}
			}

			/*
			 * drop object lock (if there is an object left).   do
			 * a safety check of nextpg to make sure it is on the
			 * inactive queue (it should be since PG_BUSY pages on
			 * the inactive queue can't be re-queued [note: not
			 * true for active queue]).
			 */

			if (anon)
				simple_unlock(&anon->an_lock);
			else if (uobj)
				simple_unlock(&uobj->vmobjlock);

a811 6

			/*
			 * if p is null in this loop, make sure it stays null
			 * in the next loop.
			 */

a812 11

			/*
			 * lock page queues here just so they're always locked
			 * at the end of the loop.
			 */

			uvm_lock_pageq();
		}

		if (nextpg && (nextpg->pqflags & PQ_INACTIVE) == 0) {
			nextpg = TAILQ_FIRST(pglst);	/* reload! */
@


1.17
log
@Merge in some parts of the ubc work that has been done in NetBSD that are not
UBC, but prerequsites for it.

- Create a daemon that processes async I/O (swap and paging in the future)
  requests that need processing in process context and that were processed
  in the pagedaemon before.
- Convert some ugly ifdef DIAGNOSTIC code to less intrusive KASSERTs.
- misc other cleanups.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pdaemon.c,v 1.16 2001/11/06 13:36:52 art Exp $	*/
/*	$NetBSD: uvm_pdaemon.c,v 1.23 2000/08/20 10:24:14 bjh21 Exp $	*/
d80 1
a101 1

d148 1
a148 1
	UVM_UNLOCK_AND_WAIT(&uvmexp.free, &uvm.pagedaemon_lock, FALSE, (char *)wmsg,
d167 1
a167 1
	uvmexp.freemin = uvmexp.npages / 30;
d169 1
a169 1
	/* between 16k and 512k */
d172 1
a172 1
	uvmexp.freemin = min(uvmexp.freemin, (512*1024) >> PAGE_SHIFT);
a250 1
#ifdef UBC
a251 6
		    uvmexp.inactive < uvmexp.inactarg ||
		    uvm_pgcnt_vnode >
		    (uvmexp.active + uvmexp.inactive + uvmexp.wired +
		     uvmexp.free) * 13 / 16) {
#else
		if (uvmexp.free < uvmexp.freetarg ||
a252 1
#endif
d372 1
a372 1
	int dirtyreacts;
d376 1
a376 1
	 * note: we currently keep swap-backed pages on a separate inactive
d385 1
a385 1
	 * to stay in the loop while we have a page to scan or we have 
d388 1
d394 1
a394 1
	for (p = pglst->tqh_first ; p != NULL || swslot != 0 ; p = nextpg) {
d400 4
d405 1
d410 1
d418 2
a419 2
				    "exit loop", 0, 0, 0, 0);
				retval = TRUE;		/* hit the target! */
d421 1
a421 1
				if (swslot == 0)
d424 1
d431 1
a431 2
		uobj = NULL;	/* be safe and shut gcc up */
		anon = NULL;	/* be safe and shut gcc up */
a432 1
		if (p) {	/* if (we have a new page to consider) */
d437 1
a437 1
			nextpg = p->pageq.tqe_next;
d441 1
a441 3
			 * skip to next page (unlikely to happen since
			 * inactive pages shouldn't have any valid mappings
			 * and we cleared reference before deactivating).
d443 1
d449 30
a478 1
			
d485 1
a485 1
			 * get deadlocked.
d487 1
a487 1
			 * the only time we exepct to see an ownerless page
d494 1
a494 1
		
a496 1

d498 2
a499 10

#ifdef DIAGNOSTIC
				/* to be on inactive q, page must be part
				 * of _something_ */
				if (anon == NULL)
					panic("pagedaemon: page with no anon "
					    "or object detected - loop 1");
#endif

				if (!simple_lock_try(&anon->an_lock))
d502 1
d508 1
d510 1
a510 6
#ifdef DIAGNOSTIC
					if (p->loan_count < 1)
						panic("pagedaemon: non-loaned "
						    "ownerless page detected -"
						    " loop 1");
#endif
d512 2
a513 1
					p->pqflags |= PQ_ANON;      /* anon now owns it */
a514 1

a520 1

a521 1

a522 1

d524 2
a525 2

				if (!simple_lock_try(&uobj->vmobjlock))
d527 2
a528 2
					continue;	

d533 1
a533 1
					continue;	
a534 1

d540 4
a543 2
			 * the page is not busy.   if the page is clean we
			 * can free it now and continue.
d546 5
a558 2
				/* zap all mappings with pmap_page_protect... */
				pmap_page_protect(p, VM_PROT_NONE);
d561 1
a561 1
			
d563 1
a563 1
#ifdef DIAGNOSTIC
d566 1
a566 1
					 * if it has valid backing store.
d568 3
a570 4
					if (anon->an_swslot == 0)
						panic("pagedaemon: clean anon "
						 "page without backing store?");
#endif
d641 1
a641 3
			 * touch the page.   we write protect all the mappings
			 * of the page so that no one touches it while it is
			 * in I/O.
d643 1
a643 1
		
a646 1
			pmap_page_protect(p, VM_PROT_READ);
d653 1
d659 1
a818 1
#ifdef UBC
a825 1
#endif
a861 6
#ifdef DIAGNOSTIC
			if (result == VM_PAGER_UNLOCK)
				panic("pagedaemon: pageout returned "
				    "invalid 'unlock' code");
#endif

d945 1
a945 1
			
d1111 3
a1113 2
		 * deactivate this page if there's a shortage of
		 * inactive pages.
d1116 2
a1117 2
		if (inactive_shortage > 0) {
			pmap_page_protect(p, VM_PROT_NONE);
@


1.16
log
@More sync to NetBSD.
 - Use malloc/free instead of MALLOC/FREE for variable sized allocations.
 - Move the memory inheritance code to sys/mman.h and rename from VM_* to MAP_*
 - various cleanups and simplifications.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.15 2001/11/06 01:35:04 art Exp $	*/
d79 1
a196 2
	int s;
	struct uvm_aiodesc *aio, *nextaio;
d198 1
a198 1
	 
a214 7
	while (TRUE) {

		/*
		 * carefully attempt to go to sleep (without losing "wakeups"!).
		 * we need splbio because we want to make sure the aio_done list
		 * is totally empty before we go to sleep.
		 */
d216 1
a216 1
		s = splbio();
d219 5
a223 37
		/*
		 * if we've got done aio's, then bypass the sleep
		 */

		if (uvm.aio_done.tqh_first == NULL) {
			UVMHIST_LOG(maphist,"  <<SLEEPING>>",0,0,0,0);
			UVM_UNLOCK_AND_WAIT(&uvm.pagedaemon,
			    &uvm.pagedaemon_lock, FALSE, "daemon_slp", 0);
			uvmexp.pdwoke++;
			UVMHIST_LOG(pdhist,"  <<WOKE UP>>",0,0,0,0);

			/* relock pagedaemon_lock, still at splbio */
			simple_lock(&uvm.pagedaemon_lock);
		}

		/*
		 * check for done aio structures
		 */

		aio = uvm.aio_done.tqh_first;	/* save current list (if any)*/
		if (aio) {
			TAILQ_INIT(&uvm.aio_done);	/* zero global list */
		}

		simple_unlock(&uvm.pagedaemon_lock);	/* unlock */
		splx(s);				/* drop splbio */
 
		/*
		 * first clear out any pending aios (to free space in case we
		 * want to pageout more stuff).
		 */

		for (/*null*/; aio != NULL ; aio = nextaio) {

			uvmexp.paging -= aio->npages;
			nextaio = aio->aioq.tqe_next;
			aio->aiodone(aio);
d225 1
a225 3
		}

		/* Next, drain pool resources */
d231 1
a232 1

d239 1
a239 1
		if (uvmexp.inactarg <= uvmexp.freetarg)
d241 1
a248 1
		 * [XXX: note we are reading uvm.free without locking]
d250 8
d259 2
a260 1
		    uvmexp.inactive < uvmexp.inactarg)
d262 1
d265 2
a266 1
		 * done scan.  unlock page queues (the only lock we are holding)
d268 10
d279 17
d298 3
a300 1
		 * done!    restart loop.
d302 44
a345 2
		if (uvmexp.free > uvmexp.reserve_kernel ||
		    uvmexp.paging == 0)
d347 2
a349 1
	/*NOTREACHED*/
d352 2
d355 1
a355 2
 * uvmpd_scan_inactive: the first loop of uvmpd_scan broken out into
 * 	its own function for ease of reading.
d572 1
d588 2
a589 5
#ifdef DIAGNOSTIC
			if (uvmexp.swpgonly > uvmexp.swpages) {
				panic("uvmexp.swpgonly botch");
			}
#endif
d607 2
a608 5
#ifdef DIAGNOSTIC
			if (uvmexp.swpginuse > uvmexp.swpages) {
				panic("uvmexp.swpginuse botch");
			}
#endif
d661 1
a662 1
					/* want this much */
a663 1

a665 1

d684 1
a692 2

				/* done (swap-backed) */
a693 2

			/* end: if (p) ["if we have new page to consider"] */ 
d701 1
a701 1
		 * now consider doing the pageout.   
d703 2
a704 2
		 * for swap-backed pages, we do the pageout if we have either 
		 * filled the cluster (in which case (swnpages == swcpages) or 
d709 1
a710 1

d733 1
a733 2
			} 
	
a734 1

a739 1

d744 1
a744 1
		 * 
d765 1
a765 1
		result = uvm_pager_put((swap_backed) ? NULL : uobj, p,
d793 1
a793 1
			uvm_lock_pageq();		/* relock page queues */
d797 1
a797 2
					/* reload! */
					nextpg = p->pageq.tqe_next;
d799 3
a801 4
					/* reload! */
					nextpg = pglst->tqh_first;
				} else {
					nextpg = NULL;		/* done list */
d806 10
d876 1
a876 1
					nextpg = p->pageq.tqe_next;
d882 1
a882 7
#ifdef DIAGNOSTIC
					if (uobj->pgops->pgo_releasepg == NULL)
						panic("pagedaemon: no "
						   "pgo_releasepg function");
#endif

					/* 
d888 1
d898 1
a900 1

a901 1

d903 1
a903 2
				nextpg = p->pageq.tqe_next;

a904 1

a910 1

a911 1

a915 3
					/* XXX: could free page here, but old
					 * pagedaemon does not */

d918 1
a918 1
			
d932 6
a937 1
		} /* if (p) */ else {
a938 2
			/* if p is null in this loop, make sure it stays null
			 * in next loop */
d945 1
d950 1
a950 3
			printf("pagedaemon: invalid nextpg!   reverting to "
			    "queue head\n");
			nextpg = pglst->tqh_first;	/* reload! */
d952 1
a952 2

	}	/* end of "inactive" 'for' loop */
d972 1
a973 3
#ifdef __GNUC__
	uobj = NULL;	/* XXX gcc */
#endif
a986 1

a991 1
		pmap_update();		/* update so we can scan inactive q */
d1007 2
a1008 2
	 * do loop #1!   alternate starting queue between swap and object based
	 * on the low bit of uvmexp.pdrevs (which we bump by one each call).
d1032 1
d1040 1
a1040 1
 
d1043 1
a1043 1
	for (p = TAILQ_FIRST(&uvm.page_active); 
d1046 1
a1046 1
		nextpg = p->pageq.tqe_next;
d1055 1
a1055 6

#ifdef DIAGNOSTIC
			if (p->uanon == NULL)
				panic("pagedaemon: page with no anon or "
				    "object detected - loop 2");
#endif
d1061 1
a1061 5
#ifdef DIAGNOSTIC
				if (p->loan_count < 1)
					panic("pagedaemon: non-loaned "
					    "ownerless page detected - loop 2");
#endif
d1069 1
d1073 1
d1081 1
a1081 1
 
d1086 1
d1104 1
a1104 1
 
d1109 1
a1116 1

@


1.15
log
@Move the last content from vm/ to uvm/
The only thing left in vm/ are just dumb wrappers.
vm/vm.h includes uvm/uvm_extern.h
vm/pmap.h includes uvm/uvm_pmap.h
vm/vm_page.h includes uvm/uvm_page.h
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pdaemon.c,v 1.14 2001/11/05 22:14:54 art Exp $	*/
/*	$NetBSD: uvm_pdaemon.c,v 1.21 2000/06/27 17:29:33 mrg Exp $	*/
d173 4
d193 1
a193 1
uvm_pageout()
@


1.14
log
@Minor sync to NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pdaemon.c,v 1.13 2001/09/11 20:05:26 miod Exp $	*/
/*	$NetBSD: uvm_pdaemon.c,v 1.20 2000/06/26 14:21:18 mrg Exp $	*/
a78 2

#include <vm/vm.h>
@


1.13
log
@Don't include <vm/vm_kern.h> if you don't need foo_map.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pdaemon.c,v 1.12 2001/08/11 10:57:22 art Exp $	*/
/*	$NetBSD: uvm_pdaemon.c,v 1.19 1999/11/04 21:51:42 thorpej Exp $	*/
a80 1
#include <vm/vm_page.h>
@


1.12
log
@Various random fixes from NetBSD.
Including support for zeroing pages in the idle loop (not enabled yet).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.11 2001/07/18 14:27:07 art Exp $	*/
a81 1
#include <vm/vm_kern.h>
@


1.11
log
@Make uvm_wait take a const char *
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.10 2001/07/18 10:47:05 art Exp $	*/
d151 1
a151 1
	UVM_UNLOCK_AND_WAIT(&uvmexp.free, &uvm.pagedaemon_lock, FALSE, wmsg,
@


1.10
log
@Get rid of the PMAP_NEW option by making it mandatory for all archs.
The archs that didn't have a proper PMAP_NEW now have a dummy implementation
with wrappers around the old functions.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pdaemon.c,v 1.9 2001/03/22 03:05:56 smart Exp $	*/
/*	$NetBSD: uvm_pdaemon.c,v 1.18 1999/09/12 01:17:41 chs Exp $	*/
d114 1
a114 1
	char *wmsg;
@


1.9
log
@Sync style, typo, and comments a little closer to NetBSD.  art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pdaemon.c,v 1.8 2001/03/08 15:21:37 smart Exp $	*/
/*	$NetBSD: uvm_pdaemon.c,v 1.17 1999/07/22 22:58:39 thorpej Exp $	*/
d405 1
a405 1
			if (pmap_is_referenced(PMAP_PGARG(p))) {
d501 1
a501 1
				pmap_page_protect(PMAP_PGARG(p), VM_PROT_NONE);
d598 1
a598 1
			pmap_page_protect(PMAP_PGARG(p), VM_PROT_READ);
d836 1
a836 2
					pmap_page_protect(PMAP_PGARG(p),
					    VM_PROT_NONE);
d879 1
a879 1
					pmap_clear_reference(PMAP_PGARG(p));
d886 2
a887 2
					pmap_clear_reference(PMAP_PGARG(p));
					pmap_clear_modify(PMAP_PGARG(p));
d1094 1
a1094 1
			pmap_page_protect(PMAP_PGARG(p), VM_PROT_NONE);
@


1.8
log
@Replace thread_wakeup() with wakeup().  It is defined in vm_extern.h as a
wrapper, so this removes a dependence on the old VM system.  From NetBSD.
art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pdaemon.c,v 1.7 2001/01/29 02:07:48 niklas Exp $	*/
d112 2
a113 1
void uvm_wait(wmsg)
@


1.7
log
@$OpenBSD$
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pdaemon.c,v 1.16 1999/05/24 19:10:57 thorpej Exp $	*/
/*	$NetBSD: uvm_pdaemon.c,v 1.16 1999/05/24 19:10:57 thorpej Exp $	*/
d149 1
a149 1
	thread_wakeup(&uvm.pagedaemon);		/* wake the daemon! */
d305 1
a305 1
			thread_wakeup(&uvmexp.free);
d822 1
a822 1
				thread_wakeup(p);
@


1.6
log
@seperate -> separate, okay aaron@@
@
text
@d1 1
@


1.5
log
@Bring in some new UVM code from NetBSD (not current).

 - Introduce a new type of map that are interrupt safe and never allow faults
   in them. mb_map and kmem_map are made intrsafe.
 - Add "access protection" to uvm_vslock (to be passed down to uvm_fault and
   later to pmap_enter).
 - madvise(2) now works.
 - various cleanups.
@
text
@d340 1
a340 1
	 * note: we currently keep swap-backed pages on a seperate inactive
@


1.4
log
@swap encryption for UVM, option UVM_SWAP_ENCRYPT.  needs to be enabled
via sysctl.
Pages are encrypted with the Blowfish encryption algorithm, the key
is initialized randomly on first swap out, ensuring that entropy has
accumulated in the kernel randomness pool.  Eventually, swap encryption
will be decided on a process by process basis, e.g. a process that reads from
a cryptographic filesystem will enable swap encrypt for its pages. okay
art@@ and deraadt@@.
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_pdaemon.c,v 1.14 1999/03/26 17:33:30 chs Exp $	*/
d368 1
a368 2
			s = splimp();
			uvm_lock_fpageq();
d370 1
a370 2
			uvm_unlock_fpageq();
			splx(s);
d953 1
a953 2
	s = splimp();
	uvm_lock_fpageq();
d955 1
a955 2
	uvm_unlock_fpageq();
	splx(s);
@


1.4.2.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_pdaemon.c,v 1.16 1999/05/24 19:10:57 thorpej Exp $	*/
d368 2
a369 1
			s = uvm_lock_fpageq();
d371 2
a372 1
			uvm_unlock_fpageq(s);
d955 2
a956 1
	s = uvm_lock_fpageq();
d958 2
a959 1
	uvm_unlock_fpageq(s);
@


1.4.2.2
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_pdaemon.c,v 1.9 2001/03/22 03:05:56 smart Exp $	*/
/*	$NetBSD: uvm_pdaemon.c,v 1.17 1999/07/22 22:58:39 thorpej Exp $	*/
d111 1
a111 2
void
uvm_wait(wmsg)
d148 1
a148 1
	wakeup(&uvm.pagedaemon);		/* wake the daemon! */
d304 1
a304 1
			wakeup(&uvmexp.free);
d340 1
a340 1
	 * note: we currently keep swap-backed pages on a separate inactive
d821 1
a821 1
				wakeup(p);
@


1.4.2.3
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_pdaemon.c,v 1.19 1999/11/04 21:51:42 thorpej Exp $	*/
d82 1
d114 1
a114 1
	const char *wmsg;
d151 1
a151 1
	UVM_UNLOCK_AND_WAIT(&uvmexp.free, &uvm.pagedaemon_lock, FALSE, (char *)wmsg,
d405 1
a405 1
			if (pmap_is_referenced(p)) {
d501 1
a501 1
				pmap_page_protect(p, VM_PROT_NONE);
d598 1
a598 1
			pmap_page_protect(p, VM_PROT_READ);
d836 2
a837 1
					pmap_page_protect(p, VM_PROT_NONE);
d880 1
a880 1
					pmap_clear_reference(p);
d887 2
a888 2
					pmap_clear_reference(p);
					pmap_clear_modify(p);
d1095 1
a1095 1
			pmap_page_protect(p, VM_PROT_NONE);
@


1.4.2.4
log
@merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_pdaemon.c,v 1.30 2001/03/09 01:02:12 chs Exp $	*/
d79 3
a81 2
#include <sys/buf.h>
#include <sys/vnode.h>
d103 1
d150 1
a150 1
	UVM_UNLOCK_AND_WAIT(&uvmexp.free, &uvm.pagedaemon_lock, FALSE, wmsg,
d169 1
a169 1
	uvmexp.freemin = uvmexp.npages / 20;
d171 1
a171 1
	/* between 16k and 256k */
d174 1
a174 5
	uvmexp.freemin = min(uvmexp.freemin, (256*1024) >> PAGE_SHIFT);

	/* Make sure there's always a user page free. */
	if (uvmexp.freemin < uvmexp.reserve_kernel + 1)
		uvmexp.freemin = uvmexp.reserve_kernel + 1;
d192 1
a192 1
uvm_pageout(void *arg)
d195 2
d198 1
a198 1

d215 1
d217 7
a223 1
	for (;;) {
d226 10
a235 5
		UVMHIST_LOG(pdhist,"  <<SLEEPING>>",0,0,0,0);
		UVM_UNLOCK_AND_WAIT(&uvm.pagedaemon,
		    &uvm.pagedaemon_lock, FALSE, "pgdaemon", 0);
		uvmexp.pdwoke++;
		UVMHIST_LOG(pdhist,"  <<WOKE UP>>",0,0,0,0);
d237 30
a266 1
		/* drain pool resources */
d272 1
a273 1
		uvm_lock_pageq();
d280 1
a280 1
		if (uvmexp.inactarg <= uvmexp.freetarg) {
a281 1
		}
d289 1
d291 2
a292 3

		if (uvmexp.free + uvmexp.paging < uvmexp.freetarg ||
		    uvmexp.inactive < uvmexp.inactarg) {
a293 1
		}
d296 1
a296 2
		 * if there's any free memory to be had,
		 * wake up any waiters.
a297 10

		if (uvmexp.free > uvmexp.reserve_kernel ||
		    uvmexp.paging == 0) {
			wakeup(&uvmexp.free);
		}

		/*
		 * scan done.  unlock page queues (the only lock we are holding)
		 */

a298 17
	}
	/*NOTREACHED*/
}


/*
 * uvm_aiodone_daemon:  main loop for the aiodone daemon.
 */

void
uvm_aiodone_daemon(void *arg)
{
	int s, free;
	struct buf *bp, *nbp;
	UVMHIST_FUNC("uvm_aiodoned"); UVMHIST_CALLED(pdhist);

	for (;;) {
d301 1
a301 3
		 * carefully attempt to go to sleep (without losing "wakeups"!).
		 * we need splbio because we want to make sure the aio_done list
		 * is totally empty before we go to sleep.
d303 2
a304 44

		s = splbio();
		simple_lock(&uvm.aiodoned_lock);
		if (TAILQ_FIRST(&uvm.aio_done) == NULL) {
			UVMHIST_LOG(pdhist,"  <<SLEEPING>>",0,0,0,0);
			UVM_UNLOCK_AND_WAIT(&uvm.aiodoned,
			    &uvm.aiodoned_lock, FALSE, "aiodoned", 0);
			UVMHIST_LOG(pdhist,"  <<WOKE UP>>",0,0,0,0);

			/* relock aiodoned_lock, still at splbio */
			simple_lock(&uvm.aiodoned_lock);
		}

		/*
		 * check for done aio structures
		 */

		bp = TAILQ_FIRST(&uvm.aio_done);
		if (bp) {
			TAILQ_INIT(&uvm.aio_done);
		}

		simple_unlock(&uvm.aiodoned_lock);
		splx(s);

		/*
		 * process each i/o that's done.
		 */

		free = uvmexp.free;
		while (bp != NULL) {
			if (bp->b_flags & B_PDAEMON) {
				uvmexp.paging -= bp->b_bufsize >> PAGE_SHIFT;
			}
			nbp = TAILQ_NEXT(bp, b_freelist);
			(*bp->b_iodone)(bp);
			bp = nbp;
		}
		if (free <= uvmexp.reserve_kernel) {
			s = uvm_lock_fpageq();
			wakeup(&uvm.pagedaemon);
			uvm_unlock_fpageq(s);
		} else {
			simple_lock(&uvm.pagedaemon_lock);
a305 2
			simple_unlock(&uvm.pagedaemon_lock);
		}
d307 1
a309 2


d311 2
a312 1
 * uvmpd_scan_inactive: scan an inactive list for pages to clean or free.
d337 1
a337 1
	int dirtyreacts, t;
d341 1
a341 1
	 * note: we currently keep swap-backed pages on a seperate inactive
d350 1
a350 1
	 * to stay in the loop while we have a page to scan or we have
a352 1

d358 1
a358 1
	for (p = TAILQ_FIRST(pglst); p != NULL || swslot != 0; p = nextpg) {
a363 4

		uobj = NULL;
		anon = NULL;

a364 1

a368 1

d376 2
a377 2
					    "exit loop", 0, 0, 0, 0);
				retval = TRUE;
d379 1
a379 1
				if (swslot == 0) {
a381 1
				}
d388 3
a391 1

d396 1
a396 1
			nextpg = TAILQ_NEXT(p, pageq);
d400 3
a402 1
			 * skip to next page.
a403 1

d409 1
a409 30

			/*
			 * enforce the minimum thresholds on different
			 * types of memory usage.  if reusing the current
			 * page would reduce that type of usage below its
			 * minimum, reactivate the page instead and move
			 * on to the next page.
			 */

			t = uvmexp.active + uvmexp.inactive + uvmexp.free;
			if (p->uanon &&
			    uvmexp.anonpages <= (t * uvmexp.anonmin) >> 8) {
				uvm_pageactivate(p);
				uvmexp.pdreanon++;
				continue;
			}
			if (p->uobject && UVM_OBJ_IS_VTEXT(p->uobject) &&
			    uvmexp.vtextpages <= (t * uvmexp.vtextmin) >> 8) {
				uvm_pageactivate(p);
				uvmexp.pdrevtext++;
				continue;
			}
			if (p->uobject && UVM_OBJ_IS_VNODE(p->uobject) &&
			    !UVM_OBJ_IS_VTEXT(p->uobject) &&
			    uvmexp.vnodepages <= (t * uvmexp.vnodemin) >> 8) {
				uvm_pageactivate(p);
				uvmexp.pdrevnode++;
				continue;
			}

d416 1
a416 1
			 * deadlock.
d418 1
a418 1
			 * the only time we expect to see an ownerless page
d425 1
a425 1

d428 1
d430 10
a439 2
				KASSERT(anon != NULL);
				if (!simple_lock_try(&anon->an_lock)) {
a441 1
				}
a446 1

d448 6
a453 1
					KASSERT(p->loan_count > 0);
d455 1
a455 2
					p->pqflags |= PQ_ANON;
					/* anon now owns it */
d457 1
d464 1
d466 1
d468 1
d470 2
a471 2
				KASSERT(uobj != NULL);
				if (!simple_lock_try(&uobj->vmobjlock)) {
d473 2
a474 2
					continue;
				}
d479 1
a479 1
					continue;
d481 1
d487 2
a488 4
			 * the page is not busy.  remove all the permissions
			 * from the page so we can sync the modified info
			 * without any race conditions.  if the page is clean
			 * we can free it now and continue.
a490 5
			pmap_page_protect(p, VM_PROT_NONE);
			if ((p->flags & PG_CLEAN) != 0 && pmap_is_modified(p)) {
				p->flags &= ~PG_CLEAN;
			}

d499 2
d503 1
a503 1

d505 1
a505 1

d508 1
a508 1
					 * if it has backing store assigned.
d510 4
a513 3

					KASSERT(anon->an_swslot != 0);

a528 1

d544 5
a548 2

			KASSERT(uvmexp.swpgonly <= uvmexp.swpages);
d566 5
a570 2

			KASSERT(uvmexp.swpginuse <= uvmexp.swpages);
d589 3
a591 1
			 * touch the page.
d593 1
a593 1

d597 1
a603 1

a608 1

a622 1

d624 1
d626 1
d629 1
a647 1

d656 2
d659 2
d668 1
a668 1
		 * now consider doing the pageout.
d670 2
a671 2
		 * for swap-backed pages, we do the pageout if we have either
		 * filled the cluster (in which case (swnpages == swcpages) or
d676 1
a677 1
		if (swap_backed) {
d700 2
a701 1
			}
d703 1
d709 1
d714 1
a714 1
		 *
d735 1
a735 1
		result = uvm_pager_put(swap_backed ? NULL : uobj, p,
d763 1
a763 1
			uvm_lock_pageq();
d767 2
a768 1
					nextpg = TAILQ_NEXT(p, pageq);
d770 4
a773 3
					nextpg = TAILQ_FIRST(pglst);
			} else {
				nextpg = NULL;
a777 8
		if (result == VM_PAGER_ERROR &&
		    curproc == uvm.pagedaemon_proc) {
			uvm_lock_pageq();
			nextpg = TAILQ_NEXT(p, pageq);
			uvm_pageactivate(p);
			continue;
		}

d813 6
d838 1
a838 1
					nextpg = TAILQ_NEXT(p, pageq);
d844 7
a850 1
					/*
a855 1

a864 1

d867 1
d869 1
d871 2
a872 1
				nextpg = TAILQ_NEXT(p, pageq);
d874 1
d881 1
d883 1
d888 3
d893 1
a893 1

d907 1
a907 6
		} else {

			/*
			 * if p is null in this loop, make sure it stays null
			 * in the next loop.
			 */
d909 2
d912 1
a912 1

a916 1

d921 3
a923 1
			nextpg = TAILQ_FIRST(pglst);	/* reload! */
d925 2
a926 1
	}
a945 1
	uobj = NULL;
d947 3
d963 1
d969 1
d985 2
a986 2
	 * alternate starting queue between swap and object based on the
	 * low bit of uvmexp.pdrevs (which we bump by one each call).
a1009 1

d1017 1
a1017 1

d1020 1
a1020 1
	for (p = TAILQ_FIRST(&uvm.page_active);
d1023 1
a1023 1
		nextpg = TAILQ_NEXT(p, pageq);
d1032 6
a1037 1
			KASSERT(p->uanon != NULL);
d1043 5
a1047 1
				KASSERT(p->loan_count > 0);
a1054 1

a1057 1

d1065 1
a1065 1

a1069 1

d1087 1
a1087 1

d1089 2
a1090 3
		 * If the page has not been referenced since the
		 * last scan, deactivate the page if there is a
		 * shortage of inactive pages.
d1092 2
a1093 3

		if (inactive_shortage > 0 &&
		    pmap_clear_reference(p) == FALSE) {
d1099 1
@


1.4.2.5
log
@Merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_pdaemon.c,v 1.36 2001/06/27 18:52:10 thorpej Exp $	*/
d4 1
a4 1
/*
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and
d48 1
a48 1
 *
d54 3
a56 3
 *
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
d58 1
a58 1
 *
d376 8
d698 2
a699 1
				if (anon) {
d701 2
a702 2
				} else {
					result = uao_set_swslot(uobj,
a704 8
					if (result == -1) {
						p->flags &= ~PG_BUSY;
						UVM_PAGE_OWN(p, NULL);
						simple_unlock(&uobj->vmobjlock);
						continue;
					}
				}
				swpps[swcpages] = p;
d770 4
a773 1
		 * OUT:!locked: pageqs, uobj
d780 2
a781 1
		/* unlocked: pageqs, uobj */
d792 10
a801 1
		 * if the pageout failed, reactivate the page and continue.
d804 17
a820 1
		if (result == EIO && curproc == uvm.pagedaemon_proc) {
d828 1
a828 2
		 * the pageout is in progress.  bump counters and set up
		 * for the next loop.
a830 3
		uvm_lock_pageq();
		uvmexp.paging += npages;
		uvmexp.pdpending++;
d832 76
a907 1
			if (p->pqflags & PQ_INACTIVE)
d909 28
a936 2
			else
				nextpg = TAILQ_FIRST(pglst);
d938 6
d945 11
d1018 6
a1023 1
	(void) uvmpd_scan_inactive(&uvm.page_inactive);
d1111 3
a1113 4
		 * If we're short on inactive pages, move this over
		 * to the inactive list.  The second hand will sweep
		 * it later, and if it has been referenced again, it
		 * will be moved back to active.
d1116 2
a1117 2
		if (inactive_shortage > 0) {
			pmap_clear_reference(p);
@


1.4.2.6
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_pdaemon.c,v 1.23 2000/08/20 10:24:14 bjh21 Exp $	*/
d4 1
a4 1
/* 
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and 
d48 1
a48 1
 * 
d54 3
a56 3
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
d58 1
a58 1
 * 
d167 1
a167 1
	uvmexp.freemin = uvmexp.npages / 30;
d169 1
a169 1
	/* between 16k and 512k */
d172 1
a172 1
	uvmexp.freemin = min(uvmexp.freemin, (512*1024) >> PAGE_SHIFT);
a250 1
#ifdef UBC
a251 6
		    uvmexp.inactive < uvmexp.inactarg ||
		    uvm_pgcnt_vnode >
		    (uvmexp.active + uvmexp.inactive + uvmexp.wired +
		     uvmexp.free) * 13 / 16) {
#else
		if (uvmexp.free < uvmexp.freetarg ||
a252 1
#endif
d372 1
a372 1
	int dirtyreacts;
a375 8
	 * note: we currently keep swap-backed pages on a separate inactive
	 * list from object-backed pages.   however, merging the two lists
	 * back together again hasn't been ruled out.   thus, we keep our
	 * swap cluster in "swpps" rather than in pps (allows us to mix
	 * clustering types in the event of a mixed inactive queue).
	 */

	/*
d433 1
a433 3
			 * skip to next page (unlikely to happen since
			 * inactive pages shouldn't have any valid mappings
			 * and we cleared reference before deactivating).
d443 29
d532 4
a535 2
			 * the page is not busy.   if the page is clean we
			 * can free it now and continue.
d538 5
a550 2
				/* zap all mappings with pmap_page_protect... */
				pmap_page_protect(p, VM_PROT_NONE);
d633 1
a633 3
			 * touch the page.   we write protect all the mappings
			 * of the page so that no one touches it while it is
			 * in I/O.
a638 1
			pmap_page_protect(p, VM_PROT_READ);
d690 1
a690 2
				swpps[swcpages] = p;
				if (anon)
d692 2
a693 2
				else
					uao_set_swslot(uobj,
d696 8
d769 1
a769 4
		 * OUT: locked: uobj (if !swap_backed && result !=VM_PAGER_PEND)
		 *     !locked: pageqs, uobj (if swap_backed || VM_PAGER_PEND)
		 *
		 * [the bit about VM_PAGER_PEND saves us one lock-unlock pair]
d776 1
a776 2
		/* locked: uobj (if !swap_backed && result != PEND) */
		/* unlocked: pageqs, object (if swap_backed ||result == PEND) */
d787 1
a787 10
		 * first, we check for VM_PAGER_PEND which means that the
		 * async I/O is in progress and the async I/O done routine
		 * will clean up after us.   in this case we move on to the
		 * next page.
		 *
		 * there is a very remote chance that the pending async i/o can
		 * finish _before_ we get here.   if that happens, our page "p"
		 * may no longer be on the inactive queue.   so we verify this
		 * when determining the next page (starting over at the head if
		 * we've lost our inactive page).
d790 1
a790 18
		if (result == VM_PAGER_PEND) {
			uvmexp.paging += npages;
			uvm_lock_pageq();
			uvmexp.pdpending++;
			if (p) {
				if (p->pqflags & PQ_INACTIVE)
					nextpg = TAILQ_NEXT(p, pageq);
				else
					nextpg = TAILQ_FIRST(pglst);
			} else {
				nextpg = NULL;
			}
			continue;
		}

#ifdef UBC
		if (result == VM_PAGER_ERROR &&
		    curproc == uvm.pagedaemon_proc) {
a795 1
#endif
d798 2
a799 1
		 * clean up "p" if we have one
d802 3
d806 1
a806 82
			/*
			 * the I/O request to "p" is done and uvm_pager_put
			 * has freed any cluster pages it may have allocated
			 * during I/O.  all that is left for us to do is
			 * clean up page "p" (which is still PG_BUSY).
			 *
			 * our result could be one of the following:
			 *   VM_PAGER_OK: successful pageout
			 *
			 *   VM_PAGER_AGAIN: tmp resource shortage, we skip
			 *     to next page
			 *   VM_PAGER_{FAIL,ERROR,BAD}: an error.   we
			 *     "reactivate" page to get it out of the way (it
			 *     will eventually drift back into the inactive
			 *     queue for a retry).
			 *   VM_PAGER_UNLOCK: should never see this as it is
			 *     only valid for "get" operations
			 */

			/* relock p's object: page queues not lock yet, so
			 * no need for "try" */

			/* !swap_backed case: already locked... */
			if (swap_backed) {
				if (anon)
					simple_lock(&anon->an_lock);
				else
					simple_lock(&uobj->vmobjlock);
			}

#ifdef DIAGNOSTIC
			if (result == VM_PAGER_UNLOCK)
				panic("pagedaemon: pageout returned "
				    "invalid 'unlock' code");
#endif

			/* handle PG_WANTED now */
			if (p->flags & PG_WANTED)
				/* still holding object lock */
				wakeup(p);

			p->flags &= ~(PG_BUSY|PG_WANTED);
			UVM_PAGE_OWN(p, NULL);

			/* released during I/O? */
			if (p->flags & PG_RELEASED) {
				if (anon) {
					/* remove page so we can get nextpg */
					anon->u.an_page = NULL;

					simple_unlock(&anon->an_lock);
					uvm_anfree(anon);	/* kills anon */
					pmap_page_protect(p, VM_PROT_NONE);
					anon = NULL;
					uvm_lock_pageq();
					nextpg = TAILQ_NEXT(p, pageq);
					/* free released page */
					uvm_pagefree(p);

				} else {

					/*
					 * pgo_releasepg nukes the page and
					 * gets "nextpg" for us.  it returns
					 * with the page queues locked (when
					 * given nextpg ptr).
					 */

					if (!uobj->pgops->pgo_releasepg(p,
					    &nextpg))
						/* uobj died after release */
						uobj = NULL;

					/*
					 * lock page queues here so that they're
					 * always locked at the end of the loop.
					 */

					uvm_lock_pageq();
				}
			} else {	/* page was not released during I/O */
				uvm_lock_pageq();
d808 2
a809 28
				if (result != VM_PAGER_OK) {
					/* pageout was a failure... */
					if (result != VM_PAGER_AGAIN)
						uvm_pageactivate(p);
					pmap_clear_reference(p);
					/* XXXCDC: if (swap_backed) FREE p's
					 * swap block? */
				} else {
					/* pageout was a success... */
					pmap_clear_reference(p);
					pmap_clear_modify(p);
					p->flags |= PG_CLEAN;
				}
			}

			/*
			 * drop object lock (if there is an object left).   do
			 * a safety check of nextpg to make sure it is on the
			 * inactive queue (it should be since PG_BUSY pages on
			 * the inactive queue can't be re-queued [note: not
			 * true for active queue]).
			 */

			if (anon)
				simple_unlock(&anon->an_lock);
			else if (uobj)
				simple_unlock(&uobj->vmobjlock);

a810 6

			/*
			 * if p is null in this loop, make sure it stays null
			 * in the next loop.
			 */

a811 11

			/*
			 * lock page queues here just so they're always locked
			 * at the end of the loop.
			 */

			uvm_lock_pageq();
		}

		if (nextpg && (nextpg->pqflags & PQ_INACTIVE) == 0) {
			nextpg = TAILQ_FIRST(pglst);	/* reload! */
d874 1
a874 6
	if ((uvmexp.pdrevs & 1) != 0 && uvmexp.nswapdev != 0)
		got_it = uvmpd_scan_inactive(&uvm.page_inactive_swp);
	if (!got_it)
		got_it = uvmpd_scan_inactive(&uvm.page_inactive_obj);
	if (!got_it && (uvmexp.pdrevs & 1) == 0 && uvmexp.nswapdev != 0)
		(void) uvmpd_scan_inactive(&uvm.page_inactive_swp);
d962 4
a965 2
		 * deactivate this page if there's a shortage of
		 * inactive pages.
d969 1
a969 1
			pmap_page_protect(p, VM_PROT_NONE);
@


1.4.2.7
log
@Merge in -current from roughly a week ago
@
text
@d98 3
a100 3
static void		uvmpd_scan(void);
static boolean_t	uvmpd_scan_inactive(struct pglist *);
static void		uvmpd_tune(void);
@


1.4.2.8
log
@Sync the SMP branch with 3.3
@
text
@a336 1
			s = splbio();	/* b_iodone must by called at splbio */
a337 1
			splx(s);
@


1.3
log
@sync with NetBSD from 1999.05.24 (there is a reason for this date)
 Mostly cleanups, but also a few improvements to pagedaemon for better
 handling of low memory and/or low swap conditions.
@
text
@d168 1
a168 1
	uvmexp.freemin = uvmexp.npages / 20;
d170 1
a170 1
	/* between 16k and 256k */
d173 1
a173 1
	uvmexp.freemin = min(uvmexp.freemin, (256*1024) >> PAGE_SHIFT);
@


1.2
log
@add OpenBSD tags
@
text
@d1 1
a1 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_pdaemon.c,v 1.12 1998/11/04 07:06:05 chs Exp $	*/
a2 4
/*
 * XXXCDC: "ROUGH DRAFT" QUALITY UVM PRE-RELEASE FILE!
 *         >>>USE AT YOUR OWN RISK, WORK IS NOT FINISHED<<<
 */
d86 10
d302 3
a304 1
		thread_wakeup(&uvmexp.free);
d332 1
a332 1
	int swslot, oldslot;
d336 1
d355 1
d374 2
a375 1
			if (free >= uvmexp.freetarg) {
d493 7
d530 24
a553 2
			if (free + uvmexp.paging > uvmexp.freetarg)
			{
d563 24
d617 2
a618 5
					oldslot = uao_set_swslot(uobj,
					    p->offset >> PAGE_SHIFT, 0);

					if (oldslot)
						uvm_swap_free(oldslot, 1);
a649 1
				uvmexp.pgswapout++;
d941 1
a941 1
	int s, free, pages_freed, page_shortage;
a986 1
	pages_freed = uvmexp.pdfreed;	/* so far... */
d994 1
d1001 1
d1008 19
a1026 9
	page_shortage = uvmexp.inactarg - uvmexp.inactive;
	pages_freed = uvmexp.pdfreed - pages_freed; /* # pages freed in loop */
	if (page_shortage <= 0 && pages_freed == 0)
		page_shortage = 1;

	UVMHIST_LOG(pdhist, "  second loop: page_shortage=%d", page_shortage,
	    0, 0, 0);
	for (p = uvm.page_active.tqh_first ; 
	    p != NULL && page_shortage > 0 ; p = nextpg) {
d1032 1
a1032 1
		 * lock owner
a1041 1

a1046 1

a1051 1

a1054 1

a1055 1

a1057 1

d1059 38
a1096 2

		if ((p->flags & PG_BUSY) == 0) {
d1101 1
a1101 1
			page_shortage--;
a1108 4

	/*
	 * done scan
	 */
@


1.1
log
@Import of uvm from NetBSD. Some local changes, some code disabled
@
text
@d1 1
@

