head	1.92;
access;
symbols
	OPENBSD_6_2:1.92.0.4
	OPENBSD_6_2_BASE:1.92
	OPENBSD_6_1:1.91.0.4
	OPENBSD_6_1_BASE:1.91
	OPENBSD_6_0:1.90.0.2
	OPENBSD_6_0_BASE:1.90
	OPENBSD_5_9:1.87.0.2
	OPENBSD_5_9_BASE:1.87
	OPENBSD_5_8:1.84.0.4
	OPENBSD_5_8_BASE:1.84
	OPENBSD_5_7:1.83.0.2
	OPENBSD_5_7_BASE:1.83
	OPENBSD_5_6:1.76.0.4
	OPENBSD_5_6_BASE:1.76
	OPENBSD_5_5:1.69.0.6
	OPENBSD_5_5_BASE:1.69
	OPENBSD_5_4:1.69.0.2
	OPENBSD_5_4_BASE:1.69
	OPENBSD_5_3:1.65.0.4
	OPENBSD_5_3_BASE:1.65
	OPENBSD_5_2:1.65.0.2
	OPENBSD_5_2_BASE:1.65
	OPENBSD_5_1_BASE:1.62
	OPENBSD_5_1:1.62.0.4
	OPENBSD_5_0:1.62.0.2
	OPENBSD_5_0_BASE:1.62
	OPENBSD_4_9:1.58.0.6
	OPENBSD_4_9_BASE:1.58
	OPENBSD_4_8:1.58.0.4
	OPENBSD_4_8_BASE:1.58
	OPENBSD_4_7:1.58.0.2
	OPENBSD_4_7_BASE:1.58
	OPENBSD_4_6:1.57.0.4
	OPENBSD_4_6_BASE:1.57
	OPENBSD_4_5:1.50.0.2
	OPENBSD_4_5_BASE:1.50
	OPENBSD_4_4:1.49.0.6
	OPENBSD_4_4_BASE:1.49
	OPENBSD_4_3:1.49.0.4
	OPENBSD_4_3_BASE:1.49
	OPENBSD_4_2:1.49.0.2
	OPENBSD_4_2_BASE:1.49
	OPENBSD_4_1:1.41.0.4
	OPENBSD_4_1_BASE:1.41
	OPENBSD_4_0:1.41.0.2
	OPENBSD_4_0_BASE:1.41
	OPENBSD_3_9:1.36.0.2
	OPENBSD_3_9_BASE:1.36
	OPENBSD_3_8:1.34.0.2
	OPENBSD_3_8_BASE:1.34
	OPENBSD_3_7:1.33.0.4
	OPENBSD_3_7_BASE:1.33
	OPENBSD_3_6:1.33.0.2
	OPENBSD_3_6_BASE:1.33
	SMP_SYNC_A:1.32
	SMP_SYNC_B:1.32
	OPENBSD_3_5:1.32.0.2
	OPENBSD_3_5_BASE:1.32
	OPENBSD_3_4:1.31.0.8
	OPENBSD_3_4_BASE:1.31
	UBC_SYNC_A:1.31
	OPENBSD_3_3:1.31.0.6
	OPENBSD_3_3_BASE:1.31
	OPENBSD_3_2:1.31.0.4
	OPENBSD_3_2_BASE:1.31
	OPENBSD_3_1:1.31.0.2
	OPENBSD_3_1_BASE:1.31
	UBC_SYNC_B:1.31
	UBC:1.28.0.2
	UBC_BASE:1.28
	OPENBSD_3_0:1.19.0.2
	OPENBSD_3_0_BASE:1.19
	OPENBSD_2_9_BASE:1.10
	OPENBSD_2_9:1.10.0.2
	OPENBSD_2_8:1.5.0.4
	OPENBSD_2_8_BASE:1.5
	OPENBSD_2_7:1.5.0.2
	OPENBSD_2_7_BASE:1.5
	SMP:1.4.0.4
	SMP_BASE:1.4
	kame_19991208:1.4
	OPENBSD_2_6:1.4.0.2
	OPENBSD_2_6_BASE:1.4
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.92
date	2017.07.20.18.22.25;	author bluhm;	state Exp;
branches;
next	1.91;
commitid	eXLHJ8oEZQtPpKri;

1.91
date	2016.09.16.01.09.53;	author dlg;	state Exp;
branches;
next	1.90;
commitid	S1LT7BcQMYzBQOe8;

1.90
date	2016.05.08.11.52.32;	author stefan;	state Exp;
branches;
next	1.89;
commitid	hUj20vPhiD6DQNDL;

1.89
date	2016.03.29.12.04.26;	author chl;	state Exp;
branches;
next	1.88;
commitid	yx3qunfiuB9a5ukd;

1.88
date	2016.03.07.18.44.00;	author naddy;	state Exp;
branches;
next	1.87;
commitid	Z6e4eqr6FuYFPnlL;

1.87
date	2015.11.10.08.21.28;	author mlarkin;	state Exp;
branches;
next	1.86;
commitid	HLPQzqC25IJVc8C6;

1.86
date	2015.09.09.14.52.12;	author miod;	state Exp;
branches;
next	1.85;
commitid	Wbcnl6am8WQAyF85;

1.85
date	2015.08.21.16.04.35;	author visa;	state Exp;
branches;
next	1.84;
commitid	gglpDr80UKmkkP9A;

1.84
date	2015.03.14.03.38.53;	author jsg;	state Exp;
branches;
next	1.83;
commitid	p4LJxGKbi0BU2cG6;

1.83
date	2015.02.08.02.17.08;	author deraadt;	state Exp;
branches;
next	1.82;
commitid	EHHLDpKiLok9N5jM;

1.82
date	2015.02.06.10.58.35;	author deraadt;	state Exp;
branches;
next	1.81;
commitid	QOLWofatH4nZnDlg;

1.81
date	2014.12.17.06.58.11;	author guenther;	state Exp;
branches;
next	1.80;
commitid	DImukoCWyTxwdbuh;

1.80
date	2014.12.15.02.24.23;	author guenther;	state Exp;
branches;
next	1.79;
commitid	ZxaujiOM0aYQRjFY;

1.79
date	2014.11.16.12.31.00;	author deraadt;	state Exp;
branches;
next	1.78;
commitid	yv0ECmCdICvq576h;

1.78
date	2014.10.03.17.41.00;	author kettenis;	state Exp;
branches;
next	1.77;
commitid	h8HwsnqXqpePzqXu;

1.77
date	2014.09.07.08.17.44;	author guenther;	state Exp;
branches;
next	1.76;
commitid	IHsgrPZ9xfvFv7Qn;

1.76
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.75;
commitid	7NtJNW9udCOFtDNM;

1.75
date	2014.07.08.11.38.48;	author deraadt;	state Exp;
branches;
next	1.74;
commitid	myoIhq56iLBOzmdA;

1.74
date	2014.07.03.11.38.46;	author kettenis;	state Exp;
branches;
next	1.73;
commitid	jjIkzZBsrJ6Rab4f;

1.73
date	2014.05.08.20.08.50;	author kettenis;	state Exp;
branches;
next	1.72;

1.72
date	2014.04.13.23.14.15;	author tedu;	state Exp;
branches;
next	1.71;

1.71
date	2014.04.03.20.21.01;	author miod;	state Exp;
branches;
next	1.70;

1.70
date	2014.03.31.20.16.39;	author miod;	state Exp;
branches;
next	1.69;

1.69
date	2013.05.30.18.02.04;	author tedu;	state Exp;
branches;
next	1.68;

1.68
date	2013.05.30.16.39.26;	author tedu;	state Exp;
branches;
next	1.67;

1.67
date	2013.05.30.16.29.46;	author tedu;	state Exp;
branches;
next	1.66;

1.66
date	2013.05.30.15.17.59;	author tedu;	state Exp;
branches;
next	1.65;

1.65
date	2012.04.12.11.55.43;	author ariane;	state Exp;
branches;
next	1.64;

1.64
date	2012.03.23.15.51.26;	author guenther;	state Exp;
branches;
next	1.63;

1.63
date	2012.03.09.13.01.29;	author ariane;	state Exp;
branches;
next	1.62;

1.62
date	2011.07.03.18.34.14;	author oga;	state Exp;
branches;
next	1.61;

1.61
date	2011.06.23.21.52.42;	author oga;	state Exp;
branches;
next	1.60;

1.60
date	2011.06.06.17.10.23;	author ariane;	state Exp;
branches;
next	1.59;

1.59
date	2011.05.24.15.27.36;	author ariane;	state Exp;
branches;
next	1.58;

1.58
date	2009.07.22.21.05.37;	author oga;	state Exp;
branches;
next	1.57;

1.57
date	2009.06.16.23.54.58;	author oga;	state Exp;
branches;
next	1.56;

1.56
date	2009.06.16.00.11.29;	author oga;	state Exp;
branches;
next	1.55;

1.55
date	2009.06.06.17.46.44;	author art;	state Exp;
branches;
next	1.54;

1.54
date	2009.06.01.19.54.02;	author oga;	state Exp;
branches;
next	1.53;

1.53
date	2009.05.08.13.50.15;	author ariane;	state Exp;
branches;
next	1.52;

1.52
date	2009.03.25.20.00.18;	author oga;	state Exp;
branches;
next	1.51;

1.51
date	2009.03.20.15.19.04;	author oga;	state Exp;
branches;
next	1.50;

1.50
date	2008.09.16.18.52.52;	author chl;	state Exp;
branches;
next	1.49;

1.49
date	2007.06.18.21.51.15;	author pedro;	state Exp;
branches;
next	1.48;

1.48
date	2007.05.31.21.20.30;	author thib;	state Exp;
branches;
next	1.47;

1.47
date	2007.04.15.11.15.08;	author art;	state Exp;
branches;
next	1.46;

1.46
date	2007.04.13.18.57.49;	author art;	state Exp;
branches;
next	1.45;

1.45
date	2007.04.11.12.10.42;	author art;	state Exp;
branches;
next	1.44;

1.44
date	2007.04.04.17.44.45;	author art;	state Exp;
branches;
next	1.43;

1.43
date	2007.03.26.08.43.34;	author art;	state Exp;
branches;
next	1.42;

1.42
date	2007.03.25.11.31.07;	author art;	state Exp;
branches;
next	1.41;

1.41
date	2006.07.31.11.51.29;	author mickey;	state Exp;
branches;
next	1.40;

1.40
date	2006.07.26.23.15.55;	author mickey;	state Exp;
branches;
next	1.39;

1.39
date	2006.07.13.22.51.26;	author deraadt;	state Exp;
branches;
next	1.38;

1.38
date	2006.06.21.16.20.05;	author mickey;	state Exp;
branches;
next	1.37;

1.37
date	2006.03.06.14.27.29;	author mickey;	state Exp;
branches;
next	1.36;

1.36
date	2005.11.29.05.37.14;	author tedu;	state Exp;
branches;
next	1.35;

1.35
date	2005.10.23.01.42.22;	author pedro;	state Exp;
branches;
next	1.34;

1.34
date	2005.05.03.11.52.35;	author mickey;	state Exp;
branches;
next	1.33;

1.33
date	2004.08.03.12.10.48;	author todd;	state Exp;
branches;
next	1.32;

1.32
date	2004.02.23.06.19.32;	author drahn;	state Exp;
branches;
next	1.31;

1.31
date	2002.03.14.01.27.18;	author millert;	state Exp;
branches;
next	1.30;

1.30
date	2002.01.02.22.23.25;	author miod;	state Exp;
branches;
next	1.29;

1.29
date	2001.12.19.08.58.07;	author art;	state Exp;
branches;
next	1.28;

1.28
date	2001.12.04.23.22.42;	author art;	state Exp;
branches
	1.28.2.1;
next	1.27;

1.27
date	2001.11.28.19.28.14;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2001.11.28.13.47.39;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2001.11.27.05.27.12;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2001.11.12.01.26.09;	author art;	state Exp;
branches;
next	1.23;

1.23
date	2001.11.09.04.34.27;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2001.11.06.13.36.52;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2001.11.06.01.35.04;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2001.11.05.22.14.54;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2001.09.19.20.50.59;	author mickey;	state Exp;
branches;
next	1.18;

1.18
date	2001.08.11.10.57.22;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2001.07.26.19.37.13;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2001.07.25.13.25.33;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2001.07.18.10.47.05;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2001.06.23.19.24.33;	author smart;	state Exp;
branches;
next	1.13;

1.13
date	2001.06.08.08.09.38;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2001.05.09.15.31.23;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2001.05.07.16.08.40;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2001.03.22.23.36.52;	author niklas;	state Exp;
branches;
next	1.9;

1.9
date	2001.03.22.03.05.55;	author smart;	state Exp;
branches;
next	1.8;

1.8
date	2001.03.09.14.20.51;	author art;	state Exp;
branches;
next	1.7;

1.7
date	2001.03.08.15.21.36;	author smart;	state Exp;
branches;
next	1.6;

1.6
date	2001.01.29.02.07.44;	author niklas;	state Exp;
branches;
next	1.5;

1.5
date	2000.03.16.22.11.04;	author art;	state Exp;
branches;
next	1.4;

1.4
date	99.09.03.18.02.21;	author art;	state Exp;
branches
	1.4.4.1;
next	1.3;

1.3
date	99.08.23.08.13.23;	author art;	state Exp;
branches;
next	1.2;

1.2
date	99.02.26.05.32.06;	author art;	state Exp;
branches;
next	1.1;

1.1
date	99.02.26.01.30.12;	author art;	state Exp;
branches;
next	;

1.4.4.1
date	2000.03.24.09.09.49;	author niklas;	state Exp;
branches;
next	1.4.4.2;

1.4.4.2
date	2001.05.14.22.47.45;	author niklas;	state Exp;
branches;
next	1.4.4.3;

1.4.4.3
date	2001.07.04.11.01.02;	author niklas;	state Exp;
branches;
next	1.4.4.4;

1.4.4.4
date	2001.10.31.03.32.14;	author nate;	state Exp;
branches;
next	1.4.4.5;

1.4.4.5
date	2001.11.13.23.02.31;	author niklas;	state Exp;
branches;
next	1.4.4.6;

1.4.4.6
date	2001.12.05.01.23.58;	author niklas;	state Exp;
branches;
next	1.4.4.7;

1.4.4.7
date	2002.03.06.02.17.14;	author niklas;	state Exp;
branches;
next	1.4.4.8;

1.4.4.8
date	2002.03.28.14.54.26;	author niklas;	state Exp;
branches;
next	1.4.4.9;

1.4.4.9
date	2004.06.05.23.13.12;	author niklas;	state Exp;
branches;
next	;

1.28.2.1
date	2002.01.31.22.55.51;	author niklas;	state Exp;
branches;
next	1.28.2.2;

1.28.2.2
date	2002.02.02.03.28.26;	author art;	state Exp;
branches;
next	1.28.2.3;

1.28.2.3
date	2002.06.11.03.33.03;	author art;	state Exp;
branches;
next	1.28.2.4;

1.28.2.4
date	2002.11.04.18.02.32;	author art;	state Exp;
branches;
next	;


desc
@@


1.92
log
@Accessing a mmap(2)ed file behind its end should result in a SIGBUS
according to POSIX.  Bring regression test and kernel in line for
amd64 and i386.  Other architectures have to follow.
OK deraadt@@ kettenis@@
@
text
@/*	$OpenBSD: uvm_fault.c,v 1.91 2016/09/16 01:09:53 dlg Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.51 2000/08/06 00:22:53 thorpej Exp $	*/

/*
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 * from: Id: uvm_fault.c,v 1.1.2.23 1998/02/06 05:29:05 chs Exp
 */

/*
 * uvm_fault.c: fault handler
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/kernel.h>
#include <sys/proc.h>
#include <sys/malloc.h>
#include <sys/mman.h>

#include <uvm/uvm.h>

/*
 *
 * a word on page faults:
 *
 * types of page faults we handle:
 *
 * CASE 1: upper layer faults                   CASE 2: lower layer faults
 *
 *    CASE 1A         CASE 1B                  CASE 2A        CASE 2B
 *    read/write1     write>1                  read/write   +-cow_write/zero
 *         |             |                         |        |        
 *      +--|--+       +--|--+     +-----+       +  |  +     | +-----+
 * amap |  V  |       |  ----------->new|          |        | |  ^  |
 *      +-----+       +-----+     +-----+       +  |  +     | +--|--+
 *                                                 |        |    |
 *      +-----+       +-----+                   +--|--+     | +--|--+
 * uobj | d/c |       | d/c |                   |  V  |     +----|  |
 *      +-----+       +-----+                   +-----+       +-----+
 *
 * d/c = don't care
 * 
 *   case [0]: layerless fault
 *	no amap or uobj is present.   this is an error.
 *
 *   case [1]: upper layer fault [anon active]
 *     1A: [read] or [write with anon->an_ref == 1]
 *		I/O takes place in top level anon and uobj is not touched.
 *     1B: [write with anon->an_ref > 1]
 *		new anon is alloc'd and data is copied off ["COW"]
 *
 *   case [2]: lower layer fault [uobj]
 *     2A: [read on non-NULL uobj] or [write to non-copy_on_write area]
 *		I/O takes place directly in object.
 *     2B: [write to copy_on_write] or [read on NULL uobj]
 *		data is "promoted" from uobj to a new anon.   
 *		if uobj is null, then we zero fill.
 *
 * we follow the standard UVM locking protocol ordering:
 *
 * MAPS => AMAP => UOBJ => ANON => PAGE QUEUES (PQ) 
 * we hold a PG_BUSY page if we unlock for I/O
 *
 *
 * the code is structured as follows:
 *  
 *     - init the "IN" params in the ufi structure
 *   ReFault:
 *     - do lookups [locks maps], check protection, handle needs_copy
 *     - check for case 0 fault (error)
 *     - establish "range" of fault
 *     - if we have an amap lock it and extract the anons
 *     - if sequential advice deactivate pages behind us
 *     - at the same time check pmap for unmapped areas and anon for pages
 *	 that we could map in (and do map it if found)
 *     - check object for resident pages that we could map in
 *     - if (case 2) goto Case2
 *     - >>> handle case 1
 *           - ensure source anon is resident in RAM
 *           - if case 1B alloc new anon and copy from source
 *           - map the correct page in
 *   Case2:
 *     - >>> handle case 2
 *           - ensure source page is resident (if uobj)
 *           - if case 2B alloc new anon and copy from source (could be zero
 *		fill if uobj == NULL)
 *           - map the correct page in
 *     - done!
 *
 * note on paging:
 *   if we have to do I/O we place a PG_BUSY page in the correct object,
 * unlock everything, and do the I/O.   when I/O is done we must reverify
 * the state of the world before assuming that our data structures are
 * valid.   [because mappings could change while the map is unlocked]
 *
 *  alternative 1: unbusy the page in question and restart the page fault
 *    from the top (ReFault).   this is easy but does not take advantage
 *    of the information that we already have from our previous lookup, 
 *    although it is possible that the "hints" in the vm_map will help here.
 *
 * alternative 2: the system already keeps track of a "version" number of
 *    a map.   [i.e. every time you write-lock a map (e.g. to change a
 *    mapping) you bump the version number up by one...]   so, we can save
 *    the version number of the map before we release the lock and start I/O.
 *    then when I/O is done we can relock and check the version numbers
 *    to see if anything changed.    this might save us some over 1 because
 *    we don't have to unbusy the page and may be less compares(?).
 *
 * alternative 3: put in backpointers or a way to "hold" part of a map
 *    in place while I/O is in progress.   this could be complex to
 *    implement (especially with structures like amap that can be referenced
 *    by multiple map entries, and figuring out what should wait could be
 *    complex as well...).
 *
 * given that we are not currently multiprocessor or multithreaded we might
 * as well choose alternative 2 now.   maybe alternative 3 would be useful
 * in the future.    XXX keep in mind for future consideration//rechecking.
 */

/*
 * local data structures
 */
struct uvm_advice {
	int nback;
	int nforw;
};

/*
 * page range array: set up in uvmfault_init().
 */
static struct uvm_advice uvmadvice[MADV_MASK + 1];

#define UVM_MAXRANGE 16	/* must be max() of nback+nforw+1 */

/*
 * private prototypes
 */
static void uvmfault_amapcopy(struct uvm_faultinfo *);
static __inline void uvmfault_anonflush(struct vm_anon **, int);
void	uvmfault_unlockmaps(struct uvm_faultinfo *, boolean_t);
void	uvmfault_update_stats(struct uvm_faultinfo *);

/*
 * inline functions
 */
/*
 * uvmfault_anonflush: try and deactivate pages in specified anons
 *
 * => does not have to deactivate page if it is busy
 */
static __inline void
uvmfault_anonflush(struct vm_anon **anons, int n)
{
	int lcv;
	struct vm_page *pg;
	
	for (lcv = 0 ; lcv < n ; lcv++) {
		if (anons[lcv] == NULL)
			continue;
		pg = anons[lcv]->an_page;
		if (pg && (pg->pg_flags & PG_BUSY) == 0) {
			uvm_lock_pageq();
			if (pg->wire_count == 0) {
				pmap_page_protect(pg, PROT_NONE);
				uvm_pagedeactivate(pg);
			}
			uvm_unlock_pageq();
		}
	}
}

/*
 * normal functions
 */
/*
 * uvmfault_init: compute proper values for the uvmadvice[] array.
 */
void
uvmfault_init(void)
{
	int npages;

	npages = atop(16384);
	if (npages > 0) {
		KASSERT(npages <= UVM_MAXRANGE / 2);
		uvmadvice[MADV_NORMAL].nforw = npages;
		uvmadvice[MADV_NORMAL].nback = npages - 1;
	}

	npages = atop(32768);
	if (npages > 0) {
		KASSERT(npages <= UVM_MAXRANGE / 2);
		uvmadvice[MADV_SEQUENTIAL].nforw = npages - 1;
		uvmadvice[MADV_SEQUENTIAL].nback = npages;
	}
}

/*
 * uvmfault_amapcopy: clear "needs_copy" in a map.
 *
 * => if we are out of RAM we sleep (waiting for more)
 */
static void
uvmfault_amapcopy(struct uvm_faultinfo *ufi)
{

	/* while we haven't done the job */
	while (1) {
		/* no mapping?  give up. */
		if (uvmfault_lookup(ufi, TRUE) == FALSE)
			return;

		/* copy if needed. */
		if (UVM_ET_ISNEEDSCOPY(ufi->entry))
			amap_copy(ufi->map, ufi->entry, M_NOWAIT, TRUE, 
				ufi->orig_rvaddr, ufi->orig_rvaddr + 1);

		/* didn't work?  must be out of RAM.  sleep. */
		if (UVM_ET_ISNEEDSCOPY(ufi->entry)) {
			uvmfault_unlockmaps(ufi, TRUE);
			uvm_wait("fltamapcopy");
			continue;
		}

		/* got it! */
		uvmfault_unlockmaps(ufi, TRUE);
		return;
	}
	/*NOTREACHED*/
}

/*
 * uvmfault_anonget: get data in an anon into a non-busy, non-released
 * page in that anon.
 *
 * => we don't move the page on the queues [gets moved later]
 * => if we allocate a new page [we_own], it gets put on the queues.
 *    either way, the result is that the page is on the queues at return time
 */
int
uvmfault_anonget(struct uvm_faultinfo *ufi, struct vm_amap *amap,
    struct vm_anon *anon)
{
	boolean_t we_own;	/* we own anon's page? */
	boolean_t locked;	/* did we relock? */
	struct vm_page *pg;
	int result;

	result = 0;		/* XXX shut up gcc */
	uvmexp.fltanget++;
        /* bump rusage counters */
	if (anon->an_page)
		curproc->p_ru.ru_minflt++;
	else
		curproc->p_ru.ru_majflt++;

	/* loop until we get it, or fail. */
	while (1) {
		we_own = FALSE;		/* TRUE if we set PG_BUSY on a page */
		pg = anon->an_page;

		/* page there?   make sure it is not busy/released. */
		if (pg) {
			KASSERT(pg->pg_flags & PQ_ANON);
			KASSERT(pg->uanon == anon);
			
			/*
			 * if the page is busy, we drop all the locks and
			 * try again.
			 */
			if ((pg->pg_flags & (PG_BUSY|PG_RELEASED)) == 0)
				return (VM_PAGER_OK);
			atomic_setbits_int(&pg->pg_flags, PG_WANTED);
			uvmexp.fltpgwait++;

			/*
			 * the last unlock must be an atomic unlock+wait on
			 * the owner of page
			 */
			uvmfault_unlockall(ufi, amap, NULL, NULL);
			UVM_WAIT(pg, 0, "anonget2", 0);
			/* ready to relock and try again */
		} else {
			/* no page, we must try and bring it in. */
			pg = uvm_pagealloc(NULL, 0, anon, 0);

			if (pg == NULL) {		/* out of RAM.  */
				uvmfault_unlockall(ufi, amap, NULL, anon);
				uvmexp.fltnoram++;
				uvm_wait("flt_noram1");
				/* ready to relock and try again */
			} else {
				/* we set the PG_BUSY bit */
				we_own = TRUE;	
				uvmfault_unlockall(ufi, amap, NULL, anon);

				/*
				 * we are passing a PG_BUSY+PG_FAKE+PG_CLEAN
				 * page into the uvm_swap_get function with
				 * all data structures unlocked.  note that
				 * it is ok to read an_swslot here because
				 * we hold PG_BUSY on the page.
				 */
				uvmexp.pageins++;
				result = uvm_swap_get(pg, anon->an_swslot,
				    PGO_SYNCIO);

				/*
				 * we clean up after the i/o below in the
				 * "we_own" case
				 */
				/* ready to relock and try again */
			}
		}

		/* now relock and try again */
		locked = uvmfault_relock(ufi);

		/*
		 * if we own the page (i.e. we set PG_BUSY), then we need
		 * to clean up after the I/O. there are three cases to
		 * consider:
		 *   [1] page released during I/O: free anon and ReFault.
		 *   [2] I/O not OK.   free the page and cause the fault 
		 *       to fail.
		 *   [3] I/O OK!   activate the page and sync with the
		 *       non-we_own case (i.e. drop anon lock if not locked).
		 */
		if (we_own) {
			if (pg->pg_flags & PG_WANTED) {
				wakeup(pg);	
			}
			/* un-busy! */
			atomic_clearbits_int(&pg->pg_flags,
			    PG_WANTED|PG_BUSY|PG_FAKE);
			UVM_PAGE_OWN(pg, NULL);

			/* 
			 * if we were RELEASED during I/O, then our anon is
			 * no longer part of an amap.   we need to free the
			 * anon and try again.
			 */
			if (pg->pg_flags & PG_RELEASED) {
				pmap_page_protect(pg, PROT_NONE);
				uvm_anfree(anon);	/* frees page for us */
				if (locked)
					uvmfault_unlockall(ufi, amap, NULL,
							   NULL);
				uvmexp.fltpgrele++;
				return (VM_PAGER_REFAULT);	/* refault! */
			}

			if (result != VM_PAGER_OK) {
				KASSERT(result != VM_PAGER_PEND);

				/* remove page from anon */
				anon->an_page = NULL;

				/*
				 * remove the swap slot from the anon
				 * and mark the anon as having no real slot.
				 * don't free the swap slot, thus preventing
				 * it from being used again.
				 */
				uvm_swap_markbad(anon->an_swslot, 1);
				anon->an_swslot = SWSLOT_BAD;

				/*
				 * note: page was never !PG_BUSY, so it
				 * can't be mapped and thus no need to
				 * pmap_page_protect it...
				 */
				uvm_lock_pageq();
				uvm_pagefree(pg);
				uvm_unlock_pageq();

				if (locked)
					uvmfault_unlockall(ufi, amap, NULL,
					    anon);
				return (VM_PAGER_ERROR);
			}
			
			/*
			 * must be OK, clear modify (already PG_CLEAN)
			 * and activate
			 */
			pmap_clear_modify(pg);
			uvm_lock_pageq();
			uvm_pageactivate(pg);
			uvm_unlock_pageq();
		}

		/* we were not able to relock.   restart fault. */
		if (!locked)
			return (VM_PAGER_REFAULT);

		/* verify no one touched the amap and moved the anon on us. */
		if (ufi != NULL &&
		    amap_lookup(&ufi->entry->aref, 
				ufi->orig_rvaddr - ufi->entry->start) != anon) {
			
			uvmfault_unlockall(ufi, amap, NULL, anon);
			return (VM_PAGER_REFAULT);
		}

		/* try it again! */
		uvmexp.fltanretry++;
		continue;

	} /* while (1) */
	/*NOTREACHED*/
}

/*
 * Update statistics after fault resolution.
 * - maxrss
 */
void
uvmfault_update_stats(struct uvm_faultinfo *ufi)
{
	struct vm_map		*map;
	struct proc		*p;
	vsize_t			 res;

	map = ufi->orig_map;

	/*
	 * If this is a nested pmap (eg, a virtual machine pmap managed
	 * by vmm(4) on amd64/i386), don't do any updating, just return.
	 *
	 * pmap_nested() on other archs is #defined to 0, so this is a
	 * no-op.
	 */
	if (pmap_nested(map->pmap))
		return;

	/* Update the maxrss for the process. */
	if (map->flags & VM_MAP_ISVMSPACE) {
		p = curproc;
		KASSERT(p != NULL && &p->p_vmspace->vm_map == map);

		res = pmap_resident_count(map->pmap);
		/* Convert res from pages to kilobytes. */
		res <<= (PAGE_SHIFT - 10);

		if (p->p_ru.ru_maxrss < res)
			p->p_ru.ru_maxrss = res;
	}
}

/*
 *   F A U L T   -   m a i n   e n t r y   p o i n t
 */

/*
 * uvm_fault: page fault handler
 *
 * => called from MD code to resolve a page fault
 * => VM data structures usually should be unlocked.   however, it is 
 *	possible to call here with the main map locked if the caller
 *	gets a write lock, sets it recursive, and then calls us (c.f.
 *	uvm_map_pageable).   this should be avoided because it keeps
 *	the map locked off during I/O.
 */
#define MASK(entry)     (UVM_ET_ISCOPYONWRITE(entry) ? \
			 ~PROT_WRITE : PROT_MASK)
int
uvm_fault(vm_map_t orig_map, vaddr_t vaddr, vm_fault_t fault_type,
    vm_prot_t access_type)
{
	struct uvm_faultinfo ufi;
	vm_prot_t enter_prot;
	boolean_t wired, narrow, promote, locked, shadowed;
	int npages, nback, nforw, centeridx, result, lcv, gotpages, ret;
	vaddr_t startva, currva;
	voff_t uoff;
	paddr_t pa; 
	struct vm_amap *amap;
	struct uvm_object *uobj;
	struct vm_anon *anons_store[UVM_MAXRANGE], **anons, *anon, *oanon;
	struct vm_page *pages[UVM_MAXRANGE], *pg, *uobjpage;

	anon = NULL;
	pg = NULL;

	uvmexp.faults++;	/* XXX: locking? */

	/* init the IN parameters in the ufi */
	ufi.orig_map = orig_map;
	ufi.orig_rvaddr = trunc_page(vaddr);
	ufi.orig_size = PAGE_SIZE;	/* can't get any smaller than this */
	if (fault_type == VM_FAULT_WIRE)
		narrow = TRUE;		/* don't look for neighborhood
					 * pages on wire */
	else
		narrow = FALSE;		/* normal fault */

	/* "goto ReFault" means restart the page fault from ground zero. */
ReFault:
	/* lookup and lock the maps */
	if (uvmfault_lookup(&ufi, FALSE) == FALSE) {
		return (EFAULT);
	}

#ifdef DIAGNOSTIC
	if ((ufi.map->flags & VM_MAP_PAGEABLE) == 0)
		panic("uvm_fault: fault on non-pageable map (%p, 0x%lx)",
		    ufi.map, vaddr);
#endif

	/* check protection */
	if ((ufi.entry->protection & access_type) != access_type) {
		uvmfault_unlockmaps(&ufi, FALSE);
		return (EACCES);
	}

	/*
	 * "enter_prot" is the protection we want to enter the page in at.
	 * for certain pages (e.g. copy-on-write pages) this protection can
	 * be more strict than ufi.entry->protection.  "wired" means either
	 * the entry is wired or we are fault-wiring the pg.
	 */

	enter_prot = ufi.entry->protection;
	wired = VM_MAPENT_ISWIRED(ufi.entry) || (fault_type == VM_FAULT_WIRE);
	if (wired)
		access_type = enter_prot; /* full access for wired */

	/* handle "needs_copy" case. */
	if (UVM_ET_ISNEEDSCOPY(ufi.entry)) {
		if ((access_type & PROT_WRITE) ||
		    (ufi.entry->object.uvm_obj == NULL)) {
			/* need to clear */
			uvmfault_unlockmaps(&ufi, FALSE);
			uvmfault_amapcopy(&ufi);
			uvmexp.fltamcopy++;
			goto ReFault;
		} else {
			/*
			 * ensure that we pmap_enter page R/O since
			 * needs_copy is still true
			 */
			enter_prot &= ~PROT_WRITE; 
		}
	}

	/* identify the players */
	amap = ufi.entry->aref.ar_amap;		/* top layer */
	uobj = ufi.entry->object.uvm_obj;	/* bottom layer */

	/*
	 * check for a case 0 fault.  if nothing backing the entry then
	 * error now.
	 */
	if (amap == NULL && uobj == NULL) {
		uvmfault_unlockmaps(&ufi, FALSE);
		return (EFAULT);
	}

	/*
	 * establish range of interest based on advice from mapper
	 * and then clip to fit map entry.   note that we only want
	 * to do this the first time through the fault.   if we 
	 * ReFault we will disable this by setting "narrow" to true.
	 */
	if (narrow == FALSE) {

		/* wide fault (!narrow) */
		nback = min(uvmadvice[ufi.entry->advice].nback,
			    (ufi.orig_rvaddr - ufi.entry->start) >> PAGE_SHIFT);
		startva = ufi.orig_rvaddr - ((vsize_t)nback << PAGE_SHIFT);
		nforw = min(uvmadvice[ufi.entry->advice].nforw,
			    ((ufi.entry->end - ufi.orig_rvaddr) >>
			     PAGE_SHIFT) - 1);
		/*
		 * note: "-1" because we don't want to count the
		 * faulting page as forw
		 */
		npages = nback + nforw + 1;
		centeridx = nback;

		narrow = TRUE;	/* ensure only once per-fault */
	} else {
		/* narrow fault! */
		nback = nforw = 0;
		startva = ufi.orig_rvaddr;
		npages = 1;
		centeridx = 0;
	}

	/* if we've got an amap, extract current anons. */
	if (amap) {
		anons = anons_store;
		amap_lookups(&ufi.entry->aref, startva - ufi.entry->start,
		    anons, npages);
	} else {
		anons = NULL;	/* to be safe */
	}

	/*
	 * for MADV_SEQUENTIAL mappings we want to deactivate the back pages
	 * now and then forget about them (for the rest of the fault).
	 */
	if (ufi.entry->advice == MADV_SEQUENTIAL && nback != 0) {
		/* flush back-page anons? */
		if (amap) 
			uvmfault_anonflush(anons, nback);

		/* flush object? */
		if (uobj) {
			uoff = (startva - ufi.entry->start) + ufi.entry->offset;
			(void) uobj->pgops->pgo_flush(uobj, uoff, uoff + 
			    ((vsize_t)nback << PAGE_SHIFT), PGO_DEACTIVATE);
		}

		/* now forget about the backpages */
		if (amap)
			anons += nback;
		startva += ((vsize_t)nback << PAGE_SHIFT);
		npages -= nback;
		centeridx = 0;
	}

	/*
	 * map in the backpages and frontpages we found in the amap in hopes
	 * of preventing future faults.    we also init the pages[] array as
	 * we go.
	 */
	currva = startva;
	shadowed = FALSE;
	for (lcv = 0 ; lcv < npages ; lcv++, currva += PAGE_SIZE) {
		/*
		 * dont play with VAs that are already mapped
		 * except for center)
		 */
		if (lcv != centeridx &&
		    pmap_extract(ufi.orig_map->pmap, currva, &pa)) {
			pages[lcv] = PGO_DONTCARE;
			continue;
		}

		/* unmapped or center page.   check if any anon at this level. */
		if (amap == NULL || anons[lcv] == NULL) {
			pages[lcv] = NULL;
			continue;
		}

		/* check for present page and map if possible.   re-activate it. */
		pages[lcv] = PGO_DONTCARE;
		if (lcv == centeridx) {		/* save center for later! */
			shadowed = TRUE;
			continue;
		}
		anon = anons[lcv];
		if (anon->an_page &&
		    (anon->an_page->pg_flags & (PG_RELEASED|PG_BUSY)) == 0) {
			uvm_lock_pageq();
			uvm_pageactivate(anon->an_page);	/* reactivate */
			uvm_unlock_pageq();
			uvmexp.fltnamap++;

			/*
			 * Since this isn't the page that's actually faulting,
			 * ignore pmap_enter() failures; it's not critical
			 * that we enter these right now.
			 */
			(void) pmap_enter(ufi.orig_map->pmap, currva,
			    VM_PAGE_TO_PHYS(anon->an_page),
			    (anon->an_ref > 1) ? (enter_prot & ~PROT_WRITE) :
			    enter_prot,
			    PMAP_CANFAIL |
			     (VM_MAPENT_ISWIRED(ufi.entry) ? PMAP_WIRED : 0));
		}
	}
	if (npages > 1)
		pmap_update(ufi.orig_map->pmap);

	/* (shadowed == TRUE) if there is an anon at the faulting address */
	/*
	 * note that if we are really short of RAM we could sleep in the above
	 * call to pmap_enter.   bad?
	 *
	 * XXX Actually, that is bad; pmap_enter() should just fail in that
	 * XXX case.  --thorpej
	 */
	/*
	 * if the desired page is not shadowed by the amap and we have a
	 * backing object, then we check to see if the backing object would
	 * prefer to handle the fault itself (rather than letting us do it
	 * with the usual pgo_get hook).  the backing object signals this by
	 * providing a pgo_fault routine.
	 */
	if (uobj && shadowed == FALSE && uobj->pgops->pgo_fault != NULL) {
		result = uobj->pgops->pgo_fault(&ufi, startva, pages, npages,
				    centeridx, fault_type, access_type,
				    PGO_LOCKED);

		if (result == VM_PAGER_OK)
			return (0);		/* pgo_fault did pmap enter */
		else if (result == VM_PAGER_REFAULT)
			goto ReFault;		/* try again! */
		else
			return (EACCES);
	}

	/*
	 * now, if the desired page is not shadowed by the amap and we have
	 * a backing object that does not have a special fault routine, then
	 * we ask (with pgo_get) the object for resident pages that we care
	 * about and attempt to map them in.  we do not let pgo_get block
	 * (PGO_LOCKED).
	 *
	 * ("get" has the option of doing a pmap_enter for us)
	 */
	if (uobj && shadowed == FALSE) {
		uvmexp.fltlget++;
		gotpages = npages;
		(void) uobj->pgops->pgo_get(uobj, ufi.entry->offset +
				(startva - ufi.entry->start),
				pages, &gotpages, centeridx,
				access_type & MASK(ufi.entry),
				ufi.entry->advice, PGO_LOCKED);

		/* check for pages to map, if we got any */
		uobjpage = NULL;
		if (gotpages) {
			currva = startva;
			for (lcv = 0 ; lcv < npages ;
			    lcv++, currva += PAGE_SIZE) {
				if (pages[lcv] == NULL ||
				    pages[lcv] == PGO_DONTCARE)
					continue;

				KASSERT((pages[lcv]->pg_flags & PG_RELEASED) == 0);

				/*
				 * if center page is resident and not
				 * PG_BUSY, then pgo_get made it PG_BUSY
				 * for us and gave us a handle to it.
				 * remember this page as "uobjpage."
				 * (for later use).
				 */
				if (lcv == centeridx) {
					uobjpage = pages[lcv];
					continue;
				}
	
				/* 
				 * note: calling pgo_get with locked data
				 * structures returns us pages which are
				 * neither busy nor released, so we don't
				 * need to check for this.   we can just
				 * directly enter the page (after moving it
				 * to the head of the active queue [useful?]).
				 */

				uvm_lock_pageq();
				uvm_pageactivate(pages[lcv]);	/* reactivate */
				uvm_unlock_pageq();
				uvmexp.fltnomap++;

				/*
				 * Since this page isn't the page that's
				 * actually faulting, ignore pmap_enter()
				 * failures; it's not critical that we
				 * enter these right now.
				 */
				(void) pmap_enter(ufi.orig_map->pmap, currva,
				    VM_PAGE_TO_PHYS(pages[lcv]),
				    enter_prot & MASK(ufi.entry),
				    PMAP_CANFAIL |
				     (wired ? PMAP_WIRED : 0));

				/* 
				 * NOTE: page can't be PG_WANTED because
				 * we've held the lock the whole time
				 * we've had the handle.
				 */
				atomic_clearbits_int(&pages[lcv]->pg_flags,
				    PG_BUSY);
				UVM_PAGE_OWN(pages[lcv], NULL);
			}	/* for "lcv" loop */
			pmap_update(ufi.orig_map->pmap);
		}   /* "gotpages" != 0 */
		/* note: object still _locked_ */
	} else {
		uobjpage = NULL;
	}

	/*
	 * note that at this point we are done with any front or back pages.
	 * we are now going to focus on the center page (i.e. the one we've
	 * faulted on).  if we have faulted on the top (anon) layer
	 * [i.e. case 1], then the anon we want is anons[centeridx] (we have
	 * not touched it yet).  if we have faulted on the bottom (uobj)
	 * layer [i.e. case 2] and the page was both present and available,
	 * then we've got a pointer to it as "uobjpage" and we've already
	 * made it BUSY.
	 */
	/*
	 * there are four possible cases we must address: 1A, 1B, 2A, and 2B
	 */
	/* redirect case 2: if we are not shadowed, go to case 2. */
	if (shadowed == FALSE) 
		goto Case2;

	/* handle case 1: fault on an anon in our amap */
	anon = anons[centeridx];

	/*
	 * no matter if we have case 1A or case 1B we are going to need to
	 * have the anon's memory resident.   ensure that now.
	 */
	/*
	 * let uvmfault_anonget do the dirty work.
	 * also, if it is OK, then the anon's page is on the queues.
	 */
	result = uvmfault_anonget(&ufi, amap, anon);
	switch (result) {
	case VM_PAGER_OK:
		break; 

	case VM_PAGER_REFAULT:
		goto ReFault;

	case VM_PAGER_ERROR:
		/*
		 * An error occured while trying to bring in the
		 * page -- this is the only error we return right
		 * now.
		 */
		return (EACCES);	/* XXX */
	default:
#ifdef DIAGNOSTIC
		panic("uvm_fault: uvmfault_anonget -> %d", result);
#else
		return (EACCES);
#endif
	}

	/*
	 * if we are case 1B then we will need to allocate a new blank
	 * anon to transfer the data into.   note that we have a lock
	 * on anon, so no one can busy or release the page until we are done.
	 * also note that the ref count can't drop to zero here because
	 * it is > 1 and we are only dropping one ref.
	 *
	 * in the (hopefully very rare) case that we are out of RAM we 
	 * will wait for more RAM, and refault.    
	 *
	 * if we are out of anon VM we wait for RAM to become available.
	 */

	if ((access_type & PROT_WRITE) != 0 && anon->an_ref > 1) {
		uvmexp.flt_acow++;
		oanon = anon;		/* oanon = old */
		anon = uvm_analloc();
		if (anon) {
			pg = uvm_pagealloc(NULL, 0, anon, 0);
		}

		/* check for out of RAM */
		if (anon == NULL || pg == NULL) {
			uvmfault_unlockall(&ufi, amap, NULL, oanon);
			KASSERT(uvmexp.swpgonly <= uvmexp.swpages);
			if (anon == NULL)
				uvmexp.fltnoanon++;
			else {
				uvm_anfree(anon);
				uvmexp.fltnoram++;
			}

			if (uvmexp.swpgonly == uvmexp.swpages)
				return (ENOMEM);

			/* out of RAM, wait for more */
			if (anon == NULL)
				uvm_anwait();
			else
				uvm_wait("flt_noram3");
			goto ReFault;
		}

		/* got all resources, replace anon with nanon */
		uvm_pagecopy(oanon->an_page, pg);	/* pg now !PG_CLEAN */
		/* un-busy! new page */
		atomic_clearbits_int(&pg->pg_flags, PG_BUSY|PG_FAKE);
		UVM_PAGE_OWN(pg, NULL);
		ret = amap_add(&ufi.entry->aref,
		    ufi.orig_rvaddr - ufi.entry->start, anon, 1);
		KASSERT(ret == 0);

		/* deref: can not drop to zero here by defn! */
		oanon->an_ref--;

		/*
		 * note: anon is _not_ locked, but we have the sole references
		 * to in from amap.
		 * thus, no one can get at it until we are done with it.
		 */
	} else {
		uvmexp.flt_anon++;
		oanon = anon;
		pg = anon->an_page;
		if (anon->an_ref > 1)     /* disallow writes to ref > 1 anons */
			enter_prot = enter_prot & ~PROT_WRITE;
	}

	/*
	 * now map the page in ...
	 * XXX: old fault unlocks object before pmap_enter.  this seems
	 * suspect since some other thread could blast the page out from
	 * under us between the unlock and the pmap_enter.
	 */
	if (pmap_enter(ufi.orig_map->pmap, ufi.orig_rvaddr, VM_PAGE_TO_PHYS(pg),
	    enter_prot, access_type | PMAP_CANFAIL | (wired ? PMAP_WIRED : 0))
	    != 0) {
		/*
		 * No need to undo what we did; we can simply think of
		 * this as the pmap throwing away the mapping information.
		 *
		 * We do, however, have to go through the ReFault path,
		 * as the map may change while we're asleep.
		 */
		uvmfault_unlockall(&ufi, amap, NULL, oanon);
		KASSERT(uvmexp.swpgonly <= uvmexp.swpages);
		if (uvmexp.swpgonly == uvmexp.swpages) {
			/* XXX instrumentation */
			return (ENOMEM);
		}
		/* XXX instrumentation */
		uvm_wait("flt_pmfail1");
		goto ReFault;
	}

	/* ... update the page queues. */
	uvm_lock_pageq();

	if (fault_type == VM_FAULT_WIRE) {
		uvm_pagewire(pg);
		/*
		 * since the now-wired page cannot be paged out,
		 * release its swap resources for others to use.
		 * since an anon with no swap cannot be PG_CLEAN,
		 * clear its clean flag now.
		 */
		atomic_clearbits_int(&pg->pg_flags, PG_CLEAN);
		uvm_anon_dropswap(anon);
	} else {
		/* activate it */
		uvm_pageactivate(pg);
	}

	uvm_unlock_pageq();

	/* done case 1!  finish up by unlocking everything and returning success */
	uvmfault_unlockall(&ufi, amap, NULL, oanon);
	pmap_update(ufi.orig_map->pmap);
	return (0);


Case2:
	/* handle case 2: faulting on backing object or zero fill */
	/*
	 * note that uobjpage can not be PGO_DONTCARE at this point.  we now
	 * set uobjpage to PGO_DONTCARE if we are doing a zero fill.  if we
	 * have a backing object, check and see if we are going to promote
	 * the data up to an anon during the fault.
	 */
	if (uobj == NULL) {
		uobjpage = PGO_DONTCARE;	
		promote = TRUE;		/* always need anon here */
	} else {
		KASSERT(uobjpage != PGO_DONTCARE);
		promote = (access_type & PROT_WRITE) &&
		     UVM_ET_ISCOPYONWRITE(ufi.entry);
	}

	/*
	 * if uobjpage is not null then we do not need to do I/O to get the
	 * uobjpage.
	 *
	 * if uobjpage is null, then we need to ask the pager to 
	 * get the data for us.   once we have the data, we need to reverify
	 * the state the world.   we are currently not holding any resources.
	 */
	if (uobjpage) {
		/* update rusage counters */
		curproc->p_ru.ru_minflt++;
	} else {
		/* update rusage counters */
		curproc->p_ru.ru_majflt++;
		
		uvmfault_unlockall(&ufi, amap, NULL, NULL);

		uvmexp.fltget++;
		gotpages = 1;
		uoff = (ufi.orig_rvaddr - ufi.entry->start) + ufi.entry->offset;
		result = uobj->pgops->pgo_get(uobj, uoff, &uobjpage, &gotpages,
		    0, access_type & MASK(ufi.entry), ufi.entry->advice,
		    PGO_SYNCIO);

		/* recover from I/O */
		if (result != VM_PAGER_OK) {
			KASSERT(result != VM_PAGER_PEND);

			if (result == VM_PAGER_AGAIN) {
				tsleep(&lbolt, PVM, "fltagain2", 0);
				goto ReFault;
			}

			if (!UVM_ET_ISNOFAULT(ufi.entry))
				return (EIO);

			uobjpage = PGO_DONTCARE;	
			promote = TRUE;
		}

		/* re-verify the state of the world.  */
		locked = uvmfault_relock(&ufi);
		
		/*
		 * Re-verify that amap slot is still free. if there is
		 * a problem, we clean up.
		 */
		if (locked && amap && amap_lookup(&ufi.entry->aref,
		      ufi.orig_rvaddr - ufi.entry->start)) {
			if (locked) 
				uvmfault_unlockall(&ufi, amap, NULL, NULL);
			locked = FALSE;
		}

		/* didn't get the lock?   release the page and retry. */
		if (locked == FALSE && uobjpage != PGO_DONTCARE) {
			uvm_lock_pageq();
			/* make sure it is in queues */
			uvm_pageactivate(uobjpage);
			uvm_unlock_pageq();

			if (uobjpage->pg_flags & PG_WANTED)
				/* still holding object lock */
				wakeup(uobjpage);
			atomic_clearbits_int(&uobjpage->pg_flags,
			    PG_BUSY|PG_WANTED);
			UVM_PAGE_OWN(uobjpage, NULL);
			goto ReFault;
		}

		/*
		 * we have the data in uobjpage which is PG_BUSY
		 */
	}

	/*
	 * notes:
	 *  - at this point uobjpage can not be NULL
	 *  - at this point uobjpage could be PG_WANTED (handle later)
	 */
	if (promote == FALSE) {
		/*
		 * we are not promoting.   if the mapping is COW ensure that we
		 * don't give more access than we should (e.g. when doing a read
		 * fault on a COPYONWRITE mapping we want to map the COW page in
		 * R/O even though the entry protection could be R/W).
		 *
		 * set "pg" to the page we want to map in (uobjpage, usually)
		 */
		uvmexp.flt_obj++;
		if (UVM_ET_ISCOPYONWRITE(ufi.entry))
			enter_prot &= ~PROT_WRITE;
		pg = uobjpage;		/* map in the actual object */

		/* assert(uobjpage != PGO_DONTCARE) */

		/*
		 * we are faulting directly on the page.
		 */
	} else {
		/*
		 * if we are going to promote the data to an anon we
		 * allocate a blank anon here and plug it into our amap.
		 */
#ifdef DIAGNOSTIC
		if (amap == NULL)
			panic("uvm_fault: want to promote data, but no anon");
#endif

		anon = uvm_analloc();
		if (anon) {
			/*
			 * In `Fill in data...' below, if
			 * uobjpage == PGO_DONTCARE, we want
			 * a zero'd, dirty page, so have
			 * uvm_pagealloc() do that for us.
			 */
			pg = uvm_pagealloc(NULL, 0, anon,
			    (uobjpage == PGO_DONTCARE) ? UVM_PGA_ZERO : 0);
		}

		/*
		 * out of memory resources?
		 */
		if (anon == NULL || pg == NULL) {
			/* arg!  must unbusy our page and fail or sleep. */
			if (uobjpage != PGO_DONTCARE) {
				uvm_lock_pageq();
				uvm_pageactivate(uobjpage);
				uvm_unlock_pageq();

				if (uobjpage->pg_flags & PG_WANTED)
					wakeup(uobjpage);
				atomic_clearbits_int(&uobjpage->pg_flags,
				    PG_BUSY|PG_WANTED);
				UVM_PAGE_OWN(uobjpage, NULL);
			}

			/* unlock and fail ... */
			uvmfault_unlockall(&ufi, amap, uobj, NULL);
			KASSERT(uvmexp.swpgonly <= uvmexp.swpages);
			if (anon == NULL)
				uvmexp.fltnoanon++;
			else {
				uvm_anfree(anon);
				uvmexp.fltnoram++;
			}

			if (uvmexp.swpgonly == uvmexp.swpages)
				return (ENOMEM);

			/* out of RAM, wait for more */
			if (anon == NULL)
				uvm_anwait();
			else
				uvm_wait("flt_noram5");
			goto ReFault;
		}

		/* fill in the data */
		if (uobjpage != PGO_DONTCARE) {
			uvmexp.flt_prcopy++;
			/* copy page [pg now dirty] */
			uvm_pagecopy(uobjpage, pg);

			/*
			 * promote to shared amap?  make sure all sharing
			 * procs see it
			 */
			if ((amap_flags(amap) & AMAP_SHARED) != 0) {
				pmap_page_protect(uobjpage, PROT_NONE);
			}
			
			/* dispose of uobjpage. drop handle to uobj as well. */
			if (uobjpage->pg_flags & PG_WANTED)
				wakeup(uobjpage);
			atomic_clearbits_int(&uobjpage->pg_flags,
			    PG_BUSY|PG_WANTED);
			UVM_PAGE_OWN(uobjpage, NULL);
			uvm_lock_pageq();
			uvm_pageactivate(uobjpage);
			uvm_unlock_pageq();
			uobj = NULL;
		} else {
			uvmexp.flt_przero++;
			/*
			 * Page is zero'd and marked dirty by uvm_pagealloc()
			 * above.
			 */
		}

		if (amap_add(&ufi.entry->aref,
		    ufi.orig_rvaddr - ufi.entry->start, anon, 0)) {
			uvmfault_unlockall(&ufi, amap, NULL, oanon);
			KASSERT(uvmexp.swpgonly <= uvmexp.swpages);
			uvm_anfree(anon);
			uvmexp.fltnoamap++;

			if (uvmexp.swpgonly == uvmexp.swpages)
				return (ENOMEM);

			amap_populate(&ufi.entry->aref,
			    ufi.orig_rvaddr - ufi.entry->start);
			goto ReFault;
		}
	}

	/* note: pg is either the uobjpage or the new page in the new anon */
	/*
	 * all resources are present.   we can now map it in and free our
	 * resources.
	 */
	if (pmap_enter(ufi.orig_map->pmap, ufi.orig_rvaddr, VM_PAGE_TO_PHYS(pg),
	    enter_prot, access_type | PMAP_CANFAIL | (wired ? PMAP_WIRED : 0))
	    != 0) {
		/*
		 * No need to undo what we did; we can simply think of
		 * this as the pmap throwing away the mapping information.
		 *
		 * We do, however, have to go through the ReFault path,
		 * as the map may change while we're asleep.
		 */
		if (pg->pg_flags & PG_WANTED)
			wakeup(pg);

		atomic_clearbits_int(&pg->pg_flags, PG_BUSY|PG_FAKE|PG_WANTED);
		UVM_PAGE_OWN(pg, NULL);
		uvmfault_unlockall(&ufi, amap, uobj, NULL);
		KASSERT(uvmexp.swpgonly <= uvmexp.swpages);
		if (uvmexp.swpgonly == uvmexp.swpages) {
			/* XXX instrumentation */
			return (ENOMEM);
		}
		/* XXX instrumentation */
		uvm_wait("flt_pmfail2");
		goto ReFault;
	}

	uvm_lock_pageq();

	if (fault_type == VM_FAULT_WIRE) {
		uvm_pagewire(pg);
		if (pg->pg_flags & PQ_AOBJ) {
			/*
			 * since the now-wired page cannot be paged out,
			 * release its swap resources for others to use.
			 * since an aobj page with no swap cannot be PG_CLEAN,
			 * clear its clean flag now.
			 */
			atomic_clearbits_int(&pg->pg_flags, PG_CLEAN);
			uao_dropswap(uobj, pg->offset >> PAGE_SHIFT);
		}
	} else {
		/* activate it */
		uvm_pageactivate(pg);
	}
	uvm_unlock_pageq();

	if (pg->pg_flags & PG_WANTED)
		wakeup(pg);

	atomic_clearbits_int(&pg->pg_flags, PG_BUSY|PG_FAKE|PG_WANTED);
	UVM_PAGE_OWN(pg, NULL);
	uvmfault_unlockall(&ufi, amap, uobj, NULL);
	pmap_update(ufi.orig_map->pmap);

	return (0);
}


/*
 * uvm_fault_wire: wire down a range of virtual addresses in a map.
 *
 * => map may be read-locked by caller, but MUST NOT be write-locked.
 * => if map is read-locked, any operations which may cause map to
 *	be write-locked in uvm_fault() must be taken care of by
 *	the caller.  See uvm_map_pageable().
 */
int
uvm_fault_wire(vm_map_t map, vaddr_t start, vaddr_t end, vm_prot_t access_type)
{
	vaddr_t va;
	int rv;

	/*
	 * now fault it in a page at a time.   if the fault fails then we have
	 * to undo what we have done.   note that in uvm_fault PROT_NONE 
	 * is replaced with the max protection if fault_type is VM_FAULT_WIRE.
	 */
	for (va = start ; va < end ; va += PAGE_SIZE) {
		rv = uvm_fault(map, va, VM_FAULT_WIRE, access_type);
		if (rv) {
			if (va != start) {
				uvm_fault_unwire(map, start, va);
			}
			return (rv);
		}
	}

	return (0);
}

/*
 * uvm_fault_unwire(): unwire range of virtual space.
 */
void
uvm_fault_unwire(vm_map_t map, vaddr_t start, vaddr_t end)
{

	vm_map_lock_read(map);
	uvm_fault_unwire_locked(map, start, end);
	vm_map_unlock_read(map);
}

/*
 * uvm_fault_unwire_locked(): the guts of uvm_fault_unwire().
 *
 * => map must be at least read-locked.
 */
void
uvm_fault_unwire_locked(vm_map_t map, vaddr_t start, vaddr_t end)
{
	vm_map_entry_t entry, next;
	pmap_t pmap = vm_map_pmap(map);
	vaddr_t va;
	paddr_t pa;
	struct vm_page *pg;

	KASSERT((map->flags & VM_MAP_INTRSAFE) == 0);

	/*
	 * we assume that the area we are unwiring has actually been wired
	 * in the first place.   this means that we should be able to extract
	 * the PAs from the pmap.   we also lock out the page daemon so that
	 * we can call uvm_pageunwire.
	 */
	uvm_lock_pageq();

	/* find the beginning map entry for the region. */
	KASSERT(start >= vm_map_min(map) && end <= vm_map_max(map));
	if (uvm_map_lookup_entry(map, start, &entry) == FALSE)
		panic("uvm_fault_unwire_locked: address not in map");

	for (va = start; va < end ; va += PAGE_SIZE) {
		if (pmap_extract(pmap, va, &pa) == FALSE)
			continue;

		/* find the map entry for the current address. */
		KASSERT(va >= entry->start);
		while (va >= entry->end) {
			next = RBT_NEXT(uvm_map_addr, entry);
			KASSERT(next != NULL && next->start <= entry->end);
			entry = next;
		}

		/* if the entry is no longer wired, tell the pmap. */
		if (VM_MAPENT_ISWIRED(entry) == 0)
			pmap_unwire(pmap, va);

		pg = PHYS_TO_VM_PAGE(pa);
		if (pg)
			uvm_pageunwire(pg);
	}

	uvm_unlock_pageq();
}

/*
 * uvmfault_unlockmaps: unlock the maps
 */
void
uvmfault_unlockmaps(struct uvm_faultinfo *ufi, boolean_t write_locked)
{
	/*
	 * ufi can be NULL when this isn't really a fault,
	 * but merely paging in anon data.
	 */
	if (ufi == NULL) {
		return;
	}

	uvmfault_update_stats(ufi);
	if (write_locked) {
		vm_map_unlock(ufi->map);
	} else {
		vm_map_unlock_read(ufi->map);
	}
}

/*
 * uvmfault_unlockall: unlock everything passed in.
 *
 * => maps must be read-locked (not write-locked).
 */
void
uvmfault_unlockall(struct uvm_faultinfo *ufi, struct vm_amap *amap,
    struct uvm_object *uobj, struct vm_anon *anon)
{

	uvmfault_unlockmaps(ufi, FALSE);
}

/*
 * uvmfault_lookup: lookup a virtual address in a map
 *
 * => caller must provide a uvm_faultinfo structure with the IN
 *	params properly filled in
 * => we will lookup the map entry (handling submaps) as we go
 * => if the lookup is a success we will return with the maps locked
 * => if "write_lock" is TRUE, we write_lock the map, otherwise we only
 *	get a read lock.
 * => note that submaps can only appear in the kernel and they are 
 *	required to use the same virtual addresses as the map they
 *	are referenced by (thus address translation between the main
 *	map and the submap is unnecessary).
 */

boolean_t
uvmfault_lookup(struct uvm_faultinfo *ufi, boolean_t write_lock)
{
	vm_map_t tmpmap;

	/* init ufi values for lookup. */
	ufi->map = ufi->orig_map;
	ufi->size = ufi->orig_size;

	/*
	 * keep going down levels until we are done.   note that there can
	 * only be two levels so we won't loop very long.
	 */
	while (1) {
		if (ufi->orig_rvaddr < ufi->map->min_offset ||
		    ufi->orig_rvaddr >= ufi->map->max_offset)
			return(FALSE);

		/* lock map */
		if (write_lock) {
			vm_map_lock(ufi->map);
		} else {
			vm_map_lock_read(ufi->map);
		}

		/* lookup */
		if (!uvm_map_lookup_entry(ufi->map, ufi->orig_rvaddr, 
		    &ufi->entry)) {
			uvmfault_unlockmaps(ufi, write_lock);
			return(FALSE);
		}

		/* reduce size if necessary */
		if (ufi->entry->end - ufi->orig_rvaddr < ufi->size)
			ufi->size = ufi->entry->end - ufi->orig_rvaddr;

		/*
		 * submap?    replace map with the submap and lookup again.
		 * note: VAs in submaps must match VAs in main map.
		 */
		if (UVM_ET_ISSUBMAP(ufi->entry)) {
			tmpmap = ufi->entry->object.sub_map;
			uvmfault_unlockmaps(ufi, write_lock);
			ufi->map = tmpmap;
			continue;
		}

		/* got it! */
		ufi->mapv = ufi->map->timestamp;
		return(TRUE);

	}
	/*NOTREACHED*/
}

/*
 * uvmfault_relock: attempt to relock the same version of the map
 *
 * => fault data structures should be unlocked before calling.
 * => if a success (TRUE) maps will be locked after call.
 */
boolean_t
uvmfault_relock(struct uvm_faultinfo *ufi)
{
	/*
	 * ufi can be NULL when this isn't really a fault,
	 * but merely paging in anon data.
	 */
	if (ufi == NULL) {
		return TRUE;
	}

	uvmexp.fltrelck++;

	/*
	 * relock map.   fail if version mismatch (in which case nothing 
	 * gets locked).
	 */
	vm_map_lock_read(ufi->map);
	if (ufi->mapv != ufi->map->timestamp) {
		vm_map_unlock_read(ufi->map);
		return(FALSE);
	}

	uvmexp.fltrelckok++;
	return(TRUE);		/* got it! */
}
@


1.91
log
@move the uvm_map_addr RB tree from RB macros to the RBT functions

this tree is interesting because it uses all the red black tree
features, specifically the augment callback thats called on tree
topology changes, and it poisons and checks entries as theyre removed
from and inserted back into the tree respectively.

ok stefan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.90 2016/05/08 11:52:32 stefan Exp $	*/
d1035 1
a1035 1
				return (EACCES); /* XXX i/o error */
@


1.90
log
@Wait for RAM in uvm_fault when allocating uvm structures fails

Only fail hard when running out of swap space also, as suggested by
kettenis@@

While there, let amap_add() return a success status and handle
amap_add() errors in uvm_fault() similar to other out of RAM situations.
These bits are needed for further amap reorganization diffs.

lots of feedback and ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.89 2016/03/29 12:04:26 chl Exp $	*/
d1351 1
a1351 1
			next = RB_NEXT(uvm_map_addr, &map->addr, entry);
@


1.89
log
@Remove dead assignments and now unused variables.

Found by LLVM/Clang Static Analyzer.

ok mpi@@ stefan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.88 2016/03/07 18:44:00 naddy Exp $	*/
d496 1
a496 1
	int npages, nback, nforw, centeridx, result, lcv, gotpages;
d873 1
a873 1
	 * if we are out of anon VM we kill the process (XXX: could wait?).
a885 2
			if (anon)
				uvm_anfree(anon);
d888 1
a888 1
			if (anon == NULL || uvmexp.swpgonly == uvmexp.swpages) {
d890 6
a896 1
			}
d898 5
a902 2
			uvmexp.fltnoram++;
			uvm_wait("flt_noram3");	/* out of RAM, wait for more */
d911 3
a913 2
		amap_add(&ufi.entry->aref, ufi.orig_rvaddr - ufi.entry->start,
		    anon, 1);
d1142 1
a1142 1
			if (anon == NULL || uvmexp.swpgonly == uvmexp.swpages) {
d1144 6
a1150 1
			}
d1152 5
a1156 3
			uvm_anfree(anon);
			uvmexp.fltnoram++;
			uvm_wait("flt_noram5");
d1192 14
a1205 2
		amap_add(&ufi.entry->aref, ufi.orig_rvaddr - ufi.entry->start,
		    anon, 0);
@


1.88
log
@Sync no-argument function declaration and definition by adding (void).
ok mpi@@ millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.87 2015/11/10 08:21:28 mlarkin Exp $	*/
a1256 1
	pmap_t  pmap;
a1257 2

	pmap = vm_map_pmap(map);
@


1.87
log
@
UVM change needed for vmm.

discussed with miod, deraadt, and guenther.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.86 2015/09/09 14:52:12 miod Exp $	*/
d201 1
a201 1
uvmfault_init()
@


1.86
log
@All our pmap implementations provide pmap_resident_count(), so remove
#ifndef pmap_resident_count code paths.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.85 2015/08/21 16:04:35 visa Exp $	*/
d448 10
@


1.85
log
@Remove the unused loan_count field and the related uvm logic. Most of
the page loaning code is already in the Attic.

ok kettenis@@, beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.84 2015/03/14 03:38:53 jsg Exp $	*/
a445 3
#ifndef pmap_resident_count
	struct vm_space		*vm;
#endif
a453 1
#ifdef pmap_resident_count
a454 10
#else
		/*
		 * Rather inaccurate, but this is the current anon size
		 * of the vmspace.  It's basically the resident size
		 * minus the mmapped in files/text.
		 */
		vm = (struct vmspace*)map;
		res = vm->dsize;
#endif

@


1.84
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.83 2015/02/08 02:17:08 deraadt Exp $	*/
d183 1
a183 1
		if (pg && (pg->pg_flags & PG_BUSY) == 0 && pg->loan_count == 0) {
a260 2
 * => for pages which are on loan from a uvm_object (and thus are not
 *    owned by the anon): if successful, we return with the owning object
a283 8
		/*
		 * if there is a resident page and it is loaned, then anon
		 * may not own it.   call out to uvm_anon_lockpage() to ensure
		 * the real owner of the page has been identified.
		 */
		if (pg && pg->loan_count)
			pg = uvm_anon_lockloanpg(anon);

d286 3
d290 2
a291 4
			 * at this point, if the page has a uobject [meaning
			 * we have it on loan], then that uobject is locked
			 * by us!   if the page is busy, we drop all the
			 * locks (including uobject) and try again.
d293 1
a293 1
			if ((pg->pg_flags & (PG_BUSY|PG_RELEASED)) == 0) {
a294 1
			}
d302 2
a303 8
			if (pg->uobject) {	/* owner is uobject ? */
				uvmfault_unlockall(ufi, amap, NULL, anon);
				UVM_WAIT(pg, FALSE, "anonget1",0);
			} else {
				/* anon owns page */
				uvmfault_unlockall(ufi, amap, NULL, NULL);
				UVM_WAIT(pg, 0, "anonget2", 0);
			}
d681 1
a681 2
		/* ignore loaned pages */
		if (anon->an_page && anon->an_page->loan_count == 0 &&
a866 67
	/* uobj is non null if the page is on loan from an object (i.e. uobj) */
	uobj = anon->an_page->uobject;

	/* special handling for loaned pages */
	if (anon->an_page->loan_count) {
		if ((access_type & PROT_WRITE) == 0) {
			/*
			 * for read faults on loaned pages we just cap the
			 * protection at read-only.
			 */
			enter_prot = enter_prot & ~PROT_WRITE;
		} else {
			/*
			 * note that we can't allow writes into a loaned page!
			 *
			 * if we have a write fault on a loaned page in an
			 * anon then we need to look at the anon's ref count.
			 * if it is greater than one then we are going to do
			 * a normal copy-on-write fault into a new anon (this
			 * is not a problem).  however, if the reference count
			 * is one (a case where we would normally allow a
			 * write directly to the page) then we need to kill
			 * the loan before we continue.
			 */

			/* >1 case is already ok */
			if (anon->an_ref == 1) {
				/* get new un-owned replacement page */
				pg = uvm_pagealloc(NULL, 0, NULL, 0);
				if (pg == NULL) {
					uvmfault_unlockall(&ufi, amap, uobj,
					    anon);
					uvm_wait("flt_noram2");
					goto ReFault;
				}

				/* copy data, kill loan */
				/* copy old -> new */
				uvm_pagecopy(anon->an_page, pg);

				/* force reload */
				pmap_page_protect(anon->an_page, PROT_NONE);
				uvm_lock_pageq();	  /* KILL loan */
				if (uobj)
					/* if we were loaning */
					anon->an_page->loan_count--;
				anon->an_page->uanon = NULL;
				/* in case we owned */
				atomic_clearbits_int(
				    &anon->an_page->pg_flags, PQ_ANON);
				uvm_pageactivate(pg);
				uvm_unlock_pageq();
				if (uobj) {
					uobj = NULL;
				}

				/* install new page in anon */
				anon->an_page = pg;
				pg->uanon = anon;
				atomic_setbits_int(&pg->pg_flags, PQ_ANON);
				atomic_clearbits_int(&pg->pg_flags,
				    PG_BUSY|PG_FAKE);
				UVM_PAGE_OWN(pg, NULL);
			}     /* ref == 1 */
		}       /* write fault */
	}         /* loan count */

d892 1
a892 1
			uvmfault_unlockall(&ufi, amap, uobj, oanon);
d944 1
a944 1
		uvmfault_unlockall(&ufi, amap, uobj, oanon);
d976 1
a976 1
	uvmfault_unlockall(&ufi, amap, uobj, oanon);
d1095 1
a1095 2
		 * we are faulting directly on the page.   be careful
		 * about writing to loaned pages...
a1096 75
		if (uobjpage->loan_count) {

			if ((access_type & PROT_WRITE) == 0) {
				/* read fault: cap the protection at readonly */
				/* cap! */
				enter_prot = enter_prot & ~PROT_WRITE;
			} else {
				/* write fault: must break the loan here */
				/* alloc new un-owned page */
				pg = uvm_pagealloc(NULL, 0, NULL, 0);

				if (pg == NULL) {
					/*
					 * drop ownership of page, it can't
					 * be released
					 */
					if (uobjpage->pg_flags & PG_WANTED)
						wakeup(uobjpage);
					atomic_clearbits_int(
					    &uobjpage->pg_flags,
					    PG_BUSY|PG_WANTED);
					UVM_PAGE_OWN(uobjpage, NULL);

					uvm_lock_pageq();
					/* activate: we will need it later */
					uvm_pageactivate(uobjpage);

					uvm_unlock_pageq();
					uvmfault_unlockall(&ufi, amap, uobj,
					  NULL);
					uvmexp.fltnoram++;
					uvm_wait("flt_noram4");
					goto ReFault;
				}

				/*
				 * copy the data from the old page to the new
				 * one and clear the fake/clean flags on the
				 * new page (keep it busy).  force a reload
				 * of the old page by clearing it from all
				 * pmaps.  then lock the page queues to
				 * rename the pages.
				 */
				uvm_pagecopy(uobjpage, pg);	/* old -> new */
				atomic_clearbits_int(&pg->pg_flags,
				    PG_FAKE|PG_CLEAN);
				pmap_page_protect(uobjpage, PROT_NONE);
				if (uobjpage->pg_flags & PG_WANTED)
					wakeup(uobjpage);
				atomic_clearbits_int(&uobjpage->pg_flags,
				    PG_BUSY|PG_WANTED);
				UVM_PAGE_OWN(uobjpage, NULL);

				uvm_lock_pageq();
				uoff = uobjpage->offset;
				/* remove old page */
				uvm_pagerealloc(uobjpage, NULL, 0);

				/*
				 * at this point we have absolutely no
				 * control over uobjpage
				 */
				/* install new page */
				uvm_pagerealloc(pg, uobj, uoff);
				uvm_unlock_pageq();

				/*
				 * done!  loan is broken and "pg" is
				 * PG_BUSY.   it can now replace uobjpage.
				 */

				uobjpage = pg;

			}		/* write fault case */
		}		/* if loan_count */
@


1.83
log
@Something is subtly wrong with this.  On ramdisks, processes run out of
mappable memory (direct or via execve), perhaps because of the address
allocator behind maps and the way wiring counts work?
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.81 2014/12/17 06:58:11 guenther Exp $	*/
a40 1
#include <sys/user.h>
@


1.82
log
@Clear PQ_AOBJ before calling uvm_pagefree(), clearing up one false XXX
comment (one is fixed, one is deleted).
ok kettenis beck
@
text
@a1390 1
			atomic_clearbits_int(&pg->pg_flags, PQ_AOBJ);
@


1.81
log
@Prefer MADV_* over POSIX_MADV_* in kernel for consistency: the latter
doesn't have all the values and therefore can't be used everywhere.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.80 2014/12/15 02:24:23 guenther Exp $	*/
d1391 1
@


1.80
log
@Use MAP_INHERIT_* for the 'inh' argument to the UMV_MAPFLAG() macro,
eliminating the must-be-kept-in-sync UVM_INH_* macros

ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.79 2014/11/16 12:31:00 deraadt Exp $	*/
d209 2
a210 2
		uvmadvice[POSIX_MADV_NORMAL].nforw = npages;
		uvmadvice[POSIX_MADV_NORMAL].nback = npages - 1;
d216 2
a217 2
		uvmadvice[POSIX_MADV_SEQUENTIAL].nforw = npages - 1;
		uvmadvice[POSIX_MADV_SEQUENTIAL].nback = npages;
@


1.79
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.78 2014/10/03 17:41:00 kettenis Exp $	*/
d154 1
a154 1
static struct uvm_advice uvmadvice[UVM_ADV_MASK + 1];
@


1.78
log
@Introduce __MAP_NOFAULT, a mmap(2) flag that makes sure a mapping will not
cause a SIGSEGV or SIGBUS when a mapped file gets truncated.  Access to
pages that are not backed by a file on such a mapping will be replaced by
zero-filled anonymous pages.  Makes passing file descriptors of mapped files
usable without having to play tricks with signal handlers.

"steal your mmap flag" deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.77 2014/09/07 08:17:44 guenther Exp $	*/
d187 1
a187 1
				pmap_page_protect(pg, VM_PROT_NONE);
d209 2
a210 2
		uvmadvice[UVM_ADV_NORMAL].nforw = npages;
		uvmadvice[UVM_ADV_NORMAL].nback = npages - 1;
d216 2
a217 2
		uvmadvice[UVM_ADV_SEQUENTIAL].nforw = npages - 1;
		uvmadvice[UVM_ADV_SEQUENTIAL].nback = npages;
d383 1
a383 1
				pmap_page_protect(pg, VM_PROT_NONE);
d509 1
a509 1
			 ~VM_PROT_WRITE : VM_PROT_ALL)
d574 1
a574 1
		if ((access_type & VM_PROT_WRITE) ||
d586 1
a586 1
			enter_prot &= ~VM_PROT_WRITE; 
d713 1
a713 1
			    (anon->an_ref > 1) ? (enter_prot & ~VM_PROT_WRITE) :
d890 1
a890 1
		if ((access_type & VM_PROT_WRITE) == 0) {
d895 1
a895 1
			enter_prot = enter_prot & ~VM_PROT_WRITE;
d926 1
a926 2
				pmap_page_protect(anon->an_page,
						  VM_PROT_NONE);
d965 1
a965 1
	if ((access_type & VM_PROT_WRITE) != 0 && anon->an_ref > 1) {
d1010 1
a1010 1
			enter_prot = enter_prot & ~VM_PROT_WRITE;
d1079 1
a1079 1
		promote = (access_type & VM_PROT_WRITE) &&
d1174 1
a1174 1
			enter_prot &= ~VM_PROT_WRITE;
d1185 1
a1185 1
			if ((access_type & VM_PROT_WRITE) == 0) {
d1188 1
a1188 1
				enter_prot = enter_prot & ~VM_PROT_WRITE;
d1229 1
a1229 1
				pmap_page_protect(uobjpage, VM_PROT_NONE);
d1322 1
a1322 1
				pmap_page_protect(uobjpage, VM_PROT_NONE);
d1429 1
a1429 1
	 * to undo what we have done.   note that in uvm_fault VM_PROT_NONE 
@


1.77
log
@typo in comment
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.76 2014/07/11 16:35:40 jsg Exp $	*/
d1117 5
a1121 1
			return (EACCES); /* XXX i/o error */
d1139 1
a1139 1
		if (locked == FALSE) {
@


1.76
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.75 2014/07/08 11:38:48 deraadt Exp $	*/
d808 1
a808 1
				 * actually fauling, ignore pmap_enter()
@


1.75
log
@bye bye UBC; ok beck dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.74 2014/07/03 11:38:46 kettenis Exp $	*/
a4 1
 *
a15 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed by Charles D. Cranor and
 *      Washington University.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
@


1.74
log
@It is important that we don't release the kernel lock between issuing a
wakeup and clearing the PG_BUSY and PG_WANTED flags, so try to keep those
bits as close together and defenitely avoid calling random code in between.

ok guenther@@, tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.73 2014/05/08 20:08:50 kettenis Exp $	*/
a193 3
#ifdef UBC
				pmap_clear_reference(pg);
#else
a194 1
#endif
@


1.73
log
@Fix some potential integer overflows caused by converting a page number into
an offset/size/address by shifting by PAGE_SHIFT.  Make uvm_objwrire/unwire
use voff_t instead of off_t.  The former is the right type here even if it is
equivalent to the latter.

Inspired by a somewhat similar changes in Bitrig.

ok deraadt@@, guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.72 2014/04/13 23:14:15 tedu Exp $	*/
a1146 4
			if (uobjpage->pg_flags & PG_WANTED)
				/* still holding object lock */
				wakeup(uobjpage);

d1150 1
d1152 3
a1154 1
			uvm_unlock_pageq();
a1158 1

a1293 3
				if (uobjpage->pg_flags & PG_WANTED)
					wakeup(uobjpage);

d1297 3
@


1.72
log
@compress code by turning four line comments into one line comments.
emphatic ok usual suspects, grudging ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.71 2014/04/03 20:21:01 miod Exp $	*/
d625 1
a625 1
		startva = ufi.orig_rvaddr - (nback << PAGE_SHIFT);
d667 1
a667 1
				    (nback << PAGE_SHIFT), PGO_DEACTIVATE);
d673 1
a673 1
		startva += (nback << PAGE_SHIFT);
@


1.71
log
@uvm_fault() will try to fault neighbouring pages for the MADV_NORMAL case,
which is the default, unless the fault call is explicitly used to wire a given
page.

The amount of pages being faulted in was borrowed from the FreeBSD VM code,
about 15 years ago, at a time FreeBSD was only reliably running on 4KB page
size systems.

It is questionable whether faulting the same amount of pages, on platforms
where the page size is larger, is a good idea, as it may cause too much I/O.

Add an uvmfault_init() routine, which will compute the proper number of pages
at runtime, depending upon the actual page size, and attempting to fault in
the same overall size the previous code would have done with 4KB pages.

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.70 2014/03/31 20:16:39 miod Exp $	*/
a152 1

a160 1

a167 1

a175 1

a180 1

a208 1

a211 1

a236 1

d241 1
a241 4
	/*
	 * while we haven't done the job
	 */

d243 1
a243 5

		/*
		 * no mapping?  give up.
		 */

d247 1
a247 4
		/*
		 * copy if needed.
		 */

d252 1
a252 4
		/*
		 * didn't work?  must be out of RAM.  sleep.
		 */

d259 1
a259 4
		/*
		 * got it!
		 */
		
a275 1

d293 1
a293 4
	/* 
	 * loop until we get it, or fail.
	 */

a294 1

a302 1

d306 1
a306 4
		/*
		 * page there?   make sure it is not busy/released.
		 */

a307 1

a313 1

a332 1

d334 1
a334 4
		
			/*
			 * no page, we must try and bring it in.
			 */
a337 1

a341 1

a342 1
	
d366 1
a366 4
		/*
		 * now relock and try again
		 */

a378 1
		
a379 1

d443 1
a443 4
		/*
		 * we were not able to relock.   restart fault.
		 */

d447 1
a447 4
		/*
		 * verify no one has touched the amap and moved the anon on us.
		 */

a454 4
			
		/*
		 * try it again! 
		 */
d456 1
a460 1

d480 1
a480 3
	/*
	 * Update the maxrss for the process.
	 */
a518 1

a520 1

d542 1
a542 4
	/*
	 * init the IN parameters in the ufi
	 */

d552 1
a552 3
	/*
	 * "goto ReFault" means restart the page fault from ground zero.
	 */
d554 1
a554 5

	/*
	 * lookup and lock the maps
	 */

d565 1
a565 4
	/*
	 * check protection
	 */

d583 1
a583 4
	/*
	 * handle "needs_copy" case.
	 */

a591 1

a592 1

a597 1

d601 1
a601 4
	/*
	 * identify the players
	 */

a608 1

a619 1

a636 1

a637 1
		
a642 1

d645 1
a645 4
	/*
	 * if we've got an amap, extract current anons.
	 */

a657 1

a658 1

a682 1

a685 1

d696 1
a696 3
		/*
		 * unmapped or center page.   check if any anon at this level.
		 */
d702 1
a702 4
		/*
		 * check for present page and map if possible.   re-activate it.
		 */

a721 1

a733 1

a740 1
	
a747 1

a769 1

d779 1
a779 4
		/*
		 * check for pages to map, if we got any
		 */

a780 1

a784 1

a797 1
				
a822 1

a833 1

a854 1

d858 1
a858 5

	/*
	 * redirect case 2: if we are not shadowed, go to case 2.
	 */

d862 1
a862 4
	/*
	 * handle case 1: fault on an anon in our amap
	 */

a868 1

a872 1

a887 1

d896 1
a896 4
	/*
	 * uobj is non null if the page is on loan from an object (i.e. uobj)
	 */

d899 1
a899 4
	/*
	 * special handling for loaned pages 
	 */

a900 1

a901 1
			
a905 1

a906 1

a922 1

d932 1
a932 3
				/*
				 * copy data, kill loan
				 */
a959 2

				/* done! */
a1001 1

a1016 1

a1017 1

a1022 1

a1030 1

d1052 1
a1052 4
	/*
	 * ... update the page queues.
	 */

a1056 1

d1072 1
a1072 4
	/*
	 * done case 1!  finish up by unlocking everything and returning success
	 */

d1079 1
a1079 4
	/*
	 * handle case 2: faulting on backing object or zero fill
	 */

a1085 1

a1102 1

d1119 1
a1119 4
		/*
		 * recover from I/O
		 */

d1131 1
a1131 4
		/*
		 * re-verify the state of the world.
		 */

a1137 1

d1145 1
a1145 4
		/*
		 * didn't get the lock?   release the page and retry.
		 */

a1165 1

a1172 1
		
a1173 1

a1181 1

a1200 1

a1267 1

a1268 1
		
d1294 1
a1294 4

			/*
			 * arg!  must unbusy our page and fail or sleep.
			 */
d1321 1
a1321 4
		/*
		 * fill in the data
		 */

d1335 1
a1335 4
			/*
			 * dispose of uobjpage. drop handle to uobj as well.
			 */

d1357 1
a1357 4
	/*
	 * note: pg is either the uobjpage or the new page in the new anon
	 */

a1361 1

a1364 1

a1371 1

a1392 1

a1427 1

a1441 1

a1457 1

a1471 1

a1488 1

d1491 1
a1491 3
	/*
	 * find the beginning map entry for the region.
	 */
d1500 1
a1500 3
		/*
		 * find the map entry for the current address.
		 */
d1508 1
a1508 3
		/*
		 * if the entry is no longer wired, tell the pmap.
		 */
a1529 1

d1575 1
a1575 4
	/*
	 * init ufi values for lookup.
	 */

a1582 1

d1588 1
a1588 3
		/*
		 * lock map
		 */
d1595 1
a1595 3
		/*
		 * lookup
		 */
d1602 1
a1602 3
		/*
		 * reduce size if necessary
		 */
d1617 1
a1617 4
		/*
		 * got it!
		 */

d1621 1
a1621 2
	}	/* while loop */

a1637 1

a1647 1

@


1.70
log
@In uvm_fault(), when attempting to map backpages and forwpages, defer
the pmap_update() to the end of the loop, rather than after each loop
iteration - which might not even end up invoking pmap_enter()!

Quiet blessing from guenther@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.69 2013/05/30 18:02:04 tedu Exp $	*/
a154 1
	int advice;
d160 1
a160 3
 * page range array:
 * note: index in array must match "advice" value 
 * XXX: borrowed numbers from freebsd.   do they work well for us?
d163 1
a163 5
static struct uvm_advice uvmadvice[] = {
	{ MADV_NORMAL, 3, 4 },
	{ MADV_RANDOM, 0, 0 },
	{ MADV_SEQUENTIAL, 8, 7},
};
d216 24
a706 2
		KASSERT(uvmadvice[ufi.entry->advice].advice ==
			 ufi.entry->advice);
@


1.69
log
@in the brave new world of void *, we don't need caddr_t casts
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.68 2013/05/30 16:39:26 tedu Exp $	*/
d814 2
a816 1
	}
@


1.68
log
@UVM_UNLOCK_AND_WAIT no longer unlocks, so rename it to UVM_WAIT.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.67 2013/05/30 16:29:46 tedu Exp $	*/
d1266 1
a1266 1
				tsleep((caddr_t)&lbolt, PVM, "fltagain2", 0);
@


1.67
log
@remove lots of comments about locking per beck's request
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.66 2013/05/30 15:17:59 tedu Exp $	*/
d344 1
a344 3
				UVM_UNLOCK_AND_WAIT(pg,
				    &pg->uobject->vmobjlock,
				    FALSE, "anonget1",0);
d348 1
a348 2
				UVM_UNLOCK_AND_WAIT(pg,&anon->an_lock,0,
				    "anonget2",0);
@


1.66
log
@remove simple_locks from uvm code. ok beck deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.65 2012/04/12 11:55:43 ariane Exp $	*/
a224 2
 * => called with VM data structures unlocked (usually, see below)
 * => we get a write lock on the maps and clear needs_copy for a VA
d254 1
a254 1
		 * didn't work?  must be out of RAM.   unlock and sleep.
d264 1
a264 1
		 * got it!   unlock and return.
a276 3
 * => maps, amap, and anon locked by caller.
 * => if we fail (result != VM_PAGER_OK) we unlock everything.
 * => if we are successful, we return with everything still locked.
a281 2
 *    locked.   the caller must unlock this object when it unlocks everything
 *    else.
d313 1
a313 1
		 * the real owner of the page has been identified and locked.
a413 1
				/* still holding object lock */
a613 1
	/* locked: maps(read) */
d643 1
a643 4
	 * handle "needs_copy" case.   if we need to copy the amap we will
	 * have to drop our readlock and relock it with a write lock.  (we
	 * need a write lock to change anything in a map entry [e.g.
	 * needs_copy]).
a719 2
	/* locked: maps(read) */

d721 1
a721 1
	 * if we've got an amap, lock it and extract current anons.
a731 2
	/* locked: maps(read), amap(if there) */

a757 2
	/* locked: maps(read), amap(if there) */

a819 1
	/* locked: maps(read), amap(if there) */
d824 1
a824 1
	 * call to pmap_enter with everything locked.   bad?
a838 1
		/* locked: maps(read), amap (if there), uobj */
a842 2
		/* locked: nothing, pgo_fault has unlocked everything */

a861 5
		/* locked (!shadowed): maps(read), amap (if there), uobj */
		/*
		 * the following call to pgo_get does _not_ change locking state
		 */

a943 4
	/* locked (shadowed): maps(read), amap */
	/* locked (!shadowed): maps(read), amap(if there), 
		 uobj(if !null), uobjpage(if !null) */

a965 2
	/* locked: maps(read), amap */

a971 2
	/* locked: maps(read), amap, anon */

a978 2
	 * if it fails (!OK) it will unlock everything for us.
	 * if it succeeds, locks are still valid and locked.
a979 2
	 * if the page is on loan from a uvm_object, then anonget will
	 * lock that object for us if it does not fail.
d1010 1
a1010 3
	uobj = anon->an_page->uobject;	/* locked by anonget if !NULL */

	/* locked: maps(read), amap, anon, uobj(if one) */
d1054 1
a1054 2
				 * copy data, kill loan, and drop uobj lock
				 * (if any)
d1097 1
a1097 1
	 * will unlock, wait for more RAM, and refault.    
d1104 1
a1104 1
		oanon = anon;		/* oanon = old, locked anon */
d1139 2
a1140 2
		 * note: oanon still locked.   anon is _not_ locked, but we
		 * have the sole references to in from amap which _is_ locked.
d1147 1
a1147 1
		oanon = anon;		/* old, locked anon is same as anon */
a1153 2
	/* locked: maps(read), amap, oanon */

a1220 5
	 * locked:
	 * maps(read), amap(if there), uobj(if !null), uobjpage(if !null)
	 */

	/*
d1240 1
a1240 1
	 * if uobjpage is null, then we need to unlock and ask the pager to 
a1251 1
		/* locked: maps(read), amap(if there), uobj */
a1252 1
		/* locked: uobj */
a1260 2
		/* locked: uobjpage(if result OK) */

a1275 2
		/* locked: uobjpage */

d1277 1
a1277 2
		 * re-verify the state of the world by first trying to relock
		 * the maps.  always relock the object.
a1281 3
		/* locked(locked): maps(read), amap(if !null), uobj, uobjpage */
		/* locked(!locked): uobj, uobjpage */

d1284 1
a1284 1
		 * a problem, we unlock and clean up.
d1316 1
a1316 2
		 * we have the data in uobjpage which is PG_BUSY and we are
		 * holding object lock.
a1318 1
		/* locked: maps(read), amap(if !null), uobj, uobjpage */
a1321 5
	 * locked:
	 * maps(read), amap(if !null), uobj(if !null), uobjpage(if uobj)
	 */

	/*
a1398 1
				/* uobj still locked */
a1458 1
					/* still holding object lock */
a1504 1
				/* still have the obj lock */
a1525 3
	 * locked:
	 * maps(read), amap(if !null), uobj(if !null), uobjpage(if uobj)
	 *
d1547 1
a1547 1
			wakeup(pg);		/* lock still held */
d1584 1
a1584 1
		wakeup(pg);		/* lock still held */
@


1.65
log
@uvm: keep track of maxrss

The fault path is used to update the maxrss of the faulting proc.
Doesn't affect anything, as it was 0 before.

Requested by espie, "just commit it" deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.64 2012/03/23 15:51:26 guenther Exp $	*/
a201 1
		simple_lock(&anons[lcv]->an_lock);
a214 1
		simple_unlock(&anons[lcv]->an_lock);
a405 2
		if (locked || we_own)
			simple_lock(&anon->an_lock);
a435 1
				simple_unlock(&anon->an_lock);
a470 2
				else
					simple_unlock(&anon->an_lock);
a481 2
			if (!locked)
				simple_unlock(&anon->an_lock);
a761 1
			simple_lock(&uobj->vmobjlock);
a763 1
			simple_unlock(&uobj->vmobjlock);
a813 1
		simple_lock(&anon->an_lock);
a834 1
		simple_unlock(&anon->an_lock);
a857 2
		simple_lock(&uobj->vmobjlock);

a883 2
		simple_lock(&uobj->vmobjlock);

a1003 1
	simple_lock(&anon->an_lock);
a1114 1
					simple_unlock(&uobj->vmobjlock);
a1336 1
		simple_lock(&uobj->vmobjlock);
a1369 1
			simple_unlock(&uobj->vmobjlock);
a1580 1
			simple_unlock(&uobj->vmobjlock);
a1810 4
	if (anon)
		simple_unlock(&anon->an_lock);
	if (uobj)
		simple_unlock(&uobj->vmobjlock);
@


1.64
log
@Make rusage totals, itimers, and profile settings per-process instead
of per-rthread.  Handling of per-thread tick and runtime counters
inspired by how FreeBSD does it.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.63 2012/03/09 13:01:29 ariane Exp $	*/
d181 1
d525 43
d1815 1
@


1.63
log
@New vmmap implementation.

no oks (it is really a pain to review properly)
extensively tested, I'm confident it'll be stable
'now is the time' from several icb inhabitants

Diff provides:
- ability to specify different allocators for different regions/maps
- a simpler implementation of the current allocator
- currently in compatibility mode: it will generate similar addresses
  as the old allocator
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.62 2011/07/03 18:34:14 oga Exp $	*/
d305 1
a305 1
		curproc->p_addr->u_stats.p_ru.ru_minflt++;
d307 1
a307 1
		curproc->p_addr->u_stats.p_ru.ru_majflt++;
d1271 1
a1271 1
		curproc->p_addr->u_stats.p_ru.ru_minflt++;
d1274 1
a1274 1
		curproc->p_addr->u_stats.p_ru.ru_majflt++;
@


1.62
log
@Rip out and burn support for UVM_HIST.

The vm hackers don't use it, don't maintain it and have to look at it all the
time. About time this 800 lines of code hit /dev/null.

``never liked it'' tedu@@. ariane@@ was very happy when i told her i wrote
this diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.61 2011/06/23 21:52:42 oga Exp $	*/
d1704 1
a1704 1
	vm_map_entry_t entry;
d1737 3
a1739 3
			KASSERT(entry->next != &map->header &&
				entry->next->start <= entry->end);
			entry = entry->next;
d1828 3
d1845 1
a1845 1
								&ufi->entry)) {
@


1.61
log
@Replace handrolled version of uvmfault_unlockmaps with uvmfault_unlockmaps.

ok ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.60 2011/06/06 17:10:23 ariane Exp $	*/
a299 1
	UVMHIST_FUNC("uvmfault_anonget"); UVMHIST_CALLED(maphist);
a340 1
				UVMHIST_LOG(maphist, "<- OK",0,0,0,0);
a351 2
				UVMHIST_LOG(maphist, " unlock+wait on uobj",0,
				    0,0,0);
a357 2
				UVMHIST_LOG(maphist, " unlock+wait on anon",0,
				    0,0,0);
a373 2
				UVMHIST_LOG(maphist, "  noram -- UVM_WAIT",0,
				    0,0,0);
a444 1
				UVMHIST_LOG(maphist, "<- REFAULT", 0,0,0,0);
a476 1
				UVMHIST_LOG(maphist, "<- ERROR", 0,0,0,0);
d496 1
a496 2
		if (!locked) {
			UVMHIST_LOG(maphist, "<- REFAULT", 0,0,0,0);
a497 1
		}
a507 1
			UVMHIST_LOG(maphist, "<- REFAULT", 0,0,0,0);
a555 4
	UVMHIST_FUNC("uvm_fault"); UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist, "(map=%p, vaddr=0x%lx, ft=%ld, at=%ld)",
	      orig_map, vaddr, fault_type, access_type);
a584 1
		UVMHIST_LOG(maphist, "<- no mapping @@ 0x%lx", vaddr, 0,0,0);
a599 3
		UVMHIST_LOG(maphist,
		    "<- protection failure (prot=0x%lx, access=0x%lx)",
		    ufi.entry->protection, access_type, 0, 0);
a626 2
			UVMHIST_LOG(maphist,
			    "  need to clear needs_copy and refault",0,0,0,0);
a656 1
		UVMHIST_LOG(maphist,"<- no backing store, no overlay",0,0,0,0);
a697 4
	UVMHIST_LOG(maphist, "  narrow=%ld, back=%ld, forw=%ld, startva=0x%lx",
		    narrow, nback, nforw, startva);
	UVMHIST_LOG(maphist, "  entry=%p, amap=%p, obj=%p", ufi.entry,
		    amap, uobj, 0);
a719 2
		UVMHIST_LOG(maphist, "  MADV_SEQUENTIAL: flushing backpages",
		    0,0,0,0);
a787 3
			UVMHIST_LOG(maphist,
			    "  MAPPING: n anon: pm=%p, va=0x%lx, pg=%p",
			    ufi.orig_map->pmap, currva, anon->an_page, 0);
a808 2
	UVMHIST_LOG(maphist, "  shadowed=%ld, will_get=%ld", shadowed, 
	    (uobj && shadowed == FALSE),0,0);
a896 3
					UVMHIST_LOG(maphist, "  got uobjpage "
					    "(%p) with locked get", 
					    uobjpage, 0,0,0);
a911 3
				UVMHIST_LOG(maphist,
				  "  MAPPING: n obj: pm=%p, va=0x%lx, pg=%p",
				  ufi.orig_map->pmap, currva, pages[lcv], 0);
a976 1
	UVMHIST_LOG(maphist, "  case 1 fault: anon=%p", anon, 0,0,0);
a1119 2

		UVMHIST_LOG(maphist, "  case 1B: COW fault",0,0,0,0);
a1133 2
				UVMHIST_LOG(maphist,
				    "<- failed.  out of VM",0,0,0,0);
a1179 2
	UVMHIST_LOG(maphist, "  MAPPING: anon: pm=%p, va=0x%lx, pg=%p",
	    ufi.orig_map->pmap, ufi.orig_rvaddr, pg, 0);
a1192 2
			UVMHIST_LOG(maphist,
			    "<- failed.  out of VM",0,0,0,0);
a1258 2
	UVMHIST_LOG(maphist, "  case 2 fault: promote=%ld, zfill=%ld",
	    promote, (uobj == NULL), 0,0);
a1296 2
				UVMHIST_LOG(maphist,
				    "  pgo_get says TRY AGAIN!",0,0,0,0);
a1300 2
			UVMHIST_LOG(maphist, "<- pgo_get failed (code %ld)",
			    result, 0,0,0);
a1333 4

			UVMHIST_LOG(maphist,
			    "  wasn't able to relock after fault: retry", 
			    0,0,0,0);
a1422 3
					UVMHIST_LOG(maphist,
					  "  out of RAM breaking loan, waiting",
					  0,0,0,0);
a1517 2
				UVMHIST_LOG(maphist, "  promote: out of VM",
				    0,0,0,0);
a1521 2
			UVMHIST_LOG(maphist, "  out of RAM, waiting for more",
			    0,0,0,0);
a1559 5

			UVMHIST_LOG(maphist,
			    "  promote uobjpage %p to anon/page %p/%p",
			    uobjpage, anon, pg, 0);

a1565 2
			UVMHIST_LOG(maphist,"  zero fill anon/page %p/%p",
			    anon, pg, 0, 0);
a1583 3
	UVMHIST_LOG(maphist,
	    "  MAPPING: case2: pm=%p, va=0x%lx, pg=%p, promote=%ld",
	    ufi.orig_map->pmap, ufi.orig_rvaddr, pg, promote);
a1603 2
			UVMHIST_LOG(maphist,
			    "<- failed.  out of VM",0,0,0,0);
a1640 1
	UVMHIST_LOG(maphist, "<- done (SUCCESS!)",0,0,0,0);
@


1.60
log
@Backout vmmap in order to repair virtual address selection algorithms
outside the tree.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.59 2011/05/24 15:27:36 ariane Exp $	*/
d1939 1
a1939 5
			if (write_lock) {
				vm_map_unlock(ufi->map);
			} else {
				vm_map_unlock_read(ufi->map);
			}
@


1.59
log
@Reimplement uvm/uvm_map.

vmmap is designed to perform address space randomized allocations,
without letting fragmentation of the address space go through the roof.

Some highlights:
- kernel address space randomization
- proper implementation of guardpages
- roughly 10% system time reduction during kernel build

Tested by alot of people on tech@@ and developers.
Theo's machines are still happy.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.58 2009/07/22 21:05:37 oga Exp $	*/
d1784 1
a1784 1
	vm_map_entry_t entry, next;
d1817 3
a1819 3
			next = RB_NEXT(uvm_map_addr, &map->addr, entry);
			KASSERT(next != NULL && next->start <= entry->end);
			entry = next;
d1908 1
d1922 1
a1922 1
		    &ufi->entry)) {
@


1.58
log
@Put the PG_RELEASED changes diff back in.

This has has been tested very very thoroughly on all archs we have
excepting 88k and 68k. Please see cvs log for the individual commit
messages.

ok beck@@, thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.57 2009/06/16 23:54:58 oga Exp $	*/
d1784 1
a1784 1
	vm_map_entry_t entry;
d1817 3
a1819 3
			KASSERT(entry->next != &map->header &&
				entry->next->start <= entry->end);
			entry = entry->next;
a1907 1

d1921 1
a1921 1
								&ufi->entry)) {
@


1.57
log
@date based reversion of uvm to the 4th May.

We still have no idea why this stops the crashes. but it does.

a machine forced to 64mb of ram cycled 10GB through swap with this diff
and is still running as I type this. Other tests by ariane@@ and thib@@
also seem to show that it's alright.

ok deraadt@@, thib@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.52 2009/03/25 20:00:18 oga Exp $	*/
d924 4
a927 4
				 * PG_BUSY|PG_RELEASED then pgo_get
				 * made it PG_BUSY for us and gave
				 * us a handle to it.   remember this
				 * page as "uobjpage." (for later use).
d969 2
a970 2
				 * NOTE: page can't be PG_WANTED or PG_RELEASED
				 * because we've held the lock the whole time
d1374 2
a1375 3
		 * verify that the page has not be released and re-verify
		 * that amap slot is still free.   if there is a problem,
		 * we unlock and clean up.
d1378 2
a1379 4
		if ((uobjpage->pg_flags & PG_RELEASED) != 0 ||
		    (locked && amap && 
		    amap_lookup(&ufi.entry->aref,
		      ufi.orig_rvaddr - ufi.entry->start))) {
a1397 11
			if (uobjpage->pg_flags & PG_RELEASED) {
				uvmexp.fltpgrele++;
				KASSERT(uobj->pgops->pgo_releasepg != NULL);

				/* frees page */
				if (uobj->pgops->pgo_releasepg(uobjpage,NULL))
					/* unlock if still alive */
					simple_unlock(&uobj->vmobjlock);
				goto ReFault;
			}

d1412 2
a1413 3
		 * we have the data in uobjpage which is PG_BUSY and
		 * !PG_RELEASED.  we are holding object lock (so the page
		 * can't be released on us).
a1426 2
	 *  - at this point uobjpage can not be PG_RELEASED (since we checked
	 *  for it above)
d1613 1
a1613 3
			 * dispose of uobjpage.  it can't be PG_RELEASED
			 * since we still hold the object lock.
			 * drop handle to uobj as well.
a1675 5
		/* 
		 * note that pg can't be PG_RELEASED since we did not drop
		 * the object lock since the last time we checked.
		 */
 
a1714 5
	/* 
	 * note that pg can't be PG_RELEASED since we did not drop the object 
	 * lock since the last time we checked.
	 */
 
@


1.56
log
@Backout all changes to uvm after pmemrange (which will be backed out
separately).

a change at or just before the hackathon has either exposed or added a
very very nasty memory corruption bug that is giving us hell right now.
So in the interest of kernel stability these diffs are being backed out
until such a time as that corruption bug has been found and squashed,
then the ones that are proven good may slowly return.

a quick hitlist of the main commits this backs out:

mine:
uvm_objwire
the lock change in uvm_swap.c
using trees for uvm objects instead of the hash
removing the pgo_releasepg callback.

art@@'s:
putting pmap_page_protect(VM_PROT_NONE) in uvm_pagedeactivate() since
all callers called that just prior anyway.

ok beck@@, ariane@@.

prompted by deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.53 2009/05/08 13:50:15 ariane Exp $	*/
d178 1
a178 1
void uvmfault_amapcopy(struct uvm_faultinfo *);
d231 1
a231 1
void
@


1.55
log
@Since all callers of uvm_pagedeactivate did pmap_page_protect(.., VM_PROT_NONE)
just move that into uvm_pagedeactivate.

oga@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.54 2009/06/01 19:54:02 oga Exp $	*/
d206 5
d924 4
a927 4
				 * PG_BUSY, then pgo_get made it PG_BUSY
				 * for us and gave us a handle to it.
				 * remember this page as "uobjpage."
				 * (for later use).
d969 2
a970 2
				 * NOTE: page can't be PG_WANTED because
				 * we've held the lock the whole time
d1374 3
a1376 2
		 * Re-verify that amap slot is still free. if there is
		 * a problem, we unlock and clean up.
d1379 4
a1382 2
		if (locked && amap && amap_lookup(&ufi.entry->aref,
		      ufi.orig_rvaddr - ufi.entry->start)) {
d1401 11
d1426 3
a1428 2
		 * we have the data in uobjpage which is PG_BUSY and we are
		 * holding object lock.
d1442 2
d1630 3
a1632 1
			 * dispose of uobjpage. drop handle to uobj as well.
d1695 5
d1739 5
@


1.54
log
@Since we've now cleared up a lot of the PG_RELEASED setting, remove the
pgo_releasepg() hook and just free the page the "normal" way in the one
place we'll ever see PG_RELEASED and should care (uvm_page_unbusy,
called in aiodoned).

ok art@@, beck@@, thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.53 2009/05/08 13:50:15 ariane Exp $	*/
a205 5
#ifdef UBC
				pmap_clear_reference(pg);
#else
				pmap_page_protect(pg, VM_PROT_NONE);
#endif
@


1.53
log
@Remove static qualifier of functions that are not inline.
Makes trace in ddb useful.

ok oga
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.52 2009/03/25 20:00:18 oga Exp $	*/
d924 4
a927 4
				 * PG_BUSY|PG_RELEASED then pgo_get
				 * made it PG_BUSY for us and gave
				 * us a handle to it.   remember this
				 * page as "uobjpage." (for later use).
d969 2
a970 2
				 * NOTE: page can't be PG_WANTED or PG_RELEASED
				 * because we've held the lock the whole time
d1374 2
a1375 3
		 * verify that the page has not be released and re-verify
		 * that amap slot is still free.   if there is a problem,
		 * we unlock and clean up.
d1378 2
a1379 4
		if ((uobjpage->pg_flags & PG_RELEASED) != 0 ||
		    (locked && amap && 
		    amap_lookup(&ufi.entry->aref,
		      ufi.orig_rvaddr - ufi.entry->start))) {
a1397 11
			if (uobjpage->pg_flags & PG_RELEASED) {
				uvmexp.fltpgrele++;
				KASSERT(uobj->pgops->pgo_releasepg != NULL);

				/* frees page */
				if (uobj->pgops->pgo_releasepg(uobjpage,NULL))
					/* unlock if still alive */
					simple_unlock(&uobj->vmobjlock);
				goto ReFault;
			}

d1412 2
a1413 3
		 * we have the data in uobjpage which is PG_BUSY and
		 * !PG_RELEASED.  we are holding object lock (so the page
		 * can't be released on us).
a1426 2
	 *  - at this point uobjpage can not be PG_RELEASED (since we checked
	 *  for it above)
d1613 1
a1613 3
			 * dispose of uobjpage.  it can't be PG_RELEASED
			 * since we still hold the object lock.
			 * drop handle to uobj as well.
a1675 5
		/* 
		 * note that pg can't be PG_RELEASED since we did not drop
		 * the object lock since the last time we checked.
		 */
 
a1714 5
	/* 
	 * note that pg can't be PG_RELEASED since we did not drop the object 
	 * lock since the last time we checked.
	 */
 
@


1.52
log
@Move all of the pseudo-inline functions in uvm into C files.

By pseudo-inline, I mean that if a certain macro was defined, they would
be inlined. However, no architecture defines that, and none has for a
very very long time. Therefore mainly this just makes the code a damned
sight easier to read. Some k&r -> ansi declarations while I'm in there.

"just commit it" art@@. ok weingart@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.51 2009/03/20 15:19:04 oga Exp $	*/
d178 1
a178 1
static void uvmfault_amapcopy(struct uvm_faultinfo *);
d231 1
a231 1
static void
@


1.51
log
@While working on some stuff in uvm I've gotten REALLY sick of reading
K&R function declarations, so switch them all over to ansi-style, in
accordance with the prophesy.

"go for it" art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.50 2008/09/16 18:52:52 chl Exp $	*/
d180 1
d1863 159
@


1.50
log
@remove dead stores and newly created unused variables.

Found by LLVM/Clang Static Analyzer.

ok miod@@ art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.49 2007/06/18 21:51:15 pedro Exp $	*/
d192 1
a192 3
uvmfault_anonflush(anons, n)
	struct vm_anon **anons;
	int n;
d231 1
a231 2
uvmfault_amapcopy(ufi)
	struct uvm_faultinfo *ufi;
d292 2
a293 4
uvmfault_anonget(ufi, amap, anon)
	struct uvm_faultinfo *ufi;
	struct vm_amap *amap;
	struct vm_anon *anon;
d554 2
a555 5
uvm_fault(orig_map, vaddr, fault_type, access_type)
	vm_map_t orig_map;
	vaddr_t vaddr;
	vm_fault_t fault_type;
	vm_prot_t access_type;
d1763 1
a1763 4
uvm_fault_wire(map, start, end, access_type)
	vm_map_t map;
	vaddr_t start, end;
	vm_prot_t access_type;
d1795 1
a1795 3
uvm_fault_unwire(map, start, end)
	vm_map_t map;
	vaddr_t start, end;
d1810 1
a1810 3
uvm_fault_unwire_locked(map, start, end)
	vm_map_t map;
	vaddr_t start, end;
@


1.49
log
@Bring back Mickey's UVM anon change. Testing by thib@@, beck@@ and
ckuethe@@ for a while. Okay beck@@, "it is good timing" deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.48 2007/05/31 21:20:30 thib Exp $	*/
d775 1
a775 1
		nback = centeridx = 0;
@


1.48
log
@zap the vm_amap am_l simplelock, and amap_{lock/unlock} macros for
simple_{lock/unlock}.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.47 2007/04/15 11:15:08 art Exp $	*/
d203 1
a203 1
		pg = anons[lcv]->u.an_page;
d309 1
a309 1
	if (anon->u.an_page)
d321 1
a321 1
		pg = anon->u.an_page;
d465 1
a465 1
				anon->u.an_page = NULL;
d820 2
a821 2
		if (anon->u.an_page && anon->u.an_page->loan_count == 0 &&
			(anon->u.an_page->pg_flags & (PG_RELEASED|PG_BUSY)) == 0) {
d823 1
a823 1
			uvm_pageactivate(anon->u.an_page);	/* reactivate */
d827 1
a827 1
			    ufi.orig_map->pmap, currva, anon->u.an_page, 0);
d837 1
a837 1
			    VM_PAGE_TO_PHYS(anon->u.an_page),
d1072 1
a1072 1
	uobj = anon->u.an_page->uobject;	/* locked by anonget if !NULL */
d1080 1
a1080 1
	if (anon->u.an_page->loan_count) {
d1122 1
a1122 1
				uvm_pagecopy(anon->u.an_page, pg);
d1125 1
a1125 1
				pmap_page_protect(anon->u.an_page,
d1130 2
a1131 2
					anon->u.an_page->loan_count--;
				anon->u.an_page->uanon = NULL;
d1134 1
a1134 1
				    &anon->u.an_page->pg_flags, PQ_ANON);
d1143 1
a1143 1
				anon->u.an_page = pg;
d1198 1
a1198 1
		uvm_pagecopy(oanon->u.an_page, pg);	/* pg now !PG_CLEAN */
d1218 1
a1218 1
		pg = anon->u.an_page;
@


1.47
log
@minor indentation.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.46 2007/04/13 18:57:49 art Exp $	*/
a418 3
		if (locked && amap != NULL) {
			amap_lock(amap);
		}
a738 1
		amap_lock(amap);
a1374 2
		if (locked && amap)
			amap_lock(amap);
@


1.46
log
@While splitting flags and pqflags might have been a good idea in theory
to separate locking, on most modern machines this is not enough
since operations on short types touch other short types that share the
same word in memory.

Merge pg_flags and pqflags again and now use atomic operations to change
the flags. Also bump wire_count to an int and pg_version might go
int as well, just for alignment.

tested by many, many. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.45 2007/04/11 12:10:42 art Exp $	*/
d989 1
a989 1
				pmap_update(ufi.orig_map->pmap);
@


1.45
log
@Instead of managing pages for intrsafe maps in special objects (aka.
kmem_object) just so that we can remove them, just use pmap_extract
to get the pages to free and simplify a lot of code to not deal with
the list of intrsafe maps, intrsafe objects, etc.

miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.44 2007/04/04 17:44:45 art Exp $	*/
d349 1
a349 1
			pg->pg_flags |= PG_WANTED;
d443 2
a444 1
			pg->pg_flags &= ~(PG_WANTED|PG_BUSY|PG_FAKE);
d985 2
a986 1
				pages[lcv]->pg_flags &= ~(PG_BUSY); /* un-busy! */
d1137 2
a1138 1
				anon->u.an_page->pqflags &= ~PQ_ANON;
d1149 3
a1151 2
				pg->pqflags |= PQ_ANON;
				pg->pg_flags &= ~(PG_BUSY|PG_FAKE);
d1203 2
a1204 1
		pg->pg_flags &= ~(PG_BUSY|PG_FAKE);	/* un-busy! new page */
d1277 1
a1277 2

		pg->pg_flags &= ~(PG_CLEAN);
d1430 2
a1431 1
			uobjpage->pg_flags &= ~(PG_BUSY|PG_WANTED);
d1501 3
a1503 1
					uobjpage->pg_flags &= ~(PG_BUSY|PG_WANTED);
d1530 2
a1531 1
				pg->pg_flags &= ~(PG_FAKE|PG_CLEAN);
d1536 2
a1537 1
				uobjpage->pg_flags &= ~(PG_WANTED|PG_BUSY);
d1602 2
a1603 1
				uobjpage->pg_flags &= ~(PG_BUSY|PG_WANTED);
d1651 2
a1652 1
			uobjpage->pg_flags &= ~(PG_BUSY|PG_WANTED);
d1713 1
a1713 1
		pg->pg_flags &= ~(PG_BUSY|PG_FAKE|PG_WANTED);
d1732 1
a1732 1
		if (pg->pqflags & PQ_AOBJ) {
d1740 1
a1740 2

			pg->pg_flags &= ~(PG_CLEAN);
d1757 1
a1757 1
	pg->pg_flags &= ~(PG_BUSY|PG_FAKE|PG_WANTED);
@


1.44
log
@Mechanically rename the "flags" and "version" fields in struct vm_page
to "pg_flags" and "pg_version", so that they are a bit easier to work with.
Whoever uses generic names like this for a popular struct obviously doesn't
read much code.

Most architectures compile and there are no functionality changes.

deraadt@@ ok ("if something fails to compile, we fix that by hand")
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.43 2007/03/26 08:43:34 art Exp $	*/
a601 13
	 * before we do anything else, if this is a fault on a kernel
	 * address, check to see if the address is managed by an
	 * interrupt-safe map.  If it is, we fail immediately.  Intrsafe
	 * maps are never pageable, and this approach avoids an evil
	 * locking mess.
	 */
	if (orig_map == kernel_map && uvmfault_check_intrsafe(&ufi)) {
		UVMHIST_LOG(maphist, "<- VA 0x%lx in intrsafe map %p",
		    ufi.orig_rvaddr, ufi.map, 0, 0);
		return (EFAULT);
	}

	/*
d616 6
a631 11
	}

	/*
	 * if the map is not a pageable map, a page fault always fails.
	 */

	if ((ufi.map->flags & VM_MAP_PAGEABLE) == 0) {
		UVMHIST_LOG(maphist,
		    "<- map %p not pageable", ufi.map, 0, 0, 0);
		uvmfault_unlockmaps(&ufi, FALSE);
		return (EFAULT);
@


1.43
log
@Rip out the KERN_ error codes.
ok otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.42 2007/03/25 11:31:07 art Exp $	*/
d204 1
a204 1
		if (pg && (pg->flags & PG_BUSY) == 0 && pg->loan_count == 0) {
d345 1
a345 1
			if ((pg->flags & (PG_BUSY|PG_RELEASED)) == 0) {
d349 1
a349 1
			pg->flags |= PG_WANTED;
d438 1
a438 1
			if (pg->flags & PG_WANTED) {
d443 1
a443 1
			pg->flags &= ~(PG_WANTED|PG_BUSY|PG_FAKE);
d451 1
a451 1
			if (pg->flags & PG_RELEASED) {
d842 1
a842 1
			(anon->u.an_page->flags & (PG_RELEASED|PG_BUSY)) == 0) {
d948 1
a948 1
				KASSERT((pages[lcv]->flags & PG_RELEASED) == 0);
d1002 1
a1002 1
				pages[lcv]->flags &= ~(PG_BUSY); /* un-busy! */
d1165 1
a1165 1
				pg->flags &= ~(PG_BUSY|PG_FAKE);
d1217 1
a1217 1
		pg->flags &= ~(PG_BUSY|PG_FAKE);	/* un-busy! new page */
d1291 1
a1291 1
		pg->flags &= ~(PG_CLEAN);
d1406 1
a1406 1
		if ((uobjpage->flags & PG_RELEASED) != 0 ||
d1424 1
a1424 1
			if (uobjpage->flags & PG_WANTED)
d1428 1
a1428 1
			if (uobjpage->flags & PG_RELEASED) {
d1444 1
a1444 1
			uobjpage->flags &= ~(PG_BUSY|PG_WANTED);
d1512 1
a1512 1
					if (uobjpage->flags & PG_WANTED)
d1514 1
a1514 1
					uobjpage->flags &= ~(PG_BUSY|PG_WANTED);
d1541 1
a1541 1
				pg->flags &= ~(PG_FAKE|PG_CLEAN);
d1543 1
a1543 1
				if (uobjpage->flags & PG_WANTED)
d1546 1
a1546 1
				uobjpage->flags &= ~(PG_WANTED|PG_BUSY);
d1604 1
a1604 1
				if (uobjpage->flags & PG_WANTED)
d1611 1
a1611 1
				uobjpage->flags &= ~(PG_BUSY|PG_WANTED);
d1656 1
a1656 1
			if (uobjpage->flags & PG_WANTED)
d1659 1
a1659 1
			uobjpage->flags &= ~(PG_BUSY|PG_WANTED);
d1712 1
a1712 1
		if (pg->flags & PG_WANTED)
d1720 1
a1720 1
		pg->flags &= ~(PG_BUSY|PG_FAKE|PG_WANTED);
d1748 1
a1748 1
			pg->flags &= ~(PG_CLEAN);
d1757 1
a1757 1
	if (pg->flags & PG_WANTED)
d1765 1
a1765 1
	pg->flags &= ~(PG_BUSY|PG_FAKE|PG_WANTED);
@


1.42
log
@remove KERN_SUCCESS and use 0 instead.
eyeballed by miod@@ and pedro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.41 2006/07/31 11:51:29 mickey Exp $	*/
d611 1
a611 1
		return (KERN_FAILURE);
d625 1
a625 1
		return (KERN_INVALID_ADDRESS);
d638 1
a638 1
		return (KERN_PROTECTION_FAILURE);
d649 1
a649 1
		return (KERN_FAILURE);
d708 1
a708 1
		return (KERN_INVALID_ADDRESS);
d904 1
a904 1
			return (KERN_PROTECTION_FAILURE);
d1078 1
a1078 1
		return (KERN_PROTECTION_FAILURE);	/* XXX */
d1084 1
a1084 1
		return (KERN_PROTECTION_FAILURE);
d1206 1
a1206 1
				return (KERN_RESOURCE_SHORTAGE);
d1268 1
a1268 1
			return (KERN_RESOURCE_SHORTAGE);
d1382 1
a1382 1
			return (KERN_PROTECTION_FAILURE); /* XXX i/o error */
d1622 1
a1622 1
				return (KERN_RESOURCE_SHORTAGE);
d1728 1
a1728 1
			return (KERN_RESOURCE_SHORTAGE);
@


1.41
log
@fix uvmhist #2: args are always u_long so fix missing %d and %x and no %ll; no change for normal code
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.40 2006/07/26 23:15:55 mickey Exp $	*/
d900 1
a900 1
			return (KERN_SUCCESS);	/* pgo_fault did pmap enter */
d1254 1
a1254 1
	    != KERN_SUCCESS) {
d1306 1
a1306 1
	return (KERN_SUCCESS);
d1702 1
a1702 1
	    != KERN_SUCCESS) {
d1771 1
a1771 1
	return (KERN_SUCCESS);
d1812 1
a1812 1
	return (KERN_SUCCESS);
@


1.40
log
@fix fmts for UVMHIST_LOG() entries making it more useful on 64bit archs; miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.39 2006/07/13 22:51:26 deraadt Exp $	*/
d580 1
a580 1
	UVMHIST_LOG(maphist, "(map=%p, vaddr=0x%lx, ft=%d, at=%d)",
d635 1
a635 1
		    "<- protection failure (prot=0x%x, access=0x%x)",
d749 1
a749 1
	UVMHIST_LOG(maphist, "  narrow=%d, back=%d, forw=%d, startva=0x%lx",
d870 1
a870 1
	UVMHIST_LOG(maphist, "  shadowed=%d, will_get=%d", shadowed, 
d1334 1
a1334 1
	UVMHIST_LOG(maphist, "  case 2 fault: promote=%d, zfill=%d",
d1380 1
a1380 1
			UVMHIST_LOG(maphist, "<- pgo_get failed (code %d)",
d1698 1
a1698 1
	    "  MAPPING: case2: pm=%p, va=0x%lx, pg=%p, promote=%d",
@


1.39
log
@Back out the anon change.  Apparently it was tested by a few, but most of
us did not see it or get a chance to test it before it was commited. It
broke cvs, in the ami driver, making it not succeed at seeing it's devices.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.37 2006/03/06 14:27:29 mickey Exp $	*/
d580 1
a580 1
	UVMHIST_LOG(maphist, "(map=0x%x, vaddr=0x%x, ft=%d, at=%d)",
d624 1
a624 1
		UVMHIST_LOG(maphist, "<- no mapping @@ 0x%x", vaddr, 0,0,0);
d749 1
a749 1
	UVMHIST_LOG(maphist, "  narrow=%d, back=%d, forw=%d, startva=0x%x",
d751 1
a751 1
	UVMHIST_LOG(maphist, "  entry=0x%x, amap=0x%x, obj=0x%x", ufi.entry,
d847 1
a847 1
			    "  MAPPING: n anon: pm=0x%x, va=0x%x, pg=0x%x",
d961 1
a961 1
					    "(0x%x) with locked get", 
d979 1
a979 1
				  "  MAPPING: n obj: pm=0x%x, va=0x%x, pg=0x%x",
d1045 1
a1045 1
	UVMHIST_LOG(maphist, "  case 1 fault: anon=0x%x", anon, 0,0,0);
d1250 1
a1250 1
	UVMHIST_LOG(maphist, "  MAPPING: anon: pm=0x%x, va=0x%x, pg=0x%x",
d1668 1
a1668 1
			    "  promote uobjpage 0x%x to anon/page 0x%x/0x%x",
d1677 1
a1677 1
			UVMHIST_LOG(maphist,"  zero fill anon/page 0x%x/0%x",
d1698 1
a1698 1
	    "  MAPPING: case2: pm=0x%x, va=0x%x, pg=0x%x, promote=%d",
@


1.38
log
@from netbsd: make anons dynamically allocated from pool.
this results in lesse kva waste due to static preallocation of those
for every phys page and also every swap page.
tested by beck krw miod
@
text
@d203 1
a203 1
		pg = anons[lcv]->an_page;
d309 1
a309 1
	if (anon->an_page)
d321 1
a321 1
		pg = anon->an_page;
d467 1
a467 1
				anon->an_page = NULL;
d841 2
a842 2
		if (anon->an_page && anon->an_page->loan_count == 0 &&
			(anon->an_page->flags & (PG_RELEASED|PG_BUSY)) == 0) {
d844 1
a844 1
			uvm_pageactivate(anon->an_page);	/* reactivate */
d848 1
a848 1
			    ufi.orig_map->pmap, currva, anon->an_page, 0);
d858 1
a858 1
			    VM_PAGE_TO_PHYS(anon->an_page),
d1092 1
a1092 1
	uobj = anon->an_page->uobject;	/* locked by anonget if !NULL */
d1100 1
a1100 1
	if (anon->an_page->loan_count) {
d1142 1
a1142 1
				uvm_pagecopy(anon->an_page, pg);
d1145 1
a1145 1
				pmap_page_protect(anon->an_page,
d1150 2
a1151 2
					anon->an_page->loan_count--;
				anon->an_page->uanon = NULL;
d1153 1
a1153 1
				anon->an_page->pqflags &= ~PQ_ANON;
d1162 1
a1162 1
				anon->an_page = pg;
d1216 1
a1216 1
		uvm_pagecopy(oanon->an_page, pg);	/* pg now !PG_CLEAN */
d1235 1
a1235 1
		pg = anon->an_page;
@


1.37
log
@do not panic unwiring unmapped memory (mmap3 regress); from netbsd; kettenis@@ tedu@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.36 2005/11/29 05:37:14 tedu Exp $	*/
d203 1
a203 1
		pg = anons[lcv]->u.an_page;
d309 1
a309 1
	if (anon->u.an_page)
d321 1
a321 1
		pg = anon->u.an_page;
d467 1
a467 1
				anon->u.an_page = NULL;
d841 2
a842 2
		if (anon->u.an_page && anon->u.an_page->loan_count == 0 &&
			(anon->u.an_page->flags & (PG_RELEASED|PG_BUSY)) == 0) {
d844 1
a844 1
			uvm_pageactivate(anon->u.an_page);	/* reactivate */
d848 1
a848 1
			    ufi.orig_map->pmap, currva, anon->u.an_page, 0);
d858 1
a858 1
			    VM_PAGE_TO_PHYS(anon->u.an_page),
d1092 1
a1092 1
	uobj = anon->u.an_page->uobject;	/* locked by anonget if !NULL */
d1100 1
a1100 1
	if (anon->u.an_page->loan_count) {
d1142 1
a1142 1
				uvm_pagecopy(anon->u.an_page, pg);
d1145 1
a1145 1
				pmap_page_protect(anon->u.an_page,
d1150 2
a1151 2
					anon->u.an_page->loan_count--;
				anon->u.an_page->uanon = NULL;
d1153 1
a1153 1
				anon->u.an_page->pqflags &= ~PQ_ANON;
d1162 1
a1162 1
				anon->u.an_page = pg;
d1216 1
a1216 1
		uvm_pagecopy(oanon->u.an_page, pg);	/* pg now !PG_CLEAN */
d1235 1
a1235 1
		pg = anon->u.an_page;
@


1.36
log
@apply patch from david hill for two netbsd prs:
14060 skip MADV_SEQUENTIAL if refaulting
18037 missing pageactivate
tested for some time by jolan krw
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.35 2005/10/23 01:42:22 pedro Exp $	*/
d1867 1
a1867 2
			panic("uvm_fault_unwire_locked: unwiring "
			    "non-wired memory");
d1870 1
a1870 2
		 * make sure the current entry is for the address we're
		 * dealing with.  if not, grab the next entry.
a1871 1

d1873 1
a1873 1
		if (va >= entry->end) {
@


1.35
log
@typo
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.34 2005/05/03 11:52:35 mickey Exp $	*/
d774 1
a774 1
	if (ufi.entry->advice == MADV_SEQUENTIAL) {
d1154 1
@


1.34
log
@repair file mmapings above 4g; found by chefren; art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.33 2004/08/03 12:10:48 todd Exp $	*/
d552 1
a552 1
 *	gets a write lock, sets it recusive, and then calls us (c.f.
@


1.33
log
@the rest of the '#if DIAGNOSTIC' -> '#ifdef DIAGNOSTIC' in the kernel; ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.32 2004/02/23 06:19:32 drahn Exp $	*/
d571 2
a572 1
	vaddr_t startva, objaddr, currva, offset, uoff;
d784 1
a784 2
			objaddr =
			    (startva - ufi.entry->start) + ufi.entry->offset;
d786 1
a786 1
			(void) uobj->pgops->pgo_flush(uobj, objaddr, objaddr + 
d1549 1
a1549 1
				offset = uobjpage->offset;
d1558 1
a1558 1
				uvm_pagerealloc(pg, uobj, offset);
@


1.32
log
@sync of pmap_update() calls with NetBSD. pmap_update is defined away on
all architectures but arm, where it is needed.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.31 2002/03/14 01:27:18 millert Exp $	*/
d1577 1
a1577 1
#if DIAGNOSTIC
@


1.31
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.30 2002/01/02 22:23:25 miod Exp $	*/
d865 1
d1005 1
d1304 1
d1767 1
@


1.30
log
@Back out a few more uvm changes, especially wrt swap usage.
This unbreaks m68k m88k sparc and perhaps others, which eventually froze
when hitting swap.
Tested by various people on various platforms.
ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.23 2001/11/09 04:34:27 art Exp $	*/
d178 2
a179 2
static void uvmfault_amapcopy __P((struct uvm_faultinfo *));
static __inline void uvmfault_anonflush __P((struct vm_anon **, int));
@


1.29
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_fault.c,v 1.24 2001/11/12 01:26:09 art Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.56 2001/02/18 21:19:08 chs Exp $	*/
a305 2
	LOCK_ASSERT(simple_lock_held(&anon->an_lock));

d894 1
a894 1
				    PGO_LOCKED|PGO_SYNCIO);
d991 1
a991 2
				    pages[lcv]->flags & PG_RDONLY ?
				    VM_PROT_READ : enter_prot & MASK(ufi.entry),
d1070 7
a1076 3
	case VM_PAGER_AGAIN:
		tsleep(&lbolt, PVM, "fltagain1", 0);
		goto ReFault;
a1189 1
			/* new anon is locked! */
d1195 1
a1195 3
			if (anon) {
				anon->an_ref--;
				simple_unlock(&anon->an_lock);
a1196 1
			}
d1223 3
a1225 3
		 * note: oanon is still locked, as is the new anon.  we
		 * need to check for this later when we unlock oanon; if
		 * oanon != anon, we'll have to unlock anon, too.
d1238 1
a1238 1
	/* locked: maps(read), amap, oanon, anon (if different from oanon) */
a1258 2
		if (anon != oanon)
			simple_unlock(&anon->an_lock);
a1300 2
	if (anon != oanon)
		simple_unlock(&anon->an_lock);
a1479 3
		/* no anon in this case. */
		anon = NULL;

a1581 2
			 * The new anon is locked.
			 *
a1595 6
			if (anon != NULL) {
				anon->an_ref--;
				simple_unlock(&anon->an_lock);
				uvm_anfree(anon);
			}

d1623 1
d1683 1
a1683 2
	 * maps(read), amap(if !null), uobj(if !null), uobjpage(if uobj),
	 *   anon(if !null), pg(if anon)
a1695 1
	KASSERT(access_type == VM_PROT_READ || (pg->flags & PG_RDONLY) == 0);
d1697 1
a1697 2
	    pg->flags & PG_RDONLY ? VM_PROT_READ : enter_prot,
	    access_type | PMAP_CANFAIL | (wired ? PMAP_WIRED : 0))
d1718 1
a1718 1
		uvmfault_unlockall(&ufi, amap, uobj, anon);
d1763 1
a1763 1
	uvmfault_unlockall(&ufi, amap, uobj, anon);
@


1.28
log
@Yet another sync to NetBSD uvm.
Today we add a pmap argument to pmap_update() and allocate map entries for
kernel_map from kmem_map instead of using the static entries. This should
get rid of MAX_KMAPENT panics. Also some uvm_loan problems are fixed.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_fault.c,v 1.27 2001/11/28 19:28:14 art Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.68 2001/09/10 21:19:42 chris Exp $	*/
d62 1
a62 1
 *         |             |                         |        |
d72 1
a72 1
 *
d86 1
a86 1
 *		data is "promoted" from uobj to a new anon.
d91 1
a91 1
 * MAPS => AMAP => UOBJ => ANON => PAGE QUEUES (PQ)
d96 1
a96 1
 *
d128 1
a128 1
 *    of the information that we already have from our previous lookup,
d162 1
a162 1
 * note: index in array must match "advice" value
d198 1
a198 1

d207 1
d209 3
d255 1
a255 1
			amap_copy(ufi->map, ufi->entry, M_NOWAIT, TRUE,
d271 1
a271 1

d283 1
a283 1
 * => if we fail (result != 0) we unlock everything.
d303 1
a303 1
	int error;
d308 1
a308 1
	error = 0;
d316 1
a316 1
	/*
d349 1
a349 1
				return (0);
d376 1
a376 1

d392 1
a392 1

d394 1
a394 1
				we_own = TRUE;
d405 1
a405 1
				error = uvm_swap_get(pg, anon->an_swslot,
d432 1
a432 1
		 *   [2] I/O not OK.   free the page and cause the fault
d437 1
a437 1

d442 1
a442 1
				wakeup(pg);
d448 1
a448 1
			/*
d462 1
a462 1
				return (ERESTART);	/* refault! */
d465 3
a467 1
			if (error) {
d495 1
a495 1
				return error;
d497 1
a497 1

d516 1
a516 1
			return (ERESTART);
d524 1
a524 1
		    amap_lookup(&ufi->entry->aref,
d526 1
a526 1

d529 1
a529 1
			return (ERESTART);
d531 1
a531 1

d533 1
a533 1
		 * try it again!
d552 1
a552 1
 * => VM data structures usually should be unlocked.   however, it is
a556 1
 * => MUST NEVER BE CALLED IN INTERRUPT CONTEXT
d564 1
a564 1
	struct vm_map *orig_map;
d572 1
a572 1
	int npages, nback, nforw, centeridx, error, lcv, gotpages;
d574 1
a574 1
	paddr_t pa;
d603 13
d626 1
a626 1
		return (EFAULT);
a629 10
#ifdef DIAGNOSTIC
	if ((ufi.map->flags & VM_MAP_PAGEABLE) == 0) {
		printf("Page fault on non-pageable map:\n");
		printf("ufi.map = %p\n", ufi.map);
		printf("ufi.orig_map = %p\n", ufi.orig_map);
		printf("ufi.orig_rvaddr = 0x%lx\n", (u_long) ufi.orig_rvaddr);
		panic("uvm_fault: (ufi.map->flags & VM_MAP_PAGEABLE) == 0");
	}
#endif

d639 12
a650 1
		return EACCES;
d689 1
a689 1
			enter_prot &= ~VM_PROT_WRITE;
d709 1
a709 1
		return (EFAULT);
d715 1
a715 1
	 * to do this the first time through the fault.   if we
d740 1
a740 1

d780 1
a780 1
		if (amap)
d788 1
a788 1
			(void) uobj->pgops->pgo_flush(uobj, objaddr, objaddr +
a866 1
		pmap_update(ufi.orig_map->pmap);
d871 1
a871 1
	UVMHIST_LOG(maphist, "  shadowed=%d, will_get=%d", shadowed,
d881 1
a881 1

d894 3
a896 2
		error = uobj->pgops->pgo_fault(&ufi, startva, pages, npages,
		    centeridx, fault_type, access_type, PGO_LOCKED|PGO_SYNCIO);
d900 3
a902 1
		if (error == ERESTART)
d904 2
a905 4
		/*
		 * object fault routine responsible for pmap_update().
		 */
		return error;
d958 1
a958 1

d962 1
a962 1
					    "(0x%x) with locked get",
d966 2
a967 2

				/*
d998 1
a998 1
				/*
a1006 1
			pmap_update(ufi.orig_map->pmap);
d1014 1
a1014 1
	/* locked (!shadowed): maps(read), amap(if there),
d1036 1
a1036 1
	if (shadowed == FALSE)
d1065 4
a1068 4
	error = uvmfault_anonget(&ufi, amap, anon);
	switch (error) {
	case 0:
		break;
d1070 1
a1070 1
	case ERESTART:
d1073 1
a1073 1
	case EAGAIN:
d1078 5
a1082 1
		return error;
d1094 1
a1094 1
	 * special handling for loaned pages
d1100 1
a1100 1

d1176 2
a1177 2
	 * in the (hopefully very rare) case that we are out of RAM we
	 * will unlock, wait for more RAM, and refault.
d1206 1
a1206 1
				return ENOMEM;
d1254 1
a1254 1
	    != 0) {
d1270 1
a1270 1
			return ENOMEM;
d1309 1
a1309 2
	pmap_update(ufi.orig_map->pmap);
	return 0;
d1330 1
a1330 1
		uobjpage = PGO_DONTCARE;
d1344 1
a1344 1
	 * if uobjpage is null, then we need to unlock and ask the pager to
d1355 1
a1355 1

d1363 1
a1363 1
		error = uobj->pgops->pgo_get(uobj, uoff, &uobjpage, &gotpages,
d1367 1
a1367 1
		/* locked: uobjpage(if no error) */
d1373 4
a1376 2
		if (error) {
			if (error == EAGAIN) {
d1379 1
a1379 1
				tsleep(&lbolt, PVM, "fltagain2", 0);
d1384 2
a1385 2
			    error, 0,0,0);
			return error;
d1399 1
a1399 1

d1410 1
a1410 1
		    (locked && amap &&
d1413 1
a1413 1
			if (locked)
d1425 1
a1425 1
			    "  wasn't able to relock after fault: retry",
d1475 1
a1475 1

d1579 1
a1579 1

d1636 1
a1636 1
				return ENOMEM;
a1660 3
				/*
				 * XXX: PAGE MIGHT BE WIRED!
				 */
d1662 1
a1662 1

d1717 2
a1718 1
	    access_type | PMAP_CANFAIL | (wired ? PMAP_WIRED : 0)) != 0) {
d1731 1
a1731 1
		/*
d1735 1
a1735 1

d1744 1
a1744 1
			return ENOMEM;
d1776 2
a1777 2
	/*
	 * note that pg can't be PG_RELEASED since we did not drop the object
d1780 1
a1780 1

a1784 2
	pmap_update(ufi.orig_map->pmap);

d1786 1
a1786 1
	return 0;
d1801 1
a1801 1
	struct vm_map *map;
d1806 4
a1809 1
	int error;
d1813 1
a1813 1
	 * to undo what we have done.   note that in uvm_fault VM_PROT_NONE
a1816 8
	/*
	 * XXX work around overflowing a vaddr_t.  this prevents us from
	 * wiring the last page in the address space, though.
	 */
	if (start > end) {
		return EFAULT;
	}

d1818 2
a1819 2
		error = uvm_fault(map, va, VM_FAULT_WIRE, access_type);
		if (error) {
d1823 1
a1823 1
			return error;
d1826 2
a1827 1
	return 0;
d1836 1
a1836 1
	struct vm_map *map;
d1853 1
a1853 1
	struct vm_map *map;
d1856 1
a1856 1
	struct vm_map_entry *entry;
@


1.28.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.30 2002/01/02 22:23:25 miod Exp $	*/
d974 2
a975 1
				    enter_prot & MASK(ufi.entry),
d1167 1
d1173 3
a1175 1
			if (anon)
d1177 1
d1204 3
a1206 3
		 * note: oanon still locked.   anon is _not_ locked, but we
		 * have the sole references to in from amap which _is_ locked.
		 * thus, no one can get at it until we are done with it.
d1219 1
a1219 1
	/* locked: maps(read), amap, oanon */
d1240 2
d1284 2
d1464 3
d1569 2
d1585 6
a1617 1
			uvm_anfree(anon);
d1680 2
a1681 1
	 * maps(read), amap(if !null), uobj(if !null), uobjpage(if uobj)
d1694 1
d1698 1
d1717 1
a1717 1
		uvmfault_unlockall(&ufi, amap, uobj, NULL);
d1762 1
a1762 1
	uvmfault_unlockall(&ufi, amap, uobj, NULL);
@


1.28.2.2
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_fault.c,v 1.28.2.1 2002/01/31 22:55:51 niklas Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.71 2001/11/10 07:36:59 lukem Exp $	*/
d172 1
a172 1
#define UVM_MAXRANGE 16	/* must be MAX() of nback+nforw+1 */
d232 6
a237 1
	for (;;) {
d316 2
a317 1
	for (;;) {
d343 1
a343 1
			if ((pg->flags & PG_BUSY) == 0) {
a353 1

d369 2
d376 1
a377 1
			pg = uvm_pagealloc(NULL, 0, anon, 0);
d379 1
d385 2
d388 1
d408 1
d435 1
d437 1
d440 21
a470 1

a478 1

d493 2
a494 1
			 * we've successfully read the page, activate it.
d496 1
a496 1

a499 2
			pg->flags &= ~(PG_WANTED|PG_BUSY|PG_FAKE);
			UVM_PAGE_OWN(pg, NULL);
d532 3
a534 1
	}
d705 1
a705 1
		nback = MIN(uvmadvice[ufi.entry->advice].nback,
d708 1
a708 1
		nforw = MIN(uvmadvice[ufi.entry->advice].nforw,
d756 1
a756 1
	if (ufi.entry->advice == MADV_SEQUENTIAL && nback != 0) {
d769 1
a769 1
			(void) (uobj->pgops->pgo_put)(uobj, objaddr, objaddr +
d771 1
d825 1
a825 1
		    (anon->u.an_page->flags & PG_BUSY) == 0) {
d827 1
a827 1
			uvm_pageactivate(anon->u.an_page);
d895 2
d923 3
a925 2
			for (lcv = 0; lcv < npages;
			     lcv++, currva += PAGE_SIZE) {
d927 1
a927 1
				    pages[lcv] == PGO_DONTCARE) {
d929 2
a930 1
				}
d949 6
a954 4
				 * calling pgo_get with PGO_LOCKED returns us
				 * pages which are neither busy nor released,
				 * so we don't need to check for this.
				 * we can just directly enter the pages.
d958 1
a958 1
				uvm_pageactivate(pages[lcv]);
d974 1
a974 2
				    pages[lcv]->flags & PG_RDONLY ?
				    VM_PROT_READ : enter_prot & MASK(ufi.entry),
d984 1
a984 1
				pages[lcv]->flags &= ~(PG_BUSY);
d986 1
a986 1
			}
d988 2
a989 1
		}
a1165 1
			/* new anon is locked! */
d1171 1
a1171 3
			if (anon) {
				anon->an_ref--;
				simple_unlock(&anon->an_lock);
a1172 1
			}
d1188 3
a1190 3
		uvm_pagecopy(oanon->u.an_page, pg);
		uvm_pageactivate(pg);
		pg->flags &= ~(PG_BUSY|PG_FAKE);
d1199 3
a1201 3
		 * note: oanon is still locked, as is the new anon.  we
		 * need to check for this later when we unlock oanon; if
		 * oanon != anon, we'll have to unlock anon, too.
d1214 1
a1214 1
	/* locked: maps(read), amap, oanon, anon (if different from oanon) */
d1217 4
a1220 1
	 * now map the page in.
a1227 1

a1234 3

		if (anon != oanon)
			simple_unlock(&anon->an_lock);
d1253 1
d1267 1
d1270 1
a1276 2
	if (anon != oanon)
		simple_unlock(&anon->an_lock);
d1281 1
d1336 1
a1357 4
		uvm_lock_pageq();
		uvm_pageactivate(uobjpage);
		uvm_unlock_pageq();

d1391 1
d1396 1
d1398 1
d1401 6
a1406 1
				uvm_pagefree(uobjpage);
d1409 6
d1419 1
d1423 2
a1424 2
		 * we have the data in uobjpage which is busy and
		 * not released.  we are holding object lock (so the page
a1454 3
		/* no anon in this case. */
		anon = NULL;

d1466 1
a1467 1
		if (uobjpage->loan_count) {
a1478 1

a1482 1

d1488 5
a1510 1

d1522 1
a1528 1

a1529 1
				uvm_pageactivate(pg);
d1539 4
a1542 2
			}
		}
a1555 1

a1556 2
			 * The new anon is locked.
			 *
a1561 1

a1568 1

a1569 5
			if (anon != NULL) {
				anon->an_ref--;
				simple_unlock(&anon->an_lock);
				uvm_anfree(anon);
			}
a1573 1

d1579 3
d1598 1
a1616 1

d1635 3
a1646 1

a1650 1

d1654 1
d1661 1
a1661 2
	 * maps(read), amap(if !null), uobj(if !null), uobjpage(if uobj),
	 *   anon(if !null), pg(if anon)
a1673 1
	KASSERT(access_type == VM_PROT_READ || (pg->flags & PG_RDONLY) == 0);
a1676 1

d1686 1
a1686 1
			wakeup(pg);
d1695 1
a1695 1
		uvmfault_unlockall(&ufi, amap, uobj, anon);
d1709 1
d1725 1
d1729 1
d1731 1
a1731 1
		wakeup(pg);
d1740 2
a1741 1
	uvmfault_unlockall(&ufi, amap, uobj, anon);
d1743 1
d1748 1
d1802 1
d1843 1
a1843 1
	for (va = start; va < end; va += PAGE_SIZE) {
@


1.28.2.3
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.28.2.2 2002/02/02 03:28:26 art Exp $	*/
d178 2
a179 2
static void uvmfault_amapcopy(struct uvm_faultinfo *);
static __inline void uvmfault_anonflush(struct vm_anon **, int);
@


1.28.2.4
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_fault.c,v 1.28.2.3 2002/06/11 03:33:03 art Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.79 2002/10/30 05:24:33 yamt Exp $	*/
d531 2
a532 2
	vm_prot_t enter_prot, check_prot;
	boolean_t wired, narrow, promote, locked, shadowed, wire_fault, cow_now;
d534 1
a534 2
	vaddr_t startva, objaddr, currva, offset;
	voff_t uoff;
d557 1
a557 3
	wire_fault = fault_type == VM_FAULT_WIRE ||
	    fault_type == VM_FAULT_WIREMAX;
	if (wire_fault)
d592 1
a592 3
	check_prot = fault_type == VM_FAULT_WIREMAX ?
	    ufi.entry->max_protection : ufi.entry->protection;
	if ((check_prot & access_type) != access_type) {
d608 2
a609 2
	wired = VM_MAPENT_ISWIRED(ufi.entry) || wire_fault;
	if (wired) {
a610 4
		cow_now = (check_prot & VM_PROT_WRITE) != 0;
	} else {
		cow_now = (access_type & VM_PROT_WRITE) != 0;
	}
d620 2
a621 2
		KASSERT(fault_type != VM_FAULT_WIREMAX);
		if (cow_now || (ufi.entry->object.uvm_obj == NULL)) {
d636 1
a637 1
			enter_prot &= ~VM_PROT_WRITE;
d934 1
a934 2
				    enter_prot & ~VM_PROT_WRITE :
				    enter_prot & MASK(ufi.entry),
d1035 1
a1035 1
		if (!cow_now) {
d1081 3
a1083 1

a1086 13

				if (uobj) {
					/* if we were receiver of loan */
					anon->u.an_page->loan_count--;
				} else {
					/*
					 * we were the lender (A->K); need
					 * to remove the page from pageq's.
					 */
					uvm_pagedequeue(anon->u.an_page);
				}

				uvm_pageactivate(pg);
d1118 1
a1118 1
	if (cow_now && anon->an_ref > 1) {
d1217 1
a1217 1
	if (wire_fault) {
d1266 2
a1267 1
		promote = cow_now && UVM_ET_ISCOPYONWRITE(ufi.entry);
d1421 1
a1421 1
			if (!cow_now) {
a1475 8
				 * if the page is no longer referenced by
				 * an anon (i.e. we are breaking an O->K
				 * loan), then remove it from any pageq's.
				 */
				if (uobjpage->uanon == NULL)
					uvm_pagedequeue(uobjpage);

				/*
d1631 1
a1631 2
	KASSERT((access_type & VM_PROT_WRITE) == 0 ||
		(pg->flags & PG_RDONLY) == 0);
d1633 1
a1633 1
	    pg->flags & PG_RDONLY ? enter_prot & ~VM_PROT_WRITE : enter_prot,
d1668 1
a1668 1
	if (wire_fault) {
d1712 1
a1712 1
uvm_fault_wire(map, start, end, fault_type, access_type)
a1714 1
	vm_fault_t fault_type;
d1735 1
a1735 1
		error = uvm_fault(map, va, fault_type, access_type);
a1790 1

d1797 2
a1798 1
			continue;
d1801 2
a1802 1
		 * find the map entry for the current address.
d1806 1
a1806 1
		while (va >= entry->end) {
a1814 1

@


1.27
log
@Sync in more uvm from NetBSD. Mostly just cosmetic stuff.
Contains also support for page coloring.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_fault.c,v 1.26 2001/11/28 13:47:39 art Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.67 2001/06/26 17:55:14 thorpej Exp $	*/
d848 1
a848 1
		pmap_update();
d988 1
a988 1
			pmap_update();
d1287 1
a1287 1
	pmap_update();
d1764 1
a1764 1
	pmap_update();
@


1.26
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_fault.c,v 1.25 2001/11/27 05:27:12 art Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.60 2001/04/01 16:45:53 chs Exp $	*/
d62 1
a62 1
 *         |             |                         |        |        
d72 1
a72 1
 * 
d86 1
a86 1
 *		data is "promoted" from uobj to a new anon.   
d91 1
a91 1
 * MAPS => AMAP => UOBJ => ANON => PAGE QUEUES (PQ) 
d96 1
a96 1
 *  
d128 1
a128 1
 *    of the information that we already have from our previous lookup, 
d162 1
a162 1
 * note: index in array must match "advice" value 
d198 1
a198 1
	
d251 1
a251 1
			amap_copy(ufi->map, ufi->entry, M_NOWAIT, TRUE, 
d267 1
a267 1
		
d312 1
a312 1
	/* 
d372 1
a372 1
		
d388 1
a388 1
	
d390 1
a390 1
				we_own = TRUE;	
d428 1
a428 1
		 *   [2] I/O not OK.   free the page and cause the fault 
d433 1
a433 1
		
d438 1
a438 1
				wakeup(pg);	
d444 1
a444 1
			/* 
d491 1
a491 1
			
d518 1
a518 1
		    amap_lookup(&ufi->entry->aref, 
d520 1
a520 1
			
d525 1
a525 1
			
d527 1
a527 1
		 * try it again! 
d546 1
a546 1
 * => VM data structures usually should be unlocked.   however, it is 
d551 1
d559 1
a559 1
	vm_map_t orig_map;
d569 1
a569 1
	paddr_t pa; 
a597 14
	 * before we do anything else, if this is a fault on a kernel
	 * address, check to see if the address is managed by an
	 * interrupt-safe map.  If it is, we fail immediately.  Intrsafe
	 * maps are never pageable, and this approach avoids an evil
	 * locking mess.
	 */

	if (orig_map == kernel_map && uvmfault_check_intrsafe(&ufi)) {
		UVMHIST_LOG(maphist, "<- VA 0x%lx in intrsafe map %p",
		    ufi.orig_rvaddr, ufi.map, 0, 0);
		return EFAULT;
	}

	/*
d612 9
a620 1
	KASSERT(ufi.map->flags & VM_MAP_PAGEABLE);
d670 1
a670 1
			enter_prot &= ~VM_PROT_WRITE; 
d696 1
a696 1
	 * to do this the first time through the fault.   if we 
d721 1
a721 1
		
d761 1
a761 1
		if (amap) 
d769 1
a769 1
			(void) uobj->pgops->pgo_flush(uobj, objaddr, objaddr + 
d848 1
d853 1
a853 1
	UVMHIST_LOG(maphist, "  shadowed=%d, will_get=%d", shadowed, 
d863 1
a863 1
	
d883 3
d939 1
a939 1
				
d943 1
a943 1
					    "(0x%x) with locked get", 
d947 2
a948 2
	
				/* 
d979 1
a979 1
				/* 
d988 1
d996 1
a996 1
	/* locked (!shadowed): maps(read), amap(if there), 
d1018 1
a1018 1
	if (shadowed == FALSE) 
d1050 1
a1050 1
		break; 
d1072 1
a1072 1
	 * special handling for loaned pages 
d1078 1
a1078 1
			
d1154 2
a1155 2
	 * in the (hopefully very rare) case that we are out of RAM we 
	 * will unlock, wait for more RAM, and refault.    
d1287 1
d1309 1
a1309 1
		uobjpage = PGO_DONTCARE;	
d1323 1
a1323 1
	 * if uobjpage is null, then we need to unlock and ask the pager to 
d1334 1
a1334 1
		
d1376 1
a1376 1
		
d1387 1
a1387 1
		    (locked && amap && 
d1390 1
a1390 1
			if (locked) 
d1402 1
a1402 1
			    "  wasn't able to relock after fault: retry", 
d1452 1
a1452 1
		
d1556 1
a1556 1
		
d1638 3
d1642 1
a1642 1
			
d1710 1
a1710 1
		/* 
d1714 1
a1714 1
 
d1755 2
a1756 2
	/* 
	 * note that pg can't be PG_RELEASED since we did not drop the object 
d1759 1
a1759 1
 
d1764 2
d1782 1
a1782 1
	vm_map_t map;
a1786 1
	pmap_t  pmap;
a1788 2
	pmap = vm_map_pmap(map);

d1791 1
a1791 1
	 * to undo what we have done.   note that in uvm_fault VM_PROT_NONE 
d1795 8
a1811 1

d1821 1
a1821 1
	vm_map_t map;
d1838 1
a1838 1
	vm_map_t map;
d1841 1
a1841 1
	vm_map_entry_t entry;
@


1.25
log
@Merge in the unified buffer cache code as found in NetBSD 2001/03/10. The
code is written mostly by Chuck Silvers <chuq@@chuq.com>/<chs@@netbsd.org>.

Tested for the past few weeks by many developers, should be in a pretty stable
state, but will require optimizations and additional cleanups.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_fault.c,v 1.24 2001/11/12 01:26:09 art Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.56 2001/02/18 21:19:08 chs Exp $	*/
d279 1
a279 1
 * => if we fail (result != VM_PAGER_OK) we unlock everything.
d299 1
a299 1
	int result;
d304 1
a304 1
	result = 0;		/* XXX shut up gcc */
d345 1
a345 1
				return (VM_PAGER_OK);
d401 1
a401 1
				result = uvm_swap_get(pg, anon->an_swslot,
d458 1
a458 1
				return (VM_PAGER_REFAULT);	/* refault! */
d461 1
a461 3
			if (result != VM_PAGER_OK) {
				KASSERT(result != VM_PAGER_PEND);

d489 1
a489 1
				return (VM_PAGER_ERROR);
d510 1
a510 1
			return (VM_PAGER_REFAULT);
d523 1
a523 1
			return (VM_PAGER_REFAULT);
d566 1
a566 1
	int npages, nback, nforw, centeridx, result, lcv, gotpages;
d603 1
d607 1
a607 1
		return (KERN_FAILURE);
d621 1
a621 1
		return (KERN_INVALID_ADDRESS);
d625 2
d636 1
a636 12
		return (KERN_PROTECTION_FAILURE);
	}

	/*
	 * if the map is not a pageable map, a page fault always fails.
	 */

	if ((ufi.map->flags & VM_MAP_PAGEABLE) == 0) {
		UVMHIST_LOG(maphist,
		    "<- map %p not pageable", ufi.map, 0, 0, 0);
		uvmfault_unlockmaps(&ufi, FALSE);
		return (KERN_FAILURE);
d695 1
a695 1
		return (KERN_INVALID_ADDRESS);
d880 2
a881 3
		result = uobj->pgops->pgo_fault(&ufi, startva, pages, npages,
				    centeridx, fault_type, access_type,
				    PGO_LOCKED|PGO_SYNCIO);
d885 1
a885 3
		if (result == VM_PAGER_OK)
			return (KERN_SUCCESS);	/* pgo_fault did pmap enter */
		else if (result == VM_PAGER_REFAULT)
d887 1
a887 2
		else
			return (KERN_PROTECTION_FAILURE);
d1047 3
a1049 3
	result = uvmfault_anonget(&ufi, amap, anon);
	switch (result) {
	case VM_PAGER_OK:
d1052 1
a1052 1
	case VM_PAGER_REFAULT:
d1055 1
a1055 1
	case VM_PAGER_AGAIN:
d1060 1
a1060 5
#ifdef DIAGNOSTIC
		panic("uvm_fault: uvmfault_anonget -> %d", result);
#else
		return (KERN_PROTECTION_FAILURE);
#endif
d1184 1
a1184 1
				return (KERN_RESOURCE_SHORTAGE);
d1232 1
a1232 1
	    != KERN_SUCCESS) {
d1248 1
a1248 1
			return (KERN_RESOURCE_SHORTAGE);
d1287 1
a1287 1
	return (KERN_SUCCESS);
d1341 1
a1341 1
		result = uobj->pgops->pgo_get(uobj, uoff, &uobjpage, &gotpages,
d1345 1
a1345 1
		/* locked: uobjpage(if result OK) */
d1351 2
a1352 4
		if (result != VM_PAGER_OK) {
			KASSERT(result != VM_PAGER_PEND);

			if (result == VM_PAGER_AGAIN) {
d1355 1
a1355 1
				tsleep((caddr_t)&lbolt, PVM, "fltagain2", 0);
d1360 2
a1361 2
			    result, 0,0,0);
			return (KERN_PROTECTION_FAILURE); /* XXX i/o error */
d1612 1
a1612 1
				return (KERN_RESOURCE_SHORTAGE);
d1693 1
a1693 2
	    access_type | PMAP_CANFAIL | (wired ? PMAP_WIRED : 0))
	    != KERN_SUCCESS) {
d1719 1
a1719 1
			return (KERN_RESOURCE_SHORTAGE);
d1761 1
a1761 1
	return (KERN_SUCCESS);
d1782 1
a1782 1
	int rv;
d1793 2
a1794 2
		rv = uvm_fault(map, va, VM_FAULT_WIRE, access_type);
		if (rv) {
d1798 1
a1798 1
			return (rv);
d1802 1
a1802 1
	return (KERN_SUCCESS);
@


1.24
log
@Bring in more changes from NetBSD. Mostly pagedaemon improvements.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.23 2001/11/09 04:34:27 art Exp $	*/
a206 1
#ifdef UBC
a207 3
#else
				pmap_page_protect(pg, VM_PROT_NONE);
#endif
@


1.23
log
@various style fixes.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_fault.c,v 1.22 2001/11/06 13:36:52 art Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.51 2000/08/06 00:22:53 thorpej Exp $	*/
d207 3
d211 1
d306 2
a890 1

d896 2
a897 1
				    PGO_LOCKED);
d928 1
a928 1
		result = uobj->pgops->pgo_get(uobj, ufi.entry->offset +
d983 1
d990 1
d993 2
a994 1
				    enter_prot & MASK(ufi.entry),
d1073 3
a1075 7
	case VM_PAGER_ERROR:
		/*
		 * An error occurred while trying to bring in the
		 * page -- this is the only error we return right
		 * now.
		 */
		return (KERN_PROTECTION_FAILURE);	/* XXX */
d1188 2
a1189 1
		if (anon)
d1191 1
d1195 3
a1197 1
			if (anon)
d1199 1
d1224 1
a1224 1
			 
d1226 3
a1228 3
		 * note: oanon still locked.   anon is _not_ locked, but we
		 * have the sole references to in from amap which _is_ locked.
		 * thus, no one can get at it until we are done with it.
d1241 1
a1241 1
	/* locked: maps(read), amap, oanon */
d1262 2
d1306 2
d1487 3
d1592 2
d1608 6
a1640 1
			uvm_anfree(anon);
a1695 1
		
d1700 2
a1701 1
	 * maps(read), amap(if !null), uobj(if !null), uobjpage(if uobj)
d1714 1
d1716 2
a1717 1
	    enter_prot, access_type | PMAP_CANFAIL | (wired ? PMAP_WIRED : 0))
d1719 1
d1738 1
a1738 1
		uvmfault_unlockall(&ufi, amap, uobj, NULL);
d1783 1
a1783 1
	uvmfault_unlockall(&ufi, amap, uobj, NULL);
d1876 1
a1876 4
#ifdef DIAGNOSTIC
	if (start < vm_map_min(map) || end > vm_map_max(map))
		panic("uvm_fault_unwire_locked: address out of range");
#endif
d1889 2
a1890 4
#ifdef DIAGNOSTIC
		if (va < entry->start)
			panic("uvm_fault_unwire_locked: hole 1");
#endif
d1892 2
a1893 5
#ifdef DIAGNOSTIC
			if (entry->next == &map->header ||
			    entry->next->start > entry->end)
				panic("uvm_fault_unwire_locked: hole 2");
#endif
@


1.22
log
@More sync to NetBSD.
 - Use malloc/free instead of MALLOC/FREE for variable sized allocations.
 - Move the memory inheritance code to sys/mman.h and rename from VM_* to MAP_*
 - various cleanups and simplifications.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.21 2001/11/06 01:35:04 art Exp $	*/
d460 2
a461 6
#ifdef DIAGNOSTIC
				if (result == VM_PAGER_PEND) {
					panic("uvmfault_anonget: "
					      "got PENDING for non-async I/O");
				}
#endif
d567 1
a567 1
	vaddr_t startva, objaddr, currva, offset;
d578 2
a579 1
	anon = NULL; /* XXX: shut up gcc */
d716 2
a717 4
#ifdef DIAGNOSTIC
		if (uvmadvice[ufi.entry->advice].advice != ufi.entry->advice)
			panic("fault: advice mismatch!");
#endif
d790 1
a790 1
		startva = startva + (nback << PAGE_SHIFT);
d811 4
a814 6
		if (lcv != centeridx) {
			if (pmap_extract(ufi.orig_map->pmap, currva, &pa) ==
			    TRUE) {
				pages[lcv] = PGO_DONTCARE;
				continue;
			}
d846 1
d852 1
d943 1
a943 9
#ifdef DIAGNOSTIC
					/*
					 * pager sanity check: pgo_get with
					 * PGO_LOCKED should never return a
					 * released page to us.
					 */
					if (pages[lcv]->flags & PG_RELEASED) 
		panic("uvm_fault: pgo_get PGO_LOCKED gave us a RELEASED page");
#endif
d945 12
a956 11
					/*
					 * if center page is resident and not
					 * PG_BUSY|PG_RELEASED then pgo_get
					 * made it PG_BUSY for us and gave
					 * us a handle to it.   remember this
					 * page as "uobjpage." (for later use).
					 */

					if (lcv == centeridx) {
						uobjpage = pages[lcv];
	UVMHIST_LOG(maphist, "  got uobjpage (0x%x) with locked get", 
d958 1
a958 1
						continue;
d994 1
a996 2
	 
				/* done! */
a998 1

a1000 1
		
a1001 1

d1091 1
a1184 4
#ifdef __GNUC__
		else
			pg = NULL; /* XXX: gcc */
#endif
d1191 1
a1191 5
#ifdef DIAGNOSTIC
			if (uvmexp.swpgonly > uvmexp.swpages) {
				panic("uvmexp.swpgonly botch");
			}
#endif
d1222 1
a1222 1
		
d1231 1
a1231 1
	/* locked: maps(read), amap, anon */
d1253 1
a1253 4
#ifdef DIAGNOSTIC
		if (uvmexp.swpgonly > uvmexp.swpages)
			panic("uvmexp.swpgonly botch");
#endif
d1319 1
a1319 1
		/* assert(uobjpage != PGO_DONTCARE) */
d1348 4
a1351 5
		result = uobj->pgops->pgo_get(uobj,
		    (ufi.orig_rvaddr - ufi.entry->start) + ufi.entry->offset,
		    &uobjpage, &gotpages, 0,
			access_type & MASK(ufi.entry),
			ufi.entry->advice, 0);
d1354 1
a1354 1
		
d1360 1
a1360 5
#ifdef DIAGNOSTIC 
			if (result == VM_PAGER_PEND)
				panic("uvm_fault: pgo_get got PENDing "
				    "on non-async I/O");
#endif
d1419 2
a1420 5
#ifdef DIAGNOSTIC
				if (uobj->pgops->pgo_releasepg == NULL)
					panic("uvm_fault: object has no "
					    "releasepg function");
#endif
a1446 1

a1582 4
#ifdef __GNUC__
		else
			pg = NULL; /* XXX: gcc */
#endif
a1597 1
				/* make sure it is in queues */
a1599 1
				/* un-busy! (still locked) */
d1606 1
a1606 5
#ifdef DIAGNOSTIC
			if (uvmexp.swpgonly > uvmexp.swpages) {
				panic("uvmexp.swpgonly botch");
			}
#endif
d1641 2
a1642 2
			 * since we still hold the object lock.   drop
			 * handle to uobj as well.
d1651 1
a1651 1
			uvm_pageactivate(uobjpage);	/* put it back */
d1655 1
d1700 1
d1712 1
a1712 4
#ifdef DIAGNOSTIC
		if (uvmexp.swpgonly > uvmexp.swpages)
			panic("uvmexp.swpgonly botch");
#endif
a1743 1

d1835 1
a1835 4
#ifdef DIAGNOSTIC
	if (map->flags & VM_MAP_INTRSAFE)
		panic("uvm_fault_unwire_locked: intrsafe map");
#endif
@


1.21
log
@Move the last content from vm/ to uvm/
The only thing left in vm/ are just dumb wrappers.
vm/vm.h includes uvm/uvm_extern.h
vm/pmap.h includes uvm/uvm_pmap.h
vm/vm_page.h includes uvm/uvm_page.h
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_fault.c,v 1.20 2001/11/05 22:14:54 art Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.50 2000/06/27 17:29:21 mrg Exp $	*/
d279 1
a279 1
 * => if we fail (result != VM_PAGER_OK) we unlock everything except anon.
d1065 1
a1065 1
	 * if it fails (!OK) it will unlock all but the anon for us.
d1073 3
a1075 3
	if (result != VM_PAGER_OK) {
		simple_unlock(&anon->an_lock);
	}
d1077 1
a1077 1
	if (result == VM_PAGER_REFAULT)
d1080 14
a1093 3
	if (result == VM_PAGER_AGAIN) {
		tsleep((caddr_t)&lbolt, PVM, "fltagain1", 0);
		goto ReFault;
a1094 3

	if (result != VM_PAGER_OK)
		return (KERN_PROTECTION_FAILURE);		/* XXX??? */
@


1.20
log
@Minor sync to NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_fault.c,v 1.19 2001/09/19 20:50:59 mickey Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.49 2000/06/26 14:21:17 mrg Exp $	*/
a48 3

#include <vm/vm.h>
#include <vm/vm_page.h>
@


1.19
log
@merge vm/vm_kern.h into uvm/uvm_extern.h; art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_fault.c,v 1.18 2001/08/11 10:57:22 art Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.48 2000/04/10 01:17:41 thorpej Exp $	*/
@


1.18
log
@Various random fixes from NetBSD.
Including support for zeroing pages in the idle loop (not enabled yet).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.17 2001/07/26 19:37:13 art Exp $	*/
a51 1
#include <vm/vm_kern.h>
@


1.17
log
@Add support for disabling swap devices (swapctl -d).
Improve error handling on I/O errors to swap.
From NetBSD
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_fault.c,v 1.16 2001/07/25 13:25:33 art Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.47 2000/01/11 06:57:50 chs Exp $	*/
d1604 10
a1613 2
		if (anon)
			pg = uvm_pagealloc(NULL, 0, anon, 0);
d1702 4
a1705 1
			uvm_pagezero(pg);	/* zero page [pg now dirty] */
@


1.16
log
@Change the pmap_enter interface to merge access_type and the wired boolean
and arbitrary flags into one argument.

One new flag is PMAP_CANFAIL that tells pmap_enter that it can fail if there
are not enough resources to satisfy the request. If this flag is not passed,
pmap_enter should panic as it should have done before this change (XXX - many
pmaps are still not doing that).

Only i386 and alpha implement CANFAIL for now.

Includes uvm updates from NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_fault.c,v 1.15 2001/07/18 10:47:05 art Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.46 1999/11/13 00:24:38 thorpej Exp $	*/
d283 1
a283 1
 * => if we fail (result != VM_PAGER_OK) we unlock everything.
d294 2
a295 1
int uvmfault_anonget(ufi, amap, anon)
d419 1
a419 1
		if (locked) {
d456 2
a457 1
				  uvmfault_unlockall(ufi, amap, NULL, NULL);
d465 4
a468 2
				if (result == VM_PAGER_PEND)
		panic("uvmfault_anonget: got PENDING for non-async I/O");
d473 10
a482 1
				/* 
d525 3
a527 2
		if (amap_lookup(&ufi->entry->aref, 
		    ufi->orig_rvaddr - ufi->entry->start) != anon) {
d1068 3
a1070 2
	 * let uvmfault_anonget do the dirty work.   if it fails (!OK) it will
	 * unlock for us.   if it is OK, locks are still valid and locked.
d1077 3
d1817 3
a1819 2
	 * fault it in page at a time.   if the fault fails then we have
	 * to undo what we have done.
@


1.15
log
@Get rid of the PMAP_NEW option by making it mandatory for all archs.
The archs that didn't have a proper PMAP_NEW now have a dummy implementation
with wrappers around the old functions.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_fault.c,v 1.14 2001/06/23 19:24:33 smart Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.45 1999/09/12 01:17:35 chs Exp $	*/
d685 1
a685 1
	amap = ufi.entry->aref.ar_amap;	/* top layer */
d843 6
a848 1
			pmap_enter(ufi.orig_map->pmap, currva,
d851 3
a853 2
			    enter_prot, 
			    VM_MAPENT_ISWIRED(ufi.entry), 0);
d866 3
d979 7
a985 1
				pmap_enter(ufi.orig_map->pmap, currva,
d987 3
a989 1
				    enter_prot & MASK(ufi.entry), wired, 0);
d1243 25
a1267 2
	pmap_enter(ufi.orig_map->pmap, ufi.orig_rvaddr, VM_PAGE_TO_PHYS(pg),
	    enter_prot, wired, access_type);
d1270 1
a1270 1
	 * ... and update the page queues.
d1328 1
a1328 1
	promote, (uobj == NULL), 0,0);
a1364 1
			
d1367 2
a1368 1
	panic("uvm_fault: pgo_get got PENDing on non-async I/O");
d1372 4
a1375 3
	UVMHIST_LOG(maphist, "  pgo_get says TRY AGAIN!",0,0,0,0);
	tsleep((caddr_t)&lbolt, PVM, "fltagain2", 0);
	goto ReFault;
d1430 2
a1431 1
			panic("uvm_fault: object has no releasepg function");
d1513 1
a1513 1
					 * */
d1701 35
a1735 2
	pmap_enter(ufi.orig_map->pmap, ufi.orig_rvaddr, VM_PAGE_TO_PHYS(pg),
	    enter_prot, wired, access_type);
@


1.14
log
@Sync with NetBSD 19990911 (just before PMAP_NEW was required)
  - thread_sleep_msg() -> uvm_sleep()
  - initialize reference count lock in uvm_anon_{init,add}()
  - add uao_flush()
  - replace boolean 'islocked' with 'lockflags'
  - in uvm_fault() change FALSE to TRUE to in 'wide' fault handling
  - get rid of uvm_km_get()
  - various bug fixes
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_fault.c,v 1.13 2001/06/08 08:09:38 art Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.44 1999/07/22 22:58:38 thorpej Exp $	*/
d211 1
a211 1
				pmap_page_protect(PMAP_PGARG(pg), VM_PROT_NONE);
d451 1
a451 2
				pmap_page_protect(PMAP_PGARG(pg),
				    VM_PROT_NONE); /* to be safe */
d491 1
a491 1
			pmap_clear_modify(PMAP_PGARG(pg));
a859 1
	 * XXXCDC: this is fixed in PMAP_NEW (no sleep alloc's in pmap)
d1113 2
a1114 2
				pmap_page_protect(PMAP_PGARG(anon->u.an_page),
				    VM_PROT_NONE); 
d1502 1
a1502 2
				pmap_page_protect(PMAP_PGARG(uobjpage),
				    VM_PROT_NONE); 
d1609 1
a1609 2
				pmap_page_protect(PMAP_PGARG(uobjpage),
				    VM_PROT_NONE);
@


1.13
log
@Change the paddr_t pmap_extract(struct pmap *, vaddr_t) interface to
boolean_t pmap_extract(struct pmap *, vaddr_t, paddr_t *).
Matches NetBSD. Tested by various people on various platforms.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_fault.c,v 1.12 2001/05/09 15:31:23 art Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.40 1999/07/08 18:11:03 thorpej Exp $	*/
d727 1
a727 1
		narrow = FALSE;	/* ensure only once per-fault */
d846 2
a847 2
			    (anon->an_ref > 1) ?
			    (enter_prot & ~VM_PROT_WRITE) : enter_prot, 
@


1.12
log
@More sync to NetBSD.

 - Change pmap_change_wiring to pmap_unwire because it's only called that way.
 - Remove pmap_pageable because it's seldom implemented and when it is, it's
   either almost useless or incorrect. The same information is already passed
   to the pmap anyway by pmap_enter and pmap_unwire.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_fault.c,v 1.11 2001/05/07 16:08:40 art Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.39 1999/06/17 19:23:21 thorpej Exp $	*/
a805 1
		 * XXX: return value of pmap_extract disallows PA 0
d808 2
a809 2
			pa = pmap_extract(ufi.orig_map->pmap, currva);
			if (pa != NULL) {
d1802 1
a1802 4
		pa = pmap_extract(pmap, va);

		/* XXX: assumes PA 0 cannot be in map */
		if (pa == (paddr_t) 0) {
a1804 1
		}
@


1.11
log
@Few fixes from NetBSD.
 - make sure that vsunlock doesn't unwire mlocked memory.
 - fix locking in uvm_useracc.
 - Return the error uvm_fault_wire in uvm_vslock (will be used soon).
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_fault.c,v 1.10 2001/03/22 23:36:52 niklas Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.37 1999/06/16 23:02:40 thorpej Exp $	*/
a1727 1
#ifndef PMAP_NEW
d1729 1
a1729 9
	 * call pmap pageable: this tells the pmap layer to lock down these
	 * page tables.
	 */

	pmap_pageable(pmap, start, end, FALSE);
#endif

	/*
	 * now fault it in page at a time.   if the fault fails then we have
d1832 1
a1832 1
			pmap_change_wiring(pmap, va, FALSE);
a1839 9

#ifndef PMAP_NEW
	/*
	 * now we call pmap_pageable to let the pmap know that the page tables
	 * in this space no longer need to be wired.
	 */

	pmap_pageable(pmap, start, end, TRUE);
#endif
@


1.10
log
@Merge in NetBSD's PMAP_NEW, still disabled
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_fault.c,v 1.9 2001/03/22 03:05:55 smart Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.35 1999/06/16 18:43:28 thorpej Exp $	*/
d1710 4
a1713 4
 * => map should be locked by caller?   If so how can we call
 *	uvm_fault?   WRONG.
 * => XXXCDC: locking here is all screwed up!!!  start with 
 *	uvm_map_pageable and fix it.
d1764 18
d1789 1
a1789 1
		panic("uvm_fault_unwire: intrsafe map");
d1801 10
d1816 2
a1817 1
			panic("uvm_fault_unwire: unwiring non-wired memory");
d1820 22
a1841 1
		pmap_change_wiring(pmap, va, FALSE);  /* tell the pmap */
@


1.9
log
@Sync style, typo, and comments a little closer to NetBSD.  art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.8 2001/03/09 14:20:51 art Exp $	*/
d847 2
a848 2
			    (anon->an_ref > 1) ? (enter_prot & ~VM_PROT_WRITE) :
			    enter_prot, 
d1728 1
d1735 1
d1790 1
d1792 1
d1800 1
d1807 1
a1807 1

@


1.8
log
@More syncing to NetBSD.

Implements mincore(2), mlockall(2) and munlockall(2). mlockall and munlockall
are disabled for the moment.

The rest is mostly cosmetic.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.7 2001/03/08 15:21:36 smart Exp $	*/
d1778 1
a1778 1
	
@


1.7
log
@Replace thread_wakeup() with wakeup().  It is defined in vm_extern.h as a
wrapper, so this removes a dependence on the old VM system.  From NetBSD.
art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_fault.c,v 1.6 2001/01/29 02:07:44 niklas Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.33 1999/06/04 23:38:41 thorpej Exp $	*/
d649 1
a649 1
	wired = (ufi.entry->wired_count != 0) || (fault_type == VM_FAULT_WIRE);
d849 1
a849 1
			    (ufi.entry->wired_count != 0), 0);
d1737 1
a1737 2
	 * to undo what we have done.   note that in uvm_fault VM_PROT_NONE 
	 * is replaced with the max protection if fault_type is VM_FAULT_WIRE.
@


1.6
log
@$OpenBSD$
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_fault.c,v 1.33 1999/06/04 23:38:41 thorpej Exp $	*/
d439 1
a439 1
				thread_wakeup(pg);	
d1386 1
a1386 1
				thread_wakeup(uobjpage);
d1476 1
a1476 1
						thread_wakeup(uobjpage);
d1508 1
a1508 1
					thread_wakeup(uobjpage);
d1566 1
a1566 1
					thread_wakeup(uobjpage);
d1625 1
a1625 1
				thread_wakeup(uobjpage);
d1691 1
a1691 1
		thread_wakeup(pg);		/* lock still held */
@


1.5
log
@Bring in some new UVM code from NetBSD (not current).

 - Introduce a new type of map that are interrupt safe and never allow faults
   in them. mb_map and kmem_map are made intrsafe.
 - Add "access protection" to uvm_vslock (to be passed down to uvm_fault and
   later to pmap_enter).
 - madvise(2) now works.
 - various cleanups.
@
text
@d1 1
@


1.4
log
@Change the pmap_enter api to pass down an argument that indicates
the access type that caused this mapping. This is to simplify pmaps
with mod/ref emulation (none for the moment) and in some cases speed
up pmap_is_{referenced,modified}.
At the same time, clean up some mappings that had too high protection.

XXX - the access type is incorrect in old vm, it's only used by uvm and MD code.
The actual use of this in pmap_enter implementations is not in this commit.
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_fault.c,v 1.28 1999/04/11 04:04:11 chs Exp $	*/
d590 13
d630 11
d1716 1
a1716 1
uvm_fault_wire(map, start, end)
d1719 1
d1741 1
a1741 1
		rv = uvm_fault(map, va, VM_FAULT_WIRE, VM_PROT_NONE);
d1744 1
a1744 1
				uvm_fault_unwire(map->pmap, start, va);
a1754 2
 *
 * => caller holds reference to pmap (via its map)
d1758 2
a1759 2
uvm_fault_unwire(pmap, start, end)
	struct pmap *pmap;
d1762 1
d1766 5
@


1.4.4.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_fault.c,v 1.33 1999/06/04 23:38:41 thorpej Exp $	*/
a589 13
	 * before we do anything else, if this is a fault on a kernel
	 * address, check to see if the address is managed by an
	 * interrupt-safe map.  If it is, we fail immediately.  Intrsafe
	 * maps are never pageable, and this approach avoids an evil
	 * locking mess.
	 */
	if (orig_map == kernel_map && uvmfault_check_intrsafe(&ufi)) {
		UVMHIST_LOG(maphist, "<- VA 0x%lx in intrsafe map %p",
		    ufi.orig_rvaddr, ufi.map, 0, 0);
		return (KERN_FAILURE);
	}

	/*
a616 11
	 * if the map is not a pageable map, a page fault always fails.
	 */

	if ((ufi.map->flags & VM_MAP_PAGEABLE) == 0) {
		UVMHIST_LOG(maphist,
		    "<- map %p not pageable", ufi.map, 0, 0, 0);
		uvmfault_unlockmaps(&ufi, FALSE);
		return (KERN_FAILURE);
	}

	/*
d1692 1
a1692 1
uvm_fault_wire(map, start, end, access_type)
a1694 1
	vm_prot_t access_type;
d1716 1
a1716 1
		rv = uvm_fault(map, va, VM_FAULT_WIRE, access_type);
d1719 1
a1719 1
				uvm_fault_unwire(map, start, va);
d1730 2
d1735 2
a1736 2
uvm_fault_unwire(map, start, end)
	vm_map_t map;
a1738 1
	pmap_t pmap = vm_map_pmap(map);
a1741 5

#ifdef DIAGNOSTIC
	if (map->flags & VM_MAP_INTRSAFE)
		panic("uvm_fault_unwire: intrsafe map");
#endif
@


1.4.4.2
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_fault.c,v 1.10 2001/03/22 23:36:52 niklas Exp $	*/
/*	$NetBSD: uvm_fault.c,v 1.35 1999/06/16 18:43:28 thorpej Exp $	*/
d438 1
a438 1
				wakeup(pg);	
d648 1
a648 1
	wired = VM_MAPENT_ISWIRED(ufi.entry) || (fault_type == VM_FAULT_WIRE);
d846 3
a848 3
			    (anon->an_ref > 1) ?
			    (enter_prot & ~VM_PROT_WRITE) : enter_prot, 
			    VM_MAPENT_ISWIRED(ufi.entry), 0);
d1385 1
a1385 1
				wakeup(uobjpage);
d1475 1
a1475 1
						wakeup(uobjpage);
d1507 1
a1507 1
					wakeup(uobjpage);
d1565 1
a1565 1
					wakeup(uobjpage);
d1624 1
a1624 1
				wakeup(uobjpage);
d1690 1
a1690 1
		wakeup(pg);		/* lock still held */
a1726 1
#ifndef PMAP_NEW
a1732 1
#endif
d1736 2
a1737 1
	 * to undo what we have done.
d1778 1
a1778 1

a1787 1

a1788 1

a1795 1
#ifndef PMAP_NEW
d1802 1
a1802 1
#endif
@


1.4.4.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_fault.c,v 1.44 1999/07/22 22:58:38 thorpej Exp $	*/
d727 1
a727 1
		narrow = TRUE;	/* ensure only once per-fault */
d806 1
d809 2
a810 2
			if (pmap_extract(ufi.orig_map->pmap, currva, &pa) ==
			    TRUE) {
d847 2
a848 2
			    (anon->an_ref > 1) ? (enter_prot & ~VM_PROT_WRITE) :
			    enter_prot, 
d1710 4
a1713 4
 * => map may be read-locked by caller, but MUST NOT be write-locked.
 * => if map is read-locked, any operations which may cause map to
 *	be write-locked in uvm_fault() must be taken care of by
 *	the caller.  See uvm_map_pageable().
d1728 1
d1730 9
a1738 1
	 * fault it in page at a time.   if the fault fails then we have
a1763 18

	vm_map_lock_read(map);
	uvm_fault_unwire_locked(map, start, end);
	vm_map_unlock_read(map);
}

/*
 * uvm_fault_unwire_locked(): the guts of uvm_fault_unwire().
 *
 * => map must be at least read-locked.
 */

void
uvm_fault_unwire_locked(map, start, end)
	vm_map_t map;
	vaddr_t start, end;
{
	vm_map_entry_t entry;
d1771 1
a1771 1
		panic("uvm_fault_unwire_locked: intrsafe map");
a1782 10
	/*
	 * find the beginning map entry for the region.
	 */
#ifdef DIAGNOSTIC
	if (start < vm_map_min(map) || end > vm_map_max(map))
		panic("uvm_fault_unwire_locked: address out of range");
#endif
	if (uvm_map_lookup_entry(map, start, &entry) == FALSE)
		panic("uvm_fault_unwire_locked: address not in map");

d1784 1
a1784 3
		if (pmap_extract(pmap, va, &pa) == FALSE)
			panic("uvm_fault_unwire_locked: unwiring "
			    "non-wired memory");
d1786 3
a1788 15
		/*
		 * make sure the current entry is for the address we're
		 * dealing with.  if not, grab the next entry.
		 */
#ifdef DIAGNOSTIC
		if (va < entry->start)
			panic("uvm_fault_unwire_locked: hole 1");
#endif
		if (va >= entry->end) {
#ifdef DIAGNOSTIC
			if (entry->next == &map->header ||
			    entry->next->start > entry->end)
				panic("uvm_fault_unwire_locked: hole 2");
#endif
			entry = entry->next;
d1791 1
a1791 5
		/*
		 * if the entry is no longer wired, tell the pmap.
		 */
		if (VM_MAPENT_ISWIRED(entry) == 0)
			pmap_unwire(pmap, va);
d1799 9
@


1.4.4.4
log
@Sync the SMP branch to something just after 3.0
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_fault.c,v 1.48 2000/04/10 01:17:41 thorpej Exp $	*/
d52 1
d211 1
a211 1
				pmap_page_protect(pg, VM_PROT_NONE);
d283 1
a283 1
 * => if we fail (result != VM_PAGER_OK) we unlock everything except anon.
d294 1
a294 2
int
uvmfault_anonget(ufi, amap, anon)
d418 1
a418 1
		if (locked && amap != NULL) {
d451 2
a452 1
				pmap_page_protect(pg, VM_PROT_NONE);
d456 1
a456 2
					uvmfault_unlockall(ufi, amap, NULL,
							   NULL);
d464 2
a465 4
				if (result == VM_PAGER_PEND) {
					panic("uvmfault_anonget: "
					      "got PENDING for non-async I/O");
				}
d470 1
a470 10
				/*
				 * remove the swap slot from the anon
				 * and mark the anon as having no real slot.
				 * don't free the swap slot, thus preventing
				 * it from being used again.
				 */
				uvm_swap_markbad(anon->an_swslot, 1);
				anon->an_swslot = SWSLOT_BAD;

				/*
d492 1
a492 1
			pmap_clear_modify(pg);
d513 2
a514 3
		if (ufi != NULL &&
		    amap_lookup(&ufi->entry->aref, 
				ufi->orig_rvaddr - ufi->entry->start) != anon) {
d686 1
a686 1
	amap = ufi.entry->aref.ar_amap;		/* top layer */
d844 1
a844 6
			/*
			 * Since this isn't the page that's actually faulting,
			 * ignore pmap_enter() failures; it's not critical
			 * that we enter these right now.
			 */
			(void) pmap_enter(ufi.orig_map->pmap, currva,
d847 2
a848 3
			    enter_prot,
			    PMAP_CANFAIL |
			     (VM_MAPENT_ISWIRED(ufi.entry) ? PMAP_WIRED : 0));
d861 1
a861 3
	 *
	 * XXX Actually, that is bad; pmap_enter() should just fail in that
	 * XXX case.  --thorpej
d972 1
a972 7
				/*
				 * Since this page isn't the page that's
				 * actually fauling, ignore pmap_enter()
				 * failures; it's not critical that we
				 * enter these right now.
				 */
				(void) pmap_enter(ufi.orig_map->pmap, currva,
d974 1
a974 3
				    enter_prot & MASK(ufi.entry),
				    PMAP_CANFAIL |
				     (wired ? PMAP_WIRED : 0));
d1039 2
a1040 3
	 * let uvmfault_anonget do the dirty work.
	 * if it fails (!OK) it will unlock all but the anon for us.
	 * if it succeeds, locks are still valid and locked.
a1046 3
	if (result != VM_PAGER_OK) {
		simple_unlock(&anon->an_lock);
	}
d1115 2
a1116 2
				pmap_page_protect(anon->u.an_page,
						  VM_PROT_NONE);
d1228 2
a1229 25
	if (pmap_enter(ufi.orig_map->pmap, ufi.orig_rvaddr, VM_PAGE_TO_PHYS(pg),
	    enter_prot, access_type | PMAP_CANFAIL | (wired ? PMAP_WIRED : 0))
	    != KERN_SUCCESS) {
		/*
		 * No need to undo what we did; we can simply think of
		 * this as the pmap throwing away the mapping information.
		 *
		 * We do, however, have to go through the ReFault path,
		 * as the map may change while we're asleep.
		 */
		uvmfault_unlockall(&ufi, amap, uobj, oanon);
#ifdef DIAGNOSTIC
		if (uvmexp.swpgonly > uvmexp.swpages)
			panic("uvmexp.swpgonly botch");
#endif
		if (uvmexp.swpgonly == uvmexp.swpages) {
			UVMHIST_LOG(maphist,
			    "<- failed.  out of VM",0,0,0,0);
			/* XXX instrumentation */
			return (KERN_RESOURCE_SHORTAGE);
		}
		/* XXX instrumentation */
		uvm_wait("flt_pmfail1");
		goto ReFault;
	}
d1232 1
a1232 1
	 * ... update the page queues.
d1290 1
a1290 1
	    promote, (uobj == NULL), 0,0);
d1327 1
d1330 1
a1330 2
				panic("uvm_fault: pgo_get got PENDing "
				    "on non-async I/O");
d1334 3
a1336 4
				UVMHIST_LOG(maphist,
				    "  pgo_get says TRY AGAIN!",0,0,0,0);
				tsleep((caddr_t)&lbolt, PVM, "fltagain2", 0);
				goto ReFault;
d1391 1
a1391 2
					panic("uvm_fault: object has no "
					    "releasepg function");
d1473 1
a1473 1
					 */
d1504 2
a1505 1
				pmap_page_protect(uobjpage, VM_PROT_NONE);
d1547 2
a1548 10
		if (anon) {
			/*
			 * In `Fill in data...' below, if
			 * uobjpage == PGO_DONTCARE, we want
			 * a zero'd, dirty page, so have
			 * uvm_pagealloc() do that for us.
			 */
			pg = uvm_pagealloc(NULL, 0, anon,
			    (uobjpage == PGO_DONTCARE) ? UVM_PGA_ZERO : 0);
		}
d1612 2
a1613 1
				pmap_page_protect(uobjpage, VM_PROT_NONE);
d1638 1
a1638 4
			/*
			 * Page is zero'd and marked dirty by uvm_pagealloc()
			 * above.
			 */
d1663 2
a1664 35
	if (pmap_enter(ufi.orig_map->pmap, ufi.orig_rvaddr, VM_PAGE_TO_PHYS(pg),
	    enter_prot, access_type | PMAP_CANFAIL | (wired ? PMAP_WIRED : 0))
	    != KERN_SUCCESS) {
		/*
		 * No need to undo what we did; we can simply think of
		 * this as the pmap throwing away the mapping information.
		 *
		 * We do, however, have to go through the ReFault path,
		 * as the map may change while we're asleep.
		 */
		if (pg->flags & PG_WANTED)
			wakeup(pg);		/* lock still held */

		/* 
		 * note that pg can't be PG_RELEASED since we did not drop
		 * the object lock since the last time we checked.
		 */
 
		pg->flags &= ~(PG_BUSY|PG_FAKE|PG_WANTED);
		UVM_PAGE_OWN(pg, NULL);
		uvmfault_unlockall(&ufi, amap, uobj, NULL);
#ifdef DIAGNOSTIC
		if (uvmexp.swpgonly > uvmexp.swpages)
			panic("uvmexp.swpgonly botch");
#endif
		if (uvmexp.swpgonly == uvmexp.swpages) {
			UVMHIST_LOG(maphist,
			    "<- failed.  out of VM",0,0,0,0);
			/* XXX instrumentation */
			return (KERN_RESOURCE_SHORTAGE);
		}
		/* XXX instrumentation */
		uvm_wait("flt_pmfail2");
		goto ReFault;
	}
d1728 2
a1729 3
	 * now fault it in a page at a time.   if the fault fails then we have
	 * to undo what we have done.   note that in uvm_fault VM_PROT_NONE 
	 * is replaced with the max protection if fault_type is VM_FAULT_WIRE.
@


1.4.4.5
log
@merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_fault.c,v 1.56 2001/02/18 21:19:08 chs Exp $	*/
d50 3
a209 3
#ifdef UBC
				pmap_clear_reference(pg);
#else
a210 1
#endif
d282 1
a282 1
 * => if we fail (result != VM_PAGER_OK) we unlock everything.
a304 2
	LOCK_ASSERT(simple_lock_held(&anon->an_lock));

d463 6
a468 2
				KASSERT(result != VM_PAGER_PEND);

d574 1
a574 1
	vaddr_t startva, objaddr, currva, offset, uoff;
d585 1
a585 2
	anon = NULL;
	pg = NULL;
d722 4
a725 2
		KASSERT(uvmadvice[ufi.entry->advice].advice ==
			 ufi.entry->advice);
d798 1
a798 1
		startva += (nback << PAGE_SHIFT);
d819 6
a824 4
		if (lcv != centeridx &&
		    pmap_extract(ufi.orig_map->pmap, currva, &pa)) {
			pages[lcv] = PGO_DONTCARE;
			continue;
a855 1

a860 1

d893 1
d899 1
a899 2
				    PGO_LOCKED|PGO_SYNCIO);

d930 1
a930 1
		(void) uobj->pgops->pgo_get(uobj, ufi.entry->offset +
d951 17
a967 1
				KASSERT((pages[lcv]->flags & PG_RELEASED) == 0);
d969 3
a971 12
				/*
				 * if center page is resident and not
				 * PG_BUSY|PG_RELEASED then pgo_get
				 * made it PG_BUSY for us and gave
				 * us a handle to it.   remember this
				 * page as "uobjpage." (for later use).
				 */
				
				if (lcv == centeridx) {
					uobjpage = pages[lcv];
					UVMHIST_LOG(maphist, "  got uobjpage "
					    "(0x%x) with locked get", 
d973 1
a973 1
					continue;
a991 1

a997 1

d1000 1
a1000 2
				    pages[lcv]->flags & PG_RDONLY ?
				    VM_PROT_READ : enter_prot & MASK(ufi.entry),
a1008 1

d1011 2
d1015 1
d1018 1
d1020 1
d1068 1
a1068 1
	 * if it fails (!OK) it will unlock everything for us.
d1076 3
a1078 3
	switch (result) {
	case VM_PAGER_OK:
		break; 
d1080 1
a1080 1
	case VM_PAGER_REFAULT:
d1083 2
a1084 2
	case VM_PAGER_AGAIN:
		tsleep(&lbolt, PVM, "fltagain1", 0);
d1086 1
d1088 2
a1089 7
	default:
#ifdef DIAGNOSTIC
		panic("uvm_fault: uvmfault_anonget -> %d", result);
#else
		return (KERN_PROTECTION_FAILURE);
#endif
	}
a1101 1

d1193 1
a1193 2
		if (anon) {
			/* new anon is locked! */
d1195 4
a1198 1
		}
d1202 1
a1202 3
			if (anon) {
				anon->an_ref--;
				simple_unlock(&anon->an_lock);
d1204 4
d1209 1
a1209 2
			uvmfault_unlockall(&ufi, amap, uobj, oanon);
			KASSERT(uvmexp.swpgonly <= uvmexp.swpages);
d1232 1
a1232 1

d1234 3
a1236 3
		 * note: oanon is still locked, as is the new anon.  we
		 * need to check for this later when we unlock oanon; if
		 * oanon != anon, we'll have to unlock anon, too.
d1240 1
a1240 1

d1249 1
a1249 1
	/* locked: maps(read), amap, oanon, anon (if different from oanon) */
a1269 2
		if (anon != oanon)
			simple_unlock(&anon->an_lock);
d1271 4
a1274 1
		KASSERT(uvmexp.swpgonly <= uvmexp.swpages);
a1314 2
	if (anon != oanon)
		simple_unlock(&anon->an_lock);
d1340 1
a1340 1
		KASSERT(uobjpage != PGO_DONTCARE);
d1369 5
a1373 4
		uoff = (ufi.orig_rvaddr - ufi.entry->start) + ufi.entry->offset;
		result = uobj->pgops->pgo_get(uobj, uoff, &uobjpage, &gotpages,
		    0, access_type & MASK(ufi.entry), ufi.entry->advice,
		    PGO_SYNCIO);
d1376 1
a1376 1

d1382 5
a1386 1
			KASSERT(result != VM_PAGER_PEND);
d1445 5
a1449 2
				KASSERT(uobj->pgops->pgo_releasepg != NULL);

d1476 1
a1502 3
		/* no anon in this case. */
		anon = NULL;

a1604 2
			 * The new anon is locked.
			 *
d1613 4
a1622 6
			if (anon != NULL) {
				anon->an_ref--;
				simple_unlock(&anon->an_lock);
				uvm_anfree(anon);
			}

d1632 1
d1635 1
d1642 5
a1646 1
			KASSERT(uvmexp.swpgonly <= uvmexp.swpages);
d1656 1
d1681 2
a1682 2
			 * since we still hold the object lock.
			 * drop handle to uobj as well.
d1691 1
a1691 1
			uvm_pageactivate(uobjpage);
a1694 1

d1711 1
d1716 1
a1716 2
	 * maps(read), amap(if !null), uobj(if !null), uobjpage(if uobj),
	 *   anon(if !null), pg(if anon)
a1728 1
	KASSERT(access_type == VM_PROT_READ || (pg->flags & PG_RDONLY) == 0);
d1730 1
a1730 2
	    pg->flags & PG_RDONLY ? VM_PROT_READ : enter_prot,
	    access_type | PMAP_CANFAIL | (wired ? PMAP_WIRED : 0))
a1731 1

a1738 1

d1749 5
a1753 2
		uvmfault_unlockall(&ufi, amap, uobj, anon);
		KASSERT(uvmexp.swpgonly <= uvmexp.swpages);
d1785 1
d1798 1
a1798 1
	uvmfault_unlockall(&ufi, amap, uobj, anon);
d1877 4
a1880 1
	KASSERT((map->flags & VM_MAP_INTRSAFE) == 0);
d1894 4
a1897 1
	KASSERT(start >= vm_map_min(map) && end <= vm_map_max(map));
d1910 4
a1913 2

		KASSERT(va >= entry->start);
d1915 5
a1919 2
			KASSERT(entry->next != &map->header &&
				entry->next->start <= entry->end);
@


1.4.4.6
log
@Merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_fault.c,v 1.67 2001/06/26 17:55:14 thorpej Exp $	*/
d62 1
a62 1
 *         |             |                         |        |
d72 1
a72 1
 *
d86 1
a86 1
 *		data is "promoted" from uobj to a new anon.
d91 1
a91 1
 * MAPS => AMAP => UOBJ => ANON => PAGE QUEUES (PQ)
d96 1
a96 1
 *
d128 1
a128 1
 *    of the information that we already have from our previous lookup,
d162 1
a162 1
 * note: index in array must match "advice" value
d198 1
a198 1

d207 1
d209 3
d255 1
a255 1
			amap_copy(ufi->map, ufi->entry, M_NOWAIT, TRUE,
d271 1
a271 1

d283 1
a283 1
 * => if we fail (result != 0) we unlock everything.
d303 1
a303 1
	int error;
d308 1
a308 1
	error = 0;
d316 1
a316 1
	/*
d349 1
a349 1
				return (0);
d376 1
a376 1

d392 1
a392 1

d394 1
a394 1
				we_own = TRUE;
d405 1
a405 1
				error = uvm_swap_get(pg, anon->an_swslot,
d432 1
a432 1
		 *   [2] I/O not OK.   free the page and cause the fault
d437 1
a437 1

d442 1
a442 1
				wakeup(pg);
d448 1
a448 1
			/*
d462 1
a462 1
				return (ERESTART);	/* refault! */
d465 3
a467 1
			if (error) {
d495 1
a495 1
				return error;
d497 1
a497 1

d516 1
a516 1
			return (ERESTART);
d524 1
a524 1
		    amap_lookup(&ufi->entry->aref,
d526 1
a526 1

d529 1
a529 1
			return (ERESTART);
d531 1
a531 1

d533 1
a533 1
		 * try it again!
d552 1
a552 1
 * => VM data structures usually should be unlocked.   however, it is
a556 1
 * => MUST NEVER BE CALLED IN INTERRUPT CONTEXT
d564 1
a564 1
	struct vm_map *orig_map;
d572 1
a572 1
	int npages, nback, nforw, centeridx, error, lcv, gotpages;
d574 1
a574 1
	paddr_t pa;
d603 13
d626 1
a626 1
		return (EFAULT);
a629 10
#ifdef DIAGNOSTIC
	if ((ufi.map->flags & VM_MAP_PAGEABLE) == 0) {
		printf("Page fault on non-pageable map:\n");
		printf("ufi.map = %p\n", ufi.map);
		printf("ufi.orig_map = %p\n", ufi.orig_map);
		printf("ufi.orig_rvaddr = 0x%lx\n", (u_long) ufi.orig_rvaddr);
		panic("uvm_fault: (ufi.map->flags & VM_MAP_PAGEABLE) == 0");
	}
#endif

d639 12
a650 1
		return EACCES;
d689 1
a689 1
			enter_prot &= ~VM_PROT_WRITE;
d709 1
a709 1
		return (EFAULT);
d715 1
a715 1
	 * to do this the first time through the fault.   if we
d740 1
a740 1

d780 1
a780 1
		if (amap)
d788 1
a788 1
			(void) uobj->pgops->pgo_flush(uobj, objaddr, objaddr +
a866 1
		pmap_update();
d871 1
a871 1
	UVMHIST_LOG(maphist, "  shadowed=%d, will_get=%d", shadowed,
d881 1
a881 1

d894 3
a896 2
		error = uobj->pgops->pgo_fault(&ufi, startva, pages, npages,
		    centeridx, fault_type, access_type, PGO_LOCKED|PGO_SYNCIO);
d900 3
a902 1
		if (error == ERESTART)
d904 2
a905 4
		/*
		 * object fault routine responsible for pmap_update().
		 */
		return error;
d958 1
a958 1

d962 1
a962 1
					    "(0x%x) with locked get",
d966 2
a967 2

				/*
d998 1
a998 1
				/*
a1006 1
			pmap_update();
d1014 1
a1014 1
	/* locked (!shadowed): maps(read), amap(if there),
d1036 1
a1036 1
	if (shadowed == FALSE)
d1065 4
a1068 4
	error = uvmfault_anonget(&ufi, amap, anon);
	switch (error) {
	case 0:
		break;
d1070 1
a1070 1
	case ERESTART:
d1073 1
a1073 1
	case EAGAIN:
d1078 5
a1082 1
		return error;
d1094 1
a1094 1
	 * special handling for loaned pages
d1100 1
a1100 1

d1176 2
a1177 2
	 * in the (hopefully very rare) case that we are out of RAM we
	 * will unlock, wait for more RAM, and refault.
d1206 1
a1206 1
				return ENOMEM;
d1254 1
a1254 1
	    != 0) {
d1270 1
a1270 1
			return ENOMEM;
d1309 1
a1309 2
	pmap_update();
	return 0;
d1330 1
a1330 1
		uobjpage = PGO_DONTCARE;
d1344 1
a1344 1
	 * if uobjpage is null, then we need to unlock and ask the pager to
d1355 1
a1355 1

d1363 1
a1363 1
		error = uobj->pgops->pgo_get(uobj, uoff, &uobjpage, &gotpages,
d1367 1
a1367 1
		/* locked: uobjpage(if no error) */
d1373 4
a1376 2
		if (error) {
			if (error == EAGAIN) {
d1379 1
a1379 1
				tsleep(&lbolt, PVM, "fltagain2", 0);
d1384 2
a1385 2
			    error, 0,0,0);
			return error;
d1399 1
a1399 1

d1410 1
a1410 1
		    (locked && amap &&
d1413 1
a1413 1
			if (locked)
d1425 1
a1425 1
			    "  wasn't able to relock after fault: retry",
d1475 1
a1475 1

d1579 1
a1579 1

d1636 1
a1636 1
				return ENOMEM;
a1660 3
				/*
				 * XXX: PAGE MIGHT BE WIRED!
				 */
d1662 1
a1662 1

d1717 2
a1718 1
	    access_type | PMAP_CANFAIL | (wired ? PMAP_WIRED : 0)) != 0) {
d1731 1
a1731 1
		/*
d1735 1
a1735 1

d1744 1
a1744 1
			return ENOMEM;
d1776 2
a1777 2
	/*
	 * note that pg can't be PG_RELEASED since we did not drop the object
d1780 1
a1780 1

a1784 2
	pmap_update();

d1786 1
a1786 1
	return 0;
d1801 1
a1801 1
	struct vm_map *map;
d1806 4
a1809 1
	int error;
d1813 1
a1813 1
	 * to undo what we have done.   note that in uvm_fault VM_PROT_NONE
a1816 8
	/*
	 * XXX work around overflowing a vaddr_t.  this prevents us from
	 * wiring the last page in the address space, though.
	 */
	if (start > end) {
		return EFAULT;
	}

d1818 2
a1819 2
		error = uvm_fault(map, va, VM_FAULT_WIRE, access_type);
		if (error) {
d1823 1
a1823 1
			return error;
d1826 2
a1827 1
	return 0;
d1836 1
a1836 1
	struct vm_map *map;
d1853 1
a1853 1
	struct vm_map *map;
d1856 1
a1856 1
	struct vm_map_entry *entry;
@


1.4.4.7
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_fault.c,v 1.51 2000/08/06 00:22:53 thorpej Exp $	*/
d62 1
a62 1
 *         |             |                         |        |        
d72 1
a72 1
 * 
d86 1
a86 1
 *		data is "promoted" from uobj to a new anon.   
d91 1
a91 1
 * MAPS => AMAP => UOBJ => ANON => PAGE QUEUES (PQ) 
d96 1
a96 1
 *  
d128 1
a128 1
 *    of the information that we already have from our previous lookup, 
d162 1
a162 1
 * note: index in array must match "advice" value 
d198 1
a198 1
	
a206 1
#ifdef UBC
a207 3
#else
				pmap_page_protect(pg, VM_PROT_NONE);
#endif
d251 1
a251 1
			amap_copy(ufi->map, ufi->entry, M_NOWAIT, TRUE, 
d267 1
a267 1
		
d279 1
a279 1
 * => if we fail (result != VM_PAGER_OK) we unlock everything.
d299 1
a299 1
	int result;
d302 3
a304 1
	result = 0;		/* XXX shut up gcc */
d312 1
a312 1
	/* 
d345 1
a345 1
				return (VM_PAGER_OK);
d372 1
a372 1
		
d388 1
a388 1
	
d390 1
a390 1
				we_own = TRUE;	
d401 1
a401 1
				result = uvm_swap_get(pg, anon->an_swslot,
d428 1
a428 1
		 *   [2] I/O not OK.   free the page and cause the fault 
d433 1
a433 1
		
d438 1
a438 1
				wakeup(pg);	
d444 1
a444 1
			/* 
d458 1
a458 1
				return (VM_PAGER_REFAULT);	/* refault! */
d461 1
a461 3
			if (result != VM_PAGER_OK) {
				KASSERT(result != VM_PAGER_PEND);

d489 1
a489 1
				return (VM_PAGER_ERROR);
d491 1
a491 1
			
d510 1
a510 1
			return (VM_PAGER_REFAULT);
d518 1
a518 1
		    amap_lookup(&ufi->entry->aref, 
d520 1
a520 1
			
d523 1
a523 1
			return (VM_PAGER_REFAULT);
d525 1
a525 1
			
d527 1
a527 1
		 * try it again! 
d546 1
a546 1
 * => VM data structures usually should be unlocked.   however, it is 
d551 1
d559 1
a559 1
	vm_map_t orig_map;
d567 1
a567 1
	int npages, nback, nforw, centeridx, result, lcv, gotpages;
d569 1
a569 1
	paddr_t pa; 
a597 13
	 * before we do anything else, if this is a fault on a kernel
	 * address, check to see if the address is managed by an
	 * interrupt-safe map.  If it is, we fail immediately.  Intrsafe
	 * maps are never pageable, and this approach avoids an evil
	 * locking mess.
	 */
	if (orig_map == kernel_map && uvmfault_check_intrsafe(&ufi)) {
		UVMHIST_LOG(maphist, "<- VA 0x%lx in intrsafe map %p",
		    ufi.orig_rvaddr, ufi.map, 0, 0);
		return (KERN_FAILURE);
	}

	/*
d608 1
a608 1
		return (KERN_INVALID_ADDRESS);
d612 10
d631 1
a631 12
		return (KERN_PROTECTION_FAILURE);
	}

	/*
	 * if the map is not a pageable map, a page fault always fails.
	 */

	if ((ufi.map->flags & VM_MAP_PAGEABLE) == 0) {
		UVMHIST_LOG(maphist,
		    "<- map %p not pageable", ufi.map, 0, 0, 0);
		uvmfault_unlockmaps(&ufi, FALSE);
		return (KERN_FAILURE);
d670 1
a670 1
			enter_prot &= ~VM_PROT_WRITE; 
d690 1
a690 1
		return (KERN_INVALID_ADDRESS);
d696 1
a696 1
	 * to do this the first time through the fault.   if we 
d721 1
a721 1
		
d761 1
a761 1
		if (amap) 
d769 1
a769 1
			(void) uobj->pgops->pgo_flush(uobj, objaddr, objaddr + 
d848 1
d853 1
a853 1
	UVMHIST_LOG(maphist, "  shadowed=%d, will_get=%d", shadowed, 
d863 1
a863 1
	
d876 2
a877 3
		result = uobj->pgops->pgo_fault(&ufi, startva, pages, npages,
				    centeridx, fault_type, access_type,
				    PGO_LOCKED);
d881 1
a881 3
		if (result == VM_PAGER_OK)
			return (KERN_SUCCESS);	/* pgo_fault did pmap enter */
		else if (result == VM_PAGER_REFAULT)
d883 4
a886 2
		else
			return (KERN_PROTECTION_FAILURE);
d939 1
a939 1
				
d943 1
a943 1
					    "(0x%x) with locked get", 
d947 2
a948 2
	
				/* 
d974 2
a975 1
				    enter_prot & MASK(ufi.entry),
d979 1
a979 1
				/* 
d988 1
d996 1
a996 1
	/* locked (!shadowed): maps(read), amap(if there), 
d1018 1
a1018 1
	if (shadowed == FALSE) 
d1047 4
a1050 4
	result = uvmfault_anonget(&ufi, amap, anon);
	switch (result) {
	case VM_PAGER_OK:
		break; 
d1052 1
a1052 1
	case VM_PAGER_REFAULT:
d1055 3
a1057 7
	case VM_PAGER_ERROR:
		/*
		 * An error occured while trying to bring in the
		 * page -- this is the only error we return right
		 * now.
		 */
		return (KERN_PROTECTION_FAILURE);	/* XXX */
d1060 1
a1060 5
#ifdef DIAGNOSTIC
		panic("uvm_fault: uvmfault_anonget -> %d", result);
#else
		return (KERN_PROTECTION_FAILURE);
#endif
d1072 1
a1072 1
	 * special handling for loaned pages 
d1078 1
a1078 1
			
d1154 2
a1155 2
	 * in the (hopefully very rare) case that we are out of RAM we 
	 * will unlock, wait for more RAM, and refault.    
d1167 1
d1173 3
a1175 1
			if (anon)
d1177 1
d1184 1
a1184 1
				return (KERN_RESOURCE_SHORTAGE);
d1204 3
a1206 3
		 * note: oanon still locked.   anon is _not_ locked, but we
		 * have the sole references to in from amap which _is_ locked.
		 * thus, no one can get at it until we are done with it.
d1219 1
a1219 1
	/* locked: maps(read), amap, oanon */
d1232 1
a1232 1
	    != KERN_SUCCESS) {
d1240 2
d1248 1
a1248 1
			return (KERN_RESOURCE_SHORTAGE);
d1284 2
d1287 2
a1288 1
	return (KERN_SUCCESS);
d1309 1
a1309 1
		uobjpage = PGO_DONTCARE;	
d1323 1
a1323 1
	 * if uobjpage is null, then we need to unlock and ask the pager to 
d1334 1
a1334 1
		
d1342 1
a1342 1
		result = uobj->pgops->pgo_get(uobj, uoff, &uobjpage, &gotpages,
d1346 1
a1346 1
		/* locked: uobjpage(if result OK) */
d1352 2
a1353 4
		if (result != VM_PAGER_OK) {
			KASSERT(result != VM_PAGER_PEND);

			if (result == VM_PAGER_AGAIN) {
d1356 1
a1356 1
				tsleep((caddr_t)&lbolt, PVM, "fltagain2", 0);
d1361 2
a1362 2
			    result, 0,0,0);
			return (KERN_PROTECTION_FAILURE); /* XXX i/o error */
d1376 1
a1376 1
		
d1387 1
a1387 1
		    (locked && amap && 
d1390 1
a1390 1
			if (locked) 
d1402 1
a1402 1
			    "  wasn't able to relock after fault: retry", 
d1452 1
a1452 1
		
d1464 3
d1556 1
a1556 1
		
d1569 2
d1585 6
d1613 1
a1613 1
				return (KERN_RESOURCE_SHORTAGE);
a1617 1
			uvm_anfree(anon);
d1638 3
d1642 1
a1642 1
			
d1680 2
a1681 1
	 * maps(read), amap(if !null), uobj(if !null), uobjpage(if uobj)
d1694 1
d1696 2
a1697 2
	    enter_prot, access_type | PMAP_CANFAIL | (wired ? PMAP_WIRED : 0))
	    != KERN_SUCCESS) {
d1710 1
a1710 1
		/* 
d1714 1
a1714 1
 
d1717 1
a1717 1
		uvmfault_unlockall(&ufi, amap, uobj, NULL);
d1723 1
a1723 1
			return (KERN_RESOURCE_SHORTAGE);
d1755 2
a1756 2
	/* 
	 * note that pg can't be PG_RELEASED since we did not drop the object 
d1759 1
a1759 1
 
d1762 3
a1764 1
	uvmfault_unlockall(&ufi, amap, uobj, NULL);
d1767 1
a1767 1
	return (KERN_SUCCESS);
d1782 1
a1782 1
	vm_map_t map;
d1787 1
a1787 4
	pmap_t  pmap;
	int rv;

	pmap = vm_map_pmap(map);
d1791 1
a1791 1
	 * to undo what we have done.   note that in uvm_fault VM_PROT_NONE 
d1795 8
d1804 2
a1805 2
		rv = uvm_fault(map, va, VM_FAULT_WIRE, access_type);
		if (rv) {
d1809 1
a1809 1
			return (rv);
d1812 1
a1812 2

	return (KERN_SUCCESS);
d1821 1
a1821 1
	vm_map_t map;
d1838 1
a1838 1
	vm_map_t map;
d1841 1
a1841 1
	vm_map_entry_t entry;
@


1.4.4.8
log
@Merge in -current from roughly a week ago
@
text
@d178 2
a179 2
static void uvmfault_amapcopy(struct uvm_faultinfo *);
static __inline void uvmfault_anonflush(struct vm_anon **, int);
@


1.4.4.9
log
@Merge with the trunk
@
text
@a864 1
		pmap_update(ufi.orig_map->pmap);
a1003 1
				pmap_update(ufi.orig_map->pmap);
a1301 1
	pmap_update(ufi.orig_map->pmap);
a1763 1
	pmap_update(ufi.orig_map->pmap);
@


1.3
log
@sync with NetBSD from 1999.05.24 (there is a reason for this date)
 Mostly cleanups, but also a few improvements to pagedaemon for better
 handling of low memory and/or low swap conditions.
@
text
@d824 1
a824 1
			    (ufi.entry->wired_count != 0));
d950 1
a950 1
				    enter_prot & MASK(ufi.entry), wired);
d1205 1
a1205 1
	    enter_prot, wired);
d1640 1
a1640 1
	    enter_prot, wired);
@


1.2
log
@add OpenBSD tags
@
text
@d1 1
a1 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_fault.c,v 1.19 1999/01/24 23:53:15 chuck Exp $	*/
a3 4
 * XXXCDC: "ROUGH DRAFT" QUALITY UVM PRE-RELEASE FILE!   
 *	   >>>USE AT YOUR OWN RISK, WORK IS NOT FINISHED<<<
 */
/*
d376 1
a376 1
			pg = uvm_pagealloc(NULL, 0, anon);
d547 3
d652 1
a652 1
			enter_prot = enter_prot & ~VM_PROT_WRITE; 
d822 2
a823 1
			    (anon->an_ref > 1) ? VM_PROT_READ : enter_prot, 
d889 1
a889 2
				UVM_ET_ISCOPYONWRITE(ufi.entry) ?
				VM_PROT_READ : access_type,
d950 1
a950 2
				    UVM_ET_ISCOPYONWRITE(ufi.entry) ?
				    VM_PROT_READ : enter_prot, wired);
d1075 1
a1075 1
				pg = uvm_pagealloc(NULL, 0, NULL);
d1138 1
a1138 1
			pg = uvm_pagealloc(NULL, 0, anon);
d1149 6
a1154 1
			if (anon == NULL) {
a1157 1
				/* XXX: OUT OF VM, ??? */
d1160 1
d1215 10
a1227 1

d1293 1
a1293 2
		    UVM_ET_ISCOPYONWRITE(ufi.entry) ?
			VM_PROT_READ : access_type,
d1424 1
a1424 1
			enter_prot = enter_prot & ~VM_PROT_WRITE;
d1443 1
a1443 1
				pg = uvm_pagealloc(NULL, 0, NULL);
d1463 2
a1464 1
					  "  out of RAM breaking loan, waiting",					  0,0,0,0);
d1524 1
a1524 1
			pg = uvm_pagealloc(NULL, 0, anon); /* BUSY+CLEAN+FAKE */
d1554 6
a1559 1
			if (anon == NULL) {
a1562 1
				/* XXX: out of VM */
d1565 1
d1646 12
a1658 1
		
a1660 1

@


1.1
log
@Import of uvm from NetBSD. Some local changes, some code disabled
@
text
@d1 1
@

