head	1.142;
access;
symbols
	OPENBSD_6_1:1.142.0.2
	OPENBSD_6_1_BASE:1.142
	OPENBSD_6_0:1.137.0.2
	OPENBSD_6_0_BASE:1.137
	OPENBSD_5_9:1.122.0.2
	OPENBSD_5_9_BASE:1.122
	OPENBSD_5_8:1.112.0.4
	OPENBSD_5_8_BASE:1.112
	OPENBSD_5_7:1.107.0.2
	OPENBSD_5_7_BASE:1.107
	OPENBSD_5_6:1.98.0.4
	OPENBSD_5_6_BASE:1.98
	OPENBSD_5_5:1.93.0.6
	OPENBSD_5_5_BASE:1.93
	OPENBSD_5_4:1.93.0.2
	OPENBSD_5_4_BASE:1.93
	OPENBSD_5_3:1.91.0.4
	OPENBSD_5_3_BASE:1.91
	OPENBSD_5_2:1.91.0.2
	OPENBSD_5_2_BASE:1.91
	OPENBSD_5_1_BASE:1.87
	OPENBSD_5_1:1.87.0.4
	OPENBSD_5_0:1.87.0.2
	OPENBSD_5_0_BASE:1.87
	OPENBSD_4_9:1.82.0.2
	OPENBSD_4_9_BASE:1.82
	OPENBSD_4_8:1.80.0.2
	OPENBSD_4_8_BASE:1.80
	OPENBSD_4_7:1.79.0.2
	OPENBSD_4_7_BASE:1.79
	OPENBSD_4_6:1.76.0.4
	OPENBSD_4_6_BASE:1.76
	OPENBSD_4_5:1.71.0.2
	OPENBSD_4_5_BASE:1.71
	OPENBSD_4_4:1.70.0.4
	OPENBSD_4_4_BASE:1.70
	OPENBSD_4_3:1.70.0.2
	OPENBSD_4_3_BASE:1.70
	OPENBSD_4_2:1.69.0.2
	OPENBSD_4_2_BASE:1.69
	OPENBSD_4_1:1.64.0.2
	OPENBSD_4_1_BASE:1.64
	OPENBSD_4_0:1.63.0.2
	OPENBSD_4_0_BASE:1.63
	OPENBSD_3_9:1.57.0.4
	OPENBSD_3_9_BASE:1.57
	OPENBSD_3_8:1.57.0.2
	OPENBSD_3_8_BASE:1.57
	OPENBSD_3_7:1.55.0.2
	OPENBSD_3_7_BASE:1.55
	OPENBSD_3_6:1.54.0.2
	OPENBSD_3_6_BASE:1.54
	SMP_SYNC_A:1.54
	SMP_SYNC_B:1.54
	OPENBSD_3_5:1.53.0.4
	OPENBSD_3_5_BASE:1.53
	OPENBSD_3_4:1.53.0.2
	OPENBSD_3_4_BASE:1.53
	UBC_SYNC_A:1.45
	OPENBSD_3_3:1.38.0.2
	OPENBSD_3_3_BASE:1.38
	OPENBSD_3_2:1.35.0.2
	OPENBSD_3_2_BASE:1.35
	OPENBSD_3_1:1.34.0.2
	OPENBSD_3_1_BASE:1.34
	UBC_SYNC_B:1.35
	UBC:1.32.0.2
	UBC_BASE:1.32
	OPENBSD_3_0:1.20.0.2
	OPENBSD_3_0_BASE:1.20
	OPENBSD_2_9_BASE:1.10
	OPENBSD_2_9:1.10.0.2
	OPENBSD_2_8:1.5.0.4
	OPENBSD_2_8_BASE:1.5
	OPENBSD_2_7:1.5.0.2
	OPENBSD_2_7_BASE:1.5
	SMP:1.4.0.4
	SMP_BASE:1.4
	kame_19991208:1.4
	OPENBSD_2_6:1.4.0.2
	OPENBSD_2_6_BASE:1.4
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.142
date	2017.01.21.05.42.03;	author guenther;	state Exp;
branches;
next	1.141;
commitid	CHRb0fCqa8XxUAMH;

1.141
date	2016.10.05.02.31.53;	author guenther;	state Exp;
branches;
next	1.140;
commitid	z4axJxYMv8DhHpqn;

1.140
date	2016.09.16.01.09.53;	author dlg;	state Exp;
branches;
next	1.139;
commitid	S1LT7BcQMYzBQOe8;

1.139
date	2016.08.18.19.59.16;	author deraadt;	state Exp;
branches;
next	1.138;
commitid	5ygPCRQIVsupW1We;

1.138
date	2016.08.08.17.15.51;	author deraadt;	state Exp;
branches;
next	1.137;
commitid	YmtE1XneIR1j678a;

1.137
date	2016.07.13.17.52.37;	author kettenis;	state Exp;
branches;
next	1.136;
commitid	a5Tam80k4Thbqwrc;

1.136
date	2016.07.13.17.49.00;	author kettenis;	state Exp;
branches;
next	1.135;
commitid	zpUvLngY5Hw7Weq3;

1.135
date	2016.07.13.15.57.35;	author kettenis;	state Exp;
branches;
next	1.134;
commitid	AYu0JR129lBR5JWe;

1.134
date	2016.06.08.15.38.28;	author deraadt;	state Exp;
branches;
next	1.133;
commitid	F211DpkSPCQIDFDO;

1.133
date	2016.06.08.15.37.20;	author deraadt;	state Exp;
branches;
next	1.132;
commitid	ajlhlMQ66DhSwXws;

1.132
date	2016.06.04.16.43.43;	author sthen;	state Exp;
branches;
next	1.131;
commitid	6XOFDu3m2CARfbgc;

1.131
date	2016.06.02.17.05.58;	author schwarze;	state Exp;
branches;
next	1.130;
commitid	O125lAQav8j1xZtF;

1.130
date	2016.06.01.04.53.54;	author guenther;	state Exp;
branches;
next	1.129;
commitid	Q8NO2OT64ejxtWtZ;

1.129
date	2016.05.30.21.31.30;	author deraadt;	state Exp;
branches;
next	1.128;
commitid	KDWNECRaSEemSTKP;

1.128
date	2016.05.30.21.25.48;	author deraadt;	state Exp;
branches;
next	1.127;
commitid	EKR1QpDNmTXzcvJY;

1.127
date	2016.05.30.21.22.46;	author deraadt;	state Exp;
branches;
next	1.126;
commitid	WldUSxuLRn3bASy2;

1.126
date	2016.05.27.19.45.04;	author deraadt;	state Exp;
branches;
next	1.125;
commitid	nQEqvpt7rKDi6fz0;

1.125
date	2016.05.11.21.52.51;	author deraadt;	state Exp;
branches;
next	1.124;
commitid	VpgRpYXqYSJy4P7J;

1.124
date	2016.03.29.12.04.26;	author chl;	state Exp;
branches;
next	1.123;
commitid	yx3qunfiuB9a5ukd;

1.123
date	2016.03.09.16.45.43;	author deraadt;	state Exp;
branches;
next	1.122;
commitid	RjqAE3tSh8ngurus;

1.122
date	2015.11.11.15.59.33;	author mmcc;	state Exp;
branches
	1.122.2.1;
next	1.121;
commitid	lKxbFcynv6SZSYfh;

1.121
date	2015.11.01.19.03.33;	author semarie;	state Exp;
branches;
next	1.120;
commitid	VKRkUfXZQNJ8UQeY;

1.120
date	2015.10.09.01.10.27;	author deraadt;	state Exp;
branches;
next	1.119;
commitid	av6ZVErLSWkVP5Zz;

1.119
date	2015.09.30.11.36.07;	author semarie;	state Exp;
branches;
next	1.118;
commitid	TMIPnGR2J26p7mlU;

1.118
date	2015.09.28.18.36.08;	author tedu;	state Exp;
branches;
next	1.117;
commitid	ijW1qCe5gqSViIUW;

1.117
date	2015.09.28.18.33.42;	author tedu;	state Exp;
branches;
next	1.116;
commitid	q0NP3po8xG1zGb8N;

1.116
date	2015.09.26.15.37.28;	author tedu;	state Exp;
branches;
next	1.115;
commitid	ZEiFD3hIpKckMFbX;

1.115
date	2015.09.23.00.16.44;	author guenther;	state Exp;
branches;
next	1.114;
commitid	8gGDzjuHZA83gbeq;

1.114
date	2015.09.06.17.06.43;	author deraadt;	state Exp;
branches;
next	1.113;
commitid	hRJP2P5vaejV6wgP;

1.113
date	2015.08.25.19.47.56;	author guenther;	state Exp;
branches;
next	1.112;
commitid	LAN35bPiXt5Xg27a;

1.112
date	2015.07.20.22.41.41;	author miod;	state Exp;
branches
	1.112.4.1;
next	1.111;
commitid	W5vJrudpAsBmqstc;

1.111
date	2015.07.20.05.49.30;	author jsg;	state Exp;
branches;
next	1.110;
commitid	yaA6rtb90AsAEAxU;

1.110
date	2015.07.20.00.56.10;	author guenther;	state Exp;
branches;
next	1.109;
commitid	pi31sn0pO464Xi5e;

1.109
date	2015.05.07.08.53.33;	author mpi;	state Exp;
branches;
next	1.108;
commitid	c9ymkCbPubGplOgk;

1.108
date	2015.03.30.21.08.40;	author miod;	state Exp;
branches;
next	1.107;
commitid	UIgWyHZZqklZyLUa;

1.107
date	2015.02.13.13.35.03;	author millert;	state Exp;
branches;
next	1.106;
commitid	AdfxL7vCLZYT5vxT;

1.106
date	2015.02.07.00.42.20;	author tedu;	state Exp;
branches;
next	1.105;
commitid	tyOUpLRjzF1Np5Wg;

1.105
date	2015.02.06.11.41.55;	author beck;	state Exp;
branches;
next	1.104;
commitid	Th5rFTtqeDivvpYH;

1.104
date	2014.12.17.06.58.11;	author guenther;	state Exp;
branches;
next	1.103;
commitid	DImukoCWyTxwdbuh;

1.103
date	2014.12.16.18.30.04;	author tedu;	state Exp;
branches;
next	1.102;
commitid	P6Av4XGqOi3rFasL;

1.102
date	2014.12.15.02.24.23;	author guenther;	state Exp;
branches;
next	1.101;
commitid	ZxaujiOM0aYQRjFY;

1.101
date	2014.12.09.07.16.41;	author doug;	state Exp;
branches;
next	1.100;
commitid	tPDMRisjAolmdVN1;

1.100
date	2014.11.16.12.31.00;	author deraadt;	state Exp;
branches;
next	1.99;
commitid	yv0ECmCdICvq576h;

1.99
date	2014.10.03.17.41.00;	author kettenis;	state Exp;
branches;
next	1.98;
commitid	h8HwsnqXqpePzqXu;

1.98
date	2014.07.12.18.44.01;	author tedu;	state Exp;
branches;
next	1.97;
commitid	bDGgAR6yEQVcVl5u;

1.97
date	2014.07.08.11.38.48;	author deraadt;	state Exp;
branches;
next	1.96;
commitid	myoIhq56iLBOzmdA;

1.96
date	2014.07.02.06.09.49;	author matthew;	state Exp;
branches;
next	1.95;
commitid	mswsoyQHeu5M87iU;

1.95
date	2014.06.27.20.50.43;	author matthew;	state Exp;
branches;
next	1.94;
commitid	cTtO84hDA01yGJfp;

1.94
date	2014.04.13.23.14.15;	author tedu;	state Exp;
branches;
next	1.93;

1.93
date	2013.05.30.16.29.46;	author tedu;	state Exp;
branches;
next	1.92;

1.92
date	2013.05.30.15.17.59;	author tedu;	state Exp;
branches;
next	1.91;

1.91
date	2012.07.21.06.46.58;	author matthew;	state Exp;
branches;
next	1.90;

1.90
date	2012.04.22.05.43.14;	author guenther;	state Exp;
branches;
next	1.89;

1.89
date	2012.04.10.10.30.44;	author ariane;	state Exp;
branches;
next	1.88;

1.88
date	2012.03.09.13.01.29;	author ariane;	state Exp;
branches;
next	1.87;

1.87
date	2011.07.09.05.31.26;	author matthew;	state Exp;
branches;
next	1.86;

1.86
date	2011.07.05.09.15.57;	author oga;	state Exp;
branches;
next	1.85;

1.85
date	2011.07.04.20.35.35;	author deraadt;	state Exp;
branches;
next	1.84;

1.84
date	2011.06.06.17.10.23;	author ariane;	state Exp;
branches;
next	1.83;

1.83
date	2011.05.24.15.27.36;	author ariane;	state Exp;
branches;
next	1.82;

1.82
date	2010.12.24.21.49.04;	author tedu;	state Exp;
branches;
next	1.81;

1.81
date	2010.12.15.04.59.53;	author tedu;	state Exp;
branches;
next	1.80;

1.80
date	2010.05.21.23.22.33;	author oga;	state Exp;
branches;
next	1.79;

1.79
date	2009.07.25.12.55.40;	author miod;	state Exp;
branches;
next	1.78;

1.78
date	2009.07.22.21.05.37;	author oga;	state Exp;
branches;
next	1.77;

1.77
date	2009.07.09.22.29.56;	author thib;	state Exp;
branches;
next	1.76;

1.76
date	2009.06.17.00.13.59;	author oga;	state Exp;
branches;
next	1.75;

1.75
date	2009.06.16.00.11.29;	author oga;	state Exp;
branches;
next	1.74;

1.74
date	2009.06.01.20.53.30;	author millert;	state Exp;
branches;
next	1.73;

1.73
date	2009.06.01.19.54.02;	author oga;	state Exp;
branches;
next	1.72;

1.72
date	2009.03.20.15.19.04;	author oga;	state Exp;
branches;
next	1.71;

1.71
date	2008.11.10.03.56.16;	author deraadt;	state Exp;
branches;
next	1.70;

1.70
date	2007.09.01.15.14.44;	author martin;	state Exp;
branches;
next	1.69;

1.69
date	2007.06.18.21.51.15;	author pedro;	state Exp;
branches;
next	1.68;

1.68
date	2007.05.31.21.20.30;	author thib;	state Exp;
branches;
next	1.67;

1.67
date	2007.03.27.16.13.46;	author art;	state Exp;
branches;
next	1.66;

1.66
date	2007.03.26.08.43.34;	author art;	state Exp;
branches;
next	1.65;

1.65
date	2007.03.25.11.31.07;	author art;	state Exp;
branches;
next	1.64;

1.64
date	2007.02.25.19.24.59;	author millert;	state Exp;
branches;
next	1.63;

1.63
date	2006.07.13.22.51.26;	author deraadt;	state Exp;
branches;
next	1.62;

1.62
date	2006.06.29.17.02.16;	author mickey;	state Exp;
branches;
next	1.61;

1.61
date	2006.06.21.16.20.05;	author mickey;	state Exp;
branches;
next	1.60;

1.60
date	2006.04.06.20.58.06;	author kurt;	state Exp;
branches;
next	1.59;

1.59
date	2006.04.04.21.10.29;	author miod;	state Exp;
branches;
next	1.58;

1.58
date	2006.03.16.21.38.35;	author miod;	state Exp;
branches;
next	1.57;

1.57
date	2005.06.01.18.34.40;	author tedu;	state Exp;
branches
	1.57.2.1
	1.57.4.1;
next	1.56;

1.56
date	2005.05.24.21.11.47;	author tedu;	state Exp;
branches;
next	1.55;

1.55
date	2005.01.15.06.54.51;	author otto;	state Exp;
branches;
next	1.54;

1.54
date	2004.05.07.22.47.47;	author tedu;	state Exp;
branches
	1.54.2.1;
next	1.53;

1.53
date	2003.09.02.17.57.12;	author tedu;	state Exp;
branches
	1.53.4.1;
next	1.52;

1.52
date	2003.09.01.18.06.44;	author henning;	state Exp;
branches;
next	1.51;

1.51
date	2003.08.15.20.32.21;	author tedu;	state Exp;
branches;
next	1.50;

1.50
date	2003.08.06.21.08.07;	author millert;	state Exp;
branches;
next	1.49;

1.49
date	2003.07.21.22.52.19;	author tedu;	state Exp;
branches;
next	1.48;

1.48
date	2003.07.01.23.23.04;	author tedu;	state Exp;
branches;
next	1.47;

1.47
date	2003.07.01.22.18.09;	author tedu;	state Exp;
branches;
next	1.46;

1.46
date	2003.05.17.14.02.06;	author grange;	state Exp;
branches;
next	1.45;

1.45
date	2003.04.28.21.32.08;	author drahn;	state Exp;
branches;
next	1.44;

1.44
date	2003.04.25.20.32.07;	author drahn;	state Exp;
branches;
next	1.43;

1.43
date	2003.04.25.18.30.18;	author drahn;	state Exp;
branches;
next	1.42;

1.42
date	2003.04.18.23.47.59;	author drahn;	state Exp;
branches;
next	1.41;

1.41
date	2003.04.17.03.50.54;	author drahn;	state Exp;
branches;
next	1.40;

1.40
date	2003.04.14.04.53.51;	author art;	state Exp;
branches;
next	1.39;

1.39
date	2003.04.07.14.47.08;	author mpech;	state Exp;
branches;
next	1.38;

1.38
date	2003.01.09.22.27.12;	author miod;	state Exp;
branches;
next	1.37;

1.37
date	2002.11.08.04.06.02;	author art;	state Exp;
branches;
next	1.36;

1.36
date	2002.10.29.18.30.21;	author art;	state Exp;
branches;
next	1.35;

1.35
date	2002.08.23.00.53.51;	author pvalchev;	state Exp;
branches;
next	1.34;

1.34
date	2002.02.14.22.46.44;	author art;	state Exp;
branches;
next	1.33;

1.33
date	2001.12.19.08.58.07;	author art;	state Exp;
branches;
next	1.32;

1.32
date	2001.12.10.02.19.34;	author art;	state Exp;
branches
	1.32.2.1;
next	1.31;

1.31
date	2001.12.04.23.22.42;	author art;	state Exp;
branches;
next	1.30;

1.30
date	2001.11.28.19.28.15;	author art;	state Exp;
branches;
next	1.29;

1.29
date	2001.11.28.13.47.40;	author art;	state Exp;
branches;
next	1.28;

1.28
date	2001.11.27.05.27.12;	author art;	state Exp;
branches;
next	1.27;

1.27
date	2001.11.12.01.26.09;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2001.11.09.03.32.23;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2001.11.07.02.55.50;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2001.11.07.01.18.01;	author art;	state Exp;
branches;
next	1.23;

1.23
date	2001.11.06.01.35.04;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2001.11.05.22.14.54;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2001.10.26.12.03.28;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2001.09.11.20.05.26;	author miod;	state Exp;
branches;
next	1.19;

1.19
date	2001.08.11.10.57.22;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2001.08.06.14.03.05;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2001.07.25.14.47.59;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2001.07.25.13.25.33;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2001.06.23.19.24.34;	author smart;	state Exp;
branches;
next	1.14;

1.14
date	2001.06.08.08.09.39;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2001.05.10.14.51.21;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2001.05.10.07.59.06;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2001.05.05.21.26.46;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2001.03.22.03.05.56;	author smart;	state Exp;
branches;
next	1.9;

1.9
date	2001.03.09.17.49.34;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2001.03.09.15.11.46;	author art;	state Exp;
branches;
next	1.7;

1.7
date	2001.03.09.14.20.52;	author art;	state Exp;
branches;
next	1.6;

1.6
date	2001.01.29.02.07.46;	author niklas;	state Exp;
branches;
next	1.5;

1.5
date	2000.03.16.22.11.04;	author art;	state Exp;
branches;
next	1.4;

1.4
date	99.08.23.08.13.24;	author art;	state Exp;
branches
	1.4.4.1;
next	1.3;

1.3
date	99.06.04.00.17.29;	author art;	state Exp;
branches;
next	1.2;

1.2
date	99.02.26.05.32.07;	author art;	state Exp;
branches;
next	1.1;

1.1
date	99.02.26.01.30.16;	author art;	state Exp;
branches;
next	;

1.4.4.1
date	2000.03.24.09.09.50;	author niklas;	state Exp;
branches;
next	1.4.4.2;

1.4.4.2
date	2001.05.14.22.47.47;	author niklas;	state Exp;
branches;
next	1.4.4.3;

1.4.4.3
date	2001.07.04.11.01.07;	author niklas;	state Exp;
branches;
next	1.4.4.4;

1.4.4.4
date	2001.10.31.03.32.14;	author nate;	state Exp;
branches;
next	1.4.4.5;

1.4.4.5
date	2001.11.13.23.02.31;	author niklas;	state Exp;
branches;
next	1.4.4.6;

1.4.4.6
date	2001.12.05.01.23.58;	author niklas;	state Exp;
branches;
next	1.4.4.7;

1.4.4.7
date	2002.03.06.02.17.14;	author niklas;	state Exp;
branches;
next	1.4.4.8;

1.4.4.8
date	2003.03.28.00.08.48;	author niklas;	state Exp;
branches;
next	1.4.4.9;

1.4.4.9
date	2003.05.13.19.36.58;	author ho;	state Exp;
branches;
next	1.4.4.10;

1.4.4.10
date	2003.06.07.11.09.09;	author ho;	state Exp;
branches;
next	1.4.4.11;

1.4.4.11
date	2004.02.19.11.01.44;	author niklas;	state Exp;
branches;
next	1.4.4.12;

1.4.4.12
date	2004.06.05.23.13.13;	author niklas;	state Exp;
branches;
next	;

1.32.2.1
date	2002.02.02.03.28.27;	author art;	state Exp;
branches;
next	1.32.2.2;

1.32.2.2
date	2002.06.11.03.33.04;	author art;	state Exp;
branches;
next	1.32.2.3;

1.32.2.3
date	2002.10.29.00.36.50;	author art;	state Exp;
branches;
next	1.32.2.4;

1.32.2.4
date	2002.11.04.18.02.33;	author art;	state Exp;
branches;
next	1.32.2.5;

1.32.2.5
date	2003.05.19.22.41.30;	author tedu;	state Exp;
branches;
next	;

1.53.4.1
date	2005.05.22.01.40.58;	author brad;	state Exp;
branches;
next	;

1.54.2.1
date	2005.05.22.01.34.27;	author brad;	state Exp;
branches;
next	;

1.57.2.1
date	2006.05.03.03.20.38;	author brad;	state Exp;
branches;
next	;

1.57.4.1
date	2006.05.03.03.57.53;	author brad;	state Exp;
branches;
next	;

1.112.4.1
date	2016.07.14.03.12.55;	author tedu;	state Exp;
branches;
next	;
commitid	q3bjkpz9JGLQeOtO;

1.122.2.1
date	2016.07.14.03.11.52;	author tedu;	state Exp;
branches;
next	;
commitid	jcaG9nePicpcA1ch;


desc
@@


1.142
log
@p_comm is the process's command and isn't per thread, so move it from
struct proc to struct process.

ok deraadt@@ kettenis@@
@
text
@/*	$OpenBSD: uvm_mmap.c,v 1.141 2016/10/05 02:31:53 guenther Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.49 2001/02/18 21:19:08 chs Exp $	*/

/*
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * Copyright (c) 1991, 1993 The Regents of the University of California.  
 * Copyright (c) 1988 University of Utah.
 * 
 * All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * the Systems Programming Group of the University of Utah Computer
 * Science Department.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed by the Charles D. Cranor,
 *	Washington University, University of California, Berkeley and 
 *	its contributors.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 * from: Utah $Hdr: vm_mmap.c 1.6 91/10/21$
 *      @@(#)vm_mmap.c   8.5 (Berkeley) 5/19/94
 * from: Id: uvm_mmap.c,v 1.1.2.14 1998/01/05 21:04:26 chuck Exp
 */

/*
 * uvm_mmap.c: system call interface into VM system, plus kernel vm_mmap
 * function.
 */
#include <sys/param.h>
#include <sys/systm.h>
#include <sys/file.h>
#include <sys/filedesc.h>
#include <sys/resourcevar.h>
#include <sys/mman.h>
#include <sys/mount.h>
#include <sys/proc.h>
#include <sys/malloc.h>
#include <sys/vnode.h>
#include <sys/conf.h>
#include <sys/signalvar.h>
#include <sys/syslog.h>
#include <sys/stat.h>
#include <sys/specdev.h>
#include <sys/stdint.h>
#include <sys/pledge.h>
#include <sys/unistd.h>		/* for KBIND* */
#include <sys/user.h>

#include <machine/exec.h>	/* for __LDPGSZ */

#include <sys/syscallargs.h>

#include <uvm/uvm.h>
#include <uvm/uvm_device.h>
#include <uvm/uvm_vnode.h>

int uvm_mmapanon(vm_map_t, vaddr_t *, vsize_t, vm_prot_t, vm_prot_t, int,
    vsize_t, struct proc *);
int uvm_mmapfile(vm_map_t, vaddr_t *, vsize_t, vm_prot_t, vm_prot_t, int,
    struct vnode *, voff_t, vsize_t, struct proc *);


/*
 * Page align addr and size, returning EINVAL on wraparound.
 */
#define ALIGN_ADDR(addr, size, pageoff)	do {				\
	pageoff = (addr & PAGE_MASK);					\
	if (pageoff != 0) {						\
		if (size > SIZE_MAX - pageoff)				\
			return (EINVAL);	/* wraparound */	\
		addr -= pageoff;					\
		size += pageoff;					\
	}								\
	if (size != 0) {						\
		size = (vsize_t)round_page(size);			\
		if (size == 0)						\
			return (EINVAL);	/* wraparound */	\
	}								\
} while (0)

/*
 * sys_mquery: provide mapping hints to applications that do fixed mappings
 *
 * flags: 0 or MAP_FIXED (MAP_FIXED - means that we insist on this addr and
 *	don't care about PMAP_PREFER or such)
 * addr: hint where we'd like to place the mapping.
 * size: size of the mapping
 * fd: fd of the file we want to map
 * off: offset within the file
 */
int
sys_mquery(struct proc *p, void *v, register_t *retval)
{
	struct sys_mquery_args /* {
		syscallarg(void *) addr;
		syscallarg(size_t) len;
		syscallarg(int) prot;
		syscallarg(int) flags;
		syscallarg(int) fd;
		syscallarg(long) pad;
		syscallarg(off_t) pos;
	} */ *uap = v;
	struct file *fp;
	voff_t uoff;
	int error;
	vaddr_t vaddr;
	int flags = 0;
	vsize_t size;
	vm_prot_t prot;
	int fd;

	vaddr = (vaddr_t) SCARG(uap, addr);
	prot = SCARG(uap, prot);
	size = (vsize_t) SCARG(uap, len);
	fd = SCARG(uap, fd);

	if ((prot & PROT_MASK) != prot)
		return (EINVAL);

	if (SCARG(uap, flags) & MAP_FIXED)
		flags |= UVM_FLAG_FIXED;

	if (fd >= 0) {
		if ((error = getvnode(p, fd, &fp)) != 0)
			return (error);
		uoff = SCARG(uap, pos);
	} else {
		fp = NULL;
		uoff = UVM_UNKNOWN_OFFSET;
	}

	if (vaddr == 0)
		vaddr = uvm_map_hint(p->p_vmspace, prot, VM_MIN_ADDRESS,
		    VM_MAXUSER_ADDRESS);

	error = uvm_map_mquery(&p->p_vmspace->vm_map, &vaddr, size, uoff,
	    flags);
	if (error == 0)
		*retval = (register_t)(vaddr);

	if (fp != NULL)
		FRELE(fp, p);
	return (error);
}

/*
 * sys_mincore: determine if pages are in core or not.
 */
/* ARGSUSED */
int
sys_mincore(struct proc *p, void *v, register_t *retval)
{
	struct sys_mincore_args /* {
		syscallarg(void *) addr;
		syscallarg(size_t) len;
		syscallarg(char *) vec;
	} */ *uap = v;
	vm_page_t m;
	char *vec, *pgi, *pgs;
	struct uvm_object *uobj;
	struct vm_amap *amap;
	struct vm_anon *anon;
	vm_map_entry_t entry, next;
	vaddr_t start, end, lim;
	vm_map_t map;
	vsize_t len, npgs;
	int error = 0; 

	map = &p->p_vmspace->vm_map;

	start = (vaddr_t)SCARG(uap, addr);
	len = SCARG(uap, len);
	vec = SCARG(uap, vec);

	if (start & PAGE_MASK)
		return (EINVAL);
	len = round_page(len);
	end = start + len;
	if (end <= start)
		return (EINVAL);

	npgs = len >> PAGE_SHIFT;

	/*
 	 * < art> Anyone trying to mincore more than 4GB of address space is
	 *	clearly insane.
	 */
	if (npgs >= (0xffffffff >> PAGE_SHIFT))
		return (E2BIG);
	pgs = mallocarray(npgs, sizeof(*pgs), M_TEMP, M_WAITOK | M_CANFAIL);
	if (pgs == NULL)
		return (ENOMEM);
	pgi = pgs;

	/*
	 * Lock down vec, so our returned status isn't outdated by
	 * storing the status byte for a page.
	 */
	if ((error = uvm_vslock(p, vec, npgs, PROT_WRITE)) != 0) {
		free(pgs, M_TEMP, npgs * sizeof(*pgs));
		return (error);
	}

	vm_map_lock_read(map);

	if (uvm_map_lookup_entry(map, start, &entry) == FALSE) {
		error = ENOMEM;
		goto out;
	}

	for (/* nothing */;
	     entry != NULL && entry->start < end;
	     entry = RBT_NEXT(uvm_map_addr, entry)) {
		KASSERT(!UVM_ET_ISSUBMAP(entry));
		KASSERT(start >= entry->start);

		/* Make sure there are no holes. */
		next = RBT_NEXT(uvm_map_addr, entry);
		if (entry->end < end &&
		     (next == NULL ||
		      next->start > entry->end)) {
			error = ENOMEM;
			goto out;
		}

		lim = end < entry->end ? end : entry->end;

		/*
		 * Special case for objects with no "real" pages.  Those
		 * are always considered resident (mapped devices).
		 */
		if (UVM_ET_ISOBJ(entry)) {
			KASSERT(!UVM_OBJ_IS_KERN_OBJECT(entry->object.uvm_obj));
			if (entry->object.uvm_obj->pgops->pgo_fault != NULL) {
				for (/* nothing */; start < lim;
				     start += PAGE_SIZE, pgi++)
					*pgi = 1;
				continue;
			}
		}

		amap = entry->aref.ar_amap;	/* top layer */
		uobj = entry->object.uvm_obj;	/* bottom layer */

		for (/* nothing */; start < lim; start += PAGE_SIZE, pgi++) {
			*pgi = 0;
			if (amap != NULL) {
				/* Check the top layer first. */
				anon = amap_lookup(&entry->aref,
				    start - entry->start);
				if (anon != NULL && anon->an_page != NULL) {
					/*
					 * Anon has the page for this entry
					 * offset.
					 */
					*pgi = 1;
				}
			}

			if (uobj != NULL && *pgi == 0) {
				/* Check the bottom layer. */
				m = uvm_pagelookup(uobj,
				    entry->offset + (start - entry->start));
				if (m != NULL) {
					/*
					 * Object has the page for this entry
					 * offset.
					 */
					*pgi = 1;
				}
			}
		}
	}

 out:
	vm_map_unlock_read(map);
	uvm_vsunlock(p, SCARG(uap, vec), npgs);
	/* now the map is unlocked we can copyout without fear. */
	if (error == 0)
		copyout(pgs, vec, npgs * sizeof(char));
	free(pgs, M_TEMP, npgs * sizeof(*pgs));
	return (error);
}

int	uvm_wxabort;

/*
 * W^X violations are only allowed on permitted filesystems.
 */
static inline int
uvm_wxcheck(struct proc *p, char *call)
{
	struct process *pr = p->p_p;
	int wxallowed = (pr->ps_textvp->v_mount &&
	    (pr->ps_textvp->v_mount->mnt_flag & MNT_WXALLOWED));

	if (wxallowed && (pr->ps_flags & PS_WXNEEDED))
		return (0);

	/* Report W^X failures, and potentially SIGABRT */
	if (pr->ps_wxcounter++ == 0)
		log(LOG_NOTICE, "%s(%d): %s W^X violation\n",
		    pr->ps_comm, pr->ps_pid, call);
	if (uvm_wxabort) {
		struct sigaction sa;

		/* Send uncatchable SIGABRT for coredump */
		memset(&sa, 0, sizeof sa);
		sa.sa_handler = SIG_DFL;
		setsigvec(p, SIGABRT, &sa);
		psignal(p, SIGABRT);
	}
	return (ENOTSUP);
}

/*
 * sys_mmap: mmap system call.
 *
 * => file offset and address may not be page aligned
 *    - if MAP_FIXED, offset and address must have remainder mod PAGE_SIZE
 *    - if address isn't page aligned the mapping starts at trunc_page(addr)
 *      and the return value is adjusted up by the page offset.
 */
int
sys_mmap(struct proc *p, void *v, register_t *retval)
{
	struct sys_mmap_args /* {
		syscallarg(void *) addr;
		syscallarg(size_t) len;
		syscallarg(int) prot;
		syscallarg(int) flags;
		syscallarg(int) fd;
		syscallarg(long) pad;
		syscallarg(off_t) pos;
	} */ *uap = v;
	vaddr_t addr;
	struct vattr va;
	off_t pos;
	vsize_t size, pageoff;
	vm_prot_t prot, maxprot;
	int flags, fd;
	vaddr_t vm_min_address = VM_MIN_ADDRESS;
	struct filedesc *fdp = p->p_fd;
	struct file *fp = NULL;
	struct vnode *vp;
	int error;

	/* first, extract syscall args from the uap. */
	addr = (vaddr_t) SCARG(uap, addr);
	size = (vsize_t) SCARG(uap, len);
	prot = SCARG(uap, prot);
	flags = SCARG(uap, flags);
	fd = SCARG(uap, fd);
	pos = SCARG(uap, pos);

	/*
	 * Validate the flags.
	 */
	if ((prot & PROT_MASK) != prot)
		return (EINVAL);
	if ((prot & (PROT_WRITE | PROT_EXEC)) == (PROT_WRITE | PROT_EXEC) &&
	    (error = uvm_wxcheck(p, "mmap")))
		return (error);

	if ((flags & MAP_FLAGMASK) != flags)
		return (EINVAL);
	if ((flags & (MAP_SHARED|MAP_PRIVATE)) == (MAP_SHARED|MAP_PRIVATE))
		return (EINVAL);
	if ((flags & (MAP_FIXED|__MAP_NOREPLACE)) == __MAP_NOREPLACE)
		return (EINVAL);
	if (size == 0)
		return (EINVAL);

	error = pledge_protexec(p, prot);
	if (error)
		return (error);

	/* align file position and save offset.  adjust size. */
	ALIGN_ADDR(pos, size, pageoff);

	/* now check (MAP_FIXED) or get (!MAP_FIXED) the "addr" */
	if (flags & MAP_FIXED) {
		/* adjust address by the same amount as we did the offset */
		addr -= pageoff;
		if (addr & PAGE_MASK)
			return (EINVAL);		/* not page aligned */

		if (addr > SIZE_MAX - size)
			return (EINVAL);		/* no wrapping! */
		if (VM_MAXUSER_ADDRESS > 0 &&
		    (addr + size) > VM_MAXUSER_ADDRESS)
			return (EINVAL);
		if (vm_min_address > 0 && addr < vm_min_address)
			return (EINVAL);

	}

	/* check for file mappings (i.e. not anonymous) and verify file. */
	if ((flags & MAP_ANON) == 0) {
		KERNEL_LOCK();
		if ((fp = fd_getfile(fdp, fd)) == NULL) {
			KERNEL_UNLOCK();
			return (EBADF);
		}

		FREF(fp);

		if (fp->f_type != DTYPE_VNODE) {
			error = ENODEV;		/* only mmap vnodes! */
			goto out;
		}
		vp = (struct vnode *)fp->f_data;	/* convert to vnode */

		if (vp->v_type != VREG && vp->v_type != VCHR &&
		    vp->v_type != VBLK) {
			error = ENODEV; /* only REG/CHR/BLK support mmap */
			goto out;
		}

		if (vp->v_type == VREG && (pos + size) < pos) {
			error = EINVAL;		/* no offset wrapping */
			goto out;
		}

		/* special case: catch SunOS style /dev/zero */
		if (vp->v_type == VCHR && iszerodev(vp->v_rdev)) {
			flags |= MAP_ANON;
			FRELE(fp, p);
			fp = NULL;
			/* XXX */
			KERNEL_UNLOCK();
			goto is_anon;
		}

		/*
		 * Old programs may not select a specific sharing type, so
		 * default to an appropriate one.
		 *
		 * XXX: how does MAP_ANON fit in the picture?
		 */
		if ((flags & (MAP_SHARED|MAP_PRIVATE)) == 0) {
#if defined(DEBUG)
			printf("WARNING: defaulted mmap() share type to"
			    " %s (pid %d comm %s)\n",
			    vp->v_type == VCHR ? "MAP_SHARED" : "MAP_PRIVATE",
			    p->p_p->ps_pid, p->p_p->ps_comm);
#endif
			if (vp->v_type == VCHR)
				flags |= MAP_SHARED;	/* for a device */
			else
				flags |= MAP_PRIVATE;	/* for a file */
		}

		/* 
		 * MAP_PRIVATE device mappings don't make sense (and aren't
		 * supported anyway).  However, some programs rely on this,
		 * so just change it to MAP_SHARED.
		 */
		if (vp->v_type == VCHR && (flags & MAP_PRIVATE) != 0) {
			flags = (flags & ~MAP_PRIVATE) | MAP_SHARED;
		}

		/* now check protection */
		maxprot = PROT_EXEC;

		/* check read access */
		if (fp->f_flag & FREAD)
			maxprot |= PROT_READ;
		else if (prot & PROT_READ) {
			error = EACCES;
			goto out;
		}

		/* check write access, shared case first */
		if (flags & MAP_SHARED) {
			/*
			 * if the file is writable, only add PROT_WRITE to
			 * maxprot if the file is not immutable, append-only.
			 * otherwise, if we have asked for PROT_WRITE, return
			 * EPERM.
			 */
			if (fp->f_flag & FWRITE) {
				if ((error =
				    VOP_GETATTR(vp, &va, p->p_ucred, p)))
					goto out;
				if ((va.va_flags & (IMMUTABLE|APPEND)) == 0)
					maxprot |= PROT_WRITE;
				else if (prot & PROT_WRITE) {
					error = EPERM;
					goto out;
				}
			} else if (prot & PROT_WRITE) {
				error = EACCES;
				goto out;
			}
		} else {
			/* MAP_PRIVATE mappings can always write to */
			maxprot |= PROT_WRITE;
		}
		if ((flags & MAP_ANON) != 0 || (flags & __MAP_NOFAULT) != 0 ||
		    ((flags & MAP_PRIVATE) != 0 && (prot & PROT_WRITE) != 0)) {
			if (p->p_rlimit[RLIMIT_DATA].rlim_cur < size ||
			    p->p_rlimit[RLIMIT_DATA].rlim_cur - size <
			    ptoa(p->p_vmspace->vm_dused)) {
				error = ENOMEM;
				goto out;
			}
		}
		error = uvm_mmapfile(&p->p_vmspace->vm_map, &addr, size, prot, maxprot,
		    flags, vp, pos, p->p_rlimit[RLIMIT_MEMLOCK].rlim_cur, p);
	} else {		/* MAP_ANON case */
		/*
		 * XXX What do we do about (MAP_SHARED|MAP_PRIVATE) == 0?
		 */
		if (fd != -1)
			return EINVAL;

is_anon:	/* label for SunOS style /dev/zero */

		if ((flags & MAP_ANON) != 0 || (flags & __MAP_NOFAULT) != 0 ||
		    ((flags & MAP_PRIVATE) != 0 && (prot & PROT_WRITE) != 0)) {
			if (p->p_rlimit[RLIMIT_DATA].rlim_cur < size ||
			    p->p_rlimit[RLIMIT_DATA].rlim_cur - size <
			    ptoa(p->p_vmspace->vm_dused)) {
				return ENOMEM;
			}
		}
		maxprot = PROT_MASK;
		error = uvm_mmapanon(&p->p_vmspace->vm_map, &addr, size, prot, maxprot,
		    flags, p->p_rlimit[RLIMIT_MEMLOCK].rlim_cur, p);
	}

	if (error == 0)
		/* remember to add offset */
		*retval = (register_t)(addr + pageoff);

out:
	if (fp) {
		FRELE(fp, p);
		KERNEL_UNLOCK();
	}
	return (error);
}

/*
 * sys_msync: the msync system call (a front-end for flush)
 */

int
sys_msync(struct proc *p, void *v, register_t *retval)
{
	struct sys_msync_args /* {
		syscallarg(void *) addr;
		syscallarg(size_t) len;
		syscallarg(int) flags;
	} */ *uap = v;
	vaddr_t addr;
	vsize_t size, pageoff;
	vm_map_t map;
	int flags, uvmflags;

	/* extract syscall args from the uap */
	addr = (vaddr_t)SCARG(uap, addr);
	size = (vsize_t)SCARG(uap, len);
	flags = SCARG(uap, flags);

	/* sanity check flags */
	if ((flags & ~(MS_ASYNC | MS_SYNC | MS_INVALIDATE)) != 0 ||
			(flags & (MS_ASYNC | MS_SYNC | MS_INVALIDATE)) == 0 ||
			(flags & (MS_ASYNC | MS_SYNC)) == (MS_ASYNC | MS_SYNC))
		return (EINVAL);
	if ((flags & (MS_ASYNC | MS_SYNC)) == 0)
		flags |= MS_SYNC;

	/* align the address to a page boundary, and adjust the size accordingly */
	ALIGN_ADDR(addr, size, pageoff);
	if (addr > SIZE_MAX - size)
		return (EINVAL);		/* disallow wrap-around. */

	/* get map */
	map = &p->p_vmspace->vm_map;

	/* translate MS_ flags into PGO_ flags */
	uvmflags = PGO_CLEANIT;
	if (flags & MS_INVALIDATE)
		uvmflags |= PGO_FREE;
	if (flags & MS_SYNC)
		uvmflags |= PGO_SYNCIO;
	else
		uvmflags |= PGO_SYNCIO;	 /* XXXCDC: force sync for now! */

	return (uvm_map_clean(map, addr, addr+size, uvmflags));
}

/*
 * sys_munmap: unmap a users memory
 */
int
sys_munmap(struct proc *p, void *v, register_t *retval)
{
	struct sys_munmap_args /* {
		syscallarg(void *) addr;
		syscallarg(size_t) len;
	} */ *uap = v;
	vaddr_t addr;
	vsize_t size, pageoff;
	vm_map_t map;
	vaddr_t vm_min_address = VM_MIN_ADDRESS;
	struct uvm_map_deadq dead_entries;

	/* get syscall args... */
	addr = (vaddr_t) SCARG(uap, addr);
	size = (vsize_t) SCARG(uap, len);
	
	/* align address to a page boundary, and adjust size accordingly */
	ALIGN_ADDR(addr, size, pageoff);

	/*
	 * Check for illegal addresses.  Watch out for address wrap...
	 * Note that VM_*_ADDRESS are not constants due to casts (argh).
	 */
	if (addr > SIZE_MAX - size)
		return (EINVAL);
	if (VM_MAXUSER_ADDRESS > 0 && addr + size > VM_MAXUSER_ADDRESS)
		return (EINVAL);
	if (vm_min_address > 0 && addr < vm_min_address)
		return (EINVAL);
	map = &p->p_vmspace->vm_map;


	vm_map_lock(map);	/* lock map so we can checkprot */

	/*
	 * interesting system call semantic: make sure entire range is 
	 * allocated before allowing an unmap.
	 */
	if (!uvm_map_checkprot(map, addr, addr + size, PROT_NONE)) {
		vm_map_unlock(map);
		return (EINVAL);
	}

	TAILQ_INIT(&dead_entries);
	uvm_unmap_remove(map, addr, addr + size, &dead_entries, FALSE, TRUE);

	vm_map_unlock(map);	/* and unlock */

	uvm_unmap_detach(&dead_entries, 0);

	return (0);
}

/*
 * sys_mprotect: the mprotect system call
 */
int
sys_mprotect(struct proc *p, void *v, register_t *retval)
{
	struct sys_mprotect_args /* {
		syscallarg(void *) addr;
		syscallarg(size_t) len;
		syscallarg(int) prot;
	} */ *uap = v;
	vaddr_t addr;
	vsize_t size, pageoff;
	vm_prot_t prot;
	int error;

	/*
	 * extract syscall args from uap
	 */

	addr = (vaddr_t)SCARG(uap, addr);
	size = (vsize_t)SCARG(uap, len);
	prot = SCARG(uap, prot);
	
	if ((prot & PROT_MASK) != prot)
		return (EINVAL);
	if ((prot & (PROT_WRITE | PROT_EXEC)) == (PROT_WRITE | PROT_EXEC) &&
	    (error = uvm_wxcheck(p, "mprotect")))
		return (error);

	error = pledge_protexec(p, prot);
	if (error)
		return (error);

	/*
	 * align the address to a page boundary, and adjust the size accordingly
	 */
	ALIGN_ADDR(addr, size, pageoff);
	if (addr > SIZE_MAX - size)
		return (EINVAL);		/* disallow wrap-around. */

	return (uvm_map_protect(&p->p_vmspace->vm_map, addr, addr+size,
	    prot, FALSE));
}

/*
 * sys_minherit: the minherit system call
 */
int
sys_minherit(struct proc *p, void *v, register_t *retval)
{
	struct sys_minherit_args /* {
		syscallarg(void *) addr;
		syscallarg(size_t) len;
		syscallarg(int) inherit;
	} */ *uap = v;
	vaddr_t addr;
	vsize_t size, pageoff;
	vm_inherit_t inherit;
	
	addr = (vaddr_t)SCARG(uap, addr);
	size = (vsize_t)SCARG(uap, len);
	inherit = SCARG(uap, inherit);

	/*
	 * align the address to a page boundary, and adjust the size accordingly
	 */
	ALIGN_ADDR(addr, size, pageoff);
	if (addr > SIZE_MAX - size)
		return (EINVAL);		/* disallow wrap-around. */
	
	return (uvm_map_inherit(&p->p_vmspace->vm_map, addr, addr+size,
	    inherit));
}

/*
 * sys_madvise: give advice about memory usage.
 */
/* ARGSUSED */
int
sys_madvise(struct proc *p, void *v, register_t *retval)
{
	struct sys_madvise_args /* {
		syscallarg(void *) addr;
		syscallarg(size_t) len;
		syscallarg(int) behav;
	} */ *uap = v;
	vaddr_t addr;
	vsize_t size, pageoff;
	int advice, error;
	
	addr = (vaddr_t)SCARG(uap, addr);
	size = (vsize_t)SCARG(uap, len);
	advice = SCARG(uap, behav);

	/*
	 * align the address to a page boundary, and adjust the size accordingly
	 */
	ALIGN_ADDR(addr, size, pageoff);
	if (addr > SIZE_MAX - size)
		return (EINVAL);		/* disallow wrap-around. */

	switch (advice) {
	case MADV_NORMAL:
	case MADV_RANDOM:
	case MADV_SEQUENTIAL:
		error = uvm_map_advice(&p->p_vmspace->vm_map, addr,
		    addr + size, advice);
		break;

	case MADV_WILLNEED:
		/*
		 * Activate all these pages, pre-faulting them in if
		 * necessary.
		 */
		/*
		 * XXX IMPLEMENT ME.
		 * Should invent a "weak" mode for uvm_fault()
		 * which would only do the PGO_LOCKED pgo_get().
		 */
		return (0);

	case MADV_DONTNEED:
		/*
		 * Deactivate all these pages.  We don't need them
		 * any more.  We don't, however, toss the data in
		 * the pages.
		 */
		error = uvm_map_clean(&p->p_vmspace->vm_map, addr, addr + size,
		    PGO_DEACTIVATE);
		break;

	case MADV_FREE:
		/*
		 * These pages contain no valid data, and may be
		 * garbage-collected.  Toss all resources, including
		 * any swap space in use.
		 */
		error = uvm_map_clean(&p->p_vmspace->vm_map, addr, addr + size,
		    PGO_FREE);
		break;

	case MADV_SPACEAVAIL:
		/*
		 * XXXMRG What is this?  I think it's:
		 *
		 *	Ensure that we have allocated backing-store
		 *	for these pages.
		 *
		 * This is going to require changes to the page daemon,
		 * as it will free swap space allocated to pages in core.
		 * There's also what to do for device/file/anonymous memory.
		 */
		return (EINVAL);

	default:
		return (EINVAL);
	}

	return (error);
}

/*
 * sys_mlock: memory lock
 */

int
sys_mlock(struct proc *p, void *v, register_t *retval)
{
	struct sys_mlock_args /* {
		syscallarg(const void *) addr;
		syscallarg(size_t) len;
	} */ *uap = v;
	vaddr_t addr;
	vsize_t size, pageoff;
	int error;

	/* extract syscall args from uap */
	addr = (vaddr_t)SCARG(uap, addr);
	size = (vsize_t)SCARG(uap, len);

	/* align address to a page boundary and adjust size accordingly */
	ALIGN_ADDR(addr, size, pageoff);
	if (addr > SIZE_MAX - size)
		return (EINVAL);		/* disallow wrap-around. */

	if (atop(size) + uvmexp.wired > uvmexp.wiredmax)
		return (EAGAIN);

#ifdef pmap_wired_count
	if (size + ptoa(pmap_wired_count(vm_map_pmap(&p->p_vmspace->vm_map))) >
			p->p_rlimit[RLIMIT_MEMLOCK].rlim_cur)
		return (EAGAIN);
#else
	if ((error = suser(p, 0)) != 0)
		return (error);
#endif

	error = uvm_map_pageable(&p->p_vmspace->vm_map, addr, addr+size, FALSE,
	    0);
	return (error == 0 ? 0 : ENOMEM);
}

/*
 * sys_munlock: unlock wired pages
 */

int
sys_munlock(struct proc *p, void *v, register_t *retval)
{
	struct sys_munlock_args /* {
		syscallarg(const void *) addr;
		syscallarg(size_t) len;
	} */ *uap = v;
	vaddr_t addr;
	vsize_t size, pageoff;
	int error;

	/* extract syscall args from uap */
	addr = (vaddr_t)SCARG(uap, addr);
	size = (vsize_t)SCARG(uap, len);

	/* align address to a page boundary, and adjust size accordingly */
	ALIGN_ADDR(addr, size, pageoff);
	if (addr > SIZE_MAX - size)
		return (EINVAL);		/* disallow wrap-around. */

#ifndef pmap_wired_count
	if ((error = suser(p, 0)) != 0)
		return (error);
#endif

	error = uvm_map_pageable(&p->p_vmspace->vm_map, addr, addr+size, TRUE,
	    0);
	return (error == 0 ? 0 : ENOMEM);
}

/*
 * sys_mlockall: lock all pages mapped into an address space.
 */
int
sys_mlockall(struct proc *p, void *v, register_t *retval)
{
	struct sys_mlockall_args /* {
		syscallarg(int) flags;
	} */ *uap = v;
	int error, flags;

	flags = SCARG(uap, flags);

	if (flags == 0 ||
	    (flags & ~(MCL_CURRENT|MCL_FUTURE)) != 0)
		return (EINVAL);

#ifndef pmap_wired_count
	if ((error = suser(p, 0)) != 0)
		return (error);
#endif

	error = uvm_map_pageable_all(&p->p_vmspace->vm_map, flags,
	    p->p_rlimit[RLIMIT_MEMLOCK].rlim_cur);
	if (error != 0 && error != ENOMEM)
		return (EAGAIN);
	return (error);
}

/*
 * sys_munlockall: unlock all pages mapped into an address space.
 */
int
sys_munlockall(struct proc *p, void *v, register_t *retval)
{

	(void) uvm_map_pageable_all(&p->p_vmspace->vm_map, 0, 0);
	return (0);
}

/*
 * common code for mmapanon and mmapfile to lock a mmaping
 */
int
uvm_mmaplock(vm_map_t map, vaddr_t *addr, vsize_t size, vm_prot_t prot,
    vsize_t locklimit)
{
	int error;

	/*
	 * POSIX 1003.1b -- if our address space was configured
	 * to lock all future mappings, wire the one we just made.
	 */
	if (prot == PROT_NONE) {
		/*
		 * No more work to do in this case.
		 */
		return (0);
	}

	vm_map_lock(map);
	if (map->flags & VM_MAP_WIREFUTURE) {
		KERNEL_LOCK();
		if ((atop(size) + uvmexp.wired) > uvmexp.wiredmax
#ifdef pmap_wired_count
		    || (locklimit != 0 && (size +
			 ptoa(pmap_wired_count(vm_map_pmap(map)))) >
			locklimit)
#endif
		) {
			error = ENOMEM;
			vm_map_unlock(map);
			/* unmap the region! */
			uvm_unmap(map, *addr, *addr + size);
			KERNEL_UNLOCK();
			return (error);
		}
		/*
		 * uvm_map_pageable() always returns the map
		 * unlocked.
		 */
		error = uvm_map_pageable(map, *addr, *addr + size,
		    FALSE, UVM_LK_ENTER);
		if (error != 0) {
			/* unmap the region! */
			uvm_unmap(map, *addr, *addr + size);
			KERNEL_UNLOCK();
			return (error);
		}
		KERNEL_UNLOCK();
		return (0);
	}
	vm_map_unlock(map);
	return (0);
}

/*
 * uvm_mmapanon: internal version of mmap for anons
 *
 * - used by sys_mmap
 */
int
uvm_mmapanon(vm_map_t map, vaddr_t *addr, vsize_t size, vm_prot_t prot,
    vm_prot_t maxprot, int flags, vsize_t locklimit, struct proc *p)
{
	int error;
	int advice = MADV_NORMAL;
	unsigned int uvmflag = 0;
	vsize_t align = 0;	/* userland page size */

	/*
	 * for non-fixed mappings, round off the suggested address.
	 * for fixed mappings, check alignment and zap old mappings.
	 */
	if ((flags & MAP_FIXED) == 0) {
		*addr = round_page(*addr);	/* round */
	} else {
		if (*addr & PAGE_MASK)
			return(EINVAL);

		uvmflag |= UVM_FLAG_FIXED;
		if ((flags & __MAP_NOREPLACE) == 0)
			uvmflag |= UVM_FLAG_UNMAP;
	}

	if ((flags & MAP_FIXED) == 0 && size >= __LDPGSZ)
		align = __LDPGSZ;
	if ((flags & MAP_SHARED) == 0)
		/* XXX: defer amap create */
		uvmflag |= UVM_FLAG_COPYONW;
	else
		/* shared: create amap now */
		uvmflag |= UVM_FLAG_OVERLAY;

	/* set up mapping flags */
	uvmflag = UVM_MAPFLAG(prot, maxprot,
	    (flags & MAP_SHARED) ? MAP_INHERIT_SHARE : MAP_INHERIT_COPY,
	    advice, uvmflag);

	error = uvm_mapanon(map, addr, size, align, uvmflag);

	if (error == 0)
		error = uvm_mmaplock(map, addr, size, prot, locklimit);
	return error;
}

/*
 * uvm_mmapfile: internal version of mmap for non-anons
 *
 * - used by sys_mmap
 * - caller must page-align the file offset
 */
int
uvm_mmapfile(vm_map_t map, vaddr_t *addr, vsize_t size, vm_prot_t prot,
    vm_prot_t maxprot, int flags, struct vnode *vp, voff_t foff,
    vsize_t locklimit, struct proc *p)
{
	struct uvm_object *uobj;
	int error;
	int advice = MADV_NORMAL;
	unsigned int uvmflag = 0;
	vsize_t align = 0;	/* userland page size */

	/*
	 * for non-fixed mappings, round off the suggested address.
	 * for fixed mappings, check alignment and zap old mappings.
	 */
	if ((flags & MAP_FIXED) == 0) {
		*addr = round_page(*addr);	/* round */
	} else {
		if (*addr & PAGE_MASK)
			return(EINVAL);

		uvmflag |= UVM_FLAG_FIXED;
		if ((flags & __MAP_NOREPLACE) == 0)
			uvmflag |= UVM_FLAG_UNMAP;
	}

	/*
	 * attach to underlying vm object.
	 */
	if (vp->v_type != VCHR) {
		uobj = uvn_attach(vp, (flags & MAP_SHARED) ?
		   maxprot : (maxprot & ~PROT_WRITE));

		/*
		 * XXXCDC: hack from old code
		 * don't allow vnodes which have been mapped
		 * shared-writeable to persist [forces them to be
		 * flushed out when last reference goes].
		 * XXXCDC: interesting side effect: avoids a bug.
		 * note that in WRITE [ufs_readwrite.c] that we
		 * allocate buffer, uncache, and then do the write.
		 * the problem with this is that if the uncache causes
		 * VM data to be flushed to the same area of the file
		 * we are writing to... in that case we've got the
		 * buffer locked and our process goes to sleep forever.
		 *
		 * XXXCDC: checking maxprot protects us from the
		 * "persistbug" program but this is not a long term
		 * solution.
		 *
		 * XXXCDC: we don't bother calling uncache with the vp
		 * VOP_LOCKed since we know that we are already
		 * holding a valid reference to the uvn (from the
		 * uvn_attach above), and thus it is impossible for
		 * the uncache to kill the uvn and trigger I/O.
		 */
		if (flags & MAP_SHARED) {
			if ((prot & PROT_WRITE) ||
			    (maxprot & PROT_WRITE)) {
				uvm_vnp_uncache(vp);
			}
		}
	} else {
		uobj = udv_attach(vp->v_rdev,
		    (flags & MAP_SHARED) ? maxprot :
		    (maxprot & ~PROT_WRITE), foff, size);
		/*
		 * XXX Some devices don't like to be mapped with
		 * XXX PROT_EXEC, but we don't really have a
		 * XXX better way of handling this, right now
		 */
		if (uobj == NULL && (prot & PROT_EXEC) == 0) {
			maxprot &= ~PROT_EXEC;
			uobj = udv_attach(vp->v_rdev,
			    (flags & MAP_SHARED) ? maxprot :
			    (maxprot & ~PROT_WRITE), foff, size);
		}
		advice = MADV_RANDOM;
	}

	if (uobj == NULL)
		return((vp->v_type == VREG) ? ENOMEM : EINVAL);

	if ((flags & MAP_SHARED) == 0)
		uvmflag |= UVM_FLAG_COPYONW;
	if (flags & __MAP_NOFAULT)
		uvmflag |= (UVM_FLAG_NOFAULT | UVM_FLAG_OVERLAY);

	/* set up mapping flags */
	uvmflag = UVM_MAPFLAG(prot, maxprot,
	    (flags & MAP_SHARED) ? MAP_INHERIT_SHARE : MAP_INHERIT_COPY,
	    advice, uvmflag);

	error = uvm_map(map, addr, size, uobj, foff, align, uvmflag);

	if (error == 0)
		return uvm_mmaplock(map, addr, size, prot, locklimit);

	/* errors: first detach from the uobj, if any.  */
	if (uobj)
		uobj->pgops->pgo_detach(uobj);

	return (error);
}

/* an address that can't be in userspace */
#define	BOGO_PC	(KERNBASE + 1)
int
sys_kbind(struct proc *p, void *v, register_t *retval)
{
	struct sys_kbind_args /* {
		syscallarg(const struct __kbind *) param;
		syscallarg(size_t) psize;
		syscallarg(uint64_t) proc_cookie;
	} */ *uap = v;
	const struct __kbind *paramp;
	union {
		struct __kbind uk[KBIND_BLOCK_MAX];
		char upad[KBIND_BLOCK_MAX * sizeof(*paramp) + KBIND_DATA_MAX];
	} param;
	struct uvm_map_deadq dead_entries;
	struct process *pr = p->p_p;
	const char *data;
	vaddr_t baseva, last_baseva, endva, pageoffset, kva;
	size_t psize, s;
	u_long pc;
	int count, i;
	int error;

	/*
	 * extract syscall args from uap
	 */
	paramp = SCARG(uap, param);
	psize = SCARG(uap, psize);

	/* a NULL paramp disables the syscall for the process */
	if (paramp == NULL) {
		pr->ps_kbind_addr = BOGO_PC;
		return (0);
	}

	/* security checks */
	pc = PROC_PC(p);
	if (pr->ps_kbind_addr == 0) {
		pr->ps_kbind_addr = pc;
		pr->ps_kbind_cookie = SCARG(uap, proc_cookie);
	} else if (pc != pr->ps_kbind_addr || pc == BOGO_PC)
		sigexit(p, SIGILL);
	else if (pr->ps_kbind_cookie != SCARG(uap, proc_cookie))
		sigexit(p, SIGILL);
	if (psize < sizeof(struct __kbind) || psize > sizeof(param))
		return (EINVAL);
	if ((error = copyin(paramp, &param, psize)))
		return (error);

	/*
	 * The param argument points to an array of __kbind structures
	 * followed by the corresponding new data areas for them.  Verify
	 * that the sizes in the __kbind structures add up to the total
	 * size and find the start of the new area.
	 */
	paramp = &param.uk[0];
	s = psize;
	for (count = 0; s > 0 && count < KBIND_BLOCK_MAX; count++) {
		if (s < sizeof(*paramp))
			return (EINVAL);
		s -= sizeof(*paramp);

		baseva = (vaddr_t)paramp[count].kb_addr;
		endva = baseva + paramp[count].kb_size - 1;
		if (paramp[count].kb_addr == NULL ||
		    paramp[count].kb_size == 0 ||
		    paramp[count].kb_size > KBIND_DATA_MAX ||
		    baseva >= VM_MAXUSER_ADDRESS ||
		    endva >= VM_MAXUSER_ADDRESS ||
		    trunc_page(baseva) != trunc_page(endva) ||
		    s < paramp[count].kb_size)
			return (EINVAL);

		s -= paramp[count].kb_size;
	}
	if (s > 0)
		return (EINVAL);
	data = (const char *)&paramp[count];

	/* all looks good, so do the bindings */
	last_baseva = VM_MAXUSER_ADDRESS;
	kva = 0;
	TAILQ_INIT(&dead_entries);
	for (i = 0; i < count; i++) {
		baseva = (vaddr_t)paramp[i].kb_addr;
		pageoffset = baseva & PAGE_MASK;
		baseva = trunc_page(baseva);

		/* make sure sure the desired page is mapped into kernel_map */
		if (baseva != last_baseva) {
			if (kva != 0) {
				vm_map_lock(kernel_map);
				uvm_unmap_remove(kernel_map, kva,
				    kva+PAGE_SIZE, &dead_entries, FALSE, TRUE);
				vm_map_unlock(kernel_map);
				kva = 0;
			}
			if ((error = uvm_map_extract(&p->p_vmspace->vm_map,
			    baseva, PAGE_SIZE, &kva, UVM_EXTRACT_FIXPROT)))
				break;
			last_baseva = baseva;
		}

		/* do the update */
		if ((error = kcopy(data, (char *)kva + pageoffset,
		    paramp[i].kb_size)))
			break;
		data += paramp[i].kb_size;
	}

	if (kva != 0) {
		vm_map_lock(kernel_map);
		uvm_unmap_remove(kernel_map, kva, kva+PAGE_SIZE,
		    &dead_entries, FALSE, TRUE);
		vm_map_unlock(kernel_map);
	}
	uvm_unmap_detach(&dead_entries, AMAP_REFALL);

	return (error);
}
@


1.141
log
@Display/test/use the process PID, not the thread's TID, in a few places.

ok mpi@@ mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.140 2016/09/16 01:09:53 dlg Exp $	*/
d327 1
a327 1
		    p->p_comm, p->p_p->ps_pid, call);
d470 1
a470 1
			    p->p_p->ps_pid, p->p_comm);
@


1.140
log
@move the uvm_map_addr RB tree from RB macros to the RBT functions

this tree is interesting because it uses all the red black tree
features, specifically the augment callback thats called on tree
topology changes, and it poisons and checks entries as theyre removed
from and inserted back into the tree respectively.

ok stefan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.139 2016/08/18 19:59:16 deraadt Exp $	*/
d327 1
a327 1
		    p->p_comm, p->p_pid, call);
d467 4
a470 4
			printf("WARNING: defaulted mmap() share type to "
			   "%s (pid %d comm %s)\n", vp->v_type == VCHR ?
			   "MAP_SHARED" : "MAP_PRIVATE", p->p_pid,
			    p->p_comm);
@


1.139
log
@uvm_wxcheck() should only abort the process if kern.wxabort is set.
The new semantics are W^X violations are reported to the application
via ENOTSUP.  Forgot to fix this during the last change.
Spotted by kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.138 2016/08/08 17:15:51 deraadt Exp $	*/
d237 1
a237 1
	     entry = RB_NEXT(uvm_map_addr, &map->addr, entry)) {
d242 1
a242 1
		next = RB_NEXT(uvm_map_addr, &map->addr, entry);
@


1.138
log
@W^X violations are only permitted for binaries marked "wxneeded" on
"wxallowed" filesystems.  mmap(2) & mprotect(2) now return ENOTSUP.
(To diagnose buggy programs, consider using sysctl kern.wxabort=1 and
looking at the coredumps)
ok kettenis tedu naddy
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.137 2016/07/13 17:52:37 kettenis Exp $	*/
d328 1
a328 1
	if (!wxallowed || uvm_wxabort) {
@


1.137
log
@Since mappings established using __MAP_NOFAULT will be converted into anonymous
memory if the file backing the mapping is truncated, we should check resource
limits.  This prevents callers from triggering a kernel panic and a potential
integer overflow in the amap code by forcing the allocation of too many slots.

Based on an analysis from Jesse Hertz and Tim Newsham.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.136 2016/07/13 17:49:00 kettenis Exp $	*/
d337 1
a337 1
	return (0);		/* ENOTSUP later */
@


1.136
log
@Revert previous; the __MAP_NOFAULT test is inverted and the commit message is
wrong.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.135 2016/07/13 15:57:35 kettenis Exp $	*/
d524 1
a524 1
		if ((flags & MAP_ANON) != 0 ||
d544 1
a544 1
		if ((flags & MAP_ANON) != 0 ||
@


1.135
log
@Since mappings established using __MAP_NOFAIL will be converted into anonymous
memory if the file backing the mapping is truncated, we should check resource
limits.  This prevents callers from triggering a kernel panic and a potential
integer overflow in the amap code by forcing the allocation of too many slots.

Based on an analysis from Jesse Hertz and Tim Newsham.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.134 2016/06/08 15:38:28 deraadt Exp $	*/
d524 1
a524 1
		if ((flags & MAP_ANON) != 0 || (flags & __MAP_NOFAULT) == 0 ||
d544 1
a544 1
		if ((flags & MAP_ANON) != 0 || (flags & __MAP_NOFAULT) == 0 ||
@


1.134
log
@Dereference p_p once rather than 4 times.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.133 2016/06/08 15:37:20 deraadt Exp $	*/
d524 1
a524 1
		if ((flags & MAP_ANON) != 0 ||
d544 1
a544 1
		if ((flags & MAP_ANON) != 0 ||
@


1.133
log
@hppa & mips64 now can do the full W^X check.  (Make sure you have
a new kernel before this change, and ld.so updated)
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.132 2016/06/04 16:43:43 sthen Exp $	*/
d317 3
a319 2
	int wxallowed = (p->p_p->ps_textvp->v_mount &&
	    (p->p_p->ps_textvp->v_mount->mnt_flag & MNT_WXALLOWED));
d321 1
a321 1
	if (wxallowed && (p->p_p->ps_flags & PS_WXNEEDED))
d325 1
a325 1
	if (p->p_p->ps_wxcounter++ == 0)
@


1.132
log
@If a process trips the W^X violation check, abort it unless it came
from a filesystem with the wxallowed flag set.  ok deraadt

Current status:

Filesystem	Binary		Action
----------	------		------
wxallowed	normal		violation -> log but don't abort
wxallowed	wxneeded	W^X silently allowed
normal		normal		violation -> abort
normal		wxneeded	process won't run at all

See http://www.openbsd.org/faq/current.html#r20160527
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.131 2016/06/02 17:05:58 schwarze Exp $	*/
a316 4
#if (defined(__mips64__) || defined(__hppa))
	/* XXX got/plt repairs still needed */
	return 0;
#endif
@


1.131
log
@Prevent vsize_t underflow when checking RLIMIT_DATA, which made the
check ineffective when you already had more memory than your limit
allowed.

I noticed after writing this diff that millert@@ already committed a fix
for this in rev. 1.74 (2009/06/01), but it got backed out with the giant
pmemrange backout two weeks later and was never restored.

OK tedu@@ ("just fix it" and "go ahead with your version")
stefan@@ also agrees that a check is needed.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.130 2016/06/01 04:53:54 guenther Exp $	*/
d331 1
a331 1
	if (uvm_wxabort) {
@


1.130
log
@Delete the kernel compat bits for old mmap() MAP_OLD* flags

ok deraadt@@ matthew@@ jca@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.129 2016/05/30 21:31:30 deraadt Exp $	*/
d529 3
a531 2
			if (size >
			    (p->p_rlimit[RLIMIT_DATA].rlim_cur - ptoa(p->p_vmspace->vm_dused))) {
d549 3
a551 2
			if (size >
			    (p->p_rlimit[RLIMIT_DATA].rlim_cur - ptoa(p->p_vmspace->vm_dused))) {
@


1.129
log
@Identify W^X labelled binaries at execve() time based upon WX_OPENBSD_WXNEEDED
flag set by ld -zwxneeded.  Such binaries are allowed to run only on wxallowed
mountpoints.  They do not report mmap/mprotect problems.

Rate limit mmap/mprotect reports from other binaries.

These semantics are chosen to encourage progress in the ports ecosystem,
without overwhelming the developers who work in the area.
ok sthen kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.127 2016/05/30 21:22:46 deraadt Exp $	*/
d384 1
a384 2
	 * Fixup the old deprecated MAP_COPY into MAP_PRIVATE, and
	 * validate the flags.
a393 2
	if (flags & MAP_OLDCOPY)
		flags = (flags & MAP_OLDCOPY) | MAP_PRIVATE;
@


1.128
log
@backout to insert correct commit message
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.126 2016/05/27 19:45:04 deraadt Exp $	*/
d315 1
a315 1
uvm_wxcheck(struct proc *p)
d321 1
a321 1
	int mpwx = (p->p_p->ps_textvp->v_mount &&
d324 8
a331 1
	if (!mpwx) {
d334 5
a338 10
		log(LOG_NOTICE, "%s(%d): mmap W^X violation\n",
		    p->p_comm, p->p_pid);
		if (uvm_wxabort) {
			/* Send uncatchable SIGABRT for coredump */
			memset(&sa, 0, sizeof sa);
			sa.sa_handler = SIG_DFL;
			setsigvec(p, SIGABRT, &sa);
			psignal(p, SIGABRT);
		}
		return (ENOTSUP);
d340 1
a340 1
	return (0);
d390 1
a390 1
	    (error = uvm_wxcheck(p)))
d707 1
a707 1
	    (error = uvm_wxcheck(p)))
@


1.127
log
@*** empty log message ***
@
text
@d315 1
a315 1
uvm_wxcheck(struct proc *p, char *call)
d321 1
a321 1
	int wxallowed = (p->p_p->ps_textvp->v_mount &&
d324 1
a324 8
	if (wxallowed && (p->p_p->ps_flags & PS_WXNEEDED))
		return (0);

	/* Report W^X failures, and potentially SIGABRT */
	if (p->p_p->ps_wxcounter++ == 0)
		log(LOG_NOTICE, "%s(%d): %s W^X violation\n",
		    p->p_comm, p->p_pid, call);
	if (uvm_wxabort) {
d327 10
a336 5
		/* Send uncatchable SIGABRT for coredump */
		memset(&sa, 0, sizeof sa);
		sa.sa_handler = SIG_DFL;
		setsigvec(p, SIGABRT, &sa);
		psignal(p, SIGABRT);
d338 1
a338 1
	return (0);		/* ENOTSUP later */
d388 1
a388 1
	    (error = uvm_wxcheck(p, "mmap")))
d705 1
a705 1
	    (error = uvm_wxcheck(p, "mprotect")))
@


1.126
log
@W^X violations are no longer permitted by default.  A kernel log message
is generated, and mprotect/mmap return ENOTSUP.  If the sysctl(8) flag
kern.wxabort is set then a SIGABRT occurs instead, for gdb use or coredump
creation.

W^X violating programs can be permitted on a ffs/nfs filesystem-basis,
using the "wxallowed" mount option.  One day far in the future
upstream software developers will understand that W^X violations are a
tremendously risky practice and that style of programming will be
banished outright.  Until then, we recommend most users need to use the
wxallowed option on their /usr/local filesystem.  At least your other
filesystems don't permit such programs.

ok jca kettenis mlarkin natano
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.125 2016/05/11 21:52:51 deraadt Exp $	*/
d315 1
a315 1
uvm_wxcheck(struct proc *p)
d321 1
a321 1
	int mpwx = (p->p_p->ps_textvp->v_mount &&
d324 8
a331 1
	if (!mpwx) {
d334 5
a338 10
		log(LOG_NOTICE, "%s(%d): mmap W^X violation\n",
		    p->p_comm, p->p_pid);
		if (uvm_wxabort) {
			/* Send uncatchable SIGABRT for coredump */
			memset(&sa, 0, sizeof sa);
			sa.sa_handler = SIG_DFL;
			setsigvec(p, SIGABRT, &sa);
			psignal(p, SIGABRT);
		}
		return (ENOTSUP);
d340 1
a340 1
	return (0);
d390 1
a390 1
	    (error = uvm_wxcheck(p)))
d707 1
a707 1
	    (error = uvm_wxcheck(p)))
@


1.125
log
@remove hppa64 port, which we never got going beyond broken single users.
hppa reverse-stack gives us a valuable test case, but most developers don't
have a 2nd one to proceed further with this.
ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.124 2016/03/29 12:04:26 chl Exp $	*/
d65 1
d309 32
d387 4
d704 3
@


1.124
log
@Remove dead assignments and now unused variables.

Found by LLVM/Clang Static Analyzer.

ok mpi@@ stefan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.123 2016/03/09 16:45:43 deraadt Exp $	*/
a1136 6
#if defined(__hppa64__)
	/* only exists to support ld.so */
	sigexit(p, SIGSYS);
	/* NOTREACHED */
	return EINVAL;
#else
a1251 1
#endif	/* !hppa64 */
@


1.123
log
@remove vaxisms
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.122 2015/11/11 15:59:33 mmcc Exp $	*/
a126 1
	struct uvm_object *uobj;
a148 1
		uobj = &((struct vnode *)fp->f_data)->v_uvm->u_obj;
a151 1
		uobj = NULL;
@


1.122
log
@Remove the superfluous typedef uvm_flag_t (unsigned int). Also, fix an
associated mistake in the uvm manpage.

Suggested by and ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.121 2015/11/01 19:03:33 semarie Exp $	*/
d1140 1
a1140 1
#if defined(__vax__) || defined(__hppa64__)
d1261 1
a1261 1
#endif	/* !vax && !hppa64 */
@


1.122.2.1
log
@backport 1.137:
Since mappings established using __MAP_NOFAULT will be converted into anonymous
memory if the file backing the mapping is truncated, we should check resource
limits.  This prevents callers from triggering a kernel panic and a potential
integer overflow in the amap code by forcing the allocation of too many slots.

Based on an analysis from Jesse Hertz and Tim Newsham.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.122 2015/11/11 15:59:33 mmcc Exp $	*/
d494 1
a494 1
		if ((flags & MAP_ANON) != 0 || (flags & __MAP_NOFAULT) != 0 ||
d513 1
a513 1
		if ((flags & MAP_ANON) != 0 || (flags & __MAP_NOFAULT) != 0 ||
@


1.121
log
@refactor pledge_*_check and pledge_fail functions

- rename _check function without suffix: a "pledge" function called from
  anywhere is a "check" function.

- makes pledge_fail call the responsability to the _check function. remove it
  from caller.

- make proper use of (potential) returned error of _check() functions.

- adds pledge_kill() and pledge_protexec()

with and OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.120 2015/10/09 01:10:27 deraadt Exp $	*/
d985 1
a985 1
	uvm_flag_t uvmflag = 0;
d1038 1
a1038 1
	uvm_flag_t uvmflag = 0;
@


1.120
log
@Rename tame() to pledge().  This fairly interface has evolved to be more
strict than anticipated.  It allows a programmer to pledge/promise/covenant
that their program will operate within an easily defined subset of the
Unix environment, or it pays the price.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.119 2015/09/30 11:36:07 semarie Exp $	*/
d368 3
a370 4
	if ((p->p_p->ps_flags & PS_PLEDGE) &&
	    !(p->p_p->ps_pledge & PLEDGE_PROTEXEC) &&
	    (prot & PROT_EXEC))
		return (pledge_fail(p, EPERM, PLEDGE_PROTEXEC));
d658 1
d671 3
a673 4
	if ((p->p_p->ps_flags & PS_PLEDGE) &&
	    !(p->p_p->ps_pledge & PLEDGE_PROTEXEC) &&
	    (prot & PROT_EXEC))
		return (pledge_fail(p, EPERM, PLEDGE_PROTEXEC));
@


1.119
log
@implement new "prot_exec" tame(2) request:
- by default, a tamed-program don't have the possibility to use PROT_EXEC for
  mmap(2) or mprotect(2)
- for that, use the request "prot_exec" (that could be dropped later)

initial idea from deraadt@@ and kettenis@@

"make complete sense" beck@@
ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.118 2015/09/28 18:36:08 tedu Exp $	*/
d68 1
a68 1
#include <sys/tame.h>
d368 2
a369 2
	if ((p->p_p->ps_flags & PS_TAMED) &&
	    !(p->p_p->ps_tame & TAME_PROTEXEC) &&
d371 1
a371 1
		return (tame_fail(p, EPERM, TAME_PROTEXEC));
d671 2
a672 2
	if ((p->p_p->ps_flags & PS_TAMED) &&
	    !(p->p_p->ps_tame & TAME_PROTEXEC) &&
d674 1
a674 1
		return (tame_fail(p, EPERM, TAME_PROTEXEC));
@


1.118
log
@the kernel lock is no longer needed in the fixed case since uvm_map
will perform the unmap as necessary, holding the vm lock.
reminded by kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.117 2015/09/28 18:33:42 tedu Exp $	*/
d68 1
d368 5
d670 5
@


1.117
log
@add a flag to indicate to uvm_map that it should unmap to make space.
this pulls all the relevant operations under the same map locking, and
relieves calling code from responsibility.
ok kettenis matthew
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.116 2015/09/26 15:37:28 tedu Exp $	*/
a515 2
		if ((flags & (MAP_FIXED|__MAP_NOREPLACE)) == MAP_FIXED)
			KERNEL_LOCK();
a517 2
		if ((flags & (MAP_FIXED|__MAP_NOREPLACE)) == MAP_FIXED)
			KERNEL_UNLOCK();
@


1.116
log
@matthew noticed there's a race where we are using the kernel lock to tie
together the unmap and map portions of a fixed mmap. make this explicit
by pulling the lock up higher. in preparation for unlocking the syscall.

there's still (always has been) a race where if the unmap sleeps, another
mmap may see partial results because the map lock isn't held througout.
another problem, another day.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.115 2015/09/23 00:16:44 guenther Exp $	*/
d993 2
a994 4
		if ((flags & __MAP_NOREPLACE) == 0) {
			/* KERNEL_LOCK held above */
			uvm_unmap(map, *addr, *addr + size);	/* zap! */
		}
d1047 1
a1047 1
			uvm_unmap(map, *addr, *addr + size);	/* zap! */
@


1.115
log
@Corect a kbind comment to describe the version that was settled on: no old
data, only new
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.114 2015/09/06 17:06:43 deraadt Exp $	*/
d516 2
d520 2
d994 1
a994 1
			KERNEL_LOCK();
a995 1
			KERNEL_UNLOCK();
@


1.114
log
@sizes for free(); ok semarie
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.113 2015/08/25 19:47:56 guenther Exp $	*/
d1186 3
a1188 3
	 * followed by the corresponding new and old data areas for them
	 * in alternation.  Verify that the sizes in the __kbind structures
	 * add up to the total size and find the start of the old+new area.
@


1.113
log
@In sys_kbind(), pages from uvm_map_extract() must be written to with kcopy()

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.112 2015/07/20 22:41:41 miod Exp $	*/
d225 1
a225 1
		free(pgs, M_TEMP, 0);
d306 1
a306 1
	free(pgs, M_TEMP, 0);
@


1.112
log
@Actually return a value from sys_kbind() in the non-ld.so case, or the
compiler will warn.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.111 2015/07/20 05:49:30 jsg Exp $	*/
d1239 3
a1241 1
		memcpy((char *)kva + pageoffset, data, paramp[i].kb_size);
@


1.112.4.1
log
@backport 1.137:
Since mappings established using __MAP_NOFAULT will be converted into anonymous
memory if the file backing the mapping is truncated, we should check resource
limits.  This prevents callers from triggering a kernel panic and a potential
integer overflow in the amap code by forcing the allocation of too many slots.

Based on an analysis from Jesse Hertz and Tim Newsham.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.112 2015/07/20 22:41:41 miod Exp $	*/
d489 1
a489 1
		if ((flags & MAP_ANON) != 0 || (flags & __MAP_NOFAULT) != 0 ||
d508 1
a508 1
		if ((flags & MAP_ANON) != 0 || (flags & __MAP_NOFAULT) != 0 ||
@


1.111
log
@include sys/user.h to unbreak the build on at least arm after rev 1.110
ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.110 2015/07/20 00:56:10 guenther Exp $	*/
d1136 2
@


1.110
log
@Add kbind, a syscall for ld.so to use to securely and efficiently update
memory for lazy binding

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.109 2015/05/07 08:53:33 mpi Exp $	*/
d69 1
@


1.109
log
@Pass a thread pointer instead of its file descriptor table to getvnode(9).

Input and ok millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.108 2015/03/30 21:08:40 miod Exp $	*/
d64 1
d68 1
d1125 125
@


1.108
log
@Extend uvm_map_hint() to get an address range as extra arguments, and make
sure it will return an address within that range.

Use this in uaddr_rnd_select() to make sure we will not attempt to pick
an address beyond what we are allowed to map.

In my trees for 9 months, blackmailed s2k15 attendees into agreeing now would
be a good time to commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.107 2015/02/13 13:35:03 millert Exp $	*/
d144 1
a144 1
		if ((error = getvnode(p->p_fd, fd, &fp)) != 0)
@


1.107
log
@Include sys/stdint.h for SIZE_MAX instead of relying on the misplaced
define in sys/limits.h.  OK guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.106 2015/02/07 00:42:20 tedu Exp $	*/
d155 2
a156 1
		vaddr = uvm_map_hint(p->p_vmspace, prot);
@


1.106
log
@recombine some of the split uvm_mmap functions. the precondition checks
are not necessary because the caller already ensures these. the tail
section for handing mlock can be shared as well.
ok beck guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.105 2015/02/06 11:41:55 beck Exp $	*/
d66 1
@


1.105
log
@-Split out uvm_mmap and uvm_map into a version for anon's and a version
for everything else.
-Adapt the anon version to be callable without the biglock held.
Done by tedu@@, kettenis@@ and me.. pounded on a bunch.

This does not yet make mmap a NOLOCK call, but permits it to be so.
ok tedu@@, kettenis@@, guenther@@ jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.104 2014/12/17 06:58:11 guenther Exp $	*/
a491 1

d904 56
a972 6
	/* check params */
	if (size == 0)
		return(0);
	if ((prot & maxprot) != prot)
		return(EINVAL);

d1007 3
a1009 48
	if (error == 0) {
		/*
		 * POSIX 1003.1b -- if our address space was configured
		 * to lock all future mappings, wire the one we just made.
		 */
		if (prot == PROT_NONE) {
			/*
			 * No more work to do in this case.
			 */
			return (0);
		}

		vm_map_lock(map);
		if (map->flags & VM_MAP_WIREFUTURE) {
			KERNEL_LOCK();
			if ((atop(size) + uvmexp.wired) > uvmexp.wiredmax
#ifdef pmap_wired_count
			    || (locklimit != 0 && (size +
			         ptoa(pmap_wired_count(vm_map_pmap(map)))) >
			        locklimit)
#endif
			) {
				error = ENOMEM;
				vm_map_unlock(map);
				/* unmap the region! */
				uvm_unmap(map, *addr, *addr + size);
				KERNEL_UNLOCK();
				return (error);
			}
			/*
			 * uvm_map_pageable() always returns the map
			 * unlocked.
			 */
			error = uvm_map_pageable(map, *addr, *addr + size,
			    FALSE, UVM_LK_ENTER);
			if (error != 0) {
				/* unmap the region! */
				uvm_unmap(map, *addr, *addr + size);
				KERNEL_UNLOCK();
				return (error);
			}
			KERNEL_UNLOCK();
			return (0);
		}
		vm_map_unlock(map);
		return (0);
	}
	return(error);
a1028 8
	/* check params */
	if (size == 0)
		return(0);
	if (foff & PAGE_MASK)
		return(EINVAL);
	if ((prot & maxprot) != prot)
		return(EINVAL);

d1113 2
a1114 43
	if (error == 0) {
		/*
		 * POSIX 1003.1b -- if our address space was configured
		 * to lock all future mappings, wire the one we just made.
		 */
		if (prot == PROT_NONE) {
			/*
			 * No more work to do in this case.
			 */
			return (0);
		}

		vm_map_lock(map);
		if (map->flags & VM_MAP_WIREFUTURE) {
			if ((atop(size) + uvmexp.wired) > uvmexp.wiredmax
#ifdef pmap_wired_count
			    || (locklimit != 0 && (size +
			         ptoa(pmap_wired_count(vm_map_pmap(map)))) >
			        locklimit)
#endif
			) {
				error = ENOMEM;
				vm_map_unlock(map);
				/* unmap the region! */
				uvm_unmap(map, *addr, *addr + size);
				goto bad;
			}
			/*
			 * uvm_map_pageable() always returns the map
			 * unlocked.
			 */
			error = uvm_map_pageable(map, *addr, *addr + size,
			    FALSE, UVM_LK_ENTER);
			if (error != 0) {
				/* unmap the region! */
				uvm_unmap(map, *addr, *addr + size);
				goto bad;
			}
			return (0);
		}
		vm_map_unlock(map);
		return (0);
	}
a1119 1
bad:
@


1.104
log
@Prefer MADV_* over POSIX_MADV_* in kernel for consistency: the latter
doesn't have all the values and therefore can't be used everywhere.

ok deraadt@@ kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.103 2014/12/16 18:30:04 tedu Exp $	*/
d75 6
a334 1
	caddr_t handle;
d384 3
a386 1
		if ((fp = fd_getfile(fdp, fd)) == NULL)
d388 1
d414 2
d484 8
d493 2
a494 2
		/* set handle to vnode */
		handle = (caddr_t)vp;
d499 2
a500 4
		if (fd != -1) {
			error = EINVAL;
			goto out;
		}
d502 1
a502 5
is_anon:		/* label for SunOS style /dev/zero */
		handle = NULL;
		maxprot = PROT_MASK;
		pos = 0;
	}
d504 6
a509 6
	if ((flags & MAP_ANON) != 0 ||
	    ((flags & MAP_PRIVATE) != 0 && (prot & PROT_WRITE) != 0)) {
		if (size >
		    (p->p_rlimit[RLIMIT_DATA].rlim_cur - ptoa(p->p_vmspace->vm_dused))) {
			error = ENOMEM;
			goto out;
d511 3
a515 4
	/* now let kernel internal function uvm_mmap do the work. */
	error = uvm_mmap(&p->p_vmspace->vm_map, &addr, size, prot, maxprot,
	    flags, handle, pos, p->p_rlimit[RLIMIT_MEMLOCK].rlim_cur, p);

d521 4
a524 2
	if (fp)
		FRELE(fp, p);	
d905 105
a1009 1
 * uvm_mmap: internal version of mmap
d1011 1
a1011 2
 * - used by sys_mmap, exec, and sysv shm
 * - handle is a vnode pointer or ignored for MAP_ANON
d1015 2
a1016 2
uvm_mmap(vm_map_t map, vaddr_t *addr, vsize_t size, vm_prot_t prot,
    vm_prot_t maxprot, int flags, caddr_t handle, voff_t foff,
a1019 1
	struct vnode *vp;
d1049 1
a1049 2
	 * handle anon vs. non-anon mappings.   for non-anon mappings attach
	 * to underlying vm object.
d1051 3
a1053 16
	if (flags & MAP_ANON) {
		if ((flags & MAP_FIXED) == 0 && size >= __LDPGSZ)
			align = __LDPGSZ;
		foff = UVM_UNKNOWN_OFFSET;
		uobj = NULL;
		if ((flags & MAP_SHARED) == 0)
			/* XXX: defer amap create */
			uvmflag |= UVM_FLAG_COPYONW;
		else
			/* shared: create amap now */
			uvmflag |= UVM_FLAG_OVERLAY;
	} else {
		vp = (struct vnode *) handle;	/* get vnode */
		if (vp->v_type != VCHR) {
			uobj = uvn_attach(vp, (flags & MAP_SHARED) ?
			   maxprot : (maxprot & ~PROT_WRITE));
d1055 27
a1081 28
			/*
			 * XXXCDC: hack from old code
			 * don't allow vnodes which have been mapped
			 * shared-writeable to persist [forces them to be
			 * flushed out when last reference goes].
			 * XXXCDC: interesting side effect: avoids a bug.
			 * note that in WRITE [ufs_readwrite.c] that we
			 * allocate buffer, uncache, and then do the write.
			 * the problem with this is that if the uncache causes
			 * VM data to be flushed to the same area of the file
			 * we are writing to... in that case we've got the
			 * buffer locked and our process goes to sleep forever.
			 *
			 * XXXCDC: checking maxprot protects us from the
			 * "persistbug" program but this is not a long term
			 * solution.
			 * 
			 * XXXCDC: we don't bother calling uncache with the vp
			 * VOP_LOCKed since we know that we are already
			 * holding a valid reference to the uvn (from the
			 * uvn_attach above), and thus it is impossible for
			 * the uncache to kill the uvn and trigger I/O.
			 */
			if (flags & MAP_SHARED) {
				if ((prot & PROT_WRITE) ||
				    (maxprot & PROT_WRITE)) {
					uvm_vnp_uncache(vp);
				}
d1083 12
a1094 1
		} else {
a1097 12
			/*
			 * XXX Some devices don't like to be mapped with
			 * XXX PROT_EXEC, but we don't really have a
			 * XXX better way of handling this, right now
			 */
			if (uobj == NULL && (prot & PROT_EXEC) == 0) {
				maxprot &= ~PROT_EXEC;
				uobj = udv_attach(vp->v_rdev,
				    (flags & MAP_SHARED) ? maxprot :
				    (maxprot & ~PROT_WRITE), foff, size);
			}
			advice = MADV_RANDOM;
d1099 1
a1099 8
		
		if (uobj == NULL)
			return((vp->v_type == VREG) ? ENOMEM : EINVAL);

		if ((flags & MAP_SHARED) == 0)
			uvmflag |= UVM_FLAG_COPYONW;
		if (flags & __MAP_NOFAULT)
			uvmflag |= (UVM_FLAG_NOFAULT | UVM_FLAG_OVERLAY);
d1102 8
d1111 1
a1111 1
	uvmflag = UVM_MAPFLAG(prot, maxprot, 
d1128 1
a1128 1
		
a1129 1

a1156 1

a1157 1

@


1.103
log
@primary change: move uvm_vnode out of vnode, keeping only a pointer.
objective: vnode.h doesn't include uvm_extern.h anymore.
followup changes: include uvm_extern.h or lock.h where necessary.
ok and help from deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.102 2014/12/15 02:24:23 guenther Exp $	*/
d906 1
a906 1
	int advice = POSIX_MADV_NORMAL;
d998 1
a998 1
			advice = POSIX_MADV_RANDOM;
@


1.102
log
@Use MAP_INHERIT_* for the 'inh' argument to the UMV_MAPFLAG() macro,
eliminating the must-be-kept-in-sync UVM_INH_* macros

ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.101 2014/12/09 07:16:41 doug Exp $	*/
d139 1
a139 1
		uobj = &((struct vnode *)fp->f_data)->v_uvm.u_obj;
@


1.101
log
@Sprinkle in a little more mallocarray().

ok deraadt@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.100 2014/11/16 12:31:00 deraadt Exp $	*/
d895 1
a895 2
 * - handle is a vnode pointer or NULL for MAP_ANON (XXX: not true,
 *	sysv shm uses "named anonymous memory")
d1012 2
a1013 2
			(flags & MAP_SHARED) ? UVM_INH_SHARE : UVM_INH_COPY,
			advice, uvmflag);
@


1.100
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.99 2014/10/03 17:41:00 kettenis Exp $	*/
d204 1
a204 1
	pgs = malloc(sizeof(*pgs) * npgs, M_TEMP, M_WAITOK | M_CANFAIL);
@


1.99
log
@Introduce __MAP_NOFAULT, a mmap(2) flag that makes sure a mapping will not
cause a SIGSEGV or SIGBUS when a mapped file gets truncated.  Access to
pages that are not backed by a file on such a mapping will be replaced by
zero-filled anonymous pages.  Makes passing file descriptors of mapped files
usable without having to play tricks with signal handlers.

"steal your mmap flag" deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.98 2014/07/12 18:44:01 tedu Exp $	*/
d130 1
a130 1
	if ((prot & VM_PROT_ALL) != prot)
d213 1
a213 1
	if ((error = uvm_vslock(p, vec, npgs, VM_PROT_WRITE)) != 0) {
d344 1
a344 1
	if ((prot & VM_PROT_ALL) != prot)
d438 1
a438 1
		maxprot = VM_PROT_EXECUTE;
d442 1
a442 1
			maxprot |= VM_PROT_READ;
d461 1
a461 1
					maxprot |= VM_PROT_WRITE;
d472 1
a472 1
			maxprot |= VM_PROT_WRITE;
d488 1
a488 1
		maxprot = VM_PROT_ALL;
d607 1
a607 1
	if (!uvm_map_checkprot(map, addr, addr + size, VM_PROT_NONE)) {
d645 1
a645 1
	if ((prot & VM_PROT_ALL) != prot)
d907 1
a907 1
	int advice = UVM_ADV_NORMAL;
d953 1
a953 1
			   maxprot : (maxprot & ~VM_PROT_WRITE));
d979 2
a980 2
				if ((prot & VM_PROT_WRITE) ||
				    (maxprot & VM_PROT_WRITE)) {
d987 1
a987 1
			    (maxprot & ~VM_PROT_WRITE), foff, size);
d994 1
a994 1
				maxprot &= ~VM_PROT_EXECUTE;
d997 1
a997 1
				    (maxprot & ~VM_PROT_WRITE), foff, size);
d999 1
a999 1
			advice = UVM_ADV_RANDOM;
d1023 1
a1023 1
		if (prot == VM_PROT_NONE) {
@


1.98
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.97 2014/07/08 11:38:48 deraadt Exp $	*/
d1007 2
@


1.97
log
@bye bye UBC; ok beck dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.96 2014/07/02 06:09:49 matthew Exp $	*/
d214 1
a214 1
		free(pgs, M_TEMP);
d295 1
a295 1
	free(pgs, M_TEMP);
@


1.96
log
@Use real parameter types for u{dv,vn}_attach() instead of void *

ok guenther
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.95 2014/06/27 20:50:43 matthew Exp $	*/
a954 1
#ifndef UBC
a983 4
#else
			/* XXX for now, attach doesn't gain a ref */
			vref(vp);
#endif
@


1.95
log
@Cleanup support for legacy mmap flags

Move all legacy MAP_FOO values behind #ifndef _KERNEL and redefine
them to either be aliases for existing flags (e.g., MAP_COPY ->
MAP_PRIVATE) or 0.

Also, add MAP_OLDFOO defines (behind #ifndef _KERNEL) so the kernel
and kdump can remain compatible with current OpenBSD binaries.

ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.94 2014/04/13 23:14:15 tedu Exp $	*/
d952 1
a952 1
			uobj = uvn_attach((void *) vp, (flags & MAP_SHARED) ?
d990 1
a990 1
			uobj = udv_attach((void *) &vp->v_rdev,
d1000 1
a1000 1
				uobj = udv_attach((void *) &vp->v_rdev,
@


1.94
log
@compress code by turning four line comments into one line comments.
emphatic ok usual suspects, grudging ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.93 2013/05/30 16:29:46 tedu Exp $	*/
d348 2
a349 2
	if (flags & MAP_COPY)
		flags = (flags & ~MAP_COPY) | MAP_PRIVATE;
@


1.93
log
@remove lots of comments about locking per beck's request
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.92 2013/05/30 15:17:59 tedu Exp $	*/
a102 1

a162 1

a306 1

d332 1
a332 4
	/*
	 * first, extract syscall args from the uap.
	 */

d357 1
a357 3
	/*
	 * align file position and save offset.  adjust size.
	 */
d360 1
a360 4
	/*
	 * now check (MAP_FIXED) or get (!MAP_FIXED) the "addr" 
	 */

a361 1

d377 1
a377 3
	/*
	 * check for file mappings (i.e. not anonymous) and verify file.
	 */
a378 1

d437 1
a437 4
		/*
		 * now check protection
		 */

d475 1
a475 4
		/*
		 * set handle to vnode
		 */

a476 1

d486 1
a486 1
 is_anon:		/* label for SunOS style /dev/zero */
d501 1
a501 4
	/*
	 * now let kernel internal function uvm_mmap do the work.
	 */

d532 1
a532 4
	/*
	 * extract syscall args from the uap
	 */

d545 1
a545 3
	/*
	 * align the address to a page boundary, and adjust the size accordingly
	 */
d550 1
a550 4
	/*
	 * get map
	 */

d553 1
a553 3
	/*
	 * translate MS_ flags into PGO_ flags
	 */
a567 1

d581 1
a581 4
	/*
	 * get syscall args...
	 */

d585 1
a585 3
	/*
	 * align the address to a page boundary, and adjust the size accordingly
	 */
a606 1

a611 3
	/*
	 * doit!
	 */
a624 1

a661 1

a691 1

d791 1
a791 3
	/*
	 * extract syscall args from uap
	 */
d795 1
a795 3
	/*
	 * align the address to a page boundary and adjust the size accordingly
	 */
d832 1
a832 4
	/*
	 * extract syscall args from uap
	 */

d836 1
a836 3
	/*
	 * align the address to a page boundary, and adjust the size accordingly
	 */
a853 1

a882 1

a898 1

d911 1
a911 4
	/*
	 * check params
	 */

a922 1

a937 1

a948 1

a949 1

d1014 1
a1014 4
	/*
	 * set up mapping flags
	 */

d1068 1
a1068 4
	/*
	 * errors: first detach from the uobj, if any.
	 */
	
@


1.92
log
@remove simple_locks from uvm code. ok beck deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.91 2012/07/21 06:46:58 matthew Exp $	*/
a266 1
				/* Don't need to lock anon here. */
@


1.91
log
@Add a new mmap(2) flag __MAP_NOREMAP for use with MAP_FIXED to
indicate that the kernel should fail with MAP_FAILED if the specified
address is not currently available instead of unmapping it.

Change ld.so on i386 to make use of __MAP_NOREMAP to improve
reliability.

__MAP_NOREMAP diff by guenther based on an earlier diff by Ariane;
ld.so bits by guenther and me
bulk build stress testing of earlier diffs by sthen
ok deraadt; committing now for further testing
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.90 2012/04/22 05:43:14 guenther Exp $	*/
a260 3
		if (uobj != NULL)
			simple_lock(&uobj->vmobjlock);

a289 3

		if (uobj != NULL)
			simple_unlock(&uobj->vmobjlock);
@


1.90
log
@Add struct proc * argument to FRELE() and FILE_SET_MATURE() in
anticipation of further changes to closef().  No binary change.

ok krw@@ miod@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.89 2012/04/10 10:30:44 ariane Exp $	*/
d365 2
d999 1
d1001 2
a1002 1
		uvm_unmap(map, *addr, *addr + size);	/* zap! */
@


1.89
log
@Return EINVAL on 0-byte mmap invocation.

- Posix rules that a 0-byte mmap must return EINVAL
- our allocators are unable to distinguish between free memory and
  0 bytes of allocated memory
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.88 2012/03/09 13:01:29 ariane Exp $	*/
d157 1
a157 1
		FRELE(fp);
d424 1
a424 1
			FRELE(fp);
d541 1
a541 1
		FRELE(fp);	
@


1.88
log
@New vmmap implementation.

no oks (it is really a pain to review properly)
extensively tested, I'm confident it'll be stable
'now is the time' from several icb inhabitants

Diff provides:
- ability to specify different allocators for different regions/maps
- a simpler implementation of the current allocator
- currently in compatibility mode: it will generate similar addresses
  as the old allocator
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.87 2011/07/09 05:31:26 matthew Exp $	*/
d364 2
@


1.87
log
@More syscalls.master cleanup:

sys_osigaltstack() is 7 years old and no longer needed; all glory to
the sys_sigaltstack()!

sys_ogetdirentries() is about 9 months old, but still acceptable
within our release cycle; move from STD to COMPAT_48 to make this
clearer for tedu@@ next year.

sys_sbrk() and sys_sstk() are completely obsolete: all they do is
return ENOSYS.

ok guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.86 2011/07/05 09:15:57 oga Exp $	*/
d145 1
a145 1
		uoff = 0;
d149 1
a149 1
		vaddr = uvm_map_hint(p, prot);
d151 4
a154 10
	/* prevent a user requested address from falling in heap space */
	if ((vaddr + size > (vaddr_t)p->p_vmspace->vm_daddr) &&
	    (vaddr < (vaddr_t)p->p_vmspace->vm_daddr + BRKSIZ)) {
		if (flags & UVM_FLAG_FIXED) {
			error = EINVAL;
			goto done;
		}
		vaddr = round_page((vaddr_t)p->p_vmspace->vm_daddr + BRKSIZ);
	}
	vm_map_lock(&p->p_vmspace->vm_map);
a155 20
again:
	if (uvm_map_findspace(&p->p_vmspace->vm_map, vaddr, size,
	    &vaddr, uobj, uoff, 0, flags) == NULL) {
		if (flags & UVM_FLAG_FIXED)
			error = EINVAL;
		else
			error = ENOMEM;
	} else {
		/* prevent a returned address from falling in heap space */
		if ((vaddr + size > (vaddr_t)p->p_vmspace->vm_daddr)
		    && (vaddr < (vaddr_t)p->p_vmspace->vm_daddr + BRKSIZ)) {
			vaddr = round_page((vaddr_t)p->p_vmspace->vm_daddr +
			    BRKSIZ);
			goto again;
		}
		error = 0;
		*retval = (register_t)(vaddr);
	}
	vm_map_unlock(&p->p_vmspace->vm_map);
done:
d179 1
a179 1
	vm_map_entry_t entry;
d228 2
a229 2
	     entry != &map->header && entry->start < end;
	     entry = entry->next) {
d234 1
d236 2
a237 2
		     (entry->next == &map->header ||
		      entry->next->start > entry->end)) {
a389 11
	} else {

		/*
		 * not fixed: make sure we skip over the largest possible heap.
		 * we will refine our guess later (e.g. to account for VAC, etc)
		 */
		if (addr == 0)
			addr = uvm_map_hint(p, prot);
		else if (!(flags & MAP_TRYFIXED) &&
		    addr < (vaddr_t)p->p_vmspace->vm_daddr)
			addr = uvm_map_hint(p, prot);
a531 7
	if (error == ENOMEM && !(flags & (MAP_FIXED | MAP_TRYFIXED))) {
		/* once more, with feeling */
		addr = uvm_map_hint1(p, prot, 0);
		error = uvm_mmap(&p->p_vmspace->vm_map, &addr, size, prot,
		    maxprot, flags, handle, pos,
		    p->p_rlimit[RLIMIT_MEMLOCK].rlim_cur, p);
	}
d618 1
a618 1
	struct vm_map_entry *dead_entries;
d660 2
a661 1
	uvm_unmap_remove(map, addr, addr + size, &dead_entries, p, FALSE);
d665 1
a665 2
	if (dead_entries != NULL)
		uvm_unmap_detach(dead_entries, 0);
d996 1
a996 1
		uvm_unmap_p(map, *addr, *addr + size, p);	/* zap! */
d1090 1
a1090 1
	error = uvm_map_p(map, addr, size, uobj, foff, align, uvmflag, p);
@


1.86
log
@msync has some code that is based on *old* bsd behaviour where
msync(size == 0) did strange things based on the original mapping
segments and trying to manipulate same. This code was copied from the
original vm when we moved to uvm.

posix says nothing about this behaviour and anything that depends on it is
systemically broken, so rip it out and make sys_msync about 30% smaller.

ok deraadt@@, tedu@@, guenther@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.85 2011/07/04 20:35:35 deraadt Exp $	*/
a91 38

/*
 * unimplemented VM system calls:
 */

/*
 * sys_sbrk: sbrk system call.
 */

/* ARGSUSED */
int
sys_sbrk(struct proc *p, void *v, register_t *retval)
{
#if 0
	struct sys_sbrk_args /* {
		syscallarg(int) incr;
	} */ *uap = v;
#endif

	return (ENOSYS);
}

/*
 * sys_sstk: sstk system call.
 */

/* ARGSUSED */
int
sys_sstk(struct proc *p, void *v, register_t *retval)
{
#if 0
	struct sys_sstk_args /* {
		syscallarg(int) incr;
	} */ *uap = v;
#endif

	return (ENOSYS);
}
@


1.85
log
@move the specfs code to a place people can see it; ok guenther thib krw
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.84 2011/06/06 17:10:23 ariane Exp $	*/
d639 1
a639 1
	int rv, flags, uvmflags;
a668 24

	/*
	 * XXXCDC: do we really need this semantic?
	 *
	 * XXX Gak!  If size is zero we are supposed to sync "all modified
	 * pages with the region containing addr".  Unfortunately, we
	 * don't really keep track of individual mmaps so we approximate
	 * by flushing the range of the map entry containing addr.
	 * This can be incorrect if the region splits or is coalesced
	 * with a neighbor.
	 */
	if (size == 0) {
		vm_map_entry_t entry;
		
		vm_map_lock_read(map);
		rv = uvm_map_lookup_entry(map, addr, &entry);
		if (rv == TRUE) {
			addr = entry->start;
			size = entry->end - entry->start;
		}
		vm_map_unlock_read(map);
		if (rv == FALSE)
			return (EINVAL);
	}
@


1.84
log
@Backout vmmap in order to repair virtual address selection algorithms
outside the tree.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.83 2011/05/24 15:27:36 ariane Exp $	*/
d65 1
a67 2

#include <miscfs/specfs/specdev.h>
@


1.83
log
@Reimplement uvm/uvm_map.

vmmap is designed to perform address space randomized allocations,
without letting fragmentation of the address space go through the roof.

Some highlights:
- kernel address space randomization
- proper implementation of guardpages
- roughly 10% system time reduction during kernel build

Tested by alot of people on tech@@ and developers.
Theo's machines are still happy.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.82 2010/12/24 21:49:04 tedu Exp $	*/
d184 1
a184 1
		uoff = UVM_UNKNOWN_OFFSET;
d187 30
a216 3
	error = uvm_map_mquery(&p->p_vmspace->vm_map, &vaddr, size, uoff,
	    flags);
	if (error == 0)
d218 3
a220 1

d244 1
a244 1
	vm_map_entry_t entry, next;
d293 2
a294 2
	     entry != NULL && entry->start < end;
	     entry = RB_NEXT(uvm_map_addr, &map->addr, entry)) {
a298 1
		next = RB_NEXT(uvm_map_addr, &map->addr, entry);
d300 2
a301 2
		     (next == NULL ||
		      next->start > entry->end)) {
d454 11
d607 7
d724 1
a724 1
	struct uvm_map_deadq dead_entries;
d766 1
a766 2
	TAILQ_INIT(&dead_entries);
	uvm_unmap_remove(map, addr, addr + size, &dead_entries, FALSE, TRUE);
d770 2
a771 1
	uvm_unmap_detach(&dead_entries, 0);
d1102 1
a1102 1
		uvm_unmap(map, *addr, *addr + size);	/* zap! */
d1196 1
a1196 1
	error = uvm_map(map, addr, size, uobj, foff, align, uvmflag);
@


1.82
log
@add a param to uvm_map_hint to not skip over the heap, and use it as a last
resort if mmap fails otherwise to enable more complete address space
utilization.  tested for a while with no ill effects.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.81 2010/12/15 04:59:53 tedu Exp $	*/
d184 1
a184 1
		uoff = 0;
d187 4
a190 2
	if (vaddr == 0)
		vaddr = uvm_map_hint(p, prot);
a191 31
	/* prevent a user requested address from falling in heap space */
	if ((vaddr + size > (vaddr_t)p->p_vmspace->vm_daddr) &&
	    (vaddr < (vaddr_t)p->p_vmspace->vm_daddr + BRKSIZ)) {
		if (flags & UVM_FLAG_FIXED) {
			error = EINVAL;
			goto done;
		}
		vaddr = round_page((vaddr_t)p->p_vmspace->vm_daddr + BRKSIZ);
	}
	vm_map_lock(&p->p_vmspace->vm_map);

again:
	if (uvm_map_findspace(&p->p_vmspace->vm_map, vaddr, size,
	    &vaddr, uobj, uoff, 0, flags) == NULL) {
		if (flags & UVM_FLAG_FIXED)
			error = EINVAL;
		else
			error = ENOMEM;
	} else {
		/* prevent a returned address from falling in heap space */
		if ((vaddr + size > (vaddr_t)p->p_vmspace->vm_daddr)
		    && (vaddr < (vaddr_t)p->p_vmspace->vm_daddr + BRKSIZ)) {
			vaddr = round_page((vaddr_t)p->p_vmspace->vm_daddr +
			    BRKSIZ);
			goto again;
		}
		error = 0;
		*retval = (register_t)(vaddr);
	}
	vm_map_unlock(&p->p_vmspace->vm_map);
done:
d215 1
a215 1
	vm_map_entry_t entry;
d264 2
a265 2
	     entry != &map->header && entry->start < end;
	     entry = entry->next) {
d270 1
d272 2
a273 2
		     (entry->next == &map->header ||
		      entry->next->start > entry->end)) {
a425 11
	} else {

		/*
		 * not fixed: make sure we skip over the largest possible heap.
		 * we will refine our guess later (e.g. to account for VAC, etc)
		 */
		if (addr == 0)
			addr = uvm_map_hint(p, prot);
		else if (!(flags & MAP_TRYFIXED) &&
		    addr < (vaddr_t)p->p_vmspace->vm_daddr)
			addr = uvm_map_hint(p, prot);
a567 7
	if (error == ENOMEM && !(flags & (MAP_FIXED | MAP_TRYFIXED))) {
		/* once more, with feeling */
		addr = uvm_map_hint1(p, prot, 0);
		error = uvm_mmap(&p->p_vmspace->vm_map, &addr, size, prot,
		    maxprot, flags, handle, pos,
		    p->p_rlimit[RLIMIT_MEMLOCK].rlim_cur, p);
	}
d678 1
a678 1
	struct vm_map_entry *dead_entries;
d720 2
a721 1
	uvm_unmap_remove(map, addr, addr + size, &dead_entries, p, FALSE);
d725 1
a725 2
	if (dead_entries != NULL)
		uvm_unmap_detach(dead_entries, 0);
d1056 1
a1056 1
		uvm_unmap_p(map, *addr, *addr + size, p);	/* zap! */
d1150 1
a1150 1
	error = uvm_map_p(map, addr, size, uobj, foff, align, uvmflag, p);
@


1.81
log
@add a BRKSIZ define and use it for the heap gap constant, decoupling
heap gap from max data size.  nothing else changes yet.  ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.80 2010/05/21 23:22:33 oga Exp $	*/
d607 7
@


1.80
log
@Fix a locking problem in mincore where it was possible for us to sleep
with a spinlock (even vslocked() buffers may fault in the right
(complicated) situation).

We solve this by preallocating a bounded array for the response and copying the
data out when all locks have been released.

ok thib@@, beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.79 2009/07/25 12:55:40 miod Exp $	*/
d192 1
a192 1
	    (vaddr < (vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ)) {
d197 1
a197 1
		vaddr = round_page((vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ);
d211 1
a211 1
		    && (vaddr < (vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ)) {
d213 1
a213 1
			    MAXDSIZ);
@


1.79
log
@Add an extra argument to uvm_unmap_remove(), for the caller to tell it
whether removing holes or parts of them is allowed or not.
Only allow hole removal in uvmspace_free(), when tearing the vmspace down.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.78 2009/07/22 21:05:37 oga Exp $	*/
d240 1
a240 1
	char *vec, pgi;
d248 1
a248 1
	int error = 0;
d266 11
d280 2
a281 1
	if ((error = uvm_vslock(p, vec, npgs, VM_PROT_WRITE)) != 0)
d283 1
a314 1
				pgi = 1;
d316 2
a317 2
				     start += PAGE_SIZE, vec++)
					copyout(&pgi, vec, sizeof(char));
d328 2
a329 2
		for (/* nothing */; start < lim; start += PAGE_SIZE, vec++) {
			pgi = 0;
d340 1
a340 1
					pgi = 1;
d344 1
a344 1
			if (uobj != NULL && pgi == 0) {
d353 1
a353 1
					pgi = 1;
a355 2

			copyout(&pgi, vec, sizeof(char));
d365 4
@


1.78
log
@Put the PG_RELEASED changes diff back in.

This has has been tested very very thoroughly on all archs we have
excepting 88k and 68k. Please see cvs log for the individual commit
messages.

ok beck@@, thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.77 2009/07/09 22:29:56 thib Exp $	*/
d745 1
a745 1
	uvm_unmap_remove(map, addr, addr + size, &dead_entries, p);
@


1.77
log
@Remove the VREF() macro and replaces all instances with a call to verf(),
which is exactly what the macro does.

Macro's that are nothing more then:
#define FUNCTION(arg) function(arg)
are almost always pointless and should go away.

OK blambert@@
Agreed by many.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.72 2009/03/20 15:19:04 oga Exp $	*/
d301 1
a301 2
			if (entry->object.uvm_obj->pgops->pgo_releasepg
			    == NULL) {
@


1.76
log
@date based reversion of uvm to the 4th May.

More backouts in line with previous ones, this appears to bring us back to a
stable condition.

A machine forced to 64mb of ram cycled 10GB through swap with this diff
and is still running as I type this. Other tests by ariane@@ and thib@@
also seem to show that it's alright.

ok deraadt@@, thib@@, ariane@@
@
text
@d1141 1
a1141 1
			VREF(vp);
@


1.75
log
@Backout all changes to uvm after pmemrange (which will be backed out
separately).

a change at or just before the hackathon has either exposed or added a
very very nasty memory corruption bug that is giving us hell right now.
So in the interest of kernel stability these diffs are being backed out
until such a time as that corruption bug has been found and squashed,
then the ones that are proven good may slowly return.

a quick hitlist of the main commits this backs out:

mine:
uvm_objwire
the lock change in uvm_swap.c
using trees for uvm objects instead of the hash
removing the pgo_releasepg callback.

art@@'s:
putting pmap_page_protect(VM_PROT_NONE) in uvm_pagedeactivate() since
all callers called that just prior anyway.

ok beck@@, ariane@@.

prompted by deraadt@@.
@
text
@@


1.74
log
@Deal with wraparound when checking RLIMIT_DATA.
OK guenther@@ otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.73 2009/06/01 19:54:02 oga Exp $	*/
d301 2
a302 1
			if (entry->object.uvm_obj->pgops->pgo_fault != NULL) {
d581 2
a582 3
		u_int64_t used = ptoa(p->p_vmspace->vm_dused);
		if (p->p_rlimit[RLIMIT_DATA].rlim_cur < used ||
		    size > p->p_rlimit[RLIMIT_DATA].rlim_cur - used) {
@


1.73
log
@Since we've now cleared up a lot of the PG_RELEASED setting, remove the
pgo_releasepg() hook and just free the page the "normal" way in the one
place we'll ever see PG_RELEASED and should care (uvm_page_unbusy,
called in aiodoned).

ok art@@, beck@@, thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.72 2009/03/20 15:19:04 oga Exp $	*/
d580 3
a582 2
		if (size >
		    (p->p_rlimit[RLIMIT_DATA].rlim_cur - ptoa(p->p_vmspace->vm_dused))) {
@


1.72
log
@While working on some stuff in uvm I've gotten REALLY sick of reading
K&R function declarations, so switch them all over to ansi-style, in
accordance with the prophesy.

"go for it" art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.71 2008/11/10 03:56:16 deraadt Exp $	*/
d301 1
a301 2
			if (entry->object.uvm_obj->pgops->pgo_releasepg
			    == NULL) {
@


1.71
log
@vm_map_lock() around calls to uvm_map_findspace(); ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.71 2008/11/10 03:54:54 deraadt Exp $	*/
d104 1
a104 4
sys_sbrk(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
d121 1
a121 4
sys_sstk(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
d144 1
a144 4
sys_mquery(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
d232 1
a232 4
sys_mincore(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
d369 1
a369 4
sys_mmap(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
d610 1
a610 4
sys_msync(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
d694 1
a694 4
sys_munmap(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
d761 1
a761 4
sys_mprotect(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
d799 1
a799 4
sys_minherit(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
d831 1
a831 4
sys_madvise(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
d918 1
a918 4
sys_mlock(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
d963 1
a963 4
sys_munlock(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
d1002 1
a1002 4
sys_mlockall(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
d1032 1
a1032 4
sys_munlockall(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
d1049 3
a1051 10
uvm_mmap(map, addr, size, prot, maxprot, flags, handle, foff, locklimit, p)
	vm_map_t map;
	vaddr_t *addr;
	vsize_t size;
	vm_prot_t prot, maxprot;
	int flags;
	caddr_t handle;		/* XXX: VNODE? */
	voff_t foff;
	vsize_t locklimit;
	struct proc *p;
@


1.70
log
@replace the machine dependant bytes-to-clicks macro by the MI ptoa()
version for i386

more architectures and ctob() replacement is being worked on

prodded by and ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.69 2007/06/18 21:51:15 pedro Exp $	*/
d208 2
a210 1

d228 1
@


1.69
log
@Bring back Mickey's UVM anon change. Testing by thib@@, beck@@ and
ckuethe@@ for a while. Okay beck@@, "it is good timing" deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.68 2007/05/31 21:20:30 thib Exp $	*/
d595 1
a595 1
		    (p->p_rlimit[RLIMIT_DATA].rlim_cur - ctob(p->p_vmspace->vm_dused))) {
@


1.68
log
@zap the vm_amap am_l simplelock, and amap_{lock/unlock} macros for
simple_{lock/unlock}.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.67 2007/03/27 16:13:46 art Exp $	*/
d334 1
a334 1
				if (anon != NULL && anon->u.an_page != NULL) {
@


1.67
log
@Clean up some return value handling now that we know that what's returned
is proper errnos.

millert@@ ok and some help
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.66 2007/03/26 08:43:34 art Exp $	*/
a323 2
		if (amap != NULL)
			amap_lock(amap);
a360 2
		if (amap != NULL)
			amap_unlock(amap);
@


1.66
log
@Rip out the KERN_ error codes.
ok otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.65 2007/03/25 11:31:07 art Exp $	*/
d654 1
a654 1
	  return (EINVAL);
d656 1
a656 1
	  flags |= MS_SYNC;
d706 1
a706 9
	/*
	 * doit!
	 */
	rv = uvm_map_clean(map, addr, addr+size, uvmflags);

	/*
	 * and return... 
	 */
	return (rv);
a796 1
	int rv;
d816 2
a817 12
	/*
	 * doit
	 */

	rv = uvm_map_protect(&p->p_vmspace->vm_map, 
			   addr, addr+size, prot, FALSE);

	if (rv == 0)
		return (0);
	if (rv == EACCES)
		return (EACCES);
	return (EINVAL);
d850 2
a851 8
	switch (uvm_map_inherit(&p->p_vmspace->vm_map, addr, addr+size,
			 inherit)) {
	case 0:
		return (0);
	case EACCES:
		return (EACCES);
	}
	return (EINVAL);
d872 1
a872 1
	int advice, rv;
d889 2
a890 2
		rv = uvm_map_advice(&p->p_vmspace->vm_map, addr, addr + size,
		    advice);
d911 1
a911 1
		rv = uvm_map_clean(&p->p_vmspace->vm_map, addr, addr + size,
d921 1
a921 1
		rv = uvm_map_clean(&p->p_vmspace->vm_map, addr, addr + size,
d942 1
a942 1
	return (rv);
d1063 2
a1064 17
	switch (error) {
	case 0:
		error = 0;
		break;

	case ENOMEM:
		error = ENOMEM;
		break;

	default:
		/*
		 * "Some or all of the memory could not be locked when
		 * the call was made."
		 */
		error = EAGAIN;
	}

d1106 1
a1106 1
	int retval;
d1227 1
a1227 3
	/*
	 * do it!
	 */
d1229 1
a1229 3
	retval = uvm_map_p(map, addr, size, uobj, foff, align, uvmflag, p);

	if (retval == 0) {
d1251 1
a1251 1
				retval = ENOMEM;
d1261 1
a1261 1
			retval = uvm_map_pageable(map, *addr, *addr + size,
d1263 1
a1263 1
			if (retval != 0) {
d1283 2
a1284 2
 bad:
	return (retval);
@


1.65
log
@remove KERN_SUCCESS and use 0 instead.
eyeballed by miod@@ and pedro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.64 2007/02/25 19:24:59 millert Exp $	*/
d834 1
a834 1
	if (rv == KERN_PROTECTION_FAILURE)
d873 1
a873 1
	case KERN_PROTECTION_FAILURE:
d1093 1
a1093 1
	case KERN_NO_SPACE:	/* XXX overloaded */
d1295 1
a1295 1
				retval = KERN_RESOURCE_SHORTAGE;
@


1.64
log
@Make integer wrap checks the same for mmap, munmap, msync, etc
by factoring most of the checks into a macro.  OK otto@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.63 2006/07/13 22:51:26 deraadt Exp $	*/
d832 1
a832 1
	if (rv == KERN_SUCCESS)
d871 1
a871 1
	case KERN_SUCCESS:
d1015 1
a1015 1
	return (error == KERN_SUCCESS ? 0 : ENOMEM);
d1057 1
a1057 1
	return (error == KERN_SUCCESS ? 0 : ENOMEM);
d1089 1
a1089 1
	case KERN_SUCCESS:
d1273 1
a1273 1
	if (retval == KERN_SUCCESS) {
d1307 1
a1307 1
			if (retval != KERN_SUCCESS) {
@


1.63
log
@Back out the anon change.  Apparently it was tested by a few, but most of
us did not see it or get a chance to test it before it was commited. It
broke cvs, in the ami driver, making it not succeed at seeing it's devices.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.60 2006/04/06 20:58:06 kurt Exp $	*/
d76 17
d437 1
a437 9

	pageoff = (pos & PAGE_MASK);
	pos  -= pageoff;
	size += pageoff;			/* add offset */
	if (size != 0) {
		size = (vsize_t) round_page(size);	/* round up */
		if (size == 0)
			return (ENOMEM);		/* don't allow wrap */
	}
d445 1
a445 1
		/* ensure address and file offset are aligned properly */
d448 1
a448 1
			return (EINVAL);
d450 2
a456 2
		if (addr > addr + size)
			return (EINVAL);		/* no wrapping! */
d661 3
a663 9

	pageoff = (addr & PAGE_MASK);
	addr -= pageoff;
	size += pageoff;
	size = (vsize_t) round_page(size);

	/* disallow wrap-around. */
	if (addr + (ssize_t)size < addr)
		return (EINVAL);
d747 1
a747 10

	pageoff = (addr & PAGE_MASK);
	addr -= pageoff;
	size += pageoff;
	size = (vsize_t) round_page(size);

	if ((ssize_t)size < 0)
		return (EINVAL);
	if (size == 0)
		return (0);
d753 2
a758 2
	if (addr > addr + size)
		return (EINVAL);
d821 3
a823 6
	pageoff = (addr & PAGE_MASK);
	addr -= pageoff;
	size += pageoff;
	size = (vsize_t) round_page(size);
	if ((ssize_t)size < 0)
		return (EINVAL);
d861 1
d865 3
a867 8

	pageoff = (addr & PAGE_MASK);
	addr -= pageoff;
	size += pageoff;
	size = (vsize_t) round_page(size);

	if ((ssize_t)size < 0)
		return (EINVAL);
d906 3
a908 7
	pageoff = (addr & PAGE_MASK);
	addr -= pageoff;
	size += pageoff;
	size = (vsize_t) round_page(size);

	if ((ssize_t)size <= 0)
		return (EINVAL);
d997 3
a999 8
	pageoff = (addr & PAGE_MASK);
	addr -= pageoff;
	size += pageoff;
	size = (vsize_t) round_page(size);
	
	/* disallow wrap-around. */
	if (addr + (ssize_t)size < addr)
		return (EINVAL);
d1046 3
a1048 8
	pageoff = (addr & PAGE_MASK);
	addr -= pageoff;
	size += pageoff;
	size = (vsize_t) round_page(size);

	/* disallow wrap-around. */
	if (addr + (ssize_t)size < addr)
		return (EINVAL);
@


1.62
log
@fallout from previous: remmapping anonymous memory did not account dsize proper; found by krause and mmap_fixed
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.61 2006/06/21 16:20:05 mickey Exp $	*/
d319 1
a319 1
				if (anon != NULL && anon->an_page != NULL) {
@


1.61
log
@from netbsd: make anons dynamically allocated from pool.
this results in lesse kva waste due to static preallocation of those
for every phys page and also every swap page.
tested by beck krw miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.57 2005/06/01 18:34:40 tedu Exp $	*/
d1200 1
a1200 1
		uvm_unmap(map, *addr, *addr + size);	/* zap! */
@


1.60
log
@Fix a process datasize leak with MAP_FIXED. When zapping old mappings
call uvm_unmap_p instead of uvm_unmap so that it has the process information
and can adjust vm_dused. okay pedro@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.59 2006/04/04 21:10:29 miod Exp $	*/
d319 1
a319 1
				if (anon != NULL && anon->u.an_page != NULL) {
d1200 1
a1200 1
		uvm_unmap_p(map, *addr, *addr + size, p);	/* zap! */
@


1.59
log
@Revert r1.58, I was on drugs - the array we are locking is one byte per
page, so the arithmetic was ok. Spotted by david@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.57 2005/06/01 18:34:40 tedu Exp $	*/
d1200 1
a1200 1
		uvm_unmap(map, *addr, *addr + size);	/* zap! */
@


1.58
log
@In sys_mincore(), pass a size in bytes, not pages, to uvm_vslock() and
uvm_vsunlock(). ok mickey@@
@
text
@d240 1
a240 1
	vsize_t len;
d256 2
d262 1
a262 1
	if ((error = uvm_vslock(p, vec, len, VM_PROT_WRITE)) != 0)
d352 1
a352 1
	uvm_vsunlock(p, SCARG(uap, vec), len);
@


1.57
log
@use vm_dused for rlimit.  much happier with mmap.  tested by several
over past week.  as a bonus, kills 5 XXXs.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.56 2005/05/24 21:11:47 tedu Exp $	*/
d241 1
a241 1
	int error = 0, npgs;
a255 2
	npgs = len >> PAGE_SHIFT;

d260 1
a260 1
	if ((error = uvm_vslock(p, vec, npgs, VM_PROT_WRITE)) != 0)
d350 1
a350 1
	uvm_vsunlock(p, SCARG(uap, vec), npgs);
@


1.57.4.1
log
@MFC:
Fix by kurt@@

Fix a process datasize leak with MAP_FIXED. When zapping old mappings
call uvm_unmap_p instead of uvm_unmap so that it has the process information
and can adjust vm_dused.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.57 2005/06/01 18:34:40 tedu Exp $	*/
d1200 1
a1200 1
		uvm_unmap_p(map, *addr, *addr + size, p);	/* zap! */
@


1.57.2.1
log
@MFC:
Fix by kurt@@

Fix a process datasize leak with MAP_FIXED. When zapping old mappings
call uvm_unmap_p instead of uvm_unmap so that it has the process information
and can adjust vm_dused.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.57 2005/06/01 18:34:40 tedu Exp $	*/
d1200 1
a1200 1
		uvm_unmap_p(map, *addr, *addr + size, p);	/* zap! */
@


1.56
log
@add a new field to vm_space and use it to track the number of anon
pages a process uses.  this is now the userland "data size" value.
ok art deraadt tdeval.  thanks testers.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.55 2005/01/15 06:54:51 otto Exp $	*/
a586 7
	/*
	 * XXX (in)sanity check.  We don't do proper datasize checking
	 * XXX for anonymous (or private writable) mmap().  However,
	 * XXX know that if we're trying to allocate more than the amount
	 * XXX remaining under our current data size limit, _that_ should
	 * XXX be disallowed.
	 */
d590 1
a590 1
		    (p->p_rlimit[RLIMIT_DATA].rlim_cur - ctob(p->p_vmspace->vm_dsize))) {
@


1.55
log
@In uvm_mmap(), check for size wrap to 0, and return ENOMEM in that
case. Do not arbitarily disallow sizes with the high bit set, they
are unsigned. With lotsa help from miod@@, test by danh@@
ok miod@@ millert@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.54 2004/05/07 22:47:47 tedu Exp $	*/
d608 1
a608 1
	    flags, handle, pos, p->p_rlimit[RLIMIT_MEMLOCK].rlim_cur);
d790 1
a790 1
	uvm_unmap_remove(map, addr, addr + size, &dead_entries);
d1167 1
a1167 1
uvm_mmap(map, addr, size, prot, maxprot, flags, handle, foff, locklimit)
d1176 1
d1305 1
a1305 1
	retval = uvm_map(map, addr, size, uobj, foff, align, uvmflag);
@


1.54
log
@align to __LDPGSZ for anon mmap.  this allows userland to be compiled
with a static page size on platforms where it may vary.
ok deraadt@@ millert@@ tdeval@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.53 2003/09/02 17:57:12 tedu Exp $	*/
d424 5
a428 3
	size = (vsize_t) round_page(size);	/* round up */
	if ((ssize_t) size < 0)
		return (EINVAL);			/* don't allow wrap */
@


1.54.2.1
log
@MFC:
Fix by otto@@

In uvm_mmap(), check for size wrap to 0, and return ENOMEM in that
case. Do not arbitarily disallow sizes with the high bit set, they
are unsigned.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.54 2004/05/07 22:47:47 tedu Exp $	*/
d424 3
a426 5
	if (size != 0) {
		size = (vsize_t) round_page(size);	/* round up */
		if (size == 0)
			return (ENOMEM);		/* don't allow wrap */
	}
@


1.53
log
@add a random offset to uvm_map_hint.  this has the primary effect of
scattering ld.so and libraries around, although all mmaps will also
have some jitter too.  better version after some discussion with drahn
testing/ok deraadt henning marcm otto pb
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.52 2003/09/01 18:06:44 henning Exp $	*/
d66 2
d1180 1
a1200 1
		
d1213 2
d1302 1
a1302 1
	retval = uvm_map(map, addr, size, uobj, foff, 0, uvmflag);
@


1.53.4.1
log
@MFC:
Fix by otto@@

In uvm_mmap(), check for size wrap to 0, and return ENOMEM in that
case. Do not arbitarily disallow sizes with the high bit set, they
are unsigned.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.53 2003/09/02 17:57:12 tedu Exp $	*/
d422 3
a424 5
	if (size != 0) {
		size = (vsize_t) round_page(size);	/* round up */
		if (size == 0)
			return (ENOMEM);		/* don't allow wrap */
	}
@


1.52
log
@match syscallargs comments with reality
from Patrick Latifi <patrick.l@@hermes.usherb.ca>
ok jason@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.51 2003/08/15 20:32:21 tedu Exp $	*/
d454 1
a454 1
		    addr < uvm_map_hint(p, prot))
@


1.51
log
@change arguments to suser.  suser now takes the process, and a flags
argument.  old cred only calls user suser_ucred.  this will allow future
work to more flexibly implement the idea of a root process.  looks like
something i saw in freebsd, but a little different.
use of suser_ucred vs suser in file system code should be looked at again,
for the moment semantics remain unchanged.
review and input from art@@  testing and further review miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.50 2003/08/06 21:08:07 millert Exp $	*/
d92 1
a92 1
		syscallarg(intptr_t) incr;
d137 1
a137 1
		syscallarg(caddr_t) addr;
d370 1
a370 1
		syscallarg(caddr_t) addr;
d627 1
a627 1
		syscallarg(caddr_t) addr;
d728 1
a728 1
		syscallarg(caddr_t) addr;
d807 2
a808 2
		syscallarg(caddr_t) addr;
		syscallarg(int) len;
d862 2
a863 2
		syscallarg(caddr_t) addr;
		syscallarg(int) len;
d907 1
a907 1
		syscallarg(caddr_t) addr;
@


1.50
log
@Remove some double semicolons (hmm, do two semis equal a maxi?).
I've skipped the GNU stuff for now.  From Patrick Latifi.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.49 2003/07/21 22:52:19 tedu Exp $	*/
d1034 1
a1034 1
	if ((error = suser(p->p_ucred, &p->p_acflag)) != 0)
d1081 1
a1081 1
	if ((error = suser(p->p_ucred, &p->p_acflag)) != 0)
d1112 1
a1112 1
	if ((error = suser(p->p_ucred, &p->p_acflag)) != 0)
@


1.49
log
@enforce restrictions on prot and flags to mprotect and mmap.  invalid or
undefined flags are now rejected instead of silently ignored.  makes
"unintentional" mprotect calls a touch harder.
ok art@@ deraadt@@ jason@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.48 2003/07/01 23:23:04 tedu Exp $	*/
d913 1
a913 1
	int advice, rv;;
@


1.48
log
@add MAP_TRYFIXED, mostly to help emulate other systems.
when set, uvm will not attempt to avoid a heap address, if requested.
from todd vierling, via
http://marc.theaimsgroup.com/?l=netbsd-tech-kern&m=105612525808607&w=1
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.46 2003/05/17 14:02:06 grange Exp $	*/
d156 1
a156 1
	prot = SCARG(uap, prot) & VM_PROT_ALL;
d160 3
d397 1
a397 1
	prot = SCARG(uap, prot) & VM_PROT_ALL;
d406 4
d822 4
a825 1
	prot = SCARG(uap, prot) & VM_PROT_ALL;
@


1.47
log
@remove sys_omquery.  it was only used for two weeks, and you can't
source upgrade from a system that used it anyway.
ok art deraadt drahn
@
text
@d444 4
a447 2

		if (addr < uvm_map_hint(p, prot))
@


1.46
log
@Typos; from Julien Bordet <zejames@@greyhats.org>
Close PR 3262
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.45 2003/04/28 21:32:08 drahn Exp $	*/
a203 72
	}
done:
	if (fp != NULL)
		FRELE(fp);
	return (error);
}

/* ARGSUSED */
int
sys_omquery(struct proc *p, void *v, register_t *retval)
{
	struct sys_omquery_args /* {
		syscallarg(int) flags;
		syscallarg(void **) addr;
		syscallarg(size_t) size;
		syscallarg(int) fd;
		syscallarg(off_t) off;
	} */ *uap = v;
	struct file *fp;
	struct uvm_object *uobj;
	voff_t uoff;
	int error;
	vaddr_t vaddr;
	int flags = 0;
	vm_prot_t prot = SCARG(uap, flags) & VM_PROT_ALL;

	if (SCARG(uap, flags) & MAP_FIXED)
		flags |= UVM_FLAG_FIXED;

	if ((error = copyin(SCARG(uap, addr), &vaddr, sizeof(void *))) != 0)
		return (error);

	if (SCARG(uap, fd) >= 0) {
		if ((error = getvnode(p->p_fd, SCARG(uap, fd), &fp)) != 0)
			return (error);
		uobj = &((struct vnode *)fp->f_data)->v_uvm.u_obj;
		uoff = SCARG(uap, off);
	} else {
		fp = NULL;
		uobj = NULL;
		uoff = 0;
	}

	if (vaddr == 0)
		vaddr = uvm_map_hint(p, prot);

	/* prevent a user requested address from falling in heap space */
	if ((vaddr + SCARG(uap, size) > (vaddr_t)p->p_vmspace->vm_daddr) &&
	    (vaddr < (vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ)) {
		if (flags & UVM_FLAG_FIXED) {
			error = EINVAL;
			goto done;
		}
		vaddr = round_page((vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ);
	}

	if (uvm_map_findspace(&p->p_vmspace->vm_map, vaddr, SCARG(uap, size),
	    &vaddr, uobj, uoff, 0, flags) == NULL) {
		if (flags & UVM_FLAG_FIXED)
			error = EINVAL;
		else
			error = ENOMEM;
	} else {
		/*
		 * XXX?
		 * is it possible for uvm_map_findspace() to return
		 * an address in vm_addr - vm_addr+MAXDSIZ ?
		 * if all of the memory below 1G (i386) is used, 
		 * this could occur. In this case, could this loop
		 * changing the hint to above daddr in that case?
		 */
		error = copyout(&vaddr, SCARG(uap, addr), sizeof(void *));
@


1.45
log
@Change mquery() function call signature to be the same a mmap(). It
needs the prot/flags info and passing the addresses via arg/return allows
it to be traced via ktrace better than an in/out paramter.
This adds a new mquery syscall and renames the old one to omquery.
New kernel _MUST_ be built, booted, and 'make includes' before building
ld.so with this change.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.44 2003/04/25 20:32:07 drahn Exp $	*/
d426 1
a426 1
 * => file offest and address may not be page aligned
@


1.44
log
@backout mquery change, something broke when not combined with a different diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.42 2003/04/18 23:47:59 drahn Exp $	*/
d130 81
d213 1
a213 1
sys_mquery(struct proc *p, void *v, register_t *retval)
d215 1
a215 1
	struct sys_mquery_args /* {
@


1.43
log
@change mquery() function call signature to be the same a mmap(). It
needs the prot/flags info and passing the addresses via arg/return allows
it to be traced via ktrace better than an in/out paramter.
This adds a new mquery syscall and renames the old one to omquery.
New kernel _MUST_ be built and installed before building ld.so with this change.
ok millert@@ tedu@@
@
text
@d130 1
d132 1
a132 4
sys_mquery(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
a134 79
		syscallarg(caddr_t) addr;
		syscallarg(size_t) len;
		syscallarg(int) prot;
		syscallarg(int) flags;
		syscallarg(int) fd;
		syscallarg(long) pad;
		syscallarg(off_t) pos;
	} */ *uap = v;
	struct file *fp;
	struct uvm_object *uobj;
	voff_t uoff;
	int error;
	vaddr_t vaddr;
	int flags = 0;
	vsize_t size;
	vm_prot_t prot;
	int fd;

	vaddr = (vaddr_t) SCARG(uap, addr);
	prot = SCARG(uap, prot) & VM_PROT_ALL;
	size = (vsize_t) SCARG(uap, len);
	fd = SCARG(uap, fd);

	if (SCARG(uap, flags) & MAP_FIXED)
		flags |= UVM_FLAG_FIXED;

	if (fd >= 0) {
		if ((error = getvnode(p->p_fd, fd, &fp)) != 0)
			return (error);
		uobj = &((struct vnode *)fp->f_data)->v_uvm.u_obj;
		uoff = SCARG(uap, pos);
	} else {
		fp = NULL;
		uobj = NULL;
		uoff = 0;
	}

	if (vaddr == 0)
		vaddr = uvm_map_hint(p, prot);

	/* prevent a user requested address from falling in heap space */
	if ((vaddr + size > (vaddr_t)p->p_vmspace->vm_daddr) &&
	    (vaddr < (vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ)) {
		if (flags & UVM_FLAG_FIXED) {
			error = EINVAL;
			goto done;
		}
		vaddr = round_page((vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ);
	}
again:

	if (uvm_map_findspace(&p->p_vmspace->vm_map, vaddr, size,
	    &vaddr, uobj, uoff, 0, flags) == NULL) {
		if (flags & UVM_FLAG_FIXED)
			error = EINVAL;
		else
			error = ENOMEM;
	} else {
		/* prevent a returned address from falling in heap space */
		if ((vaddr + size > (vaddr_t)p->p_vmspace->vm_daddr)
		    && (vaddr < (vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ)) {
			vaddr = round_page((vaddr_t)p->p_vmspace->vm_daddr +
			    MAXDSIZ);
			goto again;
		}
		error = 0;
		*retval = (register_t)(vaddr);
	}
done:
	if (fp != NULL)
		FRELE(fp);
	return (error);
}

/* ARGSUSED */
int
sys_omquery(struct proc *p, void *v, register_t *retval)
{
	struct sys_omquery_args /* {
@


1.42
log
@Return EINVAL if MAP_FIXED was specified but was not available. ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.41 2003/04/17 03:50:54 drahn Exp $	*/
d130 81
d213 1
a213 1
sys_mquery(struct proc *p, void *v, register_t *retval)
d215 1
a215 1
	struct sys_mquery_args /* {
@


1.41
log
@changes to support mquery with 1Gsep on i386. avoid heap on mappings.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.40 2003/04/14 04:53:51 art Exp $	*/
d171 5
a175 1
	    (vaddr < (vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ))
d177 1
d181 4
a184 1
		error = ENOMEM;
d196 1
a196 1

@


1.40
log
@There are two related changes.

The first one is an mquery(2) syscall. It's for asking the VM system
about where to map things. It will be used by ld.so, read the man page
for details.

The second change is related and is a centralization of uvm_map hint
that all callers of uvm_map calculated. This will allow us to adjust
this hint on architectures that have segments for non-exec mappings.

deraadt@@ drahn@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.39 2003/04/07 14:47:08 mpech Exp $	*/
d169 5
d178 8
@


1.39
log
@int -> ssize_t.
+checked by regress.

millert@@, art@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.38 2003/01/09 22:27:12 miod Exp $	*/
d120 62
d415 2
a416 4
		if (addr < round_page((vaddr_t)p->p_vmspace->vm_daddr +
		    MAXDSIZ))
			addr = round_page((vaddr_t)p->p_vmspace->vm_daddr +
			    MAXDSIZ);
@


1.38
log
@Remove fetch(9) and store(9) functions from the kernel, and replace the few
remaining instances of them with appropriate copy(9) usage.

ok art@@, tested on all arches unless my memory is non-ECC
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.37 2002/11/08 04:06:02 art Exp $	*/
d563 1
a563 1
	if (addr + size < addr)
d654 1
a654 1
	if ((int)size < 0)
d732 1
a732 1
	if ((int)size < 0)
d780 1
a780 1
	if ((int)size < 0)
d921 1
a921 1
	if (addr + (int)size < addr)
d975 1
a975 1
	if (addr + (int)size < addr)
@


1.37
log
@Don't uvm_useracc and then vslock. vslock is better at finding illegal mappings.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.36 2002/10/29 18:30:21 art Exp $	*/
d199 1
d202 1
a202 1
					subyte(vec, 1);
d244 1
a244 1
			(void) subyte(vec, pgi);
@


1.36
log
@Since memory deallocation can't fail, remove the error return from
uvm_unmap, uvm_deallocate and a few other functions.
Simplifies some code and reduces diff to the UBC branch.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.35 2002/08/23 00:53:51 pvalchev Exp $	*/
a160 3
	if (uvm_useracc(vec, npgs, B_WRITE) == FALSE)
		return (EFAULT);

d165 2
a166 1
	uvm_vslock(p, vec, npgs, VM_PROT_WRITE);
@


1.35
log
@Fix missing FRELE in mmap(2); ok art
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.34 2002/02/14 22:46:44 art Exp $	*/
d688 1
a688 1
	(void) uvm_unmap_remove(map, addr, addr + size, &dead_entries);
d1101 1
a1101 1
		(void) uvm_unmap(map, *addr, *addr + size);	/* zap! */
d1224 1
a1224 1
				(void) uvm_unmap(map, *addr, *addr + size);
d1235 1
a1235 1
				(void) uvm_unmap(map, *addr, *addr + size);
@


1.34
log
@Correctly FREF/FRELE in mmap(2).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.33 2001/12/19 08:58:07 art Exp $	*/
d366 1
a366 1
			return(EBADF);
d370 4
a373 2
		if (fp->f_type != DTYPE_VNODE)
			return (ENODEV);		/* only mmap vnodes! */
d448 1
a448 1
					return (error);
d451 4
a454 2
				else if (prot & PROT_WRITE)
					return (EPERM);
d474 4
a477 2
		if (fd != -1)
			return (EINVAL);
@


1.33
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.27 2001/11/12 01:26:09 art Exp $	*/
d292 1
a292 1
	struct file *fp;
a362 1

d368 2
d375 4
a378 2
		    vp->v_type != VBLK)
			return (ENODEV);  /* only REG/CHR/BLK support mmap */
d380 4
a383 2
		if (vp->v_type == VREG && (pos + size) < pos)
			return (EINVAL);		/* no offset wrapping */
d388 2
d430 4
a433 2
		else if (prot & PROT_READ)
			return (EACCES);
d451 3
a454 2
			else if (prot & PROT_WRITE)
				return (EACCES);
d490 2
a491 1
			return (ENOMEM);
d506 3
@


1.32
log
@Merge in struct uvm_vnode into struct vnode.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_mmap.c,v 1.31 2001/12/04 23:22:42 art Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.55 2001/08/17 05:52:46 chs Exp $	*/
d6 1
a6 1
 * Copyright (c) 1991, 1993 The Regents of the University of California.
d8 1
a8 1
 *
d26 1
a26 1
 *	Washington University, University of California, Berkeley and
d72 1
d135 1
a135 1
	struct vm_page *m;
d140 1
a140 1
	struct vm_map_entry *entry;
d142 1
a142 1
	struct vm_map *map;
d168 1
a169 1
	uvm_vslock(p, vec, npgs, VM_PROT_WRITE);
a196 1

a223 1

a227 1

d231 1
a236 1

a240 1

d244 1
d247 1
d294 1
a294 1
	void *handle;
d301 2
a302 2
	addr = (vaddr_t)SCARG(uap, addr);
	size = (vsize_t)SCARG(uap, len);
d324 1
a324 1
	size = (vsize_t)round_page(size);	/* round up */
d329 1
a329 1
	 * now check (MAP_FIXED) or get (!MAP_FIXED) the "addr"
d354 4
a357 2
		addr = MAX(addr, round_page((vaddr_t)p->p_vmspace->vm_daddr +
					    MAXDSIZ));
d405 1
a405 1
		/*
d449 6
a454 1
		handle = vp;
d479 1
a479 2
		    (p->p_rlimit[RLIMIT_DATA].rlim_cur -
		     ctob(p->p_vmspace->vm_dsize))) {
d515 2
a516 2
	struct vm_map *map;
	int error, rv, flags, uvmflags;
d535 1
a535 1
	 * align the address to a page boundary and adjust the size accordingly.
d541 1
a541 1
	size = (vsize_t)round_page(size);
a562 1

d564 2
a565 2
		struct vm_map_entry *entry;

a579 1

d588 9
a596 2
	error = uvm_map_clean(map, addr, addr+size, uvmflags);
	return error;
d615 1
a615 1
	struct vm_map *map;
d620 1
a620 1
	 * get syscall args.
d623 3
a625 3
	addr = (vaddr_t)SCARG(uap, addr);
	size = (vsize_t)SCARG(uap, len);

d627 1
a627 1
	 * align the address to a page boundary and adjust the size accordingly.
d633 1
a633 1
	size = (vsize_t)round_page(size);
d652 3
d656 1
a656 1
	 * interesting system call semantic: make sure entire range is
a659 1
	vm_map_lock(map);
d664 8
a671 2
	uvm_unmap_remove(map, addr, addr + size, &dead_entries);
	vm_map_unlock(map);
d674 1
d696 1
a696 1
	int error;
d707 1
a707 1
	 * align the address to a page boundary and adjust the size accordingly.
a708 1

d712 1
a712 2
	size = (vsize_t)round_page(size);

d715 13
a727 3
	error = uvm_map_protect(&p->p_vmspace->vm_map, addr, addr + size, prot,
				FALSE);
	return error;
d748 1
a748 2
	int error;

a751 1

d753 1
a753 1
	 * align the address to a page boundary and adjust the size accordingly.
d759 1
a759 1
	size = (vsize_t)round_page(size);
d763 9
a771 3
	error = uvm_map_inherit(&p->p_vmspace->vm_map, addr, addr + size,
				inherit);
	return error;
d792 2
a793 2
	int advice, error;

a800 1

d804 1
a804 1
	size = (vsize_t)round_page(size);
d813 1
a813 1
		error = uvm_map_advice(&p->p_vmspace->vm_map, addr, addr + size,
a817 1

a826 1

a829 1

d835 1
a835 2

		error = uvm_map_clean(&p->p_vmspace->vm_map, addr, addr + size,
a839 1

d845 1
a845 2

		error = uvm_map_clean(&p->p_vmspace->vm_map, addr, addr + size,
a849 1

a859 1

d866 1
a866 1
	return error;
a889 1

a895 1

d899 2
a900 2
	size = (vsize_t)round_page(size);

d902 1
a902 1
	if (addr + size < addr)
d919 1
a919 1
	return error;
a949 1

d953 1
a953 1
	size = (vsize_t)round_page(size);
d956 1
a956 1
	if (addr + size < addr)
d966 1
a966 1
	return error;
d997 17
d1043 1
a1043 1
	struct vm_map *map;
d1048 1
a1048 1
	void *handle;
d1054 1
a1054 1
	int error;
d1077 1
d1100 2
a1101 1
		vp = (struct vnode *)handle;
d1103 1
a1103 6
			error = VOP_MMAP(vp, 0, curproc->p_ucred, curproc);
			if (error) {
				return error;
			}

			uobj = uvn_attach((void *)vp, (flags & MAP_SHARED) ?
d1106 31
d1139 1
d1151 1
a1151 1
				uobj = udv_attach((void *)&vp->v_rdev,
d1157 1
d1160 1
d1165 5
a1169 1
	uvmflag = UVM_MAPFLAG(prot, maxprot,
a1171 6
	error = uvm_map(map, addr, size, uobj, foff, 0, uvmflag);
	if (error) {
		if (uobj)
			uobj->pgops->pgo_detach(uobj);
		return error;
	}
d1174 1
a1174 2
	 * POSIX 1003.1b -- if our address space was configured
	 * to lock all future mappings, wire the one we just made.
d1177 1
a1177 1
	if (prot == VM_PROT_NONE) {
d1179 1
d1181 2
a1182 1
		 * No more work to do in this case.
d1184 8
d1193 2
a1194 5
		return (0);
	}
	vm_map_lock(map);
	if (map->flags & VM_MAP_WIREFUTURE) {
		if ((atop(size) + uvmexp.wired) > uvmexp.wiredmax
d1196 3
a1198 3
		    || (locklimit != 0 && (size +
		    ptoa(pmap_wired_count(vm_map_pmap(map)))) >
			locklimit)
d1200 19
a1218 4
		) {
			vm_map_unlock(map);
			uvm_unmap(map, *addr, *addr + size);
			return ENOMEM;
d1221 1
a1221 3
		/*
		 * uvm_map_pageable() always returns the map unlocked.
		 */
a1222 6
		error = uvm_map_pageable(map, *addr, *addr + size,
					 FALSE, UVM_LK_ENTER);
		if (error) {
			uvm_unmap(map, *addr, *addr + size);
			return error;
		}
d1225 10
a1234 2
	vm_map_unlock(map);
	return 0;
@


1.32.2.1
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_mmap.c,v 1.61 2001/11/25 06:42:47 chs Exp $	*/
d134 1
a134 1
	struct vm_page *pg;
d199 2
a200 1
			if (!UVM_OBJ_IS_VNODE(entry->object.uvm_obj)) {
d235 1
a235 1
				pg = uvm_pagelookup(uobj,
d237 1
a237 1
				if (pg != NULL) {
d366 1
a366 1
			return (EBADF);
d376 1
a376 4
		if (vp->v_type != VCHR && pos < 0)
			return (EINVAL);

		if (vp->v_type != VCHR && (pos + size) < pos)
d999 3
a1001 2
 * - used by sys_mmap and various framebuffers
 * - handle is a vnode pointer or NULL for MAP_ANON
d1039 1
a1039 1
		*addr = round_page(*addr);
d1044 1
a1044 1
		(void) uvm_unmap(map, *addr, *addr + size);
a1063 9

		/*
		 * Don't allow mmap for EXEC if the file system
		 * is mounted NOEXEC.
		 */
		if ((prot & PROT_EXEC) != 0 &&
		    (vp->v_mount->mnt_flag & MNT_NOEXEC) != 0)
			return (EACCES);

a1074 9

#ifdef notyet	/* XXXART */
			/*
			 * If the vnode is being mapped with PROT_EXEC,
			 * then mark it as text.
			 */
			if (prot & PROT_EXEC)
				vn_markexec(vp);
#endif
@


1.32.2.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.32.2.1 2002/02/02 03:28:27 art Exp $	*/
d292 1
a292 1
	struct file *fp = NULL;
d361 1
a366 2
		FREF(fp);

d372 2
a373 4
		    vp->v_type != VBLK) {
			error = ENODEV; /* only REG/CHR/BLK support mmap */
			goto out;
		}
d375 2
a376 4
		if (vp->v_type != VCHR && pos < 0) {
			error = ENODEV;
			goto out;
		}
d378 2
a379 4
		if (vp->v_type != VCHR && (pos + size) < pos) {
			error = EINVAL;
			goto out;
		}
a383 2
			FRELE(fp);
			fp = NULL;
d424 2
a425 4
		else if (prot & PROT_READ) {
			error = EACCES;
			goto out;
		}
a442 3
			} else if (prot & PROT_WRITE) {
				error = EACCES;
				goto out;
d444 2
d475 3
a477 3
		    (p->p_rlimit[RLIMIT_DATA].rlim_cur - ctob(p->p_vmspace->vm_dsize))) {
			error = ENOMEM;
			goto out;
a491 3
out:
	if (fp)
		FRELE(fp);	
@


1.32.2.3
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.32.2.2 2002/06/11 03:33:04 art Exp $	*/
d368 2
a369 4
		if (fp->f_type != DTYPE_VNODE) {
			error = ENODEV;		/* only mmap vnodes! */
			goto out;
		}
d449 1
a449 1
					goto out;
d452 2
a453 4
				else if (prot & PROT_WRITE) {
					error = EPERM;
					goto out;
				}
d468 2
a469 4
		if (fd != -1) {
			error = EINVAL;
			goto out;
		}
@


1.32.2.4
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_mmap.c,v 1.32.2.3 2002/10/29 00:36:50 art Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.66 2002/09/27 19:13:29 mycroft Exp $	*/
d158 5
d168 1
a168 5
	npgs = len >> PAGE_SHIFT;
	error = uvm_vslock(p, vec, npgs, VM_PROT_WRITE);
	if (error) {
		return error;
	}
d263 1
a263 1
 * => file offset and address may not be page aligned
d341 1
a341 1
			return (EFBIG);
a670 1
#if 0
a674 1
#endif
@


1.32.2.5
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a118 164
 * sys_mquery: provide mapping hints to applications that do fixed mappings
 *
 * flags: 0 or MAP_FIXED (MAP_FIXED - means that we insist on this addr and
 *	don't care about PMAP_PREFER or such)
 * addr: hint where we'd like to place the mapping.
 * size: size of the mapping
 * fd: fd of the file we want to map
 * off: offset within the file
 */

int
sys_mquery(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{
	struct sys_mquery_args /* {
		syscallarg(caddr_t) addr;
		syscallarg(size_t) len;
		syscallarg(int) prot;
		syscallarg(int) flags;
		syscallarg(int) fd;
		syscallarg(long) pad;
		syscallarg(off_t) pos;
	} */ *uap = v;
	struct file *fp;
	struct uvm_object *uobj;
	voff_t uoff;
	int error;
	vaddr_t vaddr;
	int flags = 0;
	vsize_t size;
	vm_prot_t prot;
	int fd;

	vaddr = (vaddr_t) SCARG(uap, addr);
	prot = SCARG(uap, prot) & VM_PROT_ALL;
	size = (vsize_t) SCARG(uap, len);
	fd = SCARG(uap, fd);

	if (SCARG(uap, flags) & MAP_FIXED)
		flags |= UVM_FLAG_FIXED;

	if (fd >= 0) {
		if ((error = getvnode(p->p_fd, fd, &fp)) != 0)
			return (error);
		uobj = &((struct vnode *)fp->f_data)->v_uobj;
		uoff = SCARG(uap, pos);
	} else {
		fp = NULL;
		uobj = NULL;
		uoff = 0;
	}

	if (vaddr == 0)
		vaddr = uvm_map_hint(p, prot);

	/* prevent a user requested address from falling in heap space */
	if ((vaddr + size > (vaddr_t)p->p_vmspace->vm_daddr) &&
	    (vaddr < (vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ)) {
		if (flags & UVM_FLAG_FIXED) {
			error = EINVAL;
			goto done;
		}
		vaddr = round_page((vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ);
	}
again:

	if (uvm_map_findspace(&p->p_vmspace->vm_map, vaddr, size,
	    &vaddr, uobj, uoff, 0, flags) == NULL) {
		if (flags & UVM_FLAG_FIXED)
			error = EINVAL;
		else
			error = ENOMEM;
	} else {
		/* prevent a returned address from falling in heap space */
		if ((vaddr + size > (vaddr_t)p->p_vmspace->vm_daddr)
		    && (vaddr < (vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ)) {
			vaddr = round_page((vaddr_t)p->p_vmspace->vm_daddr +
			    MAXDSIZ);
			goto again;
		}
		error = 0;
		*retval = (register_t)(vaddr);
	}
done:
	if (fp != NULL)
		FRELE(fp);
	return (error);
}

/* ARGSUSED */
int
sys_omquery(struct proc *p, void *v, register_t *retval)
{
	struct sys_omquery_args /* {
		syscallarg(int) flags;
		syscallarg(void **) addr;
		syscallarg(size_t) size;
		syscallarg(int) fd;
		syscallarg(off_t) off;
	} */ *uap = v;
	struct file *fp;
	struct uvm_object *uobj;
	voff_t uoff;
	int error;
	vaddr_t vaddr;
	int flags = 0;
	vm_prot_t prot = SCARG(uap, flags) & VM_PROT_ALL;

	if (SCARG(uap, flags) & MAP_FIXED)
		flags |= UVM_FLAG_FIXED;

	if ((error = copyin(SCARG(uap, addr), &vaddr, sizeof(void *))) != 0)
		return (error);

	if (SCARG(uap, fd) >= 0) {
		if ((error = getvnode(p->p_fd, SCARG(uap, fd), &fp)) != 0)
			return (error);
		uobj = &((struct vnode *)fp->f_data)->v_uobj;
		uoff = SCARG(uap, off);
	} else {
		fp = NULL;
		uobj = NULL;
		uoff = 0;
	}

	if (vaddr == 0)
		vaddr = uvm_map_hint(p, prot);

	/* prevent a user requested address from falling in heap space */
	if ((vaddr + SCARG(uap, size) > (vaddr_t)p->p_vmspace->vm_daddr) &&
	    (vaddr < (vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ)) {
		if (flags & UVM_FLAG_FIXED) {
			error = EINVAL;
			goto done;
		}
		vaddr = round_page((vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ);
	}

	if (uvm_map_findspace(&p->p_vmspace->vm_map, vaddr, SCARG(uap, size),
	    &vaddr, uobj, uoff, 0, flags) == NULL) {
		if (flags & UVM_FLAG_FIXED)
			error = EINVAL;
		else
			error = ENOMEM;
	} else {
		/*
		 * XXX?
		 * is it possible for uvm_map_findspace() to return
		 * an address in vm_addr - vm_addr+MAXDSIZ ?
		 * if all of the memory below 1G (i386) is used, 
		 * this could occur. In this case, could this loop
		 * changing the hint to above daddr in that case?
		 */
		error = copyout(&vaddr, SCARG(uap, addr), sizeof(void *));
	}
done:
	if (fp != NULL)
		FRELE(fp);
	return (error);
}

/*
a198 1
				pgi = 1;
d201 1
a201 1
					copyout(&pgi, vec, sizeof(char));
d245 1
a245 1
			copyout(&pgi, vec, sizeof(char));
d353 2
a354 2
		if (addr < uvm_map_hint(p, prot))
			addr = uvm_map_hint(p, prot);
d561 1
a561 1
	if (addr + (ssize_t)size < addr)
d647 1
a647 1
	if ((ssize_t)size < 0)
d720 1
a720 1
	if ((ssize_t)size < 0)
d760 1
a760 1
	if ((ssize_t)size < 0)
d906 1
a906 1
	if (addr + (ssize_t)size < addr)
d961 1
a961 1
	if (addr + (ssize_t)size < addr)
d1067 1
a1067 1
		uvm_unmap(map, *addr, *addr + size);	/* zap! */
@


1.31
log
@Yet another sync to NetBSD uvm.
Today we add a pmap argument to pmap_update() and allocate map entries for
kernel_map from kmem_map instead of using the static entries. This should
get rid of MAX_KMAPENT panics. Also some uvm_loan problems are fixed.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.30 2001/11/28 19:28:15 art Exp $	*/
a71 1
#include <uvm/uvm_vnode.h>
@


1.30
log
@Sync in more uvm from NetBSD. Mostly just cosmetic stuff.
Contains also support for page coloring.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_mmap.c,v 1.29 2001/11/28 13:47:40 art Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.54 2001/06/14 20:32:49 thorpej Exp $	*/
d1066 5
@


1.29
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_mmap.c,v 1.28 2001/11/27 05:27:12 art Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.50 2001/03/15 06:10:57 chs Exp $	*/
d6 1
a6 1
 * Copyright (c) 1991, 1993 The Regents of the University of California.  
d8 1
a8 1
 * 
d26 1
a26 1
 *	Washington University, University of California, Berkeley and 
d135 1
a135 1
	vm_page_t m;
d140 1
a140 1
	vm_map_entry_t entry;
d142 1
a142 1
	vm_map_t map;
d331 1
a331 1
	 * now check (MAP_FIXED) or get (!MAP_FIXED) the "addr" 
d405 1
a405 1
		/* 
d511 1
a511 1
	vm_map_t map;
d561 2
a562 2
		vm_map_entry_t entry;
		
d606 1
a606 1
	vm_map_t map;
d616 1
a616 1
	
d644 1
a644 1
	 * interesting system call semantic: make sure entire range is 
d723 1
a723 1
	
d763 1
a763 1
	
d881 1
a881 1
	
d1008 1
a1008 1
	vm_map_t map;
d1094 1
a1094 1
	uvmflag = UVM_MAPFLAG(prot, maxprot, 
@


1.28
log
@Merge in the unified buffer cache code as found in NetBSD 2001/03/10. The
code is written mostly by Chuck Silvers <chuq@@chuq.com>/<chs@@netbsd.org>.

Tested for the past few weeks by many developers, should be in a pretty stable
state, but will require optimizations and additional cleanups.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_mmap.c,v 1.27 2001/11/12 01:26:09 art Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.49 2001/02/18 21:19:08 chs Exp $	*/
d168 1
a169 1

d197 1
d225 1
d230 1
a233 1

d239 1
d244 1
a247 1

a249 1

d296 1
a296 1
	caddr_t handle;
d303 2
a304 2
	addr = (vaddr_t) SCARG(uap, addr);
	size = (vsize_t) SCARG(uap, len);
d326 1
a326 1
	size = (vsize_t) round_page(size);	/* round up */
d356 2
a357 4
		if (addr < round_page((vaddr_t)p->p_vmspace->vm_daddr +
		    MAXDSIZ))
			addr = round_page((vaddr_t)p->p_vmspace->vm_daddr +
			    MAXDSIZ);
d449 1
a449 6

		/*
		 * set handle to vnode
		 */

		handle = (caddr_t)vp;
d474 2
a475 1
		    (p->p_rlimit[RLIMIT_DATA].rlim_cur - ctob(p->p_vmspace->vm_dsize))) {
d512 1
a512 1
	int rv, flags, uvmflags;
d531 1
a531 1
	 * align the address to a page boundary, and adjust the size accordingly
d537 1
a537 1
	size = (vsize_t) round_page(size);
d559 1
d577 1
d586 2
a587 21
	/*
	 * doit!
	 */
	rv = uvm_map_clean(map, addr, addr+size, uvmflags);

	/*
	 * and return... 
	 */
	switch (rv) {
	case KERN_SUCCESS:
		return(0);
	case KERN_INVALID_ADDRESS:
		return (ENOMEM);
	case KERN_FAILURE:
		return (EIO);
	case KERN_PAGES_LOCKED:	/* XXXCDC: uvm doesn't return this */
		return (EBUSY);
	default:
		return (EINVAL);
	}
	/*NOTREACHED*/
d611 1
a611 1
	 * get syscall args...
d614 2
a615 2
	addr = (vaddr_t) SCARG(uap, addr);
	size = (vsize_t) SCARG(uap, len);
d618 1
a618 1
	 * align the address to a page boundary, and adjust the size accordingly
d624 1
a624 1
	size = (vsize_t) round_page(size);
a642 3

	vm_map_lock(map);	/* lock map so we can checkprot */

d648 1
d653 2
a654 8

	/*
	 * doit!
	 */
	(void) uvm_unmap_remove(map, addr, addr + size, &dead_entries);

	vm_map_unlock(map);	/* and unlock */

a656 1

d678 1
a678 1
	int rv;
d689 1
a689 1
	 * align the address to a page boundary, and adjust the size accordingly
d691 1
d695 2
a696 1
	size = (vsize_t) round_page(size);
d699 3
a701 13

	/*
	 * doit
	 */

	rv = uvm_map_protect(&p->p_vmspace->vm_map, 
			   addr, addr+size, prot, FALSE);

	if (rv == KERN_SUCCESS)
		return (0);
	if (rv == KERN_PROTECTION_FAILURE)
		return (EACCES);
	return (EINVAL);
d722 1
d727 1
d729 1
a729 1
	 * align the address to a page boundary, and adjust the size accordingly
d735 1
a735 1
	size = (vsize_t) round_page(size);
d739 3
a741 9
	
	switch (uvm_map_inherit(&p->p_vmspace->vm_map, addr, addr+size,
			 inherit)) {
	case KERN_SUCCESS:
		return (0);
	case KERN_PROTECTION_FAILURE:
		return (EACCES);
	}
	return (EINVAL);
d762 1
a762 1
	int advice, rv;;
d771 1
d775 1
a775 1
	size = (vsize_t) round_page(size);
d784 1
a784 1
		rv = uvm_map_advice(&p->p_vmspace->vm_map, addr, addr + size,
d789 1
d799 1
d803 1
d809 2
a810 1
		rv = uvm_map_clean(&p->p_vmspace->vm_map, addr, addr + size,
d815 1
d821 2
a822 1
		rv = uvm_map_clean(&p->p_vmspace->vm_map, addr, addr + size,
d827 1
d838 1
d845 1
a845 12
	switch (rv) {
	case KERN_SUCCESS:
		return (0);
	case KERN_NO_SPACE:
		return (EAGAIN);
	case KERN_INVALID_ADDRESS:
		return (ENOMEM);
	case KERN_FAILURE:
		return (EIO);
	}

	return (EINVAL);
d869 1
d876 1
d880 1
a880 1
	size = (vsize_t) round_page(size);
d883 1
a883 1
	if (addr + (int)size < addr)
d900 1
a900 1
	return (error == KERN_SUCCESS ? 0 : ENOMEM);
d931 1
d935 1
a935 1
	size = (vsize_t) round_page(size);
d938 1
a938 1
	if (addr + (int)size < addr)
d948 1
a948 1
	return (error == KERN_SUCCESS ? 0 : ENOMEM);
a978 17
	switch (error) {
	case KERN_SUCCESS:
		error = 0;
		break;

	case KERN_NO_SPACE:	/* XXX overloaded */
		error = ENOMEM;
		break;

	default:
		/*
		 * "Some or all of the memory could not be locked when
		 * the call was made."
		 */
		error = EAGAIN;
	}

d1013 1
a1013 1
	caddr_t handle;		/* XXX: VNODE? */
d1019 1
a1019 1
	int retval;
a1041 1
		
d1064 1
a1064 2

		vp = (struct vnode *) handle;	/* get vnode */
d1066 1
a1066 1
			uobj = uvn_attach((void *) vp, (flags & MAP_SHARED) ?
d1082 1
a1082 1
				uobj = udv_attach((void *) &vp->v_rdev,
a1087 1
		
a1089 1

a1093 4
	/*
	 * set up mapping flags
	 */

d1097 6
d1105 2
a1106 1
	 * do it!
d1109 1
a1109 1
	retval = uvm_map(map, addr, size, uobj, foff, 0, uvmflag);
a1110 1
	if (retval == KERN_SUCCESS) {
d1112 1
a1112 2
		 * POSIX 1003.1b -- if our address space was configured
		 * to lock all future mappings, wire the one we just made.
a1113 8
		if (prot == VM_PROT_NONE) {
			/*
			 * No more work to do in this case.
			 */
			return (0);
		}
		
		vm_map_lock(map);
d1115 5
a1119 2
		if (map->flags & VM_MAP_WIREFUTURE) {
			if ((atop(size) + uvmexp.wired) > uvmexp.wiredmax
d1121 3
a1123 3
			    || (locklimit != 0 && (size +
			         ptoa(pmap_wired_count(vm_map_pmap(map)))) >
			        locklimit)
d1125 4
a1128 19
			) {
				retval = KERN_RESOURCE_SHORTAGE;
				vm_map_unlock(map);
				/* unmap the region! */
				(void) uvm_unmap(map, *addr, *addr + size);
				goto bad;
			}
			/*
			 * uvm_map_pageable() always returns the map
			 * unlocked.
			 */
			retval = uvm_map_pageable(map, *addr, *addr + size,
			    FALSE, UVM_LK_ENTER);
			if (retval != KERN_SUCCESS) {
				/* unmap the region! */
				(void) uvm_unmap(map, *addr, *addr + size);
				goto bad;
			}
			return (0);
d1131 3
a1133 1
		vm_map_unlock(map);
d1135 6
d1143 2
a1144 19

	/*
	 * errors: first detach from the uobj, if any.
	 */
	
	if (uobj)
		uobj->pgops->pgo_detach(uobj);

 bad:
	switch (retval) {
	case KERN_INVALID_ADDRESS:
	case KERN_NO_SPACE:
		return(ENOMEM);
	case KERN_RESOURCE_SHORTAGE:
		return (EAGAIN);
	case KERN_PROTECTION_FAILURE:
		return(EACCES);
	}
	return(EINVAL);
@


1.27
log
@Bring in more changes from NetBSD. Mostly pagedaemon improvements.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.26 2001/11/09 03:32:23 art Exp $	*/
a1128 31
#ifndef UBC
			/*
			 * XXXCDC: hack from old code
			 * don't allow vnodes which have been mapped
			 * shared-writeable to persist [forces them to be
			 * flushed out when last reference goes].
			 * XXXCDC: interesting side effect: avoids a bug.
			 * note that in WRITE [ufs_readwrite.c] that we
			 * allocate buffer, uncache, and then do the write.
			 * the problem with this is that if the uncache causes
			 * VM data to be flushed to the same area of the file
			 * we are writing to... in that case we've got the
			 * buffer locked and our process goes to sleep forever.
			 *
			 * XXXCDC: checking maxprot protects us from the
			 * "persistbug" program but this is not a long term
			 * solution.
			 * 
			 * XXXCDC: we don't bother calling uncache with the vp
			 * VOP_LOCKed since we know that we are already
			 * holding a valid reference to the uvn (from the
			 * uvn_attach above), and thus it is impossible for
			 * the uncache to kill the uvn and trigger I/O.
			 */
			if (flags & MAP_SHARED) {
				if ((prot & VM_PROT_WRITE) ||
				    (maxprot & VM_PROT_WRITE)) {
					uvm_vnp_uncache(vp);
				}
			}
#else
a1130 1
#endif
@


1.26
log
@minor sync to NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_mmap.c,v 1.25 2001/11/07 02:55:50 art Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.45 2000/11/24 23:30:01 soren Exp $	*/
d180 3
a182 6
#ifdef DIAGNOSTIC
		if (UVM_ET_ISSUBMAP(entry))
			panic("mincore: user map has submap");
		if (start < entry->start)
			panic("mincore: hole");
#endif
d198 1
a198 4
#ifdef DIAGNOSTIC
			if (UVM_OBJ_IS_KERN_OBJECT(entry->object.uvm_obj))
				panic("mincore: user map has kernel object");
#endif
a410 5
#if defined(DIAGNOSTIC)
			printf("WARNING: converted MAP_PRIVATE device mapping "
			    "to MAP_SHARED (pid %d comm %s)\n", p->p_pid,
			    p->p_comm);
#endif
d1129 1
d1159 4
a1162 1

d1165 13
a1177 2
			    (flags & MAP_SHARED) ?
			    maxprot : (maxprot & ~VM_PROT_WRITE), foff, size);
@


1.25
log
@Another sync of uvm to NetBSD. Just minor fiddling, no major changes.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.24 2001/11/07 01:18:01 art Exp $	*/
a265 20
#if 0
/*
 * munmapfd: unmap file descriptor
 *
 * XXX: is this acutally a useful function?   could it be useful?
 */

void
munmapfd(p, fd)
	struct proc *p;
	int fd;
{

	/*
	 * XXX should vm_deallocate any regions mapped to this file
	 */
	p->p_fd->fd_ofileflags[fd] &= ~UF_MAPPED;
}
#endif

d359 3
a361 1
		if (addr < round_page((vaddr_t)p->p_vmspace->vm_daddr+MAXDSIZ))
@


1.24
log
@Add an alignment argument to uvm_map that specifies an alignment hint
for the virtual address.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_mmap.c,v 1.23 2001/11/06 01:35:04 art Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.44 2000/09/13 15:00:25 thorpej Exp $	*/
d883 1
a883 1
		 * grbage-collected.  Toss all resources, including
@


1.23
log
@Move the last content from vm/ to uvm/
The only thing left in vm/ are just dumb wrappers.
vm/vm.h includes uvm/uvm_extern.h
vm/pmap.h includes uvm/uvm_pmap.h
vm/vm_page.h includes uvm/uvm_page.h
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_mmap.c,v 1.22 2001/11/05 22:14:54 art Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.43 2000/06/27 17:29:28 mrg Exp $	*/
d1214 1
a1214 1
	retval = uvm_map(map, addr, size, uobj, foff, uvmflag);
@


1.22
log
@Minor sync to NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_mmap.c,v 1.21 2001/10/26 12:03:28 art Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.42 2000/06/26 14:21:18 mrg Exp $	*/
a66 2

#include <vm/vm.h>
@


1.21
log
@ - every new fd created by falloc() is marked as larval and should not be used
   any anyone. Every caller of falloc matures the fd when it's usable.
 - Since every lookup in the fd table must now check this flag and all of
   them do the same thing, move all the necessary checks into a function -
   fd_getfile.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_mmap.c,v 1.20 2001/09/11 20:05:26 miod Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.41 2000/05/23 02:19:20 enami Exp $	*/
a68 1
#include <vm/vm_page.h>
@


1.20
log
@Don't include <vm/vm_kern.h> if you don't need foo_map.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.19 2001/08/11 10:57:22 art Exp $	*/
d393 1
a393 4
		if (fd < 0 || fd >= fdp->fd_nfiles)
			return(EBADF);		/* failed range check? */
		fp = fdp->fd_ofiles[fd];	/* convert to file pointer */
		if (fp == NULL)
@


1.19
log
@Various random fixes from NetBSD.
Including support for zeroing pages in the idle loop (not enabled yet).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.18 2001/08/06 14:03:05 art Exp $	*/
a69 1
#include <vm/vm_kern.h>
@


1.18
log
@Add a new type voff_t (right now it's typedefed as off_t) used for offsets
into objects.

Gives the possibilty to mmap beyond the size of vaddr_t.

From NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_mmap.c,v 1.17 2001/07/25 14:47:59 art Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.38 2000/03/26 20:54:47 kleink Exp $	*/
d305 1
a305 1
	register struct sys_mmap_args /* {
d321 2
a322 2
	register struct filedesc *fdp = p->p_fd;
	register struct file *fp;
d408 3
d653 1
a653 1
	register struct proc *p;
d657 1
a657 1
	register struct sys_munmap_args /* {
d795 1
a795 1
	register vm_inherit_t inherit;
a1237 4
			/*
			 * uvm_map_pageable() always returns the map
			 * unlocked.
			 */
d1246 1
d1251 4
@


1.17
log
@Some updates to UVM from NetBSD. Nothing really critical, just a sync.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_mmap.c,v 1.16 2001/07/25 13:25:33 art Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.37 1999/12/11 05:38:41 thorpej Exp $	*/
a347 12
	 * make sure that the newsize fits within a vaddr_t
	 * XXX: need to revise addressing data types
	 */
	if (pos + size > (vaddr_t)-PAGE_SIZE) {
#ifdef DEBUG
		printf("mmap: pos=%qx, size=%lx too big\n", (long long)pos,
		       (long)size);
#endif
		return (EINVAL);
	}

	/*
d383 3
a385 2
		if (addr < round_page((vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ))
			addr = round_page((vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ);
d1105 1
a1105 1
	vaddr_t foff;
@


1.16
log
@Change the pmap_enter interface to merge access_type and the wired boolean
and arbitrary flags into one argument.

One new flag is PMAP_CANFAIL that tells pmap_enter that it can fail if there
are not enough resources to satisfy the request. If this flag is not passed,
pmap_enter should panic as it should have done before this change (XXX - many
pmaps are still not doing that).

Only i386 and alpha implement CANFAIL for now.

Includes uvm updates from NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_mmap.c,v 1.15 2001/06/23 19:24:34 smart Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.36 1999/11/13 00:24:38 thorpej Exp $	*/
a1156 11
#if 1
		/*
		 * Specify an offset of 0 so that uvm_map_findspace() via.
		 * uvm_map() will PMAP_PREFER the address for us.  This
		 * prevents alias problems if the following occurs:
		 *
		 *	- Anon region mapped.
		 *	- File mapped over anon region (using MAP_FIXED).
		 */
		foff = 0;
#else
a1157 1
#endif
@


1.15
log
@Sync with NetBSD 19990911 (just before PMAP_NEW was required)
  - thread_sleep_msg() -> uvm_sleep()
  - initialize reference count lock in uvm_anon_{init,add}()
  - add uao_flush()
  - replace boolean 'islocked' with 'lockflags'
  - in uvm_fault() change FALSE to TRUE to in 'wide' fault handling
  - get rid of uvm_km_get()
  - various bug fixes
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_mmap.c,v 1.14 2001/06/08 08:09:39 art Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.35 1999/07/17 21:35:50 thorpej Exp $	*/
d1157 11
d1169 1
@


1.14
log
@Change the paddr_t pmap_extract(struct pmap *, vaddr_t) interface to
boolean_t pmap_extract(struct pmap *, vaddr_t, paddr_t *).
Matches NetBSD. Tested by various people on various platforms.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_mmap.c,v 1.13 2001/05/10 14:51:21 art Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.30 1999/07/08 00:52:45 thorpej Exp $	*/
d96 1
a96 1
		syscallarg(int) incr;
d201 2
a202 2
		 * Special case for mapped devices; these are always
		 * considered resident.
a204 1
			extern struct uvm_pagerops uvm_deviceops; /* XXX */
d209 2
a210 1
			if (entry->object.uvm_obj->pgops == &uvm_deviceops) {
d218 2
a219 2
		uobj = entry->object.uvm_obj;	/* top layer */
		amap = entry->aref.ar_amap;	/* bottom layer */
d624 3
a626 1
	uvmflags = PGO_CLEANIT | (flags & MS_INVALIDATE) ? PGO_FREE : 0;
d985 1
a985 1
	    FALSE);
d1032 1
a1032 1
	    FALSE);
d1263 1
a1263 1
			    FALSE, TRUE);
@


1.13
log
@More sync to NetBSD.
The highlight is some more advices to madvise(2).
 o MADV_DONTNEED will deactive the pages in the given range giving a quicker
   reuse.
 o MADV_FREE will garbage-collect the pages and swap resources causing the
   next fault to either page in new pages from backing store (mapped vnode)
   or allocate new zero-fill pages (anonymous mapping).
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_mmap.c,v 1.12 2001/05/10 07:59:06 art Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.29 1999/07/07 06:02:22 thorpej Exp $	*/
d897 1
a897 2
		 * backing store; note that if the page is not backed
		 * by swap, it will be cleaned first, for good measure.
@


1.12
log
@Some locking protocol fixes and better enforcement of wiring limits.

From NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_mmap.c,v 1.11 2001/05/05 21:26:46 art Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.28 1999/07/06 02:31:05 cgd Exp $	*/
d624 1
a624 1
	uvmflags = (flags & MS_INVALIDATE) ? PGO_FREE : 0;
d846 1
a846 1
	int advice;
d860 58
a917 1
	if ((int)size < 0)
d919 3
a921 3
	
	switch (uvm_map_advice(&p->p_vmspace->vm_map, addr, addr+size,
			 advice)) {
d924 6
a929 2
	case KERN_PROTECTION_FAILURE:
		return (EACCES);
@


1.11
log
@Remove the (vaddr_t) casts inside the round_page and trunc_page macros.
We might want to use them on types that are bigger than vaddr_t.

Fix all callers that pass pointers without casts.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_mmap.c,v 1.10 2001/03/22 03:05:56 smart Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.23 1999/06/16 17:25:39 minoura Exp $	*/
d339 9
d431 1
a431 1
		if ((flags & (MAP_SHARED|MAP_PRIVATE|MAP_COPY)) == 0) {
d501 3
a503 1

d507 1
a507 1
is_anon:		/* label for SunOS style /dev/zero */
d514 15
d533 1
a533 1
	    flags, handle, pos);
d922 2
a923 1
	error = uvm_map_pageable(&p->p_vmspace->vm_map, addr, addr+size, FALSE);
d969 2
a970 1
	error = uvm_map_pageable(&p->p_vmspace->vm_map, addr, addr+size, TRUE);
a986 1
	vsize_t limit;
d995 1
a995 5
#ifdef pmap_wired_count
	/* Actually checked in uvm_map_pageable_all() */
	limit = p->p_rlimit[RLIMIT_MEMLOCK].rlim_cur;
#else
	limit = 0;
d1000 2
a1001 1
	error = uvm_map_pageable_all(&p->p_vmspace->vm_map, flags, limit);
d1047 1
a1047 1
uvm_mmap(map, addr, size, prot, maxprot, flags, handle, foff)
d1055 1
d1169 45
a1213 2
	if (retval == KERN_SUCCESS)
		return(0);
d1222 1
d1227 2
@


1.10
log
@Sync style, typo, and comments a little closer to NetBSD.  art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.9 2001/03/09 17:49:34 art Exp $	*/
d386 2
a387 2
		if (addr < round_page(p->p_vmspace->vm_daddr + MAXDSIZ))
			addr = round_page(p->p_vmspace->vm_daddr + MAXDSIZ);
@


1.9
log
@locking typo.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.8 2001/03/09 15:11:46 art Exp $	*/
d844 1
d1070 1
a1070 2
		
		foff = UVM_UNKNOWN_OFFSET;		
@


1.8
log
@Add mlockall and munlockall (dummy for the old vm system).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.7 2001/03/09 14:20:52 art Exp $	*/
d259 1
a259 1
			simple_unlock(&obj->vmobjlock);
@


1.7
log
@More syncing to NetBSD.

Implements mincore(2), mlockall(2) and munlockall(2). mlockall and munlockall
are disabled for the moment.

The rest is mostly cosmetic.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.6 2001/01/29 02:07:46 niklas Exp $	*/
a944 1
#ifdef notyet
a1010 1
#endif
@


1.6
log
@$OpenBSD$
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_mmap.c,v 1.21 1999/05/23 06:27:13 mrg Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.21 1999/05/23 06:27:13 mrg Exp $	*/
a133 1
#if 0
d135 1
a135 1
		syscallarg(caddr_t) addr;
d139 50
d190 7
d198 70
a267 1
	return (ENOSYS);
d944 69
@


1.5
log
@Bring in some new UVM code from NetBSD (not current).

 - Introduce a new type of map that are interrupt safe and never allow faults
   in them. mb_map and kmem_map are made intrsafe.
 - Add "access protection" to uvm_vslock (to be passed down to uvm_fault and
   later to pmap_enter).
 - madvise(2) now works.
 - various cleanups.
@
text
@d1 1
@


1.4
log
@sync with NetBSD from 1999.05.24 (there is a reason for this date)
 Mostly cleanups, but also a few improvements to pagedaemon for better
 handling of low memory and/or low swap conditions.
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_mmap.c,v 1.19 1999/03/25 18:48:53 mrg Exp $	*/
a122 22
 * sys_madvise: give advice about memory usage.
 */

/* ARGSUSED */
int
sys_madvise(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{
#if 0
	struct sys_madvise_args /* {
		syscallarg(caddr_t) addr;
		syscallarg(size_t) len;
		syscallarg(int) behav;
	} */ *uap = v;
#endif

	return (ENOSYS);
}

/*
d668 45
@


1.4.4.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_mmap.c,v 1.21 1999/05/23 06:27:13 mrg Exp $	*/
d123 22
a689 45
	case KERN_SUCCESS:
		return (0);
	case KERN_PROTECTION_FAILURE:
		return (EACCES);
	}
	return (EINVAL);
}

/*
 * sys_madvise: give advice about memory usage.
 */

/* ARGSUSED */
int
sys_madvise(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{
	struct sys_madvise_args /* {
		syscallarg(caddr_t) addr;
		syscallarg(size_t) len;
		syscallarg(int) behav;
	} */ *uap = v;
	vaddr_t addr;
	vsize_t size, pageoff;
	int advice;
	
	addr = (vaddr_t)SCARG(uap, addr);
	size = (vsize_t)SCARG(uap, len);
	advice = SCARG(uap, behav);

	/*
	 * align the address to a page boundary, and adjust the size accordingly
	 */
	pageoff = (addr & PAGE_MASK);
	addr -= pageoff;
	size += pageoff;
	size = (vsize_t) round_page(size);

	if ((int)size < 0)
		return (EINVAL);
	
	switch (uvm_map_advice(&p->p_vmspace->vm_map, addr, addr+size,
			 advice)) {
@


1.4.4.2
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_mmap.c,v 1.10 2001/03/22 03:05:56 smart Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.23 1999/06/16 17:25:39 minoura Exp $	*/
d133 1
d135 1
a135 1
		syscallarg(void *) addr;
a138 50
	vm_page_t m;
	char *vec, pgi;
	struct uvm_object *uobj;
	struct vm_amap *amap;
	struct vm_anon *anon;
	vm_map_entry_t entry;
	vaddr_t start, end, lim;
	vm_map_t map;
	vsize_t len;
	int error = 0, npgs;

	map = &p->p_vmspace->vm_map;

	start = (vaddr_t)SCARG(uap, addr);
	len = SCARG(uap, len);
	vec = SCARG(uap, vec);

	if (start & PAGE_MASK)
		return (EINVAL);
	len = round_page(len);
	end = start + len;
	if (end <= start)
		return (EINVAL);

	npgs = len >> PAGE_SHIFT;

	if (uvm_useracc(vec, npgs, B_WRITE) == FALSE)
		return (EFAULT);

	/*
	 * Lock down vec, so our returned status isn't outdated by
	 * storing the status byte for a page.
	 */
	uvm_vslock(p, vec, npgs, VM_PROT_WRITE);

	vm_map_lock_read(map);

	if (uvm_map_lookup_entry(map, start, &entry) == FALSE) {
		error = ENOMEM;
		goto out;
	}

	for (/* nothing */;
	     entry != &map->header && entry->start < end;
	     entry = entry->next) {
#ifdef DIAGNOSTIC
		if (UVM_ET_ISSUBMAP(entry))
			panic("mincore: user map has submap");
		if (start < entry->start)
			panic("mincore: hole");
a139 7
		/* Make sure there are no holes. */
		if (entry->end < end &&
		     (entry->next == &map->header ||
		      entry->next->start > entry->end)) {
			error = ENOMEM;
			goto out;
		}
d141 1
a141 70
		lim = end < entry->end ? end : entry->end;

		/*
		 * Special case for mapped devices; these are always
		 * considered resident.
		 */
		if (UVM_ET_ISOBJ(entry)) {
			extern struct uvm_pagerops uvm_deviceops; /* XXX */
#ifdef DIAGNOSTIC
			if (UVM_OBJ_IS_KERN_OBJECT(entry->object.uvm_obj))
				panic("mincore: user map has kernel object");
#endif
			if (entry->object.uvm_obj->pgops == &uvm_deviceops) {
				for (/* nothing */; start < lim;
				     start += PAGE_SIZE, vec++)
					subyte(vec, 1);
				continue;
			}
		}

		uobj = entry->object.uvm_obj;	/* top layer */
		amap = entry->aref.ar_amap;	/* bottom layer */

		if (amap != NULL)
			amap_lock(amap);
		if (uobj != NULL)
			simple_lock(&uobj->vmobjlock);

		for (/* nothing */; start < lim; start += PAGE_SIZE, vec++) {
			pgi = 0;
			if (amap != NULL) {
				/* Check the top layer first. */
				anon = amap_lookup(&entry->aref,
				    start - entry->start);
				/* Don't need to lock anon here. */
				if (anon != NULL && anon->u.an_page != NULL) {
					/*
					 * Anon has the page for this entry
					 * offset.
					 */
					pgi = 1;
				}
			}

			if (uobj != NULL && pgi == 0) {
				/* Check the bottom layer. */
				m = uvm_pagelookup(uobj,
				    entry->offset + (start - entry->start));
				if (m != NULL) {
					/*
					 * Object has the page for this entry
					 * offset.
					 */
					pgi = 1;
				}
			}

			(void) subyte(vec, pgi);
		}

		if (uobj != NULL)
			simple_unlock(&uobj->vmobjlock);
		if (amap != NULL)
			amap_unlock(amap);
	}

 out:
	vm_map_unlock_read(map);
	uvm_vsunlock(p, SCARG(uap, vec), npgs);
	return (error);
a717 1

a819 67
 * sys_mlockall: lock all pages mapped into an address space.
 */

int
sys_mlockall(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{
	struct sys_mlockall_args /* {
		syscallarg(int) flags;
	} */ *uap = v;
	vsize_t limit;
	int error, flags;

	flags = SCARG(uap, flags);

	if (flags == 0 ||
	    (flags & ~(MCL_CURRENT|MCL_FUTURE)) != 0)
		return (EINVAL);

#ifdef pmap_wired_count
	/* Actually checked in uvm_map_pageable_all() */
	limit = p->p_rlimit[RLIMIT_MEMLOCK].rlim_cur;
#else
	limit = 0;
	if ((error = suser(p->p_ucred, &p->p_acflag)) != 0)
		return (error);
#endif

	error = uvm_map_pageable_all(&p->p_vmspace->vm_map, flags, limit);
	switch (error) {
	case KERN_SUCCESS:
		error = 0;
		break;

	case KERN_NO_SPACE:	/* XXX overloaded */
		error = ENOMEM;
		break;

	default:
		/*
		 * "Some or all of the memory could not be locked when
		 * the call was made."
		 */
		error = EAGAIN;
	}

	return (error);
}

/*
 * sys_munlockall: unlock all pages mapped into an address space.
 */

int
sys_munlockall(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{

	(void) uvm_map_pageable_all(&p->p_vmspace->vm_map, 0, 0);
	return (0);
}

/*
d876 2
a877 1
		foff = UVM_UNKNOWN_OFFSET;
@


1.4.4.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_mmap.c,v 1.35 1999/07/17 21:35:50 thorpej Exp $	*/
d96 1
a96 1
		syscallarg(intptr_t) incr;
d201 2
a202 2
		 * Special case for objects with no "real" pages.  Those
		 * are always considered resident (mapped devices).
d205 1
d210 1
a210 2
			if (entry->object.uvm_obj->pgops->pgo_releasepg
			    == NULL) {
d218 2
a219 2
		amap = entry->aref.ar_amap;	/* top layer */
		uobj = entry->object.uvm_obj;	/* bottom layer */
a338 9
	 * Fixup the old deprecated MAP_COPY into MAP_PRIVATE, and
	 * validate the flags.
	 */
	if (flags & MAP_COPY)
		flags = (flags & ~MAP_COPY) | MAP_PRIVATE;
	if ((flags & (MAP_SHARED|MAP_PRIVATE)) == (MAP_SHARED|MAP_PRIVATE))
		return (EINVAL);

	/*
d386 2
a387 2
		if (addr < round_page((vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ))
			addr = round_page((vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ);
d422 1
a422 1
		if ((flags & (MAP_SHARED|MAP_PRIVATE)) == 0) {
d492 1
a492 3
		/*
		 * XXX What do we do about (MAP_SHARED|MAP_PRIVATE) == 0?
		 */
d496 1
a496 1
 is_anon:		/* label for SunOS style /dev/zero */
a502 15
	 * XXX (in)sanity check.  We don't do proper datasize checking
	 * XXX for anonymous (or private writable) mmap().  However,
	 * XXX know that if we're trying to allocate more than the amount
	 * XXX remaining under our current data size limit, _that_ should
	 * XXX be disallowed.
	 */
	if ((flags & MAP_ANON) != 0 ||
	    ((flags & MAP_PRIVATE) != 0 && (prot & PROT_WRITE) != 0)) {
		if (size >
		    (p->p_rlimit[RLIMIT_DATA].rlim_cur - ctob(p->p_vmspace->vm_dsize))) {
			return (ENOMEM);
		}
	}

	/*
d507 1
a507 1
	    flags, handle, pos, p->p_rlimit[RLIMIT_MEMLOCK].rlim_cur);
d598 1
a598 3
	uvmflags = PGO_CLEANIT;
	if (flags & MS_INVALIDATE)
		uvmflags |= PGO_FREE;
d820 1
a820 1
	int advice, rv;;
d834 1
a834 1
	if ((ssize_t)size <= 0)
d836 3
a838 59

	switch (advice) {
	case MADV_NORMAL:
	case MADV_RANDOM:
	case MADV_SEQUENTIAL:
		rv = uvm_map_advice(&p->p_vmspace->vm_map, addr, addr + size,
		    advice);
		break;

	case MADV_WILLNEED:
		/*
		 * Activate all these pages, pre-faulting them in if
		 * necessary.
		 */
		/*
		 * XXX IMPLEMENT ME.
		 * Should invent a "weak" mode for uvm_fault()
		 * which would only do the PGO_LOCKED pgo_get().
		 */
		return (0);

	case MADV_DONTNEED:
		/*
		 * Deactivate all these pages.  We don't need them
		 * any more.  We don't, however, toss the data in
		 * the pages.
		 */
		rv = uvm_map_clean(&p->p_vmspace->vm_map, addr, addr + size,
		    PGO_DEACTIVATE);
		break;

	case MADV_FREE:
		/*
		 * These pages contain no valid data, and may be
		 * grbage-collected.  Toss all resources, including
		 * any swap space in use.
		 */
		rv = uvm_map_clean(&p->p_vmspace->vm_map, addr, addr + size,
		    PGO_FREE);
		break;

	case MADV_SPACEAVAIL:
		/*
		 * XXXMRG What is this?  I think it's:
		 *
		 *	Ensure that we have allocated backing-store
		 *	for these pages.
		 *
		 * This is going to require changes to the page daemon,
		 * as it will free swap space allocated to pages in core.
		 * There's also what to do for device/file/anonymous memory.
		 */
		return (EINVAL);

	default:
		return (EINVAL);
	}

	switch (rv) {
d841 2
a842 6
	case KERN_NO_SPACE:
		return (EAGAIN);
	case KERN_INVALID_ADDRESS:
		return (ENOMEM);
	case KERN_FAILURE:
		return (EIO);
d896 1
a896 2
	error = uvm_map_pageable(&p->p_vmspace->vm_map, addr, addr+size, FALSE,
	    0);
d942 1
a942 2
	error = uvm_map_pageable(&p->p_vmspace->vm_map, addr, addr+size, TRUE,
	    0);
d959 1
d968 5
a972 1
#ifndef pmap_wired_count
d977 1
a977 2
	error = uvm_map_pageable_all(&p->p_vmspace->vm_map, flags,
	    p->p_rlimit[RLIMIT_MEMLOCK].rlim_cur);
d1023 1
a1023 1
uvm_mmap(map, addr, size, prot, maxprot, flags, handle, foff, locklimit)
a1030 1
	vsize_t locklimit;
d1144 2
a1145 45
	if (retval == KERN_SUCCESS) {
		/*
		 * POSIX 1003.1b -- if our address space was configured
		 * to lock all future mappings, wire the one we just made.
		 */
		if (prot == VM_PROT_NONE) {
			/*
			 * No more work to do in this case.
			 */
			return (0);
		}
		
		vm_map_lock(map);

		if (map->flags & VM_MAP_WIREFUTURE) {
			/*
			 * uvm_map_pageable() always returns the map
			 * unlocked.
			 */
			if ((atop(size) + uvmexp.wired) > uvmexp.wiredmax
#ifdef pmap_wired_count
			    || (locklimit != 0 && (size +
			         ptoa(pmap_wired_count(vm_map_pmap(map)))) >
			        locklimit)
#endif
			) {
				retval = KERN_RESOURCE_SHORTAGE;
				/* unmap the region! */
				(void) uvm_unmap(map, *addr, *addr + size);
				goto bad;
			}
			retval = uvm_map_pageable(map, *addr, *addr + size,
			    FALSE, UVM_LK_ENTER);
			if (retval != KERN_SUCCESS) {
				/* unmap the region! */
				(void) uvm_unmap(map, *addr, *addr + size);
				goto bad;
			}
			return (0);
		}

		vm_map_unlock(map);

		return (0);
	}
a1153 1
 bad:
a1157 2
	case KERN_RESOURCE_SHORTAGE:
		return (EAGAIN);
@


1.4.4.4
log
@Sync the SMP branch to something just after 3.0
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_mmap.c,v 1.41 2000/05/23 02:19:20 enami Exp $	*/
d70 1
d305 1
a305 1
	struct sys_mmap_args /* {
d321 2
a322 2
	struct filedesc *fdp = p->p_fd;
	struct file *fp;
d348 12
d395 2
a396 3
		if (addr < round_page((vaddr_t)p->p_vmspace->vm_daddr+MAXDSIZ))
			addr = round_page((vaddr_t)p->p_vmspace->vm_daddr +
			    MAXDSIZ);
d405 4
a408 1
		if ((fp = fd_getfile(fdp, fd)) == NULL)
a418 3
		if (vp->v_type == VREG && (pos + size) < pos)
			return (EINVAL);		/* no offset wrapping */

d661 1
a661 1
	struct proc *p;
d665 1
a665 1
	struct sys_munmap_args /* {
d803 1
a803 1
	vm_inherit_t inherit;
d1116 1
a1116 1
	voff_t foff;
d1246 4
a1257 1
				vm_map_unlock(map);
a1261 4
			/*
			 * uvm_map_pageable() always returns the map
			 * unlocked.
			 */
@


1.4.4.5
log
@merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_mmap.c,v 1.49 2001/02/18 21:19:08 chs Exp $	*/
d68 3
d183 6
a188 3
		KASSERT(!UVM_ET_ISSUBMAP(entry));
		KASSERT(start >= entry->start);

d204 4
a207 1
			KASSERT(!UVM_OBJ_IS_KERN_OBJECT(entry->object.uvm_obj));
d269 20
d382 1
a382 3

		if (addr < round_page((vaddr_t)p->p_vmspace->vm_daddr +
		    MAXDSIZ))
d438 5
d886 1
a886 1
		 * garbage-collected.  Toss all resources, including
a1160 1
#ifndef UBC
d1190 1
a1190 4
#else
			/* XXX for now, attach doesn't gain a ref */
			VREF(vp);
#endif
d1193 2
a1194 13
			    (flags & MAP_SHARED) ? maxprot :
			    (maxprot & ~VM_PROT_WRITE), foff, size);
			/*
			 * XXX Some devices don't like to be mapped with
			 * XXX PROT_EXEC, but we don't really have a
			 * XXX better way of handling this, right now
			 */
			if (uobj == NULL && (prot & PROT_EXEC) == 0) {
				maxprot &= ~VM_PROT_EXECUTE;
				uobj = udv_attach((void *) &vp->v_rdev,
				    (flags & MAP_SHARED) ? maxprot :
				    (maxprot & ~VM_PROT_WRITE), foff, size);
			}
d1217 1
a1217 1
	retval = uvm_map(map, addr, size, uobj, foff, 0, uvmflag);
@


1.4.4.6
log
@Merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_mmap.c,v 1.54 2001/06/14 20:32:49 thorpej Exp $	*/
d6 1
a6 1
 * Copyright (c) 1991, 1993 The Regents of the University of California.
d8 1
a8 1
 *
d26 1
a26 1
 *	Washington University, University of California, Berkeley and
d135 1
a135 1
	struct vm_page *m;
d140 1
a140 1
	struct vm_map_entry *entry;
d142 1
a142 1
	struct vm_map *map;
d168 1
a169 1
	uvm_vslock(p, vec, npgs, VM_PROT_WRITE);
a196 1

a223 1

a227 1

d231 1
a236 1

a240 1

d244 1
d247 1
d294 1
a294 1
	void *handle;
d301 2
a302 2
	addr = (vaddr_t)SCARG(uap, addr);
	size = (vsize_t)SCARG(uap, len);
d324 1
a324 1
	size = (vsize_t)round_page(size);	/* round up */
d329 1
a329 1
	 * now check (MAP_FIXED) or get (!MAP_FIXED) the "addr"
d354 4
a357 2
		addr = MAX(addr, round_page((vaddr_t)p->p_vmspace->vm_daddr +
					    MAXDSIZ));
d405 1
a405 1
		/*
d449 6
a454 1
		handle = vp;
d479 1
a479 2
		    (p->p_rlimit[RLIMIT_DATA].rlim_cur -
		     ctob(p->p_vmspace->vm_dsize))) {
d515 2
a516 2
	struct vm_map *map;
	int error, rv, flags, uvmflags;
d535 1
a535 1
	 * align the address to a page boundary and adjust the size accordingly.
d541 1
a541 1
	size = (vsize_t)round_page(size);
a562 1

d564 2
a565 2
		struct vm_map_entry *entry;

a579 1

d588 21
a608 2
	error = uvm_map_clean(map, addr, addr+size, uvmflags);
	return error;
d627 1
a627 1
	struct vm_map *map;
d632 1
a632 1
	 * get syscall args.
d635 3
a637 3
	addr = (vaddr_t)SCARG(uap, addr);
	size = (vsize_t)SCARG(uap, len);

d639 1
a639 1
	 * align the address to a page boundary and adjust the size accordingly.
d645 1
a645 1
	size = (vsize_t)round_page(size);
d664 3
d668 1
a668 1
	 * interesting system call semantic: make sure entire range is
a671 1
	vm_map_lock(map);
d676 8
a683 2
	uvm_unmap_remove(map, addr, addr + size, &dead_entries);
	vm_map_unlock(map);
d686 1
d708 1
a708 1
	int error;
d719 1
a719 1
	 * align the address to a page boundary and adjust the size accordingly.
a720 1

d724 1
a724 2
	size = (vsize_t)round_page(size);

d727 13
a739 3
	error = uvm_map_protect(&p->p_vmspace->vm_map, addr, addr + size, prot,
				FALSE);
	return error;
d760 1
a760 2
	int error;

a763 1

d765 1
a765 1
	 * align the address to a page boundary and adjust the size accordingly.
d771 1
a771 1
	size = (vsize_t)round_page(size);
d775 9
a783 3
	error = uvm_map_inherit(&p->p_vmspace->vm_map, addr, addr + size,
				inherit);
	return error;
d804 2
a805 2
	int advice, error;

a812 1

d816 1
a816 1
	size = (vsize_t)round_page(size);
d825 1
a825 1
		error = uvm_map_advice(&p->p_vmspace->vm_map, addr, addr + size,
a829 1

a838 1

a841 1

d847 1
a847 2

		error = uvm_map_clean(&p->p_vmspace->vm_map, addr, addr + size,
a851 1

d857 1
a857 2

		error = uvm_map_clean(&p->p_vmspace->vm_map, addr, addr + size,
a861 1

a871 1

d878 12
a889 1
	return error;
a912 1

a918 1

d922 2
a923 2
	size = (vsize_t)round_page(size);

d925 1
a925 1
	if (addr + size < addr)
d942 1
a942 1
	return error;
a972 1

d976 1
a976 1
	size = (vsize_t)round_page(size);
d979 1
a979 1
	if (addr + size < addr)
d989 1
a989 1
	return error;
d1020 17
d1066 1
a1066 1
	struct vm_map *map;
d1071 1
a1071 1
	void *handle;
d1077 1
a1077 1
	int error;
d1100 1
d1123 2
a1124 1
		vp = (struct vnode *)handle;
d1126 1
a1126 1
			uobj = uvn_attach((void *)vp, (flags & MAP_SHARED) ?
d1129 31
d1162 1
d1174 1
a1174 1
				uobj = udv_attach((void *)&vp->v_rdev,
d1180 1
d1183 1
d1188 5
a1192 1
	uvmflag = UVM_MAPFLAG(prot, maxprot,
a1194 6
	error = uvm_map(map, addr, size, uobj, foff, 0, uvmflag);
	if (error) {
		if (uobj)
			uobj->pgops->pgo_detach(uobj);
		return error;
	}
d1197 1
a1197 2
	 * POSIX 1003.1b -- if our address space was configured
	 * to lock all future mappings, wire the one we just made.
d1200 1
a1200 1
	if (prot == VM_PROT_NONE) {
d1202 1
d1204 2
a1205 1
		 * No more work to do in this case.
d1207 8
d1216 2
a1217 5
		return (0);
	}
	vm_map_lock(map);
	if (map->flags & VM_MAP_WIREFUTURE) {
		if ((atop(size) + uvmexp.wired) > uvmexp.wiredmax
d1219 3
a1221 3
		    || (locklimit != 0 && (size +
		    ptoa(pmap_wired_count(vm_map_pmap(map)))) >
			locklimit)
d1223 19
a1241 4
		) {
			vm_map_unlock(map);
			uvm_unmap(map, *addr, *addr + size);
			return ENOMEM;
d1244 1
a1244 3
		/*
		 * uvm_map_pageable() always returns the map unlocked.
		 */
a1245 6
		error = uvm_map_pageable(map, *addr, *addr + size,
					 FALSE, UVM_LK_ENTER);
		if (error) {
			uvm_unmap(map, *addr, *addr + size);
			return error;
		}
d1248 19
a1266 2
	vm_map_unlock(map);
	return 0;
@


1.4.4.7
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_mmap.c,v 1.49 2001/02/18 21:19:08 chs Exp $	*/
d6 1
a6 1
 * Copyright (c) 1991, 1993 The Regents of the University of California.  
d8 1
a8 1
 * 
d26 1
a26 1
 *	Washington University, University of California, Berkeley and 
d135 1
a135 1
	vm_page_t m;
d140 1
a140 1
	vm_map_entry_t entry;
d142 1
a142 1
	vm_map_t map;
d168 1
a169 1

d197 1
d225 1
d230 1
a233 1

d239 1
d244 1
a247 1

a249 1

d294 1
a294 1
	struct file *fp = NULL;
d296 1
a296 1
	caddr_t handle;
d303 2
a304 2
	addr = (vaddr_t) SCARG(uap, addr);
	size = (vsize_t) SCARG(uap, len);
d326 1
a326 1
	size = (vsize_t) round_page(size);	/* round up */
d331 1
a331 1
	 * now check (MAP_FIXED) or get (!MAP_FIXED) the "addr" 
d356 2
a357 4
		if (addr < round_page((vaddr_t)p->p_vmspace->vm_daddr +
		    MAXDSIZ))
			addr = round_page((vaddr_t)p->p_vmspace->vm_daddr +
			    MAXDSIZ);
d363 1
a368 2
		FREF(fp);

d374 2
a375 4
		    vp->v_type != VBLK) {
			error = ENODEV; /* only REG/CHR/BLK support mmap */
			goto out;
		}
d377 2
a378 4
		if (vp->v_type == VREG && (pos + size) < pos) {
			error = EINVAL;		/* no offset wrapping */
			goto out;
		}
a382 2
			FRELE(fp);
			fp = NULL;
d405 1
a405 1
		/* 
d423 2
a424 4
		else if (prot & PROT_READ) {
			error = EACCES;
			goto out;
		}
a441 3
			} else if (prot & PROT_WRITE) {
				error = EACCES;
				goto out;
d443 2
d449 1
a449 6

		/*
		 * set handle to vnode
		 */

		handle = (caddr_t)vp;
d474 3
a476 3
		    (p->p_rlimit[RLIMIT_DATA].rlim_cur - ctob(p->p_vmspace->vm_dsize))) {
			error = ENOMEM;
			goto out;
a490 3
out:
	if (fp)
		FRELE(fp);	
d511 2
a512 2
	vm_map_t map;
	int rv, flags, uvmflags;
d531 1
a531 1
	 * align the address to a page boundary, and adjust the size accordingly
d537 1
a537 1
	size = (vsize_t) round_page(size);
d559 1
d561 2
a562 2
		vm_map_entry_t entry;
		
d577 1
d586 2
a587 9
	/*
	 * doit!
	 */
	rv = uvm_map_clean(map, addr, addr+size, uvmflags);

	/*
	 * and return... 
	 */
	return (rv);
d606 1
a606 1
	vm_map_t map;
d611 1
a611 1
	 * get syscall args...
d614 3
a616 3
	addr = (vaddr_t) SCARG(uap, addr);
	size = (vsize_t) SCARG(uap, len);
	
d618 1
a618 1
	 * align the address to a page boundary, and adjust the size accordingly
d624 1
a624 1
	size = (vsize_t) round_page(size);
a642 3

	vm_map_lock(map);	/* lock map so we can checkprot */

d644 1
a644 1
	 * interesting system call semantic: make sure entire range is 
d648 1
d653 2
a654 8

	/*
	 * doit!
	 */
	(void) uvm_unmap_remove(map, addr, addr + size, &dead_entries);

	vm_map_unlock(map);	/* and unlock */

a656 1

d678 1
a678 1
	int rv;
d689 1
a689 1
	 * align the address to a page boundary, and adjust the size accordingly
d691 1
d695 2
a696 1
	size = (vsize_t) round_page(size);
d699 3
a701 13

	/*
	 * doit
	 */

	rv = uvm_map_protect(&p->p_vmspace->vm_map, 
			   addr, addr+size, prot, FALSE);

	if (rv == KERN_SUCCESS)
		return (0);
	if (rv == KERN_PROTECTION_FAILURE)
		return (EACCES);
	return (EINVAL);
d722 2
a723 1
	
d727 1
d729 1
a729 1
	 * align the address to a page boundary, and adjust the size accordingly
d735 1
a735 1
	size = (vsize_t) round_page(size);
d739 3
a741 9
	
	switch (uvm_map_inherit(&p->p_vmspace->vm_map, addr, addr+size,
			 inherit)) {
	case KERN_SUCCESS:
		return (0);
	case KERN_PROTECTION_FAILURE:
		return (EACCES);
	}
	return (EINVAL);
d762 2
a763 2
	int advice, rv;;
	
d771 1
d775 1
a775 1
	size = (vsize_t) round_page(size);
d784 1
a784 1
		rv = uvm_map_advice(&p->p_vmspace->vm_map, addr, addr + size,
d789 1
d799 1
d803 1
d809 2
a810 1
		rv = uvm_map_clean(&p->p_vmspace->vm_map, addr, addr + size,
d815 1
d821 2
a822 1
		rv = uvm_map_clean(&p->p_vmspace->vm_map, addr, addr + size,
d827 1
d838 1
d845 1
a845 1
	return (rv);
d869 1
d876 1
d880 2
a881 2
	size = (vsize_t) round_page(size);
	
d883 1
a883 1
	if (addr + (int)size < addr)
d900 1
a900 1
	return (error == KERN_SUCCESS ? 0 : ENOMEM);
d931 1
d935 1
a935 1
	size = (vsize_t) round_page(size);
d938 1
a938 1
	if (addr + (int)size < addr)
d948 1
a948 1
	return (error == KERN_SUCCESS ? 0 : ENOMEM);
a978 17
	switch (error) {
	case KERN_SUCCESS:
		error = 0;
		break;

	case KERN_NO_SPACE:	/* XXX overloaded */
		error = ENOMEM;
		break;

	default:
		/*
		 * "Some or all of the memory could not be locked when
		 * the call was made."
		 */
		error = EAGAIN;
	}

d1008 1
a1008 1
	vm_map_t map;
d1013 1
a1013 1
	caddr_t handle;		/* XXX: VNODE? */
d1019 1
a1019 1
	int retval;
a1041 1
		
d1064 1
a1064 2

		vp = (struct vnode *) handle;	/* get vnode */
d1066 1
a1066 1
			uobj = uvn_attach((void *) vp, (flags & MAP_SHARED) ?
a1068 31
#ifndef UBC
			/*
			 * XXXCDC: hack from old code
			 * don't allow vnodes which have been mapped
			 * shared-writeable to persist [forces them to be
			 * flushed out when last reference goes].
			 * XXXCDC: interesting side effect: avoids a bug.
			 * note that in WRITE [ufs_readwrite.c] that we
			 * allocate buffer, uncache, and then do the write.
			 * the problem with this is that if the uncache causes
			 * VM data to be flushed to the same area of the file
			 * we are writing to... in that case we've got the
			 * buffer locked and our process goes to sleep forever.
			 *
			 * XXXCDC: checking maxprot protects us from the
			 * "persistbug" program but this is not a long term
			 * solution.
			 * 
			 * XXXCDC: we don't bother calling uncache with the vp
			 * VOP_LOCKed since we know that we are already
			 * holding a valid reference to the uvn (from the
			 * uvn_attach above), and thus it is impossible for
			 * the uncache to kill the uvn and trigger I/O.
			 */
			if (flags & MAP_SHARED) {
				if ((prot & VM_PROT_WRITE) ||
				    (maxprot & VM_PROT_WRITE)) {
					uvm_vnp_uncache(vp);
				}
			}
#else
a1070 1
#endif
d1082 1
a1082 1
				uobj = udv_attach((void *) &vp->v_rdev,
a1087 1
		
a1089 1

d1094 1
a1094 5
	/*
	 * set up mapping flags
	 */

	uvmflag = UVM_MAPFLAG(prot, maxprot, 
d1097 6
d1105 2
a1106 1
	 * do it!
d1109 1
a1109 1
	retval = uvm_map(map, addr, size, uobj, foff, 0, uvmflag);
a1110 1
	if (retval == KERN_SUCCESS) {
d1112 1
a1112 2
		 * POSIX 1003.1b -- if our address space was configured
		 * to lock all future mappings, wire the one we just made.
a1113 8
		if (prot == VM_PROT_NONE) {
			/*
			 * No more work to do in this case.
			 */
			return (0);
		}
		
		vm_map_lock(map);
d1115 5
a1119 2
		if (map->flags & VM_MAP_WIREFUTURE) {
			if ((atop(size) + uvmexp.wired) > uvmexp.wiredmax
d1121 3
a1123 3
			    || (locklimit != 0 && (size +
			         ptoa(pmap_wired_count(vm_map_pmap(map)))) >
			        locklimit)
d1125 4
a1128 19
			) {
				retval = KERN_RESOURCE_SHORTAGE;
				vm_map_unlock(map);
				/* unmap the region! */
				(void) uvm_unmap(map, *addr, *addr + size);
				goto bad;
			}
			/*
			 * uvm_map_pageable() always returns the map
			 * unlocked.
			 */
			retval = uvm_map_pageable(map, *addr, *addr + size,
			    FALSE, UVM_LK_ENTER);
			if (retval != KERN_SUCCESS) {
				/* unmap the region! */
				(void) uvm_unmap(map, *addr, *addr + size);
				goto bad;
			}
			return (0);
d1131 3
a1133 1
		vm_map_unlock(map);
d1135 6
d1143 2
a1144 10

	/*
	 * errors: first detach from the uobj, if any.
	 */
	
	if (uobj)
		uobj->pgops->pgo_detach(uobj);

 bad:
	return (retval);
@


1.4.4.8
log
@Sync the SMP branch with 3.3
@
text
@d161 3
d168 1
a168 2
	if ((error = uvm_vslock(p, vec, npgs, VM_PROT_WRITE)) != 0)
		return (error);
a200 1
				pgi = 1;
d203 1
a203 1
					copyout(&pgi, vec, sizeof(char));
d245 1
a245 1
			copyout(&pgi, vec, sizeof(char));
d366 1
a366 1
			return (EBADF);
d370 2
a371 4
		if (fp->f_type != DTYPE_VNODE) {
			error = ENODEV;		/* only mmap vnodes! */
			goto out;
		}
d446 1
a446 1
					goto out;
d449 2
a450 4
				else if (prot & PROT_WRITE) {
					error = EPERM;
					goto out;
				}
d470 2
a471 4
		if (fd != -1) {
			error = EINVAL;
			goto out;
		}
d682 1
a682 1
	uvm_unmap_remove(map, addr, addr + size, &dead_entries);
d1095 1
a1095 1
		uvm_unmap(map, *addr, *addr + size);	/* zap! */
d1218 1
a1218 1
				uvm_unmap(map, *addr, *addr + size);
d1229 1
a1229 1
				uvm_unmap(map, *addr, *addr + size);
@


1.4.4.9
log
@Sync the SMP branch to -current. This includes moving to ELF.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.4.4.8 2003/03/28 00:08:48 niklas Exp $	*/
a119 164
 * sys_mquery: provide mapping hints to applications that do fixed mappings
 *
 * flags: 0 or MAP_FIXED (MAP_FIXED - means that we insist on this addr and
 *	don't care about PMAP_PREFER or such)
 * addr: hint where we'd like to place the mapping.
 * size: size of the mapping
 * fd: fd of the file we want to map
 * off: offset within the file
 */

int
sys_mquery(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{
	struct sys_mquery_args /* {
		syscallarg(caddr_t) addr;
		syscallarg(size_t) len;
		syscallarg(int) prot;
		syscallarg(int) flags;
		syscallarg(int) fd;
		syscallarg(long) pad;
		syscallarg(off_t) pos;
	} */ *uap = v;
	struct file *fp;
	struct uvm_object *uobj;
	voff_t uoff;
	int error;
	vaddr_t vaddr;
	int flags = 0;
	vsize_t size;
	vm_prot_t prot;
	int fd;

	vaddr = (vaddr_t) SCARG(uap, addr);
	prot = SCARG(uap, prot) & VM_PROT_ALL;
	size = (vsize_t) SCARG(uap, len);
	fd = SCARG(uap, fd);

	if (SCARG(uap, flags) & MAP_FIXED)
		flags |= UVM_FLAG_FIXED;

	if (fd >= 0) {
		if ((error = getvnode(p->p_fd, fd, &fp)) != 0)
			return (error);
		uobj = &((struct vnode *)fp->f_data)->v_uvm.u_obj;
		uoff = SCARG(uap, pos);
	} else {
		fp = NULL;
		uobj = NULL;
		uoff = 0;
	}

	if (vaddr == 0)
		vaddr = uvm_map_hint(p, prot);

	/* prevent a user requested address from falling in heap space */
	if ((vaddr + size > (vaddr_t)p->p_vmspace->vm_daddr) &&
	    (vaddr < (vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ)) {
		if (flags & UVM_FLAG_FIXED) {
			error = EINVAL;
			goto done;
		}
		vaddr = round_page((vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ);
	}
again:

	if (uvm_map_findspace(&p->p_vmspace->vm_map, vaddr, size,
	    &vaddr, uobj, uoff, 0, flags) == NULL) {
		if (flags & UVM_FLAG_FIXED)
			error = EINVAL;
		else
			error = ENOMEM;
	} else {
		/* prevent a returned address from falling in heap space */
		if ((vaddr + size > (vaddr_t)p->p_vmspace->vm_daddr)
		    && (vaddr < (vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ)) {
			vaddr = round_page((vaddr_t)p->p_vmspace->vm_daddr +
			    MAXDSIZ);
			goto again;
		}
		error = 0;
		*retval = (register_t)(vaddr);
	}
done:
	if (fp != NULL)
		FRELE(fp);
	return (error);
}

/* ARGSUSED */
int
sys_omquery(struct proc *p, void *v, register_t *retval)
{
	struct sys_omquery_args /* {
		syscallarg(int) flags;
		syscallarg(void **) addr;
		syscallarg(size_t) size;
		syscallarg(int) fd;
		syscallarg(off_t) off;
	} */ *uap = v;
	struct file *fp;
	struct uvm_object *uobj;
	voff_t uoff;
	int error;
	vaddr_t vaddr;
	int flags = 0;
	vm_prot_t prot = SCARG(uap, flags) & VM_PROT_ALL;

	if (SCARG(uap, flags) & MAP_FIXED)
		flags |= UVM_FLAG_FIXED;

	if ((error = copyin(SCARG(uap, addr), &vaddr, sizeof(void *))) != 0)
		return (error);

	if (SCARG(uap, fd) >= 0) {
		if ((error = getvnode(p->p_fd, SCARG(uap, fd), &fp)) != 0)
			return (error);
		uobj = &((struct vnode *)fp->f_data)->v_uvm.u_obj;
		uoff = SCARG(uap, off);
	} else {
		fp = NULL;
		uobj = NULL;
		uoff = 0;
	}

	if (vaddr == 0)
		vaddr = uvm_map_hint(p, prot);

	/* prevent a user requested address from falling in heap space */
	if ((vaddr + SCARG(uap, size) > (vaddr_t)p->p_vmspace->vm_daddr) &&
	    (vaddr < (vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ)) {
		if (flags & UVM_FLAG_FIXED) {
			error = EINVAL;
			goto done;
		}
		vaddr = round_page((vaddr_t)p->p_vmspace->vm_daddr + MAXDSIZ);
	}

	if (uvm_map_findspace(&p->p_vmspace->vm_map, vaddr, SCARG(uap, size),
	    &vaddr, uobj, uoff, 0, flags) == NULL) {
		if (flags & UVM_FLAG_FIXED)
			error = EINVAL;
		else
			error = ENOMEM;
	} else {
		/*
		 * XXX?
		 * is it possible for uvm_map_findspace() to return
		 * an address in vm_addr - vm_addr+MAXDSIZ ?
		 * if all of the memory below 1G (i386) is used, 
		 * this could occur. In this case, could this loop
		 * changing the hint to above daddr in that case?
		 */
		error = copyout(&vaddr, SCARG(uap, addr), sizeof(void *));
	}
done:
	if (fp != NULL)
		FRELE(fp);
	return (error);
}

/*
d353 4
a356 2
		if (addr < uvm_map_hint(p, prot))
			addr = uvm_map_hint(p, prot);
d563 1
a563 1
	if (addr + (ssize_t)size < addr)
d654 1
a654 1
	if ((ssize_t)size < 0)
d732 1
a732 1
	if ((ssize_t)size < 0)
d780 1
a780 1
	if ((ssize_t)size < 0)
d921 1
a921 1
	if (addr + (ssize_t)size < addr)
d975 1
a975 1
	if (addr + (ssize_t)size < addr)
@


1.4.4.10
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_mmap.c,v 1.4.4.9 2003/05/13 19:36:58 ho Exp $	*/
d426 1
a426 1
 * => file offset and address may not be page aligned
@


1.4.4.11
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d92 1
a92 1
		syscallarg(int) incr;
d137 1
a137 1
		syscallarg(void *) addr;
d156 1
a156 1
	prot = SCARG(uap, prot);
a159 3
	if ((prot & VM_PROT_ALL) != prot)
		return (EINVAL);

d211 72
d439 1
a439 1
		syscallarg(void *) addr;
d466 1
a466 1
	prot = SCARG(uap, prot);
a474 4
	if ((prot & VM_PROT_ALL) != prot)
		return (EINVAL);
	if ((flags & MAP_FLAGMASK) != flags)
		return (EINVAL);
d516 2
a517 4
		if (addr == 0)
			addr = uvm_map_hint(p, prot);
		else if (!(flags & MAP_TRYFIXED) &&
		    addr < (vaddr_t)p->p_vmspace->vm_daddr)
d690 1
a690 1
		syscallarg(void *) addr;
d791 1
a791 1
		syscallarg(void *) addr;
d870 2
a871 2
		syscallarg(void *) addr;
		syscallarg(size_t) len;
d885 1
a885 4
	prot = SCARG(uap, prot);
	
	if ((prot & VM_PROT_ALL) != prot)
		return (EINVAL);
d922 2
a923 2
		syscallarg(void *) addr;
		syscallarg(size_t) len;
d967 1
a967 1
		syscallarg(void *) addr;
d973 1
a973 1
	int advice, rv;
d1094 1
a1094 1
	if ((error = suser(p, 0)) != 0)
d1141 1
a1141 1
	if ((error = suser(p, 0)) != 0)
d1172 1
a1172 1
	if ((error = suser(p, 0)) != 0)
@


1.4.4.12
log
@Merge with the trunk
@
text
@a65 2
#include <machine/exec.h>	/* for __LDPGSZ */

a1177 1
	vsize_t align = 0;	/* userland page size */
d1198 1
a1210 2
		if ((flags & MAP_FIXED) == 0 && size >= __LDPGSZ)
			align = __LDPGSZ;
d1298 1
a1298 1
	retval = uvm_map(map, addr, size, uobj, foff, align, uvmflag);
@


1.3
log
@remove sys_omsync, it's in already in compat. (how did this ever link?)
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_mmap.c,v 1.2 1999/02/26 05:32:07 art Exp $	*/
/*	$NetBSD: uvm_mmap.c,v 1.15 1998/10/11 23:18:20 chuck Exp $	*/
a3 4
 * XXXCDC: "ROUGH DRAFT" QUALITY UVM PRE-RELEASE FILE!   
 *         >>>USE AT YOUR OWN RISK, WORK IS NOT FINISHED<<<
 */
/*
d95 2
a96 2
			  syscallarg(int) incr;
			  } */ *uap = v;
d99 1
a99 1
	return (EOPNOTSUPP);
d115 2
a116 2
			  syscallarg(int) incr;
			  } */ *uap = v;
d119 1
a119 1
	return (EOPNOTSUPP);
d135 4
a138 4
			     syscallarg(caddr_t) addr;
			     syscallarg(size_t) len;
			     syscallarg(int) behav;
			     } */ *uap = v;
d141 1
a141 1
	return (EOPNOTSUPP);
d157 4
a160 4
			     syscallarg(caddr_t) addr;
			     syscallarg(size_t) len;
			     syscallarg(char *) vec;
			     } */ *uap = v;
d163 1
a163 1
	return (EOPNOTSUPP);
d240 2
a241 1
		printf("mmap: pos=%qx, size=%x too big\n", pos, (int)size);
d417 1
a417 1
sys_msync(p, v, retval)		/* ART_UVM_XXX - is this correct msync? */
d903 1
a903 1
			    maxprot : (maxprot & ~VM_PROT_WRITE));
@


1.2
log
@add OpenBSD tags
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d417 1
a417 15
 * XXX
 * XXX
 * XXX
 */
int
sys_omsync(p, v, retval)
	struct proc *p;
	void *v;
	register_t *retval;
{
	return EOPNOTSUPP;
}

/*
 * sys___msync13: the msync system call (a front-end for flush)
@


1.1
log
@Import of uvm from NetBSD. Some local changes, some code disabled
@
text
@d1 1
@

