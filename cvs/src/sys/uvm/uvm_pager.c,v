head	1.71;
access;
symbols
	OPENBSD_6_2_BASE:1.71
	OPENBSD_6_1:1.71.0.12
	OPENBSD_6_1_BASE:1.71
	OPENBSD_6_0:1.71.0.8
	OPENBSD_6_0_BASE:1.71
	OPENBSD_5_9:1.71.0.4
	OPENBSD_5_9_BASE:1.71
	OPENBSD_5_8:1.71.0.6
	OPENBSD_5_8_BASE:1.71
	OPENBSD_5_7:1.71.0.2
	OPENBSD_5_7_BASE:1.71
	OPENBSD_5_6:1.68.0.4
	OPENBSD_5_6_BASE:1.68
	OPENBSD_5_5:1.64.0.4
	OPENBSD_5_5_BASE:1.64
	OPENBSD_5_4:1.63.0.2
	OPENBSD_5_4_BASE:1.63
	OPENBSD_5_3:1.60.0.8
	OPENBSD_5_3_BASE:1.60
	OPENBSD_5_2:1.60.0.6
	OPENBSD_5_2_BASE:1.60
	OPENBSD_5_1_BASE:1.60
	OPENBSD_5_1:1.60.0.4
	OPENBSD_5_0:1.60.0.2
	OPENBSD_5_0_BASE:1.60
	OPENBSD_4_9:1.57.0.4
	OPENBSD_4_9_BASE:1.57
	OPENBSD_4_8:1.57.0.2
	OPENBSD_4_8_BASE:1.57
	OPENBSD_4_7:1.55.0.2
	OPENBSD_4_7_BASE:1.55
	OPENBSD_4_6:1.53.0.4
	OPENBSD_4_6_BASE:1.53
	OPENBSD_4_5:1.46.0.2
	OPENBSD_4_5_BASE:1.46
	OPENBSD_4_4:1.44.0.4
	OPENBSD_4_4_BASE:1.44
	OPENBSD_4_3:1.44.0.2
	OPENBSD_4_3_BASE:1.44
	OPENBSD_4_2:1.43.0.2
	OPENBSD_4_2_BASE:1.43
	OPENBSD_4_1:1.39.0.4
	OPENBSD_4_1_BASE:1.39
	OPENBSD_4_0:1.39.0.2
	OPENBSD_4_0_BASE:1.39
	OPENBSD_3_9:1.37.0.4
	OPENBSD_3_9_BASE:1.37
	OPENBSD_3_8:1.37.0.2
	OPENBSD_3_8_BASE:1.37
	OPENBSD_3_7:1.35.0.6
	OPENBSD_3_7_BASE:1.35
	OPENBSD_3_6:1.35.0.4
	OPENBSD_3_6_BASE:1.35
	SMP_SYNC_A:1.35
	SMP_SYNC_B:1.35
	OPENBSD_3_5:1.35.0.2
	OPENBSD_3_5_BASE:1.35
	OPENBSD_3_4:1.34.0.2
	OPENBSD_3_4_BASE:1.34
	UBC_SYNC_A:1.34
	OPENBSD_3_3:1.33.0.2
	OPENBSD_3_3_BASE:1.33
	OPENBSD_3_2:1.32.0.2
	OPENBSD_3_2_BASE:1.32
	OPENBSD_3_1:1.30.0.2
	OPENBSD_3_1_BASE:1.30
	UBC_SYNC_B:1.32
	UBC:1.28.0.2
	UBC_BASE:1.28
	OPENBSD_3_0:1.16.0.2
	OPENBSD_3_0_BASE:1.16
	OPENBSD_2_9_BASE:1.8
	OPENBSD_2_9:1.8.0.2
	OPENBSD_2_8:1.5.0.4
	OPENBSD_2_8_BASE:1.5
	OPENBSD_2_7:1.5.0.2
	OPENBSD_2_7_BASE:1.5
	SMP:1.4.0.4
	SMP_BASE:1.4
	kame_19991208:1.4
	OPENBSD_2_6:1.4.0.2
	OPENBSD_2_6_BASE:1.4
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.71
date	2014.12.17.19.42.15;	author tedu;	state Exp;
branches;
next	1.70;
commitid	G4ldVK4QwvfU3tRp;

1.70
date	2014.11.16.12.31.00;	author deraadt;	state Exp;
branches;
next	1.69;
commitid	yv0ECmCdICvq576h;

1.69
date	2014.09.14.14.17.27;	author jsg;	state Exp;
branches;
next	1.68;
commitid	uzzBR7hz9ncd4O6G;

1.68
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.67;
commitid	7NtJNW9udCOFtDNM;

1.67
date	2014.05.09.03.54.28;	author tedu;	state Exp;
branches;
next	1.66;

1.66
date	2014.05.08.20.08.50;	author kettenis;	state Exp;
branches;
next	1.65;

1.65
date	2014.04.13.23.14.15;	author tedu;	state Exp;
branches;
next	1.64;

1.64
date	2013.11.02.00.08.17;	author krw;	state Exp;
branches;
next	1.63;

1.63
date	2013.06.11.16.42.19;	author deraadt;	state Exp;
branches;
next	1.62;

1.62
date	2013.05.30.16.29.46;	author tedu;	state Exp;
branches;
next	1.61;

1.61
date	2013.05.30.15.17.59;	author tedu;	state Exp;
branches;
next	1.60;

1.60
date	2011.07.03.18.34.14;	author oga;	state Exp;
branches;
next	1.59;

1.59
date	2011.04.15.22.00.46;	author oga;	state Exp;
branches;
next	1.58;

1.58
date	2011.04.15.21.47.24;	author oga;	state Exp;
branches;
next	1.57;

1.57
date	2010.07.24.15.40.39;	author kettenis;	state Exp;
branches;
next	1.56;

1.56
date	2010.06.27.20.53.31;	author oga;	state Exp;
branches;
next	1.55;

1.55
date	2010.02.12.01.35.14;	author tedu;	state Exp;
branches;
next	1.54;

1.54
date	2009.07.22.21.05.37;	author oga;	state Exp;
branches;
next	1.53;

1.53
date	2009.06.17.00.13.59;	author oga;	state Exp;
branches;
next	1.52;

1.52
date	2009.06.16.17.14.15;	author oga;	state Exp;
branches;
next	1.51;

1.51
date	2009.05.23.14.06.37;	author oga;	state Exp;
branches;
next	1.50;

1.50
date	2009.05.04.18.08.06;	author oga;	state Exp;
branches;
next	1.49;

1.49
date	2009.04.06.12.02.52;	author oga;	state Exp;
branches;
next	1.48;

1.48
date	2009.03.25.20.00.18;	author oga;	state Exp;
branches;
next	1.47;

1.47
date	2009.03.20.15.19.04;	author oga;	state Exp;
branches;
next	1.46;

1.46
date	2009.01.27.19.21.03;	author ariane;	state Exp;
branches;
next	1.45;

1.45
date	2008.11.24.19.55.33;	author thib;	state Exp;
branches;
next	1.44;

1.44
date	2007.11.26.22.49.08;	author miod;	state Exp;
branches;
next	1.43;

1.43
date	2007.06.06.17.15.14;	author deraadt;	state Exp;
branches;
next	1.42;

1.42
date	2007.04.13.18.57.49;	author art;	state Exp;
branches;
next	1.41;

1.41
date	2007.04.04.17.44.45;	author art;	state Exp;
branches;
next	1.40;

1.40
date	2007.03.25.11.31.07;	author art;	state Exp;
branches;
next	1.39;

1.39
date	2006.07.31.11.51.29;	author mickey;	state Exp;
branches;
next	1.38;

1.38
date	2006.07.26.23.15.55;	author mickey;	state Exp;
branches;
next	1.37;

1.37
date	2005.07.26.07.11.55;	author art;	state Exp;
branches;
next	1.36;

1.36
date	2005.05.24.21.11.47;	author tedu;	state Exp;
branches;
next	1.35;

1.35
date	2004.02.23.06.19.32;	author drahn;	state Exp;
branches;
next	1.34;

1.34
date	2003.03.29.01.13.57;	author mickey;	state Exp;
branches;
next	1.33;

1.33
date	2002.10.29.18.30.21;	author art;	state Exp;
branches;
next	1.32;

1.32
date	2002.05.24.13.10.53;	author art;	state Exp;
branches;
next	1.31;

1.31
date	2002.05.22.14.29.20;	author art;	state Exp;
branches;
next	1.30;

1.30
date	2002.01.02.22.23.25;	author miod;	state Exp;
branches;
next	1.29;

1.29
date	2001.12.19.08.58.07;	author art;	state Exp;
branches;
next	1.28;

1.28
date	2001.12.04.23.22.42;	author art;	state Exp;
branches
	1.28.2.1;
next	1.27;

1.27
date	2001.11.30.05.45.33;	author csapuntz;	state Exp;
branches;
next	1.26;

1.26
date	2001.11.29.01.59.19;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2001.11.28.19.28.15;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2001.11.28.13.47.40;	author art;	state Exp;
branches;
next	1.23;

1.23
date	2001.11.27.05.27.12;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2001.11.12.01.26.10;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2001.11.10.18.42.31;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2001.11.07.02.55.50;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2001.11.07.01.18.01;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2001.11.06.01.35.04;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2001.11.05.22.14.54;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2001.09.20.17.02.32;	author mpech;	state Exp;
branches;
next	1.15;

1.15
date	2001.09.19.20.50.59;	author mickey;	state Exp;
branches;
next	1.14;

1.14
date	2001.08.11.10.57.22;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2001.08.06.14.03.05;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2001.07.26.19.37.13;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2001.07.25.13.25.33;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2001.07.18.10.47.05;	author art;	state Exp;
branches;
next	1.9;

1.9
date	2001.06.08.08.09.40;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2001.03.22.03.05.56;	author smart;	state Exp;
branches;
next	1.7;

1.7
date	2001.03.08.15.21.37;	author smart;	state Exp;
branches;
next	1.6;

1.6
date	2001.01.29.02.07.47;	author niklas;	state Exp;
branches;
next	1.5;

1.5
date	2000.03.16.22.11.05;	author art;	state Exp;
branches;
next	1.4;

1.4
date	99.09.03.18.02.23;	author art;	state Exp;
branches
	1.4.4.1;
next	1.3;

1.3
date	99.08.23.08.13.24;	author art;	state Exp;
branches;
next	1.2;

1.2
date	99.02.26.05.32.08;	author art;	state Exp;
branches;
next	1.1;

1.1
date	99.02.26.01.30.17;	author art;	state Exp;
branches;
next	;

1.4.4.1
date	2000.03.24.09.09.51;	author niklas;	state Exp;
branches;
next	1.4.4.2;

1.4.4.2
date	2001.05.14.22.47.47;	author niklas;	state Exp;
branches;
next	1.4.4.3;

1.4.4.3
date	2001.07.04.11.01.07;	author niklas;	state Exp;
branches;
next	1.4.4.4;

1.4.4.4
date	2001.10.31.03.32.14;	author nate;	state Exp;
branches;
next	1.4.4.5;

1.4.4.5
date	2001.11.13.23.02.31;	author niklas;	state Exp;
branches;
next	1.4.4.6;

1.4.4.6
date	2001.12.05.01.23.58;	author niklas;	state Exp;
branches;
next	1.4.4.7;

1.4.4.7
date	2002.03.06.02.17.14;	author niklas;	state Exp;
branches;
next	1.4.4.8;

1.4.4.8
date	2003.03.28.00.08.48;	author niklas;	state Exp;
branches;
next	1.4.4.9;

1.4.4.9
date	2003.05.13.19.36.58;	author ho;	state Exp;
branches;
next	1.4.4.10;

1.4.4.10
date	2004.06.05.23.13.13;	author niklas;	state Exp;
branches;
next	;

1.28.2.1
date	2002.01.31.22.55.51;	author niklas;	state Exp;
branches;
next	1.28.2.2;

1.28.2.2
date	2002.02.02.03.28.27;	author art;	state Exp;
branches;
next	1.28.2.3;

1.28.2.3
date	2002.06.11.03.33.04;	author art;	state Exp;
branches;
next	1.28.2.4;

1.28.2.4
date	2002.11.04.18.02.33;	author art;	state Exp;
branches;
next	1.28.2.5;

1.28.2.5
date	2003.05.19.22.41.30;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.71
log
@remove lock.h from uvm_extern.h. another holdover from the simpletonlock
era. fix uvm including c files to include lock.h or atomic.h as necessary.
ok deraadt
@
text
@/*	$OpenBSD: uvm_pager.c,v 1.70 2014/11/16 12:31:00 deraadt Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.36 2000/11/27 18:26:41 chs Exp $	*/

/*
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 * from: Id: uvm_pager.c,v 1.1.2.23 1998/02/02 20:38:06 chuck Exp
 */

/*
 * uvm_pager.c: generic functions used to assist the pagers.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/malloc.h>
#include <sys/pool.h>
#include <sys/buf.h>
#include <sys/atomic.h>

#include <uvm/uvm.h>

struct pool *uvm_aiobuf_pool;

struct uvm_pagerops *uvmpagerops[] = {
	&aobj_pager,
	&uvm_deviceops,
	&uvm_vnodeops,
};

/*
 * the pager map: provides KVA for I/O
 *
 * Each uvm_pseg has room for MAX_PAGERMAP_SEGS pager io space of
 * MAXBSIZE bytes.
 *
 * The number of uvm_pseg instances is dynamic using an array segs.
 * At most UVM_PSEG_COUNT instances can exist.
 *
 * psegs[0] always exists (so that the pager can always map in pages).
 * psegs[0] element 0 is always reserved for the pagedaemon.
 *
 * Any other pseg is automatically created when no space is available
 * and automatically destroyed when it is no longer in use.
 */
#define MAX_PAGER_SEGS	16
#define PSEG_NUMSEGS	(PAGER_MAP_SIZE / MAX_PAGER_SEGS / MAXBSIZE)
struct uvm_pseg {
	/* Start of virtual space; 0 if not inited. */
	vaddr_t	start;
	/* Bitmap of the segments in use in this pseg. */
	int	use;
};
struct	mutex uvm_pseg_lck;
struct	uvm_pseg psegs[PSEG_NUMSEGS];

#define UVM_PSEG_FULL(pseg)	((pseg)->use == (1 << MAX_PAGER_SEGS) - 1)
#define UVM_PSEG_EMPTY(pseg)	((pseg)->use == 0)
#define UVM_PSEG_INUSE(pseg,id)	(((pseg)->use & (1 << (id))) != 0)

void		uvm_pseg_init(struct uvm_pseg *);
vaddr_t		uvm_pseg_get(int);
void		uvm_pseg_release(vaddr_t);

/*
 * uvm_pager_init: init pagers (at boot time)
 */
void
uvm_pager_init(void)
{
	int lcv;

	/* init pager map */
	uvm_pseg_init(&psegs[0]);
	mtx_init(&uvm_pseg_lck, IPL_VM);

	/* init ASYNC I/O queue */
	TAILQ_INIT(&uvm.aio_done);

	/* call pager init functions */
	for (lcv = 0 ; lcv < sizeof(uvmpagerops)/sizeof(struct uvm_pagerops *);
	    lcv++) {
		if (uvmpagerops[lcv]->pgo_init)
			uvmpagerops[lcv]->pgo_init();
	}
}

/*
 * Initialize a uvm_pseg.
 *
 * May fail, in which case seg->start == 0.
 *
 * Caller locks uvm_pseg_lck.
 */
void
uvm_pseg_init(struct uvm_pseg *pseg)
{
	KASSERT(pseg->start == 0);
	KASSERT(pseg->use == 0);
	pseg->start = uvm_km_valloc_try(kernel_map, MAX_PAGER_SEGS * MAXBSIZE);
}

/*
 * Acquire a pager map segment.
 *
 * Returns a vaddr for paging. 0 on failure.
 *
 * Caller does not lock.
 */
vaddr_t
uvm_pseg_get(int flags)
{
	int i;
	struct uvm_pseg *pseg;

	mtx_enter(&uvm_pseg_lck);

pager_seg_restart:
	/* Find first pseg that has room. */
	for (pseg = &psegs[0]; pseg != &psegs[PSEG_NUMSEGS]; pseg++) {
		if (UVM_PSEG_FULL(pseg))
			continue;

		if (pseg->start == 0) {
			/* Need initialization. */
			uvm_pseg_init(pseg);
			if (pseg->start == 0)
				goto pager_seg_fail;
		}

		/* Keep index 0 reserved for pagedaemon. */
		if (pseg == &psegs[0] && curproc != uvm.pagedaemon_proc)
			i = 1;
		else
			i = 0;

		for (; i < MAX_PAGER_SEGS; i++) {
			if (!UVM_PSEG_INUSE(pseg, i)) {
				pseg->use |= 1 << i;
				mtx_leave(&uvm_pseg_lck);
				return pseg->start + i * MAXBSIZE;
			}
		}
	}

pager_seg_fail:
	if ((flags & UVMPAGER_MAPIN_WAITOK) != 0) {
		msleep(&psegs, &uvm_pseg_lck, PVM, "pagerseg", 0);
		goto pager_seg_restart;
	}

	mtx_leave(&uvm_pseg_lck);
	return 0;
}

/*
 * Release a pager map segment.
 *
 * Caller does not lock.
 *
 * Deallocates pseg if it is no longer in use.
 */
void
uvm_pseg_release(vaddr_t segaddr)
{
	int id;
	struct uvm_pseg *pseg;
	vaddr_t va = 0;

	for (pseg = &psegs[0]; pseg != &psegs[PSEG_NUMSEGS]; pseg++) {
		if (pseg->start <= segaddr &&
		    segaddr < pseg->start + MAX_PAGER_SEGS * MAXBSIZE)
			break;
	}
	KASSERT(pseg != &psegs[PSEG_NUMSEGS]);

	id = (segaddr - pseg->start) / MAXBSIZE;
	KASSERT(id >= 0 && id < MAX_PAGER_SEGS);

	/* test for no remainder */
	KDASSERT(segaddr == pseg->start + id * MAXBSIZE);

	mtx_enter(&uvm_pseg_lck);

	KASSERT(UVM_PSEG_INUSE(pseg, id));

	pseg->use &= ~(1 << id);
	wakeup(&psegs);

	if (pseg != &psegs[0] && UVM_PSEG_EMPTY(pseg)) {
		va = pseg->start;
		pseg->start = 0;
	}

	mtx_leave(&uvm_pseg_lck);

	if (va)
		uvm_km_free(kernel_map, va, MAX_PAGER_SEGS * MAXBSIZE);
}

/*
 * uvm_pagermapin: map pages into KVA for I/O that needs mappings
 *
 * We basically just km_valloc a blank map entry to reserve the space in the
 * kernel map and then use pmap_enter() to put the mappings in by hand.
 */
vaddr_t
uvm_pagermapin(struct vm_page **pps, int npages, int flags)
{
	vaddr_t kva, cva;
	vm_prot_t prot;
	vsize_t size;
	struct vm_page *pp;

	prot = PROT_READ;
	if (flags & UVMPAGER_MAPIN_READ)
		prot |= PROT_WRITE;
	size = ptoa(npages);

	KASSERT(size <= MAXBSIZE);

	kva = uvm_pseg_get(flags);
	if (kva == 0)
		return 0;

	for (cva = kva ; size != 0 ; size -= PAGE_SIZE, cva += PAGE_SIZE) {
		pp = *pps++;
		KASSERT(pp);
		KASSERT(pp->pg_flags & PG_BUSY);
		/* Allow pmap_enter to fail. */
		if (pmap_enter(pmap_kernel(), cva, VM_PAGE_TO_PHYS(pp),
		    prot, PMAP_WIRED | PMAP_CANFAIL | prot) != 0) {
			pmap_remove(pmap_kernel(), kva, cva);
			pmap_update(pmap_kernel());
			uvm_pseg_release(kva);
			return 0;
		}
	}
	pmap_update(pmap_kernel());
	return kva;
}

/*
 * uvm_pagermapout: remove KVA mapping
 *
 * We remove our mappings by hand and then remove the mapping.
 */
void
uvm_pagermapout(vaddr_t kva, int npages)
{

	pmap_remove(pmap_kernel(), kva, kva + ((vsize_t)npages << PAGE_SHIFT));
	pmap_update(pmap_kernel());
	uvm_pseg_release(kva);

}

/*
 * uvm_mk_pcluster
 *
 * generic "make 'pager put' cluster" function.  a pager can either
 * [1] set pgo_mk_pcluster to NULL (never cluster), [2] set it to this
 * generic function, or [3] set it to a pager specific function.
 *
 * => caller must lock object _and_ pagequeues (since we need to look
 *    at active vs. inactive bits, etc.)
 * => caller must make center page busy and write-protect it
 * => we mark all cluster pages busy for the caller
 * => the caller must unbusy all pages (and check wanted/released
 *    status if it drops the object lock)
 * => flags:
 *      PGO_ALLPAGES:  all pages in object are valid targets
 *      !PGO_ALLPAGES: use "lo" and "hi" to limit range of cluster
 *      PGO_DOACTCLUST: include active pages in cluster.
 *	PGO_FREE: set the PG_RELEASED bits on the cluster so they'll be freed
 *		in async io (caller must clean on error).
 *        NOTE: the caller should clear PG_CLEANCHK bits if PGO_DOACTCLUST.
 *              PG_CLEANCHK is only a hint, but clearing will help reduce
 *		the number of calls we make to the pmap layer.
 */

struct vm_page **
uvm_mk_pcluster(struct uvm_object *uobj, struct vm_page **pps, int *npages,
    struct vm_page *center, int flags, voff_t mlo, voff_t mhi)
{
	struct vm_page **ppsp, *pclust;
	voff_t lo, hi, curoff;
	int center_idx, forward, incr;

	/* 
	 * center page should already be busy and write protected.  XXX:
	 * suppose page is wired?  if we lock, then a process could
	 * fault/block on it.  if we don't lock, a process could write the
	 * pages in the middle of an I/O.  (consider an msync()).  let's
	 * lock it for now (better to delay than corrupt data?).
	 */
	/* get cluster boundaries, check sanity, and apply our limits as well.*/
	uobj->pgops->pgo_cluster(uobj, center->offset, &lo, &hi);
	if ((flags & PGO_ALLPAGES) == 0) {
		if (lo < mlo)
			lo = mlo;
		if (hi > mhi)
			hi = mhi;
	}
	if ((hi - lo) >> PAGE_SHIFT > *npages) { /* pps too small, bail out! */
		pps[0] = center;
		*npages = 1;
		return(pps);
	}

	/* now determine the center and attempt to cluster around the edges */
	center_idx = (center->offset - lo) >> PAGE_SHIFT;
	pps[center_idx] = center;	/* plug in the center page */
	ppsp = &pps[center_idx];
	*npages = 1;

	/*
	 * attempt to cluster around the left [backward], and then 
	 * the right side [forward].    
	 *
	 * note that for inactive pages (pages that have been deactivated)
	 * there are no valid mappings and PG_CLEAN should be up to date.
	 * [i.e. there is no need to query the pmap with pmap_is_modified
	 * since there are no mappings].
	 */
	for (forward  = 0 ; forward <= 1 ; forward++) {
		incr = forward ? PAGE_SIZE : -PAGE_SIZE;
		curoff = center->offset + incr;
		for ( ;(forward == 0 && curoff >= lo) ||
		       (forward && curoff < hi);
		      curoff += incr) {

			pclust = uvm_pagelookup(uobj, curoff); /* lookup page */
			if (pclust == NULL) {
				break;			/* no page */
			}
			/* handle active pages */
			/* NOTE: inactive pages don't have pmap mappings */
			if ((pclust->pg_flags & PQ_INACTIVE) == 0) {
				if ((flags & PGO_DOACTCLUST) == 0) {
					/* dont want mapped pages at all */
					break;
				}

				/* make sure "clean" bit is sync'd */
				if ((pclust->pg_flags & PG_CLEANCHK) == 0) {
					if ((pclust->pg_flags & (PG_CLEAN|PG_BUSY))
					   == PG_CLEAN &&
					   pmap_is_modified(pclust))
						atomic_clearbits_int(
						    &pclust->pg_flags,
						    PG_CLEAN);
					/* now checked */
					atomic_setbits_int(&pclust->pg_flags,
					    PG_CLEANCHK);
				}
			}

			/* is page available for cleaning and does it need it */
			if ((pclust->pg_flags & (PG_CLEAN|PG_BUSY)) != 0) {
				break;	/* page is already clean or is busy */
			}

			/* yes!   enroll the page in our array */
			atomic_setbits_int(&pclust->pg_flags, PG_BUSY);
			UVM_PAGE_OWN(pclust, "uvm_mk_pcluster");

			/*
			 * If we want to free after io is done, and we're
			 * async, set the released flag
			 */
			if ((flags & (PGO_FREE|PGO_SYNCIO)) == PGO_FREE)
				atomic_setbits_int(&pclust->pg_flags,
				    PG_RELEASED);

			/* XXX: protect wired page?   see above comment. */
			pmap_page_protect(pclust, PROT_READ);
			if (!forward) {
				ppsp--;			/* back up one page */
				*ppsp = pclust;
			} else {
				/* move forward one page */
				ppsp[*npages] = pclust;
			}
			(*npages)++;
		}
	}
	
	/*
	 * done!  return the cluster array to the caller!!!
	 */
	return(ppsp);
}

/*
 * uvm_pager_put: high level pageout routine
 *
 * we want to pageout page "pg" to backing store, clustering if
 * possible.
 *
 * => page queues must be locked by caller
 * => if page is not swap-backed, then "uobj" points to the object
 *	backing it.
 * => if page is swap-backed, then "uobj" should be NULL.
 * => "pg" should be PG_BUSY (by caller), and !PG_CLEAN
 *    for swap-backed memory, "pg" can be NULL if there is no page
 *    of interest [sometimes the case for the pagedaemon]
 * => "ppsp_ptr" should point to an array of npages vm_page pointers
 *	for possible cluster building
 * => flags (first two for non-swap-backed pages)
 *	PGO_ALLPAGES: all pages in uobj are valid targets
 *	PGO_DOACTCLUST: include "PQ_ACTIVE" pages as valid targets
 *	PGO_SYNCIO: do SYNC I/O (no async)
 *	PGO_PDFREECLUST: pagedaemon: drop cluster on successful I/O
 *	PGO_FREE: tell the aio daemon to free pages in the async case.
 * => start/stop: if (uobj && !PGO_ALLPAGES) limit targets to this range
 *		  if (!uobj) start is the (daddr_t) of the starting swapblk
 * => return state:
 *	1. we return the VM_PAGER status code of the pageout
 *	2. we return with the page queues unlocked
 *	3. on errors we always drop the cluster.   thus, if we return
 *		!PEND, !OK, then the caller only has to worry about
 *		un-busying the main page (not the cluster pages).
 *	4. on success, if !PGO_PDFREECLUST, we return the cluster
 *		with all pages busy (caller must un-busy and check
 *		wanted/released flags).
 */
int
uvm_pager_put(struct uvm_object *uobj, struct vm_page *pg,
    struct vm_page ***ppsp_ptr, int *npages, int flags,
    voff_t start, voff_t stop)
{
	int result;
	daddr_t swblk;
	struct vm_page **ppsp = *ppsp_ptr;

	/*
	 * note that uobj is null  if we are doing a swap-backed pageout.
	 * note that uobj is !null if we are doing normal object pageout.
	 * note that the page queues must be locked to cluster.
	 */
	if (uobj) {	/* if !swap-backed */
		/*
		 * attempt to build a cluster for pageout using its
		 * make-put-cluster function (if it has one).
		 */
		if (uobj->pgops->pgo_mk_pcluster) {
			ppsp = uobj->pgops->pgo_mk_pcluster(uobj, ppsp,
			    npages, pg, flags, start, stop);
			*ppsp_ptr = ppsp;  /* update caller's pointer */
		} else {
			ppsp[0] = pg;
			*npages = 1;
		}

		swblk = 0;		/* XXX: keep gcc happy */
	} else {
		/*
		 * for swap-backed pageout, the caller (the pagedaemon) has
		 * already built the cluster for us.   the starting swap
		 * block we are writing to has been passed in as "start."
		 * "pg" could be NULL if there is no page we are especially
		 * interested in (in which case the whole cluster gets dropped
		 * in the event of an error or a sync "done").
		 */
		swblk = start;
		/* ppsp and npages should be ok */
	}

	/* now that we've clustered we can unlock the page queues */
	uvm_unlock_pageq();

	/*
	 * now attempt the I/O.   if we have a failure and we are
	 * clustered, we will drop the cluster and try again.
	 */
ReTry:
	if (uobj) {
		result = uobj->pgops->pgo_put(uobj, ppsp, *npages, flags);
	} else {
		/* XXX daddr_t -> int */
		result = uvm_swap_put(swblk, ppsp, *npages, flags);
	}

	/*
	 * we have attempted the I/O.
	 *
	 * if the I/O was a success then:
	 * 	if !PGO_PDFREECLUST, we return the cluster to the 
	 *		caller (who must un-busy all pages)
	 *	else we un-busy cluster pages for the pagedaemon
	 *
	 * if I/O is pending (async i/o) then we return the pending code.
	 * [in this case the async i/o done function must clean up when
	 *  i/o is done...]
	 */
	if (result == VM_PAGER_PEND || result == VM_PAGER_OK) {
		if (result == VM_PAGER_OK && (flags & PGO_PDFREECLUST)) {
			/* drop cluster */
			if (*npages > 1 || pg == NULL)
				uvm_pager_dropcluster(uobj, pg, ppsp, npages,
				    PGO_PDFREECLUST);
		}
		return (result);
	}

	/*
	 * a pager error occured (even after dropping the cluster, if there
	 * was one).  give up! the caller only has one page ("pg")
	 * to worry about.
	 */
	if (*npages > 1 || pg == NULL) {
		uvm_pager_dropcluster(uobj, pg, ppsp, npages, PGO_REALLOCSWAP);

		/*
		 * for failed swap-backed pageouts with a "pg",
		 * we need to reset pg's swslot to either:
		 * "swblk" (for transient errors, so we can retry),
		 * or 0 (for hard errors).
		 */
		if (uobj == NULL && pg != NULL) {
			/* XXX daddr_t -> int */
			int nswblk = (result == VM_PAGER_AGAIN) ? swblk : 0;
			if (pg->pg_flags & PQ_ANON) {
				pg->uanon->an_swslot = nswblk;
			} else {
				uao_set_swslot(pg->uobject,
					       pg->offset >> PAGE_SHIFT,
					       nswblk);
			}
		}
		if (result == VM_PAGER_AGAIN) {
			/*
			 * for transient failures, free all the swslots that
			 * we're not going to retry with.
			 */
			if (uobj == NULL) {
				if (pg) {
					/* XXX daddr_t -> int */
					uvm_swap_free(swblk + 1, *npages - 1);
				} else {
					/* XXX daddr_t -> int */
					uvm_swap_free(swblk, *npages);
				}
			}
			if (pg) {
				ppsp[0] = pg;
				*npages = 1;
				goto ReTry;
			}
		} else if (uobj == NULL) {
			/*
			 * for hard errors on swap-backed pageouts,
			 * mark the swslots as bad.  note that we do not
			 * free swslots that we mark bad.
			 */
			/* XXX daddr_t -> int */
			uvm_swap_markbad(swblk, *npages);
		}
	}

	/*
	 * a pager error occurred (even after dropping the cluster, if there
	 * was one).    give up!   the caller only has one page ("pg")
	 * to worry about.
	 */
	
	return(result);
}

/*
 * uvm_pager_dropcluster: drop a cluster we have built (because we 
 * got an error, or, if PGO_PDFREECLUST we are un-busying the
 * cluster pages on behalf of the pagedaemon).
 *
 * => uobj, if non-null, is a non-swap-backed object
 * => page queues are not locked
 * => pg is our page of interest (the one we clustered around, can be null)
 * => ppsp/npages is our current cluster
 * => flags: PGO_PDFREECLUST: pageout was a success: un-busy cluster
 *	pages on behalf of the pagedaemon.
 *           PGO_REALLOCSWAP: drop previously allocated swap slots for 
 *		clustered swap-backed pages (except for "pg" if !NULL)
 *		"swblk" is the start of swap alloc (e.g. for ppsp[0])
 *		[only meaningful if swap-backed (uobj == NULL)]
 */

void
uvm_pager_dropcluster(struct uvm_object *uobj, struct vm_page *pg,
    struct vm_page **ppsp, int *npages, int flags)
{
	int lcv;

	/* drop all pages but "pg" */
	for (lcv = 0 ; lcv < *npages ; lcv++) {
		/* skip "pg" or empty slot */
		if (ppsp[lcv] == pg || ppsp[lcv] == NULL)
			continue;
	
		/*
		 * Note that PQ_ANON bit can't change as long as we are holding
		 * the PG_BUSY bit (so there is no need to lock the page
		 * queues to test it).
		 */
		if (!uobj) {
			if (ppsp[lcv]->pg_flags & PQ_ANON) {
				if (flags & PGO_REALLOCSWAP)
					  /* zap swap block */
					  ppsp[lcv]->uanon->an_swslot = 0;
			} else {
				if (flags & PGO_REALLOCSWAP)
					uao_set_swslot(ppsp[lcv]->uobject,
					    ppsp[lcv]->offset >> PAGE_SHIFT, 0);
			}
		}

		/* did someone want the page while we had it busy-locked? */
		if (ppsp[lcv]->pg_flags & PG_WANTED) {
			wakeup(ppsp[lcv]);
		}

		/* if page was released, release it.  otherwise un-busy it */
		if (ppsp[lcv]->pg_flags & PG_RELEASED &&
		    ppsp[lcv]->pg_flags & PQ_ANON) {
				/* so that anfree will free */
				atomic_clearbits_int(&ppsp[lcv]->pg_flags,
				    PG_BUSY);
				UVM_PAGE_OWN(ppsp[lcv], NULL);

				/* kills anon and frees pg */
				uvm_anfree(ppsp[lcv]->uanon);

				continue;
		} else {
			/*
			 * if we were planning on async io then we would
			 * have PG_RELEASED set, clear that with the others.
			 */
			atomic_clearbits_int(&ppsp[lcv]->pg_flags,
			    PG_BUSY|PG_WANTED|PG_FAKE|PG_RELEASED);
			UVM_PAGE_OWN(ppsp[lcv], NULL);
		}

		/*
		 * if we are operating on behalf of the pagedaemon and we 
		 * had a successful pageout update the page!
		 */
		if (flags & PGO_PDFREECLUST) {
			pmap_clear_reference(ppsp[lcv]);
			pmap_clear_modify(ppsp[lcv]);
			atomic_setbits_int(&ppsp[lcv]->pg_flags, PG_CLEAN);
		}
	}
}

/*
 * interrupt-context iodone handler for single-buf i/os
 * or the top-level buf of a nested-buf i/o.
 *
 * => must be at splbio().
 */

void
uvm_aio_biodone(struct buf *bp)
{
	splassert(IPL_BIO);

	/* reset b_iodone for when this is a single-buf i/o. */
	bp->b_iodone = uvm_aio_aiodone;

	mtx_enter(&uvm.aiodoned_lock);
	TAILQ_INSERT_TAIL(&uvm.aio_done, bp, b_freelist);
	wakeup(&uvm.aiodoned);
	mtx_leave(&uvm.aiodoned_lock);
}

/*
 * uvm_aio_aiodone: do iodone processing for async i/os.
 * this should be called in thread context, not interrupt context.
 */
void
uvm_aio_aiodone(struct buf *bp)
{
	int npages = bp->b_bufsize >> PAGE_SHIFT;
	struct vm_page *pg, *pgs[MAXPHYS >> PAGE_SHIFT];
	struct uvm_object *uobj;
	int i, error;
	boolean_t write, swap;

	KASSERT(npages <= MAXPHYS >> PAGE_SHIFT);
	splassert(IPL_BIO);

	error = (bp->b_flags & B_ERROR) ? (bp->b_error ? bp->b_error : EIO) : 0;
	write = (bp->b_flags & B_READ) == 0;

	uobj = NULL;
	for (i = 0; i < npages; i++)
		pgs[i] = uvm_atopg((vaddr_t)bp->b_data +
		    ((vsize_t)i << PAGE_SHIFT));
	uvm_pagermapout((vaddr_t)bp->b_data, npages);
#ifdef UVM_SWAP_ENCRYPT
	/*
	 * XXX - assumes that we only get ASYNC writes. used to be above.
	 */
	if (pgs[0]->pg_flags & PQ_ENCRYPT) {
		uvm_swap_freepages(pgs, npages);
		goto freed;
	}
#endif /* UVM_SWAP_ENCRYPT */
	for (i = 0; i < npages; i++) {
		pg = pgs[i];

		if (i == 0) {
			swap = (pg->pg_flags & PQ_SWAPBACKED) != 0;
			if (!swap) {
				uobj = pg->uobject;
			}
		}
		KASSERT(swap || pg->uobject == uobj);

		/*
		 * if this is a read and we got an error, mark the pages
		 * PG_RELEASED so that uvm_page_unbusy() will free them.
		 */
		if (!write && error) {
			atomic_setbits_int(&pg->pg_flags, PG_RELEASED);
			continue;
		}
		KASSERT(!write || (pgs[i]->pg_flags & PG_FAKE) == 0);

		/*
		 * if this is a read and the page is PG_FAKE,
		 * or this was a successful write,
		 * mark the page PG_CLEAN and not PG_FAKE.
		 */
		if ((pgs[i]->pg_flags & PG_FAKE) || (write && error != ENOMEM)) {
			pmap_clear_reference(pgs[i]);
			pmap_clear_modify(pgs[i]);
			atomic_setbits_int(&pgs[i]->pg_flags, PG_CLEAN);
			atomic_clearbits_int(&pgs[i]->pg_flags, PG_FAKE);
		}
	}
	uvm_page_unbusy(pgs, npages);

#ifdef UVM_SWAP_ENCRYPT
freed:
#endif
	pool_put(&bufpool, bp);
}
@


1.70
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.69 2014/09/14 14:17:27 jsg Exp $	*/
a38 1
#include <sys/vnode.h>
d40 1
@


1.69
log
@remove uneeded proc.h includes
ok mpi@@ kspillner@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.68 2014/07/11 16:35:40 jsg Exp $	*/
d236 1
a236 1
	prot = VM_PROT_READ;
d238 1
a238 1
		prot |= VM_PROT_WRITE;
d398 1
a398 1
			pmap_page_protect(pclust, VM_PROT_READ);
@


1.68
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.67 2014/05/09 03:54:28 tedu Exp $	*/
a36 1
#include <sys/proc.h>
@


1.67
log
@stop using B_AGE, it was effectively retired some time ago.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.66 2014/05/08 20:08:50 kettenis Exp $	*/
a4 1
 *
a15 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed by Charles D. Cranor and
 *      Washington University.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
@


1.66
log
@Fix some potential integer overflows caused by converting a page number into
an offset/size/address by shifting by PAGE_SHIFT.  Make uvm_objwrire/unwire
use voff_t instead of off_t.  The former is the right type here even if it is
equivalent to the latter.

Inspired by a somewhat similar changes in Bitrig.

ok deraadt@@, guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.65 2014/04/13 23:14:15 tedu Exp $	*/
a776 3
	if (write && (bp->b_flags & B_AGE) != 0 && bp->b_vp != NULL) {
		vwakeup(bp->b_vp);
	}
@


1.65
log
@compress code by turning four line comments into one line comments.
emphatic ok usual suspects, grudging ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.64 2013/11/02 00:08:17 krw Exp $	*/
d281 1
a281 1
	pmap_remove(pmap_kernel(), kva, kva + (npages << PAGE_SHIFT));
d727 2
a728 1
		pgs[i] = uvm_atopg((vaddr_t)bp->b_data + (i << PAGE_SHIFT));
@


1.64
log
@No need to cast constants or simple variables to (daddr_t). Use
(u_int64_t) instead of (daddr_t) when casting a variable in an
expression passed to DL_SETDSIZE().

Change a variable counting open files from daddr_t to int64_t.

ok deraadt@@ with the tweak to fix that pesky expression.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.63 2013/06/11 16:42:19 deraadt Exp $	*/
a96 1

d102 1
a102 4
	/*
	 * init pager map
	 */

d106 1
a106 4
	/*
	 * init ASYNC I/O queue
	 */

d109 1
a109 3
	/*
	 * call pager init functions
	 */
d326 1
a326 5

	/*
	 * get cluster boundaries, check sanity, and apply our limits as well.
	 */

d340 1
a340 5
	/*
	 * now determine the center and attempt to cluster around the
	 * edges
	 */

a354 1

a420 1

a456 1

a470 1

a471 1

a475 1

a485 1

a486 1

a505 1

a525 1

d528 1
a528 3
			/*
			 * drop cluster
			 */
a540 1

a549 1

a561 1

a565 1

a580 1

a585 1

d623 1
a623 4
	/*
	 * drop all pages but "pg"
	 */

a624 1

a709 1

a763 1

@


1.63
log
@final removal of daddr64_t.  daddr_t has been 64 bit for a long enough
test period; i think 3 years ago the last bugs fell out.
ok otto beck others
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.62 2013/05/30 16:29:46 tedu Exp $	*/
d520 1
a520 1
		swblk = (daddr_t) start;
@


1.62
log
@remove lots of comments about locking per beck's request
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.61 2013/05/30 15:17:59 tedu Exp $	*/
d465 1
a465 1
 *		  if (!uobj) start is the (daddr64_t) of the starting swapblk
d483 1
a483 1
	daddr64_t swblk;
d520 1
a520 1
		swblk = (daddr64_t) start;
d536 1
a536 1
		/* XXX daddr64_t -> int */
d582 1
a582 1
			/* XXX daddr64_t -> int */
d601 1
a601 1
					/* XXX daddr64_t -> int */
d604 1
a604 1
					/* XXX daddr64_t -> int */
d621 1
a621 1
			/* XXX daddr64_t -> int */
@


1.61
log
@remove simple_locks from uvm code. ok beck deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.60 2011/07/03 18:34:14 oga Exp $	*/
d451 1
a451 1
 *	backing it.   this object should be locked by the caller.
d469 1
a469 8
 *	3. if (uobj != NULL) [!swap_backed] we return with
 *		uobj locked _only_ if PGO_PDFREECLUST is set 
 *		AND result != VM_PAGER_PEND.   in all other cases
 *		we return with uobj unlocked.   [this is a hack
 *		that allows the pagedaemon to save one lock/unlock
 *		pair in the !swap_backed case since we have to
 *		lock the uobj to drop the cluster anyway]
 *	4. on errors we always drop the cluster.   thus, if we return
d472 1
a472 1
 *	5. on success, if !PGO_PDFREECLUST, we return the cluster
a533 1
		/* object is locked */
a534 1
		/* object is now unlocked */
a535 1
		/* nothing locked */
a537 1
		/* nothing locked */
d556 1
a556 2
			 * drop cluster and relock object (only if I/O is
			 * not pending)
a560 2
			/* if (uobj): object still locked, as per
			 * return-state item #3 */
d640 1
a640 3
 * => uobj, if non-null, is a non-swap-backed object that is 
 *	locked by the caller.   we return with this object still
 *	locked.
d669 1
a669 2
		 * if swap-backed, gain lock on object that owns page.  note
		 * that PQ_ANON bit can't change as long as we are holding
a671 3
		 *
		 * once we have the lock, dispose of the pointer to swap, if
		 * requested
a686 1
			/* still holding obj lock */
d739 1
a739 1
	mtx_enter(&uvm.aiodoned_lock);	/* locks uvm.aio_done */
@


1.60
log
@Rip out and burn support for UVM_HIST.

The vm hackers don't use it, don't maintain it and have to look at it all the
time. About time this 800 lines of code hit /dev/null.

``never liked it'' tedu@@. ariane@@ was very happy when i told her i wrote
this diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.59 2011/04/15 22:00:46 oga Exp $	*/
a569 3
			if (uobj)
				/* required for dropcluster */
				simple_lock(&uobj->vmobjlock);
a585 3
		if (uobj) {
			simple_lock(&uobj->vmobjlock);
		}
a598 1
				simple_lock(&pg->uanon->an_lock);
a599 1
				simple_unlock(&pg->uanon->an_lock);
a600 1
				simple_lock(&pg->uobject->vmobjlock);
a603 1
				simple_unlock(&pg->uobject->vmobjlock);
a645 2
	if (uobj && (flags & PGO_PDFREECLUST) != 0)
		simple_lock(&uobj->vmobjlock);
a694 1
				simple_lock(&ppsp[lcv]->uanon->an_lock);
a698 1
				simple_lock(&ppsp[lcv]->uobject->vmobjlock);
a718 1
				simple_unlock(&ppsp[lcv]->uanon->an_lock);
a741 8

		/* if anonymous cluster, unlock object and move on */
		if (!uobj) {
			if (ppsp[lcv]->pg_flags & PQ_ANON)
				simple_unlock(&ppsp[lcv]->uanon->an_lock);
			else
				simple_unlock(&ppsp[lcv]->uobject->vmobjlock);
		}
a805 1
				simple_lock(&uobj->vmobjlock);
a808 7
		if (swap) {
			if (pg->pg_flags & PQ_ANON) {
				simple_lock(&pg->uanon->an_lock);
			} else {
				simple_lock(&pg->uobject->vmobjlock);
			}
		}
a831 7
		if (swap) {
			if (pg->pg_flags & PQ_ANON) {
				simple_unlock(&pg->uanon->an_lock);
			} else {
				simple_unlock(&pg->uobject->vmobjlock);
			}
		}
a833 3
	if (!swap) {
		simple_unlock(&uobj->vmobjlock);
	}
@


1.59
log
@for uvm_pager_dropcluster in the PG_RELEASED case we specifically unbusy the
page so that um_anfree will free it for us.

uvm_anfree does a pmap_page_protect(, VM_PROT_NONE) just before it frees the
page, so we don't need to do it here ourselves.

ok ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.58 2011/04/15 21:47:24 oga Exp $	*/
a252 5
	UVMHIST_FUNC("uvm_pagermapin"); UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist,"(pps=%p, npages=%ld, flags=%d)",
	    pps, npages, flags,0);

d261 1
a261 2
	if (kva == 0) {
		UVMHIST_LOG(maphist,"<- NOWAIT failed", 0,0,0,0);
a262 1
	}
a273 1
			UVMHIST_LOG(maphist,"<- pmap_enter failed", 0,0,0,0);
a277 1
	UVMHIST_LOG(maphist, "<- done (KVA=0x%lx)", kva,0,0,0);
a288 3
	UVMHIST_FUNC("uvm_pagermapout"); UVMHIST_CALLED(maphist);

	UVMHIST_LOG(maphist, " (kva=0x%lx, npages=%ld)", kva, npages,0,0);
a293 1
	UVMHIST_LOG(maphist,"<- done",0,0,0,0);
a326 1
	UVMHIST_FUNC("uvm_mk_pcluster"); UVMHIST_CALLED(maphist);
a439 1
	UVMHIST_LOG(maphist, "<- done",0,0,0,0);
a491 1
	UVMHIST_FUNC("uvm_pager_put"); UVMHIST_CALLED(pdhist);
a542 1
		UVMHIST_LOG(pdhist, "put -> %ld", result, 0,0,0);
a801 2
	UVMHIST_FUNC("uvm_aio_aiodone"); UVMHIST_CALLED(pdhist);
	UVMHIST_LOG(pdhist, "bp %p", bp, 0,0,0);
d810 1
a810 1
	for (i = 0; i < npages; i++) {
a811 2
		UVMHIST_LOG(pdhist, "pgs[%ld] = %p", i, pgs[i],0,0);
	}
@


1.58
log
@move uvm_pageratop from uvm_pager.c local to a general uvm function
(uvm_atopg) and use it in uvm_km_doputpage to replace some handrolled
code. Shrinks the kernel a trivial amount.

ok beck@@ and miod@@ (who suggested i name it uvm_atopg not uvm_atop)
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.57 2010/07/24 15:40:39 kettenis Exp $	*/
a749 1
				pmap_page_protect(ppsp[lcv], VM_PROT_NONE);
@


1.57
log
@Don't sleep while holding the uvm_psel_lck mutex.  Should fix "locking against
myself" panics that some people have seen over the last year-and-a-half.

Cherry picked from a more complex (and therefore scarier) diff from oga@@.

ok tedu@@, oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.56 2010/06/27 20:53:31 oga Exp $	*/
a93 2
struct vm_page	*uvm_pageratop(vaddr_t);

d831 1
a831 1
		pgs[i] = uvm_pageratop((vaddr_t)bp->b_data + (i << PAGE_SHIFT));
a905 18

/*
 * uvm_pageratop: convert KVAs in the pager map back to their page
 * structures.
 */
struct vm_page *
uvm_pageratop(vaddr_t kva)
{
	struct vm_page *pg;
	paddr_t pa;
	boolean_t rv;
 
	rv = pmap_extract(pmap_kernel(), kva, &pa);
	KASSERT(rv);
	pg = PHYS_TO_VM_PAGE(pa);
	KASSERT(pg != NULL);
	return (pg);
} 
@


1.56
log
@Kill another #ifdef UBC chunk that was annoying me while doing something
else.

ok thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.55 2010/02/12 01:35:14 tedu Exp $	*/
a90 1
void		uvm_pseg_destroy(struct uvm_pseg *);
a143 19
 * Destroy a uvm_pseg.
 *
 * Never fails.
 *
 * Requires that seg != &psegs[0]
 *
 * Caller locks uvm_pseg_lck.
 */
void
uvm_pseg_destroy(struct uvm_pseg *pseg)
{
	KASSERT(pseg != &psegs[0]);
	KASSERT(pseg->start != 0);
	KASSERT(pseg->use == 0);
	uvm_km_free(kernel_map, pseg->start, MAX_PAGER_SEGS * MAXBSIZE);
	pseg->start = 0;
}

/*
d208 1
d230 4
a233 2
	if (pseg != &psegs[0] && UVM_PSEG_EMPTY(pseg))
		uvm_pseg_destroy(pseg);
d236 3
@


1.55
log
@introduce a uvm_km_valloc_try function that won't get a lower level lock
for use by the uvm pseg code.  this is the path of least resistance until
we sort out how many of these functions we really need.  problem found by mikeb
ok kettenis oga
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.54 2009/07/22 21:05:37 oga Exp $	*/
a843 6
#ifdef UBC
	/* XXXUBC B_NOCACHE is for swap pager, should be done differently */
	if (write && !(bp->b_flags & B_NOCACHE) && bioops.io_pageiodone) {
		(*bioops.io_pageiodone)(bp);
	}
#endif
@


1.54
log
@Put the PG_RELEASED changes diff back in.

This has has been tested very very thoroughly on all archs we have
excepting 88k and 68k. Please see cvs log for the individual commit
messages.

ok beck@@, thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.53 2009/06/17 00:13:59 oga Exp $	*/
d141 1
a141 1
	pseg->start = uvm_km_valloc(kernel_map, MAX_PAGER_SEGS * MAXBSIZE);
@


1.53
log
@date based reversion of uvm to the 4th May.

More backouts in line with previous ones, this appears to bring us back to a
stable condition.

A machine forced to 64mb of ram cycled 10GB through swap with this diff
and is still running as I type this. Other tests by ariane@@ and thib@@
also seem to show that it's alright.

ok deraadt@@, thib@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.49 2009/04/06 12:02:52 oga Exp $	*/
d342 2
d445 8
d494 1
a717 2
	boolean_t obj_is_alive; 
	struct uvm_object *saved_uobj;
d759 2
a760 3
		if (ppsp[lcv]->pg_flags & PG_RELEASED) {

			if (ppsp[lcv]->pg_flags & PQ_ANON) {
d772 1
a772 18
			}

			/*
			 * pgo_releasepg will dump the page for us
			 */

			saved_uobj = ppsp[lcv]->uobject;
			obj_is_alive =
			    saved_uobj->pgops->pgo_releasepg(ppsp[lcv], NULL);
			
			/* for normal objects, "pg" is still PG_BUSY by us,
			 * so obj can't die */
			KASSERT(!uobj || obj_is_alive);

			/* only unlock the object if it is still alive...  */
			if (obj_is_alive && saved_uobj != uobj)
				simple_unlock(&saved_uobj->vmobjlock);

d774 2
a775 3
			 * XXXCDC: suppose uobj died in the pgo_releasepg?
			 * how pass that
			 * info up to caller.  we are currently ignoring it...
a776 3

			continue;		/* next page */
		} else {
d778 1
a778 1
			    PG_BUSY|PG_WANTED|PG_FAKE);
a800 27

#ifdef UBC
/*
 * interrupt-context iodone handler for nested i/o bufs.
 *
 * => must be at splbio().
 */

void
uvm_aio_biodone1(struct buf *bp)
{
	struct buf *mbp = bp->b_private;

	splassert(IPL_BIO);

	KASSERT(mbp != bp);
	if (bp->b_flags & B_ERROR) {
		mbp->b_flags |= B_ERROR;
		mbp->b_error = bp->b_error;
	}
	mbp->b_resid -= bp->b_bcount;
	pool_put(&bufpool, bp);
	if (mbp->b_resid == 0) {
		biodone(mbp);
	}
}
#endif
@


1.52
log
@Backout all the PG_RELEASED changes.

This is for the same reason as the earlier backouts, to avoid the bug
either added or exposed sometime around c2k9. This *should* be the last
one.

prompted by deraadt@@

ok ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.50 2009/05/04 18:08:06 oga Exp $	*/
d859 1
a859 1
	wakeup(&uvm.aiodoned_proc);
@


1.51
log
@More PG_RELEASED cleaning.

similar to the aobj.c changes, this one does vnodes. Vnodes are more
complex because they actaully have to sync to backing store. So firstly,
convert sync to sleep instead of setting released.

Now, for  backing store, in the PGO_FREE case, if we set PG_RELEASED
*before* an async io, (nothing else will see it, the page is busy), then
we can ignore the page after the io is done. We could do something
similar for PGO_DEACTIVATE too, but that is another change. On error we
just clear the released flag, nothing else sets it for uobj pages other
than aiodoned.

ok thib@@, beck@@, ariane@@
@
text
@a341 2
 *	PGO_FREE: set the PG_RELEASED bits on the cluster so they'll be freed
 *		in async io (caller must clean on error).
a442 8
			/*
			 * If we want to free after io is done, and we're
			 * async, set the released flag
			 */
			if ((flags & (PGO_FREE|PGO_SYNCIO)) == PGO_FREE)
				atomic_setbits_int(&pclust->pg_flags,
				    PG_RELEASED);

a483 1
 *	PGO_FREE: tell the aio daemon to free pages in the async case.
d707 2
d750 3
a752 2
		if (ppsp[lcv]->pg_flags & PG_RELEASED &&
		    ppsp[lcv]->pg_flags & PQ_ANON) {
d764 18
a781 1
		} else {
d783 3
a785 2
			 * if we were planning on async io then we would
			 * have PG_RELEASED set, clear that with the others.
d787 3
d791 1
a791 1
			    PG_BUSY|PG_WANTED|PG_FAKE|PG_RELEASED);
d814 27
@


1.50
log
@Instead of keeping two ints in the uvm structure specifically just to
sleep on them (and otherwise ignore them) sleep on the pointer to the
{aiodoned,pagedaemon}_proc members, and nuke the two extra words.

"no objections" art@@, ok beck@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.49 2009/04/06 12:02:52 oga Exp $	*/
d342 2
d445 8
d494 1
a717 2
	boolean_t obj_is_alive; 
	struct uvm_object *saved_uobj;
d759 2
a760 3
		if (ppsp[lcv]->pg_flags & PG_RELEASED) {

			if (ppsp[lcv]->pg_flags & PQ_ANON) {
d772 1
a772 18
			}

			/*
			 * pgo_releasepg will dump the page for us
			 */

			saved_uobj = ppsp[lcv]->uobject;
			obj_is_alive =
			    saved_uobj->pgops->pgo_releasepg(ppsp[lcv], NULL);
			
			/* for normal objects, "pg" is still PG_BUSY by us,
			 * so obj can't die */
			KASSERT(!uobj || obj_is_alive);

			/* only unlock the object if it is still alive...  */
			if (obj_is_alive && saved_uobj != uobj)
				simple_unlock(&saved_uobj->vmobjlock);

d774 2
a775 3
			 * XXXCDC: suppose uobj died in the pgo_releasepg?
			 * how pass that
			 * info up to caller.  we are currently ignoring it...
a776 3

			continue;		/* next page */
		} else {
d778 1
a778 1
			    PG_BUSY|PG_WANTED|PG_FAKE);
a800 27

#ifdef UBC
/*
 * interrupt-context iodone handler for nested i/o bufs.
 *
 * => must be at splbio().
 */

void
uvm_aio_biodone1(struct buf *bp)
{
	struct buf *mbp = bp->b_private;

	splassert(IPL_BIO);

	KASSERT(mbp != bp);
	if (bp->b_flags & B_ERROR) {
		mbp->b_flags |= B_ERROR;
		mbp->b_error = bp->b_error;
	}
	mbp->b_resid -= bp->b_bcount;
	pool_put(&bufpool, bp);
	if (mbp->b_resid == 0) {
		biodone(mbp);
	}
}
#endif
@


1.49
log
@Instead of doing splbio(); simple_lock(&uvm.aiodoned_lock); just replace
the simple lock with a real lock - a IPL_BIO mutex. While i'm here, make
the sleeping condition one hell of a lot simpler in the aio daemon.

some ideas from and ok art@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.48 2009/03/25 20:00:18 oga Exp $	*/
d859 1
a859 1
	wakeup(&uvm.aiodoned);
@


1.48
log
@Move all of the pseudo-inline functions in uvm into C files.

By pseudo-inline, I mean that if a certain macro was defined, they would
be inlined. However, no architecture defines that, and none has for a
very very long time. Therefore mainly this just makes the code a damned
sight easier to read. Some k&r -> ansi declarations while I'm in there.

"just commit it" art@@. ok weingart@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.47 2009/03/20 15:19:04 oga Exp $	*/
d857 1
a857 1
	simple_lock(&uvm.aiodoned_lock);	/* locks uvm.aio_done */
d860 1
a860 1
	simple_unlock(&uvm.aiodoned_lock);
@


1.47
log
@While working on some stuff in uvm I've gotten REALLY sick of reading
K&R function declarations, so switch them all over to ansi-style, in
accordance with the prophesy.

"go for it" art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.46 2009/01/27 19:21:03 ariane Exp $	*/
a41 1
#define UVM_PAGER
d90 6
a95 4
void	uvm_pseg_init(struct uvm_pseg *);
void	uvm_pseg_destroy(struct uvm_pseg *);
vaddr_t	uvm_pseg_get(int);
void	uvm_pseg_release(vaddr_t);
d348 2
a349 5
uvm_mk_pcluster(uobj, pps, npages, center, flags, mlo, mhi)
	struct uvm_object *uobj;	/* IN */
	struct vm_page **pps, *center;  /* IN/OUT, IN */
	int *npages, flags;		/* IN/OUT, IN */
	voff_t mlo, mhi;		/* IN (if !PGO_ALLPAGES) */
d505 3
a507 6
uvm_pager_put(uobj, pg, ppsp_ptr, npages, flags, start, stop)
	struct uvm_object *uobj;	/* IN */
	struct vm_page *pg, ***ppsp_ptr;/* IN, IN/OUT */
	int *npages;			/* IN/OUT */
	int flags;			/* IN */
	voff_t start, stop;		/* IN, IN */
d703 2
a704 5
uvm_pager_dropcluster(uobj, pg, ppsp, npages, flags)
	struct uvm_object *uobj;	/* IN */
	struct vm_page *pg, **ppsp;	/* IN, IN/OUT */
	int *npages;			/* IN/OUT */
	int flags;
d823 1
a823 2
uvm_aio_biodone1(bp)
	struct buf *bp;
d850 1
a850 2
uvm_aio_biodone(bp)
	struct buf *bp;
d869 1
a869 2
uvm_aio_aiodone(bp)
	struct buf *bp;
d968 18
@


1.46
log
@Simplify page-out/page-in map management; fix rare pager deadlock.

Ok: miod, tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.45 2008/11/24 19:55:33 thib Exp $	*/
d101 1
a101 1
uvm_pager_init()
@


1.45
log
@garbage collect uvm_errno2vmerror();

ok miod@@, art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.44 2007/11/26 22:49:08 miod Exp $	*/
d63 12
d76 10
d87 8
a94 5
vm_map_t pager_map;		/* XXX */
simple_lock_data_t pager_map_wanted_lock;
boolean_t pager_map_wanted;	/* locked by pager map */
static vaddr_t emergva;
static boolean_t emerginuse;
d109 2
a110 6
	pager_map = uvm_km_suballoc(kernel_map, &uvm.pager_sva, &uvm.pager_eva,
	 			    PAGER_MAP_SIZE, 0, FALSE, NULL);
	simple_lock_init(&pager_map_wanted_lock);
	pager_map_wanted = FALSE;
	emergva = uvm_km_valloc(kernel_map, MAXBSIZE);
	emerginuse = FALSE;
d115 1
a115 1
	
d129 88
a216 1
 * uvm_pagermapin: map pages into KVA (pager_map) for I/O that needs mappings
d218 3
a220 2
 * we basically just map in a blank map entry to reserve the space in the
 * map and then use pmap_enter() to put the mappings in by hand.
d222 18
d241 19
d261 1
a261 4
uvm_pagermapin(pps, npages, flags)
	struct vm_page **pps;
	int npages;
	int flags;
d263 2
a265 2
	vaddr_t kva;
	vaddr_t cva;
d267 1
a267 1
	vm_prot_t prot;
d270 2
a271 6
	UVMHIST_LOG(maphist,"(pps=%p, npages=%ld)", pps, npages,0,0);

	/*
	 * compute protection.  outgoing I/O only needs read
	 * access to the page, whereas incoming needs read/write.
	 */
d276 3
d280 4
a283 30
ReStart:
	size = npages << PAGE_SHIFT;
	kva = 0;			/* let system choose VA */

	if (uvm_map(pager_map, &kva, size, NULL, 
	      UVM_UNKNOWN_OFFSET, 0, UVM_FLAG_NOMERGE) != 0) {
		if (curproc == uvm.pagedaemon_proc) {
			simple_lock(&pager_map_wanted_lock);
			if (emerginuse) {
				UVM_UNLOCK_AND_WAIT(&emergva,
				    &pager_map_wanted_lock, FALSE,
				    "emergva", 0);
				goto ReStart;
			}
			emerginuse = TRUE;
			simple_unlock(&pager_map_wanted_lock);
			kva = emergva;
			KASSERT(npages <= MAXBSIZE >> PAGE_SHIFT);
			goto enter;
		}
		if ((flags & UVMPAGER_MAPIN_WAITOK) == 0) {
			UVMHIST_LOG(maphist,"<- NOWAIT failed", 0,0,0,0);
			return(0);
		}
		simple_lock(&pager_map_wanted_lock);
		pager_map_wanted = TRUE; 
		UVMHIST_LOG(maphist, "  SLEEPING on pager_map",0,0,0,0);
		UVM_UNLOCK_AND_WAIT(pager_map, &pager_map_wanted_lock, FALSE, 
		    "pager_map", 0);
		goto ReStart;
a285 2
enter:
	/* got it */
d290 9
a298 2
		pmap_enter(vm_map_pmap(pager_map), cva, VM_PAGE_TO_PHYS(pp),
		    prot, PMAP_WIRED | prot);
d300 1
a300 2
	pmap_update(vm_map_pmap(pager_map));

d302 1
a302 1
	return(kva);
d306 1
a306 1
 * uvm_pagermapout: remove pager_map mapping
d308 1
a308 2
 * we remove our mappings by hand and then remove the mapping (waking
 * up anyone wanting space).
a309 1

d311 1
a311 3
uvm_pagermapout(kva, npages)
	vaddr_t kva;
	int npages;
a312 2
	vsize_t size = npages << PAGE_SHIFT;
	vm_map_entry_t entries;
a316 23
	/*
	 * duplicate uvm_unmap, but add in pager_map_wanted handling.
	 */

	if (kva == emergva) {
		simple_lock(&pager_map_wanted_lock);
		emerginuse = FALSE;
		wakeup(&emergva);
		simple_unlock(&pager_map_wanted_lock);
		entries = NULL;
		goto remove;
	}

	vm_map_lock(pager_map);
	uvm_unmap_remove(pager_map, kva, kva + size, &entries, NULL);
	simple_lock(&pager_map_wanted_lock);
	if (pager_map_wanted) {
		pager_map_wanted = FALSE;
		wakeup(pager_map);
	}
	simple_unlock(&pager_map_wanted_lock);
	vm_map_unlock(pager_map);
remove:
d318 2
a319 2
	if (entries)
		uvm_unmap_detach(entries, 0);
a320 1
	pmap_update(pmap_kernel());
@


1.44
log
@In uvm_aio_aiodone(), kill the variable-sized array on stack, and use a
fixed size array which size should match any buf; if a bogus buf is passed
to this function, the kernel will KASSERT instead of potentially running out
of stack and having an undefined behaviour.

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.43 2007/06/06 17:15:14 deraadt Exp $	*/
a887 26
}

/*
 * translate unix errno values to VM_PAGER_*.
 */

int
uvm_errno2vmerror(errno)
	int errno;
{
	switch (errno) {
	case 0:
		return VM_PAGER_OK;
	case EINVAL:
		return VM_PAGER_BAD;
	case EINPROGRESS:
		return VM_PAGER_PEND;
	case EIO:
		return VM_PAGER_ERROR;
	case EAGAIN:
		return VM_PAGER_AGAIN;
	case EBUSY:
		return VM_PAGER_UNLOCK;
	default:
		return VM_PAGER_ERROR;
	}
@


1.43
log
@now that all partition size/offsets are potentially 64-bit, change the
type of all variables to daddr64_t.  this includes the APIs for XXsize()
and XXdump(), all range checks inside bio drivers, internal variables
for disklabel handling, and even uvm's swap offsets.  re-read numerous
times by otto, miod, krw, thib to look for errors
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.42 2007/04/13 18:57:49 art Exp $	*/
d793 1
a793 1
	struct vm_page *pg, *pgs[npages];
d800 1
@


1.42
log
@While splitting flags and pqflags might have been a good idea in theory
to separate locking, on most modern machines this is not enough
since operations on short types touch other short types that share the
same word in memory.

Merge pg_flags and pqflags again and now use atomic operations to change
the flags. Also bump wire_count to an int and pg_version might go
int as well, just for alignment.

tested by many, many. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.41 2007/04/04 17:44:45 art Exp $	*/
d397 1
a397 1
 *		  if (!uobj) start is the (daddr_t) of the starting swapblk
d425 1
a425 1
	daddr_t swblk;
d463 1
a463 1
		swblk = (daddr_t) start;
d483 1
d539 1
d562 1
d565 1
d582 1
@


1.41
log
@Mechanically rename the "flags" and "version" fields in struct vm_page
to "pg_flags" and "pg_version", so that they are a bit easier to work with.
Whoever uses generic names like this for a popular struct obviously doesn't
read much code.

Most architectures compile and there are no functionality changes.

deraadt@@ ok ("if something fails to compile, we fix that by hand")
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.40 2007/03/25 11:31:07 art Exp $	*/
d326 1
a326 1
			if ((pclust->pqflags & PQ_INACTIVE) == 0) {
d337 3
a339 1
						pclust->pg_flags &= ~PG_CLEAN;
d341 2
a342 1
					pclust->pg_flags |= PG_CLEANCHK;
d352 1
a352 1
			pclust->pg_flags |= PG_BUSY;		/* busy! */
d539 1
a539 1
			if (pg->pqflags & PQ_ANON) {
d643 1
a643 1
			if (ppsp[lcv]->pqflags & PQ_ANON) {
d665 1
a665 1
			if (ppsp[lcv]->pqflags & PQ_ANON) {
d667 2
a668 1
				ppsp[lcv]->pg_flags &= ~(PG_BUSY);
a701 1

d703 2
a704 1
			ppsp[lcv]->pg_flags &= ~(PG_BUSY|PG_WANTED|PG_FAKE);
d715 1
a715 1
			ppsp[lcv]->pg_flags |= PG_CLEAN;
d720 1
a720 1
			if (ppsp[lcv]->pqflags & PQ_ANON)
d816 1
a816 1
	if (pgs[0]->pqflags & PQ_ENCRYPT) {
d825 1
a825 1
			swap = (pg->pqflags & PQ_SWAPBACKED) != 0;
d833 1
a833 1
			if (pg->pqflags & PQ_ANON) {
a843 1

d845 1
a845 1
			pg->pg_flags |= PG_RELEASED;
d859 2
a860 2
			pgs[i]->pg_flags |= PG_CLEAN;
			pgs[i]->pg_flags &= ~PG_FAKE;
d863 1
a863 1
			if (pg->pqflags & PQ_ANON) {
@


1.40
log
@remove KERN_SUCCESS and use 0 instead.
eyeballed by miod@@ and pedro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.39 2006/07/31 11:51:29 mickey Exp $	*/
d175 1
a175 1
		KASSERT(pp->flags & PG_BUSY);
d333 2
a334 2
				if ((pclust->flags & PG_CLEANCHK) == 0) {
					if ((pclust->flags & (PG_CLEAN|PG_BUSY))
d337 1
a337 1
						pclust->flags &= ~PG_CLEAN;
d339 1
a339 1
					pclust->flags |= PG_CLEANCHK;
d344 1
a344 1
			if ((pclust->flags & (PG_CLEAN|PG_BUSY)) != 0) {
d349 1
a349 1
			pclust->flags |= PG_BUSY;		/* busy! */
d654 1
a654 1
		if (ppsp[lcv]->flags & PG_WANTED) {
d660 1
a660 1
		if (ppsp[lcv]->flags & PG_RELEASED) {
d664 1
a664 1
				ppsp[lcv]->flags &= ~(PG_BUSY);
d700 1
a700 1
			ppsp[lcv]->flags &= ~(PG_BUSY|PG_WANTED|PG_FAKE);
d711 1
a711 1
			ppsp[lcv]->flags |= PG_CLEAN;
d842 1
a842 1
			pg->flags |= PG_RELEASED;
d845 1
a845 1
		KASSERT(!write || (pgs[i]->flags & PG_FAKE) == 0);
d853 1
a853 1
		if ((pgs[i]->flags & PG_FAKE) || (write && error != ENOMEM)) {
d856 2
a857 2
			pgs[i]->flags |= PG_CLEAN;
			pgs[i]->flags &= ~PG_FAKE;
@


1.39
log
@fix uvmhist #2: args are always u_long so fix missing %d and %x and no %ll; no change for normal code
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.38 2006/07/26 23:15:55 mickey Exp $	*/
d143 1
a143 1
	      UVM_UNKNOWN_OFFSET, 0, UVM_FLAG_NOMERGE) != KERN_SUCCESS) {
@


1.38
log
@fix fmts for UVMHIST_LOG() entries making it more useful on 64bit archs; miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.37 2005/07/26 07:11:55 art Exp $	*/
d127 1
a127 1
	UVMHIST_LOG(maphist,"(pps=%p, npages=%d)", pps, npages,0,0);
d201 1
a201 1
	UVMHIST_LOG(maphist, " (kva=0x%lx, npages=%d)", kva, npages,0,0);
d476 1
a476 1
		UVMHIST_LOG(pdhist, "put -> %d", result, 0,0,0);
d805 1
a805 1
		UVMHIST_LOG(pdhist, "pgs[%d] = %p", i, pgs[i],0,0);
@


1.37
log
@ - Make a UVM_OBJ_IS_DEVICE macro.
 - Use it to skip device mappings while dumping core.
 - Ignore EFAULT errors while dumping core since they can happen
   even for valid mappings. Just skip that part of the core file and
   let it get automagically zero-filled.

This fixes the broken X core dumps that people have been seeing and also
fixes some other potential problems that could prevent core dumps (mmaps
beyond EOF, etc.).

tedu@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.36 2005/05/24 21:11:47 tedu Exp $	*/
d127 1
a127 1
	UVMHIST_LOG(maphist,"(pps=0x%x, npages=%d)", pps, npages,0,0);
d181 1
a181 1
	UVMHIST_LOG(maphist, "<- done (KVA=0x%x)", kva,0,0,0);
d201 1
a201 1
	UVMHIST_LOG(maphist, " (kva=0x%x, npages=%d)", kva, npages,0,0);
@


1.36
log
@add a new field to vm_space and use it to track the number of anon
pages a process uses.  this is now the userland "data size" value.
ok art deraadt tdeval.  thanks testers.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.35 2004/02/23 06:19:32 drahn Exp $	*/
a54 10
/*
 * list of uvm pagers in the system
 */

extern struct uvm_pagerops uvm_deviceops;
extern struct uvm_pagerops uvm_vnodeops;
#ifdef UBC
extern struct uvm_pagerops ubc_pager;
#endif

a58 3
#ifdef UBC
	&ubc_pager,
#endif
@


1.35
log
@sync of pmap_update() calls with NetBSD. pmap_update is defined away on
all architectures but arm, where it is needed.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.34 2003/03/29 01:13:57 mickey Exp $	*/
d230 1
a230 1
	uvm_unmap_remove(pager_map, kva, kva + size, &entries);
@


1.34
log
@ubchist is not a fully cooked kadaver and though use the other well formed pdhist one until ubc gaets back. art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.33 2002/10/29 18:30:21 art Exp $	*/
d192 1
d243 1
@


1.33
log
@Since memory deallocation can't fail, remove the error return from
uvm_unmap, uvm_deallocate and a few other functions.
Simplifies some code and reduces diff to the UBC branch.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.32 2002/05/24 13:10:53 art Exp $	*/
d435 1
a435 1
	UVMHIST_FUNC("uvm_pager_put"); UVMHIST_CALLED(ubchist);
d487 1
a487 1
		UVMHIST_LOG(ubchist, "put -> %d", result, 0,0,0);
d799 2
a800 2
	UVMHIST_FUNC("uvm_aio_aiodone"); UVMHIST_CALLED(ubchist);
	UVMHIST_LOG(ubchist, "bp %p", bp, 0,0,0);
d816 1
a816 1
		UVMHIST_LOG(ubchist, "pgs[%d] = %p", i, pgs[i],0,0);
@


1.32
log
@Make sure that b_iodone handlers are called at splbio (and splassert(IPL_BIO) in all known callers, just to make sure).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.31 2002/05/22 14:29:20 art Exp $	*/
d229 1
a229 1
	(void) uvm_unmap_remove(pager_map, kva, kva + size, &entries);
@


1.31
log
@splassert(IPL_BIO) in the b_iodone handlers.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.30 2002/01/02 22:23:25 miod Exp $	*/
d797 1
a797 1
	int s, i, error;
d802 2
a885 1
	s = splbio();
a889 1
	splx(s);
@


1.30
log
@Back out a few more uvm changes, especially wrt swap usage.
This unbreaks m68k m88k sparc and perhaps others, which eventually froze
when hitting swap.
Tested by various people on various platforms.
ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.21 2001/11/10 18:42:31 art Exp $	*/
d748 2
d774 2
@


1.29
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pager.c,v 1.22 2001/11/12 01:26:10 art Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.41 2001/02/18 19:26:50 chs Exp $	*/
d190 1
a190 2
		    prot, PMAP_WIRED | ((pp->flags & PG_FAKE) ? prot :
					VM_PROT_READ));
d317 5
d335 7
d343 9
a351 21
			if ((flags & PGO_DOACTCLUST) == 0) {
				/* dont want mapped pages at all */
				break;
			}

			/*
			 * get an up-to-date view of the "clean" bit.
			 * note this isn't 100% accurate, but it doesn't
			 * have to be.  if it's not quite right, the
			 * worst that happens is we don't cluster as
			 * aggressively.  we'll sync-it-for-sure before
			 * we free the page, and clean it if necessary.
			 */
			if ((pclust->flags & PG_CLEANCHK) == 0) {
				if ((pclust->flags & (PG_CLEAN|PG_BUSY))
				    == PG_CLEAN &&
				   pmap_is_modified(pclust))
					pclust->flags &= ~PG_CLEAN;

				/* now checked */
				pclust->flags |= PG_CLEANCHK;
a863 1
		uvm_pageactivate(pg);
@


1.28
log
@Yet another sync to NetBSD uvm.
Today we add a pmap argument to pmap_update() and allocate map entries for
kernel_map from kmem_map instead of using the static entries. This should
get rid of MAX_KMAPENT panics. Also some uvm_loan problems are fixed.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pager.c,v 1.27 2001/11/30 05:45:33 csapuntz Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.49 2001/09/10 21:19:43 chris Exp $	*/
d61 1
d63 1
d69 1
d71 1
d78 2
a79 2
struct vm_map *pager_map;		/* XXX */
struct simplelock pager_map_wanted_lock;
d107 1
a107 1

d155 2
a156 2
	if (uvm_map(pager_map, &kva, size, NULL,
	    UVM_UNKNOWN_OFFSET, 0, UVM_FLAG_NOMERGE) != 0) {
d176 1
a176 1
		pager_map_wanted = TRUE;
d178 1
a178 1
		UVM_UNLOCK_AND_WAIT(pager_map, &pager_map_wanted_lock, FALSE,
a192 1
	pmap_update(vm_map_pmap(pager_map));
d211 1
a211 1
	struct vm_map_entry *entries;
d230 1
a230 1
	uvm_unmap_remove(pager_map, kva, kva + size, &entries);
a237 1

d242 1
a242 1
	pmap_update(pmap_kernel());
d280 1
a280 1
	/*
d316 2
a317 2
	 * attempt to cluster around the left [backward], and then
	 * the right side [forward].
d376 1
a376 1

d403 1
a403 1
 *	PGO_SYNCIO: wait for i/o to complete
d408 1
a408 1
 *	1. we return the error code of the pageout
d411 2
a412 2
 *		uobj locked _only_ if PGO_PDFREECLUST is set
 *		AND result == 0 AND async.   in all other cases
d418 1
a418 1
 *		an error, then the caller only has to worry about
a434 1
	boolean_t async = (flags & PGO_SYNCIO) == 0;
d500 1
a500 1
	 * 	if !PGO_PDFREECLUST, we return the cluster to the
d509 2
a510 3
	if (result == 0) {
		if (flags & PGO_PDFREECLUST && !async) {

d512 2
a513 1
			 * drop cluster and relock object for sync i/o.
a514 1

d521 2
a522 2

			/* if (uobj): object still locked, as per #3 */
d540 4
a543 3
		 * for hard failures on swap-backed pageouts with a "pg"
		 * we need to clear pg's swslot since uvm_pager_dropcluster()
		 * didn't do it and we aren't going to retry.
d546 2
a547 1
		if (uobj == NULL && pg != NULL && result != EAGAIN) {
d550 1
a550 1
				pg->uanon->an_swslot = 0;
d555 2
a556 1
				    pg->offset >> PAGE_SHIFT, 0);
d560 1
a560 1
		if (result == EAGAIN) {
d596 1
a596 1

d603 1
a603 1
 * uvm_pager_dropcluster: drop a cluster we have built (because we
d607 1
a607 1
 * => uobj, if non-null, is a non-swap-backed object that is
d615 1
a615 1
 *           PGO_REALLOCSWAP: drop previously allocated swap slots for
d629 1
a629 1
	boolean_t obj_is_alive;
d641 1
a641 1

d694 1
a694 1

d717 1
a717 1
		 * if we are operating on behalf of the pagedaemon and we
d736 1
d760 1
d801 1
d806 1
d883 2
a884 4
	if (bp->b_vp != NULL) {
		if (write && (bp->b_flags & B_AGE) != 0) {
			vwakeup(bp->b_vp);
		}
a885 1
	(void) buf_cleanout(bp);
d888 26
@


1.28.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.30 2002/01/02 22:23:25 miod Exp $	*/
d186 2
a187 1
		    prot, PMAP_WIRED | prot);
a328 7
			/* handle active pages */
			/* NOTE: inactive pages don't have pmap mappings */
			if ((pclust->pqflags & PQ_INACTIVE) == 0) {
				if ((flags & PGO_DOACTCLUST) == 0) {
					/* dont want mapped pages at all */
					break;
				}
d330 21
a350 9
				/* make sure "clean" bit is sync'd */
				if ((pclust->flags & PG_CLEANCHK) == 0) {
					if ((pclust->flags & (PG_CLEAN|PG_BUSY))
					   == PG_CLEAN &&
					   pmap_is_modified(pclust))
						pclust->flags &= ~PG_CLEAN;
					/* now checked */
					pclust->flags |= PG_CLEANCHK;
				}
d858 1
@


1.28.2.2
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pager.c,v 1.28.2.1 2002/01/31 22:55:51 niklas Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.54 2001/11/10 07:37:00 lukem Exp $	*/
a87 1
	vaddr_t sva, eva;
d93 2
a94 3
	sva = 0;
	pager_map = uvm_km_suballoc(kernel_map, &sva, &eva, PAGER_MAP_SIZE, 0,
	    FALSE, NULL);
d152 1
a152 1
	      UVM_UNKNOWN_OFFSET, 0, UVM_FLAG_NOMERGE) != 0) {
d185 2
a186 1
		pmap_kenter_pa(cva, VM_PAGE_TO_PHYS(pp), prot);
a215 1
	pmap_kremove(kva, npages << PAGE_SHIFT);
d221 2
a222 1
		return;
d234 3
d244 484
d783 2
a784 3
	struct simplelock *slock;
	int s, i, error, swslot;
	boolean_t write, swap, pageout;
d810 2
d813 6
a818 15
	swslot = 0;
	slock = NULL;
	swap = (pgs[0]->pqflags & PQ_SWAPBACKED) != 0;
	pageout = (pgs[0]->flags & PG_PAGEOUT) != 0;
	if (!swap) {
		uobj = pgs[0]->uobject;
		slock = &uobj->vmobjlock;
		simple_lock(slock);
		uvm_lock_pageq();
	} else if (error) {
		pg = pgs[0];
		if (pg->pqflags & PQ_ANON) {
			swslot = pg->uanon->an_swslot;
		} else {
			swslot = uao_find_swslot(pg->uobject, pg->offset);
a819 4
		KASSERT(swslot);
	}
	for (i = 0; i < npages; i++) {
		pg = pgs[i];
a820 8
		KASSERT(pageout ^ ((pg->flags & PG_PAGEOUT) == 0));
		UVMHIST_LOG(ubchist, "pg %p", pg, 0,0,0);

		/*
		 * for swap i/os, lock each page's object (or anon)
		 * individually since each page may need a different lock.
		 */

d823 1
a823 1
				slock = &pg->uanon->an_lock;
d825 1
a825 1
				slock = &pg->uobject->vmobjlock;
a826 2
			simple_lock(slock);
			uvm_lock_pageq();
d830 2
a831 6
		 * process errors.  for reads, just mark the page to be freed.
		 * for writes, if the error was ENOMEM, we assume this was
		 * a transient failure so we mark the page dirty so that
		 * we'll try to write it again later.  for all other write
		 * errors, we assume the error is permanent, thus the data
		 * in the page is lost.  bummer.
d834 3
a836 26
		if (error) {
			if (!write) {
				pg->flags |= PG_RELEASED;
				continue;
			} else if (error == ENOMEM) {
				if (pg->flags & PG_PAGEOUT) {
					pg->flags &= ~PG_PAGEOUT;
					uvmexp.paging--;
				}
				pg->flags &= ~PG_CLEAN;
				uvm_pageactivate(pg);
			}
		}

		/*
		 * if the page is PG_FAKE, this must have been a read to
		 * initialize the page.  clear PG_FAKE and activate the page.
		 * we must also clear the pmap "modified" flag since it may
		 * still be set from the page's previous identity.
		 */

		if (pg->flags & PG_FAKE) {
			KASSERT(!write);
			pg->flags &= ~PG_FAKE;
			uvm_pageactivate(pg);
			pmap_clear_modify(pg);
d838 1
d841 3
a843 2
		 * do accounting for pagedaemon i/o and arrange to free
		 * the pages instead of just unbusying them.
d846 5
a850 4
		if (pg->flags & PG_PAGEOUT) {
			pg->flags &= ~PG_PAGEOUT;
			uvmexp.paging--;
			pg->flags |= PG_RELEASED;
a851 5

		/*
		 * for swap pages, unlock everything for this page now.
		 */

d853 5
a857 3
			uvm_page_unbusy(&pg, 1);
			uvm_unlock_pageq();
			simple_unlock(slock);
d860 1
d862 1
a862 15
		uvm_page_unbusy(pgs, npages);
		uvm_unlock_pageq();
		simple_unlock(slock);
	} else {
		KASSERT(write);
		KASSERT(pageout);

		/* these pages are now only in swap. */
		simple_lock(&uvm.swap_data_lock);
		KASSERT(uvmexp.swpgonly + npages <= uvmexp.swpginuse);
		uvmexp.swpgonly += npages;
		simple_unlock(&uvm.swap_data_lock);
		if (error) {
			uvm_swap_markbad(swslot, npages);
		}
@


1.28.2.3
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.28.2.2 2002/02/02 03:28:27 art Exp $	*/
a252 2
	splassert(IPL_BIO);

a275 2
	splassert(IPL_BIO);

d298 1
a298 1
	int i, error, swslot;
a302 2
	splassert(IPL_BIO);

d444 1
d452 1
@


1.28.2.4
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pager.c,v 1.28.2.3 2002/06/11 03:33:04 art Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.58 2002/10/01 07:52:30 chs Exp $	*/
d59 5
a63 1
struct uvm_pagerops * const uvmpagerops[] = {
d303 1
a303 1
	boolean_t write, swap;
d334 2
a335 3
	pg = pgs[0];
	swap = (pg->uanon != NULL && pg->uobject == NULL) ||
		(pg->pqflags & PQ_AOBJ) != 0;
d337 1
a337 1
		uobj = pg->uobject;
d342 3
a344 3
		if (pg->uobject != NULL) {
			swslot = uao_find_swslot(pg->uobject,
			    pg->offset >> PAGE_SHIFT);
d346 1
a346 1
			swslot = pg->uanon->an_swslot;
d353 1
d362 3
a364 1
			if (pg->uobject != NULL) {
a365 2
			} else {
				slock = &pg->uanon->an_lock;
d435 1
@


1.28.2.5
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d300 2
a301 2
	UVMHIST_FUNC("uvm_aio_aiodone"); UVMHIST_CALLED(pdhist);
	UVMHIST_LOG(pdhist, "bp %p", bp, 0,0,0);
d315 1
a315 1
		UVMHIST_LOG(pdhist, "pgs[%d] = %p", i, pgs[i],0,0);
@


1.27
log
@Call buf_cleanout, which handles wakeups
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pager.c,v 1.26 2001/11/29 01:59:19 art Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.48 2001/06/23 20:47:44 chs Exp $	*/
d189 1
a189 1
	pmap_update();
d240 1
a240 1
	pmap_update();
@


1.26
log
@Correctly handle b_vp with bgetvp and brelvp in {get,put}pages.
Prevents panics caused by vnodes being recycled under our feet.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.25 2001/11/28 19:28:15 art Exp $	*/
a879 1
		brelvp(bp);
d881 1
@


1.25
log
@Sync in more uvm from NetBSD. Mostly just cosmetic stuff.
Contains also support for page coloring.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.24 2001/11/28 13:47:40 art Exp $	*/
d876 5
a880 2
	if (write && (bp->b_flags & B_AGE) != 0 && bp->b_vp != NULL) {
		vwakeup(bp->b_vp);
@


1.24
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pager.c,v 1.23 2001/11/27 05:27:12 art Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.43 2001/03/15 06:10:58 chs Exp $	*/
d74 2
a75 2
vm_map_t pager_map;		/* XXX */
simple_lock_data_t pager_map_wanted_lock;
d103 1
a103 1
	
d151 1
a151 1
	if (uvm_map(pager_map, &kva, size, NULL, 
d172 1
a172 1
		pager_map_wanted = TRUE; 
d174 1
a174 1
		UVM_UNLOCK_AND_WAIT(pager_map, &pager_map_wanted_lock, FALSE, 
d189 1
d208 1
a208 1
	vm_map_entry_t entries;
d240 1
d278 1
a278 1
	/* 
d314 2
a315 2
	 * attempt to cluster around the left [backward], and then 
	 * the right side [forward].    
d374 1
a374 1
	
d409 1
a409 1
 *		uobj locked _only_ if PGO_PDFREECLUST is set 
d499 1
a499 1
	 * 	if !PGO_PDFREECLUST, we return the cluster to the 
d540 3
a542 4
		 * for failed swap-backed pageouts with a "pg",
		 * we need to reset pg's swslot to either:
		 * "swblk" (for transient errors, so we can retry),
		 * or 0 (for hard errors).
d545 1
a545 2
		if (uobj == NULL && pg != NULL) {
			int nswblk = (result == EAGAIN) ? swblk : 0;
d548 1
a548 1
				pg->uanon->an_swslot = nswblk;
d553 1
a553 2
					       pg->offset >> PAGE_SHIFT,
					       nswblk);
d593 1
a593 1
	
d600 1
a600 1
 * uvm_pager_dropcluster: drop a cluster we have built (because we 
d604 1
a604 1
 * => uobj, if non-null, is a non-swap-backed object that is 
d612 1
a612 1
 *           PGO_REALLOCSWAP: drop previously allocated swap slots for 
d626 1
a626 1
	boolean_t obj_is_alive; 
d638 1
a638 1
	
d691 1
a691 1
			
d714 1
a714 1
		 * if we are operating on behalf of the pagedaemon and we 
@


1.23
log
@Merge in the unified buffer cache code as found in NetBSD 2001/03/10. The
code is written mostly by Chuck Silvers <chuq@@chuq.com>/<chs@@netbsd.org>.

Tested for the past few weeks by many developers, should be in a pretty stable
state, but will require optimizations and additional cleanups.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pager.c,v 1.22 2001/11/12 01:26:10 art Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.41 2001/02/18 19:26:50 chs Exp $	*/
d152 1
a152 1
	    UVM_UNKNOWN_OFFSET, 0, UVM_FLAG_NOMERGE) != KERN_SUCCESS) {
d226 1
a226 1
	(void) uvm_unmap_remove(pager_map, kva, kva + size, &entries);
d234 1
a238 1

d399 1
a399 1
 *	PGO_SYNCIO: do SYNC I/O (no async)
d404 1
a404 1
 *	1. we return the VM_PAGER status code of the pageout
d408 1
a408 1
 *		AND result != VM_PAGER_PEND.   in all other cases
d414 1
a414 1
 *		!PEND, !OK, then the caller only has to worry about
d431 1
d506 3
a508 2
	if (result == VM_PAGER_PEND || result == VM_PAGER_OK) {
		if (result == VM_PAGER_OK && (flags & PGO_PDFREECLUST)) {
d510 1
a510 2
			 * drop cluster and relock object (only if I/O is
			 * not pending)
d512 1
d519 2
a520 2
			/* if (uobj): object still locked, as per
			 * return-state item #3 */
d545 1
a545 1
			int nswblk = (result == VM_PAGER_AGAIN) ? swblk : 0;
d558 1
a558 1
		if (result == VM_PAGER_AGAIN) {
a881 26
}

/*
 * translate unix errno values to VM_PAGER_*.
 */

int
uvm_errno2vmerror(errno)
	int errno;
{
	switch (errno) {
	case 0:
		return VM_PAGER_OK;
	case EINVAL:
		return VM_PAGER_BAD;
	case EINPROGRESS:
		return VM_PAGER_PEND;
	case EIO:
		return VM_PAGER_ERROR;
	case EAGAIN:
		return VM_PAGER_AGAIN;
	case EBUSY:
		return VM_PAGER_UNLOCK;
	default:
		return VM_PAGER_ERROR;
	}
@


1.22
log
@Bring in more changes from NetBSD. Mostly pagedaemon improvements.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.21 2001/11/10 18:42:31 art Exp $	*/
a60 1
#ifdef UBC
a61 1
#endif
a66 1
#ifdef UBC
a67 1
#endif
d152 1
a152 1
	      UVM_UNKNOWN_OFFSET, 0, UVM_FLAG_NOMERGE) != KERN_SUCCESS) {
a731 1
#ifdef UBC
a754 1
#endif
a794 1
#ifdef UBC
a798 1
#endif
@


1.21
log
@Merge in some parts of the ubc work that has been done in NetBSD that are not
UBC, but prerequsites for it.

- Create a daemon that processes async I/O (swap and paging in the future)
  requests that need processing in process context and that were processed
  in the pagedaemon before.
- Convert some ugly ifdef DIAGNOSTIC code to less intrusive KASSERTs.
- misc other cleanups.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pager.c,v 1.20 2001/11/07 02:55:50 art Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.36 2000/11/27 18:26:41 chs Exp $	*/
d187 2
a188 4
#ifdef DEBUG
		if ((pp->flags & PG_BUSY) == 0)
			panic("uvm_pagermapin: pg %p not busy", pp);
#endif
d190 2
a191 1
		    prot, PMAP_WIRED | prot);
a299 5
#ifdef DIAGNOSTIC
		printf("uvm_mk_pcluster uobj %p npages %d lo 0x%llx hi 0x%llx "
		       "flags 0x%x\n", uobj, *npages, (long long)lo,
		       (long long)hi, flags);
#endif
a317 5
	 *
	 * note that for inactive pages (pages that have been deactivated)
	 * there are no valid mappings and PG_CLEAN should be up to date.
	 * [i.e. there is no need to query the pmap with pmap_is_modified
	 * since there are no mappings].
a330 7
			/* handle active pages */
			/* NOTE: inactive pages don't have pmap mappings */
			if ((pclust->pqflags & PQ_INACTIVE) == 0) {
				if ((flags & PGO_DOACTCLUST) == 0) {
					/* dont want mapped pages at all */
					break;
				}
d332 21
a352 9
				/* make sure "clean" bit is sync'd */
				if ((pclust->flags & PG_CLEANCHK) == 0) {
					if ((pclust->flags & (PG_CLEAN|PG_BUSY))
					   == PG_CLEAN &&
					   pmap_is_modified(pclust))
						pclust->flags &= ~PG_CLEAN;
					/* now checked */
					pclust->flags |= PG_CLEANCHK;
				}
a690 5
#ifdef DIAGNOSTIC
			if (ppsp[lcv]->uobject->pgops->pgo_releasepg == NULL)
				panic("uvm_pager_dropcluster: no releasepg "
				    "function");
#endif
a694 1
#ifdef DIAGNOSTIC
d697 2
a698 4
			if (uobj && !obj_is_alive)
				panic("uvm_pager_dropcluster: object died "
				    "with active page");
#endif
d794 2
a795 2
	int s, i;
	boolean_t release, write, swap;
d799 1
a799 1
	release = (bp->b_flags & (B_ERROR|B_READ)) == (B_ERROR|B_READ);
d847 1
a847 1
		if (release) {
d854 3
a856 2
		 * if this is a read and the page is PG_FAKE
		 * or this was a write, mark the page PG_CLEAN and not PG_FAKE.
d859 1
a859 1
		if (pgs[i]->flags & PG_FAKE || write) {
d865 1
@


1.20
log
@Another sync of uvm to NetBSD. Just minor fiddling, no major changes.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pager.c,v 1.19 2001/11/07 01:18:01 art Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.34 2000/11/24 22:41:39 chs Exp $	*/
d42 1
d47 3
a50 1
#define UVM_PAGER
d53 2
d61 3
d69 3
d81 2
a82 1

d97 6
a102 4
	 pager_map = uvm_km_suballoc(kernel_map, &uvm.pager_sva, &uvm.pager_eva,
	 			PAGER_MAP_SIZE, 0, FALSE, NULL);
	 simple_lock_init(&pager_map_wanted_lock);
	 pager_map_wanted = FALSE;
d128 1
a128 1
uvm_pagermapin(pps, npages, aiop, flags)
a130 1
	struct uvm_aiodesc **aiop;	/* OUT */
a134 1
	struct uvm_aiodesc *aio;
d140 1
a140 2
	UVMHIST_LOG(maphist,"(pps=0x%x, npages=%d, aiop=0x%x, flags=0x%x)",
	      pps, npages, aiop, flags);
a151 10
	if (aiop) {
		MALLOC(aio, struct uvm_aiodesc *, sizeof(*aio), M_TEMP,
		    (flags & UVMPAGER_MAPIN_WAITOK));
		if (aio == NULL)
			return(0);
		*aiop = aio;
	} else {
		aio = NULL;
	}

d157 14
a171 2
			if (aio)
				FREE(aio, M_TEMP);
d179 1
a179 1
		    "pager_map",0);
d183 1
d189 1
a189 1
			panic("uvm_pagermapin: page not busy");
d214 1
a214 1
	
d221 9
d239 2
d278 1
a278 1
	int center_idx, forward;
d300 1
a300 1
	if ((hi - lo) >> PAGE_SHIFT > *npages) {  /* pps too small, bail out! */
d302 3
a304 1
	    printf("uvm_mk_pcluster: provided page array too small (fixed)\n");
d320 1
a320 1
	
d332 2
a333 2

		curoff = center->offset + (forward ? PAGE_SIZE : -PAGE_SIZE);
d336 1
a336 1
		      curoff += (forward ? 1 : -1) << PAGE_SHIFT) {
d339 1
a339 1
			if (pclust == NULL)
d341 1
d345 1
a345 1
				if ((flags & PGO_DOACTCLUST) == 0)
d348 1
d360 1
d362 1
a362 1
			if ((pclust->flags & (PG_CLEAN|PG_BUSY)) != 0)
d364 1
d369 1
d379 1
a379 1
			*npages = *npages + 1;
d442 1
d493 2
a494 2
		result = uobj->pgops->pgo_put(uobj, ppsp, *npages,
		    flags & PGO_SYNCIO);
d498 1
a498 1
		result = uvm_swap_put(swblk, ppsp, *npages, flags & PGO_SYNCIO);
d534 3
a536 3
	 * a pager error occurred.
	 * for transient errors, drop to a cluster of 1 page ("pg")
	 * and try again.  for hard errors, don't bother retrying.
d644 2
a645 1
		if (ppsp[lcv] == pg)		/* skip "pg" */
d672 1
a672 1
		if (ppsp[lcv]->flags & PG_WANTED)
d675 1
d726 1
a726 1
			ppsp[lcv]->flags &= ~(PG_BUSY|PG_WANTED);
d747 178
@


1.19
log
@Add an alignment argument to uvm_map that specifies an alignment hint
for the virtual address.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pager.c,v 1.18 2001/11/06 01:35:04 art Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.33 2000/09/13 15:00:25 thorpej Exp $	*/
a66 1
#define PAGER_MAP_SIZE       (4 * 1024 * 1024)
@


1.18
log
@Move the last content from vm/ to uvm/
The only thing left in vm/ are just dumb wrappers.
vm/vm.h includes uvm/uvm_extern.h
vm/pmap.h includes uvm/uvm_pmap.h
vm/vm_page.h includes uvm/uvm_page.h
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pager.c,v 1.17 2001/11/05 22:14:54 art Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.32 2000/06/27 17:29:32 mrg Exp $	*/
d156 1
a156 1
	      UVM_UNKNOWN_OFFSET, UVM_FLAG_NOMERGE) != KERN_SUCCESS) {
@


1.17
log
@Minor sync to NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pager.c,v 1.16 2001/09/20 17:02:32 mpech Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.31 2000/06/26 14:21:18 mrg Exp $	*/
a45 2

#include <vm/vm.h>
@


1.16
log
@occured->occurred

idea from deraadt@@ via NetBSD
millert@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pager.c,v 1.15 2001/09/19 20:50:59 mickey Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.30 2000/05/20 03:36:06 thorpej Exp $	*/
a47 1
#include <vm/vm_page.h>
@


1.15
log
@merge vm/vm_kern.h into uvm/uvm_extern.h; art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.14 2001/08/11 10:57:22 art Exp $	*/
d505 1
a505 1
	 * a pager error occured.
d569 1
a569 1
	 * a pager error occured (even after dropping the cluster, if there
@


1.14
log
@Various random fixes from NetBSD.
Including support for zeroing pages in the idle loop (not enabled yet).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.13 2001/08/06 14:03:05 art Exp $	*/
a48 1
#include <vm/vm_kern.h>
@


1.13
log
@Add a new type voff_t (right now it's typedefed as off_t) used for offsets
into objects.

Gives the possibilty to mmap beyond the size of vaddr_t.

From NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pager.c,v 1.12 2001/07/26 19:37:13 art Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.26 2000/03/26 20:54:47 kleink Exp $	*/
a57 1
extern struct uvm_pagerops aobj_pager;
a115 3
 *
 * XXX It would be nice to know the direction of the I/O, so that we can
 * XXX map only what is necessary.
d119 1
a119 1
uvm_pagermapin(pps, npages, aiop, waitf)
d123 1
a123 1
	int waitf;
d130 1
d133 11
a143 2
	UVMHIST_LOG(maphist,"(pps=0x%x, npages=%d, aiop=0x%x, waitf=%d)",
	      pps, npages, aiop, waitf);
d147 2
a148 1
		MALLOC(aio, struct uvm_aiodesc *, sizeof(*aio), M_TEMP, waitf);
d157 1
a157 1
	kva = NULL;			/* let system choose VA */
d161 1
a161 1
		if (waitf == M_NOWAIT) {
d165 1
a165 1
			return(NULL);
a181 6

		/*
		 * XXX VM_PROT_DEFAULT includes VM_PROT_EXEC; is that
		 * XXX really necessary?  It could lead to unnecessary
		 * XXX instruction cache flushes.
		 */
d183 1
a183 1
		    VM_PROT_DEFAULT, PMAP_WIRED | VM_PROT_READ | VM_PROT_WRITE);
a361 35
}


/*
 * uvm_shareprot: generic share protect routine
 *
 * => caller must lock map entry's map
 * => caller must lock object pointed to by map entry
 */

void
uvm_shareprot(entry, prot)
	vm_map_entry_t entry;
	vm_prot_t prot;
{
	struct uvm_object *uobj = entry->object.uvm_obj;
	struct vm_page *pp;
	voff_t start, stop;
	UVMHIST_FUNC("uvm_shareprot"); UVMHIST_CALLED(maphist);

	if (UVM_ET_ISSUBMAP(entry)) 
		panic("uvm_shareprot: non-object attached");

	start = entry->offset;
	stop = start + (entry->end - entry->start);

	/*
	 * traverse list of pages in object.   if page in range, pmap_prot it
	 */

	for (pp = uobj->memq.tqh_first ; pp != NULL ; pp = pp->listq.tqe_next) {
		if (pp->offset >= start && pp->offset < stop)
			pmap_page_protect(pp, prot);
	}
	UVMHIST_LOG(maphist, "<- done",0,0,0,0);
@


1.12
log
@Add support for disabling swap devices (swapctl -d).
Improve error handling on I/O errors to swap.
From NetBSD
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pager.c,v 1.11 2001/07/25 13:25:33 art Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.25 2000/01/11 06:57:50 chs Exp $	*/
d253 1
a253 1
	vaddr_t mlo, mhi;		/* IN (if !PGO_ALLPAGES) */
d256 1
a256 1
	vaddr_t lo, hi, curoff;
d378 1
a378 1
	vaddr_t start, stop;
d444 1
a444 1
	vaddr_t start, stop;		/* IN, IN */
@


1.11
log
@Change the pmap_enter interface to merge access_type and the wired boolean
and arbitrary flags into one argument.

One new flag is PMAP_CANFAIL that tells pmap_enter that it can fail if there
are not enough resources to satisfy the request. If this flag is not passed,
pmap_enter should panic as it should have done before this change (XXX - many
pmaps are still not doing that).

Only i386 and alpha implement CANFAIL for now.

Includes uvm updates from NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pager.c,v 1.10 2001/07/18 10:47:05 art Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.24 1999/11/13 00:24:38 thorpej Exp $	*/
d532 1
a532 1
				    PGO_PDFREECLUST, 0);
d546 1
a546 1
		if (uobj)
d548 53
a600 4
		uvm_pager_dropcluster(uobj, pg, ppsp, npages, PGO_REALLOCSWAP,
		    swblk);
		if (pg != NULL)
			goto ReTry;
d634 1
a634 1
uvm_pager_dropcluster(uobj, pg, ppsp, npages, flags, swblk)
a638 2
	int swblk;			/* valid if
					   (uobj == NULL && PGO_REALLOCSWAP) */
a644 16
	 * if we need to reallocate swap space for the cluster we are dropping
	 * (true if swap-backed and PGO_REALLOCSWAP) then free the old
	 * allocation now.   save a block for "pg" if it is non-NULL.
	 *
	 * note that we will zap the object's pointer to swap in the "for" loop
	 * below...
	 */

	if (uobj == NULL && (flags & PGO_REALLOCSWAP)) {
		if (pg)
			uvm_swap_free(swblk + 1, *npages - 1);
		else
			uvm_swap_free(swblk, *npages);
	}

	/*
a749 29
		}

	}

	/*
	 * drop to a cluster of 1 page ("pg") if requested
	 */

	if (pg && (flags & PGO_PDFREECLUST) == 0) {
		/*
		 * if we are not a successful pageout, we make a 1 page cluster.
		 */
		ppsp[0] = pg;
		*npages = 1;

		/*
		 * assign new swap block to new cluster, if anon backed
		 */
		if (uobj == NULL && (flags & PGO_REALLOCSWAP)) {
			if (pg->pqflags & PQ_ANON) {
				simple_lock(&pg->uanon->an_lock);
				pg->uanon->an_swslot = swblk;	/* reassign */
				simple_unlock(&pg->uanon->an_lock);
			} else {
				simple_lock(&pg->uobject->vmobjlock);
				uao_set_swslot(pg->uobject,
				    pg->offset >> PAGE_SHIFT, swblk);
				simple_unlock(&pg->uobject->vmobjlock);
			}
@


1.10
log
@Get rid of the PMAP_NEW option by making it mandatory for all archs.
The archs that didn't have a proper PMAP_NEW now have a dummy implementation
with wrappers around the old functions.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pager.c,v 1.9 2001/06/08 08:09:40 art Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.23 1999/09/12 01:17:41 chs Exp $	*/
d182 1
a182 2
		    VM_PROT_DEFAULT, TRUE,
		    VM_PROT_READ | VM_PROT_WRITE);
@


1.9
log
@Change the paddr_t pmap_extract(struct pmap *, vaddr_t) interface to
boolean_t pmap_extract(struct pmap *, vaddr_t, paddr_t *).
Matches NetBSD. Tested by various people on various platforms.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pager.c,v 1.8 2001/03/22 03:05:56 smart Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.21 1999/07/08 01:02:44 thorpej Exp $	*/
d330 1
a330 1
					   pmap_is_modified(PMAP_PGARG(pclust)))
d344 1
a344 1
			pmap_page_protect(PMAP_PGARG(pclust), VM_PROT_READ);
d394 1
a394 1
			pmap_page_protect(PMAP_PGARG(pp), prot);
d659 1
a659 2
				pmap_page_protect(PMAP_PGARG(ppsp[lcv]),
				    VM_PROT_NONE); /* be safe */
d709 2
a710 2
			pmap_clear_reference(PMAP_PGARG(ppsp[lcv]));
			pmap_clear_modify(PMAP_PGARG(ppsp[lcv]));
@


1.8
log
@Sync style, typo, and comments a little closer to NetBSD.  art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pager.c,v 1.7 2001/03/08 15:21:37 smart Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.22 1999/07/22 22:58:39 thorpej Exp $	*/
@


1.7
log
@Replace thread_wakeup() with wakeup().  It is defined in vm_extern.h as a
wrapper, so this removes a dependence on the old VM system.  From NetBSD.
art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pager.c,v 1.6 2001/01/29 02:07:47 niklas Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.20 1999/05/26 19:16:36 thorpej Exp $	*/
d331 1
a331 1
					pclust->flags &= ~PG_CLEAN;
d445 1
a445 1
	vaddr_t start, stop;	/* IN, IN */
d471 2
a472 2
		 }
					  
d541 3
a543 2
	 * a pager error occured.    if we have clustered, we drop the 
	 * cluster and try again.
d585 8
a592 9

void uvm_pager_dropcluster(uobj, pg, ppsp, npages, flags, swblk)

struct uvm_object *uobj;	/* IN */
struct vm_page *pg, **ppsp;	/* IN, IN/OUT */
int *npages;			/* IN/OUT */
int flags;
int swblk;			/* valid if (uobj == NULL && PGO_REALLOCSWAP) */

@


1.6
log
@$OpenBSD$
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.20 1999/05/26 19:16:36 thorpej Exp $	*/
d649 1
a649 1
			thread_wakeup(ppsp[lcv]);
@


1.5
log
@Bring in some new UVM code from NetBSD (not current).

 - Introduce a new type of map that are interrupt safe and never allow faults
   in them. mb_map and kmem_map are made intrsafe.
 - Add "access protection" to uvm_vslock (to be passed down to uvm_fault and
   later to pmap_enter).
 - madvise(2) now works.
 - various cleanups.
@
text
@d1 1
@


1.4
log
@Change the pmap_enter api to pass down an argument that indicates
the access type that caused this mapping. This is to simplify pmaps
with mod/ref emulation (none for the moment) and in some cases speed
up pmap_is_{referenced,modified}.
At the same time, clean up some mappings that had too high protection.

XXX - the access type is incorrect in old vm, it's only used by uvm and MD code.
The actual use of this in pmap_enter implementations is not in this commit.
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_pager.c,v 1.15 1999/03/25 18:48:55 mrg Exp $	*/
d91 1
a91 1
	 			PAGER_MAP_SIZE, FALSE, FALSE, NULL);
d116 3
d175 5
a708 2
			/* XXX: with PMAP_NEW ref should already be clear,
			 * but don't trust! */
@


1.4.4.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_pager.c,v 1.20 1999/05/26 19:16:36 thorpej Exp $	*/
d91 1
a91 1
	 			PAGER_MAP_SIZE, 0, FALSE, NULL);
a115 3
 *
 * XXX It would be nice to know the direction of the I/O, so that we can
 * XXX map only what is necessary.
a171 5
		/*
		 * XXX VM_PROT_DEFAULT includes VM_PROT_EXEC; is that
		 * XXX really necessary?  It could lead to unnecessary
		 * XXX instruction cache flushes.
		 */
d701 2
@


1.4.4.2
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_pager.c,v 1.8 2001/03/22 03:05:56 smart Exp $	*/
/*	$NetBSD: uvm_pager.c,v 1.22 1999/07/22 22:58:39 thorpej Exp $	*/
d330 1
a330 1
						pclust->flags &= ~PG_CLEAN;
d444 1
a444 1
	vaddr_t start, stop;		/* IN, IN */
d470 2
a471 2
		}

d540 2
a541 3
	 * a pager error occured.
	 * for transient errors, drop to a cluster of 1 page ("pg")
	 * and try again.  for hard errors, don't bother retrying.
d583 9
a591 8
void
uvm_pager_dropcluster(uobj, pg, ppsp, npages, flags, swblk)
	struct uvm_object *uobj;	/* IN */
	struct vm_page *pg, **ppsp;	/* IN, IN/OUT */
	int *npages;			/* IN/OUT */
	int flags;
	int swblk;			/* valid if
					   (uobj == NULL && PGO_REALLOCSWAP) */
d648 1
a648 1
			wakeup(ppsp[lcv]);
@


1.4.4.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_pager.c,v 1.21 1999/07/08 01:02:44 thorpej Exp $	*/
@


1.4.4.4
log
@Sync the SMP branch to something just after 3.0
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_pager.c,v 1.30 2000/05/20 03:36:06 thorpej Exp $	*/
d49 1
d58 1
d117 3
d123 1
a123 1
uvm_pagermapin(pps, npages, aiop, flags)
d127 1
a127 1
	int flags;
a133 1
	vm_prot_t prot;
d136 2
a137 11
	UVMHIST_LOG(maphist,"(pps=0x%x, npages=%d, aiop=0x%x, flags=0x%x)",
	      pps, npages, aiop, flags);

	/*
	 * compute protection.  outgoing I/O only needs read
	 * access to the page, whereas incoming needs read/write.
	 */

	prot = VM_PROT_READ;
	if (flags & UVMPAGER_MAPIN_READ)
		prot |= VM_PROT_WRITE;
d141 1
a141 2
		MALLOC(aio, struct uvm_aiodesc *, sizeof(*aio), M_TEMP,
		    (flags & UVMPAGER_MAPIN_WAITOK));
d150 1
a150 1
	kva = 0;			/* let system choose VA */
d154 1
a154 1
		if ((flags & UVMPAGER_MAPIN_WAITOK) == 0) {
d158 1
a158 1
			return(0);
d175 6
d182 2
a183 1
		    prot, PMAP_WIRED | prot);
d254 1
a254 1
	voff_t mlo, mhi;		/* IN (if !PGO_ALLPAGES) */
d257 1
a257 1
	voff_t lo, hi, curoff;
d330 1
a330 1
					   pmap_is_modified(pclust))
d344 1
a344 1
			pmap_page_protect(pclust, VM_PROT_READ);
d364 35
d445 1
a445 1
	voff_t start, stop;		/* IN, IN */
d533 1
a533 1
				    PGO_PDFREECLUST);
d541 1
a541 1
	 * a pager error occurred.
d547 1
a547 1
		if (uobj) {
d549 4
a552 53
		}
		uvm_pager_dropcluster(uobj, pg, ppsp, npages, PGO_REALLOCSWAP);

		/*
		 * for failed swap-backed pageouts with a "pg",
		 * we need to reset pg's swslot to either:
		 * "swblk" (for transient errors, so we can retry),
		 * or 0 (for hard errors).
		 */

		if (uobj == NULL && pg != NULL) {
			int nswblk = (result == VM_PAGER_AGAIN) ? swblk : 0;
			if (pg->pqflags & PQ_ANON) {
				simple_lock(&pg->uanon->an_lock);
				pg->uanon->an_swslot = nswblk;
				simple_unlock(&pg->uanon->an_lock);
			} else {
				simple_lock(&pg->uobject->vmobjlock);
				uao_set_swslot(pg->uobject,
					       pg->offset >> PAGE_SHIFT,
					       nswblk);
				simple_unlock(&pg->uobject->vmobjlock);
			}
		}
		if (result == VM_PAGER_AGAIN) {

			/*
			 * for transient failures, free all the swslots that
			 * we're not going to retry with.
			 */

			if (uobj == NULL) {
				if (pg) {
					uvm_swap_free(swblk + 1, *npages - 1);
				} else {
					uvm_swap_free(swblk, *npages);
				}
			}
			if (pg) {
				ppsp[0] = pg;
				*npages = 1;
				goto ReTry;
			}
		} else if (uobj == NULL) {

			/*
			 * for hard errors on swap-backed pageouts,
			 * mark the swslots as bad.  note that we do not
			 * free swslots that we mark bad.
			 */

			uvm_swap_markbad(swblk, *npages);
		}
d556 1
a556 1
	 * a pager error occurred (even after dropping the cluster, if there
d586 1
a586 1
uvm_pager_dropcluster(uobj, pg, ppsp, npages, flags)
d591 2
d599 16
d659 2
a660 1
				pmap_page_protect(ppsp[lcv], VM_PROT_NONE);
d710 2
a711 2
			pmap_clear_reference(ppsp[lcv]);
			pmap_clear_modify(ppsp[lcv]);
d721 29
@


1.4.4.5
log
@merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_pager.c,v 1.41 2001/02/18 19:26:50 chs Exp $	*/
a41 1
#define UVM_PAGER
a45 3
#include <sys/pool.h>
#include <sys/vnode.h>
#include <sys/buf.h>
d47 4
a52 2
struct pool *uvm_aiobuf_pool;

a58 3
#ifdef UBC
extern struct uvm_pagerops ubc_pager;
#endif
a63 3
#ifdef UBC
	&ubc_pager,
#endif
d70 1
d74 1
a74 2
static vaddr_t emergva;
static boolean_t emerginuse;
d89 4
a92 6
	pager_map = uvm_km_suballoc(kernel_map, &uvm.pager_sva, &uvm.pager_eva,
	 			    PAGER_MAP_SIZE, 0, FALSE, NULL);
	simple_lock_init(&pager_map_wanted_lock);
	pager_map_wanted = FALSE;
	emergva = uvm_km_valloc(kernel_map, MAXBSIZE);
	emerginuse = FALSE;
d118 1
a118 1
uvm_pagermapin(pps, npages, flags)
d121 1
d126 1
d132 2
a133 1
	UVMHIST_LOG(maphist,"(pps=0x%x, npages=%d)", pps, npages,0,0);
d145 10
d159 1
a159 15
	      UVM_UNKNOWN_OFFSET, 0, UVM_FLAG_NOMERGE) != KERN_SUCCESS) {
		if (curproc == uvm.pagedaemon_proc) {
			simple_lock(&pager_map_wanted_lock);
			if (emerginuse) {
				UVM_UNLOCK_AND_WAIT(&emergva,
				    &pager_map_wanted_lock, FALSE,
				    "emergva", 0);
				goto ReStart;
			}
			emerginuse = TRUE;
			simple_unlock(&pager_map_wanted_lock);
			kva = emergva;
			KASSERT(npages <= MAXBSIZE >> PAGE_SHIFT);
			goto enter;
		}
d161 2
d170 1
a170 1
		    "pager_map", 0);
a173 1
enter:
d177 4
a180 2
		KASSERT(pp);
		KASSERT(pp->flags & PG_BUSY);
d182 1
a182 2
		    prot, PMAP_WIRED | ((pp->flags & PG_FAKE) ? prot :
					VM_PROT_READ));
d204 1
a204 1

a210 9
	if (kva == emergva) {
		simple_lock(&pager_map_wanted_lock);
		emerginuse = FALSE;
		wakeup(&emergva);
		simple_unlock(&pager_map_wanted_lock);
		entries = NULL;
		goto remove;
	}

a219 2
remove:
	pmap_remove(pmap_kernel(), kva, kva + (npages << PAGE_SHIFT));
d257 1
a257 1
	int center_idx, forward, incr;
d279 4
a282 1
	if ((hi - lo) >> PAGE_SHIFT > *npages) { /* pps too small, bail out! */
d297 1
a297 1

d301 5
d309 2
a310 2
		incr = forward ? PAGE_SIZE : -PAGE_SIZE;
		curoff = center->offset + incr;
d313 1
a313 1
		      curoff += incr) {
d316 1
a316 1
			if (pclust == NULL) {
d318 16
a334 24

			if ((flags & PGO_DOACTCLUST) == 0) {
				/* dont want mapped pages at all */
				break;
			}

			/*
			 * get an up-to-date view of the "clean" bit.
			 * note this isn't 100% accurate, but it doesn't
			 * have to be.  if it's not quite right, the
			 * worst that happens is we don't cluster as
			 * aggressively.  we'll sync-it-for-sure before
			 * we free the page, and clean it if necessary.
			 */
			if ((pclust->flags & PG_CLEANCHK) == 0) {
				if ((pclust->flags & (PG_CLEAN|PG_BUSY))
				    == PG_CLEAN &&
				   pmap_is_modified(pclust))
					pclust->flags &= ~PG_CLEAN;

				/* now checked */
				pclust->flags |= PG_CLEANCHK;
			}

d336 1
a336 1
			if ((pclust->flags & (PG_CLEAN|PG_BUSY)) != 0) {
a337 1
			}
a341 1

d351 1
a351 1
			(*npages)++;
a413 1
	UVMHIST_FUNC("uvm_pager_put"); UVMHIST_CALLED(ubchist);
d464 2
a465 2
		result = uobj->pgops->pgo_put(uobj, ppsp, *npages, flags);
		UVMHIST_LOG(ubchist, "put -> %d", result, 0,0,0);
d469 1
a469 1
		result = uvm_swap_put(swblk, ppsp, *npages, flags);
d505 3
a507 3
	 * a pager error occured (even after dropping the cluster, if there
	 * was one).  give up! the caller only has one page ("pg")
	 * to worry about.
d615 1
a615 2
		/* skip "pg" or empty slot */
		if (ppsp[lcv] == pg || ppsp[lcv] == NULL)
d642 1
a642 1
		if (ppsp[lcv]->flags & PG_WANTED) {
a644 1
		}
d666 5
d675 1
d678 4
a681 2
			KASSERT(!uobj || obj_is_alive);

d695 1
a695 1
			ppsp[lcv]->flags &= ~(PG_BUSY|PG_WANTED|PG_FAKE);
a715 180
	}
}

#ifdef UBC
/*
 * interrupt-context iodone handler for nested i/o bufs.
 *
 * => must be at splbio().
 */

void
uvm_aio_biodone1(bp)
	struct buf *bp;
{
	struct buf *mbp = bp->b_private;

	KASSERT(mbp != bp);
	if (bp->b_flags & B_ERROR) {
		mbp->b_flags |= B_ERROR;
		mbp->b_error = bp->b_error;
	}
	mbp->b_resid -= bp->b_bcount;
	pool_put(&bufpool, bp);
	if (mbp->b_resid == 0) {
		biodone(mbp);
	}
}
#endif

/*
 * interrupt-context iodone handler for single-buf i/os
 * or the top-level buf of a nested-buf i/o.
 *
 * => must be at splbio().
 */

void
uvm_aio_biodone(bp)
	struct buf *bp;
{
	/* reset b_iodone for when this is a single-buf i/o. */
	bp->b_iodone = uvm_aio_aiodone;

	simple_lock(&uvm.aiodoned_lock);	/* locks uvm.aio_done */
	TAILQ_INSERT_TAIL(&uvm.aio_done, bp, b_freelist);
	wakeup(&uvm.aiodoned);
	simple_unlock(&uvm.aiodoned_lock);
}

/*
 * uvm_aio_aiodone: do iodone processing for async i/os.
 * this should be called in thread context, not interrupt context.
 */

void
uvm_aio_aiodone(bp)
	struct buf *bp;
{
	int npages = bp->b_bufsize >> PAGE_SHIFT;
	struct vm_page *pg, *pgs[npages];
	struct uvm_object *uobj;
	int s, i, error;
	boolean_t write, swap;
	UVMHIST_FUNC("uvm_aio_aiodone"); UVMHIST_CALLED(ubchist);
	UVMHIST_LOG(ubchist, "bp %p", bp, 0,0,0);

	error = (bp->b_flags & B_ERROR) ? (bp->b_error ? bp->b_error : EIO) : 0;
	write = (bp->b_flags & B_READ) == 0;
#ifdef UBC
	/* XXXUBC B_NOCACHE is for swap pager, should be done differently */
	if (write && !(bp->b_flags & B_NOCACHE) && bioops.io_pageiodone) {
		(*bioops.io_pageiodone)(bp);
	}
#endif

	uobj = NULL;
	for (i = 0; i < npages; i++) {
		pgs[i] = uvm_pageratop((vaddr_t)bp->b_data + (i << PAGE_SHIFT));
		UVMHIST_LOG(ubchist, "pgs[%d] = %p", i, pgs[i],0,0);
	}
	uvm_pagermapout((vaddr_t)bp->b_data, npages);
#ifdef UVM_SWAP_ENCRYPT
	/*
	 * XXX - assumes that we only get ASYNC writes. used to be above.
	 */
	if (pgs[0]->pqflags & PQ_ENCRYPT) {
		uvm_swap_freepages(pgs, npages);
		goto freed;
	}
#endif /* UVM_SWAP_ENCRYPT */
	for (i = 0; i < npages; i++) {
		pg = pgs[i];

		if (i == 0) {
			swap = (pg->pqflags & PQ_SWAPBACKED) != 0;
			if (!swap) {
				uobj = pg->uobject;
				simple_lock(&uobj->vmobjlock);
			}
		}
		KASSERT(swap || pg->uobject == uobj);
		if (swap) {
			if (pg->pqflags & PQ_ANON) {
				simple_lock(&pg->uanon->an_lock);
			} else {
				simple_lock(&pg->uobject->vmobjlock);
			}
		}

		/*
		 * if this is a read and we got an error, mark the pages
		 * PG_RELEASED so that uvm_page_unbusy() will free them.
		 */

		if (!write && error) {
			pg->flags |= PG_RELEASED;
			continue;
		}
		KASSERT(!write || (pgs[i]->flags & PG_FAKE) == 0);

		/*
		 * if this is a read and the page is PG_FAKE,
		 * or this was a successful write,
		 * mark the page PG_CLEAN and not PG_FAKE.
		 */

		if ((pgs[i]->flags & PG_FAKE) || (write && error != ENOMEM)) {
			pmap_clear_reference(pgs[i]);
			pmap_clear_modify(pgs[i]);
			pgs[i]->flags |= PG_CLEAN;
			pgs[i]->flags &= ~PG_FAKE;
		}
		uvm_pageactivate(pg);
		if (swap) {
			if (pg->pqflags & PQ_ANON) {
				simple_unlock(&pg->uanon->an_lock);
			} else {
				simple_unlock(&pg->uobject->vmobjlock);
			}
		}
	}
	uvm_page_unbusy(pgs, npages);
	if (!swap) {
		simple_unlock(&uobj->vmobjlock);
	}

#ifdef UVM_SWAP_ENCRYPT
freed:
#endif
	s = splbio();
	if (write && (bp->b_flags & B_AGE) != 0 && bp->b_vp != NULL) {
		vwakeup(bp->b_vp);
	}
	pool_put(&bufpool, bp);
	splx(s);
}

/*
 * translate unix errno values to VM_PAGER_*.
 */

int
uvm_errno2vmerror(errno)
	int errno;
{
	switch (errno) {
	case 0:
		return VM_PAGER_OK;
	case EINVAL:
		return VM_PAGER_BAD;
	case EINPROGRESS:
		return VM_PAGER_PEND;
	case EIO:
		return VM_PAGER_ERROR;
	case EAGAIN:
		return VM_PAGER_AGAIN;
	case EBUSY:
		return VM_PAGER_UNLOCK;
	default:
		return VM_PAGER_ERROR;
@


1.4.4.6
log
@Merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_pager.c,v 1.48 2001/06/23 20:47:44 chs Exp $	*/
d61 1
d63 1
d69 1
d71 1
d78 2
a79 2
struct vm_map *pager_map;		/* XXX */
struct simplelock pager_map_wanted_lock;
d107 1
a107 1

d155 2
a156 2
	if (uvm_map(pager_map, &kva, size, NULL,
	    UVM_UNKNOWN_OFFSET, 0, UVM_FLAG_NOMERGE) != 0) {
d176 1
a176 1
		pager_map_wanted = TRUE;
d178 1
a178 1
		UVM_UNLOCK_AND_WAIT(pager_map, &pager_map_wanted_lock, FALSE,
a192 1
	pmap_update();
d211 1
a211 1
	struct vm_map_entry *entries;
d230 1
a230 1
	uvm_unmap_remove(pager_map, kva, kva + size, &entries);
a237 1

d242 1
a242 1
	pmap_update();
d280 1
a280 1
	/*
d316 2
a317 2
	 * attempt to cluster around the left [backward], and then
	 * the right side [forward].
d376 1
a376 1

d403 1
a403 1
 *	PGO_SYNCIO: wait for i/o to complete
d408 1
a408 1
 *	1. we return the error code of the pageout
d411 2
a412 2
 *		uobj locked _only_ if PGO_PDFREECLUST is set
 *		AND result == 0 AND async.   in all other cases
d418 1
a418 1
 *		an error, then the caller only has to worry about
a434 1
	boolean_t async = (flags & PGO_SYNCIO) == 0;
d500 1
a500 1
	 * 	if !PGO_PDFREECLUST, we return the cluster to the
d509 2
a510 3
	if (result == 0) {
		if (flags & PGO_PDFREECLUST && !async) {

d512 2
a513 1
			 * drop cluster and relock object for sync i/o.
a514 1

d521 2
a522 2

			/* if (uobj): object still locked, as per #3 */
d540 4
a543 3
		 * for hard failures on swap-backed pageouts with a "pg"
		 * we need to clear pg's swslot since uvm_pager_dropcluster()
		 * didn't do it and we aren't going to retry.
d546 2
a547 1
		if (uobj == NULL && pg != NULL && result != EAGAIN) {
d550 1
a550 1
				pg->uanon->an_swslot = 0;
d555 2
a556 1
				    pg->offset >> PAGE_SHIFT, 0);
d560 1
a560 1
		if (result == EAGAIN) {
d596 1
a596 1

d603 1
a603 1
 * uvm_pager_dropcluster: drop a cluster we have built (because we
d607 1
a607 1
 * => uobj, if non-null, is a non-swap-backed object that is
d615 1
a615 1
 *           PGO_REALLOCSWAP: drop previously allocated swap slots for
d629 1
a629 1
	boolean_t obj_is_alive;
d641 1
a641 1

d694 1
a694 1

d717 1
a717 1
		 * if we are operating on behalf of the pagedaemon and we
d736 1
d760 1
d801 1
d806 1
d883 2
a884 4
	if (bp->b_vp != NULL) {
		if (write && (bp->b_flags & B_AGE) != 0) {
			vwakeup(bp->b_vp);
		}
a885 1
	(void) buf_cleanout(bp);
d888 26
@


1.4.4.7
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_pager.c,v 1.36 2000/11/27 18:26:41 chs Exp $	*/
a60 1
#ifdef UBC
a61 1
#endif
a66 1
#ifdef UBC
a67 1
#endif
d74 2
a75 2
vm_map_t pager_map;		/* XXX */
simple_lock_data_t pager_map_wanted_lock;
d103 1
a103 1
	
d151 2
a152 2
	if (uvm_map(pager_map, &kva, size, NULL, 
	      UVM_UNKNOWN_OFFSET, 0, UVM_FLAG_NOMERGE) != KERN_SUCCESS) {
d172 1
a172 1
		pager_map_wanted = TRUE; 
d174 1
a174 1
		UVM_UNLOCK_AND_WAIT(pager_map, &pager_map_wanted_lock, FALSE, 
d186 2
a187 1
		    prot, PMAP_WIRED | prot);
d189 1
d208 1
a208 1
	vm_map_entry_t entries;
d227 1
a227 1
	(void) uvm_unmap_remove(pager_map, kva, kva + size, &entries);
d235 1
d240 1
a240 1

d278 1
a278 1
	/* 
d314 2
a315 7
	 * attempt to cluster around the left [backward], and then 
	 * the right side [forward].    
	 *
	 * note that for inactive pages (pages that have been deactivated)
	 * there are no valid mappings and PG_CLEAN should be up to date.
	 * [i.e. there is no need to query the pmap with pmap_is_modified
	 * since there are no mappings].
a328 7
			/* handle active pages */
			/* NOTE: inactive pages don't have pmap mappings */
			if ((pclust->pqflags & PQ_INACTIVE) == 0) {
				if ((flags & PGO_DOACTCLUST) == 0) {
					/* dont want mapped pages at all */
					break;
				}
d330 21
a350 9
				/* make sure "clean" bit is sync'd */
				if ((pclust->flags & PG_CLEANCHK) == 0) {
					if ((pclust->flags & (PG_CLEAN|PG_BUSY))
					   == PG_CLEAN &&
					   pmap_is_modified(pclust))
						pclust->flags &= ~PG_CLEAN;
					/* now checked */
					pclust->flags |= PG_CLEANCHK;
				}
d374 1
a374 1
	
d401 1
a401 1
 *	PGO_SYNCIO: do SYNC I/O (no async)
d406 1
a406 1
 *	1. we return the VM_PAGER status code of the pageout
d409 2
a410 2
 *		uobj locked _only_ if PGO_PDFREECLUST is set 
 *		AND result != VM_PAGER_PEND.   in all other cases
d416 1
a416 1
 *		!PEND, !OK, then the caller only has to worry about
d433 1
d499 1
a499 1
	 * 	if !PGO_PDFREECLUST, we return the cluster to the 
d508 3
a510 2
	if (result == VM_PAGER_PEND || result == VM_PAGER_OK) {
		if (result == VM_PAGER_OK && (flags & PGO_PDFREECLUST)) {
d512 1
a512 2
			 * drop cluster and relock object (only if I/O is
			 * not pending)
d514 1
d521 2
a522 2
			/* if (uobj): object still locked, as per
			 * return-state item #3 */
d540 3
a542 4
		 * for failed swap-backed pageouts with a "pg",
		 * we need to reset pg's swslot to either:
		 * "swblk" (for transient errors, so we can retry),
		 * or 0 (for hard errors).
d545 1
a545 2
		if (uobj == NULL && pg != NULL) {
			int nswblk = (result == VM_PAGER_AGAIN) ? swblk : 0;
d548 1
a548 1
				pg->uanon->an_swslot = nswblk;
d553 1
a553 2
					       pg->offset >> PAGE_SHIFT,
					       nswblk);
d557 1
a557 1
		if (result == VM_PAGER_AGAIN) {
d593 1
a593 1
	
d600 1
a600 1
 * uvm_pager_dropcluster: drop a cluster we have built (because we 
d604 1
a604 1
 * => uobj, if non-null, is a non-swap-backed object that is 
d612 1
a612 1
 *           PGO_REALLOCSWAP: drop previously allocated swap slots for 
d626 1
a626 1
	boolean_t obj_is_alive; 
d638 1
a638 1
	
d691 1
a691 1
			
d714 1
a714 1
		 * if we are operating on behalf of the pagedaemon and we 
a732 1
#ifdef UBC
a755 1
#endif
a795 1
#ifdef UBC
a799 1
#endif
d858 1
d876 4
a879 2
	if (write && (bp->b_flags & B_AGE) != 0 && bp->b_vp != NULL) {
		vwakeup(bp->b_vp);
d881 1
a883 26
}

/*
 * translate unix errno values to VM_PAGER_*.
 */

int
uvm_errno2vmerror(errno)
	int errno;
{
	switch (errno) {
	case 0:
		return VM_PAGER_OK;
	case EINVAL:
		return VM_PAGER_BAD;
	case EINPROGRESS:
		return VM_PAGER_PEND;
	case EIO:
		return VM_PAGER_ERROR;
	case EAGAIN:
		return VM_PAGER_AGAIN;
	case EBUSY:
		return VM_PAGER_UNLOCK;
	default:
		return VM_PAGER_ERROR;
	}
@


1.4.4.8
log
@Sync the SMP branch with 3.3
@
text
@d229 1
a229 1
	uvm_unmap_remove(pager_map, kva, kva + size, &entries);
a747 2
	splassert(IPL_BIO);

a771 2
	splassert(IPL_BIO);

d793 1
a793 1
	int i, error;
a797 2
	splassert(IPL_BIO);

d880 1
d885 1
@


1.4.4.9
log
@Sync the SMP branch to -current. This includes moving to ELF.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pager.c,v 1.4.4.8 2003/03/28 00:08:48 niklas Exp $	*/
d435 1
a435 1
	UVMHIST_FUNC("uvm_pager_put"); UVMHIST_CALLED(pdhist);
d487 1
a487 1
		UVMHIST_LOG(pdhist, "put -> %d", result, 0,0,0);
d799 2
a800 2
	UVMHIST_FUNC("uvm_aio_aiodone"); UVMHIST_CALLED(pdhist);
	UVMHIST_LOG(pdhist, "bp %p", bp, 0,0,0);
d816 1
a816 1
		UVMHIST_LOG(pdhist, "pgs[%d] = %p", i, pgs[i],0,0);
@


1.4.4.10
log
@Merge with the trunk
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a191 1
	pmap_update(vm_map_pmap(pager_map));
a241 1
	pmap_update(pmap_kernel());
@


1.3
log
@sync with NetBSD from 1999.05.24 (there is a reason for this date)
 Mostly cleanups, but also a few improvements to pagedaemon for better
 handling of low memory and/or low swap conditions.
@
text
@a127 1
#if !defined(PMAP_NEW)
a129 1
#endif
a163 10
#if defined(PMAP_NEW)
	/*
	 * XXX: (ab)using the pmap module to store state info for us.
	 * (pmap stores the PAs... we fetch them back later and convert back
	 * to pages with PHYS_TO_VM_PAGE).
	 */
	pmap_kenter_pgs(kva, pps, npages);

#else /* PMAP_NEW */

d173 2
a174 1
		    VM_PROT_DEFAULT, TRUE);
a175 2

#endif /* PMAP_NEW */
@


1.2
log
@add OpenBSD tags
@
text
@d1 1
a1 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_pager.c,v 1.14 1999/01/22 08:00:35 chs Exp $	*/
a2 4
/*
 * XXXCDC: "ROUGH DRAFT" QUALITY UVM PRE-RELEASE FILE!   
 *	   >>>USE AT YOUR OWN RISK, WORK IS NOT FINISHED<<<
 */
@


1.1
log
@Import of uvm from NetBSD. Some local changes, some code disabled
@
text
@d1 1
@

