head	1.59;
access;
symbols
	OPENBSD_6_2:1.59.0.2
	OPENBSD_6_2_BASE:1.59
	OPENBSD_6_1:1.59.0.4
	OPENBSD_6_1_BASE:1.59
	OPENBSD_6_0:1.55.0.4
	OPENBSD_6_0_BASE:1.55
	OPENBSD_5_9:1.55.0.2
	OPENBSD_5_9_BASE:1.55
	OPENBSD_5_8:1.54.0.4
	OPENBSD_5_8_BASE:1.54
	OPENBSD_5_7:1.53.0.2
	OPENBSD_5_7_BASE:1.53
	OPENBSD_5_6:1.51.0.4
	OPENBSD_5_6_BASE:1.51
	OPENBSD_5_5:1.50.0.6
	OPENBSD_5_5_BASE:1.50
	OPENBSD_5_4:1.50.0.2
	OPENBSD_5_4_BASE:1.50
	OPENBSD_5_3:1.48.0.4
	OPENBSD_5_3_BASE:1.48
	OPENBSD_5_2:1.48.0.2
	OPENBSD_5_2_BASE:1.48
	OPENBSD_5_1_BASE:1.46
	OPENBSD_5_1:1.46.0.4
	OPENBSD_5_0:1.46.0.2
	OPENBSD_5_0_BASE:1.46
	OPENBSD_4_9:1.44.0.2
	OPENBSD_4_9_BASE:1.44
	OPENBSD_4_8:1.43.0.2
	OPENBSD_4_8_BASE:1.43
	OPENBSD_4_7:1.42.0.2
	OPENBSD_4_7_BASE:1.42
	OPENBSD_4_6:1.40.0.4
	OPENBSD_4_6_BASE:1.40
	OPENBSD_4_5:1.39.0.4
	OPENBSD_4_5_BASE:1.39
	OPENBSD_4_4:1.39.0.2
	OPENBSD_4_4_BASE:1.39
	OPENBSD_4_3:1.38.0.4
	OPENBSD_4_3_BASE:1.38
	OPENBSD_4_2:1.38.0.2
	OPENBSD_4_2_BASE:1.38
	OPENBSD_4_1:1.35.0.2
	OPENBSD_4_1_BASE:1.35
	OPENBSD_4_0:1.34.0.4
	OPENBSD_4_0_BASE:1.34
	OPENBSD_3_9:1.34.0.2
	OPENBSD_3_9_BASE:1.34
	OPENBSD_3_8:1.32.0.2
	OPENBSD_3_8_BASE:1.32
	OPENBSD_3_7:1.31.0.2
	OPENBSD_3_7_BASE:1.31
	OPENBSD_3_6:1.29.0.6
	OPENBSD_3_6_BASE:1.29
	SMP_SYNC_A:1.29
	SMP_SYNC_B:1.29
	OPENBSD_3_5:1.29.0.4
	OPENBSD_3_5_BASE:1.29
	OPENBSD_3_4:1.29.0.2
	OPENBSD_3_4_BASE:1.29
	UBC_SYNC_A:1.29
	OPENBSD_3_3:1.28.0.2
	OPENBSD_3_3_BASE:1.28
	OPENBSD_3_2:1.27.0.2
	OPENBSD_3_2_BASE:1.27
	OPENBSD_3_1:1.26.0.2
	OPENBSD_3_1_BASE:1.26
	UBC_SYNC_B:1.27
	UBC:1.19.0.2
	UBC_BASE:1.19
	OPENBSD_3_0:1.10.0.2
	OPENBSD_3_0_BASE:1.10
	OPENBSD_2_9_BASE:1.7
	OPENBSD_2_9:1.7.0.2
	OPENBSD_2_8:1.4.0.4
	OPENBSD_2_8_BASE:1.4
	OPENBSD_2_7:1.4.0.2
	OPENBSD_2_7_BASE:1.4
	SMP:1.3.0.4
	SMP_BASE:1.3
	kame_19991208:1.3
	OPENBSD_2_6:1.3.0.2
	OPENBSD_2_6_BASE:1.3
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.59
date	2016.09.16.03.39.25;	author dlg;	state Exp;
branches;
next	1.58;
commitid	KPv85WLBOmByrgyC;

1.58
date	2016.09.16.01.51.40;	author dlg;	state Exp;
branches;
next	1.57;
commitid	VnUbZlTLXkN5bQhd;

1.57
date	2016.09.16.01.09.53;	author dlg;	state Exp;
branches;
next	1.56;
commitid	S1LT7BcQMYzBQOe8;

1.56
date	2016.08.11.01.17.33;	author dlg;	state Exp;
branches;
next	1.55;
commitid	3Fyaahd7uscMQePg;

1.55
date	2015.09.09.23.33.37;	author kettenis;	state Exp;
branches;
next	1.54;
commitid	FRBrgGts03IJfFKV;

1.54
date	2015.03.30.21.08.40;	author miod;	state Exp;
branches;
next	1.53;
commitid	UIgWyHZZqklZyLUa;

1.53
date	2015.02.06.09.04.34;	author tedu;	state Exp;
branches;
next	1.52;
commitid	9kMVt50b6Qp3T4s6;

1.52
date	2014.12.05.04.12.48;	author uebayasi;	state Exp;
branches;
next	1.51;
commitid	lSMz5LkZVlVJKmFM;

1.51
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.50;
commitid	7NtJNW9udCOFtDNM;

1.50
date	2013.05.30.16.29.46;	author tedu;	state Exp;
branches;
next	1.49;

1.49
date	2013.05.30.15.17.59;	author tedu;	state Exp;
branches;
next	1.48;

1.48
date	2012.04.11.11.23.22;	author ariane;	state Exp;
branches;
next	1.47;

1.47
date	2012.03.09.13.01.29;	author ariane;	state Exp;
branches;
next	1.46;

1.46
date	2011.06.06.17.10.23;	author ariane;	state Exp;
branches;
next	1.45;

1.45
date	2011.05.24.15.27.36;	author ariane;	state Exp;
branches;
next	1.44;

1.44
date	2010.12.24.21.49.04;	author tedu;	state Exp;
branches;
next	1.43;

1.43
date	2010.04.20.22.05.44;	author tedu;	state Exp;
branches;
next	1.42;

1.42
date	2009.08.28.00.40.03;	author ariane;	state Exp;
branches;
next	1.41;

1.41
date	2009.07.25.12.55.40;	author miod;	state Exp;
branches;
next	1.40;

1.40
date	2009.03.25.20.00.18;	author oga;	state Exp;
branches;
next	1.39;

1.39
date	2008.07.18.16.40.17;	author kurt;	state Exp;
branches;
next	1.38;

1.38
date	2007.04.11.12.10.42;	author art;	state Exp;
branches;
next	1.37;

1.37
date	2007.04.04.18.02.59;	author art;	state Exp;
branches;
next	1.36;

1.36
date	2007.03.25.13.02.51;	author thib;	state Exp;
branches;
next	1.35;

1.35
date	2007.01.12.21.11.38;	author mickey;	state Exp;
branches;
next	1.34;

1.34
date	2005.11.19.02.18.02;	author pedro;	state Exp;
branches;
next	1.33;

1.33
date	2005.09.30.02.42.57;	author brad;	state Exp;
branches;
next	1.32;

1.32
date	2005.05.24.21.11.47;	author tedu;	state Exp;
branches;
next	1.31;

1.31
date	2005.03.01.01.28.39;	author henning;	state Exp;
branches;
next	1.30;

1.30
date	2005.02.19.17.58.03;	author henning;	state Exp;
branches;
next	1.29;

1.29
date	2003.04.14.04.53.51;	author art;	state Exp;
branches;
next	1.28;

1.28
date	2002.10.29.18.30.21;	author art;	state Exp;
branches;
next	1.27;

1.27
date	2002.06.11.06.38.01;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2002.03.14.01.27.18;	author millert;	state Exp;
branches;
next	1.25;

1.25
date	2002.03.07.01.08.57;	author provos;	state Exp;
branches;
next	1.24;

1.24
date	2002.02.28.18.50.26;	author provos;	state Exp;
branches;
next	1.23;

1.23
date	2002.02.25.05.38.50;	author provos;	state Exp;
branches;
next	1.22;

1.22
date	2002.02.25.00.20.45;	author provos;	state Exp;
branches;
next	1.21;

1.21
date	2002.02.18.10.02.20;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2001.12.19.08.58.07;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2001.12.04.23.22.42;	author art;	state Exp;
branches
	1.19.2.1;
next	1.18;

1.18
date	2001.11.28.19.28.15;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2001.11.28.13.47.40;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2001.11.27.05.27.12;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2001.11.12.01.26.09;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2001.11.07.02.55.50;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2001.11.07.01.18.01;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2001.11.06.13.36.52;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2001.11.06.00.27.01;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2001.08.12.22.41.15;	author mickey;	state Exp;
branches;
next	1.9;

1.9
date	2001.08.11.10.57.22;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2001.08.06.14.03.05;	author art;	state Exp;
branches;
next	1.7;

1.7
date	2001.03.22.03.05.55;	author smart;	state Exp;
branches;
next	1.6;

1.6
date	2001.03.09.05.34.38;	author smart;	state Exp;
branches;
next	1.5;

1.5
date	2001.01.29.02.07.46;	author niklas;	state Exp;
branches;
next	1.4;

1.4
date	2000.03.16.22.11.04;	author art;	state Exp;
branches;
next	1.3;

1.3
date	99.08.23.08.13.24;	author art;	state Exp;
branches
	1.3.4.1;
next	1.2;

1.2
date	99.02.26.05.32.07;	author art;	state Exp;
branches;
next	1.1;

1.1
date	99.02.26.01.30.15;	author art;	state Exp;
branches;
next	;

1.3.4.1
date	2000.03.24.09.09.50;	author niklas;	state Exp;
branches;
next	1.3.4.2;

1.3.4.2
date	2001.05.14.22.47.46;	author niklas;	state Exp;
branches;
next	1.3.4.3;

1.3.4.3
date	2001.10.31.03.32.14;	author nate;	state Exp;
branches;
next	1.3.4.4;

1.3.4.4
date	2001.11.13.23.02.31;	author niklas;	state Exp;
branches;
next	1.3.4.5;

1.3.4.5
date	2001.12.05.01.23.58;	author niklas;	state Exp;
branches;
next	1.3.4.6;

1.3.4.6
date	2002.03.06.02.17.14;	author niklas;	state Exp;
branches;
next	1.3.4.7;

1.3.4.7
date	2002.03.28.14.54.26;	author niklas;	state Exp;
branches;
next	1.3.4.8;

1.3.4.8
date	2003.03.28.00.08.48;	author niklas;	state Exp;
branches;
next	1.3.4.9;

1.3.4.9
date	2003.05.13.19.36.58;	author ho;	state Exp;
branches;
next	;

1.19.2.1
date	2002.02.02.03.28.27;	author art;	state Exp;
branches;
next	1.19.2.2;

1.19.2.2
date	2002.06.11.03.33.03;	author art;	state Exp;
branches;
next	1.19.2.3;

1.19.2.3
date	2002.10.29.00.36.50;	author art;	state Exp;
branches;
next	1.19.2.4;

1.19.2.4
date	2002.11.04.18.02.33;	author art;	state Exp;
branches;
next	1.19.2.5;

1.19.2.5
date	2003.05.19.22.41.29;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.59
log
@put RBT_PROTOTYPE inside #ifdef _KERNEL
@
text
@/*	$OpenBSD: uvm_map.h,v 1.58 2016/09/16 01:51:40 dlg Exp $	*/
/*	$NetBSD: uvm_map.h,v 1.24 2001/02/18 21:19:08 chs Exp $	*/

/*
 * Copyright (c) 2011 Ariane van der Steldt <ariane@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 *
 * 
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
 *
 * All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * The Mach Operating System project at Carnegie-Mellon University.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)vm_map.h    8.3 (Berkeley) 3/15/94
 * from: Id: uvm_map.h,v 1.1.2.3 1998/02/07 01:16:55 chs Exp
 *
 *
 * Copyright (c) 1987, 1990 Carnegie-Mellon University.
 * All rights reserved.
 * 
 * Permission to use, copy, modify and distribute this software and
 * its documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
 * FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
 * 
 * Carnegie Mellon requests users of this software to return to
 *
 *  Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
 *  School of Computer Science
 *  Carnegie Mellon University
 *  Pittsburgh PA 15213-3890
 *
 * any improvements or extensions that they make and grant Carnegie the
 * rights to redistribute these changes.
 */

#ifndef _UVM_UVM_MAP_H_
#define _UVM_UVM_MAP_H_

#include <sys/mutex.h>
#include <sys/rwlock.h>

#ifdef _KERNEL

/*
 * Internal functions.
 *
 * Required by clipping macros.
 */
void			 uvm_map_clip_end(struct vm_map*, struct vm_map_entry*,
			    vaddr_t);
void			 uvm_map_clip_start(struct vm_map*,
			    struct vm_map_entry*, vaddr_t);

/*
 * UVM_MAP_CLIP_START: ensure that the entry begins at or after
 * the starting address, if it doesn't we split the entry.
 * 
 * => map must be locked by caller
 */

#define UVM_MAP_CLIP_START(_map, _entry, _addr)				\
	do {								\
		KASSERT((_entry)->end + (_entry)->fspace > (_addr));	\
		if ((_entry)->start < (_addr))				\
			uvm_map_clip_start((_map), (_entry), (_addr));	\
	} while (0)

/*
 * UVM_MAP_CLIP_END: ensure that the entry ends at or before
 *      the ending address, if it does't we split the entry.
 *
 * => map must be locked by caller
 */

#define UVM_MAP_CLIP_END(_map, _entry, _addr)				\
	do {								\
		KASSERT((_entry)->start < (_addr));			\
		if ((_entry)->end > (_addr))				\
			uvm_map_clip_end((_map), (_entry), (_addr));	\
	} while (0)

/*
 * extract flags
 */
#define UVM_EXTRACT_FIXPROT	0x8	/* set prot to maxprot as we go */

#endif /* _KERNEL */

#include <uvm/uvm_anon.h>

/*
 * types defined:
 *
 *	vm_map_t		the high-level address map data structure.
 *	vm_map_entry_t		an entry in an address map.
 *	vm_map_version_t	a timestamp of a map, for use with vm_map_lookup
 */

/*
 * Objects which live in maps may be either VM objects, or another map
 * (called a "sharing map") which denotes read-write sharing with other maps.
 *
 * XXXCDC: private pager data goes here now
 */

union vm_map_object {
	struct uvm_object	*uvm_obj;	/* UVM OBJECT */
	struct vm_map		*sub_map;	/* belongs to another map */
};

/*
 * Address map entries consist of start and end addresses,
 * a VM object (or sharing map) and offset into that object,
 * and user-exported inheritance and protection information.
 * Also included is control information for virtual copy operations.
 */
struct vm_map_entry {
	union {
		RBT_ENTRY(vm_map_entry)	addr_entry; /* address tree */
		SLIST_ENTRY(vm_map_entry) addr_kentry;
	} daddrs;

	union {
		RBT_ENTRY(vm_map_entry)	rbtree;	/* Link freespace tree. */
		TAILQ_ENTRY(vm_map_entry) tailq;/* Link freespace queue. */
		TAILQ_ENTRY(vm_map_entry) deadq;/* dead entry queue */
	} dfree;

#define uvm_map_entry_start_copy start
	vaddr_t			start;		/* start address */
	vaddr_t			end;		/* end address */

	vsize_t			guard;		/* bytes in guard */
	vsize_t			fspace;		/* free space */

	union vm_map_object	object;		/* object I point to */
	voff_t			offset;		/* offset into object */
	struct vm_aref		aref;		/* anonymous overlay */

	int			etype;		/* entry type */

	vm_prot_t		protection;	/* protection code */
	vm_prot_t		max_protection;	/* maximum protection */
	vm_inherit_t		inheritance;	/* inheritance */

	int			wired_count;	/* can be paged if == 0 */
	int			advice;		/* madvise advice */
#define uvm_map_entry_stop_copy flags
	u_int8_t		flags;		/* flags */

#define UVM_MAP_STATIC		0x01		/* static map entry */
#define UVM_MAP_KMEM		0x02		/* from kmem entry pool */

	vsize_t			fspace_augment;	/* max(fspace) in subtree */
};

#define	VM_MAPENT_ISWIRED(entry)	((entry)->wired_count != 0)

TAILQ_HEAD(uvm_map_deadq, vm_map_entry);	/* dead entry queue */
RBT_HEAD(uvm_map_addr, vm_map_entry);
#ifdef _KERNEL
RBT_PROTOTYPE(uvm_map_addr, vm_map_entry, daddrs.addr_entry,
    uvm_mapentry_addrcmp);
#endif

/*
 *	A Map is a rbtree of map entries, kept sorted by address.
 *	In addition, free space entries are also kept in a rbtree,
 *	indexed by free size.
 *
 *
 *
 *	LOCKING PROTOCOL NOTES:
 *	-----------------------
 *
 *	VM map locking is a little complicated.  There are both shared
 *	and exclusive locks on maps.  However, it is sometimes required
 *	to downgrade an exclusive lock to a shared lock, and upgrade to
 *	an exclusive lock again (to perform error recovery).  However,
 *	another thread *must not* queue itself to receive an exclusive
 *	lock while before we upgrade back to exclusive, otherwise the
 *	error recovery becomes extremely difficult, if not impossible.
 *
 *	In order to prevent this scenario, we introduce the notion of
 *	a `busy' map.  A `busy' map is read-locked, but other threads
 *	attempting to write-lock wait for this flag to clear before
 *	entering the lock manager.  A map may only be marked busy
 *	when the map is write-locked (and then the map must be downgraded
 *	to read-locked), and may only be marked unbusy by the thread
 *	which marked it busy (holding *either* a read-lock or a
 *	write-lock, the latter being gained by an upgrade).
 *
 *	Access to the map `flags' member is controlled by the `flags_lock'
 *	simple lock.  Note that some flags are static (set once at map
 *	creation time, and never changed), and thus require no locking
 *	to check those flags.  All flags which are r/w must be set or
 *	cleared while the `flags_lock' is asserted.  Additional locking
 *	requirements are:
 *
 *		VM_MAP_PAGEABLE		r/o static flag; no locking required
 *
 *		VM_MAP_INTRSAFE		r/o static flag; no locking required
 *
 *		VM_MAP_WIREFUTURE	r/w; may only be set or cleared when
 *					map is write-locked.  may be tested
 *					without asserting `flags_lock'.
 *
 *		VM_MAP_BUSY		r/w; may only be set when map is
 *					write-locked, may only be cleared by
 *					thread which set it, map read-locked
 *					or write-locked.  must be tested
 *					while `flags_lock' is asserted.
 *
 *		VM_MAP_WANTLOCK		r/w; may only be set when the map
 *					is busy, and thread is attempting
 *					to write-lock.  must be tested
 *					while `flags_lock' is asserted.
 *
 *		VM_MAP_GUARDPAGES	r/o; must be specified at map
 *					initialization time.
 *					If set, guards will appear between
 *					automatic allocations.
 *					No locking required.
 *
 *		VM_MAP_ISVMSPACE	r/o; set by uvmspace_alloc.
 *					Signifies that this map is a vmspace.
 *					(The implementation treats all maps
 *					without this bit as kernel maps.)
 *					No locking required.
 *
 *
 * All automatic allocations (uvm_map without MAP_FIXED) will allocate
 * from vm_map.free.
 * If that allocation fails:
 * - vmspace maps will spill over into vm_map.bfree,
 * - all other maps will call uvm_map_kmem_grow() to increase the arena.
 * 
 * vmspace maps have their data, brk() and stack arenas automatically
 * updated when uvm_map() is invoked without MAP_FIXED.
 * The spill over arena (vm_map.bfree) will contain the space in the brk()
 * and stack ranges.
 * Kernel maps never have a bfree arena and this tree will always be empty.
 *
 *
 * read_locks and write_locks are used in lock debugging code.
 */
struct vm_map {
	struct pmap *		pmap;		/* Physical map */
	struct rwlock		lock;		/* Lock for map data */
	struct mutex		mtx;

	struct uvm_map_addr	addr;		/* Entry tree, by addr */

	vsize_t			size;		/* virtual size */
	int			ref_count;	/* Reference count */
	int			flags;		/* flags */
	struct mutex		flags_lock;	/* flags lock */
	unsigned int		timestamp;	/* Version number */

	vaddr_t			min_offset;	/* First address in map. */
	vaddr_t			max_offset;	/* Last address in map. */

	/*
	 * Allocation overflow regions.
	 */
	vaddr_t			b_start;	/* Start for brk() alloc. */
	vaddr_t			b_end;		/* End for brk() alloc. */
	vaddr_t			s_start;	/* Start for stack alloc. */
	vaddr_t			s_end;		/* End for stack alloc. */

	/*
	 * Special address selectors.
	 *
	 * The uaddr_exe mapping is used if:
	 * - protX is selected
	 * - the pointer is not NULL
	 *
	 * If uaddr_exe is not used, the other mappings are checked in
	 * order of appearance.
	 * If a hint is given, the selection will only be used if the hint
	 * falls in the range described by the mapping.
	 *
	 * The states are pointers because:
	 * - they may not all be in use
	 * - the struct size for different schemes is variable
	 *
	 * The uaddr_brk_stack selector will select addresses that are in
	 * the brk/stack area of the map.
	 */
	struct uvm_addr_state	*uaddr_exe;	/* Executable selector. */
	struct uvm_addr_state	*uaddr_any[4];	/* More selectors. */
	struct uvm_addr_state	*uaddr_brk_stack; /* Brk/stack selector. */
};

/* vm_map flags */
#define	VM_MAP_PAGEABLE		0x01		/* ro: entries are pageable */
#define	VM_MAP_INTRSAFE		0x02		/* ro: interrupt safe map */
#define	VM_MAP_WIREFUTURE	0x04		/* rw: wire future mappings */
#define	VM_MAP_BUSY		0x08		/* rw: map is busy */
#define	VM_MAP_WANTLOCK		0x10		/* rw: want to write-lock */
#define VM_MAP_GUARDPAGES	0x20		/* rw: add guard pgs to map */
#define VM_MAP_ISVMSPACE	0x40		/* ro: map is a vmspace */

/* XXX: number of kernel maps and entries to statically allocate */

#if !defined(MAX_KMAPENT)
#define	MAX_KMAPENT	1024	/* Sufficient to make it to the scheduler. */
#endif	/* !defined MAX_KMAPENT */

#ifdef _KERNEL
#define	vm_map_modflags(map, set, clear)				\
do {									\
	mtx_enter(&(map)->flags_lock);					\
	(map)->flags = ((map)->flags | (set)) & ~(clear);		\
	mtx_leave(&(map)->flags_lock);					\
} while (0)
#endif /* _KERNEL */

/*
 *	Interrupt-safe maps must also be kept on a special list,
 *	to assist uvm_fault() in avoiding locking problems.
 */
struct vm_map_intrsafe {
	struct vm_map	vmi_map;
	LIST_ENTRY(vm_map_intrsafe) vmi_list;
};

/*
 * globals:
 */

#ifdef _KERNEL

extern vaddr_t	uvm_maxkaddr;

/*
 * protos: the following prototypes define the interface to vm_map
 */

void		uvm_map_deallocate(vm_map_t);

int		uvm_map_clean(vm_map_t, vaddr_t, vaddr_t, int);
vm_map_t	uvm_map_create(pmap_t, vaddr_t, vaddr_t, int);
int		uvm_map_extract(struct vm_map*, vaddr_t, vsize_t, vaddr_t*,
		    int);
vaddr_t		uvm_map_pie(vaddr_t);
vaddr_t		uvm_map_hint(struct vmspace *, vm_prot_t, vaddr_t, vaddr_t);
int		uvm_map_inherit(vm_map_t, vaddr_t, vaddr_t, vm_inherit_t);
int		uvm_map_advice(vm_map_t, vaddr_t, vaddr_t, int);
void		uvm_map_init(void);
boolean_t	uvm_map_lookup_entry(vm_map_t, vaddr_t, vm_map_entry_t *);
int		uvm_map_replace(vm_map_t, vaddr_t, vaddr_t,
		    vm_map_entry_t, int);
int		uvm_map_reserve(vm_map_t, vsize_t, vaddr_t, vsize_t,
		    vaddr_t *);
void		uvm_map_setup(vm_map_t, vaddr_t, vaddr_t, int);
int		uvm_map_submap(vm_map_t, vaddr_t, vaddr_t, vm_map_t);
void		uvm_unmap(vm_map_t, vaddr_t, vaddr_t);
void		uvm_map_set_uaddr(struct vm_map*, struct uvm_addr_state**,
		    struct uvm_addr_state*);
int		uvm_map_mquery(struct vm_map*, vaddr_t*, vsize_t, voff_t, int);

void		uvm_unmap_detach(struct uvm_map_deadq*, int);
void		uvm_unmap_remove(struct vm_map*, vaddr_t, vaddr_t,
		    struct uvm_map_deadq*, boolean_t, boolean_t);

struct kinfo_vmentry;

int		uvm_map_fill_vmmap(struct vm_map *, struct kinfo_vmentry *,
		    size_t *);

#endif /* _KERNEL */

/*
 * VM map locking operations:
 *
 *	These operations perform locking on the data portion of the
 *	map.
 *
 *	vm_map_lock_try: try to lock a map, failing if it is already locked.
 *
 *	vm_map_lock: acquire an exclusive (write) lock on a map.
 *
 *	vm_map_lock_read: acquire a shared (read) lock on a map.
 *
 *	vm_map_unlock: release an exclusive lock on a map.
 *
 *	vm_map_unlock_read: release a shared lock on a map.
 *
 *	vm_map_downgrade: downgrade an exclusive lock to a shared lock.
 *
 *	vm_map_upgrade: upgrade a shared lock to an exclusive lock.
 *
 *	vm_map_busy: mark a map as busy.
 *
 *	vm_map_unbusy: clear busy status on a map.
 *
 */

#ifdef _KERNEL
/*
 * XXX: clean up later
 * Half the kernel seems to depend on them being included here.
 */
#include <sys/time.h>
#include <sys/systm.h>  /* for panic() */

boolean_t	vm_map_lock_try_ln(struct vm_map*, char*, int);
void		vm_map_lock_ln(struct vm_map*, char*, int);
void		vm_map_lock_read_ln(struct vm_map*, char*, int);
void		vm_map_unlock_ln(struct vm_map*, char*, int);
void		vm_map_unlock_read_ln(struct vm_map*, char*, int);
void		vm_map_downgrade_ln(struct vm_map*, char*, int);
void		vm_map_upgrade_ln(struct vm_map*, char*, int);
void		vm_map_busy_ln(struct vm_map*, char*, int);
void		vm_map_unbusy_ln(struct vm_map*, char*, int);

#ifdef DIAGNOSTIC
#define vm_map_lock_try(map)	vm_map_lock_try_ln(map, __FILE__, __LINE__)
#define vm_map_lock(map)	vm_map_lock_ln(map, __FILE__, __LINE__)
#define vm_map_lock_read(map)	vm_map_lock_read_ln(map, __FILE__, __LINE__)
#define vm_map_unlock(map)	vm_map_unlock_ln(map, __FILE__, __LINE__)
#define vm_map_unlock_read(map)	vm_map_unlock_read_ln(map, __FILE__, __LINE__)
#define vm_map_downgrade(map)	vm_map_downgrade_ln(map, __FILE__, __LINE__)
#define vm_map_upgrade(map)	vm_map_upgrade_ln(map, __FILE__, __LINE__)
#define vm_map_busy(map)	vm_map_busy_ln(map, __FILE__, __LINE__)
#define vm_map_unbusy(map)	vm_map_unbusy_ln(map, __FILE__, __LINE__)
#else
#define vm_map_lock_try(map)	vm_map_lock_try_ln(map, NULL, 0)
#define vm_map_lock(map)	vm_map_lock_ln(map, NULL, 0)
#define vm_map_lock_read(map)	vm_map_lock_read_ln(map, NULL, 0)
#define vm_map_unlock(map)	vm_map_unlock_ln(map, NULL, 0)
#define vm_map_unlock_read(map)	vm_map_unlock_read_ln(map, NULL, 0)
#define vm_map_downgrade(map)	vm_map_downgrade_ln(map, NULL, 0)
#define vm_map_upgrade(map)	vm_map_upgrade_ln(map, NULL, 0)
#define vm_map_busy(map)	vm_map_busy_ln(map, NULL, 0)
#define vm_map_unbusy(map)	vm_map_unbusy_ln(map, NULL, 0)
#endif

#endif /* _KERNEL */

/*
 *	Functions implemented as macros
 */
#define		vm_map_min(map)		((map)->min_offset)
#define		vm_map_max(map)		((map)->max_offset)
#define		vm_map_pmap(map)	((map)->pmap)

#endif /* _UVM_UVM_MAP_H_ */
@


1.58
log
@move uaddr_free_rbtree from RB macros to RBT functions
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.57 2016/09/16 01:09:53 dlg Exp $	*/
d205 1
d208 1
@


1.57
log
@move the uvm_map_addr RB tree from RB macros to the RBT functions

this tree is interesting because it uses all the red black tree
features, specifically the augment callback thats called on tree
topology changes, and it poisons and checks entries as theyre removed
from and inserted back into the tree respectively.

ok stefan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.56 2016/08/11 01:17:33 dlg Exp $	*/
d168 1
a168 1
		RB_ENTRY(vm_map_entry)	rbtree;	/* Link freespace tree. */
@


1.56
log
@replace abuse of the static map entries RB_ENTRY pointers with an SLIST

free static entries are kept in a simple linked list, so use SLIST
to make this obvious. the RB_PARENT manipulations are ugly and
confusing.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.55 2015/09/09 23:33:37 kettenis Exp $	*/
d163 1
a163 1
		RB_ENTRY(vm_map_entry)	addr_entry; /* address tree */
d204 2
a205 2
RB_HEAD(uvm_map_addr, vm_map_entry);
RB_PROTOTYPE(uvm_map_addr, vm_map_entry, daddrs.addr_entry,
@


1.55
log
@Add locking for interrupt-safe maps (those that have the VM_MAP_INTRSAFE flag
set).  Since we cannot sleep in that case, use a mutex instead of an rwlock.
This is ok as the more complex code paths in the uvm code are not entered
for interrupt-safe maps as paging isn't allowed in those maps.

ok tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.54 2015/03/30 21:08:40 miod Exp $	*/
d164 1
@


1.54
log
@Extend uvm_map_hint() to get an address range as extra arguments, and make
sure it will return an address within that range.

Use this in uaddr_rnd_select() to make sure we will not attempt to pick
an address beyond what we are allowed to map.

In my trees for 9 months, blackmailed s2k15 attendees into agreeing now would
be a good time to commit.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.53 2015/02/06 09:04:34 tedu Exp $	*/
d291 1
@


1.53
log
@make vm_map_lock lock when it's supposed to. add mutex to protect flags
and then double check we didn't lose the unavoidable race.
ok beck guenther kettenis miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.52 2014/12/05 04:12:48 uebayasi Exp $	*/
d387 1
a387 1
vaddr_t		uvm_map_hint(struct vmspace *, vm_prot_t);
@


1.52
log
@Introduce a new sysctl to retrieve VM map entries

This adds a new sysctl KERN_PROC_VMMAP, which returns an array of VM map
entries of a specified process.  This prevents debuggers from iterating
vm_map_entry RB tree via kvm(3).

The name KERN_PROC_VMMAP and struct kinfo_vmentry are chosen from the same
function in FreeBSD.  struct kinfo_vmentry is revised to reduce size, because
OpenBSD does not keep track of filepaths.  The semantic is also changed to
return max buffer size as a hint, and start iteration at the specified base
address.

Much valuable input from deraadt@@, guenther@@, tedu@@

OK tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d83 1
d297 1
d353 1
d355 1
@


1.51
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.50 2013/05/30 16:29:46 tedu Exp $	*/
d402 5
@


1.50
log
@remove lots of comments about locking per beck's request
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.49 2013/05/30 15:17:59 tedu Exp $	*/
d36 1
a36 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by Charles D. Cranor,
 *      Washington University, the University of California, Berkeley and 
 *      its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.49
log
@remove simple_locks from uvm code. ok beck deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.48 2012/04/11 11:23:22 ariane Exp $	*/
a433 2
 * Note that "intrsafe" maps use only exclusive, spin locks.  We simply
 * use the sleep lock's interlock for this.
@


1.48
log
@vmmap: speed up allocations

Reduces O(n log n) allocations to O(log n).

ok deraadt, tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.47 2012/03/09 13:01:29 ariane Exp $	*/
a299 1
	simple_lock_data_t	ref_lock;	/* Lock for ref_count field */
@


1.47
log
@New vmmap implementation.

no oks (it is really a pain to review properly)
extensively tested, I'm confident it'll be stable
'now is the time' from several icb inhabitants

Diff provides:
- ability to specify different allocators for different regions/maps
- a simpler implementation of the current allocator
- currently in compatibility mode: it will generate similar addresses
  as the old allocator
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.46 2011/06/06 17:10:23 ariane Exp $	*/
d200 2
@


1.46
log
@Backout vmmap in order to repair virtual address selection algorithms
outside the tree.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.45 2011/05/24 15:27:36 ariane Exp $	*/
d4 16
a19 1
/* 
d93 10
d109 6
a114 2
#define UVM_MAP_CLIP_START(MAP,ENTRY,VA) { \
	if ((VA) > (ENTRY)->start) uvm_map_clip_start(MAP,ENTRY,VA); }
d123 6
a128 2
#define UVM_MAP_CLIP_END(MAP,ENTRY,VA) { \
	if ((VA) < (ENTRY)->end) uvm_map_clip_end(MAP,ENTRY,VA); }
a132 3
#define UVM_EXTRACT_REMOVE	0x1	/* remove mapping from old map */
#define UVM_EXTRACT_CONTIG	0x2	/* try to keep it contig */
#define UVM_EXTRACT_QREF	0x4	/* use quick refs */
d166 11
a176 5
	RB_ENTRY(vm_map_entry)	rb_entry;	/* tree information */
	vaddr_t			ownspace;	/* free space after */
	vaddr_t			space;		/* space in subtree */
	struct vm_map_entry	*prev;		/* previous entry */
	struct vm_map_entry	*next;		/* next entry */
d179 4
d185 2
d188 1
d192 1
a193 1
	struct vm_aref		aref;		/* anonymous overlay */
d202 1
a202 4
/*
 * Marks the map entry as a guard page, using vm_map_entry.etype.
 */
#define MAP_ET_KVAGUARD		0x10		/* guard entry */
d204 4
a207 1
#define	VM_MAPENT_ISWIRED(entry)	((entry)->wired_count != 0)
d210 5
a214 4
 *	Maps are doubly-linked lists of map entries, kept sorted
 *	by address.  A single hint is provided to start
 *	searches again from the last successful search,
 *	insertion, or removal.
d261 28
d293 3
a295 3
	RB_HEAD(uvm_tree, vm_map_entry) rbhead;	/* Tree for entries */
	struct vm_map_entry	header;		/* List of entries */
	int			nentries;	/* Number of entries */
a298 3
	vm_map_entry_t		hint;		/* hint for quick lookups */
	simple_lock_data_t	hint_lock;	/* lock for hint storage */
	vm_map_entry_t		first_free;	/* First free space hint */
d301 34
a334 2
#define	min_offset		header.start
#define max_offset		header.end
d343 2
d349 1
a349 1
#define	MAX_KMAPENT	1024  /* XXXCDC: no crash */
a373 1
#ifdef PMAP_GROWKERNEL
a374 1
#endif
a382 2
void		uvm_map_clip_start(vm_map_t, vm_map_entry_t, vaddr_t);
void		uvm_map_clip_end(vm_map_t, vm_map_entry_t, vaddr_t);
d384 2
a385 4
int		uvm_map_extract(vm_map_t, vaddr_t, vsize_t, 
		    vm_map_t, vaddr_t *, int);
vm_map_entry_t	uvm_map_findspace(vm_map_t, vaddr_t, vsize_t, vaddr_t *,
		    struct uvm_object *, voff_t, vsize_t, int);
d387 1
a387 2
#define		uvm_map_hint(p, prot) uvm_map_hint1(p, prot, 1)
vaddr_t		uvm_map_hint1(struct proc *, vm_prot_t, int);
a391 1
void		uvm_map_reference(vm_map_t);
d398 8
a405 5
#define		uvm_unmap(_m, _s, _e) uvm_unmap_p(_m, _s, _e, 0)
void		uvm_unmap_p(vm_map_t, vaddr_t, vaddr_t, struct proc *);
void		uvm_unmap_detach(vm_map_entry_t,int);
void		uvm_unmap_remove(vm_map_t, vaddr_t, vaddr_t, vm_map_entry_t *,
		    struct proc *, boolean_t);
d438 4
a441 1
/* XXX: clean up later */
d443 1
a443 1
#include <sys/systm.h>	/* for panic() */
d445 31
a475 62
static __inline boolean_t vm_map_lock_try(vm_map_t);
static __inline void vm_map_lock(vm_map_t);
extern const char vmmapbsy[];

static __inline boolean_t
vm_map_lock_try(struct vm_map *map)
{
	boolean_t rv;

	if (map->flags & VM_MAP_INTRSAFE) {
		rv = TRUE;
	} else {
		if (map->flags & VM_MAP_BUSY) {
			return (FALSE);
		}
		rv = (rw_enter(&map->lock, RW_WRITE|RW_NOSLEEP) == 0);
	}

	if (rv)
		map->timestamp++;

	return (rv);
}

static __inline void
vm_map_lock(struct vm_map *map)
{
	if (map->flags & VM_MAP_INTRSAFE)
		return;

	do {
		while (map->flags & VM_MAP_BUSY) {
			map->flags |= VM_MAP_WANTLOCK;
			tsleep(&map->flags, PVM, (char *)vmmapbsy, 0);
		}
	} while (rw_enter(&map->lock, RW_WRITE|RW_SLEEPFAIL) != 0);

	map->timestamp++;
}

#define	vm_map_lock_read(map) rw_enter_read(&(map)->lock)

#define	vm_map_unlock(map)						\
do {									\
	if (((map)->flags & VM_MAP_INTRSAFE) == 0)			\
		rw_exit(&(map)->lock);					\
} while (0)

#define	vm_map_unlock_read(map)	rw_exit_read(&(map)->lock)

#define	vm_map_downgrade(map) rw_enter(&(map)->lock, RW_DOWNGRADE)

#define	vm_map_upgrade(map)						\
do {									\
	rw_exit_read(&(map)->lock);					\
	rw_enter_write(&(map)->lock);					\
} while (0)

#define	vm_map_busy(map)						\
do {									\
	(map)->flags |= VM_MAP_BUSY;					\
} while (0)
a476 9
#define	vm_map_unbusy(map)						\
do {									\
	int oflags;							\
									\
	oflags = (map)->flags;						\
	(map)->flags &= ~(VM_MAP_BUSY|VM_MAP_WANTLOCK);			\
	if (oflags & VM_MAP_WANTLOCK)					\
		wakeup(&(map)->flags);					\
} while (0)
@


1.45
log
@Reimplement uvm/uvm_map.

vmmap is designed to perform address space randomized allocations,
without letting fragmentation of the address space go through the roof.

Some highlights:
- kernel address space randomization
- proper implementation of guardpages
- roughly 10% system time reduction during kernel build

Tested by alot of people on tech@@ and developers.
Theo's machines are still happy.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.44 2010/12/24 21:49:04 tedu Exp $	*/
d4 1
a4 16
/*
 * Copyright (c) 2011 Ariane van der Steldt <ariane@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 *
 * 
a77 10
 * Internal functions.
 *
 * Required by clipping macros.
 */
void			 uvm_map_clip_end(struct vm_map*, struct vm_map_entry*,
			    vaddr_t);
void			 uvm_map_clip_start(struct vm_map*,
			    struct vm_map_entry*, vaddr_t);

/*
d84 2
a85 6
#define UVM_MAP_CLIP_START(_map, _entry, _addr)				\
	do {								\
		KASSERT((_entry)->end + (_entry)->fspace > (_addr));	\
		if ((_entry)->start < (_addr))				\
			uvm_map_clip_start((_map), (_entry), (_addr));	\
	} while (0)
d94 2
a95 6
#define UVM_MAP_CLIP_END(_map, _entry, _addr)				\
	do {								\
		KASSERT((_entry)->start < (_addr));			\
		if ((_entry)->end > (_addr))				\
			uvm_map_clip_end((_map), (_entry), (_addr));	\
	} while (0)
d100 3
d136 5
a140 7
	union {
		RB_ENTRY(vm_map_entry)	addr_entry; /* address tree */
		TAILQ_ENTRY(vm_map_entry) deadq; /* dead entry queue */
	} daddrs;
	RB_ENTRY(vm_map_entry)	free_entry;	/* free-space tree */

#define uvm_map_entry_start_copy start
a142 4

	vsize_t			guard;		/* bytes in guard */
	vsize_t			fspace;		/* free space */

a144 2
	struct vm_aref		aref;		/* anonymous overlay */

a145 1

a148 1

d150 1
d159 5
a165 7
TAILQ_HEAD(uvm_map_deadq, vm_map_entry);	/* dead entry queue */
RB_HEAD(uvm_map_addr, vm_map_entry);
RB_HEAD(uvm_map_free_int, vm_map_entry);
RB_PROTOTYPE(uvm_map_addr, vm_map_entry, daddrs.addr_entry,
    uvm_mapentry_addrcmp);
RB_PROTOTYPE(uvm_map_free_int, vm_map_entry, free_entry, uvm_mapentry_freecmp);

d167 4
a170 13
 * Tree with size information.
 */
struct uvm_map_free {
	struct uvm_map_free_int	tree;		/* Tree of free items. */
	size_t			treesz;		/* Size of tree. */
};

/*
 *	A Map is a rbtree of map entries, kept sorted by address.
 *	In addition, free space entries are also kept in a rbtree,
 *	indexed by free size.
 *
 *
a216 28
 *
 *		VM_MAP_GUARDPAGES	r/o; must be specified at map
 *					initialization time.
 *					If set, guards will appear between
 *					automatic allocations.
 *					No locking required.
 *
 *		VM_MAP_ISVMSPACE	r/o; set by uvmspace_alloc.
 *					Signifies that this map is a vmspace.
 *					(The implementation treats all maps
 *					without this bit as kernel maps.)
 *					No locking required.
 *
 *
 * All automatic allocations (uvm_map without MAP_FIXED) will allocate
 * from vm_map.free.
 * If that allocation fails:
 * - vmspace maps will spill over into vm_map.bfree,
 * - all other maps will call uvm_map_kmem_grow() to increase the arena.
 * 
 * vmspace maps have their data, brk() and stack arenas automatically
 * updated when uvm_map() is invoked without MAP_FIXED.
 * The spill over arena (vm_map.bfree) will contain the space in the brk()
 * and stack ranges.
 * Kernel maps never have a bfree arena and this tree will always be empty.
 *
 *
 * read_locks and write_locks are used in lock debugging code.
d221 3
a223 5

	struct uvm_map_addr	addr;		/* Entry tree, by addr */
	struct uvm_map_free	free;		/* Free space tree */
	struct uvm_map_free	bfree;		/* brk() space tree */

d227 3
d232 2
a233 11

	vaddr_t			min_offset;	/* First address in map. */
	vaddr_t			max_offset;	/* Last address in map. */

	/*
	 * Allocation overflow regions.
	 */
	vaddr_t			b_start;	/* Start for brk() alloc. */
	vaddr_t			b_end;		/* End for brk() alloc. */
	vaddr_t			s_start;	/* Start for stack alloc. */
	vaddr_t			s_end;		/* End for stack alloc. */
a241 2
#define VM_MAP_GUARDPAGES	0x20		/* rw: add guard pgs to map */
#define VM_MAP_ISVMSPACE	0x40		/* ro: map is a vmspace */
d246 1
a246 6
#ifdef KVA_GUARDPAGES
/* Sufficient for UVM_KM_MAXPAGES_HIWAT(8192) + overhead. */
#define	MAX_KMAPENT	8192 + 1024
#else
#define	MAX_KMAPENT	1024	/* XXXCDC: no crash */
#endif
d282 2
d285 4
a288 2
int		uvm_map_extract(struct vm_map*, vaddr_t, vsize_t, vaddr_t*,
		    int);
d290 2
a291 1
vaddr_t		uvm_map_hint(struct proc *, vm_prot_t);
d296 1
d303 5
a307 6
void		uvm_unmap(vm_map_t, vaddr_t, vaddr_t);
int		uvm_map_mquery(struct vm_map*, vaddr_t*, vsize_t, voff_t, int);

void		uvm_unmap_detach(struct uvm_map_deadq*, int);
void		uvm_unmap_remove(struct vm_map*, vaddr_t, vaddr_t,
		    struct uvm_map_deadq*, boolean_t, boolean_t);
d340 1
a340 4
/*
 * XXX: clean up later
 * Half the kernel seems to depend on them being included here.
 */
d342 43
a384 1
#include <sys/systm.h>  /* for panic() */
d386 20
a405 31
boolean_t	vm_map_lock_try_ln(struct vm_map*, char*, int);
void		vm_map_lock_ln(struct vm_map*, char*, int);
void		vm_map_lock_read_ln(struct vm_map*, char*, int);
void		vm_map_unlock_ln(struct vm_map*, char*, int);
void		vm_map_unlock_read_ln(struct vm_map*, char*, int);
void		vm_map_downgrade_ln(struct vm_map*, char*, int);
void		vm_map_upgrade_ln(struct vm_map*, char*, int);
void		vm_map_busy_ln(struct vm_map*, char*, int);
void		vm_map_unbusy_ln(struct vm_map*, char*, int);

#ifdef DIAGNOSTIC
#define vm_map_lock_try(map)	vm_map_lock_try_ln(map, __FILE__, __LINE__)
#define vm_map_lock(map)	vm_map_lock_ln(map, __FILE__, __LINE__)
#define vm_map_lock_read(map)	vm_map_lock_read_ln(map, __FILE__, __LINE__)
#define vm_map_unlock(map)	vm_map_unlock_ln(map, __FILE__, __LINE__)
#define vm_map_unlock_read(map)	vm_map_unlock_read_ln(map, __FILE__, __LINE__)
#define vm_map_downgrade(map)	vm_map_downgrade_ln(map, __FILE__, __LINE__)
#define vm_map_upgrade(map)	vm_map_upgrade_ln(map, __FILE__, __LINE__)
#define vm_map_busy(map)	vm_map_busy_ln(map, __FILE__, __LINE__)
#define vm_map_unbusy(map)	vm_map_unbusy_ln(map, __FILE__, __LINE__)
#else
#define vm_map_lock_try(map)	vm_map_lock_try_ln(map, NULL, 0)
#define vm_map_lock(map)	vm_map_lock_ln(map, NULL, 0)
#define vm_map_lock_read(map)	vm_map_lock_read_ln(map, NULL, 0)
#define vm_map_unlock(map)	vm_map_unlock_ln(map, NULL, 0)
#define vm_map_unlock_read(map)	vm_map_unlock_read_ln(map, NULL, 0)
#define vm_map_downgrade(map)	vm_map_downgrade_ln(map, NULL, 0)
#define vm_map_upgrade(map)	vm_map_upgrade_ln(map, NULL, 0)
#define vm_map_busy(map)	vm_map_busy_ln(map, NULL, 0)
#define vm_map_unbusy(map)	vm_map_unbusy_ln(map, NULL, 0)
#endif
d407 9
@


1.44
log
@add a param to uvm_map_hint to not skip over the heap, and use it as a last
resort if mmap fails otherwise to enable more complete address space
utilization.  tested for a while with no ill effects.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.43 2010/04/20 22:05:44 tedu Exp $	*/
d4 16
a19 1
/* 
d93 10
d109 6
a114 2
#define UVM_MAP_CLIP_START(MAP,ENTRY,VA) { \
	if ((VA) > (ENTRY)->start) uvm_map_clip_start(MAP,ENTRY,VA); }
d123 6
a128 2
#define UVM_MAP_CLIP_END(MAP,ENTRY,VA) { \
	if ((VA) < (ENTRY)->end) uvm_map_clip_end(MAP,ENTRY,VA); }
a132 3
#define UVM_EXTRACT_REMOVE	0x1	/* remove mapping from old map */
#define UVM_EXTRACT_CONTIG	0x2	/* try to keep it contig */
#define UVM_EXTRACT_QREF	0x4	/* use quick refs */
d166 7
a172 5
	RB_ENTRY(vm_map_entry)	rb_entry;	/* tree information */
	vaddr_t			ownspace;	/* free space after */
	vaddr_t			space;		/* space in subtree */
	struct vm_map_entry	*prev;		/* previous entry */
	struct vm_map_entry	*next;		/* next entry */
d175 4
d181 2
d184 1
d188 1
a189 1
	struct vm_aref		aref;		/* anonymous overlay */
d198 9
d208 6
a213 5
 * Marks the map entry as a guard page, using vm_map_entry.etype.
 */
#define MAP_ET_KVAGUARD		0x10		/* guard entry */

#define	VM_MAPENT_ISWIRED(entry)	((entry)->wired_count != 0)
d216 5
a220 4
 *	Maps are doubly-linked lists of map entries, kept sorted
 *	by address.  A single hint is provided to start
 *	searches again from the last successful search,
 *	insertion, or removal.
d267 28
d299 5
a303 3
	RB_HEAD(uvm_tree, vm_map_entry) rbhead;	/* Tree for entries */
	struct vm_map_entry	header;		/* List of entries */
	int			nentries;	/* Number of entries */
a306 3
	vm_map_entry_t		hint;		/* hint for quick lookups */
	simple_lock_data_t	hint_lock;	/* lock for hint storage */
	vm_map_entry_t		first_free;	/* First free space hint */
d309 11
a319 2
#define	min_offset		header.start
#define max_offset		header.end
d328 2
d334 6
a339 1
#define	MAX_KMAPENT	1024  /* XXXCDC: no crash */
a374 2
void		uvm_map_clip_start(vm_map_t, vm_map_entry_t, vaddr_t);
void		uvm_map_clip_end(vm_map_t, vm_map_entry_t, vaddr_t);
d376 2
a377 4
int		uvm_map_extract(vm_map_t, vaddr_t, vsize_t, 
		    vm_map_t, vaddr_t *, int);
vm_map_entry_t	uvm_map_findspace(vm_map_t, vaddr_t, vsize_t, vaddr_t *,
		    struct uvm_object *, voff_t, vsize_t, int);
d379 1
a379 2
#define		uvm_map_hint(p, prot) uvm_map_hint1(p, prot, 1)
vaddr_t		uvm_map_hint1(struct proc *, vm_prot_t, int);
a383 1
void		uvm_map_reference(vm_map_t);
d390 6
a395 5
#define		uvm_unmap(_m, _s, _e) uvm_unmap_p(_m, _s, _e, 0)
void		uvm_unmap_p(vm_map_t, vaddr_t, vaddr_t, struct proc *);
void		uvm_unmap_detach(vm_map_entry_t,int);
void		uvm_unmap_remove(vm_map_t, vaddr_t, vaddr_t, vm_map_entry_t *,
		    struct proc *, boolean_t);
d428 4
a431 1
/* XXX: clean up later */
d433 1
a433 1
#include <sys/systm.h>	/* for panic() */
d435 31
a465 41
static __inline boolean_t vm_map_lock_try(vm_map_t);
static __inline void vm_map_lock(vm_map_t);
extern const char vmmapbsy[];

static __inline boolean_t
vm_map_lock_try(struct vm_map *map)
{
	boolean_t rv;

	if (map->flags & VM_MAP_INTRSAFE) {
		rv = TRUE;
	} else {
		if (map->flags & VM_MAP_BUSY) {
			return (FALSE);
		}
		rv = (rw_enter(&map->lock, RW_WRITE|RW_NOSLEEP) == 0);
	}

	if (rv)
		map->timestamp++;

	return (rv);
}

static __inline void
vm_map_lock(struct vm_map *map)
{
	if (map->flags & VM_MAP_INTRSAFE)
		return;

	do {
		while (map->flags & VM_MAP_BUSY) {
			map->flags |= VM_MAP_WANTLOCK;
			tsleep(&map->flags, PVM, (char *)vmmapbsy, 0);
		}
	} while (rw_enter(&map->lock, RW_WRITE|RW_SLEEPFAIL) != 0);

	map->timestamp++;
}

#define	vm_map_lock_read(map) rw_enter_read(&(map)->lock)
a466 30
#define	vm_map_unlock(map)						\
do {									\
	if (((map)->flags & VM_MAP_INTRSAFE) == 0)			\
		rw_exit(&(map)->lock);					\
} while (0)

#define	vm_map_unlock_read(map)	rw_exit_read(&(map)->lock)

#define	vm_map_downgrade(map) rw_enter(&(map)->lock, RW_DOWNGRADE)

#define	vm_map_upgrade(map)						\
do {									\
	rw_exit_read(&(map)->lock);					\
	rw_enter_write(&(map)->lock);					\
} while (0)

#define	vm_map_busy(map)						\
do {									\
	(map)->flags |= VM_MAP_BUSY;					\
} while (0)

#define	vm_map_unbusy(map)						\
do {									\
	int oflags;							\
									\
	oflags = (map)->flags;						\
	(map)->flags &= ~(VM_MAP_BUSY|VM_MAP_WANTLOCK);			\
	if (oflags & VM_MAP_WANTLOCK)					\
		wakeup(&(map)->flags);					\
} while (0)
@


1.43
log
@remove proc.h include from uvm_map.h.  This has far reaching effects, as
sysctl.h was reliant on this particular include, and many drivers included
sysctl.h unnecessarily.  remove sysctl.h or add proc.h as needed.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.42 2009/08/28 00:40:03 ariane Exp $	*/
d290 2
a291 1
vaddr_t		uvm_map_hint(struct proc *, vm_prot_t);
@


1.42
log
@kva_guardpages: make guard pages separate map entries
- fixes ps(1)
- fixes kva deadbeef entries
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.41 2009/07/25 12:55:40 miod Exp $	*/
a340 1
#include <sys/proc.h>	/* for tsleep(), wakeup() */
@


1.41
log
@Add an extra argument to uvm_unmap_remove(), for the caller to tell it
whether removing holes or parts of them is allowed or not.
Only allow hole removal in uvmspace_free(), when tearing the vmspace down.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.40 2009/03/25 20:00:18 oga Exp $	*/
d157 1
d159 4
a162 1
};
@


1.40
log
@Move all of the pseudo-inline functions in uvm into C files.

By pseudo-inline, I mean that if a certain macro was defined, they would
be inlined. However, no architecture defines that, and none has for a
very very long time. Therefore mainly this just makes the code a damned
sight easier to read. Some k&r -> ansi declarations while I'm in there.

"just commit it" art@@. ok weingart@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.39 2008/07/18 16:40:17 kurt Exp $	*/
d302 1
a302 1
		    struct proc *);
@


1.39
log
@Add new uvm function called uvm_map_pie() which takes align as a
parameter and returns an aligned random load address for position
independent executables to use. This also adds three new vmparam.h
defines to specify the maximum address, minimum address and minimum
allowed alignment for uvm_map_pie() to use. The PIE address range
for i386 was carefully selected to work well within the i386 W^X
framework.

With much help and feedback from weingart@@.
okay weingart@@, miod@@, kettenis@@, drahn@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.38 2007/04/11 12:10:42 art Exp $	*/
a261 10
 * handle inline options
 */

#ifdef UVM_MAP_INLINE
#define MAP_INLINE static __inline
#else 
#define MAP_INLINE /* nothing */
#endif /* UVM_MAP_INLINE */

/*
a274 1
MAP_INLINE
a279 1
MAP_INLINE
d282 1
a282 1
			vm_map_t, vaddr_t *, int);
d284 1
a284 1
			struct uvm_object *, voff_t, vsize_t, int);
a290 1
MAP_INLINE
d293 1
a293 1
			vm_map_entry_t, int);
d295 1
a295 1
			vaddr_t *);
a298 1
MAP_INLINE
d301 2
a302 2
void		uvm_unmap_remove(vm_map_t, vaddr_t, vaddr_t,
				      vm_map_entry_t *, struct proc *);
@


1.38
log
@Instead of managing pages for intrsafe maps in special objects (aka.
kmem_object) just so that we can remove them, just use pmap_extract
to get the pages to free and simplify a lot of code to not deal with
the list of intrsafe maps, intrsafe objects, etc.

miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.37 2007/04/04 18:02:59 art Exp $	*/
d297 1
@


1.37
log
@Switch vm_map lock to rwlock.
Use a simple "rw_exit_read(); rw_enter_write();" for lock upgrade, since
that's what lockmgr did anyway.

deraadt@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.36 2007/03/25 13:02:51 thib Exp $	*/
a259 28

LIST_HEAD(vmi_list, vm_map_intrsafe);
#ifdef _KERNEL
extern simple_lock_data_t vmi_list_slock;
extern struct vmi_list vmi_list;

static __inline int vmi_list_lock(void);
static __inline void vmi_list_unlock(int);

static __inline int
vmi_list_lock()
{
	int s;

	s = splhigh();
	simple_lock(&vmi_list_slock);
	return (s);
}

static __inline void
vmi_list_unlock(s)
	int s;
{

	simple_unlock(&vmi_list_slock);
	splx(s);
}
#endif /* _KERNEL */
@


1.36
log
@Remove the flags_lock simplelock from struct vm_map;
Cleanup the code accordingly.

ok pedro@@, art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.35 2007/01/12 21:11:38 mickey Exp $	*/
d73 1
a73 3
/*
 * uvm_map.h
 */
a77 4
 * macros
 */

/*
d216 1
a216 1
	lock_data_t		lock;		/* Lock for map data */
d386 1
a386 2
vm_map_lock_try(map)
	vm_map_t map;
d390 3
a392 3
	if (map->flags & VM_MAP_INTRSAFE)
		rv = simple_lock_try(&map->lock.lk_interlock);
	else {
d396 1
a396 1
		rv = (lockmgr(&map->lock, LK_EXCLUSIVE|LK_NOWAIT, NULL) == 0);
d406 1
a406 2
vm_map_lock(map)
	vm_map_t map;
d408 1
a408 4
	int error;

	if (map->flags & VM_MAP_INTRSAFE) {
		simple_lock(&map->lock.lk_interlock);
a409 1
	}
d411 6
a416 11
 try_again:
	while (map->flags & VM_MAP_BUSY) {
		map->flags |= VM_MAP_WANTLOCK;
		ltsleep(&map->flags, PVM, (char *)vmmapbsy, 0, NULL);
	}

	error = lockmgr(&map->lock, LK_EXCLUSIVE|LK_SLEEPFAIL, NULL);

	if (error) {
		goto try_again;
	}
d418 1
a418 1
	(map)->timestamp++;
d421 1
a421 11
#ifdef DIAGNOSTIC
#define	vm_map_lock_read(map)						\
do {									\
	if (map->flags & VM_MAP_INTRSAFE)				\
		panic("vm_map_lock_read: intrsafe map");		\
	(void) lockmgr(&(map)->lock, LK_SHARED, NULL);			\
} while (0)
#else
#define	vm_map_lock_read(map)						\
	(void) lockmgr(&(map)->lock, LK_SHARED, NULL)
#endif
d425 2
a426 4
	if ((map)->flags & VM_MAP_INTRSAFE)				\
		simple_unlock(&(map)->lock.lk_interlock);		\
	else								\
		(void) lockmgr(&(map)->lock, LK_RELEASE, NULL);		\
d429 1
a429 2
#define	vm_map_unlock_read(map)						\
	(void) lockmgr(&(map)->lock, LK_RELEASE, NULL)
d431 1
a431 2
#define	vm_map_downgrade(map)						\
	(void) lockmgr(&(map)->lock, LK_DOWNGRADE, NULL)
a432 1
#ifdef DIAGNOSTIC
d435 2
a436 2
	if (lockmgr(&(map)->lock, LK_UPGRADE, NULL) != 0)		\
		panic("vm_map_upgrade: failed to upgrade lock");	\
a437 4
#else
#define	vm_map_upgrade(map)						\
	(void) lockmgr(&(map)->lock, LK_UPGRADE, NULL)
#endif
@


1.35
log
@proper define for MAX_KMAPENT; miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.34 2005/11/19 02:18:02 pedro Exp $	*/
a232 1
	simple_lock_data_t	flags_lock;	/* Lock for flags field */
a253 1
	simple_lock(&(map)->flags_lock);				\
a254 1
	simple_unlock(&(map)->flags_lock);				\
a399 1
		simple_lock(&map->flags_lock);
a400 1
			simple_unlock(&map->flags_lock);
d403 1
a403 2
		rv = (lockmgr(&map->lock, LK_EXCLUSIVE|LK_NOWAIT|LK_INTERLOCK,
		    &map->flags_lock) == 0);
a423 1
	simple_lock(&map->flags_lock);
d426 1
a426 1
		ltsleep(&map->flags, PVM, (char *)vmmapbsy, 0, &map->flags_lock);
d429 1
a429 2
	error = lockmgr(&map->lock, LK_EXCLUSIVE|LK_SLEEPFAIL|LK_INTERLOCK,
	    &map->flags_lock);
a476 1
	simple_lock(&(map)->flags_lock);				\
a477 1
	simple_unlock(&(map)->flags_lock);				\
a483 1
	simple_lock(&(map)->flags_lock);				\
a485 1
	simple_unlock(&(map)->flags_lock);				\
@


1.34
log
@Remove unnecessary lockmgr() archaism that was costing too much in terms
of panics and bugfixes. Access curproc directly, do not expect a process
pointer as an argument. Should fix many "process context required" bugs.
Incentive and okay millert@@, okay marc@@. Various testing, thanks.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.33 2005/09/30 02:42:57 brad Exp $	*/
d249 1
a249 5
#if (50 + (2 * NPROC) > 1000)
#define MAX_KMAPENT (50 + (2 * NPROC))
#else
#define	MAX_KMAPENT	1000  /* XXXCDC: no crash */
#endif
@


1.33
log
@revert MAX_KMAPENT hack added just before 3.7 now that
a proper fix has been implemented in uvm_mapent_alloc().

ok pedro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.32 2005/05/24 21:11:47 tedu Exp $	*/
d413 1
a413 1
		    &map->flags_lock, curproc) == 0);
d441 1
a441 1
	    &map->flags_lock, curproc);
d455 1
a455 1
	(void) lockmgr(&(map)->lock, LK_SHARED, NULL, curproc);		\
d459 1
a459 1
	(void) lockmgr(&(map)->lock, LK_SHARED, NULL, curproc)
d467 1
a467 1
		(void) lockmgr(&(map)->lock, LK_RELEASE, NULL, curproc);\
d471 1
a471 1
	(void) lockmgr(&(map)->lock, LK_RELEASE, NULL, curproc)
d474 1
a474 1
	(void) lockmgr(&(map)->lock, LK_DOWNGRADE, NULL, curproc)
d479 1
a479 1
	if (lockmgr(&(map)->lock, LK_UPGRADE, NULL, curproc) != 0)	\
d484 1
a484 1
	(void) lockmgr(&(map)->lock, LK_UPGRADE, NULL, curproc)
@


1.32
log
@add a new field to vm_space and use it to track the number of anon
pages a process uses.  this is now the userland "data size" value.
ok art deraadt tdeval.  thanks testers.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.31 2005/03/01 01:28:39 henning Exp $	*/
a247 6
#if defined(__i386__) || defined(__amd64__) || defined (__sparc64__)
#define _MAX_KMAPENT	2000
#else
#define _MAX_KMAPENT	1000
#endif

d249 1
a249 1
#if (50 + (2 * NPROC) > _MAX_KMAPENT)
d252 1
a252 1
#define	MAX_KMAPENT	_MAX_KMAPENT  /* XXXCDC: no crash */
@


1.31
log
@only use MAX_KMAPENT 2000 on i386 amd64 sparc64 and revert to 1000 on all
other archs for now, beck theo ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.30 2005/02/19 17:58:03 henning Exp $	*/
d357 1
d359 1
a359 1
void		uvm_unmap(vm_map_t, vaddr_t, vaddr_t);
d362 1
a362 1
				      vm_map_entry_t *);
@


1.30
log
@double default MAX_KMAPENT to 2000, theo ok
everybody please update your trees and test this, we need to find out
wether there is bad side-effects from the doubling. If this does not get
enough testing by our user community we will play safe and revert this for
the 3.7 release, so please test.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.29 2003/04/14 04:53:51 art Exp $	*/
d248 6
d255 1
a255 1
#if (50 + (2 * NPROC) > 2000)
d258 1
a258 1
#define	MAX_KMAPENT	2000  /* XXXCDC: no crash */
@


1.29
log
@There are two related changes.

The first one is an mquery(2) syscall. It's for asking the VM system
about where to map things. It will be used by ld.so, read the man page
for details.

The second change is related and is a centralization of uvm_map hint
that all callers of uvm_map calculated. This will allow us to adjust
this hint on architectures that have segments for non-exec mappings.

deraadt@@ drahn@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.28 2002/10/29 18:30:21 art Exp $	*/
d249 1
a249 1
#if (50 + (2 * NPROC) > 1000)
d252 1
a252 1
#define	MAX_KMAPENT	1000  /* XXXCDC: no crash */
@


1.28
log
@Since memory deallocation can't fail, remove the error return from
uvm_unmap, uvm_deallocate and a few other functions.
Simplifies some code and reduces diff to the UBC branch.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.27 2002/06/11 06:38:01 art Exp $	*/
d338 1
@


1.27
log
@Remove a stupid assert that grows the kernel by 40kB.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.26 2002/03/14 01:27:18 millert Exp $	*/
d351 1
a351 1
int		uvm_unmap(vm_map_t, vaddr_t, vaddr_t);
d353 1
a353 1
int		uvm_unmap_remove(vm_map_t, vaddr_t, vaddr_t,
@


1.26
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.25 2002/03/07 01:08:57 provos Exp $	*/
a441 1
		KASSERT(error == ENOLCK);
@


1.25
log
@use an augmented red-black tree to keep track of free space in the vm_map.
uvm_tree_sanity is left as debugging help but needs to be enabled manually.
okay art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.24 2002/02/28 18:50:26 provos Exp $	*/
d279 2
a280 2
static __inline int vmi_list_lock __P((void));
static __inline void vmi_list_unlock __P((int));
d327 1
a327 1
void		uvm_map_deallocate __P((vm_map_t));
d329 3
a331 3
int		uvm_map_clean __P((vm_map_t, vaddr_t, vaddr_t, int));
void		uvm_map_clip_start __P((vm_map_t, vm_map_entry_t, vaddr_t));
void		uvm_map_clip_end __P((vm_map_t, vm_map_entry_t, vaddr_t));
d333 9
a341 9
vm_map_t	uvm_map_create __P((pmap_t, vaddr_t, vaddr_t, int));
int		uvm_map_extract __P((vm_map_t, vaddr_t, vsize_t, 
			vm_map_t, vaddr_t *, int));
vm_map_entry_t	uvm_map_findspace __P((vm_map_t, vaddr_t, vsize_t, vaddr_t *,
			struct uvm_object *, voff_t, vsize_t, int));
int		uvm_map_inherit __P((vm_map_t, vaddr_t, vaddr_t, vm_inherit_t));
int		uvm_map_advice __P((vm_map_t, vaddr_t, vaddr_t, int));
void		uvm_map_init __P((void));
boolean_t	uvm_map_lookup_entry __P((vm_map_t, vaddr_t, vm_map_entry_t *));
d343 7
a349 7
void		uvm_map_reference __P((vm_map_t));
int		uvm_map_replace __P((vm_map_t, vaddr_t, vaddr_t,
			vm_map_entry_t, int));
int		uvm_map_reserve __P((vm_map_t, vsize_t, vaddr_t, vsize_t,
			vaddr_t *));
void		uvm_map_setup __P((vm_map_t, vaddr_t, vaddr_t, int));
int		uvm_map_submap __P((vm_map_t, vaddr_t, vaddr_t, vm_map_t));
d351 4
a354 4
int		uvm_unmap __P((vm_map_t, vaddr_t, vaddr_t));
void		uvm_unmap_detach __P((vm_map_entry_t,int));
int		uvm_unmap_remove __P((vm_map_t, vaddr_t, vaddr_t,
				      vm_map_entry_t *));
d392 2
a393 2
static __inline boolean_t vm_map_lock_try __P((vm_map_t));
static __inline void vm_map_lock __P((vm_map_t));
@


1.24
log
@use red-black tree for lookup_entry.  the red-black tree case for
map_findspace is still broken on alpha.  this will make debugging easier.
okay millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.22 2002/02/25 00:20:45 provos Exp $	*/
d143 2
@


1.23
log
@back out red-black tree. they are very fast but alpha UVM is broken and
the tree triggers the bug, PMAP_PREFER case was broken also.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.21 2002/02/18 10:02:20 art Exp $	*/
d142 1
d221 1
@


1.22
log
@use a red-black tree to find entries in the vm_map. augment the red-black
tree to find free space between entries.  speeds up memory allocation,
etc...
@
text
@a141 3
	RB_ENTRY(vm_map_entry)	rb_entry;	/* tree information */
	vaddr_t			ownspace;	/* free space after */
	vaddr_t			space;		/* space in subtree */
a219 1
	RB_HEAD(uvm_tree, vm_map_entry) rbhead;	/* Tree for entries */
@


1.21
log
@From the UBC branch and NetBSD.

We allocate map entries for the non-intrsafe kernel map (most notably
kernel_map and exec_map) from a pool that's backed by kmem_map (to avoid
deadlocking).

This should get rid of MAX_KMAPENT panics.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.20 2001/12/19 08:58:07 art Exp $	*/
d142 3
d223 1
@


1.20
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.15 2001/11/12 01:26:09 art Exp $	*/
d159 1
@


1.19
log
@Yet another sync to NetBSD uvm.
Today we add a pmap argument to pmap_update() and allocate map entries for
kernel_map from kmem_map instead of using the static entries. This should
get rid of MAX_KMAPENT panics. Also some uvm_loan problems are fixed.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.h,v 1.18 2001/11/28 19:28:15 art Exp $	*/
/*	$NetBSD: uvm_map.h,v 1.30 2001/09/09 19:38:23 chs Exp $	*/
d4 1
a4 1
/*
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and
d48 1
a48 1
 *
d54 3
a56 3
 *
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
d58 1
a58 1
 *
d86 1
a86 1
 *
d116 20
d146 1
a146 4
	union {
		struct uvm_object *uvm_obj;	/* uvm object */
		struct vm_map	*sub_map;	/* belongs to another map */
	} object;				/* object I point to */
a158 1
#define UVM_MAP_KMEM		0x02		/* from kmem entry pool */
d218 1
a218 1
	struct lock		lock;		/* Lock for map data */
d223 4
a226 4
	struct simplelock	ref_lock;	/* Lock for ref_count field */
	struct vm_map_entry *	hint;		/* hint for quick lookups */
	struct simplelock	hint_lock;	/* lock for hint storage */
	struct vm_map_entry *	first_free;	/* First free space hint */
d228 1
a228 1
	struct simplelock	flags_lock;	/* Lock for flags field */
d261 37
d303 1
a303 1
#else
d322 1
a322 1
void		uvm_map_deallocate __P((struct vm_map *));
d324 3
a326 5
int		uvm_map_clean __P((struct vm_map *, vaddr_t, vaddr_t, int));
void		uvm_map_clip_start __P((struct vm_map *, struct vm_map_entry *,
		    vaddr_t));
void		uvm_map_clip_end __P((struct vm_map *, struct vm_map_entry *,
		    vaddr_t));
d328 7
a334 8
struct vm_map	*uvm_map_create __P((pmap_t, vaddr_t, vaddr_t, int));
int		uvm_map_extract __P((struct vm_map *, vaddr_t, vsize_t,
		    struct vm_map *, vaddr_t *, int));
struct vm_map_entry *uvm_map_findspace __P((struct vm_map *, vaddr_t, vsize_t,
		    vaddr_t *, struct uvm_object *, voff_t, vsize_t, int));
int		uvm_map_inherit __P((struct vm_map *, vaddr_t, vaddr_t,
		    vm_inherit_t));
int		uvm_map_advice __P((struct vm_map *, vaddr_t, vaddr_t, int));
d336 1
a336 2
boolean_t	uvm_map_lookup_entry __P((struct vm_map *, vaddr_t,
		    struct vm_map_entry **));
d338 7
a344 8
void		uvm_map_reference __P((struct vm_map *));
int		uvm_map_replace __P((struct vm_map *, vaddr_t, vaddr_t,
		    struct vm_map_entry *, int));
int		uvm_map_reserve __P((struct vm_map *, vsize_t, vaddr_t, vsize_t,
		    vaddr_t *));
void		uvm_map_setup __P((struct vm_map *, vaddr_t, vaddr_t, int));
int		uvm_map_submap __P((struct vm_map *, vaddr_t, vaddr_t,
		    struct vm_map *));
d346 4
a349 4
void		uvm_unmap __P((struct vm_map *, vaddr_t, vaddr_t));
void		uvm_unmap_detach __P((struct vm_map_entry *,int));
void		uvm_unmap_remove __P((struct vm_map *, vaddr_t, vaddr_t,
		    struct vm_map_entry **));
d387 2
a388 2
static __inline boolean_t vm_map_lock_try __P((struct vm_map *));
static __inline void vm_map_lock __P((struct vm_map *));
d393 1
a393 1
	struct vm_map *map;
d417 1
a417 1
	struct vm_map *map;
d430 1
a430 1
		ltsleep(&map->flags, PVM, vmmapbsy, 0, &map->flags_lock);
@


1.19.2.1
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.h,v 1.19 2001/12/04 23:22:42 art Exp $	*/
/*	$NetBSD: uvm_map.h,v 1.31 2001/10/03 13:32:23 christos Exp $	*/
d399 1
a399 1
	if ((map)->flags & VM_MAP_INTRSAFE)				\
@


1.19.2.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.19.2.1 2002/02/02 03:28:27 art Exp $	*/
a121 3
	RB_ENTRY(vm_map_entry)	rb_entry;	/* tree information */
	vaddr_t			ownspace;	/* free space after */
	vaddr_t			space;		/* space in subtree */
a202 1
	RB_HEAD(uvm_tree, vm_map_entry) rbhead;	/* Tree for entries */
d269 1
a269 1
void		uvm_map_deallocate(struct vm_map *);
d271 5
a275 5
int		uvm_map_clean(struct vm_map *, vaddr_t, vaddr_t, int);
void		uvm_map_clip_start(struct vm_map *, struct vm_map_entry *,
		    vaddr_t);
void		uvm_map_clip_end(struct vm_map *, struct vm_map_entry *,
		    vaddr_t);
d277 11
a287 11
struct vm_map	*uvm_map_create(pmap_t, vaddr_t, vaddr_t, int);
int		uvm_map_extract(struct vm_map *, vaddr_t, vsize_t,
		    struct vm_map *, vaddr_t *, int);
struct vm_map_entry *uvm_map_findspace(struct vm_map *, vaddr_t, vsize_t,
		    vaddr_t *, struct uvm_object *, voff_t, vsize_t, int);
int		uvm_map_inherit(struct vm_map *, vaddr_t, vaddr_t,
		    vm_inherit_t);
int		uvm_map_advice(struct vm_map *, vaddr_t, vaddr_t, int);
void		uvm_map_init(void);
boolean_t	uvm_map_lookup_entry(struct vm_map *, vaddr_t,
		    struct vm_map_entry **);
d289 8
a296 8
void		uvm_map_reference(struct vm_map *);
int		uvm_map_replace(struct vm_map *, vaddr_t, vaddr_t,
		    struct vm_map_entry *, int);
int		uvm_map_reserve(struct vm_map *, vsize_t, vaddr_t, vsize_t,
		    vaddr_t *);
void		uvm_map_setup(struct vm_map *, vaddr_t, vaddr_t, int);
int		uvm_map_submap(struct vm_map *, vaddr_t, vaddr_t,
		    struct vm_map *);
d298 4
a301 4
void		uvm_unmap(struct vm_map *, vaddr_t, vaddr_t);
void		uvm_unmap_detach(struct vm_map_entry *, int);
void		uvm_unmap_remove(struct vm_map *, vaddr_t, vaddr_t,
		    struct vm_map_entry **);
d339 2
a340 2
static __inline boolean_t vm_map_lock_try(struct vm_map *);
static __inline void vm_map_lock(struct vm_map *);
@


1.19.2.3
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.19.2.2 2002/06/11 03:33:03 art Exp $	*/
d393 1
@


1.19.2.4
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.h,v 1.19.2.3 2002/10/29 00:36:50 art Exp $	*/
/*	$NetBSD: uvm_map.h,v 1.33 2002/11/02 07:40:49 perry Exp $	*/
a227 1
#define	VM_MAP_DYING		0x20		/* rw: map is being destroyed */
d245 1
a245 1
} while (/*CONSTCOND*/ 0)
d405 1
a405 1
} while (/*CONSTCOND*/ 0)
d417 1
a417 1
} while (/*CONSTCOND*/ 0)
d430 1
a430 1
} while (/*CONSTCOND*/ 0)
d441 1
a441 1
} while (/*CONSTCOND*/ 0)
d453 1
a453 1
} while (/*CONSTCOND*/ 0)
@


1.19.2.5
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a289 1
vaddr_t		uvm_map_hint(struct proc *, vm_prot_t);
@


1.18
log
@Sync in more uvm from NetBSD. Mostly just cosmetic stuff.
Contains also support for page coloring.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.h,v 1.17 2001/11/28 13:47:40 art Exp $	*/
/*	$NetBSD: uvm_map.h,v 1.29 2001/06/26 17:55:15 thorpej Exp $	*/
d142 1
@


1.17
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.h,v 1.16 2001/11/27 05:27:12 art Exp $	*/
/*	$NetBSD: uvm_map.h,v 1.25 2001/03/15 06:10:57 chs Exp $	*/
d4 1
a4 1
/* 
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and 
d48 1
a48 1
 * 
d54 3
a56 3
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
d58 1
a58 1
 * 
d86 1
a86 1
 * 
a115 20
 * types defined:
 *
 *	vm_map_t		the high-level address map data structure.
 *	vm_map_entry_t		an entry in an address map.
 *	vm_map_version_t	a timestamp of a map, for use with vm_map_lookup
 */

/*
 * Objects which live in maps may be either VM objects, or another map
 * (called a "sharing map") which denotes read-write sharing with other maps.
 *
 * XXXCDC: private pager data goes here now
 */

union vm_map_object {
	struct uvm_object	*uvm_obj;	/* UVM OBJECT */
	struct vm_map		*sub_map;	/* belongs to another map */
};

/*
d126 4
a129 1
	union vm_map_object	object;		/* object I point to */
d201 1
a201 1
	lock_data_t		lock;		/* Lock for map data */
d206 4
a209 4
	simple_lock_data_t	ref_lock;	/* Lock for ref_count field */
	vm_map_entry_t		hint;		/* hint for quick lookups */
	simple_lock_data_t	hint_lock;	/* lock for hint storage */
	vm_map_entry_t		first_free;	/* First free space hint */
d211 1
a211 1
	simple_lock_data_t	flags_lock;	/* Lock for flags field */
a243 37
 *	Interrupt-safe maps must also be kept on a special list,
 *	to assist uvm_fault() in avoiding locking problems.
 */
struct vm_map_intrsafe {
	struct vm_map	vmi_map;
	LIST_ENTRY(vm_map_intrsafe) vmi_list;
};

LIST_HEAD(vmi_list, vm_map_intrsafe);
#ifdef _KERNEL
extern simple_lock_data_t vmi_list_slock;
extern struct vmi_list vmi_list;

static __inline int vmi_list_lock __P((void));
static __inline void vmi_list_unlock __P((int));

static __inline int
vmi_list_lock()
{
	int s;

	s = splhigh();
	simple_lock(&vmi_list_slock);
	return (s);
}

static __inline void
vmi_list_unlock(s)
	int s;
{

	simple_unlock(&vmi_list_slock);
	splx(s);
}
#endif /* _KERNEL */

/*
d249 1
a249 1
#else 
d268 1
a268 1
void		uvm_map_deallocate __P((vm_map_t));
d270 5
a274 3
int		uvm_map_clean __P((vm_map_t, vaddr_t, vaddr_t, int));
void		uvm_map_clip_start __P((vm_map_t, vm_map_entry_t, vaddr_t));
void		uvm_map_clip_end __P((vm_map_t, vm_map_entry_t, vaddr_t));
d276 8
a283 7
vm_map_t	uvm_map_create __P((pmap_t, vaddr_t, vaddr_t, int));
int		uvm_map_extract __P((vm_map_t, vaddr_t, vsize_t, 
			vm_map_t, vaddr_t *, int));
vm_map_entry_t	uvm_map_findspace __P((vm_map_t, vaddr_t, vsize_t, vaddr_t *,
			struct uvm_object *, voff_t, vsize_t, int));
int		uvm_map_inherit __P((vm_map_t, vaddr_t, vaddr_t, vm_inherit_t));
int		uvm_map_advice __P((vm_map_t, vaddr_t, vaddr_t, int));
d285 2
a286 1
boolean_t	uvm_map_lookup_entry __P((vm_map_t, vaddr_t, vm_map_entry_t *));
d288 8
a295 7
void		uvm_map_reference __P((vm_map_t));
int		uvm_map_replace __P((vm_map_t, vaddr_t, vaddr_t,
			vm_map_entry_t, int));
int		uvm_map_reserve __P((vm_map_t, vsize_t, vaddr_t, vsize_t,
			vaddr_t *));
void		uvm_map_setup __P((vm_map_t, vaddr_t, vaddr_t, int));
int		uvm_map_submap __P((vm_map_t, vaddr_t, vaddr_t, vm_map_t));
d297 4
a300 4
void		uvm_unmap __P((vm_map_t, vaddr_t, vaddr_t));
void		uvm_unmap_detach __P((vm_map_entry_t,int));
void		uvm_unmap_remove __P((vm_map_t, vaddr_t, vaddr_t,
				      vm_map_entry_t *));
d338 2
a339 2
static __inline boolean_t vm_map_lock_try __P((vm_map_t));
static __inline void vm_map_lock __P((vm_map_t));
d344 1
a344 1
	vm_map_t map;
d368 1
a368 1
	vm_map_t map;
@


1.16
log
@Merge in the unified buffer cache code as found in NetBSD 2001/03/10. The
code is written mostly by Chuck Silvers <chuq@@chuq.com>/<chs@@netbsd.org>.

Tested for the past few weeks by many developers, should be in a pretty stable
state, but will require optimizations and additional cleanups.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.h,v 1.15 2001/11/12 01:26:09 art Exp $	*/
/*	$NetBSD: uvm_map.h,v 1.24 2001/02/18 21:19:08 chs Exp $	*/
d346 1
a346 1
int		uvm_unmap __P((vm_map_t, vaddr_t, vaddr_t));
d348 1
a348 1
int		uvm_unmap_remove __P((vm_map_t, vaddr_t, vaddr_t,
@


1.15
log
@Bring in more changes from NetBSD. Mostly pagedaemon improvements.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.14 2001/11/07 02:55:50 art Exp $	*/
d430 1
a430 1
		ltsleep(&map->flags, PVM, (char *)vmmapbsy, 0, &map->flags_lock);
@


1.14
log
@Another sync of uvm to NetBSD. Just minor fiddling, no major changes.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.h,v 1.13 2001/11/07 01:18:01 art Exp $	*/
/*	$NetBSD: uvm_map.h,v 1.22 2000/09/13 15:00:25 thorpej Exp $	*/
d389 1
d430 1
a430 1
		ltsleep(&map->flags, PVM, "vmmapbsy", 0, &map->flags_lock);
d437 1
a437 4
#ifdef DIAGNOSTIC
		if (error != ENOLCK)
			panic("vm_map_lock: failed to get lock");
#endif
@


1.13
log
@Add an alignment argument to uvm_map that specifies an alignment hint
for the virtual address.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.12 2001/11/06 13:36:52 art Exp $	*/
d116 1
a116 1
 *	Types defined:
d124 2
a125 3
 *	Objects which live in maps may be either VM objects, or
 *	another map (called a "sharing map") which denotes read-write
 *	sharing with other maps.
d136 4
a139 4
 *	Address map entries consist of start and end addresses,
 *	a VM object (or sharing map) and offset into that object,
 *	and user-exported inheritance and protection information.
 *	Also included is control information for virtual copy operations.
@


1.12
log
@More sync to NetBSD.
 - Use malloc/free instead of MALLOC/FREE for variable sized allocations.
 - Move the memory inheritance code to sys/mman.h and rename from VM_* to MAP_*
 - various cleanups and simplifications.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.h,v 1.11 2001/11/06 00:27:01 art Exp $	*/
/*	$NetBSD: uvm_map.h,v 1.21 2000/08/16 16:32:06 thorpej Exp $	*/
d333 1
a333 1
			struct uvm_object *, voff_t, boolean_t));
d342 2
a343 1
int		uvm_map_reserve __P((vm_map_t, vsize_t, vaddr_t, vaddr_t *));
@


1.11
log
@Sync to NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.h,v 1.10 2001/08/12 22:41:15 mickey Exp $	*/
/*	$NetBSD: uvm_map.h,v 1.19 2000/06/26 17:18:40 mrg Exp $	*/
a242 1
#define MAX_KMAP	10
d427 1
a427 1
	if (map->flags & VM_MAP_BUSY) {
d429 1
a429 3
		simple_unlock(&map->flags_lock);
		(void) tsleep(&map->flags, PVM, "vmmapbsy", 0);
		goto try_again;
@


1.10
log
@merge vm_map.h into uvm_map.h, kinda matches netbsd's approach
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.h,v 1.9 2001/08/11 10:57:22 art Exp $	*/
/*	$NetBSD: uvm_map.h,v 1.17 2000/03/29 04:05:47 simonb Exp $	*/
a148 1
	/* etype is a bitmap that replaces the following 4 items */
a149 1
		/* Only in task maps: */
d160 1
d163 1
a163 1
#define		VM_MAPENT_ISWIRED(entry)	((entry)->wired_count != 0)
d236 3
a238 3
#define VM_MAP_PAGEABLE		0x01		/* ro: entries are pageable*/
#define VM_MAP_INTRSAFE		0x02		/* ro: interrupt safe map */
#define VM_MAP_WIREFUTURE	0x04		/* rw: wire future mappings */
d242 11
d263 2
a264 2
 *     Interrupt-safe maps must also be kept on a special list,
 *     to assist uvm_fault() in avoiding locking problems.
d267 1
a267 1
	struct vm_map   vmi_map;
d300 55
d445 1
a445 1
 
a511 67

/* XXX: number of kernel maps and entries to statically allocate */
#ifndef	MAX_KMAP
#define	MAX_KMAP	20
#endif
#ifndef	MAX_KMAPENT
#if (50 + (2 * NPROC) > 1000)
#define MAX_KMAPENT (50 + (2 * NPROC))
#else
#define	MAX_KMAPENT	1000  /* XXXCDC: no crash */
#endif
#endif

/*
 * handle inline options
 */

#ifdef UVM_MAP_INLINE
#define MAP_INLINE static __inline
#else 
#define MAP_INLINE /* nothing */
#endif /* UVM_MAP_INLINE */

/*
 * globals:
 */

#ifdef _KERNEL

#ifdef PMAP_GROWKERNEL
extern vaddr_t	uvm_maxkaddr;
#endif

/*
 * protos: the following prototypes define the interface to vm_map
 */

MAP_INLINE
void		uvm_map_deallocate __P((vm_map_t));

int		uvm_map_clean __P((vm_map_t, vaddr_t, vaddr_t, int));
void		uvm_map_clip_start __P((vm_map_t, vm_map_entry_t, vaddr_t));
void		uvm_map_clip_end __P((vm_map_t, vm_map_entry_t, vaddr_t));
MAP_INLINE
vm_map_t	uvm_map_create __P((pmap_t, vaddr_t, vaddr_t, int));
int		uvm_map_extract __P((vm_map_t, vaddr_t, vsize_t, 
			vm_map_t, vaddr_t *, int));
vm_map_entry_t	uvm_map_findspace __P((vm_map_t, vaddr_t, vsize_t, vaddr_t *,
			struct uvm_object *, voff_t, boolean_t));
int		uvm_map_inherit __P((vm_map_t, vaddr_t, vaddr_t, vm_inherit_t));
int		uvm_map_advice __P((vm_map_t, vaddr_t, vaddr_t, int));
void		uvm_map_init __P((void));
boolean_t	uvm_map_lookup_entry __P((vm_map_t, vaddr_t, vm_map_entry_t *));
MAP_INLINE
void		uvm_map_reference __P((vm_map_t));
int		uvm_map_replace __P((vm_map_t, vaddr_t, vaddr_t,
			vm_map_entry_t, int));
int		uvm_map_reserve __P((vm_map_t, vsize_t, vaddr_t, vaddr_t *));
void		uvm_map_setup __P((vm_map_t, vaddr_t, vaddr_t, int));
int		uvm_map_submap __P((vm_map_t, vaddr_t, vaddr_t, vm_map_t));
MAP_INLINE
int		uvm_unmap __P((vm_map_t, vaddr_t, vaddr_t));
void		uvm_unmap_detach __P((vm_map_entry_t,int));
int		uvm_unmap_remove __P((vm_map_t, vaddr_t, vaddr_t,
				      vm_map_entry_t *));

#endif /* _KERNEL */
@


1.9
log
@Various random fixes from NetBSD.
Including support for zeroing pages in the idle loop (not enabled yet).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.8 2001/08/06 14:03:05 art Exp $	*/
d111 348
d473 2
@


1.8
log
@Add a new type voff_t (right now it's typedefed as off_t) used for offsets
into objects.

Gives the possibilty to mmap beyond the size of vaddr_t.

From NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.h,v 1.7 2001/03/22 03:05:55 smart Exp $	*/
/*	$NetBSD: uvm_map.h,v 1.16 2000/03/26 20:54:47 kleink Exp $	*/
a161 2

struct vmspace *uvmspace_fork __P((struct vmspace *));
@


1.7
log
@Sync style, typo, and comments a little closer to NetBSD.  art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.h,v 1.6 2001/03/09 05:34:38 smart Exp $	*/
/*	$NetBSD: uvm_map.h,v 1.15 1999/06/21 17:25:11 thorpej Exp $	*/
d145 1
a145 1
			struct uvm_object *, vaddr_t, boolean_t));
@


1.6
log
@Protect protypes, certain macros, and inlines from userland.  Checked userland
with a 'make build'.  From NetBSD.  art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.5 2001/01/29 02:07:46 niklas Exp $	*/
d138 2
a139 4
void		uvm_map_clip_start __P((vm_map_t,
				vm_map_entry_t, vaddr_t));
void		uvm_map_clip_end __P((vm_map_t, vm_map_entry_t,
				vaddr_t));
d141 1
a141 2
vm_map_t	uvm_map_create __P((pmap_t, vaddr_t, 
			vaddr_t, int));
d144 3
a146 5
vm_map_entry_t	uvm_map_findspace __P((vm_map_t, vaddr_t, vsize_t,
			vaddr_t *, struct uvm_object *, vaddr_t, 
			boolean_t));
int		uvm_map_inherit __P((vm_map_t, vaddr_t, vaddr_t,
			vm_inherit_t));
d149 1
a149 2
boolean_t	uvm_map_lookup_entry __P((vm_map_t, vaddr_t, 
			vm_map_entry_t *));
d152 1
a152 1
int		uvm_map_replace __P((vm_map_t, vaddr_t, vaddr_t, 
d154 3
a156 6
int		uvm_map_reserve __P((vm_map_t, vsize_t, vaddr_t, 
			vaddr_t *));
void		uvm_map_setup __P((vm_map_t, vaddr_t, 
			vaddr_t, int));
int		uvm_map_submap __P((vm_map_t, vaddr_t, 
			vaddr_t, vm_map_t));
@


1.5
log
@$OpenBSD$
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_map.h,v 1.14 1999/05/26 19:16:36 thorpej Exp $	*/
/*	$NetBSD: uvm_map.h,v 1.14 1999/05/26 19:16:36 thorpej Exp $	*/
d77 2
d173 2
@


1.4
log
@Bring in some new UVM code from NetBSD (not current).

 - Introduce a new type of map that are interrupt safe and never allow faults
   in them. mb_map and kmem_map are made intrsafe.
 - Add "access protection" to uvm_vslock (to be passed down to uvm_fault and
   later to pmap_enter).
 - madvise(2) now works.
 - various cleanups.
@
text
@d1 1
@


1.3
log
@sync with NetBSD from 1999.05.24 (there is a reason for this date)
 Mostly cleanups, but also a few improvements to pagedaemon for better
 handling of low memory and/or low swap conditions.
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_map.h,v 1.11 1999/03/25 18:48:52 mrg Exp $	*/
d141 1
a141 1
			vaddr_t, boolean_t));
d149 1
d160 1
a160 1
			vaddr_t, boolean_t));
@


1.3.4.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_map.h,v 1.14 1999/05/26 19:16:36 thorpej Exp $	*/
d141 1
a141 1
			vaddr_t, int));
a148 1
int		uvm_map_advice __P((vm_map_t, vaddr_t, vaddr_t, int));
d159 1
a159 1
			vaddr_t, int));
@


1.3.4.2
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_map.h,v 1.7 2001/03/22 03:05:55 smart Exp $	*/
/*	$NetBSD: uvm_map.h,v 1.15 1999/06/21 17:25:11 thorpej Exp $	*/
a75 2
#ifdef _KERNEL

d135 4
a138 2
void		uvm_map_clip_start __P((vm_map_t, vm_map_entry_t, vaddr_t));
void		uvm_map_clip_end __P((vm_map_t, vm_map_entry_t, vaddr_t));
d140 2
a141 1
vm_map_t	uvm_map_create __P((pmap_t, vaddr_t, vaddr_t, int));
d144 5
a148 3
vm_map_entry_t	uvm_map_findspace __P((vm_map_t, vaddr_t, vsize_t, vaddr_t *,
			struct uvm_object *, vaddr_t, boolean_t));
int		uvm_map_inherit __P((vm_map_t, vaddr_t, vaddr_t, vm_inherit_t));
d151 2
a152 1
boolean_t	uvm_map_lookup_entry __P((vm_map_t, vaddr_t, vm_map_entry_t *));
d155 1
a155 1
int		uvm_map_replace __P((vm_map_t, vaddr_t, vaddr_t,
d157 6
a162 3
int		uvm_map_reserve __P((vm_map_t, vsize_t, vaddr_t, vaddr_t *));
void		uvm_map_setup __P((vm_map_t, vaddr_t, vaddr_t, int));
int		uvm_map_submap __P((vm_map_t, vaddr_t, vaddr_t, vm_map_t));
a169 2

#endif /* _KERNEL */
@


1.3.4.3
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_map.h,v 1.17 2000/03/29 04:05:47 simonb Exp $	*/
a110 348
#endif /* _KERNEL */

#include <uvm/uvm_anon.h>

/*
 *	Types defined:
 *
 *	vm_map_t		the high-level address map data structure.
 *	vm_map_entry_t		an entry in an address map.
 *	vm_map_version_t	a timestamp of a map, for use with vm_map_lookup
 */

/*
 *	Objects which live in maps may be either VM objects, or
 *	another map (called a "sharing map") which denotes read-write
 *	sharing with other maps.
 *
 * XXXCDC: private pager data goes here now
 */

union vm_map_object {
	struct uvm_object	*uvm_obj;	/* UVM OBJECT */
	struct vm_map		*sub_map;	/* belongs to another map */
};

/*
 *	Address map entries consist of start and end addresses,
 *	a VM object (or sharing map) and offset into that object,
 *	and user-exported inheritance and protection information.
 *	Also included is control information for virtual copy operations.
 */
struct vm_map_entry {
	struct vm_map_entry	*prev;		/* previous entry */
	struct vm_map_entry	*next;		/* next entry */
	vaddr_t			start;		/* start address */
	vaddr_t			end;		/* end address */
	union vm_map_object	object;		/* object I point to */
	voff_t			offset;		/* offset into object */
	/* etype is a bitmap that replaces the following 4 items */
	int			etype;		/* entry type */
		/* Only in task maps: */
	vm_prot_t		protection;	/* protection code */
	vm_prot_t		max_protection;	/* maximum protection */
	vm_inherit_t		inheritance;	/* inheritance */
	int			wired_count;	/* can be paged if == 0 */
	struct vm_aref		aref;		/* anonymous overlay */
	int			advice;		/* madvise advice */
#define uvm_map_entry_stop_copy flags
	u_int8_t		flags;		/* flags */

#define UVM_MAP_STATIC		0x01		/* static map entry */
};

#define		VM_MAPENT_ISWIRED(entry)	((entry)->wired_count != 0)

/*
 *	Maps are doubly-linked lists of map entries, kept sorted
 *	by address.  A single hint is provided to start
 *	searches again from the last successful search,
 *	insertion, or removal.
 *
 *	LOCKING PROTOCOL NOTES:
 *	-----------------------
 *
 *	VM map locking is a little complicated.  There are both shared
 *	and exclusive locks on maps.  However, it is sometimes required
 *	to downgrade an exclusive lock to a shared lock, and upgrade to
 *	an exclusive lock again (to perform error recovery).  However,
 *	another thread *must not* queue itself to receive an exclusive
 *	lock while before we upgrade back to exclusive, otherwise the
 *	error recovery becomes extremely difficult, if not impossible.
 *
 *	In order to prevent this scenario, we introduce the notion of
 *	a `busy' map.  A `busy' map is read-locked, but other threads
 *	attempting to write-lock wait for this flag to clear before
 *	entering the lock manager.  A map may only be marked busy
 *	when the map is write-locked (and then the map must be downgraded
 *	to read-locked), and may only be marked unbusy by the thread
 *	which marked it busy (holding *either* a read-lock or a
 *	write-lock, the latter being gained by an upgrade).
 *
 *	Access to the map `flags' member is controlled by the `flags_lock'
 *	simple lock.  Note that some flags are static (set once at map
 *	creation time, and never changed), and thus require no locking
 *	to check those flags.  All flags which are r/w must be set or
 *	cleared while the `flags_lock' is asserted.  Additional locking
 *	requirements are:
 *
 *		VM_MAP_PAGEABLE		r/o static flag; no locking required
 *
 *		VM_MAP_INTRSAFE		r/o static flag; no locking required
 *
 *		VM_MAP_WIREFUTURE	r/w; may only be set or cleared when
 *					map is write-locked.  may be tested
 *					without asserting `flags_lock'.
 *
 *		VM_MAP_BUSY		r/w; may only be set when map is
 *					write-locked, may only be cleared by
 *					thread which set it, map read-locked
 *					or write-locked.  must be tested
 *					while `flags_lock' is asserted.
 *
 *		VM_MAP_WANTLOCK		r/w; may only be set when the map
 *					is busy, and thread is attempting
 *					to write-lock.  must be tested
 *					while `flags_lock' is asserted.
 */
struct vm_map {
	struct pmap *		pmap;		/* Physical map */
	lock_data_t		lock;		/* Lock for map data */
	struct vm_map_entry	header;		/* List of entries */
	int			nentries;	/* Number of entries */
	vsize_t			size;		/* virtual size */
	int			ref_count;	/* Reference count */
	simple_lock_data_t	ref_lock;	/* Lock for ref_count field */
	vm_map_entry_t		hint;		/* hint for quick lookups */
	simple_lock_data_t	hint_lock;	/* lock for hint storage */
	vm_map_entry_t		first_free;	/* First free space hint */
	int			flags;		/* flags */
	simple_lock_data_t	flags_lock;	/* Lock for flags field */
	unsigned int		timestamp;	/* Version number */
#define	min_offset		header.start
#define max_offset		header.end
};

/* vm_map flags */
#define VM_MAP_PAGEABLE		0x01		/* ro: entries are pageable*/
#define VM_MAP_INTRSAFE		0x02		/* ro: interrupt safe map */
#define VM_MAP_WIREFUTURE	0x04		/* rw: wire future mappings */
#define	VM_MAP_BUSY		0x08		/* rw: map is busy */
#define	VM_MAP_WANTLOCK		0x10		/* rw: want to write-lock */

#ifdef _KERNEL
#define	vm_map_modflags(map, set, clear)				\
do {									\
	simple_lock(&(map)->flags_lock);				\
	(map)->flags = ((map)->flags | (set)) & ~(clear);		\
	simple_unlock(&(map)->flags_lock);				\
} while (0)
#endif /* _KERNEL */

/*
 *     Interrupt-safe maps must also be kept on a special list,
 *     to assist uvm_fault() in avoiding locking problems.
 */
struct vm_map_intrsafe {
	struct vm_map   vmi_map;
	LIST_ENTRY(vm_map_intrsafe) vmi_list;
};

LIST_HEAD(vmi_list, vm_map_intrsafe);
#ifdef _KERNEL
extern simple_lock_data_t vmi_list_slock;
extern struct vmi_list vmi_list;

static __inline int vmi_list_lock __P((void));
static __inline void vmi_list_unlock __P((int));

static __inline int
vmi_list_lock()
{
	int s;

	s = splhigh();
	simple_lock(&vmi_list_slock);
	return (s);
}

static __inline void
vmi_list_unlock(s)
	int s;
{

	simple_unlock(&vmi_list_slock);
	splx(s);
}
#endif /* _KERNEL */

/*
 * VM map locking operations:
 *
 *	These operations perform locking on the data portion of the
 *	map.
 *
 *	vm_map_lock_try: try to lock a map, failing if it is already locked.
 *
 *	vm_map_lock: acquire an exclusive (write) lock on a map.
 *
 *	vm_map_lock_read: acquire a shared (read) lock on a map.
 *
 *	vm_map_unlock: release an exclusive lock on a map.
 *
 *	vm_map_unlock_read: release a shared lock on a map.
 *
 *	vm_map_downgrade: downgrade an exclusive lock to a shared lock.
 *
 *	vm_map_upgrade: upgrade a shared lock to an exclusive lock.
 *
 *	vm_map_busy: mark a map as busy.
 *
 *	vm_map_unbusy: clear busy status on a map.
 *
 * Note that "intrsafe" maps use only exclusive, spin locks.  We simply
 * use the sleep lock's interlock for this.
 */

#ifdef _KERNEL
/* XXX: clean up later */
#include <sys/time.h>
#include <sys/proc.h>	/* for tsleep(), wakeup() */
#include <sys/systm.h>	/* for panic() */

static __inline boolean_t vm_map_lock_try __P((vm_map_t));
static __inline void vm_map_lock __P((vm_map_t));

static __inline boolean_t
vm_map_lock_try(map)
	vm_map_t map;
{
	boolean_t rv;

	if (map->flags & VM_MAP_INTRSAFE)
		rv = simple_lock_try(&map->lock.lk_interlock);
	else {
		simple_lock(&map->flags_lock);
		if (map->flags & VM_MAP_BUSY) {
			simple_unlock(&map->flags_lock);
			return (FALSE);
		}
		rv = (lockmgr(&map->lock, LK_EXCLUSIVE|LK_NOWAIT|LK_INTERLOCK,
		    &map->flags_lock, curproc) == 0);
	}

	if (rv)
		map->timestamp++;

	return (rv);
}

static __inline void
vm_map_lock(map)
	vm_map_t map;
{
	int error;

	if (map->flags & VM_MAP_INTRSAFE) {
		simple_lock(&map->lock.lk_interlock);
		return;
	}

 try_again:
	simple_lock(&map->flags_lock);
	if (map->flags & VM_MAP_BUSY) {
		map->flags |= VM_MAP_WANTLOCK;
		simple_unlock(&map->flags_lock);
		(void) tsleep(&map->flags, PVM, "vmmapbsy", 0);
		goto try_again;
	}

	error = lockmgr(&map->lock, LK_EXCLUSIVE|LK_SLEEPFAIL|LK_INTERLOCK,
	    &map->flags_lock, curproc);

	if (error) {
#ifdef DIAGNOSTIC
		if (error != ENOLCK)
			panic("vm_map_lock: failed to get lock");
#endif
		goto try_again;
	}
 
	(map)->timestamp++;
}

#ifdef DIAGNOSTIC
#define	vm_map_lock_read(map)						\
do {									\
	if (map->flags & VM_MAP_INTRSAFE)				\
		panic("vm_map_lock_read: intrsafe map");		\
	(void) lockmgr(&(map)->lock, LK_SHARED, NULL, curproc);		\
} while (0)
#else
#define	vm_map_lock_read(map)						\
	(void) lockmgr(&(map)->lock, LK_SHARED, NULL, curproc)
#endif

#define	vm_map_unlock(map)						\
do {									\
	if ((map)->flags & VM_MAP_INTRSAFE)				\
		simple_unlock(&(map)->lock.lk_interlock);		\
	else								\
		(void) lockmgr(&(map)->lock, LK_RELEASE, NULL, curproc);\
} while (0)

#define	vm_map_unlock_read(map)						\
	(void) lockmgr(&(map)->lock, LK_RELEASE, NULL, curproc)

#define	vm_map_downgrade(map)						\
	(void) lockmgr(&(map)->lock, LK_DOWNGRADE, NULL, curproc)

#ifdef DIAGNOSTIC
#define	vm_map_upgrade(map)						\
do {									\
	if (lockmgr(&(map)->lock, LK_UPGRADE, NULL, curproc) != 0)	\
		panic("vm_map_upgrade: failed to upgrade lock");	\
} while (0)
#else
#define	vm_map_upgrade(map)						\
	(void) lockmgr(&(map)->lock, LK_UPGRADE, NULL, curproc)
#endif

#define	vm_map_busy(map)						\
do {									\
	simple_lock(&(map)->flags_lock);				\
	(map)->flags |= VM_MAP_BUSY;					\
	simple_unlock(&(map)->flags_lock);				\
} while (0)

#define	vm_map_unbusy(map)						\
do {									\
	int oflags;							\
									\
	simple_lock(&(map)->flags_lock);				\
	oflags = (map)->flags;						\
	(map)->flags &= ~(VM_MAP_BUSY|VM_MAP_WANTLOCK);			\
	simple_unlock(&(map)->flags_lock);				\
	if (oflags & VM_MAP_WANTLOCK)					\
		wakeup(&(map)->flags);					\
} while (0)
#endif /* _KERNEL */

/*
 *	Functions implemented as macros
 */
#define		vm_map_min(map)		((map)->min_offset)
#define		vm_map_max(map)		((map)->max_offset)
#define		vm_map_pmap(map)	((map)->pmap)

/* XXX: number of kernel maps and entries to statically allocate */
#ifndef	MAX_KMAP
#define	MAX_KMAP	20
#endif
#ifndef	MAX_KMAPENT
#if (50 + (2 * NPROC) > 1000)
#define MAX_KMAPENT (50 + (2 * NPROC))
#else
#define	MAX_KMAPENT	1000  /* XXXCDC: no crash */
#endif
#endif
a125 2
#ifdef _KERNEL

d145 1
a145 1
			struct uvm_object *, voff_t, boolean_t));
d162 2
@


1.3.4.4
log
@merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_map.h,v 1.24 2001/02/18 21:19:08 chs Exp $	*/
d116 1
a116 1
 * types defined:
d124 3
a126 2
 * Objects which live in maps may be either VM objects, or another map
 * (called a "sharing map") which denotes read-write sharing with other maps.
d137 4
a140 4
 * Address map entries consist of start and end addresses,
 * a VM object (or sharing map) and offset into that object,
 * and user-exported inheritance and protection information.
 * Also included is control information for virtual copy operations.
d149 1
d151 1
a161 1

d164 1
a164 1
#define	VM_MAPENT_ISWIRED(entry)	((entry)->wired_count != 0)
d237 3
a239 3
#define	VM_MAP_PAGEABLE		0x01		/* ro: entries are pageable */
#define	VM_MAP_INTRSAFE		0x02		/* ro: interrupt safe map */
#define	VM_MAP_WIREFUTURE	0x04		/* rw: wire future mappings */
a242 10
/* XXX: number of kernel maps and entries to statically allocate */

#if !defined(MAX_KMAPENT)
#if (50 + (2 * NPROC) > 1000)
#define MAX_KMAPENT (50 + (2 * NPROC))
#else
#define	MAX_KMAPENT	1000  /* XXXCDC: no crash */
#endif
#endif	/* !defined MAX_KMAPENT */

d253 2
a254 2
 *	Interrupt-safe maps must also be kept on a special list,
 *	to assist uvm_fault() in avoiding locking problems.
d257 1
a257 1
	struct vm_map	vmi_map;
a289 56
 * handle inline options
 */

#ifdef UVM_MAP_INLINE
#define MAP_INLINE static __inline
#else 
#define MAP_INLINE /* nothing */
#endif /* UVM_MAP_INLINE */

/*
 * globals:
 */

#ifdef _KERNEL

#ifdef PMAP_GROWKERNEL
extern vaddr_t	uvm_maxkaddr;
#endif

/*
 * protos: the following prototypes define the interface to vm_map
 */

MAP_INLINE
void		uvm_map_deallocate __P((vm_map_t));

int		uvm_map_clean __P((vm_map_t, vaddr_t, vaddr_t, int));
void		uvm_map_clip_start __P((vm_map_t, vm_map_entry_t, vaddr_t));
void		uvm_map_clip_end __P((vm_map_t, vm_map_entry_t, vaddr_t));
MAP_INLINE
vm_map_t	uvm_map_create __P((pmap_t, vaddr_t, vaddr_t, int));
int		uvm_map_extract __P((vm_map_t, vaddr_t, vsize_t, 
			vm_map_t, vaddr_t *, int));
vm_map_entry_t	uvm_map_findspace __P((vm_map_t, vaddr_t, vsize_t, vaddr_t *,
			struct uvm_object *, voff_t, vsize_t, int));
int		uvm_map_inherit __P((vm_map_t, vaddr_t, vaddr_t, vm_inherit_t));
int		uvm_map_advice __P((vm_map_t, vaddr_t, vaddr_t, int));
void		uvm_map_init __P((void));
boolean_t	uvm_map_lookup_entry __P((vm_map_t, vaddr_t, vm_map_entry_t *));
MAP_INLINE
void		uvm_map_reference __P((vm_map_t));
int		uvm_map_replace __P((vm_map_t, vaddr_t, vaddr_t,
			vm_map_entry_t, int));
int		uvm_map_reserve __P((vm_map_t, vsize_t, vaddr_t, vsize_t,
			vaddr_t *));
void		uvm_map_setup __P((vm_map_t, vaddr_t, vaddr_t, int));
int		uvm_map_submap __P((vm_map_t, vaddr_t, vaddr_t, vm_map_t));
MAP_INLINE
int		uvm_unmap __P((vm_map_t, vaddr_t, vaddr_t));
void		uvm_unmap_detach __P((vm_map_entry_t,int));
int		uvm_unmap_remove __P((vm_map_t, vaddr_t, vaddr_t,
				      vm_map_entry_t *));

#endif /* _KERNEL */

/*
a324 1
extern const char vmmapbsy[];
d363 1
a363 1
	while (map->flags & VM_MAP_BUSY) {
d365 3
a367 1
		ltsleep(&map->flags, PVM, (char *)vmmapbsy, 0, &map->flags_lock);
d374 4
a377 1
		KASSERT(error == ENOLCK);
d380 1
a380 1

d447 67
@


1.3.4.5
log
@Merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_map.h,v 1.29 2001/06/26 17:55:15 thorpej Exp $	*/
d4 1
a4 1
/*
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and
d48 1
a48 1
 *
d54 3
a56 3
 *
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
d58 1
a58 1
 *
d86 1
a86 1
 *
d116 20
d146 1
a146 4
	union {
		struct uvm_object *uvm_obj;	/* uvm object */
		struct vm_map	*sub_map;	/* belongs to another map */
	} object;				/* object I point to */
d218 1
a218 1
	struct lock		lock;		/* Lock for map data */
d223 4
a226 4
	struct simplelock	ref_lock;	/* Lock for ref_count field */
	struct vm_map_entry *	hint;		/* hint for quick lookups */
	struct simplelock	hint_lock;	/* lock for hint storage */
	struct vm_map_entry *	first_free;	/* First free space hint */
d228 1
a228 1
	struct simplelock	flags_lock;	/* Lock for flags field */
d261 37
d303 1
a303 1
#else
d322 1
a322 1
void		uvm_map_deallocate __P((struct vm_map *));
d324 3
a326 5
int		uvm_map_clean __P((struct vm_map *, vaddr_t, vaddr_t, int));
void		uvm_map_clip_start __P((struct vm_map *, struct vm_map_entry *,
		    vaddr_t));
void		uvm_map_clip_end __P((struct vm_map *, struct vm_map_entry *,
		    vaddr_t));
d328 7
a334 8
struct vm_map	*uvm_map_create __P((pmap_t, vaddr_t, vaddr_t, int));
int		uvm_map_extract __P((struct vm_map *, vaddr_t, vsize_t,
		    struct vm_map *, vaddr_t *, int));
struct vm_map_entry *uvm_map_findspace __P((struct vm_map *, vaddr_t, vsize_t,
		    vaddr_t *, struct uvm_object *, voff_t, vsize_t, int));
int		uvm_map_inherit __P((struct vm_map *, vaddr_t, vaddr_t,
		    vm_inherit_t));
int		uvm_map_advice __P((struct vm_map *, vaddr_t, vaddr_t, int));
d336 1
a336 2
boolean_t	uvm_map_lookup_entry __P((struct vm_map *, vaddr_t,
		    struct vm_map_entry **));
d338 7
a344 8
void		uvm_map_reference __P((struct vm_map *));
int		uvm_map_replace __P((struct vm_map *, vaddr_t, vaddr_t,
		    struct vm_map_entry *, int));
int		uvm_map_reserve __P((struct vm_map *, vsize_t, vaddr_t, vsize_t,
		    vaddr_t *));
void		uvm_map_setup __P((struct vm_map *, vaddr_t, vaddr_t, int));
int		uvm_map_submap __P((struct vm_map *, vaddr_t, vaddr_t,
		    struct vm_map *));
d346 4
a349 4
void		uvm_unmap __P((struct vm_map *, vaddr_t, vaddr_t));
void		uvm_unmap_detach __P((struct vm_map_entry *,int));
void		uvm_unmap_remove __P((struct vm_map *, vaddr_t, vaddr_t,
		    struct vm_map_entry **));
d387 2
a388 2
static __inline boolean_t vm_map_lock_try __P((struct vm_map *));
static __inline void vm_map_lock __P((struct vm_map *));
d393 1
a393 1
	struct vm_map *map;
d417 1
a417 1
	struct vm_map *map;
d430 1
a430 1
		ltsleep(&map->flags, PVM, vmmapbsy, 0, &map->flags_lock);
@


1.3.4.6
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_map.h,v 1.24 2001/02/18 21:19:08 chs Exp $	*/
d4 1
a4 1
/* 
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and 
d48 1
a48 1
 * 
d54 3
a56 3
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
d58 1
a58 1
 * 
d86 1
a86 1
 * 
a115 20
 * types defined:
 *
 *	vm_map_t		the high-level address map data structure.
 *	vm_map_entry_t		an entry in an address map.
 *	vm_map_version_t	a timestamp of a map, for use with vm_map_lookup
 */

/*
 * Objects which live in maps may be either VM objects, or another map
 * (called a "sharing map") which denotes read-write sharing with other maps.
 *
 * XXXCDC: private pager data goes here now
 */

union vm_map_object {
	struct uvm_object	*uvm_obj;	/* UVM OBJECT */
	struct vm_map		*sub_map;	/* belongs to another map */
};

/*
a121 1
	RB_ENTRY(vm_map_entry)	rb_entry;	/* tree information */
d126 4
a129 1
	union vm_map_object	object;		/* object I point to */
a141 1
#define UVM_MAP_KMEM		0x02		/* from kmem entry pool */
d201 1
a201 2
	lock_data_t		lock;		/* Lock for map data */
	RB_HEAD(uvm_tree, vm_map_entry) rbhead;	/* Tree for entries */
d206 4
a209 4
	simple_lock_data_t	ref_lock;	/* Lock for ref_count field */
	vm_map_entry_t		hint;		/* hint for quick lookups */
	simple_lock_data_t	hint_lock;	/* lock for hint storage */
	vm_map_entry_t		first_free;	/* First free space hint */
d211 1
a211 1
	simple_lock_data_t	flags_lock;	/* Lock for flags field */
a243 37
 *	Interrupt-safe maps must also be kept on a special list,
 *	to assist uvm_fault() in avoiding locking problems.
 */
struct vm_map_intrsafe {
	struct vm_map	vmi_map;
	LIST_ENTRY(vm_map_intrsafe) vmi_list;
};

LIST_HEAD(vmi_list, vm_map_intrsafe);
#ifdef _KERNEL
extern simple_lock_data_t vmi_list_slock;
extern struct vmi_list vmi_list;

static __inline int vmi_list_lock __P((void));
static __inline void vmi_list_unlock __P((int));

static __inline int
vmi_list_lock()
{
	int s;

	s = splhigh();
	simple_lock(&vmi_list_slock);
	return (s);
}

static __inline void
vmi_list_unlock(s)
	int s;
{

	simple_unlock(&vmi_list_slock);
	splx(s);
}
#endif /* _KERNEL */

/*
d249 1
a249 1
#else 
d268 1
a268 1
void		uvm_map_deallocate __P((vm_map_t));
d270 5
a274 3
int		uvm_map_clean __P((vm_map_t, vaddr_t, vaddr_t, int));
void		uvm_map_clip_start __P((vm_map_t, vm_map_entry_t, vaddr_t));
void		uvm_map_clip_end __P((vm_map_t, vm_map_entry_t, vaddr_t));
d276 8
a283 7
vm_map_t	uvm_map_create __P((pmap_t, vaddr_t, vaddr_t, int));
int		uvm_map_extract __P((vm_map_t, vaddr_t, vsize_t, 
			vm_map_t, vaddr_t *, int));
vm_map_entry_t	uvm_map_findspace __P((vm_map_t, vaddr_t, vsize_t, vaddr_t *,
			struct uvm_object *, voff_t, vsize_t, int));
int		uvm_map_inherit __P((vm_map_t, vaddr_t, vaddr_t, vm_inherit_t));
int		uvm_map_advice __P((vm_map_t, vaddr_t, vaddr_t, int));
d285 2
a286 1
boolean_t	uvm_map_lookup_entry __P((vm_map_t, vaddr_t, vm_map_entry_t *));
d288 8
a295 7
void		uvm_map_reference __P((vm_map_t));
int		uvm_map_replace __P((vm_map_t, vaddr_t, vaddr_t,
			vm_map_entry_t, int));
int		uvm_map_reserve __P((vm_map_t, vsize_t, vaddr_t, vsize_t,
			vaddr_t *));
void		uvm_map_setup __P((vm_map_t, vaddr_t, vaddr_t, int));
int		uvm_map_submap __P((vm_map_t, vaddr_t, vaddr_t, vm_map_t));
d297 4
a300 4
int		uvm_unmap __P((vm_map_t, vaddr_t, vaddr_t));
void		uvm_unmap_detach __P((vm_map_entry_t,int));
int		uvm_unmap_remove __P((vm_map_t, vaddr_t, vaddr_t,
				      vm_map_entry_t *));
d338 2
a339 2
static __inline boolean_t vm_map_lock_try __P((vm_map_t));
static __inline void vm_map_lock __P((vm_map_t));
d344 1
a344 1
	vm_map_t map;
d368 1
a368 1
	vm_map_t map;
d381 1
a381 1
		ltsleep(&map->flags, PVM, (char *)vmmapbsy, 0, &map->flags_lock);
@


1.3.4.7
log
@Merge in -current from roughly a week ago
@
text
@a142 2
	vaddr_t			ownspace;	/* free space after */
	vaddr_t			space;		/* space in subtree */
d277 2
a278 2
static __inline int vmi_list_lock(void);
static __inline void vmi_list_unlock(int);
d325 1
a325 1
void		uvm_map_deallocate(vm_map_t);
d327 3
a329 3
int		uvm_map_clean(vm_map_t, vaddr_t, vaddr_t, int);
void		uvm_map_clip_start(vm_map_t, vm_map_entry_t, vaddr_t);
void		uvm_map_clip_end(vm_map_t, vm_map_entry_t, vaddr_t);
d331 9
a339 9
vm_map_t	uvm_map_create(pmap_t, vaddr_t, vaddr_t, int);
int		uvm_map_extract(vm_map_t, vaddr_t, vsize_t, 
			vm_map_t, vaddr_t *, int);
vm_map_entry_t	uvm_map_findspace(vm_map_t, vaddr_t, vsize_t, vaddr_t *,
			struct uvm_object *, voff_t, vsize_t, int);
int		uvm_map_inherit(vm_map_t, vaddr_t, vaddr_t, vm_inherit_t);
int		uvm_map_advice(vm_map_t, vaddr_t, vaddr_t, int);
void		uvm_map_init(void);
boolean_t	uvm_map_lookup_entry(vm_map_t, vaddr_t, vm_map_entry_t *);
d341 7
a347 7
void		uvm_map_reference(vm_map_t);
int		uvm_map_replace(vm_map_t, vaddr_t, vaddr_t,
			vm_map_entry_t, int);
int		uvm_map_reserve(vm_map_t, vsize_t, vaddr_t, vsize_t,
			vaddr_t *);
void		uvm_map_setup(vm_map_t, vaddr_t, vaddr_t, int);
int		uvm_map_submap(vm_map_t, vaddr_t, vaddr_t, vm_map_t);
d349 4
a352 4
int		uvm_unmap(vm_map_t, vaddr_t, vaddr_t);
void		uvm_unmap_detach(vm_map_entry_t,int);
int		uvm_unmap_remove(vm_map_t, vaddr_t, vaddr_t,
				      vm_map_entry_t *);
d390 2
a391 2
static __inline boolean_t vm_map_lock_try(vm_map_t);
static __inline void vm_map_lock(vm_map_t);
@


1.3.4.8
log
@Sync the SMP branch with 3.3
@
text
@d351 1
a351 1
void		uvm_unmap(vm_map_t, vaddr_t, vaddr_t);
d353 1
a353 1
void		uvm_unmap_remove(vm_map_t, vaddr_t, vaddr_t,
d442 1
@


1.3.4.9
log
@Sync the SMP branch to -current. This includes moving to ELF.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_map.h,v 1.3.4.8 2003/03/28 00:08:48 niklas Exp $	*/
a337 1
vaddr_t		uvm_map_hint(struct proc *, vm_prot_t);
@


1.2
log
@add OpenBSD tags
@
text
@d1 1
a1 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_map.h,v 1.10 1998/10/11 23:14:48 chuck Exp $	*/
a2 4
/*
 * XXXCDC: "ROUGH DRAFT" QUALITY UVM PRE-RELEASE FILE!
 *         >>>USE AT YOUR OWN RISK, WORK IS NOT FINISHED<<<
 */
d118 8
@


1.1
log
@Import of uvm from NetBSD. Some local changes, some code disabled
@
text
@d1 1
@

