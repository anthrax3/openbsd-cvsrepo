head	1.73;
access;
symbols
	OPENBSD_6_2_BASE:1.73
	OPENBSD_6_1:1.71.0.4
	OPENBSD_6_1_BASE:1.71
	OPENBSD_6_0:1.70.0.4
	OPENBSD_6_0_BASE:1.70
	OPENBSD_5_9:1.70.0.2
	OPENBSD_5_9_BASE:1.70
	OPENBSD_5_8:1.69.0.6
	OPENBSD_5_8_BASE:1.69
	OPENBSD_5_7:1.69.0.2
	OPENBSD_5_7_BASE:1.69
	OPENBSD_5_6:1.66.0.4
	OPENBSD_5_6_BASE:1.66
	OPENBSD_5_5:1.60.0.6
	OPENBSD_5_5_BASE:1.60
	OPENBSD_5_4:1.60.0.2
	OPENBSD_5_4_BASE:1.60
	OPENBSD_5_3:1.59.0.4
	OPENBSD_5_3_BASE:1.59
	OPENBSD_5_2:1.59.0.2
	OPENBSD_5_2_BASE:1.59
	OPENBSD_5_1_BASE:1.58
	OPENBSD_5_1:1.58.0.4
	OPENBSD_5_0:1.58.0.2
	OPENBSD_5_0_BASE:1.58
	OPENBSD_4_9:1.55.0.4
	OPENBSD_4_9_BASE:1.55
	OPENBSD_4_8:1.55.0.2
	OPENBSD_4_8_BASE:1.55
	OPENBSD_4_7:1.50.0.2
	OPENBSD_4_7_BASE:1.50
	OPENBSD_4_6:1.49.0.4
	OPENBSD_4_6_BASE:1.49
	OPENBSD_4_5:1.48.0.2
	OPENBSD_4_5_BASE:1.48
	OPENBSD_4_4:1.47.0.6
	OPENBSD_4_4_BASE:1.47
	OPENBSD_4_3:1.47.0.4
	OPENBSD_4_3_BASE:1.47
	OPENBSD_4_2:1.47.0.2
	OPENBSD_4_2_BASE:1.47
	OPENBSD_4_1:1.45.0.2
	OPENBSD_4_1_BASE:1.45
	OPENBSD_4_0:1.44.0.2
	OPENBSD_4_0_BASE:1.44
	OPENBSD_3_9:1.43.0.2
	OPENBSD_3_9_BASE:1.43
	OPENBSD_3_8:1.42.0.2
	OPENBSD_3_8_BASE:1.42
	OPENBSD_3_7:1.39.0.6
	OPENBSD_3_7_BASE:1.39
	OPENBSD_3_6:1.39.0.4
	OPENBSD_3_6_BASE:1.39
	SMP_SYNC_A:1.39
	SMP_SYNC_B:1.39
	OPENBSD_3_5:1.39.0.2
	OPENBSD_3_5_BASE:1.39
	OPENBSD_3_4:1.35.0.2
	OPENBSD_3_4_BASE:1.35
	UBC_SYNC_A:1.34
	OPENBSD_3_3:1.34.0.2
	OPENBSD_3_3_BASE:1.34
	OPENBSD_3_2:1.33.0.2
	OPENBSD_3_2_BASE:1.33
	OPENBSD_3_1:1.32.0.2
	OPENBSD_3_1_BASE:1.32
	UBC_SYNC_B:1.33
	UBC:1.30.0.2
	UBC_BASE:1.30
	OPENBSD_3_0:1.20.0.2
	OPENBSD_3_0_BASE:1.20
	OPENBSD_2_9_BASE:1.11
	OPENBSD_2_9:1.11.0.2
	OPENBSD_2_8:1.8.0.2
	OPENBSD_2_8_BASE:1.8
	OPENBSD_2_7:1.7.0.2
	OPENBSD_2_7_BASE:1.7
	SMP:1.5.0.4
	SMP_BASE:1.5
	kame_19991208:1.5
	OPENBSD_2_6:1.5.0.2
	OPENBSD_2_6_BASE:1.5
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.73
date	2017.05.08.09.32.19;	author mpi;	state Exp;
branches;
next	1.72;
commitid	GENXtBr8IlBkWBup;

1.72
date	2017.04.30.13.04.49;	author mpi;	state Exp;
branches;
next	1.71;
commitid	xDPbcPU6tYP39nZG;

1.71
date	2017.02.14.10.31.15;	author mpi;	state Exp;
branches;
next	1.70;
commitid	PmGi4EGraGC0Z0ml;

1.70
date	2015.09.13.17.08.03;	author guenther;	state Exp;
branches;
next	1.69;
commitid	BssYI6s6zozAyfkk;

1.69
date	2014.12.15.20.38.22;	author tedu;	state Exp;
branches;
next	1.68;
commitid	nNG4m4fWgAcWWv1G;

1.68
date	2014.12.05.04.12.48;	author uebayasi;	state Exp;
branches;
next	1.67;
commitid	lSMz5LkZVlVJKmFM;

1.67
date	2014.11.16.12.31.00;	author deraadt;	state Exp;
branches;
next	1.66;
commitid	yv0ECmCdICvq576h;

1.66
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.65;
commitid	7NtJNW9udCOFtDNM;

1.65
date	2014.05.15.03.52.25;	author guenther;	state Exp;
branches;
next	1.64;

1.64
date	2014.05.03.22.49.43;	author guenther;	state Exp;
branches;
next	1.63;

1.63
date	2014.05.03.22.44.36;	author guenther;	state Exp;
branches;
next	1.62;

1.62
date	2014.04.13.23.14.15;	author tedu;	state Exp;
branches;
next	1.61;

1.61
date	2014.04.03.21.40.10;	author tedu;	state Exp;
branches;
next	1.60;

1.60
date	2013.03.31.17.06.34;	author deraadt;	state Exp;
branches;
next	1.59;

1.59
date	2012.03.23.15.51.26;	author guenther;	state Exp;
branches;
next	1.58;

1.58
date	2011.04.15.21.47.24;	author oga;	state Exp;
branches;
next	1.57;

1.57
date	2011.04.07.13.20.25;	author miod;	state Exp;
branches;
next	1.56;

1.56
date	2011.04.01.15.43.13;	author art;	state Exp;
branches;
next	1.55;

1.55
date	2010.07.02.22.38.32;	author thib;	state Exp;
branches;
next	1.54;

1.54
date	2010.07.02.20.40.16;	author thib;	state Exp;
branches;
next	1.53;

1.53
date	2010.07.02.18.26.58;	author art;	state Exp;
branches;
next	1.52;

1.52
date	2010.07.01.21.27.39;	author art;	state Exp;
branches;
next	1.51;

1.51
date	2010.06.30.20.20.18;	author thib;	state Exp;
branches;
next	1.50;

1.50
date	2009.08.11.18.43.33;	author blambert;	state Exp;
branches;
next	1.49;

1.49
date	2009.03.20.15.19.04;	author oga;	state Exp;
branches;
next	1.48;

1.48
date	2008.10.24.06.32.46;	author deraadt;	state Exp;
branches;
next	1.47;

1.47
date	2007.05.26.20.26.51;	author pedro;	state Exp;
branches;
next	1.46;

1.46
date	2007.05.18.10.55.34;	author art;	state Exp;
branches;
next	1.45;

1.45
date	2006.11.29.12.24.18;	author miod;	state Exp;
branches;
next	1.44;

1.44
date	2006.05.25.22.42.22;	author miod;	state Exp;
branches;
next	1.43;

1.43
date	2005.11.01.14.18.14;	author aaron;	state Exp;
branches;
next	1.42;

1.42
date	2005.05.31.11.35.33;	author art;	state Exp;
branches;
next	1.41;

1.41
date	2005.05.29.03.20.43;	author deraadt;	state Exp;
branches;
next	1.40;

1.40
date	2005.05.25.23.17.47;	author niklas;	state Exp;
branches;
next	1.39;

1.39
date	2004.02.23.06.19.32;	author drahn;	state Exp;
branches;
next	1.38;

1.38
date	2003.11.20.17.54.55;	author grange;	state Exp;
branches;
next	1.37;

1.37
date	2003.11.20.14.44.31;	author grange;	state Exp;
branches;
next	1.36;

1.36
date	2003.11.08.06.11.10;	author nordin;	state Exp;
branches;
next	1.35;

1.35
date	2003.08.10.00.04.50;	author miod;	state Exp;
branches
	1.35.2.1;
next	1.34;

1.34
date	2003.01.29.22.57.10;	author mickey;	state Exp;
branches
	1.34.2.1;
next	1.33;

1.33
date	2002.06.09.02.11.47;	author jsyn;	state Exp;
branches;
next	1.32;

1.32
date	2002.03.14.01.27.18;	author millert;	state Exp;
branches;
next	1.31;

1.31
date	2001.12.19.08.58.07;	author art;	state Exp;
branches;
next	1.30;

1.30
date	2001.12.04.23.22.42;	author art;	state Exp;
branches
	1.30.2.1;
next	1.29;

1.29
date	2001.11.28.19.28.14;	author art;	state Exp;
branches;
next	1.28;

1.28
date	2001.11.28.14.29.13;	author art;	state Exp;
branches;
next	1.27;

1.27
date	2001.11.28.13.47.39;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2001.11.10.19.20.39;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2001.11.07.02.55.50;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2001.11.06.18.41.10;	author art;	state Exp;
branches;
next	1.23;

1.23
date	2001.11.06.13.36.52;	author art;	state Exp;
branches;
next	1.22;

1.22
date	2001.11.06.01.35.04;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2001.11.05.22.14.54;	author art;	state Exp;
branches;
next	1.20;

1.20
date	2001.09.19.20.50.59;	author mickey;	state Exp;
branches;
next	1.19;

1.19
date	2001.08.11.10.57.22;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2001.08.06.14.03.04;	author art;	state Exp;
branches;
next	1.17;

1.17
date	2001.07.25.13.25.33;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2001.06.23.19.24.33;	author smart;	state Exp;
branches;
next	1.15;

1.15
date	2001.06.08.08.09.39;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2001.05.07.16.08.40;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2001.05.05.23.25.54;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2001.05.05.21.26.45;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2001.04.03.15.28.06;	author aaron;	state Exp;
branches;
next	1.10;

1.10
date	2001.04.02.21.43.12;	author niklas;	state Exp;
branches;
next	1.9;

1.9
date	2001.01.29.02.07.44;	author niklas;	state Exp;
branches;
next	1.8;

1.8
date	2000.09.07.20.15.28;	author art;	state Exp;
branches;
next	1.7;

1.7
date	2000.03.16.22.11.04;	author art;	state Exp;
branches;
next	1.6;

1.6
date	2000.03.01.19.24.33;	author niklas;	state Exp;
branches;
next	1.5;

1.5
date	99.09.03.18.02.21;	author art;	state Exp;
branches
	1.5.4.1;
next	1.4;

1.4
date	99.08.23.08.13.23;	author art;	state Exp;
branches;
next	1.3;

1.3
date	99.08.17.10.32.19;	author niklas;	state Exp;
branches;
next	1.2;

1.2
date	99.02.26.05.32.06;	author art;	state Exp;
branches;
next	1.1;

1.1
date	99.02.26.01.30.12;	author art;	state Exp;
branches;
next	;

1.5.4.1
date	2000.02.29.23.29.00;	author niklas;	state Exp;
branches;
next	1.5.4.2;

1.5.4.2
date	2000.03.24.09.09.49;	author niklas;	state Exp;
branches;
next	1.5.4.3;

1.5.4.3
date	2001.05.14.22.47.45;	author niklas;	state Exp;
branches;
next	1.5.4.4;

1.5.4.4
date	2001.07.04.11.01.03;	author niklas;	state Exp;
branches;
next	1.5.4.5;

1.5.4.5
date	2001.10.31.03.32.14;	author nate;	state Exp;
branches;
next	1.5.4.6;

1.5.4.6
date	2001.11.13.23.02.31;	author niklas;	state Exp;
branches;
next	1.5.4.7;

1.5.4.7
date	2001.12.05.01.23.58;	author niklas;	state Exp;
branches;
next	1.5.4.8;

1.5.4.8
date	2002.03.06.02.17.14;	author niklas;	state Exp;
branches;
next	1.5.4.9;

1.5.4.9
date	2002.03.28.14.54.26;	author niklas;	state Exp;
branches;
next	1.5.4.10;

1.5.4.10
date	2003.03.28.00.08.48;	author niklas;	state Exp;
branches;
next	1.5.4.11;

1.5.4.11
date	2004.02.19.11.01.44;	author niklas;	state Exp;
branches;
next	1.5.4.12;

1.5.4.12
date	2004.06.05.23.13.12;	author niklas;	state Exp;
branches;
next	;

1.30.2.1
date	2002.02.02.03.28.26;	author art;	state Exp;
branches;
next	1.30.2.2;

1.30.2.2
date	2002.06.11.03.33.03;	author art;	state Exp;
branches;
next	1.30.2.3;

1.30.2.3
date	2002.11.04.18.02.33;	author art;	state Exp;
branches;
next	;

1.34.2.1
date	2003.11.20.22.43.16;	author margarida;	state Exp;
branches;
next	;

1.35.2.1
date	2003.11.20.17.46.49;	author brad;	state Exp;
branches;
next	;


desc
@@


1.73
log
@Unifed PMAP_UAREA, unused since we stopped supporting ARM < v7.

ok kettenis@@
@
text
@/*	$OpenBSD: uvm_glue.c,v 1.72 2017/04/30 13:04:49 mpi Exp $	*/
/*	$NetBSD: uvm_glue.c,v 1.44 2001/02/06 19:54:44 eeh Exp $	*/

/* 
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
 *
 * All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * The Mach Operating System project at Carnegie-Mellon University.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)vm_glue.c	8.6 (Berkeley) 1/5/94
 * from: Id: uvm_glue.c,v 1.1.2.8 1998/02/07 01:16:54 chs Exp
 *
 *
 * Copyright (c) 1987, 1990 Carnegie-Mellon University.
 * All rights reserved.
 * 
 * Permission to use, copy, modify and distribute this software and
 * its documentation is hereby granted, provided that both the copyright
 * notice and this permission notice appear in all copies of the
 * software, derivative works or modified versions, and any portions
 * thereof, and that both notices appear in supporting documentation.
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
 * FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE.
 * 
 * Carnegie Mellon requests users of this software to return to
 *
 *  Software Distribution Coordinator  or  Software.Distribution@@CS.CMU.EDU
 *  School of Computer Science
 *  Carnegie Mellon University
 *  Pittsburgh PA 15213-3890
 *
 * any improvements or extensions that they make and grant Carnegie the
 * rights to redistribute these changes.
 */

/*
 * uvm_glue.c: glue functions
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/resourcevar.h>
#include <sys/buf.h>
#include <sys/user.h>
#ifdef SYSVSHM
#include <sys/shm.h>
#endif
#include <sys/sched.h>

#include <uvm/uvm.h>

/*
 * uvm_kernacc: can the kernel access a region of memory
 *
 * - called from malloc [DIAGNOSTIC], and /dev/kmem driver (mem.c)
 */
boolean_t
uvm_kernacc(caddr_t addr, size_t len, int rw)
{
	boolean_t rv;
	vaddr_t saddr, eaddr;
	vm_prot_t prot = rw == B_READ ? PROT_READ : PROT_WRITE;

	saddr = trunc_page((vaddr_t)addr);
	eaddr = round_page((vaddr_t)addr + len);
	vm_map_lock_read(kernel_map);
	rv = uvm_map_checkprot(kernel_map, saddr, eaddr, prot);
	vm_map_unlock_read(kernel_map);

	return(rv);
}

/*
 * uvm_vslock: wire user memory for I/O
 *
 * - called from physio and sys_sysctl
 */

int
uvm_vslock(struct proc *p, caddr_t addr, size_t len, vm_prot_t access_type)
{
	struct vm_map *map;
	vaddr_t start, end;
	int rv;

	map = &p->p_vmspace->vm_map;
	start = trunc_page((vaddr_t)addr);
	end = round_page((vaddr_t)addr + len);
	if (end <= start)
		return (EINVAL);

	rv = uvm_fault_wire(map, start, end, access_type);

	return (rv);
}

/*
 * uvm_vsunlock: unwire user memory wired by uvm_vslock()
 *
 * - called from physio and sys_sysctl
 */

void
uvm_vsunlock(struct proc *p, caddr_t addr, size_t len)
{
	vaddr_t start, end;

	start = trunc_page((vaddr_t)addr);
	end = round_page((vaddr_t)addr + len);
	if (end <= start)
		return;

	uvm_fault_unwire(&p->p_vmspace->vm_map, start, end);
}

/*
 * uvm_vslock_device: wire user memory, make sure it's device reachable
 *  and bounce if necessary.
 * Always bounces for now.
 */
int
uvm_vslock_device(struct proc *p, void *addr, size_t len,
    vm_prot_t access_type, void **retp)
{
	struct vm_page *pg;
	struct pglist pgl;
	int npages;
	vaddr_t start, end, off;
	vaddr_t sva, va;
	vsize_t sz;
	int error, i;

	start = trunc_page((vaddr_t)addr);
	end = round_page((vaddr_t)addr + len);
	sz = end - start;
	off = (vaddr_t)addr - start;
	if (end <= start)
		return (EINVAL);

	if ((error = uvm_fault_wire(&p->p_vmspace->vm_map, start, end,
	    access_type))) {
		return (error);
	}

	npages = atop(sz);
	for (i = 0; i < npages; i++) {
		paddr_t pa;

		if (!pmap_extract(p->p_vmspace->vm_map.pmap,
		    start + ptoa(i), &pa)) {
			error = EFAULT;
			goto out_unwire;
		}
		if (!PADDR_IS_DMA_REACHABLE(pa))
			break;
	}
	if (i == npages) {
		*retp = NULL;
		return (0);
	}

	if ((va = uvm_km_valloc(kernel_map, sz)) == 0) {
		error = ENOMEM;
		goto out_unwire;
	}
	sva = va;

	TAILQ_INIT(&pgl);
	error = uvm_pglistalloc(npages * PAGE_SIZE, dma_constraint.ucr_low,
	    dma_constraint.ucr_high, 0, 0, &pgl, npages, UVM_PLA_WAITOK);
	if (error)
		goto out_unmap;

	while ((pg = TAILQ_FIRST(&pgl)) != NULL) {
		TAILQ_REMOVE(&pgl, pg, pageq);
		pmap_kenter_pa(va, VM_PAGE_TO_PHYS(pg), PROT_READ | PROT_WRITE);
		va += PAGE_SIZE;
	}
	pmap_update(pmap_kernel());
	KASSERT(va == sva + sz);
	*retp = (void *)(sva + off);

	if ((error = copyin(addr, *retp, len)) == 0)
		return 0;

	uvm_km_pgremove_intrsafe(sva, sva + sz);
	pmap_kremove(sva, sz);
	pmap_update(pmap_kernel());
out_unmap:
	uvm_km_free(kernel_map, sva, sz);
out_unwire:
	uvm_fault_unwire(&p->p_vmspace->vm_map, start, end);
	return (error);
}

void
uvm_vsunlock_device(struct proc *p, void *addr, size_t len, void *map)
{
	vaddr_t start, end;
	vaddr_t kva;
	vsize_t sz;

	start = trunc_page((vaddr_t)addr);
	end = round_page((vaddr_t)addr + len);
	sz = end - start;
	if (end <= start)
		return;

	if (map)
		copyout(map, addr, len);
	uvm_fault_unwire(&p->p_vmspace->vm_map, start, end);

	if (!map)
		return;

	kva = trunc_page((vaddr_t)map);
	uvm_km_pgremove_intrsafe(kva, kva + sz);
	pmap_kremove(kva, sz);
	pmap_update(pmap_kernel());
	uvm_km_free(kernel_map, kva, sz);
}

/*
 * uvm_uarea_alloc: allocate the u-area for a new thread
 */
vaddr_t
uvm_uarea_alloc(void)
{
	vaddr_t uaddr;

	uaddr = uvm_km_kmemalloc_pla(kernel_map, uvm.kernel_object, USPACE,
	    USPACE_ALIGN, UVM_KMF_ZERO,
	    no_constraint.ucr_low, no_constraint.ucr_high,
	    0, 0, USPACE/PAGE_SIZE);

	return (uaddr);
}

/*
 * uvm_uarea_free: free a dead thread's stack
 *
 * - the thread passed to us is a dead thread; we
 *   are running on a different context now (the reaper).
 */
void
uvm_uarea_free(struct proc *p)
{
	uvm_km_free(kernel_map, (vaddr_t)p->p_addr, USPACE);
	p->p_addr = NULL;
}

/*
 * uvm_exit: exit a virtual address space
 */
void
uvm_exit(struct process *pr)
{
	uvmspace_free(pr->ps_vmspace);
	pr->ps_vmspace = NULL;
}

/*
 * uvm_init_limit: init per-process VM limits
 *
 * - called for process 0 and then inherited by all others.
 */
void
uvm_init_limits(struct proc *p)
{

	/*
	 * Set up the initial limits on process VM.  Set the maximum
	 * resident set size to be all of (reasonably) available memory.
	 * This causes any single, large process to start random page
	 * replacement once it fills memory.
	 */
	p->p_rlimit[RLIMIT_STACK].rlim_cur = DFLSSIZ;
	p->p_rlimit[RLIMIT_STACK].rlim_max = MAXSSIZ;
	p->p_rlimit[RLIMIT_DATA].rlim_cur = DFLDSIZ;
	p->p_rlimit[RLIMIT_DATA].rlim_max = MAXDSIZ;
	p->p_rlimit[RLIMIT_RSS].rlim_cur = ptoa(uvmexp.free);
}

#ifdef DEBUG
int	enableswap = 1;
int	swapdebug = 0;
#define	SDB_FOLLOW	1
#define SDB_SWAPIN	2
#define SDB_SWAPOUT	4
#endif


/*
 * swapout_threads: find threads that can be swapped
 *
 * - called by the pagedaemon
 * - try and swap at least one processs
 * - processes that are sleeping or stopped for maxslp or more seconds
 *   are swapped... otherwise the longest-sleeping or stopped process
 *   is swapped, otherwise the longest resident process...
 */
void
uvm_swapout_threads(void)
{
	struct process *pr;
	struct proc *p, *slpp;
	struct process *outpr;
	int outpri;
	int didswap = 0;
	extern int maxslp; 
	/* XXXCDC: should move off to uvmexp. or uvm., also in uvm_meter */

#ifdef DEBUG
	if (!enableswap)
		return;
#endif

	/*
	 * outpr/outpri  : stop/sleep process whose most active thread has
	 *	the largest sleeptime < maxslp
	 */
	outpr = NULL;
	outpri = 0;
	LIST_FOREACH(pr, &allprocess, ps_list) {
		if (pr->ps_flags & (PS_SYSTEM | PS_EXITING))
			continue;

		/*
		 * slpp: the sleeping or stopped thread in pr with
		 * the smallest p_slptime
		 */
		slpp = NULL;
		TAILQ_FOREACH(p, &pr->ps_threads, p_thr_link) {
			switch (p->p_stat) {
			case SRUN:
			case SONPROC:
				goto next_process;

			case SSLEEP:
			case SSTOP:
				if (slpp == NULL ||
				    slpp->p_slptime < p->p_slptime)
					slpp = p;
				continue;
			}
		}

		if (slpp != NULL) {
			if (slpp->p_slptime >= maxslp) {
				pmap_collect(pr->ps_vmspace->vm_map.pmap);
				didswap++;
			} else if (slpp->p_slptime > outpri) {
				outpr = pr;
				outpri = slpp->p_slptime;
			}
		}
next_process:	;
	}

	/*
	 * If we didn't get rid of any real duds, toss out the next most
	 * likely sleeping/stopped or running candidate.  We only do this
	 * if we are real low on memory since we don't gain much by doing
	 * it.
	 */
	if (didswap == 0 && uvmexp.free <= atop(round_page(USPACE)) &&
	    outpr != NULL) {
#ifdef DEBUG
		if (swapdebug & SDB_SWAPOUT)
			printf("swapout_threads: no duds, try procpr %p\n",
			    outpr);
#endif
		pmap_collect(outpr->ps_vmspace->vm_map.pmap);
	}
}

/*
 * uvm_atopg: convert KVAs back to their page structures.
 */
struct vm_page *
uvm_atopg(vaddr_t kva)
{
	struct vm_page *pg;
	paddr_t pa;
	boolean_t rv;
 
	rv = pmap_extract(pmap_kernel(), kva, &pa);
	KASSERT(rv);
	pg = PHYS_TO_VM_PAGE(pa);
	KASSERT(pg != NULL);
	return (pg);
}

void
uvm_pause(void)
{
	static unsigned int toggle;
	if (toggle++ > 128) {
		toggle = 0;
		KERNEL_UNLOCK();
		KERNEL_LOCK();
	}
	sched_pause(preempt);
}

#ifndef SMALL_KERNEL
int
fill_vmmap(struct process *pr, struct kinfo_vmentry *kve,
    size_t *lenp)
{
	struct vm_map *map;

	if (pr != NULL)
		map = &pr->ps_vmspace->vm_map;
	else
		map = kernel_map;
	return uvm_map_fill_vmmap(map, kve, lenp);
}
#endif
@


1.72
log
@Unifdef KGDB.

It doesn't compile und hasn't been working during the last decade.

ok kettenis@@, deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.71 2017/02/14 10:31:15 mpi Exp $	*/
a264 6

#ifdef PMAP_UAREA
	/* Tell the pmap this is a u-area mapping */
	if (uaddr != 0)
		PMAP_UAREA(uaddr);
#endif
@


1.71
log
@Convert most of the manual checks for CPU hogging to sched_pause().

The distinction between preempt() and yield() stays as it is usueful
to know if a thread decided to yield by itself or if the kernel told
him to go away.

ok tedu@@, guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.70 2015/09/13 17:08:03 guenther Exp $	*/
a101 37

#ifdef KGDB
/*
 * Change protections on kernel pages from addr to addr+len
 * (presumably so debugger can plant a breakpoint).
 *
 * We force the protection change at the pmap level.  If we were
 * to use vm_map_protect a change to allow writing would be lazily-
 * applied meaning we would still take a protection fault, something
 * we really don't want to do.  It would also fragment the kernel
 * map unnecessarily.  We cannot use pmap_protect since it also won't
 * enforce a write-enable request.  Using pmap_enter is the only way
 * we can ensure the change takes place properly.
 */
void
uvm_chgkprot(caddr_t addr, size_t len, int rw)
{
	vm_prot_t prot;
	paddr_t pa;
	vaddr_t sva, eva;

	prot = rw == B_READ ? PROT_READ : PROT_READ | PROT_WRITE;
	eva = round_page((vaddr_t)addr + len);
	for (sva = trunc_page((vaddr_t)addr); sva < eva; sva += PAGE_SIZE) {
		/*
		 * Extract physical address for the page.
		 * We use a cheezy hack to differentiate physical
		 * page 0 from an invalid mapping, not that it
		 * really matters...
		 */
		if (pmap_extract(pmap_kernel(), sva, &pa) == FALSE)
			panic("chgkprot: invalid page");
		pmap_enter(pmap_kernel(), sva, pa, prot, PMAP_WIRED);
	}
	pmap_update(pmap_kernel());
}
#endif
@


1.70
log
@Rename __sysctl syscall to just sysctl, as the userland wrapper is no longer
necessary

ok deraadt@@ jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.69 2014/12/15 20:38:22 tedu Exp $	*/
d476 1
a476 2
	if (curcpu()->ci_schedstate.spc_schedflags & SPCF_SHOULDYIELD)
		preempt(NULL);
@


1.69
log
@don't drop the kernel lock everytime. on a busy system, this results in
the reaper spending more than half its time in uvm_pause. we want the
system to be interactive, but we want throughput too. this seems like a
decent balance.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.68 2014/12/05 04:12:48 uebayasi Exp $	*/
d143 1
a143 1
 * - called from physio and sys___sysctl
d167 1
a167 1
 * - called from physio and sys___sysctl
@


1.68
log
@Introduce a new sysctl to retrieve VM map entries

This adds a new sysctl KERN_PROC_VMMAP, which returns an array of VM map
entries of a specified process.  This prevents debuggers from iterating
vm_map_entry RB tree via kvm(3).

The name KERN_PROC_VMMAP and struct kinfo_vmentry are chosen from the same
function in FreeBSD.  struct kinfo_vmentry is revised to reduce size, because
OpenBSD does not keep track of filepaths.  The semantic is also changed to
return max buffer size as a hint, and start iteration at the specified base
address.

Much valuable input from deraadt@@, guenther@@, tedu@@

OK tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d470 6
a475 2
	KERNEL_UNLOCK();
	KERNEL_LOCK();
@


1.67
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.66 2014/07/11 16:35:40 jsg Exp $	*/
d475 15
@


1.66
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.65 2014/05/15 03:52:25 guenther Exp $	*/
d92 1
a92 1
	vm_prot_t prot = rw == B_READ ? VM_PROT_READ : VM_PROT_WRITE;
d123 1
a123 1
	prot = rw == B_READ ? VM_PROT_READ : VM_PROT_READ|VM_PROT_WRITE;
d243 1
a243 2
		pmap_kenter_pa(va, VM_PAGE_TO_PHYS(pg),
		    VM_PROT_READ|VM_PROT_WRITE);
@


1.65
log
@Move from struct proc to process the reference-count-holding pointers
to the process's vmspace and filedescs.  struct proc continues to
keep copies of the pointers, copying them on fork, clearing them
on exit, and (for vmspace) refreshing on exec.
Also, make uvm_swapout_threads() thread aware, eliminating p_swtime
in kernel.

particular testing by ajacoutot@@ and sebastia@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.64 2014/05/03 22:49:43 guenther Exp $	*/
d21 1
a21 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by Charles D. Cranor,
 *      Washington University, the University of California, Berkeley and 
 *      its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.64
log
@Fix whitespace fail in previous commit
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.63 2014/05/03 22:44:36 guenther Exp $	*/
d319 1
a319 1
 * uvm_fork: fork a virtual address space
d321 2
a322 9
 * - the address space is copied as per parent map's inherit values
 * - if specified, the child gets a new user stack described by
 *	stack and stacksize
 * - NOTE: the kernel stack may be at a different location in the child
 *	process, and thus addresses of automatic variables may be invalid
 *	after cpu_fork returns in the child process.  We do nothing here
 *	after cpu_fork returns.
 * - XXXCDC: we need a way for this to return a failure value rather
 *   than just hang
d325 1
a325 2
uvm_fork(struct proc *p1, struct proc *p2, boolean_t shared, void *stack,
    size_t stacksize, void (*func)(void *), void * arg)
d327 2
a328 14
	if (shared == TRUE) {
		p2->p_vmspace = NULL;
		uvmspace_share(p1, p2);			/* share vmspace */
	} else
		p2->p_vmspace = uvmspace_fork(p1->p_vmspace); /* fork vmspace */

	/*
	 * cpu_fork() copy and update the pcb, and make the child ready
	 * to run.  If this is a normal user fork, the child will exit
	 * directly to user mode via child_return() on its first time
	 * slice and will not return here.  If this is a kernel thread,
	 * the specified entry point will be executed.
	 */
	cpu_fork(p1, p2, stack, stacksize, func, arg);
a332 5
 *
 * - the process passed to us is a dead (pre-zombie) process; we
 *   are running on a different context now (the reaper).
 * - we must run in a separate thread because freeing the vmspace
 *   of the dead process may block.
d335 1
a335 1
uvm_exit(struct proc *p)
d337 2
a338 4
	uvmspace_free(p->p_vmspace);
	p->p_vmspace = NULL;
	uvm_km_free(kernel_map, (vaddr_t)p->p_addr, USPACE);
	p->p_addr = NULL;
a370 5
/*
 * swappable: is process "p" swappable?
 */

#define	swappable(p) (((p)->p_flag & (P_SYSTEM | P_WEXIT)) == 0)
d384 4
a387 3
	struct proc *p;
	struct proc *outp, *outp2;
	int outpri, outpri2;
d398 2
a399 2
	 * outp/outpri  : stop/sleep process with largest sleeptime < maxslp
	 * outp2/outpri2: the longest resident process (its swap time)
d401 4
a404 4
	outp = outp2 = NULL;
	outpri = outpri2 = 0;
	LIST_FOREACH(p, &allproc, p_list) {
		if (!swappable(p))
d406 18
a423 5
		switch (p->p_stat) {
		case SRUN:
			if (p->p_swtime > outpri2) {
				outp2 = p;
				outpri2 = p->p_swtime;
d425 5
a429 6
			continue;
			
		case SSLEEP:
		case SSTOP:
			if (p->p_slptime >= maxslp) {
				pmap_collect(p->p_vmspace->vm_map.pmap);
d431 3
a433 3
			} else if (p->p_slptime > outpri) {
				outp = p;
				outpri = p->p_slptime;
a434 1
			continue;
d436 1
d445 2
a446 3
	if (didswap == 0 && uvmexp.free <= atop(round_page(USPACE))) {
		if ((p = outp) == NULL)
			p = outp2;
d449 2
a450 1
			printf("swapout_threads: no duds, try procp %p\n", p);
d452 1
a452 2
		if (p)
			pmap_collect(p->p_vmspace->vm_map.pmap);
@


1.63
log
@Move the u-area allocation and pmap-magic logic to its own function
uvm_uarea_alloc()

function name from NetBSD; arm testing by miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.62 2014/04/13 23:14:15 tedu Exp $	*/
d308 1
a308 1
 
d315 1
a315 1
       return (uaddr);
@


1.62
log
@compress code by turning four line comments into one line comments.
emphatic ok usual suspects, grudging ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.61 2014/04/03 21:40:10 tedu Exp $	*/
d297 22
a321 2
 * - a new "user" structure is allocated for the child process
 *	[filled in by MD layer...]
a339 5

#ifdef PMAP_UAREA
	/* Tell the pmap this is a u-area mapping */
	PMAP_UAREA((vaddr_t)p2->p_addr);
#endif
@


1.61
log
@add a uvm_yield function and use it in the reaper path to prevent the
reaper from hogging the cpu. it will do the kernel lock twiddle trick to
allow other CPUs a chance to run, and also checks if the reaper has been
running for an entire timeslice and should be preempted.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.60 2013/03/31 17:06:34 deraadt Exp $	*/
a91 1

a367 1

@


1.60
log
@do not need machine/cpu.h directly
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.59 2012/03/23 15:51:26 guenther Exp $	*/
d478 10
a487 1
} 
@


1.59
log
@Make rusage totals, itimers, and profile settings per-process instead
of per-rthread.  Handling of per-thread tick and runtime counters
inspired by how FreeBSD does it.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.58 2011/04/15 21:47:24 oga Exp $	*/
a85 2

#include <machine/cpu.h>
@


1.58
log
@move uvm_pageratop from uvm_pager.c local to a general uvm function
(uvm_atopg) and use it in uvm_km_doputpage to replace some handrolled
code. Shrinks the kernel a trivial amount.

ok beck@@ and miod@@ (who suggested i name it uvm_atopg not uvm_atop)
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.57 2011/04/07 13:20:25 miod Exp $	*/
a317 2
	struct user *up = p2->p_addr;

d326 1
a326 1
	PMAP_UAREA((vaddr_t)up);
a328 12
	/*
	 * p_stats currently points at a field in the user struct.  Copy
	 * parts of p_stats, and zero out the rest.
	 */
	p2->p_stats = &up->u_stats;
	memset(&up->u_stats.pstat_startzero, 0,
	       ((caddr_t)&up->u_stats.pstat_endzero -
		(caddr_t)&up->u_stats.pstat_startzero));
	memcpy(&up->u_stats.pstat_startcopy, &p1->p_stats->pstat_startcopy,
	       ((caddr_t)&up->u_stats.pstat_endcopy -
		(caddr_t)&up->u_stats.pstat_startcopy));
	
@


1.57
log
@In uvm_vslock_device(), if uvm_pglistalloc() fails, make sure to not pass
an uninitialized variable to uvm_km_free().
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.56 2011/04/01 15:43:13 art Exp $	*/
d478 17
@


1.56
log
@Two problems with vslock_device functions.

 - Fix error handling so that we free stuff on error.
 - We use the mappings to keep track of which pages need to be
   freed so don't unmap before freeing (this is theoretically
   incorrect and will be fixed soon).

This makes fsck happy on bigmem machines (it doesn't leak all
dma:able memory anymore).

beck@@, drahn@@, oga@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.55 2010/07/02 22:38:32 thib Exp $	*/
d241 1
a248 1
	sva = va;
@


1.55
log
@Don't bother trying to handle a uvm_pglistalloc failure when called with
UVM_PLA_WAITOK as it will not fail; Rather assert that it didn't fail.

ok tedu@@, oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.54 2010/07/02 20:40:16 thib Exp $	*/
d225 4
a228 2
		    start + ptoa(i), &pa))
			return (EFAULT);
d238 2
a239 1
		return (ENOMEM);
d245 2
a246 1
	KASSERT(error == 0);
d259 10
a268 1
	error = copyin(addr, *retp, len);	
d293 1
a295 1
	uvm_km_pgremove_intrsafe(kva, kva + sz);
@


1.54
log
@nuke unused global and a comment.
ok tedu@@, oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.52 2010/07/01 21:27:39 art Exp $	*/
d238 1
d240 3
a242 5
	if (uvm_pglistalloc(npages * PAGE_SIZE, dma_constraint.ucr_low,
	    dma_constraint.ucr_high, 0, 0, &pgl, npages, UVM_PLA_WAITOK)) {
		uvm_km_free(kernel_map, va, sz);
		return (ENOMEM);
	}
d247 2
a248 1
		pmap_kenter_pa(va, VM_PAGE_TO_PHYS(pg), VM_PROT_READ|VM_PROT_WRITE);
@


1.53
log
@Add missing pmap_update. pointed out by matthew@@
@
text
@a89 8
 * XXXCDC: do these really belong here?
 */

int readbuffers = 0;		/* allow KGDB to read kern buffer pool */
				/* XXX: see uvm_kernacc */


/*
@


1.52
log
@Implement vs{,un}lock_device and use it for physio.

Just like normal vs{,un}lock, but in case the pages we get are not dma
accessible, we bounce them, if they are dma acessible, the functions behave
exactly like normal vslock. The plan for the future is to have fault_wire
allocate dma acessible pages so that we don't need to bounce (especially
in cases where the same buffer is reused for physio over and over again),
but for now, keep it as simple as possible.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.51 2010/06/30 20:20:18 thib Exp $	*/
d259 1
d289 1
@


1.51
log
@knf function decleration nit.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.50 2009/08/11 18:43:33 blambert Exp $	*/
a159 1
 * - XXXCDC: consider nuking this (or making it a macro?)
d165 1
a165 1
	vm_map_t map;
a183 1
 * - XXXCDC: consider nuking this (or making it a macro?)
d197 93
@


1.50
log
@uvm_scheduler() sounds important, but ``while(1) tsleep()'' is kinda lame

inline the loop in the one place it exists, and remove it from uvm

adjust a comment mentioning it accordingly

originally inspired by a diff fixing a comment from oga@@

ok art@@ beck@@ miod@@ oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.49 2009/03/20 15:19:04 oga Exp $	*/
d217 2
a218 7
uvm_fork(p1, p2, shared, stack, stacksize, func, arg)
	struct proc *p1, *p2;
	boolean_t shared;
	void *stack;
	size_t stacksize;
	void (*func)(void *);
	void *arg;
@


1.49
log
@While working on some stuff in uvm I've gotten REALLY sick of reading
K&R function declarations, so switch them all over to ansi-style, in
accordance with the prophesy.

"go for it" art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.48 2008/10/24 06:32:46 deraadt Exp $	*/
a306 16

/*
 * uvm_scheduler: process zero main loop
 *
 * - if not enough memory, wake the pagedaemon and let it clear space.
 */

void
uvm_scheduler(void)
{
	/*
	 * Nothing to do, back to sleep
	 */
	while (1)
		tsleep(&proc0, PVM, "scheduler", 0);
}
@


1.48
log
@it is a good policy to clear the pointer after we free something
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.47 2007/05/26 20:26:51 pedro Exp $	*/
d104 1
a104 4
uvm_kernacc(addr, len, rw)
	caddr_t addr;
	size_t len;
	int rw;
d133 1
a133 4
uvm_chgkprot(addr, len, rw)
	caddr_t addr;
	size_t len;
	int rw;
d164 1
a164 5
uvm_vslock(p, addr, len, access_type)
	struct proc *p;
	caddr_t	addr;
	size_t	len;
	vm_prot_t access_type;
d189 1
a189 4
uvm_vsunlock(p, addr, len)
	struct proc *p;
	caddr_t	addr;
	size_t	len;
@


1.47
log
@Dynamic buffer cache. Initial diff from mickey@@, okay art@@ beck@@ toby@@
deraadt@@ dlg@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.46 2007/05/18 10:55:34 art Exp $	*/
d285 1
@


1.46
log
@Instead of a silly loop with goto, just use while(1).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.45 2006/11/29 12:24:18 miod Exp $	*/
a118 12
	/*
	 * XXX there are still some things (e.g. the buffer cache) that
	 * are managed behind the VM system's back so even though an
	 * address is accessible in the mind of the VM system, there may
	 * not be physical pages where the VM thinks there is.  This can
	 * lead to bogus allocation of pages in the kernel address space
	 * or worse, inconsistencies at the pmap level.  We only worry
	 * about the buffer cache for now.
	 */
	if (!readbuffers && rv && (eaddr > (vaddr_t)buffers &&
			     saddr < (vaddr_t)buffers + MAXBSIZE * nbuf))
		rv = FALSE;
@


1.45
log
@Kernel stack can be swapped. This means that stuff that's on the stack
should never be referenced outside the context of the process to which
this stack belongs unless we do the PHOLD/PRELE dance. Loads of code
doesn't follow the rules here. Instead of trying to track down all
offenders and fix this hairy situation, it makes much more sense
to not swap kernel stacks.

From art@@, tested by many some time ago.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.44 2006/05/25 22:42:22 miod Exp $	*/
d344 2
a345 3
loop:
	tsleep(&proc0, PVM, "scheduler", 0);
	goto loop;
@


1.44
log
@Enable optional specific handling of the u-area in pmap via PMAP_UAREA if
defined; from NetBSD. Currently only used on xscale arm to use the mini data
cache for u area mappings instead of the main data cache.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.43 2005/11/01 14:18:14 aaron Exp $	*/
a89 6
 * local prototypes
 */

static void uvm_swapout(struct proc *);

/*
a250 1
	int rv;
a257 14
	/*
	 * Wire down the U-area for the process, which contains the PCB
	 * and the kernel stack.  Wired state is stored in p->p_flag's
	 * P_INMEM bit rather than in the vm_map_entry's wired count
	 * to prevent kernel_map fragmentation.
	 *
	 * Note the kernel stack gets read/write accesses right off
	 * the bat.
	 */
	rv = uvm_fault_wire(kernel_map, (vaddr_t)up,
	    (vaddr_t)up + USPACE, VM_PROT_READ | VM_PROT_WRITE);
	if (rv != KERN_SUCCESS)
		panic("uvm_fork: uvm_fault_wire failed: %d", rv);

d294 1
a294 2
uvm_exit(p)
	struct proc *p;
a295 2
	vaddr_t va = (vaddr_t)p->p_addr;

d297 1
a297 3
	p->p_flag &= ~P_INMEM;
	uvm_fault_unwire(kernel_map, va, va + USPACE);
	uvm_km_free(kernel_map, va, USPACE);
d307 1
a307 2
uvm_init_limits(p)
	struct proc *p;
a332 40
 * uvm_swapin: swap in a process's u-area.
 */

void
uvm_swapin(p)
	struct proc *p;
{
	vaddr_t addr;
	int rv, s;

	s = splstatclock();
	if (p->p_flag & P_SWAPIN) {
		splx(s);
		return;
	}
	p->p_flag |= P_SWAPIN;
	splx(s);

	addr = (vaddr_t)p->p_addr;
	/* make P_INMEM true */
	if ((rv = uvm_fault_wire(kernel_map, addr, addr + USPACE,
	    VM_PROT_READ | VM_PROT_WRITE)) != KERN_SUCCESS)
		panic("uvm_swapin: uvm_fault_wire failed: %d", rv);

	/*
	 * Some architectures need to be notified when the user area has
	 * moved to new physical page(s) (e.g.  see mips/mips/vm_machdep.c).
	 */
	cpu_swapin(p);
	SCHED_LOCK(s);
	if (p->p_stat == SRUN)
		setrunqueue(p);
	p->p_flag |= P_INMEM;
	p->p_flag &= ~P_SWAPIN;
	p->p_swtime = 0;
	SCHED_UNLOCK(s);
	++uvmexp.swapins;
}

/*
a334 2
 * - attempt to swapin every swaped-out, runnable process in order of
 *	priority.
d339 1
a339 1
uvm_scheduler()
a340 29
	struct proc *p;
	int pri;
	struct proc *pp;
	int ppri;

loop:
#ifdef DEBUG
	while (!enableswap)
		tsleep(&proc0, PVM, "noswap", 0);
#endif
	pp = NULL;		/* process to choose */
	ppri = INT_MIN;	/* its priority */
	LIST_FOREACH(p, &allproc, p_list) {

		/* is it a runnable swapped out process? */
		if (p->p_stat == SRUN && (p->p_flag & P_INMEM) == 0) {
			pri = p->p_swtime + p->p_slptime -
			    (p->p_nice - NZERO) * 8;
			if (pri > ppri) {   /* higher priority?  remember it. */
				pp = p;
				ppri = pri;
			}
		}
	}

#ifdef DEBUG
	if (swapdebug & SDB_FOLLOW)
		printf("scheduler: running, procp %p pri %d\n", pp, ppri);
#endif
d344 2
a345 35
	if ((p = pp) == NULL) {
		tsleep(&proc0, PVM, "scheduler", 0);
		goto loop;
	}

	/*
	 * we have found swapped out process which we would like to bring
	 * back in.
	 *
	 * XXX: this part is really bogus because we could deadlock on memory
	 * despite our feeble check
	 */
	if (uvmexp.free > atop(USPACE)) {
#ifdef DEBUG
		if (swapdebug & SDB_SWAPIN)
			printf("swapin: pid %d(%s)@@%p, pri %d free %d\n",
	     p->p_pid, p->p_comm, p->p_addr, ppri, uvmexp.free);
#endif
		uvm_swapin(p);
		goto loop;
	}
	/*
	 * not enough memory, jab the pageout daemon and wait til the coast
	 * is clear
	 */
#ifdef DEBUG
	if (swapdebug & SDB_FOLLOW)
		printf("scheduler: no room for pid %d(%s), free %d\n",
	   p->p_pid, p->p_comm, uvmexp.free);
#endif
	uvm_wait("schedpwait");
#ifdef DEBUG
	if (swapdebug & SDB_FOLLOW)
		printf("scheduler: room again, free %d\n", uvmexp.free);
#endif
d353 1
a353 3
#define	swappable(p)							\
	(((p)->p_flag & (P_SYSTEM | P_INMEM | P_WEXIT)) == P_INMEM &&	\
	 (p)->p_holdcnt == 0)
d356 1
a356 2
 * swapout_threads: find threads that can be swapped and unwire their
 *	u-areas.
d365 1
a365 1
uvm_swapout_threads()
d399 1
a399 1
				uvm_swapout(p);
d413 1
a413 1
	 * it (USPACE bytes).
d423 1
a423 33
			uvm_swapout(p);
	}
}

/*
 * uvm_swapout: swap out process "p"
 *
 * - currently "swapout" means "unwire U-area" and "pmap_collect()" 
 *   the pmap.
 * - XXXCDC: should deactivate all process' private anonymous memory
 */

static void
uvm_swapout(p)
	struct proc *p;
{
	vaddr_t addr;
	int s;

#ifdef DEBUG
	if (swapdebug & SDB_SWAPOUT)
		printf("swapout: pid %d(%s)@@%p, stat %x pri %d free %d\n",
		    p->p_pid, p->p_comm, p->p_addr, p->p_stat,
		    p->p_slptime, uvmexp.free);
#endif

	/*
	 * Mark it as (potentially) swapped out.
	 */
	SCHED_LOCK(s);
	if (!(p->p_flag & P_INMEM)) {
		SCHED_UNLOCK(s);
		return;
a424 19
	p->p_flag &= ~P_INMEM;
	if (p->p_stat == SRUN)
		remrunqueue(p);
	p->p_swtime = 0;
	SCHED_UNLOCK(s);
	++uvmexp.swapouts;

	/*
	 * Do any machine-specific actions necessary before swapout.
	 * This can include saving floating point state, etc.
	 */
	cpu_swapout(p);

	/*
	 * Unwire the to-be-swapped process's user struct and kernel stack.
	 */
	addr = (vaddr_t)p->p_addr;
	uvm_fault_unwire(kernel_map, addr, addr + USPACE); /* !P_INMEM */
	pmap_collect(vm_map_pmap(&p->p_vmspace->vm_map));
a425 1

@


1.43
log
@In uvm_swapout(), protect "p->p_swtime = 0;" with SCHED_LOCK() as is already
done in uvm_swapin().  Looks like this was a mistake made while editing.  No
response from art@@.  deraadt@@, miod@@, pedro@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.42 2005/05/31 11:35:33 art Exp $	*/
d278 5
@


1.42
log
@Protect the run queues with SCHED_LOCK, not just spl (ot nothing at all in
one case fixed here).

miod@@ "appears to be harmless"
markus@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.41 2005/05/29 03:20:43 deraadt Exp $	*/
d589 1
a590 1
	p->p_swtime = 0;
@


1.41
log
@sched work by niklas and art backed out; causes panics
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.39 2004/02/23 06:19:32 drahn Exp $	*/
d83 1
d384 1
a384 1
	s = splstatclock();
a388 1
	splx(s);
d390 1
d581 1
a581 1
	s = splstatclock();
d583 1
a583 1
		splx(s);
d589 1
a589 1
	splx(s);
@


1.40
log
@This patch is mortly art's work and was done *a year* ago.  Art wants to thank
everyone for the prompt review and ok of this work ;-)  Yeah, that includes me
too, or maybe especially me.  I am sorry.

Change the sched_lock to a mutex. This fixes, among other things, the infamous
"telnet localhost &" problem.  The real bug in that case was that the sched_lock
which is by design a non-recursive lock, was recursively acquired, and not
enough releases made us hold the lock in the idle loop, blocking scheduling
on the other processors.  Some of the other processors would hold the biglock though,
which made it impossible for cpu 0 to enter the kernel...  A nice deadlock.
Let me just say debugging this for days just to realize that it was all fixed
in an old diff noone ever ok'd was somewhat of an anti-climax.

This diff also changes splsched to be correct for all our architectures.
@
text
@a82 1
#include <sys/sched.h>
d383 1
a383 1
	SCHED_LOCK(s);
d388 1
a389 1
	SCHED_UNLOCK(s);
d580 1
a580 1
	SCHED_LOCK(s);
d582 1
a582 1
		SCHED_UNLOCK(s);
d588 1
a589 1
	SCHED_UNLOCK(s);
@


1.39
log
@sync of pmap_update() calls with NetBSD. pmap_update is defined away on
all architectures but arm, where it is needed.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.38 2003/11/20 17:54:55 grange Exp $	*/
d83 1
d384 1
a384 1
	s = splstatclock();
a388 1
	splx(s);
d390 1
d581 1
a581 1
	s = splstatclock();
d583 1
a583 1
		splx(s);
a588 1
	splx(s);
d590 1
@


1.38
log
@Sync comments with NetBSD.
ok millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.37 2003/11/20 14:44:31 grange Exp $	*/
d175 1
@


1.37
log
@Check for round_page() overflow in uvm_vslock()/uvm_vsunlock().
ok millert@@ henning@@ markus@@ drahn@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.36 2003/11/08 06:11:10 nordin Exp $	*/
d179 1
a179 1
 * vslock: wire user memory for I/O
d208 1
a208 1
 * vslock: wire user memory for I/O
@


1.36
log
@Avoid a race condition while swapping in a process.
Tested by mickey@@, henning@@, ericj@@, and beck@@.
ok mickey@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.35 2003/08/10 00:04:50 miod Exp $	*/
d199 2
d220 8
a227 2
	uvm_fault_unwire(&p->p_vmspace->vm_map, trunc_page((vaddr_t)addr),
		round_page((vaddr_t)addr + len));
@


1.35
log
@Remove uvm_useracc(): misleading, gives a false sentiment of security, and
eventually not used anymore. Conforming to art@@'s evil plans.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.34 2003/01/29 22:57:10 mickey Exp $	*/
d355 8
d378 1
d564 2
a565 2
	   p->p_pid, p->p_comm, p->p_addr, p->p_stat,
	   p->p_slptime, uvmexp.free);
a568 6
	 * Do any machine-specific actions necessary before swapout.
	 * This can include saving floating point state, etc.
	 */
	cpu_swapout(p);

	/*
d572 4
d582 6
@


1.35.2.1
log
@MFC:
Fix by grange@@

Check for round_page() overflow in uvm_vslock()/uvm_vsunlock().

ok millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.35 2003/08/10 00:04:50 miod Exp $	*/
a198 2
	if (end <= start)
		return (EINVAL);
d218 2
a219 8
	vaddr_t start, end;

	start = trunc_page((vaddr_t)addr);
	end = round_page((vaddr_t)addr + len);
	if (end <= start)
		return;

	uvm_fault_unwire(&p->p_vmspace->vm_map, start, end);
@


1.34
log
@check the uvm_fault_wire() for failure, just like other calls; art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.33 2002/06/09 02:11:47 jsyn Exp $	*/
a135 27
	return(rv);
}

/*
 * uvm_useracc: can the user access it?
 *
 * - called from physio() and sys___sysctl().
 */

boolean_t
uvm_useracc(addr, len, rw)
	caddr_t addr;
	size_t len;
	int rw;
{
	vm_map_t map;
	boolean_t rv;
	vm_prot_t prot = rw == B_READ ? VM_PROT_READ : VM_PROT_WRITE;

	/* XXX curproc */
	map = &curproc->p_vmspace->vm_map;

	vm_map_lock_read(map);
	rv = uvm_map_checkprot(map, trunc_page((vaddr_t)addr),
	    round_page((vaddr_t)addr + len), prot);
	vm_map_unlock_read(map);

@


1.34.2.1
log
@MFC:
Fix by grange@@

Check for round_page() overflow in uvm_vslock()/uvm_vsunlock().

ok millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.34 2003/01/29 22:57:10 mickey Exp $	*/
a225 2
	if (end <= start)
		return (EINVAL);
d245 2
a246 8
	vaddr_t start, end;

	start = trunc_page((vaddr_t)addr);
	end = round_page((vaddr_t)addr + len);
	if (end <= start)
		return;

	uvm_fault_unwire(&p->p_vmspace->vm_map, start, end);
@


1.33
log
@fix the use of "cuz" in the tree; these are all in comments

noticed by aaron@@, recommended by deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.32 2002/03/14 01:27:18 millert Exp $	*/
d380 1
a380 1
	int s;
d384 3
a386 2
	uvm_fault_wire(kernel_map, addr, addr + USPACE,
	    VM_PROT_READ | VM_PROT_WRITE);
@


1.32
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.31 2001/12/19 08:58:07 art Exp $	*/
d453 1
a453 1
	 * XXX: this part is really bogus cuz we could deadlock on memory
@


1.31
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.26 2001/11/10 19:20:39 art Exp $	*/
d92 1
a92 1
static void uvm_swapout __P((struct proc *));
d270 1
a270 1
	void (*func) __P((void *));
@


1.30
log
@Yet another sync to NetBSD uvm.
Today we add a pmap argument to pmap_update() and allocate map entries for
kernel_map from kmem_map instead of using the static entries. This should
get rid of MAX_KMAPENT panics. Also some uvm_loan problems are fixed.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_glue.c,v 1.29 2001/11/28 19:28:14 art Exp $	*/
/*	$NetBSD: uvm_glue.c,v 1.51 2001/09/10 21:19:42 chris Exp $	*/
d4 1
a4 1
/*
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and
d48 1
a48 1
 *
d54 3
a56 3
 *
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
d58 1
a58 1
 *
d151 1
a151 1
	struct vm_map *map;
d194 3
a201 1
	pmap_update(pmap_kernel());
d219 1
a219 1
	struct vm_map *map;
d221 1
a221 1
	int error;
d226 4
a229 2
	error = uvm_fault_wire(map, start, end, access_type);
	return error;
d274 1
a274 1
	int error;
d291 1
a291 1
	error = uvm_fault_wire(kernel_map, (vaddr_t)up,
d293 2
a294 2
	if (error)
		panic("uvm_fork: uvm_fault_wire failed: %d", error);
d307 1
a307 1

d507 1
a507 1
	extern int maxslp;
d531 1
a531 1

d566 1
a566 1
 * - currently "swapout" means "unwire U-area" and "pmap_collect()"
@


1.30.2.1
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_glue.c,v 1.30 2001/12/04 23:22:42 art Exp $	*/
/*	$NetBSD: uvm_glue.c,v 1.55 2001/11/10 07:36:59 lukem Exp $	*/
d204 1
a204 1
 * uvm_vslock: wire user memory for I/O
d229 1
a229 1
 * uvm_vsunlock: unwire user memory wired by uvm_vslock()
d274 1
a274 1
		uvmspace_share(p1, p2);
d276 1
a276 1
		p2->p_vmspace = uvmspace_fork(p1->p_vmspace);
d330 1
d376 1
a376 1
	int s, error;
d380 1
a380 1
	error = uvm_fault_wire(kernel_map, addr, addr + USPACE,
a381 3
	if (error) {
		panic("uvm_swapin: rewiring stack failed: %d", error);
	}
a595 1
	p->p_stats->p_ru.ru_nswap++;
@


1.30.2.2
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.30.2.1 2002/02/02 03:28:26 art Exp $	*/
d92 1
a92 1
static void uvm_swapout(struct proc *);
d266 1
a266 1
	void (*func)(void *);
d451 1
a451 1
	 * XXX: this part is really bogus because we could deadlock on memory
@


1.30.2.3
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_glue.c,v 1.30.2.2 2002/06/11 03:33:03 art Exp $	*/
/*	$NetBSD: uvm_glue.c,v 1.60 2002/09/22 07:20:32 chs Exp $	*/
a93 4
#define UVM_NUAREA_MAX 16
void *uvm_uareas;
int uvm_nuarea;

d224 1
a224 1
	error = uvm_fault_wire(map, start, end, VM_FAULT_WIRE, access_type);
d287 2
a288 2
	error = uvm_fault_wire(kernel_map, (vaddr_t)up, (vaddr_t)up + USPACE,
	    VM_FAULT_WIRE, VM_PROT_READ | VM_PROT_WRITE);
a291 7
#ifdef KSTACK_CHECK_MAGIC
	/*
	 * fill stack with magic number
	 */
	kstack_setup_magic(p2);
#endif

a321 1

d330 1
a330 1
	uvm_uarea_free(va);
a334 40
 * uvm_uarea_alloc: allocate a u-area
 */

vaddr_t
uvm_uarea_alloc(void)
{
	vaddr_t uaddr;

#ifndef USPACE_ALIGN
#define USPACE_ALIGN    0
#endif

	uaddr = (vaddr_t)uvm_uareas;
	if (uaddr) {
		uvm_uareas = *(void **)uvm_uareas;
		uvm_nuarea--;
	} else {
		uaddr = uvm_km_valloc_align(kernel_map, USPACE, USPACE_ALIGN);
	}
	return uaddr;
}

/*
 * uvm_uarea_free: free a u-area
 */

void
uvm_uarea_free(vaddr_t uaddr)
{

	if (uvm_nuarea < UVM_NUAREA_MAX) {
		*(void **)uaddr = uvm_uareas;
		uvm_uareas = (void *)uaddr;
		uvm_nuarea++;
	} else {
		uvm_km_free(kernel_map, uaddr, USPACE);
	}
}

/*
a338 1

d379 1
a379 1
	error = uvm_fault_wire(kernel_map, addr, addr + USPACE, VM_FAULT_WIRE,
a497 1

a608 64
/*
 * uvm_coredump_walkmap: walk a process's map for the purpose of dumping
 * a core file.
 */

int
uvm_coredump_walkmap(p, vp, cred, func, cookie)
	struct proc *p;
	struct vnode *vp;
	struct ucred *cred;
	int (*func)(struct proc *, struct vnode *, struct ucred *,
	    struct uvm_coredump_state *);
	void *cookie;
{
	struct uvm_coredump_state state;
	struct vmspace *vm = p->p_vmspace;
	struct vm_map *map = &vm->vm_map;
	struct vm_map_entry *entry;
	vaddr_t maxstack;
	int error;

	maxstack = trunc_page(USRSTACK - ctob(vm->vm_ssize));

	for (entry = map->header.next; entry != &map->header;
	     entry = entry->next) {  
		/* Should never happen for a user process. */
		if (UVM_ET_ISSUBMAP(entry))
			panic("uvm_coredump_walkmap: user process with "
			    "submap?");

		state.cookie = cookie;
		state.start = entry->start;
		state.end = entry->end;
		state.prot = entry->protection;
		state.flags = 0;

		if (state.start >= VM_MAXUSER_ADDRESS)  
			continue;

		if (state.end > VM_MAXUSER_ADDRESS)
			state.end = VM_MAXUSER_ADDRESS;

		if (state.start >= (vaddr_t)vm->vm_maxsaddr) {
			if (state.end <= maxstack)
				continue;
			if (state.start < maxstack)
				state.start = maxstack;
			state.flags |= UVM_COREDUMP_STACK;
		}

		if ((entry->protection & VM_PROT_WRITE) == 0)
			state.flags |= UVM_COREDUMP_NODUMP;

		if (entry->object.uvm_obj != NULL &&
		    entry->object.uvm_obj->pgops == &uvm_deviceops)
			state.flags |= UVM_COREDUMP_NODUMP;

		error = (*func)(p, vp, cred, &state);
		if (error)
			return (error);
	}

	return (0);
}
@


1.29
log
@Sync in more uvm from NetBSD. Mostly just cosmetic stuff.
Contains also support for page coloring.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_glue.c,v 1.28 2001/11/28 14:29:13 art Exp $	*/
/*	$NetBSD: uvm_glue.c,v 1.50 2001/06/02 18:09:26 chs Exp $	*/
d199 1
a199 1
	pmap_update();
@


1.28
log
@more sync to netbsd. some bugfixes in uvm_km_kmemalloc, lots of fixes in uvm_loan.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_glue.c,v 1.27 2001/11/28 13:47:39 art Exp $	*/
/*	$NetBSD: uvm_glue.c,v 1.46 2001/04/21 17:38:24 thorpej Exp $	*/
d4 1
a4 1
/* 
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and 
d48 1
a48 1
 * 
d54 3
a56 3
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
d58 1
a58 1
 * 
d151 1
a151 1
	vm_map_t map;
a193 3
		 * We use a cheezy hack to differentiate physical
		 * page 0 from an invalid mapping, not that it
		 * really matters...
d199 1
d217 1
a217 1
	vm_map_t map;
d303 1
a303 1
	
d503 1
a503 1
	extern int maxslp; 
d527 1
a527 1
			
d562 1
a562 1
 * - currently "swapout" means "unwire U-area" and "pmap_collect()" 
@


1.27
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_glue.c,v 1.26 2001/11/10 19:20:39 art Exp $	*/
/*	$NetBSD: uvm_glue.c,v 1.45 2001/03/15 06:10:57 chs Exp $	*/
a558 1
	pmap_update();
@


1.26
log
@Move maxdmap and maxsmap to kern_resource.c
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_glue.c,v 1.25 2001/11/07 02:55:50 art Exp $	*/
/*	$NetBSD: uvm_glue.c,v 1.44 2001/02/06 19:54:44 eeh Exp $	*/
d221 1
a221 1
	int rv;
d226 2
a227 4

	rv = uvm_fault_wire(map, start, end, access_type);

	return (rv);
d272 1
a272 1
	int rv;
d289 1
a289 1
	rv = uvm_fault_wire(kernel_map, (vaddr_t)up,
d291 2
a292 2
	if (rv != KERN_SUCCESS)
		panic("uvm_fork: uvm_fault_wire failed: %d", rv);
@


1.25
log
@Another sync of uvm to NetBSD. Just minor fiddling, no major changes.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_glue.c,v 1.24 2001/11/06 18:41:10 art Exp $	*/
/*	$NetBSD: uvm_glue.c,v 1.43 2000/11/25 06:27:59 chs Exp $	*/
a96 3

unsigned maxdmap = MAXDSIZ;	/* kern_resource.c: RLIMIT_DATA max */
unsigned maxsmap = MAXSSIZ;	/* kern_resource.c: RLIMIT_STACK max */
@


1.24
log
@Let fork1, uvm_fork, and cpu_fork take a function/argument pair as argument,
instead of doing fork1, cpu_set_kpc. This lets us retire cpu_set_kpc and
avoid a multiprocessor race.

This commit breaks vax because it doesn't look like any other arch, someone
working on vax might want to look at this and try to adapt the code to be
more like the rest of the world.

Idea and uvm parts from NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_glue.c,v 1.23 2001/11/06 13:36:52 art Exp $	*/
/*	$NetBSD: uvm_glue.c,v 1.40 2000/08/21 02:29:32 thorpej Exp $	*/
d122 1
a122 1
	eaddr = round_page((vaddr_t)addr+len);
d163 1
a163 1
	    round_page((vaddr_t)addr+len), prot);
d249 1
a249 1
		round_page((vaddr_t)addr+len));
d279 2
a280 1
	if (shared == TRUE)
d282 1
a282 1
	else
d305 2
a306 2
	(unsigned) ((caddr_t)&up->u_stats.pstat_endzero -
		    (caddr_t)&up->u_stats.pstat_startzero));
d308 2
a309 2
	((caddr_t)&up->u_stats.pstat_endcopy -
	 (caddr_t)&up->u_stats.pstat_startcopy));
d333 1
d336 3
a338 1
	uvm_km_free(kernel_map, (vaddr_t)p->p_addr, USPACE);
a418 1
	UVMHIST_FUNC("uvm_scheduler"); UVMHIST_CALLED(maphist);
d423 1
a423 1
		tsleep((caddr_t)&proc0, PVM, "noswap", 0);
d427 1
a427 1
	for (p = allproc.lh_first; p != 0; p = p->p_list.le_next) {
d448 1
a448 1
		tsleep((caddr_t)&proc0, PVM, "scheduler", 0);
d524 1
a524 1
	for (p = allproc.lh_first; p != 0; p = p->p_list.le_next) {
d538 1
a538 1
				uvm_swapout(p);			/* zap! */
d564 1
a595 7
	 * Unwire the to-be-swapped process's user struct and kernel stack.
	 */
	addr = (vaddr_t)p->p_addr;
	uvm_fault_unwire(kernel_map, addr, addr + USPACE); /* !P_INMEM */
	pmap_collect(vm_map_pmap(&p->p_vmspace->vm_map));

	/*
d605 7
@


1.23
log
@More sync to NetBSD.
 - Use malloc/free instead of MALLOC/FREE for variable sized allocations.
 - Move the memory inheritance code to sys/mman.h and rename from VM_* to MAP_*
 - various cleanups and simplifications.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.22 2001/11/06 01:35:04 art Exp $	*/
d268 1
a268 1
uvm_fork(p1, p2, shared, stack, stacksize)
d273 2
d311 5
a315 3
	 * cpu_fork will copy and update the kernel stack and pcb, and make
	 * the child ready to run.  The child will exit directly to user
	 * mode on its first time slice, and will not return here.
d317 1
a317 1
	cpu_fork(p1, p2, stack, stacksize);
@


1.22
log
@Move the last content from vm/ to uvm/
The only thing left in vm/ are just dumb wrappers.
vm/vm.h includes uvm/uvm_extern.h
vm/pmap.h includes uvm/uvm_pmap.h
vm/vm_page.h includes uvm/uvm_page.h
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_glue.c,v 1.21 2001/11/05 22:14:54 art Exp $	*/
/*	$NetBSD: uvm_glue.c,v 1.38 2000/06/27 17:29:22 mrg Exp $	*/
@


1.21
log
@Minor sync to NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_glue.c,v 1.20 2001/09/19 20:50:59 mickey Exp $	*/
/*	$NetBSD: uvm_glue.c,v 1.37 2000/06/26 14:21:17 mrg Exp $	*/
a82 2

#include <vm/vm.h>
@


1.20
log
@merge vm/vm_kern.h into uvm/uvm_extern.h; art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_glue.c,v 1.19 2001/08/11 10:57:22 art Exp $	*/
/*	$NetBSD: uvm_glue.c,v 1.36 2000/06/18 05:20:27 simonb Exp $	*/
a84 1
#include <vm/vm_page.h>
@


1.19
log
@Various random fixes from NetBSD.
Including support for zeroing pages in the idle loop (not enabled yet).
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.18 2001/08/06 14:03:04 art Exp $	*/
a85 1
#include <vm/vm_kern.h>
@


1.18
log
@Add a new type voff_t (right now it's typedefed as off_t) used for offsets
into objects.

Gives the possibilty to mmap beyond the size of vaddr_t.

From NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_glue.c,v 1.17 2001/07/25 13:25:33 art Exp $	*/
/*	$NetBSD: uvm_glue.c,v 1.31 2000/03/26 20:54:47 kleink Exp $	*/
a109 25
 * uvm_sleep: atomic unlock and sleep for UVM_UNLOCK_AND_WAIT().
 */

void
uvm_sleep(event, slock, canintr, msg, timo)
	void *event;
	struct simplelock *slock;
	boolean_t canintr;
	const char *msg;
	int timo;
{
	int s, pri;

	pri = PVM;
	if (canintr)
		pri |= PCATCH;

	s = splhigh();
	if (slock != NULL)
		simple_unlock(slock);
	(void) tsleep(event, pri, (char *)msg, timo);
	splx(s);
}

/*
d188 1
a188 1
	register caddr_t addr;
d411 2
a412 2
	register struct proc *p;
	register int pri;
d503 1
a503 1
	register struct proc *p;
d573 1
a573 1
	register struct proc *p;
@


1.17
log
@Change the pmap_enter interface to merge access_type and the wired boolean
and arbitrary flags into one argument.

One new flag is PMAP_CANFAIL that tells pmap_enter that it can fail if there
are not enough resources to satisfy the request. If this flag is not passed,
pmap_enter should panic as it should have done before this change (XXX - many
pmaps are still not doing that).

Only i386 and alpha implement CANFAIL for now.

Includes uvm updates from NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_glue.c,v 1.16 2001/06/23 19:24:33 smart Exp $	*/
/*	$NetBSD: uvm_glue.c,v 1.30 1999/11/13 00:24:38 thorpej Exp $	*/
d192 1
a192 1
		round_page((vaddr_t)addr+len), prot);
@


1.16
log
@Sync with NetBSD 19990911 (just before PMAP_NEW was required)
  - thread_sleep_msg() -> uvm_sleep()
  - initialize reference count lock in uvm_anon_{init,add}()
  - add uao_flush()
  - replace boolean 'islocked' with 'lockflags'
  - in uvm_fault() change FALSE to TRUE to in 'wide' fault handling
  - get rid of uvm_km_get()
  - various bug fixes
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_glue.c,v 1.15 2001/06/08 08:09:39 art Exp $	*/
/*	$NetBSD: uvm_glue.c,v 1.29 1999/07/25 06:30:36 thorpej Exp $	*/
d232 1
a232 1
		pmap_enter(pmap_kernel(), sva, pa, prot, TRUE, 0);
@


1.15
log
@Change the paddr_t pmap_extract(struct pmap *, vaddr_t) interface to
boolean_t pmap_extract(struct pmap *, vaddr_t, paddr_t *).
Matches NetBSD. Tested by various people on various platforms.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_glue.c,v 1.14 2001/05/07 16:08:40 art Exp $	*/
/*	$NetBSD: uvm_glue.c,v 1.27 1999/07/08 18:11:03 thorpej Exp $	*/
d110 25
d326 1
a326 1
	 * p_stats currently point at fields in the user struct.  Copy
@


1.14
log
@Few fixes from NetBSD.
 - make sure that vsunlock doesn't unwire mlocked memory.
 - fix locking in uvm_useracc.
 - Return the error uvm_fault_wire in uvm_vslock (will be used soon).
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_glue.c,v 1.13 2001/05/05 23:25:54 art Exp $	*/
/*	$NetBSD: uvm_glue.c,v 1.26 1999/06/17 15:47:22 thorpej Exp $	*/
d205 1
a205 2
		pa = pmap_extract(pmap_kernel(), sva|1);
		if (pa == 0)
d207 1
a207 1
		pmap_enter(pmap_kernel(), sva, pa&~1, prot, TRUE, 0);
@


1.13
log
@PMAP_NEW and UVM are no longer optional on i386.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_glue.c,v 1.12 2001/05/05 21:26:45 art Exp $	*/
/*	$NetBSD: uvm_glue.c,v 1.23 1999/05/28 20:49:51 thorpej Exp $	*/
d158 1
d162 8
a169 2
	rv = uvm_map_checkprot(&curproc->p_vmspace->vm_map,
			trunc_page((vaddr_t)addr), round_page((vaddr_t)addr+len), prot);
d220 1
a220 1
void
d227 9
d237 1
a237 2
	uvm_fault_wire(&p->p_vmspace->vm_map, trunc_page((vaddr_t)addr),
	    round_page((vaddr_t)addr+len), access_type);
@


1.12
log
@Remove the (vaddr_t) casts inside the round_page and trunc_page macros.
We might want to use them on types that are bigger than vaddr_t.

Fix all callers that pass pointers without casts.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.11 2001/04/03 15:28:06 aaron Exp $	*/
a159 11

#if (defined(i386) || defined(pc532)) && !defined(PMAP_NEW)
	/*
	 * XXX - specially disallow access to user page tables - they are
	 * in the map.  This is here until i386 & pc532 pmaps are fixed...
	 */
	if ((vaddr_t) addr >= VM_MAXUSER_ADDRESS
	    || (vaddr_t) addr + len > VM_MAXUSER_ADDRESS
	    || (vaddr_t) addr + len <= (vaddr_t) addr)
		return (FALSE);
#endif
@


1.11
log
@It is unnecessary to wrap uvm_wait() in splhigh(). Also, set p_addr to NULL
when we free it; art@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.10 2001/04/02 21:43:12 niklas Exp $	*/
d125 2
a126 2
	saddr = trunc_page(addr);
	eaddr = round_page(addr+len);
d173 1
a173 1
			trunc_page(addr), round_page(addr+len), prot);
d201 2
a202 2
	eva = round_page(addr + len);
	for (sva = trunc_page(addr); sva < eva; sva += PAGE_SIZE) {
d232 2
a233 2
	uvm_fault_wire(&p->p_vmspace->vm_map, trunc_page(addr), 
	    round_page(addr+len), access_type);
d249 2
a250 2
	uvm_fault_unwire(&p->p_vmspace->vm_map, trunc_page(addr), 
		round_page(addr+len));
@


1.10
log
@On popular demand, the Linux-compatibility clone(2) implementation based
on NetBSD's code, as well as some faked Posix RT extensions by me.  This makes
at least simple linuxthreads tests work.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.9 2001/01/29 02:07:44 niklas Exp $	*/
d332 1
a470 1
	(void) splhigh();
a471 1
	(void) spl0();
@


1.9
log
@$OpenBSD$
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_glue.c,v 1.23 1999/05/28 20:49:51 thorpej Exp $	*/
d298 2
a299 3
	 * p_stats and p_sigacts currently point at fields in the user
	 * struct but not at &u, instead at p_addr.  Copy p_sigacts and
	 * parts of p_stats; zero the rest of p_stats (statistics).
a301 2
	p2->p_sigacts = &up->u_sigacts;
	up->u_sigacts = *p1->p_sigacts;
@


1.8
log
@Convert bzero to memset(X, 0..) and bcopy to memcpy.
This is to match (make diffs smaller) the code in NetBSD.
new gcc inlines those functions, so this could also be a performance win.
@
text
@d1 1
@


1.7
log
@Bring in some new UVM code from NetBSD (not current).

 - Introduce a new type of map that are interrupt safe and never allow faults
   in them. mb_map and kmem_map are made intrsafe.
 - Add "access protection" to uvm_vslock (to be passed down to uvm_fault and
   later to pmap_enter).
 - madvise(2) now works.
 - various cleanups.
@
text
@d304 1
a304 1
	bzero(&up->u_stats.pstat_startzero,
d307 1
a307 1
	bcopy(&p1->p_stats->pstat_startcopy, &up->u_stats.pstat_startcopy, 
@


1.6
log
@Prepare for new pmap
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_glue.c,v 1.19 1999/04/30 21:23:50 thorpej Exp $	*/
d224 1
a224 1
uvm_vslock(p, addr, len)
d228 1
d230 1
d232 1
a232 1
	    round_page(addr+len));
d248 1
a248 1
	uvm_fault_unwire(p->p_vmspace->vm_map.pmap, trunc_page(addr), 
d287 3
d292 1
a292 1
	    (vaddr_t)up + USPACE);
d381 2
a382 1
	uvm_fault_wire(kernel_map, addr, addr + USPACE);
d595 1
a595 1
	uvm_fault_unwire(kernel_map->pmap, addr, addr + USPACE); /* !P_INMEM */
@


1.5
log
@Change the pmap_enter api to pass down an argument that indicates
the access type that caused this mapping. This is to simplify pmaps
with mod/ref emulation (none for the moment) and in some cases speed
up pmap_is_{referenced,modified}.
At the same time, clean up some mappings that had too high protection.

XXX - the access type is incorrect in old vm, it's only used by uvm and MD code.
The actual use of this in pmap_enter implementations is not in this commit.
@
text
@d160 1
a160 1
#if defined(i386) || defined(pc532)
@


1.5.4.1
log
@In preparation for a new pmap
@
text
@d160 1
a160 1
#if (defined(i386) || defined(pc532)) && !defined(PMAP_NEW)
@


1.5.4.2
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_glue.c,v 1.23 1999/05/28 20:49:51 thorpej Exp $	*/
d224 1
a224 1
uvm_vslock(p, addr, len, access_type)
a227 1
	vm_prot_t access_type;
a228 1

d230 1
a230 1
	    round_page(addr+len), access_type);
d246 1
a246 1
	uvm_fault_unwire(&p->p_vmspace->vm_map, trunc_page(addr), 
a284 3
	 *
	 * Note the kernel stack gets read/write accesses right off
	 * the bat.
d287 1
a287 1
	    (vaddr_t)up + USPACE, VM_PROT_READ | VM_PROT_WRITE);
d376 1
a376 2
	uvm_fault_wire(kernel_map, addr, addr + USPACE,
	    VM_PROT_READ | VM_PROT_WRITE);
d589 1
a589 1
	uvm_fault_unwire(kernel_map, addr, addr + USPACE); /* !P_INMEM */
@


1.5.4.3
log
@merge in approximately 2.9 into SMP branch
@
text
@a0 1
/*	$OpenBSD: uvm_glue.c,v 1.11 2001/04/03 15:28:06 aaron Exp $	*/
d297 3
a299 2
	 * p_stats currently point at fields in the user struct.  Copy
	 * parts of p_stats, and zero out the rest.
d302 3
a304 1
	memset(&up->u_stats.pstat_startzero, 0,
d307 1
a307 1
	memcpy(&up->u_stats.pstat_startcopy, &p1->p_stats->pstat_startcopy,
a333 1
	p->p_addr = NULL;
d472 1
d474 1
@


1.5.4.4
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_glue.c,v 1.29 1999/07/25 06:30:36 thorpej Exp $	*/
a109 25
 * uvm_sleep: atomic unlock and sleep for UVM_UNLOCK_AND_WAIT().
 */

void
uvm_sleep(event, slock, canintr, msg, timo)
	void *event;
	struct simplelock *slock;
	boolean_t canintr;
	const char *msg;
	int timo;
{
	int s, pri;

	pri = PVM;
	if (canintr)
		pri |= PCATCH;

	s = splhigh();
	if (slock != NULL)
		simple_unlock(slock);
	(void) tsleep(event, pri, (char *)msg, timo);
	splx(s);
}

/*
d125 2
a126 2
	saddr = trunc_page((vaddr_t)addr);
	eaddr = round_page((vaddr_t)addr+len);
a157 1
	vm_map_t map;
d161 10
a170 7
	/* XXX curproc */
	map = &curproc->p_vmspace->vm_map;

	vm_map_lock_read(map);
	rv = uvm_map_checkprot(map, trunc_page((vaddr_t)addr),
		round_page((vaddr_t)addr+len), prot);
	vm_map_unlock_read(map);
d172 2
d201 2
a202 2
	eva = round_page((vaddr_t)addr + len);
	for (sva = trunc_page((vaddr_t)addr); sva < eva; sva += PAGE_SIZE) {
d209 2
a210 1
		if (pmap_extract(pmap_kernel(), sva, &pa) == FALSE)
d212 1
a212 1
		pmap_enter(pmap_kernel(), sva, pa, prot, TRUE, 0);
d224 1
a224 1
int
a230 9
	vm_map_t map;
	vaddr_t start, end;
	int rv;

	map = &p->p_vmspace->vm_map;
	start = trunc_page((vaddr_t)addr);
	end = round_page((vaddr_t)addr + len);

	rv = uvm_fault_wire(map, start, end, access_type);
d232 2
a233 1
	return (rv);
d249 2
a250 2
	uvm_fault_unwire(&p->p_vmspace->vm_map, trunc_page((vaddr_t)addr),
		round_page((vaddr_t)addr+len));
d298 1
a298 1
	 * p_stats currently points at a field in the user struct.  Copy
@


1.5.4.5
log
@Sync the SMP branch to something just after 3.0
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_glue.c,v 1.36 2000/06/18 05:20:27 simonb Exp $	*/
d86 1
d110 25
d192 1
a192 1
	    round_page((vaddr_t)addr+len), prot);
d213 1
a213 1
	caddr_t addr;
d232 1
a232 1
		pmap_enter(pmap_kernel(), sva, pa, prot, PMAP_WIRED);
d436 2
a437 2
	struct proc *p;
	int pri;
d528 1
a528 1
	struct proc *p;
d598 1
a598 1
	struct proc *p;
@


1.5.4.6
log
@merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_glue.c,v 1.44 2001/02/06 19:54:44 eeh Exp $	*/
d84 3
d101 3
d125 1
a125 1
	eaddr = round_page((vaddr_t)addr + len);
d166 1
a166 1
	    round_page((vaddr_t)addr + len), prot);
d252 1
a252 1
		round_page((vaddr_t)addr + len));
d271 1
a271 1
uvm_fork(p1, p2, shared, stack, stacksize, func, arg)
a275 2
	void (*func) __P((void *));
	void *arg;
d280 1
a280 2
	if (shared == TRUE) {
		p2->p_vmspace = NULL;
d282 1
a282 1
	} else
d305 2
a306 2
	       ((caddr_t)&up->u_stats.pstat_endzero -
		(caddr_t)&up->u_stats.pstat_startzero));
d308 2
a309 2
	       ((caddr_t)&up->u_stats.pstat_endcopy -
		(caddr_t)&up->u_stats.pstat_startcopy));
d312 3
a314 5
	 * cpu_fork() copy and update the pcb, and make the child ready
	 * to run.  If this is a normal user fork, the child will exit
	 * directly to user mode via child_return() on its first time
	 * slice and will not return here.  If this is a kernel thread,
	 * the specified entry point will be executed.
d316 1
a316 1
	cpu_fork(p1, p2, stack, stacksize, func, arg);
a330 1
	vaddr_t va = (vaddr_t)p->p_addr;
d333 1
a333 3
	p->p_flag &= ~P_INMEM;
	uvm_fault_unwire(kernel_map, va, va + USPACE);
	uvm_km_free(kernel_map, va, USPACE);
d414 1
d419 1
a419 1
		tsleep(&proc0, PVM, "noswap", 0);
d423 1
a423 1
	LIST_FOREACH(p, &allproc, p_list) {
d444 1
a444 1
		tsleep(&proc0, PVM, "scheduler", 0);
d520 1
a520 1
	LIST_FOREACH(p, &allproc, p_list) {
d534 1
a534 1
				uvm_swapout(p);
a559 1
	pmap_update();
d591 7
a606 7

	/*
	 * Unwire the to-be-swapped process's user struct and kernel stack.
	 */
	addr = (vaddr_t)p->p_addr;
	uvm_fault_unwire(kernel_map, addr, addr + USPACE); /* !P_INMEM */
	pmap_collect(vm_map_pmap(&p->p_vmspace->vm_map));
@


1.5.4.7
log
@Merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_glue.c,v 1.50 2001/06/02 18:09:26 chs Exp $	*/
d4 1
a4 1
/*
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and
d48 1
a48 1
 *
d54 3
a56 3
 *
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS"
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND
d58 1
a58 1
 *
d151 1
a151 1
	struct vm_map *map;
d194 3
a201 1
	pmap_update();
d219 1
a219 1
	struct vm_map *map;
d221 1
a221 1
	int error;
d226 4
a229 2
	error = uvm_fault_wire(map, start, end, access_type);
	return error;
d274 1
a274 1
	int error;
d291 1
a291 1
	error = uvm_fault_wire(kernel_map, (vaddr_t)up,
d293 2
a294 2
	if (error)
		panic("uvm_fork: uvm_fault_wire failed: %d", error);
d307 1
a307 1

d507 1
a507 1
	extern int maxslp;
d531 1
a531 1

d561 1
d567 1
a567 1
 * - currently "swapout" means "unwire U-area" and "pmap_collect()"
@


1.5.4.8
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_glue.c,v 1.44 2001/02/06 19:54:44 eeh Exp $	*/
d4 1
a4 1
/* 
d6 1
a6 1
 * Copyright (c) 1991, 1993, The Regents of the University of California.  
d24 1
a24 1
 *      Washington University, the University of California, Berkeley and 
d48 1
a48 1
 * 
d54 3
a56 3
 * 
 * CARNEGIE MELLON ALLOWS FREE USE OF THIS SOFTWARE IN ITS "AS IS" 
 * CONDITION.  CARNEGIE MELLON DISCLAIMS ANY LIABILITY OF ANY KIND 
d58 1
a58 1
 * 
d151 1
a151 1
	vm_map_t map;
a193 3
		 * We use a cheezy hack to differentiate physical
		 * page 0 from an invalid mapping, not that it
		 * really matters...
d199 1
d217 1
a217 1
	vm_map_t map;
d219 1
a219 1
	int rv;
d224 2
a225 4

	rv = uvm_fault_wire(map, start, end, access_type);

	return (rv);
d270 1
a270 1
	int rv;
d287 1
a287 1
	rv = uvm_fault_wire(kernel_map, (vaddr_t)up,
d289 2
a290 2
	if (rv != KERN_SUCCESS)
		panic("uvm_fork: uvm_fault_wire failed: %d", rv);
d303 1
a303 1
	
d503 1
a503 1
	extern int maxslp; 
d527 1
a527 1
			
d562 1
a562 1
 * - currently "swapout" means "unwire U-area" and "pmap_collect()" 
@


1.5.4.9
log
@Merge in -current from roughly a week ago
@
text
@d92 1
a92 1
static void uvm_swapout(struct proc *);
d270 1
a270 1
	void (*func)(void *);
@


1.5.4.10
log
@Sync the SMP branch with 3.3
@
text
@d380 1
a380 1
	int rv, s;
d384 2
a385 3
	if ((rv = uvm_fault_wire(kernel_map, addr, addr + USPACE,
	    VM_PROT_READ | VM_PROT_WRITE)) != KERN_SUCCESS)
		panic("uvm_swapin: uvm_fault_wire failed: %d", rv);
d453 1
a453 1
	 * XXX: this part is really bogus because we could deadlock on memory
@


1.5.4.11
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d139 27
d206 1
a206 1
 * uvm_vslock: wire user memory for I/O
a225 2
	if (end <= start)
		return (EINVAL);
d233 1
a233 1
 * uvm_vsunlock: unwire user memory wired by uvm_vslock()
d245 2
a246 8
	vaddr_t start, end;

	start = trunc_page((vaddr_t)addr);
	end = round_page((vaddr_t)addr + len);
	if (end <= start)
		return;

	uvm_fault_unwire(&p->p_vmspace->vm_map, start, end);
a381 8
	s = splstatclock();
	if (p->p_flag & P_SWAPIN) {
		splx(s);
		return;
	}
	p->p_flag |= P_SWAPIN;
	splx(s);

a396 1
	p->p_flag &= ~P_SWAPIN;
d582 2
a583 2
		    p->p_pid, p->p_comm, p->p_addr, p->p_stat,
		    p->p_slptime, uvmexp.free);
d587 6
a595 4
	if (!(p->p_flag & P_INMEM)) {
		splx(s);
		return;
	}
a601 6

	/*
	 * Do any machine-specific actions necessary before swapout.
	 * This can include saving floating point state, etc.
	 */
	cpu_swapout(p);
@


1.5.4.12
log
@Merge with the trunk
@
text
@a174 1
	pmap_update(pmap_kernel());
@


1.4
log
@sync with NetBSD from 1999.05.24 (there is a reason for this date)
 Mostly cleanups, but also a few improvements to pagedaemon for better
 handling of low memory and/or low swap conditions.
@
text
@d211 1
a211 1
		pmap_enter(pmap_kernel(), sva, pa&~1, prot, TRUE);
@


1.3
log
@New cpu_fork API to take a stack in which you point the child's stackpointer
to, at the bottom or the top, depending on your architecture's stack growth
direction.  This is in preparation for Linux' clone(2) emulation.
port maintainers, please check that I did the work right.
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_glue.c,v 1.2 1999/02/26 05:32:06 art Exp $	*/
/*	$NetBSD: uvm_glue.c,v 1.15 1998/10/19 22:21:19 tron Exp $	*/
a2 4
/*
 * XXXCDC: "ROUGH DRAFT" QUALITY UVM PRE-RELEASE FILE!
 *         >>>USE AT YOUR OWN RISK, WORK IS NOT FINISHED<<<
 */
d256 2
a465 2
	printf("scheduler: no room for pid %d(%s), free %d\n",
	   p->p_pid, p->p_comm, uvmexp.free);/*XXXCDC: HIGHLY BOGUS */
@


1.2
log
@add OpenBSD tags
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d269 1
a269 1
uvm_fork(p1, p2, shared)
d272 2
d309 1
a309 1
/*
d314 1
a314 1
	cpu_fork(p1, p2);
@


1.1
log
@Import of uvm from NetBSD. Some local changes, some code disabled
@
text
@d1 1
@

