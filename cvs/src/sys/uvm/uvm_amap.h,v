head	1.30;
access;
symbols
	OPENBSD_6_1_BASE:1.30
	OPENBSD_6_0:1.29.0.2
	OPENBSD_6_0_BASE:1.29
	OPENBSD_5_9:1.20.0.6
	OPENBSD_5_9_BASE:1.20
	OPENBSD_5_8:1.20.0.8
	OPENBSD_5_8_BASE:1.20
	OPENBSD_5_7:1.20.0.2
	OPENBSD_5_7_BASE:1.20
	OPENBSD_5_6:1.20.0.4
	OPENBSD_5_6_BASE:1.20
	OPENBSD_5_5:1.19.0.6
	OPENBSD_5_5_BASE:1.19
	OPENBSD_5_4:1.19.0.2
	OPENBSD_5_4_BASE:1.19
	OPENBSD_5_3:1.18.0.16
	OPENBSD_5_3_BASE:1.18
	OPENBSD_5_2:1.18.0.14
	OPENBSD_5_2_BASE:1.18
	OPENBSD_5_1_BASE:1.18
	OPENBSD_5_1:1.18.0.12
	OPENBSD_5_0:1.18.0.10
	OPENBSD_5_0_BASE:1.18
	OPENBSD_4_9:1.18.0.8
	OPENBSD_4_9_BASE:1.18
	OPENBSD_4_8:1.18.0.6
	OPENBSD_4_8_BASE:1.18
	OPENBSD_4_7:1.18.0.2
	OPENBSD_4_7_BASE:1.18
	OPENBSD_4_6:1.18.0.4
	OPENBSD_4_6_BASE:1.18
	OPENBSD_4_5:1.17.0.8
	OPENBSD_4_5_BASE:1.17
	OPENBSD_4_4:1.17.0.6
	OPENBSD_4_4_BASE:1.17
	OPENBSD_4_3:1.17.0.4
	OPENBSD_4_3_BASE:1.17
	OPENBSD_4_2:1.17.0.2
	OPENBSD_4_2_BASE:1.17
	OPENBSD_4_1:1.15.0.4
	OPENBSD_4_1_BASE:1.15
	OPENBSD_4_0:1.15.0.2
	OPENBSD_4_0_BASE:1.15
	OPENBSD_3_9:1.13.0.6
	OPENBSD_3_9_BASE:1.13
	OPENBSD_3_8:1.13.0.4
	OPENBSD_3_8_BASE:1.13
	OPENBSD_3_7:1.13.0.2
	OPENBSD_3_7_BASE:1.13
	OPENBSD_3_6:1.12.0.12
	OPENBSD_3_6_BASE:1.12
	SMP_SYNC_A:1.12
	SMP_SYNC_B:1.12
	OPENBSD_3_5:1.12.0.10
	OPENBSD_3_5_BASE:1.12
	OPENBSD_3_4:1.12.0.8
	OPENBSD_3_4_BASE:1.12
	UBC_SYNC_A:1.12
	OPENBSD_3_3:1.12.0.6
	OPENBSD_3_3_BASE:1.12
	OPENBSD_3_2:1.12.0.4
	OPENBSD_3_2_BASE:1.12
	OPENBSD_3_1:1.12.0.2
	OPENBSD_3_1_BASE:1.12
	UBC_SYNC_B:1.12
	UBC:1.9.0.2
	UBC_BASE:1.9
	OPENBSD_3_0:1.6.0.2
	OPENBSD_3_0_BASE:1.6
	OPENBSD_2_9_BASE:1.5
	OPENBSD_2_9:1.5.0.2
	OPENBSD_2_8:1.3.0.4
	OPENBSD_2_8_BASE:1.3
	OPENBSD_2_7:1.3.0.2
	OPENBSD_2_7_BASE:1.3
	SMP:1.2.0.6
	SMP_BASE:1.2
	kame_19991208:1.2
	OPENBSD_2_6:1.2.0.4
	OPENBSD_2_6_BASE:1.2
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.30
date	2017.02.05.01.11.50;	author guenther;	state Exp;
branches;
next	1.29;
commitid	8XTueqM4k5roOt9t;

1.29
date	2016.05.26.13.37.26;	author stefan;	state Exp;
branches;
next	1.28;
commitid	eiMXKHK3UupAUyDE;

1.28
date	2016.05.22.22.52.01;	author guenther;	state Exp;
branches;
next	1.27;
commitid	KhyebnIkY686CCxA;

1.27
date	2016.05.22.16.18.26;	author stefan;	state Exp;
branches;
next	1.26;
commitid	biGCNWraZJf92vP1;

1.26
date	2016.05.08.16.29.57;	author stefan;	state Exp;
branches;
next	1.25;
commitid	szK6LyawtrqhVtcp;

1.25
date	2016.05.08.11.52.32;	author stefan;	state Exp;
branches;
next	1.24;
commitid	hUj20vPhiD6DQNDL;

1.24
date	2016.04.16.18.39.31;	author stefan;	state Exp;
branches;
next	1.23;
commitid	4OGLhCEcSCff5pGJ;

1.23
date	2016.04.04.16.34.16;	author stefan;	state Exp;
branches;
next	1.22;
commitid	mErYIUO2MMXVvZFw;

1.22
date	2016.03.27.09.51.37;	author stefan;	state Exp;
branches;
next	1.21;
commitid	vrFSc4NQkLUJ1a8U;

1.21
date	2016.03.06.14.47.07;	author stefan;	state Exp;
branches;
next	1.20;
commitid	j1TTEtBJGEcXIrnP;

1.20
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.19;
commitid	7NtJNW9udCOFtDNM;

1.19
date	2013.05.30.16.29.46;	author tedu;	state Exp;
branches;
next	1.18;

1.18
date	2009.03.25.20.00.18;	author oga;	state Exp;
branches;
next	1.17;

1.17
date	2007.06.18.21.51.15;	author pedro;	state Exp;
branches;
next	1.16;

1.16
date	2007.05.31.21.20.30;	author thib;	state Exp;
branches;
next	1.15;

1.15
date	2006.07.13.22.51.26;	author deraadt;	state Exp;
branches;
next	1.14;

1.14
date	2006.06.21.16.20.05;	author mickey;	state Exp;
branches;
next	1.13;

1.13
date	2004.12.30.08.28.39;	author niklas;	state Exp;
branches;
next	1.12;

1.12
date	2002.03.15.01.20.04;	author millert;	state Exp;
branches;
next	1.11;

1.11
date	2002.03.14.01.27.18;	author millert;	state Exp;
branches;
next	1.10;

1.10
date	2001.12.19.08.58.07;	author art;	state Exp;
branches;
next	1.9;

1.9
date	2001.11.28.19.28.14;	author art;	state Exp;
branches
	1.9.2.1;
next	1.8;

1.8
date	2001.11.11.01.16.56;	author art;	state Exp;
branches;
next	1.7;

1.7
date	2001.11.07.02.55.50;	author art;	state Exp;
branches;
next	1.6;

1.6
date	2001.05.10.14.51.21;	author art;	state Exp;
branches;
next	1.5;

1.5
date	2001.03.09.05.34.38;	author smart;	state Exp;
branches;
next	1.4;

1.4
date	2001.01.29.02.07.42;	author niklas;	state Exp;
branches;
next	1.3;

1.3
date	2000.03.15.15.50.18;	author art;	state Exp;
branches;
next	1.2;

1.2
date	99.02.26.05.32.06;	author art;	state Exp;
branches
	1.2.6.1;
next	1.1;

1.1
date	99.02.26.01.30.10;	author art;	state Exp;
branches;
next	;

1.2.6.1
date	2000.03.24.09.09.48;	author niklas;	state Exp;
branches;
next	1.2.6.2;

1.2.6.2
date	2001.05.14.22.47.44;	author niklas;	state Exp;
branches;
next	1.2.6.3;

1.2.6.3
date	2001.07.04.11.01.00;	author niklas;	state Exp;
branches;
next	1.2.6.4;

1.2.6.4
date	2001.11.13.23.02.31;	author niklas;	state Exp;
branches;
next	1.2.6.5;

1.2.6.5
date	2001.12.05.01.19.55;	author niklas;	state Exp;
branches;
next	1.2.6.6;

1.2.6.6
date	2002.03.06.02.17.14;	author niklas;	state Exp;
branches;
next	1.2.6.7;

1.2.6.7
date	2002.03.28.14.54.26;	author niklas;	state Exp;
branches;
next	;

1.9.2.1
date	2002.06.11.03.33.03;	author art;	state Exp;
branches;
next	1.9.2.2;

1.9.2.2
date	2002.11.04.18.02.32;	author art;	state Exp;
branches;
next	;


desc
@@


1.30
log
@Update a comment that suggested the stack was executable.  Nope!
@
text
@/*	$OpenBSD: uvm_amap.h,v 1.29 2016/05/26 13:37:26 stefan Exp $	*/
/*	$NetBSD: uvm_amap.h,v 1.14 2001/02/18 21:19:08 chs Exp $	*/

/*
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

#ifndef _UVM_UVM_AMAP_H_
#define _UVM_UVM_AMAP_H_

/*
 * uvm_amap.h: general amap interface and amap implementation-specific info
 */

/*
 * an amap structure contains pointers to a set of anons that are
 * mapped together in virtual memory (an anon is a single page of
 * anonymous virtual memory -- see uvm_anon.h).  in uvm we hide the
 * details of the implementation of amaps behind a general amap
 * interface.  this allows us to change the amap implementation
 * without having to touch the rest of the code.  this file is divided
 * into two parts: the definition of the uvm amap interface and the
 * amap implementation-specific definitions.
 */

#ifdef _KERNEL

/*
 * part 1: amap interface
 */

/*
 * forward definition of vm_amap structure.  only amap
 * implementation-specific code should directly access the fields of
 * this structure.  
 */

struct vm_amap;

/*
 * prototypes for the amap interface 
 */

					/* ensure amap can store anon */
void		amap_populate(struct vm_aref *, vaddr_t);
					/* add an anon to an amap */
int		amap_add(struct vm_aref *, vaddr_t, struct vm_anon *,
		    boolean_t);
					/* allocate a new amap */
struct vm_amap	*amap_alloc(vaddr_t, int, int);
					/* clear amap needs-copy flag */
void		amap_copy(vm_map_t, vm_map_entry_t, int, boolean_t, vaddr_t,
		    vaddr_t);
					/* resolve all COW faults now */
void		amap_cow_now(vm_map_t, vm_map_entry_t);
					/* free amap */
void		amap_free(struct vm_amap *);
					/* init amap module (at boot time) */
void		amap_init(void);
					/* lookup an anon @@ offset in amap */
struct vm_anon	*amap_lookup(struct vm_aref *, vaddr_t);
					/* lookup multiple anons */
void		amap_lookups(struct vm_aref *, vaddr_t, struct vm_anon **, int);
					/* add a reference to an amap */
void		amap_ref(struct vm_amap *, vaddr_t, vsize_t, int);
					/* split reference to amap into two */
void		amap_splitref(struct vm_aref *, struct vm_aref *, vaddr_t);
					/* remove an anon from an amap */
void		amap_unadd(struct vm_aref *, vaddr_t);
					/* drop reference to an amap */
void		amap_unref(struct vm_amap *, vaddr_t, vsize_t, int);
					/* remove all anons from amap */
void		amap_wipeout(struct vm_amap *);
boolean_t	amap_swap_off(int, int);

/*
 * amap flag values
 */

#define AMAP_SHARED	0x1	/* amap is shared */
#define AMAP_REFALL	0x2	/* amap_ref: reference entire amap */
#define AMAP_SWAPOFF	0x4	/* amap_swap_off() is in progress */

#endif /* _KERNEL */

/**********************************************************************/

/*
 * part 2: amap implementation-specific info
 */

/*
 * we currently provide an array-based amap implementation.  in this
 * implementation we provide the option of tracking split references
 * so that we don't lose track of references during partial unmaps
 * ... this is enabled with the "UVM_AMAP_PPREF" define.
 */

#define UVM_AMAP_PPREF		/* track partial references */

/*
 * here is the definition of the vm_amap structure and helper structures for
 * this implementation.
 */

struct vm_amap_chunk {
	TAILQ_ENTRY(vm_amap_chunk) ac_list;
	int ac_baseslot;
	uint16_t ac_usedmap;
	uint16_t ac_nslot;
	struct vm_anon *ac_anon[];
};

struct vm_amap {
	int am_ref;		/* reference count */
	int am_flags;		/* flags */
	int am_nslot;		/* # of slots currently in map */
	int am_nused;		/* # of slots currently in use */
#ifdef UVM_AMAP_PPREF
	int *am_ppref;		/* per page reference count (if !NULL) */
#endif
	LIST_ENTRY(vm_amap) am_list;

	union {
		struct {
			struct vm_amap_chunk **amn_buckets;
			TAILQ_HEAD(, vm_amap_chunk) amn_chunks;
			int amn_ncused;	/* # of chunkers currently in use */
			int amn_hashshift; /* shift count to hash slot to bucket */
		} ami_normal;

		/*
		 * MUST be last element in vm_amap because it contains a
		 * variably sized array element.
		 */
		struct vm_amap_chunk ami_small;
	} am_impl;

#define am_buckets	am_impl.ami_normal.amn_buckets
#define am_chunks	am_impl.ami_normal.amn_chunks
#define am_ncused	am_impl.ami_normal.amn_ncused
#define am_hashshift	am_impl.ami_normal.amn_hashshift

#define am_small	am_impl.ami_small
};

/*
 * The entries in an amap are called slots. For example an amap that
 * covers four pages is said to have four slots.
 *
 * The slots of an amap are clustered into chunks of UVM_AMAP_CHUNK
 * slots each. The data structure of a chunk is vm_amap_chunk.
 * Every chunk contains an array of pointers to vm_anon, and a bitmap
 * is used to represent which of the slots are in use.
 *
 * Small amaps of up to UVM_AMAP_CHUNK slots have the chunk directly
 * embedded in the amap structure.
 *
 * amaps with more slots are normal amaps and organize chunks in a hash
 * table. The hash table is organized as an array of buckets.
 * All chunks of the amap are additionally stored in a linked list.
 * Chunks that belong to the same hash bucket are stored in the list
 * consecutively. When all slots in a chunk are unused, the chunk is freed.
 *
 * For large amaps, the bucket array can grow large. See the description
 * below how large bucket arrays are avoided.
 */

/*
 * defines for handling of large sparce amaps:
 * 
 * one of the problems of array-based amaps is that if you allocate a
 * large sparcely-used area of virtual memory you end up allocating
 * large arrays that, for the most part, don't get used.  this is a
 * problem for BSD in that the kernel likes to make these types of
 * allocations to "reserve" memory for possible future use.
 *
 * for example, the kernel allocates (reserves) a large chunk of user
 * VM for possible stack growth.  most of the time only a page or two
 * of this VM is actually used.  since the stack is anonymous memory
 * it makes sense for it to live in an amap, but if we allocated an
 * amap for the entire stack range we could end up wasting a large
 * amount of malloc'd KVM.
 * 
 * for example, on the i386 at boot time we allocate two amaps for the stack 
 * of /sbin/init: 
 *  1. a 7680 slot amap at protection PROT_NONE (reserve space for stack)
 *  2. a 512 slot amap at protection PROT_READ|PROT_WRITE (top of stack)
 *
 * most of the array allocated for the amaps for this is never used.  
 * the amap interface provides a way for us to avoid this problem by
 * allowing amap_copy() to break larger amaps up into smaller sized 
 * chunks (controlled by the "canchunk" option).   we use this feature
 * to reduce our memory usage with the BSD stack management.  if we
 * are asked to create an amap with more than UVM_AMAP_LARGE slots in it,
 * we attempt to break it up into a UVM_AMAP_CHUNK sized amap if the
 * "canchunk" flag is set.
 *
 * so, in the i386 example, the 7680 slot area is never referenced so
 * nothing gets allocated (amap_copy is never called because the protection
 * is zero).   the 512 slot area for the top of the stack is referenced.
 * the chunking code breaks it up into 16 slot chunks (hopefully a single
 * 16 slot chunk is enough to handle the whole stack).
 */

#define UVM_AMAP_LARGE	256	/* # of slots in "large" amap */
#define UVM_AMAP_CHUNK	16	/* # of slots to chunk large amaps in */

#define UVM_AMAP_SMALL(amap)		((amap)->am_nslot <= UVM_AMAP_CHUNK)
#define UVM_AMAP_SLOTIDX(slot)		((slot) % UVM_AMAP_CHUNK)
#define UVM_AMAP_BUCKET(amap, slot)				\
	(((slot) / UVM_AMAP_CHUNK) >> (amap)->am_hashshift)

#ifdef _KERNEL

/*
 * macros
 */

/* AMAP_B2SLOT: convert byte offset to slot */
#define AMAP_B2SLOT(S,B) {						\
	KASSERT(((B) & (PAGE_SIZE - 1)) == 0);				\
	(S) = (B) >> PAGE_SHIFT;					\
}

#define AMAP_CHUNK_FOREACH(chunk, amap)					\
	for (chunk = (UVM_AMAP_SMALL(amap) ?				\
	    &(amap)->am_small : TAILQ_FIRST(&(amap)->am_chunks));	\
	    (chunk) != NULL; (chunk) = TAILQ_NEXT(chunk, ac_list))

#define AMAP_BASE_SLOT(slot)						\
	(((slot) / UVM_AMAP_CHUNK) * UVM_AMAP_CHUNK)

/*
 * flags macros
 */

#define amap_flags(AMAP)	((AMAP)->am_flags)
#define amap_refs(AMAP)		((AMAP)->am_ref)

/*
 * if we enable PPREF, then we have a couple of extra functions that
 * we need to prototype here...
 */

#ifdef UVM_AMAP_PPREF

#define PPREF_NONE ((int *) -1)	/* not using ppref */

					/* adjust references */
void		amap_pp_adjref(struct vm_amap *, int, vsize_t, int);
					/* establish ppref */
void		amap_pp_establish(struct vm_amap *);
					/* wipe part of an amap */
void		amap_wiperange(struct vm_amap *, int, int);
#endif	/* UVM_AMAP_PPREF */

#endif /* _KERNEL */

#endif /* _UVM_UVM_AMAP_H_ */
@


1.29
log
@Make amaps use less kernel memory (2nd try)

The original diff would crash at least i386 and powerpc, as spotted by
guenther@@ The reason was an incorrect use of sizeof in amap_lookups().

Confirmation that powerpc works by mpi@@ and mglocker@@

"throw it in" deraadt@@

Original commit message:

This is achieved by grouping amap slots into chunks that are allocated
on-demand by pool(9). Endless "fltamapcopy" loops because of kmem
shortage should be solved now. The kmem savings are also important to later
enable vmm(4) to use larged shared memory mappings for guest VM RAM.

This adapts libkvm also because the amap structure layout has changed.

Testing and fix of libkvm glitch in initial diff by tb@@
Feedback and "time to get this in" kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.h,v 1.28 2016/05/22 22:52:01 guenther Exp $	*/
d208 2
a209 2
 *  1. a 7680 slot amap at protection 0 (reserve space for stack)
 *  2. a 512 slot amap at protection 7 (top of stack)
@


1.28
log
@Revert previous: breaks i386 and powerpc, probably all non-PMAP_DIRECT archs
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.h,v 1.26 2016/05/08 16:29:57 stefan Exp $	*/
d123 2
a124 1
 * here is the definition of the vm_amap structure for this implementation.
d127 8
a139 3
	int *am_slots;		/* contig array of active slots */
	int *am_bckptr;		/* back pointer array to am_slots */
	struct vm_anon **am_anon; /* array of anonymous pages */
d144 22
d169 2
a170 15
 * note that am_slots, am_bckptr, and am_anon are arrays.   this allows
 * fast lookup of pages based on their virual address at the expense of
 * some extra memory.   in the future we should be smarter about memory
 * usage and fall back to a non-array based implementation on systems 
 * that are short of memory (XXXCDC).
 *
 * the entries in the array are called slots... for example an amap that
 * covers four pages of virtual memory is said to have four slots.   here
 * is an example of the array usage for a four slot amap.   note that only
 * slots one and three have anons assigned to them.  "D/C" means that we
 * "don't care" about the value.
 * 
 *            0     1      2     3
 * am_anon:   NULL, anon0, NULL, anon1		(actual pointers to anons)
 * am_bckptr: D/C,  1,     D/C,  0		(points to am_slots entry)
d172 13
a184 7
 * am_slots:  3, 1, D/C, D/C    		(says slots 3 and 1 are in use)
 * 
 * note that am_bckptr is D/C if the slot in am_anon is set to NULL.
 * to find the entry in am_slots for an anon, look at am_bckptr[slot],
 * thus the entry for slot 3 in am_slots[] is at am_slots[am_bckptr[3]].
 * in general, if am_anon[X] is non-NULL, then the following must be
 * true: am_slots[am_bckptr[X]] == X
d186 2
a187 1
 * note that am_slots is always contig-packed.
d230 5
d246 8
@


1.27
log
@Make amaps use less kernel memory

This is achieved by grouping amap slots into chunks that are allocated
on-demand by pool(9). Endless "fltamapcopy" loops because of kmem
shortage should be solved now. The kmem savings are also important to later
enable vmm(4) to use larged shared memory mappings for guest VM RAM.

This adapts libkvm also because the amap structure layout has changed.

Testing and fix of libkvm glitch in initial diff by tb@@
Feedback and "time to get this in" kettenis@@
@
text
@d123 1
a123 2
 * here is the definition of the vm_amap structure and helper structures for
 * this implementation.
a125 8
struct vm_amap_chunk {
	TAILQ_ENTRY(vm_amap_chunk) ac_list;
	int ac_baseslot;
	uint16_t ac_usedmap;
	uint16_t ac_nslot;
	struct vm_anon *ac_anon[];
};

d131 3
a137 22

	union {
		struct {
			struct vm_amap_chunk **amn_buckets;
			TAILQ_HEAD(, vm_amap_chunk) amn_chunks;
			int amn_ncused;	/* # of chunkers currently in use */
			int amn_hashshift; /* shift count to hash slot to bucket */
		} ami_normal;

		/*
		 * MUST be last element in vm_amap because it contains a
		 * variably sized array element.
		 */
		struct vm_amap_chunk ami_small;
	} am_impl;

#define am_buckets	am_impl.ami_normal.amn_buckets
#define am_chunks	am_impl.ami_normal.amn_chunks
#define am_ncused	am_impl.ami_normal.amn_ncused
#define am_hashshift	am_impl.ami_normal.amn_hashshift

#define am_small	am_impl.ami_small
d141 15
a155 2
 * The entries in an amap are called slots. For example an amap that
 * covers four pages is said to have four slots.
d157 7
a163 13
 * The slots of an amap are clustered into chunks of UVM_AMAP_CHUNK
 * slots each. The data structure of a chunk is vm_amap_chunk.
 * Every chunk contains an array of pointers to vm_anon, and a bitmap
 * is used to represent which of the slots are in use.
 *
 * Small amaps of up to UVM_AMAP_CHUNK slots have the chunk directly
 * embedded in the amap structure.
 *
 * amaps with more slots are normal amaps and organize chunks in a hash
 * table. The hash table is organized as an array of buckets.
 * All chunks of the amap are additionally stored in a linked list.
 * Chunks that belong to the same hash bucket are stored in the list
 * consecutively. When all slots in a chunk are unused, the chunk is freed.
d165 1
a165 2
 * For large amaps, the bucket array can grow large. See the description
 * below how large bucket arrays are avoided.
a207 5
#define UVM_AMAP_SMALL(amap)		((amap)->am_nslot <= UVM_AMAP_CHUNK)
#define UVM_AMAP_SLOTIDX(slot)		((slot) % UVM_AMAP_CHUNK)
#define UVM_AMAP_BUCKET(amap, slot)				\
	(((slot) / UVM_AMAP_CHUNK) >> (amap)->am_hashshift)

a218 8

#define AMAP_CHUNK_FOREACH(chunk, amap)					\
	for (chunk = (UVM_AMAP_SMALL(amap) ?				\
	    &(amap)->am_small : TAILQ_FIRST(&(amap)->am_chunks));	\
	    (chunk) != NULL; (chunk) = TAILQ_NEXT(chunk, ac_list))

#define AMAP_BASE_SLOT(slot)						\
	(((slot) / UVM_AMAP_CHUNK) * UVM_AMAP_CHUNK)
@


1.26
log
@Additional parameter for amap_alloc().

It is supposed to control whether an amap should allocate memory
to store anon pointers lazily or upfront. Needed for upcoming amap
changes.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.h,v 1.25 2016/05/08 11:52:32 stefan Exp $	*/
d123 2
a124 1
 * here is the definition of the vm_amap structure for this implementation.
d127 8
a139 3
	int *am_slots;		/* contig array of active slots */
	int *am_bckptr;		/* back pointer array to am_slots */
	struct vm_anon **am_anon; /* array of anonymous pages */
d144 22
d169 2
a170 15
 * note that am_slots, am_bckptr, and am_anon are arrays.   this allows
 * fast lookup of pages based on their virual address at the expense of
 * some extra memory.   in the future we should be smarter about memory
 * usage and fall back to a non-array based implementation on systems 
 * that are short of memory (XXXCDC).
 *
 * the entries in the array are called slots... for example an amap that
 * covers four pages of virtual memory is said to have four slots.   here
 * is an example of the array usage for a four slot amap.   note that only
 * slots one and three have anons assigned to them.  "D/C" means that we
 * "don't care" about the value.
 * 
 *            0     1      2     3
 * am_anon:   NULL, anon0, NULL, anon1		(actual pointers to anons)
 * am_bckptr: D/C,  1,     D/C,  0		(points to am_slots entry)
d172 13
a184 7
 * am_slots:  3, 1, D/C, D/C    		(says slots 3 and 1 are in use)
 * 
 * note that am_bckptr is D/C if the slot in am_anon is set to NULL.
 * to find the entry in am_slots for an anon, look at am_bckptr[slot],
 * thus the entry for slot 3 in am_slots[] is at am_slots[am_bckptr[3]].
 * in general, if am_anon[X] is non-NULL, then the following must be
 * true: am_slots[am_bckptr[X]] == X
d186 2
a187 1
 * note that am_slots is always contig-packed.
d230 5
d246 8
@


1.25
log
@Wait for RAM in uvm_fault when allocating uvm structures fails

Only fail hard when running out of swap space also, as suggested by
kettenis@@

While there, let amap_add() return a success status and handle
amap_add() errors in uvm_fault() similar to other out of RAM situations.
These bits are needed for further amap reorganization diffs.

lots of feedback and ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.h,v 1.24 2016/04/16 18:39:31 stefan Exp $	*/
d71 1
a71 1
struct vm_amap	*amap_alloc(vaddr_t, int);
@


1.24
log
@Remove am_maxslot from amap.

am_maxslot represents the total number of slots an amap can be extended
to. Since we do not extend amaps, this field as well as rounding the
number of slots to the next malloc bucket is not useful.

This also removes the corresponding output from procmap(1).

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.h,v 1.23 2016/04/04 16:34:16 stefan Exp $	*/
d65 2
d68 1
a68 1
void		amap_add(struct vm_aref *, vaddr_t, struct vm_anon *,
@


1.23
log
@UVM_FLAG_AMAPPAD has no effect anymore, nuke it.

This flag caused amaps to be allocated with additional spare slots, to
make extending them cheaper. However, the kernel never extends amaps,
so allocating spare slots is pointless. Also UVM_FLAG_AMAPPAD only
has an effect in combination with UVM_FLAG_OVERLAY. The only function
that used both flags was sys_obreak, but that function had the use of
UVM_FLAG_OVERLAY removed recently.

While there, kill the unused prototypes amap_flags and amap_refs.
They're defined as macros already.

ok mlarkin@@ kettenis@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.h,v 1.22 2016/03/27 09:51:37 stefan Exp $	*/
d127 1
a127 2
	int am_maxslot;		/* max # of slots allocated */
	int am_nslot;		/* # of slots currently in map ( <= maxslot) */
@


1.22
log
@amap_extend is never called, remove it.

In the code, this function is called when vm_map_entries are merged.
However, only kernel map entries are merged, and these do not use amaps.
Therefore amap_extend() is never called at runtime.

ok millert@@, KASSERT suggestion and ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.h,v 1.21 2016/03/06 14:47:07 stefan Exp $	*/
d69 1
a69 1
struct vm_amap	*amap_alloc(vaddr_t, vaddr_t, int);
a74 2
					/* get amap's flags */
int		amap_flags(struct vm_amap *);
a84 2
					/* get number of references of amap */
int		amap_refs(struct vm_amap *);
@


1.21
log
@Remove unused amap_share_protect().

ok mpi@@ visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.h,v 1.20 2014/07/11 16:35:40 jsg Exp $	*/
a74 2
					/* make amap larger */
int		amap_extend(vm_map_entry_t, vsize_t);
@


1.20
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.h,v 1.19 2013/05/30 16:29:46 tedu Exp $	*/
a90 2
					/* protect pages in a shared amap */
void		amap_share_protect(vm_map_entry_t, vm_prot_t);
@


1.19
log
@remove lots of comments about locking per beck's request
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.h,v 1.18 2009/03/25 20:00:18 oga Exp $	*/
a4 1
 *
a15 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed by Charles D. Cranor and
 *      Washington University.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
@


1.18
log
@Move all of the pseudo-inline functions in uvm into C files.

By pseudo-inline, I mean that if a certain macro was defined, they would
be inlined. However, no architecture defines that, and none has for a
very very long time. Therefore mainly this just makes the code a damned
sight easier to read. Some k&r -> ansi declarations while I'm in there.

"just commit it" art@@. ok weingart@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.h,v 1.17 2007/06/18 21:51:15 pedro Exp $	*/
d235 1
a235 1
 * lock/unlock/refs/flags macros
@


1.17
log
@Bring back Mickey's UVM anon change. Testing by thib@@, beck@@ and
ckuethe@@ for a while. Okay beck@@, "it is good timing" deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.h,v 1.16 2007/05/31 21:20:30 thib Exp $	*/
a68 12
 * handle inline options... we allow amap ops to be inline, but we also
 * provide a hook to turn this off.  macros can also be used.
 */

#ifdef UVM_AMAP_INLINE			/* defined/undef'd in uvm_amap.c */
#define AMAP_INLINE static __inline	/* inline enabled */
#else 
#define AMAP_INLINE			/* inline disabled */
#endif /* UVM_AMAP_INLINE */


/*
d72 3
a74 2
AMAP_INLINE				/* add an anon to an amap */
void		amap_add(struct vm_aref *, vaddr_t, struct vm_anon *, boolean_t);
d90 1
a90 1
AMAP_INLINE				/* lookup an anon @@ offset in amap */
d92 1
a92 1
AMAP_INLINE				/* lookup multiple anons */
d94 1
a94 1
AMAP_INLINE				/* add a reference to an amap */
d102 1
a102 1
AMAP_INLINE				/* remove an anon from an amap */
d104 1
a104 1
AMAP_INLINE				/* drop reference to an amap */
@


1.16
log
@zap the vm_amap am_l simplelock, and amap_{lock/unlock} macros for
simple_{lock/unlock}.

ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.h,v 1.15 2006/07/13 22:51:26 deraadt Exp $	*/
d119 1
d127 1
d162 1
@


1.15
log
@Back out the anon change.  Apparently it was tested by a few, but most of
us did not see it or get a chance to test it before it was commited. It
broke cvs, in the ami driver, making it not succeed at seeing it's devices.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.h,v 1.13 2004/12/30 08:28:39 niklas Exp $	*/
a100 2
					/* lock amap */
void		amap_lock(struct vm_amap *);
a114 2
					/* unlock amap */
void		amap_unlock(struct vm_amap *);
a148 1
	simple_lock_data_t am_l; /* simple lock [locks all vm_amap fields] */
a246 1
#define amap_lock(AMAP)		simple_lock(&(AMAP)->am_l)
a247 1
#define amap_unlock(AMAP)	simple_unlock(&(AMAP)->am_l)
@


1.14
log
@from netbsd: make anons dynamically allocated from pool.
this results in lesse kva waste due to static preallocation of those
for every phys page and also every swap page.
tested by beck krw miod
@
text
@a122 1
boolean_t	amap_swap_off(int, int);
a129 1
#define AMAP_SWAPOFF	0x4	/* amap_swap_off() is in progress */
a164 1
	LIST_ENTRY(vm_amap) am_list;
a252 1
#define amap_lock_try(AMAP)	simple_lock_try(&(AMAP)->am_l)
@


1.13
log
@Import M_CANFAIL support from NetBSD, removes a nasty panic during low-mem scenarios, instead generating an ENOMEM backfeed, ok tedu@@, prodded by many
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.h,v 1.12 2002/03/15 01:20:04 millert Exp $	*/
d123 1
d131 1
d167 1
d256 1
@


1.12
log
@Cosmetic changes only, primarily making comments line up nicely after the
__P removal.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.h,v 1.11 2002/03/14 01:27:18 millert Exp $	*/
d94 1
a94 1
void		amap_extend(vm_map_entry_t, vsize_t);
@


1.11
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.h,v 1.10 2001/12/19 08:58:07 art Exp $	*/
d84 39
a122 29
AMAP_INLINE
void		amap_add 	/* add an anon to an amap */(struct vm_aref *, vaddr_t,
			     struct vm_anon *, boolean_t);
struct vm_amap	*amap_alloc	/* allocate a new amap */(vaddr_t, vaddr_t, int);
void		amap_copy	/* clear amap needs-copy flag */(vm_map_t, vm_map_entry_t, int, 
			     boolean_t,	vaddr_t, vaddr_t);
void		amap_cow_now	/* resolve all COW faults now */(vm_map_t, vm_map_entry_t);
void		amap_extend	/* make amap larger */(vm_map_entry_t, vsize_t);
int		amap_flags	/* get amap's flags */(struct vm_amap *);
void		amap_free	/* free amap */(struct vm_amap *); 
void		amap_init	/* init amap module (at boot time) */(void);
void		amap_lock	/* lock amap */(struct vm_amap *);
AMAP_INLINE
struct vm_anon	*amap_lookup	/* lookup an anon @@ offset in amap */(struct vm_aref *, vaddr_t);
AMAP_INLINE
void		amap_lookups	/* lookup multiple anons */(struct vm_aref *, vaddr_t, 
			     struct vm_anon **, int);
AMAP_INLINE
void		amap_ref	/* add a reference to an amap */(struct vm_amap *, vaddr_t, vsize_t, int);
int		amap_refs	/* get number of references of amap */(struct vm_amap *);
void		amap_share_protect /* protect pages in a shared amap */(vm_map_entry_t, vm_prot_t);
void		amap_splitref	/* split reference to amap into two */(struct vm_aref *, struct vm_aref *, 
			     vaddr_t);
AMAP_INLINE
void		amap_unadd	/* remove an anon from an amap */(struct vm_aref *, vaddr_t);
void		amap_unlock	/* unlock amap */(struct vm_amap *);
AMAP_INLINE
void		amap_unref	/* drop reference to an amap */(struct vm_amap *, vaddr_t, vsize_t, int);
void		amap_wipeout	/* remove all anons from amap */(struct vm_amap *);
d265 6
a270 3
void		amap_pp_adjref		/* adjust references */(struct vm_amap *, int, vsize_t, int);
void		amap_pp_establish	/* establish ppref */(struct vm_amap *);
void		amap_wiperange		/* wipe part of an amap */(struct vm_amap *, int, int);
@


1.10
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.h,v 1.8 2001/11/11 01:16:56 art Exp $	*/
d85 11
a95 20
void		amap_add 	/* add an anon to an amap */
			__P((struct vm_aref *, vaddr_t,
			     struct vm_anon *, boolean_t));
struct vm_amap	*amap_alloc	/* allocate a new amap */
			__P((vaddr_t, vaddr_t, int));
void		amap_copy	/* clear amap needs-copy flag */
			__P((vm_map_t, vm_map_entry_t, int, 
			     boolean_t,	vaddr_t, vaddr_t));
void		amap_cow_now	/* resolve all COW faults now */
			__P((vm_map_t, vm_map_entry_t));
void		amap_extend	/* make amap larger */
			__P((vm_map_entry_t, vsize_t));
int		amap_flags	/* get amap's flags */
			__P((struct vm_amap *));
void		amap_free	/* free amap */
			__P((struct vm_amap *)); 
void		amap_init	/* init amap module (at boot time) */
			__P((void));
void		amap_lock	/* lock amap */
			__P((struct vm_amap *));
d97 1
a97 2
struct vm_anon	*amap_lookup	/* lookup an anon @@ offset in amap */
			__P((struct vm_aref *, vaddr_t));
d99 2
a100 3
void		amap_lookups	/* lookup multiple anons */
			__P((struct vm_aref *, vaddr_t, 
			     struct vm_anon **, int));
d102 5
a106 9
void		amap_ref	/* add a reference to an amap */
			__P((struct vm_amap *, vaddr_t, vsize_t, int));
int		amap_refs	/* get number of references of amap */
			__P((struct vm_amap *));
void		amap_share_protect /* protect pages in a shared amap */
			__P((vm_map_entry_t, vm_prot_t));
void		amap_splitref	/* split reference to amap into two */
			__P((struct vm_aref *, struct vm_aref *, 
			     vaddr_t));
d108 2
a109 4
void		amap_unadd	/* remove an anon from an amap */
			__P((struct vm_aref *, vaddr_t));
void		amap_unlock	/* unlock amap */
			__P((struct vm_amap *));
d111 2
a112 4
void		amap_unref	/* drop reference to an amap */
			 __P((struct vm_amap *, vaddr_t, vsize_t, int));
void		amap_wipeout	/* remove all anons from amap */
			__P((struct vm_amap *));
d255 3
a257 6
void		amap_pp_adjref		/* adjust references */
			 __P((struct vm_amap *, int, vsize_t, int));
void		amap_pp_establish	/* establish ppref */
			__P((struct vm_amap *));
void		amap_wiperange		/* wipe part of an amap */
			__P((struct vm_amap *, int, int));
@


1.9
log
@Sync in more uvm from NetBSD. Mostly just cosmetic stuff.
Contains also support for page coloring.
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_amap.h,v 1.17 2001/06/02 18:09:25 chs Exp $	*/
d63 1
a63 1
 * this structure.
d75 1
a75 1
#else
d81 1
a81 1
 * prototypes for the amap interface
d91 1
a91 1
			__P((struct vm_map *, struct vm_map_entry *, int,
d94 1
a94 1
			__P((struct vm_map *, struct vm_map_entry *));
d96 1
a96 1
			__P((struct vm_map_entry *, vsize_t));
d100 1
a100 1
			__P((struct vm_amap *));
d110 1
a110 1
			__P((struct vm_aref *, vaddr_t,
d118 1
a118 1
			__P((struct vm_map_entry *, vm_prot_t));
d120 1
a120 1
			__P((struct vm_aref *, struct vm_aref *,
d162 1
a162 1
	struct simplelock am_l; /* simple lock [locks all vm_amap fields] */
d180 1
a180 1
 * usage and fall back to a non-array based implementation on systems
d188 1
a188 1
 *
d194 1
a194 1
 *
d206 1
a206 1
 *
d219 3
a221 3
 *
 * for example, on the i386 at boot time we allocate two amaps for the stack
 * of /sbin/init:
d225 1
a225 1
 * most of the array allocated for the amaps for this is never used.
d227 1
a227 1
 * allowing amap_copy() to break larger amaps up into smaller sized
@


1.9.2.1
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_amap.h,v 1.9 2001/11/28 19:28:14 art Exp $	*/
d86 2
a87 2
			(struct vm_aref *, vaddr_t,
			     struct vm_anon *, boolean_t);
d89 1
a89 1
			(vaddr_t, vaddr_t, int);
d91 2
a92 2
			(struct vm_map *, struct vm_map_entry *, int,
			     boolean_t,	vaddr_t, vaddr_t);
d94 1
a94 1
			(struct vm_map *, struct vm_map_entry *);
d96 1
a96 1
			(struct vm_map_entry *, vsize_t);
d98 1
a98 1
			(struct vm_amap *);
d100 1
a100 1
			(struct vm_amap *);
d102 1
a102 1
			(void);
d104 1
a104 1
			(struct vm_amap *);
d107 1
a107 1
			(struct vm_aref *, vaddr_t);
d110 2
a111 2
			(struct vm_aref *, vaddr_t,
			     struct vm_anon **, int);
d114 1
a114 1
			(struct vm_amap *, vaddr_t, vsize_t, int);
d116 1
a116 1
			(struct vm_amap *);
d118 1
a118 1
			(struct vm_map_entry *, vm_prot_t);
d120 2
a121 2
			(struct vm_aref *, struct vm_aref *,
			     vaddr_t);
d124 1
a124 1
			(struct vm_aref *, vaddr_t);
d126 1
a126 1
			(struct vm_amap *);
d129 1
a129 1
			 (struct vm_amap *, vaddr_t, vsize_t, int);
d131 1
a131 1
			(struct vm_amap *);
d274 6
a279 6
					/* adjust references */
void		amap_pp_adjref(struct vm_amap *, int, vsize_t, int);
					/* establish ppref */
void		amap_pp_establish(struct vm_amap *);
					/* wipe part of an amap */
void		amap_wiperange(struct vm_amap *, int, int);
@


1.9.2.2
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_amap.h,v 1.9.2.1 2002/06/11 03:33:03 art Exp $	*/
/*	$NetBSD: uvm_amap.h,v 1.18 2002/09/15 16:54:29 chs Exp $	*/
d95 1
a95 1
int		amap_extend	/* make amap larger */
@


1.8
log
@Sync in more stuff from NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_amap.h,v 1.7 2001/11/07 02:55:50 art Exp $	*/
/*	$NetBSD: uvm_amap.h,v 1.14 2001/02/18 21:19:08 chs Exp $	*/
d63 1
a63 1
 * this structure.  
d75 1
a75 1
#else 
d81 1
a81 1
 * prototypes for the amap interface 
d91 1
a91 1
			__P((vm_map_t, vm_map_entry_t, int, 
d94 1
a94 1
			__P((vm_map_t, vm_map_entry_t));
d96 1
a96 1
			__P((vm_map_entry_t, vsize_t));
d100 1
a100 1
			__P((struct vm_amap *)); 
d110 1
a110 1
			__P((struct vm_aref *, vaddr_t, 
d118 1
a118 1
			__P((vm_map_entry_t, vm_prot_t));
d120 1
a120 1
			__P((struct vm_aref *, struct vm_aref *, 
d162 1
a162 1
	simple_lock_data_t am_l; /* simple lock [locks all vm_amap fields] */
d180 1
a180 1
 * usage and fall back to a non-array based implementation on systems 
d188 1
a188 1
 * 
d194 1
a194 1
 * 
d206 1
a206 1
 * 
d219 3
a221 3
 * 
 * for example, on the i386 at boot time we allocate two amaps for the stack 
 * of /sbin/init: 
d225 1
a225 1
 * most of the array allocated for the amaps for this is never used.  
d227 1
a227 1
 * allowing amap_copy() to break larger amaps up into smaller sized 
@


1.7
log
@Another sync of uvm to NetBSD. Just minor fiddling, no major changes.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_amap.h,v 1.6 2001/05/10 14:51:21 art Exp $	*/
/*	$NetBSD: uvm_amap.h,v 1.13 2000/11/25 06:27:59 chs Exp $	*/
d251 3
a253 5
#ifdef DIAGNOSTIC
#define AMAP_B2SLOT(S,B) { \
	if ((B) & (PAGE_SIZE - 1)) \
		panic("AMAP_B2SLOT: invalid byte count"); \
	(S) = (B) >> PAGE_SHIFT; \
a254 3
#else
#define AMAP_B2SLOT(S,B) (S) = (B) >> PAGE_SHIFT
#endif
@


1.6
log
@More sync to NetBSD.
The highlight is some more advices to madvise(2).
 o MADV_DONTNEED will deactive the pages in the given range giving a quicker
   reuse.
 o MADV_FREE will garbage-collect the pages and swap resources causing the
   next fault to either page in new pages from backing store (mapped vnode)
   or allocate new zero-fill pages (anonymous mapping).
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_amap.h,v 1.5 2001/03/09 05:34:38 smart Exp $	*/
/*	$NetBSD: uvm_amap.h,v 1.12 1999/07/07 05:31:40 thorpej Exp $	*/
d87 1
a87 1
			     struct vm_anon *, int));
d114 1
a114 1
			__P((vm_map_entry_t, int));
d129 1
a129 1
			 __P((vm_map_entry_t, int));
@


1.5
log
@Protect protypes, certain macros, and inlines from userland.  Checked userland
with a 'make build'.  From NetBSD.  art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_amap.h,v 1.4 2001/01/29 02:07:42 niklas Exp $	*/
/*	$NetBSD: uvm_amap.h,v 1.11 1999/06/21 17:25:11 thorpej Exp $	*/
d85 1
a85 1
vaddr_t		amap_add 	/* add an anon to an amap */
d124 1
a124 1
			__P((struct vm_amap *, vaddr_t));
@


1.4
log
@$OpenBSD$
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_amap.h,v 1.10 1999/01/28 14:46:27 chuck Exp $	*/
/*	$NetBSD: uvm_amap.h,v 1.10 1999/01/28 14:46:27 chuck Exp $	*/
d54 2
d140 1
d244 1
d286 2
@


1.3
log
@Fix the NetBSD id strings.
@
text
@d1 1
@


1.2
log
@add OpenBSD tags
@
text
@a0 1
/*	$OpenBSD$	*/
@


1.2.6.1
log
@Sync with -current
@
text
@d1 1
@


1.2.6.2
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_amap.h,v 1.5 2001/03/09 05:34:38 smart Exp $	*/
/*	$NetBSD: uvm_amap.h,v 1.11 1999/06/21 17:25:11 thorpej Exp $	*/
a52 2
#ifdef _KERNEL

a136 1
#endif /* _KERNEL */
a239 1
#ifdef _KERNEL
a280 2

#endif /* _KERNEL */
@


1.2.6.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_amap.h,v 1.12 1999/07/07 05:31:40 thorpej Exp $	*/
d85 1
a85 1
void		amap_add 	/* add an anon to an amap */
d124 1
a124 1
			__P((struct vm_aref *, vaddr_t));
@


1.2.6.4
log
@merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_amap.h,v 1.14 2001/02/18 21:19:08 chs Exp $	*/
d87 1
a87 1
			     struct vm_anon *, boolean_t));
d114 1
a114 1
			__P((struct vm_amap *, vaddr_t, vsize_t, int));
d129 1
a129 1
			 __P((struct vm_amap *, vaddr_t, vsize_t, int));
d251 5
a255 3
#define AMAP_B2SLOT(S,B) {						\
	KASSERT(((B) & (PAGE_SIZE - 1)) == 0);				\
	(S) = (B) >> PAGE_SHIFT;					\
d257 3
@


1.2.6.5
log
@Merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_amap.h,v 1.17 2001/06/02 18:09:25 chs Exp $	*/
d63 1
a63 1
 * this structure.
d75 1
a75 1
#else
d81 1
a81 1
 * prototypes for the amap interface
d91 1
a91 1
			__P((struct vm_map *, struct vm_map_entry *, int,
d94 1
a94 1
			__P((struct vm_map *, struct vm_map_entry *));
d96 1
a96 1
			__P((struct vm_map_entry *, vsize_t));
d100 1
a100 1
			__P((struct vm_amap *));
d110 1
a110 1
			__P((struct vm_aref *, vaddr_t,
d118 1
a118 1
			__P((struct vm_map_entry *, vm_prot_t));
d120 1
a120 1
			__P((struct vm_aref *, struct vm_aref *,
d162 1
a162 1
	struct simplelock am_l; /* simple lock [locks all vm_amap fields] */
d180 1
a180 1
 * usage and fall back to a non-array based implementation on systems
d188 1
a188 1
 *
d194 1
a194 1
 *
d206 1
a206 1
 *
d219 3
a221 3
 *
 * for example, on the i386 at boot time we allocate two amaps for the stack
 * of /sbin/init:
d225 1
a225 1
 * most of the array allocated for the amaps for this is never used.
d227 1
a227 1
 * allowing amap_copy() to break larger amaps up into smaller sized
@


1.2.6.6
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_amap.h,v 1.14 2001/02/18 21:19:08 chs Exp $	*/
d63 1
a63 1
 * this structure.  
d75 1
a75 1
#else 
d81 1
a81 1
 * prototypes for the amap interface 
d91 1
a91 1
			__P((vm_map_t, vm_map_entry_t, int, 
d94 1
a94 1
			__P((vm_map_t, vm_map_entry_t));
d96 1
a96 1
			__P((vm_map_entry_t, vsize_t));
d100 1
a100 1
			__P((struct vm_amap *)); 
d110 1
a110 1
			__P((struct vm_aref *, vaddr_t, 
d118 1
a118 1
			__P((vm_map_entry_t, vm_prot_t));
d120 1
a120 1
			__P((struct vm_aref *, struct vm_aref *, 
d162 1
a162 1
	simple_lock_data_t am_l; /* simple lock [locks all vm_amap fields] */
d180 1
a180 1
 * usage and fall back to a non-array based implementation on systems 
d188 1
a188 1
 * 
d194 1
a194 1
 * 
d206 1
a206 1
 * 
d219 3
a221 3
 * 
 * for example, on the i386 at boot time we allocate two amaps for the stack 
 * of /sbin/init: 
d225 1
a225 1
 * most of the array allocated for the amaps for this is never used.  
d227 1
a227 1
 * allowing amap_copy() to break larger amaps up into smaller sized 
@


1.2.6.7
log
@Merge in -current from roughly a week ago
@
text
@d84 48
a131 39
AMAP_INLINE				/* add an anon to an amap */
void		amap_add(struct vm_aref *, vaddr_t, struct vm_anon *, boolean_t);
					/* allocate a new amap */
struct vm_amap	*amap_alloc(vaddr_t, vaddr_t, int);
					/* clear amap needs-copy flag */
void		amap_copy(vm_map_t, vm_map_entry_t, int, boolean_t, vaddr_t,
		    vaddr_t);
					/* resolve all COW faults now */
void		amap_cow_now(vm_map_t, vm_map_entry_t);
					/* make amap larger */
void		amap_extend(vm_map_entry_t, vsize_t);
					/* get amap's flags */
int		amap_flags(struct vm_amap *);
					/* free amap */
void		amap_free(struct vm_amap *);
					/* init amap module (at boot time) */
void		amap_init(void);
					/* lock amap */
void		amap_lock(struct vm_amap *);
AMAP_INLINE				/* lookup an anon @@ offset in amap */
struct vm_anon	*amap_lookup(struct vm_aref *, vaddr_t);
AMAP_INLINE				/* lookup multiple anons */
void		amap_lookups(struct vm_aref *, vaddr_t, struct vm_anon **, int);
AMAP_INLINE				/* add a reference to an amap */
void		amap_ref(struct vm_amap *, vaddr_t, vsize_t, int);
					/* get number of references of amap */
int		amap_refs(struct vm_amap *);
					/* protect pages in a shared amap */
void		amap_share_protect(vm_map_entry_t, vm_prot_t);
					/* split reference to amap into two */
void		amap_splitref(struct vm_aref *, struct vm_aref *, vaddr_t);
AMAP_INLINE				/* remove an anon from an amap */
void		amap_unadd(struct vm_aref *, vaddr_t);
					/* unlock amap */
void		amap_unlock(struct vm_amap *);
AMAP_INLINE				/* drop reference to an amap */
void		amap_unref(struct vm_amap *, vaddr_t, vsize_t, int);
					/* remove all anons from amap */
void		amap_wipeout(struct vm_amap *);
d274 6
a279 6
					/* adjust references */
void		amap_pp_adjref(struct vm_amap *, int, vsize_t, int);
					/* establish ppref */
void		amap_pp_establish(struct vm_amap *);
					/* wipe part of an amap */
void		amap_wiperange(struct vm_amap *, int, int);
@


1.1
log
@Import of uvm from NetBSD. Some local changes, some code disabled
@
text
@d1 1
@

