head	1.48;
access;
symbols
	OPENBSD_6_2:1.48.0.2
	OPENBSD_6_2_BASE:1.48
	OPENBSD_6_1:1.48.0.4
	OPENBSD_6_1_BASE:1.48
	OPENBSD_6_0:1.47.0.2
	OPENBSD_6_0_BASE:1.47
	OPENBSD_5_9:1.44.0.2
	OPENBSD_5_9_BASE:1.44
	OPENBSD_5_8:1.43.0.6
	OPENBSD_5_8_BASE:1.43
	OPENBSD_5_7:1.43.0.2
	OPENBSD_5_7_BASE:1.43
	OPENBSD_5_6:1.39.0.4
	OPENBSD_5_6_BASE:1.39
	OPENBSD_5_5:1.37.0.6
	OPENBSD_5_5_BASE:1.37
	OPENBSD_5_4:1.37.0.2
	OPENBSD_5_4_BASE:1.37
	OPENBSD_5_3:1.35.0.8
	OPENBSD_5_3_BASE:1.35
	OPENBSD_5_2:1.35.0.6
	OPENBSD_5_2_BASE:1.35
	OPENBSD_5_1_BASE:1.35
	OPENBSD_5_1:1.35.0.4
	OPENBSD_5_0:1.35.0.2
	OPENBSD_5_0_BASE:1.35
	OPENBSD_4_9:1.34.0.8
	OPENBSD_4_9_BASE:1.34
	OPENBSD_4_8:1.34.0.6
	OPENBSD_4_8_BASE:1.34
	OPENBSD_4_7:1.34.0.2
	OPENBSD_4_7_BASE:1.34
	OPENBSD_4_6:1.34.0.4
	OPENBSD_4_6_BASE:1.34
	OPENBSD_4_5:1.29.0.4
	OPENBSD_4_5_BASE:1.29
	OPENBSD_4_4:1.29.0.2
	OPENBSD_4_4_BASE:1.29
	OPENBSD_4_3:1.28.0.4
	OPENBSD_4_3_BASE:1.28
	OPENBSD_4_2:1.28.0.2
	OPENBSD_4_2_BASE:1.28
	OPENBSD_4_1:1.25.0.4
	OPENBSD_4_1_BASE:1.25
	OPENBSD_4_0:1.25.0.2
	OPENBSD_4_0_BASE:1.25
	OPENBSD_3_9:1.21.0.18
	OPENBSD_3_9_BASE:1.21
	OPENBSD_3_8:1.21.0.16
	OPENBSD_3_8_BASE:1.21
	OPENBSD_3_7:1.21.0.14
	OPENBSD_3_7_BASE:1.21
	OPENBSD_3_6:1.21.0.12
	OPENBSD_3_6_BASE:1.21
	SMP_SYNC_A:1.21
	SMP_SYNC_B:1.21
	OPENBSD_3_5:1.21.0.10
	OPENBSD_3_5_BASE:1.21
	OPENBSD_3_4:1.21.0.8
	OPENBSD_3_4_BASE:1.21
	UBC_SYNC_A:1.21
	OPENBSD_3_3:1.21.0.6
	OPENBSD_3_3_BASE:1.21
	OPENBSD_3_2:1.21.0.4
	OPENBSD_3_2_BASE:1.21
	OPENBSD_3_1:1.21.0.2
	OPENBSD_3_1_BASE:1.21
	UBC_SYNC_B:1.21
	UBC:1.18.0.2
	UBC_BASE:1.18
	OPENBSD_3_0:1.10.0.2
	OPENBSD_3_0_BASE:1.10
	OPENBSD_2_9_BASE:1.6
	OPENBSD_2_9:1.6.0.2
	OPENBSD_2_8:1.5.0.2
	OPENBSD_2_8_BASE:1.5
	OPENBSD_2_7:1.4.0.2
	OPENBSD_2_7_BASE:1.4
	SMP:1.3.0.4
	SMP_BASE:1.3
	kame_19991208:1.3
	OPENBSD_2_6:1.3.0.2
	OPENBSD_2_6_BASE:1.3
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.48
date	2016.09.15.02.00.18;	author dlg;	state Exp;
branches;
next	1.47;
commitid	RlO92XR575sygHqm;

1.47
date	2016.06.17.10.48.25;	author dlg;	state Exp;
branches;
next	1.46;
commitid	3jj6c8J8OJvfGS9o;

1.46
date	2016.05.08.11.52.32;	author stefan;	state Exp;
branches;
next	1.45;
commitid	hUj20vPhiD6DQNDL;

1.45
date	2016.03.29.12.04.26;	author chl;	state Exp;
branches;
next	1.44;
commitid	yx3qunfiuB9a5ukd;

1.44
date	2015.08.21.16.04.35;	author visa;	state Exp;
branches;
next	1.43;
commitid	gglpDr80UKmkkP9A;

1.43
date	2014.12.23.04.56.47;	author tedu;	state Exp;
branches;
next	1.42;
commitid	Vcm2YxE1LuHhCHfd;

1.42
date	2014.12.17.19.42.15;	author tedu;	state Exp;
branches;
next	1.41;
commitid	G4ldVK4QwvfU3tRp;

1.41
date	2014.11.16.12.31.00;	author deraadt;	state Exp;
branches;
next	1.40;
commitid	yv0ECmCdICvq576h;

1.40
date	2014.09.14.14.17.27;	author jsg;	state Exp;
branches;
next	1.39;
commitid	uzzBR7hz9ncd4O6G;

1.39
date	2014.07.11.16.35.40;	author jsg;	state Exp;
branches;
next	1.38;
commitid	7NtJNW9udCOFtDNM;

1.38
date	2014.04.13.23.14.15;	author tedu;	state Exp;
branches;
next	1.37;

1.37
date	2013.05.30.16.29.46;	author tedu;	state Exp;
branches;
next	1.36;

1.36
date	2013.05.30.15.17.59;	author tedu;	state Exp;
branches;
next	1.35;

1.35
date	2011.07.03.18.34.14;	author oga;	state Exp;
branches;
next	1.34;

1.34
date	2009.06.16.23.54.57;	author oga;	state Exp;
branches;
next	1.33;

1.33
date	2009.06.16.00.11.29;	author oga;	state Exp;
branches;
next	1.32;

1.32
date	2009.06.06.17.46.44;	author art;	state Exp;
branches;
next	1.31;

1.31
date	2009.05.30.18.58.03;	author oga;	state Exp;
branches;
next	1.30;

1.30
date	2009.03.20.15.19.04;	author oga;	state Exp;
branches;
next	1.29;

1.29
date	2008.04.10.16.43.47;	author miod;	state Exp;
branches;
next	1.28;

1.28
date	2007.06.18.21.51.15;	author pedro;	state Exp;
branches;
next	1.27;

1.27
date	2007.04.13.18.57.49;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2007.04.04.17.44.45;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2006.07.31.11.51.29;	author mickey;	state Exp;
branches;
next	1.24;

1.24
date	2006.07.26.23.15.55;	author mickey;	state Exp;
branches;
next	1.23;

1.23
date	2006.07.13.22.51.26;	author deraadt;	state Exp;
branches;
next	1.22;

1.22
date	2006.06.21.16.20.05;	author mickey;	state Exp;
branches;
next	1.21;

1.21
date	2002.03.14.01.27.18;	author millert;	state Exp;
branches;
next	1.20;

1.20
date	2002.01.02.22.23.25;	author miod;	state Exp;
branches;
next	1.19;

1.19
date	2001.12.19.08.58.07;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2001.11.28.19.28.14;	author art;	state Exp;
branches
	1.18.2.1;
next	1.17;

1.17
date	2001.11.28.13.47.39;	author art;	state Exp;
branches;
next	1.16;

1.16
date	2001.11.27.05.27.12;	author art;	state Exp;
branches;
next	1.15;

1.15
date	2001.11.11.01.16.56;	author art;	state Exp;
branches;
next	1.14;

1.14
date	2001.11.07.02.55.50;	author art;	state Exp;
branches;
next	1.13;

1.13
date	2001.11.06.13.36.52;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2001.11.06.01.35.04;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2001.11.05.22.14.54;	author art;	state Exp;
branches;
next	1.10;

1.10
date	2001.09.19.20.50.59;	author mickey;	state Exp;
branches;
next	1.9;

1.9
date	2001.07.26.19.37.13;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2001.07.18.10.47.05;	author art;	state Exp;
branches;
next	1.7;

1.7
date	2001.06.23.19.24.33;	author smart;	state Exp;
branches;
next	1.6;

1.6
date	2001.01.29.02.07.42;	author niklas;	state Exp;
branches;
next	1.5;

1.5
date	2000.09.07.20.15.28;	author art;	state Exp;
branches;
next	1.4;

1.4
date	2000.03.15.15.50.19;	author art;	state Exp;
branches;
next	1.3;

1.3
date	99.08.23.08.13.22;	author art;	state Exp;
branches
	1.3.4.1;
next	1.2;

1.2
date	99.02.26.05.32.06;	author art;	state Exp;
branches;
next	1.1;

1.1
date	99.02.26.01.30.11;	author art;	state Exp;
branches;
next	;

1.3.4.1
date	2000.03.24.09.09.48;	author niklas;	state Exp;
branches;
next	1.3.4.2;

1.3.4.2
date	2001.05.14.22.47.44;	author niklas;	state Exp;
branches;
next	1.3.4.3;

1.3.4.3
date	2001.07.04.11.01.01;	author niklas;	state Exp;
branches;
next	1.3.4.4;

1.3.4.4
date	2001.10.31.03.32.14;	author nate;	state Exp;
branches;
next	1.3.4.5;

1.3.4.5
date	2001.11.13.23.02.31;	author niklas;	state Exp;
branches;
next	1.3.4.6;

1.3.4.6
date	2001.12.05.01.19.55;	author niklas;	state Exp;
branches;
next	1.3.4.7;

1.3.4.7
date	2002.03.06.02.17.14;	author niklas;	state Exp;
branches;
next	1.3.4.8;

1.3.4.8
date	2002.03.28.14.54.26;	author niklas;	state Exp;
branches;
next	;

1.18.2.1
date	2002.01.31.22.55.50;	author niklas;	state Exp;
branches;
next	1.18.2.2;

1.18.2.2
date	2002.02.02.03.28.26;	author art;	state Exp;
branches;
next	1.18.2.3;

1.18.2.3
date	2002.06.11.03.33.03;	author art;	state Exp;
branches;
next	1.18.2.4;

1.18.2.4
date	2002.11.04.18.02.32;	author art;	state Exp;
branches;
next	;


desc
@@


1.48
log
@all pools have their ipl set via pool_setipl, so fold it into pool_init.

the ioff argument to pool_init() is unused and has been for many
years, so this replaces it with an ipl argument. because the ipl
will be set on init we no longer need pool_setipl.

most of these changes have been done with coccinelle using the spatch
below. cocci sucks at formatting code though, so i fixed that by hand.

the manpage and subr_pool.c bits i did myself.

ok tedu@@ jmatthew@@

@@ipl@@
expression pp;
expression ipl;
expression s, a, o, f, m, p;
@@@@
-pool_init(pp, s, a, o, f, m, p);
-pool_setipl(pp, ipl);
+pool_init(pp, s, a, ipl, f, m, p);
@
text
@/*	$OpenBSD: uvm_anon.c,v 1.47 2016/06/17 10:48:25 dlg Exp $	*/
/*	$NetBSD: uvm_anon.c,v 1.10 2000/11/25 06:27:59 chs Exp $	*/

/*
 * Copyright (c) 1997 Charles D. Cranor and Washington University.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * uvm_anon.c: uvm anon ops
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/malloc.h>
#include <sys/pool.h>
#include <sys/kernel.h>
#include <sys/atomic.h>

#include <uvm/uvm.h>
#include <uvm/uvm_swap.h>

struct pool uvm_anon_pool;

/*
 * allocate anons
 */
void
uvm_anon_init(void)
{
	pool_init(&uvm_anon_pool, sizeof(struct vm_anon), 0, IPL_NONE,
	    PR_WAITOK, "anonpl", NULL);
	pool_sethiwat(&uvm_anon_pool, uvmexp.free / 16);
}

/*
 * allocate an anon
 */
struct vm_anon *
uvm_analloc(void)
{
	struct vm_anon *anon;

	anon = pool_get(&uvm_anon_pool, PR_NOWAIT);
	if (anon) {
		anon->an_ref = 1;
		anon->an_page = NULL;
		anon->an_swslot = 0;
	}
	return(anon);
}

/*
 * uvm_anfree: free a single anon structure
 *
 * => caller must remove anon from its amap before calling (if it was in
 *	an amap).
 * => we may lock the pageq's.
 */
void
uvm_anfree(struct vm_anon *anon)
{
	struct vm_page *pg;

	/* get page */
	pg = anon->an_page;

	/*
	 * if we have a resident page, we must dispose of it before freeing
	 * the anon.
	 */
	if (pg) {
		/*
		 * if page is busy then we just mark it as released (who ever
		 * has it busy must check for this when they wake up). if the
		 * page is not busy then we can free it now.
		 */
		if ((pg->pg_flags & PG_BUSY) != 0) {
			/* tell them to dump it when done */
			atomic_setbits_int(&pg->pg_flags, PG_RELEASED);
			return;
		} 
		pmap_page_protect(pg, PROT_NONE);
		uvm_lock_pageq();	/* lock out pagedaemon */
		uvm_pagefree(pg);	/* bye bye */
		uvm_unlock_pageq();	/* free the daemon */
	}
	if (pg == NULL && anon->an_swslot != 0) {
		/* this page is no longer only in swap. */
		KASSERT(uvmexp.swpgonly > 0);
		uvmexp.swpgonly--;
	}

	/* free any swap resources. */
	uvm_anon_dropswap(anon);

	/*
	 * now that we've stripped the data areas from the anon, free the anon
	 * itself!
	 */
	KASSERT(anon->an_page == NULL);
	KASSERT(anon->an_swslot == 0);

	pool_put(&uvm_anon_pool, anon);
}

/*
 * uvm_anwait: wait for memory to become available to allocate an anon.
 */
void
uvm_anwait(void)
{
	struct vm_anon *anon;

	/* XXX: Want something like pool_wait()? */
	anon = pool_get(&uvm_anon_pool, PR_WAITOK);
	pool_put(&uvm_anon_pool, anon);
}

/*
 * uvm_anon_dropswap:  release any swap resources from this anon.
 */
void
uvm_anon_dropswap(struct vm_anon *anon)
{

	if (anon->an_swslot == 0)
		return;

	uvm_swap_free(anon->an_swslot, 1);
	anon->an_swslot = 0;
}

/*
 * fetch an anon's page.
 *
 * => returns TRUE if pagein was aborted due to lack of memory.
 */

boolean_t
uvm_anon_pagein(struct vm_anon *anon)
{
	struct vm_page *pg;
	int rv;

	rv = uvmfault_anonget(NULL, NULL, anon);

	switch (rv) {
	case VM_PAGER_OK:
		break;
	case VM_PAGER_ERROR:
	case VM_PAGER_REFAULT:
		/*
		 * nothing more to do on errors.
		 * VM_PAGER_REFAULT can only mean that the anon was freed,
		 * so again there's nothing to do.
		 */
		return FALSE;
	default:
#ifdef DIAGNOSTIC
		panic("anon_pagein: uvmfault_anonget -> %d", rv);
#else
		return FALSE;
#endif
	}

	/*
	 * ok, we've got the page now.
	 * mark it as dirty, clear its swslot and un-busy it.
	 */
	pg = anon->an_page;
	uvm_swap_free(anon->an_swslot, 1);
	anon->an_swslot = 0;
	atomic_clearbits_int(&pg->pg_flags, PG_CLEAN);

	/* deactivate the page (to put it on a page queue) */
	pmap_clear_reference(pg);
	pmap_page_protect(pg, PROT_NONE);
	uvm_lock_pageq();
	uvm_pagedeactivate(pg);
	uvm_unlock_pageq();

	return FALSE;
}
@


1.47
log
@pool_setipl on all uvm pools.

ok kettenis@@ visa@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.46 2016/05/08 11:52:32 stefan Exp $	*/
d51 1
a51 1
	pool_init(&uvm_anon_pool, sizeof(struct vm_anon), 0, 0,
a52 1
	pool_setipl(&uvm_anon_pool, IPL_NONE);
@


1.46
log
@Wait for RAM in uvm_fault when allocating uvm structures fails

Only fail hard when running out of swap space also, as suggested by
kettenis@@

While there, let amap_add() return a success status and handle
amap_add() errors in uvm_fault() similar to other out of RAM situations.
These bits are needed for further amap reorganization diffs.

lots of feedback and ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.45 2016/03/29 12:04:26 chl Exp $	*/
d53 1
@


1.45
log
@Remove dead assignments and now unused variables.

Found by LLVM/Clang Static Analyzer.

ok mpi@@ stefan@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.44 2015/08/21 16:04:35 visa Exp $	*/
d124 13
@


1.44
log
@Remove the unused loan_count field and the related uvm logic. Most of
the page loaning code is already in the Attic.

ok kettenis@@, beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.43 2014/12/23 04:56:47 tedu Exp $	*/
a150 1
	struct uvm_object *uobj;
a178 1
	uobj = pg->uobject;
@


1.43
log
@convert pool_init nointr to waitok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.42 2014/12/17 19:42:15 tedu Exp $	*/
a88 8
	 * if there is a resident page and it is loaned, then anon may not
	 * own it.   call out to uvm_anon_lockpage() to ensure the real owner
 	 * of the page has been identified and locked.
	 */
	if (pg && pg->loan_count)
		pg = uvm_anon_lockloanpg(anon);

	/*
d94 3
a96 2
		 * if the page is owned by a uobject, then we must 
		 * kill the loan on the page rather than free it.
d98 9
a106 25
		if (pg->uobject) {
			uvm_lock_pageq();
			KASSERT(pg->loan_count > 0);
			pg->loan_count--;
			pg->uanon = NULL;
			uvm_unlock_pageq();
		} else {
			/*
			 * page has no uobject, so we must be the owner of it.
			 *
			 * if page is busy then we just mark it as released
			 * (who ever has it busy must check for this when they
			 * wake up).    if the page is not busy then we can
			 * free it now.
			 */
			if ((pg->pg_flags & PG_BUSY) != 0) {
				/* tell them to dump it when done */
				atomic_setbits_int(&pg->pg_flags, PG_RELEASED);
				return;
			} 
			pmap_page_protect(pg, PROT_NONE);
			uvm_lock_pageq();	/* lock out pagedaemon */
			uvm_pagefree(pg);	/* bye bye */
			uvm_unlock_pageq();	/* free the daemon */
		}
a138 44
}

/*
 * uvm_anon_lockloanpg: given a locked anon, lock its resident page
 *
 * => on return:
 *		 if there is a resident page:
 *			if it is ownerless, we take over as owner
 *		 we return the resident page (it can change during
 *		 this function)
 * => note that the only time an anon has an ownerless resident page
 *	is if the page was loaned from a uvm_object and the uvm_object
 *	disowned it
 * => this only needs to be called when you want to do an operation
 *	on an anon's resident page and that page has a non-zero loan
 *	count.
 */
struct vm_page *
uvm_anon_lockloanpg(struct vm_anon *anon)
{
	struct vm_page *pg;

	/*
	 * loop while we have a resident page that has a non-zero loan count.
	 * if we successfully get our lock, we will "break" the loop.
	 * note that the test for pg->loan_count is not protected -- this
	 * may produce false positive results.   note that a false positive
	 * result may cause us to do more work than we need to, but it will
	 * not produce an incorrect result.
	 */
	while (((pg = anon->an_page) != NULL) && pg->loan_count != 0) {
		/*
		 * if page is un-owned [i.e. the object dropped its ownership],
		 * then we can take over as owner!
		 */
		if (pg->uobject == NULL && (pg->pg_flags & PQ_ANON) == 0) {
			uvm_lock_pageq();
			atomic_setbits_int(&pg->pg_flags, PQ_ANON);
			pg->loan_count--;	/* ... and drop our loan */
			uvm_unlock_pageq();
		}
		break;
	}
	return(pg);
@


1.42
log
@remove lock.h from uvm_extern.h. another holdover from the simpletonlock
era. fix uvm including c files to include lock.h or atomic.h as necessary.
ok deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.41 2014/11/16 12:31:00 deraadt Exp $	*/
d51 2
a52 2
	pool_init(&uvm_anon_pool, sizeof(struct vm_anon), 0, 0, 0, "anonpl",
	    &pool_allocator_nointr);
@


1.41
log
@Replace a plethora of historical protection options with just
PROT_NONE, PROT_READ, PROT_WRITE, and PROT_EXEC from mman.h.
PROT_MASK is introduced as the one true way of extracting those bits.
Remove UVM_ADV_* wrapper, using the standard names.
ok doug guenther kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.40 2014/09/14 14:17:27 jsg Exp $	*/
d38 1
@


1.40
log
@remove uneeded proc.h includes
ok mpi@@ kspillner@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.39 2014/07/11 16:35:40 jsg Exp $	*/
d124 1
a124 1
			pmap_page_protect(pg, VM_PROT_NONE);
d253 1
a253 1
	pmap_page_protect(pg, VM_PROT_NONE);
@


1.39
log
@Chuck Cranor rescinded clauses in his license
on the 2nd of February 2011 in NetBSD.

http://marc.info/?l=netbsd-source-changes&m=129658899212732&w=2
http://marc.info/?l=netbsd-source-changes&m=129659095515558&w=2
http://marc.info/?l=netbsd-source-changes&m=129659157916514&w=2
http://marc.info/?l=netbsd-source-changes&m=129665962324372&w=2
http://marc.info/?l=netbsd-source-changes&m=129666033625342&w=2
http://marc.info/?l=netbsd-source-changes&m=129666052825545&w=2
http://marc.info/?l=netbsd-source-changes&m=129666922906480&w=2
http://marc.info/?l=netbsd-source-changes&m=129667725518082&w=2
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.38 2014/04/13 23:14:15 tedu Exp $	*/
a34 1
#include <sys/proc.h>
@


1.38
log
@compress code by turning four line comments into one line comments.
emphatic ok usual suspects, grudging ok miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.37 2013/05/30 16:29:46 tedu Exp $	*/
a4 1
 *
a15 6
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed by Charles D. Cranor and
 *      Washington University.
 * 4. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
@


1.37
log
@remove lots of comments about locking per beck's request
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.36 2013/05/30 15:17:59 tedu Exp $	*/
d92 1
a92 4
	/*
	 * get page
	 */

a99 1

a106 1

a107 1

a111 1

a118 1

a126 1

d144 1
a144 3
	/*
	 * free any swap resources.
	 */
a198 1

a199 2


a203 1

a209 5

		/*
		 * we did it!   break the loop
		 */

a232 1

a234 1

a239 1

a240 1

a252 1

d259 1
a259 4
	/*
	 * deactivate the page (to put it on a page queue)
	 */

@


1.36
log
@remove simple_locks from uvm code. ok beck deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.35 2011/07/03 18:34:14 oga Exp $	*/
a84 1
 * => anon must be unlocked and have a zero reference count.
d115 1
a115 1
		 * if the page is owned by a uobject (now locked), then we must 
a169 2
 * 
 * => anon must be locked or have a reference count of 0.
d185 1
a185 2
 * => anon is locked by caller
 * => on return: anon is locked
a186 1
 *			if it has a uobject, it is locked by us
a237 1
 * => anon must be locked, and is unlocked upon return.
a247 1
	/* locked: anon */
a248 4
	/*
	 * if rv == VM_PAGER_OK, anon is still locked, else anon
	 * is unlocked
	 */
a292 4

	/*
	 * unlock the anon and we're done.
	 */
@


1.35
log
@Rip out and burn support for UVM_HIST.

The vm hackers don't use it, don't maintain it and have to look at it all the
time. About time this 800 lines of code hit /dev/null.

``never liked it'' tedu@@. ariane@@ was very happy when i told her i wrote
this diff.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.34 2009/06/16 23:54:57 oga Exp $	*/
a72 1
		simple_lock_init(&anon->an_lock);
a75 1
		simple_lock(&anon->an_lock);
a125 1
			simple_unlock(&pg->uobject->vmobjlock);
a149 1
		simple_lock(&uvm.swap_data_lock);
a151 1
		simple_unlock(&uvm.swap_data_lock);
a205 1
	boolean_t locked = FALSE;
a217 38
		/*
		 * quickly check to see if the page has an object before
		 * bothering to lock the page queues.   this may also produce
		 * a false positive result, but that's ok because we do a real
		 * check after that.
		 *
		 * XXX: quick check -- worth it?   need volatile?
		 */

		if (pg->uobject) {

			uvm_lock_pageq();
			if (pg->uobject) {	/* the "real" check */
				locked =
				    simple_lock_try(&pg->uobject->vmobjlock);
			} else {
				/* object disowned before we got PQ lock */
				locked = TRUE;
			}
			uvm_unlock_pageq();

			/*
			 * if we didn't get a lock (try lock failed), then we
			 * toggle our anon lock and try again
			 */

			if (!locked) {
				simple_unlock(&anon->an_lock);

				/*
				 * someone locking the object has a chance to
				 * lock us right now
				 */

				simple_lock(&anon->an_lock);
				continue;
			}
		}
a308 4
	simple_unlock(&anon->an_lock);
	if (uobj) {
		simple_unlock(&uobj->vmobjlock);
	}
@


1.34
log
@date based reversion of uvm to the 4th May.

We still have no idea why this stops the crashes. but it does.

a machine forced to 64mb of ram cycled 10GB through swap with this diff
and is still running as I type this. Other tests by ariane@@ and thib@@
also seem to show that it's alright.

ok deraadt@@, thib@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.30 2009/03/20 15:19:04 oga Exp $	*/
a93 2
	UVMHIST_FUNC("uvm_anfree"); UVMHIST_CALLED(maphist);
	UVMHIST_LOG(maphist,"(anon=%p)", anon, 0,0,0);
a142 3
				UVMHIST_LOG(maphist,
				    "  anon %p, page %p: BUSY (released!)", 
				    anon, pg, 0, 0);
a148 2
			UVMHIST_LOG(maphist,"anon %p, page %p: freed now!",
			    anon, pg, 0, 0);
a171 1
	UVMHIST_LOG(maphist,"<- done!",0,0,0,0);
a181 1
	UVMHIST_FUNC("uvm_anon_dropswap"); UVMHIST_CALLED(maphist);
a185 2
	UVMHIST_LOG(maphist,"freeing swap for anon %p, paged to swslot 0x%lx",
		    anon, anon->an_swslot, 0, 0);
@


1.33
log
@Backout all changes to uvm after pmemrange (which will be backed out
separately).

a change at or just before the hackathon has either exposed or added a
very very nasty memory corruption bug that is giving us hell right now.
So in the interest of kernel stability these diffs are being backed out
until such a time as that corruption bug has been found and squashed,
then the ones that are proven good may slowly return.

a quick hitlist of the main commits this backs out:

mine:
uvm_objwire
the lock change in uvm_swap.c
using trees for uvm objects instead of the hash
removing the pgo_releasepg callback.

art@@'s:
putting pmap_page_protect(VM_PROT_NONE) in uvm_pagedeactivate() since
all callers called that just prior anyway.

ok beck@@, ariane@@.

prompted by deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.31 2009/05/30 18:58:03 oga Exp $	*/
a130 2
			/* not ours anymore */
			anon->an_page = NULL;
@


1.32
log
@Since all callers of uvm_pagedeactivate did pmap_page_protect(.., VM_PROT_NONE)
just move that into uvm_pagedeactivate.

oga@@ ok
@
text
@d357 1
@


1.31
log
@in the (currently unused) loaning case for uvm_anfree, zero out anon->an_page
so we won't hit the kassert a little bit below it.

"obviously better than what is there now" beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.30 2009/03/20 15:19:04 oga Exp $	*/
a356 1
	pmap_page_protect(pg, VM_PROT_NONE);
@


1.30
log
@While working on some stuff in uvm I've gotten REALLY sick of reading
K&R function declarations, so switch them all over to ansi-style, in
accordance with the prophesy.

"go for it" art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.29 2008/04/10 16:43:47 miod Exp $	*/
d131 2
@


1.29
log
@Correctly amount swap usage for anons, from NetBSD via PR 5772.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.28 2007/06/18 21:51:15 pedro Exp $	*/
d56 1
a56 1
uvm_anon_init()
d67 1
a67 1
uvm_analloc()
d91 1
a91 2
uvm_anfree(anon)
	struct vm_anon *anon;
d188 1
a188 2
uvm_anon_dropswap(anon)
	struct vm_anon *anon;
d219 1
a219 2
uvm_anon_lockloanpg(anon)
	struct vm_anon *anon;
d303 1
a303 2
uvm_anon_pagein(anon)
	struct vm_anon *anon;
@


1.28
log
@Bring back Mickey's UVM anon change. Testing by thib@@, beck@@ and
ckuethe@@ for a while. Okay beck@@, "it is good timing" deraadt@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.27 2007/04/13 18:57:49 art Exp $	*/
d159 7
a200 7

	if (anon->an_page == NULL) {
		/* this page is no longer only in swap. */
		simple_lock(&uvm.swap_data_lock);
		uvmexp.swpgonly--;
		simple_unlock(&uvm.swap_data_lock);
	} 
@


1.27
log
@While splitting flags and pqflags might have been a good idea in theory
to separate locking, on most modern machines this is not enough
since operations on short types touch other short types that share the
same word in memory.

Merge pg_flags and pqflags again and now use atomic operations to change
the flags. Also bump wire_count to an int and pg_version might go
int as well, just for alignment.

tested by many, many. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.26 2007/04/04 17:44:45 art Exp $	*/
d50 1
a50 16
/*
 * anonblock_list: global list of anon blocks,
 * locked by swap_syscall_lock (since we never remove
 * anything from this list and we only add to it via swapctl(2)).
 */

struct uvm_anonblock {
	LIST_ENTRY(uvm_anonblock) list;
	int count;
	struct vm_anon *anons;
};
static LIST_HEAD(anonlist, uvm_anonblock) anonblock_list;


static boolean_t anon_pagein(struct vm_anon *);

d58 3
a60 77
	int nanon = uvmexp.free - (uvmexp.free / 16); /* XXXCDC ??? */

	simple_lock_init(&uvm.afreelock);
	LIST_INIT(&anonblock_list);

	/*
	 * Allocate the initial anons.
	 */
	uvm_anon_add(nanon);
}

/*
 * add some more anons to the free pool.  called when we add
 * more swap space.
 *
 * => swap_syscall_lock should be held (protects anonblock_list).
 */
int
uvm_anon_add(count)
	int	count;
{
	struct uvm_anonblock *anonblock;
	struct vm_anon *anon;
	int lcv, needed;

	simple_lock(&uvm.afreelock);
	uvmexp.nanonneeded += count;
	needed = uvmexp.nanonneeded - uvmexp.nanon;
	simple_unlock(&uvm.afreelock);

	if (needed <= 0) {
		return 0;
	}
 
	anon = (void *)uvm_km_alloc(kernel_map, sizeof(*anon) * needed);

	/* XXX Should wait for VM to free up. */
	if (anon == NULL) {
		printf("uvm_anon_add: can not allocate %d anons\n", needed);
		panic("uvm_anon_add");
	}

	MALLOC(anonblock, void *, sizeof(*anonblock), M_UVMAMAP, M_WAITOK);

	anonblock->count = needed;
	anonblock->anons = anon;
	LIST_INSERT_HEAD(&anonblock_list, anonblock, list);
	memset(anon, 0, sizeof(*anon) * needed);
 
	simple_lock(&uvm.afreelock);
	uvmexp.nanon += needed;
	uvmexp.nfreeanon += needed;
	for (lcv = 0; lcv < needed; lcv++) {
		simple_lock_init(&anon->an_lock);
		anon[lcv].u.an_nxt = uvm.afree;
		uvm.afree = &anon[lcv];
		simple_lock_init(&uvm.afree->an_lock);
	}
	simple_unlock(&uvm.afreelock);
	return 0;
}

/*
 * remove anons from the free pool.
 */
void
uvm_anon_remove(count)
	int count;
{
	/*
	 * we never actually free any anons, to avoid allocation overhead.
	 * XXX someday we might want to try to free anons.
	 */

	simple_lock(&uvm.afreelock);
	uvmexp.nanonneeded -= count;
	simple_unlock(&uvm.afreelock);
d69 1
a69 1
	struct vm_anon *a;
d71 7
a77 8
	simple_lock(&uvm.afreelock);
	a = uvm.afree;
	if (a) {
		uvm.afree = a->u.an_nxt;
		uvmexp.nfreeanon--;
		a->an_ref = 1;
		a->an_swslot = 0;
		a->u.an_page = NULL;		/* so we can free quickly */
d79 1
a79 2
	simple_unlock(&uvm.afreelock);
	return(a);
d102 1
a102 1
	pg = anon->u.an_page;
d169 4
a172 5
	simple_lock(&uvm.afreelock);
	anon->u.an_nxt = uvm.afree;
	uvm.afree = anon;
	uvmexp.nfreeanon++;
	simple_unlock(&uvm.afreelock);
d195 1
a195 1
	if (anon->u.an_page == NULL) {
d236 1
a236 1
	while (((pg = anon->u.an_page) != NULL) && pg->loan_count != 0) {
a297 67


/*
 * page in every anon that is paged out to a range of swslots.
 * 
 * swap_syscall_lock should be held (protects anonblock_list).
 */

boolean_t
anon_swap_off(startslot, endslot)
	int startslot, endslot;
{
	struct uvm_anonblock *anonblock;

	for (anonblock = LIST_FIRST(&anonblock_list);
	     anonblock != NULL;
	     anonblock = LIST_NEXT(anonblock, list)) {
		int i;

		/*
		 * loop thru all the anons in the anonblock,
		 * paging in where needed.
		 */

		for (i = 0; i < anonblock->count; i++) {
			struct vm_anon *anon = &anonblock->anons[i];
			int slot;

			/*
			 * lock anon to work on it.
			 */

			simple_lock(&anon->an_lock);

			/*
			 * is this anon's swap slot in range?
			 */

			slot = anon->an_swslot;
			if (slot >= startslot && slot < endslot) {
				boolean_t rv;

				/*
				 * yup, page it in.
				 */

				/* locked: anon */
				rv = anon_pagein(anon);
				/* unlocked: anon */

				if (rv) {
					return rv;
				}
			} else {

				/*
				 * nope, unlock and proceed.
				 */

				simple_unlock(&anon->an_lock);
			}
		}
	}
	return FALSE;
}


d305 2
a306 2
static boolean_t
anon_pagein(anon)
d348 1
a348 1
	pg = anon->u.an_page;
@


1.26
log
@Mechanically rename the "flags" and "version" fields in struct vm_page
to "pg_flags" and "pg_version", so that they are a bit easier to work with.
Whoever uses generic names like this for a popular struct obviously doesn't
read much code.

Most architectures compile and there are no functionality changes.

deraadt@@ ok ("if something fails to compile, we fix that by hand")
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.25 2006/07/31 11:51:29 mickey Exp $	*/
d236 1
a236 1
				pg->pg_flags |= PG_RELEASED;
d374 1
a374 1
		if (pg->uobject == NULL && (pg->pqflags & PQ_ANON) == 0) {
d376 1
a376 1
			pg->pqflags |= PQ_ANON;		/* take ownership... */
d511 1
a511 1
	pg->pg_flags &= ~(PG_CLEAN);
a517 1
#ifndef UBC
a518 1
#endif
@


1.25
log
@fix uvmhist #2: args are always u_long so fix missing %d and %x and no %ll; no change for normal code
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.24 2006/07/26 23:15:55 mickey Exp $	*/
d234 1
a234 1
			if ((pg->flags & PG_BUSY) != 0) {
d236 1
a236 1
				pg->flags |= PG_RELEASED;
d511 1
a511 1
	pg->flags &= ~(PG_CLEAN);
@


1.24
log
@fix fmts for UVMHIST_LOG() entries making it more useful on 64bit archs; miod@@ ok
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.23 2006/07/13 22:51:26 deraadt Exp $	*/
d282 1
a282 1
	UVMHIST_LOG(maphist,"freeing swap for anon %p, paged to swslot 0x%x",
@


1.23
log
@Back out the anon change.  Apparently it was tested by a few, but most of
us did not see it or get a chance to test it before it was commited. It
broke cvs, in the ami driver, making it not succeed at seeing it's devices.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.21 2002/03/14 01:27:18 millert Exp $	*/
d187 1
a187 1
	UVMHIST_LOG(maphist,"(anon=0x%x)", anon, 0,0,0);
d238 1
a238 1
				    "  anon 0x%x, page 0x%x: BUSY (released!)", 
d246 1
a246 1
			UVMHIST_LOG(maphist,"anon 0x%x, page 0x%x: freed now!",
@


1.22
log
@from netbsd: make anons dynamically allocated from pool.
this results in lesse kva waste due to static preallocation of those
for every phys page and also every swap page.
tested by beck krw miod
@
text
@d50 16
a65 1
struct pool uvm_anon_pool;
d73 77
a149 3
	pool_init(&uvm_anon_pool, sizeof(struct vm_anon), 0, 0, 0, "anonpl",
	    &pool_allocator_nointr);
	pool_sethiwat(&uvm_anon_pool, uvmexp.free / 16);
d158 1
a158 1
	struct vm_anon *anon;
d160 8
a167 7
	anon = pool_get(&uvm_anon_pool, PR_NOWAIT);
	if (anon) {
		simple_lock_init(&anon->an_lock);
		anon->an_ref = 1;
		anon->an_page = NULL;
		anon->an_swslot = 0;
		simple_lock(&anon->an_lock);
d169 2
a170 1
	return(anon);
d193 1
a193 1
	pg = anon->an_page;
d260 5
a264 4
	KASSERT(anon->an_page == NULL);
	KASSERT(anon->an_swslot == 0);

	pool_put(&uvm_anon_pool, anon);
d287 1
a287 1
	if (anon->an_page == NULL) {
d328 1
a328 1
	while (((pg = anon->an_page) != NULL) && pg->loan_count != 0) {
d390 67
d464 2
a465 2
boolean_t
uvm_anon_pagein(anon)
d507 1
a507 1
	pg = anon->an_page;
@


1.21
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.20 2002/01/02 22:23:25 miod Exp $	*/
d50 1
a50 16
/*
 * anonblock_list: global list of anon blocks,
 * locked by swap_syscall_lock (since we never remove
 * anything from this list and we only add to it via swapctl(2)).
 */

struct uvm_anonblock {
	LIST_ENTRY(uvm_anonblock) list;
	int count;
	struct vm_anon *anons;
};
static LIST_HEAD(anonlist, uvm_anonblock) anonblock_list;


static boolean_t anon_pagein(struct vm_anon *);

d58 3
a60 77
	int nanon = uvmexp.free - (uvmexp.free / 16); /* XXXCDC ??? */

	simple_lock_init(&uvm.afreelock);
	LIST_INIT(&anonblock_list);

	/*
	 * Allocate the initial anons.
	 */
	uvm_anon_add(nanon);
}

/*
 * add some more anons to the free pool.  called when we add
 * more swap space.
 *
 * => swap_syscall_lock should be held (protects anonblock_list).
 */
int
uvm_anon_add(count)
	int	count;
{
	struct uvm_anonblock *anonblock;
	struct vm_anon *anon;
	int lcv, needed;

	simple_lock(&uvm.afreelock);
	uvmexp.nanonneeded += count;
	needed = uvmexp.nanonneeded - uvmexp.nanon;
	simple_unlock(&uvm.afreelock);

	if (needed <= 0) {
		return 0;
	}
 
	anon = (void *)uvm_km_alloc(kernel_map, sizeof(*anon) * needed);

	/* XXX Should wait for VM to free up. */
	if (anon == NULL) {
		printf("uvm_anon_add: can not allocate %d anons\n", needed);
		panic("uvm_anon_add");
	}

	MALLOC(anonblock, void *, sizeof(*anonblock), M_UVMAMAP, M_WAITOK);

	anonblock->count = needed;
	anonblock->anons = anon;
	LIST_INSERT_HEAD(&anonblock_list, anonblock, list);
	memset(anon, 0, sizeof(*anon) * needed);
 
	simple_lock(&uvm.afreelock);
	uvmexp.nanon += needed;
	uvmexp.nfreeanon += needed;
	for (lcv = 0; lcv < needed; lcv++) {
		simple_lock_init(&anon->an_lock);
		anon[lcv].u.an_nxt = uvm.afree;
		uvm.afree = &anon[lcv];
		simple_lock_init(&uvm.afree->an_lock);
	}
	simple_unlock(&uvm.afreelock);
	return 0;
}

/*
 * remove anons from the free pool.
 */
void
uvm_anon_remove(count)
	int count;
{
	/*
	 * we never actually free any anons, to avoid allocation overhead.
	 * XXX someday we might want to try to free anons.
	 */

	simple_lock(&uvm.afreelock);
	uvmexp.nanonneeded -= count;
	simple_unlock(&uvm.afreelock);
d69 1
a69 1
	struct vm_anon *a;
d71 7
a77 8
	simple_lock(&uvm.afreelock);
	a = uvm.afree;
	if (a) {
		uvm.afree = a->u.an_nxt;
		uvmexp.nfreeanon--;
		a->an_ref = 1;
		a->an_swslot = 0;
		a->u.an_page = NULL;		/* so we can free quickly */
d79 1
a79 2
	simple_unlock(&uvm.afreelock);
	return(a);
d102 1
a102 1
	pg = anon->u.an_page;
d169 4
a172 5
	simple_lock(&uvm.afreelock);
	anon->u.an_nxt = uvm.afree;
	uvm.afree = anon;
	uvmexp.nfreeanon++;
	simple_unlock(&uvm.afreelock);
d195 1
a195 1
	if (anon->u.an_page == NULL) {
d236 1
a236 1
	while (((pg = anon->u.an_page) != NULL) && pg->loan_count != 0) {
a297 67


/*
 * page in every anon that is paged out to a range of swslots.
 * 
 * swap_syscall_lock should be held (protects anonblock_list).
 */

boolean_t
anon_swap_off(startslot, endslot)
	int startslot, endslot;
{
	struct uvm_anonblock *anonblock;

	for (anonblock = LIST_FIRST(&anonblock_list);
	     anonblock != NULL;
	     anonblock = LIST_NEXT(anonblock, list)) {
		int i;

		/*
		 * loop thru all the anons in the anonblock,
		 * paging in where needed.
		 */

		for (i = 0; i < anonblock->count; i++) {
			struct vm_anon *anon = &anonblock->anons[i];
			int slot;

			/*
			 * lock anon to work on it.
			 */

			simple_lock(&anon->an_lock);

			/*
			 * is this anon's swap slot in range?
			 */

			slot = anon->an_swslot;
			if (slot >= startslot && slot < endslot) {
				boolean_t rv;

				/*
				 * yup, page it in.
				 */

				/* locked: anon */
				rv = anon_pagein(anon);
				/* unlocked: anon */

				if (rv) {
					return rv;
				}
			} else {

				/*
				 * nope, unlock and proceed.
				 */

				simple_unlock(&anon->an_lock);
			}
		}
	}
	return FALSE;
}


d305 2
a306 2
static boolean_t
anon_pagein(anon)
d348 1
a348 1
	pg = anon->u.an_page;
@


1.20
log
@Back out a few more uvm changes, especially wrt swap usage.
This unbreaks m68k m88k sparc and perhaps others, which eventually froze
when hitting swap.
Tested by various people on various platforms.
ok art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.14 2001/11/07 02:55:50 art Exp $	*/
d64 1
a64 1
static boolean_t anon_pagein __P((struct vm_anon *));
@


1.19
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_anon.c,v 1.15 2001/11/11 01:16:56 art Exp $	*/
/*	$NetBSD: uvm_anon.c,v 1.15 2001/02/18 21:19:08 chs Exp $	*/
d106 1
d108 2
d111 2
a112 4
		simple_lock(&uvm.afreelock);
		uvmexp.nanonneeded -= count;
		simple_unlock(&uvm.afreelock);
		return ENOMEM;
d114 1
a153 2
 *
 * => new anon is returned locked!
a167 2
		LOCK_ASSERT(simple_lock_held(&a->an_lock) == 0);
		simple_lock(&a->an_lock);
a188 3
	KASSERT(anon->an_ref == 0);
	LOCK_ASSERT(simple_lock_held(&anon->an_lock) == 0);

a318 2
	LOCK_ASSERT(simple_lock_held(&anon->an_lock));

a472 2
	LOCK_ASSERT(simple_lock_held(&anon->an_lock));

a473 1

d493 7
@


1.18
log
@Sync in more uvm from NetBSD. Mostly just cosmetic stuff.
Contains also support for page coloring.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_anon.c,v 1.17 2001/11/28 13:47:39 art Exp $	*/
/*	$NetBSD: uvm_anon.c,v 1.17 2001/05/25 04:06:12 chs Exp $	*/
d119 1
a119 1

d217 1
a217 1
		 * if the page is owned by a uobject (now locked), then we must
d243 1
a243 1
				    "  anon 0x%x, page 0x%x: BUSY (released!)",
d246 1
a246 1
			}
d275 1
a275 1
 *
d297 1
a297 1
	}
d401 1
a401 1
 *
d485 1
a485 1
	 * if rv == 0, anon is still locked, else anon
d490 1
a490 1
	case 0:
d493 2
a494 2
	case EIO:
	case ERESTART:
d498 1
a498 1
		 * ERESTART can only mean that the anon was freed,
d521 3
@


1.18.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.20 2002/01/02 22:23:25 miod Exp $	*/
a105 1
 
a106 2

	/* XXX Should wait for VM to free up. */
d108 4
a111 2
		printf("uvm_anon_add: can not allocate %d anons\n", needed);
		panic("uvm_anon_add");
a112 1

d152 2
d168 2
d191 3
d324 2
d480 2
d483 1
a502 7

	default:
#ifdef DIAGNOSTIC
		panic("anon_pagein: uvmfault_anonget -> %d", rv);
#else
		return FALSE;
#endif
@


1.18.2.2
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_anon.c,v 1.18.2.1 2002/01/31 22:55:50 niklas Exp $	*/
/*	$NetBSD: uvm_anon.c,v 1.21 2001/11/10 07:36:59 lukem Exp $	*/
d106 1
d108 2
d111 2
a112 4
		simple_lock(&uvm.afreelock);
		uvmexp.nanonneeded -= count;
		simple_unlock(&uvm.afreelock);
		return ENOMEM;
d114 1
a153 2
 *
 * => new anon is returned locked!
a167 2
		LOCK_ASSERT(simple_lock_held(&a->an_lock) == 0);
		simple_lock(&a->an_lock);
a180 1

a188 3
	KASSERT(anon->an_ref == 0);
	LOCK_ASSERT(!simple_lock_held(&anon->an_lock));

d227 5
a231 2
			 * if page is busy then we wait until it is not busy,
			 * and then free it.
d234 8
a241 2
			KASSERT((pg->flags & PG_RELEASED) == 0);
			simple_lock(&anon->an_lock);
d243 5
a247 15
			while ((pg = anon->u.an_page) &&
			       (pg->flags & PG_BUSY) != 0) {
				pg->flags |= PG_WANTED;
				UVM_UNLOCK_AND_WAIT(pg, &anon->an_lock, 0,
				    "anfree", 0);
				simple_lock(&anon->an_lock);
			}
			if (pg) {
				uvm_lock_pageq();
				uvm_pagefree(pg);
				uvm_unlock_pageq();
			}
			simple_unlock(&anon->an_lock);
			UVMHIST_LOG(maphist, "anon 0x%x, page 0x%x: "
				    "freed now!", anon, pg, 0, 0);
a249 7
	if (pg == NULL && anon->an_swslot != 0) {
		/* this page is no longer only in swap. */
		simple_lock(&uvm.swap_data_lock);
		KASSERT(uvmexp.swpgonly > 0);
		uvmexp.swpgonly--;
		simple_unlock(&uvm.swap_data_lock);
	}
a253 1

d257 2
a258 2
	 * now that we've stripped the data areas from the anon,
	 * free the anon itself.
a259 1

d286 7
a318 2
	LOCK_ASSERT(simple_lock_held(&anon->an_lock));

d335 2
d340 1
d342 1
a342 1
			if (pg->uobject) {
d376 2
a377 2
			pg->pqflags |= PQ_ANON;
			pg->loan_count--;
d380 5
d404 3
a406 1
	LIST_FOREACH(anonblock, &anonblock_list, list) {
a472 2
	LOCK_ASSERT(simple_lock_held(&anon->an_lock));

a473 1

d493 7
@


1.18.2.3
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.18.2.2 2002/02/02 03:28:26 art Exp $	*/
d64 1
a64 1
static boolean_t anon_pagein(struct vm_anon *);
@


1.18.2.4
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_anon.c,v 1.18.2.3 2002/06/11 03:33:03 art Exp $	*/
/*	$NetBSD: uvm_anon.c,v 1.22 2002/09/21 06:16:07 chs Exp $	*/
d207 1
a207 2
	if (pg && pg->loan_count) {
		simple_lock(&anon->an_lock);
a208 2
		simple_unlock(&anon->an_lock);
	}
@


1.17
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_anon.c,v 1.16 2001/11/27 05:27:12 art Exp $	*/
/*	$NetBSD: uvm_anon.c,v 1.16 2001/03/10 22:46:47 chs Exp $	*/
d119 1
a119 1
 
d217 1
a217 1
		 * if the page is owned by a uobject (now locked), then we must 
d243 1
a243 1
				    "  anon 0x%x, page 0x%x: BUSY (released!)", 
d246 1
a246 1
			} 
d275 1
a275 1
 * 
d297 1
a297 1
	} 
d401 1
a401 1
 * 
@


1.16
log
@Merge in the unified buffer cache code as found in NetBSD 2001/03/10. The
code is written mostly by Chuck Silvers <chuq@@chuq.com>/<chs@@netbsd.org>.

Tested for the past few weeks by many developers, should be in a pretty stable
state, but will require optimizations and additional cleanups.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_anon.c,v 1.15 2001/11/11 01:16:56 art Exp $	*/
/*	$NetBSD: uvm_anon.c,v 1.15 2001/02/18 21:19:08 chs Exp $	*/
d485 1
a485 1
	 * if rv == VM_PAGER_OK, anon is still locked, else anon
d490 1
a490 1
	case VM_PAGER_OK:
d493 2
a494 2
	case VM_PAGER_ERROR:
	case VM_PAGER_REFAULT:
d498 1
a498 1
		 * VM_PAGER_REFAULT can only mean that the anon was freed,
@


1.15
log
@Sync in more stuff from NetBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.14 2001/11/07 02:55:50 art Exp $	*/
a520 3
#ifndef UBC
	pmap_page_protect(pg, VM_PROT_NONE);
#endif
@


1.14
log
@Another sync of uvm to NetBSD. Just minor fiddling, no major changes.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_anon.c,v 1.13 2001/11/06 13:36:52 art Exp $	*/
/*	$NetBSD: uvm_anon.c,v 1.10 2000/11/25 06:27:59 chs Exp $	*/
d90 1
a90 1
void
d104 1
a104 1
		return;
a105 2
 
	MALLOC(anonblock, void *, sizeof(*anonblock), M_UVMAMAP, M_WAITOK);
d107 5
a111 5

	/* XXX Should wait for VM to free up. */
	if (anonblock == NULL || anon == NULL) {
		printf("uvm_anon_add: can not allocate %d anons\n", needed);
		panic("uvm_anon_add");
d113 1
d130 1
d152 2
d168 2
d191 3
d283 2
a284 1
	if (anon->an_swslot == 0) {
a285 1
	}
d324 2
d480 2
d483 1
a502 7

	default:
#ifdef DIAGNOSTIC
		panic("anon_pagein: uvmfault_anonget -> %d", rv);
#else
		return FALSE;
#endif
d521 1
d523 1
@


1.13
log
@More sync to NetBSD.
 - Use malloc/free instead of MALLOC/FREE for variable sized allocations.
 - Move the memory inheritance code to sys/mman.h and rename from VM_* to MAP_*
 - various cleanups and simplifications.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_anon.c,v 1.12 2001/11/06 01:35:04 art Exp $	*/
/*	$NetBSD: uvm_anon.c,v 1.9 2000/08/06 00:21:57 thorpej Exp $	*/
a214 2

			/* kill loan */
d216 1
a216 5
#ifdef DIAGNOSTIC
			if (pg->loan_count < 1)
				panic("uvm_anfree: obj owned page "
				      "with no loan count");
#endif
a220 1

a239 1

d244 1
a244 2

			UVMHIST_LOG(maphist,"  anon 0x%x, page 0x%x: freed now!", 
d356 1
d361 1
d363 1
a363 1
				continue;		/* start over */
d382 1
a384 5

	/*
	 * done!
	 */

a468 1
	UVMHIST_FUNC("anon_pagein"); UVMHIST_CALLED(pdhist);
@


1.12
log
@Move the last content from vm/ to uvm/
The only thing left in vm/ are just dumb wrappers.
vm/vm.h includes uvm/uvm_extern.h
vm/pmap.h includes uvm/uvm_pmap.h
vm/vm_page.h includes uvm/uvm_page.h
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_anon.c,v 1.11 2001/11/05 22:14:54 art Exp $	*/
/*	$NetBSD: uvm_anon.c,v 1.7 2000/06/27 17:29:18 mrg Exp $	*/
d481 1
a481 1
	
d484 4
a487 1
	/* unlocked: anon */
d504 1
a505 1
	default:
d507 2
@


1.11
log
@Minor sync to NetBSD.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_anon.c,v 1.10 2001/09/19 20:50:59 mickey Exp $	*/
/*	$NetBSD: uvm_anon.c,v 1.6 2000/06/26 14:21:16 mrg Exp $	*/
a45 2

#include <vm/vm.h>
@


1.10
log
@merge vm/vm_kern.h into uvm/uvm_extern.h; art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_anon.c,v 1.9 2001/07/26 19:37:13 art Exp $	*/
/*	$NetBSD: uvm_anon.c,v 1.5 2000/01/11 06:57:49 chs Exp $	*/
a47 1
#include <vm/vm_page.h>
@


1.9
log
@Add support for disabling swap devices (swapctl -d).
Improve error handling on I/O errors to swap.
From NetBSD
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_anon.c,v 1.8 2001/07/18 10:47:05 art Exp $	*/
a48 1
#include <vm/vm_kern.h>
@


1.8
log
@Get rid of the PMAP_NEW option by making it mandatory for all archs.
The archs that didn't have a proper PMAP_NEW now have a dummy implementation
with wrappers around the old functions.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_anon.c,v 1.7 2001/06/23 19:24:33 smart Exp $	*/
/*	$NetBSD: uvm_anon.c,v 1.4 1999/09/12 01:17:34 chs Exp $	*/
d45 1
d55 17
a76 1
	struct vm_anon *anon;
d78 3
a80 1
	int lcv;
d85 1
a85 16
	anon = (struct vm_anon *)uvm_km_alloc(kernel_map,
	    sizeof(*anon) * nanon);
	if (anon == NULL) {
		printf("uvm_anon_init: can not allocate %d anons\n", nanon);
		panic("uvm_anon_init");
	}

	memset(anon, 0, sizeof(*anon) * nanon);
	uvm.afree = NULL;
	uvmexp.nanon = uvmexp.nfreeanon = nanon;
	for (lcv = 0 ; lcv < nanon ; lcv++) {
		anon[lcv].u.an_nxt = uvm.afree;
		uvm.afree = &anon[lcv];
		simple_lock_init(&uvm.afree->an_lock);
	}
	simple_lock_init(&uvm.afreelock);
d91 2
d95 2
a96 2
uvm_anon_add(pages)
	int	pages;
d98 1
d100 1
a100 1
	int lcv;
d102 11
a112 2
	anon = (struct vm_anon *)uvm_km_alloc(kernel_map,
	    sizeof(*anon) * pages);
d115 2
a116 2
	if (anon == NULL) {
		printf("uvm_anon_add: can not allocate %d anons\n", pages);
d120 5
d126 3
a128 4
	memset(anon, 0, sizeof(*anon) * pages);
	uvmexp.nanon += pages;
	uvmexp.nfreeanon += pages;
	for (lcv = 0; lcv < pages; lcv++) {
d138 17
d401 140
@


1.7
log
@Sync with NetBSD 19990911 (just before PMAP_NEW was required)
  - thread_sleep_msg() -> uvm_sleep()
  - initialize reference count lock in uvm_anon_{init,add}()
  - add uao_flush()
  - replace boolean 'islocked' with 'lockflags'
  - in uvm_fault() change FALSE to TRUE to in 'wide' fault handling
  - get rid of uvm_km_get()
  - various bug fixes
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_anon.c,v 1.6 2001/01/29 02:07:42 niklas Exp $	*/
/*	$NetBSD: uvm_anon.c,v 1.3 1999/08/14 06:25:48 ross Exp $	*/
d215 1
a215 1
			pmap_page_protect(PMAP_PGARG(pg), VM_PROT_NONE);
@


1.6
log
@$OpenBSD$
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_anon.c,v 1.2 1999/03/26 17:34:15 chs Exp $	*/
/*	$NetBSD: uvm_anon.c,v 1.2 1999/03/26 17:34:15 chs Exp $	*/
d79 1
d112 1
@


1.5
log
@Convert bzero to memset(X, 0..) and bcopy to memcpy.
This is to match (make diffs smaller) the code in NetBSD.
new gcc inlines those functions, so this could also be a performance win.
@
text
@d1 1
@


1.4
log
@Fix the NetBSD id strings.
@
text
@d72 1
a72 1
	bzero(anon, sizeof(*anon) * nanon);
d103 1
a103 1
	bzero(anon, sizeof(*anon) * pages);
@


1.3
log
@sync with NetBSD from 1999.05.24 (there is a reason for this date)
 Mostly cleanups, but also a few improvements to pagedaemon for better
 handling of low memory and/or low swap conditions.
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_anon.c,v 1.2 1999/02/26 05:32:06 art Exp $	*/
/*	$NetBSD: uvm_anon.c,v 1.1 1999/01/24 23:53:15 chuck Exp $	*/
@


1.3.4.1
log
@Sync with -current
@
text
@d1 2
a2 1
/*	$NetBSD: uvm_anon.c,v 1.2 1999/03/26 17:34:15 chs Exp $	*/
@


1.3.4.2
log
@merge in approximately 2.9 into SMP branch
@
text
@a0 1
/*	$OpenBSD: uvm_anon.c,v 1.6 2001/01/29 02:07:42 niklas Exp $	*/
d72 1
a72 1
	memset(anon, 0, sizeof(*anon) * nanon);
d103 1
a103 1
	memset(anon, 0, sizeof(*anon) * pages);
@


1.3.4.3
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_anon.c,v 1.3 1999/08/14 06:25:48 ross Exp $	*/
a78 1
		simple_lock_init(&uvm.afree->an_lock);
a110 1
		simple_lock_init(&uvm.afree->an_lock);
@


1.3.4.4
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_anon.c,v 1.3.4.3 2001/07/04 11:01:01 niklas Exp $	*/
/*	$NetBSD: uvm_anon.c,v 1.5 2000/01/11 06:57:49 chs Exp $	*/
a44 1
#include <sys/kernel.h>
d48 1
a53 17
 * anonblock_list: global list of anon blocks,
 * locked by swap_syscall_lock (since we never remove
 * anything from this list and we only add to it via swapctl(2)).
 */

struct uvm_anonblock {
	LIST_ENTRY(uvm_anonblock) list;
	int count;
	struct vm_anon *anons;
};
static LIST_HEAD(anonlist, uvm_anonblock) anonblock_list;


static boolean_t anon_pagein __P((struct vm_anon *));


/*
d59 1
d61 1
a61 3

	simple_lock_init(&uvm.afreelock);
	LIST_INIT(&anonblock_list);
d66 16
a81 1
	uvm_anon_add(nanon);
a86 2
 *
 * => swap_syscall_lock should be held (protects anonblock_list).
d89 2
a90 2
uvm_anon_add(count)
	int	count;
a91 1
	struct uvm_anonblock *anonblock;
d93 1
a93 1
	int lcv, needed;
d95 2
a96 11
	simple_lock(&uvm.afreelock);
	uvmexp.nanonneeded += count;
	needed = uvmexp.nanonneeded - uvmexp.nanon;
	simple_unlock(&uvm.afreelock);

	if (needed <= 0) {
		return;
	}
 
	MALLOC(anonblock, void *, sizeof(*anonblock), M_UVMAMAP, M_WAITOK);
	anon = (void *)uvm_km_alloc(kernel_map, sizeof(*anon) * needed);
d99 2
a100 2
	if (anonblock == NULL || anon == NULL) {
		printf("uvm_anon_add: can not allocate %d anons\n", needed);
a103 5
	anonblock->count = needed;
	anonblock->anons = anon;
	LIST_INSERT_HEAD(&anonblock_list, anonblock, list);
	memset(anon, 0, sizeof(*anon) * needed);
 
d105 4
a108 3
	uvmexp.nanon += needed;
	uvmexp.nfreeanon += needed;
	for (lcv = 0; lcv < needed; lcv++) {
a117 17
 * remove anons from the free pool.
 */
void
uvm_anon_remove(count)
	int count;
{
	/*
	 * we never actually free any anons, to avoid allocation overhead.
	 * XXX someday we might want to try to free anons.
	 */

	simple_lock(&uvm.afreelock);
	uvmexp.nanonneeded -= count;
	simple_unlock(&uvm.afreelock);
}

/*
d215 1
a215 1
			pmap_page_protect(pg, VM_PROT_NONE);
a363 140
}



/*
 * page in every anon that is paged out to a range of swslots.
 * 
 * swap_syscall_lock should be held (protects anonblock_list).
 */

boolean_t
anon_swap_off(startslot, endslot)
	int startslot, endslot;
{
	struct uvm_anonblock *anonblock;

	for (anonblock = LIST_FIRST(&anonblock_list);
	     anonblock != NULL;
	     anonblock = LIST_NEXT(anonblock, list)) {
		int i;

		/*
		 * loop thru all the anons in the anonblock,
		 * paging in where needed.
		 */

		for (i = 0; i < anonblock->count; i++) {
			struct vm_anon *anon = &anonblock->anons[i];
			int slot;

			/*
			 * lock anon to work on it.
			 */

			simple_lock(&anon->an_lock);

			/*
			 * is this anon's swap slot in range?
			 */

			slot = anon->an_swslot;
			if (slot >= startslot && slot < endslot) {
				boolean_t rv;

				/*
				 * yup, page it in.
				 */

				/* locked: anon */
				rv = anon_pagein(anon);
				/* unlocked: anon */

				if (rv) {
					return rv;
				}
			} else {

				/*
				 * nope, unlock and proceed.
				 */

				simple_unlock(&anon->an_lock);
			}
		}
	}
	return FALSE;
}


/*
 * fetch an anon's page.
 *
 * => anon must be locked, and is unlocked upon return.
 * => returns TRUE if pagein was aborted due to lack of memory.
 */

static boolean_t
anon_pagein(anon)
	struct vm_anon *anon;
{
	struct vm_page *pg;
	struct uvm_object *uobj;
	int rv;
	UVMHIST_FUNC("anon_pagein"); UVMHIST_CALLED(pdhist);
	
	/* locked: anon */
	rv = uvmfault_anonget(NULL, NULL, anon);
	/* unlocked: anon */

	switch (rv) {
	case VM_PAGER_OK:
		break;

	case VM_PAGER_ERROR:
	case VM_PAGER_REFAULT:

		/*
		 * nothing more to do on errors.
		 * VM_PAGER_REFAULT can only mean that the anon was freed,
		 * so again there's nothing to do.
		 */

		return FALSE;

#ifdef DIAGNOSTIC
	default:
		panic("anon_pagein: uvmfault_anonget -> %d", rv);
#endif
	}

	/*
	 * ok, we've got the page now.
	 * mark it as dirty, clear its swslot and un-busy it.
	 */

	pg = anon->u.an_page;
	uobj = pg->uobject;
	uvm_swap_free(anon->an_swslot, 1);
	anon->an_swslot = 0;
	pg->flags &= ~(PG_CLEAN);

	/*
	 * deactivate the page (to put it on a page queue)
	 */

	pmap_clear_reference(pg);
	pmap_page_protect(pg, VM_PROT_NONE);
	uvm_lock_pageq();
	uvm_pagedeactivate(pg);
	uvm_unlock_pageq();

	/*
	 * unlock the anon and we're done.
	 */

	simple_unlock(&anon->an_lock);
	if (uobj) {
		simple_unlock(&uobj->vmobjlock);
	}
	return FALSE;
@


1.3.4.5
log
@merge in -current
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_anon.c,v 1.15 2001/02/18 21:19:08 chs Exp $	*/
d47 3
d93 1
a93 1
int
d107 1
a107 1
		return 0;
d109 2
d112 5
a116 5
	if (anon == NULL) {
		simple_lock(&uvm.afreelock);
		uvmexp.nanonneeded -= count;
		simple_unlock(&uvm.afreelock);
		return ENOMEM;
a117 1
	MALLOC(anonblock, void *, sizeof(*anonblock), M_UVMAMAP, M_WAITOK);
a133 1
	return 0;
a154 2
 *
 * => new anon is returned locked!
a168 2
		LOCK_ASSERT(simple_lock_held(&a->an_lock) == 0);
		simple_lock(&a->an_lock);
a189 3
	KASSERT(anon->an_ref == 0);
	LOCK_ASSERT(simple_lock_held(&anon->an_lock) == 0);

d218 2
d221 5
a225 1
			KASSERT(pg->loan_count > 0);
d230 1
d250 1
d255 2
a256 1
			UVMHIST_LOG(maphist,"anon 0x%x, page 0x%x: freed now!",
d288 1
a288 2

	if (anon->an_swslot == 0)
d290 1
a328 2
	LOCK_ASSERT(simple_lock_held(&anon->an_lock));

a367 1

a371 1

d373 1
a373 1
				continue;
a391 1

d394 5
d483 2
a484 1

a485 2
	LOCK_ASSERT(simple_lock_held(&anon->an_lock));

d487 1
a487 5

	/*
	 * if rv == VM_PAGER_OK, anon is still locked, else anon
	 * is unlocked
	 */
d503 5
a525 1
#ifndef UBC
a526 1
#endif
@


1.3.4.6
log
@Merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_anon.c,v 1.17 2001/05/25 04:06:12 chs Exp $	*/
d119 1
a119 1

d217 1
a217 1
		 * if the page is owned by a uobject (now locked), then we must
d243 1
a243 1
				    "  anon 0x%x, page 0x%x: BUSY (released!)",
d246 1
a246 1
			}
d275 1
a275 1
 *
d297 1
a297 1
	}
d401 1
a401 1
 *
d485 1
a485 1
	 * if rv == 0, anon is still locked, else anon
d490 1
a490 1
	case 0:
d493 2
a494 2
	case EIO:
	case ERESTART:
d498 1
a498 1
		 * ERESTART can only mean that the anon was freed,
d521 3
@


1.3.4.7
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_anon.c,v 1.10 2000/11/25 06:27:59 chs Exp $	*/
a105 1
 
a106 2

	/* XXX Should wait for VM to free up. */
d108 4
a111 2
		printf("uvm_anon_add: can not allocate %d anons\n", needed);
		panic("uvm_anon_add");
a112 1

d119 1
a119 1
 
d152 2
d168 2
d191 3
d217 1
a217 1
		 * if the page is owned by a uobject (now locked), then we must 
d243 1
a243 1
				    "  anon 0x%x, page 0x%x: BUSY (released!)", 
d246 1
a246 1
			} 
d275 1
a275 1
 * 
d297 1
a297 1
	} 
d324 2
d401 1
a401 1
 * 
d480 2
d483 1
d485 1
a485 1
	 * if rv == VM_PAGER_OK, anon is still locked, else anon
d490 1
a490 1
	case VM_PAGER_OK:
d493 2
a494 2
	case VM_PAGER_ERROR:
	case VM_PAGER_REFAULT:
d498 1
a498 1
		 * VM_PAGER_REFAULT can only mean that the anon was freed,
a502 7

	default:
#ifdef DIAGNOSTIC
		panic("anon_pagein: uvmfault_anonget -> %d", rv);
#else
		return FALSE;
#endif
a520 3
#ifndef UBC
	pmap_page_protect(pg, VM_PROT_NONE);
#endif
@


1.3.4.8
log
@Merge in -current from roughly a week ago
@
text
@d64 1
a64 1
static boolean_t anon_pagein(struct vm_anon *);
@


1.2
log
@add OpenBSD tags
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a206 1
				simple_unlock(&anon->an_lock);
d224 1
a224 1
	 * are we using any backing store resources?   if so, free them.
d226 1
a226 11
	if (anon->an_swslot) {
		/*
		 * on backing store: no I/O in progress.  sole amap reference
		 * is ours and we've got it locked down.   thus we can free,
		 * and be done.
		 */
		UVMHIST_LOG(maphist,"  freeing anon 0x%x, paged to swslot 0x%x",
		    anon, anon->an_swslot, 0, 0);
		uvm_swap_free(anon->an_swslot, 1);
		anon->an_swslot = 0;
	} 
d238 27
@


1.1
log
@Import of uvm from NetBSD. Some local changes, some code disabled
@
text
@d1 1
@

