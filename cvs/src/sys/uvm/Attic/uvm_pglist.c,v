head	1.39;
access;
symbols
	OPENBSD_4_9:1.38.0.4
	OPENBSD_4_9_BASE:1.38
	OPENBSD_4_8:1.38.0.2
	OPENBSD_4_8_BASE:1.38
	OPENBSD_4_7:1.35.0.2
	OPENBSD_4_7_BASE:1.35
	OPENBSD_4_6:1.32.0.4
	OPENBSD_4_6_BASE:1.32
	OPENBSD_4_5:1.24.0.2
	OPENBSD_4_5_BASE:1.24
	OPENBSD_4_4:1.23.0.2
	OPENBSD_4_4_BASE:1.23
	OPENBSD_4_3:1.21.0.2
	OPENBSD_4_3_BASE:1.21
	OPENBSD_4_2:1.20.0.2
	OPENBSD_4_2_BASE:1.20
	OPENBSD_4_1:1.18.0.2
	OPENBSD_4_1_BASE:1.18
	OPENBSD_4_0:1.16.0.2
	OPENBSD_4_0_BASE:1.16
	OPENBSD_3_9:1.15.0.2
	OPENBSD_3_9_BASE:1.15
	OPENBSD_3_8:1.14.0.12
	OPENBSD_3_8_BASE:1.14
	OPENBSD_3_7:1.14.0.10
	OPENBSD_3_7_BASE:1.14
	OPENBSD_3_6:1.14.0.8
	OPENBSD_3_6_BASE:1.14
	SMP_SYNC_A:1.14
	SMP_SYNC_B:1.14
	OPENBSD_3_5:1.14.0.6
	OPENBSD_3_5_BASE:1.14
	OPENBSD_3_4:1.14.0.4
	OPENBSD_3_4_BASE:1.14
	UBC_SYNC_A:1.14
	OPENBSD_3_3:1.14.0.2
	OPENBSD_3_3_BASE:1.14
	OPENBSD_3_2:1.12.0.4
	OPENBSD_3_2_BASE:1.12
	OPENBSD_3_1:1.12.0.2
	OPENBSD_3_1_BASE:1.12
	UBC_SYNC_B:1.14
	UBC:1.11.0.2
	UBC_BASE:1.11
	OPENBSD_3_0:1.7.0.2
	OPENBSD_3_0_BASE:1.7
	OPENBSD_2_9_BASE:1.6
	OPENBSD_2_9:1.6.0.2
	OPENBSD_2_8:1.4.0.4
	OPENBSD_2_8_BASE:1.4
	OPENBSD_2_7:1.4.0.2
	OPENBSD_2_7_BASE:1.4
	SMP:1.3.0.4
	SMP_BASE:1.3
	kame_19991208:1.3
	OPENBSD_2_6:1.3.0.2
	OPENBSD_2_6_BASE:1.3
	OPENBSD_2_5:1.2.0.2
	OPENBSD_2_5_BASE:1.2;
locks; strict;
comment	@ * @;


1.39
date	2011.06.23.21.50.26;	author oga;	state dead;
branches;
next	1.38;

1.38
date	2010.06.27.03.03.49;	author thib;	state Exp;
branches;
next	1.37;

1.37
date	2010.06.10.08.48.36;	author thib;	state Exp;
branches;
next	1.36;

1.36
date	2010.04.22.19.02.55;	author oga;	state Exp;
branches;
next	1.35;

1.35
date	2009.08.13.15.29.59;	author deraadt;	state Exp;
branches;
next	1.34;

1.34
date	2009.07.26.21.26.10;	author deraadt;	state Exp;
branches;
next	1.33;

1.33
date	2009.07.23.21.39.10;	author kettenis;	state Exp;
branches;
next	1.32;

1.32
date	2009.06.17.00.13.59;	author oga;	state Exp;
branches;
next	1.31;

1.31
date	2009.06.16.16.42.41;	author ariane;	state Exp;
branches;
next	1.30;

1.30
date	2009.06.01.17.42.33;	author ariane;	state Exp;
branches;
next	1.29;

1.29
date	2009.05.04.18.08.06;	author oga;	state Exp;
branches;
next	1.28;

1.28
date	2009.04.30.09.06.01;	author oga;	state Exp;
branches;
next	1.27;

1.27
date	2009.04.20.00.30.18;	author oga;	state Exp;
branches;
next	1.26;

1.26
date	2009.04.14.16.01.04;	author oga;	state Exp;
branches;
next	1.25;

1.25
date	2009.03.20.15.19.04;	author oga;	state Exp;
branches;
next	1.24;

1.24
date	2008.10.01.20.00.32;	author miod;	state Exp;
branches;
next	1.23;

1.23
date	2008.06.27.17.25.47;	author miod;	state Exp;
branches
	1.23.2.1;
next	1.22;

1.22
date	2008.06.26.05.42.20;	author ray;	state Exp;
branches;
next	1.21;

1.21
date	2007.12.18.11.05.52;	author thib;	state Exp;
branches;
next	1.20;

1.20
date	2007.04.13.18.57.49;	author art;	state Exp;
branches;
next	1.19;

1.19
date	2007.04.04.17.44.45;	author art;	state Exp;
branches;
next	1.18;

1.18
date	2007.02.12.11.43.47;	author tom;	state Exp;
branches;
next	1.17;

1.17
date	2007.01.16.13.36.38;	author dim;	state Exp;
branches;
next	1.16;

1.16
date	2006.06.01.05.16.49;	author krw;	state Exp;
branches;
next	1.15;

1.15
date	2006.01.16.13.11.06;	author mickey;	state Exp;
branches;
next	1.14;

1.14
date	2002.10.07.18.35.52;	author mickey;	state Exp;
branches;
next	1.13;

1.13
date	2002.10.06.22.04.41;	author art;	state Exp;
branches;
next	1.12;

1.12
date	2001.12.19.08.58.07;	author art;	state Exp;
branches;
next	1.11;

1.11
date	2001.11.28.19.28.15;	author art;	state Exp;
branches
	1.11.2.1;
next	1.10;

1.10
date	2001.11.12.01.26.10;	author art;	state Exp;
branches;
next	1.9;

1.9
date	2001.11.07.02.55.50;	author art;	state Exp;
branches;
next	1.8;

1.8
date	2001.11.06.01.35.04;	author art;	state Exp;
branches;
next	1.7;

1.7
date	2001.08.11.10.57.22;	author art;	state Exp;
branches;
next	1.6;

1.6
date	2001.03.08.15.21.37;	author smart;	state Exp;
branches;
next	1.5;

1.5
date	2001.01.29.02.07.48;	author niklas;	state Exp;
branches;
next	1.4;

1.4
date	2000.03.16.22.11.05;	author art;	state Exp;
branches;
next	1.3;

1.3
date	99.08.23.08.13.25;	author art;	state Exp;
branches
	1.3.4.1;
next	1.2;

1.2
date	99.02.26.05.32.08;	author art;	state Exp;
branches;
next	1.1;

1.1
date	99.02.26.01.30.17;	author art;	state Exp;
branches;
next	;

1.3.4.1
date	2000.03.24.09.09.51;	author niklas;	state Exp;
branches;
next	1.3.4.2;

1.3.4.2
date	2001.05.14.22.47.48;	author niklas;	state Exp;
branches;
next	1.3.4.3;

1.3.4.3
date	2001.10.31.03.32.14;	author nate;	state Exp;
branches;
next	1.3.4.4;

1.3.4.4
date	2001.11.13.23.02.31;	author niklas;	state Exp;
branches;
next	1.3.4.5;

1.3.4.5
date	2001.12.05.01.23.58;	author niklas;	state Exp;
branches;
next	1.3.4.6;

1.3.4.6
date	2002.03.06.02.17.14;	author niklas;	state Exp;
branches;
next	;

1.11.2.1
date	2002.02.02.03.28.27;	author art;	state Exp;
branches;
next	1.11.2.2;

1.11.2.2
date	2002.10.29.00.36.50;	author art;	state Exp;
branches;
next	1.11.2.3;

1.11.2.3
date	2002.11.04.18.02.33;	author art;	state Exp;
branches;
next	;

1.23.2.1
date	2008.11.07.19.24.19;	author brad;	state Exp;
branches;
next	;


desc
@@


1.39
log
@Move uvm_pglistalloc and uvm_pglistfree to uvm_page.c and garbage
college uvm_pglist.c

uvm_pglistalloc and free are just thin wrappers around pmemrange these
days and don't really need their own file.

ok ariane@@
@
text
@/*	$OpenBSD: uvm_pglist.c,v 1.38 2010/06/27 03:03:49 thib Exp $	*/
/*	$NetBSD: uvm_pglist.c,v 1.13 2001/02/18 21:19:08 chs Exp $	*/

/*-
 * Copyright (c) 1997 The NetBSD Foundation, Inc.
 * All rights reserved.
 *  
 * This code is derived from software contributed to The NetBSD Foundation
 * by Jason R. Thorpe of the Numerical Aerospace Simulation Facility,
 * NASA Ames Research Center.  
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright 
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *      
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

/*
 * uvm_pglist.c: pglist functions
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/malloc.h>
#include <sys/proc.h>

#include <uvm/uvm.h>

#ifdef VM_PAGE_ALLOC_MEMORY_STATS
#define	STAT_INCR(v)	(v)++
#define	STAT_DECR(v)	do { \
		if ((v) == 0) \
			printf("%s:%d -- Already 0!\n", __FILE__, __LINE__); \
		else \
			(v)--; \
	} while (0)
u_long	uvm_pglistalloc_npages;
#else
#define	STAT_INCR(v)
#define	STAT_DECR(v)
#endif

/*
 * uvm_pglistalloc: allocate a list of pages
 *
 * => allocated pages are placed at the tail of rlist.  rlist is
 *    assumed to be properly initialized by caller.
 * => returns 0 on success or errno on failure
 * => XXX: implementation allocates only a single segment, also
 *	might be able to better advantage of vm_physeg[].
 * => doesn't take into account clean non-busy pages on inactive list
 *	that could be used(?)
 * => params:
 *	size		the size of the allocation, rounded to page size.
 *	low		the low address of the allowed allocation range.
 *	high		the high address of the allowed allocation range.
 *	alignment	memory must be aligned to this power-of-two boundary.
 *	boundary	no segment in the allocation may cross this 
 *			power-of-two boundary (relative to zero).
 * => flags:
 *	UVM_PLA_NOWAIT	fail if allocation fails
 *	UVM_PLA_WAITOK	wait for memory to become avail
 *	UVM_PLA_ZERO	return zeroed memory
 */

int
uvm_pglistalloc(psize_t size, paddr_t low, paddr_t high, paddr_t alignment,
    paddr_t boundary, struct pglist *rlist, int nsegs, int flags)
{
	UVMHIST_FUNC("uvm_pglistalloc"); UVMHIST_CALLED(pghist);

	KASSERT((alignment & (alignment - 1)) == 0);
	KASSERT((boundary & (boundary - 1)) == 0);
	KASSERT(!(flags & UVM_PLA_WAITOK) ^ !(flags & UVM_PLA_NOWAIT));

	if (size == 0)
		return (EINVAL);

	if ((high & PAGE_MASK) != PAGE_MASK) {
		printf("uvm_pglistalloc: Upper boundary 0x%lx "
		    "not on pagemask.\n", (unsigned long)high);
	}

	/*
	 * Our allocations are always page granularity, so our alignment
	 * must be, too.
	 */
	if (alignment < PAGE_SIZE)
		alignment = PAGE_SIZE;

	low = atop(roundup(low, alignment));
	/*
	 * high + 1 may result in overflow, in which case high becomes 0x0,
	 * which is the 'don't care' value.
	 * The only requirement in that case is that low is also 0x0, or the
	 * low<high assert will fail.
	 */
	high = atop(high + 1);
	size = atop(round_page(size));
	alignment = atop(alignment);
	if (boundary < PAGE_SIZE && boundary != 0)
		boundary = PAGE_SIZE;
	boundary = atop(boundary);

	return uvm_pmr_getpages(size, low, high, alignment, boundary, nsegs,
	    flags, rlist);
}

/*
 * uvm_pglistfree: free a list of pages
 *
 * => pages should already be unmapped
 */

void
uvm_pglistfree(struct pglist *list)
{
	UVMHIST_FUNC("uvm_pglistfree"); UVMHIST_CALLED(pghist);
	uvm_pmr_freepageq(list);
}
@


1.38
log
@uvm constraints. Add two mandatory MD symbols, uvm_md_constraints
which contains the constraints for DMA/memory allocation for each
architecture, and dma_constraints which contains the range of addresses
that are dma accessable by the system.

This is based on ariane@@'s physcontig diff, with lots of bugfixes and
additions the following additions by my self:

Introduce a new function pool_set_constraints() which sets the address
range for which we allocate pages for the pool from, this is now used
for the mbuf/mbuf cluster pools to keep them dma accessible.

The !direct archs no longer stuff pages into the kernel object in
uvm_km_getpage_pla but rather do a pmap_extract() in uvm_km_putpages.

Tested heavily by my self on i386, amd64 and sparc64. Some tests on
alpha and SGI.

"commit it" beck, art, oga, deraadt
"i like the diff" deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.36 2010/04/22 19:02:55 oga Exp $	*/
@


1.37
log
@fix typos in comments: lineair -> linear.
@
text
@a79 1
 *	UVM_PLA_TRYCONTIG caller (device) prefers p-linear memory
@


1.36
log
@Committing on behalf or ariane@@.

recommit pmemrange:
        physmem allocator: change the view of free memory from single
        free pages to free ranges.  Classify memory based on region with
        associated use-counter (which is used to construct a priority
        list of where to allocate memory).

	Based on code from tedu@@, help from many.

Useable now that bugs have been found and fixed in most architecture's
pmap.c

ok by everyone who has done a pmap or uvm commit in the last year.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.35 2009/08/13 15:29:59 deraadt Exp $	*/
d80 1
a80 1
 *	UVM_PLA_TRYCONTIG caller (device) prefers p-lineair memory
@


1.35
log
@PAGEFASTRECYCLE is an option we have been using for a while to encourage
the kernel to reuse freed pages as quickly as possible, and it has been
finding bugs (some of which we have already fixed)
ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.34 2009/07/26 21:26:10 deraadt Exp $	*/
a58 106
int	uvm_pglistalloc_simple(psize_t, paddr_t, paddr_t, struct pglist *);

/*
 * Simple page allocation: pages do not need to be contiguous. We just
 * attempt to find enough free pages in the given range.
 */
int
uvm_pglistalloc_simple(psize_t size, paddr_t low, paddr_t high,
    struct pglist *rlist)
{
	psize_t todo;
	int psi;
	struct vm_page *pg;
	struct vm_physseg *seg;
	paddr_t slow, shigh;
	int pgflidx, error, free_list;
	UVMHIST_FUNC("uvm_pglistalloc_simple"); UVMHIST_CALLED(pghist);
#ifdef DEBUG
	vm_page_t tp;
#endif

	/* Default to "lose". */
	error = ENOMEM;

	todo = atop(size);

	/*
	 * Block all memory allocation and lock the free list.
	 */
	uvm_lock_fpageq();

	/* Are there even any free pages? */
	if (uvmexp.free <= (uvmexp.reserve_pagedaemon + uvmexp.reserve_kernel))
		goto out;

	for (psi = 0, seg = vm_physmem; psi < vm_nphysseg; psi++, seg++) {
		/*
		 * Skip this segment if incompatible with the address range.
		 */
		if (seg->avail_end <= atop(low))
			continue;
		if (seg->avail_start >= atop(high))
			continue;

		slow = MAX(atop(low), seg->avail_start);
		shigh = MIN(atop(high), seg->avail_end);

		/* we want to be able to allocate at least a page... */
		if (slow == shigh)
			continue;

		for (pg = &seg->pgs[slow - seg->start]; slow != shigh;
		    slow++, pg++) {
			if (VM_PAGE_IS_FREE(pg) == 0)
				continue;

			free_list = uvm_page_lookup_freelist(pg);
			pgflidx = (pg->pg_flags & PG_ZERO) ?
			    PGFL_ZEROS : PGFL_UNKNOWN;
#ifdef DEBUG
			for (tp = TAILQ_FIRST(&uvm.page_free[free_list].pgfl_queues[pgflidx]);
			     tp != NULL; tp = TAILQ_NEXT(tp, pageq)) {
				if (tp == pg)
					break;
			}
			if (tp == NULL)
				panic("uvm_pglistalloc_simple: page not on freelist");
#endif
			TAILQ_REMOVE(&uvm.page_free[free_list].pgfl_queues[pgflidx],
			    pg, pageq);
			uvmexp.free--;
			if (pg->pg_flags & PG_ZERO)
				uvmexp.zeropages--;
			pg->uobject = NULL;
			pg->uanon = NULL;
			pg->pg_version++;
			TAILQ_INSERT_TAIL(rlist, pg, pageq);
			STAT_INCR(uvm_pglistalloc_npages);
			if (--todo == 0) {
				error = 0;
				goto out;
			}
		}

	}

out:
	/*
	 * check to see if we need to generate some free pages waking
	 * the pagedaemon.
	 */

	if (!error && (uvmexp.free + uvmexp.paging < uvmexp.freemin ||
	    (uvmexp.free + uvmexp.paging < uvmexp.freetarg &&
	    uvmexp.inactive < uvmexp.inactarg))) {
		wakeup(&uvm.pagedaemon);
	}

	uvm_unlock_fpageq();

	if (error)
		uvm_pglistfree(rlist);

	return (error);
}

d76 5
a86 11
	int psi;
	struct vm_page *pgs;
	struct vm_physseg *seg;
	paddr_t slow, shigh;
	paddr_t try, idxpa, lastidxpa;
	int tryidx, idx, pgflidx, endidx, error, free_list;
	vm_page_t m;
	u_long pagemask;
#ifdef DEBUG
	vm_page_t tp;
#endif
a90 4
	/*
	 * This argument is always ignored for now, but ensure drivers always
	 * show intention.
	 */
a91 7
	
	/*
	 * Our allocations are always page granularity, so our alignment
	 * must be, too.
	 */
	if (alignment < PAGE_SIZE)
		alignment = PAGE_SIZE;
d96 3
a98 11
	size = round_page(size);
	low = roundup(low, alignment);

	/*
	 * If we are allowed to allocate as many segments as pages,
	 * no need to be smart.
	 */
	if ((nsegs >= size / PAGE_SIZE) && (alignment == PAGE_SIZE) &&
	    (boundary == 0)) {
		error = uvm_pglistalloc_simple(size, low, high, rlist);
		goto done;
a100 8
	if (boundary != 0 && boundary < size)
		return (EINVAL);

	pagemask = ~(boundary - 1);

	/* Default to "lose". */
	error = ENOMEM;

d102 2
a103 1
	 * Block all memory allocation and lock the free list.
d105 2
a106 64
	uvm_lock_fpageq();

	/* Are there even any free pages? */
	if (uvmexp.free <= (uvmexp.reserve_pagedaemon + uvmexp.reserve_kernel))
		goto out;

	for (psi = 0, seg = vm_physmem; psi < vm_nphysseg; psi++, seg++) {
		/*
		 * Skip this segment if incompatible with the address range.
		 */
		if (seg->avail_end <= atop(low))
			continue;
		if (seg->avail_start >= atop(high))
			continue;

		slow = MAX(low, ptoa(seg->avail_start));
		shigh = MIN(high, ptoa(seg->avail_end));

		try = roundup(slow, alignment);
		for (;; try += alignment) {
			if (try + size > shigh) {
				/*
				 * We've run past the allowable range, or
				 * the segment. Try another.
				 */
				break;
			}

			tryidx = idx = atop(try) - seg->start;
			endidx = idx + atop(size);
			pgs = vm_physmem[psi].pgs;

			/*
			 * Found a suitable starting page.  See if the
			 * range is free.
			 */

			for (; idx < endidx; idx++) {
				if (VM_PAGE_IS_FREE(&pgs[idx]) == 0) {
					break;
				}
				idxpa = VM_PAGE_TO_PHYS(&pgs[idx]);
				if (idx == tryidx)
					continue;

				/*
				 * Check that the region is contiguous
				 * (it really should...) and does not
				 * cross an alignment boundary.
				 */
				lastidxpa = VM_PAGE_TO_PHYS(&pgs[idx - 1]);
				if ((lastidxpa + PAGE_SIZE) != idxpa)
					break;

				if (boundary != 0 &&
				    ((lastidxpa ^ idxpa) & pagemask) != 0)
					break;
			}

			if (idx == endidx) {
				goto found;
			}
		}
	}
d108 1
d110 11
a120 8
	 * We could not allocate a contiguous range.  This is where
	 * we should try harder if nsegs > 1...
	 */
	goto out;

#if PGFL_NQUEUES != 2
#error uvm_pglistalloc needs to be updated
#endif
d122 2
a123 60
found:
	/*
	 * we have a chunk of memory that conforms to the requested constraints.
	 */
	idx = tryidx;
	while (idx < endidx) {
		m = &pgs[idx];
		free_list = uvm_page_lookup_freelist(m);
		pgflidx = (m->pg_flags & PG_ZERO) ? PGFL_ZEROS : PGFL_UNKNOWN;
#ifdef DEBUG
		for (tp = TAILQ_FIRST(&uvm.page_free[
			free_list].pgfl_queues[pgflidx]);
		     tp != NULL;
		     tp = TAILQ_NEXT(tp, pageq)) {
			if (tp == m)
				break;
		}
		if (tp == NULL)
			panic("uvm_pglistalloc: page not on freelist");
#endif
		TAILQ_REMOVE(&uvm.page_free[free_list].pgfl_queues[pgflidx],
		    m, pageq);
		uvmexp.free--;
		if (m->pg_flags & PG_ZERO)
			uvmexp.zeropages--;
		m->uobject = NULL;
		m->uanon = NULL;
		m->pg_version++;
		TAILQ_INSERT_TAIL(rlist, m, pageq);
		idx++;
		STAT_INCR(uvm_pglistalloc_npages);
	}
	error = 0;

out:
	/*
	 * check to see if we need to generate some free pages waking
	 * the pagedaemon.
	 */
	 
	if (uvmexp.free + uvmexp.paging < uvmexp.freemin ||
	    (uvmexp.free + uvmexp.paging < uvmexp.freetarg &&
	     uvmexp.inactive < uvmexp.inactarg)) {
		wakeup(&uvm.pagedaemon);
	}

	uvm_unlock_fpageq();

done: 
	/* No locking needed here, pages are not on any queue. */
	if (error == 0) {
		TAILQ_FOREACH(m, rlist, pageq) {
			if (flags & UVM_PLA_ZERO &&
			    (m->pg_flags & PG_ZERO) == 0)
				uvm_pagezero(m);
			m->pg_flags = PG_CLEAN;
		}
	}

	return (error);
a134 1
	struct vm_page *m;
d136 1
a136 37

	/*
	 * Block all memory allocation and lock the free list.
	 */
	uvm_lock_fpageq();

	while ((m = TAILQ_FIRST(list)) != NULL) {
		KASSERT((m->pg_flags & (PQ_ACTIVE|PQ_INACTIVE)) == 0);
		TAILQ_REMOVE(list, m, pageq);
#ifdef DEBUG
		if (m->uobject == (void *)0xdeadbeef &&
		    m->uanon == (void *)0xdeadbeef) {
			panic("uvm_pglistfree: freeing free page %p", m);
		}

		m->uobject = (void *)0xdeadbeef;
		m->offset = 0xdeadbeef;
		m->uanon = (void *)0xdeadbeef;
#endif
		atomic_clearbits_int(&m->pg_flags, PQ_MASK);
		atomic_setbits_int(&m->pg_flags, PQ_FREE);
#ifdef PAGEFASTRECYCLE
		TAILQ_INSERT_HEAD(&uvm.page_free[
		    uvm_page_lookup_freelist(m)].pgfl_queues[PGFL_UNKNOWN],
		    m, pageq);
#else
		TAILQ_INSERT_TAIL(&uvm.page_free[
		    uvm_page_lookup_freelist(m)].pgfl_queues[PGFL_UNKNOWN],
		    m, pageq);
#endif
		uvmexp.free++;
		if (uvmexp.zeropages < UVM_PAGEZERO_TARGET)
			uvm.page_idle_zero = vm_page_zero_enable;
		STAT_DECR(uvm_pglistalloc_npages);
	}

	uvm_unlock_fpageq();
@


1.34
log
@stop trying to fast-recycle pages for now.  a few bugs have been found and
fixed, but now it is time for a little break from the chaos.
ok kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.32 2009/06/17 00:13:59 oga Exp $	*/
d412 5
d420 1
@


1.33
log
@Insert free pages at the head of the page queues.  Should provide better
cache locality and will pave the way for the new pmemrange allocator.
Based on hints from art@@ and ariane@@.

ok ariane@@, deraadt@@, oga@@
@
text
@d412 1
a412 1
		TAILQ_INSERT_HEAD(&uvm.page_free[
@


1.32
log
@date based reversion of uvm to the 4th May.

More backouts in line with previous ones, this appears to bring us back to a
stable condition.

A machine forced to 64mb of ram cycled 10GB through swap with this diff
and is still running as I type this. Other tests by ariane@@ and thib@@
also seem to show that it's alright.

ok deraadt@@, thib@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.28 2009/04/30 09:06:01 oga Exp $	*/
d412 1
a412 1
		TAILQ_INSERT_TAIL(&uvm.page_free[
@


1.31
log
@Backout pmemrange (which to most people is more well known as physmem
allocator).

"i can't see any obvious problems" oga
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.29 2009/05/04 18:08:06 oga Exp $	*/
d154 1
a154 1
		wakeup(&uvm.pagedaemon_proc);
d361 1
a361 1
		wakeup(&uvm.pagedaemon_proc);
@


1.30
log
@physmem allocator: change the view of free memory from single free pages
to free ranges.
Classify memory based on region with associated use-counter (which is used
to construct a priority list of where to allocate memory).

Based on code from tedu@@, help from many.
Ok art@@
@
text
@d59 106
a181 5
 * => flags:
 *	UVM_PLA_NOWAIT	fail if allocation fails
 *	UVM_PLA_WAITOK	wait for memory to become avail if allocation fails
 *	UVM_PLA_ZERO	return zeroed memory
 *	UVM_PLA_TRY_CONTIG device prefers p-lineair mem
d188 11
d203 4
d208 7
d219 134
d354 2
a355 1
	 * Convert byte addresses to page numbers.
d357 19
a375 13
	if (alignment < PAGE_SIZE)
		alignment = PAGE_SIZE;
	low = atop(roundup(low, alignment));
	/* Allows for overflow: 0xffff + 1 = 0x0000 */
	if ((high & PAGE_MASK) == PAGE_MASK)
		high = atop(high) + 1;
	else
		high = atop(high);
	size = atop(round_page(size));
	alignment = atop(alignment);
	if (boundary < PAGE_SIZE && boundary != 0)
		boundary = PAGE_SIZE;
	boundary = atop(boundary);
d377 1
a377 2
	return uvm_pmr_getpages(size, low, high, alignment, boundary, nsegs,
	    flags, rlist);
d392 6
a397 1
	TAILQ_FOREACH(m, list, pageq) {
d399 1
d411 8
d420 2
a421 1
	uvm_pmr_freepageq(list);
@


1.29
log
@Instead of keeping two ints in the uvm structure specifically just to
sleep on them (and otherwise ignore them) sleep on the pointer to the
{aiodoned,pagedaemon}_proc members, and nuke the two extra words.

"no objections" art@@, ok beck@@.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.28 2009/04/30 09:06:01 oga Exp $	*/
a58 106
int	uvm_pglistalloc_simple(psize_t, paddr_t, paddr_t, struct pglist *);

/*
 * Simple page allocation: pages do not need to be contiguous. We just
 * attempt to find enough free pages in the given range.
 */
int
uvm_pglistalloc_simple(psize_t size, paddr_t low, paddr_t high,
    struct pglist *rlist)
{
	psize_t todo;
	int psi;
	struct vm_page *pg;
	struct vm_physseg *seg;
	paddr_t slow, shigh;
	int pgflidx, error, free_list;
	UVMHIST_FUNC("uvm_pglistalloc_simple"); UVMHIST_CALLED(pghist);
#ifdef DEBUG
	vm_page_t tp;
#endif

	/* Default to "lose". */
	error = ENOMEM;

	todo = atop(size);

	/*
	 * Block all memory allocation and lock the free list.
	 */
	uvm_lock_fpageq();

	/* Are there even any free pages? */
	if (uvmexp.free <= (uvmexp.reserve_pagedaemon + uvmexp.reserve_kernel))
		goto out;

	for (psi = 0, seg = vm_physmem; psi < vm_nphysseg; psi++, seg++) {
		/*
		 * Skip this segment if incompatible with the address range.
		 */
		if (seg->avail_end <= atop(low))
			continue;
		if (seg->avail_start >= atop(high))
			continue;

		slow = MAX(atop(low), seg->avail_start);
		shigh = MIN(atop(high), seg->avail_end);

		/* we want to be able to allocate at least a page... */
		if (slow == shigh)
			continue;

		for (pg = &seg->pgs[slow - seg->start]; slow != shigh;
		    slow++, pg++) {
			if (VM_PAGE_IS_FREE(pg) == 0)
				continue;

			free_list = uvm_page_lookup_freelist(pg);
			pgflidx = (pg->pg_flags & PG_ZERO) ?
			    PGFL_ZEROS : PGFL_UNKNOWN;
#ifdef DEBUG
			for (tp = TAILQ_FIRST(&uvm.page_free[free_list].pgfl_queues[pgflidx]);
			     tp != NULL; tp = TAILQ_NEXT(tp, pageq)) {
				if (tp == pg)
					break;
			}
			if (tp == NULL)
				panic("uvm_pglistalloc_simple: page not on freelist");
#endif
			TAILQ_REMOVE(&uvm.page_free[free_list].pgfl_queues[pgflidx],
			    pg, pageq);
			uvmexp.free--;
			if (pg->pg_flags & PG_ZERO)
				uvmexp.zeropages--;
			pg->uobject = NULL;
			pg->uanon = NULL;
			pg->pg_version++;
			TAILQ_INSERT_TAIL(rlist, pg, pageq);
			STAT_INCR(uvm_pglistalloc_npages);
			if (--todo == 0) {
				error = 0;
				goto out;
			}
		}

	}

out:
	/*
	 * check to see if we need to generate some free pages waking
	 * the pagedaemon.
	 */

	if (!error && (uvmexp.free + uvmexp.paging < uvmexp.freemin ||
	    (uvmexp.free + uvmexp.paging < uvmexp.freetarg &&
	    uvmexp.inactive < uvmexp.inactarg))) {
		wakeup(&uvm.pagedaemon_proc);
	}

	uvm_unlock_fpageq();

	if (error)
		uvm_pglistfree(rlist);

	return (error);
}

d76 5
a86 11
	int psi;
	struct vm_page *pgs;
	struct vm_physseg *seg;
	paddr_t slow, shigh;
	paddr_t try, idxpa, lastidxpa;
	int tryidx, idx, pgflidx, endidx, error, free_list;
	vm_page_t m;
	u_long pagemask;
#ifdef DEBUG
	vm_page_t tp;
#endif
a90 4
	/*
	 * This argument is always ignored for now, but ensure drivers always
	 * show intention.
	 */
a91 7
	
	/*
	 * Our allocations are always page granularity, so our alignment
	 * must be, too.
	 */
	if (alignment < PAGE_SIZE)
		alignment = PAGE_SIZE;
a95 21
	size = round_page(size);
	low = roundup(low, alignment);

	/*
	 * If we are allowed to allocate as many segments as pages,
	 * no need to be smart.
	 */
	if ((nsegs >= size / PAGE_SIZE) && (alignment == PAGE_SIZE) &&
	    (boundary == 0)) {
		error = uvm_pglistalloc_simple(size, low, high, rlist);
		goto done;
	}

	if (boundary != 0 && boundary < size)
		return (EINVAL);

	pagemask = ~(boundary - 1);

	/* Default to "lose". */
	error = ENOMEM;

d97 1
a97 1
	 * Block all memory allocation and lock the free list.
d99 13
a111 1
	uvm_lock_fpageq();
d113 2
a114 133
	/* Are there even any free pages? */
	if (uvmexp.free <= (uvmexp.reserve_pagedaemon + uvmexp.reserve_kernel))
		goto out;

	for (psi = 0, seg = vm_physmem; psi < vm_nphysseg; psi++, seg++) {
		/*
		 * Skip this segment if incompatible with the address range.
		 */
		if (seg->avail_end <= atop(low))
			continue;
		if (seg->avail_start >= atop(high))
			continue;

		slow = MAX(low, ptoa(seg->avail_start));
		shigh = MIN(high, ptoa(seg->avail_end));

		try = roundup(slow, alignment);
		for (;; try += alignment) {
			if (try + size > shigh) {
				/*
				 * We've run past the allowable range, or
				 * the segment. Try another.
				 */
				break;
			}

			tryidx = idx = atop(try) - seg->start;
			endidx = idx + atop(size);
			pgs = vm_physmem[psi].pgs;

			/*
			 * Found a suitable starting page.  See if the
			 * range is free.
			 */

			for (; idx < endidx; idx++) {
				if (VM_PAGE_IS_FREE(&pgs[idx]) == 0) {
					break;
				}
				idxpa = VM_PAGE_TO_PHYS(&pgs[idx]);
				if (idx == tryidx)
					continue;

				/*
				 * Check that the region is contiguous
				 * (it really should...) and does not
				 * cross an alignment boundary.
				 */
				lastidxpa = VM_PAGE_TO_PHYS(&pgs[idx - 1]);
				if ((lastidxpa + PAGE_SIZE) != idxpa)
					break;

				if (boundary != 0 &&
				    ((lastidxpa ^ idxpa) & pagemask) != 0)
					break;
			}

			if (idx == endidx) {
				goto found;
			}
		}
	}

	/*
	 * We could not allocate a contiguous range.  This is where
	 * we should try harder if nsegs > 1...
	 */
	goto out;

#if PGFL_NQUEUES != 2
#error uvm_pglistalloc needs to be updated
#endif

found:
	/*
	 * we have a chunk of memory that conforms to the requested constraints.
	 */
	idx = tryidx;
	while (idx < endidx) {
		m = &pgs[idx];
		free_list = uvm_page_lookup_freelist(m);
		pgflidx = (m->pg_flags & PG_ZERO) ? PGFL_ZEROS : PGFL_UNKNOWN;
#ifdef DEBUG
		for (tp = TAILQ_FIRST(&uvm.page_free[
			free_list].pgfl_queues[pgflidx]);
		     tp != NULL;
		     tp = TAILQ_NEXT(tp, pageq)) {
			if (tp == m)
				break;
		}
		if (tp == NULL)
			panic("uvm_pglistalloc: page not on freelist");
#endif
		TAILQ_REMOVE(&uvm.page_free[free_list].pgfl_queues[pgflidx],
		    m, pageq);
		uvmexp.free--;
		if (m->pg_flags & PG_ZERO)
			uvmexp.zeropages--;
		m->uobject = NULL;
		m->uanon = NULL;
		m->pg_version++;
		TAILQ_INSERT_TAIL(rlist, m, pageq);
		idx++;
		STAT_INCR(uvm_pglistalloc_npages);
	}
	error = 0;

out:
	/*
	 * check to see if we need to generate some free pages waking
	 * the pagedaemon.
	 */
	 
	if (uvmexp.free + uvmexp.paging < uvmexp.freemin ||
	    (uvmexp.free + uvmexp.paging < uvmexp.freetarg &&
	     uvmexp.inactive < uvmexp.inactarg)) {
		wakeup(&uvm.pagedaemon_proc);
	}

	uvm_unlock_fpageq();

done: 
	/* No locking needed here, pages are not on any queue. */
	if (error == 0) {
		TAILQ_FOREACH(m, rlist, pageq) {
			if (flags & UVM_PLA_ZERO &&
			    (m->pg_flags & PG_ZERO) == 0)
				uvm_pagezero(m);
			m->pg_flags = PG_CLEAN;
		}
	}

	return (error);
d129 1
a129 6
	/*
	 * Block all memory allocation and lock the free list.
	 */
	uvm_lock_fpageq();

	while ((m = TAILQ_FIRST(list)) != NULL) {
a130 1
		TAILQ_REMOVE(list, m, pageq);
a141 8
		atomic_setbits_int(&m->pg_flags, PQ_FREE);
		TAILQ_INSERT_TAIL(&uvm.page_free[
		    uvm_page_lookup_freelist(m)].pgfl_queues[PGFL_UNKNOWN],
		    m, pageq);
		uvmexp.free++;
		if (uvmexp.zeropages < UVM_PAGEZERO_TARGET)
			uvm.page_idle_zero = vm_page_zero_enable;
		STAT_DECR(uvm_pglistalloc_npages);
d143 1
a143 2

	uvm_unlock_fpageq();
@


1.28
log
@obvious typo in panic string.

ariane@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.27 2009/04/20 00:30:18 oga Exp $	*/
d154 1
a154 1
		wakeup(&uvm.pagedaemon);
d361 1
a361 1
		wakeup(&uvm.pagedaemon);
@


1.27
log
@add the UVM_PLA_ZERO flag for uvm_pglistalloc to make it return zeroed
pages.

"go for it" miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.26 2009/04/14 16:01:04 oga Exp $	*/
d403 1
a403 1
			panic("uvm_pagefree: freeing free page %p", m);
@


1.26
log
@Convert the waitok field of uvm_pglistalloc to "flags", more will be added soon.

For the possibility of sleeping, the first two flags are UVM_PLA_WAITOK
and UVM_PLA_NOWAIT. It is an error not to show intention, so assert that
one of the two is provided. Switch over every caller in the tree to
using the appropriate flag.

ok art@@, ariane@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.25 2009/03/20 15:19:04 oga Exp $	*/
a131 1
			pg->pg_flags = PG_CLEAN;
d227 4
a230 2
	    (boundary == 0))
		return (uvm_pglistalloc_simple(size, low, high, rlist));
a342 1
		m->pg_flags = PG_CLEAN;
d365 11
@


1.25
log
@While working on some stuff in uvm I've gotten REALLY sick of reading
K&R function declarations, so switch them all over to ansi-style, in
accordance with the prophesy.

"go for it" art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.24 2008/10/01 20:00:32 miod Exp $	*/
d187 1
a187 1
    paddr_t boundary, struct pglist *rlist, int nsegs, int waitok)
d204 5
@


1.24
log
@In uvm_pglistalloc(), do not fall through the success code if we could not find
a suitable range and ran out of memory segments. Oops.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.23 2008/06/27 17:25:47 miod Exp $	*/
d186 2
a187 5
uvm_pglistalloc(size, low, high, alignment, boundary, rlist, nsegs, waitok)
	psize_t size;
	paddr_t low, high, alignment, boundary;
	struct pglist *rlist;
	int nsegs, waitok;
@


1.23
log
@uvm_pglistalloc() works by walking the physical address range it gets invoked
with, trying to find free pages matching the callers requirement.

However, on systems with noncontiguous memory and large gaps between
segments, this is a disaster as soon as one of these gaps is hit.

Rewrite the logic by iterating on the physsegs, and the on the intersection
of the physseg range and the callers range. This also frees us from having
to check whether a given page range crosses a physseg.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.22 2008/06/26 05:42:20 ray Exp $	*/
d304 6
@


1.23.2.1
log
@In uvm_pglistalloc(), do not fall through the success code if we could not find
a suitable range and ran out of memory segments. Oops.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.23 2008/06/27 17:25:47 miod Exp $	*/
a303 6

	/*
	 * We could not allocate a contiguous range.  This is where
	 * we should try harder if nsegs > 1...
	 */
	goto out;
@


1.22
log
@First pass at removing clauses 3 and 4 from NetBSD licenses.

Not sure what's more surprising: how long it took for NetBSD to
catch up to the rest of the BSDs (including UCB), or the amount of
code that NetBSD has claimed for itself without attributing to the
actual authors.

OK deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.21 2007/12/18 11:05:52 thib Exp $	*/
d61 4
d69 1
a69 1
	psize_t try;
d72 3
a74 1
	int todo, idx, pgflidx, error, free_list;
d83 1
a83 1
	todo = size / PAGE_SIZE;
d94 1
a94 2
	for (try = low; try < high; try += PAGE_SIZE) {

d96 1
a96 1
		 * Make sure this is a managed physical page.
d98 4
d103 5
a107 4
		if ((psi = vm_physseg_find(atop(try), &idx)) == -1)
			continue; /* managed? */
		pg = &vm_physmem[psi].pgs[idx];
		if (VM_PAGE_IS_FREE(pg) == 0)
d110 8
a117 2
		free_list = uvm_page_lookup_freelist(pg);
		pgflidx = (pg->pg_flags & PG_ZERO) ? PGFL_ZEROS : PGFL_UNKNOWN;
d119 7
a125 8
		for (tp = TAILQ_FIRST(&uvm.page_free[free_list].pgfl_queues[pgflidx]);
		     tp != NULL;
		     tp = TAILQ_NEXT(tp, pageq)) {
			if (tp == pg)
				break;
		}
		if (tp == NULL)
			panic("uvm_pglistalloc_simple: page not on freelist");
d127 15
a141 13
		TAILQ_REMOVE(&uvm.page_free[free_list].pgfl_queues[pgflidx], pg, pageq);
		uvmexp.free--;
		if (pg->pg_flags & PG_ZERO)
			uvmexp.zeropages--;
		pg->pg_flags = PG_CLEAN;
		pg->uobject = NULL;
		pg->uanon = NULL;
		pg->pg_version++;
		TAILQ_INSERT_TAIL(rlist, pg, pageq);
		STAT_INCR(uvm_pglistalloc_npages);
		if (--todo == 0) {
			error = 0;
			goto out;
d143 1
a191 1
	paddr_t try, idxpa, lastidxpa;
d194 4
a197 1
	int tryidx, idx, pgflidx, end, error, free_list;
d219 1
a219 1
	try = roundup(low, alignment);
d221 4
d227 1
a227 1
		return (uvm_pglistalloc_simple(size, try, high, rlist));
d246 1
a246 10
	for (;; try += alignment) {
		if (try + size > high) {

			/*
			 * We've run past the allowable range.
			 */

			goto out;
		}

d248 1
a248 1
		 * Make sure this is a managed physical page.
d250 4
d255 2
a256 12
		if ((psi = vm_physseg_find(atop(try), &idx)) == -1)
			continue; /* managed? */
		if (vm_physseg_find(atop(try + size), NULL) != psi)
			continue; /* end must be in this segment */

		tryidx = idx;
		end = idx + (size / PAGE_SIZE);
		pgs = vm_physmem[psi].pgs;

		/*
		 * Found a suitable starting page.  See of the range is free.
		 */
d258 7
a264 2
		for (; idx < end; idx++) {
			if (VM_PAGE_IS_FREE(&pgs[idx]) == 0) {
a266 4
			idxpa = VM_PAGE_TO_PHYS(&pgs[idx]);
			if (idx > tryidx) {
				lastidxpa = VM_PAGE_TO_PHYS(&pgs[idx - 1]);
				if ((lastidxpa + PAGE_SIZE) != idxpa) {
d268 8
a275 3
					/*
					 * Region not contiguous.
					 */
d277 2
d281 13
d295 3
a297 5
				    ((lastidxpa ^ idxpa) & pagemask) != 0) {

					/*
					 * Region crosses boundary.
					 */
d299 2
a300 2
					break;
				}
a302 3
		if (idx == end) {
			break;
		}
d309 1
d314 1
a314 1
	while (idx < end) {
@


1.21
log
@Turn the uvm_{lock/unlock}_fpageq() inlines into
macros that just expand into the mutex functions
to keep the abstraction, do assorted cleanup.

ok miod@@,art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.20 2007/04/13 18:57:49 art Exp $	*/
a19 7
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *      This product includes software developed by the NetBSD
 *      Foundation, Inc. and its contributors.
 * 4. Neither the name of The NetBSD Foundation nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
@


1.20
log
@While splitting flags and pqflags might have been a good idea in theory
to separate locking, on most modern machines this is not enough
since operations on short types touch other short types that share the
same word in memory.

Merge pg_flags and pqflags again and now use atomic operations to change
the flags. Also bump wire_count to an int and pg_version might go
int as well, just for alignment.

tested by many, many. ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.19 2007/04/04 17:44:45 art Exp $	*/
d75 1
a75 1
	int s, todo, idx, pgflidx, error, free_list;
d89 1
a89 1
	s = uvm_lock_fpageq();
d147 1
a147 1
	uvm_unlock_fpageq(s);
d184 1
a184 1
	int s, tryidx, idx, pgflidx, end, error, free_list;
d223 1
a223 1
	s = uvm_lock_fpageq();
d337 1
a337 1
	uvm_unlock_fpageq(s);
a351 1
	int s;
d357 1
a357 1
	s = uvm_lock_fpageq();
d383 1
a383 1
	uvm_unlock_fpageq(s);
@


1.19
log
@Mechanically rename the "flags" and "version" fields in struct vm_page
to "pg_flags" and "pg_version", so that they are a bit easier to work with.
Whoever uses generic names like this for a popular struct obviously doesn't
read much code.

Most architectures compile and there are no functionality changes.

deraadt@@ ok ("if something fails to compile, we fix that by hand")
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.18 2007/02/12 11:43:47 tom Exp $	*/
a123 1
		pg->pqflags = 0;
a315 1
		m->pqflags = 0;
d349 1
a349 2
uvm_pglistfree(list)
	struct pglist *list;
d351 1
a351 1
	vm_page_t m;
d361 1
a361 1
		KASSERT((m->pqflags & (PQ_ACTIVE|PQ_INACTIVE)) == 0);
d373 2
a374 1
		m->pqflags = PQ_FREE;
@


1.18
log
@Let this compile with 'option DEBUG' again.

ok dim@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.17 2007/01/16 13:36:38 dim Exp $	*/
d108 1
a108 1
		pgflidx = (pg->flags & PG_ZERO) ? PGFL_ZEROS : PGFL_UNKNOWN;
d121 1
a121 1
		if (pg->flags & PG_ZERO)
d123 1
a123 1
		pg->flags = PG_CLEAN;
d127 1
a127 1
		pg->version++;
d299 1
a299 1
		pgflidx = (m->flags & PG_ZERO) ? PGFL_ZEROS : PGFL_UNKNOWN;
d314 1
a314 1
		if (m->flags & PG_ZERO)
d316 1
a316 1
		m->flags = PG_CLEAN;
d320 1
a320 1
		m->version++;
@


1.17
log
@Implement multiple segment allocation for uvm_pglistalloc, which fixes
most agp_generic_bind_memory failures when memory is limited and very
fragmented.

In effect, this should fix a lot of X startup crashes after
activities that exercise memory a lot (e.g. make builds, building big
ports, etc).

ok mickey, miod
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.16 2006/06/01 05:16:49 krw Exp $	*/
d77 3
@


1.16
log
@Make umv_pglistalloc() return EINVAL if it is asked for 0 bytes. This
will prevent panics in, e.g., bus_dmamem_alloc().

ok jason@@ art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.15 2006/01/16 13:11:06 mickey Exp $	*/
d66 87
d205 4
@


1.15
log
@add another uvm histroy for physpage alloc/free and propagate a debugging pgfree check into pglist; no functional change for normal kernels; make histories uncommon
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.14 2002/10/07 18:35:52 mickey Exp $	*/
d112 3
@


1.14
log
@this removes the functionality of adding allocated
pages into the queue already containing allocated pages.
breaks i386:setup_buffers() because of this.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.13 2002/10/06 22:04:41 art Exp $	*/
d101 1
d259 1
d269 10
@


1.13
log
@Initialize the result list in uvm_pglistalloc.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.12 2001/12/19 08:58:07 art Exp $	*/
a121 2

	TAILQ_INIT(rlist);
@


1.12
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.10 2001/11/12 01:26:10 art Exp $	*/
d122 2
@


1.11
log
@Sync in more uvm from NetBSD. Mostly just cosmetic stuff.
Contains also support for page coloring.
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_pglist.c,v 1.17 2001/06/27 21:18:34 thorpej Exp $	*/
d7 1
a7 1
 *
d10 1
a10 1
 * NASA Ames Research Center.
d17 1
a17 1
 * 2. Redistributions in binary form must reproduce the above copyright
d27 1
a27 1
 *
d81 1
a81 1
 *	boundary	no segment in the allocation may cross this
d95 2
a96 2
	int s, tryidx, idx, pgflidx, end, error, free_list, color;
	struct vm_page *m;
d99 1
a99 1
	struct vm_page *tp;
d104 1
a104 1

a200 1
		color = VM_PGCOLOR_BUCKET(m);
d204 1
a204 1
			free_list].pgfl_buckets[color].pgfl_queues[pgflidx]);
d213 2
a214 2
		TAILQ_REMOVE(&uvm.page_free[free_list].pgfl_buckets[
		    color].pgfl_queues[pgflidx], m, pageq);
d234 6
a239 2

	UVM_KICK_PDAEMON();
d256 1
a256 1
	struct vm_page *m;
d269 2
a270 2
		    uvm_page_lookup_freelist(m)].pgfl_buckets[
		    VM_PGCOLOR_BUCKET(m)].pgfl_queues[PGFL_UNKNOWN], m, pageq);
@


1.11.2.1
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pglist.c,v 1.11 2001/11/28 19:28:15 art Exp $	*/
/*	$NetBSD: uvm_pglist.c,v 1.19 2001/11/10 07:37:01 lukem Exp $	*/
d94 1
a94 1
	struct vm_page *pgs, *pg;
d96 1
a108 1

d111 1
d114 1
d117 1
a125 1

d150 1
d199 4
a202 4
		pg = &pgs[idx];
		free_list = uvm_page_lookup_freelist(pg);
		color = VM_PGCOLOR_BUCKET(pg);
		pgflidx = (pg->flags & PG_ZERO) ? PGFL_ZEROS : PGFL_UNKNOWN;
d208 1
a208 1
			if (tp == pg)
d215 1
a215 1
		    color].pgfl_queues[pgflidx], pg, pageq);
d217 1
a217 1
		if (pg->flags & PG_ZERO)
d219 6
a224 5
		pg->flags = PG_CLEAN;
		pg->pqflags = 0;
		pg->uobject = NULL;
		pg->uanon = NULL;
		TAILQ_INSERT_TAIL(rlist, pg, pageq);
d237 1
d239 1
d253 1
a253 1
	struct vm_page *pg;
d257 1
a257 1
	 * Lock the free list and free each page.
d259 1
d261 7
a267 8
	s = uvm_lock_fpageq();
	while ((pg = TAILQ_FIRST(list)) != NULL) {
		KASSERT((pg->pqflags & (PQ_ACTIVE|PQ_INACTIVE)) == 0);
		TAILQ_REMOVE(list, pg, pageq);
		pg->pqflags = PQ_FREE;
		TAILQ_INSERT_TAIL(&uvm.page_free[uvm_page_lookup_freelist(pg)].
		    pgfl_buckets[VM_PGCOLOR_BUCKET(pg)].
		    pgfl_queues[PGFL_UNKNOWN], pg, pageq);
d273 1
@


1.11.2.2
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: uvm_pglist.c,v 1.11.2.1 2002/02/02 03:28:27 art Exp $	*/
@


1.11.2.3
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pglist.c,v 1.11.2.2 2002/10/29 00:36:50 art Exp $	*/
/*	$NetBSD: uvm_pglist.c,v 1.25 2002/11/02 07:40:49 perry Exp $	*/
d59 1
a59 1
	} while (/*CONSTCOND*/ 0)
d85 4
a88 13
static void uvm_pglist_add(struct vm_page *, struct pglist *);
static int uvm_pglistalloc_c_ps(struct vm_physseg *, int, paddr_t, paddr_t,
				paddr_t, paddr_t, struct pglist *);
static int uvm_pglistalloc_contig(int, paddr_t, paddr_t, paddr_t, paddr_t,
				  struct pglist *);
static int uvm_pglistalloc_s_ps(struct vm_physseg *, int, paddr_t, paddr_t,
				struct pglist *);
static int uvm_pglistalloc_simple(int, paddr_t, paddr_t,
				  struct pglist *, int);

static void
uvm_pglist_add(pg, rlist)
	struct vm_page *pg;
d90 1
d92 5
a96 1
	int free_list, color, pgflidx;
d101 24
a124 3
#if PGFL_NQUEUES != 2
#error uvm_pglistalloc needs to be updated
#endif
d126 3
a128 44
	free_list = uvm_page_lookup_freelist(pg);
	color = VM_PGCOLOR_BUCKET(pg);
	pgflidx = (pg->flags & PG_ZERO) ? PGFL_ZEROS : PGFL_UNKNOWN;
#ifdef DEBUG
	for (tp = TAILQ_FIRST(&uvm.page_free[
		free_list].pgfl_buckets[color].pgfl_queues[pgflidx]);
	     tp != NULL;
	     tp = TAILQ_NEXT(tp, pageq)) {
		if (tp == pg)
			break;
	}
	if (tp == NULL)
		panic("uvm_pglistalloc: page not on freelist");
#endif
	TAILQ_REMOVE(&uvm.page_free[free_list].pgfl_buckets[
			color].pgfl_queues[pgflidx], pg, pageq);
	uvmexp.free--;
	if (pg->flags & PG_ZERO)
		uvmexp.zeropages--;
	pg->flags = PG_CLEAN;
	pg->pqflags = 0;
	pg->uobject = NULL;
	pg->uanon = NULL;
	TAILQ_INSERT_TAIL(rlist, pg, pageq);
	STAT_INCR(uvm_pglistalloc_npages);
}

static int
uvm_pglistalloc_c_ps(ps, num, low, high, alignment, boundary, rlist)
	struct vm_physseg *ps;
	int num;
	paddr_t low, high, alignment, boundary;
	struct pglist *rlist;
{
	int try, limit, tryidx, end, idx;
	struct vm_page *pgs;
	int pagemask;
#ifdef DEBUG
	paddr_t idxpa, lastidxpa;
	int cidx;
#endif
#ifdef PGALLOC_VERBOSE
	printf("pgalloc: contig %d pgs from psi %d\n", num, ps - vm_physmem);
#endif
d130 2
a131 3
	try = roundup(max(atop(low), ps->avail_start), atop(alignment));
	limit = min(atop(high), ps->avail_end);
	pagemask = ~((boundary >> PAGE_SHIFT) - 1);
a132 2
	for (;;) {
		if (try + num > limit) {
d136 2
a137 1
			return (0); /* FAIL */
d139 1
a139 11
		if (boundary != 0 &&
		    ((try ^ (try + num - 1)) & pagemask) != 0) {
			/*
			 * Region crosses boundary. Jump to the boundary
			 * just crossed and ensure alignment.
			 */
			try = (try + num - 1) & pagemask;
			try = roundup(try, atop(alignment));
			continue;
		}
#ifdef DEBUG
d144 7
a150 12
		if (vm_physseg_find(try, &cidx) != ps - vm_physmem)
			panic("pgalloc contig: botch1");
		if (cidx != try - ps->start)
			panic("pgalloc contig: botch2");
		if (vm_physseg_find(try + num - 1, &cidx) != ps - vm_physmem)
			panic("pgalloc contig: botch3");
		if (cidx != try - ps->start + num - 1)
			panic("pgalloc contig: botch4");		
#endif
		tryidx = try - ps->start;
		end = tryidx + num;
		pgs = ps->pgs;
d153 1
a153 1
		 * Found a suitable starting page.  See if the range is free.
d155 3
a157 2
		for (idx = tryidx; idx < end; idx++) {
			if (VM_PAGE_IS_FREE(&pgs[idx]) == 0)
d159 1
a159 2

#ifdef DEBUG
d164 1
d168 2
a169 1
					panic("pgalloc contig: botch5");
d172 2
a173 2
				    ((lastidxpa ^ idxpa) & ~(boundary - 1))
				    != 0) {
d177 2
a178 1
					panic("pgalloc contig: botch6");
a180 1
#endif
d182 1
a182 1
		if (idx == end)
d184 2
d187 3
a189 2
		try += atop(alignment);
	}
d195 12
a206 54
	while (idx < end)
		uvm_pglist_add(&pgs[idx++], rlist);

#ifdef PGALLOC_VERBOSE
	printf("got %d pgs\n", num);
#endif
	return (num); /* number of pages allocated */
}

static int
uvm_pglistalloc_contig(num, low, high, alignment, boundary, rlist)
	int num;
	paddr_t low, high, alignment, boundary;
	struct pglist *rlist;
{
	int fl, psi;
	struct vm_physseg *ps;
	int s, error;

	/* Default to "lose". */
	error = ENOMEM;

	/*
	 * Block all memory allocation and lock the free list.
	 */
	s = uvm_lock_fpageq();

	/* Are there even any free pages? */
	if (uvmexp.free <= (uvmexp.reserve_pagedaemon + uvmexp.reserve_kernel))
		goto out;

	for (fl = 0; fl < VM_NFREELIST; fl++) {
#if (VM_PHYSSEG_STRAT == VM_PSTRAT_BIGFIRST)
		for (psi = vm_nphysseg - 1 ; psi >= 0 ; psi--)
#else
		for (psi = 0 ; psi < vm_nphysseg ; psi++)
#endif
		{
			ps = &vm_physmem[psi];

			if (ps->free_list != fl)
				continue;

			num -= uvm_pglistalloc_c_ps(ps, num, low, high,
						    alignment, boundary, rlist);
			if (num == 0) {
#ifdef PGALLOC_VERBOSE
				printf("pgalloc: %lx-%lx\n",
				       TAILQ_FIRST(rlist)->phys_addr,
				       TAILQ_LAST(rlist, pglist)->phys_addr);
#endif
				error = 0;
				goto out;
			}
d208 2
a209 83
	}

out:
	/*
	 * check to see if we need to generate some free pages waking
	 * the pagedaemon.
	 */

	UVM_KICK_PDAEMON();
	uvm_unlock_fpageq(s);
	return (error);
}

static int
uvm_pglistalloc_s_ps(ps, num, low, high, rlist)
	struct vm_physseg *ps;
	int num;
	paddr_t low, high;
	struct pglist *rlist;
{
	int todo, limit, try;
	struct vm_page *pg;
#ifdef DEBUG
	int cidx;
#endif
#ifdef PGALLOC_VERBOSE
	printf("pgalloc: simple %d pgs from psi %d\n", num, ps - vm_physmem);
#endif

	todo = num;
	limit = min(atop(high), ps->avail_end);

	for (try = max(atop(low), ps->avail_start);
	     try < limit; try ++) {
#ifdef DEBUG
		if (vm_physseg_find(try, &cidx) != ps - vm_physmem)
			panic("pgalloc simple: botch1");
		if (cidx != (try - ps->start))
			panic("pgalloc simple: botch2");
#endif
		pg = &ps->pgs[try - ps->start];
		if (VM_PAGE_IS_FREE(pg) == 0)
			continue;

		uvm_pglist_add(pg, rlist);
		if (--todo == 0)
			break;
	}

#ifdef PGALLOC_VERBOSE
	printf("got %d pgs\n", num - todo);
#endif
	return (num - todo); /* number of pages allocated */
}

static int
uvm_pglistalloc_simple(num, low, high, rlist, waitok)
	int num;
	paddr_t low, high;
	struct pglist *rlist;
	int waitok;
{
	int fl, psi, s, error;
	struct vm_physseg *ps;

	/* Default to "lose". */
	error = ENOMEM;

again:
	/*
	 * Block all memory allocation and lock the free list.
	 */
	s = uvm_lock_fpageq();

	/* Are there even any free pages? */
	if (uvmexp.free <= (uvmexp.reserve_pagedaemon + uvmexp.reserve_kernel))
		goto out;

	for (fl = 0; fl < VM_NFREELIST; fl++) {
#if (VM_PHYSSEG_STRAT == VM_PSTRAT_BIGFIRST)
		for (psi = vm_nphysseg - 1 ; psi >= 0 ; psi--)
#else
		for (psi = 0 ; psi < vm_nphysseg ; psi++)
d211 12
a222 13
		{
			ps = &vm_physmem[psi];

			if (ps->free_list != fl)
				continue;

			num -= uvm_pglistalloc_s_ps(ps, num, low, high, rlist);
			if (num == 0) {
				error = 0;
				goto out;
			}
		}

d224 1
a233 17
	if (error) {
		if (waitok) {
			/* XXX perhaps some time limitation? */
#ifdef DEBUG
			printf("pglistalloc waiting\n");
#endif
			uvm_wait("pglalloc");
			goto again;
		} else
			uvm_pglistfree(rlist);
	}
#ifdef PGALLOC_VERBOSE
	if (!error)
		printf("pgalloc: %lx..%lx\n",
		       TAILQ_FIRST(rlist)->phys_addr,
		       TAILQ_LAST(rlist, pglist)->phys_addr);
#endif
a234 33
}

int
uvm_pglistalloc(size, low, high, alignment, boundary, rlist, nsegs, waitok)
	psize_t size;
	paddr_t low, high, alignment, boundary;
	struct pglist *rlist;
	int nsegs, waitok;
{
	int num, res;

	KASSERT((alignment & (alignment - 1)) == 0);
	KASSERT((boundary & (boundary - 1)) == 0);

	/*
	 * Our allocations are always page granularity, so our alignment
	 * must be, too.
	 */
	if (alignment < PAGE_SIZE)
		alignment = PAGE_SIZE;
	if (boundary != 0 && boundary < size)
		return (EINVAL);
	num = atop(round_page(size));
	low = roundup(low, alignment);

	if ((nsegs < size >> PAGE_SHIFT) || (alignment != PAGE_SIZE) ||
	    (boundary != 0))
		res = uvm_pglistalloc_contig(num, low, high, alignment,
					     boundary, rlist);
	else
		res = uvm_pglistalloc_simple(num, low, high, rlist, waitok);

	return (res);
@


1.10
log
@Bring in more changes from NetBSD. Mostly pagedaemon improvements.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pglist.c,v 1.9 2001/11/07 02:55:50 art Exp $	*/
/*	$NetBSD: uvm_pglist.c,v 1.13 2001/02/18 21:19:08 chs Exp $	*/
d7 1
a7 1
 *  
d10 1
a10 1
 * NASA Ames Research Center.  
d17 1
a17 1
 * 2. Redistributions in binary form must reproduce the above copyright 
d27 1
a27 1
 *      
d81 1
a81 1
 *	boundary	no segment in the allocation may cross this 
d95 2
a96 2
	int s, tryidx, idx, pgflidx, end, error, free_list;
	vm_page_t m;
d99 1
a99 1
	vm_page_t tp;
d104 1
a104 1
	
d201 1
d205 1
a205 1
			free_list].pgfl_queues[pgflidx]);
d214 2
a215 2
		TAILQ_REMOVE(&uvm.page_free[free_list].pgfl_queues[pgflidx],
		    m, pageq);
d235 2
a236 6
	 
	if (uvmexp.free + uvmexp.paging < uvmexp.freemin ||
	    (uvmexp.free + uvmexp.paging < uvmexp.freetarg &&
	     uvmexp.inactive < uvmexp.inactarg)) {
		wakeup(&uvm.pagedaemon);
	}
d253 1
a253 1
	vm_page_t m;
d266 2
a267 2
		    uvm_page_lookup_freelist(m)].pgfl_queues[PGFL_UNKNOWN],
		    m, pageq);
@


1.9
log
@Another sync of uvm to NetBSD. Just minor fiddling, no major changes.
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pglist.c,v 1.8 2001/11/06 01:35:04 art Exp $	*/
/*	$NetBSD: uvm_pglist.c,v 1.12 2000/11/25 06:28:00 chs Exp $	*/
d102 2
a103 7
#ifdef DIAGNOSTIC
	if ((alignment & (alignment - 1)) != 0)
		panic("uvm_pglistalloc: alignment must be power of 2");

	if ((boundary & (boundary - 1)) != 0)
		panic("uvm_pglistalloc: boundary must be power of 2");
#endif
d264 2
a265 5
	while ((m = list->tqh_first) != NULL) {
#ifdef DIAGNOSTIC
		if (m->pqflags & (PQ_ACTIVE|PQ_INACTIVE))
			panic("uvm_pglistfree: active/inactive page!");
#endif
@


1.8
log
@Move the last content from vm/ to uvm/
The only thing left in vm/ are just dumb wrappers.
vm/vm.h includes uvm/uvm_extern.h
vm/pmap.h includes uvm/uvm_pmap.h
vm/vm_page.h includes uvm/uvm_page.h
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pglist.c,v 1.7 2001/08/11 10:57:22 art Exp $	*/
/*	$NetBSD: uvm_pglist.c,v 1.11 2000/06/27 17:29:34 mrg Exp $	*/
d131 1
a131 1
	s = uvm_lock_fpageq();		/* lock free page queue */
d134 1
a134 2
	if (uvmexp.free <= (uvmexp.reserve_pagedaemon +
	    uvmexp.reserve_kernel))
d139 1
d143 1
d163 1
a165 3
				/*
				 * Page not available.
				 */
a167 1

a168 1

d171 1
a172 1
				if ((lastidxpa + PAGE_SIZE) != idxpa) {
d176 1
d181 1
d185 1
a189 1

a190 3
			/*
			 * Woo hoo!  Found one.
			 */
d209 1
a209 1
		  free_list].pgfl_queues[pgflidx]);
d227 1
a227 2
		m->wire_count = 0;
		m->loan_count = 0;
a234 2
	uvm_unlock_fpageq(s);

a237 1
	 * XXX: we read uvm.free without locking
d240 3
a242 3
	if (uvmexp.free < uvmexp.freemin ||
	    (uvmexp.free < uvmexp.freetarg &&
	    uvmexp.inactive < uvmexp.inactarg)) 
d244 3
@


1.7
log
@Various random fixes from NetBSD.
Including support for zeroing pages in the idle loop (not enabled yet).
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pglist.c,v 1.6 2001/03/08 15:21:37 smart Exp $	*/
/*	$NetBSD: uvm_pglist.c,v 1.10 2000/05/20 19:54:01 thorpej Exp $	*/
a48 2

#include <vm/vm.h>
@


1.6
log
@Replace thread_wakeup() with wakeup().  It is defined in vm_extern.h as a
wrapper, so this removes a dependence on the old VM system.  From NetBSD.
art@@ ok
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pglist.c,v 1.5 2001/01/29 02:07:48 niklas Exp $	*/
/*	$NetBSD: uvm_pglist.c,v 1.8 1999/07/22 22:58:39 thorpej Exp $	*/
a3 2
#define VM_PAGE_ALLOC_MEMORY_STATS
 
a42 3
 *
 * XXX: was part of uvm_page but has an incompatable copyright so it
 * gets its own file now.
a50 2
#include <vm/vm_page.h>
#include <vm/vm_kern.h>
d97 1
a97 1
	int s, tryidx, idx, end, error, free_list;
d106 1
a106 1
		panic("vm_page_alloc_memory: alignment must be power of 2");
d109 1
a109 1
		panic("vm_page_alloc_memory: boundary must be power of 2");
d136 2
a137 4
	for (idx = 0; idx < VM_NFREELIST; idx++)
		if (uvm.page_free[idx].tqh_first != NULL)
			break;
	if (idx == VM_NFREELIST)
d201 4
d212 1
d214 4
a217 2
		for (tp = uvm.page_free[free_list].tqh_first;
		     tp != NULL; tp = tp->pageq.tqe_next) {
d224 2
a225 1
		TAILQ_REMOVE(&uvm.page_free[free_list], m, pageq);
d227 2
d283 2
a284 1
		TAILQ_INSERT_TAIL(&uvm.page_free[uvm_page_lookup_freelist(m)],
d287 2
@


1.5
log
@$OpenBSD$
@
text
@d1 2
a2 2
/*	$OpenBSD: uvm_pglist.c,v 1.7 1999/05/24 19:10:58 thorpej Exp $	*/
/*	$NetBSD: uvm_pglist.c,v 1.7 1999/05/24 19:10:58 thorpej Exp $	*/
d252 1
a252 1
		thread_wakeup(&uvm.pagedaemon);
@


1.4
log
@Bring in some new UVM code from NetBSD (not current).

 - Introduce a new type of map that are interrupt safe and never allow faults
   in them. mb_map and kmem_map are made intrsafe.
 - Add "access protection" to uvm_vslock (to be passed down to uvm_fault and
   later to pmap_enter).
 - madvise(2) now works.
 - various cleanups.
@
text
@d1 1
@


1.3
log
@sync with NetBSD from 1999.05.24 (there is a reason for this date)
 Mostly cleanups, but also a few improvements to pagedaemon for better
 handling of low memory and/or low swap conditions.
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_pglist.c,v 1.5.2.1 1998/07/30 14:04:15 eeh Exp $	*/
d139 1
a139 2
	s = splimp();
	uvm_lock_fpageq();            /* lock free page queue */
d240 1
a240 2
	uvm_unlock_fpageq();
	splx(s);
d272 1
a272 2
	s = splimp();
	uvm_lock_fpageq();
d287 1
a287 2
	uvm_unlock_fpageq();
	splx(s);
@


1.3.4.1
log
@Sync with -current
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_pglist.c,v 1.7 1999/05/24 19:10:58 thorpej Exp $	*/
d139 2
a140 1
	s = uvm_lock_fpageq();		/* lock free page queue */
d241 2
a242 1
	uvm_unlock_fpageq(s);
d274 2
a275 1
	s = uvm_lock_fpageq();
d290 2
a291 1
	uvm_unlock_fpageq(s);
@


1.3.4.2
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 2
/*	$OpenBSD: uvm_pglist.c,v 1.6 2001/03/08 15:21:37 smart Exp $	*/
/*	$NetBSD: uvm_pglist.c,v 1.8 1999/07/22 22:58:39 thorpej Exp $	*/
d251 1
a251 1
		wakeup(&uvm.pagedaemon);
@


1.3.4.3
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_pglist.c,v 1.10 2000/05/20 19:54:01 thorpej Exp $	*/
d4 2
d45 3
d56 2
d104 1
a104 1
	int s, tryidx, idx, pgflidx, end, error, free_list;
d113 1
a113 1
		panic("uvm_pglistalloc: alignment must be power of 2");
d116 1
a116 1
		panic("uvm_pglistalloc: boundary must be power of 2");
d143 4
a146 2
	if (uvmexp.free <= (uvmexp.reserve_pagedaemon +
	    uvmexp.reserve_kernel))
a209 4
#if PGFL_NQUEUES != 2
#error uvm_pglistalloc needs to be updated
#endif

a216 1
		pgflidx = (m->flags & PG_ZERO) ? PGFL_ZEROS : PGFL_UNKNOWN;
d218 2
a219 4
		for (tp = TAILQ_FIRST(&uvm.page_free[
		  free_list].pgfl_queues[pgflidx]);
		     tp != NULL;
		     tp = TAILQ_NEXT(tp, pageq)) {
d226 1
a226 2
		TAILQ_REMOVE(&uvm.page_free[free_list].pgfl_queues[pgflidx],
		    m, pageq);
a227 2
		if (m->flags & PG_ZERO)
			uvmexp.zeropages--;
d282 1
a282 2
		TAILQ_INSERT_TAIL(&uvm.page_free[
		    uvm_page_lookup_freelist(m)].pgfl_queues[PGFL_UNKNOWN],
a284 2
		if (uvmexp.zeropages < UVM_PAGEZERO_TARGET)
			uvm.page_idle_zero = vm_page_zero_enable;
@


1.3.4.4
log
@merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_pglist.c,v 1.13 2001/02/18 21:19:08 chs Exp $	*/
d50 2
d104 7
a110 2
	KASSERT((alignment & (alignment - 1)) == 0);
	KASSERT((boundary & (boundary - 1)) == 0);
d133 1
a133 1
	s = uvm_lock_fpageq();
d136 2
a137 1
	if (uvmexp.free <= (uvmexp.reserve_pagedaemon + uvmexp.reserve_kernel))
a141 1

a144 1

a163 1

d166 3
d171 1
d173 1
d176 1
a177 1

a180 1

a184 1

a187 1

d192 1
d194 3
d215 1
a215 1
			free_list].pgfl_queues[pgflidx]);
d233 2
a234 1
		m->version++;
d242 2
d247 1
d250 3
a252 3
	if (uvmexp.free + uvmexp.paging < uvmexp.freemin ||
	    (uvmexp.free + uvmexp.paging < uvmexp.freetarg &&
	     uvmexp.inactive < uvmexp.inactarg)) {
a253 3
	}

	uvm_unlock_fpageq(s);
d276 5
a280 2
	while ((m = TAILQ_FIRST(list)) != NULL) {
		KASSERT((m->pqflags & (PQ_ACTIVE|PQ_INACTIVE)) == 0);
@


1.3.4.5
log
@Merge in -current
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_pglist.c,v 1.17 2001/06/27 21:18:34 thorpej Exp $	*/
d7 1
a7 1
 *
d10 1
a10 1
 * NASA Ames Research Center.
d17 1
a17 1
 * 2. Redistributions in binary form must reproduce the above copyright
d27 1
a27 1
 *
d81 1
a81 1
 *	boundary	no segment in the allocation may cross this
d95 2
a96 2
	int s, tryidx, idx, pgflidx, end, error, free_list, color;
	struct vm_page *m;
d99 1
a99 1
	struct vm_page *tp;
d104 1
a104 1

a200 1
		color = VM_PGCOLOR_BUCKET(m);
d204 1
a204 1
			free_list].pgfl_buckets[color].pgfl_queues[pgflidx]);
d213 2
a214 2
		TAILQ_REMOVE(&uvm.page_free[free_list].pgfl_buckets[
		    color].pgfl_queues[pgflidx], m, pageq);
d234 6
a239 2

	UVM_KICK_PDAEMON();
d256 1
a256 1
	struct vm_page *m;
d269 2
a270 2
		    uvm_page_lookup_freelist(m)].pgfl_buckets[
		    VM_PGCOLOR_BUCKET(m)].pgfl_queues[PGFL_UNKNOWN], m, pageq);
@


1.3.4.6
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$NetBSD: uvm_pglist.c,v 1.13 2001/02/18 21:19:08 chs Exp $	*/
d7 1
a7 1
 *  
d10 1
a10 1
 * NASA Ames Research Center.  
d17 1
a17 1
 * 2. Redistributions in binary form must reproduce the above copyright 
d27 1
a27 1
 *      
d81 1
a81 1
 *	boundary	no segment in the allocation may cross this 
d95 2
a96 2
	int s, tryidx, idx, pgflidx, end, error, free_list;
	vm_page_t m;
d99 1
a99 1
	vm_page_t tp;
d104 1
a104 1
	
d201 1
d205 1
a205 1
			free_list].pgfl_queues[pgflidx]);
d214 2
a215 2
		TAILQ_REMOVE(&uvm.page_free[free_list].pgfl_queues[pgflidx],
		    m, pageq);
d235 2
a236 6
	 
	if (uvmexp.free + uvmexp.paging < uvmexp.freemin ||
	    (uvmexp.free + uvmexp.paging < uvmexp.freetarg &&
	     uvmexp.inactive < uvmexp.inactarg)) {
		wakeup(&uvm.pagedaemon);
	}
d253 1
a253 1
	vm_page_t m;
d266 2
a267 2
		    uvm_page_lookup_freelist(m)].pgfl_queues[PGFL_UNKNOWN],
		    m, pageq);
@


1.2
log
@add OpenBSD tags
@
text
@d1 1
a1 2
/*	$OpenBSD$	*/
/*	$NetBSD: uvm_pglist.c,v 1.6 1998/08/13 02:11:03 eeh Exp $	*/
@


1.1
log
@Import of uvm from NetBSD. Some local changes, some code disabled
@
text
@d1 1
@

