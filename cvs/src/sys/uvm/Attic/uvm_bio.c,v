head	1.6;
access;
symbols
	SMP_SYNC_A:1.6
	SMP_SYNC_B:1.6
	UBC_SYNC_A:1.6
	UBC_SYNC_B:1.6
	UBC:1.5.0.2
	UBC_BASE:1.5
	SMP:1.4.0.2;
locks; strict;
comment	@ * @;


1.6
date	2001.12.19.08.58.07;	author art;	state dead;
branches;
next	1.5;

1.5
date	2001.12.10.02.19.34;	author art;	state Exp;
branches
	1.5.2.1;
next	1.4;

1.4
date	2001.12.04.23.22.42;	author art;	state Exp;
branches
	1.4.2.1;
next	1.3;

1.3
date	2001.11.28.19.28.14;	author art;	state Exp;
branches;
next	1.2;

1.2
date	2001.11.28.13.47.39;	author art;	state Exp;
branches;
next	1.1;

1.1
date	2001.11.27.05.27.12;	author art;	state Exp;
branches;
next	;

1.4.2.1
date	2001.12.09.16.26.06;	author mickey;	state Exp;
branches;
next	1.4.2.2;

1.4.2.2
date	2002.03.06.02.17.14;	author niklas;	state dead;
branches;
next	;

1.5.2.1
date	2002.02.02.03.28.26;	author art;	state Exp;
branches;
next	1.5.2.2;

1.5.2.2
date	2002.06.11.03.33.03;	author art;	state Exp;
branches;
next	1.5.2.3;

1.5.2.3
date	2002.11.04.18.02.32;	author art;	state Exp;
branches;
next	;


desc
@@


1.6
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@/*	$NetBSD: uvm_bio.c,v 1.17 2001/09/10 21:19:43 chris Exp $	*/

/*
 * Copyright (c) 1998 Chuck Silvers.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. The name of the author may not be used to endorse or promote products
 *    derived from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
 * BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED
 * AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 */

/*
 * uvm_bio.c: buffered i/o vnode mapping cache
 */


#include <sys/param.h>
#include <sys/systm.h>
#include <sys/malloc.h>
#include <sys/kernel.h>
#include <sys/vnode.h>

#include <uvm/uvm.h>
#include <uvm/uvm_page.h>

/*
 * global data structures
 */

/*
 * local functions
 */

static int	ubc_fault __P((struct uvm_faultinfo *, vaddr_t,
		    struct vm_page **, int, int, vm_fault_t, vm_prot_t, int));
static struct ubc_map *ubc_find_mapping __P((struct uvm_object *, voff_t));

/*
 * local data structues
 */

#define UBC_HASH(uobj, offset) (((((u_long)(uobj)) >> 8) + \
				 (((u_long)(offset)) >> PAGE_SHIFT)) & \
				ubc_object.hashmask)

#define UBC_QUEUE(offset) (&ubc_object.inactive[((offset) >> ubc_winshift) & \
					       (UBC_NQUEUES - 1)])

struct ubc_map
{
	struct uvm_object *	uobj;		/* mapped object */
	voff_t			offset;		/* offset into uobj */
	int			refcount;	/* refcount on mapping */
	voff_t			writeoff;	/* overwrite offset */
	vsize_t			writelen;	/* overwrite len */

	LIST_ENTRY(ubc_map)	hash;		/* hash table */
	TAILQ_ENTRY(ubc_map)	inactive;	/* inactive queue */
};

static struct ubc_object
{
	struct uvm_object uobj;		/* glue for uvm_map() */
	char *kva;			/* where ubc_object is mapped */
	struct ubc_map *umap;		/* array of ubc_map's */

	LIST_HEAD(, ubc_map) *hash;	/* hashtable for cached ubc_map's */
	u_long hashmask;		/* mask for hashtable */

	TAILQ_HEAD(ubc_inactive_head, ubc_map) *inactive;
					/* inactive queues for ubc_map's */

} ubc_object;

struct uvm_pagerops ubc_pager =
{
	NULL,		/* init */
	NULL,		/* reference */
	NULL,		/* detach */
	ubc_fault,	/* fault */
	/* ... rest are NULL */
};

int ubc_nwins = UBC_NWINS;
int ubc_winshift = UBC_WINSHIFT;
int ubc_winsize;
#ifdef PMAP_PREFER
int ubc_nqueues;
boolean_t ubc_release_unmap = FALSE;
#define UBC_NQUEUES ubc_nqueues
#define UBC_RELEASE_UNMAP ubc_release_unmap
#else
#define UBC_NQUEUES 1
#define UBC_RELEASE_UNMAP FALSE
#endif

/*
 * ubc_init
 *
 * init pager private data structures.
 */

void
ubc_init(void)
{
	struct ubc_map *umap;
	vaddr_t va;
	int i;

	/*
	 * Make sure ubc_winshift is sane.
	 */
	if (ubc_winshift < PAGE_SHIFT)
		ubc_winshift = PAGE_SHIFT;

	/*
	 * init ubc_object.
	 * alloc and init ubc_map's.
	 * init inactive queues.
	 * alloc and init hashtable.
	 * map in ubc_object.
	 */

	simple_lock_init(&ubc_object.uobj.vmobjlock);
	ubc_object.uobj.pgops = &ubc_pager;
	TAILQ_INIT(&ubc_object.uobj.memq);
	ubc_object.uobj.uo_npages = 0;
	ubc_object.uobj.uo_refs = UVM_OBJ_KERN;

	ubc_object.umap = malloc(ubc_nwins * sizeof(struct ubc_map),
				 M_TEMP, M_NOWAIT);
	if (ubc_object.umap == NULL)
		panic("ubc_init: failed to allocate ubc_map");
	memset(ubc_object.umap, 0, ubc_nwins * sizeof(struct ubc_map));

	va = (vaddr_t)1L;
#ifdef PMAP_PREFER
	PMAP_PREFER(0, &va);
	ubc_nqueues = va >> ubc_winshift;
	if (ubc_nqueues == 0) {
		ubc_nqueues = 1;
	}
	if (ubc_nqueues != 1) {
		ubc_release_unmap = TRUE;
	}
#endif
	ubc_winsize = 1 << ubc_winshift;
	ubc_object.inactive = malloc(UBC_NQUEUES *
				     sizeof(struct ubc_inactive_head),
				     M_TEMP, M_NOWAIT);
	if (ubc_object.inactive == NULL)
		panic("ubc_init: failed to allocate inactive queue heads");
	for (i = 0; i < UBC_NQUEUES; i++) {
		TAILQ_INIT(&ubc_object.inactive[i]);
	}
	for (i = 0; i < ubc_nwins; i++) {
		umap = &ubc_object.umap[i];
		TAILQ_INSERT_TAIL(&ubc_object.inactive[i & (UBC_NQUEUES - 1)],
				  umap, inactive);
	}

	ubc_object.hash = hashinit(ubc_nwins, M_TEMP, M_NOWAIT,
				   &ubc_object.hashmask);
	for (i = 0; i <= ubc_object.hashmask; i++) {
		LIST_INIT(&ubc_object.hash[i]);
	}

	if (uvm_map(kernel_map, (vaddr_t *)&ubc_object.kva,
		    ubc_nwins << ubc_winshift, &ubc_object.uobj, 0, (vsize_t)va,
		    UVM_MAPFLAG(UVM_PROT_ALL, UVM_PROT_ALL, UVM_INH_NONE,
				UVM_ADV_RANDOM, UVM_FLAG_NOMERGE)) != 0) {
		panic("ubc_init: failed to map ubc_object\n");
	}
	UVMHIST_INIT(ubchist, 300);
}


/*
 * ubc_fault: fault routine for ubc mapping
 */
int
ubc_fault(ufi, ign1, ign2, ign3, ign4, fault_type, access_type, flags)
	struct uvm_faultinfo *ufi;
	vaddr_t ign1;
	struct vm_page **ign2;
	int ign3, ign4;
	vm_fault_t fault_type;
	vm_prot_t access_type;
	int flags;
{
	struct uvm_object *uobj;
	struct vnode *vp;
	struct ubc_map *umap;
	vaddr_t va, eva, ubc_offset, slot_offset;
	int i, error, rv, npages;
	struct vm_page *pgs[(1 << ubc_winshift) >> PAGE_SHIFT], *pg;
	UVMHIST_FUNC("ubc_fault");  UVMHIST_CALLED(ubchist);

	/*
	 * no need to try with PGO_LOCKED...
	 * we don't need to have the map locked since we know that
	 * no one will mess with it until our reference is released.
	 */
	if (flags & PGO_LOCKED) {
#if 0
		return EBUSY;
#else
		uvmfault_unlockall(ufi, NULL, &ubc_object.uobj, NULL);
		flags &= ~PGO_LOCKED;
#endif
	}

	va = ufi->orig_rvaddr;
	ubc_offset = va - (vaddr_t)ubc_object.kva;

	UVMHIST_LOG(ubchist, "va 0x%lx ubc_offset 0x%lx at %d",
		    va, ubc_offset, access_type,0);

	umap = &ubc_object.umap[ubc_offset >> ubc_winshift];
	KASSERT(umap->refcount != 0);
	slot_offset = trunc_page(ubc_offset & (ubc_winsize - 1));

	/* no umap locking needed since we have a ref on the umap */
	uobj = umap->uobj;
	vp = (struct vnode *)uobj;
	KASSERT(uobj != NULL);

	npages = (ubc_winsize - slot_offset) >> PAGE_SHIFT;

	/*
	 * XXXUBC
	 * if npages is more than 1 we have to be sure that
	 * we set PGO_OVERWRITE correctly.
	 */
	if (access_type == VM_PROT_WRITE) {
		npages = 1;
	}

again:
	memset(pgs, 0, sizeof (pgs));
	simple_lock(&uobj->vmobjlock);

	UVMHIST_LOG(ubchist, "slot_offset 0x%x writeoff 0x%x writelen 0x%x "
		    "u_size 0x%x", slot_offset, umap->writeoff, umap->writelen,
		    vp->v_size);

	if (access_type & VM_PROT_WRITE &&
	    slot_offset >= umap->writeoff &&
	    (slot_offset + PAGE_SIZE <= umap->writeoff + umap->writelen ||
	     slot_offset + PAGE_SIZE >= vp->v_size - umap->offset)) {
		UVMHIST_LOG(ubchist, "setting PGO_OVERWRITE", 0,0,0,0);
		flags |= PGO_OVERWRITE;
	}
	else { UVMHIST_LOG(ubchist, "NOT setting PGO_OVERWRITE", 0,0,0,0); }
	/* XXX be sure to zero any part of the page past EOF */

	/*
	 * XXX
	 * ideally we'd like to pre-fault all of the pages we're overwriting.
	 * so for PGO_OVERWRITE, we should call VOP_GETPAGES() with all of the
	 * pages in [writeoff, writeoff+writesize] instead of just the one.
	 */

	UVMHIST_LOG(ubchist, "getpages vp %p offset 0x%x npages %d",
		    uobj, umap->offset + slot_offset, npages, 0);

	error = VOP_GETPAGES(vp, umap->offset + slot_offset, pgs, &npages, 0,
	    access_type, 0, flags);
	UVMHIST_LOG(ubchist, "getpages error %d npages %d", error, npages,0,0);

	if (error == EAGAIN) {
		tsleep(&lbolt, PVM, "ubc_fault", 0);
		goto again;
	}
	if (error) {
		return error;
	}
	if (npages == 0) {
		return 0;
	}

	va = ufi->orig_rvaddr;
	eva = ufi->orig_rvaddr + (npages << PAGE_SHIFT);

	UVMHIST_LOG(ubchist, "va 0x%lx eva 0x%lx", va, eva, 0,0);
	simple_lock(&uobj->vmobjlock);
	for (i = 0; va < eva; i++, va += PAGE_SIZE) {
		UVMHIST_LOG(ubchist, "pgs[%d] = %p", i, pgs[i],0,0);
		pg = pgs[i];

		if (pg == NULL || pg == PGO_DONTCARE) {
			continue;
		}
		if (pg->flags & PG_WANTED) {
			wakeup(pg);
		}
		KASSERT((pg->flags & PG_FAKE) == 0);
		if (pg->flags & PG_RELEASED) {
			rv = uobj->pgops->pgo_releasepg(pg, NULL);
			KASSERT(rv);
			continue;
		}
		KASSERT(access_type == VM_PROT_READ ||
			(pg->flags & PG_RDONLY) == 0);

		uvm_lock_pageq();
		uvm_pageactivate(pg);
		uvm_unlock_pageq();

		pmap_enter(ufi->orig_map->pmap, va, VM_PAGE_TO_PHYS(pg),
			   VM_PROT_READ | VM_PROT_WRITE, access_type);

		pg->flags &= ~(PG_BUSY);
		UVM_PAGE_OWN(pg, NULL);
	}
	simple_unlock(&uobj->vmobjlock);
	pmap_update(ufi->orig_map->pmap);
	return 0;
}

/*
 * local functions
 */

struct ubc_map *
ubc_find_mapping(uobj, offset)
	struct uvm_object *uobj;
	voff_t offset;
{
	struct ubc_map *umap;

	LIST_FOREACH(umap, &ubc_object.hash[UBC_HASH(uobj, offset)], hash) {
		if (umap->uobj == uobj && umap->offset == offset) {
			return umap;
		}
	}
	return NULL;
}


/*
 * ubc interface functions
 */

/*
 * ubc_alloc:  allocate a buffer mapping
 */
void *
ubc_alloc(uobj, offset, lenp, flags)
	struct uvm_object *uobj;
	voff_t offset;
	vsize_t *lenp;
	int flags;
{
	int s;
	vaddr_t slot_offset, va;
	struct ubc_map *umap;
	voff_t umap_offset;
	UVMHIST_FUNC("ubc_alloc"); UVMHIST_CALLED(ubchist);

	UVMHIST_LOG(ubchist, "uobj %p offset 0x%lx len 0x%lx filesize 0x%x",
		    uobj, offset, *lenp, ((struct vnode *)vp)->v_size);

	umap_offset = (offset & ~((voff_t)ubc_winsize - 1));
	slot_offset = (vaddr_t)(offset & ((voff_t)ubc_winsize - 1));
	*lenp = min(*lenp, ubc_winsize - slot_offset);

	/*
	 * the vnode is always locked here, so we don't need to add a ref.
	 */

	s = splbio();

again:
	simple_lock(&ubc_object.uobj.vmobjlock);
	umap = ubc_find_mapping(uobj, umap_offset);
	if (umap == NULL) {
		umap = TAILQ_FIRST(UBC_QUEUE(offset));
		if (umap == NULL) {
			simple_unlock(&ubc_object.uobj.vmobjlock);
			tsleep(&lbolt, PVM, "ubc_alloc", 0);
			goto again;
		}

		/*
		 * remove from old hash (if any),
		 * add to new hash.
		 */

		if (umap->uobj != NULL) {
			LIST_REMOVE(umap, hash);
		}

		umap->uobj = uobj;
		umap->offset = umap_offset;

		LIST_INSERT_HEAD(&ubc_object.hash[UBC_HASH(uobj, umap_offset)],
				 umap, hash);

		va = (vaddr_t)(ubc_object.kva +
			       ((umap - ubc_object.umap) << ubc_winshift));
		pmap_remove(pmap_kernel(), va, va + ubc_winsize);
		pmap_update(pmap_kernel());
	}

	if (umap->refcount == 0) {
		TAILQ_REMOVE(UBC_QUEUE(offset), umap, inactive);
	}

#ifdef DIAGNOSTIC
	if ((flags & UBC_WRITE) &&
	    (umap->writeoff || umap->writelen)) {
		panic("ubc_fault: concurrent writes vp %p", uobj);
	}
#endif
	if (flags & UBC_WRITE) {
		umap->writeoff = slot_offset;
		umap->writelen = *lenp;
	}

	umap->refcount++;
	simple_unlock(&ubc_object.uobj.vmobjlock);
	splx(s);
	UVMHIST_LOG(ubchist, "umap %p refs %d va %p",
		    umap, umap->refcount,
		    ubc_object.kva + ((umap - ubc_object.umap) << ubc_winshift),
		    0);

	return ubc_object.kva +
		((umap - ubc_object.umap) << ubc_winshift) + slot_offset;
}


void
ubc_release(va, wlen)
	void *va;
	vsize_t wlen;
{
	struct ubc_map *umap;
	struct uvm_object *uobj;
	int s;
	UVMHIST_FUNC("ubc_release"); UVMHIST_CALLED(ubchist);

	UVMHIST_LOG(ubchist, "va %p", va,0,0,0);

	s = splbio();
	simple_lock(&ubc_object.uobj.vmobjlock);

	umap = &ubc_object.umap[((char *)va - ubc_object.kva) >> ubc_winshift];
	uobj = umap->uobj;
	KASSERT(uobj != NULL);

	umap->writeoff = 0;
	umap->writelen = 0;
	umap->refcount--;
	if (umap->refcount == 0) {
		if (UBC_RELEASE_UNMAP &&
		    (((struct vnode *)uobj)->v_flag & VTEXT)) {
			vaddr_t va;

			/*
			 * if this file is the executable image of
			 * some process, that process will likely have
			 * the file mapped at an alignment other than
			 * what PMAP_PREFER() would like.  we'd like
			 * to have process text be able to use the
			 * cache even if someone is also reading the
			 * file, so invalidate mappings of such files
			 * as soon as possible.
			 */

			va = (vaddr_t)(ubc_object.kva +
			    ((umap - ubc_object.umap) << ubc_winshift));
			pmap_remove(pmap_kernel(), va, va + ubc_winsize);
			pmap_update(pmap_kernel());
			LIST_REMOVE(umap, hash);
			umap->uobj = NULL;
			TAILQ_INSERT_HEAD(UBC_QUEUE(umap->offset), umap,
			    inactive);
		} else {
			TAILQ_INSERT_TAIL(UBC_QUEUE(umap->offset), umap,
			    inactive);
		}
	}
	UVMHIST_LOG(ubchist, "umap %p refs %d", umap, umap->refcount,0,0);
	simple_unlock(&ubc_object.uobj.vmobjlock);
	splx(s);
}


/*
 * removing a range of mappings from the ubc mapping cache.
 */

void
ubc_flush(uobj, start, end)
	struct uvm_object *uobj;
	voff_t start, end;
{
	struct ubc_map *umap;
	vaddr_t va;
	int s;
	UVMHIST_FUNC("ubc_flush");  UVMHIST_CALLED(ubchist);

	UVMHIST_LOG(ubchist, "uobj %p start 0x%lx end 0x%lx",
		    uobj, start, end,0);

	s = splbio();
	simple_lock(&ubc_object.uobj.vmobjlock);
	for (umap = ubc_object.umap;
	     umap < &ubc_object.umap[ubc_nwins];
	     umap++) {

		if (umap->uobj != uobj ||
		    umap->offset < start ||
		    (umap->offset >= end && end != 0) ||
		    umap->refcount > 0) {
			continue;
		}

		/*
		 * remove from hash,
		 * move to head of inactive queue.
		 */

		va = (vaddr_t)(ubc_object.kva +
			       ((umap - ubc_object.umap) << ubc_winshift));
		pmap_remove(pmap_kernel(), va, va + ubc_winsize);
		pmap_update(pmap_kernel());

		LIST_REMOVE(umap, hash);
		umap->uobj = NULL;
		TAILQ_REMOVE(UBC_QUEUE(umap->offset), umap, inactive);
		TAILQ_INSERT_HEAD(UBC_QUEUE(umap->offset), umap, inactive);
	}
	simple_unlock(&ubc_object.uobj.vmobjlock);
	splx(s);
}
@


1.5
log
@Merge in struct uvm_vnode into struct vnode.
@
text
@@


1.5.2.1
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_bio.c,v 1.22 2001/11/19 02:44:27 enami Exp $	*/
a41 1
#include <sys/proc.h>
d44 1
d54 3
a56 3
int	ubc_fault __P((struct uvm_faultinfo *, vaddr_t, struct vm_page **, int,
    int, vm_fault_t, vm_prot_t, int));
struct ubc_map *ubc_find_mapping __P((struct uvm_object *, voff_t));
d62 2
a63 2
#define UBC_HASH(uobj, offset) 						\
	(((((u_long)(uobj)) >> 8) + (((u_long)(offset)) >> PAGE_SHIFT)) & \
d66 2
a67 10
#define UBC_QUEUE(offset)						\
	(&ubc_object.inactive[(((u_long)(offset)) >> ubc_winshift) &	\
			     (UBC_NQUEUES - 1)])

#define UBC_UMAP_ADDR(u)						\
	(vaddr_t)(ubc_object.kva + (((u) - ubc_object.umap) << ubc_winshift))


#define UMAP_PAGES_LOCKED	0x0001
#define UMAP_MAPPING_CACHED	0x0002
d73 1
a75 2
	int			refcount;	/* refcount on mapping */
	int			flags;		/* extra state */
a155 3
	if (ubc_winshift < PAGE_SHIFT) {
		ubc_winshift = PAGE_SHIFT;
	}
d169 2
a170 1
	    sizeof(struct ubc_inactive_head), M_TEMP, M_NOWAIT);
d197 1
a200 1

d215 2
a216 2
	int i, error, npages;
	struct vm_page *pgs[ubc_winsize >> PAGE_SHIFT], *pg;
a223 1

d225 3
d230 1
d237 1
a237 1
	    va, ubc_offset, access_type,0);
d241 1
a241 1
	slot_offset = ubc_offset & (ubc_winsize - 1);
d246 1
a246 1
	KASSERT(vp != NULL);
d248 10
a257 4
	npages = MIN(ubc_winsize - slot_offset,
		     (round_page(MAX(vp->v_size, umap->offset +
				     umap->writeoff + umap->writelen)) -
		      umap->offset)) >> PAGE_SHIFT;
d264 20
a283 2
	    "v_size 0x%x", slot_offset, umap->writeoff, umap->writelen,
	    vp->v_size);
d285 1
a285 1
	    uobj, umap->offset + slot_offset, npages, 0);
a286 1
	flags |= PGO_PASTEOF;
d298 3
a306 1
	uvm_lock_pageq();
d319 2
a320 1
			uvm_pagefree(pg);
d324 6
a329 1
		    (pg->flags & PG_RDONLY) == 0);
d331 2
a332 2
		    VM_PROT_READ | VM_PROT_WRITE, access_type);
		uvm_pageactivate(pg);
a335 1
	uvm_unlock_pageq();
d366 1
a366 1
 * ubc_alloc:  allocate a file mapping window
a367 1

d375 1
a375 1
	struct vnode *vp = (struct vnode *)uobj;
a378 1
	int error;
d382 1
a382 1
	    uobj, offset, *lenp, vp->v_size);
d386 1
a386 1
	*lenp = MIN(*lenp, ubc_winsize - slot_offset);
d392 2
d406 2
a407 1
		 * remove from old hash (if any), add to new hash.
d413 1
d416 1
d418 6
a423 9
		    umap, hash);
		va = UBC_UMAP_ADDR(umap);
		if (umap->flags & UMAP_MAPPING_CACHED) {
			umap->flags &= ~UMAP_MAPPING_CACHED;
			pmap_remove(pmap_kernel(), va, va + ubc_winsize);
			pmap_update(pmap_kernel());
		}
	} else {
		va = UBC_UMAP_ADDR(umap);
d431 2
a432 1
	if ((flags & UBC_WRITE) && (umap->writeoff || umap->writelen)) {
d443 5
a447 8
	UVMHIST_LOG(ubchist, "umap %p refs %d va %p flags 0x%x",
	    umap, umap->refcount, va, flags);

	if (flags & UBC_FAULTBUSY) {
		int npages = (*lenp + PAGE_SIZE - 1) >> PAGE_SHIFT;
		struct vm_page *pgs[npages];
		int gpflags = PGO_SYNCIO|PGO_OVERWRITE|PGO_PASTEOF;
		int i;
d449 2
a450 23
		if (umap->flags & UMAP_MAPPING_CACHED) {
			umap->flags &= ~UMAP_MAPPING_CACHED;
			pmap_remove(pmap_kernel(), va, va + ubc_winsize);
		}
		memset(pgs, 0, sizeof(pgs));
		simple_lock(&uobj->vmobjlock);
		error = VOP_GETPAGES(vp, trunc_page(offset), pgs, &npages, 0,
		    VM_PROT_READ|VM_PROT_WRITE, 0, gpflags);
		UVMHIST_LOG(ubchist, "faultbusy getpages %d", error,0,0,0);
		if (error) {
			goto out;
		}
		for (i = 0; i < npages; i++) {
			pmap_kenter_pa(va + slot_offset + (i << PAGE_SHIFT),
			    VM_PAGE_TO_PHYS(pgs[i]),
			    VM_PROT_READ | VM_PROT_WRITE);
		}
		pmap_update(pmap_kernel());
		umap->flags |= UMAP_PAGES_LOCKED;
	}

out:
	return (void *)(va + slot_offset);
a452 3
/*
 * ubc_release:  free a file mapping window.
 */
d455 1
a455 1
ubc_release(va, flags)
d457 1
a457 1
	int flags;
d461 1
a461 2
	vaddr_t umapva;
	boolean_t unmapped;
d465 4
a469 1
	umapva = UBC_UMAP_ADDR(umap);
a472 34
	if (umap->flags & UMAP_PAGES_LOCKED) {
		int slot_offset = umap->writeoff;
		int endoff = umap->writeoff + umap->writelen;
		int zerolen = round_page(endoff) - endoff;
		int npages = (int)(round_page(umap->writeoff + umap->writelen)
				   - trunc_page(umap->writeoff)) >> PAGE_SHIFT;
		struct vm_page *pgs[npages];
		paddr_t pa;
		int i;
		boolean_t rv;

		if (zerolen) {
			memset((char *)umapva + endoff, 0, zerolen);
		}
		umap->flags &= ~UMAP_PAGES_LOCKED;
		uvm_lock_pageq();
		for (i = 0; i < npages; i++) {
			rv = pmap_extract(pmap_kernel(),
			    umapva + slot_offset + (i << PAGE_SHIFT), &pa);
			KASSERT(rv);
			pgs[i] = PHYS_TO_VM_PAGE(pa);
			pgs[i]->flags &= ~(PG_FAKE|PG_CLEAN);
			uvm_pageactivate(pgs[i]);
		}
		uvm_unlock_pageq();
		pmap_kremove(umapva, ubc_winsize);
		pmap_update(pmap_kernel());
		uvm_page_unbusy(pgs, npages);
		unmapped = TRUE;
	} else {
		unmapped = FALSE;
	}

	simple_lock(&ubc_object.uobj.vmobjlock);
d479 1
d492 3
a494 3
			pmap_remove(pmap_kernel(), umapva,
				    umapva + ubc_winsize);
			umap->flags &= ~UMAP_MAPPING_CACHED;
a500 3
			if (!unmapped) {
				umap->flags |= UMAP_MAPPING_CACHED;
			}
d507 1
d522 1
d528 1
d534 2
a535 1
		if (umap->uobj != uobj || umap->offset < start ||
d547 1
a547 1
		    ((umap - ubc_object.umap) << ubc_winshift));
d549 1
a555 1
	pmap_update(pmap_kernel());
d557 1
@


1.5.2.2
log
@Sync UBC branch to -current
@
text
@d54 3
a56 3
int	ubc_fault(struct uvm_faultinfo *, vaddr_t, struct vm_page **, int,
    int, vm_fault_t, vm_prot_t, int);
struct ubc_map *ubc_find_mapping(struct uvm_object *, voff_t);
@


1.5.2.3
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_bio.c,v 1.26 2002/09/27 15:38:08 provos Exp $	*/
d203 1
a203 1
		panic("ubc_init: failed to map ubc_object");
a227 1
	vm_prot_t prot;
d245 1
a245 1
	    va, ubc_offset, access_type, 0);
d274 1
a274 2
	UVMHIST_LOG(ubchist, "getpages error %d npages %d", error, npages, 0,
	    0);
d287 1
a287 13
	/*
	 * for virtually-indexed, virtually-tagged caches we should avoid
	 * creating writable mappings when we don't absolutely need them,
	 * since the "compatible alias" trick doesn't work on such caches.
	 * otherwise, we can always map the pages writable.
	 */

#ifdef PMAP_CACHE_VIVT
	prot = VM_PROT_READ | access_type;
#else
	prot = VM_PROT_READ | VM_PROT_WRITE;
#endif
	UVMHIST_LOG(ubchist, "va 0x%lx eva 0x%lx", va, eva, 0, 0);
d291 1
a291 1
		UVMHIST_LOG(ubchist, "pgs[%d] = %p", i, pgs[i], 0, 0);
d308 1
a308 2
		    (pg->flags & PG_RDONLY) ? prot & ~VM_PROT_WRITE : prot,
		    access_type);
d437 1
a437 1
		UVMHIST_LOG(ubchist, "faultbusy getpages %d", error, 0, 0, 0);
d469 1
a469 1
	UVMHIST_LOG(ubchist, "va %p", va, 0, 0, 0);
d543 1
a543 1
	UVMHIST_LOG(ubchist, "umap %p refs %d", umap, umap->refcount, 0, 0);
d562 1
a562 1
		    uobj, start, end, 0);
@


1.4
log
@Yet another sync to NetBSD uvm.
Today we add a pmap argument to pmap_update() and allocate map entries for
kernel_map from kmem_map instead of using the static entries. This should
get rid of MAX_KMAPENT panics. Also some uvm_loan problems are fixed.
@
text
@d265 1
a265 1
		    vp->v_uvm.u_size);
d270 1
a270 1
	     slot_offset + PAGE_SIZE >= vp->v_uvm.u_size - umap->offset)) {
d382 1
a382 1
		    uobj, offset, *lenp, ((struct uvm_vnode *)uobj)->u_size);
@


1.4.2.1
log
@bad file, make this compile
@
text
@d337 1
a337 1
	pmap_update();
d423 1
a423 1
		pmap_update();
d495 1
a495 1
			pmap_update();
d549 1
a549 1
		pmap_update();
@


1.4.2.2
log
@Merge in trunk
@
text
@@


1.3
log
@Sync in more uvm from NetBSD. Mostly just cosmetic stuff.
Contains also support for page coloring.
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_bio.c,v 1.16 2001/07/18 16:44:39 thorpej Exp $	*/
d337 1
a337 1
	pmap_update();
d423 1
a423 1
		pmap_update();
d495 1
a495 1
			pmap_update();
d549 1
a549 1
		pmap_update();
@


1.2
log
@Sync in more uvm changes from NetBSD.
This time we're getting rid of KERN_* and VM_PAGER_* error codes and
use errnos instead.
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_bio.c,v 1.11 2001/03/19 00:29:04 chs Exp $	*/
d3 1
a3 1
/* 
d54 2
a55 3
static int	ubc_fault __P((struct uvm_faultinfo *, vaddr_t, 
			       vm_page_t *, int, int, vm_fault_t, vm_prot_t,
			       int));
d131 6
d154 1
a154 1
	bzero(ubc_object.umap, ubc_nwins * sizeof(struct ubc_map));
d205 1
a205 1
	vm_page_t *ign2;
d337 1
d423 1
d495 1
d528 1
a528 1
	s = splbio(); 
d534 1
a534 1
		if (umap->uobj != uobj || 
d549 1
@


1.1
log
@Merge in the unified buffer cache code as found in NetBSD 2001/03/10. The
code is written mostly by Chuck Silvers <chuq@@chuq.com>/<chs@@netbsd.org>.

Tested for the past few weeks by many developers, should be in a pretty stable
state, but will require optimizations and additional cleanups.
@
text
@d1 1
a1 1
/*	$NetBSD: uvm_bio.c,v 1.7 2001/02/02 01:55:52 enami Exp $	*/
d67 1
a67 1
#define UBC_QUEUE(offset) (&ubc_object.inactive[((offset) / ubc_winsize) & \
d106 2
a107 1
int ubc_winsize = UBC_WINSIZE;
d154 3
a156 2
	if (va < ubc_winsize) {
		va = ubc_winsize;
a157 1
	ubc_nqueues = va / ubc_winsize;
d162 1
d184 1
a184 1
		    ubc_nwins * ubc_winsize, &ubc_object.uobj, 0, (vsize_t)va,
d186 1
a186 2
				UVM_ADV_RANDOM, UVM_FLAG_NOMERGE))
	    != KERN_SUCCESS) {
d196 1
a196 1
static int
d211 1
a211 1
	struct vm_page *pgs[ubc_winsize >> PAGE_SHIFT], *pg;
d221 1
a221 1
		return VM_PAGER_UNLOCK;
d234 1
a234 1
	umap = &ubc_object.umap[ubc_offset / ubc_winsize];
d291 1
a291 1
		return VM_PAGER_ERROR;
d294 1
a294 1
		return VM_PAGER_OK;
d332 1
a332 1
	return VM_PAGER_OK;
d339 1
a339 1
static struct ubc_map *
d415 1
a415 1
			       (umap - ubc_object.umap) * ubc_winsize);
d439 2
a440 1
		    ubc_object.kva + (umap - ubc_object.umap) * ubc_winsize,0);
d443 1
a443 1
		(umap - ubc_object.umap) * ubc_winsize + slot_offset;
d462 1
a462 1
	umap = &ubc_object.umap[((char *)va - ubc_object.kva) / ubc_winsize];
d486 1
a486 1
			    (umap - ubc_object.umap) * ubc_winsize);
d539 1
a539 1
			       (umap - ubc_object.umap) * ubc_winsize);
@

