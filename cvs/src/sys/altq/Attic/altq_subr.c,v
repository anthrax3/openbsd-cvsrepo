head	1.30;
access;
symbols
	OPENBSD_5_5:1.29.0.8
	OPENBSD_5_5_BASE:1.29
	OPENBSD_5_4:1.29.0.4
	OPENBSD_5_4_BASE:1.29
	OPENBSD_5_3:1.29.0.2
	OPENBSD_5_3_BASE:1.29
	OPENBSD_5_2:1.28.0.4
	OPENBSD_5_2_BASE:1.28
	OPENBSD_5_1_BASE:1.28
	OPENBSD_5_1:1.28.0.2
	OPENBSD_5_0:1.27.0.2
	OPENBSD_5_0_BASE:1.27
	OPENBSD_4_9:1.26.0.12
	OPENBSD_4_9_BASE:1.26
	OPENBSD_4_8:1.26.0.10
	OPENBSD_4_8_BASE:1.26
	OPENBSD_4_7:1.26.0.6
	OPENBSD_4_7_BASE:1.26
	OPENBSD_4_6:1.26.0.8
	OPENBSD_4_6_BASE:1.26
	OPENBSD_4_5:1.26.0.4
	OPENBSD_4_5_BASE:1.26
	OPENBSD_4_4:1.26.0.2
	OPENBSD_4_4_BASE:1.26
	OPENBSD_4_3:1.24.0.2
	OPENBSD_4_3_BASE:1.24
	OPENBSD_4_2:1.21.0.4
	OPENBSD_4_2_BASE:1.21
	OPENBSD_4_1:1.21.0.2
	OPENBSD_4_1_BASE:1.21
	OPENBSD_4_0:1.20.0.2
	OPENBSD_4_0_BASE:1.20
	OPENBSD_3_9:1.19.0.8
	OPENBSD_3_9_BASE:1.19
	OPENBSD_3_8:1.19.0.6
	OPENBSD_3_8_BASE:1.19
	OPENBSD_3_7:1.19.0.4
	OPENBSD_3_7_BASE:1.19
	OPENBSD_3_6:1.19.0.2
	OPENBSD_3_6_BASE:1.19
	SMP_SYNC_A:1.17
	SMP_SYNC_B:1.17
	OPENBSD_3_5:1.16.0.2
	OPENBSD_3_5_BASE:1.16
	OPENBSD_3_4:1.14.0.4
	OPENBSD_3_4_BASE:1.14
	UBC_SYNC_A:1.14
	OPENBSD_3_3:1.14.0.2
	OPENBSD_3_3_BASE:1.14
	OPENBSD_3_2:1.7.0.2
	OPENBSD_3_2_BASE:1.7
	OPENBSD_3_1:1.6.0.2
	OPENBSD_3_1_BASE:1.6
	UBC_SYNC_B:1.9
	UBC:1.4.0.2
	UBC_BASE:1.4
	OPENBSD_3_0:1.3.0.2
	OPENBSD_3_0_BASE:1.3
	SMP:1.1.0.2;
locks; strict;
comment	@ * @;


1.30
date	2014.04.19.16.08.14;	author henning;	state dead;
branches;
next	1.29;

1.29
date	2012.11.05.19.39.34;	author miod;	state Exp;
branches;
next	1.28;

1.28
date	2011.10.07.17.10.08;	author henning;	state Exp;
branches;
next	1.27;

1.27
date	2011.07.03.23.48.41;	author henning;	state Exp;
branches;
next	1.26;

1.26
date	2008.05.09.14.10.05;	author dlg;	state Exp;
branches;
next	1.25;

1.25
date	2008.05.08.15.22.02;	author chl;	state Exp;
branches;
next	1.24;

1.24
date	2007.12.11.00.30.14;	author mikeb;	state Exp;
branches;
next	1.23;

1.23
date	2007.12.10.23.12.56;	author mikeb;	state Exp;
branches;
next	1.22;

1.22
date	2007.09.13.20.40.02;	author chl;	state Exp;
branches;
next	1.21;

1.21
date	2006.12.20.17.50.40;	author gwk;	state Exp;
branches;
next	1.20;

1.20
date	2006.03.04.22.40.15;	author brad;	state Exp;
branches;
next	1.19;

1.19
date	2004.07.28.17.15.12;	author tholo;	state Exp;
branches;
next	1.18;

1.18
date	2004.06.24.19.35.22;	author tholo;	state Exp;
branches;
next	1.17;

1.17
date	2004.04.27.02.56.20;	author kjc;	state Exp;
branches;
next	1.16;

1.16
date	2004.01.14.08.42.23;	author kjc;	state Exp;
branches;
next	1.15;

1.15
date	2003.10.17.19.13.01;	author henning;	state Exp;
branches;
next	1.14;

1.14
date	2002.12.16.17.27.20;	author henning;	state Exp;
branches;
next	1.13;

1.13
date	2002.12.16.09.18.05;	author kjc;	state Exp;
branches;
next	1.12;

1.12
date	2002.11.29.07.51.54;	author kjc;	state Exp;
branches;
next	1.11;

1.11
date	2002.11.26.03.44.53;	author kjc;	state Exp;
branches;
next	1.10;

1.10
date	2002.11.26.01.03.34;	author henning;	state Exp;
branches;
next	1.9;

1.9
date	2002.10.11.09.30.30;	author kjc;	state Exp;
branches;
next	1.8;

1.8
date	2002.10.08.05.12.08;	author kjc;	state Exp;
branches;
next	1.7;

1.7
date	2002.07.25.20.42.53;	author deraadt;	state Exp;
branches;
next	1.6;

1.6
date	2002.03.14.01.26.26;	author millert;	state Exp;
branches;
next	1.5;

1.5
date	2002.02.13.08.11.48;	author kjc;	state Exp;
branches;
next	1.4;

1.4
date	2001.11.06.19.53.09;	author miod;	state Exp;
branches
	1.4.2.1;
next	1.3;

1.3
date	2001.09.07.08.47.23;	author kjc;	state Exp;
branches;
next	1.2;

1.2
date	2001.08.09.14.32.59;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	2001.06.27.05.28.36;	author kjc;	state Exp;
branches
	1.1.2.1;
next	;

1.1.2.1
date	2001.10.31.02.43.21;	author nate;	state Exp;
branches;
next	1.1.2.2;

1.1.2.2
date	2001.11.13.21.05.47;	author niklas;	state Exp;
branches;
next	1.1.2.3;

1.1.2.3
date	2002.03.06.02.07.07;	author niklas;	state Exp;
branches;
next	1.1.2.4;

1.1.2.4
date	2002.03.28.11.26.45;	author niklas;	state Exp;
branches;
next	1.1.2.5;

1.1.2.5
date	2003.03.27.22.28.25;	author niklas;	state Exp;
branches;
next	1.1.2.6;

1.1.2.6
date	2004.02.19.10.51.22;	author niklas;	state Exp;
branches;
next	1.1.2.7;

1.1.2.7
date	2004.06.05.23.12.26;	author niklas;	state Exp;
branches;
next	;

1.4.2.1
date	2002.06.11.03.27.42;	author art;	state Exp;
branches;
next	1.4.2.2;

1.4.2.2
date	2002.10.29.00.28.00;	author art;	state Exp;
branches;
next	1.4.2.3;

1.4.2.3
date	2003.05.19.21.50.54;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.30
log
@bye bye
@
text
@/*	$OpenBSD: altq_subr.c,v 1.29 2012/11/05 19:39:34 miod Exp $	*/
/*	$KAME: altq_subr.c,v 1.11 2002/01/11 08:11:49 kjc Exp $	*/

/*
 * Copyright (C) 1997-2002
 *	Sony Computer Science Laboratories Inc.  All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY SONY CSL AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL SONY CSL OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 */

#include <sys/param.h>
#include <sys/malloc.h>
#include <sys/mbuf.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/socket.h>
#include <sys/socketvar.h>
#include <sys/kernel.h>
#include <sys/errno.h>
#include <sys/syslog.h>
#include <sys/sysctl.h>
#include <sys/queue.h>

#include <net/if.h>
#include <net/if_dl.h>
#include <net/if_types.h>

#include <netinet/in.h>
#include <netinet/in_systm.h>
#include <netinet/ip.h>
#ifdef INET6
#include <netinet/ip6.h>
#endif
#include <netinet/tcp.h>
#include <netinet/udp.h>

#include <net/pfvar.h>
#include <altq/altq.h>

/*
 * internal function prototypes
 */
static void	oldtbr_timeout(void *);
int (*altq_input)(struct mbuf *, int) = NULL;
static int oldtbr_timer = 0;	/* token bucket regulator timer */
static struct callout oldtbr_callout = CALLOUT_INITIALIZER;

/*
 * alternate queueing support routines
 */

/* look up the queue state by the interface name and the queueing type. */
void *
altq_lookup(name, type)
	char *name;
	int type;
{
	struct ifnet *ifp;

	if ((ifp = ifunit(name)) != NULL) {
		if (type != ALTQT_NONE && ifp->if_snd.altq_type == type)
			return (ifp->if_snd.altq_disc);
	}

	return NULL;
}

int
altq_attach(ifq, type, discipline, enqueue, dequeue, request, clfier, classify)
	struct ifaltq *ifq;
	int type;
	void *discipline;
	int (*enqueue)(struct ifaltq *, struct mbuf *, struct altq_pktattr *);
	struct mbuf *(*dequeue)(struct ifaltq *, int);
	int (*request)(struct ifaltq *, int, void *);
	void *clfier;
	void *(*classify)(void *, struct mbuf *, int);
{
	if (!ALTQ_IS_READY(ifq))
		return ENXIO;

#if 0	/* pfaltq can override the existing discipline */
	if (ALTQ_IS_ENABLED(ifq))
		return EBUSY;
	if (ALTQ_IS_ATTACHED(ifq))
		return EEXIST;
#endif
	ifq->altq_type     = type;
	ifq->altq_disc     = discipline;
	ifq->altq_enqueue  = enqueue;
	ifq->altq_dequeue  = dequeue;
	ifq->altq_request  = request;
	ifq->altq_clfier   = clfier;
	ifq->altq_classify = classify;
	ifq->altq_flags &= (ALTQF_CANTCHANGE|ALTQF_ENABLED);

	return 0;
}

int
altq_detach(ifq)
	struct ifaltq *ifq;
{
	if (!ALTQ_IS_READY(ifq))
		return ENXIO;
	if (ALTQ_IS_ENABLED(ifq))
		return EBUSY;
	if (!ALTQ_IS_ATTACHED(ifq))
		return (0);

	ifq->altq_type     = ALTQT_NONE;
	ifq->altq_disc     = NULL;
	ifq->altq_enqueue  = NULL;
	ifq->altq_dequeue  = NULL;
	ifq->altq_request  = NULL;
	ifq->altq_clfier   = NULL;
	ifq->altq_classify = NULL;
	ifq->altq_flags &= ALTQF_CANTCHANGE;
	return 0;
}

int
altq_enable(ifq)
	struct ifaltq *ifq;
{
	int s;

	if (!ALTQ_IS_READY(ifq))
		return ENXIO;
	if (ALTQ_IS_ENABLED(ifq))
		return 0;

	s = splnet();
	IFQ_PURGE(ifq);
	ASSERT(ifq->ifq_len == 0);
	ifq->altq_flags |= ALTQF_ENABLED;
	if (ifq->altq_clfier != NULL)
		ifq->altq_flags |= ALTQF_CLASSIFY;
	splx(s);

	return 0;
}

int
altq_disable(ifq)
	struct ifaltq *ifq;
{
	int s;

	if (!ALTQ_IS_ENABLED(ifq))
		return 0;

	s = splnet();
	IFQ_PURGE(ifq);
	ASSERT(ifq->ifq_len == 0);
	ifq->altq_flags &= ~(ALTQF_ENABLED|ALTQF_CLASSIFY);
	splx(s);
	return 0;
}

void
altq_assert(file, line, failedexpr)
	const char *file, *failedexpr;
	int line;
{
	(void)printf("altq assertion \"%s\" failed: file \"%s\", line %d\n",
		     failedexpr, file, line);
	panic("altq assertion");
	/* NOTREACHED */
}

/*
 * internal representation of token bucket parameters
 *	rate:	byte_per_unittime << 32
 *		(((bits_per_sec) / 8) << 32) / machclk_freq
 *	depth:	byte << 32
 *
 */
#define	OLDTBR_SHIFT	32
#define	OLDTBR_SCALE(x)	((int64_t)(x) << OLDTBR_SHIFT)
#define	OLDTBR_UNSCALE(x)	((x) >> OLDTBR_SHIFT)

struct mbuf *
oldtbr_dequeue(struct ifaltq *ifq, int op)
{
	struct oldtb_regulator *tbr;
	struct mbuf *m;
	int64_t interval;
	u_int64_t now;

	tbr = ifq->altq_tbr;
	if (op == ALTDQ_REMOVE && tbr->tbr_lastop == ALTDQ_POLL) {
		/* if this is a remove after poll, bypass tbr check */
	} else {
		/* update token only when it is negative */
		if (tbr->tbr_token <= 0) {
			now = read_machclk();
			interval = now - tbr->tbr_last;
			if (interval >= tbr->tbr_filluptime)
				tbr->tbr_token = tbr->tbr_depth;
			else {
				tbr->tbr_token += interval * tbr->tbr_rate;
				if (tbr->tbr_token > tbr->tbr_depth)
					tbr->tbr_token = tbr->tbr_depth;
			}
			tbr->tbr_last = now;
		}
		/* if token is still negative, don't allow dequeue */
		if (tbr->tbr_token <= 0)
			return (NULL);
	}

	if (ALTQ_IS_ENABLED(ifq))
		m = (*ifq->altq_dequeue)(ifq, op);
	else {
		if (op == ALTDQ_POLL)
			IF_POLL(ifq, m);
		else
			IF_DEQUEUE(ifq, m);
	}

	if (m != NULL && op == ALTDQ_REMOVE)
		tbr->tbr_token -= OLDTBR_SCALE(m_pktlen(m));
	tbr->tbr_lastop = op;
	return (m);
}

/*
 * set a token bucket regulator.
 * if the specified rate is zero, the token bucket regulator is deleted.
 */
int
oldtbr_set(ifq, profile)
	struct ifaltq *ifq;
	struct oldtb_profile *profile;
{
	struct oldtb_regulator *tbr, *otbr;

	if (machclk_freq == 0)
		init_machclk();
	if (machclk_freq == 0) {
		printf("oldtbr_set: no cpu clock available!\n");
		return (ENXIO);
	}

	if (profile->rate == 0) {
		/* delete this tbr */
		if ((tbr = ifq->altq_tbr) == NULL)
			return (ENOENT);
		ifq->altq_tbr = NULL;
		free(tbr, M_DEVBUF);
		return (0);
	}

	tbr = malloc(sizeof(struct oldtb_regulator), M_DEVBUF, M_WAITOK|M_ZERO);

	tbr->tbr_rate = OLDTBR_SCALE(profile->rate / 8) / machclk_freq;
	tbr->tbr_depth = OLDTBR_SCALE(profile->depth);
	if (tbr->tbr_rate > 0)
		tbr->tbr_filluptime = tbr->tbr_depth / tbr->tbr_rate;
	else
		tbr->tbr_filluptime = 0xffffffffffffffffLL;
	tbr->tbr_token = tbr->tbr_depth;
	tbr->tbr_last = read_machclk();
	tbr->tbr_lastop = ALTDQ_REMOVE;

	otbr = ifq->altq_tbr;
	ifq->altq_tbr = tbr;	/* set the new tbr */

	if (otbr != NULL)
		free(otbr, M_DEVBUF);
	else {
		if (oldtbr_timer == 0) {
			CALLOUT_RESET(&oldtbr_callout, 1, oldtbr_timeout, NULL);
			oldtbr_timer = 1;
		}
	}
	return (0);
}

/*
 * tbr_timeout goes through the interface list, and kicks the drivers
 * if necessary.
 */
static void
oldtbr_timeout(arg)
	void *arg;
{
	struct ifnet *ifp;
	int active, s;

	active = 0;
	s = splnet();
	for (ifp = TAILQ_FIRST(&ifnet); ifp; ifp = TAILQ_NEXT(ifp, if_list)) {
		if (!OLDTBR_IS_ENABLED(&ifp->if_snd))
			continue;
		active++;
		if (!IFQ_IS_EMPTY(&ifp->if_snd) && ifp->if_start != NULL)
			if_start(ifp);
	}
	splx(s);
	if (active > 0)
		CALLOUT_RESET(&oldtbr_callout, 1, oldtbr_timeout, NULL);
	else
		oldtbr_timer = 0;	/* don't need tbr_timer anymore */
}

/*
 * get token bucket regulator profile
 */
int
oldtbr_get(ifq, profile)
	struct ifaltq *ifq;
	struct oldtb_profile *profile;
{
	struct oldtb_regulator *tbr;

	if ((tbr = ifq->altq_tbr) == NULL) {
		profile->rate = 0;
		profile->depth = 0;
	} else {
		profile->rate =
		    (u_int)OLDTBR_UNSCALE(tbr->tbr_rate * 8 * machclk_freq);
		profile->depth = (u_int)OLDTBR_UNSCALE(tbr->tbr_depth);
	}
	return (0);
}

/*
 * attach a discipline to the interface.  if one already exists, it is
 * overridden.
 */
int
altq_pfattach(struct pf_altq *a)
{
	int error = 0;

	switch (a->scheduler) {
	case ALTQT_NONE:
		break;
#ifdef ALTQ_CBQ
	case ALTQT_CBQ:
		error = cbq_pfattach(a);
		break;
#endif
#ifdef ALTQ_PRIQ
	case ALTQT_PRIQ:
		error = priq_pfattach(a);
		break;
#endif
#ifdef ALTQ_HFSC
	case ALTQT_HFSC:
		error = hfsc_pfattach(a);
		break;
#endif
	default:
		error = ENXIO;
	}

	return (error);
}

/*
 * detach a discipline from the interface.
 * it is possible that the discipline was already overridden by another
 * discipline.
 */
int
altq_pfdetach(struct pf_altq *a)
{
	struct ifnet *ifp;
	int s, error = 0;

	if ((ifp = ifunit(a->ifname)) == NULL)
		return (EINVAL);

	/* if this discipline is no longer referenced, just return */
	if (a->altq_disc == NULL || a->altq_disc != ifp->if_snd.altq_disc)
		return (0);

	s = splnet();
	if (ALTQ_IS_ENABLED(&ifp->if_snd))
		error = altq_disable(&ifp->if_snd);
	if (error == 0)
		error = altq_detach(&ifp->if_snd);
	splx(s);

	return (error);
}

/*
 * add a discipline or a queue
 */
int
altq_add(struct pf_altq *a)
{
	int error = 0;

	if (a->qname[0] != 0)
		return (altq_add_queue(a));

	if (machclk_freq == 0)
		init_machclk();
	if (machclk_freq == 0)
		panic("altq_add: no cpu clock");

	switch (a->scheduler) {
#ifdef ALTQ_CBQ
	case ALTQT_CBQ:
		error = cbq_add_altq(a);
		break;
#endif
#ifdef ALTQ_PRIQ
	case ALTQT_PRIQ:
		error = priq_add_altq(a);
		break;
#endif
#ifdef ALTQ_HFSC
	case ALTQT_HFSC:
		error = hfsc_add_altq(a);
		break;
#endif
	default:
		error = ENXIO;
	}

	return (error);
}

/*
 * remove a discipline or a queue
 */
int
altq_remove(struct pf_altq *a)
{
	int error = 0;

	if (a->qname[0] != 0)
		return (altq_remove_queue(a));

	switch (a->scheduler) {
#ifdef ALTQ_CBQ
	case ALTQT_CBQ:
		error = cbq_remove_altq(a);
		break;
#endif
#ifdef ALTQ_PRIQ
	case ALTQT_PRIQ:
		error = priq_remove_altq(a);
		break;
#endif
#ifdef ALTQ_HFSC
	case ALTQT_HFSC:
		error = hfsc_remove_altq(a);
		break;
#endif
	default:
		error = ENXIO;
	}

	return (error);
}

/*
 * add a queue to the discipline
 */
int
altq_add_queue(struct pf_altq *a)
{
	int error = 0;

	switch (a->scheduler) {
#ifdef ALTQ_CBQ
	case ALTQT_CBQ:
		error = cbq_add_queue(a);
		break;
#endif
#ifdef ALTQ_PRIQ
	case ALTQT_PRIQ:
		error = priq_add_queue(a);
		break;
#endif
#ifdef ALTQ_HFSC
	case ALTQT_HFSC:
		error = hfsc_add_queue(a);
		break;
#endif
	default:
		error = ENXIO;
	}

	return (error);
}

/*
 * remove a queue from the discipline
 */
int
altq_remove_queue(struct pf_altq *a)
{
	int error = 0;

	switch (a->scheduler) {
#ifdef ALTQ_CBQ
	case ALTQT_CBQ:
		error = cbq_remove_queue(a);
		break;
#endif
#ifdef ALTQ_PRIQ
	case ALTQT_PRIQ:
		error = priq_remove_queue(a);
		break;
#endif
#ifdef ALTQ_HFSC
	case ALTQT_HFSC:
		error = hfsc_remove_queue(a);
		break;
#endif
	default:
		error = ENXIO;
	}

	return (error);
}

/*
 * get queue statistics
 */
int
altq_getqstats(struct pf_altq *a, void *ubuf, int *nbytes)
{
	int error = 0;

	switch (a->scheduler) {
#ifdef ALTQ_CBQ
	case ALTQT_CBQ:
		error = cbq_getqstats(a, ubuf, nbytes);
		break;
#endif
#ifdef ALTQ_PRIQ
	case ALTQT_PRIQ:
		error = priq_getqstats(a, ubuf, nbytes);
		break;
#endif
#ifdef ALTQ_HFSC
	case ALTQT_HFSC:
		error = hfsc_getqstats(a, ubuf, nbytes);
		break;
#endif
	default:
		error = ENXIO;
	}

	return (error);
}

#define	MACHCLK_SHIFT	8

u_int32_t machclk_tc = 0;
u_int32_t machclk_freq = 0;
u_int32_t machclk_per_tick = 0;

void
init_machclk(void)
{
	/*
	 * If we have timecounters, microtime is good enough and we can
	 * avoid problems on machines with variable cycle counter
	 * frequencies.
	 */
	machclk_tc = 1;

	if (machclk_tc == 1) {
		/* emulate 256MHz using microtime() */
		machclk_freq = 1000000 << MACHCLK_SHIFT;
		machclk_per_tick = machclk_freq / hz;
#ifdef ALTQ_DEBUG
		printf("altq: emulating %uHz cpu clock\n", machclk_freq);
#endif
		return;
	}

	/*
	 * if we don't know the clock frequency, measure it.
	 */
	if (machclk_freq == 0) {
		static int	wait;
		struct timeval	tv_start, tv_end;
		u_int64_t	start, end, diff;
		int		timo;

		microtime(&tv_start);
		start = read_machclk();
		timo = hz;	/* 1 sec */
		(void)tsleep(&wait, PWAIT | PCATCH, "init_machclk", timo);
		microtime(&tv_end);
		end = read_machclk();
		diff = (u_int64_t)(tv_end.tv_sec - tv_start.tv_sec) * 1000000
		    + tv_end.tv_usec - tv_start.tv_usec;
		if (diff != 0)
			machclk_freq = (u_int)((end - start) * 1000000 / diff);
	}

	machclk_per_tick = machclk_freq / hz;

#ifdef ALTQ_DEBUG
	printf("altq: CPU clock: %uHz\n", machclk_freq);
#endif
}

u_int64_t
read_machclk(void)
{
	struct timeval tv;
	u_int64_t val;

	microuptime(&tv);
	val = (((u_int64_t)(tv.tv_sec) * 1000000 +
	    tv.tv_usec) << MACHCLK_SHIFT);
	return (val);
}
@


1.29
log
@unifdef -D __HAVE_TIMECOUNTER
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.28 2011/10/07 17:10:08 henning Exp $	*/
@


1.28
log
@rename some vars and functions
unfortunately altq is one giant namespace violation. rename just those that
conflict with new stuff for now only to be found on my laptop. reduce pain,
the diff is huge already. ok ryan
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.26 2008/05/09 14:10:05 dlg Exp $	*/
a584 1
#if defined(__HAVE_TIMECOUNTER)
a590 1
#endif
@


1.27
log
@g/c code to read/write the dscp field. with pf as classifier altq has no
business in mucking with it and since the _CLEARDSCP flags were never
possibly set that is effectively dead code
@
text
@d62 1
a62 1
static void	tbr_timeout(void *);
d64 2
a65 2
static int tbr_timer = 0;	/* token bucket regulator timer */
static struct callout tbr_callout = CALLOUT_INITIALIZER;
d198 3
a200 3
#define	TBR_SHIFT	32
#define	TBR_SCALE(x)	((int64_t)(x) << TBR_SHIFT)
#define	TBR_UNSCALE(x)	((x) >> TBR_SHIFT)
d203 1
a203 3
tbr_dequeue(ifq, op)
	struct ifaltq *ifq;
	int op;
d205 1
a205 1
	struct tb_regulator *tbr;
d242 1
a242 1
		tbr->tbr_token -= TBR_SCALE(m_pktlen(m));
d252 1
a252 1
tbr_set(ifq, profile)
d254 1
a254 1
	struct tb_profile *profile;
d256 1
a256 1
	struct tb_regulator *tbr, *otbr;
d261 1
a261 1
		printf("tbr_set: no cpu clock available!\n");
d274 1
a274 1
	tbr = malloc(sizeof(struct tb_regulator), M_DEVBUF, M_WAITOK|M_ZERO);
d276 2
a277 2
	tbr->tbr_rate = TBR_SCALE(profile->rate / 8) / machclk_freq;
	tbr->tbr_depth = TBR_SCALE(profile->depth);
d292 3
a294 3
		if (tbr_timer == 0) {
			CALLOUT_RESET(&tbr_callout, 1, tbr_timeout, (void *)0);
			tbr_timer = 1;
d305 1
a305 1
tbr_timeout(arg)
d314 1
a314 1
		if (!TBR_IS_ENABLED(&ifp->if_snd))
d322 1
a322 1
		CALLOUT_RESET(&tbr_callout, 1, tbr_timeout, (void *)0);
d324 1
a324 1
		tbr_timer = 0;	/* don't need tbr_timer anymore */
d331 1
a331 1
tbr_get(ifq, profile)
d333 1
a333 1
	struct tb_profile *profile;
d335 1
a335 1
	struct tb_regulator *tbr;
d342 2
a343 2
		    (u_int)TBR_UNSCALE(tbr->tbr_rate * 8 * machclk_freq);
		profile->depth = (u_int)TBR_UNSCALE(tbr->tbr_depth);
@


1.26
log
@switch altq from ifp->if_start to if_start() to take advantage of tx mit.

ok kjc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.25 2008/05/08 15:22:02 chl Exp $	*/
a576 115

/*
 * read and write diffserv field in IPv4 or IPv6 header
 */
u_int8_t
read_dsfield(m, pktattr)
	struct mbuf *m;
	struct altq_pktattr *pktattr;
{
	struct mbuf *m0;
	u_int8_t ds_field = 0;

	if (pktattr == NULL ||
	    (pktattr->pattr_af != AF_INET && pktattr->pattr_af != AF_INET6))
		return ((u_int8_t)0);

	/* verify that pattr_hdr is within the mbuf data */
	for (m0 = m; m0 != NULL; m0 = m0->m_next)
		if ((pktattr->pattr_hdr >= m0->m_data) &&
		    (pktattr->pattr_hdr < m0->m_data + m0->m_len))
			break;
	if (m0 == NULL) {
		/* ick, pattr_hdr is stale */
		pktattr->pattr_af = AF_UNSPEC;
#ifdef ALTQ_DEBUG
		printf("read_dsfield: can't locate header!\n");
#endif
		return ((u_int8_t)0);
	}

	if (pktattr->pattr_af == AF_INET) {
		struct ip *ip = (struct ip *)pktattr->pattr_hdr;

		if (ip->ip_v != 4)
			return ((u_int8_t)0);	/* version mismatch! */
		ds_field = ip->ip_tos;
	}
#ifdef INET6
	else if (pktattr->pattr_af == AF_INET6) {
		struct ip6_hdr *ip6 = (struct ip6_hdr *)pktattr->pattr_hdr;
		u_int32_t flowlabel;

		flowlabel = ntohl(ip6->ip6_flow);
		if ((flowlabel >> 28) != 6)
			return ((u_int8_t)0);	/* version mismatch! */
		ds_field = (flowlabel >> 20) & 0xff;
	}
#endif
	return (ds_field);
}

void
write_dsfield(m, pktattr, dsfield)
	struct mbuf *m;
	struct altq_pktattr *pktattr;
	u_int8_t dsfield;
{
	struct mbuf *m0;

	if (pktattr == NULL ||
	    (pktattr->pattr_af != AF_INET && pktattr->pattr_af != AF_INET6))
		return;

	/* verify that pattr_hdr is within the mbuf data */
	for (m0 = m; m0 != NULL; m0 = m0->m_next)
		if ((pktattr->pattr_hdr >= m0->m_data) &&
		    (pktattr->pattr_hdr < m0->m_data + m0->m_len))
			break;
	if (m0 == NULL) {
		/* ick, pattr_hdr is stale */
		pktattr->pattr_af = AF_UNSPEC;
#ifdef ALTQ_DEBUG
		printf("write_dsfield: can't locate header!\n");
#endif
		return;
	}

	if (pktattr->pattr_af == AF_INET) {
		struct ip *ip = (struct ip *)pktattr->pattr_hdr;
		u_int8_t old;
		int32_t sum;

		if (ip->ip_v != 4)
			return;		/* version mismatch! */
		old = ip->ip_tos;
		dsfield |= old & 3;	/* leave CU bits */
		if (old == dsfield)
			return;
		ip->ip_tos = dsfield;
		/*
		 * update checksum (from RFC1624)
		 *	   HC' = ~(~HC + ~m + m')
		 */
		sum = ~ntohs(ip->ip_sum) & 0xffff;
		sum += 0xff00 + (~old & 0xff) + dsfield;
		sum = (sum >> 16) + (sum & 0xffff);
		sum += (sum >> 16);  /* add carry */

		ip->ip_sum = htons(~sum & 0xffff);
	}
#ifdef INET6
	else if (pktattr->pattr_af == AF_INET6) {
		struct ip6_hdr *ip6 = (struct ip6_hdr *)pktattr->pattr_hdr;
		u_int32_t flowlabel;

		flowlabel = ntohl(ip6->ip6_flow);
		if ((flowlabel >> 28) != 6)
			return;		/* version mismatch! */
		flowlabel = (flowlabel & 0xf03fffff) | (dsfield << 20);
		ip6->ip6_flow = htonl(flowlabel);
	}
#endif
	return;
}

@


1.25
log
@do not check malloc return value against NULL, as M_WAITOK is used

ok kjc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.24 2007/12/11 00:30:14 mikeb Exp $	*/
d320 1
a320 1
			(*ifp->if_start)(ifp);
@


1.24
log
@kill the comment that doesn't make sense anymore
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.23 2007/12/10 23:12:56 mikeb Exp $	*/
a276 2
	if (tbr == NULL)
		return (ENOMEM);
@


1.23
log
@Remove an MD code that was obsoleted by the timecounters.

ok kjc
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.22 2007/09/13 20:40:02 chl Exp $	*/
a694 8
/*
 * high resolution clock support taking advantage of a machine dependent
 * high resolution time counter (e.g., timestamp counter of intel pentium).
 * we assume
 *  - 64-bit-long monotonically-increasing counter
 *  - frequency range is 100M-4GHz (CPU speed)
 */
/* if pcc is not available or disabled, emulate 256MHz using microtime() */
@


1.22
log
@MALLOC/FREE -> malloc/free and M_ZERO changes

ok henning@@ krw@@ canacar@@ ray@@
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.21 2006/12/20 17:50:40 gwk Exp $	*/
a58 6
/* machine dependent clock related includes */
#if defined(__i386__)
#include <machine/cpufunc.h>		/* for pentium tsc */
#include <machine/specialreg.h>		/* for CPUID_TSC */
#endif /* __i386__ */

a328 14
#if defined(__alpha__) && !defined(ALTQ_NOPCC)
	{
		/*
		 * XXX read out the machine dependent clock once a second
		 * to detect counter wrap-around.
		 */
		static u_int cnt;

		if (++cnt >= hz) {
			(void)read_machclk();
			cnt = 0;
		}
	}
#endif /* __alpha__ && !ALTQ_NOPCC */
d705 1
a705 1
int machclk_usepcc;
a708 4
#ifdef __alpha__
extern u_int64_t cycles_per_usec;	/* alpha cpu clock frequency */
#endif /* __alpha__ */

d712 1
a712 12
	machclk_usepcc = 1;

#if (!defined(__i386__) && !defined(__alpha__)) || defined(ALTQ_NOPCC)
	machclk_usepcc = 0;
#endif
#if defined(__FreeBSD__) && defined(SMP)
	machclk_usepcc = 0;
#endif
#if defined(__NetBSD__) && defined(MULTIPROCESSOR)
	machclk_usepcc = 0;
#endif
#if defined(__OpenBSD__) && defined(__HAVE_TIMECOUNTER)
d718 1
a718 6
	machclk_usepcc = 0;
#endif
#ifdef __i386__
	/* check if TSC is available */
	if (machclk_usepcc == 1 && (cpu_feature & CPUID_TSC) == 0)
		machclk_usepcc = 0;
d721 1
a721 1
	if (machclk_usepcc == 0) {
d726 1
a726 1
		printf("altq: emulate %uHz cpu clock\n", machclk_freq);
a731 12
	 * if the clock frequency (of Pentium TSC or Alpha PCC) is
	 * accessible, just use it.
	 */
#if defined(__i386__) && (defined(I586_CPU) || defined(I686_CPU))
	/* XXX - this will break down with variable cpu frequency. */
	machclk_freq = cpuspeed * 1000000;
#endif
#if defined(__alpha__)
	machclk_freq = (u_int32_t)(cycles_per_usec * 1000000);
#endif /* __alpha__ */

	/*
a758 10
#if defined(__OpenBSD__) && defined(__i386__)
static __inline u_int64_t
rdtsc(void)
{
	u_int64_t rv;
	__asm __volatile(".byte 0x0f, 0x31" : "=A" (rv));
	return (rv);
}
#endif /* __OpenBSD__ && __i386__ */

d762 1
d765 3
a767 30
	if (machclk_usepcc) {
#if defined(__i386__)
		val = rdtsc();
#elif defined(__alpha__)
		static u_int32_t last_pcc, upper;
		u_int32_t pcc;

		/*
		 * for alpha, make a 64bit counter value out of the 32bit
		 * alpha processor cycle counter.
		 * read_machclk must be called within a half of its
		 * wrap-around cycle (about 5 sec for 400MHz cpu) to properly
		 * detect a counter wrap-around.
		 * tbr_timeout calls read_machclk once a second.
		 */
		pcc = (u_int32_t)alpha_rpcc();
		if (pcc <= last_pcc)
			upper++;
		last_pcc = pcc;
		val = ((u_int64_t)upper << 32) + pcc;
#else
		panic("read_machclk");
#endif
	} else {
		struct timeval tv;

		microuptime(&tv);
		val = (((u_int64_t)(tv.tv_sec) * 1000000
		    + tv.tv_usec) << MACHCLK_SHIFT);
	}
@


1.21
log
@"#ifdef is a tool of the weak!"
Rename pentium_mhz to cpuspeed which is consistant with amd64 making
shared ACPI code less nasty.
ok marco, deraadt
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.20 2006/03/04 22:40:15 brad Exp $	*/
d278 1
a278 1
		FREE(tbr, M_DEVBUF);
d282 1
a282 2
	MALLOC(tbr, struct tb_regulator *, sizeof(struct tb_regulator),
	       M_DEVBUF, M_WAITOK);
a284 1
	bzero(tbr, sizeof(struct tb_regulator));
d300 1
a300 1
		FREE(otbr, M_DEVBUF);
@


1.20
log
@With the exception of two other small uncommited diffs this moves
the remainder of the network stack from splimp to splnet.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.19 2004/07/28 17:15:12 tholo Exp $	*/
d779 1
a779 1
	machclk_freq = pentium_mhz * 1000000;
@


1.19
log
@This touches only MI code, and adds new time keeping code.  The
code is all conditionalized on __HAVE_TIMECOUNTER, and not
enabled on any platforms.

adjtime(2) support exists, courtesy of nordin@@, sysctl(2) support
and a concept of quality for each time source attached exists.

High quality time sources exists for PIIX4 ACPI timer as well as
some AMD power management chips.  This will have to be redone
once we actually add ACPI support (at that time we need to use
the ACPI interfaces to get at these clocks).

ok art@@ ken@@ miod@@ jmc@@ and many more
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.18 2004/06/24 19:35:22 tholo Exp $	*/
d158 1
a158 1
	s = splimp();
d178 1
a178 1
	s = splimp();
d324 1
a324 1
	s = splimp();
d426 1
a426 1
	s = splimp();
@


1.18
log
@This moves access to wall and uptime variables in MI code,
encapsulating all such access into wall-defined functions
that makes sure locking is done as needed.

It also cleans up some uses of wall time vs. uptime some
places, but there is sure to be more of these needed as
well, particularily in MD code.  Also, many current calls
to microtime() should probably be changed to getmicrotime(),
or to the {,get}microuptime() versions.

ok art@@ deraadt@@ aaron@@ matthieu@@ beck@@ sturm@@ millert@@ others
"Oh, that is not your problem!" from miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.17 2004/04/27 02:56:20 kjc Exp $	*/
d747 8
@


1.17
log
@make separate functions to enable/disable altq, and call them when we
reload rules.
this fixes an altq problem that, if you reload pf rules not containing
queues while running altq, the interface shaper is not properly removed.

make pf_altq_running local to pf_ioctl.c since it is no longer used in
altq_subr.c.

ok henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.16 2004/01/14 08:42:23 kjc Exp $	*/
d770 1
d846 2
a847 2
		microtime(&tv);
		val = (((u_int64_t)(tv.tv_sec - boottime.tv_sec) * 1000000
@


1.16
log
@eliminate the predefined special qids so that qids become simple
identifiers without embedded meanings.

this also allows us to make the semantics of the qid assignment in line
with the tag assignment in the next step.

ok, henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.15 2003/10/17 19:13:01 henning Exp $	*/
a72 2
int pfaltq_running;	/* keep track of running state */

d381 1
a381 3
	struct ifnet *ifp;
	struct tb_profile tb;
	int s, error = 0;
a402 17
	}

	ifp = ifunit(a->ifname);

	/* if the state is running, enable altq */
	if (error == 0 && pfaltq_running &&
	    ifp != NULL && ifp->if_snd.altq_type != ALTQT_NONE &&
	    !ALTQ_IS_ENABLED(&ifp->if_snd))
			error = altq_enable(&ifp->if_snd);

	/* if altq is already enabled, reset set tokenbucket regulator */
	if (error == 0 && ifp != NULL && ALTQ_IS_ENABLED(&ifp->if_snd)) {
		tb.rate = a->ifbandwidth;
		tb.depth = a->tbrsize;
		s = splimp();
		error = tbr_set(&ifp->if_snd, &tb);
		splx(s);
@


1.15
log
@typos in comments; from Jared Yanovich <jjy2+ at pitt dot edu>
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.14 2002/12/16 17:27:20 henning Exp $	*/
a39 1
#include <uvm/uvm_extern.h>
@


1.14
log
@major KNF, Take 2

ok kjc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.13 2002/12/16 09:18:05 kjc Exp $	*/
d80 1
a80 1
/* look up the queue state by the interface name and the queuing type. */
@


1.13
log
@switchover to pf-based altq.
 - remove files which are no longer used, or we don't have plans to support
   in pf in the near future.
 - remove altq ioctl related stuff.
 - convert the PRIQ, HFSC and RIO modules to pf-based altq.
   (these are not enabled in GENERIC, CDNR is not converted yet.)
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.12 2002/11/29 07:51:54 kjc Exp $	*/
d202 1
a202 1
 *	rate: 	byte_per_unittime << 32
@


1.12
log
@convert read_machclk() from a macro to a real function and
avoid kernel-rebuild with ALTQ_NOPCC for non-pentium users.
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.11 2002/11/26 03:44:53 kjc Exp $	*/
a58 1
#include <altq/altq_conf.h>
a69 27
static int 	extract_ports4(struct mbuf *, struct ip *,
				    struct flowinfo_in *);
#ifdef INET6
static int 	extract_ports6(struct mbuf *, struct ip6_hdr *,
				    struct flowinfo_in6 *);
#endif
static int	apply_filter4(u_int32_t, struct flow_filter *,
				   struct flowinfo_in *);
static int	apply_ppfilter4(u_int32_t, struct flow_filter *,
				     struct flowinfo_in *);
#ifdef INET6
static int	apply_filter6(u_int32_t, struct flow_filter6 *,
					   struct flowinfo_in6 *);
#endif
static int	apply_tosfilter4(u_int32_t, struct flow_filter *,
					     struct flowinfo_in *);
static u_long	get_filt_handle(struct acc_classifier *, int);
static struct acc_filter *filth_to_filtp(struct acc_classifier *,
					      u_long);
static u_int32_t filt2fibmask(struct flow_filter *);

static void 	ip4f_cache(struct ip *, struct flowinfo_in *);
static int 	ip4f_lookup(struct ip *, struct flowinfo_in *);
static int 	ip4f_init(void);
static struct ip4_frag	*ip4f_alloc(void);
static void 	ip4f_free(struct ip4_frag *);

d110 6
a115 8
	if (PFALTQ_IS_ACTIVE()) {
		/* pfaltq can override the existing discipline */
	} else {
		if (ALTQ_IS_ENABLED(ifq))
			return EBUSY;
		if (ALTQ_IS_ATTACHED(ifq))
			return EEXIST;
	}
d123 1
a123 4
	if (PFALTQ_IS_ACTIVE())
		ifq->altq_flags &= (ALTQF_CANTCHANGE|ALTQF_ENABLED);
	else
		ifq->altq_flags &= ALTQF_CANTCHANGE;
a124 3
#ifdef ALTQ_KLD
	altq_module_incref(type);
#endif
a138 3
#ifdef ALTQ_KLD
	altq_module_declref(ifq->altq_type);
#endif
d328 1
a328 10
#ifdef __FreeBSD__
#if (__FreeBSD_version < 300000)
	for (ifp = ifnet; ifp; ifp = ifp->if_next)
#else
	for (ifp = ifnet.tqh_first; ifp != NULL; ifp = ifp->if_link.tqe_next)
#endif
#else /* !FreeBSD */
	for (ifp = ifnet.tqh_first; ifp != NULL; ifp = ifp->if_list.tqe_next)
#endif
	{
d391 1
d395 11
d407 1
a407 1
		error = EINVAL;
d469 5
d475 1
d479 11
d491 1
a491 1
		error = EINVAL;
d509 1
d513 11
d525 1
a525 1
		error = EINVAL;
d540 1
d544 11
d556 1
a556 1
		error = EINVAL;
d571 1
d575 11
d587 1
a587 1
		error = EINVAL;
d602 1
a605 10
	default:
		error = EINVAL;
	}

	return (error);
}


#ifndef IPPROTO_ESP
#define	IPPROTO_ESP	50		/* encapsulating security payload */
d607 4
a610 2
#ifndef IPPROTO_AH
#define	IPPROTO_AH	51		/* authentication header */
d612 3
a614 84

/*
 * extract flow information from a given packet.
 * filt_mask shows flowinfo fields required.
 * we assume the ip header is in one mbuf, and addresses and ports are
 * in network byte order.
 */
int
altq_extractflow(m, af, flow, filt_bmask)
	struct mbuf *m;
	int af;
	struct flowinfo *flow;
	u_int32_t	filt_bmask;
{

	switch (af) {
	case PF_INET: {
		struct flowinfo_in *fin;
		struct ip *ip;

		ip = mtod(m, struct ip *);

		if (ip->ip_v != 4)
			break;

		fin = (struct flowinfo_in *)flow;
		fin->fi_len = sizeof(struct flowinfo_in);
		fin->fi_family = AF_INET;

		fin->fi_proto = ip->ip_p;
		fin->fi_tos = ip->ip_tos;

		fin->fi_src.s_addr = ip->ip_src.s_addr;
		fin->fi_dst.s_addr = ip->ip_dst.s_addr;

		if (filt_bmask & FIMB4_PORTS)
			/* if port info is required, extract port numbers */
			extract_ports4(m, ip, fin);
		else {
			fin->fi_sport = 0;
			fin->fi_dport = 0;
			fin->fi_gpi = 0;
		}
		return (1);
	}

#ifdef INET6
	case PF_INET6: {
		struct flowinfo_in6 *fin6;
		struct ip6_hdr *ip6;

		ip6 = mtod(m, struct ip6_hdr *);
		/* should we check the ip version? */

		fin6 = (struct flowinfo_in6 *)flow;
		fin6->fi6_len = sizeof(struct flowinfo_in6);
		fin6->fi6_family = AF_INET6;

		fin6->fi6_proto = ip6->ip6_nxt;
		fin6->fi6_tclass   = (ntohl(ip6->ip6_flow) >> 20) & 0xff;

		fin6->fi6_flowlabel = ip6->ip6_flow & htonl(0x000fffff);
		fin6->fi6_src = ip6->ip6_src;
		fin6->fi6_dst = ip6->ip6_dst;

		if ((filt_bmask & FIMB6_PORTS) ||
		    ((filt_bmask & FIMB6_PROTO)
		     && ip6->ip6_nxt > IPPROTO_IPV6))
			/*
			 * if port info is required, or proto is required
			 * but there are option headers, extract port
			 * and protocol numbers.
			 */
			extract_ports6(m, ip6, fin6);
		else {
			fin6->fi6_sport = 0;
			fin6->fi6_dport = 0;
			fin6->fi6_gpi = 0;
		}
		return (1);
	}
#endif /* INET6 */

	default:
a615 53
	}

	/* failed */
	flow->fi_len = sizeof(struct flowinfo);
	flow->fi_family = AF_UNSPEC;
	return (0);
}

/*
 * helper routine to extract port numbers
 */
/* structure for ipsec and ipv6 option header template */
struct _opt6 {
	u_int8_t	opt6_nxt;	/* next header */
	u_int8_t	opt6_hlen;	/* header extension length */
	u_int16_t	_pad;
	u_int32_t	ah_spi;		/* security parameter index
					   for authentication header */
};

/*
 * extract port numbers from a ipv4 packet.
 */
static int
extract_ports4(m, ip, fin)
	struct mbuf *m;
	struct ip *ip;
	struct flowinfo_in *fin;
{
	struct mbuf *m0;
	u_short ip_off;
	u_int8_t proto;
	int 	off;

	fin->fi_sport = 0;
	fin->fi_dport = 0;
	fin->fi_gpi = 0;

	ip_off = ntohs(ip->ip_off);
	/* if it is a fragment, try cached fragment info */
	if (ip_off & IP_OFFMASK) {
		ip4f_lookup(ip, fin);
		return (1);
	}

	/* locate the mbuf containing the protocol header */
	for (m0 = m; m0 != NULL; m0 = m0->m_next)
		if (((caddr_t)ip >= m0->m_data) &&
		    ((caddr_t)ip < m0->m_data + m0->m_len))
			break;
	if (m0 == NULL) {
#ifdef ALTQ_DEBUG
		printf("extract_ports4: can't locate header! ip=%p\n", ip);
a616 54
		return (0);
	}
	off = ((caddr_t)ip - m0->m_data) + (ip->ip_hl << 2);
	proto = ip->ip_p;

#ifdef ALTQ_IPSEC
 again:
#endif
	while (off >= m0->m_len) {
		off -= m0->m_len;
		m0 = m0->m_next;
		if (m0 == NULL)
			return (0);  /* bogus ip_hl! */
	}
	if (m0->m_len < off + 4)
		return (0);

	switch (proto) {
	case IPPROTO_TCP:
	case IPPROTO_UDP: {
		struct udphdr *udp;

		udp = (struct udphdr *)(mtod(m0, caddr_t) + off);
		fin->fi_sport = udp->uh_sport;
		fin->fi_dport = udp->uh_dport;
		fin->fi_proto = proto;
		}
		break;

#ifdef ALTQ_IPSEC
	case IPPROTO_ESP:
		if (fin->fi_gpi == 0){
			u_int32_t *gpi;

			gpi = (u_int32_t *)(mtod(m0, caddr_t) + off);
			fin->fi_gpi   = *gpi;
		}
		fin->fi_proto = proto;
		break;

	case IPPROTO_AH: {
			/* get next header and header length */
			struct _opt6 *opt6;

			opt6 = (struct _opt6 *)(mtod(m0, caddr_t) + off);
			proto = opt6->opt6_nxt;
			off += 8 + (opt6->opt6_hlen * 4);
			if (fin->fi_gpi == 0 && m0->m_len >= off + 8)
				fin->fi_gpi = opt6->ah_spi;
		}
		/* goto the next header */
		goto again;
#endif  /* ALTQ_IPSEC */

d618 1
a618 2
		fin->fi_proto = proto;
		return (0);
d621 1
a621 5
	/* if this is a first fragment, cache it. */
	if (ip_off & IP_MF)
		ip4f_cache(ip, fin);

	return (1);
a622 728

#ifdef INET6
static int
extract_ports6(m, ip6, fin6)
	struct mbuf *m;
	struct ip6_hdr *ip6;
	struct flowinfo_in6 *fin6;
{
	struct mbuf *m0;
	int	off;
	u_int8_t proto;

	fin6->fi6_gpi   = 0;
	fin6->fi6_sport = 0;
	fin6->fi6_dport = 0;

	/* locate the mbuf containing the protocol header */
	for (m0 = m; m0 != NULL; m0 = m0->m_next)
		if (((caddr_t)ip6 >= m0->m_data) &&
		    ((caddr_t)ip6 < m0->m_data + m0->m_len))
			break;
	if (m0 == NULL) {
#ifdef ALTQ_DEBUG
		printf("extract_ports6: can't locate header! ip6=%p\n", ip6);
#endif
		return (0);
	}
	off = ((caddr_t)ip6 - m0->m_data) + sizeof(struct ip6_hdr);

	proto = ip6->ip6_nxt;
	do {
		while (off >= m0->m_len) {
			off -= m0->m_len;
			m0 = m0->m_next;
			if (m0 == NULL)
				return (0);
		}
		if (m0->m_len < off + 4)
			return (0);

		switch (proto) {
		case IPPROTO_TCP:
		case IPPROTO_UDP: {
			struct udphdr *udp;

			udp = (struct udphdr *)(mtod(m0, caddr_t) + off);
			fin6->fi6_sport = udp->uh_sport;
			fin6->fi6_dport = udp->uh_dport;
			fin6->fi6_proto = proto;
			}
			return (1);

		case IPPROTO_ESP:
			if (fin6->fi6_gpi == 0) {
				u_int32_t *gpi;

				gpi = (u_int32_t *)(mtod(m0, caddr_t) + off);
				fin6->fi6_gpi   = *gpi;
			}
			fin6->fi6_proto = proto;
			return (1);

		case IPPROTO_AH: {
			/* get next header and header length */
			struct _opt6 *opt6;

			opt6 = (struct _opt6 *)(mtod(m0, caddr_t) + off);
			if (fin6->fi6_gpi == 0 && m0->m_len >= off + 8)
				fin6->fi6_gpi = opt6->ah_spi;
			proto = opt6->opt6_nxt;
			off += 8 + (opt6->opt6_hlen * 4);
			/* goto the next header */
			break;
			}

		case IPPROTO_HOPOPTS:
		case IPPROTO_ROUTING:
		case IPPROTO_DSTOPTS: {
			/* get next header and header length */
			struct _opt6 *opt6;

			opt6 = (struct _opt6 *)(mtod(m0, caddr_t) + off);
			proto = opt6->opt6_nxt;
			off += (opt6->opt6_hlen + 1) * 8;
			/* goto the next header */
			break;
			}

		case IPPROTO_FRAGMENT:
			/* ipv6 fragmentations are not supported yet */
		default:
			fin6->fi6_proto = proto;
			return (0);
		}
	} while (1);
	/*NOTREACHED*/
}
#endif /* INET6 */

/*
 * altq common classifier
 */
int
acc_add_filter(classifier, filter, class, phandle)
	struct acc_classifier *classifier;
	struct flow_filter *filter;
	void	*class;
	u_long	*phandle;
{
	struct acc_filter *afp, *prev, *tmp;
	int	i, s;

#ifdef INET6
	if (filter->ff_flow.fi_family != AF_INET &&
	    filter->ff_flow.fi_family != AF_INET6)
		return (EINVAL);
#else
	if (filter->ff_flow.fi_family != AF_INET)
		return (EINVAL);
#endif

	MALLOC(afp, struct acc_filter *, sizeof(struct acc_filter),
	       M_DEVBUF, M_WAITOK);
	if (afp == NULL)
		return (ENOMEM);
	bzero(afp, sizeof(struct acc_filter));

	afp->f_filter = *filter;
	afp->f_class = class;

	i = ACC_WILDCARD_INDEX;
	if (filter->ff_flow.fi_family == AF_INET) {
		struct flow_filter *filter4 = &afp->f_filter;

		/*
		 * if address is 0, it's a wildcard.  if address mask
		 * isn't set, use full mask.
		 */
		if (filter4->ff_flow.fi_dst.s_addr == 0)
			filter4->ff_mask.mask_dst.s_addr = 0;
		else if (filter4->ff_mask.mask_dst.s_addr == 0)
			filter4->ff_mask.mask_dst.s_addr = 0xffffffff;
		if (filter4->ff_flow.fi_src.s_addr == 0)
			filter4->ff_mask.mask_src.s_addr = 0;
		else if (filter4->ff_mask.mask_src.s_addr == 0)
			filter4->ff_mask.mask_src.s_addr = 0xffffffff;

		/* clear extra bits in addresses  */
		   filter4->ff_flow.fi_dst.s_addr &=
		       filter4->ff_mask.mask_dst.s_addr;
		   filter4->ff_flow.fi_src.s_addr &=
		       filter4->ff_mask.mask_src.s_addr;

		/*
		 * if dst address is a wildcard, use hash-entry
		 * ACC_WILDCARD_INDEX.
		 */
		if (filter4->ff_mask.mask_dst.s_addr != 0xffffffff)
			i = ACC_WILDCARD_INDEX;
		else
			i = ACC_GET_HASH_INDEX(filter4->ff_flow.fi_dst.s_addr);
	}
#ifdef INET6
	else if (filter->ff_flow.fi_family == AF_INET6) {
		struct flow_filter6 *filter6 =
			(struct flow_filter6 *)&afp->f_filter;
#ifndef IN6MASK0 /* taken from kame ipv6 */
#define	IN6MASK0	{{{ 0, 0, 0, 0 }}}
#define	IN6MASK128	{{{ 0xffffffff, 0xffffffff, 0xffffffff, 0xffffffff }}}
		const struct in6_addr in6mask0 = IN6MASK0;
		const struct in6_addr in6mask128 = IN6MASK128;
#endif

		if (IN6_IS_ADDR_UNSPECIFIED(&filter6->ff_flow6.fi6_dst))
			filter6->ff_mask6.mask6_dst = in6mask0;
		else if (IN6_IS_ADDR_UNSPECIFIED(&filter6->ff_mask6.mask6_dst))
			filter6->ff_mask6.mask6_dst = in6mask128;
		if (IN6_IS_ADDR_UNSPECIFIED(&filter6->ff_flow6.fi6_src))
			filter6->ff_mask6.mask6_src = in6mask0;
		else if (IN6_IS_ADDR_UNSPECIFIED(&filter6->ff_mask6.mask6_src))
			filter6->ff_mask6.mask6_src = in6mask128;

		/* clear extra bits in addresses  */
		for (i = 0; i < 16; i++)
			filter6->ff_flow6.fi6_dst.s6_addr[i] &=
			    filter6->ff_mask6.mask6_dst.s6_addr[i];
		for (i = 0; i < 16; i++)
			filter6->ff_flow6.fi6_src.s6_addr[i] &=
			    filter6->ff_mask6.mask6_src.s6_addr[i];

		if (filter6->ff_flow6.fi6_flowlabel == 0)
			i = ACC_WILDCARD_INDEX;
		else
			i = ACC_GET_HASH_INDEX(filter6->ff_flow6.fi6_flowlabel);
	}
#endif /* INET6 */

	afp->f_handle = get_filt_handle(classifier, i);

	/* update filter bitmask */
	afp->f_fbmask = filt2fibmask(filter);
	classifier->acc_fbmask |= afp->f_fbmask;

	/*
	 * add this filter to the filter list.
	 * filters are ordered from the highest rule number.
	 */
	s = splimp();
	prev = NULL;
	LIST_FOREACH(tmp, &classifier->acc_filters[i], f_chain) {
		if (tmp->f_filter.ff_ruleno > afp->f_filter.ff_ruleno)
			prev = tmp;
		else
			break;
	}
	if (prev == NULL)
		LIST_INSERT_HEAD(&classifier->acc_filters[i], afp, f_chain);
	else
		LIST_INSERT_AFTER(prev, afp, f_chain);
	splx(s);

	*phandle = afp->f_handle;
	return (0);
}

int
acc_delete_filter(classifier, handle)
	struct acc_classifier *classifier;
	u_long handle;
{
	struct acc_filter *afp;
	int	s;

	if ((afp = filth_to_filtp(classifier, handle)) == NULL)
		return (EINVAL);

	s = splimp();
	LIST_REMOVE(afp, f_chain);
	splx(s);

	FREE(afp, M_DEVBUF);

	/* todo: update filt_bmask */

	return (0);
}

/*
 * delete filters referencing to the specified class.
 * if the all flag is not 0, delete all the filters.
 */
int
acc_discard_filters(classifier, class, all)
	struct acc_classifier *classifier;
	void	*class;
	int	all;
{
	struct acc_filter *afp;
	int	i, s;

#if 1 /* PFALTQ */
	if (classifier == NULL)
		return (0);
#endif
	s = splimp();
	for (i = 0; i < ACC_FILTER_TABLESIZE; i++) {
		do {
			LIST_FOREACH(afp, &classifier->acc_filters[i], f_chain)
				if (all || afp->f_class == class) {
					LIST_REMOVE(afp, f_chain);
					FREE(afp, M_DEVBUF);
					/* start again from the head */
					break;
				}
		} while (afp != NULL);
	}
	splx(s);

	if (all)
		classifier->acc_fbmask = 0;

	return (0);
}

void *
acc_classify(clfier, m, af)
	void *clfier;
	struct mbuf *m;
	int af;
{
	struct acc_classifier *classifier;
	struct flowinfo flow;
	struct acc_filter *afp;
	int	i;

	classifier = (struct acc_classifier *)clfier;
	altq_extractflow(m, af, &flow, classifier->acc_fbmask);

	if (flow.fi_family == AF_INET) {
		struct flowinfo_in *fp = (struct flowinfo_in *)&flow;

		if ((classifier->acc_fbmask & FIMB4_ALL) == FIMB4_TOS) {
			/* only tos is used */
			LIST_FOREACH(afp,
				 &classifier->acc_filters[ACC_WILDCARD_INDEX],
				 f_chain)
				if (apply_tosfilter4(afp->f_fbmask,
						     &afp->f_filter, fp))
					/* filter matched */
					return (afp->f_class);
		} else if ((classifier->acc_fbmask &
			(~(FIMB4_PROTO|FIMB4_SPORT|FIMB4_DPORT) & FIMB4_ALL))
		    == 0) {
			/* only proto and ports are used */
			LIST_FOREACH(afp,
				 &classifier->acc_filters[ACC_WILDCARD_INDEX],
				 f_chain)
				if (apply_ppfilter4(afp->f_fbmask,
						    &afp->f_filter, fp))
					/* filter matched */
					return (afp->f_class);
		} else {
			/* get the filter hash entry from its dest address */
			i = ACC_GET_HASH_INDEX(fp->fi_dst.s_addr);
			do {
				/*
				 * go through this loop twice.  first for dst
				 * hash, second for wildcards.
				 */
				LIST_FOREACH(afp, &classifier->acc_filters[i],
					     f_chain)
					if (apply_filter4(afp->f_fbmask,
							  &afp->f_filter, fp))
						/* filter matched */
						return (afp->f_class);

				/*
				 * check again for filters with a dst addr
				 * wildcard.
				 * (daddr == 0 || dmask != 0xffffffff).
				 */
				if (i != ACC_WILDCARD_INDEX)
					i = ACC_WILDCARD_INDEX;
				else
					break;
			} while (1);
		}
	}
#ifdef INET6
	else if (flow.fi_family == AF_INET6) {
		struct flowinfo_in6 *fp6 = (struct flowinfo_in6 *)&flow;

		/* get the filter hash entry from its flow ID */
		if (fp6->fi6_flowlabel != 0)
			i = ACC_GET_HASH_INDEX(fp6->fi6_flowlabel);
		else
			/* flowlable can be zero */
			i = ACC_WILDCARD_INDEX;

		/* go through this loop twice.  first for flow hash, second
		   for wildcards. */
		do {
			LIST_FOREACH(afp, &classifier->acc_filters[i], f_chain)
				if (apply_filter6(afp->f_fbmask,
					(struct flow_filter6 *)&afp->f_filter,
					fp6))
					/* filter matched */
					return (afp->f_class);

			/*
			 * check again for filters with a wildcard.
			 */
			if (i != ACC_WILDCARD_INDEX)
				i = ACC_WILDCARD_INDEX;
			else
				break;
		} while (1);
	}
#endif /* INET6 */

	/* no filter matched */
	return (NULL);
}

static int
apply_filter4(fbmask, filt, pkt)
	u_int32_t	fbmask;
	struct flow_filter *filt;
	struct flowinfo_in *pkt;
{
	if (filt->ff_flow.fi_family != AF_INET)
		return (0);
	if ((fbmask & FIMB4_SPORT) && filt->ff_flow.fi_sport != pkt->fi_sport)
		return (0);
	if ((fbmask & FIMB4_DPORT) && filt->ff_flow.fi_dport != pkt->fi_dport)
		return (0);
	if ((fbmask & FIMB4_DADDR) &&
	    filt->ff_flow.fi_dst.s_addr !=
	    (pkt->fi_dst.s_addr & filt->ff_mask.mask_dst.s_addr))
		return (0);
	if ((fbmask & FIMB4_SADDR) &&
	    filt->ff_flow.fi_src.s_addr !=
	    (pkt->fi_src.s_addr & filt->ff_mask.mask_src.s_addr))
		return (0);
	if ((fbmask & FIMB4_PROTO) && filt->ff_flow.fi_proto != pkt->fi_proto)
		return (0);
	if ((fbmask & FIMB4_TOS) && filt->ff_flow.fi_tos !=
	    (pkt->fi_tos & filt->ff_mask.mask_tos))
		return (0);
	if ((fbmask & FIMB4_GPI) && filt->ff_flow.fi_gpi != (pkt->fi_gpi))
		return (0);
	/* match */
	return (1);
}

/*
 * filter matching function optimized for a common case that checks
 * only protocol and port numbers
 */
static int
apply_ppfilter4(fbmask, filt, pkt)
	u_int32_t	fbmask;
	struct flow_filter *filt;
	struct flowinfo_in *pkt;
{
	if (filt->ff_flow.fi_family != AF_INET)
		return (0);
	if ((fbmask & FIMB4_SPORT) && filt->ff_flow.fi_sport != pkt->fi_sport)
		return (0);
	if ((fbmask & FIMB4_DPORT) && filt->ff_flow.fi_dport != pkt->fi_dport)
		return (0);
	if ((fbmask & FIMB4_PROTO) && filt->ff_flow.fi_proto != pkt->fi_proto)
		return (0);
	/* match */
	return (1);
}

/*
 * filter matching function only for tos field.
 */
static int
apply_tosfilter4(fbmask, filt, pkt)
	u_int32_t	fbmask;
	struct flow_filter *filt;
	struct flowinfo_in *pkt;
{
	if (filt->ff_flow.fi_family != AF_INET)
		return (0);
	if ((fbmask & FIMB4_TOS) && filt->ff_flow.fi_tos !=
	    (pkt->fi_tos & filt->ff_mask.mask_tos))
		return (0);
	/* match */
	return (1);
}

#ifdef INET6
static int
apply_filter6(fbmask, filt, pkt)
	u_int32_t	fbmask;
	struct flow_filter6 *filt;
	struct flowinfo_in6 *pkt;
{
	int i;

	if (filt->ff_flow6.fi6_family != AF_INET6)
		return (0);
	if ((fbmask & FIMB6_FLABEL) &&
	    filt->ff_flow6.fi6_flowlabel != pkt->fi6_flowlabel)
		return (0);
	if ((fbmask & FIMB6_PROTO) &&
	    filt->ff_flow6.fi6_proto != pkt->fi6_proto)
		return (0);
	if ((fbmask & FIMB6_SPORT) &&
	    filt->ff_flow6.fi6_sport != pkt->fi6_sport)
		return (0);
	if ((fbmask & FIMB6_DPORT) &&
	    filt->ff_flow6.fi6_dport != pkt->fi6_dport)
		return (0);
	if (fbmask & FIMB6_SADDR) {
		for (i = 0; i < 4; i++)
			if (filt->ff_flow6.fi6_src.s6_addr32[i] !=
			    (pkt->fi6_src.s6_addr32[i] &
			     filt->ff_mask6.mask6_src.s6_addr32[i]))
				return (0);
	}
	if (fbmask & FIMB6_DADDR) {
		for (i = 0; i < 4; i++)
			if (filt->ff_flow6.fi6_dst.s6_addr32[i] !=
			    (pkt->fi6_dst.s6_addr32[i] &
			     filt->ff_mask6.mask6_dst.s6_addr32[i]))
				return (0);
	}
	if ((fbmask & FIMB6_TCLASS) &&
	    filt->ff_flow6.fi6_tclass !=
	    (pkt->fi6_tclass & filt->ff_mask6.mask6_tclass))
		return (0);
	if ((fbmask & FIMB6_GPI) &&
	    filt->ff_flow6.fi6_gpi != pkt->fi6_gpi)
		return (0);
	/* match */
	return (1);
}
#endif /* INET6 */

/*
 *  filter handle:
 *	bit 20-28: index to the filter hash table
 *	bit  0-19: unique id in the hash bucket.
 */
static u_long
get_filt_handle(classifier, i)
	struct acc_classifier *classifier;
	int	i;
{
	static u_long handle_number = 1;
	u_long 	handle;
	struct acc_filter *afp;

	while (1) {
		handle = handle_number++ & 0x000fffff;

		if (LIST_EMPTY(&classifier->acc_filters[i]))
			break;

		LIST_FOREACH(afp, &classifier->acc_filters[i], f_chain)
			if ((afp->f_handle & 0x000fffff) == handle)
				break;
		if (afp == NULL)
			break;
		/* this handle is already used, try again */
	}

	return ((i << 20) | handle);
}

/* convert filter handle to filter pointer */
static struct acc_filter *
filth_to_filtp(classifier, handle)
	struct acc_classifier *classifier;
	u_long handle;
{
	struct acc_filter *afp;
	int	i;

	i = ACC_GET_HINDEX(handle);

	LIST_FOREACH(afp, &classifier->acc_filters[i], f_chain)
		if (afp->f_handle == handle)
			return (afp);

	return (NULL);
}

/* create flowinfo bitmask */
static u_int32_t
filt2fibmask(filt)
	struct flow_filter *filt;
{
	u_int32_t mask = 0;
#ifdef INET6
	struct flow_filter6 *filt6;
#endif

	switch (filt->ff_flow.fi_family) {
	case AF_INET:
		if (filt->ff_flow.fi_proto != 0)
			mask |= FIMB4_PROTO;
		if (filt->ff_flow.fi_tos != 0)
			mask |= FIMB4_TOS;
		if (filt->ff_flow.fi_dst.s_addr != 0)
			mask |= FIMB4_DADDR;
		if (filt->ff_flow.fi_src.s_addr != 0)
			mask |= FIMB4_SADDR;
		if (filt->ff_flow.fi_sport != 0)
			mask |= FIMB4_SPORT;
		if (filt->ff_flow.fi_dport != 0)
			mask |= FIMB4_DPORT;
		if (filt->ff_flow.fi_gpi != 0)
			mask |= FIMB4_GPI;
		break;
#ifdef INET6
	case AF_INET6:
		filt6 = (struct flow_filter6 *)filt;

		if (filt6->ff_flow6.fi6_proto != 0)
			mask |= FIMB6_PROTO;
		if (filt6->ff_flow6.fi6_tclass != 0)
			mask |= FIMB6_TCLASS;
		if (!IN6_IS_ADDR_UNSPECIFIED(&filt6->ff_flow6.fi6_dst))
			mask |= FIMB6_DADDR;
		if (!IN6_IS_ADDR_UNSPECIFIED(&filt6->ff_flow6.fi6_src))
			mask |= FIMB6_SADDR;
		if (filt6->ff_flow6.fi6_sport != 0)
			mask |= FIMB6_SPORT;
		if (filt6->ff_flow6.fi6_dport != 0)
			mask |= FIMB6_DPORT;
		if (filt6->ff_flow6.fi6_gpi != 0)
			mask |= FIMB6_GPI;
		if (filt6->ff_flow6.fi6_flowlabel != 0)
			mask |= FIMB6_FLABEL;
		break;
#endif /* INET6 */
	}
	return (mask);
}


/*
 * helper functions to handle IPv4 fragments.
 * currently only in-sequence fragments are handled.
 *	- fragment info is cached in a LRU list.
 *	- when a first fragment is found, cache its flow info.
 *	- when a non-first fragment is found, lookup the cache.
 */

struct ip4_frag {
    TAILQ_ENTRY(ip4_frag) ip4f_chain;
    char    ip4f_valid;
    u_short ip4f_id;
    struct flowinfo_in ip4f_info;
};

static TAILQ_HEAD(ip4f_list, ip4_frag) ip4f_list; /* IPv4 fragment cache */

#define	IP4F_TABSIZE		16	/* IPv4 fragment cache size */


static void
ip4f_cache(ip, fin)
	struct ip *ip;
	struct flowinfo_in *fin;
{
	struct ip4_frag *fp;

	if (TAILQ_EMPTY(&ip4f_list)) {
		/* first time call, allocate fragment cache entries. */
		if (ip4f_init() < 0)
			/* allocation failed! */
			return;
	}

	fp = ip4f_alloc();
	fp->ip4f_id = ip->ip_id;
	fp->ip4f_info.fi_proto = ip->ip_p;
	fp->ip4f_info.fi_src.s_addr = ip->ip_src.s_addr;
	fp->ip4f_info.fi_dst.s_addr = ip->ip_dst.s_addr;

	/* save port numbers */
	fp->ip4f_info.fi_sport = fin->fi_sport;
	fp->ip4f_info.fi_dport = fin->fi_dport;
	fp->ip4f_info.fi_gpi   = fin->fi_gpi;
}

static int
ip4f_lookup(ip, fin)
	struct ip *ip;
	struct flowinfo_in *fin;
{
	struct ip4_frag *fp;

	for (fp = TAILQ_FIRST(&ip4f_list); fp != NULL && fp->ip4f_valid;
	     fp = TAILQ_NEXT(fp, ip4f_chain))
		if (ip->ip_id == fp->ip4f_id &&
		    ip->ip_src.s_addr == fp->ip4f_info.fi_src.s_addr &&
		    ip->ip_dst.s_addr == fp->ip4f_info.fi_dst.s_addr &&
		    ip->ip_p == fp->ip4f_info.fi_proto) {

			/* found the matching entry */
			fin->fi_sport = fp->ip4f_info.fi_sport;
			fin->fi_dport = fp->ip4f_info.fi_dport;
			fin->fi_gpi   = fp->ip4f_info.fi_gpi;

			if ((ntohs(ip->ip_off) & IP_MF) == 0)
				/* this is the last fragment,
				   release the entry. */
				ip4f_free(fp);

			return (1);
		}

	/* no matching entry found */
	return (0);
}

static int
ip4f_init(void)
{
	struct ip4_frag *fp;
	int i;

	TAILQ_INIT(&ip4f_list);
	for (i=0; i<IP4F_TABSIZE; i++) {
		MALLOC(fp, struct ip4_frag *, sizeof(struct ip4_frag),
		       M_DEVBUF, M_NOWAIT);
		if (fp == NULL) {
			printf("ip4f_init: can't alloc %dth entry!\n", i);
			if (i == 0)
				return (-1);
			return (0);
		}
		fp->ip4f_valid = 0;
		TAILQ_INSERT_TAIL(&ip4f_list, fp, ip4f_chain);
	}
	return (0);
}

static struct ip4_frag *
ip4f_alloc(void)
{
	struct ip4_frag *fp;

	/* reclaim an entry at the tail, put it at the head */
	fp = TAILQ_LAST(&ip4f_list, ip4f_list);
	TAILQ_REMOVE(&ip4f_list, fp, ip4f_chain);
	fp->ip4f_valid = 1;
	TAILQ_INSERT_HEAD(&ip4f_list, fp, ip4f_chain);
	return (fp);
}

static void
ip4f_free(fp)
	struct ip4_frag *fp;
{
	TAILQ_REMOVE(&ip4f_list, fp, ip4f_chain);
	fp->ip4f_valid = 0;
	TAILQ_INSERT_TAIL(&ip4f_list, fp, ip4f_chain);
}

@


1.11
log
@fix "pfctl -Fq".
after altq gets flushed, altq forgot that it was enabled since
altq is actually detached with an empty ruleset.
so, add a variable, pfaltq_running, to remember the running state
and re-enable altq when a new ruleset is loaded.

noticed, tested, and oked by henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.10 2002/11/26 01:03:34 henning Exp $	*/
a61 4
#ifdef __FreeBSD__
#include "opt_cpu.h"	/* for FreeBSD-2.2.8 to get i586_ctr_freq */
#include <machine/clock.h>
#endif
d63 1
a64 5
#ifdef __FreeBSD__
#include <machine/md_var.h>		/* for cpu_feature */
#elif defined(__NetBSD__) || defined(__OpenBSD__)
#include <machine/cpu.h>		/* for cpu_feature */
#endif
d1657 4
a1663 6
#if (defined(__i386__) || defined(__alpha__)) && !defined(ALTQ_NOPCC)

#if defined(__FreeBSD__) && defined(SMP)
#error SMP system!  use ALTQ_NOPCC option.
#endif

a1664 3
#ifdef __FreeBSD__
extern u_int32_t cycles_per_sec;	/* alpha cpu clock frequency */
#elif defined(__NetBSD__) || defined(__OpenBSD__)
a1665 1
#endif
a1666 3
#if defined(__i386__) && defined(__NetBSD__)
extern u_int64_t cpu_tsc_freq;
#endif
d1671 11
a1681 1
	/* sanity check */
d1684 11
a1694 2
	if ((cpu_feature & CPUID_TSC) == 0) {
		printf("altq: TSC isn't available! use ALTQ_NOPCC option.\n");
a1696 1
#endif
d1702 1
a1702 10
#ifdef __i386__
#ifdef __FreeBSD__
#if (__FreeBSD_version > 300000)
	machclk_freq = tsc_freq;
#else
	machclk_freq = i586_ctr_freq;
#endif
#elif defined(__NetBSD__)
	machclk_freq = (u_int32_t)cpu_tsc_freq;
#elif defined(__OpenBSD__) && (defined(I586_CPU) || defined(I686_CPU))
d1705 1
a1705 4
#elif defined(__alpha__)
#ifdef __FreeBSD__
	machclk_freq = cycles_per_sec;
#elif defined(__NetBSD__) || defined(__OpenBSD__)
a1706 1
#endif
d1737 10
a1746 7
#ifdef __alpha__
/*
 * make a 64bit counter value out of the 32bit alpha processor cycle counter.
 * read_machclk must be called within a half of its wrap-around cycle
 * (about 5 sec for 400MHz cpu) to properly detect a counter wrap-around.
 * tbr_timeout calls read_machclk once a second.
 */
d1750 8
a1757 2
	static u_int32_t last_pcc, upper;
	u_int32_t pcc;
d1759 24
a1782 15
	pcc = (u_int32_t)alpha_rpcc();
	if (pcc <= last_pcc)
		upper++;
	last_pcc = pcc;
	return (((u_int64_t)upper << 32) + pcc);
}
#endif /* __alpha__ */
#else /* !i386  && !alpha */
/* use microtime() for now */
void
init_machclk(void)
{
	machclk_freq = 1000000 << MACHCLK_SHIFT;
	machclk_per_tick = machclk_freq / hz;
	printf("altq: emulate %uHz cpu clock\n", machclk_freq);
a1783 1
#endif /* !i386 && !alpha */
@


1.10
log
@KNF
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.9 2002/10/11 09:30:30 kjc Exp $	*/
d110 2
d454 8
d463 1
a463 2
	if (error == 0 && (ifp = ifunit(a->ifname)) != NULL &&
	    ALTQ_IS_ENABLED(&ifp->if_snd)) {
@


1.9
log
@make CBQ aware of the pf API so that pf developers can play with CBQ.

for now, implemnet a tentative transition mechanism to allow the old API
and the new API to coexit.
the old API will go away when pfctl(8) becomes ready.
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.8 2002/10/08 05:12:08 kjc Exp $	*/
d200 1
a200 1
    
d222 1
a222 1
    
d320 1
a320 1
	
d475 1
a475 1
	
d602 1
a602 1
/* 
d608 1
a608 1
int 
d1677 1
a1677 1
init_machclk(void) 
@


1.8
log
@the first step of pf/altq merge.
this commit is to allow further development in both userland and kernel.

the goal is to replace altq's classifier by pf(4).
- make pf tag a queue id to mbuf and make altq read the queue id
- merge altq config into pf.conf(5)

ok dhartmei@@, henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.7 2002/07/25 20:42:53 deraadt Exp $	*/
d143 9
a151 4
	if (ALTQ_IS_ENABLED(ifq))
		return EBUSY;
	if (ALTQ_IS_ATTACHED(ifq))
		return EEXIST;
d159 5
a163 1
	ifq->altq_flags &= ALTQF_CANTCHANGE;
a445 1
#if 0 /* notyet */
a446 1
#endif
a505 1
#if 0 /* notyet */
a506 1
#endif
a527 1
#if 0 /* notyet */
a528 1
#endif
a546 1
#if 0 /* notyet */
a547 1
#endif
a565 1
#if 0 /* notyet */
a566 1
#endif
a584 1
#if 0 /* notyet */
a585 1
#endif
d1065 4
d1532 1
@


1.7
log
@support 486 which lack the pentium_mhz stuff; from rees, kjc ok
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.6 2002/03/14 01:26:26 millert Exp $	*/
d57 1
d420 175
@


1.6
log
@First round of __P removal in sys
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.5 2002/02/13 08:11:48 kjc Exp $	*/
d1523 1
a1523 1
#elif defined(__OpenBSD__)
@


1.5
log
@sync with KAME.

check if the CPU supports TSC in i386 to avoid panic
on non-pentium CPUs.
use pentium_mhz or alpha's cycles_per_usec.

add a protection against a bogus IP header length
when parsing the header.
be more careful about mbuf data length.
a backtrace reported by
"Robert John Shepherd" <robert@@reviewer.co.uk>
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.4 2001/11/06 19:53:09 miod Exp $	*/
d77 3
a79 3
static void	tbr_timeout __P((void *));
static int 	extract_ports4 __P((struct mbuf *, struct ip *,
				    struct flowinfo_in *));
d81 2
a82 2
static int 	extract_ports6 __P((struct mbuf *, struct ip6_hdr *,
				    struct flowinfo_in6 *));
d84 4
a87 4
static int	apply_filter4 __P((u_int32_t, struct flow_filter *,
				   struct flowinfo_in *));
static int	apply_ppfilter4 __P((u_int32_t, struct flow_filter *,
				     struct flowinfo_in *));
d89 2
a90 2
static int	apply_filter6 __P((u_int32_t, struct flow_filter6 *,
					   struct flowinfo_in6 *));
d92 12
a103 12
static int	apply_tosfilter4 __P((u_int32_t, struct flow_filter *,
					     struct flowinfo_in *));
static u_long	get_filt_handle __P((struct acc_classifier *, int));
static struct acc_filter *filth_to_filtp __P((struct acc_classifier *,
					      u_long));
static u_int32_t filt2fibmask __P((struct flow_filter *));

static void 	ip4f_cache __P((struct ip *, struct flowinfo_in *));
static int 	ip4f_lookup __P((struct ip *, struct flowinfo_in *));
static int 	ip4f_init __P((void));
static struct ip4_frag	*ip4f_alloc __P((void));
static void 	ip4f_free __P((struct ip4_frag *));
d105 1
a105 1
int (*altq_input) __P((struct mbuf *, int)) = NULL;
@


1.4
log
@Replace inclusion of <vm/foo.h> with the correct <uvm/bar.h> when necessary.
(Look ma, I might have broken the tree)
@
text
@d1 2
a2 2
/*	$OpenBSD: altq_subr.c,v 1.3 2001/09/07 08:47:23 kjc Exp $	*/
/*	$KAME: altq_subr.c,v 1.8 2000/12/14 08:12:46 thorpej Exp $	*/
d5 1
a5 1
 * Copyright (C) 1997-2000
d60 1
d65 8
d452 1
a452 1
		
d462 1
a462 1
    
d473 1
a473 1
		
d481 1
a481 1
		
d546 1
a546 1
	
d550 1
a550 1
	
d578 2
d581 2
a582 1
	ASSERT(m0->m_len >= off + 4);
d588 1
a588 1
		
d600 1
a600 1
			
d614 1
a614 1
			if (fin->fi_gpi == 0)
d643 1
a643 1
	
d647 1
a647 1
	
d666 2
d669 2
a670 1
		ASSERT(m0->m_len >= off + 4);
d676 1
a676 1
			
d683 1
a683 1
			
d687 1
a687 1
			
d699 1
a699 1
			if (fin6->fi6_gpi == 0)
d719 1
a719 1
				
d752 1
a752 1
		
d765 1
a765 1
		
d821 1
a821 1
		
d928 1
a928 1
	
d963 1
a963 1
				
d979 1
a979 1
	
d1317 1
a1317 1
    
d1366 1
a1366 1
	
d1387 1
a1387 1
		
d1396 1
a1396 1
		
d1452 1
a1452 1
		
d1482 6
a1488 2
/* freebsd makes clock frequency accessible */
#ifdef __alpha__
d1490 2
d1493 5
d1501 15
a1515 1
#if defined(__i386__)
d1521 5
d1527 1
d1529 3
d1533 22
a1554 24
	machclk_per_tick = machclk_freq / hz;
}
#else /* !__FreeBSD__ */
/*
 * measure Pentium TSC or Alpha PCC clock frequency 
 */
void
init_machclk(void) 
{
	static int	wait;
	struct timeval	tv_start, tv_end;
	u_int64_t	start, end, diff;
	int		timo;

	microtime(&tv_start);
	start = read_machclk();
	timo = hz;	/* 1 sec */
	(void)tsleep(&wait, PWAIT | PCATCH, "init_machclk", timo);
	microtime(&tv_end);
	end = read_machclk();
	diff = (u_int64_t)(tv_end.tv_sec - tv_start.tv_sec) * 1000000
		+ tv_end.tv_usec - tv_start.tv_usec;
	if (diff != 0)
		machclk_freq = (u_int)((end - start) * 1000000 / diff);
d1557 1
d1559 1
d1561 1
a1561 1
#endif /* !__FreeBSD__ */
d1585 1
a1585 1
init_machclk(void) 
@


1.4.2.1
log
@Sync UBC branch to -current
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$KAME: altq_subr.c,v 1.11 2002/01/11 08:11:49 kjc Exp $	*/
d5 1
a5 1
 * Copyright (C) 1997-2002
a59 1
/* machine dependent clock related includes */
a63 8
#if defined(__i386__)
#include <machine/specialreg.h>		/* for CPUID_TSC */
#ifdef __FreeBSD__
#include <machine/md_var.h>		/* for cpu_feature */
#elif defined(__NetBSD__) || defined(__OpenBSD__)
#include <machine/cpu.h>		/* for cpu_feature */
#endif
#endif /* __i386__ */
d68 3
a70 3
static void	tbr_timeout(void *);
static int 	extract_ports4(struct mbuf *, struct ip *,
				    struct flowinfo_in *);
d72 2
a73 2
static int 	extract_ports6(struct mbuf *, struct ip6_hdr *,
				    struct flowinfo_in6 *);
d75 4
a78 4
static int	apply_filter4(u_int32_t, struct flow_filter *,
				   struct flowinfo_in *);
static int	apply_ppfilter4(u_int32_t, struct flow_filter *,
				     struct flowinfo_in *);
d80 2
a81 2
static int	apply_filter6(u_int32_t, struct flow_filter6 *,
					   struct flowinfo_in6 *);
d83 12
a94 12
static int	apply_tosfilter4(u_int32_t, struct flow_filter *,
					     struct flowinfo_in *);
static u_long	get_filt_handle(struct acc_classifier *, int);
static struct acc_filter *filth_to_filtp(struct acc_classifier *,
					      u_long);
static u_int32_t filt2fibmask(struct flow_filter *);

static void 	ip4f_cache(struct ip *, struct flowinfo_in *);
static int 	ip4f_lookup(struct ip *, struct flowinfo_in *);
static int 	ip4f_init(void);
static struct ip4_frag	*ip4f_alloc(void);
static void 	ip4f_free(struct ip4_frag *);
d96 1
a96 1
int (*altq_input)(struct mbuf *, int) = NULL;
d443 1
a443 1

d453 1
a453 1

d464 1
a464 1

d472 1
a472 1

d537 1
a537 1

d541 1
a541 1

a568 2
		if (m0 == NULL)
			return (0);  /* bogus ip_hl! */
d570 1
a570 2
	if (m0->m_len < off + 4)
		return (0);
d576 1
a576 1

d588 1
a588 1

d602 1
a602 1
			if (fin->fi_gpi == 0 && m0->m_len >= off + 8)
d631 1
a631 1

d635 1
a635 1

a653 2
			if (m0 == NULL)
				return (0);
d655 1
a655 2
		if (m0->m_len < off + 4)
			return (0);
d661 1
a661 1

d668 1
a668 1

d672 1
a672 1

d684 1
a684 1
			if (fin6->fi6_gpi == 0 && m0->m_len >= off + 8)
d704 1
a704 1

d737 1
a737 1

d750 1
a750 1

d806 1
a806 1

d913 1
a913 1

d948 1
a948 1

d964 1
a964 1

d1302 1
a1302 1

d1351 1
a1351 1

d1372 1
a1372 1

d1381 1
a1381 1

d1437 1
a1437 1

d1467 2
a1468 5

#if defined(__FreeBSD__) && defined(SMP)
#error SMP system!  use ALTQ_NOPCC option.
#endif

a1469 1
#ifdef __FreeBSD__
a1470 2
#elif defined(__NetBSD__) || defined(__OpenBSD__)
extern u_int64_t cycles_per_usec;	/* alpha cpu clock frequency */
a1471 5
#endif /* __alpha__ */
#if defined(__i386__) && defined(__NetBSD__)
extern u_int64_t cpu_tsc_freq;
#endif

d1475 1
a1475 15
	/* sanity check */
#ifdef __i386__
	/* check if TSC is available */
	if ((cpu_feature & CPUID_TSC) == 0) {
		printf("altq: TSC isn't available! use ALTQ_NOPCC option.\n");
		return;
	}
#endif

	/*
	 * if the clock frequency (of Pentium TSC or Alpha PCC) is
	 * accessible, just use it.
	 */
#ifdef __i386__
#ifdef __FreeBSD__
a1480 5
#elif defined(__NetBSD__)
	machclk_freq = (u_int32_t)cpu_tsc_freq;
#elif defined(__OpenBSD__)
	machclk_freq = pentium_mhz * 1000000;
#endif
a1481 1
#ifdef __FreeBSD__
a1482 3
#elif defined(__NetBSD__) || defined(__OpenBSD__)
	machclk_freq = (u_int32_t)(cycles_per_usec * 1000000);
#endif
d1484 24
a1507 22

	/*
	 * if we don't know the clock frequency, measure it.
	 */
	if (machclk_freq == 0) {
		static int	wait;
		struct timeval	tv_start, tv_end;
		u_int64_t	start, end, diff;
		int		timo;

		microtime(&tv_start);
		start = read_machclk();
		timo = hz;	/* 1 sec */
		(void)tsleep(&wait, PWAIT | PCATCH, "init_machclk", timo);
		microtime(&tv_end);
		end = read_machclk();
		diff = (u_int64_t)(tv_end.tv_sec - tv_start.tv_sec) * 1000000
		    + tv_end.tv_usec - tv_start.tv_usec;
		if (diff != 0)
			machclk_freq = (u_int)((end - start) * 1000000 / diff);
	}

a1509 1
#ifdef ALTQ_DEBUG
a1510 1
#endif
d1512 1
a1512 1

d1536 1
a1536 1
init_machclk(void)
@


1.4.2.2
log
@sync to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.4.2.1 2002/06/11 03:27:42 art Exp $	*/
a56 1
#include <net/pfvar.h>
d142 4
a145 9

	if (PFALTQ_IS_ACTIVE()) {
		/* pfaltq can override the existing discipline */
	} else {
		if (ALTQ_IS_ENABLED(ifq))
			return EBUSY;
		if (ALTQ_IS_ATTACHED(ifq))
			return EEXIST;
	}
d153 1
a153 5
	if (PFALTQ_IS_ACTIVE())
		ifq->altq_flags &= (ALTQF_CANTCHANGE|ALTQF_ENABLED);
	else
		ifq->altq_flags &= ALTQF_CANTCHANGE;

a420 163
/*
 * attach a discipline to the interface.  if one already exists, it is
 * overridden.
 */
int
altq_pfattach(struct pf_altq *a)
{
	struct ifnet *ifp;
	struct tb_profile tb;
	int s, error = 0;

	switch (a->scheduler) {
	case ALTQT_NONE:
		break;
	case ALTQT_CBQ:
		error = cbq_pfattach(a);
		break;
	default:
		error = EINVAL;
	}

	/* if altq is already enabled, reset set tokenbucket regulator */
	if (error == 0 && (ifp = ifunit(a->ifname)) != NULL &&
	    ALTQ_IS_ENABLED(&ifp->if_snd)) {
		tb.rate = a->ifbandwidth;
		tb.depth = a->tbrsize;
		s = splimp();
		error = tbr_set(&ifp->if_snd, &tb);
		splx(s);
	}

	return (error);
}

/*
 * detach a discipline from the interface.
 * it is possible that the discipline was already overridden by another
 * discipline.
 */
int
altq_pfdetach(struct pf_altq *a)
{
	struct ifnet *ifp;
	int s, error = 0;
	
	if ((ifp = ifunit(a->ifname)) == NULL)
		return (EINVAL);

	/* if this discipline is no longer referenced, just return */
	if (a->altq_disc == NULL || a->altq_disc != ifp->if_snd.altq_disc)
		return (0);

	s = splimp();
	if (ALTQ_IS_ENABLED(&ifp->if_snd))
		error = altq_disable(&ifp->if_snd);
	if (error == 0)
		error = altq_detach(&ifp->if_snd);
	splx(s);

	return (error);
}

/*
 * add a discipline or a queue
 */
int
altq_add(struct pf_altq *a)
{
	int error = 0;

	if (a->qname[0] != 0)
		return (altq_add_queue(a));

	switch (a->scheduler) {
	case ALTQT_CBQ:
		error = cbq_add_altq(a);
		break;
	default:
		error = EINVAL;
	}

	return (error);
}

/*
 * remove a discipline or a queue
 */
int
altq_remove(struct pf_altq *a)
{
	int error = 0;

	if (a->qname[0] != 0)
		return (altq_remove_queue(a));

	switch (a->scheduler) {
	case ALTQT_CBQ:
		error = cbq_remove_altq(a);
		break;
	default:
		error = EINVAL;
	}

	return (error);
}

/*
 * add a queue to the discipline
 */
int
altq_add_queue(struct pf_altq *a)
{
	int error = 0;

	switch (a->scheduler) {
	case ALTQT_CBQ:
		error = cbq_add_queue(a);
		break;
	default:
		error = EINVAL;
	}

	return (error);
}

/*
 * remove a queue from the discipline
 */
int
altq_remove_queue(struct pf_altq *a)
{
	int error = 0;

	switch (a->scheduler) {
	case ALTQT_CBQ:
		error = cbq_remove_queue(a);
		break;
	default:
		error = EINVAL;
	}

	return (error);
}

/*
 * get queue statistics
 */
int
altq_getqstats(struct pf_altq *a, void *ubuf, int *nbytes)
{
	int error = 0;

	switch (a->scheduler) {
	case ALTQT_CBQ:
		error = cbq_getqstats(a, ubuf, nbytes);
		break;
	default:
		error = EINVAL;
	}

	return (error);
}

a891 4
#if 1 /* PFALTQ */
	if (classifier == NULL)
		return (0);
#endif
a1355 1

d1523 1
a1523 1
#elif defined(__OpenBSD__) && (defined(I586_CPU) || defined(I686_CPU))
@


1.4.2.3
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d59 1
d62 4
a66 1
#include <machine/cpufunc.h>		/* for pentium tsc */
d68 5
d79 27
a109 2
int pfaltq_running;	/* keep track of running state */

d144 8
a151 6
#if 0	/* pfaltq can override the existing discipline */
	if (ALTQ_IS_ENABLED(ifq))
		return EBUSY;
	if (ALTQ_IS_ATTACHED(ifq))
		return EEXIST;
#endif
d159 4
a162 1
	ifq->altq_flags &= (ALTQF_CANTCHANGE|ALTQF_ENABLED);
d164 3
d181 3
d200 1
a200 1

d222 1
a222 1

d247 1
a247 1
 *	rate:	byte_per_unittime << 32
d320 1
a320 1

d373 10
a382 1
	for (ifp = TAILQ_FIRST(&ifnet); ifp; ifp = TAILQ_NEXT(ifp, if_list)) {
a444 1
#ifdef ALTQ_CBQ
a447 11
#endif
#ifdef ALTQ_PRIQ
	case ALTQT_PRIQ:
		error = priq_pfattach(a);
		break;
#endif
#ifdef ALTQ_HFSC
	case ALTQT_HFSC:
		error = hfsc_pfattach(a);
		break;
#endif
d449 1
a449 1
		error = ENXIO;
a451 8
	ifp = ifunit(a->ifname);

	/* if the state is running, enable altq */
	if (error == 0 && pfaltq_running &&
	    ifp != NULL && ifp->if_snd.altq_type != ALTQT_NONE &&
	    !ALTQ_IS_ENABLED(&ifp->if_snd))
			error = altq_enable(&ifp->if_snd);

d453 2
a454 1
	if (error == 0 && ifp != NULL && ALTQ_IS_ENABLED(&ifp->if_snd)) {
d475 1
a475 1

a503 5
	if (machclk_freq == 0)
		init_machclk();
	if (machclk_freq == 0)
		panic("altq_add: no cpu clock");

a504 1
#ifdef ALTQ_CBQ
a507 11
#endif
#ifdef ALTQ_PRIQ
	case ALTQT_PRIQ:
		error = priq_add_altq(a);
		break;
#endif
#ifdef ALTQ_HFSC
	case ALTQT_HFSC:
		error = hfsc_add_altq(a);
		break;
#endif
d509 1
a509 1
		error = ENXIO;
a526 1
#ifdef ALTQ_CBQ
a529 11
#endif
#ifdef ALTQ_PRIQ
	case ALTQT_PRIQ:
		error = priq_remove_altq(a);
		break;
#endif
#ifdef ALTQ_HFSC
	case ALTQT_HFSC:
		error = hfsc_remove_altq(a);
		break;
#endif
d531 1
a531 1
		error = ENXIO;
a545 1
#ifdef ALTQ_CBQ
a548 11
#endif
#ifdef ALTQ_PRIQ
	case ALTQT_PRIQ:
		error = priq_add_queue(a);
		break;
#endif
#ifdef ALTQ_HFSC
	case ALTQT_HFSC:
		error = hfsc_add_queue(a);
		break;
#endif
d550 1
a550 1
		error = ENXIO;
a564 1
#ifdef ALTQ_CBQ
a567 11
#endif
#ifdef ALTQ_PRIQ
	case ALTQT_PRIQ:
		error = priq_remove_queue(a);
		break;
#endif
#ifdef ALTQ_HFSC
	case ALTQT_HFSC:
		error = hfsc_remove_queue(a);
		break;
#endif
d569 1
a569 1
		error = ENXIO;
a583 1
#ifdef ALTQ_CBQ
d587 10
d598 87
a684 3
#ifdef ALTQ_PRIQ
	case ALTQT_PRIQ:
		error = priq_getqstats(a, ubuf, nbytes);
d686 53
d740 38
a777 3
#ifdef ALTQ_HFSC
	case ALTQT_HFSC:
		error = hfsc_getqstats(a, ubuf, nbytes);
d779 289
d1069 438
a1506 2
	default:
		error = ENXIO;
d1508 2
d1511 20
a1530 1
	return (error);
d1533 1
a1655 4
/* if pcc is not available or disabled, emulate 256MHz using microtime() */
#define	MACHCLK_SHIFT	8

int machclk_usepcc;
d1659 6
d1666 3
d1670 1
d1672 3
d1677 1
a1677 1
init_machclk(void)
d1679 1
a1679 11
	machclk_usepcc = 1;

#if (!defined(__i386__) && !defined(__alpha__)) || defined(ALTQ_NOPCC)
	machclk_usepcc = 0;
#endif
#if defined(__FreeBSD__) && defined(SMP)
	machclk_usepcc = 0;
#endif
#if defined(__NetBSD__) && defined(MULTIPROCESSOR)
	machclk_usepcc = 0;
#endif
d1682 2
a1683 11
	if (machclk_usepcc == 1 && (cpu_feature & CPUID_TSC) == 0)
		machclk_usepcc = 0;
#endif

	if (machclk_usepcc == 0) {
		/* emulate 256MHz using microtime() */
		machclk_freq = 1000000 << MACHCLK_SHIFT;
		machclk_per_tick = machclk_freq / hz;
#ifdef ALTQ_DEBUG
		printf("altq: emulate %uHz cpu clock\n", machclk_freq);
#endif
d1686 1
d1692 10
a1701 1
#if defined(__i386__) && (defined(I586_CPU) || defined(I686_CPU))
d1704 4
a1707 1
#if defined(__alpha__)
d1709 1
d1740 7
a1746 10
#if defined(__OpenBSD__) && defined(__i386__)
static __inline u_int64_t
rdtsc(void)
{
	u_int64_t rv;
	__asm __volatile(".byte 0x0f, 0x31" : "=A" (rv));
	return (rv);
}
#endif /* __OpenBSD__ && __i386__ */

d1750 2
a1751 1
	u_int64_t val;
d1753 15
a1767 31
	if (machclk_usepcc) {
#if defined(__i386__)
		val = rdtsc();
#elif defined(__alpha__)
		static u_int32_t last_pcc, upper;
		u_int32_t pcc;

		/*
		 * for alpha, make a 64bit counter value out of the 32bit
		 * alpha processor cycle counter.
		 * read_machclk must be called within a half of its
		 * wrap-around cycle (about 5 sec for 400MHz cpu) to properly
		 * detect a counter wrap-around.
		 * tbr_timeout calls read_machclk once a second.
		 */
		pcc = (u_int32_t)alpha_rpcc();
		if (pcc <= last_pcc)
			upper++;
		last_pcc = pcc;
		val = ((u_int64_t)upper << 32) + pcc;
#else
		panic("read_machclk");
#endif
	} else {
		struct timeval tv;

		microtime(&tv);
		val = (((u_int64_t)(tv.tv_sec - boottime.tv_sec) * 1000000
		    + tv.tv_usec) << MACHCLK_SHIFT);
	}
	return (val);
d1769 1
@


1.3
log
@fix IPv4 fragment handling in the classifier.
report and fix by "Paul Koster" <list@@kosteronline.net>
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.2 2001/08/09 14:32:59 deraadt Exp $	*/
d40 1
a40 1
#include <vm/vm.h>
@


1.2
log
@change a maze of altq options into just "altq" for the base+red+cbq, and
then altq_* for each of the other * experimental options.  and.. enable
it by default in GENERIC.
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.1 2001/06/27 05:28:36 kjc Exp $	*/
d1256 3
@


1.1
log
@import ALTQ, alternate queueing support, from KAME.
ALTQ allows to switch various queueing disciplines on output network
interfaces.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a29 11
#ifdef ALTQ
#if defined(__FreeBSD__) || defined(__NetBSD__)
#include "opt_altq.h"
#if (__FreeBSD__ != 2)
#include "opt_inet.h"
#ifdef __FreeBSD__
#include "opt_inet6.h"
#endif
#endif
#endif /* __FreeBSD__ || __NetBSD__ */

a1539 2

#endif /* ALTQ */
@


1.1.2.1
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_subr.c,v 1.1 2001/06/27 05:28:36 kjc Exp $	*/
d30 11
a1266 3
	fp->ip4f_info.fi_proto = ip->ip_p;
	fp->ip4f_info.fi_src.s_addr = ip->ip_src.s_addr;
	fp->ip4f_info.fi_dst.s_addr = ip->ip_dst.s_addr;
d1551 2
@


1.1.2.2
log
@Merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d40 1
a40 1
#include <uvm/uvm_extern.h>
@


1.1.2.3
log
@Merge in trunk
@
text
@d2 1
a2 1
/*	$KAME: altq_subr.c,v 1.11 2002/01/11 08:11:49 kjc Exp $	*/
d5 1
a5 1
 * Copyright (C) 1997-2002
a59 1
/* machine dependent clock related includes */
a63 8
#if defined(__i386__)
#include <machine/specialreg.h>		/* for CPUID_TSC */
#ifdef __FreeBSD__
#include <machine/md_var.h>		/* for cpu_feature */
#elif defined(__NetBSD__) || defined(__OpenBSD__)
#include <machine/cpu.h>		/* for cpu_feature */
#endif
#endif /* __i386__ */
d443 1
a443 1

d453 1
a453 1

d464 1
a464 1

d472 1
a472 1

d537 1
a537 1

d541 1
a541 1

a568 2
		if (m0 == NULL)
			return (0);  /* bogus ip_hl! */
d570 1
a570 2
	if (m0->m_len < off + 4)
		return (0);
d576 1
a576 1

d588 1
a588 1

d602 1
a602 1
			if (fin->fi_gpi == 0 && m0->m_len >= off + 8)
d631 1
a631 1

d635 1
a635 1

a653 2
			if (m0 == NULL)
				return (0);
d655 1
a655 2
		if (m0->m_len < off + 4)
			return (0);
d661 1
a661 1

d668 1
a668 1

d672 1
a672 1

d684 1
a684 1
			if (fin6->fi6_gpi == 0 && m0->m_len >= off + 8)
d704 1
a704 1

d737 1
a737 1

d750 1
a750 1

d806 1
a806 1

d913 1
a913 1

d948 1
a948 1

d964 1
a964 1

d1302 1
a1302 1

d1351 1
a1351 1

d1372 1
a1372 1

d1381 1
a1381 1

d1437 1
a1437 1

d1467 2
a1468 5

#if defined(__FreeBSD__) && defined(SMP)
#error SMP system!  use ALTQ_NOPCC option.
#endif

a1469 1
#ifdef __FreeBSD__
a1470 2
#elif defined(__NetBSD__) || defined(__OpenBSD__)
extern u_int64_t cycles_per_usec;	/* alpha cpu clock frequency */
a1471 5
#endif /* __alpha__ */
#if defined(__i386__) && defined(__NetBSD__)
extern u_int64_t cpu_tsc_freq;
#endif

d1475 1
a1475 15
	/* sanity check */
#ifdef __i386__
	/* check if TSC is available */
	if ((cpu_feature & CPUID_TSC) == 0) {
		printf("altq: TSC isn't available! use ALTQ_NOPCC option.\n");
		return;
	}
#endif

	/*
	 * if the clock frequency (of Pentium TSC or Alpha PCC) is
	 * accessible, just use it.
	 */
#ifdef __i386__
#ifdef __FreeBSD__
a1480 5
#elif defined(__NetBSD__)
	machclk_freq = (u_int32_t)cpu_tsc_freq;
#elif defined(__OpenBSD__)
	machclk_freq = pentium_mhz * 1000000;
#endif
a1481 1
#ifdef __FreeBSD__
a1482 3
#elif defined(__NetBSD__) || defined(__OpenBSD__)
	machclk_freq = (u_int32_t)(cycles_per_usec * 1000000);
#endif
d1484 24
a1507 22

	/*
	 * if we don't know the clock frequency, measure it.
	 */
	if (machclk_freq == 0) {
		static int	wait;
		struct timeval	tv_start, tv_end;
		u_int64_t	start, end, diff;
		int		timo;

		microtime(&tv_start);
		start = read_machclk();
		timo = hz;	/* 1 sec */
		(void)tsleep(&wait, PWAIT | PCATCH, "init_machclk", timo);
		microtime(&tv_end);
		end = read_machclk();
		diff = (u_int64_t)(tv_end.tv_sec - tv_start.tv_sec) * 1000000
		    + tv_end.tv_usec - tv_start.tv_usec;
		if (diff != 0)
			machclk_freq = (u_int)((end - start) * 1000000 / diff);
	}

a1509 1
#ifdef ALTQ_DEBUG
a1510 1
#endif
d1512 1
a1512 1

d1536 1
a1536 1
init_machclk(void)
@


1.1.2.4
log
@Merge in -current from about a week ago
@
text
@d77 3
a79 3
static void	tbr_timeout(void *);
static int 	extract_ports4(struct mbuf *, struct ip *,
				    struct flowinfo_in *);
d81 2
a82 2
static int 	extract_ports6(struct mbuf *, struct ip6_hdr *,
				    struct flowinfo_in6 *);
d84 4
a87 4
static int	apply_filter4(u_int32_t, struct flow_filter *,
				   struct flowinfo_in *);
static int	apply_ppfilter4(u_int32_t, struct flow_filter *,
				     struct flowinfo_in *);
d89 2
a90 2
static int	apply_filter6(u_int32_t, struct flow_filter6 *,
					   struct flowinfo_in6 *);
d92 6
a97 6
static int	apply_tosfilter4(u_int32_t, struct flow_filter *,
					     struct flowinfo_in *);
static u_long	get_filt_handle(struct acc_classifier *, int);
static struct acc_filter *filth_to_filtp(struct acc_classifier *,
					      u_long);
static u_int32_t filt2fibmask(struct flow_filter *);
d99 5
a103 5
static void 	ip4f_cache(struct ip *, struct flowinfo_in *);
static int 	ip4f_lookup(struct ip *, struct flowinfo_in *);
static int 	ip4f_init(void);
static struct ip4_frag	*ip4f_alloc(void);
static void 	ip4f_free(struct ip4_frag *);
d105 1
a105 1
int (*altq_input)(struct mbuf *, int) = NULL;
@


1.1.2.5
log
@Sync the SMP branch with 3.3
@
text
@a56 1
#include <net/pfvar.h>
d58 1
d61 4
a65 1
#include <machine/cpufunc.h>		/* for pentium tsc */
d67 5
d78 27
a108 2
int pfaltq_running;	/* keep track of running state */

a141 2

#if 0	/* pfaltq can override the existing discipline */
a145 1
#endif
d153 4
a156 2
	ifq->altq_flags &= (ALTQF_CANTCHANGE|ALTQF_ENABLED);

d171 3
d190 1
a190 1

d212 1
a212 1

d237 1
a237 1
 *	rate:	byte_per_unittime << 32
d310 1
a310 1

d363 10
a372 1
	for (ifp = TAILQ_FIRST(&ifnet); ifp; ifp = TAILQ_NEXT(ifp, if_list)) {
d421 112
d534 1
a534 2
 * attach a discipline to the interface.  if one already exists, it is
 * overridden.
d536 5
a540 2
int
altq_pfattach(struct pf_altq *a)
d542 29
a570 3
	struct ifnet *ifp;
	struct tb_profile tb;
	int s, error = 0;
d572 2
a573 7
	switch (a->scheduler) {
	case ALTQT_NONE:
		break;
#ifdef ALTQ_CBQ
	case ALTQT_CBQ:
		error = cbq_pfattach(a);
		break;
d575 19
a593 3
#ifdef ALTQ_PRIQ
	case ALTQT_PRIQ:
		error = priq_pfattach(a);
d595 10
a604 4
#endif
#ifdef ALTQ_HFSC
	case ALTQT_HFSC:
		error = hfsc_pfattach(a);
d606 15
a620 1
#endif
d622 2
a623 1
		error = ENXIO;
d626 3
a628 1
	ifp = ifunit(a->ifname);
d630 2
a631 5
	/* if the state is running, enable altq */
	if (error == 0 && pfaltq_running &&
	    ifp != NULL && ifp->if_snd.altq_type != ALTQT_NONE &&
	    !ALTQ_IS_ENABLED(&ifp->if_snd))
			error = altq_enable(&ifp->if_snd);
d633 25
a657 7
	/* if altq is already enabled, reset set tokenbucket regulator */
	if (error == 0 && ifp != NULL && ALTQ_IS_ENABLED(&ifp->if_snd)) {
		tb.rate = a->ifbandwidth;
		tb.depth = a->tbrsize;
		s = splimp();
		error = tbr_set(&ifp->if_snd, &tb);
		splx(s);
d659 60
d720 8
a727 1
	return (error);
d729 1
d732 1
a732 3
 * detach a discipline from the interface.
 * it is possible that the discipline was already overridden by another
 * discipline.
d735 5
a739 1
altq_pfdetach(struct pf_altq *a)
d741 2
a742 2
	struct ifnet *ifp;
	int s, error = 0;
d744 6
a749 1
	if ((ifp = ifunit(a->ifname)) == NULL)
d751 10
d762 72
a833 3
	/* if this discipline is no longer referenced, just return */
	if (a->altq_disc == NULL || a->altq_disc != ifp->if_snd.altq_disc)
		return (0);
d835 4
d840 11
a850 4
	if (ALTQ_IS_ENABLED(&ifp->if_snd))
		error = altq_disable(&ifp->if_snd);
	if (error == 0)
		error = altq_detach(&ifp->if_snd);
d853 2
a854 1
	return (error);
a856 3
/*
 * add a discipline or a queue
 */
d858 3
a860 1
altq_add(struct pf_altq *a)
d862 2
a863 1
	int error = 0;
d865 2
a866 2
	if (a->qname[0] != 0)
		return (altq_add_queue(a));
d868 5
a872 4
	if (machclk_freq == 0)
		init_machclk();
	if (machclk_freq == 0)
		panic("altq_add: no cpu clock");
d874 1
a874 19
	switch (a->scheduler) {
#ifdef ALTQ_CBQ
	case ALTQT_CBQ:
		error = cbq_add_altq(a);
		break;
#endif
#ifdef ALTQ_PRIQ
	case ALTQT_PRIQ:
		error = priq_add_altq(a);
		break;
#endif
#ifdef ALTQ_HFSC
	case ALTQT_HFSC:
		error = hfsc_add_altq(a);
		break;
#endif
	default:
		error = ENXIO;
	}
d876 1
a876 1
	return (error);
d880 2
a881 1
 * remove a discipline or a queue
d884 4
a887 1
altq_remove(struct pf_altq *a)
d889 16
a904 1
	int error = 0;
d906 2
a907 2
	if (a->qname[0] != 0)
		return (altq_remove_queue(a));
d909 66
a974 18
	switch (a->scheduler) {
#ifdef ALTQ_CBQ
	case ALTQT_CBQ:
		error = cbq_remove_altq(a);
		break;
#endif
#ifdef ALTQ_PRIQ
	case ALTQT_PRIQ:
		error = priq_remove_altq(a);
		break;
#endif
#ifdef ALTQ_HFSC
	case ALTQT_HFSC:
		error = hfsc_remove_altq(a);
		break;
#endif
	default:
		error = ENXIO;
d976 66
d1043 20
a1062 1
	return (error);
d1066 1
a1066 1
 * add a queue to the discipline
d1068 5
a1072 2
int
altq_add_queue(struct pf_altq *a)
d1074 8
a1081 1
	int error = 0;
d1083 8
a1090 19
	switch (a->scheduler) {
#ifdef ALTQ_CBQ
	case ALTQT_CBQ:
		error = cbq_add_queue(a);
		break;
#endif
#ifdef ALTQ_PRIQ
	case ALTQT_PRIQ:
		error = priq_add_queue(a);
		break;
#endif
#ifdef ALTQ_HFSC
	case ALTQT_HFSC:
		error = hfsc_add_queue(a);
		break;
#endif
	default:
		error = ENXIO;
	}
d1092 37
a1128 1
	return (error);
d1130 1
d1133 3
a1135 1
 * remove a queue from the discipline
d1137 31
a1167 2
int
altq_remove_queue(struct pf_altq *a)
d1169 4
a1172 1
	int error = 0;
d1174 15
a1188 5
	switch (a->scheduler) {
#ifdef ALTQ_CBQ
	case ALTQT_CBQ:
		error = cbq_remove_queue(a);
		break;
d1190 17
a1206 3
#ifdef ALTQ_PRIQ
	case ALTQT_PRIQ:
		error = priq_remove_queue(a);
d1208 20
a1227 4
#endif
#ifdef ALTQ_HFSC
	case ALTQT_HFSC:
		error = hfsc_remove_queue(a);
d1229 1
a1229 3
#endif
	default:
		error = ENXIO;
d1231 2
a1233 2
	return (error);
}
d1236 5
a1240 1
 * get queue statistics
d1242 72
a1313 2
int
altq_getqstats(struct pf_altq *a, void *ubuf, int *nbytes)
d1315 2
a1316 1
	int error = 0;
d1318 12
a1329 18
	switch (a->scheduler) {
#ifdef ALTQ_CBQ
	case ALTQT_CBQ:
		error = cbq_getqstats(a, ubuf, nbytes);
		break;
#endif
#ifdef ALTQ_PRIQ
	case ALTQT_PRIQ:
		error = priq_getqstats(a, ubuf, nbytes);
		break;
#endif
#ifdef ALTQ_HFSC
	case ALTQT_HFSC:
		error = hfsc_getqstats(a, ubuf, nbytes);
		break;
#endif
	default:
		error = ENXIO;
d1331 15
d1347 7
a1353 1
	return (error);
a1477 4
/* if pcc is not available or disabled, emulate 256MHz using microtime() */
#define	MACHCLK_SHIFT	8

int machclk_usepcc;
d1481 6
d1488 3
d1492 1
d1494 3
d1499 1
a1499 1
init_machclk(void)
d1501 1
a1501 11
	machclk_usepcc = 1;

#if (!defined(__i386__) && !defined(__alpha__)) || defined(ALTQ_NOPCC)
	machclk_usepcc = 0;
#endif
#if defined(__FreeBSD__) && defined(SMP)
	machclk_usepcc = 0;
#endif
#if defined(__NetBSD__) && defined(MULTIPROCESSOR)
	machclk_usepcc = 0;
#endif
d1504 2
a1505 11
	if (machclk_usepcc == 1 && (cpu_feature & CPUID_TSC) == 0)
		machclk_usepcc = 0;
#endif

	if (machclk_usepcc == 0) {
		/* emulate 256MHz using microtime() */
		machclk_freq = 1000000 << MACHCLK_SHIFT;
		machclk_per_tick = machclk_freq / hz;
#ifdef ALTQ_DEBUG
		printf("altq: emulate %uHz cpu clock\n", machclk_freq);
#endif
d1508 1
d1514 10
a1523 1
#if defined(__i386__) && (defined(I586_CPU) || defined(I686_CPU))
d1526 4
a1529 1
#if defined(__alpha__)
d1531 1
d1562 7
a1568 10
#if defined(__OpenBSD__) && defined(__i386__)
static __inline u_int64_t
rdtsc(void)
{
	u_int64_t rv;
	__asm __volatile(".byte 0x0f, 0x31" : "=A" (rv));
	return (rv);
}
#endif /* __OpenBSD__ && __i386__ */

d1572 2
a1573 1
	u_int64_t val;
d1575 15
a1589 31
	if (machclk_usepcc) {
#if defined(__i386__)
		val = rdtsc();
#elif defined(__alpha__)
		static u_int32_t last_pcc, upper;
		u_int32_t pcc;

		/*
		 * for alpha, make a 64bit counter value out of the 32bit
		 * alpha processor cycle counter.
		 * read_machclk must be called within a half of its
		 * wrap-around cycle (about 5 sec for 400MHz cpu) to properly
		 * detect a counter wrap-around.
		 * tbr_timeout calls read_machclk once a second.
		 */
		pcc = (u_int32_t)alpha_rpcc();
		if (pcc <= last_pcc)
			upper++;
		last_pcc = pcc;
		val = ((u_int64_t)upper << 32) + pcc;
#else
		panic("read_machclk");
#endif
	} else {
		struct timeval tv;

		microtime(&tv);
		val = (((u_int64_t)(tv.tv_sec - boottime.tv_sec) * 1000000
		    + tv.tv_usec) << MACHCLK_SHIFT);
	}
	return (val);
d1591 1
@


1.1.2.6
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d40 1
d80 1
a80 1
/* look up the queue state by the interface name and the queueing type. */
@


1.1.2.7
log
@Merge with the trunk
@
text
@d73 2
d383 3
a385 1
	int error = 0;
d407 17
@


