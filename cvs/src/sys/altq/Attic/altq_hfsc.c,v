head	1.31;
access;
symbols
	OPENBSD_5_5:1.30.0.4
	OPENBSD_5_5_BASE:1.30
	OPENBSD_5_4:1.29.0.8
	OPENBSD_5_4_BASE:1.29
	OPENBSD_5_3:1.29.0.6
	OPENBSD_5_3_BASE:1.29
	OPENBSD_5_2:1.29.0.4
	OPENBSD_5_2_BASE:1.29
	OPENBSD_5_1_BASE:1.29
	OPENBSD_5_1:1.29.0.2
	OPENBSD_5_0:1.28.0.2
	OPENBSD_5_0_BASE:1.28
	OPENBSD_4_9:1.26.0.12
	OPENBSD_4_9_BASE:1.26
	OPENBSD_4_8:1.26.0.10
	OPENBSD_4_8_BASE:1.26
	OPENBSD_4_7:1.26.0.6
	OPENBSD_4_7_BASE:1.26
	OPENBSD_4_6:1.26.0.8
	OPENBSD_4_6_BASE:1.26
	OPENBSD_4_5:1.26.0.4
	OPENBSD_4_5_BASE:1.26
	OPENBSD_4_4:1.26.0.2
	OPENBSD_4_4_BASE:1.26
	OPENBSD_4_3:1.25.0.2
	OPENBSD_4_3_BASE:1.25
	OPENBSD_4_2:1.24.0.2
	OPENBSD_4_2_BASE:1.24
	OPENBSD_4_1:1.23.0.4
	OPENBSD_4_1_BASE:1.23
	OPENBSD_4_0:1.23.0.2
	OPENBSD_4_0_BASE:1.23
	OPENBSD_3_9:1.22.0.2
	OPENBSD_3_9_BASE:1.22
	OPENBSD_3_8:1.21.0.8
	OPENBSD_3_8_BASE:1.21
	OPENBSD_3_7:1.21.0.6
	OPENBSD_3_7_BASE:1.21
	OPENBSD_3_6:1.21.0.4
	OPENBSD_3_6_BASE:1.21
	SMP_SYNC_A:1.21
	SMP_SYNC_B:1.21
	OPENBSD_3_5:1.21.0.2
	OPENBSD_3_5_BASE:1.21
	OPENBSD_3_4:1.19.0.2
	OPENBSD_3_4_BASE:1.19
	UBC_SYNC_A:1.18
	OPENBSD_3_3:1.13.0.2
	OPENBSD_3_3_BASE:1.13
	OPENBSD_3_2:1.5.0.2
	OPENBSD_3_2_BASE:1.5
	OPENBSD_3_1:1.4.0.2
	OPENBSD_3_1_BASE:1.4
	UBC_SYNC_B:1.5
	UBC:1.3.0.2
	UBC_BASE:1.3
	OPENBSD_3_0:1.2.0.2
	OPENBSD_3_0_BASE:1.2
	SMP:1.1.0.2;
locks; strict;
comment	@ * @;


1.31
date	2014.04.19.16.08.14;	author henning;	state dead;
branches;
next	1.30;

1.30
date	2014.01.23.00.46.56;	author pelikan;	state Exp;
branches;
next	1.29;

1.29
date	2011.09.18.20.34.29;	author henning;	state Exp;
branches;
next	1.28;

1.28
date	2011.07.03.23.59.43;	author henning;	state Exp;
branches;
next	1.27;

1.27
date	2011.07.03.23.48.41;	author henning;	state Exp;
branches;
next	1.26;

1.26
date	2008.05.08.15.22.02;	author chl;	state Exp;
branches;
next	1.25;

1.25
date	2007.09.13.20.40.02;	author chl;	state Exp;
branches;
next	1.24;

1.24
date	2007.05.28.17.16.38;	author henning;	state Exp;
branches;
next	1.23;

1.23
date	2006.03.04.22.40.15;	author brad;	state Exp;
branches;
next	1.22;

1.22
date	2005.10.17.08.43.35;	author henning;	state Exp;
branches;
next	1.21;

1.21
date	2004.01.14.08.42.23;	author kjc;	state Exp;
branches;
next	1.20;

1.20
date	2003.12.06.06.39.51;	author kjc;	state Exp;
branches;
next	1.19;

1.19
date	2003.05.20.08.58.36;	author kjc;	state Exp;
branches;
next	1.18;

1.18
date	2003.04.12.20.03.22;	author henning;	state Exp;
branches;
next	1.17;

1.17
date	2003.04.12.18.59.16;	author henning;	state Exp;
branches;
next	1.16;

1.16
date	2003.04.12.15.19.54;	author henning;	state Exp;
branches;
next	1.15;

1.15
date	2003.04.12.14.07.31;	author henning;	state Exp;
branches;
next	1.14;

1.14
date	2003.03.27.11.53.13;	author henning;	state Exp;
branches;
next	1.13;

1.13
date	2003.03.24.07.33.28;	author kjc;	state Exp;
branches;
next	1.12;

1.12
date	2003.03.13.16.42.52;	author kjc;	state Exp;
branches;
next	1.11;

1.11
date	2003.03.11.02.25.59;	author kjc;	state Exp;
branches;
next	1.10;

1.10
date	2003.03.02.11.22.31;	author henning;	state Exp;
branches;
next	1.9;

1.9
date	2002.12.16.17.27.20;	author henning;	state Exp;
branches;
next	1.8;

1.8
date	2002.12.16.09.18.05;	author kjc;	state Exp;
branches;
next	1.7;

1.7
date	2002.11.29.07.52.31;	author kjc;	state Exp;
branches;
next	1.6;

1.6
date	2002.11.26.01.03.34;	author henning;	state Exp;
branches;
next	1.5;

1.5
date	2002.05.17.07.19.44;	author kjc;	state Exp;
branches;
next	1.4;

1.4
date	2002.03.14.01.26.26;	author millert;	state Exp;
branches;
next	1.3;

1.3
date	2001.10.26.07.36.46;	author kjc;	state Exp;
branches
	1.3.2.1;
next	1.2;

1.2
date	2001.08.09.14.32.59;	author deraadt;	state Exp;
branches;
next	1.1;

1.1
date	2001.06.27.05.28.35;	author kjc;	state Exp;
branches
	1.1.2.1;
next	;

1.1.2.1
date	2001.10.31.02.43.21;	author nate;	state Exp;
branches;
next	1.1.2.2;

1.1.2.2
date	2002.03.28.11.26.45;	author niklas;	state Exp;
branches;
next	1.1.2.3;

1.1.2.3
date	2003.03.27.22.28.25;	author niklas;	state Exp;
branches;
next	1.1.2.4;

1.1.2.4
date	2003.05.13.19.21.26;	author ho;	state Exp;
branches;
next	1.1.2.5;

1.1.2.5
date	2003.06.07.11.00.36;	author ho;	state Exp;
branches;
next	1.1.2.6;

1.1.2.6
date	2004.02.19.10.51.22;	author niklas;	state Exp;
branches;
next	;

1.3.2.1
date	2002.06.11.03.27.42;	author art;	state Exp;
branches;
next	1.3.2.2;

1.3.2.2
date	2003.05.19.21.50.54;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.31
log
@bye bye
@
text
@/*	$OpenBSD: altq_hfsc.c,v 1.30 2014/01/23 00:46:56 pelikan Exp $	*/
/*	$KAME: altq_hfsc.c,v 1.17 2002/11/29 07:48:33 kjc Exp $	*/

/*
 * Copyright (c) 1997-1999 Carnegie Mellon University. All Rights Reserved.
 *
 * Permission to use, copy, modify, and distribute this software and
 * its documentation is hereby granted (including for commercial or
 * for-profit use), provided that both the copyright notice and this
 * permission notice appear in all copies of the software, derivative
 * works, or modified versions, and any portions thereof.
 *
 * THIS SOFTWARE IS EXPERIMENTAL AND IS KNOWN TO HAVE BUGS, SOME OF
 * WHICH MAY HAVE SERIOUS CONSEQUENCES.  CARNEGIE MELLON PROVIDES THIS
 * SOFTWARE IN ITS ``AS IS'' CONDITION, AND ANY EXPRESS OR IMPLIED
 * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED.  IN NO EVENT SHALL CARNEGIE MELLON UNIVERSITY BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT
 * OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
 * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
 * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
 * USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
 * DAMAGE.
 *
 * Carnegie Mellon encourages (but does not require) users of this
 * software to return any improvements or extensions that they make,
 * and to grant Carnegie Mellon the rights to redistribute these
 * changes without encumbrance.
 */
/*
 * H-FSC is described in Proceedings of SIGCOMM'97,
 * "A Hierarchical Fair Service Curve Algorithm for Link-Sharing,
 * Real-Time and Priority Service"
 * by Ion Stoica, Hui Zhang, and T. S. Eugene Ng.
 *
 * Oleg Cherevko <olwi@@aq.ml.com.ua> added the upperlimit for link-sharing.
 * when a class has an upperlimit, the fit-time is computed from the
 * upperlimit service curve.  the link-sharing scheduler does not schedule
 * a class whose fit-time exceeds the current time.
 */

#include <sys/param.h>
#include <sys/malloc.h>
#include <sys/mbuf.h>
#include <sys/socket.h>
#include <sys/systm.h>
#include <sys/errno.h>
#include <sys/queue.h>

#include <net/if.h>
#include <netinet/in.h>

#include <net/pfvar.h>
#include <altq/altq.h>
#include <altq/altq_hfsc.h>

/*
 * function prototypes
 */
static int		 hfsc_clear_interface(struct altq_hfsc_if *);
static int		 altq_hfsc_request(struct ifaltq *, int, void *);
static void		 altq_hfsc_purge(struct altq_hfsc_if *);
static struct altq_hfsc_class	*altq_hfsc_class_create(struct altq_hfsc_if *,
    struct service_curve *, struct service_curve *, struct service_curve *,
    struct altq_hfsc_class *, int, int, int);
static int		 altq_hfsc_class_destroy(struct altq_hfsc_class *);
static struct altq_hfsc_class	*hfsc_nextclass(struct altq_hfsc_class *);
static int		 altq_hfsc_enqueue(struct ifaltq *, struct mbuf *,
			    struct altq_pktattr *);
static struct mbuf	*altq_hfsc_dequeue(struct ifaltq *, int);

static int		 hfsc_addq(struct altq_hfsc_class *, struct mbuf *);
static struct mbuf	*hfsc_getq(struct altq_hfsc_class *);
static struct mbuf	*hfsc_pollq(struct altq_hfsc_class *);
static void		 altq_hfsc_purgeq(struct altq_hfsc_class *);

static void		 update_cfmin(struct altq_hfsc_class *);
static void		 set_active(struct altq_hfsc_class *, int);
static void		 set_passive(struct altq_hfsc_class *);

static void		 init_ed(struct altq_hfsc_class *, int);
static void		 update_ed(struct altq_hfsc_class *, int);
static void		 update_d(struct altq_hfsc_class *, int);
static void		 init_vf(struct altq_hfsc_class *, int);
static void		 update_vf(struct altq_hfsc_class *, int, u_int64_t);
static ellist_t		*altq_ellist_alloc(void);
static void		 ellist_destroy(ellist_t *);
static void		 ellist_insert(struct altq_hfsc_class *);
static void		 ellist_remove(struct altq_hfsc_class *);
static void		 ellist_update(struct altq_hfsc_class *);
struct altq_hfsc_class	*altq_ellist_get_mindl(ellist_t *, u_int64_t);
static actlist_t	*actlist_alloc(void);
static void		 actlist_destroy(actlist_t *);
static void		 actlist_insert(struct altq_hfsc_class *);
static void		 actlist_remove(struct altq_hfsc_class *);
static void		 actlist_update(struct altq_hfsc_class *);

static struct altq_hfsc_class	*actlist_firstfit(struct altq_hfsc_class *,
				    u_int64_t);

static __inline u_int64_t	seg_x2y(u_int64_t, u_int64_t);
static __inline u_int64_t	seg_y2x(u_int64_t, u_int64_t);
static __inline u_int64_t	m2sm(u_int);
static __inline u_int64_t	m2ism(u_int);
static __inline u_int64_t	d2dx(u_int);
static u_int			sm2m(u_int64_t);
static u_int			dx2d(u_int64_t);

static void		sc2isc(struct service_curve *, struct internal_sc *);
static void		rtsc_init(struct runtime_sc *, struct internal_sc *,
			    u_int64_t, u_int64_t);
static u_int64_t	rtsc_y2x(struct runtime_sc *, u_int64_t);
static u_int64_t	rtsc_x2y(struct runtime_sc *, u_int64_t);
static void		rtsc_min(struct runtime_sc *, struct internal_sc *,
			    u_int64_t, u_int64_t);

static void			 get_class_stats(struct hfsc_classstats *,
				    struct altq_hfsc_class *);
static struct altq_hfsc_class	*clh_to_clp(struct altq_hfsc_if *, u_int32_t);

/*
 * macros
 */
#define	is_a_parent_class(cl)	((cl)->cl_children != NULL)

#define	HT_INFINITY	0xffffffffffffffffLL	/* infinite time value */

int
hfsc_pfattach(struct pf_altq *a)
{
	struct ifnet *ifp;
	int s, error;

	if ((ifp = ifunit(a->ifname)) == NULL || a->altq_disc == NULL)
		return (EINVAL);
	s = splnet();
	error = altq_attach(&ifp->if_snd, ALTQT_HFSC, a->altq_disc,
	    altq_hfsc_enqueue, altq_hfsc_dequeue, altq_hfsc_request, NULL,
	    NULL);
	splx(s);
	return (error);
}

int
hfsc_add_altq(struct pf_altq *a)
{
	struct altq_hfsc_if *hif;
	struct ifnet *ifp;

	if ((ifp = ifunit(a->ifname)) == NULL)
		return (EINVAL);
	if (!ALTQ_IS_READY(&ifp->if_snd))
		return (ENODEV);

	hif = malloc(sizeof(struct altq_hfsc_if), M_DEVBUF, M_WAITOK|M_ZERO);

	hif->hif_eligible = altq_ellist_alloc();

	hif->hif_ifq = &ifp->if_snd;

	/* keep the state in pf_altq */
	a->altq_disc = hif;

	return (0);
}

int
hfsc_remove_altq(struct pf_altq *a)
{
	struct altq_hfsc_if *hif;

	if ((hif = a->altq_disc) == NULL)
		return (EINVAL);
	a->altq_disc = NULL;

	(void)hfsc_clear_interface(hif);
	(void)altq_hfsc_class_destroy(hif->hif_rootclass);

	ellist_destroy(hif->hif_eligible);

	free(hif, M_DEVBUF);

	return (0);
}

int
hfsc_add_queue(struct pf_altq *a)
{
	struct altq_hfsc_if *hif;
	struct altq_hfsc_class *cl, *parent;
	struct hfsc_opts *opts;
	struct service_curve rtsc, lssc, ulsc;

	if ((hif = a->altq_disc) == NULL)
		return (EINVAL);

	opts = &a->pq_u.hfsc_opts;

	if (a->parent_qid == HFSC_NULLCLASS_HANDLE &&
	    hif->hif_rootclass == NULL)
		parent = NULL;
	else if ((parent = clh_to_clp(hif, a->parent_qid)) == NULL)
		return (EINVAL);

	if (a->qid == 0)
		return (EINVAL);

	if (clh_to_clp(hif, a->qid) != NULL)
		return (EBUSY);

	rtsc.m1 = opts->rtsc_m1;
	rtsc.d  = opts->rtsc_d;
	rtsc.m2 = opts->rtsc_m2;
	lssc.m1 = opts->lssc_m1;
	lssc.d  = opts->lssc_d;
	lssc.m2 = opts->lssc_m2;
	ulsc.m1 = opts->ulsc_m1;
	ulsc.d  = opts->ulsc_d;
	ulsc.m2 = opts->ulsc_m2;

	cl = altq_hfsc_class_create(hif, &rtsc, &lssc, &ulsc,
	    parent, a->qlimit, opts->flags, a->qid);
	if (cl == NULL)
		return (ENOMEM);

	return (0);
}

int
hfsc_remove_queue(struct pf_altq *a)
{
	struct altq_hfsc_if *hif;
	struct altq_hfsc_class *cl;

	if ((hif = a->altq_disc) == NULL)
		return (EINVAL);

	if ((cl = clh_to_clp(hif, a->qid)) == NULL)
		return (EINVAL);

	return (altq_hfsc_class_destroy(cl));
}

int
hfsc_getqstats(struct pf_altq *a, void *ubuf, int *nbytes)
{
	struct altq_hfsc_if *hif;
	struct altq_hfsc_class *cl;
	struct hfsc_classstats stats;
	int error = 0;

	if ((hif = altq_lookup(a->ifname, ALTQT_HFSC)) == NULL)
		return (EBADF);

	if ((cl = clh_to_clp(hif, a->qid)) == NULL)
		return (EINVAL);

	if (*nbytes < sizeof(stats))
		return (EINVAL);

	get_class_stats(&stats, cl);

	if ((error = copyout((caddr_t)&stats, ubuf, sizeof(stats))) != 0)
		return (error);
	*nbytes = sizeof(stats);
	return (0);
}

/*
 * bring the interface back to the initial state by discarding
 * all the filters and classes except the root class.
 */
static int
hfsc_clear_interface(struct altq_hfsc_if *hif)
{
	struct altq_hfsc_class	*cl;

	/* clear out the classes */
	while (hif->hif_rootclass != NULL &&
	    (cl = hif->hif_rootclass->cl_children) != NULL) {
		/*
		 * remove the first leaf class found in the hierarchy
		 * then start over
		 */
		for (; cl != NULL; cl = hfsc_nextclass(cl)) {
			if (!is_a_parent_class(cl)) {
				(void)altq_hfsc_class_destroy(cl);
				break;
			}
		}
	}

	return (0);
}

static int
altq_hfsc_request(struct ifaltq *ifq, int req, void *arg)
{
	struct altq_hfsc_if	*hif = (struct altq_hfsc_if *)ifq->altq_disc;

	switch (req) {
	case ALTRQ_PURGE:
		altq_hfsc_purge(hif);
		break;
	}
	return (0);
}

/* discard all the queued packets on the interface */
static void
altq_hfsc_purge(struct altq_hfsc_if *hif)
{
	struct altq_hfsc_class *cl;

	for (cl = hif->hif_rootclass; cl != NULL; cl = hfsc_nextclass(cl))
		if (!qempty(cl->cl_q))
			altq_hfsc_purgeq(cl);
	if (ALTQ_IS_ENABLED(hif->hif_ifq))
		hif->hif_ifq->ifq_len = 0;
}

struct altq_hfsc_class *
altq_hfsc_class_create(struct altq_hfsc_if *hif, struct service_curve *rsc,
    struct service_curve *fsc, struct service_curve *usc,
    struct altq_hfsc_class *parent, int qlimit, int flags, int qid)
{
	struct altq_hfsc_class *cl, *p;
	int i, s;

	if (hif->hif_classes >= ALTQ_HFSC_MAX_CLASSES)
		return (NULL);

#ifndef ALTQ_RED
	if (flags & HFCF_RED) {
#ifdef ALTQ_DEBUG
		printf("altq_hfsc_class_create: RED not configured for HFSC\n");
#endif
		return (NULL);
	}
#endif

	cl = malloc(sizeof(struct altq_hfsc_class), M_DEVBUF, M_WAITOK|M_ZERO);

	cl->cl_q = malloc(sizeof(class_queue_t), M_DEVBUF, M_WAITOK|M_ZERO);

	cl->cl_actc = actlist_alloc();

	if (qlimit == 0)
		qlimit = 50;  /* use default */
	qlimit(cl->cl_q) = qlimit;
	qtype(cl->cl_q) = Q_DROPTAIL;
	qlen(cl->cl_q) = 0;
	cl->cl_flags = flags;
#ifdef ALTQ_RED
	if (flags & HFCF_RED) {
		int red_flags, red_pkttime;
		u_int m2;

		m2 = 0;
		if (rsc != NULL && rsc->m2 > m2)
			m2 = rsc->m2;
		if (fsc != NULL && fsc->m2 > m2)
			m2 = fsc->m2;
		if (usc != NULL && usc->m2 > m2)
			m2 = usc->m2;

		red_flags = 0;
		if (flags & HFCF_ECN)
			red_flags |= REDF_ECN;
		if (m2 < 8)
			red_pkttime = 1000 * 1000 * 1000; /* 1 sec */
		else
			red_pkttime = (int64_t)hif->hif_ifq->altq_ifp->if_mtu
				* 1000 * 1000 * 1000 / (m2 / 8);
		if (flags & HFCF_RED) {
			cl->cl_red = red_alloc(0, 0,
			    qlimit(cl->cl_q) * 10/100,
			    qlimit(cl->cl_q) * 30/100,
			    red_flags, red_pkttime);
			qtype(cl->cl_q) = Q_RED;
		}
	}
#endif /* ALTQ_RED */

	if (rsc != NULL && (rsc->m1 != 0 || rsc->m2 != 0)) {
		cl->cl_rsc = malloc(sizeof(struct internal_sc), M_DEVBUF,
		    M_WAITOK);
		sc2isc(rsc, cl->cl_rsc);
		rtsc_init(&cl->cl_deadline, cl->cl_rsc, 0, 0);
		rtsc_init(&cl->cl_eligible, cl->cl_rsc, 0, 0);
	}
	if (fsc != NULL && (fsc->m1 != 0 || fsc->m2 != 0)) {
		cl->cl_fsc = malloc(sizeof(struct internal_sc), M_DEVBUF,
		    M_WAITOK);
		sc2isc(fsc, cl->cl_fsc);
		rtsc_init(&cl->cl_virtual, cl->cl_fsc, 0, 0);
	}
	if (usc != NULL && (usc->m1 != 0 || usc->m2 != 0)) {
		cl->cl_usc = malloc(sizeof(struct internal_sc), M_DEVBUF,
		    M_WAITOK);
		sc2isc(usc, cl->cl_usc);
		rtsc_init(&cl->cl_ulimit, cl->cl_usc, 0, 0);
	}

	cl->cl_id = hif->hif_classid++;
	cl->cl_handle = qid;
	cl->cl_hif = hif;
	cl->cl_parent = parent;

	s = splnet();
	hif->hif_classes++;

	/*
	 * find a free slot in the class table.  if the slot matching
	 * the lower bits of qid is free, use this slot.  otherwise,
	 * use the first free slot.
	 */
	i = qid % ALTQ_HFSC_MAX_CLASSES;
	if (hif->hif_class_tbl[i] == NULL)
		hif->hif_class_tbl[i] = cl;
	else {
		for (i = 0; i < ALTQ_HFSC_MAX_CLASSES; i++)
			if (hif->hif_class_tbl[i] == NULL) {
				hif->hif_class_tbl[i] = cl;
				break;
			}
		if (i == ALTQ_HFSC_MAX_CLASSES) {
			splx(s);
			goto err_ret;
		}
	}

	if (flags & HFCF_DEFAULTCLASS)
		hif->hif_defaultclass = cl;

	if (parent == NULL) {
		/* this is root class */
		hif->hif_rootclass = cl;
	} else {
		/* add this class to the children list of the parent */
		if ((p = parent->cl_children) == NULL)
			parent->cl_children = cl;
		else {
			while (p->cl_siblings != NULL)
				p = p->cl_siblings;
			p->cl_siblings = cl;
		}
	}
	splx(s);

	return (cl);

 err_ret:
	if (cl->cl_actc != NULL)
		actlist_destroy(cl->cl_actc);
	if (cl->cl_red != NULL) {
#ifdef ALTQ_RED
		if (q_is_red(cl->cl_q))
			red_destroy(cl->cl_red);
#endif
	}
	if (cl->cl_fsc != NULL)
		free(cl->cl_fsc, M_DEVBUF);
	if (cl->cl_rsc != NULL)
		free(cl->cl_rsc, M_DEVBUF);
	if (cl->cl_usc != NULL)
		free(cl->cl_usc, M_DEVBUF);
	if (cl->cl_q != NULL)
		free(cl->cl_q, M_DEVBUF);
	free(cl, M_DEVBUF);
	return (NULL);
}

static int
altq_hfsc_class_destroy(struct altq_hfsc_class *cl)
{
	int i, s;

	if (cl == NULL)
		return (0);

	if (is_a_parent_class(cl))
		return (EBUSY);

	s = splnet();

	if (!qempty(cl->cl_q))
		altq_hfsc_purgeq(cl);

	if (cl->cl_parent == NULL) {
		/* this is root class */
	} else {
		struct altq_hfsc_class *p = cl->cl_parent->cl_children;

		if (p == cl)
			cl->cl_parent->cl_children = cl->cl_siblings;
		else do {
			if (p->cl_siblings == cl) {
				p->cl_siblings = cl->cl_siblings;
				break;
			}
		} while ((p = p->cl_siblings) != NULL);
		ASSERT(p != NULL);
	}

	for (i = 0; i < ALTQ_HFSC_MAX_CLASSES; i++)
		if (cl->cl_hif->hif_class_tbl[i] == cl) {
			cl->cl_hif->hif_class_tbl[i] = NULL;
			break;
		}

	cl->cl_hif->hif_classes--;
	splx(s);

	actlist_destroy(cl->cl_actc);

	if (cl->cl_red != NULL) {
#ifdef ALTQ_RED
		if (q_is_red(cl->cl_q))
			red_destroy(cl->cl_red);
#endif
	}

	if (cl == cl->cl_hif->hif_rootclass)
		cl->cl_hif->hif_rootclass = NULL;
	if (cl == cl->cl_hif->hif_defaultclass)
		cl->cl_hif->hif_defaultclass = NULL;

	if (cl->cl_usc != NULL)
		free(cl->cl_usc, M_DEVBUF);
	if (cl->cl_fsc != NULL)
		free(cl->cl_fsc, M_DEVBUF);
	if (cl->cl_rsc != NULL)
		free(cl->cl_rsc, M_DEVBUF);
	free(cl->cl_q, M_DEVBUF);
	free(cl, M_DEVBUF);

	return (0);
}

/*
 * hfsc_nextclass returns the next class in the tree.
 *   usage:
 *	for (cl = hif->hif_rootclass; cl != NULL; cl = hfsc_nextclass(cl))
 *		do_something;
 */
static struct altq_hfsc_class *
hfsc_nextclass(struct altq_hfsc_class *cl)
{
	if (cl->cl_children != NULL)
		cl = cl->cl_children;
	else if (cl->cl_siblings != NULL)
		cl = cl->cl_siblings;
	else {
		while ((cl = cl->cl_parent) != NULL)
			if (cl->cl_siblings) {
				cl = cl->cl_siblings;
				break;
			}
	}

	return (cl);
}

/*
 * altq_hfsc_enqueue is an enqueue function to be registered to
 * (*altq_enqueue) in struct ifaltq.
 */
static int
altq_hfsc_enqueue(struct ifaltq *ifq, struct mbuf *m,
    struct altq_pktattr *pktattr)
{
	struct altq_hfsc_if	*hif = (struct altq_hfsc_if *)ifq->altq_disc;
	struct altq_hfsc_class *cl;
	int len;

	/* grab class set by classifier */
	if ((m->m_flags & M_PKTHDR) == 0) {
		/* should not happen */
		printf("altq: packet for %s does not have pkthdr\n",
		    ifq->altq_ifp->if_xname);
		m_freem(m);
		return (ENOBUFS);
	}
	if ((cl = clh_to_clp(hif, m->m_pkthdr.pf.qid)) == NULL ||
		is_a_parent_class(cl)) {
		cl = hif->hif_defaultclass;
		if (cl == NULL) {
			m_freem(m);
			return (ENOBUFS);
		}
		cl->cl_pktattr = NULL;
	}

	len = m_pktlen(m);
	if (hfsc_addq(cl, m) != 0) {
		/* drop occurred.  mbuf was freed in hfsc_addq. */
		PKTCNTR_ADD(&cl->cl_stats.drop_cnt, len);
		return (ENOBUFS);
	}
	IFQ_INC_LEN(ifq);
	cl->cl_hif->hif_packets++;

	/* successfully queued. */
	if (qlen(cl->cl_q) == 1)
		set_active(cl, m_pktlen(m));

	return (0);
}

/*
 * altq_hfsc_dequeue is a dequeue function to be registered to
 * (*altq_dequeue) in struct ifaltq.
 *
 * note: ALTDQ_POLL returns the next packet without removing the packet
 *	from the queue.  ALTDQ_REMOVE is a normal dequeue operation.
 *	ALTDQ_REMOVE must return the same packet if called immediately
 *	after ALTDQ_POLL.
 */
static struct mbuf *
altq_hfsc_dequeue(struct ifaltq *ifq, int op)
{
	struct altq_hfsc_if	*hif = (struct altq_hfsc_if *)ifq->altq_disc;
	struct altq_hfsc_class *cl;
	struct mbuf *m;
	int len, next_len;
	int realtime = 0;
	u_int64_t cur_time;

	if (hif->hif_packets == 0)
		/* no packet in the tree */
		return (NULL);

	cur_time = read_machclk();

	if (op == ALTDQ_REMOVE && hif->hif_pollcache != NULL) {

		cl = hif->hif_pollcache;
		hif->hif_pollcache = NULL;
		/* check if the class was scheduled by real-time criteria */
		if (cl->cl_rsc != NULL)
			realtime = (cl->cl_e <= cur_time);
	} else {
		/*
		 * if there are eligible classes, use real-time criteria.
		 * find the class with the minimum deadline among
		 * the eligible classes.
		 */
		if ((cl = altq_ellist_get_mindl(hif->hif_eligible, cur_time))
		    != NULL) {
			realtime = 1;
		} else {
#ifdef ALTQ_DEBUG
			int fits = 0;
#endif
			/*
			 * use link-sharing criteria
			 * get the class with the minimum vt in the hierarchy
			 */
			cl = hif->hif_rootclass;
			while (is_a_parent_class(cl)) {

				cl = actlist_firstfit(cl, cur_time);
				if (cl == NULL) {
#ifdef ALTQ_DEBUG
					if (fits > 0)
						printf("%d fit but none found\n",fits);
#endif
					return (NULL);
				}
				/*
				 * update parent's cl_cvtmin.
				 * don't update if the new vt is smaller.
				 */
				if (cl->cl_parent->cl_cvtmin < cl->cl_vt)
					cl->cl_parent->cl_cvtmin = cl->cl_vt;
#ifdef ALTQ_DEBUG
				fits++;
#endif
			}
		}

		if (op == ALTDQ_POLL) {
			hif->hif_pollcache = cl;
			m = hfsc_pollq(cl);
			return (m);
		}
	}

	m = hfsc_getq(cl);
	if (m == NULL)
		panic("altq_hfsc_dequeue:");
	len = m_pktlen(m);
	cl->cl_hif->hif_packets--;
	IFQ_DEC_LEN(ifq);
	PKTCNTR_ADD(&cl->cl_stats.xmit_cnt, len);

	update_vf(cl, len, cur_time);
	if (realtime)
		cl->cl_cumul += len;

	if (!qempty(cl->cl_q)) {
		if (cl->cl_rsc != NULL) {
			/* update ed */
			next_len = m_pktlen(qhead(cl->cl_q));

			if (realtime)
				update_ed(cl, next_len);
			else
				update_d(cl, next_len);
		}
	} else {
		/* the class becomes passive */
		set_passive(cl);
	}

	return (m);
}

static int
hfsc_addq(struct altq_hfsc_class *cl, struct mbuf *m)
{

#ifdef ALTQ_RED
	if (q_is_red(cl->cl_q))
		return red_addq(cl->cl_red, cl->cl_q, m, cl->cl_pktattr);
#endif
	if (qlen(cl->cl_q) >= qlimit(cl->cl_q)) {
		m_freem(m);
		return (-1);
	}

	_addq(cl->cl_q, m);

	return (0);
}

static struct mbuf *
hfsc_getq(struct altq_hfsc_class *cl)
{
#ifdef ALTQ_RED
	if (q_is_red(cl->cl_q))
		return red_getq(cl->cl_red, cl->cl_q);
#endif
	return _getq(cl->cl_q);
}

static struct mbuf *
hfsc_pollq(struct altq_hfsc_class *cl)
{
	return qhead(cl->cl_q);
}

static void
altq_hfsc_purgeq(struct altq_hfsc_class *cl)
{
	struct mbuf *m;

	if (qempty(cl->cl_q))
		return;

	while ((m = _getq(cl->cl_q)) != NULL) {
		PKTCNTR_ADD(&cl->cl_stats.drop_cnt, m_pktlen(m));
		m_freem(m);
		cl->cl_hif->hif_packets--;
		IFQ_DEC_LEN(cl->cl_hif->hif_ifq);
	}
	ASSERT(qlen(cl->cl_q) == 0);

	update_vf(cl, 0, 0);	/* remove cl from the actlist */
	set_passive(cl);
}

static void
set_active(struct altq_hfsc_class *cl, int len)
{
	if (cl->cl_rsc != NULL)
		init_ed(cl, len);
	if (cl->cl_fsc != NULL)
		init_vf(cl, len);

	cl->cl_stats.period++;
}

static void
set_passive(struct altq_hfsc_class *cl)
{
	if (cl->cl_rsc != NULL)
		ellist_remove(cl);

	/*
	 * actlist is now handled in update_vf() so that update_vf(cl, 0, 0)
	 * needs to be called explicitly to remove a class from actlist
	 */
}

static void
init_ed(struct altq_hfsc_class *cl, int next_len)
{
	u_int64_t cur_time;

	cur_time = read_machclk();

	/* update the deadline curve */
	rtsc_min(&cl->cl_deadline, cl->cl_rsc, cur_time, cl->cl_cumul);

	/*
	 * update the eligible curve.
	 * for concave, it is equal to the deadline curve.
	 * for convex, it is a linear curve with slope m2.
	 */
	cl->cl_eligible = cl->cl_deadline;
	if (cl->cl_rsc->sm1 <= cl->cl_rsc->sm2) {
		cl->cl_eligible.dx = 0;
		cl->cl_eligible.dy = 0;
	}

	/* compute e and d */
	cl->cl_e = rtsc_y2x(&cl->cl_eligible, cl->cl_cumul);
	cl->cl_d = rtsc_y2x(&cl->cl_deadline, cl->cl_cumul + next_len);

	ellist_insert(cl);
}

static void
update_ed(struct altq_hfsc_class *cl, int next_len)
{
	cl->cl_e = rtsc_y2x(&cl->cl_eligible, cl->cl_cumul);
	cl->cl_d = rtsc_y2x(&cl->cl_deadline, cl->cl_cumul + next_len);

	ellist_update(cl);
}

static void
update_d(struct altq_hfsc_class *cl, int next_len)
{
	cl->cl_d = rtsc_y2x(&cl->cl_deadline, cl->cl_cumul + next_len);
}

static void
init_vf(struct altq_hfsc_class *cl, int len)
{
	struct altq_hfsc_class *max_cl, *p;
	u_int64_t vt, f, cur_time;
	int go_active;

	cur_time = 0;
	go_active = 1;
	for ( ; cl->cl_parent != NULL; cl = cl->cl_parent) {

		if (go_active && cl->cl_nactive++ == 0)
			go_active = 1;
		else
			go_active = 0;

		if (go_active) {
			max_cl = actlist_last(cl->cl_parent->cl_actc);
			if (max_cl != NULL) {
				/*
				 * set vt to the average of the min and max
				 * classes.  if the parent's period didn't
				 * change, don't decrease vt of the class.
				 */
				vt = max_cl->cl_vt;
				if (cl->cl_parent->cl_cvtmin != 0)
					vt = (cl->cl_parent->cl_cvtmin + vt)/2;

				if (cl->cl_parent->cl_vtperiod !=
				    cl->cl_parentperiod || vt > cl->cl_vt)
					cl->cl_vt = vt;
			} else {
				/*
				 * first child for a new parent backlog period.
				 * add parent's cvtmax to vtoff of children
				 * to make a new vt (vtoff + vt) larger than
				 * the vt in the last period for all children.
				 */
				vt = cl->cl_parent->cl_cvtmax;
				for (p = cl->cl_parent->cl_children; p != NULL;
				     p = p->cl_siblings)
					p->cl_vtoff += vt;
				cl->cl_vt = 0;
				cl->cl_parent->cl_cvtmax = 0;
				cl->cl_parent->cl_cvtmin = 0;
			}
			cl->cl_initvt = cl->cl_vt;

			/* update the virtual curve */
			vt = cl->cl_vt + cl->cl_vtoff;
			rtsc_min(&cl->cl_virtual, cl->cl_fsc, vt, cl->cl_total);
			if (cl->cl_virtual.x == vt) {
				cl->cl_virtual.x -= cl->cl_vtoff;
				cl->cl_vtoff = 0;
			}
			cl->cl_vtadj = 0;

			cl->cl_vtperiod++;  /* increment vt period */
			cl->cl_parentperiod = cl->cl_parent->cl_vtperiod;
			if (cl->cl_parent->cl_nactive == 0)
				cl->cl_parentperiod++;
			cl->cl_f = 0;

			actlist_insert(cl);

			if (cl->cl_usc != NULL) {
				/* class has upper limit curve */
				if (cur_time == 0)
					cur_time = read_machclk();

				/* update the ulimit curve */
				rtsc_min(&cl->cl_ulimit, cl->cl_usc, cur_time,
				    cl->cl_total);
				/* compute myf */
				cl->cl_myf = rtsc_y2x(&cl->cl_ulimit,
				    cl->cl_total);
				cl->cl_myfadj = 0;
			}
		}

		if (cl->cl_myf > cl->cl_cfmin)
			f = cl->cl_myf;
		else
			f = cl->cl_cfmin;
		if (f != cl->cl_f) {
			cl->cl_f = f;
			update_cfmin(cl->cl_parent);
		}
	}
}

static void
update_vf(struct altq_hfsc_class *cl, int len, u_int64_t cur_time)
{
	u_int64_t f, myf_bound, delta;
	int go_passive;

	go_passive = qempty(cl->cl_q);

	for (; cl->cl_parent != NULL; cl = cl->cl_parent) {

		cl->cl_total += len;

		if (cl->cl_fsc == NULL || cl->cl_nactive == 0)
			continue;

		if (go_passive && --cl->cl_nactive == 0)
			go_passive = 1;
		else
			go_passive = 0;

		if (go_passive) {
			/* no more active child, going passive */

			/* update cvtmax of the parent class */
			if (cl->cl_vt > cl->cl_parent->cl_cvtmax)
				cl->cl_parent->cl_cvtmax = cl->cl_vt;

			/* remove this class from the vt list */
			actlist_remove(cl);

			update_cfmin(cl->cl_parent);

			continue;
		}

		/*
		 * update vt and f
		 */
		cl->cl_vt = rtsc_y2x(&cl->cl_virtual, cl->cl_total)
		    - cl->cl_vtoff + cl->cl_vtadj;

		/*
		 * if vt of the class is smaller than cvtmin,
		 * the class was skipped in the past due to non-fit.
		 * if so, we need to adjust vtadj.
		 */
		if (cl->cl_vt < cl->cl_parent->cl_cvtmin) {
			cl->cl_vtadj += cl->cl_parent->cl_cvtmin - cl->cl_vt;
			cl->cl_vt = cl->cl_parent->cl_cvtmin;
		}

		/* update the vt list */
		actlist_update(cl);

		if (cl->cl_usc != NULL) {
			cl->cl_myf = cl->cl_myfadj
			    + rtsc_y2x(&cl->cl_ulimit, cl->cl_total);

			/*
			 * if myf lags behind by more than one clock tick
			 * from the current time, adjust myfadj to prevent
			 * a rate-limited class from going greedy.
			 * in a steady state under rate-limiting, myf
			 * fluctuates within one clock tick.
			 */
			myf_bound = cur_time - machclk_per_tick;
			if (cl->cl_myf < myf_bound) {
				delta = cur_time - cl->cl_myf;
				cl->cl_myfadj += delta;
				cl->cl_myf += delta;
			}
		}

		/* cl_f is max(cl_myf, cl_cfmin) */
		if (cl->cl_myf > cl->cl_cfmin)
			f = cl->cl_myf;
		else
			f = cl->cl_cfmin;
		if (f != cl->cl_f) {
			cl->cl_f = f;
			update_cfmin(cl->cl_parent);
		}
	}
}

static void
update_cfmin(struct altq_hfsc_class *cl)
{
	struct altq_hfsc_class *p;
	u_int64_t cfmin;

	if (TAILQ_EMPTY(cl->cl_actc)) {
		cl->cl_cfmin = 0;
		return;
	}
	cfmin = HT_INFINITY;
	TAILQ_FOREACH(p, cl->cl_actc, cl_actlist) {
		if (p->cl_f == 0) {
			cl->cl_cfmin = 0;
			return;
		}
		if (p->cl_f < cfmin)
			cfmin = p->cl_f;
	}
	cl->cl_cfmin = cfmin;
}

/*
 * TAILQ based ellist and actlist implementation
 * (ion wanted to make a calendar queue based implementation)
 */
/*
 * eligible list holds backlogged classes being sorted by their eligible times.
 * there is one eligible list per interface.
 */

static ellist_t *
altq_ellist_alloc(void)
{
	ellist_t *head;

	head = malloc(sizeof(ellist_t), M_DEVBUF, M_WAITOK);
	TAILQ_INIT(head);
	return (head);
}

static void
ellist_destroy(ellist_t *head)
{
	free(head, M_DEVBUF);
}

static void
ellist_insert(struct altq_hfsc_class *cl)
{
	struct altq_hfsc_if	*hif = cl->cl_hif;
	struct altq_hfsc_class *p;

	/* check the last entry first */
	if ((p = TAILQ_LAST(hif->hif_eligible, _eligible)) == NULL ||
	    p->cl_e <= cl->cl_e) {
		TAILQ_INSERT_TAIL(hif->hif_eligible, cl, cl_ellist);
		return;
	}

	TAILQ_FOREACH(p, hif->hif_eligible, cl_ellist) {
		if (cl->cl_e < p->cl_e) {
			TAILQ_INSERT_BEFORE(p, cl, cl_ellist);
			return;
		}
	}
	ASSERT(0); /* should not reach here */
}

static void
ellist_remove(struct altq_hfsc_class *cl)
{
	struct altq_hfsc_if	*hif = cl->cl_hif;

	TAILQ_REMOVE(hif->hif_eligible, cl, cl_ellist);
}

static void
ellist_update(struct altq_hfsc_class *cl)
{
	struct altq_hfsc_if	*hif = cl->cl_hif;
	struct altq_hfsc_class *p, *last;

	/*
	 * the eligible time of a class increases monotonically.
	 * if the next entry has a larger eligible time, nothing to do.
	 */
	p = TAILQ_NEXT(cl, cl_ellist);
	if (p == NULL || cl->cl_e <= p->cl_e)
		return;

	/* check the last entry */
	last = TAILQ_LAST(hif->hif_eligible, _eligible);
	ASSERT(last != NULL);
	if (last->cl_e <= cl->cl_e) {
		TAILQ_REMOVE(hif->hif_eligible, cl, cl_ellist);
		TAILQ_INSERT_TAIL(hif->hif_eligible, cl, cl_ellist);
		return;
	}

	/*
	 * the new position must be between the next entry
	 * and the last entry
	 */
	while ((p = TAILQ_NEXT(p, cl_ellist)) != NULL) {
		if (cl->cl_e < p->cl_e) {
			TAILQ_REMOVE(hif->hif_eligible, cl, cl_ellist);
			TAILQ_INSERT_BEFORE(p, cl, cl_ellist);
			return;
		}
	}
	ASSERT(0); /* should not reach here */
}

/* find the class with the minimum deadline among the eligible classes */
struct altq_hfsc_class *
altq_ellist_get_mindl(ellist_t *head, u_int64_t cur_time)
{
	struct altq_hfsc_class *p, *cl = NULL;

	TAILQ_FOREACH(p, head, cl_ellist) {
		if (p->cl_e > cur_time)
			break;
		if (cl == NULL || p->cl_d < cl->cl_d)
			cl = p;
	}
	return (cl);
}

/*
 * active children list holds backlogged child classes being sorted
 * by their virtual time.
 * each intermediate class has one active children list.
 */
static actlist_t *
actlist_alloc(void)
{
	actlist_t *head;

	head = malloc(sizeof(actlist_t), M_DEVBUF, M_WAITOK);
	TAILQ_INIT(head);
	return (head);
}

static void
actlist_destroy(actlist_t *head)
{
	free(head, M_DEVBUF);
}
static void
actlist_insert(struct altq_hfsc_class *cl)
{
	struct altq_hfsc_class *p;

	/* check the last entry first */
	if ((p = TAILQ_LAST(cl->cl_parent->cl_actc, _active)) == NULL
	    || p->cl_vt <= cl->cl_vt) {
		TAILQ_INSERT_TAIL(cl->cl_parent->cl_actc, cl, cl_actlist);
		return;
	}

	TAILQ_FOREACH(p, cl->cl_parent->cl_actc, cl_actlist) {
		if (cl->cl_vt < p->cl_vt) {
			TAILQ_INSERT_BEFORE(p, cl, cl_actlist);
			return;
		}
	}
	ASSERT(0); /* should not reach here */
}

static void
actlist_remove(struct altq_hfsc_class *cl)
{
	TAILQ_REMOVE(cl->cl_parent->cl_actc, cl, cl_actlist);
}

static void
actlist_update(struct altq_hfsc_class *cl)
{
	struct altq_hfsc_class *p, *last;

	/*
	 * the virtual time of a class increases monotonically during its
	 * backlogged period.
	 * if the next entry has a larger virtual time, nothing to do.
	 */
	p = TAILQ_NEXT(cl, cl_actlist);
	if (p == NULL || cl->cl_vt < p->cl_vt)
		return;

	/* check the last entry */
	last = TAILQ_LAST(cl->cl_parent->cl_actc, _active);
	ASSERT(last != NULL);
	if (last->cl_vt <= cl->cl_vt) {
		TAILQ_REMOVE(cl->cl_parent->cl_actc, cl, cl_actlist);
		TAILQ_INSERT_TAIL(cl->cl_parent->cl_actc, cl, cl_actlist);
		return;
	}

	/*
	 * the new position must be between the next entry
	 * and the last entry
	 */
	while ((p = TAILQ_NEXT(p, cl_actlist)) != NULL) {
		if (cl->cl_vt < p->cl_vt) {
			TAILQ_REMOVE(cl->cl_parent->cl_actc, cl, cl_actlist);
			TAILQ_INSERT_BEFORE(p, cl, cl_actlist);
			return;
		}
	}
	ASSERT(0); /* should not reach here */
}

static struct altq_hfsc_class *
actlist_firstfit(struct altq_hfsc_class *cl, u_int64_t cur_time)
{
	struct altq_hfsc_class *p;

	TAILQ_FOREACH(p, cl->cl_actc, cl_actlist) {
		if (p->cl_f <= cur_time)
			return (p);
	}
	return (NULL);
}

/*
 * service curve support functions
 *
 *  external service curve parameters
 *	m: bits/sec
 *	d: msec
 *  internal service curve parameters
 *	sm: (bytes/tsc_interval) << SM_SHIFT
 *	ism: (tsc_count/byte) << ISM_SHIFT
 *	dx: tsc_count
 *
 * SM_SHIFT and ISM_SHIFT are scaled in order to keep effective digits.
 * we should be able to handle 100K-1Gbps linkspeed with 200Hz-1GHz CPU
 * speed.  SM_SHIFT and ISM_SHIFT are selected to have at least 3 effective
 * digits in decimal using the following table.
 *
 *  bits/sec    100Kbps     1Mbps     10Mbps     100Mbps    1Gbps
 *  ----------+-------------------------------------------------------
 *  bytes/nsec  12.5e-6    125e-6     1250e-6    12500e-6   125000e-6
 *  sm(500MHz)  25.0e-6    250e-6     2500e-6    25000e-6   250000e-6
 *  sm(200MHz)  62.5e-6    625e-6     6250e-6    62500e-6   625000e-6
 *
 *  nsec/byte   80000      8000       800        80         8
 *  ism(500MHz) 40000      4000       400        40         4
 *  ism(200MHz) 16000      1600       160        16         1.6
 */
#define	SM_SHIFT	24
#define	ISM_SHIFT	10

#define	SM_MASK		((1LL << SM_SHIFT) - 1)
#define	ISM_MASK	((1LL << ISM_SHIFT) - 1)

static __inline u_int64_t
seg_x2y(u_int64_t x, u_int64_t sm)
{
	u_int64_t y;

	/*
	 * compute
	 *	y = x * sm >> SM_SHIFT
	 * but divide it for the upper and lower bits to avoid overflow
	 */
	y = (x >> SM_SHIFT) * sm + (((x & SM_MASK) * sm) >> SM_SHIFT);
	return (y);
}

static __inline u_int64_t
seg_y2x(u_int64_t y, u_int64_t ism)
{
	u_int64_t x;

	if (y == 0)
		x = 0;
	else if (ism == HT_INFINITY)
		x = HT_INFINITY;
	else {
		x = (y >> ISM_SHIFT) * ism
		    + (((y & ISM_MASK) * ism) >> ISM_SHIFT);
	}
	return (x);
}

static __inline u_int64_t
m2sm(u_int m)
{
	u_int64_t sm;

	sm = ((u_int64_t)m << SM_SHIFT) / 8 / machclk_freq;
	return (sm);
}

static __inline u_int64_t
m2ism(u_int m)
{
	u_int64_t ism;

	if (m == 0)
		ism = HT_INFINITY;
	else
		ism = ((u_int64_t)machclk_freq << ISM_SHIFT) * 8 / m;
	return (ism);
}

static __inline u_int64_t
d2dx(u_int d)
{
	u_int64_t dx;

	dx = ((u_int64_t)d * machclk_freq) / 1000;
	return (dx);
}

static u_int
sm2m(u_int64_t sm)
{
	u_int64_t m;

	m = (sm * 8 * machclk_freq) >> SM_SHIFT;
	return ((u_int)m);
}

static u_int
dx2d(u_int64_t dx)
{
	u_int64_t d;

	d = dx * 1000 / machclk_freq;
	return ((u_int)d);
}

static void
sc2isc(struct service_curve *sc, struct internal_sc *isc)
{
	isc->sm1 = m2sm(sc->m1);
	isc->ism1 = m2ism(sc->m1);
	isc->dx = d2dx(sc->d);
	isc->dy = seg_x2y(isc->dx, isc->sm1);
	isc->sm2 = m2sm(sc->m2);
	isc->ism2 = m2ism(sc->m2);
}

/*
 * initialize the runtime service curve with the given internal
 * service curve starting at (x, y).
 */
static void
rtsc_init(struct runtime_sc *rtsc, struct internal_sc * isc, u_int64_t x,
    u_int64_t y)
{
	rtsc->x =	x;
	rtsc->y =	y;
	rtsc->sm1 =	isc->sm1;
	rtsc->ism1 =	isc->ism1;
	rtsc->dx =	isc->dx;
	rtsc->dy =	isc->dy;
	rtsc->sm2 =	isc->sm2;
	rtsc->ism2 =	isc->ism2;
}

/*
 * calculate the y-projection of the runtime service curve by the
 * given x-projection value
 */
static u_int64_t
rtsc_y2x(struct runtime_sc *rtsc, u_int64_t y)
{
	u_int64_t	x;

	if (y < rtsc->y)
		x = rtsc->x;
	else if (y <= rtsc->y + rtsc->dy) {
		/* x belongs to the 1st segment */
		if (rtsc->dy == 0)
			x = rtsc->x + rtsc->dx;
		else
			x = rtsc->x + seg_y2x(y - rtsc->y, rtsc->ism1);
	} else {
		/* x belongs to the 2nd segment */
		x = rtsc->x + rtsc->dx
		    + seg_y2x(y - rtsc->y - rtsc->dy, rtsc->ism2);
	}
	return (x);
}

static u_int64_t
rtsc_x2y(struct runtime_sc *rtsc, u_int64_t x)
{
	u_int64_t	y;

	if (x <= rtsc->x)
		y = rtsc->y;
	else if (x <= rtsc->x + rtsc->dx)
		/* y belongs to the 1st segment */
		y = rtsc->y + seg_x2y(x - rtsc->x, rtsc->sm1);
	else
		/* y belongs to the 2nd segment */
		y = rtsc->y + rtsc->dy
		    + seg_x2y(x - rtsc->x - rtsc->dx, rtsc->sm2);
	return (y);
}

/*
 * update the runtime service curve by taking the minimum of the current
 * runtime service curve and the service curve starting at (x, y).
 */
static void
rtsc_min(struct runtime_sc *rtsc, struct internal_sc *isc, u_int64_t x,
    u_int64_t y)
{
	u_int64_t	y1, y2, dx, dy;

	if (isc->sm1 <= isc->sm2) {
		/* service curve is convex */
		y1 = rtsc_x2y(rtsc, x);
		if (y1 < y)
			/* the current rtsc is smaller */
			return;
		rtsc->x = x;
		rtsc->y = y;
		return;
	}

	/*
	 * service curve is concave
	 * compute the two y values of the current rtsc
	 *	y1: at x
	 *	y2: at (x + dx)
	 */
	y1 = rtsc_x2y(rtsc, x);
	if (y1 <= y) {
		/* rtsc is below isc, no change to rtsc */
		return;
	}

	y2 = rtsc_x2y(rtsc, x + isc->dx);
	if (y2 >= y + isc->dy) {
		/* rtsc is above isc, replace rtsc by isc */
		rtsc->x = x;
		rtsc->y = y;
		rtsc->dx = isc->dx;
		rtsc->dy = isc->dy;
		return;
	}

	/*
	 * the two curves intersect
	 * compute the offsets (dx, dy) using the reverse
	 * function of seg_x2y()
	 *	seg_x2y(dx, sm1) == seg_x2y(dx, sm2) + (y1 - y)
	 */
	dx = ((y1 - y) << SM_SHIFT) / (isc->sm1 - isc->sm2);
	/*
	 * check if (x, y1) belongs to the 1st segment of rtsc.
	 * if so, add the offset.
	 */
	if (rtsc->x + rtsc->dx > x)
		dx += rtsc->x + rtsc->dx - x;
	dy = seg_x2y(dx, isc->sm1);

	rtsc->x = x;
	rtsc->y = y;
	rtsc->dx = dx;
	rtsc->dy = dy;
	return;
}

static void
get_class_stats(struct hfsc_classstats *sp, struct altq_hfsc_class *cl)
{
	sp->class_id = cl->cl_id;
	sp->class_handle = cl->cl_handle;

	if (cl->cl_rsc != NULL) {
		sp->rsc.m1 = sm2m(cl->cl_rsc->sm1);
		sp->rsc.d = dx2d(cl->cl_rsc->dx);
		sp->rsc.m2 = sm2m(cl->cl_rsc->sm2);
	} else {
		sp->rsc.m1 = 0;
		sp->rsc.d = 0;
		sp->rsc.m2 = 0;
	}
	if (cl->cl_fsc != NULL) {
		sp->fsc.m1 = sm2m(cl->cl_fsc->sm1);
		sp->fsc.d = dx2d(cl->cl_fsc->dx);
		sp->fsc.m2 = sm2m(cl->cl_fsc->sm2);
	} else {
		sp->fsc.m1 = 0;
		sp->fsc.d = 0;
		sp->fsc.m2 = 0;
	}
	if (cl->cl_usc != NULL) {
		sp->usc.m1 = sm2m(cl->cl_usc->sm1);
		sp->usc.d = dx2d(cl->cl_usc->dx);
		sp->usc.m2 = sm2m(cl->cl_usc->sm2);
	} else {
		sp->usc.m1 = 0;
		sp->usc.d = 0;
		sp->usc.m2 = 0;
	}

	sp->total = cl->cl_total;
	sp->cumul = cl->cl_cumul;

	sp->d = cl->cl_d;
	sp->e = cl->cl_e;
	sp->vt = cl->cl_vt;
	sp->f = cl->cl_f;

	sp->initvt = cl->cl_initvt;
	sp->vtperiod = cl->cl_vtperiod;
	sp->parentperiod = cl->cl_parentperiod;
	sp->nactive = cl->cl_nactive;
	sp->vtoff = cl->cl_vtoff;
	sp->cvtmax = cl->cl_cvtmax;
	sp->myf = cl->cl_myf;
	sp->cfmin = cl->cl_cfmin;
	sp->cvtmin = cl->cl_cvtmin;
	sp->myfadj = cl->cl_myfadj;
	sp->vtadj = cl->cl_vtadj;

	sp->cur_time = read_machclk();
	sp->machclk_freq = machclk_freq;

	sp->qlength = qlen(cl->cl_q);
	sp->qlimit = qlimit(cl->cl_q);
	sp->xmit_cnt = cl->cl_stats.xmit_cnt;
	sp->drop_cnt = cl->cl_stats.drop_cnt;
	sp->period = cl->cl_stats.period;

	sp->qtype = qtype(cl->cl_q);
#ifdef ALTQ_RED
	if (q_is_red(cl->cl_q))
		red_getstats(cl->cl_red, &sp->red[0]);
#endif
}

/* convert a class handle to the corresponding class pointer */
static struct altq_hfsc_class *
clh_to_clp(struct altq_hfsc_if *hif, u_int32_t chandle)
{
	int i;
	struct altq_hfsc_class *cl;

	if (chandle == 0)
		return (NULL);
	/*
	 * first, try the slot corresponding to the lower bits of the handle.
	 * if it does not match, do the linear table search.
	 */
	i = chandle % ALTQ_HFSC_MAX_CLASSES;
	if ((cl = hif->hif_class_tbl[i]) != NULL && cl->cl_handle == chandle)
		return (cl);
	for (i = 0; i < ALTQ_HFSC_MAX_CLASSES; i++)
		if ((cl = hif->hif_class_tbl[i]) != NULL &&
		    cl->cl_handle == chandle)
			return (cl);
	return (NULL);
}
@


1.30
log
@Rename ALTQ's HFSC_MAX_CLASSES for the upcoming change.

Only added a prefix, no functional change.

"sure" henning
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.29 2011/09/18 20:34:29 henning Exp $	*/
@


1.29
log
@rename a few functions and structs etc that collide with upcoming stuffz
ok miod mpf
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.27 2011/07/03 23:48:41 henning Exp $	*/
d333 1
a333 1
	if (hif->hif_classes >= HFSC_MAX_CLASSES)
d421 1
a421 1
	i = qid % HFSC_MAX_CLASSES;
d425 1
a425 1
		for (i = 0; i < HFSC_MAX_CLASSES; i++)
d430 1
a430 1
		if (i == HFSC_MAX_CLASSES) {
d509 1
a509 1
	for (i = 0; i < HFSC_MAX_CLASSES; i++)
d1573 1
a1573 1
	i = chandle % HFSC_MAX_CLASSES;
d1576 1
a1576 1
	for (i = 0; i < HFSC_MAX_CLASSES; i++)
@


1.28
log
@g/c RIO traces (aka clean up after tedu :))
@
text
@d63 4
a66 4
static int			 hfsc_clear_interface(struct hfsc_if *);
static int			 hfsc_request(struct ifaltq *, int, void *);
static void			 hfsc_purge(struct hfsc_if *);
static struct hfsc_class	*hfsc_class_create(struct hfsc_if *,
d68 22
a89 22
    struct hfsc_class *, int, int, int);
static int			 hfsc_class_destroy(struct hfsc_class *);
static struct hfsc_class	*hfsc_nextclass(struct hfsc_class *);
static int			 hfsc_enqueue(struct ifaltq *, struct mbuf *,
				    struct altq_pktattr *);
static struct mbuf		*hfsc_dequeue(struct ifaltq *, int);

static int		 hfsc_addq(struct hfsc_class *, struct mbuf *);
static struct mbuf	*hfsc_getq(struct hfsc_class *);
static struct mbuf	*hfsc_pollq(struct hfsc_class *);
static void		 hfsc_purgeq(struct hfsc_class *);

static void		 update_cfmin(struct hfsc_class *);
static void		 set_active(struct hfsc_class *, int);
static void		 set_passive(struct hfsc_class *);

static void		 init_ed(struct hfsc_class *, int);
static void		 update_ed(struct hfsc_class *, int);
static void		 update_d(struct hfsc_class *, int);
static void		 init_vf(struct hfsc_class *, int);
static void		 update_vf(struct hfsc_class *, int, u_int64_t);
static ellist_t		*ellist_alloc(void);
d91 4
a94 4
static void		 ellist_insert(struct hfsc_class *);
static void		 ellist_remove(struct hfsc_class *);
static void		 ellist_update(struct hfsc_class *);
struct hfsc_class	*ellist_get_mindl(ellist_t *, u_int64_t);
d97 3
a99 3
static void		 actlist_insert(struct hfsc_class *);
static void		 actlist_remove(struct hfsc_class *);
static void		 actlist_update(struct hfsc_class *);
d101 1
a101 1
static struct hfsc_class	*actlist_firstfit(struct hfsc_class *,
d121 2
a122 2
				    struct hfsc_class *);
static struct hfsc_class	*clh_to_clp(struct hfsc_if *, u_int32_t);
d141 2
a142 1
	    hfsc_enqueue, hfsc_dequeue, hfsc_request, NULL, NULL);
d150 1
a150 1
	struct hfsc_if *hif;
d158 1
a158 1
	hif = malloc(sizeof(struct hfsc_if), M_DEVBUF, M_WAITOK|M_ZERO);
d160 1
a160 1
	hif->hif_eligible = ellist_alloc();
d173 1
a173 1
	struct hfsc_if *hif;
d180 1
a180 1
	(void)hfsc_class_destroy(hif->hif_rootclass);
d192 2
a193 2
	struct hfsc_if *hif;
	struct hfsc_class *cl, *parent;
d224 1
a224 1
	cl = hfsc_class_create(hif, &rtsc, &lssc, &ulsc,
d235 2
a236 2
	struct hfsc_if *hif;
	struct hfsc_class *cl;
d244 1
a244 1
	return (hfsc_class_destroy(cl));
d250 2
a251 2
	struct hfsc_if *hif;
	struct hfsc_class *cl;
d277 1
a277 1
hfsc_clear_interface(struct hfsc_if *hif)
d279 1
a279 1
	struct hfsc_class	*cl;
d290 1
a290 1
				(void)hfsc_class_destroy(cl);
d300 1
a300 1
hfsc_request(struct ifaltq *ifq, int req, void *arg)
d302 1
a302 1
	struct hfsc_if	*hif = (struct hfsc_if *)ifq->altq_disc;
d306 1
a306 1
		hfsc_purge(hif);
d314 1
a314 1
hfsc_purge(struct hfsc_if *hif)
d316 1
a316 1
	struct hfsc_class *cl;
d320 1
a320 1
			hfsc_purgeq(cl);
d325 2
a326 2
struct hfsc_class *
hfsc_class_create(struct hfsc_if *hif, struct service_curve *rsc,
d328 1
a328 1
    struct hfsc_class *parent, int qlimit, int flags, int qid)
d330 1
a330 1
	struct hfsc_class *cl, *p;
d339 1
a339 1
		printf("hfsc_class_create: RED not configured for HFSC!\n");
d345 1
a345 1
	cl = malloc(sizeof(struct hfsc_class), M_DEVBUF, M_WAITOK|M_ZERO);
d478 1
a478 1
hfsc_class_destroy(struct hfsc_class *cl)
d491 1
a491 1
		hfsc_purgeq(cl);
d496 1
a496 1
		struct hfsc_class *p = cl->cl_parent->cl_children;
d550 2
a551 2
static struct hfsc_class *
hfsc_nextclass(struct hfsc_class *cl)
d569 1
a569 1
 * hfsc_enqueue is an enqueue function to be registered to
d573 2
a574 1
hfsc_enqueue(struct ifaltq *ifq, struct mbuf *m, struct altq_pktattr *pktattr)
d576 2
a577 2
	struct hfsc_if	*hif = (struct hfsc_if *)ifq->altq_disc;
	struct hfsc_class *cl;
d615 1
a615 1
 * hfsc_dequeue is a dequeue function to be registered to
d624 1
a624 1
hfsc_dequeue(struct ifaltq *ifq, int op)
d626 2
a627 2
	struct hfsc_if	*hif = (struct hfsc_if *)ifq->altq_disc;
	struct hfsc_class *cl;
d652 1
a652 1
		if ((cl = ellist_get_mindl(hif->hif_eligible, cur_time))
d695 1
a695 1
		panic("hfsc_dequeue:");
d724 1
a724 1
hfsc_addq(struct hfsc_class *cl, struct mbuf *m)
d742 1
a742 1
hfsc_getq(struct hfsc_class *cl)
d752 1
a752 1
hfsc_pollq(struct hfsc_class *cl)
d758 1
a758 1
hfsc_purgeq(struct hfsc_class *cl)
d778 1
a778 1
set_active(struct hfsc_class *cl, int len)
d789 1
a789 1
set_passive(struct hfsc_class *cl)
d801 1
a801 1
init_ed(struct hfsc_class *cl, int next_len)
d829 1
a829 1
update_ed(struct hfsc_class *cl, int next_len)
d838 1
a838 1
update_d(struct hfsc_class *cl, int next_len)
d844 1
a844 1
init_vf(struct hfsc_class *cl, int len)
d846 1
a846 1
	struct hfsc_class *max_cl, *p;
d935 1
a935 1
update_vf(struct hfsc_class *cl, int len, u_int64_t cur_time)
d1020 1
a1020 1
update_cfmin(struct hfsc_class *cl)
d1022 1
a1022 1
	struct hfsc_class *p;
d1051 1
a1051 1
ellist_alloc(void)
d1067 1
a1067 1
ellist_insert(struct hfsc_class *cl)
d1069 2
a1070 2
	struct hfsc_if	*hif = cl->cl_hif;
	struct hfsc_class *p;
d1089 1
a1089 1
ellist_remove(struct hfsc_class *cl)
d1091 1
a1091 1
	struct hfsc_if	*hif = cl->cl_hif;
d1097 1
a1097 1
ellist_update(struct hfsc_class *cl)
d1099 2
a1100 2
	struct hfsc_if	*hif = cl->cl_hif;
	struct hfsc_class *p, *last;
d1134 2
a1135 2
struct hfsc_class *
ellist_get_mindl(ellist_t *head, u_int64_t cur_time)
d1137 1
a1137 1
	struct hfsc_class *p, *cl = NULL;
d1169 1
a1169 1
actlist_insert(struct hfsc_class *cl)
d1171 1
a1171 1
	struct hfsc_class *p;
d1190 1
a1190 1
actlist_remove(struct hfsc_class *cl)
d1196 1
a1196 1
actlist_update(struct hfsc_class *cl)
d1198 1
a1198 1
	struct hfsc_class *p, *last;
d1232 2
a1233 2
static struct hfsc_class *
actlist_firstfit(struct hfsc_class *cl, u_int64_t cur_time)
d1235 1
a1235 1
	struct hfsc_class *p;
d1491 1
a1491 1
get_class_stats(struct hfsc_classstats *sp, struct hfsc_class *cl)
d1561 2
a1562 2
static struct hfsc_class *
clh_to_clp(struct hfsc_if *hif, u_int32_t chandle)
d1565 1
a1565 1
	struct hfsc_class *cl;
@


1.27
log
@g/c code to read/write the dscp field. with pf as classifier altq has no
business in mucking with it and since the _CLEARDSCP flags were never
possibly set that is effectively dead code
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.26 2008/05/08 15:22:02 chl Exp $	*/
d357 1
a357 1
	if (flags & (HFCF_RED|HFCF_RIO)) {
a383 7
#ifdef ALTQ_RIO
		else {
			cl->cl_red = (red_t *)rio_alloc(0, NULL,
			    red_flags, red_pkttime);
			qtype(cl->cl_q) = Q_RIO;
		}
#endif
a458 4
#ifdef ALTQ_RIO
		if (q_is_rio(cl->cl_q))
			rio_destroy((rio_t *)cl->cl_red);
#endif
a519 4
#ifdef ALTQ_RIO
		if (q_is_rio(cl->cl_q))
			rio_destroy((rio_t *)cl->cl_red);
#endif
a724 5
#ifdef ALTQ_RIO
	if (q_is_rio(cl->cl_q))
		return rio_addq((rio_t *)cl->cl_red, cl->cl_q,
				m, cl->cl_pktattr);
#endif
a741 4
#ifdef ALTQ_RIO
	if (q_is_rio(cl->cl_q))
		return rio_getq((rio_t *)cl->cl_red, cl->cl_q);
#endif
a1554 4
#endif
#ifdef ALTQ_RIO
	if (q_is_rio(cl->cl_q))
		rio_getstats((rio_t *)cl->cl_red, &sp->red[0]);
@


1.26
log
@do not check malloc return value against NULL, as M_WAITOK is used

ok kjc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.25 2007/09/13 20:40:02 chl Exp $	*/
a371 4
#ifdef ALTQ_RIO
		if (flags & HFCF_CLEARDSCP)
			red_flags |= RIOF_CLEARDSCP;
#endif
a752 3

	if (cl->cl_flags & HFCF_CLEARDSCP)
		write_dsfield(m, cl->cl_pktattr, 0);
@


1.25
log
@MALLOC/FREE -> malloc/free and M_ZERO changes

ok henning@@ krw@@ canacar@@ ray@@
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.24 2007/05/28 17:16:38 henning Exp $	*/
a157 2
	if (hif == NULL)
		return (ENOMEM);
a159 4
	if (hif->hif_eligible == NULL) {
		free(hif, M_DEVBUF);
		return (ENOMEM);
	}
a344 2
	if (cl == NULL)
		return (NULL);
a346 2
	if (cl->cl_q == NULL)
		goto err_ret;
a348 2
	if (cl->cl_actc == NULL)
		goto err_ret;
d386 1
a386 2
			if (cl->cl_red != NULL)
				qtype(cl->cl_q) = Q_RED;
d392 1
a392 2
			if (cl->cl_red != NULL)
				qtype(cl->cl_q) = Q_RIO;
a400 2
		if (cl->cl_rsc == NULL)
			goto err_ret;
a407 2
		if (cl->cl_fsc == NULL)
			goto err_ret;
a413 2
		if (cl->cl_usc == NULL)
			goto err_ret;
@


1.24
log
@double pf performance.
boring details:
pf used to use an mbuf tag to keep track of route-to etc, altq, tags,
routing table IDs, packets redirected to localhost etc. so each and every
packet going through pf got an mbuf tag. mbuf tags use malloc'd memory,
and that is knda slow.
instead, stuff the information into the mbuf header directly.
bridging soekris with just "pass" as ruleset went from 29 MBit/s to
58 MBit/s with that (before ryan's randomness fix, now it is even betterer)
thanks to chris for the test setup!
ok ryan ryan ckuethe reyk
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.23 2006/03/04 22:40:15 brad Exp $	*/
d157 1
a157 2
	MALLOC(hif, struct hfsc_if *, sizeof(struct hfsc_if),
	    M_DEVBUF, M_WAITOK);
a159 1
	bzero(hif, sizeof(struct hfsc_if));
d163 1
a163 1
		FREE(hif, M_DEVBUF);
d189 1
a189 1
	FREE(hif, M_DEVBUF);
d350 1
a350 2
	MALLOC(cl, struct hfsc_class *, sizeof(struct hfsc_class),
	       M_DEVBUF, M_WAITOK);
a352 1
	bzero(cl, sizeof(struct hfsc_class));
d354 1
a354 2
	MALLOC(cl->cl_q, class_queue_t *, sizeof(class_queue_t),
	       M_DEVBUF, M_WAITOK);
a356 1
	bzero(cl->cl_q, sizeof(class_queue_t));
d413 2
a414 2
		MALLOC(cl->cl_rsc, struct internal_sc *,
		    sizeof(struct internal_sc), M_DEVBUF, M_WAITOK);
d422 2
a423 2
		MALLOC(cl->cl_fsc, struct internal_sc *,
		    sizeof(struct internal_sc), M_DEVBUF, M_WAITOK);
d430 2
a431 2
		MALLOC(cl->cl_usc, struct internal_sc *,
		    sizeof(struct internal_sc), M_DEVBUF, M_WAITOK);
d500 1
a500 1
		FREE(cl->cl_fsc, M_DEVBUF);
d502 1
a502 1
		FREE(cl->cl_rsc, M_DEVBUF);
d504 1
a504 1
		FREE(cl->cl_usc, M_DEVBUF);
d506 2
a507 2
		FREE(cl->cl_q, M_DEVBUF);
	FREE(cl, M_DEVBUF);
d571 1
a571 1
		FREE(cl->cl_usc, M_DEVBUF);
d573 1
a573 1
		FREE(cl->cl_fsc, M_DEVBUF);
d575 3
a577 3
		FREE(cl->cl_rsc, M_DEVBUF);
	FREE(cl->cl_q, M_DEVBUF);
	FREE(cl, M_DEVBUF);
d1104 1
a1104 1
	MALLOC(head, ellist_t *, sizeof(ellist_t), M_DEVBUF, M_WAITOK);
d1112 1
a1112 1
	FREE(head, M_DEVBUF);
d1207 1
a1207 1
	MALLOC(head, actlist_t *, sizeof(actlist_t), M_DEVBUF, M_WAITOK);
d1215 1
a1215 1
	FREE(head, M_DEVBUF);
@


1.23
log
@With the exception of two other small uncommited diffs this moves
the remainder of the network stack from splimp to splnet.

ok miod@@
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.22 2005/10/17 08:43:35 henning Exp $	*/
a620 1
	struct pf_mtag *t;
d631 1
a631 3
	t = pf_find_mtag(m);
	if (t == NULL ||
	    (cl = clh_to_clp(hif, t->qid)) == NULL ||
@


1.22
log
@make pf use one mbuf tag instead of 6 distinct ones. use a little struct
in the data part for the data from the previously distinct tags.
look up the tag early and carry a pointer to it around.
makes the code easier and saves some tag lookups and thus helps performance,
as proven by tests run by Schberle Dniel <Schoeberle.Daniel@@aamtech.hu>
Initially hacked up somewhere over the atlantic ocean in an A330
early testing reyk and moritz, "put it in" theo
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.21 2004/01/14 08:42:23 kjc Exp $	*/
d139 1
a139 1
	s = splimp();
d449 1
a449 1
	s = splimp();
d528 1
a528 1
	s = splimp();
@


1.21
log
@eliminate the predefined special qids so that qids become simple
identifiers without embedded meanings.

this also allows us to make the semantics of the qid assignment in line
with the tag assignment in the next step.

ok, henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.20 2003/12/06 06:39:51 kjc Exp $	*/
d621 1
a621 1
	struct m_tag *t;
d632 1
a632 1
	t = m_tag_find(m, PACKET_TAG_PF_QID, NULL);
d634 1
a634 1
	    (cl = clh_to_clp(hif, ((struct altq_tag *)(t+1))->qid)) == NULL ||
@


1.20
log
@update the CMU license, submitted by Tze Sing Eugene Ng <eugeneng@@cs.cmu.edu>.
the change removes the advertising clause, which was requested by a linux
developer.
ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.19 2003/05/20 08:58:36 kjc Exp $	*/
d209 5
a213 4
	parent = NULL;
	if (a->qid != HFSC_ROOTCLASS_HANDLE)
		if ((parent = clh_to_clp(hif, a->parent_qid)) == NULL)
			return (EINVAL);
d215 1
a215 1
	if (a->qid >= HFSC_MAX_CLASSES || a->qid == 0)
d217 1
d338 4
a341 1
	int s;
d451 21
a471 1
	hif->hif_class_tbl[qid - 1] = cl;
d520 1
a520 1
	int s;
d548 7
a554 1
	cl->cl_hif->hif_class_tbl[cl->cl_handle - 1] = NULL;
d570 6
d1626 2
a1627 1
	u_int idx;
d1631 12
a1642 4
	idx = chandle - 1;
	if (idx >= HFSC_MAX_CLASSES)
		return (NULL);
	return (hif->hif_class_tbl[idx]);
@


1.19
log
@fix a breakage in hfsc.
hif->hif_rootclass should be initialized when the root queue is created.
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.18 2003/04/12 20:03:22 henning Exp $	*/
d11 1
a11 4
 * works, or modified versions, and any portions thereof, and that
 * both notices appear in supporting documentation, and that credit
 * is given to Carnegie Mellon University in all publications reporting
 * on direct or indirect use of this code or its derivatives.
@


1.18
log
@life's easier if one initializes pointers...
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.17 2003/04/12 18:59:16 henning Exp $	*/
a452 1
	/* add this class to the children list of the parent */
d455 10
a464 7
	}
	else if ((p = parent->cl_children) == NULL)
		parent->cl_children = cl;
	else {
		while (p->cl_siblings != NULL)
			p = p->cl_siblings;
		p->cl_siblings = cl;
@


1.17
log
@don't create a root class implicitely. we do this from pfctl, and need
control over the root class from pfctl.
also fix a few null pointer derefs in case a root class does not exist
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.16 2003/04/12 15:19:54 henning Exp $	*/
d212 1
@


1.16
log
@kill the qid assignment code here; return EINVAL if qid == 0 - similar to
what we do for teh other schedulers already
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.15 2003/04/12 14:07:31 henning Exp $	*/
a153 1
	struct service_curve root_sc;
a173 13
	/*
	 * create root class
	 */
	root_sc.m1 = a->ifbandwidth;
	root_sc.d = 0;
	root_sc.m2 = a->ifbandwidth;
	if ((hif->hif_rootclass = hfsc_class_create(hif,
	    &root_sc, &root_sc, NULL, NULL, 0, 0, 0)) == NULL) {
		ellist_destroy(hif->hif_eligible);
		FREE(hif, M_DEVBUF);
		return (ENOMEM);
	}

d212 3
a214 3
	parent = clh_to_clp(hif, a->parent_qid);
	if (parent == NULL)
		return (EINVAL);
d289 2
a290 1
	while ((cl = hif->hif_rootclass->cl_children) != NULL) {
d496 3
@


1.15
log
@ansi and a bit style; ok kjc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.14 2003/03/27 11:53:13 henning Exp $	*/
d229 6
a234 6
	if (a->qid != 0) {
		if (a->qid >= HFSC_MAX_CLASSES)
			return (EINVAL);
		if (clh_to_clp(hif, a->qid) != NULL)
			return (EBUSY);
	}
a249 3
	/* return handle to user space. */
	a->qid = cl->cl_handle;

d351 1
a351 1
	int i, s, chandle;
a361 12
	if (qid)
		chandle = qid;
	else {
		/* find a free class slot. */
		for (i = 0; i < HFSC_MAX_CLASSES; i++)
			if (hif->hif_class_tbl[i] == NULL)
				break;
		if (i == HFSC_MAX_CLASSES)
			return (NULL);
		chandle = i + 1;
	}

d455 1
a455 1
	cl->cl_handle = chandle;
d461 1
a461 1
	hif->hif_class_tbl[chandle - 1] = cl;
@


1.14
log
@idx is unsigned
from Patrick Latifi
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.13 2003/03/24 07:33:28 kjc Exp $	*/
d66 4
a69 4
static int hfsc_clear_interface(struct hfsc_if *);
static int hfsc_request(struct ifaltq *, int, void *);
static void hfsc_purge(struct hfsc_if *);
static struct hfsc_class *hfsc_class_create(struct hfsc_if *,
d72 54
a125 50
static int hfsc_class_destroy(struct hfsc_class *);
static struct hfsc_class *hfsc_nextclass(struct hfsc_class *);
static int hfsc_enqueue(struct ifaltq *, struct mbuf *, struct altq_pktattr *);
static struct mbuf *hfsc_dequeue(struct ifaltq *, int);

static int hfsc_addq(struct hfsc_class *, struct mbuf *);
static struct mbuf *hfsc_getq(struct hfsc_class *);
static struct mbuf *hfsc_pollq(struct hfsc_class *);
static void hfsc_purgeq(struct hfsc_class *);

static void update_cfmin(struct hfsc_class *);
static void set_active(struct hfsc_class *, int);
static void set_passive(struct hfsc_class *);

static void init_ed(struct hfsc_class *, int);
static void update_ed(struct hfsc_class *, int);
static void update_d(struct hfsc_class *, int);
static void init_vf(struct hfsc_class *, int);
static void update_vf(struct hfsc_class *, int, u_int64_t);
static ellist_t *ellist_alloc(void);
static void ellist_destroy(ellist_t *);
static void ellist_insert(struct hfsc_class *);
static void ellist_remove(struct hfsc_class *);
static void ellist_update(struct hfsc_class *);
struct hfsc_class *ellist_get_mindl(ellist_t *, u_int64_t);
static actlist_t *actlist_alloc(void);
static void actlist_destroy(actlist_t *);
static void actlist_insert(struct hfsc_class *);
static void actlist_remove(struct hfsc_class *);
static void actlist_update(struct hfsc_class *);
static struct hfsc_class *actlist_firstfit(struct hfsc_class *, u_int64_t);

static __inline u_int64_t seg_x2y(u_int64_t, u_int64_t);
static __inline u_int64_t seg_y2x(u_int64_t, u_int64_t);
static __inline u_int64_t m2sm(u_int);
static __inline u_int64_t m2ism(u_int);
static __inline u_int64_t d2dx(u_int);
static u_int sm2m(u_int64_t);
static u_int dx2d(u_int64_t);

static void sc2isc(struct service_curve *, struct internal_sc *);
static void rtsc_init(struct runtime_sc *, struct internal_sc *,
		      u_int64_t, u_int64_t);
static u_int64_t rtsc_y2x(struct runtime_sc *, u_int64_t);
static u_int64_t rtsc_x2y(struct runtime_sc *, u_int64_t);
static void rtsc_min(struct runtime_sc *, struct internal_sc *,
		     u_int64_t, u_int64_t);

static void get_class_stats(struct hfsc_classstats *, struct hfsc_class *);
static struct hfsc_class *clh_to_clp(struct hfsc_if *, u_int32_t);
d301 1
a301 2
hfsc_clear_interface(hif)
	struct hfsc_if *hif;
d323 1
a323 4
hfsc_request(ifq, req, arg)
	struct ifaltq *ifq;
	int req;
	void *arg;
d337 1
a337 2
hfsc_purge(hif)
	struct hfsc_if *hif;
d349 3
a351 5
hfsc_class_create(hif, rsc, fsc, usc, parent, qlimit, flags, qid)
	struct hfsc_if *hif;
	struct service_curve *rsc, *fsc, *usc;
	struct hfsc_class *parent;
	int qlimit, flags, qid;
d521 1
a521 2
hfsc_class_destroy(cl)
	struct hfsc_class *cl;
d583 1
a583 2
hfsc_nextclass(cl)
	struct hfsc_class *cl;
d605 1
a605 4
hfsc_enqueue(ifq, m, pktattr)
	struct ifaltq *ifq;
	struct mbuf *m;
	struct altq_pktattr *pktattr;
d658 1
a658 3
hfsc_dequeue(ifq, op)
	struct ifaltq	*ifq;
	int		op;
d758 1
a758 3
hfsc_addq(cl, m)
	struct hfsc_class *cl;
	struct mbuf *m;
d784 1
a784 2
hfsc_getq(cl)
	struct hfsc_class *cl;
d798 1
a798 2
hfsc_pollq(cl)
	struct hfsc_class *cl;
d804 1
a804 2
hfsc_purgeq(cl)
	struct hfsc_class *cl;
d824 1
a824 3
set_active(cl, len)
	struct hfsc_class *cl;
	int len;
d835 1
a835 2
set_passive(cl)
	struct hfsc_class *cl;
d847 1
a847 3
init_ed(cl, next_len)
	struct hfsc_class *cl;
	int next_len;
d875 1
a875 3
update_ed(cl, next_len)
	struct hfsc_class *cl;
	int next_len;
d884 1
a884 3
update_d(cl, next_len)
	struct hfsc_class *cl;
	int next_len;
d890 1
a890 3
init_vf(cl, len)
	struct hfsc_class *cl;
	int len;
d981 1
a981 4
update_vf(cl, len, cur_time)
	struct hfsc_class *cl;
	int len;
	u_int64_t cur_time;
d1066 1
a1066 2
update_cfmin(cl)
	struct hfsc_class *cl;
d1097 1
a1097 1
ellist_alloc()
d1107 1
a1107 2
ellist_destroy(head)
	ellist_t *head;
d1113 1
a1113 2
ellist_insert(cl)
	struct hfsc_class *cl;
d1135 1
a1135 2
ellist_remove(cl)
	struct hfsc_class *cl;
d1143 1
a1143 2
ellist_update(cl)
	struct hfsc_class *cl;
d1181 1
a1181 3
ellist_get_mindl(head, cur_time)
	ellist_t *head;
	u_int64_t cur_time;
d1200 1
a1200 1
actlist_alloc()
d1210 1
a1210 2
actlist_destroy(head)
	actlist_t *head;
d1215 1
a1215 2
actlist_insert(cl)
	struct hfsc_class *cl;
d1236 1
a1236 2
actlist_remove(cl)
	struct hfsc_class *cl;
d1242 1
a1242 2
actlist_update(cl)
	struct hfsc_class *cl;
d1279 1
a1279 3
actlist_firstfit(cl, cur_time)
	struct hfsc_class *cl;
	u_int64_t cur_time;
d1323 1
a1323 3
seg_x2y(x, sm)
	u_int64_t x;
	u_int64_t sm;
d1337 1
a1337 3
seg_y2x(y, ism)
	u_int64_t y;
	u_int64_t ism;
d1353 1
a1353 2
m2sm(m)
	u_int m;
d1362 1
a1362 2
m2ism(m)
	u_int m;
d1374 1
a1374 2
d2dx(d)
	u_int	d;
d1383 1
a1383 2
sm2m(sm)
	u_int64_t sm;
d1392 1
a1392 2
dx2d(dx)
	u_int64_t dx;
d1401 1
a1401 3
sc2isc(sc, isc)
	struct service_curve	*sc;
	struct internal_sc	*isc;
d1416 2
a1417 4
rtsc_init(rtsc, isc, x, y)
	struct runtime_sc	*rtsc;
	struct internal_sc	*isc;
	u_int64_t		x, y;
d1434 1
a1434 3
rtsc_y2x(rtsc, y)
	struct runtime_sc	*rtsc;
	u_int64_t		y;
d1455 1
a1455 3
rtsc_x2y(rtsc, x)
	struct runtime_sc	*rtsc;
	u_int64_t		x;
d1476 2
a1477 4
rtsc_min(rtsc, isc, x, y)
	struct runtime_sc	*rtsc;
	struct internal_sc	*isc;
	u_int64_t		x, y;
d1537 1
a1537 3
get_class_stats(sp, cl)
	struct hfsc_classstats *sp;
	struct hfsc_class *cl;
d1612 1
a1612 3
clh_to_clp(hif, chandle)
	struct hfsc_if *hif;
	u_int32_t chandle;
@


1.13
log
@fix error return values (NULL to ENOMEM) and add missing ellist_destroy()
in error handling of hfsc_add_altq().

report and fix by millert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.12 2003/03/13 16:42:52 kjc Exp $	*/
d1679 1
a1679 1
	int idx;
@


1.12
log
@scale the red thresholds according to the queue limit.
the min and max thresholds are set to 10% and 30% of the queue limit.
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.11 2003/03/11 02:25:59 kjc Exp $	*/
d160 1
a160 1
		return (NULL);
d166 1
a166 1
		return NULL;
d179 1
@


1.11
log
@add protection against packets without pkthdr.
this should not happen but just in case.
printf() is intended to be annoying so that we'll get reports on it.

original idea from dhartmei@@
ok deraadt@@, henning@@
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.10 2003/03/02 11:22:31 henning Exp $	*/
d427 3
a429 1
			cl->cl_red = red_alloc(0, 0, 0, 0,
@


1.10
log
@only assign a new queue id if a->qid is 0. otherwise, check a->qid for
validity and take that.

validity checks provided by kjc@@

ok dhartmei@@ kjc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.9 2002/12/16 17:27:20 henning Exp $	*/
d618 7
@


1.9
log
@major KNF, Take 2

ok kjc@@
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.8 2002/12/16 09:18:05 kjc Exp $	*/
d71 1
a71 1
    struct hfsc_class *, int, int);
d178 1
a178 1
	    &root_sc, &root_sc, NULL, NULL, 0, 0)) == NULL) {
d224 6
d241 1
a241 1
	    parent, a->qlimit, opts->flags);
d349 1
a349 1
hfsc_class_create(hif, rsc, fsc, usc, parent, qlimit, flags)
d353 1
a353 1
	int qlimit, flags;
d367 11
a377 7
	/* find a free class slot. */
	for (i = 0; i < HFSC_MAX_CLASSES; i++)
		if (hif->hif_class_tbl[i] == NULL)
			break;
	if (i == HFSC_MAX_CLASSES)
		return (NULL);
	chandle = i + 1;
@


1.8
log
@switchover to pf-based altq.
 - remove files which are no longer used, or we don't have plans to support
   in pf in the near future.
 - remove altq ioctl related stuff.
 - convert the PRIQ, HFSC and RIO modules to pf-based altq.
   (these are not enabled in GENERIC, CDNR is not converted yet.)
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.7 2002/11/29 07:52:31 kjc Exp $	*/
d570 1
a570 1
 * 	for (cl = hif->hif_rootclass; cl != NULL; cl = hfsc_nextclass(cl))
@


1.7
log
@supress printf's.
@
text
@d1 2
a2 2
/*	$OpenBSD: altq_hfsc.c,v 1.6 2002/11/26 01:03:34 henning Exp $	*/
/*	$KAME: altq_hfsc.c,v 1.13 2002/05/16 11:02:58 kjc Exp $	*/
d41 5
a51 1
#include <sys/sockio.h>
a52 1
#include <sys/proc.h>
a53 1
#include <sys/kernel.h>
d57 1
a57 1
#include <net/if_types.h>
d59 1
a60 1
#include <altq/altq_conf.h>
a65 2
static struct hfsc_if *hfsc_attach(struct ifaltq *, u_int);
static int hfsc_detach(struct hfsc_if *);
d70 2
a71 1
		 struct service_curve *, struct hfsc_class *, int, int);
a72 2
static int hfsc_class_modify(struct hfsc_class *,
			     struct service_curve *, struct service_curve *);
a73 1

d82 1
d89 2
a90 2
static void init_v(struct hfsc_class *, int);
static void update_v(struct hfsc_class *, int);
d96 1
a96 1
struct hfsc_class *ellist_get_mindl(ellist_t *);
d102 1
d120 2
a121 14
int hfscopen(dev_t, int, int, struct proc *);
int hfscclose(dev_t, int, int, struct proc *);
int hfscioctl(dev_t, ioctlcmd_t, caddr_t, int, struct proc *);
static int hfsccmd_if_attach(struct hfsc_attach *);
static int hfsccmd_if_detach(struct hfsc_interface *);
static int hfsccmd_add_class(struct hfsc_add_class *);
static int hfsccmd_delete_class(struct hfsc_delete_class *);
static int hfsccmd_modify_class(struct hfsc_modify_class *);
static int hfsccmd_add_filter(struct hfsc_add_filter *);
static int hfsccmd_delete_filter(struct hfsc_delete_filter *);
static int hfsccmd_class_stats(struct hfsc_class_stats *);
static void get_class_stats(struct class_stats *, struct hfsc_class *);
static struct hfsc_class *clh_to_clp(struct hfsc_if *, u_long);
static u_long clp_to_clh(struct hfsc_class *);
d128 1
a128 2
/* hif_list keeps all hfsc_if's allocated. */
static struct hfsc_if *hif_list = NULL;
d130 17
a146 4
static struct hfsc_if *
hfsc_attach(ifq, bandwidth)
	struct ifaltq *ifq;
	u_int bandwidth;
d149 1
d152 5
d158 1
a158 1
	       M_DEVBUF, M_WAITOK);
d169 1
a169 1
	hif->hif_ifq = ifq;
d174 1
a174 1
	root_sc.m1 = bandwidth;
d176 3
a178 3
	root_sc.m2 = bandwidth;
	if ((hif->hif_rootclass =
	     hfsc_class_create(hif, &root_sc, NULL, 0, 0)) == NULL) {
d180 1
a180 1
		return (NULL);
d183 2
a184 3
	/* add this state to the hfsc list */
	hif->hif_next = hif_list;
	hif_list = hif;
d186 1
a186 1
	return (hif);
d189 3
a191 2
static int
hfsc_detach(hif)
d193 5
a197 1
{
d201 69
a269 5
	/* remove this interface from the hif list */
	if (hif_list == hif)
		hif_list = hif->hif_next;
	else {
		struct hfsc_if *h;
d271 2
a272 7
		for (h = hif_list; h != NULL; h = h->hif_next)
			if (h->hif_next == hif) {
				h->hif_next = hif->hif_next;
				break;
			}
		ASSERT(h != NULL);
	}
d274 2
a275 1
	ellist_destroy(hif->hif_eligible);
d277 1
a277 1
	FREE(hif, M_DEVBUF);
d279 3
a294 3
	/* free the filters for this interface */
	acc_discard_filters(&hif->hif_classifier, NULL, 1);

d343 1
a343 1
hfsc_class_create(hif, sc, parent, qlimit, flags)
d345 1
a345 1
	struct service_curve *sc;
d350 1
a350 1
	int s;
d361 8
d394 9
d411 1
a411 1
		if (sc->m2 < 8)
d415 1
a415 1
				* 1000 * 1000 * 1000 / (sc->m2 / 8);
d418 1
a418 1
					       red_flags, red_pkttime);
d425 1
a425 1
						      red_flags, red_pkttime);
d433 1
a433 1
	if (sc != NULL && (sc->m1 != 0 || sc->m2 != 0)) {
d435 1
a435 1
		       sizeof(struct internal_sc), M_DEVBUF, M_WAITOK);
d438 1
a438 2
		bzero(cl->cl_rsc, sizeof(struct internal_sc));
		sc2isc(sc, cl->cl_rsc);
d441 2
a442 1

d444 1
a444 1
		       sizeof(struct internal_sc), M_DEVBUF, M_WAITOK);
d447 1
a447 2
		bzero(cl->cl_fsc, sizeof(struct internal_sc));
		sc2isc(sc, cl->cl_fsc);
d450 8
d460 1
a460 1
	cl->cl_handle = (u_long)cl;  /* XXX: just a pointer to this class */
d466 1
d502 2
a520 3
	/* delete filters referencing to this class */
	acc_discard_filters(&cl->cl_hif->hif_classifier, cl, 0);

d539 1
d555 2
a566 62
static int
hfsc_class_modify(cl, rsc, fsc)
	struct hfsc_class *cl;
	struct service_curve *rsc, *fsc;
{
	struct internal_sc *rsc_tmp, *fsc_tmp;
	int s;

	if (rsc != NULL && (rsc->m1 != 0 || rsc->m2 != 0) &&
	    cl->cl_rsc == NULL) {
		MALLOC(rsc_tmp, struct internal_sc *,
		       sizeof(struct internal_sc), M_DEVBUF, M_WAITOK);
		if (rsc_tmp == NULL)
			return (ENOMEM);
	}
	if (fsc != NULL && (fsc->m1 != 0 || fsc->m2 != 0) &&
	    cl->cl_fsc == NULL) {
		MALLOC(fsc_tmp, struct internal_sc *,
		       sizeof(struct internal_sc), M_DEVBUF, M_WAITOK);
		if (fsc_tmp == NULL)
			return (ENOMEM);
	}

	s = splimp();
	if (!qempty(cl->cl_q))
		hfsc_purgeq(cl);

	if (rsc != NULL) {
		if (rsc->m1 == 0 && rsc->m2 == 0) {
			if (cl->cl_rsc != NULL) {
				FREE(cl->cl_rsc, M_DEVBUF);
				cl->cl_rsc = NULL;
			}
		} else {
			if (cl->cl_rsc == NULL)
				cl->cl_rsc = rsc_tmp;
			bzero(cl->cl_rsc, sizeof(struct internal_sc));
			sc2isc(rsc, cl->cl_rsc);
			rtsc_init(&cl->cl_deadline, cl->cl_rsc, 0, 0);
			rtsc_init(&cl->cl_eligible, cl->cl_rsc, 0, 0);
		}
	}

	if (fsc != NULL) {
		if (fsc->m1 == 0 && fsc->m2 == 0) {
			if (cl->cl_fsc != NULL) {
				FREE(cl->cl_fsc, M_DEVBUF);
				cl->cl_fsc = NULL;
			}
		} else {
			if (cl->cl_fsc == NULL)
				cl->cl_fsc = fsc_tmp;
			bzero(cl->cl_fsc, sizeof(struct internal_sc));
			sc2isc(fsc, cl->cl_fsc);
			rtsc_init(&cl->cl_virtual, cl->cl_fsc, 0, 0);
		}
	}
	splx(s);

	return (0);
}

d604 1
d608 4
a611 1
	if (pktattr == NULL || (cl = pktattr->pattr_class) == NULL)
d613 6
a618 1
	cl->cl_pktattr = pktattr;  /* save proto hdr used by ECN */
a632 3
#ifdef HFSC_PKTLOG
	/* put the logging_hook here */
#endif
d655 1
d661 2
a663 1
		u_int64_t cur_time;
d668 1
a668 2
		if (cl->cl_rsc != NULL) {
			cur_time = read_machclk();
a669 1
		}
d676 2
a677 1
		if ((cl = ellist_get_mindl(hif->hif_eligible)) != NULL) {
d680 3
d689 7
a695 2
				cl = actlist_first(cl->cl_actc);
				if (cl == NULL)
d697 10
d718 2
d725 1
a725 1
	update_v(cl, len);
a743 4
#ifdef HFSC_PKTLOG
	/* put the logging_hook here */
#endif

d809 2
d814 1
d826 1
a826 1
		init_v(cl, len);
d838 4
a841 13
	if (cl->cl_fsc != NULL) {
		while (cl->cl_parent != NULL) {
			if (--cl->cl_nactive == 0) {
				/* remove this class from the vt list */
				actlist_remove(cl);
			} else
				/* still has active children */
				break;

			/* go up to the parent class */
			cl = cl->cl_parent;
		}
	}
d894 94
a987 1
init_v(cl, len)
d990 1
d992 6
a997 1
	struct hfsc_class *min_cl, *max_cl;
d999 4
a1002 1
	while (cl->cl_parent != NULL) {
d1004 25
a1028 3
		if (cl->cl_nactive++ > 0)
			/* already active */
			break;
d1031 3
a1033 2
		 * if parent became idle while this class was idle.
		 * reset vt and the runtime service curve.
d1035 3
a1037 5
		if (cl->cl_parent->cl_nactive == 0 ||
		    cl->cl_parent->cl_vtperiod != cl->cl_parentperiod) {
			cl->cl_vt = 0;
			rtsc_init(&cl->cl_virtual, cl->cl_fsc,
				  0, cl->cl_total);
d1039 7
a1045 3
		min_cl = actlist_first(cl->cl_parent->cl_actc);
		if (min_cl != NULL) {
			u_int64_t vt;
d1048 5
a1052 3
			 * set vt to the average of the min and max classes.
			 * if the parent's period didn't change,
			 * don't decrease vt of the class.
d1054 6
a1059 5
			max_cl = actlist_last(cl->cl_parent->cl_actc);
			vt = (min_cl->cl_vt + max_cl->cl_vt) / 2;
			if (cl->cl_parent->cl_vtperiod != cl->cl_parentperiod
			    || vt > cl->cl_vt)
				cl->cl_vt = vt;
d1062 9
a1070 12
		/* update the virtual curve */
		rtsc_min(&cl->cl_virtual, cl->cl_fsc, cl->cl_vt, cl->cl_total);

		cl->cl_vtperiod++;  /* increment vt period */
		cl->cl_parentperiod = cl->cl_parent->cl_vtperiod;
		if (cl->cl_parent->cl_nactive == 0)
			cl->cl_parentperiod++;

		actlist_insert(cl);

		/* go up to the parent class */
		cl = cl->cl_parent;
d1075 1
a1075 1
update_v(cl, len)
a1076 1
	int len;
d1078 2
a1079 1
	while (cl->cl_parent != NULL) {
d1081 9
a1089 7
		cl->cl_total += len;

		if (cl->cl_fsc != NULL) {
			cl->cl_vt = rtsc_y2x(&cl->cl_virtual, cl->cl_total);

			/* update the vt list */
			actlist_update(cl);
d1091 2
a1092 3

		/* go up to the parent class */
		cl = cl->cl_parent;
d1094 1
d1195 1
a1195 1
ellist_get_mindl(head)
d1197 1
a1199 3
	u_int64_t cur_time;

	cur_time = read_machclk();
d1272 1
a1272 1
	if (p == NULL || cl->cl_vt <= p->cl_vt)
d1298 14
d1328 1
a1328 1
 *  bits/set    100Kbps     1Mbps     10Mbps     100Mbps    1Gbps
d1341 2
a1342 2
#define	SC_LARGEVAL	(1LL << 32)
#define	SC_INFINITY	0xffffffffffffffffLL
d1351 6
a1356 4
	if (x < SC_LARGEVAL)
		y = x * sm >> SM_SHIFT;
	else
		y = (x >> SM_SHIFT) * sm;
d1369 6
a1374 6
	else if (ism == SC_INFINITY)
		x = SC_INFINITY;
	else if (y < SC_LARGEVAL)
		x = y * ism >> ISM_SHIFT;
	else
		x = (y >> ISM_SHIFT) * ism;
d1395 1
a1395 1
		ism = SC_INFINITY;
d1577 3
a1579 335
/*
 * hfsc device interface
 */
int
hfscopen(dev, flag, fmt, p)
	dev_t dev;
	int flag, fmt;
	struct proc *p;
{
	if (machclk_freq == 0)
		init_machclk();

	if (machclk_freq == 0) {
		printf("hfsc: no cpu clock available!\n");
		return (ENXIO);
	}

	/* everything will be done when the queueing scheme is attached. */
	return 0;
}

int
hfscclose(dev, flag, fmt, p)
	dev_t dev;
	int flag, fmt;
	struct proc *p;
{
	struct hfsc_if *hif;
	int err, error = 0;

	while ((hif = hif_list) != NULL) {
		/* destroy all */
		if (ALTQ_IS_ENABLED(hif->hif_ifq))
			altq_disable(hif->hif_ifq);

		err = altq_detach(hif->hif_ifq);
		if (err == 0)
			err = hfsc_detach(hif);
		if (err != 0 && error == 0)
			error = err;
	}

	return error;
}

int
hfscioctl(dev, cmd, addr, flag, p)
	dev_t dev;
	ioctlcmd_t cmd;
	caddr_t addr;
	int flag;
	struct proc *p;
{
	struct hfsc_if *hif;
	struct hfsc_interface *ifacep;
	int	error = 0;

	/* check super-user privilege */
	switch (cmd) {
	case HFSC_GETSTATS:
		break;
	default:
#if (__FreeBSD_version > 400000)
		if ((error = suser(p)) != 0)
			return (error);
#else
		if ((error = suser(p->p_ucred, &p->p_acflag)) != 0)
			return (error);
#endif
		break;
	}

	switch (cmd) {

	case HFSC_IF_ATTACH:
		error = hfsccmd_if_attach((struct hfsc_attach *)addr);
		break;

	case HFSC_IF_DETACH:
		error = hfsccmd_if_detach((struct hfsc_interface *)addr);
		break;

	case HFSC_ENABLE:
	case HFSC_DISABLE:
	case HFSC_CLEAR_HIERARCHY:
		ifacep = (struct hfsc_interface *)addr;
		if ((hif = altq_lookup(ifacep->hfsc_ifname,
				       ALTQT_HFSC)) == NULL) {
			error = EBADF;
			break;
		}

		switch (cmd) {

		case HFSC_ENABLE:
			if (hif->hif_defaultclass == NULL) {
#ifdef ALTQ_DEBUG
				printf("hfsc: no default class\n");
#endif
				error = EINVAL;
				break;
			}
			error = altq_enable(hif->hif_ifq);
			break;

		case HFSC_DISABLE:
			error = altq_disable(hif->hif_ifq);
			break;

		case HFSC_CLEAR_HIERARCHY:
			hfsc_clear_interface(hif);
			break;
		}
		break;

	case HFSC_ADD_CLASS:
		error = hfsccmd_add_class((struct hfsc_add_class *)addr);
		break;

	case HFSC_DEL_CLASS:
		error = hfsccmd_delete_class((struct hfsc_delete_class *)addr);
		break;

	case HFSC_MOD_CLASS:
		error = hfsccmd_modify_class((struct hfsc_modify_class *)addr);
		break;

	case HFSC_ADD_FILTER:
		error = hfsccmd_add_filter((struct hfsc_add_filter *)addr);
		break;

	case HFSC_DEL_FILTER:
		error = hfsccmd_delete_filter((struct hfsc_delete_filter *)addr);
		break;

	case HFSC_GETSTATS:
		error = hfsccmd_class_stats((struct hfsc_class_stats *)addr);
		break;

	default:
		error = EINVAL;
		break;
	}
	return error;
}

static int
hfsccmd_if_attach(ap)
	struct hfsc_attach *ap;
{
	struct hfsc_if *hif;
	struct ifnet *ifp;
	int error;

	if ((ifp = ifunit(ap->iface.hfsc_ifname)) == NULL)
		return (ENXIO);

	if ((hif = hfsc_attach(&ifp->if_snd, ap->bandwidth)) == NULL)
		return (ENOMEM);

	/*
	 * set HFSC to this ifnet structure.
	 */
	if ((error = altq_attach(&ifp->if_snd, ALTQT_HFSC, hif,
				 hfsc_enqueue, hfsc_dequeue, hfsc_request,
				 &hif->hif_classifier, acc_classify)) != 0)
		(void)hfsc_detach(hif);

	return (error);
}

static int
hfsccmd_if_detach(ap)
	struct hfsc_interface *ap;
{
	struct hfsc_if *hif;
	int error;

	if ((hif = altq_lookup(ap->hfsc_ifname, ALTQT_HFSC)) == NULL)
		return (EBADF);

	if (ALTQ_IS_ENABLED(hif->hif_ifq))
		altq_disable(hif->hif_ifq);

	if ((error = altq_detach(hif->hif_ifq)))
		return (error);

	return hfsc_detach(hif);
}

static int
hfsccmd_add_class(ap)
	struct hfsc_add_class *ap;
{
	struct hfsc_if *hif;
	struct hfsc_class *cl, *parent;

	if ((hif = altq_lookup(ap->iface.hfsc_ifname, ALTQT_HFSC)) == NULL)
		return (EBADF);

	if ((parent = clh_to_clp(hif, ap->parent_handle)) == NULL) {
		if (ap->parent_handle == HFSC_ROOTCLASS_HANDLE)
			parent = hif->hif_rootclass;
		else
			return (EINVAL);
	}

	if ((cl = hfsc_class_create(hif, &ap->service_curve, parent,
				    ap->qlimit, ap->flags)) == NULL)
		return (ENOMEM);

	/* return a class handle to the user */
	ap->class_handle = clp_to_clh(cl);
	return (0);
}

static int
hfsccmd_delete_class(ap)
	struct hfsc_delete_class *ap;
{
	struct hfsc_if *hif;
	struct hfsc_class *cl;

	if ((hif = altq_lookup(ap->iface.hfsc_ifname, ALTQT_HFSC)) == NULL)
		return (EBADF);

	if ((cl = clh_to_clp(hif, ap->class_handle)) == NULL)
		return (EINVAL);

	return hfsc_class_destroy(cl);
}

static int
hfsccmd_modify_class(ap)
	struct hfsc_modify_class *ap;
{
	struct hfsc_if *hif;
	struct hfsc_class *cl;
	struct service_curve *rsc = NULL;
	struct service_curve *fsc = NULL;

	if ((hif = altq_lookup(ap->iface.hfsc_ifname, ALTQT_HFSC)) == NULL)
		return (EBADF);

	if ((cl = clh_to_clp(hif, ap->class_handle)) == NULL)
		return (EINVAL);

	if (ap->sctype & HFSC_REALTIMESC)
		rsc = &ap->service_curve;
	if (ap->sctype & HFSC_LINKSHARINGSC)
		fsc = &ap->service_curve;

	return hfsc_class_modify(cl, rsc, fsc);
}

static int
hfsccmd_add_filter(ap)
	struct hfsc_add_filter *ap;
{
	struct hfsc_if *hif;
	struct hfsc_class *cl;

	if ((hif = altq_lookup(ap->iface.hfsc_ifname, ALTQT_HFSC)) == NULL)
		return (EBADF);

	if ((cl = clh_to_clp(hif, ap->class_handle)) == NULL)
		return (EINVAL);

	if (is_a_parent_class(cl)) {
#ifdef ALTQ_DEBUG
		printf("hfsccmd_add_filter: not a leaf class!\n");
#endif
		return (EINVAL);
	}

	return acc_add_filter(&hif->hif_classifier, &ap->filter,
			      cl, &ap->filter_handle);
}

static int
hfsccmd_delete_filter(ap)
	struct hfsc_delete_filter *ap;
{
	struct hfsc_if *hif;

	if ((hif = altq_lookup(ap->iface.hfsc_ifname, ALTQT_HFSC)) == NULL)
		return (EBADF);

	return acc_delete_filter(&hif->hif_classifier,
				 ap->filter_handle);
}

static int
hfsccmd_class_stats(ap)
	struct hfsc_class_stats *ap;
{
	struct hfsc_if *hif;
	struct hfsc_class *cl;
	struct class_stats stats, *usp;
	int	n, nclasses, error;

	if ((hif = altq_lookup(ap->iface.hfsc_ifname, ALTQT_HFSC)) == NULL)
		return (EBADF);

	ap->cur_time = read_machclk();
	ap->hif_classes = hif->hif_classes;
	ap->hif_packets = hif->hif_packets;

	/* skip the first N classes in the tree */
	nclasses = ap->nskip;
	for (cl = hif->hif_rootclass, n = 0; cl != NULL && n < nclasses;
	     cl = hfsc_nextclass(cl), n++)
		;
	if (n != nclasses)
		return (EINVAL);

	/* then, read the next N classes in the tree */
	nclasses = ap->nclasses;
	usp = ap->stats;
	for (n = 0; cl != NULL && n < nclasses; cl = hfsc_nextclass(cl), n++) {

		get_class_stats(&stats, cl);

		if ((error = copyout((caddr_t)&stats, (caddr_t)usp++,
				     sizeof(stats))) != 0)
			return (error);
	}

	ap->nclasses = n;

	return (0);
}

static void get_class_stats(sp, cl)
	struct class_stats *sp;
d1583 1
a1583 1
	sp->class_handle = clp_to_clh(cl);
d1603 9
d1619 16
d1637 1
d1657 1
a1657 1
	u_long chandle;
d1659 1
a1659 1
	struct hfsc_class *cl;
d1661 1
a1661 5
	cl = (struct hfsc_class *)chandle;
	if (chandle != ALIGN(cl)) {
#ifdef ALTQ_DEBUG
		printf("clh_to_cl: unaligned pointer %p\n", cl);
#endif
d1663 2
a1664 3
	}

	if (cl == NULL || cl->cl_handle != chandle || cl->cl_hif != hif)
d1666 1
a1666 12

	return (cl);
}

/* convert a class pointer to the corresponding class handle */
static u_long
clp_to_clh(cl)
	struct hfsc_class *cl;
{
	if (cl->cl_parent == NULL)
		return (HFSC_ROOTCLASS_HANDLE);  /* XXX */
	return (cl->cl_handle);
a1667 9

#ifdef KLD_MODULE

static struct altqsw hfsc_sw =
	{"hfsc", hfscopen, hfscclose, hfscioctl};

ALTQ_MODULE(altq_hfsc, ALTQT_HFSC, &hfsc_sw);

#endif /* KLD_MODULE */
@


1.6
log
@KNF
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.5 2002/05/17 07:19:44 kjc Exp $	*/
d284 1
d286 1
d1478 1
a1478 1
#if 1
d1651 1
a1651 1
#if 1
d1774 1
a1774 1
#if 1
@


1.5
log
@sync with KAME.
 - don't MALLOC() with M_WAITOK in the spl block.
   move the allocation before splimp().
 - when we reset vt of a class, reset the runtime service curve as well.
 - don't use max() to compare 64 bit values.
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.4 2002/03/14 01:26:26 millert Exp $	*/
d38 1
a38 1
 * "A Hierarchical Fair Service Curve Algorithm for Link-Sharing, 
d196 1
a196 1
	
d238 1
a238 1
	
d559 1
a559 1
static int 
d619 1
a619 1
		
d669 1
a669 1
		
d751 1
a751 1
	
d755 1
a755 1
static void 
d768 1
a768 1
static void 
d790 1
a790 1
static void 
d820 1
a820 1
static void 
d831 1
a831 1
static void 
d839 1
a839 1
static void 
d847 1
a847 1
		
d893 1
a893 1
static void 
d927 1
a927 1
	
d940 1
a940 1
static void 
d963 1
a963 1
static void 
d968 1
a968 1
	
d972 1
a972 1
static void 
d1038 1
a1038 1
	
d1050 1
a1050 1
static void 
d1072 1
a1072 1
static void 
d1138 1
a1138 1
 * 
d1149 1
a1149 1
static __inline u_int64_t 
d1163 1
a1163 1
static __inline u_int64_t 
d1181 1
a1181 1
static __inline u_int64_t 
d1191 1
a1191 1
static __inline u_int64_t 
d1204 1
a1204 1
static __inline u_int64_t 
d1209 1
a1209 1
	
d1214 1
a1214 1
static u_int 
d1224 1
a1224 1
static u_int 
d1234 1
a1234 1
static void 
d1251 1
a1251 1
static void 
d1271 1
a1271 1
static u_int64_t 
d1294 1
a1294 1
static u_int64_t 
d1317 1
a1317 1
static void 
d1368 1
a1368 1
	 */ 
d1451 1
a1451 1
    
d1533 1
a1533 1
	
d1539 1
a1539 1
	
d1560 1
a1560 1
	
d1586 1
a1586 1
	
d1590 1
a1590 1
		
d1608 1
a1608 1
	
d1680 1
a1680 1
	
d1702 1
a1702 1
		
@


1.4
log
@First round of __P removal in sys
@
text
@d1 2
a2 2
/*	$OpenBSD: altq_hfsc.c,v 1.3 2001/10/26 07:36:46 kjc Exp $	*/
/*	$KAME: altq_hfsc.c,v 1.8 2000/12/14 08:12:46 thorpej Exp $	*/
d73 1
a73 1
			  struct service_curve *, struct service_curve *);
d76 1
a76 2
static int hfsc_enqueue(struct ifaltq *, struct mbuf *,
			     struct altq_pktattr *);
d114 1
a114 1
			   u_int64_t, u_int64_t);
d118 1
a118 1
			  u_int64_t, u_int64_t);
d473 1
a473 1
	struct internal_sc *tmp;
d476 15
d502 2
a503 10
			if (cl->cl_rsc == NULL) {
				MALLOC(tmp, struct internal_sc *,
				       sizeof(struct internal_sc),
				       M_DEVBUF, M_WAITOK);
				if (tmp == NULL) {
					splx(s);
					return (ENOMEM);
				}
				cl->cl_rsc = tmp;
			}
d518 2
a519 10
			if (cl->cl_fsc == NULL) {
				MALLOC(tmp, struct internal_sc *,
				       sizeof(struct internal_sc),
				       M_DEVBUF, M_WAITOK);
				if (tmp == NULL) {
					splx(s);
					return (ENOMEM);
				}
				cl->cl_fsc = tmp;
			}
d852 10
d873 3
a875 6
			if (cl->cl_parent->cl_vtperiod == cl->cl_parentperiod)
				vt = max(cl->cl_vt, vt);
			cl->cl_vt = vt;
		} else {
			/* no packet is backlogged.  set vt to 0 */
			cl->cl_vt = 0;
d879 1
a879 2
		rtsc_min(&cl->cl_virtual, cl->cl_fsc,
			 cl->cl_vt, cl->cl_total);
@


1.3
log
@avoid divide-by-zero when the specified bandwidth is less than 8bps.

PR kernel/2150
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.2 2001/08/09 14:32:59 deraadt Exp $	*/
d64 71
a134 71
static struct hfsc_if *hfsc_attach __P((struct ifaltq *, u_int));
static int hfsc_detach __P((struct hfsc_if *));
static int hfsc_clear_interface __P((struct hfsc_if *));
static int hfsc_request __P((struct ifaltq *, int, void *));
static void hfsc_purge __P((struct hfsc_if *));
static struct hfsc_class *hfsc_class_create __P((struct hfsc_if *,
		 struct service_curve *, struct hfsc_class *, int, int));
static int hfsc_class_destroy __P((struct hfsc_class *));
static int hfsc_class_modify __P((struct hfsc_class *,
			  struct service_curve *, struct service_curve *));
static struct hfsc_class *hfsc_nextclass __P((struct hfsc_class *));

static int hfsc_enqueue __P((struct ifaltq *, struct mbuf *,
			     struct altq_pktattr *));
static struct mbuf *hfsc_dequeue __P((struct ifaltq *, int));

static int hfsc_addq __P((struct hfsc_class *, struct mbuf *));
static struct mbuf *hfsc_getq __P((struct hfsc_class *));
static struct mbuf *hfsc_pollq __P((struct hfsc_class *));
static void hfsc_purgeq __P((struct hfsc_class *));

static void set_active __P((struct hfsc_class *, int));
static void set_passive __P((struct hfsc_class *));

static void init_ed __P((struct hfsc_class *, int));
static void update_ed __P((struct hfsc_class *, int));
static void update_d __P((struct hfsc_class *, int));
static void init_v __P((struct hfsc_class *, int));
static void update_v __P((struct hfsc_class *, int));
static ellist_t *ellist_alloc __P((void));
static void ellist_destroy __P((ellist_t *));
static void ellist_insert __P((struct hfsc_class *));
static void ellist_remove __P((struct hfsc_class *));
static void ellist_update __P((struct hfsc_class *));
struct hfsc_class *ellist_get_mindl __P((ellist_t *));
static actlist_t *actlist_alloc __P((void));
static void actlist_destroy __P((actlist_t *));
static void actlist_insert __P((struct hfsc_class *));
static void actlist_remove __P((struct hfsc_class *));
static void actlist_update __P((struct hfsc_class *));

static __inline u_int64_t seg_x2y __P((u_int64_t, u_int64_t));
static __inline u_int64_t seg_y2x __P((u_int64_t, u_int64_t));
static __inline u_int64_t m2sm __P((u_int));
static __inline u_int64_t m2ism __P((u_int));
static __inline u_int64_t d2dx __P((u_int));
static u_int sm2m __P((u_int64_t));
static u_int dx2d __P((u_int64_t));

static void sc2isc __P((struct service_curve *, struct internal_sc *));
static void rtsc_init __P((struct runtime_sc *, struct internal_sc *,
			   u_int64_t, u_int64_t));
static u_int64_t rtsc_y2x __P((struct runtime_sc *, u_int64_t));
static u_int64_t rtsc_x2y __P((struct runtime_sc *, u_int64_t));
static void rtsc_min __P((struct runtime_sc *, struct internal_sc *,
			  u_int64_t, u_int64_t));

int hfscopen __P((dev_t, int, int, struct proc *));
int hfscclose __P((dev_t, int, int, struct proc *));
int hfscioctl __P((dev_t, ioctlcmd_t, caddr_t, int, struct proc *));
static int hfsccmd_if_attach __P((struct hfsc_attach *));
static int hfsccmd_if_detach __P((struct hfsc_interface *));
static int hfsccmd_add_class __P((struct hfsc_add_class *));
static int hfsccmd_delete_class __P((struct hfsc_delete_class *));
static int hfsccmd_modify_class __P((struct hfsc_modify_class *));
static int hfsccmd_add_filter __P((struct hfsc_add_filter *));
static int hfsccmd_delete_filter __P((struct hfsc_delete_filter *));
static int hfsccmd_class_stats __P((struct hfsc_class_stats *));
static void get_class_stats __P((struct class_stats *, struct hfsc_class *));
static struct hfsc_class *clh_to_clp __P((struct hfsc_if *, u_long));
static u_long clp_to_clh __P((struct hfsc_class *));
@


1.3.2.1
log
@Sync UBC branch to -current
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$KAME: altq_hfsc.c,v 1.13 2002/05/16 11:02:58 kjc Exp $	*/
d64 71
a134 70
static struct hfsc_if *hfsc_attach(struct ifaltq *, u_int);
static int hfsc_detach(struct hfsc_if *);
static int hfsc_clear_interface(struct hfsc_if *);
static int hfsc_request(struct ifaltq *, int, void *);
static void hfsc_purge(struct hfsc_if *);
static struct hfsc_class *hfsc_class_create(struct hfsc_if *,
		 struct service_curve *, struct hfsc_class *, int, int);
static int hfsc_class_destroy(struct hfsc_class *);
static int hfsc_class_modify(struct hfsc_class *,
			     struct service_curve *, struct service_curve *);
static struct hfsc_class *hfsc_nextclass(struct hfsc_class *);

static int hfsc_enqueue(struct ifaltq *, struct mbuf *, struct altq_pktattr *);
static struct mbuf *hfsc_dequeue(struct ifaltq *, int);

static int hfsc_addq(struct hfsc_class *, struct mbuf *);
static struct mbuf *hfsc_getq(struct hfsc_class *);
static struct mbuf *hfsc_pollq(struct hfsc_class *);
static void hfsc_purgeq(struct hfsc_class *);

static void set_active(struct hfsc_class *, int);
static void set_passive(struct hfsc_class *);

static void init_ed(struct hfsc_class *, int);
static void update_ed(struct hfsc_class *, int);
static void update_d(struct hfsc_class *, int);
static void init_v(struct hfsc_class *, int);
static void update_v(struct hfsc_class *, int);
static ellist_t *ellist_alloc(void);
static void ellist_destroy(ellist_t *);
static void ellist_insert(struct hfsc_class *);
static void ellist_remove(struct hfsc_class *);
static void ellist_update(struct hfsc_class *);
struct hfsc_class *ellist_get_mindl(ellist_t *);
static actlist_t *actlist_alloc(void);
static void actlist_destroy(actlist_t *);
static void actlist_insert(struct hfsc_class *);
static void actlist_remove(struct hfsc_class *);
static void actlist_update(struct hfsc_class *);

static __inline u_int64_t seg_x2y(u_int64_t, u_int64_t);
static __inline u_int64_t seg_y2x(u_int64_t, u_int64_t);
static __inline u_int64_t m2sm(u_int);
static __inline u_int64_t m2ism(u_int);
static __inline u_int64_t d2dx(u_int);
static u_int sm2m(u_int64_t);
static u_int dx2d(u_int64_t);

static void sc2isc(struct service_curve *, struct internal_sc *);
static void rtsc_init(struct runtime_sc *, struct internal_sc *,
		      u_int64_t, u_int64_t);
static u_int64_t rtsc_y2x(struct runtime_sc *, u_int64_t);
static u_int64_t rtsc_x2y(struct runtime_sc *, u_int64_t);
static void rtsc_min(struct runtime_sc *, struct internal_sc *,
		     u_int64_t, u_int64_t);

int hfscopen(dev_t, int, int, struct proc *);
int hfscclose(dev_t, int, int, struct proc *);
int hfscioctl(dev_t, ioctlcmd_t, caddr_t, int, struct proc *);
static int hfsccmd_if_attach(struct hfsc_attach *);
static int hfsccmd_if_detach(struct hfsc_interface *);
static int hfsccmd_add_class(struct hfsc_add_class *);
static int hfsccmd_delete_class(struct hfsc_delete_class *);
static int hfsccmd_modify_class(struct hfsc_modify_class *);
static int hfsccmd_add_filter(struct hfsc_add_filter *);
static int hfsccmd_delete_filter(struct hfsc_delete_filter *);
static int hfsccmd_class_stats(struct hfsc_class_stats *);
static void get_class_stats(struct class_stats *, struct hfsc_class *);
static struct hfsc_class *clh_to_clp(struct hfsc_if *, u_long);
static u_long clp_to_clh(struct hfsc_class *);
d474 1
a474 1
	struct internal_sc *rsc_tmp, *fsc_tmp;
a476 15
	if (rsc != NULL && (rsc->m1 != 0 || rsc->m2 != 0) &&
	    cl->cl_rsc == NULL) {
		MALLOC(rsc_tmp, struct internal_sc *,
		       sizeof(struct internal_sc), M_DEVBUF, M_WAITOK);
		if (rsc_tmp == NULL)
			return (ENOMEM);
	}
	if (fsc != NULL && (fsc->m1 != 0 || fsc->m2 != 0) &&
	    cl->cl_fsc == NULL) {
		MALLOC(fsc_tmp, struct internal_sc *,
		       sizeof(struct internal_sc), M_DEVBUF, M_WAITOK);
		if (fsc_tmp == NULL)
			return (ENOMEM);
	}

d488 10
a497 2
			if (cl->cl_rsc == NULL)
				cl->cl_rsc = rsc_tmp;
d512 10
a521 2
			if (cl->cl_fsc == NULL)
				cl->cl_fsc = fsc_tmp;
a853 10
		/*
		 * if parent became idle while this class was idle.
		 * reset vt and the runtime service curve.
		 */
		if (cl->cl_parent->cl_nactive == 0 ||
		    cl->cl_parent->cl_vtperiod != cl->cl_parentperiod) {
			cl->cl_vt = 0;
			rtsc_init(&cl->cl_virtual, cl->cl_fsc,
				  0, cl->cl_total);
		}
d865 6
a870 3
			if (cl->cl_parent->cl_vtperiod != cl->cl_parentperiod
			    || vt > cl->cl_vt)
				cl->cl_vt = vt;
d874 2
a875 1
		rtsc_min(&cl->cl_virtual, cl->cl_fsc, cl->cl_vt, cl->cl_total);
@


1.3.2.2
log
@sync
@
text
@d2 1
a2 1
/*	$KAME: altq_hfsc.c,v 1.17 2002/11/29 07:48:33 kjc Exp $	*/
d38 1
a38 1
 * "A Hierarchical Fair Service Curve Algorithm for Link-Sharing,
a40 5
 *
 * Oleg Cherevko <olwi@@aq.ml.com.ua> added the upperlimit for link-sharing.
 * when a class has an upperlimit, the fit-time is computed from the
 * upperlimit service curve.  the link-sharing scheduler does not schedule
 * a class whose fit-time exceeds the current time.
d47 1
d49 1
d51 1
d55 1
a55 1
#include <netinet/in.h>
a56 1
#include <net/pfvar.h>
d58 1
d64 70
a133 60
static int			 hfsc_clear_interface(struct hfsc_if *);
static int			 hfsc_request(struct ifaltq *, int, void *);
static void			 hfsc_purge(struct hfsc_if *);
static struct hfsc_class	*hfsc_class_create(struct hfsc_if *,
    struct service_curve *, struct service_curve *, struct service_curve *,
    struct hfsc_class *, int, int, int);
static int			 hfsc_class_destroy(struct hfsc_class *);
static struct hfsc_class	*hfsc_nextclass(struct hfsc_class *);
static int			 hfsc_enqueue(struct ifaltq *, struct mbuf *,
				    struct altq_pktattr *);
static struct mbuf		*hfsc_dequeue(struct ifaltq *, int);

static int		 hfsc_addq(struct hfsc_class *, struct mbuf *);
static struct mbuf	*hfsc_getq(struct hfsc_class *);
static struct mbuf	*hfsc_pollq(struct hfsc_class *);
static void		 hfsc_purgeq(struct hfsc_class *);

static void		 update_cfmin(struct hfsc_class *);
static void		 set_active(struct hfsc_class *, int);
static void		 set_passive(struct hfsc_class *);

static void		 init_ed(struct hfsc_class *, int);
static void		 update_ed(struct hfsc_class *, int);
static void		 update_d(struct hfsc_class *, int);
static void		 init_vf(struct hfsc_class *, int);
static void		 update_vf(struct hfsc_class *, int, u_int64_t);
static ellist_t		*ellist_alloc(void);
static void		 ellist_destroy(ellist_t *);
static void		 ellist_insert(struct hfsc_class *);
static void		 ellist_remove(struct hfsc_class *);
static void		 ellist_update(struct hfsc_class *);
struct hfsc_class	*ellist_get_mindl(ellist_t *, u_int64_t);
static actlist_t	*actlist_alloc(void);
static void		 actlist_destroy(actlist_t *);
static void		 actlist_insert(struct hfsc_class *);
static void		 actlist_remove(struct hfsc_class *);
static void		 actlist_update(struct hfsc_class *);

static struct hfsc_class	*actlist_firstfit(struct hfsc_class *,
				    u_int64_t);

static __inline u_int64_t	seg_x2y(u_int64_t, u_int64_t);
static __inline u_int64_t	seg_y2x(u_int64_t, u_int64_t);
static __inline u_int64_t	m2sm(u_int);
static __inline u_int64_t	m2ism(u_int);
static __inline u_int64_t	d2dx(u_int);
static u_int			sm2m(u_int64_t);
static u_int			dx2d(u_int64_t);

static void		sc2isc(struct service_curve *, struct internal_sc *);
static void		rtsc_init(struct runtime_sc *, struct internal_sc *,
			    u_int64_t, u_int64_t);
static u_int64_t	rtsc_y2x(struct runtime_sc *, u_int64_t);
static u_int64_t	rtsc_x2y(struct runtime_sc *, u_int64_t);
static void		rtsc_min(struct runtime_sc *, struct internal_sc *,
			    u_int64_t, u_int64_t);

static void			 get_class_stats(struct hfsc_classstats *,
				    struct hfsc_class *);
static struct hfsc_class	*clh_to_clp(struct hfsc_if *, u_int32_t);
d140 2
a141 1
#define	HT_INFINITY	0xffffffffffffffffLL	/* infinite time value */
d143 4
a146 17
int
hfsc_pfattach(struct pf_altq *a)
{
	struct ifnet *ifp;
	int s, error;

	if ((ifp = ifunit(a->ifname)) == NULL || a->altq_disc == NULL)
		return (EINVAL);
	s = splimp();
	error = altq_attach(&ifp->if_snd, ALTQT_HFSC, a->altq_disc,
	    hfsc_enqueue, hfsc_dequeue, hfsc_request, NULL, NULL);
	splx(s);
	return (error);
}

int
hfsc_add_altq(struct pf_altq *a)
d149 1
a149 6
	struct ifnet *ifp;

	if ((ifp = ifunit(a->ifname)) == NULL)
		return (EINVAL);
	if (!ALTQ_IS_READY(&ifp->if_snd))
		return (ENODEV);
d152 1
a152 1
	    M_DEVBUF, M_WAITOK);
d154 1
a154 1
		return (ENOMEM);
d160 1
a160 1
		return (ENOMEM);
d163 13
a175 1
	hif->hif_ifq = &ifp->if_snd;
d177 3
a179 2
	/* keep the state in pf_altq */
	a->altq_disc = hif;
d181 1
a181 1
	return (0);
d184 3
a186 2
int
hfsc_remove_altq(struct pf_altq *a)
a187 6
	struct hfsc_if *hif;

	if ((hif = a->altq_disc) == NULL)
		return (EINVAL);
	a->altq_disc = NULL;

d191 14
a211 81
int
hfsc_add_queue(struct pf_altq *a)
{
	struct hfsc_if *hif;
	struct hfsc_class *cl, *parent;
	struct hfsc_opts *opts;
	struct service_curve rtsc, lssc, ulsc;

	if ((hif = a->altq_disc) == NULL)
		return (EINVAL);

	opts = &a->pq_u.hfsc_opts;

	parent = NULL;
	if (a->qid != HFSC_ROOTCLASS_HANDLE)
		if ((parent = clh_to_clp(hif, a->parent_qid)) == NULL)
			return (EINVAL);

	if (a->qid >= HFSC_MAX_CLASSES || a->qid == 0)
		return (EINVAL);
	if (clh_to_clp(hif, a->qid) != NULL)
		return (EBUSY);

	rtsc.m1 = opts->rtsc_m1;
	rtsc.d  = opts->rtsc_d;
	rtsc.m2 = opts->rtsc_m2;
	lssc.m1 = opts->lssc_m1;
	lssc.d  = opts->lssc_d;
	lssc.m2 = opts->lssc_m2;
	ulsc.m1 = opts->ulsc_m1;
	ulsc.d  = opts->ulsc_d;
	ulsc.m2 = opts->ulsc_m2;

	cl = hfsc_class_create(hif, &rtsc, &lssc, &ulsc,
	    parent, a->qlimit, opts->flags, a->qid);
	if (cl == NULL)
		return (ENOMEM);

	return (0);
}

int
hfsc_remove_queue(struct pf_altq *a)
{
	struct hfsc_if *hif;
	struct hfsc_class *cl;

	if ((hif = a->altq_disc) == NULL)
		return (EINVAL);

	if ((cl = clh_to_clp(hif, a->qid)) == NULL)
		return (EINVAL);

	return (hfsc_class_destroy(cl));
}

int
hfsc_getqstats(struct pf_altq *a, void *ubuf, int *nbytes)
{
	struct hfsc_if *hif;
	struct hfsc_class *cl;
	struct hfsc_classstats stats;
	int error = 0;

	if ((hif = altq_lookup(a->ifname, ALTQT_HFSC)) == NULL)
		return (EBADF);

	if ((cl = clh_to_clp(hif, a->qid)) == NULL)
		return (EINVAL);

	if (*nbytes < sizeof(stats))
		return (EINVAL);

	get_class_stats(&stats, cl);

	if ((error = copyout((caddr_t)&stats, ubuf, sizeof(stats))) != 0)
		return (error);
	*nbytes = sizeof(stats);
	return (0);
}

d217 2
a218 1
hfsc_clear_interface(struct hfsc_if *hif)
d222 3
d226 1
a226 2
	while (hif->hif_rootclass != NULL &&
	    (cl = hif->hif_rootclass->cl_children) != NULL) {
d238 1
a238 1

d243 4
a246 1
hfsc_request(struct ifaltq *ifq, int req, void *arg)
d260 2
a261 1
hfsc_purge(struct hfsc_if *hif)
d273 5
a277 3
hfsc_class_create(struct hfsc_if *hif, struct service_curve *rsc,
    struct service_curve *fsc, struct service_curve *usc,
    struct hfsc_class *parent, int qlimit, int flags, int qid)
a283 1
#ifdef ALTQ_DEBUG
a284 1
#endif
a313 9
		u_int m2;

		m2 = 0;
		if (rsc != NULL && rsc->m2 > m2)
			m2 = rsc->m2;
		if (fsc != NULL && fsc->m2 > m2)
			m2 = fsc->m2;
		if (usc != NULL && usc->m2 > m2)
			m2 = usc->m2;
d322 1
a322 1
		if (m2 < 8)
d326 1
a326 1
				* 1000 * 1000 * 1000 / (m2 / 8);
d328 2
a329 4
			cl->cl_red = red_alloc(0, 0,
			    qlimit(cl->cl_q) * 10/100,
			    qlimit(cl->cl_q) * 30/100,
			    red_flags, red_pkttime);
d336 1
a336 1
			    red_flags, red_pkttime);
d344 1
a344 1
	if (rsc != NULL && (rsc->m1 != 0 || rsc->m2 != 0)) {
d346 1
a346 1
		    sizeof(struct internal_sc), M_DEVBUF, M_WAITOK);
d349 2
a350 1
		sc2isc(rsc, cl->cl_rsc);
d353 1
a353 2
	}
	if (fsc != NULL && (fsc->m1 != 0 || fsc->m2 != 0)) {
d355 1
a355 1
		    sizeof(struct internal_sc), M_DEVBUF, M_WAITOK);
d358 2
a359 1
		sc2isc(fsc, cl->cl_fsc);
a361 8
	if (usc != NULL && (usc->m1 != 0 || usc->m2 != 0)) {
		MALLOC(cl->cl_usc, struct internal_sc *,
		    sizeof(struct internal_sc), M_DEVBUF, M_WAITOK);
		if (cl->cl_usc == NULL)
			goto err_ret;
		sc2isc(usc, cl->cl_usc);
		rtsc_init(&cl->cl_ulimit, cl->cl_usc, 0, 0);
	}
d364 1
a364 1
	cl->cl_handle = qid;
a369 1
	hif->hif_class_tbl[qid - 1] = cl;
a404 2
	if (cl->cl_usc != NULL)
		FREE(cl->cl_usc, M_DEVBUF);
d412 2
a413 1
hfsc_class_destroy(struct hfsc_class *cl)
a416 3
	if (cl == NULL)
		return (0);

d422 3
a442 1
	cl->cl_hif->hif_class_tbl[cl->cl_handle - 1] = NULL;
a457 2
	if (cl->cl_usc != NULL)
		FREE(cl->cl_usc, M_DEVBUF);
d468 62
d533 1
a533 1
 *	for (cl = hif->hif_rootclass; cl != NULL; cl = hfsc_nextclass(cl))
d537 2
a538 1
hfsc_nextclass(struct hfsc_class *cl)
d559 5
a563 2
static int
hfsc_enqueue(struct ifaltq *ifq, struct mbuf *m, struct altq_pktattr *pktattr)
a566 1
	struct m_tag *t;
d570 1
a570 11
	if ((m->m_flags & M_PKTHDR) == 0) {
		/* should not happen */
		printf("altq: packet for %s does not have pkthdr\n",
		    ifq->altq_ifp->if_xname);
		m_freem(m);
		return (ENOBUFS);
	}
	t = m_tag_find(m, PACKET_TAG_PF_QID, NULL);
	if (t == NULL ||
	    (cl = clh_to_clp(hif, ((struct altq_tag *)(t+1))->qid)) == NULL ||
		is_a_parent_class(cl)) {
d572 1
a572 6
		if (cl == NULL) {
			m_freem(m);
			return (ENOBUFS);
		}
		cl->cl_pktattr = NULL;
	}
d587 3
d603 3
a605 1
hfsc_dequeue(struct ifaltq *ifq, int op)
a611 1
	u_int64_t cur_time;
a616 2
	cur_time = read_machclk();

d618 2
a619 1

d623 2
a624 1
		if (cl->cl_rsc != NULL)
d626 1
d633 1
a633 2
		if ((cl = ellist_get_mindl(hif->hif_eligible, cur_time))
		    != NULL) {
a635 3
#ifdef ALTQ_DEBUG
			int fits = 0;
#endif
d642 2
a643 7

				cl = actlist_firstfit(cl, cur_time);
				if (cl == NULL) {
#ifdef ALTQ_DEBUG
					if (fits > 0)
						printf("%d fit but none found\n",fits);
#endif
a644 10
				}
				/*
				 * update parent's cl_cvtmin.
				 * don't update if the new vt is smaller.
				 */
				if (cl->cl_parent->cl_cvtmin < cl->cl_vt)
					cl->cl_parent->cl_cvtmin = cl->cl_vt;
#ifdef ALTQ_DEBUG
				fits++;
#endif
a655 2
	if (m == NULL)
		panic("hfsc_dequeue:");
d661 1
a661 1
	update_vf(cl, len, cur_time);
d669 1
a669 1

d680 4
d688 3
a690 1
hfsc_addq(struct hfsc_class *cl, struct mbuf *m)
d716 2
a717 1
hfsc_getq(struct hfsc_class *cl)
d731 2
a732 1
hfsc_pollq(struct hfsc_class *cl)
d738 2
a739 1
hfsc_purgeq(struct hfsc_class *cl)
a748 2
		cl->cl_hif->hif_packets--;
		IFQ_DEC_LEN(cl->cl_hif->hif_ifq);
d751 1
a751 2

	update_vf(cl, 0, 0);	/* remove cl from the actlist */
d755 4
a758 2
static void
set_active(struct hfsc_class *cl, int len)
d763 1
a763 1
		init_vf(cl, len);
d768 3
a770 2
static void
set_passive(struct hfsc_class *cl)
d775 13
a787 4
	/*
	 * actlist is now handled in update_vf() so that update_vf(cl, 0, 0)
	 * needs to be called explicitly to remove a class from actlist
	 */
d790 4
a793 2
static void
init_ed(struct hfsc_class *cl, int next_len)
d820 4
a823 2
static void
update_ed(struct hfsc_class *cl, int next_len)
d831 4
a834 2
static void
update_d(struct hfsc_class *cl, int next_len)
d839 4
a842 2
static void
init_vf(struct hfsc_class *cl, int len)
d844 1
a844 7
	struct hfsc_class *max_cl, *p;
	u_int64_t vt, f, cur_time;
	int go_active;

	cur_time = 0;
	go_active = 1;
	for ( ; cl->cl_parent != NULL; cl = cl->cl_parent) {
d846 19
a864 4
		if (go_active && cl->cl_nactive++ == 0)
			go_active = 1;
		else
			go_active = 0;
d866 5
a870 1
		if (go_active) {
d872 8
a879 29
			if (max_cl != NULL) {
				/*
				 * set vt to the average of the min and max
				 * classes.  if the parent's period didn't
				 * change, don't decrease vt of the class.
				 */
				vt = max_cl->cl_vt;
				if (cl->cl_parent->cl_cvtmin != 0)
					vt = (cl->cl_parent->cl_cvtmin + vt)/2;

				if (cl->cl_parent->cl_vtperiod !=
				    cl->cl_parentperiod || vt > cl->cl_vt)
					cl->cl_vt = vt;
			} else {
				/*
				 * first child for a new parent backlog period.
				 * add parent's cvtmax to vtoff of children
				 * to make a new vt (vtoff + vt) larger than
				 * the vt in the last period for all children.
				 */
				vt = cl->cl_parent->cl_cvtmax;
				for (p = cl->cl_parent->cl_children; p != NULL;
				     p = p->cl_siblings)
					p->cl_vtoff += vt;
				cl->cl_vt = 0;
				cl->cl_parent->cl_cvtmax = 0;
				cl->cl_parent->cl_cvtmin = 0;
			}
			cl->cl_initvt = cl->cl_vt;
d881 4
a884 8
			/* update the virtual curve */
			vt = cl->cl_vt + cl->cl_vtoff;
			rtsc_min(&cl->cl_virtual, cl->cl_fsc, vt, cl->cl_total);
			if (cl->cl_virtual.x == vt) {
				cl->cl_virtual.x -= cl->cl_vtoff;
				cl->cl_vtoff = 0;
			}
			cl->cl_vtadj = 0;
d886 1
a886 22
			cl->cl_vtperiod++;  /* increment vt period */
			cl->cl_parentperiod = cl->cl_parent->cl_vtperiod;
			if (cl->cl_parent->cl_nactive == 0)
				cl->cl_parentperiod++;
			cl->cl_f = 0;

			actlist_insert(cl);

			if (cl->cl_usc != NULL) {
				/* class has upper limit curve */
				if (cur_time == 0)
					cur_time = read_machclk();

				/* update the ulimit curve */
				rtsc_min(&cl->cl_ulimit, cl->cl_usc, cur_time,
				    cl->cl_total);
				/* compute myf */
				cl->cl_myf = rtsc_y2x(&cl->cl_ulimit,
				    cl->cl_total);
				cl->cl_myfadj = 0;
			}
		}
d888 2
a889 8
		if (cl->cl_myf > cl->cl_cfmin)
			f = cl->cl_myf;
		else
			f = cl->cl_cfmin;
		if (f != cl->cl_f) {
			cl->cl_f = f;
			update_cfmin(cl->cl_parent);
		}
d893 4
a896 2
static void
update_vf(struct hfsc_class *cl, int len, u_int64_t cur_time)
d898 1
a898 6
	u_int64_t f, myf_bound, delta;
	int go_passive;

	go_passive = qempty(cl->cl_q);

	for (; cl->cl_parent != NULL; cl = cl->cl_parent) {
d902 2
a903 14
		if (cl->cl_fsc == NULL || cl->cl_nactive == 0)
			continue;

		if (go_passive && --cl->cl_nactive == 0)
			go_passive = 1;
		else
			go_passive = 0;

		if (go_passive) {
			/* no more active child, going passive */

			/* update cvtmax of the parent class */
			if (cl->cl_vt > cl->cl_parent->cl_cvtmax)
				cl->cl_parent->cl_cvtmax = cl->cl_vt;
d905 2
a906 44
			/* remove this class from the vt list */
			actlist_remove(cl);

			update_cfmin(cl->cl_parent);

			continue;
		}

		/*
		 * update vt and f
		 */
		cl->cl_vt = rtsc_y2x(&cl->cl_virtual, cl->cl_total)
		    - cl->cl_vtoff + cl->cl_vtadj;

		/*
		 * if vt of the class is smaller than cvtmin,
		 * the class was skipped in the past due to non-fit.
		 * if so, we need to adjust vtadj.
		 */
		if (cl->cl_vt < cl->cl_parent->cl_cvtmin) {
			cl->cl_vtadj += cl->cl_parent->cl_cvtmin - cl->cl_vt;
			cl->cl_vt = cl->cl_parent->cl_cvtmin;
		}

		/* update the vt list */
		actlist_update(cl);

		if (cl->cl_usc != NULL) {
			cl->cl_myf = cl->cl_myfadj
			    + rtsc_y2x(&cl->cl_ulimit, cl->cl_total);

			/*
			 * if myf lags behind by more than one clock tick
			 * from the current time, adjust myfadj to prevent
			 * a rate-limited class from going greedy.
			 * in a steady state under rate-limiting, myf
			 * fluctuates within one clock tick.
			 */
			myf_bound = cur_time - machclk_per_tick;
			if (cl->cl_myf < myf_bound) {
				delta = cur_time - cl->cl_myf;
				cl->cl_myfadj += delta;
				cl->cl_myf += delta;
			}
d909 2
a910 9
		/* cl_f is max(cl_myf, cl_cfmin) */
		if (cl->cl_myf > cl->cl_cfmin)
			f = cl->cl_myf;
		else
			f = cl->cl_cfmin;
		if (f != cl->cl_f) {
			cl->cl_f = f;
			update_cfmin(cl->cl_parent);
		}
a913 22
static void
update_cfmin(struct hfsc_class *cl)
{
	struct hfsc_class *p;
	u_int64_t cfmin;

	if (TAILQ_EMPTY(cl->cl_actc)) {
		cl->cl_cfmin = 0;
		return;
	}
	cfmin = HT_INFINITY;
	TAILQ_FOREACH(p, cl->cl_actc, cl_actlist) {
		if (p->cl_f == 0) {
			cl->cl_cfmin = 0;
			return;
		}
		if (p->cl_f < cfmin)
			cfmin = p->cl_f;
	}
	cl->cl_cfmin = cfmin;
}

d924 1
a924 1
ellist_alloc(void)
d927 1
a927 1

d934 2
a935 1
ellist_destroy(ellist_t *head)
d940 3
a942 2
static void
ellist_insert(struct hfsc_class *cl)
d963 3
a965 2
static void
ellist_remove(struct hfsc_class *cl)
d968 1
a968 1

d972 3
a974 2
static void
ellist_update(struct hfsc_class *cl)
d1012 2
a1013 1
ellist_get_mindl(ellist_t *head, u_int64_t cur_time)
d1016 3
d1035 1
a1035 1
actlist_alloc(void)
d1038 1
a1038 1

d1045 2
a1046 1
actlist_destroy(actlist_t *head)
d1050 3
a1052 2
static void
actlist_insert(struct hfsc_class *cl)
d1072 3
a1074 2
static void
actlist_remove(struct hfsc_class *cl)
d1080 2
a1081 1
actlist_update(struct hfsc_class *cl)
d1091 1
a1091 1
	if (p == NULL || cl->cl_vt < p->cl_vt)
a1116 12
static struct hfsc_class *
actlist_firstfit(struct hfsc_class *cl, u_int64_t cur_time)
{
	struct hfsc_class *p;

	TAILQ_FOREACH(p, cl->cl_actc, cl_actlist) {
		if (p->cl_f <= cur_time)
			return (p);
	}
	return (NULL);
}

d1133 1
a1133 1
 *  bits/sec    100Kbps     1Mbps     10Mbps     100Mbps    1Gbps
d1138 1
a1138 1
 *
d1146 2
a1147 2
#define	SM_MASK		((1LL << SM_SHIFT) - 1)
#define	ISM_MASK	((1LL << ISM_SHIFT) - 1)
d1149 4
a1152 2
static __inline u_int64_t
seg_x2y(u_int64_t x, u_int64_t sm)
d1156 4
a1159 6
	/*
	 * compute
	 *	y = x * sm >> SM_SHIFT
	 * but divide it for the upper and lower bits to avoid overflow
	 */
	y = (x >> SM_SHIFT) * sm + (((x & SM_MASK) * sm) >> SM_SHIFT);
d1163 4
a1166 2
static __inline u_int64_t
seg_y2x(u_int64_t y, u_int64_t ism)
d1172 6
a1177 6
	else if (ism == HT_INFINITY)
		x = HT_INFINITY;
	else {
		x = (y >> ISM_SHIFT) * ism
		    + (((y & ISM_MASK) * ism) >> ISM_SHIFT);
	}
d1181 3
a1183 2
static __inline u_int64_t
m2sm(u_int m)
d1191 3
a1193 2
static __inline u_int64_t
m2ism(u_int m)
d1198 1
a1198 1
		ism = HT_INFINITY;
d1204 3
a1206 2
static __inline u_int64_t
d2dx(u_int d)
d1209 1
a1209 1

d1214 3
a1216 2
static u_int
sm2m(u_int64_t sm)
d1224 3
a1226 2
static u_int
dx2d(u_int64_t dx)
d1234 4
a1237 2
static void
sc2isc(struct service_curve *sc, struct internal_sc *isc)
d1251 5
a1255 3
static void
rtsc_init(struct runtime_sc *rtsc, struct internal_sc * isc, u_int64_t x,
    u_int64_t y)
d1271 4
a1274 2
static u_int64_t
rtsc_y2x(struct runtime_sc *rtsc, u_int64_t y)
d1294 4
a1297 2
static u_int64_t
rtsc_x2y(struct runtime_sc *rtsc, u_int64_t x)
d1317 5
a1321 3
static void
rtsc_min(struct runtime_sc *rtsc, struct internal_sc *isc, u_int64_t x,
    u_int64_t y)
d1368 1
a1368 1
	 */
d1380 336
a1715 2
static void
get_class_stats(struct hfsc_classstats *sp, struct hfsc_class *cl)
d1718 1
a1718 1
	sp->class_handle = cl->cl_handle;
a1737 9
	if (cl->cl_usc != NULL) {
		sp->usc.m1 = sm2m(cl->cl_usc->sm1);
		sp->usc.d = dx2d(cl->cl_usc->dx);
		sp->usc.m2 = sm2m(cl->cl_usc->sm2);
	} else {
		sp->usc.m1 = 0;
		sp->usc.d = 0;
		sp->usc.m2 = 0;
	}
a1744 16
	sp->f = cl->cl_f;

	sp->initvt = cl->cl_initvt;
	sp->vtperiod = cl->cl_vtperiod;
	sp->parentperiod = cl->cl_parentperiod;
	sp->nactive = cl->cl_nactive;
	sp->vtoff = cl->cl_vtoff;
	sp->cvtmax = cl->cl_cvtmax;
	sp->myf = cl->cl_myf;
	sp->cfmin = cl->cl_cfmin;
	sp->cvtmin = cl->cl_cvtmin;
	sp->myfadj = cl->cl_myfadj;
	sp->vtadj = cl->cl_vtadj;

	sp->cur_time = read_machclk();
	sp->machclk_freq = machclk_freq;
a1746 1
	sp->qlimit = qlimit(cl->cl_q);
d1764 3
a1766 1
clh_to_clp(struct hfsc_if *hif, u_int32_t chandle)
d1768 1
a1768 1
	u_int idx;
d1770 5
a1774 1
	if (chandle == 0)
d1776 3
a1778 2
	idx = chandle - 1;
	if (idx >= HFSC_MAX_CLASSES)
d1780 12
a1791 1
	return (hif->hif_class_tbl[idx]);
d1793 9
@


1.2
log
@change a maze of altq options into just "altq" for the base+red+cbq, and
then altq_* for each of the other * experimental options.  and.. enable
it by default in GENERIC.
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.1 2001/06/27 05:28:35 kjc Exp $	*/
d323 1
a323 1
		if (sc->m2 == 0)
@


1.1
log
@import ALTQ, alternate queueing support, from KAME.
ALTQ allows to switch various queueing disciplines on output network
interfaces.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a42 12
#if defined(__FreeBSD__) || defined(__NetBSD__)
#include "opt_altq.h"
#if (__FreeBSD__ != 2)
#include "opt_inet.h"
#ifdef __FreeBSD__
#include "opt_inet6.h"
#endif
#endif
#endif /* __FreeBSD__ || __NetBSD__ */

#ifdef ALTQ_HFSC  /* hfsc is enabled by ALTQ_HFSC option in opt_altq.h */

a1797 2

#endif /* ALTQ_HFSC */
@


1.1.2.1
log
@Sync the SMP branch to something just after 3.0
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.1 2001/06/27 05:28:35 kjc Exp $	*/
d43 12
d335 1
a335 1
		if (sc->m2 < 8)
d1810 2
@


1.1.2.2
log
@Merge in -current from about a week ago
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d64 71
a134 71
static struct hfsc_if *hfsc_attach(struct ifaltq *, u_int);
static int hfsc_detach(struct hfsc_if *);
static int hfsc_clear_interface(struct hfsc_if *);
static int hfsc_request(struct ifaltq *, int, void *);
static void hfsc_purge(struct hfsc_if *);
static struct hfsc_class *hfsc_class_create(struct hfsc_if *,
		 struct service_curve *, struct hfsc_class *, int, int);
static int hfsc_class_destroy(struct hfsc_class *);
static int hfsc_class_modify(struct hfsc_class *,
			  struct service_curve *, struct service_curve *);
static struct hfsc_class *hfsc_nextclass(struct hfsc_class *);

static int hfsc_enqueue(struct ifaltq *, struct mbuf *,
			     struct altq_pktattr *);
static struct mbuf *hfsc_dequeue(struct ifaltq *, int);

static int hfsc_addq(struct hfsc_class *, struct mbuf *);
static struct mbuf *hfsc_getq(struct hfsc_class *);
static struct mbuf *hfsc_pollq(struct hfsc_class *);
static void hfsc_purgeq(struct hfsc_class *);

static void set_active(struct hfsc_class *, int);
static void set_passive(struct hfsc_class *);

static void init_ed(struct hfsc_class *, int);
static void update_ed(struct hfsc_class *, int);
static void update_d(struct hfsc_class *, int);
static void init_v(struct hfsc_class *, int);
static void update_v(struct hfsc_class *, int);
static ellist_t *ellist_alloc(void);
static void ellist_destroy(ellist_t *);
static void ellist_insert(struct hfsc_class *);
static void ellist_remove(struct hfsc_class *);
static void ellist_update(struct hfsc_class *);
struct hfsc_class *ellist_get_mindl(ellist_t *);
static actlist_t *actlist_alloc(void);
static void actlist_destroy(actlist_t *);
static void actlist_insert(struct hfsc_class *);
static void actlist_remove(struct hfsc_class *);
static void actlist_update(struct hfsc_class *);

static __inline u_int64_t seg_x2y(u_int64_t, u_int64_t);
static __inline u_int64_t seg_y2x(u_int64_t, u_int64_t);
static __inline u_int64_t m2sm(u_int);
static __inline u_int64_t m2ism(u_int);
static __inline u_int64_t d2dx(u_int);
static u_int sm2m(u_int64_t);
static u_int dx2d(u_int64_t);

static void sc2isc(struct service_curve *, struct internal_sc *);
static void rtsc_init(struct runtime_sc *, struct internal_sc *,
			   u_int64_t, u_int64_t);
static u_int64_t rtsc_y2x(struct runtime_sc *, u_int64_t);
static u_int64_t rtsc_x2y(struct runtime_sc *, u_int64_t);
static void rtsc_min(struct runtime_sc *, struct internal_sc *,
			  u_int64_t, u_int64_t);

int hfscopen(dev_t, int, int, struct proc *);
int hfscclose(dev_t, int, int, struct proc *);
int hfscioctl(dev_t, ioctlcmd_t, caddr_t, int, struct proc *);
static int hfsccmd_if_attach(struct hfsc_attach *);
static int hfsccmd_if_detach(struct hfsc_interface *);
static int hfsccmd_add_class(struct hfsc_add_class *);
static int hfsccmd_delete_class(struct hfsc_delete_class *);
static int hfsccmd_modify_class(struct hfsc_modify_class *);
static int hfsccmd_add_filter(struct hfsc_add_filter *);
static int hfsccmd_delete_filter(struct hfsc_delete_filter *);
static int hfsccmd_class_stats(struct hfsc_class_stats *);
static void get_class_stats(struct class_stats *, struct hfsc_class *);
static struct hfsc_class *clh_to_clp(struct hfsc_if *, u_long);
static u_long clp_to_clh(struct hfsc_class *);
@


1.1.2.3
log
@Sync the SMP branch with 3.3
@
text
@d2 1
a2 1
/*	$KAME: altq_hfsc.c,v 1.17 2002/11/29 07:48:33 kjc Exp $	*/
d38 1
a38 1
 * "A Hierarchical Fair Service Curve Algorithm for Link-Sharing,
a40 5
 *
 * Oleg Cherevko <olwi@@aq.ml.com.ua> added the upperlimit for link-sharing.
 * when a class has an upperlimit, the fit-time is computed from the
 * upperlimit service curve.  the link-sharing scheduler does not schedule
 * a class whose fit-time exceeds the current time.
d47 1
d49 1
d51 1
d55 1
a55 1
#include <netinet/in.h>
a56 1
#include <net/pfvar.h>
d58 1
d64 2
d70 1
a70 2
    struct service_curve *, struct service_curve *, struct service_curve *,
    struct hfsc_class *, int, int, int);
d72 2
d75 3
a77 1
static int hfsc_enqueue(struct ifaltq *, struct mbuf *, struct altq_pktattr *);
a84 1
static void update_cfmin(struct hfsc_class *);
d91 2
a92 2
static void init_vf(struct hfsc_class *, int);
static void update_vf(struct hfsc_class *, int, u_int64_t);
d98 1
a98 1
struct hfsc_class *ellist_get_mindl(ellist_t *, u_int64_t);
a103 1
static struct hfsc_class *actlist_firstfit(struct hfsc_class *, u_int64_t);
d115 1
a115 1
		      u_int64_t, u_int64_t);
d119 1
a119 1
		     u_int64_t, u_int64_t);
d121 14
a134 2
static void get_class_stats(struct hfsc_classstats *, struct hfsc_class *);
static struct hfsc_class *clh_to_clp(struct hfsc_if *, u_int32_t);
d141 2
a142 1
#define	HT_INFINITY	0xffffffffffffffffLL	/* infinite time value */
d144 4
a147 17
int
hfsc_pfattach(struct pf_altq *a)
{
	struct ifnet *ifp;
	int s, error;

	if ((ifp = ifunit(a->ifname)) == NULL || a->altq_disc == NULL)
		return (EINVAL);
	s = splimp();
	error = altq_attach(&ifp->if_snd, ALTQT_HFSC, a->altq_disc,
	    hfsc_enqueue, hfsc_dequeue, hfsc_request, NULL, NULL);
	splx(s);
	return (error);
}

int
hfsc_add_altq(struct pf_altq *a)
a149 1
	struct ifnet *ifp;
a151 5
	if ((ifp = ifunit(a->ifname)) == NULL)
		return (EINVAL);
	if (!ALTQ_IS_READY(&ifp->if_snd))
		return (ENODEV);

d153 1
a153 1
	    M_DEVBUF, M_WAITOK);
d155 1
a155 1
		return (ENOMEM);
d161 1
a161 1
		return (ENOMEM);
d164 1
a164 1
	hif->hif_ifq = &ifp->if_snd;
d169 1
a169 1
	root_sc.m1 = a->ifbandwidth;
d171 3
a173 4
	root_sc.m2 = a->ifbandwidth;
	if ((hif->hif_rootclass = hfsc_class_create(hif,
	    &root_sc, &root_sc, NULL, NULL, 0, 0, 0)) == NULL) {
		ellist_destroy(hif->hif_eligible);
d175 1
a175 1
		return (ENOMEM);
d178 3
a180 2
	/* keep the state in pf_altq */
	a->altq_disc = hif;
d182 1
a182 1
	return (0);
d185 3
a187 2
int
hfsc_remove_altq(struct pf_altq *a)
a188 6
	struct hfsc_if *hif;

	if ((hif = a->altq_disc) == NULL)
		return (EINVAL);
	a->altq_disc = NULL;

d192 14
a212 83
int
hfsc_add_queue(struct pf_altq *a)
{
	struct hfsc_if *hif;
	struct hfsc_class *cl, *parent;
	struct hfsc_opts *opts;
	struct service_curve rtsc, lssc, ulsc;

	if ((hif = a->altq_disc) == NULL)
		return (EINVAL);

	opts = &a->pq_u.hfsc_opts;

	parent = clh_to_clp(hif, a->parent_qid);
	if (parent == NULL)
		return (EINVAL);
	if (a->qid != 0) {
		if (a->qid >= HFSC_MAX_CLASSES)
			return (EINVAL);
		if (clh_to_clp(hif, a->qid) != NULL)
			return (EBUSY);
	}
	rtsc.m1 = opts->rtsc_m1;
	rtsc.d  = opts->rtsc_d;
	rtsc.m2 = opts->rtsc_m2;
	lssc.m1 = opts->lssc_m1;
	lssc.d  = opts->lssc_d;
	lssc.m2 = opts->lssc_m2;
	ulsc.m1 = opts->ulsc_m1;
	ulsc.d  = opts->ulsc_d;
	ulsc.m2 = opts->ulsc_m2;

	cl = hfsc_class_create(hif, &rtsc, &lssc, &ulsc,
	    parent, a->qlimit, opts->flags, a->qid);
	if (cl == NULL)
		return (ENOMEM);

	/* return handle to user space. */
	a->qid = cl->cl_handle;

	return (0);
}

int
hfsc_remove_queue(struct pf_altq *a)
{
	struct hfsc_if *hif;
	struct hfsc_class *cl;

	if ((hif = a->altq_disc) == NULL)
		return (EINVAL);

	if ((cl = clh_to_clp(hif, a->qid)) == NULL)
		return (EINVAL);

	return (hfsc_class_destroy(cl));
}

int
hfsc_getqstats(struct pf_altq *a, void *ubuf, int *nbytes)
{
	struct hfsc_if *hif;
	struct hfsc_class *cl;
	struct hfsc_classstats stats;
	int error = 0;

	if ((hif = altq_lookup(a->ifname, ALTQT_HFSC)) == NULL)
		return (EBADF);

	if ((cl = clh_to_clp(hif, a->qid)) == NULL)
		return (EINVAL);

	if (*nbytes < sizeof(stats))
		return (EINVAL);

	get_class_stats(&stats, cl);

	if ((error = copyout((caddr_t)&stats, ubuf, sizeof(stats))) != 0)
		return (error);
	*nbytes = sizeof(stats);
	return (0);
}

d223 3
d239 1
a239 1

d274 1
a274 1
hfsc_class_create(hif, rsc, fsc, usc, parent, qlimit, flags, qid)
d276 1
a276 1
	struct service_curve *rsc, *fsc, *usc;
d278 1
a278 1
	int qlimit, flags, qid;
d281 1
a281 1
	int i, s, chandle;
a284 1
#ifdef ALTQ_DEBUG
a285 1
#endif
a289 12
	if (qid)
		chandle = qid;
	else {
		/* find a free class slot. */
		for (i = 0; i < HFSC_MAX_CLASSES; i++)
			if (hif->hif_class_tbl[i] == NULL)
				break;
		if (i == HFSC_MAX_CLASSES)
			return (NULL);
		chandle = i + 1;
	}

a314 9
		u_int m2;

		m2 = 0;
		if (rsc != NULL && rsc->m2 > m2)
			m2 = rsc->m2;
		if (fsc != NULL && fsc->m2 > m2)
			m2 = fsc->m2;
		if (usc != NULL && usc->m2 > m2)
			m2 = usc->m2;
d323 1
a323 1
		if (m2 < 8)
d327 1
a327 1
				* 1000 * 1000 * 1000 / (m2 / 8);
d329 2
a330 4
			cl->cl_red = red_alloc(0, 0,
			    qlimit(cl->cl_q) * 10/100,
			    qlimit(cl->cl_q) * 30/100,
			    red_flags, red_pkttime);
d337 1
a337 1
			    red_flags, red_pkttime);
d345 1
a345 1
	if (rsc != NULL && (rsc->m1 != 0 || rsc->m2 != 0)) {
d347 1
a347 1
		    sizeof(struct internal_sc), M_DEVBUF, M_WAITOK);
d350 2
a351 1
		sc2isc(rsc, cl->cl_rsc);
d354 1
a354 2
	}
	if (fsc != NULL && (fsc->m1 != 0 || fsc->m2 != 0)) {
d356 1
a356 1
		    sizeof(struct internal_sc), M_DEVBUF, M_WAITOK);
d359 2
a360 1
		sc2isc(fsc, cl->cl_fsc);
a362 8
	if (usc != NULL && (usc->m1 != 0 || usc->m2 != 0)) {
		MALLOC(cl->cl_usc, struct internal_sc *,
		    sizeof(struct internal_sc), M_DEVBUF, M_WAITOK);
		if (cl->cl_usc == NULL)
			goto err_ret;
		sc2isc(usc, cl->cl_usc);
		rtsc_init(&cl->cl_ulimit, cl->cl_usc, 0, 0);
	}
d365 1
a365 1
	cl->cl_handle = chandle;
a370 1
	hif->hif_class_tbl[chandle - 1] = cl;
a405 2
	if (cl->cl_usc != NULL)
		FREE(cl->cl_usc, M_DEVBUF);
d423 3
a443 1
	cl->cl_hif->hif_class_tbl[cl->cl_handle - 1] = NULL;
a458 2
	if (cl->cl_usc != NULL)
		FREE(cl->cl_usc, M_DEVBUF);
d469 63
d535 1
a535 1
 *	for (cl = hif->hif_rootclass; cl != NULL; cl = hfsc_nextclass(cl))
d561 1
a561 1
static int
a568 1
	struct m_tag *t;
d572 1
a572 11
	if ((m->m_flags & M_PKTHDR) == 0) {
		/* should not happen */
		printf("altq: packet for %s does not have pkthdr\n",
		    ifq->altq_ifp->if_xname);
		m_freem(m);
		return (ENOBUFS);
	}
	t = m_tag_find(m, PACKET_TAG_PF_QID, NULL);
	if (t == NULL ||
	    (cl = clh_to_clp(hif, ((struct altq_tag *)(t+1))->qid)) == NULL ||
		is_a_parent_class(cl)) {
d574 1
a574 6
		if (cl == NULL) {
			m_freem(m);
			return (ENOBUFS);
		}
		cl->cl_pktattr = NULL;
	}
d589 3
a613 1
	u_int64_t cur_time;
a618 2
	cur_time = read_machclk();

d620 2
a621 1

d625 2
a626 1
		if (cl->cl_rsc != NULL)
d628 1
d635 1
a635 2
		if ((cl = ellist_get_mindl(hif->hif_eligible, cur_time))
		    != NULL) {
a637 3
#ifdef ALTQ_DEBUG
			int fits = 0;
#endif
d644 2
a645 7

				cl = actlist_firstfit(cl, cur_time);
				if (cl == NULL) {
#ifdef ALTQ_DEBUG
					if (fits > 0)
						printf("%d fit but none found\n",fits);
#endif
a646 10
				}
				/*
				 * update parent's cl_cvtmin.
				 * don't update if the new vt is smaller.
				 */
				if (cl->cl_parent->cl_cvtmin < cl->cl_vt)
					cl->cl_parent->cl_cvtmin = cl->cl_vt;
#ifdef ALTQ_DEBUG
				fits++;
#endif
a657 2
	if (m == NULL)
		panic("hfsc_dequeue:");
d663 1
a663 1
	update_vf(cl, len, cur_time);
d671 1
a671 1

d682 4
a750 2
		cl->cl_hif->hif_packets--;
		IFQ_DEC_LEN(cl->cl_hif->hif_ifq);
d753 1
a753 2

	update_vf(cl, 0, 0);	/* remove cl from the actlist */
d757 1
a757 1
static void
d765 1
a765 1
		init_vf(cl, len);
d770 1
a770 1
static void
d777 13
a789 4
	/*
	 * actlist is now handled in update_vf() so that update_vf(cl, 0, 0)
	 * needs to be called explicitly to remove a class from actlist
	 */
d792 1
a792 1
static void
d822 1
a822 1
static void
d833 1
a833 1
static void
d841 2
a842 2
static void
init_vf(cl, len)
d846 1
a846 7
	struct hfsc_class *max_cl, *p;
	u_int64_t vt, f, cur_time;
	int go_active;

	cur_time = 0;
	go_active = 1;
	for ( ; cl->cl_parent != NULL; cl = cl->cl_parent) {
d848 9
a856 4
		if (go_active && cl->cl_nactive++ == 0)
			go_active = 1;
		else
			go_active = 0;
d858 5
a862 1
		if (go_active) {
d864 12
a875 29
			if (max_cl != NULL) {
				/*
				 * set vt to the average of the min and max
				 * classes.  if the parent's period didn't
				 * change, don't decrease vt of the class.
				 */
				vt = max_cl->cl_vt;
				if (cl->cl_parent->cl_cvtmin != 0)
					vt = (cl->cl_parent->cl_cvtmin + vt)/2;

				if (cl->cl_parent->cl_vtperiod !=
				    cl->cl_parentperiod || vt > cl->cl_vt)
					cl->cl_vt = vt;
			} else {
				/*
				 * first child for a new parent backlog period.
				 * add parent's cvtmax to vtoff of children
				 * to make a new vt (vtoff + vt) larger than
				 * the vt in the last period for all children.
				 */
				vt = cl->cl_parent->cl_cvtmax;
				for (p = cl->cl_parent->cl_children; p != NULL;
				     p = p->cl_siblings)
					p->cl_vtoff += vt;
				cl->cl_vt = 0;
				cl->cl_parent->cl_cvtmax = 0;
				cl->cl_parent->cl_cvtmin = 0;
			}
			cl->cl_initvt = cl->cl_vt;
d877 4
a880 8
			/* update the virtual curve */
			vt = cl->cl_vt + cl->cl_vtoff;
			rtsc_min(&cl->cl_virtual, cl->cl_fsc, vt, cl->cl_total);
			if (cl->cl_virtual.x == vt) {
				cl->cl_virtual.x -= cl->cl_vtoff;
				cl->cl_vtoff = 0;
			}
			cl->cl_vtadj = 0;
d882 1
a882 22
			cl->cl_vtperiod++;  /* increment vt period */
			cl->cl_parentperiod = cl->cl_parent->cl_vtperiod;
			if (cl->cl_parent->cl_nactive == 0)
				cl->cl_parentperiod++;
			cl->cl_f = 0;

			actlist_insert(cl);

			if (cl->cl_usc != NULL) {
				/* class has upper limit curve */
				if (cur_time == 0)
					cur_time = read_machclk();

				/* update the ulimit curve */
				rtsc_min(&cl->cl_ulimit, cl->cl_usc, cur_time,
				    cl->cl_total);
				/* compute myf */
				cl->cl_myf = rtsc_y2x(&cl->cl_ulimit,
				    cl->cl_total);
				cl->cl_myfadj = 0;
			}
		}
d884 2
a885 8
		if (cl->cl_myf > cl->cl_cfmin)
			f = cl->cl_myf;
		else
			f = cl->cl_cfmin;
		if (f != cl->cl_f) {
			cl->cl_f = f;
			update_cfmin(cl->cl_parent);
		}
d889 2
a890 2
static void
update_vf(cl, len, cur_time)
a892 1
	u_int64_t cur_time;
d894 1
a894 6
	u_int64_t f, myf_bound, delta;
	int go_passive;

	go_passive = qempty(cl->cl_q);

	for (; cl->cl_parent != NULL; cl = cl->cl_parent) {
d898 2
a899 60
		if (cl->cl_fsc == NULL || cl->cl_nactive == 0)
			continue;

		if (go_passive && --cl->cl_nactive == 0)
			go_passive = 1;
		else
			go_passive = 0;

		if (go_passive) {
			/* no more active child, going passive */

			/* update cvtmax of the parent class */
			if (cl->cl_vt > cl->cl_parent->cl_cvtmax)
				cl->cl_parent->cl_cvtmax = cl->cl_vt;

			/* remove this class from the vt list */
			actlist_remove(cl);

			update_cfmin(cl->cl_parent);

			continue;
		}

		/*
		 * update vt and f
		 */
		cl->cl_vt = rtsc_y2x(&cl->cl_virtual, cl->cl_total)
		    - cl->cl_vtoff + cl->cl_vtadj;

		/*
		 * if vt of the class is smaller than cvtmin,
		 * the class was skipped in the past due to non-fit.
		 * if so, we need to adjust vtadj.
		 */
		if (cl->cl_vt < cl->cl_parent->cl_cvtmin) {
			cl->cl_vtadj += cl->cl_parent->cl_cvtmin - cl->cl_vt;
			cl->cl_vt = cl->cl_parent->cl_cvtmin;
		}

		/* update the vt list */
		actlist_update(cl);

		if (cl->cl_usc != NULL) {
			cl->cl_myf = cl->cl_myfadj
			    + rtsc_y2x(&cl->cl_ulimit, cl->cl_total);

			/*
			 * if myf lags behind by more than one clock tick
			 * from the current time, adjust myfadj to prevent
			 * a rate-limited class from going greedy.
			 * in a steady state under rate-limiting, myf
			 * fluctuates within one clock tick.
			 */
			myf_bound = cur_time - machclk_per_tick;
			if (cl->cl_myf < myf_bound) {
				delta = cur_time - cl->cl_myf;
				cl->cl_myfadj += delta;
				cl->cl_myf += delta;
			}
		}
d901 2
a902 8
		/* cl_f is max(cl_myf, cl_cfmin) */
		if (cl->cl_myf > cl->cl_cfmin)
			f = cl->cl_myf;
		else
			f = cl->cl_cfmin;
		if (f != cl->cl_f) {
			cl->cl_f = f;
			update_cfmin(cl->cl_parent);
a903 2
	}
}
d905 2
a906 19
static void
update_cfmin(cl)
	struct hfsc_class *cl;
{
	struct hfsc_class *p;
	u_int64_t cfmin;

	if (TAILQ_EMPTY(cl->cl_actc)) {
		cl->cl_cfmin = 0;
		return;
	}
	cfmin = HT_INFINITY;
	TAILQ_FOREACH(p, cl->cl_actc, cl_actlist) {
		if (p->cl_f == 0) {
			cl->cl_cfmin = 0;
			return;
		}
		if (p->cl_f < cfmin)
			cfmin = p->cl_f;
a907 1
	cl->cl_cfmin = cfmin;
d923 1
a923 1

d936 1
a936 1
static void
d959 1
a959 1
static void
d964 1
a964 1

d968 1
a968 1
static void
d1008 1
a1008 1
ellist_get_mindl(head, cur_time)
a1009 1
	u_int64_t cur_time;
d1012 3
d1034 1
a1034 1

d1046 1
a1046 1
static void
d1068 1
a1068 1
static void
d1087 1
a1087 1
	if (p == NULL || cl->cl_vt < p->cl_vt)
a1112 14
static struct hfsc_class *
actlist_firstfit(cl, cur_time)
	struct hfsc_class *cl;
	u_int64_t cur_time;
{
	struct hfsc_class *p;

	TAILQ_FOREACH(p, cl->cl_actc, cl_actlist) {
		if (p->cl_f <= cur_time)
			return (p);
	}
	return (NULL);
}

d1129 1
a1129 1
 *  bits/sec    100Kbps     1Mbps     10Mbps     100Mbps    1Gbps
d1134 1
a1134 1
 *
d1142 2
a1143 2
#define	SM_MASK		((1LL << SM_SHIFT) - 1)
#define	ISM_MASK	((1LL << ISM_SHIFT) - 1)
d1145 1
a1145 1
static __inline u_int64_t
d1152 4
a1155 6
	/*
	 * compute
	 *	y = x * sm >> SM_SHIFT
	 * but divide it for the upper and lower bits to avoid overflow
	 */
	y = (x >> SM_SHIFT) * sm + (((x & SM_MASK) * sm) >> SM_SHIFT);
d1159 1
a1159 1
static __inline u_int64_t
d1168 6
a1173 6
	else if (ism == HT_INFINITY)
		x = HT_INFINITY;
	else {
		x = (y >> ISM_SHIFT) * ism
		    + (((y & ISM_MASK) * ism) >> ISM_SHIFT);
	}
d1177 1
a1177 1
static __inline u_int64_t
d1187 1
a1187 1
static __inline u_int64_t
d1194 1
a1194 1
		ism = HT_INFINITY;
d1200 1
a1200 1
static __inline u_int64_t
d1205 1
a1205 1

d1210 1
a1210 1
static u_int
d1220 1
a1220 1
static u_int
d1230 1
a1230 1
static void
d1247 1
a1247 1
static void
d1267 1
a1267 1
static u_int64_t
d1290 1
a1290 1
static u_int64_t
d1313 1
a1313 1
static void
d1364 1
a1364 1
	 */
d1376 335
a1710 3
static void
get_class_stats(sp, cl)
	struct hfsc_classstats *sp;
d1714 1
a1714 1
	sp->class_handle = cl->cl_handle;
a1733 9
	if (cl->cl_usc != NULL) {
		sp->usc.m1 = sm2m(cl->cl_usc->sm1);
		sp->usc.d = dx2d(cl->cl_usc->dx);
		sp->usc.m2 = sm2m(cl->cl_usc->sm2);
	} else {
		sp->usc.m1 = 0;
		sp->usc.d = 0;
		sp->usc.m2 = 0;
	}
a1740 16
	sp->f = cl->cl_f;

	sp->initvt = cl->cl_initvt;
	sp->vtperiod = cl->cl_vtperiod;
	sp->parentperiod = cl->cl_parentperiod;
	sp->nactive = cl->cl_nactive;
	sp->vtoff = cl->cl_vtoff;
	sp->cvtmax = cl->cl_cvtmax;
	sp->myf = cl->cl_myf;
	sp->cfmin = cl->cl_cfmin;
	sp->cvtmin = cl->cl_cvtmin;
	sp->myfadj = cl->cl_myfadj;
	sp->vtadj = cl->cl_vtadj;

	sp->cur_time = read_machclk();
	sp->machclk_freq = machclk_freq;
a1742 1
	sp->qlimit = qlimit(cl->cl_q);
d1762 1
a1762 1
	u_int32_t chandle;
d1764 1
a1764 1
	int idx;
d1766 5
a1770 1
	if (chandle == 0)
d1772 3
a1774 2
	idx = chandle - 1;
	if (idx >= HFSC_MAX_CLASSES)
d1776 2
a1777 1
	return (hif->hif_class_tbl[idx]);
d1779 19
@


1.1.2.4
log
@Sync the SMP branch to -current. This includes moving to ELF.
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.1.2.3 2003/03/27 22:28:25 niklas Exp $	*/
d66 4
a69 4
static int			 hfsc_clear_interface(struct hfsc_if *);
static int			 hfsc_request(struct ifaltq *, int, void *);
static void			 hfsc_purge(struct hfsc_if *);
static struct hfsc_class	*hfsc_class_create(struct hfsc_if *,
d72 50
a121 54
static int			 hfsc_class_destroy(struct hfsc_class *);
static struct hfsc_class	*hfsc_nextclass(struct hfsc_class *);
static int			 hfsc_enqueue(struct ifaltq *, struct mbuf *,
				    struct altq_pktattr *);
static struct mbuf		*hfsc_dequeue(struct ifaltq *, int);

static int		 hfsc_addq(struct hfsc_class *, struct mbuf *);
static struct mbuf	*hfsc_getq(struct hfsc_class *);
static struct mbuf	*hfsc_pollq(struct hfsc_class *);
static void		 hfsc_purgeq(struct hfsc_class *);

static void		 update_cfmin(struct hfsc_class *);
static void		 set_active(struct hfsc_class *, int);
static void		 set_passive(struct hfsc_class *);

static void		 init_ed(struct hfsc_class *, int);
static void		 update_ed(struct hfsc_class *, int);
static void		 update_d(struct hfsc_class *, int);
static void		 init_vf(struct hfsc_class *, int);
static void		 update_vf(struct hfsc_class *, int, u_int64_t);
static ellist_t		*ellist_alloc(void);
static void		 ellist_destroy(ellist_t *);
static void		 ellist_insert(struct hfsc_class *);
static void		 ellist_remove(struct hfsc_class *);
static void		 ellist_update(struct hfsc_class *);
struct hfsc_class	*ellist_get_mindl(ellist_t *, u_int64_t);
static actlist_t	*actlist_alloc(void);
static void		 actlist_destroy(actlist_t *);
static void		 actlist_insert(struct hfsc_class *);
static void		 actlist_remove(struct hfsc_class *);
static void		 actlist_update(struct hfsc_class *);

static struct hfsc_class	*actlist_firstfit(struct hfsc_class *,
				    u_int64_t);

static __inline u_int64_t	seg_x2y(u_int64_t, u_int64_t);
static __inline u_int64_t	seg_y2x(u_int64_t, u_int64_t);
static __inline u_int64_t	m2sm(u_int);
static __inline u_int64_t	m2ism(u_int);
static __inline u_int64_t	d2dx(u_int);
static u_int			sm2m(u_int64_t);
static u_int			dx2d(u_int64_t);

static void		sc2isc(struct service_curve *, struct internal_sc *);
static void		rtsc_init(struct runtime_sc *, struct internal_sc *,
			    u_int64_t, u_int64_t);
static u_int64_t	rtsc_y2x(struct runtime_sc *, u_int64_t);
static u_int64_t	rtsc_x2y(struct runtime_sc *, u_int64_t);
static void		rtsc_min(struct runtime_sc *, struct internal_sc *,
			    u_int64_t, u_int64_t);

static void			 get_class_stats(struct hfsc_classstats *,
				    struct hfsc_class *);
static struct hfsc_class	*clh_to_clp(struct hfsc_if *, u_int32_t);
d150 1
d171 13
d222 5
a226 3
	parent = NULL;
	if (a->qid != HFSC_ROOTCLASS_HANDLE)
		if ((parent = clh_to_clp(hif, a->parent_qid)) == NULL)
d228 3
a230 6

	if (a->qid >= HFSC_MAX_CLASSES || a->qid == 0)
		return (EINVAL);
	if (clh_to_clp(hif, a->qid) != NULL)
		return (EBUSY);

d246 3
d297 2
a298 1
hfsc_clear_interface(struct hfsc_if *hif)
d303 1
a303 2
	while (hif->hif_rootclass != NULL &&
	    (cl = hif->hif_rootclass->cl_children) != NULL) {
d320 4
a323 1
hfsc_request(struct ifaltq *ifq, int req, void *arg)
d337 2
a338 1
hfsc_purge(struct hfsc_if *hif)
d350 5
a354 3
hfsc_class_create(struct hfsc_if *hif, struct service_curve *rsc,
    struct service_curve *fsc, struct service_curve *usc,
    struct hfsc_class *parent, int qlimit, int flags, int qid)
d357 1
a357 1
	int s;
d368 12
d473 1
a473 1
	cl->cl_handle = qid;
d479 1
a479 1
	hif->hif_class_tbl[qid - 1] = cl;
d524 2
a525 1
hfsc_class_destroy(struct hfsc_class *cl)
a528 3
	if (cl == NULL)
		return (0);

d587 2
a588 1
hfsc_nextclass(struct hfsc_class *cl)
d610 4
a613 1
hfsc_enqueue(struct ifaltq *ifq, struct mbuf *m, struct altq_pktattr *pktattr)
d666 3
a668 1
hfsc_dequeue(struct ifaltq *ifq, int op)
d768 3
a770 1
hfsc_addq(struct hfsc_class *cl, struct mbuf *m)
d796 2
a797 1
hfsc_getq(struct hfsc_class *cl)
d811 2
a812 1
hfsc_pollq(struct hfsc_class *cl)
d818 2
a819 1
hfsc_purgeq(struct hfsc_class *cl)
d839 3
a841 1
set_active(struct hfsc_class *cl, int len)
d852 2
a853 1
set_passive(struct hfsc_class *cl)
d865 3
a867 1
init_ed(struct hfsc_class *cl, int next_len)
d895 3
a897 1
update_ed(struct hfsc_class *cl, int next_len)
d906 3
a908 1
update_d(struct hfsc_class *cl, int next_len)
d914 3
a916 1
init_vf(struct hfsc_class *cl, int len)
d1007 4
a1010 1
update_vf(struct hfsc_class *cl, int len, u_int64_t cur_time)
d1095 2
a1096 1
update_cfmin(struct hfsc_class *cl)
d1127 1
a1127 1
ellist_alloc(void)
d1137 2
a1138 1
ellist_destroy(ellist_t *head)
d1144 2
a1145 1
ellist_insert(struct hfsc_class *cl)
d1167 2
a1168 1
ellist_remove(struct hfsc_class *cl)
d1176 2
a1177 1
ellist_update(struct hfsc_class *cl)
d1215 3
a1217 1
ellist_get_mindl(ellist_t *head, u_int64_t cur_time)
d1236 1
a1236 1
actlist_alloc(void)
d1246 2
a1247 1
actlist_destroy(actlist_t *head)
d1252 2
a1253 1
actlist_insert(struct hfsc_class *cl)
d1274 2
a1275 1
actlist_remove(struct hfsc_class *cl)
d1281 2
a1282 1
actlist_update(struct hfsc_class *cl)
d1319 3
a1321 1
actlist_firstfit(struct hfsc_class *cl, u_int64_t cur_time)
d1365 3
a1367 1
seg_x2y(u_int64_t x, u_int64_t sm)
d1381 3
a1383 1
seg_y2x(u_int64_t y, u_int64_t ism)
d1399 2
a1400 1
m2sm(u_int m)
d1409 2
a1410 1
m2ism(u_int m)
d1422 2
a1423 1
d2dx(u_int d)
d1432 2
a1433 1
sm2m(u_int64_t sm)
d1442 2
a1443 1
dx2d(u_int64_t dx)
d1452 3
a1454 1
sc2isc(struct service_curve *sc, struct internal_sc *isc)
d1469 4
a1472 2
rtsc_init(struct runtime_sc *rtsc, struct internal_sc * isc, u_int64_t x,
    u_int64_t y)
d1489 3
a1491 1
rtsc_y2x(struct runtime_sc *rtsc, u_int64_t y)
d1512 3
a1514 1
rtsc_x2y(struct runtime_sc *rtsc, u_int64_t x)
d1535 4
a1538 2
rtsc_min(struct runtime_sc *rtsc, struct internal_sc *isc, u_int64_t x,
    u_int64_t y)
d1598 3
a1600 1
get_class_stats(struct hfsc_classstats *sp, struct hfsc_class *cl)
d1675 3
a1677 1
clh_to_clp(struct hfsc_if *hif, u_int32_t chandle)
d1679 1
a1679 1
	u_int idx;
@


1.1.2.5
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: altq_hfsc.c,v 1.1.2.4 2003/05/13 19:21:26 ho Exp $	*/
d453 1
d456 7
a462 10
		hif->hif_rootclass = cl;
	} else {
		/* add this class to the children list of the parent */
		if ((p = parent->cl_children) == NULL)
			parent->cl_children = cl;
		else {
			while (p->cl_siblings != NULL)
				p = p->cl_siblings;
			p->cl_siblings = cl;
		}
@


1.1.2.6
log
@Merge of current from two weeks agointo the SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d11 4
a14 1
 * works, or modified versions, and any portions thereof.
d212 4
a215 5
	if (a->parent_qid == HFSC_NULLCLASS_HANDLE &&
	    hif->hif_rootclass == NULL)
		parent = NULL;
	else if ((parent = clh_to_clp(hif, a->parent_qid)) == NULL)
		return (EINVAL);
d217 1
a217 1
	if (a->qid == 0)
a218 1

d339 1
a339 4
	int i, s;

	if (hif->hif_classes >= HFSC_MAX_CLASSES)
		return (NULL);
d449 1
a449 21

	/*
	 * find a free slot in the class table.  if the slot matching
	 * the lower bits of qid is free, use this slot.  otherwise,
	 * use the first free slot.
	 */
	i = qid % HFSC_MAX_CLASSES;
	if (hif->hif_class_tbl[i] == NULL)
		hif->hif_class_tbl[i] = cl;
	else {
		for (i = 0; i < HFSC_MAX_CLASSES; i++)
			if (hif->hif_class_tbl[i] == NULL) {
				hif->hif_class_tbl[i] = cl;
				break;
			}
		if (i == HFSC_MAX_CLASSES) {
			splx(s);
			goto err_ret;
		}
	}

d498 1
a498 1
	int i, s;
d526 1
a526 7

	for (i = 0; i < HFSC_MAX_CLASSES; i++)
		if (cl->cl_hif->hif_class_tbl[i] == cl) {
			cl->cl_hif->hif_class_tbl[i] = NULL;
			break;
		}

a541 6

	if (cl == cl->cl_hif->hif_rootclass)
		cl->cl_hif->hif_rootclass = NULL;
	if (cl == cl->cl_hif->hif_defaultclass)
		cl->cl_hif->hif_defaultclass = NULL;

d1592 1
a1592 2
	int i;
	struct hfsc_class *cl;
d1596 4
a1599 12
	/*
	 * first, try the slot corresponding to the lower bits of the handle.
	 * if it does not match, do the linear table search.
	 */
	i = chandle % HFSC_MAX_CLASSES;
	if ((cl = hif->hif_class_tbl[i]) != NULL && cl->cl_handle == chandle)
		return (cl);
	for (i = 0; i < HFSC_MAX_CLASSES; i++)
		if ((cl = hif->hif_class_tbl[i]) != NULL &&
		    cl->cl_handle == chandle)
			return (cl);
	return (NULL);
@


