head	1.82;
access;
symbols
	OPENBSD_6_2:1.82.0.2
	OPENBSD_6_2_BASE:1.82
	OPENBSD_6_1:1.82.0.4
	OPENBSD_6_1_BASE:1.82
	OPENBSD_6_0:1.81.0.4
	OPENBSD_6_0_BASE:1.81
	OPENBSD_5_9:1.81.0.2
	OPENBSD_5_9_BASE:1.81
	OPENBSD_5_8:1.80.0.4
	OPENBSD_5_8_BASE:1.80
	OPENBSD_5_7:1.79.0.2
	OPENBSD_5_7_BASE:1.79
	OPENBSD_5_6:1.76.0.4
	OPENBSD_5_6_BASE:1.76
	OPENBSD_5_5:1.75.0.4
	OPENBSD_5_5_BASE:1.75
	OPENBSD_5_4:1.74.0.2
	OPENBSD_5_4_BASE:1.74
	OPENBSD_5_3:1.73.0.4
	OPENBSD_5_3_BASE:1.73
	OPENBSD_5_2:1.73.0.2
	OPENBSD_5_2_BASE:1.73
	OPENBSD_5_1_BASE:1.72
	OPENBSD_5_1:1.72.0.8
	OPENBSD_5_0:1.72.0.6
	OPENBSD_5_0_BASE:1.72
	OPENBSD_4_9:1.72.0.4
	OPENBSD_4_9_BASE:1.72
	OPENBSD_4_8:1.72.0.2
	OPENBSD_4_8_BASE:1.72
	OPENBSD_4_7:1.68.0.2
	OPENBSD_4_7_BASE:1.68
	OPENBSD_4_6:1.59.0.4
	OPENBSD_4_6_BASE:1.59
	OPENBSD_4_5:1.57.0.2
	OPENBSD_4_5_BASE:1.57
	OPENBSD_4_4:1.53.0.2
	OPENBSD_4_4_BASE:1.53
	OPENBSD_4_3:1.46.0.4
	OPENBSD_4_3_BASE:1.46
	OPENBSD_4_2:1.46.0.2
	OPENBSD_4_2_BASE:1.46
	OPENBSD_4_1:1.44.0.2
	OPENBSD_4_1_BASE:1.44
	OPENBSD_4_0:1.42.0.2
	OPENBSD_4_0_BASE:1.42
	OPENBSD_3_9:1.41.0.2
	OPENBSD_3_9_BASE:1.41
	OPENBSD_3_8:1.40.0.6
	OPENBSD_3_8_BASE:1.40
	OPENBSD_3_7:1.40.0.4
	OPENBSD_3_7_BASE:1.40
	OPENBSD_3_6:1.40.0.2
	OPENBSD_3_6_BASE:1.40
	SMP_SYNC_A:1.38
	SMP_SYNC_B:1.38
	OPENBSD_3_5:1.38.0.4
	OPENBSD_3_5_BASE:1.38
	OPENBSD_3_4:1.38.0.2
	OPENBSD_3_4_BASE:1.38
	UBC_SYNC_A:1.37
	OPENBSD_3_3:1.36.0.4
	OPENBSD_3_3_BASE:1.36
	OPENBSD_3_2:1.36.0.2
	OPENBSD_3_2_BASE:1.36
	OPENBSD_3_1:1.35.0.2
	OPENBSD_3_1_BASE:1.35
	UBC_SYNC_B:1.36
	UBC:1.32.0.2
	UBC_BASE:1.32
	OPENBSD_3_0:1.22.0.2
	OPENBSD_3_0_BASE:1.22
	OPENBSD_2_9_BASE:1.18
	OPENBSD_2_9:1.18.0.2
	OPENBSD_2_8:1.16.0.2
	OPENBSD_2_8_BASE:1.16
	OPENBSD_2_7:1.15.0.8
	OPENBSD_2_7_BASE:1.15
	SMP:1.15.0.6
	SMP_BASE:1.15
	kame_19991208:1.15
	OPENBSD_2_6:1.15.0.4
	OPENBSD_2_6_BASE:1.15
	OPENBSD_2_5:1.15.0.2
	OPENBSD_2_5_BASE:1.15
	OPENBSD_2_4:1.14.0.4
	OPENBSD_2_4_BASE:1.14
	OPENBSD_2_3:1.14.0.2
	OPENBSD_2_3_BASE:1.14
	OPENBSD_2_2:1.12.0.2
	OPENBSD_2_2_BASE:1.12
	OPENBSD_2_1:1.10.0.4
	OPENBSD_2_1_BASE:1.10
	OPENBSD_2_0:1.10.0.2
	OPENBSD_2_0_BASE:1.10
	netbsd_1_1:1.1.1.1;
locks; strict;
comment	@ * @;


1.82
date	2017.02.22.11.42.46;	author mpi;	state Exp;
branches;
next	1.81;
commitid	QPdM5oUknMlUIDue;

1.81
date	2016.02.13.15.45.05;	author stefan;	state Exp;
branches;
next	1.80;
commitid	TfAXGeazXTFF4kij;

1.80
date	2015.03.14.03.38.52;	author jsg;	state Exp;
branches;
next	1.79;
commitid	p4LJxGKbi0BU2cG6;

1.79
date	2015.02.10.21.56.10;	author miod;	state Exp;
branches;
next	1.78;
commitid	C5iGb36LQxjM60Q3;

1.78
date	2014.12.18.20.59.21;	author tedu;	state Exp;
branches;
next	1.77;
commitid	A8MlA6JGDExx7uTs;

1.77
date	2014.11.14.23.01.44;	author tedu;	state Exp;
branches;
next	1.76;
commitid	IWzLFxf7O2DJPe2b;

1.76
date	2014.07.08.17.19.26;	author deraadt;	state Exp;
branches;
next	1.75;
commitid	EF98ch02VpFassUi;

1.75
date	2013.09.14.02.28.03;	author guenther;	state Exp;
branches;
next	1.74;

1.74
date	2013.06.11.16.42.17;	author deraadt;	state Exp;
branches;
next	1.73;

1.73
date	2012.07.11.12.39.20;	author guenther;	state Exp;
branches;
next	1.72;

1.72
date	2010.08.07.03.50.02;	author krw;	state Exp;
branches;
next	1.71;

1.71
date	2010.04.12.16.37.38;	author beck;	state Exp;
branches;
next	1.70;

1.70
date	2010.04.09.22.42.10;	author oga;	state Exp;
branches;
next	1.69;

1.69
date	2010.04.09.22.08.04;	author oga;	state Exp;
branches;
next	1.68;

1.68
date	2009.10.19.22.24.18;	author jsg;	state Exp;
branches;
next	1.67;

1.67
date	2009.09.02.18.20.54;	author thib;	state Exp;
branches;
next	1.66;

1.66
date	2009.08.27.23.39.46;	author thib;	state Exp;
branches;
next	1.65;

1.65
date	2009.08.27.23.26.56;	author thib;	state Exp;
branches;
next	1.64;

1.64
date	2009.08.26.12.08.10;	author thib;	state Exp;
branches;
next	1.63;

1.63
date	2009.08.20.15.04.24;	author thib;	state Exp;
branches;
next	1.62;

1.62
date	2009.07.28.11.19.43;	author art;	state Exp;
branches;
next	1.61;

1.61
date	2009.07.22.13.02.08;	author thib;	state Exp;
branches;
next	1.60;

1.60
date	2009.07.20.16.49.40;	author thib;	state Exp;
branches;
next	1.59;

1.59
date	2009.06.23.08.08.50;	author jasper;	state Exp;
branches;
next	1.58;

1.58
date	2009.03.19.16.44.40;	author oga;	state Exp;
branches;
next	1.57;

1.57
date	2009.01.24.23.30.42;	author thib;	state Exp;
branches;
next	1.56;

1.56
date	2009.01.19.23.40.36;	author thib;	state Exp;
branches;
next	1.55;

1.55
date	2008.08.09.10.14.02;	author thib;	state Exp;
branches;
next	1.54;

1.54
date	2008.08.08.20.44.38;	author blambert;	state Exp;
branches;
next	1.53;

1.53
date	2008.07.25.14.56.47;	author beck;	state Exp;
branches;
next	1.52;

1.52
date	2008.07.23.16.24.43;	author beck;	state Exp;
branches;
next	1.51;

1.51
date	2008.06.14.19.33.58;	author beck;	state Exp;
branches;
next	1.50;

1.50
date	2008.06.12.19.14.15;	author thib;	state Exp;
branches;
next	1.49;

1.49
date	2008.06.12.16.04.37;	author art;	state Exp;
branches;
next	1.48;

1.48
date	2008.06.12.06.44.16;	author thib;	state Exp;
branches;
next	1.47;

1.47
date	2008.06.11.04.52.27;	author blambert;	state Exp;
branches;
next	1.46;

1.46
date	2007.06.01.23.47.57;	author deraadt;	state Exp;
branches;
next	1.45;

1.45
date	2007.06.01.07.13.47;	author thib;	state Exp;
branches;
next	1.44;

1.44
date	2006.11.29.12.24.18;	author miod;	state Exp;
branches;
next	1.43;

1.43
date	2006.11.01.00.12.35;	author thib;	state Exp;
branches;
next	1.42;

1.42
date	2006.04.20.14.43.32;	author pedro;	state Exp;
branches;
next	1.41;

1.41
date	2005.10.31.20.22.36;	author otto;	state Exp;
branches;
next	1.40;

1.40
date	2004.08.03.17.11.48;	author marius;	state Exp;
branches;
next	1.39;

1.39
date	2004.07.21.17.30.55;	author marius;	state Exp;
branches;
next	1.38;

1.38
date	2003.06.02.23.28.19;	author millert;	state Exp;
branches;
next	1.37;

1.37
date	2003.05.13.02.09.47;	author jason;	state Exp;
branches;
next	1.36;

1.36
date	2002.05.21.21.07.09;	author art;	state Exp;
branches;
next	1.35;

1.35
date	2002.02.08.08.20.49;	author csapuntz;	state Exp;
branches;
next	1.34;

1.34
date	2002.01.16.21.51.16;	author ericj;	state Exp;
branches;
next	1.33;

1.33
date	2001.12.19.08.58.06;	author art;	state Exp;
branches;
next	1.32;

1.32
date	2001.12.14.03.16.02;	author art;	state Exp;
branches
	1.32.2.1;
next	1.31;

1.31
date	2001.12.10.02.19.34;	author art;	state Exp;
branches;
next	1.30;

1.30
date	2001.11.30.18.58.18;	author art;	state Exp;
branches;
next	1.29;

1.29
date	2001.11.30.05.45.33;	author csapuntz;	state Exp;
branches;
next	1.28;

1.28
date	2001.11.29.12.24.28;	author art;	state Exp;
branches;
next	1.27;

1.27
date	2001.11.29.02.08.22;	author art;	state Exp;
branches;
next	1.26;

1.26
date	2001.11.29.01.59.19;	author art;	state Exp;
branches;
next	1.25;

1.25
date	2001.11.27.05.27.12;	author art;	state Exp;
branches;
next	1.24;

1.24
date	2001.11.15.23.15.15;	author art;	state Exp;
branches;
next	1.23;

1.23
date	2001.11.06.19.53.21;	author miod;	state Exp;
branches;
next	1.22;

1.22
date	2001.06.27.04.58.46;	author art;	state Exp;
branches;
next	1.21;

1.21
date	2001.06.25.05.27.54;	author csapuntz;	state Exp;
branches;
next	1.20;

1.20
date	2001.06.25.03.28.06;	author csapuntz;	state Exp;
branches;
next	1.19;

1.19
date	2001.06.25.02.15.46;	author csapuntz;	state Exp;
branches;
next	1.18;

1.18
date	2001.02.23.14.52.50;	author csapuntz;	state Exp;
branches;
next	1.17;

1.17
date	2001.02.23.14.42.38;	author csapuntz;	state Exp;
branches;
next	1.16;

1.16
date	2000.06.23.02.14.40;	author mickey;	state Exp;
branches;
next	1.15;

1.15
date	99.02.26.03.16.25;	author art;	state Exp;
branches
	1.15.6.1;
next	1.14;

1.14
date	97.12.02.16.57.57;	author csapuntz;	state Exp;
branches;
next	1.13;

1.13
date	97.11.06.05.59.00;	author csapuntz;	state Exp;
branches;
next	1.12;

1.12
date	97.10.06.20.20.44;	author deraadt;	state Exp;
branches;
next	1.11;

1.11
date	97.10.06.15.23.40;	author csapuntz;	state Exp;
branches;
next	1.10;

1.10
date	96.07.27.11.10.11;	author deraadt;	state Exp;
branches;
next	1.9;

1.9
date	96.07.21.08.05.37;	author tholo;	state Exp;
branches;
next	1.8;

1.8
date	96.06.14.04.41.07;	author tholo;	state Exp;
branches;
next	1.7;

1.7
date	96.05.28.13.44.07;	author deraadt;	state Exp;
branches;
next	1.6;

1.6
date	96.04.21.22.30.18;	author deraadt;	state Exp;
branches;
next	1.5;

1.5
date	96.04.17.04.50.24;	author mickey;	state Exp;
branches;
next	1.4;

1.4
date	96.03.31.13.15.32;	author mickey;	state Exp;
branches;
next	1.3;

1.3
date	96.02.29.09.24.48;	author niklas;	state Exp;
branches;
next	1.2;

1.2
date	96.01.08.07.36.23;	author dm;	state Exp;
branches;
next	1.1;

1.1
date	95.10.18.08.53.21;	author deraadt;	state Exp;
branches
	1.1.1.1;
next	;

1.1.1.1
date	95.10.18.08.53.21;	author deraadt;	state Exp;
branches;
next	;

1.15.6.1
date	2001.05.14.22.44.57;	author niklas;	state Exp;
branches;
next	1.15.6.2;

1.15.6.2
date	2001.07.04.10.55.45;	author niklas;	state Exp;
branches;
next	1.15.6.3;

1.15.6.3
date	2001.11.13.22.59.59;	author niklas;	state Exp;
branches;
next	1.15.6.4;

1.15.6.4
date	2001.12.05.01.02.40;	author niklas;	state Exp;
branches;
next	1.15.6.5;

1.15.6.5
date	2002.03.06.02.17.12;	author niklas;	state Exp;
branches;
next	1.15.6.6;

1.15.6.6
date	2003.03.28.00.08.46;	author niklas;	state Exp;
branches;
next	1.15.6.7;

1.15.6.7
date	2003.05.16.00.29.45;	author niklas;	state Exp;
branches;
next	1.15.6.8;

1.15.6.8
date	2003.06.07.11.07.43;	author ho;	state Exp;
branches;
next	;

1.32.2.1
date	2002.01.31.22.55.47;	author niklas;	state Exp;
branches;
next	1.32.2.2;

1.32.2.2
date	2002.02.02.03.28.26;	author art;	state Exp;
branches;
next	1.32.2.3;

1.32.2.3
date	2002.06.11.03.32.03;	author art;	state Exp;
branches;
next	1.32.2.4;

1.32.2.4
date	2002.11.04.18.02.31;	author art;	state Exp;
branches;
next	1.32.2.5;

1.32.2.5
date	2003.05.19.22.36.43;	author tedu;	state Exp;
branches;
next	;


desc
@@


1.82
log
@Keep local definitions local.

"good work" deraadt@@, ok visa@@
@
text
@/*	$OpenBSD: nfs_bio.c,v 1.81 2016/02/13 15:45:05 stefan Exp $	*/
/*	$NetBSD: nfs_bio.c,v 1.25.4.2 1996/07/08 20:47:04 jtc Exp $	*/

/*
 * Copyright (c) 1989, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * Rick Macklem at The University of Guelph.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@@(#)nfs_bio.c	8.9 (Berkeley) 3/30/95
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/resourcevar.h>
#include <sys/signalvar.h>
#include <sys/proc.h>
#include <sys/buf.h>
#include <sys/vnode.h>
#include <sys/mount.h>
#include <sys/kernel.h>
#include <sys/namei.h>
#include <sys/queue.h>
#include <sys/time.h>

#include <nfs/nfsproto.h>
#include <nfs/nfs.h>
#include <nfs/nfsmount.h>
#include <nfs/nfsnode.h>
#include <nfs/nfs_var.h>

extern int nfs_numasync;
extern struct nfsstats nfsstats;
struct nfs_bufqhead nfs_bufq;
uint32_t nfs_bufqmax, nfs_bufqlen;

struct buf *nfs_getcacheblk(struct vnode *, daddr_t, int, struct proc *);

/*
 * Vnode op for read using bio
 * Any similarity to readip() is purely coincidental
 */
int
nfs_bioread(struct vnode *vp, struct uio *uio, int ioflag, struct ucred *cred)
{
	struct nfsnode *np = VTONFS(vp);
	int biosize, diff;
	struct buf *bp = NULL, *rabp;
	struct vattr vattr;
	struct proc *p;
	struct nfsmount *nmp = VFSTONFS(vp->v_mount);
	daddr_t lbn, bn, rabn;
	caddr_t baddr;
	int got_buf = 0, nra, error = 0, n = 0, on = 0, not_readin;
	off_t offdiff;

#ifdef DIAGNOSTIC
	if (uio->uio_rw != UIO_READ)
		panic("nfs_read mode");
#endif
	if (uio->uio_resid == 0)
		return (0);
	if (uio->uio_offset < 0)
		return (EINVAL);
	p = uio->uio_procp;
	if ((nmp->nm_flag & (NFSMNT_NFSV3 | NFSMNT_GOTFSINFO)) == NFSMNT_NFSV3)
		(void)nfs_fsinfo(nmp, vp, cred, p);
	biosize = nmp->nm_rsize;
	/*
	 * For nfs, cache consistency can only be maintained approximately.
	 * Although RFC1094 does not specify the criteria, the following is
	 * believed to be compatible with the reference port.
	 * For nfs:
	 * If the file's modify time on the server has changed since the
	 * last read rpc or you have written to the file,
	 * you may have lost data cache consistency with the
	 * server, so flush all of the file's data out of the cache.
	 * Then force a getattr rpc to ensure that you have up to date
	 * attributes.
	 */
	if (np->n_flag & NMODIFIED) {
		NFS_INVALIDATE_ATTRCACHE(np);
		error = VOP_GETATTR(vp, &vattr, cred, p);
		if (error)
			return (error);
		np->n_mtime = vattr.va_mtime;
	} else {
		error = VOP_GETATTR(vp, &vattr, cred, p);
		if (error)
			return (error);
		if (timespeccmp(&np->n_mtime, &vattr.va_mtime, !=)) {
			error = nfs_vinvalbuf(vp, V_SAVE, cred, p);
			if (error)
				return (error);
			np->n_mtime = vattr.va_mtime;
		}
	}

	/*
	 * update the cache read creds for this vnode
	 */
	if (np->n_rcred)
		crfree(np->n_rcred);
	np->n_rcred = cred;
	crhold(cred);

	do {
	    if ((vp->v_flag & VROOT) && vp->v_type == VLNK) {
		    return (nfs_readlinkrpc(vp, uio, cred));
	    }
	    baddr = NULL;
	    switch (vp->v_type) {
	    case VREG:
		nfsstats.biocache_reads++;
		lbn = uio->uio_offset / biosize;
		on = uio->uio_offset & (biosize - 1);
		bn = lbn * (biosize / DEV_BSIZE);
		not_readin = 1;

		/*
		 * Start the read ahead(s), as required.
		 */
		if (nfs_numasync > 0 && nmp->nm_readahead > 0) {
		    for (nra = 0; nra < nmp->nm_readahead &&
			(lbn + 1 + nra) * biosize < np->n_size; nra++) {
			rabn = (lbn + 1 + nra) * (biosize / DEV_BSIZE);
			if (!incore(vp, rabn)) {
			    rabp = nfs_getcacheblk(vp, rabn, biosize, p);
			    if (!rabp)
				return (EINTR);
			    if ((rabp->b_flags & (B_DELWRI | B_DONE)) == 0) {
				rabp->b_flags |= (B_READ | B_ASYNC);
				if (nfs_asyncio(rabp, 1)) {
				    rabp->b_flags |= B_INVAL;
				    brelse(rabp);
				}
			    } else
				brelse(rabp);
			}
		    }
		}

again:
		bp = nfs_getcacheblk(vp, bn, biosize, p);
		if (!bp)
			return (EINTR);
		got_buf = 1;
		if ((bp->b_flags & (B_DONE | B_DELWRI)) == 0) {
			bp->b_flags |= B_READ;
			not_readin = 0;
			error = nfs_doio(bp, p);
			if (error) {
			    brelse(bp);
			    return (error);
			}
		}
		n = ulmin(biosize - on, uio->uio_resid);
		offdiff = np->n_size - uio->uio_offset;
		if (offdiff < (off_t)n)
			n = (int)offdiff;
		if (not_readin && n > 0) {
			if (on < bp->b_validoff || (on + n) > bp->b_validend) {
				bp->b_flags |= B_INVAFTERWRITE;
				if (bp->b_dirtyend > 0) {
				    if ((bp->b_flags & B_DELWRI) == 0)
					panic("nfsbioread");
				    if (VOP_BWRITE(bp) == EINTR)
					return (EINTR);
				} else
				    brelse(bp);
				goto again;
			}
		}
		diff = (on >= bp->b_validend) ? 0 : (bp->b_validend - on);
		if (diff < n)
			n = diff;
		break;
	    case VLNK:
		nfsstats.biocache_readlinks++;
		bp = nfs_getcacheblk(vp, 0, NFS_MAXPATHLEN, p);
		if (!bp)
			return (EINTR);
		if ((bp->b_flags & B_DONE) == 0) {
			bp->b_flags |= B_READ;
			error = nfs_doio(bp, p);
			if (error) {
				brelse(bp);
				return (error);
			}
		}
		n = ulmin(uio->uio_resid, NFS_MAXPATHLEN - bp->b_resid);
		got_buf = 1;
		on = 0;
		break;
	    default:
		panic("nfsbioread: type %x unexpected", vp->v_type);
		break;
	    }

	    if (n > 0) {
		if (!baddr)
			baddr = bp->b_data;
		error = uiomove(baddr + on, n, uio);
	    }

	    if (vp->v_type == VLNK)
		n = 0;

	    if (got_buf)
		brelse(bp);
	} while (error == 0 && uio->uio_resid > 0 && n > 0);
	return (error);
}

/*
 * Vnode op for write using bio
 */
int
nfs_write(void *v)
{
	struct vop_write_args *ap = v;
	int biosize;
	struct uio *uio = ap->a_uio;
	struct proc *p = uio->uio_procp;
	struct vnode *vp = ap->a_vp;
	struct nfsnode *np = VTONFS(vp);
	struct ucred *cred = ap->a_cred;
	int ioflag = ap->a_ioflag;
	struct buf *bp;
	struct vattr vattr;
	struct nfsmount *nmp = VFSTONFS(vp->v_mount);
	daddr_t lbn, bn;
	int n, on, error = 0, extended = 0, wrotedta = 0, truncated = 0;
	ssize_t overrun;

#ifdef DIAGNOSTIC
	if (uio->uio_rw != UIO_WRITE)
		panic("nfs_write mode");
	if (uio->uio_segflg == UIO_USERSPACE && uio->uio_procp != curproc)
		panic("nfs_write proc");
#endif
	if (vp->v_type != VREG)
		return (EIO);
	if (np->n_flag & NWRITEERR) {
		np->n_flag &= ~NWRITEERR;
		return (np->n_error);
	}
	if ((nmp->nm_flag & (NFSMNT_NFSV3 | NFSMNT_GOTFSINFO)) == NFSMNT_NFSV3)
		(void)nfs_fsinfo(nmp, vp, cred, p);
	if (ioflag & (IO_APPEND | IO_SYNC)) {
		if (np->n_flag & NMODIFIED) {
			NFS_INVALIDATE_ATTRCACHE(np);
			error = nfs_vinvalbuf(vp, V_SAVE, cred, p);
			if (error)
				return (error);
		}
		if (ioflag & IO_APPEND) {
			NFS_INVALIDATE_ATTRCACHE(np);
			error = VOP_GETATTR(vp, &vattr, cred, p);
			if (error)
				return (error);
			uio->uio_offset = np->n_size;
		}
	}
	if (uio->uio_offset < 0)
		return (EINVAL);
	if (uio->uio_resid == 0)
		return (0);

	/* do the filesize rlimit check */
	if ((error = vn_fsizechk(vp, uio, ioflag, &overrun)))
		return (error);

	/*
	 * update the cache write creds for this node.
	 */
	if (np->n_wcred)
		crfree(np->n_wcred);
	np->n_wcred = cred;
	crhold(cred);

	/*
	 * I use nm_rsize, not nm_wsize so that all buffer cache blocks
	 * will be the same size within a filesystem. nfs_writerpc will
	 * still use nm_wsize when sizing the rpc's.
	 */
	biosize = nmp->nm_rsize;
	do {

		/*
		 * XXX make sure we aren't cached in the VM page cache
		 */
		uvm_vnp_uncache(vp);

		nfsstats.biocache_writes++;
		lbn = uio->uio_offset / biosize;
		on = uio->uio_offset & (biosize-1);
		n = ulmin(biosize - on, uio->uio_resid);
		bn = lbn * (biosize / DEV_BSIZE);
again:
		bp = nfs_getcacheblk(vp, bn, biosize, p);
		if (!bp) {
			error = EINTR;
			goto out;
		}
		np->n_flag |= NMODIFIED;
		if (uio->uio_offset + n > np->n_size) {
			np->n_size = uio->uio_offset + n;
			uvm_vnp_setsize(vp, (u_long)np->n_size);
			extended = 1;
		} else if (uio->uio_offset + n < np->n_size)
			truncated = 1;

		/*
		 * If the new write will leave a contiguous dirty
		 * area, just update the b_dirtyoff and b_dirtyend,
		 * otherwise force a write rpc of the old dirty area.
		 */
		if (bp->b_dirtyend > 0 &&
		    (on > bp->b_dirtyend || (on + n) < bp->b_dirtyoff)) {
			bp->b_proc = p;
			if (VOP_BWRITE(bp) == EINTR) {
				error = EINTR;
				goto out;
			}
			goto again;
		}

		error = uiomove((char *)bp->b_data + on, n, uio);
		if (error) {
			bp->b_flags |= B_ERROR;
			brelse(bp);
			goto out;
		}
		if (bp->b_dirtyend > 0) {
			bp->b_dirtyoff = min(on, bp->b_dirtyoff);
			bp->b_dirtyend = max((on + n), bp->b_dirtyend);
		} else {
			bp->b_dirtyoff = on;
			bp->b_dirtyend = on + n;
		}
		if (bp->b_validend == 0 || bp->b_validend < bp->b_dirtyoff ||
		    bp->b_validoff > bp->b_dirtyend) {
			bp->b_validoff = bp->b_dirtyoff;
			bp->b_validend = bp->b_dirtyend;
		} else {
			bp->b_validoff = min(bp->b_validoff, bp->b_dirtyoff);
			bp->b_validend = max(bp->b_validend, bp->b_dirtyend);
		}

		wrotedta = 1;

		/*
		 * Since this block is being modified, it must be written
		 * again and not just committed.
		 */

		if (NFS_ISV3(vp)) {
			rw_enter_write(&np->n_commitlock);
			if (bp->b_flags & B_NEEDCOMMIT) {
				bp->b_flags &= ~B_NEEDCOMMIT;
				nfs_del_tobecommitted_range(vp, bp);
			}
			nfs_del_committed_range(vp, bp);
			rw_exit_write(&np->n_commitlock);
		} else 
			bp->b_flags &= ~B_NEEDCOMMIT;

		if (ioflag & IO_SYNC) {
			bp->b_proc = p;
			error = VOP_BWRITE(bp);
			if (error)
				goto out;
		} else if ((n + on) == biosize) {
			bp->b_proc = NULL;
			bp->b_flags |= B_ASYNC;
			(void)nfs_writebp(bp, 0);
		} else {
			bdwrite(bp);
		}
	} while (uio->uio_resid > 0 && n > 0);

/*out: XXX belongs here??? */
	if (wrotedta)
		VN_KNOTE(vp, NOTE_WRITE | (extended ? NOTE_EXTEND : 0) |
		    (truncated ? NOTE_TRUNCATE : 0));

out:
	/* correct the result for writes clamped by vn_fsizechk() */
	uio->uio_resid += overrun;

	return (error);
}

/*
 * Get an nfs cache block.
 * Allocate a new one if the block isn't currently in the cache
 * and return the block marked busy. If the calling process is
 * interrupted by a signal for an interruptible mount point, return
 * NULL.
 */
struct buf *
nfs_getcacheblk(struct vnode *vp, daddr_t bn, int size, struct proc *p)
{
	struct buf *bp;
	struct nfsmount *nmp = VFSTONFS(vp->v_mount);

	if (nmp->nm_flag & NFSMNT_INT) {
		bp = getblk(vp, bn, size, PCATCH, 0);
		while (bp == NULL) {
			if (nfs_sigintr(nmp, NULL, p))
				return (NULL);
			bp = getblk(vp, bn, size, 0, 2 * hz);
		}
	} else
		bp = getblk(vp, bn, size, 0, 0);
	return (bp);
}

/*
 * Flush and invalidate all dirty buffers. If another process is already
 * doing the flush, just wait for completion.
 */
int
nfs_vinvalbuf(struct vnode *vp, int flags, struct ucred *cred, struct proc *p)
{
	struct nfsmount		*nmp= VFSTONFS(vp->v_mount);
	struct nfsnode		*np = VTONFS(vp);
	int			 error, sintr, stimeo;

	error = sintr = stimeo = 0;

	if (ISSET(nmp->nm_flag, NFSMNT_INT)) {
		sintr = PCATCH;
		stimeo = 2 * hz;
	}

	/* First wait for any other process doing a flush to complete. */
	while (np->n_flag & NFLUSHINPROG) {
		np->n_flag |= NFLUSHWANT;
		error = tsleep(&np->n_flag, PRIBIO|sintr, "nfsvinval", stimeo);
		if (error && sintr && nfs_sigintr(nmp, NULL, p))
			return (EINTR);
	}

	/* Now, flush as required. */
	np->n_flag |= NFLUSHINPROG;
	error = vinvalbuf(vp, flags, cred, p, sintr, 0);
	while (error) {
		if (sintr && nfs_sigintr(nmp, NULL, p)) {
			np->n_flag &= ~NFLUSHINPROG;
			if (np->n_flag & NFLUSHWANT) {
				np->n_flag &= ~NFLUSHWANT;
				wakeup(&np->n_flag);
			}
			return (EINTR);
		}
		error = vinvalbuf(vp, flags, cred, p, 0, stimeo);
	}
	np->n_flag &= ~(NMODIFIED | NFLUSHINPROG);
	if (np->n_flag & NFLUSHWANT) {
		np->n_flag &= ~NFLUSHWANT;
		wakeup(&np->n_flag);
	}
	return (0);
}

/*
 * Initiate asynchronous I/O. Return an error if no nfsiods are available.
 * This is mainly to avoid queueing async I/O requests when the nfsiods
 * are all hung on a dead server.
 */
int
nfs_asyncio(struct buf *bp, int readahead)
{
	if (nfs_numasync == 0)
		goto out;

	while (nfs_bufqlen > nfs_bufqmax)
		if (readahead)
			goto out;
		else
			tsleep(&nfs_bufqlen, PRIBIO, "nfs_bufq", 0);

	if ((bp->b_flags & B_READ) == 0) {
		bp->b_flags |= B_WRITEINPROG;
	}

	TAILQ_INSERT_TAIL(&nfs_bufq, bp, b_freelist);
	nfs_bufqlen++;

	wakeup_one(&nfs_bufq);
	return (0);

out:
	nfsstats.forcedsync++;
	return (EIO);
}

/*
 * Do an I/O operation to/from a cache block. This may be called
 * synchronously or from an nfsiod.
 */
int
nfs_doio(struct buf *bp, struct proc *p)
{
	struct uio *uiop;
	struct vnode *vp;
	struct nfsnode *np;
	struct nfsmount *nmp;
	int s, error = 0, diff, len, iomode, must_commit = 0;
	struct uio uio;
	struct iovec io;

	vp = bp->b_vp;
	np = VTONFS(vp);
	nmp = VFSTONFS(vp->v_mount);
	uiop = &uio;
	uiop->uio_iov = &io;
	uiop->uio_iovcnt = 1;
	uiop->uio_segflg = UIO_SYSSPACE;
	uiop->uio_procp = p;

	/*
	 * Historically, paging was done with physio, but no more.
	 */
	if (bp->b_flags & B_PHYS) {
	    io.iov_len = uiop->uio_resid = bp->b_bcount;
	    /* mapping was done by vmapbuf() */
	    io.iov_base = bp->b_data;
	    uiop->uio_offset = ((off_t)bp->b_blkno) << DEV_BSHIFT;
	    if (bp->b_flags & B_READ) {
		uiop->uio_rw = UIO_READ;
		nfsstats.read_physios++;
		error = nfs_readrpc(vp, uiop);
	    } else {
		iomode = NFSV3WRITE_DATASYNC;
		uiop->uio_rw = UIO_WRITE;
		nfsstats.write_physios++;
		error = nfs_writerpc(vp, uiop, &iomode, &must_commit);
	    }
	    if (error) {
		bp->b_flags |= B_ERROR;
		bp->b_error = error;
	    }
	} else if (bp->b_flags & B_READ) {
	    io.iov_len = uiop->uio_resid = bp->b_bcount;
	    io.iov_base = bp->b_data;
	    uiop->uio_rw = UIO_READ;
	    switch (vp->v_type) {
	    case VREG:
		uiop->uio_offset = ((off_t)bp->b_blkno) << DEV_BSHIFT;
		nfsstats.read_bios++;
		bcstats.pendingreads++;
		bcstats.numreads++;
		error = nfs_readrpc(vp, uiop);
		if (!error) {
		    bp->b_validoff = 0;
		    if (uiop->uio_resid) {
			/*
			 * If len > 0, there is a hole in the file and
			 * no writes after the hole have been pushed to
			 * the server yet.
			 * Just zero fill the rest of the valid area.
			 */
			diff = bp->b_bcount - uiop->uio_resid;
			len = np->n_size - ((((off_t)bp->b_blkno) << DEV_BSHIFT)
				+ diff);
			if (len > 0) {
			    len = ulmin(len, uiop->uio_resid);
			    memset((char *)bp->b_data + diff, 0, len);
			    bp->b_validend = diff + len;
			} else
			    bp->b_validend = diff;
		    } else
			bp->b_validend = bp->b_bcount;
		}
		if (p && (vp->v_flag & VTEXT) &&
		    (timespeccmp(&np->n_mtime, &np->n_vattr.va_mtime, !=))) {
			uprintf("Process killed due to text file modification\n");
			psignal(p, SIGKILL);
		}
		break;
	    case VLNK:
		uiop->uio_offset = (off_t)0;
		nfsstats.readlink_bios++;
		bcstats.pendingreads++;
		bcstats.numreads++;
		error = nfs_readlinkrpc(vp, uiop, curproc->p_ucred);
		break;
	    default:
		panic("nfs_doio:  type %x unexpected", vp->v_type);
		break;
	    };
	    if (error) {
		bp->b_flags |= B_ERROR;
		bp->b_error = error;
	    }
	} else {
	    io.iov_len = uiop->uio_resid = bp->b_dirtyend
		- bp->b_dirtyoff;
	    uiop->uio_offset = ((off_t)bp->b_blkno) * DEV_BSIZE
		+ bp->b_dirtyoff;
	    io.iov_base = (char *)bp->b_data + bp->b_dirtyoff;
	    uiop->uio_rw = UIO_WRITE;
	    nfsstats.write_bios++;
	    bcstats.pendingwrites++;
	    bcstats.numwrites++;
	    if ((bp->b_flags & (B_ASYNC | B_NEEDCOMMIT | B_NOCACHE)) == B_ASYNC)
		iomode = NFSV3WRITE_UNSTABLE;
	    else
		iomode = NFSV3WRITE_FILESYNC;
	    bp->b_flags |= B_WRITEINPROG;
	    error = nfs_writerpc(vp, uiop, &iomode, &must_commit);

	    rw_enter_write(&np->n_commitlock);
	    if (!error && iomode == NFSV3WRITE_UNSTABLE) {
		bp->b_flags |= B_NEEDCOMMIT;
		nfs_add_tobecommitted_range(vp, bp);
	    } else {
		bp->b_flags &= ~B_NEEDCOMMIT;
		nfs_del_committed_range(vp, bp);
	    }
	    rw_exit_write(&np->n_commitlock);

	    bp->b_flags &= ~B_WRITEINPROG;

	    /*
	     * For an interrupted write, the buffer is still valid and the
	     * write hasn't been pushed to the server yet, so we can't set
	     * B_ERROR and report the interruption by setting B_EINTR. For
	     * the B_ASYNC case, B_EINTR is not relevant, so the rpc attempt
	     * is essentially a noop.
	     * For the case of a V3 write rpc not being committed to stable
	     * storage, the block is still dirty and requires either a commit
	     * rpc or another write rpc with iomode == NFSV3WRITE_FILESYNC
	     * before the block is reused. This is indicated by setting the
	     * B_DELWRI and B_NEEDCOMMIT flags.
	     */
	    if (error == EINTR || (!error && (bp->b_flags & B_NEEDCOMMIT))) {
		    s = splbio();
		    buf_dirty(bp);
		    splx(s);

		    if (!(bp->b_flags & B_ASYNC) && error)
			    bp->b_flags |= B_EINTR;
	    } else {
		if (error) {
		    bp->b_flags |= B_ERROR;
		    bp->b_error = np->n_error = error;
		    np->n_flag |= NWRITEERR;
		}
		bp->b_dirtyoff = bp->b_dirtyend = 0;
	    }
	}
	bp->b_resid = uiop->uio_resid;
	if (must_commit)
		nfs_clearcommit(vp->v_mount);
	s = splbio();
	biodone(bp);
	splx(s);
	return (error);
}
@


1.81
log
@Convert to uiomove. From Martin Natano.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.80 2015/03/14 03:38:52 jsg Exp $	*/
d61 2
@


1.80
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.79 2015/02/10 21:56:10 miod Exp $	*/
d180 1
a180 1
		n = min((unsigned)(biosize - on), uio->uio_resid);
d214 1
a214 1
		n = min(uio->uio_resid, NFS_MAXPATHLEN - bp->b_resid);
d226 1
a226 1
		error = uiomovei(baddr + on, (int)n, uio);
d321 1
a321 1
		n = min((unsigned)(biosize - on), uio->uio_resid);
d352 1
a352 1
		error = uiomovei((char *)bp->b_data + on, n, uio);
d593 1
a593 1
			    len = min(len, uiop->uio_resid);
@


1.79
log
@First step towards making uiomove() take a size_t size argument:
- rename uiomove() to uiomovei() and update all its users.
- introduce uiomove(), which is similar to uiomovei() but with a size_t.
- rewrite uiomovei() as an uiomove() wrapper.
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.78 2014/12/18 20:59:21 tedu Exp $	*/
a50 1
#include <nfs/rpcv2.h>
@


1.78
log
@delete a whole mess of unnecessary caddr_t casts
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.77 2014/11/14 23:01:44 tedu Exp $	*/
d227 1
a227 1
		error = uiomove(baddr + on, (int)n, uio);
d353 1
a353 1
		error = uiomove((char *)bp->b_data + on, n, uio);
@


1.77
log
@bzero -> memset
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.76 2014/07/08 17:19:26 deraadt Exp $	*/
d135 1
a135 1
	    baddr = (caddr_t)0;
@


1.76
log
@decouple struct uvmexp into a new file, so that uvm_extern.h and sysctl.h
don't need to be married.
ok guenther miod beck jsing kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.75 2013/09/14 02:28:03 guenther Exp $	*/
d595 1
a595 1
			    bzero((char *)bp->b_data + diff, len);
@


1.75
log
@Correct the handling of I/O of >=2^32 bytes and the ktracing there of
by using size_t/ssize_t instead of int/u_int to handle I/O lengths in
uiomove(), vn_fsizechk(), and ktrgenio().  Eliminate the always-zero
'error' argument to ktrgenio() at the same time.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.74 2013/06/11 16:42:17 deraadt Exp $	*/
a49 2

#include <uvm/uvm_extern.h>
@


1.74
log
@final removal of daddr64_t.  daddr_t has been 64 bit for a long enough
test period; i think 3 years ago the last bugs fell out.
ok otto beck others
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.73 2012/07/11 12:39:20 guenther Exp $	*/
d260 1
a260 1
	int overrun;
@


1.73
log
@If the current offset is strictly less than the process filesize
rlimit, then a write that would take it over the limit should be
clamped, making it a partial write.

ok beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.72 2010/08/07 03:50:02 krw Exp $	*/
d78 1
a78 1
	daddr64_t lbn, bn, rabn;
d258 1
a258 1
	daddr64_t lbn, bn;
d429 1
a429 1
nfs_getcacheblk(struct vnode *vp, daddr64_t bn, int size, struct proc *p)
@


1.72
log
@No "\n" needed at the end of panic() strings.

Bogus chunks pointed out by matthew@@ and miod@@. No cookies for
marco@@ and jasper@@.

ok deraadt@@ miod@@ matthew@@ jasper@@ macro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.71 2010/04/12 16:37:38 beck Exp $	*/
d260 1
d295 4
a298 9
	/*
	 * Maybe this should be above the vnode op call, but so long as
	 * file servers have no limits, i don't think it matters
	 */
	if (p && uio->uio_offset + uio->uio_resid >
	      p->p_rlimit[RLIMIT_FSIZE].rlim_cur) {
		psignal(p, SIGXFSZ);
		return (EFBIG);
	}
d328 4
a331 2
		if (!bp)
			return (EINTR);
d348 4
a351 2
			if (VOP_BWRITE(bp) == EINTR)
				return (EINTR);
d359 1
a359 1
			return (error);
d399 1
a399 1
				return (error);
d409 1
d414 5
a418 1
	return (0);
@


1.71
log
@
Don't jump the queue if we have to wait on the client side because
the nfs_bufq is full - instead tsleep waiting for one of our nfsiod's
to free up space for us in the queue so we can enqueue on the end.

ok blambert@@, tedu@@, oga@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.70 2010/04/09 22:42:10 oga Exp $	*/
d222 1
a222 1
		panic("nfsbioread: type %x unexpected\n", vp->v_type);
d613 1
a613 1
		panic("nfs_doio:  type %x unexpected\n", vp->v_type);
@


1.70
log
@make more bettah. instead of doing:

	switch(type) {
	case VREG:
		/*something */
		break;
	case VLNK:
		/* something */
		break;
	default:
		panic("wtf?");
	}

	do_something_that_doesn't_change_type();

	switch(type) {
	case VREG:
		/* nowt */
		break;
	case VLNK:
		n = 0;
		break;
	default:
		panic("wtf?");
	}

be a bit less silly and replace the second switch with:

if (type == VLNK)
	n = 0;

ok beck@@, blambert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.69 2010/04/09 22:08:04 oga Exp $	*/
d159 1
a159 1
				if (nfs_asyncio(rabp)) {
d495 1
a495 1
nfs_asyncio(struct buf *bp)
d500 5
a504 2
	if (nfs_bufqlen > nfs_bufqmax)
		goto out; /* too many bufs in use, force sync */
@


1.69
log
@In the nfs bio functions, instead of looking at an invalid vnode type,
deciding to do nothing, printing about it and continuing along our merry
way without even erroring the sodding buffer, just panic. by this point
we are liked very fucked up anyway.

found in either edmonton or stockholm then forgotten. ok beck@@,
blambert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.68 2009/10/19 22:24:18 jsg Exp $	*/
d231 2
a232 4
	    switch (vp->v_type) {
	    case VREG:
		break;
	    case VLNK:
d234 1
a234 4
		break;
	    default:
		panic("nfsbioread: type %x unexpected\n", vp->v_type);
	    }
@


1.68
log
@antsy
no binary change apart from nfsm_reqhead() which is clearly correct.

ok thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.67 2009/09/02 18:20:54 thib Exp $	*/
d222 1
a222 1
		printf(" nfsbioread: type %x unexpected\n",vp->v_type);
d238 1
a238 1
		printf(" nfsbioread: type %x unexpected\n",vp->v_type);
d615 1
a615 1
		printf("nfs_doio:  type %x unexpected\n", vp->v_type);
@


1.67
log
@Backout the asyncio/aiod change, as it causes buf's to get hung.
problem noticed by deraadt@@

ok beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.65 2009/08/27 23:26:56 thib Exp $	*/
d70 1
a70 5
nfs_bioread(vp, uio, ioflag, cred)
	struct vnode *vp;
	struct uio *uio;
	int ioflag;
	struct ucred *cred;
d250 1
a250 2
nfs_write(v)
	void *v;
d429 1
a429 5
nfs_getcacheblk(vp, bn, size, p)
	struct vnode *vp;
	daddr64_t bn;
	int size;
	struct proc *p;
d500 1
a500 2
nfs_asyncio(bp)
	struct buf *bp;
d528 1
a528 3
nfs_doio(bp, p)
	struct buf *bp;
	struct proc *p;
@


1.66
log
@Garbage collect two variables that where set but unused.
Tiny spacing nit.
Fix a typo, pointed out by miod@@.
@
text
@d60 1
d62 2
d153 1
a153 1
		if (nfs_numaiods > 0 && nmp->nm_readahead > 0) {
d509 2
a510 1
nfs_asyncio(struct buf *bp)
d512 1
a512 41
	struct nfs_aiod	*aiod;
	struct nfsmount	*nmp;
	int		 gotone, error;

	aiod = NULL;
	nmp = VFSTONFS(bp->b_vp->v_mount);
	gotone = error = 0;

	mtx_enter(&nfs_aiodl_mtx);
	aiod = LIST_FIRST(&nfs_aiods_idle);
	if (aiod) {
		/*
		 * Found an available aiod, wake it up and send
		 * it to work on this mount.
		 */
		LIST_REMOVE(aiod, nad_idle);
		mtx_leave(&nfs_aiodl_mtx);

		aiod->nad_flags |= NFSAIOD_WAKEUP;
		gotone = 1;
		KASSERT(aiod->nad_mnt == NULL);
		aiod->nad_mnt = nmp;
		nmp->nm_naiods++;
		wakeup_one(aiod);
	} else {
		mtx_leave(&nfs_aiodl_mtx);
	}

	/*
	 * If no aiod's are available, check if theres already an
	 * aiod assoicated with this mount, if so it will process
	 * this buf.
	 */
	if (!gotone && nmp->nm_naiods > 0)
		gotone = 1;

	/*
	 * If we still don't have an aiod to process this buffer,
	 * force it sync.
	 */
	if (!gotone)
d515 2
d518 2
a519 13
	/*
	 * Make sure we don't queue up too much.
	 * TODO: Look into implementing migration for aiods.
	 */
	if (nmp->nm_bufqlen >= nfs_aiodbufqmax) {
		if (aiod != NULL) {
			aiod->nad_flags &= ~NFSAIOD_WAKEUP;
			aiod->nad_mnt = NULL;
			mtx_enter(&nfs_aiodl_mtx);
			LIST_INSERT_HEAD(&nfs_aiods_idle, aiod, nad_idle);
			mtx_leave(&nfs_aiodl_mtx);
		}
		goto out;
d522 2
a523 1
	/* Finally, queue the buffer and return. */
d525 2
a526 2
	if ((bp->b_flags & B_READ) == 0)
		bp->b_flags |= B_WRITEINPROG;
a527 3
	TAILQ_INSERT_TAIL(&nmp->nm_bufq, bp, b_freelist);
	nmp->nm_bufqlen++;
	return (0);
@


1.65
log
@introduce a flag member to struct nfs_aiod, and use flags instead of the exit
and worked members. nad_worked becomes NFSAIOD_WAKEUP, which is set after if
an aiod was removed from the idle list and woken up by nfs_asyncio().

don't rely on tsleep wchans being unique, that is keep going back to sleep if
woken up unless the NFSAIOD_WAKEUP flag is set.

fix a divide by zero crash if nfs.vfs.iothreads is set to 0, as that can happen
when we recalculate the maximum buf's to queue up for each aiod.

in nfs_asyncio() set the nad_mnt to NULL before returning the aiod back to the
idle list in the case where we have already queued up to many bufs, otherwise
we trip an assertion.

minimize the time we are holding the nfs_aiodl_mtx to only when we are inserting
or removing from the lists, with the exception of nfs_set_naiod() as it would
make the loops more complicated and its uncommon in any case.

tested by myself and deraadt@@
"fine with me" deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.64 2009/08/26 12:08:10 thib Exp $	*/
d553 1
a553 1
	 * Make sure we don't queue up to much.
@


1.64
log
@make sure that an aiod has been removed from the nfs_aiods_idle list
before inserting it back into the list.

crashes debugged with help from deraadt@@ who also tested this fix.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.61 2009/07/22 13:02:08 thib Exp $	*/
a523 1
		aiod->nad_worked = 1;
d525 2
d531 1
a531 1
		wakeup(aiod);
d558 2
@


1.63
log
@Rework the way we do async I/O in nfs. Introduce separate buf queues for
each mount, and when work is "found", peg an aiod to that mount todo the
I/O. Make nfs_asyncio() a bit smarter when deciding when to do asyncio
and when to force it sync, this is done by keeping the aiod's one two lists,
an "idle" and an "all" list, so asyncio is only done when there are aiods
hanging around todo it for us or are already pegged to the mount.

Idea liked by at least beck@@ (and I think art@@).
Extensive testing done by myself and jasper and a few others on various
arch's.

Ideas/Code from Net/Free.

OK blambert@@.
@
text
@d520 1
a520 1
		 * Found an avilable aiod, wake it up and send
d524 1
@


1.62
log
@Using the buf pointer returned from incore is a really bad idea.
Even if we know that someone safely holds B_BUSY and will not modify
the buf (as was the case in here), we still need to be sure that
the B_BUSY will not be released while we fiddle with the buf.

In this case, it was not safe, since copyout can sleep and whoever was
writing out the buf could finish the write and release the buf which
could then get recycled or unmapped while we slept. Always acquire
B_BUSY ourselves, even when it might give a minor performance penalty.

thib@@ ok
@
text
@a59 1
extern int nfs_numasync;
a60 2
struct nfs_bufqhead nfs_bufq;
uint32_t nfs_bufqmax, nfs_bufqlen;
d150 1
a150 1
		if (nfs_numasync > 0 && nmp->nm_readahead > 0) {
d506 1
a506 2
nfs_asyncio(bp)
	struct buf *bp;
d508 39
a546 1
	if (nfs_numasync == 0)
a548 2
	if (nfs_bufqlen > nfs_bufqmax)
		goto out; /* too many bufs in use, force sync */
d550 11
a560 2
	if ((bp->b_flags & B_READ) == 0) {
		bp->b_flags |= B_WRITEINPROG;
d563 4
a566 2
	TAILQ_INSERT_TAIL(&nfs_bufq, bp, b_freelist);
	nfs_bufqlen++;
d568 2
a569 1
	wakeup_one(&nfs_bufq);
a570 1

@


1.61
log
@remove a comment thats part lie and part stating the obvious.

ok blambert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.58 2009/03/19 16:44:40 oga Exp $	*/
a172 11
		/*
		 * If the block is in the cache and has the required data
		 * in a valid region, just copy it out.
		 * Otherwise, get the block and write back/read in,
		 * as required.
		 */
		if ((bp = incore(vp, bn)) &&
		    (bp->b_flags & (B_BUSY | B_WRITEINPROG)) ==
		    (B_BUSY | B_WRITEINPROG))
			got_buf = 0;
		else {
d174 11
a184 12
			bp = nfs_getcacheblk(vp, bn, biosize, p);
			if (!bp)
				return (EINTR);
			got_buf = 1;
			if ((bp->b_flags & (B_DONE | B_DELWRI)) == 0) {
				bp->b_flags |= B_READ;
				not_readin = 0;
				error = nfs_doio(bp, p);
				if (error) {
				    brelse(bp);
				    return (error);
				}
a192 6
				if (!got_buf) {
				    bp = nfs_getcacheblk(vp, bn, biosize, p);
				    if (!bp)
					return (EINTR);
				    got_buf = 1;
				}
@


1.60
log
@(struct foo *)0 -> NULL, every where I could find it.

OK blambert@@
@
text
@a422 3
		/*
		 * If the lease is non-cachable or IO_SYNC do bwrite().
		 */
@


1.59
log
@- /dev/drum is long gone; sync comment with reality

ok thib@@
@
text
@d432 1
a432 1
			bp->b_proc = (struct proc *)0;
d466 3
a468 3
		while (bp == (struct buf *)0) {
			if (nfs_sigintr(nmp, (struct nfsreq *)0, p))
				return ((struct buf *)0);
@


1.58
log
@We don't count buffercache stats in the B_PHYS case, so fix nfs to not
increment the num{read,write} and pending{read,write} statistics in that
case, since biodone won't change them on completion.

On another note, I'm not sure that we use physical buffers for swapping
over nfs anymore, so this chunk may be superfluous.

beck@@ came up with the same diff "So anyway rather than me commiting it
from my copy, I'm giving you the OK and the commit. since it officially
makes you a buffer cache and NFS hacker };-)"
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.57 2009/01/24 23:30:42 thib Exp $	*/
d581 1
a581 1
	 * Historically, paging was done with physio, but no more...
a583 3
	    /*
	     * ...though reading /dev/drum still gets us here.
	     */
@


1.57
log
@Use a timespec instead of a time_t for the clients nfsnode
mtime, gives us better granularity, helps with cache consistency.

Idea lifted from NetBSD.

OK blambert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.56 2009/01/19 23:40:36 thib Exp $	*/
a593 2
		bcstats.pendingreads++;
		bcstats.numreads++;
a598 2
		bcstats.pendingwrites++;
		bcstats.numwrites++; 
@


1.56
log
@Introduce a macro to invalidate the attribute
cache instead of setting n_attrstamp to 0 directly.

Lift the macro name from NetBSD.
prompted by and OK blambert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.55 2008/08/09 10:14:02 thib Exp $	*/
d49 1
d116 1
a116 1
		np->n_mtime = vattr.va_mtime.tv_sec;
d121 1
a121 1
		if (np->n_mtime != vattr.va_mtime.tv_sec) {
d125 1
a125 1
			np->n_mtime = vattr.va_mtime.tv_sec;
d642 1
a642 1
		    (np->n_mtime != np->n_vattr.va_mtime.tv_sec)) {
@


1.55
log
@o nfs_vinvalbuf() is always called with the intrflag as 1, and then
  checks if the mount is actually interrutable, and if not sets it 0.
  remove this argument from nfs_vinvalbuf and just do the checking inside
  the function.
o give nfs_vinvalbuf() a makeover so it looks nice. (spacing, casts, &c);
o Actually pass PCATCH too tsleep() if the mount it interrutable.

ok art@@, blambert@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.54 2008/08/08 20:44:38 blambert Exp $	*/
a108 4
	 * NB: This implies that cache data can be read when up to
	 * NFS_ATTRTIMEO seconds out of date. If you find that you need current
	 * attributes this could be forced by setting n_attrstamp to 0 before
	 * the VOP_GETATTR() call.
d111 1
a111 1
		np->n_attrstamp = 0;
d304 1
a304 1
			np->n_attrstamp = 0;
d310 1
a310 1
			np->n_attrstamp = 0;
@


1.54
log
@After beck@@ changed the way nfsiod's are notified of work, the
nfs_iodwant array became unused. Garbage collect and free up
a few bytes.

ok thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.53 2008/07/25 14:56:47 beck Exp $	*/
d125 1
a125 1
			error = nfs_vinvalbuf(vp, V_SAVE, cred, p, 1);
d309 1
a309 1
			error = nfs_vinvalbuf(vp, V_SAVE, cred, p, 1);
d484 1
a484 6
nfs_vinvalbuf(vp, flags, cred, p, intrflg)
	struct vnode *vp;
	int flags;
	struct ucred *cred;
	struct proc *p;
	int intrflg;
d486 10
a495 3
	struct nfsnode *np = VTONFS(vp);
	struct nfsmount *nmp = VFSTONFS(vp->v_mount);
	int error = 0, slpflag, slptimeo;
d497 1
a497 12
	if ((nmp->nm_flag & NFSMNT_INT) == 0)
		intrflg = 0;
	if (intrflg) {
		slpflag = PCATCH;
		slptimeo = 2 * hz;
	} else {
		slpflag = 0;
		slptimeo = 0;
	}
	/*
	 * First wait for any other process doing a flush to complete.
	 */
d500 2
a501 3
		error = tsleep((caddr_t)&np->n_flag, PRIBIO + 2, "nfsvinval",
			slptimeo);
		if (error && intrflg && nfs_sigintr(nmp, (struct nfsreq *)0, p))
d505 1
a505 3
	/*
	 * Now, flush as required.
	 */
d507 1
a507 1
	error = vinvalbuf(vp, flags, cred, p, slpflag, 0);
d509 1
a509 1
		if (intrflg && nfs_sigintr(nmp, (struct nfsreq *)0, p)) {
d513 1
a513 1
				wakeup((caddr_t)&np->n_flag);
d517 1
a517 1
		error = vinvalbuf(vp, flags, cred, p, 0, slptimeo);
d522 1
a522 1
		wakeup((caddr_t)&np->n_flag);
@


1.53
log
@much more correct way of dealing with nfs pending reads/writes
ok thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.52 2008/07/23 16:24:43 beck Exp $	*/
a58 1
extern struct proc *nfs_iodwant[NFS_MAXASYNCDAEMON];
@


1.52
log
@
Correct cases of mishandling of pending reads and writes to prevent
them going negative - this consists of identifying a number of cases of
IO not going through the buffer cache and marking those buffers with
B_RAW - as well as fixing nfs_bio to show pending writes and reads through
the buffer cache via NFS

still has a problem with mishandling the counters I believe in the
async/sync fallback case where counters stay positive which will be
addressed seperately.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.51 2008/06/14 19:33:58 beck Exp $	*/
d610 2
a611 1
		bcstats.pendingreads++; /* XXX */
d617 2
a618 1
		bcstats.pendingwrites++; /* XXX */
d634 1
d666 2
a667 1
		bcstats.pendingreads++; /* XXX */
d686 2
a687 1
	    bcstats.pendingwrites++; /* XXX */
@


1.51
log
@
Ensure each nfsiod can actually enqueue more than one asynchio - this mirrors
the accidental situation that used to happen when it leaked buffers and allowed
the syncer to do it, however this puts a limit on how much of the buffer cache
it is allowed to consume to a sensible amount - improves nfs write performance
since we don't have to do tons of them synch now.

Modifies the existing code to use wakeup_one instead of cruft, and now
all nfsiod's tsleep the same way.

ok thib@@ art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.50 2008/06/12 19:14:15 thib Exp $	*/
d610 1
d616 1
d631 1
d663 1
d682 1
@


1.50
log
@add a statistic bit to count how often we change async to sync

you need to upgrade nfsstat and the relevant header files

ok beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.49 2008/06/12 16:04:37 art Exp $	*/
d63 1
a548 2
	int i;

d552 5
a556 11
	for (i = 0; i < NFS_MAXASYNCDAEMON; i++) {
	    if (nfs_iodwant[i]) {
		if ((bp->b_flags & B_READ) == 0) {
			bp->b_flags |= B_WRITEINPROG;
		}
	
		TAILQ_INSERT_TAIL(&nfs_bufq, bp, b_freelist);
		nfs_iodwant[i] = (struct proc *)0;
		wakeup((caddr_t)&nfs_iodwant[i]);
		return (0);
	    }
d558 6
@


1.49
log
@	if (something_complicated)
		return (EIO);
	return (EIO);
is kinda silly. Don't.

Prettify a bit in the process.

'makes perfect sense' blambert@@, ok thib@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.48 2008/06/12 06:44:16 thib Exp $	*/
d551 2
a552 1
		return (EIO);
d566 2
@


1.48
log
@
Actually return an error in nfs_asyncio() if we fail to process
the buf due too all of the nfs iod's being busy; this downgrades
the write to a sync one and allows to handle this.

ok art@@, beck@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.47 2008/06/11 04:52:27 blambert Exp $	*/
d552 1
a552 1
	for (i = 0; i < NFS_MAXASYNCDAEMON; i++)
d563 1
a563 8

	/*
	 * If it is a read or a write already marked B_WRITEINPROG or B_NOCACHE
	 * return EIO so the process will call nfs_doio() and do it
	 * synchronously.
	 */
	if (bp->b_flags & (B_READ | B_WRITEINPROG | B_NOCACHE))
		return (EIO);
@


1.47
log
@Canonical for() -> queue.h FOREACH macro conversions.
Also, it is historical practice to #include <sys/queue.h>
when using queue.h macros.

ok thib@@ krw@@

special thanks to krw@@ for reminders vice violence
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.46 2007/06/01 23:47:57 deraadt Exp $	*/
d548 1
a548 1
	int i,s;
d572 1
a572 11
	/*
	 * Just turn the async write into a delayed write, instead of
	 * doing in synchronously. Hopefully, at least one of the nfsiods
	 * is currently doing a write for this file and will pick up the
	 * delayed writes before going back to sleep.
	 */
	s = splbio();
	buf_dirty(bp);
	biodone(bp);
	splx(s);
	return (0);
@


1.46
log
@pedro ok'd this ~3500 line diff which removes the vop argument
"ap = v" comments in under 8 seconds, so it must be ok.  and it compiles
too.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.45 2007/06/01 07:13:47 thib Exp $	*/
d48 1
@


1.45
log
@daddr_t -> daddr64_t;
Basically the usage of daddr_t was to math out arguments to
nfs_getcacheblk, wich calls getblk();

ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.44 2006/11/29 12:24:18 miod Exp $	*/
d277 1
a277 6
	struct vop_write_args /* {
		struct vnode *a_vp;
		struct uio *a_uio;
		int  a_ioflag;
		struct ucred *a_cred;
	} */ *ap = v;
@


1.44
log
@Kernel stack can be swapped. This means that stuff that's on the stack
should never be referenced outside the context of the process to which
this stack belongs unless we do the PHOLD/PRELE dance. Loads of code
doesn't follow the rules here. Instead of trying to track down all
offenders and fix this hairy situation, it makes much more sense
to not swap kernel stacks.

From art@@, tested by many some time ago.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.43 2006/11/01 00:12:35 thib Exp $	*/
d80 1
a80 1
	daddr_t lbn, bn, rabn;
d230 1
a230 1
		bp = nfs_getcacheblk(vp, (daddr_t)0, NFS_MAXPATHLEN, p);
d293 1
a293 1
	daddr_t lbn, bn;
d464 1
a464 1
	daddr_t bn;
@


1.43
log
@move the declaration of nfsstats from nfs_bio.c to
nfs_subs.c so it gets pulled in for NFSSERVER only
kernels.

ok deraadt@@,krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.42 2006/04/20 14:43:32 pedro Exp $	*/
a673 1
			p->p_holdcnt++;
@


1.42
log
@Remove unused debug code that sneaked in by accident long ago
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.41 2005/10/31 20:22:36 otto Exp $	*/
d60 1
a60 1
struct nfsstats nfsstats;
@


1.41
log
@Fix reading large files; from NetBSD. Somehow this was overlooked
when earlier merges were done. Fixes PR 4250.  ok millert@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.40 2004/08/03 17:11:48 marius Exp $	*/
a702 4
#ifdef fvdl_debug
	    printf("nfs_doio(%x): bp %x doff %d dend %d\n", 
		vp, bp, bp->b_dirtyoff, bp->b_dirtyend);
#endif
@


1.40
log
@NFS commit coalescion: instead of sending a commit for each block, coalesce
these into larger ranges wherever possible.

this should speed up NFS writes quite a bit.

ok art@@ millert@@ pedro@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.39 2004/07/21 17:30:55 marius Exp $	*/
d83 1
d202 3
a204 3
		diff = np->n_size - uio->uio_offset;
		if (diff < n)
			n = diff;
@


1.39
log
@kqueue support for NFS, adapted from netbsd.

ok art@@ pedro@@, "get it in" deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.38 2003/06/02 23:28:19 millert Exp $	*/
a37 1

d417 11
a427 1
		bp->b_flags &= ~B_NEEDCOMMIT;
d682 1
a682 1
		printf("nfs_doio:  type %x unexpected\n",vp->v_type);
d707 3
a709 1
	    if (!error && iomode == NFSV3WRITE_UNSTABLE)
d711 2
a712 1
	    else
d714 4
@


1.38
log
@Remove the advertising clause in the UCB license which Berkeley
rescinded 22 July 1999.  Proofed by myself and Theo.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.37 2003/05/13 02:09:47 jason Exp $	*/
d294 1
a294 1
	int n, on, error = 0;
d373 3
a375 1
		}
d412 2
d436 5
@


1.37
log
@Kill a bunch more commons (very few left =)
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.36 2002/05/21 21:07:09 art Exp $	*/
d19 1
a19 5
 * 3. All advertising materials mentioning features or use of this software
 *    must display the following acknowledgement:
 *	This product includes software developed by the University of
 *	California, Berkeley and its contributors.
 * 4. Neither the name of the University nor the names of its contributors
@


1.36
log
@Protect calls to biodone with splbio. Some functions called
by biodone assume splbio (probably just on other filesystems) and some
callbacks from b_iodone assume it too. It's just much safer.
costa@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.35 2002/02/08 08:20:49 csapuntz Exp $	*/
d66 1
@


1.35
log
@There are NFS servers where it's possible to modify a symbolic link. Remove aggressive optimization
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.34 2002/01/16 21:51:16 ericj Exp $	*/
d568 1
a569 1
	biodone(bp);
d729 1
d731 1
@


1.34
log
@
use queue.h macro's
remove register
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.33 2001/12/19 08:58:06 art Exp $	*/
d116 12
a127 8
	/* 
	 * There is no way to modify a symbolic link via NFS or via
	 * VFS, so we don't check if the link was modified 
	 */
	if (vp->v_type != VLNK) {
		if (np->n_flag & NMODIFIED) {
			np->n_attrstamp = 0;
			error = VOP_GETATTR(vp, &vattr, cred, p);
a130 10
		} else {
			error = VOP_GETATTR(vp, &vattr, cred, p);
			if (error)
				return (error);
			if (np->n_mtime != vattr.va_mtime.tv_sec) {
				error = nfs_vinvalbuf(vp, V_SAVE, cred, p, 1);
				if (error)
					return (error);
				np->n_mtime = vattr.va_mtime.tv_sec;
			}
@


1.33
log
@UBC was a disaster. It worked very good when it worked, but on some
machines or some configurations or in some phase of the moon (we actually
don't know when or why) files disappeared. Since we've not been able to
track down the problem in two weeks intense debugging and we need -current
to be stable, back out everything to a state it had before UBC.

We apologise for the inconvenience.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.24 2001/11/15 23:15:15 art Exp $	*/
d73 2
a74 2
	register struct vnode *vp;
	register struct uio *uio;
d78 2
a79 2
	register struct nfsnode *np = VTONFS(vp);
	register int biosize, diff;
d292 2
a293 2
	register int biosize;
	register struct uio *uio = ap->a_uio;
d295 1
a295 1
	register struct vnode *vp = ap->a_vp;
d297 1
a297 1
	register struct ucred *cred = ap->a_cred;
d458 1
a458 1
	register struct buf *bp;
d485 1
a485 1
	register struct nfsnode *np = VTONFS(vp);
d588 2
a589 2
	register struct uio *uiop;
	register struct vnode *vp;
@


1.32
log
@Workaround a compiler bug on m68k.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.31 2001/12/10 02:19:34 art Exp $	*/
a52 1
#include <sys/pool.h>
d54 1
a54 1
#include <uvm/uvm.h>
d73 2
a74 2
	struct vnode *vp;
	struct uio *uio;
d78 3
a80 3
	struct nfsnode *np = VTONFS(vp);
	int biosize;
	struct buf *bp = NULL;
d84 1
d86 1
a86 1
	int got_buf = 0, error = 0, n = 0, on = 0;
d156 24
a179 14
		error = 0;
		while (uio->uio_resid > 0) {
			void *win;
			vsize_t bytelen = MIN(np->n_size - uio->uio_offset,
					      uio->uio_resid);

			if (bytelen == 0)
				break;
			win = ubc_alloc(&vp->v_uobj, uio->uio_offset,
					&bytelen, UBC_READ);
			error = uiomove(win, bytelen, uio);
			ubc_release(win, 0);
			if (error) {
				break;
d181 1
d183 53
a235 1
		n = 0;
a236 1

d250 1
a250 1
		n = MIN(uio->uio_resid, NFS_MAXPATHLEN - bp->b_resid);
d292 2
a293 2
	int biosize;
	struct uio *uio = ap->a_uio;
d295 1
a295 1
	struct vnode *vp = ap->a_vp;
d297 1
a297 1
	struct ucred *cred = ap->a_cred;
d299 1
d302 2
a303 2
	int error = 0;
	int rv;
a362 3
		void *win;
		voff_t oldoff = uio->uio_offset;
		vsize_t bytelen;
d365 1
a365 1
		 * XXXART - workaround for compiler bug on 68k. Wieee!
d367 1
a367 1
		 *((volatile vsize_t *)&bytelen) = uio->uio_resid;
d370 8
d379 3
a381 3
		if (np->n_size < uio->uio_offset + bytelen) {
			np->n_size = uio->uio_offset + bytelen;
			uvm_vnp_setsize(vp, np->n_size);
d383 12
a394 22
		win = ubc_alloc(&vp->v_uobj, uio->uio_offset, &bytelen,
				UBC_WRITE);
		error = uiomove(win, bytelen, uio);
		ubc_release(win, 0);
		rv = 1;
		if ((ioflag & IO_SYNC)) {
			simple_lock(&vp->v_uobj.vmobjlock);
			rv = vp->v_uobj.pgops->pgo_flush(
			    &vp->v_uobj,
			    oldoff & ~(nmp->nm_wsize - 1),
			    uio->uio_offset & ~(nmp->nm_wsize - 1),
			    PGO_CLEANIT|PGO_SYNCIO);
			simple_unlock(&vp->v_uobj.vmobjlock);
		} else if ((oldoff & ~(nmp->nm_wsize - 1)) !=
		    (uio->uio_offset & ~(nmp->nm_wsize - 1))) {
			simple_lock(&vp->v_uobj.vmobjlock);
			rv = vp->v_uobj.pgops->pgo_flush(
			    &vp->v_uobj,
			    oldoff & ~(nmp->nm_wsize - 1),
			    uio->uio_offset & ~(nmp->nm_wsize - 1),
			    PGO_CLEANIT|PGO_WEAK);
			simple_unlock(&vp->v_uobj.vmobjlock);
d396 13
a408 2
		if (!rv) {
			error = EIO;
d410 29
a438 2
		if (error) {
			break;
d440 2
a441 2
	} while (uio->uio_resid > 0);
	return (error);
d463 3
a465 3
		while (bp == NULL) {
			if (nfs_sigintr(nmp, NULL, p))
				return (NULL);
d505 1
a505 1
		if (error && intrflg && nfs_sigintr(nmp, NULL, p))
d515 1
a515 1
		if (intrflg && nfs_sigintr(nmp, NULL, p)) {
d542 1
a542 1
	int i;
d546 1
a546 1
	for (i = 0; i < NFS_MAXASYNCDAEMON; i++) {
d548 4
d553 1
a553 1
		nfs_iodwant[i] = NULL;
a556 1
	}
d558 19
a576 1
	return (EIO);
d592 1
a592 1
	int error = 0, diff, len, iomode, must_commit = 0;
d639 3
a641 1
		if (!error && uiop->uio_resid) {
d652 7
a658 3
				len = MIN(len, uiop->uio_resid);
				memset((char *)bp->b_data + diff, 0, len);
			}
d675 1
a675 1
	    }
d681 5
a685 3
	    io.iov_base = bp->b_data;
	    io.iov_len = uiop->uio_resid = bp->b_bcount;
	    uiop->uio_offset = ((off_t)bp->b_blkno) << DEV_BSHIFT;
d688 9
a696 1
	    iomode = NFSV3WRITE_UNSTABLE;
d698 33
a736 594
}

/*
 * Vnode op for VM getpages.
 */
int
nfs_getpages(v)
	void *v;
{
	struct vop_getpages_args /* {
		struct vnode *a_vp;
		voff_t a_offset;
		vm_page_t *a_m;
		int *a_count;
		int a_centeridx;
		vm_prot_t a_access_type;
		int a_advice;
		int a_flags;
	} */ *ap = v;

	off_t eof, offset, origoffset, startoffset, endoffset;
	int s, i, error, npages, orignpages, npgs, ridx, pidx, pcount;
	vaddr_t kva;
	struct buf *bp, *mbp;
	struct vnode *vp = ap->a_vp;
	struct nfsnode *np = VTONFS(vp);
	struct uvm_object *uobj = &vp->v_uobj;
	struct nfsmount *nmp = VFSTONFS(vp->v_mount);
	size_t bytes, iobytes, tailbytes, totalbytes, skipbytes;
	int flags = ap->a_flags;
	int bsize;
	struct vm_page *pgs[16];			/* XXXUBC 16 */
	boolean_t v3 = NFS_ISV3(vp);
	boolean_t async = (flags & PGO_SYNCIO) == 0;
	boolean_t write = (ap->a_access_type & VM_PROT_WRITE) != 0;
	struct proc *p = curproc;

	UVMHIST_FUNC("nfs_getpages"); UVMHIST_CALLED(ubchist);
	UVMHIST_LOG(ubchist, "vp %p off 0x%x count %d", vp, (int)ap->a_offset,
		    *ap->a_count,0);

#ifdef DIAGNOSTIC
	if (ap->a_centeridx < 0 || ap->a_centeridx >= *ap->a_count) {
		panic("nfs_getpages: centeridx %d out of range",
		      ap->a_centeridx);
	}
#endif

	error = 0;
	origoffset = ap->a_offset;
	eof = vp->v_size;
	if (origoffset >= eof) {
		if ((flags & PGO_LOCKED) == 0) {
			simple_unlock(&uobj->vmobjlock);
		}
		UVMHIST_LOG(ubchist, "off 0x%x past EOF 0x%x",
			    (int)origoffset, (int)eof,0,0);
		return EINVAL;
	}

	if (flags & PGO_LOCKED) {
		uvn_findpages(uobj, origoffset, ap->a_count, ap->a_m,
			      UFP_NOWAIT|UFP_NOALLOC);
		return 0;
	}

	/* vnode is VOP_LOCKed, uobj is locked */
	if (write && (vp->v_bioflag & VBIOONSYNCLIST) == 0) {
		vn_syncer_add_to_worklist(vp, syncdelay);
	}
	bsize = nmp->nm_rsize;
	orignpages = MIN(*ap->a_count,
			 round_page(eof - origoffset) >> PAGE_SHIFT);
	npages = orignpages;
	startoffset = origoffset & ~(bsize - 1);
	endoffset = round_page((origoffset + (npages << PAGE_SHIFT)
				+ bsize - 1) & ~(bsize - 1));
	endoffset = MIN(endoffset, round_page(eof));
	ridx = (origoffset - startoffset) >> PAGE_SHIFT;

	if (!async && !write) {
		int rapages = MAX(PAGE_SIZE, nmp->nm_rsize) >> PAGE_SHIFT;

		(void) VOP_GETPAGES(vp, endoffset, NULL, &rapages, 0,
				    VM_PROT_READ, 0, 0);
		simple_lock(&uobj->vmobjlock);
	}

	UVMHIST_LOG(ubchist, "npages %d offset 0x%x", npages,
		    (int)origoffset, 0,0);
	memset(pgs, 0, sizeof(pgs));
	uvn_findpages(uobj, origoffset, &npages, &pgs[ridx], UFP_ALL);

	if (flags & PGO_OVERWRITE) {
		UVMHIST_LOG(ubchist, "PGO_OVERWRITE",0,0,0,0);

		/* XXXUBC for now, zero the page if we allocated it */
		for (i = 0; i < npages; i++) {
			struct vm_page *pg = pgs[ridx + i];

			if (pg->flags & PG_FAKE) {
				uvm_pagezero(pg);
				pg->flags &= ~(PG_FAKE);
			}
		}
		npages += ridx;
		if (v3) {
			simple_unlock(&uobj->vmobjlock);
			goto uncommit;
		}
		goto out;
	}

	/*
	 * if the pages are already resident, just return them.
	 */

	for (i = 0; i < npages; i++) {
		struct vm_page *pg = pgs[ridx + i];

		if ((pg->flags & PG_FAKE) != 0 ||
		    ((ap->a_access_type & VM_PROT_WRITE) &&
		      (pg->flags & PG_RDONLY))) {
			break;
		}
	}
	if (i == npages) {
		UVMHIST_LOG(ubchist, "returning cached pages", 0,0,0,0);
		npages += ridx;
		goto out;
	}

	/*
	 * the page wasn't resident and we're not overwriting,
	 * so we're going to have to do some i/o.
	 * find any additional pages needed to cover the expanded range.
	 */

	if (startoffset != origoffset ||
	    startoffset + (npages << PAGE_SHIFT) != endoffset) {

		/*
		 * XXXUBC we need to avoid deadlocks caused by locking
		 * additional pages at lower offsets than pages we
		 * already have locked.  for now, unlock them all and
		 * start over.
		 */

		for (i = 0; i < npages; i++) {
			struct vm_page *pg = pgs[ridx + i];

			if (pg->flags & PG_FAKE) {
				pg->flags |= PG_RELEASED;
			}
		}
		uvm_page_unbusy(&pgs[ridx], npages);
		memset(pgs, 0, sizeof(pgs));

		UVMHIST_LOG(ubchist, "reset npages start 0x%x end 0x%x",
			    startoffset, endoffset, 0,0);
		npages = (endoffset - startoffset) >> PAGE_SHIFT;
		npgs = npages;
		uvn_findpages(uobj, startoffset, &npgs, pgs, UFP_ALL);
	}
	simple_unlock(&uobj->vmobjlock);

	/*
	 * update the cached read creds for this node.
	 */

	if (np->n_rcred) {
		crfree(np->n_rcred);
	}
	np->n_rcred = curproc->p_ucred;
	crhold(np->n_rcred);

	/*
	 * read the desired page(s).
	 */

	totalbytes = npages << PAGE_SHIFT;
	bytes = MIN(totalbytes, vp->v_size - startoffset);
	tailbytes = totalbytes - bytes;
	skipbytes = 0;

	kva = uvm_pagermapin(pgs, npages, UVMPAGER_MAPIN_WAITOK |
			     UVMPAGER_MAPIN_READ);

	s = splbio();
	mbp = pool_get(&bufpool, PR_WAITOK);
	splx(s);
	mbp->b_bufsize = totalbytes;
	mbp->b_data = (void *)kva;
	mbp->b_resid = mbp->b_bcount = bytes;
	mbp->b_flags = B_BUSY|B_READ| (async ? B_CALL|B_ASYNC : 0);
	mbp->b_iodone = uvm_aio_biodone;
	mbp->b_vp = NULL;
	mbp->b_proc = NULL;		/* XXXUBC */
	LIST_INIT(&mbp->b_dep);
	bgetvp(vp, mbp);

	/*
	 * if EOF is in the middle of the last page, zero the part past EOF.
	 */

	if (tailbytes > 0 && (pgs[bytes >> PAGE_SHIFT]->flags & PG_FAKE)) {
		memset((char *)kva + bytes, 0, tailbytes);
	}

	/*
	 * now loop over the pages, reading as needed.
	 */

	bp = NULL;
	for (offset = startoffset;
	     bytes > 0;
	     offset += iobytes, bytes -= iobytes) {

		/*
		 * skip pages which don't need to be read.
		 */

		pidx = (offset - startoffset) >> PAGE_SHIFT;
		UVMHIST_LOG(ubchist, "pidx %d offset 0x%x startoffset 0x%x",
			    pidx, (int)offset, (int)startoffset,0);
		while ((pgs[pidx]->flags & PG_FAKE) == 0) {
			size_t b;

			KASSERT((offset & (PAGE_SIZE - 1)) == 0);
			b = MIN(PAGE_SIZE, bytes);
			offset += b;
			bytes -= b;
			skipbytes += b;
			pidx++;
			UVMHIST_LOG(ubchist, "skipping, new offset 0x%x",
				    (int)offset, 0,0,0);
			if (bytes == 0) {
				goto loopdone;
			}
		}

		/*
		 * see how many pages can be read with this i/o.
		 * reduce the i/o size if necessary.
		 */

		iobytes = bytes;
		if (offset + iobytes > round_page(offset)) {
			pcount = 1;
			while (pidx + pcount < npages &&
			       pgs[pidx + pcount]->flags & PG_FAKE) {
				pcount++;
			}
			iobytes = MIN(iobytes, (pcount << PAGE_SHIFT) -
				      (offset - trunc_page(offset)));
		}
		iobytes = MIN(iobytes, nmp->nm_rsize);

		/*
		 * allocate a sub-buf for this piece of the i/o
		 * (or just use mbp if there's only 1 piece),
		 * and start it going.
		 */

		if (offset == startoffset && iobytes == bytes) {
			bp = mbp;
		} else {
			s = splbio();
			bp = pool_get(&bufpool, PR_WAITOK);
			splx(s);
			bp->b_data = (char *)kva + offset - startoffset;
			bp->b_resid = bp->b_bcount = iobytes;
			bp->b_flags = B_BUSY|B_READ|B_CALL|B_ASYNC;
			bp->b_iodone = uvm_aio_biodone1;
			bp->b_vp = vp;
			bp->b_proc = NULL;	/* XXXUBC */
			LIST_INIT(&bp->b_dep);
		}
		bp->b_private = mbp;
		bp->b_lblkno = bp->b_blkno = offset >> DEV_BSHIFT;

		UVMHIST_LOG(ubchist, "bp %p offset 0x%x bcount 0x%x blkno 0x%x",
			    bp, offset, iobytes, bp->b_blkno);

		VOP_STRATEGY(bp);
	}

loopdone:
	if (skipbytes) {
		s = splbio();
		mbp->b_resid -= skipbytes;
		if (mbp->b_resid == 0) {
			biodone(mbp);
		}
		splx(s);
	}
	if (async) {
		UVMHIST_LOG(ubchist, "returning 0 (async)",0,0,0,0);
		return 0;
	}
	if (bp != NULL) {
		error = biowait(mbp);
	}
	s = splbio();
	(void) buf_cleanout(mbp);
	pool_put(&bufpool, mbp);
	splx(s);
	uvm_pagermapout(kva, npages);
 
	if (write && v3) {
uncommit:
 		lockmgr(&np->n_commitlock, LK_EXCLUSIVE, NULL, p);
		nfs_del_committed_range(vp, origoffset, npages);
		nfs_del_tobecommitted_range(vp, origoffset, npages);
		simple_lock(&uobj->vmobjlock);
		for (i = 0; i < npages; i++) {
			if (pgs[i] == NULL) {
				continue;
			}
			pgs[i]->flags &= ~(PG_NEEDCOMMIT|PG_RDONLY);
		}
		simple_unlock(&uobj->vmobjlock);
 		lockmgr(&np->n_commitlock, LK_RELEASE, NULL, p);
	}

	simple_lock(&uobj->vmobjlock);

out:
	if (error) {
		uvm_lock_pageq();
		for (i = 0; i < npages; i++) {
			if (pgs[i] == NULL) {
				continue;
			}
			UVMHIST_LOG(ubchist, "examining pg %p flags 0x%x",
				    pgs[i], pgs[i]->flags, 0,0);
			if (pgs[i]->flags & PG_WANTED) {
				wakeup(pgs[i]);
			}
			if (pgs[i]->flags & PG_RELEASED) {
				uvm_unlock_pageq();
				(uobj->pgops->pgo_releasepg)(pgs[i], NULL);
				uvm_lock_pageq();
				continue;
			}
			if (pgs[i]->flags & PG_FAKE) {
				uvm_pagefree(pgs[i]);
				continue;
			}
			uvm_pageactivate(pgs[i]);
			pgs[i]->flags &= ~(PG_WANTED|PG_BUSY);
			UVM_PAGE_OWN(pgs[i], NULL);
		}
		uvm_unlock_pageq();
		simple_unlock(&uobj->vmobjlock);
		UVMHIST_LOG(ubchist, "returning error %d", error,0,0,0);
		return error;
	}

	UVMHIST_LOG(ubchist, "ridx %d count %d", ridx, npages, 0,0);
	uvm_lock_pageq();
	for (i = 0; i < npages; i++) {
		if (pgs[i] == NULL) {
			continue;
		}
		UVMHIST_LOG(ubchist, "examining pg %p flags 0x%x",
			    pgs[i], pgs[i]->flags, 0,0);
		if (pgs[i]->flags & PG_FAKE) {
			UVMHIST_LOG(ubchist, "unfaking pg %p offset 0x%x",
				    pgs[i], (int)pgs[i]->offset,0,0);
			pgs[i]->flags &= ~(PG_FAKE);
			pmap_clear_modify(pgs[i]);
			pmap_clear_reference(pgs[i]);
		}
		if (i < ridx || i >= ridx + orignpages || async) {
			UVMHIST_LOG(ubchist, "unbusy pg %p offset 0x%x",
				    pgs[i], (int)pgs[i]->offset,0,0);
			if (pgs[i]->flags & PG_WANTED) {
				wakeup(pgs[i]);
			}
			if (pgs[i]->flags & PG_RELEASED) {
				uvm_unlock_pageq();
				(uobj->pgops->pgo_releasepg)(pgs[i], NULL);
				uvm_lock_pageq();
				continue;
			}
			uvm_pageactivate(pgs[i]);
			pgs[i]->flags &= ~(PG_WANTED|PG_BUSY);
			UVM_PAGE_OWN(pgs[i], NULL);
		}
	}
	uvm_unlock_pageq();
	simple_unlock(&uobj->vmobjlock);
	if (ap->a_m != NULL) {
		memcpy(ap->a_m, &pgs[ridx],
		       *ap->a_count * sizeof(struct vm_page *));
	}
	return 0;
}

/*
 * Vnode op for VM putpages.
 */
int
nfs_putpages(v)
	void *v;
{
	struct vop_putpages_args /* {
		struct vnode *a_vp;
		struct vm_page **a_m;
		int a_count;
		int a_flags;
		int *a_rtvals;
	} */ *ap = v;

	struct vnode *vp = ap->a_vp;
	struct nfsnode *np = VTONFS(vp);
	struct nfsmount *nmp = VFSTONFS(vp->v_mount);
	struct buf *bp, *mbp;
	struct vm_page **pgs = ap->a_m;
	int flags = ap->a_flags;
	int npages = ap->a_count;
	int s, error, i;
	size_t bytes, iobytes, skipbytes;
	vaddr_t kva;
	off_t offset, origoffset, commitoff;
	uint32_t commitbytes;
	boolean_t v3 = NFS_ISV3(vp);
	boolean_t async = (flags & PGO_SYNCIO) == 0;
	boolean_t weak = (flags & PGO_WEAK) && v3;
	struct proc *p = curproc;
	UVMHIST_FUNC("nfs_putpages"); UVMHIST_CALLED(ubchist);

	UVMHIST_LOG(ubchist, "vp %p pgp %p count %d",
		    vp, ap->a_m, ap->a_count,0);

	simple_unlock(&vp->v_uobj.vmobjlock);

	error = 0;
	origoffset = pgs[0]->offset;
	bytes = MIN(ap->a_count << PAGE_SHIFT, vp->v_size - origoffset);
	skipbytes = 0;

	/*
	 * if the range has been committed already, mark the pages thus.
	 * if the range just needs to be committed, we're done
	 * if it's a weak putpage, otherwise commit the range.
	 */

	if (v3) {
 		lockmgr(&np->n_commitlock, LK_EXCLUSIVE, NULL, p);
		if (nfs_in_committed_range(vp, origoffset, bytes)) {
			goto committed;
		}
		if (nfs_in_tobecommitted_range(vp, origoffset, bytes)) {
			if (weak) {
				lockmgr(&np->n_commitlock, LK_RELEASE, NULL, p);
				return 0;
			} else {
				commitoff = np->n_pushlo;
				commitbytes = (uint32_t)(np->n_pushhi -
							 np->n_pushlo);
				goto commit;
			}
		}
		lockmgr(&np->n_commitlock, LK_RELEASE, NULL, p);
	}

	/*
	 * otherwise write or commit all the pages.
	 */

	kva = uvm_pagermapin(pgs, ap->a_count, UVMPAGER_MAPIN_WAITOK|
			     UVMPAGER_MAPIN_WRITE);

	s = splbio();
	vp->v_numoutput += 2;
	mbp = pool_get(&bufpool, PR_WAITOK);
	UVMHIST_LOG(ubchist, "vp %p mbp %p num now %d bytes 0x%x",
		    vp, mbp, vp->v_numoutput, bytes);
	splx(s);
	mbp->b_bufsize = npages << PAGE_SHIFT;
	mbp->b_data = (void *)kva;
	mbp->b_resid = mbp->b_bcount = bytes;
	mbp->b_flags = B_BUSY|B_WRITE|B_AGE |
		(async ? B_CALL|B_ASYNC : 0) |
		(curproc == uvm.pagedaemon_proc ? B_PDAEMON : 0);
	mbp->b_iodone = uvm_aio_biodone;
	mbp->b_vp = NULL;
	mbp->b_proc = NULL;		/* XXXUBC */
	LIST_INIT(&mbp->b_dep);
	bgetvp(vp, mbp);

	for (offset = origoffset;
	     bytes > 0;
	     offset += iobytes, bytes -= iobytes) {
		iobytes = MIN(nmp->nm_wsize, bytes);

 		/*
		 * skip writing any pages which only need a commit.
		 */

		if ((pgs[(offset - origoffset) >> PAGE_SHIFT]->flags &
		     PG_NEEDCOMMIT) != 0) {
			KASSERT((offset & (PAGE_SIZE - 1)) == 0);
			iobytes = MIN(PAGE_SIZE, bytes);
			skipbytes += iobytes;
			continue;
		}

		/* if it's really one i/o, don't make a second buf */
		if (offset == origoffset && iobytes == bytes) {
			bp = mbp;
		} else {
			s = splbio();
			vp->v_numoutput++;
			bp = pool_get(&bufpool, PR_WAITOK);
			UVMHIST_LOG(ubchist, "vp %p bp %p num now %d",
				    vp, bp, vp->v_numoutput, 0);
			splx(s);
			bp->b_data = (char *)kva + (offset - origoffset);
			bp->b_resid = bp->b_bcount = iobytes;
			bp->b_flags = B_BUSY|B_WRITE|B_CALL|B_ASYNC;
			bp->b_iodone = uvm_aio_biodone1;
			bp->b_vp = vp;
			bp->b_proc = NULL;	/* XXXUBC */
			LIST_INIT(&bp->b_dep);
		}
		bp->b_private = mbp;
		bp->b_lblkno = bp->b_blkno = (daddr_t)(offset >> DEV_BSHIFT);
		UVMHIST_LOG(ubchist, "bp %p numout %d",
			    bp, vp->v_numoutput,0,0);
		VOP_STRATEGY(bp);
	}
	if (skipbytes) {
		UVMHIST_LOG(ubchist, "skipbytes %d", bytes, 0,0,0);
		s = splbio();
		mbp->b_resid -= skipbytes;
		if (mbp->b_resid == 0) {
			biodone(mbp);
		}
		splx(s);
	}
	if (async) {
		return 0;
	}
	if (bp != NULL) {
		error = biowait(mbp);
	}

	s = splbio();
	if (mbp->b_vp) {
		vwakeup(mbp->b_vp);
	}
	(void) buf_cleanout(mbp);
	pool_put(&bufpool, mbp);
	splx(s);

	uvm_pagermapout(kva, ap->a_count);
	if (error || !v3) {
		UVMHIST_LOG(ubchist, "returning error %d", error, 0,0,0);
		return error;
	}

	/*
	 * for a weak put, mark the range as "to be committed"
	 * and mark the pages read-only so that we will be notified
	 * to remove the pages from the "to be committed" range
	 * if they are made dirty again.
	 * for a strong put, commit the pages and remove them from the
	 * "to be committed" range.  also, mark them as writable
	 * and not cleanable with just a commit.
	 */

	lockmgr(&np->n_commitlock, LK_EXCLUSIVE, NULL, p);
	if (weak) {
		nfs_add_tobecommitted_range(vp, origoffset,
					    npages << PAGE_SHIFT);
		for (i = 0; i < npages; i++) {
			pgs[i]->flags |= PG_NEEDCOMMIT|PG_RDONLY;
		}
	} else {
		commitoff = origoffset;
		commitbytes = npages << PAGE_SHIFT;
commit:
		error = nfs_commit(vp, commitoff, commitbytes, curproc);
		nfs_del_tobecommitted_range(vp, commitoff, commitbytes);
committed:
		for (i = 0; i < npages; i++) {
			pgs[i]->flags &= ~(PG_NEEDCOMMIT|PG_RDONLY);
		}
	}
	lockmgr(&np->n_commitlock, LK_RELEASE, NULL, p);
	return error;
@


1.32.2.1
log
@Merge in -current, builds on i386, otherwise untested
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.34 2002/01/16 21:51:16 ericj Exp $	*/
d362 1
a362 1
	struct buf *bp;
d389 1
a389 1
	struct nfsnode *np = VTONFS(vp);
d471 2
a472 2
	struct uio *uiop;
	struct vnode *vp;
@


1.32.2.2
log
@Merge in UBC performance changes from NetBSD.
Fix a bunch of merge errors from yesterday.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.32.2.1 2002/01/31 22:55:47 niklas Exp $	*/
d141 1
a141 1
	 * update the cached read creds for this vnode.
d143 1
a143 1
	if (np->n_rcred){
a144 1
	}
a155 1

a156 3
		if (uio->uio_offset >= np->n_size) {
			break;
		}
d230 1
a238 3
	void *win;
	voff_t oldoff, origoff;
	vsize_t bytelen;
d240 1
d286 1
a286 1
	 * update the cached write creds for this node.
d288 1
a288 1
	if (np->n_wcred) {
a289 1
	}
d293 6
a298 1
	origoff = uio->uio_offset;
d300 4
a303 1
		oldoff = uio->uio_offset;
d307 1
a307 1
		*((volatile vsize_t *)&bytelen) = uio->uio_resid;
a309 1

d313 1
d315 2
a316 8
		if ((uio->uio_offset & PAGE_MASK) == 0 &&
		    ((uio->uio_offset + bytelen) & PAGE_MASK) == 0) {
			win = ubc_alloc(&vp->v_uobj, uio->uio_offset, &bytelen,
			    UBC_WRITE | UBC_FAULTBUSY);
		} else {
			win = ubc_alloc(&vp->v_uobj, uio->uio_offset, &bytelen,
			    UBC_WRITE);
		}
d319 22
a343 19

		/*
		 * update UVM's notion of the size now that we've
		 * copied the data into the vnode's pages.
		 */

		if (vp->v_size < uio->uio_offset) {
			uvm_vnp_setsize(vp, uio->uio_offset);
		}

		if ((oldoff & ~(nmp->nm_wsize - 1)) !=
		    (uio->uio_offset & ~(nmp->nm_wsize - 1))) {
			simple_lock(&vp->v_interlock);
			error = VOP_PUTPAGES(vp,
			    trunc_page(oldoff & ~(nmp->nm_wsize - 1)),
			    round_page((uio->uio_offset + nmp->nm_wsize - 1) &
				       ~(nmp->nm_wsize - 1)),
			    PGO_CLEANIT | PGO_WEAK);
		}
d345 1
a345 9
	if ((ioflag & IO_SYNC)) {
		simple_lock(&vp->v_interlock);
		error = VOP_PUTPAGES(vp,
		    trunc_page(origoff & ~(nmp->nm_wsize - 1)),
		    round_page((uio->uio_offset + nmp->nm_wsize - 1) &
			       ~(nmp->nm_wsize - 1)),
		    PGO_CLEANIT | PGO_SYNCIO);
	}
	return error;
d591 4
d596 1
a596 1
#if defined(LOCKDEBUG) || defined(MULTIPROCESSOR)
d598 5
a602 6
#endif
	struct nfsnode *np = VTONFS(vp);
	struct vm_page *pg, **pgs;
	struct proc *p = curproc;
	off_t origoffset;
	int i, error, npages;
d604 1
d606 2
d609 127
d748 1
a748 1
	 * call the genfs code to get the pages.
d751 27
a777 4
	npages = *ap->a_count;
	error = genfs_getpages(v);
	if (error || !write || !v3) {
		return error;
d781 1
a781 1
	 * this is a write fault, update the commit info.
d784 111
a894 2
	origoffset = ap->a_offset;
	pgs = ap->a_m;
a895 3
	lockmgr(&np->n_commitlock, LK_EXCLUSIVE, NULL, p);
	nfs_del_committed_range(vp, origoffset, npages);
	nfs_del_tobecommitted_range(vp, origoffset, npages);
d897 35
d933 1
a933 2
		pg = pgs[i];
		if (pg == NULL || pg == PGO_DONTCARE) {
d936 25
a960 1
		pg->flags &= ~(PG_NEEDCOMMIT|PG_RDONLY);
d962 1
d964 4
a967 1
	lockmgr(&np->n_commitlock, LK_RELEASE, NULL, p);
d971 3
d975 2
a976 1
nfs_gop_write(struct vnode *vp, struct vm_page **pgs, int npages, int flags)
d978 9
a986 3
#if defined(LOCKDEBUG) || defined(MULTIPROCESSOR)
	struct uvm_object *uobj = &vp->v_uobj;
#endif
d988 9
a996 2
	struct proc *p = curproc;
	off_t origoffset, commitoff;
a997 2
	int error, i;
	int bytes;
d999 7
a1005 2
	boolean_t weak = flags & PGO_WEAK;
	UVMHIST_FUNC("nfs_gop_write"); UVMHIST_CALLED(ubchist);
d1007 6
a1012 2
	/* XXX for now, skip the v3 stuff. */
	v3 = FALSE;
d1015 3
a1017 1
	 * for NFSv2, just write normally.
d1020 17
a1036 2
	if (!v3) {
		return genfs_gop_write(vp, pgs, npages, flags);
d1040 1
a1040 2
	 * for NFSv3, use delayed writes and the "commit" operation
	 * to avoid sync writes.
d1043 41
a1083 10
	origoffset = pgs[0]->offset;
	bytes = npages << PAGE_SHIFT;
	lockmgr(&np->n_commitlock, LK_EXCLUSIVE, NULL, p);
	if (nfs_in_committed_range(vp, origoffset, bytes)) {
		goto committed;
	}
	if (nfs_in_tobecommitted_range(vp, origoffset, bytes)) {
		if (weak) {
			lockmgr(&np->n_commitlock, LK_RELEASE, NULL, p);
			return 0;
d1085 26
a1110 3
			commitoff = np->n_pushlo;
			commitbytes = (uint32_t)(np->n_pushhi - np->n_pushlo);
			goto commit;
d1112 7
a1118 3
	} else {
		commitoff = origoffset;
		commitbytes = npages << PAGE_SHIFT;
d1120 4
a1123 4
	simple_lock(&uobj->vmobjlock);
	for (i = 0; i < npages; i++) {
		pgs[i]->flags |= PG_NEEDCOMMIT|PG_RDONLY;
		pgs[i]->flags &= ~PG_CLEAN;
d1125 7
a1131 4
	simple_unlock(&uobj->vmobjlock);
	lockmgr(&np->n_commitlock, LK_RELEASE, NULL, p);
	error = genfs_gop_write(vp, pgs, npages, flags);
	if (error) {
d1134 11
d1148 4
a1151 1
		    npages << PAGE_SHIFT);
d1153 2
d1156 1
a1156 1
		error = nfs_commit(vp, commitoff, commitbytes, p);
a1158 1
		simple_lock(&uobj->vmobjlock);
a1161 1
		simple_unlock(&uobj->vmobjlock);
@


1.32.2.3
log
@Sync UBC branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.32.2.2 2002/02/02 03:28:26 art Exp $	*/
d116 8
a123 12
	if (np->n_flag & NMODIFIED) {
		np->n_attrstamp = 0;
		error = VOP_GETATTR(vp, &vattr, cred, p);
		if (error)
			return (error);
		np->n_mtime = vattr.va_mtime.tv_sec;
	} else {
		error = VOP_GETATTR(vp, &vattr, cred, p);
		if (error)
			return (error);
		if (np->n_mtime != vattr.va_mtime.tv_sec) {
			error = nfs_vinvalbuf(vp, V_SAVE, cred, p, 1);
d127 10
a487 1
	int s;
a578 1
	s = splbio();
a579 1
	splx(s);
@


1.32.2.4
log
@Huge sync to NetBSD plus lots of bugfixes.
 - uvm is as in netbsd-current minus uvm_map forward merge.
 - various locking bugfixes in nfs.
 - make sure that all specops and fifoops are correct in all vnodeop vectors.
 - make the filesystem code more like filsystem code and less like vm code.
@
text
@d1 2
a2 2
/*	$OpenBSD: nfs_bio.c,v 1.32.2.3 2002/06/11 03:32:03 art Exp $	*/
/*	$NetBSD: nfs_bio.c,v 1.81 2002/05/06 03:20:54 enami Exp $	*/
a65 1
extern int nfs_commitsize;
a240 1
	int extended = 0;
a295 3
		boolean_t extending; /* if we are extending whole pages */
		u_quad_t oldsize;

a303 1
		oldsize = np->n_size;
d308 2
a309 4
		extending = ((uio->uio_offset & PAGE_MASK) == 0 &&
		    (bytelen & PAGE_MASK) == 0 &&
		    uio->uio_offset >= vp->v_size);
		if (extending) {
a318 8
			if (extending) {
				/*
				 * backout size and free pages past eof.
				 */
				np->n_size = oldsize;
				(void)VOP_PUTPAGES(vp, round_page(vp->v_size),
				    0, PGO_SYNCIO | PGO_FREE);
			}
a328 1
			extended = 1;
d337 2
a338 1
				       ~(nmp->nm_wsize - 1)), PGO_CLEANIT);
a479 1
	int pushedrange;
a481 2
	off_t off, cnt;
	struct uvm_object *uobj;
a484 1
	uobj = &vp->v_uobj;
a562 77
	    int i, npages = bp->b_bufsize >> PAGE_SHIFT;
	    struct vm_page *pgs[npages];
	    boolean_t needcommit = TRUE;

	    if ((bp->b_flags & B_ASYNC) != 0 && NFS_ISV3(vp)) {
		    iomode = NFSV3WRITE_UNSTABLE;
	    } else {
		    iomode = NFSV3WRITE_FILESYNC;
	    }

	    for (i = 0; i < npages; i++) {
		    pgs[i] = uvm_pageratop((vaddr_t)bp->b_data +
					   (i << PAGE_SHIFT));
		    if ((pgs[i]->flags & PG_NEEDCOMMIT) == 0) {
			    needcommit = FALSE;
		    }
	    }
	    if (!needcommit && iomode == NFSV3WRITE_UNSTABLE) {
		    for (i = 0; i < npages; i++) {
			    pgs[i]->flags |= PG_NEEDCOMMIT | PG_RDONLY;
			    pmap_page_protect(pgs[i], VM_PROT_READ);
		    }
	    }

	    uiop->uio_offset = (((off_t)bp->b_blkno) << DEV_BSHIFT);
	    off = uiop->uio_offset;
	    cnt = bp->b_bcount;

	    /*
	     * Send the data to the server if necessary,
	     * otherwise just send a commit rpc.
	     */

	    if (needcommit) {

		/*
		 * If the buffer is in the range that we already committed,
		 * there's nothing to do.
		 *
		 * If it's in the range that we need to commit, push the
		 * whole range at once, otherwise only push the buffer.
		 * In both these cases, acquire the commit lock to avoid
		 * other processes modifying the range.
		 */

		lockmgr(&np->n_commitlock, LK_EXCLUSIVE, NULL, p);
		if (!nfs_in_committed_range(vp, off, bp->b_bcount)) {
			if (nfs_in_tobecommitted_range(vp, off, bp->b_bcount)) {
				pushedrange = 1;
				off = np->n_pushlo;
				cnt = np->n_pushhi - np->n_pushlo;
			} else {
				pushedrange = 0;
			}
			error = nfs_commit(vp, off, cnt, curproc);
			if (error == 0) {
				if (pushedrange) {
					nfs_merge_commit_ranges(vp);
				} else {
					nfs_add_committed_range(vp, off, cnt);
				}
			}
		}
		lockmgr(&np->n_commitlock, LK_RELEASE, NULL, p);
		if (!error) {
			bp->b_resid = 0;
			simple_lock(&uobj->vmobjlock);
			for (i = 0; i < npages; i++) {
				pgs[i]->flags &= ~(PG_NEEDCOMMIT | PG_RDONLY);
			}
			simple_unlock(&uobj->vmobjlock);
			biodone(bp);
			return (0);
		} else if (error == NFSERR_STALEWRITEVERF) {
			nfs_clearcommit(bp->b_vp->v_mount);
		}
	    }
d565 1
d568 1
a569 34
	    if (!error && iomode == NFSV3WRITE_UNSTABLE) {
		lockmgr(&np->n_commitlock, LK_EXCLUSIVE, NULL, p);
		nfs_add_tobecommitted_range(vp, off, cnt);
		simple_lock(&uobj->vmobjlock);
		for (i = 0; i < npages; i++) {
			pgs[i]->flags &= ~PG_CLEAN;
		}
		simple_unlock(&uobj->vmobjlock);
		if (np->n_pushhi - np->n_pushlo > nfs_commitsize) {
			off = np->n_pushlo;
			cnt = nfs_commitsize >> 1;
			error = nfs_commit(vp, off, cnt, curproc);
			if (!error) {
				nfs_add_committed_range(vp, off, cnt);
				nfs_del_tobecommitted_range(vp, off, cnt);
			}
		}
		lockmgr(&np->n_commitlock, LK_RELEASE, NULL, p);
	    } else if (!error && needcommit) {
		lockmgr(&np->n_commitlock, LK_EXCLUSIVE, NULL, p);
		nfs_del_committed_range(vp, off, cnt);
		lockmgr(&np->n_commitlock, LK_RELEASE, NULL, p);
		simple_lock(&uobj->vmobjlock);
		for (i = 0; i < npages; i++) {
			pgs[i]->flags &= ~(PG_NEEDCOMMIT | PG_RDONLY);
		}
		simple_unlock(&uobj->vmobjlock);
	    } else {
		if (error) {
			bp->b_flags |= B_ERROR;
			bp->b_error = np->n_error = error;
			np->n_flag |= NWRITEERR;
		}
	    }
d572 1
a572 1
	if (must_commit || (error == NFSERR_STALEWRITEVERF)) {
a573 1
	}
d603 1
a603 2
	const int npages = *ap->a_count;
	struct vm_page *pg, **pgs, *opgs[npages];
d605 2
a606 2
	off_t origoffset, len;
	int i, error;
d609 1
a609 1
	boolean_t locked = (ap->a_flags & PGO_LOCKED) != 0;
d622 1
a622 1
	 * if we have delayed truncation and it's safe, do it now.
d625 4
a628 2
	if (ap->a_flags & PGO_SYNCIO) {
		nfs_delayedtruncate(vp);
d632 1
a632 2
	 * call the genfs code to get the pages.  `pgs' may be NULL
	 * when doing read-ahead.
d635 1
a636 3
	if (write && locked && v3) {
		KASSERT(pgs != NULL);
#ifdef DEBUG
d638 15
a652 4
		/*
		 * If PGO_LOCKED is set, real pages shouldn't exists
		 * in the array.
		 */
d654 5
a658 2
		for (i = 0; i < npages; i++)
			KDASSERT(pgs[i] == NULL || pgs[i] == PGO_DONTCARE);
d660 12
a671 6
		memcpy(opgs, pgs, npages * sizeof(struct vm_pages *));
	}
	error = genfs_getpages(v);
	if (error) {
		return (error);
	}
d674 1
a674 4
	 * for read faults where the nfs node is not yet marked NMODIFIED,
	 * set PG_RDONLY on the pages so that we come back here if someone
	 * tries to modify later via the mapping that will be entered for
	 * this fault.
d677 2
a678 17
	if (!write && (np->n_flag & NMODIFIED) == 0 && pgs != NULL) {
		if (!locked) {
			simple_lock(&uobj->vmobjlock);
		}
		for (i = 0; i < npages; i++) {
			pg = pgs[i];
			if (pg == NULL || pg == PGO_DONTCARE) {
				continue;
			}
			pg->flags |= PG_RDONLY;
		}
		if (!locked) {
			simple_unlock(&uobj->vmobjlock);
		}
	}
	if (!write) {
		return (0);
d682 2
a683 1
	 * this is a write fault, update the commit info.
d686 14
a699 23
	origoffset = ap->a_offset;
	len = npages << PAGE_SHIFT;

	if (v3) {
		error = lockmgr(&np->n_commitlock,
		    LK_EXCLUSIVE | (locked ? LK_NOWAIT : 0), NULL, p);
		if (error) {
			KASSERT(locked != 0);

			/*
			 * Since PGO_LOCKED is set, we need to unbusy
			 * all pages fetched by genfs_getpages() above,
			 * tell the caller that there are no pages
			 * available and put back original pgs array.
			 */

			uvm_lock_pageq();
			uvm_page_unbusy(pgs, npages);
			uvm_unlock_pageq();
			*ap->a_count = 0;
			memcpy(pgs, opgs,
			    npages * sizeof(struct vm_pages *));
			return (error);
d701 14
a714 2
		nfs_del_committed_range(vp, origoffset, len);
		nfs_del_tobecommitted_range(vp, origoffset, len);
d716 9
a724 2
	np->n_flag |= NMODIFIED;
	if (!locked) {
d726 2
a727 5
	}
	for (i = 0; i < npages; i++) {
		pg = pgs[i];
		if (pg == NULL || pg == PGO_DONTCARE) {
			continue;
a728 3
		pg->flags &= ~(PG_NEEDCOMMIT | PG_RDONLY);
	}
	if (!locked) {
d731 2
a732 4
	if (v3) {
		lockmgr(&np->n_commitlock, LK_RELEASE, NULL, p);
	}
	return (0);
@


1.32.2.5
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a67 1
struct nfs_bufqhead nfs_bufq;
@


1.31
log
@Merge in struct uvm_vnode into struct vnode.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.30 2001/11/30 18:58:18 art Exp $	*/
d302 6
a307 1
		vsize_t bytelen = uio->uio_resid;
@


1.30
log
@Whooops.
Stop returning EINPROGRESS now that the caller doesn't understand it
anymore.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.29 2001/11/30 05:45:33 csapuntz Exp $	*/
d164 1
a164 1
			win = ubc_alloc(&vp->v_uvm.u_obj, uio->uio_offset,
d310 1
a310 1
		win = ubc_alloc(&vp->v_uvm.u_obj, uio->uio_offset, &bytelen,
d316 3
a318 3
			simple_lock(&vp->v_uvm.u_obj.vmobjlock);
			rv = vp->v_uvm.u_obj.pgops->pgo_flush(
			    &vp->v_uvm.u_obj,
d322 1
a322 1
			simple_unlock(&vp->v_uvm.u_obj.vmobjlock);
d325 3
a327 3
			simple_lock(&vp->v_uvm.u_obj.vmobjlock);
			rv = vp->v_uvm.u_obj.pgops->pgo_flush(
			    &vp->v_uvm.u_obj,
d331 1
a331 1
			simple_unlock(&vp->v_uvm.u_obj.vmobjlock);
d592 1
a592 1
	struct uvm_object *uobj = &vp->v_uvm.u_obj;
d616 1
a616 1
	eof = vp->v_uvm.u_size;
d747 1
a747 1
	bytes = MIN(totalbytes, vp->v_uvm.u_size - startoffset);
d1002 1
a1002 1
	simple_unlock(&vp->v_uvm.u_obj.vmobjlock);
d1006 1
a1006 1
	bytes = MIN(ap->a_count << PAGE_SHIFT, vp->v_uvm.u_size - origoffset);
@


1.29
log
@Call buf_cleanout, which handles wakeups
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.28 2001/11/29 12:24:28 art Exp $	*/
d863 2
a864 2
		UVMHIST_LOG(ubchist, "returning PEND",0,0,0,0);
		return EINPROGRESS;
d1110 1
a1110 1
		return EINPROGRESS;
@


1.28
log
@Make sure the nfs vnodes are on the syncer worklist.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.27 2001/11/29 02:08:22 art Exp $	*/
d870 1
a870 3
	if (mbp->b_vp != NULL) {
		brelvp(mbp);
	}
a1118 1
		brelvp(mbp);
d1120 1
@


1.27
log
@Make sure the whole buffer is initialized before calling bgetvp.
Recommended by csapuntz@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.26 2001/11/29 01:59:19 art Exp $	*/
d633 3
a635 1

@


1.26
log
@Correctly handle b_vp with bgetvp and brelvp in {get,put}pages.
Prevents panics caused by vnodes being recycled under our feet.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.25 2001/11/27 05:27:12 art Exp $	*/
a760 1
	bgetvp(vp, mbp);
d763 1
a1054 1
	bgetvp(vp, mbp);
d1057 1
@


1.25
log
@Merge in the unified buffer cache code as found in NetBSD 2001/03/10. The
code is written mostly by Chuck Silvers <chuq@@chuq.com>/<chs@@netbsd.org>.

Tested for the past few weeks by many developers, should be in a pretty stable
state, but will require optimizations and additional cleanups.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.24 2001/11/15 23:15:15 art Exp $	*/
d760 2
a761 1
	mbp->b_vp = vp;
d868 3
d1054 2
a1055 1
	mbp->b_vp = vp;
d1117 1
a1117 1
	if (mbp->b_vp)
d1119 2
@


1.24
log
@Remove creds from struct buf, move the creds that nfs need into the nfs node.
While in the area, convert nfs node allocation from malloc to pool and do
some cleanups.
Based on the UBC changes in NetBSD. niklas@@ ok.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.23 2001/11/06 19:53:21 miod Exp $	*/
d53 1
d55 1
a55 1
#include <uvm/uvm_extern.h>
d74 2
a75 2
	register struct vnode *vp;
	register struct uio *uio;
d79 3
a81 3
	register struct nfsnode *np = VTONFS(vp);
	register int biosize, diff;
	struct buf *bp = NULL, *rabp;
a84 1
	daddr_t lbn, bn, rabn;
d86 1
a86 1
	int got_buf = 0, nra, error = 0, n = 0, on = 0, not_readin;
d156 14
a169 24
		lbn = uio->uio_offset / biosize;
		on = uio->uio_offset & (biosize - 1);
		bn = lbn * (biosize / DEV_BSIZE);
		not_readin = 1;

		/*
		 * Start the read ahead(s), as required.
		 */
		if (nfs_numasync > 0 && nmp->nm_readahead > 0) {
		    for (nra = 0; nra < nmp->nm_readahead &&
			(lbn + 1 + nra) * biosize < np->n_size; nra++) {
			rabn = (lbn + 1 + nra) * (biosize / DEV_BSIZE);
			if (!incore(vp, rabn)) {
			    rabp = nfs_getcacheblk(vp, rabn, biosize, p);
			    if (!rabp)
				return (EINTR);
			    if ((rabp->b_flags & (B_DELWRI | B_DONE)) == 0) {
				rabp->b_flags |= (B_READ | B_ASYNC);
				if (nfs_asyncio(rabp)) {
				    rabp->b_flags |= B_INVAL;
				    brelse(rabp);
				}
			    } else
				brelse(rabp);
a170 1
		    }
d172 2
a174 53
		/*
		 * If the block is in the cache and has the required data
		 * in a valid region, just copy it out.
		 * Otherwise, get the block and write back/read in,
		 * as required.
		 */
		if ((bp = incore(vp, bn)) &&
		    (bp->b_flags & (B_BUSY | B_WRITEINPROG)) ==
		    (B_BUSY | B_WRITEINPROG))
			got_buf = 0;
		else {
again:
			bp = nfs_getcacheblk(vp, bn, biosize, p);
			if (!bp)
				return (EINTR);
			got_buf = 1;
			if ((bp->b_flags & (B_DONE | B_DELWRI)) == 0) {
				bp->b_flags |= B_READ;
				not_readin = 0;
				error = nfs_doio(bp, p);
				if (error) {
				    brelse(bp);
				    return (error);
				}
			}
		}
		n = min((unsigned)(biosize - on), uio->uio_resid);
		diff = np->n_size - uio->uio_offset;
		if (diff < n)
			n = diff;
		if (not_readin && n > 0) {
			if (on < bp->b_validoff || (on + n) > bp->b_validend) {
				if (!got_buf) {
				    bp = nfs_getcacheblk(vp, bn, biosize, p);
				    if (!bp)
					return (EINTR);
				    got_buf = 1;
				}
				bp->b_flags |= B_INVAFTERWRITE;
				if (bp->b_dirtyend > 0) {
				    if ((bp->b_flags & B_DELWRI) == 0)
					panic("nfsbioread");
				    if (VOP_BWRITE(bp) == EINTR)
					return (EINTR);
				} else
				    brelse(bp);
				goto again;
			}
		}
		diff = (on >= bp->b_validend) ? 0 : (bp->b_validend - on);
		if (diff < n)
			n = diff;
		break;
d188 1
a188 1
		n = min(uio->uio_resid, NFS_MAXPATHLEN - bp->b_resid);
d230 2
a231 2
	register int biosize;
	register struct uio *uio = ap->a_uio;
d233 1
a233 1
	register struct vnode *vp = ap->a_vp;
d235 1
a235 1
	register struct ucred *cred = ap->a_cred;
a236 1
	struct buf *bp;
d239 2
a240 2
	daddr_t lbn, bn;
	int n, on, error = 0;
d300 3
a302 5

		/*
		 * XXX make sure we aren't cached in the VM page cache
		 */
		uvm_vnp_uncache(vp);
a304 8
		lbn = uio->uio_offset / biosize;
		on = uio->uio_offset & (biosize-1);
		n = min((unsigned)(biosize - on), uio->uio_resid);
		bn = lbn * (biosize / DEV_BSIZE);
again:
		bp = nfs_getcacheblk(vp, bn, biosize, p);
		if (!bp)
			return (EINTR);
d306 3
a308 3
		if (uio->uio_offset + n > np->n_size) {
			np->n_size = uio->uio_offset + n;
			uvm_vnp_setsize(vp, (u_long)np->n_size);
d310 25
a334 12

		/*
		 * If the new write will leave a contiguous dirty
		 * area, just update the b_dirtyoff and b_dirtyend,
		 * otherwise force a write rpc of the old dirty area.
		 */
		if (bp->b_dirtyend > 0 &&
		    (on > bp->b_dirtyend || (on + n) < bp->b_dirtyoff)) {
			bp->b_proc = p;
			if (VOP_BWRITE(bp) == EINTR)
				return (EINTR);
			goto again;
a335 2

		error = uiomove((char *)bp->b_data + on, n, uio);
d337 1
a337 10
			bp->b_flags |= B_ERROR;
			brelse(bp);
			return (error);
		}
		if (bp->b_dirtyend > 0) {
			bp->b_dirtyoff = min(on, bp->b_dirtyoff);
			bp->b_dirtyend = max((on + n), bp->b_dirtyend);
		} else {
			bp->b_dirtyoff = on;
			bp->b_dirtyend = on + n;
d339 2
a340 32
		if (bp->b_validend == 0 || bp->b_validend < bp->b_dirtyoff ||
		    bp->b_validoff > bp->b_dirtyend) {
			bp->b_validoff = bp->b_dirtyoff;
			bp->b_validend = bp->b_dirtyend;
		} else {
			bp->b_validoff = min(bp->b_validoff, bp->b_dirtyoff);
			bp->b_validend = max(bp->b_validend, bp->b_dirtyend);
		}

		/*
		 * Since this block is being modified, it must be written
		 * again and not just committed.
		 */
		bp->b_flags &= ~B_NEEDCOMMIT;

		/*
		 * If the lease is non-cachable or IO_SYNC do bwrite().
		 */
		if (ioflag & IO_SYNC) {
			bp->b_proc = p;
			error = VOP_BWRITE(bp);
			if (error)
				return (error);
		} else if ((n + on) == biosize) {
			bp->b_proc = (struct proc *)0;
			bp->b_flags |= B_ASYNC;
			(void)nfs_writebp(bp, 0);
		} else {
			bdwrite(bp);
		}
	} while (uio->uio_resid > 0 && n > 0);
	return (0);
d362 3
a364 3
		while (bp == (struct buf *)0) {
			if (nfs_sigintr(nmp, (struct nfsreq *)0, p))
				return ((struct buf *)0);
d404 1
a404 1
		if (error && intrflg && nfs_sigintr(nmp, (struct nfsreq *)0, p))
d414 1
a414 1
		if (intrflg && nfs_sigintr(nmp, (struct nfsreq *)0, p)) {
d441 1
a441 1
	int i,s;
d445 1
a445 1
	for (i = 0; i < NFS_MAXASYNCDAEMON; i++)
a446 4
		if ((bp->b_flags & B_READ) == 0) {
			bp->b_flags |= B_WRITEINPROG;
		}
	
d448 1
a448 1
		nfs_iodwant[i] = (struct proc *)0;
d452 1
d454 1
a454 19
	/*
	 * If it is a read or a write already marked B_WRITEINPROG or B_NOCACHE
	 * return EIO so the process will call nfs_doio() and do it
	 * synchronously.
	 */
	if (bp->b_flags & (B_READ | B_WRITEINPROG | B_NOCACHE))
		return (EIO);

	/*
	 * Just turn the async write into a delayed write, instead of
	 * doing in synchronously. Hopefully, at least one of the nfsiods
	 * is currently doing a write for this file and will pick up the
	 * delayed writes before going back to sleep.
	 */
	s = splbio();
	buf_dirty(bp);
	splx(s);
	biodone(bp);
	return (0);
d470 1
a470 1
	int s, error = 0, diff, len, iomode, must_commit = 0;
d517 1
a517 3
		if (!error) {
		    bp->b_validoff = 0;
		    if (uiop->uio_resid) {
d528 3
a530 7
			    len = min(len, uiop->uio_resid);
			    bzero((char *)bp->b_data + diff, len);
			    bp->b_validend = diff + len;
			} else
			    bp->b_validend = diff;
		    } else
			bp->b_validend = bp->b_bcount;
d547 1
a547 1
	    };
d553 3
a555 5
	    io.iov_len = uiop->uio_resid = bp->b_dirtyend
		- bp->b_dirtyoff;
	    uiop->uio_offset = ((off_t)bp->b_blkno) * DEV_BSIZE
		+ bp->b_dirtyoff;
	    io.iov_base = (char *)bp->b_data + bp->b_dirtyoff;
d558 1
a558 9
	    if ((bp->b_flags & (B_ASYNC | B_NEEDCOMMIT | B_NOCACHE)) == B_ASYNC)
		iomode = NFSV3WRITE_UNSTABLE;
	    else
		iomode = NFSV3WRITE_FILESYNC;
	    bp->b_flags |= B_WRITEINPROG;
#ifdef fvdl_debug
	    printf("nfs_doio(%x): bp %x doff %d dend %d\n", 
		vp, bp, bp->b_dirtyoff, bp->b_dirtyend);
#endif
a559 33
	    if (!error && iomode == NFSV3WRITE_UNSTABLE)
		bp->b_flags |= B_NEEDCOMMIT;
	    else
		bp->b_flags &= ~B_NEEDCOMMIT;
	    bp->b_flags &= ~B_WRITEINPROG;

	    /*
	     * For an interrupted write, the buffer is still valid and the
	     * write hasn't been pushed to the server yet, so we can't set
	     * B_ERROR and report the interruption by setting B_EINTR. For
	     * the B_ASYNC case, B_EINTR is not relevant, so the rpc attempt
	     * is essentially a noop.
	     * For the case of a V3 write rpc not being committed to stable
	     * storage, the block is still dirty and requires either a commit
	     * rpc or another write rpc with iomode == NFSV3WRITE_FILESYNC
	     * before the block is reused. This is indicated by setting the
	     * B_DELWRI and B_NEEDCOMMIT flags.
	     */
	    if (error == EINTR || (!error && (bp->b_flags & B_NEEDCOMMIT))) {
		    s = splbio();
		    buf_dirty(bp);
		    splx(s);

		    if (!(bp->b_flags & B_ASYNC) && error)
			    bp->b_flags |= B_EINTR;
	    } else {
		if (error) {
		    bp->b_flags |= B_ERROR;
		    bp->b_error = np->n_error = error;
		    np->n_flag |= NWRITEERR;
		}
		bp->b_dirtyoff = bp->b_dirtyend = 0;
	    }
d566 587
@


1.23
log
@Replace inclusion of <vm/foo.h> with the correct <uvm/bar.h> when necessary.
(Look ma, I might have broken the tree)
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.22 2001/06/27 04:58:46 art Exp $	*/
d139 9
d174 1
a174 1
				if (nfs_asyncio(rabp, cred)) {
d203 1
a203 1
				error = nfs_doio(bp, cred, p);
d244 1
a244 1
			error = nfs_doio(bp, cred, p);
d257 1
a257 1
	    };
d347 9
a377 4
		if (bp->b_wcred == NOCRED) {
			crhold(cred);
			bp->b_wcred = cred;
		}
d539 2
a540 3
nfs_asyncio(bp, cred)
	register struct buf *bp;
	struct ucred *cred;
d548 1
a548 6
		if (bp->b_flags & B_READ) {
			if (bp->b_rcred == NOCRED && cred != NOCRED) {
				crhold(cred);
				bp->b_rcred = cred;
			}
		} else {
a549 4
			if (bp->b_wcred == NOCRED && cred != NOCRED) {
				crhold(cred);
				bp->b_wcred = cred;
			}
d584 2
a585 3
nfs_doio(bp, cr, p)
	register struct buf *bp;
	struct ucred *cr;
d615 1
a615 1
	    uiop->uio_offset = ((off_t)bp->b_blkno) * DEV_BSIZE;
d619 1
a619 1
		error = nfs_readrpc(vp, uiop, cr);
d624 1
a624 1
		error = nfs_writerpc(vp, uiop, cr, &iomode, &must_commit);
d636 1
a636 1
		uiop->uio_offset = ((off_t)bp->b_blkno) * DEV_BSIZE;
d638 1
a638 1
		error = nfs_readrpc(vp, uiop, cr);
d649 1
a649 1
			len = np->n_size - (((u_quad_t)bp->b_blkno) * DEV_BSIZE
d670 1
a670 1
		error = nfs_readlinkrpc(vp, uiop, cr);
d697 1
a697 1
	    error = nfs_writerpc(vp, uiop, cr, &iomode, &must_commit);
@


1.22
log
@Remove old vm.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.21 2001/06/25 05:27:54 csapuntz Exp $	*/
d54 1
a54 1
#include <vm/vm.h>
@


1.21
log
@

Get rid of some dead code caused by the last commit
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.20 2001/06/25 03:28:06 csapuntz Exp $	*/
a348 1
#if defined(UVM)
a349 3
#else
		(void)vnode_pager_uncache(vp);
#endif
a366 1
#if defined(UVM)
a367 3
#else
			vnode_pager_setsize(vp, (u_long)np->n_size);
#endif
@


1.20
log
@Remove NQNFS
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.19 2001/06/25 02:15:46 csapuntz Exp $	*/
d116 4
a139 3
	    /*
	     * Don't cache symlinks.
	     */
d141 1
a141 9
		switch (vp->v_type) {
		case VREG:
			return (nfs_readrpc(vp, uio, cred));
		case VLNK:
			return (nfs_readlinkrpc(vp, uio, cred));
		default:
			printf(" NQNFSNONCACHE: type %x unexpected\n",	
				vp->v_type);
		};
@


1.19
log
@
Get rid of old directory caching scheme which caused persistent duplicates.

Still not correct for NFSv3 but that's hard.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.18 2001/02/23 14:52:50 csapuntz Exp $	*/
a59 1
#include <nfs/nqnfs.h>
d65 1
a65 1
extern struct nfsstats nfsstats;
a103 1
	 * For nqnfs, full cache consistency is maintained within the loop.
d116 1
a116 1
	if ((nmp->nm_flag & NFSMNT_NQNFS) == 0 && vp->v_type != VLNK) {
a135 20

	    /*
	     * Get a valid lease. If cached data is stale, flush it.
	     */
	    if (nmp->nm_flag & NFSMNT_NQNFS) {
		if (NQNFS_CKINVALID(vp, np, ND_READ)) {
		    do {
			error = nqnfs_getlease(vp, ND_READ, cred, p);
		    } while (error == NQNFS_EXPIRED);
		    if (error)
			return (error);
		    if (np->n_lrev != np->n_brev ||
			(np->n_flag & NQNFSNONCACHE)) {
			error = nfs_vinvalbuf(vp, V_SAVE, cred, p, 1);
			if (error)
			    return (error);
			np->n_brev = np->n_lrev;
		    }
		}
	    }
d139 1
a139 2
	    if (np->n_flag & NQNFSNONCACHE
		|| ((vp->v_flag & VROOT) && vp->v_type == VLNK)) {
d301 1
a301 1
	int n, on, error = 0, iomode, must_commit;
a361 25
		/*
		 * Check for a valid write lease.
		 */
		if ((nmp->nm_flag & NFSMNT_NQNFS) &&
		    NQNFS_CKINVALID(vp, np, ND_WRITE)) {
			do {
				error = nqnfs_getlease(vp, ND_WRITE, cred, p);
			} while (error == NQNFS_EXPIRED);
			if (error)
				return (error);
			if (np->n_lrev != np->n_brev ||
			    (np->n_flag & NQNFSNONCACHE)) {
				error = nfs_vinvalbuf(vp, V_SAVE, cred, p, 1);
				if (error)
					return (error);
				np->n_brev = np->n_lrev;
			}
		}
		if ((np->n_flag & NQNFSNONCACHE) && uio->uio_iovcnt == 1) {
		    iomode = NFSV3WRITE_FILESYNC;
		    error = nfs_writerpc(vp, uio, cred, &iomode, &must_commit);
		    if (must_commit)
			nfs_clearcommit(vp->v_mount);
		    return (error);
		}
a397 23
		/*
		 * Check for valid write lease and get one as required.
		 * In case getblk() and/or bwrite() delayed us.
		 */
		if ((nmp->nm_flag & NFSMNT_NQNFS) &&
		    NQNFS_CKINVALID(vp, np, ND_WRITE)) {
			do {
				error = nqnfs_getlease(vp, ND_WRITE, cred, p);
			} while (error == NQNFS_EXPIRED);
			if (error) {
				brelse(bp);
				return (error);
			}
			if (np->n_lrev != np->n_brev ||
			    (np->n_flag & NQNFSNONCACHE)) {
				brelse(bp);
				error = nfs_vinvalbuf(vp, V_SAVE, cred, p, 1);
				if (error)
					return (error);
				np->n_brev = np->n_lrev;
				goto again;
			}
		}
d429 1
a429 1
		if ((np->n_flag & NQNFSNONCACHE) || (ioflag & IO_SYNC)) {
d434 1
a434 7
			if (np->n_flag & NQNFSNONCACHE) {
				error = nfs_vinvalbuf(vp, V_SAVE, cred, p, 1);
				if (error)
					return (error);
			}
		} else if ((n + on) == biosize &&
			(nmp->nm_flag & NFSMNT_NQNFS) == 0) {
d673 1
a673 5
			(((nmp->nm_flag & NFSMNT_NQNFS) &&
			  NQNFS_CKINVALID(vp, np, ND_READ) &&
			  np->n_lrev != np->n_brev) ||
			 (!(nmp->nm_flag & NFSMNT_NQNFS) &&
			  np->n_mtime != np->n_vattr.va_mtime.tv_sec))) {
@


1.18
log
@

Change the B_DELWRI flag using buf_dirty and buf_undirty instead of
manually twiddling it. This allows the buffer cache to more easily
keep track of dirty buffers and decide when it is appropriate to speed
up the syncer.

Insipired by FreeBSD.
Look over by art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.17 2001/02/23 14:42:38 csapuntz Exp $	*/
d80 1
a80 1
	register int biosize, diff, i;
a119 8
			if (vp->v_type != VREG) {
				if (vp->v_type != VDIR)
					panic("nfs: bioread, not dir");
				nfs_invaldir(vp);
				error = nfs_vinvalbuf(vp, V_SAVE, cred, p, 1);
				if (error)
					return (error);
			}
a129 2
				if (vp->v_type == VDIR)
					nfs_invaldir(vp);
d150 1
a150 4
			(np->n_flag & NQNFSNONCACHE) ||
			((np->n_flag & NMODIFIED) && vp->v_type == VDIR)) {
			if (vp->v_type == VDIR)
			    nfs_invaldir(vp);
a155 5
		} else if (vp->v_type == VDIR && (np->n_flag & NMODIFIED)) {
		    nfs_invaldir(vp);
		    error = nfs_vinvalbuf(vp, V_SAVE, cred, p, 1);
		    if (error)
			return (error);
a167 2
		case VDIR:
			break;
a274 65
	    case VDIR:
		if (uio->uio_resid < NFS_READDIRBLKSIZ)
			return (0);
		nfsstats.biocache_readdirs++;
		lbn = uio->uio_offset / NFS_DIRBLKSIZ;
		on = uio->uio_offset & (NFS_DIRBLKSIZ - 1);
		bp = nfs_getcacheblk(vp, lbn, NFS_DIRBLKSIZ, p);
		if (!bp)
		    return (EINTR);
		if ((bp->b_flags & B_DONE) == 0) {
		    bp->b_flags |= B_READ;
		    error = nfs_doio(bp, cred, p);
		    if (error) {
			brelse(bp);
			while (error == NFSERR_BAD_COOKIE) {
			    nfs_invaldir(vp);
			    error = nfs_vinvalbuf(vp, 0, cred, p, 1);
			    /*
			     * Yuck! The directory has been modified on the
			     * server. The only way to get the block is by
			     * reading from the beginning to get all the
			     * offset cookies.
			     */
			    for (i = 0; i <= lbn && !error; i++) {
				bp = nfs_getcacheblk(vp, i, NFS_DIRBLKSIZ, p);
				if (!bp)
				    return (EINTR);
				if ((bp->b_flags & B_DONE) == 0) {
				    bp->b_flags |= B_READ;
				    error = nfs_doio(bp, cred, p);
				    if (error)
					brelse(bp);
				}
			    }
			}
			if (error)
			    return (error);
		    }
		}

		/*
		 * If not eof and read aheads are enabled, start one.
		 * (You need the current block first, so that you have the
		 *  directory offset cookie of the next block.)
		 */
		if (nfs_numasync > 0 && nmp->nm_readahead > 0 &&
		    (np->n_direofoffset == 0 ||
		    (lbn + 1) * NFS_DIRBLKSIZ < np->n_direofoffset) &&
		    !(np->n_flag & NQNFSNONCACHE) &&
		    !incore(vp, lbn + 1)) {
			rabp = nfs_getcacheblk(vp, lbn + 1, NFS_DIRBLKSIZ, p);
			if (rabp) {
			    if ((rabp->b_flags & (B_DONE | B_DELWRI)) == 0) {
				rabp->b_flags |= (B_READ | B_ASYNC);
				if (nfs_asyncio(rabp, cred)) {
				    rabp->b_flags |= B_INVAL;
				    brelse(rabp);
				}
			    } else
				brelse(rabp);
			}
		}
		n = min(uio->uio_resid, NFS_DIRBLKSIZ - bp->b_resid - on);
		got_buf = 1;
		break;
a290 4
	    case VDIR:
		if (np->n_flag & NQNFSNONCACHE)
			bp->b_flags |= B_INVAL;
		break;
a763 11
		break;
	    case VDIR:
		nfsstats.readdir_bios++;
		uiop->uio_offset = ((u_quad_t)bp->b_lblkno) * NFS_DIRBLKSIZ;
		if (nmp->nm_flag & NFSMNT_RDIRPLUS) {
			error = nfs_readdirplusrpc(vp, uiop, cr);
			if (error == NFSERR_NOTSUPP)
				nmp->nm_flag &= ~NFSMNT_RDIRPLUS;
		}
		if ((nmp->nm_flag & NFSMNT_RDIRPLUS) == 0)
			error = nfs_readdirrpc(vp, uiop, cr);
@


1.17
log
@

Remove the clustering fields from the vnodes and place them in the
file system inode instead
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.16 2000/06/23 02:14:40 mickey Exp $	*/
a748 2
	bp->b_flags |= B_DELWRI;

d750 1
a750 1
	reassignbuf(bp, bp->b_vp);
a909 8
		bp->b_flags |= B_DELWRI;

		/*
		 * Since for the B_ASYNC case, nfs_bwrite() has reassigned the
		 * buffer to the clean list, we have to reassign it back to the
		 * dirty one. Ugh.
		 */
		if (bp->b_flags & B_ASYNC) {
d911 1
a911 1
		    reassignbuf(bp, vp);
d913 3
a915 3
		}
		else if (error)
		    bp->b_flags |= B_EINTR;
@


1.16
log
@remove obsolete vtrace guts; art@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.15 1999/02/26 03:16:25 art Exp $	*/
a273 1
		vp->v_lastr = lbn;
@


1.15
log
@compatibility with uvm vnode pager
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.14 1997/12/02 16:57:57 csapuntz Exp $	*/
a49 1
#include <sys/trace.h>
@


1.15.6.1
log
@merge in approximately 2.9 into SMP branch
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.18 2001/02/23 14:52:50 csapuntz Exp $	*/
d50 1
d275 1
d751 2
d754 1
a754 1
	buf_dirty(bp);
d914 8
d923 1
a923 1
		    buf_dirty(bp);
d925 3
a927 3

		    if (!(bp->b_flags & B_ASYNC) && error)
			    bp->b_flags |= B_EINTR;
@


1.15.6.2
log
@Merge in -current from two days ago in the SMP branch.
As usual with merges, they do not indicate progress, so do not hold
your breath for working SMP, and do not mail me and ask about the
state of it.  It has not changed.  There is work ongoing, but very, very
slowly.  The commit is done in parts as to not lock up the tree in too
big chunks at a time.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.15.6.1 2001/05/14 22:44:57 niklas Exp $	*/
d60 1
d66 1
a66 1
struct nfsstats nfsstats;
d80 1
a80 1
	register int biosize, diff;
d105 1
d118 1
a118 5
	/* 
	 * There is no way to modify a symbolic link via NFS or via
	 * VFS, so we don't check if the link was modified 
	 */
	if (vp->v_type != VLNK) {
d120 8
d138 2
d148 44
a191 2
	    if ((vp->v_flag & VROOT) && vp->v_type == VLNK) {
		    return (nfs_readlinkrpc(vp, uio, cred));
d295 65
d376 4
d413 1
a413 1
	int n, on, error = 0;
d468 1
d470 3
d474 25
d515 1
d517 3
d535 23
d589 1
a589 1
		if (ioflag & IO_SYNC) {
d594 7
a600 1
		} else if ((n + on) == biosize) {
d839 5
a843 1
		    (np->n_mtime != np->n_vattr.va_mtime.tv_sec)) {
d853 11
@


1.15.6.3
log
@merge in -current
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d54 1
a54 1
#include <uvm/uvm_extern.h>
@


1.15.6.4
log
@Merge in -current
@
text
@a52 1
#include <sys/pool.h>
d54 1
a54 1
#include <uvm/uvm.h>
d73 2
a74 2
	struct vnode *vp;
	struct uio *uio;
d78 3
a80 3
	struct nfsnode *np = VTONFS(vp);
	int biosize;
	struct buf *bp = NULL;
d84 1
d86 1
a86 1
	int got_buf = 0, error = 0, n = 0, on = 0;
a138 9

	/*
	 * update the cache read creds for this vnode
	 */
	if (np->n_rcred)
		crfree(np->n_rcred);
	np->n_rcred = cred;
	crhold(cred);

d147 24
a170 14
		error = 0;
		while (uio->uio_resid > 0) {
			void *win;
			vsize_t bytelen = MIN(np->n_size - uio->uio_offset,
					      uio->uio_resid);

			if (bytelen == 0)
				break;
			win = ubc_alloc(&vp->v_uvm.u_obj, uio->uio_offset,
					&bytelen, UBC_READ);
			error = uiomove(win, bytelen, uio);
			ubc_release(win, 0);
			if (error) {
				break;
d172 1
d174 53
a226 1
		n = 0;
a227 1

d235 1
a235 1
			error = nfs_doio(bp, p);
d241 1
a241 1
		n = MIN(uio->uio_resid, NFS_MAXPATHLEN - bp->b_resid);
d248 1
a248 1
	    }
d283 2
a284 2
	int biosize;
	struct uio *uio = ap->a_uio;
d286 1
a286 1
	struct vnode *vp = ap->a_vp;
d288 1
a288 1
	struct ucred *cred = ap->a_cred;
d290 1
d293 2
a294 2
	int error = 0;
	int rv;
a337 9

	/*
	 * update the cache write creds for this node.
	 */
	if (np->n_wcred)
		crfree(np->n_wcred);
	np->n_wcred = cred;
	crhold(cred);

d345 5
a349 3
		void *win;
		voff_t oldoff = uio->uio_offset;
		vsize_t bytelen = uio->uio_resid;
d352 12
d365 16
a380 3
		if (np->n_size < uio->uio_offset + bytelen) {
			np->n_size = uio->uio_offset + bytelen;
			uvm_vnp_setsize(vp, np->n_size);
d382 13
a394 22
		win = ubc_alloc(&vp->v_uvm.u_obj, uio->uio_offset, &bytelen,
				UBC_WRITE);
		error = uiomove(win, bytelen, uio);
		ubc_release(win, 0);
		rv = 1;
		if ((ioflag & IO_SYNC)) {
			simple_lock(&vp->v_uvm.u_obj.vmobjlock);
			rv = vp->v_uvm.u_obj.pgops->pgo_flush(
			    &vp->v_uvm.u_obj,
			    oldoff & ~(nmp->nm_wsize - 1),
			    uio->uio_offset & ~(nmp->nm_wsize - 1),
			    PGO_CLEANIT|PGO_SYNCIO);
			simple_unlock(&vp->v_uvm.u_obj.vmobjlock);
		} else if ((oldoff & ~(nmp->nm_wsize - 1)) !=
		    (uio->uio_offset & ~(nmp->nm_wsize - 1))) {
			simple_lock(&vp->v_uvm.u_obj.vmobjlock);
			rv = vp->v_uvm.u_obj.pgops->pgo_flush(
			    &vp->v_uvm.u_obj,
			    oldoff & ~(nmp->nm_wsize - 1),
			    uio->uio_offset & ~(nmp->nm_wsize - 1),
			    PGO_CLEANIT|PGO_WEAK);
			simple_unlock(&vp->v_uvm.u_obj.vmobjlock);
d396 7
a402 2
		if (!rv) {
			error = EIO;
d404 21
a424 2
		if (error) {
			break;
d426 2
a427 2
	} while (uio->uio_resid > 0);
	return (error);
d449 3
a451 3
		while (bp == NULL) {
			if (nfs_sigintr(nmp, NULL, p))
				return (NULL);
d491 1
a491 1
		if (error && intrflg && nfs_sigintr(nmp, NULL, p))
d501 1
a501 1
		if (intrflg && nfs_sigintr(nmp, NULL, p)) {
d525 3
a527 2
nfs_asyncio(bp)
	struct buf *bp;
d529 1
a529 1
	int i;
d533 1
a533 1
	for (i = 0; i < NFS_MAXASYNCDAEMON; i++) {
d535 13
d549 1
a549 1
		nfs_iodwant[i] = NULL;
a552 1
	}
d554 19
a572 1
	return (EIO);
d580 3
a582 2
nfs_doio(bp, p)
	struct buf *bp;
d589 1
a589 1
	int error = 0, diff, len, iomode, must_commit = 0;
d612 1
a612 1
	    uiop->uio_offset = ((off_t)bp->b_blkno) << DEV_BSHIFT;
d616 1
a616 1
		error = nfs_readrpc(vp, uiop);
d621 1
a621 1
		error = nfs_writerpc(vp, uiop, &iomode, &must_commit);
d633 1
a633 1
		uiop->uio_offset = ((off_t)bp->b_blkno) << DEV_BSHIFT;
d635 4
a638 2
		error = nfs_readrpc(vp, uiop);
		if (!error && uiop->uio_resid) {
d646 1
a646 1
			len = np->n_size - ((((off_t)bp->b_blkno) << DEV_BSHIFT)
d649 7
a655 3
				len = MIN(len, uiop->uio_resid);
				memset((char *)bp->b_data + diff, 0, len);
			}
d667 1
a667 1
		error = nfs_readlinkrpc(vp, uiop, curproc->p_ucred);
d672 1
a672 1
	    }
d678 5
a682 3
	    io.iov_base = bp->b_data;
	    io.iov_len = uiop->uio_resid = bp->b_bcount;
	    uiop->uio_offset = ((off_t)bp->b_blkno) << DEV_BSHIFT;
d685 43
a727 2
	    iomode = NFSV3WRITE_UNSTABLE;
	    error = nfs_writerpc(vp, uiop, &iomode, &must_commit);
a733 594
}

/*
 * Vnode op for VM getpages.
 */
int
nfs_getpages(v)
	void *v;
{
	struct vop_getpages_args /* {
		struct vnode *a_vp;
		voff_t a_offset;
		vm_page_t *a_m;
		int *a_count;
		int a_centeridx;
		vm_prot_t a_access_type;
		int a_advice;
		int a_flags;
	} */ *ap = v;

	off_t eof, offset, origoffset, startoffset, endoffset;
	int s, i, error, npages, orignpages, npgs, ridx, pidx, pcount;
	vaddr_t kva;
	struct buf *bp, *mbp;
	struct vnode *vp = ap->a_vp;
	struct nfsnode *np = VTONFS(vp);
	struct uvm_object *uobj = &vp->v_uvm.u_obj;
	struct nfsmount *nmp = VFSTONFS(vp->v_mount);
	size_t bytes, iobytes, tailbytes, totalbytes, skipbytes;
	int flags = ap->a_flags;
	int bsize;
	struct vm_page *pgs[16];			/* XXXUBC 16 */
	boolean_t v3 = NFS_ISV3(vp);
	boolean_t async = (flags & PGO_SYNCIO) == 0;
	boolean_t write = (ap->a_access_type & VM_PROT_WRITE) != 0;
	struct proc *p = curproc;

	UVMHIST_FUNC("nfs_getpages"); UVMHIST_CALLED(ubchist);
	UVMHIST_LOG(ubchist, "vp %p off 0x%x count %d", vp, (int)ap->a_offset,
		    *ap->a_count,0);

#ifdef DIAGNOSTIC
	if (ap->a_centeridx < 0 || ap->a_centeridx >= *ap->a_count) {
		panic("nfs_getpages: centeridx %d out of range",
		      ap->a_centeridx);
	}
#endif

	error = 0;
	origoffset = ap->a_offset;
	eof = vp->v_uvm.u_size;
	if (origoffset >= eof) {
		if ((flags & PGO_LOCKED) == 0) {
			simple_unlock(&uobj->vmobjlock);
		}
		UVMHIST_LOG(ubchist, "off 0x%x past EOF 0x%x",
			    (int)origoffset, (int)eof,0,0);
		return EINVAL;
	}

	if (flags & PGO_LOCKED) {
		uvn_findpages(uobj, origoffset, ap->a_count, ap->a_m,
			      UFP_NOWAIT|UFP_NOALLOC);
		return 0;
	}

	/* vnode is VOP_LOCKed, uobj is locked */
	if (write && (vp->v_bioflag & VBIOONSYNCLIST) == 0) {
		vn_syncer_add_to_worklist(vp, syncdelay);
	}
	bsize = nmp->nm_rsize;
	orignpages = MIN(*ap->a_count,
			 round_page(eof - origoffset) >> PAGE_SHIFT);
	npages = orignpages;
	startoffset = origoffset & ~(bsize - 1);
	endoffset = round_page((origoffset + (npages << PAGE_SHIFT)
				+ bsize - 1) & ~(bsize - 1));
	endoffset = MIN(endoffset, round_page(eof));
	ridx = (origoffset - startoffset) >> PAGE_SHIFT;

	if (!async && !write) {
		int rapages = MAX(PAGE_SIZE, nmp->nm_rsize) >> PAGE_SHIFT;

		(void) VOP_GETPAGES(vp, endoffset, NULL, &rapages, 0,
				    VM_PROT_READ, 0, 0);
		simple_lock(&uobj->vmobjlock);
	}

	UVMHIST_LOG(ubchist, "npages %d offset 0x%x", npages,
		    (int)origoffset, 0,0);
	memset(pgs, 0, sizeof(pgs));
	uvn_findpages(uobj, origoffset, &npages, &pgs[ridx], UFP_ALL);

	if (flags & PGO_OVERWRITE) {
		UVMHIST_LOG(ubchist, "PGO_OVERWRITE",0,0,0,0);

		/* XXXUBC for now, zero the page if we allocated it */
		for (i = 0; i < npages; i++) {
			struct vm_page *pg = pgs[ridx + i];

			if (pg->flags & PG_FAKE) {
				uvm_pagezero(pg);
				pg->flags &= ~(PG_FAKE);
			}
		}
		npages += ridx;
		if (v3) {
			simple_unlock(&uobj->vmobjlock);
			goto uncommit;
		}
		goto out;
	}

	/*
	 * if the pages are already resident, just return them.
	 */

	for (i = 0; i < npages; i++) {
		struct vm_page *pg = pgs[ridx + i];

		if ((pg->flags & PG_FAKE) != 0 ||
		    ((ap->a_access_type & VM_PROT_WRITE) &&
		      (pg->flags & PG_RDONLY))) {
			break;
		}
	}
	if (i == npages) {
		UVMHIST_LOG(ubchist, "returning cached pages", 0,0,0,0);
		npages += ridx;
		goto out;
	}

	/*
	 * the page wasn't resident and we're not overwriting,
	 * so we're going to have to do some i/o.
	 * find any additional pages needed to cover the expanded range.
	 */

	if (startoffset != origoffset ||
	    startoffset + (npages << PAGE_SHIFT) != endoffset) {

		/*
		 * XXXUBC we need to avoid deadlocks caused by locking
		 * additional pages at lower offsets than pages we
		 * already have locked.  for now, unlock them all and
		 * start over.
		 */

		for (i = 0; i < npages; i++) {
			struct vm_page *pg = pgs[ridx + i];

			if (pg->flags & PG_FAKE) {
				pg->flags |= PG_RELEASED;
			}
		}
		uvm_page_unbusy(&pgs[ridx], npages);
		memset(pgs, 0, sizeof(pgs));

		UVMHIST_LOG(ubchist, "reset npages start 0x%x end 0x%x",
			    startoffset, endoffset, 0,0);
		npages = (endoffset - startoffset) >> PAGE_SHIFT;
		npgs = npages;
		uvn_findpages(uobj, startoffset, &npgs, pgs, UFP_ALL);
	}
	simple_unlock(&uobj->vmobjlock);

	/*
	 * update the cached read creds for this node.
	 */

	if (np->n_rcred) {
		crfree(np->n_rcred);
	}
	np->n_rcred = curproc->p_ucred;
	crhold(np->n_rcred);

	/*
	 * read the desired page(s).
	 */

	totalbytes = npages << PAGE_SHIFT;
	bytes = MIN(totalbytes, vp->v_uvm.u_size - startoffset);
	tailbytes = totalbytes - bytes;
	skipbytes = 0;

	kva = uvm_pagermapin(pgs, npages, UVMPAGER_MAPIN_WAITOK |
			     UVMPAGER_MAPIN_READ);

	s = splbio();
	mbp = pool_get(&bufpool, PR_WAITOK);
	splx(s);
	mbp->b_bufsize = totalbytes;
	mbp->b_data = (void *)kva;
	mbp->b_resid = mbp->b_bcount = bytes;
	mbp->b_flags = B_BUSY|B_READ| (async ? B_CALL|B_ASYNC : 0);
	mbp->b_iodone = uvm_aio_biodone;
	mbp->b_vp = NULL;
	mbp->b_proc = NULL;		/* XXXUBC */
	LIST_INIT(&mbp->b_dep);
	bgetvp(vp, mbp);

	/*
	 * if EOF is in the middle of the last page, zero the part past EOF.
	 */

	if (tailbytes > 0 && (pgs[bytes >> PAGE_SHIFT]->flags & PG_FAKE)) {
		memset((char *)kva + bytes, 0, tailbytes);
	}

	/*
	 * now loop over the pages, reading as needed.
	 */

	bp = NULL;
	for (offset = startoffset;
	     bytes > 0;
	     offset += iobytes, bytes -= iobytes) {

		/*
		 * skip pages which don't need to be read.
		 */

		pidx = (offset - startoffset) >> PAGE_SHIFT;
		UVMHIST_LOG(ubchist, "pidx %d offset 0x%x startoffset 0x%x",
			    pidx, (int)offset, (int)startoffset,0);
		while ((pgs[pidx]->flags & PG_FAKE) == 0) {
			size_t b;

			KASSERT((offset & (PAGE_SIZE - 1)) == 0);
			b = MIN(PAGE_SIZE, bytes);
			offset += b;
			bytes -= b;
			skipbytes += b;
			pidx++;
			UVMHIST_LOG(ubchist, "skipping, new offset 0x%x",
				    (int)offset, 0,0,0);
			if (bytes == 0) {
				goto loopdone;
			}
		}

		/*
		 * see how many pages can be read with this i/o.
		 * reduce the i/o size if necessary.
		 */

		iobytes = bytes;
		if (offset + iobytes > round_page(offset)) {
			pcount = 1;
			while (pidx + pcount < npages &&
			       pgs[pidx + pcount]->flags & PG_FAKE) {
				pcount++;
			}
			iobytes = MIN(iobytes, (pcount << PAGE_SHIFT) -
				      (offset - trunc_page(offset)));
		}
		iobytes = MIN(iobytes, nmp->nm_rsize);

		/*
		 * allocate a sub-buf for this piece of the i/o
		 * (or just use mbp if there's only 1 piece),
		 * and start it going.
		 */

		if (offset == startoffset && iobytes == bytes) {
			bp = mbp;
		} else {
			s = splbio();
			bp = pool_get(&bufpool, PR_WAITOK);
			splx(s);
			bp->b_data = (char *)kva + offset - startoffset;
			bp->b_resid = bp->b_bcount = iobytes;
			bp->b_flags = B_BUSY|B_READ|B_CALL|B_ASYNC;
			bp->b_iodone = uvm_aio_biodone1;
			bp->b_vp = vp;
			bp->b_proc = NULL;	/* XXXUBC */
			LIST_INIT(&bp->b_dep);
		}
		bp->b_private = mbp;
		bp->b_lblkno = bp->b_blkno = offset >> DEV_BSHIFT;

		UVMHIST_LOG(ubchist, "bp %p offset 0x%x bcount 0x%x blkno 0x%x",
			    bp, offset, iobytes, bp->b_blkno);

		VOP_STRATEGY(bp);
	}

loopdone:
	if (skipbytes) {
		s = splbio();
		mbp->b_resid -= skipbytes;
		if (mbp->b_resid == 0) {
			biodone(mbp);
		}
		splx(s);
	}
	if (async) {
		UVMHIST_LOG(ubchist, "returning 0 (async)",0,0,0,0);
		return 0;
	}
	if (bp != NULL) {
		error = biowait(mbp);
	}
	s = splbio();
	(void) buf_cleanout(mbp);
	pool_put(&bufpool, mbp);
	splx(s);
	uvm_pagermapout(kva, npages);
 
	if (write && v3) {
uncommit:
 		lockmgr(&np->n_commitlock, LK_EXCLUSIVE, NULL, p);
		nfs_del_committed_range(vp, origoffset, npages);
		nfs_del_tobecommitted_range(vp, origoffset, npages);
		simple_lock(&uobj->vmobjlock);
		for (i = 0; i < npages; i++) {
			if (pgs[i] == NULL) {
				continue;
			}
			pgs[i]->flags &= ~(PG_NEEDCOMMIT|PG_RDONLY);
		}
		simple_unlock(&uobj->vmobjlock);
 		lockmgr(&np->n_commitlock, LK_RELEASE, NULL, p);
	}

	simple_lock(&uobj->vmobjlock);

out:
	if (error) {
		uvm_lock_pageq();
		for (i = 0; i < npages; i++) {
			if (pgs[i] == NULL) {
				continue;
			}
			UVMHIST_LOG(ubchist, "examining pg %p flags 0x%x",
				    pgs[i], pgs[i]->flags, 0,0);
			if (pgs[i]->flags & PG_WANTED) {
				wakeup(pgs[i]);
			}
			if (pgs[i]->flags & PG_RELEASED) {
				uvm_unlock_pageq();
				(uobj->pgops->pgo_releasepg)(pgs[i], NULL);
				uvm_lock_pageq();
				continue;
			}
			if (pgs[i]->flags & PG_FAKE) {
				uvm_pagefree(pgs[i]);
				continue;
			}
			uvm_pageactivate(pgs[i]);
			pgs[i]->flags &= ~(PG_WANTED|PG_BUSY);
			UVM_PAGE_OWN(pgs[i], NULL);
		}
		uvm_unlock_pageq();
		simple_unlock(&uobj->vmobjlock);
		UVMHIST_LOG(ubchist, "returning error %d", error,0,0,0);
		return error;
	}

	UVMHIST_LOG(ubchist, "ridx %d count %d", ridx, npages, 0,0);
	uvm_lock_pageq();
	for (i = 0; i < npages; i++) {
		if (pgs[i] == NULL) {
			continue;
		}
		UVMHIST_LOG(ubchist, "examining pg %p flags 0x%x",
			    pgs[i], pgs[i]->flags, 0,0);
		if (pgs[i]->flags & PG_FAKE) {
			UVMHIST_LOG(ubchist, "unfaking pg %p offset 0x%x",
				    pgs[i], (int)pgs[i]->offset,0,0);
			pgs[i]->flags &= ~(PG_FAKE);
			pmap_clear_modify(pgs[i]);
			pmap_clear_reference(pgs[i]);
		}
		if (i < ridx || i >= ridx + orignpages || async) {
			UVMHIST_LOG(ubchist, "unbusy pg %p offset 0x%x",
				    pgs[i], (int)pgs[i]->offset,0,0);
			if (pgs[i]->flags & PG_WANTED) {
				wakeup(pgs[i]);
			}
			if (pgs[i]->flags & PG_RELEASED) {
				uvm_unlock_pageq();
				(uobj->pgops->pgo_releasepg)(pgs[i], NULL);
				uvm_lock_pageq();
				continue;
			}
			uvm_pageactivate(pgs[i]);
			pgs[i]->flags &= ~(PG_WANTED|PG_BUSY);
			UVM_PAGE_OWN(pgs[i], NULL);
		}
	}
	uvm_unlock_pageq();
	simple_unlock(&uobj->vmobjlock);
	if (ap->a_m != NULL) {
		memcpy(ap->a_m, &pgs[ridx],
		       *ap->a_count * sizeof(struct vm_page *));
	}
	return 0;
}

/*
 * Vnode op for VM putpages.
 */
int
nfs_putpages(v)
	void *v;
{
	struct vop_putpages_args /* {
		struct vnode *a_vp;
		struct vm_page **a_m;
		int a_count;
		int a_flags;
		int *a_rtvals;
	} */ *ap = v;

	struct vnode *vp = ap->a_vp;
	struct nfsnode *np = VTONFS(vp);
	struct nfsmount *nmp = VFSTONFS(vp->v_mount);
	struct buf *bp, *mbp;
	struct vm_page **pgs = ap->a_m;
	int flags = ap->a_flags;
	int npages = ap->a_count;
	int s, error, i;
	size_t bytes, iobytes, skipbytes;
	vaddr_t kva;
	off_t offset, origoffset, commitoff;
	uint32_t commitbytes;
	boolean_t v3 = NFS_ISV3(vp);
	boolean_t async = (flags & PGO_SYNCIO) == 0;
	boolean_t weak = (flags & PGO_WEAK) && v3;
	struct proc *p = curproc;
	UVMHIST_FUNC("nfs_putpages"); UVMHIST_CALLED(ubchist);

	UVMHIST_LOG(ubchist, "vp %p pgp %p count %d",
		    vp, ap->a_m, ap->a_count,0);

	simple_unlock(&vp->v_uvm.u_obj.vmobjlock);

	error = 0;
	origoffset = pgs[0]->offset;
	bytes = MIN(ap->a_count << PAGE_SHIFT, vp->v_uvm.u_size - origoffset);
	skipbytes = 0;

	/*
	 * if the range has been committed already, mark the pages thus.
	 * if the range just needs to be committed, we're done
	 * if it's a weak putpage, otherwise commit the range.
	 */

	if (v3) {
 		lockmgr(&np->n_commitlock, LK_EXCLUSIVE, NULL, p);
		if (nfs_in_committed_range(vp, origoffset, bytes)) {
			goto committed;
		}
		if (nfs_in_tobecommitted_range(vp, origoffset, bytes)) {
			if (weak) {
				lockmgr(&np->n_commitlock, LK_RELEASE, NULL, p);
				return 0;
			} else {
				commitoff = np->n_pushlo;
				commitbytes = (uint32_t)(np->n_pushhi -
							 np->n_pushlo);
				goto commit;
			}
		}
		lockmgr(&np->n_commitlock, LK_RELEASE, NULL, p);
	}

	/*
	 * otherwise write or commit all the pages.
	 */

	kva = uvm_pagermapin(pgs, ap->a_count, UVMPAGER_MAPIN_WAITOK|
			     UVMPAGER_MAPIN_WRITE);

	s = splbio();
	vp->v_numoutput += 2;
	mbp = pool_get(&bufpool, PR_WAITOK);
	UVMHIST_LOG(ubchist, "vp %p mbp %p num now %d bytes 0x%x",
		    vp, mbp, vp->v_numoutput, bytes);
	splx(s);
	mbp->b_bufsize = npages << PAGE_SHIFT;
	mbp->b_data = (void *)kva;
	mbp->b_resid = mbp->b_bcount = bytes;
	mbp->b_flags = B_BUSY|B_WRITE|B_AGE |
		(async ? B_CALL|B_ASYNC : 0) |
		(curproc == uvm.pagedaemon_proc ? B_PDAEMON : 0);
	mbp->b_iodone = uvm_aio_biodone;
	mbp->b_vp = NULL;
	mbp->b_proc = NULL;		/* XXXUBC */
	LIST_INIT(&mbp->b_dep);
	bgetvp(vp, mbp);

	for (offset = origoffset;
	     bytes > 0;
	     offset += iobytes, bytes -= iobytes) {
		iobytes = MIN(nmp->nm_wsize, bytes);

 		/*
		 * skip writing any pages which only need a commit.
		 */

		if ((pgs[(offset - origoffset) >> PAGE_SHIFT]->flags &
		     PG_NEEDCOMMIT) != 0) {
			KASSERT((offset & (PAGE_SIZE - 1)) == 0);
			iobytes = MIN(PAGE_SIZE, bytes);
			skipbytes += iobytes;
			continue;
		}

		/* if it's really one i/o, don't make a second buf */
		if (offset == origoffset && iobytes == bytes) {
			bp = mbp;
		} else {
			s = splbio();
			vp->v_numoutput++;
			bp = pool_get(&bufpool, PR_WAITOK);
			UVMHIST_LOG(ubchist, "vp %p bp %p num now %d",
				    vp, bp, vp->v_numoutput, 0);
			splx(s);
			bp->b_data = (char *)kva + (offset - origoffset);
			bp->b_resid = bp->b_bcount = iobytes;
			bp->b_flags = B_BUSY|B_WRITE|B_CALL|B_ASYNC;
			bp->b_iodone = uvm_aio_biodone1;
			bp->b_vp = vp;
			bp->b_proc = NULL;	/* XXXUBC */
			LIST_INIT(&bp->b_dep);
		}
		bp->b_private = mbp;
		bp->b_lblkno = bp->b_blkno = (daddr_t)(offset >> DEV_BSHIFT);
		UVMHIST_LOG(ubchist, "bp %p numout %d",
			    bp, vp->v_numoutput,0,0);
		VOP_STRATEGY(bp);
	}
	if (skipbytes) {
		UVMHIST_LOG(ubchist, "skipbytes %d", bytes, 0,0,0);
		s = splbio();
		mbp->b_resid -= skipbytes;
		if (mbp->b_resid == 0) {
			biodone(mbp);
		}
		splx(s);
	}
	if (async) {
		return 0;
	}
	if (bp != NULL) {
		error = biowait(mbp);
	}

	s = splbio();
	if (mbp->b_vp) {
		vwakeup(mbp->b_vp);
	}
	(void) buf_cleanout(mbp);
	pool_put(&bufpool, mbp);
	splx(s);

	uvm_pagermapout(kva, ap->a_count);
	if (error || !v3) {
		UVMHIST_LOG(ubchist, "returning error %d", error, 0,0,0);
		return error;
	}

	/*
	 * for a weak put, mark the range as "to be committed"
	 * and mark the pages read-only so that we will be notified
	 * to remove the pages from the "to be committed" range
	 * if they are made dirty again.
	 * for a strong put, commit the pages and remove them from the
	 * "to be committed" range.  also, mark them as writable
	 * and not cleanable with just a commit.
	 */

	lockmgr(&np->n_commitlock, LK_EXCLUSIVE, NULL, p);
	if (weak) {
		nfs_add_tobecommitted_range(vp, origoffset,
					    npages << PAGE_SHIFT);
		for (i = 0; i < npages; i++) {
			pgs[i]->flags |= PG_NEEDCOMMIT|PG_RDONLY;
		}
	} else {
		commitoff = origoffset;
		commitbytes = npages << PAGE_SHIFT;
commit:
		error = nfs_commit(vp, commitoff, commitbytes, curproc);
		nfs_del_tobecommitted_range(vp, commitoff, commitbytes);
committed:
		for (i = 0; i < npages; i++) {
			pgs[i]->flags &= ~(PG_NEEDCOMMIT|PG_RDONLY);
		}
	}
	lockmgr(&np->n_commitlock, LK_RELEASE, NULL, p);
	return error;
@


1.15.6.5
log
@Merge in trunk
@
text
@d53 1
d55 1
a55 1
#include <uvm/uvm_extern.h>
d80 2
a81 2
	int biosize, diff;
	struct buf *bp = NULL, *rabp;
a84 1
	daddr_t lbn, bn, rabn;
d86 1
a86 1
	int got_buf = 0, nra, error = 0, n = 0, on = 0, not_readin;
d116 8
a123 12
	if (np->n_flag & NMODIFIED) {
		np->n_attrstamp = 0;
		error = VOP_GETATTR(vp, &vattr, cred, p);
		if (error)
			return (error);
		np->n_mtime = vattr.va_mtime.tv_sec;
	} else {
		error = VOP_GETATTR(vp, &vattr, cred, p);
		if (error)
			return (error);
		if (np->n_mtime != vattr.va_mtime.tv_sec) {
			error = nfs_vinvalbuf(vp, V_SAVE, cred, p, 1);
d127 10
d156 14
a169 24
		lbn = uio->uio_offset / biosize;
		on = uio->uio_offset & (biosize - 1);
		bn = lbn * (biosize / DEV_BSIZE);
		not_readin = 1;

		/*
		 * Start the read ahead(s), as required.
		 */
		if (nfs_numasync > 0 && nmp->nm_readahead > 0) {
		    for (nra = 0; nra < nmp->nm_readahead &&
			(lbn + 1 + nra) * biosize < np->n_size; nra++) {
			rabn = (lbn + 1 + nra) * (biosize / DEV_BSIZE);
			if (!incore(vp, rabn)) {
			    rabp = nfs_getcacheblk(vp, rabn, biosize, p);
			    if (!rabp)
				return (EINTR);
			    if ((rabp->b_flags & (B_DELWRI | B_DONE)) == 0) {
				rabp->b_flags |= (B_READ | B_ASYNC);
				if (nfs_asyncio(rabp)) {
				    rabp->b_flags |= B_INVAL;
				    brelse(rabp);
				}
			    } else
				brelse(rabp);
a170 1
		    }
d172 2
a174 53
		/*
		 * If the block is in the cache and has the required data
		 * in a valid region, just copy it out.
		 * Otherwise, get the block and write back/read in,
		 * as required.
		 */
		if ((bp = incore(vp, bn)) &&
		    (bp->b_flags & (B_BUSY | B_WRITEINPROG)) ==
		    (B_BUSY | B_WRITEINPROG))
			got_buf = 0;
		else {
again:
			bp = nfs_getcacheblk(vp, bn, biosize, p);
			if (!bp)
				return (EINTR);
			got_buf = 1;
			if ((bp->b_flags & (B_DONE | B_DELWRI)) == 0) {
				bp->b_flags |= B_READ;
				not_readin = 0;
				error = nfs_doio(bp, p);
				if (error) {
				    brelse(bp);
				    return (error);
				}
			}
		}
		n = min((unsigned)(biosize - on), uio->uio_resid);
		diff = np->n_size - uio->uio_offset;
		if (diff < n)
			n = diff;
		if (not_readin && n > 0) {
			if (on < bp->b_validoff || (on + n) > bp->b_validend) {
				if (!got_buf) {
				    bp = nfs_getcacheblk(vp, bn, biosize, p);
				    if (!bp)
					return (EINTR);
				    got_buf = 1;
				}
				bp->b_flags |= B_INVAFTERWRITE;
				if (bp->b_dirtyend > 0) {
				    if ((bp->b_flags & B_DELWRI) == 0)
					panic("nfsbioread");
				    if (VOP_BWRITE(bp) == EINTR)
					return (EINTR);
				} else
				    brelse(bp);
				goto again;
			}
		}
		diff = (on >= bp->b_validend) ? 0 : (bp->b_validend - on);
		if (diff < n)
			n = diff;
		break;
d188 1
a188 1
		n = min(uio->uio_resid, NFS_MAXPATHLEN - bp->b_resid);
a236 1
	struct buf *bp;
d239 2
a240 2
	daddr_t lbn, bn;
	int n, on, error = 0;
d300 3
a302 5

		/*
		 * XXX make sure we aren't cached in the VM page cache
		 */
		uvm_vnp_uncache(vp);
a304 8
		lbn = uio->uio_offset / biosize;
		on = uio->uio_offset & (biosize-1);
		n = min((unsigned)(biosize - on), uio->uio_resid);
		bn = lbn * (biosize / DEV_BSIZE);
again:
		bp = nfs_getcacheblk(vp, bn, biosize, p);
		if (!bp)
			return (EINTR);
d306 26
a331 3
		if (uio->uio_offset + n > np->n_size) {
			np->n_size = uio->uio_offset + n;
			uvm_vnp_setsize(vp, (u_long)np->n_size);
d333 2
a334 12

		/*
		 * If the new write will leave a contiguous dirty
		 * area, just update the b_dirtyoff and b_dirtyend,
		 * otherwise force a write rpc of the old dirty area.
		 */
		if (bp->b_dirtyend > 0 &&
		    (on > bp->b_dirtyend || (on + n) < bp->b_dirtyoff)) {
			bp->b_proc = p;
			if (VOP_BWRITE(bp) == EINTR)
				return (EINTR);
			goto again;
a335 2

		error = uiomove((char *)bp->b_data + on, n, uio);
d337 1
a337 10
			bp->b_flags |= B_ERROR;
			brelse(bp);
			return (error);
		}
		if (bp->b_dirtyend > 0) {
			bp->b_dirtyoff = min(on, bp->b_dirtyoff);
			bp->b_dirtyend = max((on + n), bp->b_dirtyend);
		} else {
			bp->b_dirtyoff = on;
			bp->b_dirtyend = on + n;
d339 2
a340 32
		if (bp->b_validend == 0 || bp->b_validend < bp->b_dirtyoff ||
		    bp->b_validoff > bp->b_dirtyend) {
			bp->b_validoff = bp->b_dirtyoff;
			bp->b_validend = bp->b_dirtyend;
		} else {
			bp->b_validoff = min(bp->b_validoff, bp->b_dirtyoff);
			bp->b_validend = max(bp->b_validend, bp->b_dirtyend);
		}

		/*
		 * Since this block is being modified, it must be written
		 * again and not just committed.
		 */
		bp->b_flags &= ~B_NEEDCOMMIT;

		/*
		 * If the lease is non-cachable or IO_SYNC do bwrite().
		 */
		if (ioflag & IO_SYNC) {
			bp->b_proc = p;
			error = VOP_BWRITE(bp);
			if (error)
				return (error);
		} else if ((n + on) == biosize) {
			bp->b_proc = (struct proc *)0;
			bp->b_flags |= B_ASYNC;
			(void)nfs_writebp(bp, 0);
		} else {
			bdwrite(bp);
		}
	} while (uio->uio_resid > 0 && n > 0);
	return (0);
d357 1
a357 1
	struct buf *bp;
d362 3
a364 3
		while (bp == (struct buf *)0) {
			if (nfs_sigintr(nmp, (struct nfsreq *)0, p))
				return ((struct buf *)0);
d384 1
a384 1
	struct nfsnode *np = VTONFS(vp);
d404 1
a404 1
		if (error && intrflg && nfs_sigintr(nmp, (struct nfsreq *)0, p))
d414 1
a414 1
		if (intrflg && nfs_sigintr(nmp, (struct nfsreq *)0, p)) {
d441 1
a441 1
	int i,s;
d445 1
a445 1
	for (i = 0; i < NFS_MAXASYNCDAEMON; i++)
a446 4
		if ((bp->b_flags & B_READ) == 0) {
			bp->b_flags |= B_WRITEINPROG;
		}
	
d448 1
a448 1
		nfs_iodwant[i] = (struct proc *)0;
d452 1
d454 1
a454 19
	/*
	 * If it is a read or a write already marked B_WRITEINPROG or B_NOCACHE
	 * return EIO so the process will call nfs_doio() and do it
	 * synchronously.
	 */
	if (bp->b_flags & (B_READ | B_WRITEINPROG | B_NOCACHE))
		return (EIO);

	/*
	 * Just turn the async write into a delayed write, instead of
	 * doing in synchronously. Hopefully, at least one of the nfsiods
	 * is currently doing a write for this file and will pick up the
	 * delayed writes before going back to sleep.
	 */
	s = splbio();
	buf_dirty(bp);
	splx(s);
	biodone(bp);
	return (0);
d466 2
a467 2
	struct uio *uiop;
	struct vnode *vp;
d470 1
a470 1
	int s, error = 0, diff, len, iomode, must_commit = 0;
d517 1
a517 3
		if (!error) {
		    bp->b_validoff = 0;
		    if (uiop->uio_resid) {
d528 3
a530 7
			    len = min(len, uiop->uio_resid);
			    bzero((char *)bp->b_data + diff, len);
			    bp->b_validend = diff + len;
			} else
			    bp->b_validend = diff;
		    } else
			bp->b_validend = bp->b_bcount;
d547 1
a547 1
	    };
d553 3
a555 5
	    io.iov_len = uiop->uio_resid = bp->b_dirtyend
		- bp->b_dirtyoff;
	    uiop->uio_offset = ((off_t)bp->b_blkno) * DEV_BSIZE
		+ bp->b_dirtyoff;
	    io.iov_base = (char *)bp->b_data + bp->b_dirtyoff;
d558 1
a558 9
	    if ((bp->b_flags & (B_ASYNC | B_NEEDCOMMIT | B_NOCACHE)) == B_ASYNC)
		iomode = NFSV3WRITE_UNSTABLE;
	    else
		iomode = NFSV3WRITE_FILESYNC;
	    bp->b_flags |= B_WRITEINPROG;
#ifdef fvdl_debug
	    printf("nfs_doio(%x): bp %x doff %d dend %d\n", 
		vp, bp, bp->b_dirtyoff, bp->b_dirtyend);
#endif
a559 33
	    if (!error && iomode == NFSV3WRITE_UNSTABLE)
		bp->b_flags |= B_NEEDCOMMIT;
	    else
		bp->b_flags &= ~B_NEEDCOMMIT;
	    bp->b_flags &= ~B_WRITEINPROG;

	    /*
	     * For an interrupted write, the buffer is still valid and the
	     * write hasn't been pushed to the server yet, so we can't set
	     * B_ERROR and report the interruption by setting B_EINTR. For
	     * the B_ASYNC case, B_EINTR is not relevant, so the rpc attempt
	     * is essentially a noop.
	     * For the case of a V3 write rpc not being committed to stable
	     * storage, the block is still dirty and requires either a commit
	     * rpc or another write rpc with iomode == NFSV3WRITE_FILESYNC
	     * before the block is reused. This is indicated by setting the
	     * B_DELWRI and B_NEEDCOMMIT flags.
	     */
	    if (error == EINTR || (!error && (bp->b_flags & B_NEEDCOMMIT))) {
		    s = splbio();
		    buf_dirty(bp);
		    splx(s);

		    if (!(bp->b_flags & B_ASYNC) && error)
			    bp->b_flags |= B_EINTR;
	    } else {
		if (error) {
		    bp->b_flags |= B_ERROR;
		    bp->b_error = np->n_error = error;
		    np->n_flag |= NWRITEERR;
		}
		bp->b_dirtyoff = bp->b_dirtyend = 0;
	    }
d566 594
@


1.15.6.6
log
@Sync the SMP branch with 3.3
@
text
@d568 1
a569 1
	splx(s);
a728 1
	s = splbio();
a729 1
	splx(s);
@


1.15.6.7
log
@merge the trunk so we will get the genfs and locking fixes
@
text
@a65 1
struct nfs_bufqhead nfs_bufq;
@


1.15.6.8
log
@Sync SMP branch to -current
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.15.6.7 2003/05/16 00:29:45 niklas Exp $	*/
d19 5
a23 1
 * 3. Neither the name of the University nor the names of its contributors
@


1.14
log
@More splbio()'s added so that reassignbuf can do its thing.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.13 1997/11/06 05:59:00 csapuntz Exp $	*/
d470 3
d474 1
d517 3
d521 1
@


1.13
log
@Updates for VFS Lite 2 + soft update.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.12 1997/10/06 20:20:44 deraadt Exp $	*/
d704 1
a704 1
	register int i;
d744 2
d747 1
d766 1
a766 1
	int error = 0, diff, len, iomode, must_commit = 0;
d913 2
a914 1
		if (bp->b_flags & B_ASYNC)
d916 2
@


1.12
log
@back out vfs lite2 till after 2.2
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.10 1996/07/27 11:10:11 deraadt Exp $	*/
a742 10
	if (bp->b_flags & B_DELWRI)
	    TAILQ_REMOVE(&bdirties, bp, b_synclist);
	TAILQ_INSERT_TAIL(&bdirties, bp, b_synclist);
	bp->b_synctime = time.tv_sec + 30;
	if (bdirties.tqh_first == bp) {
		untimeout((void (*)__P((void *)))wakeup,
			  &bdirties);
		timeout((void (*)__P((void *)))wakeup,
			&bdirties, 30 * hz);
	}
a902 10
		if (bp->b_flags & B_DELWRI)
		    TAILQ_REMOVE(&bdirties, bp, b_synclist);
		TAILQ_INSERT_TAIL(&bdirties, bp, b_synclist);
		bp->b_synctime = time.tv_sec + 30;
		if (bdirties.tqh_first == bp) {
		    untimeout((void (*)__P((void *)))wakeup,
			      &bdirties);
		    timeout((void (*)__P((void *)))wakeup,
			    &bdirties, 30 * hz);
		}
@


1.11
log
@VFS Lite2 Changes
@
text
@d743 10
d913 10
@


1.10
log
@fvdl; Don't mistake a non-async block that needs to be commited for an
interrupted write.
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.9 1996/07/21 08:05:37 tholo Exp $	*/
a742 10
	if (bp->b_flags & B_DELWRI)
	    TAILQ_REMOVE(&bdirties, bp, b_synclist);
	TAILQ_INSERT_TAIL(&bdirties, bp, b_synclist);
	bp->b_synctime = time.tv_sec + 30;
	if (bdirties.tqh_first == bp) {
		untimeout((void (*)__P((void *)))wakeup,
			  &bdirties);
		timeout((void (*)__P((void *)))wakeup,
			&bdirties, 30 * hz);
	}
a902 10
		if (bp->b_flags & B_DELWRI)
		    TAILQ_REMOVE(&bdirties, bp, b_synclist);
		TAILQ_INSERT_TAIL(&bdirties, bp, b_synclist);
		bp->b_synctime = time.tv_sec + 30;
		if (bdirties.tqh_first == bp) {
		    untimeout((void (*)__P((void *)))wakeup,
			      &bdirties);
		    timeout((void (*)__P((void *)))wakeup,
			    &bdirties, 30 * hz);
		}
@


1.9
log
@Ensure we never use more than one callout table slot
@
text
@d1 2
a2 2
/*	$OpenBSD: nfs_bio.c,v 1.8 1996/06/14 04:41:07 tholo Exp $	*/
/*	$NetBSD: nfs_bio.c,v 1.25.4.1 1996/05/25 22:40:32 fvdl Exp $	*/
d932 1
a932 1
		else
@


1.8
log
@Keep dirty list used by in-kernel update(8) in sync with buffers
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.7 1996/05/28 13:44:07 deraadt Exp $	*/
d747 6
a752 3
	if (bdirties.tqh_first == bp)
	    timeout((void (*)__P((void *)))wakeup,
		    &bdirties, 30 * hz);
d917 3
a919 1
		if (bdirties.tqh_first == bp)
d922 1
@


1.7
log
@sync
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.5 1996/04/17 04:50:24 mickey Exp $	*/
d743 7
d910 7
@


1.6
log
@partial sync with netbsd 960418, more to come
@
text
@d2 1
a2 1
/*	$NetBSD: nfs_bio.c,v 1.25 1996/02/29 20:26:16 fvdl Exp $	*/
d178 1
a178 1
	     * Don't cache magic amd symlinks.
@


1.5
log
@Minor cleanups. Checked against Lite2.
(NetBSD's was really just a Lite2's, but w/ 64bit support)
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.4 1996/03/31 13:15:32 mickey Exp $	*/
d184 1
a184 1
			return nfs_readrpc(vp, uio, cred);
d186 1
a186 1
			return nfs_readlinkrpc(vp, uio, cred);
d194 1
a194 1
	    baddr = NULL;
d595 1
a595 1
			bp->b_proc = (struct proc *)NULL;
d624 3
a626 3
		while (bp == NULL) {
			if (nfs_sigintr(nmp, (struct nfsreq *)NULL, p))
				return ((struct buf *)NULL);
d666 1
a666 1
		if (error && intrflg && nfs_sigintr(nmp, (struct nfsreq *)NULL, p))
d676 1
a676 1
		if (intrflg && nfs_sigintr(nmp, (struct nfsreq *)NULL, p)) {
d724 1
a724 1
		nfs_iodwant[i] = (struct proc *)NULL;
d843 1
a843 1
		uiop->uio_offset = (off_t)NULL;
@


1.4
log
@From NetBSD: NFSv3 import (tomorrow's Net's kernel)
Open's patches kept in. i'll possibly take a look at Lite2 soon,
is there smth usefull ?..
@
text
@d1 1
a1 1
/*	$OpenBSD: nfs_bio.c,v 1.3 1996/02/29 09:24:48 niklas Exp $	*/
d194 1
a194 1
	    baddr = (caddr_t)0;
d595 1
a595 1
			bp->b_proc = (struct proc *)0;
d624 3
a626 3
		while (bp == (struct buf *)0) {
			if (nfs_sigintr(nmp, (struct nfsreq *)0, p))
				return ((struct buf *)0);
d666 1
a666 1
		if (error && intrflg && nfs_sigintr(nmp, (struct nfsreq *)0, p))
d676 1
a676 1
		if (intrflg && nfs_sigintr(nmp, (struct nfsreq *)0, p)) {
d724 1
a724 1
		nfs_iodwant[i] = (struct proc *)0;
d843 1
a843 1
		uiop->uio_offset = (off_t)0;
@


1.3
log
@From NetBSD: merge with 960217 (still NFSv2)
@
text
@d1 2
a2 2
/*	$OpenBSD$	*/
/*	$NetBSD: nfs_bio.c,v 1.23 1996/02/09 21:48:19 christos Exp $	*/
d39 1
a39 1
 *	@@(#)nfs_bio.c	8.5 (Berkeley) 1/4/94
d42 1
d46 1
a53 1
#include <sys/signalvar.h>
a56 1
#include <nfs/nfsnode.h>
d58 1
a58 1
#include <nfs/nfsv2.h>
d62 1
d67 1
d81 1
a81 1
	register int biosize, diff;
d85 1
a85 1
	struct nfsmount *nmp;
a89 3
#ifdef lint
	ioflag = ioflag;
#endif /* lint */
d96 1
a96 1
	if (uio->uio_offset < 0 && vp->v_type != VDIR)
d98 3
a100 1
	nmp = VFSTONFS(vp->v_mount);
a101 1
	p = uio->uio_procp;
a113 2
	 * The mount flag NFSMNT_MYWRITE says "Assume that my writes are
	 * the ones changing the modify time.
d121 4
a124 2
			if ((nmp->nm_flag & NFSMNT_MYWRITE) == 0 ||
			     vp->v_type != VREG) {
a129 1
			np->n_direofoffset = 0;
d135 2
a136 1
			if ((error = VOP_GETATTR(vp, &vattr, cred, p)) != 0)
d139 2
a140 1
				np->n_direofoffset = 0;
d154 1
a154 1
		if (NQNFS_CKINVALID(vp, np, NQL_READ)) {
d156 1
a156 1
			error = nqnfs_getlease(vp, NQL_READ, cred, p);
d163 2
a164 4
			if (vp->v_type == VDIR) {
			    np->n_direofoffset = 0;
			    cache_purge(vp);
			}
d171 1
a171 2
		    np->n_direofoffset = 0;
		    cache_purge(vp);
d184 1
a184 2
			error = nfs_readrpc(vp, uio, cred);
			break;
d186 1
a186 2
			error = nfs_readlinkrpc(vp, uio, cred);
			break;
a187 8
			error = nfs_readdirrpc(vp, uio, cred);
			break;
		case VCHR:
		case VSOCK:
		case VFIFO:
		case VBAD:
		case VNON:
		case VBLK:
d189 3
a192 1
		return (error);
d199 1
a199 1
		on = uio->uio_offset & (biosize-1);
d206 1
a206 2
		if (nfs_numasync > 0 && nmp->nm_readahead > 0 &&
		    lbn == vp->v_lastr + 1) {
d264 1
a264 1
				bp->b_flags |= B_INVAL;
d287 2
a288 1
			if ((error = nfs_doio(bp, cred, p)) != 0) {
d298 1
a298 1
		if (uio->uio_resid < NFS_DIRBLKSIZ)
d301 3
a303 2
		bn = (daddr_t)uio->uio_offset;
		bp = nfs_getcacheblk(vp, bn, NFS_DIRBLKSIZ, p);
d305 1
a305 1
			return (EINTR);
d307 24
a330 4
			bp->b_flags |= B_READ;
			if ((error = nfs_doio(bp, cred, p)) != 0) {
				brelse(bp);
				return (error);
d332 3
d340 1
a340 1
		 *  directory offset cookie of the next block.
a341 1
		rabn = bp->b_blkno;
d343 5
a347 3
		    rabn != 0 && rabn != np->n_direofoffset &&
		    !incore(vp, rabn)) {
			rabp = nfs_getcacheblk(vp, rabn, NFS_DIRBLKSIZ, p);
d359 1
a359 2
		on = 0;
		n = min(uio->uio_resid, NFS_DIRBLKSIZ - bp->b_resid);
d362 2
a363 6
	    case VBAD:
	    case VSOCK:
	    case VCHR:
	    case VBLK:
	    case VNON:
	    case VFIFO:
d373 2
d379 2
a380 1
		uio->uio_offset = bp->b_blkno;
d382 3
a384 9
	    case VREG:
	    case VBAD:
	    case VFIFO:
	    case VSOCK:
	    case VCHR:
	    case VBLK:
	    case VNON:
		break;
	    };
d399 1
a399 1
		struct vnode a_vp;
d413 1
a413 1
	struct nfsmount *nmp;
d415 1
a415 1
	int n, on, error = 0;
d429 2
a445 1
	nmp = VFSTONFS(vp->v_mount);
a473 1
		 * If non-cachable, just do the rpc
d476 1
a476 1
		    NQNFS_CKINVALID(vp, np, NQL_WRITE)) {
d478 1
a478 1
				error = nqnfs_getlease(vp, NQL_WRITE, cred, p);
d490 7
a496 2
		if (np->n_flag & NQNFSNONCACHE)
			return (nfs_writerpc(vp, uio, cred, ioflag));
d534 1
a534 1
		    NQNFS_CKINVALID(vp, np, NQL_WRITE)) {
d536 1
a536 1
				error = nqnfs_getlease(vp, NQL_WRITE, cred, p);
a564 1
#ifndef notdef
d573 6
a578 6
#else
		bp->b_validoff = bp->b_dirtyoff;
		bp->b_validend = bp->b_dirtyend;
#endif
		if (ioflag & IO_APPEND)
			bp->b_flags |= B_APPENDWRITE;
d585 2
a586 1
			if ((error = VOP_BWRITE(bp)) != 0)
d588 5
d596 3
a598 2
			bawrite(bp);
		} else
d600 1
d716 1
d728 19
a746 1
	return (EIO);
d763 1
a763 1
	int error = 0, diff, len;
d786 1
a786 1
	    uiop->uio_offset = bp->b_blkno * DEV_BSIZE;
d792 1
d795 1
a795 1
		error = nfs_writerpc(vp, uiop, cr, 0);
d807 1
a807 1
		uiop->uio_offset = bp->b_blkno * DEV_BSIZE;
d820 1
a820 1
			len = np->n_size - (bp->b_blkno * DEV_BSIZE
d833 1
a833 1
			  NQNFS_CKINVALID(vp, np, NQL_READ) &&
d843 1
a843 1
		uiop->uio_offset = 0;
a847 1
		uiop->uio_offset = bp->b_lblkno;
d849 11
a859 8
		if (VFSTONFS(vp->v_mount)->nm_flag & NFSMNT_NQNFS)
		    error = nfs_readdirlookrpc(vp, uiop, cr);
		else
		    error = nfs_readdirrpc(vp, uiop, cr);
		/*
		 * Save offset cookie in b_blkno.
		 */
		bp->b_blkno = uiop->uio_offset;
a860 6
	    case VNON:
	    case VBLK:
	    case VCHR:
	    case VFIFO:
	    case VBAD:
	    case VSOCK:
d869 1
a869 1
	    uiop->uio_offset = (bp->b_blkno * DEV_BSIZE)
d874 12
a885 2
	    if (bp->b_flags & B_APPENDWRITE)
		error = nfs_writerpc(vp, uiop, cr, IO_APPEND);
d887 2
a888 2
		error = nfs_writerpc(vp, uiop, cr, 0);
	    bp->b_flags &= ~(B_WRITEINPROG | B_APPENDWRITE);
d896 5
d902 1
a902 2
	    if (error == EINTR) {
		bp->b_flags &= ~B_INVAL;
d924 2
@


1.2
log
@graichen@@freebsd.org: fixed -type:=direct mounts in amd
@
text
@d1 2
a2 1
/*	$NetBSD: nfs_bio.c,v 1.21 1995/07/24 21:20:46 cgd Exp $	*/
d51 2
d62 1
a63 1
struct buf *incore(), *nfs_getcacheblk();
d71 1
d80 1
a80 1
	struct buf *bp, *rabp;
d86 1
a86 1
	int got_buf, nra, error = 0, n, on, not_readin;
d125 2
a126 1
				if (error = nfs_vinvalbuf(vp, V_SAVE, cred, p, 1))
d131 2
a132 1
			if (error = VOP_GETATTR(vp, &vattr, cred, p))
d134 1
a134 1
			np->n_mtime = vattr.va_mtime.ts_sec;
d136 1
a136 1
			if (error = VOP_GETATTR(vp, &vattr, cred, p))
d138 1
a138 1
			if (np->n_mtime != vattr.va_mtime.ts_sec) {
d140 2
a141 1
				if (error = nfs_vinvalbuf(vp, V_SAVE, cred, p, 1))
d143 1
a143 1
				np->n_mtime = vattr.va_mtime.ts_sec;
d166 2
a167 1
			if (error = nfs_vinvalbuf(vp, V_SAVE, cred, p, 1))
d174 2
a175 1
		    if (error = nfs_vinvalbuf(vp, V_SAVE, cred, p, 1))
d194 7
d256 2
a257 1
				if (error = nfs_doio(bp, cred, p)) {
d298 1
a298 1
			if (error = nfs_doio(bp, cred, p)) {
d317 1
a317 1
			if (error = nfs_doio(bp, cred, p)) {
d348 7
d369 8
d387 4
a390 1
nfs_write(ap)
d392 1
a392 1
		struct vnode *a_vp;
d396 1
a396 2
	} */ *ap;
{
d425 2
a426 1
			if (error = nfs_vinvalbuf(vp, V_SAVE, cred, p, 1))
d431 2
a432 1
			if (error = VOP_GETATTR(vp, &vattr, cred, p))
d477 2
a478 1
				if (error = nfs_vinvalbuf(vp, V_SAVE, cred, p, 1))
d533 2
a534 1
				if (error = nfs_vinvalbuf(vp, V_SAVE, cred, p, 1))
d540 2
a541 1
		if (error = uiomove((char *)bp->b_data + on, n, uio)) {
d574 1
a574 1
			if (error = VOP_BWRITE(bp))
d619 1
d680 1
d718 1
a718 1
	struct cred *cr;
d725 1
a725 1
	int error, diff, len;
d797 1
a797 1
			  np->n_mtime != np->n_vattr.va_mtime.ts_sec))) {
d820 6
@


1.1
log
@Initial revision
@
text
@d170 5
a174 1
	    if (np->n_flag & NQNFSNONCACHE) {
@


1.1.1.1
log
@initial import of NetBSD tree
@
text
@@
