head	1.57;
access;
symbols
	OPENBSD_6_1:1.53.0.4
	OPENBSD_6_1_BASE:1.53
	OPENBSD_6_0:1.50.0.4
	OPENBSD_6_0_BASE:1.50
	OPENBSD_5_9:1.14.0.2
	OPENBSD_5_9_BASE:1.14
	OPENBSD_5_8:1.10.0.6
	OPENBSD_5_8_BASE:1.10
	OPENBSD_5_7:1.10.0.2
	OPENBSD_5_7_BASE:1.10
	OPENBSD_5_6:1.6.0.4
	OPENBSD_5_6_BASE:1.6;
locks; strict;
comment	@ * @;


1.57
date	2017.05.27.19.27.45;	author sf;	state Exp;
branches;
next	1.56;
commitid	4o3G74ePme5n8QRE;

1.56
date	2017.05.27.12.40.51;	author sf;	state Exp;
branches;
next	1.55;
commitid	uLnBuPkrKyXpxRQE;

1.55
date	2017.05.12.22.16.54;	author jcs;	state Exp;
branches;
next	1.54;
commitid	dadX8YelWpaOibYT;

1.54
date	2017.04.08.02.57.25;	author deraadt;	state Exp;
branches;
next	1.53;
commitid	6s3MfY9d6ZKdL2Uz;

1.53
date	2016.11.15.12.17.42;	author mpi;	state Exp;
branches;
next	1.52;
commitid	cNn9CPZpe109uZLi;

1.52
date	2016.11.15.12.01.11;	author mpi;	state Exp;
branches;
next	1.51;
commitid	m2qYa3eCTcf09GyS;

1.51
date	2016.10.25.06.20.41;	author dlg;	state Exp;
branches;
next	1.50;
commitid	61P1f8kxIbXrwi3t;

1.50
date	2016.05.20.11.11.05;	author dlg;	state Exp;
branches;
next	1.49;
commitid	2jxp74nEhEdV5hcR;

1.49
date	2016.04.18.05.59.50;	author dlg;	state Exp;
branches;
next	1.48;
commitid	1vKkeTLIfqva0oNC;

1.48
date	2016.04.14.12.08.21;	author dlg;	state Exp;
branches;
next	1.47;
commitid	Xkj2n6GTwPNcxuYq;

1.47
date	2016.04.14.11.20.19;	author dlg;	state Exp;
branches;
next	1.46;
commitid	Vq0e8eExNL2VtwSi;

1.46
date	2016.04.14.11.18.32;	author dlg;	state Exp;
branches;
next	1.45;
commitid	xizj9bAz5c6r27Bv;

1.45
date	2016.04.14.10.26.33;	author dlg;	state Exp;
branches;
next	1.44;
commitid	uGSl7QAPYN2XuA7z;

1.44
date	2016.04.14.09.07.30;	author dlg;	state Exp;
branches;
next	1.43;
commitid	QZGeYSbPfCSSH2Lz;

1.43
date	2016.04.14.06.23.20;	author dlg;	state Exp;
branches;
next	1.42;
commitid	bxlR8fFc8x88V6Fk;

1.42
date	2016.04.14.06.20.34;	author dlg;	state Exp;
branches;
next	1.41;
commitid	wEKyguqyom8e98r2;

1.41
date	2016.04.14.06.17.14;	author dlg;	state Exp;
branches;
next	1.40;
commitid	1yNndykTjczKZ0VM;

1.40
date	2016.04.14.06.16.36;	author dlg;	state Exp;
branches;
next	1.39;
commitid	7Plk7wGBG4iq8YC7;

1.39
date	2016.04.14.06.10.49;	author dlg;	state Exp;
branches;
next	1.38;
commitid	Xh7tZOrqYpYLH386;

1.38
date	2016.04.14.06.06.46;	author dlg;	state Exp;
branches;
next	1.37;
commitid	ZiJBZJtghOe1uU3C;

1.37
date	2016.04.14.03.04.36;	author dlg;	state Exp;
branches;
next	1.36;
commitid	jHiLxhxxsSIiG4wj;

1.36
date	2016.04.14.00.26.38;	author dlg;	state Exp;
branches;
next	1.35;
commitid	oPwq9v9MDGxufbN7;

1.35
date	2016.04.14.00.19.57;	author dlg;	state Exp;
branches;
next	1.34;
commitid	wArKWxTBZ3EuAzvc;

1.34
date	2016.04.14.00.12.51;	author dlg;	state Exp;
branches;
next	1.33;
commitid	wallQpr86FrNzep3;

1.33
date	2016.04.13.13.39.16;	author dlg;	state Exp;
branches;
next	1.32;
commitid	aRObdqFPT6gzhsrW;

1.32
date	2016.04.13.13.17.24;	author dlg;	state Exp;
branches;
next	1.31;
commitid	VLrqhwkltIFocNRY;

1.31
date	2016.04.13.13.16.32;	author dlg;	state Exp;
branches;
next	1.30;
commitid	3JpQeEixO7n5odKp;

1.30
date	2016.04.13.13.13.27;	author dlg;	state Exp;
branches;
next	1.29;
commitid	3InhGkV2omnvwrKi;

1.29
date	2016.04.13.13.09.36;	author dlg;	state Exp;
branches;
next	1.28;
commitid	m3RnFaFSCdPGQ6bt;

1.28
date	2016.04.13.13.05.10;	author dlg;	state Exp;
branches;
next	1.27;
commitid	gGgwnvndpfY7HT1K;

1.27
date	2016.04.13.12.59.28;	author dlg;	state Exp;
branches;
next	1.26;
commitid	hSDK79xxi2SoEcFT;

1.26
date	2016.04.13.12.42.09;	author dlg;	state Exp;
branches;
next	1.25;
commitid	DBhNh9Z5uLyPUDps;

1.25
date	2016.04.13.12.28.57;	author dlg;	state Exp;
branches;
next	1.24;
commitid	PZv26Yhf8dtGd4O9;

1.24
date	2016.04.13.12.21.15;	author dlg;	state Exp;
branches;
next	1.23;
commitid	JE9d4x2wXwjmlBoZ;

1.23
date	2016.04.13.12.14.12;	author dlg;	state Exp;
branches;
next	1.22;
commitid	7Io3f5uOVuqCNxgI;

1.22
date	2016.04.13.12.04.20;	author dlg;	state Exp;
branches;
next	1.21;
commitid	eZedFrZ2QDez5vtN;

1.21
date	2016.04.13.11.56.50;	author dlg;	state Exp;
branches;
next	1.20;
commitid	5SxNZfUJj1bkRSpN;

1.20
date	2016.04.13.11.54.33;	author dlg;	state Exp;
branches;
next	1.19;
commitid	Kom3tspBxpOwesLJ;

1.19
date	2016.04.13.11.51.56;	author dlg;	state Exp;
branches;
next	1.18;
commitid	XljZUvI1uS5lU5dF;

1.18
date	2016.04.13.11.48.38;	author dlg;	state Exp;
branches;
next	1.17;
commitid	YFcRCNUmGDmd1GzT;

1.17
date	2016.04.13.11.45.06;	author dlg;	state Exp;
branches;
next	1.16;
commitid	Q4ImrJYNVGloU59w;

1.16
date	2016.04.13.11.42.04;	author dlg;	state Exp;
branches;
next	1.15;
commitid	x4swAmnLAg0uqhlJ;

1.15
date	2016.04.12.10.20.25;	author dlg;	state Exp;
branches;
next	1.14;
commitid	54ISSsIHXEjIx6Hh;

1.14
date	2016.01.15.06.38.33;	author dlg;	state Exp;
branches;
next	1.13;
commitid	iA7xnOkkAO1q1VUP;

1.13
date	2016.01.15.06.34.19;	author dlg;	state Exp;
branches;
next	1.12;
commitid	NgssZfyeQPc5HJUb;

1.12
date	2016.01.15.03.28.41;	author dlg;	state Exp;
branches;
next	1.11;
commitid	fjTkAqGnFy0gUS9H;

1.11
date	2016.01.15.03.12.04;	author dlg;	state Exp;
branches;
next	1.10;
commitid	GrwVJBkxVaVAF71a;

1.10
date	2014.11.04.12.48.22;	author dlg;	state Exp;
branches;
next	1.9;
commitid	fxim6ObiZVQgocKQ;

1.9
date	2014.11.04.12.41.34;	author dlg;	state Exp;
branches;
next	1.8;
commitid	Zw7ri7pKfma0ds6O;

1.8
date	2014.09.12.06.54.38;	author dlg;	state Exp;
branches;
next	1.7;
commitid	m2lnXWInUqXzSO9W;

1.7
date	2014.09.12.06.34.14;	author dlg;	state Exp;
branches;
next	1.6;
commitid	f1DJK0Wwdd2NePE3;

1.6
date	2014.07.13.23.10.23;	author deraadt;	state Exp;
branches;
next	1.5;
commitid	JtO5uXxVcnZfhUkR;

1.5
date	2014.07.12.18.48.17;	author tedu;	state Exp;
branches;
next	1.4;
commitid	I19imNlAX05zJOED;

1.4
date	2014.04.16.01.28.02;	author dlg;	state Exp;
branches;
next	1.3;

1.3
date	2014.04.16.00.26.59;	author dlg;	state Exp;
branches;
next	1.2;

1.2
date	2014.04.15.10.28.07;	author dlg;	state Exp;
branches;
next	1.1;

1.1
date	2014.04.12.05.06.58;	author dlg;	state Exp;
branches;
next	;


desc
@@


1.57
log
@nvme: Don't set prp1 for DEL_IOCQ

NVM_ADMIN_DEL_IOCQ does not need prp1 (just as NVM_ADMIN_DEL_IOSQ).
Remove what is likely a cut'n'paste error from the *_ADD_* code.

tested by claudio@@
ok jmatthew@@
@
text
@/*	$OpenBSD: nvme.c,v 1.56 2017/05/27 12:40:51 sf Exp $ */

/*
 * Copyright (c) 2014 David Gwynne <dlg@@openbsd.org>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/buf.h>
#include <sys/kernel.h>
#include <sys/malloc.h>
#include <sys/device.h>
#include <sys/queue.h>
#include <sys/mutex.h>
#include <sys/pool.h>

#include <machine/bus.h>

#include <scsi/scsi_all.h>
#include <scsi/scsi_disk.h>
#include <scsi/scsiconf.h>

#include <dev/ic/nvmereg.h>
#include <dev/ic/nvmevar.h>

struct cfdriver nvme_cd = {
	NULL,
	"nvme",
	DV_DULL
};

int	nvme_ready(struct nvme_softc *, u_int32_t);
int	nvme_enable(struct nvme_softc *, u_int);
int	nvme_disable(struct nvme_softc *);
int	nvme_shutdown(struct nvme_softc *);
int	nvme_resume(struct nvme_softc *);

void	nvme_dumpregs(struct nvme_softc *);
int	nvme_identify(struct nvme_softc *, u_int);
void	nvme_fill_identify(struct nvme_softc *, struct nvme_ccb *, void *);

int	nvme_ccbs_alloc(struct nvme_softc *, u_int);
void	nvme_ccbs_free(struct nvme_softc *);

void *	nvme_ccb_get(void *);
void	nvme_ccb_put(void *, void *);

int	nvme_poll(struct nvme_softc *, struct nvme_queue *, struct nvme_ccb *,
	    void (*)(struct nvme_softc *, struct nvme_ccb *, void *));
void	nvme_poll_fill(struct nvme_softc *, struct nvme_ccb *, void *);
void	nvme_poll_done(struct nvme_softc *, struct nvme_ccb *,
	    struct nvme_cqe *);
void	nvme_sqe_fill(struct nvme_softc *, struct nvme_ccb *, void *);
void	nvme_empty_done(struct nvme_softc *, struct nvme_ccb *,
	    struct nvme_cqe *);

struct nvme_queue *
	nvme_q_alloc(struct nvme_softc *, u_int16_t, u_int, u_int);
int	nvme_q_create(struct nvme_softc *, struct nvme_queue *);
int	nvme_q_reset(struct nvme_softc *, struct nvme_queue *);
int	nvme_q_delete(struct nvme_softc *, struct nvme_queue *);
void	nvme_q_submit(struct nvme_softc *,
	    struct nvme_queue *, struct nvme_ccb *,
	    void (*)(struct nvme_softc *, struct nvme_ccb *, void *));
int	nvme_q_complete(struct nvme_softc *, struct nvme_queue *);
void	nvme_q_free(struct nvme_softc *, struct nvme_queue *);

struct nvme_dmamem *
	nvme_dmamem_alloc(struct nvme_softc *, size_t);
void	nvme_dmamem_free(struct nvme_softc *, struct nvme_dmamem *);
void	nvme_dmamem_sync(struct nvme_softc *, struct nvme_dmamem *, int);

void	nvme_scsi_cmd(struct scsi_xfer *);
int	nvme_scsi_probe(struct scsi_link *);
void	nvme_scsi_free(struct scsi_link *);

struct scsi_adapter nvme_switch = {
	nvme_scsi_cmd,		/* cmd */
	scsi_minphys,		/* minphys */
	nvme_scsi_probe,	/* dev probe */
	nvme_scsi_free,		/* dev free */
	NULL,			/* ioctl */
};

void	nvme_scsi_io(struct scsi_xfer *, int);
void	nvme_scsi_io_fill(struct nvme_softc *, struct nvme_ccb *, void *);
void	nvme_scsi_io_done(struct nvme_softc *, struct nvme_ccb *,
	    struct nvme_cqe *);

void	nvme_scsi_sync(struct scsi_xfer *);
void	nvme_scsi_sync_fill(struct nvme_softc *, struct nvme_ccb *, void *);
void	nvme_scsi_sync_done(struct nvme_softc *, struct nvme_ccb *,
	    struct nvme_cqe *);

void	nvme_scsi_inq(struct scsi_xfer *);
void	nvme_scsi_inquiry(struct scsi_xfer *);
void	nvme_scsi_capacity16(struct scsi_xfer *);
void	nvme_scsi_capacity(struct scsi_xfer *);

#define nvme_read4(_s, _r) \
	bus_space_read_4((_s)->sc_iot, (_s)->sc_ioh, (_r))
#define nvme_write4(_s, _r, _v) \
	bus_space_write_4((_s)->sc_iot, (_s)->sc_ioh, (_r), (_v))
/*
 * Some controllers, at least Apple NVMe, always require split
 * transfers, so don't use bus_space_{read,write}_8() on LP64.
 */
static inline u_int64_t
nvme_read8(struct nvme_softc *sc, bus_size_t r)
{
	u_int64_t v;
	u_int32_t *a = (u_int32_t *)&v;

#if _BYTE_ORDER == _LITTLE_ENDIAN
	a[0] = nvme_read4(sc, r);
	a[1] = nvme_read4(sc, r + 4);
#else /* _BYTE_ORDER == _LITTLE_ENDIAN */
	a[1] = nvme_read4(sc, r);
	a[0] = nvme_read4(sc, r + 4);
#endif

	return (v);
}

static inline void
nvme_write8(struct nvme_softc *sc, bus_size_t r, u_int64_t v)
{
	u_int32_t *a = (u_int32_t *)&v;

#if _BYTE_ORDER == _LITTLE_ENDIAN
	nvme_write4(sc, r, a[0]);
	nvme_write4(sc, r + 4, a[1]);
#else /* _BYTE_ORDER == _LITTLE_ENDIAN */
	nvme_write4(sc, r, a[1]);
	nvme_write4(sc, r + 4, a[0]);
#endif
}
#define nvme_barrier(_s, _r, _l, _f) \
	bus_space_barrier((_s)->sc_iot, (_s)->sc_ioh, (_r), (_l), (_f))

void
nvme_dumpregs(struct nvme_softc *sc)
{
	u_int64_t r8;
	u_int32_t r4;

	r8 = nvme_read8(sc, NVME_CAP);
	printf("%s: cap  0x%016llx\n", DEVNAME(sc), nvme_read8(sc, NVME_CAP));
	printf("%s:  mpsmax %u (%u)\n", DEVNAME(sc),
	    (u_int)NVME_CAP_MPSMAX(r8), (1 << NVME_CAP_MPSMAX(r8)));
	printf("%s:  mpsmin %u (%u)\n", DEVNAME(sc),
	    (u_int)NVME_CAP_MPSMIN(r8), (1 << NVME_CAP_MPSMIN(r8)));
	printf("%s:  css %llu\n", DEVNAME(sc), NVME_CAP_CSS(r8));
	printf("%s:  nssrs %llu\n", DEVNAME(sc), NVME_CAP_NSSRS(r8));
	printf("%s:  dstrd %u\n", DEVNAME(sc), NVME_CAP_DSTRD(r8));
	printf("%s:  to %llu msec\n", DEVNAME(sc), NVME_CAP_TO(r8));
	printf("%s:  ams %llu\n", DEVNAME(sc), NVME_CAP_AMS(r8));
	printf("%s:  cqr %llu\n", DEVNAME(sc), NVME_CAP_CQR(r8));
	printf("%s:  mqes %llu\n", DEVNAME(sc), NVME_CAP_MQES(r8));

	printf("%s: vs   0x%04x\n", DEVNAME(sc), nvme_read4(sc, NVME_VS));

	r4 = nvme_read4(sc, NVME_CC);
	printf("%s: cc   0x%04x\n", DEVNAME(sc), r4);
	printf("%s:  iocqes %u\n", DEVNAME(sc), NVME_CC_IOCQES_R(r4));
	printf("%s:  iosqes %u\n", DEVNAME(sc), NVME_CC_IOSQES_R(r4));
	printf("%s:  shn %u\n", DEVNAME(sc), NVME_CC_SHN_R(r4));
	printf("%s:  ams %u\n", DEVNAME(sc), NVME_CC_AMS_R(r4));
	printf("%s:  mps %u\n", DEVNAME(sc), NVME_CC_MPS_R(r4));
	printf("%s:  css %u\n", DEVNAME(sc), NVME_CC_CSS_R(r4));
	printf("%s:  en %u\n", DEVNAME(sc), ISSET(r4, NVME_CC_EN));

	printf("%s: csts 0x%08x\n", DEVNAME(sc), nvme_read4(sc, NVME_CSTS));
	printf("%s: aqa  0x%08x\n", DEVNAME(sc), nvme_read4(sc, NVME_AQA));
	printf("%s: asq  0x%016llx\n", DEVNAME(sc), nvme_read8(sc, NVME_ASQ));
	printf("%s: acq  0x%016llx\n", DEVNAME(sc), nvme_read8(sc, NVME_ACQ));
}

int
nvme_ready(struct nvme_softc *sc, u_int32_t rdy)
{
	u_int i = 0;

	while ((nvme_read4(sc, NVME_CSTS) & NVME_CSTS_RDY) != rdy) {
		if (i++ > sc->sc_rdy_to)
			return (1);

		delay(1000);
		nvme_barrier(sc, NVME_CSTS, 4, BUS_SPACE_BARRIER_READ);
	}

	return (0);
}

int
nvme_enable(struct nvme_softc *sc, u_int mps)
{
	u_int32_t cc;

	cc = nvme_read4(sc, NVME_CC);
	if (ISSET(cc, NVME_CC_EN))
		return (nvme_ready(sc, NVME_CSTS_RDY));

	nvme_write4(sc, NVME_AQA, NVME_AQA_ACQS(sc->sc_admin_q->q_entries) |
	    NVME_AQA_ASQS(sc->sc_admin_q->q_entries));
	nvme_barrier(sc, 0, sc->sc_ios, BUS_SPACE_BARRIER_WRITE);

	nvme_write8(sc, NVME_ASQ, NVME_DMA_DVA(sc->sc_admin_q->q_sq_dmamem));
	nvme_barrier(sc, 0, sc->sc_ios, BUS_SPACE_BARRIER_WRITE);
	nvme_write8(sc, NVME_ACQ, NVME_DMA_DVA(sc->sc_admin_q->q_cq_dmamem));
	nvme_barrier(sc, 0, sc->sc_ios, BUS_SPACE_BARRIER_WRITE);

	CLR(cc, NVME_CC_IOCQES_MASK | NVME_CC_IOSQES_MASK | NVME_CC_SHN_MASK |
	    NVME_CC_AMS_MASK | NVME_CC_MPS_MASK | NVME_CC_CSS_MASK);
	SET(cc, NVME_CC_IOSQES(ffs(64) - 1) | NVME_CC_IOCQES(ffs(16) - 1));
	SET(cc, NVME_CC_SHN(NVME_CC_SHN_NONE));
	SET(cc, NVME_CC_CSS(NVME_CC_CSS_NVM));
	SET(cc, NVME_CC_AMS(NVME_CC_AMS_RR));
	SET(cc, NVME_CC_MPS(mps));
	SET(cc, NVME_CC_EN);

	nvme_write4(sc, NVME_CC, cc);
	nvme_barrier(sc, 0, sc->sc_ios,
	    BUS_SPACE_BARRIER_READ | BUS_SPACE_BARRIER_WRITE);

	return (nvme_ready(sc, NVME_CSTS_RDY));
}

int
nvme_disable(struct nvme_softc *sc)
{
	u_int32_t cc, csts;

	cc = nvme_read4(sc, NVME_CC);
	if (ISSET(cc, NVME_CC_EN)) {
		csts = nvme_read4(sc, NVME_CSTS);
		if (!ISSET(csts, NVME_CSTS_CFS) &&
		    nvme_ready(sc, NVME_CSTS_RDY) != 0)
			return (1);
	}

	CLR(cc, NVME_CC_EN);

	nvme_write4(sc, NVME_CC, cc);
	nvme_barrier(sc, 0, sc->sc_ios,
	    BUS_SPACE_BARRIER_READ | BUS_SPACE_BARRIER_WRITE);

	return (nvme_ready(sc, 0));
}

int
nvme_attach(struct nvme_softc *sc)
{
	struct scsibus_attach_args saa;
	u_int64_t cap;
	u_int32_t reg;
	u_int mps = PAGE_SHIFT;

	mtx_init(&sc->sc_ccb_mtx, IPL_BIO);
	SIMPLEQ_INIT(&sc->sc_ccb_list);
	scsi_iopool_init(&sc->sc_iopool, sc, nvme_ccb_get, nvme_ccb_put);

	reg = nvme_read4(sc, NVME_VS);
	if (reg == 0xffffffff) {
		printf(", invalid mapping\n");
		return (1);
	}

	printf(", NVMe %d.%d\n", NVME_VS_MJR(reg), NVME_VS_MNR(reg));

	cap = nvme_read8(sc, NVME_CAP);
	sc->sc_dstrd = NVME_CAP_DSTRD(cap);
	if (NVME_CAP_MPSMIN(cap) > PAGE_SHIFT) {
		printf("%s: NVMe minimum page size %u "
		    "is greater than CPU page size %u\n", DEVNAME(sc),
		    1 << NVME_CAP_MPSMIN(cap), 1 << PAGE_SHIFT);
		return (1);
	}
	if (NVME_CAP_MPSMAX(cap) < mps)
		mps = NVME_CAP_MPSMAX(cap);

	sc->sc_rdy_to = NVME_CAP_TO(cap);
	sc->sc_mps = 1 << mps;
	sc->sc_mps_bits = mps;
	sc->sc_mdts = MAXPHYS;
	sc->sc_max_sgl = 2;

	if (nvme_disable(sc) != 0) {
		printf("%s: unable to disable controller\n", DEVNAME(sc));
		return (1);
	}

	sc->sc_admin_q = nvme_q_alloc(sc, NVME_ADMIN_Q, 128, sc->sc_dstrd);
	if (sc->sc_admin_q == NULL) {
		printf("%s: unable to allocate admin queue\n", DEVNAME(sc));
		return (1);
	}

	if (nvme_ccbs_alloc(sc, 16) != 0) {
		printf("%s: unable to allocate initial ccbs\n", DEVNAME(sc));
		goto free_admin_q;
	}

	if (nvme_enable(sc, mps) != 0) {
		printf("%s: unable to enable controller\n", DEVNAME(sc));
		goto free_ccbs;
	}

	if (nvme_identify(sc, NVME_CAP_MPSMIN(cap)) != 0) {
		printf("%s: unable to identify controller\n", DEVNAME(sc));
		goto disable;
	}

	/* we know how big things are now */
	sc->sc_max_sgl = sc->sc_mdts / sc->sc_mps;

	nvme_ccbs_free(sc);
	if (nvme_ccbs_alloc(sc, 64) != 0) {
		printf("%s: unable to allocate ccbs\n", DEVNAME(sc));
		goto free_admin_q;
	}

	sc->sc_q = nvme_q_alloc(sc, 1, 128, sc->sc_dstrd);
	if (sc->sc_q == NULL) {
		printf("%s: unable to allocate io q\n", DEVNAME(sc));
		goto disable;
	}

	if (nvme_q_create(sc, sc->sc_q) != 0) {
		printf("%s: unable to create io q\n", DEVNAME(sc));
		goto free_q;
	}

	nvme_write4(sc, NVME_INTMC, 1);

	sc->sc_namespaces = mallocarray(sc->sc_nn, sizeof(*sc->sc_namespaces),
	    M_DEVBUF, M_WAITOK|M_ZERO);

	sc->sc_link.adapter = &nvme_switch;
	sc->sc_link.adapter_softc = sc;
	sc->sc_link.adapter_buswidth = sc->sc_nn;
	sc->sc_link.luns = 1;
	sc->sc_link.adapter_target = sc->sc_nn;
	sc->sc_link.openings = 64;
	sc->sc_link.pool = &sc->sc_iopool;

	memset(&saa, 0, sizeof(saa));
	saa.saa_sc_link = &sc->sc_link;

	sc->sc_scsibus = (struct scsibus_softc *)config_found(&sc->sc_dev,
	    &saa, scsiprint);

	return (0);

free_q:
	nvme_q_free(sc, sc->sc_q);
disable:
	nvme_disable(sc);
free_ccbs:
	nvme_ccbs_free(sc);
free_admin_q:
	nvme_q_free(sc, sc->sc_admin_q);

	return (1);
}

int
nvme_resume(struct nvme_softc *sc)
{
	if (nvme_disable(sc) != 0) {
		printf("%s: unable to disable controller\n", DEVNAME(sc));
		return (1);
	}

	if (nvme_q_reset(sc, sc->sc_admin_q) != 0) {
		printf("%s: unable to reset admin queue\n", DEVNAME(sc));
		return (1);
	}

	if (nvme_enable(sc, sc->sc_mps_bits) != 0) {
		printf("%s: unable to enable controller\n", DEVNAME(sc));
		return (1);
	}

	sc->sc_q = nvme_q_alloc(sc, 1, 128, sc->sc_dstrd);
	if (sc->sc_q == NULL) {
		printf("%s: unable to allocate io q\n", DEVNAME(sc));
		goto disable;
	}

	if (nvme_q_create(sc, sc->sc_q) != 0) {
		printf("%s: unable to create io q\n", DEVNAME(sc));
		goto free_q;
	}

	nvme_write4(sc, NVME_INTMC, 1);

	return (0);

free_q:
	nvme_q_free(sc, sc->sc_q);
disable:
	nvme_disable(sc);

	return (1);
}

int
nvme_scsi_probe(struct scsi_link *link)
{
	struct nvme_softc *sc = link->adapter_softc;
	struct nvme_sqe sqe;
	struct nvm_identify_namespace *identify;
	struct nvme_dmamem *mem;
	struct nvme_ccb *ccb;
	int rv;

	ccb = scsi_io_get(&sc->sc_iopool, 0);
	KASSERT(ccb != NULL);

	mem = nvme_dmamem_alloc(sc, sizeof(*identify));
	if (mem == NULL)
		return (ENOMEM);

	memset(&sqe, 0, sizeof(sqe));
	sqe.opcode = NVM_ADMIN_IDENTIFY;
	htolem32(&sqe.nsid, link->target + 1);
	htolem64(&sqe.entry.prp[0], NVME_DMA_DVA(mem));
	htolem32(&sqe.cdw10, 0);

	ccb->ccb_done = nvme_empty_done;
	ccb->ccb_cookie = &sqe;

	nvme_dmamem_sync(sc, mem, BUS_DMASYNC_PREREAD);
	rv = nvme_poll(sc, sc->sc_admin_q, ccb, nvme_sqe_fill);
	nvme_dmamem_sync(sc, mem, BUS_DMASYNC_POSTREAD);

	scsi_io_put(&sc->sc_iopool, ccb);

	if (rv != 0) {
		rv = EIO;
		goto done;
	}

	/* commit */

	identify = malloc(sizeof(*identify), M_DEVBUF, M_WAITOK|M_ZERO);
	memcpy(identify, NVME_DMA_KVA(mem), sizeof(*identify));

	sc->sc_namespaces[link->target].ident = identify;

done:
	nvme_dmamem_free(sc, mem);

	return (rv);
}

int
nvme_shutdown(struct nvme_softc *sc)
{
	u_int32_t cc, csts;
	int i;

	nvme_write4(sc, NVME_INTMC, 0);

	if (nvme_q_delete(sc, sc->sc_q) != 0) {
		printf("%s: unable to delete q, disabling\n", DEVNAME(sc));
		goto disable;
	}

	cc = nvme_read4(sc, NVME_CC);
	CLR(cc, NVME_CC_SHN_MASK);
	SET(cc, NVME_CC_SHN(NVME_CC_SHN_NORMAL));
	nvme_write4(sc, NVME_CC, cc);

	for (i = 0; i < 4000; i++) {
		nvme_barrier(sc, 0, sc->sc_ios,
		    BUS_SPACE_BARRIER_READ | BUS_SPACE_BARRIER_WRITE);
		csts = nvme_read4(sc, NVME_CSTS);
		if ((csts & NVME_CSTS_SHST_MASK) == NVME_CSTS_SHST_DONE)
			return (0);

		delay(1000);
	}

	printf("%s: unable to shudown, disabling\n", DEVNAME(sc));

disable:
	nvme_disable(sc);
	return (0);
}

int
nvme_activate(struct nvme_softc *sc, int act)
{
	int rv;

	switch (act) {
	case DVACT_POWERDOWN:
		rv = config_activate_children(&sc->sc_dev, act);
		nvme_shutdown(sc);
		break;
	case DVACT_RESUME:
		rv = nvme_resume(sc);
		if (rv == 0)
			rv = config_activate_children(&sc->sc_dev, act);
		break;
	default:
		rv = config_activate_children(&sc->sc_dev, act);
		break;
	}

	return (rv);
}

void
nvme_scsi_cmd(struct scsi_xfer *xs)
{
	switch (xs->cmd->opcode) {
	case READ_COMMAND:
	case READ_BIG:
	case READ_12:
	case READ_16:
		nvme_scsi_io(xs, SCSI_DATA_IN);
		return;
	case WRITE_COMMAND:
	case WRITE_BIG:
	case WRITE_12:
	case WRITE_16:
		nvme_scsi_io(xs, SCSI_DATA_OUT);
		return;

	case SYNCHRONIZE_CACHE:
		nvme_scsi_sync(xs);
		return;

	case INQUIRY:
		nvme_scsi_inq(xs);
		return;
	case READ_CAPACITY_16:
		nvme_scsi_capacity16(xs);
		return;
	case READ_CAPACITY:
		nvme_scsi_capacity(xs);
		return;

	case TEST_UNIT_READY:
	case PREVENT_ALLOW:
	case START_STOP:
		xs->error = XS_NOERROR;
		scsi_done(xs);
		return;

	default:
		break;
	}

	xs->error = XS_DRIVER_STUFFUP;
	scsi_done(xs);
}

void
nvme_scsi_io(struct scsi_xfer *xs, int dir)
{
	struct scsi_link *link = xs->sc_link;
	struct nvme_softc *sc = link->adapter_softc;
	struct nvme_ccb *ccb = xs->io;
	bus_dmamap_t dmap = ccb->ccb_dmamap;
	int i;

	if ((xs->flags & (SCSI_DATA_IN|SCSI_DATA_OUT)) != dir)
		goto stuffup;

	ccb->ccb_done = nvme_scsi_io_done;
	ccb->ccb_cookie = xs;

	if (bus_dmamap_load(sc->sc_dmat, dmap,
	    xs->data, xs->datalen, NULL, ISSET(xs->flags, SCSI_NOSLEEP) ?
	    BUS_DMA_NOWAIT : BUS_DMA_WAITOK) != 0)
		goto stuffup;

	bus_dmamap_sync(sc->sc_dmat, dmap, 0, dmap->dm_mapsize,
	    ISSET(xs->flags, SCSI_DATA_IN) ?
	    BUS_DMASYNC_PREREAD : BUS_DMASYNC_PREWRITE);

	if (dmap->dm_nsegs > 2) {
		for (i = 1; i < dmap->dm_nsegs; i++) {
			htolem64(&ccb->ccb_prpl[i - 1],
			    dmap->dm_segs[i].ds_addr);
		}
		bus_dmamap_sync(sc->sc_dmat,
		    NVME_DMA_MAP(sc->sc_ccb_prpls),
		    ccb->ccb_prpl_off,
		    sizeof(*ccb->ccb_prpl) * dmap->dm_nsegs - 1,
		    BUS_DMASYNC_PREWRITE);
	}

	if (ISSET(xs->flags, SCSI_POLL)) {
		nvme_poll(sc, sc->sc_q, ccb, nvme_scsi_io_fill);
		return;
	}

	nvme_q_submit(sc, sc->sc_q, ccb, nvme_scsi_io_fill);
	return;

stuffup:
	xs->error = XS_DRIVER_STUFFUP;
	scsi_done(xs);
}

void
nvme_scsi_io_fill(struct nvme_softc *sc, struct nvme_ccb *ccb, void *slot)
{
	struct nvme_sqe_io *sqe = slot;
	struct scsi_xfer *xs = ccb->ccb_cookie;
	struct scsi_link *link = xs->sc_link;
	bus_dmamap_t dmap = ccb->ccb_dmamap;
	u_int64_t lba;
	u_int32_t blocks;

	scsi_cmd_rw_decode(xs->cmd, &lba, &blocks);

	sqe->opcode = ISSET(xs->flags, SCSI_DATA_IN) ?
	    NVM_CMD_READ : NVM_CMD_WRITE;
	htolem32(&sqe->nsid, link->target + 1);

	htolem64(&sqe->entry.prp[0], dmap->dm_segs[0].ds_addr);
	switch (dmap->dm_nsegs) {
	case 1:
		break;
	case 2:
		htolem64(&sqe->entry.prp[1], dmap->dm_segs[1].ds_addr);
		break;
	default:
		/* the prp list is already set up and synced */
		htolem64(&sqe->entry.prp[1], ccb->ccb_prpl_dva);
		break;
	}

	htolem64(&sqe->slba, lba);
	htolem16(&sqe->nlb, blocks - 1);
}

void
nvme_scsi_io_done(struct nvme_softc *sc, struct nvme_ccb *ccb,
    struct nvme_cqe *cqe)
{
	struct scsi_xfer *xs = ccb->ccb_cookie;
	bus_dmamap_t dmap = ccb->ccb_dmamap;
	u_int16_t flags;

	if (dmap->dm_nsegs > 2) {
		bus_dmamap_sync(sc->sc_dmat,
		    NVME_DMA_MAP(sc->sc_ccb_prpls),
		    ccb->ccb_prpl_off,
		    sizeof(*ccb->ccb_prpl) * dmap->dm_nsegs - 1,
		    BUS_DMASYNC_POSTWRITE);
	}

	bus_dmamap_sync(sc->sc_dmat, dmap, 0, dmap->dm_mapsize,
	    ISSET(xs->flags, SCSI_DATA_IN) ?
	    BUS_DMASYNC_POSTREAD : BUS_DMASYNC_POSTWRITE);

	bus_dmamap_unload(sc->sc_dmat, dmap);

	flags = lemtoh16(&cqe->flags);

	xs->error = (NVME_CQE_SC(flags) == NVME_CQE_SC_SUCCESS) ?
	    XS_NOERROR : XS_DRIVER_STUFFUP;
	xs->status = SCSI_OK;
	xs->resid = 0;
	scsi_done(xs);
}

void
nvme_scsi_sync(struct scsi_xfer *xs)
{
	struct scsi_link *link = xs->sc_link;
	struct nvme_softc *sc = link->adapter_softc;
	struct nvme_ccb *ccb = xs->io;

	ccb->ccb_done = nvme_scsi_sync_done;
	ccb->ccb_cookie = xs;

	if (ISSET(xs->flags, SCSI_POLL)) {
		nvme_poll(sc, sc->sc_q, ccb, nvme_scsi_sync_fill);
		return;
	}

	nvme_q_submit(sc, sc->sc_q, ccb, nvme_scsi_sync_fill);
}

void
nvme_scsi_sync_fill(struct nvme_softc *sc, struct nvme_ccb *ccb, void *slot)
{
	struct nvme_sqe *sqe = slot;
	struct scsi_xfer *xs = ccb->ccb_cookie;
	struct scsi_link *link = xs->sc_link;

	sqe->opcode = NVM_CMD_FLUSH;
	htolem32(&sqe->nsid, link->target + 1);
}

void
nvme_scsi_sync_done(struct nvme_softc *sc, struct nvme_ccb *ccb,
    struct nvme_cqe *cqe)
{
	struct scsi_xfer *xs = ccb->ccb_cookie;
	u_int16_t flags;

	flags = lemtoh16(&cqe->flags);

	xs->error = (NVME_CQE_SC(flags) == NVME_CQE_SC_SUCCESS) ?
	    XS_NOERROR : XS_DRIVER_STUFFUP;
	xs->status = SCSI_OK;
	xs->resid = 0;
	scsi_done(xs);
}

void
nvme_scsi_inq(struct scsi_xfer *xs)
{
	struct scsi_inquiry *inq = (struct scsi_inquiry *)xs->cmd;

	if (!ISSET(inq->flags, SI_EVPD)) {
		nvme_scsi_inquiry(xs);
		return;
	}

	switch (inq->pagecode) {
	default:
		/* printf("%s: %d\n", __func__, inq->pagecode); */
		break;
	}

	xs->error = XS_DRIVER_STUFFUP;
	scsi_done(xs);
}

void
nvme_scsi_inquiry(struct scsi_xfer *xs)
{
	struct scsi_inquiry_data inq;
	struct scsi_link *link = xs->sc_link;
	struct nvme_softc *sc = link->adapter_softc;
	struct nvm_identify_namespace *ns;

	ns = sc->sc_namespaces[link->target].ident;

	memset(&inq, 0, sizeof(inq));

	inq.device = T_DIRECT;
	inq.version = 0x06; /* SPC-4 */
	inq.response_format = 2;
	inq.additional_length = 32;
	inq.flags |= SID_CmdQue;
	memcpy(inq.vendor, "NVMe    ", sizeof(inq.vendor));
	memcpy(inq.product, sc->sc_identify.mn, sizeof(inq.product));
	memcpy(inq.revision, sc->sc_identify.fr, sizeof(inq.revision));

	memcpy(xs->data, &inq, MIN(sizeof(inq), xs->datalen));

	xs->error = XS_NOERROR;
	scsi_done(xs);
}

void
nvme_scsi_capacity16(struct scsi_xfer *xs)
{
	struct scsi_read_cap_data_16 rcd;
	struct scsi_link *link = xs->sc_link;
	struct nvme_softc *sc = link->adapter_softc;
	struct nvm_identify_namespace *ns;
	struct nvm_namespace_format *f;
	u_int64_t nsze;
	u_int16_t tpe = READ_CAP_16_TPE;

	ns = sc->sc_namespaces[link->target].ident;

	if (xs->cmdlen != sizeof(struct scsi_read_capacity_16)) {
		xs->error = XS_DRIVER_STUFFUP;
		scsi_done(xs);
		return;
	}

	/* sd_read_cap_16() will add one */
	nsze = lemtoh64(&ns->nsze) - 1;
	f = &ns->lbaf[NVME_ID_NS_FLBAS(ns->flbas)];

	memset(&rcd, 0, sizeof(rcd));
	_lto8b(nsze, rcd.addr);
	_lto4b(1 << f->lbads, rcd.length);
	_lto2b(tpe, rcd.lowest_aligned);

	memcpy(xs->data, &rcd, MIN(sizeof(rcd), xs->datalen));

	xs->error = XS_NOERROR;
	scsi_done(xs);
}

void
nvme_scsi_capacity(struct scsi_xfer *xs)
{
	struct scsi_read_cap_data rcd;
	struct scsi_link *link = xs->sc_link;
	struct nvme_softc *sc = link->adapter_softc;
	struct nvm_identify_namespace *ns;
	struct nvm_namespace_format *f;
	u_int64_t nsze;

	ns = sc->sc_namespaces[link->target].ident;

	if (xs->cmdlen != sizeof(struct scsi_read_capacity)) {
		xs->error = XS_DRIVER_STUFFUP;
		scsi_done(xs);
		return;
	}

	/* sd_read_cap_10() will add one */
	nsze = lemtoh64(&ns->nsze) - 1;
	if (nsze > 0xffffffff)
		nsze = 0xffffffff;

	f = &ns->lbaf[NVME_ID_NS_FLBAS(ns->flbas)];

	memset(&rcd, 0, sizeof(rcd));
	_lto4b(nsze, rcd.addr);
	_lto4b(1 << f->lbads, rcd.length);

	memcpy(xs->data, &rcd, MIN(sizeof(rcd), xs->datalen));

	xs->error = XS_NOERROR;
	scsi_done(xs);
}

void
nvme_scsi_free(struct scsi_link *link)
{
	struct nvme_softc *sc = link->adapter_softc;
	struct nvm_identify_namespace *identify;

	identify = sc->sc_namespaces[link->target].ident;
	sc->sc_namespaces[link->target].ident = NULL;

	free(identify, M_DEVBUF, sizeof(*identify));
}

void
nvme_q_submit(struct nvme_softc *sc, struct nvme_queue *q, struct nvme_ccb *ccb,
    void (*fill)(struct nvme_softc *, struct nvme_ccb *, void *))
{
	struct nvme_sqe *sqe = NVME_DMA_KVA(q->q_sq_dmamem);
	u_int32_t tail;

	mtx_enter(&q->q_sq_mtx);
	tail = q->q_sq_tail;
	if (++q->q_sq_tail >= q->q_entries)
		q->q_sq_tail = 0;

	sqe += tail;

	bus_dmamap_sync(sc->sc_dmat, NVME_DMA_MAP(q->q_sq_dmamem),
	    sizeof(*sqe) * tail, sizeof(*sqe), BUS_DMASYNC_POSTWRITE);
	memset(sqe, 0, sizeof(*sqe));
	(*fill)(sc, ccb, sqe);
	sqe->cid = ccb->ccb_id;
	bus_dmamap_sync(sc->sc_dmat, NVME_DMA_MAP(q->q_sq_dmamem),
	    sizeof(*sqe) * tail, sizeof(*sqe), BUS_DMASYNC_PREWRITE);

	nvme_write4(sc, q->q_sqtdbl, q->q_sq_tail);
	mtx_leave(&q->q_sq_mtx);
}

struct nvme_poll_state {
	struct nvme_sqe s;
	struct nvme_cqe c;
};

int
nvme_poll(struct nvme_softc *sc, struct nvme_queue *q, struct nvme_ccb *ccb,
    void (*fill)(struct nvme_softc *, struct nvme_ccb *, void *))
{
	struct nvme_poll_state state;
	void (*done)(struct nvme_softc *, struct nvme_ccb *, struct nvme_cqe *);
	void *cookie;
	u_int16_t flags;

	memset(&state, 0, sizeof(state));
	(*fill)(sc, ccb, &state.s);

	done = ccb->ccb_done;
	cookie = ccb->ccb_cookie;

	ccb->ccb_done = nvme_poll_done;
	ccb->ccb_cookie = &state;

	nvme_q_submit(sc, q, ccb, nvme_poll_fill);
	while (!ISSET(state.c.flags, htole16(NVME_CQE_PHASE))) {
		if (nvme_q_complete(sc, q) == 0)
			delay(10);

		/* XXX no timeout? */
	}

	ccb->ccb_cookie = cookie;
	done(sc, ccb, &state.c);

	flags = lemtoh16(&state.c.flags);

	return (flags & ~NVME_CQE_PHASE);
}

void
nvme_poll_fill(struct nvme_softc *sc, struct nvme_ccb *ccb, void *slot)
{
	struct nvme_sqe *sqe = slot;
	struct nvme_poll_state *state = ccb->ccb_cookie;

	*sqe = state->s;
}

void
nvme_poll_done(struct nvme_softc *sc, struct nvme_ccb *ccb,
    struct nvme_cqe *cqe)
{
	struct nvme_poll_state *state = ccb->ccb_cookie;

	SET(cqe->flags, htole16(NVME_CQE_PHASE));
	state->c = *cqe;
}

void
nvme_sqe_fill(struct nvme_softc *sc, struct nvme_ccb *ccb, void *slot)
{
	struct nvme_sqe *src = ccb->ccb_cookie;
	struct nvme_sqe *dst = slot;

	*dst = *src;
}

void
nvme_empty_done(struct nvme_softc *sc, struct nvme_ccb *ccb,
    struct nvme_cqe *cqe)
{
}

int
nvme_q_complete(struct nvme_softc *sc, struct nvme_queue *q)
{
	struct nvme_ccb *ccb;
	struct nvme_cqe *ring = NVME_DMA_KVA(q->q_cq_dmamem), *cqe;
	u_int32_t head;
	u_int16_t flags;
	int rv = 0;

	if (!mtx_enter_try(&q->q_cq_mtx))
		return (-1);

	head = q->q_cq_head;

	nvme_dmamem_sync(sc, q->q_cq_dmamem, BUS_DMASYNC_POSTREAD);
	for (;;) {
		cqe = &ring[head];
		flags = lemtoh16(&cqe->flags);
		if ((flags & NVME_CQE_PHASE) != q->q_cq_phase)
			break;

		ccb = &sc->sc_ccbs[cqe->cid];
		ccb->ccb_done(sc, ccb, cqe);

		if (++head >= q->q_entries) {
			head = 0;
			q->q_cq_phase ^= NVME_CQE_PHASE;
		}

		rv = 1;
	}
	nvme_dmamem_sync(sc, q->q_cq_dmamem, BUS_DMASYNC_PREREAD);

	if (rv)
		nvme_write4(sc, q->q_cqhdbl, q->q_cq_head = head);
	mtx_leave(&q->q_cq_mtx);

	return (rv);
}

int
nvme_identify(struct nvme_softc *sc, u_int mps)
{
	char sn[41], mn[81], fr[17];
	struct nvm_identify_controller *identify;
	struct nvme_dmamem *mem;
	struct nvme_ccb *ccb;
	u_int mdts;
	int rv = 1;

	ccb = nvme_ccb_get(sc);
	if (ccb == NULL)
		panic("nvme_identify: nvme_ccb_get returned NULL");

	mem = nvme_dmamem_alloc(sc, sizeof(*identify));
	if (mem == NULL)
		return (1);

	ccb->ccb_done = nvme_empty_done;
	ccb->ccb_cookie = mem;

	nvme_dmamem_sync(sc, mem, BUS_DMASYNC_PREREAD);
	rv = nvme_poll(sc, sc->sc_admin_q, ccb, nvme_fill_identify);
	nvme_dmamem_sync(sc, mem, BUS_DMASYNC_POSTREAD);

	nvme_ccb_put(sc, ccb);

	if (rv != 0)
		goto done;

	identify = NVME_DMA_KVA(mem);

	scsi_strvis(sn, identify->sn, sizeof(identify->sn));
	scsi_strvis(mn, identify->mn, sizeof(identify->mn));
	scsi_strvis(fr, identify->fr, sizeof(identify->fr));

	printf("%s: %s, firmware %s, serial %s\n", DEVNAME(sc), mn, fr, sn);

	if (identify->mdts > 0) {
		mdts = (1 << identify->mdts) * (1 << mps);
		if (mdts < sc->sc_mdts)
			sc->sc_mdts = mdts;
	}

	sc->sc_nn = lemtoh32(&identify->nn);

	memcpy(&sc->sc_identify, identify, sizeof(sc->sc_identify));

done:
	nvme_dmamem_free(sc, mem);

	return (rv);
}

int
nvme_q_create(struct nvme_softc *sc, struct nvme_queue *q)
{
	struct nvme_sqe_q sqe;
	struct nvme_ccb *ccb;
	int rv;

	ccb = scsi_io_get(&sc->sc_iopool, 0);
	KASSERT(ccb != NULL);

	ccb->ccb_done = nvme_empty_done;
	ccb->ccb_cookie = &sqe;

	memset(&sqe, 0, sizeof(sqe));
	sqe.opcode = NVM_ADMIN_ADD_IOCQ;
	htolem64(&sqe.prp1, NVME_DMA_DVA(q->q_cq_dmamem));
	htolem16(&sqe.qsize, q->q_entries - 1);
	htolem16(&sqe.qid, q->q_id);
	sqe.qflags = NVM_SQE_CQ_IEN | NVM_SQE_Q_PC;

	rv = nvme_poll(sc, sc->sc_admin_q, ccb, nvme_sqe_fill);
	if (rv != 0)
		goto fail;

	ccb->ccb_done = nvme_empty_done;
	ccb->ccb_cookie = &sqe;

	memset(&sqe, 0, sizeof(sqe));
	sqe.opcode = NVM_ADMIN_ADD_IOSQ;
	htolem64(&sqe.prp1, NVME_DMA_DVA(q->q_sq_dmamem));
	htolem16(&sqe.qsize, q->q_entries - 1);
	htolem16(&sqe.qid, q->q_id);
	htolem16(&sqe.cqid, q->q_id);
	sqe.qflags = NVM_SQE_Q_PC;

	rv = nvme_poll(sc, sc->sc_admin_q, ccb, nvme_sqe_fill);
	if (rv != 0)
		goto fail;

fail:
	scsi_io_put(&sc->sc_iopool, ccb);
	return (rv);
}

int
nvme_q_delete(struct nvme_softc *sc, struct nvme_queue *q)
{
	struct nvme_sqe_q sqe;
	struct nvme_ccb *ccb;
	int rv;

	ccb = scsi_io_get(&sc->sc_iopool, 0);
	KASSERT(ccb != NULL);

	ccb->ccb_done = nvme_empty_done;
	ccb->ccb_cookie = &sqe;

	memset(&sqe, 0, sizeof(sqe));
	sqe.opcode = NVM_ADMIN_DEL_IOSQ;
	htolem16(&sqe.qid, q->q_id);

	rv = nvme_poll(sc, sc->sc_admin_q, ccb, nvme_sqe_fill);
	if (rv != 0)
		goto fail;

	ccb->ccb_done = nvme_empty_done;
	ccb->ccb_cookie = &sqe;

	memset(&sqe, 0, sizeof(sqe));
	sqe.opcode = NVM_ADMIN_DEL_IOCQ;
	htolem16(&sqe.qid, q->q_id);

	rv = nvme_poll(sc, sc->sc_admin_q, ccb, nvme_sqe_fill);
	if (rv != 0)
		goto fail;

	nvme_q_free(sc, q);

fail:
	scsi_io_put(&sc->sc_iopool, ccb);
	return (rv);

}

void
nvme_fill_identify(struct nvme_softc *sc, struct nvme_ccb *ccb, void *slot)
{
	struct nvme_sqe *sqe = slot;
	struct nvme_dmamem *mem = ccb->ccb_cookie;

	sqe->opcode = NVM_ADMIN_IDENTIFY;
	htolem64(&sqe->entry.prp[0], NVME_DMA_DVA(mem));
	htolem32(&sqe->cdw10, 1);
}

int
nvme_ccbs_alloc(struct nvme_softc *sc, u_int nccbs)
{
	struct nvme_ccb *ccb;
	bus_addr_t off;
	u_int64_t *prpl;
	u_int i;

	sc->sc_ccbs = mallocarray(nccbs, sizeof(*ccb), M_DEVBUF,
	    M_WAITOK | M_CANFAIL);
	if (sc->sc_ccbs == NULL)
		return (1);

	sc->sc_ccb_prpls = nvme_dmamem_alloc(sc, 
	    sizeof(*prpl) * sc->sc_max_sgl * nccbs);

	prpl = NVME_DMA_KVA(sc->sc_ccb_prpls);
	off = 0;

	for (i = 0; i < nccbs; i++) {
		ccb = &sc->sc_ccbs[i];

		if (bus_dmamap_create(sc->sc_dmat, sc->sc_mdts,
		    sc->sc_max_sgl + 1 /* we get a free prp in the sqe */,
		    sc->sc_mps, sc->sc_mps, BUS_DMA_WAITOK | BUS_DMA_ALLOCNOW,
		    &ccb->ccb_dmamap) != 0)
			goto free_maps;

		ccb->ccb_id = i;
		ccb->ccb_prpl = prpl;
		ccb->ccb_prpl_off = off;
		ccb->ccb_prpl_dva = NVME_DMA_DVA(sc->sc_ccb_prpls) + off;

		SIMPLEQ_INSERT_TAIL(&sc->sc_ccb_list, ccb, ccb_entry);

		prpl += sc->sc_max_sgl;
		off += sizeof(*prpl) * sc->sc_max_sgl;
	}

	return (0);

free_maps:
	nvme_ccbs_free(sc);
	return (1);
}

void *
nvme_ccb_get(void *cookie)
{
	struct nvme_softc *sc = cookie;
	struct nvme_ccb *ccb;

	mtx_enter(&sc->sc_ccb_mtx);
	ccb = SIMPLEQ_FIRST(&sc->sc_ccb_list);
	if (ccb != NULL)
		SIMPLEQ_REMOVE_HEAD(&sc->sc_ccb_list, ccb_entry);
	mtx_leave(&sc->sc_ccb_mtx);

	return (ccb);
}

void
nvme_ccb_put(void *cookie, void *io)
{
	struct nvme_softc *sc = cookie;
	struct nvme_ccb *ccb = io;

	mtx_enter(&sc->sc_ccb_mtx);
	SIMPLEQ_INSERT_HEAD(&sc->sc_ccb_list, ccb, ccb_entry);
	mtx_leave(&sc->sc_ccb_mtx);
}

void
nvme_ccbs_free(struct nvme_softc *sc)
{
	struct nvme_ccb *ccb;

	while ((ccb = SIMPLEQ_FIRST(&sc->sc_ccb_list)) != NULL) {
		SIMPLEQ_REMOVE_HEAD(&sc->sc_ccb_list, ccb_entry);
		bus_dmamap_destroy(sc->sc_dmat, ccb->ccb_dmamap);
	}

	nvme_dmamem_free(sc, sc->sc_ccb_prpls);
	free(sc->sc_ccbs, M_DEVBUF, 0);
}

struct nvme_queue *
nvme_q_alloc(struct nvme_softc *sc, u_int16_t id, u_int entries, u_int dstrd)
{
	struct nvme_queue *q;

	q = malloc(sizeof(*q), M_DEVBUF, M_WAITOK | M_CANFAIL);
	if (q == NULL)
		return (NULL);

	q->q_sq_dmamem = nvme_dmamem_alloc(sc,
	    sizeof(struct nvme_sqe) * entries);
	if (q->q_sq_dmamem == NULL)
		goto free;

	q->q_cq_dmamem = nvme_dmamem_alloc(sc,
	    sizeof(struct nvme_cqe) * entries);
	if (q->q_cq_dmamem == NULL)
		goto free_sq;

	memset(NVME_DMA_KVA(q->q_sq_dmamem), 0, NVME_DMA_LEN(q->q_sq_dmamem));
	memset(NVME_DMA_KVA(q->q_cq_dmamem), 0, NVME_DMA_LEN(q->q_cq_dmamem));

	mtx_init(&q->q_sq_mtx, IPL_BIO);
	mtx_init(&q->q_cq_mtx, IPL_BIO);
	q->q_sqtdbl = NVME_SQTDBL(id, dstrd);
	q->q_cqhdbl = NVME_CQHDBL(id, dstrd);

	q->q_id = id;
	q->q_entries = entries;
	q->q_sq_tail = 0;
	q->q_cq_head = 0;
	q->q_cq_phase = NVME_CQE_PHASE;

	nvme_dmamem_sync(sc, q->q_sq_dmamem, BUS_DMASYNC_PREWRITE);
	nvme_dmamem_sync(sc, q->q_cq_dmamem, BUS_DMASYNC_PREREAD);

	return (q);

free_sq:
	nvme_dmamem_free(sc, q->q_sq_dmamem);
free:
	free(q, M_DEVBUF, sizeof *q);

	return (NULL);
}

int
nvme_q_reset(struct nvme_softc *sc, struct nvme_queue *q)
{
	memset(NVME_DMA_KVA(q->q_sq_dmamem), 0, NVME_DMA_LEN(q->q_sq_dmamem));
	memset(NVME_DMA_KVA(q->q_cq_dmamem), 0, NVME_DMA_LEN(q->q_cq_dmamem));

	q->q_sqtdbl = NVME_SQTDBL(q->q_id, sc->sc_dstrd);
	q->q_cqhdbl = NVME_CQHDBL(q->q_id, sc->sc_dstrd);

	q->q_sq_tail = 0;
	q->q_cq_head = 0;
	q->q_cq_phase = NVME_CQE_PHASE;

	nvme_dmamem_sync(sc, q->q_sq_dmamem, BUS_DMASYNC_PREWRITE);
	nvme_dmamem_sync(sc, q->q_cq_dmamem, BUS_DMASYNC_PREREAD);

	return (0);
}

void
nvme_q_free(struct nvme_softc *sc, struct nvme_queue *q)
{
	nvme_dmamem_sync(sc, q->q_cq_dmamem, BUS_DMASYNC_POSTREAD);
	nvme_dmamem_sync(sc, q->q_sq_dmamem, BUS_DMASYNC_POSTWRITE);
	nvme_dmamem_free(sc, q->q_cq_dmamem);
	nvme_dmamem_free(sc, q->q_sq_dmamem);
	free(q, M_DEVBUF, sizeof *q);
}

int
nvme_intr(void *xsc)
{
	struct nvme_softc *sc = xsc;
	int rv = 0;

	if (nvme_q_complete(sc, sc->sc_q))
		rv = 1;
	if (nvme_q_complete(sc, sc->sc_admin_q))
		rv = 1;

	return (rv);
}

int
nvme_intr_intx(void *xsc)
{
	struct nvme_softc *sc = xsc;
	int rv;

	nvme_write4(sc, NVME_INTMS, 1);
	rv = nvme_intr(sc);
	nvme_write4(sc, NVME_INTMC, 1);

	return (rv);
}

struct nvme_dmamem *
nvme_dmamem_alloc(struct nvme_softc *sc, size_t size)
{
	struct nvme_dmamem *ndm;
	int nsegs;

	ndm = malloc(sizeof(*ndm), M_DEVBUF, M_WAITOK | M_ZERO);
	if (ndm == NULL)
		return (NULL);

	ndm->ndm_size = size;

	if (bus_dmamap_create(sc->sc_dmat, size, 1, size, 0,
	    BUS_DMA_WAITOK | BUS_DMA_ALLOCNOW, &ndm->ndm_map) != 0)
		goto ndmfree;

	if (bus_dmamem_alloc(sc->sc_dmat, size, sc->sc_mps, 0, &ndm->ndm_seg,
	    1, &nsegs, BUS_DMA_WAITOK | BUS_DMA_ZERO) != 0)
		goto destroy;

	if (bus_dmamem_map(sc->sc_dmat, &ndm->ndm_seg, nsegs, size,
	    &ndm->ndm_kva, BUS_DMA_WAITOK) != 0)
		goto free;

	if (bus_dmamap_load(sc->sc_dmat, ndm->ndm_map, ndm->ndm_kva, size,
	    NULL, BUS_DMA_WAITOK) != 0)
		goto unmap;

	return (ndm);

unmap:
	bus_dmamem_unmap(sc->sc_dmat, ndm->ndm_kva, size);
free:
	bus_dmamem_free(sc->sc_dmat, &ndm->ndm_seg, 1);
destroy:
	bus_dmamap_destroy(sc->sc_dmat, ndm->ndm_map);
ndmfree:
	free(ndm, M_DEVBUF, sizeof *ndm);

	return (NULL);
}

void
nvme_dmamem_sync(struct nvme_softc *sc, struct nvme_dmamem *mem, int ops)
{
	bus_dmamap_sync(sc->sc_dmat, NVME_DMA_MAP(mem),
	    0, NVME_DMA_LEN(mem), ops);
}

void
nvme_dmamem_free(struct nvme_softc *sc, struct nvme_dmamem *ndm)
{
	bus_dmamap_unload(sc->sc_dmat, ndm->ndm_map);
	bus_dmamem_unmap(sc->sc_dmat, ndm->ndm_kva, ndm->ndm_size);
	bus_dmamem_free(sc->sc_dmat, &ndm->ndm_seg, 1);
	bus_dmamap_destroy(sc->sc_dmat, ndm->ndm_map);
	free(ndm, M_DEVBUF, sizeof *ndm);
}

@


1.56
log
@nvme: Add suspend/resume code

Based on an initial patch by ehrhardt@@ . Thanks to claudio@@ for testing
and deraadt@@ for advice.

"go ahead" deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.55 2017/05/12 22:16:54 jcs Exp $ */
a1122 1
	htolem64(&sqe.prp1, NVME_DMA_DVA(q->q_sq_dmamem));
@


1.55
log
@subtract one sector from the disk size before passing it back to the
scsi layer, which will add one sector back

fixes incorrect disk size reporting which was causing fdisk to
create a protective MBR of one too many sectors, which caused our
EFI bootloader to fail to recognize it as a GPT disk

ok dlg
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.54 2017/04/08 02:57:25 deraadt Exp $ */
d48 1
d72 1
a268 1
	u_int dstrd;
d284 1
a284 1
	dstrd = NVME_CAP_DSTRD(cap);
d296 1
d305 1
a305 1
	sc->sc_admin_q = nvme_q_alloc(sc, NVME_ADMIN_Q, 128, dstrd);
d335 1
a335 1
	sc->sc_q = nvme_q_alloc(sc, 1, 128, dstrd);
d380 41
d515 5
d1130 2
d1261 1
d1279 19
@


1.54
log
@A pile of sizes to free(9).  In test for a few days in snapshots.
Errors will result in nice clean panic messages so we know what's wrong.
Reviewed by dhill visa natano jsg.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.53 2016/11/15 12:17:42 mpi Exp $ */
d750 2
a751 1
	nsze = lemtoh64(&ns->nsze);
d783 2
a784 1
	nsze = lemtoh64(&ns->nsze);
@


1.53
log
@Do not use bus_space_{read,write}_8() even on LP64 archs,
some (broken) controllers require ordered split transfers.

From linux a310acd7a7ea53533886c11bb7edd11ffd61a036

Tested by gonzalo@@, ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.52 2016/11/15 12:01:11 mpi Exp $ */
d1223 1
a1223 1
	free(q, M_DEVBUF, 0);
d1235 1
a1235 1
	free(q, M_DEVBUF, 0);
d1302 1
a1302 1
	free(ndm, M_DEVBUF, 0);
d1321 1
a1321 1
	free(ndm, M_DEVBUF, 0);
@


1.52
log
@Mask non relevant bits when pritting version number.

Makes gonzalo@@'s Macbookair7,1 NVMe report the correct version.

ok dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.51 2016/10/25 06:20:41 dlg Exp $ */
d114 4
a117 6
#ifdef __LP64__
#define nvme_read8(_s, _r) \
	bus_space_read_8((_s)->sc_iot, (_s)->sc_ioh, (_r))
#define nvme_write8(_s, _r, _v) \
	bus_space_write_8((_s)->sc_iot, (_s)->sc_ioh, (_r), (_v))
#else /* __LP64__ */
a147 1
#endif /* __LP64__ */
@


1.51
log
@mask and unmask the interrupt source in an intx specific intr handler.

it seems devices using levelled intx interrupts need to explicitely ack
interrupts by masking and unmasking the source around the completion
ring handling. without this completions can be lost, which in turn
causes long (permanent?) stalls in the block layer under heavy write
load.

ive experienced this problem with an intel nvme part that only has
intx and msix support. because we dont support msix yet we only
use intx on it. it appeared to lock up before this fix.

this has been tested on both that intel board and a samsung with msi.
this fix was based on work found in code by nonaka
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.50 2016/05/20 11:11:05 dlg Exp $ */
a48 1
void	nvme_version(struct nvme_softc *, u_int32_t);
a154 23
nvme_version(struct nvme_softc *sc, u_int32_t version)
{
	const char *v = NULL;

	switch (version) {
	case NVME_VS_1_0:
		v = "1.0";
		break;
	case NVME_VS_1_1:
		v = "1.1";
		break;
	case NVME_VS_1_2:
		v = "1.2";
		break;
	default:
		printf(", unknown version 0x%08x", version);
		return;
	}

	printf(", NVMe %s", v);
}

void
d185 1
a185 1
	
d283 1
a283 2
	nvme_version(sc, reg);
	printf("\n");
@


1.50
log
@check we allocated the cq, not the sq, after trying to allocate the cq

found by NONAKA Kimihiro while he was porting nvme to netbsd.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.49 2016/04/18 05:59:50 dlg Exp $ */
d1276 13
@


1.49
log
@allocate an array of entries, not pointers for the queues

this solves my memory corruption problem with a samsung sm951 in a
particular slot on a dell 2950.

hilariously, i had picked values which masked this problem on
sparc64. i randomly picked 128 as the number of entries on the
queues, and dmamem allocs get rounded up to PAGE_SIZE. on amd64 and
sparc64 this meant i was asking for 128 * 8 (sizeof pointer), or
1024 bytes, which got rounded up to 4096 and 8192 on each arch
respectively. 128 * 64 (the size of a submission queue entry) is
8192, so it worked fine on sparc64 for that reason, but randomly
blows up on amd64. the 2950 above allocated mbufs out of the page
after the submission queue, which i ended over overwriting.

anyway. let's move on.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.48 2016/04/14 12:08:21 dlg Exp $ */
d1227 1
a1227 1
	if (q->q_sq_dmamem == NULL)
@


1.48
log
@shorten the io path slightly
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.47 2016/04/14 11:20:19 dlg Exp $ */
d1221 1
a1221 1
	    sizeof(struct nvme_sqe *) * entries);
d1226 1
a1226 1
	    sizeof(struct nvme_cqe *) * entries);
@


1.47
log
@apparently it's spelled NVMe, not NVME
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.46 2016/04/14 11:18:32 dlg Exp $ */
d96 2
a97 6
void	nvme_scsi_io(struct scsi_xfer *,
	    void (*)(struct nvme_softc *, struct nvme_ccb *, void *));
void	nvme_scsi_rd_fill(struct nvme_softc *, struct nvme_ccb *, void *);
void	nvme_scsi_wr_fill(struct nvme_softc *, struct nvme_ccb *, void *);
void	nvme_scsi_io_fill(struct nvme_softc *, struct nvme_ccb *,
	    struct nvme_sqe_io *);
d516 1
a516 1
		nvme_scsi_io(xs, nvme_scsi_rd_fill);
d522 1
a522 1
		nvme_scsi_io(xs, nvme_scsi_wr_fill);
d555 1
a555 2
nvme_scsi_io(struct scsi_xfer *xs,
    void (*fill)(struct nvme_softc *, struct nvme_ccb *, void *))
d563 3
d591 1
a591 1
		nvme_poll(sc, sc->sc_q, ccb, fill);
d595 1
a595 1
	nvme_q_submit(sc, sc->sc_q, ccb, fill);
d604 1
a604 10
nvme_scsi_rd_fill(struct nvme_softc *sc, struct nvme_ccb *ccb, void *slot)
{
	struct nvme_sqe_io *sqe = slot;

	sqe->opcode = NVM_CMD_READ;
	nvme_scsi_io_fill(sc, ccb, sqe);
}

void
nvme_scsi_wr_fill(struct nvme_softc *sc, struct nvme_ccb *ccb, void *slot)
a606 9

	sqe->opcode = NVM_CMD_WRITE;
	nvme_scsi_io_fill(sc, ccb, sqe);
}

void
nvme_scsi_io_fill(struct nvme_softc *sc, struct nvme_ccb *ccb,
    struct nvme_sqe_io *sqe)
{
d615 2
@


1.46
log
@provide a shutdown hook that follows the procedure in the docs
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.45 2016/04/14 10:26:33 dlg Exp $ */
d179 1
a179 1
	printf(", NVME %s", v);
@


1.45
log
@implement translation of scsi SYNC CACHE to nvme FLUSH
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.44 2016/04/14 09:07:30 dlg Exp $ */
d47 1
d72 1
d459 53
d1091 39
@


1.44
log
@bump openings to 64 to match the number of ccbs.

still a bit magical, but good enough for now.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.43 2016/04/14 06:23:20 dlg Exp $ */
d103 5
d474 4
d620 45
@


1.43
log
@if io needs more than two prpe slots, overflow into the ccb prpl

this should be enough to make io reliable
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.42 2016/04/14 06:20:34 dlg Exp $ */
d379 1
a379 1
	sc->sc_link.openings = 1; /* XXX */
@


1.42
log
@reallocate the ccbs after we figure out how big the sgls can be

we run with 2 entries for the nvme controller identify, and then bump it
up to cover the maxphys divided by the page size we negotiate.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.39 2016/04/14 06:10:49 dlg Exp $ */
d502 1
d516 12
d581 3
a583 1
		panic("not yet");
d597 8
@


1.41
log
@set the scsi status to SCSI_OK
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.40 2016/04/14 06:16:36 dlg Exp $ */
d347 9
@


1.40
log
@allocate dma memory for ccbs to use as prpe lists

prpe is short for Physical Region Page Entry. this is where long
lists of dma regions go when they wont fit into a submission queue
entry.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.39 2016/04/14 06:10:49 dlg Exp $ */
d584 1
@


1.39
log
@dont attach if the min nvme page size is bigger than the cpu page size

nvme and the host cpu need to agree on the page size because its
the implicit size of the elements in the chips scatter gather lists.
if the min nvme size is greater than the cpus page size then we
cant guarantee that io buffers are contig for nvme pages.

nvme 1.1 provides an alternative sgl mechanism, so if this really
becomes a problem in the future we can fix it on 1.1 and later
devices.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.36 2016/04/14 00:26:38 dlg Exp $ */
d966 2
d975 6
d984 2
a985 1
		if (bus_dmamap_create(sc->sc_dmat, sc->sc_mdts, sc->sc_max_sgl,
d991 4
d996 3
d1044 1
@


1.38
log
@dont complete scsi writes twice

it ends up being a use after free, which disagrees with the midlayer.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.37 2016/04/14 03:04:36 dlg Exp $ */
d309 7
a315 3
	if (NVME_CAP_MPSMIN(cap) > mps)
		mps = NVME_CAP_MPSMIN(cap);
	else if (NVME_CAP_MPSMAX(cap) < mps)
@


1.37
log
@WAITOK for the dmamap create for ccbs too

again, only called during autoconf which is a kind of process context.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.36 2016/04/14 00:26:38 dlg Exp $ */
d454 1
a454 1
		break;
@


1.36
log
@cut the memory for io buffers up into page sized chunks

nvme 1.0 does not use conventional scatter gather lists of
address+length pairs. instead, it simply expects a list of page
addresses. this should be ok if we only feed it single VA chunks
which map directly to only whole physical pages.

nvme 1.1 introduced another scather format, but still accepts the
1.0 format too. we'll stick to the 1.0 format so we can support 1.0
devs.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.35 2016/04/14 00:19:57 dlg Exp $ */
d973 1
a973 1
		    sc->sc_mps, sc->sc_mps, BUS_DMA_NOWAIT | BUS_DMA_ALLOCNOW,
@


1.35
log
@check both the admin and io queue for completions in the interrupt handler

this means we'll notice io completions.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.34 2016/04/14 00:12:51 dlg Exp $ */
d973 1
a973 1
		    sc->sc_mdts, 0, BUS_DMA_NOWAIT | BUS_DMA_ALLOCNOW,
@


1.34
log
@dont put names in arguments.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.33 2016/04/13 13:39:16 dlg Exp $ */
d1086 1
d1088 6
a1093 1
	return (nvme_q_complete(sc, sc->sc_admin_q));
@


1.33
log
@implement handling of scsi reads and writes

ive only tested reads, and not very big ones.

nvme 1.0 has a very stupid/naive idea about what constitutes a
scatter gather list. it assumes io is in whole pages in memory, but
i dont know if that is true in our kernel.

this could be cleaned up a bit, and it currently runs with a single
opening for the whole scsi layer and a bunch of magic values for
the size and number of the io queues.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.29 2016/04/13 13:09:36 dlg Exp $ */
d60 1
a60 1
	    void (*fill)(struct nvme_softc *, struct nvme_ccb *, void *));
@


1.32
log
@enable interrupts before attaching the scsibus
@
text
@d94 9
d443 13
d478 103
@


1.31
log
@allocate a queue for io commands and tell the chip about it.

this is necessary to run reads and writes against the device.
@
text
@d347 2
@


1.30
log
@nvme_q_create() issues the commands to tell the chip about io queues
@
text
@d336 11
d366 2
@


1.29
log
@stub out handling of TEST_UNIT_READY, PREVENT_ALLOW, and START_STOP

at the moment this just pretends the commands completed fine.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.27 2016/04/13 12:59:28 dlg Exp $ */
d70 1
d760 44
@


1.28
log
@implement handling of scsi read capacity commands

read cap 16 claims the devices are thin.
@
text
@d428 7
@


1.27
log
@implement basic scsi inquiry handling

most values are as per the nvm to scsi mapping guide. this doesnt
do vpd at all, so no devids or serial numbers just yet.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.24 2016/04/13 12:21:15 dlg Exp $ */
d32 1
a32 1
#include <scsi/scsi_all.h>
d95 2
d421 7
d478 67
@


1.26
log
@implement the guts of the scsi probe and free function

probe issues a namespace identify against the "target". if it works
it stashes a copy of the info, otherwise it tells the midlayer to
avoid it.

free gets rid of the stashed info.
@
text
@d93 3
d415 8
d424 47
@


1.25
log
@wire up the scsi midlayer. scsibus should appear after this.
@
text
@d362 45
a406 1
	return (ENXIO);
d419 5
d425 1
@


1.24
log
@allocate an array of things to hold info about namespaces

so far the only useful info is namespace identify info
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.23 2016/04/13 12:14:12 dlg Exp $ */
d273 1
d332 14
@


1.23
log
@stash the controller identify and number of namespaces in the softc.

the nn is used to size the scsi bus, and the controller identify is used
to build responses for various scsi commands.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.21 2016/04/13 11:56:50 dlg Exp $ */
d328 3
@


1.22
log
@provide an scsi_adapter and stub functions for emulation to sit in
@
text
@a516 2
	identify = NVME_DMA_KVA(mem);

d529 2
d542 4
@


1.21
log
@nvme_sqe_fill will post a copy of an sqe from a caller
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.20 2016/04/13 11:54:33 dlg Exp $ */
d32 1
d81 12
d339 19
@


1.20
log
@poll for command completion on the cqe itll be of calling nvme_intr
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.18 2016/04/13 11:48:38 dlg Exp $ */
d63 1
d410 9
@


1.19
log
@make nvme_poll return the flags from the completion queue entry

it's still 0 on success, but is the actual bits rather than a mashup of it
@
text
@d378 1
a378 1
		if (nvme_intr(sc) == 0)
@


1.18
log
@keep track of the queue id in nvme_queue.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.17 2016/04/13 11:45:06 dlg Exp $ */
d389 1
a389 1
	return (NVME_CQE_SCT(flags) | NVME_CQE_SC(flags));
@


1.17
log
@rename idx to id in nvme_q_alloc

make it a u_int16_t like the hw while here.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.16 2016/04/13 11:42:04 dlg Exp $ */
d613 1
@


1.16
log
@nvme_dmamem_alloc runs in autoconf or process context, so it can sleep
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.15 2016/04/12 10:20:25 dlg Exp $ */
d67 1
a67 1
	nvme_q_alloc(struct nvme_softc *, u_int, u_int, u_int);
d588 1
a588 1
nvme_q_alloc(struct nvme_softc *sc, u_int idx, u_int entries, u_int dstrd)
d611 2
a612 2
	q->q_sqtdbl = NVME_SQTDBL(idx, dstrd);
	q->q_cqhdbl = NVME_CQHDBL(idx, dstrd);
@


1.15
log
@shuffle attach so we read chip capabilities before operating on it

most importantly this gets the proper timeout for chip enables/disables.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.14 2016/01/15 06:38:33 dlg Exp $ */
d655 1
a655 1
	ndm = malloc(sizeof(*ndm), M_DEVBUF, M_NOWAIT | M_ZERO);
d662 1
a662 1
	    BUS_DMA_NOWAIT | BUS_DMA_ALLOCNOW, &ndm->ndm_map) != 0)
d666 1
a666 1
	    1, &nsegs, BUS_DMA_NOWAIT | BUS_DMA_ZERO) != 0)
d670 1
a670 1
	    &ndm->ndm_kva, BUS_DMA_NOWAIT) != 0)
d674 1
a674 1
	    NULL, BUS_DMA_NOWAIT) != 0)
@


1.14
log
@when enabling the controller, wait till CSTS.RDY lights up.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.13 2016/01/15 06:34:19 dlg Exp $ */
d264 4
a276 5
	if (nvme_disable(sc) != 0) {
		printf("%s: unable to disable controller\n", DEVNAME(sc));
		return (1);
	}

d288 5
a292 3
	mtx_init(&sc->sc_ccb_mtx, IPL_BIO);
	SIMPLEQ_INIT(&sc->sc_ccb_list);
	scsi_iopool_init(&sc->sc_iopool, sc, nvme_ccb_get, nvme_ccb_put);
@


1.13
log
@feng shui.

dont need a billion tabs for the function prototypes. just some
akward line wrappings.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.12 2016/01/15 03:28:41 dlg Exp $ */
d209 4
a217 4
	nvme_write4(sc, NVME_AQA, NVME_AQA_ACQS(sc->sc_admin_q->q_entries) |
	    NVME_AQA_ASQS(sc->sc_admin_q->q_entries));
	nvme_barrier(sc, 0, sc->sc_ios, BUS_SPACE_BARRIER_WRITE);

d231 1
a231 1
	return (0);
@


1.12
log
@handle the version register like ahci.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.11 2016/01/15 03:12:04 dlg Exp $ */
d43 35
a77 43
int			nvme_ready(struct nvme_softc *, u_int32_t);
int			nvme_enable(struct nvme_softc *, u_int);
int			nvme_disable(struct nvme_softc *);

void			nvme_version(struct nvme_softc *, u_int32_t);
void			nvme_dumpregs(struct nvme_softc *);
int			nvme_identify(struct nvme_softc *, u_int);
void			nvme_fill_identify(struct nvme_softc *,
			    struct nvme_ccb *, void *);

int			nvme_ccbs_alloc(struct nvme_softc *, u_int);
void			nvme_ccbs_free(struct nvme_softc *);

void *			nvme_ccb_get(void *);
void			nvme_ccb_put(void *, void *);

int			nvme_poll(struct nvme_softc *, struct nvme_queue *,
			    struct nvme_ccb *,
			    void (*fill)(struct nvme_softc *,
			     struct nvme_ccb *, void *));
void			nvme_poll_fill(struct nvme_softc *,
			    struct nvme_ccb *, void *);
void			nvme_poll_done(struct nvme_softc *,
			    struct nvme_ccb *, struct nvme_cqe *);
void			nvme_empty_done(struct nvme_softc *,
			    struct nvme_ccb *, struct nvme_cqe *);

struct nvme_queue *	nvme_q_alloc(struct nvme_softc *,
			    u_int, u_int, u_int);
void			nvme_q_submit(struct nvme_softc *,
			    struct nvme_queue *, struct nvme_ccb *,
			    void (*)(struct nvme_softc *,
			     struct nvme_ccb *, void *));
int			nvme_q_complete(struct nvme_softc *,
			    struct nvme_queue *q);
void			nvme_q_free(struct nvme_softc *,
			    struct nvme_queue *);

struct nvme_dmamem *	nvme_dmamem_alloc(struct nvme_softc *, size_t);
void			nvme_dmamem_free(struct nvme_softc *,
			    struct nvme_dmamem *);
void			nvme_dmamem_sync(struct nvme_softc *,
			    struct nvme_dmamem *, int);
@


1.11
log
@wrap up dma syncs for the whole mapping.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.10 2014/11/04 12:48:22 dlg Exp $ */
d134 1
a134 1
	u_int16_t minor;
d136 16
a151 3
	minor = NVME_VS_MNR(version);
	minor = ((minor >> 8) * 10) + (minor & 0xff);
	printf(", NVME %d.%d", NVME_VS_MJR(version), minor);
@


1.10
log
@shuffle when the cid is set on submission queue entries so its after
when the caller fills the entry.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.9 2014/11/04 12:41:34 dlg Exp $ */
d84 2
d424 2
d442 1
d474 1
a474 2
	bus_dmamap_sync(sc->sc_dmat, NVME_DMA_MAP(mem),
	    0, sizeof(*identify), BUS_DMASYNC_PREREAD);
d476 1
a476 2
	bus_dmamap_sync(sc->sc_dmat, NVME_DMA_MAP(mem),
	    0, sizeof(*identify), BUS_DMASYNC_POSTREAD);
d612 2
a613 4
	bus_dmamap_sync(sc->sc_dmat, NVME_DMA_MAP(q->q_sq_dmamem),
	    0, NVME_DMA_LEN(q->q_sq_dmamem), BUS_DMASYNC_PREWRITE);
	bus_dmamap_sync(sc->sc_dmat, NVME_DMA_MAP(q->q_cq_dmamem),
	    0, NVME_DMA_LEN(q->q_cq_dmamem), BUS_DMASYNC_PREREAD);
d628 2
a629 4
	bus_dmamap_sync(sc->sc_dmat, NVME_DMA_MAP(q->q_cq_dmamem),
	    0, NVME_DMA_LEN(q->q_cq_dmamem), BUS_DMASYNC_POSTREAD);
	bus_dmamap_sync(sc->sc_dmat, NVME_DMA_MAP(q->q_sq_dmamem),
	    0, NVME_DMA_LEN(q->q_sq_dmamem), BUS_DMASYNC_POSTWRITE);
d683 7
@


1.9
log
@when reading the completion queue, it helps to write where we've read up
to to the completion queue head doorbell instead of the submission queue
tail doorbell.

this lets us submit more than one command to the chip.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.8 2014/09/12 06:54:38 dlg Exp $ */
d336 1
a337 1
	(*fill)(sc, ccb, sqe);
a359 1
	state.s.cid = ccb->ccb_id;
@


1.8
log
@dont leak a ccb in identify
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.7 2014/09/12 06:34:14 dlg Exp $ */
d441 1
a441 1
		nvme_write4(sc, q->q_sqtdbl, q->q_cq_head = head);
@


1.7
log
@fix some format string issues
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.6 2014/07/13 23:10:23 deraadt Exp $ */
d475 2
@


1.6
log
@Some reallocarray() use; review Jean-Philippe Ouellet, patrick keshishian
ok tedu
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.5 2014/07/12 18:48:17 tedu Exp $ */
d153 1
a153 1
	printf("%s:  dstrd %llu\n", DEVNAME(sc), NVME_CAP_DSTRD(r8));
d159 1
a159 1
	printf("%s: vs   0x%08lx\n", DEVNAME(sc), nvme_read4(sc, NVME_VS));
d162 1
a162 1
	printf("%s: cc   0x%08lx\n", DEVNAME(sc), r4);
d171 2
a172 2
	printf("%s: csts 0x%08lx\n", DEVNAME(sc), nvme_read4(sc, NVME_CSTS));
	printf("%s: aqa  0x%08lx\n", DEVNAME(sc), nvme_read4(sc, NVME_AQA));
@


1.5
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.4 2014/04/16 01:28:02 dlg Exp $ */
d514 1
a514 1
	sc->sc_ccbs = malloc(sizeof(*ccb) * nccbs, M_DEVBUF,
@


1.4
log
@set ourselves up to respect the max data transfer size.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.3 2014/04/16 00:26:59 dlg Exp $ */
d574 1
a574 1
	free(sc->sc_ccbs, M_DEVBUF);
d618 1
a618 1
	free(q, M_DEVBUF);
d632 1
a632 1
	free(q, M_DEVBUF);
d680 1
a680 1
	free(ndm, M_DEVBUF);
d692 1
a692 1
	free(ndm, M_DEVBUF);
@


1.3
log
@have nvme_poll return the status bigs from the completion ring
entry. if i ever implement timeouts ill use high bits in the int
or the phase bit to indicate non chip related errors. a successful
chip status conveniently maps to 0. how handy.

this lets me move the completion handling for the controller
identification commands back into the caller. at the moment im just
printing out controller and firmware details like we do on
mfi/mpii/mfii.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.2 2014/04/15 10:28:07 dlg Exp $ */
d49 1
a49 1
int			nvme_identify(struct nvme_softc *);
d302 1
a302 1
	if (nvme_identify(sc) != 0) {
d448 1
a448 1
nvme_identify(struct nvme_softc *sc)
d454 1
d484 6
@


1.2
log
@i wanted to work on this in the tree so i could commit lots of small
steps, but unfortunately the next step after "talk to the chips
registers" was "get command queues working" which ended up being a
huge amount of plumbing.

anyway, this lets me successfully run an identify controller command
against the chip and should be cut up the right way to be usable
for io command submissions. will need to think about how to avoid
overflowing rings though.
@
text
@d1 1
a1 1
/*	$OpenBSD: nvme.c,v 1.1 2014/04/12 05:06:58 dlg Exp $ */
a51 2
void			nvme_identify_done(struct nvme_softc *,
			    struct nvme_ccb *, struct nvme_cqe *);
d67 2
d357 1
d380 3
a382 1
	return (0);
d404 6
d450 3
a453 1
	struct nvme_dmamem *mem;
d460 1
a460 1
	mem = nvme_dmamem_alloc(sc, sizeof(struct nvm_identify_controller));
d464 3
a466 1
	ccb->ccb_done = nvme_identify_done;
d469 2
d472 2
d475 10
a494 4
	struct nvm_identify_controller *identify = NVME_DMA_KVA(mem);

	bus_dmamap_sync(sc->sc_dmat, NVME_DMA_MAP(mem),
	    0, sizeof(*identify), BUS_DMASYNC_PREREAD);
a498 20
}

void
nvme_identify_done(struct nvme_softc *sc, struct nvme_ccb *ccb,
    struct nvme_cqe *cqe)
{
	struct nvme_dmamem *mem = ccb->ccb_cookie;
	struct nvm_identify_controller *identify = NVME_DMA_KVA(mem);
	u_int64_t flags = lemtoh16(&cqe->flags);

	bus_dmamap_sync(sc->sc_dmat, NVME_DMA_MAP(mem),
	    0, sizeof(*identify), BUS_DMASYNC_POSTREAD);

	printf("%s: dnr %c m %c sqt %x sc %x\n", DEVNAME(sc),
	    ISSET(flags, NVME_CQE_DNR) ? 'Y' : 'N',
	    ISSET(flags, NVME_CQE_M) ? 'Y' : 'N',
	    NVME_CQE_SQT(flags), NVME_CQE_SC(flags));

	printf("%s: identify %p sn %s mn %s fr %s\n", DEVNAME(sc), mem,
	    identify->sn, identify->mn, identify->fr);
@


1.1
log
@start working on a driver for non volatile memory express controllers.
@
text
@d1 1
a1 1
/*	$OpenBSD$ */
a30 3
#include <dev/ic/nvmevar.h>
#include <dev/ic/nvmereg.h>

d34 3
d49 30
a78 4

struct nvme_queue *	nvme_queue_alloc(struct nvme_softc *,
			   u_int, u_int, u_int);
void			nvme_queue_free(struct nvme_softc *,
d236 1
a236 1
		    nvme_ready(sc, NVME_CSTS) != 0)
a265 1
	nvme_dumpregs(sc);
a269 1
	nvme_dumpregs(sc);
d279 6
d286 1
a286 1
	sc->sc_admin_q = nvme_queue_alloc(sc, NVME_ADMIN_Q, 128, dstrd);
d292 2
a293 3
	sc->sc_q = nvme_queue_alloc(sc, 1, 128, dstrd);
	if (sc->sc_q == NULL) {
		printf("%s: unable to allocate queue\n", DEVNAME(sc));
d297 9
a305 2
	nvme_enable(sc, mps);
	nvme_dumpregs(sc);
d309 4
d314 123
a436 1
	nvme_queue_free(sc, sc->sc_admin_q);
d438 87
d528 39
d568 1
a568 1
nvme_queue_alloc(struct nvme_softc *sc, u_int idx, u_int entries, u_int dstrd)
d576 2
a577 1
	q->q_sq_dmamem = nvme_dmamem_alloc(sc, 64 * entries); /* XXX */
d581 2
a582 1
	q->q_cq_dmamem = nvme_dmamem_alloc(sc, 16 * entries); /* XXX */
d586 5
d592 1
a592 1
	q->q_cqhdbl = NVME_CQTDBL(idx, dstrd);
d594 8
a601 2
	q->q_sq_head = 0;
	q->q_cq_tail = 0;
d614 1
a614 1
nvme_queue_free(struct nvme_softc *sc, struct nvme_queue *q)
d616 4
d628 3
a630 1
	return (-1);
d649 1
a649 1
	if (bus_dmamem_alloc(sc->sc_dmat, size, PAGE_SIZE, 0, &ndm->ndm_seg,
@

