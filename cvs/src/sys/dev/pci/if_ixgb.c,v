head	1.71;
access;
symbols
	OPENBSD_6_2:1.71.0.2
	OPENBSD_6_2_BASE:1.71
	OPENBSD_6_1:1.71.0.4
	OPENBSD_6_1_BASE:1.71
	OPENBSD_6_0:1.69.0.4
	OPENBSD_6_0_BASE:1.69
	OPENBSD_5_9:1.68.0.2
	OPENBSD_5_9_BASE:1.68
	OPENBSD_5_8:1.65.0.4
	OPENBSD_5_8_BASE:1.65
	OPENBSD_5_7:1.63.0.4
	OPENBSD_5_7_BASE:1.63
	OPENBSD_5_6:1.62.0.4
	OPENBSD_5_6_BASE:1.62
	OPENBSD_5_5:1.60.0.4
	OPENBSD_5_5_BASE:1.60
	OPENBSD_5_4:1.59.0.10
	OPENBSD_5_4_BASE:1.59
	OPENBSD_5_3:1.59.0.8
	OPENBSD_5_3_BASE:1.59
	OPENBSD_5_2:1.59.0.6
	OPENBSD_5_2_BASE:1.59
	OPENBSD_5_1_BASE:1.59
	OPENBSD_5_1:1.59.0.4
	OPENBSD_5_0:1.59.0.2
	OPENBSD_5_0_BASE:1.59
	OPENBSD_4_9:1.57.0.2
	OPENBSD_4_9_BASE:1.57
	OPENBSD_4_8:1.55.0.4
	OPENBSD_4_8_BASE:1.55
	OPENBSD_4_7:1.55.0.2
	OPENBSD_4_7_BASE:1.55
	OPENBSD_4_6:1.53.0.4
	OPENBSD_4_6_BASE:1.53
	OPENBSD_4_5:1.52.0.2
	OPENBSD_4_5_BASE:1.52
	OPENBSD_4_4:1.44.0.2
	OPENBSD_4_4_BASE:1.44
	OPENBSD_4_3:1.40.0.2
	OPENBSD_4_3_BASE:1.40
	OPENBSD_4_2:1.36.0.4
	OPENBSD_4_2_BASE:1.36
	OPENBSD_4_1:1.36.0.2
	OPENBSD_4_1_BASE:1.36
	OPENBSD_4_0:1.29.0.2
	OPENBSD_4_0_BASE:1.29
	OPENBSD_3_9:1.9.0.2
	OPENBSD_3_9_BASE:1.9;
locks; strict;
comment	@ * @;


1.71
date	2017.01.22.10.17.38;	author dlg;	state Exp;
branches;
next	1.70;
commitid	VyLWTsbepAOk7VQM;

1.70
date	2016.11.29.10.22.30;	author jsg;	state Exp;
branches;
next	1.69;
commitid	ZQetSMB5ilG2z10X;

1.69
date	2016.04.13.10.34.32;	author mpi;	state Exp;
branches;
next	1.68;
commitid	8YSL8ByWzGeIGBiJ;

1.68
date	2015.11.25.03.09.59;	author dlg;	state Exp;
branches;
next	1.67;
commitid	B0kwmVGiD5DVx4kv;

1.67
date	2015.11.20.03.35.23;	author dlg;	state Exp;
branches;
next	1.66;
commitid	eYnPulzvLjDImPCa;

1.66
date	2015.10.25.13.04.28;	author mpi;	state Exp;
branches;
next	1.65;
commitid	hPF95ClMUQfeqQDX;

1.65
date	2015.06.24.09.40.54;	author mpi;	state Exp;
branches;
next	1.64;
commitid	MVWrtktB46JRxFWT;

1.64
date	2015.04.30.07.51.07;	author mpi;	state Exp;
branches;
next	1.63;
commitid	H09AuNxNnUcYramX;

1.63
date	2014.12.22.02.28.52;	author tedu;	state Exp;
branches;
next	1.62;
commitid	yM2VFFhpDTeFQlve;

1.62
date	2014.07.13.23.10.23;	author deraadt;	state Exp;
branches;
next	1.61;
commitid	JtO5uXxVcnZfhUkR;

1.61
date	2014.07.12.18.48.51;	author tedu;	state Exp;
branches;
next	1.60;
commitid	OBNa5kfxQ2UXoiIw;

1.60
date	2013.11.26.09.50.33;	author mpi;	state Exp;
branches;
next	1.59;

1.59
date	2011.04.05.18.01.21;	author henning;	state Exp;
branches;
next	1.58;

1.58
date	2011.04.03.15.36.02;	author jasper;	state Exp;
branches;
next	1.57;

1.57
date	2010.09.20.07.50.19;	author deraadt;	state Exp;
branches;
next	1.56;

1.56
date	2010.08.27.08.24.53;	author deraadt;	state Exp;
branches;
next	1.55;

1.55
date	2009.08.13.14.24.47;	author jasper;	state Exp;
branches;
next	1.54;

1.54
date	2009.08.10.19.41.05;	author deraadt;	state Exp;
branches;
next	1.53;

1.53
date	2009.06.24.13.36.56;	author deraadt;	state Exp;
branches;
next	1.52;

1.52
date	2008.11.28.02.44.18;	author brad;	state Exp;
branches;
next	1.51;

1.51
date	2008.11.09.15.08.26;	author naddy;	state Exp;
branches;
next	1.50;

1.50
date	2008.10.28.05.43.11;	author brad;	state Exp;
branches;
next	1.49;

1.49
date	2008.10.21.00.26.04;	author brad;	state Exp;
branches;
next	1.48;

1.48
date	2008.10.02.20.21.14;	author brad;	state Exp;
branches;
next	1.47;

1.47
date	2008.09.30.17.59.22;	author brad;	state Exp;
branches;
next	1.46;

1.46
date	2008.09.24.19.12.59;	author chl;	state Exp;
branches;
next	1.45;

1.45
date	2008.09.10.14.01.22;	author blambert;	state Exp;
branches;
next	1.44;

1.44
date	2008.06.08.16.54.34;	author brad;	state Exp;
branches;
next	1.43;

1.43
date	2008.06.08.16.53.23;	author brad;	state Exp;
branches;
next	1.42;

1.42
date	2008.06.08.16.20.27;	author reyk;	state Exp;
branches;
next	1.41;

1.41
date	2008.06.03.12.27.23;	author reyk;	state Exp;
branches;
next	1.40;

1.40
date	2008.03.02.08.42.42;	author brad;	state Exp;
branches;
next	1.39;

1.39
date	2008.02.19.18.47.18;	author brad;	state Exp;
branches;
next	1.38;

1.38
date	2007.10.01.15.34.48;	author krw;	state Exp;
branches;
next	1.37;

1.37
date	2007.09.19.03.50.24;	author brad;	state Exp;
branches;
next	1.36;

1.36
date	2006.12.04.14.35.20;	author reyk;	state Exp;
branches;
next	1.35;

1.35
date	2006.11.28.04.45.08;	author brad;	state Exp;
branches;
next	1.34;

1.34
date	2006.11.28.04.26.50;	author brad;	state Exp;
branches;
next	1.33;

1.33
date	2006.11.28.02.22.39;	author brad;	state Exp;
branches;
next	1.32;

1.32
date	2006.11.18.19.20.17;	author brad;	state Exp;
branches;
next	1.31;

1.31
date	2006.11.18.18.46.20;	author brad;	state Exp;
branches;
next	1.30;

1.30
date	2006.10.02.00.28.09;	author brad;	state Exp;
branches;
next	1.29;

1.29
date	2006.08.18.06.02.45;	author brad;	state Exp;
branches;
next	1.28;

1.28
date	2006.08.14.02.22.13;	author brad;	state Exp;
branches;
next	1.27;

1.27
date	2006.08.09.05.22.17;	author brad;	state Exp;
branches;
next	1.26;

1.26
date	2006.08.09.03.48.25;	author brad;	state Exp;
branches;
next	1.25;

1.25
date	2006.08.04.14.33.02;	author brad;	state Exp;
branches;
next	1.24;

1.24
date	2006.08.04.14.25.24;	author brad;	state Exp;
branches;
next	1.23;

1.23
date	2006.08.04.02.44.50;	author brad;	state Exp;
branches;
next	1.22;

1.22
date	2006.08.01.23.50.14;	author brad;	state Exp;
branches;
next	1.21;

1.21
date	2006.07.10.00.25.23;	author brad;	state Exp;
branches;
next	1.20;

1.20
date	2006.06.22.04.57.28;	author brad;	state Exp;
branches;
next	1.19;

1.19
date	2006.06.21.07.15.58;	author brad;	state Exp;
branches;
next	1.18;

1.18
date	2006.05.28.00.04.24;	author jason;	state Exp;
branches;
next	1.17;

1.17
date	2006.05.27.10.03.15;	author brad;	state Exp;
branches;
next	1.16;

1.16
date	2006.05.26.20.50.41;	author deraadt;	state Exp;
branches;
next	1.15;

1.15
date	2006.05.20.03.47.56;	author brad;	state Exp;
branches;
next	1.14;

1.14
date	2006.05.01.21.03.42;	author brad;	state Exp;
branches;
next	1.13;

1.13
date	2006.04.20.20.31.12;	author miod;	state Exp;
branches;
next	1.12;

1.12
date	2006.03.27.17.07.10;	author brad;	state Exp;
branches;
next	1.11;

1.11
date	2006.03.25.22.41.45;	author djm;	state Exp;
branches;
next	1.10;

1.10
date	2006.03.05.01.13.37;	author brad;	state Exp;
branches;
next	1.9;

1.9
date	2006.02.26.01.27.16;	author brad;	state Exp;
branches;
next	1.8;

1.8
date	2006.02.10.09.12.26;	author brad;	state Exp;
branches;
next	1.7;

1.7
date	2006.02.10.08.56.14;	author brad;	state Exp;
branches;
next	1.6;

1.6
date	2006.02.01.20.13.49;	author brad;	state Exp;
branches;
next	1.5;

1.5
date	2006.02.01.20.11.30;	author brad;	state Exp;
branches;
next	1.4;

1.4
date	2006.02.01.19.07.13;	author otto;	state Exp;
branches;
next	1.3;

1.3
date	2005.12.10.19.19.40;	author brad;	state Exp;
branches;
next	1.2;

1.2
date	2005.11.15.18.17.06;	author brad;	state Exp;
branches;
next	1.1;

1.1
date	2005.11.14.23.25.43;	author brad;	state Exp;
branches;
next	;


desc
@@


1.71
log
@move counting if_opackets next to counting if_obytes in if_enqueue.

this means packets are consistently counted in one place, unlike the
many and various ways that drivers thought they should do it.

ok mpi@@ deraadt@@
@
text
@/**************************************************************************

Copyright (c) 2001-2005, Intel Corporation
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

 1. Redistributions of source code must retain the above copyright notice,
    this list of conditions and the following disclaimer.

 2. Redistributions in binary form must reproduce the above copyright
    notice, this list of conditions and the following disclaimer in the
    documentation and/or other materials provided with the distribution.

 3. Neither the name of the Intel Corporation nor the names of its
    contributors may be used to endorse or promote products derived from
    this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.

***************************************************************************/

/* $OpenBSD: if_ixgb.c,v 1.70 2016/11/29 10:22:30 jsg Exp $ */

#include <dev/pci/if_ixgb.h>

#ifdef IXGB_DEBUG
/*********************************************************************
 *  Set this to one to display debug statistics
 *********************************************************************/
int             ixgb_display_debug_stats = 0;
#endif

/*********************************************************************
 *  Driver version
 *********************************************************************/

#define IXGB_DRIVER_VERSION	"6.1.0"

/*********************************************************************
 *  PCI Device ID Table
 *********************************************************************/

const struct pci_matchid ixgb_devices[] = {
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82597EX },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82597EX_SR },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82597EX_LR },
	{ PCI_VENDOR_INTEL, PCI_PRODUCT_INTEL_82597EX_CX4 },
};

/*********************************************************************
 *  Function prototypes
 *********************************************************************/
int  ixgb_probe(struct device *, void *, void *);
void ixgb_attach(struct device *, struct device *, void *);
int  ixgb_intr(void *);
void ixgb_start(struct ifnet *);
int  ixgb_ioctl(struct ifnet *, u_long, caddr_t);
void ixgb_watchdog(struct ifnet *);
void ixgb_init(void *);
void ixgb_stop(void *);
void ixgb_media_status(struct ifnet *, struct ifmediareq *);
int  ixgb_media_change(struct ifnet *);
void ixgb_identify_hardware(struct ixgb_softc *);
int  ixgb_allocate_pci_resources(struct ixgb_softc *);
void ixgb_free_pci_resources(struct ixgb_softc *);
void ixgb_local_timer(void *);
int  ixgb_hardware_init(struct ixgb_softc *);
void ixgb_setup_interface(struct ixgb_softc *);
int  ixgb_setup_transmit_structures(struct ixgb_softc *);
void ixgb_initialize_transmit_unit(struct ixgb_softc *);
int  ixgb_setup_receive_structures(struct ixgb_softc *);
void ixgb_initialize_receive_unit(struct ixgb_softc *);
void ixgb_enable_intr(struct ixgb_softc *);
void ixgb_disable_intr(struct ixgb_softc *);
void ixgb_free_transmit_structures(struct ixgb_softc *);
void ixgb_free_receive_structures(struct ixgb_softc *);
void ixgb_update_stats_counters(struct ixgb_softc *);
void ixgb_txeof(struct ixgb_softc *);
int  ixgb_allocate_receive_structures(struct ixgb_softc *);
int  ixgb_allocate_transmit_structures(struct ixgb_softc *);
void ixgb_rxeof(struct ixgb_softc *, int);
void
ixgb_receive_checksum(struct ixgb_softc *,
		      struct ixgb_rx_desc * rx_desc,
		      struct mbuf *);
void
ixgb_transmit_checksum_setup(struct ixgb_softc *,
			     struct mbuf *,
			     u_int8_t *);
void ixgb_set_promisc(struct ixgb_softc *);
void ixgb_set_multi(struct ixgb_softc *);
#ifdef IXGB_DEBUG
void ixgb_print_hw_stats(struct ixgb_softc *);
#endif
void ixgb_update_link_status(struct ixgb_softc *);
int
ixgb_get_buf(struct ixgb_softc *, int i,
	     struct mbuf *);
void ixgb_enable_hw_vlans(struct ixgb_softc *);
int  ixgb_encap(struct ixgb_softc *, struct mbuf *);
int
ixgb_dma_malloc(struct ixgb_softc *, bus_size_t,
		struct ixgb_dma_alloc *, int);
void ixgb_dma_free(struct ixgb_softc *, struct ixgb_dma_alloc *);

/*********************************************************************
 *  OpenBSD Device Interface Entry Points
 *********************************************************************/

struct cfattach ixgb_ca = {
	sizeof(struct ixgb_softc), ixgb_probe, ixgb_attach
};

struct cfdriver ixgb_cd = {
	NULL, "ixgb", DV_IFNET
};

/* some defines for controlling descriptor fetches in h/w */
#define RXDCTL_PTHRESH_DEFAULT 0	/* chip considers prefech below this */
#define RXDCTL_HTHRESH_DEFAULT 0	/* chip will only prefetch if tail is
					 * pushed this many descriptors from
					 * head */
#define RXDCTL_WTHRESH_DEFAULT 0	/* chip writes back at this many or RXT0 */


/*********************************************************************
 *  Device identification routine
 *
 *  ixgb_probe determines if the driver should be loaded on
 *  adapter based on PCI vendor/device id of the adapter.
 *
 *  return 0 on no match, positive on match
 *********************************************************************/

int
ixgb_probe(struct device *parent, void *match, void *aux)
{
	INIT_DEBUGOUT("ixgb_probe: begin");

	return (pci_matchbyid((struct pci_attach_args *)aux, ixgb_devices,
	    nitems(ixgb_devices)));
}

/*********************************************************************
 *  Device initialization routine
 *
 *  The attach entry point is called when the driver is being loaded.
 *  This routine identifies the type of hardware, allocates all resources
 *  and initializes the hardware.
 *
 *********************************************************************/

void
ixgb_attach(struct device *parent, struct device *self, void *aux)
{
	struct pci_attach_args *pa = aux;
	struct ixgb_softc *sc;
	int             tsize, rsize;

	INIT_DEBUGOUT("ixgb_attach: begin");

	sc = (struct ixgb_softc *)self;
	sc->osdep.ixgb_pa = *pa;

	timeout_set(&sc->timer_handle, ixgb_local_timer, sc);

	/* Determine hardware revision */
	ixgb_identify_hardware(sc);

	/* Parameters (to be read from user) */
	sc->num_tx_desc = IXGB_MAX_TXD;
	sc->num_rx_desc = IXGB_MAX_RXD;
	sc->tx_int_delay = TIDV;
	sc->rx_int_delay = RDTR;
	sc->rx_buffer_len = IXGB_RXBUFFER_2048;

	/*
	 * These parameters control the automatic generation(Tx) and
	 * response(Rx) to Ethernet PAUSE frames.
	 */
	sc->hw.fc.high_water = FCRTH;
	sc->hw.fc.low_water = FCRTL;
	sc->hw.fc.pause_time = FCPAUSE;
	sc->hw.fc.send_xon = TRUE;
	sc->hw.fc.type = FLOW_CONTROL;

	/* Set the max frame size assuming standard ethernet sized frames */
	sc->hw.max_frame_size = IXGB_MAX_JUMBO_FRAME_SIZE;

	if (ixgb_allocate_pci_resources(sc))
		goto err_pci;

	tsize = IXGB_ROUNDUP(sc->num_tx_desc * sizeof(struct ixgb_tx_desc),
	    IXGB_MAX_TXD * sizeof(struct ixgb_tx_desc));
	tsize = IXGB_ROUNDUP(tsize, PAGE_SIZE);

	/* Allocate Transmit Descriptor ring */
	if (ixgb_dma_malloc(sc, tsize, &sc->txdma, BUS_DMA_NOWAIT)) {
		printf("%s: Unable to allocate TxDescriptor memory\n",
		       sc->sc_dv.dv_xname);
		goto err_tx_desc;
	}
	sc->tx_desc_base = (struct ixgb_tx_desc *) sc->txdma.dma_vaddr;

	rsize = IXGB_ROUNDUP(sc->num_rx_desc * sizeof(struct ixgb_rx_desc),
	    IXGB_MAX_RXD * sizeof(struct ixgb_rx_desc));
	rsize = IXGB_ROUNDUP(rsize, PAGE_SIZE);

	/* Allocate Receive Descriptor ring */
	if (ixgb_dma_malloc(sc, rsize, &sc->rxdma, BUS_DMA_NOWAIT)) {
		printf("%s: Unable to allocate rx_desc memory\n",
		       sc->sc_dv.dv_xname);
		goto err_rx_desc;
	}
	sc->rx_desc_base = (struct ixgb_rx_desc *) sc->rxdma.dma_vaddr;

	/* Initialize the hardware */
	if (ixgb_hardware_init(sc)) {
		printf("%s: Unable to initialize the hardware\n",
		       sc->sc_dv.dv_xname);
		goto err_hw_init;
	}

	/* Setup OS specific network interface */
	ixgb_setup_interface(sc);

	/* Initialize statistics */
	ixgb_clear_hw_cntrs(&sc->hw);
	ixgb_update_stats_counters(sc);
	ixgb_update_link_status(sc);

	printf(", address %s\n", ether_sprintf(sc->interface_data.ac_enaddr));

	INIT_DEBUGOUT("ixgb_attach: end");
	return;

err_hw_init:
	ixgb_dma_free(sc, &sc->rxdma);
err_rx_desc:
	ixgb_dma_free(sc, &sc->txdma);
err_tx_desc:
err_pci:
	ixgb_free_pci_resources(sc);
}

/*********************************************************************
 *  Transmit entry point
 *
 *  ixgb_start is called by the stack to initiate a transmit.
 *  The driver will remain in this routine as long as there are
 *  packets to transmit and transmit resources are available.
 *  In case resources are not available stack is notified and
 *  the packet is requeued.
 **********************************************************************/

void
ixgb_start(struct ifnet *ifp)
{
	struct mbuf    *m_head;
	struct ixgb_softc *sc = ifp->if_softc;
	int		post = 0;

	if (!(ifp->if_flags & IFF_RUNNING) || ifq_is_oactive(&ifp->if_snd))
		return;

	if (!sc->link_active)
		return;

	bus_dmamap_sync(sc->txdma.dma_tag, sc->txdma.dma_map, 0,
	    sc->txdma.dma_map->dm_mapsize,
	    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);

	for (;;) {
		m_head = ifq_deq_begin(&ifp->if_snd);
		if (m_head == NULL)
			break;

		if (ixgb_encap(sc, m_head)) {
			ifq_deq_rollback(&ifp->if_snd, m_head);
			ifq_set_oactive(&ifp->if_snd);
			break;
		}

		ifq_deq_commit(&ifp->if_snd, m_head);

#if NBPFILTER > 0
		/* Send a copy of the frame to the BPF listener */
		if (ifp->if_bpf)
			bpf_mtap_ether(ifp->if_bpf, m_head, BPF_DIRECTION_OUT);
#endif

		/* Set timeout in case hardware has problems transmitting */
		ifp->if_timer = IXGB_TX_TIMEOUT;

		post = 1;
	}

	bus_dmamap_sync(sc->txdma.dma_tag, sc->txdma.dma_map, 0,
	    sc->txdma.dma_map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
	/*
	 * Advance the Transmit Descriptor Tail (Tdt),
	 * this tells the E1000 that this frame
	 * is available to transmit.
	 */
	if (post)
		IXGB_WRITE_REG(&sc->hw, TDT, sc->next_avail_tx_desc);
}

/*********************************************************************
 *  Ioctl entry point
 *
 *  ixgb_ioctl is called when the user wants to configure the
 *  interface.
 *
 *  return 0 on success, positive on failure
 **********************************************************************/

int
ixgb_ioctl(struct ifnet *ifp, u_long command, caddr_t data)
{
	struct ixgb_softc *sc = ifp->if_softc;
	struct ifreq	*ifr = (struct ifreq *) data;
	int		s, error = 0;

	s = splnet();

	switch (command) {
	case SIOCSIFADDR:
		IOCTL_DEBUGOUT("ioctl rcv'd: SIOCSIFADDR (Set Interface "
			       "Addr)");
		ifp->if_flags |= IFF_UP;
		if (!(ifp->if_flags & IFF_RUNNING))
			ixgb_init(sc);
		break;

	case SIOCSIFFLAGS:
		IOCTL_DEBUGOUT("ioctl rcv'd: SIOCSIFFLAGS (Set Interface Flags)");
		if (ifp->if_flags & IFF_UP) {
			/*
			 * If only the PROMISC or ALLMULTI flag changes, then
			 * don't do a full re-init of the chip, just update
			 * the Rx filter.
			 */
			if ((ifp->if_flags & IFF_RUNNING) &&
			    ((ifp->if_flags ^ sc->if_flags) &
			     (IFF_ALLMULTI | IFF_PROMISC)) != 0) {
				ixgb_set_promisc(sc);
			} else {
				if (!(ifp->if_flags & IFF_RUNNING))
					ixgb_init(sc);
			}
		} else {
			if (ifp->if_flags & IFF_RUNNING)
				ixgb_stop(sc);
		}
		sc->if_flags = ifp->if_flags;
		break;

	case SIOCSIFMEDIA:
	case SIOCGIFMEDIA:
		IOCTL_DEBUGOUT("ioctl rcv'd: SIOCxIFMEDIA (Get/Set Interface Media)");
		error = ifmedia_ioctl(ifp, ifr, &sc->media, command);
		break;

	default:
		error = ether_ioctl(ifp, &sc->interface_data, command, data);
	}

	if (error == ENETRESET) {
		if (ifp->if_flags & IFF_RUNNING) {
			ixgb_disable_intr(sc);
			ixgb_set_multi(sc);
			ixgb_enable_intr(sc);
		}
		error = 0;
	}

	splx(s);
	return (error);
}

/*********************************************************************
 *  Watchdog entry point
 *
 *  This routine is called whenever hardware quits transmitting.
 *
 **********************************************************************/

void
ixgb_watchdog(struct ifnet * ifp)
{
	struct ixgb_softc *sc = ifp->if_softc;

	/*
	 * If we are in this routine because of pause frames, then don't
	 * reset the hardware.
	 */
	if (IXGB_READ_REG(&sc->hw, STATUS) & IXGB_STATUS_TXOFF) {
		ifp->if_timer = IXGB_TX_TIMEOUT;
		return;
	}

	printf("%s: watchdog timeout -- resetting\n", sc->sc_dv.dv_xname);

	ixgb_init(sc);

	sc->watchdog_events++;
}

/*********************************************************************
 *  Init entry point
 *
 *  This routine is used in two ways. It is used by the stack as
 *  init entry point in network interface structure. It is also used
 *  by the driver as a hw/sw initialization routine to get to a
 *  consistent state.
 *
 **********************************************************************/

void
ixgb_init(void *arg)
{
	struct ixgb_softc *sc = arg;
	struct ifnet   *ifp = &sc->interface_data.ac_if;
	uint32_t temp_reg;
	int s;

	INIT_DEBUGOUT("ixgb_init: begin");

	s = splnet();

	ixgb_stop(sc);

	/* Get the latest mac address, User can use a LAA */
	bcopy(sc->interface_data.ac_enaddr, sc->hw.curr_mac_addr,
	      IXGB_ETH_LENGTH_OF_ADDRESS);

	/* Initialize the hardware */
	if (ixgb_hardware_init(sc)) {
		printf("%s: Unable to initialize the hardware\n",
		       sc->sc_dv.dv_xname);
		splx(s);
		return;
	}

	if (ifp->if_capabilities & IFCAP_VLAN_HWTAGGING)
		ixgb_enable_hw_vlans(sc);

	/* Prepare transmit descriptors and buffers */
	if (ixgb_setup_transmit_structures(sc)) {
		printf("%s: Could not setup transmit structures\n",
		       sc->sc_dv.dv_xname);
		ixgb_stop(sc);
		splx(s);
		return;
	}
	ixgb_initialize_transmit_unit(sc);

	/* Setup Multicast table */
	ixgb_set_multi(sc);

	/* Prepare receive descriptors and buffers */
	if (ixgb_setup_receive_structures(sc)) {
		printf("%s: Could not setup receive structures\n",
		       sc->sc_dv.dv_xname);
		ixgb_stop(sc);
		splx(s);
		return;
	}
	ixgb_initialize_receive_unit(sc);

	/* Don't lose promiscuous settings */
	ixgb_set_promisc(sc);

	ifp->if_flags |= IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);

	/* Enable jumbo frames */
	IXGB_WRITE_REG(&sc->hw, MFRMS,
	    sc->hw.max_frame_size << IXGB_MFRMS_SHIFT);
	temp_reg = IXGB_READ_REG(&sc->hw, CTRL0);
	temp_reg |= IXGB_CTRL0_JFE;
	IXGB_WRITE_REG(&sc->hw, CTRL0, temp_reg);

	timeout_add_sec(&sc->timer_handle, 1);
	ixgb_clear_hw_cntrs(&sc->hw);
	ixgb_enable_intr(sc);

	splx(s);
}

/*********************************************************************
 *
 *  Interrupt Service routine
 *
 **********************************************************************/

int
ixgb_intr(void *arg)
{
	struct ixgb_softc *sc = arg;
	struct ifnet	*ifp;
	u_int32_t	reg_icr;
	boolean_t	rxdmt0 = FALSE;
	int claimed = 0;

	ifp = &sc->interface_data.ac_if;

	for (;;) {
		reg_icr = IXGB_READ_REG(&sc->hw, ICR);
		if (reg_icr == 0)
			break;

		claimed = 1;

		if (reg_icr & IXGB_INT_RXDMT0)
			rxdmt0 = TRUE;

		if (ifp->if_flags & IFF_RUNNING) {
			ixgb_rxeof(sc, -1);
			ixgb_txeof(sc);
		}

		/* Link status change */
		if (reg_icr & (IXGB_INT_RXSEQ | IXGB_INT_LSC)) {
			timeout_del(&sc->timer_handle);
			ixgb_check_for_link(&sc->hw);
			ixgb_update_link_status(sc);
			timeout_add_sec(&sc->timer_handle, 1);
		}

		if (rxdmt0 && sc->raidc) {
			IXGB_WRITE_REG(&sc->hw, IMC, IXGB_INT_RXDMT0);
			IXGB_WRITE_REG(&sc->hw, IMS, IXGB_INT_RXDMT0);
		}
	}

	if (ifp->if_flags & IFF_RUNNING && !IFQ_IS_EMPTY(&ifp->if_snd))
		ixgb_start(ifp);

	return (claimed);
}


/*********************************************************************
 *
 *  Media Ioctl callback
 *
 *  This routine is called whenever the user queries the status of
 *  the interface using ifconfig.
 *
 **********************************************************************/
void
ixgb_media_status(struct ifnet *ifp, struct ifmediareq *ifmr)
{
	struct ixgb_softc *sc = ifp->if_softc;

	INIT_DEBUGOUT("ixgb_media_status: begin");

	ixgb_check_for_link(&sc->hw);
	ixgb_update_link_status(sc);

	ifmr->ifm_status = IFM_AVALID;
	ifmr->ifm_active = IFM_ETHER;

	if (!sc->hw.link_up) {
		ifmr->ifm_active |= IFM_NONE;
		return;
	}

	ifmr->ifm_status |= IFM_ACTIVE;
	if ((sc->hw.phy_type == ixgb_phy_type_g6104) ||
	    (sc->hw.phy_type == ixgb_phy_type_txn17401))
		ifmr->ifm_active |= IFM_10G_LR | IFM_FDX;
	else
		ifmr->ifm_active |= IFM_10G_SR | IFM_FDX;

	return;
}

/*********************************************************************
 *
 *  Media Ioctl callback
 *
 *  This routine is called when the user changes speed/duplex using
 *  media/mediopt option with ifconfig.
 *
 **********************************************************************/
int
ixgb_media_change(struct ifnet * ifp)
{
	struct ixgb_softc *sc = ifp->if_softc;
	struct ifmedia *ifm = &sc->media;

	INIT_DEBUGOUT("ixgb_media_change: begin");

	if (IFM_TYPE(ifm->ifm_media) != IFM_ETHER)
		return (EINVAL);

	return (0);
}

/*********************************************************************
 *
 *  This routine maps the mbufs to tx descriptors.
 *
 *  return 0 on success, positive on failure
 **********************************************************************/

int
ixgb_encap(struct ixgb_softc *sc, struct mbuf *m_head)
{
	u_int8_t        txd_popts;
	int             i, j, error = 0;
	bus_dmamap_t	map;

	struct ixgb_buffer *tx_buffer;
	struct ixgb_tx_desc *current_tx_desc = NULL;

	/*
	 * Force a cleanup if number of TX descriptors available hits the
	 * threshold
	 */
	if (sc->num_tx_desc_avail <= IXGB_TX_CLEANUP_THRESHOLD) {
		ixgb_txeof(sc);
		/* Now do we at least have a minimal? */
		if (sc->num_tx_desc_avail <= IXGB_TX_CLEANUP_THRESHOLD) {
			sc->no_tx_desc_avail1++;
			return (ENOBUFS);
		}
	}

	/*
	 * Map the packet for DMA.
	 */
	tx_buffer = &sc->tx_buffer_area[sc->next_avail_tx_desc];
	map = tx_buffer->map;

	error = bus_dmamap_load_mbuf(sc->txtag, map,
				     m_head, BUS_DMA_NOWAIT);
	if (error != 0) {
		sc->no_tx_dma_setup++;
		return (error);
	}
	IXGB_KASSERT(map->dm_nsegs != 0, ("ixgb_encap: empty packet"));

	if (map->dm_nsegs > sc->num_tx_desc_avail)
		goto fail;

#ifdef IXGB_CSUM_OFFLOAD
	ixgb_transmit_checksum_setup(sc, m_head, &txd_popts);
#else
	txd_popts = 0;
#endif

	i = sc->next_avail_tx_desc;
	for (j = 0; j < map->dm_nsegs; j++) {
		tx_buffer = &sc->tx_buffer_area[i];
		current_tx_desc = &sc->tx_desc_base[i];

		current_tx_desc->buff_addr = htole64(map->dm_segs[j].ds_addr);
		current_tx_desc->cmd_type_len = htole32((sc->txd_cmd | map->dm_segs[j].ds_len));
		current_tx_desc->popts = txd_popts;
		if (++i == sc->num_tx_desc)
			i = 0;

		tx_buffer->m_head = NULL;
	}

	sc->num_tx_desc_avail -= map->dm_nsegs;
	sc->next_avail_tx_desc = i;

	/* Find out if we are in VLAN mode */
	if (m_head->m_flags & M_VLANTAG) {
		/* Set the VLAN id */
		current_tx_desc->vlan = htole16(m_head->m_pkthdr.ether_vtag);

		/* Tell hardware to add tag */
		current_tx_desc->cmd_type_len |= htole32(IXGB_TX_DESC_CMD_VLE);
	}

	tx_buffer->m_head = m_head;
	bus_dmamap_sync(sc->txtag, map, 0, map->dm_mapsize,
	    BUS_DMASYNC_PREWRITE);

	/*
	 * Last Descriptor of Packet needs End Of Packet (EOP)
	 */
	current_tx_desc->cmd_type_len |= htole32(IXGB_TX_DESC_CMD_EOP);

	return (0);

fail:
	sc->no_tx_desc_avail2++;
	bus_dmamap_unload(sc->txtag, map);
	return (ENOBUFS);
}

void
ixgb_set_promisc(struct ixgb_softc *sc)
{

	u_int32_t       reg_rctl;
	struct ifnet   *ifp = &sc->interface_data.ac_if;

	reg_rctl = IXGB_READ_REG(&sc->hw, RCTL);

	if (ifp->if_flags & IFF_PROMISC) {
		reg_rctl |= (IXGB_RCTL_UPE | IXGB_RCTL_MPE);
	} else if (ifp->if_flags & IFF_ALLMULTI) {
		reg_rctl |= IXGB_RCTL_MPE;
		reg_rctl &= ~IXGB_RCTL_UPE;
	} else {
		reg_rctl &= ~(IXGB_RCTL_UPE | IXGB_RCTL_MPE);
	}
	IXGB_WRITE_REG(&sc->hw, RCTL, reg_rctl);
}

/*********************************************************************
 *  Multicast Update
 *
 *  This routine is called whenever multicast address list is updated.
 *
 **********************************************************************/

void
ixgb_set_multi(struct ixgb_softc *sc)
{
	u_int32_t       reg_rctl = 0;
	u_int8_t        mta[MAX_NUM_MULTICAST_ADDRESSES * IXGB_ETH_LENGTH_OF_ADDRESS];
	int             mcnt = 0;
	struct ifnet   *ifp = &sc->interface_data.ac_if;
	struct arpcom *ac = &sc->interface_data;
	struct ether_multi *enm;
	struct ether_multistep step;

	IOCTL_DEBUGOUT("ixgb_set_multi: begin");

	if (ac->ac_multirangecnt > 0) {
		ifp->if_flags |= IFF_ALLMULTI;
		mcnt = MAX_NUM_MULTICAST_ADDRESSES;
		goto setit;
	}

	ETHER_FIRST_MULTI(step, ac, enm);
	while (enm != NULL) {
		if (mcnt == MAX_NUM_MULTICAST_ADDRESSES)
			break;
		bcopy(enm->enm_addrlo, &mta[mcnt*IXGB_ETH_LENGTH_OF_ADDRESS],
		      IXGB_ETH_LENGTH_OF_ADDRESS);
		mcnt++;
		ETHER_NEXT_MULTI(step, enm);
	}

setit:
	if (mcnt >= MAX_NUM_MULTICAST_ADDRESSES) {
		reg_rctl = IXGB_READ_REG(&sc->hw, RCTL);
		reg_rctl |= IXGB_RCTL_MPE;
		IXGB_WRITE_REG(&sc->hw, RCTL, reg_rctl);
	} else
		ixgb_mc_addr_list_update(&sc->hw, mta, mcnt, 0);
}


/*********************************************************************
 *  Timer routine
 *
 *  This routine checks for link status and updates statistics.
 *
 **********************************************************************/

void
ixgb_local_timer(void *arg)
{
	struct ifnet   *ifp;
	struct ixgb_softc *sc = arg;
	int s;

	ifp = &sc->interface_data.ac_if;

	s = splnet();

	ixgb_check_for_link(&sc->hw);
	ixgb_update_link_status(sc);
	ixgb_update_stats_counters(sc);
#ifdef IXGB_DEBUG
	if (ixgb_display_debug_stats && ifp->if_flags & IFF_RUNNING)
		ixgb_print_hw_stats(sc);
#endif

	timeout_add_sec(&sc->timer_handle, 1);

	splx(s);
}

void
ixgb_update_link_status(struct ixgb_softc *sc)
{
	struct ifnet *ifp = &sc->interface_data.ac_if;

	if (sc->hw.link_up) {
		if (!sc->link_active) {
			ifp->if_baudrate = IF_Gbps(10);
			sc->link_active = 1;
			ifp->if_link_state = LINK_STATE_FULL_DUPLEX;
			if_link_state_change(ifp);
		}
	} else {
		if (sc->link_active) {
			ifp->if_baudrate = 0;
			sc->link_active = 0;
			ifp->if_link_state = LINK_STATE_DOWN;
			if_link_state_change(ifp);
		}
	}
}

/*********************************************************************
 *
 *  This routine disables all traffic on the adapter by issuing a
 *  global reset on the MAC and deallocates TX/RX buffers.
 *
 **********************************************************************/

void
ixgb_stop(void *arg)
{
	struct ifnet   *ifp;
	struct ixgb_softc *sc = arg;
	ifp = &sc->interface_data.ac_if;

	INIT_DEBUGOUT("ixgb_stop: begin\n");
	ixgb_disable_intr(sc);
	sc->hw.adapter_stopped = FALSE;
	ixgb_adapter_stop(&sc->hw);
	timeout_del(&sc->timer_handle);

	/* Tell the stack that the interface is no longer active */
	ifp->if_flags &= ~IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);

	ixgb_free_transmit_structures(sc);
	ixgb_free_receive_structures(sc);
}


/*********************************************************************
 *
 *  Determine hardware revision.
 *
 **********************************************************************/
void
ixgb_identify_hardware(struct ixgb_softc *sc)
{
	u_int32_t	reg;
	struct pci_attach_args *pa = &sc->osdep.ixgb_pa;

	/* Make sure our PCI config space has the necessary stuff set */
	sc->hw.pci_cmd_word = pci_conf_read(pa->pa_pc, pa->pa_tag,
					    PCI_COMMAND_STATUS_REG);

	/* Save off the information about this board */
	sc->hw.vendor_id = PCI_VENDOR(pa->pa_id);
	sc->hw.device_id = PCI_PRODUCT(pa->pa_id);

	reg = pci_conf_read(pa->pa_pc, pa->pa_tag, PCI_CLASS_REG);
	sc->hw.revision_id = PCI_REVISION(reg);

	reg = pci_conf_read(pa->pa_pc, pa->pa_tag, PCI_SUBSYS_ID_REG);
	sc->hw.subsystem_vendor_id = PCI_VENDOR(reg);
	sc->hw.subsystem_id = PCI_PRODUCT(reg);

	/* Set MacType, etc. based on this PCI info */
	switch (sc->hw.device_id) {
	case IXGB_DEVICE_ID_82597EX:
	case IXGB_DEVICE_ID_82597EX_SR:
	case IXGB_DEVICE_ID_82597EX_LR:
	case IXGB_DEVICE_ID_82597EX_CX4:
		sc->hw.mac_type = ixgb_82597;
		break;
	default:
		INIT_DEBUGOUT1("Unknown device if 0x%x", sc->hw.device_id);
		printf("%s: unsupported device id 0x%x\n",
		    sc->sc_dv.dv_xname, sc->hw.device_id);
	}
}

int
ixgb_allocate_pci_resources(struct ixgb_softc *sc)
	
{
	int val;
	pci_intr_handle_t	ih;
	const char		*intrstr = NULL;
	struct pci_attach_args *pa =  &sc->osdep.ixgb_pa;
	pci_chipset_tag_t	pc = pa->pa_pc;

	val = pci_conf_read(pa->pa_pc, pa->pa_tag, IXGB_MMBA);
	if (PCI_MAPREG_TYPE(val) != PCI_MAPREG_TYPE_MEM) {
		printf(": mmba is not mem space\n");
		return (ENXIO);
	}
	if (pci_mapreg_map(pa, IXGB_MMBA, PCI_MAPREG_MEM_TYPE(val), 0,
	    &sc->osdep.mem_bus_space_tag, &sc->osdep.mem_bus_space_handle,
	    &sc->osdep.ixgb_membase, &sc->osdep.ixgb_memsize, 0)) {
		printf(": cannot find mem space\n");
		return (ENXIO);
	}

	if (pci_intr_map(pa, &ih)) {
		printf(": couldn't map interrupt\n");
		return (ENXIO);
	}

	sc->hw.back = &sc->osdep;

	intrstr = pci_intr_string(pc, ih);
	sc->sc_intrhand = pci_intr_establish(pc, ih, IPL_NET, ixgb_intr, sc,
					    sc->sc_dv.dv_xname);
	if (sc->sc_intrhand == NULL) {
		printf(": couldn't establish interrupt");
		if (intrstr != NULL)
			printf(" at %s", intrstr);
		printf("\n");
		return (ENXIO);
	}
	printf(": %s", intrstr);

	return (0);
}

void
ixgb_free_pci_resources(struct ixgb_softc *sc)
{
	struct pci_attach_args *pa = &sc->osdep.ixgb_pa;
	pci_chipset_tag_t	pc = pa->pa_pc;

	if (sc->sc_intrhand)
		pci_intr_disestablish(pc, sc->sc_intrhand);
	sc->sc_intrhand = 0;

	if (sc->osdep.ixgb_membase)
		bus_space_unmap(sc->osdep.mem_bus_space_tag, sc->osdep.mem_bus_space_handle,
				sc->osdep.ixgb_memsize);
	sc->osdep.ixgb_membase = 0;
}

/*********************************************************************
 *
 *  Initialize the hardware to a configuration as specified by the
 *  adapter structure. The controller is reset, the EEPROM is
 *  verified, the MAC address is set, then the shared initialization
 *  routines are called.
 *
 **********************************************************************/
int
ixgb_hardware_init(struct ixgb_softc *sc)
{
	/* Issue a global reset */
	sc->hw.adapter_stopped = FALSE;
	ixgb_adapter_stop(&sc->hw);

	/* Make sure we have a good EEPROM before we read from it */
	if (!ixgb_validate_eeprom_checksum(&sc->hw)) {
		printf("%s: The EEPROM Checksum Is Not Valid\n",
		       sc->sc_dv.dv_xname);
		return (EIO);
	}
	if (!ixgb_init_hw(&sc->hw)) {
		printf("%s: Hardware Initialization Failed",
		       sc->sc_dv.dv_xname);
		return (EIO);
	}
	bcopy(sc->hw.curr_mac_addr, sc->interface_data.ac_enaddr,
	      IXGB_ETH_LENGTH_OF_ADDRESS);

	return (0);
}

/*********************************************************************
 *
 *  Setup networking device structure and register an interface.
 *
 **********************************************************************/
void
ixgb_setup_interface(struct ixgb_softc *sc)
{
	struct ifnet   *ifp;
	INIT_DEBUGOUT("ixgb_setup_interface: begin");

	ifp = &sc->interface_data.ac_if;
	strlcpy(ifp->if_xname, sc->sc_dv.dv_xname, IFNAMSIZ);

	ifp->if_softc = sc;
	ifp->if_flags = IFF_BROADCAST | IFF_SIMPLEX | IFF_MULTICAST;
	ifp->if_ioctl = ixgb_ioctl;
	ifp->if_start = ixgb_start;
	ifp->if_watchdog = ixgb_watchdog;
	ifp->if_hardmtu =
		IXGB_MAX_JUMBO_FRAME_SIZE - ETHER_HDR_LEN - ETHER_CRC_LEN;
	IFQ_SET_MAXLEN(&ifp->if_snd, sc->num_tx_desc - 1);

	ifp->if_capabilities = IFCAP_VLAN_MTU;

#if NVLAN > 0
	ifp->if_capabilities |= IFCAP_VLAN_HWTAGGING;
#endif

#ifdef IXGB_CSUM_OFFLOAD
	ifp->if_capabilities |= IFCAP_CSUM_TCPv4|IFCAP_CSUM_UDPv4;
#endif

	/*
	 * Specify the media types supported by this adapter and register
	 * callbacks to update media and link information
	 */
	ifmedia_init(&sc->media, IFM_IMASK, ixgb_media_change,
		     ixgb_media_status);
	if ((sc->hw.phy_type == ixgb_phy_type_g6104) ||
	    (sc->hw.phy_type == ixgb_phy_type_txn17401)) {
		ifmedia_add(&sc->media, IFM_ETHER | IFM_10G_LR |
		    IFM_FDX, 0, NULL);
	} else {
		ifmedia_add(&sc->media, IFM_ETHER | IFM_10G_SR |
		    IFM_FDX, 0, NULL);
	}
	ifmedia_add(&sc->media, IFM_ETHER | IFM_AUTO, 0, NULL);
	ifmedia_set(&sc->media, IFM_ETHER | IFM_AUTO);

	if_attach(ifp);
	ether_ifattach(ifp);
}

/********************************************************************
 * Manage DMA'able memory.
 *******************************************************************/
int
ixgb_dma_malloc(struct ixgb_softc *sc, bus_size_t size,
		struct ixgb_dma_alloc * dma, int mapflags)
{
	int r;

	dma->dma_tag = sc->osdep.ixgb_pa.pa_dmat;
	r = bus_dmamap_create(dma->dma_tag, size, 1,
	    size, 0, BUS_DMA_NOWAIT, &dma->dma_map);
	if (r != 0) {
		printf("%s: ixgb_dma_malloc: bus_dmamap_create failed; "
			"error %u\n", sc->sc_dv.dv_xname, r);
		goto fail_0;
	}

	r = bus_dmamem_alloc(dma->dma_tag, size, PAGE_SIZE, 0, &dma->dma_seg,
	    1, &dma->dma_nseg, BUS_DMA_NOWAIT);
	if (r != 0) {
		printf("%s: ixgb_dma_malloc: bus_dmammem_alloc failed; "
			"size %lu, error %d\n", sc->sc_dv.dv_xname,
			(unsigned long)size, r);
		goto fail_1;
	}

	r = bus_dmamem_map(dma->dma_tag, &dma->dma_seg, dma->dma_nseg, size,
	    &dma->dma_vaddr, BUS_DMA_NOWAIT);
	if (r != 0) {
		printf("%s: ixgb_dma_malloc: bus_dmammem_map failed; "
			"size %lu, error %d\n", sc->sc_dv.dv_xname,
			(unsigned long)size, r);
		goto fail_2;
	}

	r = bus_dmamap_load(sc->osdep.ixgb_pa.pa_dmat, dma->dma_map,
			    dma->dma_vaddr, size, NULL,
			    mapflags | BUS_DMA_NOWAIT);
	if (r != 0) {
		printf("%s: ixgb_dma_malloc: bus_dmamap_load failed; "
			"error %u\n", sc->sc_dv.dv_xname, r);
		goto fail_3;
	}

	dma->dma_size = size;
	return (0);

fail_3: 
	bus_dmamem_unmap(dma->dma_tag, dma->dma_vaddr, size);
fail_2: 
	bus_dmamem_free(dma->dma_tag, &dma->dma_seg, dma->dma_nseg);
fail_1: 
	bus_dmamap_destroy(dma->dma_tag, dma->dma_map);
fail_0: 
	dma->dma_map = NULL;
	dma->dma_tag = NULL;

	return (r);
}

void
ixgb_dma_free(struct ixgb_softc *sc, struct ixgb_dma_alloc *dma)
{
	if (dma->dma_tag == NULL)
		return;

	if (dma->dma_map != NULL) {
		bus_dmamap_sync(dma->dma_tag, dma->dma_map, 0,
		    dma->dma_map->dm_mapsize,
		    BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
		bus_dmamap_unload(dma->dma_tag, dma->dma_map);
		bus_dmamem_unmap(dma->dma_tag, dma->dma_vaddr, dma->dma_size);
		bus_dmamem_free(dma->dma_tag, &dma->dma_seg, dma->dma_nseg);
		bus_dmamap_destroy(dma->dma_tag, dma->dma_map);
	}
}

/*********************************************************************
 *
 *  Allocate memory for tx_buffer structures. The tx_buffer stores all
 *  the information needed to transmit a packet on the wire.
 *
 **********************************************************************/
int
ixgb_allocate_transmit_structures(struct ixgb_softc *sc)
{
	if (!(sc->tx_buffer_area = mallocarray(sc->num_tx_desc,
	    sizeof(struct ixgb_buffer), M_DEVBUF, M_NOWAIT | M_ZERO))) {
		printf("%s: Unable to allocate tx_buffer memory\n",
		       sc->sc_dv.dv_xname);
		return (ENOMEM);
	}

	return (0);
}

/*********************************************************************
 *
 *  Allocate and initialize transmit structures.
 *
 **********************************************************************/
int
ixgb_setup_transmit_structures(struct ixgb_softc *sc)
{
	struct	ixgb_buffer *tx_buffer;
	int error, i;

	if ((error = ixgb_allocate_transmit_structures(sc)) != 0)
		goto fail;

	bzero((void *)sc->tx_desc_base,
	      (sizeof(struct ixgb_tx_desc)) * sc->num_tx_desc);

	sc->txtag = sc->osdep.ixgb_pa.pa_dmat;

	tx_buffer = sc->tx_buffer_area;
	for (i = 0; i < sc->num_tx_desc; i++) {
		error = bus_dmamap_create(sc->txtag, IXGB_MAX_JUMBO_FRAME_SIZE,
			    IXGB_MAX_SCATTER, IXGB_MAX_JUMBO_FRAME_SIZE, 0,
			    BUS_DMA_NOWAIT, &tx_buffer->map);
		if (error != 0) {
			printf("%s: Unable to create TX DMA map\n",
			    sc->sc_dv.dv_xname);
			goto fail;
		}
		tx_buffer++;
	}

	sc->next_avail_tx_desc = 0;
	sc->oldest_used_tx_desc = 0;

	/* Set number of descriptors available */
	sc->num_tx_desc_avail = sc->num_tx_desc;

	/* Set checksum context */
	sc->active_checksum_context = OFFLOAD_NONE;
	bus_dmamap_sync(sc->txdma.dma_tag, sc->txdma.dma_map, 0,
	   sc->txdma.dma_size, BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

	return (0);

fail:
	ixgb_free_transmit_structures(sc);
	return (error);
}

/*********************************************************************
 *
 *  Enable transmit unit.
 *
 **********************************************************************/
void
ixgb_initialize_transmit_unit(struct ixgb_softc *sc)
{
	u_int32_t       reg_tctl;
	u_int64_t       bus_addr;

	/* Setup the Base and Length of the Tx Descriptor Ring */
	bus_addr = sc->txdma.dma_map->dm_segs[0].ds_addr;
	IXGB_WRITE_REG(&sc->hw, TDBAL, (u_int32_t)bus_addr);
	IXGB_WRITE_REG(&sc->hw, TDBAH, (u_int32_t)(bus_addr >> 32));
	IXGB_WRITE_REG(&sc->hw, TDLEN,
		       sc->num_tx_desc *
		       sizeof(struct ixgb_tx_desc));

	/* Setup the HW Tx Head and Tail descriptor pointers */
	IXGB_WRITE_REG(&sc->hw, TDH, 0);
	IXGB_WRITE_REG(&sc->hw, TDT, 0);

	HW_DEBUGOUT2("Base = %x, Length = %x\n",
		     IXGB_READ_REG(&sc->hw, TDBAL),
		     IXGB_READ_REG(&sc->hw, TDLEN));

	IXGB_WRITE_REG(&sc->hw, TIDV, sc->tx_int_delay);

	/* Program the Transmit Control Register */
	reg_tctl = IXGB_READ_REG(&sc->hw, TCTL);
	reg_tctl = IXGB_TCTL_TCE | IXGB_TCTL_TXEN | IXGB_TCTL_TPDE;
	IXGB_WRITE_REG(&sc->hw, TCTL, reg_tctl);

	/* Setup Transmit Descriptor Settings for this adapter */
	sc->txd_cmd = IXGB_TX_DESC_TYPE | IXGB_TX_DESC_CMD_RS;

	if (sc->tx_int_delay > 0)
		sc->txd_cmd |= IXGB_TX_DESC_CMD_IDE;
}

/*********************************************************************
 *
 *  Free all transmit related data structures.
 *
 **********************************************************************/
void
ixgb_free_transmit_structures(struct ixgb_softc *sc)
{
	struct ixgb_buffer *tx_buffer;
	int             i;

	INIT_DEBUGOUT("free_transmit_structures: begin");

	if (sc->tx_buffer_area != NULL) {
		tx_buffer = sc->tx_buffer_area;
		for (i = 0; i < sc->num_tx_desc; i++, tx_buffer++) {
			if (tx_buffer->map != NULL &&
			    tx_buffer->map->dm_nsegs > 0) {
				bus_dmamap_sync(sc->txtag, tx_buffer->map,
				    0, tx_buffer->map->dm_mapsize,
				    BUS_DMASYNC_POSTWRITE);
				bus_dmamap_unload(sc->txtag,
				    tx_buffer->map);
			}

			if (tx_buffer->m_head != NULL) {
				m_freem(tx_buffer->m_head);
				tx_buffer->m_head = NULL;
			}
			if (tx_buffer->map != NULL) {
				bus_dmamap_destroy(sc->txtag,
				    tx_buffer->map);
				tx_buffer->map = NULL;
			}
		}
	}
	if (sc->tx_buffer_area != NULL) {
		free(sc->tx_buffer_area, M_DEVBUF, 0);
		sc->tx_buffer_area = NULL;
	}
	if (sc->txtag != NULL) {
		sc->txtag = NULL;
	}
}

/*********************************************************************
 *
 *  The offload context needs to be set when we transfer the first
 *  packet of a particular protocol (TCP/UDP). We change the
 *  context only if the protocol type changes.
 *
 **********************************************************************/
void
ixgb_transmit_checksum_setup(struct ixgb_softc *sc,
			     struct mbuf *mp,
			     u_int8_t *txd_popts)
{
	struct ixgb_context_desc *TXD;
	struct ixgb_buffer *tx_buffer;
	int             curr_txd;

	if (mp->m_pkthdr.csum_flags) {

		if (mp->m_pkthdr.csum_flags & M_TCP_CSUM_OUT) {
			*txd_popts = IXGB_TX_DESC_POPTS_TXSM;
			if (sc->active_checksum_context == OFFLOAD_TCP_IP)
				return;
			else
				sc->active_checksum_context = OFFLOAD_TCP_IP;

		} else if (mp->m_pkthdr.csum_flags & M_UDP_CSUM_OUT) {
			*txd_popts = IXGB_TX_DESC_POPTS_TXSM;
			if (sc->active_checksum_context == OFFLOAD_UDP_IP)
				return;
			else
				sc->active_checksum_context = OFFLOAD_UDP_IP;
		} else {
			*txd_popts = 0;
			return;
		}
	} else {
		*txd_popts = 0;
		return;
	}

	/*
	 * If we reach this point, the checksum offload context needs to be
	 * reset.
	 */
	curr_txd = sc->next_avail_tx_desc;
	tx_buffer = &sc->tx_buffer_area[curr_txd];
	TXD = (struct ixgb_context_desc *) & sc->tx_desc_base[curr_txd];

	TXD->tucss = ENET_HEADER_SIZE + sizeof(struct ip);
	TXD->tucse = 0;

	TXD->mss = 0;

	if (sc->active_checksum_context == OFFLOAD_TCP_IP) {
		TXD->tucso =
			ENET_HEADER_SIZE + sizeof(struct ip) +
			offsetof(struct tcphdr, th_sum);
	} else if (sc->active_checksum_context == OFFLOAD_UDP_IP) {
		TXD->tucso =
			ENET_HEADER_SIZE + sizeof(struct ip) +
			offsetof(struct udphdr, uh_sum);
	}
	TXD->cmd_type_len = htole32(IXGB_CONTEXT_DESC_CMD_TCP |
	    IXGB_TX_DESC_CMD_RS | IXGB_CONTEXT_DESC_CMD_IDE);

	tx_buffer->m_head = NULL;

	if (++curr_txd == sc->num_tx_desc)
		curr_txd = 0;

	sc->num_tx_desc_avail--;
	sc->next_avail_tx_desc = curr_txd;
}

/**********************************************************************
 *
 *  Examine each tx_buffer in the used queue. If the hardware is done
 *  processing the packet then free associated resources. The
 *  tx_buffer is put back on the free queue.
 *
 **********************************************************************/
void
ixgb_txeof(struct ixgb_softc *sc)
{
	int             i, num_avail;
	struct ixgb_buffer *tx_buffer;
	struct ixgb_tx_desc *tx_desc;
	struct ifnet	*ifp = &sc->interface_data.ac_if;

	if (sc->num_tx_desc_avail == sc->num_tx_desc)
		return;

	num_avail = sc->num_tx_desc_avail;
	i = sc->oldest_used_tx_desc;

	tx_buffer = &sc->tx_buffer_area[i];
	tx_desc = &sc->tx_desc_base[i];

	bus_dmamap_sync(sc->txdma.dma_tag, sc->txdma.dma_map, 0,
	    sc->txdma.dma_map->dm_mapsize, BUS_DMASYNC_POSTREAD);
	while (tx_desc->status & IXGB_TX_DESC_STATUS_DD) {

		tx_desc->status = 0;
		num_avail++;

		if (tx_buffer->m_head != NULL) {
			if (tx_buffer->map->dm_nsegs > 0) {
				bus_dmamap_sync(sc->txtag, tx_buffer->map,
				    0, tx_buffer->map->dm_mapsize,
				    BUS_DMASYNC_POSTWRITE);
				bus_dmamap_unload(sc->txtag, tx_buffer->map);
			}

			m_freem(tx_buffer->m_head);
			tx_buffer->m_head = NULL;
		}
		if (++i == sc->num_tx_desc)
			i = 0;

		tx_buffer = &sc->tx_buffer_area[i];
		tx_desc = &sc->tx_desc_base[i];
	}
	bus_dmamap_sync(sc->txdma.dma_tag, sc->txdma.dma_map, 0,
	    sc->txdma.dma_map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

	sc->oldest_used_tx_desc = i;

	/*
	 * If we have enough room, clear IFF_OACTIVE to tell the stack that
	 * it is OK to send packets. If there are no pending descriptors,
	 * clear the timeout. Otherwise, if some descriptors have been freed,
	 * restart the timeout.
	 */
	if (num_avail > IXGB_TX_CLEANUP_THRESHOLD)
		ifq_clr_oactive(&ifp->if_snd);

	/* All clean, turn off the timer */
	if (num_avail == sc->num_tx_desc)
		ifp->if_timer = 0;
	/* Some cleaned, reset the timer */
	else if (num_avail != sc->num_tx_desc_avail)
		ifp->if_timer = IXGB_TX_TIMEOUT;

	sc->num_tx_desc_avail = num_avail;
}


/*********************************************************************
 *
 *  Get a buffer from system mbuf buffer pool.
 *
 **********************************************************************/
int
ixgb_get_buf(struct ixgb_softc *sc, int i,
	     struct mbuf *nmp)
{
	struct mbuf *mp = nmp;
	struct ixgb_buffer *rx_buffer;
	int             error;

	if (mp == NULL) {
		MGETHDR(mp, M_DONTWAIT, MT_DATA);
		if (mp == NULL) {
			sc->mbuf_alloc_failed++;
			return (ENOBUFS);
		}
		MCLGET(mp, M_DONTWAIT);
		if ((mp->m_flags & M_EXT) == 0) {
			m_freem(mp);
			sc->mbuf_cluster_failed++;
			return (ENOBUFS);
		}
		mp->m_len = mp->m_pkthdr.len = MCLBYTES;
	} else {
		mp->m_len = mp->m_pkthdr.len = MCLBYTES;
		mp->m_data = mp->m_ext.ext_buf;
		mp->m_next = NULL;
	}

	if (sc->hw.max_frame_size <= (MCLBYTES - ETHER_ALIGN))
		m_adj(mp, ETHER_ALIGN);

	rx_buffer = &sc->rx_buffer_area[i];

	/*
	 * Using memory from the mbuf cluster pool, invoke the bus_dma
	 * machinery to arrange the memory mapping.
	 */
	error = bus_dmamap_load_mbuf(sc->rxtag, rx_buffer->map,
	    mp, BUS_DMA_NOWAIT);
	if (error) {
		m_freem(mp);
		return (error);
	}
	rx_buffer->m_head = mp;
	bzero(&sc->rx_desc_base[i], sizeof(sc->rx_desc_base[i]));
	sc->rx_desc_base[i].buff_addr = htole64(rx_buffer->map->dm_segs[0].ds_addr);
	bus_dmamap_sync(sc->rxtag, rx_buffer->map, 0,
	    rx_buffer->map->dm_mapsize, BUS_DMASYNC_PREREAD);

	return (0);
}

/*********************************************************************
 *
 *  Allocate memory for rx_buffer structures. Since we use one
 *  rx_buffer per received packet, the maximum number of rx_buffer's
 *  that we'll need is equal to the number of receive descriptors
 *  that we've allocated.
 *
 **********************************************************************/
int
ixgb_allocate_receive_structures(struct ixgb_softc *sc)
{
	int             i, error;
	struct ixgb_buffer *rx_buffer;

	if (!(sc->rx_buffer_area = mallocarray(sc->num_rx_desc,
	    sizeof(struct ixgb_buffer), M_DEVBUF, M_NOWAIT | M_ZERO))) {
		printf("%s: Unable to allocate rx_buffer memory\n",
		       sc->sc_dv.dv_xname);
		return (ENOMEM);
	}

	sc->rxtag = sc->osdep.ixgb_pa.pa_dmat;

	rx_buffer = sc->rx_buffer_area;
	for (i = 0; i < sc->num_rx_desc; i++, rx_buffer++) {
		error = bus_dmamap_create(sc->rxtag, MCLBYTES, 1,
					  MCLBYTES, 0, BUS_DMA_NOWAIT,
					  &rx_buffer->map);
		if (error != 0) {
			printf("%s: ixgb_allocate_receive_structures: "
			       "bus_dmamap_create failed; error %u\n",
			       sc->sc_dv.dv_xname, error);
			goto fail;
		}
	}

	for (i = 0; i < sc->num_rx_desc; i++) {
		error = ixgb_get_buf(sc, i, NULL);
		if (error != 0)
			goto fail;
	}
	bus_dmamap_sync(sc->rxdma.dma_tag, sc->rxdma.dma_map, 0,
	    sc->rxdma.dma_map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

	return (0);

fail:
	ixgb_free_receive_structures(sc);
	return (error);
}

/*********************************************************************
 *
 *  Allocate and initialize receive structures.
 *
 **********************************************************************/
int
ixgb_setup_receive_structures(struct ixgb_softc *sc)
{
	bzero((void *)sc->rx_desc_base,
	      (sizeof(struct ixgb_rx_desc)) * sc->num_rx_desc);

	if (ixgb_allocate_receive_structures(sc))
		return (ENOMEM);

	/* Setup our descriptor pointers */
	sc->next_rx_desc_to_check = 0;
	sc->next_rx_desc_to_use = 0;
	return (0);
}

/*********************************************************************
 *
 *  Enable receive unit.
 *
 **********************************************************************/
void
ixgb_initialize_receive_unit(struct ixgb_softc *sc)
{
	u_int32_t       reg_rctl;
	u_int32_t       reg_rxcsum;
	u_int32_t       reg_rxdctl;
	u_int64_t       bus_addr;

	/*
	 * Make sure receives are disabled while setting up the descriptor
	 * ring
	 */
	reg_rctl = IXGB_READ_REG(&sc->hw, RCTL);
	IXGB_WRITE_REG(&sc->hw, RCTL, reg_rctl & ~IXGB_RCTL_RXEN);

	/* Set the Receive Delay Timer Register */
	IXGB_WRITE_REG(&sc->hw, RDTR,
		       sc->rx_int_delay);

	/* Setup the Base and Length of the Rx Descriptor Ring */
	bus_addr = sc->rxdma.dma_map->dm_segs[0].ds_addr;
	IXGB_WRITE_REG(&sc->hw, RDBAL, (u_int32_t)bus_addr);
	IXGB_WRITE_REG(&sc->hw, RDBAH, (u_int32_t)(bus_addr >> 32));
	IXGB_WRITE_REG(&sc->hw, RDLEN, sc->num_rx_desc *
		       sizeof(struct ixgb_rx_desc));

	/* Setup the HW Rx Head and Tail Descriptor Pointers */
	IXGB_WRITE_REG(&sc->hw, RDH, 0);

	IXGB_WRITE_REG(&sc->hw, RDT, sc->num_rx_desc - 1);

	reg_rxdctl = RXDCTL_WTHRESH_DEFAULT << IXGB_RXDCTL_WTHRESH_SHIFT
		| RXDCTL_HTHRESH_DEFAULT << IXGB_RXDCTL_HTHRESH_SHIFT
		| RXDCTL_PTHRESH_DEFAULT << IXGB_RXDCTL_PTHRESH_SHIFT;
	IXGB_WRITE_REG(&sc->hw, RXDCTL, reg_rxdctl);

	sc->raidc = 1;
	if (sc->raidc) {
		uint32_t        raidc;
		uint8_t         poll_threshold;
#define IXGB_RAIDC_POLL_DEFAULT 120

		poll_threshold = ((sc->num_rx_desc - 1) >> 3);
		poll_threshold >>= 1;
		poll_threshold &= 0x3F;
		raidc = IXGB_RAIDC_EN | IXGB_RAIDC_RXT_GATE |
			(IXGB_RAIDC_POLL_DEFAULT << IXGB_RAIDC_POLL_SHIFT) |
			(sc->rx_int_delay << IXGB_RAIDC_DELAY_SHIFT) |
			poll_threshold;
		IXGB_WRITE_REG(&sc->hw, RAIDC, raidc);
	}

	/* Enable Receive Checksum Offload for TCP and UDP ? */
	reg_rxcsum = IXGB_READ_REG(&sc->hw, RXCSUM);
	reg_rxcsum |= IXGB_RXCSUM_TUOFL;
	IXGB_WRITE_REG(&sc->hw, RXCSUM, reg_rxcsum);

	/* Setup the Receive Control Register */
	reg_rctl = IXGB_READ_REG(&sc->hw, RCTL);
	reg_rctl &= ~(3 << IXGB_RCTL_MO_SHIFT);
	reg_rctl |= IXGB_RCTL_BAM | IXGB_RCTL_RDMTS_1_2 | IXGB_RCTL_SECRC |
		IXGB_RCTL_CFF |
		(sc->hw.mc_filter_type << IXGB_RCTL_MO_SHIFT);

	switch (sc->rx_buffer_len) {
	default:
	case IXGB_RXBUFFER_2048:
		reg_rctl |= IXGB_RCTL_BSIZE_2048;
		break;
	case IXGB_RXBUFFER_4096:
		reg_rctl |= IXGB_RCTL_BSIZE_4096;
		break;
	case IXGB_RXBUFFER_8192:
		reg_rctl |= IXGB_RCTL_BSIZE_8192;
		break;
	case IXGB_RXBUFFER_16384:
		reg_rctl |= IXGB_RCTL_BSIZE_16384;
		break;
	}

	reg_rctl |= IXGB_RCTL_RXEN;

	/* Enable Receives */
	IXGB_WRITE_REG(&sc->hw, RCTL, reg_rctl);
}

/*********************************************************************
 *
 *  Free receive related data structures.
 *
 **********************************************************************/
void
ixgb_free_receive_structures(struct ixgb_softc *sc)
{
	struct ixgb_buffer *rx_buffer;
	int             i;

	INIT_DEBUGOUT("free_receive_structures: begin");

	if (sc->rx_buffer_area != NULL) {
		rx_buffer = sc->rx_buffer_area;
		for (i = 0; i < sc->num_rx_desc; i++, rx_buffer++) {
			if (rx_buffer->map != NULL &&
			    rx_buffer->map->dm_nsegs > 0) {
				bus_dmamap_sync(sc->rxtag, rx_buffer->map,
				    0, rx_buffer->map->dm_mapsize,
				    BUS_DMASYNC_POSTREAD);
				bus_dmamap_unload(sc->rxtag,
				    rx_buffer->map);
			}
			if (rx_buffer->m_head != NULL) {
				m_freem(rx_buffer->m_head);
				rx_buffer->m_head = NULL;
			}
			if (rx_buffer->map != NULL) {
				bus_dmamap_destroy(sc->rxtag,
				    rx_buffer->map);
				rx_buffer->map = NULL;
			}
		}
	}
	if (sc->rx_buffer_area != NULL) {
		free(sc->rx_buffer_area, M_DEVBUF, 0);
		sc->rx_buffer_area = NULL;
	}
	if (sc->rxtag != NULL)
		sc->rxtag = NULL;
}

/*********************************************************************
 *
 *  This routine executes in interrupt context. It replenishes
 *  the mbufs in the descriptor and sends data which has been
 *  dma'ed into host memory to upper layer.
 *
 *  We loop at most count times if count is > 0, or until done if
 *  count < 0.
 *
 *********************************************************************/
void
ixgb_rxeof(struct ixgb_softc *sc, int count)
{
	struct ifnet   *ifp;
	struct mbuf_list ml = MBUF_LIST_INITIALIZER();
	struct mbuf    *mp;
	int             eop = 0;
	int             len;
	u_int8_t        accept_frame = 0;
	int             i;
	int             next_to_use = 0;
	int             eop_desc;

	/* Pointer to the receive descriptor being examined. */
	struct ixgb_rx_desc *current_desc;

	ifp = &sc->interface_data.ac_if;
	i = sc->next_rx_desc_to_check;
	next_to_use = sc->next_rx_desc_to_use;
	eop_desc = sc->next_rx_desc_to_check;
	current_desc = &sc->rx_desc_base[i];
	bus_dmamap_sync(sc->rxdma.dma_tag, sc->rxdma.dma_map, 0,
	    sc->rxdma.dma_map->dm_mapsize, BUS_DMASYNC_POSTREAD);

	if (!((current_desc->status) & IXGB_RX_DESC_STATUS_DD))
		return;

	while ((current_desc->status & IXGB_RX_DESC_STATUS_DD) &&
		    (count != 0) &&
		    (ifp->if_flags & IFF_RUNNING)) {

		mp = sc->rx_buffer_area[i].m_head;
		bus_dmamap_sync(sc->rxtag, sc->rx_buffer_area[i].map,
		    0, sc->rx_buffer_area[i].map->dm_mapsize,
		    BUS_DMASYNC_POSTREAD);
		bus_dmamap_unload(sc->rxtag, sc->rx_buffer_area[i].map);

		accept_frame = 1;
		if (current_desc->status & IXGB_RX_DESC_STATUS_EOP) {
			count--;
			eop = 1;
		} else {
			eop = 0;
		}
		len = letoh16(current_desc->length);

		if (current_desc->errors & (IXGB_RX_DESC_ERRORS_CE |
			    IXGB_RX_DESC_ERRORS_SE | IXGB_RX_DESC_ERRORS_P |
					    IXGB_RX_DESC_ERRORS_RXE))
			accept_frame = 0;
		if (accept_frame) {

			/* Assign correct length to the current fragment */
			mp->m_len = len;

			if (sc->fmp == NULL) {
				mp->m_pkthdr.len = len;
				sc->fmp = mp;	/* Store the first mbuf */
				sc->lmp = mp;
			} else {
				/* Chain mbuf's together */
				mp->m_flags &= ~M_PKTHDR;
				sc->lmp->m_next = mp;
				sc->lmp = sc->lmp->m_next;
				sc->fmp->m_pkthdr.len += len;
			}

			if (eop) {
				eop_desc = i;
				ixgb_receive_checksum(sc, current_desc, sc->fmp);

#if NVLAN > 0
				if (current_desc->status & IXGB_RX_DESC_STATUS_VP) {
					sc->fmp->m_pkthdr.ether_vtag =
					    letoh16(current_desc->special);
					sc->fmp->m_flags |= M_VLANTAG;
				}
#endif


				ml_enqueue(&ml, sc->fmp);
				sc->fmp = NULL;
				sc->lmp = NULL;
			}
			sc->rx_buffer_area[i].m_head = NULL;
		} else {
			sc->dropped_pkts++;
			m_freem(sc->fmp);
			sc->fmp = NULL;
			sc->lmp = NULL;
		}

		/* Zero out the receive descriptors status  */
		current_desc->status = 0;
		bus_dmamap_sync(sc->rxdma.dma_tag, sc->rxdma.dma_map, 0,
		    sc->rxdma.dma_map->dm_mapsize,
		    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);

		/* Advance our pointers to the next descriptor */
		if (++i == sc->num_rx_desc) {
			i = 0;
			current_desc = sc->rx_desc_base;
		} else
			current_desc++;
	}
	sc->next_rx_desc_to_check = i;

	if (--i < 0)
		i = (sc->num_rx_desc - 1);

	/*
	 * 82597EX: Workaround for redundent write back in receive descriptor ring (causes
 	 * memory corruption). Avoid using and re-submitting the most recently received RX
	 * descriptor back to hardware.
	 *
	 * if(Last written back descriptor == EOP bit set descriptor)
	 * 	then avoid re-submitting the most recently received RX descriptor 
	 *	back to hardware.
	 * if(Last written back descriptor != EOP bit set descriptor)
	 *	then avoid re-submitting the most recently received RX descriptors
	 * 	till last EOP bit set descriptor. 
	 */
	if (eop_desc != i) {
		if (++eop_desc == sc->num_rx_desc)
			eop_desc = 0;
		i = eop_desc;
	} 
	/* Replenish the descriptors with new mbufs till last EOP bit set descriptor */
	while (next_to_use != i) {
		current_desc = &sc->rx_desc_base[next_to_use];
		if ((current_desc->errors & (IXGB_RX_DESC_ERRORS_CE |
			    IXGB_RX_DESC_ERRORS_SE | IXGB_RX_DESC_ERRORS_P |
					     IXGB_RX_DESC_ERRORS_RXE))) {
			mp = sc->rx_buffer_area[next_to_use].m_head;
			ixgb_get_buf(sc, next_to_use, mp);
		} else {
			if (ixgb_get_buf(sc, next_to_use, NULL) == ENOBUFS)
				break;
		}
		/* Advance our pointers to the next descriptor */
		if (++next_to_use == sc->num_rx_desc) 
			next_to_use = 0;
	}
	sc->next_rx_desc_to_use = next_to_use;
	if (--next_to_use < 0)
                next_to_use = (sc->num_rx_desc - 1);
        /* Advance the IXGB's Receive Queue #0  "Tail Pointer" */
        IXGB_WRITE_REG(&sc->hw, RDT, next_to_use);

	if_input(ifp, &ml);
}

/*********************************************************************
 *
 *  Verify that the hardware indicated that the checksum is valid.
 *  Inform the stack about the status of checksum so that stack
 *  doesn't spend time verifying the checksum.
 *
 *********************************************************************/
void
ixgb_receive_checksum(struct ixgb_softc *sc,
		      struct ixgb_rx_desc *rx_desc,
		      struct mbuf *mp)
{
	if (rx_desc->status & IXGB_RX_DESC_STATUS_IXSM) {
		mp->m_pkthdr.csum_flags = 0;
		return;
	}

	if (rx_desc->status & IXGB_RX_DESC_STATUS_IPCS) {
		/* Did it pass? */
		if (!(rx_desc->errors & IXGB_RX_DESC_ERRORS_IPE)) {
			/* IP Checksum Good */
			mp->m_pkthdr.csum_flags = M_IPV4_CSUM_IN_OK;

		} else {
			mp->m_pkthdr.csum_flags = 0;
		}
	}
	if (rx_desc->status & IXGB_RX_DESC_STATUS_TCPCS) {
		/* Did it pass? */
		if (!(rx_desc->errors & IXGB_RX_DESC_ERRORS_TCPE)) {
			mp->m_pkthdr.csum_flags |=
				M_TCP_CSUM_IN_OK | M_UDP_CSUM_IN_OK;
		}
	}
}

/*
 * This turns on the hardware offload of the VLAN
 * tag insertion and strip
 */
void
ixgb_enable_hw_vlans(struct ixgb_softc *sc)
{
	uint32_t ctrl;

	ctrl = IXGB_READ_REG(&sc->hw, CTRL0);
	ctrl |= IXGB_CTRL0_VME;
	IXGB_WRITE_REG(&sc->hw, CTRL0, ctrl);
}

void
ixgb_enable_intr(struct ixgb_softc *sc)
{
	uint32_t val;

	val = IXGB_INT_RXT0 | IXGB_INT_TXDW | IXGB_INT_RXDMT0 |
	      IXGB_INT_LSC | IXGB_INT_RXO;
	if (sc->hw.subsystem_vendor_id == SUN_SUBVENDOR_ID)
		val |= IXGB_INT_GPI0;
	IXGB_WRITE_REG(&sc->hw, IMS, val);
}

void
ixgb_disable_intr(struct ixgb_softc *sc)
{
	IXGB_WRITE_REG(&sc->hw, IMC, ~0);
}

void
ixgb_write_pci_cfg(struct ixgb_hw *hw,
		   uint32_t reg,
		   uint16_t *value)
{
	struct pci_attach_args *pa = &((struct ixgb_osdep *)hw->back)->ixgb_pa;
	pci_chipset_tag_t pc = pa->pa_pc;
	/* Should we do read/mask/write...?  16 vs 32 bit!!! */
	pci_conf_write(pc, pa->pa_tag, reg, *value);
}

/**********************************************************************
 *
 *  Update the board statistics counters.
 *
 **********************************************************************/
void
ixgb_update_stats_counters(struct ixgb_softc *sc)
{
	struct ifnet   *ifp;

	sc->stats.crcerrs += IXGB_READ_REG(&sc->hw, CRCERRS);
	sc->stats.gprcl += IXGB_READ_REG(&sc->hw, GPRCL);
	sc->stats.gprch += IXGB_READ_REG(&sc->hw, GPRCH);
	sc->stats.gorcl += IXGB_READ_REG(&sc->hw, GORCL);
	sc->stats.gorch += IXGB_READ_REG(&sc->hw, GORCH);
	sc->stats.bprcl += IXGB_READ_REG(&sc->hw, BPRCL);
	sc->stats.bprch += IXGB_READ_REG(&sc->hw, BPRCH);
	sc->stats.mprcl += IXGB_READ_REG(&sc->hw, MPRCL);
	sc->stats.mprch += IXGB_READ_REG(&sc->hw, MPRCH);
	sc->stats.roc += IXGB_READ_REG(&sc->hw, ROC);

	sc->stats.mpc += IXGB_READ_REG(&sc->hw, MPC);
	sc->stats.dc += IXGB_READ_REG(&sc->hw, DC);
	sc->stats.rlec += IXGB_READ_REG(&sc->hw, RLEC);
	sc->stats.xonrxc += IXGB_READ_REG(&sc->hw, XONRXC);
	sc->stats.xontxc += IXGB_READ_REG(&sc->hw, XONTXC);
	sc->stats.xoffrxc += IXGB_READ_REG(&sc->hw, XOFFRXC);
	sc->stats.xofftxc += IXGB_READ_REG(&sc->hw, XOFFTXC);
	sc->stats.gptcl += IXGB_READ_REG(&sc->hw, GPTCL);
	sc->stats.gptch += IXGB_READ_REG(&sc->hw, GPTCH);
	sc->stats.gotcl += IXGB_READ_REG(&sc->hw, GOTCL);
	sc->stats.gotch += IXGB_READ_REG(&sc->hw, GOTCH);
	sc->stats.ruc += IXGB_READ_REG(&sc->hw, RUC);
	sc->stats.rfc += IXGB_READ_REG(&sc->hw, RFC);
	sc->stats.rjc += IXGB_READ_REG(&sc->hw, RJC);
	sc->stats.torl += IXGB_READ_REG(&sc->hw, TORL);
	sc->stats.torh += IXGB_READ_REG(&sc->hw, TORH);
	sc->stats.totl += IXGB_READ_REG(&sc->hw, TOTL);
	sc->stats.toth += IXGB_READ_REG(&sc->hw, TOTH);
	sc->stats.tprl += IXGB_READ_REG(&sc->hw, TPRL);
	sc->stats.tprh += IXGB_READ_REG(&sc->hw, TPRH);
	sc->stats.tptl += IXGB_READ_REG(&sc->hw, TPTL);
	sc->stats.tpth += IXGB_READ_REG(&sc->hw, TPTH);
	sc->stats.plt64c += IXGB_READ_REG(&sc->hw, PLT64C);
	sc->stats.mptcl += IXGB_READ_REG(&sc->hw, MPTCL);
	sc->stats.mptch += IXGB_READ_REG(&sc->hw, MPTCH);
	sc->stats.bptcl += IXGB_READ_REG(&sc->hw, BPTCL);
	sc->stats.bptch += IXGB_READ_REG(&sc->hw, BPTCH);

	sc->stats.uprcl += IXGB_READ_REG(&sc->hw, UPRCL);
	sc->stats.uprch += IXGB_READ_REG(&sc->hw, UPRCH);
	sc->stats.vprcl += IXGB_READ_REG(&sc->hw, VPRCL);
	sc->stats.vprch += IXGB_READ_REG(&sc->hw, VPRCH);
	sc->stats.jprcl += IXGB_READ_REG(&sc->hw, JPRCL);
	sc->stats.jprch += IXGB_READ_REG(&sc->hw, JPRCH);
	sc->stats.rnbc += IXGB_READ_REG(&sc->hw, RNBC);
	sc->stats.icbc += IXGB_READ_REG(&sc->hw, ICBC);
	sc->stats.ecbc += IXGB_READ_REG(&sc->hw, ECBC);
	sc->stats.uptcl += IXGB_READ_REG(&sc->hw, UPTCL);
	sc->stats.uptch += IXGB_READ_REG(&sc->hw, UPTCH);
	sc->stats.vptcl += IXGB_READ_REG(&sc->hw, VPTCL);
	sc->stats.vptch += IXGB_READ_REG(&sc->hw, VPTCH);
	sc->stats.jptcl += IXGB_READ_REG(&sc->hw, JPTCL);
	sc->stats.jptch += IXGB_READ_REG(&sc->hw, JPTCH);
	sc->stats.tsctc += IXGB_READ_REG(&sc->hw, TSCTC);
	sc->stats.tsctfc += IXGB_READ_REG(&sc->hw, TSCTFC);
	sc->stats.ibic += IXGB_READ_REG(&sc->hw, IBIC);
	sc->stats.lfc += IXGB_READ_REG(&sc->hw, LFC);
	sc->stats.pfrc += IXGB_READ_REG(&sc->hw, PFRC);
	sc->stats.pftc += IXGB_READ_REG(&sc->hw, PFTC);
	sc->stats.mcfrc += IXGB_READ_REG(&sc->hw, MCFRC);

	ifp = &sc->interface_data.ac_if;

	/* Fill out the OS statistics structure */
	ifp->if_collisions = 0;

	/* Rx Errors */
	ifp->if_ierrors =
		sc->dropped_pkts +
		sc->stats.crcerrs +
		sc->stats.rnbc +
		sc->stats.mpc +
		sc->stats.rlec;

	/* Tx Errors */
	ifp->if_oerrors =
		sc->watchdog_events;
}

#ifdef IXGB_DEBUG
/**********************************************************************
 *
 *  This routine is called only when ixgb_display_debug_stats is enabled.
 *  This routine provides a way to take a look at important statistics
 *  maintained by the driver and hardware.
 *
 **********************************************************************/
void
ixgb_print_hw_stats(struct ixgb_softc *sc)
{
	char            buf_speed[100], buf_type[100];
	ixgb_bus_speed  bus_speed;
	ixgb_bus_type   bus_type;
	const char * const unit = sc->sc_dv.dv_xname;

	bus_speed = sc->hw.bus.speed;
	bus_type = sc->hw.bus.type;
	snprintf(buf_speed, sizeof(buf_speed),
		bus_speed == ixgb_bus_speed_33 ? "33MHz" :
		bus_speed == ixgb_bus_speed_66 ? "66MHz" :
		bus_speed == ixgb_bus_speed_100 ? "100MHz" :
		bus_speed == ixgb_bus_speed_133 ? "133MHz" :
		"UNKNOWN");
	printf("%s: PCI_Bus_Speed = %s\n", unit,
		buf_speed);

	snprintf(buf_type, sizeof(buf_type),
		bus_type == ixgb_bus_type_pci ? "PCI" :
		bus_type == ixgb_bus_type_pcix ? "PCI-X" :
		"UNKNOWN");
	printf("%s: PCI_Bus_Type = %s\n", unit,
		buf_type);

	printf("%s: Tx Descriptors not Avail1 = %ld\n", unit,
		sc->no_tx_desc_avail1);
	printf("%s: Tx Descriptors not Avail2 = %ld\n", unit,
		sc->no_tx_desc_avail2);
	printf("%s: Std Mbuf Failed = %ld\n", unit,
		sc->mbuf_alloc_failed);
	printf("%s: Std Cluster Failed = %ld\n", unit,
		sc->mbuf_cluster_failed);

	printf("%s: Defer count = %lld\n", unit,
		(long long)sc->stats.dc);
	printf("%s: Missed Packets = %lld\n", unit,
		(long long)sc->stats.mpc);
	printf("%s: Receive No Buffers = %lld\n", unit,
		(long long)sc->stats.rnbc);
	printf("%s: Receive length errors = %lld\n", unit,
		(long long)sc->stats.rlec);
	printf("%s: Crc errors = %lld\n", unit,
		(long long)sc->stats.crcerrs);
	printf("%s: Driver dropped packets = %ld\n", unit,
		sc->dropped_pkts);

	printf("%s: XON Rcvd = %lld\n", unit,
		(long long)sc->stats.xonrxc);
	printf("%s: XON Xmtd = %lld\n", unit,
		(long long)sc->stats.xontxc);
	printf("%s: XOFF Rcvd = %lld\n", unit,
		(long long)sc->stats.xoffrxc);
	printf("%s: XOFF Xmtd = %lld\n", unit,
		(long long)sc->stats.xofftxc);

	printf("%s: Good Packets Rcvd = %lld\n", unit,
		(long long)sc->stats.gprcl);
	printf("%s: Good Packets Xmtd = %lld\n", unit,
		(long long)sc->stats.gptcl);

	printf("%s: Jumbo frames recvd = %lld\n", unit,
		(long long)sc->stats.jprcl);
	printf("%s: Jumbo frames Xmtd = %lld\n", unit,
		(long long)sc->stats.jptcl);
}
#endif
@


1.70
log
@m_free() and m_freem() test for NULL.  Simplify callers which had their own
NULL tests.

ok mpi@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.69 2016/04/13 10:34:32 mpi Exp $ */
a1385 2
			ifp->if_opackets++;

@


1.69
log
@G/C IFQ_SET_READY().
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.68 2015/11/25 03:09:59 dlg Exp $ */
d1789 1
a1789 2
			if (sc->fmp != NULL)
				m_freem(sc->fmp);
@


1.68
log
@replace IFF_OACTIVE manipulation with mpsafe operations.

there are two things shared between the network stack and drivers
in the send path: the send queue and the IFF_OACTIVE flag. the send
queue is now protected by a mutex. this diff makes the oactive
functionality mpsafe too.

IFF_OACTIVE is part of if_flags. there are two problems with that.
firstly, if_flags is a short and we dont have any MI atomic operations
to manipulate a short. secondly, while we could make the IFF_OACTIVE
operates mpsafe, all changes to other flags would have to be made
safe at the same time, otherwise a read-modify-write cycle on their
updates could clobber the oactive change.

instead, this moves the oactive mark into struct ifqueue and provides
an API for changing it. there's ifq_set_oactive, ifq_clr_oactive,
and ifq_is_oactive. these are modelled on ifsq_set_oactive,
ifsq_clr_oactive, and ifsq_is_oactive in dragonflybsd.

this diff includes changes to all the drivers manipulating IFF_OACTIVE
to now use the ifsq_{set,clr_is}_oactive API too.

ok kettenis@@ mpi@@ jmatthew@@ deraadt@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.67 2015/11/20 03:35:23 dlg Exp $ */
a1014 1
	IFQ_SET_READY(&ifp->if_snd);
@


1.67
log
@shuffle struct ifqueue so in flight mbufs are protected by a mutex.

the code is refactored so the IFQ macros call newly implemented ifq
functions. the ifq code is split so each discipline (priq and hfsc
in our case) is an opaque set of operations that the common ifq
code can call. the common code does the locking, accounting (ifq_len
manipulation), and freeing of the mbuf if the disciplines enqueue
function rejects it. theyre kind of like bufqs in the block layer
with their fifo and nscan disciplines.

the new api also supports atomic switching of disciplines at runtime.
the hfsc setup in pf_ioctl.c has been tweaked to build a complete
hfsc_if structure which it attaches to the send queue in a single
operation, rather than attaching to the interface up front and
building up a list of queues.

the send queue is now mutexed, which raises the expectation that
packets can be enqueued or purged on one cpu while another cpu is
dequeueing them in a driver for transmission. a lot of drivers use
IFQ_POLL to peek at an mbuf and attempt to fit it on the ring before
committing to it with a later IFQ_DEQUEUE operation. if the mbuf
gets freed in between the POLL and DEQUEUE operations, fireworks
will ensue.

to avoid this, the ifq api introduces ifq_deq_begin, ifq_deq_rollback,
and ifq_deq_commit. ifq_deq_begin allows a driver to take the ifq
mutex and get a reference to the mbuf they wish to try and tx. if
there's space, they can ifq_deq_commit it to remove the mbuf and
release the mutex. if there's no space, ifq_deq_rollback simply
releases the mutex. this api was developed to make updating the
drivers using IFQ_POLL easy, instead of having to do significant
semantic changes to avoid POLL that we cannot test on all the
hardware.

the common code has been tested pretty hard, and all the driver
modifications are straightforward except for de(4). if that breaks
it can be dealt with later.

ok mpi@@ jmatthew@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.66 2015/10/25 13:04:28 mpi Exp $ */
d275 1
a275 1
	if ((ifp->if_flags & (IFF_OACTIVE | IFF_RUNNING)) != IFF_RUNNING)
d292 1
a292 1
			ifp->if_flags |= IFF_OACTIVE;
d489 1
a489 1
	ifp->if_flags &= ~IFF_OACTIVE;
d852 2
a853 1
	ifp->if_flags &= ~(IFF_RUNNING | IFF_OACTIVE);
d1418 1
a1418 1
		ifp->if_flags &= ~IFF_OACTIVE;
@


1.66
log
@arp_ifinit() is no longer needed.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.65 2015/06/24 09:40:54 mpi Exp $ */
d286 1
a286 1
		IFQ_POLL(&ifp->if_snd, m_head);
d291 1
d296 1
a296 1
		IFQ_DEQUEUE(&ifp->if_snd, m_head);
@


1.65
log
@Increment if_ipackets in if_input().

Note that pseudo-drivers not using if_input() are not affected by this
conversion.

ok mikeb@@, kettenis@@, claudio@@, dlg@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.64 2015/04/30 07:51:07 mpi Exp $ */
a333 1
	struct ifaddr	*ifa = (struct ifaddr *) data;
a345 2
		if (ifa->ifa_addr->sa_family == AF_INET)
			arp_ifinit(&sc->interface_data, ifa);
@


1.64
log
@Convert moar drivers to if_input().

ok dlg@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.63 2014/12/22 02:28:52 tedu Exp $ */
a1772 1
				ifp->if_ipackets++;
@


1.63
log
@unifdef INET
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.62 2014/07/13 23:10:23 deraadt Exp $ */
d1708 1
a1772 1
				sc->fmp->m_pkthdr.rcvif = ifp;
a1783 9
#if NBPFILTER > 0
				/*
				 * Handle BPF listeners. Let the BPF
				 * user see the packet.
				 */
				if (ifp->if_bpf)
					bpf_mtap_ether(ifp->if_bpf, sc->fmp,
					    BPF_DIRECTION_IN);
#endif
d1785 1
a1785 1
				ether_input_mbuf(ifp, sc->fmp);
d1854 2
@


1.62
log
@Some reallocarray() use; review Jean-Philippe Ouellet, patrick keshishian
ok tedu
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.61 2014/07/12 18:48:51 tedu Exp $ */
a346 1
#ifdef INET
a348 1
#endif /* INET */
@


1.61
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.60 2013/11/26 09:50:33 mpi Exp $ */
d1138 2
a1139 2
	if (!(sc->tx_buffer_area = malloc(sizeof(struct ixgb_buffer) *
	    sc->num_tx_desc, M_DEVBUF, M_NOWAIT | M_ZERO))) {
d1504 2
a1505 2
	if (!(sc->rx_buffer_area = malloc(sizeof(struct ixgb_buffer) *
	    sc->num_rx_desc, M_DEVBUF, M_NOWAIT | M_ZERO))) {
@


1.60
log
@Instead of comparing the lower and higher addresses of all the multicast
entries to decide if the IFF_ALLMULTI flag should be set, check if there
is at least one real range between them.

This should not change the behavior of any driver but if you encounter
any problem, feel free to revert the offending chunk and ping me about
it.

ok naddy@@, dlg@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.59 2011/04/05 18:01:21 henning Exp $ */
d1276 1
a1276 1
		free(sc->tx_buffer_area, M_DEVBUF);
d1689 1
a1689 1
		free(sc->rx_buffer_area, M_DEVBUF);
@


1.59
log
@mechanic rename M_{TCP|UDP}V4_CSUM_OUT -> M_{TCP|UDP}_CSUM_OUT
ok claudio krw
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.58 2011/04/03 15:36:02 jasper Exp $ */
d756 6
a763 4
		if (bcmp(enm->enm_addrlo, enm->enm_addrhi, ETHER_ADDR_LEN)) {
			ifp->if_flags |= IFF_ALLMULTI;
			mcnt = MAX_NUM_MULTICAST_ADDRESSES;
		}
d772 1
@


1.58
log
@use nitems(); no binary change for drivers that are compiled on amd64.

ok claudio@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.57 2010/09/20 07:50:19 deraadt Exp $ */
d1299 1
a1299 1
		if (mp->m_pkthdr.csum_flags & M_TCPV4_CSUM_OUT) {
d1306 1
a1306 1
		} else if (mp->m_pkthdr.csum_flags & M_UDPV4_CSUM_OUT) {
@


1.57
log
@Fix a variety of structure packing and byte order bugs to try to get
BE support working.  Doesn't hurt LE32 or LE64.  Doesn't make BE64
(sparc64) move traffic yet, but it no longer crashes as hard.
ok jsg
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.56 2010/08/27 08:24:53 deraadt Exp $ */
d153 1
a153 1
	    sizeof(ixgb_devices)/sizeof(ixgb_devices[0])));
@


1.56
log
@These do not need powerhook functions.
ok jsg
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.55 2009/08/13 14:24:47 jasper Exp $ */
d693 1
a693 1
		current_tx_desc->vlan = m_head->m_pkthdr.ether_vtag;
d696 1
a696 1
		current_tx_desc->cmd_type_len |= IXGB_TX_DESC_CMD_VLE;
d1479 1
d1746 1
a1746 1
		len = current_desc->length;
d1778 1
a1778 1
					    current_desc->special;
@


1.55
log
@- consistify cfdriver for the ethernet drivers (0 -> NULL)

ok dlg@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.54 2009/08/10 19:41:05 deraadt Exp $ */
a67 1
void ixgb_power(int, void *);
a246 1
	sc->sc_powerhook = powerhook_establish(ixgb_power, sc);
a255 13
}

void
ixgb_power(int why, void *arg)
{
	struct ixgb_softc *sc = (struct ixgb_softc *)arg;
	struct ifnet *ifp;

	if (why == PWR_RESUME) {
		ifp = &sc->interface_data.ac_if;
		if (ifp->if_flags & IFF_UP)
			ixgb_init(sc);
	}
@


1.54
log
@A few more simple cases of shutdown hooks which only call xxstop, when
we now know the interface has already been stopped
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.53 2009/06/24 13:36:56 deraadt Exp $ */
d128 1
a128 1
	0, "ixgb", DV_IFNET
@


1.53
log
@like I did for em(4) before, doubled error messages are silly
from brad
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.52 2008/11/28 02:44:18 brad Exp $ */
a66 1
void ixgb_shutdown(void *);
a248 1
	sc->sc_shutdownhook = shutdownhook_establish(ixgb_shutdown, sc);
a270 14
}

/*********************************************************************
 *
 *  Shutdown entry point
 *
 **********************************************************************/ 

void
ixgb_shutdown(void *arg)
{
	struct ixgb_softc *sc = arg;

	ixgb_stop(sc);
@


1.52
log
@Eliminate the redundant bits of code for MTU and multicast handling
from the individual drivers now that ether_ioctl() handles this.

Shrinks the i386 kernels by..
RAMDISK - 2176 bytes
RAMDISKB - 1504 bytes
RAMDISKC - 736 bytes

Tested by naddy@@/okan@@/sthen@@/brad@@/todd@@/jmc@@ and lots of users.
Build tested on almost all archs by todd@@/brad@@

ok naddy@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.51 2008/11/09 15:08:26 naddy Exp $ */
d204 1
a204 3
	if (ixgb_allocate_pci_resources(sc)) {
		printf("%s: Allocation of PCI resources failed\n",
		       sc->sc_dv.dv_xname);
a205 1
	}
@


1.51
log
@Introduce bpf_mtap_ether(), which for the benefit of bpf listeners
creates the VLAN encapsulation from the tag stored in the mbuf
header.  Idea from FreeBSD, input from claudio@@ and canacar@@.

Switch all hardware VLAN enabled drivers to the new function.

ok claudio@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.50 2008/10/28 05:43:11 brad Exp $ */
a366 3
        int		s, error = 0;
	struct ifreq   *ifr = (struct ifreq *) data;
	struct ifaddr  *ifa = (struct ifaddr *)data;
d368 3
d386 1
a386 7
	case SIOCSIFMTU:
		IOCTL_DEBUGOUT("ioctl rcv'd: SIOCSIFMTU (Set Interface MTU)");
		if (ifr->ifr_mtu < ETHERMIN || ifr->ifr_mtu > ifp->if_hardmtu)
			error = EINVAL;
		else if (ifp->if_mtu != ifr->ifr_mtu)
			ifp->if_mtu = ifr->ifr_mtu;
		break;
d409 1
a409 16
	case SIOCADDMULTI:
	case SIOCDELMULTI:
		IOCTL_DEBUGOUT("ioctl rcv'd: SIOC(ADD|DEL)MULTI");
		error = (command == SIOCADDMULTI)
			? ether_addmulti(ifr, &sc->interface_data)
			: ether_delmulti(ifr, &sc->interface_data);

		if (error == ENETRESET) {
			if (ifp->if_flags & IFF_RUNNING) {
				ixgb_disable_intr(sc);
				ixgb_set_multi(sc);
				ixgb_enable_intr(sc);
			}
			error = 0;
		}
		break;
d415 1
d418 9
@


1.50
log
@Some tweaks for the usage of NVLAN > 0 checks in the code.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.49 2008/10/21 00:26:04 brad Exp $ */
d334 1
a334 1
			bpf_mtap(ifp->if_bpf, m_head, BPF_DIRECTION_OUT);
d1833 1
a1833 1
					bpf_mtap(ifp->if_bpf, sc->fmp,
@


1.49
log
@Re-add support TX VLAN tag insertion and RX VLAN tag stripping.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.48 2008/10/02 20:21:14 brad Exp $ */
d1818 2
d1825 1
@


1.48
log
@First step towards cleaning up the Ethernet driver ioctl handling.
Move calling ether_ioctl() from the top of the ioctl function, which
at the moment does absolutely nothing, to the default switch case.
Thus allowing drivers to define their own ioctl handlers and then
falling back on ether_ioctl(). The only functional change this results
in at the moment is having all Ethernet drivers returning the proper
errno of ENOTTY instead of EINVAL/ENXIO when encountering unknown
ioctl's.

Shrinks the i386 kernels by..
RAMDISK - 1024 bytes
RAMDISKB -  1120 bytes
RAMDISKC - 832 bytes

Tested by martin@@/jsing@@/todd@@/brad@@
Build tested on almost all archs by todd@@/brad@@

ok jsing@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.47 2008/09/30 17:59:22 brad Exp $ */
d113 1
d508 3
d735 9
d1064 4
d1817 6
a1833 2
				ixgb_receive_checksum(sc, current_desc,
						      sc->fmp);
d1939 14
@


1.47
log
@style nits.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.46 2008/09/24 19:12:59 chl Exp $ */
a372 5
	if ((error = ether_ioctl(ifp, &sc->interface_data, command, data)) > 0) {
		splx(s);
		return (error);
	}

d436 1
a436 2
		IOCTL_DEBUGOUT1("ioctl received: UNKNOWN (0x%X)\n", (int)command);
		error = ENOTTY;
@


1.46
log
@remove dead stores and newly created unused variables.

Found by LLVM/Clang Static Analyzer.

ok henning@@ brad@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.45 2008/09/10 14:01:22 blambert Exp $ */
d1119 1
a1119 3
			    dma->dma_vaddr,
			    size,
			    NULL,
@


1.45
log
@Convert timeout_add() calls using multiples of hz to timeout_add_sec()

Really just the low-hanging fruit of (hopefully) forthcoming timeout
conversions.

ok art@@, krw@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.44 2008/06/08 16:54:34 brad Exp $ */
a1477 1
	struct ifnet   *ifp;
a1479 2
	ifp = &sc->interface_data.ac_if;

a1604 1
	struct ifnet   *ifp;
a1605 2

	ifp = &sc->interface_data.ac_if;
@


1.44
log
@Correct the watchdog timer by moving it out from under the condition check
for the IFF_OACTIVE flag.

ok reyk@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.43 2008/06/08 16:53:23 brad Exp $ */
d549 1
a549 1
	timeout_add(&sc->timer_handle, hz);
d593 1
a593 1
			timeout_add(&sc->timer_handle, hz);
d843 1
a843 1
	timeout_add(&sc->timer_handle, hz);
@


1.43
log
@dma sync the tx ring and post new packets to the chip once per call to
the start routine instead of once per packet.

ok reyk@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.42 2008/06/08 16:20:27 reyk Exp $ */
d1453 1
a1453 1
	if (num_avail > IXGB_TX_CLEANUP_THRESHOLD) {
d1455 8
a1462 7
		/* All clean, turn off the timer */
		if (num_avail == sc->num_tx_desc)
			ifp->if_timer = 0;
		/* Some cleaned, reset the timer */
		else if (num_avail != sc->num_tx_desc_avail)
			ifp->if_timer = IXGB_TX_TIMEOUT;
	}
@


1.42
log
@don't declare foo_driver_version[] strings and turn them into defines,
nothing uses them and it saves a few bytes in the kernel.

ok claudio@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.41 2008/06/03 12:27:23 reyk Exp $ */
d306 1
d314 4
a319 1

d338 2
d341 11
a744 9

	/*
	 * Advance the Transmit Descriptor Tail (Tdt), this tells the E1000
	 * that this frame is available to transmit.
	 */
	bus_dmamap_sync(sc->txdma.dma_tag, sc->txdma.dma_map, 0,
	    sc->txdma.dma_map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
	IXGB_WRITE_REG(&sc->hw, TDT, i);
@


1.41
log
@put code to print periodic debug statistics in #ifdef IXGB_DEBUG, shrinks
the driver for about 990 bytes on i386.

ok brad@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.40 2008/03/02 08:42:42 brad Exp $ */
d49 1
a49 1
char ixgb_driver_version[] = "6.1.0";
@


1.40
log
@If bus_dmamap_load_mbuf() fails in ixgb_get_buf() use m_freem() intead of
m_free() to free the mbuf cluster.

ok mglocker@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.39 2008/02/19 18:47:18 brad Exp $ */
d38 1
d43 1
d106 1
d108 1
d830 1
d833 1
d2044 1
d2119 1
@


1.39
log
@Add support for the optics on the Sun variant of ixgb(4) boards.

From Intel

tested by and ok dlg@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.38 2007/10/01 15:34:48 krw Exp $ */
d1499 1
a1499 1
		m_free(mp);
@


1.38
log
@More easy bzero() -> M_ZERO. Use 'p = malloc(sizeof(*p) ...' where
obvious.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.37 2007/09/19 03:50:24 brad Exp $ */
d1922 7
a1928 2
	IXGB_WRITE_REG(&sc->hw, IMS, (IXGB_INT_RXT0 | IXGB_INT_TXDW |
			    IXGB_INT_RXDMT0 | IXGB_INT_LSC | IXGB_INT_RXO));
@


1.37
log
@Use the proper baudrate for 10Gb hw now that it can fit into the buadrate
field.

ok claudio@@ dlg@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.36 2006/12/04 14:35:20 reyk Exp $ */
d1157 2
a1158 4
	if (!(sc->tx_buffer_area =
	      (struct ixgb_buffer *) malloc(sizeof(struct ixgb_buffer) *
					    sc->num_tx_desc, M_DEVBUF,
					    M_NOWAIT))) {
a1162 2
	bzero(sc->tx_buffer_area,
	      sizeof(struct ixgb_buffer) * sc->num_tx_desc);
d1524 2
a1525 4
	if (!(sc->rx_buffer_area =
	      (struct ixgb_buffer *) malloc(sizeof(struct ixgb_buffer) *
					    sc->num_rx_desc, M_DEVBUF,
					    M_NOWAIT))) {
a1529 3

	bzero(sc->rx_buffer_area,
	      sizeof(struct ixgb_buffer) * sc->num_rx_desc);
@


1.36
log
@report full/half duplex state for non-MII interfaces

ok brad@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.35 2006/11/28 04:45:08 brad Exp $ */
d841 1
a841 1
			ifp->if_baudrate = 1000000000;
@


1.35
log
@Pre-allocate the TX DMA maps intead of creating and destroying a DMA map
per packet sent.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.34 2006/11/28 04:26:50 brad Exp $ */
d843 1
a843 1
			ifp->if_link_state = LINK_STATE_UP;
@


1.34
log
@- ixgb_dma_alloc(): Uncomment the line setting the DMA tag to NUL upon failure.
- ixgb_dma_free(): Return if the DMA tag is NUL.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.33 2006/11/28 02:22:39 brad Exp $ */
d657 2
a658 1
	int             i, j, error;
d660 1
a660 2
	struct ixgb_q   q;
	struct ixgb_buffer *tx_buffer = NULL;
d675 1
d679 4
a682 7
	if (bus_dmamap_create(sc->txtag, IXGB_MAX_JUMBO_FRAME_SIZE,
	    IXGB_MAX_SCATTER, IXGB_MAX_JUMBO_FRAME_SIZE, 0,
	    BUS_DMA_NOWAIT, &q.map)) {
		sc->no_tx_map_avail++;
		return (ENOMEM);
	}
	error = bus_dmamap_load_mbuf(sc->txtag, q.map,
a685 1
		bus_dmamap_destroy(sc->txtag, q.map);
d688 1
a688 1
	IXGB_KASSERT(q.map->dm_nsegs != 0, ("ixgb_encap: empty packet"));
d690 2
a691 5
	if (q.map->dm_nsegs > sc->num_tx_desc_avail) {
		sc->no_tx_desc_avail2++;
		bus_dmamap_destroy(sc->txtag, q.map);
		return (ENOBUFS);
	}
d700 1
a700 1
	for (j = 0; j < q.map->dm_nsegs; j++) {
d704 2
a705 2
		current_tx_desc->buff_addr = htole64(q.map->dm_segs[j].ds_addr);
		current_tx_desc->cmd_type_len = htole32((sc->txd_cmd | q.map->dm_segs[j].ds_len));
d713 1
a713 1
	sc->num_tx_desc_avail -= q.map->dm_nsegs;
d717 1
a717 2
	tx_buffer->map = q.map;
	bus_dmamap_sync(sc->txtag, q.map, 0, q.map->dm_mapsize,
d735 5
d1127 1
d1179 2
a1180 1
	sc->txtag = sc->osdep.ixgb_pa.pa_dmat;
d1182 2
a1183 2
	if (ixgb_allocate_transmit_structures(sc))
		return (ENOMEM);
d1188 15
d1211 2
d1215 4
d1278 9
a1287 2
				bus_dmamap_unload(sc->txtag, tx_buffer->map);
				bus_dmamap_destroy(sc->txtag, tx_buffer->map);
d1289 6
a1295 1
			tx_buffer->m_head = NULL;
d1412 1
a1412 1
		if (tx_buffer->m_head) {
d1414 7
a1420 2
			bus_dmamap_unload(sc->txtag, tx_buffer->map);
			bus_dmamap_destroy(sc->txtag, tx_buffer->map);
@


1.33
log
@style changes and cleaning. no op.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.32 2006/11/18 19:20:17 brad Exp $ */
d1128 1
a1128 1
	/* dma->dma_tag = NULL; */
d1135 12
a1146 4
	bus_dmamap_unload(dma->dma_tag, dma->dma_map);
	bus_dmamem_unmap(dma->dma_tag, dma->dma_vaddr, dma->dma_size);
	bus_dmamem_free(dma->dma_tag, &dma->dma_seg, dma->dma_nseg);
	bus_dmamap_destroy(dma->dma_tag, dma->dma_map);
@


1.32
log
@ixgb_get_buf():
- Use bus_dmamap_load_mbuf() instead of bus_dmamap_load() + mtod().
- Only BUS_DMASYNC_PREREAD is necessary for the bus_dmamap_sync().
ixgb_allocate_receive_structures():
- Clean up error handling for receive buffer allocation and just
have everything done by ixgb_free_receive_structures() now.
ixgb_free_receive_structures():
- A few changes here to allow this function to be called from
ixgb_stop() as well as ixgb_allocate_receive_structures().
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.31 2006/11/18 18:46:20 brad Exp $ */
d438 1
a438 2
	struct ixgb_softc *sc;
	sc = ifp->if_softc;
d448 1
a450 1
	ixgb_stop(sc);
d581 1
a581 2
	if (ifp->if_flags & IFF_RUNNING &&
	    IFQ_IS_EMPTY(&ifp->if_snd) == 0)
a1119 2
/* fail_4: */
	bus_dmamap_unload(dma->dma_tag, dma->dma_map);
@


1.31
log
@add a few comments
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.30 2006/10/02 00:28:09 brad Exp $ */
d107 1
a107 1
ixgb_get_buf(int i, struct ixgb_softc *,
d1420 1
a1420 1
ixgb_get_buf(int i, struct ixgb_softc *sc,
d1458 2
a1459 2
	error = bus_dmamap_load(sc->rxtag, rx_buffer->map,
	    mtod(mp, void *), mp->m_len, NULL, 0);
d1467 1
a1467 2
	    rx_buffer->map->dm_mapsize,
	    BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
d1514 3
a1516 5
		if (ixgb_get_buf(i, sc, NULL) == ENOBUFS) {
			sc->rx_buffer_area[i].m_head = NULL;
			sc->rx_desc_base[i].buff_addr = 0;
			return (ENOBUFS);
		}
d1525 1
a1525 3
	sc->rxtag = NULL;
	free(sc->rx_buffer_area, M_DEVBUF);
	sc->rx_buffer_area = NULL;
d1659 12
d1672 3
a1674 2
				bus_dmamap_unload(sc->rxtag, rx_buffer->map);
				bus_dmamap_destroy(sc->rxtag, rx_buffer->map);
a1675 3
			if (rx_buffer->m_head != NULL)
				m_freem(rx_buffer->m_head);
			rx_buffer->m_head = NULL;
d1834 1
a1834 1
			ixgb_get_buf(next_to_use, sc, mp);
d1836 1
a1836 1
			if (ixgb_get_buf(next_to_use, sc, NULL) == ENOBUFS)
@


1.30
log
@move the checksum stuff under IXGB_CSUM_OFFLOAD.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.29 2006/08/18 06:02:45 brad Exp $ */
d671 1
d1403 1
d1406 1
@


1.29
log
@check hw.max_frame_size when deciding when to use m_adj().
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.28 2006/08/14 02:22:13 brad Exp $ */
d700 1
a700 1
#if 0
d702 2
a704 1
	txd_popts = 0;
d1045 4
@


1.28
log
@Use if_hardmtu and simplify the MTU ioctl handler. no-op change.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.27 2006/08/09 05:22:17 brad Exp $ */
d1441 1
a1441 1
	if (ifp->if_mtu <= ETHERMTU) {
d1443 1
a1443 1
	}
@


1.27
log
@update the version I am tracking.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.26 2006/08/09 03:48:25 brad Exp $ */
d371 1
a371 2
		if (ifr->ifr_mtu < ETHERMIN ||
		    ifr->ifr_mtu > IXGB_MAX_JUMBO_FRAME_SIZE - ETHER_HDR_LEN - ETHER_CRC_LEN) {
d373 1
a373 1
		} else if (ifp->if_mtu != ifr->ifr_mtu) {
a374 1
		}
@


1.26
log
@Use the DMA map size from the DMA map instead of the dma_size field with
bus_dma sync's.

ok dlg@@ marco@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.25 2006/08/04 14:33:02 brad Exp $ */
d47 1
a47 1
char ixgb_driver_version[] = "6.0.5";
@


1.25
log
@typo, loose -> lose
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.24 2006/08/04 14:25:24 brad Exp $ */
d739 2
a740 1
	    sc->txdma.dma_size, BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
d1365 1
a1365 1
	    sc->txdma.dma_size, BUS_DMASYNC_POSTREAD);
d1386 2
a1387 1
	    sc->txdma.dma_size, BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
d1516 2
a1517 1
	    sc->rxdma.dma_size, BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
d1706 1
a1706 1
	    sc->rxdma.dma_size, BUS_DMASYNC_POSTREAD);
d1784 2
a1785 1
		    sc->rxdma.dma_size, BUS_DMASYNC_PREREAD | BUS_DMASYNC_PREWRITE);
@


1.24
log
@- merge em/ixgb_disable_promisc() into em/ixgb_set_promisc().
- rearrange interface flags ioctl handler.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.23 2006/08/04 02:44:50 brad Exp $ */
d518 1
a518 1
	/* Don't loose promiscuous settings */
@


1.23
log
@fix up error messages in em/ixgb_allocate_pci_resources().
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.22 2006/08/01 23:50:14 brad Exp $ */
a102 1
void ixgb_disable_promisc(struct ixgb_softc *);
d381 13
a393 5
			if (!(ifp->if_flags & IFF_RUNNING))
				ixgb_init(sc);

			ixgb_disable_promisc(sc);
			ixgb_set_promisc(sc);
d398 1
a755 1
		IXGB_WRITE_REG(&sc->hw, RCTL, reg_rctl);
d759 2
a760 1
		IXGB_WRITE_REG(&sc->hw, RCTL, reg_rctl);
a761 11
}

void
ixgb_disable_promisc(struct ixgb_softc *sc)
{
	u_int32_t       reg_rctl;

	reg_rctl = IXGB_READ_REG(&sc->hw, RCTL);

	reg_rctl &= (~IXGB_RCTL_UPE);
	reg_rctl &= (~IXGB_RCTL_MPE);
a763 1

@


1.22
log
@(em/ixgb)_(clean_transmit_interrupts/process_receive_interrupts) ->
(em/ixgb)_(txeof/rxeof)
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.21 2006/07/10 00:25:23 brad Exp $ */
d944 1
a944 1
		printf(": mmba isn't memory");
d950 1
a950 1
		printf(": can't find mem space\n");
d973 1
a973 1
	return(0);
@


1.21
log
@Fully initialize the softc structure before enabling interrupt.
Copied from drahn@@'s commit to if_em.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.20 2006/06/22 04:57:28 brad Exp $ */
d90 1
a90 1
void ixgb_clean_transmit_interrupts(struct ixgb_softc *);
d93 1
a93 1
void ixgb_process_receive_interrupts(struct ixgb_softc *, int);
d558 2
a559 2
			ixgb_process_receive_interrupts(sc, -1);
			ixgb_clean_transmit_interrupts(sc);
a662 2
	if (sc->num_tx_desc_avail <= IXGB_TX_CLEANUP_THRESHOLD)
		ixgb_clean_transmit_interrupts(sc);
d664 5
a668 2
		sc->no_tx_desc_avail1++;
		return (ENOBUFS);
d1351 1
a1351 1
ixgb_clean_transmit_interrupts(struct ixgb_softc *sc)
d1687 1
a1687 1
ixgb_process_receive_interrupts(struct ixgb_softc *sc, int count)
@


1.20
log
@better media handling.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.19 2006/06/21 07:15:58 brad Exp $ */
d958 2
a971 2
	sc->hw.back = &sc->osdep;

d981 1
a981 1
	if(sc->sc_intrhand)
d985 1
a985 1
	if(sc->osdep.ixgb_membase)
@


1.19
log
@use the 10Gb SR media type for now.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.18 2006/05/28 00:04:24 jason Exp $ */
d47 1
a47 1
char ixgb_driver_version[] = "5.0.1";
d611 5
a615 1
	ifmr->ifm_active |= IFM_10G_SR | IFM_FDX;
d926 2
a927 1
		printf("%s: unsupported device id 0x%x\n", sc->sc_dv.dv_xname, sc->hw.device_id);
d1055 8
a1062 4
	ifmedia_add(&sc->media, IFM_ETHER | IFM_10G_SR | IFM_FDX,
		    0, NULL);
	ifmedia_add(&sc->media, IFM_ETHER | IFM_10G_SR,
		    0, NULL);
d1330 2
a1331 1
	TXD->cmd_type_len = htole32(IXGB_CONTEXT_DESC_CMD_TCP | IXGB_TX_DESC_CMD_RS | IXGB_CONTEXT_DESC_CMD_IDE);
@


1.18
log
@unknown ioctl is ENOTTY not EINVAL
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.17 2006/05/27 10:03:15 brad Exp $ */
d611 1
a611 1
	ifmr->ifm_active |= IFM_1000_SX | IFM_FDX;
d1050 1
a1050 1
	ifmedia_add(&sc->media, IFM_ETHER | IFM_1000_SX | IFM_FDX,
d1052 1
a1052 1
	ifmedia_add(&sc->media, IFM_ETHER | IFM_1000_SX,
@


1.17
log
@remove IFCAP_JUMBO_MTU interface capabilities flag and set if_hardmtu in a few
more drivers.

ok reyk@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.16 2006/05/26 20:50:41 deraadt Exp $ */
d415 1
a415 1
		error = EINVAL;
@


1.16
log
@rename jumbo mtu to if_hardmtu; ok brad reyk
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.15 2006/05/20 03:47:56 brad Exp $ */
d1042 1
a1042 1
	ifp->if_capabilities = IFCAP_VLAN_MTU | IFCAP_JUMBO_MTU;
@


1.15
log
@set if_jumbo_mtu and the IFCAP_JUMBO_MTU capabilities flag where
appropriate.

ok reyk@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.14 2006/05/01 21:03:42 brad Exp $ */
d1037 1
a1037 1
	ifp->if_jumbo_mtu =
@


1.14
log
@- when setting the interface address, only call ixgb_init() if the interface
is not already running.
- remove splnet usage from ixgb_intr().
- replace magic value of 32 with IXGB_MAX_SCATTER.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.13 2006/04/20 20:31:12 miod Exp $ */
d1037 2
d1042 1
a1042 1
	ifp->if_capabilities = IFCAP_VLAN_MTU;
@


1.13
log
@Fix various printf() issues: too many arguments, not enough arguments, argument
order reversed, wrong modifiers. ok deraadt@@ marco@@ mickey@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.12 2006/03/27 17:07:10 brad Exp $ */
d363 2
a364 2
		ixgb_init(sc);
		switch (ifa->ifa_addr->sa_family) {
d366 1
a366 1
		case AF_INET:
a367 1
			break;
a368 3
		default:
			break;
		}
d543 1
a543 3
	int s, claimed = 0;

	s = splnet();
a579 1
	splx(s);
d668 3
a670 2
	if (bus_dmamap_create(sc->txtag, IXGB_MAX_JUMBO_FRAME_SIZE, 32,
	    IXGB_MAX_JUMBO_FRAME_SIZE, 0, BUS_DMA_NOWAIT, &q.map)) {
@


1.12
log
@Sync up to Intel's latest FreeBSD ixgb driver (5.0.1).

From: Intel's web-site
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.11 2006/03/25 22:41:45 djm Exp $ */
d2005 1
a2005 1
	printf("ixgb%d: PCI_Bus_Speed = %s\n", unit,
@


1.11
log
@allow bpf(4) to ignore packets based on their direction (inbound or
outbound), using a new BIOCSDIRFILT ioctl;
guidance, feedback and ok canacar@@
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.10 2006/03/05 01:13:37 brad Exp $ */
d47 1
a47 1
char ixgb_driver_version[] = "1.0.26";
d129 2
a130 2
#define RXDCTL_PTHRESH_DEFAULT 128	/* chip considers prefech below this */
#define RXDCTL_HTHRESH_DEFAULT 16	/* chip will only prefetch if tail is
@


1.10
log
@tabs are holier than spaces.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.9 2006/02/26 01:27:16 brad Exp $ */
d326 1
a326 1
			bpf_mtap(ifp->if_bpf, m_head);
d1756 2
a1757 1
					bpf_mtap(ifp->if_bpf, sc->fmp);
@


1.9
log
@- set baud rate in ixgb_update_link_status().
- remove some FreeBSD code.
- set IFM_NONE if no link.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.8 2006/02/10 09:12:26 brad Exp $ */
d553 1
a553 1
	 for (;;) {
@


1.8
log
@fix a typo in em_clean_transmit_interrupts() that will cause the
watchdog timer to fire if the TX descriptor ring is emptied for
EM_TX_TIMEOUT seconds.

The same bug exists in ixgb(4) too.

From FreeSBD
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.7 2006/02/10 08:56:14 brad Exp $ */
d243 1
a352 3
	if (sc->in_detach)
		return (error);

d612 2
a613 1
	if (!sc->hw.link_up)
d615 1
d849 1
d856 1
a863 2


a1037 1
	ifp->if_baudrate = 1000000000;
@


1.7
log
@Remove clearing of the IFF_RUNNING flag from the watchdog handler as
ixgb_stop() will do this anyway.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.6 2006/02/01 20:13:49 brad Exp $ */
d1397 1
a1397 1
		else if (num_avail == sc->num_tx_desc_avail)
@


1.6
log
@remove return values in this comment too.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.5 2006/02/01 20:11:30 brad Exp $ */
a449 2

	ifp->if_flags &= ~IFF_RUNNING;
@


1.5
log
@fix comments.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.4 2006/02/01 19:07:13 otto Exp $ */
a466 1
 *  return 0 on success, positive on failure
@


1.4
log
@double semicolon; from Daniel Matic in PR 4929
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.3 2005/12/10 19:19:40 brad Exp $ */
d142 1
a142 1
 *  return 0 on success, positive on failure
a160 1
 *  return 0 on success, positive on failure
@


1.3
log
@add a shutdown function and register it with shutdownhook_establish().
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.2 2005/11/15 18:17:06 brad Exp $ */
d457 1
a457 1
	sc->watchdog_events++;;
@


1.2
log
@a little cleaning.
@
text
@d34 1
a34 1
/* $OpenBSD: if_ixgb.c,v 1.1 2005/11/14 23:25:43 brad Exp $ */
d65 2
a67 1
int  ixgb_intr(void *);
d249 1
d272 14
@


1.1
log
@add Intel 10Gb Ethernet driver with support for LR/SR and CX4 cards.

From: Intel's web-site

ok deraadt@@
@
text
@d34 1
a34 1
/* $OpenBSD$ */
a256 1

a314 1

d373 1
a373 1
			if (!(ifp->if_flags & IFF_RUNNING)) {
a374 1
			}
d379 1
a379 1
			if (ifp->if_flags & IFF_RUNNING) {
a380 1
			}
d654 1
a654 1
	if (sc->num_tx_desc_avail <= IXGB_TX_CLEANUP_THRESHOLD) {
a655 1
	}
d822 1
a822 1
	if (ixgb_display_debug_stats && ifp->if_flags & IFF_RUNNING) {
a823 1
	}
d1142 1
a1142 1
		return ENOMEM;
d1654 1
a1654 1
	if (sc->rxtag != NULL) {
a1655 1
	}
d1691 1
a1691 1
	if (!((current_desc->status) & IXGB_RX_DESC_STATUS_DD)) {
a1692 1
	}
d1715 1
a1715 1
					    IXGB_RX_DESC_ERRORS_RXE)) {
a1716 1
		}
d1994 1
a1994 1
	       buf_speed);
d2001 1
a2001 1
	       buf_type);
d2004 1
a2004 1
	       sc->no_tx_desc_avail1);
d2006 1
a2006 1
	       sc->no_tx_desc_avail2);
d2008 1
a2008 1
	       sc->mbuf_alloc_failed);
d2010 1
a2010 1
	       sc->mbuf_cluster_failed);
d2013 1
a2013 1
	       (long long)sc->stats.dc);
d2015 1
a2015 1
	       (long long)sc->stats.mpc);
d2017 1
a2017 1
	       (long long)sc->stats.rnbc);
d2019 1
a2019 1
	       (long long)sc->stats.rlec);
d2021 1
a2021 1
	       (long long)sc->stats.crcerrs);
d2023 1
a2023 1
	       sc->dropped_pkts);
d2026 1
a2026 1
	       (long long)sc->stats.xonrxc);
d2028 1
a2028 1
	       (long long)sc->stats.xontxc);
d2030 1
a2030 1
	       (long long)sc->stats.xoffrxc);
d2032 1
a2032 1
	       (long long)sc->stats.xofftxc);
d2035 1
a2035 1
	       (long long)sc->stats.gprcl);
d2037 1
a2037 1
	       (long long)sc->stats.gptcl);
d2040 1
a2040 1
	       (long long)sc->stats.jprcl);
d2042 1
a2042 1
	       (long long)sc->stats.jptcl);
@

