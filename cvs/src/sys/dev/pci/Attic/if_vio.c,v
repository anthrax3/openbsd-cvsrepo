head	1.44;
access;
symbols
	OPENBSD_6_0:1.42.0.4
	OPENBSD_6_0_BASE:1.42
	OPENBSD_5_9:1.39.0.2
	OPENBSD_5_9_BASE:1.39
	OPENBSD_5_8:1.33.0.4
	OPENBSD_5_8_BASE:1.33
	OPENBSD_5_7:1.24.0.4
	OPENBSD_5_7_BASE:1.24
	OPENBSD_5_6:1.18.0.4
	OPENBSD_5_6_BASE:1.18
	OPENBSD_5_5:1.15.0.4
	OPENBSD_5_5_BASE:1.15
	OPENBSD_5_4:1.13.0.2
	OPENBSD_5_4_BASE:1.13
	OPENBSD_5_3:1.9.0.2
	OPENBSD_5_3_BASE:1.9;
locks; strict;
comment	@ * @;


1.44
date	2017.01.21.11.47.16;	author reyk;	state dead;
branches;
next	1.43;
commitid	xPiew1FXTdpA182A;

1.43
date	2016.12.13.19.54.29;	author mikeb;	state Exp;
branches;
next	1.42;
commitid	bn6sk9febmQ7I9xO;

1.42
date	2016.07.14.12.42.00;	author sf;	state Exp;
branches;
next	1.41;
commitid	dRWcvkhmYqa5q2C1;

1.41
date	2016.04.13.10.34.32;	author mpi;	state Exp;
branches;
next	1.40;
commitid	8YSL8ByWzGeIGBiJ;

1.40
date	2016.03.15.16.45.52;	author naddy;	state Exp;
branches;
next	1.39;
commitid	X5t9omeXAp1mh2AJ;

1.39
date	2015.11.25.03.09.59;	author dlg;	state Exp;
branches;
next	1.38;
commitid	B0kwmVGiD5DVx4kv;

1.38
date	2015.11.24.17.11.39;	author mpi;	state Exp;
branches;
next	1.37;
commitid	5gdEnqVoJuTuwdTu;

1.37
date	2015.11.24.13.33.17;	author mpi;	state Exp;
branches;
next	1.36;
commitid	5DvsamK0GblTp8ww;

1.36
date	2015.11.24.12.32.53;	author mpi;	state Exp;
branches;
next	1.35;
commitid	N4FwuXZDsGAH4cQz;

1.35
date	2015.11.20.03.35.23;	author dlg;	state Exp;
branches;
next	1.34;
commitid	eYnPulzvLjDImPCa;

1.34
date	2015.10.25.13.04.28;	author mpi;	state Exp;
branches;
next	1.33;
commitid	hPF95ClMUQfeqQDX;

1.33
date	2015.06.24.09.40.54;	author mpi;	state Exp;
branches;
next	1.32;
commitid	MVWrtktB46JRxFWT;

1.32
date	2015.06.11.04.38.23;	author jsg;	state Exp;
branches;
next	1.31;
commitid	Ib0e1KxCIfibAVKC;

1.31
date	2015.05.26.19.12.24;	author sf;	state Exp;
branches;
next	1.30;
commitid	wbUA91Q2zGATZ6qY;

1.30
date	2015.04.26.12.27.29;	author sf;	state Exp;
branches;
next	1.29;
commitid	FCeWT3HoSz2owSXB;

1.29
date	2015.04.26.12.19.24;	author sf;	state Exp;
branches;
next	1.28;
commitid	t6cFxzIWxOcMxSYF;

1.28
date	2015.04.24.12.53.35;	author sf;	state Exp;
branches;
next	1.27;
commitid	d6FM42QSaQWUfHH2;

1.27
date	2015.04.18.14.38.38;	author sf;	state Exp;
branches;
next	1.26;
commitid	TtcoEJ3voZDrZB61;

1.26
date	2015.04.07.19.31.42;	author sf;	state Exp;
branches;
next	1.25;
commitid	0VO7LUU4r5O2stOy;

1.25
date	2015.03.14.03.38.48;	author jsg;	state Exp;
branches;
next	1.24;
commitid	p4LJxGKbi0BU2cG6;

1.24
date	2015.02.09.03.09.57;	author dlg;	state Exp;
branches;
next	1.23;
commitid	fE9MPAUoNdw8sZYO;

1.23
date	2015.02.09.00.27.58;	author pelikan;	state Exp;
branches;
next	1.22;
commitid	N8SnNlrukNXj8OJl;

1.22
date	2014.12.22.02.28.52;	author tedu;	state Exp;
branches;
next	1.21;
commitid	yM2VFFhpDTeFQlve;

1.21
date	2014.12.13.21.05.33;	author doug;	state Exp;
branches;
next	1.20;
commitid	20ZyHa9gTJxHxhwD;

1.20
date	2014.12.06.10.09.10;	author sf;	state Exp;
branches;
next	1.19;
commitid	eyKvJKOHiRLah8EH;

1.19
date	2014.11.29.22.53.58;	author brad;	state Exp;
branches;
next	1.18;
commitid	IzcVqr4ghufcoQnb;

1.18
date	2014.07.12.18.48.52;	author tedu;	state Exp;
branches;
next	1.17;
commitid	OBNa5kfxQ2UXoiIw;

1.17
date	2014.07.08.05.35.19;	author dlg;	state Exp;
branches;
next	1.16;
commitid	0QJleeeWqZmC5anF;

1.16
date	2014.06.17.19.46.13;	author sf;	state Exp;
branches;
next	1.15;
commitid	OsPukIOYfpYFjAbG;

1.15
date	2014.01.19.20.49.02;	author bluhm;	state Exp;
branches;
next	1.14;

1.14
date	2014.01.08.22.24.35;	author bluhm;	state Exp;
branches;
next	1.13;

1.13
date	2013.05.12.17.10.57;	author sf;	state Exp;
branches
	1.13.2.1;
next	1.12;

1.12
date	2013.03.16.19.08.37;	author sf;	state Exp;
branches;
next	1.11;

1.11
date	2013.03.15.15.44.54;	author sf;	state Exp;
branches;
next	1.10;

1.10
date	2013.03.10.21.54.46;	author sf;	state Exp;
branches;
next	1.9;

1.9
date	2012.12.05.23.20.20;	author deraadt;	state Exp;
branches
	1.9.2.1;
next	1.8;

1.8
date	2012.12.03.21.04.49;	author sf;	state Exp;
branches;
next	1.7;

1.7
date	2012.12.01.04.16.03;	author brad;	state Exp;
branches;
next	1.6;

1.6
date	2012.11.30.08.07.24;	author sf;	state Exp;
branches;
next	1.5;

1.5
date	2012.11.17.20.55.38;	author sf;	state Exp;
branches;
next	1.4;

1.4
date	2012.11.10.18.53.32;	author sf;	state Exp;
branches;
next	1.3;

1.3
date	2012.10.31.00.07.21;	author brad;	state Exp;
branches;
next	1.2;

1.2
date	2012.10.12.21.12.19;	author reyk;	state Exp;
branches;
next	1.1;

1.1
date	2012.09.19.19.24.33;	author sf;	state Exp;
branches;
next	;

1.9.2.1
date	2013.05.30.21.49.33;	author sthen;	state Exp;
branches;
next	1.9.2.2;

1.9.2.2
date	2013.05.30.21.52.12;	author sthen;	state Exp;
branches;
next	;

1.13.2.1
date	2014.09.07.19.44.29;	author sf;	state Exp;
branches;
next	;
commitid	hMmASPylStuwN4wU;


desc
@@


1.44
log
@non-PCI virtio files have been moved to sys/dev/pv
@
text
@/*	$OpenBSD: if_vio.c,v 1.43 2016/12/13 19:54:29 mikeb Exp $	*/

/*
 * Copyright (c) 2012 Stefan Fritsch, Alexander Fiveg.
 * Copyright (c) 2010 Minoura Makoto.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
 * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
 * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
 * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
 * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
 * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

#include "bpfilter.h"
#include "vlan.h"

#include <sys/param.h>
#include <sys/systm.h>
#include <sys/kernel.h>
#include <sys/device.h>
#include <sys/mbuf.h>
#include <sys/socket.h>
#include <sys/sockio.h>
#include <sys/timeout.h>

#include <dev/pci/virtioreg.h>
#include <dev/pci/virtiovar.h>

#include <net/if.h>
#include <net/if_media.h>

#include <netinet/in.h>
#include <netinet/if_ether.h>
#include <netinet/ip.h>
#include <netinet/tcp.h>
#include <netinet/udp.h>

#if NBPFILTER > 0
#include <net/bpf.h>
#endif

#if VIRTIO_DEBUG
#define DBGPRINT(fmt, args...) printf("%s: " fmt "\n", __func__, ## args)
#else
#define DBGPRINT(fmt, args...)
#endif

/*
 * if_vioreg.h:
 */
/* Configuration registers */
#define VIRTIO_NET_CONFIG_MAC		0 /* 8bit x 6byte */
#define VIRTIO_NET_CONFIG_STATUS	6 /* 16bit */

/* Feature bits */
#define VIRTIO_NET_F_CSUM		(1<<0)
#define VIRTIO_NET_F_GUEST_CSUM		(1<<1)
#define VIRTIO_NET_F_MAC		(1<<5)
#define VIRTIO_NET_F_GSO		(1<<6)
#define VIRTIO_NET_F_GUEST_TSO4		(1<<7)
#define VIRTIO_NET_F_GUEST_TSO6		(1<<8)
#define VIRTIO_NET_F_GUEST_ECN		(1<<9)
#define VIRTIO_NET_F_GUEST_UFO		(1<<10)
#define VIRTIO_NET_F_HOST_TSO4		(1<<11)
#define VIRTIO_NET_F_HOST_TSO6		(1<<12)
#define VIRTIO_NET_F_HOST_ECN		(1<<13)
#define VIRTIO_NET_F_HOST_UFO		(1<<14)
#define VIRTIO_NET_F_MRG_RXBUF		(1<<15)
#define VIRTIO_NET_F_STATUS		(1<<16)
#define VIRTIO_NET_F_CTRL_VQ		(1<<17)
#define VIRTIO_NET_F_CTRL_RX		(1<<18)
#define VIRTIO_NET_F_CTRL_VLAN		(1<<19)
#define VIRTIO_NET_F_CTRL_RX_EXTRA	(1<<20)
#define VIRTIO_NET_F_GUEST_ANNOUNCE	(1<<21)

/*
 * Config(8) flags. The lowest byte is reserved for generic virtio stuff.
 */

/* Workaround for vlan related bug in qemu < version 2.0 */
#define CONFFLAG_QEMU_VLAN_BUG		(1<<8)

static const struct virtio_feature_name virtio_net_feature_names[] = {
	{ VIRTIO_NET_F_CSUM,		"CSum" },
	{ VIRTIO_NET_F_GUEST_CSUM,	"GuestCSum" },
	{ VIRTIO_NET_F_MAC,		"MAC" },
	{ VIRTIO_NET_F_GSO,		"GSO" },
	{ VIRTIO_NET_F_GUEST_TSO4,	"GuestTSO4" },
	{ VIRTIO_NET_F_GUEST_TSO6,	"GuestTSO6" },
	{ VIRTIO_NET_F_GUEST_ECN,	"GuestECN" },
	{ VIRTIO_NET_F_GUEST_UFO,	"GuestUFO" },
	{ VIRTIO_NET_F_HOST_TSO4,	"HostTSO4" },
	{ VIRTIO_NET_F_HOST_TSO6,	"HostTSO6" },
	{ VIRTIO_NET_F_HOST_ECN, 	"HostECN" },
	{ VIRTIO_NET_F_HOST_UFO, 	"HostUFO" },
	{ VIRTIO_NET_F_MRG_RXBUF,	"MrgRXBuf" },
	{ VIRTIO_NET_F_STATUS,		"Status" },
	{ VIRTIO_NET_F_CTRL_VQ,		"CtrlVQ" },
	{ VIRTIO_NET_F_CTRL_RX,		"CtrlRX" },
	{ VIRTIO_NET_F_CTRL_VLAN,	"CtrlVLAN" },
	{ VIRTIO_NET_F_CTRL_RX_EXTRA,	"CtrlRXExtra" },
	{ VIRTIO_NET_F_GUEST_ANNOUNCE,	"GuestAnnounce" },
	{ 0, 				NULL }
};

/* Status */
#define VIRTIO_NET_S_LINK_UP	1

/* Packet header structure */
struct virtio_net_hdr {
	uint8_t		flags;
	uint8_t		gso_type;
	uint16_t	hdr_len;
	uint16_t	gso_size;
	uint16_t	csum_start;
	uint16_t	csum_offset;

	/* only present if VIRTIO_NET_F_MRG_RXBUF is negotiated */
	uint16_t	num_buffers;
} __packed;

#define VIRTIO_NET_HDR_F_NEEDS_CSUM	1 /* flags */
#define VIRTIO_NET_HDR_GSO_NONE		0 /* gso_type */
#define VIRTIO_NET_HDR_GSO_TCPV4	1 /* gso_type */
#define VIRTIO_NET_HDR_GSO_UDP		3 /* gso_type */
#define VIRTIO_NET_HDR_GSO_TCPV6	4 /* gso_type */
#define VIRTIO_NET_HDR_GSO_ECN		0x80 /* gso_type, |'ed */

#define VIRTIO_NET_MAX_GSO_LEN		(65536+ETHER_HDR_LEN)

/* Control virtqueue */
struct virtio_net_ctrl_cmd {
	uint8_t	class;
	uint8_t	command;
} __packed;
#define VIRTIO_NET_CTRL_RX		0
# define VIRTIO_NET_CTRL_RX_PROMISC	0
# define VIRTIO_NET_CTRL_RX_ALLMULTI	1

#define VIRTIO_NET_CTRL_MAC		1
# define VIRTIO_NET_CTRL_MAC_TABLE_SET	0

#define VIRTIO_NET_CTRL_VLAN		2
# define VIRTIO_NET_CTRL_VLAN_ADD	0
# define VIRTIO_NET_CTRL_VLAN_DEL	1

struct virtio_net_ctrl_status {
	uint8_t	ack;
} __packed;
#define VIRTIO_NET_OK			0
#define VIRTIO_NET_ERR			1

struct virtio_net_ctrl_rx {
	uint8_t	onoff;
} __packed;

struct virtio_net_ctrl_mac_tbl {
	uint32_t nentries;
	uint8_t macs[][ETHER_ADDR_LEN];
} __packed;

struct virtio_net_ctrl_vlan {
	uint16_t id;
} __packed;

/*
 * if_viovar.h:
 */
enum vio_ctrl_state {
	FREE, INUSE, DONE, RESET
};

struct vio_softc {
	struct device		sc_dev;

	struct virtio_softc	*sc_virtio;
#define	VQRX	0
#define	VQTX	1
#define	VQCTL	2
	struct virtqueue	sc_vq[3];

	struct arpcom		sc_ac;
	struct ifmedia		sc_media;

	short			sc_ifflags;

	/* bus_dmamem */
	bus_dma_segment_t	sc_dma_seg;
	bus_dmamap_t		sc_dma_map;
	size_t			sc_dma_size;
	caddr_t			sc_dma_kva;

	int			sc_hdr_size;
	struct virtio_net_hdr	*sc_tx_hdrs;
	struct virtio_net_ctrl_cmd *sc_ctrl_cmd;
	struct virtio_net_ctrl_status *sc_ctrl_status;
	struct virtio_net_ctrl_rx *sc_ctrl_rx;
	struct virtio_net_ctrl_mac_tbl *sc_ctrl_mac_tbl_uc;
#define sc_ctrl_mac_info sc_ctrl_mac_tbl_uc
	struct virtio_net_ctrl_mac_tbl *sc_ctrl_mac_tbl_mc;

	/* kmem */
	bus_dmamap_t		*sc_arrays;
#define sc_rx_dmamaps sc_arrays
	bus_dmamap_t		*sc_tx_dmamaps;
	struct mbuf		**sc_rx_mbufs;
	struct mbuf		**sc_tx_mbufs;
	struct if_rxring	sc_rx_ring;

	enum vio_ctrl_state	sc_ctrl_inuse;

	struct timeout		sc_txtick, sc_rxtick;
};

#define VIO_DMAMEM_OFFSET(sc, p) ((caddr_t)(p) - (sc)->sc_dma_kva)
#define VIO_DMAMEM_SYNC(vsc, sc, p, size, flags)		\
	bus_dmamap_sync((vsc)->sc_dmat, (sc)->sc_dma_map,	\
	    VIO_DMAMEM_OFFSET((sc), (p)), (size), (flags))
#define VIO_DMAMEM_ENQUEUE(sc, vq, slot, p, size, write)	\
	virtio_enqueue_p((vq), (slot), (sc)->sc_dma_map,	\
	    VIO_DMAMEM_OFFSET((sc), (p)), (size), (write))
#define VIO_HAVE_MRG_RXBUF(sc)					\
	((sc)->sc_hdr_size == sizeof(struct virtio_net_hdr))

#define VIRTIO_NET_TX_MAXNSEGS		16 /* for larger chains, defrag */
#define VIRTIO_NET_CTRL_MAC_MC_ENTRIES	64 /* for more entries, use ALLMULTI */
#define VIRTIO_NET_CTRL_MAC_UC_ENTRIES	 1 /* one entry for own unicast addr */

#define VIO_CTRL_MAC_INFO_SIZE 					\
	(2*sizeof(struct virtio_net_ctrl_mac_tbl) + 		\
	 (VIRTIO_NET_CTRL_MAC_MC_ENTRIES + 			\
	  VIRTIO_NET_CTRL_MAC_UC_ENTRIES) * ETHER_ADDR_LEN)

/* cfattach interface functions */
int	vio_match(struct device *, void *, void *);
void	vio_attach(struct device *, struct device *, void *);

/* ifnet interface functions */
int	vio_init(struct ifnet *);
void	vio_stop(struct ifnet *, int);
void	vio_start(struct ifnet *);
int	vio_ioctl(struct ifnet *, u_long, caddr_t);
void	vio_get_lladr(struct arpcom *ac, struct virtio_softc *vsc);
void	vio_put_lladr(struct arpcom *ac, struct virtio_softc *vsc);

/* rx */
int	vio_add_rx_mbuf(struct vio_softc *, int);
void	vio_free_rx_mbuf(struct vio_softc *, int);
void	vio_populate_rx_mbufs(struct vio_softc *);
int	vio_rxeof(struct vio_softc *);
int	vio_rx_intr(struct virtqueue *);
void	vio_rx_drain(struct vio_softc *);
void	vio_rxtick(void *);

/* tx */
int	vio_tx_intr(struct virtqueue *);
int	vio_txeof(struct virtqueue *);
void	vio_tx_drain(struct vio_softc *);
int	vio_encap(struct vio_softc *, int, struct mbuf *);
void	vio_txtick(void *);

/* other control */
void	vio_link_state(struct ifnet *);
int	vio_config_change(struct virtio_softc *);
int	vio_ctrl_rx(struct vio_softc *, int, int);
int	vio_set_rx_filter(struct vio_softc *);
void	vio_iff(struct vio_softc *);
int	vio_media_change(struct ifnet *);
void	vio_media_status(struct ifnet *, struct ifmediareq *);
int	vio_ctrleof(struct virtqueue *);
int	vio_wait_ctrl(struct vio_softc *sc);
int	vio_wait_ctrl_done(struct vio_softc *sc);
void	vio_ctrl_wakeup(struct vio_softc *, enum vio_ctrl_state);
int	vio_alloc_mem(struct vio_softc *);
int	vio_alloc_dmamem(struct vio_softc *);
void	vio_free_dmamem(struct vio_softc *);

#if VIRTIO_DEBUG
void	vio_dump(struct vio_softc *);
#endif

int
vio_match(struct device *parent, void *match, void *aux)
{
	struct virtio_softc *va = aux;

	if (va->sc_childdevid == PCI_PRODUCT_VIRTIO_NETWORK)
		return 1;

	return 0;
}

struct cfattach vio_ca = {
	sizeof(struct vio_softc), vio_match, vio_attach, NULL
};

struct cfdriver vio_cd = {
	NULL, "vio", DV_IFNET
};

int
vio_alloc_dmamem(struct vio_softc *sc)
{
	struct virtio_softc *vsc = sc->sc_virtio;
	int nsegs;

	if (bus_dmamap_create(vsc->sc_dmat, sc->sc_dma_size, 1,
	    sc->sc_dma_size, 0, BUS_DMA_NOWAIT | BUS_DMA_ALLOCNOW,
	    &sc->sc_dma_map) != 0)
		goto err;
	if (bus_dmamem_alloc(vsc->sc_dmat, sc->sc_dma_size, 16, 0,
	    &sc->sc_dma_seg, 1, &nsegs, BUS_DMA_NOWAIT | BUS_DMA_ZERO) != 0)
		goto destroy;
	if (bus_dmamem_map(vsc->sc_dmat, &sc->sc_dma_seg, nsegs,
	    sc->sc_dma_size, &sc->sc_dma_kva, BUS_DMA_NOWAIT) != 0)
		goto free;
	if (bus_dmamap_load(vsc->sc_dmat, sc->sc_dma_map, sc->sc_dma_kva,
	    sc->sc_dma_size, NULL, BUS_DMA_NOWAIT) != 0)
		goto unmap;
	return (0);

unmap:
	bus_dmamem_unmap(vsc->sc_dmat, sc->sc_dma_kva, sc->sc_dma_size);
free:
	bus_dmamem_free(vsc->sc_dmat, &sc->sc_dma_seg, 1);
destroy:
	bus_dmamap_destroy(vsc->sc_dmat, sc->sc_dma_map);
err:
	return (1);
}

void
vio_free_dmamem(struct vio_softc *sc)
{
	struct virtio_softc *vsc = sc->sc_virtio;
	bus_dmamap_unload(vsc->sc_dmat, sc->sc_dma_map);
	bus_dmamem_unmap(vsc->sc_dmat, sc->sc_dma_kva, sc->sc_dma_size);
	bus_dmamem_free(vsc->sc_dmat, &sc->sc_dma_seg, 1);
	bus_dmamap_destroy(vsc->sc_dmat, sc->sc_dma_map);
}

/* allocate memory */
/*
 * dma memory is used for:
 *   sc_tx_hdrs[slot]:	 metadata array for frames to be sent (WRITE)
 *   sc_ctrl_cmd:	 command to be sent via ctrl vq (WRITE)
 *   sc_ctrl_status:	 return value for a command via ctrl vq (READ)
 *   sc_ctrl_rx:	 parameter for a VIRTIO_NET_CTRL_RX class command
 *			 (WRITE)
 *   sc_ctrl_mac_tbl_uc: unicast MAC address filter for a VIRTIO_NET_CTRL_MAC
 *			 class command (WRITE)
 *   sc_ctrl_mac_tbl_mc: multicast MAC address filter for a VIRTIO_NET_CTRL_MAC
 *			 class command (WRITE)
 * sc_ctrl_* structures are allocated only one each; they are protected by
 * sc_ctrl_inuse, which must only be accessed at splnet
 *
 * metadata headers for received frames are stored at the start of the
 * rx mbufs.
 */
/*
 * dynamically allocated memory is used for:
 *   sc_rx_dmamaps[slot]:	bus_dmamap_t array for received payload
 *   sc_tx_dmamaps[slot]:	bus_dmamap_t array for sent payload
 *   sc_rx_mbufs[slot]:		mbuf pointer array for received frames
 *   sc_tx_mbufs[slot]:		mbuf pointer array for sent frames
 */
int
vio_alloc_mem(struct vio_softc *sc)
{
	struct virtio_softc *vsc = sc->sc_virtio;
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	int allocsize, r, i, txsize;
	unsigned int offset = 0;
	int rxqsize, txqsize;
	caddr_t kva;

	rxqsize = vsc->sc_vqs[0].vq_num;
	txqsize = vsc->sc_vqs[1].vq_num;

	/*
	 * For simplicity, we always allocate the full virtio_net_hdr size
	 * even if VIRTIO_NET_F_MRG_RXBUF is not negotiated and
	 * only a part of the memory is ever used.
	 */
	allocsize = sizeof(struct virtio_net_hdr) * txqsize;

	if (vsc->sc_nvqs == 3) {
		allocsize += sizeof(struct virtio_net_ctrl_cmd) * 1;
		allocsize += sizeof(struct virtio_net_ctrl_status) * 1;
		allocsize += sizeof(struct virtio_net_ctrl_rx) * 1;
		allocsize += VIO_CTRL_MAC_INFO_SIZE;
	}
	sc->sc_dma_size = allocsize;

	if (vio_alloc_dmamem(sc) != 0) {
		printf("unable to allocate dma region\n");
		return  -1;
	}

	kva = sc->sc_dma_kva;
	sc->sc_tx_hdrs = (struct virtio_net_hdr*)(kva + offset);
	offset += sizeof(struct virtio_net_hdr) * txqsize;
	if (vsc->sc_nvqs == 3) {
		sc->sc_ctrl_cmd = (void*)(kva + offset);
		offset += sizeof(*sc->sc_ctrl_cmd);
		sc->sc_ctrl_status = (void*)(kva + offset);
		offset += sizeof(*sc->sc_ctrl_status);
		sc->sc_ctrl_rx = (void*)(kva + offset);
		offset += sizeof(*sc->sc_ctrl_rx);
		sc->sc_ctrl_mac_tbl_uc = (void*)(kva + offset);
		offset += sizeof(*sc->sc_ctrl_mac_tbl_uc) +
		    ETHER_ADDR_LEN * VIRTIO_NET_CTRL_MAC_UC_ENTRIES;
		sc->sc_ctrl_mac_tbl_mc = (void*)(kva + offset);
	}

	sc->sc_arrays = mallocarray(rxqsize + txqsize,
	    2 * sizeof(bus_dmamap_t) + sizeof(struct mbuf *), M_DEVBUF,
	    M_WAITOK | M_CANFAIL | M_ZERO);
	if (sc->sc_arrays == NULL) {
		printf("unable to allocate mem for dmamaps\n");
		goto err_hdr;
	}
	allocsize = (rxqsize + txqsize) *
	    (2 * sizeof(bus_dmamap_t) + sizeof(struct mbuf *));

	sc->sc_tx_dmamaps = sc->sc_arrays + rxqsize;
	sc->sc_rx_mbufs = (void*) (sc->sc_tx_dmamaps + txqsize);
	sc->sc_tx_mbufs = sc->sc_rx_mbufs + rxqsize;

	for (i = 0; i < rxqsize; i++) {
		r = bus_dmamap_create(vsc->sc_dmat, MCLBYTES, 1, MCLBYTES, 0,
		    BUS_DMA_NOWAIT|BUS_DMA_ALLOCNOW, &sc->sc_rx_dmamaps[i]);
		if (r != 0)
			goto err_reqs;
	}

	txsize = ifp->if_hardmtu + sc->sc_hdr_size + ETHER_HDR_LEN;
	for (i = 0; i < txqsize; i++) {
		r = bus_dmamap_create(vsc->sc_dmat, txsize,
		    VIRTIO_NET_TX_MAXNSEGS, txsize, 0,
		    BUS_DMA_NOWAIT|BUS_DMA_ALLOCNOW,
		    &sc->sc_tx_dmamaps[i]);
		if (r != 0)
			goto err_reqs;
	}

	return 0;

err_reqs:
	printf("dmamap creation failed, error %d\n", r);
	for (i = 0; i < txqsize; i++) {
		if (sc->sc_tx_dmamaps[i])
			bus_dmamap_destroy(vsc->sc_dmat, sc->sc_tx_dmamaps[i]);
	}
	for (i = 0; i < rxqsize; i++) {
		if (sc->sc_tx_dmamaps[i])
			bus_dmamap_destroy(vsc->sc_dmat, sc->sc_rx_dmamaps[i]);
	}
	if (sc->sc_arrays) {
		free(sc->sc_arrays, M_DEVBUF, 0);
		sc->sc_arrays = 0;
	}
err_hdr:
	vio_free_dmamem(sc);
	return -1;
}

void
vio_get_lladr(struct arpcom *ac, struct virtio_softc *vsc)
{
	int i;
	for (i = 0; i < ETHER_ADDR_LEN; i++) {
		ac->ac_enaddr[i] = virtio_read_device_config_1(vsc,
		    VIRTIO_NET_CONFIG_MAC + i);
	}
}

void
vio_put_lladr(struct arpcom *ac, struct virtio_softc *vsc)
{
	int i;
	for (i = 0; i < ETHER_ADDR_LEN; i++) {
		virtio_write_device_config_1(vsc, VIRTIO_NET_CONFIG_MAC + i,
		     ac->ac_enaddr[i]);
	}
}

void
vio_attach(struct device *parent, struct device *self, void *aux)
{
	struct vio_softc *sc = (struct vio_softc *)self;
	struct virtio_softc *vsc = (struct virtio_softc *)parent;
	uint32_t features;
	int i;
	struct ifnet *ifp = &sc->sc_ac.ac_if;

	if (vsc->sc_child != NULL) {
		printf(": child already attached for %s; something wrong...\n",
		       parent->dv_xname);
		return;
	}

	sc->sc_virtio = vsc;

	vsc->sc_child = self;
	vsc->sc_ipl = IPL_NET;
	vsc->sc_vqs = &sc->sc_vq[0];
	vsc->sc_config_change = 0;

	features = VIRTIO_NET_F_MAC | VIRTIO_NET_F_STATUS |
	    VIRTIO_NET_F_CTRL_VQ | VIRTIO_NET_F_CTRL_RX |
	    VIRTIO_NET_F_MRG_RXBUF | VIRTIO_NET_F_CSUM;
	/*
	 * VIRTIO_F_RING_EVENT_IDX can be switched off by setting bit 2 in the
	 * driver flags, see config(8)
	 */
	if (!(sc->sc_dev.dv_cfdata->cf_flags & 2) &&
	    !(vsc->sc_dev.dv_cfdata->cf_flags & 2))
		features |= VIRTIO_F_RING_EVENT_IDX;
	else
		printf(": RingEventIdx disabled by UKC");

	features = virtio_negotiate_features(vsc, features,
	    virtio_net_feature_names);
	if (features & VIRTIO_NET_F_MAC) {
		vio_get_lladr(&sc->sc_ac, vsc);
	} else {
		ether_fakeaddr(ifp);
		vio_put_lladr(&sc->sc_ac, vsc);
	}
	printf(": address %s\n", ether_sprintf(sc->sc_ac.ac_enaddr));

	if (features & VIRTIO_NET_F_MRG_RXBUF) {
		sc->sc_hdr_size = sizeof(struct virtio_net_hdr);
		ifp->if_hardmtu = 16000; /* arbitrary limit */
	} else {
		sc->sc_hdr_size = offsetof(struct virtio_net_hdr, num_buffers);
		ifp->if_hardmtu = MCLBYTES - sc->sc_hdr_size - ETHER_HDR_LEN;
	}

	if (virtio_alloc_vq(vsc, &sc->sc_vq[VQRX], 0, MCLBYTES, 2, "rx") != 0)
		goto err;
	vsc->sc_nvqs = 1;
	sc->sc_vq[VQRX].vq_done = vio_rx_intr;
	if (virtio_alloc_vq(vsc, &sc->sc_vq[VQTX], 1,
	    sc->sc_hdr_size + ifp->if_hardmtu + ETHER_HDR_LEN,
	    VIRTIO_NET_TX_MAXNSEGS + 1, "tx") != 0) {
		goto err;
	}
	vsc->sc_nvqs = 2;
	sc->sc_vq[VQTX].vq_done = vio_tx_intr;
	virtio_start_vq_intr(vsc, &sc->sc_vq[VQRX]);
	if (features & VIRTIO_F_RING_EVENT_IDX)
		virtio_postpone_intr_far(&sc->sc_vq[VQTX]);
	else
		virtio_stop_vq_intr(vsc, &sc->sc_vq[VQTX]);
	if ((features & VIRTIO_NET_F_CTRL_VQ)
	    && (features & VIRTIO_NET_F_CTRL_RX)) {
		if (virtio_alloc_vq(vsc, &sc->sc_vq[VQCTL], 2, NBPG, 1,
		    "control") == 0) {
			sc->sc_vq[VQCTL].vq_done = vio_ctrleof;
			virtio_start_vq_intr(vsc, &sc->sc_vq[VQCTL]);
			vsc->sc_nvqs = 3;
		}
	}

	if (vio_alloc_mem(sc) < 0)
		goto err;

	strlcpy(ifp->if_xname, self->dv_xname, IFNAMSIZ);
	ifp->if_softc = sc;
	ifp->if_flags = IFF_BROADCAST | IFF_SIMPLEX | IFF_MULTICAST;
	ifp->if_start = vio_start;
	ifp->if_ioctl = vio_ioctl;
	ifp->if_capabilities = IFCAP_VLAN_MTU;
	if (features & VIRTIO_NET_F_CSUM)
		ifp->if_capabilities |= IFCAP_CSUM_TCPv4|IFCAP_CSUM_UDPv4;
	IFQ_SET_MAXLEN(&ifp->if_snd, vsc->sc_vqs[1].vq_num - 1);
	ifmedia_init(&sc->sc_media, 0, vio_media_change, vio_media_status);
	ifmedia_add(&sc->sc_media, IFM_ETHER | IFM_AUTO, 0, NULL);
	ifmedia_set(&sc->sc_media, IFM_ETHER | IFM_AUTO);
	vsc->sc_config_change = vio_config_change;
	timeout_set(&sc->sc_txtick, vio_txtick, &sc->sc_vq[VQTX]);
	timeout_set(&sc->sc_rxtick, vio_rxtick, &sc->sc_vq[VQRX]);

	if_attach(ifp);
	ether_ifattach(ifp);

	return;

err:
	for (i = 0; i < vsc->sc_nvqs; i++)
		virtio_free_vq(vsc, &sc->sc_vq[i]);
	vsc->sc_nvqs = 0;
	vsc->sc_child = VIRTIO_CHILD_ERROR;
	return;
}

/* check link status */
void
vio_link_state(struct ifnet *ifp)
{
	struct vio_softc *sc = ifp->if_softc;
	struct virtio_softc *vsc = sc->sc_virtio;
	int link_state = LINK_STATE_FULL_DUPLEX;

	if (vsc->sc_features & VIRTIO_NET_F_STATUS) {
		int status = virtio_read_device_config_2(vsc,
		    VIRTIO_NET_CONFIG_STATUS);
		if (!(status & VIRTIO_NET_S_LINK_UP))
			link_state = LINK_STATE_DOWN;
	}
	if (ifp->if_link_state != link_state) {
		ifp->if_link_state = link_state;
		if_link_state_change(ifp);
	}
}

int
vio_config_change(struct virtio_softc *vsc)
{
	struct vio_softc *sc = (struct vio_softc *)vsc->sc_child;
	vio_link_state(&sc->sc_ac.ac_if);
	return 1;
}

int
vio_media_change(struct ifnet *ifp)
{
	/* Ignore */
	return (0);
}

void
vio_media_status(struct ifnet *ifp, struct ifmediareq *imr)
{
	imr->ifm_active = IFM_ETHER | IFM_AUTO;
	imr->ifm_status = IFM_AVALID;

	vio_link_state(ifp);
	if (LINK_STATE_IS_UP(ifp->if_link_state) && ifp->if_flags & IFF_UP)
		imr->ifm_status |= IFM_ACTIVE|IFM_FDX;
}

/*
 * Interface functions for ifnet
 */
int
vio_init(struct ifnet *ifp)
{
	struct vio_softc *sc = ifp->if_softc;

	vio_stop(ifp, 0);
	if_rxr_init(&sc->sc_rx_ring, 2 * ((ifp->if_hardmtu / MCLBYTES) + 1),
	    sc->sc_vq[VQRX].vq_num);
	vio_populate_rx_mbufs(sc);
	ifp->if_flags |= IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);
	vio_iff(sc);
	vio_link_state(ifp);
	return 0;
}

void
vio_stop(struct ifnet *ifp, int disable)
{
	struct vio_softc *sc = ifp->if_softc;
	struct virtio_softc *vsc = sc->sc_virtio;

	timeout_del(&sc->sc_txtick);
	timeout_del(&sc->sc_rxtick);
	ifp->if_flags &= ~IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);
	/* only way to stop I/O and DMA is resetting... */
	virtio_reset(vsc);
	vio_rxeof(sc);
	if (vsc->sc_nvqs >= 3)
		vio_ctrleof(&sc->sc_vq[VQCTL]);
	vio_tx_drain(sc);
	if (disable)
		vio_rx_drain(sc);

	virtio_reinit_start(vsc);
	virtio_negotiate_features(vsc, vsc->sc_features, NULL);
	virtio_start_vq_intr(vsc, &sc->sc_vq[VQRX]);
	virtio_stop_vq_intr(vsc, &sc->sc_vq[VQTX]);
	if (vsc->sc_nvqs >= 3)
		virtio_start_vq_intr(vsc, &sc->sc_vq[VQCTL]);
	virtio_reinit_end(vsc);
	if (vsc->sc_nvqs >= 3) {
		if (sc->sc_ctrl_inuse != FREE)
			sc->sc_ctrl_inuse = RESET;
		wakeup(&sc->sc_ctrl_inuse);
	}
}

void
vio_start(struct ifnet *ifp)
{
	struct vio_softc *sc = ifp->if_softc;
	struct virtio_softc *vsc = sc->sc_virtio;
	struct virtqueue *vq = &sc->sc_vq[VQTX];
	struct mbuf *m;
	int queued = 0;

	vio_txeof(vq);

	if (!(ifp->if_flags & IFF_RUNNING) || ifq_is_oactive(&ifp->if_snd))
		return;
	if (IFQ_IS_EMPTY(&ifp->if_snd))
		return;

again:
	for (;;) {
		int slot, r;
		struct virtio_net_hdr *hdr;

		m = ifq_deq_begin(&ifp->if_snd);
		if (m == NULL)
			break;

		r = virtio_enqueue_prep(vq, &slot);
		if (r == EAGAIN) {
			ifq_deq_rollback(&ifp->if_snd, m);
			ifq_set_oactive(&ifp->if_snd);
			break;
		}
		if (r != 0)
			panic("enqueue_prep for a tx buffer: %d", r);

		hdr = &sc->sc_tx_hdrs[slot];
		memset(hdr, 0, sc->sc_hdr_size);
		if (m->m_pkthdr.csum_flags & (M_TCP_CSUM_OUT|M_UDP_CSUM_OUT)) {
			struct mbuf *mip;
			struct ip *ip;
			int ehdrlen = ETHER_HDR_LEN;
			int ipoff;
#if NVLAN > 0
			struct ether_vlan_header *eh;

			eh = mtod(m, struct ether_vlan_header *);
			if (eh->evl_encap_proto == htons(ETHERTYPE_VLAN))
				ehdrlen += ETHER_VLAN_ENCAP_LEN;
#endif

			if (m->m_pkthdr.csum_flags & M_TCP_CSUM_OUT)
				hdr->csum_offset = offsetof(struct tcphdr, th_sum);
			else
				hdr->csum_offset = offsetof(struct udphdr, uh_sum);

			mip = m_getptr(m, ehdrlen, &ipoff);
			KASSERT(mip != NULL && mip->m_len - ipoff >= sizeof(*ip));
			ip = (struct ip *)(mip->m_data + ipoff);
			hdr->csum_start = ehdrlen + (ip->ip_hl << 2);
			hdr->flags = VIRTIO_NET_HDR_F_NEEDS_CSUM;
		}

		r = vio_encap(sc, slot, m);
		if (r != 0) {
			virtio_enqueue_abort(vq, slot);
			ifq_deq_commit(&ifp->if_snd, m);
			m_freem(m);
			ifp->if_oerrors++;
			continue;
		}
		r = virtio_enqueue_reserve(vq, slot,
		    sc->sc_tx_dmamaps[slot]->dm_nsegs + 1);
		if (r != 0) {
			bus_dmamap_unload(vsc->sc_dmat,
			    sc->sc_tx_dmamaps[slot]);
			ifq_deq_rollback(&ifp->if_snd, m);
			sc->sc_tx_mbufs[slot] = NULL;
			ifq_set_oactive(&ifp->if_snd);
			break;
		}
		ifq_deq_commit(&ifp->if_snd, m);

		bus_dmamap_sync(vsc->sc_dmat, sc->sc_tx_dmamaps[slot], 0,
		    sc->sc_tx_dmamaps[slot]->dm_mapsize, BUS_DMASYNC_PREWRITE);
		VIO_DMAMEM_SYNC(vsc, sc, hdr, sc->sc_hdr_size,
		    BUS_DMASYNC_PREWRITE);
		VIO_DMAMEM_ENQUEUE(sc, vq, slot, hdr, sc->sc_hdr_size, 1);
		virtio_enqueue(vq, slot, sc->sc_tx_dmamaps[slot], 1);
		virtio_enqueue_commit(vsc, vq, slot, 0);
		queued++;
#if NBPFILTER > 0
		if (ifp->if_bpf)
			bpf_mtap(ifp->if_bpf, m, BPF_DIRECTION_OUT);
#endif
	}
	if (ifq_is_oactive(&ifp->if_snd)) {
		int r;
		if (vsc->sc_features & VIRTIO_F_RING_EVENT_IDX)
			r = virtio_postpone_intr_smart(&sc->sc_vq[VQTX]);
		else
			r = virtio_start_vq_intr(vsc, &sc->sc_vq[VQTX]);
		if (r) {
			vio_txeof(vq);
			goto again;
		}
	}

	if (queued > 0) {
		virtio_notify(vsc, vq);
		timeout_add_sec(&sc->sc_txtick, 1);
	}
}

#if VIRTIO_DEBUG
void
vio_dump(struct vio_softc *sc)
{
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	struct virtio_softc *vsc = sc->sc_virtio;

	printf("%s status dump:\n", ifp->if_xname);
	printf("TX virtqueue:\n");
	virtio_vq_dump(&vsc->sc_vqs[VQTX]);
	printf("tx tick active: %d\n", !timeout_triggered(&sc->sc_txtick));
	printf("rx tick active: %d\n", !timeout_triggered(&sc->sc_rxtick));
	printf("RX virtqueue:\n");
	virtio_vq_dump(&vsc->sc_vqs[VQRX]);
	if (vsc->sc_nvqs == 3) {
		printf("CTL virtqueue:\n");
		virtio_vq_dump(&vsc->sc_vqs[VQCTL]);
		printf("ctrl_inuse: %d\n", sc->sc_ctrl_inuse);
	}
}
#endif

int
vio_ioctl(struct ifnet *ifp, u_long cmd, caddr_t data)
{
	struct vio_softc *sc = ifp->if_softc;
	struct ifreq *ifr = (struct ifreq *)data;
	int s, r = 0;

	s = splnet();
	switch (cmd) {
	case SIOCSIFADDR:
		ifp->if_flags |= IFF_UP;
		if (!(ifp->if_flags & IFF_RUNNING))
			vio_init(ifp);
		break;
	case SIOCSIFFLAGS:
		if (ifp->if_flags & IFF_UP) {
#if VIRTIO_DEBUG
			if (ifp->if_flags & IFF_DEBUG)
				vio_dump(sc);
#endif
			if (ifp->if_flags & IFF_RUNNING)
				r = ENETRESET;
			else
				vio_init(ifp);
		} else {
			if (ifp->if_flags & IFF_RUNNING)
				vio_stop(ifp, 1);
		}
		break;
	case SIOCGIFMEDIA:
	case SIOCSIFMEDIA:
		r = ifmedia_ioctl(ifp, ifr, &sc->sc_media, cmd);
		break;
	case SIOCGIFRXR:
		r = if_rxr_ioctl((struct if_rxrinfo *)ifr->ifr_data,
		    NULL, MCLBYTES, &sc->sc_rx_ring);
		break;
	default:
		r = ether_ioctl(ifp, &sc->sc_ac, cmd, data);
	}

	if (r == ENETRESET) {
		if (ifp->if_flags & IFF_RUNNING)
			vio_iff(sc);
		r = 0;
	}
	splx(s);
	return r;
}

/*
 * Recieve implementation
 */
/* allocate and initialize a mbuf for receive */
int
vio_add_rx_mbuf(struct vio_softc *sc, int i)
{
	struct mbuf *m;
	int r;

	m = MCLGETI(NULL, M_DONTWAIT, NULL, MCLBYTES);
	if (m == NULL)
		return ENOBUFS;
	sc->sc_rx_mbufs[i] = m;
	m->m_len = m->m_pkthdr.len = m->m_ext.ext_size;
	r = bus_dmamap_load_mbuf(sc->sc_virtio->sc_dmat, sc->sc_rx_dmamaps[i],
	    m, BUS_DMA_READ|BUS_DMA_NOWAIT);
	if (r) {
		m_freem(m);
		sc->sc_rx_mbufs[i] = 0;
		return r;
	}

	return 0;
}

/* free a mbuf for receive */
void
vio_free_rx_mbuf(struct vio_softc *sc, int i)
{
	bus_dmamap_unload(sc->sc_virtio->sc_dmat, sc->sc_rx_dmamaps[i]);
	m_freem(sc->sc_rx_mbufs[i]);
	sc->sc_rx_mbufs[i] = NULL;
}

/* add mbufs for all the empty receive slots */
void
vio_populate_rx_mbufs(struct vio_softc *sc)
{
	struct virtio_softc *vsc = sc->sc_virtio;
	int r, done = 0;
	u_int slots;
	struct virtqueue *vq = &sc->sc_vq[VQRX];
	int mrg_rxbuf = VIO_HAVE_MRG_RXBUF(sc);

	for (slots = if_rxr_get(&sc->sc_rx_ring, vq->vq_num);
	    slots > 0; slots--) {
		int slot;
		r = virtio_enqueue_prep(vq, &slot);
		if (r == EAGAIN)
			break;
		if (r != 0)
			panic("enqueue_prep for rx buffers: %d", r);
		if (sc->sc_rx_mbufs[slot] == NULL) {
			r = vio_add_rx_mbuf(sc, slot);
			if (r != 0) {
				virtio_enqueue_abort(vq, slot);
				break;
			}
		}
		r = virtio_enqueue_reserve(vq, slot,
		    sc->sc_rx_dmamaps[slot]->dm_nsegs + (mrg_rxbuf ? 0 : 1));
		if (r != 0) {
			vio_free_rx_mbuf(sc, slot);
			break;
		}
		bus_dmamap_sync(vsc->sc_dmat, sc->sc_rx_dmamaps[slot], 0,
		    MCLBYTES, BUS_DMASYNC_PREREAD);
		if (mrg_rxbuf) {
			virtio_enqueue(vq, slot, sc->sc_rx_dmamaps[slot], 0);
		} else {
			/*
			 * Buggy kvm wants a buffer of exactly the size of
			 * the header in this case, so we have to split in
			 * two.
			 */
			virtio_enqueue_p(vq, slot, sc->sc_rx_dmamaps[slot],
			    0, sc->sc_hdr_size, 0);
			virtio_enqueue_p(vq, slot, sc->sc_rx_dmamaps[slot],
			    sc->sc_hdr_size, MCLBYTES - sc->sc_hdr_size, 0);
		}
		virtio_enqueue_commit(vsc, vq, slot, 0);
		done = 1;
	}
	if_rxr_put(&sc->sc_rx_ring, slots);

	if (done)
		virtio_notify(vsc, vq);
	if (vq->vq_used_idx != vq->vq_avail_idx)
		timeout_del(&sc->sc_rxtick);
	else
		timeout_add_sec(&sc->sc_rxtick, 1);
}

/* dequeue received packets */
int
vio_rxeof(struct vio_softc *sc)
{
	struct virtio_softc *vsc = sc->sc_virtio;
	struct virtqueue *vq = &sc->sc_vq[VQRX];
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	struct mbuf_list ml = MBUF_LIST_INITIALIZER();
	struct mbuf *m, *m0 = NULL, *mlast;
	int r = 0;
	int slot, len, bufs_left;
	struct virtio_net_hdr *hdr;

	while (virtio_dequeue(vsc, vq, &slot, &len) == 0) {
		r = 1;
		bus_dmamap_sync(vsc->sc_dmat, sc->sc_rx_dmamaps[slot], 0,
		    MCLBYTES, BUS_DMASYNC_POSTREAD);
		m = sc->sc_rx_mbufs[slot];
		KASSERT(m != NULL);
		bus_dmamap_unload(vsc->sc_dmat, sc->sc_rx_dmamaps[slot]);
		sc->sc_rx_mbufs[slot] = NULL;
		virtio_dequeue_commit(vq, slot);
		if_rxr_put(&sc->sc_rx_ring, 1);
		m->m_len = m->m_pkthdr.len = len;
		m->m_pkthdr.csum_flags = 0;
		if (m0 == NULL) {
			hdr = mtod(m, struct virtio_net_hdr *);
			m_adj(m, sc->sc_hdr_size);
			m0 = mlast = m;
			if (VIO_HAVE_MRG_RXBUF(sc))
				bufs_left = hdr->num_buffers - 1;
			else
				bufs_left = 0;
		}
		else {
			m->m_flags &= ~M_PKTHDR;
			m0->m_pkthdr.len += m->m_len;
			mlast->m_next = m;
			mlast = m;
			bufs_left--;
		}

		if (bufs_left == 0) {
			ml_enqueue(&ml, m0);
			m0 = NULL;
		}
	}
	if (m0 != NULL) {
		DBGPRINT("expected %d buffers, got %d", (int)hdr->num_buffers,
		    (int)hdr->num_buffers - bufs_left);
		ifp->if_ierrors++;
		m_freem(m0);
	}

	if_input(ifp, &ml);
	return r;
}

int
vio_rx_intr(struct virtqueue *vq)
{
	struct virtio_softc *vsc = vq->vq_owner;
	struct vio_softc *sc = (struct vio_softc *)vsc->sc_child;
	int r, sum = 0;

again:
	r = vio_rxeof(sc);
	sum += r;
	if (r) {
		vio_populate_rx_mbufs(sc);
		/* set used event index to the next slot */
		if (vsc->sc_features & VIRTIO_F_RING_EVENT_IDX) {
			if (virtio_start_vq_intr(vq->vq_owner, vq))
				goto again;
		}
	}

	return sum;
}

void
vio_rxtick(void *arg)
{
	struct virtqueue *vq = arg;
	struct virtio_softc *vsc = vq->vq_owner;
	struct vio_softc *sc = (struct vio_softc *)vsc->sc_child;
	int s;

	s = splnet();
	vio_populate_rx_mbufs(sc);
	splx(s);
}

/* free all the mbufs; called from if_stop(disable) */
void
vio_rx_drain(struct vio_softc *sc)
{
	struct virtqueue *vq = &sc->sc_vq[VQRX];
	int i;

	for (i = 0; i < vq->vq_num; i++) {
		if (sc->sc_rx_mbufs[i] == NULL)
			continue;
		vio_free_rx_mbuf(sc, i);
	}
}

/*
 * Transmition implementation
 */
/* actual transmission is done in if_start */
/* tx interrupt; dequeue and free mbufs */
/*
 * tx interrupt is actually disabled unless the tx queue is full, i.e.
 * IFF_OACTIVE is set. vio_txtick is used to make sure that mbufs
 * are dequeued and freed even if no further transfer happens.
 */
int
vio_tx_intr(struct virtqueue *vq)
{
	struct virtio_softc *vsc = vq->vq_owner;
	struct vio_softc *sc = (struct vio_softc *)vsc->sc_child;
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	int r;

	r = vio_txeof(vq);
	vio_start(ifp);
	return r;
}

void
vio_txtick(void *arg)
{
	struct virtqueue *vq = arg;
	int s = splnet();
	vio_tx_intr(vq);
	splx(s);
}

int
vio_txeof(struct virtqueue *vq)
{
	struct virtio_softc *vsc = vq->vq_owner;
	struct vio_softc *sc = (struct vio_softc *)vsc->sc_child;
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	struct mbuf *m;
	int r = 0;
	int slot, len;

	while (virtio_dequeue(vsc, vq, &slot, &len) == 0) {
		struct virtio_net_hdr *hdr = &sc->sc_tx_hdrs[slot];
		r++;
		VIO_DMAMEM_SYNC(vsc, sc, hdr, sc->sc_hdr_size,
		    BUS_DMASYNC_POSTWRITE);
		bus_dmamap_sync(vsc->sc_dmat, sc->sc_tx_dmamaps[slot], 0,
		    sc->sc_tx_dmamaps[slot]->dm_mapsize,
		    BUS_DMASYNC_POSTWRITE);
		m = sc->sc_tx_mbufs[slot];
		bus_dmamap_unload(vsc->sc_dmat, sc->sc_tx_dmamaps[slot]);
		sc->sc_tx_mbufs[slot] = 0;
		virtio_dequeue_commit(vq, slot);
		ifp->if_opackets++;
		m_freem(m);
	}

	if (r) {
		ifq_clr_oactive(&ifp->if_snd);
		virtio_stop_vq_intr(vsc, &sc->sc_vq[VQTX]);
	}
	if (vq->vq_used_idx == vq->vq_avail_idx)
		timeout_del(&sc->sc_txtick);
	else if (r)
		timeout_add_sec(&sc->sc_txtick, 1);
	return r;
}

int
vio_encap(struct vio_softc *sc, int slot, struct mbuf *m)
{
	struct virtio_softc	*vsc = sc->sc_virtio;
	bus_dmamap_t		 dmap= sc->sc_tx_dmamaps[slot];
	int			 r;

	r = bus_dmamap_load_mbuf(vsc->sc_dmat, dmap, m,
	    BUS_DMA_WRITE|BUS_DMA_NOWAIT);
	switch (r) {
	case 0:
		break;
	case EFBIG:
		if (m_defrag(m, M_DONTWAIT) == 0 &&
		    bus_dmamap_load_mbuf(vsc->sc_dmat, dmap, m,
		    BUS_DMA_WRITE|BUS_DMA_NOWAIT) == 0)
			break;

		/* FALLTHROUGH */
	default:
		return ENOBUFS;
	}
	sc->sc_tx_mbufs[slot] = m;
	return 0;
}

/* free all the mbufs already put on vq; called from if_stop(disable) */
void
vio_tx_drain(struct vio_softc *sc)
{
	struct virtio_softc *vsc = sc->sc_virtio;
	struct virtqueue *vq = &sc->sc_vq[VQTX];
	int i;

	for (i = 0; i < vq->vq_num; i++) {
		if (sc->sc_tx_mbufs[i] == NULL)
			continue;
		bus_dmamap_unload(vsc->sc_dmat, sc->sc_tx_dmamaps[i]);
		m_freem(sc->sc_tx_mbufs[i]);
		sc->sc_tx_mbufs[i] = NULL;
	}
}

/*
 * Control vq
 */
/* issue a VIRTIO_NET_CTRL_RX class command and wait for completion */
int
vio_ctrl_rx(struct vio_softc *sc, int cmd, int onoff)
{
	struct virtio_softc *vsc = sc->sc_virtio;
	struct virtqueue *vq = &sc->sc_vq[VQCTL];
	int r, slot;

	splassert(IPL_NET);

	if ((r = vio_wait_ctrl(sc)) != 0)
		return r;

	sc->sc_ctrl_cmd->class = VIRTIO_NET_CTRL_RX;
	sc->sc_ctrl_cmd->command = cmd;
	sc->sc_ctrl_rx->onoff = onoff;

	VIO_DMAMEM_SYNC(vsc, sc, sc->sc_ctrl_cmd,
	    sizeof(*sc->sc_ctrl_cmd), BUS_DMASYNC_PREWRITE);
	VIO_DMAMEM_SYNC(vsc, sc, sc->sc_ctrl_rx,
	    sizeof(*sc->sc_ctrl_rx), BUS_DMASYNC_PREWRITE);
	VIO_DMAMEM_SYNC(vsc, sc, sc->sc_ctrl_status,
	    sizeof(*sc->sc_ctrl_status), BUS_DMASYNC_PREREAD);

	r = virtio_enqueue_prep(vq, &slot);
	if (r != 0)
		panic("%s: control vq busy!?", sc->sc_dev.dv_xname);
	r = virtio_enqueue_reserve(vq, slot, 3);
	if (r != 0)
		panic("%s: control vq busy!?", sc->sc_dev.dv_xname);
	VIO_DMAMEM_ENQUEUE(sc, vq, slot, sc->sc_ctrl_cmd,
	    sizeof(*sc->sc_ctrl_cmd), 1);
	VIO_DMAMEM_ENQUEUE(sc, vq, slot, sc->sc_ctrl_rx,
	    sizeof(*sc->sc_ctrl_rx), 1);
	VIO_DMAMEM_ENQUEUE(sc, vq, slot, sc->sc_ctrl_status,
	    sizeof(*sc->sc_ctrl_status), 0);
	virtio_enqueue_commit(vsc, vq, slot, 1);

	if ((r = vio_wait_ctrl_done(sc)) != 0)
		goto out;

	VIO_DMAMEM_SYNC(vsc, sc, sc->sc_ctrl_cmd,
	    sizeof(*sc->sc_ctrl_cmd), BUS_DMASYNC_POSTWRITE);
	VIO_DMAMEM_SYNC(vsc, sc, sc->sc_ctrl_rx,
	    sizeof(*sc->sc_ctrl_rx), BUS_DMASYNC_POSTWRITE);
	VIO_DMAMEM_SYNC(vsc, sc, sc->sc_ctrl_status,
	    sizeof(*sc->sc_ctrl_status), BUS_DMASYNC_POSTREAD);

	if (sc->sc_ctrl_status->ack == VIRTIO_NET_OK) {
		r = 0;
	} else {
		printf("%s: ctrl cmd %d failed\n", sc->sc_dev.dv_xname, cmd);
		r = EIO;
	}

	DBGPRINT("cmd %d %d: %d", cmd, (int)onoff, r);
out:
	vio_ctrl_wakeup(sc, FREE);
	return r;
}

int
vio_wait_ctrl(struct vio_softc *sc)
{
	int r = 0;

	while (sc->sc_ctrl_inuse != FREE) {
		r = tsleep(&sc->sc_ctrl_inuse, PRIBIO|PCATCH, "viowait", 0);
		if (r == EINTR)
			return r;
	}
	sc->sc_ctrl_inuse = INUSE;

	return r;
}

int
vio_wait_ctrl_done(struct vio_softc *sc)
{
	int r = 0;

	while (sc->sc_ctrl_inuse != DONE && sc->sc_ctrl_inuse != RESET) {
		if (sc->sc_ctrl_inuse == RESET) {
			r = 1;
			break;
		}
		r = tsleep(&sc->sc_ctrl_inuse, PRIBIO|PCATCH, "viodone", 0);
		if (r == EINTR)
			break;
	}
	return r;
}

void
vio_ctrl_wakeup(struct vio_softc *sc, enum vio_ctrl_state new)
{
	sc->sc_ctrl_inuse = new;
	wakeup(&sc->sc_ctrl_inuse);
}

int
vio_ctrleof(struct virtqueue *vq)
{
	struct virtio_softc *vsc = vq->vq_owner;
	struct vio_softc *sc = (struct vio_softc *)vsc->sc_child;
	int r = 0, ret, slot;

again:
	ret = virtio_dequeue(vsc, vq, &slot, NULL);
	if (ret == ENOENT)
		return r;
	virtio_dequeue_commit(vq, slot);
	r++;
	vio_ctrl_wakeup(sc, DONE);
	if (virtio_start_vq_intr(vsc, vq))
		goto again;

	return r;
}

/* issue VIRTIO_NET_CTRL_MAC_TABLE_SET command and wait for completion */
int
vio_set_rx_filter(struct vio_softc *sc)
{
	/* filter already set in sc_ctrl_mac_tbl */
	struct virtio_softc *vsc = sc->sc_virtio;
	struct virtqueue *vq = &sc->sc_vq[VQCTL];
	int r, slot;

	splassert(IPL_NET);

	if ((r = vio_wait_ctrl(sc)) != 0)
		return r;

	sc->sc_ctrl_cmd->class = VIRTIO_NET_CTRL_MAC;
	sc->sc_ctrl_cmd->command = VIRTIO_NET_CTRL_MAC_TABLE_SET;

	VIO_DMAMEM_SYNC(vsc, sc, sc->sc_ctrl_cmd,
	    sizeof(*sc->sc_ctrl_cmd), BUS_DMASYNC_PREWRITE);
	VIO_DMAMEM_SYNC(vsc, sc, sc->sc_ctrl_mac_info,
	    VIO_CTRL_MAC_INFO_SIZE, BUS_DMASYNC_PREWRITE);
	VIO_DMAMEM_SYNC(vsc, sc, sc->sc_ctrl_status,
	    sizeof(*sc->sc_ctrl_status), BUS_DMASYNC_PREREAD);

	r = virtio_enqueue_prep(vq, &slot);
	if (r != 0)
		panic("%s: control vq busy!?", sc->sc_dev.dv_xname);
	r = virtio_enqueue_reserve(vq, slot, 4);
	if (r != 0)
		panic("%s: control vq busy!?", sc->sc_dev.dv_xname);
	VIO_DMAMEM_ENQUEUE(sc, vq, slot, sc->sc_ctrl_cmd,
	    sizeof(*sc->sc_ctrl_cmd), 1);
	VIO_DMAMEM_ENQUEUE(sc, vq, slot, sc->sc_ctrl_mac_tbl_uc,
	    sizeof(*sc->sc_ctrl_mac_tbl_uc) +
	    sc->sc_ctrl_mac_tbl_uc->nentries * ETHER_ADDR_LEN, 1);
	VIO_DMAMEM_ENQUEUE(sc, vq, slot, sc->sc_ctrl_mac_tbl_mc,
	    sizeof(*sc->sc_ctrl_mac_tbl_mc) +
	    sc->sc_ctrl_mac_tbl_mc->nentries * ETHER_ADDR_LEN, 1);
	VIO_DMAMEM_ENQUEUE(sc, vq, slot, sc->sc_ctrl_status,
	    sizeof(*sc->sc_ctrl_status), 0);
	virtio_enqueue_commit(vsc, vq, slot, 1);

	if ((r = vio_wait_ctrl_done(sc)) != 0)
		goto out;

	VIO_DMAMEM_SYNC(vsc, sc, sc->sc_ctrl_cmd,
	    sizeof(*sc->sc_ctrl_cmd), BUS_DMASYNC_POSTWRITE);
	VIO_DMAMEM_SYNC(vsc, sc, sc->sc_ctrl_mac_info,
	    VIO_CTRL_MAC_INFO_SIZE, BUS_DMASYNC_POSTWRITE);
	VIO_DMAMEM_SYNC(vsc, sc, sc->sc_ctrl_status,
	    sizeof(*sc->sc_ctrl_status), BUS_DMASYNC_POSTREAD);

	if (sc->sc_ctrl_status->ack == VIRTIO_NET_OK) {
		r = 0;
	} else {
		/* The host's filter table is not large enough */
		printf("%s: failed setting rx filter\n", sc->sc_dev.dv_xname);
		r = EIO;
	}

out:
	vio_ctrl_wakeup(sc, FREE);
	return r;
}

void
vio_iff(struct vio_softc *sc)
{
	struct virtio_softc *vsc = sc->sc_virtio;
	struct ifnet *ifp = &sc->sc_ac.ac_if;
	struct arpcom *ac = &sc->sc_ac;
	struct ether_multi *enm;
	struct ether_multistep step;
	int nentries = 0;
	int promisc = 0, allmulti = 0, rxfilter = 0;
	int r;

	splassert(IPL_NET);

	ifp->if_flags &= ~IFF_ALLMULTI;

	if (vsc->sc_nvqs < 3) {
		/* no ctrl vq; always promisc */
		ifp->if_flags |= IFF_ALLMULTI | IFF_PROMISC;
		return;
	}

	if (sc->sc_dev.dv_cfdata->cf_flags & CONFFLAG_QEMU_VLAN_BUG)
		ifp->if_flags |= IFF_PROMISC;

	if (ifp->if_flags & IFF_PROMISC || ac->ac_multirangecnt > 0 ||
	    ac->ac_multicnt >= VIRTIO_NET_CTRL_MAC_MC_ENTRIES) {
		ifp->if_flags |= IFF_ALLMULTI;
		if (ifp->if_flags & IFF_PROMISC)
			promisc = 1;
		else
			allmulti = 1;
	} else {
		rxfilter = 1;

		ETHER_FIRST_MULTI(step, ac, enm);
		while (enm != NULL) {
			memcpy(sc->sc_ctrl_mac_tbl_mc->macs[nentries++],
			    enm->enm_addrlo, ETHER_ADDR_LEN);

			ETHER_NEXT_MULTI(step, enm);
		}
	}

	/* set unicast address, VirtualBox wants that */
	memcpy(sc->sc_ctrl_mac_tbl_uc->macs[0], ac->ac_enaddr, ETHER_ADDR_LEN);
	sc->sc_ctrl_mac_tbl_uc->nentries = 1;

	sc->sc_ctrl_mac_tbl_mc->nentries = rxfilter ? nentries : 0;

	if (vsc->sc_nvqs < 3)
		return;

	r = vio_set_rx_filter(sc);
	if (r == EIO)
		allmulti = 1; /* fallback */
	else if (r != 0)
		return;

	r = vio_ctrl_rx(sc, VIRTIO_NET_CTRL_RX_ALLMULTI, allmulti);
	if (r == EIO)
		promisc = 1; /* fallback */
	else if (r != 0)
		return;

	vio_ctrl_rx(sc, VIRTIO_NET_CTRL_RX_PROMISC, promisc);
}
@


1.43
log
@Fix up tsleep priorities and make them interruptible

With help from and OK sf, OK mpi on the previous version.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.42 2016/07/14 12:42:00 sf Exp $	*/
@


1.42
log
@virtio: Move interrupt handler into transport specific code

For MSI-X (and also possibly for other transports), the interrupt
handler must do different things.  Move it out of virtio.c and into
virtio_pci.

ARM part tested by patrick@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.41 2016/04/13 10:34:32 mpi Exp $	*/
d286 1
a286 1
void	vio_wait_ctrl(struct vio_softc *sc);
d1220 1
a1220 2
	if (vsc->sc_nvqs < 3)
		return ENOTSUP;
d1222 2
a1223 2
	splassert(IPL_NET);
	vio_wait_ctrl(sc);
d1250 1
a1250 2
	if (vio_wait_ctrl_done(sc)) {
		r = EIO;
a1251 1
	}
d1273 1
a1273 1
void
d1276 7
a1282 2
	while (sc->sc_ctrl_inuse != FREE)
		tsleep(&sc->sc_ctrl_inuse, IPL_NET, "vio_wait", 0);
d1284 2
d1292 1
d1298 3
a1300 1
		tsleep(&sc->sc_ctrl_inuse, IPL_NET, "vio_wait", 0);
d1343 2
a1344 4
	if (vsc->sc_nvqs < 3)
		return ENOTSUP;

	vio_wait_ctrl(sc);
d1374 1
a1374 2
	if (vio_wait_ctrl_done(sc)) {
		r = EIO;
a1375 1
	}
d1387 1
d1445 10
a1454 11
	if (rxfilter) {
		sc->sc_ctrl_mac_tbl_mc->nentries = nentries;
		r = vio_set_rx_filter(sc);
		if (r != 0) {
			rxfilter = 0;
			allmulti = 1; /* fallback */
		}
	} else {
		sc->sc_ctrl_mac_tbl_mc->nentries = 0;
		vio_set_rx_filter(sc);
	}
d1456 5
a1460 9
	if (allmulti) {
		r = vio_ctrl_rx(sc, VIRTIO_NET_CTRL_RX_ALLMULTI, 1);
		if (r != 0) {
			allmulti = 0;
			promisc = 1; /* fallback */
		}
	} else {
		vio_ctrl_rx(sc, VIRTIO_NET_CTRL_RX_ALLMULTI, 0);
	}
@


1.41
log
@G/C IFQ_SET_READY().
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.40 2016/03/15 16:45:52 naddy Exp $	*/
a523 1
	vsc->sc_intrhand = virtio_vq_intr;
@


1.40
log
@Ethernet drivers no longer need to include if_vlan_var.h for the VLAN
definitions; ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.39 2015/11/25 03:09:59 dlg Exp $	*/
a594 1
	IFQ_SET_READY(&ifp->if_snd);
@


1.39
log
@replace IFF_OACTIVE manipulation with mpsafe operations.

there are two things shared between the network stack and drivers
in the send path: the send queue and the IFF_OACTIVE flag. the send
queue is now protected by a mutex. this diff makes the oactive
functionality mpsafe too.

IFF_OACTIVE is part of if_flags. there are two problems with that.
firstly, if_flags is a short and we dont have any MI atomic operations
to manipulate a short. secondly, while we could make the IFF_OACTIVE
operates mpsafe, all changes to other flags would have to be made
safe at the same time, otherwise a read-modify-write cycle on their
updates could clobber the oactive change.

instead, this moves the oactive mark into struct ifqueue and provides
an API for changing it. there's ifq_set_oactive, ifq_clr_oactive,
and ifq_is_oactive. these are modelled on ifsq_set_oactive,
ifsq_clr_oactive, and ifsq_is_oactive in dragonflybsd.

this diff includes changes to all the drivers manipulating IFF_OACTIVE
to now use the ifsq_{set,clr_is}_oactive API too.

ok kettenis@@ mpi@@ jmatthew@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.38 2015/11/24 17:11:39 mpi Exp $	*/
a51 4

#if NVLAN > 0
#include <net/if_vlan_var.h>
#endif
@


1.38
log
@You only need <net/if_dl.h> if you're using LLADDR() or a sockaddr_dl.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.37 2015/11/24 13:33:17 mpi Exp $	*/
d679 1
a679 1
	ifp->if_flags &= ~IFF_OACTIVE;
d693 2
a694 1
	ifp->if_flags &= ~(IFF_RUNNING | IFF_OACTIVE);
d729 1
a729 1
	if ((ifp->if_flags & (IFF_RUNNING|IFF_OACTIVE)) != IFF_RUNNING)
d746 1
a746 1
			ifp->if_flags |= IFF_OACTIVE;
d794 1
a794 1
			ifp->if_flags |= IFF_OACTIVE;
d812 1
a812 1
	if (ifp->if_flags & IFF_OACTIVE) {
d1162 1
a1162 1
		ifp->if_flags &= ~IFF_OACTIVE;
@


1.37
log
@The only network driver needing <net/if_types.h> is upl(4) for IFT_OTHER.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.36 2015/11/24 12:32:53 mpi Exp $	*/
a44 1
#include <net/if_dl.h>
@


1.36
log
@No need to include <net/if_types.h> for <net/if_vlan_var.h>
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.35 2015/11/20 03:35:23 dlg Exp $	*/
a46 1
#include <net/if_types.h>
@


1.35
log
@shuffle struct ifqueue so in flight mbufs are protected by a mutex.

the code is refactored so the IFQ macros call newly implemented ifq
functions. the ifq code is split so each discipline (priq and hfsc
in our case) is an opaque set of operations that the common ifq
code can call. the common code does the locking, accounting (ifq_len
manipulation), and freeing of the mbuf if the disciplines enqueue
function rejects it. theyre kind of like bufqs in the block layer
with their fifo and nscan disciplines.

the new api also supports atomic switching of disciplines at runtime.
the hfsc setup in pf_ioctl.c has been tweaked to build a complete
hfsc_if structure which it attaches to the send queue in a single
operation, rather than attaching to the interface up front and
building up a list of queues.

the send queue is now mutexed, which raises the expectation that
packets can be enqueued or purged on one cpu while another cpu is
dequeueing them in a driver for transmission. a lot of drivers use
IFQ_POLL to peek at an mbuf and attempt to fit it on the ring before
committing to it with a later IFQ_DEQUEUE operation. if the mbuf
gets freed in between the POLL and DEQUEUE operations, fireworks
will ensue.

to avoid this, the ifq api introduces ifq_deq_begin, ifq_deq_rollback,
and ifq_deq_commit. ifq_deq_begin allows a driver to take the ifq
mutex and get a reference to the mbuf they wish to try and tx. if
there's space, they can ifq_deq_commit it to remove the mbuf and
release the mutex. if there's no space, ifq_deq_rollback simply
releases the mutex. this api was developed to make updating the
drivers using IFQ_POLL easy, instead of having to do significant
semantic changes to avoid POLL that we cannot test on all the
hardware.

the common code has been tested pretty hard, and all the driver
modifications are straightforward except for de(4). if that breaks
it can be dealt with later.

ok mpi@@ jmatthew@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.34 2015/10/25 13:04:28 mpi Exp $	*/
a55 1
#include <net/if_types.h>
@


1.34
log
@arp_ifinit() is no longer needed.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.33 2015/06/24 09:40:54 mpi Exp $	*/
d741 1
a741 1
		IFQ_POLL(&ifp->if_snd, m);
d747 1
d784 1
a784 1
			IFQ_DEQUEUE(&ifp->if_snd, m);
d794 1
d799 1
a799 1
		IFQ_DEQUEUE(&ifp->if_snd, m);
@


1.33
log
@Increment if_ipackets in if_input().

Note that pseudo-drivers not using if_input() are not affected by this
conversion.

ok mikeb@@, kettenis@@, claudio@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.32 2015/06/11 04:38:23 jsg Exp $	*/
a855 1
	struct ifaddr *ifa = (struct ifaddr *)data;
a864 2
		if (ifa->ifa_addr->sa_family == AF_INET)
			arp_ifinit(&sc->sc_ac, ifa);
@


1.32
log
@remove uneeded pci includes
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.31 2015/05/26 19:12:24 sf Exp $	*/
a1041 1
			ifp->if_ipackets++;
@


1.31
log
@Fix missing vlan.h include in if_vio.c

patch by jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.30 2015/04/26 12:27:29 sf Exp $	*/
a40 2
#include <dev/pci/pcireg.h>
#include <dev/pci/pcivar.h>
@


1.30
log
@Have vio_start() check if the queue is empty.

from brad@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.29 2015/04/26 12:19:24 sf Exp $	*/
d30 1
d56 5
@


1.29
log
@vio: Support checksum offloading for IPv4 TX

"Looks good to me" brad@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.28 2015/04/24 12:53:35 sf Exp $	*/
d729 2
d1122 1
a1122 2
	if (!IFQ_IS_EMPTY(&ifp->if_snd))
		vio_start(ifp);
@


1.28
log
@vio: If enqueue fails, drop packet.

From brad@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.27 2015/04/18 14:38:38 sf Exp $	*/
d52 3
d531 1
a531 1
	    VIRTIO_NET_F_MRG_RXBUF;
d595 2
d746 28
a792 2
		hdr = &sc->sc_tx_hdrs[slot];
		memset(hdr, 0, sc->sc_hdr_size);
@


1.27
log
@Simplify vio_encap() a bit.

From brad@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.26 2015/04/07 19:31:42 sf Exp $	*/
a742 4
#if VIRTIO_DEBUG
			if (r != ENOBUFS)
				printf("%s: error %d\n", __func__, r);
#endif
d744 4
a747 2
			ifp->if_flags |= IFF_OACTIVE;
			break;
@


1.26
log
@Use m_defrag when mbuf chains get fragmented.

Patch by Kimberley Manning <kmanning at gmx ! com> with some
additional tweaks.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.25 2015/03/14 03:38:48 jsg Exp $	*/
d1155 3
a1157 3
		if ((r = m_defrag(m, M_DONTWAIT)) == 0 &&
		    (r = bus_dmamap_load_mbuf(vsc->sc_dmat, dmap, m,
		     BUS_DMA_WRITE|BUS_DMA_NOWAIT)) == 0)
a1161 2
		printf("%s: tx dmamap load error %d\n", sc->sc_dev.dv_xname,
		    r);
d1165 1
a1165 1
	return r;
@


1.25
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.24 2015/02/09 03:09:57 dlg Exp $	*/
d274 1
a274 1
int	vio_encap(struct vio_softc *, int, struct mbuf *, struct mbuf **);
d741 1
a741 1
		r = vio_encap(sc, slot, m, &sc->sc_tx_mbufs[slot]);
a760 4
		if (m != sc->sc_tx_mbufs[slot]) {
			m_freem(m);
			m = sc->sc_tx_mbufs[slot];
		}
d1143 1
a1143 2
vio_encap(struct vio_softc *sc, int slot, struct mbuf *m,
	      struct mbuf **mnew)
a1146 1
	struct mbuf		*m0 = NULL;
d1151 11
a1161 23
	if (r == 0) {
		*mnew = m;
		return r;
	}
	if (r != EFBIG)
		return r;
	/* EFBIG: mbuf chain is too fragmented */
	MGETHDR(m0, M_DONTWAIT, MT_DATA);
	if (m0 == NULL)
		return ENOBUFS;
	if (m->m_pkthdr.len > MHLEN) {
		MCLGETI(m0, M_DONTWAIT, NULL, m->m_pkthdr.len);
		if (!(m0->m_flags & M_EXT)) {
			m_freem(m0);
			return ENOBUFS;
		}
	}
	m_copydata(m, 0, m->m_pkthdr.len, mtod(m0, caddr_t));
	m0->m_pkthdr.len = m0->m_len = m->m_pkthdr.len;
	r = bus_dmamap_load_mbuf(vsc->sc_dmat, dmap, m0,
	    BUS_DMA_NOWAIT|BUS_DMA_WRITE);
	if (r != 0) {
		m_freem(m0);
d1166 2
a1167 2
	*mnew = m0;
	return 0;
@


1.24
log
@tweak the new if_input function so it takes an mbuf_list instead
of a single mbuf. this forces us to batch work between the hardware
rx handlers and the stack.

this includes a converstion of bge from ether_input to if_input.

ok claudio@@ pelikan@@ mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.23 2015/02/09 00:27:58 pelikan Exp $	*/
a39 1
#include <dev/pci/pcidevs.h>
@


1.23
log
@convert vio(4) to if_input().

ok mpi
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.22 2014/12/22 02:28:52 tedu Exp $	*/
d976 1
d1013 1
a1013 1
			if_input(ifp, m0);
d1023 2
@


1.22
log
@unifdef INET
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.21 2014/12/13 21:05:33 doug Exp $	*/
a990 1
		m->m_pkthdr.rcvif = ifp;
d1012 1
a1012 5
#if NBPFILTER > 0
			if (ifp->if_bpf)
				bpf_mtap(ifp->if_bpf, m0, BPF_DIRECTION_IN);
#endif
			ether_input_mbuf(ifp, m0);
@


1.21
log
@yet more mallocarray() changes.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.20 2014/12/06 10:09:10 sf Exp $	*/
a50 1
#ifdef INET
a52 1
#endif
a835 1
#ifdef INET
a837 1
#endif
@


1.20
log
@Increase low rxr watermark to a value suitable for jumbo frames.

Patch provided by brad@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.19 2014/11/29 22:53:58 brad Exp $	*/
d434 3
a436 3
	allocsize = (rxqsize + txqsize) *
	    (2 * sizeof(bus_dmamap_t) + sizeof(struct mbuf *));
	sc->sc_arrays = malloc(allocsize, M_DEVBUF, M_WAITOK|M_CANFAIL|M_ZERO);
d441 2
@


1.19
log
@rxr ioctl handling.

ok sf@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.18 2014/07/12 18:48:52 tedu Exp $	*/
d670 2
a671 1
	if_rxr_init(&sc->sc_rx_ring, 4, sc->sc_vq[VQRX].vq_num);
@


1.18
log
@add a size argument to free. will be used soon, but for now default to 0.
after discussions with beck deraadt kettenis.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.17 2014/07/08 05:35:19 dlg Exp $	*/
d825 2
a827 1
	struct ifaddr *ifa = (struct ifaddr *)data;
d857 5
a861 2
		r = ifmedia_ioctl(ifp, (struct ifreq *)data, &sc->sc_media,
		    cmd);
@


1.17
log
@cut things that relied on mclgeti for rx ring accounting/restriction over
to using if_rxr.

cut the reporting systat did over to the rxr ioctl.

tested as much as i can on alpha, amd64, and sparc64.
mpi@@ has run it on macppc.
ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.16 2014/06/17 19:46:13 sf Exp $	*/
d476 1
a476 1
		free(sc->sc_arrays, M_DEVBUF);
@


1.16
log
@Add a configurable workaround for a bug in qemu < 2.0 that prevented VLANs
from working.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.15 2014/01/19 20:49:02 bluhm Exp $	*/
d226 1
a598 1
	m_clsetwms(ifp, MCLBYTES, 4, sc->sc_vq[VQRX].vq_num);
d670 1
d882 1
a882 1
	m = MCLGETI(NULL, M_DONTWAIT, &sc->sc_ac.ac_if, MCLBYTES);
d912 2
a913 1
	int i, r, ndone = 0;
d917 2
a918 1
	for (i = 0; i < vq->vq_num; i++) {
d954 1
a954 1
		ndone++;
d956 3
a958 1
	if (ndone > 0)
d987 1
@


1.15
log
@If the system runs out ouf mbufs, the receive queue of the vio
interface may get emtpy.  The driver did not recover from that
starvation as it does not get receive interrupts anymore.  Fix this
by adding a one second receive timeout in that case.
OK sf@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.14 2014/01/08 22:24:35 bluhm Exp $	*/
d94 7
d1404 3
@


1.14
log
@Fix typo recieve -> receive.
OK sf@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.13 2013/05/12 17:10:57 sf Exp $	*/
d222 1
a222 1
	struct timeout		sc_tick;
d263 1
d592 2
a593 1
	timeout_set(&sc->sc_tick, vio_txtick, &sc->sc_vq[VQTX]);
d677 2
a678 1
	timeout_del(&sc->sc_tick);
d787 1
a787 1
		timeout_add_sec(&sc->sc_tick, 1);
d801 2
a802 1
	printf("tx tick active: %d\n", !timeout_triggered(&sc->sc_tick));
d948 4
d1036 13
d1127 1
a1127 1
		timeout_del(&sc->sc_tick);
d1129 1
a1129 1
		timeout_add_sec(&sc->sc_tick, 1);
@


1.13
log
@fix use after free in case the mbuf needs defragmentation

This fixes a panic found by Matthieu Herrb.

OK mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.12 2013/03/16 19:08:37 sf Exp $	*/
d371 1
a371 1
 *   sc_rx_dmamaps[slot]:	bus_dmamap_t array for recieved payload
d373 1
a373 1
 *   sc_rx_mbufs[slot]:		mbuf pointer array for recieved frames
d863 1
a863 1
/* allocate and initialize a mbuf for recieve */
d886 1
a886 1
/* free a mbuf for recieve */
d895 1
a895 1
/* add mbufs for all the empty recieve slots */
d946 1
a946 1
/* dequeue recieved packets */
@


1.13.2.1
log
@Backport from current revision 1.15:
  date: 2014/01/19 20:49:02;  author: bluhm;  state: Exp;  lines: +29 -8;

If the system runs out ouf mbufs, the receive queue of the vio
interface may get emtpy.  The driver did not recover from that
starvation as it does not get receive interrupts anymore.  Fix this
by adding a one second receive timeout in that case.
OK sf@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.13 2013/05/12 17:10:57 sf Exp $	*/
d222 1
a222 1
	struct timeout		sc_txtick, sc_rxtick;
a262 1
void	vio_rxtick(void *);
d591 1
a591 2
	timeout_set(&sc->sc_txtick, vio_txtick, &sc->sc_vq[VQTX]);
	timeout_set(&sc->sc_rxtick, vio_rxtick, &sc->sc_vq[VQRX]);
d675 1
a675 2
	timeout_del(&sc->sc_txtick);
	timeout_del(&sc->sc_rxtick);
d784 1
a784 1
		timeout_add_sec(&sc->sc_txtick, 1);
d798 1
a798 2
	printf("tx tick active: %d\n", !timeout_triggered(&sc->sc_txtick));
	printf("rx tick active: %d\n", !timeout_triggered(&sc->sc_rxtick));
a943 4
	if (vq->vq_used_idx != vq->vq_avail_idx)
		timeout_del(&sc->sc_rxtick);
	else
		timeout_add_sec(&sc->sc_rxtick, 1);
a1027 13
void
vio_rxtick(void *arg)
{
	struct virtqueue *vq = arg;
	struct virtio_softc *vsc = vq->vq_owner;
	struct vio_softc *sc = (struct vio_softc *)vsc->sc_child;
	int s;

	s = splnet();
	vio_populate_rx_mbufs(sc);
	splx(s);
}

d1106 1
a1106 1
		timeout_del(&sc->sc_txtick);
d1108 1
a1108 1
		timeout_add_sec(&sc->sc_txtick, 1);
@


1.12
log
@fix size of unicast rx filter table

    When changing the unicast rx filter table from zero to one entry, the length
    has not been properly adjusted. This lead to the unicast address overwriting
    part of the multicast rx filter table.

OK mikeb@@
tested by weerd@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.11 2013/03/15 15:44:54 sf Exp $	*/
d750 4
a1149 1
	m_freem(m);
@


1.11
log
@add some space before messages
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.10 2013/03/10 21:54:46 sf Exp $	*/
d236 2
a237 1
#define VIRTIO_NET_CTRL_MAC_MAXENTRIES	64 /* for more entries, use ALLMULTI */
d241 2
a242 1
	 (VIRTIO_NET_CTRL_MAC_MAXENTRIES + 1) * ETHER_ADDR_LEN)
d400 1
a400 3
		allocsize += sizeof(struct virtio_net_ctrl_mac_tbl)
			+ sizeof(struct virtio_net_ctrl_mac_tbl)
			+ ETHER_ADDR_LEN * VIRTIO_NET_CTRL_MAC_MAXENTRIES;
d420 2
a421 1
		offset += sizeof(*sc->sc_ctrl_mac_tbl_uc);
d1375 1
a1375 1
	    ac->ac_multicnt >= VIRTIO_NET_CTRL_MAC_MAXENTRIES) {
@


1.10
log
@if_vio: Add some more debug output if VIRTIO_DEBUG is defined
        Add GuestCSum feature string
virtio: Make some printfs depend on VIRTIO_DEBUG

OK jasper@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.9 2012/12/05 23:20:20 deraadt Exp $	*/
d504 1
a504 1
		printf("child already attached for %s; something wrong...\n",
d528 1
a528 1
		printf("RingEventIdx disabled by UKC\n");
@


1.9
log
@Remove excessive sys/cdefs.h inclusion
ok guenther millert kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.8 2012/12/03 21:04:49 sf Exp $	*/
d96 1
d285 3
d783 21
d824 4
@


1.9.2.1
log
@MFC r1.12, req by brad@@

date: 2013/03/16 19:08:37;  author: sf;  state: Exp;  lines: +9 -8
fix size of unicast rx filter table

    When changing the unicast rx filter table from zero to one entry, the length
    has not been properly adjusted. This lead to the unicast address overwriting
    part of the multicast rx filter table.

OK mikeb@@
tested by weerd@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.9 2012/12/05 23:20:20 deraadt Exp $	*/
d235 1
a235 2
#define VIRTIO_NET_CTRL_MAC_MC_ENTRIES	64 /* for more entries, use ALLMULTI */
#define VIRTIO_NET_CTRL_MAC_UC_ENTRIES	 1 /* one entry for own unicast addr */
d239 1
a239 2
	 (VIRTIO_NET_CTRL_MAC_MC_ENTRIES + 			\
	  VIRTIO_NET_CTRL_MAC_UC_ENTRIES) * ETHER_ADDR_LEN)
d394 3
a396 1
		allocsize += VIO_CTRL_MAC_INFO_SIZE;
d416 1
a416 2
		offset += sizeof(*sc->sc_ctrl_mac_tbl_uc) +
		    ETHER_ADDR_LEN * VIRTIO_NET_CTRL_MAC_UC_ENTRIES;
d1345 1
a1345 1
	    ac->ac_multicnt >= VIRTIO_NET_CTRL_MAC_MC_ENTRIES) {
@


1.9.2.2
log
@MFC if_vio.c r1.13, req by brad@@

date: 2013/05/12 17:10:57;  author: sf;  state: Exp;  lines: +5 -2
fix use after free in case the mbuf needs defragmentation

This fixes a panic found by Matthieu Herrb.

OK mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.9.2.1 2013/05/30 21:49:33 sthen Exp $	*/
a745 4
		if (m != sc->sc_tx_mbufs[slot]) {
			m_freem(m);
			m = sc->sc_tx_mbufs[slot];
		}
d1117 1
@


1.8
log
@wrap bpf.h header with NBPFILTER

ok gsoares@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.7 2012/12/01 04:16:03 brad Exp $	*/
a28 1
#include <sys/cdefs.h>
@


1.7
log
@The return value of vio_link_state() is not used so change the return
type from int to void.

ok sf@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.6 2012/11/30 08:07:24 sf Exp $	*/
d57 1
d59 1
@


1.6
log
@Support large MTUs

Add support for the VIRTIO_NET_F_MRG_RXBUF feature, i.e. allow to
chain several rx buffers when receiving large packets.
This requires to put the rx meta data headers at the beginning of
the mbuf cluster instead of dedicated buffers.

ok mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.5 2012/11/17 20:55:38 sf Exp $	*/
d268 1
a268 1
int	vio_link_state(struct ifnet *);
d601 1
a601 1
int
a617 1
	return 0;
@


1.5
log
@Simplify vio_iff() a bit, set own MAC in unicast MAC filter (needed by
virtualbox).

Original patch by Dinar Talypov t.dinar.m AT gmail.com
Tweaks by brad@@ and sf@@

OK brad@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.4 2012/11/10 18:53:32 sf Exp $	*/
d126 3
a128 3
#if 0
	uint16_t	num_buffers; /* if VIRTIO_NET_F_MRG_RXBUF enabled */
#endif
d202 1
a202 1
	struct virtio_net_hdr	*sc_rx_hdrs;
d230 2
a346 1
 *   sc_rx_hdrs[slot]:	 metadata array for recieved frames (READ)
d358 3
d373 2
a374 1
	int allocsize, r, i;
d382 7
a388 2
	allocsize = sizeof(struct virtio_net_hdr) * rxqsize;
	allocsize += sizeof(struct virtio_net_hdr) * txqsize;
a404 2
	sc->sc_rx_hdrs = (struct virtio_net_hdr*)kva;
	offset += sizeof(struct virtio_net_hdr) * rxqsize;
d438 1
d440 2
a441 2
		r = bus_dmamap_create(vsc->sc_dmat, ETHER_MAX_LEN,
		    VIRTIO_NET_TX_MAXNSEGS, ETHER_MAX_LEN, 0,
d513 2
a514 1
	    VIRTIO_NET_F_CTRL_VQ | VIRTIO_NET_F_CTRL_RX;
d535 9
a543 2
	if (virtio_alloc_vq(vsc, &sc->sc_vq[VQRX], 0,
	    MCLBYTES + sizeof(struct virtio_net_hdr), 2, "rx") != 0) {
a544 1
	}
d548 1
a548 1
	    (sizeof(struct virtio_net_hdr) + (ETHER_MAX_LEN - ETHER_HDR_LEN)),
d727 4
d747 1
a747 1
		memset(hdr, 0, sizeof(*hdr));
d750 1
a750 1
		VIO_DMAMEM_SYNC(vsc, sc, hdr, sizeof(*hdr),
d752 1
a752 1
		VIO_DMAMEM_ENQUEUE(sc, vq, slot, hdr, sizeof(*hdr), 1);
d868 1
a871 1
		struct virtio_net_hdr *hdr;
d885 1
a885 1
		    sc->sc_rx_dmamaps[slot]->dm_nsegs + 1);
a889 3
		hdr = &sc->sc_rx_hdrs[slot];
		VIO_DMAMEM_SYNC(vsc, sc, hdr, sizeof(*hdr),
		    BUS_DMASYNC_PREREAD);
d892 13
a904 2
		VIO_DMAMEM_ENQUEUE(sc, vq, slot, hdr, sizeof(*hdr), 0);
		virtio_enqueue(vq, slot, sc->sc_rx_dmamaps[slot], 0);
d919 1
a919 1
	struct mbuf *m;
d921 2
a922 1
	int slot, len;
a924 2
		struct virtio_net_hdr *hdr =  &sc->sc_rx_hdrs[slot];
		len -= sizeof(struct virtio_net_hdr);
a925 2
		VIO_DMAMEM_SYNC(vsc, sc, hdr, sizeof(*hdr),
		    BUS_DMASYNC_POSTREAD);
d931 1
a931 1
		sc->sc_rx_mbufs[slot] = 0;
d936 19
a954 1
		ifp->if_ipackets++;
d956 2
a957 2
		if (ifp->if_bpf)
			bpf_mtap(ifp->if_bpf, m, BPF_DIRECTION_IN);
d959 9
a967 1
		ether_input_mbuf(ifp, m);
d1054 1
a1054 1
		VIO_DMAMEM_SYNC(vsc, sc, hdr, sizeof(*hdr),
@


1.4
log
@Simplify error handling, add IFCAP_VLAN_MTU

Patch by Dinar Talypov t.dinar.m AT gmail.com
OK claudio@@, mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.3 2012/10/31 00:07:21 brad Exp $	*/
a233 1
/* for now, sc_ctrl_mac_tbl_uc has always 0 entries */
d236 1
a236 1
	 0 + VIRTIO_NET_CTRL_MAC_MAXENTRIES * ETHER_ADDR_LEN)
d270 1
a270 1
int	vio_iff(struct vio_softc *);
a407 1
		/* For now, sc_ctrl_mac_tbl_uc is followed by 0 MAC entries */
d1271 1
a1271 10
/*
 * If IFF_PROMISC requested,  set promiscuous
 * If multicast filter small enough (<=MAXENTRIES) set rx filter
 * If large multicast filter exist use ALLMULTI
 */
/*
 * If setting rx filter fails fall back to ALLMULTI
 * If ALLMULTI fails fall back to PROMISC
 */
int
d1290 1
a1290 1
		return 0;
d1303 1
a1303 1
		ETHER_FIRST_MULTI(step, &sc->sc_ac, enm);
d1312 4
a1316 1
		sc->sc_ctrl_mac_tbl_uc->nentries = 0;
a1323 2
		/* remove rx filter */
		sc->sc_ctrl_mac_tbl_uc->nentries = 0;
d1325 1
a1325 2
		r = vio_set_rx_filter(sc);
		/* what to do on failure? */
d1335 1
a1335 2
		r = vio_ctrl_rx(sc, VIRTIO_NET_CTRL_RX_ALLMULTI, 0);
		/* what to do on failure? */
d1338 1
a1338 1
	return vio_ctrl_rx(sc, VIRTIO_NET_CTRL_RX_PROMISC, promisc);
@


1.3
log
@Some fixes for the receive filter handling.

ok sf@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_vio.c,v 1.2 2012/10/12 21:12:19 reyk Exp $	*/
d488 1
d563 1
a563 1
	ifp->if_capabilities = 0;
d579 3
a581 12
	if (vsc->sc_nvqs == 3) {
		virtio_free_vq(vsc, &sc->sc_vq[2]);
		vsc->sc_nvqs = 2;
	}
	if (vsc->sc_nvqs == 2) {
		virtio_free_vq(vsc, &sc->sc_vq[1]);
		vsc->sc_nvqs = 1;
	}
	if (vsc->sc_nvqs == 1) {
		virtio_free_vq(vsc, &sc->sc_vq[0]);
		vsc->sc_nvqs = 0;
	}
@


1.2
log
@Add $OpenBSD$ CVS Ids.

ok sf@@
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d1295 1
d1304 2
d1308 1
a1308 1
		ifp->if_flags |= IFF_PROMISC;
d1312 14
a1325 4
	if (ifp->if_flags & IFF_PROMISC) {
		promisc = 1;
		goto set;
	}
d1327 1
a1327 5
	ETHER_FIRST_MULTI(step, &sc->sc_ac, enm);
	while (enm != NULL) {
		if (nentries >= VIRTIO_NET_CTRL_MAC_MAXENTRIES) {
			allmulti = 1;
			goto set;
a1328 8
		if (memcmp(enm->enm_addrlo, enm->enm_addrhi, ETHER_ADDR_LEN)) {
			allmulti = 1;
			goto set;
		}
		memcpy(sc->sc_ctrl_mac_tbl_mc->macs[nentries], enm->enm_addrlo,
		    ETHER_ADDR_LEN);
		ETHER_NEXT_MULTI(step, enm);
		nentries++;
a1329 1
	rxfilter = 1;
a1330 1
set:
d1346 1
@


1.1
log
@Add new drivers for virtio network (vio) and block devices (vioblk, the disks
attach as scsi disks).  These are paravirtualized devices offered by some
hypervisors like kvm and virtualbox.

The virtio transport driver has the pci specific parts separated out. This
will make it easier to add support for mmio (e.g. for ARM) later.

OK mikeb
OK jasper
"commit what you have" deraadt
@
text
@d1 2
@

