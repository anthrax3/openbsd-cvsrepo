head	1.50;
access;
symbols
	OPENBSD_6_2:1.50.0.2
	OPENBSD_6_2_BASE:1.50
	OPENBSD_6_1:1.49.0.4
	OPENBSD_6_1_BASE:1.49
	OPENBSD_6_0:1.47.0.4
	OPENBSD_6_0_BASE:1.47
	OPENBSD_5_9:1.45.0.2
	OPENBSD_5_9_BASE:1.45
	OPENBSD_5_8:1.41.0.4
	OPENBSD_5_8_BASE:1.41
	OPENBSD_5_7:1.37.0.4
	OPENBSD_5_7_BASE:1.37
	OPENBSD_5_6:1.36.0.4
	OPENBSD_5_6_BASE:1.36
	OPENBSD_5_5:1.35.0.4
	OPENBSD_5_5_BASE:1.35
	OPENBSD_5_4:1.29.0.4
	OPENBSD_5_4_BASE:1.29
	OPENBSD_5_3:1.29.0.2
	OPENBSD_5_3_BASE:1.29
	OPENBSD_5_2:1.27.0.2
	OPENBSD_5_2_BASE:1.27
	OPENBSD_5_1_BASE:1.26
	OPENBSD_5_1:1.26.0.2
	OPENBSD_5_0:1.25.0.2
	OPENBSD_5_0_BASE:1.25
	OPENBSD_4_9:1.24.0.2
	OPENBSD_4_9_BASE:1.24
	OPENBSD_4_8:1.23.0.2
	OPENBSD_4_8_BASE:1.23
	OPENBSD_4_7:1.21.0.2
	OPENBSD_4_7_BASE:1.21
	OPENBSD_4_6:1.19.0.4
	OPENBSD_4_6_BASE:1.19
	OPENBSD_4_5:1.17.0.2
	OPENBSD_4_5_BASE:1.17;
locks; strict;
comment	@ * @;


1.50
date	2017.09.08.05.36.52;	author deraadt;	state Exp;
branches;
next	1.49;
commitid	uRv5pa9QDlZaYgwD;

1.49
date	2017.01.22.10.17.38;	author dlg;	state Exp;
branches;
next	1.48;
commitid	VyLWTsbepAOk7VQM;

1.48
date	2016.11.29.10.22.30;	author jsg;	state Exp;
branches;
next	1.47;
commitid	ZQetSMB5ilG2z10X;

1.47
date	2016.04.13.10.34.32;	author mpi;	state Exp;
branches;
next	1.46;
commitid	8YSL8ByWzGeIGBiJ;

1.46
date	2016.03.15.16.45.52;	author naddy;	state Exp;
branches;
next	1.45;
commitid	X5t9omeXAp1mh2AJ;

1.45
date	2015.11.25.03.09.59;	author dlg;	state Exp;
branches;
next	1.44;
commitid	B0kwmVGiD5DVx4kv;

1.44
date	2015.11.24.12.32.53;	author mpi;	state Exp;
branches;
next	1.43;
commitid	N4FwuXZDsGAH4cQz;

1.43
date	2015.11.09.00.29.06;	author dlg;	state Exp;
branches;
next	1.42;
commitid	rcVfmI5b2gosTZgD;

1.42
date	2015.10.25.13.04.28;	author mpi;	state Exp;
branches;
next	1.41;
commitid	hPF95ClMUQfeqQDX;

1.41
date	2015.06.24.09.40.54;	author mpi;	state Exp;
branches;
next	1.40;
commitid	MVWrtktB46JRxFWT;

1.40
date	2015.04.30.07.52.00;	author mpi;	state Exp;
branches;
next	1.39;
commitid	pTRq8Jb3T5WeNn6X;

1.39
date	2015.04.10.16.04.47;	author mpi;	state Exp;
branches;
next	1.38;
commitid	pTztwgJyi8Q84Yik;

1.38
date	2015.03.14.03.38.48;	author jsg;	state Exp;
branches;
next	1.37;
commitid	p4LJxGKbi0BU2cG6;

1.37
date	2014.12.22.02.28.52;	author tedu;	state Exp;
branches;
next	1.36;
commitid	yM2VFFhpDTeFQlve;

1.36
date	2014.07.22.13.12.11;	author mpi;	state Exp;
branches;
next	1.35;
commitid	TGHgrLxu6sxZoiFt;

1.35
date	2014.01.27.12.04.46;	author brad;	state Exp;
branches;
next	1.34;

1.34
date	2014.01.10.22.01.30;	author brad;	state Exp;
branches;
next	1.33;

1.33
date	2013.12.07.07.22.37;	author brad;	state Exp;
branches;
next	1.32;

1.32
date	2013.11.03.23.27.33;	author brad;	state Exp;
branches;
next	1.31;

1.31
date	2013.11.03.04.24.28;	author brad;	state Exp;
branches;
next	1.30;

1.30
date	2013.08.07.01.06.35;	author bluhm;	state Exp;
branches;
next	1.29;

1.29
date	2012.11.29.21.10.32;	author brad;	state Exp;
branches;
next	1.28;

1.28
date	2012.10.22.09.19.17;	author brad;	state Exp;
branches;
next	1.27;

1.27
date	2012.02.28.03.58.16;	author jsg;	state Exp;
branches;
next	1.26;

1.26
date	2012.02.08.13.16.59;	author jsg;	state Exp;
branches;
next	1.25;

1.25
date	2011.04.05.18.01.21;	author henning;	state Exp;
branches;
next	1.24;

1.24
date	2010.08.27.17.08.00;	author jsg;	state Exp;
branches;
next	1.23;

1.23
date	2010.08.07.03.50.02;	author krw;	state Exp;
branches;
next	1.22;

1.22
date	2010.05.19.15.27.35;	author oga;	state Exp;
branches;
next	1.21;

1.21
date	2010.01.07.12.26.06;	author sthen;	state Exp;
branches;
next	1.20;

1.20
date	2009.09.13.14.42.52;	author krw;	state Exp;
branches;
next	1.19;

1.19
date	2009.06.05.06.05.06;	author naddy;	state Exp;
branches;
next	1.18;

1.18
date	2009.03.29.21.53.52;	author sthen;	state Exp;
branches;
next	1.17;

1.17
date	2009.02.25.13.10.38;	author jsg;	state Exp;
branches;
next	1.16;

1.16
date	2009.02.25.11.41.58;	author jsg;	state Exp;
branches;
next	1.15;

1.15
date	2009.01.10.15.33.05;	author kevlo;	state Exp;
branches;
next	1.14;

1.14
date	2008.12.01.09.12.59;	author jsg;	state Exp;
branches;
next	1.13;

1.13
date	2008.11.09.15.08.26;	author naddy;	state Exp;
branches;
next	1.12;

1.12
date	2008.10.29.01.55.53;	author brad;	state Exp;
branches;
next	1.11;

1.11
date	2008.10.21.19.41.13;	author brad;	state Exp;
branches;
next	1.10;

1.10
date	2008.10.21.19.39.43;	author brad;	state Exp;
branches;
next	1.9;

1.9
date	2008.10.20.19.40.54;	author brad;	state Exp;
branches;
next	1.8;

1.8
date	2008.10.20.19.39.37;	author brad;	state Exp;
branches;
next	1.7;

1.7
date	2008.10.20.19.36.54;	author brad;	state Exp;
branches;
next	1.6;

1.6
date	2008.10.14.11.45.40;	author jsg;	state Exp;
branches;
next	1.5;

1.5
date	2008.10.14.11.41.47;	author jsg;	state Exp;
branches;
next	1.4;

1.4
date	2008.10.02.20.21.14;	author brad;	state Exp;
branches;
next	1.3;

1.3
date	2008.09.29.22.43.45;	author deraadt;	state Exp;
branches;
next	1.2;

1.2
date	2008.09.27.13.03.30;	author jsg;	state Exp;
branches;
next	1.1;

1.1
date	2008.09.26.10.35.15;	author jsg;	state Exp;
branches;
next	;


desc
@@


1.50
log
@If you use sys/param.h, you don't need sys/types.h
@
text
@/*	$OpenBSD: if_jme.c,v 1.49 2017/01/22 10:17:38 dlg Exp $	*/
/*-
 * Copyright (c) 2008, Pyun YongHyeon <yongari@@FreeBSD.org>
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice unmodified, this list of conditions, and the following
 *    disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 * $FreeBSD: src/sys/dev/jme/if_jme.c,v 1.2 2008/07/18 04:20:48 yongari Exp $
 * $DragonFly: src/sys/dev/netif/jme/if_jme.c,v 1.7 2008/09/13 04:04:39 sephe Exp $
 */

#include "bpfilter.h"
#include "vlan.h"

#include <sys/param.h>
#include <sys/endian.h>
#include <sys/systm.h>
#include <sys/sockio.h>
#include <sys/mbuf.h>
#include <sys/queue.h>
#include <sys/kernel.h>
#include <sys/device.h>
#include <sys/timeout.h>
#include <sys/socket.h>

#include <machine/bus.h>

#include <net/if.h>
#include <net/if_dl.h>
#include <net/if_media.h>

#include <netinet/in.h>
#include <netinet/if_ether.h>

#if NBPFILTER > 0
#include <net/bpf.h>
#endif

#include <dev/mii/miivar.h>
#include <dev/mii/jmphyreg.h>

#include <dev/pci/pcireg.h>
#include <dev/pci/pcivar.h>
#include <dev/pci/pcidevs.h>

#include <dev/pci/if_jmereg.h>
#include <dev/pci/if_jmevar.h>

/* Define the following to disable printing Rx errors. */
#undef	JME_SHOW_ERRORS

int	jme_match(struct device *, void *, void *);
void	jme_map_intr_vector(struct jme_softc *);
void	jme_attach(struct device *, struct device *, void *);
int	jme_detach(struct device *, int);

int	jme_miibus_readreg(struct device *, int, int);
void	jme_miibus_writereg(struct device *, int, int, int);
void	jme_miibus_statchg(struct device *);

int	jme_init(struct ifnet *);
int	jme_ioctl(struct ifnet *, u_long, caddr_t);

void	jme_start(struct ifnet *);
void	jme_watchdog(struct ifnet *);
void	jme_mediastatus(struct ifnet *, struct ifmediareq *);
int	jme_mediachange(struct ifnet *);

int	jme_intr(void *);
void	jme_txeof(struct jme_softc *);
void	jme_rxeof(struct jme_softc *);

int	jme_dma_alloc(struct jme_softc *);
void	jme_dma_free(struct jme_softc *);
int	jme_init_rx_ring(struct jme_softc *);
void	jme_init_tx_ring(struct jme_softc *);
void	jme_init_ssb(struct jme_softc *);
int	jme_newbuf(struct jme_softc *, struct jme_rxdesc *);
int	jme_encap(struct jme_softc *, struct mbuf *);
void	jme_rxpkt(struct jme_softc *);

void	jme_tick(void *);
void	jme_stop(struct jme_softc *);
void	jme_reset(struct jme_softc *);
void	jme_set_vlan(struct jme_softc *);
void	jme_iff(struct jme_softc *);
void	jme_stop_tx(struct jme_softc *);
void	jme_stop_rx(struct jme_softc *);
void	jme_mac_config(struct jme_softc *);
void	jme_reg_macaddr(struct jme_softc *, uint8_t[]);
int	jme_eeprom_macaddr(struct jme_softc *, uint8_t[]);
int	jme_eeprom_read_byte(struct jme_softc *, uint8_t, uint8_t *);
void	jme_discard_rxbufs(struct jme_softc *, int, int);
#ifdef notyet
void	jme_setwol(struct jme_softc *);
void	jme_setlinkspeed(struct jme_softc *);
#endif

/*
 * Devices supported by this driver.
 */
const struct pci_matchid jme_devices[] = {
	{ PCI_VENDOR_JMICRON, PCI_PRODUCT_JMICRON_JMC250 },
	{ PCI_VENDOR_JMICRON, PCI_PRODUCT_JMICRON_JMC260 }
};

struct cfattach jme_ca = {
	sizeof (struct jme_softc), jme_match, jme_attach
};

struct cfdriver jme_cd = {
	NULL, "jme", DV_IFNET
};

int jmedebug = 0;
#define DPRINTF(x)	do { if (jmedebug) printf x; } while (0)

/*
 *	Read a PHY register on the MII of the JMC250.
 */
int
jme_miibus_readreg(struct device *dev, int phy, int reg)
{
	struct jme_softc *sc = (struct jme_softc *)dev;
	uint32_t val;
	int i;

	/* For FPGA version, PHY address 0 should be ignored. */
	if ((sc->jme_caps & JME_CAP_FPGA) && phy == 0)
		return (0);

	CSR_WRITE_4(sc, JME_SMI, SMI_OP_READ | SMI_OP_EXECUTE |
	    SMI_PHY_ADDR(phy) | SMI_REG_ADDR(reg));

	for (i = JME_PHY_TIMEOUT; i > 0; i--) {
		DELAY(1);
		if (((val = CSR_READ_4(sc, JME_SMI)) & SMI_OP_EXECUTE) == 0)
			break;
	}
	if (i == 0) {
		printf("%s: phy read timeout: phy %d, reg %d\n",
		    sc->sc_dev.dv_xname, phy, reg);
		return (0);
	}

	return ((val & SMI_DATA_MASK) >> SMI_DATA_SHIFT);
}

/*
 *	Write a PHY register on the MII of the JMC250.
 */
void
jme_miibus_writereg(struct device *dev, int phy, int reg, int val)
{
	struct jme_softc *sc = (struct jme_softc *)dev;
	int i;

	/* For FPGA version, PHY address 0 should be ignored. */
	if ((sc->jme_caps & JME_CAP_FPGA) && phy == 0)
		return;

	CSR_WRITE_4(sc, JME_SMI, SMI_OP_WRITE | SMI_OP_EXECUTE |
	    ((val << SMI_DATA_SHIFT) & SMI_DATA_MASK) |
	    SMI_PHY_ADDR(phy) | SMI_REG_ADDR(reg));

	for (i = JME_PHY_TIMEOUT; i > 0; i--) {
		DELAY(1);
		if (((val = CSR_READ_4(sc, JME_SMI)) & SMI_OP_EXECUTE) == 0)
			break;
	}
	if (i == 0) {
		printf("%s: phy write timeout: phy %d, reg %d\n",
		    sc->sc_dev.dv_xname, phy, reg);
	}
}

/*
 *	Callback from MII layer when media changes.
 */
void
jme_miibus_statchg(struct device *dev)
{
	struct jme_softc *sc = (struct jme_softc *)dev;
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	struct mii_data *mii;
	struct jme_txdesc *txd;
	bus_addr_t paddr;
	int i;

	if ((ifp->if_flags & IFF_RUNNING) == 0)
		return;

	mii = &sc->sc_miibus;

	sc->jme_flags &= ~JME_FLAG_LINK;
	if ((mii->mii_media_status & IFM_AVALID) != 0) {
		switch (IFM_SUBTYPE(mii->mii_media_active)) {
		case IFM_10_T:
		case IFM_100_TX:
			sc->jme_flags |= JME_FLAG_LINK;
			break;
		case IFM_1000_T:
			if (sc->jme_caps & JME_CAP_FASTETH)
				break;
			sc->jme_flags |= JME_FLAG_LINK;
			break;
		default:
			break;
		}
	}

	/*
	 * Disabling Rx/Tx MACs have a side-effect of resetting
	 * JME_TXNDA/JME_RXNDA register to the first address of
	 * Tx/Rx descriptor address. So driver should reset its
	 * internal procucer/consumer pointer and reclaim any
	 * allocated resources.  Note, just saving the value of
	 * JME_TXNDA and JME_RXNDA registers before stopping MAC
	 * and restoring JME_TXNDA/JME_RXNDA register is not
	 * sufficient to make sure correct MAC state because
	 * stopping MAC operation can take a while and hardware
	 * might have updated JME_TXNDA/JME_RXNDA registers
	 * during the stop operation.
	 */

	/* Disable interrupts */
	CSR_WRITE_4(sc, JME_INTR_MASK_CLR, JME_INTRS);

	/* Stop driver */
	ifp->if_flags &= ~IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);
	ifp->if_timer = 0;
	timeout_del(&sc->jme_tick_ch);

	/* Stop receiver/transmitter. */
	jme_stop_rx(sc);
	jme_stop_tx(sc);

	jme_rxeof(sc);
	m_freem(sc->jme_cdata.jme_rxhead);
	JME_RXCHAIN_RESET(sc);

	jme_txeof(sc);
	if (sc->jme_cdata.jme_tx_cnt != 0) {
		/* Remove queued packets for transmit. */
		for (i = 0; i < JME_TX_RING_CNT; i++) {
			txd = &sc->jme_cdata.jme_txdesc[i];
			if (txd->tx_m != NULL) {
				bus_dmamap_unload(sc->sc_dmat, txd->tx_dmamap);
				m_freem(txd->tx_m);
				txd->tx_m = NULL;
				txd->tx_ndesc = 0;
				ifp->if_oerrors++;
			}
		}
	}

	/*
	 * Reuse configured Rx descriptors and reset
	 * procuder/consumer index.
	 */
	sc->jme_cdata.jme_rx_cons = 0;

	jme_init_tx_ring(sc);

	/* Initialize shadow status block. */
	jme_init_ssb(sc);

	/* Program MAC with resolved speed/duplex/flow-control. */
	if (sc->jme_flags & JME_FLAG_LINK) {
		jme_mac_config(sc);

		CSR_WRITE_4(sc, JME_RXCSR, sc->jme_rxcsr);
		CSR_WRITE_4(sc, JME_TXCSR, sc->jme_txcsr);

		/* Set Tx ring address to the hardware. */
		paddr = JME_TX_RING_ADDR(sc, 0);
		CSR_WRITE_4(sc, JME_TXDBA_HI, JME_ADDR_HI(paddr));
		CSR_WRITE_4(sc, JME_TXDBA_LO, JME_ADDR_LO(paddr));

		/* Set Rx ring address to the hardware. */
		paddr = JME_RX_RING_ADDR(sc, 0);
		CSR_WRITE_4(sc, JME_RXDBA_HI, JME_ADDR_HI(paddr));
		CSR_WRITE_4(sc, JME_RXDBA_LO, JME_ADDR_LO(paddr));

		/* Restart receiver/transmitter. */
		CSR_WRITE_4(sc, JME_RXCSR, sc->jme_rxcsr | RXCSR_RX_ENB |
		    RXCSR_RXQ_START);
		CSR_WRITE_4(sc, JME_TXCSR, sc->jme_txcsr | TXCSR_TX_ENB);
	}

	ifp->if_flags |= IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);
	timeout_add_sec(&sc->jme_tick_ch, 1);

	/* Reenable interrupts. */
	CSR_WRITE_4(sc, JME_INTR_MASK_SET, JME_INTRS);
}

/*
 *	Get the current interface media status.
 */
void
jme_mediastatus(struct ifnet *ifp, struct ifmediareq *ifmr)
{
	struct jme_softc *sc = ifp->if_softc;
	struct mii_data *mii = &sc->sc_miibus;

	mii_pollstat(mii);
	ifmr->ifm_status = mii->mii_media_status;
	ifmr->ifm_active = mii->mii_media_active;
}

/*
 *	Set hardware to newly-selected media.
 */
int
jme_mediachange(struct ifnet *ifp)
{
	struct jme_softc *sc = ifp->if_softc;
	struct mii_data *mii = &sc->sc_miibus;
	int error;

	if (mii->mii_instance != 0) {
		struct mii_softc *miisc;

		LIST_FOREACH(miisc, &mii->mii_phys, mii_list)
			mii_phy_reset(miisc);
	}
	error = mii_mediachg(mii);

	return (error);
}

int
jme_match(struct device *dev, void *match, void *aux)
{
	return pci_matchbyid((struct pci_attach_args *)aux, jme_devices,
	    sizeof (jme_devices) / sizeof (jme_devices[0]));
}

int
jme_eeprom_read_byte(struct jme_softc *sc, uint8_t addr, uint8_t *val)
{
	uint32_t reg;
	int i;

	*val = 0;
	for (i = JME_TIMEOUT; i > 0; i--) {
		reg = CSR_READ_4(sc, JME_SMBCSR);
		if ((reg & SMBCSR_HW_BUSY_MASK) == SMBCSR_HW_IDLE)
			break;
		DELAY(1);
	}

	if (i == 0) {
		printf("%s: EEPROM idle timeout!\n", sc->sc_dev.dv_xname);
		return (ETIMEDOUT);
	}

	reg = ((uint32_t)addr << SMBINTF_ADDR_SHIFT) & SMBINTF_ADDR_MASK;
	CSR_WRITE_4(sc, JME_SMBINTF, reg | SMBINTF_RD | SMBINTF_CMD_TRIGGER);
	for (i = JME_TIMEOUT; i > 0; i--) {
		DELAY(1);
		reg = CSR_READ_4(sc, JME_SMBINTF);
		if ((reg & SMBINTF_CMD_TRIGGER) == 0)
			break;
	}

	if (i == 0) {
		printf("%s: EEPROM read timeout!\n", sc->sc_dev.dv_xname);
		return (ETIMEDOUT);
	}

	reg = CSR_READ_4(sc, JME_SMBINTF);
	*val = (reg & SMBINTF_RD_DATA_MASK) >> SMBINTF_RD_DATA_SHIFT;

	return (0);
}

int
jme_eeprom_macaddr(struct jme_softc *sc, uint8_t eaddr[])
{
	uint8_t fup, reg, val;
	uint32_t offset;
	int match;

	offset = 0;
	if (jme_eeprom_read_byte(sc, offset++, &fup) != 0 ||
	    fup != JME_EEPROM_SIG0)
		return (ENOENT);
	if (jme_eeprom_read_byte(sc, offset++, &fup) != 0 ||
	    fup != JME_EEPROM_SIG1)
		return (ENOENT);
	match = 0;
	do {
		if (jme_eeprom_read_byte(sc, offset, &fup) != 0)
			break;
		if (JME_EEPROM_MKDESC(JME_EEPROM_FUNC0, JME_EEPROM_PAGE_BAR1) ==
		    (fup & (JME_EEPROM_FUNC_MASK | JME_EEPROM_PAGE_MASK))) {
			if (jme_eeprom_read_byte(sc, offset + 1, &reg) != 0)
				break;
			if (reg >= JME_PAR0 &&
			    reg < JME_PAR0 + ETHER_ADDR_LEN) {
				if (jme_eeprom_read_byte(sc, offset + 2,
				    &val) != 0)
					break;
				eaddr[reg - JME_PAR0] = val;
				match++;
			}
		}
		/* Check for the end of EEPROM descriptor. */
		if ((fup & JME_EEPROM_DESC_END) == JME_EEPROM_DESC_END)
			break;
		/* Try next eeprom descriptor. */
		offset += JME_EEPROM_DESC_BYTES;
	} while (match != ETHER_ADDR_LEN && offset < JME_EEPROM_END);

	if (match == ETHER_ADDR_LEN)
		return (0);

	return (ENOENT);
}

void
jme_reg_macaddr(struct jme_softc *sc, uint8_t eaddr[])
{
	uint32_t par0, par1;

	/* Read station address. */
	par0 = CSR_READ_4(sc, JME_PAR0);
	par1 = CSR_READ_4(sc, JME_PAR1);
	par1 &= 0xFFFF;

	eaddr[0] = (par0 >> 0) & 0xFF;
	eaddr[1] = (par0 >> 8) & 0xFF;
	eaddr[2] = (par0 >> 16) & 0xFF;
	eaddr[3] = (par0 >> 24) & 0xFF;
	eaddr[4] = (par1 >> 0) & 0xFF;
	eaddr[5] = (par1 >> 8) & 0xFF;
}

void
jme_map_intr_vector(struct jme_softc *sc)
{
	uint32_t map[MSINUM_NUM_INTR_SOURCE / JME_MSI_MESSAGES];

	bzero(map, sizeof(map));

	/* Map Tx interrupts source to MSI/MSIX vector 2. */
	map[MSINUM_REG_INDEX(N_INTR_TXQ0_COMP)] =
	    MSINUM_INTR_SOURCE(2, N_INTR_TXQ0_COMP);
	map[MSINUM_REG_INDEX(N_INTR_TXQ1_COMP)] |=
	    MSINUM_INTR_SOURCE(2, N_INTR_TXQ1_COMP);
	map[MSINUM_REG_INDEX(N_INTR_TXQ2_COMP)] |=
	    MSINUM_INTR_SOURCE(2, N_INTR_TXQ2_COMP);
	map[MSINUM_REG_INDEX(N_INTR_TXQ3_COMP)] |=
	    MSINUM_INTR_SOURCE(2, N_INTR_TXQ3_COMP);
	map[MSINUM_REG_INDEX(N_INTR_TXQ4_COMP)] |=
	    MSINUM_INTR_SOURCE(2, N_INTR_TXQ4_COMP);
	map[MSINUM_REG_INDEX(N_INTR_TXQ4_COMP)] |=
	    MSINUM_INTR_SOURCE(2, N_INTR_TXQ5_COMP);
	map[MSINUM_REG_INDEX(N_INTR_TXQ6_COMP)] |=
	    MSINUM_INTR_SOURCE(2, N_INTR_TXQ6_COMP);
	map[MSINUM_REG_INDEX(N_INTR_TXQ7_COMP)] |=
	    MSINUM_INTR_SOURCE(2, N_INTR_TXQ7_COMP);
	map[MSINUM_REG_INDEX(N_INTR_TXQ_COAL)] |=
	    MSINUM_INTR_SOURCE(2, N_INTR_TXQ_COAL);
	map[MSINUM_REG_INDEX(N_INTR_TXQ_COAL_TO)] |=
	    MSINUM_INTR_SOURCE(2, N_INTR_TXQ_COAL_TO);

	/* Map Rx interrupts source to MSI/MSIX vector 1. */
	map[MSINUM_REG_INDEX(N_INTR_RXQ0_COMP)] =
	    MSINUM_INTR_SOURCE(1, N_INTR_RXQ0_COMP);
	map[MSINUM_REG_INDEX(N_INTR_RXQ1_COMP)] =
	    MSINUM_INTR_SOURCE(1, N_INTR_RXQ1_COMP);
	map[MSINUM_REG_INDEX(N_INTR_RXQ2_COMP)] =
	    MSINUM_INTR_SOURCE(1, N_INTR_RXQ2_COMP);
	map[MSINUM_REG_INDEX(N_INTR_RXQ3_COMP)] =
	    MSINUM_INTR_SOURCE(1, N_INTR_RXQ3_COMP);
	map[MSINUM_REG_INDEX(N_INTR_RXQ0_DESC_EMPTY)] =
	    MSINUM_INTR_SOURCE(1, N_INTR_RXQ0_DESC_EMPTY);
	map[MSINUM_REG_INDEX(N_INTR_RXQ1_DESC_EMPTY)] =
	    MSINUM_INTR_SOURCE(1, N_INTR_RXQ1_DESC_EMPTY);
	map[MSINUM_REG_INDEX(N_INTR_RXQ2_DESC_EMPTY)] =
	    MSINUM_INTR_SOURCE(1, N_INTR_RXQ2_DESC_EMPTY);
	map[MSINUM_REG_INDEX(N_INTR_RXQ3_DESC_EMPTY)] =
	    MSINUM_INTR_SOURCE(1, N_INTR_RXQ3_DESC_EMPTY);
	map[MSINUM_REG_INDEX(N_INTR_RXQ0_COAL)] =
	    MSINUM_INTR_SOURCE(1, N_INTR_RXQ0_COAL);
	map[MSINUM_REG_INDEX(N_INTR_RXQ1_COAL)] =
	    MSINUM_INTR_SOURCE(1, N_INTR_RXQ1_COAL);
	map[MSINUM_REG_INDEX(N_INTR_RXQ2_COAL)] =
	    MSINUM_INTR_SOURCE(1, N_INTR_RXQ2_COAL);
	map[MSINUM_REG_INDEX(N_INTR_RXQ3_COAL)] =
	    MSINUM_INTR_SOURCE(1, N_INTR_RXQ3_COAL);
	map[MSINUM_REG_INDEX(N_INTR_RXQ0_COAL_TO)] =
	    MSINUM_INTR_SOURCE(1, N_INTR_RXQ0_COAL_TO);
	map[MSINUM_REG_INDEX(N_INTR_RXQ1_COAL_TO)] =
	    MSINUM_INTR_SOURCE(1, N_INTR_RXQ1_COAL_TO);
	map[MSINUM_REG_INDEX(N_INTR_RXQ2_COAL_TO)] =
	    MSINUM_INTR_SOURCE(1, N_INTR_RXQ2_COAL_TO);
	map[MSINUM_REG_INDEX(N_INTR_RXQ3_COAL_TO)] =
	    MSINUM_INTR_SOURCE(1, N_INTR_RXQ3_COAL_TO);

	/* Map all other interrupts source to MSI/MSIX vector 0. */
	CSR_WRITE_4(sc, JME_MSINUM_BASE + sizeof(uint32_t) * 0, map[0]);
	CSR_WRITE_4(sc, JME_MSINUM_BASE + sizeof(uint32_t) * 1, map[1]);
	CSR_WRITE_4(sc, JME_MSINUM_BASE + sizeof(uint32_t) * 2, map[2]);
	CSR_WRITE_4(sc, JME_MSINUM_BASE + sizeof(uint32_t) * 3, map[3]);
}

void
jme_attach(struct device *parent, struct device *self, void *aux)
{
	struct jme_softc *sc = (struct jme_softc *)self;
	struct pci_attach_args *pa = aux;
	pci_chipset_tag_t pc = pa->pa_pc;
	pci_intr_handle_t ih;
	const char *intrstr;
	pcireg_t memtype;

	struct ifnet *ifp;
	uint32_t reg;
	int error = 0;

	/*
	 * Allocate IO memory
	 *
	 * JMC250 supports both memory mapped and I/O register space
	 * access.  Because I/O register access should use different
	 * BARs to access registers it's waste of time to use I/O
	 * register spce access.  JMC250 uses 16K to map entire memory
	 * space.
	 */

	memtype = pci_mapreg_type(pa->pa_pc, pa->pa_tag, JME_PCIR_BAR);
	if (pci_mapreg_map(pa, JME_PCIR_BAR, memtype, 0, &sc->jme_mem_bt,
	    &sc->jme_mem_bh, NULL, &sc->jme_mem_size, 0)) {
		printf(": can't map mem space\n");
		return;
	}

	if (pci_intr_map_msi(pa, &ih) == 0)
		jme_map_intr_vector(sc);
	else if (pci_intr_map(pa, &ih) != 0) {
		printf(": can't map interrupt\n");
		return;
	}

	/*
	 * Allocate IRQ
	 */
	intrstr = pci_intr_string(pc, ih);
	sc->sc_irq_handle = pci_intr_establish(pc, ih, IPL_NET, jme_intr, sc,
	    sc->sc_dev.dv_xname);
	if (sc->sc_irq_handle == NULL) {
		printf(": could not establish interrupt");
		if (intrstr != NULL)
			printf(" at %s", intrstr);
		printf("\n");
		return;
	}
	printf(": %s", intrstr);

	sc->sc_dmat = pa->pa_dmat;
	sc->jme_pct = pa->pa_pc;
	sc->jme_pcitag = pa->pa_tag;

	/*
	 * Extract FPGA revision
	 */
	reg = CSR_READ_4(sc, JME_CHIPMODE);
	if (((reg & CHIPMODE_FPGA_REV_MASK) >> CHIPMODE_FPGA_REV_SHIFT) !=
	    CHIPMODE_NOT_FPGA) {
		sc->jme_caps |= JME_CAP_FPGA;

		if (jmedebug) {
			printf("%s: FPGA revision : 0x%04x\n",
			    sc->sc_dev.dv_xname, 
			    (reg & CHIPMODE_FPGA_REV_MASK) >>
			    CHIPMODE_FPGA_REV_SHIFT);
		}
	}

	sc->jme_revfm = (reg & CHIPMODE_REVFM_MASK) >> CHIPMODE_REVFM_SHIFT;

	if (PCI_PRODUCT(pa->pa_id) == PCI_PRODUCT_JMICRON_JMC250 &&
	    PCI_REVISION(pa->pa_class) == JME_REV_JMC250_A2)
		sc->jme_workaround |= JME_WA_CRCERRORS | JME_WA_PACKETLOSS;

	/* Reset the ethernet controller. */
	jme_reset(sc);

	/* Get station address. */
	reg = CSR_READ_4(sc, JME_SMBCSR);
	if (reg & SMBCSR_EEPROM_PRESENT)
		error = jme_eeprom_macaddr(sc, sc->sc_arpcom.ac_enaddr);
	if (error != 0 || (reg & SMBCSR_EEPROM_PRESENT) == 0) {
		if (error != 0 && (jmedebug)) {
			printf("%s: ethernet hardware address "
			    "not found in EEPROM.\n", sc->sc_dev.dv_xname);
		}
		jme_reg_macaddr(sc, sc->sc_arpcom.ac_enaddr);
	}

	printf(", address %s\n", ether_sprintf(sc->sc_arpcom.ac_enaddr));

	/*
	 * Save PHY address.
	 * Integrated JR0211 has fixed PHY address whereas FPGA version
	 * requires PHY probing to get correct PHY address.
	 */
	if ((sc->jme_caps & JME_CAP_FPGA) == 0) {
		sc->jme_phyaddr = CSR_READ_4(sc, JME_GPREG0) &
		    GPREG0_PHY_ADDR_MASK;
		if (jmedebug) {
			printf("%s: PHY is at address %d.\n",
			    sc->sc_dev.dv_xname, sc->jme_phyaddr);
		}
	} else {
		sc->jme_phyaddr = 0;
	}

	/* Set max allowable DMA size. */
	sc->jme_tx_dma_size = TXCSR_DMA_SIZE_512;
	sc->jme_rx_dma_size = RXCSR_DMA_SIZE_128;

#ifdef notyet
	if (pci_find_extcap(dev, PCIY_PMG, &pmc) == 0)
		sc->jme_caps |= JME_CAP_PMCAP;
#endif

	/* Allocate DMA stuffs */
	error = jme_dma_alloc(sc);
	if (error)
		goto fail;

	ifp = &sc->sc_arpcom.ac_if;
	ifp->if_softc = sc;
	ifp->if_flags = IFF_BROADCAST | IFF_SIMPLEX | IFF_MULTICAST;
	ifp->if_ioctl = jme_ioctl;
	ifp->if_start = jme_start;
	ifp->if_watchdog = jme_watchdog;
	IFQ_SET_MAXLEN(&ifp->if_snd, JME_TX_RING_CNT - 1);
	strlcpy(ifp->if_xname, sc->sc_dev.dv_xname, IFNAMSIZ);

	ifp->if_capabilities = IFCAP_VLAN_MTU | IFCAP_CSUM_IPv4 |
	    IFCAP_CSUM_TCPv4 | IFCAP_CSUM_UDPv4 | IFCAP_CSUM_TCPv6 |
	    IFCAP_CSUM_UDPv6;

#if NVLAN > 0
	ifp->if_capabilities |= IFCAP_VLAN_HWTAGGING;
#endif

	/* Set up MII bus. */
	sc->sc_miibus.mii_ifp = ifp;
	sc->sc_miibus.mii_readreg = jme_miibus_readreg;
	sc->sc_miibus.mii_writereg = jme_miibus_writereg;
	sc->sc_miibus.mii_statchg = jme_miibus_statchg;

	ifmedia_init(&sc->sc_miibus.mii_media, 0, jme_mediachange,
	    jme_mediastatus);
	mii_attach(self, &sc->sc_miibus, 0xffffffff,
	    sc->jme_caps & JME_CAP_FPGA ? MII_PHY_ANY : sc->jme_phyaddr,
	    MII_OFFSET_ANY, MIIF_DOPAUSE);

	if (LIST_FIRST(&sc->sc_miibus.mii_phys) == NULL) {
		printf("%s: no PHY found!\n", sc->sc_dev.dv_xname);
		ifmedia_add(&sc->sc_miibus.mii_media, IFM_ETHER | IFM_MANUAL,
		    0, NULL);
		ifmedia_set(&sc->sc_miibus.mii_media, IFM_ETHER | IFM_MANUAL);
	} else
		ifmedia_set(&sc->sc_miibus.mii_media, IFM_ETHER | IFM_AUTO);

	/*
	 * Save PHYADDR for FPGA mode PHY not handled, not production hw
	 */

	if_attach(ifp);
	ether_ifattach(ifp);

	timeout_set(&sc->jme_tick_ch, jme_tick, sc);

	return;
fail:
	jme_detach(&sc->sc_dev, 0);
}

int
jme_detach(struct device *self, int flags)
{
	struct jme_softc *sc = (struct jme_softc *)self;
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	int s;

	s = splnet();
	jme_stop(sc);
	splx(s);

	mii_detach(&sc->sc_miibus, MII_PHY_ANY, MII_OFFSET_ANY);

	/* Delete all remaining media. */
	ifmedia_delete_instance(&sc->sc_miibus.mii_media, IFM_INST_ANY);

	ether_ifdetach(ifp);
	if_detach(ifp);
	jme_dma_free(sc);

	if (sc->sc_irq_handle != NULL) {
		pci_intr_disestablish(sc->jme_pct, sc->sc_irq_handle);
		sc->sc_irq_handle = NULL;
	}

	return (0);
}

int
jme_dma_alloc(struct jme_softc *sc)
{
	struct jme_txdesc *txd;
	struct jme_rxdesc *rxd;
	int error, i, nsegs;

	/*
	 * Create DMA stuffs for TX ring
	 */

	error = bus_dmamap_create(sc->sc_dmat, JME_TX_RING_SIZE, 1,
	    JME_TX_RING_SIZE, 0, BUS_DMA_NOWAIT,
	    &sc->jme_cdata.jme_tx_ring_map);
	if (error)
		return (ENOBUFS);

	/* Allocate DMA'able memory for TX ring */
	error = bus_dmamem_alloc(sc->sc_dmat, JME_TX_RING_SIZE, ETHER_ALIGN, 0,
	    &sc->jme_rdata.jme_tx_ring_seg, 1, &nsegs,
	    BUS_DMA_WAITOK);
/* XXX zero */
	if (error) {
		printf("%s: could not allocate DMA'able memory for Tx ring.\n",
		    sc->sc_dev.dv_xname);
		return error;
	}

	error = bus_dmamem_map(sc->sc_dmat, &sc->jme_rdata.jme_tx_ring_seg,
	    nsegs, JME_TX_RING_SIZE, (caddr_t *)&sc->jme_rdata.jme_tx_ring,
	    BUS_DMA_NOWAIT);
	if (error)
		return (ENOBUFS);

	/*  Load the DMA map for Tx ring. */
	error = bus_dmamap_load(sc->sc_dmat,
	    sc->jme_cdata.jme_tx_ring_map, sc->jme_rdata.jme_tx_ring,
	    JME_TX_RING_SIZE, NULL, BUS_DMA_NOWAIT);
	if (error) {
		printf("%s: could not load DMA'able memory for Tx ring.\n",
		    sc->sc_dev.dv_xname);
		bus_dmamem_free(sc->sc_dmat,
		    (bus_dma_segment_t *)&sc->jme_rdata.jme_tx_ring, 1);
		return error;
	}
	sc->jme_rdata.jme_tx_ring_paddr =
	    sc->jme_cdata.jme_tx_ring_map->dm_segs[0].ds_addr;

	/*
	 * Create DMA stuffs for RX ring
	 */

	error = bus_dmamap_create(sc->sc_dmat, JME_RX_RING_SIZE, 1,
	    JME_RX_RING_SIZE, 0, BUS_DMA_NOWAIT,
	    &sc->jme_cdata.jme_rx_ring_map);
	if (error)
		return (ENOBUFS);

	/* Allocate DMA'able memory for RX ring */
	error = bus_dmamem_alloc(sc->sc_dmat, JME_RX_RING_SIZE, ETHER_ALIGN, 0,
	    &sc->jme_rdata.jme_rx_ring_seg, 1, &nsegs,
	    BUS_DMA_WAITOK | BUS_DMA_ZERO);
/* XXX zero */
	if (error) {
		printf("%s: could not allocate DMA'able memory for Rx ring.\n",
		    sc->sc_dev.dv_xname);
		return error;
	}

	error = bus_dmamem_map(sc->sc_dmat, &sc->jme_rdata.jme_rx_ring_seg,
	    nsegs, JME_RX_RING_SIZE, (caddr_t *)&sc->jme_rdata.jme_rx_ring,
	    BUS_DMA_NOWAIT);
	if (error)
		return (ENOBUFS);

	/* Load the DMA map for Rx ring. */
	error = bus_dmamap_load(sc->sc_dmat,
	    sc->jme_cdata.jme_rx_ring_map, sc->jme_rdata.jme_rx_ring,
	    JME_RX_RING_SIZE, NULL, BUS_DMA_NOWAIT);
	if (error) {
		printf("%s: could not load DMA'able memory for Rx ring.\n",
		    sc->sc_dev.dv_xname);
		bus_dmamem_free(sc->sc_dmat,
		    (bus_dma_segment_t *)sc->jme_rdata.jme_rx_ring, 1);
		return error;
	}
	sc->jme_rdata.jme_rx_ring_paddr =
	    sc->jme_cdata.jme_rx_ring_map->dm_segs[0].ds_addr;

#if 0
	/* Tx/Rx descriptor queue should reside within 4GB boundary. */
	tx_ring_end = sc->jme_rdata.jme_tx_ring_paddr + JME_TX_RING_SIZE;
	rx_ring_end = sc->jme_rdata.jme_rx_ring_paddr + JME_RX_RING_SIZE;
	if ((JME_ADDR_HI(tx_ring_end) !=
	     JME_ADDR_HI(sc->jme_rdata.jme_tx_ring_paddr)) ||
	    (JME_ADDR_HI(rx_ring_end) !=
	     JME_ADDR_HI(sc->jme_rdata.jme_rx_ring_paddr))) {
		printf("%s: 4GB boundary crossed, switching to 32bit "
		    "DMA address mode.\n", sc->sc_dev.dv_xname);
		jme_dma_free(sc);
		/* Limit DMA address space to 32bit and try again. */
		lowaddr = BUS_SPACE_MAXADDR_32BIT;
		goto again;
	}
#endif

	/*
	 * Create DMA stuffs for shadow status block
	 */

	error = bus_dmamap_create(sc->sc_dmat, JME_SSB_SIZE, 1,
	    JME_SSB_SIZE, 0, BUS_DMA_NOWAIT, &sc->jme_cdata.jme_ssb_map);
	if (error)
		return (ENOBUFS);

	/* Allocate DMA'able memory for shared status block. */
	error = bus_dmamem_alloc(sc->sc_dmat, JME_SSB_SIZE, 1, 0,
	    &sc->jme_rdata.jme_ssb_block_seg, 1, &nsegs, BUS_DMA_WAITOK);
	if (error) {
		printf("%s: could not allocate DMA'able "
		    "memory for shared status block.\n", sc->sc_dev.dv_xname);
		return error;
	}

	error = bus_dmamem_map(sc->sc_dmat, &sc->jme_rdata.jme_ssb_block_seg,
	    nsegs, JME_SSB_SIZE, (caddr_t *)&sc->jme_rdata.jme_ssb_block,
	    BUS_DMA_NOWAIT);
	if (error)
		return (ENOBUFS);

	/* Load the DMA map for shared status block */
	error = bus_dmamap_load(sc->sc_dmat,
	    sc->jme_cdata.jme_ssb_map, sc->jme_rdata.jme_ssb_block,
	    JME_SSB_SIZE, NULL, BUS_DMA_NOWAIT);
	if (error) {
		printf("%s: could not load DMA'able memory "
		    "for shared status block.\n", sc->sc_dev.dv_xname);
		bus_dmamem_free(sc->sc_dmat,
		    (bus_dma_segment_t *)sc->jme_rdata.jme_ssb_block, 1);
		return error;
	}
	sc->jme_rdata.jme_ssb_block_paddr =
	    sc->jme_cdata.jme_ssb_map->dm_segs[0].ds_addr;

	/*
	 * Create DMA stuffs for TX buffers
	 */

	/* Create DMA maps for Tx buffers. */
	for (i = 0; i < JME_TX_RING_CNT; i++) {
		txd = &sc->jme_cdata.jme_txdesc[i];
		error = bus_dmamap_create(sc->sc_dmat, JME_TSO_MAXSIZE,
		    JME_MAXTXSEGS, JME_TSO_MAXSEGSIZE, 0, BUS_DMA_NOWAIT,
		    &txd->tx_dmamap);
		if (error) {
			int j;

			printf("%s: could not create %dth Tx dmamap.\n",
			    sc->sc_dev.dv_xname, i);

			for (j = 0; j < i; ++j) {
				txd = &sc->jme_cdata.jme_txdesc[j];
				bus_dmamap_destroy(sc->sc_dmat, txd->tx_dmamap);
			}
			return error;
		}

	}

	/*
	 * Create DMA stuffs for RX buffers
	 */

	/* Create DMA maps for Rx buffers. */
	error = bus_dmamap_create(sc->sc_dmat, MCLBYTES, 1, MCLBYTES,
	    0, BUS_DMA_NOWAIT, &sc->jme_cdata.jme_rx_sparemap);
	if (error) {
		printf("%s: could not create spare Rx dmamap.\n",
		    sc->sc_dev.dv_xname);
		return error;
	}
	for (i = 0; i < JME_RX_RING_CNT; i++) {
		rxd = &sc->jme_cdata.jme_rxdesc[i];
		error = bus_dmamap_create(sc->sc_dmat, MCLBYTES, 1, MCLBYTES,
		    0, BUS_DMA_NOWAIT, &rxd->rx_dmamap);
		if (error) {
			int j;

			printf("%s: could not create %dth Rx dmamap.\n",
			    sc->sc_dev.dv_xname, i);

			for (j = 0; j < i; ++j) {
				rxd = &sc->jme_cdata.jme_rxdesc[j];
				bus_dmamap_destroy(sc->sc_dmat, rxd->rx_dmamap);
			}
			bus_dmamap_destroy(sc->sc_dmat,
			    sc->jme_cdata.jme_rx_sparemap);
			sc->jme_cdata.jme_rx_tag = NULL;
			return error;
		}
	}

	return 0;
}

void
jme_dma_free(struct jme_softc *sc)
{
	struct jme_txdesc *txd;
	struct jme_rxdesc *rxd;
	int i;

	/* Tx ring */
	bus_dmamap_unload(sc->sc_dmat,
	    sc->jme_cdata.jme_tx_ring_map);
	bus_dmamem_free(sc->sc_dmat,
	    (bus_dma_segment_t *)sc->jme_rdata.jme_tx_ring, 1);

	/* Rx ring */
	bus_dmamap_unload(sc->sc_dmat,
	    sc->jme_cdata.jme_rx_ring_map);
	bus_dmamem_free(sc->sc_dmat,
	    (bus_dma_segment_t *)sc->jme_rdata.jme_rx_ring, 1);

	/* Tx buffers */
	for (i = 0; i < JME_TX_RING_CNT; i++) {
		txd = &sc->jme_cdata.jme_txdesc[i];
		bus_dmamap_destroy(sc->sc_dmat, txd->tx_dmamap);
	}

	/* Rx buffers */
	for (i = 0; i < JME_RX_RING_CNT; i++) {
		rxd = &sc->jme_cdata.jme_rxdesc[i];
		bus_dmamap_destroy(sc->sc_dmat, rxd->rx_dmamap);
	}
	bus_dmamap_destroy(sc->sc_dmat,
	    sc->jme_cdata.jme_rx_sparemap);

	/* Shadow status block. */
	bus_dmamap_unload(sc->sc_dmat,
	    sc->jme_cdata.jme_ssb_map);
	bus_dmamem_free(sc->sc_dmat,
	    (bus_dma_segment_t *)sc->jme_rdata.jme_ssb_block, 1);
}

#ifdef notyet
/*
 * Unlike other ethernet controllers, JMC250 requires
 * explicit resetting link speed to 10/100Mbps as gigabit
 * link will cunsume more power than 375mA.
 * Note, we reset the link speed to 10/100Mbps with
 * auto-negotiation but we don't know whether that operation
 * would succeed or not as we have no control after powering
 * off. If the renegotiation fail WOL may not work. Running
 * at 1Gbps draws more power than 375mA at 3.3V which is
 * specified in PCI specification and that would result in
 * complete shutdowning power to ethernet controller.
 *
 * TODO
 *  Save current negotiated media speed/duplex/flow-control
 *  to softc and restore the same link again after resuming.
 *  PHY handling such as power down/resetting to 100Mbps
 *  may be better handled in suspend method in phy driver.
 */
void
jme_setlinkspeed(struct jme_softc *sc)
{
	struct mii_data *mii;
	int aneg, i;

	JME_LOCK_ASSERT(sc);

	mii = &sc->sc_miibus;
	mii_pollstat(mii);
	aneg = 0;
	if ((mii->mii_media_status & IFM_AVALID) != 0) {
		switch IFM_SUBTYPE(mii->mii_media_active) {
		case IFM_10_T:
		case IFM_100_TX:
			return;
		case IFM_1000_T:
			aneg++;
		default:
			break;
		}
	}
	jme_miibus_writereg(&sc->sc_dev, sc->jme_phyaddr, MII_100T2CR, 0);
	jme_miibus_writereg(&sc->sc_dev, sc->jme_phyaddr, MII_ANAR,
	    ANAR_TX_FD | ANAR_TX | ANAR_10_FD | ANAR_10 | ANAR_CSMA);
	jme_miibus_writereg(&sc->sc_dev, sc->jme_phyaddr, MII_BMCR,
	    BMCR_AUTOEN | BMCR_STARTNEG);
	DELAY(1000);
	if (aneg != 0) {
		/* Poll link state until jme(4) get a 10/100 link. */
		for (i = 0; i < MII_ANEGTICKS_GIGE; i++) {
			mii_pollstat(mii);
			if ((mii->mii_media_status & IFM_AVALID) != 0) {
				switch (IFM_SUBTYPE(mii->mii_media_active)) {
				case IFM_10_T:
				case IFM_100_TX:
					jme_mac_config(sc);
					return;
				default:
					break;
				}
			}
			JME_UNLOCK(sc);
			pause("jmelnk", hz);
			JME_LOCK(sc);
		}
		if (i == MII_ANEGTICKS_GIGE)
			printf("%s: establishing link failed, "
			    "WOL may not work!\n", sc->sc_dev.dv_xname);
	}
	/*
	 * No link, force MAC to have 100Mbps, full-duplex link.
	 * This is the last resort and may/may not work.
	 */
	mii->mii_media_status = IFM_AVALID | IFM_ACTIVE;
	mii->mii_media_active = IFM_ETHER | IFM_100_TX | IFM_FDX;
	jme_mac_config(sc);
}

void
jme_setwol(struct jme_softc *sc)
{
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	uint32_t gpr, pmcs;
	uint16_t pmstat;
	int pmc;

	if (pci_find_extcap(sc->sc_dev, PCIY_PMG, &pmc) != 0) {
		/* No PME capability, PHY power down. */
		jme_miibus_writereg(&sc->sc_dev, sc->jme_phyaddr,
		    MII_BMCR, BMCR_PDOWN);
		return;
	}

	gpr = CSR_READ_4(sc, JME_GPREG0) & ~GPREG0_PME_ENB;
	pmcs = CSR_READ_4(sc, JME_PMCS);
	pmcs &= ~PMCS_WOL_ENB_MASK;
	if ((ifp->if_capenable & IFCAP_WOL_MAGIC) != 0) {
		pmcs |= PMCS_MAGIC_FRAME | PMCS_MAGIC_FRAME_ENB;
		/* Enable PME message. */
		gpr |= GPREG0_PME_ENB;
		/* For gigabit controllers, reset link speed to 10/100. */
		if ((sc->jme_caps & JME_CAP_FASTETH) == 0)
			jme_setlinkspeed(sc);
	}

	CSR_WRITE_4(sc, JME_PMCS, pmcs);
	CSR_WRITE_4(sc, JME_GPREG0, gpr);

	/* Request PME. */
	pmstat = pci_read_config(sc->sc_dev, pmc + PCIR_POWER_STATUS, 2);
	pmstat &= ~(PCIM_PSTAT_PME | PCIM_PSTAT_PMEENABLE);
	if ((ifp->if_capenable & IFCAP_WOL) != 0)
		pmstat |= PCIM_PSTAT_PME | PCIM_PSTAT_PMEENABLE;
	pci_write_config(sc->sc_dev, pmc + PCIR_POWER_STATUS, pmstat, 2);
	if ((ifp->if_capenable & IFCAP_WOL) == 0) {
		/* No WOL, PHY power down. */
		jme_miibus_writereg(&sc->sc_dev, sc->jme_phyaddr,
		    MII_BMCR, BMCR_PDOWN);
	}
}
#endif

int
jme_encap(struct jme_softc *sc, struct mbuf *m)
{
	struct jme_txdesc *txd;
	struct jme_desc *desc;
	int error, i, prod;
	uint32_t cflags;

	prod = sc->jme_cdata.jme_tx_prod;
	txd = &sc->jme_cdata.jme_txdesc[prod];

	error = bus_dmamap_load_mbuf(sc->sc_dmat, txd->tx_dmamap,
	    m, BUS_DMA_NOWAIT);
	if (error != 0 && error != EFBIG)
		goto drop;
	if (error != 0) {
		if (m_defrag(m, M_DONTWAIT)) {
			error = ENOBUFS;
			goto drop;
		}
		error = bus_dmamap_load_mbuf(sc->sc_dmat, txd->tx_dmamap,
					     m, BUS_DMA_NOWAIT);
		if (error != 0)
			goto drop;
	}

	cflags = 0;

	/* Configure checksum offload. */
	if (m->m_pkthdr.csum_flags & M_IPV4_CSUM_OUT)
		cflags |= JME_TD_IPCSUM;
	if (m->m_pkthdr.csum_flags & M_TCP_CSUM_OUT)
		cflags |= JME_TD_TCPCSUM;
	if (m->m_pkthdr.csum_flags & M_UDP_CSUM_OUT)
		cflags |= JME_TD_UDPCSUM;

#if NVLAN > 0
	/* Configure VLAN. */
	if (m->m_flags & M_VLANTAG) {
		cflags |= (m->m_pkthdr.ether_vtag & JME_TD_VLAN_MASK);
		cflags |= JME_TD_VLAN_TAG;
	}
#endif

	desc = &sc->jme_rdata.jme_tx_ring[prod];
	desc->flags = htole32(cflags);
	desc->buflen = 0;
	desc->addr_hi = htole32(m->m_pkthdr.len);
	desc->addr_lo = 0;
	sc->jme_cdata.jme_tx_cnt++;
	JME_DESC_INC(prod, JME_TX_RING_CNT);
	for (i = 0; i < txd->tx_dmamap->dm_nsegs; i++) {
		desc = &sc->jme_rdata.jme_tx_ring[prod];
		desc->flags = htole32(JME_TD_OWN | JME_TD_64BIT);
		desc->buflen = htole32(txd->tx_dmamap->dm_segs[i].ds_len);
		desc->addr_hi =
		    htole32(JME_ADDR_HI(txd->tx_dmamap->dm_segs[i].ds_addr));
		desc->addr_lo =
		    htole32(JME_ADDR_LO(txd->tx_dmamap->dm_segs[i].ds_addr));
		sc->jme_cdata.jme_tx_cnt++;
		JME_DESC_INC(prod, JME_TX_RING_CNT);
	}

	/* Update producer index. */
	sc->jme_cdata.jme_tx_prod = prod;
	/*
	 * Finally request interrupt and give the first descriptor
	 * owenership to hardware.
	 */
	desc = txd->tx_desc;
	desc->flags |= htole32(JME_TD_OWN | JME_TD_INTR);

	txd->tx_m = m;
	txd->tx_ndesc = txd->tx_dmamap->dm_nsegs + JME_TXD_RSVD;

	/* Sync descriptors. */
	bus_dmamap_sync(sc->sc_dmat, txd->tx_dmamap, 0,
	    txd->tx_dmamap->dm_mapsize, BUS_DMASYNC_PREWRITE);
	bus_dmamap_sync(sc->sc_dmat, sc->jme_cdata.jme_tx_ring_map, 0,
	     sc->jme_cdata.jme_tx_ring_map->dm_mapsize, BUS_DMASYNC_PREWRITE);

	return (0);

  drop:
	m_freem(m);
	return (error);
}

void
jme_start(struct ifnet *ifp)
{
	struct jme_softc *sc = ifp->if_softc;
	struct mbuf *m;
	int enq = 0;

	/* Reclaim transmitted frames. */
	if (sc->jme_cdata.jme_tx_cnt >= JME_TX_DESC_HIWAT)
		jme_txeof(sc);

	if (!(ifp->if_flags & IFF_RUNNING) || ifq_is_oactive(&ifp->if_snd))
		return;
	if ((sc->jme_flags & JME_FLAG_LINK) == 0)
		return;  
	if (IFQ_IS_EMPTY(&ifp->if_snd))
		return;

	for (;;) {
		/*
		 * Check number of available TX descs, always
		 * leave JME_TXD_RSVD free TX descs.
		 */
		if (sc->jme_cdata.jme_tx_cnt + JME_TXD_RSVD >
		    JME_TX_RING_CNT - JME_TXD_RSVD) {
			ifq_set_oactive(&ifp->if_snd);
			break;
		}

		IFQ_DEQUEUE(&ifp->if_snd, m);
		if (m == NULL)
			break;

		/*
		 * Pack the data into the transmit ring. If we
		 * don't have room, set the OACTIVE flag and wait
		 * for the NIC to drain the ring.
		 */
		if (jme_encap(sc, m) != 0) {
			ifp->if_oerrors++;
			continue;
		}

		enq++;

#if NBPFILTER > 0
		/*
		 * If there's a BPF listener, bounce a copy of this frame
		 * to him.
		 */
		if (ifp->if_bpf != NULL)
			bpf_mtap_ether(ifp->if_bpf, m, BPF_DIRECTION_OUT);
#endif
	}

	if (enq > 0) {
		/*
		 * Reading TXCSR takes very long time under heavy load
		 * so cache TXCSR value and writes the ORed value with
		 * the kick command to the TXCSR. This saves one register
		 * access cycle.
		 */
		CSR_WRITE_4(sc, JME_TXCSR, sc->jme_txcsr | TXCSR_TX_ENB |
		    TXCSR_TXQ_N_START(TXCSR_TXQ0));
		/* Set a timeout in case the chip goes out to lunch. */
		ifp->if_timer = JME_TX_TIMEOUT;
	}
}

void
jme_watchdog(struct ifnet *ifp)
{
	struct jme_softc *sc = ifp->if_softc;

	if ((sc->jme_flags & JME_FLAG_LINK) == 0) {
		printf("%s: watchdog timeout (missed link)\n",
		    sc->sc_dev.dv_xname);
		ifp->if_oerrors++;
		jme_init(ifp);
		return;
	}

	jme_txeof(sc);
	if (sc->jme_cdata.jme_tx_cnt == 0) {
		printf("%s: watchdog timeout (missed Tx interrupts) "
			  "-- recovering\n", sc->sc_dev.dv_xname);
		jme_start(ifp);
		return;
	}

	printf("%s: watchdog timeout\n", sc->sc_dev.dv_xname);
	ifp->if_oerrors++;
	jme_init(ifp);
	jme_start(ifp);
}

int
jme_ioctl(struct ifnet *ifp, u_long cmd, caddr_t data)
{
	struct jme_softc *sc = ifp->if_softc;
	struct mii_data *mii = &sc->sc_miibus;
	struct ifreq *ifr = (struct ifreq *)data;
	int error = 0, s;

	s = splnet();

	switch (cmd) {
	case SIOCSIFADDR:
		ifp->if_flags |= IFF_UP;
		if (!(ifp->if_flags & IFF_RUNNING))
			jme_init(ifp);
		break;

	case SIOCSIFFLAGS:
		if (ifp->if_flags & IFF_UP) {
			if (ifp->if_flags & IFF_RUNNING)
				error = ENETRESET;
			else
				jme_init(ifp);
		} else {
			if (ifp->if_flags & IFF_RUNNING)
				jme_stop(sc);
		}
		break;

	case SIOCSIFMEDIA:
	case SIOCGIFMEDIA:
		error = ifmedia_ioctl(ifp, ifr, &mii->mii_media, cmd);
		break;

	default:
		error = ether_ioctl(ifp, &sc->sc_arpcom, cmd, data);
	}

	if (error == ENETRESET) {
		if (ifp->if_flags & IFF_RUNNING)
			jme_iff(sc);
		error = 0;
	}

	splx(s);
	return (error);
}

void
jme_mac_config(struct jme_softc *sc)
{
	struct mii_data *mii;
	uint32_t ghc, rxmac, txmac, txpause, gp1;
	int phyconf = JMPHY_CONF_DEFFIFO, hdx = 0;

	mii = &sc->sc_miibus;

	CSR_WRITE_4(sc, JME_GHC, GHC_RESET);
	DELAY(10);
	CSR_WRITE_4(sc, JME_GHC, 0);
	ghc = 0;
	rxmac = CSR_READ_4(sc, JME_RXMAC);
	rxmac &= ~RXMAC_FC_ENB;
	txmac = CSR_READ_4(sc, JME_TXMAC);
	txmac &= ~(TXMAC_CARRIER_EXT | TXMAC_FRAME_BURST);
	txpause = CSR_READ_4(sc, JME_TXPFC);
	txpause &= ~TXPFC_PAUSE_ENB;
	if ((IFM_OPTIONS(mii->mii_media_active) & IFM_FDX) != 0) {
		ghc |= GHC_FULL_DUPLEX;
		rxmac &= ~RXMAC_COLL_DET_ENB;
		txmac &= ~(TXMAC_COLL_ENB | TXMAC_CARRIER_SENSE |
		    TXMAC_BACKOFF | TXMAC_CARRIER_EXT |
		    TXMAC_FRAME_BURST);
		if ((IFM_OPTIONS(mii->mii_media_active) & IFM_ETH_TXPAUSE) != 0)
			txpause |= TXPFC_PAUSE_ENB;
		if ((IFM_OPTIONS(mii->mii_media_active) & IFM_ETH_RXPAUSE) != 0)
			rxmac |= RXMAC_FC_ENB;
		/* Disable retry transmit timer/retry limit. */
		CSR_WRITE_4(sc, JME_TXTRHD, CSR_READ_4(sc, JME_TXTRHD) &
		    ~(TXTRHD_RT_PERIOD_ENB | TXTRHD_RT_LIMIT_ENB));
	} else {
		rxmac |= RXMAC_COLL_DET_ENB;
		txmac |= TXMAC_COLL_ENB | TXMAC_CARRIER_SENSE | TXMAC_BACKOFF;
		/* Enable retry transmit timer/retry limit. */
		CSR_WRITE_4(sc, JME_TXTRHD, CSR_READ_4(sc, JME_TXTRHD) |
		    TXTRHD_RT_PERIOD_ENB | TXTRHD_RT_LIMIT_ENB);
	}

	/*
	 * Reprogram Tx/Rx MACs with resolved speed/duplex.
	 */
	gp1 = CSR_READ_4(sc, JME_GPREG1);
	gp1 &= ~GPREG1_HALF_PATCH;

	if ((IFM_OPTIONS(mii->mii_media_active) & IFM_FDX) == 0)
		hdx = 1;

	switch (IFM_SUBTYPE(mii->mii_media_active)) {
	case IFM_10_T:
		ghc |= GHC_SPEED_10;
		if (hdx)
			gp1 |= GPREG1_HALF_PATCH;
		break;

	case IFM_100_TX:
		ghc |= GHC_SPEED_100;
		if (hdx)
			gp1 |= GPREG1_HALF_PATCH;

		/*
		 * Use extended FIFO depth to workaround CRC errors
		 * emitted by chips before JMC250B
		 */
		phyconf = JMPHY_CONF_EXTFIFO;
		break;

	case IFM_1000_T:
		if (sc->jme_caps & JME_CAP_FASTETH)
			break;

		ghc |= GHC_SPEED_1000;
		if (hdx)
			txmac |= TXMAC_CARRIER_EXT | TXMAC_FRAME_BURST;
		break;

	default:
		break;
	}

	if (sc->jme_revfm >= 2) {
		/* set clock sources for tx mac and offload engine */
		if (IFM_SUBTYPE(mii->mii_media_active) == IFM_1000_T)
			ghc |= GHC_TCPCK_1000 | GHC_TXCK_1000;
		else
			ghc |= GHC_TCPCK_10_100 | GHC_TXCK_10_100;
	}

	CSR_WRITE_4(sc, JME_GHC, ghc);
	CSR_WRITE_4(sc, JME_RXMAC, rxmac);
	CSR_WRITE_4(sc, JME_TXMAC, txmac);
	CSR_WRITE_4(sc, JME_TXPFC, txpause);

	if (sc->jme_workaround & JME_WA_CRCERRORS) {
		jme_miibus_writereg(&sc->sc_dev, sc->jme_phyaddr,
				    JMPHY_CONF, phyconf);
	}
	if (sc->jme_workaround & JME_WA_PACKETLOSS)
		CSR_WRITE_4(sc, JME_GPREG1, gp1);
}

int
jme_intr(void *xsc)
{
	struct jme_softc *sc = xsc;
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	uint32_t status;
	int claimed = 0;

	status = CSR_READ_4(sc, JME_INTR_REQ_STATUS);
	if (status == 0 || status == 0xFFFFFFFF)
		return (0);

	/* Disable interrupts. */
	CSR_WRITE_4(sc, JME_INTR_MASK_CLR, JME_INTRS);

	status = CSR_READ_4(sc, JME_INTR_STATUS);
	if ((status & JME_INTRS) == 0 || status == 0xFFFFFFFF)
		goto back;

	/* Reset PCC counter/timer and Ack interrupts. */
	status &= ~(INTR_TXQ_COMP | INTR_RXQ_COMP);
	if (status & (INTR_TXQ_COAL | INTR_TXQ_COAL_TO))
		status |= INTR_TXQ_COAL | INTR_TXQ_COAL_TO | INTR_TXQ_COMP;
	if (status & (INTR_RXQ_COAL | INTR_RXQ_COAL_TO))
		status |= INTR_RXQ_COAL | INTR_RXQ_COAL_TO | INTR_RXQ_COMP;
	CSR_WRITE_4(sc, JME_INTR_STATUS, status);

	if (ifp->if_flags & IFF_RUNNING) {
		if (status & (INTR_RXQ_COAL | INTR_RXQ_COAL_TO))
			jme_rxeof(sc);

		if (status & INTR_RXQ_DESC_EMPTY) {
			/*
			 * Notify hardware availability of new Rx buffers.
			 * Reading RXCSR takes very long time under heavy
			 * load so cache RXCSR value and writes the ORed
			 * value with the kick command to the RXCSR. This
			 * saves one register access cycle.
			 */
			CSR_WRITE_4(sc, JME_RXCSR, sc->jme_rxcsr |
			    RXCSR_RX_ENB | RXCSR_RXQ_START);
		}

		if (status & (INTR_TXQ_COAL | INTR_TXQ_COAL_TO)) {
			jme_txeof(sc);
			jme_start(ifp);
		}
	}
	claimed = 1;
back:
	/* Reenable interrupts. */
	CSR_WRITE_4(sc, JME_INTR_MASK_SET, JME_INTRS);

	return (claimed);
}

void
jme_txeof(struct jme_softc *sc)
{
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	struct jme_txdesc *txd;
	uint32_t status;
	int cons, nsegs;

	cons = sc->jme_cdata.jme_tx_cons;
	if (cons == sc->jme_cdata.jme_tx_prod)
		return;

	bus_dmamap_sync(sc->sc_dmat, sc->jme_cdata.jme_tx_ring_map, 0,
	    sc->jme_cdata.jme_tx_ring_map->dm_mapsize, BUS_DMASYNC_POSTREAD);

	/*
	 * Go through our Tx list and free mbufs for those
	 * frames which have been transmitted.
	 */
	while (cons != sc->jme_cdata.jme_tx_prod) {
		txd = &sc->jme_cdata.jme_txdesc[cons];

		if (txd->tx_m == NULL)
			panic("%s: freeing NULL mbuf!", sc->sc_dev.dv_xname);

		status = letoh32(txd->tx_desc->flags);
		if ((status & JME_TD_OWN) == JME_TD_OWN)
			break;

		if (status & (JME_TD_TMOUT | JME_TD_RETRY_EXP)) {
			ifp->if_oerrors++;
		} else {
			if (status & JME_TD_COLLISION) {
				ifp->if_collisions +=
				    letoh32(txd->tx_desc->buflen) &
				    JME_TD_BUF_LEN_MASK;
			}
		}

		/*
		 * Only the first descriptor of multi-descriptor
		 * transmission is updated so driver have to skip entire
		 * chained buffers for the transmiited frame. In other
		 * words, JME_TD_OWN bit is valid only at the first
		 * descriptor of a multi-descriptor transmission.
		 */
		for (nsegs = 0; nsegs < txd->tx_ndesc; nsegs++) {
			sc->jme_rdata.jme_tx_ring[cons].flags = 0;
			JME_DESC_INC(cons, JME_TX_RING_CNT);
		}

		/* Reclaim transferred mbufs. */
		bus_dmamap_unload(sc->sc_dmat, txd->tx_dmamap);
		m_freem(txd->tx_m);
		txd->tx_m = NULL;
		sc->jme_cdata.jme_tx_cnt -= txd->tx_ndesc;
		if (sc->jme_cdata.jme_tx_cnt < 0)
			panic("%s: Active Tx desc counter was garbled",
			    sc->sc_dev.dv_xname);
		txd->tx_ndesc = 0;
	}
	sc->jme_cdata.jme_tx_cons = cons;

	if (sc->jme_cdata.jme_tx_cnt == 0)
		ifp->if_timer = 0;

	if (sc->jme_cdata.jme_tx_cnt + JME_TXD_RSVD <=
	    JME_TX_RING_CNT - JME_TXD_RSVD)
		ifq_clr_oactive(&ifp->if_snd);

	bus_dmamap_sync(sc->sc_dmat, sc->jme_cdata.jme_tx_ring_map, 0,
	    sc->jme_cdata.jme_tx_ring_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
}

void
jme_discard_rxbufs(struct jme_softc *sc, int cons, int count)
{
	int i;

	for (i = 0; i < count; ++i) {
		struct jme_desc *desc = &sc->jme_rdata.jme_rx_ring[cons];

		desc->flags = htole32(JME_RD_OWN | JME_RD_INTR | JME_RD_64BIT);
		desc->buflen = htole32(MCLBYTES);
		JME_DESC_INC(cons, JME_RX_RING_CNT);
	}
}

/* Receive a frame. */
void
jme_rxpkt(struct jme_softc *sc)
{
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	struct jme_desc *desc;
	struct jme_rxdesc *rxd;
	struct mbuf_list ml = MBUF_LIST_INITIALIZER();
	struct mbuf *mp, *m;
	uint32_t flags, status;
	int cons, count, nsegs;

	cons = sc->jme_cdata.jme_rx_cons;
	desc = &sc->jme_rdata.jme_rx_ring[cons];
	flags = letoh32(desc->flags);
	status = letoh32(desc->buflen);
	nsegs = JME_RX_NSEGS(status);

	if (status & JME_RX_ERR_STAT) {
		ifp->if_ierrors++;
		jme_discard_rxbufs(sc, cons, nsegs);
#ifdef JME_SHOW_ERRORS
		printf("%s : receive error = 0x%b\n",
		    sc->sc_dev.dv_xname, JME_RX_ERR(status), JME_RX_ERR_BITS);
#endif
		sc->jme_cdata.jme_rx_cons += nsegs;
		sc->jme_cdata.jme_rx_cons %= JME_RX_RING_CNT;
		return;
	}

	sc->jme_cdata.jme_rxlen = JME_RX_BYTES(status) - JME_RX_PAD_BYTES;
	for (count = 0; count < nsegs; count++,
	     JME_DESC_INC(cons, JME_RX_RING_CNT)) {
		rxd = &sc->jme_cdata.jme_rxdesc[cons];
		mp = rxd->rx_m;

		/* Add a new receive buffer to the ring. */
		if (jme_newbuf(sc, rxd) != 0) {
			ifp->if_iqdrops++;
			/* Reuse buffer. */
			jme_discard_rxbufs(sc, cons, nsegs - count);
			if (sc->jme_cdata.jme_rxhead != NULL) {
				m_freem(sc->jme_cdata.jme_rxhead);
				JME_RXCHAIN_RESET(sc);
			}
			break;
		}

		/*
		 * Assume we've received a full sized frame.
		 * Actual size is fixed when we encounter the end of
		 * multi-segmented frame.
		 */
		mp->m_len = MCLBYTES;

		/* Chain received mbufs. */
		if (sc->jme_cdata.jme_rxhead == NULL) {
			sc->jme_cdata.jme_rxhead = mp;
			sc->jme_cdata.jme_rxtail = mp;
		} else {
			/*
			 * Receive processor can receive a maximum frame
			 * size of 65535 bytes.
			 */
			mp->m_flags &= ~M_PKTHDR;
			sc->jme_cdata.jme_rxtail->m_next = mp;
			sc->jme_cdata.jme_rxtail = mp;
		}

		if (count == nsegs - 1) {
			/* Last desc. for this frame. */
			m = sc->jme_cdata.jme_rxhead;
			/* XXX assert PKTHDR? */
			m->m_flags |= M_PKTHDR;
			m->m_pkthdr.len = sc->jme_cdata.jme_rxlen;
			if (nsegs > 1) {
				/* Set first mbuf size. */
				m->m_len = MCLBYTES - JME_RX_PAD_BYTES;
				/* Set last mbuf size. */
				mp->m_len = sc->jme_cdata.jme_rxlen -
				    ((MCLBYTES - JME_RX_PAD_BYTES) +
				    (MCLBYTES * (nsegs - 2)));
			} else {
				m->m_len = sc->jme_cdata.jme_rxlen;
			}

			/*
			 * Account for 10bytes auto padding which is used
			 * to align IP header on 32bit boundary. Also note,
			 * CRC bytes is automatically removed by the
			 * hardware.
			 */
			m->m_data += JME_RX_PAD_BYTES;

			/* Set checksum information. */
			if (flags & (JME_RD_IPV4|JME_RD_IPV6)) {
				if ((flags & JME_RD_IPV4) &&
				    (flags & JME_RD_IPCSUM))
					m->m_pkthdr.csum_flags |=
					    M_IPV4_CSUM_IN_OK;
				if ((flags & JME_RD_MORE_FRAG) == 0 &&
				    ((flags & (JME_RD_TCP | JME_RD_TCPCSUM)) ==
				     (JME_RD_TCP | JME_RD_TCPCSUM) ||
				     (flags & (JME_RD_UDP | JME_RD_UDPCSUM)) ==
				     (JME_RD_UDP | JME_RD_UDPCSUM))) {
					m->m_pkthdr.csum_flags |=
					    M_TCP_CSUM_IN_OK | M_UDP_CSUM_IN_OK;
				}
			}

#if NVLAN > 0
			/* Check for VLAN tagged packets. */
			if (flags & JME_RD_VLAN_TAG) {
				m->m_pkthdr.ether_vtag = flags & JME_RD_VLAN_MASK;
				m->m_flags |= M_VLANTAG;
			}
#endif

			ml_enqueue(&ml, m);

			/* Reset mbuf chains. */
			JME_RXCHAIN_RESET(sc);
		}
	}

	if_input(ifp, &ml);

	sc->jme_cdata.jme_rx_cons += nsegs;
	sc->jme_cdata.jme_rx_cons %= JME_RX_RING_CNT;
}

void
jme_rxeof(struct jme_softc *sc)
{
	struct jme_desc *desc;
	int nsegs, prog, pktlen;

	bus_dmamap_sync(sc->sc_dmat, sc->jme_cdata.jme_rx_ring_map, 0,
	    sc->jme_cdata.jme_rx_ring_map->dm_mapsize, BUS_DMASYNC_POSTREAD);

	prog = 0;
	for (;;) {
		desc = &sc->jme_rdata.jme_rx_ring[sc->jme_cdata.jme_rx_cons];
		if ((letoh32(desc->flags) & JME_RD_OWN) == JME_RD_OWN)
			break;
		if ((letoh32(desc->buflen) & JME_RD_VALID) == 0)
			break;

		/*
		 * Check number of segments against received bytes.
		 * Non-matching value would indicate that hardware
		 * is still trying to update Rx descriptors. I'm not
		 * sure whether this check is needed.
		 */
		nsegs = JME_RX_NSEGS(letoh32(desc->buflen));
		pktlen = JME_RX_BYTES(letoh32(desc->buflen));
		if (nsegs != howmany(pktlen, MCLBYTES)) {
			printf("%s: RX fragment count(%d) "
			    "and packet size(%d) mismach\n",
			     sc->sc_dev.dv_xname, nsegs, pktlen);
			break;
		}

		/* Received a frame. */
		jme_rxpkt(sc);
		prog++;
	}

	if (prog > 0) {
		bus_dmamap_sync(sc->sc_dmat, sc->jme_cdata.jme_rx_ring_map, 0,
		    sc->jme_cdata.jme_rx_ring_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
	}
}

void
jme_tick(void *xsc)
{
	struct jme_softc *sc = xsc;
	struct mii_data *mii = &sc->sc_miibus;
	int s;

	s = splnet();
	mii_tick(mii);
	timeout_add_sec(&sc->jme_tick_ch, 1);
	splx(s);
}

void
jme_reset(struct jme_softc *sc)
{
#ifdef foo
	/* Stop receiver, transmitter. */
	jme_stop_rx(sc);
	jme_stop_tx(sc);
#endif
	CSR_WRITE_4(sc, JME_GHC, GHC_RESET);
	DELAY(10);
	CSR_WRITE_4(sc, JME_GHC, 0);
}

int
jme_init(struct ifnet *ifp)
{
	struct jme_softc *sc = ifp->if_softc;
	struct mii_data *mii;
	uint8_t eaddr[ETHER_ADDR_LEN];
	bus_addr_t paddr;
	uint32_t reg;
	int error;

	/*
	 * Cancel any pending I/O.
	 */
	jme_stop(sc);

	/*
	 * Reset the chip to a known state.
	 */
	jme_reset(sc);

	/* Init descriptors. */
	error = jme_init_rx_ring(sc);
        if (error != 0) {
                printf("%s: initialization failed: no memory for Rx buffers.\n",
		    sc->sc_dev.dv_xname);
                jme_stop(sc);
		return (error);
        }
	jme_init_tx_ring(sc);

	/* Initialize shadow status block. */
	jme_init_ssb(sc);

	/* Reprogram the station address. */
	bcopy(LLADDR(ifp->if_sadl), eaddr, ETHER_ADDR_LEN);
	CSR_WRITE_4(sc, JME_PAR0,
	    eaddr[3] << 24 | eaddr[2] << 16 | eaddr[1] << 8 | eaddr[0]);
	CSR_WRITE_4(sc, JME_PAR1, eaddr[5] << 8 | eaddr[4]);

	/*
	 * Configure Tx queue.
	 *  Tx priority queue weight value : 0
	 *  Tx FIFO threshold for processing next packet : 16QW
	 *  Maximum Tx DMA length : 512
	 *  Allow Tx DMA burst.
	 */
	sc->jme_txcsr = TXCSR_TXQ_N_SEL(TXCSR_TXQ0);
	sc->jme_txcsr |= TXCSR_TXQ_WEIGHT(TXCSR_TXQ_WEIGHT_MIN);
	sc->jme_txcsr |= TXCSR_FIFO_THRESH_16QW;
	sc->jme_txcsr |= sc->jme_tx_dma_size;
	sc->jme_txcsr |= TXCSR_DMA_BURST;
	CSR_WRITE_4(sc, JME_TXCSR, sc->jme_txcsr);

	/* Set Tx descriptor counter. */
	CSR_WRITE_4(sc, JME_TXQDC, JME_TX_RING_CNT);

	/* Set Tx ring address to the hardware. */
	paddr = JME_TX_RING_ADDR(sc, 0);
	CSR_WRITE_4(sc, JME_TXDBA_HI, JME_ADDR_HI(paddr));
	CSR_WRITE_4(sc, JME_TXDBA_LO, JME_ADDR_LO(paddr));

	/* Configure TxMAC parameters. */
	reg = TXMAC_IFG1_DEFAULT | TXMAC_IFG2_DEFAULT | TXMAC_IFG_ENB;
	reg |= TXMAC_THRESH_1_PKT;
	reg |= TXMAC_CRC_ENB | TXMAC_PAD_ENB;
	CSR_WRITE_4(sc, JME_TXMAC, reg);

	/*
	 * Configure Rx queue.
	 *  FIFO full threshold for transmitting Tx pause packet : 128T
	 *  FIFO threshold for processing next packet : 128QW
	 *  Rx queue 0 select
	 *  Max Rx DMA length : 128
	 *  Rx descriptor retry : 32
	 *  Rx descriptor retry time gap : 256ns
	 *  Don't receive runt/bad frame.
	 */
	sc->jme_rxcsr = RXCSR_FIFO_FTHRESH_128T;

	/*
	 * Since Rx FIFO size is 4K bytes, receiving frames larger
	 * than 4K bytes will suffer from Rx FIFO overruns. So
	 * decrease FIFO threshold to reduce the FIFO overruns for
	 * frames larger than 4000 bytes.
	 * For best performance of standard MTU sized frames use
	 * maximum allowable FIFO threshold, which is 32QW for
	 * chips with a full mask >= 2 otherwise 128QW. FIFO
	 * thresholds of 64QW and 128QW are not valid for chips
	 * with a full mask >= 2.
	 */
	if (sc->jme_revfm >= 2)
		sc->jme_rxcsr |= RXCSR_FIFO_THRESH_16QW;
	else {
		if ((ifp->if_mtu + ETHER_HDR_LEN + ETHER_CRC_LEN +
		    ETHER_VLAN_ENCAP_LEN) > JME_RX_FIFO_SIZE)
			sc->jme_rxcsr |= RXCSR_FIFO_THRESH_16QW;
		else
			sc->jme_rxcsr |= RXCSR_FIFO_THRESH_128QW;
	}
	sc->jme_rxcsr |= sc->jme_rx_dma_size | RXCSR_RXQ_N_SEL(RXCSR_RXQ0);
	sc->jme_rxcsr |= RXCSR_DESC_RT_CNT(RXCSR_DESC_RT_CNT_DEFAULT);
	sc->jme_rxcsr |= RXCSR_DESC_RT_GAP_256 & RXCSR_DESC_RT_GAP_MASK;
	/* XXX TODO DROP_BAD */
	CSR_WRITE_4(sc, JME_RXCSR, sc->jme_rxcsr);

	/* Set Rx descriptor counter. */
	CSR_WRITE_4(sc, JME_RXQDC, JME_RX_RING_CNT);

	/* Set Rx ring address to the hardware. */
	paddr = JME_RX_RING_ADDR(sc, 0);
	CSR_WRITE_4(sc, JME_RXDBA_HI, JME_ADDR_HI(paddr));
	CSR_WRITE_4(sc, JME_RXDBA_LO, JME_ADDR_LO(paddr));

	/* Clear receive filter. */
	CSR_WRITE_4(sc, JME_RXMAC, 0);

	/* Set up the receive filter. */
	jme_iff(sc);

	jme_set_vlan(sc);

	/*
	 * Disable all WOL bits as WOL can interfere normal Rx
	 * operation. Also clear WOL detection status bits.
	 */
	reg = CSR_READ_4(sc, JME_PMCS);
	reg &= ~PMCS_WOL_ENB_MASK;
	CSR_WRITE_4(sc, JME_PMCS, reg);

	/*
	 * Pad 10bytes right before received frame. This will greatly
	 * help Rx performance on strict-alignment architectures as
	 * it does not need to copy the frame to align the payload.
	 */
	reg = CSR_READ_4(sc, JME_RXMAC);
	reg |= RXMAC_PAD_10BYTES;
	reg |= RXMAC_CSUM_ENB;
	CSR_WRITE_4(sc, JME_RXMAC, reg);

	/* Configure general purpose reg0 */
	reg = CSR_READ_4(sc, JME_GPREG0);
	reg &= ~GPREG0_PCC_UNIT_MASK;
	/* Set PCC timer resolution to micro-seconds unit. */
	reg |= GPREG0_PCC_UNIT_US;
	/*
	 * Disable all shadow register posting as we have to read
	 * JME_INTR_STATUS register in jme_intr. Also it seems
	 * that it's hard to synchronize interrupt status between
	 * hardware and software with shadow posting due to
	 * requirements of bus_dmamap_sync(9).
	 */
	reg |= GPREG0_SH_POST_DW7_DIS | GPREG0_SH_POST_DW6_DIS |
	    GPREG0_SH_POST_DW5_DIS | GPREG0_SH_POST_DW4_DIS |
	    GPREG0_SH_POST_DW3_DIS | GPREG0_SH_POST_DW2_DIS |
	    GPREG0_SH_POST_DW1_DIS | GPREG0_SH_POST_DW0_DIS;
	/* Disable posting of DW0. */
	reg &= ~GPREG0_POST_DW0_ENB;
	/* Clear PME message. */
	reg &= ~GPREG0_PME_ENB;
	/* Set PHY address. */
	reg &= ~GPREG0_PHY_ADDR_MASK;
	reg |= sc->jme_phyaddr;
	CSR_WRITE_4(sc, JME_GPREG0, reg);

	/* Configure Tx queue 0 packet completion coalescing. */
	sc->jme_tx_coal_to = PCCTX_COAL_TO_DEFAULT;
	reg = (sc->jme_tx_coal_to << PCCTX_COAL_TO_SHIFT) &
	    PCCTX_COAL_TO_MASK;
	sc->jme_tx_coal_pkt = PCCTX_COAL_PKT_DEFAULT;
	reg |= (sc->jme_tx_coal_pkt << PCCTX_COAL_PKT_SHIFT) &
	    PCCTX_COAL_PKT_MASK;
	reg |= PCCTX_COAL_TXQ0;
	CSR_WRITE_4(sc, JME_PCCTX, reg);

	/* Configure Rx queue 0 packet completion coalescing. */
	sc->jme_rx_coal_to = PCCRX_COAL_TO_DEFAULT;
	reg = (sc->jme_rx_coal_to << PCCRX_COAL_TO_SHIFT) &
	    PCCRX_COAL_TO_MASK;
	sc->jme_rx_coal_pkt = PCCRX_COAL_PKT_DEFAULT;
	reg |= (sc->jme_rx_coal_pkt << PCCRX_COAL_PKT_SHIFT) &
	    PCCRX_COAL_PKT_MASK;
	CSR_WRITE_4(sc, JME_PCCRX0, reg);

	/* Configure shadow status block but don't enable posting. */
	paddr = sc->jme_rdata.jme_ssb_block_paddr;
	CSR_WRITE_4(sc, JME_SHBASE_ADDR_HI, JME_ADDR_HI(paddr));
	CSR_WRITE_4(sc, JME_SHBASE_ADDR_LO, JME_ADDR_LO(paddr));

	/* Disable Timer 1 and Timer 2. */
	CSR_WRITE_4(sc, JME_TIMER1, 0);
	CSR_WRITE_4(sc, JME_TIMER2, 0);

	/* Configure retry transmit period, retry limit value. */
	CSR_WRITE_4(sc, JME_TXTRHD,
	    ((TXTRHD_RT_PERIOD_DEFAULT << TXTRHD_RT_PERIOD_SHIFT) &
	    TXTRHD_RT_PERIOD_MASK) |
	    ((TXTRHD_RT_LIMIT_DEFAULT << TXTRHD_RT_LIMIT_SHIFT) &
	    TXTRHD_RT_LIMIT_SHIFT));

	/* Disable RSS. */
	CSR_WRITE_4(sc, JME_RSSC, RSSC_DIS_RSS);

	/* Initialize the interrupt mask. */
	CSR_WRITE_4(sc, JME_INTR_MASK_SET, JME_INTRS);
	CSR_WRITE_4(sc, JME_INTR_STATUS, 0xFFFFFFFF);

	/*
	 * Enabling Tx/Rx DMA engines and Rx queue processing is
	 * done after detection of valid link in jme_miibus_statchg.
	 */
	sc->jme_flags &= ~JME_FLAG_LINK;

	/* Set the current media. */
	mii = &sc->sc_miibus;
	mii_mediachg(mii);

	timeout_add_sec(&sc->jme_tick_ch, 1);

	ifp->if_flags |= IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);

	return (0);
}

void
jme_stop(struct jme_softc *sc)
{
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	struct jme_txdesc *txd;
	struct jme_rxdesc *rxd;
	int i;

	/*
	 * Mark the interface down and cancel the watchdog timer.
	 */
	ifp->if_flags &= ~IFF_RUNNING;
	ifq_clr_oactive(&ifp->if_snd);
	ifp->if_timer = 0;

	timeout_del(&sc->jme_tick_ch);
	sc->jme_flags &= ~JME_FLAG_LINK;

	/*
	 * Disable interrupts.
	 */
	CSR_WRITE_4(sc, JME_INTR_MASK_CLR, JME_INTRS);
	CSR_WRITE_4(sc, JME_INTR_STATUS, 0xFFFFFFFF);

	/* Disable updating shadow status block. */
	CSR_WRITE_4(sc, JME_SHBASE_ADDR_LO,
	    CSR_READ_4(sc, JME_SHBASE_ADDR_LO) & ~SHBASE_POST_ENB);

	/* Stop receiver, transmitter. */
	jme_stop_rx(sc);
	jme_stop_tx(sc);

#ifdef foo
	 /* Reclaim Rx/Tx buffers that have been completed. */
	jme_rxeof(sc);
	m_freem(sc->jme_cdata.jme_rxhead);
	JME_RXCHAIN_RESET(sc);
	jme_txeof(sc);
#endif

	/*
	 * Free partial finished RX segments
	 */
	m_freem(sc->jme_cdata.jme_rxhead);
	JME_RXCHAIN_RESET(sc);

	/*
	 * Free RX and TX mbufs still in the queues.
	 */
	for (i = 0; i < JME_RX_RING_CNT; i++) {
		rxd = &sc->jme_cdata.jme_rxdesc[i];
		if (rxd->rx_m != NULL) {
			bus_dmamap_unload(sc->sc_dmat, rxd->rx_dmamap);
			m_freem(rxd->rx_m);
			rxd->rx_m = NULL;
		}
        }
	for (i = 0; i < JME_TX_RING_CNT; i++) {
		txd = &sc->jme_cdata.jme_txdesc[i];
		if (txd->tx_m != NULL) {
			bus_dmamap_unload(sc->sc_dmat, txd->tx_dmamap);
			m_freem(txd->tx_m);
			txd->tx_m = NULL;
			txd->tx_ndesc = 0;
		}
        }
}

void
jme_stop_tx(struct jme_softc *sc)
{
	uint32_t reg;
	int i;

	reg = CSR_READ_4(sc, JME_TXCSR);
	if ((reg & TXCSR_TX_ENB) == 0)
		return;
	reg &= ~TXCSR_TX_ENB;
	CSR_WRITE_4(sc, JME_TXCSR, reg);
	for (i = JME_TIMEOUT; i > 0; i--) {
		DELAY(1);
		if ((CSR_READ_4(sc, JME_TXCSR) & TXCSR_TX_ENB) == 0)
			break;
	}
	if (i == 0)
		printf("%s: stopping transmitter timeout!\n",
		    sc->sc_dev.dv_xname);
}

void
jme_stop_rx(struct jme_softc *sc)
{
	uint32_t reg;
	int i;

	reg = CSR_READ_4(sc, JME_RXCSR);
	if ((reg & RXCSR_RX_ENB) == 0)
		return;
	reg &= ~RXCSR_RX_ENB;
	CSR_WRITE_4(sc, JME_RXCSR, reg);
	for (i = JME_TIMEOUT; i > 0; i--) {
		DELAY(1);
		if ((CSR_READ_4(sc, JME_RXCSR) & RXCSR_RX_ENB) == 0)
			break;
	}
	if (i == 0)
		printf("%s: stopping recevier timeout!\n", sc->sc_dev.dv_xname);
}

void
jme_init_tx_ring(struct jme_softc *sc)
{
	struct jme_ring_data *rd;
	struct jme_txdesc *txd;
	int i;

	sc->jme_cdata.jme_tx_prod = 0;
	sc->jme_cdata.jme_tx_cons = 0;
	sc->jme_cdata.jme_tx_cnt = 0;

	rd = &sc->jme_rdata;
	bzero(rd->jme_tx_ring, JME_TX_RING_SIZE);
	for (i = 0; i < JME_TX_RING_CNT; i++) {
		txd = &sc->jme_cdata.jme_txdesc[i];
		txd->tx_m = NULL;
		txd->tx_desc = &rd->jme_tx_ring[i];
		txd->tx_ndesc = 0;
	}

	bus_dmamap_sync(sc->sc_dmat, sc->jme_cdata.jme_tx_ring_map, 0,
	    sc->jme_cdata.jme_tx_ring_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
}

void
jme_init_ssb(struct jme_softc *sc)
{
	struct jme_ring_data *rd;

	rd = &sc->jme_rdata;
	bzero(rd->jme_ssb_block, JME_SSB_SIZE);
	bus_dmamap_sync(sc->sc_dmat, sc->jme_cdata.jme_ssb_map, 0,
	    sc->jme_cdata.jme_ssb_map->dm_mapsize, BUS_DMASYNC_PREWRITE);
}

int
jme_init_rx_ring(struct jme_softc *sc)
{
	struct jme_ring_data *rd;
	struct jme_rxdesc *rxd;
	int i;

	KASSERT(sc->jme_cdata.jme_rxhead == NULL &&
		 sc->jme_cdata.jme_rxtail == NULL &&
		 sc->jme_cdata.jme_rxlen == 0);
	sc->jme_cdata.jme_rx_cons = 0;

	rd = &sc->jme_rdata;
	bzero(rd->jme_rx_ring, JME_RX_RING_SIZE);
	for (i = 0; i < JME_RX_RING_CNT; i++) {
		int error;

		rxd = &sc->jme_cdata.jme_rxdesc[i];
		rxd->rx_m = NULL;
		rxd->rx_desc = &rd->jme_rx_ring[i];
		error = jme_newbuf(sc, rxd);
		if (error)
			return (error);
	}

	bus_dmamap_sync(sc->sc_dmat, sc->jme_cdata.jme_rx_ring_map, 0,
	    sc->jme_cdata.jme_rx_ring_map->dm_mapsize, BUS_DMASYNC_PREWRITE);

	return (0);
}

int
jme_newbuf(struct jme_softc *sc, struct jme_rxdesc *rxd)
{
	struct jme_desc *desc;
	struct mbuf *m;
	bus_dmamap_t map;
	int error;

	MGETHDR(m, M_DONTWAIT, MT_DATA);
	if (m == NULL)
		return (ENOBUFS);
	MCLGET(m, M_DONTWAIT);
	if (!(m->m_flags & M_EXT)) {
		m_freem(m);
		return (ENOBUFS);
	}

	/*
	 * JMC250 has 64bit boundary alignment limitation so jme(4)
	 * takes advantage of 10 bytes padding feature of hardware
	 * in order not to copy entire frame to align IP header on
	 * 32bit boundary.
	 */
	m->m_len = m->m_pkthdr.len = MCLBYTES;

	error = bus_dmamap_load_mbuf(sc->sc_dmat,
	    sc->jme_cdata.jme_rx_sparemap, m, BUS_DMA_NOWAIT);

	if (error != 0) {
		m_freem(m);
		printf("%s: can't load RX mbuf\n", sc->sc_dev.dv_xname);
		return (error);
	}

	if (rxd->rx_m != NULL) {
		bus_dmamap_sync(sc->sc_dmat, rxd->rx_dmamap, 0,
		    rxd->rx_dmamap->dm_mapsize, BUS_DMASYNC_POSTREAD);
		bus_dmamap_unload(sc->sc_dmat, rxd->rx_dmamap);
	}
	map = rxd->rx_dmamap;
	rxd->rx_dmamap = sc->jme_cdata.jme_rx_sparemap;
	sc->jme_cdata.jme_rx_sparemap = map;
	rxd->rx_m = m;

	desc = rxd->rx_desc;
	desc->buflen = htole32(rxd->rx_dmamap->dm_segs[0].ds_len);
	desc->addr_lo =
	    htole32(JME_ADDR_LO(rxd->rx_dmamap->dm_segs[0].ds_addr));
	desc->addr_hi =
	    htole32(JME_ADDR_HI(rxd->rx_dmamap->dm_segs[0].ds_addr));
	desc->flags = htole32(JME_RD_OWN | JME_RD_INTR | JME_RD_64BIT);

	return (0);
}

void
jme_set_vlan(struct jme_softc *sc)
{
	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
	uint32_t reg;

	reg = CSR_READ_4(sc, JME_RXMAC);
	reg &= ~RXMAC_VLAN_ENB;
	if (ifp->if_capabilities & IFCAP_VLAN_HWTAGGING)
		reg |= RXMAC_VLAN_ENB;
	CSR_WRITE_4(sc, JME_RXMAC, reg);
}

void
jme_iff(struct jme_softc *sc)
{
	struct arpcom *ac = &sc->sc_arpcom;
	struct ifnet *ifp = &ac->ac_if;
	struct ether_multi *enm;
	struct ether_multistep step;
	uint32_t crc;
	uint32_t mchash[2];
	uint32_t rxcfg;

	rxcfg = CSR_READ_4(sc, JME_RXMAC);
	rxcfg &= ~(RXMAC_BROADCAST | RXMAC_PROMISC | RXMAC_MULTICAST |
	    RXMAC_ALLMULTI);
	ifp->if_flags &= ~IFF_ALLMULTI;

	/*
	 * Always accept frames destined to our station address.
	 * Always accept broadcast frames.
	 */
	rxcfg |= RXMAC_UNICAST | RXMAC_BROADCAST;

	if (ifp->if_flags & IFF_PROMISC || ac->ac_multirangecnt > 0) {
		ifp->if_flags |= IFF_ALLMULTI;
		if (ifp->if_flags & IFF_PROMISC)
			rxcfg |= RXMAC_PROMISC;
		else
			rxcfg |= RXMAC_ALLMULTI;
		mchash[0] = mchash[1] = 0xFFFFFFFF;
	} else {
		/*
		 * Set up the multicast address filter by passing all
		 * multicast addresses through a CRC generator, and then
		 * using the low-order 6 bits as an index into the 64 bit
		 * multicast hash table.  The high order bits select the
		 * register, while the rest of the bits select the bit
		 * within the register.
		 */
		rxcfg |= RXMAC_MULTICAST;
		bzero(mchash, sizeof(mchash));

		ETHER_FIRST_MULTI(step, ac, enm);
		while (enm != NULL) {
			crc = ether_crc32_be(enm->enm_addrlo, ETHER_ADDR_LEN);

			/* Just want the 6 least significant bits. */
			crc &= 0x3f;

			/* Set the corresponding bit in the hash table. */
			mchash[crc >> 5] |= 1 << (crc & 0x1f);

			ETHER_NEXT_MULTI(step, enm);
		}
	}

	CSR_WRITE_4(sc, JME_MAR0, mchash[0]);
	CSR_WRITE_4(sc, JME_MAR1, mchash[1]);
	CSR_WRITE_4(sc, JME_RXMAC, rxcfg);
}
@


1.49
log
@move counting if_opackets next to counting if_obytes in if_enqueue.

this means packets are consistently counted in one place, unlike the
many and various ways that drivers thought they should do it.

ok mpi@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.48 2016/11/29 10:22:30 jsg Exp $	*/
a37 1
#include <sys/types.h>
@


1.48
log
@m_free() and m_freem() test for NULL.  Simplify callers which had their own
NULL tests.

ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.47 2016/04/13 10:34:32 mpi Exp $	*/
a1527 1
			ifp->if_opackets++;
@


1.47
log
@G/C IFQ_SET_READY().
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.46 2016/03/15 16:45:52 naddy Exp $	*/
d261 1
a261 2
	if (sc->jme_cdata.jme_rxhead != NULL)
		m_freem(sc->jme_cdata.jme_rxhead);
d2044 1
a2044 2
	if (sc->jme_cdata.jme_rxhead != NULL)
		m_freem(sc->jme_cdata.jme_rxhead);
d2052 1
a2052 2
	if (sc->jme_cdata.jme_rxhead != NULL)
		m_freem(sc->jme_cdata.jme_rxhead);
@


1.46
log
@Ethernet drivers no longer need to include if_vlan_var.h for the VLAN
definitions; ok mpi@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.45 2015/11/25 03:09:59 dlg Exp $	*/
a667 1
	IFQ_SET_READY(&ifp->if_snd);
@


1.45
log
@replace IFF_OACTIVE manipulation with mpsafe operations.

there are two things shared between the network stack and drivers
in the send path: the send queue and the IFF_OACTIVE flag. the send
queue is now protected by a mutex. this diff makes the oactive
functionality mpsafe too.

IFF_OACTIVE is part of if_flags. there are two problems with that.
firstly, if_flags is a short and we dont have any MI atomic operations
to manipulate a short. secondly, while we could make the IFF_OACTIVE
operates mpsafe, all changes to other flags would have to be made
safe at the same time, otherwise a read-modify-write cycle on their
updates could clobber the oactive change.

instead, this moves the oactive mark into struct ifqueue and provides
an API for changing it. there's ifq_set_oactive, ifq_clr_oactive,
and ifq_is_oactive. these are modelled on ifsq_set_oactive,
ifsq_clr_oactive, and ifsq_is_oactive in dragonflybsd.

this diff includes changes to all the drivers manipulating IFF_OACTIVE
to now use the ifsq_{set,clr_is}_oactive API too.

ok kettenis@@ mpi@@ jmatthew@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.44 2015/11/24 12:32:53 mpi Exp $	*/
a54 2

#include <net/if_vlan_var.h>
@


1.44
log
@No need to include <net/if_types.h> for <net/if_vlan_var.h>
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.43 2015/11/09 00:29:06 dlg Exp $	*/
d253 2
a254 1
	ifp->if_flags &= ~(IFF_RUNNING | IFF_OACTIVE);
d317 1
a317 1
	ifp->if_flags &= ~IFF_OACTIVE;
d1209 1
a1209 1
	if ((ifp->if_flags & (IFF_RUNNING | IFF_OACTIVE)) != IFF_RUNNING)
d1223 1
a1223 1
			ifp->if_flags |= IFF_OACTIVE;
d1569 1
a1569 1
		ifp->if_flags &= ~IFF_OACTIVE;
d2008 1
a2008 1
	ifp->if_flags &= ~IFF_OACTIVE;
d2024 2
a2025 1
	ifp->if_flags &= ~(IFF_RUNNING | IFF_OACTIVE);
@


1.43
log
@rework the start routines to avoid IF_PREPEND.

IF_PREPEND assumes the underlying send queue is priq, while hfsc may be
enabled on it.

the previous code pattern to DEQUEUE, try and encap the mbuf on the
ring, and if that failed cos there was no space it would PREPEND
it.

now it checks for space on the ring before it attempts to DEQUEUE.
failure to encap means the mbuf is now unconditionally dropped.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.42 2015/10/25 13:04:28 mpi Exp $	*/
a55 1
#include <net/if_types.h>
@


1.42
log
@arp_ifinit() is no longer needed.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.41 2015/06/24 09:40:54 mpi Exp $	*/
d103 1
a103 1
int	jme_encap(struct jme_softc *, struct mbuf **);
d1111 1
a1111 1
jme_encap(struct jme_softc *sc, struct mbuf **m_head)
a1114 1
	struct mbuf *m;
d1122 1
a1122 1
				     *m_head, BUS_DMA_NOWAIT);
d1126 1
a1126 1
		if (m_defrag(*m_head, M_DONTWAIT)) {
d1131 1
a1131 1
					     *m_head, BUS_DMA_NOWAIT);
a1135 12
	/*
	 * Check descriptor overrun. Leave one free descriptor.
	 * Since we always use 64bit address mode for transmitting,
	 * each Tx request requires one more dummy descriptor.
	 */
	if (sc->jme_cdata.jme_tx_cnt + txd->tx_dmamap->dm_nsegs + JME_TXD_RSVD >
	    JME_TX_RING_CNT - JME_TXD_RSVD) {
		bus_dmamap_unload(sc->sc_dmat, txd->tx_dmamap);
		return (ENOBUFS);
	}

	m = *m_head;
d1194 1
a1194 2
	m_freem(*m_head);
	*m_head = NULL;
d1202 1
a1202 1
	struct mbuf *m_head;
d1227 2
a1228 2
		IFQ_DEQUEUE(&ifp->if_snd, m_head);
		if (m_head == NULL)
d1236 3
a1238 8
		if (jme_encap(sc, &m_head)) {
			if (m_head == NULL)
				ifp->if_oerrors++;
			else {
				IF_PREPEND(&ifp->if_snd, m_head);
				ifp->if_flags |= IFF_OACTIVE;
			}
			break;
d1249 1
a1249 1
			bpf_mtap_ether(ifp->if_bpf, m_head, BPF_DIRECTION_OUT);
@


1.41
log
@Increment if_ipackets in if_input().

Note that pseudo-drivers not using if_input() are not affected by this
conversion.

ok mikeb@@, kettenis@@, claudio@@, dlg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.40 2015/04/30 07:52:00 mpi Exp $	*/
a1317 1
	struct ifaddr *ifa = (struct ifaddr *)data;
a1327 2
		if (ifa->ifa_addr->sa_family == AF_INET)
			arp_ifinit(&sc->sc_arpcom, ifa);
@


1.40
log
@No need to set `rcvif', if_input() does it for you!
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.39 2015/04/10 16:04:47 mpi Exp $	*/
a1727 2

			ifp->if_ipackets++;
@


1.39
log
@Convert to if_input().

Tested by Daniel Jakots, thanks!
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.38 2015/03/14 03:38:48 jsg Exp $	*/
a1695 1
			m->m_pkthdr.rcvif = ifp;
@


1.38
log
@Remove some includes include-what-you-use claims don't
have any direct symbols used.  Tested for indirect use by compiling
amd64/i386/sparc64 kernels.

ok tedu@@ deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.37 2014/12/22 02:28:52 tedu Exp $	*/
d1618 1
d1730 1
a1730 5
#if NBPFILTER > 0
			if (ifp->if_bpf)
				bpf_mtap_ether(ifp->if_bpf, m,
				    BPF_DIRECTION_IN);
#endif
d1732 1
a1732 3
			ifp->if_ipackets++;
			/* Pass it on. */
			ether_input_mbuf(ifp, m);
d1738 2
@


1.37
log
@unifdef INET
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.36 2014/07/22 13:12:11 mpi Exp $	*/
a62 1
#include <dev/mii/mii.h>
@


1.36
log
@Fewer <netinet/in_systm.h>
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.35 2014/01/27 12:04:46 brad Exp $	*/
a52 1
#ifdef INET
a54 1
#endif
a1329 1
#ifdef INET
a1331 1
#endif
@


1.35
log
@Enable IPv6 checksum offload.

ok naddy@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.34 2014/01/10 22:01:30 brad Exp $	*/
a54 2
#include <netinet/in_systm.h>
#include <netinet/ip.h>
@


1.34
log
@Add MSI support.

Tested by comete@@daknet.org and vigdis+obsd@@chown.me.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.33 2013/12/07 07:22:37 brad Exp $	*/
d679 2
a680 1
	    IFCAP_CSUM_TCPv4 | IFCAP_CSUM_UDPv4;
@


1.33
log
@Some fixes for jme_encap()..

- Remove the maximum DMA segments handling bits as it is unused between
  DragonFly and OpenBSD.
- Fix error handling for bus_dmamap_load_mbuf() so as to not try unloading
  a DMA map that had not already been loaded.
- Clean up the DMA chain defragmenting path to remove unwanted printfs and
  simplify things a bit.
- Have jme_encap() check the number of mapped DMA segments against the TX
  ring to see if it'll fit as do most of the driver nowadays.
- Remove the KASSERT's that shouldn't be there.
- Simplify the dummy descriptor handling to be closer to the FreeBSD
  driver since unlike the DragonFly driver this originated from our
  driver always uses the 64-bit dummy descriptor.
- If the ring was full make sure to IF_PREPEND() the packet back on
  the queue since it wasn't transmitted.

Tested by myself, comete@@daknet.org and vigdis+obsd@@chown.me.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.32 2013/11/03 23:27:33 brad Exp $	*/
d82 1
d473 70
d573 3
a575 1
	if (pci_intr_map(pa, &ih) != 0) {
@


1.32
log
@Simplify jme_miibus_readreg() / jme_miibus_writereg() a bit by using the
mii_attach() parameter to be able to specify the location of the PHY.

From FreeBSD

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.31 2013/11/03 04:24:28 brad Exp $	*/
a1046 1
	int maxsegs;
a1052 8
	maxsegs = (JME_TX_RING_CNT - sc->jme_cdata.jme_tx_cnt) -
		  (JME_TXD_RSVD + 1);
	if (maxsegs > JME_MAXTXSEGS)
		maxsegs = JME_MAXTXSEGS;
	if (maxsegs < (sc->jme_txd_spare - 1))
		panic("%s: not enough segments %d", sc->sc_dev.dv_xname,
		    maxsegs);

d1055 2
d1058 17
d1076 1
a1076 23
		error = EFBIG;
	}
	if (error == EFBIG) {
		if (m_defrag(*m_head, M_DONTWAIT)) {
			printf("%s: can't defrag TX mbuf\n",
			    sc->sc_dev.dv_xname);
			m_freem(*m_head);
			*m_head = NULL;
			return (ENOBUFS);
		}
		error = bus_dmamap_load_mbuf(sc->sc_dmat,
					     txd->tx_dmamap, *m_head,
					     BUS_DMA_NOWAIT);
		if (error != 0) {
			printf("%s: could not load defragged TX mbuf\n",
			    sc->sc_dev.dv_xname);
			m_freem(*m_head);
			*m_head = NULL;
			return (error);
		}
	} else if (error) {
		printf("%s: could not load TX mbuf\n", sc->sc_dev.dv_xname);
		return (error);
a1103 1
	KASSERT(sc->jme_cdata.jme_tx_cnt < JME_TX_RING_CNT - JME_TXD_RSVD);
a1112 1

a1113 2
		KASSERT(sc->jme_cdata.jme_tx_cnt <=
			 JME_TX_RING_CNT - JME_TXD_RSVD);
d1127 1
a1127 1
	txd->tx_ndesc = txd->tx_dmamap->dm_nsegs + 1;
d1136 5
d1166 1
a1166 1
		if (sc->jme_cdata.jme_tx_cnt + sc->jme_txd_spare >
d1182 1
a1182 1
			if (m_head == NULL) {
d1184 3
a1186 1
				break;
a1187 1
			ifp->if_flags |= IFF_OACTIVE;
d1190 1
d1522 1
a1522 1
	if (sc->jme_cdata.jme_tx_cnt + sc->jme_txd_spare <=
a1768 8

	/*
	 * Since we always use 64bit address mode for transmitting,
	 * each Tx request requires one more dummy descriptor.
	 */
	sc->jme_txd_spare =
	howmany(ifp->if_mtu + sizeof(struct ether_vlan_header), MCLBYTES) + 1;
	KASSERT(sc->jme_txd_spare >= 2);
@


1.31
log
@Enable TX checksum offload.

ok naddy@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.30 2013/08/07 01:06:35 bluhm Exp $	*/
d157 2
a158 7
	if (sc->jme_caps & JME_CAP_FPGA) {
		if (phy == 0)
			return (0);
	} else {
		if (sc->jme_phyaddr != phy)
			return (0);
	}
d187 2
a188 7
	if (sc->jme_caps & JME_CAP_FPGA) {
		if (phy == 0)
			return;
	} else {
		if (sc->jme_phyaddr != phy)
			return;
	}
d620 2
a621 1
	mii_attach(self, &sc->sc_miibus, 0xffffffff, MII_PHY_ANY,
@


1.30
log
@Most network drivers include netinet/in_var.h, but apparently they
don't have to.  Just remove these include lines.
Compiled on amd64 i386 sparc64; OK henning@@ mikeb@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.29 2012/11/29 21:10:32 brad Exp $	*/
d615 2
a616 6
	ifp->if_capabilities = IFCAP_VLAN_MTU;

#ifdef JME_CHECKSUM
	ifp->if_capabilities |= IFCAP_CSUM_IPv4 | IFCAP_CSUM_TCPv4 |
				IFCAP_CSUM_UDPv4;
#endif
@


1.29
log
@Remove setting an initial assumed baudrate upon driver attach which is not
necessarily correct, there might not even be a link when attaching.

ok mikeb@@ reyk@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.28 2012/10/22 09:19:17 brad Exp $	*/
a55 1
#include <netinet/in_var.h>
@


1.28
log
@Add flow control support.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.27 2012/02/28 03:58:16 jsg Exp $	*/
a611 1
	ifp->if_baudrate = IF_Gbps(1);
@


1.27
log
@- Always try to reclaim transmitted frames instead of returning
  from jme_start() if IFF_OACTIVE is set before that happens; as
  the original driver did before it was ported to DragonFly
- Return from jme_start() if there is no link or an empty queue.

prevents watchdog timeouts when there is no link present. from brad
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.26 2012/02/08 13:16:59 jsg Exp $	*/
d637 1
a637 1
	    MII_OFFSET_ANY, 0);
a1345 1
#ifdef notyet
a1349 1
#endif
@


1.26
log
@Remove the init path of jme_newbuf that used M_WAITOK
as this can be called from an interrupt context.

From brad.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.25 2011/04/05 18:01:21 henning Exp $	*/
d1176 4
d1182 4
a1185 3

	if (sc->jme_cdata.jme_tx_cnt >= JME_TX_DESC_HIWAT)
		jme_txeof(sc);
d1258 1
a1258 2
		if (!IFQ_IS_EMPTY(&ifp->if_snd))
			jme_start(ifp);
d1265 1
a1265 3

	if (!IFQ_IS_EMPTY(&ifp->if_snd))
		jme_start(ifp);
d1470 1
a1470 2
			if (!IFQ_IS_EMPTY(&ifp->if_snd))
				jme_start(ifp);
@


1.25
log
@mechanic rename M_{TCP|UDP}V4_CSUM_OUT -> M_{TCP|UDP}_CSUM_OUT
ok claudio krw
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.24 2010/08/27 17:08:00 jsg Exp $	*/
d107 1
a107 1
int	jme_newbuf(struct jme_softc *, struct jme_rxdesc *, int);
d1605 1
a1605 1
		if (jme_newbuf(sc, rxd, 0) != 0) {
d2172 1
a2172 1
		error = jme_newbuf(sc, rxd, 1);
d2184 1
a2184 1
jme_newbuf(struct jme_softc *sc, struct jme_rxdesc *rxd, int init)
d2191 1
a2191 1
	MGETHDR(m, init ? M_WAITOK : M_DONTWAIT, MT_DATA);
d2194 1
a2194 1
	MCLGET(m, init ? M_WAITOK : M_DONTWAIT);
d2209 2
a2210 2
				     sc->jme_cdata.jme_rx_sparemap,
				     m, BUS_DMA_NOWAIT);
a2211 7
		if (!error) {
			bus_dmamap_unload(sc->sc_dmat, 
					  sc->jme_cdata.jme_rx_sparemap);
			error = EFBIG;
			printf("%s: too many segments?!\n",
			    sc->sc_dev.dv_xname);
		}
d2213 1
a2213 3

		if (init)
			printf("%s: can't load RX mbuf\n", sc->sc_dev.dv_xname);
@


1.24
log
@remove the unused if_init callback in struct ifnet
ok deraadt@@ henning@@ claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.23 2010/08/07 03:50:02 krw Exp $	*/
d1112 1
a1112 1
	if (m->m_pkthdr.csum_flags & M_TCPV4_CSUM_OUT)
d1114 1
a1114 1
	if (m->m_pkthdr.csum_flags & M_UDPV4_CSUM_OUT)
@


1.23
log
@No "\n" needed at the end of panic() strings.

Bogus chunks pointed out by matthew@@ and miod@@. No cookies for
marco@@ and jasper@@.

ok deraadt@@ miod@@ matthew@@ jasper@@ macro@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.22 2010/05/19 15:27:35 oga Exp $	*/
a608 1
	ifp->if_init = jme_init;
@


1.22
log
@BUS_DMA_ZERO instead of alloc, map, bzero.

ok krw@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.21 2010/01/07 12:26:06 sthen Exp $	*/
d1075 1
a1075 1
		panic("%s: not enough segments %d\n", sc->sc_dev.dv_xname,
d1504 1
a1504 1
			panic("%s: freeing NULL mbuf!\n", sc->sc_dev.dv_xname);
d1539 1
a1539 1
			panic("%s: Active Tx desc counter was garbled\n",
@


1.21
log
@Rename _rxfilter functions to _iff for consistency. From Brad, ok kevlo@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.20 2009/09/13 14:42:52 krw Exp $	*/
d751 1
a751 1
	    BUS_DMA_WAITOK);
a763 2

	bzero(sc->jme_rdata.jme_rx_ring, JME_RX_RING_SIZE);
@


1.20
log
@M_DUP_PKTHDR() define -> m_dup_pkthdr() function to properly deal
with m_tag_copy_chain() failures.

Use m_defrag() to eliminate hand rolled defragging of mbufs and
some uses of M_DUP_PKTHDR().

Original diff from thib@@, claudio@@'s feedback integrated by me.

Tests kevlo@@ claudio@@, "reads ok" blambert@@

ok thib@@ claudio@@, "m_defrag() bits ok" kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.19 2009/06/05 06:05:06 naddy Exp $	*/
d115 1
a115 1
void	jme_set_filter(struct jme_softc *);
d1314 1
a1314 1
			jme_set_filter(sc);
d1903 2
a1904 1
	jme_set_filter(sc);
d2264 1
a2264 1
jme_set_filter(struct jme_softc *sc)
@


1.19
log
@enable IPv6 receive TCP/UDP checksum offload; from Brad
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.18 2009/03/29 21:53:52 sthen Exp $	*/
d1087 1
a1087 4
		error = 0;

		MGETHDR(m, M_DONTWAIT, MT_DATA);
		if (m == NULL) {
a1093 17

		M_DUP_PKTHDR(m, *m_head);
		if ((*m_head)->m_pkthdr.len > MHLEN) {
			MCLGET(m, M_DONTWAIT);
			if (!(m->m_flags & M_EXT)) {
				m_freem(*m_head);
				m_freem(m);
				*m_head = NULL;
				return (ENOBUFS);
			}
		}
		
		m_copydata(*m_head, 0, (*m_head)->m_pkthdr.len, mtod(m, caddr_t));
		m_freem(*m_head);
		m->m_len = m->m_pkthdr.len;
		*m_head = m;

a1099 5
			if (!error) {
				bus_dmamap_unload(sc->sc_dmat,
						  txd->tx_dmamap);
				error = EFBIG;
			}
@


1.18
log
@make various strings ("can't map mem space" and similar) more consistent
between instances, saving space in the kernel. feedback from many (some
incorporated, some left for future work).

ok deraadt, kettenis, "why not" miod.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.17 2009/02/25 13:10:38 jsg Exp $	*/
d1692 5
a1696 3
			if (flags & JME_RD_IPV4) {
				if (flags & JME_RD_IPCSUM)
					m->m_pkthdr.csum_flags |= M_IPV4_CSUM_IN_OK;
@


1.17
log
@Fix up multicast support, from brad.
ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.16 2009/02/25 11:41:58 jsg Exp $	*/
d509 1
a509 1
		printf(": could not map mem space\n");
d514 1
a514 1
		printf(": could not map interrupt\n");
@


1.16
log
@On full mask revision 2 or later default to
16QW for "FIFO Threshold for processing next packet"
to increase compatibility with different PCI-E implementations
as advised by the datasheet.

The 64WQ and 128WQ options are obsolete on full revision
mask >= 2 as well.

From Pyun YongHyeon in FreeBSD via brad. ok deraadt@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.15 2009/01/10 15:33:05 kevlo Exp $	*/
d1319 1
a1319 1
				jme_set_filter(sc);
a1327 13
	case SIOCADDMULTI:
	case SIOCDELMULTI:
		error = (cmd == SIOCADDMULTI) ?
		    ether_addmulti(ifr, &sc->sc_arpcom) :
		    ether_delmulti(ifr, &sc->sc_arpcom);

		if (error == ENETRESET) {
			if (ifp->if_flags & IFF_RUNNING)
				jme_set_filter(sc);
			error = 0;
		}
		break;

d2299 1
d2307 2
a2308 1
	if (ifp->if_flags & (IFF_PROMISC | IFF_ALLMULTI)) {
d2311 1
a2311 1
		if (ifp->if_flags & IFF_ALLMULTI)
d2313 12
a2324 5
		CSR_WRITE_4(sc, JME_MAR0, 0xFFFFFFFF);
		CSR_WRITE_4(sc, JME_MAR1, 0xFFFFFFFF);
		CSR_WRITE_4(sc, JME_RXMAC, rxcfg);
		return;
	}
d2326 3
a2328 14
	/*
	 * Set up the multicast address filter by passing all multicast
	 * addresses through a CRC generator, and then using the low-order
	 * 6 bits as an index into the 64 bit multicast hash table.  The
	 * high order bits select the register, while the rest of the bits
	 * select the bit within the register.
	 */
	rxcfg |= RXMAC_MULTICAST;
	bzero(mchash, sizeof(mchash));

	ETHER_FIRST_MULTI(step, ac, enm);
	while (enm != NULL) {
		crc = ether_crc32_be(LLADDR((struct sockaddr_dl *)
		    enm->enm_addrlo), ETHER_ADDR_LEN);
d2330 2
a2331 2
		/* Just want the 6 least significant bits. */
		crc &= 0x3f;
d2333 2
a2334 2
		/* Set the corresponding bit in the hash table. */
		mchash[crc >> 5] |= 1 << (crc & 0x1f);
d2336 2
a2337 1
		ETHER_NEXT_MULTI(step, enm);
@


1.15
log
@- remove unused variable
- eliminate hardcoded return value of jme_init_rx_ring()

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.14 2008/12/01 09:12:59 jsg Exp $	*/
d1900 1
d1907 4
a1910 1
	 * maximum allowable FIFO threshold, 128QW.
d1912 1
a1912 2
	if ((ifp->if_mtu + ETHER_HDR_LEN + ETHER_CRC_LEN + ETHER_VLAN_ENCAP_LEN) >
	    JME_RX_FIFO_SIZE)
d1914 7
a1920 2
	else
		sc->jme_rxcsr |= RXCSR_FIFO_THRESH_128QW;
@


1.14
log
@Configure the clock sources for tx mac/offload engines, required
for newer revisions, makes the JMC261 work.

Thanks to JMicron for supplying hardware to test against.

ok brad@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.13 2008/11/09 15:08:26 naddy Exp $	*/
a855 1
			sc->jme_cdata.jme_tx_tag = NULL;
d1848 1
a1848 1
		return (1);
@


1.13
log
@Introduce bpf_mtap_ether(), which for the benefit of bpf listeners
creates the VLAN encapsulation from the tag stored in the mbuf
header.  Idea from FreeBSD, input from claudio@@ and canacar@@.

Switch all hardware VLAN enabled drivers to the new function.

ok claudio@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.12 2008/10/29 01:55:53 brad Exp $	*/
d553 2
d1443 9
@


1.12
log
@- Add a workaround field to the softc struct.
- Move the extended FIFO workaround from the capabilities field to the
  workaround field.
- Add General purpose register 1 defines for the workarounds.
- Enable a workaround for CRC errors that can be experienced with A2 revision
  adapters.
- Add a workaround for packet loss that can be experienced with A2 revision
  adapters when in 10/100 mode with half duplex.

From DraonFly

- Rename the workaround flags so they describe what is being worked around.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.11 2008/10/21 19:41:13 brad Exp $	*/
d1245 1
a1245 1
			bpf_mtap(ifp->if_bpf, m_head, BPF_DIRECTION_OUT);
d1718 2
a1719 1
				bpf_mtap(ifp->if_bpf, m, BPF_DIRECTION_IN);
@


1.11
log
@Make sure to read the last byte of EEPROM descriptor. Previously
the last byte of the Ethernet address was not read which in turn
resulted in getting 5 out of the 6 bytes of Ethernet address and
always returning ENOENT.

From FreeBSD

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.10 2008/10/21 19:39:43 brad Exp $	*/
d553 4
d1363 2
a1364 2
	uint32_t ghc, rxmac, txmac, txpause;
	int phyconf = JMPHY_CONF_DEFFIFO;
d1401 9
a1409 1
	/* Reprogram Tx/Rx MACs with resolved speed/duplex. */
d1413 2
d1419 2
d1434 1
a1434 1
		if ((IFM_OPTIONS(mii->mii_media_active) & IFM_FDX) == 0)
d1446 1
a1446 1
	if (sc->jme_caps & JME_CAP_EXTFIFO) {
d1450 2
@


1.10
log
@Don't bounce transmitted packets up to the BPF listeners twice.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.9 2008/10/20 19:40:54 brad Exp $	*/
d438 2
a439 5
		/* Check for the end of EEPROM descriptor. */
		if ((fup & JME_EEPROM_DESC_END) == JME_EEPROM_DESC_END)
			break;
		if ((uint8_t)JME_EEPROM_MKDESC(JME_EEPROM_FUNC0,
		    JME_EEPROM_PAGE_BAR1) == fup) {
d451 3
@


1.9
log
@oops, testing printf shouldn't have been commited.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.8 2008/10/20 19:39:37 brad Exp $	*/
a1219 5
#if NBPFILTER > 0
		if (ifp->if_bpf != NULL)
			bpf_mtap(ifp->if_bpf, m_head, BPF_DIRECTION_OUT);
#endif

d1235 1
a1239 1
#if NBPFILTER > 0
@


1.8
log
@Enable support for RX checksum offload, RX VLAN tag stripping and TX
VLAN tag insertion.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.7 2008/10/20 19:36:54 brad Exp $	*/
a1144 1
		printf("%s: VLAN tag\n", sc->sc_dev.dv_xname);
@


1.7
log
@Remove ioctl handlers for MTU changing and multicast, they're already
handled by ether_ioctl() and simplify the interface flags handler.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.6 2008/10/14 11:45:40 jsg Exp $	*/
d612 9
a620 7
	/* JMC250 supports Tx/Rx checksum offload and hardware vlan tagging. */
#if 0
	ifp->if_capabilities = IFCAP_HWCSUM |
			       IFCAP_VLAN_MTU |
			       IFCAP_VLAN_HWTAGGING;
	ifp->if_hwassist = JME_CSUM_FEATURES;
	ifp->if_capenable = ifp->if_capabilities;
a621 1
	ifp->if_capabilities = IFCAP_VLAN_MTU;
a1131 1
#if 0
d1133 1
a1133 1
	if (m->m_pkthdr.csum_flags & CSUM_IP)
d1135 1
a1135 1
	if (m->m_pkthdr.csum_flags & CSUM_TCP)
d1137 1
a1137 1
	if (m->m_pkthdr.csum_flags & CSUM_UDP)
d1140 1
d1143 1
a1143 1
		cflags |= (m->m_pkthdr.ether_vlantag & JME_TD_VLAN_MASK);
d1145 1
a1681 1
#if 0
d1683 1
a1683 3
			if ((ifp->if_capenable & IFCAP_RXCSUM) &&
			    (flags & JME_RD_IPV4)) {
				m->m_pkthdr.csum_flags |= CSUM_IP_CHECKED;
d1685 1
a1685 1
					m->m_pkthdr.csum_flags |= CSUM_IP_VALID;
d1692 1
a1692 2
					    CSUM_DATA_VALID | CSUM_PSEUDO_HDR;
					m->m_pkthdr.csum_data = 0xffff;
d1696 1
d1698 2
a1699 4
			if ((ifp->if_capenable & IFCAP_VLAN_HWTAGGING) &&
			    (flags & JME_RD_VLAN_TAG)) {
				m->m_pkthdr.ether_vlantag =
				    flags & JME_RD_VLAN_MASK;
d1885 1
a1885 1
	if ((ifp->if_mtu + ETHER_HDR_LEN + EVL_ENCAPLEN + ETHER_CRC_LEN) >
d1926 1
a1926 5

#if 0
	if (ifp->if_capenable & IFCAP_RXCSUM)
		reg |= RXMAC_CSUM_ENB;
#endif
d2257 1
a2257 1
/*	struct ifnet *ifp = &sc->sc_arpcom.ac_if; */
d2262 1
a2262 2
#if 0
	if (ifp->if_capenable & IFCAP_VLAN_HWTAGGING)
a2263 1
#endif
@


1.6
log
@remove C99/C++ style comments
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.5 2008/10/14 11:41:47 jsg Exp $	*/
d1298 1
a1299 1
	struct ifaddr *ifa = (struct ifaddr *)data;
a1313 8
	case SIOCSIFMTU:
#if 0
		if (ifr->ifr_mtu < ETHERMIN || ifr->ifr_mtu > JME_JUMBO_MTU ||
		    (!(sc->jme_caps & JME_CAP_JUMBO) &&
		     ifr->ifr_mtu > JME_MAX_MTU)) {
			error = EINVAL;
			break;
		}
d1315 2
a1316 12
		if (ifp->if_mtu != ifr->ifr_mtu) {
			/*
			 * No special configuration is required when interface
			 * MTU is changed but availability of Tx checksum
			 * offload should be chcked against new MTU size as
			 * FIFO size is just 2K.
			 */
			if (ifr->ifr_mtu >= JME_TX_FIFO_SIZE) {
				ifp->if_capenable &= ~IFCAP_TXCSUM;
				ifp->if_hwassist &= ~JME_CSUM_FEATURES;
			}
			ifp->if_mtu = ifr->ifr_mtu;
d1318 2
a1320 18
		}
#endif
		if (ifr->ifr_mtu < ETHERMIN || ifr->ifr_mtu > ifp->if_hardmtu)
			error = EINVAL;
		else if (ifp->if_mtu != ifr->ifr_mtu)
			ifp->if_mtu = ifr->ifr_mtu;
		break;

	case SIOCSIFFLAGS:
		if (ifp->if_flags & IFF_UP) {
			if (ifp->if_flags & IFF_RUNNING) {
				if ((ifp->if_flags ^ sc->jme_if_flags) &
				    (IFF_PROMISC | IFF_ALLMULTI))
					jme_set_filter(sc);
			} else {
				if (!(ifp->if_flags & IFF_RUNNING))
					jme_init(ifp);
			}
a1324 1
		sc->jme_if_flags = ifp->if_flags;
d1344 1
d1347 6
@


1.5
log
@Don't be so pessimistic about the prospects of getting a
valid MAC address.

Pyun YongHyeon seems to have taken code for generating an
address when something looks invalid from other drivers
in FreeBSD, there are no known problems with getting
valid addresses for jme, so remove this.

Additionally it isn't appropriate to call arc4random() at
that point in the boot process so the code needs to go.
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.4 2008/10/02 20:21:14 brad Exp $	*/
a81 2
//#define	JME_CSUM_FEATURES	(CSUM_IP | CSUM_TCP | CSUM_UDP)

a493 1
//	uint8_t pcie_ptr;
a494 1
//	uint8_t eaddr[ETHER_ADDR_LEN];
a1233 1
//			ifq_prepend(&ifp->if_snd, m_head);
d2292 1
a2292 1
//	struct ifnet *ifp = &sc->sc_arpcom.ac_if;
@


1.4
log
@First step towards cleaning up the Ethernet driver ioctl handling.
Move calling ether_ioctl() from the top of the ioctl function, which
at the moment does absolutely nothing, to the default switch case.
Thus allowing drivers to define their own ioctl handlers and then
falling back on ether_ioctl(). The only functional change this results
in at the moment is having all Ethernet drivers returning the proper
errno of ENOTTY instead of EINVAL/ENXIO when encountering unknown
ioctl's.

Shrinks the i386 kernels by..
RAMDISK - 1024 bytes
RAMDISKB -  1120 bytes
RAMDISKC - 832 bytes

Tested by martin@@/jsing@@/todd@@/brad@@
Build tested on almost all archs by todd@@/brad@@

ok jsing@@
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.3 2008/09/29 22:43:45 deraadt Exp $	*/
a67 2
#include <dev/rndvar.h>

d475 7
a481 19
	if ((par0 == 0 && par1 == 0) || (par0 & 0x1)) {
		printf("%s: generating fake ethernet address.\n",
		    sc->sc_dev.dv_xname);
		par0 = arc4random();
		/* Set OUI to JMicron. */
		eaddr[0] = 0x00;
		eaddr[1] = 0x1B;
		eaddr[2] = 0x8C;
		eaddr[3] = (par0 >> 16) & 0xff;
		eaddr[4] = (par0 >> 8) & 0xff;
		eaddr[5] = par0 & 0xff;
	} else {
		eaddr[0] = (par0 >> 0) & 0xFF;
		eaddr[1] = (par0 >> 8) & 0xFF;
		eaddr[2] = (par0 >> 16) & 0xFF;
		eaddr[3] = (par0 >> 24) & 0xFF;
		eaddr[4] = (par1 >> 0) & 0xFF;
		eaddr[5] = (par1 >> 8) & 0xFF;
	}
@


1.3
log
@always need the vlan includes; ok jsg
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.2 2008/09/27 13:03:30 jsg Exp $	*/
a1322 5
	if ((error = ether_ioctl(ifp, &sc->sc_arpcom, cmd, data)) > 0) {
		splx(s);
		return error;
	}

d1399 1
a1399 2
		error = ENOTTY;
		break;
@


1.2
log
@remove a debug printf that is no longer required
@
text
@d1 1
a1 1
/*	$OpenBSD: if_jme.c,v 1.1 2008/09/26 10:35:15 jsg Exp $	*/
a60 1
#if NVLAN > 0
a62 1
#endif
@


1.1
log
@Add drivers for the JMicron JMC250/JMC260 Ethernet controllers
and JMicron JMP202/JMP211 Ethernet PHYs.

Written by Pyun YongHyeon for FreeBSD, ported to DragonFlyBSD
by Sepherosa Ziehau and then ported to OpenBSD by me.

Thanks once again to JMicron for supplying hardware and
information which made this possible.

Some cleanup still needs to be done, and checksum offload
needs to be sorted out, but the driver otherwise seems
to work great.  Comitted over a JMC250 card.
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
a630 1
	printf("XNAME= '%s'\n", ifp->if_xname);
@

