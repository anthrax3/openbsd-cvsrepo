head	1.51;
access;
symbols
	OPENBSD_6_1:1.48.0.4
	OPENBSD_6_1_BASE:1.48
	OPENBSD_6_0:1.47.0.4
	OPENBSD_6_0_BASE:1.47
	OPENBSD_5_9:1.45.0.2
	OPENBSD_5_9_BASE:1.45
	OPENBSD_5_8:1.33.0.4
	OPENBSD_5_8_BASE:1.33
	OPENBSD_5_7:1.7.0.4
	OPENBSD_5_7_BASE:1.7
	OPENBSD_5_6:1.1.0.4
	OPENBSD_5_6_BASE:1.1;
locks; strict;
comment	@ * @;


1.51
date	2017.07.01.16.14.10;	author kettenis;	state Exp;
branches;
next	1.50;
commitid	KnwRPOZok9A30HI4;

1.50
date	2017.05.26.14.26.33;	author kettenis;	state Exp;
branches;
next	1.49;
commitid	yIZMpJ33E8Epw3jj;

1.49
date	2017.04.16.17.16.21;	author bluhm;	state Exp;
branches;
next	1.48;
commitid	VKMewPGe3MQHoyIZ;

1.48
date	2016.10.08.05.52.06;	author guenther;	state Exp;
branches;
next	1.47;
commitid	gFhAhnnQ2SlMacGa;

1.47
date	2016.04.05.20.44.03;	author kettenis;	state Exp;
branches;
next	1.46;
commitid	NmhNQ0K4GRfCKIMY;

1.46
date	2016.04.05.08.19.00;	author kettenis;	state Exp;
branches;
next	1.45;
commitid	BaK28M7uSySRlqZl;

1.45
date	2016.02.05.15.51.10;	author kettenis;	state Exp;
branches;
next	1.44;
commitid	4O5RxyA1IYOPmGvL;

1.44
date	2016.02.05.10.05.12;	author kettenis;	state Exp;
branches;
next	1.43;
commitid	gX0sRFu8RGimO2hC;

1.43
date	2015.12.31.13.01.00;	author kettenis;	state Exp;
branches;
next	1.42;
commitid	PRTGqV1xSkCKPIeR;

1.42
date	2015.10.17.21.41.12;	author kettenis;	state Exp;
branches;
next	1.41;
commitid	bHt1y1EnMlJS0gRQ;

1.41
date	2015.09.27.21.28.14;	author kettenis;	state Exp;
branches;
next	1.40;
commitid	Np8P85ZmWpjis6qB;

1.40
date	2015.09.27.11.09.26;	author jsg;	state Exp;
branches;
next	1.39;
commitid	OkoKp05dU7tP7DK3;

1.39
date	2015.09.26.22.00.00;	author kettenis;	state Exp;
branches;
next	1.38;
commitid	Mvg1eWGKMP5bOyT1;

1.38
date	2015.09.26.19.52.16;	author kettenis;	state Exp;
branches;
next	1.37;
commitid	PpEJMvqmELqAFnQv;

1.37
date	2015.09.26.11.17.15;	author kettenis;	state Exp;
branches;
next	1.36;
commitid	DUG1LQonw3dWeI9h;

1.36
date	2015.09.25.20.27.52;	author kettenis;	state Exp;
branches;
next	1.35;
commitid	jaD3JYENYge6B4QV;

1.35
date	2015.09.24.20.52.28;	author kettenis;	state Exp;
branches;
next	1.34;
commitid	7aH5GgFn0EMAmSm3;

1.34
date	2015.09.23.23.12.11;	author kettenis;	state Exp;
branches;
next	1.33;
commitid	lQlppvmETCN49oZe;

1.33
date	2015.07.16.18.48.51;	author kettenis;	state Exp;
branches;
next	1.32;
commitid	L0a8i9q8D7R6QA4N;

1.32
date	2015.07.11.04.00.46;	author jsg;	state Exp;
branches;
next	1.31;
commitid	eVKv6xUxvZe6BPUh;

1.31
date	2015.06.26.15.22.23;	author kettenis;	state Exp;
branches;
next	1.30;
commitid	zE715ZsjyYsYQNQ0;

1.30
date	2015.06.24.19.01.51;	author kettenis;	state Exp;
branches;
next	1.29;
commitid	gUkVFrZSFfq0QPRv;

1.29
date	2015.06.24.17.59.42;	author kettenis;	state Exp;
branches;
next	1.28;
commitid	3wz7SV1D1yJbfWE9;

1.28
date	2015.06.24.08.32.39;	author kettenis;	state Exp;
branches;
next	1.27;
commitid	hfEqCdm8ecmxIgUE;

1.27
date	2015.04.18.14.47.34;	author jsg;	state Exp;
branches;
next	1.26;
commitid	c1fUeeFWMNg4COgR;

1.26
date	2015.04.18.11.41.28;	author jsg;	state Exp;
branches;
next	1.25;
commitid	c3CbYQJYD10xhd6O;

1.25
date	2015.04.18.11.05.32;	author jsg;	state Exp;
branches;
next	1.24;
commitid	N5rmYm7ybHimJmMa;

1.24
date	2015.04.12.17.10.07;	author kettenis;	state Exp;
branches;
next	1.23;
commitid	7RIU3AxWXbuzxDet;

1.23
date	2015.04.12.12.14.30;	author jsg;	state Exp;
branches;
next	1.22;
commitid	os4Nms6pAXvmynAf;

1.22
date	2015.04.12.05.31.23;	author jsg;	state Exp;
branches;
next	1.21;
commitid	09jvnnyFAQrVbK5f;

1.21
date	2015.04.12.03.54.10;	author jsg;	state Exp;
branches;
next	1.20;
commitid	uVTyY1h8Sggc8pFj;

1.20
date	2015.04.11.14.39.37;	author jsg;	state Exp;
branches;
next	1.19;
commitid	MAOlCKi3JlwvLeJa;

1.19
date	2015.04.11.05.10.13;	author jsg;	state Exp;
branches;
next	1.18;
commitid	pjaRMzmEKTQk8EZt;

1.18
date	2015.04.11.02.59.05;	author jsg;	state Exp;
branches;
next	1.17;
commitid	NptfR81VAOcNKwQU;

1.17
date	2015.04.10.12.06.52;	author jsg;	state Exp;
branches;
next	1.16;
commitid	5bVyebI6DOf5FKY3;

1.16
date	2015.04.10.05.31.25;	author jsg;	state Exp;
branches;
next	1.15;
commitid	BNr9TiYARwtS3iCu;

1.15
date	2015.04.08.04.03.06;	author jsg;	state Exp;
branches;
next	1.14;
commitid	yFnN7rFafIBD7cOc;

1.14
date	2015.04.08.02.28.13;	author jsg;	state Exp;
branches;
next	1.13;
commitid	pBZw25gbMMahiUV2;

1.13
date	2015.04.06.15.43.15;	author jsg;	state Exp;
branches;
next	1.12;
commitid	DCPoYnokQzAg9pjM;

1.12
date	2015.04.06.12.25.10;	author jsg;	state Exp;
branches;
next	1.11;
commitid	CN1fAwudhSb2ckyl;

1.11
date	2015.04.06.08.14.00;	author kettenis;	state Exp;
branches;
next	1.10;
commitid	3FenpHSGCnS2uSz0;

1.10
date	2015.04.06.05.35.29;	author jsg;	state Exp;
branches;
next	1.9;
commitid	oeVBooRupIYToF2n;

1.9
date	2015.04.05.11.53.53;	author kettenis;	state Exp;
branches;
next	1.8;
commitid	3YXcRggXXMDC9Cpg;

1.8
date	2015.04.03.13.10.59;	author jsg;	state Exp;
branches;
next	1.7;
commitid	PZIJ62HYZVx8fbpI;

1.7
date	2015.02.12.11.11.45;	author jsg;	state Exp;
branches;
next	1.6;
commitid	SzbWuibzaFWN7p7E;

1.6
date	2015.02.12.06.30.56;	author jsg;	state Exp;
branches;
next	1.5;
commitid	pZ1bJqLc6BdU0mTp;

1.5
date	2015.02.12.02.12.02;	author kettenis;	state Exp;
branches;
next	1.4;
commitid	cYXtgYH6nnLqDRGU;

1.4
date	2015.02.11.07.01.36;	author jsg;	state Exp;
branches;
next	1.3;
commitid	dLgISW35NAmGN8Xl;

1.3
date	2015.02.10.00.23.53;	author jsg;	state Exp;
branches;
next	1.2;
commitid	G1YbxwpP7YxexmMy;

1.2
date	2014.09.20.21.17.43;	author kettenis;	state Exp;
branches;
next	1.1;
commitid	EVxnMw291QGly5Fu;

1.1
date	2014.04.01.20.16.50;	author kettenis;	state Exp;
branches;
next	;


desc
@@


1.51
log
@Update inteldrm(4) to code based on Linux 4.4.70.  This brings us support for
Skylake and Cherryview and better support for Broadwell and Valleyview.  Also
adds MST support.  Some tweaks to the TTM code and radeondrm(4) to keep it
working with the updated generic DRM code needed for inteldrm(4).

Tested by many.
@
text
@/*	$OpenBSD: drm_linux.h,v 1.50 2017/05/26 14:26:33 kettenis Exp $	*/
/*
 * Copyright (c) 2013, 2014, 2015 Mark Kettenis
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#ifndef _DRM_LINUX_H_
#define _DRM_LINUX_H_

#include <sys/param.h>
#include <sys/atomic.h>
#include <sys/errno.h>
#include <sys/kernel.h>
#include <sys/signalvar.h>
#include <sys/stdint.h>
#include <sys/systm.h>
#include <sys/task.h>
#include <sys/time.h>
#include <sys/timeout.h>
#include <sys/tree.h>

#include <uvm/uvm_extern.h>

#include <ddb/db_var.h>

#include <dev/i2c/i2cvar.h>
#include <dev/pci/pcireg.h>
#include <dev/pci/pcivar.h>

#include <dev/pci/drm/linux_types.h>
#include <dev/pci/drm/drm_linux_atomic.h>

/* The Linux code doesn't meet our usual standards! */
#ifdef __clang__
#pragma clang diagnostic ignored "-Wenum-conversion"
#pragma clang diagnostic ignored "-Winitializer-overrides"
#pragma clang diagnostic ignored "-Wtautological-pointer-compare"
#pragma clang diagnostic ignored "-Wunneeded-internal-declaration"
#pragma clang diagnostic ignored "-Wunused-const-variable"
#endif

struct i2c_algorithm;

struct i2c_adapter {
	struct i2c_controller ic;

	char name[48];
	const struct i2c_algorithm *algo;
	void *algo_data;
	int retries;

	void *data;
};

#define I2C_NAME_SIZE	20

struct i2c_msg {
	uint16_t addr;
	uint16_t flags;
	uint16_t len;
	uint8_t *buf;
};

#define I2C_M_RD	0x0001
#define I2C_M_NOSTART	0x0002

struct i2c_algorithm {
	int (*master_xfer)(struct i2c_adapter *, struct i2c_msg *, int);
};

int i2c_transfer(struct i2c_adapter *, struct i2c_msg *, int);
#define i2c_add_adapter(x) 0
#define i2c_del_adapter(x)

static inline void *
i2c_get_adapdata(struct i2c_adapter *adap)
{
	return adap->data;
}

static inline void
i2c_set_adapdata(struct i2c_adapter *adap, void *data)
{
	adap->data = data;
}

typedef int irqreturn_t;
enum irqreturn {
	IRQ_NONE = 0,
	IRQ_HANDLED = 1
};

typedef int8_t   s8;
typedef uint8_t  u8;
typedef int16_t  s16;
typedef uint16_t u16;
typedef int32_t  s32;
typedef uint32_t u32;
typedef int64_t  s64;
typedef uint64_t u64;

#define U64_C(x) UINT64_C(x)
#define U64_MAX UINT64_MAX

typedef uint16_t __le16;
typedef uint16_t __be16;
typedef uint32_t __le32;
typedef uint32_t __be32;

typedef bus_addr_t dma_addr_t;
typedef bus_addr_t phys_addr_t;

typedef bus_addr_t resource_size_t;

typedef off_t loff_t;

#define __force
#define __always_unused	__unused
#define __read_mostly
#define __iomem
#define __must_check
#define __init
#define __exit

#define __printf(x, y)

#define barrier()		__asm __volatile("" : : : "memory");

#define uninitialized_var(x) x

#if BYTE_ORDER == BIG_ENDIAN
#define __BIG_ENDIAN
#else
#define __LITTLE_ENDIAN
#endif

#define le16_to_cpu(x) letoh16(x)
#define le32_to_cpu(x) letoh32(x)
#define cpu_to_le16(x) htole16(x)
#define cpu_to_le32(x) htole32(x)

#define be32_to_cpup(x) betoh32(*x)

static inline uint8_t
hweight8(uint32_t x)
{
	x = (x & 0x55) + ((x & 0xaa) >> 1);
	x = (x & 0x33) + ((x & 0xcc) >> 2);
	x = (x + (x >> 4)) & 0x0f;
	return (x);
}

static inline uint16_t
hweight16(uint32_t x)
{
	x = (x & 0x5555) + ((x & 0xaaaa) >> 1);
	x = (x & 0x3333) + ((x & 0xcccc) >> 2);
	x = (x + (x >> 4)) & 0x0f0f;
	x = (x + (x >> 8)) & 0x00ff;
	return (x);
}

static inline uint32_t
hweight32(uint32_t x)
{
	x = (x & 0x55555555) + ((x & 0xaaaaaaaa) >> 1);
	x = (x & 0x33333333) + ((x & 0xcccccccc) >> 2);
	x = (x + (x >> 4)) & 0x0f0f0f0f;
	x = (x + (x >> 8));
	x = (x + (x >> 16)) & 0x000000ff;
	return x;
}

static inline uint32_t
hweight64(uint64_t x)
{
	x = (x & 0x5555555555555555ULL) + ((x & 0xaaaaaaaaaaaaaaaaULL) >> 1);
	x = (x & 0x3333333333333333ULL) + ((x & 0xccccccccccccccccULL) >> 2);
	x = (x + (x >> 4)) & 0x0f0f0f0f0f0f0f0fULL;
	x = (x + (x >> 8));
	x = (x + (x >> 16));
	x = (x + (x >> 32)) & 0x000000ff;
	return x;
}

#define lower_32_bits(n)	((u32)(n))
#define upper_32_bits(_val)	((u32)(((_val) >> 16) >> 16))
#define DMA_BIT_MASK(n) (((n) == 64) ? ~0ULL : (1ULL<<(n)) -1)
#define BIT(x)			(1UL << x)
#define BITS_TO_LONGS(x)	howmany((x), 8 * sizeof(long))

#define DECLARE_BITMAP(x, y)	unsigned long x[BITS_TO_LONGS(y)];
#define bitmap_empty(p, n)	(find_first_bit(p, n) == n)
#define GENMASK(h, l)		((~0U >> (32 - h -1)) & (~0U << l))

static inline void
bitmap_set(void *p, int b, u_int n)
{
	u_int end = b + n;

	for (; b < end; b++)
		__set_bit(b, p);
}

static inline void
bitmap_zero(void *p, u_int n)
{
	u_int *ptr = p;
	u_int b;

	for (b = 0; b < n; b += 32)
		ptr[b >> 5] = 0;
}

static inline void
bitmap_or(void *d, void *s1, void *s2, u_int n)
{
	u_int *dst = d;
	u_int *src1 = s1;
	u_int *src2 = s2;
	u_int b;

	for (b = 0; b < n; b += 32)
		dst[b >> 5] = src1[b >> 5] | src2[b >> 5];
}

static inline int
bitmap_weight(void *p, u_int n)
{
	u_int *ptr = p;
	u_int b;
	int sum = 0;

	for (b = 0; b < n; b += 32)
		sum += hweight32(ptr[b >> 5]);
	return sum;
}

#define DECLARE_HASHTABLE(x, y) struct hlist_head x;

#define hash_init(x)		INIT_HLIST_HEAD(&(x))
#define hash_add(x, y, z)	hlist_add_head(y, &(x))
#define hash_del(x)		hlist_del_init(x)
#define hash_empty(x)		hlist_empty(&(x))
#define hash_for_each_possible(a, b, c, d) \
	hlist_for_each_entry(b, &(a), c)
#define hash_for_each_safe(a, b, c, d, e) (void)(b); \
	hlist_for_each_entry_safe(d, c, &(a), e)

#define ACCESS_ONCE(x)		(x)

#define EXPORT_SYMBOL(x)

#define IS_ENABLED(x) x - 0

#define IS_BUILTIN(x) 1

struct device_driver {
	struct device *dev;
};

#define dev_get_drvdata(x)	NULL
#define dev_set_drvdata(x, y)
#define dev_name(dev)		""

#define devm_kzalloc(x, y, z)	kzalloc(y, z)

struct module;

#define MODULE_AUTHOR(x)
#define MODULE_DESCRIPTION(x)
#define MODULE_LICENSE(x)
#define MODULE_FIRMWARE(x)
#define MODULE_DEVICE_TABLE(x, y)
#define MODULE_PARM_DESC(parm, desc)
#define module_param_named(name, value, type, perm)
#define module_param_named_unsafe(name, value, type, perm)
#define module_param_unsafe(name, type, perm)

#define THIS_MODULE	NULL

#define ARRAY_SIZE nitems

#define ERESTARTSYS	EINTR
#define ETIME		ETIMEDOUT
#define EREMOTEIO	EIO
#define EPROTO		EIO
#define ENOTSUPP	ENOTSUP
#define ENODATA		ENOTSUP
#define ECHRNG		EINVAL

#define KERN_INFO	""
#define KERN_WARNING	""
#define KERN_NOTICE	""
#define KERN_DEBUG	""
#define KERN_CRIT	""
#define KERN_ERR	""

#define KBUILD_MODNAME "drm"

#define UTS_RELEASE	""

#define TASK_COMM_LEN	(MAXCOMLEN + 1)

#ifndef pr_fmt
#define pr_fmt(fmt) fmt
#endif

#define printk_once(fmt, arg...) ({		\
	static int __warned;			\
	if (!__warned) {			\
		printf(fmt, ## arg);		\
		__warned = 1;			\
	}					\
})

#define printk(fmt, arg...)	printf(fmt, ## arg)
#define pr_warn(fmt, arg...)	printf(pr_fmt(fmt), ## arg)
#define pr_warn_once(fmt, arg...)	printk_once(pr_fmt(fmt), ## arg)
#define pr_notice(fmt, arg...)	printf(pr_fmt(fmt), ## arg)
#define pr_crit(fmt, arg...)	printf(pr_fmt(fmt), ## arg)
#define pr_err(fmt, arg...)	printf(pr_fmt(fmt), ## arg)

#ifdef DRMDEBUG
#define pr_info(fmt, arg...)	printf(pr_fmt(fmt), ## arg)
#define pr_info_once(fmt, arg...)	printk_once(pr_fmt(fmt), ## arg)
#define pr_debug(fmt, arg...)	printf(pr_fmt(fmt), ## arg)
#else
#define pr_info(fmt, arg...)	do { } while(0)
#define pr_info_once(fmt, arg...)	do { } while(0)
#define pr_debug(fmt, arg...)	do { } while(0)
#endif

#define dev_warn(dev, fmt, arg...)				\
	printf("drm:pid%d:%s *WARNING* " fmt, curproc->p_p->ps_pid,	\
	    __func__ , ## arg)
#define dev_notice(dev, fmt, arg...)				\
	printf("drm:pid%d:%s *NOTICE* " fmt, curproc->p_p->ps_pid,	\
	    __func__ , ## arg)
#define dev_crit(dev, fmt, arg...)				\
	printf("drm:pid%d:%s *ERROR* " fmt, curproc->p_p->ps_pid,	\
	    __func__ , ## arg)
#define dev_err(dev, fmt, arg...)				\
	printf("drm:pid%d:%s *ERROR* " fmt, curproc->p_p->ps_pid,	\
	    __func__ , ## arg)

#ifdef DRMDEBUG
#define dev_info(dev, fmt, arg...)				\
	printf("drm: " fmt, ## arg)
#define dev_debug(dev, fmt, arg...)				\
	printf("drm:pid%d:%s *DEBUG* " fmt, curproc->p_p->ps_pid,	\
	    __func__ , ## arg)
#else
#define dev_info(dev, fmt, arg...) 				\
	    do { } while(0)
#define dev_debug(dev, fmt, arg...) 				\
	    do { } while(0)
#endif

enum {
	DUMP_PREFIX_NONE,
	DUMP_PREFIX_ADDRESS,
	DUMP_PREFIX_OFFSET
};

void print_hex_dump(const char *, const char *, int, int, int,
	 const void *, size_t, bool);

#define scnprintf(str, size, fmt, arg...) snprintf(str, size, fmt, ## arg)

#define unlikely(x)	__builtin_expect(!!(x), 0)
#define likely(x)	__builtin_expect(!!(x), 1)

#define BUG()								\
do {									\
	panic("BUG at %s:%d", __FILE__, __LINE__);			\
} while (0)

#ifndef DIAGNOSTIC
#define BUG_ON(x)	((void)(x))
#else
#define BUG_ON(x)	KASSERT(!(x))
#endif

#define BUILD_BUG()
#define BUILD_BUG_ON(x) CTASSERT(!(x))
#define BUILD_BUG_ON_NOT_POWER_OF_2(x)
#define BUILD_BUG_ON_MSG(x, y)

#define WARN(condition, fmt...) ({ 					\
	int __ret = !!(condition);					\
	if (__ret)							\
		printf(fmt);						\
	unlikely(__ret);						\
})

#define WARN_ONCE(condition, fmt...) ({					\
	static int __warned;						\
	int __ret = !!(condition);					\
	if (__ret && !__warned) {					\
		printf(fmt);						\
		__warned = 1;						\
	}								\
	unlikely(__ret);						\
})

#define _WARN_STR(x) #x

#define WARN_ON(condition) ({						\
	int __ret = !!(condition);					\
	if (__ret)							\
		printf("WARNING %s failed at %s:%d\n",			\
		    _WARN_STR(condition), __FILE__, __LINE__);		\
	unlikely(__ret);						\
})

#define WARN_ON_ONCE(condition) ({					\
	static int __warned;						\
	int __ret = !!(condition);					\
	if (__ret && !__warned) {					\
		printf("WARNING %s failed at %s:%d\n",			\
		    _WARN_STR(condition), __FILE__, __LINE__);		\
		__warned = 1;						\
	}								\
	unlikely(__ret);						\
})

#define TP_PROTO(x...) x

#define DEFINE_EVENT(template, name, proto, args) \
static inline void trace_##name(proto) {}

#define DEFINE_EVENT_PRINT(template, name, proto, args, print) \
static inline void trace_##name(proto) {}

#define TRACE_EVENT(name, proto, args, tstruct, assign, print) \
static inline void trace_##name(proto) {}

#define TRACE_EVENT_CONDITION(name, proto, args, cond, tstruct, assign, print) \
static inline void trace_##name(proto) {}

#define DECLARE_EVENT_CLASS(name, proto, args, tstruct, assign, print) \
static inline void trace_##name(proto) {}

#define IS_ERR_VALUE(x) unlikely((x) >= (unsigned long)-ELAST)

static inline void *
ERR_PTR(long error)
{
	return (void *) error;
}

static inline long
PTR_ERR(const void *ptr)
{
	return (long) ptr;
}

static inline long
IS_ERR(const void *ptr)
{
        return IS_ERR_VALUE((unsigned long)ptr);
}

static inline long
IS_ERR_OR_NULL(const void *ptr)
{
        return !ptr || IS_ERR_VALUE((unsigned long)ptr);
}

static inline void *
ERR_CAST(const void *ptr)
{
	return (void *)ptr;
}

static inline int
PTR_ERR_OR_ZERO(const void *ptr)
{
	return IS_ERR(ptr)? PTR_ERR(ptr) : 0;
}

#define swap(a, b) \
	do { __typeof(a) __tmp = (a); (a) = (b); (b) = __tmp; } while(0)

#define container_of(ptr, type, member) ({                      \
	__typeof( ((type *)0)->member ) *__mptr = (ptr);        \
	(type *)( (char *)__mptr - offsetof(type,member) );})

#ifndef __DECONST
#define __DECONST(type, var)    ((type)(__uintptr_t)(const void *)(var))
#endif

typedef struct rwlock rwlock_t;
typedef struct mutex spinlock_t;
#define DEFINE_SPINLOCK(x)	struct mutex x

static inline void
spin_lock_irqsave(struct mutex *mtxp, __unused unsigned long flags)
{
	mtx_enter(mtxp);
}
static inline void
spin_unlock_irqrestore(struct mutex *mtxp, __unused unsigned long flags)
{
	mtx_leave(mtxp);
}
#define spin_lock(mtxp)			mtx_enter(mtxp)
#define spin_unlock(mtxp)		mtx_leave(mtxp)
#define spin_lock_irq(mtxp)		mtx_enter(mtxp)
#define spin_unlock_irq(mtxp)		mtx_leave(mtxp)
#define assert_spin_locked(mtxp)	MUTEX_ASSERT_LOCKED(mtxp)
#define mutex_lock_interruptible(rwl)	-rw_enter(rwl, RW_WRITE | RW_INTR)
#define mutex_lock(rwl)			rw_enter_write(rwl)
#define mutex_lock_nest_lock(rwl, sub)	rw_enter_write(rwl)
#define mutex_trylock(rwl)		(rw_enter(rwl, RW_WRITE | RW_NOSLEEP) == 0)
#define mutex_unlock(rwl)		rw_exit_write(rwl)
#define mutex_is_locked(rwl)		(rw_status(rwl) == RW_WRITE)
#define down_read(rwl)			rw_enter_read(rwl)
#define up_read(rwl)			rw_exit_read(rwl)
#define down_write(rwl)			rw_enter_write(rwl)
#define up_write(rwl)			rw_exit_write(rwl)
#define read_lock(rwl)			rw_enter_read(rwl)
#define read_unlock(rwl)		rw_exit_read(rwl)
#define write_lock(rwl)			rw_enter_write(rwl)
#define write_unlock(rwl)		rw_exit_write(rwl)

#define might_lock(lock)
#define lockdep_assert_held(lock)	do { (void)(lock); } while(0)

#define local_irq_save(x)		(x) = splhigh()
#define local_irq_restore(x)		splx((x))

#define synchronize_irq(x)

#define fence_wait(x, y)
#define fence_put(x)

struct wait_queue_head {
	struct mutex lock;
	unsigned int count;
};
typedef struct wait_queue_head wait_queue_head_t;

static inline void
init_waitqueue_head(wait_queue_head_t *wq)
{
	mtx_init(&wq->lock, IPL_NONE);
	wq->count = 0;
}

#define wait_event(wq, condition) \
do {						\
	struct sleep_state sls;			\
						\
	KASSERT(!cold);				\
	if (condition)				\
		break;				\
	atomic_inc_int(&(wq).count);		\
	sleep_setup(&sls, &wq, 0, "drmwe");	\
	sleep_finish(&sls, !(condition));	\
	atomic_dec_int(&(wq).count);		\
} while (!(condition))

#define __wait_event_timeout(wq, condition, ret) \
do {						\
	struct sleep_state sls;			\
	int deadline, __error;			\
						\
	KASSERT(!cold);				\
	atomic_inc_int(&(wq).count);		\
	sleep_setup(&sls, &wq, 0, "drmwet");	\
	sleep_setup_timeout(&sls, ret);		\
	deadline = ticks + ret;			\
	sleep_finish(&sls, !(condition));	\
	ret = deadline - ticks;			\
	__error = sleep_finish_timeout(&sls);	\
	atomic_dec_int(&(wq).count);		\
	if (ret < 0 || __error == EWOULDBLOCK)	\
		ret = 0;			\
	if (ret == 0 && (condition)) {		\
		ret = 1;			\
		break;				\
	}					\
} while (ret > 0 && !(condition))

#define wait_event_timeout(wq, condition, timo)	\
({						\
	long __ret = timo;			\
	if (!(condition))			\
		__wait_event_timeout(wq, condition, __ret); \
	__ret;					\
})

#define __wait_event_interruptible_timeout(wq, condition, ret) \
do {						\
	struct sleep_state sls;			\
	int deadline, __error, __error1;	\
						\
	KASSERT(!cold);				\
	atomic_inc_int(&(wq).count);		\
	sleep_setup(&sls, &wq, PCATCH, "drmweti"); \
	sleep_setup_timeout(&sls, ret);		\
	sleep_setup_signal(&sls, PCATCH);	\
	deadline = ticks + ret;			\
	sleep_finish(&sls, !(condition));	\
	ret = deadline - ticks;			\
	__error1 = sleep_finish_timeout(&sls);	\
	__error = sleep_finish_signal(&sls);	\
	atomic_dec_int(&(wq).count);		\
	if (ret < 0 || __error1 == EWOULDBLOCK)	\
		ret = 0;			\
	if (__error == ERESTART)		\
		ret = -ERESTARTSYS;		\
	else if (__error)			\
		ret = -__error;			\
	if (ret == 0 && (condition)) {		\
		ret = 1;			\
		break;				\
	}					\
} while (ret > 0 && !(condition))

#define wait_event_interruptible_timeout(wq, condition, timo) \
({						\
	long __ret = timo;			\
	if (!(condition))			\
		__wait_event_interruptible_timeout(wq, condition, __ret); \
	__ret;					\
})

#define wake_up(x)			wakeup(x)
#define wake_up_all(x)			wakeup(x)
#define wake_up_all_locked(x)		wakeup(x)
#define wake_up_interruptible(x)	wakeup(x)

#define waitqueue_active(wq)		((wq)->count > 0)

struct completion {
	u_int done;
	wait_queue_head_t wait;
};

#define INIT_COMPLETION(x) ((x).done = 0)

static inline void
init_completion(struct completion *x)
{
	x->done = 0;
	mtx_init(&x->wait.lock, IPL_NONE);
}

static inline u_long
wait_for_completion_interruptible_timeout(struct completion *x, u_long timo)
{
	int ret;

	mtx_enter(&x->wait.lock);
	while (x->done == 0) {
		ret = msleep(x, &x->wait.lock, PCATCH, "wfcit", timo);
		if (ret) {
			mtx_leave(&x->wait.lock);
			return (ret == EWOULDBLOCK) ? 0 : -ret;
		}
	}

	return 1;
}

static inline void
complete_all(struct completion *x)
{
	mtx_enter(&x->wait.lock);
	x->done = 1;
	mtx_leave(&x->wait.lock);
	wakeup(x);
}

struct workqueue_struct;

#define system_wq (struct workqueue_struct *)systq
#define system_long_wq (struct workqueue_struct *)systq

static inline struct workqueue_struct *
alloc_ordered_workqueue(const char *name, int flags)
{
	struct taskq *tq = taskq_create(name, 1, IPL_TTY, 0);
	return (struct workqueue_struct *)tq;
}

static inline void
destroy_workqueue(struct workqueue_struct *wq)
{
	taskq_destroy((struct taskq *)wq);
}

struct work_struct {
	struct task task;
	struct taskq *tq;
};

typedef void (*work_func_t)(struct work_struct *);

static inline void
INIT_WORK(struct work_struct *work, work_func_t func)
{
	work->tq = systq;
	task_set(&work->task, (void (*)(void *))func, work);
}

#define INIT_WORK_ONSTACK(x, y)	INIT_WORK((x), (y))

static inline bool
queue_work(struct workqueue_struct *wq, struct work_struct *work)
{
	work->tq = (struct taskq *)wq;
	return task_add(work->tq, &work->task);
}

static inline void
cancel_work_sync(struct work_struct *work)
{
	task_del(work->tq, &work->task);
}

struct delayed_work {
	struct work_struct work;
	struct timeout to;
	struct taskq *tq;
};

static inline struct delayed_work *
to_delayed_work(struct work_struct *work)
{
	return container_of(work, struct delayed_work, work);
}

static void
__delayed_work_tick(void *arg)
{
	struct delayed_work *dwork = arg;

	task_add(dwork->tq, &dwork->work.task);
}

static inline void
INIT_DELAYED_WORK(struct delayed_work *dwork, work_func_t func)
{
	INIT_WORK(&dwork->work, func);
	timeout_set(&dwork->to, __delayed_work_tick, &dwork->work);
}

static inline bool
schedule_work(struct work_struct *work)
{
	return task_add(work->tq, &work->task);
}

static inline bool
schedule_delayed_work(struct delayed_work *dwork, int jiffies)
{
	dwork->tq = systq;
	return timeout_add(&dwork->to, jiffies);
}

static inline bool
queue_delayed_work(struct workqueue_struct *wq,
    struct delayed_work *dwork, int jiffies)
{
	dwork->tq = (struct taskq *)wq;
	return timeout_add(&dwork->to, jiffies);
}

static inline bool
mod_delayed_work(struct workqueue_struct *wq,
    struct delayed_work *dwork, int jiffies)
{
	dwork->tq = (struct taskq *)wq;
	return (timeout_add(&dwork->to, jiffies) == 0);
}

static inline bool
cancel_delayed_work(struct delayed_work *dwork)
{
	if (timeout_del(&dwork->to))
		return true;
	return task_del(dwork->tq, &dwork->work.task);
}

static inline bool
cancel_delayed_work_sync(struct delayed_work *dwork)
{
	if (timeout_del(&dwork->to))
		return true;
	return task_del(dwork->tq, &dwork->work.task);
}

void flush_workqueue(struct workqueue_struct *);
void flush_work(struct work_struct *);
void flush_delayed_work(struct delayed_work *);
#define flush_scheduled_work()	flush_workqueue(system_wq)

#define destroy_work_on_stack(x)

typedef void *async_cookie_t;
#define async_schedule(func, data)	(func)((data), NULL)

#define local_irq_disable()	disable_intr()
#define local_irq_enable()	enable_intr()

#define setup_timer(x, y, z)	timeout_set((x), (void (*)(void *))(y), (void *)(z))
#define mod_timer(x, y)		timeout_add((x), (y - jiffies))
#define mod_timer_pinned(x, y)	timeout_add((x), (y - jiffies))
#define del_timer_sync(x)	timeout_del((x))
#define timer_pending(x)	timeout_pending((x))

#define cond_resched()		sched_pause(yield)
#define drm_need_resched() \
    (curcpu()->ci_schedstate.spc_schedflags & SPCF_SHOULDYIELD)

#define TASK_UNINTERRUPTIBLE	0
#define TASK_INTERRUPTIBLE	PCATCH

#define signal_pending_state(x, y) CURSIG(curproc)

#define NSEC_PER_USEC	1000L
#define NSEC_PER_MSEC	1000000L
#define NSEC_PER_SEC	1000000000L
#define KHZ2PICOS(a)	(1000000000UL/(a))

extern struct timespec ns_to_timespec(const int64_t);
extern int64_t timeval_to_ns(const struct timeval *);
extern int64_t timeval_to_us(const struct timeval *);
extern struct timeval ns_to_timeval(const int64_t);

static inline struct timespec
timespec_sub(struct timespec t1, struct timespec t2)
{
	struct timespec diff;

	timespecsub(&t1, &t2, &diff);
	return diff;
}

#define time_in_range(x, min, max) ((x) >= (min) && (x) <= (max))

extern int ticks;
#define jiffies ticks
#undef HZ
#define HZ	hz

#define MAX_JIFFY_OFFSET	((INT_MAX >> 1) - 1)

static inline unsigned long
round_jiffies_up(unsigned long j)
{
	return roundup(j, hz);
}

static inline unsigned long
round_jiffies_up_relative(unsigned long j)
{
	return roundup(j, hz);
}

#define jiffies_to_msecs(x)	(((int64_t)(x)) * 1000 / hz)
#define jiffies_to_usecs(x)	(((int64_t)(x)) * 1000000 / hz)
#define msecs_to_jiffies(x)	(((int64_t)(x)) * hz / 1000)
#define nsecs_to_jiffies64(x)	(((int64_t)(x)) * hz / 1000000000)
#define get_jiffies_64()	jiffies
#define time_after(a,b)		((long)(b) - (long)(a) < 0)
#define time_after_eq(a,b)	((long)(b) - (long)(a) <= 0)
#define get_seconds()		time_second
#define getrawmonotonic(x)	nanouptime(x)

static inline void
set_normalized_timespec(struct timespec *ts, time_t sec, int64_t nsec)
{
	while (nsec > NSEC_PER_SEC) {
		nsec -= NSEC_PER_SEC;
		sec++;
	}

	ts->tv_sec = sec;
	ts->tv_nsec = nsec;
}

static inline int64_t
timespec_to_ns(const struct timespec *ts)
{
	return ((ts->tv_sec * NSEC_PER_SEC) + ts->tv_nsec);
}

static inline int
timespec_to_jiffies(const struct timespec *ts)
{
	long long to_ticks;

	to_ticks = (long long)hz * ts->tv_sec + ts->tv_nsec / (tick * 1000);
	if (to_ticks > INT_MAX)
		to_ticks = INT_MAX;

	return ((int)to_ticks);
}

static inline int
timespec_valid(const struct timespec *ts)
{
	if (ts->tv_sec < 0 || ts->tv_sec > 100000000 ||
	    ts->tv_nsec < 0 || ts->tv_nsec >= 1000000000)
		return (0);
	return (1);
}

typedef struct timeval ktime_t;

static inline struct timeval
ktime_get(void)
{
	struct timeval tv;
	
	getmicrouptime(&tv);
	return tv;
}

static inline struct timeval
ktime_get_monotonic_offset(void)
{
	struct timeval tv = {0, 0};
	return tv;
}

static inline int64_t
ktime_to_us(struct timeval tv)
{
	return timeval_to_us(&tv);
}

static inline int64_t
ktime_to_ns(struct timeval tv)
{
	return timeval_to_ns(&tv);
}

static inline int64_t
ktime_get_raw_ns(void)
{
	return ktime_to_ns(ktime_get());
}

#define ktime_to_timeval(tv) (tv)

static inline struct timeval
ktime_sub(struct timeval a, struct timeval b)
{
	struct timeval res;
	timersub(&a, &b, &res);
	return res;
}

static inline struct timeval
ktime_add_ns(struct timeval tv, int64_t ns)
{
	return ns_to_timeval(timeval_to_ns(&tv) + ns);
}

static inline struct timeval
ktime_sub_ns(struct timeval tv, int64_t ns)
{
	return ns_to_timeval(timeval_to_ns(&tv) - ns);
}

static inline int64_t
ktime_us_delta(struct timeval a, struct timeval b)
{
	return ktime_to_us(ktime_sub(a, b));
}

#define ktime_mono_to_real(x) (x)
#define ktime_get_real() ktime_get()

#define do_gettimeofday(tv) getmicrouptime(tv)

#define GFP_ATOMIC	M_NOWAIT
#define GFP_NOWAIT	M_NOWAIT
#define GFP_KERNEL	(M_WAITOK | M_CANFAIL)
#define GFP_TEMPORARY	(M_WAITOK | M_CANFAIL)
#define GFP_HIGHUSER	0
#define GFP_DMA32	0
#define __GFP_NOWARN	0
#define __GFP_NORETRY	0
#define __GFP_ZERO	M_ZERO

static inline void *
kmalloc(size_t size, int flags)
{
	return malloc(size, M_DRM, flags);
}

static inline void *
kmalloc_array(size_t n, size_t size, int flags)
{
	if (n == 0 || SIZE_MAX / n < size)
		return NULL;
	return malloc(n * size, M_DRM, flags);
}

static inline void *
kcalloc(size_t n, size_t size, int flags)
{
	if (n == 0 || SIZE_MAX / n < size)
		return NULL;
	return malloc(n * size, M_DRM, flags | M_ZERO);
}

static inline void *
kzalloc(size_t size, int flags)
{
	return malloc(size, M_DRM, flags | M_ZERO);
}

static inline void
kfree(const void *objp)
{
	free((void *)objp, M_DRM, 0);
}

static inline void *
kmemdup(const void *src, size_t len, int flags)
{
	void *p = malloc(len, M_DRM, flags);
	if (p)
		memcpy(p, src, len);
	return (p);
}

static inline char *
kasprintf(int flags, const char *fmt, ...)
{
	char *buf;
	size_t len;
	va_list ap;

	va_start(ap, fmt);
	len = vsnprintf(NULL, 0, fmt, ap);
	va_end(ap);

	buf = kmalloc(len, flags);
	if (buf) {
		va_start(ap, fmt);
		vsnprintf(buf, len, fmt, ap);
		va_end(ap);
	}

	return buf;
}

static inline void *
vzalloc(unsigned long size)
{
	return malloc(size, M_DRM, M_WAITOK | M_CANFAIL | M_ZERO);
}

static inline void
vfree(void *objp)
{
	free(objp, M_DRM, 0);
}

struct kref {
	uint32_t refcount;
};

static inline void
kref_init(struct kref *ref)
{
	ref->refcount = 1;
}

static inline void
kref_get(struct kref *ref)
{
	atomic_inc_int(&ref->refcount);
}

static inline int
kref_get_unless_zero(struct kref *ref)
{
	if (ref->refcount != 0) {
		atomic_inc_int(&ref->refcount);
		return (1);
	} else {
		return (0);
	}
}

static inline void
kref_put(struct kref *ref, void (*release)(struct kref *ref))
{
	if (atomic_dec_int_nv(&ref->refcount) == 0)
		release(ref);
}

static inline void
kref_sub(struct kref *ref, unsigned int v, void (*release)(struct kref *ref))
{
	if (atomic_sub_int_nv(&ref->refcount, v) == 0)
		release(ref);
}

static inline int
kref_put_mutex(struct kref *kref, void (*release)(struct kref *kref),
    struct rwlock *lock)
{
	if (!atomic_add_unless(&kref->refcount, -1, 1)) {
		rw_enter_write(lock);
		if (likely(atomic_dec_and_test(&kref->refcount))) {
			release(kref);
			return 1;
		}
		rw_exit_write(lock);
		return 0;
	}

	return 0;
}

struct kobject {
	struct kref kref;
	struct kobj_type *type;
};

struct kobj_type {
	void (*release)(struct kobject *);
};

static inline void
kobject_init(struct kobject *obj, struct kobj_type *type)
{
	kref_init(&obj->kref);
	obj->type = type;
}

static inline int
kobject_init_and_add(struct kobject *obj, struct kobj_type *type,
    struct kobject *parent, const char *fmt, ...)
{
	kobject_init(obj, type);
	return (0);
}

static inline struct kobject *
kobject_get(struct kobject *obj)
{
	if (obj != NULL)
		kref_get(&obj->kref);
	return (obj);
}

static inline void
kobject_release(struct kref *ref)
{
	struct kobject *obj = container_of(ref, struct kobject, kref);
	if (obj->type && obj->type->release)
		obj->type->release(obj);
}

static inline void
kobject_put(struct kobject *obj)
{
	if (obj != NULL)
		kref_put(&obj->kref, kobject_release);
}

static inline void
kobject_del(struct kobject *obj)
{
}

struct idr_entry {
	SPLAY_ENTRY(idr_entry) entry;
	int id;
	void *ptr;
};

struct idr {
	SPLAY_HEAD(idr_tree, idr_entry) tree;
};

void idr_init(struct idr *);
void idr_preload(unsigned int);
int idr_alloc(struct idr *, void *, int, int, unsigned int);
#define idr_preload_end()
void *idr_find(struct idr *, int);
void *idr_replace(struct idr *, void *ptr, int);
void idr_remove(struct idr *, int);
void idr_destroy(struct idr *);
int idr_for_each(struct idr *, int (*)(int, void *, void *), void *);
void *idr_get_next(struct idr *, int *);

#define idr_for_each_entry(idp, entry, id) \
	for (id = 0; ((entry) = idr_get_next(idp, &(id))) != NULL; id++)


struct ida {
	int counter;
};

void ida_init(struct ida *);
void ida_destroy(struct ida *);
int ida_simple_get(struct ida *, unsigned int, unsigned nt, int);
void ida_remove(struct ida *, int);

struct notifier_block {
	void *notifier_call;
};

#define register_reboot_notifier(x)
#define unregister_reboot_notifier(x)

#define SYS_RESTART 0

#define min_t(t, a, b) ({ \
	t __min_a = (a); \
	t __min_b = (b); \
	__min_a < __min_b ? __min_a : __min_b; })

#define max_t(t, a, b) ({ \
	t __max_a = (a); \
	t __max_b = (b); \
	__max_a > __max_b ? __max_a : __max_b; })

#define clamp_t(t, x, a, b) min_t(t, max_t(t, x, a), b)
#define clamp(x, a, b) clamp_t(__typeof(x), x, a, b)

#define min3(x, y, z) MIN(x, MIN(y, z))

#define do_div(n, base) ({				\
	uint32_t __base = (base);			\
	uint32_t __rem = ((uint64_t)(n)) % __base;	\
	(n) = ((uint64_t)(n)) / __base;			\
	__rem;						\
})

static inline uint64_t
div_u64(uint64_t x, uint32_t y)
{
	return (x / y);
}

static inline int64_t
div_s64(int64_t x, int64_t y)
{
	return (x / y);
}

static inline uint64_t
div64_u64(uint64_t x, uint64_t y)
{
	return (x / y);
}

static inline uint64_t
div64_u64_rem(uint64_t x, uint64_t y, uint64_t *rem)
{
	*rem = x % y;
	return (x / y);
}

static inline int64_t
div64_s64(int64_t x, int64_t y)
{
	return (x / y);
}

#define mult_frac(x, n, d) (((x) * (n)) / (d))

static inline int64_t
abs64(int64_t x)
{
	return (x < 0 ? -x : x);
}

static inline unsigned long
__copy_to_user(void *to, const void *from, unsigned len)
{
	if (copyout(from, to, len))
		return len;
	return 0;
}

static inline unsigned long
copy_to_user(void *to, const void *from, unsigned len)
{
	return __copy_to_user(to, from, len);
}

static inline unsigned long
__copy_from_user(void *to, const void *from, unsigned len)
{
	if (copyin(from, to, len))
		return len;
	return 0;
}

static inline unsigned long
copy_from_user(void *to, const void *from, unsigned len)
{
	return __copy_from_user(to, from, len);
}

#define get_user(x, ptr)	-copyin(ptr, &(x), sizeof(x))
#define put_user(x, ptr)	-copyout(&(x), ptr, sizeof(x))

#define console_lock()
#define console_trylock()	1
#define console_unlock()

#ifndef PCI_MEM_START
#define PCI_MEM_START	0
#endif

#ifndef PCI_MEM_END
#define PCI_MEM_END	0xffffffff
#endif

enum dmi_field {
        DMI_NONE,
        DMI_BIOS_VENDOR,
        DMI_BIOS_VERSION,
        DMI_BIOS_DATE,
        DMI_SYS_VENDOR,
        DMI_PRODUCT_NAME,
        DMI_PRODUCT_VERSION,
        DMI_PRODUCT_SERIAL,
        DMI_PRODUCT_UUID,
        DMI_BOARD_VENDOR,
        DMI_BOARD_NAME,
        DMI_BOARD_VERSION,
        DMI_BOARD_SERIAL,
        DMI_BOARD_ASSET_TAG,
        DMI_CHASSIS_VENDOR,
        DMI_CHASSIS_TYPE,
        DMI_CHASSIS_VERSION,
        DMI_CHASSIS_SERIAL,
        DMI_CHASSIS_ASSET_TAG,
        DMI_STRING_MAX,
};

struct dmi_strmatch {
	unsigned char slot;
	char substr[79];
};

struct dmi_system_id {
        int (*callback)(const struct dmi_system_id *);
        const char *ident;
        struct dmi_strmatch matches[4];
};
#define	DMI_MATCH(a, b) {(a), (b)}
#define	DMI_EXACT_MATCH(a, b) {(a), (b)}
int dmi_check_system(const struct dmi_system_id *);

struct resource {
	u_long	start;
};

struct pci_bus {
	pci_chipset_tag_t pc;
	unsigned char	number;
};

struct pci_dev {
	struct pci_bus	_bus;
	struct pci_bus	*bus;

	unsigned int	devfn;
	uint16_t	vendor;
	uint16_t	device;
	uint16_t	subsystem_vendor;
	uint16_t	subsystem_device;
	uint8_t		revision;

	pci_chipset_tag_t pc;
	pcitag_t	tag;
	struct pci_softc *pci;

	int		irq;
	int		msi_enabled;
};
#define PCI_ANY_ID (uint16_t) (~0U)

#define PCI_VENDOR_ID_ASUSTEK	PCI_VENDOR_ASUSTEK
#define PCI_VENDOR_ID_ATI	PCI_VENDOR_ATI
#define PCI_VENDOR_ID_DELL	PCI_VENDOR_DELL
#define PCI_VENDOR_ID_HP	PCI_VENDOR_HP
#define PCI_VENDOR_ID_IBM	PCI_VENDOR_IBM
#define PCI_VENDOR_ID_INTEL	PCI_VENDOR_INTEL
#define PCI_VENDOR_ID_SONY	PCI_VENDOR_SONY
#define PCI_VENDOR_ID_VIA	PCI_VENDOR_VIATECH

#define PCI_DEVICE_ID_ATI_RADEON_QY	PCI_PRODUCT_ATI_RADEON_QY

#define PCI_DEVFN(slot, func)	((slot) << 3 | (func))
#define PCI_SLOT(devfn)		((devfn) >> 3)
#define PCI_FUNC(devfn)		((devfn) & 0x7)

#define pci_dev_put(x)


static inline int
pci_read_config_dword(struct pci_dev *pdev, int reg, u32 *val)
{
	*val = pci_conf_read(pdev->pc, pdev->tag, reg);
	return 0;
} 

static inline int
pci_read_config_word(struct pci_dev *pdev, int reg, u16 *val)
{
	uint32_t v;

	v = pci_conf_read(pdev->pc, pdev->tag, (reg & ~0x2));
	*val = (v >> ((reg & 0x2) * 8));
	return 0;
} 

static inline int
pci_read_config_byte(struct pci_dev *pdev, int reg, u8 *val)
{
	uint32_t v;

	v = pci_conf_read(pdev->pc, pdev->tag, (reg & ~0x3));
	*val = (v >> ((reg & 0x3) * 8));
	return 0;
} 

static inline int
pci_write_config_dword(struct pci_dev *pdev, int reg, u32 val)
{
	pci_conf_write(pdev->pc, pdev->tag, reg, val);
	return 0;
} 

static inline int
pci_write_config_word(struct pci_dev *pdev, int reg, u16 val)
{
	uint32_t v;

	v = pci_conf_read(pdev->pc, pdev->tag, (reg & ~0x2));
	v &= ~(0xffff << ((reg & 0x2) * 8));
	v |= (val << ((reg & 0x2) * 8));
	pci_conf_write(pdev->pc, pdev->tag, (reg & ~0x2), v);
	return 0;
} 

static inline int
pci_write_config_byte(struct pci_dev *pdev, int reg, u8 val)
{
	uint32_t v;

	v = pci_conf_read(pdev->pc, pdev->tag, (reg & ~0x3));
	v &= ~(0xff << ((reg & 0x3) * 8));
	v |= (val << ((reg & 0x3) * 8));
	pci_conf_write(pdev->pc, pdev->tag, (reg & ~0x3), v);
	return 0;
}

static inline int
pci_bus_read_config_word(struct pci_bus *bus, unsigned int devfn,
    int reg, u16 *val)
{
	pcitag_t tag = pci_make_tag(bus->pc, bus->number,
	    PCI_SLOT(devfn), PCI_FUNC(devfn));
	uint32_t v;

	v = pci_conf_read(bus->pc, tag, (reg & ~0x2));
	*val = (v >> ((reg & 0x2) * 8));
	return 0;
}

static inline int
pci_bus_read_config_byte(struct pci_bus *bus, unsigned int devfn,
    int reg, u8 *val)
{
	pcitag_t tag = pci_make_tag(bus->pc, bus->number,
	    PCI_SLOT(devfn), PCI_FUNC(devfn));
	uint32_t v;

	v = pci_conf_read(bus->pc, tag, (reg & ~0x3));
	*val = (v >> ((reg & 0x3) * 8));
	return 0;
}

#define pci_set_master(x)

#define pci_enable_msi(x)
#define pci_disable_msi(x)

typedef enum {
	PCI_D0,
	PCI_D1,
	PCI_D2,
	PCI_D3hot,
	PCI_D3cold
} pci_power_t;

#define pci_save_state(x)
#define pci_enable_device(x)	0
#define pci_disable_device(x)

#if defined(__amd64__) || defined(__i386__)

#define AGP_USER_MEMORY			0
#define AGP_USER_CACHED_MEMORY		BUS_DMA_COHERENT

#define PCI_DMA_BIDIRECTIONAL	0

static inline dma_addr_t
pci_map_page(struct pci_dev *pdev, struct vm_page *page, unsigned long offset, size_t size, int direction)
{
	return VM_PAGE_TO_PHYS(page);
}

static inline void
pci_unmap_page(struct pci_dev *pdev, dma_addr_t dma_address, size_t size, int direction)
{
}

static inline int
pci_dma_mapping_error(struct pci_dev *pdev, dma_addr_t dma_addr)
{
	return 0;
}

#define dma_set_coherent_mask(x, y)

#define VGA_RSRC_LEGACY_IO	0x01

void vga_get_uninterruptible(struct pci_dev *, int);
void vga_put(struct pci_dev *, int);

static inline int
vga_client_register(struct pci_dev *a, void *b, void *c, void *d)
{
	return -ENODEV;
}

#define vga_switcheroo_register_client(a, b, c)	0
#define vga_switcheroo_unregister_client(a)
#define vga_switcheroo_process_delayed_switch()

#endif

#define memcpy_toio(d, s, n)	memcpy(d, s, n)
#define memcpy_fromio(d, s, n)	memcpy(d, s, n)
#define memset_io(d, b, n)	memset(d, b, n)

static inline u32
ioread32(const volatile void __iomem *addr)
{
	return (*(volatile uint32_t *)addr);
}

static inline u64
ioread64(const volatile void __iomem *addr)
{
	return (*(volatile uint64_t *)addr);
}

static inline void
iowrite32(u32 val, volatile void __iomem *addr)
{
	*(volatile uint32_t *)addr = val;
}

#define readl(p) ioread32(p)
#define writel(v, p) iowrite32(v, p)
#define readq(p) ioread64(p)

#define page_to_phys(page)	(VM_PAGE_TO_PHYS(page))
#define page_to_pfn(pp)		(VM_PAGE_TO_PHYS(pp) / PAGE_SIZE)
#define offset_in_page(off)	((off) & PAGE_MASK)
#define set_page_dirty(page)	atomic_clearbits_int(&page->pg_flags, PG_CLEAN)

#define VERIFY_READ	0x1
#define VERIFY_WRITE	0x2
static inline int
access_ok(int type, const void *addr, unsigned long size)
{
	return true;
}

#define CAP_SYS_ADMIN	0x1
static inline int
capable(int cap)
{
	KASSERT(cap == CAP_SYS_ADMIN);
	return suser(curproc, 0);
}

typedef int pgprot_t;
#define pgprot_val(v)	(v)
#define PAGE_KERNEL	0

void	*kmap(struct vm_page *);
void	 kunmap(void *addr);
void	*vmap(struct vm_page **, unsigned int, unsigned long, pgprot_t);
void	 vunmap(void *, size_t);

#define round_up(x, y) ((((x) + ((y) - 1)) / (y)) * (y))
#define round_down(x, y) (((x) / (y)) * (y))
#define roundup2(x, y) (((x)+((y)-1))&(~((y)-1))) /* if y is powers of two */
#define DIV_ROUND_UP(x, y)	(((x) + ((y) - 1)) / (y))
#define DIV_ROUND_UP_ULL(x, y)	DIV_ROUND_UP(x, y)
#define DIV_ROUND_CLOSEST(x, y)	(((x) + ((y) / 2)) / (y))
#define DIV_ROUND_CLOSEST_ULL(x, y)	DIV_ROUND_CLOSEST(x, y)

static inline unsigned long
roundup_pow_of_two(unsigned long x)
{
	return (1UL << flsl(x - 1));
}

#define is_power_of_2(x)	(x != 0 && (((x) - 1) & (x)) == 0)

#define PAGE_ALIGN(addr)	(((addr) + PAGE_MASK) & ~PAGE_MASK)
#define IS_ALIGNED(x, y)	(((x) & ((y) - 1)) == 0)

static __inline void
udelay(unsigned long usecs)
{
	DELAY(usecs);
}

static __inline void
ndelay(unsigned long nsecs)
{
	DELAY(max(nsecs / 1000, 1));
}

static __inline void
usleep_range(unsigned long min, unsigned long max)
{
	DELAY(min);
}

static __inline void
mdelay(unsigned long msecs)
{
	int loops = msecs;
	while (loops--)
		DELAY(1000);
}

static __inline void
cpu_relax(void)
{
	CPU_BUSY_CYCLE();
	if (cold) {
		delay(tick);
		jiffies++;
	}
}

#define cpu_relax_lowlatency() CPU_BUSY_CYCLE()
#define cpu_has_pat	1
#define cpu_has_clflush	1

static inline uint32_t ror32(uint32_t word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
}

static inline int
irqs_disabled(void)
{
	return (cold);
}

static inline int
in_dbg_master(void)
{
#ifdef DDB
	return (db_is_active);
#endif
	return (0);
}

#define oops_in_progress in_dbg_master()

static inline int
power_supply_is_system_supplied(void)
{
	/* XXX return 0 if on battery */
	return (1);
}

#define pm_qos_update_request(x, y)
#define pm_qos_remove_request(x)

#define _U      0x01
#define _L      0x02
#define _N      0x04
#define _S      0x08
#define _P      0x10
#define _C      0x20
#define _X      0x40
#define _B      0x80

static inline int
isascii(int c)
{
	return ((unsigned int)c <= 0177);
}

static inline int
isprint(int c)
{
	if (c == -1)
		return (0);
	if ((unsigned char)c >= 040 && (unsigned char)c <= 0176)
		return (1);
	return (0);
}

#ifdef __macppc__
static __inline int
of_machine_is_compatible(const char *model)
{
	extern char *hw_prod;
	return (strcmp(model, hw_prod) == 0);
}
#endif

typedef unsigned int gfp_t;

struct vm_page *alloc_pages(unsigned int, unsigned int);
void	__free_pages(struct vm_page *, unsigned int);

static inline struct vm_page *
alloc_page(unsigned int gfp_mask)
{
	return alloc_pages(gfp_mask, 0);
}

static inline void
__free_page(struct vm_page *page)
{
	return __free_pages(page, 0);
}

static inline unsigned int
get_order(size_t size)
{
	return flsl((size - 1) >> PAGE_SHIFT);
}

#if defined(__i386__) || defined(__amd64__)

#define _PAGE_PRESENT	PG_V
#define _PAGE_RW	PG_RW
#define _PAGE_PAT	PG_PAT
#define _PAGE_PWT	PG_WT
#define _PAGE_PCD	PG_N

static inline void
pagefault_disable(void)
{
	KASSERT(curcpu()->ci_inatomic == 0);
	curcpu()->ci_inatomic = 1;
}

static inline void
pagefault_enable(void)
{
	KASSERT(curcpu()->ci_inatomic == 1);
	curcpu()->ci_inatomic = 0;
}

static inline int
pagefault_disabled(void)
{
	return curcpu()->ci_inatomic;
}

static inline void *
kmap_atomic(struct vm_page *pg)
{
	vaddr_t va;

#if defined (__HAVE_PMAP_DIRECT)
	va = pmap_map_direct(pg);
#else
	extern vaddr_t pmap_tmpmap_pa(paddr_t);
	va = pmap_tmpmap_pa(VM_PAGE_TO_PHYS(pg));
#endif
	return (void *)va;
}

static inline void
kunmap_atomic(void *addr)
{
#if defined (__HAVE_PMAP_DIRECT)
	pmap_unmap_direct((vaddr_t)addr);
#else
	extern void pmap_tmpunmap_pa(void);
	pmap_tmpunmap_pa();
#endif
}

static inline unsigned long
__copy_to_user_inatomic(void *to, const void *from, unsigned len)
{
	struct cpu_info *ci = curcpu();
	int inatomic = ci->ci_inatomic;
	int error;

	ci->ci_inatomic = 1;
	error = copyout(from, to, len);
	ci->ci_inatomic = inatomic;

	return (error ? len : 0);
}

static inline unsigned long
__copy_from_user_inatomic(void *to, const void *from, unsigned len)
{
	struct cpu_info *ci = curcpu();
	int inatomic = ci->ci_inatomic;
	int error;

	ci->ci_inatomic = 1;
	error = copyin(from, to, len);
	ci->ci_inatomic = inatomic;

	return (error ? len : 0);
}

static inline unsigned long
__copy_from_user_inatomic_nocache(void *to, const void *from, unsigned len)
{
	return __copy_from_user_inatomic(to, from, len);
}

#endif

struct fb_var_screeninfo {
	int pixclock;
};

struct fb_info {
	struct fb_var_screeninfo var;
	void *par;
};

#define FB_BLANK_UNBLANK	0
#define FB_BLANK_NORMAL		1
#define FB_BLANK_HSYNC_SUSPEND	2
#define FB_BLANK_VSYNC_SUSPEND	3
#define FB_BLANK_POWERDOWN	4

#define FBINFO_STATE_RUNNING	0
#define FBINFO_STATE_SUSPENDED	1

#define framebuffer_alloc(flags, device) \
	kzalloc(sizeof(struct fb_info), GFP_KERNEL)

struct address_space;
#define unmap_mapping_range(mapping, holebegin, holeend, even_cows)

/*
 * ACPI types and interfaces.
 */

typedef size_t acpi_size;
typedef int acpi_status;

struct acpi_table_header;

#define ACPI_SUCCESS(x) ((x) == 0)

#define AE_NOT_FOUND	0x0005

acpi_status acpi_get_table_with_size(const char *, int, struct acpi_table_header **, acpi_size *);

#define acpi_video_register()
#define acpi_video_unregister()

#define MIPI_DSI_GENERIC_SHORT_WRITE_0_PARAM	0x03
#define MIPI_DSI_GENERIC_SHORT_WRITE_1_PARAM	0x13
#define MIPI_DSI_GENERIC_SHORT_WRITE_2_PARAM	0x23
#define MIPI_DSI_GENERIC_READ_REQUEST_0_PARAM	0x04
#define MIPI_DSI_GENERIC_READ_REQUEST_1_PARAM	0x14
#define MIPI_DSI_GENERIC_READ_REQUEST_2_PARAM	0x24
#define MIPI_DSI_DCS_SHORT_WRITE		0x05
#define MIPI_DSI_DCS_SHORT_WRITE_PARAM		0x15
#define MIPI_DSI_DCS_READ			0x06
#define MIPI_DSI_GENERIC_LONG_WRITE		0x29
#define MIPI_DSI_DCS_LONG_WRITE			0x39

struct pwm_device;

static inline struct pwm_device *
pwm_get(struct device *dev, const char *consumer)
{
	return ERR_PTR(-ENODEV);
}

static inline void
pwm_put(struct pwm_device *pwm)
{
}

static inline unsigned int
pwm_get_duty_cycle(const struct pwm_device *pwm)
{
	return 0;
}

static inline int
pwm_config(struct pwm_device *pwm, int duty_ns, int period_ns)
{
	return -EINVAL;
}

static inline int
pwm_enable(struct pwm_device *pwm)
{
	return -EINVAL;
}

static inline void
pwm_disable(struct pwm_device *pwm)
{
}

struct scatterlist {
	dma_addr_t dma_address;
	unsigned int offset;
	unsigned int length;
};

struct sg_table {
	struct scatterlist *sgl;
	unsigned int nents;
	unsigned int orig_nents;
};

struct sg_page_iter {
	struct scatterlist *sg;
	unsigned int sg_pgoffset;
	unsigned int __nents;
};

int sg_alloc_table(struct sg_table *, unsigned int, gfp_t);
void sg_free_table(struct sg_table *);

#define sg_mark_end(x)

static __inline void
__sg_page_iter_start(struct sg_page_iter *iter, struct scatterlist *sgl,
    unsigned int nents, unsigned long pgoffset)
{
	iter->sg = sgl;
	iter->sg_pgoffset = pgoffset - 1;
	iter->__nents = nents;
}

static inline bool
__sg_page_iter_next(struct sg_page_iter *iter)
{
	iter->sg_pgoffset++;
	while (iter->__nents > 0 && 
	    iter->sg_pgoffset >= (iter->sg->length / PAGE_SIZE)) {
		iter->sg_pgoffset -= (iter->sg->length / PAGE_SIZE);
		iter->sg++;
		iter->__nents--;
	}

	return (iter->__nents > 0);
}

static inline paddr_t
sg_page_iter_dma_address(struct sg_page_iter *iter)
{
	return iter->sg->dma_address + (iter->sg_pgoffset << PAGE_SHIFT);
}

static inline struct vm_page *
sg_page_iter_page(struct sg_page_iter *iter)
{
	return PHYS_TO_VM_PAGE(sg_page_iter_dma_address(iter));
}

static inline struct vm_page *
sg_page(struct scatterlist *sgl)
{
	return PHYS_TO_VM_PAGE(sgl->dma_address);
}

#define sg_dma_address(sg)	((sg)->dma_address)
#define sg_dma_len(sg)		((sg)->length)

#define for_each_sg_page(sgl, iter, nents, pgoffset) \
  __sg_page_iter_start((iter), (sgl), (nents), (pgoffset)); \
  while (__sg_page_iter_next(iter))

size_t sg_copy_from_buffer(struct scatterlist *, unsigned int,
    const void *, size_t);

struct firmware {
	const u8 *data;
};

static inline int
request_firmware(const struct firmware **fw, const char *name,
    struct device *device)
{
	return -EINVAL;
}

#define request_firmware_nowait(a, b, c, d, e, f, g) -EINVAL

static inline void
release_firmware(const struct firmware *fw)
{
}

void *memchr_inv(const void *, int, size_t);

#endif
@


1.50
log
@The Linux code really isn't up to the same standard as our other code, and
clang complains about it a lot.  Since we don't want to fix the code,
suppress certain warnings using #pragma clang diagnostic.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.49 2017/04/16 17:16:21 bluhm Exp $	*/
d18 4
d23 5
d29 14
d53 45
d99 4
a102 7
#define IRQ_NONE	0
#define IRQ_HANDLED	1

typedef u_int64_t u64;
typedef u_int32_t u32;
typedef u_int16_t u16;
typedef u_int8_t u8;
d104 8
a111 2
typedef int32_t s32;
typedef int64_t s64;
d113 2
a114 1
typedef uint64_t __u64;
d124 2
d134 3
d155 42
d200 1
a200 1
#define BIT(x)			(1 << x)
d203 58
d267 17
d285 1
d288 4
d300 2
d303 6
a308 6
#define KERN_INFO
#define KERN_WARNING
#define KERN_NOTICE
#define KERN_DEBUG
#define KERN_CRIT
#define KERN_ERR
d312 4
d371 11
d396 1
d399 1
d444 3
d482 15
d539 3
d545 5
d567 1
d581 1
d609 1
a609 1
	int deadline, __error, __error1;		\
d611 1
d624 1
a624 1
	if (__error == ERESTART)			\
d626 1
a626 1
	else if (__error)				\
d691 3
d721 2
d808 12
a819 3
#define flush_workqueue(x)
#define flush_scheduled_work(x)
#define flush_delayed_work(x) (void)(x)
d823 1
d825 10
d837 1
d843 1
d877 1
d879 2
d944 6
d955 6
d983 11
d998 2
d1002 1
d1033 1
a1033 1
kfree(void *objp)
d1035 1
a1035 1
	free(objp, M_DRM, 0);
d1047 21
d1121 17
d1205 1
d1209 23
d1244 1
d1246 8
a1253 2
#define do_div(n, base) \
	n = n / base
d1261 6
d1273 7
a1324 21
static __inline uint16_t
hweight16(uint32_t x)
{
	x = (x & 0x5555) + ((x & 0xaaaa) >> 1);
	x = (x & 0x3333) + ((x & 0xcccc) >> 2);
	x = (x + (x >> 4)) & 0x0f0f;
	x = (x + (x >> 8)) & 0x00ff;
	return (x);
}

static inline uint32_t
hweight32(uint32_t x)
{
	x = (x & 0x55555555) + ((x & 0xaaaaaaaa) >> 1);
	x = (x & 0x33333333) + ((x & 0xcccccccc) >> 2);
	x = (x + (x >> 4)) & 0x0f0f0f0f;
	x = (x + (x >> 8));
	x = (x + (x >> 16)) & 0x000000ff;
	return x;
}

d1326 1
d1379 1
d1392 1
d1397 3
d1418 4
a1421 1
static inline void
d1425 1
d1428 1
a1428 1
static inline void
d1435 1
d1438 1
a1438 1
static inline void
d1445 1
d1448 1
a1448 1
static inline void
d1452 1
d1455 1
a1455 1
static inline void
d1464 1
d1467 1
a1467 1
static inline void
d1476 14
d1492 18
d1518 4
d1524 3
d1546 2
d1553 10
d1627 1
d1666 14
d1709 3
d1746 2
d1771 6
d1792 1
a1792 1
in_atomic(void)
d1867 9
d1896 147
@


1.49
log
@Always evaluate expression in BUG_ON() macro to avoid unused variable
warnings.  Makes non diagnostic kernel compile.
OK kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.48 2016/10/08 05:52:06 guenther Exp $	*/
d20 9
@


1.48
log
@Various printf claim to report the PID, so actually report that and not the TID

ok kettenis@@ tedu@@
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.47 2016/04/05 20:44:03 kettenis Exp $	*/
d165 5
a169 1
#define BUG_ON(x) KASSERT(!(x))
@


1.47
log
@Add an implementation of the Linux "idr" API.
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.46 2016/04/05 08:19:00 kettenis Exp $	*/
d132 1
a132 1
	printf("drm:pid%d:%s *WARNING* " fmt, curproc->p_pid,	\
d135 1
a135 1
	printf("drm:pid%d:%s *NOTICE* " fmt, curproc->p_pid,	\
d138 1
a138 1
	printf("drm:pid%d:%s *ERROR* " fmt, curproc->p_pid,	\
d141 1
a141 1
	printf("drm:pid%d:%s *ERROR* " fmt, curproc->p_pid,	\
d148 1
a148 1
	printf("drm:pid%d:%s *DEBUG* " fmt, curproc->p_pid,	\
@


1.46
log
@Add vma offset manager code.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.45 2016/02/05 15:51:10 kettenis Exp $	*/
d682 1
d835 19
@


1.45
log
@Implement acpi_get_table_with_size().  Will soon be used to read VFCT
tables in radeondrm(4).
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.44 2016/02/05 10:05:12 kettenis Exp $	*/
d33 2
d1381 3
@


1.44
log
@Improve Linux PCI compatibility code.
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.43 2015/12/31 13:01:00 kettenis Exp $	*/
d1379 15
@


1.43
log
@Provide a minimal implementation of the Linux vga_get/vga_put API and use it
in inteldrm(4).

The Intel integrated graphics device has a major design flaw where it needs
legacy VGA io access to disable VGA mode completely.  This only works if
legacy VGA io routing is setup such that it actually reaches the IGD.  This
typically isn't the case if the primary VGA device is a discrete graphics
device.  To make sure we don't whack that device we have to temporarily
route legacy VGA io access to the IGD.

Fixes the "black screen" issue reported by Timo Myrra and others.
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.42 2015/10/17 21:41:12 kettenis Exp $	*/
d978 4
d983 4
d991 1
d1010 2
@


1.42
log
@Fix the code that sets up the MCH BAR on systems where the (buggy) BIOS
doesn't do this for us.  The code was poking registers on the wrong PCI
device.  We were just lucky that it worked on most systems.

This should fix machines such as the Asus EeePC 701 and get rid of the

error: [drm:pid0:i915_gem_detect_bit_6_swizzle] *ERROR* Couldn't read from
MC HBAR.  Disabling tiling.

messages on that machine.
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.41 2015/09/27 21:28:14 kettenis Exp $	*/
d985 1
d1082 5
@


1.41
log
@Enable monitor hot plugging for the framebuffer console.

Tested on the VGA port of a Radeon 7500 and Radeon 9250 (aka 9200 PRO).
Hopefully this works on Intel Graphics as well.
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.40 2015/09/27 11:09:26 jsg Exp $	*/
d973 4
@


1.40
log
@Switch remaining users of the FreeBSD refcount apis back to the original
linux kref/kobject use.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.39 2015/09/26 22:00:00 kettenis Exp $	*/
d1346 12
@


1.39
log
@Try a little bit harder to clean up if attaching inteldrm(4) fails.
The crucial bit is that we now clear dev->dev_priv, which prevents the X
server from opening /dev/drmN and crashing the kernel because the driver
isn't fully initialized.

While there, try a little bit harder to print error messages the proper way.
Things will still look ugly though if the failure is somewhere in the Linux
code.
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.38 2015/09/26 19:52:16 kettenis Exp $	*/
d741 1
a741 1
	uint32_t count;
d747 1
a747 1
	ref->count = 1;
d753 12
a764 1
	atomic_inc_int(&ref->count);
d770 1
a770 1
	if (atomic_dec_int_nv(&ref->count) == 0)
d772 59
@


1.38
log
@Update drm_irq.c to the version from Linux 3.14.52.
Disable the DRM_IOCTL_IRQ_BUSID and DRM_IOCTL_CONTROL ioctls.
These are legacy ioctls for DRI1 support, which we no longer support on
OpenBSD.
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.37 2015/09/26 11:17:15 kettenis Exp $	*/
d428 13
@


1.37
log
@Make the PPGTT code work.  Seems to fix the caching issues on Broadwell.
Comments on some of the later Broadwell-related commits in the Linux tree
seem to say that the PPAT flags in for the (global) GTT are simply broken in
the hardware.
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.36 2015/09/25 20:27:52 kettenis Exp $	*/
d383 1
d621 1
d629 35
@


1.36
log
@Apparently 0 is not a power of 2 (despite <sys/param.h> claiming that it is).
Fixes inteldrm(4) on the GM45 chipset.
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.35 2015/09/24 20:52:28 kettenis Exp $	*/
d937 23
d983 1
d1122 21
@


1.35
log
@Properly implement waitqueue_active().  Gets rid of spurious

  *ERROR* Hangcheck timer elapsed... xxx ring idle

messages.
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.34 2015/09/23 23:12:11 kettenis Exp $	*/
d1005 1
a1005 1
#define is_power_of_2(x)	((((x)-1)&(x))==0)
@


1.34
log
@Update inteldrm to the code from Linux 3.14.52 (which corresponds to
commit 48f8f36a6c8018c2b36ea207aaf68ef5326c5075 on the linux-3.14.y
branch of the linux-stable tree).  This brings preliminary support for
the GPU on Intel's Broadwell CPUs.  Don't expect these to work
perfectly yet.  There are some remaining issues with older hardware as
well, but no significant regressions have been uncovered.

This also updates some of drm core code.  The radeondrm code remains
based on Linux 3.8 with some minimal canges to adjust to changes in
the core drm APIs.

Joint effort with jsg@@, who did the initial update of the relevant drm
core bits.  Committing this early to make sure it gets more testing
and make it possible for others to help getting the remaining wrinkles
straightened out.
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.33 2015/07/16 18:48:51 kettenis Exp $	*/
d3 1
a3 1
 * Copyright (c) 2013, 2014 Mark Kettenis
d293 1
d301 1
d310 1
d313 1
d321 1
d328 1
d350 1
d359 1
d384 1
a384 1
#define waitqueue_active(x)		true
@


1.33
log
@Introduce a Linux compatible wait_event API and use it in the inteldrm code.

ok jsg@@, guenther@@
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.32 2015/07/11 04:00:46 jsg Exp $	*/
d18 1
d41 2
d44 1
a44 1
#define __always_unused
d50 2
d70 4
d77 2
d121 1
d125 1
d166 1
d211 4
a214 1
#define TRACE_EVENT(name, proto, args, struct, assign, print) \
d275 2
d288 3
d315 1
a315 1
	int deadline, error;			\
d322 2
a323 2
	error = sleep_finish_timeout(&sls);	\
	if (ret < 0 || error == EWOULDBLOCK)	\
d342 1
a342 1
	int deadline, error, error1;		\
d350 3
a352 3
	error1 = sleep_finish_timeout(&sls);	\
	error = sleep_finish_signal(&sls);	\
	if (ret < 0 || error1 == EWOULDBLOCK)	\
d354 1
a354 1
	if (error == ERESTART)			\
d356 2
a357 2
	else if (error)				\
		ret = -error;			\
d376 2
d430 1
d475 6
d496 8
d519 8
d535 11
d551 8
d611 10
d623 1
d662 9
d682 23
d715 5
d733 8
d851 2
d867 62
d936 7
a942 3
	u32 r;
	memcpy(&r, (void *)addr, 4);
	return (r);
d948 1
a948 1
	memcpy((void *)addr, &val, 4);
d951 3
d959 16
d988 1
d997 2
d1009 6
d1048 2
d1055 25
@


1.32
log
@Make use of recent drm_linux.h additions to further reduce the
diff to linux.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.31 2015/06/26 15:22:23 kettenis Exp $	*/
d279 70
@


1.31
log
@Add Linux completion API and use it.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.30 2015/06/24 19:01:51 kettenis Exp $	*/
d402 1
a402 1
static inline void
d405 3
a407 2
	timeout_del(&dwork->to);
	task_del(dwork->tq, &dwork->work.task);
@


1.30
log
@#undef HZ before defining it such that compiling a kernel with -DHZ=xxx works.
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.29 2015/06/24 17:59:42 kettenis Exp $	*/
d283 40
@


1.29
log
@Linux jiffies and OpenBSD ticks are the same thing.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.28 2015/06/24 08:32:39 kettenis Exp $	*/
d379 1
@


1.28
log
@Introduce Linux work queue APIs and use them.  As a side-effect, this will
move some of the work from the system task queue to the driver-specific
task queue.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.27 2015/04/18 14:47:34 jsg Exp $	*/
d377 2
@


1.27
log
@another round of reducing the diff to linux
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.26 2015/04/18 11:41:28 jsg Exp $	*/
d18 2
d284 85
d376 8
@


1.26
log
@define and use trace macros
discussed with kettenis
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.25 2015/04/18 11:05:32 jsg Exp $	*/
d45 2
d91 8
d101 1
d235 1
d294 2
d403 6
d555 1
d619 7
@


1.25
log
@add and use module param macros
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.24 2015/04/12 17:10:07 kettenis Exp $	*/
d178 11
@


1.24
log
@Add a few missing trace functions, and "use" them.  Add back the WATCH_GTT
code (that isn't actually compiled in).  Use dev_priv->dev in one more place
now that we have it, and add set_normalized_timespec() and use it.
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.23 2015/04/12 12:14:30 jsg Exp $	*/
d63 1
d65 3
@


1.23
log
@change back to linux style pci vendor/device defines
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.22 2015/04/12 05:31:23 jsg Exp $	*/
d267 12
@


1.22
log
@Switch back to ioread32 and iowrite32 for cases where bus_space_vaddr is
used instead of bus_space_read/bus_space_write.
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.21 2015/04/12 03:54:10 jsg Exp $	*/
d475 11
@


1.21
log
@make wait_queue_head a struct with a mutex
better matches linux behaviour
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.20 2015/04/11 14:39:37 jsg Exp $	*/
d479 14
@


1.20
log
@rename i915 interrupt handlers from *_intr back to *_irq_handler
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.19 2015/04/11 05:10:13 jsg Exp $	*/
a36 1
typedef int wait_queue_head_t;
d239 11
@


1.19
log
@change back to spinlock_t/DEFINE_SPINLOCK
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.18 2015/04/11 02:59:05 jsg Exp $	*/
d18 1
@


1.18
log
@add/use max_t()
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.17 2015/04/10 12:06:52 jsg Exp $	*/
d210 1
@


1.17
log
@Move irqs_disabled() and in_dbg_master() out of the i386/amd64 ifdef block
and change drm_can_sleep() to only use in_atomic() on i386/amd64 as
it isn't defined for other archs currently.  Unbreaks the sparc64 build.
Found the hard way by benoit@@
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.16 2015/04/10 05:31:25 jsg Exp $	*/
d338 5
@


1.16
log
@add irqs_disabled() and in_dbg_master() using cold and db_is_active
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.15 2015/04/08 04:03:06 jsg Exp $	*/
d516 15
a559 15
}

static inline int
irqs_disabled(void)
{
	return (cold);
}

static inline int
in_dbg_master(void)
{
#ifdef DDB
	return (db_is_active);
#endif
	return (0);
@


1.15
log
@change back to memcpy_toio/memcpy_fromio/memset_io
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.14 2015/04/08 02:28:13 jsg Exp $	*/
d545 15
@


1.14
log
@ttm has it's own version of kmap/kunmap that uses
kernel_map/uvm_km_valloc and i915 has a version that uses
phys_map/uvm_km_valloc_wait as calling code assumes kmap would
sleep if no memory is available.

Move these and ttm's vmap/vunmap into the linux compat files
and make them all use phys_map/uvm_km_valloc_wait.

looks good kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.13 2015/04/06 15:43:15 jsg Exp $	*/
d459 3
a461 1
#define memcpy_toio(d, s, n) memcpy(d, s, n)
@


1.13
log
@Move almost all of the linux compat from drmP.h to drm_linux.h.
The exception being the barrier defines that are implemented in terms of
DRM_* defines.

ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.12 2015/04/06 12:25:10 jsg Exp $	*/
d464 9
@


1.12
log
@move some inline linux compat into the dedicated files
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.11 2015/04/06 08:14:00 kettenis Exp $	*/
d21 13
d41 30
d125 84
d251 5
d283 62
d451 8
d461 1
a461 1
#define page_to_phys(page)	(VM_PAGE_TO_PHYS(page)
d467 46
@


1.11
log
@Add get_user() and put_user() compatibility interfaces and use them.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.10 2015/04/06 05:35:29 jsg Exp $	*/
d116 1
d118 1
d121 2
d151 6
d187 78
@


1.10
log
@add and use macros for wake_up/wake_up_all/wake_up_all_locked
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.9 2015/04/05 11:53:53 kettenis Exp $	*/
d174 3
@


1.9
log
@Another round of reducing diffs with Linux.  This one moves the various
copy_to_user and copy_from_user functions into drm_linux.h and uses them
instead of copyin/copyout and DRM_COPY_*.  Also move the timespec functions,
and put i915_gem_object_is_purgable() where it belongs.

Uncovered a bug where the arguments to copyout() were in the wrong order.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.8 2015/04/03 13:10:59 jsg Exp $	*/
d112 3
@


1.8
log
@resync i915_drv.h to make it diffable to linux
ok kettenis@@
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.7 2015/02/12 11:11:45 jsg Exp $	*/
d112 60
d217 34
@


1.7
log
@switch back to IRQ_NONE/IRQ_HANDLED
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.6 2015/02/12 06:30:56 jsg Exp $	*/
d21 4
d26 3
d81 2
@


1.6
log
@switch MUTEX_ASSERT_LOCKED calls back to assert_spin_locked
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.5 2015/02/12 02:12:02 kettenis Exp $	*/
d17 3
@


1.5
log
@Add mutex_is_locked and use it wherever linux uses it.
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.4 2015/02/11 07:01:36 jsg Exp $	*/
d86 1
@


1.4
log
@Switch most printf style functions calls back to linux function names
and move DRM_INFO/pr_info/dev_info messages under DRMDEBUG.
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.3 2015/02/10 00:23:53 jsg Exp $	*/
d89 1
@


1.3
log
@add definitions for linux style locks
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.2 2014/09/20 21:17:43 kettenis Exp $	*/
d19 52
@


1.2
log
@Make another fast path properly atomic.
@
text
@d1 1
a1 1
/*	$OpenBSD: drm_linux.h,v 1.1 2014/04/01 20:16:50 kettenis Exp $	*/
d19 26
@


1.1
log
@Move some duplicated code implementing Linux compatibility APIs and stick it
in a seperate header file.  This will become a dumping ground for similar code.

ok jsg@@
@
text
@d1 1
a1 1
/*	$OpenBSD$	*/
d17 2
@

