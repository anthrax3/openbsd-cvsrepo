head	1.54;
access;
symbols
	OPENBSD_4_7:1.50.0.2
	OPENBSD_4_7_BASE:1.50
	OPENBSD_4_6:1.46.0.4
	OPENBSD_4_6_BASE:1.46
	OPENBSD_4_5:1.42.0.2
	OPENBSD_4_5_BASE:1.42
	OPENBSD_4_4:1.6.0.2
	OPENBSD_4_4_BASE:1.6
	OPENBSD_4_3:1.1.0.2
	OPENBSD_4_3_BASE:1.1;
locks; strict;
comment	@ * @;


1.54
date	2010.05.25.17.15.49;	author oga;	state dead;
branches;
next	1.53;

1.53
date	2010.05.10.22.28.17;	author oga;	state Exp;
branches;
next	1.52;

1.52
date	2010.04.29.15.23.28;	author oga;	state Exp;
branches;
next	1.51;

1.51
date	2010.04.08.11.45.54;	author oga;	state Exp;
branches;
next	1.50;

1.50
date	2009.12.14.22.29.35;	author matthieu;	state Exp;
branches;
next	1.49;

1.49
date	2009.11.23.23.05.52;	author oga;	state Exp;
branches;
next	1.48;

1.48
date	2009.11.23.23.03.17;	author oga;	state Exp;
branches;
next	1.47;

1.47
date	2009.11.23.21.19.43;	author oga;	state Exp;
branches;
next	1.46;

1.46
date	2009.04.06.20.56.25;	author oga;	state Exp;
branches;
next	1.45;

1.45
date	2009.04.05.00.32.54;	author oga;	state Exp;
branches;
next	1.44;

1.44
date	2009.04.03.14.41.23;	author oga;	state Exp;
branches;
next	1.43;

1.43
date	2009.03.05.23.13.19;	author oga;	state Exp;
branches;
next	1.42;

1.42
date	2009.02.15.23.10.48;	author oga;	state Exp;
branches;
next	1.41;

1.41
date	2009.02.15.20.33.04;	author oga;	state Exp;
branches;
next	1.40;

1.40
date	2009.02.15.20.04.34;	author oga;	state Exp;
branches;
next	1.39;

1.39
date	2009.02.09.06.14.50;	author oga;	state Exp;
branches;
next	1.38;

1.38
date	2009.02.05.00.25.51;	author oga;	state Exp;
branches;
next	1.37;

1.37
date	2008.11.24.07.06.27;	author oga;	state Exp;
branches;
next	1.36;

1.36
date	2008.11.24.03.49.19;	author oga;	state Exp;
branches;
next	1.35;

1.35
date	2008.11.24.03.33.50;	author oga;	state Exp;
branches;
next	1.34;

1.34
date	2008.11.23.21.55.57;	author oga;	state Exp;
branches;
next	1.33;

1.33
date	2008.11.22.22.10.42;	author oga;	state Exp;
branches;
next	1.32;

1.32
date	2008.11.22.20.49.36;	author oga;	state Exp;
branches;
next	1.31;

1.31
date	2008.11.22.14.42.36;	author oga;	state Exp;
branches;
next	1.30;

1.30
date	2008.11.22.05.03.26;	author oga;	state Exp;
branches;
next	1.29;

1.29
date	2008.11.22.04.52.45;	author oga;	state Exp;
branches;
next	1.28;

1.28
date	2008.11.19.16.57.20;	author oga;	state Exp;
branches;
next	1.27;

1.27
date	2008.11.18.16.08.51;	author oga;	state Exp;
branches;
next	1.26;

1.26
date	2008.11.17.00.45.39;	author oga;	state Exp;
branches;
next	1.25;

1.25
date	2008.11.17.00.43.13;	author oga;	state Exp;
branches;
next	1.24;

1.24
date	2008.11.06.14.08.03;	author oga;	state Exp;
branches;
next	1.23;

1.23
date	2008.11.04.00.38.14;	author oga;	state Exp;
branches;
next	1.22;

1.22
date	2008.11.04.00.22.12;	author oga;	state Exp;
branches;
next	1.21;

1.21
date	2008.11.04.00.00.23;	author oga;	state Exp;
branches;
next	1.20;

1.20
date	2008.10.31.19.48.57;	author oga;	state Exp;
branches;
next	1.19;

1.19
date	2008.10.31.18.12.39;	author oga;	state Exp;
branches;
next	1.18;

1.18
date	2008.10.30.21.33.00;	author oga;	state Exp;
branches;
next	1.17;

1.17
date	2008.10.07.22.25.12;	author oga;	state Exp;
branches;
next	1.16;

1.16
date	2008.10.07.21.59.32;	author oga;	state Exp;
branches;
next	1.15;

1.15
date	2008.09.18.15.10.57;	author oga;	state Exp;
branches;
next	1.14;

1.14
date	2008.09.06.01.32.08;	author oga;	state Exp;
branches;
next	1.13;

1.13
date	2008.09.02.20.11.19;	author oga;	state Exp;
branches;
next	1.12;

1.12
date	2008.09.02.20.02.34;	author oga;	state Exp;
branches;
next	1.11;

1.11
date	2008.09.01.17.37.05;	author oga;	state Exp;
branches;
next	1.10;

1.10
date	2008.08.28.18.29.08;	author oga;	state Exp;
branches;
next	1.9;

1.9
date	2008.08.28.02.26.16;	author oga;	state Exp;
branches;
next	1.8;

1.8
date	2008.08.28.01.51.52;	author oga;	state Exp;
branches;
next	1.7;

1.7
date	2008.08.28.01.34.18;	author oga;	state Exp;
branches;
next	1.6;

1.6
date	2008.07.29.22.23.50;	author oga;	state Exp;
branches;
next	1.5;

1.5
date	2008.07.29.19.44.13;	author oga;	state Exp;
branches;
next	1.4;

1.4
date	2008.07.29.19.13.56;	author miod;	state Exp;
branches;
next	1.3;

1.3
date	2008.07.03.18.27.51;	author oga;	state Exp;
branches;
next	1.2;

1.2
date	2008.06.11.09.33.01;	author oga;	state Exp;
branches;
next	1.1;

1.1
date	2007.11.28.23.37.34;	author oga;	state Exp;
branches;
next	;


desc
@@


1.54
log
@Remove the DRI1 code paths from inteldrm as promised two weeks ago.

We no longer support these paths, only memory managed mode is now allowed.
@
text
@/* i915_dma.c -- DMA support for the I915 -*- linux-c -*-
 */
/*
 * Copyright 2003 Tungsten Graphics, Inc., Cedar Park, Texas.
 * All Rights Reserved.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sub license, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice (including the
 * next paragraph) shall be included in all copies or substantial portions
 * of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT.
 * IN NO EVENT SHALL TUNGSTEN GRAPHICS AND/OR ITS SUPPLIERS BE LIABLE FOR
 * ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *
 */

#include "drmP.h"
#include "drm.h"
#include "i915_drm.h"
#include "i915_drv.h"

int	i915_dispatch_batchbuffer(struct drm_device *,
	    drm_i915_batchbuffer_t *, struct drm_clip_rect *);
int	i915_dispatch_cmdbuffer(struct drm_device *,
	    drm_i915_cmdbuffer_t *, struct drm_clip_rect *);

/**
 * Sets up the hardware status page for devices that need a physical address
 * in the register.
 */
int
i915_init_phys_hws(drm_i915_private_t *dev_priv, bus_dma_tag_t dmat)
{
	/* Program Hardware Status Page */
	if ((dev_priv->hws_dmamem = drm_dmamem_alloc(dmat, PAGE_SIZE,
	    PAGE_SIZE, 1, PAGE_SIZE, 0, BUS_DMA_READ)) == NULL) {
		return (ENOMEM);
	}

	dev_priv->hw_status_page = dev_priv->hws_dmamem->kva;

	memset(dev_priv->hw_status_page, 0, PAGE_SIZE);

	bus_dmamap_sync(dmat, dev_priv->hws_dmamem->map, 0, PAGE_SIZE,
	    BUS_DMASYNC_PREREAD);
	I915_WRITE(HWS_PGA, dev_priv->hws_dmamem->map->dm_segs[0].ds_addr);
	DRM_DEBUG("Enabled hardware status page\n");
	return (0);
}

/**
 * Frees the hardware status page, whether it's a physical address of a virtual
 * address set up by the X Server.
 */
void
i915_free_hws(drm_i915_private_t *dev_priv, bus_dma_tag_t dmat)
{
	if (dev_priv->hws_dmamem) {
		drm_dmamem_free(dmat, dev_priv->hws_dmamem);
		dev_priv->hws_dmamem = NULL;
	}

	if (dev_priv->status_gfx_addr) {
		dev_priv->status_gfx_addr = 0;
		bus_space_unmap(dev_priv->bst, dev_priv->hws_map.bsh, 4 * 1024);
		dev_priv->hws_map.bsh = NULL;
	}

	/* Need to rewrite hardware status page */
	I915_WRITE(HWS_PGA, 0x1ffff000);
	dev_priv->hw_status_page = NULL;
}

int
i915_dma_cleanup(struct drm_device *dev)
{
	drm_i915_private_t *dev_priv = dev->dev_private;

	if (dev_priv->ring.ring_obj == NULL && dev_priv->ring.bsh != NULL) {
		bus_space_unmap(dev_priv->bst, dev_priv->ring.bsh,
		    dev_priv->ring.size);
		memset(&dev_priv->ring, 0, sizeof(dev_priv->ring));
	}

	/* Clear the HWS virtual address at teardown */
	if (dev_priv->hws_obj == NULL && I915_NEED_GFX_HWS(dev_priv))
		i915_free_hws(dev_priv, dev->dmat);

	return 0;
}

static int i915_initialize(struct drm_device * dev, drm_i915_init_t * init)
{
	drm_i915_private_t	*dev_priv = dev->dev_private;
	int			 ret;

	dev_priv->sarea = drm_getsarea(dev);
	if (!dev_priv->sarea) {
		DRM_ERROR("can not find sarea!\n");
		i915_dma_cleanup(dev);
		return EINVAL;
	}

	if (init->sarea_priv_offset)
		dev_priv->sarea_priv = (drm_i915_sarea_t *)
		    ((u8 *) dev_priv->sarea->handle + init->sarea_priv_offset);
	else {
		/* No sarea_priv for you! */
		dev_priv->sarea_priv = NULL;
	}

	if (init->ring_size != 0) {
		if (dev_priv->ring.ring_obj != NULL) {
			i915_dma_cleanup(dev);
			DRM_ERROR("Client tried to initialize ringbuffer in "
			    "GEM mode\n");
			return (-EINVAL);
		}
		dev_priv->ring.size = init->ring_size;

		if ((ret = bus_space_map(dev_priv->bst, init->ring_start,
		    init->ring_size, BUS_SPACE_MAP_PREFETCHABLE,
		    &dev_priv->ring.bsh)) != 0) {
			DRM_INFO("can't map ringbuffer\n");
			i915_dma_cleanup(dev);
			return (ret);
		}
	}

	/* Allow hardware batchbuffers unless told otherwise.
	 */
	dev_priv->allow_batchbuffer = 1;

	return 0;
}

static int i915_dma_resume(struct drm_device * dev)
{
	drm_i915_private_t *dev_priv = (drm_i915_private_t *) dev->dev_private;

	DRM_DEBUG("\n");

	if (!dev_priv->sarea) {
		DRM_ERROR("can not find sarea!\n");
		return EINVAL;
	}

	if (dev_priv->ring.bsh == NULL) {
		DRM_ERROR("dma_resume without mapped ring buffer\n");
		return ENOMEM;
	}

	/* Program Hardware Status Page */
	if (!dev_priv->hw_status_page) {
		DRM_ERROR("Can not find hardware status page\n");
		return EINVAL;
	}
	DRM_DEBUG("hw status page @@ %p\n", dev_priv->hw_status_page);

	if (dev_priv->status_gfx_addr != 0)
		I915_WRITE(HWS_PGA, dev_priv->status_gfx_addr);
	else {
		bus_dmamap_sync(dev->dmat, dev_priv->hws_dmamem->map, 0,
		    PAGE_SIZE, BUS_DMASYNC_PREREAD);
		I915_WRITE(HWS_PGA,
		    dev_priv->hws_dmamem->map->dm_segs[0].ds_addr);
	}
	DRM_DEBUG("Enabled hardware status page\n");

	return 0;
}

int i915_dma_init(struct drm_device *dev, void *data,
			 struct drm_file *file_priv)
{
	drm_i915_init_t *init = data;
	int retcode = 0;

	switch (init->func) {
	case I915_INIT_DMA:
		retcode = i915_initialize(dev, init);
		break;
	case I915_CLEANUP_DMA:
		retcode = i915_dma_cleanup(dev);
		break;
	case I915_RESUME_DMA:
		retcode = i915_dma_resume(dev);
		break;
	default:
		retcode = EINVAL;
		break;
	}

	return retcode;
}

/* Implement basically the same security restrictions as hardware does
 * for MI_BATCH_NON_SECURE.  These can be made stricter at any time.
 *
 * Most of the calculations below involve calculating the size of a
 * particular instruction.  It's important to get the size right as
 * that tells us where the next instruction to check is.  Any illegal
 * instruction detected will be given a size of zero, which is a
 * signal to abort the rest of the buffer.
 */
static int validate_cmd(int cmd)
{
	switch (((cmd >> 29) & 0x7)) {
	case 0x0:
		switch ((cmd >> 23) & 0x3f) {
		case 0x0:
			return 1;	/* MI_NOOP */
		case 0x4:
			return 1;	/* MI_FLUSH */
		default:
			return 0;	/* disallow everything else */
		}
		break;
	case 0x1:
		return 0;	/* reserved */
	case 0x2:
		return (cmd & 0xff) + 2;	/* 2d commands */
	case 0x3:
		if (((cmd >> 24) & 0x1f) <= 0x18)
			return 1;

		switch ((cmd >> 24) & 0x1f) {
		case 0x1c:
			return 1;
		case 0x1d:
			switch ((cmd >> 16) & 0xff) {
			case 0x3:
				return (cmd & 0x1f) + 2;
			case 0x4:
				return (cmd & 0xf) + 2;
			default:
				return (cmd & 0xffff) + 2;
			}
		case 0x1e:
			if (cmd & (1 << 23))
				return (cmd & 0xffff) + 1;
			else
				return 1;
		case 0x1f:
			if ((cmd & (1 << 23)) == 0)	/* inline vertices */
				return (cmd & 0x1ffff) + 2;
			else if (cmd & (1 << 17))	/* indirect random */
				if ((cmd & 0xffff) == 0)
					return 0;	/* unknown length, too hard */
				else
					return (((cmd & 0xffff) + 1) / 2) + 1;
			else
				return 2;	/* indirect sequential */
		default:
			return 0;
		}
	default:
		return 0;
	}

	return 0;
}

static int i915_emit_cmds(struct drm_device *dev, int __user *buffer,
			  int dwords)
{
	drm_i915_private_t *dev_priv = dev->dev_private;
	int i;

	if ((dwords + 1) * sizeof(u_int32_t) >= dev_priv->ring.size - 8)
		return EINVAL;

	BEGIN_LP_RING((dwords+1)&~1);

	for (i = 0; i < dwords;) {
		int cmd, sz;

		if (DRM_COPY_FROM_USER(&cmd, &buffer[i], sizeof(cmd)))
			return EINVAL;

		if ((sz = validate_cmd(cmd)) == 0 || i + sz > dwords)
			return EINVAL;

		OUT_RING(cmd);

		while (++i, --sz) {
			if (DRM_COPY_FROM_USER(&cmd, &buffer[i], sizeof(cmd))) {
				return EINVAL;
			}
			OUT_RING(cmd);
		}
	}

	if (dwords & 1)
		OUT_RING(0);

	ADVANCE_LP_RING();

	return 0;
}


/*
 * Emit a box for a cliprect. cliprect must already have been copied in and
 * sanity checked (the reason for this is so that everything can be checked
 * before any gpu state is modified.
 */
void
i915_emit_box(struct drm_device * dev, struct drm_clip_rect *box,
    int DR1, int DR4)
{
	drm_i915_private_t *dev_priv = dev->dev_private;

	if (IS_I965G(dev_priv)) {
		BEGIN_LP_RING(4);
		OUT_RING(GFX_OP_DRAWRECT_INFO_I965);
		OUT_RING((box->x1 & 0xffff) | (box->y1 << 16));
		OUT_RING(((box->x2 - 1) & 0xffff) | ((box->y2 - 1) << 16));
		OUT_RING(DR4);
		ADVANCE_LP_RING();
	} else {
		BEGIN_LP_RING(6);
		OUT_RING(GFX_OP_DRAWRECT_INFO);
		OUT_RING(DR1);
		OUT_RING((box->x1 & 0xffff) | (box->y1 << 16));
		OUT_RING(((box->x2 - 1) & 0xffff) | ((box->y2 - 1) << 16));
		OUT_RING(DR4);
		OUT_RING(0);
		ADVANCE_LP_RING();
	}
}

/* XXX: Emitting the counter should really be moved to part of the IRQ
 * emit. For now, do it in both places:
 */

void i915_emit_breadcrumb(struct drm_device *dev)
{
	drm_i915_private_t *dev_priv = dev->dev_private;

	if (++dev_priv->counter > BREADCRUMB_MASK) {
		 dev_priv->counter = 1;
		 DRM_DEBUG("Breadcrumb counter wrapped around\n");
	}

	if (dev_priv->sarea_priv)
		dev_priv->sarea_priv->last_enqueue = dev_priv->counter;

	BEGIN_LP_RING(4);
	OUT_RING(MI_STORE_DWORD_INDEX);
	OUT_RING(I915_BREADCRUMB_INDEX << MI_STORE_DWORD_INDEX_SHIFT);
	OUT_RING(dev_priv->counter);
	OUT_RING(0);
	ADVANCE_LP_RING();
}

int
i915_dispatch_cmdbuffer(struct drm_device * dev,
    drm_i915_cmdbuffer_t *cmd, struct drm_clip_rect *cliprects)
{
	struct drm_i915_private *dev_priv = dev->dev_private;
	int nbox = cmd->num_cliprects;
	int i = 0, count, ret;

	if (cmd->sz <= 0 || (cmd->sz & 0x3) != 0) {
		DRM_ERROR("negative value or incorrect alignment\n");
		return EINVAL;
	}

	inteldrm_update_ring(dev_priv);

	count = nbox ? nbox : 1;

	for (i = 0; i < count; i++) {
		if (i < nbox)
			i915_emit_box(dev, &cliprects[i], cmd->DR1, cmd->DR4);

		ret = i915_emit_cmds(dev, (int __user *)cmd->buf, cmd->sz / 4);
		if (ret)
			return ret;
	}

	i915_emit_breadcrumb(dev);
	return 0;
}

int
i915_dispatch_batchbuffer(struct drm_device *dev,
    drm_i915_batchbuffer_t *batch, struct drm_clip_rect *cliprects)
{
	drm_i915_private_t	*dev_priv = dev->dev_private;
	int			 nbox = batch->num_cliprects, i = 0, count;

	if ((batch->start | batch->used) & 0x7) {
		DRM_ERROR("alignment\n");
		return EINVAL;
	}

	inteldrm_update_ring(dev_priv);

	/* XXX use gem code */
	count = nbox ? nbox : 1;

	for (i = 0; i < count; i++) {
		if (i < nbox)
			i915_emit_box(dev, &cliprects[i],
			    batch->DR1, batch->DR4);

		if (!IS_I830(dev_priv) && !IS_845G(dev_priv)) {
			BEGIN_LP_RING(2);
			if (IS_I965G(dev_priv)) {
				OUT_RING(MI_BATCH_BUFFER_START | (2 << 6) | MI_BATCH_NON_SECURE_I965);
				OUT_RING(batch->start);
			} else {
				OUT_RING(MI_BATCH_BUFFER_START | (2 << 6));
				OUT_RING(batch->start | MI_BATCH_NON_SECURE);
			}
			ADVANCE_LP_RING();
		} else {
			BEGIN_LP_RING(4);
			OUT_RING(MI_BATCH_BUFFER);
			OUT_RING(batch->start | MI_BATCH_NON_SECURE);
			OUT_RING(batch->start + batch->used - 4);
			OUT_RING(0);
			ADVANCE_LP_RING();
		}
	}

	i915_emit_breadcrumb(dev);

	return 0;
}

int i915_flush_ioctl(struct drm_device *dev, void *data,
			    struct drm_file *file_priv)
{
	drm_i915_private_t	*dev_priv = dev->dev_private;
	int			 ret;

	RING_LOCK_TEST_WITH_RETURN(dev, file_priv);

	DRM_LOCK();
	inteldrm_update_ring(dev_priv);
	ret = inteldrm_wait_ring(dev_priv, dev_priv->ring.size - 8);
	DRM_UNLOCK();

	return (ret);
}

int i915_batchbuffer(struct drm_device *dev, void *data,
			    struct drm_file *file_priv)
{
	struct drm_i915_private	*dev_priv =
				     (drm_i915_private_t *)dev->dev_private;
	drm_i915_batchbuffer_t	*batch = data;
	struct drm_clip_rect	*cliprects = NULL;
	int			 i, ret;

	if (!dev_priv->allow_batchbuffer) {
		DRM_ERROR("Batchbuffer ioctl disabled\n");
		return EINVAL;
	}

	DRM_DEBUG("i915 batchbuffer, start %x used %d cliprects %d\n",
		  batch->start, batch->used, batch->num_cliprects);

	RING_LOCK_TEST_WITH_RETURN(dev, file_priv);

	if (batch->num_cliprects < 0)
		return (EINVAL);

	if (batch->num_cliprects) {
		if (SIZE_MAX / batch->num_cliprects < sizeof(*cliprects))
			return (EINVAL);
		cliprects = drm_alloc(batch->num_cliprects *
		    sizeof(*cliprects));
		if (cliprects == NULL)
			return (ENOMEM);

		ret = copyin((void *)(uintptr_t)batch->cliprects, cliprects,
		    sizeof(*cliprects) * batch->num_cliprects);
		if (ret != 0)
			goto free_cliprects;

		for (i = 0; i < batch->num_cliprects; i++) {
			if (cliprects[i].y2 <= cliprects[i].y1 ||
			    cliprects[i].x2 <= cliprects[i].x1 ||
			    cliprects[i].y2 <= 0 || cliprects[i].x2 <= 0) {
				ret = EINVAL;
				goto free_cliprects;
			}
		}
	}

	DRM_LOCK();
	ret = i915_dispatch_batchbuffer(dev, batch, cliprects);
	DRM_UNLOCK();

	if (dev_priv->sarea_priv != NULL)
		dev_priv->sarea_priv->last_dispatch = READ_BREADCRUMB(dev_priv);
free_cliprects:
	drm_free(cliprects);
	return (ret);
}

int i915_cmdbuffer(struct drm_device *dev, void *data,
			  struct drm_file *file_priv)
{
	struct drm_i915_private	*dev_priv =
				     (drm_i915_private_t *)dev->dev_private;
	drm_i915_cmdbuffer_t	*cmdbuf = data;
	struct drm_clip_rect	*cliprects = NULL;
	int			 i, ret;

	DRM_DEBUG("i915 cmdbuffer, buf %p sz %d cliprects %d\n",
		  cmdbuf->buf, cmdbuf->sz, cmdbuf->num_cliprects);

	RING_LOCK_TEST_WITH_RETURN(dev, file_priv);

	if (cmdbuf->num_cliprects < 0)
		return EINVAL;

	if (cmdbuf->num_cliprects) {
		if (SIZE_MAX / cmdbuf->num_cliprects < sizeof(*cliprects))
			return (EINVAL);
		cliprects = drm_alloc(cmdbuf->num_cliprects *
		    sizeof(*cliprects));
		if (cliprects == NULL)
			return (ENOMEM);

		ret = copyin((void *)(uintptr_t)cmdbuf->cliprects, cliprects,
		    sizeof(*cliprects) * cmdbuf->num_cliprects);
		if (ret != 0)
			goto free_cliprects;

		for (i = 0; i < cmdbuf->num_cliprects; i++) {
			if (cliprects[i].y2 <= cliprects[i].y1 ||
			    cliprects[i].x2 <= cliprects[i].x1 ||
			    cliprects[i].y2 <= 0 || cliprects[i].x2 <= 0) {
				ret = EINVAL;
				goto free_cliprects;
			}
		}
	}

	DRM_LOCK();
	ret = i915_dispatch_cmdbuffer(dev, cmdbuf, cliprects);
	DRM_UNLOCK();
	if (ret == 0 && dev_priv->sarea_priv != NULL)
		dev_priv->sarea_priv->last_dispatch = READ_BREADCRUMB(dev_priv);

free_cliprects:
	drm_free(cliprects);
	return (ret);
}

int i915_getparam(struct drm_device *dev, void *data,
			 struct drm_file *file_priv)
{
	drm_i915_private_t *dev_priv = dev->dev_private;
	drm_i915_getparam_t *param = data;
	int value;

	if (!dev_priv) {
		DRM_ERROR("called with no initialization\n");
		return EINVAL;
	}

	switch (param->param) {
	case I915_PARAM_IRQ_ACTIVE:
		value = dev->irq_enabled;
		break;
	case I915_PARAM_ALLOW_BATCHBUFFER:
		value = dev_priv->allow_batchbuffer ? 1 : 0;
		break;
	case I915_PARAM_LAST_DISPATCH:
		value = READ_BREADCRUMB(dev_priv);
		break;
	case I915_PARAM_CHIPSET_ID:
		value = dev_priv->pci_device;
		break;
	case I915_PARAM_HAS_GEM:
		value = 1;
		break;
	case I915_PARAM_NUM_FENCES_AVAIL:
		value = dev_priv->num_fence_regs - dev_priv->fence_reg_start;
		break;
	case I915_PARAM_HAS_EXECBUF2:
		value = 1;
		break;
	default:
		DRM_DEBUG("Unknown parameter %d\n", param->param);
		return EINVAL;
	}

	if (DRM_COPY_TO_USER(param->value, &value, sizeof(int))) {
		DRM_ERROR("DRM_COPY_TO_USER failed\n");
		return EFAULT;
	}

	return 0;
}

int i915_setparam(struct drm_device *dev, void *data,
			 struct drm_file *file_priv)
{
	drm_i915_private_t *dev_priv = dev->dev_private;
	drm_i915_setparam_t *param = data;

	if (!dev_priv) {
		DRM_ERROR("called with no initialization\n");
		return EINVAL;
	}

	switch (param->param) {
	case I915_SETPARAM_USE_MI_BATCHBUFFER_START:
		break;
	case I915_SETPARAM_TEX_LRU_LOG_GRANULARITY:
		/* We really don't care anymore */
		break;
	case I915_SETPARAM_ALLOW_BATCHBUFFER:
		dev_priv->allow_batchbuffer = param->value;
		break;
	case I915_SETPARAM_NUM_USED_FENCES:
		if (param->value > dev_priv->num_fence_regs ||
		    param->value < 0)
			return EINVAL;
		/* Userspace can use first N regs */
		dev_priv->fence_reg_start = param->value;
		break;
	default:
		DRM_DEBUG("unknown parameter %d\n", param->param);
		return EINVAL;
	}

	return 0;
}

int i915_set_status_page(struct drm_device *dev, void *data,
				struct drm_file *file_priv)
{
	drm_i915_private_t	*dev_priv = dev->dev_private;
	drm_i915_hws_addr_t	*hws = data;
	int			 ret;

	if (!I915_NEED_GFX_HWS(dev_priv))
		return EINVAL;

	if (!dev_priv) {
		DRM_ERROR("called with no initialization\n");
		return EINVAL;
	}
	DRM_DEBUG("set status page addr 0x%08x\n", (u32)hws->addr);

	dev_priv->status_gfx_addr = hws->addr & (0x1ffff<<12);

	dev_priv->hws_map.offset = dev->agp->base + hws->addr;
	if (dev_priv->hws_map.offset > (dev->agp->base +
	    dev->agp->info.ai_aperture_size)) {
		DRM_INFO("tried to map hws past end of aperture!\n");
		return (EINVAL);
	}
	dev_priv->hws_map.size = 4*1024;

	if ((ret = bus_space_map(dev_priv->bst, dev_priv->hws_map.offset,
	    dev_priv->hws_map.size, BUS_SPACE_MAP_LINEAR |
	    BUS_SPACE_MAP_PREFETCHABLE, &dev_priv->hws_map.bsh)) != 0) {
		DRM_INFO("can't hws page\n");
		i915_dma_cleanup(dev);
		dev_priv->status_gfx_addr = 0;
		return (ret);
	}
	dev_priv->hw_status_page = bus_space_vaddr(dev_priv->bst,
	    dev_priv->hws_map.bsh);

	memset(dev_priv->hw_status_page, 0, PAGE_SIZE);
	I915_WRITE(HWS_PGA, dev_priv->status_gfx_addr);
	DRM_DEBUG("load hws HWS_PGA with gfx mem 0x%x\n",
			dev_priv->status_gfx_addr);
	DRM_DEBUG("load hws at %p\n", dev_priv->hw_status_page);
	return 0;
}
@


1.53
log
@unifdef INTELDRM_GEM.

This enabled GEM for the intel driver unconditionally. The legacy
codepaths will be removed in approximately one week since they are now
completely unused.

After discussion with matthieu@@, drahn@@, kettenis@@ and marco@@ (well,
mostly nagging from marco ;).
@
text
@@


1.52
log
@use BUS_SPACE_MAP_PREFETCHABLE on maps that should be WC (soon this will
actually do so).
@
text
@a594 1
#ifdef INTELDRM_GEM
a595 3
#else
		value = 0;
#endif /* INTELDRM_GEM */
@


1.51
log
@DRM memory management for inteldrm(4) using the Graphics Execution
Manager api.

This is currently disabled by default under ifdef INTELDRM_GEM (option
INTELDRM_GEM in a kernel config or a patch to i915_drv.h needed to
enable), mostly because the intel X driver currently in tree does not
always play well with GEM and needs to be switched to UXA accelmethod
(which is the only option on later drivers).

While based on the intel drm code in the linux kernel this has come cleanups and
some obvious behaviour changes:

1) mmap through the GTT is entirely coherent with the gpu cache, removing
mappings whenever the cache dirties so you can not race with userland to write
to memory at the same time as the gpu.

2) pread/pwrite access is tiling-correct, so userland does not need to tile
manually (mesa has already been fixed for this a while ago). The straw that
broke the camels back here was the bit17 swizzling stuff on some mobile gpus
meansing that while userland did the swizzle itself, the kernel had to do
*extra* swizzling, this was frankly retarded so the option was dropped.

3) there is no option to map via the cpu from userland, again due to
coherency issues.

4) additional integer overflow checking in some areas.

5) we only support the newer EXECBUFFER2 command submission ioctl. the
old one is not supported at all (libdrm was fixed WRT over a week ago).


now the TODOs:

1) the locking is funky and is only correct due to biglock. it does
however work due to that. This will be fixed in tree, the design in
formulating in my head as I type.

2) there are currently no memory limits on drm memory, this needs to be
changed.

3) we really need PAT support for the machines where MTRRs are useless, else drm
uses quite a lot of cpu (this is a bug that continues from the older code
though, nothing new).

4) gpu resetting support on other than 965 is not written yet.

5) currently a lot of the code is in inteldrm, when memory management
comes to other chipset common code will be factored out into the drm
midlayer.

Tested on: 855 (x40), GM965 and 915 by me. 945 by armani@@ and jkmeuser@@,
gm45 by armani@@ and marco@@. More testing is needed before I enable this
by default though.  Others also provided testing by what they tested
escapes me right now.

In order to test this enable INTELDRM_GEM in a kernel and add the following line
to the driver section in xorg.conf (I am working on a patch to autodetect the X
stuff):

Option "AccelMethod" "UXA"
@
text
@d134 2
a135 1
		    init->ring_size, 0, &dev_priv->ring.bsh)) != 0) {
d682 2
a683 2
	    dev_priv->hws_map.size, BUS_SPACE_MAP_LINEAR,
	    &dev_priv->hws_map.bsh)) != 0) {
@


1.50
log
@Add a dummy I915_PARAM_NUM_FENCES_AVAIL parameter for Mesa 7.5.
ok oga@@ during h2k9.
@
text
@d34 5
d91 1
a91 1
	if (dev_priv->ring.bsh != NULL) {
d94 1
a94 2
		dev_priv->ring.bsh = NULL;
		dev_priv->ring.size = 0;
d98 1
a98 1
	if (I915_NEED_GFX_HWS(dev_priv))
d118 1
a118 2
			((u8 *) dev_priv->sarea->handle +
			 init->sarea_priv_offset);
d124 8
a131 1
	dev_priv->ring.size = init->ring_size;
d133 6
a138 5
	if ((ret = bus_space_map(dev_priv->bst, init->ring_start,
	    init->ring_size, 0, &dev_priv->ring.bsh)) != 0) {
		DRM_INFO("can't map ringbuffer\n");
		i915_dma_cleanup(dev);
		return (ret);
d313 9
a321 3
static int
i915_emit_box(struct drm_device * dev, struct drm_clip_rect *boxes,
    int i, int DR1, int DR4)
a323 12
	struct drm_clip_rect box;

	if (DRM_COPY_FROM_USER(&box, &boxes[i], sizeof(box))) {
		return EFAULT;
	}

	if (box.y2 <= box.y1 || box.x2 <= box.x1 || box.y2 <= 0 ||
	    box.x2 <= 0) {
		DRM_ERROR("Bad box %d,%d..%d,%d\n",
		    box.x1, box.y1, box.x2, box.y2);
		return EINVAL;
	}
d328 2
a329 2
		OUT_RING((box.x1 & 0xffff) | (box.y1 << 16));
		OUT_RING(((box.x2 - 1) & 0xffff) | ((box.y2 - 1) << 16));
d336 2
a337 2
		OUT_RING((box.x1 & 0xffff) | (box.y1 << 16));
		OUT_RING(((box.x2 - 1) & 0xffff) | ((box.y2 - 1) << 16));
a341 2

	return 0;
d368 3
a370 2
static int i915_dispatch_cmdbuffer(struct drm_device * dev,
				   drm_i915_cmdbuffer_t * cmd)
d386 2
a387 6
		if (i < nbox) {
			ret = i915_emit_box(dev, cmd->cliprects, i,
					    cmd->DR1, cmd->DR4);
			if (ret)
				return ret;
		}
d398 3
a400 2
int i915_dispatch_batchbuffer(struct drm_device * dev,
			      drm_i915_batchbuffer_t * batch)
d402 2
a403 4
	drm_i915_private_t *dev_priv = dev->dev_private;
	struct drm_clip_rect __user *boxes = batch->cliprects;
	int nbox = batch->num_cliprects;
	int i = 0, count;
d412 1
d416 3
a418 6
		if (i < nbox) {
			int ret = i915_emit_box(dev, boxes, i,
						batch->DR1, batch->DR4);
			if (ret)
				return ret;
		}
d451 1
a451 1
	LOCK_TEST_WITH_RETURN(dev, file_priv);
d464 5
a468 3
	drm_i915_private_t *dev_priv = (drm_i915_private_t *) dev->dev_private;
	drm_i915_batchbuffer_t *batch = data;
	int ret;
d478 2
d481 1
a481 1
		return EINVAL;
d483 22
a504 1
	LOCK_TEST_WITH_RETURN(dev, file_priv);
d507 1
a507 1
	ret = i915_dispatch_batchbuffer(dev, batch);
d512 3
a514 1
	return ret;
d520 5
a524 3
	drm_i915_private_t *dev_priv = (drm_i915_private_t *)dev->dev_private;
	drm_i915_cmdbuffer_t *cmdbuf = data;
	int ret;
d529 2
d534 22
a555 1
	LOCK_TEST_WITH_RETURN(dev, file_priv);
d558 1
a558 1
	ret = i915_dispatch_cmdbuffer(dev, cmdbuf);
d560 2
a561 4
	if (ret) {
		DRM_ERROR("i915_dispatch_cmdbuffer failed\n");
		return ret;
	}
d563 3
a565 3
	if (dev_priv->sarea_priv != NULL)
		dev_priv->sarea_priv->last_dispatch = READ_BREADCRUMB(dev_priv);
	return 0;
d594 3
d598 1
d601 4
a604 1
		value = 0;
d607 1
a607 1
		DRM_ERROR("Unknown parameter %d\n", param->param);
d639 7
d647 1
a647 1
		DRM_ERROR("unknown parameter %d\n", param->param);
@


1.49
log
@some whitespace cleanups.

there's more in here that needs doing though.
@
text
@d549 3
@


1.48
log
@move the lastclose function into i915_drv.c where it belongs.

kill some more dead protos while i'm touching the lines around them
anyway.
@
text
@d62 2
a63 1
void i915_free_hws(drm_i915_private_t *dev_priv, bus_dma_tag_t dmat)
d81 2
a82 1
int i915_dma_cleanup(struct drm_device *dev)
a633 1

@


1.47
log
@Pad the ringbuffer with NOOPs before wrapping around, instead of
wrapping our commands over.

The documentation says that wrap must not happen in the middle of
commands. and upstream have seen some odd bugs that may be attributed to
this.

Based on a diff by Chris Wilson (ickle) from Intel to the linux driver.
@
text
@d80 1
a80 1
static int i915_dma_cleanup(struct drm_device * dev)
a632 11
void i915_driver_lastclose(struct drm_device * dev)
{
	drm_i915_private_t *dev_priv = dev->dev_private;

	if (dev_priv == NULL)
		return;

	dev_priv->sarea_priv = NULL;

	i915_dma_cleanup(dev);
}
@


1.46
log
@Turns out that the intel version of the heap interface isn't used
anywhere. And hasn't been for a long time.

The ddx sets up the heap (so just always pass that call and do nothing),
but nothing that touches inteldrm actually uses the other ioctls. So
just kill them and have one lese thing to worry about. Still got sis and
radeon using the common code though.
@
text
@a119 1
	dev_priv->ring.tail_mask = dev_priv->ring.size - 1;
@


1.45
log
@kill DRM_VERIFYAREA_READ, it's part of a linux optimisation where we can
guarrantee that copyin won't pagefault and use a faster version in that
case.

Since we just use plain copyin, it's stupid.
@
text
@d576 1
a576 1
		dev_priv->tex_lru_log_granularity = param->value;
a642 2
	i915_mem_takedown(&dev_priv->agp_heap);

a643 6
}

void i915_driver_close(struct drm_device * dev, struct drm_file *file_priv)
{
	drm_i915_private_t *dev_priv = dev->dev_private;
	i915_mem_release(dev, file_priv, &dev_priv->agp_heap);
@


1.44
log
@Change a bunch of "printk" in commented out debug code into "printf" and
kill the compat define. Should have done this ages ago.
@
text
@d277 1
a277 1
		if (DRM_COPY_FROM_USER_UNCHECKED(&cmd, &buffer[i], sizeof(cmd)))
d286 1
a286 2
			if (DRM_COPY_FROM_USER_UNCHECKED(&cmd, &buffer[i],
							 sizeof(cmd))) {
d301 3
a303 3
static int i915_emit_box(struct drm_device * dev,
			 struct drm_clip_rect __user * boxes,
			 int i, int DR1, int DR4)
d308 1
a308 1
	if (DRM_COPY_FROM_USER_UNCHECKED(&box, &boxes[i], sizeof(box))) {
d312 2
a313 1
	if (box.y2 <= box.y1 || box.x2 <= box.x1 || box.y2 <= 0 || box.x2 <= 0) {
d315 1
a315 1
			  box.x1, box.y1, box.x2, box.y2);
a482 4
	if (batch->num_cliprects && DRM_VERIFYAREA_READ(batch->cliprects,
	    batch->num_cliprects * sizeof(struct drm_clip_rect)))
		return EFAULT;

a505 8

	if (cmdbuf->num_cliprects &&
	    DRM_VERIFYAREA_READ(cmdbuf->cliprects,
				cmdbuf->num_cliprects *
				sizeof(struct drm_clip_rect))) {
		DRM_ERROR("Fault accessing cliprects\n");
		return EFAULT;
	}
@


1.43
log
@Demacro the inteldrm ring macros too, making them use bus_space_write
instead of assuming BUS_SPACE_LINEAR + bus_space_vaddr  while i'm at
it.

Cleans things up nicely, and shaves a little bit of space, too.
@
text
@d205 1
a205 1
static int do_validate_cmd(int cmd)
a260 9
}

static int validate_cmd(int cmd)
{
	int ret = do_validate_cmd(cmd);

/*	printk("validate_cmd( %x ): %d\n", cmd, ret); */

	return ret;
@


1.42
log
@The core drm code calls drm_irq_uninstall() when needed at lastclose.

Due to the fact that most of the drivers didn't keep their mmio regions
mapped from attach, all irq-using drm drivers have a hook in lastclose()
to remove the irq before it unmaps its registers. Since now they all
keep them mapped, this isn't an issue. Remove the redundant calls.
@
text
@a33 38
/* Really want an OS-independent resettable timer.  Would like to have
 * this loop run for (eg) 3 sec, but have the timer reset every time
 * the head pointer changes, so that EBUSY only happens if the ring
 * actually stalls for (eg) 3 seconds.
 */
int i915_wait_ring(struct drm_device * dev, int n, const char *caller)
{
	drm_i915_private_t *dev_priv = dev->dev_private;
	drm_i915_ring_buffer_t *ring = &(dev_priv->ring);
	u_int32_t acthd_reg = IS_I965G(dev_priv) ? ACTHD_I965 : ACTHD;
	u_int32_t last_acthd = I915_READ(acthd_reg);
	u_int32_t acthd;
	u_int32_t last_head = I915_READ(PRB0_HEAD) & HEAD_ADDR;
	int i;

	for (i = 0; i < 100000; i++) {
		ring->head = I915_READ(PRB0_HEAD) & HEAD_ADDR;
		acthd = I915_READ(acthd_reg);
		ring->space = ring->head - (ring->tail + 8);
		if (ring->space < 0)
			ring->space += ring->Size;
		if (ring->space >= n)
			return 0;

		if (ring->head != last_head)
			i = 0;
		if (acthd != last_acthd)
			i = 0;

		last_head = ring->head;
		last_acthd = acthd;
		tsleep(dev_priv, PZERO | PCATCH, "i915wt",
		    hz / 100);
	}

	return EBUSY;
}

d71 2
a72 1
		drm_core_ioremapfree(&dev_priv->hws_map);
a79 12
void i915_kernel_lost_context(struct drm_device * dev)
{
	drm_i915_private_t *dev_priv = dev->dev_private;
	drm_i915_ring_buffer_t *ring = &(dev_priv->ring);

	ring->head = I915_READ(PRB0_HEAD) & HEAD_ADDR;
	ring->tail = I915_READ(PRB0_TAIL) & TAIL_ADDR;
	ring->space = ring->head - (ring->tail + 8);
	if (ring->space < 0)
		ring->space += ring->Size;
}

d84 5
a88 5
	if (dev_priv->ring.virtual_start) {
		drm_core_ioremapfree(&dev_priv->ring.map);
		dev_priv->ring.virtual_start = NULL;
		dev_priv->ring.map.handle = NULL;
		dev_priv->ring.map.size = 0;
d100 2
a101 1
	drm_i915_private_t *dev_priv = dev->dev_private;
d119 2
a120 10
	dev_priv->ring.Size = init->ring_size;
	dev_priv->ring.tail_mask = dev_priv->ring.Size - 1;

	dev_priv->ring.map.offset = init->ring_start;
	dev_priv->ring.map.size = init->ring_size;
	dev_priv->ring.map.type = 0;
	dev_priv->ring.map.flags = 0;
	dev_priv->ring.map.mtrr = 0;

	drm_core_ioremap(&dev_priv->ring.map, dev);
d122 3
a124 1
	if (dev_priv->ring.map.handle == NULL) {
d126 1
a126 3
		DRM_ERROR("can not ioremap virtual address for"
			  " ring buffer\n");
		return ENOMEM;
a128 2
	dev_priv->ring.virtual_start = dev_priv->ring.map.handle;

d147 2
a148 3
	if (dev_priv->ring.map.handle == NULL) {
		DRM_ERROR("can not ioremap virtual address for"
			  " ring buffer\n");
a276 1
	RING_LOCALS;
d278 1
a278 1
	if ((dwords+1) * sizeof(int) >= dev_priv->ring.Size - 8)
a316 1
	RING_LOCALS;
a355 1
	RING_LOCALS;
d376 1
d385 1
a385 1
	i915_kernel_lost_context(dev);
a412 1
	RING_LOCALS;
d419 1
a419 1
	i915_kernel_lost_context(dev);
a455 8
int i915_quiescent(struct drm_device *dev)
{
	drm_i915_private_t *dev_priv = dev->dev_private;

	i915_kernel_lost_context(dev);
	return i915_wait_ring(dev, dev_priv->ring.Size - 8, __func__);
}

d459 2
a460 1
	int ret;
d465 2
a466 1
	ret = i915_quiescent(dev);
d613 3
a615 2
	drm_i915_private_t *dev_priv = dev->dev_private;
	drm_i915_hws_addr_t *hws = data;
d629 5
a634 3
	dev_priv->hws_map.type = 0;
	dev_priv->hws_map.flags = 0;
	dev_priv->hws_map.mtrr = 0;
d636 4
a639 2
	drm_core_ioremap(&dev_priv->hws_map, dev);
	if (dev_priv->hws_map.handle == NULL) {
d642 1
a642 3
		DRM_ERROR("can not ioremap virtual address for"
				" G33 hw status page\n");
		return ENOMEM;
d644 2
a645 1
	dev_priv->hw_status_page = dev_priv->hws_map.handle;
@


1.41
log
@for the sake of correctness bus_dmamap_sync when necessary for the
hardware-status-page.
@
text
@a131 6
	/* Make sure interrupts are disabled here because the uninstall ioctl
	 * may not have been called from userspace and after dev_private
	 * is freed, it's too late.
	 */
	if (dev->irq_enabled)
		drm_irq_uninstall(dev);
@


1.40
log
@Convert intel hardware-status-page allocation over to new dmamem api.
@
text
@d89 2
d226 3
a228 1
	else
d231 1
@


1.39
log
@Remove the inteldrm pageflipping code.

Deprecated and broken. Sanity checked by a few people, no problems
caused.
@
text
@d76 2
a77 1
int i915_init_phys_hws(drm_i915_private_t *dev_priv, bus_dma_tag_t dmat)
d80 4
a83 2
	dev_priv->status_page_dmah =
	    drm_pci_alloc(dmat, PAGE_SIZE, PAGE_SIZE, 0xffffffff);
d85 1
a85 6
	if (!dev_priv->status_page_dmah) {
		DRM_ERROR("Can not allocate hardware status page\n");
		return ENOMEM;
	}
	dev_priv->hw_status_page = dev_priv->status_page_dmah->vaddr;
	dev_priv->dma_status_page = dev_priv->status_page_dmah->busaddr;
d89 1
a89 1
	I915_WRITE(HWS_PGA, dev_priv->dma_status_page);
d91 1
a91 1
	return 0;
d100 3
a102 3
	if (dev_priv->status_page_dmah) {
		drm_pci_free(dmat, dev_priv->status_page_dmah);
		dev_priv->status_page_dmah = NULL;
d112 1
d225 2
a226 1
		I915_WRITE(HWS_PGA, dev_priv->dma_status_page);
@


1.38
log
@Merge the static block allocation code from {i915,radeon}_mem.c into
non-static code that's shared between both.  While i'm here convert them
to TAILQ.

Eventually, both of these will die, but until then I'd rather shave the
space in the kernel.

Tested on radeon and intel.
@
text
@a191 7
	dev_priv->cpp = init->cpp;
	dev_priv->back_offset = init->back_offset;
	dev_priv->front_offset = init->front_offset;
	dev_priv->current_page = 0;
	if (dev_priv->sarea_priv)
		dev_priv->sarea_priv->pf_current_page = 0;

a435 22

int i915_emit_mi_flush(struct drm_device *dev, uint32_t flush)
{
	drm_i915_private_t *dev_priv = dev->dev_private;
	uint32_t flush_cmd = MI_FLUSH;
	RING_LOCALS;

	flush_cmd |= flush;

	i915_kernel_lost_context(dev);

	BEGIN_LP_RING(4);
	OUT_RING(flush_cmd);
	OUT_RING(0);
	OUT_RING(0);
	OUT_RING(0);
	ADVANCE_LP_RING();

	return 0;
}


a518 37
int i915_dispatch_flip(struct drm_device * dev)
{
	drm_i915_private_t *dev_priv = dev->dev_private;
	RING_LOCALS;

	if (dev_priv->sarea_priv == NULL)
		return EINVAL;

	DRM_DEBUG("page=%d pfCurrentPage=%d\n", dev_priv->current_page,
	    dev_priv->sarea_priv->pf_current_page);

	i915_emit_mi_flush(dev, MI_READ_FLUSH | MI_EXE_FLUSH);

	BEGIN_LP_RING(6);
	OUT_RING(CMD_OP_DISPLAYBUFFER_INFO | ASYNC_FLIP);
	OUT_RING(0);
	if (dev_priv->current_page == 0) {
		OUT_RING(dev_priv->back_offset);
		dev_priv->current_page = 1;
	} else {
		OUT_RING(dev_priv->front_offset);
		dev_priv->current_page = 0;
	}
	OUT_RING(0);
	ADVANCE_LP_RING();

	BEGIN_LP_RING(2);
	OUT_RING(MI_WAIT_FOR_EVENT | MI_WAIT_FOR_PLANE_A_FLIP);
	OUT_RING(0);
	ADVANCE_LP_RING();

	i915_emit_breadcrumb(dev);

	dev_priv->sarea_priv->pf_current_page = dev_priv->current_page;
	return (0);
}

a608 16

int i915_flip_bufs(struct drm_device *dev, void *data, struct drm_file *file_priv)
{
	int ret;

	DRM_DEBUG("\n");

	LOCK_TEST_WITH_RETURN(dev, file_priv);

	DRM_LOCK();
	ret = i915_dispatch_flip(dev);
	DRM_UNLOCK();

	return (ret);
}

@


1.37
log
@do the same for struct drm_file (file_priv) as previously done for
dma_bufs; allow the driver to provide the size and not have the private
data pointer.  only radeon only needs this so far, but intel with GEM
also needs it.

Postclose was only used for freeing said private data, so this allows me
to remove the postclose callback and rename preclose to close.
@
text
@d811 1
a811 2
	if (dev_priv->agp_heap)
		i915_mem_takedown(&(dev_priv->agp_heap));
d819 1
a819 1
	i915_mem_release(dev, file_priv, dev_priv->agp_heap);
@


1.36
log
@make device works out if it's agp and tells the drm driver, not the
other way round. More dev->pa reduction
@
text
@d817 1
a817 1
void i915_driver_preclose(struct drm_device * dev, struct drm_file *file_priv)
@


1.35
log
@use dev->dmat.  Missed this one in an earlier diff.
@
text
@a821 17


/**
 * Determine if the device really is AGP or not.
 *
 * All Intel graphics chipsets are treated as AGP, even if they are really
 * PCI-e.
 *
 * \param dev   The device to be tested.
 *
 * \returns
 * A value of 1 is always retured to indictate every i9x5 is AGP.
 */
int i915_driver_device_is_agp(struct drm_device * dev)
{
	return 1;
}
@


1.34
log
@Include the GEM interface in i915_drm.h.

While we don't have code for this (yet!), mean I will be able to update
libdrm, and consequently the xf86-video-intel driver to 2.5.x.

Add PARAM_HAS_GEM to the getparam ioctl, and return no support.
@
text
@d147 1
a147 1
		i915_free_hws(dev_priv, dev->pa.pa_dmat);
@


1.33
log
@drm_device_is_pcie is only needed in one place: radeondrm_attach

so just inline it there.

also remove dev->pci_vendor and dev->pci_device, and insert pci_device
into the one place any of them are needed (inteldrm's interface can give
this info to the X driver. to remove that you'd need to fix X too).
@
text
@d717 3
@


1.32
log
@Remove the driver->load callback and just do all the initialization in
the attach function. First step towards splitting drm off as it's own
(bus independant) device, as it should be.
@
text
@d715 1
a715 1
		value = dev->pci_device;
@


1.31
log
@reduce the dependancy of drm_pci_alloc upon the drm device softc. Just
pass in the dma tag
@
text
@a33 3
int	i915_init_phys_hws(drm_i915_private_t *, bus_dma_tag_t);
void	i915_free_hws(drm_i915_private_t *, bus_dma_tag_t);

a795 56
	return 0;
}

int i915_driver_load(struct drm_device *dev, unsigned long flags)
{
	struct drm_i915_private *dev_priv;
	struct vga_pci_bar	*bar;
	int			 ret;

	dev_priv = drm_calloc(1, sizeof(drm_i915_private_t), DRM_MEM_DRIVER);
	if (dev_priv == NULL)
		return ENOMEM;

	dev->dev_private = (void *)dev_priv;

	dev_priv->flags = flags;

	/* Add register map (needed for suspend/resume) */
	bar = vga_pci_bar_info(dev->vga_softc, (IS_I9XX(dev_priv) ? 0 : 1));
	if (bar == NULL) {
		printf(": can't get BAR info\n");
		return (EINVAL);
	}

	dev_priv->regs = vga_pci_bar_map(dev->vga_softc,
	    bar->addr, bar->size, 0);
	if (dev_priv->regs == NULL) {
		printf(": can't map mmio space\n");
		return (ENOMEM);
	}

	/* Init HWS */
	if (!I915_NEED_GFX_HWS(dev_priv)) {
		ret = i915_init_phys_hws(dev_priv, dev->pa.pa_dmat);
		if (ret != 0)
			return ret;
	}

	mtx_init(&dev_priv->user_irq_lock, IPL_BIO);

	return ret;
}

int i915_driver_unload(struct drm_device *dev)
{
	struct drm_i915_private *dev_priv = dev->dev_private;

	i915_free_hws(dev_priv, dev->pa.pa_dmat);

	if (dev_priv->regs != NULL)
		vga_pci_bar_unmap(dev_priv->regs);

	DRM_SPINUNINIT(&dev_priv->user_irq_lock);

	drm_free(dev->dev_private, sizeof(drm_i915_private_t), DRM_MEM_DRIVER);

@


1.30
log
@Slightly fix up previous
@
text
@d34 2
a35 2
int	i915_init_phys_hws(struct drm_device *);
void	i915_free_hws(struct drm_device *);
d79 1
a79 1
int i915_init_phys_hws(struct drm_device *dev)
a80 1
	drm_i915_private_t *dev_priv = dev->dev_private;
d83 1
a83 1
	    drm_pci_alloc(dev, PAGE_SIZE, PAGE_SIZE, 0xffffffff);
d103 1
a103 1
void i915_free_hws(struct drm_device *dev)
a104 1
	drm_i915_private_t *dev_priv = dev->dev_private;
d106 1
a106 1
		drm_pci_free(dev, dev_priv->status_page_dmah);
d112 1
a112 1
		drm_core_ioremapfree(&dev_priv->hws_map, dev);
d142 1
a142 1
		drm_core_ioremapfree(&dev_priv->ring.map, dev);
d150 1
a150 1
		i915_free_hws(dev);
d832 1
a832 1
		ret = i915_init_phys_hws(dev);
d846 1
a846 1
	i915_free_hws(dev);
@


1.29
log
@inteldrm currently checks the pcidev of the device every time it needs to check
what revision it is to determine which route to take.

instead, use the flags field of the pcidevs array to provide a static
list of flags related to series, type, and certain features and check those
instead. Makes me less sad and knocks another 600 bytes off my kernel.
@
text
@d46 1
a46 1
	u_int32_t acthd_reg = IS_I965G(dev) ? ACTHD_I965 : ACTHD;
d151 1
a151 1
	if (I915_NEED_GFX_HWS(dev))
d402 1
a402 1
	if (IS_I965G(dev)) {
d528 1
a528 1
		if (!IS_I830(dev) && !IS_845G(dev)) {
d530 1
a530 1
			if (IS_I965G(dev)) {
d769 1
a769 1
	if (!I915_NEED_GFX_HWS(dev))
d819 1
a819 1
	bar = vga_pci_bar_info(dev->vga_softc, (IS_I9XX(dev) ? 0 : 1));
d833 1
a833 1
	if (!I915_NEED_GFX_HWS(dev)) {
@


1.28
log
@I'm clever and commited the wrong patch. Here's the correct one.
This should close kernel/5995.
@
text
@d816 2
@


1.27
log
@instead of going through the drm_map interface, just map the mmio
registers directly (via the memory sharing interface that intagp uses).
It doesn't need to be in a map structure.

Idea taken from some of intel's work.
@
text
@d807 2
a808 2
	unsigned long base, size;
	int ret = 0, mmio_bar = IS_I9XX(dev) ? 0 : 1;
d817 5
a821 2
	base = drm_get_resource_start(dev, mmio_bar);
	size = drm_get_resource_len(dev, mmio_bar);
d823 4
a826 2
	dev_priv->regs = vga_pci_bar_map(dev->vga_softc, base, size, 0);
	if (dev_priv->regs == NULL)
d828 1
@


1.26
log
@Don't put the buffer counter in a reserved part of the status page.

From intel.
@
text
@d820 3
a822 2
	ret = drm_addmap(dev, base, size, _DRM_REGISTERS,
		_DRM_KERNEL | _DRM_DRIVER, &dev_priv->mmio_map);
d842 2
a843 2
	if (dev_priv->mmio_map)
		drm_rmmap(dev, dev_priv->mmio_map);
@


1.25
log
@Conditionalise the use of the SAREA in inteldrm. In DRI2 setups (which we don't
support yet, but will) it won't exist, prepare for this by only writing to it if
it's there.

Bits of this came from Eric Anholt at intel.
@
text
@d442 1
a442 1
	OUT_RING(5 << MI_STORE_DWORD_INDEX_SHIFT);
@


1.24
log
@The i915 vblank swap ioctl is fundamentally racy.

using it allowed rendering to continue while waiting for a vblank swap,
and often this lead to flickering and rendering a new scene before the
swap. this broke a lot of things.

With the removal of this swap, userland falls back to the old way of
waiting for the vblank then doing the swap itself, this is smooth
enough.

I decided independantly to kill this, but the intel guys recently
concurred.  Comment change comes from Eric Anholt at intel.
@
text
@d553 1
a553 1
void i915_dispatch_flip(struct drm_device * dev)
d558 3
d587 1
a615 2
	drm_i915_sarea_t *sarea_priv = (drm_i915_sarea_t *)
	    dev_priv->sarea_priv;
d633 1
a633 2
							batch->num_cliprects *
							sizeof(struct drm_clip_rect)))
d640 2
a641 1
	sarea_priv->last_dispatch = READ_BREADCRUMB(dev_priv);
a648 1
	drm_i915_sarea_t *sarea_priv = (drm_i915_sarea_t *)dev_priv->sarea_priv;
d676 2
a677 1
	sarea_priv->last_dispatch = READ_BREADCRUMB(dev_priv);
d683 2
d690 1
a690 1
	i915_dispatch_flip(dev);
d693 1
a693 1
	return 0;
d855 1
a855 2
	/* agp off can use this to get called before dev_priv */
	if (!dev_priv)
@


1.23
log
@Enable IMR passthrough of vblank events before enabling it in the
pipestat register. Fixes a nasty race where the bit would get set
without being reflected in the interrupt register, so we'd never get
another vblank interrupt.

Also, use the user_irq_lock to also protect vblank register writes, since it
covers the same register.

From Eric Anholt and  Keith Packard at Intel.
@
text
@a825 1
	mtx_init(&dev_priv->swaps_lock, IPL_NONE);
a839 1
	DRM_SPINUNINIT(&dev_priv->swaps_lock);
@


1.22
log
@Hold the drm lock around all things that touch the ringbuffer.

Not strictly needed in the non-gem case, but it will be needed then, and
doesn't hurt now.

From Eric Anholt at intel.
@
text
@d827 1
a827 1
	DRM_SPININIT(&dev_priv->user_irq_lock, "I915 irq lock");
@


1.21
log
@revert the pageflipping and vblank sync code to the older style that
doesn't handle triple buffering (which has been marked ``don't use this,
it's unstable'' for ever anyway)

While the code just removed is in drm git. it's not planned to go any
further, due to being a horribly ugly hack. Instead a proper fix which
will depend on memory management is planned. So revert this stuff here,
since it's now dead.

Testing shows no regressions.
@
text
@d597 1
d601 5
a605 1
	return i915_quiescent(dev);
d635 1
d637 1
d667 1
d669 1
d685 1
a685 2


d687 1
@


1.20
log
@Remove the vblank_pipe member of the softc and noop the ioctl that sets
it.

Since the vblank handling got reworked this is very much unneeded.
@
text
@d198 3
a200 1

d553 1
a553 1
static void i915_do_dispatch_flip(struct drm_device * dev, int plane, int sync)
a555 2
	u32 num_pages, current_page, next_page, dspbase;
	int shift = 2 * plane, x, y;
d558 2
a559 4
	/* Calculate display base offset */
	num_pages = dev_priv->sarea_priv->third_handle ? 3 : 2;
	current_page = (dev_priv->sarea_priv->pf_current_page >> shift) & 0x3;
	next_page = (current_page + 1) % num_pages;
d561 1
a561 12
	switch (next_page) {
	default:
	case 0:
		dspbase = dev_priv->sarea_priv->front_offset;
		break;
	case 1:
		dspbase = dev_priv->sarea_priv->back_offset;
		break;
	case 2:
		dspbase = dev_priv->sarea_priv->third_offset;
		break;
	}
d563 6
a568 3
	if (plane == 0) {
		x = dev_priv->sarea_priv->planeA_x;
		y = dev_priv->sarea_priv->planeA_y;
d570 2
a571 2
		x = dev_priv->sarea_priv->planeB_x;
		y = dev_priv->sarea_priv->planeB_y;
d573 2
d576 3
a578 13
	dspbase += (y * dev_priv->sarea_priv->pitch + x) * dev_priv->cpp;

	DRM_DEBUG("plane=%d current_page=%d dspbase=0x%x\n", plane, current_page,
		  dspbase);

	BEGIN_LP_RING(4);
	OUT_RING(sync ? 0 :
		 (MI_WAIT_FOR_EVENT | (plane ? MI_WAIT_FOR_PLANE_B_FLIP :
				       MI_WAIT_FOR_PLANE_A_FLIP)));
	OUT_RING(CMD_OP_DISPLAYBUFFER_INFO | (sync ? 0 : ASYNC_FLIP) |
		 (plane ? DISPLAY_PLANE_B : DISPLAY_PLANE_A));
	OUT_RING(dev_priv->sarea_priv->pitch * dev_priv->cpp);
	OUT_RING(dspbase);
d581 1
a581 11
	dev_priv->sarea_priv->pf_current_page &= ~(0x3 << shift);
	dev_priv->sarea_priv->pf_current_page |= next_page << shift;
}

void i915_dispatch_flip(struct drm_device * dev, int planes, int sync)
{
	drm_i915_private_t *dev_priv = dev->dev_private;
	int i;

	DRM_DEBUG("planes=0x%x pfCurrentPage=%d\n",
		  planes, dev_priv->sarea_priv->pf_current_page);
d583 1
a583 7
	i915_emit_mi_flush(dev, MI_READ_FLUSH | MI_EXE_FLUSH);

	for (i = 0; i < 2; i++)
		if (planes & (1 << i))
			i915_do_dispatch_flip(dev, i, sync);

	i915_emit_breadcrumb(dev);
a669 22
static int i915_do_cleanup_pageflip(struct drm_device * dev)
{
	drm_i915_private_t *dev_priv = dev->dev_private;
	int i, planes, num_pages = dev_priv->sarea_priv->third_handle ? 3 : 2;

	DRM_DEBUG("\n");

	for (i = 0, planes = 0; i < 2; i++)
		if (dev_priv->sarea_priv->pf_current_page & (0x3 << (2 * i))) {
			dev_priv->sarea_priv->pf_current_page =
				(dev_priv->sarea_priv->pf_current_page &
				 ~(0x3 << (2 * i))) | ((num_pages - 1) << (2 * i));

			planes |= 1 << i;
		}

	if (planes)
		i915_dispatch_flip(dev, planes, 0);

	return 0;
}

a671 2
	drm_i915_flip_t *param = data;

a675 6
	/* This is really planes */
	if (param->pipes & ~0x3) {
		DRM_ERROR("Invalid planes 0x%x, only <= 0x3 is valid\n",
			  param->pipes);
		return EINVAL;
	}
d677 2
a678 1
	i915_dispatch_flip(dev, param->pipes, 0);
d817 1
a817 1
	mtx_init(&dev_priv->swaps_lock, IPL_BIO);
a847 2
	if (drm_getsarea(dev) && dev_priv->sarea_priv)
		i915_do_cleanup_pageflip(dev);
@


1.19
log
@Remove another interface (this one for ARB_Occlusion_Query) that we've
never used (mesa did a version check that never passed on our kernel).
The correct way to do this is in mesa master and requires memory
management.
@
text
@a205 4
	/* Enable vblank on pipe A for older X servers
	 */
	dev_priv->vblank_pipe = DRM_I915_VBLANK_PIPE_A;

@


1.18
log
@Bunch of cleanup. Kill some dead variables and some stupid code.

Some ideas taken from upstream.
@
text
@a816 57
drm_i915_mmio_entry_t mmio_table[] = {
	[MMIO_REGS_PS_DEPTH_COUNT] = {
		I915_MMIO_MAY_READ|I915_MMIO_MAY_WRITE,
		0x2350,
		8
	}
};

static int mmio_table_size = sizeof(mmio_table)/sizeof(drm_i915_mmio_entry_t);

int i915_mmio(struct drm_device *dev, void *data,
		     struct drm_file *file_priv)
{
	uint32_t buf[8];
	drm_i915_private_t *dev_priv = dev->dev_private;
	drm_i915_mmio_entry_t *e;
	drm_i915_mmio_t *mmio = data;
	void __iomem *base;
	int i;

	if (!dev_priv) {
		DRM_ERROR("called with no initialization\n");
		return EINVAL;
	}

	if (mmio->reg >= mmio_table_size)
		return EINVAL;

	e = &mmio_table[mmio->reg];
	base = (u8 *) dev_priv->mmio_map->handle + e->offset;

	switch (mmio->read_write) {
	case I915_MMIO_READ:
		if (!(e->flag & I915_MMIO_MAY_READ))
			return EINVAL;
		for (i = 0; i < e->size / 4; i++)
			buf[i] = I915_READ(e->offset + i * 4);
		if (DRM_COPY_TO_USER(mmio->data, buf, e->size)) {
			DRM_ERROR("DRM_COPY_TO_USER failed\n");
			return EFAULT;
		}
		break;
		
	case I915_MMIO_WRITE:
		if (!(e->flag & I915_MMIO_MAY_WRITE))
			return EINVAL;
		if (DRM_COPY_FROM_USER(buf, mmio->data, e->size)) {
			DRM_ERROR("DRM_COPY_TO_USER failed\n");
			return EFAULT;
		}
		for (i = 0; i < e->size / 4; i++)
			I915_WRITE(e->offset + i * 4, buf[i]);
		break;
	}
	return 0;
}

@


1.17
log
@Kill the linux-ready negative return codes in ``shared'' code. We handle
them wrong in several cases that i've noticed and Merging when needed is
still fairly simple, anyway. This shaves another 500 bytes from an amd64
kernel due to not having to flip the sign on some things. It also stops
my eyes bleeding.

Tested by a few along with the last diff that went in.
@
text
@d145 2
a146 2
		dev_priv->ring.virtual_start = 0;
		dev_priv->ring.map.handle = 0;
d150 1
a150 1
	/* Clear the HWS virtual address as teardown */
d157 1
a157 3
static int i915_initialize(struct drm_device * dev,
			   struct drm_file *file_priv,
			   drm_i915_init_t * init)
a176 2
	dev_priv->ring.Start = init->ring_start;
	dev_priv->ring.End = init->ring_end;
d238 1
a238 1
		I915_WRITE(0x02080, dev_priv->status_gfx_addr);
d240 1
a240 1
		I915_WRITE(0x02080, dev_priv->dma_status_page);
d254 1
a254 2
	case I915_INIT_DMA2:
		retcode = i915_initialize(dev, file_priv, init);
d444 1
a444 1
	OUT_RING(20);
a539 1

d551 1
d629 1
a629 1
	return i915_wait_ring(dev, dev_priv->ring.Size - 8, __FUNCTION__);
d677 2
a678 3
	drm_i915_private_t *dev_priv = (drm_i915_private_t *) dev->dev_private;
	drm_i915_sarea_t *sarea_priv = (drm_i915_sarea_t *)
	    dev_priv->sarea_priv;
a707 6
#if defined(DRM_DEBUG_CODE)
#define DRM_DEBUG_RELOCATION	(drm_debug != 0)
#else
#define DRM_DEBUG_RELOCATION	0
#endif

d909 1
a909 1
	DRM_DEBUG("load hws 0x2080 with gfx mem 0x%x\n",
a943 7
#ifdef __linux__
#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,25)
	intel_init_chipset_flush_compat(dev);
#endif
	intel_opregion_init(dev);
#endif

d959 2
a960 11
#ifdef __linux__
	intel_opregion_free(dev);
#endif

	drm_free(dev->dev_private, sizeof(drm_i915_private_t),
		 DRM_MEM_DRIVER);
#ifdef __linux__
#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,25)
	intel_fini_chipset_flush_compat(dev);
#endif
#endif
d978 1
a1002 5
}

int i915_driver_firstopen(struct drm_device *dev)
{
	return 0;
@


1.16
log
@Move dev->driver over to being a pointer to a const struct, instead of stupidly
filling in a pre-allocated one on each attach.

Makes the code a bunch nicer, shrinks a kernel by about 1.5k on amd64,
helps with my sanity, and paves way for later changes.

Tested by a few for a couple of weeks now.
@
text
@d72 1
a72 1
	return -EBUSY;
d88 1
a88 1
		return -ENOMEM;
d167 1
a167 1
		return -EINVAL;
d196 1
a196 1
		return -ENOMEM;
d225 1
a225 1
		return -EINVAL;
d231 1
a231 1
		return -ENOMEM;
d237 1
a237 1
		return -EINVAL;
d268 1
a268 1
		retcode = -EINVAL;
d359 1
a359 1
		return -EINVAL;
d367 1
a367 1
			return -EINVAL;
d370 1
a370 1
			return -EINVAL;
d377 1
a377 1
				return -EINVAL;
d400 1
a400 1
		return -EFAULT;
d406 1
a406 1
		return -EINVAL;
d485 1
a485 1
		return -EINVAL;
d520 1
a520 1
		return -EINVAL;
d657 1
a657 1
		return -EINVAL;
d664 1
a664 1
		return -EINVAL;
d671 1
a671 1
		return -EFAULT;
d692 1
a692 1
		return -EINVAL;
d701 1
a701 1
		return -EFAULT;
d754 1
a754 1
		return -EINVAL;
d772 1
a772 1
		return -EINVAL;
d790 1
a790 1
		return -EINVAL;
d795 1
a795 1
		return -EFAULT;
d809 1
a809 1
		return -EINVAL;
d823 1
a823 1
		return -EINVAL;
d851 1
a851 1
		return -EINVAL;
d855 1
a855 1
		return -EINVAL;
d863 1
a863 1
			return -EINVAL;
d868 1
a868 1
			return -EFAULT;
d874 1
a874 1
			return -EINVAL;
d877 1
a877 1
			return -EFAULT;
d893 1
a893 1
		return -EINVAL;
d897 1
a897 1
		return -EINVAL;
d915 1
a915 1
		return -ENOMEM;
d935 1
a935 1
		return -ENOMEM;
@


1.15
log
@Rework the drm locking to be at least halfway sane. The freebsd code
held a lock over all driver ioctls in order to be ``mpsafe''. Stop lying
to ourselves for a start. This code is not fully mpsafe, and should not
pretend to be so.  Put the locking around where it should, and rely on
biglock for the rest. This will need to be fixed, but avoids some of the
horrible that we have right now.

Tested by many over a long time and several iterations.
@
text
@d250 1
a250 1
static int i915_dma_init(struct drm_device *dev, void *data,
d637 1
a637 1
static int i915_flush_ioctl(struct drm_device *dev, void *data,
d646 1
a646 1
static int i915_batchbuffer(struct drm_device *dev, void *data,
d679 1
a679 1
static int i915_cmdbuffer(struct drm_device *dev, void *data,
d742 1
a742 1
static int i915_flip_bufs(struct drm_device *dev, void *data, struct drm_file *file_priv)
d763 1
a763 1
static int i915_getparam(struct drm_device *dev, void *data,
d801 1
a801 1
static int i915_setparam(struct drm_device *dev, void *data,
d839 1
a839 1
static int i915_mmio(struct drm_device *dev, void *data,
d886 1
a886 1
static int i915_set_status_page(struct drm_device *dev, void *data,
a1014 22
struct drm_ioctl_desc i915_ioctls[] = {
	DRM_IOCTL_DEF(DRM_I915_INIT, i915_dma_init, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
	DRM_IOCTL_DEF(DRM_I915_FLUSH, i915_flush_ioctl, DRM_AUTH),
	DRM_IOCTL_DEF(DRM_I915_FLIP, i915_flip_bufs, DRM_AUTH),
	DRM_IOCTL_DEF(DRM_I915_BATCHBUFFER, i915_batchbuffer, DRM_AUTH),
	DRM_IOCTL_DEF(DRM_I915_IRQ_EMIT, i915_irq_emit, DRM_AUTH),
	DRM_IOCTL_DEF(DRM_I915_IRQ_WAIT, i915_irq_wait, DRM_AUTH),
	DRM_IOCTL_DEF(DRM_I915_GETPARAM, i915_getparam, DRM_AUTH),
	DRM_IOCTL_DEF(DRM_I915_SETPARAM, i915_setparam, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
	DRM_IOCTL_DEF(DRM_I915_ALLOC, i915_mem_alloc, DRM_AUTH),
	DRM_IOCTL_DEF(DRM_I915_FREE, i915_mem_free, DRM_AUTH),
	DRM_IOCTL_DEF(DRM_I915_INIT_HEAP, i915_mem_init_heap, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
	DRM_IOCTL_DEF(DRM_I915_CMDBUFFER, i915_cmdbuffer, DRM_AUTH),
	DRM_IOCTL_DEF(DRM_I915_DESTROY_HEAP,  i915_mem_destroy_heap, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY ),
	DRM_IOCTL_DEF(DRM_I915_SET_VBLANK_PIPE,  i915_vblank_pipe_set, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY ),
	DRM_IOCTL_DEF(DRM_I915_GET_VBLANK_PIPE,  i915_vblank_pipe_get, DRM_AUTH ),
	DRM_IOCTL_DEF(DRM_I915_VBLANK_SWAP, i915_vblank_swap, DRM_AUTH),
	DRM_IOCTL_DEF(DRM_I915_MMIO, i915_mmio, DRM_AUTH),
	DRM_IOCTL_DEF(DRM_I915_HWS_ADDR, i915_set_status_page, DRM_AUTH),
};

int i915_max_ioctl = DRM_ARRAY_SIZE(i915_ioctls);
@


1.14
log
@Kill the stats data structures and noop some other parts. Nothing in
userland asks for these stats, and we stopped recording anything
interesting a while back.
@
text
@d68 1
a68 1
		msleep(dev_priv, &dev->dev_lock, PZERO | PCATCH, "i915wt",
@


1.13
log
@If we need a physical hardware status page, initialise it at device attach
instead of putting up and tearing down on open and close, some chips got
unstable with it being done repeatedly.

From drm git. Tested by several.
@
text
@a932 7
	/* i915 has 4 more counters */
	dev->counters += 4;
	dev->types[6] = _DRM_STAT_IRQ;
	dev->types[7] = _DRM_STAT_PRIMARY;
	dev->types[8] = _DRM_STAT_SECONDARY;
	dev->types[9] = _DRM_STAT_DMA;

@


1.12
log
@Track progress inside of batchbuffers so we know the hardware isn't
wedged.

This avoids early temination of long-running commands.

From Keith Packard, via drm git. Tested by several on various chipsets.
@
text
@d34 3
d75 46
d150 3
a152 12
	if (dev_priv->status_page_dmah) {
		drm_pci_free(dev, dev_priv->status_page_dmah);
		dev_priv->status_page_dmah = NULL;
		/* Need to rewrite hardware status page */
		I915_WRITE(0x02080, 0x1ffff000);
	}

	if (dev_priv->status_gfx_addr) {
		dev_priv->status_gfx_addr = 0;
		drm_core_ioremapfree(&dev_priv->hws_map, dev);
		I915_WRITE(0x02080, 0x1ffff000);
	}
a213 19
	/* Program Hardware Status Page */
	if (!I915_NEED_GFX_HWS(dev)) {
		dev_priv->status_page_dmah =
			drm_pci_alloc(dev, PAGE_SIZE, PAGE_SIZE, 0xffffffff);

		if (!dev_priv->status_page_dmah) {
			i915_dma_cleanup(dev);
			DRM_ERROR("Can not allocate hardware status page\n");
			return -ENOMEM;
		}
		dev_priv->hw_status_page = dev_priv->status_page_dmah->vaddr;
		dev_priv->dma_status_page = dev_priv->status_page_dmah->busaddr;

		memset(dev_priv->hw_status_page, 0, PAGE_SIZE);

		I915_WRITE(0x02080, dev_priv->dma_status_page);
	}
	DRM_DEBUG("Enabled hardware status page\n");

d953 7
d976 2
@


1.11
log
@Add Interrupt mitigation for the i9XX user_irq, should save a bit of power;
from drm git.
@
text
@d43 4
a46 1
	u32 last_head = I915_READ(PRB0_HEAD) & HEAD_ADDR;
d49 1
a49 1
	for (i = 0; i < 10000; i++) {
d51 1
d60 2
d64 3
a66 1
		DRM_UDELAY(1);
@


1.10
log
@Ignore xserver provided mmio address. We already have it. From git.
@
text
@d83 1
a83 1
	if (dev->irq)
d748 1
a748 1
		value = dev->irq ? 1 : 0;
d924 3
d943 3
@


1.9
log
@Ignore userland provided batchbuffer start parameter, we can determine
it ourselves.

From drm git.
@
text
@a121 8
	if (init->mmio_offset != 0)
		dev_priv->mmio_map = drm_core_findmap(dev, init->mmio_offset);
	if (!dev_priv->mmio_map) {
		i915_dma_cleanup(dev);
		DRM_ERROR("can not find mmio map!\n");
		return -EINVAL;
	}

a195 5
		return -EINVAL;
	}

	if (!dev_priv->mmio_map) {
		DRM_ERROR("can not find mmio map!\n");
@


1.8
log
@set sarea_priv to NULL to prevent possible access on next open/close.

From Robert Noland via drm git.
@
text
@a165 7
	/* We are using separate values as placeholders for mechanisms for
	 * private backbuffer/depthbuffer usage.
	 */
	dev_priv->use_mi_batchbuffer_start = 0;
	if (IS_I965G(dev)) /* 965 doesn't support older method */
		dev_priv->use_mi_batchbuffer_start = 1;

d519 1
a519 1
		if (dev_priv->use_mi_batchbuffer_start) {
a797 2
		if (!IS_I965G(dev))
			dev_priv->use_mi_batchbuffer_start = param->value;
@


1.7
log
@Kill the ifdefed out ttm interface stuff. The intel driver has
definitively gone for GEM so this will not be needed.

No binary change.
@
text
@d987 2
@


1.6
log
@Update to DRM git.

Some stability fixes for radeon. The most part of this diff is related
to fixing up the VBLANK (vertical blank interrupt) handling. Now, if the
X driver supports the DRM_IOCTL_MODESET_CTL ioctl, (to be used when
changing the video modes), then allow the vblank to be disabled once
that ioctl has been called. Otherwise, keep the interrupt enabled at all
time, since disabling it otherwise will lead to problems.

Tested by a few. "no problem" on API/ABI deraadt@@.
@
text
@a108 61
#if defined(I915_HAVE_BUFFER)
#define DRI2_SAREA_BLOCK_TYPE(b) ((b) >> 16)
#define DRI2_SAREA_BLOCK_SIZE(b) ((b) & 0xffff)
#define DRI2_SAREA_BLOCK_NEXT(p)				\
	((void *) ((unsigned char *) (p) +			\
		   DRI2_SAREA_BLOCK_SIZE(*(unsigned int *) p)))

#define DRI2_SAREA_BLOCK_END		0x0000
#define DRI2_SAREA_BLOCK_LOCK		0x0001
#define DRI2_SAREA_BLOCK_EVENT_BUFFER	0x0002

static int
setup_dri2_sarea(struct drm_device * dev,
		 struct drm_file *file_priv,
		 drm_i915_init_t * init)
{
	drm_i915_private_t *dev_priv = dev->dev_private;
	int ret;
	unsigned int *p, *end, *next;

	mutex_lock(&dev->struct_mutex);
	dev_priv->sarea_bo =
		drm_lookup_buffer_object(file_priv,
					 init->sarea_handle, 1);
	mutex_unlock(&dev->struct_mutex);

	if (!dev_priv->sarea_bo) {
		DRM_ERROR("did not find sarea bo\n");
		return -EINVAL;
	}

	ret = drm_bo_kmap(dev_priv->sarea_bo, 0,
			  dev_priv->sarea_bo->num_pages,
			  &dev_priv->sarea_kmap);
	if (ret) {
		DRM_ERROR("could not map sarea bo\n");
		return ret;
	}

	p = dev_priv->sarea_kmap.virtual;
	end = (void *) p + (dev_priv->sarea_bo->num_pages << PAGE_SHIFT);
	while (p < end && DRI2_SAREA_BLOCK_TYPE(*p) != DRI2_SAREA_BLOCK_END) {
		switch (DRI2_SAREA_BLOCK_TYPE(*p)) {
		case DRI2_SAREA_BLOCK_LOCK:
			dev->lock.hw_lock = (void *) (p + 1);
			dev->sigdata.lock = dev->lock.hw_lock;
			break;
		}
		next = DRI2_SAREA_BLOCK_NEXT(p);
		if (next <= p || end < next) {
			DRM_ERROR("malformed dri2 sarea: next is %p should be within %p-%p\n",
				  next, p, end);
			return -EINVAL;
		}
		p = next;
	}

	return 0;
}
#endif

d114 1
a114 3
#if defined(I915_HAVE_BUFFER)
	int ret;
#endif
a129 4
#ifdef I915_HAVE_BUFFER
	dev_priv->max_validate_buffers = I915_MAX_VALIDATE_BUFFERS;
#endif

a198 13
#ifdef I915_HAVE_BUFFER
	mutex_init(&dev_priv->cmdbuf_mutex);
#endif
#if defined(I915_HAVE_BUFFER)
	if (init->func == I915_INIT_DMA2) {
		ret = setup_dri2_sarea(dev, file_priv, init);
		if (ret) {
			i915_dma_cleanup(dev);
			DRM_ERROR("could not set up dri2 sarea\n");
			return ret;
		}
	}
#endif
a470 3
#ifdef I915_HAVE_FENCE
	drm_i915_private_t *dev_priv = dev->dev_private;
#endif
a496 4
#ifdef I915_HAVE_FENCE
	if (unlikely((dev_priv->counter & 0xFF) == 0))
		drm_fence_flush_old(dev, 0, dev_priv->counter);
#endif
a547 4
#ifdef I915_HAVE_FENCE
	if (unlikely((dev_priv->counter & 0xFF) == 0))
		drm_fence_flush_old(dev, 0, dev_priv->counter);
#endif
a617 4
#ifdef I915_HAVE_FENCE
	if (unlikely(!sync && ((dev_priv->counter & 0xFF) == 0)))
		drm_fence_flush_old(dev, 0, dev_priv->counter);
#endif
a984 7
#ifdef I915_HAVE_BUFFER
	if (dev_priv->val_bufs) {
		vfree(dev_priv->val_bufs);
		dev_priv->val_bufs = NULL;
	}
#endif

a988 15
#if defined(I915_HAVE_BUFFER)
	if (dev_priv->sarea_kmap.virtual) {
		drm_bo_kunmap(&dev_priv->sarea_kmap);
		dev_priv->sarea_kmap.virtual = NULL;
		dev->lock.hw_lock = NULL;
		dev->sigdata.lock = NULL;
	}

	if (dev_priv->sarea_bo) {
		mutex_lock(&dev->struct_mutex);
		drm_bo_usage_deref_locked(&dev_priv->sarea_bo);
		mutex_unlock(&dev->struct_mutex);
		dev_priv->sarea_bo = NULL;
	}
#endif
a1016 3
#ifdef I915_HAVE_BUFFER
	DRM_IOCTL_DEF(DRM_I915_EXECBUFFER, i915_execbuffer, DRM_AUTH),
#endif
a1038 3
#ifdef I915_HAVE_BUFFER
	drm_bo_driver_init(dev);
#endif
@


1.5
log
@Switch all instances of malloc/free in the DRM to drm_alloc, drm_free
and drm_calloc.

With the recent change to check overflow in drm_calloc, this means that
all allocations that require multiplication are now checked. Also use
drm_calloc() when zeroing is needed and drop the bzero/memset
afterwards.  Finally, make drm_free() check for NULL, so we don't need
to do so every time.

ok miod@@, deraadt@@
@
text
@d1045 1
d1057 4
@


1.4
log
@Stricter bounds checking for values controlling loops or memory allocations,
which may come from userland via ioctls. ok oga@@
@
text
@d1028 1
a1028 1
	dev_priv = drm_alloc(sizeof(drm_i915_private_t), DRM_MEM_DRIVER);
a1030 2

	memset(dev_priv, 0, sizeof(drm_i915_private_t));
@


1.3
log
@Update the inteldrm driver to drm git.

changes:
- Support for intel 4 series chipsets (i'll do any relavent agp bits for
these as soon as i grab the datasheet and find a testcase)
- fix scheduled buffer swaps on non 965 chipsets
- major reorder, dedup and general cleanup of register definition and
the header file

Tested by a few, no regressions
@
text
@d557 2
a558 2
	if (cmd->sz & 0x3) {
		DRM_ERROR("alignment\n");
d749 3
d776 3
@


1.2
log
@Update to DRM git as of a few days ago. This mostly affects the
card-specific files with a few minor changes elsewhere.

The main change to the OpenBSD specific stuff is the change to the irq
api due to the vblank rework.

4 more large bugs known, I have a fix for one.

Tested by many.  prompted by deraadt@@.
@
text
@d43 1
a43 1
	u32 last_head = I915_READ(LP_RING + RING_HEAD) & HEAD_ADDR;
d47 1
a47 1
		ring->head = I915_READ(LP_RING + RING_HEAD) & HEAD_ADDR;
d69 2
a70 2
	ring->head = I915_READ(LP_RING + RING_HEAD) & HEAD_ADDR;
	ring->tail = I915_READ(LP_RING + RING_TAIL) & TAIL_ADDR;
d519 1
a519 1
	OUT_RING(CMD_STORE_DWORD_IDX);
d530 1
a530 1
	uint32_t flush_cmd = CMD_MI_FLUSH;
d1002 1
a1002 1
	I915_WRITE(0x02080, dev_priv->status_gfx_addr);
d1011 1
a1011 1
	struct drm_i915_private *dev_priv = dev->dev_private;
@


1.1
log
@Initial import of the DRM (direct rendering manager).

This is the kernel part necessary for DRI support in X. Disabled for now
because it still has a few bugs, but now I can work on it in tree. Also
requires the requisite bits in X, which are currently under discussion
on how to deal with them with privsep. ported from a combination of the
free and netbsd implementations.

Known bugs:
1) only the first occurence of X in any session will have dri, after
that something prevents it working.
2) if the machine does not have a dri capable card, the kernel panics.
Something's up in one of the probe functions. I haven't been able to
find it though.
3) radeon cards need to be forced to use PCI mode otherwise they get
into an infinite loop.

This is known to at least kinda work with SiS, radeons in pci mode and
intel cards.

ok deraadt, kinda ok art, a few other people had a quick look.
@
text
@a53 2
		dev_priv->sarea_priv->perf_boxes |= I915_BOX_WAIT;

a73 3

	if (ring->head == ring->tail)
		dev_priv->sarea_priv->perf_boxes |= I915_BOX_RING_EMPTY;
d109 15
a123 1
static int i915_initialize(struct drm_device * dev, drm_i915_init_t * init)
d126 13
d140 38
d185 2
a186 1
	dev_priv->mmio_map = drm_core_findmap(dev, init->mmio_offset);
d197 8
a204 2
	dev_priv->sarea_priv = (drm_i915_sarea_t *)
	    ((u8 *) dev_priv->sarea->handle + init->sarea_priv_offset);
d229 3
a231 1
	dev_priv->sarea_priv->pf_current_page = 0;
d237 2
d249 1
a249 1
	if (!IS_G33(dev)) {
d267 11
a277 1
	DRM_SPININIT(&dev_priv->cmdbuf_mutex, "915_cmdbuf");
d279 1
d287 1
a287 1
	DRM_DEBUG("%s\n", __FUNCTION__);
d329 2
a330 1
		retcode = i915_initialize(dev, init);
d422 1
a422 1
static int i915_emit_cmds(struct drm_device * dev, int __user * buffer,
d502 1
a502 1
 * emit.  For now, do it in both places:
d515 2
a516 1
	dev_priv->sarea_priv->last_enqueue = dev_priv->counter;
d579 1
a579 1
	i915_emit_breadcrumb( dev );
d581 2
a582 1
	drm_fence_flush_old(dev, 0, dev_priv->counter);
d587 2
a588 2
static int i915_dispatch_batchbuffer(struct drm_device * dev,
				     drm_i915_batchbuffer_t * batch)
d634 1
a634 1
	i915_emit_breadcrumb( dev );
d636 2
a637 1
	drm_fence_flush_old(dev, 0, dev_priv->counter);
d699 1
a699 2
	DRM_DEBUG("%s: planes=0x%x pfCurrentPage=%d\n",
		  __FUNCTION__,
d710 1
a710 1
	if (!sync)
d715 1
a715 1
static int i915_quiescent(struct drm_device * dev)
d794 4
a797 382
#ifdef I915_HAVE_BUFFER
struct i915_relocatee_info {
	struct drm_buffer_object *buf;
	unsigned long offset;
	u32 *data_page;
	unsigned page_offset;
	struct drm_bo_kmap_obj kmap;
	int is_iomem;
};

static void i915_dereference_buffers_locked(struct drm_buffer_object **buffers,
					    unsigned num_buffers)
{
	while (num_buffers--)
		drm_bo_usage_deref_locked(&buffers[num_buffers]);
}

int i915_apply_reloc(struct drm_file *file_priv, int num_buffers,
		     struct drm_buffer_object **buffers,
		     struct i915_relocatee_info *relocatee,
		     uint32_t *reloc)
{
	unsigned index;
	unsigned long new_cmd_offset;
	u32 val;
	int ret;

	if (reloc[2] >= num_buffers) {
		DRM_ERROR("Illegal relocation buffer %08X\n", reloc[2]);
		return -EINVAL;
	}

	new_cmd_offset = reloc[0];
	if (!relocatee->data_page ||
	    !drm_bo_same_page(relocatee->offset, new_cmd_offset)) {
		drm_bo_kunmap(&relocatee->kmap);
		relocatee->offset = new_cmd_offset;
		ret = drm_bo_kmap(relocatee->buf, new_cmd_offset >> PAGE_SHIFT,
				  1, &relocatee->kmap);
		if (ret) {
			DRM_ERROR("Could not map command buffer to apply relocs\n %08lx", new_cmd_offset);
			return ret;
		}

		relocatee->data_page = drm_bmo_virtual(&relocatee->kmap,
						       &relocatee->is_iomem);
		relocatee->page_offset = (relocatee->offset & PAGE_MASK);
	}

	val = buffers[reloc[2]]->offset;
	index = (reloc[0] - relocatee->page_offset) >> 2;

	/* add in validate */
	val = val + reloc[1];

	relocatee->data_page[index] = val;
	return 0;
}

int i915_process_relocs(struct drm_file *file_priv,
			uint32_t buf_handle,
			uint32_t *reloc_buf_handle,
			struct i915_relocatee_info *relocatee,
			struct drm_buffer_object **buffers,
			uint32_t num_buffers)
{
	struct drm_device *dev = file_priv->head->dev;
	struct drm_buffer_object *reloc_list_object;
	uint32_t cur_handle = *reloc_buf_handle;
	uint32_t *reloc_page;
	int ret, reloc_is_iomem, reloc_stride;
	uint32_t num_relocs, reloc_offset, reloc_end, reloc_page_offset, next_offset, cur_offset;
	struct drm_bo_kmap_obj reloc_kmap;

	memset(&reloc_kmap, 0, sizeof(reloc_kmap));

	mutex_lock(&dev->struct_mutex);
	reloc_list_object = drm_lookup_buffer_object(file_priv, cur_handle, 1);
	mutex_unlock(&dev->struct_mutex);
	if (!reloc_list_object)
		return -EINVAL;

	ret = drm_bo_kmap(reloc_list_object, 0, 1, &reloc_kmap);
	if (ret) {
		DRM_ERROR("Could not map relocation buffer.\n");
		goto out;
	}

	reloc_page = drm_bmo_virtual(&reloc_kmap, &reloc_is_iomem);
	num_relocs = reloc_page[0] & 0xffff;

	if ((reloc_page[0] >> 16) & 0xffff) {
		DRM_ERROR("Unsupported relocation type requested\n");
		goto out;
	}

	/* get next relocate buffer handle */
	*reloc_buf_handle = reloc_page[1];
	reloc_stride = I915_RELOC0_STRIDE * sizeof(uint32_t); /* may be different for other types of relocs */

	DRM_DEBUG("num relocs is %d, next is %08X\n", num_relocs, reloc_page[1]);

	reloc_page_offset = 0;
	reloc_offset = I915_RELOC_HEADER * sizeof(uint32_t);
	reloc_end = reloc_offset + (num_relocs * reloc_stride);

	do {
		next_offset = drm_bo_offset_end(reloc_offset, reloc_end);

		do {
			cur_offset = ((reloc_offset + reloc_page_offset) & ~PAGE_MASK) / sizeof(uint32_t);
			ret = i915_apply_reloc(file_priv, num_buffers,
					 buffers, relocatee, &reloc_page[cur_offset]);
			if (ret)
				goto out;

			reloc_offset += reloc_stride;
		} while (reloc_offset < next_offset);

		drm_bo_kunmap(&reloc_kmap);

		reloc_offset = next_offset;
		if (reloc_offset != reloc_end) {
			ret = drm_bo_kmap(reloc_list_object, reloc_offset >> PAGE_SHIFT, 1, &reloc_kmap);
			if (ret) {
				DRM_ERROR("Could not map relocation buffer.\n");
				goto out;
			}

			reloc_page = drm_bmo_virtual(&reloc_kmap, &reloc_is_iomem);
			reloc_page_offset = reloc_offset & ~PAGE_MASK;
		}

	} while (reloc_offset != reloc_end);
out:
	drm_bo_kunmap(&relocatee->kmap);
	relocatee->data_page = NULL;

	drm_bo_kunmap(&reloc_kmap);

	mutex_lock(&dev->struct_mutex);
	drm_bo_usage_deref_locked(&reloc_list_object);
	mutex_unlock(&dev->struct_mutex);

	return ret;
}

static int i915_exec_reloc(struct drm_file *file_priv, drm_handle_t buf_handle,
			   drm_handle_t buf_reloc_handle,
			   struct drm_buffer_object **buffers,
			   uint32_t buf_count)
{
	struct drm_device *dev = file_priv->head->dev;
	struct i915_relocatee_info relocatee;
	int ret = 0;

	memset(&relocatee, 0, sizeof(relocatee));

	mutex_lock(&dev->struct_mutex);
	relocatee.buf = drm_lookup_buffer_object(file_priv, buf_handle, 1);
	mutex_unlock(&dev->struct_mutex);
	if (!relocatee.buf) {
		DRM_DEBUG("relocatee buffer invalid %08x\n", buf_handle);
		ret = -EINVAL;
		goto out_err;
	}

	while (buf_reloc_handle) {
		ret = i915_process_relocs(file_priv, buf_handle, &buf_reloc_handle, &relocatee, buffers, buf_count);
		if (ret) {
			DRM_ERROR("process relocs failed\n");
			break;
		}
	}

	mutex_lock(&dev->struct_mutex);
	drm_bo_usage_deref_locked(&relocatee.buf);
	mutex_unlock(&dev->struct_mutex);

out_err:
	return ret;
}

/*
 * Validate, add fence and relocate a block of bos from a userspace list
 */
int i915_validate_buffer_list(struct drm_file *file_priv,
			      unsigned int fence_class, uint64_t data,
			      struct drm_buffer_object **buffers,
			      uint32_t *num_buffers)
{
	struct drm_i915_op_arg arg;
	struct drm_bo_op_req *req = &arg.d.req;
	struct drm_bo_arg_rep rep;
	unsigned long next = 0;
	int ret = 0;
	unsigned buf_count = 0;
	struct drm_device *dev = file_priv->head->dev;
	uint32_t buf_reloc_handle, buf_handle;


	do {
		if (buf_count >= *num_buffers) {
			DRM_ERROR("Buffer count exceeded %d\n.", *num_buffers);
			ret = -EINVAL;
			goto out_err;
		}

		buffers[buf_count] = NULL;

		if (copy_from_user(&arg, (void __user *)(unsigned)data, sizeof(arg))) {
			ret = -EFAULT;
			goto out_err;
		}

		if (arg.handled) {
			data = arg.next;
			mutex_lock(&dev->struct_mutex);
			buffers[buf_count] = drm_lookup_buffer_object(file_priv, req->arg_handle, 1);
			mutex_unlock(&dev->struct_mutex);
			buf_count++;
			continue;
		}

		rep.ret = 0;
		if (req->op != drm_bo_validate) {
			DRM_ERROR
			    ("Buffer object operation wasn't \"validate\".\n");
			rep.ret = -EINVAL;
			goto out_err;
		}

		buf_handle = req->bo_req.handle;
		buf_reloc_handle = arg.reloc_handle;

		if (buf_reloc_handle) {
			ret = i915_exec_reloc(file_priv, buf_handle, buf_reloc_handle, buffers, buf_count);
			if (ret)
				goto out_err;
			DRM_MEMORYBARRIER();
		}

		rep.ret = drm_bo_handle_validate(file_priv, req->bo_req.handle,
						 req->bo_req.fence_class,
						 req->bo_req.flags,
						 req->bo_req.mask,
						 req->bo_req.hint,
						 0,
						 &rep.bo_info,
						 &buffers[buf_count]);

		if (rep.ret) {
			DRM_ERROR("error on handle validate %d\n", rep.ret);
			goto out_err;
		}


		next = arg.next;
		arg.handled = 1;
		arg.d.rep = rep;

		if (copy_to_user((void __user *)(unsigned)data, &arg, sizeof(arg)))
			return -EFAULT;

		data = next;
		buf_count++;

	} while (next != 0);
	*num_buffers = buf_count;
	return 0;
out_err:
	mutex_lock(&dev->struct_mutex);
	i915_dereference_buffers_locked(buffers, buf_count);
	mutex_unlock(&dev->struct_mutex);
	*num_buffers = 0;
	return (ret) ? ret : rep.ret;
}

static int i915_execbuffer(struct drm_device *dev, void *data,
			   struct drm_file *file_priv)
{
	drm_i915_private_t *dev_priv = (drm_i915_private_t *) dev->dev_private;
	drm_i915_sarea_t *sarea_priv = (drm_i915_sarea_t *)
		dev_priv->sarea_priv;
	struct drm_i915_execbuffer *exec_buf = data;
	struct _drm_i915_batchbuffer *batch = &exec_buf->batch;
	struct drm_fence_arg *fence_arg = &exec_buf->fence_arg;
	int num_buffers;
	int ret;
	struct drm_buffer_object **buffers;
	struct drm_fence_object *fence;

	if (!dev_priv->allow_batchbuffer) {
		DRM_ERROR("Batchbuffer ioctl disabled\n");
		return -EINVAL;
	}


	if (batch->num_cliprects && DRM_VERIFYAREA_READ(batch->cliprects,
							batch->num_cliprects *
							sizeof(struct drm_clip_rect)))
		return -EFAULT;

	if (exec_buf->num_buffers > dev_priv->max_validate_buffers)
		return -EINVAL;


	ret = drm_bo_read_lock(&dev->bm.bm_lock);
	if (ret)
		return ret;

	/*
	 * The cmdbuf_mutex makes sure the validate-submit-fence
	 * operation is atomic.
	 */

	ret = mutex_lock_interruptible(&dev_priv->cmdbuf_mutex);
	if (ret) {
		drm_bo_read_unlock(&dev->bm.bm_lock);
		return -EAGAIN;
	}

	num_buffers = exec_buf->num_buffers;

	buffers = drm_calloc(num_buffers, sizeof(struct drm_buffer_object *), DRM_MEM_DRIVER);
	if (!buffers) {
	        drm_bo_read_unlock(&dev->bm.bm_lock);
		mutex_unlock(&dev_priv->cmdbuf_mutex);
		return -ENOMEM;
        }

	/* validate buffer list + fixup relocations */
	ret = i915_validate_buffer_list(file_priv, 0, exec_buf->ops_list,
					buffers, &num_buffers);
	if (ret)
		goto out_free;

	/* make sure all previous memory operations have passed */
	DRM_MEMORYBARRIER();
	drm_agp_chipset_flush(dev);

	/* submit buffer */
	batch->start = buffers[num_buffers-1]->offset;

	DRM_DEBUG("i915 exec batchbuffer, start %x used %d cliprects %d\n",
		  batch->start, batch->used, batch->num_cliprects);

	ret = i915_dispatch_batchbuffer(dev, batch);
	if (ret)
		goto out_err0;

	sarea_priv->last_dispatch = READ_BREADCRUMB(dev_priv);

	/* fence */
	ret = drm_fence_buffer_objects(dev, NULL, 0, NULL, &fence);
	if (ret)
		goto out_err0;

	if (!(fence_arg->flags & DRM_FENCE_FLAG_NO_USER)) {
		ret = drm_fence_add_user_object(file_priv, fence, fence_arg->flags & DRM_FENCE_FLAG_SHAREABLE);
		if (!ret) {
			fence_arg->handle = fence->base.hash.key;
			fence_arg->fence_class = fence->fence_class;
			fence_arg->type = fence->type;
			fence_arg->signaled = fence->signaled;
		}
	}
	drm_fence_usage_deref_unlocked(&fence);
out_err0:

	/* handle errors */
	mutex_lock(&dev->struct_mutex);
	i915_dereference_buffers_locked(buffers, num_buffers);
	mutex_unlock(&dev->struct_mutex);

out_free:
	drm_free(buffers, (exec_buf->num_buffers * sizeof(struct drm_buffer_object *)), DRM_MEM_DRIVER);

	mutex_unlock(&dev_priv->cmdbuf_mutex);
	drm_bo_read_unlock(&dev->bm.bm_lock);
	return ret;
}
d805 1
a805 1
	DRM_DEBUG("%s\n", __FUNCTION__);
d811 1
a811 1
				 ~(0x3 << (2 * i))) | (num_pages - 1) << (2 * i);
d826 1
a826 1
	DRM_DEBUG("%s\n", __FUNCTION__);
d851 1
a851 1
		DRM_ERROR("%s called with no initialization\n", __FUNCTION__);
d865 3
d888 1
a888 1
		DRM_ERROR("%s called with no initialization\n", __FUNCTION__);
d894 2
a895 1
		dev_priv->use_mi_batchbuffer_start = param->value;
d932 1
a932 1
		DRM_ERROR("%s called with no initialization\n", __FUNCTION__);
d943 21
a963 21
		case I915_MMIO_READ:
			if (!(e->flag & I915_MMIO_MAY_READ))
				return -EINVAL;
			for (i = 0; i < e->size / 4; i++)
				buf[i] = I915_READ(e->offset + i * 4);
			if (DRM_COPY_TO_USER(mmio->data, buf, e->size)) {
				DRM_ERROR("DRM_COPY_TO_USER failed\n");
				return -EFAULT;
			}
			break;

		case I915_MMIO_WRITE:
			if (!(e->flag & I915_MMIO_MAY_WRITE))
				return -EINVAL;
			if(DRM_COPY_FROM_USER(buf, mmio->data, e->size)) {
				DRM_ERROR("DRM_COPY_TO_USER failed\n");
				return -EFAULT;
			}
			for (i = 0; i < e->size / 4; i++)
				I915_WRITE(e->offset + i * 4, buf[i]);
			break;
d974 3
d978 1
a978 1
		DRM_ERROR("%s called with no initialization\n", __FUNCTION__);
d1034 2
a1035 2
	ret = drm_addmap(dev, base, size, _DRM_REGISTERS, _DRM_KERNEL,
			 &dev_priv->mmio_map);
d1037 1
d1041 2
d1055 1
d1059 1
d1067 11
d1082 7
d1090 7
d1151 1
a1151 2
	if (IS_I9XX(dev))
		drm_bo_driver_init(dev);
@

