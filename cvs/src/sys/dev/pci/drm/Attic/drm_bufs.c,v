head	1.50;
access;
symbols
	OPENBSD_5_5:1.49.0.10
	OPENBSD_5_5_BASE:1.49
	OPENBSD_5_4:1.49.0.6
	OPENBSD_5_4_BASE:1.49
	OPENBSD_5_3:1.49.0.4
	OPENBSD_5_3_BASE:1.49
	OPENBSD_5_2:1.49.0.2
	OPENBSD_5_2_BASE:1.49
	OPENBSD_5_1_BASE:1.48
	OPENBSD_5_1:1.48.0.4
	OPENBSD_5_0:1.48.0.2
	OPENBSD_5_0_BASE:1.48
	OPENBSD_4_9:1.47.0.2
	OPENBSD_4_9_BASE:1.47
	OPENBSD_4_8:1.45.0.6
	OPENBSD_4_8_BASE:1.45
	OPENBSD_4_7:1.45.0.2
	OPENBSD_4_7_BASE:1.45
	OPENBSD_4_6:1.45.0.4
	OPENBSD_4_6_BASE:1.45
	OPENBSD_4_5:1.34.0.2
	OPENBSD_4_5_BASE:1.34
	OPENBSD_4_4:1.12.0.2
	OPENBSD_4_4_BASE:1.12
	OPENBSD_4_3:1.2.0.2
	OPENBSD_4_3_BASE:1.2;
locks; strict;
comment	@ * @;


1.50
date	2014.03.09.07.42.29;	author jsg;	state dead;
branches;
next	1.49;

1.49
date	2012.03.09.13.01.28;	author ariane;	state Exp;
branches;
next	1.48;

1.48
date	2011.06.02.18.22.00;	author weerd;	state Exp;
branches;
next	1.47;

1.47
date	2010.12.16.01.09.58;	author tedu;	state Exp;
branches;
next	1.46;

1.46
date	2010.12.15.04.59.52;	author tedu;	state Exp;
branches;
next	1.45;

1.45
date	2009.06.02.12.38.14;	author guenther;	state Exp;
branches;
next	1.44;

1.44
date	2009.04.04.16.02.57;	author oga;	state Exp;
branches;
next	1.43;

1.43
date	2009.04.03.15.22.30;	author oga;	state Exp;
branches;
next	1.42;

1.42
date	2009.04.03.15.07.59;	author oga;	state Exp;
branches;
next	1.41;

1.41
date	2009.04.03.15.04.46;	author oga;	state Exp;
branches;
next	1.40;

1.40
date	2009.04.03.14.59.27;	author oga;	state Exp;
branches;
next	1.39;

1.39
date	2009.03.27.19.36.55;	author oga;	state Exp;
branches;
next	1.38;

1.38
date	2009.03.27.19.00.45;	author oga;	state Exp;
branches;
next	1.37;

1.37
date	2009.03.27.17.44.12;	author oga;	state Exp;
branches;
next	1.36;

1.36
date	2009.03.26.07.25.06;	author oga;	state Exp;
branches;
next	1.35;

1.35
date	2009.03.20.09.35.19;	author oga;	state Exp;
branches;
next	1.34;

1.34
date	2009.02.15.19.58.08;	author oga;	state Exp;
branches;
next	1.33;

1.33
date	2009.02.15.19.56.04;	author oga;	state Exp;
branches;
next	1.32;

1.32
date	2008.11.24.16.56.25;	author oga;	state Exp;
branches;
next	1.31;

1.31
date	2008.11.24.12.22.17;	author oga;	state Exp;
branches;
next	1.30;

1.30
date	2008.11.24.05.51.23;	author oga;	state Exp;
branches;
next	1.29;

1.29
date	2008.11.23.22.56.02;	author oga;	state Exp;
branches;
next	1.28;

1.28
date	2008.11.23.21.35.00;	author oga;	state Exp;
branches;
next	1.27;

1.27
date	2008.11.22.22.43.53;	author oga;	state Exp;
branches;
next	1.26;

1.26
date	2008.11.22.14.42.36;	author oga;	state Exp;
branches;
next	1.25;

1.25
date	2008.11.17.00.30.41;	author oga;	state Exp;
branches;
next	1.24;

1.24
date	2008.10.07.21.59.32;	author oga;	state Exp;
branches;
next	1.23;

1.23
date	2008.09.29.22.50.07;	author oga;	state Exp;
branches;
next	1.22;

1.22
date	2008.09.18.15.10.57;	author oga;	state Exp;
branches;
next	1.21;

1.21
date	2008.09.06.02.54.52;	author oga;	state Exp;
branches;
next	1.20;

1.20
date	2008.09.05.23.51.03;	author oga;	state Exp;
branches;
next	1.19;

1.19
date	2008.09.02.01.12.04;	author oga;	state Exp;
branches;
next	1.18;

1.18
date	2008.08.28.00.19.27;	author oga;	state Exp;
branches;
next	1.17;

1.17
date	2008.08.18.00.01.49;	author oga;	state Exp;
branches;
next	1.16;

1.16
date	2008.08.17.19.21.36;	author oga;	state Exp;
branches;
next	1.15;

1.15
date	2008.08.17.15.31.39;	author oga;	state Exp;
branches;
next	1.14;

1.14
date	2008.08.16.01.53.05;	author oga;	state Exp;
branches;
next	1.13;

1.13
date	2008.08.13.20.38.26;	author oga;	state Exp;
branches;
next	1.12;

1.12
date	2008.07.29.19.44.13;	author oga;	state Exp;
branches;
next	1.11;

1.11
date	2008.06.26.19.33.19;	author oga;	state Exp;
branches;
next	1.10;

1.10
date	2008.06.26.18.35.24;	author oga;	state Exp;
branches;
next	1.9;

1.9
date	2008.06.26.16.42.47;	author oga;	state Exp;
branches;
next	1.8;

1.8
date	2008.06.12.19.14.53;	author oga;	state Exp;
branches;
next	1.7;

1.7
date	2008.06.11.09.33.01;	author oga;	state Exp;
branches;
next	1.6;

1.6
date	2008.06.03.17.21.22;	author oga;	state Exp;
branches;
next	1.5;

1.5
date	2008.05.06.19.24.56;	author oga;	state Exp;
branches;
next	1.4;

1.4
date	2008.05.06.19.19.02;	author oga;	state Exp;
branches;
next	1.3;

1.3
date	2008.04.12.13.55.59;	author oga;	state Exp;
branches;
next	1.2;

1.2
date	2007.12.16.01.02.31;	author oga;	state Exp;
branches;
next	1.1;

1.1
date	2007.11.28.23.37.34;	author oga;	state Exp;
branches;
next	;


desc
@@


1.50
log
@remove dri1 interfaces
discussed with kettenis some time last year
@
text
@/* $OpenBSD: drm_bufs.c,v 1.49 2012/03/09 13:01:28 ariane Exp $ */
/*-
 * Copyright 1999, 2000 Precision Insight, Inc., Cedar Park, Texas.
 * Copyright 2000 VA Linux Systems, Inc., Sunnyvale, California.
 * All Rights Reserved.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice (including the next
 * paragraph) shall be included in all copies or substantial portions of the
 * Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
 * VA LINUX SYSTEMS AND/OR ITS SUPPLIERS BE LIABLE FOR ANY CLAIM, DAMAGES OR
 * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
 * OTHER DEALINGS IN THE SOFTWARE.
 *
 * Authors:
 *    Rickard E. (Rik) Faith <faith@@valinux.com>
 *    Gareth Hughes <gareth@@valinux.com>
 *
 */

/** @@file drm_bufs.c
 * Implementation of the ioctls for setup of DRM mappings and DMA buffers.
 */

#include "sys/types.h"
#include "dev/pci/pcireg.h"

#include "drmP.h"

int	drm_addbufs_pci(struct drm_device *, struct drm_buf_desc *);
int	drm_addbufs_sg(struct drm_device *, struct drm_buf_desc *);
int	drm_addbufs_agp(struct drm_device *, struct drm_buf_desc *);

/*
 * Compute order.  Can be made faster.
 */
int
drm_order(unsigned long size)
{
	int order;
	unsigned long tmp;

	for (order = 0, tmp = size; tmp >>= 1; ++order)
		;

	if (size & ~(1 << order))
		++order;

	return order;
}

struct drm_local_map *
drm_core_findmap(struct drm_device *dev, unsigned long offset)
{
	struct drm_local_map	*map;

	DRM_LOCK();
	TAILQ_FOREACH(map, &dev->maplist, link) {
		if (offset == map->ext)
			break;
	}
	DRM_UNLOCK();
	return (map);
}

int
drm_addmap(struct drm_device * dev, unsigned long offset, unsigned long size,
    enum drm_map_type type, enum drm_map_flags flags,
    struct drm_local_map **map_ptr)
{
	struct drm_local_map	*map;
	int			 align, ret = 0;
#if 0 /* disabled for now */
	struct drm_agp_mem	*entry;
	int			 valid;
#endif 

	/* Only allow shared memory to be removable since we only keep enough
	 * book keeping information about shared memory to allow for removal
	 * when processes fork.
	 */
	if ((flags & _DRM_REMOVABLE) && type != _DRM_SHM) {
		DRM_ERROR("Requested removable map for non-DRM_SHM\n");
		return EINVAL;
	}
	if ((offset & PAGE_MASK) || (size & PAGE_MASK)) {
		DRM_ERROR("offset/size not page aligned: 0x%lx/0x%lx\n",
		    offset, size);
		return EINVAL;
	}
	if (offset + size < offset) {
		DRM_ERROR("offset and size wrap around: 0x%lx/0x%lx\n",
		    offset, size);
		return EINVAL;
	}

	DRM_DEBUG("offset = 0x%08lx, size = 0x%08lx, type = %d\n", offset,
	    size, type);

	/*
	 * Check if this is just another version of a kernel-allocated map, and
	 * just hand that back if so.
	 */
	DRM_LOCK();
	if (type == _DRM_REGISTERS || type == _DRM_FRAME_BUFFER ||
	    type == _DRM_SHM) {
		TAILQ_FOREACH(map, &dev->maplist, link) {
			if (map->type == type && (map->offset == offset ||
			    (map->type == _DRM_SHM &&
			    map->flags == _DRM_CONTAINS_LOCK))) {
				DRM_DEBUG("Found kernel map %d\n", type);
				goto done;
			}
		}
	}
	DRM_UNLOCK();

	/* Allocate a new map structure, fill it in, and do any type-specific
	 * initialization necessary.
	 */
	map = drm_calloc(1, sizeof(*map));
	if (map == NULL) {
		DRM_LOCK();
		return ENOMEM;
	}

	map->offset = offset;
	map->size = size;
	map->type = type;
	map->flags = flags;


	DRM_LOCK();
	ret = extent_alloc(dev->handle_ext, map->size, PAGE_SIZE, 0,
	    0, EX_NOWAIT, &map->ext);
	if (ret) {
		DRM_ERROR("can't find free offset\n");
		DRM_UNLOCK();
		drm_free(map);
		return (ret);
	}
	DRM_UNLOCK();

	switch (map->type) {
	case _DRM_REGISTERS:
		if (!(map->flags & _DRM_WRITE_COMBINING))
			break;
		/* FALLTHROUGH */
	case _DRM_FRAME_BUFFER:
		if (drm_mtrr_add(map->offset, map->size, DRM_MTRR_WC) == 0)
			map->mtrr = 1;
		break;
	case _DRM_AGP:
		/*valid = 0;*/
		/* In some cases (i810 driver), user space may have already
		 * added the AGP base itself, because dev->agp->base previously
		 * only got set during AGP enable.  So, only add the base
		 * address if the map's offset isn't already within the
		 * aperture.
		 */
		if (map->offset < dev->agp->base ||
		    map->offset > dev->agp->base +
		    dev->agp->info.ai_aperture_size - 1) {
			map->offset += dev->agp->base;
		}
		map->mtrr   = dev->agp->mtrr; /* for getmap */
#if 0 /* disabled for now */
		/*
		 * If agp is in control of userspace (some intel drivers for
		 * example. In which case ignore this loop.
		 */
		DRM_LOCK();
		TAILQ_FOREACH(entry, &dev->agp->memory, link) {
			DRM_DEBUG("bound = %p, pages = %p, %p\n",
			    entry->bound, entry->pages,
			    entry->pages * PAGE_SIZE);
			if ((map->offset >= entry->bound) &&
			    (map->offset + map->size <=
			    entry->bound + entry->pages * PAGE_SIZE)) {
				valid = 1;
				break;
			}
		}
		if (!TAILQ_EMPTY(&dev->agp->memory) && !valid) {
			DRM_UNLOCK();
			drm_free(map);
			DRM_ERROR("invalid agp map requested\n");
			return (EACCES);
		}
		DRM_UNLOCK();
#endif
		break;
	case _DRM_SCATTER_GATHER:
		if (dev->sg == NULL) {
			drm_free(map);
			return (EINVAL);
		}
		map->offset += dev->sg->handle;
		break;
	case _DRM_SHM:
	case _DRM_CONSISTENT:
		/*
		 * Unfortunately, we don't get any alignment specification from
		 * the caller, so we have to guess. So try to align the bus
		 * address of the map to its size if possible, otherwise just
		 * assume PAGE_SIZE alignment.
		 */
		align = map->size;
		if ((align & (align - 1)) != 0)
			align = PAGE_SIZE;
		map->dmamem = drm_dmamem_alloc(dev->dmat, map->size, align,
		    1, map->size, 0, 0);
		if (map->dmamem == NULL) {
			drm_free(map);
			return (ENOMEM);
		}
		map->handle = map->dmamem->kva;
		map->offset = map->dmamem->map->dm_segs[0].ds_addr;
		if (map->type == _DRM_SHM && map->flags & _DRM_CONTAINS_LOCK) {
			DRM_LOCK();
			/* Prevent a 2nd X Server from creating a 2nd lock */
			if (dev->lock.hw_lock != NULL) {
				DRM_UNLOCK();
				drm_dmamem_free(dev->dmat, map->dmamem);
				drm_free(map);
				return (EBUSY);
			}
			dev->lock.hw_lock = map->handle;
			DRM_UNLOCK();
		}
		break;
	default:
		DRM_ERROR("Bad map type %d\n", map->type);
		drm_free(map);
		return EINVAL;
	}

	DRM_LOCK();
	TAILQ_INSERT_TAIL(&dev->maplist, map, link);
done:
	DRM_UNLOCK();

	DRM_DEBUG("Added map %d 0x%lx/0x%lx\n", map->type, map->offset,
	    map->size);

	*map_ptr = map;

	return 0;
}

int
drm_addmap_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
{
	struct drm_map		*request = data;
	struct drm_local_map	*map;
	int			 err;

	if (!(file_priv->flags & (FREAD|FWRITE)))
		return EACCES; /* Require read/write */

	err = drm_addmap(dev, request->offset, request->size, request->type,
	    request->flags, &map);
	if (err != 0)
		return err;

	request->offset = map->offset;
	request->size = map->size;
	request->type = map->type;
	request->flags = map->flags;
	request->mtrr = map->mtrr;
	request->handle = map->handle;

	request->handle = (void *)map->ext;

	return 0;
}

void
drm_rmmap(struct drm_device *dev, struct drm_local_map *map)
{
	DRM_LOCK();
	drm_rmmap_locked(dev, map);
	DRM_UNLOCK();
}


void
drm_rmmap_locked(struct drm_device *dev, struct drm_local_map *map)
{
	TAILQ_REMOVE(&dev->maplist, map, link);

	switch (map->type) {
	case _DRM_REGISTERS:
		/* FALLTHROUGH */
	case _DRM_FRAME_BUFFER:
		if (map->mtrr) {
			int retcode;
			
			retcode = drm_mtrr_del(0, map->offset, map->size,
			    DRM_MTRR_WC);
			DRM_DEBUG("mtrr_del = %d\n", retcode);
		}
		break;
	case _DRM_AGP:
		/* FALLTHROUGH */
	case _DRM_SCATTER_GATHER:
		break;
	case _DRM_SHM:
		/* FALLTHROUGH */
	case _DRM_CONSISTENT:
		drm_dmamem_free(dev->dmat, map->dmamem);
		break;
	default:
		DRM_ERROR("Bad map type %d\n", map->type);
		break;
	}

	/* NOCOALESCE set, can't fail */
	extent_free(dev->handle_ext, map->ext, map->size, EX_NOWAIT);

	drm_free(map);
}

/* Remove a map private from list and deallocate resources if the mapping
 * isn't in use.
 */

int
drm_rmmap_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)
{
	struct drm_local_map	*map;
	struct drm_map		*request = data;

	DRM_LOCK();
	TAILQ_FOREACH(map, &dev->maplist, link) {
		if (map->handle == request->handle &&
		    map->flags & _DRM_REMOVABLE)
			break;
	}

	/* No match found. */
	if (map == NULL) {
		DRM_UNLOCK();
		return (EINVAL);
	}

	drm_rmmap_locked(dev, map);

	DRM_UNLOCK();

	return 0;
}

/*
 * DMA buffers api.
 *
 * The implementation used to be significantly more complicated, but the
 * complexity has been moved into the drivers as different buffer management
 * schemes evolved.
 *
 * This api is going to die eventually.
 */

int
drm_dma_setup(struct drm_device *dev)
{

	dev->dma = drm_calloc(1, sizeof(*dev->dma));
	if (dev->dma == NULL)
		return (ENOMEM);

	rw_init(&dev->dma->dma_lock, "drmdma");

	return (0);
}

void
drm_cleanup_buf(struct drm_device *dev, struct drm_buf_entry *entry)
{
	int i;

	if (entry->seg_count) {
		for (i = 0; i < entry->seg_count; i++)
			drm_dmamem_free(dev->dmat, entry->seglist[i]);
		drm_free(entry->seglist);

		entry->seg_count = 0;
	}

   	if (entry->buf_count) {
	   	for (i = 0; i < entry->buf_count; i++) {
			drm_free(entry->buflist[i].dev_private);
		}
		drm_free(entry->buflist);

		entry->buf_count = 0;
	}
}

void
drm_dma_takedown(struct drm_device *dev)
{
	struct drm_device_dma	*dma = dev->dma;
	int			 i;

	if (dma == NULL)
		return;

	/* Clear dma buffers */
	for (i = 0; i <= DRM_MAX_ORDER; i++)
		drm_cleanup_buf(dev, &dma->bufs[i]);

	drm_free(dma->buflist);
	drm_free(dma->pagelist);
	drm_free(dev->dma);
	dev->dma = NULL;
}


void
drm_free_buffer(struct drm_device *dev, struct drm_buf *buf)
{
	if (buf == NULL)
		return;

	buf->pending = 0;
	buf->file_priv= NULL;
	buf->used = 0;
}

void
drm_reclaim_buffers(struct drm_device *dev, struct drm_file *file_priv)
{
	struct drm_device_dma	*dma = dev->dma;
	int			 i;

	if (dma == NULL)
		return;
	for (i = 0; i < dma->buf_count; i++) {
		if (dma->buflist[i]->file_priv == file_priv)
				drm_free_buffer(dev, dma->buflist[i]);
	}
}

/* Call into the driver-specific DMA handler */
int
drm_dma(struct drm_device *dev, void *data, struct drm_file *file_priv)
{
	struct drm_device_dma	*dma = dev->dma;
	struct drm_dma		*d = data;
	int			 ret = 0;

	if (dev->driver->dma_ioctl == NULL) {
		DRM_DEBUG("DMA ioctl on driver with no dma handler\n");
		return (EINVAL);
	}

	LOCK_TEST_WITH_RETURN(dev, file_priv);

	/* Please don't send us buffers.
	 */
	if (d->send_count != 0) {
		DRM_ERROR("process trying to send %d buffers via drmDMA\n",
		    d->send_count);
		return (EINVAL);
	}

	/* We'll send you buffers.
	 */
	if (d->request_count < 0 || d->request_count > dma->buf_count) {
		DRM_ERROR("Process trying to get %d buffers (of %d max)\n",
			  d->request_count, dma->buf_count);
		return (EINVAL);
	}
	d->granted_count = 0;

	if (d->request_count)
		ret = dev->driver->dma_ioctl(dev, d, file_priv);
	return (ret);
}

int
drm_addbufs_agp(struct drm_device *dev, struct drm_buf_desc *request)
{
	struct drm_device_dma	*dma = dev->dma;
	struct drm_buf_entry	*entry;
	struct drm_buf		*buf, **temp_buflist;
	unsigned long		 agp_offset, offset;
	int			 alignment, count, order, page_order, size;
	int			 total, byte_count, i;
#if 0 /* disabled for now */
	struct drm_agp_mem	*agp_entry;
	int			 valid;
#endif

	count = request->count;
	order = drm_order(request->size);
	size = 1 << order;

	alignment  = (request->flags & _DRM_PAGE_ALIGN)
	    ? round_page(size) : size;
	page_order = order - PAGE_SHIFT > 0 ? order - PAGE_SHIFT : 0;
	total = PAGE_SIZE << page_order;

	byte_count = 0;
	agp_offset = dev->agp->base + request->agp_start;

	DRM_DEBUG("count:      %d\n",  count);
	DRM_DEBUG("order:      %d\n",  order);
	DRM_DEBUG("size:       %d\n",  size);
	DRM_DEBUG("agp_offset: 0x%lx\n", agp_offset);
	DRM_DEBUG("alignment:  %d\n",  alignment);
	DRM_DEBUG("page_order: %d\n",  page_order);
	DRM_DEBUG("total:      %d\n",  total);

	/* Make sure buffers are located in AGP memory that we own */

	/* Breaks MGA due to drm_alloc_agp not setting up entries for the
	 * memory.  Safe to ignore for now because these ioctls are still
	 * root-only.
	 */
#if 0 /* disabled for now */
	valid = 0;
	DRM_LOCK();
	TAILQ_FOREACH(agp_entry, &dev->agp->memory, link) {
		if ((agp_offset >= agp_entry->bound) &&
		    (agp_offset + total * count <=
		    agp_entry->bound + agp_entry->pages * PAGE_SIZE)) {
			valid = 1;
			break;
		}
	}
	if (!TAILQ_EMPTY(&dev->agp->memory) && !valid) {
		DRM_DEBUG("zone invalid\n");
		DRM_UNLOCK();
		return (EINVAL);
	}
	DRM_UNLOCK();
#endif

	entry = &dma->bufs[order];

	entry->buflist = drm_calloc(count, sizeof(*entry->buflist));
	if (entry->buflist == NULL)
		return ENOMEM;

	entry->buf_size = size;
	entry->page_order = page_order;

	offset = 0;

	while (entry->buf_count < count) {
		buf = &entry->buflist[entry->buf_count];
		buf->idx = dma->buf_count + entry->buf_count;
		buf->total = alignment;
		buf->used = 0;

		buf->offset = (dma->byte_count + offset);
		buf->bus_address = agp_offset + offset;
		buf->pending = 0;
		buf->file_priv = NULL;

		buf->dev_private = drm_calloc(1, dev->driver->buf_priv_size);
		if (buf->dev_private == NULL) {
			/* Set count correctly so we free the proper amount. */
			entry->buf_count = count;
			drm_cleanup_buf(dev, entry);
			return ENOMEM;
		}

		offset += alignment;
		entry->buf_count++;
		byte_count += PAGE_SIZE << page_order;
	}

	DRM_DEBUG("byte_count: %d\n", byte_count);

	/* OpenBSD lacks realloc in kernel */
	temp_buflist = drm_realloc(dma->buflist,
	    dma->buf_count * sizeof(*dma->buflist),
	    (dma->buf_count + entry->buf_count) * sizeof(*dma->buflist));
	if (temp_buflist == NULL) {
		/* Free the entry because it isn't valid */
		drm_cleanup_buf(dev, entry);
		return ENOMEM;
	}
	dma->buflist = temp_buflist;

	for (i = 0; i < entry->buf_count; i++)
		dma->buflist[i + dma->buf_count] = &entry->buflist[i];

	dma->buf_count += entry->buf_count;
	dma->byte_count += byte_count;

	DRM_DEBUG("dma->buf_count : %d\n", dma->buf_count);
	DRM_DEBUG("entry->buf_count : %d\n", entry->buf_count);

	request->count = entry->buf_count;
	request->size = size;

	dma->flags = _DRM_DMA_USE_AGP;

	return 0;
}

int
drm_addbufs_pci(struct drm_device *dev, struct drm_buf_desc *request)
{
	struct drm_device_dma	*dma = dev->dma;
	struct drm_buf		*buf, **temp_buflist;
	struct drm_buf_entry	*entry;
	int			 alignment, byte_count, count, i, order;
	int			 page_count, page_order, size, total;
	unsigned long		 offset, *temp_pagelist;

	count = request->count;
	order = drm_order(request->size);
	size = 1 << order;

	DRM_DEBUG("count=%d, size=%d (%d), order=%d\n",
	    request->count, request->size, size, order);

	alignment = (request->flags & _DRM_PAGE_ALIGN)
	    ? round_page(size) : size;
	page_order = order - PAGE_SHIFT > 0 ? order - PAGE_SHIFT : 0;
	total = PAGE_SIZE << page_order;

	entry = &dma->bufs[order];

	entry->buflist = drm_calloc(count, sizeof(*entry->buflist));
	entry->seglist = drm_calloc(count, sizeof(*entry->seglist));

	/* Keep the original pagelist until we know all the allocations
	 * have succeeded
	 */
	temp_pagelist = drm_calloc((dma->page_count + (count << page_order)),
	    sizeof(*dma->pagelist));

	if (entry->buflist == NULL || entry->seglist == NULL || 
	    temp_pagelist == NULL) {
		drm_free(temp_pagelist);
		drm_free(entry->seglist);
		drm_free(entry->buflist);
		return ENOMEM;
	}

	memcpy(temp_pagelist, dma->pagelist, dma->page_count * 
	    sizeof(*dma->pagelist));

	DRM_DEBUG("pagelist: %d entries\n",
	    dma->page_count + (count << page_order));

	entry->buf_size	= size;
	entry->page_order = page_order;
	byte_count = 0;
	page_count = 0;

	while (entry->buf_count < count) {
		struct drm_dmamem *mem = drm_dmamem_alloc(dev->dmat, size,
		    alignment, 1, size, 0, 0);
		if (mem == NULL) {
			/* Set count correctly so we free the proper amount. */
			entry->buf_count = count;
			entry->seg_count = count;
			drm_cleanup_buf(dev, entry);
			drm_free(temp_pagelist);
			return ENOMEM;
		}

		entry->seglist[entry->seg_count++] = mem;
		for (i = 0; i < (1 << page_order); i++) {
			DRM_DEBUG("page %d @@ %p\n", dma->page_count +
			    page_count, mem->kva + PAGE_SIZE * i);
			temp_pagelist[dma->page_count + page_count++] = 
			    (long)mem->kva + PAGE_SIZE * i;
		}
		for (offset = 0;
		    offset + size <= total && entry->buf_count < count;
		    offset += alignment, ++entry->buf_count) {
			buf = &entry->buflist[entry->buf_count];
			buf->idx = dma->buf_count + entry->buf_count;
			buf->total = alignment;
			buf->used = 0;
			buf->offset = (dma->byte_count + byte_count + offset);
			buf->address = mem->kva + offset;
			buf->bus_address = mem->map->dm_segs[0].ds_addr +
			    offset;
			buf->pending = 0;
			buf->file_priv = NULL;

			buf->dev_private = drm_calloc(1,
			    dev->driver->buf_priv_size);
			if (buf->dev_private == NULL) {
				/* Set count so we free the proper amount. */
				entry->buf_count = count;
				entry->seg_count = count;
				drm_cleanup_buf(dev, entry);
				drm_free(temp_pagelist);
				return ENOMEM;
			}

			DRM_DEBUG("buffer %d\n",
			    entry->buf_count);
		}
		byte_count += PAGE_SIZE << page_order;
	}

	temp_buflist = drm_realloc(dma->buflist,
	    dma->buf_count * sizeof(*dma->buflist),
	    (dma->buf_count + entry->buf_count) * sizeof(*dma->buflist));
	if (temp_buflist == NULL) {
		/* Free the entry because it isn't valid */
		drm_cleanup_buf(dev, entry);
		drm_free(temp_pagelist);
		return ENOMEM;
	}
	dma->buflist = temp_buflist;

	for (i = 0; i < entry->buf_count; i++)
		dma->buflist[i + dma->buf_count] = &entry->buflist[i];

	/* No allocations failed, so now we can replace the orginal pagelist
	 * with the new one.
	 */
	drm_free(dma->pagelist);
	dma->pagelist = temp_pagelist;

	dma->buf_count += entry->buf_count;
	dma->seg_count += entry->seg_count;
	dma->page_count += entry->seg_count << page_order;
	dma->byte_count += PAGE_SIZE * (entry->seg_count << page_order);

	request->count = entry->buf_count;
	request->size = size;

	return 0;

}

int
drm_addbufs_sg(struct drm_device *dev, struct drm_buf_desc *request)
{
	struct drm_device_dma	*dma = dev->dma;
	struct drm_buf_entry	*entry;
	struct drm_buf		*buf, **temp_buflist;
	unsigned long		 agp_offset, offset;
	int			 alignment, byte_count, count, i, order;
	int			 page_order, size, total;

	count = request->count;
	order = drm_order(request->size);
	size = 1 << order;

	alignment  = (request->flags & _DRM_PAGE_ALIGN)
	    ? round_page(size) : size;
	page_order = order - PAGE_SHIFT > 0 ? order - PAGE_SHIFT : 0;
	total = PAGE_SIZE << page_order;

	byte_count = 0;
	agp_offset = request->agp_start;

	DRM_DEBUG("count:      %d\n",  count);
	DRM_DEBUG("order:      %d\n",  order);
	DRM_DEBUG("size:       %d\n",  size);
	DRM_DEBUG("agp_offset: %ld\n", agp_offset);
	DRM_DEBUG("alignment:  %d\n",  alignment);
	DRM_DEBUG("page_order: %d\n",  page_order);
	DRM_DEBUG("total:      %d\n",  total);

	entry = &dma->bufs[order];

	entry->buflist = drm_calloc(count, sizeof(*entry->buflist));
	if (entry->buflist == NULL)
		return ENOMEM;

	entry->buf_size = size;
	entry->page_order = page_order;

	offset = 0;

	while (entry->buf_count < count) {
		buf = &entry->buflist[entry->buf_count];
		buf->idx = dma->buf_count + entry->buf_count;
		buf->total = alignment;
		buf->used = 0;

		buf->offset = (dma->byte_count + offset);
		buf->bus_address = agp_offset + offset;
		buf->pending = 0;
		buf->file_priv = NULL;

		buf->dev_private = drm_calloc(1, dev->driver->buf_priv_size);
		if (buf->dev_private == NULL) {
			/* Set count correctly so we free the proper amount. */
			entry->buf_count = count;
			drm_cleanup_buf(dev, entry);
			return ENOMEM;
		}

		DRM_DEBUG("buffer %d\n", entry->buf_count);

		offset += alignment;
		entry->buf_count++;
		byte_count += PAGE_SIZE << page_order;
	}

	DRM_DEBUG("byte_count: %d\n", byte_count);

	temp_buflist = drm_realloc(dma->buflist, 
	    dma->buf_count * sizeof(*dma->buflist),
	    (dma->buf_count + entry->buf_count) * sizeof(*dma->buflist));
	if (temp_buflist == NULL) {
		/* Free the entry because it isn't valid */
		drm_cleanup_buf(dev, entry);
		return ENOMEM;
	}
	dma->buflist = temp_buflist;

	for (i = 0; i < entry->buf_count; i++)
		dma->buflist[i + dma->buf_count] = &entry->buflist[i];

	dma->buf_count += entry->buf_count;
	dma->byte_count += byte_count;

	DRM_DEBUG("dma->buf_count : %d\n", dma->buf_count);
	DRM_DEBUG("entry->buf_count : %d\n", entry->buf_count);

	request->count = entry->buf_count;
	request->size = size;

	dma->flags = _DRM_DMA_USE_SG;

	return 0;
}

int
drm_addbufs(struct drm_device *dev, struct drm_buf_desc *request)
{
	struct drm_device_dma	*dma = dev->dma;
	int			 order, ret;

	if (request->count < 0 || request->count > 4096)
		return (EINVAL);
	
	order = drm_order(request->size);
	if (order < DRM_MIN_ORDER || order > DRM_MAX_ORDER)
		return (EINVAL);

	rw_enter_write(&dma->dma_lock);

	/* No more allocations after first buffer-using ioctl. */
	if (dma->buf_use != 0) {
		rw_exit_write(&dma->dma_lock);
		return (EBUSY);
	}
	/* No more than one allocation per order */
	if (dma->bufs[order].buf_count != 0) {
		rw_exit_write(&dma->dma_lock);
		return (ENOMEM);
	}

	if (request->flags & _DRM_AGP_BUFFER)
		ret = drm_addbufs_agp(dev, request);
	else if (request->flags & _DRM_SG_BUFFER)
		ret = drm_addbufs_sg(dev, request);
	else
		ret = drm_addbufs_pci(dev, request);

	rw_exit_write(&dma->dma_lock);

	return (ret);
}

int
drm_freebufs(struct drm_device *dev, void *data, struct drm_file *file_priv)
{
	struct drm_device_dma	*dma = dev->dma;
	struct drm_buf_free	*request = data;
	struct drm_buf		*buf;
	int			 i, idx, retcode = 0;

	DRM_DEBUG("%d\n", request->count);
	
	rw_enter_write(&dma->dma_lock);
	for (i = 0; i < request->count; i++) {
		if (DRM_COPY_FROM_USER(&idx, &request->list[i], sizeof(idx))) {
			retcode = EFAULT;
			break;
		}
		if (idx < 0 || idx >= dma->buf_count) {
			DRM_ERROR("Index %d (of %d max)\n", idx,
			    dma->buf_count - 1);
			retcode = EINVAL;
			break;
		}
		buf = dma->buflist[idx];
		if (buf->file_priv != file_priv) {
			DRM_ERROR("Process %d freeing buffer not owned\n",
			    DRM_CURRENTPID);
			retcode = EINVAL;
			break;
		}
		drm_free_buffer(dev, buf);
	}
	rw_exit_write(&dma->dma_lock);

	return retcode;
}

int
drm_mapbufs(struct drm_device *dev, void *data, struct drm_file *file_priv)
{
	struct drm_device_dma	*dma = dev->dma;
	struct drm_buf_map	*request = data;
	struct vnode		*vn;
	vaddr_t			 address, vaddr;
	voff_t			 foff;
	vsize_t			 size;
	const int		 zero = 0;
	int			 i, retcode = 0;

	if (!vfinddev(file_priv->kdev, VCHR, &vn))
		return EINVAL;

	rw_enter_write(&dma->dma_lock);
	dev->dma->buf_use++;	/* Can't allocate more after this call */
	rw_exit_write(&dma->dma_lock);

	if (request->count < dma->buf_count)
		goto done;

	if ((dev->driver->flags & DRIVER_AGP &&
	    (dma->flags & _DRM_DMA_USE_AGP)) ||
	    (dev->driver->flags & DRIVER_SG &&
	    (dma->flags & _DRM_DMA_USE_SG))) {
		struct drm_local_map *map = dev->agp_buffer_map;

		if (map == NULL) {
			DRM_DEBUG("couldn't find agp buffer map\n");
			retcode = EINVAL;
			goto done;
		}
		size = round_page(map->size);
		foff = map->ext;
	} else {
		size = round_page(dma->byte_count),
		foff = 0;
	}

	vaddr = 0;
	retcode = uvm_mmap(&curproc->p_vmspace->vm_map, &vaddr, size,
	    UVM_PROT_READ | UVM_PROT_WRITE, UVM_PROT_ALL, MAP_SHARED,
	    (caddr_t)vn, foff, curproc->p_rlimit[RLIMIT_MEMLOCK].rlim_cur,
	    curproc);
	if (retcode) {
		DRM_DEBUG("uvm_mmap failed\n");
		goto done;
	}

	request->virtual = (void *)vaddr;

	for (i = 0; i < dma->buf_count; i++) {
		if (DRM_COPY_TO_USER(&request->list[i].idx,
		    &dma->buflist[i]->idx, sizeof(request->list[0].idx))) {
			retcode = EFAULT;
			goto done;
		}
		if (DRM_COPY_TO_USER(&request->list[i].total,
		    &dma->buflist[i]->total, sizeof(request->list[0].total))) {
			retcode = EFAULT;
			goto done;
		}
		if (DRM_COPY_TO_USER(&request->list[i].used, &zero,
		    sizeof(zero))) {
			retcode = EFAULT;
			goto done;
		}
		address = vaddr + dma->buflist[i]->offset; /* *** */
		if (DRM_COPY_TO_USER(&request->list[i].address, &address,
		    sizeof(address))) {
			retcode = EFAULT;
			goto done;
		}
	}

 done:
	request->count = dma->buf_count;

	DRM_DEBUG("%d buffers, retcode = %d\n", request->count, retcode);

	return retcode;
}
@


1.49
log
@New vmmap implementation.

no oks (it is really a pain to review properly)
extensively tested, I'm confident it'll be stable
'now is the time' from several icb inhabitants

Diff provides:
- ability to specify different allocators for different regions/maps
- a simpler implementation of the current allocator
- currently in compatibility mode: it will generate similar addresses
  as the old allocator
@
text
@d1 1
a1 1
/* $OpenBSD: drm_bufs.c,v 1.48 2011/06/02 18:22:00 weerd Exp $ */
@


1.48
log
@Add $OpenBSD$ after oga said 'go ahead and fix that'

'go for it' oga@@
@
text
@d1 1
a1 1
/* $OpenBSD$ */
d961 1
a961 1
	vaddr = uvm_map_hint(curproc, VM_PROT_READ | VM_PROT_WRITE);
@


1.47
log
@no need to poke the vmspace, uvm_map_hint does what we want. ok oga
@
text
@d1 1
@


1.46
log
@add a BRKSIZ define and use it for the heap gap constant, decoupling
heap gap from max data size.  nothing else changes yet.  ok deraadt
@
text
@a924 1
	struct vmspace		*vms;
a934 2
	vms = curproc->p_vmspace;

d960 2
a961 2
	vaddr = round_page((vaddr_t)vms->vm_daddr + BRKSIZ);
	retcode = uvm_mmap(&vms->vm_map, &vaddr, size,
@


1.45
log
@Drop an argument to DRM_ERROR() that was both unmatched and superfluous

ok oga@@
@
text
@d963 1
a963 1
	vaddr = round_page((vaddr_t)vms->vm_daddr + MAXDSIZ);
@


1.44
log
@Remove the three inline functions from drmP.h.

deiline drm_core_findmap(), and merge drm_core_ioremap{,free}() into
drm_ioremap{,free}() instead of having an inline that just calls another
function.
@
text
@d482 1
a482 1
			  curproc->p_pid, d->request_count, dma->buf_count);
@


1.43
log
@detypedef drm_local_map.
@
text
@d62 14
@


1.42
log
@detypedef drm_device_dma_t.
@
text
@d64 2
a65 1
    enum drm_map_type type, enum drm_map_flags flags, drm_local_map_t **map_ptr)
d67 2
a68 2
	drm_local_map_t *map;
	int align, ret = 0;
d70 2
a71 2
	struct drm_agp_mem *entry;
	int valid;
d251 1
a251 1
	drm_local_map_t		*map;
d275 1
a275 1
drm_rmmap(struct drm_device *dev, drm_local_map_t *map)
d284 1
a284 1
drm_rmmap_locked(struct drm_device *dev, drm_local_map_t *map)
d327 2
a328 2
	drm_local_map_t	*map;
	struct drm_map	*request = data;
d935 1
a935 1
		drm_local_map_t *map = dev->agp_buffer_map;
@


1.41
log
@detypedef drm_buf_entry_t too.
@
text
@d398 2
a399 2
	drm_device_dma_t *dma = dev->dma;
	int i;
d429 2
a430 2
	drm_device_dma_t *dma = dev->dma;
	int i;
d480 1
a480 1
	drm_device_dma_t	*dma = dev->dma;
d604 1
a604 1
	drm_device_dma_t	*dma = dev->dma;
d738 1
a738 1
	drm_device_dma_t	*dma = dev->dma;
d872 1
a872 1
	drm_device_dma_t	*dma = dev->dma;
d908 1
a909 1
	drm_device_dma_t	*dma = dev->dma;
@


1.40
log
@detypedef struct drm_buf from the dma_buffers api. Just because it's
dying as soon as I can make it doesn't mean I shouldn't be able to look
at it in the meantime.
@
text
@d373 1
a373 1
drm_cleanup_buf(struct drm_device *dev, drm_buf_entry_t *entry)
d481 1
a481 1
	drm_buf_entry_t		*entry;
d606 1
a606 1
	drm_buf_entry_t		*entry;
d739 1
a739 1
	drm_buf_entry_t		*entry;
@


1.39
log
@Push the per-driver dma hook a little further down.

All for all the drivers using the dma-bufs interface, their per-driver
ioctl hooks all started out the same way, followed by a call to another
function to actually select the buffer. Save some space by moving that
selection logic into the main dma_ioctl call, and make the second
function the hook.
@
text
@d416 1
a416 1
drm_free_buffer(struct drm_device *dev, drm_buf_t *buf)
d480 6
a485 14
	drm_device_dma_t *dma = dev->dma;
	drm_buf_entry_t *entry;
	drm_buf_t *buf;
	unsigned long offset;
	unsigned long agp_offset;
	int count;
	int order;
	int size;
	int alignment;
	int page_order;
	int total;
	int byte_count;
	int i;
	drm_buf_t **temp_buflist;
d487 2
a488 2
	struct drm_agp_mem *agp_entry;
	int valid;
d604 6
a609 15
	drm_device_dma_t *dma = dev->dma;
	int count;
	int order;
	int size;
	int total;
	int page_order;
	drm_buf_entry_t *entry;
	drm_buf_t *buf;
	int alignment;
	unsigned long offset;
	int i;
	int byte_count;
	int page_count;
	unsigned long *temp_pagelist;
	drm_buf_t **temp_buflist;
d738 6
a743 14
	drm_device_dma_t *dma = dev->dma;
	drm_buf_entry_t *entry;
	drm_buf_t *buf;
	unsigned long offset;
	unsigned long agp_offset;
	int count;
	int order;
	int size;
	int alignment;
	int page_order;
	int total;
	int byte_count;
	int i;
	drm_buf_t **temp_buflist;
d874 1
a874 1
	drm_buf_t		*buf;
d908 9
a916 12
	drm_device_dma_t *dma = dev->dma;
	struct vmspace *vms;
	struct vnode *vn;
	vaddr_t address;
	voff_t foff;
	vsize_t size;
	vaddr_t vaddr;
	int retcode = 0;
	const int zero = 0;

	struct drm_buf_map *request = data;
	int i;
@


1.38
log
@Rework the dma buffer api a bit to make it smaller and to have less
duplicated code. Also, switch the dma_lock to a rwlock (it never should
have been a spinlock) and move it and some other accounting data into
the dma structure, not the main softc.

Finally, the funcitons in drm_dma are tiny, move them in with the rest
of the dma_bufs api in drm_bufs and remove the file.
@
text
@d444 3
d448 1
a448 3
	if (dev->driver->dma_ioctl != NULL) {
		return (dev->driver->dma_ioctl(dev, data, file_priv));
	} else {
d450 1
a450 1
		return EINVAL;
d452 23
@


1.37
log
@Remove a bunch of compat macros, just expand them to the openbsd
equivalent.
@
text
@d40 3
a42 3
int	drm_do_addbufs_agp(struct drm_device *, drm_buf_desc_t *);
int	drm_do_addbufs_pci(struct drm_device *, drm_buf_desc_t *);
int	drm_do_addbufs_sg(struct drm_device *, drm_buf_desc_t *);
d349 54
d404 48
d454 1
a454 1
drm_do_addbufs_agp(struct drm_device *dev, struct drm_buf_desc *request)
d586 1
a586 1
drm_do_addbufs_pci(struct drm_device *dev, struct drm_buf_desc *request)
d729 1
a729 1
drm_do_addbufs_sg(struct drm_device *dev, struct drm_buf_desc *request)
d833 1
a833 34
drm_addbufs_agp(struct drm_device *dev, struct drm_buf_desc *request)
{
	int order, ret;


	if (request->count < 0 || request->count > 4096)
		return EINVAL;
	
	order = drm_order(request->size);
	if (order < DRM_MIN_ORDER || order > DRM_MAX_ORDER)
		return EINVAL;

	DRM_SPINLOCK(&dev->dma_lock);

	/* No more allocations after first buffer-using ioctl. */
	if (dev->buf_use != 0) {
		DRM_SPINUNLOCK(&dev->dma_lock);
		return EBUSY;
	}
	/* No more than one allocation per order */
	if (dev->dma->bufs[order].buf_count != 0) {
		DRM_SPINUNLOCK(&dev->dma_lock);
		return ENOMEM;
	}

	ret = drm_do_addbufs_agp(dev, request);

	DRM_SPINUNLOCK(&dev->dma_lock);

	return ret;
}

int
drm_addbufs_sg(struct drm_device *dev, struct drm_buf_desc *request)
d835 2
a836 2
	int order, ret;

d839 1
a839 1
		return EINVAL;
d843 1
a843 16
		return EINVAL;

	DRM_SPINLOCK(&dev->dma_lock);

	/* No more allocations after first buffer-using ioctl. */
	if (dev->buf_use != 0) {
		DRM_SPINUNLOCK(&dev->dma_lock);
		return EBUSY;
	}
	/* No more than one allocation per order */
	if (dev->dma->bufs[order].buf_count != 0) {
		DRM_SPINUNLOCK(&dev->dma_lock);
		return ENOMEM;
	}

	ret = drm_do_addbufs_sg(dev, request);
d845 1
a845 18
	DRM_SPINUNLOCK(&dev->dma_lock);

	return ret;
}

int
drm_addbufs_pci(struct drm_device *dev, struct drm_buf_desc *request)
{
	int order, ret;

	if (request->count < 0 || request->count > 4096)
		return EINVAL;
	
	order = drm_order(request->size);
	if (order < DRM_MIN_ORDER || order > DRM_MAX_ORDER)
		return EINVAL;

	DRM_SPINLOCK(&dev->dma_lock);
d848 3
a850 3
	if (dev->buf_use != 0) {
		DRM_SPINUNLOCK(&dev->dma_lock);
		return EBUSY;
d853 3
a855 3
	if (dev->dma->bufs[order].buf_count != 0) {
		DRM_SPINUNLOCK(&dev->dma_lock);
		return ENOMEM;
a857 14
	ret = drm_do_addbufs_pci(dev, request);

	DRM_SPINUNLOCK(&dev->dma_lock);

	return ret;
}

int
drm_addbufs_ioctl(struct drm_device *dev, void *data,
    struct drm_file *file_priv)
{
	struct drm_buf_desc *request = data;
	int err;

d859 1
a859 1
		err = drm_addbufs_agp(dev, request);
d861 1
a861 1
		err = drm_addbufs_sg(dev, request);
d863 3
a865 1
		err = drm_addbufs_pci(dev, request);
d867 1
a867 1
	return err;
d880 1
a880 1
	DRM_SPINLOCK(&dev->dma_lock);
d901 1
a901 1
	DRM_SPINUNLOCK(&dev->dma_lock);
d927 3
a929 3
	DRM_SPINLOCK(&dev->dma_lock);
	dev->buf_use++;		/* Can't allocate more after this call */
	DRM_SPINUNLOCK(&dev->dma_lock);
@


1.36
log
@The drm_* allocation functions have been #defined to not use most of
their arguments for a while. Actually go through the code and remove the
extraneous arguments. Makes things easier to read.
@
text
@d900 1
a900 1
	vms = DRM_CURPROC->p_vmspace;
d930 2
a931 1
	    (caddr_t)vn, foff, DRM_CURPROC->p_rlimit[RLIMIT_MEMLOCK].rlim_cur,DRM_CURPROC);
@


1.35
log
@Minor style nit; rnoland (freebsd).
@
text
@d116 1
a116 1
	map = drm_calloc(1, sizeof(*map), DRM_MEM_MAPS);
d134 1
a134 1
		drm_free(map, sizeof(*map), DRM_MEM_MAPS);
d181 1
a181 1
			drm_free(map, sizeof(*map), DRM_MEM_MAPS);
d190 1
a190 1
			drm_free(map, sizeof(*map), DRM_MEM_MAPS);
d209 1
a209 1
			drm_free(map, sizeof(*map), DRM_MEM_MAPS);
d220 1
a220 1
				drm_free(map, sizeof(*map), DRM_MEM_MAPS);
d229 1
a229 1
		drm_free(map, sizeof(*map), DRM_MEM_MAPS);
d316 1
a316 1
	drm_free(map, sizeof(*map), DRM_MEM_MAPS);
d420 1
a420 2
	entry->buflist = drm_calloc(count, sizeof(*entry->buflist),
	    DRM_MEM_BUFS);
d440 1
a440 2
		buf->dev_private = drm_calloc(1, dev->driver->buf_priv_size,
		    DRM_MEM_BUFS);
d458 1
a458 1
	    (dma->buf_count + entry->buf_count) * sizeof(*dma->buflist), M_DRM);
d516 2
a517 4
	entry->buflist = drm_calloc(count, sizeof(*entry->buflist),
	    DRM_MEM_BUFS);
	entry->seglist = drm_calloc(count, sizeof(*entry->seglist),
	    DRM_MEM_BUFS);
d523 1
a523 1
	    sizeof(*dma->pagelist), DRM_MEM_BUFS);
d527 3
a529 6
		drm_free(temp_pagelist, (dma->page_count + (count <<
		    page_order)) * sizeof(*dma->pagelist), DRM_MEM_BUFS);
		drm_free(entry->seglist, count * sizeof(*entry->seglist),
		    DRM_MEM_BUFS);
		drm_free(entry->buflist, count * sizeof(*entry->buflist),
		    DRM_MEM_BUFS);
d552 1
a552 3
			drm_free(temp_pagelist, (dma->page_count +
			   (count << page_order)) * sizeof(*dma->pagelist),
			   DRM_MEM_BUFS);
d578 1
a578 1
			    dev->driver->buf_priv_size, DRM_MEM_BUFS);
d584 1
a584 4
				drm_free(temp_pagelist, (dma->page_count +
				    (count << page_order)) *
				    sizeof(*dma->pagelist),
				    DRM_MEM_BUFS);
d596 1
a596 1
	    (dma->buf_count + entry->buf_count) * sizeof(*dma->buflist), M_DRM);
d600 1
a600 3
		drm_free(temp_pagelist, (dma->page_count +
		    (count << page_order)) * sizeof(*dma->pagelist),
		    DRM_MEM_BUFS);
d611 1
a611 2
	drm_free(dma->pagelist, dma->page_count * sizeof(*dma->pagelist),
	    DRM_MEM_BUFS);
d666 1
a666 2
	entry->buflist = drm_calloc(count, sizeof(*entry->buflist),
	    DRM_MEM_BUFS);
d686 1
a686 2
		buf->dev_private = drm_calloc(1, dev->driver->buf_priv_size,
		    DRM_MEM_BUFS);
d705 1
a705 1
	    (dma->buf_count + entry->buf_count) * sizeof(*dma->buflist), M_DRM);
@


1.34
log
@Convert CONSISTENT maps over to dmamem api.
@
text
@d193 1
a193 1
		map->offset = map->offset + dev->sg->handle;
@


1.33
log
@convert drm_addbufs_pci over to new dmamem api.
@
text
@d197 5
a201 5
		/* Unfortunately, we don't get any alignment specification from
		 * the caller, so we have to guess.  drm_pci_alloc requires
		 * a power-of-two alignment, so try to align the bus address of
		 * the map to it size if possible, otherwise just assume
		 * PAGE_SIZE alignment.
d206 3
a208 3
		map->dmah = drm_pci_alloc(dev->dmat, map->size, align,
		    0xfffffffful);
		if (map->dmah == NULL) {
d212 2
a213 2
		map->handle = map->dmah->vaddr;
		map->offset = map->dmah->busaddr;
d219 1
a219 1
				drm_pci_free(dev->dmat, map->dmah);
d306 1
a306 1
		drm_pci_free(dev->dmat, map->dmah);
@


1.32
log
@back out the buf_priv change. it made some incorrect assumptions and
broke radeondrm. Fixing it is ugly, so another change will have to be made
later.


I /hate/ the drm_buf api, it will die as soon as it can.
@
text
@d552 3
a554 3
		drm_dma_handle_t *dmah = drm_pci_alloc(dev->dmat, size,
		    alignment, 0xfffffffful);
		if (dmah == NULL) {
d565 1
a565 1
		entry->seglist[entry->seg_count++] = dmah;
d567 2
a568 3
			DRM_DEBUG("page %d @@ %p\n",
			    dma->page_count + page_count,
			    (char *)dmah->vaddr + PAGE_SIZE * i);
d570 1
a570 1
			    (long)dmah->vaddr + PAGE_SIZE * i;
d580 3
a582 2
			buf->address = dmah->vaddr + offset;
			buf->bus_address = dmah->busaddr + offset;
@


1.31
log
@Don't unmap REGISTERS maps, we don't map them anymore.
@
text
@d420 1
a420 4
	if (dev->driver->buf_priv_size == 0)
		return (ENOMEM);

	entry->buflist = drm_calloc(count, dev->driver->buf_priv_size,
d441 9
d518 1
a518 4
	if (dev->driver->buf_priv_size == 0)
		return (EINVAL);

	entry->buflist = drm_calloc(count, dev->driver->buf_priv_size,
d586 14
d681 1
a681 4
	if (dev->driver->buf_priv_size == 0)
		return (ENOMEM);

	entry->buflist = drm_calloc(count, dev->driver->buf_priv_size,
d701 9
@


1.30
log
@Instead of having a ``private data'' pointer in the dma buffers, just
ask the driver how large they need the structure we allocate to be, and
use inheritance like we do for struct device. Simplifies things a little
bit and saves us a pointer.
@
text
@a140 3
		map->handle = drm_ioremap(dev, map);
		if (map->handle == NULL)
			return (EINVAL);
@


1.29
log
@pass in the dmat when we attach the drm driver. reduces pci dependancy.
@
text
@d423 4
a426 1
	entry->buflist = drm_calloc(count, sizeof(*entry->buflist),
a446 9
		buf->dev_private = drm_calloc(1, dev->driver->buf_priv_size,
		    DRM_MEM_BUFS);
		if (buf->dev_private == NULL) {
			/* Set count correctly so we free the proper amount. */
			entry->buf_count = count;
			drm_cleanup_buf(dev, entry);
			return ENOMEM;
		}

d515 4
a518 1
	entry->buflist = drm_calloc(count, sizeof(*entry->buflist),
a585 14
			buf->dev_private = drm_calloc(1,
			    dev->driver->buf_priv_size, DRM_MEM_BUFS);
			if (buf->dev_private == NULL) {
				/* Set count so we free the proper amount. */
				entry->buf_count = count;
				entry->seg_count = count;
				drm_cleanup_buf(dev, entry);
				drm_free(temp_pagelist, (dma->page_count +
				    (count << page_order)) *
				    sizeof(*dma->pagelist),
				    DRM_MEM_BUFS);
				return ENOMEM;
			}

d667 4
a670 1
	entry->buflist = drm_calloc(count, sizeof(*entry->buflist),
a689 9

		buf->dev_private = drm_calloc(1, dev->driver->buf_priv_size,
		    DRM_MEM_BUFS);
		if (buf->dev_private == NULL) {
			/* Set count correctly so we free the proper amount. */
			entry->buf_count = count;
			drm_cleanup_buf(dev, entry);
			return ENOMEM;
		}
@


1.28
log
@Map device interrupts in the attach routine. and remove more need for
dev->pa by doing the pci_intr_establish/disestablish dance in the driver
function, not in drm. This removes the need for
interrupt_{pre,post}install callbacks, instead just provide a
interrupt_install() callback.
@
text
@d209 1
a209 1
		map->dmah = drm_pci_alloc(dev->pa.pa_dmat, map->size, align,
d222 1
a222 1
				drm_pci_free(dev->pa.pa_dmat, map->dmah);
d309 1
a309 1
		drm_pci_free(dev->pa.pa_dmat, map->dmah);
d555 1
a555 1
		drm_dma_handle_t *dmah = drm_pci_alloc(dev->pa.pa_dmat, size,
@


1.27
log
@Make all drm drivers map their mmio register space on attach instead of
using the drm_maps interface (this was done for inteldrm a few days
ago).  All drivers now ignore the mmio_offset argument that the init
ioctl takes.

This clears up the code and makes sure that drm_ioremap_core() doesn't
need the vga_pci_map inteface anymore, so we don't have to pass in the
vga softc anymore. We also get to kill the drm_resource_{start,length}
linux-alike functions since we just calculate all the requisite offsets
at startup and cache those we need. This now means that technically the
only driver that needs the vga_pci_map api is inteldrm (due to sharing
with intagp issues), though this diff doesn't convert them over.
@
text
@a291 1
		drm_ioremapfree(map);
d303 1
d307 1
@


1.26
log
@reduce the dependancy of drm_pci_alloc upon the drm device softc. Just
pass in the dma tag
@
text
@a39 1
int	drm_alloc_resource(struct drm_device *, int);
a59 43
}

/* Allocation of PCI memory resources (framebuffer, registers, etc.) for
 * drm_get_resource_*.  Note that they are not RF_ACTIVE, so there's no virtual
 * address for accessing them.  Cleaned up at unload.
 */
int
drm_alloc_resource(struct drm_device *dev, int resource)
{
	if (resource >= DRM_MAX_PCI_RESOURCE) {
		DRM_ERROR("Resource %d too large\n", resource);
		return 1;
	}

	if (dev->pcir[resource] != NULL)
		return 0;

	dev->pcir[resource] = vga_pci_bar_info(dev->vga_softc, resource);
	if (dev->pcir[resource] == NULL) {
		DRM_ERROR("Can't get bar info for resource 0x%x\n", resource);
		return 1;
	}

	return 0;
}


unsigned long
drm_get_resource_start(struct drm_device *dev, unsigned int resource)
{
	if (drm_alloc_resource(dev, resource) != 0)
		return 0;

	return dev->pcir[resource]->base;
}

unsigned long
drm_get_resource_len(struct drm_device *dev, unsigned int resource)
{
	if (drm_alloc_resource(dev, resource) != 0)
		return 0;

	return dev->pcir[resource]->maxsize;
@


1.25
log
@Instead of using a width-1 bitfield for storing the driver capabilities
in the callback structure, just use a bunch of ORed together bits. This
has been annoying me for ages.
@
text
@d253 2
a254 1
		map->dmah = drm_pci_alloc(dev, map->size, align, 0xfffffffful);
d266 1
a266 1
				drm_pci_free(dev, map->dmah);
d352 1
a352 1
		drm_pci_free(dev, map->dmah);
d598 2
a599 2
		drm_dma_handle_t *dmah = drm_pci_alloc(dev, size, alignment,
		    0xfffffffful);
@


1.24
log
@Move dev->driver over to being a pointer to a const struct, instead of stupidly
filling in a pre-allocated one on each attach.

Makes the code a bunch nicer, shrinks a kernel by about 1.5k on amd64,
helps with my sanity, and paves way for later changes.

Tested by a few for a couple of weeks now.
@
text
@d971 4
a974 2
	if ((dev->driver->use_agp && (dma->flags & _DRM_DMA_USE_AGP)) ||
	    (dev->driver->use_sg && (dma->flags & _DRM_DMA_USE_SG))) {
@


1.23
log
@Put back the ``address'' field to struct drm_buf, this allows machdrm to
actually compile again (no one's tested it yet, still and I don't have
one).
@
text
@d486 1
a486 1
		buf->dev_private = drm_calloc(1, dev->driver.buf_priv_size,
d632 1
a632 1
			    dev->driver.buf_priv_size, DRM_MEM_BUFS);
d747 1
a747 1
		buf->dev_private = drm_calloc(1, dev->driver.buf_priv_size,
d971 2
a972 2
	if ((dev->driver.use_agp && (dma->flags & _DRM_DMA_USE_AGP)) ||
	    (dev->driver.use_sg && (dma->flags & _DRM_DMA_USE_SG))) {
@


1.22
log
@Rework the drm locking to be at least halfway sane. The freebsd code
held a lock over all driver ioctls in order to be ``mpsafe''. Stop lying
to ourselves for a start. This code is not fully mpsafe, and should not
pretend to be so.  Put the locking around where it should, and rely on
biglock for the rest. This will need to be fixed, but avoids some of the
horrible that we have right now.

Tested by many over a long time and several iterations.
@
text
@d626 1
@


1.21
log
@Kill some more unused struct fields and the cases for them.
@
text
@d75 1
a75 3
	DRM_UNLOCK();
	if (dev->pcir[resource] != NULL) {
		DRM_LOCK();
a76 1
	}
a78 1
	DRM_LOCK();
d143 1
d179 1
a179 2
		DRM_LOCK();
		return ret;
d186 2
a187 4
		if (map->handle == NULL) {
			DRM_LOCK();
			return EINVAL;
		}
d214 1
d227 1
a228 1
			DRM_LOCK();
d230 1
a230 1
			return EACCES;
d232 1
d238 1
a238 2
			DRM_LOCK();
			return EINVAL;
d256 1
a256 2
			DRM_LOCK();
			return ENOMEM;
d267 1
a267 2
				DRM_LOCK();
				return EBUSY;
a275 1
		DRM_LOCK();
a280 1

d282 1
a282 1
	/* Jumped to, with lock held, when a kernel map is found. */
a301 1
	DRM_LOCK();
a303 1
	DRM_UNLOCK();
d322 4
a325 1
	DRM_SPINLOCK_ASSERT(&dev->dev_lock);
d327 4
a360 1

d384 1
a384 1
		return EINVAL;
d387 1
a387 1
	drm_rmmap(dev, map);
d446 1
d457 2
a458 1
		return EINVAL;
d460 1
@


1.20
log
@The code for cleaning up errored buffers and for cleaning up at the end
is the same. Factor them into one function. Saves another 450 bytes on
amd64.
@
text
@a480 1
		buf->order = order;
a484 1
		buf->address = (void *)(agp_offset + offset);
d488 1
a488 2
		buf->dev_priv_size = dev->driver.buf_priv_size;
		buf->dev_private = drm_calloc(1, buf->dev_priv_size,
a625 1
			buf->order = order;
a627 1
			buf->address = ((char *)dmah->vaddr + offset);
d632 2
a633 3
			buf->dev_priv_size = dev->driver.buf_priv_size;
			buf->dev_private = drm_calloc(1, buf->dev_priv_size,
			    DRM_MEM_BUFS);
d646 2
a647 2
			DRM_DEBUG("buffer %d @@ %p\n",
			    entry->buf_count, buf->address);
a740 1
		buf->order = order;
a744 1
		buf->address = (void *)(agp_offset + offset + dev->sg->handle);
d748 1
a748 2
		buf->dev_priv_size = dev->driver.buf_priv_size;
		buf->dev_private = drm_calloc(1, buf->dev_priv_size,
d757 1
a757 1
		DRM_DEBUG("buffer %d @@ %p\n", entry->buf_count, buf->address);
@


1.19
log
@detypedef some more. No functional change.
@
text
@a40 1
void	drm_cleanup_buf_error(struct drm_device *, drm_buf_entry_t *);
a399 25
void
drm_cleanup_buf_error(struct drm_device *dev, drm_buf_entry_t *entry)
{
	int i;

	if (entry->seg_count) {
		for (i = 0; i < entry->seg_count; i++)
			drm_pci_free(dev, entry->seglist[i]);
		drm_free(entry->seglist, entry->seg_count *
		    sizeof(*entry->seglist), DRM_MEM_BUFS);

		entry->seg_count = 0;
	}

   	if (entry->buf_count) {
	   	for (i = 0; i < entry->buf_count; i++) {
			drm_free(entry->buflist[i].dev_private,
			    entry->buflist[i].dev_priv_size, DRM_MEM_BUFS);
		}
		drm_free(entry->buflist, entry->buf_count *
		    sizeof(*entry->buflist), DRM_MEM_BUFS);

		entry->buf_count = 0;
	}
}
d496 1
a496 1
			drm_cleanup_buf_error(dev, entry);
d513 1
a513 1
		drm_cleanup_buf_error(dev, entry);
d608 1
a608 1
			drm_cleanup_buf_error(dev, entry);
d644 1
a644 1
				drm_cleanup_buf_error(dev, entry);
d663 1
a663 1
		drm_cleanup_buf_error(dev, entry);
d762 1
a762 1
			drm_cleanup_buf_error(dev, entry);
d780 1
a780 1
		drm_cleanup_buf_error(dev, entry);
@


1.18
log
@replace usage of drm_memrange with extent(9). No functional change, but
should shrink the kernel somewhat. For some strange reason I was unaware
of this api when I pulled in these changes.

tested by myself and Paul de Weerd, thanks!
@
text
@d113 1
a113 1
    drm_map_type_t type, drm_map_flags_t flags, drm_local_map_t **map_ptr)
d305 3
a307 3
	drm_map_t *request = data;
	drm_local_map_t *map;
	int err;
d377 2
a378 2
	drm_local_map_t *map;
	drm_map_t *request = data;
d428 1
a428 1
drm_do_addbufs_agp(struct drm_device *dev, drm_buf_desc_t *request)
d562 1
a562 1
drm_do_addbufs_pci(struct drm_device *dev, drm_buf_desc_t *request)
d720 1
a720 1
drm_do_addbufs_sg(struct drm_device *dev, drm_buf_desc_t *request)
d829 1
a829 1
drm_addbufs_agp(struct drm_device *dev, drm_buf_desc_t *request)
d862 1
a862 1
drm_addbufs_sg(struct drm_device *dev, drm_buf_desc_t *request)
d895 1
a895 1
drm_addbufs_pci(struct drm_device *dev, drm_buf_desc_t *request)
d930 1
a930 1
	drm_buf_desc_t *request = data;
d946 4
a949 6
	drm_device_dma_t *dma = dev->dma;
	drm_buf_free_t *request = data;
	int i;
	int idx;
	drm_buf_t *buf;
	int retcode = 0;
d992 1
a992 1
	drm_buf_map_t *request = data;
@


1.17
log
@Another dead struct member.
@
text
@d116 1
a116 1
	int align;
a153 1
				map->size = size;
d176 4
a179 3
	map->mm = drm_memrange_search_free(&dev->handle_mm, map->size,
	    PAGE_SIZE, 0);
	if (map->mm == NULL) {
d181 1
d183 2
a184 8
		return ENOMEM;
	}
	map->mm = drm_memrange_get_block(map->mm, map->size,
	    PAGE_SIZE); 
	if (map->mm == NULL) {
		DRM_ERROR("can't get block\n");
		drm_free(map, sizeof(*map), DRM_MEM_MAPS);
		return ENOMEM;
d186 1
d326 1
a326 1
	request->handle = (void *)map->mm->start;
d363 3
a365 1
	drm_memrange_put_block(map->mm);
d1019 1
a1019 1
		foff = map->mm->start;
@


1.16
log
@Kill the infobufs and markbufs calls. Nothing uses them.
@
text
@a514 1
		buf->next = NULL;
a661 1
			buf->next = NULL;
a780 1
		buf->next = NULL;
@


1.15
log
@Kill some redundant ifdefs, they're taken care of elsewhere.
@
text
@a948 82
drm_infobufs(struct drm_device *dev, void *data, struct drm_file *file_priv)
{
	drm_device_dma_t *dma = dev->dma;
	drm_buf_info_t *request = data;
	int i;
	int count;
	int retcode = 0;

	DRM_SPINLOCK(&dev->dma_lock);
	++dev->buf_use;		/* Can't allocate more after this call */
	DRM_SPINUNLOCK(&dev->dma_lock);

	for (i = 0, count = 0; i < DRM_MAX_ORDER + 1; i++) {
		if (dma->bufs[i].buf_count)
		    ++count;
	}

	DRM_DEBUG("count = %d\n", count);

	if (request->count >= count) {
		for (i = 0, count = 0; i < DRM_MAX_ORDER + 1; i++) {
			if (dma->bufs[i].buf_count) {
				drm_buf_desc_t from;

				from.count = dma->bufs[i].buf_count;
				from.size = dma->bufs[i].buf_size;
				from.low_mark = dma->bufs[i].freelist.low_mark;
				from.high_mark =
				    dma->bufs[i].freelist.high_mark;

				if (DRM_COPY_TO_USER(&request->list[count],
				    &from, sizeof(drm_buf_desc_t)) != 0) {
					retcode = EFAULT;
					break;
				}

				DRM_DEBUG("%d %d %d %d %d\n", i,
				    dma->bufs[i].buf_count,
				    dma->bufs[i].buf_size,
				    dma->bufs[i].freelist.low_mark,
				    dma->bufs[i].freelist.high_mark);
				++count;
			}
		}
	}
	request->count = count;

	return retcode;
}

int
drm_markbufs(struct drm_device *dev, void *data, struct drm_file *file_priv)
{
	drm_device_dma_t *dma = dev->dma;
	drm_buf_desc_t *request = data;
	int order;

	DRM_DEBUG("%d, %d, %d\n", request->size, request->low_mark,
	    request->high_mark);
	

	order = drm_order(request->size);	
	if (order < DRM_MIN_ORDER || order > DRM_MAX_ORDER ||
	    request->low_mark < 0 || request->high_mark < 0) {
		return EINVAL;
	}

	DRM_SPINLOCK(&dev->dma_lock);
	if (request->low_mark > dma->bufs[order].buf_count ||
	    request->high_mark > dma->bufs[order].buf_count) {
		DRM_SPINUNLOCK(&dev->dma_lock);
		return EINVAL;
	}

	dma->bufs[order].freelist.low_mark  = request->low_mark;
	dma->bufs[order].freelist.high_mark = request->high_mark;
	DRM_SPINUNLOCK(&dev->dma_lock);

	return 0;
}

int
@


1.14
log
@Apply a light paddling with the knf stick. No binary change.
@
text
@a202 1
#ifndef DRM_NO_MTRR
a204 1
#endif
a346 1
#ifndef DRM_NO_MTRR
a353 1
#endif
@


1.13
log
@Kill file_priv->refs, it is always 1. Also  move two things from the
softc into file_priv since otherwise the wrong value could rarely be
used.
@
text
@d55 2
a56 1
	for ( order = 0, tmp = size ; tmp >>= 1 ; ++order );
d58 1
a58 1
	if ( size & ~(1 << order) )
d144 2
a145 1
	/* Check if this is just another version of a kernel-allocated map, and
d166 1
a166 1
	if ( !map ) {
d192 1
a192 1
	switch ( map->type ) {
d195 1
a195 1
		if (!map->handle) {
d247 1
a247 1
		if (!dev->sg) {
d273 1
a273 2
		if (map->type == _DRM_SHM &&
		    map->flags & _DRM_CONTAINS_LOCK) {
d329 1
a329 1
	request->mtrr   = map->mtrr;
d413 1
a413 1
		for (i = 0; i < entry->seg_count; i++) {
a414 1
		}
d467 7
a473 7
	DRM_DEBUG( "count:      %d\n",  count );
	DRM_DEBUG( "order:      %d\n",  order );
	DRM_DEBUG( "size:       %d\n",  size );
	DRM_DEBUG( "agp_offset: 0x%lx\n", agp_offset );
	DRM_DEBUG( "alignment:  %d\n",  alignment );
	DRM_DEBUG( "page_order: %d\n",  page_order );
	DRM_DEBUG( "total:      %d\n",  total );
d501 1
a501 1
	if ( !entry->buflist ) {
a502 1
	}
d509 1
a509 1
	while ( entry->buf_count < count ) {
d538 1
a538 1
	DRM_DEBUG( "byte_count: %d\n", byte_count );
d551 1
a551 1
	for ( i = 0 ; i < entry->buf_count ; i++ ) {
a552 1
	}
d557 2
a558 2
	DRM_DEBUG( "dma->buf_count : %d\n", dma->buf_count );
	DRM_DEBUG( "entry->buf_count : %d\n", entry->buf_count );
d591 2
a592 2
	DRM_DEBUG( "count=%d, size=%d (%d), order=%d\n",
	    request->count, request->size, size, order );
d626 2
a627 2
	DRM_DEBUG( "pagelist: %d entries\n",
	    dma->page_count + (count << page_order) );
d634 1
a634 1
	while ( entry->buf_count < count ) {
d649 2
a650 2
		for ( i = 0 ; i < (1 << page_order) ; i++ ) {
			DRM_DEBUG( "page %d @@ %p\n",
d652 1
a652 1
			    (char *)dmah->vaddr + PAGE_SIZE * i );
d656 9
a664 9
		for ( offset = 0 ;
		    offset + size <= total && entry->buf_count < count ;
		    offset += alignment, ++entry->buf_count ) {
			buf	     = &entry->buflist[entry->buf_count];
			buf->idx     = dma->buf_count + entry->buf_count;
			buf->total   = alignment;
			buf->order   = order;
			buf->used    = 0;
			buf->offset  = (dma->byte_count + byte_count + offset);
d667 1
a667 1
			buf->next    = NULL;
d669 1
a669 1
			buf->file_priv    = NULL;
d675 1
a675 1
				/* Set count correctly so we free the proper amount. */
d686 2
a687 2
			DRM_DEBUG( "buffer %d @@ %p\n",
			    entry->buf_count, buf->address );
d692 1
a692 1
	temp_buflist = drm_realloc( dma->buflist,
d705 1
a705 1
	for ( i = 0 ; i < entry->buf_count ; i++ ) {
a706 1
	}
d757 7
a763 7
	DRM_DEBUG( "count:      %d\n",  count );
	DRM_DEBUG( "order:      %d\n",  order );
	DRM_DEBUG( "size:       %d\n",  size );
	DRM_DEBUG( "agp_offset: %ld\n", agp_offset );
	DRM_DEBUG( "alignment:  %d\n",  alignment );
	DRM_DEBUG( "page_order: %d\n",  page_order );
	DRM_DEBUG( "total:      %d\n",  total );
d777 1
a777 1
	while ( entry->buf_count < count ) {
d801 1
a801 2
		DRM_DEBUG( "buffer %d @@ %p\n",
		    entry->buf_count, buf->address );
d808 1
a808 1
	DRM_DEBUG( "byte_count: %d\n", byte_count );
d820 1
a820 1
	for ( i = 0 ; i < entry->buf_count ; i++ ) {
a821 1
	}
d826 2
a827 2
	DRM_DEBUG( "dma->buf_count : %d\n", dma->buf_count );
	DRM_DEBUG( "entry->buf_count : %d\n", entry->buf_count );
d965 3
a967 2
	for ( i = 0, count = 0 ; i < DRM_MAX_ORDER + 1 ; i++ ) {
		if ( dma->bufs[i].buf_count ) ++count;
d970 1
a970 1
	DRM_DEBUG( "count = %d\n", count );
d972 3
a974 3
	if ( request->count >= count ) {
		for ( i = 0, count = 0 ; i < DRM_MAX_ORDER + 1 ; i++ ) {
			if ( dma->bufs[i].buf_count ) {
d980 2
a981 1
				from.high_mark = dma->bufs[i].freelist.high_mark;
d983 2
a984 2
				if (DRM_COPY_TO_USER(&request->list[count], &from,
				    sizeof(drm_buf_desc_t)) != 0) {
d989 1
a989 2
				DRM_DEBUG( "%d %d %d %d %d\n",
				    i,
d993 1
a993 1
				    dma->bufs[i].freelist.high_mark );
d1010 2
a1011 2
	DRM_DEBUG( "%d, %d, %d\n",
	    request->size, request->low_mark, request->high_mark );
d1044 1
a1044 1
	DRM_DEBUG( "%d\n", request->count );
d1047 1
a1047 1
	for ( i = 0 ; i < request->count ; i++ ) {
d1052 3
a1054 3
		if ( idx < 0 || idx >= dma->buf_count ) {
			DRM_ERROR( "Index %d (of %d max)\n",
			    idx, dma->buf_count - 1 );
d1059 1
a1059 1
		if ( buf->file_priv != file_priv ) {
d1127 1
a1127 1
	for ( i = 0 ; i < dma->buf_count ; i++ ) {
d1154 1
a1154 1
	DRM_DEBUG( "%d buffers, retcode = %d\n", request->count, retcode );
@


1.12
log
@Switch all instances of malloc/free in the DRM to drm_alloc, drm_free
and drm_calloc.

With the recent change to check overflow in drm_calloc, this means that
all allocations that require multiplication are now checked. Also use
drm_calloc() when zeroing is needed and drop the bzero/memset
afterwards.  Finally, make drm_free() check for NULL, so we don't need
to do so every time.

ok miod@@, deraadt@@
@
text
@d314 1
a314 1
	if (!(dev->flags & (FREAD|FWRITE)))
d1092 1
a1092 1
	if (!vfinddev(dev->kdev, VCHR, &vn))
@


1.11
log
@Kill the silly ``drm_device_t'' and ``drm_file_t'' typedefs. just use
``struct drm_device'' and ``struct drm_file'' respectively. Since i'm
changing a lot of prototypes anyway, remove all parameter names from
prototypes, in accordance with style(9) (and sanity).
@
text
@d163 1
a163 1
	map = malloc(sizeof(*map), M_DRM, M_ZERO | M_NOWAIT);
d179 1
a179 1
		free(map, M_DRM);
d186 1
a186 1
		free(map, M_DRM);
d237 1
a237 1
			free(map, M_DRM);
d246 1
a246 1
			free(map, M_DRM);
d265 1
a265 1
			free(map, M_DRM);
d278 1
a278 1
				free(map, M_DRM);
d288 1
a288 1
		free(map, M_DRM);
d372 1
a372 1
	free(map, M_DRM);
d415 2
a416 1
		free(entry->seglist, M_DRM);
d423 2
a424 1
			free(entry->buflist[i].dev_private, M_DRM);
d426 2
a427 1
		free(entry->buflist, M_DRM);
d499 2
a500 2
	entry->buflist = malloc(count * sizeof(*entry->buflist), M_DRM,
	    M_NOWAIT | M_ZERO);
d525 2
a526 2
		buf->dev_private = malloc(buf->dev_priv_size, M_DRM,
		    M_NOWAIT | M_ZERO);
d603 4
a606 4
	entry->buflist = malloc(count * sizeof(*entry->buflist), M_DRM,
	    M_NOWAIT | M_ZERO);
	entry->seglist = malloc(count * sizeof(*entry->seglist), M_DRM,
	    M_NOWAIT | M_ZERO);
d611 2
a612 2
	temp_pagelist = malloc((dma->page_count + (count << page_order)) *
	    sizeof(*dma->pagelist), M_DRM, M_NOWAIT);
d616 6
a621 6
		if (temp_pagelist)
			free(temp_pagelist, M_DRM);
		if (entry->seglist)
			free(entry->seglist, M_DRM);
		if (entry->buflist)
			free(entry->buflist, M_DRM);
d644 3
a646 1
			free(temp_pagelist, M_DRM);
d674 2
a675 2
			buf->dev_private = malloc(buf->dev_priv_size, M_DRM,
			    M_NOWAIT | M_ZERO);
d681 4
a684 1
				free(temp_pagelist, M_DRM);
d700 3
a702 1
		free(temp_pagelist, M_DRM);
d714 2
a715 1
	free(dma->pagelist, M_DRM);
d770 2
a771 2
	entry->buflist = malloc(count * sizeof(*entry->buflist), M_DRM,
	    M_NOWAIT | M_ZERO);
d795 2
a796 2
		buf->dev_private = malloc(buf->dev_priv_size, M_DRM,
		    M_NOWAIT | M_ZERO);
@


1.10
log
@Kill the device_t and vm_offset_t typedefs.
@
text
@d40 5
a44 5
int	drm_alloc_resource(drm_device_t *, int);
void	drm_cleanup_buf_error(drm_device_t *, drm_buf_entry_t *);
int	drm_do_addbufs_agp(drm_device_t *, drm_buf_desc_t *);
int	drm_do_addbufs_pci(drm_device_t *, drm_buf_desc_t *);
int	drm_do_addbufs_sg(drm_device_t *, drm_buf_desc_t *);
d68 1
a68 1
drm_alloc_resource(drm_device_t *dev, int resource)
d93 1
a93 1
drm_get_resource_start(drm_device_t *dev, unsigned int resource)
d102 1
a102 1
drm_get_resource_len(drm_device_t *dev, unsigned int resource)
d111 1
a111 1
drm_addmap(drm_device_t * dev, unsigned long offset, unsigned long size,
d308 1
a308 1
drm_addmap_ioctl(drm_device_t *dev, void *data, struct drm_file *file_priv)
d337 1
a337 1
drm_rmmap(drm_device_t *dev, drm_local_map_t *map)
d380 1
a380 1
drm_rmmap_ioctl(drm_device_t *dev, void *data, struct drm_file *file_priv)
d407 1
a407 1
drm_cleanup_buf_error(drm_device_t *dev, drm_buf_entry_t *entry)
d431 1
a431 1
drm_do_addbufs_agp(drm_device_t *dev, drm_buf_desc_t *request)
d541 1
a541 1
	    (dma->buf_count + entry->buf_count) * sizeof(*dma->buflist), M_DRM );
d568 1
a568 1
drm_do_addbufs_pci(drm_device_t *dev, drm_buf_desc_t *request)
d720 1
a720 1
drm_do_addbufs_sg(drm_device_t *dev, drm_buf_desc_t *request)
d832 1
a832 1
drm_addbufs_agp(drm_device_t *dev, drm_buf_desc_t *request)
d865 1
a865 1
drm_addbufs_sg(drm_device_t *dev, drm_buf_desc_t *request)
d898 1
a898 1
drm_addbufs_pci(drm_device_t *dev, drm_buf_desc_t *request)
d930 2
a931 1
drm_addbufs_ioctl(drm_device_t *dev, void *data, struct drm_file *file_priv)
d947 1
a947 1
drm_infobufs(drm_device_t *dev, void *data, struct drm_file *file_priv)
d997 1
a997 1
drm_markbufs(drm_device_t *dev, void *data, struct drm_file *file_priv)
d1028 1
a1028 1
drm_freebufs(drm_device_t *dev, void *data, struct drm_file *file_priv)
d1066 1
a1066 1
drm_mapbufs(drm_device_t *dev, void *data, struct drm_file *file_priv)
@


1.9
log
@Kill the rest of the ifdef maze in drm_*.c

I was going to do this per file, but decided all it would achieve was
artificially increasing my commit count.

>Kill the ifdef maze. I've been considering doing this for months, it
>doesn't make getting changes to and  from upstream much easier, and just
>makes the code hard to read.
@
text
@a1067 3
	int retcode = 0;
	const int zero = 0;
	vm_offset_t address;
d1070 1
d1074 2
@


1.8
log
@The mmap offsets for memory buffers currently are the kernel virtual
address.  This is just plain wrong. scatter/gather on amd64 didn't work
here, since char device mmap doesn't take negative offsets so higher
kvas fail.

Instead, prematurely import drm_memrange which is needed for the memory
managers (GEM or TTM), and is used to manage GART space. Then, horribly
abuse it to allocate mmap offsets, fixes up the issues.

"just commit it" art@@.
@
text
@a80 11
#if defined (__FreeBSD__)
	dev->pcirid[resource] = PCIR_BAR(resource);
	dev->pcir[resource] = bus_alloc_resource_any(dev->device,
	    SYS_RES_MEMORY, &dev->pcirid[resource], RF_SHAREABLE);

	DRM_LOCK();
	if (dev->pcir[resource] == NULL) {
		DRM_ERROR("Couldn't find resource 0x%x\n", resource);
		return 1;
	}
#elif defined (__OpenBSD__)
a86 1
#endif
a97 3
#ifdef __FreeBSD__
	return rman_get_start(dev->pcir[resource]);
#elif defined(__NetBSD__) || defined(__OpenBSD__)
a98 1
#endif
a106 3
#ifdef __FreeBSD__
	return rman_get_size(dev->pcir[resource]);
#elif defined(__NetBSD__) || defined(__OpenBSD__)
a107 1
#endif
a205 26
#ifndef __OpenBSD__
	case _DRM_SHM:
		map->handle = malloc(map->size, M_DRM, M_NOWAIT);
		DRM_DEBUG( "%lu %d %p\n",
		    map->size, drm_order(map->size), map->handle );
		if ( !map->handle ) {
			free(map, M_DRM);
			DRM_LOCK();
			return ENOMEM;
		}
		map->offset = (unsigned long)map->handle;
		if ( map->flags & _DRM_CONTAINS_LOCK ) {
			/* Prevent a 2nd X Server from creating a 2nd lock */
			DRM_LOCK();
			if (dev->lock.hw_lock != NULL) {
				DRM_UNLOCK();
				free(map->handle, M_DRM);
				free(map, M_DRM);
				DRM_LOCK();
				return EBUSY;
			}
			dev->lock.hw_lock = map->handle; /* Pointer to lock */
			DRM_UNLOCK();
		}
		break;
#endif
a251 1
#ifdef __OpenBSD__
a252 1
#endif
a270 1
#ifdef __OpenBSD__
a284 1
#endif
a330 3
#ifndef __OpenBSD__
	if (request->type != _DRM_SHM) 
#endif
d345 1
a345 4
#ifdef __FreeBSD__
		if (map->bsr == NULL)
#endif
			drm_ioremapfree(map);
a357 5
#ifndef __OpenBSD__
	case _DRM_SHM:
		free(map->handle, M_DRM);
		break;
#endif
a360 1
#ifdef __OpenBSD__
a361 1
#endif
a369 7
#ifdef __FreeBSD__
	if (map->bsr != NULL) {
		bus_release_resource(dev->device, SYS_RES_MEMORY, map->rid,
		    map->bsr);
	}
#endif

a537 1
#if defined(__OpenBSD__)
a541 5
#else
	temp_buflist = realloc(dma->buflist,
	    (dma->buf_count + entry->buf_count) * sizeof(*dma->buflist), M_DRM,
	    M_NOWAIT);
#endif
a685 1
#if defined(__OpenBSD__)
a688 5
#else
	temp_buflist = realloc(dma->buflist,
	    (dma->buf_count + entry->buf_count) * sizeof(*dma->buflist), M_DRM,
	    M_NOWAIT);
#endif
a802 1
#if defined(__OpenBSD__)
a805 5
#else
	temp_buflist = realloc(dma->buflist,
	    (dma->buf_count + entry->buf_count) * sizeof(*dma->buflist), M_DRM,
	    M_NOWAIT);
#endif
a1071 5
#ifdef __FreeBSD__
	vm_ooffset_t foff;
	vm_size_t size;
	vm_offset_t vaddr;
#elif defined(__NetBSD__) || defined(__OpenBSD__)
a1074 3
#ifdef __NetBSD__
	vsize_t rsize;
#endif
a1075 1
#endif /* __NetBSD__ || __OpenBSD__ */
a1079 1
#if defined(__NetBSD__) || defined(__OpenBSD__)
a1081 1
#endif /* __NetBSD__ || __OpenBSD__ */
a1082 3
#if defined(__FreeBSD__) && __FreeBSD_version >= 500000
	vms = DRM_CURPROC->td_proc->p_vmspace;
#else
a1083 1
#endif
a1107 19
#ifdef __FreeBSD__
	vaddr = round_page((vm_offset_t)vms->vm_daddr + MAXDSIZ);
#if __FreeBSD_version >= 600023
	retcode = vm_mmap(&vms->vm_map, &vaddr, size, PROT_READ | PROT_WRITE,
	    VM_PROT_ALL, MAP_SHARED, OBJT_DEVICE, dev->devnode, foff);
#else
	retcode = vm_mmap(&vms->vm_map, &vaddr, size, PROT_READ | PROT_WRITE,
	    VM_PROT_ALL, MAP_SHARED, SLIST_FIRST(&dev->devnode->si_hlist),
	    foff);
#endif
#elif defined(__NetBSD__) 
	vaddr = p->l_proc->p_emul->e_vm_default_addr(p->l_proc,
	    (vaddr_t)vms->vm_daddr, size);
	rsize = round_page(size);
	DRM_DEBUG("mmap %lx/%ld\n", vaddr, rsize);
	retcode = uvm_mmap(&vms->vm_map, &vaddr, rsize,
	    UVM_PROT_READ | UVM_PROT_WRITE, UVM_PROT_ALL, MAP_SHARED,
	    &vn->v_uobj, foff, p->l_proc->p_rlimit[RLIMIT_MEMLOCK].rlim_cur);
#else /* __OpenBSD__ */
a1111 1
#endif /* __NetBSD__ || __OpenBSD__ */
@


1.7
log
@Update to DRM git as of a few days ago. This mostly affects the
card-specific files with a few minor changes elsewhere.

The main change to the OpenBSD specific stuff is the change to the irq
api due to the vblank rework.

4 more large bugs known, I have a fix for one.

Tested by many.  prompted by deraadt@@.
@
text
@d194 16
d384 1
a384 1
		request->handle = (void *)request->offset;
d440 2
d697 1
a697 1
	
d1205 1
a1205 1
		foff = map->offset;
@


1.6
log
@Make *drm(4) use D_CLONE so that the per-open data actually works. Since
i'm modifying this code anyway, prepare for privsep by making it so that
"master" openers must be root, and remove some spurious suser() checks.
For example, every DRM_ROOT_ONLY ioctl is also DRM_MASTER. Without this
change, privsep wouldn't work since the fd is no longer root owned.

With this, X privsep should work as soon as the userland bits are done
(currently unwritten).

Looked over by kettenis@@, ok thib@@.
@
text
@a905 1
	DRM_SPINLOCK(&dev->dma_lock);
d907 1
a907 2
	if (request->count < 0 || request->count > 4096) {
		DRM_SPINUNLOCK(&dev->dma_lock);
a908 1
	}
d911 1
a911 2
	if (order < DRM_MIN_ORDER || order > DRM_MAX_ORDER) {
		DRM_SPINUNLOCK(&dev->dma_lock);
d913 2
a914 1
	}
a938 1
	DRM_SPINLOCK(&dev->dma_lock);
d940 1
a940 2
	if (request->count < 0 || request->count > 4096) {
		DRM_SPINUNLOCK(&dev->dma_lock);
a941 1
	}
d944 1
a944 2
	if (order < DRM_MIN_ORDER || order > DRM_MAX_ORDER) {
		DRM_SPINUNLOCK(&dev->dma_lock);
d946 2
a947 1
	}
d972 1
a972 4
	DRM_SPINLOCK(&dev->dma_lock);

	if (request->count < 0 || request->count > 4096) {
		DRM_SPINUNLOCK(&dev->dma_lock);
a973 1
	}
d976 1
a976 2
	if (order < DRM_MIN_ORDER || order > DRM_MAX_ORDER){
		DRM_SPINUNLOCK(&dev->dma_lock);
d978 2
a979 1
	}
@


1.5
log
@Kill vtophys in drm.

The code we inherited from FreeBSD used vtophys is a fair few places,
nuke that and replace with bus_dma. technically _DRM_SHM should be
managed with uao_create(), but until we move away from mmap and into an
ioctl to map, this will do.

This also paves the way for amd64 support (it lacks vtophys).

ok kettenis, miod looked at it a while back too.
@
text
@a350 3
	if (!DRM_SUSER(DRM_CURPROC) && request->type != _DRM_AGP)
		return EACCES;

a943 5
	if (!DRM_SUSER(DRM_CURPROC)) {
		DRM_SPINUNLOCK(&dev->dma_lock);
		return EACCES;
	}

a978 5

	if (!DRM_SUSER(DRM_CURPROC)) {
		DRM_SPINUNLOCK(&dev->dma_lock);
		return EACCES;
	}
@


1.4
log
@currently agp_i810 needs to map the same BAR as inteldrm, this obviously
fails.

In order to allow this, implement an API so that drm and agp can share
mappings for the BARs. Now it works as it should.

tested by many.
ok kettenis, miod said he'd look at it when it's in tree.
@
text
@d210 1
d228 1
d235 1
d282 3
d303 16
d368 3
a370 1
	if (request->type != _DRM_SHM) {
a371 1
	}
d401 1
d405 1
d409 3
@


1.3
log
@Convert the list of agp memory over to a TAILQ instead of using a hand-
rolled list.

Tested by many
@
text
@a69 3
#ifdef __OpenBSD__
#define PCIR_BAR(x)     (PCI_MAPS + (x) * 4)
#endif
d81 1
a82 1
#if defined (__FreeBSD__)
d92 2
a93 1
	dev->pcir[resource] = malloc(sizeof(*(dev->pcir)), M_DRM, M_NOWAIT | M_ZERO);
d95 1
a95 2
		DRM_ERROR("Couldn't allocate memory for resource 0x%x\n", resource);
		DRM_LOCK();
a97 15
	dev->pcir[resource]->maptype = 
		pci_mapreg_type(dev->pa.pa_pc, dev->pa.pa_tag, 
		    dev->pcirid[resource]);
	if(pci_mapreg_info(dev->pa.pa_pc, dev->pa.pa_tag,
	    dev->pcirid[resource],
	    dev->pcir[resource]->maptype,
	    &(dev->pcir[resource]->base),
	    &(dev->pcir[resource]->size),
	    &(dev->pcir[resource]->flags))) {
		dev->pcir[resource]->base = 0;
		dev->pcir[resource]->size = 0;
	}
	if(dev->pcir[resource]->maptype == PCI_MAPREG_TYPE_MEM)
		dev->pcir[resource]->flags |= BUS_SPACE_MAP_LINEAR;
	DRM_LOCK();
d126 1
a126 1
	return dev->pcir[resource]->size;
a195 1
		map->bst = dev->pa.pa_iot;
@


1.2
log
@destatic the static functions, they only make debugging harder.
While i'm here:
remove a couple of debug printfs that shouldn't have gone in anyway.
trowel on some KNF (I really need to get around to sending some of this upstream).
remove some netbsd specific code that netbsd doesn't have anymore.

ok tedu.
@
text
@d154 4
d266 9
a274 2
#if 0 /* disabled */
		for (entry = dev->agp->memory; entry; entry = entry->next) {
d282 1
a282 1
		if (!valid) {
d285 1
d493 4
d519 1
d524 1
a524 1
#if 0 /* disabled */
d526 1
a526 2
	for (agp_entry = dev->agp->memory; agp_entry;
	    agp_entry = agp_entry->next) {
d534 1
a534 1
	if (!valid) {
@


1.1
log
@Initial import of the DRM (direct rendering manager).

This is the kernel part necessary for DRI support in X. Disabled for now
because it still has a few bugs, but now I can work on it in tree. Also
requires the requisite bits in X, which are currently under discussion
on how to deal with them with privsep. ported from a combination of the
free and netbsd implementations.

Known bugs:
1) only the first occurence of X in any session will have dri, after
that something prevents it working.
2) if the machine does not have a dri capable card, the kernel panics.
Something's up in one of the probe functions. I haven't been able to
find it though.
3) radeon cards need to be forced to use PCI mode otherwise they get
into an infinite loop.

This is known to at least kinda work with SiS, radeons in pci mode and
intel cards.

ok deraadt, kinda ok art, a few other people had a quick look.
@
text
@d40 6
d49 2
a50 1
int drm_order(unsigned long size)
d67 2
a68 1
static int drm_alloc_resource(drm_device_t *dev, int resource)
d103 1
a103 1
			dev->pcirid[resource]);
d105 5
a109 5
			dev->pcirid[resource],
			dev->pcir[resource]->maptype,
			&(dev->pcir[resource]->base),
			&(dev->pcir[resource]->size),
			&(dev->pcir[resource]->flags))) {
d122 2
a123 1
unsigned long drm_get_resource_start(drm_device_t *dev, unsigned int resource)
d135 2
a136 1
unsigned long drm_get_resource_len(drm_device_t *dev, unsigned int resource)
d148 2
a149 1
int drm_addmap(drm_device_t * dev, unsigned long offset, unsigned long size,
a153 2
	/*drm_agp_mem_t *entry;
	int valid;*/
d228 1
a228 1
			   map->size, drm_order(map->size), map->handle );
d326 2
a327 1
int drm_addmap_ioctl(drm_device_t *dev, void *data, struct drm_file *file_priv)
d360 2
a361 1
void drm_rmmap(drm_device_t *dev, drm_local_map_t *map)
d413 2
a414 1
int drm_rmmap_ioctl(drm_device_t *dev, void *data, struct drm_file *file_priv)
d440 2
a441 1
static void drm_cleanup_buf_error(drm_device_t *dev, drm_buf_entry_t *entry)
d464 2
a465 1
static int drm_do_addbufs_agp(drm_device_t *dev, drm_buf_desc_t *request)
a468 2
	/*drm_agp_mem_t *agp_entry;
	int valid*/
d487 1
a487 1
		? round_page(size) : size;
d538 5
a542 5
		buf          = &entry->buflist[entry->buf_count];
		buf->idx     = dma->buf_count + entry->buf_count;
		buf->total   = alignment;
		buf->order   = order;
		buf->used    = 0;
d544 1
a544 1
		buf->offset  = (dma->byte_count + offset);
d547 1
a547 1
		buf->next    = NULL;
d549 1
a549 1
		buf->file_priv    = NULL;
a569 1
	/* XXX overflow */
d603 2
a604 1
static int drm_do_addbufs_pci(drm_device_t *dev, drm_buf_desc_t *request)
d627 1
a627 1
		   request->count, request->size, size, order );
d630 1
a630 1
		? round_page(size) : size;
d662 1
a662 1
		   dma->page_count + (count << page_order) );
d684 2
a685 2
				   dma->page_count + page_count,
				   (char *)dmah->vaddr + PAGE_SIZE * i );
d690 2
a691 2
		      offset + size <= total && entry->buf_count < count ;
		      offset += alignment, ++entry->buf_count ) {
d717 1
a717 1
				   entry->buf_count, buf->address );
d761 2
a762 1
static int drm_do_addbufs_sg(drm_device_t *dev, drm_buf_desc_t *request)
d784 1
a784 1
		? round_page(size) : size;
d812 5
a816 5
		buf          = &entry->buflist[entry->buf_count];
		buf->idx     = dma->buf_count + entry->buf_count;
		buf->total   = alignment;
		buf->order   = order;
		buf->used    = 0;
d818 1
a818 1
		buf->offset  = (dma->byte_count + offset);
d821 1
a821 1
		buf->next    = NULL;
d823 1
a823 1
		buf->file_priv    = NULL;
d836 1
a836 1
			   entry->buf_count, buf->address );
d879 2
a880 1
int drm_addbufs_agp(drm_device_t *dev, drm_buf_desc_t *request)
d915 2
a916 1
int drm_addbufs_sg(drm_device_t *dev, drm_buf_desc_t *request)
d956 2
a957 1
int drm_addbufs_pci(drm_device_t *dev, drm_buf_desc_t *request)
d997 2
a998 1
int drm_addbufs_ioctl(drm_device_t *dev, void *data, struct drm_file *file_priv)
d1013 2
a1014 1
int drm_infobufs(drm_device_t *dev, void *data, struct drm_file *file_priv)
d1049 5
a1053 5
					   i,
					   dma->bufs[i].buf_count,
					   dma->bufs[i].buf_size,
					   dma->bufs[i].freelist.low_mark,
					   dma->bufs[i].freelist.high_mark );
d1063 2
a1064 1
int drm_markbufs(drm_device_t *dev, void *data, struct drm_file *file_priv)
d1071 1
a1071 1
		   request->size, request->low_mark, request->high_mark );
d1094 2
a1095 1
int drm_freebufs(drm_device_t *dev, void *data, struct drm_file *file_priv)
d1114 1
a1114 1
				   idx, dma->buf_count - 1 );
d1121 1
a1121 1
				   DRM_CURRENTPID);
d1132 2
a1133 1
int drm_mapbufs(drm_device_t *dev, void *data, struct drm_file *file_priv)
a1157 1
/* makedev( ,dev->unit) */
d1159 1
a1159 1
		return 0;	/* FIXME: Shouldn't this be EINVAL or something? */
@

